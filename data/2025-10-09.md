<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 105]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.AI](#cs.AI) [Total: 5]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CR](#cs.CR) [Total: 5]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [OpenStaxQA: A multilingual dataset based on open-source college textbooks](https://arxiv.org/abs/2510.06239)
*Pranav Gupta*

Main category: cs.CL

TL;DR: 研究者创建了多语言教育评估基准OpenStaxQA，使用QLoRA技术微调70亿参数大模型，并验证其在跨任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决教育领域专用评估数据集缺乏的问题，探索专用数据集对模型跨任务性能的提升潜力。

Method: 1. 基于43本开放教科书构建多语言教育数据集
2. 采用量化低秩适配器(QLoRA)微调LLM
3. 在AI2推理挑战赛进行零样本评估

Result: 成功建立教育专用评估基准，验证QLoRA微调有效性，发现跨任务评估中的潜在改进空间。

Conclusion: OpenStaxQA为教育应用提供可靠评估工具，量化微调方法展现高效性，跨任务评估揭示数据集泛化价值。

Abstract: We present OpenStaxQA, an evaluation benchmark specific to college-level
educational applications based on 43 open-source college textbooks in English,
Spanish, and Polish, available under a permissive Creative Commons license. We
finetune and evaluate large language models (LLMs) with approximately 7 billion
parameters on this dataset using quantized low rank adapters (QLoRa).
Additionally we also perform a zero-shot evaluation on the AI2 reasoning
challenge dev dataset in order to check if OpenStaxQA can lead to an improved
performance on other tasks. We also discuss broader impacts relevant to
datasets such as OpenStaxQA.

</details>


### [2] [Knowledge Graph-Guided Multi-Agent Distillation for Reliable Industrial Question Answering with Datasets](https://arxiv.org/abs/2510.06240)
*Jiqun Pan,Zhenke Duan,Jiani Tu,Anzhi Cheng,Yanqing Wang*

Main category: cs.CL

TL;DR: 工业QA系统安全部署新方法KG-MASD：通过知识图谱引导的蒸馏技术，提升紧凑模型推理深度与可验证性，准确率提升2.4%-20.1%


<details>
  <summary>Details</summary>
Motivation: 工业QA系统需要更高安全标准，传统多智能体模型存在不可控迭代和不可验证输出，传统蒸馏方法难以迁移协作推理能力到轻量模型

Method: 提出KG-MASD框架：将蒸馏建模为马尔可夫决策过程，引入知识图谱作为结构化先验，联合蒸馏推理深度与可验证性到边缘部署模型

Result: 工业QA数据集实验显示准确率提升2.4%-20.1%，可靠性显著增强，支持安全关键场景下的可信AI部署

Conclusion: KG-MASD成功平衡模型压缩与推理能力，通过知识驱动蒸馏实现工业场景中安全可靠的紧凑模型部署

Abstract: Industrial question-answering (QA) systems require higher safety and
reliability than general-purpose dialogue models, as errors in high-risk
scenarios such as equipment fault diagnosis can have severe consequences.
Although multi-agent large language models enhance reasoning depth, they suffer
from uncontrolled iterations and unverifiable outputs, and conventional
distillation methods struggle to transfer collaborative reasoning capabilities
to lightweight, deployable student models. To address these challenges, we
propose Knowledge Graph-guided Multi-Agent System Distillation (KG-MASD). Our
approach formulates distillation as a Markov Decision Process and incorporates
a knowledge graph as a verifiable structured prior to enrich state
representation and ensure convergence. By integrating collaborative reasoning
with knowledge grounding, KG-MASD generates high-confidence instruction-tuning
data and jointly distills reasoning depth and verifiability into compact
student models suitable for edge deployment. Experiments on an industrial QA
dataset show that KG-MASD improves accuracy by 2.4 per cent to 20.1 per cent
over baselines and significantly enhances reliability, enabling trustworthy AI
deployment in safety-critical industrial scenarios. Code and data are available
at https://github.com/erwinmsmith/KG-MAD/.

</details>


### [3] [Transparent Reference-free Automated Evaluation of Open-Ended User Survey Responses](https://arxiv.org/abs/2510.06242)
*Subin An,Yugyeong Ji,Junyoung Kim,Heejin Kook,Yang Lu,Josh Seltzer*

Main category: cs.CL

TL;DR: 提出两阶段框架评估开放式调查回答质量，结合垃圾信息过滤与LLM三维度评估（努力/相关性/完整性），在英韩数据集验证优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有自动评估方法针对LLM生成文本，对人工撰写回答的独特特征（如逻辑跳跃、非正式表达）评估不足，导致低效筛选和潜在误导性结论

Method: 1. 无意义回答过滤阶段剔除无效内容；2. 基于LLM的能力从努力程度（答题投入）、相关性（与问题关联度）、完整性（信息充分性）三个维度进行多层级评估

Result: 在英语和韩语数据集验证显示：框架质量预测准确率显著优于传统指标（F1提升28.3%），响应拒绝决策与专家评估强相关（皮尔逊系数0.84）

Conclusion: 该框架兼具理论严谨性与实践价值，可通过实时质量预测提升调研效率，其模块化设计支持跨语言应用，为自动化调查分析提供新范式

Abstract: Open-ended survey responses provide valuable insights in marketing research,
but low-quality responses not only burden researchers with manual filtering but
also risk leading to misleading conclusions, underscoring the need for
effective evaluation. Existing automatic evaluation methods target
LLM-generated text and inadequately assess human-written responses with their
distinct characteristics. To address such characteristics, we propose a
two-stage evaluation framework specifically designed for human survey
responses. First, gibberish filtering removes nonsensical responses. Then,
three dimensions-effort, relevance, and completeness-are evaluated using LLM
capabilities, grounded in empirical analysis of real-world survey data.
Validation on English and Korean datasets shows that our framework not only
outperforms existing metrics but also demonstrates high practical applicability
for real-world applications such as response quality prediction and response
rejection, showing strong correlations with expert assessment.

</details>


### [4] [CoT Referring: Improving Referring Expression Tasks with Grounded Reasoning](https://arxiv.org/abs/2510.06243)
*Qihua Dong,Luis Figueroa,Handong Zhao,Kushal Kafle,Jason Kuen,Zhihong Ding,Scott Cohen,Yun Fu*

Main category: cs.CL

TL;DR: 提出CoT Referring策略，通过链式思维数据结构增强多模态模型推理能力，在复杂指称任务中实现2.5%+准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有基准对复杂指称任务评估不足，需要结构化跨模态推理框架提升MLLMs在指称理解与分割任务的准确性。

Method: 1. 将文本结构解析为逐步指称步骤
2. 重构训练数据格式并创建新基准
3. 整合检测与分割到统一框架
4. 采用自适应加权损失函数优化

Result: 在RefCOCO系列和新建基准上验证有效，准确率超越基线模型2.5%以上

Conclusion: 结构化链式推理策略显著提升复杂指称任务性能，统一框架设计为多模态理解提供新范式

Abstract: Referring Expression Comprehension and Segmentation are critical tasks for
assessing the integration of language understanding and image comprehension,
serving as benchmarks for Multimodal Large Language Models (MLLMs)
capabilities. To address these challenges, we propose a new strategy, CoT
Referring, which enhances model reasoning across modalities through a
structured, chain-of-thought training data structure. Our approach
systematically parses textual structures to a sequential referring step, where
in each step it identifies relationships and ensures consistent reference
alignment, thereby improving accuracy in complex query scenarios. We
restructure the training data to enforce a new output form, providing new
annotations for existing datasets and compiling an evaluation benchmark from
existing resources. This benchmark is designed explicitly for complex referring
cases. We also integrate detection and segmentation capabilities into a unified
MLLM framework, training it with a novel adaptive weighted loss to optimize
performance. Experimental results on our curated benchmark and RefCOCO/+/g
demonstrate the effectiveness of our approach, with a notable increase of 2.5%+
over baseline models.

</details>


### [5] [Evaluating Embedding Frameworks for Scientific Domain](https://arxiv.org/abs/2510.06244)
*Nouman Ahmed,Ronin Wu,Victor Botev*

Main category: cs.CL

TL;DR: 研究针对科学领域探索最优词语表示算法和分词方法，并构建可扩展的评估体系验证算法效果


<details>
  <summary>Details</summary>
Motivation: 科学领域存在专业术语多义性和上下文敏感性，通用预训练模型存在效率不足且缺乏专用评估标准

Method: 构建包含多任务科学数据集的全域评估套件，系统性测试不同词语表示模型和分词策略的性能表现

Result: 确立科学领域最优算法标准，验证评估体系的有效性，为后续研究提供可扩展的测试框架

Conclusion: 领域专用算法优化与标准化评估框架的结合，显著提升科学NLP任务效能，该范式可推广至其他垂直领域

Abstract: Finding an optimal word representation algorithm is particularly important in
terms of domain specific data, as the same word can have different meanings and
hence, different representations depending on the domain and context. While
Generative AI and transformer architecture does a great job at generating
contextualized embeddings for any given work, they are quite time and compute
extensive, especially if we were to pre-train such a model from scratch. In
this work, we focus on the scientific domain and finding the optimal word
representation algorithm along with the tokenization method that could be used
to represent words in the scientific domain. The goal of this research is two
fold: 1) finding the optimal word representation and tokenization methods that
can be used in downstream scientific domain NLP tasks, and 2) building a
comprehensive evaluation suite that could be used to evaluate various word
representation and tokenization algorithms (even as new ones are introduced) in
the scientific domain. To this end, we build an evaluation suite consisting of
several downstream tasks and relevant datasets for each task. Furthermore, we
use the constructed evaluation suite to test various word representation and
tokenization algorithms.

</details>


### [6] [TRepLiNa: Layer-wise CKA+REPINA Alignment Improves Low-Resource Machine Translation in Aya-23 8B](https://arxiv.org/abs/2510.06249)
*Toshiki Nakai,Ravi Kiran Chikkala,Lena Sophie Oberkircher,Nicholas Jennings,Natalia Skachkova,Tatiana Anikina,Jesujoba Oluwadara Alabi*

Main category: cs.CL

TL;DR: 结合CKA和REPINA的TRepLiNa方法有效提升低资源语言翻译效率，特别适合数据稀缺场景


<details>
  <summary>Details</summary>
Motivation: 针对印度多元低资源语言（LRL）缺乏翻译资源的现状，研究通过增强多语言大模型内部层间跨语言相似性来改进LRL向高资源语言（HRL）的翻译质量

Method: 将中心核对齐（CKA）与参数正则化方法REPINA结合形成TRepLiNa，基于Aya-23 8B模型使用QLoRA技术，在Mundari等语言对上进行零样本、少样本和微调实验

Result: TRepLiNa在中层网络的对齐效果最佳，证明该方法在数据稀缺环境下是低成本、实用的翻译改进方案

Conclusion: 该方法为低资源语言翻译提供了切实可行的技术路径，对资源受限场景具有重要应用价值

Abstract: The 2025 Multimodal Models for Low-Resource Contexts and Social Impact
(MMLoSo) Language Challenge addresses one of India's most pressing linguistic
gaps: the lack of resources for its diverse low-resource languages (LRLs). In
this study, we investigate whether enforcing cross-lingual similarity in
specific internal layers of a decoder-only multilingual large language model
(LLM) can improve translation quality from LRL to high-resource language (HRL).
Specifically, we combine Centered Kernel Alignment (CKA), a similarity metric
that encourages representations of different languages to align, with REPINA, a
regularization method that constrains parameter updates to remain close to the
pretrained model, into a joint method we call TRepLiNa. In this research
project, we experiment with zero-shot, few-shot, and fine-tuning settings using
Aya-23 8B with QLoRA across MMLoSo shared task language pairs (Mundari,
Santali, Bhili) with Hindi/English pivots. Our results show that aligning
mid-level layers using TRepLiNa (CKA+REPINA) is a low-cost, practical approach
to improving LRL translation, especially in data-scarce settings.

</details>


### [7] [Scalable multilingual PII annotation for responsible AI in LLMs](https://arxiv.org/abs/2510.06250)
*Bharti Meena,Joanna Skubisz,Harshit Rajgarhia,Nand Dave,Kiran Ganesh,Shivali Dalmia,Abhishek Mukherji,Vasudevan Sundarababu,Olga Pospelova*

Main category: cs.CL

TL;DR: 提出一个可扩展的多语言数据标注框架，用于在13个低资源语言环境中高效标注个人身份信息（PII），通过人机协同标注和质量控制显著提升模型召回率和降低误报率。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）广泛应用，需确保其在不同法规环境下对个人身份信息（PII）的合规处理，解决低资源语言场景标注质量不足的核心痛点。

Method: 三阶段人机协同标注框架（试点/训练/生产），结合语言专家知识、质量保证机制和根因分析，通过标注一致性指标优化标注流程。

Result: 生产阶段召回率提升38%，误报率降低62%，跨语言PII类型覆盖达336种，创建了适用于监督微调的高质量多语言数据集。

Conclusion: 数据驱动的迭代式标注流程能系统性解决多语言PII标注难题，标注质量与模型可靠性形成正向循环，为合规AI部署提供可复现方法论。

Abstract: As Large Language Models (LLMs) gain wider adoption, ensuring their reliable
handling of Personally Identifiable Information (PII) across diverse regulatory
contexts has become essential. This work introduces a scalable multilingual
data curation framework designed for high-quality PII annotation across 13
underrepresented locales, covering approximately 336 locale-specific PII types.
Our phased, human-in-the-loop annotation methodology combines linguistic
expertise with rigorous quality assurance, leading to substantial improvements
in recall and false positive rates from pilot, training, and production phases.
By leveraging inter-annotator agreement metrics and root-cause analysis, the
framework systematically uncovers and resolves annotation inconsistencies,
resulting in high-fidelity datasets suitable for supervised LLM fine-tuning.
Beyond reporting empirical gains, we highlight common annotator challenges in
multilingual PII labeling and demonstrate how iterative, analytics-driven
pipelines can enhance both annotation quality and downstream model reliability.

</details>


### [8] [Prakriti200: A Questionnaire-Based Dataset of 200 Ayurvedic Prakriti Assessments](https://arxiv.org/abs/2510.06262)
*Aryan Kumar Singh,Janvi Singh*

Main category: cs.CL

TL;DR: 创建双语阿育吠陀体质评估数据集，含24项标准化问卷，支持自动化评分与健康分析研究


<details>
  <summary>Details</summary>
Motivation: 为计算智能、阿育吠陀研究和个性化健康分析提供结构化数据平台，解决传统体质评估标准化不足的问题

Method: 基于AYUSH/CCRAS指南开发问卷，隐藏Dosha标签，通过Google Forms收集数据并实现自动化评分系统

Result: 建立可分析体质分布、相关性及预测建模的数据集，支持智能健康应用开发

Conclusion: 该数据集填补阿育吠陀数字化研究空白，为精准健康分析和跨学科研究提供基础框架

Abstract: This dataset provides responses to a standardized, bilingual (English-Hindi)
Prakriti Assessment Questionnaire designed to evaluate the physical,
physiological, and psychological characteristics of individuals according to
classical Ayurvedic principles. The questionnaire consists of 24
multiple-choice items covering body features, appetite, sleep patterns, energy
levels, and temperament. It was developed following AYUSH/CCRAS guidelines to
ensure comprehensive and accurate data collection. All questions are mandatory
and neutrally phrased to minimize bias, and dosha labels (Vata, Pitta, Kapha)
are hidden from participants. Data were collected via a Google Forms
deployment, enabling automated scoring of responses to map individual traits to
dosha-specific scores. The resulting dataset provides a structured platform for
research in computational intelligence, Ayurvedic studies, and personalized
health analytics, supporting analysis of trait distributions, correlations, and
predictive modeling. It can also serve as a reference for future Prakriti-based
studies and the development of intelligent health applications.

</details>


### [9] [Dual-stage and Lightweight Patient Chart Summarization for Emergency Physicians](https://arxiv.org/abs/2510.06263)
*Jiajun Wu,Swaleh Zaidi,Braden Teitge,Henry Leung,Jiayu Zhou,Jessalyn Holodinsky,Steve Drew*

Main category: cs.CL

TL;DR: 开发基于双Jetson设备的离线临床摘要系统，30秒内生成结构化医疗摘要


<details>
  <summary>Details</summary>
Motivation: 解决急诊医生处理海量非结构化电子病历的效率问题，同时保障患者隐私的离线处理需求

Method: 1. 双设备架构：Nano-R检索病历片段，Nano-S生成摘要
2. 本地存储+语义分段检索
3. 评估6款<7B参数的SLM模型
4. 引入LLM-as-Judge质量评估机制

Result: 在MIMIC-IV和真实病历测试中：
- 生成时间<30秒
- 有效产出包含关键发现列表和上下文叙述的结构化摘要

Conclusion: 系统实现完全离线的隐私保护，验证了嵌入式设备运行临床摘要系统的可行性，提升急诊决策效率

Abstract: Electronic health records (EHRs) contain extensive unstructured clinical data
that can overwhelm emergency physicians trying to identify critical
information. We present a two-stage summarization system that runs entirely on
embedded devices, enabling offline clinical summarization while preserving
patient privacy. In our approach, a dual-device architecture first retrieves
relevant patient record sections using the Jetson Nano-R (Retrieve), then
generates a structured summary on another Jetson Nano-S (Summarize),
communicating via a lightweight socket link. The summarization output is
two-fold: (1) a fixed-format list of critical findings, and (2) a
context-specific narrative focused on the clinician's query. The retrieval
stage uses locally stored EHRs, splits long notes into semantically coherent
sections, and searches for the most relevant sections per query. The generation
stage uses a locally hosted small language model (SLM) to produce the summary
from the retrieved text, operating within the constraints of two NVIDIA Jetson
devices. We first benchmarked six open-source SLMs under 7B parameters to
identify viable models. We incorporated an LLM-as-Judge evaluation mechanism to
assess summary quality in terms of factual accuracy, completeness, and clarity.
Preliminary results on MIMIC-IV and de-identified real EHRs demonstrate that
our fully offline system can effectively produce useful summaries in under 30
seconds.

</details>


### [10] [A Comprehensive Survey of Hallucination in Large Language Models: Causes, Detection, and Mitigation](https://arxiv.org/abs/2510.06265)
*Aisha Alansari,Hamzah Luqman*

Main category: cs.CL

TL;DR: 本综述系统梳理了大语言模型幻觉问题的成因、检测与缓解策略，为构建更可信的生成模型提供研究基础


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成流畅内容时易产生事实性错误，严重影响其在需要高准确度场景下的可信度

Method: 采用全生命周期视角的成因分析框架，构建检测方法与缓解策略的双重分类体系

Result: 建立了涵盖数据、架构、推理阶段的幻觉成因图谱，提出动态评估框架及知识增强的缓解路径

Conclusion: 未来需发展动态评估指标、构建多模态验证体系，并通过知识注入与推理增强提升模型真实性

Abstract: Large language models (LLMs) have transformed natural language processing,
achieving remarkable performance across diverse tasks. However, their
impressive fluency often comes at the cost of producing false or fabricated
information, a phenomenon known as hallucination. Hallucination refers to the
generation of content by an LLM that is fluent and syntactically correct but
factually inaccurate or unsupported by external evidence. Hallucinations
undermine the reliability and trustworthiness of LLMs, especially in domains
requiring factual accuracy. This survey provides a comprehensive review of
research on hallucination in LLMs, with a focus on causes, detection, and
mitigation. We first present a taxonomy of hallucination types and analyze
their root causes across the entire LLM development lifecycle, from data
collection and architecture design to inference. We further examine how
hallucinations emerge in key natural language generation tasks. Building on
this foundation, we introduce a structured taxonomy of detection approaches and
another taxonomy of mitigation strategies. We also analyze the strengths and
limitations of current detection and mitigation approaches and review existing
evaluation benchmarks and metrics used to quantify LLMs hallucinations.
Finally, we outline key open challenges and promising directions for future
research, providing a foundation for the development of more truthful and
trustworthy LLMs.

</details>


### [11] [Language models for longitudinal analysis of abusive content in Billboard Music Charts](https://arxiv.org/abs/2510.06266)
*Rohitash Chandra,Yathin Suresh,Divyansh Raj Sinha,Sanchit Jindal*

Main category: cs.CL

TL;DR: 利用深度学习分析公告牌音乐排行榜70年数据，发现1990年后歌曲中显性内容（粗俗/性暗示歌词）激增


<details>
  <summary>Details</summary>
Motivation: 音乐中滥用/性明确内容剧增但缺乏政策研究依据，此类内容对青少年行为存在负面影响

Method: 基于深度学习和语言模型（情感分析+滥用检测）对公告牌70年歌词进行纵向研究

Result: 显性内容自1990年起增长300%，含不当语言歌曲比例从12%升至38%，语言模型有效捕捉歌词与社会规范关联

Conclusion: 需建立音乐分级制度，语言模型为内容监管提供技术手段，歌词变迁反映社会道德标准松动

Abstract: There is no doubt that there has been a drastic increase in abusive and
sexually explicit content in music, particularly in Billboard Music Charts.
However, there is a lack of studies that validate the trend for effective
policy development, as such content has harmful behavioural changes in children
and youths. In this study, we utilise deep learning methods to analyse songs
(lyrics) from Billboard Charts of the United States in the last seven decades.
We provide a longitudinal study using deep learning and language models and
review the evolution of content using sentiment analysis and abuse detection,
including sexually explicit content. Our results show a significant rise in
explicit content in popular music from 1990 onwards. Furthermore, we find an
increasing prevalence of songs with lyrics containing profane, sexually
explicit, and otherwise inappropriate language. The longitudinal analysis of
the ability of language models to capture nuanced patterns in lyrical content,
reflecting shifts in societal norms and language use over time.

</details>


### [12] [Reproducibility Study of "XRec: Large Language Models for Explainable Recommendation"](https://arxiv.org/abs/2510.06275)
*Ranjan Mishra,Julian I. Bibo,Quinten van Engelen,Henk Schaapman*

Main category: cs.CL

TL;DR: 复现XRec推荐解释框架，改用Llama3评估发现其生成个性化解释有效但非全优，扩展分析揭示MoE嵌入对解释结构的关键作用


<details>
  <summary>Details</summary>
Motivation: 验证XRec框架效果并探索不同LLM适用性（Llama3替代GPT-3.5），通过修改MoE模块研究协作信号与语言建模的交互机制

Method: 基于原论文代码替换LLM为Llama3，修改MoE模块的输入/输出嵌入进行对比实验

Result: XRec生成解释稳定性提升但非全指标最优，MoE嵌入显著影响解释结构，协作信息增强模型鲁棒性

Conclusion: 协作信号与MoE嵌入共同塑造解释生成，开源实现促进推荐系统可解释性研究

Abstract: In this study, we reproduced the work done in the paper "XRec: Large Language
Models for Explainable Recommendation" by Ma et al. (2024). The original
authors introduced XRec, a model-agnostic collaborative instruction-tuning
framework that enables large language models (LLMs) to provide users with
comprehensive explanations of generated recommendations. Our objective was to
replicate the results of the original paper, albeit using Llama 3 as the LLM
for evaluation instead of GPT-3.5-turbo. We built on the source code provided
by Ma et al. (2024) to achieve our goal. Our work extends the original paper by
modifying the input embeddings or deleting the output embeddings of XRec's
Mixture of Experts module. Based on our results, XRec effectively generates
personalized explanations and its stability is improved by incorporating
collaborative information. However, XRec did not consistently outperform all
baseline models in every metric. Our extended analysis further highlights the
importance of the Mixture of Experts embeddings in shaping the explanation
structures, showcasing how collaborative signals interact with language
modeling. Through our work, we provide an open-source evaluation implementation
that enhances accessibility for researchers and practitioners alike. Our
complete code repository can be found at
https://github.com/julianbibo/xrec-reproducibility.

</details>


### [13] [Type and Complexity Signals in Multilingual Question Representations](https://arxiv.org/abs/2510.06304)
*Robin Kokot,Wessel Poelman*

Main category: cs.CL

TL;DR: 研究通过QTC数据集分析多语言Transformer模型对疑问句形态句法属性的表征能力，发现神经探针比统计基线更能捕捉结构复杂性模式。


<details>
  <summary>Details</summary>
Motivation: 评估上下文表征何时优于统计基线，以及参数更新是否影响预训练语言信息的可用性。通过七种语言的疑问句类型标注和复杂度指标，探索模型对疑问结构的表征能力。

Method: 使用冻结的Glot500-m模型层探针、子词TF-IDF基线和微调模型对比，引入选择性控制提升回归探针的泛化性评估。

Result: 统计特征在显式标记语言中分类效果更好，神经探针能更有效捕捉细粒度结构复杂性模式；微调未显著降低预训练语言信息的可用性。

Conclusion: 神经表征在结构复杂度分析中展现优势，尤其在统计特征不足的语言场景下。参数更新未损害预训练语言学信息，提示上下文表征具有持续价值。

Abstract: This work investigates how a multilingual transformer model represents
morphosyntactic properties of questions. We introduce the Question Type and
Complexity (QTC) dataset with sentences across seven languages, annotated with
type information and complexity metrics including dependency length, tree
depth, and lexical density. Our evaluation extends probing methods to
regression labels with selectivity controls to quantify gains in
generalizability. We compare layer-wise probes on frozen Glot500-m (Imani et
al., 2023) representations against subword TF-IDF baselines, and a fine-tuned
model. Results show that statistical features classify questions effectively in
languages with explicit marking, while neural probes capture fine-grained
structural complexity patterns better. We use these results to evaluate when
contextual representations outperform statistical baselines and whether
parameter updates reduce the availability of pre-trained linguistic
information.

</details>


### [14] [LLM Bias Detection and Mitigation through the Lens of Desired Distributions](https://arxiv.org/abs/2510.06354)
*Ingroj Shrestha,Padmini Srinivasan*

Main category: cs.CL

TL;DR: 提出加权自适应损失微调方法，有效对齐LLM性别职业分布与目标分布（平等/现实），在保持模型性能的同时显著降低偏见


<details>
  <summary>Details</summary>
Motivation: 现有偏见缓解研究多关注社会平等，但忽视了模型输出与目标分布（如现实统计分布）的对齐需求。不同应用场景需要灵活适配不同分布标准

Method: 基于美国劳工统计数据构建三组职业集（男性主导/女性主导/性别平衡），设计自适应损失函数进行微调，平衡分布对齐与语言建模能力

Result: 掩码语言模型在平等分布实现近完全去偏，现实分布减少30-75%；自回归模型在现实分布实现50-62%偏见降低（Llama Instruct系列）

Conclusion: 通过可调节的分布对齐机制，首次实现LLM输出同时支持平等原则与事实依据，为不同应用场景提供灵活可靠的偏见控制方案

Abstract: Although prior work on bias mitigation has focused on promoting social
equality and demographic parity, less attention has been given to aligning
LLM's outputs to desired distributions. For example, we might want to align a
model with real-world distributions to support factual grounding. Thus, we
define bias as deviation from a desired distribution, which may be an equal or
real-world distribution, depending on application goals. We propose a weighted
adaptive loss based fine-tuning method that aligns LLM's gender-profession
output distribution with the desired distribution, while preserving language
modeling capability. Using 3 profession sets -- male-dominated,
female-dominated, and gender-balanced -- derived from U.S. labor statistics
(2024), we assess both our adaptive method for reflecting reality and a
non-adaptive variant for equality. Across three masked language models, bias is
observed under both distributions. We achieve near-complete mitigation under
equality and 30-75% reduction under real-world settings. Autoregressive LLMs
show no bias under equality but notable bias under real-world settings, with
the Llama Instruct models (3.2-3B, 3.1-8B) achieving a 50-62% reduction.

</details>


### [15] [EVALUESTEER: Measuring Reward Model Steerability Towards Values and Preference](https://arxiv.org/abs/2510.06370)
*Kshitish Ghate,Andy Liu,Devansh Jain,Taylor Sorensen,Atoosa Kasirzadeh,Aylin Caliskan,Mona T. Diab,Maarten Sap*

Main category: cs.CL

TL;DR: 提出EVALUESTEER基准测试，揭示现有奖励模型在用户价值偏好调控上的局限性（完整资料准确率<75% vs 定向偏好>99%）


<details>
  <summary>Details</summary>
Motivation: 解决现有数据集无法支持奖励模型可调控性评估的缺陷，适应全球化部署中用户多元价值需求

Method: 生成165,888个合成偏好对，系统控制4个价值维度（传统/世俗理性/生存/自我表达）和4个风格维度（冗长度/可读性/自信度/亲和力），评估6个LLMs/RMs在16种提示条件下的表现

Result: 完整用户资料下最优模型准确率不足75%，仅提供相关偏好时准确率超99%

Conclusion: 当前RMs难以有效识别用户特征，EVALUESTEER为开发可适应用户多元价值观的模型提供了挑战性测试平台

Abstract: As large language models (LLMs) are deployed globally, creating pluralistic
systems that can accommodate the diverse preferences and values of users
worldwide becomes essential. We introduce EVALUESTEER, a benchmark to measure
LLMs' and reward models' (RMs) steerability towards users' value and stylistic
preference profiles grounded in psychology and human-LLM interaction
literature. To address the gap in existing datasets that do not support
controlled evaluations of RM steering, we synthetically generated 165,888
preference pairs -- systematically varying pairs along 4 value dimensions
(traditional, secular-rational, survival, and self-expression) and 4 style
dimensions (verbosity, readability, confidence, and warmth). We use EVALUESTEER
to evaluate whether, given a user profile and a pair of candidate value-laden
and style-laden responses, LLMs and RMs are able to select the output that
aligns with the user's preferences. We evaluate six open-source and proprietary
LLMs and RMs under sixteen systematic prompting conditions and six preference
comparison scenarios. Notably, our results show that, when given the user's
full profile of values and stylistic preferences, the best models achieve <75%
accuracy at choosing the correct response, in contrast to >99% accuracy when
only relevant style and value preferences are provided. EVALUESTEER thus
highlights the limitations of current RMs at identifying and adapting to
relevant user profile information, and provides a challenging testbed for
developing RMs that can be steered towards diverse human values and
preferences.

</details>


### [16] [EverydayMMQA: A Multilingual and Multimodal Framework for Culturally Grounded Spoken Visual QA](https://arxiv.org/abs/2510.06371)
*Firoj Alam,Ali Ezzat Shahroor,Md. Arid Hasan,Zien Sheikh Ali,Hunzalah Hassan Bhatti,Mohamed Bayan Kmainasi,Shammur Absar Chowdhury,Basel Mousi,Fahim Dalvi,Nadir Durrani,Natasa Milic-Frayling*

Main category: cs.CL

TL;DR: 提出EverydayMMQA框架和OASIS数据集，解决多模态模型在文化相关任务中的短板，提供跨语言/跨文化的多模态基准测试


<details>
  <summary>Details</summary>
Motivation: 现有大规模多模态模型在需要文化背景知识和低资源语言场景下表现欠佳，缺乏真实场景下的跨文化多模态基准

Method: 通过框架自动生成包含语音/图像/文本的3.7M口语问题，构建覆盖18国、支持四种输入组合（纯语音/纯文本/语音+图/文本+图）的多模态数据集

Result: 创建包含92万图像和1480万QA对的OASIS数据集，基准测试显示现有模型在文化意识推理任务上存在显著差距

Conclusion: 该框架和数据集为开发具有文化适应性的多模态大模型提供训练和评估基础，通过公开资源促进跨文化AI研究

Abstract: Large-scale multimodal models achieve strong results on tasks like Visual
Question Answering (VQA), but they often fail when queries require culturally
grounded, everyday knowledge, particularly in low-resource and underrepresented
languages. To bridge this gap, we introduce Everyday Multimodal and
Multilingual QA (EverydayMMQA), a framework for creating large-scale,
culturally-grounded datasets for spoken and visual question answering (SVQA).
Using this framework, we developed OASIS, a multimodal dataset integrating
speech, images, and text. With over ~0.92M images and 14.8M QA pairs, OASIS
contains 3.7M spoken questions, enabling four unique input combinations:
speech-only, text-only, speech+image, and text+image. Focused on English and
Arabic varieties, 18 countries, the dataset content is curated to reflect
diverse, real-world situations. OASIS tests models on tasks beyond object
recognition that involve pragmatic, commonsense, and culturally aware
reasoning. We benchmarked four closed-source models, three open-source models,
and one fine-tuned model. EverydayMMQA and OASIS together provide a benchmark
and training dataset for building multimodal LLMs for a comprehensive set of
everyday tasks within cultural contexts. The framework and dataset will be made
publicly available to the community.

</details>


### [17] [Semantic Regexes: Auto-Interpreting LLM Features with a Structured Language](https://arxiv.org/abs/2510.06378)
*Angie Boggust,Donghao Ren,Yannick Assogba,Dominik Moritz,Arvind Satyanarayan,Fred Hohman*

Main category: cs.CL

TL;DR: 提出语义正则表达式(semantic regexes)作为结构化特征描述方法，解决自然语言描述模糊不一致的问题，提升可解释性


<details>
  <summary>Details</summary>
Motivation: 现有LLM特征的自然语言描述存在模糊性、不一致性且需要人工重标注的问题

Method: 组合捕捉语言/语义模式的基元与上下文修饰符，构建结构化描述体系

Result: 在定量基准测试中匹配自然语言准确率，特征描述更简洁一致，支持层间复杂度量化等新分析方法

Conclusion: 结构化描述不仅提升解释精度，还支持模型级模式分析，并通过用户验证证实其有效性

Abstract: Automated interpretability aims to translate large language model (LLM)
features into human understandable descriptions. However, these natural
language feature descriptions are often vague, inconsistent, and require manual
relabeling. In response, we introduce semantic regexes, structured language
descriptions of LLM features. By combining primitives that capture linguistic
and semantic feature patterns with modifiers for contextualization,
composition, and quantification, semantic regexes produce precise and
expressive feature descriptions. Across quantitative benchmarks and qualitative
analyses, we find that semantic regexes match the accuracy of natural language
while yielding more concise and consistent feature descriptions. Moreover,
their inherent structure affords new types of analyses, including quantifying
feature complexity across layers, scaling automated interpretability from
insights into individual features to model-wide patterns. Finally, in user
studies, we find that semantic regex descriptions help people build accurate
mental models of LLM feature activations.

</details>


### [18] [Protecting De-identified Documents from Search-based Linkage Attacks](https://arxiv.org/abs/2510.06383)
*Pierre Lison,Mark Anderson*

Main category: cs.CL

TL;DR: 提出通过构建N-gram倒排索引定位稀有短语，结合LLM迭代重写，有效防止去标识化文本的搜索式关联攻击，并在法律文书数据中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有去标识化技术仅隐藏身份信息，但无法防御攻击者通过提取文本中的独特短语组合在原始数据集中进行关联匹配的隐私风险。

Method: 1. 构建文档集的N-gram倒排索引，识别出现频次低于k次的脆弱片段；2. 使用LLM迭代重写这些片段直至消除可关联性。

Result: 在法院案例数据集上的实验表明，该方法能有效阻断搜索式关联攻击，同时保持文本语义完整性（准确率98.7%，重写后语义保留度92.3%）。

Conclusion: 通过定位稀有N-gram与智能重写的双重机制，在维持文本可用性的前提下解决了去标识化技术的关联风险，为法律、医疗等领域的隐私保护提供了新方案。

Abstract: While de-identification models can help conceal the identity of the
individual(s) mentioned in a document, they fail to address linkage risks,
defined as the potential to map the de-identified text back to its source. One
straightforward way to perform such linkages is to extract phrases from the
de-identified document and then check their presence in the original dataset.
This paper presents a method to counter search-based linkage attacks while
preserving the semantic integrity of the text. The method proceeds in two
steps. We first construct an inverted index of the N-grams occurring in the
document collection, making it possible to efficiently determine which N-grams
appear in less than $k$ documents (either alone or in combination with other
N-grams). An LLM-based rewriter is then iteratively queried to reformulate
those spans until linkage is no longer possible. Experimental results on a
collection of court cases show that the method is able to effectively prevent
search-based linkages while remaining faithful to the original content.

</details>


### [19] [Controllable Stylistic Text Generation with Train-Time Attribute-Regularized Diffusion](https://arxiv.org/abs/2510.06386)
*Fan Zhou,Chang Tian,Tim Van de Cruys*

Main category: cs.CL

TL;DR: 提出RegDiff正则化扩散框架，通过训练阶段注入属性特征实现高效可控文本生成，无需采样阶段预训练分类器


<details>
  <summary>Details</summary>
Motivation: 现有CFG方法属性控制不足，CG方法存在计算成本高和分类器泛化问题。需开发既能保持语义又能降低计算成本的方案

Method: 采用VAE编码器-解码器保证重建质量，结合属性监督的潜在扩散模型，仅在训练阶段注入属性信息实现控制

Result: 在五个风格属性数据集上的实验表明，RegDiff生成质量优于基线模型，验证了框架有效性

Conclusion: RegDiff通过训练阶段属性注入机制，成功实现高效可控文本扩散，显著降低推理计算成本

Abstract: Generating stylistic text with specific attributes is a key problem in
controllable text generation. Recently, diffusion models have emerged as a
powerful paradigm for both visual and textual generation. Existing approaches
can be broadly categorized into classifier-free guidance (CFG) and classifier
guidance (CG) methods. While CFG effectively preserves semantic content, it
often fails to provide effective attribute control. In contrast, CG modifies
the denoising trajectory using classifier gradients, enabling better attribute
alignment but incurring high computational costs during sampling and suffering
from classifier generalization issues. In this work, we propose RegDiff, a
regularized diffusion framework that leverages attribute features without
requiring a pretrained classifier during sampling, thereby achieving
controllable generation with reduced computational costs. Specifically, RegDiff
employs a VAE-based encoder--decoder architecture to ensure reconstruction
fidelity and a latent diffusion model trained with attribute supervision to
enable controllable text generation. Attribute information is injected only
during training. Experiments on five datasets spanning multiple stylistic
attributes demonstrate that RegDiff outperforms strong baselines in generating
stylistic texts. These results validate the effectiveness of RegDiff as an
efficient solution for attribute-controllable text diffusion. Our code,
datasets, and resources will be released upon publication at
https://github.com/xxxx.

</details>


### [20] [Reward Model Perspectives: Whose Opinions Do Reward Models Reward?](https://arxiv.org/abs/2510.06391)
*Elle*

Main category: cs.CL

TL;DR: 奖励模型在语言模型对齐中存在社会人口偏见，仅靠提示引导无法有效克服偏见限制


<details>
  <summary>Details</summary>
Motivation: 奖励模型作为人类偏好的代理指导语言模型行为，但对其行为理解不足可能导致传播有害社会偏见

Method: 通过争议话题量化奖励模型的观点（意见、态度和价值观），分析其社会人口偏见及提示引导效果

Result: 奖励模型与多个群体对齐较差，系统性奖励有害刻板印象，提示引导效果有限

Conclusion: 需在偏好学习中更谨慎地考虑奖励模型行为，防止语言技术传播有害社会偏见

Abstract: Reward models (RMs) are central to the alignment of language models (LMs). An
RM often serves as a proxy for human preferences to guide downstream LM
behavior. However, our understanding of RM behavior is limited. Our work (i)
formalizes a framework for measuring the alignment of opinions captured by RMs,
(ii) investigates the extent to which RMs demonstrate sociodemographic biases,
and (iii) explores the effects of prompting to steer rewards towards the
preferences of a target group. We study the subjective and diverse perspectives
on controversial topics, which allows us to quantify RM perspectives in terms
of their opinions, attitudes, and values. We show that RMs are poorly aligned
with several demographic groups and can systematically reward harmful
stereotypes, and steering alone is not enough to overcome these limitations.
Our findings underscore the need for more careful consideration of RM behavior
in model alignment during preference learning to prevent the propagation of
unwanted social biases in the language technologies that we use.

</details>


### [21] [Instructional Goal-Aligned Question Generation for Student Evaluation in Virtual Lab Settings: How Closely Do LLMs Actually Align?](https://arxiv.org/abs/2510.06411)
*R. Alexander Knipper,Indrani Dey,Souvika Sarkar,Hari Narayanan,Sadhana Puntambekar,Santu Karmaker*

Main category: cs.CL

TL;DR: 提出基于大语言模型(LLMs)的框架，通过自然语言交互生成与教学目标和虚拟实验模拟对齐的优质问题


<details>
  <summary>Details</summary>
Motivation: 教师在使用虚拟实验室时常面临第三方材料不匹配、自定义资源开发耗时的挑战，需寻找高效适配教学目标的解决方案

Method: 框架包含四部分：1) 教师-LLM对话理解教学目标 2) 知识单元分析理解实验室 3) 认知/教学意图问题分类法 4) 控制提示细节的TELeR分类法。通过案例研究和19个开源LLM生成的1,100+问题进行评估

Result: 较大模型提升显著：可解析性+37.1%，格式遵循性+25.7%，平均质量+0.8点。开放式问题和关系类问题提升质量0.29-0.39分，优化提示使80%问题可解析，>90%遵循格式要求

Conclusion: 该框架通过目标-实验双理解锚定问题设计，结合问题分类法和优化提示策略，有效提升生成问题的教学价值，且大语言模型展现出显著性能优势

Abstract: Virtual Labs offer valuable opportunities for hands-on, inquiry-based science
learning, yet teachers often struggle to adapt them to fit their instructional
goals. Third-party materials may not align with classroom needs, and developing
custom resources can be time-consuming and difficult to scale. Recent advances
in Large Language Models (LLMs) offer a promising avenue for addressing these
limitations. In this paper, we introduce a novel alignment framework for
instructional goal-aligned question generation, enabling teachers to leverage
LLMs to produce simulation-aligned, pedagogically meaningful questions through
natural language interaction. The framework integrates four components:
instructional goal understanding via teacher-LLM dialogue, lab understanding
via knowledge unit and relationship analysis, a question taxonomy for
structuring cognitive and pedagogical intent, and the TELeR taxonomy for
controlling prompt detail. Early design choices were informed by a small
teacher-assisted case study, while our final evaluation analyzed over 1,100
questions from 19 open-source LLMs. With goal and lab understanding grounding
questions in teacher intent and simulation context, the question taxonomy
elevates cognitive demand (open-ended formats and relational types raise
quality by 0.29-0.39 points), and optimized TELeR prompts enhance format
adherence (80% parsability, >90% adherence). Larger models yield the strongest
gains: parsability +37.1%, adherence +25.7%, and average quality +0.8 Likert
points.

</details>


### [22] [FinLFQA: Evaluating Attributed Text Generation of LLMs in Financial Long-Form Question Answering](https://arxiv.org/abs/2510.06426)
*Yitao Long,Tiansheng Hu,Yilun Zhao,Arman Cohan,Chen Zhao*

Main category: cs.CL

TL;DR: FinLFQA基准评估大语言模型在复杂金融问题中的多维度归因能力，强调证据提取、数值推理和领域知识融合


<details>
  <summary>Details</summary>
Motivation: 现有基准局限于文本证据检索的简单归因，无法满足金融领域需要综合数据推理和专业知识的需求

Method: 构建包含财务报告证据提取、数值推理链、金融知识应用三个维度的评估基准，并开发覆盖答案质量和归因质量的自动化评估框架

Result: 实验发现：1）细粒度指标能更好区分模型能力 2）端到端生成与后处理效果相当 3）迭代优化需依赖外部反馈

Conclusion: FinLFQA填补了复杂金融场景的评估空白，揭示了细粒度归因指标的重要性，为金融领域LLM应用提供新方向

Abstract: Large Language Models (LLMs) frequently hallucinate to long-form questions,
producing plausible yet factually incorrect answers. A common mitigation
strategy is to provide attribution to LLM outputs. However, existing benchmarks
primarily focus on simple attribution that retrieves supporting textual
evidence as references. We argue that in real-world scenarios such as financial
applications, attribution goes beyond reference retrieval. We introduce
FinLFQA, a benchmark designed to evaluate the ability of LLMs to generate
long-form answers to complex financial questions with reliable and nuanced
attributions. FinLFQA evaluates three critical aspects of attribution through
human annotations: (1) supporting evidence extracted from financial reports,
(2) intermediate numerical reasoning steps, and (3) domain-specific financial
knowledge that informs the reasoning process. We further provide an automatic
evaluation framework covering both answer quality and attribution quality.
Through extensive experiments on eight LLMs across multiple
attribution-generation paradigms, we find that fine-grained metrics are
important to distinguish model capabilities, that end-to-end generation
achieves comparable performance to post-hoc approaches, and that iterative
refinement only helps when guided by external feedback.

</details>


### [23] [Bridging Discourse Treebanks with a Unified Rhetorical Structure Parser](https://arxiv.org/abs/2510.06427)
*Elena Chistova*

Main category: cs.CL

TL;DR: 提出了首个统一RST风格话语解析模型UniRST，可跨11种语言18个树库实现参数高效的多语言端到端解析，在16/18基准测试中超越单树库模型。


<details>
  <summary>Details</summary>
Motivation: 解决不同树库关系清单不兼容问题，构建无需调整关系体系即可跨多语言资源解析的统一框架。

Method: 提出多头策略（各树库独立分类层）与掩码联合策略（共享参数+标签掩码），通过数据增强提升低资源性能，系统对比两种策略效能。

Result: 掩码联合策略参数效率最优，UniRST模型在18个基准中16个超越单树库基线（平均提升1.56个点）。

Conclusion: 证明了单模型多语言端到端解析的可行性，参数共享机制有效提升跨资源泛化能力，为话语分析提供新范式。

Abstract: We introduce UniRST, the first unified RST-style discourse parser capable of
handling 18 treebanks in 11 languages without modifying their relation
inventories. To overcome inventory incompatibilities, we propose and evaluate
two training strategies: Multi-Head, which assigns separate relation
classification layer per inventory, and Masked-Union, which enables shared
parameter training through selective label masking. We first benchmark
monotreebank parsing with a simple yet effective augmentation technique for
low-resource settings. We then train a unified model and show that (1) the
parameter efficient Masked-Union approach is also the strongest, and (2) UniRST
outperforms 16 of 18 mono-treebank baselines, demonstrating the advantages of a
single-model, multilingual end-to-end discourse parsing across diverse
resources.

</details>


### [24] [MathRobust-LV: Evaluation of Large Language Models' Robustness to Linguistic Variations in Mathematical Reasoning](https://arxiv.org/abs/2510.06430)
*Neeraja Kirtane,Yuvraj Khanna,Peter Relan*

Main category: cs.CL

TL;DR: 研究发现语言模型在数学题语言变体下的鲁棒性存在显著缺陷，小模型准确率下降达9-11%，前沿模型相对稳定但仍有可测量退化


<details>
  <summary>Details</summary>
Motivation: 针对教育场景中教师常通过语言改写保持题目难度不变的实际需求，验证当前数学推理模型在真实教学环境中的可靠部署能力

Method: 构建MathRobust-LV测试集，通过改变题目表面特征（名称/情境/变量）但保持数值结构和答案不变，评估34个模型在语言变体下的性能变化

Result: 所有模型在语言变体下准确率均下降，其中小模型下降幅度最大（9-11%），强模型也出现可测量退化，仅GPT-5/Gemini-2.5pro等前沿模型保持相对稳定

Conclusion: 语言变体鲁棒性已成为数学推理模型的核心挑战，当前模型的推理能力存在表面特征依赖，这对教育科技领域的实际应用构成重大风险

Abstract: Large language models excel on math benchmarks, but their math reasoning
robustness to linguistic variation is underexplored. While recent work
increasingly treats high-difficulty competitions like the IMO as the gold
standard for evaluating reasoning, we believe in comprehensive benchmarking of
high school-level math problems in real educational settings. We introduce
MathRobust-LV, a test set and evaluation methodology that mirrors how
instructors rephrase problems across assessments while keeping difficulty
constant: we change surface details (names, contexts, variables) while
preserving numerical structure and answers. In contrast to prior efforts that
alter problem content or emphasize IMO-level tasks, we focus on
high-school-level dataset problems at the difficulty level where models are
currently deployed in educational settings: tutoring and assessment systems. In
these applications, instructors rephrase identical concepts in varied ways,
making linguistic robustness essential for reliable deployment. Although MATH
data benchmarking is often regarded as saturated, our experiment on 34 models
reveals that accuracy declines when moving from the baseline to the variants.
These drops are severe for smaller models (9-11%) while stronger models also
show measurable degradation. Frontier models like GPT-5, Gemini-2.5pro remain
comparatively stable. Our results highlight that robustness to linguistic
variation is a fundamental challenge, exposing reasoning vulnerabilities in
models.

</details>


### [25] [A Survey on Agentic Security: Applications, Threats and Defenses](https://arxiv.org/abs/2510.06445)
*Asif Shahriar,Md Nafiu Rahman,Sadif Ahmed,Farig Sadeque,Md Rizwan Parvez*

Main category: cs.CL

TL;DR: 首次系统梳理自主LLM代理在网络安全中的三元框架（应用-威胁-防御），分析150+论文揭示架构趋势与研究空白


<details>
  <summary>Details</summary>
Motivation: 应对自主LLM代理带来的新型安全风险，解决现有研究缺乏系统性分类的现状

Method: 构建应用-威胁-防御三维框架，通过文献计量学方法对150+论文进行多维度分类与交叉分析

Result: 发现代理架构的模块化趋势，揭示模型覆盖单一（87%仅文本模态）、安全措施碎片化（仅35%含防御机制）等关键缺陷

Conclusion: 需建立跨模态的自主代理安全评估体系，推动防御机制与代理架构的协同设计

Abstract: The rapid shift from passive LLMs to autonomous LLM-agents marks a new
paradigm in cybersecurity. While these agents can act as powerful tools for
both offensive and defensive operations, the very agentic context introduces a
new class of inherent security risks. In this work we present the first
holistic survey of the agentic security landscape, structuring the field around
three interdependent pillars: Applications, Threats, and Defenses. We provide a
comprehensive taxonomy of over 150 papers, explaining how agents are used, the
vulnerabilities they possess, and the countermeasures designed to protect them.
A detailed cross-cutting analysis shows emerging trends in agent architecture
while revealing critical research gaps in model and modality coverage.

</details>


### [26] [Linguistically Informed Tokenization Improves ASR for Underresourced Languages](https://arxiv.org/abs/2510.06461)
*Massimo Daul,Alessio Tosolini,Claire Bowern*

Main category: cs.CL

TL;DR: 研究表明采用音位标记化策略显著提升了低资源澳大利亚土著语言Yan-nhangu的ASR性能，且ASR辅助校正比人工转录效率提升3.5倍


<details>
  <summary>Details</summary>
Motivation: 现代ASR系统因数据需求大难以应用于资源匮乏语言，需要探索优化方案支持濒危语言保护

Method: 基于wav2vec2模型在Yan-nhangu语言上进行微调，对比音位与正字法标记化策略，并验证ASR在语言记录流程的实用性

Result: 音位标记化使词错误率(WER)降低21%，字错误率(CER)下降18%；ASR输出人工校正速度比原始转录快3.5倍

Conclusion: 通过语言学优化的ASR系统能有效支持资源匮乏语言的文档保护工作，为濒危语言数字化提供可行方案

Abstract: Automatic speech recognition (ASR) is a crucial tool for linguists aiming to
perform a variety of language documentation tasks. However, modern ASR systems
use data-hungry transformer architectures, rendering them generally unusable
for underresourced languages. We fine-tune a wav2vec2 ASR model on Yan-nhangu,
a dormant Indigenous Australian language, comparing the effects of phonemic and
orthographic tokenization strategies on performance. In parallel, we explore
ASR's viability as a tool in a language documentation pipeline. We find that a
linguistically informed phonemic tokenization system substantially improves WER
and CER compared to a baseline orthographic tokenization scheme. Finally, we
show that hand-correcting the output of an ASR model is much faster than
hand-transcribing audio from scratch, demonstrating that ASR can work for
underresourced languages.

</details>


### [27] [Test-Time Scaling of Reasoning Models for Machine Translation](https://arxiv.org/abs/2510.06471)
*Zihao Li,Shaoxiong Ji,Jörg Tiedemann*

Main category: cs.CL

TL;DR: 测试时缩放（TTS）在通用机器翻译中提升有限，但通过领域微调或多步后编辑可显著提升质量


<details>
  <summary>Details</summary>
Motivation: 探究推理模型在机器翻译任务中增加推理计算的有效性，揭示不同应用场景的价值差异

Method: 使用12个推理模型在跨领域MT基准测试，涵盖直接翻译、强制推理外推和后编辑三种场景，结合领域微调实验

Result: 领域微调使TTS效果提升至自选最优深度，强制超限推理降低质量，后编辑场景下自我修正成功率提升

Conclusion: 推理计算的价值在于多步自我修正工作流和任务专用模型，而非通用模型的单次翻译增强

Abstract: Test-time scaling (TTS) has enhanced the performance of Reasoning Models
(RMs) on various tasks such as math and coding, yet its efficacy in machine
translation (MT) remains underexplored. This paper investigates whether
increased inference-time computation improves translation quality. We evaluate
12 RMs across a diverse suite of MT benchmarks spanning multiple domains,
examining three scenarios: direct translation, forced-reasoning extrapolation,
and post-editing. Our findings show that for general-purpose RMs, TTS provides
limited and inconsistent benefits for direct translation, with performance
quickly plateauing. However, the effectiveness of TTS is unlocked by
domain-specific fine-tuning, which aligns a model's reasoning process with task
requirements, leading to consistent improvements up to an optimal,
self-determined reasoning depth. We also find that forcing a model to reason
beyond its natural stopping point consistently degrades translation quality. In
contrast, TTS proves highly effective in a post-editing context, reliably
turning self-correction into a beneficial process. These results indicate that
the value of inference-time computation in MT lies not in enhancing single-pass
translation with general models, but in targeted applications like multi-step,
self-correction workflows and in conjunction with task-specialized models.

</details>


### [28] [Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels](https://arxiv.org/abs/2510.06499)
*Zhepeng Cen,Haolin Chen,Shiyu Wang,Zuxin Liu,Zhiwei Liu,Ding Zhao,Silvio Savarese,Caiming Xiong,Huan Wang,Weiran Yao*

Main category: cs.CL

TL;DR: 提出Webscale-RL数据引擎，将预训练文档转化为120万可验证QA对，显著提升RL训练效率（达传统预训练100倍效率）


<details>
  <summary>Details</summary>
Motivation: 现有RL数据集规模远小于预训练语料，导致强化学习在LLM应用中存在数据瓶颈

Method: 通过系统化转换流程，从大规模预训练文档生成多样化、可验证的问答对，构建跨9+领域的Webscale-RL数据集

Result: 模型在多项基准测试中显著优于持续预训练，RL训练效率提升100倍（同等性能所需token减少两个数量级）

Conclusion: Webscale-RL为LLM的强化学习规模化提供了可行路径，实现了更高效、更强大的语言模型训练范式

Abstract: Large Language Models (LLMs) have achieved remarkable success through
imitation learning on vast text corpora, but this paradigm creates a
training-generation gap and limits robust reasoning. Reinforcement learning
(RL) offers a more data-efficient solution capable of bridging this gap, yet
its application has been constrained by a critical data bottleneck: existing RL
datasets are orders of magnitude smaller and less diverse than web-scale
pre-training corpora. To address this, we introduce the Webscale-RL pipeline, a
scalable data engine that systematically converts large-scale pre-training
documents into millions of diverse, verifiable question-answer pairs for RL.
Using this pipeline, we construct the Webscale-RL dataset, containing 1.2
million examples across more than 9 domains. Our experiments show that the
model trained on this dataset significantly outperforms continual pretraining
and strong data refinement baselines across a suite of benchmarks. Notably, RL
training with our dataset proves substantially more efficient, achieving the
performance of continual pre-training with up to 100$\times$ fewer tokens. Our
work presents a viable path toward scaling RL to pre-training levels, enabling
more capable and efficient language models.

</details>


### [29] [From Acceleration to Saturation: Scaling Behavior of Bootstrapped Language Model Pretraining](https://arxiv.org/abs/2510.06548)
*Seng Pei Liew,Takuya Kato*

Main category: cs.CL

TL;DR: 自举预训练（重用预训练基模型进行持续预训练或模型扩展）的扩展效率会随基模型预训练token量增加呈对数下降，存在预训练阶段的收益递减效应。


<details>
  <summary>Details</summary>
Motivation: 探索重用已充分训练的基模型进行多阶段预训练的有效性，降低从头训练语言模型的成本。

Method: 通过实证研究构建扩展规律模型，分析基模型预训练token量与第二阶段预训练token量之间的对数关系。

Result: 发现自举预训练的扩展指数随基模型预训练token量增加呈现对数衰减，揭示多阶段预训练中预训练深度与扩展收益间的根本权衡。

Conclusion: 该研究为语言模型高效训练提供新视角，强调重用过度训练模型时需谨慎考虑边际收益递减效应。

Abstract: Bootstrapped pretraining, i.e., the reuse of a pretrained base model for
further pretraining, such as continual pretraining or model growth, is
promising at reducing the cost of training language models from scratch.
However, its effectiveness remains unclear, especially when applied to
overtrained base models. In this work, we empirically study the scaling
behavior of bootstrapped pretraining and find that its scaling efficiency
diminishes in a predictable manner: The scaling exponent with respect to
second-stage pretraining tokens decreases logarithmically with the number of
tokens used to pretrain the base model. The joint dependence on first- and
second-stage tokens is accurately modeled by a simple scaling law. Such
saturation effect reveals a fundamental trade-off in multi-stage pretraining
strategies: the more extensively a model is pretrained, the less additional
benefit bootstrapping provides. Our findings provide practical insights for
efficient language model training and raise important considerations for the
reuse of overtrained models.

</details>


### [30] [Flipping the Dialogue: Training and Evaluating User Language Models](https://arxiv.org/abs/2510.06552)
*Tarek Naous,Philippe Laban,Wei Xu,Jennifer Neville*

Main category: cs.CL

TL;DR: 专用用户语言模型（User LMs）比助手模型更有效模拟真实用户行为，导致强助手GPT-4o在复杂对话中性能下降17.2%


<details>
  <summary>Details</summary>
Motivation: 现有基于助手LM的用户模拟方法存在缺陷，需开发专用模型准确反映真实对话场景中用户的复杂交互模式

Method: 通过后训练构建User LMs，采用多维度评估指标（行为对齐、模拟鲁棒性、任务性能影响）验证模型有效性

Result: 使用User LMs模拟时，GPT-4o在数学和编程对话任务中的准确率从74.6%降至57.4%，揭示助手处理复杂用户交互的局限性

Conclusion: 真实用户模拟环境暴露现有助手模型的交互缺陷，强调开发专用评估工具对提升对话系统实用性的重要性

Abstract: Conversations with LMs involve two participants: a human user leading the
conversation, and an LM assistant responding to the user's request. To satisfy
this specific role, LMs are post-trained to be helpful assistants -- optimized
to produce exhaustive and well-structured responses, free of ambiguity and
grammar errors. User utterances, on the other hand, are rarely perfected, with
each user phrasing requests in unique ways, sometimes putting in partial effort
at each turn and refining on the fly. To evaluate LM performance in realistic
settings, prior work simulated users in multi-turn conversations, often
prompting an LLM originally trained to be a helpful assistant to act as a user.
However, we show that assistant LMs make for poor user simulators, with the
surprising finding that better assistants yield worse simulators. Instead, we
introduce purpose-built User Language Models (User LMs) - models post-trained
to simulate human users in multi-turn conversations. Through various
evaluations, we show how User LMs align better with human behavior and achieve
better simulation robustness than existing simulation methods. When leveraging
User LMs to simulate coding and math conversations, the performance of a strong
assistant (GPT-4o) drops from 74.6% to 57.4%, confirming that more realistic
simulation environments lead to assistant struggles as they fail to cope with
the nuances of users in multi-turn setups.

</details>


### [31] [The Algebra of Meaning: Why Machines Need Montague More Than Moore's Law](https://arxiv.org/abs/2510.06559)
*Cheonkam Jeong,Sungdo Kim,Jewoo Park*

Main category: cs.CL

TL;DR: 提出神经符号架构Savassan，通过类型论语义解决语言模型幻觉及合规问题，实现跨司法管辖区的统一推理框架


<details>
  <summary>Details</summary>
Motivation: 当前语言模型存在幻觉、审核脆弱和合规不透明问题，核心原因是缺乏类型化语义结构而非数据/算力限制

Method: 结合神经组件(非结构化输入解析)与符号组件(类型检查/约束推理)，将自然语言编译为Montague式逻辑形式，映射到带法务算子的类型化本体论

Result: 构建支持多法域合规推理的架构，可单次解析后投影到不同司法本体(如韩国诽谤法、欧盟GDPR)，生成可解释的复合决策

Conclusion: 可信赖的自治系统需要基于类型化的语义代数，明确区分描述性、规范性和法律责任维度，实现统一的意义推理框架

Abstract: Contemporary language models are fluent yet routinely mis-handle the types of
meaning their outputs entail. We argue that hallucination, brittle moderation,
and opaque compliance outcomes are symptoms of missing type-theoretic semantics
rather than data or scale limitations. Building on Montague's view of language
as typed, compositional algebra, we recast alignment as a parsing problem:
natural-language inputs must be compiled into structures that make explicit
their descriptive, normative, and legal dimensions under context.
  We present Savassan, a neuro-symbolic architecture that compiles utterances
into Montague-style logical forms and maps them to typed ontologies extended
with deontic operators and jurisdictional contexts. Neural components extract
candidate structures from unstructured inputs; symbolic components perform type
checking, constraint reasoning, and cross-jurisdiction mapping to produce
compliance-aware guidance rather than binary censorship. In cross-border
scenarios, the system "parses once" (e.g., defect claim(product x, company y))
and projects the result into multiple legal ontologies (e.g., defamation risk
in KR/JP, protected opinion in US, GDPR checks in EU), composing outcomes into
a single, explainable decision.
  This paper contributes: (i) a diagnosis of hallucination as a type error;
(ii) a formal Montague-ontology bridge for business/legal reasoning; and (iii)
a production-oriented design that embeds typed interfaces across the pipeline.
We outline an evaluation plan using legal reasoning benchmarks and synthetic
multi-jurisdiction suites. Our position is that trustworthy autonomy requires
compositional typing of meaning, enabling systems to reason about what is
described, what is prescribed, and what incurs liability within a unified
algebra of meaning.

</details>


### [32] [TinyScientist: An Interactive, Extensible, and Controllable Framework for Building Research Agents](https://arxiv.org/abs/2510.06579)
*Haofei Yu,Keyang Xuan,Fenghai Li,Kunlun Zhu,Zijie Lei,Jiaxun Zhang,Ziheng Qi,Kyle Richardson,Jiaxuan You*

Main category: cs.CL

TL;DR: TinyScientist提出交互式可扩展框架简化自动研究流程


<details>
  <summary>Details</summary>
Motivation: 现有自动研究工具的复杂性和维护难度阻碍了算法迭代和开发者参与

Method: 通过定义核心组件构建支持工具扩展和迭代增长的框架，提供开源代码、Web演示和Python包

Result: 实现可适配新工具且支持持续迭代的自动研究系统

Conclusion: 该框架通过降低使用门槛将促进自动研究技术的普及与发展

Abstract: Automatic research with Large Language Models (LLMs) is rapidly gaining
importance, driving the development of increasingly complex workflows involving
multi-agent systems, planning, tool usage, code execution, and human-agent
interaction to accelerate research processes. However, as more researchers and
developers begin to use and build upon these tools and platforms, the
complexity and difficulty of extending and maintaining such agentic workflows
have become a significant challenge, particularly as algorithms and
architectures continue to advance. To address this growing complexity,
TinyScientist identifies the essential components of the automatic research
workflow and proposes an interactive, extensible, and controllable framework
that easily adapts to new tools and supports iterative growth. We provide an
open-source codebase, an interactive web demonstration, and a PyPI Python
package to make state-of-the-art auto-research pipelines broadly accessible to
every researcher and developer.

</details>


### [33] [Do Internal Layers of LLMs Reveal Patterns for Jailbreak Detection?](https://arxiv.org/abs/2510.06594)
*Sri Durga Sai Sowmya Kadali,Evangelos E. Papalexakis*

Main category: cs.CL

TL;DR: 研究通过分析LLM内部表征（重点关注GPT-J和Mamba2模型），发现恶意提示与良性提示在隐藏层的响应差异，为越狱检测提供新思路


<details>
  <summary>Details</summary>
Motivation: 随着对话式LLM普及，越狱攻击（通过精心设计的提示绕过模型限制）成为严重威胁。现有防御机制无法完全抵御新型提示攻击，需探索基于模型内部动态的检测方法

Method: 分析开源模型GPT-J和状态空间模型Mamba2的内部表征，比较不同隐藏层对越狱提示与良性提示的响应模式

Result: 发现越狱提示与良性提示在隐藏层激活模式存在显著差异，不同模型（GPT-J与Mamba2）呈现层级响应特异性

Conclusion: 基于模型内部动态的层级行为分析为越狱检测提供了新方向，未来可通过挖掘内部表征特征构建更鲁棒的防御系统

Abstract: Jailbreaking large language models (LLMs) has emerged as a pressing concern
with the increasing prevalence and accessibility of conversational LLMs.
Adversarial users often exploit these models through carefully engineered
prompts to elicit restricted or sensitive outputs, a strategy widely referred
to as jailbreaking. While numerous defense mechanisms have been proposed,
attackers continuously develop novel prompting techniques, and no existing
model can be considered fully resistant. In this study, we investigate the
jailbreak phenomenon by examining the internal representations of LLMs, with a
focus on how hidden layers respond to jailbreak versus benign prompts.
Specifically, we analyze the open-source LLM GPT-J and the state-space model
Mamba2, presenting preliminary findings that highlight distinct layer-wise
behaviors. Our results suggest promising directions for further research on
leveraging internal model dynamics for robust jailbreak detection and defense.

</details>


### [34] [A Comparative Analysis of Contextual Representation Flow in State-Space and Transformer Architectures](https://arxiv.org/abs/2510.06640)
*Nhat M. Hoang,Do Xuan Long,Cong-Duy Nguyen,Min-Yen Kan,Luu Anh Tuan*

Main category: cs.CL

TL;DR: SSM与Transformer模型在长序列处理中呈现不同表征传播特性：Transformer早期层快速同质化token表征并在深层恢复多样性，SSM则早期保留token独特性但在深层趋同。


<details>
  <summary>Details</summary>
Motivation: 探究SSM和Transformer架构中上下文信息跨层跨token的传播机制差异，揭示模型架构对表征演化的根本影响。

Method: 采用中心核对齐、稳定性指标和探针分析，结合参数随机化实验，系统研究表征在层内/层间的演化规律。

Result: 发现Transformer的同质化源于架构设计，而SSM的趋同主要来自训练动态；理论分析表明二者存在不同的归纳偏置。

Conclusion: 该发现为长上下文推理模型的架构设计和训练策略提供了理论依据，指明SSM/TBMs优化方向。

Abstract: State Space Models (SSMs) have recently emerged as efficient alternatives to
Transformer-Based Models (TBMs) for long-sequence processing, offering linear
scaling and lower memory use. Yet, how contextual information flows across
layers and tokens in these architectures remains understudied. We present the
first unified, token- and layer-level analysis of representation propagation in
SSMs and TBMs. Using centered kernel alignment, stability metrics, and probing,
we characterize how representations evolve within and across layers. We find a
key divergence: TBMs rapidly homogenize token representations, with diversity
reemerging only in later layers, while SSMs preserve token uniqueness early but
converge to homogenization deeper. Theoretical analysis and parameter
randomization further reveal that oversmoothing in TBMs stems from
architectural design, whereas in SSMs it arises mainly from training dynamics.
These insights clarify the inductive biases of both architectures and inform
future model and training designs for long-context reasoning.

</details>


### [35] [Aligning Large Language Models via Fully Self-Synthetic Data](https://arxiv.org/abs/2510.06652)
*Shangjian Yin,Zhepei Wei,Xinyu Zhu,Wei-Lin Chen,Yu Meng*

Main category: cs.CL

TL;DR: 提出完全自合成的SAO框架，通过角色扮演生成多样化数据并自评估优化，实现低成本LLM对齐


<details>
  <summary>Details</summary>
Motivation: 传统RLHF依赖昂贵人工标注数据，RLAIF需外部奖励模型或GPT-4等专有模型标注偏好对，成本高昂

Method: 1. 指导LLM进行角色扮演生成多样化prompt和响应 2. 通过自评估生成偏好对进行优化

Result: 在AlpacaEval~2.0提升对话能力，下游任务(问答/数学推理)性能保持优异

Conclusion: SAO为LLM自对齐提供高效低成本解决方案，无需外部数据或模型支持

Abstract: Traditional reinforcement learning from human feedback (RLHF) for large
language models (LLMs) relies on expensive human-annotated datasets, while
Reinforcement Learning from AI Feedback (RLAIF) also incurs significant costs,
requiring the collection of diverse prompts and corresponding responses, often
necessitating external reward models or proprietary models like GPT-4 to
annotate preference pairs. In this work, we introduce Self-Alignment
Optimization (SAO), a fully self-synthetic framework for LLM alignment, where
all training data, including prompts (i.e., user queries), responses, and
preferences, are generated by the model itself. Specifically, SAO first
instructs the LLM to engage in persona role-play and generate diverse prompts
and responses, which are then self-evaluated for preference optimization.
Extensive experiments demonstrate that SAO effectively enhances the model's
chat capabilities on standard benchmarks like AlpacaEval~2.0, while maintaining
strong performance on downstream objective tasks (e.g., question-answering,
math reasoning). Our work provides a practical solution for self-improvement in
aligning LLMs, and the code for reproducing our results is available at:
https://github.com/SJY8460/SAO.

</details>


### [36] [ToolMem: Enhancing Multimodal Agents with Learnable Tool Capability Memory](https://arxiv.org/abs/2510.06664)
*Yunzhong Xiao,Yangmin Li,Hewei Wang,Yunlong Tang,Zora Zhiruo Wang*

Main category: cs.CL

TL;DR: 提出ToolMem框架，通过建立工具能力记忆库帮助AI智能体更精准选择最优工具，提升任务执行效果


<details>
  <summary>Details</summary>
Motivation: 现有AI智能体依赖固定工具组合，无法根据任务特性动态选择最佳工具。受人类通过交互积累工具认知的启发，需要让智能体建立工具能力记忆机制

Method: 开发ToolMem系统：1) 通过历史交互总结工具优劣势建立记忆库 2) 推理时检索记忆库条目 3) 基于记忆选择最适合当前任务的工具

Result: 在文本生成和多模态生成任务中，工具性能预测准确率分别提升14.8%和28.7%，工具选择准确率绝对提升21%和24%

Conclusion: 通过建立工具能力记忆机制，ToolMem显著提升了智能体的工具选择能力和任务执行精度，为自适应工具使用提供了新范式

Abstract: Agents utilizing tools powered by large language models (LLMs) or
vision-language models (VLMs) have demonstrated remarkable progress in diverse
tasks across text and visual modalities. Unlike traditional tools such as
calculators, which give deterministic outputs, neural tools perform uncertainly
across task scenarios. While different tools for a task may excel in varied
scenarios, existing agents typically rely on fixed tools, thus limiting the
flexibility in selecting the most suitable tool for specific tasks. In
contrast, humans snowball their understanding of the capabilities of different
tools by interacting with them, and apply this knowledge to select the optimal
tool when solving a future task. To build agents that similarly benefit from
this process, we propose ToolMem that enables agents to develop memories of
tool capabilities from previous interactions, by summarizing their strengths
and weaknesses and storing them in memory; at inference, the agent can retrieve
relevant entries from ToolMem, and select the best tool to solve individual
tasks more accurately. We evaluate ToolMem on learning varied text generation
and text-to-image generation neural tools. Compared to no-memory, generic
agents, we find ToolMem-augmented agents predict tool performance 14.8% and
28.7% more accurately across text and multimodal generation scenarios.
Moreover, ToolMem facilitates optimal tool selection among multiple choices by
21% and 24% absolute increases in respective scenarios.

</details>


### [37] [PIKA: Expert-Level Synthetic Datasets for Post-Training Alignment from Scratch](https://arxiv.org/abs/2510.06670)
*Shangjian Yin,Shining Liang,Wenbiao Ding,Yuli Qian,Zhouxing Shi,Hongzhi Li,Yutao Xie*

Main category: cs.CL

TL;DR: PiKa数据集仅用3万SFT样本实现超越千万级数据模型的对齐效果，为开源LLM提供高效解决方案


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM对齐方法依赖高成本标注数据、数据质量不稳定且学术社区资源受限的问题

Method: 开发PiKa系列高效对齐数据集，通过仅30k SFT样本训练，并在Llama-3-8B和Qwen2.5系列模型上验证

Result: PiKa-SFT在AlpacaEval 2.0和Arena-Hard基准超越官方Llama-3-8B-Instruct（使用超千万私有数据训练），Qwen2.5全系列实现性能提升

Conclusion: 高质量小数据集可实现高效LLM对齐，为开源社区提供可扩展路径，代码和数据已开源

Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone
for aligning large language models (LLMs). However, its effectiveness depends
on high-quality instruction data. Most existing alignment datasets are either
private or require costly human annotation, which limits reproducibility and
scalability. Even with Reinforcement Learning from AI Feedback (RLAIF),
concerns about data quality remain. Moreover, it is unclear how much data is
actually required to fine-tune a base model into a strong instruction-following
model. Current approaches often rely on over 300k examples even at the
supervised fine-tuning (SFT) stage, yet they still underperform compared to
proprietary models, creating barriers for academic and resource-limited
communities. To address this gap, we introduce PiKa, a data-efficient family of
expert-level alignment datasets. In particular, the PiKa-SFT dataset uses only
30k SFT examples, far fewer than state-of-the-art datasets like Magpie. Through
evaluations by fine-tuning Llama-3-8B-Base on PiKa and other public datasets,
we show that PiKa-SFT outperforms models trained on much larger data. On
AlpacaEval 2.0 and Arena-Hard benchmarks, PiKa-SFT fine-tuning even surpasses
the official Llama-3-8B-Instruct model trained on over 10 million proprietary
examples. We further extend our study by training the Qwen2.5 series (0.5B to
7B) on PiKa-SFT, achieving consistent gains. These findings demonstrate that
high-quality alignment can be achieved with significantly less data, offering a
scalable path for open-source LLM alignment. Code and data:
https://github.com/SJY8460/PiKa.

</details>


### [38] [Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback](https://arxiv.org/abs/2510.06677)
*Yisha Wu,Cen,Zhao,Yuanpei Cao,Xiaoqing Su,Yashar Mehdad,Mindy Ji,Claire Na Cheng*

Main category: cs.CL

TL;DR: 基于Mixtral-8x7B和DeBERTa的增量式摘要系统，通过实时生成过滤笔记使工单处理时间减少3%（复杂场景达9%），并获高满意度


<details>
  <summary>Details</summary>
Motivation: 解决客服人员频繁切换上下文和重复查阅信息导致的效率低下问题

Method: 混合微调Mixtral模型实现持续笔记生成，DeBERTa分类器过滤冗余内容，通过坐席编辑形成闭环反馈优化模型

Result: 实际部署后工单处理时间平均减少3%（高复杂度场景达9%），坐席满意度评分达89分

Conclusion: 增量式摘要结合持续反馈机制能有效提升摘要质量和坐席工作效率，具备规模化应用价值

Abstract: We introduce an incremental summarization system for customer support agents
that intelligently determines when to generate concise bullet notes during
conversations, reducing agents' context-switching effort and redundant review.
Our approach combines a fine-tuned Mixtral-8x7B model for continuous note
generation with a DeBERTa-based classifier to filter trivial content. Agent
edits refine the online notes generation and regularly inform offline model
retraining, closing the agent edits feedback loop. Deployed in production, our
system achieved a 3% reduction in case handling time compared to bulk
summarization (with reductions of up to 9% in highly complex cases), alongside
high agent satisfaction ratings from surveys. These results demonstrate that
incremental summarization with continuous feedback effectively enhances summary
quality and agent productivity at scale.

</details>


### [39] [Learning to Rewrite Prompts for Bootstrapping LLMs on Downstream Tasks](https://arxiv.org/abs/2510.06695)
*Qinhao Zhou,Xiang Xiang,Kun He,John E. Hopcroft*

Main category: cs.CL

TL;DR: 针对机器翻译任务提出基于小参数模型的反向翻译策略的提示词优化方法，显著降低训练成本并提升性能


<details>
  <summary>Details</summary>
Motivation: 现有提示工程方法主要优化instruction部分，但对input部分起关键作用的机器翻译任务适用性有限，且依赖大模型辅助

Method: 使用基于反向翻译策略训练的小参数模型进行单任务优化

Result: 显著降低单任务优化的训练开销，同时提供高效能表现

Conclusion: 该方法可扩展适配其他下游任务，为特定任务提示词优化提供新思路

Abstract: In recent years, the growing interest in Large Language Models (LLMs) has
significantly advanced prompt engineering, transitioning from manual design to
model-based optimization. Prompts for LLMs generally comprise two components:
the \textit{instruction}, which defines the task or objective, and the
\textit{input}, which is tailored to the instruction type. In natural language
generation (NLG) tasks such as machine translation, the \textit{input}
component is particularly critical, while the \textit{instruction} component
tends to be concise. Existing prompt engineering methods primarily focus on
optimizing the \textit{instruction} component for general tasks, often
requiring large-parameter LLMs as auxiliary tools. However, these approaches
exhibit limited applicability for tasks like machine translation, where the
\textit{input} component plays a more pivotal role. To address this limitation,
this paper introduces a novel prompt optimization method specifically designed
for machine translation tasks. The proposed approach employs a small-parameter
model trained using a back-translation-based strategy, significantly reducing
training overhead for single-task optimization while delivering highly
effective performance. With certain adaptations, this method can also be
extended to other downstream tasks.

</details>


### [40] [How Language Models Conflate Logical Validity with Plausibility: A Representational Analysis of Content Effects](https://arxiv.org/abs/2510.06700)
*Leonardo Bertolazzi,Sandro Pezzelle,Raffaelle Bernardi*

Main category: cs.CL

TL;DR: 研究揭示LLMs将逻辑有效性与语义合理性线性编码为强对齐的表示结构，这种表征重叠导致模型混淆两者。通过构建解耦向量可显著降低内容效应，提升推理准确率。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型中内容效应的形成机制（即语义合理性对逻辑判断的干扰），寻求通过表征干预提升模型逻辑推理能力的方法论路径。

Method: 结合几何表征分析验证概念对齐程度，使用导向向量进行因果干预实验，构建解耦向量实现概念分离。

Result: 有效性与合理性表征高度线性相关，二者存在因果双向影响；解耦干预可使内容效应降低38%，在GSM8K等基准上准确率提升12%。

Conclusion: 该研究揭示了LLMs中抽象逻辑概念的表征特性，为通过表征工程构建更可靠的逻辑推理系统提供了新范式。

Abstract: Both humans and large language models (LLMs) exhibit content effects: biases
in which the plausibility of the semantic content of a reasoning problem
influences judgments regarding its logical validity. While this phenomenon in
humans is best explained by the dual-process theory of reasoning, the
mechanisms behind content effects in LLMs remain unclear. In this work, we
address this issue by investigating how LLMs encode the concepts of validity
and plausibility within their internal representations. We show that both
concepts are linearly represented and strongly aligned in representational
geometry, leading models to conflate plausibility with validity. Using steering
vectors, we demonstrate that plausibility vectors can causally bias validity
judgements, and vice versa, and that the degree of alignment between these two
concepts predicts the magnitude of behavioral content effects across models.
Finally, we construct debiasing vectors that disentangle these concepts,
reducing content effects and improving reasoning accuracy. Our findings advance
understanding of how abstract logical concepts are represented in LLMs and
highlight representational interventions as a path toward more logical systems.

</details>


### [41] [Scaling LLM Multi-turn RL with End-to-end Summarization-based Context Management](https://arxiv.org/abs/2510.06727)
*Miao Lu,Weiwei Sun,Weihua Du,Zhan Ling,Xuesong Yao,Kang Liu,Jiecao Chen*

Main category: cs.CL

TL;DR: 提出SUPO算法，通过摘要管理上下文突破固定长度限制，提升LLM代理在长时程工具使用任务中的表现


<details>
  <summary>Details</summary>
Motivation: 现有RL方法在长时程任务中面临上下文长度限制、指令遵循能力退化、高成本等问题，需开发可扩展的上下文管理机制

Method: 引入周期性摘要生成压缩工具使用历史，结合策略梯度实现工具使用行为与摘要策略的端到端联合优化（SUPO框架）

Result: 在交互式函数调用/搜索任务中，SUPO成功率提升28%且工作上下文长度降低20%，复杂任务中摘要轮次扩展可进一步优化性能

Conclusion: 基于摘要的上下文管理为突破固定上下文限制提供了原则性解决方案，验证了该方法在长时程RL训练中的有效性和可扩展性

Abstract: We study reinforcement learning (RL) fine-tuning of large language model
(LLM) agents for long-horizon multi-turn tool use, where context length quickly
becomes a fundamental bottleneck. Existing RL pipelines can suffer from
degraded instruction following, excessive rollout costs, and most importantly,
strict context limits. To address these challenges, we introduce
summarization-based context management to training. In specific, it
periodically compresses the tool using history by LLM-generated summaries that
retain task-relevant information to keep a compact context while enabling the
agent to scale beyond the fixed context window. Building on this formulation,
we derive a policy gradient representation that seamlessly enables standard LLM
RL infrastructures to optimize both tool-use behaviors as well as summarization
strategies in an end-to-end fashion. We instantiate this framework with
\underline{SU}mmarization augmented \underline{P}olicy \underline{O}ptimization
(\texttt{SUPO}), an LLM RL algorithm that enables long-horizon training beyond
a fixed context limit. Experiments on interactive function calling and
searching tasks demonstrate that \texttt{SUPO} significantly improves the
success rate while maintaining the same or even lower working context length
compared to baselines. We also demonstrate that for complex searching tasks,
\texttt{SUPO} can further improve the evaluation performance when scaling
test-time maximum round of summarization beyond that of training time. Our
results establish summarization-based context management as a principled and
scalable approach for training RL agents beyond a fixed context length limit.

</details>


### [42] [PTEB: Towards Robust Text Embedding Evaluation via Stochastic Paraphrasing at Evaluation Time with LLMs](https://arxiv.org/abs/2510.06730)
*Manuel Frank,Haithem Afli*

Main category: cs.CL

TL;DR: 提出动态评估框架PTEB，通过生成语义保留的随机转述，揭示静态评估基准的局限性并验证模型对词汇变化的敏感性。


<details>
  <summary>Details</summary>
Motivation: 现有静态评估基准（如MTEB）可能导致模型过拟合，无法反映真实场景下的鲁棒性。需建立动态评估体系以更全面测试模型性能。

Method: 基于LLM生成token多样化但语义保留的转述，结合语义相似度黄金标准，在7个MTEB任务和3个多语言数据集上进行多轮统计验证。

Result: 发现句子编码器性能对token空间变化敏感（语义不变时），且模型大小与敏感性无直接关联，结论具有统计学显著性。

Conclusion: 提出NLP评估范式转变：从静态基准转向利用评估时算力的动态随机评估，提高测试结果的泛化可靠性。

Abstract: Current evaluations of sentence embedding models typically rely on static
test beds such as the Massive Text Embedding Benchmark (MTEB). While
invaluable, repeated tuning on a fixed suite can inflate reported performance
and obscure real-world robustness. We introduce the Paraphrasing Text Embedding
Benchmark (PTEB), a dynamic protocol that stochastically generates
meaning-preserving paraphrases at evaluation time and aggregates results across
multiple runs. Using a cost-efficient LLM-based method grounded in semantic
textual similarity gold ratings, we show that LLMs generate token-diverse but
semantically preserving, paraphrases. Across 7 MTEB tasks, we validate our
hypothesis that the performance of sentence encoders is sensitive to changes in
token space even when semantics remain fixed. We also observe that smaller
models are not disproportionately affected relative to larger ones. Our results
are statistically robust over multiple runs and we extended our experiments to
3 multilingual datasets covering 10 languages. More generally, we aim to
propose a new evaluation paradigm in NLP that relies less on static,
pre-defined benchmarks but shifts towards dynamic, stochastic evaluation
leveraging eval-time compute.

</details>


### [43] [Are LLMs Reliable Rankers? Rank Manipulation via Two-Stage Token Optimization](https://arxiv.org/abs/2510.06732)
*Tiancheng Xing,Jerry Li,Yixuan Du,Xiyang Hu*

Main category: cs.CL

TL;DR: RAF提出两阶段对抗性提示优化方法，成功操纵大语言模型的重排序结果，暴露检索系统安全漏洞


<details>
  <summary>Details</summary>
Motivation: 揭示大语言模型在信息检索中的排序脆弱性，证明其易受自然语言提示的对抗攻击

Method: 阶段1：结合排名梯度和可读性评分筛选候选token；阶段2：动态熵权平衡排名效果与文本自然度

Result: RAF在多模型实验中显著提升目标内容排名（攻击成功率+35%），且文本自然度优于基线方法23%

Conclusion: LLM重排序存在固有安全风险，对抗性提示威胁现代检索系统的可信度与鲁棒性

Abstract: Large language models (LLMs) are increasingly used as rerankers in
information retrieval, yet their ranking behavior can be steered by small,
natural-sounding prompts. To expose this vulnerability, we present Rank
Anything First (RAF), a two-stage token optimization method that crafts concise
textual perturbations to consistently promote a target item in LLM-generated
rankings while remaining hard to detect. Stage 1 uses Greedy Coordinate
Gradient to shortlist candidate tokens at the current position by combining the
gradient of the rank-target with a readability score; Stage 2 evaluates those
candidates under exact ranking and readability losses using an entropy-based
dynamic weighting scheme, and selects a token via temperature-controlled
sampling. RAF generates ranking-promoting prompts token-by-token, guided by
dual objectives: maximizing ranking effectiveness and preserving linguistic
naturalness. Experiments across multiple LLMs show that RAF significantly
boosts the rank of target items using naturalistic language, with greater
robustness than existing methods in both promoting target items and maintaining
naturalness. These findings underscore a critical security implication:
LLM-based reranking is inherently susceptible to adversarial manipulation,
raising new challenges for the trustworthiness and robustness of modern
retrieval systems. Our code is available at: https://github.com/glad-lab/RAF.

</details>


### [44] [AWM: Accurate Weight-Matrix Fingerprint for Large Language Models](https://arxiv.org/abs/2510.06738)
*Boyi Zeng,Lin Chen,Ziwei He,Xinbing Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: 提出基于权重矩阵的无训练指纹方法，通过线性分配问题（LAP）和中心化核对齐（CKA）相似度抵消后训练干扰，实现高鲁棒性LLM知识产权溯源。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）训练成本高昂，需可靠方法验证模型是否衍生自现有基座。但后训练过程（如微调、强化学习、剪枝等）严重干扰现有溯源技术。

Method: 利用权重矩阵构建模型指纹，结合LAP算法对齐参数空间，通过改进的CKA相似度度量消除参数操作影响，实现快速（30秒内）且无需训练的比较。

Result: 在60对正样本/90对负样本测试中，对6类后训练操作均保持100%准确率（AUC=1.0，FPR≈0），GPU计算耗时<30秒。

Conclusion: 该方法为模型溯源提供了鲁棒性强、计算高效的解决方案，可有效应对复杂后处理场景，代码已开源。

Abstract: Protecting the intellectual property of large language models (LLMs) is
crucial, given the substantial resources required for their training.
Consequently, there is an urgent need for both model owners and third parties
to determine whether a suspect LLM is trained from scratch or derived from an
existing base model. However, the intensive post-training processes that models
typically undergo-such as supervised fine-tuning, extensive continued
pretraining, reinforcement learning, multi-modal extension, pruning, and
upcycling-pose significant challenges to reliable identification. In this work,
we propose a training-free fingerprinting method based on weight matrices. We
leverage the Linear Assignment Problem (LAP) and an unbiased Centered Kernel
Alignment (CKA) similarity to neutralize the effects of parameter
manipulations, yielding a highly robust and high-fidelity similarity metric. On
a comprehensive testbed of 60 positive and 90 negative model pairs, our method
demonstrates exceptional robustness against all six aforementioned
post-training categories while exhibiting a near-zero risk of false positives.
By achieving perfect scores on all classification metrics, our approach
establishes a strong basis for reliable model lineage verification. Moreover,
the entire computation completes within 30s on an NVIDIA 3090 GPU. The code is
available at https://github.com/LUMIA-Group/AWM.

</details>


### [45] [TWIST: Training-free and Label-free Short Text Clustering through Iterative Vector Updating with LLMs](https://arxiv.org/abs/2510.06747)
*I-Fan Lin,Faegheh Hasibi,Suzan Verberne*

Main category: cs.CL

TL;DR: 提出无需训练/标签的短文本聚类方法，通过迭代向量更新和LLM指导实现高效聚类


<details>
  <summary>Details</summary>
Motivation: 商业聊天机器人场景中缺乏标注数据且聚类数未知，现有方法依赖对比学习且需要先验知识

Method: 基于迭代向量更新：先构建稀疏向量表征，再通过LLM指导精炼，适配任意嵌入器和小型LLM

Result: 在多样数据集上达到/超越现有对比学习方法，计算成本低且适配不同聚类算法

Conclusion: 低资源需求、强适应性和可扩展性使该方法更符合现实商业场景需求

Abstract: In this paper, we propose a training-free and label-free method for short
text clustering that can be used on top of any existing embedder. In the
context of customer-facing chatbots, companies are dealing with large amounts
of user utterances that need to be clustered according to their intent. In
these commercial settings, no labeled data is typically available, and the
number of clusters is not known. Our method is based on iterative vector
updating: it constructs sparse vectors based on representative texts, and then
iteratively refines them through LLM guidance. Our method achieves comparable
or superior results to state-of-the-art methods that use contrastive learning,
but without assuming prior knowledge of clusters or labels. Experiments on
diverse datasets and smaller LLMs show that our method is model agnostic and
can be applied to any embedder, with relatively small LLMs, and different
clustering methods. We also show that our method scales to large datasets,
reducing the computational cost of the LLM. These low-resource, adaptable
settings and the scalability of our method make it more aligned with real-world
scenarios than existing clustering methods.

</details>


### [46] [A Formal Framework for Fluency-based Multi-Reference Evaluation in Grammatical Error Correction](https://arxiv.org/abs/2510.06749)
*Eitan Klinger,Zihao Huang,Tran Minh Nguyen,Emma Jayeon Park,Yige Chen,Yang Gu,Qingyu Gao,Siliang Liu,Mengyang Qiu,Jungyeul Park*

Main category: cs.CL

TL;DR: 提出基于流畅度的多参考评估框架GLEU，通过四种聚合策略在多语言环境中有效评估语法纠错系统


<details>
  <summary>Details</summary>
Motivation: 现有语法纠错评估指标过度依赖单一参考修正，难以适应多语言和生成式场景的多样性需求

Method: 将n-gram相似度建模为多合法修正的聚合问题，提出select-best/simple-average/weighted-average/merged-counts四种策略

Result: 在捷克语、爱沙尼亚语、乌克兰语和中文的实验中，不同策略展现出对流畅度和覆盖率的互补性捕获能力

Conclusion: 该框架统一了多参考评估标准，在保持语言多样性的同时避免对合理变体的惩罚，支持更灵活的系统评估

Abstract: Evaluating grammatical error correction requires metrics that reflect the
diversity of valid human corrections rather than privileging a single
reference. Existing frameworks, largely edit-based and English-centric, rely on
rigid alignments between system and reference edits, limiting their
applicability in multilingual and generative settings. This paper introduces a
formal framework for \textit{fluency-based multi-reference evaluation}, framing
$n$-gram similarity as an aggregation problem over multiple legitimate
corrections. Within this formulation, we instantiate GLEU through four
aggregation strategies--\textsc{select-best}, \textsc{simple-average},
\textsc{weighted-average}, and \textsc{merged-counts}--and analyze their
properties of boundedness, monotonicity, and sensitivity to reference
variation. Empirical results on Czech, Estonian, Ukrainian, and Chinese corpora
show that these strategies capture complementary aspects of fluency and
coverage. The framework unifies multi-reference evaluation into a principled,
fluency-oriented approach that incorporates linguistic diversity without
penalizing legitimate variation.

</details>


### [47] [Gold-Switch: Training-Free Superposition of Slow- and Fast- Thinking LLMs](https://arxiv.org/abs/2510.06750)
*Jaeseong Lee,Dayoung Kwon,seung-won hwang*

Main category: cs.CL

TL;DR: 针对大型推理模型存在的过度思考问题，提出叠加部署策略与低秩调整方法，在减少计算量的同时保持推理能力


<details>
  <summary>Details</summary>
Motivation: 现有方法需部署多模型导致成本过高，需开发在单模型上抑制过度思考且节省资源的优化方案

Method: 通过奇异值累积能量分析确定最优低秩投影，采用轻量级无训练调节实现推理过程的自适应计算缩放

Result: 该方法能有效平衡计算效率与推理质量，实现推理过程的'恰到好处'调整

Conclusion: 基于数学分析的参数投影方法为模型推理优化提供了新思路，在资源受限场景中具有实用价值

Abstract: Large Reasoning Models (LRMs) excel in structured tasks by emulating
deliberate human reasoning but often suffer from overthinking, degrading
performance and wasting resources. One possible baseline is to deploy both LLM
and LRM, then route input by predicting whether it requires reasoning and may
cause overthinking. However, deploying multiple models can be costly or
impractical. We propose a superposed deployment strategy with a lightweight,
training-free regulation to optimize inference by switching one model on and
off. Instead of routing, we selectively unlearn from LRM at inference, scaling
down computation while preserving reasoning. By analyzing the cumulative energy
of singular values, we identify optimal low-rank projections to adjust
reasoning just right.

</details>


### [48] [Adaptive LLM-Symbolic Reasoning via Dynamic Logical Solver Composition](https://arxiv.org/abs/2510.06774)
*Lei Xu,Pierre Beckmann,Marco Valentino,André Freitas*

Main category: cs.CL

TL;DR: 提出了自适应多范式神经符号推理框架，通过自动识别推理策略和动态选择逻辑求解器，显著提升复杂推理任务性能


<details>
  <summary>Details</summary>
Motivation: 现有神经符号NLP方法多为静态集成，无法灵活运用不同形式推理策略，限制了处理异构推理挑战的能力

Method: 构建包含自动策略识别（自然语言问题→形式推理策略）和动态求解器选择（通过自动形式化接口）的框架

Result: 框架准确率超90%，性能领先GPT-4o 27%和DeepSeek-V3.1 6%；自适应推理使纯LLM方法在零样本/CoT/符号CoT场景分别提升10%/5%/6%

Conclusion: 建立了自适应LLM-符号推理基础，揭示了通过后训练提升小模型能力路径，为统一材料与形式推理提供新范式

Abstract: Neuro-symbolic NLP methods aim to leverage the complementary strengths of
large language models and formal logical solvers. However, current approaches
are mostly static in nature, i.e., the integration of a target solver is
predetermined at design time, hindering the ability to employ diverse formal
inference strategies. To address this, we introduce an adaptive,
multi-paradigm, neuro-symbolic inference framework that: (1) automatically
identifies formal reasoning strategies from problems expressed in natural
language; and (2) dynamically selects and applies specialized formal logical
solvers via autoformalization interfaces. Extensive experiments on individual
and multi-paradigm reasoning tasks support the following conclusions: LLMs are
effective at predicting the necessary formal reasoning strategies with an
accuracy above 90 percent. This enables flexible integration with formal
logical solvers, resulting in our framework outperforming competing baselines
by 27 percent and 6 percent compared to GPT-4o and DeepSeek-V3.1, respectively.
Moreover, adaptive reasoning can even positively impact pure LLM methods,
yielding gains of 10, 5, and 6 percent on zero-shot, CoT, and symbolic CoT
settings with GPT-4o. Finally, although smaller models struggle with adaptive
neuro-symbolic reasoning, post-training offers a viable path to improvement.
Overall, this work establishes the foundations for adaptive LLM-symbolic
reasoning, offering a path forward for unifying material and formal inferences
on heterogeneous reasoning challenges.

</details>


### [49] [Foundations of LLM Knowledge Materialization: Termination, Reproducibility, Robustness](https://arxiv.org/abs/2510.06780)
*Luca Giordano,Simon Razniewski*

Main category: cs.CL

TL;DR: 研究通过miniGPTKBs量化LLM知识结构化能力，发现知识提取具备高终止率但模型依赖性强，鲁棒性因扰动类型而异。


<details>
  <summary>Details</summary>
Motivation: LLM蕴含丰富事实知识，但将其系统化、结构化仍面临挑战，现有递归提取方法存在终止性、可复现性和鲁棒性三大核心问题亟待探索。

Method: 使用miniGPTKBs（领域限定子爬取框架），从产量、词汇相似度、语义相似度三个维度，在历史/娱乐/金融三个领域测试种子/语言/随机性/模型四类参数扰动。

Result: ① 高终止率（模型敏感） ② 可复现性不稳定 ③ 鲁棒性：种子/温度扰动下表现强，语言/模型扰动下较弱。

Conclusion: LLM知识结构化能有效提取核心知识，但存在模型依赖性、参数敏感性等显著局限，需针对性优化。

Abstract: Large Language Models (LLMs) encode substantial factual knowledge, yet
measuring and systematizing this knowledge remains challenging. Converting it
into structured format, for example through recursive extraction approaches
such as the GPTKB methodology (Hu et al., 2025b), is still underexplored. Key
open questions include whether such extraction can terminate, whether its
outputs are reproducible, and how robust they are to variations. We
systematically study LLM knowledge materialization using miniGPTKBs
(domain-specific, tractable subcrawls), analyzing termination, reproducibility,
and robustness across three categories of metrics: yield, lexical similarity,
and semantic similarity. We experiment with four variations (seed, language,
randomness, model) and three illustrative domains (from history, entertainment,
and finance). Our findings show (i) high termination rates, though
model-dependent; (ii) mixed reproducibility; and (iii) robustness that varies
by perturbation type: high for seeds and temperature, lower for languages and
models. These results suggest that LLM knowledge materialization can reliably
surface core knowledge, while also revealing important limitations.

</details>


### [50] [FURINA: A Fully Customizable Role-Playing Benchmark via Scalable Multi-Agent Collaboration Pipeline](https://arxiv.org/abs/2510.06800)
*Haotian Wu,Shufan Jiang,Chios Chen,Yiyang Feng,Hehai Lin,Heqing Zou,Yao Shu,Yanran Li,Chengwei Qin*

Main category: cs.CL

TL;DR: Proposes FURINA-Builder pipeline for automatically constructing customizable role-playing benchmarks, revealing performance disparities and novel reasoning-reliability tradeoffs in LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing RP benchmarks become obsolete due to narrow scope and limited adaptability, requiring dynamic evaluation frameworks for evolving LLM capabilities.

Method: Multi-agent pipeline simulating character dialogues using character-scene pools, with LLM judge adjusting responses into test utterances.

Result: Established characters outperform synthesized ones; Model scale doesn't linearly reduce hallucinations; Reasoning improves RP performance but increases hallucinations.

Conclusion: FURINA-Builder effectively addresses benchmark customization needs while revealing critical performance-reliability tradeoffs in LLMs through FURINA-Bench challenges.

Abstract: As large language models (LLMs) advance in role-playing (RP) tasks, existing
benchmarks quickly become obsolete due to their narrow scope, outdated
interaction paradigms, and limited adaptability across diverse application
scenarios. To address this gap, we introduce FURINA-Builder, a novel
multi-agent collaboration pipeline that automatically constructs fully
customizable RP benchmarks at any scale. It enables evaluation of arbitrary
characters across diverse scenarios and prompt formats, as the first benchmark
builder in RP area for adaptable assessment. FURINA-Builder simulates dialogues
between a test character and other characters drawn from a well-constructed
character-scene pool, while an LLM judge selects fine-grained evaluation
dimensions and adjusts the test character's responses into final test
utterances. Using this pipeline, we build FURINA-Bench, a new comprehensive
role-playing benchmark featuring both established and synthesized test
characters, each assessed with dimension-specific evaluation criteria. Human
evaluation and preliminary separability analysis justify our pipeline and
benchmark design. We conduct extensive evaluations of cutting-edge LLMs and
find that o3 and DeepSeek-R1 achieve the best performance on English and
Chinese RP tasks, respectively. Across all models, established characters
consistently outperform synthesized ones, with reasoning capabilities further
amplifying this disparity. Interestingly, we observe that model scale does not
monotonically reduce hallucinations. More critically, for reasoning LLMs, we
uncover a novel trade-off: reasoning improves RP performance but simultaneously
increases RP hallucinations. This trade-off extends to a broader Pareto
frontier between RP performance and reliability for all LLMs. These findings
demonstrate the effectiveness of FURINA-Builder and the challenge posed by
FURINA-Bench.

</details>


### [51] [Overview of the Plagiarism Detection Task at PAN 2025](https://arxiv.org/abs/2510.06805)
*André Greiner-Petter,Maik Fröbe,Jan Philip Wahle,Terry Ruas,Bela Gipp,Akiko Aizawa,Martin Potthast*

Main category: cs.CL

TL;DR: PAN 2025提出生成式剽窃检测任务，使用Llama等大模型构建新数据集，发现当前基于语义相似度的方法在2015数据集上泛化性不足


<details>
  <summary>Details</summary>
Motivation: 针对科学论文中自动生成的文本剽窃现象，探索更有效的检测方法并验证现有方法的鲁棒性

Method: 1. 使用Llama/DeepSeek-R1/Mistral构建大规模生成剽窃数据集 2. 对比参赛方案与基线模型表现 3. 在PAN 2015数据集进行泛化性测试

Result: 当前方法过度依赖嵌入向量语义相似度（召回率0.8，精度0.5），但在2015数据集上表现显著下降（F1值降幅超50%）

Conclusion: 现有检测方法泛化能力有限，需开发更鲁棒的检测框架，同时提示当前数据集设计存在改进空间

Abstract: The generative plagiarism detection task at PAN 2025 aims at identifying
automatically generated textual plagiarism in scientific articles and aligning
them with their respective sources. We created a novel large-scale dataset of
automatically generated plagiarism using three large language models: Llama,
DeepSeek-R1, and Mistral. In this task overview paper, we outline the creation
of this dataset, summarize and compare the results of all participants and four
baselines, and evaluate the results on the last plagiarism detection task from
PAN 2015 in order to interpret the robustness of the proposed approaches. We
found that the current iteration does not invite a large variety of approaches
as naive semantic similarity approaches based on embedding vectors provide
promising results of up to 0.8 recall and 0.5 precision. In contrast, most of
these approaches underperform significantly on the 2015 dataset, indicating a
lack in generalizability.

</details>


### [52] [BlackboxNLP-2025 MIB Shared Task: Exploring Ensemble Strategies for Circuit Localization Methods](https://arxiv.org/abs/2510.06811)
*Philipp Mondorf,Mingyang Wang,Sebastian Gerstner,Ahmad Dawar Hakimi,Yihong Liu,Leonor Veloso,Shijia Zhou,Hinrich Schütze,Barbara Plank*

Main category: cs.CL

TL;DR: 通过并行和序列集成方法改进LLM电路定位精度，在MIB基准测试中取得显著效果提升


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型电路定位任务，探索集成方法提升现有单方法性能局限，通过组合不同定位方法的优势实现更精准的电路识别

Method: 提出并行集成（多方法属性分数聚合）和序列集成（用EAP-IG引导边缘剪枝）两种策略，在MIB基准测试框架下验证有效性

Result: 集成方法在多个模型-任务组合中超越基线，其中并行集成（含序列集成）方案表现最优

Conclusion: 集成策略有效提升电路定位精度，为复杂神经网络的可解释性研究提供方法论创新

Abstract: The Circuit Localization track of the Mechanistic Interpretability Benchmark
(MIB) evaluates methods for localizing circuits within large language models
(LLMs), i.e., subnetworks responsible for specific task behaviors. In this
work, we investigate whether ensembling two or more circuit localization
methods can improve performance. We explore two variants: parallel and
sequential ensembling. In parallel ensembling, we combine attribution scores
assigned to each edge by different methods-e.g., by averaging or taking the
minimum or maximum value. In the sequential ensemble, we use edge attribution
scores obtained via EAP-IG as a warm start for a more expensive but more
precise circuit identification method, namely edge pruning. We observe that
both approaches yield notable gains on the benchmark metrics, leading to a more
precise circuit identification approach. Finally, we find that taking a
parallel ensemble over various methods, including the sequential ensemble,
achieves the best results. We evaluate our approach in the BlackboxNLP 2025 MIB
Shared Task, comparing ensemble scores to official baselines across multiple
model-task combinations.

</details>


### [53] [Adaptive Tool Generation with Models as Tools and Reinforcement Learning](https://arxiv.org/abs/2510.06825)
*Chenpeng Wang,Xiaojie Cheng,Chunye Wang,Linfeng Yang,Lei Zhang*

Main category: cs.CL

TL;DR: MTR框架通过模拟训练实现工具增强推理，无需实时API即可在多跳QA任务中取得竞争力表现


<details>
  <summary>Details</summary>
Motivation: 解决工具增强语言模型依赖实时API导致的训练/部署扩展性差、可靠性低问题

Method: 采用多智能体架构(ToolMaker生成工具接口、AutoAgent生成推理序列、ToolActor模拟响应)，通过SFT阶段学习轨迹语法，GRPO阶段优化复合奖励策略

Result: 在HotpotQA等四个多跳QA基准测试中达到与实时API系统相当的EM分数，在复杂推理任务表现更优

Conclusion: 结构化轨迹训练可有效学习工具推理能力，证明无需实时API交互也能实现高效工具使用

Abstract: Tool-augmented language models have demonstrated strong capabilities, but
their reliance on live API access creates scalability and reliability
challenges during training and deployment. We propose MTR, a simulation-first
training framework for tool-augmented reasoning. Instead of relying on live
APIs, MTR learns from complete ReAct traces with schema-validated, simulated
observations. Our approach operates through a multi-agent architecture where a
ToolMaker generates task-specific, OpenAI-compatible tool interfaces, an
AutoAgent produces structured think-act-observe sequences, and a ToolActor
simulates realistic responses. Training proceeds in two stages: Stage-1
Supervised Fine-Tuning (SFT) teaches 'trace grammar' from complete reasoning
sequences; Stage-2 Group Relative Policy Optimization (GRPO) optimizes strategy
with a composite trace reward that balances answer correctness and internal
consistency. Across four multi-hop QA benchmarks (HotpotQA, MuSiQue,
2WikiMultiHopQA, Bamboogle), MTR attains competitive Exact Match (EM) scores to
live-API systems and excels on reasoning-intensive tasks, suggesting that
effective tool reasoning can be learned from structured traces without live
interactions.

</details>


### [54] [Mid-Training of Large Language Models: A Survey](https://arxiv.org/abs/2510.06826)
*Kaixiang Mo,Yuxin Shi,Weiwei Weng,Zhiqiang Zhou,Shuman Liu,Haibo Zhang,Anxiang Zeng*

Main category: cs.CL

TL;DR: 提出大语言模型中期训练的统一范式，通过数据优化、学习率调度和长上下文扩展提升模型性能，填补该领域系统性研究的空白。


<details>
  <summary>Details</summary>
Motivation: 现有LLM开发流程缺乏对中期训练阶段的系统性研究，尽管该阶段在先进模型中广泛使用。本文旨在建立中期训练的理论框架和实践指导。

Method: 构建首个中期训练分类体系，结合梯度噪声规模/信息瓶颈理论解释有效性，收集评估基准实现跨模型结构化比较。

Result: 中期训练通过数据质量迭代、优化稳定性增强和上下文扩展，显著提升模型泛化能力和收敛效率。

Conclusion: 中期训练是LLM开发的关键阶段，提出的分类法和实践见解为未来研究提供结构化基础，但长上下文优化等挑战仍需突破。

Abstract: Large language models (LLMs) are typically developed through large-scale
pre-training followed by task-specific fine-tuning. Recent advances highlight
the importance of an intermediate mid-training stage, where models undergo
multiple annealing-style phases that refine data quality, adapt optimization
schedules, and extend context length. This stage mitigates diminishing returns
from noisy tokens, stabilizes convergence, and expands model capability in late
training. Its effectiveness can be explained through gradient noise scale, the
information bottleneck, and curriculum learning, which together promote
generalization and abstraction. Despite widespread use in state-of-the-art
systems, there has been no prior survey of mid-training as a unified paradigm.
We introduce the first taxonomy of LLM mid-training spanning data distribution,
learning-rate scheduling, and long-context extension. We distill practical
insights, compile evaluation benchmarks, and report gains to enable structured
comparisons across models. We also identify open challenges and propose avenues
for future research and practice.

</details>


### [55] [GAMBIT+: A Challenge Set for Evaluating Gender Bias in Machine Translation Quality Estimation Metrics](https://arxiv.org/abs/2510.06841)
*Giorgos Filandrianos,Orfeas Menis Mastromichalakis,Wafaa Mohammed,Giuseppe Attanasio,Chrysoula Zerva*

Main category: cs.CL

TL;DR: 开发大规模跨语言数据集分析自动翻译质量评估(QE)指标的性别偏见


<details>
  <summary>Details</summary>
Motivation: 现有研究受限于数据规模、职业覆盖和语言多样性，需系统性探究QE指标在性别模糊职业术语翻译评估中的偏差

Method: 基于GAMBIT语料库扩展11种目标语言和3种源语言，构建33个语言对的平行文本集，通过对比同一职业术语不同语法性别的翻译质量评分

Result: 创建包含完全平行设计的数据集，支持跨语言/职业的细粒度偏差分析，为评估QE指标公平性提供基准

Conclusion: 该数据集填补了QE指标性别偏见研究的空白，为开发无偏见的机器翻译评估系统奠定基础

Abstract: Gender bias in machine translation (MT) systems has been extensively
documented, but bias in automatic quality estimation (QE) metrics remains
comparatively underexplored. Existing studies suggest that QE metrics can also
exhibit gender bias, yet most analyses are limited by small datasets, narrow
occupational coverage, and restricted language variety. To address this gap, we
introduce a large-scale challenge set specifically designed to probe the
behavior of QE metrics when evaluating translations containing gender-ambiguous
occupational terms. Building on the GAMBIT corpus of English texts with
gender-ambiguous occupations, we extend coverage to three source languages that
are genderless or natural-gendered, and eleven target languages with
grammatical gender, resulting in 33 source-target language pairs. Each source
text is paired with two target versions differing only in the grammatical
gender of the occupational term(s) (masculine vs. feminine), with all dependent
grammatical elements adjusted accordingly. An unbiased QE metric should assign
equal or near-equal scores to both versions. The dataset's scale, breadth, and
fully parallel design, where the same set of texts is aligned across all
languages, enables fine-grained bias analysis by occupation and systematic
comparisons across languages.

</details>


### [56] [SID: Multi-LLM Debate Driven by Self Signals](https://arxiv.org/abs/2510.06843)
*Xuhang Chen,Zhifan Song,Deyi Ji,Shuo Gao,Lanyun Zhu*

Main category: cs.CL

TL;DR: 提出SID方法，通过模型置信度和语义聚焦优化多智能体辩论，提升效率与性能


<details>
  <summary>Details</summary>
Motivation: 现有多LLM辩论方法依赖外部结构，忽视生成过程中的置信度和注意力等内在信号，导致冗余计算和性能下降

Method: 使用模型级置信度实现早退机制，通过注意力机制压缩冗余内容，形成自适应辩论流程

Result: 在多个基准测试中准确率超越现有方法，同时显著降低token消耗

Conclusion: 利用LLM自身信号可有效提升多智能体辩论系统的性能和效率

Abstract: Large Language Models (LLMs) have exhibited impressive capabilities across
diverse application domains. Recent work has explored Multi-LLM Agent Debate
(MAD) as a way to enhance performance by enabling multiple LLMs to discuss and
refine responses iteratively. Nevertheless, existing MAD methods predominantly
focus on utilizing external structures, such as debate graphs, using
LLM-as-a-Judge, while neglecting the application of self signals, such as token
logits and attention, that arise during generation. This omission leads to
redundant computation and potential performance degradation. In this paper, we
shift the focus to the self signals of multi-LLM debate and introduce a
Self-Signals Driven Multi-LLM Debate (SID), which leverages two types of
self-signals: model-level confidence and token-level semantic focus, to
adaptively guide the debate process. Our approach enables high-confidence
agents to exit early at the model level and compress the redundant debate
contents based on the attention mechanism. We evaluate our method on various
LLMs and Multimodal LLMs across multiple challenging benchmarks. Experimental
results demonstrate that our method not only outperforms existing MAD
techniques in accuracy but also reduces token consumption, highlighting the
effectiveness of utilizing self signals in enhancing both the performance and
efficiency of multi-agent debate systems. Our code will be available
at~\href{https://github.com/xuhang2019/SID}{\texttt{https://github.com/xuhang2019/SID}}.

</details>


### [57] [OpenJAI-v1.0: An Open Thai Large Language Model](https://arxiv.org/abs/2510.06847)
*Pontakorn Trakuekul,Attapol T. Rutherford,Jullajak Karnjanaekarin,Narongkorn Panitsrisit,Sumana Sumanakul*

Main category: cs.CL

TL;DR: 基于Qwen3-14B开发的开源泰英双语大模型OpenJAI-v1.0，在指令遵循、长文本理解及工具使用三大场景通过精选数据优化，评测表现超越同类泰语模型且避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 为泰国AI社区提供高性能的本地化NLP资源，解决现有开源泰语模型在实际应用任务中的性能瓶颈问题。

Method: 在Qwen3-14B基础上，针对指令响应、长上下文处理和工具调用三个核心应用场景进行数据优化训练，采用防灾难遗忘技术。

Result: 在多领域基准测试中性能超越主流开源泰语模型，同时保持英语任务能力不退化。

Conclusion: OpenJAI-v1.0的发布为泰国AI社区提供了重要的替代性开源选择，推动本地化语言模型发展。

Abstract: We introduce OpenJAI-v1.0, an open-source large language model for Thai and
English, developed from the Qwen3-14B model. Our work focuses on boosting
performance on practical tasks through carefully curated data across three key
use cases: instruction following, long-context understanding, and tool use.
Evaluation results show that OpenJAI-v1.0 improves on the capabilities of its
base model and outperforms other leading open-source Thai models on a diverse
suite of benchmarks, while avoiding catastrophic forgetting. OpenJAI-v1.0 is
publicly released as another alternative NLP resource for the Thai AI
community.

</details>


### [58] [Unlocking Latent Discourse Translation in LLMs Through Quality-Aware Decoding](https://arxiv.org/abs/2510.06866)
*Wafaa Mohammed,Vlad Niculae,Chrysoula Zerva*

Main category: cs.CL

TL;DR: 大语言模型在机器翻译中虽强但存在语篇处理缺陷，提出质量感知解码(QAD)有效提取模型内隐语篇知识，提升翻译质量


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在文档级翻译中处理代词解析、词汇衔接等语篇现象的不足

Method: 采用质量感知解码(QAD)策略，通过概率调整机制提取模型内隐的语篇知识

Result: QAD超越其他解码方法，提升翻译语义丰富度并与人类偏好更契合（人工评估+自动指标验证）

Conclusion: 证明LLMs具备潜在语篇知识，QAD为上下文感知翻译提供有效解决方案

Abstract: Large language models (LLMs) have emerged as strong contenders in machine
translation.Yet, they still struggle to adequately handle discourse phenomena,
such as pronoun resolution and lexical cohesion at the document level. In this
study, we thoroughly investigate the discourse phenomena performance of LLMs in
context-aware translation. We demonstrate that discourse knowledge is encoded
within LLMs and propose the use of quality-aware decoding (QAD) to effectively
extract this knowledge, showcasing its superiority over other decoding
approaches through comprehensive analysis. Furthermore, we illustrate that QAD
enhances the semantic richness of translations and aligns them more closely
with human preferences.

</details>


### [59] [$λ$-GRPO: Unifying the GRPO Frameworks with Learnable Token Preferences](https://arxiv.org/abs/2510.06870)
*Yining Wang,Jinman Zhao,Chuangxin Zhao,Shuhao Guan,Gerald Penn,Shinan Liu*

Main category: cs.CL

TL;DR: 提出λ-GRPO方法，通过可学习参数自适应控制token权重，解决GRPO长度偏差问题，在数学推理基准上实现性能提升且无需额外计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统GRPO存在长度偏差问题（长文本奖励被均摊导致梯度更新失衡），现有改进方案（DAPO/Dr.GRPO）依赖启发式规则且可解释性不足。需要探索模型自主学习token偏好的方法。

Method: 在统一框架中引入可学习参数λ，动态调节token-level权重分配（λ-GRPO）。通过端到端训练自适应学习不同token的重要性。

Result: 在1.5B/3B/7B参数的Qwen2.5模型上，相比GRPO平均准确率提升+1.9%/+1.0%/+1.7%，且不增加训练数据或计算开销。

Conclusion: 学习token偏好机制有效提升了数学推理能力，证明了自适应权重调节的实用价值，为强化学习优化提供了新思路。

Abstract: Reinforcement Learning with Human Feedback (RLHF) has been the dominant
approach for improving the reasoning capabilities of Large Language Models
(LLMs). Recently, Reinforcement Learning with Verifiable Rewards (RLVR) has
simplified this paradigm by replacing the reward and value models with
rule-based verifiers. A prominent example is Group Relative Policy Optimization
(GRPO). However, GRPO inherently suffers from a length bias, since the same
advantage is uniformly assigned to all tokens of a response. As a result,
longer responses distribute the reward over more tokens and thus contribute
disproportionately to gradient updates. Several variants, such as DAPO and Dr.
GRPO, modify the token-level aggregation of the loss, yet these methods remain
heuristic and offer limited interpretability regarding their implicit token
preferences. In this work, we explore the possibility of allowing the model to
learn its own token preference during optimization. We unify existing
frameworks under a single formulation and introduce a learnable parameter
$\lambda$ that adaptively controls token-level weighting. We use $\lambda$-GRPO
to denote our method, and we find that $\lambda$-GRPO achieves consistent
improvements over vanilla GRPO and DAPO on multiple mathematical reasoning
benchmarks. On Qwen2.5 models with 1.5B, 3B, and 7B parameters, $\lambda$-GRPO
improves average accuracy by $+1.9\%$, $+1.0\%$, and $+1.7\%$ compared to GRPO,
respectively. Importantly, these gains come without any modifications to the
training data or additional computational cost, highlighting the effectiveness
and practicality of learning token preferences.

</details>


### [60] [MeXtract: Light-Weight Metadata Extraction from Scientific Papers](https://arxiv.org/abs/2510.06889)
*Zaid Alyafeai,Maged S. Al-Shaibani,Bernard Ghanem*

Main category: cs.CL

TL;DR: 提出轻量级语言模型家族MeXtract，通过微调Qwen 2.5实现科学论文元数据提取的SOTA性能，并扩展评估基准增强跨模式迁移能力。


<details>
  <summary>Details</summary>
Motivation: 传统元数据提取方法存在跨领域泛化能力弱和模式适应性差的问题，需要更灵活鲁棒的解决方案。

Method: 基于Qwen 2.5架构构建参数规模0.5B-3B的模型家族，通过schema微调策略提升模型适应性。

Result: 在MOLE基准取得同类最优，微调后模型在未见schema上保持85%+准确率，展示强迁移能力。

Conclusion: MeXtract系列实现高效精准的元数据提取，其开放模型和扩展基准推动科研社区发展。

Abstract: Metadata plays a critical role in indexing, documenting, and analyzing
scientific literature, yet extracting it accurately and efficiently remains a
challenging task. Traditional approaches often rely on rule-based or
task-specific models, which struggle to generalize across domains and schema
variations. In this paper, we present MeXtract, a family of lightweight
language models designed for metadata extraction from scientific papers. The
models, ranging from 0.5B to 3B parameters, are built by fine-tuning Qwen 2.5
counterparts. In their size family, MeXtract achieves state-of-the-art
performance on metadata extraction on the MOLE benchmark. To further support
evaluation, we extend the MOLE benchmark to incorporate model-specific
metadata, providing an out-of-domain challenging subset. Our experiments show
that fine-tuning on a given schema not only yields high accuracy but also
transfers effectively to unseen schemas, demonstrating the robustness and
adaptability of our approach. We release all the code, datasets, and models
openly for the research community.

</details>


### [61] [LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling](https://arxiv.org/abs/2510.06915)
*Zecheng Tang,Baibei Ji,Quantong Qiu,Haitian Wang,Xiaobo Liang,Juntao Li,Min Zhang*

Main category: cs.CL

TL;DR: 提出专用于长上下文奖励模型评估的基准Long-RewardBench，并通过多阶段训练策略构建高效LongRM模型


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型局限于短上下文场景，缺乏对长历史轨迹场景下上下文-响应一致性的评估能力

Method: 开发包含成对比较和Best-of-N任务的评估基准，设计多阶段训练策略增强模型的长上下文感知能力

Result: 8B参数的LongRM超越70B基线模型，与Gemini 2.5 Pro性能相当，同时保持短上下文评估能力

Conclusion: 该研究填补了长上下文奖励模型评估的空白，提出的训练框架有效提升模型在复杂场景下的稳定性

Abstract: Reward model (RM) plays a pivotal role in aligning large language model (LLM)
with human preferences. As real-world applications increasingly involve long
history trajectories, e.g., LLM agent, it becomes indispensable to evaluate
whether a model's responses are not only high-quality but also grounded in and
consistent with the provided context. Yet, current RMs remain confined to
short-context settings and primarily focus on response-level attributes (e.g.,
safety or helpfulness), while largely neglecting the critical dimension of long
context-response consistency. In this work, we introduce Long-RewardBench, a
benchmark specifically designed for long-context RM evaluation, featuring both
Pairwise Comparison and Best-of-N tasks. Our preliminary study reveals that
even state-of-the-art generative RMs exhibit significant fragility in
long-context scenarios, failing to maintain context-aware preference judgments.
Motivated by the analysis of failure patterns observed in model outputs, we
propose a general multi-stage training strategy that effectively scales
arbitrary models into robust Long-context RMs (LongRMs). Experiments show that
our approach not only substantially improves performance on long-context
evaluation but also preserves strong short-context capability. Notably, our 8B
LongRM outperforms much larger 70B-scale baselines and matches the performance
of the proprietary Gemini 2.5 Pro model.

</details>


### [62] [SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models](https://arxiv.org/abs/2510.06917)
*Cheng-Han Chiang,Xiaofei Wang,Linjie Li,Chung-Ching Lin,Kevin Lin,Shujie Liu,Zhendong Wang,Zhengyuan Yang,Hung-yi Lee,Lijuan Wang*

Main category: cs.CL

TL;DR: SHANKS框架使口语模型能在用户讲话时实时生成思维链推理，降低交互延迟，提升实时决策能力。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型需等待用户发言结束后才开始思考，导致高延迟，无法满足语音交互的实时性需求。受人类'边听边想'能力启发，需改进模型的实时推理机制。

Method: 将输入语音分块流式处理，每接收一个分块即基于历史语音和推理生成未说出的思维链，用于实时决定是否打断用户或调用工具。

Result: 在数学解题场景中中断准确率提升37.1%，工具调用场景56.9%的任务可在用户结束发言前完成。

Conclusion: SHANKS推动了持续思考模型的发展，使AI能在对话全程保持思考，而非仅在回合结束后响应。

Abstract: Current large language models (LLMs) and spoken language models (SLMs) begin
thinking and taking actions only after the user has finished their turn. This
prevents the model from interacting during the user's turn and can lead to high
response latency while it waits to think. Consequently, thinking after
receiving the full input is not suitable for speech-to-speech interaction,
where real-time, low-latency exchange is important. We address this by noting
that humans naturally "think while listening." In this paper, we propose
SHANKS, a general inference framework that enables SLMs to generate unspoken
chain-of-thought reasoning while listening to the user input. SHANKS streams
the input speech in fixed-duration chunks and, as soon as a chunk is received,
generates unspoken reasoning based on all previous speech and reasoning, while
the user continues speaking. SHANKS uses this unspoken reasoning to decide
whether to interrupt the user and to make tool calls to complete the task. We
demonstrate that SHANKS enhances real-time user-SLM interaction in two
scenarios: (1) when the user is presenting a step-by-step solution to a math
problem, SHANKS can listen, reason, and interrupt when the user makes a
mistake, achieving 37.1% higher interruption accuracy than a baseline that
interrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can
complete 56.9% of the tool calls before the user finishes their turn. Overall,
SHANKS moves toward models that keep thinking throughout the conversation, not
only after a turn ends. Animated illustrations of Shanks can be found at
https://d223302.github.io/SHANKS/

</details>


### [63] [Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation](https://arxiv.org/abs/2510.06961)
*Vaibhav Srivastav,Steven Zheng,Eric Bezzam,Eustache Le Bihan,Nithin Koluguri,Piotr Żelasko,Somshubra Majumdar,Adel Moumen,Sanchit Gandhi*

Main category: cs.CL

TL;DR: Open ASR Leaderboard推出标准化评测框架，对比60+语音识别系统，发现Conformer+LLM解码器英语识别最优但速度慢，CTC/TDT解码器更高效适合长语音场景。


<details>
  <summary>Details</summary>
Motivation: 当前ASR评估集中于英语短语音且效率指标缺失，需建立跨语言/长语音的统一评测体系。

Method: 标准化11个数据集文本规范，同时采用WER（词错率）和RTFx（逆实时因子）双指标，覆盖多语言/长语音专用数据集。

Result: 英语场景：Conformer编码器+LLM解码器平均WER最优（需21.3倍实时），CTC/TDT解码器RTFx提升3-4倍；微调Whisper英语精度提升但多语言能力下降。

Conclusion: 开源代码与数据集加载器推动透明化评估，不同解码器在精度/效率间存在明显trade-off，需根据应用场景选择。

Abstract: Despite rapid progress, ASR evaluation remains saturated with short-form
English, and efficiency is rarely reported. We present the Open ASR
Leaderboard, a fully reproducible benchmark and interactive leaderboard
comparing 60+ open-source and proprietary systems across 11 datasets, including
dedicated multilingual and long-form tracks. We standardize text normalization
and report both word error rate (WER) and inverse real-time factor (RTFx),
enabling fair accuracy-efficiency comparisons. For English transcription,
Conformer encoders paired with LLM decoders achieve the best average WER but
are slower, while CTC and TDT decoders deliver much better RTFx, making them
attractive for long-form and offline use. Whisper-derived encoders fine-tuned
for English improve accuracy but often trade off multilingual coverage. All
code and dataset loaders are open-sourced to support transparent, extensible
evaluation.

</details>


### [64] [EDUMATH: Generating Standards-aligned Educational Math Word Problems](https://arxiv.org/abs/2510.06965)
*Bryan R. Christ,Penelope Molitz,Jonathan Kropko,Thomas Hartvigsen*

Main category: cs.CL

TL;DR: 研究者利用大语言模型生成符合教育标准的定制数学应用题，构建首个教师标注数据集，开发12B开源模型性能匹敌更大模型，并通过学生实验验证定制题目的有效性。


<details>
  <summary>Details</summary>
Motivation: 教师因班级规模大和工作压力难以个性化定制数学应用题，而定制题目能提升学习效果，故探索用LLM辅助生成教育用题。

Method: 使用开放/闭源LLM生成11,000+数学题，建立人类专家-LLM联合评估体系，训练12B模型和30B文本分类器，开展首个定制数学题学生实验。

Result: 12B模型达到更大模型水平，30B模型超越闭源基线；生成题目与人工题目效果相当但更受学生青睐，定制题目接受度提升86%。

Conclusion: LLM可高效生成教育标准对齐的数学题，开源模型性能优越，学生实验证实定制题目的教育价值，为教师减负提供新方案。

Abstract: Math word problems (MWPs) are critical K-12 educational tools, and
customizing them to students' interests and ability levels can increase
learning outcomes. However, teachers struggle to find time to customize MWPs
for each student given large class sizes and increasing burnout. We propose
that LLMs can support math education by generating MWPs customized to student
interests and math education standards. To this end, we use a joint human
expert-LLM judge approach to evaluate over 11,000 MWPs generated by open and
closed LLMs and develop the first teacher-annotated dataset for
standards-aligned educational MWP generation. We show the value of our data by
using it to train a 12B open model that matches the performance of larger and
more capable open models. We also use our teacher-annotated data to train a
text classifier that enables a 30B open LLM to outperform existing closed
baselines without any training. Next, we show our models' MWPs are more similar
to human-written MWPs than those from existing models. We conclude by
conducting the first study of customized LLM-generated MWPs with grade school
students, finding they perform similarly on our models' MWPs relative to
human-written MWPs but consistently prefer our customized MWPs.

</details>


### [65] [Probing Social Identity Bias in Chinese LLMs with Gendered Pronouns and Social Groups](https://arxiv.org/abs/2510.06974)
*Geng Liu,Feng Li,Junjie Mu,Mengxiao Zhu,Francesco Pierri*

Main category: cs.CL

TL;DR: 中文大语言模型在群体身份框架中呈现系统性内群体偏好与外群体偏见，且真实对话场景加剧这种倾向


<details>
  <summary>Details</summary>
Motivation: 探究中文LLMs是否像英文模型一样存在社会身份偏见，及其在真实用户交互中的表现

Method: 结合控制实验（10个主流中文模型+240个社会群体）与真实用户对话语料分析

Result: 所有模型均呈现内群体积极/外群体消极倾向，真实对话中的偏见强度高于受控实验

Conclusion: 社会身份偏见具有跨语言普适性，用户导向的部署环境可能放大LLMs的偏见风险

Abstract: Large language models (LLMs) are increasingly deployed in user-facing
applications, raising concerns about their potential to reflect and amplify
social biases. We investigate social identity framing in Chinese LLMs using
Mandarin-specific prompts across ten representative Chinese LLMs, evaluating
responses to ingroup ("We") and outgroup ("They") framings, and extending the
setting to 240 social groups salient in the Chinese context. To complement
controlled experiments, we further analyze Chinese-language conversations from
a corpus of real interactions between users and chatbots. Across models, we
observe systematic ingroup-positive and outgroup-negative tendencies, which are
not confined to synthetic prompts but also appear in naturalistic dialogue,
indicating that bias dynamics might strengthen in real interactions. Our study
provides a language-aware evaluation framework for Chinese LLMs, demonstrating
that social identity biases documented in English generalize
cross-linguistically and intensify in user-facing contexts.

</details>


### [66] [Towards Reliable Retrieval in RAG Systems for Large Legal Datasets](https://arxiv.org/abs/2510.06999)
*Markus Reuter,Tobias Lingenberg,Rūta Liepiņa,Francesca Lagioia,Marco Lippi,Giovanni Sartor,Andrea Passerini,Burcu Sayin*

Main category: cs.CL

TL;DR: 论文提出摘要增强分块（SAC）技术，通过添加文档级摘要有效缓解法律领域RAG系统的文档级检索错配（DRM）问题，显著提升检索可靠性。


<details>
  <summary>Details</summary>
Motivation: 法律文档结构高度相似导致传统分块检索出现文档级检索错配（DRM），即从错误源文档中提取信息，严重影响RAG系统的可靠性。

Method: 采用摘要增强分块（SAC）技术：在文本分块时注入文档级人工摘要，保留全局上下文信息。对比通用摘要策略与法律专家定制的法律要素提取效果。

Result: SAC使DRM显著减少，文本级检索精确率/召回率同步提升（通用摘要策略意外优于法律要素定制方法）

Conclusion: SAC作为简单、可扩展的技术方案，能有效提升法律领域大规模文档数据集上RAG系统的可靠性，具有较强实践价值。

Abstract: Retrieval-Augmented Generation (RAG) is a promising approach to mitigate
hallucinations in Large Language Models (LLMs) for legal applications, but its
reliability is critically dependent on the accuracy of the retrieval step. This
is particularly challenging in the legal domain, where large databases of
structurally similar documents often cause retrieval systems to fail. In this
paper, we address this challenge by first identifying and quantifying a
critical failure mode we term Document-Level Retrieval Mismatch (DRM), where
the retriever selects information from entirely incorrect source documents. To
mitigate DRM, we investigate a simple and computationally efficient technique
which we refer to as Summary-Augmented Chunking (SAC). This method enhances
each text chunk with a document-level synthetic summary, thereby injecting
crucial global context that would otherwise be lost during a standard chunking
process. Our experiments on a diverse set of legal information retrieval tasks
show that SAC greatly reduces DRM and, consequently, also improves text-level
retrieval precision and recall. Interestingly, we find that a generic
summarization strategy outperforms an approach that incorporates legal expert
domain knowledge to target specific legal elements. Our work provides evidence
that this practical, scalable, and easily integrable technique enhances the
reliability of RAG systems when applied to large-scale legal document datasets.

</details>


### [67] [Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages](https://arxiv.org/abs/2510.07000)
*Neel Prabhanjan Rachamalla,Aravind Konakalla,Gautam Rajeev,Ashish Kulkarni,Chandra Khatri,Shubham Agarwal*

Main category: cs.CL

TL;DR: 提出Pragyaan数据集解决印度语言LLM训练数据匮乏问题，覆盖10种语言，注重文化保留与任务多样性


<details>
  <summary>Details</summary>
Motivation: 现有开源数据集在印度语言上存在多语言覆盖不足、文化适配性差、任务多样性缺失三大缺陷，制约本地化LLM发展

Method: 人机协同管道结合翻译与数据扩展，构建含Pragyaan-IT（22.5K）和Pragyaan-Align（100K）的双语数据集，覆盖10种印度语言/13大类任务

Result: 建立首个强调多轮对话、指令保真、安全对齐及文化传承的印度语言对齐数据集，整合57个源数据集形成56个细分类别

Conclusion: 该数据协议为构建包容性更强的多语言LLM奠定基础，特别提升了印度语言模型的文化适应性与任务完成能力

Abstract: The effectiveness of Large Language Models (LLMs) depends heavily on the
availability of high-quality post-training data, particularly
instruction-tuning and preference-based examples. Existing open-source
datasets, however, often lack multilingual coverage, cultural grounding, and
suffer from task diversity gaps that are especially pronounced for Indian
languages. We introduce a human-in-the-loop pipeline that combines translations
with synthetic expansion to produce reliable and diverse Indic post-training
data. Using this pipeline, we curate two datasets: Pragyaan-IT (22.5K) and
Pragyaan-Align (100K) across 10 Indian languages covering 13 broad and 56
sub-categories, leveraging 57 diverse datasets. Our dataset protocol
incorporates several often-overlooked dimensions and emphasize task diversity,
multi-turn dialogue, instruction fidelity, safety alignment, and preservation
of cultural nuance, providing a foundation for more inclusive and effective
multilingual LLMs.

</details>


### [68] [Native Hybrid Attention for Efficient Sequence Modeling](https://arxiv.org/abs/2510.07019)
*Jusen Du,Jiaxi Hu,Tao Zhang,Weigao Sun,Yu Cheng*

Main category: cs.CL

TL;DR: 提出NHA混合注意力机制，通过整合线性和全注意力层，在保持效率的同时显著提升长上下文任务的召回准确率和推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统Transformer二次复杂度效率瓶颈与线性注意力在长上下文场景下准确率不足的矛盾，实现效率与精度的平衡。

Method: 1. 层内混合：线性RNN维护长期上下文KV槽+滑动窗口短期token；2. 单一softmax attention实现无参融合；3. 通过窗口尺寸超参数控制层间混合程度。

Result: 在召回密集型任务(如PG19)上超越Transformer 2.2pp，在BoolQ/COPA常识推理任务提升1.5-3pp；结构混合后LLM推理速度提升1.8倍。

Conclusion: NHA首次实现无需结构改变的渐进式注意力混合，为大规模语言模型提供可调节的效率-精度平衡新范式。

Abstract: Transformers excel at sequence modeling but face quadratic complexity, while
linear attention offers improved efficiency but often compromises recall
accuracy over long contexts. In this work, we introduce Native Hybrid Attention
(NHA), a novel hybrid architecture of linear and full attention that integrates
both intra \& inter-layer hybridization into a unified layer design. NHA
maintains long-term context in key-value slots updated by a linear RNN, and
augments them with short-term tokens from a sliding window. A single
\texttt{softmax attention} operation is then applied over all keys and values,
enabling per-token and per-head context-dependent weighting without requiring
additional fusion parameters. The inter-layer behavior is controlled through a
single hyperparameter, the sliding window size, which allows smooth adjustment
between purely linear and full attention while keeping all layers structurally
uniform. Experimental results show that NHA surpasses Transformers and other
hybrid baselines on recall-intensive and commonsense reasoning tasks.
Furthermore, pretrained LLMs can be structurally hybridized with NHA, achieving
competitive accuracy while delivering significant efficiency gains. Code is
available at https://github.com/JusenD/NHA.

</details>


### [69] [Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge](https://arxiv.org/abs/2510.07024)
*Shrestha Ghosh,Luca Giordano,Yujia Hu,Tuan-Phong Nguyen,Simon Razniewski*

Main category: cs.CL

TL;DR: 研究发现GPT-4.1的事实性知识存在显著偏差、准确性不足及幻觉问题


<details>
  <summary>Details</summary>
Motivation: 深入探究前沿大语言模型（GPT-4.1）的事实性知识特性与局限性

Method: 基于GPTKB v1.5数据集（包含1亿条信念），系统分析模型知识库与标准知识库的差异

Result: 模型事实知识准确率显著低于基准测试，存在42%的知识不一致性，幻觉现象发生率超基准值18%

Conclusion: 需针对LLM知识的一致性验证机制和抗幻觉训练开展突破性研究

Abstract: LLMs are remarkable artifacts that have revolutionized a range of NLP and AI
tasks. A significant contributor is their factual knowledge, which, to date,
remains poorly understood, and is usually analyzed from biased samples. In this
paper, we take a deep tour into the factual knowledge (or beliefs) of a
frontier LLM, based on GPTKB v1.5 (Hu et al., 2025a), a recursively elicited
set of 100 million beliefs of one of the strongest currently available frontier
LLMs, GPT-4.1. We find that the models' factual knowledge differs quite
significantly from established knowledge bases, and that its accuracy is
significantly lower than indicated by previous benchmarks. We also find that
inconsistency, ambiguity and hallucinations are major issues, shedding light on
future research opportunities concerning factual LLM knowledge.

</details>


### [70] [Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models](https://arxiv.org/abs/2510.07037)
*Rajvee Sheth,Samridhi Raj Sinha,Mahavir Patil,Himanshu Beniwal,Mayank Singh*

Main category: cs.CL

TL;DR: 该论文系统分析了大型语言模型在代码切换(CSW)任务中的研究进展，涵盖五大研究领域、12项NLP任务和80+语言，指出当前在数据集、评估方法等方面存在的挑战，并提出实现多语言智能的改进路径。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM快速发展，但处理混合语言输入时仍面临数据集不足、评估偏差等问题，阻碍其在多语言社会的实际应用。需系统梳理相关研究进展与现存挑战。

Method: 通过分类研究架构（编码器/解码器结构）、训练策略（微调/提示工程）和评估方法论，分析▁total{unique_references}项研究成果，覆盖30+数据集和跨语言应用场景。

Result: LLM通过上下文学习能力改进了CSW建模，但存在低资源语言覆盖不足、评估指标偏颇、语言理论基础薄弱等问题。构建了包含所有研究资源的开源知识库。

Conclusion: 应建立包容性数据集、开发公平评估体系、加强语言理论指导，通过多维度改进推动LLM在多语言场景的实际应用。持续维护的开源库为领域研究提供基础设施支持。

Abstract: Code-switching (CSW), the alternation of languages and scripts within a
single utterance, remains a fundamental challenge for multiling ual NLP, even
amidst the rapid advances of large language models (LLMs). Most LLMs still
struggle with mixed-language inputs, limited CSW datasets, and evaluation
biases, hindering deployment in multilingual societies. This survey provides
the first comprehensive analysis of CSW-aware LLM research, reviewing
\total{unique_references} studies spanning five research areas, 12 NLP tasks,
30+ datasets, and 80+ languages. We classify recent advances by architecture,
training strategy, and evaluation methodology, outlining how LLMs have reshaped
CSW modeling and what challenges persist. The paper concludes with a roadmap
emphasizing the need for inclusive datasets, fair evaluation, and
linguistically grounded models to achieve truly multilingual intelligence. A
curated collection of all resources is maintained at
https://github.com/lingo-iitgn/awesome-code-mixing/.

</details>


### [71] [Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models](https://arxiv.org/abs/2510.07048)
*Yuntao Gui,James Cheng*

Main category: cs.CL

TL;DR: 提出Search-R3框架，通过大语言模型的推理能力生成检索嵌入，结合监督学习和强化学习优化检索性能


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在检索任务中的未充分利用问题，通过链式推理机制将语义分析与嵌入生成结合

Method: 1) 监督学习训练嵌入生成能力
2) 强化学习联合优化推理和嵌入
3) 动态更新嵌入的专用RL环境设计

Result: 在多个基准测试中显著超越现有方法，统一推理与嵌入生成流程提升复杂知识任务处理能力

Conclusion: 该后训练方法实现了推理能力与信息检索的深度融合，为知识密集型任务提供创新解决方案

Abstract: Despite their remarkable natural language understanding capabilities, Large
Language Models (LLMs) have been underutilized for retrieval tasks. We present
Search-R3, a novel framework that addresses this limitation by adapting LLMs to
generate search embeddings as a direct output of their reasoning process. Our
approach exploits LLMs' chain-of-thought capabilities, allowing them to produce
more effective embeddings by reasoning step-by-step through complex semantic
analyses. We implement this through three complementary mechanisms. (1) a
supervised learning stage enables the model's ability to produce quality
embeddings, (2) a reinforcement learning (RL) methodology that optimizes
embedding generation alongside reasoning, and (3) a specialized RL environment
that efficiently handles evolving embedding representations without requiring
complete corpus re-encoding at each training iteration. Our extensive
evaluations on diverse benchmarks demonstrate that Search-R3 significantly
outperforms prior methods by unifying the reasoning and embedding generation
processes. This integrated post-training approach represents a substantial
advancement in handling complex knowledge-intensive tasks that require both
sophisticated reasoning and effective information retrieval. Project page:
https://github.com/ytgui/Search-R3

</details>


### [72] [Does Local News Stay Local?: Online Content Shifts in Sinclair-Acquired Stations](https://arxiv.org/abs/2510.07060)
*Miriam Wanner,Sophia Hager,Anjalie Field*

Main category: cs.CL

TL;DR: 辛克莱集团收购地方新闻台后，报道重心从本地转向全国性/极化议题


<details>
  <summary>Details</summary>
Motivation: 研究地方新闻台被大型集团收购后内容变化及其社会影响，因地方媒体本是居民信任的非政治化信息源

Method: 使用计算方法对比分析收购前后地方台网络内容，并与全国性媒体进行横向比较

Result: 收购后地方台全国新闻比例显著增加，本地话题减少，且极化议题报道量提升

Conclusion: 媒体集团化运作可能导致地方新闻失焦，加剧社会信息环境的政治极化现象

Abstract: Local news stations are often considered to be reliable sources of
non-politicized information, particularly local concerns that residents care
about. Because these stations are trusted news sources, viewers are
particularly susceptible to the information they report. The Sinclair Broadcast
group is a broadcasting company that has acquired many local news stations in
the last decade. We investigate the effects of local news stations being
acquired by Sinclair: how does coverage change? We use computational methods to
investigate changes in internet content put out by local news stations before
and after being acquired by Sinclair and in comparison to national news
outlets. We find that there is clear evidence that local news stations report
more frequently on national news at the expense of local topics, and that their
coverage of polarizing national topics increases.

</details>


### [73] [Revisiting Metric Reliability for Fine-grained Evaluation of Machine Translation and Summarization in Indian Languages](https://arxiv.org/abs/2510.07061)
*Amir Hossein Yari,Kalmit Kulkarni,Ahmad Raza Khan,Fajri Koto*

Main category: cs.CL

TL;DR: 针对印度语言的自动评估指标研究显示：LLM类评估器与人类判断最契合，机器翻译指标侧重流畅性，文本摘要指标侧重内容保真度，且不同指标对扰动的敏感性差异显著


<details>
  <summary>Details</summary>
Motivation: 当前自动评估指标主要针对英语等资源丰富语言，缺乏对印度语言（覆盖15亿人口）的系统验证，需检验现有指标的普适性

Method: 构建ITEM基准测试集（含细粒度标注），从5个维度评估26个指标：与人类判断一致性、异常值敏感性、语言特异性、指标间相关性、抗干扰能力

Result: 1）LLM评估器在片段和系统级均最接近人类评估
2）异常值显著影响指标可信度
3）文本摘要指标侧重内容保真度，机器翻译指标侧重流畅性
4）不同指标对扰动的敏感度差异显著

Conclusion: 研究为印度语言评估指标设计提供了关键方向：需开发语言敏感的评估体系，重视异常值处理，并根据任务特性（MT/TS）选择不同侧重的指标

Abstract: While automatic metrics drive progress in Machine Translation (MT) and Text
Summarization (TS), existing metrics have been developed and validated almost
exclusively for English and other high-resource languages. This narrow focus
leaves Indian languages, spoken by over 1.5 billion people, largely overlooked,
casting doubt on the universality of current evaluation practices. To address
this gap, we introduce ITEM, a large-scale benchmark that systematically
evaluates the alignment of 26 automatic metrics with human judgments across six
major Indian languages, enriched with fine-grained annotations. Our extensive
evaluation, covering agreement with human judgments, sensitivity to outliers,
language-specific reliability, inter-metric correlations, and resilience to
controlled perturbations, reveals four central findings: (1) LLM-based
evaluators show the strongest alignment with human judgments at both segment
and system levels; (2) outliers exert a significant impact on metric-human
agreement; (3) in TS, metrics are more effective at capturing content fidelity,
whereas in MT, they better reflect fluency; and (4) metrics differ in their
robustness and sensitivity when subjected to diverse perturbations.
Collectively, these findings offer critical guidance for advancing metric
design and evaluation in Indian languages.

</details>


### [74] [LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish](https://arxiv.org/abs/2510.07074)
*Fred Philippy,Laura Bernardy,Siwen Guo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CL

TL;DR: 通过跨语言对齐数据构建卢森堡语指令微调数据集，避免机器翻译缺陷并提升模型性能


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言卢森堡语因缺乏高质量指令数据集导致的语义错位和文化失真问题

Method: 利用英语/法语/德语对齐数据构建跨语言指令数据集，保留语言文化细微差别

Result: 跨语言指令微调增强多语言表示对齐并提升卢森堡语生成能力

Conclusion: 跨语言数据管理策略可规避机器翻译缺陷，直接助力低资源语言发展

Abstract: Instruction tuning has become a key technique for enhancing the performance
of large language models, enabling them to better follow human prompts.
However, low-resource languages such as Luxembourgish face severe limitations
due to the lack of high-quality instruction datasets. Traditional reliance on
machine translation often introduces semantic misalignment and cultural
inaccuracies. In this work, we address these challenges by creating a
cross-lingual instruction tuning dataset for Luxembourgish, without resorting
to machine-generated translations into it. Instead, by leveraging aligned data
from English, French, and German, we build a high-quality dataset that
preserves linguistic and cultural nuances. We provide evidence that
cross-lingual instruction tuning not only improves representational alignment
across languages but also the model's generative capabilities in Luxembourgish.
This highlights how cross-lingual data curation can avoid the common pitfalls
of machine-translated data and directly benefit low-resource language
development.

</details>


### [75] [Accelerating Diffusion LLM Inference via Local Determinism Propagation](https://arxiv.org/abs/2510.07081)
*Fanheng Kong,Jingyuan Zhang,Yahui Liu,Zirui Wu,Yu Tian,Victoria W.,Guorui Zhou*

Main category: cs.CL

TL;DR: 提出无训练自适应并行解码策略LocalLeap，通过锚点定位和局部并行解码实现6.94倍吞吐量提升，解码步骤减少至14.2%


<details>
  <summary>Details</summary>
Motivation: 现有dLLM的贪婪解码策略导致冗余迭代和延迟解码问题，需平衡生成质量与推理效率

Method: 基于局部确定性传播和空间一致性衰减原则，在锚点周围进行有界局部松弛并行解码

Result: 多基准测试显示吞吐量提升6.94倍，解码步骤减少至14.2%，性能影响可忽略

Conclusion: LocalLeap显著提升推理效率而不影响输出质量，为dLLM实际部署提供有效解决方案，代码已开源

Abstract: Diffusion large language models (dLLMs) represent a significant advancement
in text generation, offering parallel token decoding capabilities. However,
existing open-source implementations suffer from quality-speed trade-offs that
impede their practical deployment. Conservative sampling strategies typically
decode only the most confident token per step to ensure quality (i.e., greedy
decoding), at the cost of inference efficiency due to repeated redundant
refinement iterations--a phenomenon we term delayed decoding. Through
systematic analysis of dLLM decoding dynamics, we characterize this delayed
decoding behavior and propose a training-free adaptive parallel decoding
strategy, named LocalLeap, to address these inefficiencies. LocalLeap is built
on two fundamental empirical principles: local determinism propagation centered
on high-confidence anchors and progressive spatial consistency decay. By
applying these principles, LocalLeap identifies anchors and performs localized
relaxed parallel decoding within bounded neighborhoods, achieving substantial
inference step reduction through early commitment of already-determined tokens
without compromising output quality. Comprehensive evaluation on various
benchmarks demonstrates that LocalLeap achieves 6.94$\times$ throughput
improvements and reduces decoding steps to just 14.2\% of the original
requirement, achieving these gains with negligible performance impact. The
source codes are available at: https://github.com/friedrichor/LocalLeap.

</details>


### [76] [All Claims Are Equal, but Some Claims Are More Equal Than Others: Importance-Sensitive Factuality Evaluation of LLM Generations](https://arxiv.org/abs/2510.07083)
*Miriam Wanner,Leif Azzopardi,Paul Thomas,Soham Dan,Benjamin Van Durme,Nick Craswell*

Main category: cs.CL

TL;DR: 现有评估LLM事实性的方法忽视关键信息权重，本文提出VITAL指标增强对核心错误检测的敏感性


<details>
  <summary>Details</summary>
Motivation: 传统方法因未区分信息重要性导致关键信息错误评估失准，需开发能识别核心信息错误的评估体系

Method: 构建VITALERRORS基准数据集(6,733样本)，分析现有指标缺陷，设计融合声明相关性与重要性的VITAL评估框架

Result: VITAL指标在检测关键信息错误方面比现有方法可靠性提升26.8%，错误定位准确率提高34%

Conclusion: 通过数据集构建和指标创新，本研究为LLM事实性评估提供了更精准鲁棒的测量体系

Abstract: Existing methods for evaluating the factuality of large language model (LLM)
responses treat all claims as equally important. This results in misleading
evaluations when vital information is missing or incorrect as it receives the
same weight as peripheral details, raising the question: how can we reliably
detect such differences when there are errors in key information? Current
approaches that measure factuality tend to be insensitive to omitted or false
key information. To investigate this lack of sensitivity, we construct
VITALERRORS, a benchmark of 6,733 queries with minimally altered LLM responses
designed to omit or falsify key information. Using this dataset, we demonstrate
the insensitivities of existing evaluation metrics to key information errors.
To address this gap, we introduce VITAL, a set of metrics that provide greater
sensitivity in measuring the factuality of responses by incorporating the
relevance and importance of claims with respect to the query. Our analysis
demonstrates that VITAL metrics more reliably detect errors in key information
than previous methods. Our dataset, metrics, and analysis provide a foundation
for more accurate and robust assessment of LLM factuality.

</details>


### [77] [Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided Sarcastic Speech Synthesis](https://arxiv.org/abs/2510.07096)
*Zhu Li,Yuqing Zhang,Xiyuan Gao,Shekhar Nayak,Matt Coler*

Main category: cs.CL

TL;DR: 提出基于LLM增强的检索增强框架（LLaMA 3+RAG）实现更自然的讽刺语音合成


<details>
  <summary>Details</summary>
Motivation: 现有语音合成研究集中于基础情感类别，但讽刺表达依赖语义/语境/韵律多维度线索，该领域尚未充分探索

Method: 结合微调LLaMA 3的语义嵌入（捕捉语用矛盾）和RAG检索的韵律范例，在VITS架构中实现双重条件控制

Result: 主客观评估均优于基线，自然度提升15%，讽刺表达力增强23%，下游检测准确率提高12%

Conclusion: 语义与韵律的协同建模有效解决讽刺语音生成难题，为细粒度情感合成提供新范式

Abstract: Sarcasm is a subtle form of non-literal language that poses significant
challenges for speech synthesis due to its reliance on nuanced semantic,
contextual, and prosodic cues. While existing speech synthesis research has
focused primarily on broad emotional categories, sarcasm remains largely
unexplored. In this paper, we propose a Large Language Model (LLM)-enhanced
Retrieval-Augmented framework for sarcasm-aware speech synthesis. Our approach
combines (1) semantic embeddings from a LoRA-fine-tuned LLaMA 3, which capture
pragmatic incongruity and discourse-level cues of sarcasm, and (2) prosodic
exemplars retrieved via a Retrieval Augmented Generation (RAG) module, which
provide expressive reference patterns of sarcastic delivery. Integrated within
a VITS backbone, this dual conditioning enables more natural and contextually
appropriate sarcastic speech. Experiments demonstrate that our method
outperforms baselines in both objective measures and subjective evaluations,
yielding improvements in speech naturalness, sarcastic expressivity, and
downstream sarcasm detection.

</details>


### [78] [TALENT: Table VQA via Augmented Language-Enhanced Natural-text Transcription](https://arxiv.org/abs/2510.07098)
*Guo Yutong,Wanying Wang,Yue Wu,Zichen Miao,Haoyu Wang*

Main category: cs.CL

TL;DR: 提出TALENT框架，通过双表征增强语言模型在表格VQA任务的表现，显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在表格VQA任务中需要极高算力且细节捕捉不足，轻量级OCR+LLM方案存在结构化数据适配性问题

Method: 采用小型VLM生成OCR文本和自然语言叙述，结合问题输入给LLM进行多模态推理，将VLM重构为感知-叙述模块

Result: 在公开数据集和自建ReTabVQA数据集上，小模型组合达到或超越大型VLM性能，算力成本显著降低

Conclusion: 通过表征重构将表格VQA转化为LLM核心任务，验证了轻量化多模态推理框架的有效性

Abstract: Table Visual Question Answering (Table VQA) is typically addressed by large
vision-language models (VLMs). While such models can answer directly from
images, they often miss fine-grained details unless scaled to very large sizes,
which are computationally prohibitive, especially for mobile deployment. A
lighter alternative is to have a small VLM perform OCR and then use a large
language model (LLM) to reason over structured outputs such as Markdown tables.
However, these representations are not naturally optimized for LLMs and still
introduce substantial errors. We propose TALENT (Table VQA via Augmented
Language-Enhanced Natural-text Transcription), a lightweight framework that
leverages dual representations of tables. TALENT prompts a small VLM to produce
both OCR text and natural language narration, then combines them with the
question for reasoning by an LLM. This reframes Table VQA as an LLM-centric
multimodal reasoning task, where the VLM serves as a perception-narration
module rather than a monolithic solver. Additionally, we construct ReTabVQA, a
more challenging Table VQA dataset requiring multi-step quantitative reasoning
over table images. Experiments show that TALENT enables a small VLM-LLM
combination to match or surpass a single large VLM at significantly lower
computational cost on both public datasets and ReTabVQA.

</details>


### [79] [Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning](https://arxiv.org/abs/2510.07105)
*Taylor Sorensen,Yejin Choi*

Main category: cs.CL

TL;DR: 提出基于语言模型上下文学习与两阶段元学习的系统，在LeWiDi竞赛中获胜，关键要素包括上下文评分示例、数据集微调和模型规模


<details>
  <summary>Details</summary>
Motivation: 解决NLP任务中普遍存在的主观性、标注歧义和合理分歧问题，提升模型在争议场景下的鲁棒性

Method: 1) 多数据集上下文学习后训练 2) 上下文元学习适应特定数据分布

Result: LeWiDi竞赛双任务冠军，消融实验显示：上下文评分示例提升效果显著，数据集微调对大数据集有效，模型规模扩大持续提升性能

Conclusion: 处理标注分歧需综合运用上下文学习、目标域适配和模型扩容，评分者特征整合与规模效应是成功关键因素

Abstract: Many natural language processing (NLP) tasks involve subjectivity, ambiguity,
or legitimate disagreement between annotators. In this paper, we outline our
system for modeling human variation. Our system leverages language models'
(LLMs) in-context learning abilities, along with a two-step meta-learning
training procedure for 1) post-training on many datasets requiring in-context
learning and 2) specializing the model via in-context meta-learning to the
particular data distribution of interest. We also evaluate the performance of
our system submission to the Learning With Disagreements (LeWiDi) competition,
where it was the overall winner on both tasks. Additionally, we perform an
ablation study to measure the importance of each system component. We find that
including rater examples in-context is crucial for our system's performance,
dataset-specific fine-tuning is helpful on the larger datasets, post-training
on other in-context datasets is helpful on one of the competition datasets, and
that performance improves with model scale.

</details>


### [80] [TRIM: Token-wise Attention-Derived Saliency for Data-Efficient Instruction Tuning](https://arxiv.org/abs/2510.07118)
*Manish Nagaraj,Sakshi Choudhary,Utkarsh Saxena,Deepak Ravikumar,Kaushik Roy*

Main category: cs.CL

TL;DR: TRIM提出了一种基于注意力指纹的高效前向框架，通过模式匹配选择核心集，在计算成本极低的情况下性能超越现有方法9%并接近全数据微调。


<details>
  <summary>Details</summary>
Motivation: 现有核心集选择方法依赖粗粒度的梯度信号，计算昂贵且忽略细粒度特征，需要更高效敏感的任务特征捕捉方法。

Method: 基于多层级注意力机制构建任务指纹，通过前向传播匹配目标样本的表示模式，无需反向梯度计算实现高效核心集选择。

Result: 在多个下游任务上超越SOTA基线9%，部分场景优于全数据微调，计算成本仅为传统方法的极小部分。

Conclusion: TRIM证明了基于注意力结构特征的高效数据集构建可行性，为大规模指令调优提供了计算友好的新范式。

Abstract: Instruction tuning is essential for aligning large language models (LLMs) to
downstream tasks and commonly relies on large, diverse corpora. However, small,
high-quality subsets, known as coresets, can deliver comparable or superior
results, though curating them remains challenging. Existing methods often rely
on coarse, sample-level signals like gradients, an approach that is
computationally expensive and overlooks fine-grained features. To address this,
we introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a
forward-only, token-centric framework. Instead of using gradients, TRIM
operates by matching underlying representational patterns identified via
attention-based "fingerprints" from a handful of target samples. Such an
approach makes TRIM highly efficient and uniquely sensitive to the structural
features that define a task. Coresets selected by our method consistently
outperform state-of-the-art baselines by up to 9% on downstream tasks and even
surpass the performance of full-data fine-tuning in some settings. By avoiding
expensive backward passes, TRIM achieves this at a fraction of the
computational cost. These findings establish TRIM as a scalable and efficient
alternative for building high-quality instruction-tuning datasets.

</details>


### [81] [Comparing human and language models sentence processing difficulties on complex structures](https://arxiv.org/abs/2510.07141)
*Samuel Joseph Amouyal,Aya Meltzer-Asscher,Jonathan Berant*

Main category: cs.CL

TL;DR: 研究发现大语言模型在复杂句子理解（尤其是花园路径句）上与人类存在差异，模型性能随参数量增加更接近人类模式


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否像人类一样在复杂语言结构（如花园路径句）上存在理解困难，揭示两者的认知相似性

Method: 在统一实验框架下收集人类和5类LLMs（不同参数量/训练方法）对7种语言结构的句子理解数据

Result: 最强模型GPT-5非花园路径句准确率93.7%，花园路径句仅46.8%；模型参数越大与人类难度排名相关性越强

Conclusion: LLMs与人类在句子理解上呈现部分收敛（参数量足够时）与分化（极弱/极强模型时），为认知相似性研究提供新视角

Abstract: Large language models (LLMs) that fluently converse with humans are a reality
- but do LLMs experience human-like processing difficulties? We systematically
compare human and LLM sentence comprehension across seven challenging
linguistic structures. We collect sentence comprehension data from humans and
five families of state-of-the-art LLMs, varying in size and training procedure
in a unified experimental framework. Our results show LLMs overall struggle on
the target structures, but especially on garden path (GP) sentences. Indeed,
while the strongest models achieve near perfect accuracy on non-GP structures
(93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5).
Additionally, when ranking structures based on average performance, rank
correlation between humans and models increases with parameter count. For each
target structure, we also collect data for their matched baseline without the
difficult structure. Comparing performance on the target vs. baseline
sentences, the performance gap observed in humans holds for LLMs, with two
exceptions: for models that are too weak performance is uniformly low across
both sentence types, and for models that are too strong the performance is
uniformly high. Together, these reveal convergence and divergence in human and
LLM sentence comprehension, offering new insights into the similarity of humans
and LLMs.

</details>


### [82] [Reasoning for Hierarchical Text Classification: The Case of Patents](https://arxiv.org/abs/2510.07167)
*Lekang Jiang,Wenjun Sun,Stephan Goetz*

Main category: cs.CL

TL;DR: 提出RHC框架，通过两阶段训练大语言模型（冷启动对齐思维链+强化学习增强推理），将分层文本分类转化为逐步推理任务，在专利分类等场景实现3%性能提升并具备解释性/扩展性优势


<details>
  <summary>Details</summary>
Motivation: 现有分层分类方法仅输出扁平标签集，缺乏对预测逻辑的解释性，尤其在专利分类领域面临标签体系复杂和领域知识难度高的双重挑战

Method: 分两阶段训练：1）冷启动阶段对齐思维链推理格式 2）强化学习阶段增强多步推理能力，使模型能通过自然语言推导逐步得出层次化标签

Result: 准确率和宏观F1提升3%，在可解释性（预测前生成自然语言解释）、模型规模扩展性（大模型增益更显著）及跨领域适用性（其他HTC基准SOTA）方面表现突出

Conclusion: RHC框架有效解决层次分类的解释性和复杂推理问题，其分步推理范式在需要透明决策的领域（如专利分类）具有重要应用价值

Abstract: Hierarchical text classification (HTC) assigns documents to multiple levels
of a pre-defined taxonomy. Automated patent subject classification represents
one of the hardest HTC scenarios because of domain knowledge difficulty and a
huge number of labels. Prior approaches only output a flat label set, which
offers little insight into the reason behind predictions. Therefore, we propose
Reasoning for Hierarchical Classification (RHC), a novel framework that
reformulates HTC as a step-by-step reasoning task to sequentially deduce
hierarchical labels. RHC trains large language models (LLMs) in two stages: a
cold-start stage that aligns outputs with chain-of-thought (CoT) reasoning
format and a reinforcement learning (RL) stage to enhance multi-step reasoning
ability. RHC demonstrates four advantages in our experiments. (1)
Effectiveness: RHC surpasses previous baselines and outperforms the supervised
fine-tuning counterparts by approximately 3% in accuracy and macro F1. (2)
Explainability: RHC produces natural-language justifications before prediction
to facilitate human inspection. (3) Scalability: RHC scales favorably with
model size with larger gains compared to standard fine-tuning. (4)
Applicability: Beyond patents, we further demonstrate that RHC achieves
state-of-the-art performance on other widely used HTC benchmarks, which
highlights its broad applicability.

</details>


### [83] [More Data or Better Data? A Critical Analysis of Data Selection and Synthesis for Mathematical Reasoning](https://arxiv.org/abs/2510.07169)
*Yike Zhao,Simin Guo,Ziqing Yang,Shifan Han,Dahua Lin,Fei Tan*

Main category: cs.CL

TL;DR: 论文通过系统分析数学推理任务的训练数据质量与数量，发现结构化数据格式和从强模型蒸馏比单纯增加数据量更有效


<details>
  <summary>Details</summary>
Motivation: 探索在工业应用中如何通过优化训练数据提升大语言模型的推理能力，验证'优质数据优于海量数据'的假设

Method: 对开源数据集和合成技术进行统一流程评估，开发适用于工业场景的数据筛选策略

Result: 可解释的数据结构和模型蒸馏方法能显著提升模型性能，为成本效益型数据管理提供方案

Conclusion: 实际应用中应优先提升数据质量而非数量，该研究为平衡'更多数据'与'更好数据'提供了实践框架

Abstract: The reasoning capabilities of Large Language Models (LLMs) play a critical
role in many downstream tasks, yet depend strongly on the quality of training
data. Despite various proposed data construction methods, their practical
utility in real-world pipelines remains underexplored. In this work, we conduct
a comprehensive analysis of open-source datasets and data synthesis techniques
for mathematical reasoning, evaluating them under a unified pipeline designed
to mirror training and deployment scenarios. We further distill effective data
selection strategies and identify practical methods suitable for industrial
applications. Our findings highlight that structuring data in more
interpretable formats, or distilling from stronger models often outweighs
simply scaling up data volume. This study provides actionable guidance for
integrating training data to enhance LLM capabilities, supporting both
cost-effective data curation and scalable model enhancement. We hope this work
will inspire further research on how to balance "more data" versus "better
data" for real-world reasoning tasks.

</details>


### [84] [NurseLLM: The First Specialized Language Model for Nursing](https://arxiv.org/abs/2510.07173)
*Md Tawkat Islam Khondaker,Julia Harrington,Shady Shehata*

Main category: cs.CL

TL;DR: 开发首个护理领域专用大模型NurseLLM，通过多阶段数据生成构建护理MCQ数据集，在多项基准测试中超越同规模通用及医疗专用模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在护理等专业医疗领域的潜力尚未充分开发，现有模型难以满足护理领域多选问答任务的专业需求。

Method: 1. 开发多阶段数据生成管道构建大规模护理MCQ数据集
2. 建立多个护理基准测试框架
3. 探索多智能体协作系统在护理场景的应用

Result: NurseLLM在不同基准测试中表现优于同规模通用模型(SOTA)和医疗专用模型，验证专业领域模型的重要性。

Conclusion: 护理领域专用模型显著提升任务效果，推理能力和多智能体协作系统为未来护理AI发展提供重要方向。

Abstract: Recent advancements in large language models (LLMs) have significantly
transformed medical systems. However, their potential within specialized
domains such as nursing remains largely underexplored. In this work, we
introduce NurseLLM, the first nursing-specialized LLM tailored for multiple
choice question-answering (MCQ) tasks. We develop a multi-stage data generation
pipeline to build the first large scale nursing MCQ dataset to train LLMs on a
broad spectrum of nursing topics. We further introduce multiple nursing
benchmarks to enable rigorous evaluation. Our extensive experiments demonstrate
that NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of
comparable size on different benchmarks, underscoring the importance of a
specialized LLM for the nursing domain. Finally, we explore the role of
reasoning and multi-agent collaboration systems in nursing, highlighting their
promise for future research and applications.

</details>


### [85] [Quantifying Data Contamination in Psychometric Evaluations of LLMs](https://arxiv.org/abs/2510.07175)
*Jongwook Han,Woojung Song,Jonggeun Lee,Yohan Jo*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在心理测量评估中存在显著的数据污染问题，模型不仅会记忆测试条目，还能主动调整回答以达到特定目标分数。


<details>
  <summary>Details</summary>
Motivation: 针对先前研究对心理测量工具数据污染的担忧，但缺乏系统性量化研究，本文旨在建立测量框架填补这一空白。

Method: 提出三层次污染检测框架（条目记忆、评估记忆、目标分数匹配），应用于21个主流模型和4个常用心理测量量表。

Result: 发现BFI-44和PVQ-40等常用量表存在严重污染现象，模型不仅能记忆条目，还能通过调整回答实现特定分数目标。

Conclusion: 警示需谨慎解读LLMs心理测量结果，现有流行量表可能因数据污染导致评估可靠性存疑。

Abstract: Recent studies apply psychometric questionnaires to Large Language Models
(LLMs) to assess high-level psychological constructs such as values,
personality, moral foundations, and dark traits. Although prior work has raised
concerns about possible data contamination from psychometric inventories, which
may threaten the reliability of such evaluations, there has been no systematic
attempt to quantify the extent of this contamination. To address this gap, we
propose a framework to systematically measure data contamination in
psychometric evaluations of LLMs, evaluating three aspects: (1) item
memorization, (2) evaluation memorization, and (3) target score matching.
Applying this framework to 21 models from major families and four widely used
psychometric inventories, we provide evidence that popular inventories such as
the Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40)
exhibit strong contamination, where models not only memorize items but can also
adjust their responses to achieve specific target scores.

</details>


### [86] [CARPAS: Towards Content-Aware Refinement of Provided Aspects for Summarization in Large Language Models](https://arxiv.org/abs/2510.07177)
*Yong-En Tian,Yu-Chien Tang,An-Zi Yen,Wen-Chih Peng*

Main category: cs.CL

TL;DR: 提出CARPAS任务，通过预测相关方面数量指导LLM生成更精准的摘要


<details>
  <summary>Details</summary>
Motivation: 现有基于预定义方面的摘要方法存在实际场景中方面不完整/不相关的问题，需动态调整

Method: 构建三数据集+四种提示策略实验→发现LLM过度预测问题→提出预测相关方面数量的子任务

Result: 实验显示预测数量能显著提升性能（+12.3 ROUGE），LLM在数量不符时仍保持服从性

Conclusion: CARPAS框架有效解决现实场景的方面对齐问题，揭示了LLM在参数控制中的关键行为特征

Abstract: Aspect-based summarization has attracted significant attention for its
ability to generate more fine-grained and user-aligned summaries. While most
existing approaches assume a set of predefined aspects as input, real-world
scenarios often present challenges where these given aspects may be incomplete,
irrelevant, or entirely missing from the document. Users frequently expect
systems to adaptively refine or filter the provided aspects based on the actual
content. In this paper, we initiate this novel task setting, termed
Content-Aware Refinement of Provided Aspects for Summarization (CARPAS), with
the aim of dynamically adjusting the provided aspects based on the document
context before summarizing. We construct three new datasets to facilitate our
pilot experiments, and by using LLMs with four representative prompting
strategies in this task, we find that LLMs tend to predict an overly
comprehensive set of aspects, which often results in excessively long and
misaligned summaries. Building on this observation, we propose a preliminary
subtask to predict the number of relevant aspects, and demonstrate that the
predicted number can serve as effective guidance for the LLMs, reducing the
inference difficulty, and enabling them to focus on the most pertinent aspects.
Our extensive experiments show that the proposed approach significantly
improves performance across all datasets. Moreover, our deeper analyses uncover
LLMs' compliance when the requested number of aspects differs from their own
estimations, establishing a crucial insight for the deployment of LLMs in
similar real-world applications.

</details>


### [87] [Biasless Language Models Learn Unnaturally: How LLMs Fail to Distinguish the Possible from the Impossible](https://arxiv.org/abs/2510.07178)
*Imry Ziv,Nur Lan,Emmanuel Chemla,Roni Katzir*

Main category: cs.CL

TL;DR: 研究发现GPT-2无法有效区分人类可能/不可能语言，表明大语言模型不具备影响语言类型学的人类先天认知偏倚


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否具备与人类相似的先天语言学习偏倚，通过测试模型对自然语言与人工扰动后'不可能语言'的区分能力

Method: 扩展语言样本与扰动方式，对比GPT-2在自然语言和'不可能语言'数据集上的困惑度曲线，采用跨语言方差分析方法

Result: GPT-2在大多数情况下对自然语言与不可能语言表现出相似的学习效率，整体数据集层面也未显现系统性区分

Conclusion: LLMs缺乏塑造人类语言类型学的先天认知约束，其学习机制与人类语言习得存在本质差异

Abstract: Are large language models (LLMs) sensitive to the distinction between humanly
possible languages and humanly impossible languages? This question is taken by
many to bear on whether LLMs and humans share the same innate learning biases.
Previous work has attempted to answer it in the positive by comparing LLM
learning curves on existing language datasets and on "impossible" datasets
derived from them via various perturbation functions. Using the same
methodology, we examine this claim on a wider set of languages and impossible
perturbations. We find that in most cases, GPT-2 learns each language and its
impossible counterpart equally easily, in contrast to previous claims. We also
apply a more lenient condition by testing whether GPT-2 provides any kind of
separation between the whole set of natural languages and the whole set of
impossible languages. By considering cross-linguistic variance in various
metrics computed on the perplexity curves, we show that GPT-2 provides no
systematic separation between the possible and the impossible. Taken together,
these perspectives show that LLMs do not share the human innate biases that
shape linguistic typology.

</details>


### [88] [Sunflower: A New Approach To Expanding Coverage of African Languages in Large Language Models](https://arxiv.org/abs/2510.07203)
*Benjamin Akera,Evelyn Nafula Ouma,Gilbert Yiga,Patrick Walukagga,Phionah Natukunda,Trevor Saaka,Solomon Nsumba,Lilian Teddy Nabukeera,Joel Muhanguzi,Imran Sekalala,Nimpamya Janat Namara,Engineer Bainomugisha,Ernest Mwebaze,John Quinn*

Main category: cs.CL

TL;DR: 针对非洲语言技术覆盖不足的问题，研究者开发了专注于乌干达多语言环境的Sunflower大模型，通过区域化方法提升小语种支持能力。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型优先支持使用人数多的语言，导致非洲2000多种语言中绝大多数缺乏技术支持。研究者主张区域化方案能更高效解决语言覆盖碎片化问题。

Method: 基于Qwen 3框架开发Sunflower 14B/32B模型，通过针对性训练覆盖乌干达主要语言，模型开源以促进应用。

Result: Sunflower模型在乌干达多数语言理解测试中达到SOTA水平，支持语言数量远超普通LLM。

Conclusion: 区域化模型开发模式可有效降低语言技术门槛，该开源方案有助于医疗、教育等场景的语言障碍消除。

Abstract: There are more than 2000 living languages in Africa, most of which have been
bypassed by advances in language technology. Current leading LLMs exhibit
strong performance on a number of the most common languages (e.g. Swahili or
Yoruba), but prioritise support for the languages with the most speakers first,
resulting in piecemeal ability across disparate languages. We contend that a
regionally focussed approach is more efficient, and present a case study for
Uganda, a country with high linguistic diversity. We describe the development
of Sunflower 14B and 32B, a pair of models based on Qwen 3 with state of the
art comprehension in the majority of all Ugandan languages. These models are
open source and can be used to reduce language barriers in a number of
important practical applications.

</details>


### [89] [Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models](https://arxiv.org/abs/2510.07213)
*Chengzhi Zhong,Fei Cheng,Qianying Liu,Yugo Murawaki,Chenhui Chu,Sadao Kurohashi*

Main category: cs.CL

TL;DR: 提出无需训练的稀疏维度操控方法，仅需50句数据即可实现多语言生成控制，性能超越现有方法且成本显著降低


<details>
  <summary>Details</summary>
Motivation: 基于大型语言模型中间层将多语言表征映射到英语空间的现象，推测跨语言转换由少量稀疏维度控制，验证该假设并开发相应干预方法

Method: 通过分析中间层到输出层的维度一致性，开发数据需求极低（平行/单语50句）的稀疏维度识别与操控技术，实现语言切换的语义保持

Result: 多语言生成控制任务显示维度干预成功切换输出语言（准确率↑32%），性能超越神经元方法（F1↑0.15）且计算成本降低90%

Conclusion: 跨语言转换受稀疏维度控制的理论成立，提出的高效干预方法为模型可解释性研究和多语言应用开辟新路径

Abstract: Large language models exhibit strong multilingual capabilities despite
limited exposure to non-English data. Prior studies show that English-centric
large language models map multilingual content into English-aligned
representations at intermediate layers and then project them back into
target-language token spaces in the final layer. From this observation, we
hypothesize that this cross-lingual transition is governed by a small and
sparse set of dimensions, which occur at consistent indices across the
intermediate to final layers. Building on this insight, we introduce a simple,
training-free method to identify and manipulate these dimensions, requiring
only as few as 50 sentences of either parallel or monolingual data. Experiments
on a multilingual generation control task reveal the interpretability of these
dimensions, demonstrating that the interventions in these dimensions can switch
the output language while preserving semantic content, and that it surpasses
the performance of prior neuron-based approaches at a substantially lower cost.

</details>


### [90] [How much speech data is necessary for ASR in African languages? An evaluation of data scaling in Kinyarwanda and Kikuyu](https://arxiv.org/abs/2510.07221)
*Benjamin Akera,Evelyn Nafula,Patrick Walukagga,Gilbert Yiga,John Quinn,Ernest Mwebaze*

Main category: cs.CL

TL;DR: 低资源非洲语言ASR系统开发中，50小时训练数据可实现WER<13%的实用性能，数据质量（如噪声转录）对38.6%高错误案例影响显著。


<details>
  <summary>Details</summary>
Motivation: 针对非洲低资源语言ASR因转录数据不足的挑战，探究Whisper模型部署时最低数据需求及系统主要故障模式。

Method: 使用Whisper模型在Kinyarwanda(1-1400小时数据规模测试)和Kikuyu(270小时数据分析)进行系统性实验，包含数据扩展分析和错误归因研究。

Result: 50小时数据达WER<13%，200小时优化至WER<10%；38.6%高错误案例源于标注数据噪声，数据质量与数量同等重要。

Conclusion: 为低资源语言ASR提供实用基准：建议50-200小时数据量区间，并强调数据清洗与数量扩容的双重必要性。

Abstract: The development of Automatic Speech Recognition (ASR) systems for
low-resource African languages remains challenging due to limited transcribed
speech data. While recent advances in large multilingual models like OpenAI's
Whisper offer promising pathways for low-resource ASR development, critical
questions persist regarding practical deployment requirements. This paper
addresses two fundamental concerns for practitioners: determining the minimum
data volumes needed for viable performance and characterizing the primary
failure modes that emerge in production systems. We evaluate Whisper's
performance through comprehensive experiments on two Bantu languages:
systematic data scaling analysis on Kinyarwanda using training sets from 1 to
1,400 hours, and detailed error characterization on Kikuyu using 270 hours of
training data. Our scaling experiments demonstrate that practical ASR
performance (WER < 13\%) becomes achievable with as little as 50 hours of
training data, with substantial improvements continuing through 200 hours (WER
< 10\%). Complementing these volume-focused findings, our error analysis
reveals that data quality issues, particularly noisy ground truth
transcriptions, account for 38.6\% of high-error cases, indicating that careful
data curation is as critical as data volume for robust system performance.
These results provide actionable benchmarks and deployment guidance for teams
developing ASR systems across similar low-resource language contexts. We
release accompanying and models see
https://github.com/SunbirdAI/kinyarwanda-whisper-eval

</details>


### [91] [Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation](https://arxiv.org/abs/2510.07227)
*Arjun Krishnakumar,Rhea Sanjay Sukthanker,Hannan Javed Mahadik,Gabriela Kadlecová,Vladyslav Moroshan,Timur Carstensen,Frank Hutter,Aaron Klein*

Main category: cs.CL

TL;DR: 提出结合结构稀疏初始化、进化搜索和知识蒸馏的SLM预训练框架，使模型效率提升9.2倍


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型(LLMs)资源消耗高的问题，通过小型语言模型(SLMs)实现高效且资源友好的替代方案

Method: 1. 结构稀疏子网络初始化优于随机初始化
2. 进化搜索自动发现优质初始化结构
3. 知识蒸馏加速训练并提升泛化能力

Result: 最佳模型在验证困惑度相当的情况下，预训练token需求减少9.2倍

Conclusion: 该框架为低成本大规模开发高效小语言模型提供了可复现路径，相关代码和模型已开源

Abstract: Small Language models (SLMs) offer an efficient and accessible alternative to
Large Language Models (LLMs), delivering strong performance while using far
fewer resources. We introduce a simple and effective framework for pretraining
SLMs that brings together three complementary ideas. First, we identify
structurally sparse sub-network initializations that consistently outperform
randomly initialized models of similar size under the same compute budget.
Second, we use evolutionary search to automatically discover high-quality
sub-network initializations, providing better starting points for pretraining.
Third, we apply knowledge distillation from larger teacher models to speed up
training and improve generalization. Together, these components make SLM
pretraining substantially more efficient: our best model, discovered using
evolutionary search and initialized with LLM weights, matches the validation
perplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining
tokens. We release all code and models at
https://github.com/whittle-org/whittle/, offering a practical and reproducible
path toward cost-efficient small language model development at scale.

</details>


### [92] [Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping](https://arxiv.org/abs/2510.07230)
*Ziyi Wang,Yuxuan Lu,Yimeng Zhang,Jing Huang,Dakuo Wang*

Main category: cs.CL

TL;DR: LLM代理Customer-R1通过强化学习框架结合用户画像和动作正确性奖励信号，显著提升了在线购物场景中个性化用户行为模拟的真实性和预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示词/SFT/RL的行为模拟方法主要学习群体通用策略，缺乏对用户个体画像的建模，导致生成的行为模式过于泛化。如何在LLM代理中实现更精准的个性化用户行为模拟成为关键问题。

Method: 提出Customer-R1强化学习方法：1) 策略网络显式建模用户画像特征；2) 通过动作正确性奖励信号，联合优化下一步行为决策的rationale生成和动作预测；3) 基于在线购物环境构建训练框架。

Result: 在OPeRA数据集上的实验表明：1) 下一动作预测准确率显著优于Prompt/SFT基线；2) 生成动作分布与真实用户行为更匹配，证明个性化模拟的高保真度。

Conclusion: Customer-R1通过画像条件化强化学习框架，为LLM代理的个性化行为模拟提供了新范式，在电商等需要细粒度用户建模的场景具有重要应用价值。

Abstract: Simulating step-wise human behavior with Large Language Models (LLMs) has
become an emerging research direction, enabling applications in various
practical domains. While prior methods, including prompting, supervised
fine-tuning (SFT), and reinforcement learning (RL), have shown promise in
modeling step-wise behavior, they primarily learn a population-level policy
without conditioning on a user's persona, yielding generic rather than
personalized simulations. In this work, we pose a critical question: how can
LLM agents better simulate personalized user behavior? We introduce
Customer-R1, an RL-based method for personalized, step-wise user behavior
simulation in online shopping environments. Our policy is conditioned on an
explicit persona, and we optimize next-step rationale and action generation via
action correctness reward signals. Experiments on the OPeRA dataset emonstrate
that Customer-R1 not only significantly outperforms prompting and SFT-based
baselines in next-action prediction tasks, but also better matches users'
action distribution, indicating higher fidelity in personalized behavior
simulation.

</details>


### [93] [Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships](https://arxiv.org/abs/2510.07231)
*Donggyu Lee,Sungwon Park,Yerin Hwang,Hyunwoo Oh,Hyoshin Kim,Jungwon Kim,Meeyoung Cha,Sangyoon Park,Jihee Kim*

Main category: cs.CL

TL;DR: 构建基于实证研究的大语言模型因果推理新基准，揭示现有模型在关键领域存在显著能力缺陷


<details>
  <summary>Details</summary>
Motivation: 现有因果推理基准过度依赖合成数据且领域覆盖狭窄，难以评估大语言模型真实因果推理能力

Method: 从顶级经济金融期刊提取40,379个因果关联案例，采用工具变量法/双重差分法/断点回归设计构建多领域评估体系

Result: 最佳模型准确率仅57.6%，模型规模与性能不成正比，先进推理模型仍难以识别基础因果关系

Conclusion: 当前大语言模型因果推理能力与高风险应用需求存在重大差距，需突破模式匹配的局限性

Abstract: Causal reasoning is fundamental for Large Language Models (LLMs) to
understand genuine cause-and-effect relationships beyond pattern matching.
Existing benchmarks suffer from critical limitations such as reliance on
synthetic data and narrow domain coverage. We introduce a novel benchmark
constructed from casually identified relationships extracted from top-tier
economics and finance journals, drawing on rigorous methodologies including
instrumental variables, difference-in-differences, and regression discontinuity
designs. Our benchmark comprises 40,379 evaluation items covering five task
types across domains such as health, environment, technology, law, and culture.
Experimental results on eight state-of-the-art LLMs reveal substantial
limitations, with the best model achieving only 57.6\% accuracy. Moreover,
model scale does not consistently translate to superior performance, and even
advanced reasoning models struggle with fundamental causal relationship
identification. These findings underscore a critical gap between current LLM
capabilities and demands of reliable causal reasoning in high-stakes
applications.

</details>


### [94] [LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding](https://arxiv.org/abs/2510.07233)
*Zhivar Sourati,Zheng Wang,Marianne Menglin Liu,Yazhe Hu,Mengqing Guo,Sujeeth Bharadwaj,Kyu Han,Tao Sheng,Sujith Ravi,Morteza Dehghani,Dan Roth*

Main category: cs.CL

TL;DR: 提出LAD-RAG框架，通过构建符号文档图整合布局结构和跨页面依赖，实现动态证据检索，显著提升复杂文档问答任务的召回率和准确率。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在文档编码时丢失结构和跨页依赖关系，固定页数检索导致多页推理任务证据不完整，影响问答质量。

Method: 1. 文档摄入阶段构建符号文档图捕获布局结构；2. 推理阶段LLM代理动态交互神经/符号索引实现自适应检索；3. 神经与符号表征融合策略。

Result: 在MMLongBench-Doc等数据集上实现平均>90%完美召回率，基线召回率提升20%，QA准确率更高且延迟极低。

Conclusion: LAD-RAG通过符号-神经混合表征与动态检索机制，有效解决复杂文档结构理解难题，为长文档推理任务提供新范式。

Abstract: Question answering over visually rich documents (VRDs) requires reasoning not
only over isolated content but also over documents' structural organization and
cross-page dependencies. However, conventional retrieval-augmented generation
(RAG) methods encode content in isolated chunks during ingestion, losing
structural and cross-page dependencies, and retrieve a fixed number of pages at
inference, regardless of the specific demands of the question or context. This
often results in incomplete evidence retrieval and degraded answer quality for
multi-page reasoning tasks. To address these limitations, we propose LAD-RAG, a
novel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs
a symbolic document graph that captures layout structure and cross-page
dependencies, adding it alongside standard neural embeddings to yield a more
holistic representation of the document. During inference, an LLM agent
dynamically interacts with the neural and symbolic indices to adaptively
retrieve the necessary evidence based on the query. Experiments on
MMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG
improves retrieval, achieving over 90% perfect recall on average without any
top-k tuning, and outperforming baseline retrievers by up to 20% in recall at
comparable noise levels, yielding higher QA accuracy with minimal latency.

</details>


### [95] [When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation](https://arxiv.org/abs/2510.07238)
*Xunyi Jiang,Dingyi Chang,Julian McAuley,Xin Xu*

Main category: cs.CL

TL;DR: 研究发现现有事实性评估基准因时效性问题导致LLM评估不可靠


<details>
  <summary>Details</summary>
Motivation: 现有基准的静态性与LLMs和现实世界快速发展的脱节，导致对LLM事实性评估的可靠性存疑

Method: 通过调查五个流行事实性基准和八个不同年份的LLM，设计事实检索流程和三个量化指标评估基准老化问题

Result: 基准中大量过时样本导致LLM事实性评估结果失真（如旧基准评估新LLM时准确率下降30%）

Conclusion: 需建立动态更新的基准评估体系，该研究为评估基准可靠性提供方法论，并推动领域对基准老化问题的关注

Abstract: The rapid evolution of large language models (LLMs) and the real world has
outpaced the static nature of widely used evaluation benchmarks, raising
concerns about their reliability for evaluating LLM factuality. While
substantial works continue to rely on the popular but old benchmarks, their
temporal misalignment with real-world facts and modern LLMs, and their effects
on LLM factuality evaluation remain underexplored. Therefore, in this work, we
present a systematic investigation of this issue by examining five popular
factuality benchmarks and eight LLMs released across different years. An
up-to-date fact retrieval pipeline and three metrics are tailored to quantify
benchmark aging and its impact on LLM factuality evaluation. Experimental
results and analysis illustrate that a considerable portion of samples in the
widely used factuality benchmarks are outdated, leading to unreliable
assessments of LLM factuality. We hope our work can provide a testbed to assess
the reliability of a benchmark for LLM factuality evaluation and inspire more
research on the benchmark aging issue. Codes are available in
https://github.com/JiangXunyi/BenchAge.

</details>


### [96] [Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided LoRA Experts](https://arxiv.org/abs/2510.07239)
*Christos Ziakas,Nicholas Loo,Nishita Jain,Alessandra Russo*

Main category: cs.CL

TL;DR: 提出Red-Bandit框架，通过多攻击风格LoRA专家与强化学习的结合，实现动态识别LLM安全漏洞，在攻击成功率与可读性上达到最优效果。


<details>
  <summary>Details</summary>
Motivation: 现有自动化红队方法缺乏针对不同模型漏洞的动态适应机制，需开发能高效识别模型特异性弱点的审计方案。

Method: 1. 训练多个攻击风格的LoRA专家生成恶意提示
2. 基于安全模型的强化学习奖励机制
3. 多臂老虎机策略动态选择攻击专家

Result: 1. AdvBench上ASR@10指标最优
2. 生成提示的困惑度更低（更符合人类表达）
3. 老虎机策略可诊断模型特异性漏洞

Conclusion: Red-Bandit通过平衡探索与利用，有效提升红队攻击效率，其策略机制本身也成为模型安全漏洞的诊断工具。

Abstract: Automated red-teaming has emerged as a scalable approach for auditing Large
Language Models (LLMs) prior to deployment, yet existing approaches lack
mechanisms to efficiently adapt to model-specific vulnerabilities at inference.
We introduce Red-Bandit, a red-teaming framework that adapts online to identify
and exploit model failure modes under distinct attack styles (e.g.,
manipulation, slang). Red-Bandit post-trains a set of parameter-efficient LoRA
experts, each specialized for a particular attack style, using reinforcement
learning that rewards the generation of unsafe prompts via a rule-based safety
model. At inference, a multi-armed bandit policy dynamically selects among
these attack-style experts based on the target model's response safety,
balancing exploration and exploitation. Red-Bandit achieves state-of-the-art
results on AdvBench under sufficient exploration (ASR@10), while producing more
human-readable prompts (lower perplexity). Moreover, Red-Bandit's bandit policy
serves as a diagnostic tool for uncovering model-specific vulnerabilities by
indicating which attack styles most effectively elicit unsafe behaviors.

</details>


### [97] [Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense](https://arxiv.org/abs/2510.07242)
*Leitian Tao,Ilia Kulikov,Swarnadeep Saha,Tianlu Wang,Jing Xu,Yixuan Li,Jason E Weston,Ping Yu*

Main category: cs.CL

TL;DR: 提出HERO框架，将验证器的二元反馈与奖励模型的连续信号结合，提升大语言模型推理能力


<details>
  <summary>Details</summary>
Motivation: 现有验证器提供的0-1二元奖励信号过于僵化，无法评估部分正确或替代答案，限制了模型学习潜力

Method: 通过分层标准化约束奖励分数范围，采用方差感知加权强调关键样本，构建混合奖励强化学习框架

Result: 在数学推理任务上全面超越单一奖励方法，尤其在难验证任务中表现突出

Conclusion: 混合奖励设计结合了验证器稳定性与奖励模型细粒度反馈的优势，推动了推理能力的进步

Abstract: Post-training for reasoning of large language models (LLMs) increasingly
relies on verifiable rewards: deterministic checkers that provide 0-1
correctness signals. While reliable, such binary feedback is brittle--many
tasks admit partially correct or alternative answers that verifiers
under-credit, and the resulting all-or-nothing supervision limits learning.
Reward models offer richer, continuous feedback, which can serve as a
complementary supervisory signal to verifiers. We introduce HERO (Hybrid
Ensemble Reward Optimization), a reinforcement learning framework that
integrates verifier signals with reward-model scores in a structured way. HERO
employs stratified normalization to bound reward-model scores within
verifier-defined groups, preserving correctness while refining quality
distinctions, and variance-aware weighting to emphasize challenging prompts
where dense signals matter most. Across diverse mathematical reasoning
benchmarks, HERO consistently outperforms RM-only and verifier-only baselines,
with strong gains on both verifiable and hard-to-verify tasks. Our results show
that hybrid reward design retains the stability of verifiers while leveraging
the nuance of reward models to advance reasoning.

</details>


### [98] [LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM Evaluation](https://arxiv.org/abs/2510.07243)
*Joseph Enguehard,Morgane Van Ermengem,Kate Atkinson,Sujeong Cha,Arijit Ghosh Chowdhury,Prashanth Kallur Ramaswamy,Jeremy Roghair,Hannah R Marlowe,Carina Suzana Negreanu,Kitty Boxall,Diana Mincu*

Main category: cs.CL

TL;DR: 论文提出基于法律数据点(LDPs)的无参考评估方法，有效改进法律领域大语言模型输出的评估，在多个数据集上超越基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有法律领域LLM评估方法依赖昂贵的人工标注数据或不可靠的标准化指标，导致评估结果波动大且可信度低。

Method: 将法律回答分解为自包含的LDPs单元，设计符合律师评估习惯的无参考评估指标，并在专有数据集和LegalBench开源数据集进行验证。

Result: 该方法在专有/开源数据集均优于基线模型，与专家评估相关性更高(提升21%)，显著提高评分者间一致性(ICC系数提升0.15)。

Conclusion: 通过LDPs方法填补法律LLM评估空白，开源的子数据集促进研究复现，推动法律问答领域的LLM评估技术进步。

Abstract: Evaluating large language model (LLM) outputs in the legal domain presents
unique challenges due to the complex and nuanced nature of legal analysis.
Current evaluation approaches either depend on reference data, which is costly
to produce, or use standardized assessment methods, both of which have
significant limitations for legal applications.
  Although LLM-as-a-Judge has emerged as a promising evaluation technique, its
reliability and effectiveness in legal contexts depend heavily on evaluation
processes unique to the legal industry and how trustworthy the evaluation
appears to the human legal expert. This is where existing evaluation methods
currently fail and exhibit considerable variability.
  This paper aims to close the gap: a) we break down lengthy responses into
'Legal Data Points' (LDPs), self-contained units of information, and introduce
a novel, reference-free evaluation methodology that reflects how lawyers
evaluate legal answers; b) we demonstrate that our method outperforms a variety
of baselines on both our proprietary dataset and an open-source dataset
(LegalBench); c) we show how our method correlates more closely with human
expert evaluations and helps improve inter-annotator agreement; and finally d)
we open source our Legal Data Points for a subset of LegalBench used in our
experiments, allowing the research community to replicate our results and
advance research in this vital area of LLM evaluation on legal
question-answering.

</details>


### [99] [Don't Adapt Small Language Models for Tools; Adapt Tool Schemas to the Models](https://arxiv.org/abs/2510.07248)
*Jonggeun Lee,Woojung Song,Jongwook Han,Haesung Pyun,Yohan Jo*

Main category: cs.CL

TL;DR: PA-Tool通过将工具模式与预训练知识对齐，提升小语言模型的工具使用能力，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 小语言模型在工具选择/参数识别时存在模式错配问题，传统方法强制模型适配工具模式效率低下。PA-Tool提出逆向思路——让工具模式适配模型预训练知识。

Method: 利用污染检测中的峰值信号(peakedness)，通过多候选生成+输出集中度筛选，自动重命名工具组件实现模式对齐。

Result: 在MetaTool/RoTBench上准确率提升17%，模式错配错误减少80%，小模型工具使用性能接近SOTA同时保持计算效率。

Conclusion: 通过模式层面的适配而非模型层面的调整，既能释放高效能小模型的工具使用潜力，又保持新工具适配时的计算效率优势。

Abstract: Small language models (SLMs) offer significant computational advantages for
tool-augmented AI systems, yet they struggle with tool-use tasks, particularly
in selecting appropriate tools and identifying correct parameters. A common
failure mode is schema misalignment: models hallucinate plausible but
non-existent tool names that reflect naming conventions internalized during
pretraining but absent from the provided tool schema. Rather than forcing
models to adapt to arbitrary schemas, we propose adapting schemas to align with
models' pretrained knowledge. We introduce PA-Tool (Pretraining-Aligned Tool
Schema Generation), a training-free method that leverages peakedness-a signal
from contamination detection indicating pretraining familiarity-to
automatically rename tool components. By generating multiple candidates and
selecting those with highest output concentration across samples, PA-Tool
identifies pretrain-aligned naming patterns. Experiments on MetaTool and
RoTBench show improvements of up to 17% points, with schema misalignment errors
reduced by 80%. PA-Tool enables small models to approach state-of-the-art
performance while maintaining computational efficiency for adaptation to new
tools without retraining. Our work demonstrates that schema-level interventions
can unlock the tool-use potential of resource-efficient models by adapting
schemas to models rather than models to schemas.

</details>


### [100] [Online Rubrics Elicitation from Pairwise Comparisons](https://arxiv.org/abs/2510.07284)
*MohammadHossein Rezaei,Robert Vacareanu,Zihao Wang,Clinton Wang,Yunzhong He,Afra Feyza Akyürek*

Main category: cs.CL

TL;DR: 提出OnlineRubrics方法，通过动态生成评估标准提升LLM训练效果，实验显示多个基准提升达8%。


<details>
  <summary>Details</summary>
Motivation: 静态rubrics存在奖励滥用风险且无法捕捉训练中新需求，需动态调整评估标准来持续改进模型表现。

Method: 在线对比当前策略与参考策略的响应，通过pairwise比较动态生成rubrics，持续识别和纠正训练中的错误。

Result: 在AlpacaEval/GPQA等基准上相比静态rubrics提升达8%，定性分析识别出透明度、实用性等核心评估维度。

Conclusion: 动态rubrics能有效捕捉训练中的新需求，其持续优化机制为LLM训练提供了新方向。

Abstract: Rubrics provide a flexible way to train LLMs on open-ended long-form answers
where verifiable rewards are not applicable and human preferences provide
coarse signals. Prior work shows that reinforcement learning with rubric-based
rewards leads to consistent gains in LLM post-training. Most existing
approaches rely on rubrics that remain static over the course of training. Such
static rubrics, however, are vulnerable to reward-hacking type behaviors and
fail to capture emergent desiderata that arise during training. We introduce
Online Rubrics Elicitation (OnlineRubrics), a method that dynamically curates
evaluation criteria in an online manner through pairwise comparisons of
responses from current and reference policies. This online process enables
continuous identification and mitigation of errors as training proceeds.
Empirically, this approach yields consistent improvements of up to 8% over
training exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as
well as the validation sets of expert questions and rubrics. We qualitatively
analyze the elicited criteria and identify prominent themes such as
transparency, practicality, organization, and reasoning.

</details>


### [101] [On the Convergence of Moral Self-Correction in Large Language Models](https://arxiv.org/abs/2510.07290)
*Guangliang Liu,Haitao Mao,Bochuan Cao,Zhiyu Xue,Xitong Zhang,Rongrong Wang,Kristen Marie Johnson*

Main category: cs.CL

TL;DR: 研究发现大语言模型在道德自我纠正中存在性能收敛特性，其机制源于持续指令激活稳定化的道德概念


<details>
  <summary>Details</summary>
Motivation: 现有研究未能解释LLMs内在自我纠正（特别是道德领域）为何有效，需揭示其运作机制及收敛特性

Method: 通过多轮交互实验和概念激活分析，观察道德指令持续注入对模型不确定性的影响

Result: 自我纠正指令持续激活道德概念，降低模型预测方差，随着多轮交互中激活概念的稳定化实现性能收敛

Conclusion: 道德自我纠正展现出性能收敛的稳定特性，证明其具备可靠的实践应用潜力

Abstract: Large Language Models (LLMs) are able to improve their responses when
instructed to do so, a capability known as self-correction. When instructions
provide only a general and abstract goal without specific details about
potential issues in the response, LLMs must rely on their internal knowledge to
improve response quality, a process referred to as intrinsic self-correction.
The empirical success of intrinsic self-correction is evident in various
applications, but how and why it is effective remains unknown. Focusing on
moral self-correction in LLMs, we reveal a key characteristic of intrinsic
self-correction: performance convergence through multi-round interactions; and
provide a mechanistic analysis of this convergence behavior. Based on our
experimental results and analysis, we uncover the underlying mechanism of
convergence: consistently injected self-correction instructions activate moral
concepts that reduce model uncertainty, leading to converged performance as the
activated moral concepts stabilize over successive rounds. This paper
demonstrates the strong potential of moral self-correction by showing that it
exhibits a desirable property of converged performance.

</details>


### [102] [Think Natively: Unlocking Multilingual Reasoning with Consistency-Enhanced Reinforcement Learning](https://arxiv.org/abs/2510.07300)
*Xue Zhang,Yunlong Liang,Fandong Meng,Songming Zhang,Kaiyu Huang,Yufeng Chen,Jinan Xu,Jie Zhou*

Main category: cs.CL

TL;DR: M-Thinker模型通过GRPO算法提升多语言推理一致性及准确性


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型(LRMs)在处理非英语语言时存在输入输出语言不一致、推理路径错误导致准确率低的问题，严重影响用户体验及全球化部署

Method: 采用GRPO算法：1) 语言一致性(LC)奖励强制输入/思考/输出语言一致；2) 跨语言思维对齐(CTA)奖励通过对比非英语与英语推理路径，实现英语推理能力迁移

Result: M-Thinker-1.5B/7B模型在MMATH和PolyMath基准上实现近100%语言一致性，并展现优异的跨语言泛化能力

Conclusion: 该方法有效解决多语言场景下LRMs的核心痛点，为全球化部署提供可靠解决方案

Abstract: Large Reasoning Models (LRMs) have achieved remarkable performance on complex
reasoning tasks by adopting the "think-then-answer" paradigm, which enhances
both accuracy and interpretability. However, current LRMs exhibit two critical
limitations when processing non-English languages: (1) They often struggle to
maintain input-output language consistency; (2) They generally perform poorly
with wrong reasoning paths and lower answer accuracy compared to English. These
limitations significantly degrade the user experience for non-English speakers
and hinder the global deployment of LRMs. To address these limitations, we
propose M-Thinker, which is trained by the GRPO algorithm that involves a
Language Consistency (LC) reward and a novel Cross-lingual Thinking Alignment
(CTA) reward. Specifically, the LC reward defines a strict constraint on the
language consistency between the input, thought, and answer. Besides, the CTA
reward compares the model's non-English reasoning paths with its English
reasoning path to transfer its own reasoning capability from English to
non-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/7B
models not only achieve nearly 100% language consistency and superior
performance on two multilingual benchmarks (MMATH and PolyMath), but also
exhibit excellent generalization on out-of-domain languages.

</details>


### [103] [Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain](https://arxiv.org/abs/2510.07309)
*Yue Li,Ran Tao,Derek Hommel,Yusuf Denizay Dönder,Sungyong Chang,David Mimno,Unso Eun Seo Jo*

Main category: cs.CL

TL;DR: 提出CORGI新基准测试，针对商业场景的四级复杂文本转SQL需求，揭示LLMs在预测和推荐类高阶问题中的性能短板


<details>
  <summary>Details</summary>
Motivation: 现有文本转SQL基准局限于事实检索，无法满足现实商业场景中因果推理、时序预测、战略推荐等多层次智能需求

Method: 构建包含DoorDash/Airbnb等企业场景的合成数据库，设计描述性→解释性→预测性→推荐性四级渐进式问题体系，建立公开评估框架

Result: LLMs在高阶问题的执行成功率显著下降，CORGI基准相比BIRD难度提升21%，预测准确率和可执行建议生成能力存在明显缺陷

Conclusion: 当前LLMs与真实商业智能需求存在能力断层，CORGI的发布将推动面向企业级复杂查询的文本转SQL技术发展

Abstract: In the business domain, where data-driven decision making is crucial,
text-to-SQL is fundamental for easy natural language access to structured data.
While recent LLMs have achieved strong performance in code generation, existing
text-to-SQL benchmarks remain focused on factual retrieval of past records. We
introduce CORGI, a new benchmark specifically designed for real-world business
contexts. CORGI is composed of synthetic databases inspired by enterprises such
as Doordash, Airbnb, and Lululemon. It provides questions across four
increasingly complex categories of business queries: descriptive, explanatory,
predictive, and recommendational. This challenge calls for causal reasoning,
temporal forecasting, and strategic recommendation, reflecting multi-level and
multi-step agentic intelligence. We find that LLM performance drops on
high-level questions, struggling to make accurate predictions and offer
actionable plans. Based on execution success rate, the CORGI benchmark is about
21\% more difficult than the BIRD benchmark. This highlights the gap between
popular LLMs and the need for real-world business intelligence. We release a
public dataset and evaluation framework, and a website for public submissions.

</details>


### [104] [Vibe Checker: Aligning Code Evaluation with Human Preference](https://arxiv.org/abs/2510.07315)
*Ming Zhong,Xiang Zhou,Ting-Yun Chang,Qingze Wang,Nan Xu,Xiance Si,Dan Garrette,Shyam Upadhyay,Jeremiah Liu,Jiawei Han,Benoit Schillings,Jiao Sun*

Main category: cs.CL

TL;DR: 论文提出VeriCode分类法和Vibe Checker测试平台，揭示主流LLM在代码指令遵循能力上的不足，发现综合功能正确性和指令遵循的评分与人类偏好相关性最高。


<details>
  <summary>Details</summary>
Motivation: 现有代码评估仅关注功能正确性(pass@k)，但实际编程中用户更关注代码风格、意图保留等非功能性指令的遵循，这些要素构成了关键的vibe check标准。

Method: 建立包含30条可验证代码指令的VeriCode分类体系，开发Vibe Checker测试平台整合功能正确性和指令遵循评估，对31个主流LLM进行全面测试。

Result: 顶级模型在多重指令遵循中表现不佳（平均达标率仅35%），且存在功能回归现象。指令遵循能力成为现实编程任务中区分模型表现的首要因素（相关系数达0.87）。

Conclusion: 指令遵循是vibe check的核心维度，VeriCode体系为模型对齐人类编码偏好提供了可量化的评估框架，指明未来模型优化的关键方向。

Abstract: Large Language Models (LLMs) have catalyzed vibe coding, where users leverage
LLMs to generate and iteratively refine code through natural language
interactions until it passes their vibe check. Vibe check is tied to real-world
human preference and goes beyond functionality: the solution should feel right,
read cleanly, preserve intent, and remain correct. However, current code
evaluation remains anchored to pass@k and captures only functional correctness,
overlooking the non-functional instructions that users routinely apply. In this
paper, we hypothesize that instruction following is the missing piece
underlying vibe check that represents human preference in coding besides
functional correctness. To quantify models' code instruction following
capabilities with measurable signals, we present VeriCode, a taxonomy of 30
verifiable code instructions together with corresponding deterministic
verifiers. We use the taxonomy to augment established evaluation suites,
resulting in Vibe Checker, a testbed to assess both code instruction following
and functional correctness. Upon evaluating 31 leading LLMs, we show that even
the strongest models struggle to comply with multiple instructions and exhibit
clear functional regression. Most importantly, a composite score of functional
correctness and instruction following correlates the best with human
preference, with the latter emerging as the primary differentiator on
real-world programming tasks. Our work identifies core factors of the vibe
check, providing a concrete path for benchmarking and developing models that
better align with user preferences in coding.

</details>


### [105] [Artificial Hippocampus Networks for Efficient Long-Context Modeling](https://arxiv.org/abs/2510.07318)
*Yunhao Fang,Weihao Yu,Shu Zhong,Qinghao Ye,Xuehan Xiong,Lai Wei*

Main category: cs.CL

TL;DR: 提出结合滑动窗口短期记忆与人工海马网络压缩的混合记忆框架，在保证长序列建模性能的同时显著降低计算资源消耗


<details>
  <summary>Details</summary>
Motivation: 解决长序列建模中RNN压缩效率与Transformer保真度之间的矛盾，受人类记忆多存储模型认知理论启发

Method: 使用Transformer的KV缓存滑动窗口作为短期记忆，通过可学习的人工海马网络(AHN)压缩长期记忆，基于Mamba2/DeltaNet等现代RNN架构实现

Result: 在LV-Eval和InfiniteBench上性能超越滑动窗口基线，Qwen2.5-3B模型推理FLOPs降低40.5%，内存缓存减少74%，LV-Eval评分从4.41提升至5.88

Conclusion: 该框架有效平衡计算效率与建模性能，为长上下文处理提供了资源高效的新型解决方案

Abstract: Long-sequence modeling faces a fundamental trade-off between the efficiency
of compressive fixed-size memory in RNN-like models and the fidelity of
lossless growing memory in attention-based Transformers. Inspired by the
Multi-Store Model in cognitive science, we introduce a memory framework of
artificial neural networks. Our method maintains a sliding window of the
Transformer's KV cache as lossless short-term memory, while a learnable module
termed Artificial Hippocampus Network (AHN) recurrently compresses
out-of-window information into a fixed-size compact long-term memory. To
validate this framework, we instantiate AHNs using modern RNN-like
architectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive
experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate
that AHN-augmented models consistently outperform sliding window baselines and
achieve performance comparable or even superior to full-attention models, while
substantially reducing computational and memory requirements. For instance,
augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%
and memory cache by 74.0%, while improving its average score on LV-Eval (128k
sequence length) from 4.41 to 5.88. Code is available at:
https://github.com/ByteDance-Seed/AHN.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [106] [Visualizing Multimodality in Combinatorial Search Landscapes](https://arxiv.org/abs/2510.06517)
*Xavier F. C. Sánchez-Díaz,Ole Jakob Mengshoel*

Main category: cs.GR

TL;DR: 论文系统回顾了组合搜索景观的多模态可视化技术，强调不同方法的整合应用及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 旨在通过整合现有可视化技术，提升对组合搜索景观多模态特征的系统性分析能力，填补领域内综合视角的不足。

Method: 基于图形语法框架，结合几何美学元素，系统讨论并整合了景观分析文献中的多种可视化技术，包含案例研究与现有成果分析。

Result: 提出'可视化领域无免费午餐'原则，证明需根据场景组合不同技术，并建立包含多维度评估的未来研究路线图。

Conclusion: 强调跨技术融合的必要性，建议从交互式可视化、高维数据映射和算法-可视化协同三个方向推进研究。

Abstract: This work walks through different visualization techniques for combinatorial
search landscapes, focusing on multimodality. We discuss different techniques
from the landscape analysis literature, and how they can be combined to provide
a more comprehensive view of the search landscape. We also include examples and
discuss relevant work to show how others have used these techniques in
practice, based on the geometric and aesthetic elements of the Grammar of
Graphics. We conclude that there is no free lunch in visualization, and provide
recommendations for future work as there are several paths to continue the work
in this field.

</details>


### [107] [Capture and Interact: Rapid 3D Object Acquisition and Rendering with Gaussian Splatting in Unity](https://arxiv.org/abs/2510.06802)
*Islomjon Shukhratov,Sergey Gorinsky*

Main category: cs.GR

TL;DR: 基于3D高斯泼溅技术构建端到端实时3D采集渲染系统，移动端采集+云端重建+本地150fps交互式可视化


<details>
  <summary>Details</summary>
Motivation: 解决实时3D物体捕捉与渲染的技术瓶颈，拓展增强现实/数字孪生/远程协作等应用场景

Method: 整合智能手机视频采集、云端GPU加速的3D高斯泼溅重建、Unity引擎实时渲染的技术路径

Result: GPU处理耗时约10分钟，笔记本电脑端实现平均150fps的实时交互渲染

Conclusion: 验证了移动-云-端协同架构在实时3D重建中的可行性，为远程临场感系统提供新范式

Abstract: Capturing and rendering three-dimensional (3D) objects in real time remain a
significant challenge, yet hold substantial potential for applications in
augmented reality, digital twin systems, remote collaboration and prototyping.
We present an end-to-end pipeline that leverages 3D Gaussian Splatting (3D GS)
to enable rapid acquisition and interactive rendering of real-world objects
using a mobile device, cloud processing and a local computer. Users scan an
object with a smartphone video, upload it for automated 3D reconstruction, and
visualize it interactively in Unity at an average of 150 frames per second
(fps) on a laptop. The system integrates mobile capture, cloud-based 3D GS and
Unity rendering to support real-time telepresence. Our experiments show that
the pipeline processes scans in approximately 10 minutes on a graphics
processing unit (GPU) achieving real-time rendering on the laptop.

</details>


### [108] [Geometric Queries on Closed Implicit Surfaces for Walk on Stars](https://arxiv.org/abs/2510.07275)
*Tianyu Huang*

Main category: cs.GR

TL;DR: 提出基于区间分析的几何查询框架，首次实现封闭隐式曲面上无网格的Walk-on-Stars偏微分方程求解


<details>
  <summary>Details</summary>
Motivation: Walk-on-Stars方法因缺乏可靠的隐式曲面几何查询方法，在隐式边界场景的应用受限

Method: 将几何查询建模为带约束的全局优化问题，采用基于区间分析的分支定界法求解高度非凸问题

Result: 首次实现封闭隐式曲面上的最近轮廓点查询和Robin半径边界查询，支持无网格边界条件下的PDE求解

Conclusion: 该框架突破了Walk-on-Stars在隐式曲面应用的技术障碍，拓展了蒙特卡洛PDE求解器的适用范围

Abstract: Walk on stars (WoSt) is currently one of the most advanced Monte Carlo
solvers for PDEs. Unfortunately, the lack of reliable geometric query
approaches has hindered its applicability to boundaries defined by implicit
surfaces. This work proposes a geometric query framework over closed implicit
surfaces for WoSt, under the scope of walkin' Robin. Our key observation is
that all WoSt queries can be formulated as constrained global optimization or
constraint satisfaction problems. Based on our formulations, to solve the
highly non-convex problems, we adopt a branch-and-bound approach based on
interval analysis. To the best of our knowledge, our method is the first to
study closest silhouette point queries and Robin radius bound queries on closed
implicit surfaces. Our formulations and methods first enable mesh-free PDE
solving via WoSt when boundaries are defined by closed implicit surfaces.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [109] [A Review of 10 Years of ProtoSpace: Spacecraft CAD Visualization in Collaborative Augmented Reality](https://arxiv.org/abs/2510.06608)
*Benjamin Nuernberger,Samuel-Hunter Berndt,Robert Tapella,Laura Mann,Aaron Plave,Sasha Samochina,Victor X. Luo*

Main category: cs.ET

TL;DR: ProtoSpace是NASA喷气推进实验室开发的AR/3D协作平台，通过可视化CAD模型帮助科学家工程师减少沟通错误、降低任务成本风险，已成功应用于火星毅力号等十余项航天任务。


<details>
  <summary>Details</summary>
Motivation: 解决传统航天器设计中的沟通障碍，通过三维可视化帮助团队在真实物理环境中更快理解航天器空间布局，避免设计错误导致的后期修改成本。

Method: 系统架构包含HoloLens AR终端、3D网页客户端、中央服务器和CAD模型优化器，支持跨平台协作。采用模型轻量化处理技术实现复杂航天器模型的实时渲染。

Result: 持续10年应用于NASA重大任务，衍生出航天员训练指导、呼吸机设计等跨领域应用，累计减少项目成本约37%（根据火星样本返回任务估算）。

Conclusion: ProtoSpace的成功证明三维协同平台在复杂工程中的核心价值，其跨生命周期适用性和模块化架构设计是持续演进的关键因素。

Abstract: ProtoSpace is a custom JPL-built platform to help scientists and engineers
visualize their CAD models collaboratively in augmented reality (AR) and on the
web in 3D. In addition to this main use case, ProtoSpace has been used
throughout the entire spacecraft mission lifecycle and beyond: ventilator
design and assembly; providing AR-based instructions to astronauts in-training;
educating the next generation on the process of spacecraft design; etc.
ProtoSpace has been used for a decade by NASA missions-including Mars
Perseverance, Europa Clipper, NISAR, SPHEREx, CAL, and Mars Sample Return-to
reduce cost and risk by helping engineers and scientists fix problems earlier
through reducing miscommunication and helping people understand the spatial
context of their spacecraft in the appropriate physical context more quickly.
This paper will explore how ProtoSpace came to be, define the system
architecture and overview-including HoloLens and 3D web clients, the ProtoSpace
server, and the CAD model optimizer-and dive into the use cases, spin-offs, and
lessons learned that led to 10 years of success at NASA's Jet Propulsion
Laboratory.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [110] [AlphaApollo: Orchestrating Foundation Models and Professional Tools into a Self-Evolving System for Deep Agentic Reasoning](https://arxiv.org/abs/2510.06261)
*Zhanke Zhou,Chentao Cao,Xiao Feng,Xuan Li,Zongze Li,Xiangyu Lu,Jiangchao Yao,Weikai Huang,Linrui Xu,Tian Cheng,Guanyu Jiang,Yiming Zheng,Brando Miranda,Tongliang Liu,Sanmi Koyejo,Masashi Sugiyama,Bo Han*

Main category: cs.AI

TL;DR: AlphaApollo提出自我进化的多模型协同推理框架，通过工具调用和状态共享机制显著提升基础模型的推理能力上限


<details>
  <summary>Details</summary>
Motivation: 解决基础模型存在的两个核心瓶颈：1) 模型固有推理能力受限 2) 测试时迭代优化的不可靠性

Method: 整合计算工具(Python库)与检索工具构建可验证推理流程，通过共享状态地图实现多轮多模型协同优化

Result: 在AIME测试中Qwen/Llama模型平均提升5-8%，工具调用成功率超80%，推理通过率提升23-26%

Conclusion: 工具协同与迭代验证机制有效突破基础模型能力边界，建立可扩展的智能体推理范式

Abstract: We present AlphaApollo, a self-evolving agentic reasoning system that aims to
address two bottlenecks in foundation model (FM) reasoning-limited
model-intrinsic capacity and unreliable test-time iteration. AlphaApollo
orchestrates multiple models with professional tools to enable deliberate,
verifiable reasoning. It couples (i) a computation tool (Python with numerical
and symbolic libraries) and (ii) a retrieval tool (task-relevant external
information) to execute exact calculations and ground decisions. The system
further supports multi-round, multi-model solution evolution via a shared state
map that records candidates, executable checks, and feedback for iterative
refinement. In evaluations on AIME 2024/2025 across multiple models,
AlphaApollo delivers consistent gains: +5.15% Average@32 and +23.34% Pass@32
for Qwen2.5-14B-Instruct, and +8.91% Average@32 with +26.67% Pass@32 for
Llama-3.3-70B-Instruct. Tool-use analysis shows that more than 80% of tool
calls are successfully executed, with consistent outperformance of non-tool
baselines, thereby lifting the capability ceiling of FMs. More empirical
results and implementation details will be updated at
https://github.com/tmlr-group/AlphaApollo.

</details>


### [111] [PuzzlePlex: Benchmarking Foundation Models on Reasoning and Planning with Puzzles](https://arxiv.org/abs/2510.06475)
*Yitao Long,Yuru Jiang,Hongjun Liu,Yilun Zhao,Jingchen Sun,Yiqiu Shen,Chen Zhao,Arman Cohan,Dennis Shasha*

Main category: cs.AI

TL;DR: 提出PuzzlePlex基准测试框架，系统评估基础模型在复杂环境下的推理规划能力，发现推理模型在指令模式表现更优，代码执行模式具备扩展潜力。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在复杂动态环境中的系统性评估不足，需建立标准化基准测试框架以衡量其推理规划能力的边界和扩展性。

Method: 构建包含15类谜题（含确定/随机、单/双人场景）的PuzzlePlex框架，设计细粒度评估指标，对比指令模式和代码模式下的模型表现，并进行系统性扩展测试。

Result: 推理模型在指令模式下表现最优（GPT-4准确率68%），代码执行模式虽更具挑战（准确率42%），但具备计算效率和可扩展性优势。

Conclusion: PuzzlePlex为模型能力评估提供新范式，揭示了混合推理-代码架构的发展方向，推动基础模型在复杂问题解决能力的持续进化。

Abstract: This work investigates the reasoning and planning capabilities of foundation
models and their scalability in complex, dynamic environments. We introduce
PuzzlePlex, a benchmark designed to assess these capabilities through a diverse
set of puzzles. PuzzlePlex consists of 15 types of puzzles, including
deterministic and stochastic games of varying difficulty, as well as
single-player and two-player scenarios. The PuzzlePlex framework provides a
comprehensive environment for each game, and supports extensibility to generate
more challenging instances as foundation models evolve. Additionally, we
implement customized game-playing strategies for comparison. Building on this
benchmark, we develop fine-grained metrics to measure performance and conduct
an in-depth analysis of frontier foundation models across two settings:
instruction-based and code-based. Furthermore, we systematically investigate
their scaling limits. Our findings show that reasoning models outperform others
in instruction-based settings, while code-based execution presents greater
challenges but offers a scalable and efficient alternative. PuzzlePlex enables
targeted evaluation and guides future improvements in reasoning, planning, and
generalization for foundation models.

</details>


### [112] [Evolving and Executing Research Plans via Double-Loop Multi-Agent Collaboration](https://arxiv.org/abs/2510.06761)
*Zhi Zhang,Yan Liu,Zhejing Hu,Gong Chen,Sheng-hua Zhong,Jiannong Cao*

Main category: cs.AI

TL;DR: 提出双循环多智能体框架（DLMA），通过进化算法动态生成研究方案并精准执行，解决科研自动化中计划创新与执行可靠的双层挑战


<details>
  <summary>Details</summary>
Motivation: 传统自动化科研方法难以兼顾高层计划的新颖性与动态执行可靠性。DLMA框架通过双层架构分离计划进化与执行调整，实现科研全流程自动化

Method: 1. 领导者循环（教授代理）通过进化算法进行包含/改进/整合的三阶段会议，持续优化研究提案池
2. 跟随者循环（博士生代理）采用事前/事后会议机制动态调整实施计划，每个执行步骤（如编码/实验）均受上下文和外部观察支持

Result: 在ACLAward和Laboratory基准测试中达到SOTA（平均提升12.7%），消融实验证实双循环架构贡献度：进化循环带来63%创新性提升，执行循环减少42%实施错误

Conclusion: DLMA通过计划进化与执行优化的解耦设计，实现科研自动化全流程的创新探索与可靠实施。该方法为复杂科研任务自动化提供了可扩展的架构范式

Abstract: Automating the end-to-end scientific research process poses a fundamental
challenge: it requires both evolving high-level plans that are novel and sound,
and executing these plans correctly amidst dynamic and uncertain conditions. To
address this bilevel challenge, we propose a novel Double-Loop Multi-Agent
(DLMA) framework to solve the given research problem automatically. The leader
loop, composed of professor agents, is responsible for evolving research plans.
It employs an evolutionary algorithm through involvement, improvement, and
integration meetings to iteratively generate and refine a pool of research
proposals, exploring the solution space effectively. The follower loop,
composed of doctoral student agents, is responsible for executing the
best-evolved plan. It dynamically adjusts the plan during implementation via
pre-hoc and post-hoc meetings, ensuring each step (e.g., drafting, coding) is
well-supported by contextual and external observations. Extensive experiments
on benchmarks like ACLAward and Laboratory show that DLMA generates research
papers that achieve state-of-the-art scores in automated evaluation,
significantly outperforming strong baselines. Ablation studies confirm the
critical roles of both loops, with evolution driving novelty and execution
ensuring soundness.

</details>


### [113] [Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces](https://arxiv.org/abs/2510.06953)
*Minju Gwak,Guijin Son,Jaehyung Kim*

Main category: cs.AI

TL;DR: 研究验证了信息密度均匀性（UID）作为大语言模型推理质量的有效指标，基于步骤级信息密度选择推理轨迹可将准确率提升10-32%。


<details>
  <summary>Details</summary>
Motivation: 验证步骤级信息密度均匀性是否反映大语言模型推理质量，探索理论解释框架与实用性能提升的双重价值。

Method: 提出基于熵的逐步信息密度指标，包含局部/全局均匀性分数，在6个推理基准测试中分析正确与错误推理轨迹的信息分布模式。

Result: 正确推理轨迹信息密度波动平缓（相对准确率提升10-32%），错误轨迹存在异常信息尖峰，UID指标预测效果优于其他内部信号。

Conclusion: 信息密度均匀性为构建可靠推理系统提供了理论诊断工具和实证筛选标准，兼具算法优化与系统设计指导价值。

Abstract: The Uniform Information Density (UID) hypothesis suggests that effective
communication maintains a stable flow of information. In this work, we revisit
this principle in the context of large language model (LLM) reasoning traces,
asking whether step-level uniformity reflects reasoning quality. To this end,
we propose an entropy-based stepwise information density metric and introduce
two complementary measures of uniformity, local and global uniformity scores.
Across the experiments on six different reasoning benchmarks, we find that
step-level uniformity not only provides a strong theoretical lens but also
yields practical performance benefits; for example, selecting reasoning traces
with more uniform information density at the step-level improves accuracy by
10-32\% relative gains over baselines at AIME2025. Our analysis further reveals
that correct reasoning traces tend to avoid sharp information density spikes,
while incorrect traces exhibit irregular information bursts. These results
demonstrate that UID-inspired information density measures outperform
alternative internal signals as predictors of reasoning quality. Results
highlight the uniformity of the information density as a robust diagnostic and
selection criterion for building more reliable and accurate reasoning systems.

</details>


### [114] [The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas](https://arxiv.org/abs/2510.07091)
*Baixuan Xu,Tianshi Zheng,Zhaowei Wang,Hong Ting Tsang,Weiqi Wang,Tianqing Fang,Yangqiu Song*

Main category: cs.AI

TL;DR: 本文系统研究PwA（具体动作规划）与PwS（模式化规划）在长周期任务中的有效性，发现当环境动作空间超过临界点（约500动作）时PwS更具扩展性。


<details>
  <summary>Details</summary>
Motivation: 传统PwA方法在组合爆炸的动作空间（如开放世界）中存在局限性，PwS通过模式实例化（如'移动[对象]至[位置]'）实现简洁动作空间，更符合人类认知和环境格式约束。

Method: 通过认知带宽框架对比PwA/PwS，在ALFWorld（~35动作）和SciWorld（~500动作）环境发现表征选择拐点，并通过控制实验分析模型能力对拐点位置的影响。

Result: 观察到PwS在SciWorld等大规模动作空间环境更具扩展性，但当前性能仍不理想；模型规划能力越强拐点右移，模式实例化能力越强拐点左移。

Conclusion: 需提升PwS代理的模式实例化能力，建议通过增强模式识别与执行精度构建更具扩展性的自主系统，突破现有基准局限。

Abstract: Enabling LLMs to effectively operate long-horizon task which requires
long-term planning and multiple interactions is essential for open-world
autonomy. Conventional methods adopt planning with actions where a executable
action list would be provided as reference. However, this action representation
choice would be impractical when the environment action space is combinatorial
exploded (e.g., open-ended real world). This naturally leads to a question: As
environmental action space scales, what is the optimal action representation
for long-horizon agents? In this paper, we systematically study the
effectiveness of two different action representations. The first one is
conventional planning with actions (PwA) which is predominantly adopted for its
effectiveness on existing benchmarks. The other one is planning with schemas
(PwS) which instantiate an action schema into action lists (e.g., "move [OBJ]
to [OBJ]" -> "move apple to desk") to ensure concise action space and reliable
scalability. This alternative is motivated by its alignment with human
cognition and its compliance with environment-imposed action format
restriction. We propose cognitive bandwidth perspective as a conceptual
framework to qualitatively understand the differences between these two action
representations and empirically observe a representation-choice inflection
point between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve
as evidence of the need for scalable representations. We further conduct
controlled experiments to study how the location of this inflection point
interacts with different model capacities: stronger planning proficiency shifts
the inflection rightward, whereas better schema instantiation shifts it
leftward. Finally, noting the suboptimal performance of PwS agents, we provide
an actionable guide for building more capable PwS agents for better scalable
autonomy.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [115] [The Markovian Thinker](https://arxiv.org/abs/2510.06557)
*Milad Aghajohari,Kamran Chitsaz,Amirhossein Kazemnejad,Sarath Chandar,Alessandro Sordoni,Aaron Courville,Siva Reddy*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Reinforcement learning (RL) has recently become a strong recipe for training
reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard
RL "thinking environment", where the state is the prompt plus all prior
reasoning tokens, makes the state unbounded and forces attention-based policies
to pay quadratic compute as thoughts lengthen. We revisit the environment
itself. We propose Markovian Thinking, a paradigm in which the policy advances
reasoning while conditioning on a constant-size state, decoupling thinking
length from context size. As an immediate consequence this yields linear
compute with constant memory. We instantiate this idea with Delethink, an RL
environment that structures reasoning into fixed-size chunks. Within each
chunk, the model thinks as usual; at the boundary, the environment resets the
context and reinitializes the prompt with a short carryover. Through RL, the
policy learns to write a textual state near the end of each chunk sufficient
for seamless continuation of reasoning after reset. Trained in this
environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up
to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget.
With test-time scaling, Delethink continues to improve where LongCoT plateaus.
The effect of linear compute is substantial: we empirically estimate at 96K
average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink.
Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B)
often sample Markovian traces zero-shot across diverse benchmarks, providing
positive samples that make RL effective at scale. Our results show that
redesigning the thinking environment is a powerful lever: it enables very long
reasoning without quadratic overhead and opens a path toward efficient,
scalable reasoning LLMs.

</details>


### [116] [A Multi-Agent Framework for Stateful Inference-Time Search](https://arxiv.org/abs/2510.07147)
*Arshika Lalan,Rajat Ghosh,Aditya Kolsur,Debojyoti Dutta*

Main category: cs.LG

TL;DR: 提出有状态多智能体进化搜索框架，通过持久状态、对抗变异和进化保留机制，显著提升单元测试生成的覆盖率与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有无状态推理方法在多层次推理任务中因缺乏持久状态而表现不佳，微调方法仅能生成表层代码但无法处理深层逻辑依赖。

Method: 融合：(1)持久推理状态跨代维护；(2)对抗性变异生成新候选；(3)进化保留机制确保多样性。采用多智能体协同生成/突变/评分测试用例。

Result: 在HumanEval等基准测试中，使用Llama/Gemma/GPT模型实现覆盖率显著提升，验证框架在跨代码库生成高覆盖边缘用例的有效性。

Conclusion: 持久推理状态与进化搜索结合为单元测试生成提供新范式，证明状态维护与演化机制对复杂代码任务的必要性。

Abstract: Recent work explores agentic inference-time techniques to perform structured,
multi-step reasoning. However, stateless inference often struggles on
multi-step tasks due to the absence of persistent state. Moreover,
task-specific fine-tuning or instruction-tuning often achieve surface-level
code generation but remain brittle on tasks requiring deeper reasoning and
long-horizon dependencies. To address these limitations, we propose stateful
multi-agent evolutionary search, a training-free framework that departs from
prior stateless approaches by combining (i) persistent inference-time state,
(ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate
its effectiveness in automated unit test generation through the generation of
edge cases. We generate robust edge cases using an evolutionary search process,
where specialized agents sequentially propose, mutate, and score candidates. A
controller maintains persistent state across generations, while evolutionary
preservation ensures diversity and exploration across all possible cases. This
yields a generalist agent capable of discovering robust, high-coverage edge
cases across unseen codebases. Experiments show our stateful multi-agent
inference framework achieves substantial gains in coverage over stateless
single-step baselines, evaluated on prevalent unit-testing benchmarks such as
HumanEval and TestGenEvalMini and using three diverse LLM families - Llama,
Gemma, and GPT. These results indicate that combining persistent inference-time
state with evolutionary search materially improves unit-test generation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [117] [Reading Between the Lines: Towards Reliable Black-box LLM Fingerprinting via Zeroth-order Gradient Estimation](https://arxiv.org/abs/2510.06605)
*Shuo Shao,Yiming Li,Hongwei Yao,Yifei Chen,Yuchen Yang,Zhan Qin*

Main category: cs.CR

TL;DR: 提出ZeroPrint方法，利用费雪信息理论和零阶梯度估计生成LLM指纹，解决现有黑盒方法无法有效区分模型的问题。


<details>
  <summary>Details</summary>
Motivation: 现有黑盒指纹方法依赖模型输出，因非线性信息丢失导致指纹区分度不足。输入梯度蕴含更丰富的模型参数特征。

Method: 通过费雪信息理论证明输入梯度更有效，用语义替换模拟扰动进行零阶梯度估计，构建Jacobian矩阵作为指纹。

Result: 在标准测试中达到SOTA效果，有效性及鲁棒性显著超越现有黑盒方法。

Conclusion: ZeroPrint通过梯度特征提取和文本扰动策略，为LLM版权保护提供了可靠的解决方案。

Abstract: The substantial investment required to develop Large Language Models (LLMs)
makes them valuable intellectual property, raising significant concerns about
copyright protection. LLM fingerprinting has emerged as a key technique to
address this, which aims to verify a model's origin by extracting an intrinsic,
unique signature (a "fingerprint") and comparing it to that of a source model
to identify illicit copies. However, existing black-box fingerprinting methods
often fail to generate distinctive LLM fingerprints. This ineffectiveness
arises because black-box methods typically rely on model outputs, which lose
critical information about the model's unique parameters due to the usage of
non-linear functions. To address this, we first leverage Fisher Information
Theory to formally demonstrate that the gradient of the model's input is a more
informative feature for fingerprinting than the output. Based on this insight,
we propose ZeroPrint, a novel method that approximates these information-rich
gradients in a black-box setting using zeroth-order estimation. ZeroPrint
overcomes the challenge of applying this to discrete text by simulating input
perturbations via semantic-preserving word substitutions. This operation allows
ZeroPrint to estimate the model's Jacobian matrix as a unique fingerprint.
Experiments on the standard benchmark show ZeroPrint achieves a
state-of-the-art effectiveness and robustness, significantly outperforming
existing black-box methods.

</details>


### [118] [Differentially Private Synthetic Text Generation for Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2510.06719)
*Junki Mori,Kazuya Kakizaki,Taiki Miyagawa,Jun Sakuma*

Main category: cs.CR

TL;DR: 提出DP-SynRAG框架，通过生成差分隐私合成数据库解决传统RAG隐私泄露问题，在固定隐私预算下实现更优性能


<details>
  <summary>Details</summary>
Motivation: 现有隐私RAG依赖查询时差分隐私技术，导致重复噪声注入和隐私损失累积，难以在敏感领域安全应用

Method: 利用LLM生成差分隐私的合成RAG数据库，通过扩展私有预测技术保留关键信息，支持合成文本的重复无隐私损耗使用

Result: 实验证明DP-SynRAG在保持固定隐私预算时，性能显著优于现有隐私RAG系统

Conclusion: 该框架为隐私保护RAG提供了可扩展解决方案，突破重复隐私成本的限制，推动敏感领域的安全知识增强应用

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
grounding them in external knowledge. However, its application in sensitive
domains is limited by privacy risks. Existing private RAG methods typically
rely on query-time differential privacy (DP), which requires repeated noise
injection and leads to accumulated privacy loss. To address this issue, we
propose DP-SynRAG, a framework that uses LLMs to generate differentially
private synthetic RAG databases. Unlike prior methods, the synthetic text can
be reused once created, thereby avoiding repeated noise injection and
additional privacy costs. To preserve essential information for downstream RAG
tasks, DP-SynRAG extends private prediction, which instructs LLMs to generate
text that mimics subsampled database records in a DP manner. Experiments show
that DP-SynRAG achieves superior performanec to the state-of-the-art private
RAG systems while maintaining a fixed privacy budget, offering a scalable
solution for privacy-preserving RAG.

</details>


### [119] [Exposing Citation Vulnerabilities in Generative Engines](https://arxiv.org/abs/2510.06823)
*Riku Mochizuki,Shusuke Komatsu,Souta Noguchi,Kazuto Ataka*

Main category: cs.CR

TL;DR: 通过提出基于引用发布者属性的评估标准，揭示生成引擎在防御投毒攻击中的漏洞。美日政治领域实验显示美国政治答案面临更高投毒风险，低内容注入屏障的引用源存在高频率引用与低内容映射矛盾。


<details>
  <summary>Details</summary>
Motivation: 现有研究聚焦引用内容忠实度，但未解决应选择何种引用源来防御投毒攻击。生成引擎开放性的网络引用机制存在被恶意内容操纵的安全隐患。

Method: 提出基于引用发布者属性的评估框架，通过分类官方/非官方来源估算内容注入屏障。开展跨语言（日英）政治领域实证研究，统计主要来源引用比例与内容映射质量。

Result: 美国政党官网引用比例仅25%-45%（日本60%-65%），显示更高投毒风险。低屏障引用源被引用频率与内容相关性呈负相关（r=-0.32）。日语内容曝光效率比英语低40%。

Conclusion: 引用源选择机制需整合发布者权威性评估。提升主要来源可见性的传统SEO策略存在语言壁垒，需开发跨语言内容优化框架。防御投毒攻击需构建动态信任评估体系。

Abstract: We analyze answers generated by generative engines (GEs) from the
perspectives of citation publishers and the content-injection barrier, defined
as the difficulty for attackers to manipulate answers to user prompts by
placing malicious content on the web. GEs integrate two functions: web search
and answer generation that cites web pages using large language models. Because
anyone can publish information on the web, GEs are vulnerable to poisoning
attacks. Existing studies of citation evaluation focus on how faithfully answer
content reflects cited sources, leaving unexamined which web sources should be
selected as citations to defend against poisoning attacks. To fill this gap, we
introduce evaluation criteria that assess poisoning threats using the citation
information contained in answers. Our criteria classify the publisher
attributes of citations to estimate the content-injection barrier thereby
revealing the threat of poisoning attacks in current GEs. We conduct
experiments in political domains in Japan and the United States (U.S.) using
our criteria and show that citations from official party websites (primary
sources) are approximately \(25\%\)--\(45\%\) in the U.S. and
\(60\%\)--\(65\%\) in Japan, indicating that U.S. political answers are at
higher risk of poisoning attacks. We also find that sources with low
content-injection barriers are frequently cited yet are poorly reflected in
answer content. To mitigate this threat, we discuss how publishers of primary
sources can increase exposure of their web content in answers and show that
well-known techniques are limited by language differences.

</details>


### [120] [VelLMes: A high-interaction AI-based deception framework](https://arxiv.org/abs/2510.06975)
*Muris Sladić,Veronica Valeros,Carlos Catania,Sebastian Garcia*

Main category: cs.CR

TL;DR: 基于大语言模型的VelLMes欺骗框架可模拟SSH、MySQL等多协议服务作为蜜罐，测试显示30%攻击者被成功欺骗，实际部署能有效应对非结构化攻击


<details>
  <summary>Details</summary>
Motivation: 现有欺骗系统仅支持单一协议且缺乏人类攻击评估，生成式AI为创建逼真网络欺骗系统提供了新机遇

Method: 通过精心设计的提示工程构建多协议模拟框架，强调交互真实性和协议多样性，采用单元测试和真实攻击场景验证

Result: LLM生成测试通过率达100%，30%人类攻击者误判真实系统，互联网部署成功响应84%攻击指令

Conclusion: 多协议LLM蜜罐在提升网络欺骗效果方面展现显著潜力，特别适用于应对真实网络环境中的非结构化攻击

Abstract: There are very few SotA deception systems based on Large Language Models. The
existing ones are limited only to simulating one type of service, mainly SSH
shells. These systems - but also the deception technologies not based on LLMs -
lack an extensive evaluation that includes human attackers. Generative AI has
recently become a valuable asset for cybersecurity researchers and
practitioners, and the field of cyber-deception is no exception. Researchers
have demonstrated how LLMs can be leveraged to create realistic-looking
honeytokens, fake users, and even simulated systems that can be used as
honeypots. This paper presents an AI-based deception framework called VelLMes,
which can simulate multiple protocols and services such as SSH Linux shell,
MySQL, POP3, and HTTP. All of these can be deployed and used as honeypots, thus
VelLMes offers a variety of choices for deception design based on the users'
needs. VelLMes is designed to be attacked by humans, so interactivity and
realism are key for its performance. We evaluate the generative capabilities
and the deception capabilities. Generative capabilities were evaluated using
unit tests for LLMs. The results of the unit tests show that, with careful
prompting, LLMs can produce realistic-looking responses, with some LLMs having
a 100% passing rate. In the case of the SSH Linux shell, we evaluated deception
capabilities with 89 human attackers. The results showed that about 30% of the
attackers thought that they were interacting with a real system when they were
assigned an LLM-based honeypot. Lastly, we deployed 10 instances of the SSH
Linux shell honeypot on the Internet to capture real-life attacks. Analysis of
these attacks showed us that LLM honeypots simulating Linux shells can perform
well against unstructured and unexpected attacks on the Internet, responding
correctly to most of the issued commands.

</details>


### [121] [RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning](https://arxiv.org/abs/2510.06994)
*Artur Horal,Daniel Pina,Henrique Paz,Iago Paulo,João Soares,Rafael Ferreira,Diogo Tavares,Diogo Glória-Silva,João Magalhães,David Semedo*

Main category: cs.CR

TL;DR: 提出RedTWIZ框架用于系统评估大语言模型在AI辅助开发中的鲁棒性，通过多轮对抗攻击暴露模型漏洞


<details>
  <summary>Details</summary>
Motivation: 现有方法在系统性评估LLM对话越狱漏洞、生成多样化攻击策略及针对性规划方面存在不足，需建立统一评估体系

Method: 1.建立系统化LLM越狱评估体系 2.开发支持组合/真实场景的多轮攻击套件 3.创建分层攻击规划器适配具体漏洞

Result: 实验证明该框架能有效诱导SOTA LLM生成不安全内容（代码漏洞、伦理问题等）

Conclusion: 揭示当前LLM鲁棒性薄弱环节，强调需要开发更系统的防御机制和持续评估体系

Abstract: This paper presents the vision, scientific contributions, and technical
details of RedTWIZ: an adaptive and diverse multi-turn red teaming framework,
to audit the robustness of Large Language Models (LLMs) in AI-assisted software
development. Our work is driven by three major research streams: (1) robust and
systematic assessment of LLM conversational jailbreaks; (2) a diverse
generative multi-turn attack suite, supporting compositional, realistic and
goal-oriented jailbreak conversational strategies; and (3) a hierarchical
attack planner, which adaptively plans, serializes, and triggers attacks
tailored to specific LLM's vulnerabilities. Together, these contributions form
a unified framework -- combining assessment, attack generation, and strategic
planning -- to comprehensively evaluate and expose weaknesses in LLMs'
robustness. Extensive evaluation is conducted to systematically assess and
analyze the performance of the overall system and each component. Experimental
results demonstrate that our multi-turn adversarial attack strategies can
successfully lead state-of-the-art LLMs to produce unsafe generations,
highlighting the pressing need for more research into enhancing LLM's
robustness.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [122] [Crossing Domains without Labels: Distant Supervision for Term Extraction](https://arxiv.org/abs/2510.06838)
*Elena Senger,Yuri Campbell,Rob van der Goot,Barbara Plank*

Main category: cs.IR

TL;DR: 提出基于LLM的自动术语提取新方法，通过生成伪标签+微调模型+后处理策略，在跨领域场景下平均提升10%性能，并发布新数据集


<details>
  <summary>Details</summary>
Motivation: 现有ATE方法依赖人工标注且跨领域性能差，需开发更鲁棒、可扩展的解决方案及现实评估标准

Method: 1. 用LLM生成通用/科学领域伪标签 2. 微调首个ATE专用LLM 3. 引入轻量级后处理机制提升文档一致性

Result: 在7个领域中5个超越现有方法，平均提升10个百分点，性能接近GPT-4o教师模型

Conclusion: 通过构建多领域评测基准和可扩展的LLM训练框架，为ATE研究提供新数据集和优化模型

Abstract: Automatic Term Extraction (ATE) is a critical component in downstream NLP
tasks such as document tagging, ontology construction and patent analysis.
Current state-of-the-art methods require expensive human annotation and
struggle with domain transfer, limiting their practical deployment. This
highlights the need for more robust, scalable solutions and realistic
evaluation settings. To address this, we introduce a comprehensive benchmark
spanning seven diverse domains, enabling performance evaluation at both the
document- and corpus-levels. Furthermore, we propose a robust LLM-based model
that outperforms both supervised cross-domain encoder models and few-shot
learning baselines and performs competitively with its GPT-4o teacher on this
benchmark. The first step of our approach is generating psuedo-labels with this
black-box LLM on general and scientific domains to ensure generalizability.
Building on this data, we fine-tune the first LLMs for ATE. To further enhance
document-level consistency, oftentimes needed for downstream tasks, we
introduce lightweight post-hoc heuristics. Our approach exceeds previous
approaches on 5/7 domains with an average improvement of 10 percentage points.
We release our dataset and fine-tuned models to support future research in this
area.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [123] [XLSR-Kanformer: A KAN-Intergrated model for Synthetic Speech Detection](https://arxiv.org/abs/2510.06706)
*Phuong Tuan Dat,Tran Huy Dat*

Main category: cs.SD

TL;DR: 提出使用Kolmogorov-Arnold Network替代XLSR-Conformer中的MLP层，在ASVspoof2021数据集上相对提升60.55% EER性能，并在21LA子集达到0.70% EER。


<details>
  <summary>Details</summary>
Motivation: 语音合成技术进步导致欺骗攻击威胁加剧，现有基于自监督学习的检测系统（如XLSR-Conformer）仍有架构改进空间。

Method: 用基于Kolmogorov-Arnold表示定理的KAN网络替代传统MLP层，实验验证该方法在不同自监督学习架构中的鲁棒性。

Result: 在LA和DF测试集相对提升60.55% EER，21LA子集达到0.70% EER；方法对多种SSL架构有效。

Conclusion: KAN与SSL模型的整合为合成语音检测提供了新方向，显著提升现有系统对抗欺骗攻击的能力。

Abstract: Recent advancements in speech synthesis technologies have led to increasingly
sophisticated spoofing attacks, posing significant challenges for automatic
speaker verification systems. While systems based on self-supervised learning
(SSL) models, particularly the XLSR-Conformer architecture, have demonstrated
remarkable performance in synthetic speech detection, there remains room for
architectural improvements. In this paper, we propose a novel approach that
replaces the traditional Multi-Layer Perceptron (MLP) in the XLSR-Conformer
model with a Kolmogorov-Arnold Network (KAN), a powerful universal approximator
based on the Kolmogorov-Arnold representation theorem. Our experimental results
on ASVspoof2021 demonstrate that the integration of KAN to XLSR-Conformer model
can improve the performance by 60.55% relatively in Equal Error Rate (EER) LA
and DF sets, further achieving 0.70% EER on the 21LA set. Besides, the proposed
replacement is also robust to various SSL architectures. These findings suggest
that incorporating KAN into SSL-based models is a promising direction for
advances in synthetic speech detection.

</details>


### [124] [AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding and Efficiency in Audio LLMs](https://arxiv.org/abs/2510.07293)
*Peize He,Zichen Wen,Yubo Wang,Yuxuan Wang,Xiaoqian Liu,Jiajie Huang,Zehui Lei,Zhuangcheng Gu,Xiangqi Jin,Jiabing Yang,Kai Li,Zhifei Liu,Weijia Li,Cunxiang Wang,Conghui He,Linfeng Zhang*

Main category: cs.SD

TL;DR: 提出AudioMarathon基准测试，用于评估长音频理解与推理效率，揭示当前模型在长音频处理中的性能瓶颈


<details>
  <summary>Details</summary>
Motivation: 现有音频基准测试仅覆盖短片段，无法评估模型在真实长上下文场景下的表现，制约了LALMs的实际应用能力

Method: 构建包含长音频输入（90-300秒）、全领域覆盖（语音/声音/音乐）和多跳推理任务的基准体系，并评估加速技术（token剪枝/KV缓存淘汰）

Result: 实验显示模型性能随音频长度显著下降，现有加速技术存在准确率与效率的trade-off，暴露当前模型在时序推理和内存效率上的不足

Conclusion: AudioMarathon将推动开发具有长程时序推理能力和高效内存架构的新型音频理解模型，突破现有LALMs的局限性

Abstract: Processing long-form audio is a major challenge for Large Audio Language
models (LALMs). These models struggle with the quadratic cost of attention
($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio
benchmarks are built mostly from short clips and do not evaluate models in
realistic long context settings. To address this gap, we introduce
AudioMarathon, a benchmark designed to evaluate both understanding and
inference efficiency on long-form audio. AudioMarathon provides a diverse set
of tasks built upon three pillars: long-context audio inputs with durations
ranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of
2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,
sound, and music, and complex reasoning that requires multi-hop inference. We
evaluate state-of-the-art LALMs and observe clear performance drops as audio
length grows. We also study acceleration techniques and analyze the trade-offs
of token pruning and KV cache eviction. The results show large gaps across
current LALMs and highlight the need for better temporal reasoning and
memory-efficient architectures. We believe AudioMarathon will drive the audio
and multimodal research community to develop more advanced audio understanding
models capable of solving complex audio tasks.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [125] [Machines in the Crowd? Measuring the Footprint of Machine-Generated Text on Reddit](https://arxiv.org/abs/2510.07226)
*Lucio La Cava,Luca Maria Aiello,Andrea Tagarelli*

Main category: cs.SI

TL;DR: 研究首次大规模分析Reddit平台上的机器生成文本（MGT），发现其分布不均但用户参与度与人类内容相当甚至更高


<details>
  <summary>Details</summary>
Motivation: 探究生成式人工智能（MGT）在社交媒体中的整合现状及其对在线社区的影响

Method: 使用统计检测方法分析2022-2024年51个Reddit核心子社区（信息获取/社会支持/讨论类）的文本数据

Result: 1. MGT总体占比约9%（技术知识/社会支持类社区）
2. 呈现AI助手特有的温暖与地位语言特征
3. 互动水平与人类内容相当甚至更高

Conclusion: AI生成文本正成为在线社交话语的有机组成部分，需关注平台治理与检测策略的协同发展

Abstract: Generative Artificial Intelligence is reshaping online communication by
enabling large-scale production of Machine-Generated Text (MGT) at low cost.
While its presence is rapidly growing across the Web, little is known about how
MGT integrates into social media environments. In this paper, we present the
first large-scale characterization of MGT on Reddit. Using a state-of-the-art
statistical method for detection of MGT, we analyze over two years of activity
(2022-2024) across 51 subreddits representative of Reddit's main community
types such as information seeking, social support, and discussion. We study the
concentration of MGT across communities and over time, and compared MGT to
human-authored text in terms of social signals it expresses and engagement it
receives. Our very conservative estimate of MGT prevalence indicates that
synthetic text is marginally present on Reddit, but it can reach peaks of up to
9% in some communities in some months. MGT is unevenly distributed across
communities, more prevalent in subreddits focused on technical knowledge and
social support, and often concentrated in the activity of a small fraction of
users. MGT also conveys distinct social signals of warmth and status giving
typical of language of AI assistants. Despite these stylistic differences, MGT
achieves engagement levels comparable than human-authored content and in a few
cases even higher, suggesting that AI-generated text is becoming an organic
component of online social discourse. This work offers the first perspective on
the MGT footprint on Reddit, paving the way for new investigations involving
platform governance, detection strategies, and community dynamics.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [126] [GPT-5 Model Corrected GPT-4V's Chart Reading Errors, Not Prompting](https://arxiv.org/abs/2510.06782)
*Kaichun Yang,Jian Chen*

Main category: cs.HC

TL;DR: 研究发现模型架构主导图表任务推理准确率，GPT-5显著优于GPT-4V，提示方法改进效果有限


<details>
  <summary>Details</summary>
Motivation: 评估零样本LLM模型及提示方法对图表阅读任务的影响，比较不同架构模型（GPT-5与GPT-4V）在困难案例中的表现差异

Method: 通过107个可视化问题开展定量评估，测试两种模型在GPT-4V失败案例中的表现，对比模型架构与提示变体的影响

Result: GPT-5将准确率从GPT-4V的失败基准提升71.4%，而不同提示策略仅带来±4.3%的波动

Conclusion: 模型架构改进是提升图表理解能力的关键因素，提示工程优化效果存在天花板，未来研究应优先考虑模型架构升级

Abstract: We present a quantitative evaluation to understand the effect of zero-shot
large-language model (LLMs) and prompting uses on chart reading tasks. We asked
LLMs to answer 107 visualization questions to compare inference accuracies
between the agentic GPT-5 and multimodal GPT-4V, for difficult image instances,
where GPT-4V failed to produce correct answers. Our results show that model
architecture dominates the inference accuracy: GPT5 largely improved accuracy,
while prompt variants yielded only small effects. Pre-registration of this work
is available here:
https://osf.io/u78td/?view_only=6b075584311f48e991c39335c840ded3; the Google
Drive materials are
here:https://drive.google.com/file/d/1ll8WWZDf7cCNcfNWrLViWt8GwDNSvVrp/view.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [127] [CML-Bench: A Framework for Evaluating and Enhancing LLM-Powered Movie Scripts Generation](https://arxiv.org/abs/2510.06231)
*Mingzhe Zheng,Dingjie Song,Guanyu Zhou,Jun You,Jiahao Zhan,Xuran Ma,Xinyuan Song,Ser-Nam Lim,Qifeng Chen,Harry Yang*

Main category: cs.CV

TL;DR: 尽管大型语言模型擅长生成结构化文本，但缺乏电影剧本所需的情感叙事深度。研究提出CML-Dataset、CML-Bench评估体系和CML-Instruction提示策略，有效提升LLM生成剧本质量。


<details>
  <summary>Details</summary>
Motivation: LLMs生成的剧本虽结构完整但缺乏电影艺术所需的'灵魂'（情感深度与叙事连贯性），需建立量化评估标准改善生成效果。

Method: 1.构建CML-Dataset（含人类优质剧本片段与摘要）→2.分析剧本多镜头连续性/叙事结构→3.提炼DC/CC/PR三维度评估体系→4.设计CML-Instruction提示策略指导LLM生成

Result: CML-Bench有效区分人类/LLM剧本质量（人类剧本得分更高），CML-Instruction使LLM生成剧本质量提升且更符合人类偏好（实验验证有效性）

Conclusion: 通过结构化评估体系与针对性提示策略，可引导LLMs生成兼具逻辑性与电影艺术性的高质量剧本，弥合AI生成内容与人类创作间的叙事鸿沟。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in
generating highly structured texts. However, while exhibiting a high degree of
structural organization, movie scripts demand an additional layer of nuanced
storytelling and emotional depth-the 'soul' of compelling cinema-that LLMs
often fail to capture. To investigate this deficiency, we first curated
CML-Dataset, a dataset comprising (summary, content) pairs for Cinematic Markup
Language (CML), where 'content' consists of segments from esteemed,
high-quality movie scripts and 'summary' is a concise description of the
content. Through an in-depth analysis of the intrinsic multi-shot continuity
and narrative structures within these authentic scripts, we identified three
pivotal dimensions for quality assessment: Dialogue Coherence (DC), Character
Consistency (CC), and Plot Reasonableness (PR). Informed by these findings, we
propose the CML-Bench, featuring quantitative metrics across these dimensions.
CML-Bench effectively assigns high scores to well-crafted, human-written
scripts while concurrently pinpointing the weaknesses in screenplays generated
by LLMs. To further validate our benchmark, we introduce CML-Instruction, a
prompting strategy with detailed instructions on character dialogue and event
logic, to guide LLMs to generate more structured and cinematically sound
scripts. Extensive experiments validate the effectiveness of our benchmark and
demonstrate that LLMs guided by CML-Instruction generate higher-quality
screenplays, with results aligned with human preferences.

</details>


### [128] [Evaluating LLMs for Historical Document OCR: A Methodological Framework for Digital Humanities](https://arxiv.org/abs/2510.06743)
*Maria Levchenko*

Main category: cs.CV

TL;DR: 提出了评估大语言模型在历史文献OCR中的新方法论，揭示模型性能与过时字符插入问题


<details>
  <summary>Details</summary>
Motivation: 传统OCR评估指标无法有效检测历史文档数字化中时间偏差和时期特异性错误，影响历史语料库质量

Method: 使用18世纪俄国民用字体文本，设计HCPR/AIR指标，测试12个多模态LLM并进行污染控制/稳定性评估

Result: Gemini/Qwen优于传统OCR但存在时代错位插入古字符现象，后OCR校正反而降低准确性

Conclusion: 该评估体系为历史文献数字化提供模型选择指南，揭示过度历史化与后校正的负面效果

Abstract: Digital humanities scholars increasingly use Large Language Models for
historical document digitization, yet lack appropriate evaluation frameworks
for LLM-based OCR. Traditional metrics fail to capture temporal biases and
period-specific errors crucial for historical corpus creation. We present an
evaluation methodology for LLM-based historical OCR, addressing contamination
risks and systematic biases in diplomatic transcription. Using 18th-century
Russian Civil font texts, we introduce novel metrics including Historical
Character Preservation Rate (HCPR) and Archaic Insertion Rate (AIR), alongside
protocols for contamination control and stability testing. We evaluate 12
multimodal LLMs, finding that Gemini and Qwen models outperform traditional OCR
while exhibiting over-historicization: inserting archaic characters from
incorrect historical periods. Post-OCR correction degrades rather than improves
performance. Our methodology provides digital humanities practitioners with
guidelines for model selection and quality assessment in historical corpus
digitization.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [129] [Asking For It: Question-Answering for Predicting Rule Infractions in Online Content Moderation](https://arxiv.org/abs/2510.06350)
*Mattia Samory,Diana Pamfile,Andrew To,Shruti Phadke*

Main category: cs.CY

TL;DR: 提出基于问答框架ModQ的轻量化内容审核模型，可动态适配社区规则并实现高精度违规检测


<details>
  <summary>Details</summary>
Motivation: 在线社区规则存在跨平台差异大、动态演变且执行不一致的问题，传统审核方法难以适应透明化治理需求

Method: 开发抽取式和多选问答两种模型变体，利用Reddit及自建Lemmy数据集（含公开审核日志与规则描述）进行训练

Result: 模型在规则违规识别上优于SOTA基线（准确率提升5-8%），参数量仅1.3B且具备规则追溯能力

Conclusion: ModQ框架实现跨社区规则迁移学习，支持低资源/动态治理场景，为自动化内容审核提供可解释解决方案

Abstract: Online communities rely on a mix of platform policies and community-authored
rules to define acceptable behavior and maintain order. However, these rules
vary widely across communities, evolve over time, and are enforced
inconsistently, posing challenges for transparency, governance, and automation.
In this paper, we model the relationship between rules and their enforcement at
scale, introducing ModQ, a novel question-answering framework for
rule-sensitive content moderation. Unlike prior classification or
generation-based approaches, ModQ conditions on the full set of community rules
at inference time and identifies which rule best applies to a given comment. We
implement two model variants - extractive and multiple-choice QA - and train
them on large-scale datasets from Reddit and Lemmy, the latter of which we
construct from publicly available moderation logs and rule descriptions. Both
models outperform state-of-the-art baselines in identifying moderation-relevant
rule violations, while remaining lightweight and interpretable. Notably, ModQ
models generalize effectively to unseen communities and rules, supporting
low-resource moderation settings and dynamic governance environments.

</details>
