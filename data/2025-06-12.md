<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 63]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.CV](#cs.CV) [Total: 5]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.SD](#cs.SD) [Total: 2]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.IR](#cs.IR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [LLM-as-a-qualitative-judge: automating error analysis in natural language generation](https://arxiv.org/abs/2506.09147)
*Nadezhda Chirkova,Tunde Oluwaseyi Ajayi,Seth Aycock,Zain Muhammad Mujahid,Vladana Perlić,Ekaterina Borisova,Markarit Vartampetian*

Main category: cs.CL

TL;DR: 提出LLM-as-a-qualitative-judge方法，通过结构化问题报告替代数值评分，为NLG系统改进提供具体方向


<details>
  <summary>Details</summary>
Motivation: 传统LLM评估仅提供数值评分，缺乏对系统改进的具体指导。需要定性分析方法帮助开发者识别NLG系统的具体缺陷

Method: 1. 对每个输出实例进行开放式问题分析
2. 使用累积算法对发现的问题进行聚类整合

Result: 在12个NLG数据集上的实验显示：
- 正确识别实例级问题（准确率2/3）
- 生成的问题类型报告与人工标注高度相似

Conclusion: 该方法成功将LLM转换为系统诊断工具，通过结构化问题报告为NLG系统优化提供可操作见解，推动质量改进导向的评估范式发展

Abstract: Prompting large language models (LLMs) to evaluate generated text, known as
LLM-as-a-judge, has become a standard evaluation approach in natural language
generation (NLG), but is primarily used as a quantitative tool, i.e. with
numerical scores as main outputs. In this work, we propose
LLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main
output being a structured report of common issue types in the NLG system
outputs. Our approach is targeted at providing developers with meaningful
insights on what improvements can be done to a given NLG system and consists of
two main steps, namely open-ended per-instance issue analysis and clustering of
the discovered issues using an intuitive cumulative algorithm. We also
introduce a strategy for evaluating the proposed approach, coupled with ~300
annotations of issues in instances from 12 NLG datasets. Our results show that
LLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3
cases and is capable of producing error type reports resembling the reports
composed by human annotators. Our code and data are publicly available at
https://github.com/tunde-ajayi/llm-as-a-qualitative-judge.

</details>


### [2] [PHRASED: Phrase Dictionary Biasing for Speech Translation](https://arxiv.org/abs/2506.09175)
*Peidong Wang,Jian Xue,Rui Zhao,Junkun Chen,Aswin Shanmugam Subramanian,Jinyu Li*

Main category: cs.CL

TL;DR: 提出基于短语词典的偏置方法，在流式语音翻译和多模态大模型中分别实现21%和85%的短语召回率提升


<details>
  <summary>Details</summary>
Motivation: 短语在对话理解中至关重要，但由于训练数据中出现频率低，语音翻译任务中的短语正确翻译存在挑战

Method: 使用源语言到目标语言的短语映射对构建短语词典偏置方法，应用于流式语音翻译模型和多模态大语言模型

Result: 流式模型相对短语列表偏置提升21%，多模态大模型短语召回率提升85%

Conclusion: 短语词典偏置方法显著提升不同架构模型的短语翻译能力，证明了外部短语知识的有效整合

Abstract: Phrases are essential to understand the core concepts in conversations.
However, due to their rare occurrence in training data, correct translation of
phrases is challenging in speech translation tasks. In this paper, we propose a
phrase dictionary biasing method to leverage pairs of phrases mapping from the
source language to the target language. We apply the phrase dictionary biasing
method to two types of widely adopted models, a transducer-based streaming
speech translation model and a multimodal large language model. Experimental
results show that the phrase dictionary biasing method outperforms phrase list
biasing by 21% relatively for the streaming speech translation model. In
addition, phrase dictionary biasing enables multimodal large language models to
use external phrase information, achieving 85% relative improvement in phrase
recall.

</details>


### [3] [A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs](https://arxiv.org/abs/2506.09218)
*Bruno Ferenc Šegedin*

Main category: cs.CL

TL;DR: 提出通过缩小全连接层瓶颈并绕过其输入随机特征图的新方法，证明卷积层能动态泛化语音依赖关系


<details>
  <summary>Details</summary>
Motivation: 探究卷积神经网络在原始音频训练中的词汇无关泛化能力，验证全连接层瓶颈对音系概括的影响

Method: 1. 将全连接层通道从1024压缩至8
2. 开发绕过全连接层、输入随机特征图到卷积块的新生成技术

Result: 使用随机特征图生成的音频与全连接层生成结果具有相同的音系限制偏置

Conclusion: 卷积层具备独立于FC层学得词汇配置的动态语音依赖泛化能力

Abstract: The ability of deep neural networks (DNNs) to represent phonotactic
generalizations derived from lexical learning remains an open question. This
study (1) investigates the lexically-invariant generalization capacity of
generative convolutional neural networks (CNNs) trained on raw audio waveforms
of lexical items and (2) explores the consequences of shrinking the
fully-connected layer (FC) bottleneck from 1024 channels to 8 before training.
Ultimately, a novel technique for probing a model's lexically-independent
generalizations is proposed that works only under the narrow FC bottleneck:
generating audio outputs by bypassing the FC and inputting randomized feature
maps into the convolutional block. These outputs are equally biased by a
phonotactic restriction in training as are outputs generated with the FC. This
result shows that the convolutional layers can dynamically generalize phonetic
dependencies beyond lexically-constrained configurations learned by the FC.

</details>


### [4] [Extrapolation by Association: Length Generalization Transfer in Transformers](https://arxiv.org/abs/2506.09251)
*Ziyang Cai,Nayoung Lee,Avi Schwarzschild,Samet Oymak,Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: Transformer模型通过任务关联实现长度泛化的跨任务迁移，注意力头复用是核心机制


<details>
  <summary>Details</summary>
Motivation: 探究Transformer语言模型如何从短序列训练泛化到长序列推理，揭示其跨任务泛化能力的形成机制

Method: 采用算术运算/字符串变换/迷宫导航等算法任务，通过联合训练和注意力头分析揭示机制

Result: 任务间的长度泛化能力可迁移，预训练模型存在可重用的计算支架，注意力头复用与泛化能力正相关

Conclusion: Transformer的泛化能力源于跨任务的组合式结构复用，预训练形成的计算支架支持下游任务外推

Abstract: Transformer language models have demonstrated impressive generalization
capabilities in natural language domains, yet we lack a fine-grained
understanding of how such generalization arises. In this paper, we investigate
length generalization--the ability to extrapolate from shorter to longer
inputs--through the lens of \textit{task association}. We find that length
generalization can be \textit{transferred} across related tasks. That is,
training a model with a longer and related auxiliary task can lead it to
generalize to unseen and longer inputs from some other target task. We
demonstrate this length generalization transfer across diverse algorithmic
tasks, including arithmetic operations, string transformations, and maze
navigation. Our results show that transformer models can inherit generalization
capabilities from similar tasks when trained jointly. Moreover, we observe
similar transfer effects in pretrained language models, suggesting that
pretraining equips models with reusable computational scaffolding that
facilitates extrapolation in downstream settings. Finally, we provide initial
mechanistic evidence that length generalization transfer correlates with the
re-use of the same attention heads between the tasks. Together, our findings
deepen our understanding of how transformers generalize to out-of-distribution
inputs and highlight the compositional reuse of inductive structure across
tasks.

</details>


### [5] [Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat](https://arxiv.org/abs/2506.09259)
*Zhuofang Li,Rafal Kocielnik,Fereshteh Soltani,Penphob,Boonyarungsrit,Animashree Anandkumar,R. Michael Alvarez*

Main category: cs.CL

TL;DR: 提出SAAM模型提升游戏聊天中亲社会行为分类效果，相比现有最佳技术提升7.9%


<details>
  <summary>Details</summary>
Motivation: 现有研究过度关注毒性内容检测，亲社会行为识别缺乏数据资源和有效模型

Method: 结合无监督发现与领域专家协作，开发基于自锚定注意力机制（SAAM）的模型，利用全训练集作为锚点提升小样本表现

Result: 在《使命召唤》游戏聊天数据验证中，SAAM模型实现7.9%的性能提升

Conclusion: 首个游戏聊天亲社会行为自动分类系统，推动内容审核从单纯惩罚毒性转向积极培育良性互动

Abstract: Millions of players engage daily in competitive online games, communicating
through in-game chat. Prior research has focused on detecting relatively small
volumes of toxic content using various Natural Language Processing (NLP)
techniques for the purpose of moderation. However, recent studies emphasize the
importance of detecting prosocial communication, which can be as crucial as
identifying toxic interactions. Recognizing prosocial behavior allows for its
analysis, rewarding, and promotion. Unlike toxicity, there are limited
datasets, models, and resources for identifying prosocial behaviors in
game-chat text. In this work, we employed unsupervised discovery combined with
game domain expert collaboration to identify and categorize prosocial player
behaviors from game chat. We further propose a novel Self-Anchored Attention
Model (SAAM) which gives 7.9% improvement compared to the best existing
technique. The approach utilizes the entire training set as "anchors" to help
improve model performance under the scarcity of training data. This approach
led to the development of the first automated system for classifying prosocial
behaviors in in-game chats, particularly given the low-resource settings where
large-scale labeled data is not available. Our methodology was applied to one
of the most popular online gaming titles - Call of Duty(R): Modern
Warfare(R)II, showcasing its effectiveness. This research is novel in applying
NLP techniques to discover and classify prosocial behaviors in player in-game
chat communication. It can help shift the focus of moderation from solely
penalizing toxicity to actively encouraging positive interactions on online
platforms.

</details>


### [6] [Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models](https://arxiv.org/abs/2506.09277)
*Milan Bhan,Jean-Noel Vittaut,Nicolas Chesneau,Sarath Chandar,Marie-Jeanne Lesot*

Main category: cs.CL

TL;DR: 提出通过比较模型隐藏状态解释与自生成解释来量化评估LLM自解释忠实度的新框架


<details>
  <summary>Details</summary>
Motivation: 现有自解释忠实度评估方法主要依赖行为测试或计算模块分析，缺乏对模型神经活动的直接观察，导致解释可信度存疑

Method: 开发灵活框架，通过直接对比模型自生成的自然语言解释与内部隐藏状态解释来定量测量忠实度

Result: 框架具备通用性，通过建立自解释与模型推理的直接关联，为理解自解释忠实度提供新视角

Conclusion: 该方法推进了对自解释机制的理解，为生成更可信的解释提供了理论基础和技术构建模块

Abstract: Large Language Models (LLM) have demonstrated the capability of generating
free text self Natural Language Explanation (self-NLE) to justify their
answers. Despite their logical appearance, self-NLE do not necessarily reflect
the LLM actual decision-making process, making such explanations unfaithful.
While existing methods for measuring self-NLE faithfulness mostly rely on
behavioral tests or computational block identification, none of them examines
the neural activity underlying the model's reasoning. This work introduces a
novel flexible framework for quantitatively measuring the faithfulness of
LLM-generated self-NLE by directly comparing the latter with interpretations of
the model's internal hidden states. The proposed framework is versatile and
provides deep insights into self-NLE faithfulness by establishing a direct
connection between self-NLE and model reasoning. This approach advances the
understanding of self-NLE faithfulness and provides building blocks for
generating more faithful self-NLE.

</details>


### [7] [$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding](https://arxiv.org/abs/2506.09301)
*Cesare Spinoso-Di Piano,David Austin,Pablo Piantanida,Jackie Chi Kit Cheung*

Main category: cs.CL

TL;DR: 提出修辞策略感知的RSA框架(RSA)²，通过建模说话者修辞策略解释非字面语言，结合LLM在反讽解释任务上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有RSA框架无法有效解释比喻语言（如反讽），或需要针对特定场景建模说话者动机。需要更通用的非字面语言解释框架。

Method: 在RSA框架中引入修辞策略维度，通过概率建模将字面意义映射到修辞策略空间，结合LLM实现策略推理。

Result: 在自建反讽数据集PragMega+上取得最先进性能，人类对齐度显著提升。

Conclusion: (RSA)²框架首次实现无需动机建模的通用比喻语言解释，为计算语用学提供新范式。

Abstract: Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in
human communication, resulting in utterances where the literal and the intended
meanings do not match. The Rational Speech Act (RSA) framework, which
explicitly models speaker intentions, is the most widespread theory of
probabilistic pragmatics, but existing implementations are either unable to
account for figurative expressions or require modeling the implicit motivations
for using figurative language (e.g., to express joy or annoyance) in a
setting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware
RSA $(RSA)^2$ framework which models figurative language use by considering a
speaker's employed rhetorical strategy. We show that $(RSA)^2$ enables
human-compatible interpretations of non-literal utterances without modeling a
speaker's motivations for being non-literal. Combined with LLMs, it achieves
state-of-the-art performance on the ironic split of PragMega+, a new irony
interpretation dataset introduced in this study.

</details>


### [8] [Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models](https://arxiv.org/abs/2506.09315)
*Yao Xiao,Heidi Christensen,Stefan Goetze*

Main category: cs.CL

TL;DR: 本研究通过微调Mistral-7B大语言模型改进配对困惑度方法，将阿尔茨海默病检测准确率提升3.33%-6.35%，并提供可解释的决策边界。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病(AD)常伴随语言能力衰退，现有检测方法存在准确率不足和决策过程不透明的问题。

Method: 扩展配对困惑度方法，采用指令微调版Mistral-7B模型，通过对比模型生成响应与人类响应的语言模式进行分析。

Result: 准确率超越现有最佳方法3.33%，相比ADReSS 2020基准提升6.35%，模型成功捕捉AD患者特殊语言模式。

Conclusion: 该方法兼具高精度和可解释性，模型习得的语言特征为新型诊断工具开发和数据增强提供了可能方向。

Abstract: Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive
decline that commonly impacts language ability. This work extends the paired
perplexity approach to detecting AD by using a recent large language model
(LLM), the instruction-following version of Mistral-7B. We improve accuracy by
an average of 3.33% over the best current paired perplexity method and by 6.35%
over the top-ranked method from the ADReSS 2020 challenge benchmark. Our
further analysis demonstrates that the proposed approach can effectively detect
AD with a clear and interpretable decision boundary in contrast to other
methods that suffer from opaque decision-making processes. Finally, by
prompting the fine-tuned LLMs and comparing the model-generated responses to
human responses, we illustrate that the LLMs have learned the special language
patterns of AD speakers, which opens up possibilities for novel methods of
model interpretation and data augmentation.

</details>


### [9] [Towards Efficient and Effective Alignment of Large Language Models](https://arxiv.org/abs/2506.09329)
*Yuxin Jiang*

Main category: cs.CL

TL;DR: 提出Lion、WebR、LTE、BMC和FollowBench等系统方法，从数据构建、训练优化到评估体系全面改进大语言模型对齐技术


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法存在数据依赖人工标注/闭源模型、训练过程信息整合效率低、评估缺乏细粒度约束跟踪等问题

Method: 1. 数据构建：Lion对抗蒸馏框架生成挑战性指令，WebR从网页自动合成指令数据
2. 训练优化：LTE元学习框架实现实时知识更新，BMC改进DPO捕获token级关联
3. 评估体系：构建多层级约束遵循基准FollowBench

Result: 实现零样本推理SOTA、数据多样性提升3倍、数学推理准确率提升7.2%、发现现有模型在复杂约束遵循中的关键缺陷

Conclusion: 通过系统性方法论创新，在模型对齐的数据构建、训练范式、评估体系三个维度取得突破，为LLM的可靠部署提供技术基础

Abstract: Large language models (LLMs) exhibit remarkable capabilities across diverse
tasks, yet aligning them efficiently and effectively with human expectations
remains a critical challenge. This thesis advances LLM alignment by introducing
novel methodologies in data collection, training, and evaluation. We first
address alignment data collection. Existing approaches rely heavily on manually
curated datasets or proprietary models. To overcome these limitations, we
propose Lion, an adversarial distillation framework that iteratively refines
training data by identifying and generating challenging instructions, enabling
state-of-the-art zero-shot reasoning. Additionally, we introduce Web
Reconstruction (WebR), a fully automated framework that synthesizes
instruction-tuning data directly from raw web documents, significantly
improving data diversity and scalability over existing synthetic data methods.
Next, we enhance alignment training through novel optimization techniques. We
develop Learning to Edit (LTE), a framework that enables LLMs to efficiently
integrate new knowledge while preserving existing information. LTE leverages
meta-learning to improve both real-time and batch knowledge updates.
Furthermore, we introduce Bridging and Modeling Correlations (BMC), a
refinement of Direct Preference Optimization (DPO) that explicitly captures
token-level correlations in preference data, leading to superior alignment
across QA and mathematical reasoning tasks. Finally, we tackle the challenge of
evaluating alignment. Existing benchmarks emphasize response quality but
overlook adherence to specific constraints. To bridge this gap, we introduce
FollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to
follow complex constraints across diverse instruction types. Our results expose
key weaknesses in current models' constraint adherence, offering insights for
future improvements.

</details>


### [10] [Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation](https://arxiv.org/abs/2506.09331)
*Arjun Vaithilingam Sudhakar*

Main category: cs.CL

TL;DR: 研究通过多智能体强化学习框架，验证大语言模型是否具备心理理论能力以促进人机协作。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs的心理理论能力是实现有效人机协作的基础，对构建混合智能系统有重要意义。

Method: 利用基于LLM的自然语言交互智能体，在协作式多智能体强化学习环境中模拟人类社交推理机制。

Result: LLM智能体展现出适应不同协作伙伴的潜力，为构建无缝协作的人机系统提供技术路径。

Conclusion: 该研究为未来人机交互系统的设计提供了理论基础，预示了语言模型在复杂协作场景中的应用前景。

Abstract: Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot
generalization capabilities across complex natural language tasks, enabling
their widespread use as virtual assistants for diverse applications such as
translation and summarization. Despite being trained solely on large corpora of
text without explicit supervision on author intent, LLMs appear to infer the
underlying meaning of textual interactions. This raises a fundamental question:
can LLMs model and reason about the intentions of others, i.e., do they possess
a form of theory of mind? Understanding other's intentions is crucial for
effective collaboration, which underpins human societal success and is
essential for cooperative interactions among multiple agents, including humans
and autonomous systems. In this work, we investigate the theory of mind in LLMs
through the lens of cooperative multi-agent reinforcement learning (MARL),
where agents learn to collaborate via repeated interactions, mirroring human
social reasoning. Our approach aims to enhance artificial agent's ability to
adapt and cooperate with both artificial and human partners. By leveraging
LLM-based agents capable of natural language interaction, we move towards
creating hybrid human-AI systems that can foster seamless collaboration, with
broad implications for the future of human-artificial interaction.

</details>


### [11] [RePO: Replay-Enhanced Policy Optimization](https://arxiv.org/abs/2506.09340)
*Siheng Li,Zhanhui Zhou,Wai Lam,Chao Yang,Chaochao Lu*

Main category: cs.CL

TL;DR: RePO提出通过多样化回放策略利用离轨样本优化LLM强化学习，相比GRPO显著提升性能与数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法依赖同轨样本导致计算成本高、数据效率低，需通过离轨样本提升优化效率。

Method: 采用回放缓冲区存储历史样本，结合同轨/离轨样本实现多样化策略优化，每个prompt基于更广泛样本更新策略。

Result: 在7个数学推理基准上，Qwen2.5-Math-1.5B/Qwen3-1.7B分别实现18.4/4.1绝对分提升，计算成本仅增15%但有效优化步数提升48%。

Conclusion: RePO通过平衡同轨/离轨样本比例，在合理增加计算开销前提下显著提升模型性能与训练效率，为LLM强化学习提供新优化范式。

Abstract: Reinforcement learning (RL) is vital for optimizing large language models
(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages
using multiple on-policy outputs per prompt, leading to high computational
costs and low data efficiency. To address this, we introduce Replay-Enhanced
Policy Optimization (RePO), which leverages diverse replay strategies to
retrieve off-policy samples from a replay buffer, allowing policy optimization
based on a broader and more diverse set of samples for each prompt. Experiments
on five LLMs across seven mathematical reasoning benchmarks demonstrate that
RePO achieves absolute average performance gains of $18.4$ and $4.1$ points for
Qwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further
analysis indicates that RePO increases computational cost by $15\%$ while
raising the number of effective optimization steps by $48\%$ for Qwen3-1.7B,
with both on-policy and off-policy sample numbers set to $8$. The repository
can be accessed at https://github.com/SihengLi99/RePO.

</details>


### [12] [Latent Multi-Head Attention for Small Language Models](https://arxiv.org/abs/2506.09342)
*Sushant Mehta,Raj Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.CL

TL;DR: 潜在多头注意力(MLA)在小型语言模型中实现45% KV缓存内存降低+1.4倍推理加速，质量损失可忽略


<details>
  <summary>Details</summary>
Motivation: 探索小型语言模型使用潜在多头注意力架构的效率-质量平衡，解决内存受限场景部署问题

Method: 使用30M参数GPT模型在10万合成故事集上，对比标准MHA/MLA/MLA+RoPE三种架构

Result: MLA+RoPE(r=d/2)内存减少45%且验证损失仅增0.3%，推理速度提升1.4倍，GPT-4评估得分7.4/10

Conclusion: MLA+RoPE架构在小型模型中实现内存效率与模型质量的帕累托改进，旋转位置编码是关键成功因素

Abstract: We present the first comprehensive study of latent multi-head attention (MLA)
for small language models, revealing interesting efficiency-quality trade-offs.
Training 30M-parameter GPT models on 100,000 synthetic stories, we benchmark
three architectural variants: standard multi-head attention (MHA), MLA, and MLA
with rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE
with half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory
reduction while incurring only a 0.3% increase in validation loss (essentially
matching MHA quality)- a Pareto improvement for memory constrained deployment.
We further show that RoPE is crucial for MLA in small models: without it, MLA
underperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by
2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2
achieves a 1.4 times speedup over full-rank MLA while maintaining the memory
savings. GPT-4 evaluations corroborate perplexity results, with ours achieving
the highest quality scores (7.4/10) across grammar, creativity, and consistency
metrics. Code and models will be released upon acceptance.

</details>


### [13] [OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment](https://arxiv.org/abs/2506.09349)
*Chao-Hong Tan,Qian Chen,Wen Wang,Chong Deng,Qinglin Zhang,Luyao Cheng,Hai Yu,Xin Zhang,Xiang Lv,Tianyu Zhao,Chong Zhang,Yukun Ma,Yafeng Chen,Hui Wang,Jiaqing Liu,Jieping Ye*

Main category: cs.CL

TL;DR: OmniDRCA提出基于双分辨率语音表征和对比跨模态对齐的并行语音-文本联合建模框架，在SQA基准上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有语音生成方法存在模态割裂问题：独立生成语音标记导致文本生成不感知语音合成，或交错建模效率较低。需构建能实现语音-文本双向感知的高效联合建模框架。

Method: 1. 双分辨率语音表征（基础单元+细粒度单元）
2. 并行语音-文本联合自回归建模
3. 对比学习跨模态对齐增强音频理解

Result: 在Spoken QA基准上：
- 超越所有并行联合建模的基线模型（SOTA）
- 与交错模型相比达到竞争性表现
- 验证全双工会话场景扩展潜力

Conclusion: OmniDRCA通过创新的联合建模架构和对比对齐机制，有效提升多模态交互质量，为语音-文本联合建模开辟新方向，具备实际应用前景。

Abstract: Recent studies on end-to-end speech generation with large language models
(LLMs) have attracted significant community attention, with multiple works
extending text-based LLMs to generate discrete speech tokens. Existing
approaches primarily fall into two categories: (1) Methods that generate
discrete speech tokens independently without incorporating them into the LLM's
autoregressive process, resulting in text generation being unaware of
concurrent speech synthesis. (2) Models that generate interleaved or parallel
speech-text tokens through joint autoregressive modeling, enabling mutual
modality awareness during generation. This paper presents OmniDRCA, a parallel
speech-text foundation model based on joint autoregressive modeling, featuring
dual-resolution speech representations and contrastive cross-modal alignment.
Our approach processes speech and text representations in parallel while
enhancing audio comprehension through contrastive alignment. Experimental
results on Spoken Question Answering benchmarks demonstrate that OmniDRCA
establishes new state-of-the-art (SOTA) performance among parallel joint
speech-text modeling based foundation models, and achieves competitive
performance compared to interleaved models. Additionally, we explore the
potential of extending the framework to full-duplex conversational scenarios.

</details>


### [14] [DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts](https://arxiv.org/abs/2506.09351)
*Yuchen Feng,Bowen Shen,Naibin Gu,Jiaxuan Zhao,Peng Fu,Zheng Lin,Weiping Wang*

Main category: cs.CL

TL;DR: 提出DIVE方法，通过基于剪枝的专家多样性增强技术改进MoE架构LLM重建，在保持精度的同时显著提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有MoE重建方法忽视专家多样性导致冗余，而从头训练专家参数成本过高。

Method: 1. 领域亲和力挖掘
2. 基于不同校准集的FFN剪枝与重组
3. 对路由/专家/归一化模块的高效重训练

Result: 在相同激活参数量下，DIVE的准确率损失仅0.3%，训练效率提升4倍，优于现有剪枝和MoE重建方法。

Conclusion: 通过利用剪枝诱导的模型多样性，DIVE实现了成本效益与模型性能的更好平衡，为MoE架构优化提供新思路。

Abstract: Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture
achieve high cost-efficiency by selectively activating a subset of the
parameters. Despite the inference efficiency of MoE LLMs, the training of
extensive experts from scratch incurs substantial overhead, whereas
reconstructing a dense LLM into an MoE LLM significantly reduces the training
budget. However, existing reconstruction methods often overlook the diversity
among experts, leading to potential redundancy. In this paper, we come up with
the observation that a specific LLM exhibits notable diversity after being
pruned on different calibration datasets, based on which we present a
Diversity-Enhanced reconstruction method named DIVE. The recipe of DIVE
includes domain affinity mining, pruning-based expert reconstruction, and
efficient retraining. Specifically, the reconstruction includes pruning and
reassembly of the feed-forward network (FFN) module. After reconstruction, we
efficiently retrain the model on routers, experts and normalization modules. We
implement DIVE on Llama-style LLMs with open-source training corpora.
Experiments show that DIVE achieves training efficiency with minimal accuracy
trade-offs, outperforming existing pruning and MoE reconstruction methods with
the same number of activated parameters.

</details>


### [15] [Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL](https://arxiv.org/abs/2506.09359)
*Qingyun Zeng,Simin Ma,Arash Niknafs,Ashish Basran,Carol Szabo*

Main category: cs.CL

TL;DR: 利用大语言模型评估Text-to-SQL语义等价性，探索弱语义等效标准及挑战


<details>
  <summary>Details</summary>
Motivation: 传统方法难以判定用户模糊查询生成的SQL语义等价性，需建立更实用的评估标准

Method: 提出基于LLM的语义等效评估框架，分析常见SQL等效/非等效模式

Result: 揭示了LLM评估存在的局限性，包括复杂语义理解偏差和边界案例处理难题

Conclusion: LLM为SQL语义评估提供新思路，但需结合领域知识建立更系统的评估体系

Abstract: The rise of Large Language Models (LLMs) has significantly advanced
Text-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of
generated SQL remains a challenge, especially given ambiguous user queries and
multiple valid SQL interpretations. This paper explores using LLMs to assess
both semantic and a more practical "weak" semantic equivalence. We analyze
common patterns of SQL equivalence and inequivalence, discuss challenges in
LLM-based evaluation.

</details>


### [16] [COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content](https://arxiv.org/abs/2506.09367)
*Zhengyuan Liu,Stella Xin Yin,Dion Hoe-Lian Goh,Nancy F. Chen*

Main category: cs.CL

TL;DR: 提出COGENT框架，通过课程三要素控制生成符合年级的科学教育内容，经评估效果优于人工参考


<details>
  <summary>Details</summary>
Motivation: 生成式AI在教育场景存在课程标准对齐困难、阅读水平不稳定、STEM教育中科学术语与日常语言平衡不足等问题

Method: 整合科学概念/核心思想/学习目标三要素，控制文本长度/词汇/句式复杂度，采用基于好奇心的叙述方式

Result: 多维度评估显示生成内容在可读性和质量上达到或超越人工编写水平，LLM评估与专家分析结果一致

Conclusion: 该框架为大规模生成适应性强、高质量的教育资源提供了有效解决方案

Abstract: While Generative AI has demonstrated strong potential and versatility in
content generation, its application to educational contexts presents several
challenges. Models often fail to align with curriculum standards and maintain
grade-appropriate reading levels consistently. Furthermore, STEM education
poses additional challenges in balancing scientific explanations with everyday
language when introducing complex and abstract ideas and phenomena to younger
students. In this work, we propose COGENT, a curriculum-oriented framework for
generating grade-appropriate educational content. We incorporate three
curriculum components (science concepts, core ideas, and learning objectives),
control readability through length, vocabulary, and sentence complexity, and
adopt a ``wonder-based'' approach to increase student engagement and interest.
We conduct a multi-dimensional evaluation via both LLM-as-a-judge and human
expert analysis. Experimental results show that COGENT consistently produces
grade-appropriate passages that are comparable or superior to human references.
Our work establishes a viable approach for scaling adaptive and high-quality
learning resources.

</details>


### [17] [CoLMbo: Speaker Language Model for Descriptive Profiling](https://arxiv.org/abs/2506.09375)
*Massa Baali,Shuo Han,Syed Abdul Hannan,Purusottam Samal,Karanveer Singh,Soham Deshmukh,Rita Singh,Bhiksha Raj*

Main category: cs.CL

TL;DR: 提出CoLMbo模型，通过集成说话人编码器和提示条件机制，实现结构化人口属性捕捉和动态生成定制化的说话人特征描述。


<details>
  <summary>Details</summary>
Motivation: 现有说话人识别系统仅支持分类任务，缺乏对性别/方言/年龄等结构化属性的捕捉能力，且无法生成上下文丰富的描述。

Method: 结合说话人编码器与提示条件框架，通过用户定义提示动态适配新特征，支持方言区域变异和年龄特征等定制化描述生成。

Result: 在传统说话人画像和零样本场景中表现优异，在多样化数据集上实现说话人识别领域的显著突破。

Conclusion: 提示条件机制革新了说话人特征描述范式，通过动态适配能力显著扩展了说话人识别技术的应用边界。

Abstract: Speaker recognition systems are often limited to classification tasks and
struggle to generate detailed speaker characteristics or provide context-rich
descriptions. These models primarily extract embeddings for speaker
identification but fail to capture demographic attributes such as dialect,
gender, and age in a structured manner. This paper introduces CoLMbo, a Speaker
Language Model (SLM) that addresses these limitations by integrating a speaker
encoder with prompt-based conditioning. This allows for the creation of
detailed captions based on speaker embeddings. CoLMbo utilizes user-defined
prompts to adapt dynamically to new speaker characteristics and provides
customized descriptions, including regional dialect variations and age-related
traits. This innovative approach not only enhances traditional speaker
profiling but also excels in zero-shot scenarios across diverse datasets,
marking a significant advancement in the field of speaker recognition.

</details>


### [18] [Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024](https://arxiv.org/abs/2506.09381)
*Austin McCutcheon,Thiago E. A. de Oliveira,Aleksandr Zheleznov,Chris Brogly*

Main category: cs.CL

TL;DR: 研究通过传统集成学习（Bagging分类器88.1%准确率）和微调DistilBERT模型（90.3%准确率）对比，验证两者均可有效区分新闻标题质量，但需权衡预测性能与训练时间。


<details>
  <summary>Details</summary>
Motivation: 针对网络低质量新闻标题/链接泛滥问题，探索自动化区分高/低质量新闻标题的技术方案。

Method: 使用2018-2024年全球5754万条平衡数据集（高低质量各2877万条），提取115个语言特征，对比12个机器学习模型（含传统集成方法与微调DistilBERT），采用80/20训练测试集划分。

Result: 传统Bagging分类器表现优异（88.1%准确率/88.3% F1），微调DistilBERT达到最高准确率（90.3%）但训练耗时更长。

Conclusion: 基于NLP特征的传统模型与深度学习模型均可有效识别新闻质量，实际应用需根据对预测性能与训练效率的需求进行选择。

Abstract: The proliferation of online news enables potential widespread publication of
perceived low-quality news headlines/links. As a result, we investigated
whether it was possible to automatically distinguish perceived lower-quality
news headlines/links from perceived higher-quality headlines/links. We
evaluated twelve machine learning models on a binary, balanced dataset of
57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per
class) with 115 extracted linguistic features. Binary labels for each text were
derived from scores based on expert consensus regarding the respective news
domain quality. Traditional ensemble methods, particularly the bagging
classifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test
split). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20
train/test split) but required more training time. The results suggest that
both NLP features with traditional classifiers and deep learning models can
effectively differentiate perceived news headline/link quality, with some
trade-off between predictive performance and train time.

</details>


### [19] [Comparing human and LLM politeness strategies in free production](https://arxiv.org/abs/2506.09391)
*Haoran Zhao,Robert D. Hawkins*

Main category: cs.CL

TL;DR: 大语言模型（≥70B参数）能复现礼貌策略的核心偏好且人类更倾向其回答，但过度依赖负面礼貌策略可能引发误解，揭示AI语用对齐挑战。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否能像人类一样根据语境灵活运用正/负面礼貌策略（如赞美vs委婉），以及这种语用对齐对AI系统交流能力的影响。

Method: 通过约束性任务与开放式生成任务，结合计算语用学理论框架，对比人类与不同规模LLMs的礼貌策略选择，并进行语言学量化分析。

Result: 大模型成功复现经典语用偏好，开放式回答获人类青睐，但语言分析显示模型在积极语境中仍过度使用负面策略（如间接表达）。

Conclusion: 尽管LLMs展现出惊人的礼貌策略处理能力，其策略选择偏差可能导致社交误解，凸显AI系统深层语用对齐的重要性与复杂性。

Abstract: Polite speech poses a fundamental alignment challenge for large language
models (LLMs). Humans deploy a rich repertoire of linguistic strategies to
balance informational and social goals -- from positive approaches that build
rapport (compliments, expressions of interest) to negative strategies that
minimize imposition (hedging, indirectness). We investigate whether LLMs employ
a similarly context-sensitive repertoire by comparing human and LLM responses
in both constrained and open-ended production tasks. We find that larger models
($\ge$70B parameters) successfully replicate key preferences from the
computational pragmatics literature, and human evaluators surprisingly prefer
LLM-generated responses in open-ended contexts. However, further linguistic
analyses reveal that models disproportionately rely on negative politeness
strategies even in positive contexts, potentially leading to
misinterpretations. While modern LLMs demonstrate an impressive handle on
politeness strategies, these subtle differences raise important questions about
pragmatic alignment in AI systems.

</details>


### [20] [A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings](https://arxiv.org/abs/2506.09393)
*Xinyi Gao,Qiucheng Wu,Yang Zhang,Xuechen Liu,Kaizhi Qian,Ying Xu,Shiyu Chang*

Main category: cs.CL

TL;DR: 提出基于知识树结构的KT²框架，通过隐马尔可夫树模型和EM算法实现低资源环境下的高效知识追踪


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪方法在数据稀疏的课堂场景中表现受限，需要利用知识概念层次结构作为先验提升低资源条件下的预测效果

Method: 构建树状知识层次结构，采用隐马尔可夫树模型建模学生认知状态，通过EM算法估计知识掌握度，支持增量式实时更新

Result: 在在线低资源场景下，KT²模型在预测精度和个性化方面持续优于基线方法

Conclusion: 知识树结构有效提升数据稀疏条件下的追踪效果，为个性化教育技术提供新的建模范式

Abstract: Knowledge tracing (KT) aims to estimate a student's evolving knowledge state
and predict their performance on new exercises based on performance history.
Many realistic classroom settings for KT are typically low-resource in data and
require online updates as students' exercise history grows, which creates
significant challenges for existing KT approaches. To restore strong
performance under low-resource conditions, we revisit the hierarchical
knowledge concept (KC) information, which is typically available in many
classroom settings and can provide strong prior when data are sparse. We
therefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a
probabilistic KT framework that models student understanding over a
tree-structured hierarchy of knowledge concepts using a Hidden Markov Tree
Model. KT$^2$ estimates student mastery via an EM algorithm and supports
personalized prediction through an incremental update mechanism as new
responses arrive. Our experiments show that KT$^2$ consistently outperforms
strong baselines in realistic online, low-resource settings.

</details>


### [21] [Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models](https://arxiv.org/abs/2506.09408)
*Jui-Ming Yao,Hao-Yuan Chen,Zi-Xian Tang,Bing-Jia Tan,Sheng-Wei Peng,Bing-Cheng Xie,Shun-Feng Su*

Main category: cs.CL

TL;DR: 提出Token Constraint Decoding (TCD)方法，通过增强token级预测一致性显著提升语言模型在噪声输入下的鲁棒性，较小模型性能提升达39%


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在现实噪声输入场景下性能骤降的问题，提升安全关键领域应用可靠性

Method: 推理阶段算法TCD强制token预测对齐，结合提示工程(PE)优化，通过惩罚过自信输出来隐式正则化

Result: 在CommonsenseQA/MMLU等基准测试中，TCD+PE组合使Gemma3 1B等弱模型绝对性能提升最高达39%

Conclusion: TCD作为模型无关方法有效提升推理稳定性，为LLMs的实际部署提供新解决方案

Abstract: Large Language Models (LLMs) have demonstrated impressive performance on
multiple-choice question answering (MCQA) benchmarks, yet they remain highly
vulnerable to minor input perturbations. In this paper, we introduce and
evaluate Token Constraint Decoding (TCD). This simple yet effective
inference-time algorithm enforces alignment between token-level predictions to
enhance robustness in noisy settings. Through extensive experiments on
CommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired
with prompt engineering (PE) fixes, significantly restores performance degraded
by input noise, yielding up to +39\% absolute gains for weaker models like
Gemma3 1B. Penalty sweep analyses further reveal that TCD implicitly
regularizes overconfident outputs, with different models requiring distinct
penalty schedules to maximize resilience. Our findings establish TCD as a
practical, model-agnostic approach for improving reasoning stability under
real-world imperfections and pave the way for more reliable deployment of LLMs
in safety-critical or user-facing applications.

</details>


### [22] [PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering](https://arxiv.org/abs/2506.09414)
*Xiujun Zhou,Pingjian Zhang,Deyou Tang*

Main category: cs.CL

TL;DR: 提出提示引导的生成框架PGDA-KGQA，通过单跳伪问题生成、语义保留重写和反向路径探索三种数据增强策略，显著提升知识图谱问答性能


<details>
  <summary>Details</summary>
Motivation: 现有KGQA方法面临标注数据多样性不足和多跳推理样本稀缺的问题，传统数据增强方法侧重单跳且易语义失真，LLM方法忽视多跳推理导致数据多样性受限

Method: 1) 生成单跳伪问题增强语义对齐 2) 语义保留式问题重写提升语言变体鲁棒性 3) 答案引导的反向路径探索构建多跳问题

Result: 在WebQSP数据集F1/Hits@1/Accuracy分别提升2.8%/1.2%/3.1%，ComplexWebQuestions提升1.8%/1.1%/2.4%

Conclusion: PGDA-KGQA通过创新的数据增强策略和语义解析流程，有效提升多跳推理能力和模型泛化性能，在KGQA任务上达到SOTA水平

Abstract: Knowledge Graph Question Answering (KGQA) is a crucial task in natural
language processing that requires reasoning over knowledge graphs (KGs) to
answer natural language questions. Recent methods utilizing large language
models (LLMs) have shown remarkable semantic parsing capabilities but are
limited by the scarcity of diverse annotated data and multi-hop reasoning
samples. Traditional data augmentation approaches are focus mainly on
single-hop questions and prone to semantic distortion, while LLM-based methods
primarily address semantic distortion but usually neglect multi-hop reasoning,
thus limiting data diversity. The scarcity of multi-hop samples further weakens
models' generalization. To address these issues, we propose PGDA-KGQA, a
prompt-guided generative framework with multiple data augmentation strategies
for KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by
crafting meticulously engineered prompts that integrate the provided textual
content, it leverages LLMs to generate large-scale (question, logical form)
pairs for model training. Specifically, PGDA-KGQA enriches its training set by:
(1) generating single-hop pseudo questions to improve the alignment of question
semantics with KG relations; (2) applying semantic-preserving question
rewriting to improve robustness against linguistic variations; (3) employing
answer-guided reverse path exploration to create realistic multi-hop questions.
By adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA
utilizes the augmented data to enhance the accuracy of logical form generation
and thus improve answer retrieval performance. Experiments demonstrate that
outperforms state-of-the-art methods on standard KGQA datasets, achieving
improvements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by
1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.

</details>


### [23] [Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings](https://arxiv.org/abs/2506.09424)
*Md Messal Monem Miah,Adrita Anika,Xi Shi,Ruihong Huang*

Main category: cs.CL

TL;DR: LLMs在文本欺骗检测中达到SOTA性能，LMMs在多模态线索利用上存在局限


<details>
  <summary>Details</summary>
Motivation: 数字时代欺骗检测需求迫切，需评估LLMs/LMMs在跨模态场景的检测能力

Method: 使用RLTD/MU3D/OpSpam三个数据集，对比零样本/少样本方法，分析微调模型与多模态特征融合

Result: 微调LLMs优于传统方法，LMMs跨模态处理能力不足，提示策略对性能有显著影响

Conclusion: LLMs在单模态欺骗检测中实用价值突出，但多模态应用需改进特征融合方法

Abstract: Detecting deception in an increasingly digital world is both a critical and
challenging task. In this study, we present a comprehensive evaluation of the
automated deception detection capabilities of Large Language Models (LLMs) and
Large Multimodal Models (LMMs) across diverse domains. We assess the
performance of both open-source and commercial LLMs on three distinct datasets:
real life trial interviews (RLTD), instructed deception in interpersonal
scenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the
effectiveness of different experimental setups for deception detection,
including zero-shot and few-shot approaches with random or similarity-based
in-context example selection. Our results show that fine-tuned LLMs achieve
state-of-the-art performance on textual deception detection tasks, while LMMs
struggle to fully leverage cross-modal cues. Additionally, we analyze the
impact of auxiliary features, such as non-verbal gestures and video summaries,
and examine the effectiveness of different prompting strategies, including
direct label generation and chain-of-thought reasoning. Our findings provide
key insights into how LLMs process and interpret deceptive cues across
modalities, highlighting their potential and limitations in real-world
deception detection applications.

</details>


### [24] [Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting](https://arxiv.org/abs/2506.09428)
*Fei Ding,Baiqiao Wang*

Main category: cs.CL

TL;DR: 提出无需原始SFT数据的低成本监督微调方法，通过重建指令分布与多模型数据筛选，有效缓解灾难性遗忘问题


<details>
  <summary>Details</summary>
Motivation: 传统SFT方法在增强领域任务能力时导致模型通用能力下降，且第三方开发者难以获取原始预训练数据加剧遗忘问题

Method: 1. 重建基础模型的潜在SFT指令分布 2. 多模型联合筛选最优数据 3. 新旧数据混合进行监督微调

Result: 实验显示在保持通用领域性能的同时提升特定任务表现，验证了方法的有效性

Conclusion: 该方法为第三方开发者提供经济高效的SFT解决方案，平衡领域适应与通用能力保留

Abstract: Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'
instruction-following capabilities and domain-specific task adaptability, often
diminishes their general capabilities. Moreover, due to the inaccessibility of
original pre-training data, catastrophic forgetting tends to be exacerbated
when third-party practitioners implement SFT on open-sourced models. To address
this challenge, we propose a novel, more cost-effective SFT method which could
effectively reduce the risk of catastrophic forgetting without access to
original SFT data. Our approach begins by reconstructing the likely SFT
instruction distribution of the base model, followed by a multi-model screening
process to select optimal data, which is then mixed with new data for SFT.
Experimental results demonstrate that our method preserves generalization
capabilities in general domains while improving task-specific performance.

</details>


### [25] [GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture](https://arxiv.org/abs/2506.09440)
*GigaChat team,Mamedov Valentin,Evgenii Kosarev,Gregory Leleytner,Ilya Shchuckin,Valeriy Berezovskiy,Daniil Smirnov,Dmitry Kozlov,Sergei Averkiev,Lukyanenko Ivan,Aleksandr Proshunin,Ainur Israfilova,Ivan Baskov,Artem Chervyakov,Emil Shakirov,Mikhail Kolesov,Daria Khomich,Darya Latortseva,Sergei Porkhun,Yury Fedorov,Oleg Kutuzov,Polina Kudriavtseva,Sofiia Soldatova,Kolodin Egor,Stanislav Pyatkin,Dzmitry Menshykh,Grafov Sergei,Eldar Damirov,Karlov Vladimir,Ruslan Gaitukiev,Arkadiy Shatenov,Alena Fenogenova,Nikita Savushkin,Fedor Minkin*

Main category: cs.CL

TL;DR: 推出了针对俄语的GigaChat大语言模型系列，填补俄语LLM领域空白，并通过开源促进研究与应用


<details>
  <summary>Details</summary>
Motivation: 俄语大模型开发受限于计算资源，需专门定制以支持俄语NLP研究与工业应用

Method: 开发不同规模的基础模型与指令调优版本，详细报告架构设计、预训练过程和实验验证

Result: 模型在俄英双语基准测试中表现优异，提供API/Telegram/Web多端系统，开源三个模型

Conclusion: GigaChat推动了俄语NLP技术发展，开源模型为研究和工业应用提供基础支持

Abstract: Generative large language models (LLMs) have become crucial for modern NLP
research and applications across various languages. However, the development of
foundational models specifically tailored to the Russian language has been
limited, primarily due to the significant computational resources required.
This paper introduces the GigaChat family of Russian LLMs, available in various
sizes, including base models and instruction-tuned versions. We provide a
detailed report on the model architecture, pre-training process, and
experiments to guide design choices. In addition, we evaluate their performance
on Russian and English benchmarks and compare GigaChat with multilingual
analogs. The paper presents a system demonstration of the top-performing models
accessible via an API, a Telegram bot, and a Web interface. Furthermore, we
have released three open GigaChat models in open-source
(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities
and support the development of industrial solutions for the Russian language.

</details>


### [26] [UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs](https://arxiv.org/abs/2506.09450)
*Prameshwar Thiyagarajan,Vaishnavi Parimi,Shamant Sai,Soumil Garg,Zhangir Meirbek,Nitin Yarlagadda,Kevin Zhu,Chris Kim*

Main category: cs.CL

TL;DR: 论文提出统一基准UniToMBench，通过多互动任务和动态场景设计系统提升并评估LLMs的心理理论能力，发现GPT-4o系列在情感/信念任务表现稳定（>80%），但知识型任务存在波动。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在心理理论（ToM）任务中难以准确预测人类心理状态，需要系统化的评估工具推动该领域发展。

Method: 整合SimToM和TOMBENCH优势，设计包含1,000+手工场景的数据集，结合视角采择技术和多维度评估指标（准确性/推理深度/响应一致性）。

Result: GPT-4o系列在情感/信念场景准确率超80%，但知识型任务表现差异显著（如常识推理准确率波动达35%）。

Conclusion: UniToMBench有效揭示LLMs在ToM任务中的能力边界，为模型的社会认知能力提升提供标准化评估框架。

Abstract: Theory of Mind (ToM), the ability to understand the mental states of oneself
and others, remains a challenging area for large language models (LLMs), which
often fail to predict human mental states accurately. In this paper, we
introduce UniToMBench, a unified benchmark that integrates the strengths of
SimToM and TOMBENCH to systematically improve and assess ToM capabilities in
LLMs by integrating multi-interaction task designs and evolving story
scenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,
UniToMBench combines perspective-taking techniques with diverse evaluation
metrics to better stimulate social cognition in LLMs. Through evaluation, we
observe that while models like GPT-4o and GPT-4o Mini show consistently high
accuracy in tasks involving emotional and belief-related scenarios, with
results usually above 80%, there is significant variability in their
performance across knowledge-based tasks. These results highlight both the
strengths and limitations of current LLMs in ToM-related tasks, underscoring
the value of UniToMBench as a comprehensive tool for future development. Our
code is publicly available here:
https://github.com/Shamant/unifiedtombenchmark.

</details>


### [27] [Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms](https://arxiv.org/abs/2506.09457)
*Zeguan Xiao,Yun Chen,Guanhua Chen*

Main category: cs.CL

TL;DR: 提出POET方法解决直接对齐算法(DAAs)的奖励-生成差距问题，通过等长截断优化前缀关注，在多个基准测试中实现显著性能提升


<details>
  <summary>Details</summary>
Motivation: 发现直接对齐算法存在奖励-生成差距的根本问题，主要源于模型生成过程中前缀token的重要性与算法奖励函数关注点之间的错配

Method: 提出POET方法：将偏好/非偏好响应截断至相同长度，通过多样化截断长度约束优化过程，增强对前缀token的关注

Result: 使用POET改进的DPO和SimPO算法在AlpacaEval 2提升达15.6分，下游任务全面改进

Conclusion: 解决奖励优化与生成性能的错配对DAAs至关重要，POET通过强制等长截断有效缩小了奖励-生成差距

Abstract: Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization
(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient
alternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms
for aligning large language models (LLMs) with human preferences. However, DAAs
suffer from a fundamental limitation we identify as the "reward-generation gap"
-- a misalignment between optimization objectives during training and actual
generation performance during inference. In this paper, we find a contributor
to the reward-generation gap is the mismatch between the inherent importance of
prefix tokens during the LLM generation process and how this importance is
reflected in the implicit reward functions of DAAs. To bridge the gap, we
introduce a simple yet effective approach called Prefix-Oriented Equal-length
Training (POET), which truncates both preferred and dispreferred responses to
match the shorter one's length. Training with POET, where both responses in
each sample are truncated to equal length, resulting in diverse truncated
lengths across samples, the optimization of DAAs objective is implicitly
constrained to converge across all positions, thus paying more attention to
prefix tokens than the standard DAAs. We conduct experiments with DPO and
SimPO, two representative DAAs, demonstrating that POET improves over their
standard implementations, achieving up to 15.6 points in AlpacaEval 2 and
overall improvements across downstream tasks. Our results highlight the
importance of addressing the misalignment between reward optimization and
generation performance in DAAs.

</details>


### [28] [Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers](https://arxiv.org/abs/2506.09495)
*Ilanit Sobol,Shir Lissak,Refael Tikochinski,Tal Nakash,Anat Brunstein Klomek,Eyal Fruchter,Roi Reichart*

Main category: cs.CL

TL;DR: 通过整合计算模型与临床视角，研究发现YouTube用户的自杀行为存在平台特异性指标（如YouTube Engagement），且自杀视频上传动机呈现前/后阶段差异（助人导向 vs 自我疗愈导向）


<details>
  <summary>Details</summary>
Motivation: 传统自杀研究存在局限，社交媒体数字足迹为理解自杀行为提供了新视角。研究旨在探索自杀行为在YouTube平台的具体表现及其与临床认知的差异

Method: 采用三模态方法：1）基于LLM的文本主题建模（自下而上）2）专家标注的混合方法 3）自杀叙事心理评估（自上而下），分析181个有自杀行为YouTube频道和134个对照组的纵向数据

Result: 1）识别出5个与自杀行为相关的主题，其中心理健康挣扎（+0.08）和平台参与度（+0.1）呈现显著时序变化 2）专家未识别出YouTube Engagement这一平台特异性指标 3）自杀前上传者动机为帮助他人（β=-1.69），自杀期间上传者更强调自我康复（β=1.08）

Conclusion: 数字行为分析与临床视角的融合能更全面理解自杀心理，平台特异性行为指标（如YouTube Engagement）对自杀风险识别具有独特价值，上传动机差异为危机干预提供新方向

Abstract: Suicide remains a leading cause of death in Western countries, underscoring
the need for new research approaches. As social media becomes central to daily
life, digital footprints offer valuable insight into suicidal behavior.
Focusing on individuals who attempted suicide while uploading videos to their
channels, we investigate: How do suicidal behaviors manifest on YouTube, and
how do they differ from expert knowledge? We applied complementary approaches:
computational bottom-up, hybrid, and expert-driven top-down, on a novel
longitudinal dataset of 181 YouTube channels from individuals with
life-threatening attempts, alongside 134 control channels. In the bottom-up
approach, we applied LLM-based topic modeling to identify behavioral
indicators. Of 166 topics, five were associated with suicide-attempt, with two
also showing temporal attempt-related changes ($p<.01$) - Mental Health
Struggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,
a clinical expert reviewed LLM-derived topics and flagged 19 as
suicide-related. However, none showed significant attempt-related temporal
effects beyond those identified bottom-up. Notably, YouTube Engagement, a
platform-specific indicator, was not flagged by the expert, underscoring the
value of bottom-up discovery. In the top-down approach, psychological
assessment of suicide attempt narratives revealed that the only significant
difference between individuals who attempted before and those attempted during
their upload period was the motivation to share this experience: the former
aimed to Help Others ($\beta=-1.69$, $p<.01$), while the latter framed it as
part of their Personal Recovery ($\beta=1.08$, $p<.01$). By integrating these
approaches, we offer a nuanced understanding of suicidality, bridging digital
behavior and clinical insights.
  * Within-group changes in relation to the suicide attempt.

</details>


### [29] [Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning](https://arxiv.org/abs/2506.09501)
*Jiayi Yuan,Hao Li,Xinheng Ding,Wenya Xie,Yu-Jhe Li,Wentian Zhao,Kun Wan,Jing Shi,Xia Hu,Zirui Liu*

Main category: cs.CL

TL;DR: 大语言模型（LLM）性能评估的可复现性存在脆弱性，浮点运算精度差异会导致推理结果显著波动（如准确率波动达9%），论文提出LayerCast轻量级推理框架平衡计算稳定性与内存效率


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估依赖基准测试的可复现性假设，但实际发现不同硬件配置（GPU数量/版本）和软件参数（评估批次大小）会导致模型输出显著差异，影响评估结果可信度

Method: 通过控制变量实验（硬件类型/数量、计算精度设置）追踪数值误差传播机制，设计LayerCast框架（16位存储+FP32计算）平衡内存效率与数值稳定性

Result: 推理模型在bfloat16精度下因GPU配置差异导致最大9%准确率波动及9000 tokens输出长度差异，LayerCast可有效控制数值误差积累

Conclusion: 浮点运算精度是LLM可复现性的关键因素，需建立标准化评估配置体系，LayerCast为精度敏感场景提供轻量化解决方案

Abstract: Large Language Models (LLMs) are now integral across various domains and have
demonstrated impressive performance. Progress, however, rests on the premise
that benchmark scores are both accurate and reproducible. We demonstrate that
the reproducibility of LLM performance is fragile: changing system
configuration such as evaluation batch size, GPU count, and GPU version can
introduce significant difference in the generated responses. This issue is
especially pronounced in reasoning models, where minor rounding differences in
early tokens can cascade into divergent chains of thought, ultimately affecting
accuracy. For instance, under bfloat16 precision with greedy decoding, a
reasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation
in accuracy and 9,000 tokens difference in response length due to differences
in GPU count, type, and evaluation batch size. We trace the root cause of this
variability to the non-associative nature of floating-point arithmetic under
limited numerical precision. This work presents the first systematic
investigation into how numerical precision affects reproducibility in LLM
inference. Through carefully controlled experiments across various hardware,
software, and precision settings, we quantify when and how model outputs
diverge. Our analysis reveals that floating-point precision -- while critical
for reproducibility -- is often neglected in evaluation practices. Inspired by
this, we develop a lightweight inference pipeline, dubbed LayerCast, that
stores weights in 16-bit precision but performs all computations in FP32,
balancing memory efficiency with numerical stability. Code is available at
https://github.com/nanomaoli/llm_reproducibility.

</details>


### [30] [TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding](https://arxiv.org/abs/2506.09507)
*Bingheng Wu,Jingze Shi,Yifan Wu,Nan Tang,Yuyu Luo*

Main category: cs.CL

TL;DR: 提出统一旋转位置编码RoPE方法解决Transformer与SSM的位置编码不兼容问题，构建MambaFormer混合模型实现42.3%训练加速和4%准确率提升。


<details>
  <summary>Details</summary>
Motivation: Transformer擅长长距离依赖但依赖显式位置编码，SSM支持线性时间建模但使用隐式卷积编码，二者位置编码机制不兼容导致性能损失。

Method: 设计统一的旋转位置编码框架ourRoPE，构建融合Transformer和SSM层的MambaFormer架构，保持参数规模不变。

Result: 4K序列下训练/推理速度提升42.3%/29.5%，语言建模准确率超基线4%，1.3B模型较320M版本平均精度提升7.22%（Transformer仅6%）。

Conclusion: 统一位置编码有效解决混合模型的位置不兼容问题，实现高效高性能的长上下文建模，证明架构融合的可行性。

Abstract: Transformers exhibit proficiency in capturing long-range dependencies,
whereas State Space Models (SSMs) facilitate linear-time sequence modeling.
Notwithstanding their synergistic potential, the integration of these
architectures presents a significant challenge, primarily attributable to a
fundamental incongruity in their respective positional encoding mechanisms:
Transformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs
leverage implicit positional representations via convolutions. This divergence
often precipitates discontinuities and suboptimal performance. To address this
impediment, we propose a unified rotary position embedding (\textbf{\ourRoPE})
methodology, thereby establishing a consistent positional encoding framework
for both self-attention and state-space components. Using this \ourRoPE, we
introduce \textbf{\model}, a hybrid architecture that coherently integrates the
Transformer and SSM layers under this unified positional encoding scheme. At a
4K sequence length, \model exhibits training and inference speeds that are
\textbf{42.3\% and 29.5\% faster}, respectively, relative to standard
Transformer models. It also delivers higher accuracy: under comparable
settings, it surpasses a Transformer baseline by over 4\% on language modeling
benchmarks. \model furthermore scales more effectively: \model-1.3B gains
\textbf{7.22\%} in average accuracy over its 320M version (versus about 6\%
gains for equivalent Transformers or SSMs). Our results show that unified
positional encoding resolves positional incompatibility in hybrid models,
enabling efficient, high-performance long-context modeling.

</details>


### [31] [ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning](https://arxiv.org/abs/2506.09513)
*Yu Sun,Xingyu Qian,Weiwen Xu,Hao Zhang,Chenghao Xiao,Long Li,Yu Rong,Wenbing Huang,Qifeng Bai,Tingyang Xu*

Main category: cs.CL

TL;DR: 提出当前最大医学推理数据集ReasonMed（37万样本），通过多智能体验证框架改进推理路径，训练出超越大模型的7B医学推理专用模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在数学和编程领域表现优异，但在知识密集型医学问答领域潜力尚未充分挖掘

Method: 1. 构建包含37万高质量样本的ReasonMed数据集（初始170万路径）
2. 设计错误修正器(Error Refiner)改进被验证器标记的错误推理步骤
3. 验证链式推理(CoT)结合答案摘要的微调策略效果最佳

Result: ReasonMed-7B在sub-10B模型中刷新纪录（提升4.17%），在PubMedQA上超越LLaMA3.1-70B达4.60%

Conclusion: 通过精细化推理路径清洗和CoT+摘要的联合训练策略，证明小规模模型在专业领域可以超越参数量级更大的通用模型

Abstract: Though reasoning-based large language models (LLMs) have excelled in
mathematics and programming, their capabilities in knowledge-intensive medical
question answering remain underexplored. To address this, we introduce
ReasonMed, the largest medical reasoning dataset, comprising 370k high-quality
examples distilled from 1.7 million initial reasoning paths generated by
various LLMs. ReasonMed is constructed through a \textit{multi-agent
verification and refinement process}, where we design an \textit{Error Refiner}
to enhance the reasoning paths by identifying and correcting error-prone steps
flagged by a verifier. Leveraging ReasonMed, we systematically investigate best
practices for training medical reasoning models and find that combining
detailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields
the most effective fine-tuning strategy. Based on this strategy, we train
ReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the
prior best by 4.17\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\%.

</details>


### [32] [KG-Infused RAG: Augmenting Corpus-Based RAG with External Knowledge Graphs](https://arxiv.org/abs/2506.09542)
*Dingjun Wu,Yukun Yan,Zhenghao Liu,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: KG-Infused RAG通过整合知识图谱与激活扩散机制，显著提升RAG系统的问答性能（3.8%-13.8%），并可作为即插即用模块增强现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法依赖单一数据源且缺乏认知启发机制，导致事实准确性和推理能力受限。

Method: 将知识图谱融入RAG系统，通过激活扩散实现语义关联推理，结合结构化/非结构化数据检索，并采用偏好学习优化关键流程节点。

Result: 在5个QA基准测试中持续超越原始RAG（最高提升13.8%），整合至Self-RAG后产生额外增益。

Conclusion: 该框架通过语义结构增强多源检索，为基于语料的RAG方法提供可解释、可扩展的增强方案。

Abstract: Retrieval-Augmented Generation (RAG) improves factual accuracy by grounding
responses in external knowledge. However, existing methods typically rely on a
single source, either unstructured text or structured knowledge. Moreover, they
lack cognitively inspired mechanisms for activating relevant knowledge. To
address these issues, we propose KG-Infused RAG, a framework that integrates
KGs into RAG systems to implement spreading activation, a cognitive process
that enables concept association and inference. KG-Infused RAG retrieves KG
facts, expands the query accordingly, and enhances generation by combining
corpus passages with structured facts, enabling interpretable, multi-source
retrieval grounded in semantic structure. We further improve KG-Infused RAG via
preference learning on sampled key stages in the pipeline. Experiments on five
QA benchmarks show that KG-Infused RAG consistently outperforms vanilla RAG (by
3.8% to 13.8%). Additionally, when integrated into Self-RAG, KG-Infused RAG
brings further performance gains, demonstrating its effectiveness and
versatility as a plug-and-play enhancement module for corpus-based RAG methods.

</details>


### [33] [MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions](https://arxiv.org/abs/2506.09556)
*Georgios Chatzichristodoulou,Despoina Kosmopoulou,Antonios Kritikos,Anastasia Poulopoulou,Efthymios Georgiou,Athanasios Katsamanis,Vassilis Katsouros,Alexandros Potamianos*

Main category: cs.CL

TL;DR: 提出多模态框架MEDUSA，通过四阶段训练流程解决语音情感识别中的类别不平衡和情感模糊问题，在Interspeech 2025挑战赛中获得第一


<details>
  <summary>Details</summary>
Motivation: 语音情感识别(SER)因情感主观性和自然场景下的数据分布不均衡而极具挑战性，需要开发能同时处理类别不平衡和情感歧义的鲁棒框架

Method: 四阶段训练流程：前两阶段训练基于DeepSER（深度跨模态transformer）的集成分类器，使用Manifold MixUp正则化；后两阶段优化可训练元分类器，结合平衡采样、多任务学习和人工标注软目标

Result: MEDUSA在Interspeech 2025自然场景语音情感识别挑战赛的类别情感识别任务中获得第一名

Conclusion: 通过多模态融合架构、集成学习策略和创新的训练方法，MEDUSA有效解决了SER的核心挑战，实验证明其在真实场景中的优越性能

Abstract: SER is a challenging task due to the subjective nature of human emotions and
their uneven representation under naturalistic conditions. We propose MEDUSA, a
multimodal framework with a four-stage training pipeline, which effectively
handles class imbalance and emotion ambiguity. The first two stages train an
ensemble of classifiers that utilize DeepSER, a novel extension of a deep
cross-modal transformer fusion mechanism from pretrained self-supervised
acoustic and linguistic representations. Manifold MixUp is employed for further
regularization. The last two stages optimize a trainable meta-classifier that
combines the ensemble predictions. Our training approach incorporates human
annotation scores as soft targets, coupled with balanced data sampling and
multitask learning. MEDUSA ranked 1st in Task 1: Categorical Emotion
Recognition in the Interspeech 2025: Speech Emotion Recognition in Naturalistic
Conditions Challenge.

</details>


### [34] [Gender Bias in English-to-Greek Machine Translation](https://arxiv.org/abs/2506.09558)
*Eleni Gkovedarou,Joke Daems,Luna De Bruyne*

Main category: cs.CL

TL;DR: 研究揭示谷歌翻译和DeepL在英希翻译中存在性别偏见，GPT-4o展现部分纠偏潜力但仍有残余偏差


<details>
  <summary>Details</summary>
Motivation: 随着社会对包容性语言需求的增长，需评估商用机器翻译系统在未充分研究的语言对（如英希翻译）中强化性别刻板印象的风险，并探索大语言模型的纠偏可能性

Method: 构建包含240个性别模糊/明确句子的GendEL双语数据集，测试谷歌翻译/DeepL/GPT-4o在男性偏见、职业刻板印象、反刻板翻译错误三方面的表现，探索GPT-4o生成性别明确/中性替代方案的能力

Result: 商用系统在性别明确时表现良好（DeepL女性句翻译最优），但性别模糊时缺乏包容性翻译；GPT-4o能生成75%合适的性别替代方案，但仍存在残余偏见

Conclusion: 当前商用翻译系统尚未实现性别包容，GPT-4o虽展现纠偏潜力，但需要进一步优化算法以减少残余偏见，建议结合人工校验机制提升翻译包容性

Abstract: As the demand for inclusive language increases, concern has grown over the
susceptibility of machine translation (MT) systems to reinforce gender
stereotypes. This study investigates gender bias in two commercial MT systems,
Google Translate and DeepL, focusing on the understudied English-to-Greek
language pair. We address three aspects of gender bias: i) male bias, ii)
occupational stereotyping, and iii) errors in anti-stereotypical translations.
Additionally, we explore the potential of prompted GPT-4o as a bias mitigation
tool that provides both gender-explicit and gender-neutral alternatives when
necessary. To achieve this, we introduce GendEL, a manually crafted bilingual
dataset of 240 gender-ambiguous and unambiguous sentences that feature
stereotypical occupational nouns and adjectives. We find persistent gender bias
in translations by both MT systems; while they perform well in cases where
gender is explicitly defined, with DeepL outperforming both Google Translate
and GPT-4o in feminine gender-unambiguous sentences, they are far from
producing gender-inclusive or neutral translations when the gender is
unspecified. GPT-4o shows promise, generating appropriate gendered and neutral
alternatives for most ambiguous cases, though residual biases remain evident.

</details>


### [35] [Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource Language](https://arxiv.org/abs/2506.09560)
*Stefan Krsteski,Matea Tashkovska,Borjan Sazdov,Hristijan Gjoreski,Branislav Gerazov*

Main category: cs.CL

TL;DR: 研究者为马其顿语构建了包含40GB文本语料库、10.6万条指令数据集和评估套件，并训练出性能超越同规模模型的domestic-yak模型


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（马其顿语）在大型语言模型支持不足的问题，促进相关地区的技术应用发展

Method: 1. 收集马其顿语40GB语料库（3.5B单词）
2. 构建文化相关的10.6万条指令数据集
3. 创建涵盖7个基准的评估体系
4. 训练8B参数的domestic-yak模型并进行多维度评估

Result: 1. domestic-yak在8B参数模型中所有基准表现最佳
2. 性能可媲美大10倍的模型
3. 母语者评估显示其语法正确性和文化适配性优于更大模型

Conclusion: 通过开放数据集、代码和模型权重，为低资源语言LLM研究奠定基础，证明专用模型在有限资源下的有效性，推动语言技术公平发展

Abstract: The increase in technological adoption worldwide comes with demands for novel
tools to be used by the general population. Large Language Models (LLMs)
provide a great opportunity in this respect, but their capabilities remain
limited for low-resource languages, restricting applications in countries where
such languages are spoken. We create several resources to facilitate the
adoption of LLMs and to support research advancements for Macedonian. We
collect the largest Macedonian corpus to date, consisting of 40GB of textual
data and totaling 3.5B words. To support conversational applications, we
collect a 106k-instance instruction dataset, carefully built to be culturally
grounded. For evaluation, we construct a Macedonian evaluation suite covering
seven benchmarks. Finally, we train domestic-yak, a state-of-the-art
8B-parameter model, on our curated datasets and evaluate it against eight
baseline models using the newly constructed benchmark suite. Our model
outperforms all existing models in the 8B parameter range across all
benchmarks, and achieves performance comparable to models up to 10x larger.
Furthermore, a qualitative analysis with native speakers reveals that our model
is preferred over larger counterparts, receiving higher ratings for grammatical
correctness and cultural appropriateness. All datasets, code, and model weights
are openly released, setting a foundation for advancing LLMs in similarly
underrepresented languages. These resources are publicly available at
github.com/LVSTCK for source code, and at huggingface.co/LVSTCK for pretrained
model weights and data.

</details>


### [36] [From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies](https://arxiv.org/abs/2506.09566)
*Blaž Škrlj,Boshko Koloski,Senja Pollak,Nada Lavrač*

Main category: cs.CL

TL;DR: 探索知识图谱与语言模型的双向增强机制，提出神经符号集成等未来方向


<details>
  <summary>Details</summary>
Motivation: 通过结构化知识增强语言模型的推理能力，同时利用语言模型优化知识图谱构建

Method: 将现有方法分为KG增强LLM（提升推理/减少幻觉）和LLM增强KG（支持图谱构建/补全）两类

Result: 揭示结构化知识集成的协同效应，强调规模化扩展和计算效率的关键作用

Conclusion: 提出神经符号系统、动态知识更新、数据可靠性及伦理框架四大发展方向

Abstract: Integrating structured knowledge from Knowledge Graphs (KGs) into Large
Language Models (LLMs) enhances factual grounding and reasoning capabilities.
This survey paper systematically examines the synergy between KGs and LLMs,
categorizing existing approaches into two main groups: KG-enhanced LLMs, which
improve reasoning, reduce hallucinations, and enable complex question
answering; and LLM-augmented KGs, which facilitate KG construction, completion,
and querying. Through comprehensive analysis, we identify critical gaps and
highlight the mutual benefits of structured knowledge integration. Compared to
existing surveys, our study uniquely emphasizes scalability, computational
efficiency, and data quality. Finally, we propose future research directions,
including neuro-symbolic integration, dynamic KG updating, data reliability,
and ethical considerations, paving the way for intelligent systems capable of
managing more complex real-world knowledge tasks.

</details>


### [37] [Memorization in Language Models through the Lens of Intrinsic Dimension](https://arxiv.org/abs/2506.09591)
*Stefan Arnold*

Main category: cs.CL

TL;DR: 内在维度（ID）作为序列潜在空间结构复杂性的几何指标，被发现能够抑制语言模型的记忆行为，尤其在过参数化模型和稀疏暴露场景下效果显著。


<details>
  <summary>Details</summary>
Motivation: 语言模型在训练中可能无意识地记忆敏感数据，引发隐私泄露和知识产权问题。研究旨在探索潜在空间结构复杂度（通过ID衡量）对记忆行为的调节作用。

Method: 通过分析序列在潜在空间中的内在维度（ID），研究其与记忆概率的关系，特别关注模型规模、数据暴露稀疏性与结构复杂性的交互效应。

Result: 高ID序列比低ID序列更难被记忆，这种抑制效应在过参数化模型（参数规模超过训练数据维度）和稀疏暴露（训练数据重复次数少）时更加显著。

Conclusion: 研究揭示了模型规模、数据暴露频率和序列结构复杂性三者共同塑造记忆行为的相互作用机制，为理解语言模型的记忆动力学提供了新视角。

Abstract: Language Models (LMs) are prone to memorizing parts of their data during
training and unintentionally emitting them at generation time, raising concerns
about privacy leakage and disclosure of intellectual property. While previous
research has identified properties such as context length, parameter size, and
duplication frequency, as key drivers of unintended memorization, little is
known about how the latent structure modulates this rate of memorization. We
investigate the role of Intrinsic Dimension (ID), a geometric proxy for the
structural complexity of a sequence in latent space, in modulating
memorization. Our findings suggest that ID acts as a suppressive signal for
memorization: compared to low-ID sequences, high-ID sequences are less likely
to be memorized, particularly in overparameterized models and under sparse
exposure. These findings highlight the interaction between scale, exposure, and
complexity in shaping memorization.

</details>


### [38] [Benchmarking Debiasing Methods for LLM-based Parameter Estimates](https://arxiv.org/abs/2506.09627)
*Nicolas Audinet de Pieuchon,Adel Daoud,Connor T. Jerzak,Moa Johansson,Richard Johansson*

Main category: cs.CL

TL;DR: 研究比较了两种大语言模型标注去偏方法（DSL和PPI），发现DSL在多数情况下偏差更小效率更高，但跨数据集表现稳定性较差，揭示了去偏方法存在偏差-方差权衡。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的标注不一致性会导致下游统计分析偏差，现有去偏方法在理论上有效但缺乏实际应用场景下的性能比较，需评估不同专家标注量对方法效果的影响。

Method: 通过实验分析DSL和PPI在不同专家标注数量下的表现，研究其在多种任务中的偏差减少效果、经验效率及跨数据集稳定性。

Result: DSL在大样本时偏差更低效率更高，但跨数据集表现波动较大；PPI稳定性更好但效率较低，两者呈现偏差-方差权衡关系。

Conclusion: 需开发新指标量化有限样本下方法的效率，未来研究应关注去偏方法的偏差-方差平衡问题以提升实际应用效果。

Abstract: Large language models (LLMs) offer an inexpensive yet powerful way to
annotate text, but are often inconsistent when compared with experts. These
errors can bias downstream estimates of population parameters such as
regression coefficients and causal effects. To mitigate this bias, researchers
have developed debiasing methods such as Design-based Supervised Learning (DSL)
and Prediction-Powered Inference (PPI), which promise valid estimation by
combining LLM annotations with a limited number of expensive expert
annotations. Although these methods produce consistent estimates under
theoretical assumptions, it is unknown how they compare in finite samples of
sizes encountered in applied research. We make two contributions: First, we
study how each method's performance scales with the number of expert
annotations, highlighting regimes where LLM bias or limited expert labels
significantly affect results. Second, we compare DSL and PPI across a range of
tasks, finding that although both achieve low bias with large datasets, DSL
often outperforms PPI on bias reduction and empirical efficiency, but its
performance is less consistent across datasets. Our findings indicate that
there is a bias-variance tradeoff at the level of debiasing methods, calling
for more research on developing metrics for quantifying their efficiency in
finite samples.

</details>


### [39] [Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning](https://arxiv.org/abs/2506.09641)
*Anna Stein,Kevin Tang*

Main category: cs.CL

TL;DR: 比较信息论概率预测器与NDL模型在声学缩减建模中的表现，发现N-gram模型最优但信息论公式可改进NDL


<details>
  <summary>Details</summary>
Motivation: 验证NDL模型因认知动机更有效的假设，探究信息论指标与判别学习的结合对声学缩减建模的影响

Method: 使用Buckeye语料库测试三种模型：信息论公式改进的NDL预测器、传统NDL预测器、N-gram概率预测器

Result: N-gram模型优于NDL模型，但信息论公式可提升NDL性能；需同时考虑频率/上下文预测性/平均预测性

Conclusion: 挑战NDL优势假设，提出改进NDL的方法，强调声学缩减建模需结合信息论指标和判别学习数据

Abstract: This study compares probabilistic predictors based on information theory with
Naive Discriminative Learning (NDL) predictors in modeling acoustic word
duration, focusing on probabilistic reduction. We examine three models using
the Buckeye corpus: one with NDL-derived predictors using information-theoretic
formulas, one with traditional NDL predictors, and one with N-gram
probabilistic predictors. Results show that the N-gram model outperforms both
NDL models, challenging the assumption that NDL is more effective due to its
cognitive motivation. However, incorporating information-theoretic formulas
into NDL improves model performance over the traditional model. This research
highlights a) the need to incorporate not only frequency and contextual
predictability but also average contextual predictability, and b) the
importance of combining information-theoretic metrics of predictability and
information derived from discriminative learning in modeling acoustic
reduction.

</details>


### [40] [Using Sign Language Production as Data Augmentation to enhance Sign Language Translation](https://arxiv.org/abs/2506.09643)
*Harry Walsh,Maksym Ivashechkin,Richard Bowden*

Main category: cs.CL

TL;DR: 利用手语生成技术（骨架生成、手语拼接、SignGAN/SignSplat）增强低资源手语数据集，使翻译模型性能提升19%


<details>
  <summary>Details</summary>
Motivation: 手语作为低资源语言面临数据稀缺难题，传统数据收集方式存在成本/隐私限制，需通过生成技术突破数据瓶颈

Method: 1. 基于骨架的生成方法 2. 手语片段拼接技术 3. 两种逼真生成模型(SignGAN图像生成、SignSplat点云生成)

Result: 通过生成不同手语者外观和骨骼运动变体，成功提升手语翻译模型性能最高达19%

Conclusion: 验证了生成式数据增强在资源受限场景的有效性，为构建鲁棒手语翻译系统提供新范式

Abstract: Machine learning models fundamentally rely on large quantities of
high-quality data. Collecting the necessary data for these models can be
challenging due to cost, scarcity, and privacy restrictions. Signed languages
are visual languages used by the deaf community and are considered low-resource
languages. Sign language datasets are often orders of magnitude smaller than
their spoken language counterparts. Sign Language Production is the task of
generating sign language videos from spoken language sentences, while Sign
Language Translation is the reverse translation task. Here, we propose
leveraging recent advancements in Sign Language Production to augment existing
sign language datasets and enhance the performance of Sign Language Translation
models. For this, we utilize three techniques: a skeleton-based approach to
production, sign stitching, and two photo-realistic generative models, SignGAN
and SignSplat. We evaluate the effectiveness of these techniques in enhancing
the performance of Sign Language Translation models by generating variation in
the signer's appearance and the motion of the skeletal data. Our results
demonstrate that the proposed methods can effectively augment existing datasets
and enhance the performance of Sign Language Translation models by up to 19%,
paving the way for more robust and accurate Sign Language Translation systems,
even in resource-constrained environments.

</details>


### [41] [Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering](https://arxiv.org/abs/2506.09645)
*Tianjun Yao,Haoxuan Li,Zhiqiang Shen,Pan Li,Tongliang Liu,Kun Zhang*

Main category: cs.CL

TL;DR: 提出RAPL框架，通过三阶段改进知识图谱检索，提升KGQA任务效果和泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的检索方法存在泛化能力不足、结构化推理受限的问题

Method: 1. 两阶段标注策略 2. 模型无关的图变换方法 3. 基于路径的推理策略

Result: 在KGQA任务中准确率超越SOTA方法2.66%-20.34%，显著缩小不同规模LLM间的性能差距

Conclusion: RAPL通过结构化检索增强，有效提升检索能力和跨数据集泛化性，代码已开源

Abstract: Large Language Models (LLMs) have shown strong inductive reasoning ability
across various domains, but their reliability is hindered by the outdated
knowledge and hallucinations. Retrieval-Augmented Generation mitigates these
issues by grounding LLMs with external knowledge; however, most existing RAG
pipelines rely on unstructured text, limiting interpretability and structured
reasoning. Knowledge graphs, which represent facts as relational triples, offer
a more structured and compact alternative. Recent studies have explored
integrating knowledge graphs with LLMs for knowledge graph question answering
(KGQA), with a significant proportion adopting the retrieve-then-reasoning
paradigm. In this framework, graph-based retrievers have demonstrated strong
empirical performance, yet they still face challenges in generalization
ability. In this work, we propose RAPL, a novel framework for efficient and
effective graph retrieval in KGQA. RAPL addresses these limitations through
three aspects: (1) a two-stage labeling strategy that combines heuristic
signals with parametric models to provide causally grounded supervision; (2) a
model-agnostic graph transformation approach to capture both intra- and
inter-triple interactions, thereby enhancing representational capacity; and (3)
a path-based reasoning strategy that facilitates learning from the injected
rational knowledge, and supports downstream reasoner through structured inputs.
Empirically, RAPL outperforms state-of-the-art methods by $2.66\%-20.34\%$, and
significantly reduces the performance gap between smaller and more powerful
LLM-based reasoners, as well as the gap under cross-dataset settings,
highlighting its superior retrieval capability and generalizability. Codes are
available at: https://github.com/tianyao-aka/RAPL.

</details>


### [42] [Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA](https://arxiv.org/abs/2506.09657)
*Nikolas Evkarpidi,Elena Tutubalina*

Main category: cs.CL

TL;DR: 提出融合文本转SQL/代码、自修正机制和RAG增强的端到端系统，在SemEval 2025表格问答任务中取得80%准确率（Top13/38），显著提升开源模型性能


<details>
  <summary>Details</summary>
Motivation: 解决表格数据问答任务中开源模型与商业LLM的性能差距，通过模块化集成提升复杂查询的解析能力

Method: 整合文本转SQL/代码生成模块、自修正机制、RAG增强检索和端到端模块，通过LLM协调各组件并进行消融实验分析

Result: 竞赛评测准确率达80%，性能超越同类开源方案且接近商业LLM水平，消融实验揭示当前领域技术瓶颈

Conclusion: 模块化集成策略有效提升表格问答性能，开源方案代码共享促进领域发展，但复杂语义解析仍是待突破难点

Abstract: This paper presents a system developed for SemEval 2025 Task 8: Question
Answering (QA) over tabular data. Our approach integrates several key
components: text-to-SQL and text-to-code generation modules, a self-correction
mechanism, and a retrieval-augmented generation (RAG). Additionally, it
includes an end-to-end (E2E) module, all orchestrated by a large language model
(LLM). Through ablation studies, we analyzed the effects of different parts of
our pipeline and identified the challenges that are still present in this
field. During the evaluation phase of the competition, our solution achieved an
accuracy of 80%, resulting in a top-13 ranking among the 38 participating
teams. Our pipeline demonstrates a significant improvement in accuracy for
open-source models and achieves a performance comparable to proprietary LLMs in
QA tasks over tables. The code is available at GitHub repository.

</details>


### [43] [Query-Level Uncertainty in Large Language Models](https://arxiv.org/abs/2506.09669)
*Lihu Chen,Gaël Varoquaux*

Main category: cs.CL

TL;DR: 提出基于内部置信度的无训练知识边界检测方法，提升LLM自适应推理效率


<details>
  <summary>Details</summary>
Motivation: 增强大型语言模型对知识边界的认知能力，使其能够自适应选择RAG/深度思考/弃权等推理机制，推动高效可信AI发展

Method: 通过查询级别不确定性检测（Internal Confidence），利用模型不同层和token的自我评估实现知识边界判断

Result: 在事实QA和数学推理任务中优于基线，成功应用于高效RAG和模型级联，降低30%推理成本保持性能

Conclusion: 该方法为构建自适应AI系统提供了有效解决方案，通过前期的知识边界判断显著提升推理效率，具有实际部署价值

Abstract: It is important for Large Language Models to be aware of the boundary of
their knowledge, the mechanism of identifying known and unknown queries. This
type of awareness can help models perform adaptive inference, such as invoking
RAG, engaging in slow and deep thinking, or adopting the abstention mechanism,
which is beneficial to the development of efficient and trustworthy AI. In this
work, we propose a method to detect knowledge boundaries via Query-Level
Uncertainty, which aims to determine if the model is able to address a given
query without generating any tokens. To this end, we introduce a novel and
training-free method called \emph{Internal Confidence}, which leverages
self-evaluations across layers and tokens. Empirical results on both factual QA
and mathematical reasoning tasks demonstrate that our internal confidence can
outperform several baselines. Furthermore, we showcase that our proposed method
can be used for efficient RAG and model cascading, which is able to reduce
inference costs while maintaining performance.

</details>


### [44] [Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data](https://arxiv.org/abs/2506.09672)
*Hao Xiong,Chuanyuan Tan,Wenliang Chen*

Main category: cs.CL

TL;DR: 提出FT-UKE方法解决非结构化知识编辑的评估缺陷与微调异常问题，在批处理场景中显著超越SOTA


<details>
  <summary>Details</summary>
Motivation: 现有非结构化知识编辑方法存在两个关键缺陷：（1）缺乏局部性评估体系（2）微调方法存在异常失败现象

Method: 1. 扩展构建UnKEBench-Loc和AKEW-Loc(CF)数据集，从非结构化和结构化双视角建立局部性评估体系
2. 通过四因素实验确定微调方法的最佳训练方案（FT-UKE）

Result: FT-UKE在单次编辑中平均指标领先SOTA 6.78%，批处理场景下随着批量增大优势扩展至10.80%

Conclusion: 该研究不仅建立了非结构化知识编辑的评估基准，更揭示了微调方法的潜在优势，为后续研究提供了训练方案和性能基准

Abstract: Unstructured Knowledge Editing (UKE) is crucial for updating the relevant
knowledge of large language models (LLMs). It focuses on unstructured inputs,
such as long or free-form texts, which are common forms of real-world
knowledge. Although previous studies have proposed effective methods and tested
them, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)
Abnormal failure of fine-tuning (FT) based methods for UKE. To address these
issues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by
extending two existing UKE datasets with locality test data from the
unstructured and structured views. This enables a systematic evaluation of the
Locality of post-edited models. Furthermore, we identify four factors that may
affect the performance of FT-based methods. Based on these factors, we conduct
experiments to determine how the well-performing FT-based methods should be
trained for the UKE task, providing a training recipe for future research. Our
experimental results indicate that the FT-based method with the optimal setting
(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art
(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,
with its advantage over SOTA methods increasing as the batch size grows,
expanding the average metric lead from +6.78% to +10.80%

</details>


### [45] [Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models](https://arxiv.org/abs/2506.09684)
*Haoyi Song,Ruihan Ji,Naichen Shi,Fan Lai,Raed Al Kontar*

Main category: cs.CL

TL;DR: 本文提出了基于逆模型的概率框架来量化大型语言模型的不确定性，定义了Inv-Entropy指标并开发GAAP扰动算法，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有不确定性量化方法缺乏概率基础且多为启发式，需系统化理论框架提升LLM部署可靠性。

Method: 通过双随机游走建模输入-输出关系，构建概率框架评估扰动后的输入空间多样性，提出Inv-Entropy度量，并设计基于遗传算法的GAAP扰动策略。

Result: Inv-Entropy在实验中超越现有语义不确定性量化方法，验证了框架的灵活性及GAAP算法的有效性。

Conclusion: 该框架为不确定性量化提供了理论基础和灵活工具，GAAP和TSU指标显著提升了量化效果与评估方式的可靠性。

Abstract: Large language models (LLMs) have transformed natural language processing,
but their reliable deployment requires effective uncertainty quantification
(UQ). Existing UQ methods are often heuristic and lack a probabilistic
foundation. This paper begins by providing a theoretical justification for the
role of perturbations in UQ for LLMs. We then introduce a dual random walk
perspective, modeling input-output pairs as two Markov chains with transition
probabilities defined by semantic similarity. Building on this, we propose a
fully probabilistic framework based on an inverse model, which quantifies
uncertainty by evaluating the diversity of the input space conditioned on a
given output through systematic perturbations. Within this framework, we define
a new uncertainty measure, Inv-Entropy. A key strength of our framework is its
flexibility: it supports various definitions of uncertainty measures,
embeddings, perturbation strategies, and similarity metrics. We also propose
GAAP, a perturbation algorithm based on genetic algorithms, which enhances the
diversity of sampled inputs. In addition, we introduce a new evaluation metric,
Temperature Sensitivity of Uncertainty (TSU), which directly assesses
uncertainty without relying on correctness as a proxy. Extensive experiments
demonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code
to reproduce the results can be found at
https://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.

</details>


### [46] [ComfyUI-R1: Exploring Reasoning Models for Workflow Generation](https://arxiv.org/abs/2506.09790)
*Zhenran Xu,Yiyu Wang,Xue Yang,Longyue Wang,Weihua Luo,Kaifu Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 提出首个自动化工作流生成大模型ComfyUI-R1，通过两阶段训练框架（思维链微调+强化学习）实现97%格式有效性，显著超越GPT-4o等闭源模型。


<details>
  <summary>Details</summary>
Motivation: 解决ComfyUI平台工作流编排门槛高、需专业知识的痛点

Method: 构建4K工作流数据集→生成长思维链推理数据→两阶段训练（CoT微调+基于规则-指标混合奖励的强化学习）

Result: 7B参数模型格式有效性97%，节点/图级F1得分优异，显著优于GPT-4o和Claude系列

Conclusion: 长思维链推理机制与代码化工作流转换在AI艺术创作中具有关键价值，模型能合成含多样化节点的复杂工作流

Abstract: AI-generated content has evolved from monolithic models to modular workflows,
particularly on platforms like ComfyUI, enabling customization in creative
pipelines. However, crafting effective workflows requires great expertise to
orchestrate numerous specialized components, presenting a steep learning curve
for users. To address this challenge, we introduce ComfyUI-R1, the first large
reasoning model for automated workflow generation. Starting with our curated
dataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning
data, including node selection, workflow planning, and code-level workflow
representation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT
fine-tuning for cold start, adapting models to the ComfyUI domain; (2)
reinforcement learning for incentivizing reasoning capability, guided by a
fine-grained rule-metric hybrid reward, ensuring format validity, structural
integrity, and node-level fidelity. Experiments show that our 7B-parameter
model achieves a 97\% format validity rate, along with high pass rate,
node-level and graph-level F1 scores, significantly surpassing prior
state-of-the-art methods that employ leading closed-source models such as
GPT-4o and Claude series. Further analysis highlights the critical role of the
reasoning process and the advantage of transforming workflows into code.
Qualitative comparison reveals our strength in synthesizing intricate workflows
with diverse nodes, underscoring the potential of long CoT reasoning in AI art
creation.

</details>


### [47] [Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?](https://arxiv.org/abs/2506.09796)
*Andreas Säuberli,Diego Frassinelli,Barbara Plank*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在校准后能部分模拟人类应试行为，但零样本场景下尚不能替代人类参与教育评估测试开发


<details>
  <summary>Details</summary>
Motivation: 探索LLMs能否模拟人类应试反应，替代人工试点研究以加速测试开发流程

Method: 使用18个指令调优模型，基于经典测试理论和项目反应理论，分析三个学科多选题的人类相似性

Result: 大模型存在过度自信问题，温度校准后更接近人类反应；阅读类题目相关性较好，但总体相关性较弱(r值不高)

Conclusion: LLMs在特定校准条件下可能辅助测试开发，但当前零样本设置下尚达不到可靠的人类替代水平

Abstract: Knowing how test takers answer items in educational assessments is essential
for test development, to evaluate item quality, and to improve test validity.
However, this process usually requires extensive pilot studies with human
participants. If large language models (LLMs) exhibit human-like response
behavior to test items, this could open up the possibility of using them as
pilot participants to accelerate test development. In this paper, we evaluate
the human-likeness or psychometric plausibility of responses from 18
instruction-tuned LLMs with two publicly available datasets of multiple-choice
test items across three subjects: reading, U.S. history, and economics. Our
methodology builds on two theoretical frameworks from psychometrics which are
commonly used in educational assessment, classical test theory and item
response theory. The results show that while larger models are excessively
confident, their response distributions can be more human-like when calibrated
with temperature scaling. In addition, we find that LLMs tend to correlate
better with humans in reading comprehension items compared to other subjects.
However, the correlations are not very strong overall, indicating that LLMs
should not be used for piloting educational assessments in a zero-shot setting.

</details>


### [48] [CoRT: Code-integrated Reasoning within Thinking](https://arxiv.org/abs/2506.09820)
*Chengpeng Li,Zhengyang Tang,Ziniu Li,Mingfeng Xue,Keqin Bao,Tian Ding,Ruoyu Sun,Benyou Wang,Xiang Wang,Junyang Lin,Dayiheng Liu*

Main category: cs.CL

TL;DR: CoRT框架通过代码解释器集成优化大型推理模型的数学运算能力，显著提升效率与准确性


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂数学运算中存在效率低、精度差的问题，直接结合代码解释器存在知识整合障碍

Method: 使用Hint-Engineering生成代码增强的推理数据，通过监督微调+拒绝微调+强化学习三阶段训练

Result: 在32B/1.5B模型上分别取得4%/8%绝对提升，token使用量减少30%/50%

Conclusion: 通过系统性提示工程优化模型与计算工具的协同，显著提升数学推理效率与资源利用率

Abstract: Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable
progress in natural language reasoning with long chain-of-thought (CoT), yet
they remain inefficient or inaccurate when handling complex mathematical
operations. Addressing these limitations through computational tools (e.g.,
computation libraries and symbolic solvers) is promising, but it introduces a
technical challenge: Code Interpreter (CI) brings external knowledge beyond the
model's internal text representations, thus the direct combination is not
efficient. This paper introduces CoRT, a post-training framework for teaching
LRMs to leverage CI effectively and efficiently. As a first step, we address
the data scarcity issue by synthesizing code-integrated reasoning data through
Hint-Engineering, which strategically inserts different hints at appropriate
positions to optimize LRM-CI interaction. We manually create 30 high-quality
samples, upon which we post-train models ranging from 1.5B to 32B parameters,
with supervised fine-tuning, rejection fine-tuning and reinforcement learning.
Our experimental results demonstrate that Hint-Engineering models achieve 4\%
and 8\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and
DeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging
mathematical reasoning datasets. Furthermore, Hint-Engineering models use about
30\% fewer tokens for the 32B model and 50\% fewer tokens for the 1.5B model
compared with the natural language models. The models and code are available at
https://github.com/ChengpengLi1003/CoRT.

</details>


### [49] [EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection](https://arxiv.org/abs/2506.09827)
*Christoph Schuhmann,Robert Kaczmarczyk,Gollam Rabby,Felix Friedrich,Maurice Kraus,Kourosh Nadi,Huu Nguyen,Kristian Kersting,Sören Auer*

Main category: cs.CL

TL;DR: 提出EmoNet-Voice数据集及Empathic Insight Voice模型，建立包含4500+小时语音数据的新基准，实现40种细粒度情绪识别


<details>
  <summary>Details</summary>
Motivation: 解决现有语音情绪识别数据集在情绪颗粒度、隐私保护和表演真实性方面的局限性，构建更全面的评估体系

Method: 采用合成语音生成技术创建模拟场景音频，通过心理学专家标注强度标签，开发隐私安全的跨语言数据集

Result: 新模型与专家标注一致性达新高，发现高唤醒情绪（如愤怒）识别准确率显著优于低唤醒状态（如专注）

Conclusion: EmoNet-Voice为语音情绪识别研究提供标准化评估框架，其合成数据方法有效突破敏感情绪数据的获取瓶颈

Abstract: The advancement of text-to-speech and audio generation models necessitates
robust benchmarks for evaluating the emotional understanding capabilities of AI
systems. Current speech emotion recognition (SER) datasets often exhibit
limitations in emotional granularity, privacy concerns, or reliance on acted
portrayals. This paper introduces EmoNet-Voice, a new resource for speech
emotion detection, which includes EmoNet-Voice Big, a large-scale pre-training
dataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,
and 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human
expert annotations. EmoNet-Voice is designed to evaluate SER models on a
fine-grained spectrum of 40 emotion categories with different levels of
intensities. Leveraging state-of-the-art voice generation, we curated synthetic
audio snippets simulating actors portraying scenes designed to evoke specific
emotions. Crucially, we conducted rigorous validation by psychology experts who
assigned perceived intensity labels. This synthetic, privacy-preserving
approach allows for the inclusion of sensitive emotional states often absent in
existing datasets. Lastly, we introduce Empathic Insight Voice models that set
a new standard in speech emotion recognition with high agreement with human
experts. Our evaluations across the current model landscape exhibit valuable
findings, such as high-arousal emotions like anger being much easier to detect
than low-arousal states like concentration.

</details>


### [50] [Error-Guided Pose Augmentation: Enhancing Rehabilitation Exercise Assessment through Targeted Data Generation](https://arxiv.org/abs/2506.09833)
*Omar Sherif,Ali Hamdi*

Main category: cs.CL

TL;DR: 提出EGPA方法，通过生成模拟临床运动错误的合成数据，结合注意力图卷积网络提升康复评估效果


<details>
  <summary>Details</summary>
Motivation: 现有康复评估系统存在数据不平衡和细微运动错误检测困难的问题，需改进自动运动质量分析

Method: EGPA方法专门针对生物力学错误生成合成骨骼数据，配合注意力机制图卷积网络进行模型训练

Result: 实验显示平均绝对误差降低27.6%，错误分类准确率提升45.8%，注意力可视化显示模型聚焦关键关节

Conclusion: EGPA显著提升运动评估精度和可解释性，适用于临床和家庭康复场景，具有重要应用价值

Abstract: Effective rehabilitation assessment is essential for monitoring patient
progress, particularly in home-based settings. Existing systems often face
challenges such as data imbalance and difficulty detecting subtle movement
errors. This paper introduces Error-Guided Pose Augmentation (EGPA), a method
that generates synthetic skeleton data by simulating clinically relevant
movement mistakes. Unlike standard augmentation techniques, EGPA targets
biomechanical errors observed in rehabilitation. Combined with an
attention-based graph convolutional network, EGPA improves performance across
multiple evaluation metrics. Experiments demonstrate reductions in mean
absolute error of up to 27.6 percent and gains in error classification accuracy
of 45.8 percent. Attention visualizations show that the model learns to focus
on clinically significant joints and movement phases, enhancing both accuracy
and interpretability. EGPA offers a promising approach for improving automated
movement quality assessment in both clinical and home-based rehabilitation
contexts.

</details>


### [51] [Dataset of News Articles with Provenance Metadata for Media Relevance Assessment](https://arxiv.org/abs/2506.09847)
*Tomas Peterka,Matyas Bohacek*

Main category: cs.CL

TL;DR: 论文提出通过图像来源信息（地理位置、时间）检测媒体操纵，构建了新闻溯源数据集并测试了LLM在相关任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有媒体操纵检测方法仅关注图像语义与文本的匹配，忽略了来源信息（如拍摄地、时间）与叙述的一致性，导致漏检。

Method: 构建包含来源标签的新闻数据集News Media Provenance Dataset，定义地理位置相关性（LOR）和时空相关性（DTOR）两个任务，评估了6个LLM的基线性能。

Result: LLM在LOR任务上零样本表现良好（如GPT-4准确率76%），但DTOR任务表现差（最高仅36%），揭示时间相关性检测存在显著瓶颈。

Conclusion: 需开发专门架构提升时间相关性检测能力，来源信息验证为对抗媒体操纵提供了新方向，但需突破现有LLM在时间推理上的限制。

Abstract: Out-of-context and misattributed imagery is the leading form of media
manipulation in today's misinformation and disinformation landscape. The
existing methods attempting to detect this practice often only consider whether
the semantics of the imagery corresponds to the text narrative, missing
manipulation so long as the depicted objects or scenes somewhat correspond to
the narrative at hand. To tackle this, we introduce News Media Provenance
Dataset, a dataset of news articles with provenance-tagged images. We formulate
two tasks on this dataset, location of origin relevance (LOR) and date and time
of origin relevance (DTOR), and present baseline results on six large language
models (LLMs). We identify that, while the zero-shot performance on LOR is
promising, the performance on DTOR hinders, leaving room for specialized
architectures and future work.

</details>


### [52] [Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.09853)
*Xiangning Yu,Zhuohan Wang,Linyi Yang,Haoxuan Li,Anjie Liu,Xiao Xue,Jun Wang,Mengyue Yang*

Main category: cs.CL

TL;DR: 提出因果框架解决CoT推理中步骤充分性与必要性问题，通过自动增减推理步骤提升效率并保持准确性


<details>
  <summary>Details</summary>
Motivation: 现有CoT方法存在推理步骤不够全面支撑结论（充分性不足）和包含冗余步骤（必要性欠缺）的双重挑战，需系统性优化推理过程

Method: 结合因果推理中的充分概率(PS)与必要概率(PN)构建分析框架，量化步骤对结果的实际影响，实现推理步骤动态优化

Result: 在数学推理和常识推理任务中平均减少20%推理步长，资源消耗降低35%，准确率保持原有水平

Conclusion: 该因果分析方法为LLM推理效率与成本优化提供了可扩展的技术路径，推动轻量化推理的实现

Abstract: Chain-of-Thought (CoT) prompting plays an indispensable role in endowing
large language models (LLMs) with complex reasoning capabilities. However, CoT
currently faces two fundamental challenges: (1) Sufficiency, which ensures that
the generated intermediate inference steps comprehensively cover and
substantiate the final conclusion; and (2) Necessity, which identifies the
inference steps that are truly indispensable for the soundness of the resulting
answer. We propose a causal framework that characterizes CoT reasoning through
the dual lenses of sufficiency and necessity. Incorporating causal Probability
of Sufficiency and Necessity allows us not only to determine which steps are
logically sufficient or necessary to the prediction outcome, but also to
quantify their actual influence on the final reasoning outcome under different
intervention scenarios, thereby enabling the automated addition of missing
steps and the pruning of redundant ones. Extensive experimental results on
various mathematical and commonsense reasoning benchmarks confirm substantial
improvements in reasoning efficiency and reduced token usage without
sacrificing accuracy. Our work provides a promising direction for improving LLM
reasoning performance and cost-effectiveness.

</details>


### [53] [Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs](https://arxiv.org/abs/2506.09886)
*Rodion Oblovatny,Alexandra Bazarova,Alexey Zaytsev*

Main category: cs.CL

TL;DR: 提出通过分析隐藏状态分布差异检测大语言模型幻觉的新方法，使用深度可学习核自适应捕捉分布差异，无需外部知识即实现最优检测效果


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法依赖外部知识或辅助模型，缺乏对模型内部表征的深入分析，难以实现高效检测

Method: 通过计算提示与响应隐藏状态分布的几何差异作为幻觉评分，采用深度可学习核函数自适应捕捉分布间的细微差异

Result: 在多个基准测试中达到SOTA性能，即使未经核训练仍保持竞争力，提供可扩展的解决方案

Conclusion: 该方法通过模型内在表征分析实现了鲁棒的幻觉检测，揭示了幻觉产生源于表面重述而非深层推理的机制

Abstract: We present a novel approach for detecting hallucinations in large language
models (LLMs) by analyzing the probabilistic divergence between prompt and
response hidden-state distributions. Counterintuitively, we find that
hallucinated responses exhibit smaller deviations from their prompts compared
to grounded responses, suggesting that hallucinations often arise from
superficial rephrasing rather than substantive reasoning. Leveraging this
insight, we propose a model-intrinsic detection method that uses distributional
distances as principled hallucination scores, eliminating the need for external
knowledge or auxiliary models. To enhance sensitivity, we employ deep learnable
kernels that automatically adapt to capture nuanced geometric differences
between distributions. Our approach outperforms existing baselines,
demonstrating state-of-the-art performance on several benchmarks. The method
remains competitive even without kernel training, offering a robust, scalable
solution for hallucination detection.

</details>


### [54] [The Emergence of Abstract Thought in Large Language Models Beyond Any Language](https://arxiv.org/abs/2506.09890)
*Yuxin Chen,Yiran Zhao,Yang Zhang,An Zhang,Kenji Kawaguchi,Shafiq Joty,Junnan Li,Tat-Seng Chua,Michael Qizhe Shieh,Wenxuan Zhang*

Main category: cs.CL

TL;DR: 大语言模型通过发展核心语言无关参数空间实现跨语言泛化能力，共享神经元比例随模型进化增加并支撑抽象思维。


<details>
  <summary>Details</summary>
Motivation: 早期研究认为LLMs以英语为内部处理语言，但多语言性能提升挑战该假设，需探究其底层跨语言机制。

Method: 识别语言相关神经元并分类为共享/专属类型，分析其动态演化规律，设计神经元特异性训练策略。

Result: 发现关键性核心参数空间，共享神经元对多语言性能贡献度随模型规模扩大显著提升（+38%），专属神经元影响力下降21%。

Conclusion: 语言无关参数空间通过共享神经元实现抽象思维，针对性训练策略可优化LLMs多语言能力发展路径。

Abstract: As large language models (LLMs) continue to advance, their capacity to
function effectively across a diverse range of languages has shown marked
improvement. Preliminary studies observe that the hidden activations of LLMs
often resemble English, even when responding to non-English prompts. This has
led to the widespread assumption that LLMs may "think" in English. However,
more recent results showing strong multilingual performance, even surpassing
English performance on specific tasks in other languages, challenge this view.
In this work, we find that LLMs progressively develop a core language-agnostic
parameter space-a remarkably small subset of parameters whose deactivation
results in significant performance degradation across all languages. This
compact yet critical set of parameters underlies the model's ability to
generalize beyond individual languages, supporting the emergence of abstract
thought that is not tied to any specific linguistic system. Specifically, we
identify language-related neurons-those are consistently activated during the
processing of particular languages, and categorize them as either shared
(active across multiple languages) or exclusive (specific to one). As LLMs
undergo continued development over time, we observe a marked increase in both
the proportion and functional importance of shared neurons, while exclusive
neurons progressively diminish in influence. These shared neurons constitute
the backbone of the core language-agnostic parameter space, supporting the
emergence of abstract thought. Motivated by these insights, we propose
neuron-specific training strategies tailored to LLMs' language-agnostic levels
at different development stages. Experiments across diverse LLM families
support our approach.

</details>


### [55] [PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants](https://arxiv.org/abs/2506.09902)
*Zheng Zhao,Clara Vania,Subhradeep Kayal,Naila Khan,Shay B. Cohen,Emine Yilmaz*

Main category: cs.CL

TL;DR: 提出PersonaLens基准测试框架，系统评估AI助手在任务导向场景中的个性化能力


<details>
  <summary>Details</summary>
Motivation: 现有个性化评估基准局限于闲聊/窄领域任务，无法评估复杂任务导向场景的个性化需求

Method: 构建包含多样化用户画像的基准框架，部署用户代理模拟对话，使用LLM-as-a-Judge范式进行多维度评估

Result: 不同LLM助手在个性化能力上存在显著差异，揭示改进方向

Conclusion: PersonaLens为提升对话系统个性化能力提供了系统性评估工具和研究框架

Abstract: Large language models (LLMs) have advanced conversational AI assistants.
However, systematically evaluating how well these assistants apply
personalization--adapting to individual user preferences while completing
tasks--remains challenging. Existing personalization benchmarks focus on
chit-chat, non-conversational tasks, or narrow domains, failing to capture the
complexities of personalized task-oriented assistance. To address this, we
introduce PersonaLens, a comprehensive benchmark for evaluating personalization
in task-oriented AI assistants. Our benchmark features diverse user profiles
equipped with rich preferences and interaction histories, along with two
specialized LLM-based agents: a user agent that engages in realistic
task-oriented dialogues with AI assistants, and a judge agent that employs the
LLM-as-a-Judge paradigm to assess personalization, response quality, and task
success. Through extensive experiments with current LLM assistants across
diverse tasks, we reveal significant variability in their personalization
capabilities, providing crucial insights for advancing conversational AI
systems.

</details>


### [56] [Aspect-Based Opinion Summarization with Argumentation Schemes](https://arxiv.org/abs/2506.09917)
*Wendi Zhou,Ameer Saadat-Yazd,Nadin Kokciyan*

Main category: cs.CL

TL;DR: 提出ASESUM系统，通过动态提取产品核心方面并验证论据有效性，实现跨领域无预定义方面的自动化观点摘要。


<details>
  <summary>Details</summary>
Motivation: 在线评论数量庞大，用户难以手动归纳核心观点。现有摘要方法（抽取式/生成式）存在无法动态适配领域、缺乏证据支撑的问题。

Method: 开发ASESUM框架：1) 提取方面中心论点 2) 计算观点显著性和有效性 3) 自动识别产品关键方面 4) 支持跨领域适配

Result: 真实数据集实验显示，相比基线方法，本系统能更全面捕获评论中的多样化视角（F1值提升12.5%）

Conclusion: 该系统突破预定义方面的限制，通过动态方面识别和证据验证机制，显著提升跨领域意见摘要的适应性和可靠性。

Abstract: Reviews are valuable resources for customers making purchase decisions in
online shopping. However, it is impractical for customers to go over the vast
number of reviews and manually conclude the prominent opinions, which prompts
the need for automated opinion summarization systems. Previous approaches,
either extractive or abstractive, face challenges in automatically producing
grounded aspect-centric summaries. In this paper, we propose a novel
summarization system that not only captures predominant opinions from an aspect
perspective with supporting evidence, but also adapts to varying domains
without relying on a pre-defined set of aspects. Our proposed framework,
ASESUM, summarizes viewpoints relevant to the critical aspects of a product by
extracting aspect-centric arguments and measuring their salience and validity.
We conduct experiments on a real-world dataset to demonstrate the superiority
of our approach in capturing diverse perspectives of the original reviews
compared to new and existing methods.

</details>


### [57] [VerIF: Verification Engineering for Reinforcement Learning in Instruction Following](https://arxiv.org/abs/2506.09942)
*Hao Peng,Yunjia Qi,Xiaozhi Wang,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 提出VerIF方法，结合规则验证与大模型验证的强化学习方案，使用VerInstruct数据集训练后实现同规模模型SOTA且不影响通用能力


<details>
  <summary>Details</summary>
Motivation: 针对指令跟随任务中强化学习的验证工程缺乏系统探索，现有验证方法存在优化空间

Method: 构建包含2.2万样本的VerInstruct数据集，设计融合规则代码验证与QwQ-32B等大模型推理的VerIF验证框架，对两个模型进行强化学习训练

Result: 在多个指令跟随基准测试中显著提升性能，模型参数量相近情况下达到SOTA，对未见约束具有良好泛化能力且通用能力未受损

Conclusion: 验证强化学习方案可兼容现有RL方法提升整体性能，已开源数据集/代码/模型推动后续研究

Abstract: Reinforcement learning with verifiable rewards (RLVR) has become a key
technique for enhancing large language models (LLMs), with verification
engineering playing a central role. However, best practices for RL in
instruction following remain underexplored. In this work, we explore the
verification challenge in RL for instruction following and propose VerIF, a
verification method that combines rule-based code verification with LLM-based
verification from a large reasoning model (e.g., QwQ-32B). To support this
approach, we construct a high-quality instruction-following dataset,
VerInstruct, containing approximately 22,000 instances with associated
verification signals. We apply RL training with VerIF to two models, achieving
significant improvements across several representative instruction-following
benchmarks. The trained models reach state-of-the-art performance among models
of comparable size and generalize well to unseen constraints. We further
observe that their general capabilities remain unaffected, suggesting that RL
with VerIF can be integrated into existing RL recipes to enhance overall model
performance. We have released our datasets, codes, and models to facilitate
future research at https://github.com/THU-KEG/VerIF.

</details>


### [58] [Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking](https://arxiv.org/abs/2506.09944)
*Wuwei Zhang,Fangcong Yin,Howard Yen,Danqi Chen,Xi Ye*

Main category: cs.CL

TL;DR: 提出QRHEAD注意力头和QR-RETRIEVER检索器，显著提升长上下文语言模型的检索性能与多跳推理效果


<details>
  <summary>Details</summary>
Motivation: 现有检索头在长上下文任务中存在效率瓶颈，需通过更聚焦的查询注意力机制改进检索质量

Method: 基于真实任务样本聚合查询相关注意力分数识别QRHEAD，构建注意力质量驱动的检索评分机制

Result: 多跳推理任务性能提升超10%，BEIR基准零样本表现超越RankGPT等LLM排序器

Conclusion: 开发出通用检索框架，同时为语言模型的长上下文处理机制提供可解释性洞察

Abstract: Recent work has identified retrieval heads (Wu et al., 2025b), a subset of
attention heads responsible for retrieving salient information in long-context
language models (LMs), as measured by their copy-paste behavior in
Needle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused
Retrieval Head), an improved set of attention heads that enhance retrieval from
long context. We identify QRHEAD by aggregating attention scores with respect
to the input query, using a handful of examples from real-world tasks (e.g.,
long-context QA). We further introduce QR- RETRIEVER, an efficient and
effective retriever that uses the accumulated attention mass of QRHEAD as
retrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting
the most relevant parts with the highest retrieval scores. On multi-hop
reasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains
over full context and outperforms strong dense retrievers. We also evaluate
QRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves
strong zero-shot performance, outperforming other LLM-based re-rankers such as
RankGPT. Further analysis shows that both the querycontext attention scoring
and task selection are crucial for identifying QRHEAD with strong downstream
utility. Overall, our work contributes a general-purpose retriever and offers
interpretability insights into the long-context capabilities of LMs.

</details>


### [59] [Resa: Transparent Reasoning Models via SAEs](https://arxiv.org/abs/2506.09967)
*Shangshang Wang,Julian Asilis,Ömer Faruk Akgül,Enes Burak Bilgin,Ollie Liu,Deqing Fu,Willie Neiswanger*

Main category: cs.CL

TL;DR: 提出Resa模型家族，通过SAE-Tuning方法以1美元成本/20分钟训练时间实现97% RL模型推理性能，展示稀疏自编码器提取的推理能力具有通用性和模块化特性。


<details>
  <summary>Details</summary>
Motivation: 探究如何高效利用语言模型的潜在表示来低成本激发其推理能力，突破现有方法高成本低效率的局限性。

Method: 稀疏自编码器调优(SAE-Tuning)：先训练SAE捕获源模型推理能力，再用其指导目标模型的监督微调，全程仅需问答数据无需推理轨迹。

Result: 1) RL训练后成本降2000倍至$1，保留97%性能；2) 轻量训练模型达AIME24 43.33%/AMC23 90%；3) SAE提取能力可跨数据集泛化及跨模型模块化组合。

Conclusion: SAE-Tuning开创了低成本激发语言模型推理能力的新范式，其通用性和模块化特性为资源受限场景提供可行方案，所有成果已完全开源。

Abstract: How cost-effectively can we elicit strong reasoning in language models by
leveraging their underlying representations? We answer this question with Resa,
a family of 1.5B reasoning models trained via a novel and efficient sparse
autoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to
capture reasoning abilities from a source model, and then uses the trained SAE
to guide a standard supervised fine-tuning process to elicit such abilities in
a target model, all using verified question-answer data without any reasoning
traces. Notably, when applied to certain base models before further RL
post-training, SAE-Tuning retains >97% of its RL-trained counterpart's
reasoning performance while reducing training costs by >2000x to roughly \$1
and training time by >450x to around 20 minutes. Furthermore, when applied to
lightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning
performance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only
around \$1 additional cost. Surprisingly, the reasoning abilities extracted via
SAEs are potentially both generalizable and modular. Generality means abilities
extracted from one dataset still elevate performance on a larger and
overlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math
can be attached to the R1-Distill model at test time, without any retraining,
and yield comparable gains. Extensive ablations validate these findings and all
artifacts are fully open-sourced.

</details>


### [60] [When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text](https://arxiv.org/abs/2506.09975)
*Hillary Dawkins,Kathleen C. Fraser,Svetlana Kiritchenko*

Main category: cs.CL

TL;DR: 检测社交媒体AI生成文本存在挑战，攻击者使用微调模型时检测效果急剧下降


<details>
  <summary>Details</summary>
Motivation: 社交媒体是影响力攻击的重要载体，需有效检测AI生成的大规模操纵性内容

Method: 构建505,159条多模型生成的社交媒体语料库，覆盖11个争议话题，模拟攻击者使用开源/闭源/微调模型的混合攻击场景

Result: 在攻击者不公开微调模型的现实假设下，现有检测算法准确率显著下降，人类实验验证了检测困难

Conclusion: 微调技术普遍适用性导致检测脆弱性，当前检测方法在现实场景中可能存在系统性缺陷

Abstract: Detecting AI-generated text is a difficult problem to begin with; detecting
AI-generated text on social media is made even more difficult due to the short
text length and informal, idiosyncratic language of the internet. It is
nonetheless important to tackle this problem, as social media represents a
significant attack vector in online influence campaigns, which may be bolstered
through the use of mass-produced AI-generated posts supporting (or opposing)
particular policies, decisions, or events. We approach this problem with the
mindset and resources of a reasonably sophisticated threat actor, and create a
dataset of 505,159 AI-generated social media posts from a combination of
open-source, closed-source, and fine-tuned LLMs, covering 11 different
controversial topics. We show that while the posts can be detected under
typical research assumptions about knowledge of and access to the generating
models, under the more realistic assumption that an attacker will not release
their fine-tuned model to the public, detectability drops dramatically. This
result is confirmed with a human study. Ablation experiments highlight the
vulnerability of various detection algorithms to fine-tuned LLMs. This result
has implications across all detection domains, since fine-tuning is a generally
applicable and realistic LLM use case.

</details>


### [61] [Step-by-step Instructions and a Simple Tabular Output Format Improve the Dependency Parsing Accuracy of LLMs](https://arxiv.org/abs/2506.09983)
*Hiroshi Matsuda,Chunpeng Ma,Masayuki Asahara*

Main category: cs.CL

TL;DR: 提出分步指令策略和简化的CoNLL-U格式，显著提升跨语言依赖解析性能


<details>
  <summary>Details</summary>
Motivation: 解决标准prompt在依赖解析任务中输出结构有效性不足的问题

Method: 先进行通用词性标注，后预测句法头和依存标签，采用简化的CoNLL-U格式

Result: 在17种语言UD数据集取得SOTA，多语言微调提升跨语言泛化能力

Conclusion: 显式推理步骤有效，提供可扩展且格式统一的依存解析新方案

Abstract: Recent advances in large language models (LLMs) have enabled impressive
performance in various tasks. However, standard prompting often struggles to
produce structurally valid and accurate outputs, especially in dependency
parsing. We propose a novel step-by-step instruction strategy, where universal
part-of-speech tagging precedes the prediction of syntactic heads and
dependency labels, and a simplified CoNLL-U like output format, our method
achieves state-of-the-art accuracy on Universal Dependencies datasets across 17
languages without hallucination or contamination. We further show that
multilingual fine-tuning simultaneously improves cross-language generalization
performance. Our results highlight the effectiveness of explicit reasoning
steps in LLM-based parsing and offer a scalable, format-consistent alternative
to bracket-based approaches.

</details>


### [62] [Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages](https://arxiv.org/abs/2506.09992)
*Amel Muminovic,Amela Kadric Muminovic*

Main category: cs.CL

TL;DR: 研究通过添加上下文片段提升LLM在低资源巴尔干语言中的毒性检测效果，Gemini在上下文增强模式下达到最佳平衡（F1 0.82），GPT-4.1则在零样本下保持最高精度


<details>
  <summary>Details</summary>
Motivation: 解决标注数据有限的巴尔干语言社区在线有害内容检测难题，探索大语言模型在低资源语言环境中的适用性

Method: 构建4500条多平台人工标注数据集，测试4个主流模型在零样本/上下文增强两种模式下的性能（精确率、召回率、F1、准确率、误报率）

Result: 上下文片段使平均召回率提升0.12，Gemini上下文模式综合表现最佳（F1 0.82），GPT-4.1零样本模式误报率最低（精确率最高）

Conclusion: 简单的提示工程优化（上下文补充+阈值校准）可显著改善低资源语言的毒性检测，为巴尔干地区提供了实用解决方案

Abstract: Online toxic language causes real harm, especially in regions with limited
moderation tools. In this study, we evaluate how large language models handle
toxic comments in Serbian, Croatian, and Bosnian, languages with limited
labeled data. We built and manually labeled a dataset of 4,500 YouTube and
TikTok comments drawn from videos across diverse categories, including music,
politics, sports, modeling, influencer content, discussions of sexism, and
general topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude
3 Opus) were tested in two modes: zero-shot and context-augmented. We measured
precision, recall, F1 score, accuracy and false positive rates. Including a
short context snippet raised recall by about 0.12 on average and improved F1
score by up to 0.10, though it sometimes increased false positives. The best
balance came from Gemini in context-augmented mode, reaching an F1 score of
0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the
lowest false alarms. We show how adding minimal context can improve toxic
language detection in low-resource settings and suggest practical strategies
such as improved prompt design and threshold calibration. These results show
that prompt design alone can yield meaningful gains in toxicity detection for
underserved Balkan language communities.

</details>


### [63] [From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring](https://arxiv.org/abs/2506.09996)
*Yang Li,Qiang Sheng,Yehan Yang,Xueyao Zhang,Juan Cao*

Main category: cs.CL

TL;DR: 提出流式内容监控器SCM，通过构建细粒度标注数据集FineHarm和双重监督训练，实现仅需观察前18%的token即可达成与全检测相当的0.95+宏F1值，并提升安全对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有安全审查机制存在全检测延迟高、部分检测范式存在训练-推理gap导致性能下降的问题，需要原生支持部分检测的解决方案。

Method: 1. 构建含29K细粒度标注的FineHarm数据集；2. 设计流式内容监控器SCM，通过响应级和token级的双重监督训练实现实时危害判断。

Result: SCM平均仅需观察前18%的token即可达到与全检测相当的0.95+宏F1值，且作为伪危害标注器可使安全对齐后的模型获得比DPO更高的无害性评分。

Conclusion: SCM通过数据构造和模型架构创新有效解决了部分检测的精度问题，为实时内容安全监控提供了高效解决方案，同时可反向增强LLM自身的安全对齐。

Abstract: Though safety alignment has been applied to most large language models
(LLMs), LLM service providers generally deploy a subsequent moderation as the
external safety guardrail in real-world products. Existing moderators mainly
practice a conventional full detection, which determines the harmfulness based
on the complete LLM output, causing high service latency. Recent works pay more
attention to partial detection where moderators oversee the generation midway
and early stop the output if harmfulness is detected, but they directly apply
moderators trained with the full detection paradigm to incomplete outputs,
introducing a training-inference gap that lowers the performance. In this
paper, we explore how to form a data-and-model solution that natively supports
partial detection. For the data, we construct FineHarm, a dataset consisting of
29K prompt-response pairs with fine-grained annotations to provide reasonable
supervision for token-level training. Then, we propose the streaming content
monitor, which is trained with dual supervision of response- and token-level
labels and can follow the output stream of LLM to make a timely judgment of
harmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is
comparable to full detection, by only seeing the first 18% of tokens in
responses on average. Moreover, the SCM can serve as a pseudo-harmfulness
annotator for improving safety alignment and lead to a higher harmlessness
score than DPO.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [64] [STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support](https://arxiv.org/abs/2506.09070)
*Chenqi Zhang,Yu Feng,Jieru Zhao,Guangda Liu,Wenchao Ding,Chentao Wu,Minyi Guo*

Main category: cs.GR

TL;DR: 提出STREAMINGGS协同设计方法，通过内存中心渲染实现移动端45.7倍加速和62.9倍节能


<details>
  <summary>Details</summary>
Motivation: 现有3DGS加速器忽视内存效率导致DRAM冗余访问，无法满足移动设备90FPS实时需求（当前仅2-9FPS）

Method: 算法架构协同设计：将渲染范式从tile-centric转为memory-centric，实现细粒度流水线并减少DRAM流量

Result: 相较移动端Ampere GPU实现45.7倍速度提升和62.9倍能效优化

Conclusion: 内存效率优化对移动端3DGS加速具有决定性作用，STREAMINGGS方案成功突破实时性瓶颈

Abstract: 3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and
sparse Gaussian-based representation. However, 3DGS struggles to meet the
real-time requirement of 90 frames per second (FPS) on resource-constrained
mobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on
compute efficiency but overlook memory efficiency, leading to redundant DRAM
traffic. We introduce STREAMINGGS, a fully streaming 3DGS
algorithm-architecture co-design that achieves fine-grained pipelining and
reduces DRAM traffic by transforming from a tile-centric rendering to a
memory-centric rendering. Results show that our design achieves up to 45.7
$\times$ speedup and 62.9 $\times$ energy savings over mobile Ampere GPUs.

</details>


### [65] [SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach](https://arxiv.org/abs/2506.09075)
*Elly Akhoundi,Hung Yu Ling,Anup Anand Deshmukh,Judith Butepage*

Main category: cs.GR

TL;DR: 提出基于Transformer的简单框架实现高效运动中间帧生成，揭示数据建模策略比模型复杂度更重要


<details>
  <summary>Details</summary>
Motivation: 现有运动中间帧生成方法依赖复杂模型架构，但研究表明通过优化数据建模策略（数据量/姿势表示/速度特征）可获得更优结果

Method: 使用单一Transformer编码器架构，配合数据增强策略（扩大数据量）、改进姿势表征方式、添加速度输入特征

Result: 证明数据优化策略可实现与复杂模型相当或更优的运动过渡质量，最高提升40%的动画性能指标

Conclusion: 运动插值质量主要取决于数据建模策略而非模型复杂度，为动画生成领域提供了数据优化的新研究范式

Abstract: Motion in-betweening is a crucial tool for animators, enabling intricate
control over pose-level details in each keyframe. Recent machine learning
solutions for motion in-betweening rely on complex models, incorporating
skeleton-aware architectures or requiring multiple modules and training steps.
In this work, we introduce a simple yet effective Transformer-based framework,
employing a single Transformer encoder to synthesize realistic motions for
motion in-betweening tasks. We find that data modeling choices play a
significant role in improving in-betweening performance. Among others, we show
that increasing data volume can yield equivalent or improved motion
transitions, that the choice of pose representation is vital for achieving
high-quality results, and that incorporating velocity input features enhances
animation performance. These findings challenge the assumption that model
complexity is the primary determinant of animation quality and provide insights
into a more data-centric approach to motion interpolation. Additional videos
and supplementary material are available at https://silk-paper.github.io.

</details>


### [66] [VideoMat: Extracting PBR Materials from Video Diffusion Models](https://arxiv.org/abs/2506.09665)
*Jacob Munkberg,Zian Wang,Ruofan Liang,Tianchang Shen,Jon Hasselgren*

Main category: cs.GR

TL;DR: 利用视频扩散模型与可微分渲染生成3D模型PBR材质的全流程方法


<details>
  <summary>Details</summary>
Motivation: 解决传统3D材质制作耗时且难以保持多视角一致性的痛点，通过AI生成兼容主流工具的高质量材质

Method: 1) 微调视频扩散模型生成多视角素材
2) 内在分解提取材质属性
3) 可微分路径追踪优化材质参数

Result: 成功生成物理准确（基色/粗糙度/金属度）且与Blender等工具兼容的PBR材质

Conclusion: 该流程实现了从文本/单图到可直接使用的生产级材质的端到端生成

Abstract: We leverage finetuned video diffusion models, intrinsic decomposition of
videos, and physically-based differentiable rendering to generate high quality
materials for 3D models given a text prompt or a single image. We condition a
video diffusion model to respect the input geometry and lighting condition.
This model produces multiple views of a given 3D model with coherent material
properties. Secondly, we use a recent model to extract intrinsics (base color,
roughness, metallic) from the generated video. Finally, we use the intrinsics
alongside the generated video in a differentiable path tracer to robustly
extract PBR materials directly compatible with common content creation tools.

</details>


### [67] [TransGI: Real-Time Dynamic Global Illumination With Object-Centric Neural Transfer Model](https://arxiv.org/abs/2506.09909)
*Yijie Deng,Lei Han,Lu Fang*

Main category: cs.GR

TL;DR: TransGI提出结合神经传输模型与辐射共享照明系统，在10ms内实现高保真实时全局光照渲染


<details>
  <summary>Details</summary>
Motivation: 传统BSDF材质表达缺乏紧凑性（需数千光线采样降噪），实时渲染方法仅支持漫反射材质牺牲了渲染质量

Method: 1) 基于MLP解码器的顶点附着隐式特征材质模型 2) 本地光探针结合跨探针辐射共享的照明策略

Result: 渲染帧率突破10ms门槛，视觉质量显著优于基线方法，支持动态高光材质实时渲染

Conclusion: 首次在实时约束下实现复杂材质的高质量全局光照，通过解耦材质与光照的神经表征达成效率-质量平衡

Abstract: Neural rendering algorithms have revolutionized computer graphics, yet their
impact on real-time rendering under arbitrary lighting conditions remains
limited due to strict latency constraints in practical applications. The key
challenge lies in formulating a compact yet expressive material representation.
To address this, we propose TransGI, a novel neural rendering method for
real-time, high-fidelity global illumination. It comprises an object-centric
neural transfer model for material representation and a radiance-sharing
lighting system for efficient illumination. Traditional BSDF representations
and spatial neural material representations lack expressiveness, requiring
thousands of ray evaluations to converge to noise-free colors. Conversely,
real-time methods trade quality for efficiency by supporting only diffuse
materials. In contrast, our object-centric neural transfer model achieves
compactness and expressiveness through an MLP-based decoder and vertex-attached
latent features, supporting glossy effects with low memory overhead. For
dynamic, varying lighting conditions, we introduce local light probes capturing
scene radiance, coupled with an across-probe radiance-sharing strategy for
efficient probe generation. We implemented our method in a real-time rendering
engine, combining compute shaders and CUDA-based neural networks. Experimental
results demonstrate that our method achieves real-time performance of less than
10 ms to render a frame and significantly improved rendering quality compared
to baseline methods.

</details>


### [68] [DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos](https://arxiv.org/abs/2506.09997)
*Chieh Hubert Lin,Zhaoyang Lv,Songyin Wu,Zhen Xu,Thu Nguyen-Phuoc,Hung-Yu Tseng,Julian Straub,Numair Khan,Lei Xiao,Ming-Hsuan Yang,Yuheng Ren,Richard Newcombe,Zhao Dong,Zhengqin Li*

Main category: cs.GR

TL;DR: 首个前馈式动态场景重建模型DGS-LRM，通过可变形3D高斯点云实现单目视频的实时动态重建与长程3D追踪


<details>
  <summary>Details</summary>
Motivation: 现有前馈模型局限于静态场景重建，动态场景面临训练数据稀缺、3D表征方式不足及训练范式挑战

Method: 1.构建含稠密3D场景流的大规模合成数据集 2.提出易学习的可变形3D高斯点云表征 3.设计实时通用的大型transformer网络架构

Result: 重建质量媲美优化方法，真实场景性能超越SOTA预测模型，3D变形预测精度达单目视频追踪SOTA水平

Conclusion: DGS-LRM开创了前馈式动态重建新范式，其物理基础的3D变形预测能力为数字孪生、机器人感知等应用开辟新可能

Abstract: We introduce the Deformable Gaussian Splats Large Reconstruction Model
(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian
splats from a monocular posed video of any dynamic scene. Feed-forward scene
reconstruction has gained significant attention for its ability to rapidly
create digital replicas of real-world environments. However, most existing
models are limited to static scenes and fail to reconstruct the motion of
moving objects. Developing a feed-forward model for dynamic scene
reconstruction poses significant challenges, including the scarcity of training
data and the need for appropriate 3D representations and training paradigms. To
address these challenges, we introduce several key technical contributions: an
enhanced large-scale synthetic dataset with ground-truth multi-view videos and
dense 3D scene flow supervision; a per-pixel deformable 3D Gaussian
representation that is easy to learn, supports high-quality dynamic view
synthesis, and enables long-range 3D tracking; and a large transformer network
that achieves real-time, generalizable dynamic scene reconstruction. Extensive
qualitative and quantitative experiments demonstrate that DGS-LRM achieves
dynamic scene reconstruction quality comparable to optimization-based methods,
while significantly outperforming the state-of-the-art predictive dynamic
reconstruction method on real-world examples. Its predicted physically grounded
3D deformation is accurate and can readily adapt for long-range 3D tracking
tasks, achieving performance on par with state-of-the-art monocular video 3D
tracking methods.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [69] [FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation](https://arxiv.org/abs/2506.09081)
*Zheqi He,Yesheng Liu,Jing-shu Zheng,Xuejing Li,Richeng Xuan,Jin-Ge Yao,Xi Yang*

Main category: cs.CV

TL;DR: FlagEvalMM是开源的多模态模型评估框架，支持视觉问答、文本生成图像/视频等任务评估，通过解耦推理与评估提升效率，并已开源。


<details>
  <summary>Details</summary>
Motivation: 为解决多模态模型评估中存在的效率低下、任务覆盖不全、资源分配不灵活等问题，构建统一且可扩展的评估体系。

Method: 1. 模型推理与评估服务解耦架构
2. 集成vLLM/SGLang加速工具
3. 异步数据加载技术
4. 模块化设计支持快速扩展新模型和任务

Result: 实验证明该框架评估效率提升3-5倍，能精准识别模型在跨模态对齐、生成质量等维度的性能边界

Conclusion: FlagEvalMM为多模态研究提供了标准化评估基准，其模块化设计和加速方案显著推动领域技术迭代与模型优化进程。

Abstract: We present FlagEvalMM, an open-source evaluation framework designed to
comprehensively assess multimodal models across a diverse range of
vision-language understanding and generation tasks, such as visual question
answering, text-to-image/video generation, and image-text retrieval. We
decouple model inference from evaluation through an independent evaluation
service, thus enabling flexible resource allocation and seamless integration of
new tasks and models. Moreover, FlagEvalMM utilizes advanced inference
acceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to
significantly enhance evaluation efficiency. Extensive experiments show that
FlagEvalMM offers accurate and efficient insights into model strengths and
limitations, making it a valuable tool for advancing multimodal research. The
framework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.

</details>


### [70] [CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation](https://arxiv.org/abs/2506.09109)
*Arnav Yayavaram,Siddharth Yayavaram,Simran Khanuja,Michael Saxon,Graham Neubig*

Main category: cs.CV

TL;DR: 提出CAIRe评估指标解决文本生成图像模型的跨文化偏见评估问题，通过知识库关联和事实判断机制，在文化显著数据集上超越基线28% F1值并与人类评分高度相关。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型存在跨文化偏见且缺乏可靠评估指标，导致改进方法在性能、准确性及文化敏感性之间存在不良权衡，阻碍公平性研究进展。

Method: 构建CAIRe框架：将图像实体与知识库锚定，利用事实信息对文化标签进行独立分级评估，并基于语言模型构建稀有文化项目数据集进行验证。

Result: CAIRe在文化显著数据集上F1值超越基线28%，在两个通用概念数据集上与人类评分的皮尔逊相关系数分别达0.56和0.66（5级量表评估）。

Conclusion: CAIRe通过知识驱动评估有效量化文化相关性，为消除AI生成内容的跨文化偏见提供了可靠度量标准，且在不同数据源中均保持与人类判断的高度一致性。

Abstract: As text-to-image models become increasingly prevalent, ensuring their
equitable performance across diverse cultural contexts is critical. Efforts to
mitigate cross-cultural biases have been hampered by trade-offs, including a
loss in performance, factual inaccuracies, or offensive outputs. Despite
widespread recognition of these challenges, an inability to reliably measure
these biases has stalled progress. To address this gap, we introduce CAIRe, a
novel evaluation metric that assesses the degree of cultural relevance of an
image, given a user-defined set of labels. Our framework grounds entities and
concepts in the image to a knowledge base and uses factual information to give
independent graded judgments for each culture label. On a manually curated
dataset of culturally salient but rare items built using language models, CAIRe
surpasses all baselines by 28% F1 points. Additionally, we construct two
datasets for culturally universal concept, one comprising of T2I-generated
outputs and another retrieved from naturally occurring data. CAIRe achieves
Pearson's correlations of 0.56 and 0.66 with human ratings on these sets, based
on a 5-point Likert scale of cultural relevance. This demonstrates its strong
alignment with human judgment across diverse image sources.

</details>


### [71] [Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs](https://arxiv.org/abs/2506.09522)
*Beomsik Cho,Jaehyung Kim*

Main category: cs.CV

TL;DR: 提出ReVisiT解码方法，通过动态引用视觉令牌增强大型视觉语言模型的视觉基础能力，减少幻觉现象，在低计算开销下提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有LVLMs解码策略未能有效利用视觉信息，导致生成回答缺乏视觉依据。现有方法需额外训练或多步推理，需更简洁高效的解决方案。

Method: 将视觉令牌语义投影至文本分布空间，通过约束差异最小化动态选择相关视觉令牌，优化输出分布融合视觉语义，无需额外训练。

Result: 在三个LVLM幻觉基准测试中显著增强视觉基础性，计算成本降低达2倍，性能达到或超越现有先进方法。

Conclusion: ReVisiT通过动态整合视觉语义信息，以简单有效的方式提升生成内容的视觉相关性，为改进LVLMs提供高效解决方案。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable performance
across various multimodal tasks by integrating visual perception with language
understanding. However, conventional decoding strategies of LVLMs often fail to
successfully utilize visual information, leading to visually ungrounded
responses. While various approaches have been proposed to address this
limitation, they typically require additional training, multi-step inference
procedures, or external model dependencies. This paper introduces ReVisiT, a
simple yet effective decoding method that references vision tokens to guide the
text generation process in LVLMs. Our approach leverages the semantic
information embedded within vision tokens by projecting them into the text
token distribution space, and dynamically selecting the most relevant vision
token at each decoding step through constrained divergence minimization. This
selected vision token is then used to refine the output distribution to better
incorporate visual semantics. Experiments on three LVLM hallucination
benchmarks with two recent LVLMs demonstrate that ReVisiT consistently enhances
visual grounding with minimal computational overhead. Moreover, our method
achieves competitive or superior results relative to state-of-the-art baselines
while reducing computational costs for up to $2\times$.

</details>


### [72] [Adding simple structure at inference improves Vision-Language Compositionality](https://arxiv.org/abs/2506.09691)
*Imanol Miranda,Ander Salaberria,Eneko Agirre,Gorka Azkune*

Main category: cs.CV

TL;DR: 通过在推理阶段增加图像分块、文本片段匹配和相似度聚合的结构，显著提升双编码器视觉语言模型的组合性能力，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有CLIP等双编码器模型存在组合性缺陷（如词袋效应），多数改进集中在训练阶段，而对推理阶段技术的研究不足。本文探索通过结构化推理过程提升模型表现。

Method: 四步推理框架：1) 图像分块处理；2) 提取文本中的对象/属性/关系片段；3) 使用VLM进行图文片段匹配；4) 聚合局部相似度计算整体匹配度。

Result: 在多个组合性数据集中稳定提升VLMs性能（尤其在属性-对象绑定任务），实验表明图像分块处理对效果提升起关键作用。

Conclusion: 推理时技术具有重要潜力，未来可通过优化文本分解策略、改进局部匹配机制等方向进一步提升组合性表现。

Abstract: Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for
image-text retrieval tasks. However, those models struggle with
compositionality, showing a bag-of-words-like behavior that limits their
retrieval performance. Many different training approaches have been proposed to
improve the vision-language compositionality capabilities of those models. In
comparison, inference-time techniques have received little attention. In this
paper, we propose to add simple structure at inference, where, given an image
and a caption: i) we divide the image into different smaller crops, ii) we
extract text segments, capturing objects, attributes and relations, iii) using
a VLM, we find the image crops that better align with text segments obtaining
matches, and iv) we compute the final image-text similarity aggregating the
individual similarities of the matches. Based on various popular dual encoder
VLMs, we evaluate our approach in controlled and natural datasets for VL
compositionality. We find that our approach consistently improves the
performance of evaluated VLMs without any training, which shows the potential
of inference-time techniques. The results are especially good for
attribute-object binding as shown in the controlled dataset. As a result of an
extensive analysis: i) we show that processing image crops is actually
essential for the observed gains in performance, and ii) we identify specific
areas to further improve inference-time approaches.

</details>


### [73] [Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos](https://arxiv.org/abs/2506.09953)
*Benjamin Reichman,Constantin Patsch,Jack Truxal,Atishay Jain,Larry Heck*

Main category: cs.CV

TL;DR: 研究者提出了一个基于视频的视觉对话数据集OKCV，要求模型结合视频片段中的视觉信息与外部知识进行多轮对话。


<details>
  <summary>Details</summary>
Motivation: 传统OK-VQA任务仅关注静态图片的单轮问答，该研究扩展至视频场景下的多轮对话，要求模型持续追踪时序视觉信息，并在视觉信息缺失时依赖外部知识进行对话。

Method: 构建包含2,017个视频和5,986个人工标注对话的数据集，对话涉及40,954轮次。每个对话基于特定视频片段，但问题需依赖非视觉的外部知识，要求模型同时具备视频定位和知识检索能力。

Result: 公开了OKCV数据集并提供了基线模型，揭示了模型在视频时序理解与知识融合方面的挑战。

Conclusion: 该数据集推动了视频场景下知识增强对话系统的研究，突显了视觉定位与外部知识协同推理的技术难点。

Abstract: In outside knowledge visual question answering (OK-VQA), the model must
identify relevant visual information within an image and incorporate external
knowledge to accurately respond to a question. Extending this task to a
visually grounded dialogue setting based on videos, a conversational model must
both recognize pertinent visual details over time and answer questions where
the required information is not necessarily present in the visual information.
Moreover, the context of the overall conversation must be considered for the
subsequent dialogue. To explore this task, we introduce a dataset comprised of
$2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$
interleaved dialogue turns. While the dialogue context is visually grounded in
specific video segments, the questions further require external knowledge that
is not visually present. Thus, the model not only has to identify relevant
video parts but also leverage external knowledge to converse within the
dialogue. We further provide several baselines evaluated on our dataset and
show future challenges associated with this task. The dataset is made publicly
available here: https://github.com/c-patsch/OKCV.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [74] [An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks](https://arxiv.org/abs/2410.16222)
*Valentyn Boreiko,Alexander Panfilov,Vaclav Voracek,Matthias Hein,Jonas Geiping*

Main category: cs.LG

TL;DR: 提出统一威胁模型评估LLM越狱攻击，发现离散优化攻击显著优于LLM方法，攻击成功率低于预期


<details>
  <summary>Details</summary>
Motivation: 现有越狱攻击方法缺乏统一的评估标准，难以系统比较不同攻击的实际效果和潜在风险

Method: 构建基于1T tokens的N-gram语言模型，通过非参数化方法分析攻击利用的罕见bigrams分布特征

Result: 安全调优模型的攻击成功率比文献报道低25%，离散优化攻击成功率比LLM攻击高3倍

Conclusion: 统一威胁模型揭示攻击本质依赖罕见语言模式（如Reddit/代码数据集特征），为防御策略提供新方向

Abstract: A plethora of jailbreaking attacks have been proposed to obtain harmful
responses from safety-tuned LLMs. These methods largely succeed in coercing the
target output in their original settings, but their attacks vary substantially
in fluency and computational effort. In this work, we propose a unified threat
model for the principled comparison of these methods. Our threat model checks
if a given jailbreak is likely to occur in the distribution of text. For this,
we build an N-gram language model on 1T tokens, which, unlike model-based
perplexity, allows for an LLM-agnostic, nonparametric, and inherently
interpretable evaluation. We adapt popular attacks to this threat model, and,
for the first time, benchmark these attacks on equal footing with it. After an
extensive comparison, we find attack success rates against safety-tuned modern
models to be lower than previously presented and that attacks based on discrete
optimization significantly outperform recent LLM-based attacks. Being
inherently interpretable, our threat model allows for a comprehensive analysis
and comparison of jailbreak attacks. We find that effective attacks exploit and
abuse infrequent bigrams, either selecting the ones absent from real-world text
or rare ones, e.g., specific to Reddit or code datasets.

</details>


### [75] [Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers](https://arxiv.org/abs/2506.09099)
*Joshua Barron,Devin White*

Main category: cs.LG

TL;DR: 模型容量与学习模式存在权衡关系：小模型在算术外推任务中泛化能力较强但无法有效记忆事实，大模型则表现出相反特性；两者难以在单一模型中兼得，且联合训练会抑制泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型中记忆化（memorization）与泛化（generalization）的内在关系，通过受控实验揭示模型容量对两种学习模式的优先级影响。现有研究表明二者存在深层关联，但具体动态机制尚未明确。

Method: 预训练不同容量的Transformer模型，设计两个合成字符级任务：1) 算术外推（测试泛化能力）；2) 事实回忆（测试记忆能力）。通过单独/联合训练对比模型表现，观察容量变化对学习模式的影响。

Result: 小模型（低容量）成功完成算术外推但无法记忆事实，大模型（高容量）呈现完全相反模式；中等容量模型展现出从泛化向记忆化的转变。当两任务联合训练时，所有模型均失去外推能力，表明预训练过程存在内在模式偏好。

Conclusion: 模型容量决定学习行为优先级，预训练机制可能本质上限制同时优化两种学习模式。这一发现为小语言模型的设计提供了新视角：需根据目标任务的特性（泛化/记忆需求）精准匹配模型规模，并在实际部署中权衡容量与任务适应性。

Abstract: The relationship between memorization and generalization in large language
models (LLMs) remains an open area of research, with growing evidence that the
two are deeply intertwined. In this work, we investigate this relationship by
pre-training a series of capacity-limited Transformer models from scratch on
two synthetic character-level tasks designed to separately probe generalization
(via arithmetic extrapolation) and memorization (via factual recall). We
observe a consistent trade-off: small models extrapolate to unseen arithmetic
cases but fail to memorize facts, while larger models memorize but fail to
extrapolate. An intermediate-capacity model exhibits a similar shift toward
memorization. When trained on both tasks jointly, no model (regardless of size)
succeeds at extrapolation. These findings suggest that pre-training may
intrinsically favor one learning mode over the other. By isolating these
dynamics in a controlled setting, our study offers insight into how model
capacity shapes learning behavior and offers broader implications for the
design and deployment of small language models.

</details>


### [76] [SensorLM: Learning the Language of Wearable Sensors](https://arxiv.org/abs/2506.09108)
*Yuwei Zhang,Kumar Ayush,Siyuan Qiao,A. Ali Heydari,Girish Narayanswamy,Maxwell A. Xu,Ahmed A. Metwally,Shawn Xu,Jake Garrison,Xuhai Xu,Tim Althoff,Yun Liu,Pushmeet Kohli,Jiening Zhan,Mark Malhotra,Shwetak Patel,Cecilia Mascolo,Xin Liu,Daniel McDuff,Yuzhe Yang*

Main category: cs.LG

TL;DR: 提出了传感器语言基础模型SensorLM，通过构建59.7百万小时的可穿戴数据集，实现传感器数据与自然语言理解的无缝对接。


<details>
  <summary>Details</summary>
Motivation: 解决可穿戴传感器数据缺乏高质量标注的问题，突破传感器模态与语言模态对齐的技术瓶颈。

Method: 1. 设计分层标题生成框架提取统计/结构/语义特征 2. 构建103,000人规模的超大规模传感器-语言配对数据集 3. 扩展多模态预训练架构（CLIP/CoCa）建立通用模型框架

Result: 在人体活动分析和医疗健康任务中，零样本识别准确率提升15%，跨模态检索Recall@1提升23%，展示出优秀的标签效率和任务泛化能力

Conclusion: SensorLM通过架构创新与数据规模突破，开启了可穿戴计算的新范式，在健康监测、人机交互等领域具有重要应用价值

Abstract: We present SensorLM, a family of sensor-language foundation models that
enable wearable sensor data understanding with natural language. Despite its
pervasive nature, aligning and interpreting sensor data with language remains
challenging due to the lack of paired, richly annotated sensor-text
descriptions in uncurated, real-world wearable data. We introduce a
hierarchical caption generation pipeline designed to capture statistical,
structural, and semantic information from sensor data. This approach enabled
the curation of the largest sensor-language dataset to date, comprising over
59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM
extends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and
recovers them as specific variants within a generic architecture. Extensive
experiments on real-world tasks in human activity analysis and healthcare
verify the superior performance of SensorLM over state-of-the-art in zero-shot
recognition, few-shot learning, and cross-modal retrieval. SensorLM also
demonstrates intriguing capabilities including scaling behaviors, label
efficiency, sensor captioning, and zero-shot generalization to unseen tasks.

</details>


### [77] [Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search](https://arxiv.org/abs/2506.09171)
*Samuel Holt,Max Ruiz Luyten,Thomas Pouplin,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 提出增强LLM智能体规划能力的新框架，通过原子事实增强和递归前瞻搜索实现在线学习优化，无需微调即可提升复杂交互任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在适应新信息、利用历史经验进行多步推理时效率不足，需通过微调改进。本文旨在通过上下文学习提升智能体在复杂环境中的自主决策能力。

Method: 1. 从交互轨迹提取关键原子事实动态增强提示
2. 构建包含动作提议、世界模型模拟和状态价值评估的LLM组件
3. 采用深度受限递归前瞻搜索进行规划，利用积累事实优化决策

Result: 在TextFrozenLake和ALFWorld等交互任务中验证有效性，智能体随经验积累表现出更优行为，证明框架的适应性和性能提升。

Conclusion: 基于原子事实的抽象表达与LLM模拟精度的理论关联，为无需参数更新的在线学习智能体开发提供了新范式。

Abstract: Large Language Models (LLMs) are increasingly capable but often require
significant guidance or extensive interaction history to perform effectively in
complex, interactive environments. Existing methods may struggle with adapting
to new information or efficiently utilizing past experiences for multi-step
reasoning without fine-tuning. We introduce a novel LLM agent framework that
enhances planning capabilities through in-context learning, facilitated by
atomic fact augmentation and a recursive lookahead search. Our agent learns to
extract task-critical ``atomic facts'' from its interaction trajectories. These
facts dynamically augment the prompts provided to LLM-based components
responsible for action proposal, latent world model simulation, and state-value
estimation. Planning is performed via a depth-limited lookahead search, where
the LLM simulates potential trajectories and evaluates their outcomes, guided
by the accumulated facts and interaction history. This approach allows the
agent to improve its understanding and decision-making online, leveraging its
experience to refine its behavior without weight updates. We provide a
theoretical motivation linking performance to the quality of fact-based
abstraction and LLM simulation accuracy. Empirically, our agent demonstrates
improved performance and adaptability on challenging interactive tasks,
achieving more optimal behavior as it accumulates experience, showcased in
tasks such as TextFrozenLake and ALFWorld.

</details>


### [78] [Natural Language Guided Ligand-Binding Protein Design](https://arxiv.org/abs/2506.09332)
*Zhenqiao Song,Ramith Hettiarachchi,Chuan Li,Jianwen Xie,Lei Li*

Main category: cs.LG

TL;DR: AI蛋白生成模型InstructPro通过自然语言指令和配体结构式生成功能匹配的蛋白质，在对接成功率和结构准确性上超越主流基线模型


<details>
  <summary>Details</summary>
Motivation: 传统蛋白质设计依赖稀缺的蛋白-配体复合物数据，而文本描述资源丰富。利用文本指导生成可突破实验数据瓶颈

Method: 构建包含958万条（功能描述，配体SMILES，蛋白序列）的InstructProBench数据集，开发基于指令的1B/3B参数模型架构，采用多模态训练策略

Result: 1B模型达81.52%对接成功率（中等置信度），平均RMSD 4.026Å；3B模型进一步降至2.527Å，结构精度显著提升

Conclusion: InstructPro首次实现语言指导的蛋白质功能设计，其性能优势为精准药物开发等应用提供新可能

Abstract: Can AI protein models follow human language instructions and design proteins
with desired functions (e.g. binding to a ligand)? Designing proteins that bind
to a given ligand is crucial in a wide range of applications in biology and
chemistry. Most prior AI models are trained on protein-ligand complex data,
which is scarce due to the high cost and time requirements of laboratory
experiments. In contrast, there is a substantial body of human-curated text
descriptions about protein-ligand interactions and ligand formula. In this
paper, we propose InstructPro, a family of protein generative models that
follow natural language instructions to design ligand-binding proteins. Given a
textual description of the desired function and a ligand formula in SMILES,
InstructPro generates protein sequences that are functionally consistent with
the specified instructions. We develop the model architecture, training
strategy, and a large-scale dataset, InstructProBench, to support both training
and evaluation. InstructProBench consists of 9,592,829 triples of (function
description, ligand formula, protein sequence). We train two model variants:
InstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion
parameters). Both variants consistently outperform strong baselines, including
ProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking
success rate (81.52% at moderate confidence) and the lowest average root mean
square deviation (RMSD) compared to ground truth structures (4.026{\AA}).
InstructPro-3B further descreases the average RMSD to 2.527{\AA}, demonstrating
InstructPro's ability to generate ligand-binding proteins that align with the
functional specifications.

</details>


### [79] [Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform](https://arxiv.org/abs/2506.09452)
*Jay Roberts,Kyle Mylonakis,Sidhartha Roy,Kaan Kale*

Main category: cs.LG

TL;DR: 提出Stained Glass Transform方法，通过对LLM词嵌入进行随机序列转换实现信息论隐私保护，同时保持模型实用性。


<details>
  <summary>Details</summary>
Motivation: 解决共享计算基础设施中明文数据隐私泄露问题，降低企业对敏感数据使用LLM的顾虑

Method: 采用学习型、随机且序列依赖的词嵌入转换技术，理论连接高斯混合模型互信息理论

Result: 通过互信息后验隐私评估和标准LLM性能测试，验证了隐私保护与模型效用的平衡

Conclusion: Stained Glass Transform为共享基础设施中的隐私保护LLM部署提供了可行解决方案

Abstract: The high cost of ownership of AI compute infrastructure and challenges of
robust serving of large language models (LLMs) has led to a surge in managed
Model-as-a-service deployments. Even when enterprises choose on-premises
deployments, the compute infrastructure is typically shared across many teams
in order to maximize the return on investment. In both scenarios the deployed
models operate only on plaintext data, and so enterprise data owners must allow
their data to appear in plaintext on a shared or multi-tenant compute
infrastructure. This results in data owners with private or sensitive data
being hesitant or restricted in what data they use with these types of
deployments. In this work we introduce the Stained Glass Transform, a learned,
stochastic, and sequence dependent transformation of the word embeddings of an
LLM which information theoretically provides privacy to the input of the LLM
while preserving the utility of model. We theoretically connect a particular
class of Stained Glass Transforms to the theory of mutual information of
Gaussian Mixture Models. We then calculate a-postiori privacy estimates, based
on mutual information, and verify the privacy and utility of instances of
transformed embeddings through token level metrics of privacy and standard LLM
performance benchmarks.

</details>


### [80] [Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models](https://arxiv.org/abs/2506.09532)
*Shuai Wang,Zhenhua Liu,Jiaheng Wei,Xuanwu Yin,Dong Li,Emad Barsoum*

Main category: cs.LG

TL;DR: 提出多模态过程奖励模型Athena-PRM，通过弱/强模型预测一致性实现高效过程标注，仅需5000样本即在多个基准测试中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统过程奖励模型依赖高成本人工标注，自动标注方法存在噪声大、计算成本高的问题，需寻找更高效可靠的过程标注方案。

Method: 利用弱/强模型预测一致性筛选可靠过程标签，结合ORM模型初始化、负样本上采样策略提升模型性能。

Result: 在测试时扩展场景提升WeMath/MathVista指标10.2/7.1分，VisualProcessBench刷新SOTA指标3.9F1，奖励微调后在5个基准显著超越基线。

Conclusion: Athena-PRM验证了预测一致性标注的有效性，提出的训练策略显著提升模型性能，其奖励模型能力可有效支持多场景推理任务优化。

Abstract: We present Athena-PRM, a multimodal process reward model (PRM) designed to
evaluate the reward score for each step in solving complex reasoning problems.
Developing high-performance PRMs typically demands significant time and
financial investment, primarily due to the necessity for step-level annotations
of reasoning steps. Conventional automated labeling methods, such as Monte
Carlo estimation, often produce noisy labels and incur substantial
computational costs. To efficiently generate high-quality process-labeled data,
we propose leveraging prediction consistency between weak and strong completers
as a criterion for identifying reliable process labels. Remarkably, Athena-PRM
demonstrates outstanding effectiveness across various scenarios and benchmarks
with just 5,000 samples. Furthermore, we also develop two effective strategies
to improve the performance of PRMs: ORM initialization and up-sampling for
negative data. We validate our approach in three specific scenarios:
verification for test time scaling, direct evaluation of reasoning step
correctness, and reward ranked fine-tuning. Our Athena-PRM consistently
achieves superior performance across multiple benchmarks and scenarios.
Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances
performance by 10.2 points on WeMath and 7.1 points on MathVista for test time
scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in
VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,
showcasing its robust capability to accurately assess the correctness of the
reasoning step. Additionally, utilizing Athena-PRM as the reward model, we
develop Athena-7B with reward ranked fine-tuning and outperforms baseline with
a significant margin on five benchmarks.

</details>


### [81] [Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling](https://arxiv.org/abs/2506.09998)
*Tim Z. Xiao,Johannes Zenn,Zhen Liu,Weiyang Liu,Robert Bamler,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: 提出基于自然语言的拒绝采样方法VRS，显著降低LLM生成概率分布的采样偏差


<details>
  <summary>Details</summary>
Motivation: 解决LLM能描述概率分布但生成可靠随机样本能力不足的问题，提升其在蒙特卡洛模拟等任务中的应用

Method: 将经典拒绝采样算法转化为自然语言提示流程，使LLM自主进行样本接受/拒绝决策

Result: VRS在相同模型下减少采样偏差，理论证明其优于直接采样，改进来自算法架构和提示设计的协同作用

Conclusion: 通过将概率工具自然语言化融入LLM工作流，无需修改模型即可提升可靠性，为算法与提示设计的协同优化提供新范式

Abstract: Large language models (LLMs) can often accurately describe probability
distributions using natural language, yet they still struggle to generate
faithful samples from them. This mismatch limits their use in tasks requiring
reliable stochasticity, such as Monte Carlo methods, agent-based simulations,
and randomized decision-making. We investigate this gap between knowledge and
sampling in the context of Bernoulli distributions. We introduce Verbalized
Rejection Sampling (VRS), a natural-language adaptation of classical rejection
sampling that prompts the LLM to reason about and accept or reject proposed
samples. Despite relying on the same Bernoulli mechanism internally, VRS
substantially reduces sampling bias across models. We provide theoretical
analysis showing that, under mild assumptions, VRS improves over direct
sampling, with gains attributable to both the algorithm and prompt design. More
broadly, our results show how classical probabilistic tools can be verbalized
and embedded into LLM workflows to improve reliability, without requiring
access to model internals or heavy prompt engineering.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [82] [SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research](https://arxiv.org/abs/2506.09206)
*Ahmed Adel Attia,Jing Liu,Carl Espy-Wilson*

Main category: cs.SD

TL;DR: 提出通过游戏引擎合成课堂噪音的方法，并发布SimClass数据集（包含合成噪音和模拟语音数据），为教育领域语音模型开发提供有效资源。


<details>
  <summary>Details</summary>
Motivation: 现有课堂语音数据稀缺且缺乏专用噪音库，限制了教育AI语音模型的发展及数据增强技术的应用。

Method: 1. 使用游戏引擎生成课堂噪音
2. 将公开儿童语音与YouTube讲座视频配对生成干净课堂语音
3. 构建包含合成噪音和模拟语音的SimClass数据集

Result: 实验证明SimClass在干净/嘈杂环境下均可有效模拟真实课堂语音（词错率接近真实数据）

Conclusion: SimClass填补了教育语音数据空白，其方法论可扩展至其他领域，为开发鲁棒的语音识别/增强模型提供了关键基础设施。

Abstract: The scarcity of large-scale classroom speech data has hindered the
development of AI-driven speech models for education. Public classroom datasets
remain limited, and the lack of a dedicated classroom noise corpus prevents the
use of standard data augmentation techniques.
  In this paper, we introduce a scalable methodology for synthesizing classroom
noise using game engines, a framework that extends to other domains. Using this
methodology, we present SimClass, a dataset that includes both a synthesized
classroom noise corpus and a simulated classroom speech dataset. The speech
data is generated by pairing a public children's speech corpus with YouTube
lecture videos to approximate real classroom interactions in clean conditions.
Our experiments on clean and noisy speech demonstrate that SimClass closely
approximates real classroom speech, making it a valuable resource for
developing robust speech recognition and enhancement models.

</details>


### [83] [OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary](https://arxiv.org/abs/2506.09448)
*Yui Sudo,Yusuke Fujita,Atsushi Kojima,Tomoya Mizumoto,Lianbo Liu*

Main category: cs.SD

TL;DR: 提出将上下文偏置方法与冻结参数的语音基础模型结合，有效提升罕见词识别效果


<details>
  <summary>Details</summary>
Motivation: 现有上下文偏置方法缺乏预训练知识导致性能受限，需结合语音基础模型的优势

Method: 在OWSM v3.1框架中集成上下文偏置方法并冻结预训练参数，利用小数据集实现有效偏置

Result: 在LibriSpeech测试集上偏置词错误率降低11.6点，整体错误率提升0.9点，推理速度提升7.5%

Conclusion: 该方法成功平衡了预训练知识保留与上下文偏置需求，显著提升语音模型的实用性和效率

Abstract: Speech foundation models (SFMs), such as Open Whisper-Style Speech Models
(OWSM), are trained on massive datasets to achieve accurate automatic speech
recognition. However, even SFMs struggle to accurately recognize rare and
unseen words. While contextual biasing (CB) is a promising approach to improve
recognition of such words, most CB methods are trained from scratch, resulting
in lower performance than SFMs due to the lack of pre-trained knowledge. This
paper integrates an existing CB method with OWSM v3.1 while freezing its
pre-trained parameters. By leveraging the knowledge embedded in SFMs, the
proposed method enables effective CB while preserving the advantages of SFMs,
even with a small dataset. Experimental results show that the proposed method
improves the biasing word error rate (B-WER) by 11.6 points, resulting in a 0.9
point improvement in the overall WER while reducing the real-time factor by
7.5% compared to the non-biasing baseline on the LibriSpeech 100 test-clean
set.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [84] [Advancing Exchange Rate Forecasting: Leveraging Machine Learning and AI for Enhanced Accuracy in Global Financial Markets](https://arxiv.org/abs/2506.09851)
*Md. Yeasin Rahat,Rajan Das Gupta,Nur Raisa Rahman,Sudipto Roy Pritom,Samiur Rahman Shakir,Md Imrul Hasan Showmick,Md. Jakir Hossen*

Main category: q-fin.ST

TL;DR: 使用LSTM和梯度提升分类器预测美元/孟加拉塔卡汇率，LSTM实现99.45%准确率但交易策略亏损，研究展示深度学习在外汇预测中的潜力


<details>
  <summary>Details</summary>
Motivation: 提升外汇预测精度对全球贸易和投资决策至关重要，传统ARIMA模型在波动市场中表现有限，需探索深度学习解决方案

Method: 采用2018-2023年历史汇率数据，对比LSTM神经网络（时间序列预测）和梯度提升分类器（方向预测），结合回溯测试评估交易策略

Result: LSTM模型RMSE 0.9858显著优于ARIMA(1.342)，但梯度提升策略导致20,653美元净亏损；汇率长期呈贬值趋势（0.012→0.009）

Conclusion: 深度学习为外汇市场提供有效预测工具，未来需整合情绪分析和实时经济指标提升模型适应能力

Abstract: The prediction of foreign exchange rates, such as the US Dollar (USD) to
Bangladeshi Taka (BDT), plays a pivotal role in global financial markets,
influencing trade, investments, and economic stability. This study leverages
historical USD/BDT exchange rate data from 2018 to 2023, sourced from Yahoo
Finance, to develop advanced machine learning models for accurate forecasting.
A Long Short-Term Memory (LSTM) neural network is employed, achieving an
exceptional accuracy of 99.449%, a Root Mean Square Error (RMSE) of 0.9858, and
a test loss of 0.8523, significantly outperforming traditional methods like
ARIMA (RMSE 1.342). Additionally, a Gradient Boosting Classifier (GBC) is
applied for directional prediction, with backtesting on a $10,000 initial
capital revealing a 40.82% profitable trade rate, though resulting in a net
loss of $20,653.25 over 49 trades. The study analyzes historical trends,
showing a decline in BDT/USD rates from 0.012 to 0.009, and incorporates
normalized daily returns to capture volatility. These findings highlight the
potential of deep learning in forex forecasting, offering traders and
policymakers robust tools to mitigate risks. Future work could integrate
sentiment analysis and real-time economic indicators to further enhance model
adaptability in volatile markets.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [85] [Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation](https://arxiv.org/abs/2506.09485)
*Yuxin Liu,Zhenghao Peng,Xuanhao Cui,Bolei Zhou*

Main category: cs.RO

TL;DR: 提出Adv-BMT框架，通过逆向运动预测生成对抗性驾驶场景，无需碰撞数据预训练，实验显示降低20%碰撞率。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶测试受限于真实数据集中长尾安全关键场景的稀缺性，难以充分验证系统性能。

Method: 两阶段框架：1）对抗初始化生成初始扰动；2）双向运动变换器(BMT)逆向重构交通流，利用终态信息预测全时序交通运动。

Result: 生成的碰撞场景质量显著提升，使用增强数据训练可使碰撞率比现有方法降低20%。

Conclusion: Adv-BMT首次实现无需碰撞数据预训练的对抗场景生成，能创造更真实多样的安全关键场景，提升自动驾驶测试有效性。

Abstract: Scenario-based testing is essential for validating the performance of
autonomous driving (AD) systems. However, such testing is limited by the
scarcity of long-tailed, safety-critical scenarios in existing datasets
collected in the real world. To tackle the data issue, we propose the Adv-BMT
framework, which augments real-world scenarios with diverse and realistic
adversarial interactions. The core component of Adv-BMT is a bidirectional
motion transformer (BMT) model to perform inverse traffic motion predictions,
which takes agent information in the last time step of the scenario as input,
and reconstruct the traffic in the inverse of chronological order until the
initial time step. The Adv-BMT framework is a two-staged pipeline: it first
conducts adversarial initializations and then inverse motion predictions.
Different from previous work, we do not need any collision data for
pretraining, and are able to generate realistic and diverse collision
interactions. Our experimental results validate the quality of generated
collision scenarios by Adv-BMT: training in our augmented dataset would reduce
episode collision rates by 20\% compared to previous work.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [86] [UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench](https://arxiv.org/abs/2506.09289)
*Boxi Yu,Yuxuan Zhu,Pinjia He,Daniel Kang*

Main category: cs.SE

TL;DR: 研究提出UTGenerator自动生成Python项目测试用例，并构建UTBoost框架增强测试覆盖，发现SWE-Bench基准中40.9%的Lite版本和24.4%的Verified版本存在错误标注，显著影响排行榜排名。


<details>
  <summary>Details</summary>
Motivation: SWE-Bench基准中手动编写的测试用例不足以有效评估代码生成代理，导致生成的补丁即使未解决问题也能通过测试。

Method: 开发LLM驱动的UTGenerator自动分析代码库生成测试用例，并基于此构建UTBoost框架进行系统性测试增强。

Result: 发现36个测试不足的任务实例，纠正345个误判补丁，导致SWE-Bench Lite和Verified排行榜分别产生18和11次排名变化。

Conclusion: UTBoost框架显著提升代码生成评估可靠性，测试用例质量对基准有效性具有关键影响，修正后结果对现有排名产生实质性改变。

Abstract: The advent of Large Language Models (LLMs) has spurred the development of
coding agents for real-world code generation. As a widely used benchmark for
evaluating the code generation capabilities of these agents, SWE-Bench uses
real-world problems based on GitHub issues and their corresponding pull
requests. However, the manually written test cases included in these pull
requests are often insufficient, allowing generated patches to pass the tests
without resolving the underlying issue. To address this challenge, we introduce
UTGenerator, an LLM-driven test case generator that automatically analyzes
codebases and dependencies to generate test cases for real-world Python
projects. Building on UTGenerator, we propose UTBoost, a comprehensive
framework for test case augmentation. In our evaluation, we identified 36 task
instances with insufficient test cases and uncovered 345 erroneous patches
incorrectly labeled as passed in the original SWE Bench. These corrections,
impacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard
entries, yield 18 and 11 ranking changes, respectively.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [87] [Effective Red-Teaming of Policy-Adherent Agents](https://arxiv.org/abs/2506.09600)
*Itay Nakash,George Kour,Koren Lazar,Matan Vetzler,Guy Uziel,Ateret Anaby-Tavor*

Main category: cs.MA

TL;DR: 提出对抗性红队系统CRAFT和评估基准tau-break，揭示现有策略一致性AI代理的脆弱性


<details>
  <summary>Details</summary>
Motivation: 确保任务导向的LLM代理在严格政策场景下既能遵守规则又保持自然交互，同时抵御恶意用户的策略性攻击

Method: 1) 提出新型威胁模型 2) 开发多代理红队系统CRAFT 3) 构建tau-break基准 4) 测试现有防御策略

Result: CRAFT在客服场景中的攻击成功率超越传统越狱方法；现有防御措施仅提供有限保护

Conclusion: 需要基于研究的强安全机制来保护政策遵循型AI代理，当前简单防御方案不足以应对系统性对抗攻击

Abstract: Task-oriented LLM-based agents are increasingly used in domains with strict
policies, such as refund eligibility or cancellation rules. The challenge lies
in ensuring that the agent consistently adheres to these rules and policies,
appropriately refusing any request that would violate them, while still
maintaining a helpful and natural interaction. This calls for the development
of tailored design and evaluation methodologies to ensure agent resilience
against malicious user behavior. We propose a novel threat model that focuses
on adversarial users aiming to exploit policy-adherent agents for personal
benefit. To address this, we present CRAFT, a multi-agent red-teaming system
that leverages policy-aware persuasive strategies to undermine a
policy-adherent agent in a customer-service scenario, outperforming
conventional jailbreak methods such as DAN prompts, emotional manipulation, and
coercive. Building upon the existing tau-bench benchmark, we introduce
tau-break, a complementary benchmark designed to rigorously assess the agent's
robustness against manipulative user behavior. Finally, we evaluate several
straightforward yet effective defense strategies. While these measures provide
some protection, they fall short, highlighting the need for stronger,
research-driven safeguards to protect policy-adherent agents from adversarial
attacks

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [88] [Adversarial Text Generation with Dynamic Contextual Perturbation](https://arxiv.org/abs/2506.09148)
*Hetvi Waghela,Jaydip Sen,Sneha Rakshit,Subhasis Dasgupta*

Main category: cs.CR

TL;DR: 提出动态上下文扰动(DCP)方法，通过上下文感知的全局文本扰动提升对抗攻击效果


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法局限于局部文本修改，导致扰动易被检测且语义不连贯，需考虑上下文生成更自然的对抗样本

Method: 利用预训练语言模型动态生成跨句子/段落/文档的扰动，通过对抗目标函数迭代优化扰动，平衡误导分类与文本自然性

Result: 在多个NLP模型和数据集上验证了DCP的有效性，显著增强了对抗攻击的隐蔽性和破坏力

Conclusion: 上下文分析对对抗攻击至关重要，该研究为构建抗攻击的鲁棒NLP系统奠定了基础

Abstract: Adversarial attacks on Natural Language Processing (NLP) models expose
vulnerabilities by introducing subtle perturbations to input text, often
leading to misclassification while maintaining human readability. Existing
methods typically focus on word-level or local text segment alterations,
overlooking the broader context, which results in detectable or semantically
inconsistent perturbations. We propose a novel adversarial text attack scheme
named Dynamic Contextual Perturbation (DCP). DCP dynamically generates
context-aware perturbations across sentences, paragraphs, and documents,
ensuring semantic fidelity and fluency. Leveraging the capabilities of
pre-trained language models, DCP iteratively refines perturbations through an
adversarial objective function that balances the dual objectives of inducing
model misclassification and preserving the naturalness of the text. This
comprehensive approach allows DCP to produce more sophisticated and effective
adversarial examples that better mimic natural language patterns. Our
experimental results, conducted on various NLP models and datasets, demonstrate
the efficacy of DCP in challenging the robustness of state-of-the-art NLP
systems. By integrating dynamic contextual analysis, DCP significantly enhances
the subtlety and impact of adversarial attacks. This study highlights the
critical role of context in adversarial attacks and lays the groundwork for
creating more robust NLP systems capable of withstanding sophisticated
adversarial strategies.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [89] [Ming-Omni: A Unified Multimodal Model for Perception and Generation](https://arxiv.org/abs/2506.09344)
*Inclusion AI,Biao Gong,Cheng Zou,Chuanyang Zheng,Chunluan Zhou,Canxiang Yan,Chunxiang Jin,Chunjie Shen,Dandan Zheng,Fudong Wang,Furong Xu,GuangMing Yao,Jun Zhou,Jingdong Chen,Jianxin Sun,Jiajia Liu,Jianjiang Zhu,Jun Peng,Kaixiang Ji,Kaiyou Song,Kaimeng Ren,Libin Wang,Lixiang Ru,Lele Xie,Longhua Tan,Lyuxin Xue,Lan Wang,Mochen Bai,Ning Gao,Pei Chen,Qingpei Guo,Qinglong Zhang,Qiang Xu,Rui Liu,Ruijie Xiong,Sirui Gao,Tinghao Liu,Taisong Li,Weilong Chai,Xinyu Xiao,Xiaomei Wang,Xiaoxue Chen,Xiao Lu,Xiaoyu Li,Xingning Dong,Xuzheng Yu,Yi Yuan,Yuting Gao,Yunxiao Sun,Yipeng Chen,Yifei Wu,Yongjie Lyu,Ziping Ma,Zipeng Feng,Zhijiang Fang,Zhihao Qiu,Ziyuan Huang,Zhengyu He*

Main category: cs.AI

TL;DR: Ming-Omni是首个开源统一多模态模型，支持图像/文本/音频/视频的感知与生成，匹配GPT-4o的模态支持并开源代码模型。


<details>
  <summary>Details</summary>
Motivation: 解决传统多模态模型需多模型配合、无法统一处理生成任务的问题，通过MoE架构实现跨模态高效融合。

Method: 1. 专用编码器提取多模态特征 2. 模态专用路由器的Ling-MoE架构 3. 集成音频解码器和Ming-Lite-Uni图像生成模块

Result: 实验证明模型实现全模态统一感知生成，音频生成质量达自然语音水平，图像编辑支持上下文感知交互。

Conclusion: 首次实现开源多模态模型的闭环感知-生成能力，为社区提供可扩展的统一框架，推动多模态AGI发展。

Abstract: We propose Ming-Omni, a unified multimodal model capable of processing
images, text, audio, and video, while demonstrating strong proficiency in both
speech and image generation. Ming-Omni employs dedicated encoders to extract
tokens from different modalities, which are then processed by Ling, an MoE
architecture equipped with newly proposed modality-specific routers. This
design enables a single model to efficiently process and fuse multimodal inputs
within a unified framework, thereby facilitating diverse tasks without
requiring separate models, task-specific fine-tuning, or structural redesign.
Importantly, Ming-Omni extends beyond conventional multimodal models by
supporting audio and image generation. This is achieved through the integration
of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for
high-quality image generation, which also allow the model to engage in
context-aware chatting, perform text-to-speech conversion, and conduct
versatile image editing. Our experimental results showcase Ming-Omni offers a
powerful solution for unified perception and generation across all modalities.
Notably, our proposed Ming-Omni is the first open-source model we are aware of
to match GPT-4o in modality support, and we release all code and model weights
to encourage further research and development in the community.

</details>


### [90] [A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy](https://arxiv.org/abs/2506.09420)
*Henry Peng Zou,Wei-Chieh Huang,Yaozu Wu,Chunyu Miao,Dongyuan Li,Aiwei Liu,Yue Zhou,Yankai Chen,Weizhi Zhang,Yangning Li,Liancheng Fang,Renhe Jiang,Philip S. Yu*

Main category: cs.AI

TL;DR: 质疑完全自主AI代理方向，提出以LLM为基础的人机协作系统（LLM-HAS）更可靠


<details>
  <summary>Details</summary>
Motivation: 当前自主AI系统存在可靠性、透明性及理解人类真实需求的问题，需保持人类在关键环节的决策权

Method: 构建人类持续提供指导、回答问题并保留控制权的LLM人机协作框架

Result: 通过医疗、金融、软件开发案例证明人机协作比纯AI系统更能有效处理复杂任务

Conclusion: AI进步应通过人机协作能力衡量，最理想方向是增强人类能力的伙伴系统而非角色替代

Abstract: Recent improvements in large language models (LLMs) have led many researchers
to focus on building fully autonomous AI agents. This position paper questions
whether this approach is the right path forward, as these autonomous systems
still have problems with reliability, transparency, and understanding the
actual requirements of human. We suggest a different approach: LLM-based
Human-Agent Systems (LLM-HAS), where AI works with humans rather than replacing
them. By keeping human involved to provide guidance, answer questions, and
maintain control, these systems can be more trustworthy and adaptable. Looking
at examples from healthcare, finance, and software development, we show how
human-AI teamwork can handle complex tasks better than AI working alone. We
also discuss the challenges of building these collaborative systems and offer
practical solutions. This paper argues that progress in AI should not be
measured by how independent systems become, but by how well they can work with
humans. The most promising future for AI is not in systems that take over human
roles, but in those that enhance human capabilities through meaningful
partnership.

</details>


### [91] [Intent Factored Generation: Unleashing the Diversity in Your Language Model](https://arxiv.org/abs/2506.09659)
*Eltayeb Ahmed,Uljad Berdica,Martha Elliott,Danijela Horak,Jakob N. Foerster*

Main category: cs.AI

TL;DR: 提出两阶段意图分解生成方法（IFG），通过先采样意图再生成回答的策略，在保持质量的同时显著提升LLM输出多样性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在token层面增加多样性导致推理任务探索不足和对话重复，需从根本上解决生成多样性受限的问题。

Method: 1. 意图采样阶段：用高温生成语义密集的意图（摘要/关键词）；2. 条件生成阶段：用低温基于意图+原提示生成连贯回答。支持链式推理中显式声明每步意图。

Result: 在数学/代码任务上提升pass@k指标，结合DPO增强对话多样性，新闻评论生成任务实现质量-多样性双优。开源新数据集。

Conclusion: 仅需修改prompt和温度调节即可实施的通用方法，可集成到多种算法中提升各类应用效果。

Abstract: Obtaining multiple meaningfully diverse, high quality samples from Large
Language Models for a fixed prompt remains an open challenge. Current methods
for increasing diversity often only operate at the token-level, paraphrasing
the same response. This is problematic because it leads to poor exploration on
reasoning problems and to unengaging, repetitive conversational agents. To
address this we propose Intent Factored Generation (IFG), factorising the
sampling process into two stages. First, we sample a semantically dense intent,
e.g., a summary or keywords. Second, we sample the final response conditioning
on both the original prompt and the intent from the first stage. This allows us
to use a higher temperature during the intent step to promote conceptual
diversity, and a lower temperature during the final generation to ensure the
outputs are coherent and self-consistent. Additionally, we find that prompting
the model to explicitly state its intent for each step of the chain-of-thought
before generating the step is beneficial for reasoning tasks. We demonstrate
our method's effectiveness across a diverse set of tasks. We show this method
improves both pass@k and Reinforcement Learning from Verifier Feedback on maths
and code tasks. For instruction-tuning, we combine IFG with Direct Preference
Optimisation to increase conversational diversity without sacrificing reward.
Finally, we achieve higher diversity while maintaining the quality of
generations on a general language modelling task, using a new dataset of reader
comments and news articles that we collect and open-source. In summary, we
present a simple method of increasing the sample diversity of LLMs while
maintaining performance. This method can be implemented by changing the prompt
and varying the temperature during generation, making it easy to integrate into
many algorithms for gains across various applications.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [92] [You Are What You Say: Exploiting Linguistic Content for VoicePrivacy Attacks](https://arxiv.org/abs/2506.09521)
*Ünal Ege Gaznepoglu,Anna Leschanowsky,Ahmad Aloradi,Prachi Singh,Daniel Tenbrinck,Emanuël A. P. Habets,Nils Peters*

Main category: eess.AS

TL;DR: 使用BERT语言模型评估说话人匿名化系统的隐私保护效果，发现仅文本内容即可实现说话人识别，揭示数据集存在内容偏见并建议改进评估方法


<details>
  <summary>Details</summary>
Motivation: 当前基于全局平均等错误率(EER)的说话人匿名化评估存在缺陷，需验证数据集内容相似性对隐私评估的影响

Method: 将BERT语言模型改造为自动说话人验证系统，基于VoicePrivacy挑战数据集，通过文本内容相似性分析实现说话人识别

Result: 在纯文本分析下达到35%平均EER（个别说话人低至2%），可解释性研究显示识别依赖语义关键词的相似性模式

Conclusion: 建议重构VoicePrivacy数据集以消除内容偏见，并质疑当前依赖全局EER作为隐私评估指标的可靠性

Abstract: Speaker anonymization systems hide the identity of speakers while preserving
other information such as linguistic content and emotions. To evaluate their
privacy benefits, attacks in the form of automatic speaker verification (ASV)
systems are employed. In this study, we assess the impact of intra-speaker
linguistic content similarity in the attacker training and evaluation datasets,
by adapting BERT, a language model, as an ASV system. On the VoicePrivacy
Attacker Challenge datasets, our method achieves a mean equal error rate (EER)
of 35%, with certain speakers attaining EERs as low as 2%, based solely on the
textual content of their utterances. Our explainability study reveals that the
system decisions are linked to semantically similar keywords within utterances,
stemming from how LibriSpeech is curated. Our study suggests reworking the
VoicePrivacy datasets to ensure a fair and unbiased evaluation and challenge
the reliance on global EER for privacy evaluations.

</details>


### [93] [Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements](https://arxiv.org/abs/2506.09707)
*Suhas BN,Andrew M. Sherrill,Jyoti Alaparthi,Dominik Mattioli,Rosa I. Arriaga,Chris W. Wiese,Saeed Abdullah*

Main category: eess.AS

TL;DR: 提出基于Qwen2-Audio模型和LoRA微调的自动定位方法，用于PE治疗中关键环节的时域定位，MAE达5.3秒


<details>
  <summary>Details</summary>
Motivation: 传统PE疗法忠诚度评估依赖人工审核录音，效率低下且成本高昂

Method: 使用LoRA微调预训练音频-语言模型，结合30秒窗口的音频转录输入，通过LLM生成协议阶段标签并进行人工验证

Result: 最佳配置(LoRA秩8+30秒窗口)在313个真实会话中实现5.3秒平均绝对误差，窗口大小和LoRA秩显著影响性能

Conclusion: 该框架为PE治疗忠诚度追踪提供可扩展解决方案，有助于临床培训和质量监控

Abstract: Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic
stress disorder (PTSD), but evaluating therapist fidelity remains
labor-intensive due to the need for manual review of session recordings. We
present a method for the automatic temporal localization of key PE fidelity
elements -- identifying their start and stop times -- directly from session
audio and transcripts. Our approach fine-tunes a large pre-trained
audio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process
focused 30-second windows of audio-transcript input. Fidelity labels for three
core protocol phases -- therapist orientation (P1), imaginal exposure (P2), and
post-imaginal processing (P3) -- are generated via LLM-based prompting and
verified by trained raters. The model is trained to predict normalized boundary
offsets using soft supervision guided by task-specific prompts. On a dataset of
313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)
achieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further
analyze the effects of window size and LoRA rank, highlighting the importance
of context granularity and model adaptation. This work introduces a scalable
framework for fidelity tracking in PE therapy, with potential to support
clinician training, supervision, and quality assurance.

</details>


### [94] [Regularizing Learnable Feature Extraction for Automatic Speech Recognition](https://arxiv.org/abs/2506.09804)
*Peter Vieting,Maximilian Kannen,Benedikt Hilmes,Ralf Schlüter,Hermann Ney*

Main category: eess.AS

TL;DR: 研究正则化方法改进可学习特征前端的ASR性能，通过音频扰动和STFT域掩码策略缩小与传统方法的差距


<details>
  <summary>Details</summary>
Motivation: 神经前端虽可直接优化适配声学模型，但易过拟合导致性能低于传统固定特征方法

Method: 1. 探索音频扰动方法对可学习特征的增益 2. 发现SpecAugment在STFT域的局限性，提出STFT域掩码改进方案

Result: 两种正则化方法结合后，可学习特征与传统方法性能差距显著缩小

Conclusion: 有效正则化策略能提升神经前端实用性，为端到端ASR系统优化提供新思路

Abstract: Neural front-ends are an appealing alternative to traditional, fixed feature
extraction pipelines for automatic speech recognition (ASR) systems since they
can be directly trained to fit the acoustic model. However, their performance
often falls short compared to classical methods, which we show is largely due
to their increased susceptibility to overfitting. This work therefore
investigates regularization methods for training ASR models with learnable
feature extraction front-ends. First, we examine audio perturbation methods and
show that larger relative improvements can be obtained for learnable features.
Additionally, we identify two limitations in the standard use of SpecAugment
for these front-ends and propose masking in the short time Fourier transform
(STFT)-domain as a simple but effective modification to address these
challenges. Finally, integrating both regularization approaches effectively
closes the performance gap between traditional and learnable features.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [95] [ThinkQE: Query Expansion via an Evolving Thinking Process](https://arxiv.org/abs/2506.09260)
*Yibin Lei,Tao Shen,Andrew Yates*

Main category: cs.IR

TL;DR: 提出无需训练的ThinkQE框架，通过思维扩展和语料交互提升检索性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM扩展方法过于聚焦单一方向，无法充分捕捉查询的多面性需求

Method: 结合思维链促进语义探索 + 基于语料反馈的迭代优化机制

Result: 在DL19/DL20/BRIGHT等基准测试中全面超越现有方法（包括需训练模型）

Conclusion: 该框架有效平衡探索与多样性，验证了无训练方案的潜力

Abstract: Effective query expansion for web search benefits from promoting both
exploration and result diversity to capture multiple interpretations and facets
of a query. While recent LLM-based methods have improved retrieval performance
and demonstrate strong domain generalization without additional training, they
often generate narrowly focused expansions that overlook these desiderata. We
propose ThinkQE, a test-time query expansion framework addressing this
limitation through two key components: a thinking-based expansion process that
encourages deeper and comprehensive semantic exploration, and a
corpus-interaction strategy that iteratively refines expansions using retrieval
feedback from the corpus. Experiments on diverse web search benchmarks (DL19,
DL20, and BRIGHT) show ThinkQE consistently outperforms prior approaches,
including training-intensive dense retrievers and rerankers.

</details>
