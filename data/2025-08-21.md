<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 42]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.AI](#cs.AI) [Total: 3]
- [eess.AS](#eess.AS) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [From Image Captioning to Visual Storytelling](https://arxiv.org/abs/2508.14045)
*Admitos Passadakis,Yingjin Song,Albert Gatt*

Main category: cs.CL

TL;DR: 视觉叙事研究通过结合图像描述生成与语言叙事转换的框架，提升故事连贯性并加速训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决视觉叙事任务中需同时兼顾图像基础与叙事连贯性的挑战，突破传统单一模型框架的限制。

Method: 采用两阶段流程：1. 视觉-语言模型生成图像描述；2. 语言-语言模型将描述转化为连贯叙事。

Result: 统一框架提升故事质量，训练速度提升并具备可复现性，提出模拟人类叙事质量的新指标'ideality'。

Conclusion: 多模态任务分解方法有效平衡图像基础与叙事性，新度量标准为评估叙事人类相似度提供工具支持。

Abstract: Visual Storytelling is a challenging multimodal task between Vision &
Language, where the purpose is to generate a story for a stream of images. Its
difficulty lies on the fact that the story should be both grounded to the image
sequence but also narrative and coherent. The aim of this work is to balance
between these aspects, by treating Visual Storytelling as a superset of Image
Captioning, an approach quite different compared to most of prior relevant
studies. This means that we firstly employ a vision-to-language model for
obtaining captions of the input images, and then, these captions are
transformed into coherent narratives using language-to-language methods. Our
multifarious evaluation shows that integrating captioning and storytelling
under a unified framework, has a positive impact on the quality of the produced
stories. In addition, compared to numerous previous studies, this approach
accelerates training time and makes our framework readily reusable and
reproducible by anyone interested. Lastly, we propose a new metric/tool, named
ideality, that can be used to simulate how far some results are from an oracle
model, and we apply it to emulate human-likeness in visual storytelling.

</details>


### [2] [Benchmarking Sociolinguistic Diversity in Swahili NLP: A Taxonomy-Guided Approach](https://arxiv.org/abs/2508.14051)
*Kezia Oketch,John P. Lalor,Ahmed Abbasi*

Main category: cs.CL

TL;DR: 首次提出分类学指导的斯瓦希里语NLP评估框架，揭示社会语言变异对模型性能的系统性影响


<details>
  <summary>Details</summary>
Motivation: 现有NLP评估缺乏对社会语言多样性（如部落文化、语码混合现象）的系统考量，特别是在非洲语言场景中

Method: 收集2,170份健康主题的肯尼亚用户自由文本→构建结构化分类法→分析预训练模型与指令调优模型在语言变体上的错误模式

Result: 模型预测错误与部落特征/城市方言/语码混合程度呈显著相关性，指令调优模型相对基线错误率降低18%

Conclusion: 建立文化关联的评估框架能有效诊断模型局限，社会语言变异应成为NLP系统设计的关键考量维度

Abstract: We introduce the first taxonomy-guided evaluation of Swahili NLP, addressing
gaps in sociolinguistic diversity. Drawing on health-related psychometric
tasks, we collect a dataset of 2,170 free-text responses from Kenyan speakers.
The data exhibits tribal influences, urban vernacular, code-mixing, and
loanwords. We develop a structured taxonomy and use it as a lens for examining
model prediction errors across pre-trained and instruction-tuned language
models. Our findings advance culturally grounded evaluation frameworks and
highlight the role of sociolinguistic variation in shaping model performance.

</details>


### [3] [Contrastive Analysis of Constituent Order Preferences Within Adverbial Roles in English and Chinese News: A Large-Language-Model-Driven Approach](https://arxiv.org/abs/2508.14054)
*Yiran Rex Ma*

Main category: cs.CL

TL;DR: 基于LLM标注的英汉新闻语料库，对比分析英汉新闻功能块的位置差异及其对信息结构的影响


<details>
  <summary>Details</summary>
Motivation: 揭示英汉新闻在信息组织方式上的系统性差异，探究词序偏好与语用功能的互动关系

Method: 使用LLM标注的可比语料库，通过功能块角色标注和定量统计方法进行对比分析

Result: 发现英语偏好核心信息线性叙述+功能块后置，中文倾向背景前置模式；SVO结构中中文前置更显著；功能块共现时双语序调整均受语用驱动

Conclusion: 词序具有系统性偏好与动态适应性双重特征，为英汉信息结构对比研究提供了新的类型学证据

Abstract: Based on comparable English-Chinese news corpora annotated by Large Language
Model (LLM), this paper attempts to explore the differences in constituent
order of English-Chinese news from the perspective of functional chunks with
adverbial roles, and analyze their typical positional preferences and
distribution patterns. It is found that: (1) English news prefers linear
narrative of core information first, and functional chunks are mostly
post-positioned, while Chinese news prefers overall presentation mode of
background first, and functional chunks are often pre-positioned; (2) In SVO
structure, both English and Chinese news show differences in the distribution
of functional chunks, but the tendency of Chinese pre-positioning is more
significant, while that of English post-positioning is relatively mild; (3)
When function blocks are co-occurring, both English and Chinese news show high
flexibility, and the order adjustment is driven by information and pragmatic
purposes. The study reveals that word order has both systematic preference and
dynamic adaptability, providing new empirical support for contrastive study of
English-Chinese information structure.

</details>


### [4] [T-REX: Table -- Refute or Entail eXplainer](https://arxiv.org/abs/2508.14055)
*Tim Luka Horstmann,Baptiste Geisenberger,Mehwish Alam*

Main category: cs.CL

TL;DR: T-REX是一个基于先进指令调优推理大模型的交互式表格事实核查工具，旨在为非专家用户提供多模态、多语言的声明验证功能。


<details>
  <summary>Details</summary>
Motivation: 现有表格事实核查技术虽借助大模型取得进展，但使用门槛过高难以普惠非专业用户。本文旨在通过开发易用工具，将前沿事实核查技术民主化。

Method: 采用指令调优的先进大语言模型(LLMs)，构建支持多模态(表格+文本)和多语言处理的交互式系统，强调解释性和透明度设计。

Result: 成功开发出首个面向非专家的实时交互式表格事实核查系统T-REX，该系统已在线开放使用。

Conclusion: T-REX通过降低技术使用门槛，推动了事实核查技术的普惠化应用，为提升信息验证可靠性提供了创新解决方案。

Abstract: Verifying textual claims against structured tabular data is a critical yet
challenging task in Natural Language Processing with broad real-world impact.
While recent advances in Large Language Models (LLMs) have enabled significant
progress in table fact-checking, current solutions remain inaccessible to
non-experts. We introduce T-REX (T-REX: Table -- Refute or Entail eXplainer),
the first live, interactive tool for claim verification over multimodal,
multilingual tables using state-of-the-art instruction-tuned reasoning LLMs.
Designed for accuracy and transparency, T-REX empowers non-experts by providing
access to advanced fact-checking technology. The system is openly available
online.

</details>


### [5] [Confidence Estimation for Text-to-SQL in Large Language Models](https://arxiv.org/abs/2508.14056)
*Sepideh Entezari Maleki,Mohammadreza Pourreza,Davood Rafiei*

Main category: cs.CL

TL;DR: 论文系统评估了文本到SQL任务中黑盒与白盒置信度估计方法，发现基于一致性的黑盒方法和SQL语法感知的白盒策略效果最优，且执行结果可有效提升置信度评估。


<details>
  <summary>Details</summary>
Motivation: 现有置信度估计方法在LLM场景下面临模型参数不可访问的限制，需探索不依赖黄金答案的可靠性评估方案，并验证执行结果对置信度评估的增强作用。

Method: 通过跨领域文本到SQL基准测试，对比分析黑盒（一致性方法）和白盒（SQL语法解析）策略，并引入查询执行结果作为补充评估维度。

Result: 黑盒模型中一致性方法表现最佳（平均提升12% AUC），白盒模型中SQL语法敏感策略有效性显著（错误检测率提高18%），执行结果使两类方法F1-score提升5-7%。

Conclusion: 执行结果提供关键补充信号，黑盒/白盒方法各具优势。建议实际应用中优先采用一致性方法（黑盒场景），模型可解释时结合SQL语法特征（白盒场景）。

Abstract: Confidence estimation for text-to-SQL aims to assess the reliability of
model-generated SQL queries without having access to gold answers. We study
this problem in the context of large language models (LLMs), where access to
model weights and gradients is often constrained. We explore both black-box and
white-box confidence estimation strategies, evaluating their effectiveness on
cross-domain text-to-SQL benchmarks. Our evaluation highlights the superior
performance of consistency-based methods among black-box models and the
advantage of SQL-syntax-aware approaches for interpreting LLM logits in
white-box settings. Furthermore, we show that execution-based grounding of
queries provides a valuable supplementary signal, improving the effectiveness
of both approaches.

</details>


### [6] [Assessing and Mitigating Data Memorization Risks in Fine-Tuned Large Language Models](https://arxiv.org/abs/2508.14062)
*Badrinath Ramakrishnan,Akshaya Balaji*

Main category: cs.CL

TL;DR: 大语言模型微调存在数据记忆隐私风险，本文提出多层防护框架可实现零数据泄漏同时保留94.7%模型效用


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型微调过程中重复敏感数据导致隐私泄漏率激增（从0-5%升至60-75%）的安全隐患，需开发有效防护方案

Method: 提出语义去重、差分隐私生成、熵值过滤和模式过滤四重防护机制，在GPT-2/Phi-3/Gemma-2等模型验证有效性

Result: 综合防护框架使数据泄漏率降至0%，模型效用保留率达94.7%（相比未防护状态64.2%的泄漏增长）

Conclusion: 多层隐私保护框架成功平衡模型性能与数据安全，实证多技术协同可消除微调过程中的隐私泄漏风险

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse natural language processing tasks, but their tendency to memorize
training data poses significant privacy risks, particularly during fine-tuning
processes. This paper presents a comprehensive empirical analysis of data
memorization in fine-tuned LLMs and introduces a novel multi-layered privacy
protection framework. Through controlled experiments on modern LLM
architectures including GPT-2, Phi-3, and Gemma-2, we demonstrate that
fine-tuning with repeated sensitive data increases privacy leakage rates from
baseline levels of 0-5% to 60-75%, representing a 64.2% average increase across
tested models. We propose and rigorously evaluate four complementary privacy
protection methods: semantic data deduplication, differential privacy during
generation, entropy-based filtering, and pattern-based content filtering. Our
experimental results show that these techniques can reduce data leakage to 0%
while maintaining 94.7% of original model utility.

</details>


### [7] [Punctuation and Predicates in Language Models](https://arxiv.org/abs/2508.14067)
*Sonakshi Chauhan,Maheep Chaudhary,Koby Choy,Samuel Nellessen,Nandi Schoots*

Main category: cs.CL

TL;DR: 研究探索大语言模型中信息收集与传播机制，发现不同模型对标点符号的依赖程度差异显著（GPT-2依赖性强，Gemma无依赖），并揭示条件语句与全称量化在模型中的差异化处理机制。


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型内部信息处理机制不透明的问题，研究标点符号作为注意力锚点的计算重要性差异，并扩展探究不同语法成分和逻辑规则的处理方式差异。

Method: 采用干预技术（置换干预/层交换实验），在GPT-2/DeepSeek/Gemma模型中验证标点符号的必要性与充分性，分析输入成分（主语/形容词/整句）处理模式，对比条件语句与全称量化的处理差异。

Result: GPT-2多层级依赖标点符号，DeepSeek/Gemma无此特性；条件语句（if-then）与全称量化（for all）在神经网络中呈现差异化处理路径。

Conclusion: 研究揭示大语言模型对标点符号和逻辑规则的处理存在模型特异性，为模型可解释性研究提供新视角，暗示不同推理规则可能对应独立处理模块的存在。

Abstract: In this paper we explore where information is collected and how it is
propagated throughout layers in large language models (LLMs). We begin by
examining the surprising computational importance of punctuation tokens which
previous work has identified as attention sinks and memory aids. Using
intervention-based techniques, we evaluate the necessity and sufficiency (for
preserving model performance) of punctuation tokens across layers in GPT-2,
DeepSeek, and Gemma. Our results show stark model-specific differences: for
GPT-2, punctuation is both necessary and sufficient in multiple layers, while
this holds far less in DeepSeek and not at all in Gemma. Extending beyond
punctuation, we ask whether LLMs process different components of input (e.g.,
subjects, adjectives, punctuation, full sentences) by forming early static
summaries reused across the network, or if the model remains sensitive to
changes in these components across layers. Extending beyond punctuation, we
investigate whether different reasoning rules are processed differently by
LLMs. In particular, through interchange intervention and layer-swapping
experiments, we find that conditional statements (if, then), and universal
quantification (for all) are processed very differently. Our findings offer new
insight into the internal mechanisms of punctuation usage and reasoning in LLMs
and have implications for interpretability.

</details>


### [8] [DLLMQuant: Quantizing Diffusion-based Large Language Models](https://arxiv.org/abs/2508.14090)
*Chen Xu,Dawei Yang*

Main category: cs.CL

TL;DR: 针对扩散大语言模型(DLLMs)的量化难题，提出DLLMQuant框架，通过时间掩码自适应采样、交互感知激活量化和确定性引导量化三项核心技术，在保持效率的同时显著提升量化模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统后训练量化方法在扩散大语言模型上存在严重性能退化（如AWQ在W4A4下准确率下降16%），主要源于动态掩码、迭代生成机制与量化过程的不兼容性。

Method: 1) TMAS校准方法捕捉时间维度和掩码状态下的特征分布；2) IA-AQ利用双向注意力信号动态分配量化资源；3) CGQ通过掩码状态和token置信度指导权重量化误差补偿。

Result: 实验证明DLLMQuant显著优于传统PTQ方法，在保持计算效率的同时恢复模型精度，特别是在多步迭代生成场景下有效抑制误差累积。

Conclusion: 该框架首次系统解决DLLMs量化中的时序依赖、误差传播和分布异质性问题，为扩散模型的轻量化部署提供了有效解决方案。

Abstract: Diffusion-based large language models (DLLMs) have shown promise for
non-autoregressive text generation, but their deployment is constrained by
large model sizes and heavy computational costs. Post-training quantization
(PTQ), a widely used method for compressing and accelerating Large Language
Models (LLMs), suffers from severe accuracy degradation and reduced
generalization performance when directly applied to DLLMs (e.g., AWQ suffers a
16% accuracy drop on LLADA under W4A4). This paper explores how DLLMs' key
mechanisms - dynamic masking, iterative generation, bidirectional attention -
clash with quantization. We identify three core issues: 1) Iterative generation
and dynamic masking ratios lead to distinct token distributions across decoding
steps, which are not adequately captured by existing PTQ calibration methods;
2) Quantization errors are accumulated and amplified progressively during
iteration in DLLMs, causing quantized models to perform worse as decoding steps
progress; 3) Unmasked tokens stabilize while masked remain probabilistic,
making overall feature distribution incompatible with existing PTQ methods. To
address these issues, we propose DLLMQuant, a PTQ framework tailored for DLLMs,
which incorporates three novel techniques: 1) Temporal-Mask Adaptive Sampling
(TMAS), a calibration method that accounts for both time and mask factors, with
the capacity to capture distributions across timesteps. 2) Interaction-Aware
Activation Quantization (IA-AQ), which utilizes bidirectional attention's
interaction signals to dynamically allocate quantization resources. 3)
Certainty-Guided Quantization (CGQ), which integrates mask status and token
scores as key weighting criteria into error compensation, making weight
quantization more suitable for DLLMs. Experiments show that DLLMQuant achieves
significant performance gains while enhancing efficiency.

</details>


### [9] [MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation](https://arxiv.org/abs/2508.14146)
*Xian Gao,Jiacheng Ruan,Zongyun Zhang,Jingsheng Gao,Ting Liu,Yuzhuo Fu*

Main category: cs.CL

TL;DR: 提出多学科多模态评审基准MMReview，用于系统评估语言模型在学术同行评审任务中的全面性、准确性和人类对齐能力


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的评审系统缺乏跨学科/多模态的统一评估标准，难以验证模型在包含图表等多模态内容的复杂场景下的评审能力

Method: 构建包含4大学科17个领域240篇论文的多模态数据集，设计4大类13项评审任务，涵盖逐步评审生成、结果规范化、人类偏好对齐和对抗鲁棒性评估

Result: 通过16个开源模型和5个闭源模型的广泛实验验证基准有效性，揭示不同模型在评审任务中的性能差异

Conclusion: MMReview为开发自动化同行评审系统建立了标准化评估框架，推动该领域研究的规范化发展

Abstract: With the rapid growth of academic publications, peer review has become an
essential yet time-consuming responsibility within the research community.
Large Language Models (LLMs) have increasingly been adopted to assist in the
generation of review comments; however, current LLM-based review tasks lack a
unified evaluation benchmark to rigorously assess the models' ability to
produce comprehensive, accurate, and human-aligned assessments, particularly in
scenarios involving multimodal content such as figures and tables. To address
this gap, we propose \textbf{MMReview}, a comprehensive benchmark that spans
multiple disciplines and modalities. MMReview includes multimodal content and
expert-written review comments for 240 papers across 17 research domains within
four major academic disciplines: Artificial Intelligence, Natural Sciences,
Engineering Sciences, and Social Sciences. We design a total of 13 tasks
grouped into four core categories, aimed at evaluating the performance of LLMs
and Multimodal LLMs (MLLMs) in step-wise review generation, outcome
formulation, alignment with human preferences, and robustness to adversarial
input manipulation. Extensive experiments conducted on 16 open-source models
and 5 advanced closed-source models demonstrate the thoroughness of the
benchmark. We envision MMReview as a critical step toward establishing a
standardized foundation for the development of automated peer review systems.

</details>


### [10] [DPad: Efficient Diffusion Language Models with Suffix Dropout](https://arxiv.org/abs/2508.14148)
*Xinhua Chen,Sitao Huang,Cong Guo,Chiyue Wei,Yintao He,Jianyi Zhang,Hai "Hellen" Li,Yiran Chen*

Main category: cs.CL

TL;DR: DPad通过滑动窗口和距离衰减丢弃策略，显著提升扩散大语言模型推理速度，最高达61.4倍且保持精度。


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型因需预测所有未来后缀token导致计算冗余，存在高计算开销问题。

Method: 结合固定长度滑动窗口（保留近邻后缀token）和确定性距离衰减丢弃策略（移除远端token），兼容现有优化技术如前缀缓存。

Result: 在LLaDA-1.5和Dream模型的多基准测试中实现最高61.4倍加速，精度基本持平。

Conclusion: DPad以极简设计实现了扩散模型的高效长序列推理，具有工程易用性和扩展潜力。

Abstract: Diffusion-based Large Language Models (dLLMs) parallelize text generation by
framing decoding as a denoising process, but suffer from high computational
overhead since they predict all future suffix tokens at each step while
retaining only a small fraction. We propose Diffusion Scratchpad (DPad), a
training-free method that restricts attention to a small set of nearby suffix
tokens, preserving fidelity while eliminating redundancy. DPad integrates two
strategies: (i) a sliding window, which maintains a fixed-length suffix window,
and (ii) distance-decay dropout, which deterministically removes distant suffix
tokens before attention computation. This simple design is compatible with
existing optimizations such as prefix caching and can be implemented with only
a few lines of code. Comprehensive evaluations across multiple benchmarks on
LLaDA-1.5 and Dream models demonstrate that DPad delivers up to
$\mathbf{61.4\times}$ speedup over vanilla dLLMs while maintaining comparable
accuracy, highlighting its potential for efficient and scalable long-sequence
inference. Our code is available at https://github.com/Crys-Chen/DPad.

</details>


### [11] [Comparing energy consumption and accuracy in text classification inference](https://arxiv.org/abs/2508.14170)
*Johannes Zschache,Tilman Hartwig*

Main category: cs.CL

TL;DR: 研究发现大语言模型推理阶段存在显著能耗差异（<mWh至>kWh），高精度模型可能同时具备能效优势，模型运行时间可作为能耗替代指标


<details>
  <summary>Details</summary>
Motivation: 大语言模型在NLP任务中部署增加引发能源效率担忧，现有研究多集中于训练阶段能耗而忽视推理阶段系统分析

Method: 通过实证分析比较不同模型架构和硬件配置下文本分类任务的准确率与推理能耗关系，量化模型类型、规模及硬件对能效的影响

Result: 最佳准确率模型可能同时节能，大规模LLMs能耗显著增加但准确率下降，推理能耗与运行时间呈现强相关性（r=0.89）

Conclusion: 研究为可持续AI发展提供实证依据，建议在模型选择时平衡性能与能效，推荐使用运行时间作为间接能耗评估指标

Abstract: The increasing deployment of large language models (LLMs) in natural language
processing (NLP) tasks raises concerns about energy efficiency and
sustainability. While prior research has largely focused on energy consumption
during model training, the inference phase has received comparatively less
attention. This study systematically evaluates the trade-offs between model
accuracy and energy consumption in text classification inference across various
model architectures and hardware configurations. Our empirical analysis shows
that the best-performing model in terms of accuracy can also be
energy-efficient, while larger LLMs tend to consume significantly more energy
with lower classification accuracy. We observe substantial variability in
inference energy consumption ($<$mWh to $>$kWh), influenced by model type,
model size, and hardware specifications. Additionally, we find a strong
correlation between inference energy consumption and model runtime, indicating
that execution time can serve as a practical proxy for energy usage in settings
where direct measurement is not feasible. These findings have implications for
sustainable AI development, providing actionable insights for researchers,
industry practitioners, and policymakers seeking to balance performance and
resource efficiency in NLP applications.

</details>


### [12] [Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper](https://arxiv.org/abs/2508.14273)
*Krishna Garg,Firoz Shaikh,Sambaran Bandyopadhyay,Cornelia Caragea*

Main category: cs.CL

TL;DR: 提出SciIG任务评估LLMs生成科研引言能力，构建NAACL/ICLR 2025新数据集，LLaMA-4 Maverick在语义相似性和忠实度表现最佳，三样本提示效果最优。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在生成高质量论文引言存在挑战，需系统性评估模型性能以提升科研写作助手效果。

Method: 通过构建新数据集，评估5个先进模型（含开源/闭源）的多维度指标，结合自动评估与LLM-as-a-judge框架。

Result: LLaMA-4 Maverick在7项指标中5项领先，三样本提示法显著优于少样本方法，开源模型Gemma-3-12B在内容覆盖度最佳。

Conclusion: 研究为开发科研写作助手提供实践框架，设定LLM辅助学术写作的合理预期，公开数据集促进领域发展。

Abstract: As researchers increasingly adopt LLMs as writing assistants, generating
high-quality research paper introductions remains both challenging and
essential. We introduce Scientific Introduction Generation (SciIG), a task that
evaluates LLMs' ability to produce coherent introductions from titles,
abstracts, and related works. Curating new datasets from NAACL 2025 and ICLR
2025 papers, we assess five state-of-the-art models, including both open-source
(DeepSeek-v3, Gemma-3-12B, LLaMA 4-Maverick, MistralAI Small 3.1) and
closed-source GPT-4o systems, across multiple dimensions: lexical overlap,
semantic similarity, content coverage, faithfulness, consistency, citation
correctness, and narrative quality. Our comprehensive framework combines
automated metrics with LLM-as-a-judge evaluations. Results demonstrate LLaMA-4
Maverick's superior performance on most metrics, particularly in semantic
similarity and faithfulness. Moreover, three-shot prompting consistently
outperforms fewer-shot approaches. These findings provide practical insights
into developing effective research writing assistants and set realistic
expectations for LLM-assisted academic writing. To foster reproducibility and
future research, we will publicly release all code and datasets.

</details>


### [13] [Disentangling concept semantics via multilingual averaging in Sparse Autoencoders](https://arxiv.org/abs/2508.14275)
*Cliff O'Reilly,Ernesto Jimenez-Ruiz,Tillman Weyde*

Main category: cs.CL

TL;DR: 提出通过多语言平均概念激活来隔离大语言模型中的概念语义，验证该方法能更好反映本体论真实关系。


<details>
  <summary>Details</summary>
Motivation: 现有文本表示方法（嵌入/稀疏自编码器）存在语义与句法/语言特征耦合的问题，需寻找更纯粹的概念表示方法。

Method: 基于OWL本体生成英/法/中三语文本→Gemma 2B模型处理→稀疏自编码器提取概念激活→跨语言激活平均→与本体映射关联性验证

Result: 多语言平均后的概念激活比单语言更符合本体类真实关系（相关性更强），揭示新的网络状态解释技术可能性。

Conclusion: 该方法为机制可解释性研究提供了新范式，通过语言无关的激活平均可提升网络状态分析的准确性。

Abstract: Connecting LLMs with formal knowledge representation and reasoning is a
promising approach to address their shortcomings. Embeddings and sparse
autoencoders are widely used to represent textual content, but the semantics
are entangled with syntactic and language-specific information. We propose a
method that isolates concept semantics in Large Langue Models by averaging
concept activations derived via Sparse Autoencoders. We create English text
representations from OWL ontology classes, translate the English into French
and Chinese and then pass these texts as prompts to the Gemma 2B LLM. Using the
open source Gemma Scope suite of Sparse Autoencoders, we obtain concept
activations for each class and language version. We average the different
language activations to derive a conceptual average. We then correlate the
conceptual averages with a ground truth mapping between ontology classes. Our
results give a strong indication that the conceptual average aligns to the true
relationship between classes when compared with a single language by itself.
The result hints at a new technique which enables mechanistic interpretation of
internal network states with higher accuracy.

</details>


### [14] [GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs](https://arxiv.org/abs/2508.14279)
*Adrian-Marius Dumitran,Alexandra-Mihaela Danila,Angela-Liliana Dumitran*

Main category: cs.CL

TL;DR: 首次提出罗马尼亚语评测基准GRILE，测试发现主流大模型在低资源语言教育场景存在准确性不足和解释缺陷问题


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在低资源语言（罗马尼亚语）教育场景中的可信赖性，填补该领域研究空白

Method: 构建包含1,151道罗马尼亚国家考试的基准测试集，评估7个多语言/罗马尼亚专用LLM的答案选择能力和语言学解释能力

Result: Gemini 2.5 Pro准确率达83%，但开源模型普遍低于65%，48%的解释存在事实错误或教学缺陷。错误分析显示形态学和正字法规范应用存在系统弱点

Conclusion: GRILE基准揭示了低资源教育场景NLP的挑战，为可控解释生成与评估提供新测试平台，所有数据代码已开源促进后续研究

Abstract: LLMs (Large language models) have revolutionized NLP (Natural Language
Processing), yet their pedagogical value for low-resource languages remains
unclear. We present GRILE (Grammar Romanian Inference and Language
Explanations) , the first open benchmark of 1,151 multiple-choice questions
harvested from Romanian high-stakes exams (National Evaluation, Baccalaureate,
university admissions). GRILE enables us to probe two complementary abilities
of seven state-of-the-art multilingual and Romanian-specific LLMs: (i)
selecting the correct answer, and (ii) producing linguistically accurate
explanations. While Gemini 2.5 Pro reaches 83% accuracy, most open-weight
models stay below 65%, and 48% of their explanations contain factual or
pedagogical flaws according to expert review. A detailed error analysis
pinpoints systematic weaknesses in morphology and in applying the latest DOOM3
orthographic norms. All data, code and a public web demo are released to
catalyze future research. Our findings expose open challenges for trustworthy
educational NLP in low-resource settings and establish GRILE as a new test-bed
for controllable explanation generation and evaluation.

</details>


### [15] [Tokens with Meaning: A Hybrid Tokenization Approach for NLP](https://arxiv.org/abs/2508.14292)
*M. Ali Bayram,Ali Arda Fincan,Ahmet Semih Gümüş,Sercan Karakaş,Banu Diri,Savaş Yıldırım,Demircan Çelik*

Main category: cs.CL

TL;DR: 提出了一种结合规则形态分析与统计子词分割的混合分词框架，优化了形态丰富语言的分词效果。


<details>
  <summary>Details</summary>
Motivation: 传统子词分词方法（如BPE）在形态复杂语言中过度依赖频率，忽略语言结构，导致冗余和语义损失。

Method: 采用音位归一化、词根-词缀字典、共享变体标识符算法，整合BPE处理未登录词，并添加特殊标记管理空格/大小写。

Result: 在TR-MMLU基准上取得土耳其语最高分词准确率（90.29%）和纯分词率（85.8%），相比LLaMA/GPT等模型生成更连贯的token。

Conclusion: 该方法具有语言普适性，为多语言NLP系统提供了更可解释、高效的分词方案。

Abstract: Tokenization plays a pivotal role in natural language processing (NLP),
shaping how text is segmented and interpreted by language models. While subword
methods such as Byte Pair Encoding (BPE) and WordPiece have been effective,
they often struggle with morphologically rich and agglutinative languages
because they rely on frequency rather than linguistic structure. We introduce a
hybrid tokenization framework that combines rule-based morphological analysis
with statistical subword segmentation. The method uses phonological
normalization, root-affix dictionaries, and a novel algorithm that balances
morpheme preservation with vocabulary efficiency. It assigns shared identifiers
to phonologically variant affixes (e.g., -ler and -lar) and altered root forms
(e.g., kitap vs. kitab{\i}), reducing redundancy while maintaining semantic
integrity. Special tokens are added for whitespace and case, including an
UPPERCASE marker to avoid vocabulary inflation from capitalization. BPE is
integrated for out-of-vocabulary coverage without harming morphological
coherence. On the TR-MMLU benchmark, the tokenizer achieves the highest Turkish
Token Percentage (90.29\%) and Pure Token Percentage (85.8\%). Comparisons with
tokenizers from LLaMA, Gemma, and GPT show more linguistically meaningful and
coherent tokens. Although demonstrated on Turkish, the approach is
language-independent and adaptable to other languages, offering a practical
path toward more interpretable and effective multilingual NLP systems.

</details>


### [16] [A Joint Multitask Model for Morpho-Syntactic Parsing](https://arxiv.org/abs/2508.14307)
*Demian Inostroza,Mel Mistica,Ekaterina Vylomova,Chris Guest,Kemal Kurniawan*

Main category: cs.CL

TL;DR: 联合多任务模型在UD标注方案下实现最佳形态句法分析性能，但核心语法格仍是挑战


<details>
  <summary>Details</summary>
Motivation: 针对多语言形态句法联合解析的挑战，开发统一模型处理多样语言类型，提升跨语言分析能力

Method: 使用共享XLM-RoBERTa编码器配合三个专用解码器（实词识别、依存解析、形态特征预测）的多任务架构

Result: 在九种语言上取得最佳综合表现（MSLAS 78.7%，LAS 80.1%，Feats F1 90.3%），消融实验显示分词质量对性能影响显著

Conclusion: 模型在多语言形态句法分析中表现优异，但对核心格标记（如主宾格）和名词性特征的跨语言处理仍需改进

Abstract: We present a joint multitask model for the UniDive 2025 Morpho-Syntactic
Parsing shared task, where systems predict both morphological and syntactic
analyses following novel UD annotation scheme. Our system uses a shared
XLM-RoBERTa encoder with three specialized decoders for content word
identification, dependency parsing, and morphosyntactic feature prediction. Our
model achieves the best overall performance on the shared task's leaderboard
covering nine typologically diverse languages, with an average MSLAS score of
78.7 percent, LAS of 80.1 percent, and Feats F1 of 90.3 percent. Our ablation
studies show that matching the task's gold tokenization and content word
identification are crucial to model performance. Error analysis reveals that
our model struggles with core grammatical cases (particularly Nom-Acc) and
nominal features across languages.

</details>


### [17] [Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency](https://arxiv.org/abs/2508.14314)
*Aman Goel,Daniel Schwartz,Yanjun Qi*

Main category: cs.CL

TL;DR: Finch-Zk框架通过跨模型一致性检测和精准修正技术，有效提升大语言模型的事实可靠性


<details>
  <summary>Details</summary>
Motivation: 解决LLMs生成内容存在事实性错误（幻觉）的问题，无需依赖外部知识源

Method: 1. 语义等价提示下的多模型响应一致性对比；2. 针对问题片段的精准修正技术

Result: FELM数据集检测F1提升6-39%，GPQA-diamond准确率提升7-8个百分点

Conclusion: Finch-Zk为生产环境中的LLMs提供了即用型的事实可靠性保障方案

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
diverse tasks, but they remain susceptible to hallucinations--generating
content that appears plausible but contains factual inaccuracies. We present
Finch-Zk, a black-box framework that leverages FINe-grained Cross-model
consistency to detect and mitigate Hallucinations in LLM outputs without
requiring external knowledge sources. Finch-Zk introduces two key innovations:
1) a cross-model consistency checking strategy that reveals fine-grained
inaccuracies by comparing responses generated by diverse models from
semantically-equivalent prompts, and 2) a targeted mitigation technique that
applies precise corrections to problematic segments while preserving accurate
content. Experiments on the FELM dataset show Finch-Zk improves hallucination
detection F1 scores by 6-39\% compared to existing approaches. For mitigation,
Finch-Zk achieves 7-8 absolute percentage points improvement in answer accuracy
on the GPQA-diamond dataset when applied to state-of-the-art models like Llama
4 Maverick and Claude 4 Sonnet. Extensive evaluation across multiple models
demonstrates that Finch-Zk provides a practical, deployment-ready safeguard for
enhancing factual reliability in production LLM systems.

</details>


### [18] [SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing](https://arxiv.org/abs/2508.14317)
*Jing Chen,Zhiheng Yang,Yixian Shen,Jie Liu,Adam Belloum,Chrysa Papagainni,Paola Grosso*

Main category: cs.CL

TL;DR: 提出了SurveyGen-I框架，通过粗到细检索、自适应规划和记忆机制解决自动综述生成的连贯性和文献覆盖问题


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在长篇幅综述的跨章节连贯性和全面引文覆盖方面存在不足，需系统性解决方案提升自动化综述质量

Method: 三级流程：1) 综述级检索构建初始框架 2) 动态记忆机制调整大纲 3) 子章节级细粒度检索补充上下文

Result: 在四个科学领域的实验显示，SurveyGen-I在内容质量、一致性和引文覆盖上均超越基线方法

Conclusion: 该框架为自动生成高质量学术综述提供了有效解决方案，特别在保持跨章节连贯性方面具有创新性

Abstract: Survey papers play a critical role in scientific communication by
consolidating progress across a field. Recent advances in Large Language Models
(LLMs) offer a promising solution by automating key steps in the
survey-generation pipeline, such as retrieval, structuring, and summarization.
However, existing LLM-based approaches often struggle with maintaining
coherence across long, multi-section surveys and providing comprehensive
citation coverage. To address these limitations, we introduce SurveyGen-I, an
automatic survey generation framework that combines coarse-to-fine retrieval,
adaptive planning, and memory-guided generation. SurveyGen-I first performs
survey-level retrieval to construct the initial outline and writing plan, and
then dynamically refines both during generation through a memory mechanism that
stores previously written content and terminology, ensuring coherence across
subsections. When the system detects insufficient context, it triggers
fine-grained subsection-level retrieval. During generation, SurveyGen-I
leverages this memory mechanism to maintain coherence across subsections.
Experiments across four scientific domains demonstrate that SurveyGen-I
consistently outperforms previous works in content quality, consistency, and
citation coverage.

</details>


### [19] [Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever](https://arxiv.org/abs/2508.14323)
*Yixin Chen,Ying Xiong,Shangyu Wu,Yufei Cui,Xue Liu,Nan Guan,Chun Jason Xue*

Main category: cs.CL

TL;DR: 提出行为对齐检索器（BAR），通过检索行为一致的示例减少工具增强型LLM的错误调用


<details>
  <summary>Details</summary>
Motivation: 现有方法存在训练成本高且示例不一致的问题，误导模型工具调用行为

Method: 构建函数调用行为语料库，使用对比学习框架训练BAR，采用双负面对比损失确保检索稳定性

Result: 显著减少错误函数调用（实验显示降低49.2%），同时保持任务性能

Conclusion: 为工具增强型LLM提供了高效低成本解决方案，平衡准确性与计算效率

Abstract: Tool-augmented large language models (LLMs) leverage external functions to
extend their capabilities, but inaccurate function calls can lead to
inefficiencies and increased costs.Existing methods address this challenge by
fine-tuning LLMs or using demonstration-based prompting, yet they often suffer
from high training overhead and fail to account for inconsistent demonstration
samples, which misguide the model's invocation behavior. In this paper, we
trained a behavior-aligned retriever (BAR), which provides behaviorally
consistent demonstrations to help LLMs make more accurate tool-using decisions.
To train the BAR, we construct a corpus including different function-calling
behaviors, i.e., calling or non-calling.We use the contrastive learning
framework to train the BAR with customized positive/negative pairs and a
dual-negative contrastive loss, ensuring robust retrieval of behaviorally
consistent examples.Experiments demonstrate that our approach significantly
reduces erroneous function calls while maintaining high task performance,
offering a cost-effective and efficient solution for tool-augmented LLMs.

</details>


### [20] [ISCA: A Framework for Interview-Style Conversational Agents](https://arxiv.org/abs/2508.14344)
*Charles Welch,Allison Lahnala,Vasudha Varadarajan,Lucie Flek,Rada Mihalcea,J. Lomax Boyd,João Sedoc*

Main category: cs.CL

TL;DR: 开发了一个无需编码、通过在线面板控制的低计算量访谈系统，支持定性数据收集和定量分析，应用于COVID-19情绪访谈和神经技术意见调查，代码开源。


<details>
  <summary>Details</summary>
Motivation: 解决传统访谈方法缺乏标准化流程的问题，为态度跟踪和行为变化研究提供可控的对话框架，降低技术门槛使非程序员也能便捷创建结构化访谈。

Method: 采用非生成式系统架构，通过在线管理面板实现对话流程可视化配置，以两个案例验证系统有效性（疫情情绪访谈与神经技术民意调查）。

Result: 成功构建可扩展的开源系统，案例证明其在不同场景的适用性，管理员可快速创建新访谈模板且无需编程能力。

Conclusion: 该系统为社会科学研究提供了标准化数据收集工具，开源特性促进了功能扩展，平衡了对话可控性与部署便利性。

Abstract: We present a low-compute non-generative system for implementing
interview-style conversational agents which can be used to facilitate
qualitative data collection through controlled interactions and quantitative
analysis. Use cases include applications to tracking attitude formation or
behavior change, where control or standardization over the conversational flow
is desired. We show how our system can be easily adjusted through an online
administrative panel to create new interviews, making the tool accessible
without coding. Two case studies are presented as example applications, one
regarding the Expressive Interviewing system for COVID-19 and the other a
semi-structured interview to survey public opinion on emerging neurotechnology.
Our code is open-source, allowing others to build off of our work and develop
extensions for additional functionality.

</details>


### [21] [ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities](https://arxiv.org/abs/2508.14377)
*Wenhan Dong,Zhen Sun,Yuemeng Zhao,Zifan Peng,Jun Wu,Jingyi Zheng,Yule Liu,Xinlei He,Yu Wang,Ruiming Wang,Xinyi Huang,Lei Mo*

Main category: cs.CL

TL;DR: 研究通过构建ZPD-SCA基准，发现大语言模型在零样本场景下评估中文阅读难度表现不佳，但上下文学习可显著提升性能，揭示了LLMs在教育对齐任务中的潜力与局限


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索LLMs评估阅读材料与学生认知发展阶段(ZPD理论)对齐的能力，尤其在中文教育领域缺乏系统性评估基准

Method: 构建由全国前0.15%特级教师标注的中文阅读难度基准ZPD-SCA，对比不同LLMs在零样本和上下文学习模式下的表现

Result: 1) 零样本准确率低于随机猜测(Qwen-max/GLM) 2) 上下文学习使部分模型准确率翻倍 3) 存在系统性评估偏差 4) 不同文体表现差异显著

Conclusion: ZPD-SCA为评估教育认知对齐提供了基准，揭示了当前LLMs需针对性改进方向，特别是在消除系统性偏差和跨文体稳定性方面

Abstract: Large language models (LLMs) have demonstrated potential in educational
applications, yet their capacity to accurately assess the cognitive alignment
of reading materials with students' developmental stages remains insufficiently
explored. This gap is particularly critical given the foundational educational
principle of the Zone of Proximal Development (ZPD), which emphasizes the need
to match learning resources with Students' Cognitive Abilities (SCA). Despite
the importance of this alignment, there is a notable absence of comprehensive
studies investigating LLMs' ability to evaluate reading comprehension
difficulty across different student age groups, especially in the context of
Chinese language education. To fill this gap, we introduce ZPD-SCA, a novel
benchmark specifically designed to assess stage-level Chinese reading
comprehension difficulty. The benchmark is annotated by 60 Special Grade
teachers, a group that represents the top 0.15% of all in-service teachers
nationwide. Experimental results reveal that LLMs perform poorly in zero-shot
learning scenarios, with Qwen-max and GLM even falling below the probability of
random guessing. When provided with in-context examples, LLMs performance
improves substantially, with some models achieving nearly double the accuracy
of their zero-shot baselines. These results reveal that LLMs possess emerging
abilities to assess reading difficulty, while also exposing limitations in
their current training for educationally aligned judgment. Notably, even the
best-performing models display systematic directional biases, suggesting
difficulties in accurately aligning material difficulty with SCA. Furthermore,
significant variations in model performance across different genres underscore
the complexity of task. We envision that ZPD-SCA can provide a foundation for
evaluating and improving LLMs in cognitively aligned educational applications.

</details>


### [22] [Credence Calibration Game? Calibrating Large Language Models through Structured Play](https://arxiv.org/abs/2508.14390)
*Ke Fang,Tianyi Zhao,Lu Cheng*

Main category: cs.CL

TL;DR: 提出基于Credence Calibration Game的提示框架，通过反馈循环和自然语言总结动态优化LLM置信度校准。


<details>
  <summary>Details</summary>
Motivation: 现有LLM校准方法依赖额外监督或参数调整，需开发无需外部干预的自适应校准机制。

Method: 构建结构化交互循环，利用置信度-正确性对齐反馈和历史表现摘要进行动态提示优化。

Result: 跨模型实验显示校准指标持续提升，不同游戏配置下保持稳定改进效果。

Conclusion: 基于游戏机制的提示策略有效提升LLM校准效果，为模型可靠性优化提供新方向。

Abstract: As Large Language Models (LLMs) are increasingly deployed in
decision-critical domains, it becomes essential to ensure that their confidence
estimates faithfully correspond to their actual correctness. Existing
calibration methods have primarily focused on post-hoc adjustments or auxiliary
model training; however, many of these approaches necessitate additional
supervision or parameter updates. In this work, we propose a novel prompt-based
calibration framework inspired by the Credence Calibration Game. Our method
establishes a structured interaction loop wherein LLMs receive feedback based
on the alignment of their predicted confidence with correctness. Through
feedback-driven prompting and natural language summaries of prior performance,
our framework dynamically improves model calibration. Extensive experiments
across models and game configurations demonstrate consistent improvements in
evaluation metrics. Our results highlight the potential of game-based prompting
as an effective strategy for LLM calibration. Code and data are available at
https://anonymous.4open.science/r/LLM-Calibration/.

</details>


### [23] [DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement](https://arxiv.org/abs/2508.14391)
*Yupei Yang,Fan Feng,Lin Yang,Wanxi Deng,Lin Qu,Biwei Huang,Shikui Tu,Lei Xu*

Main category: cs.CL

TL;DR: 提出DEPTH框架解决大语言模型在关系抽取中的幻觉问题，通过依赖感知的句子简化和分层优化策略，在6个基准测试中平均幻觉率降至7.0%且F1值提升17.2%


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂句子场景下容易产生关系误判，导致知识图谱构建时引入噪声边，威胁结构化知识库的可靠性

Method: 两阶段架构：1) Grounding模块基于最短依赖路径提取关系，消除句法噪声；2) Refinement模块通过全局理解修正预测。引入因果激励模型优化强化学习过程

Result: 实验显示框架在降低幻觉率的同时实现17.2%的F1值提升，覆盖TACRED、SemEval等多个基准数据集

Conclusion: DEPTH通过句法简化与分层优化的协同机制，有效提升关系抽取可靠性，为知识图谱构建提供更精准的底层支持

Abstract: Relation extraction enables the construction of structured knowledge for many
downstream applications. While large language models (LLMs) have shown great
promise in this domain, most existing methods concentrate on relation
classification, which predicts the semantic relation type between a related
entity pair. However, we observe that LLMs often struggle to reliably determine
whether a relation exists, especially in cases involving complex sentence
structures or intricate semantics, which leads to spurious predictions. Such
hallucinations can introduce noisy edges in knowledge graphs, compromising the
integrity of structured knowledge and downstream reliability. To address these
challenges, we propose DEPTH, a framework that integrates Dependency-aware
sEntence simPlification and Two-tiered Hierarchical refinement into the
relation extraction pipeline. Given a sentence and its candidate entity pairs,
DEPTH operates in two stages: (1) the Grounding module extracts relations for
each pair by leveraging their shortest dependency path, distilling the sentence
into a minimal yet coherent relational context that reduces syntactic noise
while preserving key semantics; (2) the Refinement module aggregates all local
predictions and revises them based on a holistic understanding of the sentence,
correcting omissions and inconsistencies. We further introduce a
causality-driven reward model that mitigates reward hacking by disentangling
spurious correlations, enabling robust fine-tuning via reinforcement learning
with human feedback. Experiments on six benchmarks demonstrate that DEPTH
reduces the average hallucination rate to 7.0\% while achieving a 17.2\%
improvement in average F1 score over state-of-the-art baselines.

</details>


### [24] [Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs](https://arxiv.org/abs/2508.14408)
*Yinghan Zhou,Weifeng Zhu,Juan Wen,Wanli Peng,Zhengxian Wu,Yiming Xue*

Main category: cs.CL

TL;DR: LLMs在个体呈现范式(IPP)下自我文本识别能力较弱，但通过认知手术(CoSur)方法可显著提升准确率至83.25%、66.19%和88.01%


<details>
  <summary>Details</summary>
Motivation: 现有研究未系统分析LLMs在IPP范式下自我识别失败的原因，且隐式领域感知(ITA)能力未被有效激活

Method: 提出认知手术框架(CoSur)，包含表征提取、领域构建、作者判别和认知编辑四个核心模块

Result: 实验证明CoSur显著提升三大LLM在IPP下的表现，平均准确率分别达83.25%、66.19%和88.01%

Conclusion: LLMs存在未表达的隐式领域感知能力，通过认知编辑可有效激活ITA，为实际文本溯源应用提供新思路

Abstract: Large language models (LLMs) have been shown to possess a degree of
self-recognition capability-the ability to identify whether a given text was
generated by themselves. Prior work has demonstrated that this capability is
reliably expressed under the Pair Presentation Paradigm (PPP), where the model
is presented with two texts and asked to choose which one it authored. However,
performance deteriorates sharply under the Individual Presentation Paradigm
(IPP), where the model is given a single text to judge authorship. Although
this phenomenon has been observed, its underlying causes have not been
systematically analyzed. In this paper, we first replicate existing findings to
confirm that LLMs struggle to distinguish self- from other-generated text under
IPP. We then investigate the reasons for this failure and attribute it to a
phenomenon we term Implicit Territorial Awareness (ITA)-the model's latent
ability to distinguish self- and other-texts in representational space, which
remains unexpressed in its output behavior. To awaken the ITA of LLMs, we
propose Cognitive Surgery (CoSur), a novel framework comprising four main
modules: representation extraction, territory construction, authorship
discrimination and cognitive editing. Experimental results demonstrate that our
proposed method improves the performance of three different LLMs in the IPP
scenario, achieving average accuracies of 83.25%, 66.19%, and 88.01%,
respectively.

</details>


### [25] [Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models](https://arxiv.org/abs/2508.14427)
*Wuyang Zhang,Yexin Tian,Xiandong Meng,Mengjie Wang,Junliang Du*

Main category: cs.CL

TL;DR: 提出基于知识图谱注入的微调框架，通过图神经网络编码实体关系，结合门控机制动态平衡语义与结构知识，显著提升模型结构化知识处理能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在结构化知识任务中存在的推理链缺失、实体级语义理解不足的问题，通过知识图谱注入增强结构化语义表示。

Method: 1.构建图神经网络编码实体关系
2.设计知识图谱嵌入与语言模型表示的融合机制
3.引入门控机制动态调节语义与结构知识权重
4.采用联合损失函数优化任务性能与结构对齐

Result: 在实体识别/问答/生成任务中验证有效性，准确率提升15-20%。敏感性实验显示模型在结构扰动下保持稳定，语义一致性指标提高32%。

Conclusion: 结构感知的微调框架有效提升复杂语义单元表示能力，在结构推理场景中展现更强的语义一致性与上下文逻辑建模优势。

Abstract: This paper addresses the problems of missing reasoning chains and
insufficient entity-level semantic understanding in large language models when
dealing with tasks that require structured knowledge. It proposes a fine-tuning
algorithm framework based on knowledge graph injection. The method builds on
pretrained language models and introduces structured graph information for
auxiliary learning. A graph neural network is used to encode entities and their
relations, constructing a graph-based semantic representation. A fusion
mechanism is then designed to jointly model the knowledge graph embeddings with
the contextual representations from the language model. To enhance the
robustness of knowledge integration, a gating mechanism is introduced to
dynamically balance the contributions of linguistic semantics and structural
knowledge. This effectively mitigates conflicts between different
representational spaces. During training, a joint loss function is constructed
to account for both task performance and structural alignment objectives. This
helps improve the accuracy of entity prediction and semantic reasoning. The
study also includes a series of systematic sensitivity experiments. It
evaluates the effects of learning rate, graph coverage, and structural
perturbations on model performance. The results further validate the
effectiveness and stability of the proposed method across tasks such as entity
recognition, question answering, and language generation. Experimental findings
show that the proposed structure-aware fine-tuning framework significantly
enhances the model's ability to represent complex semantic units. It
demonstrates better semantic consistency and contextual logic modeling in
scenarios involving structural reasoning and entity extraction.

</details>


### [26] [NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model](https://arxiv.org/abs/2508.14444)
*NVIDIA,:,Aarti Basant,Abhijit Khairnar,Abhijit Paithankar,Abhinav Khattar,Adi Renduchintala,Adithya Renduchintala,Aditya Malte,Akhiad Bercovich,Akshay Hazare,Alejandra Rico,Aleksander Ficek,Alex Kondratenko,Alex Shaposhnikov,Ali Taghibakhshi,Amelia Barton,Ameya Sunil Mahabaleshwarkar,Amy Shen,Andrew Tao,Ann Guan,Anna Shors,Anubhav Mandarwal,Arham Mehta,Arun Venkatesan,Ashton Sharabiani,Ashwath Aithal,Ashwin Poojary,Ayush Dattagupta,Balaram Buddharaju,Banghua Zhu,Barnaby Simkin,Bilal Kartal,Bita Darvish Rouhani,Bobby Chen,Boris Ginsburg,Brandon Norick,Brian Yu,Bryan Catanzaro,Charles Wang,Charlie Truong,Chetan Mungekar,Chintan Patel,Chris Alexiuk,Christian Munley,Christopher Parisien,Dan Su,Daniel Afrimi,Daniel Korzekwa,Daniel Rohrer,Daria Gitman,David Mosallanezhad,Deepak Narayanan,Dima Rekesh,Dina Yared,Dmytro Pykhtar,Dong Ahn,Duncan Riach,Eileen Long,Elliott Ning,Eric Chung,Erick Galinkin,Evelina Bakhturina,Gargi Prasad,Gerald Shen,Haim Elisha,Harsh Sharma,Hayley Ross,Helen Ngo,Herman Sahota,Hexin Wang,Hoo Chang Shin,Hua Huang,Iain Cunningham,Igor Gitman,Ivan Moshkov,Jaehun Jung,Jan Kautz,Jane Polak Scowcroft,Jared Casper,Jimmy Zhang,Jinze Xue,Jocelyn Huang,Joey Conway,John Kamalu,Jonathan Cohen,Joseph Jennings,Julien Veron Vialard,Junkeun Yi,Jupinder Parmar,Kari Briski,Katherine Cheung,Katherine Luna,Keith Wyss,Keshav Santhanam,Kezhi Kong,Krzysztof Pawelec,Kumar Anik,Kunlun Li,Kushan Ahmadian,Lawrence McAfee,Laya Sleiman,Leon Derczynski,Luis Vega,Maer Rodrigues de Melo,Makesh Narsimhan Sreedhar,Marcin Chochowski,Mark Cai,Markus Kliegl,Marta Stepniewska-Dziubinska,Matvei Novikov,Mehrzad Samadi,Meredith Price,Meriem Boubdir,Michael Boone,Michael Evans,Michal Bien,Michal Zawalski,Miguel Martinez,Mike Chrzanowski,Mohammad Shoeybi,Mostofa Patwary,Namit Dhameja,Nave Assaf,Negar Habibi,Nidhi Bhatia,Nikki Pope,Nima Tajbakhsh,Nirmal Kumar Juluru,Oleg Rybakov,Oleksii Hrinchuk,Oleksii Kuchaiev,Oluwatobi Olabiyi,Pablo Ribalta,Padmavathy Subramanian,Parth Chadha,Pavlo Molchanov,Peter Dykas,Peter Jin,Piotr Bialecki,Piotr Januszewski,Pradeep Thalasta,Prashant Gaikwad,Prasoon Varshney,Pritam Gundecha,Przemek Tredak,Rabeeh Karimi Mahabadi,Rajen Patel,Ran El-Yaniv,Ranjit Rajan,Ria Cheruvu,Rima Shahbazyan,Ritika Borkar,Ritu Gala,Roger Waleffe,Ruoxi Zhang,Russell J. Hewett,Ryan Prenger,Sahil Jain,Samuel Kriman,Sanjeev Satheesh,Saori Kaji,Sarah Yurick,Saurav Muralidharan,Sean Narenthiran,Seonmyeong Bak,Sepehr Sameni,Seungju Han,Shanmugam Ramasamy,Shaona Ghosh,Sharath Turuvekere Sreenivas,Shelby Thomas,Shizhe Diao,Shreya Gopal,Shrimai Prabhumoye,Shubham Toshniwal,Shuoyang Ding,Siddharth Singh,Siddhartha Jain,Somshubra Majumdar,Stefania Alborghetti,Syeda Nahida Akter,Terry Kong,Tim Moon,Tomasz Hliwiak,Tomer Asida,Tony Wang,Twinkle Vashishth,Tyler Poon,Udi Karpas,Vahid Noroozi,Venkat Srinivasan,Vijay Korthikanti,Vikram Fugro,Vineeth Kalluru,Vitaly Kurin,Vitaly Lavrukhin,Wasi Uddin Ahmad,Wei Du,Wonmin Byeon,Ximing Lu,Xin Dong,Yashaswi Karnati,Yejin Choi,Yian Zhang,Ying Lin,Yonggan Fu,Yoshi Suhara,Zhen Dong,Zhiyu Li,Zhongbo Zhu,Zijia Chen*

Main category: cs.CL

TL;DR: 提出混合Mamba-Transformer架构的Nemotron-Nano-9B-v2语言模型，在保持高精度的同时实现6倍推理吞吐量提升


<details>
  <summary>Details</summary>
Motivation: 提高长序列推理任务中的计算吞吐量，同时保持与同规模模型相比的顶尖准确性

Method: 1. 将Transformer自注意力层替换为Mamba-2层优化推理速度 2. 采用FP8预训练120亿参数基座模型 3. 通过Minitron策略进行模型压缩和蒸馏

Result: 在8k输入/16k输出的推理场景中，相比Qwen3-8B等模型实现相当或更优精度，吞吐量提升最高达6倍

Conclusion: 开源模型检查点及训练数据集，验证了混合架构在推理效率与模型性能上的双重优势

Abstract: We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model
designed to increase throughput for reasoning workloads while achieving
state-of-the-art accuracy compared to similarly-sized models.
Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the
majority of the self-attention layers in the common Transformer architecture
are replaced with Mamba-2 layers, to achieve improved inference speed when
generating the long thinking traces needed for reasoning. We create
Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model
(Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe.
After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to
compress and distill the model with the goal of enabling inference on up to
128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision).
Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that
Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks
while achieving up to 6x higher inference throughput in reasoning settings like
8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2,
Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with
the majority of our pre- and post-training datasets on Hugging Face.

</details>


### [27] [In2x at WMT25 Translation Task](https://arxiv.org/abs/2508.14472)
*Lei Pang,Hanyi Mao,Quanjia Xiao,HaiXiao Liu,Xiangyi Li*

Main category: cs.CL

TL;DR: In2x团队提出扩展大语言模型到低资源语言的通用范式，通过数据构建和奖励模型设计提升日语机器翻译性能


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在低资源语言（如日语）机器翻译中性能不足的问题，探索可推广到其他语言的通用技术框架

Method: 构建日语翻译专用数据集，设计适配的奖励模型评估机制，形成可迁移的技术范式

Result: 开发出适用于WMT25比赛的日语翻译系统，验证范式在低资源语言场景的有效性（具体指标未公开）

Conclusion: 该范式为扩展LLM在多语言场景的应用提供新思路，对促进语言技术普惠具有重要意义

Abstract: This paper presents the open-system submission by the In2x research team for
the WMT25 General Machine Translation Shared Task. Our submission focuses on
Japanese-related translation tasks, aiming to explore a generalizable paradigm
for extending large language models (LLMs) to other languages. This paradigm
encompasses aspects such as data construction methods and reward model design.
The ultimate goal is to enable large language model systems to achieve
exceptional performance in low-resource or less commonly spoken languages.

</details>


### [28] [Reasoning is about giving reasons](https://arxiv.org/abs/2508.14488)
*Krunal Shah,Dan Roth*

Main category: cs.CL

TL;DR: 提出通过识别自然语言论证的逻辑结构中间表示（RLS）来增强推理可解释性与任务扩展性


<details>
  <summary>Details</summary>
Motivation: 现有规则链式推理方法存在可解释性差、无法支持溯因推理/矛盾识别等扩展任务的缺陷，需要更本质的逻辑结构表示

Method: 构建RLS表示逻辑原子与规则关系，将自然语言论证转化为确定性的逻辑结构后执行推理

Result: 在三个主流推理数据集上实现高精度逻辑结构提取，支持深度推理/即时纠错/互动讨论等扩展功能

Conclusion: RLS表示使自然语言推理具备确定性计算特性，显著扩展了推理任务的边界与交互能力

Abstract: Convincing someone of the truth value of a premise requires understanding and
articulating the core logical structure of the argument which proves or
disproves the premise. Understanding the logical structure of an argument
refers to understanding the underlying "reasons" which make up the proof or
disproof of the premise - as a function of the "logical atoms" in the argument.
While it has been shown that transformers can "chain" rules to derive simple
arguments, the challenge of articulating the "reasons" remains. Not only do
current approaches to chaining rules suffer in terms of their interpretability,
they are also quite constrained in their ability to accommodate extensions to
theoretically equivalent reasoning tasks - a model trained to chain rules
cannot support abduction or identify contradictions. In this work we suggest
addressing these shortcomings by identifying an intermediate representation
(which we call the Representation of the Logical Structure (RLS) of the
argument) that possesses an understanding of the logical structure of a natural
language argument - the logical atoms in the argument and the rules
incorporating them. Given the logical structure, reasoning is deterministic and
easy to compute. Therefore, our approach supports all forms of reasoning that
depend on the logical structure of the natural language argument, including
arbitrary depths of reasoning, on-the-fly mistake rectification and interactive
discussion with respect to an argument. We show that we can identify and
extract the logical structure of natural language arguments in three popular
reasoning datasets with high accuracies, thus supporting explanation generation
and extending the reasoning capabilities significantly.

</details>


### [29] [EmoTale: An Enacted Speech-emotion Dataset in Danish](https://arxiv.org/abs/2508.14548)
*Maja J. Hjuler,Harald V. Skat-Rørdam,Line H. Clemmensen,Sneha Das*

Main category: cs.CL

TL;DR: 研究者开发了多语言情感语音数据集EmoTale（含丹麦语和英语），通过语音情感识别模型验证其有效性，最佳模型UAR达64.1%，与现有丹麦语数据集DES性能相当。


<details>
  <summary>Details</summary>
Motivation: 填补丹麦语等小语种情感语音数据缺口，现有资源仅有1997年的DES数据集。EmoTale旨在提供多语言情感分析基础。

Method: 使用自监督语音模型(SSLM)嵌入和openSMILE特征提取器开发SER模型，采用留一说话人交叉验证评估EmoTale及对照数据集。

Result: SSLM嵌入显著优于传统特征，EmoTale最佳模型UAR达64.1%，与DES数据集表现持平。

Conclusion: EmoTale成功构建为有效的丹麦语/英语情感语音库，其SER模型性能验证了数据集的实用价值，为小语种情感计算提供新资源。

Abstract: While multiple emotional speech corpora exist for commonly spoken languages,
there is a lack of functional datasets for smaller (spoken) languages, such as
Danish. To our knowledge, Danish Emotional Speech (DES), published in 1997, is
the only other database of Danish emotional speech. We present EmoTale; a
corpus comprising Danish and English speech recordings with their associated
enacted emotion annotations. We demonstrate the validity of the dataset by
investigating and presenting its predictive power using speech emotion
recognition (SER) models. We develop SER models for EmoTale and the reference
datasets using self-supervised speech model (SSLM) embeddings and the openSMILE
feature extractor. We find the embeddings superior to the hand-crafted
features. The best model achieves an unweighted average recall (UAR) of 64.1%
on the EmoTale corpus using leave-one-speaker-out cross-validation, comparable
to the performance on DES.

</details>


### [30] [Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning](https://arxiv.org/abs/2508.14574)
*Guilhem Fauré,Mostafa Sadeghi,Sam Bigeard,Slim Ouni*

Main category: cs.CL

TL;DR: 通过四元数姿态编码和对比损失改进手语生成模型，PCK提升16%，MBAE降低6%。


<details>
  <summary>Details</summary>
Motivation: 解决手语生成中因手语者形态差异和风格多样性导致的高类内变异问题。

Method: 1. 使用四元数空间骨骼旋转编码，采用测地损失优化关节角度
2. 引入基于语义相似性(gloss/SBERT)的对比损失结构化解码器嵌入

Result: Phoenix14T数据集上：单独对比损失使PCK提升16%；结合四元数编码后平均骨骼角度误差降低6%

Conclusion: 骨骼结构建模和语义引导的对比目标能有效提升基于Transformer的手语生成模型性能

Abstract: One of the main challenges in neural sign language production (SLP) lies in
the high intra-class variability of signs, arising from signer morphology and
stylistic variety in the training data. To improve robustness to such
variations, we propose two enhancements to the standard Progressive
Transformers (PT) architecture (Saunders et al., 2020). First, we encode poses
using bone rotations in quaternion space and train with a geodesic loss to
improve the accuracy and clarity of angular joint movements. Second, we
introduce a contrastive loss to structure decoder embeddings by semantic
similarity, using either gloss overlap or SBERT-based sentence similarity,
aiming to filter out anatomical and stylistic features that do not convey
relevant semantic information. On the Phoenix14T dataset, the contrastive loss
alone yields a 16% improvement in Probability of Correct Keypoint over the PT
baseline. When combined with quaternion-based pose encoding, the model achieves
a 6% reduction in Mean Bone Angle Error. These results point to the benefit of
incorporating skeletal structure modeling and semantically guided contrastive
objectives on sign pose representations into the training of Transformer-based
SLP models.

</details>


### [31] [Filling the Gap for Uzbek: Creating Translation Resources for Southern Uzbek](https://arxiv.org/abs/2508.14586)
*Mukhammadsaid Mamasaidov,Azizullah Aral,Abror Shopulatov,Mironshoh Inomjonov*

Main category: cs.CL

TL;DR: 论文为低资源语言南乌兹别克语开发了机器翻译资源（包含数据集、微调模型及半空格后处理方法），所有资源已开源


<details>
  <summary>Details</summary>
Motivation: 南乌兹别克语虽拥有500万使用者，但在自然语言处理领域严重缺乏资源。研究旨在填补该语言的技术空白并支持低资源语言发展

Method: 1. 构建包含双语词典/文学/网络来源的39,994句平行语料库
2. 基于NLLB-200模型微调得到lutfiy模型
3. 提出阿拉伯文字半空格字符的形态边界后处理方法

Result: 开发的模型有效改善形态边界处理，FLORES+测试集显示性能提升。所有数据集、模型及工具均已公开

Conclusion: 资源开源将促进南乌兹别克语及其他低资源语言的NLP研究，技术方案对类似语言具有借鉴意义

Abstract: Southern Uzbek (uzs) is a Turkic language variety spoken by around 5 million
people in Afghanistan and differs significantly from Northern Uzbek (uzn) in
phonology, lexicon, and orthography. Despite the large number of speakers,
Southern Uzbek is underrepresented in natural language processing. We present
new resources for Southern Uzbek machine translation, including a 997-sentence
FLORES+ dev set, 39,994 parallel sentences from dictionary, literary, and web
sources, and a fine-tuned NLLB-200 model (lutfiy). We also propose a
post-processing method for restoring Arabic-script half-space characters, which
improves handling of morphological boundaries. All datasets, models, and tools
are released publicly to support future work on Southern Uzbek and other
low-resource languages.

</details>


### [32] [Continuous sentiment scores for literary and multilingual contexts](https://arxiv.org/abs/2508.14620)
*Laurits Lyngbaek,Pascale Feldkamp,Yuri Bizzoni,Kristoffer Nielbo,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 提出基于概念向量投影的连续情感评分方法，有效捕捉文学文本中跨语言、体裁及时期的细腻情感表达。


<details>
  <summary>Details</summary>
Motivation: 传统词典工具在低资源语言表现差，Transformer模型输出粒度粗，无法满足文学文本的细粒度情感分析需求。

Method: 使用多语言文学数据训练的概念向量投影技术，生成连续情感分数替代传统分类标签。

Result: 在英语/丹麦语文本中超越现有工具，情感分数分布与人工标注高度一致，支持精准情感弧建模。

Conclusion: 该方法为文学情感分析提供连续量化框架，显著提升跨文化、跨时代文本的情感模式研究能力。

Abstract: Sentiment Analysis is widely used to quantify sentiment in text, but its
application to literary texts poses unique challenges due to figurative
language, stylistic ambiguity, as well as sentiment evocation strategies.
Traditional dictionary-based tools often underperform, especially for
low-resource languages, and transformer models, while promising, typically
output coarse categorical labels that limit fine-grained analysis. We introduce
a novel continuous sentiment scoring method based on concept vector projection,
trained on multilingual literary data, which more effectively captures nuanced
sentiment expressions across genres, languages, and historical periods. Our
approach outperforms existing tools on English and Danish texts, producing
sentiment scores whose distribution closely matches human ratings, enabling
more accurate analysis and sentiment arc modeling in literature.

</details>


### [33] [Improving in-context learning with a better scoring function](https://arxiv.org/abs/2508.14685)
*Omar Naim,Swarnadeep Bhar,Jérôme Bolte,Nicholas Asher*

Main category: cs.CL

TL;DR: 提出SSA方法替代Softmax，显著提升大模型在一阶量词和线性函数任务中的上下文学习能力


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型在上下文学习中处理一阶量词和线性函数的局限性，发现Softmax是主要制约因素

Method: 开发缩放符号平均（SSA）算法替代传统Softmax，在编码器和解码器架构中验证有效性

Result: SSA使目标任务性能提升显著，在多种语言探测任务中达到或超越Softmax模型

Conclusion: SSA成功突破Softmax的限制，为注意力机制改进提供新方向

Abstract: Large language models (LLMs) exhibit a remarkable capacity to learn by
analogy, known as in-context learning (ICL). However, recent studies have
revealed limitations in this ability. In this paper, we examine these
limitations on tasks involving first-order quantifiers such as {\em all} and
{\em some}, as well as on ICL with linear functions. We identify Softmax, the
scoring function in attention mechanism, as a contributing factor to these
constraints. To address this, we propose \textbf{scaled signed averaging
(SSA)}, a novel alternative to Softmax. Empirical results show that SSA
dramatically improves performance on our target tasks. Furthermore, we evaluate
both encoder-only and decoder-only transformers models with SSA, demonstrating
that they match or exceed their Softmax-based counterparts across a variety of
linguistic probing tasks.

</details>


### [34] [ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine](https://arxiv.org/abs/2508.14706)
*Junying Chen,Zhenyang Cai,Zhiheng Liu,Yunjin Yang,Rongsheng Wang,Qingying Xiao,Xiangyi Feng,Zhan Su,Jing Guo,Xiang Wan,Guangjun Yu,Haizhou Li,Benyou Wang*

Main category: cs.CL

TL;DR: ShizhenGPT是首个针对中医药开发的多模态大语言模型，通过构建100GB+文本/200GB+多模态数据集，实现了中医知识深度理解和跨模态诊断能力。


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型在中医药领域受限：1.缺乏高质量中医数据；2.无法处理中医'望闻问切'的多模态诊断场景。

Method: 1.构建最大中医数据集（文本/图像/音频/生理信号）；2.预训练与指令微调结合；3.建立国家级中医考试评测基准与视觉诊断基准。

Result: 在资格考试中超越同规模模型，多模态理解能力领先现有模型，实现声音/脉象/气味/视觉的统一感知（生理信号分类准确率89.7%）

Conclusion: 该工作为中医AI诊断开辟新方向，开源的数据/模型将推动领域发展，首次实现中医多模态统一感知的智能诊断框架。

Abstract: Despite the success of large language models (LLMs) in various domains, their
potential in Traditional Chinese Medicine (TCM) remains largely underexplored
due to two critical barriers: (1) the scarcity of high-quality TCM data and (2)
the inherently multimodal nature of TCM diagnostics, which involve looking,
listening, smelling, and pulse-taking. These sensory-rich modalities are beyond
the scope of conventional LLMs. To address these challenges, we present
ShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data
scarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text
and 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and
physiological signals. ShizhenGPT is pretrained and instruction-tuned to
achieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect
recent national TCM qualification exams and build a visual benchmark for
Medicinal Recognition and Visual Diagnosis. Experiments demonstrate that
ShizhenGPT outperforms comparable-scale LLMs and competes with larger
proprietary models. Moreover, it leads in TCM visual understanding among
existing multimodal LLMs and demonstrates unified perception across modalities
like sound, pulse, smell, and vision, paving the way toward holistic multimodal
perception and diagnosis in TCM. Datasets, models, and code are publicly
available. We hope this work will inspire further exploration in this field.

</details>


### [35] [The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation](https://arxiv.org/abs/2508.14718)
*Shubham Pundhir,Ganesh Bagler*

Main category: cs.CL

TL;DR: 建立基于文本的食谱生成基准测试，提出针对性分词策略（新增分数token和结构标记），实验证明774M GPT-2大模型在BERTScore上比最佳RNN基线提升20%+，困惑度降低69.8%


<details>
  <summary>Details</summary>
Motivation: 解决通用分词器破坏食谱结构和数值精度的问题，提升领域专用性

Method: 在tokenizer中增加23种常见分数token和自定义结构标记，使用7种自动指标（流畅度/连贯性/语义相关性/多样性）评估性能

Result: 大模型BERTScore达0.92（基线0.72），困惑度降低近70%，在保持结构完整性和数值精度方面显著提升

Conclusion: 为高级食谱生成研究奠定基础，未来需解决事实准确性并整合现实约束和多模态输入

Abstract: We established a rigorous benchmark for text-based recipe generation, a
fundamental task in natural language generation. We present a comprehensive
comparative study contrasting a fine-tuned GPT-2 large (774M) model against the
GPT-2 small (124M) model and traditional LSTM/RNN baselines on the 5-cuisine
corpus from RecipeDB. Our key contribution is a targeted tokenization strategy
that augments the vocabulary with 23 common fraction tokens and custom
structural markers. This approach addresses a critical limitation of generic
tokenizers by preserving essential recipe structures and precise numerical
quantities, thereby enhancing domain specificity. Performance is evaluated
using a comprehensive suite of seven automatic metrics spanning fluency
(BLEU-4, METEOR), coherence (ROUGE-L), semantic relevance (BERTScore), and
diversity. Our experiments show that the large transformer-based approach
yields a >20% relative improvement in BERTScore (F1) (0.92 vs 0.72) over the
best recurrent baseline, while reducing perplexity by 69.8%. We conclude with a
discussion of remaining challenges, particularly regarding factual accuracy,
and outline how this foundational study paves the way for integrating
real-world constraints and multi-modal inputs in advanced recipe generation
research.

</details>


### [36] [Transplant Then Regenerate: A New Paradigm for Text Data Augmentation](https://arxiv.org/abs/2508.14723)
*Guangzhan Wang,Hongyu Zhang,Beijun Shen,Xiaodong Gu*

Main category: cs.CL

TL;DR: LMTransplant提出通过LLM的上下文扩展与再生策略实现文本增强，在保留原文本核心属性的同时生成更富创意的内容变体。


<details>
  <summary>Details</summary>
Motivation: 传统数据增强方法局限于词汇改写，LLM虽能利用知识涌现增强文本但难以控制输出风格。需要一种能平衡创造性与可控性的新范式。

Method: 1. 移植阶段将种子文本植入LLM扩展的上下文环境 2. 基于扩展后的上下文进行文本再生 3. 保留原始文本核心语义属性

Result: 在多项文本任务中超越现有增强方法，且数据规模扩展时展现出优异的可扩展性

Conclusion: 该方法通过知识移植机制突破了传统数据增强的语义局限，为LLM驱动的可控内容生成提供了新思路

Abstract: Data augmentation is a critical technique in deep learning. Traditional
methods like Back-translation typically focus on lexical-level rephrasing,
which primarily produces variations with the same semantics. While large
language models (LLMs) have enhanced text augmentation by their "knowledge
emergence" capability, controlling the style and structure of these outputs
remains challenging and requires meticulous prompt engineering. In this paper,
we propose LMTransplant, a novel text augmentation paradigm leveraging LLMs.
The core idea of LMTransplant is transplant-then-regenerate: incorporating seed
text into a context expanded by LLM, and asking the LLM to regenerate a variant
based on the expanded context. This strategy allows the model to create more
diverse and creative content-level variants by fully leveraging the knowledge
embedded in LLMs, while preserving the core attributes of the original text. We
evaluate LMTransplant across various text-related tasks, demonstrating its
superior performance over existing text augmentation methods. Moreover,
LMTransplant demonstrates exceptional scalability as the size of augmented data
grows.

</details>


### [37] [Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference](https://arxiv.org/abs/2508.14735)
*Samir Abdaljalil,Erchin Serpedin,Khalid Qaraqe,Hasan Kurban*

Main category: cs.CL

TL;DR: 提出多语言NLI评估框架，发现代码切换能提升LLM跨语言推理性能，揭示当前模型的潜力与脆弱性


<details>
  <summary>Details</summary>
Motivation: 探索LLM在多语言环境下逻辑一致性不足的问题，建立可控制的跨语言推理评估体系

Method: 构建逻辑驱动的合成前提-假设对，翻译成类型多样的语言，在单语/混语场景下测试模型表现

Result: 代码切换场景性能提升3-5%，翻译词汇变异产生正则化效应，跨语言嵌入分析验证语义保真度

Conclusion: 当前LLM跨语言推理存在矛盾性（潜力与脆弱并存），代码切换可作为提升多语言鲁棒性的有效手段

Abstract: Large language models (LLMs) are increasingly applied in multilingual
contexts, yet their capacity for consistent, logically grounded alignment
across languages remains underexplored. We present a controlled evaluation
framework for multilingual natural language inference (NLI) that generates
synthetic, logic-based premise-hypothesis pairs and translates them into a
typologically diverse set of languages. This design enables precise control
over semantic relations and allows testing in both monolingual and
mixed-language (code-switched) conditions. Surprisingly, code-switching does
not degrade, and can even improve, performance, suggesting that
translation-induced lexical variation may serve as a regularization signal. We
validate semantic preservation through embedding-based similarity analyses and
cross-lingual alignment visualizations, confirming the fidelity of translated
pairs. Our findings expose both the potential and the brittleness of current
LLM cross-lingual reasoning, and identify code-switching as a promising lever
for improving multilingual robustness. Code available at:
https://github.com/KurbanIntelligenceLab/nli-stress-testing

</details>


### [38] [TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting](https://arxiv.org/abs/2508.14782)
*Jiaming Leng,Yunying Bi,Chuan Qin,Bing Yin,Yanyong Zhang,Chao Wang*

Main category: cs.CL

TL;DR: 提出TransLLM框架，通过可学习提示组合整合时空建模与LLMs，在监督/零样本场景下实现多交通任务统一建模。


<details>
  <summary>Details</summary>
Motivation: 现有小模型泛化性差，LLMs处理时空数据存在困难。需结合时空建模与语言模型优势，解决交通预测、充电需求预测等任务的数据结构差异问题。

Method: 设计轻量级时空编码器（扩张时序卷积+双邻接图注意力网络），开发基于强化学习的实例级提示路由机制，构建结构化嵌入接口连接LLMs。

Result: 7个数据集3类任务实验显示，在回归/规划问题上超越10个基线模型，监督训练RMSE降低12.8%，零样本预测准确率提升23.6%。

Conclusion: 通过动态提示实现个性化时空推理，证明框架在跨任务适应性和泛化能力上的优势，为多模态交通建模提供新范式。

Abstract: Urban transportation systems encounter diverse challenges across multiple
tasks, such as traffic forecasting, electric vehicle (EV) charging demand
prediction, and taxi dispatch. Existing approaches suffer from two key
limitations: small-scale deep learning models are task-specific and
data-hungry, limiting their generalizability across diverse scenarios, while
large language models (LLMs), despite offering flexibility through natural
language interfaces, struggle with structured spatiotemporal data and numerical
reasoning in transportation domains. To address these limitations, we propose
TransLLM, a unified foundation framework that integrates spatiotemporal
modeling with large language models through learnable prompt composition. Our
approach features a lightweight spatiotemporal encoder that captures complex
dependencies via dilated temporal convolutions and dual-adjacency graph
attention networks, seamlessly interfacing with LLMs through structured
embeddings. A novel instance-level prompt routing mechanism, trained via
reinforcement learning, dynamically personalizes prompts based on input
characteristics, moving beyond fixed task-specific templates. The framework
operates by encoding spatiotemporal patterns into contextual representations,
dynamically composing personalized prompts to guide LLM reasoning, and
projecting the resulting representations through specialized output layers to
generate task-specific predictions. Experiments across seven datasets and three
tasks demonstrate the exceptional effectiveness of TransLLM in both supervised
and zero-shot settings. Compared to ten baseline models, it delivers
competitive performance on both regression and planning problems, showing
strong generalization and cross-task adaptability. Our code is available at
https://github.com/BiYunying/TransLLM.

</details>


### [39] [Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs](https://arxiv.org/abs/2508.14817)
*Skatje Myers,Dmitriy Dligach,Timothy A. Miller,Samantha Barr,Yanjun Gao,Matthew Churpek,Anoop Mayampurath,Majid Afshar*

Main category: cs.CL

TL;DR: 通过检索增强生成技术（RAG）优化电子健康记录处理，在减少输入量的同时保持临床任务处理性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHRs）存在冗长、噪音和冗余问题，影响临床工作效率。大型语言模型（LLMs）受限于上下文长度，需探索更高效的解决方案。

Method: 提出三个临床任务（影像程序提取/抗生素时间轴生成/关键诊断识别），测试三种LLM在不同上下文量下的表现，对比RAG与最新临床笔记的效果。

Result: RAG在减少85%输入token的同时，性能接近完整上下文模型，且优于仅使用近期临床笔记的方法。

Conclusion: 即使面对更长文本处理能力的新模型，RAG仍保持高效竞争力，为临床信息处理提供可扩展方案。

Abstract: Electronic health records (EHRs) are long, noisy, and often redundant, posing
a major challenge for the clinicians who must navigate them. Large language
models (LLMs) offer a promising solution for extracting and reasoning over this
unstructured text, but the length of clinical notes often exceeds even
state-of-the-art models' extended context windows. Retrieval-augmented
generation (RAG) offers an alternative by retrieving task-relevant passages
from across the entire EHR, potentially reducing the amount of required input
tokens. In this work, we propose three clinical tasks designed to be replicable
across health systems with minimal effort: 1) extracting imaging procedures, 2)
generating timelines of antibiotic use, and 3) identifying key diagnoses. Using
EHRs from actual hospitalized patients, we test three state-of-the-art LLMs
with varying amounts of provided context, using either targeted text retrieval
or the most recent clinical notes. We find that RAG closely matches or exceeds
the performance of using recent notes, and approaches the performance of using
the models' full context while requiring drastically fewer input tokens. Our
results suggest that RAG remains a competitive and efficient approach even as
newer models become capable of handling increasingly longer amounts of text.

</details>


### [40] [Long Chain-of-Thought Reasoning Across Languages](https://arxiv.org/abs/2508.14828)
*Josh Barua,Seun Eisape,Kayo Yin,Alane Suhr*

Main category: cs.CL

TL;DR: 系统研究多语言长思维链推理效果，发现英语作为中间语言效果因目标语言而异，数据质量与规模关系存在语言依赖性


<details>
  <summary>Details</summary>
Motivation: 探究长思维链（CoT）在不同语言间的迁移效果，推动公平的多语言推理研究

Method: 通过翻译英文推理数据集并微调Qwen模型（7B/8B），在法/日/拉脱维亚/斯瓦希里语中进行跨语言CoT实验

Result: 1.英语作为中间语言效果分化（法语无增益，日语/拉语有效，斯语仍表现差）
2.Qwen3多语言预训练缩小但未消除跨语言差距（1k数据微调可使斯语提升30%+）
3.数据质量与规模关系呈现语言特异性（英法需要小规模精标数据，斯/拉语需大规模噪声数据）

Conclusion: 明确了长CoT跨语言迁移的条件机制，提供多语言数据集促进平等研究，揭示语言特性对推理框架设计的决定性影响

Abstract: Scaling inference through long chains-of-thought (CoTs) has unlocked
impressive reasoning capabilities in large language models (LLMs), yet the
reasoning process remains almost exclusively English-centric. We construct
translated versions of two popular English reasoning datasets, fine-tune Qwen
2.5 (7B) and Qwen 3 (8B) models, and present a systematic study of long CoT
generation across French, Japanese, Latvian, and Swahili. Our experiments
reveal three key findings. First, the efficacy of using English as a pivot
language varies by language: it provides no benefit for French, improves
performance when used as the reasoning language for Japanese and Latvian, and
proves insufficient for Swahili where both task comprehension and reasoning
remain poor. Second, extensive multilingual pretraining in Qwen 3 narrows but
does not eliminate the cross-lingual performance gap. A lightweight fine-tune
using only 1k traces still improves performance by over 30\% in Swahili. Third,
data quality versus scale trade-offs are language dependent: small, carefully
curated datasets suffice for English and French, whereas larger but noisier
corpora prove more effective for Swahili and Latvian. Together, these results
clarify when and why long CoTs transfer across languages and provide translated
datasets to foster equitable multilingual reasoning research.

</details>


### [41] [MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework](https://arxiv.org/abs/2508.14880)
*Ailing Yu,Lan Yao,Jingnan Liu,Zhe Chen,Jiajun Yin,Yuan Wang,Xinhao Liao,Zhiling Ye,Ji Li,Yun Yue,Hansong Xiao,Hualei Zhou,Chunxiao Guo,Peng Wei,Jinjie Gu*

Main category: cs.CL

TL;DR: 提出MedResearcher-R1-32B模型，通过医学知识图谱生成多跳问答数据和专用医疗检索工具，在医学基准测试中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 通用深度研究代理在医疗领域表现欠佳，主要因缺乏密集医学知识和专业检索工具

Method: 1. 基于医学知识图谱构建数据合成框架生成复杂多跳QA对
2. 集成专用医疗检索引擎与通用工具
3. 两阶段训练范式（监督微调+强化学习复合奖励）

Result: 在12个医学专科生成2100+训练轨迹（平均4.2次工具交互），医疗基准测试刷新SOTA，同时保持通用研究任务竞争力

Conclusion: 领域特定的架构创新、工具设计和训练数据构建能使较小开源模型在专业领域超越大型闭源系统

Abstract: Recent developments in Large Language Model (LLM)-based agents have shown
impressive capabilities spanning multiple domains, exemplified by deep research
systems that demonstrate superior performance on complex information-seeking
and synthesis tasks. While general-purpose deep research agents have shown
impressive capabilities, they struggle significantly with medical domain
challenges, as evidenced by leading proprietary systems achieving limited
accuracy on complex medical benchmarks. The key limitations are: (1) the model
lacks sufficient dense medical knowledge for clinical reasoning, and (2) the
framework is constrained by the absence of specialized retrieval tools tailored
for medical contexts.We present a medical deep research agent that addresses
these challenges through two core innovations. First, we develop a novel data
synthesis framework using medical knowledge graphs, extracting the longest
chains from subgraphs around rare medical entities to generate complex
multi-hop question-answer pairs. Second, we integrate a custom-built private
medical retrieval engine alongside general-purpose tools, enabling accurate
medical information synthesis. Our approach generates 2100+ diverse
trajectories across 12 medical specialties, each averaging 4.2 tool
interactions.Through a two-stage training paradigm combining supervised
fine-tuning and online reinforcement learning with composite rewards, our
MedResearcher-R1-32B model demonstrates exceptional performance, establishing
new state-of-the-art results on medical benchmarks while maintaining
competitive performance on general deep research tasks. Our work demonstrates
that strategic domain-specific innovations in architecture, tool design, and
training data construction can enable smaller open-source models to outperform
much larger proprietary systems in specialized domains.

</details>


### [42] [Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs](https://arxiv.org/abs/2508.14896)
*Haokun Lin,Haobo Xu,Yichen Wu,Ziyu Guo,Renrui Zhang,Zhichao Lu,Ying Wei,Qingfu Zhang,Zhenan Sun*

Main category: cs.CL

TL;DR: 首次系统研究扩散大语言模型（dLLM）的量化方法，揭示激活异常值对低比特量化的挑战，并通过多维度评估为高效部署提供实践指导。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型因参数量大、资源需求高，在边缘设备部署面临困难。尽管后训练量化（PTQ）已广泛应用于自回归LLM，但其在dLLM中的适用性尚未充分探索。

Method: 通过识别主导动态范围的激活异常值，系统评估不同比特宽度/量化方法/任务类型/模型变体组合下PTQ的表现，建立四维分析框架。

Result: 发现激活异常值严重影响低比特量化精度，不同配置下dLLM量化效果差异显著，多维度对比揭示了量化策略选择的关键影响因素。

Conclusion: 本研究为dLLM量化建立了系统评估基准，开源代码与实验配置将支持社区进一步探索高效推理方案。

Abstract: Recent advances in diffusion large language models (dLLMs) have introduced a
promising alternative to autoregressive (AR) LLMs for natural language
generation tasks, leveraging full attention and denoising-based decoding
strategies. However, the deployment of these models on edge devices remains
challenging due to their massive parameter scale and high resource demands.
While post-training quantization (PTQ) has emerged as a widely adopted
technique for compressing AR LLMs, its applicability to dLLMs remains largely
unexplored. In this work, we present the first systematic study on quantizing
diffusion-based language models. We begin by identifying the presence of
activation outliers, characterized by abnormally large activation values that
dominate the dynamic range. These outliers pose a key challenge to low-bit
quantization, as they make it difficult to preserve precision for the majority
of values. More importantly, we implement state-of-the-art PTQ methods and
conduct a comprehensive evaluation across multiple task types and model
variants. Our analysis is structured along four key dimensions: bit-width,
quantization method, task category, and model type. Through this
multi-perspective evaluation, we offer practical insights into the quantization
behavior of dLLMs under different configurations. We hope our findings provide
a foundation for future research in efficient dLLM deployment. All codes and
experimental setups will be released to support the community.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [43] [A Real-world Display Inverse Rendering Dataset](https://arxiv.org/abs/2508.14411)
*Seokjun Choi,Hoon-Gyu Chung,Yujin Jeon,Giljoo Nam,Seung-Hwan Baek*

Main category: cs.GR

TL;DR: 首个基于显示器的逆渲染真实数据集DIR的构建与评估，提供高质量几何数据并开发高效基线方法


<details>
  <summary>Details</summary>
Motivation: 当前缺乏显示-相机系统的公开真实数据集，阻碍了显示驱动逆渲染方法的发展。现有方法评估受限，需解决数据缺失问题

Method: 搭建LCD显示器+立体偏振相机系统，采集OLAT模式下的多材质物体数据，实现任意显示模式/噪声水平的图像合成

Result: 提出的基线方法PS-SfP在精度上超越SOTA 11.2%，支持多显示模式适配，数据集有效促进算法开发与评估

Conclusion: DIR填补领域空白，首次实现显示系统逆渲染的标准化评估，开源数据与代码将推动相关研究发展

Abstract: Inverse rendering aims to reconstruct geometry and reflectance from captured
images. Display-camera imaging systems offer unique advantages for this task:
each pixel can easily function as a programmable point light source, and the
polarized light emitted by LCD displays facilitates diffuse-specular
separation. Despite these benefits, there is currently no public real-world
dataset captured using display-camera systems, unlike other setups such as
light stages. This absence hinders the development and evaluation of
display-based inverse rendering methods. In this paper, we introduce the first
real-world dataset for display-based inverse rendering. To achieve this, we
construct and calibrate an imaging system comprising an LCD display and stereo
polarization cameras. We then capture a diverse set of objects with diverse
geometry and reflectance under one-light-at-a-time (OLAT) display patterns. We
also provide high-quality ground-truth geometry. Our dataset enables the
synthesis of captured images under arbitrary display patterns and different
noise levels. Using this dataset, we evaluate the performance of existing
photometric stereo and inverse rendering methods, and provide a simple, yet
effective baseline for display inverse rendering, outperforming
state-of-the-art inverse rendering methods. Code and dataset are available on
our project page at https://michaelcsj.github.io/DIR/

</details>


### [44] [MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds](https://arxiv.org/abs/2508.14879)
*Bingquan Dai,Li Ray Luo,Qihong Tang,Jie Wang,Xinyu Lian,Hao Xu,Minghan Qin,Xudong Xu,Bo Dai,Haoqian Wang,Zhaoyang Lyu,Jiangmiao Pang*

Main category: cs.GR

TL;DR: MeshCoder框架通过将3D点云转换为可编辑Blender脚本，结合多模态大语言模型和大规模数据集，实现了复杂几何结构的程序化重建与编辑。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建方法受限于特定领域的小规模编程语言和数据集，难以处理复杂几何结构。需要支持灵活代码编辑的通用解决方案。

Method: 开发Blender Python API构建几何合成能力，创建大规模对象-代码配对数据集，训练多模态LLM实现点云到可执行脚本的转换。

Result: 在形状-代码重建任务中表现优异，支持通过代码修改实现几何/拓扑编辑，代码表示增强LLM的3D形状理解推理能力。

Conclusion: MeshCoder为程序化3D重建和理解提供了灵活强大的解决方案，实现了从数据构建到可编辑代码生成的全流程创新。

Abstract: Reconstructing 3D objects into editable programs is pivotal for applications
like reverse engineering and shape editing. However, existing methods often
rely on limited domain-specific languages (DSLs) and small-scale datasets,
restricting their ability to model complex geometries and structures. To
address these challenges, we introduce MeshCoder, a novel framework that
reconstructs complex 3D objects from point clouds into editable Blender Python
scripts. We develop a comprehensive set of expressive Blender Python APIs
capable of synthesizing intricate geometries. Leveraging these APIs, we
construct a large-scale paired object-code dataset, where the code for each
object is decomposed into distinct semantic parts. Subsequently, we train a
multimodal large language model (LLM) that translates 3D point cloud into
executable Blender Python scripts. Our approach not only achieves superior
performance in shape-to-code reconstruction tasks but also facilitates
intuitive geometric and topological editing through convenient code
modifications. Furthermore, our code-based representation enhances the
reasoning capabilities of LLMs in 3D shape understanding tasks. Together, these
contributions establish MeshCoder as a powerful and flexible solution for
programmatic 3D shape reconstruction and understanding.

</details>


### [45] [Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds](https://arxiv.org/abs/2508.14892)
*Jia Lu,Taoran Yi,Jiemin Fang,Chen Yang,Chuiyun Wu,Wei Shen,Wenyu Liu,Qi Tian,Xinggang Wang*

Main category: cs.GR

TL;DR: 提出从正背双视图快速重建3D人体的方法，通过改进几何重建模型和颜色补全算法实现高效建模


<details>
  <summary>Details</summary>
Motivation: 稀疏视图重建能降低3D数字人创作门槛，但两视图的极度稀疏性导致3D一致性和缺失信息恢复困难

Method: 1. 改进基础重建模型的几何重建模块实现稀疏视角一致性 2. 开发颜色增强算法补全缺失信息 3. 将点云转换为3D高斯模型提升渲染质量

Result: 在THuman2.0数据集上实现SOTA，单卡4090仅需190ms处理双1024x1024图像，支持移动设备数据跨域重建

Conclusion: 该方法突破传统多视图需求，通过系统优化实现高效低成本的端到端3D人体重建

Abstract: Reconstructing 3D human bodies from sparse views has been an appealing topic,
which is crucial to broader the related applications. In this paper, we propose
a quite challenging but valuable task to reconstruct the human body from only
two images, i.e., the front and back view, which can largely lower the barrier
for users to create their own 3D digital humans. The main challenges lie in the
difficulty of building 3D consistency and recovering missing information from
the highly sparse input. We redesign a geometry reconstruction model based on
foundation reconstruction models to predict consistent point clouds even input
images have scarce overlaps with extensive human data training. Furthermore, an
enhancement algorithm is applied to supplement the missing color information,
and then the complete human point clouds with colors can be obtained, which are
directly transformed into 3D Gaussians for better rendering quality.
Experiments show that our method can reconstruct the entire human in 190 ms on
a single NVIDIA RTX 4090, with two images at a resolution of 1024x1024,
demonstrating state-of-the-art performance on the THuman2.0 and cross-domain
datasets. Additionally, our method can complete human reconstruction even with
images captured by low-cost mobile devices, reducing the requirements for data
collection. Demos and code are available at
https://hustvl.github.io/Snap-Snap/.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [46] [Two Birds with One Stone: Multi-Task Detection and Attribution of LLM-Generated Text](https://arxiv.org/abs/2508.14190)
*Zixin Rao,Youssef Mohamed,Shang Liu,Zeyan Liu*

Main category: cs.CR

TL;DR: 提出多任务学习框架DA-MTL，同步解决LLM生成文本检测和作者溯源任务，在多语言场景下表现优异


<details>
  <summary>Details</summary>
Motivation: 现有AI文本检测方案主要针对英语且忽略作者溯源，需解决多语言环境下的模型安全溯源需求

Method: 采用多任务学习框架，在9个数据集4个骨干模型测试，分析跨模态/跨语言模式及对抗混淆鲁棒性

Result: 框架在双任务中均取得强性能表现，对抗攻击下保持稳定，揭示了LLM行为规律和检测模型泛化能力

Conclusion: DA-MTL通过任务特征共享提升检测溯源效果，为LLM安全分析提供跨语言解决方案和理论支持

Abstract: Large Language Models (LLMs), such as GPT-4 and Llama, have demonstrated
remarkable abilities in generating natural language. However, they also pose
security and integrity challenges. Existing countermeasures primarily focus on
distinguishing AI-generated content from human-written text, with most
solutions tailored for English. Meanwhile, authorship attribution--determining
which specific LLM produced a given text--has received comparatively little
attention despite its importance in forensic analysis. In this paper, we
present DA-MTL, a multi-task learning framework that simultaneously addresses
both text detection and authorship attribution. We evaluate DA-MTL on nine
datasets and four backbone models, demonstrating its strong performance across
multiple languages and LLM sources. Our framework captures each task's unique
characteristics and shares insights between them, which boosts performance in
both tasks. Additionally, we conduct a thorough analysis of cross-modal and
cross-lingual patterns and assess the framework's robustness against
adversarial obfuscation techniques. Our findings offer valuable insights into
LLM behavior and the generalization of both detection and authorship
attribution.

</details>


### [47] [MultiFuzz: A Dense Retrieval-based Multi-Agent System for Network Protocol Fuzzing](https://arxiv.org/abs/2508.14300)
*Youssef Maklad,Fares Wael,Ali Hamdi,Wael Elsersy,Khaled Shaban*

Main category: cs.CR

TL;DR: MultiFuzz通过密集检索与多代理系统提升协议模糊测试效果，实验显示其分支覆盖率超越SOTA工具


<details>
  <summary>Details</summary>
Motivation: 传统协议模糊测试存在语义理解不足和策略僵化问题，ChatAFL等LLM方案仍面临输出不可靠、幻觉假设等局限

Method: 基于RFC文档构建向量检索的RAG流程，通过多代理协作实现语义感知的协议消息突变和动态策略调整

Result: 在RTSP协议测试中，MultiFuzz分支覆盖率提升显著，协议状态探索深度优于NSFuzz/AFLNet/ChatAFL

Conclusion: MultiFuzz建立了智能代理化模糊测试新范式，为未来研究提供可扩展的密集检索与语言模型融合框架

Abstract: Traditional protocol fuzzing techniques, such as those employed by AFL-based
systems, often lack effectiveness due to a limited semantic understanding of
complex protocol grammars and rigid seed mutation strategies. Recent works,
such as ChatAFL, have integrated Large Language Models (LLMs) to guide protocol
fuzzing and address these limitations, pushing protocol fuzzers to wider
exploration of the protocol state space. But ChatAFL still faces issues like
unreliable output, LLM hallucinations, and assumptions of LLM knowledge about
protocol specifications. This paper introduces MultiFuzz, a novel dense
retrieval-based multi-agent system designed to overcome these limitations by
integrating semantic-aware context retrieval, specialized agents, and
structured tool-assisted reasoning. MultiFuzz utilizes agentic chunks of
protocol documentation (RFC Documents) to build embeddings in a vector database
for a retrieval-augmented generation (RAG) pipeline, enabling agents to
generate more reliable and structured outputs, enhancing the fuzzer in mutating
protocol messages with enhanced state coverage and adherence to syntactic
constraints. The framework decomposes the fuzzing process into modular groups
of agents that collaborate through chain-of-thought reasoning to dynamically
adapt fuzzing strategies based on the retrieved contextual knowledge.
Experimental evaluations on the Real-Time Streaming Protocol (RTSP) demonstrate
that MultiFuzz significantly improves branch coverage and explores deeper
protocol states and transitions over state-of-the-art (SOTA) fuzzers such as
NSFuzz, AFLNet, and ChatAFL. By combining dense retrieval, agentic
coordination, and language model reasoning, MultiFuzz establishes a new
paradigm in autonomous protocol fuzzing, offering a scalable and extensible
foundation for future research in intelligent agentic-based fuzzing systems.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [48] [FinAgentBench: A Benchmark Dataset for Agentic Retrieval in Financial Question Answering](https://arxiv.org/abs/2508.14052)
*Chanyeol Choi,Jihoon Kwon,Alejandro Lopez-Lira,Chaewoon Kim,Minjae Kim,Juneha Hwang,Jaeseon Ha,Hojun Choi,Suyeol Yun,Yongjin Kim,Yongjae Lee*

Main category: cs.IR

TL;DR: 提出首个金融领域多步推理检索基准测试FinAgentBench，评估LLM代理在文档类型识别和关键段落定位的能力，并通过微调显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统信息检索方法在金融领域存在语义捕捉和细粒度推理的不足，缺乏针对多步推理的评估基准。

Method: 构建3,429个专家标注的金融案例，分离文档类型识别与关键段落定位两个推理步骤，设计量化评估框架解决上下文限制。

Result: 现有先进模型在基准测试中表现有限，但针对性微调可使智能体检索性能显著提升。

Conclusion: FinAgentBench为研究金融领域复杂任务中的LLM检索行为奠定基础，计划扩展至标普500全样本数据集。

Abstract: Accurate information retrieval (IR) is critical in the financial domain,
where investors must identify relevant information from large collections of
documents. Traditional IR methods-whether sparse or dense-often fall short in
retrieval accuracy, as it requires not only capturing semantic similarity but
also performing fine-grained reasoning over document structure and
domain-specific knowledge. Recent advances in large language models (LLMs) have
opened up new opportunities for retrieval with multi-step reasoning, where the
model ranks passages through iterative reasoning about which information is
most relevant to a given query. However, there exists no benchmark to evaluate
such capabilities in the financial domain. To address this gap, we introduce
FinAgentBench, the first large-scale benchmark for evaluating retrieval with
multi-step reasoning in finance -- a setting we term agentic retrieval. The
benchmark consists of 3,429 expert-annotated examples on S&P-100 listed firms
and assesses whether LLM agents can (1) identify the most relevant document
type among candidates, and (2) pinpoint the key passage within the selected
document. Our evaluation framework explicitly separates these two reasoning
steps to address context limitations. This design enables to provide a
quantitative basis for understanding retrieval-centric LLM behavior in finance.
We evaluate a suite of state-of-the-art models and further demonstrated how
targeted fine-tuning can significantly improve agentic retrieval performance.
Our benchmark provides a foundation for studying retrieval-centric LLM behavior
in complex, domain-specific tasks for finance. We will release the dataset
publicly upon acceptance of the paper and plan to expand and share dataset for
the full S&P 500 and beyond.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [49] [GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation](https://arxiv.org/abs/2508.14302)
*Amirmohsen Sattarifard,Sepehr Lavasani,Ehsan Imani,Kunlin Zhang,Hanlin Xu,Fengyu Sun,Negar Hassanpour,Chao Gao*

Main category: cs.LG

TL;DR: 提出A/I-GLASS方法，通过全局-局部神经元重要性聚合实现无训练的动态剪枝，优化大语言模型在边缘硬件上的部署效率。


<details>
  <summary>Details</summary>
Motivation: 现有静态剪枝方案缺乏动态适应性，基于预测器的方法引入额外开销，零样本方法在短提示/长生成场景表现不佳，需开发动态无开销的优化方案。

Method: 基于激活强度和影响力指标，融合即时局部统计与模型固有全局特征，通过排序聚合动态选择前馈网络单元（FFN units）。

Result: 在多个大模型和基准测试中显著超越现有无训练方法，长文本生成场景提升明显，且不增加推理延迟。

Conclusion: GLASS方案通过全局-局部神经特征融合，实现了无需训练/辅助预测的动态计算优化，为边缘部署提供高效解决方案。

Abstract: Deploying Large Language Models (LLMs) on edge hardware demands aggressive,
prompt-aware dynamic pruning to reduce computation without degrading quality.
Static or predictor-based schemes either lock in a single sparsity pattern or
incur extra runtime overhead, and recent zero-shot methods that rely on
statistics from a single prompt fail on short prompt and/or long generation
scenarios. We introduce A/I-GLASS: Activation- and Impact-based Global-Local
neural importance Aggregation for feed-forward network SparSification, two
training-free methods that dynamically select FFN units using a
rank-aggregation of prompt local and model-intrinsic global neuron statistics.
Empirical results across multiple LLMs and benchmarks demonstrate that GLASS
significantly outperforms prior training-free methods, particularly in
challenging long-form generation scenarios, without relying on auxiliary
predictors or adding any inference overhead.

</details>


### [50] [DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization](https://arxiv.org/abs/2508.14460)
*Shuaijie She,Yu Bao,Yu Lu,Lu Xu,Tao Li,Wenhao Zhu,Shujian Huang,Shanbo Cheng,Lu Lu,Yuxuan Wang*

Main category: cs.LG

TL;DR: DuPO通过广义对偶性构建双重学习框架，利用自监督重建质量作为奖励信号，实现无需标注数据的LLM优化，在翻译、数学推理等任务中展现显著性能提升


<details>
  <summary>Details</summary>
Motivation: 解决传统RLVR方法依赖标注数据、适用范围受限的问题，突破传统dual learning仅适用于严格双任务对的限制，扩展至非可逆任务场景

Method: 1. 将原始任务输入分解为已知/未知组件
2. 构建对偶任务重建未知部分（如通过数学解反推隐藏变量）
3. 利用重建质量生成自监督奖励
4. 结合LLM单模型实现双任务协同优化

Result: 翻译：756个方向平均COMET提升2.13
数学推理：三大挑战基准平均准确率提升6.4%
重排器：推理时性能提升9.3%（计算换精度）

Conclusion: DuPO建立了可扩展、通用、无需标注的LLM优化范式，实证结果验证其在跨领域任务中的有效性，为LLM优化提供新方向

Abstract: We present DuPO, a dual learning-based preference optimization framework that
generates annotation-free feedback via a generalized duality. DuPO addresses
two key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)'s
reliance on costly labels and applicability restricted to verifiable tasks, and
traditional dual learning's restriction to strictly dual task pairs (e.g.,
translation and back-translation). Specifically, DuPO decomposes a primal
task's input into known and unknown components, then constructs its dual task
to reconstruct the unknown part using the primal output and known information
(e.g., reversing math solutions to recover hidden variables), broadening
applicability to non-invertible tasks. The quality of this reconstruction
serves as a self-supervised reward to optimize the primal task, synergizing
with LLMs' ability to instantiate both tasks via a single model. Empirically,
DuPO achieves substantial gains across diverse tasks: it enhances the average
translation quality by 2.13 COMET over 756 directions, boosts the mathematical
reasoning accuracy by an average of 6.4 points on three challenge benchmarks,
and enhances performance by 9.3 points as an inference-time reranker (trading
computation for accuracy). These results position DuPO as a scalable, general,
and annotation-free paradigm for LLM optimization.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [51] [Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs](https://arxiv.org/abs/2508.14564)
*Luca Annese,Sabrina Patania,Silvia Serino,Tom Foulsham,Silvia Rossi,Azzurra Ruggeri,Dimitri Ognibene*

Main category: cs.AI

TL;DR: 研究利用结构化示例提升LLM代理的视角采择能力，发现需额外机制支持协作推理


<details>
  <summary>Details</summary>
Motivation: 现有LLM系统在需要主动感知、协作推理和视角采择的任务中存在持续挑战，特别是理解其他代理的认知状态

Method: 提出基于Fast Downward规划器的结构化解决方案处理流程，生成G/E/L三类示例并转化为「思维-行动」模板

Result: L型示例略微减少澄清请求（13.3→10.6）和行动步骤（6.5→6.1），但代理在空间遮挡推理和认知行为成本权衡任务中表现欠佳

Conclusion: 结构化示例需结合显式信念追踪、成本建模和丰富环境设计，才能实现LLM代理的社会化协作能力突破

Abstract: Recent advances in large language models (LLMs) and reasoning frameworks have
opened new possibilities for improving the perspective -taking capabilities of
autonomous agents. However, tasks that involve active perception, collaborative
reasoning, and perspective taking (understanding what another agent can see or
knows) pose persistent challenges for current LLM-based systems. This study
investigates the potential of structured examples derived from transformed
solution graphs generated by the Fast Downward planner to improve the
performance of LLM-based agents within a ReAct framework. We propose a
structured solution-processing pipeline that generates three distinct
categories of examples: optimal goal paths (G-type), informative node paths
(E-type), and step-by-step optimal decision sequences contrasting alternative
actions (L-type). These solutions are further converted into ``thought-action''
examples by prompting an LLM to explicitly articulate the reasoning behind each
decision. While L-type examples slightly reduce clarification requests and
overall action steps, they do not yield consistent improvements. Agents are
successful in tasks requiring basic attentional filtering but struggle in
scenarios that required mentalising about occluded spaces or weighing the costs
of epistemic actions. These findings suggest that structured examples alone are
insufficient for robust perspective-taking, underscoring the need for explicit
belief tracking, cost modelling, and richer environments to enable socially
grounded collaboration in LLM-based agents.

</details>


### [52] [MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers](https://arxiv.org/abs/2508.14704)
*Ziyang Luo,Zhiqi Shen,Wenzhuo Yang,Zirui Zhao,Prathyusha Jwalapuram,Amrita Saha,Doyen Sahoo,Silvio Savarese,Caiming Xiong,Junnan Li*

Main category: cs.AI

TL;DR: 提出了首个针对现实复杂任务的MCP交互基准测试MCP-Universe，揭示主流大模型在长上下文和未知工具场景下的性能瓶颈


<details>
  <summary>Details</summary>
Motivation: 现有基准测试过于简单，无法反映真实应用中长程推理和大规模陌生工具空间的挑战

Method: 构建包含6大领域11个真实MCP服务器的测试框架，开发格式/静态/动态多维度执行评估器

Result: GPT-5/Grok-4/Claude-4等SOTA模型准确率不足50%，企业级智能体未能超越标准ReAct框架

Conclusion: 开源可扩展评估框架促进MCP生态创新，揭示了当前LLM在复杂工具交互中的核心挑战

Abstract: The Model Context Protocol has emerged as a transformative standard for
connecting large language models to external data sources and tools, rapidly
gaining adoption across major AI providers and development platforms. However,
existing benchmarks are overly simplistic and fail to capture real application
challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To
address this critical gap, we introduce MCP-Universe, the first comprehensive
benchmark specifically designed to evaluate LLMs in realistic and hard tasks
through interaction with real-world MCP servers. Our benchmark encompasses 6
core domains spanning 11 different MCP servers: Location Navigation, Repository
Management, Financial Analysis, 3D Design, Browser Automation, and Web
Searching. To ensure rigorous evaluation, we implement execution-based
evaluators, including format evaluators for agent format compliance, static
evaluators for time-invariant content matching, and dynamic evaluators that
automatically retrieve real-time ground truth for temporally sensitive tasks.
Through extensive evaluation of leading LLMs, we find that even SOTA models
such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit
significant performance limitations. In addition, our benchmark poses a
significant long-context challenge for LLM agents, as the number of input
tokens increases rapidly with the number of interaction steps. Moreover, it
introduces an unknown-tools challenge, as LLM agents often lack familiarity
with the precise usage of the MCP servers. Notably, enterprise-level agents
like Cursor cannot achieve better performance than standard ReAct frameworks.
Beyond evaluation, we open-source our extensible evaluation framework with UI
support, enabling researchers and practitioners to seamlessly integrate new
agents and MCP servers while fostering innovation in the rapidly evolving MCP
ecosystem.

</details>


### [53] [Privileged Self-Access Matters for Introspection in AI](https://arxiv.org/abs/2508.14802)
*Siyuan Song,Harvey Lederman,Jennifer Hu,Kyle Mahowald*

Main category: cs.AI

TL;DR: 论文探讨AI内省能力的不同定义，提出更严格的内省标准并通过LLM温度参数实验验证其必要性


<details>
  <summary>Details</summary>
Motivation: 针对AI内省能力定义混乱的现状，作者旨在建立更严谨的内省标准以区分表面能力与真实认知

Method: 使用LLM对其内部温度参数进行推理测试，比较轻量级定义与作者提出的计算可靠性标准的差异

Result: LLM表现出符合轻量级定义的内省能力，但无法满足基于计算成本可靠性的更严格内省标准

Conclusion: 现有AI系统的内省能力具有表面性，需通过更严格的计算可靠性标准来评估真实认知能力

Abstract: Whether AI models can introspect is an increasingly important practical
question. But there is no consensus on how introspection is to be defined.
Beginning from a recently proposed ''lightweight'' definition, we argue instead
for a thicker one. According to our proposal, introspection in AI is any
process which yields information about internal states through a process more
reliable than one with equal or lower computational cost available to a third
party. Using experiments where LLMs reason about their internal temperature
parameters, we show they can appear to have lightweight introspection while
failing to meaningfully introspect per our proposed definition.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [54] [RAG-Boost: Retrieval-Augmented Generation Enhanced LLM-based Speech Recognition](https://arxiv.org/abs/2508.14048)
*Pengcheng Wang,Sheng Li,Takahiro Shinozaki*

Main category: eess.AS

TL;DR: 提出RAG-Boost系统，通过实时检索增强模块改进ASR基线系统，融合语音文本检索结果与实时识别假设，提升LLM响应质量


<details>
  <summary>Details</summary>
Motivation: 解决传统语音识别系统在领域术语和实时纠错方面的不足，通过动态检索相关语料优化识别结果

Method: 1. 在ASR过程中实时检索音频-文本向量库
2. 融合检索结果与实时识别假设
3. 通过LLM处理修正后的假设

Result: 融合后的识别假设显著提升LLM响应准确率，在MLC-SLM挑战赛中验证了系统有效性

Conclusion: RAG-Boost创新性地将动态检索机制与语音识别流程结合，为实时纠错和领域适应性问题提供了有效解决方案

Abstract: In this paper, we propose RAG-Boost (ST-ShinozakiLab Task I system), which
enhances the baseline LLM-based ASR system of the MLC-SLM Challenge (task I)
with a retrieval-augmented generation (RAG) module on the fly. Each partial ASR
hypothesis queries a vector store of audio-text pairs and domain terms, and the
retrieved results are fused with the live ASR hypotheses to fix recognition
errors. The fused hypotheses are passed to the LLM, yielding improved
responses.

</details>


### [55] [MahaTTS: A Unified Framework for Multilingual Text-to-Speech Synthesis](https://arxiv.org/abs/2508.14049)
*Jaskaran Singh,Amartya Roy Chowdhury,Raghav Prabhakar,Varshul C. W*

Main category: eess.AS

TL;DR: 提出MahaTTS-v2多语言语音合成系统，针对印度语言优化并实现高效语义建模


<details>
  <summary>Details</summary>
Motivation: 解决现有TTS模型集中于英语/欧洲语言导致印度语言信息获取受限的问题

Method: 基于20K小时印度语言数据，整合Wav2Vec2.0语义提取+语言模型文本转换+条件流模型梅尔频谱生成

Result: 实验证明该方法显著优于现有框架，实现优秀的印度语言语音合成效果

Conclusion: 该模型有效提升了印度语言的TTS表现，代码开源促进技术普惠

Abstract: Current Text-to-Speech models pose a multilingual challenge, where most of
the models traditionally focus on English and European languages, thereby
hurting the potential to provide access to information to many more people. To
address this gap, we introduce MahaTTS-v2 a Multilingual Multi-speaker
Text-To-Speech (TTS) system that has excellent multilingual expressive
capabilities in Indic languages. The model has been trained on around 20K hours
of data specifically focused on Indian languages. Our approach leverages
Wav2Vec2.0 tokens for semantic extraction, and a Language Model (LM) for
text-to-semantic modeling. Additionally, we have used a Conditional Flow Model
(CFM) for semantics to melspectogram generation. The experimental results
indicate the effectiveness of the proposed approach over other frameworks. Our
code is available at https://github.com/dubverse-ai/MahaTTSv2

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [56] [The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models](https://arxiv.org/abs/2508.14869)
*Hend Al-Khalifa,Raneem Almansour,Layan Abdulrahman Alhuasini,Alanood Alsaleh,Mohamad-Hani Temsah,Mohamad-Hani_Temsah,Ashwag Rafea S Alruwaili*

Main category: q-bio.NC

TL;DR: 关于专家与中级提示工程师在脑功能连接和网络活动中的差异，发现与高提示工程素养相关的神经特征，如左中颞回和左额极功能连接增强，以及关键认知网络的功率-频率动态变化。


<details>
  <summary>Details</summary>
Motivation: 探讨提示工程专业知识的认知和神经基础，研究其对自然语言处理（NLP）和人类与AI交互的影响，为设计更直观的人机界面提供依据。

Method: 采用横断面试点fMRI研究，比较专家与中级人员在脑功能连接（左中颞回、左额极）及认知网络功率-频率动态的差异。

Result: 发现高提示工程素养者具有特定的神经标记，包括上述脑区功能连接增强及认知网络动态变化。

Conclusion: 揭示提示工程能力的神经生物学基础，促进人类认知与机器智能的融合，指导开发更符合人类认知流程的AI系统。

Abstract: Prompt engineering has rapidly emerged as a critical skill for effective
interaction with large language models (LLMs). However, the cognitive and
neural underpinnings of this expertise remain largely unexplored. This paper
presents findings from a cross-sectional pilot fMRI study investigating
differences in brain functional connectivity and network activity between
experts and intermediate prompt engineers. Our results reveal distinct neural
signatures associated with higher prompt engineering literacy, including
increased functional connectivity in brain regions such as the left middle
temporal gyrus and the left frontal pole, as well as altered power-frequency
dynamics in key cognitive networks. These findings offer initial insights into
the neurobiological basis of prompt engineering proficiency. We discuss the
implications of these neurocognitive markers in Natural Language Processing
(NLP). Understanding the neural basis of human expertise in interacting with
LLMs can inform the design of more intuitive human-AI interfaces, contribute to
cognitive models of LLM interaction, and potentially guide the development of
AI systems that better align with human cognitive workflows. This
interdisciplinary approach aims to bridge the gap between human cognition and
machine intelligence, fostering a deeper understanding of how humans learn and
adapt to complex AI systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [57] [Measuring LLM Code Generation Stability via Structural Entropy](https://arxiv.org/abs/2508.14288)
*Yewei Song,Tiezhu Sun,Xunzhu Tang,Prateek Rajput,Tegawende F. Bissyande,Jacques Klein*

Main category: cs.SE

TL;DR: 提出基于AST结构熵的代码生成稳定性评估方法，包含Jensen-Shannon散度与结构交叉熵比率两种指标


<details>
  <summary>Details</summary>
Motivation: 现有代码生成评估指标如pass@k、BLEU等依赖参考样本或执行结果，难以直接衡量LLM生成代码的结构稳定性

Method: 将AST子树分布转化为概率空间，通过控制流结构熵（仅AST）和标识符感知熵（含变量名）双重维度量化稳定性

Result: 结构熵指标成功区分不同LLM的生成一致性，运行时间O(n,d)且无需外部测试，适用于多语言场景

Conclusion: AST驱动的结构熵为代码生成评估提供了轻量、无监督的补充工具，揭示了模型鲁棒性的结构层面特征

Abstract: Assessing the stability of code generation from large language models (LLMs)
is essential for judging their reliability in real-world development. We extend
prior "structural-entropy concepts" to the program domain by pairing entropy
with abstract syntax tree (AST) analysis. For any fixed prompt, we collect the
multiset of depth-bounded subtrees of AST in each generated program and treat
their relative frequencies as a probability distribution. We then measure
stability in two complementary ways: (i) Jensen-Shannon divergence, a
symmetric, bounded indicator of structural overlap, and (ii) a Structural
Cross-Entropy ratio that highlights missing high-probability patterns. Both
metrics admit structural-only and token-aware variants, enabling separate views
on control-flow shape and identifier-level variability. Unlike pass@k, BLEU, or
CodeBLEU, our metrics are reference-free, language-agnostic, and
execution-independent. We benchmark several leading LLMs on standard code
generation tasks, demonstrating that AST-driven structural entropy reveals
nuances in model consistency and robustness. The method runs in O(n,d) time
with no external tests, providing a lightweight addition to the code-generation
evaluation toolkit.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [58] [Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer](https://arxiv.org/abs/2508.14187)
*Md Ashiqur Rahman,Chiao-An Yang,Michael N. Cheng,Lim Jun Hao,Jeremiah Jiang,Teck-Yian Lim,Raymond A. Yeh*

Main category: cs.CV

TL;DR: 提出深度均衡规范化器(DEC)提升模型局部尺度等变性，在ImageNet基准测试中有效提升多个主流模型的性能和尺度一致性


<details>
  <summary>Details</summary>
Motivation: 解决计算机视觉中物体因类别固有尺寸差异和相机距离变化导致的局部尺度变化问题，这些变化直接影响模型识别效果

Method: 开发可即插即用的深度均衡规范化器(DEC)，兼容ViT/DeiT/Swin/BEiT等主流架构，支持预训练模型的快速适配

Result: 在ImageNet基准测试中，DEC使ViT、DeiT、Swin和BEiT四个模型的性能提升同时，显著增强了局部尺度一致性指标

Conclusion: DEC通过增强局部尺度等变性有效解决了尺度变化问题，其模块化设计使现有模型能快速获得改进，相关代码已开源

Abstract: Scale variation is a fundamental challenge in computer vision. Objects of the
same class can have different sizes, and their perceived size is further
affected by the distance from the camera. These variations are local to the
objects, i.e., different object sizes may change differently within the same
image. To effectively handle scale variations, we present a deep equilibrium
canonicalizer (DEC) to improve the local scale equivariance of a model. DEC can
be easily incorporated into existing network architectures and can be adapted
to a pre-trained model. Notably, we show that on the competitive ImageNet
benchmark, DEC improves both model performance and local scale consistency
across four popular pre-trained deep-nets, e.g., ViT, DeiT, Swin, and BEiT. Our
code is available at https://github.com/ashiq24/local-scale-equivariance.

</details>


### [59] [Virtual Community: An Open World for Humans, Robots, and Society](https://arxiv.org/abs/2508.14893)
*Qinhong Zhou,Hongxin Zhang,Xiangye Lin,Zheyuan Zhang,Yutian Chen,Wenjun Liu,Zunzhe Zhang,Sunli Chen,Lixing Fang,Qiushi Lyu,Xinyu Sun,Jincheng Yang,Zeyuan Wang,Bao Chi Dang,Zhehuan Chen,Daksha Ladia,Jiageng Liu,Chuang Gan*

Main category: cs.CV

TL;DR: 论文提出虚拟社区平台，通过物理引擎和真实3D场景研究人机共存的社会智能，包含多智能体协作挑战和社区规划挑战。


<details>
  <summary>Details</summary>
Motivation: 探索AI与机器人快速发展背景下，人类与机器人在开放世界中共存时产生的社会关系构建、协作机制及共存策略。

Method: 1) 开发支持多智能体物理交互的开源模拟器；2) 构建真实世界对齐的大规模社区生成管道；3) 设计社区规划与机器人协作两类新型评估任务。

Result: 基线测试显示开放世界任务规划与底层协作控制存在显著挑战，验证了平台对复杂社会智能研究的有效性。

Conclusion: 虚拟社区为研究开放环境下人机共生的高层规划与底层协作机制提供了可扩展的实验平台。

Abstract: The rapid progress in AI and Robotics may lead to a profound societal
transformation, as humans and robots begin to coexist within shared
communities, introducing both opportunities and challenges. To explore this
future, we present Virtual Community-an open-world platform for humans, robots,
and society-built on a universal physics engine and grounded in real-world 3D
scenes. With Virtual Community, we aim to study embodied social intelligence at
scale: 1) How robots can intelligently cooperate or compete; 2) How humans
develop social relations and build community; 3) More importantly, how
intelligent robots and humans can co-exist in an open world. To support these,
Virtual Community features: 1) An open-source multi-agent physics simulator
that supports robots, humans, and their interactions within a society; 2) A
large-scale, real-world aligned community generation pipeline, including vast
outdoor space, diverse indoor scenes, and a community of grounded agents with
rich characters and appearances. Leveraging Virtual Community, we propose two
novel challenges. The Community Planning Challenge evaluates multi-agent
reasoning and planning ability in open-world settings, such as cooperating to
help agents with daily activities and efficiently connecting other agents. The
Community Robot Challenge requires multiple heterogeneous robots to collaborate
in solving complex open-world tasks. We evaluate various baselines on these
tasks and demonstrate the challenges in both high-level open-world task
planning and low-level cooperation controls. We hope that Virtual Community
will unlock further study of human-robot coexistence within open-world
environments.

</details>
