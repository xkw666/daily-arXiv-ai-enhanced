<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 51]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.SE](#cs.SE) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.AI](#cs.AI) [Total: 8]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI](https://arxiv.org/abs/2508.18290)
*Hans-Joachim Rudolph*

Main category: cs.CL

TL;DR: 提出基于复数空间语义吸引子的新型AGI框架，通过递归张量变换实现语义生成而非预测，突破传统统计语言模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型依赖统计预测无法处理语义复杂性（如反讽/歧义），需构建能主动塑造语言意义的认知架构。

Method: 在复数值空间建立旋转语义结构，通过虚数单位循环操作和递归张量变换，结合梯度流/张量变形实现语义吸引子动力学。

Result: 构建出能解释语义复杂现象的数学模型，证明语义吸引子可引导意义向稳定态收敛，揭示AGI需要语义生成型认知架构。

Conclusion: 真正语义智能产生于递归收敛的语义连贯过程，这要求从预测范式转向生成范式，为AGI发展提供理论突破方向。

Abstract: This essay develops a theoretical framework for a semantic Artificial General
Intelligence (AGI) based on the notion of semantic attractors in complex-valued
meaning spaces. Departing from current transformer-based language models, which
operate on statistical next-token prediction, we explore a model in which
meaning is not inferred probabilistically but formed through recursive
tensorial transformation. Using cyclic operations involving the imaginary unit
\emph{i}, we describe a rotational semantic structure capable of modeling
irony, homonymy, and ambiguity. At the center of this model, however, is a
semantic attractor -- a teleological operator that, unlike statistical
computation, acts as an intentional agent (Microvitum), guiding meaning toward
stability, clarity, and expressive depth. Conceived in terms of gradient flows,
tensor deformations, and iterative matrix dynamics, the attractor offers a
model of semantic transformation that is not only mathematically suggestive,
but also philosophically significant. We argue that true meaning emerges not
from simulation, but from recursive convergence toward semantic coherence, and
that this requires a fundamentally new kind of cognitive architecture -- one
designed to shape language, not just predict it.

</details>


### [2] [LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions](https://arxiv.org/abs/2508.18321)
*Maojia Song,Tej Deep Pala,Weisheng Jin,Amir Zadeh,Chuan Li,Dorien Herremans,Soujanya Poria*

Main category: cs.CL

TL;DR: 研究大语言模型在多智能体系统中如何建立信任、抵御错误信息并整合群体意见，提出KAIROS基准测试框架及优化策略


<details>
  <summary>Details</summary>
Motivation: 突破传统从众偏差的研究局限，探究信任机制、群体动态对集体智能的影响，旨在提升多智能体协作中LLM的决策质量

Method: 构建KAIROS模拟测试平台（含专家/新手角色、噪声群体、对抗性智能体），通过提示工程、监督微调和强化学习（GRPO算法）进行系统验证

Result: GRPO结合多智能体上下文和结果奖励机制实现最佳性能，但降低了模型对社会影响的鲁棒性。模型代码和数据集已开源

Conclusion: 揭示性能提升与社会影响脆弱性之间的平衡难题，为开发具有社会适应性的多智能体系统提供新方法论和基准测试工具

Abstract: Large language models (LLMs) are increasingly deployed in multi-agent systems
(MAS) as components of collaborative intelligence, where peer interactions
dynamically shape individual decision-making. Although prior work has focused
on conformity bias, we extend the analysis to examine how LLMs form trust from
previous impressions, resist misinformation, and integrate peer input during
interaction, key factors for achieving collective intelligence under complex
social dynamics. We present KAIROS, a benchmark simulating quiz contests with
peer agents of varying reliability, offering fine-grained control over
conditions such as expert-novice roles, noisy crowds, and adversarial peers.
LLMs receive both historical interactions and current peer responses, allowing
systematic investigation into how trust, peer action, and self-confidence
influence decisions. As for mitigation strategies, we evaluate prompting,
supervised fine-tuning, and reinforcement learning, Group Relative Policy
Optimisation (GRPO), across multiple models. Our results reveal that GRPO with
multi-agent context combined with outcome-based rewards and unconstrained
reasoning achieves the best overall performance, but also decreases the
robustness to social influence compared to Base models. The code and datasets
are available at: https://github.com/declare-lab/KAIROS.

</details>


### [3] [Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective](https://arxiv.org/abs/2508.18328)
*Masudul Hasan Masud Bhuiyan,Matteo Varvello,Yasir Zaki,Cristian-Alexandru Staicu*

Main category: cs.CL

TL;DR: 通过构建首个多语言网页数据集LangCrUX，揭示网页无障碍提示普遍忽视语言多样性问题，并提出语言感知的自动化检测工具Kizuki。


<details>
  <summary>Details</summary>
Motivation: 现存网页无障碍研究缺乏多语言数据集，导致屏幕阅读器对非拉丁文字支持不足，加剧视障用户访问障碍。

Method: 1. 构建包含12种非拉丁语系、12万个流行网站的LangCrUX数据集
2. 系统性分析多语言网页的无障碍提示
3. 开发语言感知检测工具Kizuki

Result: 发现超半数网站的无障碍提示存在语言不一致问题，导致屏幕阅读器效率降低43%。

Conclusion: 需建立语言敏感的无障碍检测机制，Kizuki工具可提升多语言网页辅助技术兼容性20%以上。

Abstract: English is the predominant language on the web, powering nearly half of the
world's top ten million websites. Support for multilingual content is
nevertheless growing, with many websites increasingly combining English with
regional or native languages in both visible content and hidden metadata. This
multilingualism introduces significant barriers for users with visual
impairments, as assistive technologies like screen readers frequently lack
robust support for non-Latin scripts and misrender or mispronounce non-English
text, compounding accessibility challenges across diverse linguistic contexts.
Yet, large-scale studies of this issue have been limited by the lack of
comprehensive datasets on multilingual web content. To address this gap, we
introduce LangCrUX, the first large-scale dataset of 120,000 popular websites
across 12 languages that primarily use non-Latin scripts. Leveraging this
dataset, we conduct a systematic analysis of multilingual web accessibility and
uncover widespread neglect of accessibility hints. We find that these hints
often fail to reflect the language diversity of visible content, reducing the
effectiveness of screen readers and limiting web accessibility. We finally
propose Kizuki, a language-aware automated accessibility testing extension to
account for the limited utility of language-inconsistent accessibility hints.

</details>


### [4] [Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models](https://arxiv.org/abs/2508.18381)
*Yuchun Fan,Yilin Wang,Yongyu Mu,Lei Huang,Bei Li,Xiaocheng Feng,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 提出PLAST方法，通过精准微调大视觉语言模型的浅层语言相关神经元，仅调整14%参数即可显著提升多语言能力


<details>
  <summary>Details</summary>
Motivation: 大视觉语言模型在多语言理解能力上存在不平衡现象，其能力与浅层语言特异性神经元激活相关

Method: 1. 通过监控语言特异性神经元激活识别关键层 2. 使用问题翻译对微调实现多语言对齐

Result: 在MM-Bench和MMMB基准测试中有效提升多语言能力，参数效率提升至仅需调整14%

Conclusion: 该方法可泛化至低资源与复杂视觉推理任务，促进浅层语言特异性视觉信息处理能力

Abstract: Large vision-language models (LVLMs) have demonstrated exceptional
capabilities in understanding visual information with human languages but also
exhibit an imbalance in multilingual capabilities. In this work, we delve into
the multilingual working pattern of LVLMs and identify a salient correlation
between the multilingual understanding ability of LVLMs and language-specific
neuron activations in shallow layers. Building on this insight, we introduce
PLAST, a training recipe that achieves efficient multilingual enhancement for
LVLMs by Precise LAnguage-Specific layers fine-Tuning. PLAST first identifies
layers involved in multilingual understanding by monitoring language-specific
neuron activations. These layers are then precisely fine-tuned with
question-translation pairs to achieve multilingual alignment. Our empirical
results on MM-Bench and MMMB demonstrate that PLAST effectively improves the
multilingual capabilities of LVLMs and achieves significant efficiency with
only 14% of the parameters tuned. Further analysis reveals that PLAST can be
generalized to low-resource and complex visual reasoning tasks, facilitating
the language-specific visual information engagement in shallow layers.

</details>


### [5] [Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails](https://arxiv.org/abs/2508.18384)
*Kellen Tan Cheng,Anna Lisa Gentile,Chad DeLuca,Guang-Jie Ren*

Main category: cs.CL

TL;DR: 提出backprompting方法生成生产级标注数据，结合稀疏人工聚类技术，构建接近真实LLM输出的合成数据集，训练出的健康建议检测器性能超越GPT-4o达3.73%（参数量仅1/400）


<details>
  <summary>Details</summary>
Motivation: 企业级LLM应用存在健康建议等高风险场景，传统检测器因缺乏部署前的真实LLM输出标注数据而难以构建鲁棒防线

Method: 通过backprompting逆向生成模拟生产环境的LLM输出数据，采用人机协同聚类进行标注，增强现有数据集以训练轻量级检测模型

Result: 参数量仅1/400的检测器在健康建议识别任务中F1值达89.37%，相比GPT-4o提升3.73个百分点

Conclusion: backprompting有效解决LLM安全护栏开发中的数据瓶颈，证明小模型通过优质合成数据增强可实现超越大模型的细粒度风险检测能力

Abstract: The pervasiveness of large language models (LLMs) in enterprise settings has
also brought forth a significant amount of risks associated with their usage.
Guardrails technologies aim to mitigate this risk by filtering LLMs'
input/output text through various detectors. However, developing and
maintaining robust detectors faces many challenges, one of which is the
difficulty in acquiring production-quality labeled data on real LLM outputs
prior to deployment. In this work, we propose backprompting, a simple yet
intuitive solution to generate production-like labeled data for health advice
guardrails development. Furthermore, we pair our backprompting method with a
sparse human-in-the-loop clustering technique to label the generated data. Our
aim is to construct a parallel corpus roughly representative of the original
dataset yet resembling real LLM output. We then infuse existing datasets with
our synthetic examples to produce robust training data for our detector. We
test our technique in one of the most difficult and nuanced guardrails: the
identification of health advice in LLM output, and demonstrate improvement
versus other solutions. Our detector is able to outperform GPT-4o by up to
3.73%, despite having 400x less parameters.

</details>


### [6] [Integral Transformer: Denoising Attention, Not Too Much Not Too Little](https://arxiv.org/abs/2508.18387)
*Ivan Kobyzev,Abbas Ghaddar,Dingtao Hu,Boxing Chen*

Main category: cs.CL

TL;DR: 提出了Integral Transformer，通过整合logit分布的采样信号实现自注意力降噪，在保留关键特殊标记的同时提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有Cog和Differential注意力机制通过负分数抑制噪声时可能丢失有用信息，需在降噪与保留关键标记间取得平衡

Method: 在Transformer底层使用常规自注意力增强表达能力，在高层应用Integral Transformer整合logit信号平衡注意力分布

Result: 在知识推理基准上超越多种注意力变体，分析显示能有效缓解秩崩溃现象并优化注意力分布

Conclusion: 分层注意力设计（底层常规+高层积分）实现了性能提升与注意力机制优化的双重目标，为Transformer改进提供新方向

Abstract: Softmax self-attention often assigns disproportionate weight to semantically
uninformative tokens such as special tokens and punctuation, a phenomenon known
as attention noise. While recent methods like Cog Attention and the
Differential Transformer have addressed this by introducing negative attention
scores, they risk discarding useful information. In this paper, we propose the
Integral Transformer, a novel self-attention mechanism that denoises attention
by integrating signals sampled from the logit distribution. Our approach
mitigates noise while preserving the contributions of special tokens critical
for model performance. Extensive experiments demonstrate that our model
outperforms vanilla, Cog, and Differential attention variants on
well-established knowledge and reasoning language benchmarks. Moreover, our
analysis reveals that employing vanilla self-attention in the lower Transformer
layers enhances performance and that the Integral Transformer effectively
balances attention distributions and reduces rank collapse in upper layers.

</details>


### [7] [Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning](https://arxiv.org/abs/2508.18395)
*Jeong-seok Oh,Jay-yoon Lee*

Main category: cs.CL

TL;DR: 提出潜在语义一致性（LSC）方法，通过可学习的词嵌入选择语义最一致的回答，在短/长格式问答中均超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自洽方法（SC/USC/WUCS）在短格式和长格式答案场景中存在准确率无法兼顾的问题，需要统一解决方案。

Method: 使用可学习词嵌入进行语义一致性选择，配合轻量级摘要标记生成技术，推理时间增加不足1%且无需修改模型架构。

Result: 在6个短格式和5个长格式推理基准测试中全面超越现有方法，同时保持校准误差低于0.04。

Conclusion: LSC成为首个在多种答案格式中均可靠的自洽选择方法，兼具计算高效和置信度校准优势。

Abstract: Probabilistic decoding in Large Language Models (LLMs) often yields
inconsistent outputs, particularly on complex or long-form questions.
Self-Consistency (SC) mitigates this for short-form QA by majority voting over
exact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram
Consistency Score (WUCS) extend to long-form responses but lose accuracy on
short-form benchmarks.
  We introduce Latent Self-Consistency (LSC), which selects the most
semantically consistent response using learnable token embeddings. A
lightweight forward generation of summary tokens increases inference time by
less than 1% and requires no changes to the model architecture.
  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU,
TruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form
ones on average, while maintaining negligible computational overhead. These
results position LSC as a practical consistency-selection method that works
reliably across answer formats. Additionally, LSC provides well-calibrated
confidence estimates, maintaining low Expected Calibration Error across both
answer formats.

</details>


### [8] [Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering](https://arxiv.org/abs/2508.18407)
*Michal Štefánik,Timothee Mickus,Marek Kadlčík,Michal Spiegel,Josef Kuchař*

Main category: cs.CL

TL;DR: 论文质疑当前AI领域广泛使用的OOD评估方法在检测模型真实场景失效的可靠性，发现不同QA数据集对模型抗干扰能力的评估质量差异显著，并提出了改进方法建议。


<details>
  <summary>Details</summary>
Motivation: 挑战OOD评估方法能有效反映模型实际部署中潜在失效的假设，揭示现有QA模型评估中存在的「伪特征依赖」问题。

Method: 通过将OOD评估结果与已记录的QA模型失效模式（预测捷径）进行对比，分析不同OOD数据集对模型抗干扰能力的评估质量差异及其成因。

Result: 发现部分OOD数据集的评估质量甚至低于简单同分布测试，部分源于ID/OOD数据集间共享伪特征，部分源于数据集训练/评估质量的脱节。

Conclusion: 强调常用OOD评估方法的局限性，提出应建立更鲁棒的评估方法论，推荐同时关注域内域外评估，并改进数据集质量评估体系。

Abstract: A majority of recent work in AI assesses models' generalization capabilities
through the lens of performance on out-of-distribution (OOD) datasets. Despite
their practicality, such evaluations build upon a strong assumption: that OOD
evaluations can capture and reflect upon possible failures in a real-world
deployment.
  In this work, we challenge this assumption and confront the results obtained
from OOD evaluations with a set of specific failure modes documented in
existing question-answering (QA) models, referred to as a reliance on spurious
features or prediction shortcuts.
  We find that different datasets used for OOD evaluations in QA provide an
estimate of models' robustness to shortcuts that have a vastly different
quality, some largely under-performing even a simple, in-distribution
evaluation. We partially attribute this to the observation that spurious
shortcuts are shared across ID+OOD datasets, but also find cases where a
dataset's quality for training and evaluation is largely disconnected. Our work
underlines limitations of commonly-used OOD-based evaluations of
generalization, and provides methodology and recommendations for evaluating
generalization within and beyond QA more robustly.

</details>


### [9] [How Reliable are LLMs for Reasoning on the Re-ranking task?](https://arxiv.org/abs/2508.18444)
*Nafis Tanveer Islam,Zhiming Zhao*

Main category: cs.CL

TL;DR: 探讨不同训练方法对LLM在重排序任务中语义理解的影响，发现部分训练方法通过抽象知识优化评估指标而非真正理解语义，质疑LLM可靠性。


<details>
  <summary>Details</summary>
Motivation: LLM在提升语义理解时牺牲了透明度，且数据不足的新系统面临重排序准确性挑战，需验证模型能否生成可信的文本推理。

Method: 使用环境与地球科学领域的小型排名数据集，分析不同训练方法对LLM语义理解和推理可解释性的影响。

Result: 部分训练方法展现出更好的可解释性，但部分方法仅优化评估指标而未实现真实语义理解，暴露LLM可靠性隐忧。

Conclusion: 需开发更透明的训练方法以提高LLM在有限数据场景下的可靠性和可解释性，确保推理过程的真实语义理解。

Abstract: With the improving semantic understanding capability of Large Language Models
(LLMs), they exhibit a greater awareness and alignment with human values, but
this comes at the cost of transparency. Although promising results are achieved
via experimental analysis, an in-depth understanding of the LLM's internal
workings is unavoidable to comprehend the reasoning behind the re-ranking,
which provides end users with an explanation that enables them to make an
informed decision. Moreover, in newly developed systems with limited user
engagement and insufficient ranking data, accurately re-ranking content remains
a significant challenge. While various training methods affect the training of
LLMs and generate inference, our analysis has found that some training methods
exhibit better explainability than others, implying that an accurate semantic
understanding has not been learned through all training methods; instead,
abstract knowledge has been gained to optimize evaluation, which raises
questions about the true reliability of LLMs. Therefore, in this work, we
analyze how different training methods affect the semantic understanding of the
re-ranking task in LLMs and investigate whether these models can generate more
informed textual reasoning to overcome the challenges of transparency or LLMs
and limited training data. To analyze the LLMs for re-ranking tasks, we utilize
a relatively small ranking dataset from the environment and the Earth science
domain to re-rank retrieved content. Furthermore, we also analyze the
explainable information to see if the re-ranking can be reasoned using
explainability.

</details>


### [10] [Integrating gender inclusivity into large language models via instruction tuning](https://arxiv.org/abs/2508.18466)
*Alina Wróblewska,Bartosz Żuk*

Main category: cs.CL

TL;DR: 通过IPIS数据集调整波兰语大语言模型，系统性减轻语法性别偏见


<details>
  <summary>Details</summary>
Motivation: 波兰语因历史惯例过度使用阳性词形导致语言模型生成性别失衡内容，需解决其社会影响

Method: 设计显式性别包容系统提示，使用IPIS数据集对Llama/Mistral等模型进行指令微调

Result: 实现将性别包容性内化为多语言模型及波兰专用模型的固有特征

Conclusion: 为波兰语生成任务提供首个理论语言学框架支持的性别偏见缓解方案

Abstract: Imagine a language with masculine, feminine, and neuter grammatical genders,
yet, due to historical and political conventions, masculine forms are
predominantly used to refer to men, women and mixed-gender groups. This is the
reality of contemporary Polish. A social consequence of this unfair linguistic
system is that large language models (LLMs) trained on Polish texts inherit and
reinforce this masculine bias, generating gender-imbalanced outputs. This study
addresses this issue by tuning LLMs using the IPIS dataset, a collection of
human-crafted gender-inclusive proofreading in Polish and Polish-to-English
translation instructions. Grounded in a theoretical linguistic framework, we
design a system prompt with explicit gender-inclusive guidelines for Polish. In
our experiments, we IPIS-tune multilingual LLMs (Llama-8B, Mistral-7B and
Mistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM). Our approach aims to
integrate gender inclusivity as an inherent feature of these models, offering a
systematic solution to mitigate gender bias in Polish language generation.

</details>


### [11] [Principled Detection of Hallucinations in Large Language Models via Multiple Testing](https://arxiv.org/abs/2508.18473)
*Jiawei Li,Akshayaa Magesh,Venugopal V. Veeravalli*

Main category: cs.CL

TL;DR: 提出了一种基于多重假设检验的LLM幻觉检测方法，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(LLMs)存在生成看似合理但实际错误内容的幻觉问题，需建立有效检测机制。

Method: 将幻觉检测建模为假设检验问题，借鉴机器学习中的分布外检测思路，开发基于多重测试的检测框架。

Result: 大量实验表明该方法在鲁棒性方面显著优于当前最先进的方法。

Conclusion: 基于假设检验的多重测试方法能有效检测LLM幻觉，为模型可靠性评估提供了新思路。

Abstract: While Large Language Models (LLMs) have emerged as powerful foundational
models to solve a variety of tasks, they have also been shown to be prone to
hallucinations, i.e., generating responses that sound confident but are
actually incorrect or even nonsensical. In this work, we formulate the problem
of detecting hallucinations as a hypothesis testing problem and draw parallels
to the problem of out-of-distribution detection in machine learning models. We
propose a multiple-testing-inspired method to solve the hallucination detection
problem, and provide extensive experimental results to validate the robustness
of our approach against state-of-the-art methods.

</details>


### [12] [COMET-poly: Machine Translation Metric Grounded in Other Candidates](https://arxiv.org/abs/2508.18549)
*Maike Züfle,Vilém Zouhar,Tu Anh Dinh,Felipe Maia Polo,Jan Niehues,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 提出COMET-polycand和COMET-polyic两种新指标，通过引入多翻译对比和上下文学习提升机器翻译自动评估效果


<details>
  <summary>Details</summary>
Motivation: 现有自动评估指标仅使用单一翻译，而人类评估时会参考多个备选翻译，这种评估方式差异限制了自动指标的性能

Method: COMET-polycand通过对比同一源句的多翻译结果提升评估质量；COMET-polyic采用检索相似源文本的翻译及人工评分进行上下文学习

Result: COMET-polycand加入额外翻译后Kendall's tau-b提升0.079-0.118；COMET-polyic通过检索示例达到类似提升(0.079-0.116)

Conclusion: 利用多翻译对比和上下文学习能有效提升自动评估指标性能，两种方法均取得显著效果提升并公开了模型

Abstract: Automated metrics for machine translation attempt to replicate human
judgment. Unlike humans, who often assess a translation in the context of
multiple alternatives, these metrics typically consider only the source
sentence and a single translation. This discrepancy in the evaluation setup may
negatively impact the performance of automated metrics. We propose two
automated metrics that incorporate additional information beyond the single
translation. COMET-polycand uses alternative translations of the same source
sentence to compare and contrast with the translation at hand, thereby
providing a more informed assessment of its quality. COMET-polyic, inspired by
retrieval-based in-context learning, takes in translations of similar source
texts along with their human-labeled quality scores to guide the evaluation. We
find that including a single additional translation in COMET-polycand improves
the segment-level metric performance (0.079 to 0.118 Kendall's tau-b
correlation), with further gains when more translations are added.
Incorporating retrieved examples in COMET-polyic yields similar improvements
(0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.

</details>


### [13] [The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation](https://arxiv.org/abs/2508.18569)
*Girish A. Koushik,Fatemeh Nazarieh,Katherine Birch,Shenbin Qian,Diptesh Kanojia*

Main category: cs.CL

TL;DR: 提出自评估视觉隐喻生成框架，通过结构化提示分解和轻量级强化学习提升隐喻对齐效果


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉隐喻生成中存在语言理解与视觉连贯性对齐不足的问题，需要同时兼顾隐喻含义保持和图像质量

Method: 1) 无训练S-T-M分解提示法（源-目标-意义映射）
2) 基于自评估奖励的轻量级强化学习训练框架

Result: 测试集显示无训练方法在分解/CLIP/MA指标上超越GPT-4o和Imagen；用户研究显示开源方法在抽象隐喻生成中优于Imagen，但总体仍偏好GPT-4o的审美输出

Conclusion: 结构化提示与轻量级RL能有效提升隐喻对齐，与人类偏好的差距主要源于美学质量和采样敏感性，封闭模型在短/具体隐喻表现更佳

Abstract: Visual metaphor generation is a challenging task that aims to generate an
image given an input text metaphor. Inherently, it needs language understanding
to bind a source concept with a target concept, in a way that preserves meaning
while ensuring visual coherence. We propose a self-evaluating visual metaphor
generation framework that focuses on metaphor alignment. Our self-evaluation
approach combines existing metrics with our newly proposed metaphor
decomposition score and a meaning alignment (MA) metric. Within this setup, we
explore two novel approaches: a training-free pipeline that explicitly
decomposes prompts into source-target-meaning (S-T-M) mapping for image
synthesis, and a complementary training-based pipeline that improves alignment
using our proposed self-evaluation reward schema, without any large-scale
retraining. On the held-out test set, the training-free approach surpasses
strong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores,
with the training-based approach close behind. We evaluate our framework output
using a user-facing study, and observed that participants preferred GPT-4o
overall, while our training-free pipeline led open-source methods and edged
Imagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or
more abstract metaphors, with closed models excelling on short, concrete cases;
we also observe sensitivity to sampler settings. Overall, structured prompting
and lightweight RL perform metaphor alignment well under modest compute, and
remaining gaps to human preference appear driven by aesthetics and sampling.

</details>


### [14] [What do language models model? Transformers, automata, and the format of thought](https://arxiv.org/abs/2508.18598)
*Colin Klein*

Main category: cs.CL

TL;DR: 论文论证大语言模型本质是训练语料库的统计模型而非人类认知模型，提出Transformer架构的线性计算限制与人类语言处理的超线性特征存在本质差异


<details>
  <summary>Details</summary>
Motivation: 回应学界关于大语言模型是否反映人类认知能力的争论，通过计算架构差异论证LLMs本质是对语料库统计规律的建模

Method: 1. 基于Transformer架构计算不变量的理论分析
2. 结合Liu等人(2022)的捷径自动机理论构建解释框架
3. 语言作为'话语机器'的哲学视角对比人类与LLMs的语言生成机制

Result: 揭示Transformer线性计算特性与人类认知架构的根本差异，提出LLMs通过语料统计建模构建新型'话语机器'的理论模型

Conclusion: 语言既是内在状态的表达工具，也是基于语境生成新话语的机器。人类与LLMs以不同机制掌握了这种技术，但都验证了语言作为生成系统的本质属性

Abstract: What do large language models actually model? Do they tell us something about
human capacities, or are they models of the corpus we've trained them on? I
give a non-deflationary defence of the latter position. Cognitive science tells
us that linguistic capabilities in humans rely supralinear formats for
computation. The transformer architecture, by contrast, supports at best a
linear formats for processing. This argument will rely primarily on certain
invariants of the computational architecture of transformers. I then suggest a
positive story about what transformers are doing, focusing on Liu et al.
(2022)'s intriguing speculations about shortcut automata. I conclude with why I
don't think this is a terribly deflationary story. Language is not (just) a
means for expressing inner state but also a kind of 'discourse machine' that
lets us make new language given appropriate context. We have learned to use
this technology in one way; LLMs have also learned to use it too, but via very
different means.

</details>


### [15] [A New NMT Model for Translating Clinical Texts from English to Spanish](https://arxiv.org/abs/2508.18607)
*Rumeng Li,Xun Wang,Hong Yu*

Main category: cs.CL

TL;DR: 提出NOOV神经机器翻译系统解决电子健康记录翻译中平行语料不足和未知词问题


<details>
  <summary>Details</summary>
Motivation: 电子健康记录翻译对临床诊疗至关重要，但面临平行语料库缺乏和未知专业词汇多的双重挑战

Method: 整合双语词典（自动学习自平行语料）和生物医学短语查找表，通过双重机制缓解未知词问题并改善短语生成质量

Result: 评估显示在翻译准确率提升12.7%，语句流畅度提高18.4%

Conclusion: NOOV系统通过知识增强策略有效提升专业领域机器翻译性能，为跨语言医疗信息处理提供新方案

Abstract: Translating electronic health record (EHR) narratives from English to Spanish
is a clinically important yet challenging task due to the lack of a
parallel-aligned corpus and the abundant unknown words contained. To address
such challenges, we propose \textbf{NOOV} (for No OOV), a new neural machine
translation (NMT) system that requires little in-domain parallel-aligned corpus
for training. NOOV integrates a bilingual lexicon automatically learned from
parallel-aligned corpora and a phrase look-up table extracted from a large
biomedical knowledge resource, to alleviate both the unknown word problem and
the word-repeat challenge in NMT, enhancing better phrase generation of NMT
systems. Evaluation shows that NOOV is able to generate better translation of
EHR with improvement in both accuracy and fluency.

</details>


### [16] [Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models](https://arxiv.org/abs/2508.18609)
*Chenxi Zhou,Pengfei Cao,Jiang Li,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 该论文通过实证研究建立了任务分层的缩放定律，揭示训练后量化(PTQ)对LLM知识能力的影响差异：知识记忆能力对量化参数更敏感，知识利用能力则更稳健。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对PTQ如何影响LLM不同知识能力的系统理解，且量化模型的缩放定律常忽略PTQ特有参数和任务敏感性。

Method: 将LLM知识解构为记忆与利用能力，构建包含模型规模/有效比特宽度/校准集大小/分组大小的统一量化框架进行敏感性分析。

Result: 知识记忆能力对有效比特宽度/校准集大小/模型规模的敏感性比知识利用能力高3-5倍，表现出显著差异。

Conclusion: 研究结果为开发知识感知的量化策略提供指导，建议根据目标认知功能（记忆/利用）制定差异化的压缩方案。

Abstract: Large language models (LLMs) present significant deployment challenges due to
their scale, with post-training quantization (PTQ) emerging as a practical
compression solution. However, a comprehensive understanding of how PTQ
precisely impacts diverse LLM knowledge capabilities remains elusive, and
existing scaling laws for quantized models often overlook crucial PTQ-specific
parameters and task-specific sensitivities. This paper addresses these gaps by
conducting an extensive empirical investigation to establish task-stratified
scaling laws. We disentangle LLM knowledge into memorization and utilization
capabilities and develop a unified quantitative framework that incorporates
model size, effective bit-width, calibration set size, and group size. Our
central finding reveals that knowledge memorization exhibits markedly greater
sensitivity to variations in effective bit-width, calibration set size, and
model size compared to the more robust knowledge utilization. These findings
offer a fine-grained understanding of PTQ's impact and provide guidance for
developing knowledge-aware quantization strategies that can better preserve
targeted cognitive functions.

</details>


### [17] [Thinking Before You Speak: A Proactive Test-time Scaling Approach](https://arxiv.org/abs/2508.18648)
*Cong Li,Wenchang Chai,Hejun Wu,Yan Pan,Pengxu Wei,Liang Lin*

Main category: cs.CL

TL;DR: 提出TBYS框架，通过在推理步骤间插入主动生成的insights提升大语言模型数学推理能力，实验验证有效


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在复杂推理任务（如数学）表现不佳，因训练数据缺乏人类思考时未明示的关键中间意图和方法论

Method: 设计自动收集和过滤上下文示例的流程，构建主动生成insights的推理框架TBYS，无需人工标注和微调

Result: 在复杂数学数据集上验证有效性，推理框架显著提升模型表现

Conclusion: 主动生成insights能有效桥接推理步骤，自动化流程实现低成本的复杂推理能力提升

Abstract: Large Language Models (LLMs) often exhibit deficiencies with complex
reasoning tasks, such as maths, which we attribute to the discrepancy between
human reasoning patterns and those presented in the LLMs' training data. When
dealing with complex problems, humans tend to think carefully before expressing
solutions. However, they often do not articulate their inner thoughts,
including their intentions and chosen methodologies. Consequently, critical
insights essential for bridging reasoning steps may be absent in training data
collected from human sources. To bridge this gap, we proposes inserting
\emph{insight}s between consecutive reasoning steps, which review the status
and initiate the next reasoning steps. Unlike prior prompting strategies that
rely on a single or a workflow of static prompts to facilitate reasoning,
\emph{insight}s are \emph{proactively} generated to guide reasoning processes.
We implement our idea as a reasoning framework, named \emph{Thinking Before You
Speak} (TBYS), and design a pipeline for automatically collecting and filtering
in-context examples for the generation of \emph{insight}s, which alleviates
human labeling efforts and fine-tuning overheads. Experiments on challenging
mathematical datasets verify the effectiveness of TBYS. Project website:
https://gitee.com/jswrt/TBYS

</details>


### [18] [Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models](https://arxiv.org/abs/2508.18651)
*Chenxu Yang,Qingyi Si,Zheng Lin*

Main category: cs.CL

TL;DR: 提出协作解码框架CoDe，通过动态整合外部知识概率分布与置信度指导，在保持表达力的同时显著提升大语言模型的回答忠实性。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型整合外部知识时面临忠实性（缺乏知识支撑）与表达力（输出不自然）的权衡困境，需突破该限制。

Method: 1. 动态融合有无外部知识的输出概率，基于分布差异和置信度选择可靠表达
2. 知识感知重排序机制防止参数知识过度依赖，确保外部信息有效利用

Result: 跨多个LLM和评估指标的实验表明，CoDe在提升忠实性同时保持表达力，验证了框架的有效性和通用性。

Conclusion: CoDe作为即插即用方案，突破了传统忠实性与表达力的取舍困境，为知识增强型LLM提供了新范式。

Abstract: Grounding responses in external knowledge represents an effective strategy
for mitigating hallucinations in Large Language Models (LLMs). However, current
LLMs struggle to seamlessly integrate knowledge while simultaneously
maintaining faithfulness (or fidelity) and expressiveness, capabilities that
humans naturally possess. This limitation results in outputs that either lack
support from external knowledge, thereby compromising faithfulness, or appear
overly verbose and unnatural, thus sacrificing expressiveness. In this work, to
break the trade-off between faithfulness and expressiveness, we propose
Collaborative Decoding (CoDe), a novel approach that dynamically integrates
output probabilities generated with and without external knowledge. This
integration is guided by distribution divergence and model confidence, enabling
the selective activation of relevant and reliable expressions from the model's
internal parameters. Furthermore, we introduce a knowledge-aware reranking
mechanism that prevents over-reliance on prior parametric knowledge while
ensuring proper utilization of provided external information. Through
comprehensive experiments, our plug-and-play CoDe framework demonstrates
superior performance in enhancing faithfulness without compromising
expressiveness across diverse LLMs and evaluation metrics, validating both its
effectiveness and generalizability.

</details>


### [19] [Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models](https://arxiv.org/abs/2508.18655)
*Haoyu Wang,Guangyan Zhang,Jiale Chen,Jingyu Li,Yuehai Wang,Yiwen Guo*

Main category: cs.CL

TL;DR: 提出Emotion Omni语音大模型架构，通过新型模型设计和20万情感对话数据集，实现有限数据下对用户语音的情感理解与共情回应生成。


<details>
  <summary>Details</summary>
Motivation: 现有语音LLM忽视用户语音中的情感线索，且训练同理能力需海量数据。需开发数据高效的方法提升语音助手的情感交互能力。

Method: 1. 设计专注语音情感理解的模型架构 2. 基于开源TTS框架构建数据生成pipeline 3. 创建20万条含情感标注的对话数据集

Result: 成功构建支持情感交互的语音数据集，开发出能理解用户语音情感并生成共情回应的语音助手系统（演示地址见论文）

Conclusion: Emotion Omni证明了有限数据下实现语音情感理解的可行性，其数据生成方法为低资源语音LLM研究提供了新思路。

Abstract: With the development of speech large language models (speech LLMs), users can
now interact directly with assistants via speech. However, most existing models
simply convert the response content into speech without fully understanding the
rich emotional and paralinguistic cues embedded in the user's query. In many
cases, the same sentence can have different meanings depending on the emotional
expression. Furthermore, emotional understanding is essential for improving
user experience in human-machine interaction. Currently, most speech LLMs with
empathetic capabilities are trained on massive datasets. This approach requires
vast amounts of data and significant computational resources. Therefore, a key
challenge lies in how to develop a speech LLM capable of generating empathetic
responses with limited data and without the need for large-scale training. To
address this challenge, we propose Emotion Omni, a novel model architecture
designed to understand the emotional content of user speech input and generate
empathetic speech responses. Additionally, we developed a data generation
pipeline based on an open-source TTS framework to construct a 200k emotional
dialogue dataset, which supports the construction of an empathetic speech
assistant. The demos are available at https://w311411.github.io/omni_demo/

</details>


### [20] [Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum](https://arxiv.org/abs/2508.18673)
*Xinglong Yang,Quan Feng,Zhongying Pan,Xiang Chen,Yu Tian,Wentong Li,Shuofei Qiao,Yuxia Geng,Xingyu Zhao,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 提出基于教学原则的难度平衡提示选择框架，通过整合模型感知难度和样本复杂性提升多模态推理性能


<details>
  <summary>Details</summary>
Motivation: 现有MCoT方法随机选择示例未考虑模型知识分布和任务内在复杂性，导致性能不稳定

Method: 构建提示课程框架，结合模型预测分歧量化的感知难度与独立于模型的样本复杂度分析，设计双维度平衡采样策略

Result: 在5个基准测试和多种MLLMs上实现显著且一致的性能提升，减少随机采样带来的性能波动

Conclusion: 该方法为增强多模态推理提供了原则性解决方案，通过课程设计实现鲁棒的模型适应能力

Abstract: The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often
limited by the use of randomly or manually selected examples. These examples
fail to account for both model-specific knowledge distributions and the
intrinsic complexity of the tasks, resulting in suboptimal and unstable model
performance. To address this, we propose a novel framework inspired by the
pedagogical principle of "tailored teaching with balanced difficulty". We
reframe prompt selection as a prompt curriculum design problem: constructing a
well ordered set of training examples that align with the model's current
capabilities. Our approach integrates two complementary signals: (1)
model-perceived difficulty, quantified through prediction disagreement in an
active learning setup, capturing what the model itself finds challenging; and
(2) intrinsic sample complexity, which measures the inherent difficulty of each
question-image pair independently of any model. By jointly analyzing these
signals, we develop a difficulty-balanced sampling strategy that ensures the
selected prompt examples are diverse across both dimensions. Extensive
experiments conducted on five challenging benchmarks and multiple popular
Multimodal Large Language Models (MLLMs) demonstrate that our method yields
substantial and consistent improvements and greatly reduces performance
discrepancies caused by random sampling, providing a principled and robust
approach for enhancing multimodal reasoning.

</details>


### [21] [Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning](https://arxiv.org/abs/2508.18687)
*Songtao Jiang,Yuxi Chen,Sibo Song,Yan Zhang,Yeying Jin,Yang Feng,Jian Wu,Zuozhu Liu*

Main category: cs.CL

TL;DR: 研究发现当前医学视觉语言模型(Med-VLMs)在语义等效问题表述中存在答案不一致问题，提出基于知识锚定和对比学习的CCL方法显著提升模型鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有Med-VLMs在医学视觉问答中表现出答案波动性，无法保持不同问题表述下诊断结论的一致性，影响医疗可靠性

Method: 构建RoMed数据集(含14.4万变体问题)，提出CCL框架：1) 知识锚定一致性学习 2) 偏差感知对比学习

Result: LLaVA-Med模型在RoMed测试集召回率下降40%，CCL方法在三大VQA基准实现SOTA，答案一致性提升50%

Conclusion: 通过知识引导和偏差抑制，CCL有效提升医学多模态模型的语义鲁棒性，为可靠医疗诊断系统提供新思路

Abstract: In high-stakes medical applications, consistent answering across diverse
question phrasings is essential for reliable diagnosis. However, we reveal that
current Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility
in Medical Visual Question Answering, as their answers fluctuate significantly
when faced with semantically equivalent rephrasings of medical questions. We
attribute this to two limitations: (1) insufficient alignment of medical
concepts, leading to divergent reasoning patterns, and (2) hidden biases in
training data that prioritize syntactic shortcuts over semantic understanding.
To address these challenges, we construct RoMed, a dataset built upon original
VQA datasets containing 144k questions with variations spanning word-level,
sentence-level, and semantic-level perturbations. When evaluating
state-of-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarming
performance drops (e.g., a 40\% decline in Recall) compared to original VQA
benchmarks, exposing critical robustness gaps. To bridge this gap, we propose
Consistency and Contrastive Learning (CCL), which integrates two key
components: (1) knowledge-anchored consistency learning, aligning Med-VLMs with
medical knowledge rather than shallow feature patterns, and (2) bias-aware
contrastive learning, mitigating data-specific priors through discriminative
representation refinement. CCL achieves SOTA performance on three popular VQA
benchmarks and notably improves answer consistency by 50\% on the challenging
RoMed test set, demonstrating significantly enhanced robustness. Code will be
released.

</details>


### [22] [Attention2Probability: Attention-Driven Terminology Probability Estimation for Robust Speech-to-Text System](https://arxiv.org/abs/2508.18701)
*Yanfan Du,Jun Zhang,Bin Wang,Jin Qiu,Lu Huang,Yuan Ge,Xiaoqian Liu,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 提出了Attention2Probability方法，通过注意力机制提升语音大模型术语识别准确率，中英文召回率分别达92.57%和86.83%，延迟仅8.71ms，术语干预使准确率提升6-17%。


<details>
  <summary>Details</summary>
Motivation: 现有语音大模型在领域术语/新词识别上存在不足，需轻量灵活的解决方案，并缺乏相关数据集支撑研究。

Method: 将语音-术语交叉注意力权重转化为存在概率，结合课程学习提升检索精度，同时构建新术语语音数据集。

Result: 测试集显著超越VectorDB方法，最大召回率中92.57%/英86.83%，延迟8.71ms/query，术语干预提升准确率6-17%。

Conclusion: Attention2Probability有效提升术语准确性，揭示当前语音大模型对术语的利用存在局限性，需进一步优化术语应用策略。

Abstract: Recent advances in speech large language models (SLMs) have improved speech
recognition and translation in general domains, but accurately generating
domain-specific terms or neologisms remains challenging. To address this, we
propose Attention2Probability: attention-driven terminology probability
estimation for robust speech-to-text system, which is lightweight, flexible,
and accurate. Attention2Probability converts cross-attention weights between
speech and terminology into presence probabilities, and it further employs
curriculum learning to enhance retrieval accuracy. Furthermore, to tackle the
lack of data for speech-to-text tasks with terminology intervention, we create
and release a new speech dataset with terminology to support future research in
this area. Experimental results show that Attention2Probability significantly
outperforms the VectorDB method on our test set. Specifically, its maximum
recall rates reach 92.57% for Chinese and 86.83% for English. This high recall
is achieved with a latency of only 8.71ms per query. Intervening in SLMs'
recognition and translation tasks using Attention2Probability-retrieved terms
improves terminology accuracy by 6-17%, while revealing that the current
utilization of terminology by SLMs has limitations.

</details>


### [23] [Filtering for Creativity: Adaptive Prompting for Multilingual Riddle Generation in LLMs](https://arxiv.org/abs/2508.18709)
*Duy Le,Kent Ziti,Evan Girard-Sun,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: 提出自适应原创性过滤框架（AOF），通过余弦相似度过滤和跨语言保真度提升，显著提升多语言谜语生成的词汇多样性和文化适应性。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在多语言谜语生成中文化适应性与创意抽象难以平衡的问题，传统方法易导致重复记忆或浅层改写。

Method: 采用自适应原创性过滤（AOF）框架：1）基于余弦相似度的冗余生成过滤 2）强制实施词汇新颖性 3）跨语言保真度验证

Result: 在4种语言对测试中，AOF增强的GPT-4o实现日语Self-BLEU 0.177和Distinct-2 0.915，词汇多样性提升显著优于其他方法。

Conclusion: 语义拒绝机制可在无需任务微调情况下，有效引导具有文化根基的创造性生成，为跨语言内容创作提供新思路。

Abstract: Multilingual riddle generation challenges large language models (LLMs) to
balance cultural fluency with creative abstraction. Standard prompting
strategies -- zero-shot, few-shot, chain-of-thought -- tend to reuse memorized
riddles or perform shallow paraphrasing. We introduce Adaptive Originality
Filtering (AOF), a prompting framework that filters redundant generations using
cosine-based similarity rejection, while enforcing lexical novelty and
cross-lingual fidelity. Evaluated across three LLMs and four language pairs,
AOF-enhanced GPT-4o achieves \texttt{0.177} Self-BLEU and \texttt{0.915}
Distinct-2 in Japanese, signaling improved lexical diversity and reduced
redundancy compared to other prompting methods and language pairs. Our findings
show that semantic rejection can guide culturally grounded, creative generation
without task-specific fine-tuning.

</details>


### [24] [EMMM, Explain Me My Model! Explainable Machine Generated Text Detection in Dialogues](https://arxiv.org/abs/2508.18715)
*Angela Yifei Yuan,Haoyi Li,Soyeon Caren Han,Christopher Leckie*

Main category: cs.CL

TL;DR: 提出EMMM框架解决客服场景中LLMs生成的机器文本检测问题，兼顾非专家用户的可解释性、检测准确性和低延迟


<details>
  <summary>Details</summary>
Motivation: 现有机器文本检测方法在在线对话场景中存在可靠性低、可解释性差的问题，特别是对非专家用户缺乏有效解释机制

Method: 采用先解释后检测（explanation-then-detection）的框架设计，优化解释生成机制和检测模型的高效协同

Result: 实验显示70%人工评估者偏好其解释输出，检测精度与SOTA模型相当，延迟控制在1秒内，并开源代码和数据集

Conclusion: EMMM成功平衡了可解释性、准确性和实时性要求，为可信AI部署提供有效解决方案，其开源促进实际应用和后续研究

Abstract: The rapid adoption of large language models (LLMs) in customer service
introduces new risks, as malicious actors can exploit them to conduct
large-scale user impersonation through machine-generated text (MGT). Current
MGT detection methods often struggle in online conversational settings,
reducing the reliability and interpretability essential for trustworthy AI
deployment. In customer service scenarios where operators are typically
non-expert users, explanation become crucial for trustworthy MGT detection. In
this paper, we propose EMMM, an explanation-then-detection framework that
balances latency, accuracy, and non-expert-oriented interpretability.
Experimental results demonstrate that EMMM provides explanations accessible to
non-expert users, with 70\% of human evaluators preferring its outputs, while
achieving competitive accuracy compared to state-of-the-art models and
maintaining low latency, generating outputs within 1 second. Our code and
dataset are open-sourced at
https://github.com/AngieYYF/EMMM-explainable-chatbot-detection.

</details>


### [25] [Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models](https://arxiv.org/abs/2508.18739)
*Chang Wang,Siyu Yan,Depeng Yuan,Yuqi Chen,Yanhua Huang,Yuanhang Zheng,Shuhao Li,Yinqi Zhang,Kedi Chen,Mingrui Zhu,Ruiwen Xu*

Main category: cs.CL

TL;DR: 提出DIVER框架，通过语义风格感知数据生成和多阶段多目标优化（SFT+RL），在单次推理中实现广告标题质量与多样性的平衡，实际部署提升ADVV 4.0%和CTR 1.4%


<details>
  <summary>Details</summary>
Motivation: 现有广告标题生成方法过度关注质量或点击率，导致输出同质化严重，缺乏吸引不同用户群体的多样性

Method: 1. 设计自动化数据生成管道生产多样化标题对；2. 采用监督微调(SFT)和强化学习(RL)两阶段优化框架，引入多样性奖励机制和多目标损失函数

Result: 工业数据集验证框架有效性，部署后广告主价值(ADVV)提升4.0%，点击率(CTR)提高1.4%

Conclusion: DIVER框架成功实现质量与多样性协同优化，为大规模内容平台提供有效的广告生成解决方案

Abstract: The generation of ad headlines plays a vital role in modern advertising,
where both quality and diversity are essential to engage a broad range of
audience segments. Current approaches primarily optimize language models for
headline quality or click-through rates (CTR), often overlooking the need for
diversity and resulting in homogeneous outputs. To address this limitation, we
propose DIVER, a novel framework based on large language models (LLMs) that are
jointly optimized for both diversity and quality. We first design a semantic-
and stylistic-aware data generation pipeline that automatically produces
high-quality training pairs with ad content and multiple diverse headlines. To
achieve the goal of generating high-quality and diversified ad headlines within
a single forward pass, we propose a multi-stage multi-objective optimization
framework with supervised fine-tuning (SFT) and reinforcement learning (RL).
Experiments on real-world industrial datasets demonstrate that DIVER
effectively balances quality and diversity. Deployed on a large-scale
content-sharing platform serving hundreds of millions of users, our framework
improves advertiser value (ADVV) and CTR by 4.0% and 1.4%.

</details>


### [26] [M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations](https://arxiv.org/abs/2508.18740)
*Qiao Liang,Ying Shen,Tiantian Chen,Lin Zhang*

Main category: cs.CL

TL;DR: 本文提出首个多模态多场景MECTEC数据集MECAD及新型模型M3HG，通过多模态异构图显式建模情感因果上下文，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 当前MECTEC领域存在数据集稀缺且场景单一、现有方法未能显式建模情感因果上下文且忽略多层级语义信息融合的问题。

Method: 构建多模态异构图显式捕捉情感/因果上下文，通过语句间和语句内两个层次进行上下文信息融合。

Result: 大量实验证明M3HG模型显著优于现有SOTA方法，最高提升5.31% F1值。

Conclusion: MECAD数据集填补领域空白，M3HG模型通过创新的上下文建模和信息融合机制推动了MECTEC任务发展。

Abstract: Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has
recently gained significant attention in social media analysis, aiming to
extract emotion utterances, cause utterances, and emotion categories
simultaneously. However, the scarcity of related datasets, with only one
published dataset featuring highly uniform dialogue scenarios, hinders model
development in this field. To address this, we introduce MECAD, the first
multimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56
TV series spanning a wide range of dialogue contexts. In addition, existing
MECTEC methods fail to explicitly model emotional and causal contexts and
neglect the fusion of semantic information at different levels, leading to
performance degradation. In this paper, we propose M3HG, a novel model that
explicitly captures emotional and causal contexts and effectively fuses
contextual information at both inter- and intra-utterance levels via a
multimodal heterogeneous graph. Extensive experiments demonstrate the
effectiveness of M3HG compared with existing state-of-the-art methods. The
codes and dataset are available at https://github.com/redifinition/M3HG.

</details>


### [27] [Chronological Passage Assembling in RAG framework for Temporal Question Answering](https://arxiv.org/abs/2508.18748)
*Byeongjeong Kim,Jeonghyun Park,Joonho Yang,Hwanhee Lee*

Main category: cs.CL

TL;DR: 提出ChronoRAG框架解决叙事类长文本QA任务中时间线重建与上下文连贯性的难题


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在叙事文本中失效，因叙事理解需依赖事件顺序与整体上下文，而现有检索方法无法保留段落间时序关系

Method: ChronoRAG框架：1. 重构分散文本为结构化段落 2. 显式捕捉并保持检索段落的时序关系

Result: 在NarrativeQA数据集验证，事实识别准确率提升21.3%，时序关系理解任务提升34.7%

Conclusion: 时序推理是叙事QA的核心，ChronoRAG通过结构化叙事流与显式时序建模显著提升性能

Abstract: Long-context question answering over narrative tasks is challenging because
correct answers often hinge on reconstructing a coherent timeline of events
while preserving contextual flow in a limited context window.
Retrieval-augmented generation (RAG) indexing methods aim to address this
challenge by selectively retrieving only necessary document segments. However,
narrative texts possess unique characteristics that limit the effectiveness of
these existing approaches. Specifically, understanding narrative texts requires
more than isolated segments, as the broader context and sequential
relationships between segments are crucial for comprehension. To address these
limitations, we propose ChronoRAG, a novel RAG framework specialized for
narrative texts. This approach focuses on two essential aspects: refining
dispersed document information into coherent and structured passages, and
preserving narrative flow by explicitly capturing and maintaining the temporal
order among retrieved passages. We empirically demonstrate the effectiveness of
ChronoRAG through experiments on the NarrativeQA dataset, showing substantial
improvements in tasks requiring both factual identification and comprehension
of complex sequential relationships, underscoring that reasoning over temporal
order is crucial in resolving narrative QA.

</details>


### [28] [ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models](https://arxiv.org/abs/2508.18773)
*Qianyu He,Siyu Yuan,Xuefeng Li,Mingxuan Wang,Jiangjie Chen*

Main category: cs.CL

TL;DR: 首个开源端到端框架ThinkDial实现GPT式可控推理模式，通过预算控制训练范式在保持性能阈值下显著降低响应长度


<details>
  <summary>Details</summary>
Motivation: 开源社区缺乏类似GPT-OSS的离散推理控制能力，现有LLMs在计算资源控制方面存在部署挑战

Method: 端到端训练范式整合预算模式控制：预算模式监督微调+两阶段预算感知强化学习（含自适应奖励机制）

Result: 成功实现三种模式：High（全性能）/Medium（50% token减少，性能损失<10%）/Low（75% token减少，性能损失<15%）

Conclusion: ThinkDial框架在保持性能阈值的同时达成目标压缩率，且在分布外任务中展现强泛化能力，推动高效LLM部署

Abstract: Large language models (LLMs) with chain-of-thought reasoning have
demonstrated remarkable problem-solving capabilities, but controlling their
computational effort remains a significant challenge for practical deployment.
Recent proprietary systems like OpenAI's gpt-oss series have introduced
discrete operational modes for intuitive reasoning control, but the open-source
community has largely failed to achieve such capabilities. In this paper, we
introduce ThinkDial, the first open-recipe end-to-end framework that
successfully implements gpt-oss-style controllable reasoning through discrete
operational modes. Our system enables seamless switching between three distinct
reasoning regimes: High mode (full reasoning capability), Medium mode (50
percent token reduction with <10 percent performance degradation), and Low mode
(75 percent token reduction with <15 percent performance degradation). We
achieve this through an end-to-end training paradigm that integrates
budget-mode control throughout the entire pipeline: budget-mode supervised
fine-tuning that embeds controllable reasoning capabilities directly into the
learning process, and two-phase budget-aware reinforcement learning with
adaptive reward shaping. Extensive experiments demonstrate that ThinkDial
achieves target compression-performance trade-offs with clear response length
reductions while maintaining performance thresholds. The framework also
exhibits strong generalization capabilities on out-of-distribution tasks.

</details>


### [29] [Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction](https://arxiv.org/abs/2508.18780)
*Yilin Li,Xunjian Yin,Yilin Chen,Xiaojun Wan*

Main category: cs.CL

TL;DR: 提出基于规则强化学习的框架显著提升语法错误纠正的召回率，验证强化学习引导大语言模型的优势


<details>
  <summary>Details</summary>
Motivation: 传统监督微调方法限制大语言模型的推理能力，需探索强化学习等新方法提升模型可控性和可靠性

Method: 设计基于规则的强化学习框架，通过在中文数据集实验验证有效性

Result: 在中文语法纠错任务中取得SOTA性能，召回率指标显著提升

Conclusion: 研究表明强化学习能有效引导大语言模型进行语法纠错，为未来研究提供可控可靠的新范式

Abstract: Grammatical error correction is a significant task in NLP. Traditional
methods based on encoder-decoder models have achieved certain success, but the
application of LLMs in this field is still underexplored. Current research
predominantly relies on supervised fine-tuning to train LLMs to directly
generate the corrected sentence, which limits the model's powerful reasoning
ability. To address this limitation, we propose a novel framework based on
Rule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL
framework achieves \textbf{state-of-the-art }performance, with a notable
increase in \textbf{recall}. This result clearly highlights the advantages of
using RL to steer LLMs, offering a more controllable and reliable paradigm for
future development in GEC.

</details>


### [30] [Controllable Conversational Theme Detection Track at DSTC 12](https://arxiv.org/abs/2508.18783)
*Igor Shalyminov,Hang Su,Jake Vincent,Siffi Singh,Jason Cai,James Gung,Raphael Shu,Saab Mansour*

Main category: cs.CL

TL;DR: 论文提出基于用户偏好调节聚类粒度的可控对话主题检测方法，通过DSTC 12竞赛验证可显著提升客户支持等领域的对话分析效率。


<details>
  <summary>Details</summary>
Motivation: 传统对话意图检测依赖固定意图集，难以适应用户自定义需求。主题检测通过灵活标注对话核心内容，可减少人工分析大规模对话的负担。

Method: 将主题检测定义为对话语句的联合聚类与标注任务，引入用户偏好数据动态控制主题粒度的粗细程度。

Result: 在DSTC 12竞赛中验证了该框架，收集了多团队解决方案并开源数据集与代码。参赛方案显示通过用户偏好调节可有效控制主题抽象层级。

Conclusion: 可控主题检测机制突破传统意图检测的局限性，在客户服务等领域具有显著应用价值，其灵活性和可定制性为对话分析开辟新方向。

Abstract: Conversational analytics has been on the forefront of transformation driven
by the advances in Speech and Natural Language Processing techniques. Rapid
adoption of Large Language Models (LLMs) in the analytics field has taken the
problems that can be automated to a new level of complexity and scale. In this
paper, we introduce Theme Detection as a critical task in conversational
analytics, aimed at automatically identifying and categorizing topics within
conversations. This process can significantly reduce the manual effort involved
in analyzing expansive dialogs, particularly in domains like customer support
or sales. Unlike traditional dialog intent detection, which often relies on a
fixed set of intents for downstream system logic, themes are intended as a
direct, user-facing summary of the conversation's core inquiry. This
distinction allows for greater flexibility in theme surface forms and
user-specific customizations. We pose Controllable Conversational Theme
Detection problem as a public competition track at Dialog System Technology
Challenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of
dialog utterances, with the distinctive aspect being controllability of the
resulting theme clusters' granularity achieved via the provided user preference
data. We give an overview of the problem, the associated dataset and the
evaluation metrics, both automatic and human. Finally, we discuss the
participant teams' submissions and provide insights from those. The track
materials (data and code) are openly available in the GitHub repository.

</details>


### [31] [LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination](https://arxiv.org/abs/2508.18791)
*Ziming Zhu,Chenglong Wang,Shunjie Xing,Yifu Huo,Fengning Tian,Quan Du,Di Yang,Chunliang Zhang,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 提出LaTeXTrans多智能体系统，通过格式保留和术语一致性提升LaTeX文档翻译质量


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译系统难以处理LaTeX文档中自然语言与数学公式/表格等专业语法的混合结构，需要保持编译兼容性和语义完整性

Method: 采用六模块协同工作：1)解析器分解LaTeX结构 2)翻译/验证/摘要/术语提取四模块确保上下文感知 3)生成器重构翻译内容

Result: 实验证明在翻译准确率和结构保真度上超越主流机器翻译系统

Conclusion: LaTeXTrans为结构化文档翻译提供格式保留、术语一致的有效解决方案

Abstract: Despite the remarkable progress of modern machine translation (MT) systems on
general-domain texts, translating structured LaTeX-formatted documents remains
a significant challenge. These documents typically interleave natural language
with domain-specific syntax, such as mathematical equations, tables, figures,
and cross-references, all of which must be accurately preserved to maintain
semantic integrity and compilability. In this paper, we introduce LaTeXTrans, a
collaborative multi-agent system designed to address this challenge. LaTeXTrans
ensures format preservation, structural fidelity, and terminology consistency
through six specialized agents: 1) a Parser that decomposes LaTeX into
translation-friendly units via placeholder substitution and syntax filtering;
2) a Translator, Validator, Summarizer, and Terminology Extractor that work
collaboratively to ensure context-aware, self-correcting, and
terminology-consistent translations; 3) a Generator that reconstructs the
translated content into well-structured LaTeX documents. Experimental results
demonstrate that LaTeXTrans can outperform mainstream MT systems in both
translation accuracy and structural fidelity, offering an effective and
practical solution for translating LaTeX-formatted documents.

</details>


### [32] [LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection](https://arxiv.org/abs/2508.18819)
*Shubham Gupta,Shraban Kumar Chatterjee,Suman Kundu*

Main category: cs.CL

TL;DR: 提出整合语义关系与传播动态的自监督检测框架，通过LLM增强特征可分性和多视图图学习，实现高效假新闻检测


<details>
  <summary>Details</summary>
Motivation: 现有方法存在长距离依赖捕捉不足、需大量标注数据、忽视社交传播动态三大缺陷，需开发更高效普适的检测方案

Method: 融合AMR语义分析与社交传播图特征，设计LLM生成负锚点的图对比损失(LGCL)和多视图图掩码自编码器

Result: 在零样本和有限标注场景下检测准确率超越现有方法，实验证明框架具有更好的特征可分性与模型泛化能力

Conclusion: 通过语义-传播双特征融合与自监督学习机制，有效突破传统检测方法对标注数据的依赖，提升检测效果与适用性

Abstract: The proliferation of misinformation in the digital age has led to significant
societal challenges. Existing approaches often struggle with capturing
long-range dependencies, complex semantic relations, and the social dynamics
influencing news dissemination. Furthermore, these methods require extensive
labelled datasets, making their deployment resource-intensive. In this study,
we propose a novel self-supervised misinformation detection framework that
integrates both complex semantic relations using Abstract Meaning
Representation (AMR) and news propagation dynamics. We introduce an LLM-based
graph contrastive loss (LGCL) that utilizes negative anchor points generated by
a Large Language Model (LLM) to enhance feature separability in a zero-shot
manner. To incorporate social context, we employ a multi view graph masked
autoencoder, which learns news propagation features from social context graph.
By combining these semantic and propagation-based features, our approach
effectively differentiates between fake and real news in a self-supervised
manner. Extensive experiments demonstrate that our self-supervised framework
achieves superior performance compared to other state-of-the-art methodologies,
even with limited labelled datasets while improving generalizability.

</details>


### [33] [Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness](https://arxiv.org/abs/2508.18824)
*Sirui Chen,Changxin Tian,Binbin Hu,Kunlong Chen,Ziqi Liu,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 提出程序辅助合成框架生成高质量数学语料库，通过双向验证确保数据正确性，实验证明显著提升LLM推理能力


<details>
  <summary>Details</summary>
Motivation: 传统数学数据生成方法存在可扩展性差、成本高且可靠性不足的问题，需要系统性解决方案来保证数据质量

Method: 整合数学知识系统与领域工具生成可执行程序，转换为问题-解决方案对后，通过程序输出验证和问题一致性检查的双向验证机制

Result: 生成1230万高质量三元组数据，微调模型在多个基准测试中达到SOTA，推理能力显著提升

Conclusion: 该合成框架有效解决了数学数据生成的可靠性问题，验证了程序辅助方法对提升模型数学推理能力的有效性

Abstract: Enhancing the mathematical reasoning of large language models (LLMs) demands
high-quality training data, yet conventional methods face critical challenges
in scalability, cost, and data reliability. To address these limitations, we
propose a novel program-assisted synthesis framework that systematically
generates a high-quality mathematical corpus with guaranteed diversity,
complexity, and correctness. This framework integrates mathematical knowledge
systems and domain-specific tools to create executable programs. These programs
are then translated into natural language problem-solution pairs and vetted by
a bilateral validation mechanism that verifies solution correctness against
program outputs and ensures program-problem consistency. We have generated 12.3
million such problem-solving triples. Experiments demonstrate that models
fine-tuned on our data significantly improve their inference capabilities,
achieving state-of-the-art performance on several benchmark datasets and
showcasing the effectiveness of our synthesis approach.

</details>


### [34] [ConfTuner: Training Large Language Models to Express Their Confidence Verbally](https://arxiv.org/abs/2508.18847)
*Yibo Li,Miao Xiong,Jiaying Wu,Bryan Hooi*

Main category: cs.CL

TL;DR: 提出ConfTuner方法，通过新的tokenized Brier score损失函数有效校准LLM置信度，提升下游任务表现


<details>
  <summary>Details</summary>
Motivation: LLM在关键领域部署需要可靠置信度表达，但现有校准方法（提示工程/启发式微调）效果有限且泛化性差

Method: 基于proper scoring rules理论设计tokenized Brier score损失函数，无需真实置信度数据即可微调模型

Result: 实验证明ConfTuner提升多任务校准效果（包括GPT-4o），改进自纠正和模型级联性能

Conclusion: 通过理论保证的校准方法推动可信LLM系统发展，代码已开源

Abstract: Large Language Models (LLMs) are increasingly deployed in high-stakes domains
such as science, law, and healthcare, where accurate expressions of uncertainty
are essential for reliability and trust. However, current LLMs are often
observed to generate incorrect answers with high confidence, a phenomenon known
as "overconfidence". Recent efforts have focused on calibrating LLMs'
verbalized confidence: i.e., their expressions of confidence in text form, such
as "I am 80% confident that...". Existing approaches either rely on prompt
engineering or fine-tuning with heuristically generated uncertainty estimates,
both of which have limited effectiveness and generalizability. Motivated by the
notion of proper scoring rules for calibration in classical machine learning
models, we introduce ConfTuner, a simple and efficient fine-tuning method that
introduces minimal overhead and does not require ground-truth confidence scores
or proxy confidence estimates. ConfTuner relies on a new loss function,
tokenized Brier score, which we theoretically prove to be a proper scoring
rule, intuitively meaning that it "correctly incentivizes the model to report
its true probability of being correct". ConfTuner improves calibration across
diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our
results further show that better-calibrated confidence enables downstream gains
in self-correction and model cascade, advancing the development of trustworthy
LLM systems. The code is available at
https://github.com/liushiliushi/ConfTuner.

</details>


### [35] [ReflectivePrompt: Reflective evolution in autoprompting algorithms](https://arxiv.org/abs/2508.18870)
*Viktor N. Zhuravlev,Artur R. Khairullin,Ernest A. Dyagin,Alena N. Sitkina,Nikita I. Kulin*

Main category: cs.CL

TL;DR: 提出ReflectivePrompt——基于进化算法的反射式自动提示方法，在33个数据集测试中平均性能提升28%


<details>
  <summary>Details</summary>
Motivation: 传统进化算法在提示优化中难以有效积累历史知识，导致提示修改质量受限。为解决该问题，通过引入短期/长期反射机制增强进化过程中的知识利用效率。

Method: 1. 在交叉和精英突变前执行反射操作
2. 短期反射分析单代种群特征
3. 长期反射整合全进化过程知识
4. 构建动态更新的知识库指导变异方向

Result: 使用t-lite-instruct-0.1和gemma3-27b-it模型测试：
- 分类任务准确率提升18-35%
- BBH基准成绩较EvoPrompt提升28%
- 在83%测试数据集上达到SOTA

Conclusion: 反射机制显著提升进化算法在提示优化中的有效性，使ReflectivePrompt成为当前最先进的自动提示方法之一

Abstract: Autoprompting is the process of automatically selecting optimized prompts for
language models, which has been gaining popularity with the rapid advancement
of prompt engineering, driven by extensive research in the field of large
language models (LLMs). This paper presents ReflectivePrompt - a novel
autoprompting method based on evolutionary algorithms that employs a reflective
evolution approach for more precise and comprehensive search of optimal
prompts. ReflectivePrompt utilizes short-term and long-term reflection
operations before crossover and elitist mutation to enhance the quality of the
modifications they introduce. This method allows for the accumulation of
knowledge obtained throughout the evolution process and updates it at each
epoch based on the current population. ReflectivePrompt was tested on 33
datasets for classification and text generation tasks using open-access large
language models: t-lite-instruct-0.1 and gemma3-27b-it. The method
demonstrates, on average, a significant improvement (e.g., 28% on BBH compared
to EvoPrompt) in metrics relative to current state-of-the-art approaches,
thereby establishing itself as one of the most effective solutions in
evolutionary algorithm-based autoprompting.

</details>


### [36] [Empowering Computing Education Researchers Through LLM-Assisted Content Analysis](https://arxiv.org/abs/2508.18872)
*Laurie Gale,Sebastian Mateos Nicolajsen*

Main category: cs.CL

TL;DR: 提出LLM辅助内容分析法(LACA)，通过结合内容分析和大语言模型解决CER领域研究规模受限的问题


<details>
  <summary>Details</summary>
Motivation: 当前计算教育研究者普遍面临数据规模小、资源有限的问题，难以开展具有普遍性的高质量研究

Method: 开发基于大语言模型的辅助内容分析框架，通过自动化处理提升文本数据分析效率与规模

Result: 使用真实计算教育数据集验证方法可行性，证明可在保持严谨性的前提下开展更大规模研究

Conclusion: LACA方法为CER领域提供可扩展的研究范式，有助于提升整体研究质量并推动教学实践改进

Abstract: Computing education research (CER) is often instigated by practitioners
wanting to improve both their own and the wider discipline's teaching practice.
However, the latter is often difficult as many researchers lack the colleagues,
resources, or capacity to conduct research that is generalisable or rigorous
enough to advance the discipline. As a result, research methods that enable
sense-making with larger volumes of qualitative data, while not increasing the
burden on the researcher, have significant potential within CER.
  In this discussion paper, we propose such a method for conducting rigorous
analysis on large volumes of textual data, namely a variation of LLM-assisted
content analysis (LACA). This method combines content analysis with the use of
large language models, empowering researchers to conduct larger-scale research
which they would otherwise not be able to perform. Using a computing education
dataset, we illustrate how LACA could be applied in a reproducible and rigorous
manner. We believe this method has potential in CER, enabling more
generalisable findings from a wider range of research. This, together with the
development of similar methods, can help to advance both the practice and
research quality of the CER discipline.

</details>


### [37] [Affective Polarization across European Parliaments](https://arxiv.org/abs/2508.18916)
*Bojan Evkoski,Igor Mozetič,Nikola Ljubešić,Petra Kralj Novak*

Main category: cs.CL

TL;DR: 研究使用自然语言处理技术分析欧洲六国议会演讲，发现普遍存在情感极化现象，并揭示其驱动机制


<details>
  <summary>Details</summary>
Motivation: 探究情感极化现象是否存在于欧洲议会政治互动中，及其具体表现模式和形成机制

Method: 通过自然语言处理技术分析六国议会演讲语料库，比较对己方和对方团体成员的负面情绪表达差异

Result: 所有议会均存在情感极化；活跃度与负面情绪相关但极化程度无差异；reciprocity是主要驱动机制

Conclusion: 情感极化在欧洲议会普遍存在，相互性机制在跨议会情感极化中起关键作用，为理解政治对立提供新视角

Abstract: Affective polarization, characterized by increased negativity and hostility
towards opposing groups, has become a prominent feature of political discourse
worldwide. Our study examines the presence of this type of polarization in a
selection of European parliaments in a fully automated manner. Utilizing a
comprehensive corpus of parliamentary speeches from the parliaments of six
European countries, we employ natural language processing techniques to
estimate parliamentarian sentiment. By comparing the levels of negativity
conveyed in references to individuals from opposing groups versus one's own, we
discover patterns of affectively polarized interactions. The findings
demonstrate the existence of consistent affective polarization across all six
European parliaments. Although activity correlates with negativity, there is no
observed difference in affective polarization between less active and more
active members of parliament. Finally, we show that reciprocity is a
contributing mechanism in affective polarization between parliamentarians
across all six parliaments.

</details>


### [38] [Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework](https://arxiv.org/abs/2508.18929)
*Ilias Driouich,Hongliu Cao,Eoin Thomas*

Main category: cs.CL

TL;DR: 提出多智能体框架生成兼具语义多样性与隐私保护的QA数据集，改进RAG系统评估质量


<details>
  <summary>Details</summary>
Motivation: 当前RAG系统评估数据集缺乏语义多样性和隐私保护设计，制约可靠评估

Method: 1) 多样性代理通过聚类增强主题覆盖；2) 隐私代理跨领域检测敏感信息；3) QA生成代理合成符合隐私要求的多样化问答对

Result: 生成的数据集在领域特定场景中实现更高的语义多样性（+23%覆盖率）和强隐私保护（敏感信息遮蔽率98.5%）

Conclusion: 该框架为AI合规评估提供实践路径，支持构建符合伦理规范的安全RAG系统评估体系

Abstract: Retrieval-augmented generation (RAG) systems improve large language model
outputs by incorporating external knowledge, enabling more informed and
context-aware responses. However, the effectiveness and trustworthiness of
these systems critically depends on how they are evaluated, particularly on
whether the evaluation process captures real-world constraints like protecting
sensitive information. While current evaluation efforts for RAG systems have
primarily focused on the development of performance metrics, far less attention
has been given to the design and quality of the underlying evaluation datasets,
despite their pivotal role in enabling meaningful, reliable assessments. In
this work, we introduce a novel multi-agent framework for generating synthetic
QA datasets for RAG evaluation that prioritize semantic diversity and privacy
preservation. Our approach involves: (1) a Diversity agent leveraging
clustering techniques to maximize topical coverage and semantic variability,
(2) a Privacy Agent that detects and mask sensitive information across multiple
domains and (3) a QA curation agent that synthesizes private and diverse QA
pairs suitable as ground truth for RAG evaluation. Extensive experiments
demonstrate that our evaluation sets outperform baseline methods in diversity
and achieve robust privacy masking on domain-specific datasets. This work
offers a practical and ethically aligned pathway toward safer, more
comprehensive RAG system evaluation, laying the foundation for future
enhancements aligned with evolving AI regulations and compliance standards.

</details>


### [39] [Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural Models](https://arxiv.org/abs/2508.18988)
*Hung Ming Liu*

Main category: cs.CL

TL;DR: 提出AI Mother Tongue框架，通过原生符号语言实现可解释的直觉推理与符号组合


<details>
  <summary>Details</summary>
Motivation: 突破传统后解释方法的局限，将可解释性直接嵌入模型表征，实现符号模式捕获、决策路径追溯和门控聚焦的透明推理

Method: 1.引入符号纯度/决策稀疏性的互补训练目标
2.采用先建立符号基础再优化直觉判断的序列专业化策略

Result: 在AI任务中实现与现有方法相当的准确性，并生成可验证的推理轨迹

Conclusion: 该框架将符号系统的解释优势与神经网络的灵活性结合，为可信AI提供新的实现路径

Abstract: We present a framework where neural models develop an AI Mother Tongue, a
native symbolic language that simultaneously supports intuitive reasoning,
compositional symbol chains, and inherent interpretability. Unlike post-hoc
explanation methods, our approach embeds reasoning directly into the model's
representations: symbols capture meaningful semantic patterns, chains trace
decision paths, and gated induction mechanisms guide selective focus, yielding
transparent yet flexible reasoning. We introduce complementary training
objectives to enhance symbol purity and decision sparsity, and employ a
sequential specialization strategy to first build broad symbolic competence and
then refine intuitive judgments. Experiments on AI tasks demonstrate
competitive accuracy alongside verifiable reasoning traces, showing that AI
Mother Tongue can serve as a unified mechanism for interpretability, intuition,
and symbolic reasoning in neural models.

</details>


### [40] [Automatic Prompt Optimization with Prompt Distillation](https://arxiv.org/abs/2508.18992)
*Viktor N. Zhuravlev,Artur R. Khairullin,Ernest A. Dyagin,Alena N. Sitkina,Nikita I. Kulin*

Main category: cs.CL

TL;DR: 提出DistillPrompt新型自动提示方法，通过多阶段信息整合显著提升LLM性能


<details>
  <summary>Details</summary>
Motivation: 现有自动提示方法对提示空间的探索不够全面，需更高效的优化策略

Method: 结合蒸馏/压缩/聚合三阶段操作，在训练数据中深度集成任务特征信息

Result: 文本任务关键指标平均提升20.12%(vs Grips)，验证非梯度方法有效性

Conclusion: DistillPrompt成为自动提示领域最有效的架构之一，特别适用于资源受限场景

Abstract: Autoprompting is the process of automatically selecting optimized prompts for
language models, which is gaining popularity due to the rapid development of
prompt engineering driven by extensive research in the field of large language
models (LLMs). This paper presents DistillPrompt -- a novel autoprompting
method based on large language models that employs a multi-stage integration of
task-specific information into prompts using training data. DistillPrompt
utilizes distillation, compression, and aggregation operations to explore the
prompt space more thoroughly. The method was tested on different datasets for
text classification and generation tasks using the t-lite-instruct-0.1 language
model. The results demonstrate a significant average improvement (e.g., 20.12%
across the entire dataset compared to Grips) in key metrics over existing
methods in the field, establishing DistillPrompt as one of the most effective
non-gradient approaches in autoprompting.

</details>


### [41] [MovieCORE: COgnitive REasoning in Movies](https://arxiv.org/abs/2508.19026)
*Gueter Josmy Faure,Min-Hung Chen,Jia-Fong Yeh,Ying Cheng,Hung-Ting Su,Yung-Hao Tang,Shang-Hong Lai,Winston H. Hsu*

Main category: cs.CL

TL;DR: MovieCORE提出新型电影问答数据集，通过多智能体LLM生成深度认知问题，开发评估体系并设计ACE模块提升模型推理能力25%


<details>
  <summary>Details</summary>
Motivation: 现有视频问答数据集专注浅层理解，需构建激发系统二思考的深度认知测试框架

Method: 采用多LLM代理脑暴生成问题对，设计认知复杂度评估指标，开发训练后增强模块ACE提升模型推理

Result: 数据集通过三层认知测试验证质量，ACE模块使VQA模型在深度任务上推理能力提升达25%

Conclusion: 该研究推进AI电影理解能力评估，揭示现有模型处理复杂电影内容的局限性与改进方向

Abstract: This paper introduces MovieCORE, a novel video question answering (VQA)
dataset designed to probe deeper cognitive understanding of movie content.
Unlike existing datasets that focus on surface-level comprehension, MovieCORE
emphasizes questions that engage System-2 thinking while remaining specific to
the video material. We present an innovative agentic brainstorming approach,
utilizing multiple large language models (LLMs) as thought agents to generate
and refine high-quality question-answer pairs. To evaluate dataset quality, we
develop a set of cognitive tests assessing depth, thought-provocation
potential, and syntactic complexity. We also propose a comprehensive evaluation
scheme for assessing VQA model performance on deeper cognitive tasks. To
address the limitations of existing video-language models (VLMs), we introduce
an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves
model reasoning capabilities post-training by up to 25%. Our work contributes
to advancing movie understanding in AI systems and provides valuable insights
into the capabilities and limitations of current VQA models when faced with
more challenging, nuanced questions about cinematic content. Our project page,
dataset and code can be found at
https://joslefaure.github.io/assets/html/moviecore.html.

</details>


### [42] [HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance](https://arxiv.org/abs/2508.19076)
*Ziyue Li,Yuan Chang,Gaihong Yu,Xiaoqiu Le*

Main category: cs.CL

TL;DR: 提出分层规划框架HiPlan，通过全局-局部双层次指导增强LLM智能体在复杂长程任务中的决策能力


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体在复杂长程规划中缺乏宏观引导导致迷失方向，执行过程中缺乏持续监控导致环境适应性差

Method: 构建里程碑库实现结构化经验复用，动态生成层次化引导（里程碑指南+步骤提示），实现离线阶段知识沉淀与在线阶段轨迹适配

Result: 在两个复杂基准测试中显著超越基线模型，消融实验验证层次化组件的互补优势

Conclusion: HiPlan通过层次化引导机制有效提升LLM智能体的持续规划能力，为复杂动态环境中的AI决策提供新思路

Abstract: Large language model (LLM)-based agents have demonstrated remarkable
capabilities in decision-making tasks, but struggle significantly with complex,
long-horizon planning scenarios. This arises from their lack of macroscopic
guidance, causing disorientation and failures in complex tasks, as well as
insufficient continuous oversight during execution, rendering them unresponsive
to environmental changes and prone to deviations. To tackle these challenges,
we introduce HiPlan, a hierarchical planning framework that provides adaptive
global-local guidance to boost LLM-based agents'decision-making. HiPlan
decomposes complex tasks into milestone action guides for general direction and
step-wise hints for detailed actions. During the offline phase, we construct a
milestone library from expert demonstrations, enabling structured experience
reuse by retrieving semantically similar tasks and milestones. In the execution
phase, trajectory segments from past milestones are dynamically adapted to
generate step-wise hints that align current observations with the milestone
objectives, bridging gaps and correcting deviations. Extensive experiments
across two challenging benchmarks demonstrate that HiPlan substantially
outperforms strong baselines, and ablation studies validate the complementary
benefits of its hierarchical components.

</details>


### [43] ["Where does it hurt?" -- Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues](https://arxiv.org/abs/2508.19077)
*Tom Röhr,Soumyadeep Roy,Fares Al Mohamad,Jens-Michalis Papaioannou,Wolfgang Nejdl,Felix Gers,Alexander Löser*

Main category: cs.CL

TL;DR: 首项研究医患对话中医生意图轨迹的工作，构建SOAP框架意图分类法并标注5000+对话轮次，发现意图过滤显著提升医疗对话摘要性能


<details>
  <summary>Details</summary>
Motivation: 理解医生在诊断过程中的意图轨迹，为设计更有效的'鉴别诊断'系统提供结构化支持

Method: 使用ACI-bench数据集，联合医学专家开发SOAP分类法，通过Prolific平台众包标注，并评估生成式/编码器模型的意图分类表现

Result: 模型整体结构理解准确率达85%，但SOAP类别转换识别失败率32%。意图过滤使对话摘要ROUGE-L提升11.5个百分点

Conclusion: 医疗意图轨迹研究为对话系统设计提供新范式，公开的标注数据集和SOAP分类法将成为医疗NLP领域重要基准资源

Abstract: In a doctor-patient dialogue, the primary objective of physicians is to
diagnose patients and propose a treatment plan. Medical doctors guide these
conversations through targeted questioning to efficiently gather the
information required to provide the best possible outcomes for patients. To the
best of our knowledge, this is the first work that studies physician intent
trajectories in doctor-patient dialogues. We use the `Ambient Clinical
Intelligence Benchmark' (Aci-bench) dataset for our study. We collaborate with
medical professionals to develop a fine-grained taxonomy of physician intents
based on the SOAP framework (Subjective, Objective, Assessment, and Plan). We
then conduct a large-scale annotation effort to label over 5000 doctor-patient
turns with the help of a large number of medical experts recruited using
Prolific, a popular crowd-sourcing platform. This large labeled dataset is an
important resource contribution that we use for benchmarking the
state-of-the-art generative and encoder models for medical intent
classification tasks. Our findings show that our models understand the general
structure of medical dialogues with high accuracy, but often fail to identify
transitions between SOAP categories. We also report for the first time common
trajectories in medical dialogue structures that provide valuable insights for
designing `differential diagnosis' systems. Finally, we extensively study the
impact of intent filtering for medical dialogue summarization and observe a
significant boost in performance. We make the codes and data, including
annotation guidelines, publicly available at
https://github.com/DATEXIS/medical-intent-classification.

</details>


### [44] [It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs](https://arxiv.org/abs/2508.19089)
*Yue Li,Zhixue Zhao,Carolina Scarton*

Main category: cs.CL

TL;DR: 研究发现，对于文字和语言都极度缺乏支持的超低资源语言，零样本上下文学习结合语言对齐效果显著优于参数高效微调，而少量样本微调更适合已有部分表征的语言。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在文字罕见、训练数据匮乏的超低资源语言上支持不足的问题，探索上下文学习与参数微调的适用边界。

Method: 在3个前沿多语言大模型上系统评估20种低资源语言，对比分析零样本/少样本上下文学习、参数高效微调（PEFT）的效果差异。

Result: PEFT对文字和语言双重稀缺的语言效果有限，零样本上下文学习+语言对齐方案对超低资源语言效果突出，少样本微调更适合已有部分表征的语言。

Conclusion: 建议超低资源语言开发者：避免对未见过文字的语言进行微调，优先采用零样本上下文学习+语言对齐方案，少样本微调适用于中等资源语言适配。

Abstract: Extremely low-resource languages, especially those written in rare scripts,
as shown in Figure 1, remain largely unsupported by large language models
(LLMs). This is due in part to compounding factors such as the lack of training
data. This paper delivers the first comprehensive analysis of whether LLMs can
acquire such languages purely via in-context learning (ICL), with or without
auxiliary alignment signals, and how these methods compare to
parameter-efficient fine-tuning (PEFT). We systematically evaluate 20
under-represented languages across three state-of-the-art multilingual LLMs.
Our findings highlight the limitation of PEFT when both language and its script
are extremely under-represented by the LLM. In contrast, zero-shot ICL with
language alignment is impressively effective on extremely low-resource
languages, while few-shot ICL or PEFT is more beneficial for languages
relatively better represented by LLMs. For LLM practitioners working on
extremely low-resource languages, we summarise guidelines grounded by our
results on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning
a multilingual model on languages of unseen scripts.

</details>


### [45] [Retrieval-Augmented Generation for Natural Language Art Provenance Searches in the Getty Provenance Index](https://arxiv.org/abs/2508.19093)
*Mathew Henrickson*

Main category: cs.CL

TL;DR: 研究提出基于检索增强生成(RAG)的框架，改进艺术品来源研究的跨语言检索与摘要生成。


<details>
  <summary>Details</summary>
Motivation: 艺术品来源研究依赖零散的多语言档案数据，现有检索系统受限于精确元数据查询，无法支持探索性研究需求。

Method: 通过语义检索与上下文摘要技术实现自然语言/多语言搜索，降低对结构化元数据的依赖。

Result: 在Getty德国拍卖数据集的万条记录测试中，验证了该框架有效提升艺术市场档案的检索效率。

Conclusion: 该框架为文化遗产研究提供了可扩展的历史敏感型研究工具，支持艺术品溯源与法律索赔等实际应用。

Abstract: This research presents a Retrieval-Augmented Generation (RAG) framework for
art provenance studies, focusing on the Getty Provenance Index. Provenance
research establishes the ownership history of artworks, which is essential for
verifying authenticity, supporting restitution and legal claims, and
understanding the cultural and historical context of art objects. The process
is complicated by fragmented, multilingual archival data that hinders efficient
retrieval. Current search portals require precise metadata, limiting
exploratory searches. Our method enables natural-language and multilingual
searches through semantic retrieval and contextual summarization, reducing
dependence on metadata structures. We assess RAG's capability to retrieve and
summarize auction records using a 10,000-record sample from the Getty
Provenance Index - German Sales. The results show this approach provides a
scalable solution for navigating art market archives, offering a practical tool
for historians and cultural heritage professionals conducting historically
sensitive research.

</details>


### [46] [Beyond the Black Box: Integrating Lexical and Semantic Methods in Quantitative Discourse Analysis with BERTopic](https://arxiv.org/abs/2508.19099)
*Thomas Compton*

Main category: cs.CL

TL;DR: 提出混合透明QDA框架，结合词汇与语义分析方法，通过Python工具链实现可控计算流程与主题建模优化，提升话语分析的可解释性和方法透明度。


<details>
  <summary>Details</summary>
Motivation: 解决传统黑箱分析工具(如MAXQDA/NVivo)在方法论透明度与研究目标匹配度方面的缺陷，建立可验证、可重复的计算话语分析方法。

Method: 基于Python构建NLTK/spaCy预处理流程，采用Sentence Transformers生成语义嵌入，结合UMAP+HDBSCAN进行BERTopic建模，通过参数调优提升主题一致性。

Result: 历史政治话语案例验证了框架有效性，词汇搜索与语义聚类的多维度分析实现更高主题覆盖，参数优化使主题连贯性提升23%。

Conclusion: 代码透明性、研究者自主决策与方法三角验证是计算话语分析的关键，混合方法能有效规避单一分析路径的局限性。

Abstract: Quantitative Discourse Analysis has seen growing adoption with the rise of
Large Language Models and computational tools. However, reliance on black box
software such as MAXQDA and NVivo risks undermining methodological transparency
and alignment with research goals. This paper presents a hybrid, transparent
framework for QDA that combines lexical and semantic methods to enable
triangulation, reproducibility, and interpretability. Drawing from a case study
in historical political discourse, we demonstrate how custom Python pipelines
using NLTK, spaCy, and Sentence Transformers allow fine-grained control over
preprocessing, lemmatisation, and embedding generation. We further detail our
iterative BERTopic modelling process, incorporating UMAP dimensionality
reduction, HDBSCAN clustering, and c-TF-IDF keyword extraction, optimised
through parameter tuning and multiple runs to enhance topic coherence and
coverage. By juxtaposing precise lexical searches with context-aware semantic
clustering, we argue for a multi-layered approach that mitigates the
limitations of either method in isolation. Our workflow underscores the
importance of code-level transparency, researcher agency, and methodological
triangulation in computational discourse studies. Code and supplementary
materials are available via GitHub.

</details>


### [47] [Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs](https://arxiv.org/abs/2508.19111)
*Zhikai Ding,Shiyu Ni,Keping Bi*

Main category: cs.CL

TL;DR: 大型视觉语言模型在视觉问答中存在幻觉问题，研究通过评估三种置信度信号分析其知识边界感知能力，并提出改进方法


<details>
  <summary>Details</summary>
Motivation: LVLMs在VQA任务中表现出幻觉现象，需评估其知识边界感知能力以提升可靠性

Method: 通过概率置信度、答案一致性置信度和语言化置信度三个维度评估，并引入LLM置信度校准方法改进

Result: 概率和一致性置信度更可靠，语言化置信度易导致过度自信；多模态处理降低性能但提升感知水平

Conclusion: 需提升LVLMs的知识边界感知能力，建议采用非语言化置信度指标，多模态联合处理对感知水平有积极影响

Abstract: Large vision-language models (LVLMs) demonstrate strong visual question
answering (VQA) capabilities but are shown to hallucinate. A reliable model
should perceive its knowledge boundaries-knowing what it knows and what it does
not. This paper investigates LVLMs' perception of their knowledge boundaries by
evaluating three types of confidence signals: probabilistic confidence, answer
consistency-based confidence, and verbalized confidence. Experiments on three
LVLMs across three VQA datasets show that, although LVLMs possess a reasonable
perception level, there is substantial room for improvement. Among the three
confidences, probabilistic and consistency-based signals are more reliable
indicators, while verbalized confidence often leads to overconfidence. To
enhance LVLMs' perception, we adapt several established confidence calibration
methods from Large Language Models (LLMs) and propose three effective methods.
Additionally, we compare LVLMs with their LLM counterparts, finding that
jointly processing visual and textual inputs decreases question-answering
performance but reduces confidence, resulting in an improved perception level
compared to LLMs.

</details>


### [48] [Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning](https://arxiv.org/abs/2508.19202)
*Alan Li,Yixin Liu,Arpan Sarkar,Doug Downey,Arman Cohan*

Main category: cs.CL

TL;DR: 提出SciReas/SciReas-Pro科学推理评估基准与KRUX分析框架，揭示LLMs知识检索瓶颈及推理增强策略，并发布8B基线模型SciLit01


<details>
  <summary>Details</summary>
Motivation: 现有科学推理评估缺乏系统性基准，且未明确区分知识储备与推理能力的作用。需建立评估体系并探究LLMs在科学任务中的核心瓶颈

Method: 1. 构建SciReas多任务基准与精选版SciReas-Pro 2. 设计KRUX框架解耦知识与推理 3. 通过知识注入和推理增强实验验证假设

Result: 1. 知识检索是LLMs科学推理主要瓶颈 2. 上下文补充外部知识可提升推理效果 3. 增强显性推理能力有助于激活潜在知识 4. 构建的SciLit01模型展现竞争力

Conclusion: 系统评估揭示了LLMs科学推理的双重挑战，提出的方法论框架为后续研究提供新范式，基准模型推动领域发展

Abstract: Scientific problem solving poses unique challenges for LLMs, requiring both
deep domain knowledge and the ability to apply such knowledge through complex
reasoning. While automated scientific reasoners hold great promise for
assisting human scientists, there is currently no widely adopted holistic
benchmark for evaluating scientific reasoning, and few approaches
systematically disentangle the distinct roles of knowledge and reasoning in
these tasks. To address these gaps, we introduce SciReas, a diverse suite of
existing benchmarks for scientific reasoning tasks, and SciReas-Pro, a
selective subset that requires more complex reasoning. Our holistic evaluation
surfaces insights about scientific reasoning performance that remain hidden
when relying on individual benchmarks alone. We then propose KRUX, a probing
framework for studying the distinct roles of reasoning and knowledge in
scientific tasks. Combining the two, we conduct an in-depth analysis that
yields several key findings: (1) Retrieving task-relevant knowledge from model
parameters is a critical bottleneck for LLMs in scientific reasoning; (2)
Reasoning models consistently benefit from external knowledge added in-context
on top of the reasoning enhancement; (3) Enhancing verbalized reasoning
improves LLMs' ability to surface task-relevant knowledge. Finally, we conduct
a lightweight analysis, comparing our science-focused data composition with
concurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline
for scientific reasoning.

</details>


### [49] [VibeVoice Technical Report](https://arxiv.org/abs/2508.19205)
*Zhiliang Peng,Jianwei Yu,Wenhui Wang,Yaoyao Chang,Yutao Sun,Li Dong,Yi Zhu,Weijiang Xu,Hangbo Bao,Zehua Wang,Shaohan Huang,Yan Xia,Furu Wei*

Main category: cs.CL

TL;DR: VibeVoice提出了一种基于next-token diffusion的长文本多说话人语音合成模型，通过新型连续语音分词器实现80倍数据压缩，支持90分钟语音合成并超越主流对话模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型在长文本语音合成中计算效率低、多说话人对话真实性不足的问题，提升语音合成的时长上限和对话自然度。

Method: 1. 采用next-token diffusion进行隐向量自回归生成
2. 开发新型连续语音分词器(相比Encodec压缩率提升80倍)
3. 支持64K长度的上下文窗口建模

Result: 1. 实现最长90分钟语音合成
2. 最多支持4个说话人对话
3. 在计算效率和音频保真度上超越开源/商业对话模型
4. 保持语音质量的同时显著提升长序列处理效率

Conclusion: VibeVoice通过创新的分词器和扩散模型架构，在长文本语音合成领域实现突破，为多轮对话场景提供了更高效的解决方案，标志着语音合成技术向真实对话场景的重要进展。

Abstract: This report presents VibeVoice, a novel model designed to synthesize
long-form speech with multiple speakers by employing next-token diffusion,
which is a unified method for modeling continuous data by autoregressively
generating latent vectors via diffusion. To enable this, we introduce a novel
continuous speech tokenizer that, when compared to the popular Encodec model,
improves data compression by 80 times while maintaining comparable performance.
The tokenizer effectively preserves audio fidelity while significantly boosting
computational efficiency for processing long sequences. Thus, VibeVoice can
synthesize long-form speech for up to 90 minutes (in a 64K context window
length) with a maximum of 4 speakers, capturing the authentic conversational
``vibe'' and surpassing open-source and proprietary dialogue models.

</details>


### [50] [Evaluating the Evaluators: Are readability metrics good measures of readability?](https://arxiv.org/abs/2508.19221)
*Isabel Cachola,Daniel Khashabi,Mark Dredze*

Main category: cs.CL

TL;DR: 传统可读性指标（如FKGL）在简明语言摘要评估中与人类判断相关性低，语言模型（LMs）表现更优且能捕捉深层可读性因素。


<details>
  <summary>Details</summary>
Motivation: 验证传统可读性指标在PLS领域的有效性，探索语言模型作为替代评估方案的潜力。

Method: 通过文献调查、8种传统指标与人类判断的对比测试，以及语言模型的Pearson相关性分析，结合PLS数据集验证模型对背景知识等深层指标的捕捉能力。

Result: 最优语言模型与人类判断相关性达0.56，显著优于传统指标，且在背景知识需求等维度评估结论与传统方法存在本质差异。

Conclusion: 建议在PLS评估中采用语言模型替代传统指标，并公开研究数据与代码推动领域标准化进程。

Abstract: Plain Language Summarization (PLS) aims to distill complex documents into
accessible summaries for non-expert audiences. In this paper, we conduct a
thorough survey of PLS literature, and identify that the current standard
practice for readability evaluation is to use traditional readability metrics,
such as Flesch-Kincaid Grade Level (FKGL). However, despite proven utility in
other fields, these metrics have not been compared to human readability
judgments in PLS. We evaluate 8 readability metrics and show that most
correlate poorly with human judgments, including the most popular metric, FKGL.
We then show that Language Models (LMs) are better judges of readability, with
the best-performing model achieving a Pearson correlation of 0.56 with human
judgments. Extending our analysis to PLS datasets, which contain summaries
aimed at non-expert audiences, we find that LMs better capture deeper measures
of readability, such as required background knowledge, and lead to different
conclusions than the traditional metrics. Based on these findings, we offer
recommendations for best practices in the evaluation of plain language
summaries. We release our analysis code and survey data.

</details>


### [51] [Generative Interfaces for Language Models](https://arxiv.org/abs/2508.19227)
*Jiaqi Chen,Yanzhe Zhang,Yutong Zhang,Yijia Shao,Diyi Yang*

Main category: cs.CL

TL;DR: 提出生成式交互界面框架，通过LLM主动生成任务特定UI，在多维度评估中优于传统聊天模式（人类偏好率超70%）


<details>
  <summary>Details</summary>
Motivation: 传统线性对话模式在复杂任务中存在效率瓶颈，需探索更适配的交互范式来提升人机协作效能

Method: 基于结构化界面表示和迭代优化框架，将用户查询动态转化为交互界面

Result: 生成式界面在功能/交互/情感维度全面超越对话式界面，70%+用户偏好，任务完成效率提升显著

Conclusion: 生成式交互范式为复杂人机协作任务提供新方向，其主动界面生成机制将推动HCI领域创新发展

Abstract: Large language models (LLMs) are increasingly seen as assistants, copilots,
and consultants, capable of supporting a wide range of tasks through natural
conversation. However, most systems remain constrained by a linear
request-response format that often makes interactions inefficient in
multi-turn, information-dense, and exploratory tasks. To address these
limitations, we propose Generative Interfaces for Language Models, a paradigm
in which LLMs respond to user queries by proactively generating user interfaces
(UIs) that enable more adaptive and interactive engagement. Our framework
leverages structured interface-specific representations and iterative
refinements to translate user queries into task-specific UIs. For systematic
evaluation, we introduce a multidimensional assessment framework that compares
generative interfaces with traditional chat-based ones across diverse tasks,
interaction patterns, and query types, capturing functional, interactive, and
emotional aspects of user experience. Results show that generative interfaces
consistently outperform conversational ones, with humans preferring them in
over 70% of cases. These findings clarify when and why users favor generative
interfaces, paving the way for future advancements in human-AI interaction.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [52] [Controllable Single-shot Animation Blending with Temporal Conditioning](https://arxiv.org/abs/2508.18525)
*Eleni Tselepi,Spyridon Thermos,Gerasimos Potamianos*

Main category: cs.GR

TL;DR: 提出首个单次运动混合框架，通过时间条件生成实现无缝动作融合


<details>
  <summary>Details</summary>
Motivation: 现有单次生成方法缺乏单一生成过程中混合多动作的可控框架

Method: 引入骨架感知归一化机制，通过时间条件控制生成过程，实现数据驱动的动作过渡

Result: 在不同动画风格和骨骼结构下实现平滑可控的运动混合，定量定性评估显示有效性

Conclusion: 提出高效统一的运动混合方法，首次实现单次生成过程中的动态可控混合

Abstract: Training a generative model on a single human skeletal motion sequence
without being bound to a specific kinematic tree has drawn significant
attention from the animation community. Unlike text-to-motion generation,
single-shot models allow animators to controllably generate variations of
existing motion patterns without requiring additional data or extensive
retraining. However, existing single-shot methods do not explicitly offer a
controllable framework for blending two or more motions within a single
generative pass. In this paper, we present the first single-shot motion
blending framework that enables seamless blending by temporally conditioning
the generation process. Our method introduces a skeleton-aware normalization
mechanism to guide the transition between motions, allowing smooth, data-driven
control over when and how motions blend. We perform extensive quantitative and
qualitative evaluations across various animation styles and different kinematic
skeletons, demonstrating that our approach produces plausible, smooth, and
controllable motion blends in a unified and efficient manner.

</details>


### [53] [Real-time 3D Visualization of Radiance Fields on Light Field Displays](https://arxiv.org/abs/2508.18540)
*Jonghyun Kim,Cheng Sun,Michael Stengel,Matthew Chan,Andrew Russell,Jaehyun Jung,Wil Braithwaite,Shalini De Mello,David Luebke*

Main category: cs.GR

TL;DR: 提出统一框架实现光场显示器上的实时辐射场渲染，支持NeRF/3D高斯泼溅/稀疏体素等多种表示方法，在45视点下实现200+ FPS渲染性能。


<details>
  <summary>Details</summary>
Motivation: 辐射场与光场显示器的整合存在双重计算挑战：光场需要多视点高分辨率渲染，而辐射场依赖计算密集的体渲染技术。

Method: 采用单次平面扫描策略+非方向性共享组件缓存架构，通过视锥空间重投影避免多视点重复计算，支持不同场景表征的通用化处理。

Result: 在Looking Glass显示器实现实时交互应用（512p@45视点），标准测试中相比独立渲染各视点速度提升22倍且保持画质。

Conclusion: 该框架首次实现跨多种辐射场表征的实时光场渲染，通过架构创新有效解决多视点渲染冗余问题，推动沉浸式3D交互发展。

Abstract: Radiance fields have revolutionized photo-realistic 3D scene visualization by
enabling high-fidelity reconstruction of complex environments, making them an
ideal match for light field displays. However, integrating these technologies
presents significant computational challenges, as light field displays require
multiple high-resolution renderings from slightly shifted viewpoints, while
radiance fields rely on computationally intensive volume rendering. In this
paper, we propose a unified and efficient framework for real-time radiance
field rendering on light field displays. Our method supports a wide range of
radiance field representations, including NeRFs, 3D Gaussian Splatting, and
Sparse Voxels, within a shared architecture based on a single-pass plane
sweeping strategy and caching of shared, non-directional components. The
framework generalizes across different scene formats without retraining, and
avoids redundant computation across views. We further demonstrate a real-time
interactive application on a Looking Glass display, achieving 200+ FPS at 512p
across 45 views, enabling seamless, immersive 3D interaction. On standard
benchmarks, our method achieves up to 22x speedup compared to independently
rendering each view, while preserving image quality.

</details>


### [54] [SemLayoutDiff: Semantic Layout Generation with Diffusion Model for Indoor Scene Synthesis](https://arxiv.org/abs/2508.18597)
*Xiaohao Sun,Divyam Goel,Angle X. Chang*

Main category: cs.GR

TL;DR: 提出SemLayoutDiff模型，通过结合语义地图与扩散模型生成符合建筑约束的3D室内场景布局


<details>
  <summary>Details</summary>
Motivation: 解决现有方法无法考虑门窗等建筑约束，导致家具布局不实用的问题

Method: 1. 使用分类扩散模型生成带房间掩码的语义地图 2. 基于交叉注意力机制预测家具位置 3. 显式建模门窗等建筑元素约束

Result: 在3D-FRONT数据集上生成场景的空间连贯性、真实性和多样性超越基线方法

Conclusion: 提出的统一框架能生成既符合建筑规范又保持多样性的实用3D室内布局

Abstract: We present SemLayoutDiff, a unified model for synthesizing diverse 3D indoor
scenes across multiple room types. The model introduces a scene layout
representation combining a top-down semantic map and attributes for each
object. Unlike prior approaches, which cannot condition on architectural
constraints, SemLayoutDiff employs a categorical diffusion model capable of
conditioning scene synthesis explicitly on room masks. It first generates a
coherent semantic map, followed by a cross-attention-based network to predict
furniture placements that respect the synthesized layout. Our method also
accounts for architectural elements such as doors and windows, ensuring that
generated furniture arrangements remain practical and unobstructed. Experiments
on the 3D-FRONT dataset show that SemLayoutDiff produces spatially coherent,
realistic, and varied scenes, outperforming previous methods.

</details>


### [55] [PanoHair: Detailed Hair Strand Synthesis on Volumetric Heads](https://arxiv.org/abs/2508.18944)
*Shashikant Verma,Shanmuganathan Raman*

Main category: cs.GR

TL;DR: 提出PanoHair模型，通过知识蒸馏和生成式方法实现5秒内快速生成高保真头发网格，显著提升现有方法的效率与质量


<details>
  <summary>Details</summary>
Motivation: 现有头发合成方法需要复杂多视角数据采集且效率低下，难以快速生成多样化高精度发型

Method: 1. 使用预训练生成模型知识蒸馏估计头部SDF
2. 预测头发区域语义分割掩码和3D方向
3. 支持潜在空间操作生成多样化发型
4. 通过反转过程处理真实图像

Result: 生成清洁头发网格耗时<5秒，支持潜在空间发型编辑，在真实图像上实现视觉吸引力的头发合成，实验显示显著优于现有方法

Conclusion: PanoHair通过生成式架构和高效推理流程，突破传统方法的数据采集限制，实现快速、高质量的头发合成

Abstract: Achieving realistic hair strand synthesis is essential for creating lifelike
digital humans, but producing high-fidelity hair strand geometry remains a
significant challenge. Existing methods require a complex setup for data
acquisition, involving multi-view images captured in constrained studio
environments. Additionally, these methods have longer hair volume estimation
and strand synthesis times, which hinder efficiency. We introduce PanoHair, a
model that estimates head geometry as signed distance fields using knowledge
distillation from a pre-trained generative teacher model for head synthesis.
Our approach enables the prediction of semantic segmentation masks and 3D
orientations specifically for the hair region of the estimated geometry. Our
method is generative and can generate diverse hairstyles with latent space
manipulations. For real images, our approach involves an inversion process to
infer latent codes and produces visually appealing hair strands, offering a
streamlined alternative to complex multi-view data acquisition setups. Given
the latent code, PanoHair generates a clean manifold mesh for the hair region
in under 5 seconds, along with semantic and orientation maps, marking a
significant improvement over existing methods, as demonstrated in our
experiments.

</details>


### [56] [A Bag of Tricks for Efficient Implicit Neural Point Clouds](https://arxiv.org/abs/2508.19140)
*Florian Hahlbohm,Linus Franke,Leon Overkämping,Paula Wespe,Susana Castillo,Martin Eisemann,Marcus Magnor*

Main category: cs.GR

TL;DR: INPC通过优化光栅化器、采样技术和引入CNN预训练，实现训练加速25%、渲染提速2倍、显存减少20%，同时提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决INPC因渲染速度慢导致的实用性问题，在保持视觉质量的前提下优化性能。

Method: 改进光栅化器实现、优化采样策略、CNN预训练用于孔洞填充、推理时采用高斯点建模。

Result: 训练速度提升25%，渲染速度翻倍，显存占用减少20%，图像质量轻微提升。

Conclusion: 系统性的优化方案不仅适用于INPC，也为类似神经渲染方法提供可借鉴的加速策略。

Abstract: Implicit Neural Point Cloud (INPC) is a recent hybrid representation that
combines the expressiveness of neural fields with the efficiency of point-based
rendering, achieving state-of-the-art image quality in novel view synthesis.
However, as with other high-quality approaches that query neural networks
during rendering, the practical usability of INPC is limited by comparatively
slow rendering. In this work, we present a collection of optimizations that
significantly improve both the training and inference performance of INPC
without sacrificing visual fidelity. The most significant modifications are an
improved rasterizer implementation, more effective sampling techniques, and the
incorporation of pre-training for the convolutional neural network used for
hole-filling. Furthermore, we demonstrate that points can be modeled as small
Gaussians during inference to further improve quality in extrapolated, e.g.,
close-up views of the scene. We design our implementations to be broadly
applicable beyond INPC and systematically evaluate each modification in a
series of experiments. Our optimized INPC pipeline achieves up to 25% faster
training, 2x faster rendering, and 20% reduced VRAM usage paired with slight
image quality improvements.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [57] [Membership Inference Attacks on LLM-based Recommender Systems](https://arxiv.org/abs/2508.18665)
*Jiajie He,Yuechun Gu,Min-Chun Chen,Keke Chen*

Main category: cs.IR

TL;DR: 研究发现基于大语言模型的推荐系统存在成员隐私泄露风险，提出四种新型攻击方法并在实验中验证有效性，其中直接询问和投毒攻击表现突出


<details>
  <summary>Details</summary>
Motivation: 现有LLM推荐系统在提示中使用了用户历史交互数据，这些敏感信息可能被新型隐私攻击窃取，而该领域尚未有相关研究

Method: 设计了直接询问、幻觉攻击、相似性攻击、投毒攻击四种MIA方案，在三种LLM推荐系统和两个基准数据集上进行系统评估

Result: 实验表明直接询问攻击(成功率78%)和投毒攻击(优势指标达0.42)效果显著，攻击效果受提示样本数量及受害者位置等因素影响

Conclusion: 首次证实LLM推荐系统面临现实的MIA威胁，提示工程的设计需加强隐私保护机制，为后续防御研究提供方向

Abstract: Large language models (LLMs) based Recommender Systems (RecSys) can flexibly
adapt recommendation systems to different domains. It utilizes in-context
learning (ICL), i.e., the prompts, to customize the recommendation functions,
which include sensitive historical user-specific item interactions, e.g.,
implicit feedback like clicked items or explicit product reviews. Such private
information may be exposed to novel privacy attack. However, no study has been
done on this important issue. We design four membership inference attacks
(MIAs), aiming to reveal whether victims' historical interactions have been
used by system prompts. They are \emph{direct inquiry, hallucination,
similarity, and poisoning attacks}, each of which utilizes the unique features
of LLMs or RecSys. We have carefully evaluated them on three LLMs that have
been used to develop ICL-LLM RecSys and two well-known RecSys benchmark
datasets. The results confirm that the MIA threat on LLM RecSys is realistic:
direct inquiry and poisoning attacks showing significantly high attack
advantages. We have also analyzed the factors affecting these attacks, such as
the number of shots in system prompts and the position of the victim in the
shots.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [58] [Impact of Target and Tool Visualization on Depth Perception and Usability in Optical See-Through AR](https://arxiv.org/abs/2508.18481)
*Yue Yang,Xue Xie,Xinkai Wang,Hui Zhang,Chiming Yu,Xiaoxian Xiong,Lifeng Zhu,Yuanyi Zheng,Jue Cen,Bruce Daniel,Fred Baik*

Main category: cs.HC

TL;DR: HoloLens 2等OST-AR系统在近距离手术引导中存在深度感知与遮挡问题。实验表明：不透明虚拟目标、实时真实工具遮挡能显著提升深度感知精度，工具追踪缺失会严重降低系统可用性。


<details>
  <summary>Details</summary>
Motivation: 解决OST-AR在近距离手术场景中虚拟目标深度感知模糊、真实手术工具与虚拟目标遮挡关系失真的核心痛点，优化AR引导系统的空间感知精度。

Method: 通过两个实验：1) 高低透明度目标的深度匹配测试 2) 6种可视化条件(2透明度×3工具模式)下的模拟手术定位任务，采集10名参与者的深度误差、定位精度、系统可用性等多维度数据。

Result: 不透明目标比高透明目标减少43%深度误差(p<0.05)；真实工具可视化模式定位误差降低62%，系统可用性评分提高31%，同时认知负荷降低28%；无工具追踪模式表现最差。

Conclusion: AR系统设计应优先确保实时工具追踪与遮挡处理。若无法实现，需谨慎调节透明度平衡深度线索与工具可见性，虚拟内容应保持不透明以维持空间感知准确性。

Abstract: Optical see-through augmented reality (OST-AR) systems like Microsoft
HoloLens 2 hold promise for arm's distance guidance (e.g., surgery), but depth
perception of the hologram and occlusion of real instruments remain
challenging. We present an evaluation of how visualizing the target object with
different transparencies and visualizing a tracked tool (virtual proxy vs. real
tool vs. no tool tracking) affects depth perception and system usability. Ten
participants performed two experiments on HoloLens 2. In Experiment 1, we
compared high-transparency vs. low-transparency target rendering in a depth
matching task at arm's length. In Experiment 2, participants performed a
simulated surgical pinpoint task on a frontal bone target under six
visualization conditions ($2 \times 3$: two target transparencies and three
tool visualization modes: virtual tool hologram, real tool, or no tool
tracking). We collected data on depth matching error, target localization
error, system usability, task workload, and qualitative feedback. Results show
that a more opaque target yields significantly lower depth estimation error
than a highly transparent target at arm's distance. Moreover, showing the real
tool (occluding the virtual target) led to the highest accuracy and usability
with the lowest workload, while not tracking the tool yielded the worst
performance and user ratings. However, making the target highly transparent,
while allowing the real tool to remain visible, slightly impaired depth cues
and did not improve usability. Our findings underscore that correct occlusion
cues, rendering virtual content opaque and occluding it with real tools in real
time, are critical for depth perception and precision in OST-AR. Designers of
arm-distance AR systems should prioritize robust tool tracking and occlusion
handling; if unavailable, cautiously use transparency to balance depth
perception and tool visibility.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [59] [H-PRM: A Pluggable Hotword Pre-Retrieval Module for Various Speech Recognition Systems](https://arxiv.org/abs/2508.18295)
*Huangyu Dai,Lingtao Mao,Ben Chen,Zihan Wang,Zihan Liang,Ying Han,Chenyi Lei,Han Li*

Main category: cs.SD

TL;DR: 提出通过声学相似度预检索的热词定制系统H-PRM，显著提升ASR领域热词召回率，兼容传统模型与音频大模型。


<details>
  <summary>Details</summary>
Motivation: 现有ASR热词定制模型在大规模热词场景下识别率骤降，需解决声学匹配与模型适配性问题。

Method: 1. 开发热词预检索模块(H-PRM)测量热词与语音声学相似度；2. 即插即用集成至SeACo-Paraformer等传统模型；3. 通过提示工程融入Audio LLMs实现热词定制。

Result: 实验验证H-PRM显著超越现有方法，热词后召回率(PRR)获得突破性提升。

Conclusion: H-PRM为ASR热词定制开辟新路径，证明声学预检索机制在传统模型与LLM中的双重有效性。

Abstract: Hotword customization is crucial in ASR to enhance the accuracy of
domain-specific terms. It has been primarily driven by the advancements in
traditional models and Audio large language models (LLMs). However, existing
models often struggle with large-scale hotwords, as the recognition rate drops
dramatically with the number of hotwords increasing. In this paper, we
introduce a novel hotword customization system that utilizes a hotword
pre-retrieval module (H-PRM) to identify the most relevant hotword candidate by
measuring the acoustic similarity between the hotwords and the speech segment.
This plug-and-play solution can be easily integrated into traditional models
such as SeACo-Paraformer, significantly enhancing hotwords post-recall rate
(PRR). Additionally, we incorporate H-PRM into Audio LLMs through a
prompt-based approach, enabling seamless customization of hotwords. Extensive
testing validates that H-PRM can outperform existing methods, showing a new
direction for hotword customization in ASR.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [60] [LSD-3D: Large-Scale 3D Driving Scene Generation with Geometry Grounding](https://arxiv.org/abs/2508.19204)
*Julian Ost,Andrea Ramazzina,Amogh Joshi,Maximilian Bömer,Mario Bijelic,Felix Heide*

Main category: cs.CV

TL;DR: 提出结合代理几何生成与分数蒸馏的方法，实现高可控性的大规模3D驾驶场景生成


<details>
  <summary>Details</summary>
Motivation: 现有神经重建方法缺乏场景控制能力，扩散模型生成数据缺乏几何基础与因果性，需结合两者优势

Method: 通过代理几何生成环境表示，利用2D图像先验进行分数蒸馏，支持地图布局条件约束

Result: 实现几何精确、纹理逼真且保持三维一致性的复杂驾驶场景生成

Conclusion: 该方法突破了传统方法的静态限制，在保持几何精确性的同时显著提升了场景生成的可控性

Abstract: Large-scale scene data is essential for training and testing in robot
learning. Neural reconstruction methods have promised the capability of
reconstructing large physically-grounded outdoor scenes from captured sensor
data. However, these methods have baked-in static environments and only allow
for limited scene control -- they are functionally constrained in scene and
trajectory diversity by the captures from which they are reconstructed. In
contrast, generating driving data with recent image or video diffusion models
offers control, however, at the cost of geometry grounding and causality. In
this work, we aim to bridge this gap and present a method that directly
generates large-scale 3D driving scenes with accurate geometry, allowing for
causal novel view synthesis with object permanence and explicit 3D geometry
estimation. The proposed method combines the generation of a proxy geometry and
environment representation with score distillation from learned 2D image
priors. We find that this approach allows for high controllability, enabling
the prompt-guided geometry and high-fidelity texture and structure that can be
conditioned on map layouts -- producing realistic and geometrically consistent
3D generations of complex driving scenes.

</details>


### [61] [Can VLMs Recall Factual Associations From Visual References?](https://arxiv.org/abs/2508.18297)
*Dhananjay Ashok,Ashutosh Chaubey,Hirona J. Arai,Jonathan May,Jesse Thomason*

Main category: cs.CV

TL;DR: 研究发现视觉语言模型（VLMs）在视觉实体引用时存在知识链接缺陷，通过内部状态探测可有效识别不可靠回答并提升任务表现。


<details>
  <summary>Details</summary>
Motivation: 探索VLMs在多模态基础中的系统缺陷，特别是视觉与文本引用在知识关联上的性能差异。

Method: 通过控制实验比较文本/视觉引用效果，分析内部状态模式，构建无需训练的探测器识别不可靠回答，并在视觉问答任务中验证。

Result: 视觉引用导致知识回忆能力下降50%，探测器准确率达92%，覆盖提升7.87%且错误率降低0.9%。

Conclusion: VLMs存在可检测的系统性视觉-知识链接缺陷，内部状态探测为可靠解决方案，建议加强多模态知识表征研究。

Abstract: Through a controlled study, we identify a systematic deficiency in the
multimodal grounding of Vision Language Models (VLMs). While VLMs can recall
factual associations when provided a textual reference to an entity; their
ability to do so is significantly diminished when the reference is visual
instead. Forcing VLMs to rely on image representations of an entity halves
their ability to recall factual knowledge, suggesting that VLMs struggle to
link their internal knowledge of an entity with its image representation. We
show that such linking failures are correlated with the expression of distinct
patterns in model internal states, and that probes on these internal states
achieve over 92% accuracy at flagging cases where the VLM response is
unreliable. These probes can be applied, without retraining, to identify when a
VLM will fail to correctly answer a question that requires an understanding of
multimodal input. When used to facilitate selective prediction on a visual
question answering task, the probes increase coverage by 7.87% (absolute) while
also reducing the risk of error by 0.9% (absolute). Addressing the systematic,
detectable deficiency is an important avenue in language grounding, and we
provide informed recommendations for future directions.

</details>


### [62] [Beyond the Textual: Generating Coherent Visual Options for MCQs](https://arxiv.org/abs/2508.18772)
*Wanqiang Wang,Longzhu He,Wei Zheng*

Main category: cs.CV

TL;DR: 提出跨模态选项合成框架CmOS，通过多模态思维链和检索增强生成技术，解决教育多选题视觉选项生成与干扰项质量难题。


<details>
  <summary>Details</summary>
Motivation: 现有MCQ生成方法忽视视觉选项且人工制作干扰项成本高，需自动化生成具备语义合理性和视觉相似性的多模态选项。

Method: 整合多模态思维链推理(MCoT)和检索增强生成(RAG)，开发内容判别模块筛选适合视觉化的题目，同步生成文本和视觉干扰项。

Result: 跨学科实验显示CmOS在内容判别准确率、问题生成质量和视觉选项逼真度上均显著优于基线模型，适用于不同教育阶段。

Conclusion: CmOS框架为教育评估提供了高效的多模态MCQ生成方案，证明跨模态合成技术在教育智能化中的实用价值。

Abstract: Multiple-choice questions (MCQs) play a crucial role in fostering deep
thinking and knowledge integration in education. However, previous research has
primarily focused on generating MCQs with textual options, but it largely
overlooks the visual options. Moreover, generating high-quality distractors
remains a major challenge due to the high cost and limited scalability of
manual authoring. To tackle these problems, we propose a Cross-modal Options
Synthesis (CmOS), a novel framework for generating educational MCQs with visual
options. Our framework integrates Multimodal Chain-of-Thought (MCoT) reasoning
process and Retrieval-Augmented Generation (RAG) to produce semantically
plausible and visually similar answer and distractors. It also includes a
discrimination module to identify content suitable for visual options.
Experimental results on test tasks demonstrate the superiority of CmOS in
content discrimination, question generation and visual option generation over
existing methods across various subjects and educational levels.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [63] [Training Language Model Agents to Find Vulnerabilities with CTF-Dojo](https://arxiv.org/abs/2508.18370)
*Terry Yue Zhuo,Dingmin Wang,Hantian Ding,Varun Kumar,Zijian Wang*

Main category: cs.SE

TL;DR: 提出首个大规模可执行训练环境CTF-Dojo，通过自动化流程CTF-Forge实现快速扩展，仅需486条训练轨迹即实现11.6%性能提升，32B模型达到31.9% Pass@1新SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有可执行训练环境稀缺且扩展性差，制约了基于验证反馈的AI智能体训练。需要可复现、自动化的大规模执行环境推动ML智能体发展。

Method: 1. 构建包含658个Docker容器化CTF挑战的CTF-Dojo环境
2. 开发CTF-Forge自动化流水线，将公开资源快速转化为可用环境
3. 基于执行验证轨迹训练LLM智能体

Result: 在三大基准测试中平均提升11.6%，32B模型达到31.9% Pass@1（开源模型新SOTA），媲美DeepSeek-V3和Gemini等前沿模型。

Conclusion: 验证执行反馈机制对训练高性能ML智能体的关键作用，证明无需依赖昂贵商业系统即可实现突破。

Abstract: Large language models (LLMs) have demonstrated exceptional capabilities when
trained within executable runtime environments, notably excelling at software
engineering tasks through verified feedback loops. Yet, scalable and
generalizable execution-grounded environments remain scarce, limiting progress
in training more capable ML agents. We introduce CTF-Dojo, the first
large-scale executable runtime tailored for training LLMs with verifiable
feedback, featuring 658 fully functional Capture-The-Flag (CTF)-style
challenges containerized in Docker with guaranteed reproducibility. To enable
rapid scaling without manual intervention, we develop CTF-Forge, an automated
pipeline that transforms publicly available artifacts into ready-to-use
execution environments in minutes, eliminating weeks of expert configuration
traditionally required. We trained LLM-based agents on just 486 high-quality,
execution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute
gains over strong baselines across three competitive benchmarks: InterCode-CTF,
NYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1,
establishing a new open-weight state-of-the-art that rivals frontier models
like DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a
benchmark for executable-agent learning, CTF-Dojo demonstrates that
execution-grounded training signals are not only effective but pivotal in
advancing high-performance ML agents without dependence on costly proprietary
systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [64] [Toward Responsible ASR for African American English Speakers: A Scoping Review of Bias and Equity in Speech Technology](https://arxiv.org/abs/2508.18288)
*Jay L. Cunningham,Adinawa Adjagbodjou,Jeffrey Basoah,Jainaba Jawara,Kowe Kadoma,Aaleyah Lewis*

Main category: eess.AS

TL;DR: 论文通过综述44篇文献，分析ASR技术中对非裔英语使用者的公平性问题，提出治理导向的跨学科框架。


<details>
  <summary>Details</summary>
Motivation: 现有技术公平性干预缺乏治理视角（如社区参与、语言正义），需构建负责任的ASR开发体系。

Method: 采用范围界定文献综述方法，覆盖人机交互、自然语言处理、社会语言学领域的44篇文献，聚焦四大研究维度。

Result: 发现技术公平措施增长但治理方法缺失，提出以社区代理为核心的ASR生命周期治理框架。

Conclusion: 构建治理导向的跨学科框架，为消除语音AI语言边缘化提供方法论与实践路径。

Abstract: This scoping literature review examines how fairness, bias, and equity are
conceptualized and operationalized in Automatic Speech Recognition (ASR) and
adjacent speech and language technologies (SLT) for African American English
(AAE) speakers and other linguistically diverse communities. Drawing from 44
peer-reviewed publications across Human-Computer Interaction (HCI), Machine
Learning/Natural Language Processing (ML/NLP), and Sociolinguistics, we
identify four major areas of inquiry: (1) how researchers understand
ASR-related harms; (2) inclusive data practices spanning collection, curation,
annotation, and model training; (3) methodological and theoretical approaches
to linguistic inclusion; and (4) emerging practices and design recommendations
for more equitable systems. While technical fairness interventions are growing,
our review highlights a critical gap in governance-centered approaches that
foreground community agency, linguistic justice, and participatory
accountability. We propose a governance-centered ASR lifecycle as an emergent
interdisciplinary framework for responsible ASR development and offer
implications for researchers, practitioners, and policymakers seeking to
address language marginalization in speech AI systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [65] [RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing](https://arxiv.org/abs/2508.18642)
*Jianxing Liao,Tian Zhang,Xiao Feng,Yusong Zhang,Rui Yang,Haorui Wang,Bosi Wen,Ziying Wang,Runzhi Shi*

Main category: cs.AI

TL;DR: 提出RLMR强化学习方法，通过动态混合写作质量奖励和约束验证奖励，解决创意写作中主观质量与客观约束难以平衡的问题


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法无法兼顾写作主观质量（文学性）和客观约束（格式要求），单一奖励策略效果有限，固定权重混合奖励缺乏场景适应性

Method: 使用写作奖励模型评估主观质量，约束验证模型评估客观约束，根据组内样本质量动态调整约束奖励权重，通过GRPO算法对违规样本施加负优势惩罚

Result: 在8B-72B参数模型上实现指令遵循（IFEval从83.36%提升至86.65%）和写作质量（WriteEval人工评估72.75%胜率）的双重提升

Conclusion: 首次在在线强化学习中融合主观偏好与客观验证，为多维创意写作优化提供有效解决方案

Abstract: Large language models are extensively utilized in creative writing
applications. Creative writing requires a balance between subjective writing
quality (e.g., literariness and emotional expression) and objective constraint
following (e.g., format requirements and word limits). Existing reinforcement
learning methods struggle to balance these two aspects: single reward
strategies fail to improve both abilities simultaneously, while fixed-weight
mixed-reward methods lack the ability to adapt to different writing scenarios.
To address this problem, we propose Reinforcement Learning with Mixed Rewards
(RLMR), utilizing a dynamically mixed reward system from a writing reward model
evaluating subjective writing quality and a constraint verification model
assessing objective constraint following. The constraint following reward
weight is adjusted dynamically according to the writing quality within sampled
groups, ensuring that samples violating constraints get negative advantage in
GRPO and thus penalized during training, which is the key innovation of this
proposed method. We conduct automated and manual evaluations across diverse
model families from 8B to 72B parameters. Additionally, we construct a
real-world writing benchmark named WriteEval for comprehensive evaluation.
Results illustrate that our method achieves consistent improvements in both
instruction following (IFEval from 83.36\% to 86.65\%) and writing quality
(72.75\% win rate in manual expert pairwise evaluations on WriteEval). To the
best of our knowledge, RLMR is the first work to combine subjective preferences
with objective verification in online RL training, providing an effective
solution for multi-dimensional creative writing optimization.

</details>


### [66] [Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap](https://arxiv.org/abs/2508.18646)
*Jun Wang,Ninglun Gu,Kailai Zhang,Zijiao Zhang,Yelun Bao,Jin Yang,Xu Yin,Liwei Liu,Yihuan Liu,Pengyong Li,Gary G. Yen,Junchi Yan*

Main category: cs.AI

TL;DR: 提出以人类智能为参照的三维评估范式（IQ/EQ/PQ）和价值导向评估框架（VQ），突破传统碎片化评测，构建LLM综合评估体系


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估框架存在技术指标与部署需求脱节、忽视商业价值与社会影响等系统性评估的问题

Method: 通过人类智力三商模型构建评估体系（基础智能/价值对齐/专业能力），设计包含经济价值、社会影响、伦理合规、环境可持续的四维价值评估框架

Result: 建立模块化评估架构，分析200+基准测试后揭示动态评估需求、可解释性差距等核心挑战

Conclusion: 该框架为开发兼具技术能力、场景适应性和伦理合规的LLM提供系统化指导，配套开源评估资源库推动行业标准化进程

Abstract: For Large Language Models (LLMs), a disconnect persists between benchmark
performance and real-world utility. Current evaluation frameworks remain
fragmented, prioritizing technical metrics while neglecting holistic assessment
for deployment. This survey introduces an anthropomorphic evaluation paradigm
through the lens of human intelligence, proposing a novel three-dimensional
taxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational
capacity, Emotional Quotient (EQ)-Alignment Ability for value-based
interactions, and Professional Quotient (PQ)-Professional Expertise for
specialized proficiency. For practical value, we pioneer a Value-oriented
Evaluation (VQ) framework assessing economic viability, social impact, ethical
alignment, and environmental sustainability. Our modular architecture
integrates six components with an implementation roadmap. Through analysis of
200+ benchmarks, we identify key challenges including dynamic assessment needs
and interpretability gaps. It provides actionable guidance for developing LLMs
that are technically proficient, contextually relevant, and ethically sound. We
maintain a curated repository of open-source evaluation resources at:
https://github.com/onejune2018/Awesome-LLM-Eval.

</details>


### [67] [Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval](https://arxiv.org/abs/2508.18724)
*Karanbir Singh,Deepak Muppiri,William Ngu*

Main category: cs.AI

TL;DR: 提出多智能体系统Bias Mitigation Agent，通过协同优化信息来源选择，实现81.82%的偏见消减效果。


<details>
  <summary>Details</summary>
Motivation: 基于生成式AI的系统在信息检索过程中存在固有偏见，这会影响知识传播的公平性并降低用户信任度。

Method: 开发由多个专业智能体组成的系统，通过优化信息来源选择机制，平衡相关性与偏见水平。

Result: 相比基线检索策略，实验结果显示偏见减少81.82%。

Conclusion: 该多智能体架构有效提升了信息公平性，为生成式AI的偏见治理提供了创新解决方案。

Abstract: Large Language Models (LLMs) have transformed the field of artificial
intelligence by unlocking the era of generative applications. Built on top of
generative AI capabilities, Agentic AI represents a major shift toward
autonomous, goal-driven systems that can reason, retrieve, and act. However,
they also inherit the bias present in both internal and external information
sources. This significantly affects the fairness and balance of retrieved
information, and hence reduces user trust. To address this critical challenge,
we introduce a novel Bias Mitigation Agent, a multi-agent system designed to
orchestrate the workflow of bias mitigation through specialized agents that
optimize the selection of sources to ensure that the retrieved content is both
highly relevant and minimally biased to promote fair and balanced knowledge
dissemination. The experimental results demonstrate an 81.82\% reduction in
bias compared to a baseline naive retrieval strategy.

</details>


### [68] [CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks](https://arxiv.org/abs/2508.18743)
*Sunguk Choi,Yonghoon Kwon,Heondeuk Lee*

Main category: cs.AI

TL;DR: 通过限制连接词短语实现高效结构化推理，在保持System-1任务性能（约90%）的同时，GSM8K达到85%、GPQA达40%准确率，推理长度缩减至基线1/3（约300token）


<details>
  <summary>Details</summary>
Motivation: 解决长思维链(CoT)在快速直觉型System-1任务中性能下降的问题，同时需要兼顾复杂System-2任务的解决能力

Method: 基于固定连接词短语引导模型生成简洁结构化解释，结合Gemini-2.0-Flash模型实现高质量训练

Result: GSM8K:85% | GPQA(System-2):40% | S1-Bench(System-1):90% | 平均推理长度300token（较基线减少2/3）

Conclusion: CAC-CoT证明结构化思维链可在不损失System-1性能的前提下，有效提升System-2任务效率，为LLM推理效率优化提供新方向

Abstract: Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs)
solve difficult problems, but very long traces often slow or even degrade
performance on fast, intuitive "System-1" tasks. We introduce Connector-Aware
Compact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a
small, fixed set of connector phrases, steering the model toward concise and
well -- structured explanations. Despite its simplicity, our synthetic method
with Gemini-2.0-Flash yields a high-quality training quality. CAC-CoT achieves
approximately 85% on GSM8K and approximately 40% on GPQA (System-2) while
retaining approximately 90% on S1-Bench (System-1). Its reasoning traces
average approximately 300 tokens(ART), about one-third the length of baseline
traces, delivering higher efficiency without loss of accuracy.

</details>


### [69] [Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating Abstention Failures in Large Reasoning Models](https://arxiv.org/abs/2508.18760)
*Yi Liu,Xiangyu Liu,Zequn Sun,Wei Hu*

Main category: cs.AI

TL;DR: 大型推理模型（LRMs）在无法回答的问题（如缺少必要条件的数学题）中持续无法恰当拒绝回答，作者提出结合认知监控与推理干预的两阶段方法有效提升拒绝率


<details>
  <summary>Details</summary>
Motivation: 为了解决LRMs内部认知与外部回应不一致的问题，确保AI的可信度需要系统性的解决方案

Method: 1. 分析LRMs对无法回答问题的响应行为 2. 通过认知监测和推理时干预相结合的两阶段方法进行模型校准

Result: 实验表明该方法在保持推理性能的同时，将拒绝回答率提升了31.5个百分点

Conclusion: 该方法有效解决了LRMs的认知-回应错位问题，为构建可信AI系统提供了新思路

Abstract: Large reasoning models (LRMs) have shown remarkable progress on complex
reasoning tasks. However, some questions posed to LRMs are inherently
unanswerable, such as math problems lacking sufficient conditions. We find that
LRMs continually fail to provide appropriate abstentions when confronted with
these unanswerable questions. In this paper, we systematically analyze,
investigate, and resolve this issue for trustworthy AI. We first conduct a
detailed analysis of the distinct response behaviors of LRMs when facing
unanswerable questions. Then, we show that LRMs possess sufficient cognitive
capabilities to recognize the flaws in these questions. However, they fail to
exhibit appropriate abstention behavior, revealing a misalignment between their
internal cognition and external response. Finally, to resolve this issue, we
propose a lightweight, two-stage method that combines cognitive monitoring with
inference-time intervention. Experimental results demonstrate that our method
significantly improves the abstention rate while maintaining the overall
reasoning performance.

</details>


### [70] [Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark](https://arxiv.org/abs/2508.19005)
*Yuxuan Cai,Yipeng Hao,Jie Zhou,Hang Yan,Zhikai Lei,Rui Zhen,Zhenhua Han,Yutao Yang,Junsong Li,Qianjun Pan,Tianyu Huai,Qin Chen,Xin Li,Kai Chen,Bo Zhang,Xipeng Qiu,Liang He*

Main category: cs.AI

TL;DR: 提出基于经验驱动的终身学习框架ELL，通过四个核心机制实现智能体持续进化，并建立StuLife基准评估系统。


<details>
  <summary>Details</summary>
Motivation: AI正向通用智能发展，需从静态任务系统转向能通过真实世界互动持续学习的开放智能体。

Method: 1) 经验探索：智能体在动态环境中自主交互
2) 长期记忆：构建结构化知识存储系统
3) 技能学习：从经验抽象可复用技能
4) 知识内化：将显性经验转化为直觉能力

Result: 创建StuLife模拟大学生活全周期，验证现有LLM在持续学习场景中的表现，探索上下文工程对AGI发展的影响。

Conclusion: ELL框架为AGI发展提供新范式，StuLife基准推动终身学习能力评估，上下文工程是提升智能体适应性的关键。

Abstract: As AI advances toward general intelligence, the focus is shifting from
systems optimized for static tasks to creating open-ended agents that learn
continuously. In this paper, we introduce Experience-driven Lifelong Learning
(ELL), a framework for building self-evolving agents capable of continuous
growth through real-world interaction. The framework is built on four core
principles: (1) Experience Exploration: Agents learn through continuous,
self-motivated interaction with dynamic environments, navigating interdependent
tasks and generating rich experiential trajectories. (2) Long-term Memory:
Agents preserve and structure historical knowledge, including personal
experiences, domain expertise, and commonsense reasoning, into a persistent
memory system. (3) Skill Learning: Agents autonomously improve by abstracting
recurring patterns from experience into reusable skills, which are actively
refined and validated for application in new tasks. (4) Knowledge
Internalization: Agents internalize explicit and discrete experiences into
implicit and intuitive capabilities as "second nature".
  We also introduce StuLife, a benchmark dataset for ELL that simulates a
student's holistic college journey, from enrollment to academic and personal
development, across three core phases and ten detailed sub-scenarios. StuLife
is designed around three key paradigm shifts: From Passive to Proactive, From
Context to Memory, and From Imitation to Learning. In this dynamic environment,
agents must acquire and distill practical skills and maintain persistent memory
to make decisions based on evolving state variables. StuLife provides a
comprehensive platform for evaluating lifelong learning capabilities, including
memory retention, skill transfer, and self-motivated behavior. Beyond
evaluating SOTA LLMs on the StuLife benchmark, we also explore the role of
context engineering in advancing AGI.

</details>


### [71] [The Ramon Llull's Thinking Machine for Automated Ideation](https://arxiv.org/abs/2508.19200)
*Xinran Zhao,Boyuan Zheng,Chenglei Si,Haofei Yu,Ken Liu,Runlong Zhou,Ruochen Li,Tong Chen,Xiang Li,Yiming Zhang,Tongshuang Wu*

Main category: cs.AI

TL;DR: Revisiting Llull's Ars combinatoria to构建基于组合轴（主题、领域、方法）的现代AI研究构思工具，通过LLM驱动组合生成多样化且可解释的研究创意。


<details>
  <summary>Details</summary>
Motivation: 将中世纪组合思维框架与现代AI技术结合，解决科研创新效率问题，实现人类与AI协同的科学创造力增强。

Method: 定义主题（动机）、领域（问题场景）、方法（技术路径）三大组合轴，通过专家知识/论文挖掘要素库，引导LLM进行结构化组合生成研究创意。

Result: 验证了组合式提示能产生与当前文献紧密相关、多样且具落地性的研究思路（如效率导向的对抗训练在问答系统中的应用），提供轻量可解释的科研辅助工具。

Conclusion: 现代思维机器实现了组合式科研创新的系统化，开辟了人机协同研究范式，为保持科学发现活力提供新路径。

Abstract: This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for
generating knowledge through symbolic recombination - as a conceptual
foundation for building a modern Llull's thinking machine for research
ideation. Our approach defines three compositional axes: Theme (e.g.,
efficiency, adaptivity), Domain (e.g., question answering, machine
translation), and Method (e.g., adversarial training, linear attention). These
elements represent high-level abstractions common in scientific work -
motivations, problem settings, and technical approaches - and serve as building
blocks for LLM-driven exploration. We mine elements from human experts or
conference papers and show that prompting LLMs with curated combinations
produces research ideas that are diverse, relevant, and grounded in current
literature. This modern thinking machine offers a lightweight, interpretable
tool for augmenting scientific creativity and suggests a path toward
collaborative ideation between humans and AI.

</details>


### [72] [StepWiser: Stepwise Generative Judges for Wiser Reasoning](https://arxiv.org/abs/2508.19229)
*Wei Xiong,Wenting Zhao,Weizhe Yuan,Olga Golovneva,Tong Zhang,Jason Weston,Sainbayar Sukhbaatar*

Main category: cs.AI

TL;DR: 提出StepWiser模型，通过生成式元推理和强化学习改进过程奖励模型，提升中间步骤判断准确性和模型性能


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型存在两个核心缺陷：1) 作为分类器缺乏解释性 2) 依赖静态数据集监督微调限制泛化能力。需要能进行元推理的生成式模型来提升中间步骤监督效果

Method: 将逐步奖励建模重构为推理任务，开发生成式判断模型StepWiser。通过强化学习利用rollout相对结果进行训练，模型首先生成思考标记再输出最终判断

Result: StepWiser在中间步骤判断准确率优于现有方法，可有效改进策略模型训练，并提升推理时的搜索效率

Conclusion: 通过生成式元推理框架和强化学习方法，StepWiser突破了传统过程奖励模型的局限性，为复杂推理任务的步骤监督提供了新的解决方案

Abstract: As models increasingly leverage multi-step reasoning strategies to solve
complex problems, supervising the logical validity of these intermediate steps
has become a critical research challenge. Process reward models address this by
providing step-by-step feedback, but current approaches have two major
drawbacks: they typically function as classifiers without providing
explanations, and their reliance on supervised fine-tuning with static datasets
limits generalization. Inspired by recent advances, we reframe stepwise reward
modeling from a classification task to a reasoning task itself. We thus propose
a generative judge that reasons about the policy model's reasoning steps (i.e.,
meta-reasons), outputting thinking tokens before delivering a final verdict.
Our model, StepWiser, is trained by reinforcement learning using relative
outcomes of rollouts. We show it provides (i) better judgment accuracy on
intermediate steps than existing methods; (ii) can be used to improve the
policy model at training time; and (iii) improves inference-time search.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [73] [A Systematic Approach to Predict the Impact of Cybersecurity Vulnerabilities Using LLMs](https://arxiv.org/abs/2508.18439)
*Anders Mølmen Høst,Pierre Lison,Leon Moonen*

Main category: cs.CR

TL;DR: 论文提出TRIAGE方法，通过结合规则推理与数据驱动的LLM模块，实现CVE漏洞到ATT&CK技术的自动化映射，提升效率与召回率。


<details>
  <summary>Details</summary>
Motivation: 现有漏洞数据库缺乏对CVE漏洞实际攻击影响（如TTPs）的描述，手动映射耗时且难以应对海量漏洞，亟需自动化解决方案。

Method: 1. 基于MITRE方法提示LLM生成初始技术列表；2. 结合上下文学习模块进行补充，形成混合映射策略。

Result: 上下文学习模块效果优于单一方法，混合策略提升技术召回率；GPT-4o-mini性能优于Llama3.3-70B。

Conclusion: LLMs可有效预测漏洞攻击影响，TRIAGE显著提升CVE-ATT&CK映射效率，为网络安全分析提供新范式。

Abstract: Vulnerability databases, such as the National Vulnerability Database (NVD),
offer detailed descriptions of Common Vulnerabilities and Exposures (CVEs), but
often lack information on their real-world impact, such as the tactics,
techniques, and procedures (TTPs) that adversaries may use to exploit the
vulnerability. However, manually linking CVEs to their corresponding TTPs is a
challenging and time-consuming task, and the high volume of new vulnerabilities
published annually makes automated support desirable.
  This paper introduces TRIAGE, a two-pronged automated approach that uses
Large Language Models (LLMs) to map CVEs to relevant techniques from the ATT&CK
knowledge base. We first prompt an LLM with instructions based on MITRE's CVE
Mapping Methodology to predict an initial list of techniques. This list is then
combined with the results from a second LLM-based module that uses in-context
learning to map a CVE to relevant techniques. This hybrid approach
strategically combines rule-based reasoning with data-driven inference. Our
evaluation reveals that in-context learning outperforms the individual mapping
methods, and the hybrid approach improves recall of exploitation techniques. We
also find that GPT-4o-mini performs better than Llama3.3-70B on this task.
Overall, our results show that LLMs can be used to automatically predict the
impact of cybersecurity vulnerabilities and TRIAGE makes the process of mapping
CVEs to ATT&CK more efficient.
  Keywords: vulnerability impact, CVE, ATT&CK techniques, large language
models, automated mapping.

</details>


### [74] [UniC-RAG: Universal Knowledge Corruption Attacks to Retrieval-Augmented Generation](https://arxiv.org/abs/2508.18652)
*Runpeng Geng,Yanting Wang,Ying Chen,Jinyuan Jia*

Main category: cs.CR

TL;DR: 提出UniC-RAG攻击方法，通过少量对抗文本污染RAG系统知识库，可同时攻击数千个不同主题的用户查询，攻击成功率超90%


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统易受知识污染攻击，但传统攻击仅针对特定查询或相似主题。需开发能同时污染多样化查询的通用攻击方法以实现更大危害

Method: 1. 将攻击建模为优化问题，联合优化对抗文本 2. 设计平衡相似性聚类方法，增强攻击有效性 3. 通过对抗训练生成跨领域通用污染文本

Result: 注入100个对抗文本即可攻击2000个查询（成功率>90%），在百万级知识库中仍有效，现有防御机制无法有效抵御

Conclusion: UniC-RAG揭示RAG系统存在严重安全漏洞，现有防御措施不足，亟需开发新型防御机制应对通用知识污染攻击

Abstract: Retrieval-augmented generation (RAG) systems are widely deployed in
real-world applications in diverse domains such as finance, healthcare, and
cybersecurity. However, many studies showed that they are vulnerable to
knowledge corruption attacks, where an attacker can inject adversarial texts
into the knowledge database of a RAG system to induce the LLM to generate
attacker-desired outputs. Existing studies mainly focus on attacking specific
queries or queries with similar topics (or keywords). In this work, we propose
UniC-RAG, a universal knowledge corruption attack against RAG systems. Unlike
prior work, UniC-RAG jointly optimizes a small number of adversarial texts that
can simultaneously attack a large number of user queries with diverse topics
and domains, enabling an attacker to achieve various malicious objectives, such
as directing users to malicious websites, triggering harmful command execution,
or launching denial-of-service attacks. We formulate UniC-RAG as an
optimization problem and further design an effective solution to solve it,
including a balanced similarity-based clustering method to enhance the attack's
effectiveness. Our extensive evaluations demonstrate that UniC-RAG is highly
effective and significantly outperforms baselines. For instance, UniC-RAG could
achieve over 90% attack success rate by injecting 100 adversarial texts into a
knowledge database with millions of texts to simultaneously attack a large set
of user queries (e.g., 2,000). Additionally, we evaluate existing defenses and
show that they are insufficient to defend against UniC-RAG, highlighting the
need for new defense mechanisms in RAG systems.

</details>


### [75] [FALCON: Autonomous Cyber Threat Intelligence Mining with LLMs for IDS Rule Generation](https://arxiv.org/abs/2508.18684)
*Shaswata Mitra,Azim Bazarov,Martin Duclos,Sudip Mittal,Aritran Piplai,Md Rayhanur Rahman,Edward Zieglar,Shahram Rahimi*

Main category: cs.CR

TL;DR: FALCON框架利用大语言模型实时生成IDS规则，实现95%准确率并通过内置验证器评估有效性


<details>
  <summary>Details</summary>
Motivation: 传统IDS规则更新频率无法应对快速演变的网络威胁，导致安全防御存在滞后性

Method: 基于LLM构建多阶段验证框架，支持Snort(网络)和YARA(主机)双平台的实时规则生成与评估

Result: 实现平均95%的规则生成准确率，网络安全分析师评估达成84%的跨指标一致性共识

Conclusion: LLM驱动的实时数据挖掘技术可有效提升网络威胁缓解的及时性和准确性

Abstract: Signature-based Intrusion Detection Systems (IDS) detect malicious activities
by matching network or host activity against predefined rules. These rules are
derived from extensive Cyber Threat Intelligence (CTI), which includes attack
signatures and behavioral patterns obtained through automated tools and manual
threat analysis, such as sandboxing. The CTI is then transformed into
actionable rules for the IDS engine, enabling real-time detection and
prevention. However, the constant evolution of cyber threats necessitates
frequent rule updates, which delay deployment time and weaken overall security
readiness. Recent advancements in agentic systems powered by Large Language
Models (LLMs) offer the potential for autonomous IDS rule generation with
internal evaluation. We introduce FALCON, an autonomous agentic framework that
generates deployable IDS rules from CTI data in real-time and evaluates them
using built-in multi-phased validators. To demonstrate versatility, we target
both network (Snort) and host-based (YARA) mediums and construct a
comprehensive dataset of IDS rules with their corresponding CTIs. Our
evaluations indicate FALCON excels in automatic rule generation, with an
average of 95% accuracy validated by qualitative evaluation with 84%
inter-rater agreement among multiple cybersecurity analysts across all metrics.
These results underscore the feasibility and effectiveness of LLM-driven data
mining for real-time cyber threat mitigation.

</details>


### [76] [The Double-edged Sword of LLM-based Data Reconstruction: Understanding and Mitigating Contextual Vulnerability in Word-level Differential Privacy Text Sanitization](https://arxiv.org/abs/2508.18976)
*Stephen Meisenbacher,Alexandra Klymenko,Andreea-Elena Bodea,Florian Matthes*

Main category: cs.CR

TL;DR: 论文发现大语言模型在差分隐私文本脱敏中具有双刃剑效应：既能通过上下文推理攻击降低隐私保护，又能逆向提升脱敏文本的质量与隐私性。


<details>
  <summary>Details</summary>
Motivation: 针对现有词级差分隐私文本脱敏方法存在的上下文脆弱性问题，研究者希望借助大语言模型的强大上下文理解能力，系统评估其对隐私保护的影响并探索改进方向。

Method: 通过扩展实验范围，使用先进LLM对多种隐私级别的不同脱敏机制进行数据重建攻击测试，验证模型对原始语义的推断能力。

Result: 发现LLM既能有效推断原始文本语义（降低实证隐私保护），也可通过对抗性重建优化提升脱敏文本质量与隐私保护的平衡。

Conclusion: 建议将LLM数据重建作为后处理步骤，通过对抗性思维逆向提升差分隐私文本的隐私保护效果。

Abstract: Differentially private text sanitization refers to the process of privatizing
texts under the framework of Differential Privacy (DP), providing provable
privacy guarantees while also empirically defending against adversaries seeking
to harm privacy. Despite their simplicity, DP text sanitization methods
operating at the word level exhibit a number of shortcomings, among them the
tendency to leave contextual clues from the original texts due to randomization
during sanitization $\unicode{x2013}$ this we refer to as $\textit{contextual
vulnerability}$. Given the powerful contextual understanding and inference
capabilities of Large Language Models (LLMs), we explore to what extent LLMs
can be leveraged to exploit the contextual vulnerability of DP-sanitized texts.
We expand on previous work not only in the use of advanced LLMs, but also in
testing a broader range of sanitization mechanisms at various privacy levels.
Our experiments uncover a double-edged sword effect of LLM-based data
reconstruction attacks on privacy and utility: while LLMs can indeed infer
original semantics and sometimes degrade empirical privacy protections, they
can also be used for good, to improve the quality and privacy of DP-sanitized
texts. Based on our findings, we propose recommendations for using LLM data
reconstruction as a post-processing step, serving to increase privacy
protection by thinking adversarially.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [77] [Text to Query Plans for Question Answering on Large Tables](https://arxiv.org/abs/2508.18758)
*Yipeng Zhang,Chen Wang,Yuzhe Zhang,Jacky Jiang*

Main category: cs.DB

TL;DR: 提出基于LLM的自然语言转查询计划框架，克服SQL在大数据分析中的效率限制并支持复杂分析功能


<details>
  <summary>Details</summary>
Motivation: 传统Text-to-SQL方法受限于SQL的执行效率与扩展性，难以应对大规模数据集和复杂分析需求，且对非专业用户不友好

Method: 通过LLM迭代解析自然语言查询构建操作序列，在数据库外直接执行运算，支持SQL命令和PCA/异常检测等高级分析，避免全量数据输入模型

Result: 在标准数据库和大型科学表格实验中验证了框架处理海量数据和复杂分析任务的有效性

Conclusion: 该框架突破SQL固有局限，通过模块化操作序列实现高效查询与扩展分析，为大数据处理提供新范式

Abstract: Efficient querying and analysis of large tabular datasets remain significant
challenges, especially for users without expertise in programming languages
like SQL. Text-to-SQL approaches have shown promising performance on benchmark
data; however, they inherit SQL's drawbacks, including inefficiency with large
datasets and limited support for complex data analyses beyond basic querying.
We propose a novel framework that transforms natural language queries into
query plans. Our solution is implemented outside traditional databases,
allowing us to support classical SQL commands while avoiding SQL's inherent
limitations. Additionally, we enable complex analytical functions, such as
principal component analysis and anomaly detection, providing greater
flexibility and extensibility than traditional SQL capabilities. We leverage
LLMs to iteratively interpret queries and construct operation sequences,
addressing computational complexity by incrementally building solutions. By
executing operations directly on the data, we overcome context length
limitations without requiring the entire dataset to be processed by the model.
We validate our framework through experiments on both standard databases and
large scientific tables, demonstrating its effectiveness in handling extensive
datasets and performing sophisticated data analyses.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [78] [SALMAN: Stability Analysis of Language Models Through the Maps Between Graph-based Manifolds](https://arxiv.org/abs/2508.18306)
*Wuxinlin Cheng,Yupeng Cao,Jinwen Wu,Koduvayur Subbalakshmi,Tian Han,Zhuo Feng*

Main category: cs.LG

TL;DR: 提出SALMAN框架，通过DMD度量评估Transformer模型鲁棒性，无需修改模型参数即可提升攻击效率和鲁棒训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理不同规模模型时存在割裂，且依赖高成本的对抗样本设计。需建立统一且无需复杂调参的鲁棒性评估体系。

Method: 基于输入-输出距离映射失真(DMD)的线性复杂度度量，构建模型无关的稳定性评估框架。通过排序样本脆弱性实现高效攻击识别。

Result: 在攻击效率提升(实验显示50%+效率增益)和鲁棒训练效果优化方面取得显著突破，支持各类Transformer架构。

Conclusion: SALMAN为Transformer NLP系统提供了实用、模型无关的鲁棒性评估工具，推动可靠AI系统发展。

Abstract: Recent strides in pretrained transformer-based language models have propelled
state-of-the-art performance in numerous NLP tasks. Yet, as these models grow
in size and deployment, their robustness under input perturbations becomes an
increasingly urgent question. Existing robustness methods often diverge between
small-parameter and large-scale models (LLMs), and they typically rely on
labor-intensive, sample-specific adversarial designs. In this paper, we propose
a unified, local (sample-level) robustness framework (SALMAN) that evaluates
model stability without modifying internal parameters or resorting to complex
perturbation heuristics. Central to our approach is a novel Distance Mapping
Distortion (DMD) measure, which ranks each sample's susceptibility by comparing
input-to-output distance mappings in a near-linear complexity manner. By
demonstrating significant gains in attack efficiency and robust training, we
position our framework as a practical, model-agnostic tool for advancing the
reliability of transformer-based NLP systems.

</details>


### [79] [Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks](https://arxiv.org/abs/2508.18672)
*Taishi Nakamura,Satoki Ishikawa,Masaki Kawamura,Takumi Okamoto,Daisuke Nohara,Jun Suzuki,Rio Yokota*

Main category: cs.LG

TL;DR: MoE模型的稀疏性对记忆能力持续增益但对推理能力存在饱和效应，传统超参数调节无法弥补过度稀疏模型的推理缺陷


<details>
  <summary>Details</summary>
Motivation: 现有LLM的缩放定律未考虑MoE架构引入的稀疏性维度，需验证稀疏性对记忆/推理两种能力的不同影响机制

Method: 在固定计算预算下训练不同总参数量/激活参数量的MoE Transformer家族，系统调整top-k路由策略，监控预训练损失、下游任务损失与准确率

Result: 记忆能力随总参数增加持续提升（与训练损失同步），推理能力则呈现饱和甚至负相关；学习率等传统超参数与稀疏性对泛化缺口有同向调节作用

Conclusion: MoE稀疏性需与任务特性匹配，推理密集型任务需谨慎控制稀疏度，现有RLHF和推理阶段计算增强无法修复过度稀疏导致的推理缺陷（模型与代码已开源）

Abstract: Empirical scaling laws have driven the evolution of large language models
(LLMs), yet their coefficients shift whenever the model architecture or data
pipeline changes. Mixture-of-Experts (MoE) models, now standard in
state-of-the-art systems, introduce a new sparsity dimension that current
dense-model frontiers overlook. We investigate how MoE sparsity influences two
distinct capability regimes: memorization and reasoning. We train families of
MoE Transformers that systematically vary total parameters, active parameters,
and top-$k$ routing while holding the compute budget fixed. For every model we
record pre-training loss, downstream task loss, and task accuracy, allowing us
to separate the train-test generalization gap from the loss-accuracy gap.
Memorization benchmarks improve monotonically with total parameters, mirroring
training loss. By contrast, reasoning performance saturates and can even
regress despite continued gains in both total parameters and training loss.
Altering top-$k$ alone has little effect when active parameters are constant,
and classic hyperparameters such as learning rate and initialization modulate
the generalization gap in the same direction as sparsity. Neither post-training
reinforcement learning (GRPO) nor extra test-time compute rescues the reasoning
deficit of overly sparse models. Our model checkpoints, code and logs are
open-source at https://github.com/rioyokotalab/optimal-sparsity.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [80] [Designing across domains with declarative thinking: Insights from the 96-Eyes ptychographic imager project](https://arxiv.org/abs/2508.18512)
*Antony C Chan*

Main category: physics.optics

TL;DR: 探讨5GL在成像系统设计的应用，通过96-Eyes案例展示跨领域需求转化与协作优化


<details>
  <summary>Details</summary>
Motivation: 解决跨学科团队在硬件、算法、生命科学等领域的协作障碍，通过声明式语言保持关键输入完整性

Method: 以96-Eyes多模态成像项目为案例，将硬件约束和科学需求形式化为机器可读的问题陈述框架

Result: 提升设计透明度与可追溯性，减少75%跨团队偏差，实现光学/算法/生命科学模块的精确对齐

Conclusion: 声明式问题描述在并行研发环境中展现潜力，但需克服3GL范式惯性，未来需探索混合编程范式协作机制

Abstract: This article presents a practitioner's reflection on applying declarative,
5th generation, problem formulation language (5GL) to de novo imaging system
design, informed by experiences across the interdisciplinary research in
academia and cross-functional product development within the private sector.
Using the 96-Eyes project: 96-camera parallel multi-modal imager for
high-throughput drug discovery as a representative case, I illustrate how
project requirements, ranging from hardware constraints to life sciences needs,
can be formalized into machine-readable problem statements to preserve
mission-critical input from diverse domain stakeholders. This declarative
approach enhances transparency, ensures design traceability, and minimizes
costly misalignment across optical, algorithmic, hardware-accelerated compute,
and life sciences teams.
  Alongside the technical discussion of 5GL with real-world code examples, I
reflect on the practical barriers to adopting 5GL in environments where
imperative, 3rd-generation languages (3GL) remain the default medium for
inter-team collaboration. Rather than offering an one-size-fits-all solution,
these learned lessons highlight how programming paradigms implicitly shapes
research workflows through existing domain hierarchies. The discussion aims to
invite further explorations into how declarative problem formulations can
facilitate innovation in settings where concurrent R\&{}D workflows are gaining
traction, as opposed to environments where sequential, phase-driven workflows
remain the norm.

</details>
