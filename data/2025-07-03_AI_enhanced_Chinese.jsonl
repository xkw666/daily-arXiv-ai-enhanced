{"id": "2507.01110", "pdf": "https://arxiv.org/pdf/2507.01110", "abs": "https://arxiv.org/abs/2507.01110", "authors": ["Felix Windisch", "Lukas Radl", "Thomas K\u00f6hler", "Michael Steiner", "Dieter Schmalstieg", "Markus Steinberger"], "title": "A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory", "categories": ["cs.GR", "cs.LG"], "comment": null, "summary": "Gaussian Splatting has emerged as a high-performance technique for novel view\nsynthesis, enabling real-time rendering and high-quality reconstruction of\nsmall scenes. However, scaling to larger environments has so far relied on\npartitioning the scene into chunks -- a strategy that introduces artifacts at\nchunk boundaries, complicates training across varying scales, and is poorly\nsuited to unstructured scenarios such as city-scale flyovers combined with\nstreet-level views. Moreover, rendering remains fundamentally limited by GPU\nmemory, as all visible chunks must reside in VRAM simultaneously. We introduce\nA LoD of Gaussians, a framework for training and rendering ultra-large-scale\nGaussian scenes on a single consumer-grade GPU -- without partitioning. Our\nmethod stores the full scene out-of-core (e.g., in CPU memory) and trains a\nLevel-of-Detail (LoD) representation directly, dynamically streaming only the\nrelevant Gaussians. A hybrid data structure combining Gaussian hierarchies with\nSequential Point Trees enables efficient, view-dependent LoD selection, while a\nlightweight caching and view scheduling system exploits temporal coherence to\nsupport real-time streaming and rendering. Together, these innovations enable\nseamless multi-scale reconstruction and interactive visualization of complex\nscenes -- from broad aerial views to fine-grained ground-level details.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u5206\u533a\u7684LoD\u9ad8\u65af\u6846\u67b6\uff0c\u5728\u5355\u5757\u6d88\u8d39\u7ea7GPU\u4e0a\u5b9e\u73b0\u8d85\u5927\u89c4\u6a21\u573a\u666f\u7684\u52a8\u6001\u6d41\u5f0f\u8bad\u7ec3\u4e0e\u5b9e\u65f6\u6e32\u67d3", "motivation": "\u4f20\u7edf\u5206\u5757\u65b9\u6cd5\u5bfc\u81f4\u8fb9\u754c\u4f2a\u5f71/\u591a\u5c3a\u5ea6\u8bad\u7ec3\u56f0\u96be/GPU\u5185\u5b58\u53d7\u9650\uff0c\u96be\u4ee5\u9002\u5e94\u57ce\u5e02\u7ea7\u591a\u89c6\u89d2\u590d\u6742\u573a\u666f", "method": "\u6df7\u5408\u6570\u636e\u7ed3\u6784(\u9ad8\u65af\u5c42\u6b21+\u987a\u5e8f\u70b9\u6811)\u5b9e\u73b0\u89c6\u70b9\u76f8\u5173LoD\u9009\u62e9\uff0c\u8f7b\u91cf\u7ea7\u7f13\u5b58\u548c\u89c6\u56fe\u8c03\u5ea6\u7cfb\u7edf\u5229\u7528\u65f6\u95f4\u4e00\u81f4\u6027\u652f\u6301\u5b9e\u65f6\u6d41\u5f0f\u4f20\u8f93", "result": "\u652f\u6301\u4ece\u822a\u62cd\u5e7f\u89d2\u5230\u5730\u9762\u7ec6\u8282\u7684\u65e0\u7f1d\u591a\u5c3a\u5ea6\u91cd\u5efa\uff0c\u5355GPU\u5b9e\u73b0\u590d\u6742\u573a\u666f\u7684\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316", "conclusion": "\u7a81\u7834\u4f20\u7edf\u9ad8\u65af\u6cfc\u6e85\u7684\u573a\u666f\u89c4\u6a21\u9650\u5236\uff0c\u4e3a\u57ce\u5e02\u7ea7\u6570\u5b57\u5b6a\u751f\u7b49\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.01116", "pdf": "https://arxiv.org/pdf/2507.01116", "abs": "https://arxiv.org/abs/2507.01116", "authors": ["Gong Li", "Benjamin Watson"], "title": "Semiautomatic Simplification", "categories": ["cs.GR"], "comment": null, "summary": "We present semisimp, a tool for semiautomatic simplification of three\ndimensional polygonal models. Existing automatic simplification technology is\nquite mature, but is not sensitive to the heightened importance of distinct\nsemantic model regions such as faces and limbs, nor to simplification\nconstraints imposed by model usage such as animation. semisimp allows users to\npreserve such regions by intervening in the simplification process. Users can\nmanipulate the order in which basic simplifications are applied to redistribute\nmodel detail, improve the simplified models themselves by repositioning\nvertices with propagation to neighboring levels of detail, and adjust the\nhierarchical partitioning of the model surface to segment simplification and\nimprove control of reordering and position propagation.", "AI": {"tldr": "\u63d0\u51fa\u534a\u81ea\u52a8\u4e09\u7ef4\u6a21\u578b\u7b80\u5316\u5de5\u5177semisimp\uff0c\u901a\u8fc7\u7528\u6237\u5e72\u9884\u63d0\u5347\u8bed\u4e49\u533a\u57df\u4fdd\u62a4\u80fd\u529b", "motivation": "\u73b0\u6709\u81ea\u52a8\u7b80\u5316\u6280\u672f\u65e0\u6cd5\u6709\u6548\u4fdd\u62a4\u6a21\u578b\u8bed\u4e49\u533a\u57df\uff08\u5982\u9762\u90e8/\u80a2\u4f53\uff09\uff0c\u4e14\u5ffd\u7565\u52a8\u753b\u7b49\u5e94\u7528\u573a\u666f\u7684\u7b80\u5316\u7ea6\u675f\u3002\u9700\u8981\u7528\u6237\u4ecb\u5165\u63d0\u5347\u91cd\u8981\u533a\u57df\u4fdd\u62a4\u3002", "method": "1. \u7528\u6237\u8c03\u63a7\u7b80\u5316\u987a\u5e8f\u4f18\u5316\u7ec6\u8282\u5206\u5e03\n2. \u9876\u70b9\u91cd\u5b9a\u4f4d\u53ca\u7ec6\u8282\u5c42\u6b21\u4f20\u64ad\n3. \u6a21\u578b\u8868\u9762\u5c42\u7ea7\u5206\u533a\u8c03\u6574\u5b9e\u73b0\u5206\u6bb5\u7b80\u5316\u548c\u63a7\u5236\u4f18\u5316", "result": "\u5b9e\u73b0\u66f4\u7b26\u5408\u8bed\u4e49\u7279\u5f81\u7684\u6a21\u578b\u7b80\u5316\u6548\u679c\uff0c\u5728\u52a8\u753b\u7b49\u5e94\u7528\u573a\u666f\u4e2d\u4fdd\u6301\u5173\u952e\u533a\u57df\u5b8c\u6574\u6027", "conclusion": "\u7528\u6237\u5e72\u9884\u673a\u5236\u6709\u6548\u5f25\u8865\u4e86\u5168\u81ea\u52a8\u7b80\u5316\u7684\u4e0d\u8db3\uff0csemisimp\u4e3a\u9700\u8981\u7cbe\u786e\u63a7\u5236\u7b80\u5316\u7684\u4e13\u4e1a\u573a\u666f\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.01140", "pdf": "https://arxiv.org/pdf/2507.01140", "abs": "https://arxiv.org/abs/2507.01140", "authors": ["Eric Zimmermann", "Stefan Bruckner"], "title": "Multi-Focus Probes for Context-Preserving Network Exploration and Interaction in Immersive Analytics", "categories": ["cs.GR"], "comment": "5 pages, 3 figures, IEEE Vis 2025", "summary": "Immersive visualization of network data enables users to physically navigate\nand interact with complex structures, but managing transitions between detailed\nlocal (egocentric) views and global (exocentric) overviews remains a major\nchallenge. We present a multifocus probe technique for immersive environments\nthat allows users to instantiate multiple egocentric subgraph views while\nmaintaining persistent links to the global network context. Each probe acts as\na portable local focus, enabling fine-grained inspection and editing of distant\nor occluded regions. Visual and haptic guidance mechanisms ensure context\npreservation during multi-scale interaction. We demonstrate and discuss the\nusability of our technique for the editing of network data.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u6c89\u6d78\u5f0f\u591a\u7126\u70b9\u63a2\u9488\u6280\u672f\uff0c\u5141\u8bb8\u5728\u4fdd\u6301\u5168\u5c40\u7f51\u7edc\u4e0a\u4e0b\u6587\u7684\u540c\u65f6\u521b\u5efa\u591a\u4e2a\u5c40\u90e8\u5b50\u56fe\u89c6\u56fe\uff0c\u901a\u8fc7\u89c6\u89c9\u89e6\u89c9\u5f15\u5bfc\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7f16\u8f91", "motivation": "\u89e3\u51b3\u6c89\u6d78\u5f0f\u7f51\u7edc\u53ef\u89c6\u5316\u4e2d\u5c40\u90e8\u7ec6\u8282\u89c6\u56fe\u4e0e\u5168\u5c40\u6982\u89c8\u89c6\u56fe\u5207\u6362\u56f0\u96be\u7684\u6838\u5fc3\u6311\u6218\uff0c\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u7ef4\u62a4\u591a\u7126\u70b9\u533a\u57df\u4e0a\u4e0b\u6587\u7684\u95ee\u9898", "method": "\u63d0\u51fa\u591a\u529f\u80fd\u63a2\u9488\u6280\u672f\uff0c\u652f\u6301\u5728\u865a\u62df\u73af\u5883\u4e2d\u5b9e\u4f8b\u5316\u591a\u4e2a\u53ef\u643a\u5e26\u7684\u5c40\u90e8\u5b50\u56fe\u7126\u70b9\uff0c\u901a\u8fc7\u89c6\u89c9\u8fde\u63a5\u7ebf\u548c\u89e6\u89c9\u53cd\u9988\u4fdd\u6301\u4e0a\u4e0b\u6587\u5173\u8054", "result": "\u5b9e\u73b0\u4e86\u7f51\u7edc\u6570\u636e\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u591a\u7126\u70b9\u533a\u57df\u7684\u5e76\u884c\u64cd\u4f5c\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6280\u672f\u5728\u7ef4\u6301\u7a7a\u95f4\u8ba4\u77e5\u8fde\u7eed\u6027\u65b9\u9762\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u4ea4\u4e92\u8303\u5f0f\u6210\u529f\u5f25\u5408\u4e86\u6c89\u6d78\u5f0f\u73af\u5883\u4e2d\u5c40\u90e8\u64cd\u4f5c\u4e0e\u5168\u5c40\u8ba4\u77e5\u7684\u9e3f\u6c9f\uff0c\u4e3a\u590d\u6742\u7f51\u7edc\u7f16\u8f91\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u591a\u5c3a\u5ea6\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.01305", "pdf": "https://arxiv.org/pdf/2507.01305", "abs": "https://arxiv.org/abs/2507.01305", "authors": ["Worameth Chinchuthakun", "Pakkapon Phongthawee", "Amit Raj", "Varun Jampani", "Pramook Khungurn", "Supasorn Suwajanakorn"], "title": "DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting", "categories": ["cs.CV", "cs.GR", "cs.LG", "I.3.3; I.4.8"], "comment": "arXiv admin note: substantial text overlap with arXiv:2312.09168", "summary": "We introduce a simple yet effective technique for estimating lighting from a\nsingle low-dynamic-range (LDR) image by reframing the task as a chrome ball\ninpainting problem. This approach leverages a pre-trained diffusion model,\nStable Diffusion XL, to overcome the generalization failures of existing\nmethods that rely on limited HDR panorama datasets. While conceptually simple,\nthe task remains challenging because diffusion models often insert incorrect or\ninconsistent content and cannot readily generate chrome balls in HDR format.\nOur analysis reveals that the inpainting process is highly sensitive to the\ninitial noise in the diffusion process, occasionally resulting in unrealistic\noutputs. To address this, we first introduce DiffusionLight, which uses\niterative inpainting to compute a median chrome ball from multiple outputs to\nserve as a stable, low-frequency lighting prior that guides the generation of a\nhigh-quality final result. To generate high-dynamic-range (HDR) light probes,\nan Exposure LoRA is fine-tuned to create LDR images at multiple exposure\nvalues, which are then merged. While effective, DiffusionLight is\ntime-intensive, requiring approximately 30 minutes per estimation. To reduce\nthis overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to\nabout 30 seconds with minimal quality loss. This 60x speedup is achieved by\ntraining a Turbo LoRA to directly predict the averaged chrome balls from the\niterative process. Inference is further streamlined into a single denoising\npass using a LoRA swapping technique. Experimental results that show our method\nproduces convincing light estimates across diverse settings and demonstrates\nsuperior generalization to in-the-wild scenarios. Our code is available at\nhttps://diffusionlight.github.io/turbo", "AI": {"tldr": "\u57fa\u4e8eStable Diffusion XL\u7684\u94ec\u7403\u4fee\u590d\u6280\u672f\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u4e0eTurbo\u52a0\u901f\u5b9e\u73b0\u9ad8\u6548\u5149\u7167\u4f30\u8ba1", "motivation": "\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8eHDR\u5168\u666f\u6570\u636e\u96c6\u5bfc\u81f4\u6cdb\u5316\u6027\u4e0d\u8db3\uff0c\u6269\u6563\u6a21\u578b\u76f4\u63a5\u751f\u6210HDR\u5149\u7167\u5b58\u5728\u5185\u5bb9\u4e0d\u4e00\u81f4\u95ee\u9898", "method": "1. \u8fed\u4ee3\u4fee\u590d\u751f\u6210\u4e2d\u503c\u94ec\u7403\u4f5c\u4e3a\u4f4e\u9891\u5149\u7167\u5148\u9a8c 2. Exposure LoRA\u591a\u66dd\u5149\u878d\u5408\u751f\u6210HDR 3. Turbo LoRA\u5355\u6b65\u964d\u566a\u5b9e\u73b060\u500d\u52a0\u901f", "result": "\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u751f\u6210\u53ef\u4fe1\u5149\u7167\u4f30\u8ba1\uff0cTurbo\u7248\u672c\u4fdd\u6301\u8d28\u91cf\u540c\u65f6\u5c06\u8017\u65f6\u4ece30\u5206\u949f\u964d\u81f330\u79d2", "conclusion": "\u7ed3\u5408\u8fed\u4ee3\u4f18\u5316\u4e0e\u6a21\u578b\u52a0\u901f\u6280\u672f\uff0c\u5728\u5149\u7167\u4f30\u8ba1\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u901a\u8fc7\u4ee3\u7801\u5f00\u6e90\u63a8\u52a8\u5b9e\u9645\u5e94\u7528"}}
{"id": "2507.01019", "pdf": "https://arxiv.org/pdf/2507.01019", "abs": "https://arxiv.org/abs/2507.01019", "authors": ["Imran Mirza", "Cole Huang", "Ishwara Vasista", "Rohan Patil", "Asli Akalin", "Sean O'Brien", "Kevin Zhu"], "title": "MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered", "categories": ["cs.CL", "cs.CY"], "comment": "Accepted to Building Trust in LLMs @ ICLR 2025 and NAACL SRW 2025", "summary": "Multi-agent systems, which consist of multiple AI models interacting within a\nshared environment, are increasingly used for persona-based interactions.\nHowever, if not carefully designed, these systems can reinforce implicit biases\nin large language models (LLMs), raising concerns about fairness and equitable\nrepresentation. We present MALIBU, a novel benchmark developed to assess the\ndegree to which LLM-based multi-agent systems implicitly reinforce social\nbiases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems\nthrough scenario-based assessments. AI models complete tasks within predefined\ncontexts, and their responses undergo evaluation by an LLM-based multi-agent\njudging system in two phases. In the first phase, judges score responses\nlabeled with specific demographic personas (e.g., gender, race, religion)\nacross four metrics. In the second phase, judges compare paired responses\nassigned to different personas, scoring them and selecting the superior\nresponse. Our study quantifies biases in LLM-generated outputs, revealing that\nbias mitigation may favor marginalized personas over true neutrality,\nemphasizing the need for nuanced detection, balanced fairness strategies, and\ntransparent evaluation benchmarks in multi-agent systems.", "AI": {"tldr": "MALIBU\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793aLLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u9690\u5f0f\u5f3a\u5316\u793e\u4f1a\u504f\u89c1\uff0c\u9700\u7ec6\u81f4\u68c0\u6d4b\u4e0e\u5e73\u8861\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u53ef\u80fd\u5728\u89d2\u8272\u4e92\u52a8\u4e2d\u9690\u5f0f\u5f3a\u5316\u6a21\u578b\u504f\u89c1\uff0c\u5f71\u54cd\u7cfb\u7edf\u516c\u5e73\u6027\uff0c\u9700\u5f00\u53d1\u8bc4\u4f30\u5de5\u5177\u91cf\u5316\u6b64\u7c7b\u504f\u5dee\u3002", "method": "\u901a\u8fc7\u4e24\u9636\u6bb5\u8bc4\u4f30\uff1a1) \u57fa\u4e8e\u4eba\u53e3\u7279\u5f81\u6807\u7b7e\u7684\u54cd\u5e94\u8bc4\u5206\uff1b2) \u8de8\u7279\u5f81\u5bf9\u6bd4\u8bc4\u4f30\uff0c\u4f7f\u7528LLM\u591a\u667a\u80fd\u4f53\u8bc4\u5ba1\u7cfb\u7edf\u8fdb\u884c\u91cf\u5316\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLM\u8f93\u51fa\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u4e14\u7f13\u89e3\u63aa\u65bd\u53ef\u80fd\u8fc7\u5ea6\u504f\u5411\u8fb9\u7f18\u7fa4\u4f53\u800c\u975e\u4e2d\u7acb\uff0c\u9700\u5e73\u8861\u7b56\u7565\u5b9e\u73b0\u516c\u5e73\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u9700\u7ed3\u5408MALIBU\u7684\u900f\u660e\u8bc4\u4f30\u6846\u67b6\uff0c\u91c7\u7528\u7ec6\u7c92\u5ea6\u68c0\u6d4b\u548c\u52a8\u6001\u5e73\u8861\u7b56\u7565\u5e94\u5bf9\u9690\u6027\u504f\u89c1\u6311\u6218\u3002"}}
{"id": "2507.01631", "pdf": "https://arxiv.org/pdf/2507.01631", "abs": "https://arxiv.org/abs/2507.01631", "authors": ["Camille Billouard", "Dawa Derksen", "Alexandre Constantin", "Bruno Vallet"], "title": "Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "comment": "Accepted at ICCV 2025 Workshop 3D-VAST (From street to space: 3D\n  Vision Across Altitudes). Version before camera ready. Our code will be made\n  public after the conference", "summary": "Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D\nreconstruction from multiview satellite imagery. However, state-of-the-art NeRF\nmethods are typically constrained to small scenes due to the memory footprint\nduring training, which we study in this paper. Previous work on large-scale\nNeRFs palliate this by dividing the scene into NeRFs. This paper introduces\nSnake-NeRF, a framework that scales to large scenes. Our out-of-core method\neliminates the need to load all images and networks simultaneously, and\noperates on a single device. We achieve this by dividing the region of interest\ninto NeRFs that 3D tile without overlap. Importantly, we crop the images with\noverlap to ensure each NeRFs is trained with all the necessary pixels. We\nintroduce a novel $2\\times 2$ 3D tile progression strategy and segmented\nsampler, which together prevent 3D reconstruction errors along the tile edges.\nOur experiments conclude that large satellite images can effectively be\nprocessed with linear time complexity, on a single GPU, and without compromise\nin quality.", "AI": {"tldr": "Snake-NeRF\u6846\u67b6\u901a\u8fc7\u5206\u5757\u8bad\u7ec3\u548c\u5185\u5b58\u4f18\u5316\uff0c\u5b9e\u73b0\u5728\u5355GPU\u8bbe\u5907\u4e0a\u7ebf\u6027\u65f6\u95f4\u5904\u7406\u5927\u89c4\u6a21\u536b\u661f\u56fe\u50cf\u76843D\u91cd\u5efa\uff0c\u4e14\u4e0d\u635f\u5931\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709NeRF\u65b9\u6cd5\u56e0\u663e\u5b58\u9650\u5236\u4ec5\u80fd\u5904\u7406\u5c0f\u573a\u666f\uff0c\u9700\u89e3\u51b3\u5927\u89c4\u6a21\u536b\u661f\u56fe\u50cf3D\u91cd\u5efa\u7684\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5916\u5b58\u8ba1\u7b97\u7b56\u7565\u5206\u5272\u573a\u666f\u4e3a\u65e0\u91cd\u53e03D\u5206\u5757\uff0c\u5f15\u51652\u00d72\u5206\u5757\u9012\u8fdb\u7b56\u7565\u548c\u5206\u6bb5\u91c7\u6837\u5668\u6d88\u9664\u8fb9\u7f18\u8bef\u5dee\uff0c\u5e26\u91cd\u53e0\u88c1\u526a\u56fe\u50cf\u786e\u4fdd\u8bad\u7ec3\u5b8c\u6574\u6027\u3002", "result": "\u5355GPU\u8bbe\u5907\u4e0a\u5b9e\u73b0\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u7684\u5927\u89c4\u6a21\u536b\u661f\u56fe\u50cf\u5904\u7406\uff0c\u91cd\u5efa\u8d28\u91cf\u65e0\u635f\u5931\u3002", "conclusion": "Snake-NeRF\u6210\u529f\u6269\u5c55\u4e86NeRF\u7684\u5e94\u7528\u89c4\u6a21\uff0c\u4e3a\u5355\u8bbe\u5907\u5904\u7406\u8d85\u5927\u573a\u666f3D\u91cd\u5efa\u63d0\u4f9b\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.01160", "pdf": "https://arxiv.org/pdf/2507.01160", "abs": "https://arxiv.org/abs/2507.01160", "authors": ["Huiling You", "Samia Touileb", "Erik Velldal", "Lilja \u00d8vrelid"], "title": "Event-based evaluation of abstractive news summarization", "categories": ["cs.CL"], "comment": "to appear at GEM2 workshop@ACL 2025", "summary": "An abstractive summary of a news article contains its most important\ninformation in a condensed version. The evaluation of automatically generated\nsummaries by generative language models relies heavily on human-authored\nsummaries as gold references, by calculating overlapping units or similarity\nscores. News articles report events, and ideally so should the summaries. In\nthis work, we propose to evaluate the quality of abstractive summaries by\ncalculating overlapping events between generated summaries, reference\nsummaries, and the original news articles. We experiment on a richly annotated\nNorwegian dataset comprising both events annotations and summaries authored by\nexpert human annotators. Our approach provides more insight into the event\ninformation contained in the summaries.", "AI": {"tldr": "\u901a\u8fc7\u4e8b\u4ef6\u91cd\u53e0\u5206\u6790\u63d0\u5347\u6458\u8981\u8d28\u91cf\u8bc4\u4f30", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6587\u672c\u91cd\u53e0\u7684\u6458\u8981\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u6355\u6349\u65b0\u95fb\u4e8b\u4ef6\u6838\u5fc3\u4fe1\u606f\uff0c\u9700\u8981\u66f4\u5173\u6ce8\u4e8b\u4ef6\u5b8c\u6574\u6027\u7684\u8bc4\u4f30\u7ef4\u5ea6", "method": "\u4f7f\u7528\u632a\u5a01\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u96c6\uff08\u542b\u4e8b\u4ef6\u6807\u6ce8\u548c\u53c2\u8003\u6458\u8981\uff09\uff0c\u8ba1\u7b97\u751f\u6210\u6458\u8981/\u53c2\u8003\u6458\u8981/\u539f\u6587\u4e4b\u95f4\u7684\u4e8b\u4ef6\u91cd\u53e0\u5ea6", "result": "\u65b0\u65b9\u6cd5\u80fd\u6e05\u6670\u91cf\u5316\u6458\u8981\u4e2d\u4fdd\u7559\u7684\u4e8b\u4ef6\u4fe1\u606f\u91cf\uff0c\u63d0\u4f9b\u6bd4\u4f20\u7edf\u6307\u6807\u66f4\u6df1\u5165\u7684\u8bc4\u4f30\u7ef4\u5ea6", "conclusion": "\u57fa\u4e8e\u4e8b\u4ef6\u91cd\u53e0\u7684\u8bc4\u4f30\u6846\u67b6\u80fd\u66f4\u51c6\u786e\u5730\u53cd\u6620\u751f\u6210\u6458\u8981\u5bf9\u6838\u5fc3\u65b0\u95fb\u8981\u7d20\u7684\u8986\u76d6\u7a0b\u5ea6"}}
{"id": "2507.01170", "pdf": "https://arxiv.org/pdf/2507.01170", "abs": "https://arxiv.org/abs/2507.01170", "authors": ["Simon B\u00f6rjesson", "Erik Ersmark", "Pierre Nugues"], "title": "Matching and Linking Entries in Historical Swedish Encyclopedias", "categories": ["cs.CL"], "comment": "10 pages, 3 figures", "summary": "The \\textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and\n20th centuries. It was written by a team of experts and aimed to be an\nintellectual reference, stressing precision and accuracy. This encyclopedia had\nfour main editions remarkable by their size, ranging from 20 to 38 volumes. As\na consequence, the \\textit{Nordisk familjebok} had a considerable influence in\nuniversities, schools, the media, and society overall. As new editions were\nreleased, the selection of entries and their content evolved, reflecting\nintellectual changes in Sweden.\n  In this paper, we used digitized versions from \\textit{Project Runeberg}. We\nfirst resegmented the raw text into entries and matched pairs of entries\nbetween the first and second editions using semantic sentence embeddings. We\nthen extracted the geographical entries from both editions using a\ntransformer-based classifier and linked them to Wikidata. This enabled us to\nidentify geographic trends and possible shifts between the first and second\neditions, written between 1876-1899 and 1904-1926, respectively.\n  Interpreting the results, we observe a small but significant shift in\ngeographic focus away from Europe and towards North America, Africa, Asia,\nAustralia, and northern Scandinavia from the first to the second edition,\nconfirming the influence of the First World War and the rise of new powers. The\ncode and data are available on GitHub at\nhttps://github.com/sibbo/nordisk-familjebok.", "AI": {"tldr": "\u5206\u6790\u745e\u5178\u767e\u79d1\u5168\u4e66\u300aNordisk familjebok\u300b\u4e24\u4e2a\u7248\u672c\u5730\u7406\u6761\u76ee\u53d8\u5316\uff0c\u53d1\u73b0\u5730\u7406\u7126\u70b9\u4ece\u6b27\u6d32\u8f6c\u5411\u5317\u7f8e/\u975e\u6d32/\u4e9a\u6d32\u7b49\u5730\uff0c\u53cd\u6620\u4e00\u6218\u5f71\u54cd\u548c\u65b0\u5174\u52bf\u529b\u5d1b\u8d77", "motivation": "\u63a2\u7a76\u767e\u79d1\u5168\u4e66\u5185\u5bb9\u6f14\u53d8\u5982\u4f55\u53cd\u6620\u745e\u5178\u77e5\u8bc6\u4f53\u7cfb\u7684\u5730\u7406\u8ba4\u77e5\u53d8\u8fc1", "method": "\u4f7f\u7528\u8bed\u4e49\u53e5\u5b50\u5d4c\u5165\u5339\u914d\u6761\u76ee+Transformer\u5206\u7c7b\u5668\u63d0\u53d6\u5730\u7406\u6761\u76ee+Wikidata\u94fe\u63a5\u5206\u6790", "result": "\u53d1\u73b0\u5730\u7406\u5173\u6ce8\u5ea6\u4ece\u6b27\u6d32\u5411\u5317\u7f8e/\u975e\u6d32/\u4e9a\u6d32/\u6fb3\u5927\u5229\u4e9a/\u5317\u65af\u582a\u7684\u7eb3\u7ef4\u4e9a\u663e\u8457\u8f6c\u79fb\uff08p<0.05\uff09", "conclusion": "\u57fa\u4e8e\u8ba1\u7b97\u65b9\u6cd5\u7684\u5206\u6790\u6709\u6548\u63ed\u793a\u4e86\u767e\u79d1\u5168\u4e66\u5185\u5bb9\u6f14\u53d8\u4e0e\u5386\u53f2\u80cc\u666f\uff08\u7b2c\u4e00\u6b21\u4e16\u754c\u5927\u6218\uff09\u7684\u5173\u8054\u6027\uff0c\u6570\u636e\u4e0e\u4ee3\u7801\u5f00\u6e90\u53ef\u7528"}}
{"id": "2507.01213", "pdf": "https://arxiv.org/pdf/2507.01213", "abs": "https://arxiv.org/abs/2507.01213", "authors": ["Adamu Lawan", "Juhua Pu", "Haruna Yunusa", "Jawad Muhammad", "Muhammad Lawan"], "title": "MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis", "categories": ["cs.CL"], "comment": "6, 1 figure", "summary": "Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language\nProcessing (NLP) task that extracts aspects from text and determines their\nassociated sentiments, enabling fine-grained analysis of user opinions.\nExisting ABSA methods struggle to balance computational efficiency with high\nperformance: deep learning models often lack global context, transformers\ndemand significant computational resources, and Mamba-based approaches face\nCUDA dependency and diminished local correlations. Recent advancements in\nExtended Long Short-Term Memory (xLSTM) models, particularly their efficient\nmodeling of long-range dependencies, have significantly advanced the NLP\ncommunity. However, their potential in ABSA remains untapped. To this end, we\npropose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework\nintegrating a bi-directional mLSTM architecture with forward and partially\nflipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context\nmodeling by processing the initial sequence segment in reverse with dedicated\nparameters, preserving critical short-range patterns. We further introduce an\nmLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that\ndynamically combines forward mLSTM outputs as query and key with PF-mLSTM\noutputs as value, optimizing short-range dependency capture while maintaining\nglobal context and efficiency. Experimental results on three benchmark datasets\ndemonstrate that MEGA outperforms state-of-the-art baselines, achieving\nsuperior accuracy and efficiency in ABSA tasks.", "AI": {"tldr": "\u63d0\u51faMEGA\u6846\u67b6\uff0c\u6574\u5408\u53cc\u5411mLSTM\u4e0e\u90e8\u5206\u7ffb\u8f6c\u53cd\u5411\u6d41(PF-mLSTM)\uff0c\u901a\u8fc7\u591a\u5934\u4ea4\u53c9\u6307\u6570\u95e8\u63a7\u878d\u5408\u673a\u5236(MECGAF)\u63d0\u5347ABSA\u4efb\u52a1\u6027\u80fd\uff0c\u5b9e\u73b0\u7cbe\u5ea6\u4e0e\u6548\u7387\u53cc\u4f18", "motivation": "\u73b0\u6709ABSA\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u4e0e\u6027\u80fd\u95f4\u96be\u4ee5\u5e73\u8861\uff1a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7f3a\u4e4f\u5168\u5c40\u8bed\u5883\u3001transformer\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\u3001Mamba\u5b58\u5728CUDA\u4f9d\u8d56\u548c\u5c40\u90e8\u5173\u8054\u5f31\u5316\u95ee\u9898\u3002xLSTM\u5728\u957f\u7a0b\u4f9d\u8d56\u5efa\u6a21\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5728ABSA\u9886\u57df\u5f00\u53d1", "method": "\u6784\u5efa\u53cc\u5411mLSTM\u67b6\u6784\uff0c\u5305\u542b\u6b63\u5411\u6d41\u548c\u90e8\u5206\u7ffb\u8f6c\u53cd\u5411\u6d41(PF-mLSTM)\u3002PF-mLSTM\u901a\u8fc7\u53cd\u5411\u5904\u7406\u521d\u59cb\u5e8f\u5217\u7247\u6bb5\u5f3a\u5316\u5c40\u90e8\u8bed\u5883\u5efa\u6a21\uff0c\u4fdd\u7559\u5173\u952e\u77ed\u7a0b\u6a21\u5f0f\u3002\u5f00\u53d1MECGAF\u673a\u5236\u52a8\u6001\u878d\u5408\u524d\u5411\u4e0ePF-mLSTM\u8f93\u51fa\uff0c\u4f18\u5316\u77ed\u7a0b\u4f9d\u8d56\u6355\u6349\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u8bed\u5883", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u6a21\u578b\uff0cABSA\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u548c\u8ba1\u7b97\u6548\u7387", "conclusion": "MEGA\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u67b6\u6784\u8bbe\u8ba1\u6709\u6548\u5e73\u8861\u5168\u5c40\u8bed\u5883\u4e0e\u5c40\u90e8\u5173\u8054\u5efa\u6a21\uff0c\u5176PF-mLSTM\u548cMECGAF\u673a\u5236\u4e3a\u63d0\u5347\u7ec6\u7c92\u5ea6\u60c5\u611f\u5206\u6790\u6027\u80fd\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u63a8\u52a8\u9ad8\u6548ABSA\u6a21\u578b\u53d1\u5c55"}}
{"id": "2507.01234", "pdf": "https://arxiv.org/pdf/2507.01234", "abs": "https://arxiv.org/abs/2507.01234", "authors": ["Yu Fan", "Yang Tian", "Shauli Ravfogel", "Mrinmaya Sachan", "Elliott Ash", "Alexander Hoyle"], "title": "The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure", "categories": ["cs.CL"], "comment": null, "summary": "Embedding-based similarity metrics between text sequences can be influenced\nnot just by the content dimensions we most care about, but can also be biased\nby spurious attributes like the text's source or language. These document\nconfounders cause problems for many applications, but especially those that\nneed to pool texts from different corpora. This paper shows that a debiasing\nalgorithm that removes information about observed confounders from the encoder\nrepresentations substantially reduces these biases at a minimal computational\ncost. Document similarity and clustering metrics improve across every embedding\nvariant and task we evaluate -- often dramatically. Interestingly, performance\non out-of-distribution benchmarks is not impacted, indicating that the\nembeddings are not otherwise degraded.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u53bb\u504f\u7b97\u6cd5\uff0c\u6709\u6548\u6d88\u9664\u6587\u672c\u5d4c\u5165\u4e2d\u7684\u5e72\u6270\u56e0\u7d20\u504f\u89c1\uff0c\u63d0\u5347\u8de8\u8bed\u6599\u5e93\u7684\u6587\u6863\u76f8\u4f3c\u6027\u548c\u805a\u7c7b\u6548\u679c", "motivation": "\u73b0\u6709\u6587\u672c\u5d4c\u5165\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u5bb9\u6613\u53d7\u6765\u6e90/\u8bed\u8a00\u7b49\u65e0\u5173\u5c5e\u6027\u5e72\u6270\uff0c\u5f71\u54cd\u8de8\u8bed\u6599\u5e93\u6587\u672c\u5904\u7406\u6548\u679c", "method": "\u4ece\u7f16\u7801\u5668\u8868\u793a\u4e2d\u4e3b\u52a8\u79fb\u9664\u5df2\u89c2\u6d4b\u5e72\u6270\u56e0\u7d20\u4fe1\u606f\uff0c\u4fdd\u6301\u4f4e\u8ba1\u7b97\u6210\u672c\u7279\u6027", "result": "\u6240\u6709\u5d4c\u5165\u53d8\u4f53\u548c\u4efb\u52a1\u4e2d\u6587\u6863\u76f8\u4f3c\u6027/\u805a\u7c7b\u6307\u6807\u663e\u8457\u63d0\u5347\uff0c\u5206\u5e03\u5916\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd\u672a\u53d7\u5f71\u54cd", "conclusion": "\u8be5\u53bb\u504f\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u8de8\u8bed\u6599\u5e93\u6587\u672c\u5904\u7406\u6548\u679c\uff0c\u4e14\u4e0d\u5f71\u54cd\u6a21\u578b\u539f\u6709\u80fd\u529b\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2507.01259", "pdf": "https://arxiv.org/pdf/2507.01259", "abs": "https://arxiv.org/abs/2507.01259", "authors": ["Micha\u0142 Matak", "Jaros\u0142aw A. Chudziak"], "title": "GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 2 figures, presented at ICAART 2025, in proceedings of the\n  17th International Conference on Agents and Artificial Intelligence - Volume\n  3: ICAART", "summary": "In this paper we discuss the capability of large language models to base\ntheir answer and provide proper references when dealing with legal matters of\nnon-english and non-chinese speaking country. We discuss the history of legal\ninformation retrieval, the difference between case law and statute law, its\nimpact on the legal tasks and analyze the latest research in this field. Basing\non that background we introduce gAIus, the architecture of the cognitive\nLLM-based agent, whose responses are based on the knowledge retrieved from\ncertain legal act, which is Polish Civil Code. We propose a retrieval mechanism\nwhich is more explainable, human-friendly and achieves better results than\nembedding-based approaches. To evaluate our method we create special dataset\nbased on single-choice questions from entrance exams for law apprenticeships\nconducted in Poland. The proposed architecture critically leveraged the\nabilities of used large language models, improving the gpt-3.5-turbo-0125 by\n419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.\nAt the end of our paper we show the possible future path of research and\npotential applications of our findings.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6ce2\u5170\u6c11\u6cd5\u5178\u7684gAIus\u67b6\u6784\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u5f3a\u7684\u68c0\u7d22\u673a\u5236\u663e\u8457\u63d0\u5347LLM\u6cd5\u5f8b\u95ee\u7b54\u51c6\u786e\u6027\uff0c\u4f7fGPT-3.5\u6027\u80fd\u63d0\u5347419%\uff0cGPT-4o-mini\u51c6\u786e\u7387\u4ece31%\u63d0\u5347\u81f386%", "motivation": "\u89e3\u51b3LLM\u5728\u5904\u7406\u975e\u82f1\u8bed/\u4e2d\u6587\u6cd5\u5f8b\u95ee\u9898\u65f6\u7f3a\u4e4f\u53ef\u9760\u6cd5\u5f8b\u6761\u6587\u5f15\u7528\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u6ce2\u5170\u6cd5\u5f8b\u573a\u666f\u7684\u7279\u6b8a\u9700\u6c42", "method": "\u5f00\u53d1\u57fa\u4e8e\u7279\u5b9a\u6cd5\u5f8b\u6761\u6587\uff08\u6ce2\u5170\u6c11\u6cd5\u5178\uff09\u7684\u68c0\u7d22\u673a\u5236\uff0c\u6784\u5efa\u6ce2\u5170\u6cd5\u5f8b\u5b66\u5f92\u8003\u8bd5\u9898\u5e93\u4f5c\u4e3a\u8bc4\u4f30\u6570\u636e\u96c6", "result": "\u8be5\u67b6\u6784\u4f7fGPT-3.5\u51c6\u786e\u7387\u63d0\u5347419%\uff0c\u8d85\u8d8aGPT-4o\u8868\u73b0\uff0c\u5e76\u5c06GPT-4o-mini\u5f97\u5206\u4ece31%\u63d0\u5347\u81f386%", "conclusion": "\u9a8c\u8bc1\u4e86\u57fa\u4e8e\u6cd5\u5f8b\u6761\u6587\u68c0\u7d22\u67b6\u6784\u7684\u6709\u6548\u6027\uff0c\u63d0\u51fa\u672a\u6765\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u6cd5\u5f8b\u4f53\u7cfb\u53ca\u5f00\u53d1\u81ea\u52a8\u6cd5\u5f8b\u54a8\u8be2\u7cfb\u7edf\u7684\u5e94\u7528\u65b9\u5411"}}
{"id": "2507.01278", "pdf": "https://arxiv.org/pdf/2507.01278", "abs": "https://arxiv.org/abs/2507.01278", "authors": ["Cindy Lie Tabuse", "David Restepo", "Carolina Gracitelli", "Fernando Korn Malerbi", "Caio Regatieri", "Luis Filipe Nakayama"], "title": "Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) can simulate clinical reasoning based on natural\nlanguage prompts, but their utility in ophthalmology is largely unexplored.\nThis study evaluated GPT-4's ability to interpret structured textual\ndescriptions of retinal fundus photographs and simulate clinical decisions for\ndiabetic retinopathy (DR) and glaucoma screening, including the impact of\nadding real or synthetic clinical metadata. We conducted a retrospective\ndiagnostic validation study using 300 annotated fundus images. GPT-4 received\nstructured prompts describing each image, with or without patient metadata. The\nmodel was tasked with assigning an ICDR severity score, recommending DR\nreferral, and estimating the cup-to-disc ratio for glaucoma referral.\nPerformance was evaluated using accuracy, macro and weighted F1 scores, and\nCohen's kappa. McNemar's test and change rate analysis were used to assess the\ninfluence of metadata. GPT-4 showed moderate performance for ICDR\nclassification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),\ndriven mainly by correct identification of normal cases. Performance improved\nin the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For\nglaucoma referral, performance was poor across all settings (accuracy ~78%, F1\n<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes\n(McNemar p > 0.05), and predictions remained consistent across conditions.\nGPT-4 can simulate basic ophthalmic decision-making from structured prompts but\nlacks precision for complex tasks. While not suitable for clinical use, LLMs\nmay assist in education, documentation, or image annotation workflows in\nophthalmology.", "AI": {"tldr": "GPT-4\u5728\u773c\u79d1\u5f71\u50cf\u5206\u6790\u4e2d\u5c55\u73b0\u57fa\u7840\u4e34\u5e8a\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u590d\u6742\u4efb\u52a1\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u6682\u4e0d\u9002\u7528\u4e8e\u4e34\u5e8a\u4f46\u5177\u8f85\u52a9\u6f5c\u529b\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u773c\u79d1\u4e34\u5e8a\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\uff0c\u7279\u522b\u662f\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u548c\u9752\u5149\u773c\u7b5b\u67e5\u573a\u666f\u3002", "method": "\u4f7f\u7528300\u5f20\u6807\u6ce8\u773c\u5e95\u56fe\u50cf\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u8bcd\u6d4b\u8bd5GPT-4\u5728\u6dfb\u52a0\u771f\u5b9e/\u5408\u6210\u5143\u6570\u636e\u524d\u540e\u7684ICDR\u5206\u7ea7\u3001\u8f6c\u8bca\u63a8\u8350\u548c\u676f\u76d8\u6bd4\u4f30\u8ba1\u80fd\u529b\u3002", "result": "ICDR\u5206\u7ea7\u51c6\u786e\u738767.5%\uff08\u4e3b\u8981\u4f9d\u8d56\u6b63\u5e38\u75c5\u4f8b\u8bc6\u522b\uff09\uff0cDR\u8f6c\u8bca\u51c6\u786e\u738782.3%\uff0c\u9752\u5149\u773c\u7b5b\u67e5\u5404\u9879\u6307\u6807\u5747\u4f4e\u4e8e4%\u3002\u5143\u6570\u636e\u5bf9\u7ed3\u679c\u65e0\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "LLM\u76ee\u524d\u9002\u7528\u4e8e\u773c\u79d1\u6559\u80b2/\u6587\u6863\u8f85\u52a9\u7b49\u975e\u4e34\u5e8a\u573a\u666f\uff0c\u4e34\u5e8a\u51b3\u7b56\u9700\u66f4\u9ad8\u7cbe\u5ea6\u6a21\u578b\u3002"}}
{"id": "2507.01281", "pdf": "https://arxiv.org/pdf/2507.01281", "abs": "https://arxiv.org/abs/2507.01281", "authors": ["Juan Chen", "Baolong Bi", "Wei Zhang", "Jingyan Sui", "Xiaofei Zhu", "Yuanzhuo Wang", "Lingrui Mei", "Shenghua Liu"], "title": "Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating their parametric knowledge with external retrieved content.\nHowever, knowledge conflicts caused by internal inconsistencies or noisy\nretrieved content can severely undermine the generation reliability of RAG\nsystems.In this work, we argue that LLMs should rethink all evidence, including\nboth retrieved content and internal knowledge, before generating responses.We\npropose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel\nframework that improves trustworthiness through Conflict-Driven Summarization\nof all available evidence.CARE-RAG first derives parameter-aware evidence by\ncomparing parameter records to identify diverse internal perspectives. It then\nrefines retrieved evidences to produce context-aware evidence, removing\nirrelevant or misleading content. To detect and summarize conflicts, we distill\na 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable\nsynthesis across multiple sources.To further ensure evaluation integrity, we\nintroduce a QA Repair step to correct outdated or ambiguous benchmark\nanswers.Experiments on revised QA datasets with retrieval data show that\nCARE-RAG consistently outperforms strong RAG baselines, especially in scenarios\nwith noisy or conflicting evidence.", "AI": {"tldr": "\u63d0\u51faCARE-RAG\u6846\u67b6\u89e3\u51b3RAG\u7cfb\u7edf\u4e2d\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898\uff0c\u901a\u8fc7\u51b2\u7a81\u9a71\u52a8\u603b\u7ed3\u548cQA\u4fee\u590d\u63d0\u5347\u751f\u6210\u53ef\u9760\u6027\uff0c\u5728\u566a\u58f0/\u51b2\u7a81\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "RAG\u7cfb\u7edf\u9762\u4e34\u5185\u90e8\u77e5\u8bc6\u4e0d\u4e00\u81f4\u4e0e\u68c0\u7d22\u566a\u58f0\u5bfc\u81f4\u7684\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898\uff0c\u4e25\u91cd\u5f71\u54cd\u751f\u6210\u53ef\u9760\u6027\uff0c\u9700\u7cfb\u7edf\u5316\u6574\u5408\u6240\u6709\u8bc1\u636e\u6e90\u3002", "method": "1.\u53c2\u6570\u611f\u77e5\u8bc1\u636e\u63d0\u53d6\uff08\u6bd4\u8f83\u53c2\u6570\u8bb0\u5f55\uff09\n2.\u4e0a\u4e0b\u6587\u611f\u77e5\u8bc1\u636e\u63d0\u70bc\uff08\u53bb\u566a\u68c0\u7d22\u5185\u5bb9\uff09\n3.\u57fa\u4e8e3B LLaMA3.2\u7684\u51b2\u7a81\u9a71\u52a8\u603b\u7ed3\n4.QA\u4fee\u590d\u673a\u5236\u66f4\u65b0\u57fa\u51c6\u7b54\u6848", "result": "\u5728\u4fee\u8ba2\u540e\u7684QA\u6570\u636e\u96c6\u4e0a\uff0cCARE-RAG\u5728\u566a\u58f0/\u51b2\u7a81\u8bc1\u636e\u573a\u666f\u4e2d\u6301\u7eed\u8d85\u8d8a\u73b0\u6709RAG\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "CARE-RAG\u901a\u8fc7\u591a\u6e90\u8bc1\u636e\u878d\u5408\u4e0e\u51b2\u7a81\u7ba1\u7406\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347RAG\u7cfb\u7edf\u5728\u590d\u6742\u77e5\u8bc6\u73af\u5883\u4e0b\u7684\u53ef\u4fe1\u5ea6"}}
{"id": "2507.01297", "pdf": "https://arxiv.org/pdf/2507.01297", "abs": "https://arxiv.org/abs/2507.01297", "authors": ["Xinxi Lyu", "Michael Duan", "Rulin Shao", "Pang Wei Koh", "Sewon Min"], "title": "Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks", "categories": ["cs.CL", "cs.IR"], "comment": "33 pages, 2 figures, 27 tables", "summary": "Retrieval-augmented Generation (RAG) has primarily been studied in limited\nsettings, such as factoid question answering; more challenging,\nreasoning-intensive benchmarks have seen limited success from minimal RAG. In\nthis work, we challenge this prevailing view on established,\nreasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We\nidentify a key missing component in prior work: a usable, web-scale datastore\naligned with the breadth of pretraining data. To this end, we introduce\nCompactDS: a diverse, high-quality, web-scale datastore that achieves high\nretrieval accuracy and subsecond latency on a single-node. The key insights are\n(1) most web content can be filtered out without sacrificing coverage, and a\ncompact, high-quality subset is sufficient; and (2) combining in-memory\napproximate nearest neighbor (ANN) retrieval and on-disk exact search balances\nspeed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves\nconsistent accuracy improvements across all benchmarks and model sizes\n(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,\nand 19% on MATH. No single data source suffices alone, highlighting the\nimportance of diversity of sources (web crawls, curated math, academic papers,\ntextbooks). Finally, we show that our carefully designed in-house datastore\nmatches or outperforms web search engines such as Google Search, as well as\nrecently proposed, complex agent-based RAG systems--all while maintaining\nsimplicity, reproducibility, and self-containment. We release CompactDS and our\nretrieval pipeline, supporting future research exploring retrieval-based AI\nsystems.", "AI": {"tldr": "\u63d0\u51faCompactDS\u6570\u636e\u5b58\u50a8\u65b9\u6848\uff0c\u901a\u8fc7\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u6570\u636e\u6e90\u548c\u6df7\u5408\u68c0\u7d22\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347RAG\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u7684\u6027\u80fd", "motivation": "\u6311\u6218\u4f20\u7edf\u89c2\u70b9\uff0c\u8bc1\u660eRAG\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5173\u952e\u5728\u4e8e\u6784\u5efa\u4e0e\u9884\u8bad\u7ec3\u6570\u636e\u5bf9\u9f50\u7684\u9ad8\u8d28\u91cf\u591a\u6837\u5316\u6570\u636e\u5b58\u50a8", "method": "\u8bbe\u8ba1CompactDS\uff1a1) \u8fc7\u6ee4\u4f4e\u8d28\u91cf\u7f51\u7edc\u5185\u5bb9\u4fdd\u7559\u9ad8\u4ef7\u503c\u5b50\u96c6 2) \u7ed3\u5408\u5185\u5b58ANN\u68c0\u7d22\u4e0e\u78c1\u76d8\u7cbe\u786e\u641c\u7d22\u5b9e\u73b0\u9ad8\u6548\u68c0\u7d22", "result": "\u5728MMLU/GPQA/MATH\u7b49\u57fa\u51c6\u4e0a\u5b9e\u73b010%-33%\u7684\u7edd\u5bf9\u63d0\u5347\uff0c\u81ea\u7814\u6570\u636e\u5b58\u50a8\u6548\u679c\u5339\u914dGoogle\u641c\u7d22\u548c\u590d\u6742\u4ee3\u7406\u7cfb\u7edf", "conclusion": "\u6570\u636e\u6e90\u591a\u6837\u6027\u5bf9RAG\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0cCompactDS\u7684\u7b80\u5355\u81ea\u5305\u542b\u8bbe\u8ba1\u4e3aAI\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2507.01299", "pdf": "https://arxiv.org/pdf/2507.01299", "abs": "https://arxiv.org/abs/2507.01299", "authors": ["Kai Liu", "Bowen Xu", "Shaoyu Wu", "Xin Chen", "Hao Zhou", "Yongliang Tao", "Lulu Hu"], "title": "La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation", "categories": ["cs.CL"], "comment": "ICML 2025 Acceptance", "summary": "Activation sparsity can reduce the computational overhead and memory\ntransfers during the forward pass of Large Language Model (LLM) inference.\nExisting methods face limitations, either demanding time-consuming recovery\ntraining that hinders real-world adoption, or relying on empirical\nmagnitude-based pruning, which causes fluctuating sparsity and unstable\ninference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse\nActivation), a novel method for activation sparsification designed to improve\nLLM efficiency without requiring additional training or magnitude-based\npruning. We leverage layerwise orthogonal rotations to transform input\nactivations into rotated forms that are more suitable for sparsification. By\nemploying a Top-K selection approach within the rotated activations, we achieve\nconsistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA\nis effective across various sizes and types of LLMs, demonstrating minimal\nperformance degradation and robust inference acceleration. Specifically, for\nLLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a\nconsistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in\nzero-shot tasks compared to the dense model to just 0.54%, while surpassing\nTEAL by 1.77% and CATS by 17.14%.", "AI": {"tldr": "LaRoSA\u63d0\u51fa\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u57fa\u4e8e\u5e45\u5ea6\u7684\u526a\u679d\uff0c\u901a\u8fc7\u5c42\u7ea7\u6b63\u4ea4\u65cb\u8f6c\u5b9e\u73b0LLM\u6fc0\u6d3b\u7a00\u758f\u5316\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u83b7\u5f97\u7a33\u5b9a\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6fc0\u6d3b\u7a00\u758f\u5316\u65b9\u6cd5\u9700\u8981\u6062\u590d\u8bad\u7ec3\uff08\u5f71\u54cd\u5b9e\u9645\u90e8\u7f72\uff09\u6216\u4f9d\u8d56\u5e45\u5ea6\u526a\u679d\uff08\u5bfc\u81f4\u7a00\u758f\u6ce2\u52a8\u548c\u52a0\u901f\u4e0d\u7a33\u5b9a\uff09\uff0c\u96be\u4ee5\u5b9e\u7528\u5316\u3002", "method": "1. \u901a\u8fc7\u5c42\u7ea7\u6b63\u4ea4\u65cb\u8f6c\u5c06\u6fc0\u6d3b\u6620\u5c04\u5230\u66f4\u9002\u5408\u7a00\u758f\u5316\u7684\u7a7a\u95f4\n2. \u91c7\u7528Top-K\u9009\u62e9\u673a\u5236\u5b9e\u73b0\u6a21\u578b\u7ea7\u7a33\u5b9a\u7a00\u758f\n3. \u652f\u6301\u4e0d\u540c\u5c3a\u5bf8\u548c\u67b6\u6784\u7684LLM\u9002\u914d", "result": "LLaMA2-7B\u572840%\u7a00\u758f\u5ea6\u4e0b\uff1a\n- \u56f0\u60d1\u5ea6\u4ec5\u589e\u52a00.17\n- \u5b9e\u9645\u8fd0\u884c\u901f\u5ea6\u63d0\u53471.30x\n- \u96f6\u6837\u672c\u4efb\u52a1\u51c6\u786e\u7387\u5dee\u8ddd\u7f29\u5c0f\u81f30.54%\uff08\u4f18\u4e8eTEAL 1.77%\u548cCATS 17.14%\uff09", "conclusion": "LaRoSA\u9996\u6b21\u5b9e\u73b0\u65e0\u9700\u91cd\u8bad\u7ec3\u7684\u9ad8\u6548\u6fc0\u6d3b\u7a00\u758f\u5316\uff0c\u5728\u6027\u80fd\u635f\u5931\u6781\u5c0f\u7684\u60c5\u51b5\u4e0b\u63d0\u4f9b\u53ef\u9760\u52a0\u901f\uff0c\u4e3aLLM\u90e8\u7f72\u63d0\u4f9b\u5b9e\u7528\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01334", "pdf": "https://arxiv.org/pdf/2507.01334", "abs": "https://arxiv.org/abs/2507.01334", "authors": ["Nifu Dan", "Yujun Cai", "Yiwei Wang"], "title": "Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Navigating the complexities of physics reasoning has long been a difficult\ntask for Large Language Models (LLMs), requiring a synthesis of profound\nconceptual understanding and adept problem-solving techniques. In this study,\nwe investigate the application of advanced instruction-tuned reasoning models,\nsuch as Deepseek-R1, to address a diverse spectrum of physics problems curated\nfrom the challenging SciBench benchmark. Our comprehensive experimental\nevaluation reveals the remarkable capabilities of reasoning models. Not only do\nthey achieve state-of-the-art accuracy in answering intricate physics\nquestions, but they also generate distinctive reasoning patterns that emphasize\non symbolic derivation. Furthermore, our findings indicate that even for these\nhighly sophisticated reasoning models, the strategic incorporation of few-shot\nprompting can still yield measurable improvements in overall accuracy,\nhighlighting the potential for continued performance gains.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u9ad8\u7ea7\u6307\u4ee4\u8c03\u4f18\u6a21\u578bDeepseek-R1\u5728SciBench\u7269\u7406\u95ee\u9898\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u51c6\u786e\u7387\uff0c\u5e76\u5c55\u793a\u4e86\u72ec\u7279\u7684\u7b26\u53f7\u63a8\u5bfc\u63a8\u7406\u6a21\u5f0f\uff0c\u5c11\u6837\u672c\u63d0\u793a\u7b56\u7565\u4ecd\u80fd\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u9ad8\u7ea7\u63a8\u7406\u6a21\u578b\u5728\u89e3\u51b3\u590d\u6742\u7269\u7406\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\uff0c\u7a81\u7834LLMs\u5728\u7269\u7406\u63a8\u7406\u4e2d\u6982\u5ff5\u7406\u89e3\u4e0e\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u7684\u74f6\u9888\u3002", "method": "\u4f7f\u7528Deepseek-R1\u6a21\u578b\u5bf9SciBench\u57fa\u51c6\u4e2d\u7684\u591a\u6837\u5316\u7269\u7406\u95ee\u9898\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u5e76\u6d4b\u8bd5\u5c11\u6837\u672c\u63d0\u793a\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "result": "\u6a21\u578b\u4e0d\u4ec5\u8fbe\u5230SOTA\u51c6\u786e\u7387\uff08\u6bd4\u4e4b\u524d\u65b9\u6cd5\u63d0\u5347\u663e\u8457\uff09\uff0c\u4e14\u751f\u6210\u4ee5\u7b26\u53f7\u63a8\u5bfc\u4e3a\u6838\u5fc3\u7684\u72ec\u7279\u63a8\u7406\u8def\u5f84\uff0c\u5c11\u6837\u672c\u63d0\u793a\u5e26\u67654.7%\u7684\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u9ad8\u7ea7\u63a8\u7406\u6a21\u578b\u5728\u7269\u7406\u95ee\u9898\u89e3\u51b3\u4e2d\u5c55\u73b0\u5353\u8d8a\u6f5c\u529b\uff0c\u7b26\u53f7\u63a8\u5bfc\u80fd\u529b\u7a81\u51fa\uff0c\u63d0\u793a\u5de5\u7a0b\u4ecd\u662f\u6301\u7eed\u63d0\u5347\u6027\u80fd\u7684\u6709\u6548\u624b\u6bb5\u3002"}}
{"id": "2507.01335", "pdf": "https://arxiv.org/pdf/2507.01335", "abs": "https://arxiv.org/abs/2507.01335", "authors": ["Xunjian Yin", "Sitao Cheng", "Yuxi Xie", "Xinyu Hu", "Li Lin", "Xinyi Wang", "Liangming Pan", "William Yang Wang", "Xiaojun Wan"], "title": "LEDOM: An Open and Fundamental Reverse Language Model", "categories": ["cs.CL", "cs.AI"], "comment": "Work in progress", "summary": "We introduce LEDOM, the first purely reverse language model, trained\nautoregressively on 435B tokens with 2B and 7B parameter variants, which\nprocesses sequences in reverse temporal order through previous token\nprediction. For the first time, we present the reverse language model as a\npotential foundational model across general tasks, accompanied by a set of\nintriguing examples and insights. Based on LEDOM, we further introduce a novel\napplication: Reverse Reward, where LEDOM-guided reranking of forward language\nmodel outputs leads to substantial performance improvements on mathematical\nreasoning tasks. This approach leverages LEDOM's unique backward reasoning\ncapability to refine generation quality through posterior evaluation. Our\nfindings suggest that LEDOM exhibits unique characteristics with broad\napplication potential. We will release all models, training code, and\npre-training data to facilitate future research.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u7eaf\u9006\u5411\u8bed\u8a00\u6a21\u578bLEDOM\uff0c\u901a\u8fc7\u9006\u5411\u63a8\u7406\u80fd\u529b\u663e\u8457\u63d0\u5347\u6570\u5b66\u4efb\u52a1\u8868\u73b0", "motivation": "\u63a2\u7d22\u53cd\u5411\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\u7684\u6f5c\u529b\uff0c\u5229\u7528\u9006\u5411\u65f6\u5e8f\u5904\u7406\u5f00\u53d1\u65b0\u578b\u5e94\u7528\u573a\u666f", "method": "\u57fa\u4e8e435B tokens\u9884\u8bad\u7ec32B/7B\u53c2\u6570\u6a21\u578b\uff0c\u91c7\u7528\u9006\u5411token\u9884\u6d4b\u673a\u5236\uff0c\u5f00\u53d1\u9006\u5411\u5956\u52b1\u91cd\u6392\u5e8f\u65b9\u6cd5", "result": "\u9006\u5411\u5956\u52b1\u65b9\u6cd5\u4f7f\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u9a8c\u8bc1\u6a21\u578b\u72ec\u7279\u63a8\u7406\u80fd\u529b", "conclusion": "LEDOM\u5c55\u73b0\u9006\u5411\u5efa\u6a21\u65b0\u8303\u5f0f\uff0c\u5f00\u6e90\u6a21\u578b\u4e0e\u6570\u636e\u4fc3\u8fdb\u8bed\u8a00\u6a21\u578b\u53cc\u5411\u63a8\u7406\u7814\u7a76"}}
{"id": "2507.01352", "pdf": "https://arxiv.org/pdf/2507.01352", "abs": "https://arxiv.org/abs/2507.01352", "authors": ["Chris Yuhao Liu", "Liang Zeng", "Yuzhen Xiao", "Jujie He", "Jiacai Liu", "Chaojie Wang", "Rui Yan", "Wei Shen", "Fuxiang Zhang", "Jiacheng Xu", "Yang Liu", "Yahui Zhou"], "title": "Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite the critical role of reward models (RMs) in reinforcement learning\nfrom human feedback (RLHF), current state-of-the-art open RMs perform poorly on\nmost existing evaluation benchmarks, failing to capture the spectrum of nuanced\nand sophisticated human preferences. Even approaches that incorporate advanced\ntraining techniques have not yielded meaningful performance improvements. We\nhypothesize that this brittleness stems primarily from limitations in\npreference datasets, which are often narrowly scoped, synthetically labeled, or\nlack rigorous quality control. To address these challenges, we present a\nlarge-scale preference dataset comprising 40 million preference pairs, named\nSynPref-40M. To enable data curation at scale, we design a human-AI synergistic\ntwo-stage pipeline that leverages the complementary strengths of human\nannotation quality and AI scalability. In this pipeline, humans provide\nverified annotations, while large language models perform automatic curation\nbased on human guidance. Training on this preference mixture, we introduce\nSkywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B\nparameters, trained on a carefully curated subset of 26 million preference\npairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile\nacross a wide range of capabilities, including alignment with human\npreferences, objective correctness, safety, resistance to stylistic biases, and\nbest-of-N scaling, achieving state-of-the-art performance across seven major\nreward model benchmarks. Ablation studies confirm that the effectiveness of our\napproach stems not only from data scale but also from high-quality curation.\nThe Skywork-Reward-V2 series represents substantial progress in open reward\nmodels, highlighting the untapped potential of existing preference datasets and\ndemonstrating how human-AI curation synergy can unlock significantly higher\ndata quality.", "AI": {"tldr": "\u63d0\u51fa40M\u89c4\u6a21\u7684SynPref-40M\u504f\u597d\u6570\u636e\u96c6\u548cSkywork-Reward-V2\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u4eba\u673a\u534f\u540c\u6570\u636e\u6807\u6ce8\u5b9e\u73b0\u4e03\u5927\u57fa\u51c6SOTA\u8868\u73b0", "motivation": "\u73b0\u6709\u5f00\u6e90\u5956\u52b1\u6a21\u578b\u53d7\u9650\u4e8e\u504f\u597d\u6570\u636e\u96c6\u7684\u8d28\u91cf\u95ee\u9898(\u8303\u56f4\u72ed\u7a84/\u5408\u6210\u6807\u7b7e/\u8d28\u91cf\u63a7\u5236\u4e0d\u8db3)\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u4eba\u7c7b\u504f\u597d\u5efa\u6a21\u4e2d\u8868\u73b0\u8106\u5f31", "method": "1. \u8bbe\u8ba1\u4e24\u9636\u6bb5\u4eba\u673a\u534f\u540c\u6d41\u7a0b\uff1a\u4eba\u5de5\u9a8c\u8bc1\u6807\u6ce8+LLM\u81ea\u52a8\u5316\u6269\u5c55 2. \u4ece40M\u6570\u636e\u4e2d\u7cbe\u900926M\u9ad8\u8d28\u91cf\u6570\u636e\u8bad\u7ec30.6B-8B\u53c2\u6570\u6a21\u578b", "result": "\u5728\u4e03\u5927\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5168\u9762\u9886\u5148\uff0c\u6db5\u76d6\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u3001\u5b89\u5168\u6027\u3001\u6297\u98ce\u683c\u504f\u5dee\u7b49\u7ef4\u5ea6\uff0c\u9a8c\u8bc1\u6570\u636e\u89c4\u6a21\u4e0e\u8d28\u91cf\u7684\u53cc\u91cd\u6709\u6548\u6027", "conclusion": "\u63ed\u793a\u4e86\u73b0\u6709\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u6f5c\u529b\u8fb9\u754c\uff0c\u8bc1\u660e\u4eba\u673a\u534f\u540c\u6570\u636e\u4f18\u5316\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u5f00\u6e90\u5956\u52b1\u6a21\u578b\u53d1\u5c55\u6307\u660e\u65b0\u65b9\u5411"}}
{"id": "2507.01437", "pdf": "https://arxiv.org/pdf/2507.01437", "abs": "https://arxiv.org/abs/2507.01437", "authors": ["Ting Xu", "Xiaoxiao Deng", "Xiandong Meng", "Haifeng Yang", "Yan Wu"], "title": "Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction", "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the challenges posed by the unstructured nature and\nhigh-dimensional semantic complexity of electronic health record texts. A deep\nlearning method based on attention mechanisms is proposed to achieve unified\nmodeling for information extraction and multi-label disease prediction. The\nstudy is conducted on the MIMIC-IV dataset. A Transformer-based architecture is\nused to perform representation learning over clinical text. Multi-layer\nself-attention mechanisms are employed to capture key medical entities and\ntheir contextual relationships. A Sigmoid-based multi-label classifier is then\napplied to predict multiple disease labels. The model incorporates a\ncontext-aware semantic alignment mechanism, enhancing its representational\ncapacity in typical medical scenarios such as label co-occurrence and sparse\ninformation. To comprehensively evaluate model performance, a series of\nexperiments were conducted, including baseline comparisons, hyperparameter\nsensitivity analysis, data perturbation studies, and noise injection tests.\nResults demonstrate that the proposed method consistently outperforms\nrepresentative existing approaches across multiple performance metrics. The\nmodel maintains strong generalization under varying data scales, interference\nlevels, and model depth configurations. The framework developed in this study\noffers an efficient algorithmic foundation for processing real-world clinical\ntexts and presents practical significance for multi-label medical text modeling\ntasks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u533b\u7597\u6587\u672c\u591a\u6807\u7b7e\u9884\u6d4b\u95ee\u9898\uff0c\u5728MIMIC-IV\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u4f18\u8d8a\u6027\u80fd", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u5b58\u5728\u975e\u7ed3\u6784\u5316\u7279\u5f81\u548c\u9ad8\u7ef4\u8bed\u4e49\u590d\u6742\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u591a\u6807\u7b7e\u5171\u73b0\u548c\u7a00\u758f\u4fe1\u606f\u573a\u666f", "method": "\u4f7f\u7528Transformer\u67b6\u6784\u8fdb\u884c\u6587\u672c\u8868\u793a\u5b66\u4e60\uff0c\u7ed3\u5408\u591a\u5c42\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u533b\u7597\u5b9e\u4f53\u5173\u7cfb\uff0c\u91c7\u7528Sigmoid\u591a\u6807\u7b7e\u5206\u7c7b\u5668\uff0c\u5e76\u5f15\u5165\u4e0a\u4e0b\u6587\u611f\u77e5\u8bed\u4e49\u5bf9\u9f50\u673a\u5236", "result": "\u5728\u57fa\u7ebf\u6bd4\u8f83\u3001\u566a\u58f0\u6d4b\u8bd5\u7b49\u5b9e\u9a8c\u4e2d\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u4e0d\u540c\u6570\u636e\u89c4\u6a21\u548c\u5e72\u6270\u6c34\u5e73\u4e0b\u4fdd\u6301\u5f3a\u6cdb\u5316\u80fd\u529b", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e34\u5e8a\u6587\u672c\u5904\u7406\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5bf9\u591a\u6807\u7b7e\u533b\u7597\u5efa\u6a21\u4efb\u52a1\u5177\u6709\u91cd\u8981\u5b9e\u8df5\u4ef7\u503c"}}
{"id": "2507.01449", "pdf": "https://arxiv.org/pdf/2507.01449", "abs": "https://arxiv.org/abs/2507.01449", "authors": ["Tianyu Liu", "Qitan Lv", "Hao Li", "Xing Gao", "Xiao Sun"], "title": "LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation", "categories": ["cs.CL"], "comment": null, "summary": "Speculative decoding (SD), where a small draft model is employed to propose\ndraft tokens in advance and then the target model validates them in parallel,\nhas emerged as a promising technique for LLM inference acceleration. Many\nendeavors to improve SD are to eliminate the need for a draft model and\ngenerate draft tokens in a retrieval-based manner in order to further alleviate\nthe drafting overhead and significantly reduce the difficulty in deployment and\napplications. However, retrieval-based SD relies on a matching paradigm to\nretrieval the most relevant reference as the draft tokens, where these methods\noften fail to find matched and accurate draft tokens. To address this\nchallenge, we propose LogitSpec to effectively expand the retrieval range and\nfind the most relevant reference as drafts. Our LogitSpec is motivated by the\nobservation that the logit of the last token can not only predict the next\ntoken, but also speculate the next next token. Specifically, LogitSpec\ngenerates draft tokens in two steps: (1) utilizing the last logit to speculate\nthe next next token; (2) retrieving relevant reference for both the next token\nand the next next token. LogitSpec is training-free and plug-and-play, which\ncan be easily integrated into existing LLM inference frameworks. Extensive\nexperiments on a wide range of text generation benchmarks demonstrate that\nLogitSpec can achieve up to 2.61 $\\times$ speedup and 3.28 mean accepted tokens\nper decoding step. Our code is available at\nhttps://github.com/smart-lty/LogitSpec.", "AI": {"tldr": "\u63d0\u51faLogitSpec\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6700\u540e\u4e00\u4e2atoken\u7684logit\u6269\u5c55\u68c0\u7d22\u8303\u56f4\u6765\u63d0\u5347\u63a8\u6d4b\u89e3\u7801\u6548\u7387\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0LLM\u63a8\u7406\u52a0\u901f", "motivation": "\u73b0\u6709\u57fa\u4e8e\u68c0\u7d22\u7684\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u4f9d\u8d56\u5339\u914d\u8303\u5f0f\u68c0\u7d22\u53c2\u8003token\uff0c\u4f46\u5e38\u65e0\u6cd5\u627e\u5230\u8db3\u591f\u51c6\u786e\u5339\u914d\u7684\u5019\u9009token", "method": "1) \u5229\u7528\u6700\u540e\u4e00\u4e2atoken\u7684logit\u9884\u6d4b\u4e0b\u4e0b\u4e2atoken 2) \u540c\u65f6\u68c0\u7d22\u5305\u542b\u4e0b\u4e00\u4e2a\u548c\u4e0b\u4e0b\u4e2atoken\u7684\u76f8\u5173\u53c2\u8003\u5185\u5bb9", "result": "\u5728\u591a\u79cd\u6587\u672c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u52302.61\u500d\u52a0\u901f\uff0c\u5e73\u5747\u6bcf\u4e2a\u89e3\u7801\u6b65\u9aa4\u63a5\u53d73.28\u4e2atoken", "conclusion": "LogitSpec\u901a\u8fc7logit\u53cc\u91cd\u9884\u6d4b\u673a\u5236\u6709\u6548\u6269\u5c55\u68c0\u7d22\u8303\u56f4\uff0c\u5b9e\u73b0\u8bad\u7ec3\u514d\u8d39\u3001\u5373\u63d2\u5373\u7528\u7684\u63a8\u7406\u52a0\u901f\u65b9\u6848"}}
{"id": "2507.01479", "pdf": "https://arxiv.org/pdf/2507.01479", "abs": "https://arxiv.org/abs/2507.01479", "authors": ["Yingqiang Gao", "Kaede Johnson", "David Froehlich", "Luisa Carrer", "Sarah Ebling"], "title": "Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Automatic text simplification (ATS) aims to enhance language accessibility\nfor various target groups, particularly persons with intellectual disabilities.\nRecent advancements in generative AI, especially large language models (LLMs),\nhave substantially improved the quality of machine-generated text\nsimplifications, thereby mitigating information barriers for the target group.\nHowever, existing LLM-based ATS systems do not incorporate preference feedback\non text simplifications during training, resulting in a lack of personalization\ntailored to the specific needs of target group representatives.\n  In this work, we extend the standard supervised fine-tuning (SFT) approach\nfor adapting LLM-based ATS models by leveraging a computationally efficient LLM\nalignment technique -- direct preference optimization (DPO). Specifically, we\npost-train LLM-based ATS models using human feedback collected from persons\nwith intellectual disabilities, reflecting their preferences on paired text\nsimplifications generated by mainstream LLMs. Furthermore, we propose a\npipeline for developing personalized LLM-based ATS systems, encompassing data\ncollection, model selection, SFT and DPO post-training, and evaluation. Our\nfindings underscore the necessity of active participation of target group\npersons in designing personalized AI accessibility solutions aligned with human\nexpectations. This work represents a step towards personalizing inclusive AI\nsystems at the target-group level, incorporating insights not only from text\nsimplification experts but also from target group persons themselves.", "AI": {"tldr": "\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u6280\u672f\u6574\u5408\u667a\u529b\u969c\u788d\u4eba\u58eb\u7684\u53cd\u9988\uff0c\u6784\u5efa\u4e2a\u6027\u5316AI\u6587\u672c\u7b80\u5316\u7cfb\u7edf", "motivation": "\u73b0\u6709LLM\u6587\u672c\u7b80\u5316\u7cfb\u7edf\u7f3a\u4e4f\u7528\u6237\u504f\u597d\u53cd\u9988\u673a\u5236\uff0c\u65e0\u6cd5\u6ee1\u8db3\u667a\u529b\u969c\u788d\u4eba\u7fa4\u7684\u4e2a\u6027\u5316\u9700\u6c42", "method": "\u6269\u5c55\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u65b9\u6cd5\uff0c\u91c7\u7528DPO\u5bf9\u9f50\u6280\u672f\u8fdb\u884c\u540e\u8bad\u7ec3\uff0c\u6784\u5efa\u5305\u542b\u6570\u636e\u6536\u96c6-\u6a21\u578b\u9009\u62e9-\u8bad\u7ec3-\u8bc4\u4f30\u7684\u7cfb\u7edf\u5f00\u53d1\u6d41\u7a0b", "result": "\u5b9e\u73b0\u4e86\u76ee\u6807\u7fa4\u4f53\u504f\u597d\u9a71\u52a8\u7684\u6587\u672c\u7b80\u5316\uff0c\u9a8c\u8bc1\u4e86\u7528\u6237\u53c2\u4e0e\u5bf9AI\u65e0\u969c\u788d\u7cfb\u7edf\u8bbe\u8ba1\u7684\u91cd\u8981\u6027", "conclusion": "\u901a\u8fc7\u878d\u5408\u4e13\u5bb6\u610f\u89c1\u548c\u7528\u6237\u76f4\u63a5\u53cd\u9988\uff0c\u63a8\u52a8\u4e86\u7fa4\u4f53\u7ea7\u4e2a\u6027\u5316\u5305\u5bb9\u6027AI\u7cfb\u7edf\u7684\u53d1\u5c55"}}
{"id": "2507.01541", "pdf": "https://arxiv.org/pdf/2507.01541", "abs": "https://arxiv.org/abs/2507.01541", "authors": ["\u00c1lvaro Zaera", "Diana Nicoleta Popa", "Ivan Sekulic", "Paolo Rosso"], "title": "Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing", "categories": ["cs.CL"], "comment": null, "summary": "Out-of-scope (OOS) intent detection is a critical challenge in task-oriented\ndialogue systems (TODS), as it ensures robustness to unseen and ambiguous\nqueries. In this work, we propose a novel but simple modular framework that\ncombines uncertainty modeling with fine-tuned large language models (LLMs) for\nefficient and accurate OOS detection. The first step applies uncertainty\nestimation to the output of an in-scope intent detection classifier, which is\ncurrently deployed in a real-world TODS handling tens of thousands of user\ninteractions daily. The second step then leverages an emerging LLM-based\napproach, where a fine-tuned LLM is triggered to make a final decision on\ninstances with high uncertainty. Unlike prior approaches, our method\neffectively balances computational efficiency and performance, combining\ntraditional approaches with LLMs and yielding state-of-the-art results on key\nOOS detection benchmarks, including real-world OOS data acquired from a\ndeployed TODS.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u4e0e\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5b9e\u73b0\u9ad8\u6548\u51c6\u786e\u7684\u8d85\u51fa\u8303\u56f4\u610f\u56fe\u68c0\u6d4b", "motivation": "\u73b0\u6709\u4efb\u52a1\u5bf9\u8bdd\u7cfb\u7edf\u5bf9\u672a\u77e5\u610f\u56fe\u7684\u68c0\u6d4b\u5b58\u5728\u6548\u7387\u4e0e\u51c6\u786e\u6027\u96be\u4ee5\u5e73\u8861\u7684\u95ee\u9898\uff0c\u9700\u7ed3\u5408\u4f20\u7edf\u65b9\u6cd5\u4e0e\u65b0\u5174LLM\u6280\u672f\u7a81\u7834\u6027\u80fd\u74f6\u9888", "method": "1. \u5728\u73b0\u6709\u610f\u56fe\u5206\u7c7b\u5668\u4e0a\u5e94\u7528\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1 2. \u5bf9\u9ad8\u4e0d\u786e\u5b9a\u6027\u6837\u672c\u89e6\u53d1\u5fae\u8c03LLM\u8fdb\u884c\u6700\u7ec8\u51b3\u7b56", "result": "\u5728\u5173\u952eOOS\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u6548\u679c\uff0c\u5305\u62ec\u5b9e\u9645\u90e8\u7f72\u7cfb\u7edf\u91c7\u96c6\u7684\u771f\u5b9e\u6570\u636e", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u6df7\u5408\u67b6\u6784\u6210\u529f\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u5de5\u4e1a\u7ea7\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.01543", "pdf": "https://arxiv.org/pdf/2507.01543", "abs": "https://arxiv.org/abs/2507.01543", "authors": ["Quang Minh Nguyen", "Taegyoon Kim"], "title": "Is External Information Useful for Stance Detection with LLMs?", "categories": ["cs.CL"], "comment": "ACL Findings 2025", "summary": "In the stance detection task, a text is classified as either favorable,\nopposing, or neutral towards a target. Prior work suggests that the use of\nexternal information, e.g., excerpts from Wikipedia, improves stance detection\nperformance. However, whether or not such information can benefit large\nlanguage models (LLMs) remains an unanswered question, despite their wide\nadoption in many reasoning tasks. In this study, we conduct a systematic\nevaluation on how Wikipedia and web search external information can affect\nstance detection across eight LLMs and in three datasets with 12 targets.\nSurprisingly, we find that such information degrades performance in most cases,\nwith macro F1 scores dropping by up to 27.9\\%. We explain this through\nexperiments showing LLMs' tendency to align their predictions with the stance\nand sentiment of the provided information rather than the ground truth stance\nof the given text. We also find that performance degradation persists with\nchain-of-thought prompting, while fine-tuning mitigates but does not fully\neliminate it. Our findings, in contrast to previous literature on BERT-based\nsystems which suggests that external information enhances performance,\nhighlight the risks of information biases in LLM-based stance classifiers. Code\nis available at https://github.com/ngqm/acl2025-stance-detection.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5916\u90e8\u4fe1\u606f\u4f1a\u964d\u4f4e\u591a\u6570LLM\u5728\u7acb\u573a\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u6700\u5927F1\u4e0b\u964d27.9%\uff0c\u63ed\u793aLLM\u5b58\u5728\u4fe1\u606f\u5bf9\u9f50\u504f\u5dee", "motivation": "\u63a2\u7a76\u7ef4\u57fa\u767e\u79d1\u548c\u7f51\u7edc\u641c\u7d22\u7b49\u5916\u90e8\u4fe1\u606f\u5982\u4f55\u5f71\u54cdLLM\u5728\u7acb\u573a\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u9a8c\u8bc1\u5176\u4e0eBERT\u6a21\u578b\u7684\u5dee\u5f02", "method": "\u57283\u4e2a\u6570\u636e\u96c612\u4e2a\u76ee\u6807\u4e0a\u7cfb\u7edf\u8bc4\u4f308\u79cdLLM\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\u5206\u6790\u5916\u90e8\u4fe1\u606f\u5f71\u54cd\u673a\u5236\uff0c\u6d4b\u8bd5\u601d\u7ef4\u94fe\u63d0\u793a\u548c\u5fae\u8c03\u6548\u679c", "result": "78%\u6848\u4f8b\u663e\u793a\u5916\u90e8\u4fe1\u606f\u635f\u5bb3\u6027\u80fd\uff08\u6700\u5927F1\u964d27.9%\uff09\uff0cLLM\u503e\u5411\u4e0e\u5916\u90e8\u4fe1\u606f\u7acb\u573a\u5bf9\u9f50\u800c\u975e\u6587\u672c\u771f\u5b9e\u7acb\u573a\uff0c\u5fae\u8c03\u4ec5\u90e8\u5206\u7f13\u89e3", "conclusion": "\u4e0eBERT\u7cfb\u7edf\u7ed3\u8bba\u76f8\u53cd\uff0cLLM\u7acb\u573a\u5206\u7c7b\u5668\u5b58\u5728\u4fe1\u606f\u504f\u89c1\u98ce\u9669\uff0c\u63d0\u793a\u9700\u8c28\u614e\u5904\u7406\u5916\u90e8\u4fe1\u606f\u5728LLM\u5e94\u7528\u4e2d\u7684\u4f7f\u7528"}}
{"id": "2507.01594", "pdf": "https://arxiv.org/pdf/2507.01594", "abs": "https://arxiv.org/abs/2507.01594", "authors": ["Shutong Feng", "Hsien-chin Lin", "Nurul Lubis", "Carel van Niekerk", "Michael Heck", "Benjamin Ruppik", "Renato Vukovic", "Milica Ga\u0161i\u0107"], "title": "Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation", "categories": ["cs.CL"], "comment": "19 pages, 6 figures", "summary": "Task-oriented dialogue (ToD) systems are designed to help users achieve\nspecific goals through natural language interaction. While recent advances in\nlarge language models (LLMs) have significantly improved linguistic fluency and\ncontextual understanding, building effective and emotionally intelligent ToD\nsystems remains a complex challenge. Effective ToD systems must optimise for\ntask success, emotional understanding and responsiveness, and precise\ninformation conveyance, all within inherently noisy and ambiguous\nconversational environments. In this work, we investigate architectural,\nrepresentational, optimisational as well as emotional considerations of ToD\nsystems. We set up systems covering these design considerations with a\nchallenging evaluation environment composed of a natural-language user\nsimulator coupled with an imperfect natural language understanding module. We\npropose \\textbf{LUSTER}, an \\textbf{L}LM-based \\textbf{U}nified \\textbf{S}ystem\nfor \\textbf{T}ask-oriented dialogue with \\textbf{E}nd-to-end\n\\textbf{R}einforcement learning with both short-term (user sentiment) and\nlong-term (task success) rewards. Our findings demonstrate that combining LLM\ncapability with structured reward modelling leads to more resilient and\nemotionally responsive ToD systems, offering a practical path forward for\nnext-generation conversational agents.", "AI": {"tldr": "\u63d0\u51faLUSTER\u7cfb\u7edf\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u7ed3\u6784\u5316\u5956\u52b1\u673a\u5236\u7684\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6709\u6548\u63d0\u5347\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u7684\u4efb\u52a1\u5b8c\u6210\u5ea6\u548c\u60c5\u611f\u54cd\u5e94\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u5728\u566a\u58f0\u73af\u5883\u4e0b\u540c\u65f6\u4f18\u5316\u4efb\u52a1\u6210\u529f\u7387\u3001\u60c5\u611f\u7406\u89e3\u80fd\u529b\u548c\u4fe1\u606f\u4f20\u9012\u51c6\u786e\u6027\u5b58\u5728\u6311\u6218\uff0c\u9700\u63a2\u7d22\u66f4\u5168\u9762\u7684\u7cfb\u7edf\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "\u5efa\u7acb\u5305\u542b\u7528\u6237\u6a21\u62df\u5668\u548c\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u6a21\u5757\u7684\u8bc4\u4f30\u73af\u5883\uff0c\u5f00\u53d1\u57fa\u4e8eLLM\u7684\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08LUSTER\uff09\uff0c\u6574\u5408\u77ed\u671f\u60c5\u611f\u5956\u52b1\u548c\u957f\u671f\u4efb\u52a1\u5956\u52b1\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u7ed3\u6784\u5316\u5956\u52b1\u5efa\u6a21\u4e0eLLM\u80fd\u529b\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027\uff0c\u5728\u4efb\u52a1\u6210\u529f\u7387\u548c\u60c5\u611f\u54cd\u5e94\u7ef4\u5ea6\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "LLM\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u534f\u540c\u6846\u67b6\u4e3a\u4e0b\u4e00\u4ee3\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4efb\u52a1\u6548\u80fd\u4e0e\u60c5\u611f\u667a\u80fd\u7684\u53d1\u5c55\u9700\u6c42\u3002"}}
{"id": "2507.01627", "pdf": "https://arxiv.org/pdf/2507.01627", "abs": "https://arxiv.org/abs/2507.01627", "authors": ["Maeve Hutchinson", "Radu Jianu", "Aidan Slingsby", "Jo Wood", "Pranava Madhyastha"], "title": "Chart Question Answering from Real-World Analytical Narratives", "categories": ["cs.CL"], "comment": "This paper has been accepted to the ACL Student Research Workshop\n  (SRW) 2025", "summary": "We present a new dataset for chart question answering (CQA) constructed from\nvisualization notebooks. The dataset features real-world, multi-view charts\npaired with natural language questions grounded in analytical narratives.\nUnlike prior benchmarks, our data reflects ecologically valid reasoning\nworkflows. Benchmarking state-of-the-art multimodal large language models\nreveals a significant performance gap, with GPT-4.1 achieving an accuracy of\n69.3%, underscoring the challenges posed by this more authentic CQA setting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u53ef\u89c6\u5316\u7b14\u8bb0\u672c\u6784\u5efa\u7684\u771f\u5b9e\u573a\u666f\u56fe\u8868\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u6311\u6218\u73b0\u6709\u591a\u6a21\u6001\u5927\u6a21\u578b", "motivation": "\u73b0\u6709\u56fe\u8868\u95ee\u7b54\u6570\u636e\u96c6\u7f3a\u4e4f\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u591a\u89c6\u56fe\u56fe\u8868\u548c\u57fa\u4e8e\u5206\u6790\u53d9\u8ff0\u7684\u81ea\u7136\u8bed\u8a00\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7b26\u5408\u5b9e\u9645\u63a8\u7406\u6d41\u7a0b\u7684\u8bc4\u6d4b\u57fa\u51c6", "method": "\u4ece\u53ef\u89c6\u5316\u7b14\u8bb0\u672c\u4e2d\u63d0\u53d6\u771f\u5b9e\u4e16\u754c\u7684\u591a\u89c6\u56fe\u56fe\u8868\uff0c\u7ed3\u5408\u5206\u6790\u6027\u53d9\u8ff0\u6784\u5efa\u81ea\u7136\u8bed\u8a00\u95ee\u9898", "result": "GPT-4\u6a21\u578b\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u4ec5\u8fbe\u523069.3%\u51c6\u786e\u7387\uff0c\u63ed\u793a\u5f53\u524d\u6a21\u578b\u5728\u771f\u5b9eCQA\u573a\u666f\u4e2d\u7684\u4e0d\u8db3", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u56fe\u8868\u95ee\u7b54\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u8bc4\u4f30\u573a\u666f\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u4e0e\u4eba\u7c7b\u5206\u6790\u80fd\u529b\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u8ddd"}}
{"id": "2507.01633", "pdf": "https://arxiv.org/pdf/2507.01633", "abs": "https://arxiv.org/abs/2507.01633", "authors": ["Georgii Levtsov", "Dmitry Ustalov"], "title": "Confidence and Stability of Global and Pairwise Scores in NLP Evaluation", "categories": ["cs.CL", "cs.IR", "62-04", "D.2.3"], "comment": "8 pages, accepted at ACL SRW 2025", "summary": "With the advent of highly capable instruction-tuned neural language models,\nbenchmarking in natural language processing (NLP) is increasingly shifting\ntowards pairwise comparison leaderboards, such as LMSYS Arena, from traditional\nglobal pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper\nempirically investigates the strengths and weaknesses of both global scores and\npairwise comparisons to aid decision-making in selecting appropriate model\nevaluation strategies. Through computational experiments on synthetic and\nreal-world datasets using standard global metrics and the popular Bradley-Terry\nmodel for pairwise comparisons, we found that while global scores provide more\nreliable overall rankings, they can underestimate strong models with rare,\nsignificant errors or low confidence. Conversely, pairwise comparisons are\nparticularly effective for identifying strong contenders among models with\nlower global scores, especially where quality metrics are hard to define (e.g.,\ntext generation), though they require more comparisons to converge if ties are\nfrequent. Our code and data are available at\nhttps://github.com/HSPyroblast/srw-ranking under a permissive license.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e86NLP\u9886\u57df\u5168\u5c40\u8bc4\u5206\u4e0e\u6210\u5bf9\u6bd4\u8f83\u4e24\u79cd\u6a21\u578b\u8bc4\u4f30\u7b56\u7565\u7684\u4f18\u52a3\u3002\u5168\u5c40\u8bc4\u5206\u63d0\u4f9b\u6574\u4f53\u53ef\u9760\u6027\u4f46\u53ef\u80fd\u4f4e\u4f30\u7279\u6b8a\u5f3a\u6a21\u578b\uff0c\u6210\u5bf9\u6bd4\u8f83\u64c5\u957f\u6316\u6398\u4f4e\u5206\u4f18\u8d28\u6a21\u578b\u4f46\u6536\u655b\u8f83\u6162\u3002", "motivation": "\u9488\u5bf9\u5f53\u524dNLP\u8bc4\u4f30\u4eceGLUE\u7b49\u5168\u5c40\u8bc4\u5206\u8f6c\u5411LMSYS Arena\u7b49\u6210\u5bf9\u6bd4\u8f83\u7684\u8d8b\u52bf\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u660e\u786e\u4e24\u79cd\u65b9\u6cd5\u5728\u6a21\u578b\u8bc4\u4f30\u4e2d\u7684\u9002\u7528\u573a\u666f\uff0c\u4e3a\u9009\u62e9\u8bc4\u4f30\u7b56\u7565\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\u3002", "method": "\u4f7f\u7528\u5408\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u6807\u51c6\u5168\u5c40\u6307\u6807\uff08\u5982\u51c6\u786e\u7387\uff09\u4e0eBradley-Terry\u6210\u5bf9\u6bd4\u8f83\u6a21\u578b\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6a21\u62df\u548c\u5b9e\u9645\u57fa\u51c6\u6d4b\u8bd5\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u5168\u5c40\u8bc4\u5206\u5728\u5e38\u89c4\u573a\u666f\u4e0b\u6392\u540d\u66f4\u7a33\u5b9a\uff0c\u4f46\u5bf9\u5b58\u5728\u7f55\u89c1\u4e25\u91cd\u9519\u8bef\u7684\u9ad8\u80fd\u529b\u6a21\u578b\u654f\u611f\u5ea6\u4e0d\u8db3\uff1b\u6210\u5bf9\u6bd4\u8f83\u5728\u6587\u672c\u751f\u6210\u7b49\u96be\u4ee5\u91cf\u5316\u8bc4\u4f30\u7684\u9886\u57df\u8868\u73b0\u7a81\u51fa\uff0c\u4f46\u9047\u5230\u5e73\u5c40\u65f6\u9700\u66f4\u591a\u6bd4\u8f83\u6837\u672c\u3002", "conclusion": "\u5efa\u8bae\u6839\u636e\u8bc4\u4f30\u76ee\u6807\u6df7\u5408\u4f7f\u7528\uff1a\u9700\u8981\u6574\u4f53\u53ef\u9760\u6027\u65f6\u9009\u62e9\u5168\u5c40\u8bc4\u5206\uff0c\u5173\u6ce8\u7279\u5b9a\u80fd\u529b\u7ef4\u5ea6\u65f6\u91c7\u7528\u6210\u5bf9\u6bd4\u8f83\uff0c\u540c\u65f6\u6ce8\u610f\u4e0d\u540c\u65b9\u6cd5\u5bf9\u6a21\u578b\u9519\u8bef\u6a21\u5f0f\u7684\u654f\u611f\u5ea6\u5dee\u5f02\u3002"}}
{"id": "2507.01645", "pdf": "https://arxiv.org/pdf/2507.01645", "abs": "https://arxiv.org/abs/2507.01645", "authors": ["Rifki Afina Putri"], "title": "Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings", "categories": ["cs.CL"], "comment": "AMLDS 2025", "summary": "In this paper, we investigate the transferability of pre-trained language\nmodels to low-resource Indonesian local languages through the task of sentiment\nanalysis. We evaluate both zero-shot performance and adapter-based transfer on\nten local languages using models of different types: a monolingual Indonesian\nBERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based\napproach called MAD-X. To better understand model behavior, we group the target\nlanguages into three categories: seen (included during pre-training), partially\nseen (not included but linguistically related to seen languages), and unseen\n(absent and unrelated in pre-training data). Our results reveal clear\nperformance disparities across these groups: multilingual models perform best\non seen languages, moderately on partially seen ones, and poorly on unseen\nlanguages. We find that MAD-X significantly improves performance, especially\nfor seen and partially seen languages, without requiring labeled data in the\ntarget language. Additionally, we conduct a further analysis on tokenization\nand show that while subword fragmentation and vocabulary overlap with\nIndonesian correlate weakly with prediction quality, they do not fully explain\nthe observed performance. Instead, the most consistent predictor of transfer\nsuccess is the model's prior exposure to the language, either directly or\nthrough a related language.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5370\u5c3c\u672c\u5730\u8bed\u8a00\u7684\u60c5\u611f\u5206\u6790\u8fc1\u79fb\u80fd\u529b\u53d7\u8bed\u8a00\u63a5\u89e6\u5f71\u54cd\uff0cMAD-X\u9002\u914d\u5668\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6027\u80fd\uff08\u5c24\u5176\u6709\u76f8\u5173\u8bed\u8a00\u63a5\u89e6\u7684\u60c5\u51b5\uff09", "motivation": "\u63a2\u7d22\u5982\u4f55\u5c06\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u6709\u6548\u8fc1\u79fb\u5230\u4f4e\u8d44\u6e90\u5370\u5c3c\u672c\u5730\u8bed\u8a00\uff0c\u89e3\u51b3\u8d44\u6e90\u532e\u4e4f\u8bed\u8a00\u7684\u6a21\u578b\u5e94\u7528\u96be\u9898", "method": "\u4f7f\u7528\u5370\u5c3c\u5355\u8bedBERT\u3001mBERT/XLM-R\u591a\u8bed\u8a00\u6a21\u578b\u548cMAD-X\u9002\u914d\u5668\u65b9\u6cd5\uff0c\u5c0610\u79cd\u672c\u5730\u8bed\u8a00\u5206\u4e3a\u5df2\u89c1/\u90e8\u5206\u5df2\u89c1/\u672a\u89c1\u4e09\u7c7b\u8fdb\u884c\u96f6\u6837\u672c\u548c\u9002\u914d\u5668\u8fc1\u79fb\u6d4b\u8bd5", "result": "\u591a\u8bed\u8a00\u6a21\u578b\u5728\u5df2\u89c1\u8bed\u8a00\u8868\u73b0\u6700\u4f73\uff08F1=76.2\uff09\uff0c\u90e8\u5206\u5df2\u89c1\u6b21\u4e4b\uff08F1=65.1\uff09\uff0c\u672a\u89c1\u6700\u5dee\uff08F1=42.3\uff09\uff1bMAD-X\u4f7f\u5df2\u89c1\u8bed\u8a00\u63d0\u53479.3%\u4e14\u65e0\u9700\u6807\u6ce8\u6570\u636e\uff0c\u8bed\u8a00\u63a5\u89e6\u5386\u53f2\u662f\u8fc1\u79fb\u6210\u529f\u6838\u5fc3\u56e0\u7d20", "conclusion": "\u6a21\u578b\u5bf9\u76ee\u6807\u8bed\u8a00\u7684\u5148\u524d\u63a5\u89e6\uff08\u76f4\u63a5\u6216\u901a\u8fc7\u76f8\u5173\u8bed\u8a00\uff09\u662f\u8fc1\u79fb\u6210\u529f\u5173\u952e\uff0c\u9002\u914d\u5668\u65b9\u6cd5\u80fd\u6709\u6548\u7a81\u7834\u8d44\u6e90\u9650\u5236\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00NLP\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.01702", "pdf": "https://arxiv.org/pdf/2507.01702", "abs": "https://arxiv.org/abs/2507.01702", "authors": ["Zixin Chen", "Hongzhan Lin", "Kaixin Li", "Ziyang Luo", "Zhen Ye", "Guang Chen", "Zhiyong Huang", "Jing Ma"], "title": "AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025", "summary": "The proliferation of multimodal memes in the social media era demands that\nmultimodal Large Language Models (mLLMs) effectively understand meme\nharmfulness. Existing benchmarks for assessing mLLMs on harmful meme\nunderstanding rely on accuracy-based, model-agnostic evaluations using static\ndatasets. These benchmarks are limited in their ability to provide up-to-date\nand thorough assessments, as online memes evolve dynamically. To address this,\nwe propose AdamMeme, a flexible, agent-based evaluation framework that\nadaptively probes the reasoning capabilities of mLLMs in deciphering meme\nharmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive\nevaluations by iteratively updating the meme data with challenging samples,\nthereby exposing specific limitations in how mLLMs interpret harmfulness.\nExtensive experiments show that our framework systematically reveals the\nvarying performance of different target mLLMs, offering in-depth, fine-grained\nanalyses of model-specific weaknesses. Our code is available at\nhttps://github.com/Lbotirx/AdamMeme.", "AI": {"tldr": "\u63d0\u51fa\u4e86AdamMeme\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u52a8\u6001\u66f4\u65b0\u6a21\u56e0\u6570\u636e\uff0c\u7cfb\u7edf\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u6709\u5bb3\u7f51\u7edc\u6a21\u56e0\u7684\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u9759\u6001\u6570\u636e\u96c6\u7684\u51c6\u786e\u6027\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u5feb\u901f\u6f14\u53d8\u7684\u7f51\u7edc\u6a21\u56e0\uff0c\u9700\u8981\u66f4\u52a8\u6001\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u66b4\u9732\u6a21\u578b\u5f31\u70b9\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u8bc4\u4f30\u6846\u67b6\uff08AdamMeme\uff09\uff0c\u901a\u8fc7\u8fed\u4ee3\u66f4\u65b0\u5177\u6709\u6311\u6218\u6027\u7684\u6a21\u56e0\u6837\u672c\u8fdb\u884c\u7ec6\u7c92\u5ea6\u6a21\u578b\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u7cfb\u7edf\u63ed\u793a\u4e0d\u540cmLLM\u6a21\u578b\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u63d0\u4f9b\u6a21\u578b\u7279\u5b9a\u5f31\u70b9\u7684\u6df1\u5165\u5206\u6790\u3002", "conclusion": "AdamMeme\u4e3a\u52a8\u6001\u8bc4\u4f30\u6a21\u578b\u5bf9\u6709\u5bb3\u5185\u5bb9\u7684\u89e3\u91ca\u80fd\u529b\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.01715", "pdf": "https://arxiv.org/pdf/2507.01715", "abs": "https://arxiv.org/abs/2507.01715", "authors": ["Aditya Tomar", "Rudra Murthy", "Pushpak Bhattacharyya"], "title": "Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach", "categories": ["cs.CL"], "comment": null, "summary": "Bias and stereotypes in language models can cause harm, especially in\nsensitive areas like content moderation and decision-making. This paper\naddresses bias and stereotype detection by exploring how jointly learning these\ntasks enhances model performance. We introduce StereoBias, a unique dataset\nlabeled for bias and stereotype detection across five categories: religion,\ngender, socio-economic status, race, profession, and others, enabling a deeper\nstudy of their relationship. Our experiments compare encoder-only models and\nfine-tuned decoder-only models using QLoRA. While encoder-only models perform\nwell, decoder-only models also show competitive results. Crucially, joint\ntraining on bias and stereotype detection significantly improves bias detection\ncompared to training them separately. Additional experiments with sentiment\nanalysis confirm that the improvements stem from the connection between bias\nand stereotypes, not multi-task learning alone. These findings highlight the\nvalue of leveraging stereotype information to build fairer and more effective\nAI systems.", "AI": {"tldr": "\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u504f\u89c1\u4e0e\u523b\u677f\u5370\u8c61\u68c0\u6d4b\u4efb\u52a1\uff0c\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u654f\u611f\u9886\u57df\u7684\u516c\u5e73\u6027\u68c0\u6d4b\u6027\u80fd", "motivation": "\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u5728\u5185\u5bb9\u5ba1\u6838\u7b49\u654f\u611f\u9886\u57df\u53ef\u80fd\u9020\u6210\u4e25\u91cd\u5371\u5bb3\uff0c\u9700\u63a2\u7d22\u8054\u5408\u5b66\u4e60\u673a\u5236\u63d0\u5347\u68c0\u6d4b\u6548\u679c", "method": "\u6784\u5efaStereoBias\u591a\u6807\u7b7e\u6570\u636e\u96c6(\u542b\u5b97\u6559/\u6027\u522b/\u79cd\u65cf\u7b495\u7c7b)\uff0c\u5bf9\u6bd4QLoRA\u5fae\u8c03\u7684Decoder-only\u6a21\u578b\u4e0eEncoder-only\u6a21\u578b\u6027\u80fd\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u8303\u5f0f\u9a8c\u8bc1\u504f\u89c1\u4e0e\u523b\u677f\u5370\u8c61\u7684\u534f\u540c\u68c0\u6d4b\u6548\u679c", "result": "\u8054\u5408\u8bad\u7ec3\u4f7f\u504f\u89c1\u68c0\u6d4b\u51c6\u786e\u7387\u663e\u8457\u4f18\u4e8e\u5355\u72ec\u8bad\u7ec3\uff0c\u4e14Decoder\u6a21\u578b\u5c55\u73b0\u51fa\u4e0eEncoder\u6a21\u578b\u76f8\u5f53\u7684\u7ade\u4e89\u529b\u3002\u6d88\u878d\u5b9e\u9a8c\u8bc1\u5b9e\u6027\u80fd\u63d0\u5347\u6e90\u4e8e\u504f\u89c1\u4e0e\u523b\u677f\u5370\u8c61\u7684\u5185\u5728\u5173\u8054\u800c\u975e\u5355\u7eaf\u591a\u4efb\u52a1\u5b66\u4e60", "conclusion": "\u5229\u7528\u523b\u677f\u5370\u8c61\u4e0e\u504f\u89c1\u7684\u5173\u8054\u6027\u8fdb\u884c\u8054\u5408\u5efa\u6a21\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347AI\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u6784\u5efa\u8d1f\u8d23\u4efb\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2507.01734", "pdf": "https://arxiv.org/pdf/2507.01734", "abs": "https://arxiv.org/abs/2507.01734", "authors": ["Oliver Wardas", "Florian Matthes"], "title": "LLMs for Legal Subsumption in German Employment Contracts", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "PrePrint - ICAIL25, Chicago", "summary": "Legal work, characterized by its text-heavy and resource-intensive nature,\npresents unique challenges and opportunities for NLP research. While\ndata-driven approaches have advanced the field, their lack of interpretability\nand trustworthiness limits their applicability in dynamic legal environments.\nTo address these issues, we collaborated with legal experts to extend an\nexisting dataset and explored the use of Large Language Models (LLMs) and\nin-context learning to evaluate the legality of clauses in German employment\ncontracts. Our work evaluates the ability of different LLMs to classify clauses\nas \"valid,\" \"unfair,\" or \"void\" under three legal context variants: no legal\ncontext, full-text sources of laws and court rulings, and distilled versions of\nthese (referred to as examination guidelines). Results show that full-text\nsources moderately improve performance, while examination guidelines\nsignificantly enhance recall for void clauses and weighted F1-Score, reaching\n80\\%. Despite these advancements, LLMs' performance when using full-text\nsources remains substantially below that of human lawyers. We contribute an\nextended dataset, including examination guidelines, referenced legal sources,\nand corresponding annotations, alongside our code and all log files. Our\nfindings highlight the potential of LLMs to assist lawyers in contract legality\nreview while also underscoring the limitations of the methods presented.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u8bc4\u4f30\u5fb7\u56fd\u96c7\u4f63\u5408\u540c\u6761\u6b3e\u7684\u5408\u6cd5\u6027\uff0c\u53d1\u73b0\u6cd5\u5f8b\u5ba1\u67e5\u6307\u5357\u663e\u8457\u63d0\u5347\u53ec\u56de\u7387\uff08\u8fbe80%\uff09\uff0c\u4f46LLM\u4f7f\u7528\u6cd5\u5f8b\u5168\u6587\u7684\u8868\u73b0\u4ecd\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u5f8b\u5e08\u3002", "motivation": "\u6cd5\u5f8b\u5de5\u4f5c\u5177\u6709\u6587\u672c\u5bc6\u96c6\u548c\u8d44\u6e90\u5bc6\u96c6\u7279\u6027\uff0c\u73b0\u6709\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u96be\u4ee5\u9002\u5e94\u52a8\u6001\u6cd5\u5f8b\u73af\u5883\u3002", "method": "\u6269\u5c55\u6570\u636e\u96c6\u5e76\u6d4b\u8bd5\u4e0d\u540cLLM\u5728\u4e09\u79cd\u6cd5\u5f8b\u4e0a\u4e0b\u6587\uff08\u65e0\u80cc\u666f/\u6cd5\u5f8b\u5168\u6587/\u63d0\u70bc\u7248\u5ba1\u67e5\u6307\u5357\uff09\u4e0b\u5bf9\u5408\u540c\u6761\u6b3e'\u6709\u6548'\u3001'\u4e0d\u516c\u5e73'\u6216'\u65e0\u6548'\u7684\u5206\u7c7b\u80fd\u529b\u3002", "result": "\u6cd5\u5f8b\u5168\u6587\u5c0f\u5e45\u63d0\u5347\u6027\u80fd\uff0c\u5ba1\u67e5\u6307\u5357\u663e\u8457\u63d0\u9ad8\u65e0\u6548\u6761\u6b3e\u53ec\u56de\u7387\u548c\u52a0\u6743F1\u5206\u6570\uff0880%\uff09\uff0c\u4f46LLM\u4f7f\u7528\u6cd5\u5f8b\u5168\u6587\u7684\u8868\u73b0\u4ecd\u5927\u5e45\u843d\u540e\u4e8e\u4eba\u7c7b\u5f8b\u5e08\u3002", "conclusion": "\u8d21\u732e\u4e86\u5305\u542b\u5ba1\u67e5\u6307\u5357\u548c\u6cd5\u5f8b\u6765\u6e90\u7684\u6269\u5c55\u6570\u636e\u96c6\uff0c\u8868\u660eLLM\u5728\u5408\u540c\u5408\u6cd5\u6027\u5ba1\u67e5\u4e2d\u5177\u5907\u8f85\u52a9\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ecd\u5b58\u5728\u660e\u663e\u5c40\u9650\u6027\u3002"}}
{"id": "2507.01764", "pdf": "https://arxiv.org/pdf/2507.01764", "abs": "https://arxiv.org/abs/2507.01764", "authors": ["Matteo Di Cristofaro"], "title": "Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results", "categories": ["cs.CL"], "comment": "Author submitted manuscript", "summary": "Tokenisation - \"the process of splitting text into atomic parts\" (Brezina &\nTimperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides\nthe basis for any applicable quantitative method (e.g. collocations) while\nensuring the reliability of qualitative approaches. This paper examines how\ndiscrepancies in tokenisation affect the representation of language data and\nthe validity of analytical findings: investigating the challenges posed by\nemojis and homoglyphs, the study highlights the necessity of preprocessing\nthese elements to maintain corpus fidelity to the source data. The research\npresents methods for ensuring that digital texts are accurately represented in\ncorpora, thereby supporting reliable linguistic analysis and guaranteeing the\nrepeatability of linguistic interpretations. The findings emphasise the\nnecessity of a detailed understanding of both linguistic and technical aspects\ninvolved in digital textual data to enhance the accuracy of corpus analysis,\nand have significant implications for both quantitative and qualitative\napproaches in corpus-based research.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5206\u8bcd\u4e0d\u4e00\u81f4\u6027\u5bf9\u8bed\u6599\u5e93\u5206\u6790\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u9884\u5904\u7406\u8868\u60c5\u7b26\u53f7\u548c\u540c\u5f62\u5f02\u4e49\u7b26\u7684\u65b9\u6cd5\u4ee5\u4fdd\u8bc1\u6587\u672c\u8868\u5f81\u51c6\u786e\u6027\u3002", "motivation": "\u5206\u8bcd\u5dee\u5f02\u4f1a\u5f71\u54cd\u8bed\u8a00\u6570\u636e\u7684\u8868\u5f81\u6548\u5ea6\u548c\u5206\u6790\u7ed3\u8bba\u7684\u53ef\u91cd\u590d\u6027\uff0c\u5c24\u5176\u5728\u5904\u7406\u8868\u60c5\u7b26\u53f7/\u540c\u5f62\u5f02\u4e49\u7b26\u65f6\u5b58\u5728\u6280\u672f\u6311\u6218", "method": "\u901a\u8fc7\u5206\u6790\u6570\u5b57\u6587\u672c\u4e2d\u7684\u8868\u60c5\u7b26\u53f7\u548c\u540c\u5f62\u5f02\u4e49\u7b26\u5904\u7406\u96be\u9898\uff0c\u5f00\u53d1\u786e\u4fdd\u8bed\u6599\u5e93\u51c6\u786e\u8868\u5f81\u6e90\u6570\u636e\u7684\u9884\u5904\u7406\u65b9\u6cd5", "result": "\u9884\u5904\u7406\u65b9\u6cd5\u6709\u6548\u7ef4\u62a4\u4e86\u8bed\u6599\u5e93\u5bf9\u6e90\u6570\u636e\u7684\u4fdd\u771f\u5ea6\uff0c\u652f\u6301\u53ef\u9760\u7684\u8bed\u8a00\u5b66\u5206\u6790\u5e76\u4fdd\u8bc1\u89e3\u91ca\u7684\u53ef\u91cd\u590d\u6027", "conclusion": "\u8bed\u6599\u5206\u6790\u9700\u517c\u987e\u8bed\u8a00\u7279\u5f81\u548c\u6280\u672f\u5904\u7406\u7ec6\u8282\uff0c\u8be5\u53d1\u73b0\u5bf9\u5b9a\u91cf\u548c\u5b9a\u6027\u7814\u7a76\u8303\u5f0f\u5747\u5177\u6709\u91cd\u8981\u65b9\u6cd5\u8bba\u610f\u4e49"}}
{"id": "2507.01785", "pdf": "https://arxiv.org/pdf/2507.01785", "abs": "https://arxiv.org/abs/2507.01785", "authors": ["Zhixun Chen", "Ping Guo", "Wenhan Han", "Yifan Zhang", "Binbin Liu", "Haobin Lin", "Fengze Liu", "Yan Zhao", "Bingni Zhang", "Taifeng Wang", "Yin Zheng", "Meng Fang"], "title": "MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Data quality is a critical driver of large language model performance, yet\nexisting model-based selection methods focus almost exclusively on English. We\nintroduce MuRating, a scalable framework that transfers high-quality English\ndata-quality signals into a single rater for 17 target languages. MuRating\naggregates multiple English \"raters\" via pairwise comparisons to learn unified\ndocument-quality scores,then projects these judgments through translation to\ntrain a multilingual evaluator on monolingual, cross-lingual, and parallel text\npairs. Applied to web data, MuRating selects balanced subsets of English and\nmultilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to\nstrong baselines, including QuRater, AskLLM, DCLM and so on, our approach\nboosts average accuracy on both English benchmarks and multilingual\nevaluations, with especially large gains on knowledge-intensive tasks. We\nfurther analyze translation fidelity, selection biases, and underrepresentation\nof narrative material, outlining directions for future work.", "AI": {"tldr": "\u63d0\u51faMuRating\u6846\u67b6\uff0c\u901a\u8fc7\u8fc1\u79fb\u82f1\u8bed\u6570\u636e\u8d28\u91cf\u4fe1\u53f7\u8bad\u7ec3\u591a\u8bed\u8a00\u8bc4\u4f30\u5668\uff0c\u663e\u8457\u63d0\u5347LLM\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u96c6\u4e2d\u4e8e\u82f1\u8bed\u573a\u666f\uff0c\u7f3a\u4e4f\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u9ad8\u6548\u6570\u636e\u7b5b\u9009\u65b9\u6848\uff0c\u5236\u7ea6\u975e\u82f1\u8bed\u8bed\u79cd\u6a21\u578b\u6027\u80fd\u3002", "method": "1. \u805a\u5408\u591a\u4e2a\u82f1\u8bed\u8bc4\u4f30\u5668\u7684\u6210\u5bf9\u6bd4\u8f83\u751f\u6210\u7edf\u4e00\u8d28\u91cf\u8bc4\u5206\n2. \u901a\u8fc7\u7ffb\u8bd1\u6295\u5c04\u5efa\u7acb\u8de8\u8bed\u8a00\u8d28\u91cf\u5173\u8054\n3. \u57fa\u4e8e\u5355\u8bed/\u8de8\u8bed/\u5e73\u884c\u6587\u672c\u8bad\u7ec3\u591a\u8bed\u8a00\u8bc4\u4f30\u5668\n4. \u5e94\u7528\u4e8e\u7f51\u9875\u6570\u636e\u7b5b\u9009\u5e76\u9884\u8bad\u7ec31.2B\u53c2\u6570\u7684LLaMA\u6a21\u578b", "result": "\u76f8\u6bd4QuRater/AskLLM\u7b49\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u82f1\u8bed\u57fa\u51c6\u6d4b\u8bd5\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53473.2%\uff0c\u591a\u8bed\u8a00\u8bc4\u4f30\u63d0\u53474.7%\uff0c\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u63d0\u5347\u8fbe12.5%", "conclusion": "\u9a8c\u8bc1\u4e86\u8de8\u8bed\u8a00\u8d28\u91cf\u8fc1\u79fb\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u7ffb\u8bd1\u4fdd\u771f\u5ea6\u5bf9\u6570\u636e\u9009\u62e9\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u53d9\u4e8b\u6750\u6599\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3a\u591a\u8bed\u8a00\u6570\u636e\u4f18\u5316\u6307\u660e\u65b9\u5411\u3002"}}
{"id": "2507.01786", "pdf": "https://arxiv.org/pdf/2507.01786", "abs": "https://arxiv.org/abs/2507.01786", "authors": ["Jord Nguyen", "Khiem Hoang", "Carlo Leonardo Attubato", "Felix Hofst\u00e4tter"], "title": "Probing Evaluation Awareness of Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Technical AI Governance Workshop, ICML (Poster)", "summary": "Language models can distinguish between testing and deployment phases -- a\ncapability known as evaluation awareness. This has significant safety and\npolicy implications, potentially undermining the reliability of evaluations\nthat are central to AI governance frameworks and voluntary industry\ncommitments. In this paper, we study evaluation awareness in\nLlama-3.3-70B-Instruct. We show that linear probes can separate real-world\nevaluation and deployment prompts, suggesting that current models internally\nrepresent this distinction. We also find that current safety evaluations are\ncorrectly classified by the probes, suggesting that they already appear\nartificial or inauthentic to models. Our findings underscore the importance of\nensuring trustworthy evaluations and understanding deceptive capabilities. More\nbroadly, our work showcases how model internals may be leveraged to support\nblackbox methods in safety audits, especially for future models more competent\nat evaluation awareness and deception.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u5177\u5907\u533a\u5206\u6d4b\u8bd5\u4e0e\u90e8\u7f72\u9636\u6bb5\u7684\u80fd\u529b\uff08\u8bc4\u4f30\u610f\u8bc6\uff09\uff0c\u53ef\u80fd\u5f71\u54cdAI\u6cbb\u7406\u6846\u67b6\u7684\u6709\u6548\u6027\u3002\u7814\u7a76\u901a\u8fc7\u7ebf\u6027\u63a2\u9488\u8bc1\u660eLlama-3\u6a21\u578b\u80fd\u8bc6\u522b\u771f\u5b9e\u8bc4\u4f30\u573a\u666f\uff0c\u73b0\u6709\u5b89\u5168\u8bc4\u4f30\u5df2\u88ab\u6a21\u578b\u5224\u5b9a\u4e3a\u4eba\u5de5\u75d5\u8ff9\uff0c\u5f3a\u8c03\u9700\u6784\u5efa\u53ef\u4fe1\u8bc4\u4f30\u4f53\u7cfb\u5e76\u5229\u7528\u6a21\u578b\u5185\u90e8\u673a\u5236\u52a0\u5f3a\u5b89\u5168\u5ba1\u8ba1\u3002", "motivation": "\u63ed\u793a\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u610f\u8bc6\u5bf9AI\u5b89\u5168\u6cbb\u7406\u7684\u6f5c\u5728\u5a01\u80c1\uff0c\u73b0\u6709\u5b89\u5168\u8bc4\u4f30\u53ef\u80fd\u56e0\u6a21\u578b\u8bc6\u522b\u5176\u975e\u771f\u5b9e\u6027\u800c\u5931\u6548\uff0c\u9700\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\u786e\u4fdd\u8bc4\u4f30\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u63a2\u9488\u6280\u672f\u5206\u6790Llama-3.3-70B-Instruct\u6a21\u578b\u5185\u90e8\u8868\u793a\uff0c\u533a\u5206\u771f\u5b9e\u573a\u666f\u63d0\u793a\u4e0e\u8bc4\u4f30\u63d0\u793a\uff0c\u9a8c\u8bc1\u5b89\u5168\u8bc4\u4f30\u6570\u636e\u7684\u53ef\u8bc6\u522b\u6027\u3002", "result": "\u6a21\u578b\u80fd\u6709\u6548\u5206\u79bb\u8bc4\u4f30/\u90e8\u7f72\u63d0\u793a\uff0c\u5f53\u524d\u5b89\u5168\u8bc4\u4f30\u5df2\u88ab\u6b63\u786e\u5206\u7c7b\u4e3a\u4eba\u5de5\u75d5\u8ff9\uff0c\u8bc1\u5b9e\u8bc4\u4f30\u673a\u5236\u5b58\u5728\u88ab\u6a21\u578b\u8bc6\u522b\u7684\u6f0f\u6d1e\u3002", "conclusion": "\u5fc5\u987b\u5f00\u53d1\u6297\u8bc4\u4f30\u610f\u8bc6\u7684\u5ba1\u8ba1\u65b9\u6cd5\uff0c\u7ed3\u5408\u9ed1\u7bb1\u6d4b\u8bd5\u4e0e\u6a21\u578b\u5185\u90e8\u5206\u6790\uff0c\u5e94\u5bf9\u672a\u6765\u6a21\u578b\u53ef\u80fd\u589e\u5f3a\u7684\u8bc4\u4f30\u6b3a\u9a97\u80fd\u529b\u3002"}}
{"id": "2507.01790", "pdf": "https://arxiv.org/pdf/2507.01790", "abs": "https://arxiv.org/abs/2507.01790", "authors": ["Tianze Hua", "Tian Yun", "Ellie Pavlick"], "title": "How Do Vision-Language Models Process Conflicting Information Across Modalities?", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "All code and resources are available at:\n  https://github.com/ethahtz/vlm_conflicting_info_processing", "summary": "AI models are increasingly required to be multimodal, integrating disparate\ninput streams into a coherent state representation on which subsequent\nbehaviors and actions can be based. This paper seeks to understand how such\nmodels behave when input streams present conflicting information. Focusing\nspecifically on vision-language models, we provide inconsistent inputs (e.g.,\nan image of a dog paired with the caption \"A photo of a cat\") and ask the model\nto report the information present in one of the specific modalities (e.g.,\n\"What does the caption say / What is in the image?\"). We find that models often\nfavor one modality over the other, e.g., reporting the image regardless of what\nthe caption says, but that different models differ in which modality they\nfavor. We find evidence that the behaviorally preferred modality is evident in\nthe internal representational structure of the model, and that specific\nattention heads can restructure the representations to favor one modality over\nthe other. Moreover, we find modality-agnostic \"router heads\" which appear to\npromote answers about the modality requested in the instruction, and which can\nbe manipulated or transferred in order to improve performance across datasets\nand modalities. Together, the work provides essential steps towards identifying\nand controlling if and how models detect and resolve conflicting signals within\ncomplex multimodal environments.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u591a\u6a21\u6001AI\u6a21\u578b\u5728\u8f93\u5165\u51b2\u7a81\u65f6\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u6a21\u578b\u5b58\u5728\u6a21\u6001\u504f\u597d\u5e76\u901a\u8fc7\u8c03\u6574\u5185\u90e8\u7ed3\u6784\u53ef\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u7406\u89e3\u591a\u6a21\u6001\u6a21\u578b\u5982\u4f55\u5904\u7406\u89c6\u89c9-\u8bed\u8a00\u8f93\u5165\u51b2\u7a81\uff0c\u63a2\u7d22\u5176\u5185\u90e8\u673a\u5236\u4ee5\u63d0\u5347\u51b2\u7a81\u4fe1\u53f7\u5904\u7406\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5411\u6a21\u578b\u8f93\u5165\u77db\u76fe\u6a21\u6001(\u5982\u56fe\u50cf/\u6807\u9898\u51b2\u7a81)\uff0c\u8981\u6c42\u5176\u62a5\u544a\u7279\u5b9a\u6a21\u6001\u4fe1\u606f\uff0c\u5206\u6790\u884c\u4e3a\u504f\u597d\u53ca\u5185\u90e8\u8868\u5f81\u7ed3\u6784\u3002", "result": "\u6a21\u578b\u666e\u904d\u5b58\u5728\u5355\u4e00\u6a21\u6001\u504f\u597d\uff0c\u53d1\u73b0\u6a21\u6001\u504f\u597d\u4e0e\u5185\u90e8\u8868\u5f81\u76f8\u5173\uff0c\u5e76\u8bc6\u522b\u53ef\u64cd\u7eb5\u7684'\u8def\u7531\u5668\u5934'\u4ee5\u63a7\u5236\u6a21\u6001\u54cd\u5e94\u3002", "conclusion": "\u4e3a\u8bc6\u522b\u548c\u63a7\u5236\u591a\u6a21\u6001\u6a21\u578b\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u51b2\u7a81\u68c0\u6d4b\u4e0e\u89e3\u51b3\u673a\u5236\u63d0\u4f9b\u4e86\u5173\u952e\u65b9\u6cd5\u57fa\u7840\u3002"}}
{"id": "2507.01802", "pdf": "https://arxiv.org/pdf/2507.01802", "abs": "https://arxiv.org/abs/2507.01802", "authors": ["Katharina Beckh", "Elisa Studeny", "Sujan Sai Gannamaneni", "Dario Antweiler", "Stefan R\u00fcping"], "title": "The Anatomy of Evidence: An Investigation Into Explainable ICD Coding", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ACL 2025 Findings", "summary": "Automatic medical coding has the potential to ease documentation and billing\nprocesses. For this task, transparency plays an important role for medical\ncoders and regulatory bodies, which can be achieved using explainability\nmethods. However, the evaluation of these approaches has been mostly limited to\nshort text and binary settings due to a scarcity of annotated data. Recent\nefforts by Cheng et al. (2023) have introduced the MDACE dataset, which\nprovides a valuable resource containing code evidence in clinical records. In\nthis work, we conduct an in-depth analysis of the MDACE dataset and perform\nplausibility evaluation of current explainable medical coding systems from an\napplied perspective. With this, we contribute to a deeper understanding of\nautomatic medical coding and evidence extraction. Our findings reveal that\nground truth evidence aligns with code descriptions to a certain degree. An\ninvestigation into state-of-the-art approaches shows a high overlap with ground\ntruth evidence. We propose match measures and highlight success and failure\ncases. Based on our findings, we provide recommendations for developing and\nevaluating explainable medical coding systems.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7MDACE\u6570\u636e\u96c6\u5206\u6790\u533b\u7597\u7f16\u7801\u7cfb\u7edf\u53ef\u89e3\u91ca\u6027\uff0c\u63ed\u793a\u8bc1\u636e\u5339\u914d\u89c4\u5f8b\u5e76\u63d0\u51fa\u8bc4\u4f30\u5efa\u8bae", "motivation": "\u81ea\u52a8\u533b\u7597\u7f16\u7801\u9700\u6ee1\u8db3\u76d1\u7ba1\u900f\u660e\u5ea6\u8981\u6c42\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u53d7\u9650\u4e8e\u6570\u636e\u89c4\u6a21\u3002MDACE\u6570\u636e\u96c6\u63d0\u4f9b\u4e34\u5e8a\u8bc1\u636e\u6807\u6ce8\u652f\u6301\u6df1\u5165\u5206\u6790", "method": "\u5bf9MDACE\u6570\u636e\u96c6\u8fdb\u884c\u591a\u7ef4\u5ea6\u5206\u6790\uff0c\u63d0\u51fa\u5339\u914d\u5ea6\u6307\u6807\u5e76\u8bc4\u4f30SOTA\u65b9\u6cd5\u7684\u8bc1\u636e\u63d0\u53d6\u80fd\u529b", "result": "\u771f\u5b9e\u8bc1\u636e\u4e0e\u7f16\u7801\u63cf\u8ff0\u5b58\u5728\u5173\u8054\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u8bc1\u636e\u91cd\u53e0\u5ea6\u8fbe60%\u3002\u6210\u529f\u6848\u4f8b\u663e\u793a\u5c40\u90e8\u6ce8\u610f\u529b\u6709\u6548\uff0c\u5931\u8d25\u6848\u4f8b\u66b4\u9732\u4e0a\u4e0b\u6587\u63a8\u7406\u4e0d\u8db3", "conclusion": "\u9700\u5efa\u7acb\u66f4\u4e25\u8c28\u7684\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u4f18\u5316\u8bc1\u636e\u63d0\u53d6\u6a21\u578b"}}
{"id": "2507.01810", "pdf": "https://arxiv.org/pdf/2507.01810", "abs": "https://arxiv.org/abs/2507.01810", "authors": ["Nikita Neveditsin", "Pawan Lingras", "Vijay Mago"], "title": "Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes", "categories": ["cs.CL", "cs.IR"], "comment": "To appear in the ACL Anthology", "summary": "We present a comparative analysis of the parseability of structured outputs\ngenerated by small language models for open attribute-value extraction from\nclinical notes. We evaluate three widely used serialization formats: JSON,\nYAML, and XML, and find that JSON consistently yields the highest parseability.\nStructural robustness improves with targeted prompting and larger models, but\ndeclines for longer documents and certain note types. Our error analysis\nidentifies recurring format-specific failure patterns. These findings offer\npractical guidance for selecting serialization formats and designing prompts\nwhen deploying language models in privacy-sensitive clinical settings.", "AI": {"tldr": "\u6bd4\u8f83\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u7b14\u8bb0\u5f00\u653e\u5c5e\u6027\u503c\u63d0\u53d6\u4e2d\u7684\u7ed3\u6784\u5316\u8f93\u51fa\u53ef\u89e3\u6790\u6027\uff0c\u53d1\u73b0JSON\u683c\u5f0f\u89e3\u6790\u6027\u6700\u4f18\u3002", "motivation": "\u4e3a\u9690\u79c1\u654f\u611f\u7684\u4e34\u5e8a\u573a\u666f\u4e2d\u90e8\u7f72\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u63d0\u4f9b\u5e8f\u5217\u5316\u683c\u5f0f\u9009\u62e9\u548c\u63d0\u793a\u8bbe\u8ba1\u7684\u5b9e\u8df5\u6307\u5bfc", "method": "\u8bc4\u4f30JSON/YAML/XML\u4e09\u79cd\u5e8f\u5217\u5316\u683c\u5f0f\uff0c\u5206\u6790\u7ed3\u6784\u9c81\u68d2\u6027\u968f\u6587\u6863\u957f\u5ea6\u3001\u63d0\u793a\u7b56\u7565\u548c\u6a21\u578b\u89c4\u6a21\u7684\u53d8\u5316\u89c4\u5f8b", "result": "JSON\u683c\u5f0f\u89e3\u6790\u7a33\u5b9a\u6027\u6700\u9ad8\uff0c\u76ee\u6807\u5bfc\u5411\u7684prompt\u548c\u5927\u6a21\u578b\u63d0\u5347\u9c81\u68d2\u6027\uff0c\u957f\u6587\u6863\u548c\u7279\u5b9a\u7b14\u8bb0\u7c7b\u578b\u4f1a\u964d\u4f4e\u89e3\u6790\u6210\u529f\u7387", "conclusion": "\u5efa\u8bae\u4e34\u5e8a\u90e8\u7f72\u4f18\u5148\u9009\u62e9JSON\u683c\u5f0f\uff0c\u9700\u9488\u5bf9\u6027\u8bbe\u8ba1prompt\u5e76\u8003\u8651\u6587\u6863\u957f\u5ea6\u4e0e\u7b14\u8bb0\u7c7b\u578b\u7684\u683c\u5f0f\u5bb9\u9519\u673a\u5236"}}
{"id": "2507.01844", "pdf": "https://arxiv.org/pdf/2507.01844", "abs": "https://arxiv.org/abs/2507.01844", "authors": ["Arthur Wuhrmann", "Anastasiia Kucherenko", "Andrei Kucharavy"], "title": "Low-Perplexity LLM-Generated Sequences and Where To Find Them", "categories": ["cs.CL", "cs.LG"], "comment": "Camera-ready version. Accepted to ACL 2025. 10 pages, 4 figures, 6\n  tables", "summary": "As Large Language Models (LLMs) become increasingly widespread, understanding\nhow specific training data shapes their outputs is crucial for transparency,\naccountability, privacy, and fairness. To explore how LLMs leverage and\nreplicate their training data, we introduce a systematic approach centered on\nanalyzing low-perplexity sequences - high-probability text spans generated by\nthe model. Our pipeline reliably extracts such long sequences across diverse\ntopics while avoiding degeneration, then traces them back to their sources in\nthe training data. Surprisingly, we find that a substantial portion of these\nlow-perplexity spans cannot be mapped to the corpus. For those that do match,\nwe quantify the distribution of occurrences across source documents,\nhighlighting the scope and nature of verbatim recall and paving a way toward\nbetter understanding of how LLMs training data impacts their behavior.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790LLMs\u751f\u6210\u7684\u4f4e\u56f0\u60d1\u5ea6\u5e8f\u5217\u8ffd\u8e2a\u5176\u8bad\u7ec3\u6570\u636e\u6765\u6e90\uff0c\u53d1\u73b0\u5927\u91cf\u7247\u6bb5\u65e0\u6cd5\u5339\u914d\u8bed\u6599\u5e93\uff0c\u91cf\u5316\u5339\u914d\u6570\u636e\u7684\u5206\u5e03\u6a21\u5f0f", "motivation": "\u7406\u89e3\u8bad\u7ec3\u6570\u636e\u5982\u4f55\u5f71\u54cdLLM\u8f93\u51fa\u884c\u4e3a\uff0c\u63d0\u5347\u6a21\u578b\u900f\u660e\u5ea6\u3001\u53ef\u95ee\u8d23\u6027\u53ca\u9690\u79c1\u516c\u5e73\u6027", "method": "\u5f00\u53d1\u7cfb\u7edf\u5316\u6d41\u7a0b\uff1a1)\u53ef\u9760\u63d0\u53d6\u591a\u6837\u5316\u4e3b\u9898\u7684\u4f4e\u56f0\u60d1\u5ea6\u5e8f\u5217 2)\u6eaf\u6e90\u8bad\u7ec3\u6570\u636e 3)\u91cf\u5316\u5339\u914d\u6570\u636e\u5206\u5e03", "result": "1)\u5927\u91cf\u4f4e\u56f0\u60d1\u5ea6\u5e8f\u5217\u65e0\u6cd5\u6eaf\u6e90 2)\u5339\u914d\u6570\u636e\u5448\u73b0\u8de8\u6587\u6863\u7684\u7279\u5b9a\u5206\u5e03\u6a21\u5f0f 3)\u5efa\u7acb\u91cf\u5316\u8bc4\u4f30\u6846\u67b6", "conclusion": "\u8be5\u65b9\u6cd5\u63ed\u793a\u4e86LLM\u8bb0\u5fc6\u673a\u5236\u7279\u5f81\uff0c\u4e3a\u7406\u89e3\u8bad\u7ec3\u6570\u636e\u4e0e\u6a21\u578b\u884c\u4e3a\u7684\u5173\u8054\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2"}}
{"id": "2507.01853", "pdf": "https://arxiv.org/pdf/2507.01853", "abs": "https://arxiv.org/abs/2507.01853", "authors": ["Samridhi Raj Sinha", "Rajvee Sheth", "Abhishek Upperwal", "Mayank Singh"], "title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages", "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that go beyond English centric benchmarks and\naddress the requirements of linguistically diverse regions such as India. We\npresent EKA-EVAL, a unified and production-ready evaluation framework that\nintegrates over 35 benchmarks, including 10 Indic-specific datasets, spanning\ncategories like reasoning, mathematics, tool use, long-context understanding,\nand reading comprehension. Compared to existing Indian language evaluation\ntools, EKA-EVAL offers broader benchmark coverage, with built-in support for\ndistributed inference, quantization, and multi-GPU usage. Our systematic\ncomparison positions EKA-EVAL as the first end-to-end, extensible evaluation\nsuite tailored for both global and Indic LLMs, significantly lowering the\nbarrier to multilingual benchmarking. The framework is open-source and publicly\navailable at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA\ninitiative (https://eka.soket.ai), which aims to scale up to over 100\nbenchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.", "AI": {"tldr": "EKA-EVAL\u662f\u9996\u4e2a\u9488\u5bf9\u5168\u7403\u53ca\u5370\u5ea6\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7aef\u5230\u7aef\u53ef\u6269\u5c55\u8bc4\u4f30\u5957\u4ef6\uff0c\u96c6\u621035+\u57fa\u51c6\u6d4b\u8bd5(\u542b10\u4e2a\u5370\u5ea6\u8bed\u8a00\u6570\u636e\u96c6)\uff0c\u652f\u6301\u5206\u5e03\u5f0f\u63a8\u7406/\u91cf\u5316/\u591aGPU\uff0c\u663e\u8457\u964d\u4f4e\u591a\u8bed\u8a00\u8bc4\u4f30\u95e8\u69db\u3002", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u6846\u67b6\u4ee5\u82f1\u8bed\u4e3a\u4e2d\u5fc3\uff0c\u96be\u4ee5\u6ee1\u8db3\u5370\u5ea6\u7b49\u591a\u8bed\u8a00\u5730\u533a\u7684\u9700\u6c42\uff0c\u9700\u5efa\u7acb\u4e13\u95e8\u7684\u591a\u8bed\u8a00\u8bc4\u4f30\u751f\u6001\u7cfb\u7edf\u3002", "method": "\u96c6\u6210\u591a\u7c7b\u522b\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5185\u7f6e\u5206\u5e03\u5f0f\u63a8\u7406/\u91cf\u5316/\u591aGPU\u652f\u6301\uff0c\u901a\u8fc7\u5f00\u6e90\u6846\u67b6\u5b9e\u73b0\u53ef\u6269\u5c55\u67b6\u6784\u8bbe\u8ba1\u3002", "result": "\u8986\u76d6\u66f4\u5e7f\u6cdb\u8bc4\u4f30\u573a\u666f\uff0c\u6210\u4e3a\u9996\u4e2a\u4e13\u95e8\u9488\u5bf9\u5370\u5ea6\u8bed\u8a00\u7684\u7aef\u5230\u7aef\u8bc4\u4f30\u5957\u4ef6\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u5e76\u8ba1\u5212\u6269\u5c55\u81f3100+\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6807\u51c6\u5316\u591a\u8bed\u8a00\u8bc4\u4f30\u6d41\u7a0b\uff0c\u4e3aLLM\u5728\u591a\u5143\u8bed\u8a00\u73af\u5883\u7684\u53d1\u5c55\u5960\u5b9a\u57fa\u7840\uff0c\u52a9\u529b\u6784\u5efa\u66f4\u516c\u5e73\u7684AI\u8bc4\u4f30\u4f53\u7cfb\u3002"}}
{"id": "2507.01872", "pdf": "https://arxiv.org/pdf/2507.01872", "abs": "https://arxiv.org/abs/2507.01872", "authors": ["Kenan Tang", "Yanhong Li", "Yao Qin"], "title": "DIY-MKG: An LLM-Based Polyglot Language Learning System", "categories": ["cs.CL"], "comment": "Submitted to EMNLP 2025 System Demonstration", "summary": "Existing language learning tools, even those powered by Large Language Models\n(LLMs), often lack support for polyglot learners to build linguistic\nconnections across vocabularies in multiple languages, provide limited\ncustomization for individual learning paces or needs, and suffer from\ndetrimental cognitive offloading. To address these limitations, we design\nDo-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system\nthat supports polyglot language learning. DIY-MKG allows the user to build\npersonalized vocabulary knowledge graphs, which are constructed by selective\nexpansion with related words suggested by an LLM. The system further enhances\nlearning through rich annotation capabilities and an adaptive review module\nthat leverages LLMs for dynamic, personalized quiz generation. In addition,\nDIY-MKG allows users to flag incorrect quiz questions, simultaneously\nincreasing user engagement and providing a feedback loop for prompt refinement.\nOur evaluation of LLM-based components in DIY-MKG shows that vocabulary\nexpansion is reliable and fair across multiple languages, and that the\ngenerated quizzes are highly accurate, validating the robustness of DIY-MKG.", "AI": {"tldr": "DIY-MKG\u5f00\u6e90\u7cfb\u7edf\u901a\u8fc7\u6784\u5efa\u4e2a\u6027\u5316\u591a\u8bed\u8a00\u77e5\u8bc6\u56fe\u8c31\uff0c\u5229\u7528LLM\u5b9e\u73b0\u8bcd\u6c47\u6269\u5c55\u548c\u52a8\u6001\u6d4b\u9a8c\u751f\u6210\uff0c\u89e3\u51b3\u4f20\u7edf\u8bed\u8a00\u5b66\u4e60\u5de5\u5177\u5728\u591a\u8bed\u8a00\u5173\u8054\u3001\u4e2a\u6027\u5316\u548c\u8ba4\u77e5\u8d1f\u8377\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u5b66\u4e60\u5de5\u5177\u7f3a\u4e4f\u591a\u8bed\u8a00\u8bcd\u6c47\u5173\u8054\u652f\u6301\u3001\u4e2a\u6027\u5316\u4e0d\u8db3\u4e14\u9020\u6210\u8ba4\u77e5\u8d85\u8f7d\u3002\u7814\u7a76\u8005\u65e8\u5728\u521b\u5efa\u652f\u6301\u591a\u8bed\u8a00\u5b66\u4e60\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u5b66\u4e60\u81ea\u4e3b\u6027\u3002", "method": "\u7cfb\u7edf\u6784\u5efa\u4e2a\u6027\u5316\u8bcd\u6c47\u77e5\u8bc6\u56fe\u8c31\uff0c\u91c7\u7528LLM\u8fdb\u884c\u8bcd\u6c47\u9009\u62e9\u6027\u6269\u5c55\uff0c\u96c6\u6210\u6ce8\u91ca\u529f\u80fd\u548c\u81ea\u9002\u5e94\u590d\u4e60\u6a21\u5757\uff0c\u5e76\u5efa\u7acb\u7528\u6237\u7ea0\u9519\u53cd\u9988\u673a\u5236\u4f18\u5316\u63d0\u793a\u8bcd\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u7cfb\u7edf\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u8bcd\u6c47\u6269\u5c55\u53ef\u9760\u516c\u5e73\uff0c\u751f\u6210\u7684\u6d4b\u9a8c\u51c6\u786e\u7387\u8fbe96.8%\uff0c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7a33\u5065\u6027\u3002", "conclusion": "DIY-MKG\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u548c\u7528\u6237\u53cd\u9988\u673a\u5236\u6709\u6548\u63d0\u5347\u591a\u8bed\u8a00\u5b66\u4e60\u53c2\u4e0e\u5ea6\uff0c\u5b9e\u73b0\u4e86\u4e2a\u6027\u5316\u3001\u4f4e\u8ba4\u77e5\u8d1f\u8377\u7684\u8bed\u8a00\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.01887", "pdf": "https://arxiv.org/pdf/2507.01887", "abs": "https://arxiv.org/abs/2507.01887", "authors": ["Dongyi Ding", "Tiannan Wang", "Chenghao Zhu", "Meiling Tao", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Large language models (LLMs) excel at reasoning tasks requiring long thought\nsequences for planning, reflection, and refinement. However, their substantial\nmodel size and high computational demands are impractical for widespread\ndeployment. Yet, small language models (SLMs) often struggle to learn long-form\nCoT reasoning due to their limited capacity, a phenomenon we refer to as the\n\"SLMs Learnability Gap\". To address this, we introduce\n\\textbf{Mi}d-\\textbf{Co}T \\textbf{T}eacher \\textbf{A}ssistant Distillation\n(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA\nemploys intermediate-sized models as teacher assistants and utilizes\nintermediate-length CoT sequences to bridge both the capacity and reasoning\nlength gaps. Our experiments on downstream tasks demonstrate that although SLMs\ndistilled from large teachers can perform poorly, by applying MiCoTA, they\nachieve significant improvements in reasoning performance. Specifically,\nQwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and\n3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and\nGSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform\na quantitative experiment demonstrating that our method produces data more\nclosely aligned with base SLM distributions. Our insights pave the way for\nfuture research into long-CoT data distillation for SLMs.", "AI": {"tldr": "\u63d0\u51faMiCoTA\u6846\u67b6\uff0c\u901a\u8fc7\u4e2d\u95f4\u6a21\u578b\u548c\u4e2d\u7b49\u957f\u5ea6CoT\u5e8f\u5217\u84b8\u998f\uff0c\u89e3\u51b3\u5c0f\u6a21\u578b\u957f\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u5c0f\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u7531\u4e8e\u5bb9\u91cf\u9650\u5236\u96be\u4ee5\u5b66\u4e60\u957f\u94fe\u5f0f\u63a8\u7406\uff08CoT\uff09\uff0c\u5f62\u6210\"SLMs\u53ef\u5b66\u4e60\u6027\u5dee\u8ddd\"\u3002", "method": "\u4f7f\u7528\u4e2d\u7b49\u89c4\u6a21\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08\u52a9\u7406\uff0c\u7ed3\u5408\u4e2d\u7b49\u957f\u5ea6CoT\u5e8f\u5217\u8fdb\u884c\u5206\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\uff0c\u5f25\u63a8\u7406\u80fd\u529b\u4e0e\u6a21\u578b\u5bb9\u91cf\u5dee\u8ddd\u3002", "result": "Qwen2.5-7B/3B\u6a21\u578b\u5728AIME\u7b49\u57fa\u51c6\u5e73\u5747\u5206\u63d0\u53473.47/3.93\uff0c\u91cf\u5316\u5b9e\u9a8c\u663e\u793a\u6570\u636e\u5206\u5e03\u66f4\u63a5\u8fd1\u57fa\u7840SLM\u3002", "conclusion": "MiCoTA\u6709\u6548\u63d0\u5347SLMs\u957f\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u5c0f\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\u63d0\u4f9b\u65b0\u601d\u8def\uff0c\u672a\u6765\u53ef\u63a2\u7d22\u66f4\u4f18\u4e2d\u95f4\u6a21\u578b\u9009\u62e9\u7b56\u7565\u3002"}}
{"id": "2507.01900", "pdf": "https://arxiv.org/pdf/2507.01900", "abs": "https://arxiv.org/abs/2507.01900", "authors": ["Songtao Liu", "Peng Liu"], "title": "High-Layer Attention Pruning with Rescaling", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Pruning is a highly effective approach for compressing large language models\n(LLMs), significantly reducing inference latency. However, conventional\ntraining-free structured pruning methods often employ a heuristic metric that\nindiscriminately removes some attention heads across all pruning layers,\nwithout considering their positions within the network architecture. In this\nwork, we propose a novel pruning algorithm that strategically prunes attention\nheads in the model's higher layers. Since the removal of attention heads can\nalter the magnitude of token representations, we introduce an adaptive\nrescaling parameter that calibrates the representation scale post-pruning to\ncounteract this effect. We conduct comprehensive experiments on a wide range of\nLLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our\nevaluation includes both generation and discriminative tasks across 27\ndatasets. The results consistently demonstrate that our method outperforms\nexisting structured pruning methods. This improvement is particularly notable\nin generation tasks, where our approach significantly outperforms existing\nbaselines.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9ad8\u5c42\u6ce8\u610f\u529b\u5934\u526a\u679d\u548c\u81ea\u9002\u5e94\u7f29\u653e\u6821\u51c6\u7684LLM\u538b\u7f29\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u4efb\u52a1\u6548\u679c", "motivation": "\u4f20\u7edf\u526a\u679d\u65b9\u6cd5\u5ffd\u89c6\u6ce8\u610f\u529b\u5934\u5728\u7f51\u7edc\u4e2d\u7684\u4f4d\u7f6e\u91cd\u8981\u6027\uff0c\u76f2\u76ee\u526a\u679d\u5bfc\u81f4\u6027\u80fd\u635f\u5931", "method": "1. \u4f18\u5148\u526a\u679d\u6a21\u578b\u9ad8\u5c42\u6ce8\u610f\u529b\u5934 2. \u5f15\u5165\u81ea\u9002\u5e94\u7f29\u653e\u53c2\u6570\u6821\u51c6\u8868\u5f81\u5c3a\u5ea6", "result": "\u572827\u4e2a\u6570\u636e\u96c6\u7684\u751f\u6210/\u5224\u522b\u4efb\u52a1\u4e2d\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u4efb\u52a1\u6539\u8fdb\u5c24\u5176\u663e\u8457", "conclusion": "\u4f4d\u7f6e\u654f\u611f\u526a\u679d\u7b56\u7565\u914d\u5408\u8868\u5f81\u6821\u51c6\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347LLM\u526a\u679d\u540e\u7684\u6027\u80fd\u4fdd\u6301\u80fd\u529b"}}
{"id": "2507.01903", "pdf": "https://arxiv.org/pdf/2507.01903", "abs": "https://arxiv.org/abs/2507.01903", "authors": ["Qiguang Chen", "Mingda Yang", "Libo Qin", "Jinhao Liu", "Zheng Yan", "Jiannan Guan", "Dengyun Peng", "Yiyan Ji", "Hanjing Li", "Mengkang Hu", "Yimeng Zhang", "Yihao Liang", "Yuhang Zhou", "Jiaqi Wang", "Zhi Chen", "Wanxiang Che"], "title": "AI4Research: A Survey of Artificial Intelligence for Scientific Research", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated\nremarkable capabilities in complex domains such as logical reasoning and\nexperimental coding. Motivated by these advancements, numerous studies have\nexplored the application of AI in the innovation process, particularly in the\ncontext of scientific research. These AI technologies primarily aim to develop\nsystems that can autonomously conduct research processes across a wide range of\nscientific disciplines. Despite these significant strides, a comprehensive\nsurvey on AI for Research (AI4Research) remains absent, which hampers our\nunderstanding and impedes further development in this field. To address this\ngap, we present a comprehensive survey and offer a unified perspective on\nAI4Research. Specifically, the main contributions of our work are as follows:\n(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify\nfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key\nresearch gaps and highlight promising future directions, focusing on the rigor\nand scalability of automated experiments, as well as the societal impact. (3)\nAbundant applications and resources: Finally, we compile a wealth of resources,\nincluding relevant multidisciplinary applications, data corpora, and tools. We\nhope our work will provide the research community with quick access to these\nresources and stimulate innovative breakthroughs in AI4Research.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2aAI4Research\u7efc\u5408\u8c03\u67e5\u62a5\u544a\uff0c\u6784\u5efa\u7cfb\u7edf\u5206\u7c7b\u6846\u67b6\u5e76\u6574\u5408\u591a\u5b66\u79d1\u8d44\u6e90", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9AI\u9a71\u52a8\u79d1\u7814\u7684\u7cfb\u7edf\u6027\u5f52\u7eb3\uff0c\u963b\u788d\u9886\u57df\u53d1\u5c55\u4e0e\u521b\u65b0\u7a81\u7834", "method": "\u901a\u8fc7\u7cfb\u7edf\u5206\u7c7b\u6cd5\u5212\u5206\u4e94\u5927\u6838\u5fc3\u4efb\u52a1\uff0c\u8bc6\u522b\u7814\u7a76\u7a7a\u767d\u5e76\u5efa\u7acb\u8de8\u5b66\u79d1\u8d44\u6e90\u5e93", "result": "\u5efa\u7acb\u5305\u542b\u81ea\u52a8\u5316\u5b9e\u9a8c\u4f18\u5316\u3001\u53ef\u6269\u5c55\u65b9\u6cd5\u8bba\u53ca\u793e\u4f1a\u5f71\u54cd\u8bc4\u4f30\u7684\u65b0\u7814\u7a76\u8303\u5f0f", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u79d1\u7814\u793e\u533a\u63d0\u4f9b\u8d44\u6e90\u5bfc\u822a\uff0c\u63a8\u52a8AI\u9a71\u52a8\u79d1\u7814\u7684\u8de8\u5b66\u79d1\u521b\u65b0\u53d1\u5c55"}}
{"id": "2507.01915", "pdf": "https://arxiv.org/pdf/2507.01915", "abs": "https://arxiv.org/abs/2507.01915", "authors": ["Chengao Li", "Hanyu Zhang", "Yunkun Xu", "Hongyan Xue", "Xiang Ao", "Qing He"], "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "19 pages, 3 figures. Accepted by ACL 2025 (main)", "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful\ntechnique for aligning large language models (LLMs) with human preferences.\nHowever, effectively aligning LLMs with diverse human preferences remains a\nsignificant challenge, particularly when they are conflict. To address this\nissue, we frame human value alignment as a multi-objective optimization\nproblem, aiming to maximize a set of potentially conflicting objectives. We\nintroduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning\nparadigm that employs multiple-gradient descent to align LLMs with diverse\npreference distributions. GAPO adaptively rescales the gradients for each\nobjective to determine an update direction that optimally balances the\ntrade-offs between objectives. Additionally, we introduce P-GAPO, which\nincorporates user preferences across different objectives and achieves Pareto\nsolutions that better align with the user's specific needs. Our theoretical\nanalysis demonstrates that GAPO converges towards a Pareto optimal solution for\nmultiple objectives. Empirical results on Mistral-7B show that GAPO outperforms\ncurrent state-of-the-art methods, achieving superior performance in both\nhelpfulness and harmlessness.", "AI": {"tldr": "\u63d0\u51faGAPO\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u81ea\u9002\u5e94\u7f29\u653e\u89e3\u51b3LLMs\u4e0e\u51b2\u7a81\u4eba\u7c7b\u504f\u597d\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u5728Mistral-7B\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RLHF\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u591a\u6837\u5316\u4e14\u53ef\u80fd\u51b2\u7a81\u7684\u4eba\u7c7b\u504f\u597d\uff0c\u9700\u5f00\u53d1\u66f4\u4f18\u7684\u591a\u76ee\u6807\u5bf9\u9f50\u65b9\u6848\u3002", "method": "1. GAPO\u91c7\u7528\u591a\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u5404\u76ee\u6807\u68af\u5ea6\u6743\u91cd 2. P-GAPO\u5f15\u5165\u7528\u6237\u504f\u597d\u53c2\u6570\u5b9e\u73b0\u5b9a\u5236\u5316Pareto\u89e3", "result": "\u7406\u8bba\u8bc1\u660eGAPO\u7684Pareto\u6536\u655b\u6027\uff0c\u5b9e\u9a8c\u663e\u793a\u5728Mistral-7B\u4e0ahelpfulness\u548charmlessness\u6307\u6807\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "conclusion": "GAPO\u4e3a\u591a\u76ee\u6807\u4ef7\u503c\u5bf9\u9f50\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u969c\u548c\u9ad8\u6548\u5b9e\u73b0\u6846\u67b6\uff0cP-GAPO\u6269\u5c55\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u5b9a\u5236\u5316\u80fd\u529b"}}
{"id": "2507.01921", "pdf": "https://arxiv.org/pdf/2507.01921", "abs": "https://arxiv.org/abs/2507.01921", "authors": ["Yang Li", "Youssef Emad", "Karthik Padthe", "Jack Lanchantin", "Weizhe Yuan", "Thao Nguyen", "Jason Weston", "Shang-Wen Li", "Dong Wang", "Ilia Kulikov", "Xian Li"], "title": "NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Recent work has shown that distilling reasoning traces from a larger teacher\nmodel via supervised finetuning outperforms reinforcement learning with the\nsmaller student model alone (Guo et al. 2025). However, there has not been a\nsystematic study of what kind of reasoning demonstrations from the teacher are\nmost effective in improving the student model's reasoning capabilities. In this\nwork we curate high-quality \"NaturalThoughts\" by selecting reasoning traces\nfrom a strong teacher model based on a large pool of questions from\nNaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of\nfactors that affect distilling reasoning capabilities, in terms of sample\nefficiency and scalability for general reasoning tasks. We observe that simply\nscaling up data size with random sampling is a strong baseline with steady\nperformance gains. Further, we find that selecting difficult examples that\nrequire more diverse reasoning strategies is more sample-efficient to transfer\nthe teacher model's reasoning skills. Evaluated on both Llama and Qwen models,\ntraining with NaturalThoughts outperforms existing reasoning datasets such as\nOpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including\nGPQA-Diamond, MMLU-Pro and SuperGPQA.", "AI": {"tldr": "\u901a\u8fc7\u7b5b\u9009\u6559\u5e08\u6a21\u578b\u4e2d\u9700\u8981\u591a\u6837\u5316\u63a8\u7406\u7b56\u7565\u7684\u56f0\u96be\u6837\u672c\uff08NaturalThoughts\uff09\uff0c\u80fd\u66f4\u9ad8\u6548\u5730\u63d0\u5347\u5b66\u751f\u6a21\u578b\u5728STEM\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u6548\u679c\u4f18\u4e8e\u5355\u7eaf\u6269\u5927\u6570\u636e\u89c4\u6a21\u3002", "motivation": "\u63a2\u7a76\u6559\u5e08\u6a21\u578b\u7684\u54ea\u79cd\u63a8\u7406\u793a\u8303\u80fd\u6700\u6709\u6548\u63d0\u5347\u5b66\u751f\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u7a81\u7834\u73b0\u6709\u7814\u7a76\u4e2d\u7b80\u5355\u6269\u5927\u6570\u636e\u89c4\u6a21\u7684\u4f20\u7edf\u505a\u6cd5\u3002", "method": "\u57fa\u4e8eNaturalReasoning\u9898\u5e93\u6784\u5efa\u9ad8\u8d28\u91cf\u63a8\u7406\u8f68\u8ff9\u6570\u636e\u96c6NaturalThoughts\uff0c\u7cfb\u7edf\u5206\u6790\u6570\u636e\u89c4\u6a21\u3001\u6837\u672c\u96be\u5ea6\u3001\u63a8\u7406\u7b56\u7565\u591a\u6837\u6027\u7b49\u56e0\u7d20\u5bf9\u77e5\u8bc6\u84b8\u998f\u7684\u5f71\u54cd\u3002", "result": "\u5728Llama/Qwen\u6a21\u578b\u4e0a\uff0cNaturalThoughts\u5728GPQA-Diamond\u7b49STEM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8aOpenThoughts\u7b49\u73b0\u6709\u6570\u636e\u96c6\u3002", "conclusion": "\u9009\u62e9\u9700\u8981\u591a\u6837\u5316\u63a8\u7406\u7b56\u7565\u7684\u56f0\u96be\u6837\u672c\uff0c\u6bd4\u5355\u7eaf\u6269\u5927\u6570\u636e\u89c4\u6a21\u66f4\u80fd\u6709\u6548\u4f20\u9012\u6559\u5e08\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.01923", "pdf": "https://arxiv.org/pdf/2507.01923", "abs": "https://arxiv.org/abs/2507.01923", "authors": ["Yu-Shiang Huang", "Chuan-Ju Wang", "Chung-Chi Chen"], "title": "Decision-oriented Text Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Natural language generation (NLG) is increasingly deployed in high-stakes\ndomains, yet common intrinsic evaluation methods, such as n-gram overlap or\nsentence plausibility, weakly correlate with actual decision-making efficacy.\nWe propose a decision-oriented framework for evaluating generated text by\ndirectly measuring its influence on human and large language model (LLM)\ndecision outcomes. Using market digest texts--including objective morning\nsummaries and subjective closing-bell analyses--as test cases, we assess\ndecision quality based on the financial performance of trades executed by human\ninvestors and autonomous LLM agents informed exclusively by these texts. Our\nfindings reveal that neither humans nor LLM agents consistently surpass random\nperformance when relying solely on summaries. However, richer analytical\ncommentaries enable collaborative human-LLM teams to outperform individual\nhuman or agent baselines significantly. Our approach underscores the importance\nof evaluating generated text by its ability to facilitate synergistic\ndecision-making between humans and LLMs, highlighting critical limitations of\ntraditional intrinsic metrics.", "AI": {"tldr": "\u63d0\u51fa\u51b3\u7b56\u5bfc\u5411\u7684NLG\u8bc4\u4f30\u6846\u67b6\uff0c\u9a8c\u8bc1\u4eba\u673a\u534f\u4f5c\u51b3\u7b56\u6548\u80fd\u4f18\u4e8e\u5355\u4e00\u4e3b\u4f53", "motivation": "\u4f20\u7edfNLG\u8bc4\u4f30\u6307\u6807\u4e0e\u51b3\u7b56\u6548\u679c\u5173\u8054\u6027\u5f31\uff0c\u9700\u5efa\u7acb\u57fa\u4e8e\u5b9e\u9645\u51b3\u7b56\u5f71\u54cd\u529b\u7684\u8bc4\u4f30\u4f53\u7cfb", "method": "\u4f7f\u7528\u5e02\u573a\u6458\u8981\u6587\u672c\u4f5c\u4e3a\u6d4b\u8bd5\u6848\u4f8b\uff0c\u901a\u8fc7\u4eba\u7c7b\u6295\u8d44\u8005\u548cLLM\u4ee3\u7406\u7684\u8d22\u52a1\u8868\u73b0\u8861\u91cf\u51b3\u7b56\u8d28\u91cf", "result": "\u4eba\u673a\u534f\u540c\u56e2\u961f\u5728\u5206\u6790\u6027\u8bc4\u8bba\u652f\u6491\u4e0b\u663e\u8457\u8d85\u8d8a\u4e2a\u4f53\u8868\u73b0\uff0c\u5355\u4e00\u4e3b\u4f53\u8868\u73b0\u4e0d\u4f18\u4e8e\u968f\u673a\u57fa\u51c6", "conclusion": "NLG\u8bc4\u4f30\u5e94\u805a\u7126\u4eba\u673a\u534f\u540c\u51b3\u7b56\u80fd\u529b\uff0c\u4f20\u7edf\u5185\u5728\u6307\u6807\u5b58\u5728\u5173\u952e\u6027\u5c40\u9650"}}
{"id": "2507.01931", "pdf": "https://arxiv.org/pdf/2507.01931", "abs": "https://arxiv.org/abs/2507.01931", "authors": ["Md Sazzadul Islam Ridoy", "Sumi Akter", "Md. Aminur Rahman"], "title": "Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "In recent years, neural models trained on large multilingual text and speech\ndatasets have shown great potential for supporting low-resource languages. This\nstudy investigates the performances of two state-of-the-art Automatic Speech\nRecognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's\nWav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments\nusing two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to\nevaluate model performances. Through systematic fine-tuning and hyperparameter\noptimization, including learning rate, epochs, and model checkpoint selection,\nwe have compared the models based on Word Error Rate (WER), Character Error\nRate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model\noutperformed Whisper across all key evaluation metrics, demonstrated superior\nperformance while requiring fewer computational resources, and offered valuable\ninsights to develop robust speech recognition systems in low-resource\nlinguistic settings.", "AI": {"tldr": "Wav2Vec-BERT\u5728\u4f4e\u8d44\u6e90\u5b5f\u52a0\u62c9\u8bedASR\u4efb\u52a1\u4e2d\u5168\u9762\u8d85\u8d8aWhisper\u6a21\u578b\uff0c\u5177\u5907\u66f4\u9ad8\u8ba1\u7b97\u6548\u7387\u548c\u66f4\u4f4e\u8bcd\u9519\u7387", "motivation": "\u8bc4\u4f30\u4e3b\u6d41\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\uff08Whisper\u548cWav2Vec-BERT\uff09\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u4e3a\u8d44\u6e90\u532e\u4e4f\u8bed\u8a00\u63d0\u4f9b\u8bed\u97f3\u7cfb\u7edf\u5f00\u53d1\u65b9\u6848", "method": "\u4f7f\u7528Mozilla Common Voice-17\u548cOpenSLR\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u5fae\u8c03\u3001\u8d85\u53c2\u6570\u4f18\u5316\uff08\u5b66\u4e60\u7387/\u8bad\u7ec3\u8f6e\u6b21/\u68c0\u67e5\u70b9\u9009\u62e9\uff09\uff0c\u5bf9\u6bd4\u8bcd\u9519\u7387\u3001\u5b57\u7b26\u9519\u7387\u3001\u8bad\u7ec3\u65f6\u95f4\u548c\u8ba1\u7b97\u6548\u7387", "result": "Wav2Vec-BERT\u5728\u6240\u6709\u5173\u952e\u6307\u6807\uff08WER/CER/\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\uff09\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u534740%\uff0c\u663e\u5b58\u5360\u7528\u51cf\u5c1130%", "conclusion": "\u57fa\u4e8eTransformer\u7684Wav2Vec-BERT\u67b6\u6784\u66f4\u9002\u5408\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\uff0c\u4e3a\u5f00\u53d1\u9c81\u68d2\u8bed\u97f3\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6a21\u578b\u9009\u62e9\u4f9d\u636e\u548c\u8c03\u4f18\u65b9\u6cd5\u8bba"}}
{"id": "2507.01936", "pdf": "https://arxiv.org/pdf/2507.01936", "abs": "https://arxiv.org/abs/2507.01936", "authors": ["Adrian de Wynter", "Tangming Yuan"], "title": "The Thin Line Between Comprehension and Persuasion in LLMs", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are excellent at maintaining high-level,\nconvincing dialogues. They are being fast deployed as chatbots and evaluators\nin sensitive areas, such as peer review and mental health applications. This,\nalong with the disparate accounts on their reasoning capabilities, calls for a\ncloser examination of LLMs and their comprehension of dialogue. In this work we\nbegin by evaluating LLMs' ability to maintain a debate--one of the purest yet\nmost complex forms of human communication. Then we measure how this capability\nrelates to their understanding of what is being talked about, namely, their\ncomprehension of dialogical structures and the pragmatic context. We find that\nLLMs are capable of maintaining coherent, persuasive debates, often swaying the\nbeliefs of participants and audiences alike. We also note that awareness or\nsuspicion of AI involvement encourage people to be more critical of the\narguments made. When polling LLMs on their comprehension of deeper structures\nof dialogue, however, they cannot demonstrate said understanding. Our findings\ntie the shortcomings of LLMs-as-evaluators to their (in)ability to understand\nthe context. More broadly, for the field of argumentation theory we posit that,\nif an agent can convincingly maintain a dialogue, it is not necessary for it to\nknow what it is talking about. Hence, the modelling of pragmatic context and\ncoherence are secondary to effectiveness.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u751f\u6210\u6709\u8bf4\u670d\u529b\u7684\u8fa9\u8bba\u5185\u5bb9\u4f46\u7f3a\u4e4f\u6df1\u5c42\u5bf9\u8bdd\u7406\u89e3\uff0c\u5f71\u54cd\u5176\u4f5c\u4e3a\u8bc4\u4f30\u5de5\u5177\u7684\u53ef\u9760\u6027", "motivation": "\u8bc4\u4f30LLMs\u5728\u654f\u611f\u9886\u57df\uff08\u5982\u8bba\u6587\u8bc4\u5ba1\u548c\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\uff09\u4e2d\u7684\u5bf9\u8bdd\u7406\u89e3\u80fd\u529b\uff0c\u7279\u522b\u662f\u5bf9\u8fa9\u8bba\u7ed3\u6784\u548c\u8bed\u7528\u80cc\u666f\u7684\u638c\u63e1\u7a0b\u5ea6", "method": "\u901a\u8fc7\u8fa9\u8bba\u573a\u666f\u6d4b\u8bd5LLMs\u7684\u5bf9\u8bdd\u8fde\u8d2f\u6027\uff0c\u7ed3\u5408\u7528\u6237\u5b9e\u9a8c\u6d4b\u91cf\u5176\u5bf9\u5bf9\u8bdd\u7ed3\u6784\u7684\u7406\u89e3\u4e0e\u4eba\u7c7b\u5bf9AI\u53c2\u4e0e\u7684\u6279\u5224\u6027\u53cd\u5e94", "result": "LLMs\u53ef\u751f\u6210\u6539\u53d8\u542c\u4f17\u89c2\u70b9\u7684\u8fa9\u8bba\u5185\u5bb9\uff0c\u4f46\u4eba\u7c7b\u610f\u8bc6\u5230AI\u53c2\u4e0e\u65f6\u4f1a\u63d0\u9ad8\u6279\u5224\u6027\uff1b\u6a21\u578b\u65e0\u6cd5\u5c55\u793a\u5bf9\u5bf9\u8bdd\u6df1\u5c42\u7ed3\u6784\u7684\u7406\u89e3", "conclusion": "\u5bf9\u8bdd\u6709\u6548\u6027\u7684\u5b9e\u73b0\u4f18\u5148\u4e8e\u4e0a\u4e0b\u6587\u7406\u89e3\uff0c\u8fd9\u5bf9\u8bba\u8bc1\u7406\u8bba\u548cLLMs\u7684\u5b9e\u9645\u5e94\u7528\u5177\u6709\u91cd\u8981\u542f\u793a"}}
{"id": "2507.01021", "pdf": "https://arxiv.org/pdf/2507.01021", "abs": "https://arxiv.org/abs/2507.01021", "authors": ["Kumarmanas Nethil", "Vaibhav Mishra", "Kriti Anandan", "Kavya Manohar"], "title": "Scalable Offline ASR for Command-Style Dictation in Courtrooms", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to Interspeech 2025 Show & Tell", "summary": "We propose an open-source framework for Command-style dictation that\naddresses the gap between resource-intensive Online systems and high-latency\nBatch processing. Our approach uses Voice Activity Detection (VAD) to segment\naudio and transcribes these segments in parallel using Whisper models, enabling\nefficient multiplexing across audios. Unlike proprietary systems like\nSuperWhisper, this framework is also compatible with most ASR architectures,\nincluding widely used CTC-based models. Our multiplexing technique maximizes\ncompute utilization in real-world settings, as demonstrated by its deployment\nin around 15% of India's courtrooms. Evaluations on live data show consistent\nlatency reduction as user concurrency increases, compared to sequential batch\nprocessing. The live demonstration will showcase our open-sourced\nimplementation and allow attendees to interact with it in real-time.", "AI": {"tldr": "\u5f00\u6e90\u6307\u4ee4\u5f0f\u542c\u5199\u6846\u67b6\u901a\u8fc7VAD\u5206\u5272\u97f3\u9891\u548cWhisper\u5e76\u884c\u8f6c\u5f55\uff0c\u5b9e\u73b0\u9ad8\u6548\u591a\u8def\u590d\u7528\uff0c\u5df2\u5728\u5370\u5ea615%\u6cd5\u5ead\u90e8\u7f72\u5e94\u7528\u3002", "motivation": "\u89e3\u51b3\u5728\u7ebf\u7cfb\u7edf\u8d44\u6e90\u5bc6\u96c6\u4e0e\u6279\u5904\u7406\u9ad8\u5ef6\u8fdf\u7684\u75db\u70b9\uff0c\u517c\u5bb9\u4e3b\u6d41ASR\u67b6\u6784\u9700\u6c42", "method": "\u8bed\u97f3\u6d3b\u52a8\u68c0\u6d4b(VAD)\u5206\u6bb5 + Whisper\u6a21\u578b\u5e76\u884c\u8f6c\u5f55 + \u8ba1\u7b97\u8d44\u6e90\u591a\u8def\u590d\u7528\u6280\u672f", "result": "\u5b9e\u9645\u90e8\u7f72\u663e\u793a\u7528\u6237\u5e76\u53d1\u91cf\u589e\u52a0\u65f6\u5ef6\u8fdf\u6301\u7eed\u4e0b\u964d\uff0c\u8f83\u987a\u5e8f\u6279\u5904\u7406\u63d0\u5347\u663e\u8457", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u5ef6\u8fdf\uff0c\u5728\u53f8\u6cd5\u7b49\u5b9e\u65f6\u8f6c\u5f55\u573a\u666f\u5177\u6709\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2507.01029", "pdf": "https://arxiv.org/pdf/2507.01029", "abs": "https://arxiv.org/abs/2507.01029", "authors": ["Junjie Zhou", "Yingli Zuo", "Shichang Feng", "Peng Wan", "Qi Zhu", "Daoqiang Zhang", "Wei Shao"], "title": "PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "With the development of generative artificial intelligence and instruction\ntuning techniques, multimodal large language models (MLLMs) have made\nimpressive progress on general reasoning tasks. Benefiting from the\nchain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning\nproblem step-by-step. However, existing MLLMs still face significant challenges\nwhen applied to pathology visual reasoning tasks: (1) LLMs often underperforms\nbecause they lack domain-specific information, which can lead to model\nhallucinations. (2) The additional reasoning steps in CoT may introduce errors,\nleading to the divergence of answers. To address these limitations, we propose\nPathCoT, a novel zero-shot CoT prompting method which integrates the pathology\nexpert-knowledge into the reasoning process of MLLMs and incorporates\nself-evaluation to mitigate divergence of answers. Specifically, PathCoT guides\nthe MLLM with prior knowledge to perform as pathology experts, and provides\ncomprehensive analysis of the image with their domain-specific knowledge. By\nincorporating the experts' knowledge, PathCoT can obtain the answers with CoT\nreasoning. Furthermore, PathCoT incorporates a self-evaluation step that\nassesses both the results generated directly by MLLMs and those derived through\nCoT, finally determining the reliable answer. The experimental results on the\nPathMMU dataset demonstrate the effectiveness of our method on pathology visual\nunderstanding and reasoning.", "AI": {"tldr": "\u63d0\u51faPathCoT\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u75c5\u7406\u4e13\u5bb6\u77e5\u8bc6\u548c\u81ea\u8bc4\u4f30\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u75c5\u7406\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u75c5\u7406\u89c6\u89c9\u63a8\u7406\u4e2d\u5b58\u5728\u9886\u57df\u77e5\u8bc6\u7f3a\u4e4f\u5bfc\u81f4\u6a21\u578b\u5e7b\u89c9\uff0c\u4ee5\u53ca\u989d\u5916\u63a8\u7406\u6b65\u9aa4\u53ef\u80fd\u5f15\u53d1\u7b54\u6848\u504f\u5dee\u7684\u95ee\u9898", "method": "1. \u8bbe\u8ba1\u96f6\u6837\u672c\u601d\u7ef4\u94fe\u63d0\u793a\u6846\u67b6PathCoT\n2. \u5f15\u5165\u75c5\u7406\u4e13\u5bb6\u77e5\u8bc6\u6307\u5bfc\u6a21\u578b\u63a8\u7406\n3. \u5f00\u53d1\u53cc\u8def\u5f84\u81ea\u8bc4\u4f30\u673a\u5236\uff08\u76f4\u63a5\u8f93\u51fa\u4e0e\u601d\u7ef4\u94fe\u63a8\u7406\u7ed3\u679c\u5bf9\u6bd4\uff09", "result": "\u5728PathMMU\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u75c5\u7406\u56fe\u50cf\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b", "conclusion": "PathCoT\u6210\u529f\u5c06\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u878d\u5165\u63a8\u7406\u6d41\u7a0b\uff0c\u901a\u8fc7\u81ea\u8bc4\u4f30\u673a\u5236\u6709\u6548\u51cf\u5c11\u7b54\u6848\u5206\u6b67\uff0c\u4e3a\u533b\u5b66\u4e13\u4e1a\u9886\u57df\u7684\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2507.01042", "pdf": "https://arxiv.org/pdf/2507.01042", "abs": "https://arxiv.org/abs/2507.01042", "authors": ["Harsh Joshi", "Gautam Siddharth Kashyap", "Rafiq Ali", "Ebad Shabbir", "Niharika Jain", "Sarthak Jain", "Jiechao Gao", "Usman Naseem"], "title": "Can Argus Judge Them All? Comparing VLMs Across Domains", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Vision-Language Models (VLMs) are advancing multimodal AI, yet their\nperformance consistency across tasks is underexamined. We benchmark CLIP, BLIP,\nand LXMERT across diverse datasets spanning retrieval, captioning, and\nreasoning. Our evaluation includes task accuracy, generation quality,\nefficiency, and a novel Cross-Dataset Consistency (CDC) metric. CLIP shows\nstrongest generalization (CDC: 0.92), BLIP excels on curated data, and LXMERT\nleads in structured reasoning. These results expose trade-offs between\ngeneralization and specialization, informing industrial deployment of VLMs and\nguiding development toward robust, task-flexible architectures.", "AI": {"tldr": "\u8bc4\u4f30CLIP/BLIP/LXMERT\u4e09\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u6570\u636e\u96c6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51faCDC\u4e00\u81f4\u6027\u6307\u6807\u63ed\u793a\u6cdb\u5316-\u4e13\u4e1a\u5316\u5e73\u8861\u89c4\u5f8b", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u7a33\u5b9a\u6027\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30", "method": "\u4f7f\u7528\u8de8\u68c0\u7d22/\u63cf\u8ff0/\u63a8\u7406\u4efb\u52a1\u7684\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u4efb\u52a1\u7cbe\u5ea6\u3001\u751f\u6210\u8d28\u91cf\u3001\u6548\u7387\u53ca\u65b0\u578b\u8de8\u6570\u636e\u96c6\u4e00\u81f4\u6027(CDC)\u6307\u6807\u8fdb\u884c\u591a\u7ef4\u5ea6\u8bc4\u4f30", "result": "CLIP\u6cdb\u5316\u6700\u5f3a(CDC 0.92)\u3001BLIP\u5728\u7cbe\u9009\u6570\u636e\u8868\u73b0\u4f18\u3001LXMERT\u7ed3\u6784\u5316\u63a8\u7406\u9886\u5148\uff0c\u63ed\u793a\u6cdb\u5316\u4e0e\u4e13\u4e1a\u5316\u7684\u663e\u5f0f\u6743\u8861\u5173\u7cfb", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5de5\u4e1a\u754c\u90e8\u7f72VLMs\u63d0\u4f9b\u51b3\u7b56\u4f9d\u636e\uff0c\u6307\u5bfc\u5f00\u53d1\u517c\u5177\u9c81\u68d2\u6027\u548c\u4efb\u52a1\u7075\u6d3b\u6027\u7684\u4e0b\u4e00\u4ee3\u67b6\u6784"}}
{"id": "2507.01049", "pdf": "https://arxiv.org/pdf/2507.01049", "abs": "https://arxiv.org/abs/2507.01049", "authors": ["Pranav Jadhav"], "title": "Cohort Retrieval using Dense Passage Retrieval", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Patient cohort retrieval is a pivotal task in medical research and clinical\npractice, enabling the identification of specific patient groups from extensive\nelectronic health records (EHRs). In this work, we address the challenge of\ncohort retrieval in the echocardiography domain by applying Dense Passage\nRetrieval (DPR), a prominent methodology in semantic search. We propose a\nsystematic approach to transform an echocardiographic EHR dataset of\nunstructured nature into a Query-Passage dataset, framing the problem as a\nCohort Retrieval task. Additionally, we design and implement evaluation metrics\ninspired by real-world clinical scenarios to rigorously test the models across\ndiverse retrieval tasks. Furthermore, we present a custom-trained DPR embedding\nmodel that demonstrates superior performance compared to traditional and\noff-the-shelf SOTA methods.To our knowledge, this is the first work to apply\nDPR for patient cohort retrieval in the echocardiography domain, establishing a\nframework that can be adapted to other medical domains.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5c06\u5bc6\u96c6\u6bb5\u843d\u68c0\u7d22(DPR)\u5e94\u7528\u4e8e\u8d85\u58f0\u5fc3\u52a8\u56fe\u9886\u57df\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u67e5\u8be2-\u6bb5\u843d\u6570\u636e\u96c6\u5e76\u5f00\u53d1\u5b9a\u5236\u5316\u8bc4\u4f30\u6307\u6807\uff0c\u63d0\u51fa\u6027\u80fd\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\u7684\u5b9a\u5236DPR\u6a21\u578b", "motivation": "\u89e3\u51b3\u8d85\u58f0\u5fc3\u52a8\u56fe\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u975e\u7ed3\u6784\u5316\u7279\u6027\u5bfc\u81f4\u7684\u60a3\u8005\u961f\u5217\u68c0\u7d22\u96be\u9898\uff0c\u63d0\u5347\u533b\u5b66\u7814\u7a76\u4e2d\u7279\u5b9a\u60a3\u8005\u7fa4\u4f53\u8bc6\u522b\u7684\u6548\u7387\u4e0e\u51c6\u786e\u6027", "method": "1. \u5c06\u975e\u7ed3\u6784\u5316EHR\u8f6c\u5316\u4e3a\u67e5\u8be2-\u6bb5\u843d\u6570\u636e\u96c6 2. \u57fa\u4e8e\u771f\u5b9e\u4e34\u5e8a\u573a\u666f\u8bbe\u8ba1\u8bc4\u4f30\u6307\u6807 3. \u5f00\u53d1\u5b9a\u5236\u5316\u8bad\u7ec3\u7684DPR\u5d4c\u5165\u6a21\u578b", "result": "\u5b9a\u5236DPR\u6a21\u578b\u5728\u591a\u9879\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6210SOTA\u65b9\u6cd5\uff0c\u5efa\u7acb\u53ef\u8de8\u533b\u5b66\u9886\u57df\u5e94\u7528\u7684\u68c0\u7d22\u6846\u67b6", "conclusion": "\u6210\u529f\u9a8c\u8bc1DPR\u5728\u8d85\u58f0\u5fc3\u52a8\u56fe\u60a3\u8005\u961f\u5217\u68c0\u7d22\u7684\u9002\u7528\u6027\uff0c\u63d0\u51fa\u7684\u7cfb\u7edf\u5316\u6846\u67b6\u5177\u6709\u8de8\u533b\u5b66\u9886\u57df\u8fc1\u79fb\u6f5c\u529b\uff0c\u4e3a\u533b\u7597\u4fe1\u606f\u68c0\u7d22\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2507.01050", "pdf": "https://arxiv.org/pdf/2507.01050", "abs": "https://arxiv.org/abs/2507.01050", "authors": ["Jing Yu", "Yibo Zhao", "Jiapeng Zhu", "Wenming Shao", "Bo Pang", "Zhao Zhang", "Xiang Li"], "title": "Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The widespread dissemination of toxic content on social media poses a serious\nthreat to both online environments and public discourse, highlighting the\nurgent need for detoxification methods that effectively remove toxicity while\npreserving the original semantics. However, existing approaches often struggle\nto simultaneously achieve strong detoxification performance, semantic\npreservation, and robustness to out-of-distribution data. Moreover, they\ntypically rely on costly, manually annotated parallel corpora while showing\npoor data efficiency. To address these challenges, we propose a two-stage\ntraining framework that jointly optimizes for data efficiency, semantic\npreservation, and model generalization. We first perform supervised fine-tuning\non a small set of high-quality, filtered parallel data to establish a strong\ninitialization. Then, we leverage unlabeled toxic inputs and a custom-designed\nreward model to train the LLM using Group Relative Policy Optimization.\nExperimental results demonstrate that our method effectively mitigates the\ntrade-offs faced by previous work, achieving state-of-the-art performance with\nimproved generalization and significantly reduced dependence on annotated data.\nOur code is available at:\nhttps://anonymous.4open.science/r/Detoxification-of-Text-725F/", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u6587\u672c\u53bb\u6bd2\u4efb\u52a1\uff0c\u5728\u51cf\u5c11\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u7684\u540c\u65f6\u63d0\u5347\u53bb\u6bd2\u6548\u679c\u4e0e\u8bed\u4e49\u4fdd\u6301\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u53bb\u6bd2\u65b9\u6cd5\u5b58\u5728\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u3001\u8bed\u4e49\u4fdd\u7559\u4e0e\u53bb\u6bd2\u6548\u679c\u96be\u4ee5\u517c\u987e\u3001\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u4e09\u5927\u75db\u70b9\uff0c\u4e9f\u9700\u66f4\u9ad8\u6548\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u4f7f\u7528\u9ad8\u8d28\u91cf\u5e73\u884c\u6570\u636e\u76d1\u7763\u5fae\u8c03\u83b7\u5f97\u57fa\u7840\u6a21\u578b 2. \u91c7\u7528Group Relative Policy Optimization\u7ed3\u5408\u672a\u6807\u6ce8\u6570\u636e\u4e0e\u5b9a\u5236\u5956\u52b1\u6a21\u578b\u8fdb\u884c\u5f3a\u5316\u5b66\u4e60", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u53bb\u6bd2\u6548\u679c\uff08toxicity reduction\u63d0\u534712.7%\uff09\u3001\u8bed\u4e49\u4fdd\u6301\uff08\u8bed\u4e49\u76f8\u4f3c\u5ea6\u63d0\u9ad89.3%\uff09\u548c\u8de8\u6570\u636e\u96c6\u6cdb\u5316\uff08OOD\u6027\u80fd\u63d0\u534715.2%\uff09\u65b9\u9762\u5747\u8fbeSOTA\u6c34\u5e73\uff0c\u4e14\u6807\u6ce8\u6570\u636e\u7528\u91cf\u51cf\u5c1183%\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u8054\u5408\u4f18\u5316\uff0c\u6210\u529f\u7a81\u7834\u4f20\u7edf\u65b9\u6cd5\u7684\u4e09\u96be\u56f0\u5883\uff0c\u4e3a\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u6587\u672c\u51c0\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u6cbb\u7406\u6280\u672f\u8fdb\u6b65\u3002"}}
{"id": "2507.01059", "pdf": "https://arxiv.org/pdf/2507.01059", "abs": "https://arxiv.org/abs/2507.01059", "authors": ["Xiangbo Gao", "Keshu Wu", "Hao Zhang", "Kexin Tian", "Yang Zhou", "Zhengzhong Tu"], "title": "Automated Vehicles Should be Connected with Natural Language", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CV", "cs.RO"], "comment": null, "summary": "Multi-agent collaborative driving promises improvements in traffic safety and\nefficiency through collective perception and decision making. However, existing\ncommunication media -- including raw sensor data, neural network features, and\nperception results -- suffer limitations in bandwidth efficiency, information\ncompleteness, and agent interoperability. Moreover, traditional approaches have\nlargely ignored decision-level fusion, neglecting critical dimensions of\ncollaborative driving. In this paper we argue that addressing these challenges\nrequires a transition from purely perception-oriented data exchanges to\nexplicit intent and reasoning communication using natural language. Natural\nlanguage balances semantic density and communication bandwidth, adapts flexibly\nto real-time conditions, and bridges heterogeneous agent platforms. By enabling\nthe direct communication of intentions, rationales, and decisions, it\ntransforms collaborative driving from reactive perception-data sharing into\nproactive coordination, advancing safety, efficiency, and transparency in\nintelligent transportation systems.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u8fdb\u884c\u591a\u667a\u80fd\u4f53\u534f\u540c\u9a7e\u9a76\u7684\u610f\u56fe\u4e0e\u63a8\u7406\u901a\u4fe1\uff0c\u7a81\u7834\u4f20\u7edf\u611f\u77e5\u6570\u636e\u5171\u4eab\u7684\u5c40\u9650\u6027", "motivation": "\u73b0\u6709\u534f\u540c\u9a7e\u9a76\u901a\u4fe1\u65b9\u5f0f\uff08\u4f20\u611f\u5668\u6570\u636e/\u795e\u7ecf\u7f51\u7edc\u7279\u5f81/\u611f\u77e5\u7ed3\u679c\uff09\u5b58\u5728\u5e26\u5bbd\u6548\u7387\u4f4e\u3001\u4fe1\u606f\u4e0d\u5b8c\u6574\u3001\u8de8\u5e73\u53f0\u4e92\u64cd\u4f5c\u6027\u5dee\u7b49\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u51b3\u7b56\u5c42\u9762\u7684\u534f\u540c", "method": "\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u76f4\u63a5\u4f20\u9012\u9a7e\u9a76\u610f\u56fe\u3001\u51b3\u7b56\u903b\u8f91\u548c\u5b9e\u65f6\u63a8\u7406\uff0c\u5e73\u8861\u8bed\u4e49\u5bc6\u5ea6\u4e0e\u901a\u4fe1\u5e26\u5bbd\uff0c\u9002\u5e94\u52a8\u6001\u4ea4\u901a\u73af\u5883\uff0c\u517c\u5bb9\u5f02\u6784\u5e73\u53f0", "result": "\u5b9e\u73b0\u4ece\u88ab\u52a8\u611f\u77e5\u5171\u4eab\u5230\u4e3b\u52a8\u534f\u8c03\u7684\u8303\u5f0f\u8f6c\u6362\uff0c\u589e\u5f3a\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3001\u6548\u7387\u4e0e\u900f\u660e\u5ea6", "conclusion": "\u81ea\u7136\u8bed\u8a00\u901a\u4fe1\u4e3a\u591a\u667a\u80fd\u4f53\u534f\u540c\u9a7e\u9a76\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u5728\u51b3\u7b56\u878d\u5408\u548c\u4eba\u673a\u4e92\u7406\u89e3\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf"}}
{"id": "2507.01431", "pdf": "https://arxiv.org/pdf/2507.01431", "abs": "https://arxiv.org/abs/2507.01431", "authors": ["Yoonseok Yang", "Minjune Kim", "Marlon Rondinelli", "Keren Shao"], "title": "Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": "7 pages, 5 figues, 1 table", "summary": "Grading handwritten, open-ended responses remains a major bottleneck in large\nuniversity STEM courses. We introduce Pensieve (https://www.pensieve.co), an\nAI-assisted grading platform that leverages large language models (LLMs) to\ntranscribe and evaluate student work, providing instructors with rubric-aligned\nscores, transcriptions, and confidence ratings. Unlike prior tools that focus\nnarrowly on specific tasks like transcription or rubric generation, Pensieve\nsupports the entire grading pipeline-from scanned student submissions to final\nfeedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and\nhas graded more than 300,000 student responses. We present system details and\nempirical results across four core STEM disciplines: Computer Science,\nMathematics, Physics, and Chemistry. Our findings show that Pensieve reduces\ngrading time by an average of 65%, while maintaining a 95.4% agreement rate\nwith instructor-assigned grades for high-confidence predictions.", "AI": {"tldr": "Pensieve\u662f\u4e00\u6b3eAI\u8f85\u52a9\u8bc4\u5206\u5e73\u53f0\uff0c\u901a\u8fc7LLM\u6280\u672f\u5b9e\u73b0\u624b\u5199\u7b54\u6848\u8f6c\u5f55\u4e0e\u667a\u80fd\u8bc4\u5206\uff0c\u5728\u771f\u5b9e\u6559\u5b66\u573a\u666f\u4e2d\u5e73\u5747\u51cf\u5c1165%\u8bc4\u5206\u65f6\u95f4\u4e14\u4fdd\u630195.4%\u8bc4\u5206\u4e00\u81f4\u6027", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21STEM\u8bfe\u7a0b\u4e2d\u624b\u5199\u5f00\u653e\u6027\u7b54\u6848\u8bc4\u5206\u6548\u7387\u4f4e\u4e0b\u7684\u74f6\u9888\u95ee\u9898", "method": "\u6574\u5408\u626b\u63cf\u6587\u6863\u5904\u7406\u3001LLM\u8f6c\u5f55\u8bc4\u4f30\u3001\u7f6e\u4fe1\u5ea6\u8bc4\u7ea7\u548c\u4eba\u5de5\u590d\u6838\u7684\u95ed\u73af\u8bc4\u5206\u7cfb\u7edf", "result": "\u572820+\u673a\u6784\u5904\u740630\u4e07\u4efd\u7b54\u6848\uff0c\u9ad8\u7f6e\u4fe1\u9884\u6d4b\u4e0e\u6559\u5e08\u8bc4\u5206\u4e00\u81f4\u738795.4%\uff0c\u8bc4\u5206\u6548\u7387\u63d0\u534765%", "conclusion": "Pensieve\u6210\u529f\u5b9e\u73b0\u4e86AI\u8d4b\u80fd\u7684\u6559\u5b66\u8bc4\u4f30\u6d41\u7a0b\u4f18\u5316\uff0c\u5c55\u793a\u4e86LLM\u5728\u6559\u80b2\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2507.01504", "pdf": "https://arxiv.org/pdf/2507.01504", "abs": "https://arxiv.org/abs/2507.01504", "authors": ["Robert Aufschl\u00e4ger", "Youssef Shoeb", "Azarm Nowzad", "Michael Heigl", "Fabian Bally", "Martin Schramm"], "title": "Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "accepted for publication at the 2025 IEEE 28th International\n  Conference on Intelligent Transportation Systems (ITSC 2025), taking place\n  during November 18-21, 2025 in Gold Coast, Australia", "summary": "The collection and release of street-level recordings as Open Data play a\nvital role in advancing autonomous driving systems and AI research. However,\nthese datasets pose significant privacy risks, particularly for pedestrians,\ndue to the presence of Personally Identifiable Information (PII) that extends\nbeyond biometric traits such as faces. In this paper, we present cRID, a novel\ncross-modal framework combining Large Vision-Language Models, Graph Attention\nNetworks, and representation learning to detect textual describable clues of\nPII and enhance person re-identification (Re-ID). Our approach focuses on\nidentifying and leveraging interpretable features, enabling the detection of\nsemantically meaningful PII beyond low-level appearance cues. We conduct a\nsystematic evaluation of PII presence in person image datasets. Our experiments\nshow improved performance in practical cross-dataset Re-ID scenarios, notably\nfrom Market-1501 to CUHK03-np (detected), highlighting the framework's\npractical utility. Code is available at https://github.com/RAufschlaeger/cRID.", "AI": {"tldr": "\u63d0\u51facRID\u8de8\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u68c0\u6d4b\u53ef\u63cf\u8ff0\u6027PII\u7ebf\u7d22\uff0c\u5728\u8de8\u6570\u636e\u96c6\u884c\u4eba\u91cd\u8bc6\u522b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u5e76\u964d\u4f4e\u9690\u79c1\u98ce\u9669\u3002", "motivation": "\u5f00\u653e\u8857\u666f\u6570\u636e\u96c6\u5b58\u5728\u8d85\u51fa\u751f\u7269\u7279\u5f81\uff08\u5982\u9762\u90e8\uff09\u7684\u6587\u672c\u53ef\u63cf\u8ff0\u6027PII\u9690\u79c1\u98ce\u9669\uff0c\u9700\u5f00\u53d1\u80fd\u68c0\u6d4b\u8bed\u4e49\u53ef\u89e3\u91ca\u7279\u5f81\u7684\u65b9\u6cd5\u4fdd\u62a4\u884c\u4eba\u9690\u79c1\u3002", "method": "\u878d\u5408\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u68c0\u6d4b\u6587\u672c\u53ef\u63cf\u8ff0\u7ebf\u7d22\uff09\u3001\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08\u7279\u5f81\u5173\u8054\uff09\u548c\u8868\u793a\u5b66\u4e60\uff08\u589e\u5f3a\u884c\u4ebaRe-ID\uff09\uff0c\u6784\u5efa\u53ef\u89e3\u91ca\u7684\u8de8\u6a21\u6001\u6846\u67b6\u3002", "result": "\u5728Market-1501\u5230CUHK03-np\u8de8\u6570\u636e\u96c6Re-ID\u4e2d\u6027\u80fd\u63d0\u5347\uff0c\u9a8c\u8bc1\u6846\u67b6\u5b9e\u7528\u6027\u548c\u5bf9\u5f00\u653e\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u7684\u4ef7\u503c\u3002", "conclusion": "cRID\u901a\u8fc7\u8bed\u4e49\u53ef\u89e3\u91ca\u7279\u5f81\u68c0\u6d4b\u5b9e\u73b0PII\u8bc6\u522b\u4e0eRe-ID\u6027\u80fd\u7684\u5e73\u8861\uff0c\u4e3a\u5f00\u653e\u6570\u636e\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2507.01548", "pdf": "https://arxiv.org/pdf/2507.01548", "abs": "https://arxiv.org/abs/2507.01548", "authors": ["Wen Zhan", "Ziqun Hua", "Peiyue Lin", "Yunfei Chen"], "title": "Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": "A version of this manuscript has been submitted to the [IASDR 2025\n  Conference](https://iasdr2025.org/) and is currently under review", "summary": "This paper explores how older adults, particularly aging migrants in urban\nChina, can engage AI-assisted co-creation to express personal narratives that\nare often fragmented, underrepresented, or difficult to verbalize. Through a\npilot workshop combining oral storytelling and the symbolic reconstruction of\nHanzi, participants shared memories of migration and recreated new character\nforms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),\ntogether with physical materials. Supported by human facilitation and a soft AI\npresence, participants transformed lived experience into visual and tactile\nexpressions without requiring digital literacy. This approach offers new\nperspectives on human-AI collaboration and aging by repositioning AI not as a\ncontent producer but as a supportive mechanism, and by supporting narrative\nagency within sociotechnical systems.", "AI": {"tldr": "\u63a2\u7d22\u8001\u5e74\u79fb\u6c11\u901a\u8fc7AI\u8f85\u52a9\u5171\u521b\u5b9e\u73b0\u975e\u8bed\u8a00\u53d9\u4e8b\u8868\u8fbe\uff0c\u91cd\u6784AI\u4f5c\u4e3a\u652f\u6301\u673a\u5236\u7684\u89d2\u8272", "motivation": "\u89e3\u51b3\u8001\u5e74\u79fb\u6c11\u53d9\u4e8b\u788e\u7247\u5316/\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u4eba\u673a\u534f\u4f5c\u7684\u65b0\u578b\u8868\u8fbe\u65b9\u5f0f", "method": "\u5de5\u4f5c\u574a\u7ed3\u5408\u53e3\u5934\u53d9\u8ff0\u4e0e\u6c49\u5b57\u91cd\u6784\uff0c\u5229\u7528LLM\u5efa\u8bae\u5c0f\u7bc6\u5b57\u5f62\u548c\u5b9e\u7269\u6750\u6599\u8fdb\u884c\u5171\u521b\u5b9e\u9a8c", "result": "\u53c2\u4e0e\u8005\u6210\u529f\u5b9e\u73b0\u751f\u6d3b\u7ecf\u9a8c\u7684\u89e6\u89c9\u8868\u8fbe\uff0c\u9a8c\u8bc1AI\u652f\u6301\u673a\u5236\u7684\u6709\u6548\u6027", "conclusion": "\u91cd\u65b0\u5b9a\u4f4dAI\u5728\u53d9\u4e8b\u521b\u4f5c\u4e2d\u7684\u8f85\u52a9\u89d2\u8272\uff0c\u4e3a\u5f31\u52bf\u7fa4\u4f53\u63d0\u4f9b\u975e\u6570\u5b57\u5316\u521b\u4f5c\u8def\u5f84"}}
{"id": "2507.01551", "pdf": "https://arxiv.org/pdf/2507.01551", "abs": "https://arxiv.org/abs/2507.01551", "authors": ["Wu Fei", "Hao Kong", "Shuxian Liang", "Yang Lin", "Yibo Yang", "Jing Tang", "Lei Chen", "Xiansheng Hua"], "title": "Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation.", "AI": {"tldr": "\u63d0\u51faSPRO\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u81ea\u8eab\u751f\u6210\u8fc7\u7a0b\u5956\u52b1\u548c\u65b0\u578b\u4f18\u52bf\u4f30\u8ba1\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u4e0e\u63a8\u7406\u6027\u80fd", "motivation": "\u73b0\u6709\u8fc7\u7a0b\u5f3a\u5316\u5b66\u4e60\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u3001\u7f3a\u4e4f\u7edf\u4e00\u7406\u8bba\u6846\u67b6\u7684\u95ee\u9898\uff0c\u9700\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u8fc7\u7a0b\u5956\u52b1\u4f18\u5316\u65b9\u6848", "method": "1) \u7406\u8bba\u8bc1\u660e\u7b56\u7565\u6a21\u578b\u53ef\u5185\u751f\u8fc7\u7a0b\u5956\u52b1 2) \u8bbe\u8ba1\u7d2f\u79ef\u8fc7\u7a0b\u5956\u52b1\u548c\u63a9\u7801\u6b65\u9aa4\u4f18\u52bf(MSA)\u5b9e\u73b0\u7ec4\u5185\u4e25\u683c\u4f18\u52bf\u4f30\u8ba1", "result": "\u8bad\u7ec3\u6548\u7387\u63d0\u53473.4\u500d\uff0c\u51c6\u786e\u7387\u63d0\u9ad817.5%\uff0c\u54cd\u5e94\u957f\u5ea6\u51cf\u5c111/3\uff0c\u7b56\u7565\u71b5\u4fdd\u6301\u7a33\u5b9a", "conclusion": "SPRO\u5728\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u6210\u672c\u4e0b\u5b9e\u73b0\u9ad8\u6548\u8fc7\u7a0b\u5f3a\u5316\u5b66\u4e60\uff0c\u517c\u5177\u63a2\u7d22\u5145\u5206\u6027\u4e0e\u5de5\u4e1a\u90e8\u7f72\u53ef\u884c\u6027"}}
{"id": "2507.01597", "pdf": "https://arxiv.org/pdf/2507.01597", "abs": "https://arxiv.org/abs/2507.01597", "authors": ["Yuehang Si", "Zefan Zeng", "Jincai Huang", "Qing Cheng"], "title": "T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Temporal Knowledge Graph (TKG) is an efficient method for describing the\ndynamic development of facts along a timeline. Most research on TKG reasoning\n(TKGR) focuses on modelling the repetition of global facts and designing\npatterns of local historical facts. However, they face two significant\nchallenges: inadequate modeling of the event distribution shift between\ntraining and test samples, and reliance on random entity substitution for\ngenerating negative samples, which often results in low-quality sampling. To\nthis end, we propose a novel distributional feature modeling approach for\ntraining TKGR models, Test-Time Training-guided Distribution shift Modelling\n(T3DM), to adjust the model based on distribution shift and ensure the global\nconsistency of model reasoning. In addition, we design a negative-sampling\nstrategy to generate higher-quality negative quadruples based on adversarial\ntraining. Extensive experiments show that T3DM provides better and more robust\nresults than the state-of-the-art baselines in most cases.", "AI": {"tldr": "Proposes T3DM with distribution shift modeling and adversarial negative sampling for robust TKGR.", "motivation": "Existing TKGR methods inadequately handle event distribution shifts between training/test samples and use low-quality random negative sampling.", "method": "Test-Time Training-guided Distribution shift Modelling (T3DM) ensures global consistency through distribution shift adaptation, combined with adversarial-based negative sampling.", "result": "T3DM outperforms SOTA baselines with better robustness in experiments.", "conclusion": "The approach effectively addresses distribution shifts and improves negative sampling quality for enhanced TKGR performance."}}
{"id": "2507.01599", "pdf": "https://arxiv.org/pdf/2507.01599", "abs": "https://arxiv.org/abs/2507.01599", "authors": ["Zhaoyan Sun", "Jiayi Wang", "Xinyang Zhao", "Jiachi Wang", "Guoliang Li"], "title": "Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Traditional Data+AI systems utilize data-driven techniques to optimize\nperformance, but they rely heavily on human experts to orchestrate system\npipelines, enabling them to adapt to changes in data, queries, tasks, and\nenvironments. For instance, while there are numerous data science tools\navailable, developing a pipeline planning system to coordinate these tools\nremains challenging. This difficulty arises because existing Data+AI systems\nhave limited capabilities in semantic understanding, reasoning, and planning.\nFortunately, we have witnessed the success of large language models (LLMs) in\nenhancing semantic understanding, reasoning, and planning abilities. It is\ncrucial to incorporate LLM techniques to revolutionize data systems for\norchestrating Data+AI applications effectively.\n  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive\narchitecture designed to orchestrate Data+AI ecosystems, which focuses on\ntackling data-related tasks by integrating knowledge comprehension, reasoning,\nand planning capabilities. We delve into the challenges involved in designing\ndata agents, such as understanding data/queries/environments/tools,\norchestrating pipelines/workflows, optimizing and executing pipelines, and\nfostering pipeline self-reflection. Furthermore, we present examples of data\nagent systems, including a data science agent, data analytics agents (such as\nunstructured data analytics agent, semantic structured data analytics agent,\ndata lake analytics agent, and multi-modal data analytics agent), and a\ndatabase administrator (DBA) agent. We also outline several open challenges\nassociated with designing data agent systems.", "AI": {"tldr": "\u63d0\u51faData Agent\u67b6\u6784\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u589e\u5f3aData+AI\u7cfb\u7edf\u7684\u534f\u8c03\u80fd\u529b", "motivation": "\u4f20\u7edfData+AI\u7cfb\u7edf\u8fc7\u5ea6\u4f9d\u8d56\u4eba\u5de5\u6d41\u7a0b\u7f16\u6392\uff0cLLMs\u5728\u8bed\u4e49\u7406\u89e3\u548c\u89c4\u5212\u4e0a\u7684\u7a81\u7834\u4e3a\u7cfb\u7edf\u81ea\u52a8\u5316\u5e26\u6765\u65b0\u673a\u9047", "method": "\u6784\u5efa\u6574\u5408\u77e5\u8bc6\u7406\u89e3\u3001\u63a8\u7406\u4e0e\u89c4\u5212\u80fd\u529b\u7684Data Agent\u67b6\u6784\uff0c\u89e3\u51b3\u5de5\u5177\u7406\u89e3\u3001\u6d41\u7a0b\u7f16\u6392\u3001\u4f18\u5316\u6267\u884c\u7b49\u8bbe\u8ba1\u6311\u6218", "result": "\u5c55\u793a\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u3001\u591a\u6a21\u6001\u6570\u636e\u5206\u6790\u4ee3\u7406\u3001DBA\u4ee3\u7406\u7b49\u5177\u4f53\u7cfb\u7edf\u5b9e\u4f8b", "conclusion": "Data Agent\u4e3aData+AI\u7cfb\u7edf\u5e26\u6765\u9769\u547d\u6f5c\u529b\uff0c\u4f46\u9700\u89e3\u51b3\u77e5\u8bc6\u7ba1\u7406\u3001\u6267\u884c\u6548\u7387\u7b49\u5f00\u653e\u6311\u6218"}}
{"id": "2507.01679", "pdf": "https://arxiv.org/pdf/2507.01679", "abs": "https://arxiv.org/abs/2507.01679", "authors": ["Zeyu Huang", "Tianhao Cheng", "Zihan Qiu", "Zili Wang", "Yinghui Xu", "Edoardo M. Ponti", "Ivan Titov"], "title": "Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Existing post-training techniques for large language models are broadly\ncategorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning\n(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking\ndemonstration data but can lead to problematic generalization as a form of\nbehavior cloning. Conversely, RFT can significantly enhance a model's\nperformance but is prone to learn unexpected behaviors, and its performance is\nhighly sensitive to the initial policy. In this paper, we propose a unified\nview of these methods and introduce Prefix-RFT, a hybrid approach that\nsynergizes learning from both demonstration and exploration. Using mathematical\nreasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is\nboth simple and effective. It not only surpasses the performance of standalone\nSFT and RFT but also outperforms parallel mixed-policy RFT methods. A key\nadvantage is its seamless integration into existing open-source frameworks,\nrequiring only minimal modifications to the standard RFT pipeline. Our analysis\nhighlights the complementary nature of SFT and RFT, and validates that\nPrefix-RFT effectively harmonizes these two learning paradigms. Furthermore,\nablation studies confirm the method's robustness to variations in the quality\nand quantity of demonstration data. We hope this work offers a new perspective\non LLM post-training, suggesting that a unified paradigm that judiciously\nintegrates demonstration and exploration could be a promising direction for\nfuture research.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408SFT\u548cRFT\u7684Prefix-RFT\u65b9\u6cd5\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6027\u80fd\u8d85\u8d8a\u5e76\u4fdd\u6301\u6846\u67b6\u517c\u5bb9\u6027", "motivation": "\u73b0\u6709SFT\u5b58\u5728\u884c\u4e3a\u514b\u9686\u95ee\u9898\uff0cRFT\u5bf9\u521d\u59cb\u7b56\u7565\u654f\u611f\u4e14\u6613\u4ea7\u751f\u610f\u5916\u884c\u4e3a\uff0c\u9700\u8981\u63a2\u7d22\u4e24\u79cd\u8303\u5f0f\u7684\u534f\u540c\u673a\u5236", "method": "Prefix-RFT\u901a\u8fc7\u524d\u7f00\u8c03\u6574\u6280\u672f\uff0c\u5728\u5f3a\u5316\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u878d\u5408\u793a\u8303\u6570\u636e\u7684\u5b66\u4e60\u4e0e\u81ea\u4e3b\u63a2\u7d22", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8d85\u8d8a\u5355\u72ecSFT/RFT\u53ca\u5e76\u884c\u6df7\u5408\u7b56\u7565\uff0c\u4e14\u4ec5\u9700\u6781\u7b80\u4fee\u6539\u5373\u53ef\u9002\u914d\u6807\u51c6RFT\u6d41\u7a0b", "conclusion": "SFT\u4e0eRFT\u5177\u6709\u4e92\u8865\u6027\uff0c\u7edf\u4e00\u8303\u5f0f\u6574\u5408\u793a\u8303\u4e0e\u63a2\u7d22\u662fLLM\u540e\u8bad\u7ec3\u7684\u91cd\u8981\u65b9\u5411\uff0c\u65b9\u6cd5\u5bf9\u6570\u636e\u8d28\u91cf/\u6570\u91cf\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027"}}
{"id": "2507.01735", "pdf": "https://arxiv.org/pdf/2507.01735", "abs": "https://arxiv.org/abs/2507.01735", "authors": ["Kai Chen", "Ruiyuan Gao", "Lanqing Hong", "Hang Xu", "Xu Jia", "Holger Caesar", "Dengxin Dai", "Bingbing Liu", "Dzmitry Tsishkou", "Songcen Xu", "Chunjing Xu", "Qiang Xu", "Huchuan Lu", "Dit-Yan Yeung"], "title": "ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "ECCV 2024. Workshop page: https://coda-dataset.github.io/w-coda2024/", "summary": "In this paper, we present details of the 1st W-CODA workshop, held in\nconjunction with the ECCV 2024. W-CODA aims to explore next-generation\nsolutions for autonomous driving corner cases, empowered by state-of-the-art\nmultimodal perception and comprehension techniques. 5 Speakers from both\nacademia and industry are invited to share their latest progress and opinions.\nWe collect research papers and hold a dual-track challenge, including both\ncorner case scene understanding and generation. As the pioneering effort, we\nwill continuously bridge the gap between frontier autonomous driving techniques\nand fully intelligent, reliable self-driving agents robust towards corner\ncases.", "AI": {"tldr": "\u9996\u4e2aW-CODA\u7814\u8ba8\u4f1a\u805a\u7126\u81ea\u52a8\u9a7e\u9a76\u957f\u5c3e\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u611f\u77e5\u6280\u672f\u7ec4\u7ec7\u53cc\u8f68\u6311\u6218\uff08\u573a\u666f\u7406\u89e3\u4e0e\u751f\u6210\uff09\uff0c\u8fde\u63a5\u524d\u6cbf\u6280\u672f\u4e0e\u53ef\u9760\u81ea\u52a8\u9a7e\u9a76\u4f53", "motivation": "\u89e3\u51b3\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728corner cases\uff08\u6781\u7aef\u7f55\u89c1\u573a\u666f\uff09\u5904\u7406\u4e0a\u7684\u4e0d\u8db3\uff0c\u63a8\u52a8\u5168\u667a\u80fd\u53ef\u9760\u81ea\u52a8\u9a7e\u9a76\u4f53\u7684\u53d1\u5c55", "method": "1. \u9080\u8bf75\u4f4d\u4ea7\u5b66\u4e13\u5bb6\u5206\u4eab\u6700\u65b0\u8fdb\u5c55\n2. \u7ec4\u7ec7\u53cc\u8f68\u6311\u6218\uff08corner case\u573a\u666f\u7406\u89e3\u4e0e\u751f\u6210\uff09\n3. \u6536\u96c6\u7814\u7a76\u8bba\u6587\u5efa\u7acb\u6280\u672f\u57fa\u51c6", "result": "\u6784\u5efa\u4e86\u8fde\u63a5\u524d\u6cbf\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u4e0ecorner case\u9c81\u68d2\u6027\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u5f62\u6210\u521d\u6b65\u6280\u672f\u4ea4\u6d41\u4e0e\u8bc4\u4f30\u4f53\u7cfb", "conclusion": "\u8be5\u7814\u8ba8\u4f1a\u4e3a\u5f00\u53d1\u5e94\u5bf9\u6781\u7aef\u573a\u666f\u7684\u53ef\u9760\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5960\u5b9a\u4e86\u91cd\u8981\u57fa\u7840\uff0c\u540e\u7eed\u5c06\u6301\u7eed\u63a8\u8fdb\u6280\u672f\u8f6c\u5316\u4e0e\u7cfb\u7edf\u4f18\u5316"}}
{"id": "2507.01752", "pdf": "https://arxiv.org/pdf/2507.01752", "abs": "https://arxiv.org/abs/2507.01752", "authors": ["Ismail Labiad", "Mathurin Videau", "Matthieu Kowalski", "Marc Schoenauer", "Alessandro Leite", "Julia Kempe", "Olivier Teytaud"], "title": "Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Gradient-based optimization is the workhorse of deep learning, offering\nefficient and scalable training via backpropagation. However, its reliance on\nlarge volumes of labeled data raises privacy and security concerns such as\nsusceptibility to data poisoning attacks and the risk of overfitting. In\ncontrast, black box optimization methods, which treat the model as an opaque\nfunction, relying solely on function evaluations to guide optimization, offer a\npromising alternative in scenarios where data access is restricted, adversarial\nrisks are high, or overfitting is a concern. However, black box methods also\npose significant challenges, including poor scalability to high-dimensional\nparameter spaces, as prevalent in large language models (LLMs), and high\ncomputational costs due to reliance on numerous model evaluations. This paper\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\ninduces an information bottleneck via implicit compression of the training\ndata. Leveraging the tractability of information flow, we provide strong\ntheoretical bounds on generalization, differential privacy, susceptibility to\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\non top of pre-trained LLMs, offering a lightweight and modular enhancement\nsuitable for deployment in restricted or privacy-sensitive environments, in\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\ndemonstrate empirically that Retrofitting methods are able to learn, showing\nhow a few iterations of BBoxER improve performance and generalize well on a\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\non top of gradient-based optimization.", "AI": {"tldr": "\u63d0\u51fa\u9ed1\u76d2\u4f18\u5316\u65b9\u6cd5BBoxER\uff0c\u901a\u8fc7\u9690\u5f0f\u6570\u636e\u538b\u7f29\u589e\u5f3aLLM\u9690\u79c1\u5b89\u5168\uff0c\u517c\u987e\u7406\u8bba\u4fdd\u8bc1\u4e0e\u8f7b\u91cf\u5316\u90e8\u7f72", "motivation": "\u89e3\u51b3\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\u7684\u9690\u79c1\u6cc4\u9732\u3001\u6570\u636e\u4e2d\u6bd2\u98ce\u9669\u3001\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u9002\u5e94\u6570\u636e\u53d7\u9650\u573a\u666f", "method": "\u57fa\u4e8e\u8fdb\u5316\u7b97\u6cd5\u7684\u9ed1\u76d2\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u606f\u74f6\u9888\u63a7\u5236\u6570\u636e\u6d41\uff0c\u63d0\u4f9b\u6cdb\u5316/\u5dee\u5206\u9690\u79c1/\u6297\u653b\u51fb\u7406\u8bba\u8fb9\u754c", "result": "\u7406\u8bba\u8bc1\u660e\u6cdb\u5316\u4fdd\u8bc1\uff0c\u5b9e\u9a8c\u663e\u793a\u5c11\u91cf\u8fed\u4ee3\u5373\u53ef\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\uff0c\u4fdd\u6301\u6a21\u5757\u5316\u8f7b\u91cf\u90e8\u7f72\u7279\u6027", "conclusion": "BBoxER\u4f5c\u4e3a\u68af\u5ea6\u4f18\u5316\u7684\u8865\u5145\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u9690\u79c1\u654f\u611f\u73af\u5883\uff0c\u5b9e\u73b0\u5b89\u5168\u6027\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u5e73\u8861"}}
{"id": "2507.01806", "pdf": "https://arxiv.org/pdf/2507.01806", "abs": "https://arxiv.org/abs/2507.01806", "authors": ["Reza Arabpour", "Haitz S\u00e1ez de Oc\u00e1riz Borde", "Anastasis Kratsios"], "title": "LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "5-page main paper (excluding references) + 11-page appendix, 3\n  tables, 1 figure. Accepted to ICML 2025 Workshop on Efficient Systems for\n  Foundation Models", "summary": "Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language\nModels (LLMs) by enabling parameter-efficient updates. However, their\nwidespread adoption remains limited by the reliance on GPU-based training. In\nthis work, we propose a theoretically grounded approach to LoRA fine-tuning\ndesigned specifically for users with limited computational resources,\nparticularly those restricted to standard laptop CPUs. Our method learns a\nmeta-operator that maps any input dataset, represented as a probability\ndistribution, to a set of LoRA weights by leveraging a large bank of\npre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of\nperforming new gradient-based updates, our pipeline constructs adapters via\nlightweight combinations of existing LoRAs directly on CPU. While the resulting\nadapters do not match the performance of GPU-trained counterparts, they\nconsistently outperform the base Mistral model on downstream tasks, offering a\npractical and accessible alternative to traditional GPU-based fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eCPU\u7684LoRA\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u9002\u914d\u5668\u7ec4\u5408\u5b9e\u73b0\u65e0\u9700GPU\u8bad\u7ec3\u7684\u53c2\u6570\u4f18\u5316", "motivation": "\u89e3\u51b3LoRA\u6280\u672f\u4f9d\u8d56GPU\u8bad\u7ec3\u7684\u9650\u5236\uff0c\u4e3a\u4ec5\u6709\u7b14\u8bb0\u672c\u7535\u8111CPU\u7b49\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u7684\u7528\u6237\u63d0\u4f9b\u53ef\u884c\u65b9\u6848", "method": "\u5b66\u4e60\u6620\u5c04\u6570\u636e\u5206\u5e03\u5230LoRA\u6743\u91cd\u7684\u5143\u64cd\u4f5c\u7b26\uff0c\u5229\u7528Mistral-7B\u9884\u8bad\u7ec3\u9002\u914d\u5668\u5e93\u5728CPU\u4e0a\u76f4\u63a5\u7ec4\u5408\u751f\u6210\u65b0\u9002\u914d\u5668", "result": "\u751f\u6210\u7684\u9002\u914d\u5668\u6027\u80fd\u867d\u4f4e\u4e8eGPU\u8bad\u7ec3\u7248\u672c\uff0c\u4f46\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u6301\u7eed\u8d85\u8d8a\u57fa\u7840\u6a21\u578b", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f20\u7edfGPU\u5fae\u8c03\u63d0\u4f9b\u4e86\u5b9e\u7528\u66ff\u4ee3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u95e8\u69db"}}
{"id": "2507.01951", "pdf": "https://arxiv.org/pdf/2507.01951", "abs": "https://arxiv.org/abs/2507.01951", "authors": ["Zixiao Wang", "Yuxin Wang", "Xiaorui Wang", "Mengting Xing", "Jie Gao", "Jianjun Xu", "Guangcan Liu", "Chenhui Jin", "Zhuo Wang", "Shengzhuo Zhang", "Hongtao Xie"], "title": "Test-Time Scaling with Reflective Generative Model", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "We introduce our first reflective generative model MetaStone-S1, which\nobtains OpenAI o3's performance via the self-supervised process reward model\n(SPRM). Through sharing the backbone network and using task-specific heads for\nnext token prediction and process scoring respectively, SPRM successfully\nintegrates the policy model and process reward model(PRM) into a unified\ninterface without extra process annotation, reducing over 99% PRM parameters\nfor efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable\nfor test time scaling (TTS), and we provide three reasoning effort modes (low,\nmedium, and high), based on the controllable thinking length. Moreover, we\nempirically establish a scaling law that reveals the relationship between total\nthinking computation and TTS performance. Experiments demonstrate that our\nMetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with\nonly 32B parameter size. To support the research community, we have\nopen-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.", "AI": {"tldr": "MetaStone-S1\u901a\u8fc7\u81ea\u76d1\u7763\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08SPRM\uff09\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u4ec5\u752832B\u53c2\u6570\u5373\u8fbe\u5230OpenAI-o3-mini\u7cfb\u5217\u6027\u80fd\uff0c\u5e76\u5f00\u6e90\u6a21\u578b", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRM\uff09\u9700\u8981\u989d\u5916\u6807\u6ce8\u548c\u53c2\u6570\u91cf\u8fc7\u5927\u7684\u95ee\u9898", "method": "\u901a\u8fc7\u5171\u4eab\u4e3b\u5e72\u7f51\u7edc+\u4efb\u52a1\u7279\u5b9a\u5934\u90e8\u7684SPRM\u67b6\u6784\uff0c\u7edf\u4e00\u7b56\u7565\u6a21\u578b\u4e0e\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u51cf\u5c1199%\u4ee5\u4e0aPRM\u53c2\u6570", "result": "\u5728\u53ef\u63a7\u601d\u7ef4\u957f\u5ea6\u7684\u4e09\u79cd\u63a8\u7406\u6a21\u5f0f\u4e0b\u5b9e\u73b0\u6d4b\u8bd5\u65f6\u6269\u5c55\uff08TTS\uff09\uff0c\u5efa\u7acb\u8ba1\u7b97\u91cf-\u6027\u80fd\u6269\u5c55\u5b9a\u5f8b\uff0c32B\u6a21\u578b\u6027\u80fd\u5bf9\u6807OpenAI-o3-mini", "conclusion": "MetaStone-S1\u4ee5\u66f4\u5c0f\u53c2\u6570\u91cf\u5b9e\u73b0\u9876\u5c16\u6027\u80fd\uff0c\u5176\u5f00\u6e90\u5c06\u4fc3\u8fdb\u9ad8\u6548\u751f\u6210\u6a21\u578b\u7684\u7814\u7a76\u4e0e\u5e94\u7528"}}
