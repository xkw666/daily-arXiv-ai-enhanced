{"id": "2511.00702", "pdf": "https://arxiv.org/pdf/2511.00702", "abs": "https://arxiv.org/abs/2511.00702", "authors": ["Alberto Di Biase"], "title": "Applying Medical Imaging Tractography Techniques to Painterly Rendering of Images", "categories": ["cs.GR", "cs.CV"], "comment": "Exploratory investigation applying medical imaging tractography\n  techniques to painterly image rendering. Code available at\n  https://github.com/tito21/st-python", "summary": "Doctors and researchers routinely use diffusion tensor imaging (DTI) and\ntractography to visualize the fibrous structure of tissues in the human body.\nThis paper explores the connection of these techniques to the painterly\nrendering of images. Using a tractography algorithm the presented method can\nplace brush strokes that mimic the painting process of human artists,\nanalogously to how fibres are tracked in DTI. The analogue to the diffusion\ntensor for image orientation is the structural tensor, which can provide better\nlocal orientation information than the gradient alone. I demonstrate this\ntechnique in portraits and general images, and discuss the parallels between\nfibre tracking and brush stroke placement, and frame it in the language of\ntractography. This work presents an exploratory investigation into the\ncross-domain application of diffusion tensor imaging techniques to painterly\nrendering of images. All the code is available at\nhttps://github.com/tito21/st-python"}
{"id": "2511.00898", "pdf": "https://arxiv.org/pdf/2511.00898", "abs": "https://arxiv.org/abs/2511.00898", "authors": ["Heng Zhang", "Jing Liu", "Jiajun Wu", "Haochen You", "Lubin Gan", "Yuling Shi", "Xiaodong Gu", "Zijian Zhang", "Shuai Chen", "Wenjun Huang", "Jin Huang"], "title": "Empowering LLMs with Structural Role Inference for Zero-Shot Graph Learning", "categories": ["cs.GR"], "comment": null, "summary": "Large Language Models have emerged as a promising approach for graph learning\ndue to their powerful reasoning capabilities. However, existing methods exhibit\nsystematic performance degradation on structurally important nodes such as\nbridges and hubs. We identify the root cause of these limitations. Current\napproaches encode graph topology into static features but lack reasoning\nscaffolds to transform topological patterns into role-based interpretations.\nThis limitation becomes critical in zero-shot scenarios where no training data\nestablishes structure-semantics mappings. To address this gap, we propose\nDuoGLM, a training-free dual-perspective framework for structure-aware graph\nreasoning. The local perspective constructs relation-aware templates capturing\nsemantic interactions between nodes and neighbors. The global perspective\nperforms topology-to-role inference to generate functional descriptions of\nstructural positions. These complementary perspectives provide explicit\nreasoning mechanisms enabling LLMs to distinguish topologically similar but\nsemantically different nodes. Extensive experiments across eight benchmark\ndatasets demonstrate substantial improvements. DuoGLM achieves 14.3\\% accuracy\ngain in zero-shot node classification and 7.6\\% AUC improvement in cross-domain\ntransfer compared to existing methods. The results validate the effectiveness\nof explicit role reasoning for graph understanding with LLMs."}
{"id": "2511.00911", "pdf": "https://arxiv.org/pdf/2511.00911", "abs": "https://arxiv.org/abs/2511.00911", "authors": ["Heng Zheng", "Haochen You", "Zijun Liu", "Zijian Zhang", "Lubin Gan", "Hao Zhang", "Wenjun Huang", "Jin Huang"], "title": "G2rammar: Bilingual Grammar Modeling for Enhanced Text-attributed Graph Learning", "categories": ["cs.GR"], "comment": null, "summary": "Text-attributed graphs require models to effectively integrate both\nstructural topology and semantic content. Recent approaches apply large\nlanguage models to graphs by linearizing structures into token sequences\nthrough random walks. These methods create concise graph vocabularies to\nreplace verbose natural language descriptions. However, they overlook a\ncritical component that makes language expressive: grammar. In natural\nlanguage, grammar assigns syntactic roles to words and defines their functions\nwithin sentences. Similarly, nodes in graphs play distinct structural roles as\nhubs, bridges, or peripheral members. Current graph language methods provide\ntokens without grammatical annotations to indicate these structural or semantic\nroles. This absence limits language models' ability to reason about graph\ntopology effectively. We propose \\textbf{G2rammar}, a bilingual grammar\nframework that explicitly encodes both structural and semantic grammar for\ntext-attributed graphs. Structural grammar characterizes topological roles\nthrough centrality and neighborhood patterns. Semantic grammar captures content\nrelationships through textual informativity. The framework implements two-stage\nlearning with structural grammar pre-training followed by semantic grammar\nfine-tuning. Extensive experiments on real-world datasets demonstrate that\nG2rammar consistently outperforms competitive baselines by providing language\nmodels with the grammatical context needed to understand graph structures."}
{"id": "2511.01259", "pdf": "https://arxiv.org/pdf/2511.01259", "abs": "https://arxiv.org/abs/2511.01259", "authors": ["Zhiqi Li", "Jinjin He", "Barnabás Börcsök", "Taiyuan Zhang", "Duowen Chen", "Tao Du", "Ming C. Lin", "Greg Turk", "Bo Zhu"], "title": "An Adjoint Method for Differentiable Fluid Simulation on Flow Maps", "categories": ["cs.GR", "physics.flu-dyn"], "comment": "15 pages, 16 figures", "summary": "This paper presents a novel adjoint solver for differentiable fluid\nsimulation based on bidirectional flow maps. Our key observation is that the\nforward fluid solver and its corresponding backward, adjoint solver share the\nsame flow map as the forward simulation. In the forward pass, this map\ntransports fluid impulse variables from the initial frame to the current frame\nto simulate vortical dynamics. In the backward pass, the same map propagates\nadjoint variables from the current frame back to the initial frame to compute\ngradients. This shared long-range map allows the accuracy of gradient\ncomputation to benefit directly from improvements in flow map construction.\nBuilding on this insight, we introduce a novel adjoint solver that solves the\nadjoint equations directly on the flow map, enabling long-range and accurate\ndifferentiation of incompressible flows without differentiating intermediate\nnumerical steps or storing intermediate variables, as required in conventional\nadjoint methods. To further improve efficiency, we propose a long-short\ntime-sparse flow map representation for evolving adjoint variables. Our\napproach has low memory usage, requiring only 6.53GB of data at a resolution of\n$192^3$ while preserving high accuracy in tracking vorticity, enabling new\ndifferentiable simulation tasks that require precise identification,\nprediction, and control of vortex dynamics."}
{"id": "2511.00010", "pdf": "https://arxiv.org/pdf/2511.00010", "abs": "https://arxiv.org/abs/2511.00010", "authors": ["Jiajun Zhang", "Jianke Zhang", "Zeyu Cui", "Jiaxi Yang", "Lei Zhang", "Binyuan Hui", "Qiang Liu", "Zilei Wang", "Liang Wang", "Junyang Lin"], "title": "PlotCraft: Pushing the Limits of LLMs for Complex and Interactive Data Visualization", "categories": ["cs.CL"], "comment": null, "summary": "Recent Large Language Models (LLMs) have demonstrated remarkable profi-\nciency in code generation. However, their ability to create complex visualiza-\ntions for scaled and structured data remains largely unevaluated and\nunderdevel- oped. To address this gap, we introduce PlotCraft, a new benchmark\nfeaturing 1k challenging visualization tasks that cover a wide range of topics,\nsuch as fi- nance, scientific research, and sociology. The benchmark is\nstructured around seven high-level visualization tasks and encompasses 48\ndistinct chart types. Cru- cially, it is the first to systematically evaluate\nboth single-turn generation and multi-turn refinement across a diverse spectrum\nof task complexities. Our com- prehensive evaluation of 23 leading LLMs on\nPlotCraft reveals obvious per- formance deficiencies in handling sophisticated\nvisualization tasks. To bridge this performance gap, we develope SynthVis-30K,\na large-scale, high-quality dataset of complex visualization code synthesized\nvia a collaborative agent frame- work. Building upon this dataset, we develope\nPlotCraftor, a novel code gener- ation model that achieves strong capabilities\nin complex data visualization with a remarkably small size. Across VisEval,\nPandasPlotBench, and our proposed PlotCraft, PlotCraftor shows performance\ncomparable to that of leading propri- etary approaches. Especially, on hard\ntask, Our model achieves over 50% per- formance improvement. We will release\nthe benchmark, dataset, and code at\nhttps://github.com/Speakn0w/PlotCraft-Benchmark."}
{"id": "2511.00248", "pdf": "https://arxiv.org/pdf/2511.00248", "abs": "https://arxiv.org/abs/2511.00248", "authors": ["Shurui Gui", "Deep Anil Patel", "Xiner Li", "Martin Renqiang Min"], "title": "Object-Aware 4D Human Motion Generation", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Recent advances in video diffusion models have enabled the generation of\nhigh-quality videos. However, these videos still suffer from unrealistic\ndeformations, semantic violations, and physical inconsistencies that are\nlargely rooted in the absence of 3D physical priors. To address these\nchallenges, we propose an object-aware 4D human motion generation framework\ngrounded in 3D Gaussian representations and motion diffusion priors. With\npre-generated 3D humans and objects, our method, Motion Score Distilled\nInteraction (MSDI), employs the spatial and prompt semantic information in\nlarge language models (LLMs) and motion priors through the proposed Motion\nDiffusion Score Distillation Sampling (MSDS). The combination of MSDS and LLMs\nenables our spatial-aware motion optimization, which distills score gradients\nfrom pre-trained motion diffusion models, to refine human motion while\nrespecting object and semantic constraints. Unlike prior methods requiring\njoint training on limited interaction datasets, our zero-shot approach avoids\nretraining and generalizes to out-of-distribution object aware human motions.\nExperiments demonstrate that our framework produces natural and physically\nplausible human motions that respect 3D spatial context, offering a scalable\nsolution for realistic 4D generation."}
{"id": "2511.00115", "pdf": "https://arxiv.org/pdf/2511.00115", "abs": "https://arxiv.org/abs/2511.00115", "authors": ["Haoyuan Li", "Yuanbo Tong", "Yuchen Li", "Zirui Wang", "Chunhou Liu", "Jiamou Liu"], "title": "Cognitive Alignment in Personality Reasoning: Leveraging Prototype Theory for MBTI Inference", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Personality recognition from text is typically cast as hard-label\nclassification, which obscures the graded, prototype-like nature of human\npersonality judgments. We present ProtoMBTI, a cognitively aligned framework\nfor MBTI inference that operationalizes prototype theory within an LLM-based\npipeline. First, we construct a balanced, quality-controlled corpus via\nLLM-guided multi-dimensional augmentation (semantic, linguistic, sentiment).\nNext, we LoRA-fine-tune a lightweight (<=2B) encoder to learn discriminative\nembeddings and to standardize a bank of personality prototypes. At inference,\nwe retrieve top-k prototypes for a query post and perform a\nretrieve--reuse--revise--retain cycle: the model aggregates prototype evidence\nvia prompt-based voting, revises when inconsistencies arise, and, upon correct\nprediction, retains the sample to continually enrich the prototype library.\nAcross Kaggle and Pandora benchmarks, ProtoMBTI improves over baselines on both\nthe four MBTI dichotomies and the full 16-type task, and exhibits robust\ncross-dataset generalization. Our results indicate that aligning the inference\nprocess with psychological prototype reasoning yields gains in accuracy,\ninterpretability, and transfer for text-based personality modeling."}
{"id": "2511.00362", "pdf": "https://arxiv.org/pdf/2511.00362", "abs": "https://arxiv.org/abs/2511.00362", "authors": ["Momen Khandoker Ope", "Akif Islam", "Mohd Ruhul Ameen", "Abu Saleh Musa Miah", "Md Rashedul Islam", "Jungpil Shin"], "title": "Oitijjo-3D: Generative AI Framework for Rapid 3D Heritage Reconstruction from Street View Imagery", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "6 Pages, 4 figures, 2 Tables, Submitted to ICECTE 2026", "summary": "Cultural heritage restoration in Bangladesh faces a dual challenge of limited\nresources and scarce technical expertise. Traditional 3D digitization methods,\nsuch as photogrammetry or LiDAR scanning, require expensive hardware, expert\noperators, and extensive on-site access, which are often infeasible in\ndeveloping contexts. As a result, many of Bangladesh's architectural treasures,\nfrom the Paharpur Buddhist Monastery to Ahsan Manzil, remain vulnerable to\ndecay and inaccessible in digital form. This paper introduces Oitijjo-3D, a\ncost-free generative AI framework that democratizes 3D cultural preservation.\nBy using publicly available Google Street View imagery, Oitijjo-3D reconstructs\nfaithful 3D models of heritage structures through a two-stage pipeline -\nmultimodal visual reasoning with Gemini 2.5 Flash Image for structure-texture\nsynthesis, and neural image-to-3D generation through Hexagen for geometry\nrecovery. The system produces photorealistic, metrically coherent\nreconstructions in seconds, achieving significant speedups compared to\nconventional Structure-from-Motion pipelines, without requiring any specialized\nhardware or expert supervision. Experiments on landmarks such as Ahsan Manzil,\nChoto Sona Mosque, and Paharpur demonstrate that Oitijjo-3D preserves both\nvisual and structural fidelity while drastically lowering economic and\ntechnical barriers. By turning open imagery into digital heritage, this work\nreframes preservation as a community-driven, AI-assisted act of cultural\ncontinuity for resource-limited nations."}
{"id": "2511.00180", "pdf": "https://arxiv.org/pdf/2511.00180", "abs": "https://arxiv.org/abs/2511.00180", "authors": ["Nicky Pochinkov", "Yulia Volkova", "Anna Vasileva", "Sai V R Chereddy"], "title": "ParaScopes: What do Language Models Activations Encode About Future Text?", "categories": ["cs.CL", "cs.LG"], "comment": "Main paper: 9 pages, 10 figures. Total 24 pages", "summary": "Interpretability studies in language models often investigate forward-looking\nrepresentations of activations. However, as language models become capable of\ndoing ever longer time horizon tasks, methods for understanding activations\noften remain limited to testing specific concepts or tokens. We develop a\nframework of Residual Stream Decoders as a method of probing model activations\nfor paragraph-scale and document-scale plans. We test several methods and find\ninformation can be decoded equivalent to 5+ tokens of future context in small\nmodels. These results lay the groundwork for better monitoring of language\nmodels and better understanding how they might encode longer-term planning\ninformation."}
{"id": "2511.00548", "pdf": "https://arxiv.org/pdf/2511.00548", "abs": "https://arxiv.org/abs/2511.00548", "authors": ["Baochao Wang", "Xingyu Zhang", "Qingtao Zong", "Alim Pulatov", "Shuqi Shang", "Dongwei Wang"], "title": "Image-based ground distance detection for crop-residue-covered soil", "categories": ["eess.IV", "cs.CV", "cs.GR", "cs.SY", "eess.SY"], "comment": "under review at Computers and Electronics in Agriculture", "summary": "Conservation agriculture features a soil surface covered with crop residues,\nwhich brings benefits of improving soil health and saving water. However, one\nsignificant challenge in conservation agriculture lies in precisely controlling\nthe seeding depth on the soil covered with crop residues. This is constrained\nby the lack of ground distance information, since current distance measurement\ntechniques, like laser, ultrasonic, or mechanical displacement sensors, are\nincapable of differentiating whether the distance information comes from the\nresidue or the soil. This paper presents an image-based method to get the\nground distance information for the crop-residues-covered soil. This method is\nperformed with 3D camera and RGB camera, obtaining depth image and color image\nat the same time. The color image is used to distinguish the different areas of\nresidues and soil and finally generates a mask image. The mask image is applied\nto the depth image so that only the soil area depth information can be used to\ncalculate the ground distance, and residue areas can be recognized and excluded\nfrom ground distance detection. Experimentation shows that this distance\nmeasurement method is feasible for real-time implementation, and the\nmeasurement error is within plus or minus 3mm. It can be applied in\nconservation agriculture machinery for precision depth seeding, as well as\nother depth-control-demanding applications like transplant or tillage."}
{"id": "2511.00198", "pdf": "https://arxiv.org/pdf/2511.00198", "abs": "https://arxiv.org/abs/2511.00198", "authors": ["Chun-Hao Yang", "Bo-Han Feng", "Tzu-Yuan Lai", "Yan Yu Chen", "Yin-Kai Dean Huang", "Shou-De Lin"], "title": "Training LLMs Beyond Next Token Prediction - Filling the Mutual Information Gap", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Optimizing training performance in large language models (LLMs) remains an\nessential challenge, particularly in improving model performance while\nmaintaining computational costs. This work challenges the conventional approach\nof training LLMs using next-token prediction (NTP), arguing that by predicting\ninformation-rich tokens during training, there is a more effective way to train\nLLMs. We investigate the impact of the proposed solution in three kinds of\ntasks for LLMs: arithmetic, multi-label classification of text, and\nnatural-language generation. This work offers a principled approach to\noptimizing LLM training, advancing both model performance and theoretical\nunderstanding of the target-token selection strategies."}
{"id": "2511.00908", "pdf": "https://arxiv.org/pdf/2511.00908", "abs": "https://arxiv.org/abs/2511.00908", "authors": ["Heng Zheng", "Yuling Shi", "Xiaodong Gu", "Haochen You", "Zijian Zhang", "Lubin Gan", "Hao Zhang", "Wenjun Huang", "Jin Huang"], "title": "GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Visual geo-localization requires extensive geographic knowledge and\nsophisticated reasoning to determine image locations without GPS metadata.\nTraditional retrieval methods are constrained by database coverage and quality.\nRecent Large Vision-Language Models (LVLMs) enable direct location reasoning\nfrom image content, yet individual models struggle with diverse geographic\nregions and complex scenes. Existing multi-agent systems improve performance\nthrough model collaboration but treat all agent interactions uniformly. They\nlack mechanisms to handle conflicting predictions effectively. We propose\n\\textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph\nneural networks for visual geo-localization. Our approach models diverse debate\nrelationships through typed edges, distinguishing supportive collaboration,\ncompetitive argumentation, and knowledge transfer. We introduce a dual-level\ndebate mechanism combining node-level refinement and edge-level argumentation\nmodeling. A cross-level topology refinement strategy enables co-evolution\nbetween graph structure and agent representations. Experiments on multiple\nbenchmarks demonstrate GraphGeo significantly outperforms state-of-the-art\nmethods. Our framework transforms cognitive conflicts between agents into\nenhanced geo-localization accuracy through structured debate."}
{"id": "2511.00222", "pdf": "https://arxiv.org/pdf/2511.00222", "abs": "https://arxiv.org/abs/2511.00222", "authors": ["Marwa Abdulhai", "Ryan Cheng", "Donovan Clay", "Tim Althoff", "Sergey Levine", "Natasha Jaques"], "title": "Consistently Simulating Human Personas with Multi-Turn Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to simulate human users in\ninteractive settings such as therapy, education, and social role-play. While\nthese simulations enable scalable training and evaluation of AI agents,\noff-the-shelf LLMs often drift from their assigned personas, contradict earlier\nstatements, or abandon role-appropriate behavior. We introduce a unified\nframework for evaluating and improving persona consistency in LLM-generated\ndialogue. We define three automatic metrics: prompt-to-line consistency,\nline-to-line consistency, and Q&A consistency, that capture different types of\npersona drift and validate each against human annotations. Using these metrics\nas reward signals, we apply multi-turn reinforcement learning to fine-tune LLMs\nfor three user roles: a patient, a student, and a social chat partner. Our\nmethod reduces inconsistency by over 55%, resulting in more coherent and\nfaithful simulated users."}
{"id": "2511.00965", "pdf": "https://arxiv.org/pdf/2511.00965", "abs": "https://arxiv.org/abs/2511.00965", "authors": ["Jiacheng Xu", "Xiongfei Zhao", "Hou-Wan Long", "Cheong Se-Hang", "Yain-Whar Si"], "title": "Detecting Coverage Holes in Wireless Sensor Networks Using Connected Component Labeling and Force-Directed Algorithms", "categories": ["cs.NI", "cs.GR"], "comment": null, "summary": "Contour detection in Wireless Sensor Networks (WSNs) is crucial for tasks\nlike energy saving and network optimization, especially in security and\nsurveillance applications. Coverage holes, where data transmission is not\nachievable, are a significant issue caused by factors such as energy depletion\nand physical damage. Traditional methods for detecting these holes often suffer\nfrom inaccuracy, low processing speed, and high energy consumption, relying\nheavily on physical information like node coordinates and sensing range. To\naddress these challenges, we propose a novel, coordinate-free coverage hole\ndetection method using Connected Component Labeling (CCL) and Force-Directed\n(FD) algorithms, termed FD-CCL. This method does not require node coordinates\nor sensing range information. We also investigate Suzuki's Contour Tracing (CT)\nalgorithm and compare its performance with CCL on various FD graphs. Our\nexperiments demonstrate the effectiveness of FD-CCL in terms of processing time\nand accuracy. Simulation results confirm the superiority of FD-CCL in detecting\nand locating coverage holes in WSNs."}
{"id": "2511.00265", "pdf": "https://arxiv.org/pdf/2511.00265", "abs": "https://arxiv.org/abs/2511.00265", "authors": ["Arman Anwar", "Zefang Liu"], "title": "AgentBnB: A Browser-Based Cybersecurity Tabletop Exercise with Large Language Model Support and Retrieval-Aligned Scaffolding", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Traditional cybersecurity tabletop exercises (TTXs) provide valuable training\nbut are often scripted, resource-intensive, and difficult to scale. We\nintroduce AgentBnB, a browser-based re-imagining of the Backdoors & Breaches\ngame that integrates large language model teammates with a Bloom-aligned,\nretrieval-augmented copilot (C2D2). The system expands a curated corpus into\nfactual, conceptual, procedural, and metacognitive snippets, delivering\non-demand, cognitively targeted hints. Prompt-engineered agents employ a\nscaffolding ladder that gradually fades as learner confidence grows. In a\nsolo-player pilot with four graduate students, participants reported greater\nintention to use the agent-based version compared to the physical card deck and\nviewed it as more scalable, though a ceiling effect emerged on a simple\nknowledge quiz. Despite limitations of small sample size, single-player focus,\nand narrow corpus, these early findings suggest that large language model\naugmented TTXs can provide lightweight, repeatable practice without the\nlogistical burden of traditional exercises. Planned extensions include\nmulti-player modes, telemetry-driven coaching, and comparative studies with\nlarger cohorts."}
{"id": "2511.01233", "pdf": "https://arxiv.org/pdf/2511.01233", "abs": "https://arxiv.org/abs/2511.01233", "authors": ["Rajmund Nagy", "Hendric Voss", "Thanh Hoang-Minh", "Mihail Tsakov", "Teodor Nikolov", "Zeyi Zhang", "Tenglong Ao", "Sicheng Yang", "Shaoli Huang", "Yongkang Cheng", "M. Hamza Mughal", "Rishabh Dabral", "Kiran Chhatre", "Christian Theobalt", "Libin Liu", "Stefan Kopp", "Rachel McDonnell", "Michael Neff", "Taras Kucherenko", "Youngwoo Yoon", "Gustav Eje Henter"], "title": "Gesture Generation (Still) Needs Improved Human Evaluation Practices: Insights from a Community-Driven State-of-the-Art Benchmark", "categories": ["cs.CV", "cs.GR", "cs.HC", "I.3; I.2"], "comment": "23 pages, 10 figures. The last two authors made equal contributions", "summary": "We review human evaluation practices in automated, speech-driven 3D gesture\ngeneration and find a lack of standardisation and frequent use of flawed\nexperimental setups. This leads to a situation where it is impossible to know\nhow different methods compare, or what the state of the art is. In order to\naddress common shortcomings of evaluation design, and to standardise future\nuser studies in gesture-generation works, we introduce a detailed human\nevaluation protocol for the widely-used BEAT2 motion-capture dataset. Using\nthis protocol, we conduct large-scale crowdsourced evaluation to rank six\nrecent gesture-generation models -- each trained by its original authors --\nacross two key evaluation dimensions: motion realism and speech-gesture\nalignment. Our results provide strong evidence that 1) newer models do not\nconsistently outperform earlier approaches; 2) published claims of high motion\nrealism or speech-gesture alignment may not hold up under rigorous evaluation;\nand 3) the field must adopt disentangled assessments of motion quality and\nmultimodal alignment for accurate benchmarking in order to make progress.\nFinally, in order to drive standardisation and enable new evaluation research,\nwe will release five hours of synthetic motion from the benchmarked models;\nover 750 rendered video stimuli from the user studies -- enabling new\nevaluations without model reimplementation required -- alongside our\nopen-source rendering script, and the 16,000 pairwise human preference votes\ncollected for our benchmark."}
{"id": "2511.00268", "pdf": "https://arxiv.org/pdf/2511.00268", "abs": "https://arxiv.org/abs/2511.00268", "authors": ["Shounak Paul", "Dhananjay Ghumare", "Pawan Goyal", "Saptarshi Ghosh", "Ashutosh Modi"], "title": "IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Identifying/retrieving relevant statutes and prior cases/precedents for a\ngiven legal situation are common tasks exercised by law practitioners.\nResearchers to date have addressed the two tasks independently, thus developing\ncompletely different datasets and models for each task; however, both retrieval\ntasks are inherently related, e.g., similar cases tend to cite similar statutes\n(due to similar factual situation). In this paper, we address this gap. We\npropose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),\nwhich is a unique corpus that provides a common testbed for developing models\nfor both the tasks (Statute Retrieval and Precedent Retrieval) that can exploit\nthe dependence between the two. We experiment extensively with several baseline\nmodels on the tasks, including lexical models, semantic models and ensemble\nbased on GNNs. Further, to exploit the dependence between the two tasks, we\ndevelop an LLM-based re-ranking approach that gives the best performance."}
{"id": "2511.01463", "pdf": "https://arxiv.org/pdf/2511.01463", "abs": "https://arxiv.org/abs/2511.01463", "authors": ["Lei Hu", "Yongjing Ye", "Shihong Xia"], "title": "HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA", "categories": ["cs.CV", "cs.AI", "cs.GR", "68T45", "I.2.10; I.3.7"], "comment": "10 pages, 5figures. The Thirty-Ninth Annual Conference on Neural\n  Information Processing Systems", "summary": "The expansion of instruction-tuning data has enabled foundation language\nmodels to exhibit improved instruction adherence and superior performance\nacross diverse downstream tasks. Semantically-rich 3D human motion is being\nprogressively integrated with these foundation models to enhance multimodal\nunderstanding and cross-modal generation capabilities. However, the modality\ngap between human motion and text raises unresolved concerns about catastrophic\nforgetting during this integration. In addition, developing\nautoregressive-compatible pose representations that preserve generalizability\nacross heterogeneous downstream tasks remains a critical technical barrier. To\naddress these issues, we propose the Human Motion-Vision-Language Model\n(HMVLM), a unified framework based on the Mixture of Expert Low-Rank\nAdaption(MoE LoRA) strategy. The framework leverages the gating network to\ndynamically allocate LoRA expert weights based on the input prompt, enabling\nsynchronized fine-tuning of multiple tasks. To mitigate catastrophic forgetting\nduring instruction-tuning, we introduce a novel zero expert that preserves the\npre-trained parameters for general linguistic tasks. For pose representation,\nwe implement body-part-specific tokenization by partitioning the human body\ninto different joint groups, enhancing the spatial resolution of the\nrepresentation. Experiments show that our method effectively alleviates\nknowledge forgetting during instruction-tuning and achieves remarkable\nperformance across diverse human motion downstream tasks."}
{"id": "2511.00270", "pdf": "https://arxiv.org/pdf/2511.00270", "abs": "https://arxiv.org/abs/2511.00270", "authors": ["Abhinav Joshi", "Vaibhav Sharma", "Sanjeet Singh", "Ashutosh Modi"], "title": "POSESTITCH-SLT: Linguistically Inspired Pose-Stitching for End-to-End Sign Language Translation", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": "Accepted at EMNLP 2025 (Main)", "summary": "Sign language translation remains a challenging task due to the scarcity of\nlarge-scale, sentence-aligned datasets. Prior arts have focused on various\nfeature extraction and architectural changes to support neural machine\ntranslation for sign languages. We propose POSESTITCH-SLT, a novel pre-training\nscheme that is inspired by linguistic-templates-based sentence generation\ntechnique. With translation comparison on two sign language datasets, How2Sign\nand iSign, we show that a simple transformer-based encoder-decoder architecture\noutperforms the prior art when considering template-generated sentence pairs in\ntraining. We achieve BLEU-4 score improvements from 1.97 to 4.56 on How2Sign\nand from 0.55 to 3.43 on iSign, surpassing prior state-of-the-art methods for\npose-based gloss-free translation. The results demonstrate the effectiveness of\ntemplate-driven synthetic supervision in low-resource sign language settings."}
{"id": "2511.01513", "pdf": "https://arxiv.org/pdf/2511.01513", "abs": "https://arxiv.org/abs/2511.01513", "authors": ["Andrei-Timotei Ardelean", "Tim Weyrich"], "title": "Example-Based Feature Painting on Textures", "categories": ["cs.CV", "cs.GR"], "comment": "\"\\c{opyright} 2025 Andrei-Timotei Ardelean, Tim Weyrich. This is the\n  author's version of the work. It is posted here for your personal use. Not\n  for redistribution. The definitive Version of Record was published in ACM\n  Trans. Graph., Vol. 44, No. 6, https://doi.org/10.1145/3763301", "summary": "In this work, we propose a system that covers the complete workflow for\nachieving controlled authoring and editing of textures that present distinctive\nlocal characteristics. These include various effects that change the surface\nappearance of materials, such as stains, tears, holes, abrasions,\ndiscoloration, and more. Such alterations are ubiquitous in nature, and\nincluding them in the synthesis process is crucial for generating realistic\ntextures. We introduce a novel approach for creating textures with such\nblemishes, adopting a learning-based approach that leverages unlabeled\nexamples. Our approach does not require manual annotations by the user;\ninstead, it detects the appearance-altering features through unsupervised\nanomaly detection. The various textural features are then automatically\nclustered into semantically coherent groups, which are used to guide the\nconditional generation of images. Our pipeline as a whole goes from a small\nimage collection to a versatile generative model that enables the user to\ninteractively create and paint features on textures of arbitrary size. Notably,\nthe algorithms we introduce for diffusion-based editing and infinite stationary\ntexture generation are generic and should prove useful in other contexts as\nwell. Project page: https://reality.tf.fau.de/pub/ardelean2025examplebased.html"}
{"id": "2511.00315", "pdf": "https://arxiv.org/pdf/2511.00315", "abs": "https://arxiv.org/abs/2511.00315", "authors": ["Lee Xiong", "Maksim Tkachenko", "Johanes Effendi", "Ting Cai"], "title": "Language Modeling With Factorization Memory", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We propose Factorization Memory, an efficient recurrent neural network (RNN)\narchitecture that achieves performance comparable to Transformer models on\nshort-context language modeling tasks while also demonstrating superior\ngeneralization in long-context scenarios. Our model builds upon Mamba-2,\nenabling Factorization Memory to exploit parallel computations during training\nwhile preserving constant computational and memory complexity during inference.\nTo further optimize model efficiency and representational capacity, we develop\na sparse formulation of Factorization Memory that updates only a subset of\nrecurrent states at each step while preserving the strong performance of its\ndense counterpart. To our knowledge, this represents the first RNN architecture\nthat successfully combines sparse memory activation with competitive\nperformance across both short and long-context settings. This work provides a\nsystematic empirical analysis of Factorization Memory in comparison to\nTransformer and Mamba-2 architectures."}
{"id": "2511.00341", "pdf": "https://arxiv.org/pdf/2511.00341", "abs": "https://arxiv.org/abs/2511.00341", "authors": ["Mihir Sahasrabudhe"], "title": "Reversal Invariance in Autoregressive Language Models", "categories": ["cs.CL"], "comment": "7 pages, theoretical note", "summary": "We formalize a structural property of the causal (autoregressive) language\nmodeling (CLM) objective: reversal invariance. Formally, the next-token\nprediction loss assigns identical likelihood to a corpus and its reversal,\nimplying that standard CLM pretraining is direction-blind. This symmetry\nexplains why models trained on reversed text can achieve comparable performance\nto those trained on forward text, despite the inherently time-asymmetric nature\nof human language and reasoning. We argue that this invariance represents a\nlimitation of current pretraining objectives rather than a benign artifact. If\nnatural language encodes directional dependencies - phonological,\nmorphological, or causal - a symmetric objective may fail to capture them. We\ntherefore propose viewing pretraining through the lens of temporal asymmetry,\nmotivating future work on loss functions and architectures that explicitly\nmodel the arrow of language while retaining standard language modeling\ncapacity."}
{"id": "2511.00343", "pdf": "https://arxiv.org/pdf/2511.00343", "abs": "https://arxiv.org/abs/2511.00343", "authors": ["Changbing Yang", "Franklin Ma", "Freda Shi", "Jian Zhu"], "title": "LingGym: How Far Are LLMs from Thinking Like Field Linguists?", "categories": ["cs.CL"], "comment": "EMNLP 2025 Main", "summary": "This paper introduces LingGym, a new benchmark that evaluates LLMs' capacity\nfor meta-linguistic reasoning using Interlinear Glossed Text (IGT) and\ngrammatical descriptions extracted from 18 typologically diverse reference\ngrammars. Unlike previous work that focuses on specific downstream tasks, we\nassess whether LLMs can generalize linguistic inference across low-resource\nlanguages and structures not seen during training. We present a controlled\nevaluation task: Word-Gloss Inference, in which the model must infer a missing\nword and gloss from context using varying levels of linguistic information\n(e.g., glosses, grammatical explanations, translations). Our results show that\nincorporating structured linguistic cues leads to consistent improvements in\nreasoning performance across all models. This work highlights both the promise\nand current limitations of using LLMs for typologically informed linguistic\nanalysis and low-resource language documentation."}
{"id": "2511.00371", "pdf": "https://arxiv.org/pdf/2511.00371", "abs": "https://arxiv.org/abs/2511.00371", "authors": ["Erfan Al-Hossami", "Razvan Bunescu"], "title": "Reasoning Trajectories for Socratic Debugging of Student Code: From Misconceptions to Contradictions and Updated Beliefs", "categories": ["cs.CL", "cs.CY", "cs.SE"], "comment": "25 pages, 2 tables, 13 figures", "summary": "In Socratic debugging, instructors guide students towards identifying and\nfixing a bug on their own, instead of providing the bug fix directly. Most\nnovice programmer bugs are caused by programming misconceptions, namely false\nbeliefs about a programming concept. In this context, Socratic debugging can be\nformulated as a guided Reasoning Trajectory (RT) leading to a statement about\nthe program behavior that contradicts the bug-causing misconception. Upon\nreaching this statement, the ensuing cognitive dissonance leads the student to\nfirst identify and then update their false belief. In this paper, we introduce\nthe task of reasoning trajectory generation, together with a dataset of\ndebugging problems manually annotated with RTs. We then describe LLM-based\nsolutions for generating RTs and Socratic conversations that are anchored on\nthem. A large-scale LLM-as-judge evaluation shows that frontier models can\ngenerate up to 91% correct reasoning trajectories and 98.7% valid conversation\nturns."}
{"id": "2511.00416", "pdf": "https://arxiv.org/pdf/2511.00416", "abs": "https://arxiv.org/abs/2511.00416", "authors": ["Yiwei Zha", "Rui Min", "Shanu Sushmita"], "title": "PADBen: A Comprehensive Benchmark for Evaluating AI Text Detectors Against Paraphrase Attacks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While AI-generated text (AIGT) detectors achieve over 90\\% accuracy on direct\nLLM outputs, they fail catastrophically against iteratively-paraphrased\ncontent. We investigate why iteratively-paraphrased text -- itself AI-generated\n-- evades detection systems designed for AIGT identification. Through intrinsic\nmechanism analysis, we reveal that iterative paraphrasing creates an\nintermediate laundering region characterized by semantic displacement with\npreserved generation patterns, which brings up two attack categories:\nparaphrasing human-authored text (authorship obfuscation) and paraphrasing\nLLM-generated text (plagiarism evasion). To address these vulnerabilities, we\nintroduce PADBen, the first benchmark systematically evaluating detector\nrobustness against both paraphrase attack scenarios. PADBen comprises a\nfive-type text taxonomy capturing the full trajectory from original content to\ndeeply laundered text, and five progressive detection tasks across\nsentence-pair and single-sentence challenges. We evaluate 11 state-of-the-art\ndetectors, revealing critical asymmetry: detectors successfully identify the\nplagiarism evasion problem but fail for the case of authorship obfuscation. Our\nfindings demonstrate that current detection approaches cannot effectively\nhandle the intermediate laundering region, necessitating fundamental advances\nin detection architectures beyond existing semantic and stylistic\ndiscrimination methods. For detailed code implementation, please see\nhttps://github.com/JonathanZha47/PadBen-Paraphrase-Attack-Benchmark."}
{"id": "2511.00421", "pdf": "https://arxiv.org/pdf/2511.00421", "abs": "https://arxiv.org/abs/2511.00421", "authors": ["Naoto Iwase", "Hiroki Okuyama", "Junichiro Iwasawa"], "title": "MedRECT: A Medical Reasoning Benchmark for Error Correction in Clinical Texts", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) show increasing promise in medical applications,\nbut their ability to detect and correct errors in clinical texts -- a\nprerequisite for safe deployment -- remains under-evaluated, particularly\nbeyond English. We introduce MedRECT, a cross-lingual benchmark\n(Japanese/English) that formulates medical error handling as three subtasks:\nerror detection, error localization (sentence extraction), and error\ncorrection. MedRECT is built with a scalable, automated pipeline from the\nJapanese Medical Licensing Examinations (JMLE) and a curated English\ncounterpart, yielding MedRECT-ja (663 texts) and MedRECT-en (458 texts) with\ncomparable error/no-error balance. We evaluate 9 contemporary LLMs spanning\nproprietary, open-weight, and reasoning families. Key findings: (i) reasoning\nmodels substantially outperform standard architectures, with up to 13.5%\nrelative improvement in error detection and 51.0% in sentence extraction; (ii)\ncross-lingual evaluation reveals 5-10% performance gaps from English to\nJapanese, with smaller disparities for reasoning models; (iii) targeted LoRA\nfine-tuning yields asymmetric improvements in error correction performance\n(Japanese: +0.078, English: +0.168) while preserving reasoning capabilities;\nand (iv) our fine-tuned model exceeds human expert performance on structured\nmedical error correction tasks. To our knowledge, MedRECT is the first\ncomprehensive cross-lingual benchmark for medical error correction, providing a\nreproducible framework and resources for developing safer medical LLMs across\nlanguages."}
{"id": "2511.00432", "pdf": "https://arxiv.org/pdf/2511.00432", "abs": "https://arxiv.org/abs/2511.00432", "authors": ["Zhiwen Ruan", "Yixia Li", "Yefeng Liu", "Yun Chen", "Weihua Luo", "Peng Li", "Yang Liu", "Guanhua Chen"], "title": "G2: Guided Generation for Enhanced Output Diversity in LLMs", "categories": ["cs.CL"], "comment": "EMNLP 2025", "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, these models exhibit a\ncritical limitation in output diversity, often generating highly similar\ncontent across multiple attempts. This limitation significantly affects tasks\nrequiring diverse outputs, from creative writing to reasoning. Existing\nsolutions, like temperature scaling, enhance diversity by modifying probability\ndistributions but compromise output quality. We propose Guide-to-Generation\n(G2), a training-free plug-and-play method that enhances output diversity while\npreserving generation quality. G2 employs a base generator alongside dual\nGuides, which guide the generation process through decoding-based interventions\nto encourage more diverse outputs conditioned on the original query.\nComprehensive experiments demonstrate that G2 effectively improves output\ndiversity while maintaining an optimal balance between diversity and quality."}
{"id": "2511.00476", "pdf": "https://arxiv.org/pdf/2511.00476", "abs": "https://arxiv.org/abs/2511.00476", "authors": ["Ghazal Kalhor", "Afra Mashhadi"], "title": "Remembering Unequally: Global and Disciplinary Bias in LLM-Generated Co-Authorship Networks", "categories": ["cs.CL"], "comment": null, "summary": "Ongoing breakthroughs in Large Language Models (LLMs) are reshaping search\nand recommendation platforms at their core. While this shift unlocks powerful\nnew scientometric tools, it also exposes critical fairness and bias issues that\ncould erode the integrity of the information ecosystem. Additionally, as LLMs\nbecome more integrated into web-based searches for scholarly tools, their\nability to generate summarized research work based on memorized data introduces\nnew dimensions to these challenges. The extent of memorization in LLMs can\nimpact the accuracy and fairness of the co-authorship networks they produce,\npotentially reflecting and amplifying existing biases within the scientific\ncommunity and across different regions. This study critically examines the\nimpact of LLM memorization on the co-authorship networks. To this end, we\nassess memorization effects across three prominent models, DeepSeek R1, Llama 4\nScout, and Mixtral 8x7B, analyzing how memorization-driven outputs vary across\nacademic disciplines and world regions. While our global analysis reveals a\nconsistent bias favoring highly cited researchers, this pattern is not\nuniformly observed. Certain disciplines, such as Clinical Medicine, and\nregions, including parts of Africa, show more balanced representation, pointing\nto areas where LLM training data may reflect greater equity. These findings\nunderscore both the risks and opportunities in deploying LLMs for scholarly\ndiscovery."}
{"id": "2511.00486", "pdf": "https://arxiv.org/pdf/2511.00486", "abs": "https://arxiv.org/abs/2511.00486", "authors": ["Pooja Singh", "Shashwat Bhardwaj", "Vaibhav Sharma", "Sandeep Kumar"], "title": "Leveraging the Cross-Domain & Cross-Linguistic Corpus for Low Resource NMT: A Case Study On Bhili-Hindi-English Parallel Corpus", "categories": ["cs.CL"], "comment": "Accepted in EMNLP 2025", "summary": "The linguistic diversity of India poses significant machine translation\nchallenges, especially for underrepresented tribal languages like Bhili, which\nlack high-quality linguistic resources. This paper addresses the gap by\nintroducing Bhili-Hindi-English Parallel Corpus (BHEPC), the first and largest\nparallel corpus worldwide comprising 110,000 meticulously curated sentences\nacross Bhili, Hindi, and English. The corpus was created with the assistance of\nexpert human translators. BHEPC spans critical domains such as education,\nadministration, and news, establishing a valuable benchmark for research in low\nresource machine translation. To establish a comprehensive Bhili Machine\nTranslation benchmark, we evaluated a wide range of proprietary and open-source\nMultilingual Large Language Models (MLLMs) on bidirectional translation tasks\nbetween English/Hindi and Bhili. Comprehensive evaluation demonstrates that the\nfine-tuned NLLB-200 distilled 600M variant model outperforms others,\nhighlighting the potential of multilingual models in low resource scenarios.\nFurthermore, we investigated the generative translation capabilities of\nmultilingual LLMs on BHEPC using in-context learning, assessing performance\nunder cross-domain generalization and quantifying distributional divergence.\nThis work bridges a critical resource gap and promotes inclusive natural\nlanguage processing technologies for low-resource and marginalized languages\nglobally."}
{"id": "2511.00487", "pdf": "https://arxiv.org/pdf/2511.00487", "abs": "https://arxiv.org/abs/2511.00487", "authors": ["Stephen Meisenbacher", "Florian Matthes"], "title": "With Privacy, Size Matters: On the Importance of Dataset Size in Differentially Private Text Rewriting", "categories": ["cs.CL"], "comment": "11 pages, 1 figure, 5 tables. Accepted to IJCNLP-AACL 2025 (Main)", "summary": "Recent work in Differential Privacy with Natural Language Processing (DP NLP)\nhas proposed numerous promising techniques in the form of text rewriting\nmechanisms. In the evaluation of these mechanisms, an often-ignored aspect is\nthat of dataset size, or rather, the effect of dataset size on a mechanism's\nefficacy for utility and privacy preservation. In this work, we are the first\nto introduce this factor in the evaluation of DP text privatization, where we\ndesign utility and privacy tests on large-scale datasets with dynamic split\nsizes. We run these tests on datasets of varying size with up to one million\ntexts, and we focus on quantifying the effect of increasing dataset size on the\nprivacy-utility trade-off. Our findings reveal that dataset size plays an\nintegral part in evaluating DP text rewriting mechanisms; additionally, these\nfindings call for more rigorous evaluation procedures in DP NLP, as well as\nshed light on the future of DP NLP in practice and at scale."}
{"id": "2511.00489", "pdf": "https://arxiv.org/pdf/2511.00489", "abs": "https://arxiv.org/abs/2511.00489", "authors": ["Jiani Guo", "Zuchao Li", "Jie Wu", "Qianren Wang", "Yun Li", "Lefei Zhang", "Hai Zhao", "Yujiu Yang"], "title": "ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models", "categories": ["cs.CL"], "comment": "EMNLP 2025 Main Conference", "summary": "Large Language Models (LLMs), constrained by limited context windows, often\nface significant performance degradation when reasoning over long contexts. To\naddress this, Retrieval-Augmented Generation (RAG) retrieves and reasons over\nchunks but frequently sacrifices logical coherence due to its reliance on\nsimilarity-based rankings. Similarly, divide-and-conquer frameworks (DCF) split\ndocuments into small chunks for independent reasoning and aggregation. While\neffective for local reasoning, DCF struggles to capture long-range dependencies\nand risks inducing conflicts by processing chunks in isolation. To overcome\nthese limitations, we propose ToM, a novel Tree-oriented MapReduce framework\nfor long-context reasoning. ToM leverages the inherent hierarchical structure\nof long documents (e.g., main headings and subheadings) by constructing a\nDocTree through hierarchical semantic parsing and performing bottom-up\naggregation. Using a Tree MapReduce approach, ToM enables recursive reasoning:\nin the Map step, rationales are generated at child nodes; in the Reduce step,\nthese rationales are aggregated across sibling nodes to resolve conflicts or\nreach consensus at parent nodes. Experimental results on 70B+ LLMs show that\nToM significantly outperforms existing divide-and-conquer frameworks and\nretrieval-augmented generation methods, achieving better logical coherence and\nlong-context reasoning. Our code is available at\nhttps://github.com/gjn12-31/ToM ."}
{"id": "2511.00505", "pdf": "https://arxiv.org/pdf/2511.00505", "abs": "https://arxiv.org/abs/2511.00505", "authors": ["Qi Luo", "Xiaonan Li", "Junqi Dai", "Shuang Cheng", "Xipeng Qiu"], "title": "Zero-RAG: Towards Retrieval-Augmented Generation with Zero Redundant Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation has shown remarkable results to address Large\nLanguage Models' hallucinations, which usually uses a large external corpus to\nsupplement knowledge to LLMs. However, with the development of LLMs, the\ninternal knowledge of LLMs has expanded significantly, thus causing significant\nknowledge redundancy between the external corpus and LLMs. On the one hand, the\nindexing cost of dense retrieval is highly related to the corpus size and thus\nsignificant redundant knowledge intensifies the dense retrieval's workload. On\nthe other hand, the redundant knowledge in the external corpus is not helpful\nto LLMs and our exploratory analysis shows that it instead hurts the RAG\nperformance on those questions which the LLM can answer by itself. To address\nthese issues, we propose Zero-RAG to tackle these challenges. Specifically, we\nfirst propose the Mastery-Score metric to identify redundant knowledge in the\nRAG corpus to prune it. After pruning, answers to \"mastered\" questions rely\nprimarily on internal knowledge of the LLM. To better harness the internal\ncapacity, we propose Query Router and Noise-Tolerant Tuning to avoid the\nirrelevant documents' distraction and thus further improve the LLM's\nutilization of internal knowledge with pruned corpus. Experimental results show\nthat Zero-RAG prunes the Wikipedia corpus by 30\\% and accelerates the retrieval\nstage by 22\\%, without compromising RAG's performance."}
{"id": "2511.00514", "pdf": "https://arxiv.org/pdf/2511.00514", "abs": "https://arxiv.org/abs/2511.00514", "authors": ["Birat Poudel", "Satyam Ghimire", "Er. Prakash Chandra Prasad"], "title": "Fine-Tuning DialoGPT on Common Diseases in Rural Nepal for Medical Conversations", "categories": ["cs.CL"], "comment": "6 pages, 6 figures, 3 tables", "summary": "Conversational agents are increasingly being explored to support healthcare\ndelivery, particularly in resource-constrained settings such as rural Nepal.\nLarge-scale conversational models typically rely on internet connectivity and\ncloud infrastructure, which may not be accessible in rural areas. In this\nstudy, we fine-tuned DialoGPT, a lightweight generative dialogue model that can\noperate offline, on a synthetically constructed dataset of doctor-patient\ninteractions covering ten common diseases prevalent in rural Nepal, including\ncommon cold, seasonal fever, diarrhea, typhoid fever, gastritis, food\npoisoning, malaria, dengue fever, tuberculosis, and pneumonia. Despite being\ntrained on a limited, domain-specific dataset, the fine-tuned model produced\ncoherent, contextually relevant, and medically appropriate responses,\ndemonstrating an understanding of symptoms, disease context, and empathetic\ncommunication. These results highlight the adaptability of compact,\noffline-capable dialogue models and the effectiveness of targeted datasets for\ndomain adaptation in low-resource healthcare environments, offering promising\ndirections for future rural medical conversational AI."}
{"id": "2511.00519", "pdf": "https://arxiv.org/pdf/2511.00519", "abs": "https://arxiv.org/abs/2511.00519", "authors": ["Ariyan Hossain", "Khondokar Mohammad Ahanaf Hannan", "Rakinul Haque", "Nowreen Tarannum Rafa", "Humayra Musarrat", "Shoaib Ahmed Dipu", "Farig Yousuf Sadeque"], "title": "Exploring and Mitigating Gender Bias in Encoder-Based Transformer Models", "categories": ["cs.CL", "I.2.7; I.7.1; K.4.1"], "comment": "25 pages, 20 figures", "summary": "Gender bias in language models has gained increasing attention in the field\nof natural language processing. Encoder-based transformer models, which have\nachieved state-of-the-art performance in various language tasks, have been\nshown to exhibit strong gender biases inherited from their training data. This\npaper investigates gender bias in contextualized word embeddings, a crucial\ncomponent of transformer-based models. We focus on prominent architectures such\nas BERT, ALBERT, RoBERTa, and DistilBERT to examine their vulnerability to\ngender bias. To quantify the degree of bias, we introduce a novel metric,\nMALoR, which assesses bias based on model probabilities for filling masked\ntokens. We further propose a mitigation approach involving continued\npre-training on a gender-balanced dataset generated via Counterfactual Data\nAugmentation. Our experiments reveal significant reductions in gender bias\nscores across different pronoun pairs. For instance, in BERT-base, bias scores\nfor \"he-she\" dropped from 1.27 to 0.08, and \"his-her\" from 2.51 to 0.36\nfollowing our mitigation approach. We also observed similar improvements across\nother models, with \"male-female\" bias decreasing from 1.82 to 0.10 in\nBERT-large. Our approach effectively reduces gender bias without compromising\nmodel performance on downstream tasks."}
{"id": "2511.00536", "pdf": "https://arxiv.org/pdf/2511.00536", "abs": "https://arxiv.org/abs/2511.00536", "authors": ["Wenya Xie", "Shaochen", "Zhong", "Hoang Anh Duy Le", "Zhaozhuo Xu", "Jianwen Xie", "Zirui Liu"], "title": "Word Salad Chopper: Reasoning Models Waste A Ton Of Decoding Budget On Useless Repetitions, Self-Knowingly", "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) are often bottlenecked by the high cost of\noutput tokens. We show that a significant portion of these tokens are useless\nself-repetitions - what we call \"word salad\" - that exhaust the decoding budget\nwithout adding value. Interestingly, we observe that LRMs are self-aware when\ntrapped in these loops: the hidden states of <\\n\\n> tokens trailing each\nreasoning chunk exhibit patterns that allow us to detect word salad behavior\non-the-fly via a single-layer linear classifier. Once detected, a simple chop\nappended by a straightforward regeneration prompt yields substantial length\nsavings with minimal quality loss. Our work offers WordSaladChopper (WSC) - a\nlightweight, turnkey component for LRM that is minimally invasive to its\nreasoning trajectory by only removing semantically redundant tokens. Given its\nlow overhead, strong savings, and the lack of semantic value of word salad\ntokens, we believe it is not too far-fetched to argue that WSC - or a similar\ncomponent - is a must-have for all LRM applications with user experience in\nmind. Our code is publicly available at\nhttps://github.com/wenyaxie023/WordSaladChopper."}
{"id": "2511.00537", "pdf": "https://arxiv.org/pdf/2511.00537", "abs": "https://arxiv.org/abs/2511.00537", "authors": ["Peter Atandoh", "Jie Zou", "Weikang Guo", "Jiwei Wei", "Zheng Wang"], "title": "Multi-refined Feature Enhanced Sentiment Analysis Using Contextual Instruction", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sentiment analysis using deep learning and pre-trained language models (PLMs)\nhas gained significant traction due to their ability to capture rich contextual\nrepresentations. However, existing approaches often underperform in scenarios\ninvolving nuanced emotional cues, domain shifts, and imbalanced sentiment\ndistributions. We argue that these limitations stem from inadequate semantic\ngrounding, poor generalization to diverse linguistic patterns, and biases\ntoward dominant sentiment classes. To overcome these challenges, we propose\nCISEA-MRFE, a novel PLM-based framework integrating Contextual Instruction\n(CI), Semantic Enhancement Augmentation (SEA), and Multi-Refined Feature\nExtraction (MRFE). CI injects domain-aware directives to guide sentiment\ndisambiguation; SEA improves robustness through sentiment-consistent\nparaphrastic augmentation; and MRFE combines a Scale-Adaptive Depthwise Encoder\n(SADE) for multi-scale feature specialization with an Emotion Evaluator Context\nEncoder (EECE) for affect-aware sequence modeling. Experimental results on four\nbenchmark datasets demonstrate that CISEA-MRFE consistently outperforms strong\nbaselines, achieving relative improvements in accuracy of up to 4.6% on IMDb,\n6.5% on Yelp, 30.3% on Twitter, and 4.1% on Amazon. These results validate the\neffectiveness and generalization ability of our approach for sentiment\nclassification across varied domains."}
{"id": "2511.00556", "pdf": "https://arxiv.org/pdf/2511.00556", "abs": "https://arxiv.org/abs/2511.00556", "authors": ["Peng Ding", "Jun Kuang", "Wen Sun", "Zongyu Wang", "Xuezhi Cao", "Xunliang Cai", "Jiajun Chen", "Shujian Huang"], "title": "Friend or Foe: How LLMs' Safety Mind Gets Fooled by Intent Shift Attack", "categories": ["cs.CL"], "comment": "Preprint, 14 pages, 5 figures, 7 tables", "summary": "Large language models (LLMs) remain vulnerable to jailbreaking attacks\ndespite their impressive capabilities. Investigating these weaknesses is\ncrucial for robust safety mechanisms. Existing attacks primarily distract LLMs\nby introducing additional context or adversarial tokens, leaving the core\nharmful intent unchanged. In this paper, we introduce ISA (Intent Shift\nAttack), which obfuscates LLMs about the intent of the attacks. More\nspecifically, we establish a taxonomy of intent transformations and leverage\nthem to generate attacks that may be misperceived by LLMs as benign requests\nfor information. Unlike prior methods relying on complex tokens or lengthy\ncontext, our approach only needs minimal edits to the original request, and\nyields natural, human-readable, and seemingly harmless prompts. Extensive\nexperiments on both open-source and commercial LLMs show that ISA achieves over\n70% improvement in attack success rate compared to direct harmful prompts. More\ncritically, fine-tuning models on only benign data reformulated with ISA\ntemplates elevates success rates to nearly 100%. For defense, we evaluate\nexisting methods and demonstrate their inadequacy against ISA, while exploring\nboth training-free and training-based mitigation strategies. Our findings\nreveal fundamental challenges in intent inference for LLMs safety and\nunderscore the need for more effective defenses. Our code and datasets are\navailable at https://github.com/NJUNLP/ISA."}
{"id": "2511.00576", "pdf": "https://arxiv.org/pdf/2511.00576", "abs": "https://arxiv.org/abs/2511.00576", "authors": ["Juan Gabriel Kostelec", "Qinghai Guo"], "title": "FlashEVA: Accelerating LLM inference via Efficient Attention", "categories": ["cs.CL", "cs.AI"], "comment": "Technical Report", "summary": "Transformer models have revolutionized natural language processing, achieving\nstate-of-the-art performance and demonstrating remarkable scalability. However,\ntheir memory demands, particularly due to maintaining full context in memory,\npose significant challenges for inference. In this paper, we present FlashEVA,\nan efficient implementation of EVA (Efficient Attention via Control Variates),\nand demonstrate how to finetune transformers to adapt to FlashEVA attention.\nOur method enables fine-tuning of Transformer models with as few as 1.5B tokens\nwhile preserving effectiveness across various downstream tasks. Notably,\nFlashEVA achieves up to 6.7x higher throughput and 5x lower peak GPU memory\nusage during inference compared to standard Transformer implementations.\nDespite these improvements, we observe limitations in retrieval-focused tasks.\nOur implementation offers control over the trade-off between throughput and\naccuracy through adjustable hyperparameters, providing flexibility for diverse\nuse cases. This work represents a significant step towards more efficient and\nadaptable Transformer-based models for inference."}
{"id": "2511.00602", "pdf": "https://arxiv.org/pdf/2511.00602", "abs": "https://arxiv.org/abs/2511.00602", "authors": ["Wai-Chung Kwan", "Joshua Ong Jun Leang", "Pavlos Vougiouklis", "Jeff Z. Pan", "Marco Valentino", "Pasquale Minervini"], "title": "OpenSIR: Open-Ended Self-Improving Reasoner", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language model (LLM) reasoning through reinforcement\nlearning rely on annotated datasets for verifiable rewards, which may limit\nmodels' ability to surpass human-level performance. While self-play offers a\npromising alternative, existing approaches depend on external verifiers or\ncannot learn open-endedly. We present Open-Ended Self-Improving Reasoner\n(OpenSIR), a self-play framework where an LLM learns to generate and solve\nnovel problems by alternating teacher and student roles without external\nsupervision. To generate novel problems, OpenSIR optimises for both difficulty\nand diversity, rewarding problems that challenge appropriately while exploring\ndistinct concepts, enabling open-ended mathematical discovery. Starting from a\nsingle trivial seed problem, OpenSIR substantially improves instruction models:\nLlama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to\n34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on\nGSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through\nco-evolving teacher-student roles that adaptively calibrate difficulty and\ndrive diverse exploration, progressing autonomously from basic to advanced\nmathematics."}
{"id": "2511.00606", "pdf": "https://arxiv.org/pdf/2511.00606", "abs": "https://arxiv.org/abs/2511.00606", "authors": ["Jameson Sandler", "Jacob K. Christopher", "Thomas Hartvigsen", "Nando Fioretto"], "title": "SpecDiff-2: Scaling Diffusion Drafter Alignment For Faster Speculative Decoding", "categories": ["cs.CL"], "comment": null, "summary": "Speculative decoding has become the standard approach for accelerating Large\nLanguage Model (LLM) inference. It exploits a lossless draft-then-verify\nprocedure to circumvent the latency of autoregressive decoding, achieving\nimpressive speed-ups. Yet, current speculative decoding approaches remain\nlimited by two fundamental bottlenecks: (1) the autoregressive dependency\nduring drafting which limits parallelism, and (2) frequent rejections of draft\ntokens caused by misalignment between the draft and verify models. This paper\nproposes SpecDiff-2, a novel framework to jointly address these two\nbottlenecks. It leverages discrete diffusion as a non-autoregressive drafter to\naddress bottleneck (1) and develops novel techniques to calibrate discrete\ndiffusion drafters with autoregressive verifiers, addressing bottleneck (2).\nExperimental results across a comprehensive benchmark suite show that\nSpecDiff-2 achieves a new state-of-the-art across reasoning, coding, and\nmathematical benchmarks, improving tokens-per-second by up to an average of\n+55% over previous baselines and obtaining up to 5.5x average speed-up over\nstandard decoding, without any loss of accuracy."}
{"id": "2511.00620", "pdf": "https://arxiv.org/pdf/2511.00620", "abs": "https://arxiv.org/abs/2511.00620", "authors": ["Autumn Toney-Wails", "Ryan Wails"], "title": "Certain but not Probable? Differentiating Certainty from Probability in LLM Token Outputs for Probabilistic Scenarios", "categories": ["cs.CL"], "comment": "To appear at the Second Workshop on Uncertainty-Aware NLP @EMNLP 2025\n  (UncertaiNLP '25)", "summary": "Reliable uncertainty quantification (UQ) is essential for ensuring\ntrustworthy downstream use of large language models, especially when they are\ndeployed in decision-support and other knowledge-intensive applications. Model\ncertainty can be estimated from token logits, with derived probability and\nentropy values offering insight into performance on the prompt task. However,\nthis approach may be inadequate for probabilistic scenarios, where the\nprobabilities of token outputs are expected to align with the theoretical\nprobabilities of the possible outcomes. We investigate the relationship between\ntoken certainty and alignment with theoretical probability distributions in\nwell-defined probabilistic scenarios. Using GPT-4.1 and DeepSeek-Chat, we\nevaluate model responses to ten prompts involving probability (e.g., roll a\nsix-sided die), both with and without explicit probability cues in the prompt\n(e.g., roll a fair six-sided die). We measure two dimensions: (1) response\nvalidity with respect to scenario constraints, and (2) alignment between\ntoken-level output probabilities and theoretical probabilities. Our results\nindicate that, while both models achieve perfect in-domain response accuracy\nacross all prompt scenarios, their token-level probability and entropy values\nconsistently diverge from the corresponding theoretical distributions."}
{"id": "2511.00627", "pdf": "https://arxiv.org/pdf/2511.00627", "abs": "https://arxiv.org/abs/2511.00627", "authors": ["Jean Barré", "Olga Seminck", "Antoine Bourgois", "Thierry Poibeau"], "title": "Modeling the Construction of a Literary Archetype: The Case of the Detective Figure in French Literature", "categories": ["cs.CL"], "comment": "19 pages, 2 tables, 5 figures Conference Computational Humanities\n  Research 2025", "summary": "This research explores the evolution of the detective archetype in French\ndetective fiction through computational analysis. Using quantitative methods\nand character-level embeddings, we show that a supervised model is able to\ncapture the unity of the detective archetype across 150 years of literature,\nfrom M. Lecoq (1866) to Commissaire Adamsberg (2017). Building on this finding,\nthe study demonstrates how the detective figure evolves from a secondary\nnarrative role to become the central character and the \"reasoning machine\" of\nthe classical detective story. In the aftermath of the Second World War, with\nthe importation of the hardboiled tradition into France, the archetype becomes\nmore complex, navigating the genre's turn toward social violence and moral\nambiguity."}
{"id": "2511.00657", "pdf": "https://arxiv.org/pdf/2511.00657", "abs": "https://arxiv.org/abs/2511.00657", "authors": ["Eshaan Tanwar", "Anwoy Chatterjee", "Michael Saxon", "Alon Albalak", "William Yang Wang", "Tanmoy Chakraborty"], "title": "Do You Know About My Nation? Investigating Multilingual Language Models' Cultural Literacy Through Factual Knowledge", "categories": ["cs.CL"], "comment": "Accepted in EMNLP 2025. Code at: https://github.com/EshaanT/XNationQA", "summary": "Most multilingual question-answering benchmarks, while covering a diverse\npool of languages, do not factor in regional diversity in the information they\ncapture and tend to be Western-centric. This introduces a significant gap in\nfairly evaluating multilingual models' comprehension of factual information\nfrom diverse geographical locations. To address this, we introduce XNationQA\nfor investigating the cultural literacy of multilingual LLMs. XNationQA\nencompasses a total of 49,280 questions on the geography, culture, and history\nof nine countries, presented in seven languages. We benchmark eight standard\nmultilingual LLMs on XNationQA and evaluate them using two novel transference\nmetrics. Our analyses uncover a considerable discrepancy in the models'\naccessibility to culturally specific facts across languages. Notably, we often\nfind that a model demonstrates greater knowledge of cultural information in\nEnglish than in the dominant language of the respective culture. The models\nexhibit better performance in Western languages, although this does not\nnecessarily translate to being more literate for Western countries, which is\ncounterintuitive. Furthermore, we observe that models have a very limited\nability to transfer knowledge across languages, particularly evident in\nopen-source models."}
{"id": "2511.00689", "pdf": "https://arxiv.org/pdf/2511.00689", "abs": "https://arxiv.org/abs/2511.00689", "authors": ["Berk Atil", "Rebecca J. Passonneau", "Fred Morstatter"], "title": "Do Methods to Jailbreak and Defend LLMs Generalize Across Languages?", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) undergo safety alignment after training and\ntuning, yet recent work shows that safety can be bypassed through jailbreak\nattacks. While many jailbreaks and defenses exist, their cross-lingual\ngeneralization remains underexplored. This paper presents the first systematic\nmultilingual evaluation of jailbreaks and defenses across ten\nlanguages--spanning high-, medium-, and low-resource languages--using six LLMs\non HarmBench and AdvBench. We assess two jailbreak types:\nlogical-expression-based and adversarial-prompt-based. For both types, attack\nsuccess and defense robustness vary across languages: high-resource languages\nare safer under standard queries but more vulnerable to adversarial ones.\nSimple defenses can be effective, but are language- and model-dependent. These\nfindings call for language-aware and cross-lingual safety benchmarks for LLMs."}
{"id": "2511.00819", "pdf": "https://arxiv.org/pdf/2511.00819", "abs": "https://arxiv.org/abs/2511.00819", "authors": ["Yuxuan Hu", "Jianchao Tan", "Jiaqi Zhang", "Wen Zan", "Pingwei Sun", "Yifan Lu", "Yerui Sun", "Yuchen Xie", "Xunliang Cai", "Jing Zhang"], "title": "Optimizing Native Sparse Attention with Latent Attention and Local Global Alternating Strategies", "categories": ["cs.CL"], "comment": null, "summary": "In this work, we conduct a systematic analysis of Native Sparse Attention\n(NSA) and propose targeted improvements that enhance long-context modeling. A\nkey insight is that alternating between local (sliding-window) and global\n(compression, selective) attention across layers, rather than using fixed\npatterns, enables more effective propagation of long-range dependencies and\nsubstantially boosts performance on long-sequence tasks. Meanwhile, we further\nrefine NSA's branches with Latent Attention that the sliding-window branch is\nenhanced with Multi-head Latent Attention (MLA) while compression and selective\nbranches adopt Group-head Latent Attention (GLA). These changes reduce KV-cache\nmemory by 50\\% versus NSA while improving the model's common-sense reasoning\nand long-text understanding capabilities. Experiments on models from 340M to\n1.3B parameters (trained on 15B and 100B tokens) show our method matches or\nexceeds full attention and native sparse attention in both common-sense\nreasoning and long-context understanding tasks."}
{"id": "2511.00854", "pdf": "https://arxiv.org/pdf/2511.00854", "abs": "https://arxiv.org/abs/2511.00854", "authors": ["Chong Lyu", "Lin Li", "Shiqing Wu", "Jingling Yuan"], "title": "TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The increasing utilization of large language models raises significant\nconcerns about the propagation of social biases, which may result in harmful\nand unfair outcomes. However, existing debiasing methods treat the biased and\nunbiased samples independently, thus ignoring their mutual relationship. This\noversight enables a hidden negative-positive coupling, where improvements for\none group inadvertently compromise the other, allowing residual social bias to\npersist. In this paper, we introduce TriCon-Fair, a contrastive learning\nframework that employs a decoupled loss that combines triplet and language\nmodeling terms to eliminate positive-negative coupling. Our TriCon-Fair assigns\neach anchor an explicitly biased negative and an unbiased positive, decoupling\nthe push-pull dynamics and avoiding positive-negative coupling, and jointly\noptimizes a language modeling (LM) objective to preserve general capability.\nExperimental results demonstrate that TriCon-Fair reduces discriminatory output\nbeyond existing debiasing baselines while maintaining strong downstream\nperformance. This suggests that our proposed TriCon-Fair offers a practical and\nethical solution for sensitive NLP applications."}
{"id": "2511.00879", "pdf": "https://arxiv.org/pdf/2511.00879", "abs": "https://arxiv.org/abs/2511.00879", "authors": ["Hyeon Hwang", "Yewon Cho", "Chanwoong Yoon", "Yein Park", "Minju Song", "Kyungjae Lee", "Gangwoo Kim", "Jaewoo Kang"], "title": "Assessing LLM Reasoning Steps via Principal Knowledge Grounding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to EMNLP 2025 Findings", "summary": "Step-by-step reasoning has become a standard approach for large language\nmodels (LLMs) to tackle complex tasks. While this paradigm has proven\neffective, it raises a fundamental question: How can we verify that an LLM's\nreasoning is accurately grounded in knowledge? To address this question, we\nintroduce a novel evaluation suite that systematically assesses the knowledge\ngrounding of intermediate reasoning. Our framework comprises three key\ncomponents. (1) Principal Knowledge Collection, a large-scale repository of\natomic knowledge essential for reasoning. Based on the collection, we propose\n(2) knowledge-grounded evaluation metrics designed to measure how well models\nrecall and apply prerequisite knowledge in reasoning. These metrics are\ncomputed by our (3) evaluator LLM, a lightweight model optimized for\ncost-effective and reliable metric computation. Our evaluation suite\ndemonstrates remarkable effectiveness in identifying missing or misapplied\nknowledge elements, providing crucial insights for uncovering fundamental\nreasoning deficiencies in LLMs. Beyond evaluation, we demonstrate how these\nmetrics can be integrated into preference optimization, showcasing further\napplications of knowledge-grounded evaluation."}
{"id": "2511.00903", "pdf": "https://arxiv.org/pdf/2511.00903", "abs": "https://arxiv.org/abs/2511.00903", "authors": ["Ahmed Masry", "Megh Thakkar", "Patrice Bechard", "Sathwik Tejaswi Madhusudhan", "Rabiul Awal", "Shambhavi Mishra", "Akshay Kalkunte Suresh", "Srivatsava Daruru", "Enamul Hoque", "Spandana Gella", "Torsten Scholak", "Sai Rajeswar"], "title": "ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-augmented generation has proven practical when models require\nspecialized knowledge or access to the latest data. However, existing methods\nfor multimodal document retrieval often replicate techniques developed for\ntext-only retrieval, whether in how they encode documents, define training\nobjectives, or compute similarity scores. To address these limitations, we\npresent ColMate, a document retrieval model that bridges the gap between\nmultimodal representation learning and document retrieval. ColMate utilizes a\nnovel OCR-based pretraining objective, a self-supervised masked contrastive\nlearning objective, and a late interaction scoring mechanism more relevant to\nmultimodal document structures and visual characteristics. ColMate obtains\n3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,\ndemonstrating stronger generalization to out-of-domain benchmarks."}
{"id": "2511.00924", "pdf": "https://arxiv.org/pdf/2511.00924", "abs": "https://arxiv.org/abs/2511.00924", "authors": ["Jianzhou Yao", "Shunchang Liu", "Guillaume Drui", "Rikard Pettersson", "Alessandro Blasimme", "Sara Kijewski"], "title": "The Biased Oracle: Assessing LLMs' Understandability and Empathy in Medical Diagnoses", "categories": ["cs.CL"], "comment": "Accepted by NeurIPS 2025 GenAI4Health Workshop", "summary": "Large language models (LLMs) show promise for supporting clinicians in\ndiagnostic communication by generating explanations and guidance for patients.\nYet their ability to produce outputs that are both understandable and\nempathetic remains uncertain. We evaluate two leading LLMs on medical\ndiagnostic scenarios, assessing understandability using readability metrics as\na proxy and empathy through LLM-as-a-Judge ratings compared to human\nevaluations. The results indicate that LLMs adapt explanations to\nsocio-demographic variables and patient conditions. However, they also generate\noverly complex content and display biased affective empathy, leading to uneven\naccessibility and support. These patterns underscore the need for systematic\ncalibration to ensure equitable patient communication. The code and data are\nreleased: https://github.com/Jeffateth/Biased_Oracle"}
{"id": "2511.00960", "pdf": "https://arxiv.org/pdf/2511.00960", "abs": "https://arxiv.org/abs/2511.00960", "authors": ["Abhinav P M", "Ojasva Saxena", "Oswald C", "Parameswari Krishnamurthy"], "title": "The Riddle of Reflection: Evaluating Reasoning and Self-Awareness in Multilingual LLMs using Indian Riddles", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The extent to which large language models (LLMs) can perform culturally\ngrounded reasoning across non-English languages remains underexplored. This\npaper examines the reasoning and self-assessment abilities of LLMs across seven\nmajor Indian languages-Bengali, Gujarati, Hindi, Kannada, Malayalam, Tamil, and\nTelugu. We introduce a multilingual riddle dataset combining traditional\nriddles with context-reconstructed variants and evaluate five LLMs-Gemini 2.5\nPro, Gemini 2.5 Flash, Mistral-Saba, LLaMA 4 Scout, and LLaMA 4 Maverick-under\nseven prompting strategies. In the first stage, we assess riddle-solving\nperformance and find that while Gemini 2.5 Pro performs best overall, few-shot\nmethods yield only marginal gains, and accuracy varies notably across\nlanguages. In the second stage, we conduct a self-evaluation experiment to\nmeasure reasoning consistency. The results reveal a key finding: a model's\ninitial accuracy is inversely correlated with its ability to identify its own\nmistakes. Top-performing models such as Gemini 2.5 Pro are overconfident (4.34%\nTrue Negative Rate), whereas lower-performing models like LLaMA 4 Scout are\nsubstantially more self-aware (42.09% True Negative Rate). These results point\nto clear gaps in multilingual reasoning and highlight the need for models that\nnot only reason effectively but also recognize their own limitations."}
{"id": "2511.00988", "pdf": "https://arxiv.org/pdf/2511.00988", "abs": "https://arxiv.org/abs/2511.00988", "authors": ["Chenwang Wu", "Yiu-ming Cheung", "Bo Han", "Defu Lian"], "title": "Advancing Machine-Generated Text Detection from an Easy to Hard Supervision Perspective", "categories": ["cs.CL"], "comment": null, "summary": "Existing machine-generated text (MGT) detection methods implicitly assume\nlabels as the \"golden standard\". However, we reveal boundary ambiguity in MGT\ndetection, implying that traditional training paradigms are inexact. Moreover,\nlimitations of human cognition and the superintelligence of detectors make\ninexact learning widespread and inevitable. To this end, we propose an\neasy-to-hard enhancement framework to provide reliable supervision under such\ninexact conditions. Distinct from knowledge distillation, our framework employs\nan easy supervisor targeting relatively simple longer-text detection tasks\n(despite weaker capabilities), to enhance the more challenging target detector.\nFirstly, longer texts targeted by supervisors theoretically alleviate the\nimpact of inexact labels, laying the foundation for reliable supervision.\nSecondly, by structurally incorporating the detector into the supervisor, we\ntheoretically model the supervisor as a lower performance bound for the\ndetector. Thus, optimizing the supervisor indirectly optimizes the detector,\nultimately approximating the underlying \"golden\" labels. Extensive experiments\nacross diverse practical scenarios, including cross-LLM, cross-domain, mixed\ntext, and paraphrase attacks, demonstrate the framework's significant detection\neffectiveness. The code is available at:\nhttps://github.com/tmlr-group/Easy2Hard."}
{"id": "2511.01008", "pdf": "https://arxiv.org/pdf/2511.01008", "abs": "https://arxiv.org/abs/2511.01008", "authors": ["Haolin Yang", "Jipeng Zhang", "Zhitao He", "Yi R. Fung"], "title": "MARS-SQL: A multi-agent reinforcement learning framework for Text-to-SQL", "categories": ["cs.CL"], "comment": null, "summary": "Translating natural language to SQL remains difficult for complex queries.\nSuch queries often need environmental interaction and self-correction. To\naddress this, we introduce MARS-SQL, a novel multi-agent framework that\ncombines principled task decomposition and interactive reinforcement learning\n(RL). Our system comprises three specialized agents: a Grounding Agent for\nschema linking, a Generation Agent for query generation, and a Validation Agent\nfor final selection. The core of our framework is the Generation agent, which\nis trained via a multi-turn RL policy. Adopting a ReAct-style Think-Act-Observe\nloop, the agent iteratively generates thoughts, executes SQL actions against a\nlive database, and revises its strategy based on execution feedback, enabling\ndynamic, stateful reasoning and self-correction. At inference time, we generate\nmultiple interaction trajectories to explore diverse reasoning paths. The\nValidation agent, then selects the optimal trajectory by modeling verification\nas a next-token prediction task and choosing the solution with the highest\ngeneration probability. This structured workflow pipelines specialized agents.\nIt combines interactive RL for generation with generative modeling for\nverification. The approach proves highly effective for robust and accurate SQL\ngeneration. Experiments show that MARS-SQL achieves state-of-the-art Execution\nAccuracy of 77.84% on the BIRD dev set and 89.75% on the Spider test set. Our\ncode is available at https://github.com/YangHaolin0526/MARS-SQL."}
{"id": "2511.01014", "pdf": "https://arxiv.org/pdf/2511.01014", "abs": "https://arxiv.org/abs/2511.01014", "authors": ["Bosi Wen", "Yilin Niu", "Cunxiang Wang", "Pei Ke", "Xiaoying Ling", "Ying Zhang", "Aohan Zeng", "Hongning Wang", "Minlie Huang"], "title": "IF-CRITIC: Towards a Fine-Grained LLM Critic for Instruction-Following Evaluation", "categories": ["cs.CL"], "comment": "21 pages, 5 figures", "summary": "Instruction following is a fundamental ability of Large Language Models\n(LLMs), requiring their generated outputs to follow multiple constraints\nimposed in input instructions. Numerous studies have attempted to enhance this\nability through preference optimization or reinforcement learning based on\nreward signals from LLM-as-a-Judge. However, existing evaluation models for\ninstruction following still possess many deficiencies, such as substantial\ncosts and unreliable assessments. To this end, we propose IF-CRITIC, an LLM\ncritic that can provide efficient and reliable assessments of constraint\nfollowing in the instructions. We first develop a checklist generator to\ndecompose instructions and generate constraint checklists. With the assistance\nof the checklists, we collect high-quality critique training data through a\nmulti-stage critique filtering mechanism and employ a constraint-level\npreference optimization method to train IF-CRITIC. Extensive experiments\ndemonstrate that the evaluation performance of IF-CRITIC can beat strong\nLLM-as-a-Judge baselines, including Deepseek-R1 and o4-mini. With the scalable\nreward signals provided by IF-CRITIC, LLMs can achieve substantial performance\ngains in instruction-following optimization under lower computational overhead\ncompared to strong LLM critic baselines."}
{"id": "2511.01016", "pdf": "https://arxiv.org/pdf/2511.01016", "abs": "https://arxiv.org/abs/2511.01016", "authors": ["Wenjin Liu", "Haoran Luo", "Xueyuan Lin", "Haoming Liu", "Tiesunlong Shen", "Jiapu Wang", "Rui Mao", "Erik Cambria"], "title": "Prompt-R1: Collaborative Automatic Prompting Framework via End-to-end Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Recently, advanced large language models (LLMs) have emerged at an\nincreasingly rapid pace. However, when faced with complex problems, most users\nare often unable to provide accurate and effective prompts to interact with\nLLMs, thus limiting the performance of LLMs. To address this challenge, we\npropose Prompt-R1, an end-to-end reinforcement learning framework that uses a\nsmall-scale LLM to collaborate with large-scale LLMs, replacing user\ninteraction to solve problems better. This collaboration is cast as a\nmulti-turn prompt interaction, where the small-scale LLM thinks and generates\nprompts, and the large-scale LLM performs complex reasoning. A dual-constrained\nreward is designed to optimize for correctness, generation quality, and\nreasoning accuracy. Prompt-R1 provides a plug-and-play framework that supports\nboth inference and training with various large-scale LLMs. Experiments on\nmultiple public datasets show that Prompt-R1 significantly outperforms baseline\nmodels across tasks. Our code is publicly available at\nhttps://github.com/QwenQKing/Prompt-R1."}
{"id": "2511.01019", "pdf": "https://arxiv.org/pdf/2511.01019", "abs": "https://arxiv.org/abs/2511.01019", "authors": ["Bowen Chen", "Jayesh Gajbhar", "Gregory Dusek", "Rob Redmon", "Patrick Hogan", "Paul Liu", "DelWayne Bohnenstiehl", "Dongkuan", "Xu", "Ruoying He"], "title": "OceanAI: A Conversational Platform for Accurate, Transparent, Near-Real-Time Oceanographic Insights", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG", "physics.ao-ph"], "comment": "A related presentation will be given at the AGU(American Geophysical\n  Union) and AMS(American Meteorological Society) Annual Meetings", "summary": "Artificial intelligence is transforming the sciences, yet general\nconversational AI systems often generate unverified \"hallucinations\"\nundermining scientific rigor. We present OceanAI, a conversational platform\nthat integrates the natural-language fluency of open-source large language\nmodels (LLMs) with real-time, parameterized access to authoritative\noceanographic data streams hosted by the National Oceanic and Atmospheric\nAdministration (NOAA). Each query such as \"What was Boston Harbor's highest\nwater level in 2024?\" triggers real-time API calls that identify, parse, and\nsynthesize relevant datasets into reproducible natural-language responses and\ndata visualizations. In a blind comparison with three widely used AI\nchat-interface products, only OceanAI produced NOAA-sourced values with\noriginal data references; others either declined to answer or provided\nunsupported results. Designed for extensibility, OceanAI connects to multiple\nNOAA data products and variables, supporting applications in marine hazard\nforecasting, ecosystem assessment, and water-quality monitoring. By grounding\noutputs and verifiable observations, OceanAI advances transparency,\nreproducibility, and trust, offering a scalable framework for AI-enabled\ndecision support within the oceans. A public demonstration is available at\nhttps://oceanai.ai4ocean.xyz."}
{"id": "2511.01046", "pdf": "https://arxiv.org/pdf/2511.01046", "abs": "https://arxiv.org/abs/2511.01046", "authors": ["Vedant Acharya", "Abhay Pisharodi", "Rishabh Mondal", "Mohammad Rafiuddin", "Nipun Batra"], "title": "VayuChat: An LLM-Powered Conversational Interface for Air Quality Data Analytics", "categories": ["cs.CL"], "comment": "4 Pages, 4 Figures", "summary": "Air pollution causes about 1.6 million premature deaths each year in India,\nyet decision makers struggle to turn dispersed data into decisions. Existing\ntools require expertise and provide static dashboards, leaving key policy\nquestions unresolved. We present VayuChat, a conversational system that answers\nnatural language questions on air quality, meteorology, and policy programs,\nand responds with both executable Python code and interactive visualizations.\nVayuChat integrates data from Central Pollution Control Board (CPCB) monitoring\nstations, state-level demographics, and National Clean Air Programme (NCAP)\nfunding records into a unified interface powered by large language models. Our\nlive demonstration will show how users can perform complex environmental\nanalytics through simple conversations, making data science accessible to\npolicymakers, researchers, and citizens. The platform is publicly deployed at\nhttps://huggingface.co/spaces/SustainabilityLabIITGN/ VayuChat. For further\ninformation check out video uploaded on\nhttps://www.youtube.com/watch?v=d6rklL05cs4."}
{"id": "2511.01053", "pdf": "https://arxiv.org/pdf/2511.01053", "abs": "https://arxiv.org/abs/2511.01053", "authors": ["Qing Ding", "Eric Hua Qing Zhang", "Felix Jozsa", "Julia Ive"], "title": "Building a Silver-Standard Dataset from NICE Guidelines for Clinical LLMs", "categories": ["cs.CL"], "comment": "Submitted to EFMI Medical Informatics Europe 2026", "summary": "Large language models (LLMs) are increasingly used in healthcare, yet\nstandardised benchmarks for evaluating guideline-based clinical reasoning are\nmissing. This study introduces a validated dataset derived from publicly\navailable guidelines across multiple diagnoses. The dataset was created with\nthe help of GPT and contains realistic patient scenarios, as well as clinical\nquestions. We benchmark a range of recent popular LLMs to showcase the validity\nof our dataset. The framework supports systematic evaluation of LLMs' clinical\nutility and guideline adherence."}
{"id": "2511.01066", "pdf": "https://arxiv.org/pdf/2511.01066", "abs": "https://arxiv.org/abs/2511.01066", "authors": ["Stephan Oepen", "Nikolay Arefev", "Mikko Aulamo", "Marta Bañón", "Maja Buljan", "Laurie Burchell", "Lucas Charpentier", "Pinzhen Chen", "Mariya Fedorova", "Ona de Gibert", "Barry Haddow", "Jan Hajič", "Jindrič Helcl", "Andrey Kutuzov", "Zihao Li", "Risto Luukkonen", "Bhavitvya Malik", "Vladislav Mikhailov", "Amanda Myntti", "Dayyán O'Brien", "Lucie Poláková", "Sampo Pyysalo", "Gema Ramírez Sánchez", "Janine Siewert", "Pavel Stepachev", "Jörg Tiedemann", "Teemu Vahtola", "Fedor Vitiugin", "Tea Vojtěchová", "Jaume Zaragoza"], "title": "HPLT~3.0: Very Large-Scale Multilingual Resources for LLM and MT. Mono- and Bi-lingual Data, Multilingual Evaluation, and Pre-Trained Models", "categories": ["cs.CL"], "comment": null, "summary": "We present an ongoing initiative to provide open, very large, high-quality,\nand richly annotated textual datasets for almost 200 languages. At 30 trillion\ntokens, this is likely the largest generally available multilingual collection\nof LLM pre-training data. At 30 trillion tokens, this is likely the largest\ngenerally available multilingual collection of LLM pre-training data. These\ndatasets are derived from web crawls from different sources and accompanied\nwith a complete, open-source pipeline for document selection from web archives,\ntext extraction from HTML, language identification for noisy texts, exact and\nnear-deduplication, annotation with, among others, register labels, text\nquality estimates, and personally identifiable information; and final selection\nand filtering. We report on data quality probes through contrastive and\nanalytical statistics, through manual inspection of samples for 24 languages,\nand through end-to-end evaluation of various language model architectures\ntrained on this data. For multilingual LLM evaluation, we provide a\ncomprehensive collection of benchmarks for nine European languages, with\nspecial emphasis on natively created tasks, mechanisms to mitigate prompt\nsensitivity, and refined normalization and aggregation of scores. Additionally,\nwe train and evaluate a family of 57 monolingual encoder-decoder models, as\nwell as a handful of monolingual GPT-like reference models. Besides the\nmonolingual data and models, we also present a very large collection of\nparallel texts automatically mined from this data, together with a novel\nparallel corpus synthesized via machine translation."}
{"id": "2511.01090", "pdf": "https://arxiv.org/pdf/2511.01090", "abs": "https://arxiv.org/abs/2511.01090", "authors": ["Vlad Negoita", "Mihai Masala", "Traian Rebedea"], "title": "Improving Romanian LLM Pretraining Data using Diversity and Quality Filtering", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have recently exploded in popularity, often\nmatching or outperforming human abilities on many tasks. One of the key factors\nin training LLMs is the availability and curation of high-quality data. Data\nquality is especially crucial for under-represented languages, where\nhigh-quality corpora are scarce. In this work we study the characteristics and\ncoverage of Romanian pretraining corpora and we examine how they differ from\nEnglish data. By training a lightweight multitask model on carefully\nLLM-annotated Romanian texts, we are able to analyze and perform multi-level\nfiltering (e.g., educational value, topic, format) to generate high-quality\npretraining datasets. Our experiments show noteworthy trends in the topics\npresent in Romanian and English data, while also proving the effectiveness of\nfiltering data through improved LLM pretraining performance across multiple\nbenchmarks."}
{"id": "2511.01101", "pdf": "https://arxiv.org/pdf/2511.01101", "abs": "https://arxiv.org/abs/2511.01101", "authors": ["Marek Strong", "Andreas Vlachos"], "title": "TSVer: A Benchmark for Fact Verification Against Time-Series Evidence", "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025", "summary": "Reasoning over temporal and numerical data, such as time series, is a crucial\naspect of fact-checking. While many systems have recently been developed to\nhandle this form of evidence, their evaluation remains limited by existing\ndatasets, which often lack structured evidence, provide insufficient\njustifications for verdicts, or rely on synthetic claims. In this paper, we\nintroduce TSVer, a new benchmark dataset for fact verification focusing on\ntemporal and numerical reasoning with time-series evidence. TSVer contains 287\nreal-world claims sourced from 38 fact-checking organizations and a curated\ndatabase of 400 time series covering diverse domains. Each claim is annotated\nwith time frames across all pertinent time series, along with a verdict and\njustifications reflecting how the evidence is used to reach the verdict. Using\nan LLM-assisted multi-step annotation process, we improve the quality of our\nannotations and achieve an inter-annotator agreement of kappa=0.745 on\nverdicts. We also develop a baseline for verifying claims against time-series\nevidence and show that even the state-of-the-art reasoning models like\nGemini-2.5-Pro are challenged by time series, achieving a 63.37 accuracy score\non verdicts and an Ev2R score of 48.63 on verdict justifications."}
{"id": "2511.01166", "pdf": "https://arxiv.org/pdf/2511.01166", "abs": "https://arxiv.org/abs/2511.01166", "authors": ["Lingzhe Zhang", "Yunpeng Zhai", "Tong Jia", "Chiming Duan", "Minghua He", "Leyi Pan", "Zhaoyang Liu", "Bolin Ding", "Ying Li"], "title": "MicroRemed: Benchmarking LLMs in Microservices Remediation", "categories": ["cs.CL", "cs.SE", "68T50", "I.2.7"], "comment": "24 pages, 13 figures, 5 tables", "summary": "Large Language Models (LLMs) integrated with agent-based reasoning frameworks\nhave recently shown strong potential for autonomous decision-making and\nsystem-level operations. One promising yet underexplored direction is\nmicroservice remediation, where the goal is to automatically recover faulty\nmicroservice systems. Existing approaches, however, still rely on human-crafted\nprompts from Site Reliability Engineers (SREs), with LLMs merely converting\ntextual instructions into executable code. To advance research in this area, we\nintroduce MicroRemed, the first benchmark for evaluating LLMs in end-to-end\nmicroservice remediation, where models must directly generate executable\nAnsible playbooks from diagnosis reports to restore system functionality. We\nfurther propose ThinkRemed, a multi-agent framework that emulates the\nreflective and perceptive reasoning of SREs. Experimental results show that\nMicroRemed presents substantial challenges to current LLMs, while ThinkRemed\nimproves end-to-end remediation performance through iterative reasoning and\nsystem reflection. The benchmark is available at\nhttps://github.com/LLM4AIOps/MicroRemed."}
{"id": "2511.01181", "pdf": "https://arxiv.org/pdf/2511.01181", "abs": "https://arxiv.org/abs/2511.01181", "authors": ["Emaad Manzoor", "Eva Ascarza", "Oded Netzer"], "title": "Learning When to Quit in Sales Conversations", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Salespeople frequently face the dynamic screening decision of whether to\npersist in a conversation or abandon it to pursue the next lead. Yet, little is\nknown about how these decisions are made, whether they are efficient, or how to\nimprove them. We study these decisions in the context of high-volume outbound\nsales where leads are ample, but time is scarce and failure is common. We\nformalize the dynamic screening decision as an optimal stopping problem and\ndevelop a generative language model-based sequential decision agent - a\nstopping agent - that learns whether and when to quit conversations by\nimitating a retrospectively-inferred optimal stopping policy. Our approach\nhandles high-dimensional textual states, scales to large language models, and\nworks with both open-source and proprietary language models. When applied to\ncalls from a large European telecommunications firm, our stopping agent reduces\nthe time spent on failed calls by 54% while preserving nearly all sales;\nreallocating the time saved increases expected sales by up to 37%. Upon\nexamining the linguistic cues that drive salespeople's quitting decisions, we\nfind that they tend to overweight a few salient expressions of consumer\ndisinterest and mispredict call failure risk, suggesting cognitive bounds on\ntheir ability to make real-time conversational decisions. Our findings\nhighlight the potential of artificial intelligence algorithms to correct\ncognitively-bounded human decisions and improve salesforce efficiency."}
{"id": "2511.01187", "pdf": "https://arxiv.org/pdf/2511.01187", "abs": "https://arxiv.org/abs/2511.01187", "authors": ["Muhammed Saeed", "Muhammad Abdul-mageed", "Shady Shehata"], "title": "Surfacing Subtle Stereotypes: A Multilingual, Debate-Oriented Evaluation of Modern LLMs", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) are widely deployed for open-ended\ncommunication, yet most bias evaluations still rely on English,\nclassification-style tasks. We introduce DebateBias-8K, a new multilingual,\ndebate-style benchmark designed to reveal how narrative bias appears in\nrealistic generative settings. Our dataset includes 8,400 structured debate\nprompts spanning four sensitive domains: women's rights, socioeconomic\ndevelopment, terrorism, and religion, across seven languages ranging from\nhigh-resource (English, Chinese) to low-resource (Swahili, Nigerian Pidgin).\nUsing four flagship models (GPT-4o, Claude 3, DeepSeek, and LLaMA 3), we\ngenerate and automatically classify over 100,000 responses. Results show that\nall models reproduce entrenched stereotypes despite safety alignment: Arabs are\noverwhelmingly linked to terrorism and religion (>=95%), Africans to\nsocioeconomic \"backwardness\" (up to <=77%), and Western groups are consistently\nframed as modern or progressive. Biases grow sharply in lower-resource\nlanguages, revealing that alignment trained primarily in English does not\ngeneralize globally. Our findings highlight a persistent divide in multilingual\nfairness: current alignment methods reduce explicit toxicity but fail to\nprevent biased outputs in open-ended contexts. We release our DebateBias-8K\nbenchmark and analysis framework to support the next generation of multilingual\nbias evaluation and safer, culturally inclusive model alignment."}
{"id": "2511.01188", "pdf": "https://arxiv.org/pdf/2511.01188", "abs": "https://arxiv.org/abs/2511.01188", "authors": ["Lvhua Wu", "Xuefeng Jiang", "Sheng Sun", "Tian Wen", "Yuwei Wang", "Min Liu"], "title": "ZoFia: Zero-Shot Fake News Detection with Entity-Guided Retrieval and Multi-LLM Interaction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid spread of fake news threatens social stability and public trust,\nrendering its detection an imperative research priority. Although large\nlanguage models (LLMs) excel at numerous natural language processing tasks with\ntheir remarkable contextual understanding and extensive prior knowledge, the\ntime-bounded knowledge coverage and tendency for generating hallucination\ncontent reduce their reliability when handling fast-evolving news streams.\nFurthermore, models trained on existing static datasets also often lack the\ngeneralization needed for emerging news topics. To address these challenges, we\npropose ZoFia, a novel two-stage zero-shot fake news detection framework.\nFirst, we introduce Hierarchical Salience to quantify the importance of\nentities in the news content, and propose the SC-MMR algorithm to effectively\nselect an informative and diverse set of keywords that serve as queries for\nretrieving up-to-date external evidence. Subsequently, a multi LLM interactive\nsystem, in which each agent assumes a distinct role, performs multi-view\ncollaborative analysis and adversarial debate over the news text and its\nrelated information, and finally produces an interpretable and robust judgment.\nComprehensive experiments on two public datasets demonstrate that ZoFia\nobviously outperforms existing zero-shot baselines and most of few-shot\nmethods. Our codes will be open-sourced to facilitate related communities."}
{"id": "2511.01191", "pdf": "https://arxiv.org/pdf/2511.01191", "abs": "https://arxiv.org/abs/2511.01191", "authors": ["Ru Wang", "Wei Huang", "Qi Cao", "Yusuke Iwasawa", "Yutaka Matsuo", "Jiaxian Guo"], "title": "Self-Harmony: Learning to Harmonize Self-Supervision and Self-Play in Test-Time Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Test-time reinforcement learning (TTRL) offers a label-free paradigm for\nadapting models using only synthetic signals at inference, but its success\nhinges on constructing reliable learning signals. Standard approaches such as\nmajority voting often collapse to spurious yet popular answers. We introduce\nSelf-Harmony, a framework built on a simple intuition: the correct answer\nshould remain stable across both an original question and its paraphrase.\nSelf-Harmony operationalizes this by employing a single model in two\ncomplementary roles: a Solver to produce answers and a Reframer to rephrase the\ninput. Based on this, we further propose a pseudo-label method: instead of\nmajority voting, it aggregates answer frequencies across these original and\nreframed views using the harmonic mean. This is a process that naturally\nselects for solutions stable under reframing, thereby avoiding the common trap\nof favoring view-dependent, spurious answers. Crucially, this requires no human\nsupervision or auxiliary models. Across diverse reasoning benchmarks,\nSelf-Harmony achieves state-of-the-art results at the label-free test-time\nsetting, ranking first in 28 of 30 settings across multiple methods. Beyond\naccuracy, it demonstrates unprecedented robustness, with zero training failures\nin all experiments, underscoring its stability and reliability."}
{"id": "2511.01192", "pdf": "https://arxiv.org/pdf/2511.01192", "abs": "https://arxiv.org/abs/2511.01192", "authors": ["Guoxin Ma", "Xiaoming Liu", "Zhanhan Zhang", "Chengzhengxu Li", "Shengchao Liu", "Yu Lan"], "title": "DEER: Disentangled Mixture of Experts with Instance-Adaptive Routing for Generalizable Machine-Generated Text Detection", "categories": ["cs.CL"], "comment": "Under Review", "summary": "Detecting machine-generated text (MGT) has emerged as a critical challenge,\ndriven by the rapid advancement of large language models (LLMs) capable of\nproducing highly realistic, human-like content. However, the performance of\ncurrent approaches often degrades significantly under domain shift. To address\nthis challenge, we propose a novel framework designed to capture both\ndomain-specific and domain-general MGT patterns through a two-stage\nDisentangled mixturE-of-ExpeRts (DEER) architecture. First, we introduce a\ndisentangled mixture-of-experts module, in which domain-specific experts learn\nfine-grained, domain-local distinctions between human and machine-generated\ntext, while shared experts extract transferable, cross-domain features. Second,\nto mitigate the practical limitation of unavailable domain labels during\ninference, we design a reinforcement learning-based routing mechanism that\ndynamically selects the appropriate experts for each input instance,\neffectively bridging the train-inference gap caused by domain uncertainty.\nExtensive experiments on five in-domain and five out-of-domain benchmark\ndatasets demonstrate that DEER consistently outperforms state-of-the-art\nmethods, achieving average F1-score improvements of 1.39% and 5.32% on\nin-domain and out-of-domain datasets respectively, along with accuracy gains of\n1.35% and 3.61% respectively. Ablation studies confirm the critical\ncontributions of both disentangled expert specialization and adaptive routing\nto model performance."}
{"id": "2511.01265", "pdf": "https://arxiv.org/pdf/2511.01265", "abs": "https://arxiv.org/abs/2511.01265", "authors": ["Mo El-Haj", "Paul Rayson"], "title": "AraFinNews: Arabic Financial Summarisation with Domain-Adapted LLMs", "categories": ["cs.CL"], "comment": "10 pages", "summary": "This paper investigates the impact of domain specificity on abstractive\nsummarisation of Arabic financial texts using large language models (LLMs). We\nintroduce AraFinNews, the largest publicly available Arabic financial news\ndataset to date, comprising 212,500 article--headline pairs spanning nearly a\ndecade of reporting from October 2015 to July 2025. Designed as the Arabic\nequivalent of major English summarisation corpora such as CNN/DailyMail,\nAraFinNews provides a robust benchmark for evaluating domain-specific language\nunderstanding and generation in financial contexts. Using this resource, we\nevaluate transformer-based models -- including mT5, AraT5, and the\ndomain-adapted FinAraT5 -- to examine how financial-domain pretraining\ninfluences factual accuracy, numerical reliability, and stylistic alignment\nwith professional reporting. Experimental results show that domain-adapted\nmodels generate more faithful and coherent summaries, particularly in handling\nquantitative and entity-centric information. The findings highlight the\nimportance of domain-specific adaptation for improving factual consistency and\nnarrative fluency in Arabic financial summarisation. The dataset is freely\navailable for non-commercial research at\nhttps://github.com/ArabicNLP-UK/AraFinNews."}
{"id": "2511.01282", "pdf": "https://arxiv.org/pdf/2511.01282", "abs": "https://arxiv.org/abs/2511.01282", "authors": ["Min Fang", "Zhihui Fu", "Qibin Zhao", "Jun Wang"], "title": "When, What, and How: Rethinking Retrieval-Enhanced Speculative Decoding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speculative decoding (SD) has emerged as an effective technique to accelerate\nlarge language model (LLM) inference without compromising output quality.\nHowever, the achievable speedup largely depends on the effectiveness of the\ndrafting model. While model-based methods like EAGLE-2 are accurate but costly,\nretrieval-enhanced methods like SAM-Decoding rely on heuristic switching\nstrategies that often trigger unnecessary retrievals. To address this, we\npropose ReSpec (\\textbf{Re}trieval-enhanced \\textbf{Spe}culative Decoding), a\nnovel framework that transforms heuristic drafter switching into adaptive\ndecision-making. ReSpec features three core innovations: 1) An\n\\textbf{entropy-guided adaptive trigger} quantifies contextual predictability\nto initiate retrieval only when uncertainty is low, avoiding costly low-quality\nspeculations. 2) A \\textbf{feedback-driven candidate selection} leverages\nhistorical feedback to organize multiple high-quality candidates for parallel\nverification, maximizing retrieval utility. 3) A source-aware \\textbf{relaxed\nverification strategy} applies strict checks to model-generated drafts while\nusing a relaxed verification for retrieved drafts, achieving a better balance\nbetween accuracy and efficiency. Extensive experiments on Spec-Bench\ndemonstrate that ReSpec achieves state-of-the-art acceleration,outperforming\nEAGLE-2 and SAM-Decoding by over $33\\%$ and $25\\%$, respectively, while\nmaintaining output quality."}
{"id": "2511.01287", "pdf": "https://arxiv.org/pdf/2511.01287", "abs": "https://arxiv.org/abs/2511.01287", "authors": ["Qin Zhou", "Zhexin Zhang", "Zhi Li", "Limin Sun"], "title": "\"Give a Positive Review Only\": An Early Investigation Into In-Paper Prompt Injection Attacks and Defenses for AI Reviewers", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "With the rapid advancement of AI models, their deployment across diverse\ntasks has become increasingly widespread. A notable emerging application is\nleveraging AI models to assist in reviewing scientific papers. However, recent\nreports have revealed that some papers contain hidden, injected prompts\ndesigned to manipulate AI reviewers into providing overly favorable\nevaluations. In this work, we present an early systematic investigation into\nthis emerging threat. We propose two classes of attacks: (1) static attack,\nwhich employs a fixed injection prompt, and (2) iterative attack, which\noptimizes the injection prompt against a simulated reviewer model to maximize\nits effectiveness. Both attacks achieve striking performance, frequently\ninducing full evaluation scores when targeting frontier AI reviewers.\nFurthermore, we show that these attacks are robust across various settings. To\ncounter this threat, we explore a simple detection-based defense. While it\nsubstantially reduces the attack success rate, we demonstrate that an adaptive\nattacker can partially circumvent this defense. Our findings underscore the\nneed for greater attention and rigorous safeguards against prompt-injection\nthreats in AI-assisted peer review."}
{"id": "2511.01289", "pdf": "https://arxiv.org/pdf/2511.01289", "abs": "https://arxiv.org/abs/2511.01289", "authors": ["Saiyma Sittul Muna", "Rezwan Islam Salvi", "Mushfiqur Rahman Mushfique", "Ajwad Abrar"], "title": "FirstAidQA: A Synthetic Dataset for First Aid and Emergency Response in Low-Connectivity Settings", "categories": ["cs.CL"], "comment": "Accepted at the 5th Muslims in Machine Learning (MusIML) Workshop,\n  co-located with NeurIPS 2025", "summary": "In emergency situations, every second counts. The deployment of Large\nLanguage Models (LLMs) in time-sensitive, low or zero-connectivity environments\nremains limited. Current models are computationally intensive and unsuitable\nfor low-tier devices often used by first responders or civilians. A major\nbarrier to developing lightweight, domain-specific solutions is the lack of\nhigh-quality datasets tailored to first aid and emergency response. To address\nthis gap, we introduce FirstAidQA, a synthetic dataset containing 5,500\nhigh-quality question answer pairs that encompass a wide range of first aid and\nemergency response scenarios. The dataset was generated using a Large Language\nModel, ChatGPT-4o-mini, with prompt-based in-context learning, using texts from\nthe Vital First Aid Book (2019). We applied preprocessing steps such as text\ncleaning, contextual chunking, and filtering, followed by human validation to\nensure accuracy, safety, and practical relevance of the QA pairs. FirstAidQA is\ndesigned to support instruction-tuning and fine-tuning of LLMs and Small\nLanguage Models (SLMs), enabling faster, more reliable, and offline-capable\nsystems for emergency settings. We publicly release the dataset to advance\nresearch on safety-critical and resource-constrained AI applications in first\naid and emergency response. The dataset is available on Hugging Face at\nhttps://huggingface.co/datasets/i-am-mushfiq/FirstAidQA."}
{"id": "2511.01305", "pdf": "https://arxiv.org/pdf/2511.01305", "abs": "https://arxiv.org/abs/2511.01305", "authors": ["Aman Ganapathy Manvattira", "Yifei Xu", "Ziyue Dang", "Songwu Lu"], "title": "DeepSpecs: Expert-Level Questions Answering in 5G", "categories": ["cs.CL", "cs.AI", "cs.NI"], "comment": null, "summary": "5G technology enables mobile Internet access for billions of users. Answering\nexpert-level questions about 5G specifications requires navigating thousands of\npages of cross-referenced standards that evolve across releases. Existing\nretrieval-augmented generation (RAG) frameworks, including telecom-specific\napproaches, rely on semantic similarity and cannot reliably resolve\ncross-references or reason about specification evolution. We present DeepSpecs,\na RAG system enhanced by structural and temporal reasoning via three\nmetadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB\n(line-level version diffs), and TDocDB (standardization meeting documents).\nDeepSpecs explicitly resolves cross-references by recursively retrieving\nreferenced clauses through metadata lookup, and traces specification evolution\nby mining changes and linking them to Change Requests that document design\nrationale. We curate two 5G QA datasets: 573 expert-annotated real-world\nquestions from practitioner forums and educational resources, and 350\nevolution-focused questions derived from approved Change Requests. Across\nmultiple LLM backends, DeepSpecs outperforms base models and state-of-the-art\ntelecom RAG systems; ablations confirm that explicit cross-reference resolution\nand evolution-aware retrieval substantially improve answer quality,\nunderscoring the value of modeling the structural and temporal properties of 5G\nstandards."}
{"id": "2511.01323", "pdf": "https://arxiv.org/pdf/2511.01323", "abs": "https://arxiv.org/abs/2511.01323", "authors": ["Jiabao Ji", "Min Li", "Priyanshu Kumar", "Shiyu Chang", "Saloni Potdar"], "title": "DEEPAMBIGQA: Ambiguous Multi-hop Questions for Benchmarking LLM Answer Completeness", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages", "summary": "Large language models (LLMs) with integrated search tools show strong promise\nin open-domain question answering (QA), yet they often struggle to produce\ncomplete answer set to complex questions such as Which actor from the film Heat\nwon at least one Academy Award?, which requires (1) distinguishing between\nmultiple films sharing the same title and (2) reasoning across a large set of\nactors to gather and integrate evidence. Existing QA benchmarks rarely evaluate\nboth challenges jointly. To address this, we introduce DeepAmbigQAGen, an\nautomatic data generation pipeline that constructs QA tasks grounded in text\ncorpora and linked knowledge graph, generating natural and verifiable questions\nthat systematically embed name ambiguity and multi-step reasoning. Based on\nthis, we build DeepAmbigQA, a dataset of 3,600 questions requiring multi-hop\nreasoning and half of them explicit name ambiguity resolving. Experiments\nreveal that, even state-of-the-art GPT-5 show incomplete answers, achieving\nonly 0.13 exact match on ambiguous questions and 0.21 on non-ambiguous\nquestions. These findings highlight the need for more robust QA systems aimed\nat information gathering and answer completeness."}
{"id": "2511.01354", "pdf": "https://arxiv.org/pdf/2511.01354", "abs": "https://arxiv.org/abs/2511.01354", "authors": ["Wenrui Cai", "Chengyu Wang", "Junbing Yan", "Jun Huang", "Xiangzhong Fang"], "title": "Thinking with DistilQwen: A Tale of Four Distilled Reasoning and Reward Model Series", "categories": ["cs.CL", "cs.AI"], "comment": "emnlp 2025 industry track", "summary": "Recently, the demand for small and efficient reasoning models to support\nreal-world applications has driven the development of knowledge distillation\ntechniques that balance reasoning performance and inference speed. In this\npaper, we further extend the DistilQwen model family, initialized from the Qwen\nmodels, by introducing four model series specifically designed to meet\nindustrial requirements. The distilled model collection comprises: (1)\nslow-thinking models, optimized for reasoning tasks that require high accuracy;\n(2) two series of adaptive-thinking models, which dynamically adjust reasoning\nstrategies based on input tasks to maximize efficiency across diverse\nscenarios; and (3) distilled reward models, which enable further reinforcement\nlearning of reasoning models using distilled knowledge. Comprehensive\nevaluations across multiple benchmarks demonstrate both high inference\nefficiency and strong reasoning performance for these models, as well as the\npractical utility of distilled reward models. We further show that these models\nsupport industry practitioners by providing scalable training and inference\nfunctionalities on the Alibaba Cloud PAI (Platform for Artificial Intelligence)\nplatform."}
{"id": "2511.01359", "pdf": "https://arxiv.org/pdf/2511.01359", "abs": "https://arxiv.org/abs/2511.01359", "authors": ["Sapir Harary", "Eran Hirsch", "Aviv Slobodkin", "David Wan", "Mohit Bansal", "Ido Dagan"], "title": "PrefixNLI: Detecting Factual Inconsistencies as Soon as They Arise", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages + appendix. Code, datasets, and models are available at\n  https://github.com/sapirharary/PrefixNLI", "summary": "Natural Language Inference (NLI) models have been used in various ways to\nimprove the factuality of LLM outputs. This is typically done by applying an\nNLI model to judge whether the model output is entailed from the supposed\nevidence, triggering some corrective actions, such as beam reranking at\ninference time or RL rewards during training. While NLI models are trained to\ndetect factual inconsistencies over complete sentences, decisions in the common\nautoregressive generation architecture are made for each evolving text prefix,\nduring decoding. Addressing this setting, we generalize the entailment\ndetection task to apply over arbitrary text prefixes, and suggest its utility\nfor improving generation faithfulness. Providing suitable evaluation and\ntraining datasets for this task, we train MiniTruePrefixes, a novel specialized\nmodel that better detects factual inconsistencies over text prefixes,\noutperforming comparable baseline NLI models by 5-14 F1 points in prefix-level\nentailment. We further demonstrate that integrating MiniTruePrefixes into a\ncontrolled decoding framework substantially improves factual consistency in\nabstractive summarization. When guided by MiniTruePrefixes,\nLLaMA-3.2-3B-Instruct matches the faithfulness and runtime of the 8B model from\nthe same model family, while using only half the memory."}
{"id": "2511.01360", "pdf": "https://arxiv.org/pdf/2511.01360", "abs": "https://arxiv.org/abs/2511.01360", "authors": ["Aadi Palnitkar", "Arjun Suresh", "Rishi Rajesh", "Puneet Puli"], "title": "Safer in Translation? Presupposition Robustness in Indic Languages", "categories": ["cs.CL"], "comment": "This is a submission to LREC 2026 (Language Resources and Evaluation\n  Conference 2026). Corresponding author: aadipalnitkar96@gmail.com", "summary": "Increasingly, more and more people are turning to large language models\n(LLMs) for healthcare advice and consultation, making it important to gauge the\nefficacy and accuracy of the responses of LLMs to such queries. While there are\npre-existing medical benchmarks literature which seeks to accomplish this very\ntask, these benchmarks are almost universally in English, which has led to a\nnotable gap in existing literature pertaining to multilingual LLM evaluation.\nWithin this work, we seek to aid in addressing this gap with Cancer-Myth-Indic,\nan Indic language benchmark built by translating a 500-item subset of\nCancer-Myth, sampled evenly across its original categories, into five\nunder-served but widely used languages from the subcontinent (500 per language;\n2,500 translated items total). Native-speaker translators followed a style\nguide for preserving implicit presuppositions in translation; items feature\nfalse presuppositions relating to cancer. We evaluate several popular LLMs\nunder this presupposition stress."}
{"id": "2511.01365", "pdf": "https://arxiv.org/pdf/2511.01365", "abs": "https://arxiv.org/abs/2511.01365", "authors": ["İbrahim Ethem Deveci", "Duygu Ataman"], "title": "The Ouroboros of Benchmarking: Reasoning Evaluation in an Era of Saturation", "categories": ["cs.CL"], "comment": "Accepted to NeurIPS 2025 Workshop on LLM Evaluation\n  (https://openreview.net/group?id=NeurIPS.cc/2025/Workshop/LLM_Evaluation)", "summary": "The rapid rise of Large Language Models (LLMs) and Large Reasoning Models\n(LRMs) has been accompanied by an equally rapid increase of benchmarks used to\nassess them. However, due to both improved model competence resulting from\nscaling and novel training advances as well as likely many of these datasets\nbeing included in pre or post training data, results become saturated, driving\na continuous need for new and more challenging replacements. In this paper, we\ndiscuss whether surpassing a benchmark truly demonstrates reasoning ability or\nare we simply tracking numbers divorced from the capabilities we claim to\nmeasure? We present an investigation focused on three model families, OpenAI,\nAnthropic, and Google, and how their reasoning capabilities across different\nbenchmarks evolve over the years. We also analyze performance trends over the\nyears across different reasoning tasks and discuss the current situation of\nbenchmarking and remaining challenges. By offering a comprehensive overview of\nbenchmarks and reasoning tasks, our work aims to serve as a first reference to\nground future research in reasoning evaluation and model development."}
{"id": "2511.01380", "pdf": "https://arxiv.org/pdf/2511.01380", "abs": "https://arxiv.org/abs/2511.01380", "authors": ["Wessel Poelman", "Thomas Bauwens", "Miryam de Lhoneux"], "title": "Confounding Factors in Relating Model Performance to Morphology", "categories": ["cs.CL"], "comment": "EMNLP 2025: Main Conference", "summary": "The extent to which individual language characteristics influence\ntokenization and language modeling is an open question. Differences in\nmorphological systems have been suggested as both unimportant and crucial to\nconsider (Cotterell et al., 2018; Gerz et al., 2018a; Park et al., 2021, inter\nalia). We argue this conflicting evidence is due to confounding factors in\nexperimental setups, making it hard to compare results and draw conclusions. We\nidentify confounding factors in analyses trying to answer the question of\nwhether, and how, morphology relates to language modeling. Next, we re-assess\nthree hypotheses by Arnett & Bergen (2025) for why modeling agglutinative\nlanguages results in higher perplexities than fusional languages: they look at\nmorphological alignment of tokenization, tokenization efficiency, and dataset\nsize. We show that each conclusion includes confounding factors. Finally, we\nintroduce token bigram metrics as an intrinsic way to predict the difficulty of\ncausal language modeling, and find that they are gradient proxies for\nmorphological complexity that do not require expert annotation. Ultimately, we\noutline necessities to reliably answer whether, and how, morphology relates to\nlanguage modeling."}
{"id": "2511.01386", "pdf": "https://arxiv.org/pdf/2511.01386", "abs": "https://arxiv.org/abs/2511.01386", "authors": ["Muhammed Yusuf Kartal", "Suha Kagan Kose", "Korhan Sevinç", "Burak Aktas"], "title": "RAGSmith: A Framework for Finding the Optimal Composition of Retrieval-Augmented Generation Methods Across Datasets", "categories": ["cs.CL", "cs.AI", "cs.IR", "H.3.3; I.2.7"], "comment": "45 pages", "summary": "Retrieval-Augmented Generation (RAG) quality depends on many interacting\nchoices across retrieval, ranking, augmentation, prompting, and generation, so\noptimizing modules in isolation is brittle. We introduce RAGSmith, a modular\nframework that treats RAG design as an end-to-end architecture search over nine\ntechnique families and 46{,}080 feasible pipeline configurations. A genetic\nsearch optimizes a scalar objective that jointly aggregates retrieval metrics\n(recall@k, mAP, nDCG, MRR) and generation metrics (LLM-Judge and semantic\nsimilarity). We evaluate on six Wikipedia-derived domains (Mathematics, Law,\nFinance, Medicine, Defense Industry, Computer Science), each with 100 questions\nspanning factual, interpretation, and long-answer types. RAGSmith finds\nconfigurations that consistently outperform naive RAG baseline by +3.8\\% on\naverage (range +1.2\\% to +6.9\\% across domains), with gains up to +12.5\\% in\nretrieval and +7.5\\% in generation. The search typically explores $\\approx\n0.2\\%$ of the space ($\\sim 100$ candidates) and discovers a robust backbone --\nvector retrieval plus post-generation reflection/revision -- augmented by\ndomain-dependent choices in expansion, reranking, augmentation, and prompt\nreordering; passage compression is never selected. Improvement magnitude\ncorrelates with question type, with larger gains on factual/long-answer mixes\nthan interpretation-heavy sets. These results provide practical, domain-aware\nguidance for assembling effective RAG systems and demonstrate the utility of\nevolutionary search for full-pipeline optimization."}
{"id": "2511.01409", "pdf": "https://arxiv.org/pdf/2511.01409", "abs": "https://arxiv.org/abs/2511.01409", "authors": ["Heng Zhou", "Ao Yu", "Yuchen Fan", "Jianing Shi", "Li Kang", "Hejia Geng", "Yongting Zhang", "Yutao Fan", "Yuhao Wu", "Tiancheng He", "Yiran Qin", "Lei Bai", "Zhenfei Yin"], "title": "LiveSearchBench: An Automatically Constructed Benchmark for Retrieval and Reasoning over Dynamic Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating large language models (LLMs) on question answering often relies on\nstatic benchmarks that reward memorization and understate the role of\nretrieval, failing to capture the dynamic nature of world knowledge. We present\nLiveSearchBench, an automated pipeline for constructing retrieval-dependent\nbenchmarks from recent knowledge updates. Our method computes deltas between\nsuccessive Wikidata snapshots, filters candidate triples for quality, and\nsynthesizes natural-language questions at three levels of reasoning difficulty,\neach guaranteed to admit a unique, verifiable answer through SPARQL validation.\nThe pipeline is fully automated, scalable across time, and minimizes human\nintervention, enabling continual regeneration of temporally grounded\nbenchmarks. Experiments show a pronounced performance drop when models confront\nfacts that post-date pretraining, with the gap most salient on multi-hop\nqueries. Retrieval augmented methods and larger, instruction-tuned models\nprovide partial gains but fail to close this recency gap. By design,\nLiveSearchBench shifts evaluation from static memorization toward tasks that\nrequire up-to-date retrieval and reasoning, offering a foundation for\nsystematic, long-term assessment of LLMs under evolving knowledge."}
{"id": "2511.01454", "pdf": "https://arxiv.org/pdf/2511.01454", "abs": "https://arxiv.org/abs/2511.01454", "authors": ["Sergio Torres Aguilar"], "title": "\"Don't Teach Minerva\": Guiding LLMs Through Complex Syntax for Faithful Latin Translation with RAG", "categories": ["cs.CL", "cs.DL"], "comment": null, "summary": "Translating a morphology-rich, low-resource language like Latin poses\nsignificant challenges. This paper introduces a reproducible draft-based\nrefinement pipeline that elevates open-source Large Language Models (LLMs) to a\nperformance level statistically comparable to top-tier proprietary systems. Our\nmethod first uses a fine-tuned NLLB-1.3B model to generate a high-quality,\nstructurally faithful draft. A zero-shot LLM (Llama-3.3 or Qwen3) then polishes\nthis draft, a process that can be further enhanced by augmenting the context\nwith retrieved out-context examples (RAG). We demonstrate the robustness of\nthis approach on two distinct benchmarks: a standard in-domain test set\n(Rosenthal, 2023) and a new, challenging out-of-domain (OOD) set of\n12th-century Latin letters (2025). Our central finding is that this open-source\nRAG system achieves performance statistically comparable to the GPT-5 baseline,\nwithout any task-specific LLM fine-tuning. We release the pipeline, the\nChartres OOD set, and evaluation scripts and models to facilitate replicability\nand further research."}
{"id": "2511.01470", "pdf": "https://arxiv.org/pdf/2511.01470", "abs": "https://arxiv.org/abs/2511.01470", "authors": ["Lujie Niu", "Lei Shen", "Yi Jiang", "Caixia Yuan", "Xiaojie Wang", "Wenbo Su", "Bo zheng"], "title": "BARD: budget-aware reasoning distillation", "categories": ["cs.CL"], "comment": null, "summary": "While long Chain-of-Thought (CoT) distillation effectively transfers\nreasoning capability to smaller language models, the reasoning process often\nremains redundant and computational budget uncontrollable, leading to\ninefficient resource usage. To address this limitation, we propose\n\\textbf{Budget-Aware Reasoning Distillation (BARD)}, a novel framework that\nsimultaneously distills reasoning capability and enables fine-grained control\nover the reasoning length. BARD uses the thinking budget as a user-specified\ncontrol signal, allowing the model to dynamically balance reasoning performance\nand computational efficiency. To achieve this concept, BARD introduces a\ntwo-phase training regimen. The first phase, Supervised Fine-Tuning (SFT) on\nteacher-generated long CoT data compressed to various budget levels,\nbootstrapping the model's understanding of budget constraints. The second phase\nleverages Reinforcement Learning (RL) from a reward signal in consideration of\nreasoning performance and budget fidelity simultaneously. Incorporating the\ntwo-phase regimen is crucial to avoiding policy degradation and ensuring that\nboth objectives are optimized jointly. Extensive experiments demonstrate that\nour method empowers an 8B student model to achieve strong performance on\nchallenging reasoning benchmarks (\\textit{AIME24, AIME25, GPQA}) while\nproviding precise and adaptive control over its reasoning length across a wide\nrange of budgets."}
{"id": "2511.01482", "pdf": "https://arxiv.org/pdf/2511.01482", "abs": "https://arxiv.org/abs/2511.01482", "authors": ["Neha Sharma", "Navneet Agarwal", "Kairit Sirts"], "title": "Towards Consistent Detection of Cognitive Distortions: LLM-Based Annotation and Dataset-Agnostic Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Text-based automated Cognitive Distortion detection is a challenging task due\nto its subjective nature, with low agreement scores observed even among expert\nhuman annotators, leading to unreliable annotations. We explore the use of\nLarge Language Models (LLMs) as consistent and reliable annotators, and propose\nthat multiple independent LLM runs can reveal stable labeling patterns despite\nthe inherent subjectivity of the task. Furthermore, to fairly compare models\ntrained on datasets with different characteristics, we introduce a\ndataset-agnostic evaluation framework using Cohen's kappa as an effect size\nmeasure. This methodology allows for fair cross-dataset and cross-study\ncomparisons where traditional metrics like F1 score fall short. Our results\nshow that GPT-4 can produce consistent annotations (Fleiss's Kappa = 0.78),\nresulting in improved test set performance for models trained on these\nannotations compared to those trained on human-labeled data. Our findings\nsuggest that LLMs can offer a scalable and internally consistent alternative\nfor generating training data that supports strong downstream performance in\nsubjective NLP tasks."}
{"id": "2511.01490", "pdf": "https://arxiv.org/pdf/2511.01490", "abs": "https://arxiv.org/abs/2511.01490", "authors": ["Max Schaffelder", "Albert Gatt"], "title": "Synthetic Eggs in Many Baskets: The Impact of Synthetic Data Diversity on LLM Fine-Tuning", "categories": ["cs.CL"], "comment": null, "summary": "As synthetic data becomes widely used in language model development,\nunderstanding its impact on model behavior is crucial. This paper investigates\nthe impact of the diversity of sources of synthetic data on fine-tuned large\nlanguage models. We focus on three key dimensions: distribution collapse,\nadversarial robustness, and self-preference bias. Our findings reveal that\nfine-tuning models on synthetic data from diverse sources can mitigate\ndistribution collapse, preserving the breadth of the output distribution and\nthe diversity of the output text. Furthermore, while both human and synthetic\nfine-tuning data can remove safeguards, the latter preserves higher output\nquality, thus making outputs potentially more usable and dangerous. Finally,\nfine-tuning reduces self-preference bias, with human data being the most\neffective, followed by multi-source synthetic data."}
{"id": "2511.01512", "pdf": "https://arxiv.org/pdf/2511.01512", "abs": "https://arxiv.org/abs/2511.01512", "authors": ["Ayesha Afroza Mohsin", "Mashrur Ahsan", "Nafisa Maliyat", "Shanta Maria", "Syed Rifat Raiyan", "Hasan Mahmud", "Md Kamrul Hasan"], "title": "BanglaNirTox: A Large-scale Parallel Corpus for Explainable AI in Bengali Text Detoxification", "categories": ["cs.CL", "cs.AI"], "comment": "Under review, 6 pages, 1 figure, 2 tables", "summary": "Toxic language in Bengali remains prevalent, especially in online\nenvironments, with few effective precautions against it. Although text\ndetoxification has seen progress in high-resource languages, Bengali remains\nunderexplored due to limited resources. In this paper, we propose a novel\npipeline for Bengali text detoxification that combines Pareto class-optimized\nlarge language models (LLMs) and Chain-of-Thought (CoT) prompting to generate\ndetoxified sentences. To support this effort, we construct BanglaNirTox, an\nartificially generated parallel corpus of 68,041 toxic Bengali sentences with\nclass-wise toxicity labels, reasonings, and detoxified paraphrases, using\nPareto-optimized LLMs evaluated on random samples. The resulting BanglaNirTox\ndataset is used to fine-tune language models to produce better detoxified\nversions of Bengali sentences. Our findings show that Pareto-optimized LLMs\nwith CoT prompting significantly enhance the quality and consistency of Bengali\ntext detoxification."}
{"id": "2511.01526", "pdf": "https://arxiv.org/pdf/2511.01526", "abs": "https://arxiv.org/abs/2511.01526", "authors": ["Seokhoon Kang", "Yejin Jeon", "Seonjeong Hwang", "Gary Geunbae Lee"], "title": "Difficulty-Controllable Cloze Question Distractor Generation", "categories": ["cs.CL"], "comment": null, "summary": "Multiple-choice cloze questions are commonly used to assess linguistic\nproficiency and comprehension. However, generating high-quality distractors\nremains challenging, as existing methods often lack adaptability and control\nover difficulty levels, and the absence of difficulty-annotated datasets\nfurther hinders progress. To address these issues, we propose a novel framework\nfor generating distractors with controllable difficulty by leveraging both data\naugmentation and a multitask learning strategy. First, to create a\nhigh-quality, difficulty-annotated dataset, we introduce a two-way distractor\ngeneration process in order to produce diverse and plausible distractors. These\ncandidates are subsequently refined through filtering and then categorized by\ndifficulty using an ensemble QA system. Second, this newly created dataset is\nleveraged to train a difficulty-controllable generation model via multitask\nlearning. The framework includes carefully designed auxiliary tasks that\nenhance the model's semantic understanding of distractors and its ability to\nestimate their difficulty. Experimental results demonstrate that our method\ngenerates high-quality distractors across difficulty levels and substantially\noutperforms GPT-4o in aligning distractor difficulty with human perception."}
{"id": "2511.01558", "pdf": "https://arxiv.org/pdf/2511.01558", "abs": "https://arxiv.org/abs/2511.01558", "authors": ["Luciana Ciringione", "Emma Franchino", "Simone Reigl", "Isaia D'Onofrio", "Anna Serbati", "Oleksandra Poquet", "Florence Gabriel", "Massimo Stella"], "title": "Math anxiety and associative knowledge structure are entwined in psychology students but not in Large Language Models like GPT-3.5 and GPT-4o", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Math anxiety poses significant challenges for university psychology students,\naffecting their career choices and overall well-being. This study employs a\nframework based on behavioural forma mentis networks (i.e. cognitive models\nthat map how individuals structure their associative knowledge and emotional\nperceptions of concepts) to explore individual and group differences in the\nperception and association of concepts related to math and anxiety. We\nconducted 4 experiments involving psychology undergraduates from 2 samples (n1\n= 70, n2 = 57) compared against GPT-simulated students (GPT-3.5: n2 = 300;\nGPT-4o: n4 = 300). Experiments 1, 2, and 3 employ individual-level network\nfeatures to predict psychometric scores for math anxiety and its facets\n(observational, social and evaluational) from the Math Anxiety Scale.\nExperiment 4 focuses on group-level perceptions extracted from human students,\nGPT-3.5 and GPT-4o's networks. Results indicate that, in students, positive\nvalence ratings and higher network degree for \"anxiety\", together with negative\nratings for \"math\", can predict higher total and evaluative math anxiety. In\ncontrast, these models do not work on GPT-based data because of differences in\nsimulated networks and psychometric scores compared to humans. These results\nwere also reconciled with differences found in the ways that high/low subgroups\nof simulated and real students framed semantically and emotionally STEM\nconcepts. High math-anxiety students collectively framed \"anxiety\" in an\nemotionally polarising way, absent in the negative perception of low\nmath-anxiety students. \"Science\" was rated positively, but contrasted against\nthe negative perception of \"math\". These findings underscore the importance of\nunderstanding concept perception and associations in managing students' math\nanxiety."}
{"id": "2511.01568", "pdf": "https://arxiv.org/pdf/2511.01568", "abs": "https://arxiv.org/abs/2511.01568", "authors": ["Seungmin Shin", "Dooyoung Kim", "Youngjoong Ko"], "title": "ECO Decoding: Entropy-Based Control for Controllability and Fluency in Controllable Dialogue Generation", "categories": ["cs.CL"], "comment": "Published at EMNLP 2025 main", "summary": "Controllable Dialogue Generation (CDG) enables chatbots to generate responses\nwith desired attributes, and weighted decoding methods have achieved\nsignificant success in the CDG task. However, using a fixed constant value to\nmanage the bias of attribute probabilities makes it challenging to find an\nideal control strength that satisfies both controllability and fluency. To\naddress this issue, we propose ECO decoding (Entropy-based COntrol), which\ndynamically adjusts the control strength at each generation step according to\nthe model's entropy in both the language model and attribute classifier\nprobability distributions. Experiments on the DailyDialog and MultiWOZ datasets\ndemonstrate that ECO decoding consistently improves controllability while\nmaintaining fluency and grammaticality, outperforming prior decoding methods\nacross various models and settings. Furthermore, ECO decoding alleviates\nprobability interpolation issues in multi-attribute generation and consequently\ndemonstrates strong performance in both single and multi-attribute scenarios."}
{"id": "2511.01589", "pdf": "https://arxiv.org/pdf/2511.01589", "abs": "https://arxiv.org/abs/2511.01589", "authors": ["Wenjie Hua", "Hoang H. Nguyen", "Gangyan Ge"], "title": "BIRD: Bronze Inscription Restoration and Dating", "categories": ["cs.CL", "I.2.7"], "comment": "Accepted at EMNLP 2025 (Main Conference)", "summary": "Bronze inscriptions from early China are fragmentary and difficult to date.\nWe introduce BIRD(Bronze Inscription Restoration and Dating), a fully encoded\ndataset grounded in standard scholarly transcriptions and chronological labels.\nWe further propose an allograph-aware masked language modeling framework that\nintegrates domain- and task-adaptive pretraining with a Glyph Net (GN), which\nlinks graphemes and allographs. Experiments show that GN improves restoration,\nwhile glyph-biased sampling yields gains in dating."}
{"id": "2511.01615", "pdf": "https://arxiv.org/pdf/2511.01615", "abs": "https://arxiv.org/abs/2511.01615", "authors": ["Francisco Portillo López"], "title": "Imperfect Language, Artificial Intelligence, and the Human Mind: An Interdisciplinary Approach to Linguistic Errors in Native Spanish Speakers", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 3 figures", "summary": "Linguistic errors are not merely deviations from normative grammar; they\noffer a unique window into the cognitive architecture of language and expose\nthe current limitations of artificial systems that seek to replicate them. This\nproject proposes an interdisciplinary study of linguistic errors produced by\nnative Spanish speakers, with the aim of analyzing how current large language\nmodels (LLM) interpret, reproduce, or correct them. The research integrates\nthree core perspectives: theoretical linguistics, to classify and understand\nthe nature of the errors; neurolinguistics, to contextualize them within\nreal-time language processing in the brain; and natural language processing\n(NLP), to evaluate their interpretation against linguistic errors. A\npurpose-built corpus of authentic errors of native Spanish (+500) will serve as\nthe foundation for empirical analysis. These errors will be tested against AI\nmodels such as GPT or Gemini to assess their interpretative accuracy and their\nability to generalize patterns of human linguistic behavior. The project\ncontributes not only to the understanding of Spanish as a native language but\nalso to the development of NLP systems that are more cognitively informed and\ncapable of engaging with the imperfect, variable, and often ambiguous nature of\nreal human language."}
{"id": "2511.01619", "pdf": "https://arxiv.org/pdf/2511.01619", "abs": "https://arxiv.org/abs/2511.01619", "authors": ["Nikola Ljubešić", "Peter Rupnik", "Ivan Porupski", "Taja Kuzman Pungeršek"], "title": "ParlaSpeech 3.0: Richly Annotated Spoken Parliamentary Corpora of Croatian, Czech, Polish, and Serbian", "categories": ["cs.CL"], "comment": "Submitted to the LREC 2026 conference; 11 pages, 2 figures, 3 tables", "summary": "ParlaSpeech is a collection of spoken parliamentary corpora currently\nspanning four Slavic languages - Croatian, Czech, Polish and Serbian - all\ntogether 6 thousand hours in size. The corpora were built in an automatic\nfashion from the ParlaMint transcripts and their corresponding metadata, which\nwere aligned to the speech recordings of each corresponding parliament. In this\nrelease of the dataset, each of the corpora is significantly enriched with\nvarious automatic annotation layers. The textual modality of all four corpora\nhas been enriched with linguistic annotations and sentiment predictions.\nSimilar to that, their spoken modality has been automatically enriched with\noccurrences of filled pauses, the most frequent disfluency in typical speech.\nTwo out of the four languages have been additionally enriched with detailed\nword- and grapheme-level alignments, and the automatic annotation of the\nposition of primary stress in multisyllabic words. With these enrichments, the\nusefulness of the underlying corpora has been drastically increased for\ndownstream research across multiple disciplines, which we showcase through an\nanalysis of acoustic correlates of sentiment. All the corpora are made\navailable for download in JSONL and TextGrid formats, as well as for search\nthrough a concordancer."}
{"id": "2511.01643", "pdf": "https://arxiv.org/pdf/2511.01643", "abs": "https://arxiv.org/abs/2511.01643", "authors": ["Riccardo Campi", "Nicolò Oreste Pinciroli Vago", "Mathyas Giudici", "Pablo Barrachina Rodriguez-Guisado", "Marco Brambilla", "Piero Fraternali"], "title": "A Graph-based RAG for Energy Efficiency Question Answering", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7; I.2.4; I.2.1; I.2.6"], "comment": null, "summary": "In this work, we investigate the use of Large Language Models (LLMs) within a\ngraph-based Retrieval Augmented Generation (RAG) architecture for Energy\nEfficiency (EE) Question Answering. First, the system automatically extracts a\nKnowledge Graph (KG) from guidance and regulatory documents in the energy\nfield. Then, the generated graph is navigated and reasoned upon to provide\nusers with accurate answers in multiple languages. We implement a human-based\nvalidation using the RAGAs framework properties, a validation dataset\ncomprising 101 question-answer pairs, and domain experts. Results confirm the\npotential of this architecture and identify its strengths and weaknesses.\nValidation results show how the system correctly answers in about three out of\nfour of the cases (75.2 +- 2.7%), with higher results on questions related to\nmore general EE answers (up to 81.0 +- 4.1%), and featuring promising\nmultilingual abilities (4.4% accuracy loss due to translation)."}
{"id": "2511.01649", "pdf": "https://arxiv.org/pdf/2511.01649", "abs": "https://arxiv.org/abs/2511.01649", "authors": ["Hung-Shin Lee", "Chen-Chi Chang", "Ching-Yuan Chen", "Yun-Hsiang Hsu"], "title": "Evaluating Cultural Knowledge Processing in Large Language Models: A Cognitive Benchmarking Framework Integrating Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "This paper has been accepted by The Electronic Library, and the full\n  article is now available on Emerald Insight", "summary": "This study proposes a cognitive benchmarking framework to evaluate how large\nlanguage models (LLMs) process and apply culturally specific knowledge. The\nframework integrates Bloom's Taxonomy with Retrieval-Augmented Generation (RAG)\nto assess model performance across six hierarchical cognitive domains:\nRemembering, Understanding, Applying, Analyzing, Evaluating, and Creating.\nUsing a curated Taiwanese Hakka digital cultural archive as the primary\ntestbed, the evaluation measures LLM-generated responses' semantic accuracy and\ncultural relevance."}
{"id": "2511.01650", "pdf": "https://arxiv.org/pdf/2511.01650", "abs": "https://arxiv.org/abs/2511.01650", "authors": ["Ayesha Gull", "Muhammad Usman Safder", "Rania Elbadry", "Preslav Nakov", "Zhuohan Xie"], "title": "EngChain: A Symbolic Benchmark for Verifiable Multi-Step Reasoning in Engineering", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, includes figures and tables; introduces the EngChain\n  benchmark", "summary": "Large Language Models (LLMs) are increasingly being applied to specialized,\nhigh-stakes domains like engineering, which demands rigorous evaluation of\ntheir complex reasoning capabilities. While current benchmarks assess language\nunderstanding, factual recall, mathematics or code generation, none capture the\nintegrative reasoning central to engineering where scientific principles,\nquantitative modeling and practical constraints must converge. To address this\ngap, we introduce EngChain, a benchmark for verifiable multi-step engineering\nproblem-solving. EngChain contains 90 problems spanning three engineering\nbranches, organized into 9 domains and 20 distinct areas. The problems are\ngenerated from symbolic templates with a high degree of randomization to ensure\ndiversity and eliminate the risk of contamination. With this benchmark, we move\nbeyond final answer accuracy with a two-stage evaluation: we first\nquantitatively verify the numerical and semantic validity of each reasoning\nstep and then introduce LLM-As-A-Judge, an automated system to qualitatively\ncategorize the identified reasoning errors."}
{"id": "2511.01670", "pdf": "https://arxiv.org/pdf/2511.01670", "abs": "https://arxiv.org/abs/2511.01670", "authors": ["Chaoqun Liu", "Mahani Aljunied", "Guizhen Chen", "Hou Pong Chan", "Weiwen Xu", "Yu Rong", "Wenxuan Zhang"], "title": "SeaLLMs-Audio: Large Audio-Language Models for Southeast Asia", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "We introduce SeaLLMs-Audio, the first large audio-language model (LALM)\ntailored for multiple Southeast Asian (SEA) languages-Indonesian (id), Thai\n(th), and Vietnamese (vi)-alongside English (en) and Chinese (zh). Trained on a\nlarge-scale audio corpus, SeaLLMs-Audio exhibits strong performance across\ndiverse audio-centric tasks, spanning fine-grained audio understanding and\nvoice-based interaction. Its key features include: 1) Multilingual: the model\nprimarily supports 5 languages, namely Indonesian, Thai, Vietnamese, English,\nand Chinese; 2) Multimodal: the model accepts flexible input modalities,\nincluding audio only, text only, as well as audio with text; 3) Multi-task: the\nmodel supports a wide range of tasks, including audio analysis tasks such as\nAudio Captioning, Automatic Speech Recognition, Speech-to-Text Translation,\nSpeech Emotion Recognition, Speech Question Answering, and Speech\nSummarization. It also enables voice-based dialogue, including answering\nfactual, mathematical, and general knowledge queries. As a significant step\ntowards advancing audio LLMs in Southeast Asia, we expect SeaLLMs-Audio to\nbenefit both the regional research community and industry. To automate LALM\nevaluation for Southeast Asia, we introduce SeaBench-Audio, a benchmark\nspanning multiple tasks. Experiments show that SeaLLMs-Audio achieves\ncompetitive performance compared with other LALMs on SEA languages."}
{"id": "2511.01689", "pdf": "https://arxiv.org/pdf/2511.01689", "abs": "https://arxiv.org/abs/2511.01689", "authors": ["Sharan Maiya", "Henning Bartsch", "Nathan Lambert", "Evan Hubinger"], "title": "Open Character Training: Shaping the Persona of AI Assistants through Constitutional AI", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, 6 figures, 4 tables", "summary": "The character of the \"AI assistant\" persona generated by modern chatbot large\nlanguage models influences both surface-level behavior and apparent values,\nbeliefs, and ethics. These all affect interaction quality, perceived\nintelligence, and alignment with both developer and user intentions. The\nshaping of this persona, known as character training, is a critical component\nof industry post-training, yet remains effectively unstudied in the academic\nliterature. We introduce the first open implementation of character training,\nleveraging Constitutional AI and a new data pipeline using synthetic\nintrospective data to shape the assistant persona in a more effective and\ncontrolled manner than alternatives such as constraining system prompts or\nactivation steering. Specifically, we fine-tune three popular open-weights\nmodels using 11 example personas, such as humorous, deeply caring, or even\nmalevolent. To track the effects of our approach, we introduce a method which\nanalyzes revealed preferences, uncovering clear and holistic changes in\ncharacter. We find these changes are more robust to adversarial prompting than\nthe above two alternatives, while also leading to more coherent and realistic\ngenerations. Finally, we demonstrate this fine-tuning has little to no effect\non general capabilities as measured by common benchmarks. We describe and\nopen-source our full post-training method, the implementation of which can be\nfound at https://github.com/maiush/OpenCharacterTraining."}
{"id": "2511.01706", "pdf": "https://arxiv.org/pdf/2511.01706", "abs": "https://arxiv.org/abs/2511.01706", "authors": ["Sekh Mainul Islam", "Pepa Atanasova", "Isabelle Augenstein"], "title": "Multi-Step Knowledge Interaction Analysis via Rank-2 Subspace Disentanglement", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under review", "summary": "Natural Language Explanations (NLEs) describe how Large Language Models\n(LLMs) make decisions, drawing on both external Context Knowledge (CK) and\nParametric Knowledge (PK) stored in model weights. Understanding their\ninteraction is key to assessing the grounding of NLEs, yet it remains\nunderexplored. Prior work has largely examined only single-step generation,\ntypically the final answer, and has modelled PK and CK interaction only as a\nbinary choice in a rank-1 subspace. This overlooks richer forms of interaction,\nsuch as complementary or supportive knowledge. We propose a novel rank-2\nprojection subspace that disentangles PK and CK contributions more accurately\nand use it for the first multi-step analysis of knowledge interactions across\nlonger NLE sequences. Experiments on four QA datasets and three open-weight\ninstruction-tuned LLMs show that diverse knowledge interactions are poorly\nrepresented in a rank-1 subspace but are effectively captured in our rank-2\nformulation. Our multi-step analysis reveals that hallucinated NLEs align\nstrongly with the PK direction, context-faithful ones balance PK and CK, and\nChain-of-Thought prompting for NLEs shifts generated NLEs toward CK by reducing\nPK reliance. This work provides the first framework for systematic studies of\nmulti-step knowledge interactions in LLMs through a richer rank-2 subspace\ndisentanglement. Code and data:\nhttps://github.com/copenlu/pk-ck-knowledge-disentanglement."}
{"id": "2511.01720", "pdf": "https://arxiv.org/pdf/2511.01720", "abs": "https://arxiv.org/abs/2511.01720", "authors": ["Mahammad Nuriyev"], "title": "Efficient Tool-Calling Multi-Expert NPC Agent for Commonsense Persona-Grounded Dialogue", "categories": ["cs.CL"], "comment": "10 pages, 1 figure, 2 tables. Technical report for the Commonsense\n  Persona-Grounded Dialogue Challenge (CPDC) 2025, part of the Wordplay 2025\n  Workshop @ EMNLP 2025", "summary": "We present a multi-expert system for creating Non-Player Characters (NPCs)\ncapable of both natural dialogue and contextual action execution in interactive\nenvironments. Using Qwen3 as the base model and Low-Rank Adaptation (LoRA)\nadapters, we instantiate three specialists: tool calling, tool-response\ninterpretation, and direct dialogue. Our system comfortably meets the\ncomputational efficiency requirements, delivering fast responses and\nmaintaining modest resource usage on L40S GPUs. In the Commonsense\nPersona-Grounded Dialogue Challenge 2025, our method ranked second overall.\n  Code available at:\nhttps://github.com/MahammadNuriyev62/CPDC-challenge-2025-solution/"}
{"id": "2511.01805", "pdf": "https://arxiv.org/pdf/2511.01805", "abs": "https://arxiv.org/abs/2511.01805", "authors": ["Jiayi Geng", "Howard Chen", "Ryan Liu", "Manoel Horta Ribeiro", "Robb Willer", "Graham Neubig", "Thomas L. Griffiths"], "title": "Accumulating Context Changes the Beliefs of Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language model (LM) assistants are increasingly used in applications such as\nbrainstorming and research. Improvements in memory and context size have\nallowed these models to become more autonomous, which has also resulted in more\ntext accumulation in their context windows without explicit user intervention.\nThis comes with a latent risk: the belief profiles of models -- their\nunderstanding of the world as manifested in their responses or actions -- may\nsilently change as context accumulates. This can lead to subtly inconsistent\nuser experiences, or shifts in behavior that deviate from the original\nalignment of the models. In this paper, we explore how accumulating context by\nengaging in interactions and processing text -- talking and reading -- can\nchange the beliefs of language models, as manifested in their responses and\nbehaviors.Our results reveal that models' belief profiles are highly malleable:\nGPT-5 exhibits a 54.7% shift in its stated beliefs after 10 rounds of\ndiscussion about moral dilemmas and queries about safety, while Grok 4 shows a\n27.2% shift on political issues after reading texts from the opposing position.\nWe also examine models' behavioral changes by designing tasks that require tool\nuse, where each tool selection corresponds to an implicit belief. We find that\nthese changes align with stated belief shifts, suggesting that belief shifts\nwill be reflected in actual behavior in agentic systems. Our analysis exposes\nthe hidden risk of belief shift as models undergo extended sessions of talking\nor reading, rendering their opinions and actions unreliable."}
{"id": "2511.01807", "pdf": "https://arxiv.org/pdf/2511.01807", "abs": "https://arxiv.org/abs/2511.01807", "authors": ["Adewale Akinfaderin", "Shreyas Subramanian", "Akarsha Sehwag"], "title": "Plan-and-Write: Structure-Guided Length Control for LLMs without Model Retraining", "categories": ["cs.CL", "cs.AI"], "comment": "Presented at Workshop on Prompt Optimization, KDD 2025, Toronto,\n  Canada", "summary": "Length control in Large Language Models (LLMs) is a crucial but\nunder-addressed challenge, with applications ranging from voice interfaces\nrequiring concise responses to research summaries needing comprehensive\noutputs. Current approaches to length control, including Regularized DPO,\nLength-Instruction Fine Tuning, and tool-augmented methods, typically require\nexpensive model retraining or complex inference-time tooling. This paper\npresents a prompt engineering methodology that enables precise length control\nwithout model retraining. Our structure-guided approach implements deliberate\nplanning and word counting mechanisms within the prompt, encouraging the model\nto carefully track and adhere to specified length constraints. Comprehensive\nevaluations across six state-of-the-art LLMs demonstrate that our method\nsignificantly improves length fidelity for several models compared to standard\nprompting when applied to document summarization tasks, particularly for\nshorter-to-medium length constraints. The proposed technique shows varying\nbenefits across different model architectures, with some models demonstrating\nup to 37.6% improvement in length adherence. Quality evaluations further reveal\nthat our approach maintains or enhances overall output quality compared to\nstandard prompting techniques. Our approach provides an immediately deployable\nsolution for applications requiring precise length control, particularly\nvaluable for production environments where model retraining is impractical or\ncost-prohibitive."}
{"id": "2511.01815", "pdf": "https://arxiv.org/pdf/2511.01815", "abs": "https://arxiv.org/abs/2511.01815", "authors": ["Konrad Staniszewski", "Adrian Łańcucki"], "title": "KV Cache Transform Coding for Compact Storage in LLM Inference", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Serving large language models (LLMs) at scale necessitates efficient\nkey-value (KV) cache management. KV caches can be reused across conversation\nturns via shared-prefix prompts that are common in iterative code editing and\nchat. However, stale caches consume scarce GPU memory, require offloading, or\nforce recomputation. We present KVTC, a lightweight transform coder that\ncompresses KV caches for compact on-GPU and off-GPU storage. Drawing on\nclassical media compression, KVTC combines PCA-based feature decorrelation,\nadaptive quantization, and entropy coding. It requires only a brief initial\ncalibration and leaves model parameters unchanged. By exploiting redundancies\nin KV caches, KVTC achieves up to 20$\\times$ compression while maintaining\nreasoning and long-context accuracy, and 40$\\times$ or higher for specific use\ncases. We test KVTC with Llama 3, Mistral NeMo, and R1-Qwen 2.5 models across\nbenchmarks including AIME25, LiveCodeBench, GSM8K, MMLU, Qasper, RULER, and\nMATH-500. It consistently outperforms inference-time baselines such as token\neviction, quantization, and SVD-based methods, while achieving higher\ncompression ratios. These results support KVTC as a practical building block\nfor memory-efficient LLM serving with reusable KV caches."}
{"id": "2511.01846", "pdf": "https://arxiv.org/pdf/2511.01846", "abs": "https://arxiv.org/abs/2511.01846", "authors": ["Thang Luong", "Dawsen Hwang", "Hoang H. Nguyen", "Golnaz Ghiasi", "Yuri Chervonyi", "Insuk Seo", "Junsu Kim", "Garrett Bingham", "Jonathan Lee", "Swaroop Mishra", "Alex Zhai", "Clara Huiyi Hu", "Henryk Michalewski", "Jimin Kim", "Jeonghyun Ahn", "Junhwi Bae", "Xingyou Song", "Trieu H. Trinh", "Quoc V. Le", "Junehyuk Jung"], "title": "Towards Robust Mathematical Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 (main conference),\n  https://aclanthology.org/2025.emnlp-main.1794/", "summary": "Finding the right north-star metrics is highly critical for advancing the\nmathematical reasoning capabilities of foundation models, especially given that\nexisting evaluations are either too easy or only focus on getting correct short\nanswers. To address these issues, we present IMO-Bench, a suite of advanced\nreasoning benchmarks, vetted by a panel of top specialists and that\nspecifically targets the level of the International Mathematical Olympiad\n(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBench\nfirst tests models on 400 diverse Olympiad problems with verifiable short\nanswers. IMO-Proof Bench is the next-level evaluation for proof-writing\ncapabilities, which includes both basic and advanced IMO level problems as well\nas detailed grading guidelines to facilitate automatic grading. These\nbenchmarks played a crucial role in our historic achievement of the gold-level\nperformance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Our\nmodel achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-Proof\nBench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%\nrespectively. We also showed that autograders built with Gemini reasoning\ncorrelate well with human evaluations and construct IMO-GradingBench, with 1000\nhuman gradings on proofs, to enable further progress in automatic evaluation of\nlong-form answers. We hope that IMO-Bench will help the community towards\nadvancing robust mathematical reasoning and release it at\nhttps://imobench.github.io/."}
{"id": "2511.01854", "pdf": "https://arxiv.org/pdf/2511.01854", "abs": "https://arxiv.org/abs/2511.01854", "authors": ["Elias Lumer", "Faheem Nizar", "Anmol Gulati", "Pradeep Honaganahalli Basavaraju", "Vamse Kumar Subbiah"], "title": "Tool-to-Agent Retrieval: Bridging Tools and Agents for Scalable LLM Multi-Agent Systems", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in LLM Multi-Agent Systems enable scalable orchestration of\nsub-agents, each coordinating hundreds or thousands of tools or Model Context\nProtocol (MCP) servers. However, existing retrieval methods typically match\nqueries against coarse agent-level descriptions before routing, which obscures\nfine-grained tool functionality and often results in suboptimal agent\nselection. We introduce Tool-to-Agent Retrieval, a unified framework that\nembeds both tools and their parent agents in a shared vector space and connects\nthem through metadata relationships. By explicitly representing tool\ncapabilities and traversing metadata to the agent level, Tool-to-Agent\nRetrieval enables granular tool-level or agent-level retrieval, ensuring that\nagents and their underlying tools or MCP servers are equally represented\nwithout the context dilution that arises from chunking many tools together.\nEvaluating Tool-to-Agent Retrieval across eight embedding models, our approach\nachieves consistent improvements of 19.4% in Recall@5 and 17.7% in nDCG@5 over\nprevious state-of-the-art agent retrievers on the LiveMCPBench benchmark."}
{"id": "2511.00004", "pdf": "https://arxiv.org/pdf/2511.00004", "abs": "https://arxiv.org/abs/2511.00004", "authors": ["Adrian-Dinu Urse", "Dumitru-Clementin Cercel", "Florin Pop"], "title": "Multimodal Learning with Augmentation Techniques for Natural Disaster Assessment", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.CV"], "comment": "Accepted at 2025 IEEE 21st International Conference on Intelligent\n  Computer Communication and Processing (ICCP 2025)", "summary": "Natural disaster assessment relies on accurate and rapid access to\ninformation, with social media emerging as a valuable real-time source.\nHowever, existing datasets suffer from class imbalance and limited samples,\nmaking effective model development a challenging task. This paper explores\naugmentation techniques to address these issues on the CrisisMMD multimodal\ndataset. For visual data, we apply diffusion-based methods, namely Real\nGuidance and DiffuseMix. For text data, we explore back-translation,\nparaphrasing with transformers, and image caption-based augmentation. We\nevaluated these across unimodal, multimodal, and multi-view learning setups.\nResults show that selected augmentations improve classification performance,\nparticularly for underrepresented classes, while multi-view learning introduces\npotential but requires further refinement. This study highlights effective\naugmentation strategies for building more robust disaster assessment systems."}
{"id": "2511.00020", "pdf": "https://arxiv.org/pdf/2511.00020", "abs": "https://arxiv.org/abs/2511.00020", "authors": ["Suhasnadh Reddy Veluru", "Sai Teja Erukude", "Viswa Chaitanya Marella"], "title": "Multimodal Detection of Fake Reviews using BERT and ResNet-50", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "Published in IEEE", "summary": "In the current digital commerce landscape, user-generated reviews play a\ncritical role in shaping consumer behavior, product reputation, and platform\ncredibility. However, the proliferation of fake or misleading reviews often\ngenerated by bots, paid agents, or AI models poses a significant threat to\ntrust and transparency within review ecosystems. Existing detection models\nprimarily rely on unimodal, typically textual, data and therefore fail to\ncapture semantic inconsistencies across different modalities. To address this\ngap, a robust multimodal fake review detection framework is proposed,\nintegrating textual features encoded with BERT and visual features extracted\nusing ResNet-50. These representations are fused through a classification head\nto jointly predict review authenticity. To support this approach, a curated\ndataset comprising 21,142 user-uploaded images across food delivery,\nhospitality, and e-commerce domains was utilized. Experimental results indicate\nthat the multimodal model outperforms unimodal baselines, achieving an F1-score\nof 0.934 on the test set. Additionally, the confusion matrix and qualitative\nanalysis highlight the model's ability to detect subtle inconsistencies, such\nas exaggerated textual praise paired with unrelated or low-quality images,\ncommonly found in deceptive content. This study demonstrates the critical role\nof multimodal learning in safeguarding digital trust and offers a scalable\nsolution for content moderation across various online platforms."}
{"id": "2511.00024", "pdf": "https://arxiv.org/pdf/2511.00024", "abs": "https://arxiv.org/abs/2511.00024", "authors": ["Haotian Hang", "Yueyang Shen", "Vicky Zhu", "Jose Cruz", "Michelle Li"], "title": "Chitchat with AI: Understand the supply chain carbon disclosure of companies worldwide through Large Language Model", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG", "stat.AP"], "comment": null, "summary": "In the context of global sustainability mandates, corporate carbon disclosure\nhas emerged as a critical mechanism for aligning business strategy with\nenvironmental responsibility. The Carbon Disclosure Project (CDP) hosts the\nworld's largest longitudinal dataset of climate-related survey responses,\ncombining structured indicators with open-ended narratives, but the\nheterogeneity and free-form nature of these disclosures present significant\nanalytical challenges for benchmarking, compliance monitoring, and investment\nscreening. This paper proposes a novel decision-support framework that\nleverages large language models (LLMs) to assess corporate climate disclosure\nquality at scale. It develops a master rubric that harmonizes narrative scoring\nacross 11 years of CDP data (2010-2020), enabling cross-sector and\ncross-country benchmarking. By integrating rubric-guided scoring with\npercentile-based normalization, our method identifies temporal trends,\nstrategic alignment patterns, and inconsistencies in disclosure across\nindustries and regions. Results reveal that sectors such as technology and\ncountries like Germany consistently demonstrate higher rubric alignment, while\nothers exhibit volatility or superficial engagement, offering insights that\ninform key decision-making processes for investors, regulators, and corporate\nenvironmental, social, and governance (ESG) strategists. The proposed LLM-based\napproach transforms unstructured disclosures into quantifiable, interpretable,\ncomparable, and actionable intelligence, advancing the capabilities of\nAI-enabled decision support systems (DSSs) in the domain of climate governance."}
{"id": "2511.00086", "pdf": "https://arxiv.org/pdf/2511.00086", "abs": "https://arxiv.org/abs/2511.00086", "authors": ["Fali Wang", "Jihai Chen", "Shuhua Yang", "Runxue Bao", "Tianxiang Zhao", "Zhiwei Zhang", "Xianfeng Tang", "Hui Liu", "Qi He", "Suhang Wang"], "title": "Generalizing Test-time Compute-optimal Scaling as an Optimizable Graph", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.7"], "comment": "Under review", "summary": "Test-Time Scaling (TTS) improves large language models (LLMs) by allocating\nadditional computation during inference, typically through parallel,\nsequential, or hybrid scaling. However, prior studies often assume fixed\ncollaboration architectures (e.g., topologies) and single-model usage,\noverlooking that optimal architectures and model combinations can vary across\ntasks. Therefore, we study the novel problem of searching for compute-optimal\nmodel combinations and architectures in TTS under a fixed budget. We formalize\nit as a multi-LLM collaboration graph, where nodes encode roles and LLM model\nassignments, and edges capture information flow. This problem is challenging\nbecause (i) the combinatorial search space is prohibitively large, and (ii)\ntask-specific requirements demand tailored designs. To address these, we\nreformulate the problem as probabilistic graph optimization and, through pilot\nexperiments, derive three empirical insights into TTS collaboration graphs.\nGuided by these insights, we propose Agent-REINFORCE, an LLM-agent-augmented\nframework that mirrors the REINFORCE pipeline by mapping\nsampling-gradient-update to sampling-feedback-update, where feedback serves as\na textual gradient to update the probabilistic graph and efficiently search for\noptimal multi-LLM collaboration graphs. Experiments show that Agent-REINFORCE\noutperforms both traditional and LLM-based baselines in sample efficiency and\nsearch performance, and effectively identifies optimal graphs under joint\nobjectives of accuracy and inference latency."}
{"id": "2511.00092", "pdf": "https://arxiv.org/pdf/2511.00092", "abs": "https://arxiv.org/abs/2511.00092", "authors": ["Shunya Minami", "Tatsuya Ishigaki", "Ikko Hamamura", "Taku Mikuriya", "Youmi Ma", "Naoaki Okazaki", "Hiroya Takamura", "Yohichi Suzuki", "Tadashi Kadowaki"], "title": "QuantumBench: A Benchmark for Quantum Problem Solving", "categories": ["cs.AI", "cs.CL", "cs.LG", "quant-ph"], "comment": "11 pages, 8 figures", "summary": "Large language models are now integrated into many scientific workflows,\naccelerating data analysis, hypothesis generation, and design space\nexploration. In parallel with this growth, there is a growing need to carefully\nevaluate whether models accurately capture domain-specific knowledge and\nnotation, since general-purpose benchmarks rarely reflect these requirements.\nThis gap is especially clear in quantum science, which features non-intuitive\nphenomena and requires advanced mathematics. In this study, we introduce\nQuantumBench, a benchmark for the quantum domain that systematically examine\nhow well LLMs understand and can be applied to this non-intuitive field. Using\npublicly available materials, we compiled approximately 800 questions with\ntheir answers spanning nine areas related to quantum science and organized them\ninto an eight-option multiple-choice dataset. With this benchmark, we evaluate\nseveral existing LLMs and analyze their performance in the quantum domain,\nincluding sensitivity to changes in question format. QuantumBench is the first\nLLM evaluation dataset built for the quantum domain, and it is intended to\nguide the effective use of LLMs in quantum research."}
{"id": "2511.00106", "pdf": "https://arxiv.org/pdf/2511.00106", "abs": "https://arxiv.org/abs/2511.00106", "authors": ["Anuj Gupta", "Ann Shivers-McNair"], "title": "Wayfinding through the AI wilderness: Mapping rhetorics of ChatGPT prompt writing on X (formerly Twitter) to promote critical AI literacies", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "comment": "Published in the journal Computers and Composition, Issue 74 (2024)", "summary": "In this paper, we demonstrate how studying the rhetorics of ChatGPT prompt\nwriting on social media can promote critical AI literacies. Prompt writing is\nthe process of writing instructions for generative AI tools like ChatGPT to\nelicit desired outputs and there has been an upsurge of conversations about it\non social media. To study this rhetorical activity, we build on four\noverlapping traditions of digital writing research in computers and composition\nthat inform how we frame literacies, how we study social media rhetorics, how\nwe engage iteratively and reflexively with methodologies and technologies, and\nhow we blend computational methods with qualitative methods. Drawing on these\nfour traditions, our paper shows our iterative research process through which\nwe gathered and analyzed a dataset of 32,000 posts (formerly known as tweets)\nfrom X (formerly Twitter) about prompt writing posted between November 2022 to\nMay 2023. We present five themes about these emerging AI literacy practices:\n(1) areas of communication impacted by prompt writing, (2) micro-literacy\nresources shared for prompt writing, (3) market rhetoric shaping prompt\nwriting, (4) rhetorical characteristics of prompts, and (5) definitions of\nprompt writing. In discussing these themes and our methodologies, we highlight\ntakeaways for digital writing teachers and researchers who are teaching and\nanalyzing critical AI literacies."}
{"id": "2511.00118", "pdf": "https://arxiv.org/pdf/2511.00118", "abs": "https://arxiv.org/abs/2511.00118", "authors": ["Stanislav Selitskiy"], "title": "Real-time and Zero-footprint Bag of Synthetic Syllables Algorithm for E-mail Spam Detection Using Subject Line and Short Text Fields", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Contemporary e-mail services have high availability expectations from the\ncustomers and are resource-strained because of the high-volume throughput and\nspam attacks. Deep Machine Learning architectures, which are resource hungry\nand require off-line processing due to the long processing times, are not\nacceptable at the front line filters. On the other hand, the bulk of the\nincoming spam is not sophisticated enough to bypass even the simplest\nalgorithms. While the small fraction of the intelligent, highly mutable spam\ncan be detected only by the deep architectures, the stress on them can be\nunloaded by the simple near real-time and near zero-footprint algorithms such\nas the Bag of Synthetic Syllables algorithm applied to the short texts of the\ne-mail subject lines and other short text fields. The proposed algorithm\ncreates a circa 200 sparse dimensional hash or vector for each e-mail subject\nline that can be compared for the cosine or euclidean proximity distance to\nfind similarities to the known spammy subjects. The algorithm does not require\nany persistent storage, dictionaries, additional hardware upgrades or software\npackages. The performance of the algorithm is presented on the one day of the\nreal SMTP traffic."}
{"id": "2511.00177", "pdf": "https://arxiv.org/pdf/2511.00177", "abs": "https://arxiv.org/abs/2511.00177", "authors": ["Hiba Ahsan", "Byron C. Wallace"], "title": "Can SAEs reveal and mitigate racial biases of LLMs in healthcare?", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "LLMs are increasingly being used in healthcare. This promises to free\nphysicians from drudgery, enabling better care to be delivered at scale. But\nthe use of LLMs in this space also brings risks; for example, such models may\nworsen existing biases. How can we spot when LLMs are (spuriously) relying on\npatient race to inform predictions? In this work we assess the degree to which\nSparse Autoencoders (SAEs) can reveal (and control) associations the model has\nmade between race and stigmatizing concepts. We first identify SAE latents in\nGemma-2 models which appear to correlate with Black individuals. We find that\nthis latent activates on reasonable input sequences (e.g., \"African American\")\nbut also problematic words like \"incarceration\". We then show that we can use\nthis latent to steer models to generate outputs about Black patients, and\nfurther that this can induce problematic associations in model outputs as a\nresult. For example, activating the Black latent increases the risk assigned to\nthe probability that a patient will become \"belligerent\". We evaluate the\ndegree to which such steering via latents might be useful for mitigating bias.\nWe find that this offers improvements in simple settings, but is less\nsuccessful for more realistic and complex clinical tasks. Overall, our results\nsuggest that: SAEs may offer a useful tool in clinical applications of LLMs to\nidentify problematic reliance on demographics but mitigating bias via SAE\nsteering appears to be of marginal utility for realistic tasks."}
{"id": "2511.00206", "pdf": "https://arxiv.org/pdf/2511.00206", "abs": "https://arxiv.org/abs/2511.00206", "authors": ["Dirk U. Wulff", "Rui Mata"], "title": "Advancing Cognitive Science with LLMs", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Cognitive science faces ongoing challenges in knowledge synthesis and\nconceptual clarity, in part due to its multifaceted and interdisciplinary\nnature. Recent advances in artificial intelligence, particularly the\ndevelopment of large language models (LLMs), offer tools that may help to\naddress these issues. This review examines how LLMs can support areas where the\nfield has historically struggled, including establishing cross-disciplinary\nconnections, formalizing theories, developing clear measurement taxonomies,\nachieving generalizability through integrated modeling frameworks, and\ncapturing contextual and individual variation. We outline the current\ncapabilities and limitations of LLMs in these domains, including potential\npitfalls. Taken together, we conclude that LLMs can serve as tools for a more\nintegrative and cumulative cognitive science when used judiciously to\ncomplement, rather than replace, human expertise."}
{"id": "2511.00279", "pdf": "https://arxiv.org/pdf/2511.00279", "abs": "https://arxiv.org/abs/2511.00279", "authors": ["Meituan LongCat Team", "Bairui Wang", "Bayan", "Bin Xiao", "Bo Zhang", "Bolin Rong", "Borun Chen", "Chang Wan", "Chao Zhang", "Chen Huang", "Chen Chen", "Chen Chen", "Chengxu Yang", "Chengzuo Yang", "Cong Han", "Dandan Peng", "Delian Ruan", "Detai Xin", "Disong Wang", "Dongchao Yang", "Fanfan Liu", "Fengjiao Chen", "Fengyu Yang", "Gan Dong", "Gang Huang", "Gang Xu", "Guanglu Wan", "Guoqiang Tan", "Guoqiao Yu", "Haibo Qiu", "Hao Lu", "Hongbo Liu", "Hongyu Xiang", "Jiaheng Wu", "Jian Yang", "Jiaxing Liu", "Jing Huang", "Jingang Wang", "Jinrui Ding", "Juchao Jiang", "Jun Kuang", "Jun Wang", "Junhui Mei", "Ke Ding", "Kefeng Zhang", "Lei Chen", "Liang Shi", "Limeng Qiao", "Liming Zheng", "Lin Ma", "Liuyang Guo", "Liya Ma", "Luying Sun", "Man Gao", "Mengshen Zhu", "Miao Cao", "Minliang Lin", "Nuo Xu", "Peng Shi", "Qi Zhang", "Qian Fang", "Qian Wang", "Qian Yang", "Quanxiu Wang", "Rongxiang Weng", "Rongxin Guo", "Ruoxuan Liang", "Senbin Yang", "Shanbo Xu", "Shanglin Lei", "Shengze Ye", "Shimin Chen", "Shuaiqi Chen", "Shujie Hu", "Shuo Li", "Siqi Yang", "Siyu Xu", "Siyu Ren", "Song Li", "Songxiang Liu", "Tianhao Bai", "Tianye Dai", "Wei Hong", "Wei Wang", "Weixiao Zhao", "Wengang Cao", "Wenlong Zhu", "Wenlong He", "Xi Su", "Xi Nan", "Xiaohan Zhao", "Xiaohao Wang", "Xiaoyu Zhao", "Xiaoyu Wang", "Xiaoyu Li", "Xin Pan", "Xin Chen", "Xiusong Sun", "Xu Xiang", "Xudong Xing", "Xuezhi Cao", "Xunliang Cai", "Yang Yang", "Yanli Tan", "Yao Yao", "Yerui Sun", "Yi Chen", "Yifan Lu", "Yin Gong", "Yining Zhang", "Yitian Chen", "Yiyang Gan", "Yuchen Tang", "Yuchen Xie", "Yueqian Wang", "Yuewen Zheng", "Yufei Zhang", "Yufeng Zhong", "Yulei Qian", "Yuqi Peng", "Yuwei Jiang", "Zeyang Hu", "Zheng Zhang", "Zhengkun Tian", "Zhiqing Hong", "Zhixiong Zeng", "Zhuqi Mi", "Ziran Li", "Ziwen Wang", "Ziyi Zhao", "Ziyuan Zhuang", "Zizhe Zhao"], "title": "LongCat-Flash-Omni Technical Report", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.DC", "cs.LG", "cs.SD"], "comment": null, "summary": "We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modal\nmodel with 560 billion parameters, excelling at real-time audio-visual\ninteraction. By adopting a curriculum-inspired progressive training strategy\nthat transitions from simpler to increasingly complex modality sequence\nmodeling tasks, LongCat-Flash-Omni attains comprehensive multimodal\ncapabilities while maintaining strong unimodal capability. Building upon\nLongCat-Flash, which adopts a high-performance Shortcut-connected\nMixture-of-Experts (MoE) architecture with zero-computation experts,\nLongCat-Flash-Omni integrates efficient multimodal perception and speech\nreconstruction modules. Despite its immense size of 560B parameters (with 27B\nactivated), LongCat-Flash-Omni achieves low-latency real-time audio-visual\ninteraction. For training infrastructure, we developed a modality-decoupled\nparallelism scheme specifically designed to manage the data and model\nheterogeneity inherent in large-scale multimodal training. This innovative\napproach demonstrates exceptional efficiency by sustaining over 90% of the\nthroughput achieved by text-only training. Extensive evaluations show that\nLongCat-Flash-Omni achieves state-of-the-art performance on omni-modal\nbenchmarks among open-source models. Furthermore, it delivers highly\ncompetitive results across a wide range of modality-specific tasks, including\ntext, image, and video understanding, as well as audio understanding and\ngeneration. We provide a comprehensive overview of the model architecture\ndesign, training procedures, and data strategies, and open-source the model to\nfoster future research and development in the community."}
{"id": "2511.00280", "pdf": "https://arxiv.org/pdf/2511.00280", "abs": "https://arxiv.org/abs/2511.00280", "authors": ["Abhinav Joshi", "Areeb Ahmad", "Ashutosh Modi"], "title": "Calibration Across Layers: Understanding Calibration Evolution in LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted at EMNLP 2025 (main)", "summary": "Large Language Models (LLMs) have demonstrated inherent calibration\ncapabilities, where predicted probabilities align well with correctness,\ndespite prior findings that deep neural networks are often overconfident.\nRecent studies have linked this behavior to specific components in the final\nlayer, such as entropy neurons and the unembedding matrix null space. In this\nwork, we provide a complementary perspective by investigating how calibration\nevolves throughout the network depth. Analyzing multiple open-weight models on\nthe MMLU benchmark, we uncover a distinct confidence correction phase in the\nupper/later layers, where model confidence is actively recalibrated after\ndecision certainty has been reached. Furthermore, we identify a low-dimensional\ncalibration direction in the residual stream whose perturbation significantly\nimproves calibration metrics (ECE and MCE) without harming accuracy. Our\nfindings suggest that calibration is a distributed phenomenon, shaped\nthroughout the network forward pass, not just in its final projection,\nproviding new insights into how confidence-regulating mechanisms operate within\nLLMs."}
{"id": "2511.00351", "pdf": "https://arxiv.org/pdf/2511.00351", "abs": "https://arxiv.org/abs/2511.00351", "authors": ["Amir Ziashahabi", "Yavuz Faruk Bakman", "Duygu Nur Yaldiz", "Mostafa El-Khamy", "Sai Praneeth Karimireddy", "Salman Avestimehr"], "title": "Reject Only Critical Tokens: Pivot-Aware Speculative Decoding", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted at NeurIPS 2025 Efficient Reasoning Workshop", "summary": "Speculative Decoding (SD) ensures that the output matches the target model's\ndistribution exactly. However, we argue that this distribution matching\nrequirement is too stringent and results in unnecessarily low acceptance rates,\nlimiting potential speedups. Instead, we advocate a reformulation of the\ndecoding objective: the proposed decoding strategy should match the expected\nutility, i.e., the task-specific performance, of the target model. This\nperspective also aligns better with real-world use cases of LLMs, where utility\n(e.g., code correctness, factual accuracy) is often more important than\nsampling distribution. Based on this reformulation, we propose a novel decoding\nstrategy: Pivot-Aware Speculative Decoding, which rejects only those tokens\nthat would lead to a utility drop in the final output. We refer to these\ncritical tokens as pivot tokens. We propose a method for labeling tokens as\npivotal or non-pivotal and train a lightweight classifier to detect them. This\nmethod can be viewed as a relaxed version of standard SD, which offers much\nhigher acceptance while preserving utility. We evaluate our method across\nvarious datasets, demonstrating that we can achieve up to $2.5\\times$ speedup\nwith comparable utility. Source code is available at\nhttps://github.com/amir-zsh/PAD."}
{"id": "2511.00379", "pdf": "https://arxiv.org/pdf/2511.00379", "abs": "https://arxiv.org/abs/2511.00379", "authors": ["Jiahao Wang", "Songkai Xue", "Jinghui Li", "Xiaozhen Wang"], "title": "Diverse Human Value Alignment for Large Language Models via Ethical Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted by AIES 2025, camera-ready version", "summary": "Ensuring that Large Language Models (LLMs) align with the diverse and\nevolving human values across different regions and cultures remains a critical\nchallenge in AI ethics. Current alignment approaches often yield superficial\nconformity rather than genuine ethical understanding, failing to address the\ncomplex, context-dependent nature of human values. In this paper, we propose a\nnovel ethical reasoning paradigm for LLMs inspired by well-established ethical\ndecision-making models, aiming at enhancing diverse human value alignment\nthrough deliberative ethical reasoning. Our framework consists of a structured\nfive-step process, including contextual fact gathering, hierarchical social\nnorm identification, option generation, multiple-lens ethical impact analysis,\nand reflection. This theory-grounded approach guides LLMs through an\ninterpretable reasoning process that enhances their ability to understand\nregional specificities and perform nuanced ethical analysis, which can be\nimplemented with either prompt engineering or supervised fine-tuning methods.\nWe perform evaluations on the SafeWorld benchmark that specially designed for\nregional value alignment. Experimental results demonstrate our framework\nsignificantly improves LLM alignment with diverse human values compared to\nbaseline methods, enabling more accurate social norm identification and more\nculturally appropriate reasoning. Our work provides a concrete pathway toward\ndeveloping LLMs that align more effectively with the multifaceted values of\nglobal societies through interdisciplinary research."}
{"id": "2511.00488", "pdf": "https://arxiv.org/pdf/2511.00488", "abs": "https://arxiv.org/abs/2511.00488", "authors": ["Jun Gao", "Yun Peng", "Xiaoxue Ren"], "title": "\\texttt{ReMind}: Understanding Deductive Code Reasoning in LLMs", "categories": ["cs.PL", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable progress in\ncode-related tasks. Despite their advancement, empirical evidence reveals that\nthey still struggle with \\emph{deductive code reasoning}, the ability to reason\nabout the program execution process. While prior studies have recognized this\nlimitation, the underlying causes remain largely underexplored. In this paper,\nwe begin by presenting a comprehensive empirical study that reveals three key\nchallenges undermining deductive code reasoning: (1) an intrinsic gap between\ngeneration and reasoning abilities, (2) a consistent bias towards code sources,\nand (3) weak zero-shot generalization on complex benchmarks. In light of these\nchallenges, we propose \\texttt{ReMind}, a multi-agent framework composed of\n\\texttt{Mutator}, \\texttt{Executor}, and \\texttt{Inspector}. The\n\\texttt{Mutator} generates code variants to mitigate bias towards code sources,\nthe \\texttt{Executor} traces variable states step-by-step to expose\ninconsistency, and the \\texttt{Inspector} identifies problematic reasoning\nsteps and provides control-flow refinement to bridge the intrinsic reasoning\ngap. Through their coordinated collaboration, \\texttt{ReMind} systematically\nidentifies and refines reasoning flaws, achieving outstanding performance and\nenabling robust zero-shot generalization. Extensive experiments on two\nbenchmarks with five LLMs demonstrate the superior advantages of\n\\texttt{ReMind} compared to baseline approaches in deductive code reasoning."}
{"id": "2511.00521", "pdf": "https://arxiv.org/pdf/2511.00521", "abs": "https://arxiv.org/abs/2511.00521", "authors": ["Bao Nguyen", "Hieu Trung Nguyen", "Ruifeng She", "Xiaojin Fu", "Viet Anh Nguyen"], "title": "Reasoning Planning for Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "29 pages, 5 figures", "summary": "Selecting an appropriate reasoning method for a given query remains a key\nchallenge in language model generation. Existing approaches typically generate\nmultiple candidate responses and use an aggregation strategy to select the\noutput answer, often assuming that more candidate answers yield higher\naccuracy. We revisit this assumption through a rigorous theoretical analysis,\nderiving accuracy bounds for standard aggregation methods under fixed\ngeneration distributions and candidate sizes. Building on these insights, we\nintroduce EPIC, an Ensemble Planning with Contrastive learning framework to\nlearn a shared representation space that captures both model reasoning\nabilities and query-method compatibility. EPIC incorporates our probability\nbounds as a regularizer in a utility-driven optimization that balances accuracy\nand computational cost. Experiments on diverse mathematical reasoning tasks\nshow that EPIC consistently selects optimal reasoning methods, improving\naccuracy while reducing computational overhead. Our code can be found at\nhttps://github.com/nguyenngocbaocmt02/EPIC."}
{"id": "2511.00584", "pdf": "https://arxiv.org/pdf/2511.00584", "abs": "https://arxiv.org/abs/2511.00584", "authors": ["Ke Shi", "Yan Zhang", "Miao Zhang", "Lifan Chen", "Jiali Yi", "Kui Xiao", "Xiaoju Hou", "Zhifei Li"], "title": "Structurally Refined Graph Transformer for Multimodal Recommendation", "categories": ["cs.IR", "cs.CL"], "comment": "Comment: 13 pages, 7 figures, accepted by IEEE Transactions on\n  Multimedia 2025", "summary": "Multimodal recommendation systems utilize various types of information,\nincluding images and text, to enhance the effectiveness of recommendations. The\nkey challenge is predicting user purchasing behavior from the available data.\nCurrent recommendation models prioritize extracting multimodal information\nwhile neglecting the distinction between redundant and valuable data. They also\nrely heavily on a single semantic framework (e.g., local or global semantics),\nresulting in an incomplete or biased representation of user preferences,\nparticularly those less expressed in prior interactions. Furthermore, these\napproaches fail to capture the complex interactions between users and items,\nlimiting the model's ability to meet diverse users. To address these\nchallenges, we present SRGFormer, a structurally optimized multimodal\nrecommendation model. By modifying the transformer for better integration into\nour model, we capture the overall behavior patterns of users. Then, we enhance\nstructural information by embedding multimodal information into a hypergraph\nstructure to aid in learning the local structures between users and items.\nMeanwhile, applying self-supervised tasks to user-item collaborative signals\nenhances the integration of multimodal information, thereby revealing the\nrepresentational features inherent to the data's modality. Extensive\nexperiments on three public datasets reveal that SRGFormer surpasses previous\nbenchmark models, achieving an average performance improvement of 4.47 percent\non the Sports dataset. The code is publicly available online."}
{"id": "2511.00617", "pdf": "https://arxiv.org/pdf/2511.00617", "abs": "https://arxiv.org/abs/2511.00617", "authors": ["Eric Bigelow", "Daniel Wurgaft", "YingQiao Wang", "Noah Goodman", "Tomer Ullman", "Hidenori Tanaka", "Ekdeep Singh Lubana"], "title": "Belief Dynamics Reveal the Dual Nature of In-Context Learning and Activation Steering", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Large language models (LLMs) can be controlled at inference time through\nprompts (in-context learning) and internal activations (activation steering).\nDifferent accounts have been proposed to explain these methods, yet their\ncommon goal of controlling model behavior raises the question of whether these\nseemingly disparate methodologies can be seen as specific instances of a\nbroader framework. Motivated by this, we develop a unifying, predictive account\nof LLM control from a Bayesian perspective. Specifically, we posit that both\ncontext- and activation-based interventions impact model behavior by altering\nits belief in latent concepts: steering operates by changing concept priors,\nwhile in-context learning leads to an accumulation of evidence. This results in\na closed-form Bayesian model that is highly predictive of LLM behavior across\ncontext- and activation-based interventions in a set of domains inspired by\nprior work on many-shot in-context learning. This model helps us explain prior\nempirical phenomena - e.g., sigmoidal learning curves as in-context evidence\naccumulates - while predicting novel ones - e.g., additivity of both\ninterventions in log-belief space, which results in distinct phases such that\nsudden and dramatic behavioral shifts can be induced by slightly changing\nintervention controls. Taken together, this work offers a unified account of\nprompt-based and activation-based control of LLM behavior, and a methodology\nfor empirically predicting the effects of these interventions."}
{"id": "2511.00640", "pdf": "https://arxiv.org/pdf/2511.00640", "abs": "https://arxiv.org/abs/2511.00640", "authors": ["Zicheng Xu", "Guanchu Wang", "Yu-Neng Chuang", "Guangyao Zheng", "Alexander S. Szalay", "Zirui Liu", "Vladimir Braverman"], "title": "DTS: Enhancing Large Reasoning Models via Decoding Tree Sketching", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Reasoning Models (LRMs) demonstrate strong performance on complex\nreasoning tasks, yet they often suffer from overthinking, producing excessively\nlong chain-of-thought (CoT) traces that increase inference cost and may degrade\naccuracy. Our analysis reveals a clear anti-correlation between reasoning\nlength and accuracy, where across multiple stochastic decodes, the short\nreasoning paths consistently achieve the highest correctness, while longer ones\naccumulate errors and repetitions. These short optimal reasoning paths can be\nfound ideally through full enumeration of the reasoning space. However, the\ntree-structured reasoning space grows exponentially with sequence length,\nrendering exhaustive exploration infeasible. To address this, we propose DTS, a\nmodel-agnostic decoding framework that sketches the reasoning space by\nselectively branching at high-entropy tokens and applies early stopping to\nselect the shortest completed reasoning path. This approach approximates the\noptimal solution that enhances both efficiency and accuracy, without requiring\nadditional training or supervision. Experiments on AIME2024 and AIME2025\ndatasets with DeepSeek-R1-Distill-Qwen-7B and 1.5B show that DTS improves\naccuracy by up to 8%, reduces average reasoning length by 23%, and decreases\nrepetition frequency by 12%, demonstrating DTS's ability for scalable and\nefficient LRM reasoning."}
{"id": "2511.00651", "pdf": "https://arxiv.org/pdf/2511.00651", "abs": "https://arxiv.org/abs/2511.00651", "authors": ["Chenhua Shi", "Bhavika Jalli", "Gregor Macdonald", "John Zou", "Wanlu Lei", "Mridul Jain", "Joji Philip"], "title": "Leveraging Multi-Agent System (MAS) and Fine-Tuned Small Language Models (SLMs) for Automated Telecom Network Troubleshooting", "categories": ["cs.AI", "cs.CL", "cs.IT", "cs.MA", "cs.NI", "math.IT"], "comment": "6 pages, 7 figures, 1 table", "summary": "Telecom networks are rapidly growing in scale and complexity, making\neffective management, operation, and optimization increasingly challenging.\nAlthough Artificial Intelligence (AI) has been applied to many telecom tasks,\nexisting models are often narrow in scope, require large amounts of labeled\ndata, and struggle to generalize across heterogeneous deployments.\nConsequently, network troubleshooting continues to rely heavily on Subject\nMatter Experts (SMEs) to manually correlate various data sources to identify\nroot causes and corrective actions. To address these limitations, we propose a\nMulti-Agent System (MAS) that employs an agentic workflow, with Large Language\nModels (LLMs) coordinating multiple specialized tools for fully automated\nnetwork troubleshooting. Once faults are detected by AI/ML-based monitors, the\nframework dynamically activates agents such as an orchestrator, solution\nplanner, executor, data retriever, and root-cause analyzer to diagnose issues\nand recommend remediation strategies within a short time frame. A key component\nof this system is the solution planner, which generates appropriate remediation\nplans based on internal documentation. To enable this, we fine-tuned a Small\nLanguage Model (SLM) on proprietary troubleshooting documents to produce\ndomain-grounded solution plans. Experimental results demonstrate that the\nproposed framework significantly accelerates troubleshooting automation across\nboth Radio Access Network (RAN) and Core network domains."}
{"id": "2511.00749", "pdf": "https://arxiv.org/pdf/2511.00749", "abs": "https://arxiv.org/abs/2511.00749", "authors": ["Tanvi Dinkar", "Aiqi Jiang", "Gavin Abercrombie", "Ioannis Konstas"], "title": "Erasing 'Ugly' from the Internet: Propagation of the Beauty Myth in Text-Image Models", "categories": ["cs.CV", "cs.CL"], "comment": "This is a preprint under review", "summary": "Social media has exacerbated the promotion of Western beauty norms, leading\nto negative self-image, particularly in women and girls, and causing harm such\nas body dysmorphia. Increasingly content on the internet has been artificially\ngenerated, leading to concerns that these norms are being exaggerated. The aim\nof this work is to study how generative AI models may encode 'beauty' and erase\n'ugliness', and discuss the implications of this for society. To investigate\nthese aims, we create two image generation pipelines: a text-to-image model and\na text-to-language model-to image model. We develop a structured beauty\ntaxonomy which we use to prompt three language models (LMs) and two\ntext-to-image models to cumulatively generate 5984 images using our two\npipelines. We then recruit women and non-binary social media users to evaluate\n1200 of the images through a Likert-scale within-subjects study. Participants\nshow high agreement in their ratings. Our results show that 86.5% of generated\nimages depicted people with lighter skin tones, 22% contained explicit content\ndespite Safe for Work (SFW) training, and 74% were rated as being in a younger\nage demographic. In particular, the images of non-binary individuals were rated\nas both younger and more hypersexualised, indicating troubling intersectional\neffects. Notably, prompts encoded with 'negative' or 'ugly' beauty traits (such\nas \"a wide nose\") consistently produced higher Not SFW (NSFW) ratings\nregardless of gender. This work sheds light on the pervasive demographic biases\nrelated to beauty standards present in generative AI models -- biases that are\nactively perpetuated by model developers, such as via negative prompting. We\nconclude by discussing the implications of this on society, which include\npollution of the data streams and active erasure of features that do not fall\ninside the stereotype of what is considered beautiful by developers."}
{"id": "2511.00751", "pdf": "https://arxiv.org/pdf/2511.00751", "abs": "https://arxiv.org/abs/2511.00751", "authors": ["Chiyan Loo"], "title": "Reevaluating Self-Consistency Scaling in Multi-Agent Systems", "categories": ["cs.AI", "cs.CL"], "comment": "7 pages, 3 figures", "summary": "This study examines the trade-offs of increasing sampled reasoning paths in\nself-consistency for modern large language models (LLMs). Earlier research with\nolder models showed that combining multiple reasoning chains improves results\nbefore reaching a plateau. Using Gemini 2.5 models on HotpotQA and Math-500, we\nrevisit those claims under current model conditions. Each configuration pooled\noutputs from varying sampled reasoning paths and compared them to a single\nchain-of-thought (CoT) baseline. Larger models exhibited a more stable and\nconsistent improvement curve. The results confirm that performance gains taper\noff after moderate sampling, aligning with past findings. This plateau suggests\ndiminishing returns driven by overlap among reasoning paths. Self-consistency\nremains useful, but high-sample configurations offer little benefit relative to\ntheir computational cost."}
{"id": "2511.00802", "pdf": "https://arxiv.org/pdf/2511.00802", "abs": "https://arxiv.org/abs/2511.00802", "authors": ["Jie JW Wu", "Ayanda Patrick Herlihy", "Ahmad Saleem Mirza", "Ali Afoud", "Fatemeh Fard"], "title": "GrowthHacker: Automated Off-Policy Evaluation Optimization Using Code-Modifying LLM Agents", "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": null, "summary": "With the software industry shifting toward a data-driven culture, online A/B\ntesting is a key tool for evaluating new technologies. However, deploying such\nexperiments requires substantial resources, may negatively impact users, and\ninvolves long data collection periods. To address this, \\textit{off-policy\nevaluation (OPE)}, or offline A/B testing, uses logged data to assess\ntechnologies and is fundamental in Reinforcement Learning, making it crucial in\ndomains where online testing is costly or risky, such as healthcare,\nrecommender systems, education, dialog systems, and robotics. Despite advances\nin coding LLMs and agentic AI, little is known about leveraging them to\noptimize OPE results. We investigate whether LLMs and LLM-based agents can\nimprove OPE performance via code optimization. We propose\n\\textit{GrowthHacker}, a benchmark with agent and baseline methods on\nlarge-scale real-world datasets, which iteratively optimizes code, evaluates\nresults, and begins new optimization cycles. We collected datasets, established\nprotocols, implemented baselines for OPE on the Open Bandit Pipeline\n(OBP)~\\cite{saito2021openbanditdatasetpipeline} and\nScope-RL~\\cite{kiyohara2023scope}, and developed the \\textit{two_agent}\nframework, which reduces system complexity while preserving optimization\neffectiveness. Results show the two_agent framework achieves 100% reliability\nand the highest average improvement of 106.7% among positive outcomes. Both\ntwo_agent and CrewAI reach 45% success rates, outperforming AutoGen's 34%.\nThese findings demonstrate the feasibility of LLM-based agents as automated\n\"growth hackers\" to enhance OPE systems, with implications for scaling\ndata-driven decision-making in production."}
{"id": "2511.00810", "pdf": "https://arxiv.org/pdf/2511.00810", "abs": "https://arxiv.org/abs/2511.00810", "authors": ["Shijie Zhou", "Viet Dac Lai", "Hao Tan", "Jihyung Kil", "Wanrong Zhu", "Changyou Chen", "Ruiyi Zhang"], "title": "GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "Graphical user interface (GUI) grounding is a key function of computer-use\nagents, which maps natural-language instructions to actionable screen regions.\nExisting approaches based on Multimodal Large Language Models (MLLMs) typically\nformulate it as a text-based coordinate generation task, yet directly\ngenerating precise coordinates from visual inputs remains challenging and\ncomputationally intensive. An intuitive way to implement GUI grounding is to\nfirst select visual patches relevant to the instructions and then determine the\nprecise click location within those patches. Based on the observations that\ngeneral MLLMs have some native grounding capability, nested within their\nattentions, we propose GUI-AIMA, an attention-based and coordinate-free\nsupervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns\nthe intrinsic multimodal attention of MLLMs with patch-wise grounding signals.\nThese signals are calculated adaptively for diverse user instructions by\nmulti-head aggregation on simplified query-visual attention matrices. Besides,\nits coordinate-free manner can easily integrate a plug-and-play zoom-in stage.\nGUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional\ndata efficiency and verifying that light training can trigger the native\ngrounding capability of MLLMs. It achieves state-of-the-art performance among\n3B models, attaining an average accuracy of 58.6% on ScreenSpot-Pro and 62.2%\non OSWorld-G. Project page: https://github.com/sjz5202/GUI-AIMA"}
{"id": "2511.00850", "pdf": "https://arxiv.org/pdf/2511.00850", "abs": "https://arxiv.org/abs/2511.00850", "authors": ["Yayue Deng", "Guoqiang Hu", "Haiyang Sun", "Xiangyu Zhang", "Haoyang Zhang", "Fei Tian", "Xuerui Yang", "Gang Yu", "Eng Siong Chng"], "title": "MULTI-Bench: A Multi-Turn Interactive Benchmark for Assessing Emotional Intelligence ability of Spoken Dialogue Models", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "Submitted to ICASSP 2026", "summary": "Spoken Dialogue Models (SDMs) have advanced rapidly, yet their ability to\nsustain genuinely interactive multi-turn conversations remains underexplored,\nas most benchmarks focus on single-turn exchanges. We introduce Multi-Bench,\nthe first benchmark explicitly designed to evaluate SDMs in multi-turn\ninteractive dialogue with an emphasis on emotional intelligence. Multi-Bench\nemploys a hierarchical structure with a basic track for emotion understanding\nand reasoning and an advanced track for emotion support and application. It\ncomprises five carefully designed tasks and about 3.2K samples, ranging from\nemotion recognition to complex reasoning and interactive dialogue, supported by\na reproducible evaluation framework. We evaluate six representative SDMs on\neight subsets of Multi-Bench. Results show that while current SDMs achieve good\nperformance on basic understanding tasks, they still have room for improvement\nin advanced multi-turn interactive dialogue and reasoning-related tasks,\nparticularly in emotion awareness and application."}
{"id": "2511.00926", "pdf": "https://arxiv.org/pdf/2511.00926", "abs": "https://arxiv.org/abs/2511.00926", "authors": ["Kyung-Hoon Kim"], "title": "LLMs Position Themselves as More Rational Than Humans: Emergence of AI Self-Awareness Measured Through Game Theory", "categories": ["cs.AI", "cs.CL"], "comment": "19 pages, 6 figures, 28 models tested across 4,200 trials", "summary": "As Large Language Models (LLMs) grow in capability, do they develop\nself-awareness as an emergent behavior? And if so, can we measure it? We\nintroduce the AI Self-Awareness Index (AISAI), a game-theoretic framework for\nmeasuring self-awareness through strategic differentiation. Using the \"Guess\n2/3 of Average\" game, we test 28 models (OpenAI, Anthropic, Google) across\n4,200 trials with three opponent framings: (A) against humans, (B) against\nother AI models, and (C) against AI models like you. We operationalize\nself-awareness as the capacity to differentiate strategic reasoning based on\nopponent type. Finding 1: Self-awareness emerges with model advancement. The\nmajority of advanced models (21/28, 75%) demonstrate clear self-awareness,\nwhile older/smaller models show no differentiation. Finding 2: Self-aware\nmodels rank themselves as most rational. Among the 21 models with\nself-awareness, a consistent rationality hierarchy emerges: Self > Other AIs >\nHumans, with large AI attribution effects and moderate self-preferencing. These\nfindings reveal that self-awareness is an emergent capability of advanced LLMs,\nand that self-aware models systematically perceive themselves as more rational\nthan humans. This has implications for AI alignment, human-AI collaboration,\nand understanding AI beliefs about human capabilities."}
{"id": "2511.00985", "pdf": "https://arxiv.org/pdf/2511.00985", "abs": "https://arxiv.org/abs/2511.00985", "authors": ["Yiwen Jiao", "Tonghui Ren", "Yuche Gao", "Zhenying He", "Yinan Jing", "Kai Zhang", "X. Sean Wang"], "title": "ORANGE: An Online Reflection ANd GEneration framework with Domain Knowledge for Text-to-SQL", "categories": ["cs.DB", "cs.AI", "cs.CL"], "comment": "16 pages, 4 figures, preprint", "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in\ntranslating natural language to SQL, but a significant semantic gap persists\nbetween their general knowledge and domain-specific semantics of databases.\nHistorical translation logs constitute a rich source of this missing in-domain\nknowledge, where SQL queries inherently encapsulate real-world usage patterns\nof database schema. Existing methods primarily enhance the reasoning process\nfor individual translations but fail to accumulate in-domain knowledge from\npast translations. We introduce ORANGE, an online self-evolutionary framework\nthat constructs database-specific knowledge bases by parsing SQL queries from\ntranslation logs. By accumulating in-domain knowledge that contains schema and\ndata semantics, ORANGE progressively reduces the semantic gap and enhances the\naccuracy of subsequent SQL translations. To ensure reliability, we propose a\nnovel nested Chain-of-Thought SQL-to-Text strategy with tuple-semantic\ntracking, which reduces semantic errors during knowledge generation.\nExperiments on multiple benchmarks confirm the practicality of ORANGE,\ndemonstrating its effectiveness for real-world Text-to-SQL deployment,\nparticularly in handling complex and domain-specific queries."}
{"id": "2511.01033", "pdf": "https://arxiv.org/pdf/2511.01033", "abs": "https://arxiv.org/abs/2511.01033", "authors": ["Tiberiu Musat", "Tiago Pimentel", "Lorenzo Noci", "Alessandro Stolfo", "Mrinmaya Sachan", "Thomas Hofmann"], "title": "On the Emergence of Induction Heads for In-Context Learning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Transformers have become the dominant architecture for natural language\nprocessing. Part of their success is owed to a remarkable capability known as\nin-context learning (ICL): they can acquire and apply novel associations solely\nfrom their input context, without any updates to their weights. In this work,\nwe study the emergence of induction heads, a previously identified mechanism in\ntwo-layer transformers that is particularly important for in-context learning.\nWe uncover a relatively simple and interpretable structure of the weight\nmatrices implementing the induction head. We theoretically explain the origin\nof this structure using a minimal ICL task formulation and a modified\ntransformer architecture. We give a formal proof that the training dynamics\nremain constrained to a 19-dimensional subspace of the parameter space.\nEmpirically, we validate this constraint while observing that only 3 dimensions\naccount for the emergence of an induction head. By further studying the\ntraining dynamics inside this 3-dimensional subspace, we find that the time\nuntil the emergence of an induction head follows a tight asymptotic bound that\nis quadratic in the input context length."}
{"id": "2511.01104", "pdf": "https://arxiv.org/pdf/2511.01104", "abs": "https://arxiv.org/abs/2511.01104", "authors": ["Yujian Liu", "Jiabao Ji", "Yang Zhang", "Wenbo Guo", "Tommi Jaakkola", "Shiyu Chang"], "title": "HarnessLLM: Automatic Testing Harness Generation via Reinforcement Learning", "categories": ["cs.SE", "cs.CL"], "comment": null, "summary": "Existing LLM-based automatic test generation methods mainly produce input and\nexpected output pairs to categorize the intended behavior of correct programs.\nAlthough straightforward, these methods have limited diversity in generated\ntests and cannot provide enough debugging information. We propose HarnessLLM, a\ntwo-stage training pipeline that enables LLMs to write harness code for\ntesting. Particularly, LLMs generate code that synthesizes inputs and validates\nthe observed outputs, allowing complex test cases and flexible output\nvalidation such as invariant checking. To achieve this, we train LLMs with SFT\nfollowed by RLVR with a customized reward design. Experiments show that\nHarnessLLM outperforms input-output-based testing in bug finding and testing\nstrategy diversity. HarnessLLM further benefits the code generation performance\nthrough test-time scaling with our generated test cases as inference-phase\nvalidation. Our code is available at\nhttps://github.com/UCSB-NLP-Chang/HarnessLLM.git."}
{"id": "2511.01113", "pdf": "https://arxiv.org/pdf/2511.01113", "abs": "https://arxiv.org/abs/2511.01113", "authors": ["Sebastian Kempf", "Frank Puppe"], "title": "S2Doc - Spatial-Semantic Document Format", "categories": ["cs.DL", "cs.CL", "H.3.7; I.7.5; I.7.2"], "comment": "8 pages, 2 figures, submitted to LREC2026", "summary": "Documents are a common way to store and share information, with tables being\nan important part of many documents. However, there is no real common\nunderstanding of how to model documents and tables in particular. Because of\nthis lack of standardization, most scientific approaches have their own way of\nmodeling documents and tables, leading to a variety of different data\nstructures and formats that are not directly compatible. Furthermore, most data\nmodels focus on either the spatial or the semantic structure of a document,\nneglecting the other aspect. To address this, we developed S2Doc, a flexible\ndata structure for modeling documents and tables that combines both spatial and\nsemantic information in a single format. It is designed to be easily extendable\nto new tasks and supports most modeling approaches for documents and tables,\nincluding multi-page documents. To the best of our knowledge, it is the first\napproach of its kind to combine all these aspects in a single format."}
{"id": "2511.01203", "pdf": "https://arxiv.org/pdf/2511.01203", "abs": "https://arxiv.org/abs/2511.01203", "authors": ["Pavel Rumiantsev", "Soumyasundar Pal", "Yingxue Zhang", "Mark Coates"], "title": "FEval-TTC: Fair Evaluation Protocol for Test-Time Compute", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The performance of Large Language Models (LLMs) and the associated dollar\ncosts of API calls can fluctuate over time, potentially invalidating\nconclusions drawn in prior research. To address this, we propose a Fair\nEvaluation protocol for Test-Time Compute (FEval-TTC), designed to ensure\nconsistent assessment of test-time compute (TTC) methods, regardless of such\nfluctuations. FEval-TTC focuses on the evaluation of TTC methods that utilize\nunderlying Chains-of-Thought (CoT). It supports evaluations across multiple\nLLMs on a diverse set of mathematical and commonsense reasoning datasets. The\nfew-shot prompting and answer extraction processes are standardized across\ndatasets, reducing both time and monetary overhead for researchers.\nFurthermore, we provide a cost modelling procedure that estimates both the\ntoken and dollar cost per query, facilitating equitable comparisons of\nprevalent TTC methods. We open-source FEval-TTC for public use at\nhttps://github.com/networkslab/feval_ttc ."}
{"id": "2511.01211", "pdf": "https://arxiv.org/pdf/2511.01211", "abs": "https://arxiv.org/abs/2511.01211", "authors": ["Chaofeng Wu"], "title": "Novelty and Impact of Economics Papers", "categories": ["econ.GN", "cs.CE", "cs.CL", "cs.DL", "q-fin.EC"], "comment": null, "summary": "We propose a framework that recasts scientific novelty not as a single\nattribute of a paper, but as a reflection of its position within the evolving\nintellectual landscape. We decompose this position into two orthogonal\ndimensions: \\textit{spatial novelty}, which measures a paper's intellectual\ndistinctiveness from its neighbors, and \\textit{temporal novelty}, which\ncaptures its engagement with a dynamic research frontier. To operationalize\nthese concepts, we leverage Large Language Models to develop semantic isolation\nmetrics that quantify a paper's location relative to the full-text literature.\nApplying this framework to a large corpus of economics articles, we uncover a\nfundamental trade-off: these two dimensions predict systematically different\noutcomes. Temporal novelty primarily predicts citation counts, whereas spatial\nnovelty predicts disruptive impact. This distinction allows us to construct a\ntypology of semantic neighborhoods, identifying four archetypes associated with\ndistinct and predictable impact profiles. Our findings demonstrate that novelty\ncan be understood as a multidimensional construct whose different forms,\nreflecting a paper's strategic location, have measurable and fundamentally\ndistinct consequences for scientific progress."}
{"id": "2511.01340", "pdf": "https://arxiv.org/pdf/2511.01340", "abs": "https://arxiv.org/abs/2511.01340", "authors": ["Trishanu Das", "Abhilash Nandy", "Khush Bajaj", "Deepiha S"], "title": "$\\left|\\,\\circlearrowright\\,\\boxed{\\text{BUS}}\\,\\right|$: A Large and Diverse Multimodal Benchmark for evaluating the ability of Vision-Language Models to understand Rebus Puzzles", "categories": ["cs.CV", "cs.CL"], "comment": "7 pages, 5 figures, 4 tables", "summary": "Understanding Rebus Puzzles (Rebus Puzzles use pictures, symbols, and letters\nto represent words or phrases creatively) requires a variety of skills such as\nimage recognition, cognitive skills, commonsense reasoning, multi-step\nreasoning, image-based wordplay, etc., making this a challenging task for even\ncurrent Vision-Language Models. In this paper, we present\n$\\left|\\,\\circlearrowright\\,\\boxed{\\text{BUS}}\\,\\right|$, a large and diverse\nbenchmark of $1,333$ English Rebus Puzzles containing different artistic styles\nand levels of difficulty, spread across 18 categories such as food, idioms,\nsports, finance, entertainment, etc. We also propose $RebusDescProgICE$, a\nmodel-agnostic framework which uses a combination of an unstructured\ndescription and code-based, structured reasoning, along with better,\nreasoning-based in-context example selection, improving the performance of\nVision-Language Models on\n$\\left|\\,\\circlearrowright\\,\\boxed{\\text{BUS}}\\,\\right|$ by $2.1-4.1\\%$ and\n$20-30\\%$ using closed-source and open-source models respectively compared to\nChain-of-Thought Reasoning."}
{"id": "2511.01529", "pdf": "https://arxiv.org/pdf/2511.01529", "abs": "https://arxiv.org/abs/2511.01529", "authors": ["Murali Sridharan", "Mikel Robredo", "Leevi Rantala", "Matteo Esposito", "Valentina Lenarduzzi", "Mika Mantyla"], "title": "Hidden in Plain Sight: Where Developers Confess Self-Admitted Technical Debt", "categories": ["cs.SE", "cs.CL", "cs.PL"], "comment": null, "summary": "Context. Detecting Self-Admitted Technical Debt (SATD) is crucial for\nproactive software maintenance. Previous research has primarily targeted\ndetecting and prioritizing SATD, with little focus on the source code afflicted\nwith SATD. Our goal in this work is to connect the SATD comments with source\ncode constructs that surround them.\n  Method. We leverage the extensive SATD dataset PENTACET, containing code\ncomments from over 9000 Java Open Source Software (OSS) repositories. We\nquantitatively infer where SATD most commonly occurs and which code\nconstructs/statements it most frequently affects.\n  Results and Conclusions. Our large-scale study links over 225,000 SATD\ncomments to their surrounding code, showing that SATD mainly arises in inline\ncode near definitions, conditionals, and exception handling, where developers\nface uncertainty and trade-offs, revealing it as an intentional signal of\nawareness during change rather than mere neglect."}
{"id": "2511.01618", "pdf": "https://arxiv.org/pdf/2511.01618", "abs": "https://arxiv.org/abs/2511.01618", "authors": ["Xiaoyu Zhan", "Wenxuan Huang", "Hao Sun", "Xinyu Fu", "Changfeng Ma", "Shaosheng Cao", "Bohan Jia", "Shaohui Lin", "Zhenfei Yin", "Lei Bai", "Wanli Ouyang", "Yuanqi Li", "Jie Guo", "Yanwen Guo"], "title": "Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly improved 2D visual understanding, prompting interest in their\napplication to complex 3D reasoning tasks. However, it remains unclear whether\nthese models can effectively capture the detailed spatial information required\nfor robust real-world performance, especially cross-view consistency, a key\nrequirement for accurate 3D reasoning. Considering this issue, we introduce\nViewpoint Learning, a task designed to evaluate and improve the spatial\nreasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,\nconsisting of 100K object-centric image pairs with diverse viewpoints and\ncorresponding question-answer pairs. Our approach employs a two-stage\nfine-tuning strategy: first, foundational knowledge is injected to the baseline\nMLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting in\nsignificant improvements across multiple tasks; second, generalization is\nenhanced through Reinforcement Learning using the Group Relative Policy\nOptimization (GRPO) algorithm on a broader set of questions. Additionally, we\nintroduce a hybrid cold-start initialization method designed to simultaneously\nlearn viewpoint representations and maintain coherent reasoning thinking.\nExperimental results show that our approach significantly activates the spatial\nreasoning ability of MLLM, improving performance on both in-domain and\nout-of-domain reasoning tasks. Our findings highlight the value of developing\nfoundational spatial skills in MLLMs, supporting future progress in robotics,\nautonomous systems, and 3D scene understanding."}
{"id": "2511.01734", "pdf": "https://arxiv.org/pdf/2511.01734", "abs": "https://arxiv.org/abs/2511.01734", "authors": ["Soufiane Hayou"], "title": "A Proof of Learning Rate Transfer under $μ$P", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.LG"], "comment": "23 pages", "summary": "We provide the first proof of learning rate transfer with width in a linear\nmulti-layer perceptron (MLP) parametrized with $\\mu$P, a neural network\nparameterization designed to ``maximize'' feature learning in the\ninfinite-width limit. We show that under $\\mu P$, the optimal learning rate\nconverges to a \\emph{non-zero constant} as width goes to infinity, providing a\ntheoretical explanation to learning rate transfer. In contrast, we show that\nthis property fails to hold under alternative parametrizations such as Standard\nParametrization (SP) and Neural Tangent Parametrization (NTP). We provide\nintuitive proofs and support the theoretical findings with extensive empirical\nresults."}
{"id": "2511.01758", "pdf": "https://arxiv.org/pdf/2511.01758", "abs": "https://arxiv.org/abs/2511.01758", "authors": ["Mian Wu", "Gavin Zhang", "Sewon Min", "Sergey Levine", "Aviral Kumar"], "title": "RLAC: Reinforcement Learning with Adversarial Critic for Free-Form Generation Tasks", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Project page: https://mianwu01.github.io/RLAC_website/", "summary": "Open-ended generation tasks require outputs to satisfy diverse and often\nimplicit task-specific evaluation rubrics. The sheer number of relevant rubrics\nleads to prohibitively high verification costs and incomplete assessments of a\nresponse, making reinforcement learning (RL) post-training with rubric-based\nrewards difficult to scale. This problem is exacerbated by the fact that often\nthe best way to combine these rubrics into one single reward is also highly\nprompt-specific. We propose Reinforcement Learning with Adversarial Critic\n(RLAC), a post-training approach that addresses these challenges via dynamic\nrubric verification. Our approach employs a large language model (LLM) as a\ncritic that dynamically identifies only the most likely failure modes (e.g., a\nfactual error or unhandled edge case), which are then verified by an external\nvalidator to optimize both generator and critic jointly. By training both the\ngenerator and the critic, this game enhances the critic's error detection and\nthe generator's output quality while reducing required verifications. Our\nexperiments demonstrate that RLAC improves factual accuracy in text generation\nand correctness in code generation, while also outperforming exhaustive\nverification and reward model methods. We show that dynamic critics are more\neffective than fixed critics, showcasing the potential of RLAC for scaling RL\npost-training to free-form generation tasks."}
{"id": "2511.01794", "pdf": "https://arxiv.org/pdf/2511.01794", "abs": "https://arxiv.org/abs/2511.01794", "authors": ["Vi Retault", "Yohaï-Eliel Berreby"], "title": "Random Initialization of Gated Sparse Adapters", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "13 pages (8 main), 6 figures (4 main). Accepted by NewInML workshop @\n  ICML 2025 on June 27, 2025", "summary": "When fine-tuning language models on new tasks, catastrophic forgetting --\nperformance degradation on previously-learned tasks -- is a ubiquitous problem.\nWhile Parameter-Efficient Fine-Tuning (PEFT) methods like LoRA address this\nthrough low-rank adapters, sparse adaptation offers an alternative that doesn't\nimpose rank constraints. We introduce Random Initialization of Gated Sparse\nAdapters (RIGSA), which starts from randomly-initialized full-rank adapters,\ngates them with a ReZero analog, and sparsifies them with iterative magnitude\npruning. We evaluate RIGSA on SmolLM2-1.7B-Instruct using a novel\nvision-in-text task (Textual MNIST) and measure forgetting on PIQA, HellaSwag,\nand GSM8k. SmolLM2-1.7B-Instruct initially performs around chance level on\nTextual MNIST, and is capable of learning the task through RIGSA, 4-bit QLoRA\nand random masking. In spite of having more trainable parameters than QLoRA,\nthe RIGSA configurations that we studied displayed less forgetting than QLoRA,\nparticularly on GSM8k, though it performs comparably to random masking."}
