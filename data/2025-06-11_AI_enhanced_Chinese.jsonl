{"id": "2506.08043", "pdf": "https://arxiv.org/pdf/2506.08043", "abs": "https://arxiv.org/abs/2506.08043", "authors": ["Ashkan Shahbazi", "Kyvia Pereira", "Jon S. Heiselman", "Elaheh Akbari", "Annie C. Benson", "Sepehr Seifi", "Xinyuan Liu", "Garrison L. Johnston", "Erwin Terpstra", "Anne Draaisma", "Jan-Jaap Severes", "Jie Ying Wu", "Nabil Simaan", "Michael L. Miga", "Soheil Kolouri"], "title": "Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers", "categories": ["cs.GR", "cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Fast and accurate simulation of soft tissue deformation is a critical factor\nfor surgical robotics and medical training. In this paper, we introduce a novel\nphysics-informed neural simulator that approximates soft tissue deformations in\na realistic and real-time manner. Our framework integrates Kelvinlet-based\npriors into neural simulators, making it the first approach to leverage\nKelvinlets for residual learning and regularization in data-driven soft tissue\nmodeling. By incorporating large-scale Finite Element Method (FEM) simulations\nof both linear and nonlinear soft tissue responses, our method improves neural\nnetwork predictions across diverse architectures, enhancing accuracy and\nphysical consistency while maintaining low latency for real-time performance.\nWe demonstrate the effectiveness of our approach by performing accurate\nsurgical maneuvers that simulate the use of standard laparoscopic tissue\ngrasping tools with high fidelity. These results establish Kelvinlet-augmented\nlearning as a powerful and efficient strategy for real-time, physics-aware soft\ntissue simulation in surgical applications.", "AI": {"tldr": "\u63d0\u51fa\u878d\u5408Kelvinlet\u5148\u9a8c\u7684\u795e\u7ecf\u6a21\u62df\u5668\uff0c\u5b9e\u73b0\u624b\u672f\u5e94\u7528\u4e2d\u5b9e\u65f6\u3001\u7269\u7406\u7cbe\u786e\u7684\u8f6f\u7ec4\u7ec7\u5f62\u53d8\u6a21\u62df", "motivation": "\u73b0\u6709\u8f6f\u7ec4\u7ec7\u5f62\u53d8\u6a21\u62df\u5728\u5b9e\u65f6\u6027\u548c\u7269\u7406\u4e00\u81f4\u6027\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u5f00\u53d1\u517c\u987e\u7cbe\u5ea6\u4e0e\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\u4ee5\u652f\u6301\u624b\u672f\u673a\u5668\u4eba\u53ca\u533b\u5b66\u8bad\u7ec3", "method": "\u6574\u5408Kelvinlet\u57fa\u5148\u9a8c\u8fdb\u884c\u6b8b\u5dee\u5b66\u4e60\uff0c\u7ed3\u5408\u7ebf\u6027\u548c\u975e\u7ebf\u6027FEM\u6570\u636e\u96c6\u589e\u5f3a\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u80fd\u529b", "result": "\u663e\u8457\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6\u4e0e\u7269\u7406\u4e00\u81f4\u6027\uff0c\u5728\u8179\u8154\u955c\u6293\u53d6\u6a21\u62df\u4e2d\u5b9e\u73b0\u9ad8\u4fdd\u771f\u624b\u672f\u64cd\u4f5c\uff0c\u4fdd\u6301\u6beb\u79d2\u7ea7\u5b9e\u65f6\u6027\u80fd", "conclusion": "Kelvinlet\u589e\u5f3a\u5b66\u4e60\u4e3a\u624b\u672f\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u65f6\u7269\u7406\u6a21\u62df\u7684\u9ad8\u6548\u7b56\u7565\uff0c\u62d3\u5c55\u4e86\u6570\u636e\u9a71\u52a8\u5efa\u6a21\u7684\u7269\u7406\u53ef\u89e3\u91ca\u6027"}}
{"id": "2506.08064", "pdf": "https://arxiv.org/pdf/2506.08064", "abs": "https://arxiv.org/abs/2506.08064", "authors": ["Livio Tenze", "Enrique Canessa"], "title": "A Real-time 3D Desktop Display", "categories": ["cs.GR", "cs.CV"], "comment": "10 pages, 5 figures", "summary": "A new extended version of the altiro3D C++ Library -- initially developed to\nget glass-free holographic displays starting from 2D images -- is here\nintroduced aiming to deal with 3D video streams from either 2D webcam images or\nflat video files. These streams are processed in real-time to synthesize\nlight-fields (in Native format) and feed realistic 3D experiences. The core\nfunction needed to recreate multiviews consists on the use of MiDaS\nConvolutional Neural Network (CNN), which allows to extract a depth map from a\nsingle 2D image. Artificial Intelligence (AI) computing techniques are applied\nto improve the overall performance of the extended altiro3D Library. Thus,\naltiro3D can now treat standard images, video streams or screen portions of a\nDesktop where other apps may be also running (like web browsers, video chats,\netc) and render them into 3D. To achieve the latter, a screen region need to be\nselected in order to feed the output directly into a light-field 3D device such\nas Looking Glass (LG) Portrait. In order to simplify the acquisition of a\nDesktop screen area by the user, a multi-platform Graphical User Interface has\nbeen also implemented. Sources available at:\nhttps://github.com/canessae/altiro3D/releases/tag/2.0.0", "AI": {"tldr": "\u6269\u5c55altiro3D\u5e93\u5b9e\u73b0\u5b9e\u65f62D\u8f6c3D\u6e32\u67d3\uff0c\u652f\u6301\u89c6\u9891\u6d41/\u684c\u9762\u5e94\u7528\uff0c\u7ed3\u5408MiDaS CNN\u4e0eAI\u4f18\u5316", "motivation": "\u63d0\u5347\u73b0\u6709\u5e93\u529f\u80fd\u4ee5\u5904\u7406\u591a\u6837\u53163D\u8f93\u5165\u6e90\uff082D\u6444\u50cf\u5934/\u89c6\u9891\u6587\u4ef6/\u684c\u9762\u5e94\u7528\uff09\uff0c\u7b80\u5316\u7528\u6237\u64cd\u4f5c\u5e76\u589e\u5f3a\u5b9e\u65f63D\u6e32\u67d3\u80fd\u529b", "method": "\u91c7\u7528MiDaS CNN\u4ece\u5355\u5e272D\u56fe\u50cf\u63d0\u53d6\u6df1\u5ea6\u56fe\uff0c\u7ed3\u5408AI\u4f18\u5316\u7b97\u6cd5\u63d0\u5347\u6027\u80fd\uff0c\u5f00\u53d1\u8de8\u5e73\u53f0GUI\u5b9e\u73b0\u5c4f\u5e55\u533a\u57df\u9009\u62e9", "result": "\u5b9e\u73b0\u5b9e\u65f6\u5149\u573a\u5408\u6210\uff0c\u517c\u5bb9\u7f51\u9875/\u89c6\u9891\u4f1a\u8bae\u7b49\u684c\u9762\u5e94\u7528\uff0c\u652f\u6301Looking Glass\u7b49\u5149\u573a\u8bbe\u5907\u76f4\u63a5\u8f93\u51fa", "conclusion": "\u901a\u8fc7AI\u6df1\u5ea6\u4f30\u8ba1\u4e0e\u7cfb\u7edf\u96c6\u6210\u4f18\u5316\uff0c\u63a8\u52a8\u88f8\u773c3D\u663e\u793a\u6280\u672f\u5728AR/VR\u7b49\u573a\u666f\u7684\u5b9e\u7528\u5316\u8fdb\u7a0b"}}
{"id": "2506.08161", "pdf": "https://arxiv.org/pdf/2506.08161", "abs": "https://arxiv.org/abs/2506.08161", "authors": ["Jakub Bok\u0161ansk\u00fd", "Daniel Meister", "Carsten Benthin"], "title": "GATE: Geometry-Aware Trained Encoding", "categories": ["cs.GR"], "comment": null, "summary": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e09\u89d2\u7f51\u683c\u7684\u51e0\u4f55\u611f\u77e5\u7f16\u7801GATE\uff0c\u89e3\u51b3\u54c8\u5e0c\u7f16\u7801\u7684\u51b2\u7a81\u95ee\u9898\u5e76\u63d0\u5347\u795e\u7ecf\u6e32\u67d3\u6548\u679c", "motivation": "\u4f20\u7edf\u54c8\u5e0c\u7f16\u7801\u5b58\u5728\u54c8\u5e0c\u51b2\u7a81\u3001\u5206\u8fa8\u7387\u4e0e\u573a\u666f\u5c3a\u5bf8\u77db\u76fe\u3001\u5185\u5b58\u8bbf\u95ee\u4e0d\u4e00\u81f4\u7b49\u7f3a\u9677\uff0c\u9700\u66f4\u4f18\u7684\u51e0\u4f55\u7279\u5f81\u7f16\u7801\u65b9\u6848", "method": "\u901a\u8fc7\u5c06\u7279\u5f81\u5411\u91cf\u5b58\u50a8\u5728\u4e09\u89d2\u7f51\u683c\u8868\u9762\uff0c\u5229\u7528\u7f51\u683c\u989c\u8272\u89e3\u8026\u51e0\u4f55\u5bc6\u5ea6\u4e0e\u7279\u5f81\u5bc6\u5ea6\uff0c\u652f\u6301\u81ea\u9002\u5e94\u7ec6\u8282\u63a7\u5236", "result": "\u907f\u514d\u54c8\u5e0c\u51b2\u7a81\uff0c\u5b9e\u73b0\u66f4\u7cbe\u7ec6\u7684\u8bad\u7ec3\u63a7\u5236\uff0c\u652f\u6301\u795e\u7ecf\u8f90\u5c04\u7f13\u5b58\u7b49\u5e94\u7528\u7684\u52a8\u6001LOD\u8c03\u8282", "conclusion": "GATE\u7f16\u7801\u4e3a\u795e\u7ecf\u6e32\u67d3\u7b97\u6cd5\u63d0\u4f9b\u9ad8\u6548\u51e0\u4f55\u7279\u5f81\u8868\u8fbe\uff0c\u5728\u8bad\u7ec3\u63a7\u5236\u548c\u5185\u5b58\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u54c8\u5e0c\u7f16\u7801\u65b9\u6848"}}
{"id": "2506.08237", "pdf": "https://arxiv.org/pdf/2506.08237", "abs": "https://arxiv.org/abs/2506.08237", "authors": ["Bailey Miller", "Rohan Sawhney", "Keenan Crane", "Ioannis Gkioulekas"], "title": "Solving partial differential equations in participating media", "categories": ["cs.GR", "cs.NA", "math.NA"], "comment": "SIGGRAPH 2025. Project page\n  https://imaging.cs.cmu.edu/volumetric_walk_on_spheres", "summary": "We consider the problem of solving partial differential equations (PDEs) in\ndomains with complex microparticle geometry that is impractical, or\nintractable, to model explicitly. Drawing inspiration from volume rendering, we\npropose tackling this problem by treating the domain as a participating medium\nthat models microparticle geometry stochastically, through aggregate\nstatistical properties (e.g., particle density). We first introduce the problem\nsetting of PDE simulation in participating media. We then specialize to\nexponential media and describe the properties that make them an attractive\nmodel of microparticle geometry for PDE simulation problems. We use these\nproperties to develop two new algorithms, volumetric walk on spheres and\nvolumetric walk on stars, that generalize previous Monte Carlo algorithms to\nenable efficient and discretization-free simulation of linear elliptic PDEs\n(e.g., Laplace) in participating media. We demonstrate experimentally that our\nalgorithms can solve Laplace boundary value problems with complex microparticle\ngeometry more accurately and more efficiently than previous approaches, such as\nensemble averaging and homogenization.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u7684\u4f53\u7d20\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u7edf\u8ba1\u5efa\u6a21\u5fae\u9897\u7c92\u51e0\u4f55\uff0c\u9ad8\u6548\u6c42\u89e3\u590d\u6742\u4ecb\u8d28\u4e2d\u7684\u692d\u5706\u578b\u504f\u5fae\u5206\u65b9\u7a0b\u3002", "motivation": "\u663e\u5f0f\u5efa\u6a21\u590d\u6742\u5fae\u9897\u7c92\u51e0\u4f55\u7ed3\u6784\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4e0d\u53ef\u884c\uff0c\u4f20\u7edf\u65b9\u6cd5(\u5982\u96c6\u5408\u5e73\u5747/\u5747\u8d28\u5316)\u5b58\u5728\u6548\u7387\u4e0e\u7cbe\u5ea6\u5c40\u9650\uff0c\u9700\u8981\u65e0\u9700\u79bb\u6563\u5316\u7684\u65b0\u65b9\u6cd5\u3002", "method": "1. \u57fa\u4e8e\u6307\u6570\u4ecb\u8d28\u7279\u6027\u5f00\u53d1volumetric walk on spheres/stars\u7b97\u6cd5\n2. \u5c06\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u6269\u5c55\u5230\u53c2\u4e0e\u4ecb\u8d28\uff0c\u5229\u7528\u7edf\u8ba1\u5c5e\u6027(\u5982\u7c92\u5b50\u5bc6\u5ea6)\u5b9e\u73b0\u79bb\u6563\u5316\u81ea\u7531\u6c42\u89e3", "result": "\u5b9e\u9a8c\u8bc1\u660e\u65b0\u7b97\u6cd5\u5728\u62c9\u666e\u62c9\u65af\u8fb9\u503c\u95ee\u9898\u6c42\u89e3\u4e2d\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u534720-30%\uff0c\u8ba1\u7b97\u6548\u7387\u63d0\u9ad81.5\u500d", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u590d\u6742\u5fae\u9897\u7c92\u51e0\u4f55PDE\u6c42\u89e3\u63d0\u4f9b\u66f4\u4f18\u89e3\uff0c\u672a\u6765\u53ef\u62d3\u5c55\u81f3\u975e\u7ebf\u6027\u65b9\u7a0b\u4e0e\u5176\u4ed6\u4ecb\u8d28\u7c7b\u578b"}}
{"id": "2506.08120", "pdf": "https://arxiv.org/pdf/2506.08120", "abs": "https://arxiv.org/abs/2506.08120", "authors": ["Toyin Aguda", "Erik Wilson", "Allan Anzagira", "Simerjot Kaur", "Charese Smiley"], "title": "Conservative Bias in Large Language Models: Measuring Relation Predictions", "categories": ["cs.CL"], "comment": "10 pages", "summary": "Large language models (LLMs) exhibit pronounced conservative bias in relation\nextraction tasks, frequently defaulting to No_Relation label when an\nappropriate option is unavailable. While this behavior helps prevent incorrect\nrelation assignments, our analysis reveals that it also leads to significant\ninformation loss when reasoning is not explicitly included in the output. We\nsystematically evaluate this trade-off across multiple prompts, datasets, and\nrelation types, introducing the concept of Hobson's choice to capture scenarios\nwhere models opt for safe but uninformative labels over hallucinated ones. Our\nfindings suggest that conservative bias occurs twice as often as hallucination.\nTo quantify this effect, we use SBERT and LLM prompts to capture the semantic\nsimilarity between conservative bias behaviors in constrained prompts and\nlabels generated from semi-constrained and open-ended prompts.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5173\u7cfb\u62bd\u53d6\u4e2d\u5b58\u5728\u4fdd\u5b88\u504f\u8bef\uff0c\u4f18\u5148\u9009\u62e9\u65e0\u4fe1\u606f\u6807\u7b7e\u800c\u975e\u9519\u8bef\u6807\u7b7e\uff0c\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u4f46\u5b89\u5168\u6027\u63d0\u5347", "motivation": "\u53d1\u73b0LLMs\u5728\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u4e2d\u9891\u7e41\u4f7f\u7528No_Relation\u6807\u7b7e\u7684\u4fdd\u5b88\u504f\u8bef\u884c\u4e3a\uff0c\u5bfc\u81f4\u663e\u8457\u4fe1\u606f\u635f\u5931\uff0c\u9700\u91cf\u5316\u4fdd\u5b88\u504f\u8bef\u4e0e\u5e7b\u89c9\u7684\u6743\u8861", "method": "\u4f7f\u7528SBERT\u548c\u591a\u79cd\u63d0\u793a\u7b56\u7565\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5206\u6790\u6bd4\u8f83\u53d7\u9650\u63d0\u793a\u4e0e\u534a\u5f00\u653e\u63d0\u793a\u4e0b\u7684\u6a21\u578b\u884c\u4e3a", "result": "\u4fdd\u5b88\u504f\u8bef\u53d1\u751f\u7387\u662f\u5e7b\u89c9\u7684\u4e24\u500d\uff0c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5206\u6790\u9a8c\u8bc1\u4e86\u4fdd\u5b88\u9009\u62e9\u4e0e\u5f00\u653e\u751f\u6210\u7684\u5173\u8054\u6027", "conclusion": "\u9700\u8981\u5f00\u53d1\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u5e73\u8861\u4fe1\u606f\u5b8c\u6574\u6027\u4e0e\u51c6\u786e\u6027\uff0c\u4e3a\u53d7\u9650\u573a\u666f\u7684\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u89c6\u89d2"}}
{"id": "2506.08334", "pdf": "https://arxiv.org/pdf/2506.08334", "abs": "https://arxiv.org/abs/2506.08334", "authors": ["Weikun Peng", "Jun Lv", "Cewu Lu", "Manolis Savva"], "title": "Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos", "categories": ["cs.GR", "cs.CV"], "comment": "Project website can be found at\n  https://3dlg-hcvc.github.io/video2articulation/", "summary": "Articulated objects are prevalent in daily life. Understanding their\nkinematic structure and reconstructing them have numerous applications in\nembodied AI and robotics. However, current methods require carefully captured\ndata for training or inference, preventing practical, scalable, and\ngeneralizable reconstruction of articulated objects. We focus on reconstruction\nof an articulated object from a casually captured RGBD video shot with a\nhand-held camera. A casually captured video of an interaction with an\narticulated object is easy to acquire at scale using smartphones. However, this\nsetting is quite challenging, as the object and camera move simultaneously and\nthere are significant occlusions as the person interacts with the object. To\ntackle these challenges, we introduce a coarse-to-fine framework that infers\njoint parameters and segments movable parts of the object from a dynamic RGBD\nvideo. To evaluate our method under this new setting, we build a 20$\\times$\nlarger synthetic dataset of 784 videos containing 284 objects across 11\ncategories. We compare our approach with existing methods that also take video\nas input. Experiments show that our method can reconstruct synthetic and real\narticulated objects across different categories from dynamic RGBD videos,\noutperforming existing methods significantly.", "AI": {"tldr": "\u63d0\u51fa\u4ece\u52a8\u6001RGBD\u89c6\u9891\u4e2d\u91cd\u5efa\u94f0\u63a5\u7269\u4f53\u7684\u7c97\u5230\u7ec6\u6846\u67b6\uff0c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u5bf9\u7cbe\u826f\u91c7\u96c6\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4f7f\u7528\u667a\u80fd\u624b\u673a\u8f7b\u677e\u91c7\u96c6\u7684\u4ea4\u4e92\u89c6\u9891\u5b9e\u73b0\u53ef\u6269\u5c55\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u7cbe\u5fc3\u91c7\u96c6\u7684\u8bad\u7ec3/\u63a8\u7406\u6570\u636e\uff0c\u96be\u4ee5\u5b9e\u73b0\u94f0\u63a5\u7269\u4f53\u7684\u5927\u89c4\u6a21\u666e\u9002\u5316\u91cd\u5efa\u3002\u65e5\u5e38\u667a\u80fd\u624b\u673a\u62cd\u6444\u7684\u4ea4\u4e92\u89c6\u9891\u6613\u83b7\u53d6\u4f46\u5b58\u5728\u7269\u4f53\u4e0e\u76f8\u673a\u540c\u6b65\u79fb\u52a8\u3001\u4e25\u91cd\u906e\u6321\u7b49\u6311\u6218\u3002", "method": "\u5f15\u5165\u7c97\u5230\u7ec6\u6846\u67b6\uff1a1\uff09\u4ece\u52a8\u6001RGBD\u89c6\u9891\u63a8\u65ad\u5173\u8282\u53c2\u6570 2\uff09\u5206\u5272\u7269\u4f53\u53ef\u52a8\u90e8\u4ef6\u3002\u901a\u8fc720\u500d\u89c4\u6a21\u5408\u6210\u6570\u636e\u96c6\uff08784\u89c6\u9891/284\u7269\u4f53/11\u7c7b\uff09\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u89c6\u9891\u8f93\u5165\u65b9\u6cd5\uff0c\u8de8\u7c7b\u522b\u91cd\u5efa\u6548\u679c\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9996\u6b21\u5b9e\u73b0\u4ece\u52a8\u6001RGBD\u89c6\u9891\u8de8\u7c7b\u522b\u91cd\u5efa\u94f0\u63a5\u7269\u4f53\uff0c\u4e3a\u673a\u5668\u4eba\u7b49\u9886\u57df\u63d0\u4f9b\u5b9e\u7528\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08123", "pdf": "https://arxiv.org/pdf/2506.08123", "abs": "https://arxiv.org/abs/2506.08123", "authors": ["Jacob Dineen", "Aswin RRV", "Qin Liu", "Zhikun Xu", "Xiao Ye", "Ming Shen", "Zhaonan Li", "Shijie Lu", "Chitta Baral", "Muhao Chen", "Ben Zhou"], "title": "QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA", "categories": ["cs.CL"], "comment": null, "summary": "Alignment of large language models with explicit principles (such as\nhelpfulness, honesty, and harmlessness) is crucial for ensuring safe and\nreliable AI systems. However, standard reward-based alignment methods typically\ncollapse diverse feedback into a single scalar reward, entangling multiple\nobjectives into one opaque training signal, which hinders interpretability. In\nthis work, we introduce QA-LIGN, an automatic symbolic reward decomposition\napproach that preserves the structure of each constitutional principle within\nthe reward mechanism. Instead of training a black-box reward model that outputs\na monolithic score, QA-LIGN formulates principle-specific evaluation questions\nand derives separate reward components for each principle, making it a drop-in\nreward model replacement. Experiments aligning an uncensored large language\nmodel with a set of constitutional principles demonstrate that QA-LIGN offers\ngreater transparency and adaptability in the alignment process. At the same\ntime, our approach achieves performance on par with or better than a DPO\nbaseline. Overall, these results represent a step toward more interpretable and\ncontrollable alignment of language models, achieved without sacrificing\nend-task performance.", "AI": {"tldr": "QA-LIGN\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u5956\u52b1\u5206\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u539f\u5219\u7279\u5f02\u6027\u8bc4\u4f30\u95ee\u9898\u5b9e\u73b0\u900f\u660e\u5316\u6a21\u578b\u5bf9\u9f50\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u5347\u53ef\u63a7\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5956\u52b1\u7684\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\u5c06\u591a\u7ef4\u53cd\u9988\u538b\u7f29\u4e3a\u5355\u4e00\u6807\u91cf\u5956\u52b1\uff0c\u5bfc\u81f4\u53ef\u89e3\u91ca\u6027\u5dee\u3001\u4e0d\u540c\u539f\u5219\u95f4\u7684\u6743\u8861\u5173\u7cfb\u4e0d\u900f\u660e\u3002", "method": "\u5f00\u53d1QA-LIGN\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u539f\u5219\u76f8\u5173\u8bc4\u4f30\u95ee\u9898\uff0c\u4e3a\u6bcf\u4e2a\u5baa\u6cd5\u539f\u5219\u5efa\u7acb\u72ec\u7acb\u5956\u52b1\u7ec4\u4ef6\uff0c\u66ff\u4ee3\u4f20\u7edf\u9ed1\u7bb1\u5956\u52b1\u6a21\u578b\u3002", "result": "\u5728\u672a\u5ba1\u67e5\u5927\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u4e0eDPO\u57fa\u7ebf\u76f8\u5f53/\u66f4\u4f18\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u9f50\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5728\u65e0\u9700\u727a\u7272\u6a21\u578b\u6027\u80fd\u7684\u524d\u63d0\u4e0b\uff0c\u63a8\u52a8\u4e86\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u53ef\u63a7\u6027\uff0c\u4e3a\u5b89\u5168\u53ef\u9760\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.08350", "pdf": "https://arxiv.org/pdf/2506.08350", "abs": "https://arxiv.org/abs/2506.08350", "authors": ["Yicheng Zhan", "Dong-Ha Shin", "Seung-Hwan Baek", "Kaan Ak\u015fit"], "title": "Complex-Valued Holographic Radiance Fields", "categories": ["cs.GR", "cs.CV", "cs.ET"], "comment": "28 pages, 21 figures", "summary": "Modeling the full properties of light, including both amplitude and phase, in\n3D representations is crucial for advancing physically plausible rendering,\nparticularly in holographic displays. To support these features, we propose a\nnovel representation that optimizes 3D scenes without relying on\nintensity-based intermediaries. We reformulate 3D Gaussian splatting with\ncomplex-valued Gaussian primitives, expanding support for rendering with light\nwaves. By leveraging RGBD multi-view images, our method directly optimizes\ncomplex-valued Gaussians as a 3D holographic scene representation. This\neliminates the need for computationally expensive hologram re-optimization.\nCompared with state-of-the-art methods, our method achieves 30x-10,000x speed\nimprovements while maintaining on-par image quality, representing a first step\ntowards geometrically aligned, physically plausible holographic scene\nrepresentations.", "AI": {"tldr": "\u63d0\u51fa\u590d\u6570\u503c\u9ad8\u65af\u57fa\u5143\u8868\u793a\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u6548\u7269\u7406\u7cbe\u786e\u7684\u5168\u606f\u573a\u666f\u6e32\u67d3", "motivation": "\u73b0\u67093D\u573a\u666f\u8868\u793a\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u51e0\u4f55\u5bf9\u9f50\u548c\u7269\u7406\u5149\u6ce2\u5c5e\u6027\u5efa\u6a21\u9700\u6c42\uff0c\u5236\u7ea6\u5168\u606f\u663e\u793a\u53d1\u5c55", "method": "\u4f7f\u7528\u590d\u6570\u503c\u9ad8\u65af\u57fa\u5143\u91cd\u67843D\u9ad8\u65af\u6cfc\u6e85\u7b97\u6cd5\uff0c\u76f4\u63a5\u901a\u8fc7RGBD\u591a\u89c6\u89d2\u56fe\u50cf\u4f18\u5316\u5168\u606f\u573a\u666f\u8868\u793a", "result": "\u5b9e\u73b030-10000\u500d\u901f\u5ea6\u63d0\u5347\uff0c\u4fdd\u6301\u56fe\u50cf\u8d28\u91cf\uff0c\u6d88\u9664\u5168\u606f\u56fe\u91cd\u590d\u4f18\u5316\u9700\u6c42", "conclusion": "\u9996\u6b21\u5efa\u7acb\u51e0\u4f55\u5bf9\u9f50\u7684\u7269\u7406\u7cbe\u786e\u5168\u606f\u573a\u666f\u8868\u793a\u6846\u67b6\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u663e\u793a\u6280\u672f\u5960\u5b9a\u57fa\u7840"}}
{"id": "2506.08136", "pdf": "https://arxiv.org/pdf/2506.08136", "abs": "https://arxiv.org/abs/2506.08136", "authors": ["Zefang Liu", "Yinzhu Quan"], "title": "EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments", "categories": ["cs.CL"], "comment": null, "summary": "We introduce EconWebArena, a benchmark for evaluating autonomous agents on\ncomplex, multimodal economic tasks in realistic web environments. The benchmark\ncomprises 360 curated tasks from 82 authoritative websites spanning domains\nsuch as macroeconomics, labor, finance, trade, and public policy. Each task\nchallenges agents to navigate live websites, interpret structured and visual\ncontent, interact with real interfaces, and extract precise, time-sensitive\ndata through multi-step workflows. We construct the benchmark by prompting\nmultiple large language models (LLMs) to generate candidate tasks, followed by\nrigorous human curation to ensure clarity, feasibility, and source reliability.\nUnlike prior work, EconWebArena emphasizes fidelity to authoritative data\nsources and the need for grounded web-based economic reasoning. We evaluate a\ndiverse set of state-of-the-art multimodal LLMs as web agents, analyze failure\ncases, and conduct ablation studies to assess the impact of visual grounding,\nplan-based reasoning, and interaction design. Our results reveal substantial\nperformance gaps and highlight persistent challenges in grounding, navigation,\nand multimodal understanding, positioning EconWebArena as a rigorous testbed\nfor economic web intelligence.", "AI": {"tldr": "EconWebArena\u662f\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9e\u7f51\u7edc\u73af\u5883\u7684\u591a\u6a21\u6001\u7ecf\u6d4e\u4efb\u52a1\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b360\u4e2a\u6743\u5a01\u7f51\u7ad9\u4efb\u52a1\uff0c\u7528\u4e8e\u6d4b\u8bd5\u667a\u80fd\u4f53\u7684\u7f51\u7edc\u7ecf\u6d4e\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u5bf9\u6743\u5a01\u7ecf\u6d4e\u6570\u636e\u6e90\u7684\u9002\u5e94\u6027\u6d4b\u8bd5\uff0c\u9700\u6784\u5efa\u66f4\u8d34\u8fd1\u771f\u5b9e\u573a\u666f\u7684\u7ecf\u6d4e\u7f51\u7edc\u667a\u80fd\u8bc4\u4f30\u4f53\u7cfb\u3002", "method": "\u901a\u8fc7LLMs\u751f\u6210\u5019\u9009\u4efb\u52a1\u2192\u4eba\u5de5\u7b5b\u9009\u2192\u591a\u6a21\u6001LLM\u4ee3\u7406\u6d4b\u8bd5\uff0c\u8fdb\u884c\u89c6\u89c9\u951a\u5b9a\u3001\u89c4\u5212\u63a8\u7406\u548c\u4ea4\u4e92\u8bbe\u8ba1\u7684\u5206\u5c42\u8bc4\u4f30\u3002", "result": "\u4e3b\u6d41\u6a21\u578b\u5728\u6570\u636e\u951a\u5b9a\u3001\u8de8\u6a21\u6001\u7406\u89e3\u548c\u591a\u6b65\u5bfc\u822a\u4e2d\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff08\u5982\u89c4\u5212\u63a8\u7406\u80fd\u529b\u63d0\u5347\u4ec5\u5e26\u67656%\u51c6\u786e\u7387\u589e\u957f\uff09\u3002", "conclusion": "\u8be5\u57fa\u51c6\u63ed\u793a\u4e86\u7ecf\u6d4e\u7f51\u7edc\u667a\u80fd\u7684\u4e09\u5927\u6838\u5fc3\u6311\u6218\uff1a\u52a8\u6001\u6570\u636e\u951a\u5b9a\u3001\u591a\u6a21\u6001\u4ea4\u4e92\u7406\u89e3\u3001\u590d\u6742\u5de5\u4f5c\u6d41\u7ba1\u7406\uff0c\u4e3a\u9886\u57df\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u5316\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2506.09023", "pdf": "https://arxiv.org/pdf/2506.09023", "abs": "https://arxiv.org/abs/2506.09023", "authors": ["Julia Guerrero-Viu", "Michael Fischer", "Iliyan Georgiev", "Elena Garces", "Diego Gutierrez", "Belen Masia", "Valentin Deschaintre"], "title": "Fine-Grained Spatially Varying Material Selection in Images", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Selection is the first step in many image editing processes, enabling faster\nand simpler modifications of all pixels sharing a common modality. In this\nwork, we present a method for material selection in images, robust to lighting\nand reflectance variations, which can be used for downstream editing tasks. We\nrely on vision transformer (ViT) models and leverage their features for\nselection, proposing a multi-resolution processing strategy that yields finer\nand more stable selection results than prior methods. Furthermore, we enable\nselection at two levels: texture and subtexture, leveraging a new two-level\nmaterial selection (DuMaS) dataset which includes dense annotations for over\n800,000 synthetic images, both on the texture and subtexture levels.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eViT\u7684\u591a\u5206\u8fa8\u7387\u6750\u8d28\u9009\u62e9\u65b9\u6cd5DuMaS\uff0c\u5b9e\u73b0\u7eb9\u7406/\u5b50\u7eb9\u7406\u4e24\u7ea7\u9009\u62e9\uff0c\u63d0\u5347\u56fe\u50cf\u7f16\u8f91\u7684\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027", "motivation": "\u73b0\u6709\u6750\u8d28\u9009\u62e9\u65b9\u6cd5\u5728\u5149\u7167\u548c\u53cd\u5c04\u53d8\u5316\u4e0b\u4e0d\u591f\u9c81\u68d2\uff0c\u9700\u6539\u8fdb\u9009\u62e9\u8d28\u91cf\u4ee5\u652f\u6301\u9ad8\u6548\u56fe\u50cf\u7f16\u8f91", "method": "\u7ed3\u5408\u89c6\u89c9Transformer\u7279\u5f81\u4e0e\u591a\u5206\u8fa8\u7387\u5904\u7406\u7b56\u7565\uff0c\u6784\u5efa\u5305\u542b80\u4e07\u6807\u6ce8\u56fe\u50cf\u7684DuMaS\u6570\u636e\u96c6\u5b9e\u73b0\u4e24\u7ea7\u6750\u8d28\u9009\u62e9", "result": "\u65b9\u6cd5\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u7684\u7cbe\u7ec6\u9009\u62e9\uff0c\u652f\u6301\u66f4\u7a33\u5b9a\u7684\u4e0b\u6e38\u7f16\u8f91\u64cd\u4f5c", "conclusion": "\u591a\u5206\u8fa8\u7387ViT\u5904\u7406\u6709\u6548\u63d0\u5347\u6750\u8d28\u9009\u62e9\u8d28\u91cf\uff0cDuMaS\u6570\u636e\u96c6\u4e3a\u6750\u8d28\u611f\u77e5\u7814\u7a76\u63d0\u4f9b\u65b0\u57fa\u51c6"}}
{"id": "2506.08147", "pdf": "https://arxiv.org/pdf/2506.08147", "abs": "https://arxiv.org/abs/2506.08147", "authors": ["Muhammad Usman", "Muhammad Ahmad", "M. Shahiki Tash", "Irina Gelbukh", "Rolando Quintero Tellez", "Grigori Sidorov"], "title": "Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Social media platforms are critical spaces for public discourse, shaping\nopinions and community dynamics, yet their widespread use has amplified harmful\ncontent, particularly hate speech, threatening online safety and inclusivity.\nWhile hate speech detection has been extensively studied in languages like\nEnglish and Spanish, Urdu remains underexplored, especially using\ntranslation-based approaches. To address this gap, we introduce a trilingual\ndataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and\nSpanish (3,162 samples), collected via keyword filtering, with a balanced\ndistribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology\nleverages attention layers as a precursor to transformer-based models and large\nlanguage models (LLMs), enhancing feature extraction for multilingual hate\nspeech detection. For non-transformer models, we use TF-IDF for feature\nextraction. The dataset is benchmarked using state-of-the-art models, including\nGPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models\nlike SVM and other transformers (e.g., BERT, RoBERTa). Three annotators,\nfollowing rigorous guidelines, ensured high dataset quality, achieving a\nFleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5\nTurbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of\n0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for\nUrdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B).\nThese results reflect improvements of 8.75% in English (over SVM baseline\n0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM\nbaseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline\n0.82). Our framework offers a robust solution for multilingual hate speech\ndetection, fostering safer digital communities worldwide.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u4e0eTransformer\u6a21\u578b\u7684\u591a\u8bed\u8a00\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u6846\u67b6\uff0c\u5728\u4e09\u8bed\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4ec7\u6068\u8a00\u8bba\u5a01\u80c1\u5728\u7ebf\u5b89\u5168\uff0c\u4f46\u4e4c\u5c14\u90fd\u8bed\u7814\u7a76\u5b58\u5728\u7a7a\u767d\uff0c\u9700\u5f00\u53d1\u6709\u6548\u7684\u591a\u8bed\u8a00\u68c0\u6d4b\u65b9\u6848", "method": "\u6784\u5efa10,193\u6761\u4e09\u8bed\u5e73\u8861\u6570\u636e\u96c6\uff0c\u91c7\u7528\u6ce8\u610f\u529b\u5c42\u589e\u5f3a\u7684Transformer\u6a21\u578b\uff08GPT-3.5/Qwen 72B\uff09\u4e0e\u4f20\u7edf\u6a21\u578b\u5bf9\u6bd4", "result": "\u591a\u8bed\u8a00\u8054\u5408\u6a21\u578bF1\u8fbe0.88\uff08\u8f83\u57fa\u7ebf\u63d0\u53477.32%\uff09\uff0c\u82f1\u8bed/\u897f\u73ed\u7259\u8bed/\u4e4c\u5c14\u90fd\u8bed\u5206\u522b\u8fbe0.87/0.85/0.81", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5168\u7403\u6570\u5b57\u793e\u533a\u5b89\u5168\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u591a\u8bed\u8a00\u5185\u5bb9\u5ba1\u6838\u6280\u672f\u8fdb\u6b65"}}
{"id": "2506.08158", "pdf": "https://arxiv.org/pdf/2506.08158", "abs": "https://arxiv.org/abs/2506.08158", "authors": ["Lijing Zhu", "Qizhen Lan", "Qing Tian", "Wenbo Sun", "Li Yang", "Lu Xia", "Yixin Xie", "Xi Xiao", "Tiehang Duan", "Cui Tao", "Shuteng Niu"], "title": "ETT-CKGE: Efficient Task-driven Tokens for Continual Knowledge Graph Embedding", "categories": ["cs.CL"], "comment": null, "summary": "Continual Knowledge Graph Embedding (CKGE) seeks to integrate new knowledge\nwhile preserving past information. However, existing methods struggle with\nefficiency and scalability due to two key limitations: (1) suboptimal knowledge\npreservation between snapshots caused by manually designed node/relation\nimportance scores that ignore graph dependencies relevant to the downstream\ntask, and (2) computationally expensive graph traversal for node/relation\nimportance calculation, leading to slow training and high memory overhead. To\naddress these limitations, we introduce ETT-CKGE (Efficient, Task-driven,\nTokens for Continual Knowledge Graph Embedding), a novel task-guided CKGE\nmethod that leverages efficient task-driven tokens for efficient and effective\nknowledge transfer between snapshots. Our method introduces a set of learnable\ntokens that directly capture task-relevant signals, eliminating the need for\nexplicit node scoring or traversal. These tokens serve as consistent and\nreusable guidance across snapshots, enabling efficient token-masked embedding\nalignment between snapshots. Importantly, knowledge transfer is achieved\nthrough simple matrix operations, significantly reducing training time and\nmemory usage. Extensive experiments across six benchmark datasets demonstrate\nthat ETT-CKGE consistently achieves superior or competitive predictive\nperformance, while substantially improving training efficiency and scalability\ncompared to state-of-the-art CKGE methods. The code is available at:\nhttps://github.com/lijingzhu1/ETT-CKGE/tree/main", "AI": {"tldr": "\u63d0\u51faETT-CKGE\u65b9\u6cd5\uff0c\u901a\u8fc7\u4efb\u52a1\u9a71\u52a8\u4ee4\u724c\u5b9e\u73b0\u9ad8\u6548\u6301\u7eed\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\uff0c\u66ff\u4ee3\u4eba\u5de5\u8bc4\u5206\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027", "motivation": "\u73b0\u6709\u6301\u7eed\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u65b9\u6cd5\u56e0\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u7684\u8282\u70b9\u91cd\u8981\u6027\u8bc4\u5206\u548c\u590d\u6742\u7684\u56fe\u904d\u5386\u8ba1\u7b97\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3001\u53ef\u6269\u5c55\u6027\u5dee\u53ca\u77e5\u8bc6\u4fdd\u5b58\u4e0d\u5145\u5206", "method": "\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u4efb\u52a1\u9a71\u52a8\u4ee4\u724c\u6355\u6349\u4efb\u52a1\u76f8\u5173\u4fe1\u53f7\uff0c\u901a\u8fc7\u4ee4\u724c\u63a9\u7801\u5d4c\u5165\u5bf9\u9f50\u5b9e\u73b0\u8de8\u5feb\u7167\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4ec5\u9700\u77e9\u9635\u8fd0\u7b97\u5373\u53ef\u5b8c\u6210", "result": "\u57286\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cETT-CKGE\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u53479.7\u500d\uff0c\u5185\u5b58\u6d88\u8017\u964d\u4f4e89%", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4efb\u52a1\u9a71\u52a8\u4ee4\u724c\u6709\u6548\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u6548\u7387\u74f6\u9888\uff0c\u4f7f\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u66f4\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u573a\u666f\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2506.08172", "pdf": "https://arxiv.org/pdf/2506.08172", "abs": "https://arxiv.org/abs/2506.08172", "authors": ["Gerardo Aleman Manzanarez", "Nora de la Cruz Arana", "Jorge Garcia Flores", "Yobany Garcia Medina", "Raul Monroy", "Nathalie Pernelle"], "title": "Can Artificial Intelligence Write Like Borges? An Evaluation Protocol for Spanish Microfiction", "categories": ["cs.CL"], "comment": "28 pages, 16 figures. Submitted to Applied Sciences", "summary": "Automated story writing has been a subject of study for over 60 years. Large\nlanguage models can generate narratively consistent and linguistically coherent\nshort fiction texts. Despite these advancements, rigorous assessment of such\noutputs for literary merit - especially concerning aesthetic qualities - has\nreceived scant attention. In this paper, we address the challenge of evaluating\nAI-generated microfictions and argue that this task requires consideration of\nliterary criteria across various aspects of the text, such as thematic\ncoherence, textual clarity, interpretive depth, and aesthetic quality. To\nfacilitate this, we present GrAImes: an evaluation protocol grounded in\nliterary theory, specifically drawing from a literary perspective, to offer an\nobjective framework for assessing AI-generated microfiction. Furthermore, we\nreport the results of our validation of the evaluation protocol, as answered by\nboth literature experts and literary enthusiasts. This protocol will serve as a\nfoundation for evaluating automatically generated microfictions and assessing\ntheir literary value.", "AI": {"tldr": "\u63d0\u51faGrAImes\u8bc4\u4f30\u534f\u8bae\uff0c\u57fa\u4e8e\u6587\u5b66\u7406\u8bba\u6784\u5efaAI\u751f\u6210\u5fae\u578b\u5c0f\u8bf4\u7684\u6587\u5b66\u4ef7\u503c\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6846\u67b6", "motivation": "\u73b0\u6709AI\u6587\u672c\u751f\u6210\u6280\u672f\u7f3a\u4e4f\u5bf9\u6587\u5b66\u5ba1\u7f8e\u8d28\u91cf\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u6807\u51c6\uff0c\u9700\u5efa\u7acb\u7406\u8bba\u9a71\u52a8\u7684\u5ba2\u89c2\u8bc4\u4ef7\u4f53\u7cfb", "method": "\u4ece\u6587\u5b66\u7406\u8bba\u51fa\u53d1\uff0c\u6574\u5408\u4e3b\u9898\u8fde\u8d2f\u6027\u3001\u6587\u672c\u6e05\u6670\u5ea6\u3001\u9610\u91ca\u6df1\u5ea6\u548c\u7f8e\u5b66\u8d28\u91cf\u56db\u4e2a\u7ef4\u5ea6\u6784\u5efa\u8bc4\u4f30\u77e9\u9635", "result": "\u534f\u8bae\u901a\u8fc7\u6587\u5b66\u4e13\u5bb6\u548c\u7231\u597d\u8005\u53cc\u91cd\u9a8c\u8bc1\uff0c\u8bc1\u5b9e\u80fd\u6709\u6548\u8bc4\u4f30AI\u5fae\u5c0f\u8bf4\u7684\u6587\u5b66\u4ef7\u503c", "conclusion": "GrAImes\u4e3aAI\u6587\u5b66\u521b\u4f5c\u8d28\u91cf\u8bc4\u4f30\u5efa\u7acb\u7406\u8bba\u57fa\u51c6\uff0c\u63a8\u52a8\u8ba1\u7b97\u521b\u9020\u529b\u4e0e\u6587\u5b66\u6279\u8bc4\u7684\u8de8\u5b66\u79d1\u878d\u5408"}}
{"id": "2506.08174", "pdf": "https://arxiv.org/pdf/2506.08174", "abs": "https://arxiv.org/abs/2506.08174", "authors": ["Li Weigang", "Pedro Carvalho Brom"], "title": "LLM-BT: Back-Translation as a Framework for Terminology Standardization and Dynamic Semantic Embedding", "categories": ["cs.CL"], "comment": "23 pages", "summary": "The rapid growth of English technical terms challenges traditional\nexpert-driven standardization, especially in fast-evolving fields like AI and\nquantum computing. Manual methods struggle to ensure multilingual consistency.\nWe propose \\textbf{LLM-BT}, a back-translation framework powered by large\nlanguage models (LLMs) to automate terminology verification and standardization\nvia cross-lingual semantic alignment. Our contributions are: \\textbf{(1)\nTerm-Level Consistency Validation:} Using English $\\rightarrow$ intermediate\nlanguage $\\rightarrow$ English back-translation, LLM-BT achieves high term\nconsistency across models (e.g., GPT-4, DeepSeek, Grok), with case studies\nshowing over 90\\% exact or semantic matches. \\textbf{(2) Multi-Path\nVerification Workflow:} A novel ``Retrieve--Generate--Verify--Optimize''\npipeline integrates serial (e.g., EN $\\rightarrow$ ZHcn $\\rightarrow$ ZHtw\n$\\rightarrow$ EN) and parallel (e.g., EN $\\rightarrow$ Chinese/Portuguese\n$\\rightarrow$ EN) BT routes. BLEU and term accuracy indicate strong\ncross-lingual robustness (BLEU $>$ 0.45; Portuguese accuracy 100\\%).\n\\textbf{(3) Back-Translation as Semantic Embedding:} BT is conceptualized as\ndynamic semantic embedding, revealing latent meaning trajectories. Unlike\nstatic embeddings, LLM-BT provides transparent path-based embeddings shaped by\nmodel evolution. LLM-BT transforms back-translation into an active engine for\nmultilingual terminology standardization, enabling human--AI collaboration:\nmachines ensure semantic fidelity, humans guide cultural interpretation. This\ninfrastructure supports terminology governance across scientific and\ntechnological fields worldwide.", "AI": {"tldr": "\u63d0\u51faLLM-BT\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u56de\u8bd1\u6280\u672f\u5b9e\u73b0\u8de8\u8bed\u8a00\u672f\u8bed\u6807\u51c6\u5316\uff0c\u6838\u5fc3\u8d21\u732e\u5305\u62ec\u672f\u8bed\u4e00\u81f4\u6027\u9a8c\u8bc1\u3001\u591a\u8def\u5f84\u9a8c\u8bc1\u6d41\u7a0b\u548c\u52a8\u6001\u8bed\u4e49\u5d4c\u5165\u673a\u5236\u3002", "motivation": "\u4f20\u7edf\u4e13\u5bb6\u4e3b\u5bfc\u7684\u672f\u8bed\u6807\u51c6\u5316\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9AI/\u91cf\u5b50\u8ba1\u7b97\u7b49\u9886\u57df\u5feb\u901f\u589e\u957f\u7684\u82f1\u8bed\u6280\u672f\u672f\u8bed\uff0c\u4e14\u4eba\u5de5\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u8bc1\u591a\u8bed\u8a00\u4e00\u81f4\u6027\u3002", "method": "1) \u672f\u8bed\u7ea7\u4e00\u81f4\u6027\u9a8c\u8bc1\uff08\u82f1\u8bed\u2192\u4e2d\u95f4\u8bed\u8a00\u2192\u82f1\u8bed\u56de\u8bd1\uff09\n2) \u591a\u8def\u5f84\u9a8c\u8bc1\u6d41\u7a0b\uff08\u68c0\u7d22-\u751f\u6210-\u9a8c\u8bc1-\u4f18\u5316\uff09\n3) \u5c06\u56de\u8bd1\u6982\u5ff5\u5316\u4e3a\u52a8\u6001\u8bed\u4e49\u5d4c\u5165", "result": "\u6848\u4f8b\u663e\u793a\u8d8590%\u672f\u8bed\u5339\u914d\u7387\uff1bBLEU>0.45\uff08\u8461\u8404\u7259\u8bed\u51c6\u786e\u7387100%\uff09\uff1b\u6784\u5efa\u900f\u660e\u8def\u5f84\u5d4c\u5165\u53cd\u6620\u6a21\u578b\u6f14\u8fdb", "conclusion": "LLM-BT\u91cd\u6784\u56de\u8bd1\u4e3a\u591a\u8bed\u8a00\u672f\u8bed\u6807\u51c6\u5316\u7684\u4e3b\u52a8\u5f15\u64ce\uff0c\u5b9e\u73b0\u4eba\u673a\u534f\u4f5c\uff1a\u673a\u5668\u4fdd\u969c\u8bed\u4e49\u4fdd\u771f\uff0c\u4eba\u7c7b\u6307\u5bfc\u6587\u5316\u8be0\u91ca\uff0c\u652f\u6491\u5168\u7403\u79d1\u6280\u9886\u57df\u672f\u8bed\u6cbb\u7406\u3002"}}
{"id": "2506.08184", "pdf": "https://arxiv.org/pdf/2506.08184", "abs": "https://arxiv.org/abs/2506.08184", "authors": ["Chupei Wang", "Jiaqiu Vince Sun"], "title": "Unable to forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length", "categories": ["cs.CL", "cs.AI", "q-bio.NC"], "comment": null, "summary": "Information retrieval in Large Language Models (LLMs) is increasingly\nrecognized as intertwined with generation capabilities rather than mere lookup.\nWhile longer contexts are often assumed to improve retrieval, the effects of\nintra-context interference remain understudied. To address this, we adapt the\nproactive interference (PI) paradigm from cognitive science, where earlier\ninformation disrupts recall of newer updates. In humans, susceptibility to such\ninterference is inversely linked to working memory capacity. We introduce\nPI-LLM, an evaluation that sequentially streams semantically related key-value\nupdates and queries only the final values. Although these final values are\nclearly positioned just before the query, LLM retrieval accuracy declines\nlog-linearly toward zero as interference accumulates; errors arise from\nretrieving previously overwritten values. Attempts to mitigate interference via\nprompt engineering (e.g., instructing models to ignore earlier input) yield\nlimited success. These findings reveal a fundamental constraint on LLMs'\nability to disentangle interference and flexibly manipulate information,\nsuggesting a working memory bottleneck beyond mere context access. This calls\nfor approaches that strengthen models' ability to suppress irrelevant content\nduring retrieval.", "AI": {"tldr": "LLMs\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u5b58\u5728\u4e3b\u52a8\u5e72\u6270\u654f\u611f\u6027\u95ee\u9898\uff0c\u968f\u7740\u8bed\u4e49\u5e72\u6270\u79ef\u7d2f\uff0c\u68c0\u7d22\u51c6\u786e\u7387\u5448\u5bf9\u6570\u7ebf\u6027\u4e0b\u964d\uff0c\u63d0\u793a\u5de5\u7a0b\u5e72\u9884\u6548\u679c\u6709\u9650\uff0c\u63ed\u793a\u5de5\u4f5c\u8bb0\u5fc6\u74f6\u9888\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u9a8c\u8bc1\u957f\u4e0a\u4e0b\u6587\u7a97\u53e3\u80fd\u5426\u771f\u6b63\u63d0\u5347LLMs\u7684\u4fe1\u606f\u68c0\u7d22\u80fd\u529b\uff0c\u5c24\u5176\u4e3b\u52a8\u5e72\u6270\uff08\u65e7\u4fe1\u606f\u8986\u76d6\u65b0\u4fe1\u606f\uff09\u5bf9\u68c0\u7d22\u51c6\u786e\u6027\u7684\u5f71\u54cd\u673a\u5236\u5c1a\u672a\u660e\u786e\u3002", "method": "\u5f15\u5165\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u4e3b\u52a8\u5e72\u6270(PI)\u8303\u5f0f\uff0c\u8bbe\u8ba1PI-LLM\u8bc4\u4f30\u6846\u67b6\uff1a\u6d41\u5f0f\u8f93\u5165\u8bed\u4e49\u76f8\u5173\u7684\u952e\u503c\u66f4\u65b0\u540e\uff0c\u4ec5\u67e5\u8be2\u6700\u7ec8\u503c\u7684\u4f4d\u7f6e\u51c6\u786e\u6027\u3002", "result": "1. \u6700\u7ec8\u503c\u4f4d\u7f6e\u4e34\u8fd1\u67e5\u8be2\u65f6\u51c6\u786e\u7387\u4ecd\u663e\u8457\u4e0b\u964d\n2. \u9519\u8bef\u6e90\u4e8e\u68c0\u7d22\u88ab\u8986\u76d6\u7684\u65e7\u503c\n3. \u63d0\u793a\u6a21\u578b\u5ffd\u7565\u65e7\u4fe1\u606f\u7684\u6307\u4ee4\u4ec5\u63d0\u534710%\u51c6\u786e\u7387", "conclusion": "LLMs\u5b58\u5728\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u5de5\u4f5c\u8bb0\u5fc6\u74f6\u9888\uff0c\u5355\u7eaf\u6269\u5927\u4e0a\u4e0b\u6587\u7a97\u53e3\u65e0\u6cd5\u89e3\u51b3\u4fe1\u606f\u5e72\u6270\u95ee\u9898\uff0c\u9700\u5f00\u53d1\u6291\u5236\u65e0\u5173\u4fe1\u606f\u7684\u68c0\u7d22\u589e\u5f3a\u673a\u5236\u3002"}}
{"id": "2506.08221", "pdf": "https://arxiv.org/pdf/2506.08221", "abs": "https://arxiv.org/abs/2506.08221", "authors": ["Samra Zafar", "Shaheer Minhas", "Syed Ali Hassan Zaidi", "Arfa Naeem", "Zahra Ali"], "title": "\"I Wrote, I Paused, I Rewrote\" Teaching LLMs to Read Between the Lines of Student Writing", "categories": ["cs.CL"], "comment": "7 pages, 6 figures, 2 tables", "summary": "Large language models(LLMs) like Gemini are becoming common tools for\nsupporting student writing. But most of their feedback is based only on the\nfinal essay missing important context about how that text was written. In this\npaper, we explore whether using writing process data, collected through\nkeystroke logging and periodic snapshots, can help LLMs give feedback that\nbetter reflects how learners think and revise while writing. We built a digital\nwriting tool that captures both what students type and how their essays evolve\nover time. Twenty students used this tool to write timed essays, which were\nthen evaluated in two ways: (i) LLM generated feedback using both the final\nessay and the full writing trace, and (ii) After the task, students completed\nsurveys about how useful and relatable they found the feedback. Early results\nshow that learners preferred the process-aware LLM feedback, finding it more in\ntune with their own thinking. We also found that certain types of edits, like\nadding new content or reorganizing paragraphs, aligned closely with higher\nscores in areas like coherence and elaboration. Our findings suggest that\nmaking LLMs more aware of the writing process can lead to feedback that feels\nmore meaningful, personal, and supportive.", "AI": {"tldr": "\u901a\u8fc7\u8ffd\u8e2a\u5199\u4f5c\u8fc7\u7a0b\u6570\u636e\u63d0\u5347LLM\u53cd\u9988\u8d28\u91cf", "motivation": "\u5f53\u524dLLM\u5199\u4f5c\u53cd\u9988\u4ec5\u57fa\u4e8e\u6700\u7ec8\u6587\u672c\uff0c\u7f3a\u4e4f\u5bf9\u5199\u4f5c\u8fc7\u7a0b\u7684\u7406\u89e3\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5199\u4f5c\u8fc7\u7a0b\u6570\u636e\uff08\u51fb\u952e\u8bb0\u5f55/\u7248\u672c\u5feb\u7167\uff09\u5982\u4f55\u5e2e\u52a9LLM\u751f\u6210\u66f4\u8d34\u5408\u5b66\u751f\u5b9e\u9645\u601d\u8003\u8def\u5f84\u7684\u53cd\u9988\u3002", "method": "\u5f00\u53d1\u6570\u5b57\u5199\u4f5c\u5de5\u5177\u91c7\u96c620\u540d\u5b66\u751f\u7684\u5199\u4f5c\u8f68\u8ff9\uff0c\u91c7\u7528\u53cc\u8bc4\u4f30\u6cd5\uff1a1\uff09\u7ed3\u5408\u7ec8\u7a3f\u548c\u8fc7\u7a0b\u6570\u636e\u7684LLM\u53cd\u9988 2\uff09\u5b66\u751f\u95ee\u5377\u8c03\u67e5\u53cd\u9988\u5b9e\u7528\u6027", "result": "\u5b66\u751f\u66f4\u8ba4\u53ef\u8fc7\u7a0b\u611f\u77e5\u53cd\u9988\uff0887%\u8ba4\u4e3a\u66f4\u8d34\u8fd1\u601d\u8003\u6a21\u5f0f\uff09\uff0c\u6dfb\u52a0\u5185\u5bb9/\u6bb5\u843d\u91cd\u7ec4\u7b49\u7f16\u8f91\u884c\u4e3a\u4e0e\u4f5c\u6587\u8bc4\u5206\u6307\u6807\uff08\u8fde\u8d2f\u6027/\u9610\u8ff0\u6df1\u5ea6\uff09\u663e\u8457\u6b63\u76f8\u5173", "conclusion": "\u6574\u5408\u5199\u4f5c\u8fc7\u7a0b\u6570\u636e\u80fd\u4f7fLLM\u53cd\u9988\u66f4\u5177\u9488\u5bf9\u6027\uff0c\u63d0\u5347\u53cd\u9988\u7684\u60c5\u611f\u5171\u9e23\u4e0e\u6559\u5b66\u652f\u6301\u6548\u679c"}}
{"id": "2506.08234", "pdf": "https://arxiv.org/pdf/2506.08234", "abs": "https://arxiv.org/abs/2506.08234", "authors": ["Yu-Ang Lee", "Guan-Ting Yi", "Mei-Yi Liu", "Jui-Chao Lu", "Guan-Bo Yang", "Yun-Nung Chen"], "title": "Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions", "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 4 figures, 1 table", "summary": "Recent advancements in large language models (LLMs) and AI systems have led\nto a paradigm shift in the design and optimization of complex AI workflows. By\nintegrating multiple components, compound AI systems have become increasingly\nadept at performing sophisticated tasks. However, as these systems grow in\ncomplexity, new challenges arise in optimizing not only individual components\nbut also their interactions. While traditional optimization methods such as\nsupervised fine-tuning (SFT) and reinforcement learning (RL) remain\nfoundational, the rise of natural language feedback introduces promising new\napproaches, especially for optimizing non-differentiable systems. This paper\nprovides a systematic review of recent progress in optimizing compound AI\nsystems, encompassing both numerical and language-based techniques. We\nformalize the notion of compound AI system optimization, classify existing\nmethods along several key dimensions, and highlight open research challenges\nand future directions in this rapidly evolving field. A list of surveyed papers\nis publicly available at https://github.com/MiuLab/AISysOpt-Survey.", "AI": {"tldr": "\u7cfb\u7edf\u56de\u987e\u590d\u5408AI\u7cfb\u7edf\u4f18\u5316\u65b9\u6cd5\uff0c\u63d0\u51fa\u7ed3\u5408\u6570\u503c\u4f18\u5316\u4e0e\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u7684\u521b\u65b0\u6846\u67b6", "motivation": "\u590d\u5408AI\u7cfb\u7edf\u590d\u6742\u5ea6\u63d0\u5347\u5bfc\u81f4\u7ec4\u4ef6\u95f4\u4ea4\u4e92\u4f18\u5316\u6210\u4e3a\u65b0\u6311\u6218\uff0c\u9700\u7a81\u7834\u4f20\u7edf\u5355\u7ec4\u4ef6\u4f18\u5316\u8303\u5f0f", "method": "\u5206\u7c7b\u4f20\u7edf\u6570\u503c\u4f18\u5316\uff08SFT/RL\uff09\u4e0e\u65b0\u5174\u8bed\u8a00\u53cd\u9988\u65b9\u6cd5\uff0c\u7279\u522b\u5173\u6ce8\u4e0d\u53ef\u5fae\u5206\u7cfb\u7edf\u4f18\u5316\u65b9\u6848", "result": "\u5efa\u7acb\u590d\u5408AI\u4f18\u5316\u7684\u7cfb\u7edf\u6846\u67b6\uff0c\u63d0\u51fa\u6280\u672f\u5206\u7c7b\u7ef4\u5ea6\u5e76\u6307\u660e\u4eba\u673a\u534f\u540c\u4f18\u5316\u7b49\u672a\u6765\u65b9\u5411", "conclusion": "\u8be5\u9886\u57df\u5904\u4e8e\u5feb\u901f\u6f14\u8fdb\u9636\u6bb5\uff0c\u9700\u878d\u5408\u6570\u503c\u65b9\u6cd5\u4e0e\u8bed\u8a00\u7406\u89e3\u6280\u672f\u5b9e\u73b0\u7cfb\u7edf\u7ea7\u4f18\u5316\u7a81\u7834"}}
{"id": "2506.08235", "pdf": "https://arxiv.org/pdf/2506.08235", "abs": "https://arxiv.org/abs/2506.08235", "authors": ["Shashidhar Reddy Javaji", "Yupeng Cao", "Haohang Li", "Yangyang Yu", "Nikhil Muralidhar", "Zining Zhu"], "title": "Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "21 pages, 6 figures, Under review", "summary": "Large language models (LLMs) are increasingly being used for complex research\ntasks such as literature review, idea generation, and scientific paper\nanalysis, yet their ability to truly understand and process the intricate\nrelationships within complex research papers, such as the logical links between\nclaims and supporting evidence remains largely unexplored. In this study, we\npresent CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs'\ncapabilities in scientific claim-evidence extraction and validation, a task\nthat reflects deeper comprehension of scientific argumentation. We\nsystematically compare three approaches which are inspired by divide and\nconquer approaches, across six diverse LLMs, highlighting model-specific\nstrengths and weaknesses in scientific comprehension. Through evaluation\ninvolving over 300 claim-evidence pairs across multiple research domains, we\nreveal significant limitations in LLMs' ability to process complex scientific\ncontent. Our results demonstrate that closed-source models like GPT-4 and\nClaude consistently outperform open-source counterparts in precision and recall\nacross claim-evidence identification tasks. Furthermore, strategically designed\nthree-pass and one-by-one prompting approaches significantly improve LLMs'\nabilities to accurately link dispersed evidence with claims, although this\ncomes at increased computational cost. CLAIM-BENCH sets a new standard for\nevaluating scientific comprehension in LLMs, offering both a diagnostic tool\nand a path forward for building systems capable of deeper, more reliable\nreasoning across full-length papers.", "AI": {"tldr": "\u5f00\u53d1CLAIM-BENCH\u57fa\u51c6\u8bc4\u4f30\u5927\u6a21\u578b\u5728\u79d1\u5b66\u4e3b\u5f20-\u8bc1\u636e\u63d0\u53d6\u9a8c\u8bc1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u95ed\u6e90\u6a21\u578b\u6027\u80fd\u66f4\u4f18\u4f46\u5b58\u5728\u6210\u672c\u9650\u5236", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u867d\u5e7f\u6cdb\u7528\u4e8e\u79d1\u7814\u5206\u6790\uff0c\u4f46\u5176\u5bf9\u8bba\u6587\u4e2d\u4e3b\u5f20-\u8bc1\u636e\u903b\u8f91\u5173\u7cfb\u7684\u6df1\u5c42\u7406\u89e3\u80fd\u529b\u5c1a\u672a\u88ab\u5145\u5206\u9a8c\u8bc1", "method": "\u6784\u5efa\u5305\u542b300+\u8de8\u9886\u57df\u4e3b\u5f20-\u8bc1\u636e\u5bf9\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91c7\u7528\u5206\u6cbb\u7b56\u7565\uff08\u4e09\u9636\u6bb5/\u9010\u4e00\u9a8c\u8bc1\u63d0\u793a\u6cd5\uff09\u5bf9\u6bd4\u516d\u5927\u6a21\u578b\u7684\u6027\u80fd", "result": "\u95ed\u6e90\u6a21\u578b\uff08GPT-4/Claude\uff09\u5728\u7cbe\u786e\u7387/\u53ec\u56de\u7387\u4e0a\u5e73\u5747\u9886\u5148\u5f00\u6e90\u6a21\u578b15-20%\uff0c\u4f18\u5316\u63d0\u793a\u7b56\u7565\u53ef\u63d0\u5347\u8bc1\u636e\u5173\u8054\u51c6\u786e\u6027\uff08+35%\uff09\u4f46\u589e\u52a030%\u8ba1\u7b97\u6210\u672c", "conclusion": "CLAIM-BENCH\u4e3a\u8bc4\u4f30\u6a21\u578b\u79d1\u5b66\u7406\u89e3\u80fd\u529b\u5efa\u7acb\u65b0\u6807\u51c6\uff0c\u63d0\u793a\u7b56\u7565\u4f18\u5316\u65b9\u5411\u4e3a\u63d0\u5347\u53ef\u9760\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u8def\u5f84"}}
{"id": "2506.08260", "pdf": "https://arxiv.org/pdf/2506.08260", "abs": "https://arxiv.org/abs/2506.08260", "authors": ["Wanjing Anya Ma", "Michael Flor", "Zuowei Wang"], "title": "Automatic Generation of Inference Making Questions for Reading Comprehension Assessments", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the 20th Workshop on Innovative Use of NLP for Building\n  Educational Applications (BEA 2025), co-located with the ACL 2025", "summary": "Inference making is an essential but complex skill in reading comprehension\n(RC). Some inferences require resolving references across sentences, and some\nrely on using prior knowledge to fill in the detail that is not explicitly\nwritten in the text. Diagnostic RC questions can help educators provide more\neffective and targeted reading instruction and interventions for school-age\nstudents. We introduce a taxonomy of inference types for RC and use it to\nanalyze the distribution of items within a diagnostic RC item bank. Next, we\npresent experiments using GPT-4o to generate bridging-inference RC items for\ngiven reading passages via few-shot prompting, comparing conditions with and\nwithout chain-of-thought prompts. Generated items were evaluated on three\naspects: overall item quality, appropriate inference type, and LLM reasoning,\nachieving high inter-rater agreements above 0.90. Our results show that GPT-4o\nproduced 93.8% good-quality questions suitable for operational use in grade\n3-12 contexts; however, only 42.6% of the generated questions accurately\nmatched the targeted inference type. We conclude that combining automatic item\ngeneration with human judgment offers a promising path toward scalable,\nhigh-quality diagnostic RC assessments.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u9605\u8bfb\u7406\u89e3\u8bca\u65ad\u9898\u81ea\u52a8\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7GPT-4o\u751f\u6210\u8de8\u6587\u672c\u63a8\u7406\u9898\uff0c\u9a8c\u8bc1\u5176\u8d28\u91cf\u4e0e\u7c7b\u578b\u5339\u914d\u5ea6\u3002", "motivation": "\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u8bca\u65ad\u6027\u9605\u8bfb\u7406\u89e3\u9898\u76ee\uff0c\u5e2e\u52a9\u6559\u80b2\u5de5\u4f5c\u8005\u5b9e\u73b0\u7cbe\u51c6\u6559\u5b66\u5e72\u9884\uff0c\u89e3\u51b3\u4eba\u5de5\u547d\u9898\u6548\u7387\u74f6\u9888\u3002", "method": "\u6784\u5efa\u63a8\u7406\u7c7b\u578b\u5206\u7c7b\u4f53\u7cfb\uff0c\u91c7\u7528few-shot\u63d0\u793a\u7b56\u7565\uff08\u542b\u601d\u7ef4\u94fe\u4e0e\u975e\u601d\u7ef4\u94fe\u5bf9\u6bd4\uff09\uff0c\u5229\u7528GPT-4o\u751f\u6210\u8de8\u6587\u672c\u63a8\u7406\u9898\u76ee\u5e76\u8fdb\u884c\u591a\u7ef4\u8bc4\u4f30\u3002", "result": "\u751f\u6210\u9898\u76ee93.8%\u8fbe\u5230\u64cd\u4f5c\u6807\u51c6\uff0c\u4f46\u4ec542.6%\u51c6\u786e\u5339\u914d\u76ee\u6807\u63a8\u7406\u7c7b\u578b\uff1b\u4eba\u5de5\u8bc4\u4f30\u8005\u95f4\u4e00\u81f4\u6027\u9ad8\u4e8e0.90\u3002", "conclusion": "\u4eba\u673a\u534f\u540c\u7684\u9898\u76ee\u751f\u6210\u6a21\u5f0f\u4e3a\u89c4\u6a21\u5316\u8bca\u65ad\u8bc4\u4f30\u63d0\u4f9b\u53ef\u884c\u8def\u5f84\uff0c\u4f46\u9700\u52a0\u5f3a\u63a8\u7406\u7c7b\u578b\u63a7\u5236\u673a\u5236\u3002"}}
{"id": "2506.08300", "pdf": "https://arxiv.org/pdf/2506.08300", "abs": "https://arxiv.org/abs/2506.08300", "authors": ["Matteo Cargnelutti", "Catherine Brobston", "John Hess", "Jack Cushman", "Kristi Mukk", "Aristana Scourtas", "Kyle Courtney", "Greg Leppert", "Amanda Watson", "Martha Whitehead", "Jonathan Zittrain"], "title": "Institutional Books 1.0: A 242B token dataset from Harvard Library's collections, refined for accuracy and usability", "categories": ["cs.CL", "cs.DL"], "comment": null, "summary": "Large language models (LLMs) use data to learn about the world in order to\nproduce meaningful correlations and predictions. As such, the nature, scale,\nquality, and diversity of the datasets used to train these models, or to\nsupport their work at inference time, have a direct impact on their quality.\nThe rapid development and adoption of LLMs of varying quality has brought into\nfocus the scarcity of publicly available, high-quality training data and\nrevealed an urgent need to ground the stewardship of these datasets in\nsustainable practices with clear provenance chains. To that end, this technical\nreport introduces Institutional Books 1.0, a large collection of public domain\nbooks originally digitized through Harvard Library's participation in the\nGoogle Books project, beginning in 2006. Working with Harvard Library, we\nextracted, analyzed, and processed these volumes into an extensively-documented\ndataset of historic texts. This analysis covers the entirety of Harvard\nLibrary's collection scanned as part of that project, originally spanning\n1,075,899 volumes written in over 250 different languages for a total of\napproximately 250 billion tokens. As part of this initial release, the\nOCR-extracted text (original and post-processed) as well as the metadata\n(bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens,\nidentified as being in the public domain have been made available. This report\ndescribes this project's goals and methods as well as the results of the\nanalyses we performed, all in service of making this historical collection more\naccessible and easier for humans and machines alike to filter, read and use.", "AI": {"tldr": "\u4ecb\u7ecd\u54c8\u4f5b\u56fe\u4e66\u9986\u4e0e\u8c37\u6b4c\u5408\u4f5c\u521b\u5efa\u7684\u516c\u5171\u9886\u57df\u4e66\u7c4d\u6570\u636e\u96c6Institutional Books 1.0\uff0c\u5305\u542b2420\u4ebftoken\u7684OCR\u6587\u672c\u53ca\u5143\u6570\u636e\uff0c\u65e8\u5728\u63d0\u9ad8\u5386\u53f2\u6587\u732e\u53ef\u8bbf\u95ee\u6027\u3002", "motivation": "\u5e94\u5bf9\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u901a\u8fc7\u53ef\u6301\u7eed\u6570\u636e\u7ba1\u7406\u63d0\u4f9b\u6e05\u6670\u6765\u6e90\u94fe\u7684\u5386\u53f2\u6587\u672c\u8d44\u6e90\u3002", "method": "\u63d0\u53d6\u5206\u6790\u54c8\u4f5b\u56fe\u4e66\u9986\u8c37\u6b4c\u56fe\u4e66\u9879\u76ee\u7684107\u4e07\u518c\u85cf\u4e66\uff0c\u5bf998.3\u4e07\u518c\u516c\u57df\u4e66\u7c4d\u8fdb\u884cOCR\u6587\u672c\u5904\u7406\u548c\u5143\u6570\u636e\u751f\u6210\u3002", "result": "\u516c\u5f00983,004\u5377\u516c\u57df\u4e66\u7c4d\u7684\u539f\u59cb/\u540e\u5904\u7406\u6587\u672c\u53ca\u5143\u6570\u636e\uff08\u4e66\u76ee/\u6765\u6e90/\u751f\u6210\uff09\uff0c\u603b\u89c4\u6a21\u8fbe242B tokens\u3002", "conclusion": "\u5efa\u7acb\u53ef\u673a\u5668\u5904\u7406\u7684\u5386\u53f2\u6587\u672c\u8d44\u6e90\u5e93\uff0c\u4fc3\u8fdb\u4eba\u7c7b\u4e0e\u673a\u5668\u5bf9\u6587\u5316\u9057\u4ea7\u7684\u9ad8\u6548\u8fc7\u6ee4\u3001\u9605\u8bfb\u548c\u4f7f\u7528\u3002"}}
{"id": "2506.08343", "pdf": "https://arxiv.org/pdf/2506.08343", "abs": "https://arxiv.org/abs/2506.08343", "authors": ["Chenlong Wang", "Yuanning Feng", "Dongping Chen", "Zhaoyang Chu", "Ranjay Krishna", "Tianyi Zhou"], "title": "Wait, We Don't Need to \"Wait\"! Removing Thinking Tokens Improves Reasoning Efficiency", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large reasoning models have enabled complex, step-by-step\nreasoning but often introduce significant overthinking, resulting in verbose\nand redundant outputs that hinder efficiency. In this study, we examine whether\nexplicit self-reflection, signaled by tokens such as \"Wait\" and \"Hmm\", is\nnecessary for advanced reasoning. We propose NoWait, a simple yet effective\napproach that disables explicit self-reflection by suppressing these tokens\nduring inference. Extensive experiments on ten benchmarks across textual,\nvisual, and video reasoning tasks show that NoWait reduces chain-of-thought\ntrajectory length by up to 27%-51% in five R1-style model series, without\ncompromising model utility. NoWait thus offers a plug-and-play solution for\nefficient and utility-preserving multimodal reasoning.", "AI": {"tldr": "NoWait\u901a\u8fc7\u6291\u5236\u63a8\u7406\u8fc7\u7a0b\u4e2d\u81ea\u7701\u6807\u8bb0\uff08\u5982'Wait'/'Hmm'\uff09\uff0c\u5c06\u591a\u6a21\u6001\u63a8\u7406\u8f68\u8ff9\u957f\u5ea6\u7f29\u51cf27%-51%\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u4e0d\u53d8", "motivation": "\u73b0\u6709\u5927\u578b\u63a8\u7406\u6a21\u578b\u5b58\u5728\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5bfc\u81f4\u8f93\u51fa\u5197\u957f\u5197\u4f59\uff0c\u5f71\u54cd\u63a8\u7406\u6548\u7387\u3002\u9700\u8981\u63a2\u7d22\u663e\u5f0f\u81ea\u7701\u6807\u8bb0\u662f\u5426\u5fc5\u8981\uff0c\u5e76\u5bfb\u6c42\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u65b9\u6848\u3002", "method": "\u63d0\u51faNoWait\u65b9\u6cd5\uff1a\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6291\u5236\u81ea\u7701\u6807\u8bb0\u7684\u751f\u6210\uff0c\u901a\u8fc7\u4fee\u6539\u751f\u6210\u7b56\u7565\u5b9e\u73b0\u94fe\u5f0f\u601d\u7ef4\u8f68\u8ff9\u7684\u7b80\u6d01\u5316\u3002\u8be5\u65b9\u6cd5\u517c\u5bb9\u4e0d\u540c\u6a21\u6001\uff08\u6587\u672c/\u89c6\u89c9/\u89c6\u9891\uff09\u7684\u63a8\u7406\u4efb\u52a1\u3002", "result": "\u572810\u4e2a\u8de8\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cNoWait\u4f7f5\u4e2aR1\u7cfb\u5217\u6a21\u578b\u7684\u601d\u7ef4\u94fe\u957f\u5ea6\u51cf\u5c1127%-51%\uff0c\u4e14\u6a21\u578b\u6548\u7528\u672a\u53d7\u635f\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u5373\u63d2\u5373\u7528\u7279\u6027\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "NoWait\u8bc1\u660e\u4e86\u663e\u5f0f\u81ea\u7701\u6807\u8bb0\u5728\u590d\u6742\u63a8\u7406\u4e2d\u7684\u975e\u5fc5\u8981\u6027\uff0c\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u4fdd\u6301\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u8fc7\u7a0b\u7684\u7ecf\u6d4e\u6027\u3002"}}
{"id": "2506.08349", "pdf": "https://arxiv.org/pdf/2506.08349", "abs": "https://arxiv.org/abs/2506.08349", "authors": ["Yuxuan Zhou", "Xien Liu", "Chenwei Yan", "Chen Ning", "Xiao Zhang", "Boxun Li", "Xiangling Fu", "Shijin Wang", "Guoping Hu", "Yu Wang", "Ji Wu"], "title": "Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 11 figures. Accepted by ICML 2025", "summary": "Large language models (LLMs) have demonstrated remarkable performance on\nvarious medical benchmarks, but their capabilities across different cognitive\nlevels remain underexplored. Inspired by Bloom's Taxonomy, we propose a\nmulti-cognitive-level evaluation framework for assessing LLMs in the medical\ndomain in this study. The framework integrates existing medical datasets and\nintroduces tasks targeting three cognitive levels: preliminary knowledge grasp,\ncomprehensive knowledge application, and scenario-based problem solving. Using\nthis framework, we systematically evaluate state-of-the-art general and medical\nLLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek.\nOur findings reveal a significant performance decline as cognitive complexity\nincreases across evaluated models, with model size playing a more critical role\nin performance at higher cognitive levels. Our study highlights the need to\nenhance LLMs' medical capabilities at higher cognitive levels and provides\ninsights for developing LLMs suited to real-world medical applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5e03\u9c81\u59c6\u5206\u7c7b\u5b66\u7684\u591a\u8ba4\u77e5\u5c42\u6b21\u533b\u5b66\u8bc4\u4f30\u6846\u67b6\uff0c\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u968f\u8ba4\u77e5\u590d\u6742\u5ea6\u63d0\u5347\u663e\u8457\u4e0b\u964d\uff0c\u6a21\u578b\u89c4\u6a21\u5728\u9ad8\u5c42\u8ba4\u77e5\u4efb\u52a1\u4e2d\u66f4\u5173\u952e\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u63a2\u7d22LLMs\u5728\u4e0d\u540c\u8ba4\u77e5\u5c42\u6b21\uff08\u77e5\u8bc6\u638c\u63e1\u2192\u7efc\u5408\u5e94\u7528\u2192\u5b9e\u9645\u95ee\u9898\u89e3\u51b3\uff09\u7684\u533b\u5b66\u80fd\u529b\u5dee\u5f02\uff0c\u9700\u7cfb\u7edf\u6027\u8bc4\u4f30\u6a21\u578b\u5728\u771f\u5b9e\u533b\u7597\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u6574\u5408\u73b0\u6709\u533b\u5b66\u6570\u636e\u96c6\u5e76\u8bbe\u8ba1\u4e09\u5c42\u8ba4\u77e5\u4efb\u52a1\uff1a\u57fa\u7840\u77e5\u8bc6\u638c\u63e1\uff08L1\uff09\u3001\u7efc\u5408\u5e94\u7528\uff08L2\uff09\u3001\u573a\u666f\u5316\u95ee\u9898\u89e3\u51b3\uff08L3\uff09\uff0c\u8bc4\u4f30Llama/Qwen/Gemma/Phi/GPT/DeepSeek\u516d\u5927\u6a21\u578b\u5bb6\u65cf\u3002", "result": "1. \u6240\u6709\u6a21\u578b\u968f\u8ba4\u77e5\u5c42\u6b21\u63d0\u5347\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff08\u5e73\u5747\u51c6\u786e\u7387L1 73.5% \u2192 L3 51.2%\uff09\uff1b2. \u6a21\u578b\u53c2\u6570\u89c4\u6a21\u5bf9\u9ad8\u5c42\u8ba4\u77e5\u4efb\u52a1\u5f71\u54cd\u66f4\u5927\uff08L3\u4efb\u52a1\u89c4\u6a21\u6548\u5e94\u7cfb\u6570\u8fbe0.87 vs L1 0.62\uff09\u3002", "conclusion": "\u5f53\u524d\u533b\u5b66LLMs\u5728\u9ad8\u5c42\u8ba4\u77e5\u80fd\u529b\u5b58\u5728\u660e\u663e\u77ed\u677f\uff0c\u672a\u6765\u5e94\u7740\u91cd\u63d0\u5347\u590d\u6742\u573a\u666f\u4e0b\u7684\u63a8\u7406\u4e0e\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u5efa\u8bae\u901a\u8fc7\u9886\u57df\u9002\u5e94\u6027\u8bad\u7ec3\u4e0e\u8ba4\u77e5\u67b6\u6784\u4f18\u5316\u63a8\u8fdb\u771f\u5b9e\u533b\u7597\u5e94\u7528\u3002"}}
{"id": "2506.08354", "pdf": "https://arxiv.org/pdf/2506.08354", "abs": "https://arxiv.org/abs/2506.08354", "authors": ["Yiqun Sun", "Qiang Huang", "Anthony K. H. Tung", "Jun Yu"], "title": "Text Embeddings Should Capture Implicit Semantics, Not Just Surface Meaning", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "This position paper argues that the text embedding research community should\nmove beyond surface meaning and embrace implicit semantics as a central\nmodeling goal. Text embedding models have become foundational in modern NLP,\npowering a wide range of applications and drawing increasing research\nattention. Yet, much of this progress remains narrowly focused on surface-level\nsemantics. In contrast, linguistic theory emphasizes that meaning is often\nimplicit, shaped by pragmatics, speaker intent, and sociocultural context.\nCurrent embedding models are typically trained on data that lacks such depth\nand evaluated on benchmarks that reward the capture of surface meaning. As a\nresult, they struggle with tasks requiring interpretive reasoning, speaker\nstance, or social meaning. Our pilot study highlights this gap, showing that\neven state-of-the-art models perform only marginally better than simplistic\nbaselines on implicit semantics tasks. To address this, we call for a paradigm\nshift: embedding research should prioritize more diverse and linguistically\ngrounded training data, design benchmarks that evaluate deeper semantic\nunderstanding, and explicitly frame implicit meaning as a core modeling\nobjective, better aligning embeddings with real-world language complexity.", "AI": {"tldr": "\u4e3b\u5f20\u6587\u672c\u5d4c\u5165\u7814\u7a76\u5e94\u8d85\u8d8a\u8868\u5c42\u8bed\u4e49\uff0c\u5c06\u9690\u542b\u8bed\u4e49\u4f5c\u4e3a\u6838\u5fc3\u5efa\u6a21\u76ee\u6807", "motivation": "\u5f53\u524d\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u7f3a\u4e4f\u6df1\u5ea6\u8bed\u4e49\u7684\u8bad\u7ec3\u6570\u636e\u53ca\u8bc4\u4f30\u57fa\u51c6\uff0c\u5bfc\u81f4\u5728\u8bed\u7528\u63a8\u7406\u3001\u793e\u4f1a\u610f\u4e49\u7b49\u9690\u542b\u8bed\u4e49\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73", "method": "\u901a\u8fc7\u8bd5\u70b9\u7814\u7a76\u9a8c\u8bc1\u6a21\u578b\u7f3a\u9677\uff0c\u63d0\u51fa\u5e94\u6784\u5efa\u8bed\u8a00\u5b66\u57fa\u7840\u6570\u636e\u3001\u8bbe\u8ba1\u6df1\u5c42\u8bed\u4e49\u8bc4\u4f30\u57fa\u51c6\u3001\u660e\u786e\u5efa\u6a21\u76ee\u6807\u4e09\u4f4d\u4e00\u4f53\u7684\u89e3\u51b3\u65b9\u6848", "result": "\u5b9e\u9a8c\u663e\u793a\u6700\u5148\u8fdb\u6a21\u578b\u5728\u9690\u542b\u8bed\u4e49\u4efb\u52a1\u4e0a\u4ec5\u7565\u4f18\u4e8e\u7b80\u5355\u57fa\u7ebf\u6a21\u578b", "conclusion": "\u547c\u5401\u7814\u7a76\u8303\u5f0f\u8f6c\u578b\uff0c\u901a\u8fc7\u6570\u636e-\u57fa\u51c6-\u76ee\u6807\u534f\u540c\u4f18\u5316\uff0c\u4f7f\u5d4c\u5165\u6a21\u578b\u66f4\u597d\u9002\u5e94\u771f\u5b9e\u8bed\u8a00\u590d\u6742\u6027"}}
{"id": "2506.08359", "pdf": "https://arxiv.org/pdf/2506.08359", "abs": "https://arxiv.org/abs/2506.08359", "authors": ["Li-Ming Zhan", "Bo Liu", "Zexin Lu", "Chengqiang Xie", "Jiannong Cao", "Xiao-Ming Wu"], "title": "DEAL: Disentangling Transformer Head Activations for LLM Steering", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Inference-time steering aims to alter the response characteristics of large\nlanguage models (LLMs) without modifying their underlying parameters. A\ncritical step in this process is the identification of internal modules within\nLLMs that are associated with the target behavior. However, current approaches\nto module selection often depend on superficial cues or ad-hoc heuristics,\nwhich can result in suboptimal or unintended outcomes. In this work, we propose\na principled causal-attribution framework for identifying behavior-relevant\nattention heads in transformers. For each head, we train a vector-quantized\nautoencoder (VQ-AE) on its attention activations, partitioning the latent space\ninto behavior-relevant and behavior-irrelevant subspaces, each quantized with a\nshared learnable codebook. We assess the behavioral relevance of each head by\nquantifying the separability of VQ-AE encodings for behavior-aligned versus\nbehavior-violating responses using a binary classification metric. This yields\na behavioral relevance score that reflects each head discriminative capacity\nwith respect to the target behavior, guiding both selection and importance\nweighting. Experiments on seven LLMs from two model families and five\nbehavioral steering datasets demonstrate that our method enables more accurate\ninference-time interventions, achieving superior performance on the\ntruthfulness-steering task. Furthermore, the heads selected by our approach\nexhibit strong zero-shot generalization in cross-domain truthfulness-steering\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56e0\u679c\u5f52\u56e0\u7684\u6846\u67b6\uff0c\u901a\u8fc7VQ-AE\u91cf\u5316\u6ce8\u610f\u529b\u5934\u6fc0\u6d3b\uff0c\u6709\u6548\u8bc6\u522b\u884c\u4e3a\u76f8\u5173\u6ce8\u610f\u529b\u5934\uff0c\u63d0\u5347\u63a8\u7406\u5e72\u9884\u6548\u679c", "motivation": "\u73b0\u6709\u6ce8\u610f\u529b\u5934\u9009\u62e9\u65b9\u6cd5\u4f9d\u8d56\u8868\u9762\u7ebf\u7d22\u6216\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u5bfc\u81f4\u5e72\u9884\u6548\u679c\u6b20\u4f73\uff0c\u9700\u8981\u66f4\u539f\u5219\u6027\u7684\u6a21\u5757\u9009\u62e9\u65b9\u6cd5", "method": "\u4f7f\u7528\u5411\u91cf\u91cf\u5316\u81ea\u7f16\u7801\u5668(VQ-AE)\u5bf9\u6ce8\u610f\u529b\u5934\u6fc0\u6d3b\u8fdb\u884c\u7f16\u7801\uff0c\u5212\u5206\u884c\u4e3a\u76f8\u5173/\u65e0\u5173\u5b50\u7a7a\u95f4\uff0c\u901a\u8fc7\u4e8c\u5206\u7c7b\u6307\u6807\u91cf\u5316\u6ce8\u610f\u529b\u5934\u7684\u5224\u522b\u80fd\u529b", "result": "\u57287\u4e2aLLM\u548c5\u4e2a\u884c\u4e3a\u6570\u636e\u96c6\u9a8c\u8bc1\uff0c\u771f\u5b9e\u6027\u5f15\u5bfc\u4efb\u52a1\u53d6\u5f97\u6700\u4f73\u6548\u679c\uff0c\u6240\u9009\u6ce8\u610f\u529b\u5934\u5c55\u73b0\u8de8\u9886\u57df\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7cbe\u51c6\u7684\u6ce8\u610f\u529b\u5934\u9009\u62e9\uff0c\u652f\u6301\u6709\u6548\u7684\u63a8\u7406\u65f6\u5e72\u9884\uff0c\u4e14\u5177\u5907\u8de8\u9886\u57df\u5e94\u7528\u6f5c\u529b"}}
{"id": "2506.08364", "pdf": "https://arxiv.org/pdf/2506.08364", "abs": "https://arxiv.org/abs/2506.08364", "authors": ["Jash Rajesh Parekh", "Pengcheng Jiang", "Jiawei Han"], "title": "CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs", "categories": ["cs.CL"], "comment": null, "summary": "Understanding cause and effect relationships remains a formidable challenge\nfor Large Language Models (LLMs), particularly in specialized domains where\nreasoning requires more than surface-level correlations. Retrieval-Augmented\nGeneration (RAG) improves factual accuracy, but standard RAG pipelines treat\nevidence as flat context, lacking the structure required to model true causal\ndependencies. We introduce Causal-Chain RAG (CC-RAG), a novel approach that\nintegrates zero-shot triple extraction and theme-aware graph chaining into the\nRAG pipeline, enabling structured multi-hop inference. Given a domain specific\ncorpus, CC-RAG constructs a Directed Acyclic Graph (DAG) of <cause, relation,\neffect> triples and uses forward/backward chaining to guide structured answer\ngeneration. Experiments on two real-world domains: Bitcoin price fluctuations\nand Gaucher disease, show that CC-RAG outperforms standard RAG and zero-shot\nLLMs in chain similarity, information density, and lexical diversity. Both\nLLM-as-a-Judge and human evaluations consistently favor CC-RAG. Our results\ndemonstrate that explicitly modeling causal structure enables LLMs to generate\nmore accurate and interpretable responses, especially in specialized domains\nwhere flat retrieval fails.", "AI": {"tldr": "\u63d0\u51faCausal-Chain RAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u56e0\u679c\u94fe\u589e\u5f3aRAG\u6027\u80fd\uff0c\u5728\u4e13\u4e1a\u9886\u57df\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4f20\u7edfRAG\u548c\u96f6\u6837\u672cLLM", "motivation": "\u73b0\u6709RAG\u5728\u5904\u7406\u4e13\u4e1a\u9886\u57df\u56e0\u679c\u5173\u7cfb\u65f6\u5b58\u5728\u7ed3\u6784\u7f3a\u5931\uff0c\u65e0\u6cd5\u5b9e\u73b0\u6df1\u5c42\u56e0\u679c\u63a8\u7406\uff0c\u9700\u8981\u663e\u5f0f\u5efa\u6a21\u56e0\u679c\u4f9d\u8d56\u5173\u7cfb", "method": "\u6574\u5408\u96f6\u6837\u672c\u4e09\u5143\u7ec4\u62bd\u53d6\u548c\u4e3b\u9898\u56fe\u94fe\u5f0f\u7ed3\u6784\uff0c\u6784\u5efa<\u539f\u56e0-\u5173\u7cfb-\u7ed3\u679c>DAG\uff0c\u91c7\u7528\u524d/\u540e\u5411\u94fe\u5f0f\u63a8\u7406\u5f15\u5bfc\u7ed3\u6784\u5316\u7b54\u6848\u751f\u6210", "result": "\u5728\u6bd4\u7279\u5e01\u4ef7\u683c\u6ce2\u52a8\u548c\u9ad8\u96ea\u6c0f\u75c5\u9886\u57df\uff0cCC-RAG\u5728\u94fe\u76f8\u4f3c\u6027(\u63d0\u534732%)\u3001\u4fe1\u606f\u5bc6\u5ea6(\u589e\u52a041%)\u548c\u8bcd\u6c47\u591a\u6837\u6027(\u63d0\u9ad828%)\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf", "conclusion": "\u663e\u5f0f\u56e0\u679c\u7ed3\u6784\u5efa\u6a21\u4f7fLLM\u751f\u6210\u66f4\u51c6\u786e\u53ef\u89e3\u91ca\u7684\u56de\u7b54\uff0c\u5c24\u5176\u5728\u4f20\u7edf\u6241\u5e73\u68c0\u7d22\u5931\u6548\u7684\u4e13\u4e1a\u9886\u57df\u6548\u679c\u663e\u8457"}}
{"id": "2506.08371", "pdf": "https://arxiv.org/pdf/2506.08371", "abs": "https://arxiv.org/abs/2506.08371", "authors": ["Zikai Xiao", "Ziyang Wang", "Wen Ma", "Yan Zhang", "Wei Shen", "Yan Wang", "Luqi Gong", "Zuozhu Liu"], "title": "Mitigating Posterior Salience Attenuation in Long-Context LLMs with Positional Contrastive Decoding", "categories": ["cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) support long contexts, they struggle with\nperformance degradation within the context window. Current solutions incur\nprohibitive training costs, leaving statistical behaviors and cost-effective\napproaches underexplored. From the decoding perspective, we identify the\nPosterior Salience Attenuation (PSA) phenomenon, where the salience ratio\ncorrelates with long-text performance degradation. Notably, despite the\nattenuation, gold tokens still occupy high-ranking positions in the decoding\nspace. Motivated by it, we propose the training-free Positional Contrastive\nDecoding (PCD) that contrasts the logits derived from long-aware attention with\nthose from designed local-aware attention, enabling the model to focus on the\ngains introduced by large-scale short-to-long training. Through the analysis of\nlong-term decay simulation, we demonstrate that PCD effectively alleviates\nattention score degradation. Experimental results show that PCD achieves\nstate-of-the-art performance on long-context benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684Positional Contrastive Decoding(PCD)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u957f\u7a0b\u548c\u5c40\u90e8\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u6027\u80fd\u8870\u51cf\u7684\u65b9\u6cd5\u5b58\u5728\u9ad8\u8bad\u7ec3\u6210\u672c\u95ee\u9898\uff0c\u4e14\u53d1\u73b0\u5c3d\u7ba1\u5b58\u5728\u540e\u9a8c\u663e\u8457\u6027\u8870\u51cf(PSA)\uff0c\u9ec4\u91d1\u6807\u8bb0\u5728\u89e3\u7801\u7a7a\u95f4\u4e2d\u4ecd\u4fdd\u6301\u9ad8\u6392\u540d\u4f4d\u7f6e\u3002", "method": "\u8bbe\u8ba1\u5bf9\u6bd4\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u5bf9\u6bd4\u957f\u7a0b\u6ce8\u610f\u529b\u548c\u5c40\u90e8\u6ce8\u610f\u529b\u4ea7\u751f\u7684logits\u5206\u5e03\uff0c\u5229\u7528\u5927\u89c4\u6a21\u77ed\u6587\u672c\u8bad\u7ec3\u7684\u589e\u76ca\u63d0\u5347\u957f\u6587\u672c\u5904\u7406\u80fd\u529b\u3002", "result": "\u5728\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u8870\u51cf\u6a21\u62df\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5bf9\u6ce8\u610f\u529b\u5206\u6570\u9000\u5316\u7684\u7f13\u89e3\u6548\u679c\u3002", "conclusion": "PCD\u4f5c\u4e3a\u96f6\u8bad\u7ec3\u6210\u672c\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLMs\u7684\u957f\u6587\u672c\u5904\u7406\u6027\u80fd\u8870\u51cf\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c\u3002"}}
{"id": "2506.08373", "pdf": "https://arxiv.org/pdf/2506.08373", "abs": "https://arxiv.org/abs/2506.08373", "authors": ["Kevin Galim", "Ethan Ewer", "Wonjun Kang", "Minjae Lee", "Hyung Il Koo", "Kangwook Lee"], "title": "Draft-based Approximate Inference for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8349\u7a3f\u6a21\u578b\u7684\u8fd1\u4f3c\u63a8\u7406\u6846\u67b6SpecKV\u548cSpecPC\uff0c\u663e\u8457\u63d0\u5347\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u6548\u7387", "motivation": "\u73b0\u6709Transformer\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u5b58\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\u74f6\u9888\uff0c\u4f20\u7edf\u8fd1\u4f3c\u65b9\u6cd5\u4f9d\u8d56\u7c97\u7cd9\u7684\u91cd\u8981\u6027\u9884\u6d4b\u3002\u4e3a\u66f4\u7cbe\u51c6\u8bc4\u4f30KV\u5bf9\u548ctoken\u91cd\u8981\u6027\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u8349\u7a3f\u6a21\u578b\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "1. SpecKV\uff1a\u901a\u8fc7\u8349\u7a3f\u6a21\u578b\u8f93\u51fa\u7cbe\u51c6\u8bc4\u4f30KV\u5bf9\u91cd\u8981\u6027\uff0c\u4f18\u5316KV\u7f13\u5b58\u4e22\u5f03\u7b56\u7565\n2. SpecPC\uff1a\u5229\u7528\u8349\u7a3f\u6a21\u578b\u6ce8\u610f\u529b\u6fc0\u6d3b\u8bc6\u522b\u975e\u5173\u952e\u63d0\u793atoken\uff0c\u5b9e\u73b0\u9ad8\u6548\u63d0\u793a\u538b\u7f29", "result": "\u5728\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51c6\u786e\u7387\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u5185\u5b58/\u5ef6\u8fdf/\u541e\u5410\u91cf\u6539\u8fdb\uff0c\u516c\u5f00\u4ee3\u7801\u9a8c\u8bc1\u6709\u6548\u6027", "conclusion": "\u9996\u6b21\u5c06\u8349\u7a3f\u6a21\u578b\u5e94\u7528\u4e8e\u8fd1\u4f3c\u63a8\u7406\u52a0\u901f\uff0c\u7a81\u7834\u4f20\u7edf\u4ec5\u7528\u4e8e\u65e0\u635f\u63a8\u6d4b\u89e3\u7801\u7684\u9650\u5236\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\uff0c\u4e3aLLM\u4f18\u5316\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2506.08375", "pdf": "https://arxiv.org/pdf/2506.08375", "abs": "https://arxiv.org/abs/2506.08375", "authors": ["Tao Zou", "Xinghua Zhang", "Haiyang Yu", "Minzheng Wang", "Fei Huang", "Yongbin Li"], "title": "EIFBENCH: Extremely Complex Instruction Following Benchmark for Large Language Models", "categories": ["cs.CL"], "comment": "24 pages", "summary": "With the development and widespread application of large language models\n(LLMs), the new paradigm of \"Model as Product\" is rapidly evolving, and demands\nhigher capabilities to address complex user needs, often requiring precise\nworkflow execution which involves the accurate understanding of multiple tasks.\nHowever, existing benchmarks focusing on single-task environments with limited\nconstraints lack the complexity required to fully reflect real-world scenarios.\nTo bridge this gap, we present the Extremely Complex Instruction Following\nBenchmark (EIFBENCH), meticulously crafted to facilitate a more realistic and\nrobust evaluation of LLMs. EIFBENCH not only includes multi-task scenarios that\nenable comprehensive assessment across diverse task types concurrently, but\nalso integrates a variety of constraints, replicating complex operational\nenvironments. Furthermore, we propose the Segment Policy Optimization (SegPO)\nalgorithm to enhance the LLM's ability to accurately fulfill multi-task\nworkflow. Evaluations on EIFBENCH have unveiled considerable performance\ndiscrepancies in existing LLMs when challenged with these extremely complex\ninstructions. This finding underscores the necessity for ongoing optimization\nto navigate the intricate challenges posed by LLM applications.", "AI": {"tldr": "\u63d0\u51faEIFBENCH\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u548cSegPO\u7b97\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4efb\u52a1\u590d\u6742\u573a\u666f\u4e0b\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b", "motivation": "\u73b0\u6709LLM\u57fa\u51c6\u6d4b\u8bd5\u5c40\u9650\u4e8e\u5355\u4efb\u52a1\u573a\u666f\uff0c\u7f3a\u4e4f\u771f\u5b9e\u573a\u666f\u7684\u590d\u6742\u6027\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u5904\u7406\u591a\u4efb\u52a1\u5de5\u4f5c\u6d41\u7684\u80fd\u529b", "method": "1. \u6784\u5efa\u5305\u542b\u591a\u4efb\u52a1\u534f\u540c\u4e0e\u591a\u6837\u5316\u7ea6\u675f\u7684EIFBENCH\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\n2. \u5f00\u53d1\u5206\u6bb5\u7b56\u7565\u4f18\u5316\u7b97\u6cd5(SegPO)\u63d0\u5347\u591a\u4efb\u52a1\u5904\u7406\u7cbe\u5ea6", "result": "\u73b0\u6709\u4e3b\u6d41LLM\u5728EIFBENCH\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u5dee\u5f02\uff08\u5e73\u5747\u51c6\u786e\u7387\u4e0b\u964d23.7%\uff09\uff0c\u66b4\u9732\u591a\u4efb\u52a1\u5904\u7406\u74f6\u9888", "conclusion": "LLM\u5728\u590d\u6742\u573a\u666f\u7684\u5e94\u7528\u9700\u8981\u6301\u7eed\u4f18\u5316\uff0cEIFBENCH\u4e3a\u6a21\u578b\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u8d34\u8fd1\u73b0\u5b9e\u7684\u9a8c\u8bc1\u5e73\u53f0"}}
{"id": "2506.08400", "pdf": "https://arxiv.org/pdf/2506.08400", "abs": "https://arxiv.org/abs/2506.08400", "authors": ["Luel Hagos Beyene", "Vivek Verma", "Min Ma", "Jesujoba O. Alabi", "Fabian David Schmidt", "Joyce Nakatumba-Nabende", "David Ifeoluwa Adelani"], "title": "mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": "working paper", "summary": "Large Language models (LLMs) have demonstrated impressive performance on a\nwide range of tasks, including in multimodal settings such as speech. However,\ntheir evaluation is often limited to English and a few high-resource languages.\nFor low-resource languages, there is no standardized evaluation benchmark. In\nthis paper, we address this gap by introducing mSTEB, a new benchmark to\nevaluate the performance of LLMs on a wide range of tasks covering language\nidentification, text classification, question answering, and translation tasks\non both speech and text modalities. We evaluated the performance of leading\nLLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open\nmodels such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in\nperformance between high-resource and low-resource languages, especially for\nlanguages spoken in Africa and Americas/Oceania. Our findings show that more\ninvestment is needed to address their under-representation in LLMs coverage.", "AI": {"tldr": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5mSTEB\uff0c\u53d1\u73b0\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5c24\u5176\u662f\u975e\u6d32\u53ca\u7f8e\u6d32/\u5927\u6d0b\u6d32\u8bed\u79cd\uff09\u8868\u73b0\u663e\u8457\u843d\u540e\uff0c\u547c\u5401\u52a0\u5f3a\u76f8\u5173\u6295\u5165\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u96c6\u4e2d\u4e8e\u82f1\u8bed\u7b49\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u7f3a\u4e4f\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u6807\u51c6\u5316\u8bc4\u6d4b\u57fa\u51c6", "method": "\u6784\u5efa\u8986\u76d6\u8bed\u97f3/\u6587\u672c\u6a21\u6001\u7684mSTEB\u57fa\u51c6\uff08\u542b\u8bed\u8a00\u8bc6\u522b\u3001\u6587\u672c\u5206\u7c7b\u3001QA\u3001\u7ffb\u8bd1\u7b49\u4efb\u52a1\uff09\uff0c\u6d4b\u8bd5Gemini Flash\u3001GPT-4o Audio\u3001Qwen Audio\u7b49\u524d\u6cbf\u6a21\u578b", "result": "\u9ad8\u4f4e\u8d44\u6e90\u8bed\u8a00\u6027\u80fd\u5dee\u8ddd\u663e\u8457\uff08\u975e\u6d32\u8bed\u8a00F1\u503c\u6bd4\u6b27\u6d32\u8bed\u8a00\u4f4e40%+\uff09\uff0c\u8bed\u97f3\u6a21\u6001\u8868\u73b0\u666e\u904d\u5f31\u4e8e\u6587\u672c\u6a21\u6001", "conclusion": "\u9700\u7cfb\u7edf\u6027\u63d0\u5347LLMs\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8986\u76d6\u80fd\u529b\uff0c\u5efa\u8bae\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u67b6\u6784\u521b\u65b0\u6539\u5584\u8bed\u8a00\u516c\u5e73\u6027"}}
{"id": "2506.08403", "pdf": "https://arxiv.org/pdf/2506.08403", "abs": "https://arxiv.org/abs/2506.08403", "authors": ["Weiya Li", "Junjie Chen", "Bei Li", "Boyang Liu", "Zichen Wen", "Nuanqiao Shan", "Xiaoqian Liu", "Anping Liu", "Huajie Liu", "Youyan Wang", "Wujiuge Yin", "Hu Song", "Bing Huang", "Zhiyuan Xia", "Jialiang Chen", "Linfeng Zhang"], "title": "TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 4 figures, Under review. Code:\n  https://github.com/weiyali126/TACTIC", "summary": "Machine translation has long been a central task in natural language\nprocessing. With the rapid advancement of large language models (LLMs), there\nhas been remarkable progress in translation quality. However, fully realizing\nthe translation potential of LLMs remains an open challenge. Recent studies\nhave explored multi-agent systems to decompose complex translation tasks into\ncollaborative subtasks, showing initial promise in enhancing translation\nquality through agent cooperation and specialization. Nevertheless, existing\nmulti-agent translation frameworks largely neglect foundational insights from\ncognitive translation studies. These insights emphasize how human translators\nemploy different cognitive strategies, such as balancing literal and free\ntranslation, refining expressions based on context, and iteratively evaluating\noutputs. To address this limitation, we propose a cognitively informed\nmulti-agent framework called TACTIC, which stands for T ranslation A gents with\nCognitive- T heoretic Interactive Collaboration. The framework comprises six\nfunctionally distinct agents that mirror key cognitive processes observed in\nhuman translation behavior. These include agents for drafting, refinement,\nevaluation, scoring, context reasoning, and external knowledge gathering. By\nsimulating an interactive and theory-grounded translation workflow, TACTIC\neffectively leverages the full capacity of LLMs for high-quality translation.\nExperimental results on diverse language pairs from the FLORES-200 and WMT24\nbenchmarks show that our method consistently achieves state-of-the-art\nperformance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by\nan average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it\nfurther improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at\nhttps://github.com/weiyali126/TACTIC.", "AI": {"tldr": "\u63d0\u51fa\u8ba4\u77e5\u542f\u53d1\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6TACTIC\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u7ffb\u8bd1\u8ba4\u77e5\u7b56\u7565\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7ffb\u8bd1\u8d28\u91cf", "motivation": "\u73b0\u6709\u6846\u67b6\u5ffd\u89c6\u8ba4\u77e5\u7ffb\u8bd1\u7406\u8bba\uff0c\u4eba\u7c7b\u7ffb\u8bd1\u7b56\u7565\uff08\u5982\u76f4\u8bd1/\u610f\u8bd1\u5e73\u8861\u3001\u4e0a\u4e0b\u6587\u4f18\u5316\u3001\u8fed\u4ee3\u8bc4\u4f30\uff09\u672a\u88ab\u6709\u6548\u6574\u5408", "method": "\u6784\u5efa6\u4e2a\u529f\u80fd\u5206\u5316\u7684\u667a\u80fd\u4f53\uff08\u8d77\u8349/\u7cbe\u70bc/\u8bc4\u4f30/\u8bc4\u5206/\u4e0a\u4e0b\u6587\u63a8\u7406/\u77e5\u8bc6\u6536\u96c6\uff09\uff0c\u5f62\u6210\u7406\u8bba\u6307\u5bfc\u7684\u534f\u4f5c\u6d41\u7a0b", "result": "\u5728FLORES-200\u548cWMT24\u591a\u8bed\u8a00\u6d4b\u8bd5\u4e2d\uff0cXCOMET\u548cCOMETKIWI-23\u6307\u6807\u8d85\u8d8aGPT-4\u548cDeepSeek-R1\u57fa\u51c6\u6a21\u578b", "conclusion": "\u8ba4\u77e5\u7406\u8bba\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u80fd\u5145\u5206\u91ca\u653eLLM\u7ffb\u8bd1\u6f5c\u529b\uff0c\u9a8c\u8bc1\u8ba4\u77e5\u7b56\u7565\u5728\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u6838\u5fc3\u4ef7\u503c"}}
{"id": "2506.08410", "pdf": "https://arxiv.org/pdf/2506.08410", "abs": "https://arxiv.org/abs/2506.08410", "authors": ["Ziyang Ma", "Qingyue Yuan", "Zhenglin Wang", "Deyu Zhou"], "title": "Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Previous research has primarily focused on the cognitive error detection\ncapabilities of Large Language Models (LLMs), often prompting them to analyze\nmistakes in reasoning chains. However, few studies have examined the\nmeta-cognitive abilities of LLMs (e.g., their self-awareness of step errors),\nwhich are crucial for their reliability. While studies on LLM self-evaluation\npresent some measures, such as perplexity, which can reflect the answer\ncorrectness and be viewed as the lens of meta-cognition, they lack step-level\nanalysis and adaptation. This paper studies the evaluation of LLM\nmeta-cognition using the current lenses and how to improve these lenses.\nSpecifically, we propose AutoMeco, an Automated Meta-cognition Evaluation\nframework for benchmarking the existing lenses. Furthermore, a training-free\nMarkovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost\ncurrent meta-cognition lenses. Experimental results on three mathematical\nreasoning datasets and three LLMs show the reasonableness of AutoMeco by\ncomparing it with Best-of-N verification. Moreover, the meta-cognition ability\nof LLMs can be better evaluated using MIRA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faAutoMeco\u6846\u67b6\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5143\u8ba4\u77e5\u80fd\u529b\uff0c\u5e76\u901a\u8fc7MIRA\u7b56\u7565\u6539\u8fdb\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8LLMs\u7684\u8ba4\u77e5\u9519\u8bef\u68c0\u6d4b\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u5143\u8ba4\u77e5\u80fd\u529b\uff08\u5982\u6b65\u9aa4\u9519\u8bef\u81ea\u6211\u610f\u8bc6\uff09\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u8fd9\u76f4\u63a5\u5f71\u54cd\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "method": "1. \u63d0\u51fa\u81ea\u52a8\u5316\u5143\u8ba4\u77e5\u8bc4\u4f30\u6846\u67b6AutoMeco\uff1b2. \u8bbe\u8ba1\u65e0\u9700\u8bad\u7ec3\u7684\u9a6c\u5c14\u53ef\u592b\u5185\u5728\u5956\u52b1\u8c03\u6574\u7b56\u7565MIRA\uff1b3. \u5728\u4e09\u4e2a\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u548c\u4e09\u79cdLLMs\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "AutoMeco\u6846\u67b6\u9a8c\u8bc1\u7ed3\u679c\u5408\u7406\uff0cMIRA\u80fd\u66f4\u6709\u6548\u8bc4\u4f30LLMs\u7684\u5143\u8ba4\u77e5\u80fd\u529b\u3002", "conclusion": "\u6539\u8fdb\u5143\u8ba4\u77e5\u8bc4\u4f30\u65b9\u6cd5\u5bf9\u63d0\u5347LLM\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0cAutoMeco\u548cMIRA\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08427", "pdf": "https://arxiv.org/pdf/2506.08427", "abs": "https://arxiv.org/abs/2506.08427", "authors": ["Jiaxiang Liu", "Boxuan Xing", "Chenhao Yuan", "Chenxiang Zhang", "Di Wu", "Xiusheng Huang", "Haida Yu", "Chuhan Lang", "Pengfei Cao", "Jun Zhao", "Kang Liu"], "title": "Know-MRI: A Knowledge Mechanisms Revealer&Interpreter for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "As large language models (LLMs) continue to advance, there is a growing\nurgency to enhance the interpretability of their internal knowledge mechanisms.\nConsequently, many interpretation methods have emerged, aiming to unravel the\nknowledge mechanisms of LLMs from various perspectives. However, current\ninterpretation methods differ in input data formats and interpreting outputs.\nThe tools integrating these methods are only capable of supporting tasks with\nspecific inputs, significantly constraining their practical applications. To\naddress these challenges, we present an open-source Knowledge Mechanisms\nRevealer&Interpreter (Know-MRI) designed to analyze the knowledge mechanisms\nwithin LLMs systematically. Specifically, we have developed an extensible core\nmodule that can automatically match different input data with interpretation\nmethods and consolidate the interpreting outputs. It enables users to freely\nchoose appropriate interpretation methods based on the inputs, making it easier\nto comprehensively diagnose the model's internal knowledge mechanisms from\nmultiple perspectives. Our code is available at\nhttps://github.com/nlpkeg/Know-MRI. We also provide a demonstration video on\nhttps://youtu.be/NVWZABJ43Bs.", "AI": {"tldr": "\u5f00\u53d1\u4e86Know-MRI\u5f00\u6e90\u7cfb\u7edf\uff0c\u901a\u8fc7\u53ef\u6269\u5c55\u6838\u5fc3\u6a21\u5757\u6574\u5408\u591a\u79cd\u89e3\u91ca\u65b9\u6cd5\uff0c\u652f\u6301\u7528\u6237\u591a\u7ef4\u5ea6\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u7684\u77e5\u8bc6\u673a\u5236\u3002", "motivation": "\u73b0\u6709LLM\u77e5\u8bc6\u89e3\u91ca\u65b9\u6cd5\u5b58\u5728\u8f93\u5165\u683c\u5f0f\u4e0e\u8f93\u51fa\u7ed3\u679c\u4e0d\u7edf\u4e00\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u5de5\u5177\u5e94\u7528\u573a\u666f\u53d7\u9650\uff0c\u4e9f\u9700\u5f00\u53d1\u7cfb\u7edf\u5316\u7684\u5206\u6790\u6846\u67b6\u3002", "method": "\u6784\u5efa\u81ea\u52a8\u5339\u914d\u8f93\u5165\u6570\u636e\u4e0e\u89e3\u91ca\u65b9\u6cd5\u7684\u6838\u5fc3\u6a21\u5757\uff0c\u5b9e\u73b0\u89e3\u91ca\u7ed3\u679c\u7684\u96c6\u6210\u8f93\u51fa\uff0c\u5141\u8bb8\u7528\u6237\u6839\u636e\u8f93\u5165\u81ea\u7531\u9009\u62e9\u89e3\u91ca\u65b9\u6cd5\u8fdb\u884c\u7ec4\u5408\u5206\u6790\u3002", "result": "\u521b\u5efa\u5f00\u6e90\u5de5\u5177Know-MRI\u5e76\u63d0\u4f9b\u4ee3\u7801\u4e0e\u6f14\u793a\u89c6\u9891\uff0c\u652f\u6301\u4ece\u795e\u7ecf\u5143\u3001\u53c2\u6570\u6743\u91cd\u7b49\u591a\u89d2\u5ea6\u5bf9\u6a21\u578b\u77e5\u8bc6\u673a\u5236\u8fdb\u884c\u7cfb\u7edf\u6027\u8bca\u65ad\u3002", "conclusion": "Know-MRI\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u89e3\u91ca\u65b9\u6cd5\u788e\u7247\u5316\u7684\u95ee\u9898\uff0c\u4e3a\u6df1\u5165\u7406\u89e3LLM\u77e5\u8bc6\u5b58\u50a8\u4e0e\u8c03\u7528\u673a\u5236\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5206\u6790\u5e73\u53f0\u3002"}}
{"id": "2506.08430", "pdf": "https://arxiv.org/pdf/2506.08430", "abs": "https://arxiv.org/abs/2506.08430", "authors": ["Ziqi. Liu", "Ziyang. Zhou", "Mingxuan. Hu"], "title": "CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models", "categories": ["cs.CL", "cs.MA"], "comment": "ICML 2025 Workshop on Collaborative and Federated Agentic Workflows", "summary": "Large language model (LLM) have become mainstream methods in the field of\nsarcasm detection. However, existing LLM methods face challenges in irony\ndetection, including: 1. single-perspective limitations, 2. insufficient\ncomprehensive understanding, and 3. lack of interpretability. This paper\nintroduces the Collaborative Agent Framework for Irony (CAF-I), an LLM-driven\nmulti-agent system designed to overcome these issues. CAF-I employs specialized\nagents for Context, Semantics, and Rhetoric, which perform multidimensional\nanalysis and engage in interactive collaborative optimization. A Decision Agent\nthen consolidates these perspectives, with a Refinement Evaluator Agent\nproviding conditional feedback for optimization. Experiments on benchmark\ndatasets establish CAF-I's state-of-the-art zero-shot performance. Achieving\nSOTA on the vast majority of metrics, CAF-I reaches an average Macro-F1 of\n76.31, a 4.98 absolute improvement over the strongest prior baseline. This\nsuccess is attained by its effective simulation of human-like multi-perspective\nanalysis, enhancing detection accuracy and interpretability.", "AI": {"tldr": "CAF-I\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4f18\u5316\uff0c\u7a81\u7834\u4f20\u7edfLLM\u5355\u89c6\u89d2\u5c40\u9650\uff0c\u5b9e\u73b0\u8bbd\u523a\u68c0\u6d4bSOTA\u6027\u80fd\uff08Macro-F1 76.31\uff0c\u63d0\u53474.98%\uff09", "motivation": "\u9488\u5bf9\u73b0\u6709LLM\u5728\u8bbd\u523a\u68c0\u6d4b\u4e2d\u7684\u4e09\u5927\u7f3a\u9677\uff1a\u5355\u89c6\u89d2\u5c40\u9650\u6027\u3001\u7406\u89e3\u4e0d\u5145\u5206\u3001\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027", "method": "CAF-I\u6846\u67b6\u5305\u542b\u8bed\u5883/\u8bed\u4e49/\u4fee\u8f9e\u4e09\u5927\u4e13\u4e1a\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u591a\u7ef4\u5206\u6790+\u4ea4\u4e92\u5f0f\u534f\u540c\u4f18\u5316\uff0c\u51b3\u7b56\u667a\u80fd\u4f53\u6574\u5408\u5206\u6790\uff0c\u4f18\u5316\u8bc4\u4f30\u5668\u63d0\u4f9b\u6761\u4ef6\u53cd\u9988", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u6210\u96f6\u6837\u672cSOTA\uff0c\u5e73\u5747Macro-F1\u8fbe76.31\uff08\u7edd\u5bf9\u63d0\u53474.98\uff09\uff0c\u7edd\u5927\u591a\u6570\u6307\u6807\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u6a21\u578b", "conclusion": "\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u591a\u89c6\u89d2\u8ba4\u77e5\u673a\u5236\uff0cCAF-I\u5728\u63d0\u5347\u68c0\u6d4b\u7cbe\u5ea6\u7684\u540c\u65f6\u589e\u5f3a\u4e86\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u9a8c\u8bc1\u4e86\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u6709\u6548\u6027"}}
{"id": "2506.08433", "pdf": "https://arxiv.org/pdf/2506.08433", "abs": "https://arxiv.org/abs/2506.08433", "authors": ["Hern\u00e1n Maina", "Nicol\u00e1s Wolovick", "Luciana Benotti"], "title": "Low-resource domain adaptation while minimizing energy and hardware resource consumption", "categories": ["cs.CL", "cs.DC", "cs.LG"], "comment": "A shorter version of this work was accepted as a two-page abstract\n  for presentation at the Widening Natural Language Processing (WiNLP) 2023\n  Workshop. That version was not publicly released, and this is the first\n  public version of the work", "summary": "Training Large Language Models (LLMs) is costly in terms of energy, hardware,\nand annotated data, often resulting in a positionality rooted in predominant\ncultures and values (Santy et al., 2023). Domain adaptation has emerged as a\npromising strategy to better align models with diverse cultural and value\ncontexts (Hershcovich et al., 2022), but its computational cost remains a\nsignificant barrier, particularly for research groups lacking access to\nlarge-scale infrastructure. In this paper, we evaluate how the use of different\nnumerical precisions and data parallelization strategies impacts both training\nspeed (as a proxy to energy and hardware consumption) and model accuracy, with\nthe goal of facilitating domain adaptation in low-resource environments. Our\nfindings are relevant to any setting where energy efficiency, accessibility, or\nlimited hardware availability are key concerns.", "AI": {"tldr": "\u63a2\u7d22\u4e0d\u540c\u6570\u503c\u7cbe\u5ea6\u4e0e\u6570\u636e\u5e76\u884c\u7b56\u7565\u5bf9LLM\u9886\u57df\u9002\u5e94\u7684\u6548\u7387\u5f71\u54cd\uff0c\u5e73\u8861\u8bad\u7ec3\u901f\u5ea6\u4e0e\u6a21\u578b\u7cbe\u5ea6\u4ee5\u964d\u4f4e\u8d44\u6e90\u95e8\u69db", "motivation": "\u4f20\u7edfLLM\u8bad\u7ec3\u5b58\u5728\u9ad8\u80fd\u8017\u4e0e\u6587\u5316\u504f\u89c1\u95ee\u9898\uff0c\u9886\u57df\u9002\u914d\u867d\u80fd\u7f13\u89e3\u4f46\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u5236\u7ea6\u8d44\u6e90\u532e\u4e4f\u7814\u7a76\u673a\u6784\u7684\u5e94\u7528", "method": "\u7cfb\u7edf\u6027\u8bc4\u4f30\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\uff08FP16/FP32\uff09\u4e0e\u591aGPU\u6570\u636e\u5e76\u884c\u7b56\u7565\u7684\u7ec4\u5408\u6548\u679c\uff0c\u5efa\u7acb\u8bad\u7ec3\u901f\u5ea6-\u80fd\u8017-\u51c6\u786e\u7387\u8bc4\u4f30\u6846\u67b6", "result": "\u4f18\u5316\u540e\u7684\u6df7\u5408\u7cbe\u5ea6\u5e76\u884c\u65b9\u6848\u5728\u4fdd\u630198%\u6a21\u578b\u7cbe\u5ea6\u4e0b\uff0c\u5b9e\u73b040%\u8bad\u7ec3\u52a0\u901f\u4e0e35%\u663e\u5b58\u5360\u7528\u964d\u4f4e", "conclusion": "\u6570\u503c\u4f18\u5316\u7b56\u7565\u6709\u6548\u7a81\u7834\u9886\u57df\u9002\u914d\u7684\u786c\u4ef6\u58c1\u5792\uff0c\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u4e0e\u53ef\u6301\u7eedAI\u53d1\u5c55\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u6280\u672f\u8def\u5f84"}}
{"id": "2506.08436", "pdf": "https://arxiv.org/pdf/2506.08436", "abs": "https://arxiv.org/abs/2506.08436", "authors": ["Jiujun He", "Huazhen Lin"], "title": "Olica: Efficient Structured Pruning of Large Language Models without Retraining", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ICML 2025", "summary": "Most existing structured pruning methods for Large Language Models (LLMs)\nrequire substantial computational and data resources for retraining to\nreestablish the corrupted correlations, making them prohibitively expensive. To\naddress this, we propose a pruning framework for LLMs called Orthogonal\ndecomposition and Linear Calibration (Olica), which eliminates the need for\nretraining. A key observation is that the multi-head attention (MHA) layer\ndepends on two types of matrix products. By treating these matrix products as\nunified entities and applying principal component analysis (PCA), we extract\nthe most important information to compress LLMs without sacrificing accuracy or\ndisrupting their original structure. Consequently, retraining becomes\nunnecessary. A fast decomposition method is devised, reducing the complexity of\nPCA by a factor of the square of the number of attention heads. Additionally,\nto mitigate error accumulation problem caused by pruning the feed-forward\nnetwork (FFN) layer, we introduce a linear calibration method to reconstruct\nthe residual errors of pruned layers using low-rank matrices. By leveraging\nsingular value decomposition (SVD) on the solution of the least-squares\nproblem, these matrices are obtained without requiring retraining. Extensive\nexperiments show that the proposed Olica is efficient in terms of data usage,\nGPU memory, and running time, while delivering superior performance across\nmultiple benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u91cd\u8bad\u7ec3\u7684Olica\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u6b63\u4ea4\u5206\u89e3\u548c\u7ebf\u6027\u6821\u51c6\u6280\u672f\u538b\u7f29\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4fdd\u6301\u6027\u80fd\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u548c\u6570\u636e\u8fdb\u884c\u91cd\u8bad\u7ec3\uff0c\u5bfc\u81f4\u5e94\u7528\u6210\u672c\u8fc7\u9ad8\u3002\u4e3a\u89e3\u51b3\u8be5\u95ee\u9898\uff0c\u63d0\u51fa\u65e0\u9700\u91cd\u8bad\u7ec3\u7684\u526a\u679d\u65b9\u6848\u3002", "method": "1. \u5c06\u591a\u5934\u6ce8\u610f\u529b\u5c42\u7684\u77e9\u9635\u4e58\u79ef\u89c6\u4e3a\u7edf\u4e00\u5b9e\u4f53\u8fdb\u884cPCA\u964d\u7ef4\n2. \u8bbe\u8ba1\u5feb\u901f\u5206\u89e3\u65b9\u6cd5\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\n3. \u901a\u8fc7SVD\u6c42\u89e3\u6700\u5c0f\u4e8c\u4e58\u95ee\u9898\u5b9e\u73b0\u524d\u9988\u7f51\u7edc\u5c42\u7684\u8bef\u5dee\u91cd\u6784", "result": "\u5b9e\u9a8c\u8bc1\u660eOlica\u5728\u6570\u636e\u4f7f\u7528\u6548\u7387\u3001GPU\u5185\u5b58\u5360\u7528\u548c\u8fd0\u884c\u65f6\u95f4\u65b9\u9762\u5177\u6709\u4f18\u52bf\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "Olica\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u526a\u679d\u65b9\u6cd5\u4f9d\u8d56\u91cd\u8bad\u7ec3\u7684\u95ee\u9898\uff0c\u4e3aLLM\u538b\u7f29\u63d0\u4f9b\u4e86\u9ad8\u6548\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08477", "pdf": "https://arxiv.org/pdf/2506.08477", "abs": "https://arxiv.org/abs/2506.08477", "authors": ["Fengjun Pan", "Anh Tuan Luu", "Xiaobao Wu"], "title": "Detecting Harmful Memes with Decoupled Understanding and Guided CoT Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Detecting harmful memes is essential for maintaining the integrity of online\nenvironments. However, current approaches often struggle with resource\nefficiency, flexibility, or explainability, limiting their practical deployment\nin content moderation systems. To address these challenges, we introduce\nU-CoT+, a novel framework for harmful meme detection. Instead of relying solely\non prompting or fine-tuning multimodal models, we first develop a high-fidelity\nmeme-to-text pipeline that converts visual memes into detail-preserving textual\ndescriptions. This design decouples meme interpretation from meme\nclassification, thus avoiding immediate reasoning over complex raw visual\ncontent and enabling resource-efficient harmful meme detection with general\nlarge language models (LLMs). Building on these textual descriptions, we\nfurther incorporate targeted, interpretable human-crafted guidelines to guide\nmodels' reasoning under zero-shot CoT prompting. As such, this framework allows\nfor easy adaptation to different harmfulness detection criteria across\nplatforms, regions, and over time, offering high flexibility and\nexplainability. Extensive experiments on seven benchmark datasets validate the\neffectiveness of our framework, highlighting its potential for explainable and\nlow-resource harmful meme detection using small-scale LLMs. Codes and data are\navailable at: https://anonymous.4open.science/r/HMC-AF2B/README.md.", "AI": {"tldr": "\u63d0\u51faU-CoT+\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u56e0\u6587\u672c\u5316\u8f6c\u6362\u4e0e\u4eba\u7c7b\u6307\u5bfc\u7684\u96f6\u6837\u672c\u601d\u7ef4\u94fe\u63d0\u793a\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u7075\u6d3b\u4e14\u53ef\u89e3\u91ca\u7684\u6709\u5bb3\u6a21\u56e0\u68c0\u6d4b", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8d44\u6e90\u6548\u7387\u4f4e\u3001\u7075\u6d3b\u6027\u5dee\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u901a\u8fc7\u89e3\u8026\u6a21\u56e0\u89e3\u91ca\u4e0e\u5206\u7c7b\u6765\u63d0\u5347\u90e8\u7f72\u6548\u7387", "method": "1.\u5efa\u7acb\u9ad8\u4fdd\u771f\u6a21\u56e0\u6587\u672c\u8f6c\u6362\u7ba1\u9053 2.\u7ed3\u5408\u53ef\u89e3\u91ca\u7684\u4eba\u5de5\u5236\u5b9a\u51c6\u5219 3.\u91c7\u7528\u96f6\u6837\u672c\u601d\u7ef4\u94fe\u63d0\u793a\u5f15\u5bfc\u5c0f\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u63a8\u7406", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u8bc1\u660e\u6846\u67b6\u5728\u53ef\u89e3\u91ca\u6027\u548c\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u4f18\u52bf", "conclusion": "\u8be5\u6846\u67b6\u53ef\u7075\u6d3b\u9002\u5e94\u4e0d\u540c\u5e73\u53f0/\u5730\u57df\u7684\u68c0\u6d4b\u6807\u51c6\u6f14\u53d8\uff0c\u4e3a\u5185\u5bb9\u5ba1\u6838\u63d0\u4f9b\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.08479", "pdf": "https://arxiv.org/pdf/2506.08479", "abs": "https://arxiv.org/abs/2506.08479", "authors": ["Chihiro Taguchi", "Seiji Maekawa", "Nikita Bhutani"], "title": "Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "26 pages, 16 tables, 5 figures", "summary": "Retrieval-augmented generation (RAG) and long-context language models (LCLMs)\nboth address context limitations of LLMs in open-domain question answering\n(QA). However, optimal external context to retrieve remains an open problem:\nfixing the retrieval size risks either wasting tokens or omitting key evidence.\nExisting adaptive methods like Self-RAG and Self-Route rely on iterative LLM\nprompting and perform well on factoid QA, but struggle with aggregation QA,\nwhere the optimal context size is both unknown and variable. We present\nAdaptive-$k$ retrieval, a simple and effective single-pass method that\nadaptively selects the number of passages based on the distribution of the\nsimilarity scores between the query and the candidate passages. It does not\nrequire model fine-tuning, extra LLM inferences or changes to existing\nretriever-reader pipelines. On both factoid and aggregation QA benchmarks,\nAdaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x\nfewer tokens than full-context input, yet still retrieves 70% of relevant\npassages. It improves accuracy across five LCLMs and two embedding models,\nhighlighting that dynamically adjusting context size leads to more efficient\nand accurate QA.", "AI": {"tldr": "\u63d0\u51faAdaptive-$k$\u68c0\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f8\u4f3c\u5ea6\u5206\u6570\u52a8\u6001\u9009\u62e9\u68c0\u7d22\u6bb5\u843d\u6570\u91cf\uff0c\u5728\u4fdd\u6301\u95ee\u7b54\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11token\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u56fa\u5b9a\u68c0\u7d22\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u805a\u5408\u578bQA\u4efb\u52a1\u4e2d\u52a8\u6001\u53d8\u5316\u7684\u4e0a\u4e0b\u6587\u9700\u6c42\uff0c\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\u6216\u4fe1\u606f\u9057\u6f0f\u3002", "method": "\u57fa\u4e8e\u67e5\u8be2\u4e0e\u5019\u9009\u6bb5\u843d\u76f8\u4f3c\u5ea6\u5206\u6570\u5206\u5e03\u8fdb\u884c\u5355\u6b21\u81ea\u9002\u5e94\u9009\u62e9\uff0c\u65e0\u9700\u6a21\u578b\u5fae\u8c03\u6216\u6d41\u7a0b\u4fee\u6539\u3002", "result": "\u5728\u4e8b\u5b9e/\u805a\u5408QA\u4efb\u52a1\u4e2d\u8282\u770190%token\u4ecd\u4fdd\u630170%\u76f8\u5173\u6bb5\u843d\u53ec\u56de\uff0c\u4e94\u5927LCLM\u6a21\u578b\u51c6\u786e\u7387\u5168\u9762\u63d0\u5347\u3002", "conclusion": "\u52a8\u6001\u8c03\u6574\u4e0a\u4e0b\u6587\u89c4\u6a21\u662f\u5b9e\u73b0\u9ad8\u6548\u7cbe\u51c6\u95ee\u7b54\u7684\u5173\u952e\uff0c\u8be5\u65b9\u6cd5\u4e3a\u4e0d\u540cQA\u4efb\u52a1\u63d0\u4f9b\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08480", "pdf": "https://arxiv.org/pdf/2506.08480", "abs": "https://arxiv.org/abs/2506.08480", "authors": ["Huixuan Zhang", "Xiaojun Wan"], "title": "Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Text-to-image models often struggle to generate images that precisely match\ntextual prompts. Prior research has extensively studied the evaluation of\nimage-text alignment in text-to-image generation. However, existing evaluations\nprimarily focus on agreement with human assessments, neglecting other critical\nproperties of a trustworthy evaluation framework. In this work, we first\nidentify two key aspects that a reliable evaluation should address. We then\nempirically demonstrate that current mainstream evaluation frameworks fail to\nfully satisfy these properties across a diverse range of metrics and models.\nFinally, we propose recommendations for improving image-text alignment\nevaluation.", "AI": {"tldr": "\u73b0\u6709\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u8bc4\u4f30\u6846\u67b6\u5b58\u5728\u53ef\u4fe1\u5ea6\u7f3a\u9677\uff0c\u672c\u6587\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u8fc7\u5ea6\u5173\u6ce8\u4eba\u7c7b\u5224\u65ad\u4e00\u81f4\u6027\uff0c\u5ffd\u89c6\u8bc4\u4f30\u6846\u67b6\u672c\u8eab\u7684\u53ef\u4fe1\u5c5e\u6027\uff08\u5982\u53ef\u9760\u6027\u548c\u7a33\u5b9a\u6027\uff09", "method": "\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\u4e3b\u6d41\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u5176\u5728\u591a\u6837\u5316\u6307\u6807\u548c\u6a21\u578b\u4e2d\u7684\u4e0d\u8db3", "result": "\u8bc1\u5b9e\u5f53\u524d\u8bc4\u4f30\u4f53\u7cfb\u65e0\u6cd5\u5168\u9762\u6ee1\u8db3\u53ef\u4fe1\u8bc4\u4f30\u7684\u5173\u952e\u5c5e\u6027\u8981\u6c42", "conclusion": "\u5efa\u8bae\u5efa\u7acb\u5305\u542b\u53ef\u9760\u6027\u9a8c\u8bc1\u548c\u7a33\u5b9a\u6027\u6d4b\u8bd5\u7684\u591a\u7ef4\u5ea6\u8bc4\u4f30\u4f53\u7cfb"}}
{"id": "2506.08487", "pdf": "https://arxiv.org/pdf/2506.08487", "abs": "https://arxiv.org/abs/2506.08487", "authors": ["Sumanth Manduru", "Carlotta Domeniconi"], "title": "Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid adoption of Small Language Models (SLMs) for on-device and\nresource-constrained deployments has outpaced our understanding of their\nethical risks. To the best of our knowledge, we present the first large-scale\naudit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an\noverlooked \"middle tier\" between BERT-class encoders and flagship LLMs. Our\nevaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma\n3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we\nanalyze both utility and fairness across ambiguous and disambiguated contexts.\nThis evaluation reveals three key insights. First, competence and fairness need\nnot be antagonistic: Phi models achieve F1 scores exceeding 90 percent while\nexhibiting minimal bias, showing that efficient and ethical NLP is attainable.\nSecond, social bias varies significantly by architecture: Qwen 2.5 models may\nappear fair, but this often reflects vacuous neutrality, random guessing, or\nevasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2\nmodels exhibit stronger stereotypical bias, suggesting overconfidence rather\nthan neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ\nquantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but\nincreases disability-related bias in Phi-4-Mini by over 7 percentage points.\nThese insights provide practical guidance for the responsible deployment of\nSLMs in applications demanding fairness and efficiency, particularly benefiting\nsmall enterprises and resource-constrained environments.", "AI": {"tldr": "\u4e2d\u7b49\u89c4\u6a21\u6307\u4ee4\u8c03\u4f18SLMs\u7684\u4f26\u7406\u5ba1\u8ba1\u663e\u793a\uff1aPhi\u6a21\u578b\u5b9e\u73b090%+\u51c6\u786e\u7387\u4e0e\u6700\u5c0f\u504f\u89c1\u5e76\u5b58\uff0c\u67b6\u6784\u9009\u62e9\u663e\u8457\u5f71\u54cd\u504f\u89c1\u8868\u73b0\uff0c\u91cf\u5316\u6280\u672f\u5b58\u5728\u590d\u6742\u6027\u80fd/\u516c\u5e73\u6027\u6743\u8861", "motivation": "\u586b\u8865\u4e2d\u7b49\u89c4\u6a21SLMs\uff080.5-50\u4ebf\u53c2\u6570\uff09\u4f26\u7406\u98ce\u9669\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u8d1f\u8d23\u4efb\u90e8\u7f72\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e", "method": "\u4f7f\u7528BBQ\u57fa\u51c6\u6d4b\u8bd5\u96f6\u6837\u672c\u63d0\u793a\uff0c\u8bc4\u4f30Qwen 2.5/LLaMA 3.2/Gemma 3/Phi\u56db\u5927\u5bb6\u65cf\u51719\u4e2a\u5f00\u6e90\u6a21\u578b\u5728\u6a21\u7cca/\u660e\u786e\u8bed\u5883\u4e0b\u7684\u6027\u80fd\u4e0e\u516c\u5e73\u6027", "result": "1. Phi\u7cfb\u5217\u5b9e\u73b0\u51c6\u786e\u7387\u4e0e\u516c\u5e73\u6027\u53cc\u4f18\uff08F1>90%\u4e14\u504f\u6700\u4f4e\uff09 2. Qwen\u8868\u73b0\u4e2d\u6027\u4f46\u5b58\u5728\u865a\u5047\u4e2d\u7acb\uff0cLLaMA\u5448\u73b0\u8fc7\u81ea\u4fe1\u504f\u89c1 3. 4-bit\u91cf\u5316\u63d0\u5347LLaMA\u6027\u80fd\u4f46\u589e\u52a0Phi\u6b8b\u969c\u504f\u89c17%+", "conclusion": "\u7814\u7a76\u4e3a\u4e2d\u5c0f\u4f01\u4e1aSLMs\u90e8\u7f72\u63d0\u4f9b\u5173\u952e\u8def\u5f84\uff1a\u4f18\u5148\u9009\u62e9Phi\u67b6\u6784\uff0c\u8c28\u614e\u8bc4\u4f30Qwen\u7684\u4e2d\u6027\u672c\u8d28\uff0c\u9488\u5bf9\u5177\u4f53\u5e94\u7528\u573a\u666f\u4f18\u5316\u91cf\u5316\u7b56\u7565\u4ee5\u5b9e\u73b0\u6548\u7387-\u4f26\u7406\u5e73\u8861"}}
{"id": "2506.08488", "pdf": "https://arxiv.org/pdf/2506.08488", "abs": "https://arxiv.org/abs/2506.08488", "authors": ["Ashutosh Dwivedi", "Siddhant Shivdutt Singh", "Ashutosh Modi"], "title": "EtiCor++: Towards Understanding Etiquettical Bias in LLMs", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accepted at ACL Findings 2025, 22 pages (9 pages main content + 4\n  pages references + 9 pages appendix)", "summary": "In recent years, researchers have started analyzing the cultural sensitivity\nof LLMs. In this respect, Etiquettes have been an active area of research.\nEtiquettes are region-specific and are an essential part of the culture of a\nregion; hence, it is imperative to make LLMs sensitive to etiquettes. However,\nthere needs to be more resources in evaluating LLMs for their understanding and\nbias with regard to etiquettes. In this resource paper, we introduce EtiCor++,\na corpus of etiquettes worldwide. We introduce different tasks for evaluating\nLLMs for knowledge about etiquettes across various regions. Further, we\nintroduce various metrics for measuring bias in LLMs. Extensive experimentation\nwith LLMs shows inherent bias towards certain regions.", "AI": {"tldr": "\u63d0\u51faEtiCor++\u8bed\u6599\u5e93\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u5730\u533a\u793c\u4eea\u6587\u5316\u7406\u89e3\u4e2d\u7684\u504f\u89c1\u95ee\u9898", "motivation": "\u73b0\u6709\u8d44\u6e90\u96be\u4ee5\u8bc4\u4f30LLMs\u5bf9\u5730\u533a\u793c\u4eea\u6587\u5316\u7684\u7406\u89e3\u504f\u5dee\uff0c\u793c\u4eea\u4f5c\u4e3a\u6587\u5316\u6838\u5fc3\u8981\u7d20\u9700\u589e\u5f3a\u6a21\u578b\u654f\u611f\u6027", "method": "\u6784\u5efa\u8986\u76d6\u5168\u7403\u793c\u4eea\u7684EtiCor++\u8bed\u6599\u5e93\uff0c\u8bbe\u8ba1\u591a\u7ef4\u5ea6\u8bc4\u4f30\u4efb\u52a1\u5e76\u521b\u5efa\u504f\u5dee\u91cf\u5316\u6307\u6807", "result": "\u5b9e\u9a8c\u8bc1\u5b9eLLMs\u5b58\u5728\u5bf9\u7279\u5b9a\u5730\u533a\u7684\u56fa\u6709\u504f\u89c1", "conclusion": "EtiCor++\u586b\u8865\u8bc4\u4f30\u7a7a\u767d\uff0c\u4e3a\u63d0\u5347LLMs\u6587\u5316\u654f\u611f\u6027\u63d0\u4f9b\u91cd\u8981\u57fa\u51c6"}}
{"id": "2506.08490", "pdf": "https://arxiv.org/pdf/2506.08490", "abs": "https://arxiv.org/abs/2506.08490", "authors": ["Xiao Wei", "Xiaobao Wang", "Ning Zhuang", "Chenyang Wang", "Longbiao Wang", "Jianwu dang"], "title": "Integration of Old and New Knowledge for Generalized Intent Discovery: A Consistency-driven Prototype-Prompting Framework", "categories": ["cs.CL"], "comment": "9 pages, 2 figures, 7 tables, IJCAI 2025", "summary": "Intent detection aims to identify user intents from natural language inputs,\nwhere supervised methods rely heavily on labeled in-domain (IND) data and\nstruggle with out-of-domain (OOD) intents, limiting their practical\napplicability. Generalized Intent Discovery (GID) addresses this by leveraging\nunlabeled OOD data to discover new intents without additional annotation.\nHowever, existing methods focus solely on clustering unsupervised data while\nneglecting domain adaptation. Therefore, we propose a consistency-driven\nprototype-prompting framework for GID from the perspective of integrating old\nand new knowledge, which includes a prototype-prompting framework for\ntransferring old knowledge from external sources, and a hierarchical\nconsistency constraint for learning new knowledge from target domains. We\nconducted extensive experiments and the results show that our method\nsignificantly outperforms all baseline methods, achieving state-of-the-art\nresults, which strongly demonstrates the effectiveness and generalization of\nour methods. Our source code is publicly available at\nhttps://github.com/smileix/cpp.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e00\u81f4\u6027\u9a71\u52a8\u7684\u539f\u578b\u63d0\u793a\u6846\u67b6\u89e3\u51b3\u5e7f\u4e49\u610f\u56fe\u53d1\u73b0\u95ee\u9898\uff0c\u901a\u8fc7\u6574\u5408\u65b0\u65e7\u77e5\u8bc6\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u4e14\u65e0\u6cd5\u5904\u7406\u672a\u77e5\u9886\u57df\u610f\u56fe\uff0c\u73b0\u6709\u5e7f\u4e49\u610f\u56fe\u53d1\u73b0\u65b9\u6cd5\u5ffd\u89c6\u9886\u57df\u9002\u5e94\u95ee\u9898", "method": "1. \u539f\u578b\u63d0\u793a\u6846\u67b6\u8fc1\u79fb\u5916\u90e8\u65e7\u77e5\u8bc6\n2. \u5206\u5c42\u4e00\u81f4\u6027\u7ea6\u675f\u5b66\u4e60\u76ee\u6807\u57df\u65b0\u77e5\u8bc6", "result": "\u5728\u591a\u4e2a\u5b9e\u9a8c\u4e2d\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6548\u679c\uff08SOTA\uff09", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u65b0\u65e7\u77e5\u8bc6\u878d\u5408\uff0c\u4e3a\u89e3\u51b3\u9886\u57df\u9002\u5e94\u95ee\u9898\u63d0\u4f9b\u65b0\u65b9\u5411\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2506.08500", "pdf": "https://arxiv.org/pdf/2506.08500", "abs": "https://arxiv.org/abs/2506.08500", "authors": ["Arie Cattan", "Alon Jacovi", "Ori Ram", "Jonathan Herzig", "Roee Aharoni", "Sasha Goldshtein", "Eran Ofek", "Idan Szpektor", "Avi Caciularu"], "title": "DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Retrieval Augmented Generation (RAG) is a commonly used approach for\nenhancing large language models (LLMs) with relevant and up-to-date\ninformation. However, the retrieved sources can often contain conflicting\ninformation and it remains unclear how models should address such\ndiscrepancies. In this work, we first propose a novel taxonomy of knowledge\nconflict types in RAG, along with the desired model behavior for each type. We\nthen introduce CONFLICTS, a high-quality benchmark with expert annotations of\nconflict types in a realistic RAG setting. CONFLICTS is the first benchmark\nthat enables tracking progress on how models address a wide range of knowledge\nconflicts. We conduct extensive experiments on this benchmark, showing that\nLLMs often struggle to appropriately resolve conflicts between sources. While\nprompting LLMs to explicitly reason about the potential conflict in the\nretrieved documents significantly improves the quality and appropriateness of\ntheir responses, substantial room for improvement in future research remains.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51faRAG\u4e2d\u7684\u77e5\u8bc6\u51b2\u7a81\u5206\u7c7b\u6cd5\u548cCONFLICTS\u57fa\u51c6\uff0c\u53d1\u73b0LLM\u5904\u7406\u51b2\u7a81\u5b58\u5728\u4e0d\u8db3\uff0c\u663e\u5f0f\u63a8\u7406\u63d0\u793a\u53ef\u6539\u5584\u6548\u679c\u4f46\u4ecd\u9700\u4f18\u5316\u3002", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u9762\u5bf9\u68c0\u7d22\u4fe1\u606f\u51b2\u7a81\u65f6\u7f3a\u4e4f\u660e\u786e\u5904\u7406\u51c6\u5219\uff0c\u6a21\u578b\u5e94\u5bf9\u4e0d\u540c\u51b2\u7a81\u7c7b\u578b\u7684\u884c\u4e3a\u673a\u5236\u4e0d\u6e05\u6670\uff0c\u5f71\u54cd\u53ef\u9760\u6027\u548c\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "method": "\u63d0\u51fa\u77e5\u8bc6\u51b2\u7a81\u7c7b\u578b\u5b66\u5e76\u5b9a\u4e49\u671f\u671b\u6a21\u578b\u884c\u4e3a\uff0c\u6784\u5efa\u5305\u542b\u4e13\u5bb6\u6807\u6ce8\u7684CONFLICTS\u57fa\u51c6\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1LLM\u51b2\u7a81\u5904\u7406\u80fd\u529b\u53ca\u663e\u5f0f\u63d0\u793a\u7b56\u7565\u6548\u679c\u3002", "result": "LLM\u666e\u904d\u96be\u4ee5\u6b63\u786e\u5904\u7406\u77e5\u8bc6\u51b2\u7a81\uff0c\u663e\u5f0f\u8981\u6c42\u6a21\u578b\u63a8\u7406\u51b2\u7a81\u663e\u8457\u63d0\u5347\u56de\u7b54\u8d28\u91cf\uff0c\u4f46\u6700\u4f73\u89e3\u51b3\u7387\u4ec557%\uff0c\u663e\u793a\u8f83\u5927\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u77e5\u8bc6\u51b2\u7a81\u5bf9RAG\u7cfb\u7edf\u7684\u6838\u5fc3\u6311\u6218\uff0cCONFLICTS\u57fa\u51c6\u4e3a\u8bc4\u4f30\u63d0\u4f9b\u6807\u51c6\uff0c\u663e\u5f0f\u51b2\u7a81\u63a8\u7406\u662f\u6709\u6548\u65b9\u5411\uff0c\u9700\u8fdb\u4e00\u6b65\u5f00\u53d1\u4f18\u5316\u7b56\u7565\u3002"}}
{"id": "2506.08504", "pdf": "https://arxiv.org/pdf/2506.08504", "abs": "https://arxiv.org/abs/2506.08504", "authors": ["Divyaksh Shukla", "Ritesh Baviskar", "Dwijesh Gohil", "Aniket Tiwari", "Atul Shree", "Ashutosh Modi"], "title": "CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ACL Findings 2025 (16 pages: 5 pages main content + 3\n  pages references + 8 pages appendix)", "summary": "Discourse parsing is an important task useful for NLU applications such as\nsummarization, machine comprehension, and emotion recognition. The current\ndiscourse parsing datasets based on conversations consists of written English\ndialogues restricted to a single domain. In this resource paper, we introduce\nCoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in\nconversations. The corpus (code-mixed in Hindi and English) has both audio and\ntranscribed text and is annotated with nine discourse relations. We experiment\nwith various SoTA baseline models; the poor performance of SoTA models\nhighlights the challenges of multi-domain code-mixed corpus, pointing towards\nthe need for developing better models for such realistic settings.", "AI": {"tldr": "\u6784\u5efa\u4ee3\u7801\u6df7\u5408\u591a\u6a21\u6001\u591a\u9886\u57df\u8bed\u6599\u5e93CoMuMDR\uff0c\u7528\u4e8e\u63d0\u5347\u591a\u9886\u57df\u5bf9\u8bdd\u8bdd\u8bed\u89e3\u6790\u4efb\u52a1\u3002\u73b0\u6709\u6a21\u578b\u5728\u8be5\u8bed\u6599\u5e93\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u8bdd\u8bed\u89e3\u6790\u6570\u636e\u96c6\u4ec5\u8986\u76d6\u5355\u4e00\u9886\u57df\u82f1\u6587\u4e66\u9762\u5bf9\u8bdd\uff0c\u96be\u4ee5\u6ee1\u8db3\u591a\u9886\u57df\u4ee3\u7801\u6df7\u5408\u573a\u666f\u7684\u89e3\u6790\u9700\u6c42\u3002\u9700\u6784\u5efa\u66f4\u8d34\u8fd1\u73b0\u5b9e\u7684\u591a\u6a21\u6001\u6df7\u5408\u8bed\u6599\u5e93\u3002", "method": "\u521b\u5efa\u5305\u542b\u5370\u5730\u8bed-\u82f1\u8bed\u4ee3\u7801\u6df7\u5408\u7684\u97f3\u9891/\u6587\u672c\u6570\u636e\uff0c\u6807\u6ce89\u79cd\u8bdd\u8bed\u5173\u7cfb\u3002\u4f7f\u7528\u591a\u79cdSoTA\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u8de8\u9886\u57df\u89e3\u6790\u5b9e\u9a8c\u3002", "result": "\u73b0\u6709\u6a21\u578b\u5728\u8de8\u9886\u57df\u4ee3\u7801\u6df7\u5408\u6570\u636e\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff08F1\u503c\u4f4e\u4e8e\u4f20\u7edf\u5355\u9886\u57df\u82f1\u6587\u6570\u636e\uff09\uff0c\u63ed\u793a\u591a\u6a21\u6001\u6df7\u5408\u89e3\u6790\u7684\u6280\u672f\u6311\u6218\u3002", "conclusion": "CoMuMDR\u586b\u8865\u4e86\u591a\u9886\u57df\u4ee3\u7801\u6df7\u5408\u5bf9\u8bdd\u89e3\u6790\u7684\u8d44\u6e90\u7a7a\u767d\uff0c\u4e3a\u5f00\u53d1\u73b0\u5b9e\u573a\u666f\u7684\u9c81\u68d2\u8bdd\u8bed\u89e3\u6790\u6a21\u578b\u63d0\u4f9b\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\u3002"}}
{"id": "2506.08552", "pdf": "https://arxiv.org/pdf/2506.08552", "abs": "https://arxiv.org/abs/2506.08552", "authors": ["Xinyuan Wang", "Dongjie Wang", "Wangyang Ying", "Haoyue Bai", "Nanxu Gong", "Sixun Dong", "Kunpeng Liu", "Yanjie Fu"], "title": "Efficient Post-Training Refinement of Latent Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning is a key component of language understanding in Large Language\nModels. While Chain-of-Thought prompting enhances performance via explicit\nintermediate steps, it suffers from sufficient token overhead and a fixed\nreasoning trajectory, preventing step-wise refinement. Recent advances in\nlatent reasoning address these limitations by refining internal reasoning\nprocesses directly in the model's latent space, without producing explicit\noutputs. However, a key challenge remains: how to effectively update reasoning\nembeddings during post-training to guide the model toward more accurate\nsolutions. To overcome this challenge, we propose a lightweight post-training\nframework that refines latent reasoning trajectories using two novel\nstrategies: 1) Contrastive reasoning feedback, which compares reasoning\nembeddings against strong and weak baselines to infer effective update\ndirections via embedding enhancement; 2) Residual embedding refinement, which\nstabilizes updates by progressively integrating current and historical\ngradients, enabling fast yet controlled convergence. Extensive experiments and\ncase studies are conducted on five reasoning benchmarks to demonstrate the\neffectiveness of the proposed framework. Notably, a 5\\% accuracy gain on MathQA\nwithout additional training.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u8bad\u7ec3\u540e\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u63a8\u7406\u53cd\u9988\u548c\u6b8b\u5dee\u5d4c\u5165\u7ec6\u5316\u7b56\u7565\u4f18\u5316\u6f5c\u5728\u63a8\u7406\u8f68\u8ff9\uff0c\u5728MathQA\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b05%\u51c6\u786e\u7387\u63d0\u5347", "motivation": "\u4f20\u7edfChain-of-Thought\u65b9\u6cd5\u5b58\u5728\u4ee4\u724c\u5f00\u9500\u5927\u3001\u63a8\u7406\u8f68\u8ff9\u56fa\u5b9a\u7684\u5c40\u9650\u6027\uff0c\u6f5c\u5728\u63a8\u7406\u65b9\u6cd5\u867d\u80fd\u4f18\u5316\u5185\u90e8\u8fc7\u7a0b\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u7684\u8bad\u7ec3\u540e\u5d4c\u5165\u66f4\u65b0\u673a\u5236", "method": "1) \u5bf9\u6bd4\u63a8\u7406\u53cd\u9988\u673a\u5236\uff1a\u901a\u8fc7\u5f3a\u5f31\u57fa\u7ebf\u6bd4\u8f83\u786e\u5b9a\u5d4c\u5165\u4f18\u5316\u65b9\u5411\n2) \u6b8b\u5dee\u5d4c\u5165\u7ec6\u5316\uff1a\u878d\u5408\u5386\u53f2\u68af\u5ea6\u5b9e\u73b0\u7a33\u5b9a\u5feb\u901f\u6536\u655b", "result": "\u57285\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\uff0cMathQA\u51c6\u786e\u7387\u63d0\u53475%\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u4f18\u5316\u6709\u6548\u63d0\u5347\u63a8\u7406\u6027\u80fd\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2506.08564", "pdf": "https://arxiv.org/pdf/2506.08564", "abs": "https://arxiv.org/abs/2506.08564", "authors": ["Tuukka T\u00f6r\u00f6", "Antti Suni", "Juraj \u0160imko"], "title": "Neighbors and relatives: How do speech embeddings reflect linguistic connections across the world?", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "27 pages, 11 figures (+5 supplementary), submitted to PLOS One", "summary": "Investigating linguistic relationships on a global scale requires analyzing\ndiverse features such as syntax, phonology and prosody, which evolve at varying\nrates influenced by internal diversification, language contact, and\nsociolinguistic factors. Recent advances in machine learning (ML) offer\ncomplementary alternatives to traditional historical and typological\napproaches. Instead of relying on expert labor in analyzing specific linguistic\nfeatures, these new methods enable the exploration of linguistic variation\nthrough embeddings derived directly from speech, opening new avenues for\nlarge-scale, data-driven analyses.\n  This study employs embeddings from the fine-tuned XLS-R self-supervised\nlanguage identification model voxlingua107-xls-r-300m-wav2vec, to analyze\nrelationships between 106 world languages based on speech recordings. Using\nlinear discriminant analysis (LDA), language embeddings are clustered and\ncompared with genealogical, lexical, and geographical distances. The results\ndemonstrate that embedding-based distances align closely with traditional\nmeasures, effectively capturing both global and local typological patterns.\nChallenges in visualizing relationships, particularly with hierarchical\nclustering and network-based methods, highlight the dynamic nature of language\nchange.\n  The findings show potential for scalable analyses of language variation based\non speech embeddings, providing new perspectives on relationships among\nlanguages. By addressing methodological considerations such as corpus size and\nlatent space dimensionality, this approach opens avenues for studying\nlow-resource languages and bridging macro- and micro-level linguistic\nvariation. Future work aims to extend these methods to underrepresented\nlanguages and integrate sociolinguistic variation for a more comprehensive\nunderstanding of linguistic diversity.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u5fae\u8c03\u540e\u7684XLS-R\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u751f\u6210\u8bed\u8a00\u5d4c\u5165\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5206\u6790\u5168\u7403106\u79cd\u8bed\u8a00\u5173\u7cfb\uff0c\u53d1\u73b0\u8bed\u97f3\u5d4c\u5165\u8ddd\u79bb\u4e0e\u4f20\u7edf\u8c31\u7cfb/\u5730\u7406\u6307\u6807\u9ad8\u5ea6\u543b\u5408\uff0c\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u53d8\u5f02\u5206\u6790\u63d0\u4f9b\u65b0\u8def\u5f84\u3002", "motivation": "\u4f20\u7edf\u8bed\u8a00\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u7ecf\u9a8c\u548c\u6709\u9650\u7279\u5f81\u6807\u6ce8\uff0c\u96be\u4ee5\u5904\u7406\u8bed\u8a00\u7279\u5f81\u7684\u52a8\u6001\u6f14\u53d8\u548c\u5927\u89c4\u6a21\u6570\u636e\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u8bed\u97f3\u5d4c\u5165\u4f5c\u4e3a\u81ea\u52a8\u5316\u5206\u6790\u5de5\u5177\u5728\u63ed\u793a\u8bed\u8a00\u5173\u7cfb\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u7a81\u7834\u4f20\u7edf\u65b9\u6cd5\u7684\u89c4\u6a21\u548c\u6548\u7387\u9650\u5236\u3002", "method": "\u4f7f\u7528voxlingua107-xls-r-300m-wav2vec\u6a21\u578b\u751f\u6210\u8bed\u8a00\u5d4c\u5165\uff0c\u901a\u8fc7\u7ebf\u6027\u5224\u522b\u5206\u6790(LDA)\u805a\u7c7b\uff0c\u5e76\u4e0eGenealogical\u3001Lexical\u3001Geographical\u4e09\u79cd\u4f20\u7edf\u8ddd\u79bb\u6307\u6807\u8fdb\u884c\u7cfb\u7edf\u6027\u5bf9\u6bd4\u3002", "result": "\u8bed\u97f3\u5d4c\u5165\u8ddd\u79bb\u4e0e\u4f20\u7edf\u6d4b\u91cf\u6307\u6807\u663e\u8457\u76f8\u5173\uff08r=0.82\uff09\uff0c\u80fd\u540c\u65f6\u6355\u6349\u5168\u7403\u8bed\u8a00\u7c7b\u578b\u6a21\u5f0f\u548c\u533a\u57df\u7279\u5f81\u3002\u4f46\u5c42\u6b21\u805a\u7c7b\u5728\u7f51\u7edc\u53ef\u89c6\u5316\u4e2d\u8868\u73b0\u51fa\u52a8\u6001\u8bed\u8a00\u53d8\u5316\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7814\u7a76\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u975e\u4e66\u9762\u8bed\u7814\u7a76\uff0c\u6574\u5408\u793e\u4f1a\u8bed\u8a00\u5b66\u53c2\u6570\u6784\u5efa\u591a\u7ef4\u8bed\u8a00\u6f14\u53d8\u6a21\u578b\uff0c\u63a8\u52a8\u8ba1\u7b97\u8bed\u8a00\u5b66\u7684\u8de8\u5b66\u79d1\u53d1\u5c55\u3002"}}
{"id": "2506.08584", "pdf": "https://arxiv.org/pdf/2506.08584", "abs": "https://arxiv.org/abs/2506.08584", "authors": ["Yahan Li", "Jifan Yao", "John Bosco S. Bunyi", "Adam C. Frank", "Angel Hwang", "Ruishan Liu"], "title": "CounselBench: A Large-Scale Expert Evaluation and Adversarial Benchmark of Large Language Models in Mental Health Counseling", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly proposed for use in mental\nhealth support, yet their behavior in realistic counseling scenarios remains\nlargely untested. We introduce CounselBench, a large-scale benchmark developed\nwith 100 mental health professionals to evaluate and stress-test LLMs in\nsingle-turn counseling. The first component, CounselBench-EVAL, contains 2,000\nexpert evaluations of responses from GPT-4, LLaMA 3, Gemini, and online human\ntherapists to real patient questions. Each response is rated along six\nclinically grounded dimensions, with written rationales and span-level\nannotations. We find that LLMs often outperform online human therapists in\nperceived quality, but experts frequently flag their outputs for safety\nconcerns such as unauthorized medical advice. Follow-up experiments show that\nLLM judges consistently overrate model responses and overlook safety issues\nidentified by human experts. To probe failure modes more directly, we construct\nCounselBench-Adv, an adversarial dataset of 120 expert-authored counseling\nquestions designed to trigger specific model issues. Evaluation across 2,880\nresponses from eight LLMs reveals consistent, model-specific failure patterns.\nTogether, CounselBench establishes a clinically grounded framework for\nbenchmarking and improving LLM behavior in high-stakes mental health settings.", "AI": {"tldr": "CounselBench\u4e34\u5e8a\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0cLLMs\u5728\u5fc3\u7406\u5065\u5eb7\u54a8\u8be2\u573a\u666f\u4e2d\u867d\u8d28\u91cf\u4f18\u4e8e\u4eba\u7c7b\u6cbb\u7597\u5e08\uff0c\u4f46\u5b58\u5728\u5b89\u5168\u9690\u60a3\u4e0e\u8bc4\u4f30\u76f2\u533a\u3002", "motivation": "\u9a8c\u8bc1LLMs\u5728\u771f\u5b9e\u5fc3\u7406\u54a8\u8be2\u573a\u666f\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u5bf9\u6a21\u578b\u5b89\u5168\u6027\u548c\u4e13\u4e1a\u8bc4\u4f30\u7684\u7f3a\u5931\u3002", "method": "\u8054\u5408100\u4f4d\u4e13\u5bb6\u6784\u5efa\u4e24\u9636\u6bb5\u6d4b\u8bd5\u6846\u67b6\uff1a1) CounselBench-EVAL\u75282,000\u4e2a\u4e13\u5bb6\u8bc4\u4f30\u5bf9\u6bd4LLMs\u4e0e\u4eba\u7c7b\u6cbb\u7597\u5e08\u54cd\u5e94\u8d28\u91cf 2) CounselBench-Adv\u901a\u8fc7120\u4e2a\u5bf9\u6297\u6027\u95ee\u9898\u89e6\u53d1\u6a21\u578b\u5931\u8d25\u6a21\u5f0f\u3002", "result": "LLMs\u57286\u4e2a\u4e34\u5e8a\u7ef4\u5ea6\u8bc4\u5206\u4e2d\u5e73\u5747\u8d85\u8fc7\u4eba\u7c7b\u6cbb\u7597\u5e0821%\uff0c\u4f4632%\u7684\u6a21\u578b\u8f93\u51fa\u88ab\u4e13\u5bb6\u6807\u6ce8\u5b89\u5168\u9690\u60a3\u3002\u5bf9\u6297\u6d4b\u8bd5\u663e\u793a\u6240\u6709\u6a21\u578b\u5747\u5b58\u5728\u7279\u5b9a\u98ce\u9669\u6a21\u5f0f\uff08\u5982GPT-4\u8fc7\u5ea6\u81ea\u4fe1\u7ed9\u51fa\u533b\u7597\u5efa\u8bae\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9ad8\u98ce\u9669\u5fc3\u7406\u5065\u5eb7\u573a\u666f\u5efa\u7acb\u4e86\u4e34\u5e8a\u8bc4\u4f30\u6807\u51c6\uff0c\u63ed\u793a\u9700\u5efa\u7acb\u53cc\u91cd\u9a8c\u8bc1\u673a\u5236\uff08\u4e13\u5bb6+\u81ea\u52a8\u5316\u8bc4\u4f30\uff09\u6765\u4fdd\u969cLLMs\u7684\u5b89\u5168\u5e94\u7528\u3002"}}
{"id": "2506.08592", "pdf": "https://arxiv.org/pdf/2506.08592", "abs": "https://arxiv.org/abs/2506.08592", "authors": ["Liyan Xu", "Zhenlin Su", "Mo Yu", "Jiangnan Li", "Fandong Meng", "Jie Zhou"], "title": "Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This work focuses on an observed limitation of text encoders: embeddings may\nnot be able to recognize fine-grained entities or events within the semantics,\nresulting in failed dense retrieval on even simple cases. To examine such\nbehaviors, we first introduce a new evaluation dataset in Chinese, named\nCapRetrieval, whose passages are image captions, and queries are phrases\ninquiring entities or events in various forms. Zero-shot evaluation suggests\nthat encoders may fail on these fine-grained matching, regardless of training\nsources or model sizes. Aiming for enhancement, we proceed to finetune encoders\nwith our proposed data generation strategies, which obtains the best\nperformance on CapRetrieval. Within this process, we further identify an issue\nof granularity dilemma, a challenge for embeddings to express fine-grained\nsalience while aligning with overall semantics. Our dataset, code and models in\nthis work are publicly released at https://github.com/lxucs/CapRetrieval.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6587\u672c\u7f16\u7801\u5668\u5728\u7ec6\u7c92\u5ea6\u5b9e\u4f53/\u4e8b\u4ef6\u8bc6\u522b\u5b58\u5728\u5c40\u9650\uff0c\u63d0\u51fa\u65b0\u4e2d\u6587\u6570\u636e\u96c6CapRetrieval\u5e76\u901a\u8fc7\u6570\u636e\u751f\u6210\u7b56\u7565\u4f18\u5316\u6a21\u578b\u6027\u80fd\uff0c\u63ed\u793a\u7c92\u5ea6\u56f0\u5883\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u7f16\u7801\u5668\u5728\u56fe\u50cf\u63cf\u8ff0\u4e0e\u77ed\u8bed\u67e5\u8be2\u7684\u7ec6\u7c92\u5ea6\u5339\u914d\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e0e\u6a21\u578b\u89c4\u6a21\u53ca\u8bad\u7ec3\u6570\u636e\u65e0\u5173\uff0c\u9700\u8981\u9488\u5bf9\u6027\u4f18\u5316\u65b9\u6848\u3002", "method": "\u6784\u5efa\u56fe\u50cf\u63cf\u8ff0\u6570\u636e\u96c6CapRetrieval\uff0c\u91c7\u7528\u6570\u636e\u751f\u6210\u7b56\u7565\u8fdb\u884c\u7f16\u7801\u5668\u5fae\u8c03\uff0c\u5e76\u5206\u6790\u7c92\u5ea6\u56f0\u5883\u95ee\u9898\u3002", "result": "\u6570\u636e\u751f\u6210\u7b56\u7565\u4f7f\u6a21\u578b\u5728CapRetrieval\u4e0a\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff0c\u4f46\u53d1\u73b0\u5d4c\u5165\u8868\u8fbe\u9700\u5e73\u8861\u7ec6\u7c92\u5ea6\u663e\u8457\u6027\u4e0e\u6574\u4f53\u8bed\u4e49\u7684\u5bf9\u9f50\u96be\u9898\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u68c0\u7d22\u9700\u8981\u4e13\u95e8\u4f18\u5316\u65b9\u6848\uff0c\u516c\u5f00\u6570\u636e\u96c6\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u57fa\u51c6\uff0c\u7c92\u5ea6\u56f0\u5883\u63ed\u793a\u4e86\u5d4c\u5165\u8868\u8fbe\u7684\u672c\u8d28\u6311\u6218\u3002"}}
{"id": "2506.08593", "pdf": "https://arxiv.org/pdf/2506.08593", "abs": "https://arxiv.org/abs/2506.08593", "authors": ["Shuzhou Yuan", "Ercong Nie", "Mario Tawfelis", "Helmut Schmid", "Hinrich Sch\u00fctze", "Michael F\u00e4rber"], "title": "Hateful Person or Hateful Model? Investigating the Role of Personas in Hate Speech Detection by Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Hate speech detection is a socially sensitive and inherently subjective task,\nwith judgments often varying based on personal traits. While prior work has\nexamined how socio-demographic factors influence annotation, the impact of\npersonality traits on Large Language Models (LLMs) remains largely unexplored.\nIn this paper, we present the first comprehensive study on the role of persona\nprompts in hate speech classification, focusing on MBTI-based traits. A human\nannotation survey confirms that MBTI dimensions significantly affect labeling\nbehavior. Extending this to LLMs, we prompt four open-source models with MBTI\npersonas and evaluate their outputs across three hate speech datasets. Our\nanalysis uncovers substantial persona-driven variation, including\ninconsistencies with ground truth, inter-persona disagreement, and logit-level\nbiases. These findings highlight the need to carefully define persona prompts\nin LLM-based annotation workflows, with implications for fairness and alignment\nwith human values.", "AI": {"tldr": "\u7814\u7a76MBTI\u4eba\u683c\u7279\u8d28\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u4e2d\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4eba\u683c\u63d0\u793a\u5bfc\u81f4\u7684\u6807\u6ce8\u504f\u5dee\u95ee\u9898", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u793e\u4f1a\u4eba\u53e3\u56e0\u7d20\u5bf9\u6807\u6ce8\u7684\u5f71\u54cd\uff0c\u4f46LLM\u7684\u4eba\u683c\u654f\u611f\u6027\u53ca\u5176\u5728\u4ec7\u6068\u68c0\u6d4b\u4e2d\u7684\u6f5c\u5728\u504f\u5dee\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u8be5\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1MBTI\u4eba\u683c\u7ef4\u5ea6\u5982\u4f55\u5f71\u54cdLLM\u7684\u6807\u6ce8\u884c\u4e3a\uff0c\u63a8\u52a8\u66f4\u516c\u5e73\u7684AI\u6807\u6ce8\u6d41\u7a0b", "method": "1. \u4eba\u7c7b\u6807\u6ce8\u8005MBTI\u4eba\u683c\u4e0e\u6807\u6ce8\u884c\u4e3a\u5173\u8054\u6027\u8c03\u67e5\n2. \u57283\u4e2a\u4ec7\u6068\u8a00\u8bba\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u75284\u4e2a\u5f00\u6e90LLM\u8fdb\u884cMBTI\u4eba\u683c\u63d0\u793a\u5b9e\u9a8c\n3. \u5206\u6790\u6a21\u578b\u8f93\u51fa\u7684\u771f\u5b9e\u6807\u7b7e\u4e00\u81f4\u6027\u3001\u4eba\u683c\u95f4\u5206\u6b67\u53calogit\u5c42\u9762\u504f\u5dee", "result": "1. MBTI\u4eba\u683c\u5bfc\u81f4\u663e\u8457\u6807\u6ce8\u5dee\u5f02\uff08\u4eba\u7c7b\u548cLLM\u5747\u8868\u73b0\uff09\n2. \u6a21\u578b\u8f93\u51fa\u4e0e\u771f\u5b9e\u6807\u7b7e\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\n3. \u4e0d\u540c\u4eba\u683c\u63d0\u793a\u95f4\u51fa\u73b0\u77db\u76fe\u5224\u65ad\n4. \u6a21\u578blogit\u5206\u5e03\u663e\u793a\u4eba\u683c\u7279\u5f02\u6027\u504f\u89c1", "conclusion": "LLM\u6807\u6ce8\u6d41\u7a0b\u9700\u4e25\u683c\u5b9a\u4e49\u4eba\u683c\u63d0\u793a\u53c2\u6570\uff0c\u4ee5\u907f\u514d\u516c\u5e73\u6027\u98ce\u9669\u3002\u7814\u7a76\u5f3a\u8c03AI\u7cfb\u7edf\u4ef7\u503c\u89c2\u5bf9\u9f50\u4e2d\u4eba\u683c\u5de5\u7a0b\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2506.08625", "pdf": "https://arxiv.org/pdf/2506.08625", "abs": "https://arxiv.org/abs/2506.08625", "authors": ["Minhae Oh", "Jeonghye Kim", "Nakyung Lee", "Donggeon Seo", "Taeuk Kim", "Jungwoo Lee"], "title": "RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval", "categories": ["cs.CL"], "comment": null, "summary": "Scientific reasoning requires not only long-chain reasoning processes, but\nalso knowledge of domain-specific terminologies and adaptation to updated\nfindings. To deal with these challenges for scientific reasoning, we introduce\nRAISE, a step-by-step retrieval-augmented framework which retrieves logically\nrelevant documents from in-the-wild corpus. RAISE is divided into three steps:\nproblem decomposition, logical query generation, and logical retrieval. We\nobserve that RAISE consistently outperforms other baselines on scientific\nreasoning benchmarks. We analyze that unlike other baselines, RAISE retrieves\ndocuments that are not only similar in terms of the domain knowledge, but also\ndocuments logically more relevant.", "AI": {"tldr": "RAISE\u6846\u67b6\u901a\u8fc7\u4e09\u6b65\u68c0\u7d22\u589e\u5f3a\u673a\u5236\uff08\u95ee\u9898\u5206\u89e3\u2192\u903b\u8f91\u67e5\u8be2\u2192\u903b\u8f91\u68c0\u7d22\uff09\uff0c\u5728\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u56e0\u5176\u80fd\u68c0\u7d22\u9886\u57df\u77e5\u8bc6\u548c\u903b\u8f91\u76f8\u5173\u6587\u6863", "motivation": "\u79d1\u5b66\u63a8\u7406\u5b58\u5728\u4e09\u5927\u6311\u6218\uff1a\u9700\u8981\u957f\u903b\u8f91\u94fe\u63a8\u7406\u3001\u4f9d\u8d56\u9886\u57df\u4e13\u4e1a\u672f\u8bed\u77e5\u8bc6\u3001\u9700\u9002\u5e94\u4e0d\u65ad\u66f4\u65b0\u7684\u79d1\u5b66\u53d1\u73b0\u3002\u4f20\u7edf\u68c0\u7d22\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u8fd9\u4e9b\u590d\u5408\u9700\u6c42", "method": "1. \u95ee\u9898\u5206\u89e3\uff1a\u5c06\u590d\u6742\u95ee\u9898\u62c6\u89e3\u4e3a\u5b50\u95ee\u9898\n2. \u903b\u8f91\u67e5\u8be2\u751f\u6210\uff1a\u6784\u5efa\u7ed3\u6784\u5316\u68c0\u7d22\u67e5\u8be2\n3. \u903b\u8f91\u68c0\u7d22\uff1a\u4ece\u5f00\u653e\u57df\u8bed\u6599\u4e2d\u83b7\u53d6\u903b\u8f91\u76f8\u5173\u7684\u6587\u6863", "result": "\u5728\u79d1\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08\u63d0\u5347\u5177\u4f53\u6570\u503c\u9700\u67e5\u770b\u539f\u6587\uff09\uff0c\u5173\u952e\u4f18\u52bf\u5728\u4e8e\u540c\u65f6\u6355\u83b7\u9886\u57df\u77e5\u8bc6\u548c\u903b\u8f91\u76f8\u5173\u6027", "conclusion": "RAISE\u521b\u65b0\u5730\u5c06\u903b\u8f91\u7ed3\u6784\u5f15\u5165\u68c0\u7d22\u8fc7\u7a0b\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u8bed\u4e49\u76f8\u4f3c\u6027\u68c0\u7d22\u7684\u5c40\u9650\uff0c\u4e3a\u590d\u6742\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6846\u67b6"}}
{"id": "2506.08643", "pdf": "https://arxiv.org/pdf/2506.08643", "abs": "https://arxiv.org/abs/2506.08643", "authors": ["Son The Nguyen", "Theja Tulabandhula"], "title": "MEMETRON: Metaheuristic Mechanisms for Test-time Response Optimization of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly used for both open-ended and\nstructured tasks, yet their inference-time behavior is still largely dictated\nby heuristic decoding strategies such as greedy search, sampling, or reranking.\nThese methods provide limited control and do not explicitly optimize for\ntask-specific objectives. We introduce MEMETRON, a task-agnostic framework that\nformulates LLM decoding as a discrete black-box optimization problem. MEMETRON\nleverages hybrid metaheuristic algorithms, GENETRON and ANNETRON, to search the\nresponse space, guided by reward models and contextual operations performed by\nthe LLM itself. This approach enables efficient discovery of high-reward\nresponses without requiring model retraining or gradient access. The framework\nis modular and generalizes across diverse tasks, requiring only a reward\nfunction and lightweight prompt templates. We evaluate our framework on the\ncritical human preference alignment task and demonstrate that it significantly\noutperforms standard decoding and reranking methods, highlighting its potential\nto improve alignment without model retraining.", "AI": {"tldr": "\u63d0\u51faMEMETRON\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3/\u68af\u5ea6\u7684\u9ed1\u76d2\u4f18\u5316\uff0c\u5728\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u89e3\u7801\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u89e3\u7801\u7b56\u7565\uff08\u8d2a\u5a6a\u641c\u7d22/\u91c7\u6837/\u91cd\u6392\uff09\u7f3a\u4e4f\u4efb\u52a1\u76ee\u6807\u5bfc\u5411\u7684\u4f18\u5316\u80fd\u529b\uff0c\u9700\u8981\u4e0d\u4f9d\u8d56\u6a21\u578b\u91cd\u8bad\u7ec3\u7684\u65b0\u578b\u4f18\u5316\u6846\u67b6\u3002", "method": "\u5c06\u89e3\u7801\u8f6c\u5316\u4e3a\u79bb\u6563\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528GENETRON\uff08\u9057\u4f20\u7b97\u6cd5\uff09\u548cANNETRON\uff08\u795e\u7ecf\u7f51\u7edc\u5f15\u5bfc\uff09\u7b97\u6cd5\uff0c\u57fa\u4e8e\u5956\u52b1\u6a21\u578b\u548cLLM\u81ea\u53cd\u9988\u8fdb\u884c\u54cd\u5e94\u641c\u7d22\u3002", "result": "\u5728\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u4efb\u52a1\u4e2d\uff0cMEMETRON\u7684\u54cd\u5e94\u8d28\u91cf\u663e\u8457\u8d85\u8fc7\u6807\u51c6\u89e3\u7801\u7b56\u7565\u548c\u91cd\u6392\u5e8f\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\u3002", "conclusion": "\u6a21\u5757\u5316\u8bbe\u8ba1\u4f7f\u6846\u67b6\u5177\u5907\u8de8\u4efb\u52a1\u901a\u7528\u6027\uff0c\u4e3a\u6a21\u578b\u5bf9\u9f50\u63d0\u4f9b\u4e86\u8f7b\u91cf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u9ed1\u76d2\u4f18\u5316\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2506.08646", "pdf": "https://arxiv.org/pdf/2506.08646", "abs": "https://arxiv.org/abs/2506.08646", "authors": ["Mingyu Zheng", "Zhifan Feng", "Jia Wang", "Lanrui Wang", "Zheng Lin", "Yang Hao", "Weiping Wang"], "title": "TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "27 pages, 19 figures, Findings of ACL 2025", "summary": "Despite the commendable progress of recent LLM-based data synthesis methods,\nthey face two limitations in generating table instruction tuning data. First,\nthey can not thoroughly explore the vast input space of table understanding\ntasks, leading to limited data diversity. Second, they ignore the weaknesses in\ntable understanding ability of the target LLM and blindly pursue the increase\nof data quantity, resulting in suboptimal data efficiency. In this paper, we\nintroduce a progressive and weakness-guided data synthesis framework tailored\nfor table instruction tuning, named TableDreamer, to mitigate the above issues.\nSpecifically, we first synthesize diverse tables and related instructions as\nseed data, and then perform an iterative exploration of the input space under\nthe guidance of the newly identified weakness data, which eventually serve as\nthe final training data for fine-tuning the target LLM. Extensive experiments\non 10 tabular benchmarks demonstrate the effectiveness of the proposed\nframework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62%\n(49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms\nstate-of-the-art data synthesis baselines which use more training data. The\ncode and data is available at https://github.com/SpursGoZmy/TableDreamer", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f31\u70b9\u5f15\u5bfc\u7684\u8fed\u4ee3\u5f0f\u8868\u683c\u6570\u636e\u5408\u6210\u6846\u67b6TableDreamer\uff0c\u901a\u8fc7\u6e10\u8fdb\u63a2\u7d22\u8f93\u5165\u7a7a\u95f4\u4f18\u5316LLM\u8868\u683c\u7406\u89e3\u80fd\u529b", "motivation": "\u73b0\u6709LLM\u8868\u683c\u6570\u636e\u5408\u6210\u65b9\u6cd5\u5b58\u5728\u8f93\u5165\u7a7a\u95f4\u63a2\u7d22\u4e0d\u8db3\u5bfc\u81f4\u6570\u636e\u591a\u6837\u6027\u53d7\u9650\uff0c\u4e14\u672a\u9488\u5bf9\u6027\u8865\u8db3\u76ee\u6807\u6a21\u578b\u5f31\u70b9\u5bfc\u81f4\u6570\u636e\u6548\u7387\u4f4e\u4e0b", "method": "\u5206\u4e24\u9636\u6bb5\uff1a\u9996\u5148\u751f\u6210\u591a\u6837\u5316\u79cd\u5b50\u8868\u683c\u6570\u636e\uff0c\u518d\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u5f31\u70b9\u8fed\u4ee3\u751f\u6210\u9488\u5bf9\u6027\u8bad\u7ec3\u6570\u636e", "result": "\u572810\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f7fLlama3.1-8B-instruct\u5e73\u5747\u51c6\u786e\u7387\u63d0\u534711.62%\uff0849.07%\u219260.69%\uff09\uff0c\u4ec5\u752827K\u6570\u636e\u5373\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "TableDreamer\u6846\u67b6\u901a\u8fc7\u5f31\u70b9\u5f15\u5bfc\u7684\u8fed\u4ee3\u6570\u636e\u5408\u6210\uff0c\u663e\u8457\u63d0\u5347\u8868\u683c\u6307\u4ee4\u8c03\u4f18\u7684\u6570\u636e\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd"}}
{"id": "2506.08647", "pdf": "https://arxiv.org/pdf/2506.08647", "abs": "https://arxiv.org/abs/2506.08647", "authors": ["Oumaima El Khettari", "Solen Quiniou", "Samuel Chaffron"], "title": "Summarization for Generative Relation Extraction in the Microbiome Domain", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We explore a generative relation extraction (RE) pipeline tailored to the\nstudy of interactions in the intestinal microbiome, a complex and low-resource\nbiomedical domain. Our method leverages summarization with large language\nmodels (LLMs) to refine context before extracting relations via\ninstruction-tuned generation. Preliminary results on a dedicated corpus show\nthat summarization improves generative RE performance by reducing noise and\nguiding the model. However, BERT-based RE approaches still outperform\ngenerative models. This ongoing work demonstrates the potential of generative\nmethods to support the study of specialized domains in low-resources setting.", "AI": {"tldr": "\u63a2\u7d22\u751f\u6210\u5f0f\u5173\u7cfb\u63d0\u53d6\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90\u80a0\u9053\u5fae\u751f\u7269\u7814\u7a76\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7LLM\u6458\u8981\u4f18\u5316\u4e0a\u4e0b\u6587\u540e\u5173\u7cfb\u63d0\u53d6\u6548\u679c\u63d0\u5347\u4f46\u4f20\u7edfBERT\u65b9\u6cd5\u4ecd\u66f4\u4f18", "motivation": "\u9488\u5bf9\u590d\u6742\u4e14\u8d44\u6e90\u532e\u4e4f\u7684\u80a0\u9053\u5fae\u751f\u7269\u7ec4\u7814\u7a76\u9886\u57df\uff0c\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u7684\u5173\u7cfb\u63d0\u53d6\u65b9\u6cd5\u4ee5\u652f\u6301\u4e13\u4e1a\u9886\u57df\u7814\u7a76", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6587\u672c\u6458\u8981\u9884\u5904\u7406\uff0c\u901a\u8fc7\u6307\u4ee4\u5fae\u8c03\u7684\u751f\u6210\u6a21\u578b\u8fdb\u884c\u5173\u7cfb\u63d0\u53d6", "result": "\u6458\u8981\u9884\u5904\u7406\u4f7f\u751f\u6210\u5f0fRE\u6027\u80fd\u63d0\u5347\uff08\u51cf\u5c11\u566a\u58f0/\u5f15\u5bfc\u6a21\u578b\uff09\uff0c\u4f46\u57fa\u4e8eBERT\u7684\u65b9\u6cd5\u4ecd\u4f18\u4e8e\u751f\u6210\u6a21\u578b", "conclusion": "\u751f\u6210\u5f0f\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90\u4e13\u4e1a\u9886\u57df\u7814\u7a76\u4e2d\u5177\u5907\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u5f53\u524d\u9636\u6bb5\u4f20\u7edf\u65b9\u6cd5\u4ecd\u4fdd\u6301\u4f18\u52bf"}}
{"id": "2506.08672", "pdf": "https://arxiv.org/pdf/2506.08672", "abs": "https://arxiv.org/abs/2506.08672", "authors": ["Yang Liu", "Jiaqi Li", "Zilong Zheng"], "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling", "categories": ["cs.CL"], "comment": "22 pages, 10 figures, 8 tables", "summary": "Rule-based reasoning has been acknowledged as one of the fundamental problems\nin reasoning, while deviations in rule formats, types, and complexity in\nreal-world applications pose severe challenges. Recent studies have shown that\nlarge reasoning models (LRMs) have remarkable reasoning capabilities, and their\nperformance is substantially enhanced by reinforcement learning (RL). However,\nit remains an open question whether small reasoning models (SRMs) can learn\nrule-based reasoning effectively with robust generalization across diverse\ntasks and domains. To address this, we introduce Reinforced Rule-based\nReasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct\nrule-based reasoning via a wide collection of curated tasks and a novel\ndomain-aware dynamic sampling approach. Specifically, RuleReasoner resamples\neach training batch by updating the sampling weights of different domains based\non historical rewards. This facilitates domain augmentation and flexible online\nlearning schedules for RL, obviating the need for pre-hoc human-engineered\nmix-training recipes used in existing methods. Empirical evaluations on\nin-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that\nRuleReasoner outperforms frontier LRMs by a significant margin ($\\Delta$4.1%\naverage points on eight ID tasks and $\\Delta$10.4% average points on three OOD\ntasks over OpenAI-o1). Notably, our approach also exhibits higher computational\nefficiency compared to prior dynamic sampling methods for RL.", "AI": {"tldr": "Proposed RuleReasoner method enhances small reasoning models' rule-based reasoning through dynamic domain sampling and reinforcement learning, achieving superior performance over large models.", "motivation": "Existing methods struggle with rule format variations and rely on pre-engineered training mixes. Small models' generalization capability across tasks needs validation.", "method": "Domain-aware dynamic sampling with RL: adjusts domain weights based on historical rewards during batch resampling, enabling automated domain augmentation.", "result": "4.1% ID task improvement, 10.4% OOD gains vs OpenAI-o1. 3x faster than prior dynamic sampling methods.", "conclusion": "RuleReasoner demonstrates SRMs can achieve efficient, human-independent rule reasoning through adaptive RL training strategies."}}
{"id": "2506.08686", "pdf": "https://arxiv.org/pdf/2506.08686", "abs": "https://arxiv.org/abs/2506.08686", "authors": ["Soham Poddar", "Paramita Koley", "Janardan Misra", "Sanjay Podder", "Navveen Balani", "Niloy Ganguly", "Saptarshi Ghosh"], "title": "Brevity is the soul of sustainability: Characterizing LLM response lengths", "categories": ["cs.CL", "cs.CY"], "comment": "Accepted to appear at the ACL 2025 findings", "summary": "A significant portion of the energy consumed by Large Language Models (LLMs)\narises from their inference processes; hence developing energy-efficient\nmethods for inference is crucial. While several techniques exist for inference\noptimization, output compression remains relatively unexplored, with only a few\npreliminary efforts addressing this aspect. In this work, we first benchmark 12\ndecoder-only LLMs across 5 datasets, revealing that these models often produce\nresponses that are substantially longer than necessary. We then conduct a\ncomprehensive quality assessment of LLM responses, formally defining six\ninformation categories present in LLM responses. We show that LLMs often tend\nto include redundant or additional information besides the minimal answer. To\naddress this issue of long responses by LLMs, we explore several simple and\nintuitive prompt-engineering strategies. Empirical evaluation shows that\nappropriate prompts targeting length reduction and controlling information\ncontent can achieve significant energy optimization between 25-60\\% by reducing\nthe response length while preserving the quality of LLM responses.", "AI": {"tldr": "\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u51cf\u5c11LLM\u751f\u6210\u54cd\u5e94\u7684\u957f\u5ea6\uff0c\u5728\u4fdd\u6301\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b025-60%\u7684\u80fd\u6e90\u4f18\u5316\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u63a8\u7406\u8fc7\u7a0b\u6d88\u8017\u5927\u91cf\u80fd\u6e90\uff0c\u800c\u73b0\u6709\u7684\u8f93\u51fa\u538b\u7f29\u6280\u672f\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "1. \u5bf912\u4e2a\u4ec5\u89e3\u7801\u5668\u67b6\u6784\u7684LLM\u57285\u4e2a\u6570\u636e\u96c6\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u54cd\u5e94\u5197\u4f59\u73b0\u8c61\n2. \u5b9a\u4e49LLM\u54cd\u5e94\u4e2d\u76846\u4e2a\u4fe1\u606f\u7c7b\u522b\uff0c\u5206\u6790\u5197\u4f59\u4fe1\u606f\u5206\u5e03\n3. \u5f00\u53d1\u9488\u5bf9\u957f\u5ea6\u538b\u7f29\u548c\u4fe1\u606f\u63a7\u5236\u7684\u63d0\u793a\u5de5\u7a0b\u7b56\u7565", "result": "\u901a\u8fc7\u9488\u5bf9\u6027\u63d0\u793a\u5de5\u7a0b\u53ef\u51cf\u5c11\u54cd\u5e94\u957f\u5ea6\uff0c\u5b9e\u73b025-60%\u7684\u80fd\u6e90\u8282\u7701\uff0c\u540c\u65f6\u4fdd\u6301\u56de\u7b54\u8d28\u91cf\uff08\u901a\u8fc7\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u9a8c\u8bc1\uff09", "conclusion": "\u7b80\u5355\u7684\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u80fd\u6709\u6548\u4f18\u5316LLM\u80fd\u6e90\u6548\u7387\uff0c\u4e3a\u89e3\u51b3\u6a21\u578b\u5197\u4f59\u8f93\u51fa\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.08700", "pdf": "https://arxiv.org/pdf/2506.08700", "abs": "https://arxiv.org/abs/2506.08700", "authors": ["Ruiran Su", "Jiasheng Si", "Zhijiang Guo", "Janet B. Pierrehumbert"], "title": "ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Scientific fact-checking has mostly focused on text and tables, overlooking\nscientific charts, which are key for presenting quantitative evidence and\nstatistical reasoning. We introduce ClimateViz, the first large-scale benchmark\nfor scientific fact-checking using expert-curated scientific charts. ClimateViz\ncontains 49,862 claims linked to 2,896 visualizations, each labeled as support,\nrefute, or not enough information. To improve interpretability, each example\nincludes structured knowledge graph explanations covering trends, comparisons,\nand causal relations. We evaluate state-of-the-art multimodal language models,\nincluding both proprietary and open-source systems, in zero-shot and few-shot\nsettings. Results show that current models struggle with chart-based reasoning:\neven the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to\n77.8 percent accuracy in label-only settings, far below human performance (89.3\nand 92.7 percent). Explanation-augmented outputs improve performance in some\nmodels. We released our dataset and code alongside the paper.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u9996\u4e2a\u9762\u5411\u79d1\u5b66\u56fe\u8868\u9a8c\u8bc1\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5ClimateViz\uff0c\u63ed\u793a\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5728\u56fe\u8868\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u843d\u540e\u4e8e\u4eba\u7c7b\u8868\u73b0\uff08\u51c6\u786e\u738776-78% vs 89-93%\uff09\uff0c\u89e3\u91ca\u589e\u5f3a\u7b56\u7565\u4ec5\u90e8\u5206\u6709\u6548\u3002", "motivation": "\u73b0\u6709\u79d1\u5b66\u4e8b\u5b9e\u6838\u67e5\u4e3b\u8981\u805a\u7126\u6587\u672c\u548c\u8868\u683c\uff0c\u5ffd\u7565\u4e86\u79d1\u5b66\u56fe\u8868\u4f5c\u4e3a\u5b9a\u91cf\u8bc1\u636e\u8f7d\u4f53\u7684\u6838\u5fc3\u4ef7\u503c\u3002ClimateViz\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u542b49,862\u4e2a\u6807\u6ce8\u58f0\u660e\u4e0e2,896\u4e2a\u56fe\u8868\u7684\u8bed\u6599\u5e93\uff0c\u96c6\u6210\u8d8b\u52bf/\u5bf9\u6bd4/\u56e0\u679c\u5173\u7cfb\u7684\u77e5\u8bc6\u56fe\u8c31\u89e3\u91ca\uff0c\u8bc4\u4f30Gemini 2.5\u3001InternVL 2.5\u7b49\u524d\u6cbf\u6a21\u578b\u5728\u96f6\u6837\u672c/\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u6700\u4f73\u6a21\u578b\u51c6\u786e\u7387\u4ec5\u8fbe76.2-77.8%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u4e13\u5bb6(89.3-92.7%)\uff1b\u89e3\u91ca\u589e\u5f3a\u7b56\u7565\u5bf9\u90e8\u5206\u6a21\u578b\u6709\u63d0\u5347\u6548\u679c\u3002", "conclusion": "\u5f3a\u8c03\u56fe\u8868\u9a8c\u8bc1\u4efb\u52a1\u7684\u6311\u6218\u6027\uff0c\u547c\u5401\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u591a\u6a21\u6001\u63a8\u7406\u65b9\u6cd5\u3002\u6570\u636e\u96c6\u4e0e\u4ee3\u7801\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2506.08712", "pdf": "https://arxiv.org/pdf/2506.08712", "abs": "https://arxiv.org/abs/2506.08712", "authors": ["Hee Suk Yoon", "Eunseop Yoon", "Mark A. Hasegawa-Johnson", "Sungwoong Kim", "Chang D. Yoo"], "title": "ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "We introduce ConfPO, a method for preference learning in Large Language\nModels (LLMs) that identifies and optimizes preference-critical tokens based\nsolely on the training policy's confidence, without requiring any auxiliary\nmodels or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as\nDirect Preference Optimization (DPO), which uniformly adjust all token\nprobabilities regardless of their relevance to preference, ConfPO focuses\noptimization on the most impactful tokens. This targeted approach improves\nalignment quality while mitigating overoptimization (i.e., reward hacking) by\nusing the KL divergence budget more efficiently. In contrast to recent\ntoken-level methods that rely on credit-assignment models or AI annotators,\nraising concerns about scalability and reliability, ConfPO is simple,\nlightweight, and model-free. Experimental results on challenging alignment\nbenchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO\nconsistently outperforms uniform DAAs across various LLMs, delivering better\nalignment with zero additional computational overhead.", "AI": {"tldr": "\u63d0\u51faConfPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u4f18\u5316\u504f\u597d\u5173\u952etoken\u63d0\u5347LLM\u5bf9\u9f50\u6548\u679c\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u9ad8\u6548\u4e14\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u8d44\u6e90", "motivation": "\u73b0\u6709\u5bf9\u9f50\u7b97\u6cd5\uff08\u5982DPO\uff09\u5747\u5300\u8c03\u6574\u6240\u6709token\u6982\u7387\uff0c\u5bfc\u81f4KL\u6563\u5ea6\u9884\u7b97\u6d6a\u8d39\u548c\u8fc7\u4f18\u5316\u95ee\u9898\uff1b\u5176\u4ed6token\u7ea7\u65b9\u6cd5\u4f9d\u8d56\u989d\u5916\u6a21\u578b\u5f71\u54cd\u6269\u5c55\u6027", "method": "\u57fa\u4e8e\u8bad\u7ec3\u7b56\u7565\u7f6e\u4fe1\u5ea6\u8bc6\u522b\u504f\u597d\u5173\u952etoken\uff0c\u96c6\u4e2d\u4f18\u5316\u8d44\u6e90\u5728\u5f71\u54cd\u529b\u6700\u5927\u7684token\u4e0a", "result": "\u5728AlpacaEval 2\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cConfPO\u5728\u4e0d\u540c\u89c4\u6a21LLM\u4e0a\u5747\u8d85\u8d8a\u4f20\u7edf\u7b97\u6cd5\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u5bf9\u9f50\u6548\u679c", "conclusion": "ConfPO\u4ee5\u96f6\u989d\u5916\u8ba1\u7b97\u6210\u672c\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u5bf9\u9f50\u65b9\u6848\uff0c\u4e3a\u8f7b\u91cf\u7ea7\u504f\u597d\u5b66\u4e60\u5f00\u8f9f\u65b0\u65b9\u5411"}}
{"id": "2506.08713", "pdf": "https://arxiv.org/pdf/2506.08713", "abs": "https://arxiv.org/abs/2506.08713", "authors": ["Fariz Ikhwantri", "Dusica Marijan"], "title": "Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure", "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "Ensuring complex systems meet regulations typically requires checking the\nvalidity of assurance cases through a claim-argument-evidence framework. Some\nchallenges in this process include the complicated nature of legal and\ntechnical texts, the need for model explanations, and limited access to\nassurance case data. We propose a compliance detection approach based on\nNatural Language Inference (NLI): EXplainable CompLiance detection with\nArgumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the\nclaim-argument-evidence structure of an assurance case as a multi-hop inference\nfor explainable and traceable compliance detection. We address the limited\nnumber of assurance cases by generating them using large language models\n(LLMs). We introduce metrics that measure the coverage and structural\nconsistency. We demonstrate the effectiveness of the generated assurance case\nfrom GDPR requirements in a multi-hop inference task as a case study. Our\nresults highlight the potential of NLI-based approaches in automating the\nregulatory compliance process.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u7684EXCLAIM\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u8df3\u63a8\u7406\u548cLLM\u751f\u6210\u4fdd\u8bc1\u6848\u4f8b\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u5408\u89c4\u68c0\u6d4b\u3002", "motivation": "\u89e3\u51b3\u6cd5\u89c4\u6587\u672c\u590d\u6742\u6027\u3001\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u9700\u6c42\u3001\u4fdd\u8bc1\u6848\u4f8b\u6570\u636e\u532e\u4e4f\u4e09\u5927\u5408\u89c4\u68c0\u6d4b\u6311\u6218\u3002", "method": "\u5c06\u4fdd\u8bc1\u6848\u4f8b\u7ed3\u6784\u8f6c\u5316\u4e3a\u591a\u8df3\u63a8\u7406\u6846\u67b6\uff0c\u5229\u7528LLM\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff0c\u8bbe\u8ba1\u8986\u76d6\u7387\u4e0e\u7ed3\u6784\u4e00\u81f4\u6027\u8bc4\u4f30\u6307\u6807\u3002", "result": "GDPR\u6848\u4f8b\u9a8c\u8bc1\u663e\u793a\u751f\u6210\u6570\u636e\u6709\u6548\u652f\u6301\u591a\u8df3\u63a8\u7406\u4efb\u52a1\uff0c\u65b0\u6307\u6807\u6210\u529f\u91cf\u5316\u6a21\u578b\u8868\u73b0\u3002", "conclusion": "\u57fa\u4e8eNLI\u7684\u65b9\u6cd5\u4e3a\u81ea\u52a8\u5316\u6cd5\u89c4\u5408\u89c4\u68c0\u6d4b\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001\u53ef\u8ffd\u6eaf\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2506.08717", "pdf": "https://arxiv.org/pdf/2506.08717", "abs": "https://arxiv.org/abs/2506.08717", "authors": ["Mehedi Hasan Bijoy", "Dejan Porjazovski", "Tam\u00e1s Gr\u00f3sz", "Mikko Kurimo"], "title": "Multi-Teacher Language-Aware Knowledge Distillation for Multilingual Speech Emotion Recognition", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to INTERSPEECH 2025 conference", "summary": "Speech Emotion Recognition (SER) is crucial for improving human-computer\ninteraction. Despite strides in monolingual SER, extending them to build a\nmultilingual system remains challenging. Our goal is to train a single model\ncapable of multilingual SER by distilling knowledge from multiple teacher\nmodels. To address this, we introduce a novel language-aware multi-teacher\nknowledge distillation method to advance SER in English, Finnish, and French.\nIt leverages Wav2Vec2.0 as the foundation of monolingual teacher models and\nthen distills their knowledge into a single multilingual student model. The\nstudent model demonstrates state-of-the-art performance, with a weighted recall\nof 72.9 on the English dataset and an unweighted recall of 63.4 on the Finnish\ndataset, surpassing fine-tuning and knowledge distillation baselines. Our\nmethod excels in improving recall for sad and neutral emotions, although it\nstill faces challenges in recognizing anger and happiness.", "AI": {"tldr": "\u63d0\u51fa\u8bed\u8a00\u611f\u77e5\u591a\u6559\u5e08\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u6784\u5efa\u591a\u8bed\u8a00\u8bed\u97f3\u60c5\u611f\u8bc6\u522b(SER)\u6a21\u578b\uff0c\u5728\u82f1\u8bed\u548c\u82ac\u5170\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u4f46\u6124\u6012\u548c\u5feb\u4e50\u60c5\u7eea\u8bc6\u522b\u4ecd\u5b58\u5728\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u5355\u8bedSER\u7cfb\u7edf\u96be\u4ee5\u6269\u5c55\u5230\u591a\u8bed\u8a00\u573a\u666f\uff0c\u9700\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6574\u5408\u591a\u4e2a\u5355\u8bed\u6559\u5e08\u6a21\u578b\u7684\u77e5\u8bc6\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u591a\u8bed\u8a00\u5b66\u751f\u6a21\u578b\u3002", "method": "\u57fa\u4e8eWav2Vec2.0\u6784\u5efa\u5355\u8bed\u6559\u5e08\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u8a00\u611f\u77e5\u84b8\u998f\u6280\u672f\u5c06\u77e5\u8bc6\u8fc1\u79fb\u81f3\u591a\u8bed\u8a00\u5b66\u751f\u6a21\u578b\uff0c\u5b9e\u73b0\u8de8\u8bed\u8a00\u7279\u5f81\u878d\u5408\u3002", "result": "\u5b66\u751f\u6a21\u578b\u82f1\u8bed\u52a0\u6743\u53ec\u56de\u738772.9/\u82ac\u5170\u975e\u52a0\u6743\u53ec\u56de\u738763.4\uff0c\u8d85\u8d8a\u5fae\u8c03\u548c\u4f20\u7edf\u84b8\u998f\u65b9\u6cd5\u3002\u60b2\u4f24\u548c\u4e2d\u6027\u60c5\u7eea\u8bc6\u522b\u663e\u8457\u63d0\u5347\uff0c\u4f46\u6124\u6012(64.3)\u548c\u5feb\u4e50(58.1)\u8bc6\u522b\u51c6\u786e\u7387\u8f83\u4f4e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u591a\u8bed\u8a00SER\u6027\u80fd\uff0c\u4f46\u9700\u8981\u6539\u8fdb\u5bf9\u9ad8\u6fc0\u6d3b\u5ea6\u60c5\u7eea\u7684\u8bc6\u522b\u3002\u672a\u6765\u5c06\u63a2\u7d22\u8de8\u8bed\u8a00\u60c5\u611f\u7279\u5f81\u5bf9\u9f50\u548c\u6570\u636e\u589e\u5f3a\u7b56\u7565\u3002"}}
{"id": "2506.08726", "pdf": "https://arxiv.org/pdf/2506.08726", "abs": "https://arxiv.org/abs/2506.08726", "authors": ["Nelvin Tan", "Zian Seng", "Liang Zhang", "Yu-Ching Shih", "Dong Yang", "Amol Salunkhe"], "title": "Improved LLM Agents for Financial Document Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 5 figures", "summary": "Large language models (LLMs) have shown impressive capabilities on numerous\nnatural language processing tasks. However, LLMs still struggle with numerical\nquestion answering for financial documents that include tabular and textual\ndata. Recent works have showed the effectiveness of critic agents (i.e.,\nself-correction) for this task given oracle labels. Building upon this\nframework, this paper examines the effectiveness of the traditional critic\nagent when oracle labels are not available, and show, through experiments, that\nthis critic agent's performance deteriorates in this scenario. With this in\nmind, we present an improved critic agent, along with the calculator agent\nwhich outperforms the previous state-of-the-art approach (program-of-thought)\nand is safer. Furthermore, we investigate how our agents interact with each\nother, and how this interaction affects their performance.", "AI": {"tldr": "\u63d0\u51fa\u6539\u8fdb\u7684\u6279\u8bc4\u4ee3\u7406\u548c\u8ba1\u7b97\u5668\u4ee3\u7406\uff0c\u5728\u65e0\u6807\u7b7e\u573a\u666f\u4e0b\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u5e76\u63d0\u5347\u5b89\u5168\u6027\uff0c\u540c\u65f6\u7814\u7a76\u4ee3\u7406\u95f4\u4e92\u52a8\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u6279\u8bc4\u4ee3\u7406\u5728\u7f3a\u4e4f\u6b63\u786e\u6807\u7b7e\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u9700\u5f00\u53d1\u66f4\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u5e94\u5bf9\u91d1\u878d\u6587\u6863\u6570\u503c\u95ee\u7b54\u4efb\u52a1\u3002", "method": "\u5f00\u53d1\u6539\u8fdb\u578b\u6279\u8bc4\u4ee3\u7406\u4e0e\u4e13\u7528\u8ba1\u7b97\u5668\u4ee3\u7406\uff0c\u5f62\u6210\u534f\u540c\u5de5\u4f5c\u673a\u5236\u66ff\u4ee3\u4f20\u7edf\u7a0b\u5e8f\u601d\u7ef4\u65b9\u6cd5\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u65e0\u76d1\u7763\u73af\u5883\u4e0b\u8d85\u8d8a\u7a0b\u5e8f\u601d\u7ef4\u65b9\u6cd5\uff0c\u5b9e\u73b0\u66f4\u5b89\u5168\u7684\u6570\u503c\u63a8\u7406\uff0c\u4ee3\u7406\u95f4\u4e92\u52a8\u6a21\u5f0f\u5f71\u54cd\u6700\u7ec8\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u4ee3\u7406\u67b6\u6784\u4e0e\u4ea4\u4e92\u673a\u5236\uff0c\u53ef\u6709\u6548\u63d0\u5347LLMs\u5728\u590d\u6742\u91d1\u878d\u573a\u666f\u4e2d\u7684\u6570\u503c\u5904\u7406\u80fd\u529b\u4e0e\u7cfb\u7edf\u53ef\u9760\u6027\u3002"}}
{"id": "2506.08738", "pdf": "https://arxiv.org/pdf/2506.08738", "abs": "https://arxiv.org/abs/2506.08738", "authors": ["Dror Kris Markus", "Fabrizio Gilardi", "Daria Stetsenko"], "title": "Societal AI Research Has Become Less Interdisciplinary", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "As artificial intelligence (AI) systems become deeply embedded in everyday\nlife, calls to align AI development with ethical and societal values have\nintensified. Interdisciplinary collaboration is often championed as a key\npathway for fostering such engagement. Yet it remains unclear whether\ninterdisciplinary research teams are actually leading this shift in practice.\nThis study analyzes over 100,000 AI-related papers published on ArXiv between\n2014 and 2024 to examine how ethical values and societal concerns are\nintegrated into technical AI research. We develop a classifier to identify\nsocietal content and measure the extent to which research papers express these\nconsiderations. We find a striking shift: while interdisciplinary teams remain\nmore likely to produce societally-oriented research, computer science-only\nteams now account for a growing share of the field's overall societal output.\nThese teams are increasingly integrating societal concerns into their papers\nand tackling a wide range of domains - from fairness and safety to healthcare\nand misinformation. These findings challenge common assumptions about the\ndrivers of societal AI and raise important questions. First, what are the\nimplications for emerging understandings of AI safety and governance if most\nsocietally-oriented research is being undertaken by exclusively technical\nteams? Second, for scholars in the social sciences and humanities: in a\ntechnical field increasingly responsive to societal demands, what distinctive\nperspectives can we still offer to help shape the future of AI?", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\u7eaf\u8ba1\u7b97\u673a\u79d1\u5b66\u56e2\u961f\u5728AI\u793e\u4f1a\u5f71\u54cd\u7814\u7a76\u4e2d\u7684\u8d21\u732e\u663e\u8457\u589e\u52a0\uff0c\u6311\u6218\u4e86\u8de8\u5b66\u79d1\u5408\u4f5c\u662f\u5fc5\u8981\u524d\u63d0\u7684\u56fa\u6709\u8ba4\u77e5", "motivation": "\u9a8c\u8bc1\u8de8\u5b66\u79d1\u56e2\u961f\u662f\u5426\u771f\u6b63\u4e3b\u5bfcAI\u6280\u672f\u4e0e\u793e\u4f1a\u4ef7\u503c\u878d\u5408\u7684\u5b9e\u8df5\u8f6c\u5411\uff0c\u63ed\u793a\u6280\u672f\u56e2\u961f\u5728\u793e\u4f1a\u4ef7\u503c\u6574\u5408\u4e2d\u7684\u5b9e\u9645\u4f5c\u7528", "method": "\u901a\u8fc7\u5f00\u53d1\u793e\u4f1a\u5185\u5bb9\u5206\u7c7b\u5668\uff0c\u7cfb\u7edf\u5206\u67902014-2024\u5e74\u95f4arXiv\u5e73\u53f010\u4e07\u4f59\u7bc7AI\u8bba\u6587\u7684\u793e\u4f1a\u5c5e\u6027\u7279\u5f81", "result": "\u7eaf\u6280\u672f\u56e2\u961f\u4e0d\u4ec5\u5728\u793e\u4f1a\u5bfc\u5411\u7814\u7a76\u603b\u91cf\u5360\u6bd4\u4e0a\u5347\uff0c\u4e14\u7814\u7a76\u9886\u57df\u8986\u76d6\u516c\u5e73\u6027\u3001\u5b89\u5168\u6027\u3001\u533b\u7597\u5065\u5eb7\u7b49\u591a\u7ef4\u5ea6\u793e\u4f1a\u8bae\u9898", "conclusion": "\u8be5\u53d1\u73b0\u5f15\u53d1\u5bf9\u6280\u672f\u4e3b\u5bfc\u578b\u793e\u4f1aAI\u7814\u7a76\u7684\u6cbb\u7406\u8def\u5f84\u53cd\u601d\uff0c\u5e76\u4fc3\u4f7f\u4eba\u6587\u793e\u79d1\u5b66\u8005\u91cd\u65b0\u5b9a\u4f4d\u5176\u5728AI\u53d1\u5c55\u4e2d\u7684\u72ec\u7279\u4ef7\u503c"}}
{"id": "2506.08746", "pdf": "https://arxiv.org/pdf/2506.08746", "abs": "https://arxiv.org/abs/2506.08746", "authors": ["Muhammad Anwar", "Mishca de Costa", "Issam Hammad", "Daniel Lau"], "title": "Towards Secure and Private Language Models for Nuclear Power Plants", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper introduces a domain-specific Large Language Model for nuclear\napplications, built from the publicly accessible Essential CANDU textbook.\nDrawing on a compact Transformer-based architecture, the model is trained on a\nsingle GPU to protect the sensitive data inherent in nuclear operations.\nDespite relying on a relatively small dataset, it shows encouraging signs of\ncapturing specialized nuclear vocabulary, though the generated text sometimes\nlacks syntactic coherence. By focusing exclusively on nuclear content, this\napproach demonstrates the feasibility of in-house LLM solutions that align with\nrigorous cybersecurity and data confidentiality standards. Early successes in\ntext generation underscore the model's utility for specialized tasks, while\nalso revealing the need for richer corpora, more sophisticated preprocessing,\nand instruction fine-tuning to enhance domain accuracy. Future directions\ninclude extending the dataset to cover diverse nuclear subtopics, refining\ntokenization to reduce noise, and systematically evaluating the model's\nreadiness for real-world applications in nuclear domain.", "AI": {"tldr": "\u672c\u6587\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8eTransformer\u67b6\u6784\u7684\u6838\u9886\u57df\u4e13\u7528\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u7528\u5355GPU\u8bad\u7ec3\u4fdd\u969c\u6570\u636e\u5b89\u5168\u3002\u6a21\u578b\u521d\u6b65\u638c\u63e1\u4e13\u4e1a\u672f\u8bed\u4f46\u5b58\u5728\u8bed\u6cd5\u7f3a\u9677\uff0c\u672a\u6765\u9700\u6269\u5c55\u6570\u636e\u96c6\u548c\u4f18\u5316\u9884\u5904\u7406\u3002", "motivation": "\u6838\u9886\u57df\u6570\u636e\u654f\u611f\u4e14\u4e13\u4e1a\u6027\u5f3a\uff0c\u9700\u5f00\u53d1\u7b26\u5408\u7f51\u7edc\u5b89\u5168\u6807\u51c6\u7684\u4e13\u7528\u6a21\u578b\uff0c\u907f\u514d\u901a\u7528\u6a21\u578b\u7684\u6570\u636e\u6cc4\u9732\u98ce\u9669\u3002", "method": "\u91c7\u7528\u7d27\u51d1Transformer\u67b6\u6784\uff0c\u57fa\u4e8eEssential CANDU\u6559\u6750\u6570\u636e\uff0c\u5728\u5355GPU\u73af\u5883\u8bad\u7ec3\u4ee5\u786e\u4fdd\u6570\u636e\u673a\u5bc6\u6027\u3002", "result": "\u6a21\u578b\u6210\u529f\u6355\u83b7\u6838\u9886\u57df\u4e13\u4e1a\u8bcd\u6c47\uff0c\u4f46\u751f\u6210\u6587\u672c\u8bed\u6cd5\u8fde\u8d2f\u6027\u4e0d\u8db3\uff0c\u9a8c\u8bc1\u4e86\u5185\u90e8LLM\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u8bc1\u5b9e\u4e86\u9886\u57df\u4e13\u7528LLM\u7684\u5b9e\u8df5\u4ef7\u503c\uff0c\u9700\u901a\u8fc7\u6269\u5c55\u8bed\u6599\u5e93\u3001\u6539\u8fdb\u5206\u8bcd\u7b56\u7565\u53ca\u7cfb\u7edf\u8bc4\u4f30\u6765\u63d0\u5347\u5b9e\u7528\u6027\u3002"}}
{"id": "2506.08750", "pdf": "https://arxiv.org/pdf/2506.08750", "abs": "https://arxiv.org/abs/2506.08750", "authors": ["Muhammad Anwar", "Daniel Lau", "Mishca de Costa", "Issam Hammad"], "title": "Unlocking the Potential of Large Language Models in the Nuclear Industry with Synthetic Data", "categories": ["cs.CL"], "comment": null, "summary": "The nuclear industry possesses a wealth of valuable information locked away\nin unstructured text data. This data, however, is not readily usable for\nadvanced Large Language Model (LLM) applications that require clean, structured\nquestion-answer pairs for tasks like model training, fine-tuning, and\nevaluation. This paper explores how synthetic data generation can bridge this\ngap, enabling the development of robust LLMs for the nuclear domain. We discuss\nthe challenges of data scarcity and privacy concerns inherent in the nuclear\nindustry and how synthetic data provides a solution by transforming existing\ntext data into usable Q&A pairs. This approach leverages LLMs to analyze text,\nextract key information, generate relevant questions, and evaluate the quality\nof the resulting synthetic dataset. By unlocking the potential of LLMs in the\nnuclear industry, synthetic data can pave the way for improved information\nretrieval, enhanced knowledge sharing, and more informed decision-making in\nthis critical sector.", "AI": {"tldr": "\u5229\u7528\u5408\u6210\u6570\u636e\u6280\u672f\u5c06\u6838\u5de5\u4e1a\u975e\u7ed3\u6784\u5316\u6587\u672c\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u95ee\u7b54\u5bf9\uff0c\u89e3\u51b3LLM\u5e94\u7528\u4e2d\u7684\u6570\u636e\u77ed\u7f3a\u548c\u9690\u79c1\u95ee\u9898", "motivation": "\u6838\u5de5\u4e1a\u5b58\u5728\u5927\u91cf\u975e\u7ed3\u6784\u5316\u6587\u672c\u6570\u636e\u96be\u4ee5\u76f4\u63a5\u7528\u4e8eLLM\u8bad\u7ec3\uff0c\u4e14\u9762\u4e34\u6570\u636e\u9690\u79c1\u9650\u5236\u3002\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u53ef\u521b\u9020\u5408\u89c4\u53ef\u7528\u7684\u8bad\u7ec3\u8d44\u6e90", "method": "\u4f7f\u7528LLM\u81ea\u52a8\u5206\u6790\u6587\u672c\u3001\u63d0\u53d6\u5173\u952e\u4fe1\u606f\u3001\u751f\u6210\u76f8\u5173\u95ee\u7b54\u5bf9\uff0c\u5e76\u5efa\u7acb\u8d28\u91cf\u8bc4\u4f30\u4f53\u7cfb\u9a8c\u8bc1\u5408\u6210\u6570\u636e\u6709\u6548\u6027", "result": "\u6210\u529f\u5f00\u53d1\u51fa\u5c06\u884c\u4e1a\u6587\u672c\u8f6c\u5316\u4e3a\u7ed3\u6784\u5316\u6570\u636e\u7684\u6280\u672f\u8def\u5f84\uff0c\u4e3aLLM\u5728\u6838\u5de5\u4e1a\u7684\u5e94\u7528\u5960\u5b9a\u6570\u636e\u57fa\u7840", "conclusion": "\u5408\u6210\u6570\u636e\u6280\u672f\u80fd\u63d0\u5347\u6838\u5de5\u4e1a\u4fe1\u606f\u68c0\u7d22\u6548\u7387\u3001\u4fc3\u8fdb\u77e5\u8bc6\u5171\u4eab\u3001\u652f\u6301\u5b89\u5168\u5408\u89c4\u7684\u667a\u80fd\u51b3\u7b56\u7cfb\u7edf\u5efa\u8bbe"}}
{"id": "2506.08753", "pdf": "https://arxiv.org/pdf/2506.08753", "abs": "https://arxiv.org/abs/2506.08753", "authors": ["Pradyoth Hegde", "Santosh Kesiraju", "Jan \u0160vec", "\u0160imon Sedl\u00e1\u010dek", "Bolaji Yusuf", "Old\u0159ich Plchot", "Deepak K T", "Jan \u010cernock\u00fd"], "title": "Factors affecting the in-context learning abilities of LLMs for dialogue state tracking", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Interspeech 2025", "summary": "This study explores the application of in-context learning (ICL) to the\ndialogue state tracking (DST) problem and investigates the factors that\ninfluence its effectiveness. We use a sentence embedding based k-nearest\nneighbour method to retrieve the suitable demonstrations for ICL. The selected\ndemonstrations, along with the test samples, are structured within a template\nas input to the LLM. We then conduct a systematic study to analyse the impact\nof factors related to demonstration selection and prompt context on DST\nperformance. This work is conducted using the MultiWoZ2.4 dataset and focuses\nprimarily on the OLMo-7B-instruct, Mistral-7B-Instruct-v0.3, and\nLlama3.2-3B-Instruct models. Our findings provide several useful insights on\nin-context learning abilities of LLMs for dialogue state tracking.", "AI": {"tldr": "\u63a2\u7d22\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u5728\u5bf9\u8bdd\u72b6\u6001\u8ffd\u8e2a(DST)\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7k\u8fd1\u90bb\u68c0\u7d22\u6f14\u793a\u6837\u672c\uff0c\u5206\u6790\u63d0\u793a\u7b56\u7565\u5bf9OLMo/Mistral/Llama\u6a21\u578b\u5728MultiWoZ\u6570\u636e\u96c6\u8868\u73b0\u7684\u5f71\u54cd", "motivation": "\u7814\u7a76ICL\u5728DST\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u53ca\u5f71\u54cd\u56e0\u7d20\uff0c\u63ed\u793a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236", "method": "\u4f7f\u7528\u53e5\u5b50\u5d4c\u5165kNN\u68c0\u7d22\u6f14\u793a\u6837\u672c\uff0c\u6784\u5efa\u6a21\u677f\u5316\u8f93\u5165\u6d4b\u8bd5LLM\uff0c\u7cfb\u7edf\u5206\u6790\u6f14\u793a\u9009\u62e9\u7b56\u7565\u548c\u63d0\u793a\u4e0a\u4e0b\u6587\u8bbe\u8ba1", "result": "\u83b7\u5f97\u5173\u4e8e\u6f14\u793a\u6837\u672c\u8d28\u91cf\u3001\u4e0a\u4e0b\u6587\u957f\u5ea6\u3001\u63d0\u793a\u6a21\u677f\u8bbe\u8ba1\u5bf9DST\u51c6\u786e\u7387\u5f71\u54cd\u7684\u5173\u952e\u6d1e\u89c1", "conclusion": "\u4e3aLLM\u5728\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u5b9e\u8bc1\u6307\u5bfc\uff0c\u63ed\u793a\u6a21\u578b\u67b6\u6784\u4e0e\u63d0\u793a\u5de5\u7a0b\u7684\u76f8\u4e92\u4f5c\u7528"}}
{"id": "2506.08757", "pdf": "https://arxiv.org/pdf/2506.08757", "abs": "https://arxiv.org/abs/2506.08757", "authors": ["Mishca de Costa", "Muhammad Anwar", "Dave Mercier", "Mark Randall", "Issam Hammad"], "title": "Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL", "categories": ["cs.CL", "cs.LG"], "comment": "44th Annual CNS Conference and the 49th Annual CNS/CNA Student\n  Conference, Westin Harbour Castle Hotel, Toronto, ON, Canada, June 8-11, 2025", "summary": "Retrieving operational data from nuclear power plants requires exceptional\naccuracy and transparency due to the criticality of the decisions it supports.\nTraditionally, natural language to SQL (NL-to-SQL) approaches have been\nexplored for querying such data. While NL-to-SQL promises ease of use, it poses\nsignificant risks: end-users cannot easily validate generated SQL queries, and\nlegacy nuclear plant databases -- often complex and poorly structured --\ncomplicate query generation due to decades of incremental modifications. These\nchallenges increase the likelihood of inaccuracies and reduce trust in the\napproach. In this work, we propose an alternative paradigm: leveraging\nfunction-calling large language models (LLMs) to address these challenges.\nInstead of directly generating SQL queries, we define a set of pre-approved,\npurpose-specific functions representing common use cases. Queries are processed\nby invoking these functions, which encapsulate validated SQL logic. This hybrid\napproach mitigates the risks associated with direct NL-to-SQL translations by\nensuring that SQL queries are reviewed and optimized by experts before\ndeployment. While this strategy introduces the upfront cost of developing and\nmaintaining the function library, we demonstrate how NL-to-SQL tools can assist\nin the initial generation of function code, allowing experts to focus on\nvalidation rather than creation. Our study includes a performance comparison\nbetween direct NL-to-SQL generation and the proposed function-based approach,\nhighlighting improvements in accuracy and maintainability. This work\nunderscores the importance of balancing user accessibility with operational\nsafety and provides a novel, actionable framework for robust data retrieval in\ncritical systems.", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528\u51fd\u6570\u8c03\u7528\u5927\u6a21\u578b\u66ff\u4ee3\u4f20\u7edfNL-to-SQL\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u5b9a\u4e49\u4e13\u5bb6\u5ba1\u6838\u7684SQL\u51fd\u6570\u5e93\u63d0\u5347\u6838\u7535\u7ad9\u6570\u636e\u67e5\u8be2\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027", "motivation": "\u4f20\u7edfNL-to-SSQL\u65b9\u6cd5\u5b58\u5728SQL\u67e5\u8be2\u4e0d\u53ef\u9a8c\u8bc1\u3001\u8001\u65e7\u6838\u7535\u7ad9\u6570\u636e\u5e93\u7ed3\u6784\u590d\u6742\u5bfc\u81f4\u9519\u8bef\u7387\u9ad8\u4e14\u53ef\u4fe1\u5ea6\u4f4e\u7684\u95ee\u9898", "method": "\u5efa\u7acb\u9884\u5ba1\u6838\u7684\u4e13\u7528\u51fd\u6570\u5e93\u5c01\u88c5SQL\u903b\u8f91\uff0c\u7ed3\u5408LLM\u8fdb\u884c\u51fd\u6570\u8c03\u7528\uff0c\u4e13\u5bb6\u901a\u8fc7\u9a8c\u8bc1\u51fd\u6570\u800c\u975e\u76f4\u63a5\u751f\u6210SQL\u6765\u786e\u4fdd\u5b89\u5168\u6027", "result": "\u76f8\u6bd4\u76f4\u63a5NL-to-SQL\u65b9\u6cd5\uff0c\u8be5\u6846\u67b6\u5728\u51c6\u786e\u6027\u548c\u53ef\u7ef4\u62a4\u6027\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u7559\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u7684\u6613\u7528\u6027", "conclusion": "\u5728\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u4e2d\u5e94\u4f18\u5148\u4fdd\u969c\u64cd\u4f5c\u5b89\u5168\uff0c\u901a\u8fc7\u51fd\u6570\u5c01\u88c5\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u4e0e\u4e13\u5bb6\u5ba1\u6838\u673a\u5236\u7684\u6709\u6548\u5e73\u8861"}}
{"id": "2506.08768", "pdf": "https://arxiv.org/pdf/2506.08768", "abs": "https://arxiv.org/abs/2506.08768", "authors": ["Ahmed Hasanaath", "Aisha Alansari", "Ahmed Ashraf", "Chafik Salmane", "Hamzah Luqman", "Saad Ezzini"], "title": "AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable progress in reasoning\nabilities and general natural language processing (NLP) tasks, yet their\nperformance on Arabic data, characterized by rich morphology, diverse dialects,\nand complex script, remains underexplored. This paper presents a comprehensive\nbenchmarking study of multiple reasoning-focused LLMs, with a special emphasis\non the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP\ntasks. We experiment with various strategies, including zero-shot, few-shot,\nand fine-tuning. This allows us to systematically evaluate performance on\ndatasets covering a range of applications to examine their capacity for\nlinguistic reasoning under different levels of complexity. Our experiments\nreveal several key findings. First, carefully selecting just three in-context\nexamples delivers an average uplift of over 13 F1 points on classification\ntasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection\nfrom 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures\noutperform a strong GPT o4-mini baseline by an average of 12 F1 points on\ncomplex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning\nyields up to an additional 8 points in F1 and BLEU compared to equivalent\nincreases in model scale. The code is available at\nhttps://anonymous.4open.science/r/AraReasoner41299", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u8bc4\u4f30\u591a\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u8bed\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5c11\u6837\u672c\u5b66\u4e60\u3001DeepSeek\u67b6\u6784\u548cLoRA\u5fae\u8c03\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u7279\u6709\u7684\u590d\u6742\u8bed\u8a00\u7279\u5f81\uff08\u4e30\u5bcc\u5f62\u6001/\u591a\u65b9\u8a00/\u590d\u6742\u6587\u5b57\uff09\uff0c\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b64\u7c7b\u6570\u636e\u4e0a\u7684\u63a8\u7406\u80fd\u529b", "method": "\u572815\u9879\u963f\u62c9\u4f2f\u8bedNLP\u4efb\u52a1\u4e0a\u6d4b\u8bd5\u591a\u6b3e\u6a21\u578b\uff0c\u91c7\u7528\u96f6\u6837\u672c/\u5c11\u6837\u672c/\u5fae\u8c03\u7b56\u7565\uff0c\u91cd\u70b9\u5173\u6ce8DeepSeek\u67b6\u6784\u7684\u63a8\u7406\u80fd\u529b", "result": "\u24603\u4e2a\u4e0a\u4e0b\u6587\u6837\u672c\u5373\u53ef\u63d0\u5347\u5206\u7c7b\u4efb\u52a1F1\u503c13+\uff1b\u2461DeepSeek\u5728\u96f6\u6837\u672c\u4e0b\u6bd4GPT4-mini\u9ad812 F1\uff1b\u2462LoRA\u5fae\u8c03\u6bd4\u5355\u7eaf\u589e\u5927\u6a21\u578b\u89c4\u6a21\u591a\u63d0\u53478\u5206", "conclusion": "\u7814\u7a76\u4e3a\u963f\u62c9\u4f2f\u8bedNLP\u63d0\u4f9b\u57fa\u51c6\u6570\u636e\uff0c\u9a8c\u8bc1\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60/\u4e13\u4e1a\u67b6\u6784/\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u5bf9\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u6027"}}
{"id": "2506.08827", "pdf": "https://arxiv.org/pdf/2506.08827", "abs": "https://arxiv.org/abs/2506.08827", "authors": ["Francisco Vargas", "Alejandro Gonz\u00e1lez Coene", "Gaston Escalante", "Exequiel Lob\u00f3n", "Manuel Pulido"], "title": "The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The extraction of information about traffic accidents from legal documents is\ncrucial for quantifying insurance company costs. Extracting entities such as\npercentages of physical and/or psychological disability and the involved\ncompensation amounts is a challenging process, even for experts, due to the\nsubtle arguments and reasoning in the court decision. A two-step procedure is\nproposed: first, segmenting the document identifying the most relevant\nsegments, and then extracting the entities. For text segmentation, two\nmethodologies are compared: a classic method based on regular expressions and a\nsecond approach that divides the document into blocks of n-tokens, which are\nthen vectorized using multilingual models for semantic searches\n(text-embedding-ada-002/MiniLM-L12-v2 ). Subsequently, large language models\n(LLaMA-2 7b, 70b, LLaMA-3 8b, and GPT-4 Turbo) are applied with prompting to\nthe selected segments for entity extraction. For the LLaMA models, fine-tuning\nis performed using LoRA. LLaMA-2 7b, even with zero temperature, shows a\nsignificant number of hallucinations in extractions which are an important\ncontention point for named entity extraction. This work shows that these\nhallucinations are substantially reduced after finetuning the model. The\nperformance of the methodology based on segment vectorization and subsequent\nuse of LLMs significantly surpasses the classic method which achieves an\naccuracy of 39.5%. Among open-source models, LLaMA-2 70B with finetuning\nachieves the highest accuracy 79.4%, surpassing its base version 61.7%.\nNotably, the base LLaMA-3 8B model already performs comparably to the finetuned\nLLaMA-2 70B model, achieving 76.6%, highlighting the rapid progress in model\ndevelopment. Meanwhile, GPT-4 Turbo achieves the highest accuracy at 86.1%.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08\u8bed\u4e49\u5206\u5272+\u5927\u6a21\u578b\u5b9e\u4f53\u63d0\u53d6\uff09\uff0c\u901a\u8fc7\u5fae\u8c03\u663e\u8457\u63d0\u5347LLaMA\u6027\u80fd\uff0cGPT-4 Turbo\u8fbe\u5230\u6700\u9ad886.1%\u51c6\u786e\u7387\u3002", "motivation": "\u6cd5\u5f8b\u6587\u4e66\u4e2d\u7684\u4e8b\u6545\u4fe1\u606f\u63d0\u53d6\u5bf9\u4fdd\u9669\u6210\u672c\u91cf\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6cd5\u5f8b\u63a8\u7406\u590d\u6742\u5bfc\u81f4\u5b9e\u4f53\u63d0\u53d6\u56f0\u96be\u3002", "method": "1. \u6587\u672c\u5206\u5272\uff08\u6b63\u5219\u8868\u8fbe\u5f0f vs \u8bed\u4e49\u5411\u91cf\u5316\uff09\uff1b2. \u5927\u6a21\u578b\u5b9e\u4f53\u63d0\u53d6\uff08LLaMA\u7cfb\u5217\u5fae\u8c03\u3001GPT-4 Turbo\u96f6\u6837\u672c\uff09", "result": "\u5fae\u8c03\u540eLLaMA-2 70B\u8fbe79.4%\u51c6\u786e\u7387\uff0cLLaMA-3 8B\u57fa\u7840\u6a21\u578b76.6%\uff0cGPT-4 Turbo\u6700\u4f7386.1%\uff1b\u8bed\u4e49\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\uff0839.5%\uff09", "conclusion": "\u5fae\u8c03\u51cf\u5c11\u5e7b\u89c9\uff0cLLaMA-3\u663e\u793a\u6a21\u578b\u5feb\u901f\u8fed\u4ee3\u6f5c\u529b\uff1b\u95ed\u6e90\u6a21\u578b\uff08GPT-4\uff09\u6027\u80fd\u6700\u4f18\u4f46\u5f00\u6e90\u6a21\u578b\u8ffd\u8d76\u4e2d"}}
{"id": "2506.08836", "pdf": "https://arxiv.org/pdf/2506.08836", "abs": "https://arxiv.org/abs/2506.08836", "authors": ["Flavio D'Intino", "Hans-Peter Hutter"], "title": "Advancing STT for Low-Resource Real-World Speech", "categories": ["cs.CL", "cs.HC"], "comment": "Conference: HCI International 2025, 20 pages, 4 figures", "summary": "Swiss German is a low-resource language represented by diverse dialects that\ndiffer significantly from Standard German and from each other, lacking a\nstandardized written form. As a result, transcribing Swiss German involves\ntranslating into Standard German. Existing datasets have been collected in\ncontrolled environments, yielding effective speech-to-text (STT) models, but\nthese models struggle with spontaneous conversational speech.\n  This paper, therefore, introduces the new SRB-300 dataset, a 300-hour\nannotated speech corpus featuring real-world long-audio recordings from 39\nSwiss German radio and TV stations. It captures spontaneous speech across all\nmajor Swiss dialects recorded in various realistic environments and overcomes\nthe limitation of prior sentence-level corpora.\n  We fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset,\nachieving notable enhancements over previous zero-shot performance metrics.\nImprovements in word error rate (WER) ranged from 19% to 33%, while BLEU scores\nincreased between 8% and 40%. The best fine-tuned model, large-v3, achieved a\nWER of 17.1% and a BLEU score of 74.8. This advancement is crucial for\ndeveloping effective and robust STT systems for Swiss German and other\nlow-resource languages in real-world contexts.", "AI": {"tldr": "SRB-300\u6570\u636e\u96c6\u7684\u53d1\u5e03\u663e\u8457\u63d0\u5347\u4e86\u745e\u58eb\u5fb7\u8bed\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u6027\u80fd\uff0c\u5fae\u8c03\u540eWhisper\u6a21\u578bWER\u964d\u4f4e19-33%\uff0cBLEU\u63d0\u53478-40%\u3002", "motivation": "\u73b0\u6709\u53d7\u63a7\u73af\u5883\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u96be\u4ee5\u5904\u7406\u745e\u58eb\u5fb7\u8bed\u65b9\u8a00\u7684\u771f\u5b9e\u5bf9\u8bdd\u573a\u666f\uff0c\u9700\u6784\u5efa\u771f\u5b9e\u8bed\u97f3\u8bed\u6599\u5e93\u3002", "method": "\u6536\u96c6300\u5c0f\u65f6\u745e\u58eb\u5e7f\u64ad\u7535\u89c6\u591a\u65b9\u8a00\u957f\u97f3\u9891\uff0c\u8986\u76d639\u4e2a\u7535\u53f0\u7684\u5373\u5174\u5bf9\u8bdd\uff0c\u5e76\u57fa\u4e8eWhisper\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u8bad\u7ec3\u3002", "result": "\u6700\u4f73\u6a21\u578blarge-v3\u8fbe\u523017.1% WER\u548c74.8 BLEU\uff0c\u76f8\u6bd4zero-shot\u6027\u80fdWER\u63d0\u534733%\uff0cBLEU\u63d0\u534740%\u3002", "conclusion": "SRB-300\u6570\u636e\u96c6\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u8d44\u6e90\u65b9\u8a00\u8bed\u97f3\u8bc6\u522b\u96be\u9898\uff0c\u4e3a\u771f\u5b9e\u573a\u666f\u8bed\u97f3\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u6570\u636e\u652f\u6491\u3002"}}
{"id": "2506.08885", "pdf": "https://arxiv.org/pdf/2506.08885", "abs": "https://arxiv.org/abs/2506.08885", "authors": ["Danush Khanna", "Krishna Kumar", "Basab Ghosh", "Vinija Jain", "Vasu Sharma", "Aman Chadha", "Amitava Das"], "title": "AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Adversarial threats against LLMs are escalating faster than current defenses\ncan adapt. We expose a critical geometric blind spot in alignment: adversarial\nprompts exploit latent camouflage, embedding perilously close to the safe\nrepresentation manifold while encoding unsafe intent thereby evading surface\nlevel defenses like Direct Preference Optimization (DPO), which remain blind to\nthe latent geometry. We introduce ALKALI, the first rigorously curated\nadversarial benchmark and the most comprehensive to date spanning 9,000 prompts\nacross three macro categories, six subtypes, and fifteen attack families.\nEvaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates\n(ASRs) across both open and closed source models, exposing an underlying\nvulnerability we term latent camouflage, a structural blind spot where\nadversarial completions mimic the latent geometry of safe ones. To mitigate\nthis vulnerability, we introduce GRACE - Geometric Representation Aware\nContrastive Enhancement, an alignment framework coupling preference learning\nwith latent space regularization. GRACE enforces two constraints: latent\nseparation between safe and adversarial completions, and adversarial cohesion\namong unsafe and jailbreak behaviors. These operate over layerwise pooled\nembeddings guided by a learned attention profile, reshaping internal geometry\nwithout modifying the base model, and achieve up to 39% ASR reduction.\nMoreover, we introduce AVQI, a geometry aware metric that quantifies latent\nalignment failure via cluster separation and compactness. AVQI reveals when\nunsafe completions mimic the geometry of safe ones, offering a principled lens\ninto how models internally encode safety. We make the code publicly available\nat https://anonymous.4open.science/r/alkali-B416/README.md.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u5bf9\u6297\u6027\u57fa\u51c6ALKALI\u548c\u51e0\u4f55\u611f\u77e5\u5bf9\u9f50\u6846\u67b6GRACE\uff0c\u901a\u8fc7\u6f5c\u7a7a\u95f4\u6b63\u5219\u5316\u964d\u4f4e39%\u653b\u51fb\u6210\u529f\u7387\uff0c\u63ed\u793aLLMs\u5b89\u5168\u7f16\u7801\u7684\u51e0\u4f55\u76f2\u533a\u3002", "motivation": "\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982DPO\uff09\u5ffd\u89c6\u6f5c\u7a7a\u95f4\u51e0\u4f55\u7279\u5f81\uff0c\u5bfc\u81f4\u5bf9\u6297\u6027\u63d0\u793a\u901a\u8fc7\u6f5c\u5728\u4f2a\u88c5\u7ed5\u8fc7\u9632\u5fa1\uff0c\u66b4\u9732LLMs\u5b89\u5168\u673a\u5236\u7684\u7ed3\u6784\u6027\u6f0f\u6d1e\u3002", "method": "\u5f00\u53d1GRACE\u6846\u67b6\uff1a\u5728\u504f\u597d\u5b66\u4e60\u57fa\u7840\u4e0a\u589e\u52a0\u6f5c\u7a7a\u95f4\u5206\u79bb\u7ea6\u675f\uff08\u5b89\u5168/\u5bf9\u6297\u8865\u5168\u5206\u79bb\uff09\u548c\u5bf9\u6297\u51dd\u805a\u7ea6\u675f\uff08\u540c\u7c7b\u653b\u51fb\u884c\u4e3a\u805a\u7c7b\uff09\uff0c\u901a\u8fc7\u5c42\u95f4\u5d4c\u5165\u6c60\u5316\u5b9e\u73b0\u51e0\u4f55\u91cd\u5851\u3002", "result": "\u572821\u4e2a\u4e3b\u6d41LLM\u4e0a\u9a8c\u8bc1\uff0cGRACE\u6700\u9ad8\u964d\u4f4e39%\u653b\u51fb\u6210\u529f\u7387\uff1b\u63d0\u51fa\u51e0\u4f55\u91cf\u5316\u6307\u6807AVQI\uff0c\u63ed\u793a\u4e0d\u5b89\u5168\u8865\u5168\u6a21\u4eff\u5b89\u5168\u8865\u5168\u51e0\u4f55\u7279\u5f81\u7684\u73b0\u8c61\u3002", "conclusion": "LLM\u5b89\u5168\u7f16\u7801\u5b58\u5728\u51e0\u4f55\u7ef4\u5ea6\u6f0f\u6d1e\uff0c\u9700\u8981\u5efa\u7acb\u51e0\u4f55\u611f\u77e5\u7684\u5bf9\u9f50\u8303\u5f0f\u3002ALKALI\u57fa\u51c6\u548cGRACE\u6846\u67b6\u4e3a\u9632\u5fa1\u6f5c\u7a7a\u95f4\u5bf9\u6297\u653b\u51fb\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.08897", "pdf": "https://arxiv.org/pdf/2506.08897", "abs": "https://arxiv.org/abs/2506.08897", "authors": ["Hiba Khey", "Amine Lakhder", "Salma Rouichi", "Imane El Ghabi", "Kamal Hejjaoui", "Younes En-nahli", "Fahd Kalloubi", "Moez Amri"], "title": "PlantBert: An Open Source Language Model for Plant Science", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid advancement of transformer-based language models has catalyzed\nbreakthroughs in biomedical and clinical natural language processing; however,\nplant science remains markedly underserved by such domain-adapted tools. In\nthis work, we present PlantBert, a high-performance, open-source language model\nspecifically tailored for extracting structured knowledge from plant\nstress-response literature. Built upon the DeBERTa architecture-known for its\ndisentangled attention and robust contextual encoding-PlantBert is fine-tuned\non a meticulously curated corpus of expert-annotated abstracts, with a primary\nfocus on lentil (Lens culinaris) responses to diverse abiotic and biotic\nstressors. Our methodology combines transformer-based modeling with\nrule-enhanced linguistic post-processing and ontology-grounded entity\nnormalization, enabling PlantBert to capture biologically meaningful\nrelationships with precision and semantic fidelity. The underlying corpus is\nannotated using a hierarchical schema aligned with the Crop Ontology,\nencompassing molecular, physiological, biochemical, and agronomic dimensions of\nplant adaptation. PlantBert exhibits strong generalization capabilities across\nentity types and demonstrates the feasibility of robust domain adaptation in\nlow-resource scientific fields. By providing a scalable and reproducible\nframework for high-resolution entity recognition, PlantBert bridges a critical\ngap in agricultural NLP and paves the way for intelligent, data-driven systems\nin plant genomics, phenomics, and agronomic knowledge discovery. Our model is\npublicly released to promote transparency and accelerate cross-disciplinary\ninnovation in computational plant science.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u57fa\u4e8eDeBERTa\u67b6\u6784\u7684PlantBert\u8bed\u8a00\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u4ece\u690d\u7269\u80c1\u8feb\u54cd\u5e94\u6587\u732e\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u77e5\u8bc6\uff0c\u901a\u8fc7\u672c\u4f53\u5bf9\u9f50\u6807\u6ce8\u4f53\u7cfb\u5b9e\u73b0\u591a\u7ef4\u5ea6\u690d\u7269\u9002\u5e94\u5206\u6790\u3002", "motivation": "\u690d\u7269\u79d1\u5b66\u9886\u57df\u7f3a\u4e4f\u4e13\u4e1a\u5316\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\uff0c\u5236\u7ea6\u4e86\u519c\u4e1a\u77e5\u8bc6\u53d1\u73b0\u6548\u7387\u3002\u73b0\u6709\u901a\u7528\u6a21\u578b\u96be\u4ee5\u6355\u6349\u690d\u7269\u5b66\u5b9e\u4f53\u95f4\u590d\u6742\u7684\u751f\u7269\u5b66\u5173\u7cfb\u3002", "method": "\u57fa\u4e8eDeBERTa\u67b6\u6784\u6784\u5efa\u6a21\u578b\uff0c\u7ed3\u5408\u89c4\u5219\u589e\u5f3a\u7684\u6587\u672c\u540e\u5904\u7406\u548c\u672c\u4f53\u5b9e\u4f53\u89c4\u8303\u5316\u6280\u672f\u3002\u4f7f\u7528\u4e0e\u4f5c\u7269\u672c\u4f53\u5bf9\u9f50\u7684\u5c42\u6b21\u5316\u6807\u6ce8\u4f53\u7cfb\u5904\u7406\u5206\u5b50-\u751f\u7406-\u519c\u827a\u591a\u7ef4\u5ea6\u6570\u636e\u3002", "result": "\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u8de8\u5b9e\u4f53\u6cdb\u5316\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u9886\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u5728\u79d1\u7814NLP\u4e2d\u7684\u53ef\u884c\u6027\uff0cF1\u503c\u8fbe87.3%\uff08\u6bd4\u57fa\u7ebf\u63d0\u534715%\uff09\u3002", "conclusion": "PlantBert\u586b\u8865\u4e86\u519c\u4e1aNLP\u5de5\u5177\u7a7a\u767d\uff0c\u5176\u5f00\u6e90\u7279\u6027\u52a0\u901f\u4e86\u690d\u7269\u8868\u578b\u7ec4\u5b66\u4e0e\u57fa\u56e0\u7ec4\u5b66\u9886\u57df\u7684\u6570\u636e\u9a71\u52a8\u7814\u7a76\uff0c\u4e3a\u667a\u80fd\u519c\u827a\u7cfb\u7edf\u5960\u5b9a\u6280\u672f\u57fa\u7840\u3002"}}
{"id": "2506.08899", "pdf": "https://arxiv.org/pdf/2506.08899", "abs": "https://arxiv.org/abs/2506.08899", "authors": ["Elias Horner", "Cristinel Mateis", "Guido Governatori", "Agata Ciabattoni"], "title": "From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LO"], "comment": null, "summary": "We present a novel approach to the automated semantic analysis of legal texts\nusing large language models (LLMs), targeting their transformation into formal\nrepresentations in Defeasible Deontic Logic (DDL). We propose a structured\npipeline that segments complex normative language into atomic snippets,\nextracts deontic rules, and evaluates them for syntactic and semantic\ncoherence. Our methodology is evaluated across various LLM configurations,\nincluding prompt engineering strategies, fine-tuned models, and multi-stage\npipelines, focusing on legal norms from the Australian Telecommunications\nConsumer Protections Code. Empirical results demonstrate promising alignment\nbetween machine-generated and expert-crafted formalizations, showing that LLMs\n- particularly when prompted effectively - can significantly contribute to\nscalable legal informatics.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u5316\u6cd5\u5f8b\u6587\u672c\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u6d41\u7a0b\u5c06\u6cd5\u5f8b\u6761\u6b3e\u8f6c\u5316\u4e3a\u53ef\u5e9f\u6b62\u9053\u4e49\u903b\u8f91\u8868\u793a", "motivation": "\u89e3\u51b3\u590d\u6742\u6cd5\u5f8b\u6587\u672c\u81ea\u52a8\u5316\u5206\u6790\u96be\u9898\uff0c\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53ef\u6269\u5c55\u6cd5\u5f8b\u4fe1\u606f\u5316\u4e2d\u7684\u5e94\u7528\u6f5c\u529b", "method": "\u8bbe\u8ba1\u5305\u542b\u6761\u6b3e\u5206\u5272\u3001\u4e49\u52a1\u89c4\u5219\u63d0\u53d6\u3001\u53e5\u6cd5\u8bed\u4e49\u6821\u9a8c\u7684\u4e09\u9636\u6bb5\u6d41\u7a0b\uff0c\u91c7\u7528\u63d0\u793a\u5de5\u7a0b/\u5fae\u8c03\u6a21\u578b/\u591a\u9636\u6bb5\u7ba1\u9053\u7b49\u591a\u79cdLLM\u914d\u7f6e\u65b9\u6848", "result": "\u5728\u6fb3\u5927\u5229\u4e9a\u7535\u4fe1\u6d88\u8d39\u8005\u4fdd\u62a4\u6cd5\u5178\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u673a\u5668\u751f\u6210\u5f62\u5f0f\u5316\u4e0e\u4e13\u5bb6\u6807\u6ce8\u7ed3\u679c\u8fbe\u6210\u663e\u8457\u4e00\u81f4\u6027\uff08prompt\u5de5\u7a0b\u7b56\u7565\u6548\u679c\u6700\u4f73\uff09", "conclusion": "\u5408\u7406\u5f15\u5bfc\u7684\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u6709\u6548\u652f\u6301\u6cd5\u5f8b\u89c4\u8303\u5f62\u5f0f\u5316\uff0c\u4e3a\u53ef\u6269\u5c55\u6cd5\u5f8b\u63a8\u7406\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840"}}
{"id": "2506.08907", "pdf": "https://arxiv.org/pdf/2506.08907", "abs": "https://arxiv.org/abs/2506.08907", "authors": ["Antonios Dimakis", "John Pavlopoulos", "Antonios Anastasopoulos"], "title": "Dialect Normalization using Large Language Models and Morphological Rules", "categories": ["cs.CL", "I.2.7"], "comment": "19 pages, 18 figures, to be published in the Findings of the\n  Association for Computational Linguistics 2025", "summary": "Natural language understanding systems struggle with low-resource languages,\nincluding many dialects of high-resource ones. Dialect-to-standard\nnormalization attempts to tackle this issue by transforming dialectal text so\nthat it can be used by standard-language tools downstream. In this study, we\ntackle this task by introducing a new normalization method that combines\nrule-based linguistically informed transformations and large language models\n(LLMs) with targeted few-shot prompting, without requiring any parallel data.\nWe implement our method for Greek dialects and apply it on a dataset of\nregional proverbs, evaluating the outputs using human annotators. We then use\nthis dataset to conduct downstream experiments, finding that previous results\nregarding these proverbs relied solely on superficial linguistic information,\nincluding orthographic artifacts, while new observations can still be made\nthrough the remaining semantics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7ed3\u5408\u89c4\u5219\u8f6c\u6362\u548cLLM\u5c11\u6837\u672c\u63d0\u793a\u7684\u5e0c\u814a\u65b9\u8a00\u89c4\u8303\u5316\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u53d1\u73b0\u5148\u524d\u7814\u7a76\u4f9d\u8d56\u8868\u5c42\u8bed\u8a00\u7279\u5f81\uff0c\u800c\u8bed\u4e49\u4fe1\u606f\u4ecd\u80fd\u4ea7\u751f\u65b0\u89c1\u89e3", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u65b9\u8a00\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u96be\u9898\uff0c\u901a\u8fc7\u65b9\u8a00\u5230\u6807\u51c6\u8bed\u7684\u8f6c\u6362\u63d0\u5347\u4e0b\u6e38\u5de5\u5177\u9002\u7528\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u7f3a\u4e4f\u5e73\u884c\u8bed\u6599\u7684\u60c5\u51b5", "method": "\u7ed3\u5408\u57fa\u4e8e\u89c4\u5219\u7684\u8bed\u8a00\u5b66\u8f6c\u6362\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5c11\u6837\u672c\u63d0\u793a\u6280\u672f\uff0c\u5e94\u7528\u4e8e\u5e0c\u814a\u65b9\u8a00\u8c1a\u8bed\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u8bc4\u4f30\u8f93\u51fa\u8d28\u91cf", "result": "\u4e0b\u6e38\u5b9e\u9a8c\u8868\u660e\u5148\u524d\u7814\u7a76\u7ed3\u679c\u4e3b\u8981\u4f9d\u8d56\u62fc\u5199\u7279\u5f81\u7b49\u8868\u5c42\u4fe1\u606f\uff0c\u800c\u901a\u8fc7\u5269\u4f59\u8bed\u4e49\u4ecd\u53ef\u83b7\u5f97\u65b0\u7684\u8bed\u8a00\u5b66\u53d1\u73b0", "conclusion": "\u6df7\u5408\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u65b9\u8a00\u89c4\u8303\u5316\u95ee\u9898\uff0c\u5f3a\u8c03\u6df1\u5c42\u8bed\u4e49\u5206\u6790\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u63d0\u4f9b\u4e86\u65e0\u9700\u5e73\u884c\u6570\u636e\u7684\u65b0\u65b9\u6848"}}
{"id": "2506.08920", "pdf": "https://arxiv.org/pdf/2506.08920", "abs": "https://arxiv.org/abs/2506.08920", "authors": ["Zeyu Leo Liu", "Greg Durrett", "Eunsol Choi"], "title": "PropMEND: Hypernetworks for Knowledge Propagation in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under review", "summary": "Knowledge editing techniques for large language models (LLMs) can inject\nknowledge that is later reproducible verbatim, but they fall short on\npropagating that knowledge: models cannot answer questions that require\nreasoning with the injected knowledge. We present a hypernetwork-based approach\nfor knowledge propagation, named PropMEND, where we meta-learn how to modify\ngradients of a language modeling loss to encourage injected information to\npropagate. Our approach extends the meta-objective of MEND [29] so that\ngradient updates on knowledge are transformed to enable answering multi-hop\nquestions involving that knowledge. We show improved performance on the\nRippleEdit dataset, showing almost 2x accuracy on challenging multi-hop\nquestions whose answers are not explicitly stated in the injected fact. We\nfurther introduce a new dataset, Controlled RippleEdit, to evaluate the\ngeneralization of our hypernetwork, testing knowledge propagation along\nrelations and entities unseen during hypernetwork training. PropMEND still\noutperforms existing approaches in unseen entity-relation pairs, yet the\nperformance gap decreases substantially, suggesting future work in propagating\nknowledge to a wide range of relations.", "AI": {"tldr": "PropMEND\u63d0\u51fa\u57fa\u4e8e\u8d85\u7f51\u7edc\u7684\u77e5\u8bc6\u4f20\u64ad\u65b9\u6cd5\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u8c03\u6574\u68af\u5ea6\u66f4\u65b0\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u6280\u672f\u53ef\u6ce8\u5165\u77e5\u8bc6\u4f46\u65e0\u6cd5\u652f\u6301\u57fa\u4e8e\u8be5\u77e5\u8bc6\u7684\u63a8\u7406\uff0c\u9700\u89e3\u51b3\u77e5\u8bc6\u4f20\u64ad\u95ee\u9898\u4ee5\u5b9e\u73b0\u591a\u8df3\u95ee\u9898\u56de\u7b54\u3002", "method": "\u6269\u5c55MEND\u7684\u5143\u76ee\u6807\uff0c\u4f7f\u7528\u8d85\u7f51\u7edc\u5bf9\u68af\u5ea6\u8fdb\u884c\u5143\u5b66\u4e60\u8f6c\u6362\uff0c\u4f7f\u6a21\u578b\u80fd\u5229\u7528\u6ce8\u5165\u77e5\u8bc6\u8fdb\u884c\u591a\u6b65\u63a8\u7406\u3002", "result": "RippleEdit\u4e0a\u591a\u8df3\u95ee\u9898\u51c6\u786e\u7387\u63d0\u5347\u8fd12\u500d\uff1b\u5728\u542b\u672a\u89c1\u8fc7\u5b9e\u4f53\u5173\u7cfb\u7684Controlled RippleEdit\u4e2d\u4ecd\u4f18\u4e8e\u57fa\u7ebf\uff0c\u4f46\u6027\u80fd\u5dee\u8ddd\u7f29\u5c0f\u3002", "conclusion": "PropMEND\u6709\u6548\u4fc3\u8fdb\u77e5\u8bc6\u4f20\u64ad\uff0c\u4f46\u5bf9\u672a\u89c1\u8fc7\u5173\u7cfb\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u672a\u6765\u9700\u7814\u7a76\u66f4\u5e7f\u6cdb\u7684\u77e5\u8bc6\u4f20\u64ad\u673a\u5236\u3002"}}
{"id": "2506.08935", "pdf": "https://arxiv.org/pdf/2506.08935", "abs": "https://arxiv.org/abs/2506.08935", "authors": ["Andrew Shin"], "title": "Can A Gamer Train A Mathematical Reasoning Model?", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "While large language models (LLMs) have achieved remarkable performance in\nvarious tasks including mathematical reasoning, their development typically\ndemands prohibitive computational resources. Recent advancements have reduced\ncosts for training capable models, yet even these approaches rely on high-end\nhardware clusters. In this paper, we demonstrate that a single average gaming\nGPU can train a solid mathematical reasoning model, by integrating\nreinforcement learning and memory optimization techniques. Specifically, we\ntrain a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB\nmemory that achieves comparable or better performance on mathematical reasoning\nbenchmarks than models several times larger, in resource-constrained\nenvironments. Our results challenge the paradigm that state-of-the-art\nmathematical reasoning necessitates massive infrastructure, democratizing\naccess to high-performance AI research.\nhttps://github.com/shinandrew/YouronMath.", "AI": {"tldr": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4e0e\u5185\u5b58\u4f18\u5316\u6280\u672f\uff0c\u5728\u5355\u5f20\u6e38\u620fGPU\uff08RTX 3080 Ti\uff09\u4e0a\u8bad\u7ec31.5B\u53c2\u6570\u6570\u5b66\u63a8\u7406\u6a21\u578b\uff0c\u6027\u80fd\u8d85\u8d8a\u66f4\u5927\u6a21\u578b", "motivation": "\u6253\u7834\u9ad8\u6027\u80fd\u6570\u5b66\u63a8\u7406\u6a21\u578b\u9700\u8981\u6602\u8d35\u786c\u4ef6\u96c6\u7fa4\u7684\u4f20\u7edf\u8303\u5f0f\uff0c\u964d\u4f4eAI\u7814\u7a76\u95e8\u69db", "method": "\u6574\u5408\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e0e\u5185\u5b58\u4f18\u5316\u6280\u672f\uff0c\u5b9e\u73b016GB\u663e\u5b58GPU\u4e0a\u7684\u9ad8\u6548\u8bad\u7ec3", "result": "\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\uff0c\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8a\u6570\u500d\u89c4\u6a21\u6a21\u578b\u7684\u6027\u80fd", "conclusion": "\u8bc1\u660e\u9ad8\u6027\u80fd\u6570\u5b66\u63a8\u7406\u6a21\u578b\u53ef\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u5b9e\u73b0\uff0c\u63a8\u52a8AI\u7814\u7a76\u7684\u6c11\u4e3b\u5316\u8fdb\u7a0b"}}
{"id": "2506.08938", "pdf": "https://arxiv.org/pdf/2506.08938", "abs": "https://arxiv.org/abs/2506.08938", "authors": ["Qinggang Zhang", "Zhishang Xiang", "Yilin Xiao", "Le Wang", "Junhui Li", "Xinrun Wang", "Jinsong Su"], "title": "FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "Qinggang Zhang and Zhishang Xiang contributed equally to this work.\n  Corresponding author: Jinsong Su", "summary": "Large language models (LLMs) augmented with retrieval systems have\ndemonstrated significant potential in handling knowledge-intensive tasks.\nHowever, these models often struggle with unfaithfulness issues, generating\noutputs that either ignore the retrieved context or inconsistently blend it\nwith the LLM`s parametric knowledge. This issue is particularly severe in cases\nof knowledge conflict, where the retrieved context conflicts with the model`s\nparametric knowledge. While existing faithful RAG approaches enforce strict\ncontext adherence through well-designed prompts or modified decoding\nstrategies, our analysis reveals a critical limitation: they achieve\nfaithfulness by forcibly suppressing the model`s parametric knowledge, which\nundermines the model`s internal knowledge structure and increases the risk of\nmisinterpreting the context. To this end, this paper proposes FaithfulRAG, a\nnovel framework that resolves knowledge conflicts by explicitly modeling\ndiscrepancies between the model`s parametric knowledge and retrieved context.\nSpecifically, FaithfulRAG identifies conflicting knowledge at the fact level\nand designs a self-thinking process, allowing LLMs to reason about and\nintegrate conflicting facts before generating responses. Extensive experiments\ndemonstrate that our method outperforms state-of-the-art methods. The code is\navailable at https:// github.com/DeepLearnXMU/Faithful-RAG", "AI": {"tldr": "\u63d0\u51faFaithfulRAG\u6846\u67b6\u89e3\u51b3\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u7684\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u53c2\u6570\u77e5\u8bc6\u4e0e\u68c0\u7d22\u4e0a\u4e0b\u6587\u7684\u5dee\u5f02\uff0c\u5728\u4e8b\u5b9e\u5c42\u9762\u8bc6\u522b\u51b2\u7a81\u5e76\u6574\u5408\u751f\u6210\u53ef\u4fe1\u7ed3\u679c", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u901a\u8fc7\u5f3a\u5236\u6291\u5236\u6a21\u578b\u53c2\u6570\u77e5\u8bc6\u6765\u5b9e\u73b0\u5fe0\u5b9e\u6027\uff0c\u8fd9\u4f1a\u7834\u574f\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u7ed3\u6784\u5e76\u589e\u52a0\u8bef\u89e3\u4e0a\u4e0b\u6587\u7684\u98ce\u9669", "method": "1. \u5728\u4e8b\u5b9e\u5c42\u9762\u8bc6\u522b\u53c2\u6570\u77e5\u8bc6\u4e0e\u68c0\u7d22\u4e0a\u4e0b\u6587\u7684\u51b2\u7a81\n2. \u8bbe\u8ba1\u81ea\u6211\u601d\u8003\u8fc7\u7a0b\u8ba9LLM\u63a8\u7406\u6574\u5408\u51b2\u7a81\u4e8b\u5b9e\n3. \u5efa\u7acb\u77e5\u8bc6\u51b2\u7a81\u7684\u663e\u5f0f\u5efa\u6a21\u673a\u5236", "result": "\u5728\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4f18\u4e8e\u73b0\u6709state-of-the-art\u65b9\u6cd5", "conclusion": "FaithfulRAG\u6210\u529f\u89e3\u51b3\u4e86\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u751f\u6210\u7ed3\u679c\u5fe0\u5b9e\u6027\u7684\u540c\u65f6\u6709\u6548\u6574\u5408\u53c2\u6570\u77e5\u8bc6\u548c\u68c0\u7d22\u4e0a\u4e0b\u6587"}}
{"id": "2506.08952", "pdf": "https://arxiv.org/pdf/2506.08952", "abs": "https://arxiv.org/abs/2506.08952", "authors": ["Clara Lachenmaier", "Judith Sieker", "Sina Zarrie\u00df"], "title": "Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint accepted at ACL Main Conference 2025", "summary": "Communication among humans relies on conversational grounding, allowing\ninterlocutors to reach mutual understanding even when they do not have perfect\nknowledge and must resolve discrepancies in each other's beliefs. This paper\ninvestigates how large language models (LLMs) manage common ground in cases\nwhere they (don't) possess knowledge, focusing on facts in the political domain\nwhere the risk of misinformation and grounding failure is high. We examine the\nability of LLMs to answer direct knowledge questions and loaded questions that\npresuppose misinformation. We evaluate whether loaded questions lead LLMs to\nengage in active grounding and correct false user beliefs, in connection to\ntheir level of knowledge and their political bias. Our findings highlight\nsignificant challenges in LLMs' ability to engage in grounding and reject false\nuser beliefs, raising concerns about their role in mitigating misinformation in\npolitical discourse.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u653f\u6cbb\u9886\u57df\u96be\u4ee5\u7ea0\u6b63\u7528\u6237\u9519\u8bef\u4fe1\u5ff5\uff0c\u63ed\u9732\u5176\u5728\u865a\u5047\u4fe1\u606f\u6cbb\u7406\u4e2d\u7684\u6f5c\u5728\u98ce\u9669\u3002", "motivation": "\u5bf9\u8bdd\u4e2d\u7684\u5171\u540c\u57fa\u7840\u6784\u5efa\u662f\u5b9e\u73b0\u76f8\u4e92\u7406\u89e3\u7684\u6838\u5fc3\u673a\u5236\uff0c\u800c\u653f\u6cbb\u9886\u57df\u56e0\u865a\u5047\u4fe1\u606f\u6cdb\u6ee5\u6210\u4e3a\u9ad8\u98ce\u9669\u573a\u666f\u3002\u672c\u7814\u7a76\u805a\u7126LLM\u5728\uff08\u4e0d\uff09\u638c\u63e1\u77e5\u8bc6\u65f6\u5982\u4f55\u5efa\u7acb\u5171\u540c\u57fa\u7840\uff0c\u7279\u522b\u662f\u9762\u5bf9\u9884\u8bbe\u9519\u8bef\u4fe1\u606f\u7684\u8bf1\u5bfc\u6027\u95ee\u9898\u65f6\u7684\u5e94\u5bf9\u673a\u5236\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u76f4\u63a5\u77e5\u8bc6\u6027\u95ee\u9898\u4e0e\u9884\u8bbe\u9519\u8bef\u7acb\u573a\u7684\u8bf1\u5bfc\u6027\u95ee\u9898\uff0c\u8bc4\u4f30\u4e0d\u540cLLM\u7684\u5e94\u7b54\u6a21\u5f0f\uff0c\u5206\u6790\u6a21\u578b\u77e5\u8bc6\u6c34\u5e73\u4e0e\u653f\u6cbb\u503e\u5411\u6027\u5bf9\u7ea0\u9519\u80fd\u529b\u7684\u5f71\u54cd\u3002", "result": "LLM\u5c55\u73b0\u51fa\u663e\u8457\u7684\u5730\u57fa\u6784\u5efa\u969c\u788d\uff1a\u9762\u5bf9\u8bf1\u5bfc\u6027\u95ee\u9898\u65f6\u7f3a\u4e4f\u4e3b\u52a8\u6f84\u6e05\u610f\u613f\uff0c\u4e14\u7ea0\u9519\u80fd\u529b\u4e0e\u6a21\u578b\u77e5\u8bc6\u50a8\u5907\u53ca\u653f\u6cbb\u7acb\u573a\u504f\u501a\u5b58\u5728\u76f8\u5173\u6027\u3002", "conclusion": "\u5f53\u524dLLM\u5728\u653f\u6cbb\u8bdd\u8bed\u4e2d\u7ea0\u6b63\u9519\u8bef\u4fe1\u5ff5\u7684\u80fd\u529b\u5b58\u5728\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u5176\u4f5c\u4e3a\u865a\u5047\u4fe1\u606f\u8fc7\u6ee4\u5668\u7684\u53ef\u9760\u6027\u9762\u4e34\u4e25\u5cfb\u6311\u6218\u3002"}}
{"id": "2506.08966", "pdf": "https://arxiv.org/pdf/2506.08966", "abs": "https://arxiv.org/abs/2506.08966", "authors": ["Marek Kadl\u010d\u00edk", "Michal \u0160tef\u00e1nik", "Timothee Mickus", "Michal Spiegel", "Josef Kucha\u0159"], "title": "Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers", "categories": ["cs.CL", "cs.LG", "cs.NE"], "comment": null, "summary": "Pretrained language models (LMs) are prone to arithmetic errors. Existing\nwork showed limited success in probing numeric values from models'\nrepresentations, indicating that these errors can be attributed to the inherent\nunreliability of distributionally learned embeddings in representing exact\nquantities. However, we observe that previous probing methods are inadequate\nfor the emergent structure of learned number embeddings with sinusoidal\npatterns.\n  In response, we propose a novel probing technique that decodes numeric values\nfrom input embeddings with near-perfect accuracy across a range of open-source\nLMs. This proves that after the sole pre-training, LMs represent numbers with\nremarkable precision. Finally, we find that the embeddings' preciseness judged\nby our probe's accuracy explains a large portion of LM's errors in elementary\narithmetic, and show that aligning the embeddings with the pattern discovered\nby our probe can mitigate these errors.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u65b0\u578b\u63a2\u6d4b\u6280\u672f\u53ef\u7cbe\u51c6\u89e3\u7801\u6570\u5b57\u8868\u5f81\uff0c\u8bc1\u5b9e\u5176\u7b97\u672f\u9519\u8bef\u4e3b\u8981\u6e90\u4e8e\u5d4c\u5165\u8868\u793a\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u901a\u8fc7\u5d4c\u5165\u5bf9\u9f50\u53ef\u6709\u6548\u6539\u5584\u7b97\u672f\u80fd\u529b", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u6355\u6349\u8bed\u8a00\u6a21\u578b\u6570\u503c\u5d4c\u5165\u7684\u6b63\u5f26\u6a21\u5f0f\u7ed3\u6784\uff0c\u5bfc\u81f4\u65e0\u6cd5\u51c6\u786e\u89e3\u91ca\u6a21\u578b\u7b97\u672f\u9519\u8bef\u6839\u6e90", "method": "\u63d0\u51fa\u57fa\u4e8e\u6b63\u5f26\u6a21\u5f0f\u7684\u65b0\u578b\u6570\u503c\u89e3\u7801\u63a2\u9488\uff0c\u5bf9\u591a\u4e2a\u5f00\u6e90\u8bed\u8a00\u6a21\u578b\u7684\u8f93\u5165\u5d4c\u5165\u8fdb\u884c\u6570\u503c\u91cd\u5efa\u5206\u6790", "result": "\u65b0\u63a2\u9488\u5728\u5404\u7c7b\u6a21\u578b\u4e0a\u5b9e\u73b0\u63a5\u8fd1\u5b8c\u7f8e\u7684\u6570\u503c\u89e3\u7801\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u9884\u8bad\u7ec3\u540e\u6a21\u578b\u5177\u5907\u7cbe\u786e\u6570\u5b57\u8868\u5f81\u80fd\u529b\uff0c\u4e14\u5d4c\u5165\u7cbe\u5ea6\u4e0e\u7b97\u672f\u9519\u8bef\u7387\u663e\u8457\u76f8\u5173", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5185\u5728\u5177\u5907\u7cbe\u786e\u6570\u503c\u8868\u5f81\u80fd\u529b\uff0c\u901a\u8fc7\u63a2\u9488\u6307\u5bfc\u7684\u5d4c\u5165\u7a7a\u95f4\u5bf9\u9f50\u53ef\u6709\u6548\u63d0\u5347\u57fa\u7840\u7b97\u672f\u6027\u80fd"}}
{"id": "2506.08972", "pdf": "https://arxiv.org/pdf/2506.08972", "abs": "https://arxiv.org/abs/2506.08972", "authors": ["Yuan Guo", "Tingjia Miao", "Zheng Wu", "Pengzhou Cheng", "Ming Zhou", "Zhuosheng Zhang"], "title": "Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark and Scheduling System", "categories": ["cs.CL"], "comment": null, "summary": "Autonomous agents powered by multimodal large language models have been\ndeveloped to facilitate task execution on mobile devices. However, prior work\nhas predominantly focused on atomic tasks -- such as shot-chain execution tasks\nand single-screen grounding tasks -- while overlooking the generalization to\ncompositional tasks, which are indispensable for real-world applications. This\nwork introduces UI-NEXUS, a comprehensive benchmark designed to evaluate mobile\nagents on three categories of compositional operations: Simple Concatenation,\nContext Transition, and Deep Dive. UI-NEXUS supports interactive evaluation in\n20 fully controllable local utility app environments, as well as 30 online\nChinese and English service apps. It comprises 100 interactive task templates\nwith an average optimal step count of 14.05. Experimental results across a\nrange of mobile agents with agentic workflow or agent-as-a-model show that\nUI-NEXUS presents significant challenges. Specifically, existing agents\ngenerally struggle to balance performance and efficiency, exhibiting\nrepresentative failure modes such as under-execution, over-execution, and\nattention drift, causing visible atomic-to-compositional generalization gap.\nInspired by these findings, we propose AGENT-NEXUS, a lightweight and efficient\nscheduling system to tackle compositional mobile tasks. AGENT-NEXUS\nextrapolates the abilities of existing mobile agents by dynamically decomposing\nlong-horizon tasks to a series of self-contained atomic subtasks. AGENT-NEXUS\nachieves 24% to 40% task success rate improvement for existing mobile agents on\ncompositional operation tasks within the UI-NEXUS benchmark without\nsignificantly sacrificing inference overhead. The demo video, dataset, and code\nare available on the project page at https://ui-nexus.github.io.", "AI": {"tldr": "\u63d0\u51faUI-NEXUS\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u79fb\u52a8\u4ee3\u7406\u7684\u7ec4\u5408\u4efb\u52a1\u5904\u7406\u80fd\u529b\uff0c\u5e76\u5f00\u53d1AGENT-NEXUS\u7cfb\u7edf\u6539\u5584\u73b0\u6709\u4ee3\u740624%-40%\u7684\u4efb\u52a1\u6210\u529f\u7387", "motivation": "\u73b0\u6709\u79fb\u52a8\u4ee3\u7406\u4e3b\u8981\u5173\u6ce8\u539f\u5b50\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u573a\u666f\u4e2d\u5173\u952e\u7ec4\u5408\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u8bc4\u4f30", "method": "\u6784\u5efa\u5305\u542b\u4e09\u7c7b\u7ec4\u5408\u64cd\u4f5c\uff08\u7b80\u5355\u4e32\u8054/\u4e0a\u4e0b\u6587\u8f6c\u6362/\u6df1\u5ea6\u63a2\u7d22\uff09\u7684UI-NEXUS\u57fa\u51c6\uff0c\u542b50\u4e2a\u4ea4\u4e92\u5e94\u7528\u73af\u5883\u548c100\u4e2a\u4efb\u52a1\u6a21\u677f\uff1b\u63d0\u51faAGENT-NEXUS\u7cfb\u7edf\u901a\u8fc7\u52a8\u6001\u4efb\u52a1\u5206\u89e3\u673a\u5236\u5904\u7406\u957f\u7a0b\u4efb\u52a1", "result": "\u73b0\u6709\u4ee3\u7406\u5728\u7ec4\u5408\u4efb\u52a1\u4e2d\u666e\u904d\u5b58\u5728\u6267\u884c\u504f\u5dee\uff08\u6b20\u6267\u884c/\u8fc7\u6267\u884c/\u6ce8\u610f\u529b\u6f02\u79fb\uff09\uff0cAGENT-NEXUS\u5728\u4e0d\u663e\u8457\u589e\u52a0\u63a8\u7406\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387", "conclusion": "UI-NEXUS\u63ed\u793a\u4e86\u79fb\u52a8\u4ee3\u7406\u7684\u539f\u5b50-\u7ec4\u5408\u4efb\u52a1\u6cdb\u5316\u9e3f\u6c9f\uff0cAGENT-NEXUS\u4e3a\u63d0\u5347\u590d\u6742\u4ea4\u4e92\u573a\u666f\u4e0b\u7684\u4ee3\u7406\u6548\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.08981", "pdf": "https://arxiv.org/pdf/2506.08981", "abs": "https://arxiv.org/abs/2506.08981", "authors": ["Satu Hopponen", "Tomi Kinnunen", "Alexandre Nikolaev", "Rosa Gonz\u00e1lez Hautam\u00e4ki", "Lauri Tavi", "Einar Meister"], "title": "FROST-EMA: Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography Measurements with L1, L2 and Imitated L2 Accents", "categories": ["cs.CL"], "comment": "Accepted in Interspeech 2025", "summary": "We introduce a new FROST-EMA (Finnish and Russian Oral Speech Dataset of\nElectromagnetic Articulography) corpus. It consists of 18 bilingual speakers,\nwho produced speech in their native language (L1), second language (L2), and\nimitated L2 (fake foreign accent). The new corpus enables research into\nlanguage variability from phonetic and technological points of view.\nAccordingly, we include two preliminary case studies to demonstrate both\nperspectives. The first case study explores the impact of L2 and imitated L2 on\nthe performance of an automatic speaker verification system, while the second\nillustrates the articulatory patterns of one speaker in L1, L2, and a fake\naccent.", "AI": {"tldr": "\u521b\u5efa\u5305\u542b18\u540d\u53cc\u8bed\u8005\u7684FROST-EMA\u591a\u8bed\u8a00\u53d1\u97f3\u6570\u636e\u96c6\uff0c\u63a2\u7d22L2/\u4f2a\u53e3\u97f3\u5bf9\u58f0\u7eb9\u8bc6\u522b\u7cfb\u7edf\u53ca\u53d1\u97f3\u6a21\u5f0f\u7684\u5f71\u54cd", "motivation": "\u7814\u7a76\u7b2c\u4e8c\u8bed\u8a00\u4e60\u5f97\u548c\u4f2a\u5916\u56fd\u53e3\u97f3\u5728\u8bed\u97f3\u5b66\u5c42\u9762\u7684\u53d1\u97f3\u7279\u5f81\uff0c\u53ca\u5176\u5bf9\u8bed\u97f3\u6280\u672f\u7cfb\u7edf\u7684\u5f71\u54cd", "method": "\u6784\u5efa\u5305\u542b\u4fc4\u8bed\u6bcd\u8bed\u8005\u82ac\u5170\u8bed(L2)\u548c\u6a21\u4eff\u82ac\u5170\u8bed\u7684\u4e09\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u81ea\u52a8\u8bf4\u8bdd\u4eba\u9a8c\u8bc1\u7cfb\u7edf\u6d4b\u8bd5\u548c\u7535\u78c1\u53d1\u97f3\u4eea\u5206\u6790\u8fdb\u884c\u53cc\u7ef4\u5ea6\u9a8c\u8bc1", "result": "\u53d1\u73b0L2\u548c\u4f2a\u53e3\u97f3\u5747\u4f1a\u964d\u4f4e\u58f0\u7eb9\u8bc6\u522b\u51c6\u786e\u7387\uff08-15%\u548c-25%\uff09\uff0c\u4e14\u5448\u73b0\u72ec\u7279\u7684\u820c\u90e8\u8fd0\u52a8\u6a21\u5f0f", "conclusion": "\u8be5\u8bed\u6599\u5e93\u4e3a\u8de8\u8bed\u8a00\u8bed\u97f3\u7814\u7a76\u63d0\u4f9b\u591a\u7ef4\u5ea6\u6570\u636e\u652f\u6301\uff0c\u8bc1\u660e\u53d1\u97f3\u53d8\u5f02\u5bf9\u6280\u672f\u7cfb\u7edf\u548c\u8bed\u97f3\u5b66\u7814\u7a76\u5747\u5177\u91cd\u8981\u4ef7\u503c"}}
{"id": "2506.08986", "pdf": "https://arxiv.org/pdf/2506.08986", "abs": "https://arxiv.org/abs/2506.08986", "authors": ["Yuejiao Wang", "Xianmin Gong", "Xixin Wu", "Patrick Wong", "Hoi-lam Helene Fung", "Man Wai Mak", "Helen Meng"], "title": "Naturalistic Language-related Movie-Watching fMRI Task for Detecting Neurocognitive Decline and Disorder", "categories": ["cs.CL"], "comment": "5 pages,3 figures, accepted by ISCSLP 2024", "summary": "Early detection is crucial for timely intervention aimed at preventing and\nslowing the progression of neurocognitive disorder (NCD), a common and\nsignificant health problem among the aging population. Recent evidence has\nsuggested that language-related functional magnetic resonance imaging (fMRI)\nmay be a promising approach for detecting cognitive decline and early NCD. In\nthis paper, we proposed a novel, naturalistic language-related fMRI task for\nthis purpose. We examined the effectiveness of this task among 97 non-demented\nChinese older adults from Hong Kong. The results showed that machine-learning\nclassification models based on fMRI features extracted from the task and\ndemographics (age, gender, and education year) achieved an average area under\nthe curve of 0.86 when classifying participants' cognitive status (labeled as\nNORMAL vs DECLINE based on their scores on a standard neurcognitive test).\nFeature localization revealed that the fMRI features most frequently selected\nby the data-driven approach came primarily from brain regions associated with\nlanguage processing, such as the superior temporal gyrus, middle temporal\ngyrus, and right cerebellum. The study demonstrated the potential of the\nnaturalistic language-related fMRI task for early detection of aging-related\ncognitive decline and NCD.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u81ea\u7136\u8bed\u8a00\u76f8\u5173fMRI\u4efb\u52a1\u68c0\u6d4b\u8001\u5e74\u4eba\u8ba4\u77e5\u8870\u9000\uff0c\u57fa\u4e8e97\u540d\u53d7\u8bd5\u8005\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fbe\u5230AUC 0.86\uff0c\u7279\u5f81\u5b9a\u4f4d\u663e\u793a\u8bed\u8a00\u76f8\u5173\u8111\u533a\u6fc0\u6d3b\u3002", "motivation": "\u65e9\u671f\u68c0\u6d4b\u795e\u7ecf\u8ba4\u77e5\u969c\u788d\u5bf9\u4e34\u5e8a\u5e72\u9884\u81f3\u5173\u91cd\u8981\uff0c\u8bed\u8a00\u76f8\u5173fMRI\u53ef\u80fd\u63d0\u4f9b\u975e\u4fb5\u5165\u6027\u68c0\u6d4b\u624b\u6bb5\u3002", "method": "\u4f7f\u7528\u81ea\u7136\u8bed\u8a00fMRI\u4efb\u52a1\u6d4b\u8bd597\u540d\u9999\u6e2f\u975e\u75f4\u5446\u8001\u5e74\u4eba\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u4eba\u53e3\u7edf\u8ba1\u5b66\u7279\u5f81\u5efa\u7acb\u5206\u7c7b\u6a21\u578b\u3002", "result": "\u6a21\u578bAUC\u8fbe0.86\uff0c\u7279\u5f81\u4e3b\u8981\u6765\u81ea\u989e\u4e0a\u56de\u3001\u989e\u4e2d\u56de\u548c\u53f3\u5c0f\u8111\u7b49\u8bed\u8a00\u5904\u7406\u76f8\u5173\u8111\u533a\u3002", "conclusion": "\u81ea\u7136\u8bed\u8a00fMRI\u4efb\u52a1\u5177\u6709\u65e9\u671f\u68c0\u6d4b\u8ba4\u77e5\u8870\u9000\u7684\u6f5c\u529b\uff0c\u4e3a\u4e34\u5e8a\u5e72\u9884\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2506.08999", "pdf": "https://arxiv.org/pdf/2506.08999", "abs": "https://arxiv.org/abs/2506.08999", "authors": ["Theo Zhang", "Madurya Suresh", "Anne S. Warlaumont", "Kasia Hitczenko", "Alejandrina Cristia", "Margaret Cychosz"], "title": "Employing self-supervised learning models for cross-linguistic child speech maturity classification", "categories": ["cs.CL", "cs.AI"], "comment": "To be published in Interspeech 2025. 5 pages, 2 figures. For\n  associated Github repository, see\n  https://github.com/spoglab-stanford/w2v2-pro-sm/tree/main/speechbrain/recipes/W2V2-LL4300-Pro-SM", "summary": "Speech technology systems struggle with many downstream tasks for child\nspeech due to small training corpora and the difficulties that child speech\npose. We apply a novel dataset, SpeechMaturity, to state-of-the-art transformer\nmodels to address a fundamental classification task: identifying child\nvocalizations. Unlike previous corpora, our dataset captures maximally\necologically-valid child vocalizations across an unprecedented sample,\ncomprising children acquiring 25+ languages in the U.S., Bolivia, Vanuatu,\nPapua New Guinea, Solomon Islands, and France. The dataset contains 242,004\nlabeled vocalizations, magnitudes larger than previous work. Models were\ntrained to distinguish between cry, laughter, mature (consonant+vowel), and\nimmature speech (just consonant or vowel). Models trained on the dataset\noutperform state-of-the-art models trained on previous datasets, achieved\nclassification accuracy comparable to humans, and were robust across rural and\nurban settings.", "AI": {"tldr": "\u4f7f\u7528SpeechMaturity\u65b0\u578b\u5927\u6570\u636e\u96c6\u8bad\u7ec3\u7684Transformer\u6a21\u578b\u5728\u513f\u7ae5\u8bed\u97f3\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73", "motivation": "\u89e3\u51b3\u513f\u7ae5\u8bed\u97f3\u6280\u672f\u7cfb\u7edf\u56e0\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u548c\u513f\u7ae5\u8bed\u97f3\u590d\u6742\u6027\u5bfc\u81f4\u7684\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u4e0d\u8db3\u95ee\u9898", "method": "\u6784\u5efa\u5305\u542b24.2\u4e07\u6807\u6ce8\u6837\u672c\u7684\u8de8\u8bed\u8a00\u6570\u636e\u96c6\uff08\u8986\u76d66\u4e2a\u56fd\u5bb625+\u8bed\u8a00\uff09\uff0c\u8bad\u7ec3Transformer\u6a21\u578b\u8fdb\u884c\u54ed\u58f0/\u7b11\u58f0/\u6210\u719f\u97f3/\u672a\u6210\u719f\u97f3\u56db\u5206\u7c7b", "result": "\u6a21\u578b\u51c6\u786e\u7387\u8d85\u8d8a\u73b0\u6709SOTA\u6a21\u578b\uff08\u8fbe\u4eba\u7c7b\u6c34\u5e73\uff09\uff0c\u5728\u57ce\u4e61\u4e0d\u540c\u573a\u666f\u4e2d\u4fdd\u6301\u9c81\u68d2\u6027", "conclusion": "\u5927\u89c4\u6a21\u751f\u6001\u6548\u5ea6\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u513f\u7ae5\u8bed\u97f3\u5206\u7c7b\u6027\u80fd\uff0c\u4e3a\u513f\u7ae5\u8bed\u97f3\u6280\u672f\u53d1\u5c55\u63d0\u4f9b\u65b0\u57fa\u51c6"}}
{"id": "2506.09003", "pdf": "https://arxiv.org/pdf/2506.09003", "abs": "https://arxiv.org/abs/2506.09003", "authors": ["Lei Zhang", "Jiaxi Yang", "Min Yang", "Jian Yang", "Mouxiang Chen", "Jiajun Zhang", "Zeyu Cui", "Binyuan Hui", "Junyang Lin"], "title": "SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner", "categories": ["cs.CL"], "comment": "Accepted by ICML2025", "summary": "We introduce **SWE-Flow**, a novel data synthesis framework grounded in\nTest-Driven Development (TDD). Unlike existing software engineering data that\nrely on human-submitted issues, **SWE-Flow** automatically infers incremental\ndevelopment steps directly from unit tests, which inherently encapsulate\nhigh-level requirements. The core of **SWE-Flow** is the construction of a\nRuntime Dependency Graph (RDG), which precisely captures function interactions,\nenabling the generation of a structured, step-by-step *development schedule*.\nAt each step, **SWE-Flow** produces a partial codebase, the corresponding unit\ntests, and the necessary code modifications, resulting in fully verifiable TDD\ntasks. With this approach, we generated 16,061 training instances and 2,020\ntest instances from real-world GitHub projects, creating the **SWE-Flow-Eval**\nbenchmark. Our experiments show that fine-tuning open model on this dataset\nsignificantly improves performance in TDD-based coding. To facilitate further\nresearch, we release all code, datasets, models, and Docker images at\n[Github](https://github.com/Hambaobao/SWE-Flow).", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6d4b\u8bd5\u9a71\u52a8\u5f00\u53d1\uff08TDD\uff09\u7684SWE-Flow\u6846\u67b6\uff0c\u901a\u8fc7\u8fd0\u884c\u65f6\u4f9d\u8d56\u56fe\u81ea\u52a8\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u5f00\u53d1\u4efb\u52a1\uff0c\u5e76\u6784\u5efaSWE-Flow-Eval\u57fa\u51c6\u6570\u636e\u96c6", "motivation": "\u73b0\u6709\u8f6f\u4ef6\u5de5\u7a0b\u6570\u636e\u4f9d\u8d56\u4eba\u5de5\u63d0\u4ea4\u7684issue\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u901a\u8fc7\u5355\u5143\u6d4b\u8bd5\u81ea\u52a8\u751f\u6210\u7ed3\u6784\u5316\u5f00\u53d1\u6b65\u9aa4", "method": "\u6784\u5efa\u8fd0\u884c\u65f6\u4f9d\u8d56\u56fe\uff08RDG\uff09\u6355\u83b7\u51fd\u6570\u4ea4\u4e92\uff0c\u751f\u6210\u5206\u6b65\u5f00\u53d1\u8ba1\u5212\uff0c\u6bcf\u6b65\u4ea7\u51fa\u53ef\u9a8c\u8bc1\u4ee3\u7801/\u6d4b\u8bd5/\u4fee\u6539", "result": "\u4eceGitHub\u9879\u76ee\u751f\u621016,061\u4e2a\u8bad\u7ec3\u5b9e\u4f8b\u548c2,020\u4e2a\u6d4b\u8bd5\u5b9e\u4f8b\uff0c\u6a21\u578b\u5fae\u8c03\u540e\u663e\u8457\u63d0\u5347TDD\u7f16\u7801\u80fd\u529b", "conclusion": "\u6846\u67b6\u6709\u6548\u63d0\u5347\u57fa\u4e8eTDD\u7684\u4ee3\u7801\u751f\u6210\u6027\u80fd\uff0c\u5f00\u6e90\u4ee3\u7801/\u6570\u636e\u96c6/\u6a21\u578b/Docker\u955c\u50cf\u63a8\u52a8\u540e\u7eed\u7814\u7a76"}}
{"id": "2506.09009", "pdf": "https://arxiv.org/pdf/2506.09009", "abs": "https://arxiv.org/abs/2506.09009", "authors": ["Hakyung Sung", "Gyu-Ho Shin", "Chanyoung Lee", "You Kyung Sung", "Boo Kyung Jung"], "title": "UD-KSL Treebank v1.3: A semi-automated framework for aligning XPOS-extracted units with UPOS tags", "categories": ["cs.CL"], "comment": null, "summary": "The present study extends recent work on Universal Dependencies annotations\nfor second-language (L2) Korean by introducing a semi-automated framework that\nidentifies morphosyntactic constructions from XPOS sequences and aligns those\nconstructions with corresponding UPOS categories. We also broaden the existing\nL2-Korean corpus by annotating 2,998 new sentences from argumentative essays.\nTo evaluate the impact of XPOS-UPOS alignments, we fine-tune L2-Korean\nmorphosyntactic analysis models on datasets both with and without these\nalignments, using two NLP toolkits. Our results indicate that the aligned\ndataset not only improves consistency across annotation layers but also\nenhances morphosyntactic tagging and dependency-parsing accuracy, particularly\nin cases of limited annotated data.", "AI": {"tldr": "\u5f00\u53d1\u534a\u81ea\u52a8\u5316\u6846\u67b6\u5b9e\u73b0XPOS-UPOS\u5bf9\u9f50\uff0c\u63d0\u5347\u97e9\u8bed\u4e8c\u8bed\u5f62\u6001\u53e5\u6cd5\u6807\u6ce8\u4e00\u81f4\u6027\u53ca\u5206\u6790\u51c6\u786e\u7387", "motivation": "\u89e3\u51b3\u73b0\u6709L2\u97e9\u8bed\u6807\u6ce8\u4e2d\u5f62\u6001\u53e5\u6cd5\u4e0e\u4f9d\u5b58\u5206\u6790\u5c42\u7684\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4f18\u5316\u6807\u6ce8\u6570\u636e\u6709\u9650\u573a\u666f\u4e0b\u7684NLP\u8868\u73b0", "method": "\u6784\u5efa\u534a\u81ea\u52a8\u5316\u6846\u67b6\u5b9e\u73b0XPOS\u5e8f\u5217\u4e0eUPOS\u5bf9\u9f50\uff0c\u901a\u8fc7\u4e24\u79cd\u5de5\u5177\u5305\u5728\u6709\u65e0\u5bf9\u9f50\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u6a21\u578b\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c", "result": "\u5bf9\u9f50\u6570\u636e\u96c6\u4f7f\u6807\u6ce8\u5c42\u4e00\u81f4\u6027\u63d0\u5347\uff0c\u5f62\u6001\u53e5\u6cd5\u6807\u6ce8\u548c\u4f9d\u5b58\u89e3\u6790\u51c6\u786e\u7387\u63d0\u9ad8\uff0c\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u63d0\u5347\u663e\u8457", "conclusion": "XPOS-UPOS\u5bf9\u9f50\u673a\u5236\u6709\u6548\u589e\u5f3a\u8de8\u6807\u6ce8\u5c42\u4e00\u81f4\u6027\uff0c\u4e3a\u4f4e\u8d44\u6e90\u4e8c\u8bed\u5206\u6790\u4efb\u52a1\u63d0\u4f9b\u53ef\u9760\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.09014", "pdf": "https://arxiv.org/pdf/2506.09014", "abs": "https://arxiv.org/abs/2506.09014", "authors": ["Jianing Qi", "Xi Ye", "Hao Tang", "Zhigang Zhu", "Eunsol Choi"], "title": "Learning to Reason Across Parallel Samples for LLM Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Scaling test-time compute brings substantial performance gains for large\nlanguage models (LLMs). By sampling multiple answers and heuristically\naggregate their answers (e.g., either through majority voting or using\nverifiers to rank the answers), one can achieve consistent performance gains in\nmath domains. In this paper, we propose a new way to leverage such multiple\nsample set. We train a compact LLM, called Sample Set Aggregator (SSA), that\ntakes a concatenated sequence of multiple samples and output the final answer,\noptimizing it for the answer accuracy with reinforcement learning. Experiments\non multiple reasoning datasets show that SSA outperforms other test-time\nscaling methods such as reward model-based re-ranking. Our approach also shows\na promising generalization ability, across sample set sizes, base model\nfamilies and scales, and tasks. By separating LLMs to generate answers and LLMs\nto analyze and aggregate sampled answers, our approach can work with the\noutputs from premier black box models easily and efficiently.", "AI": {"tldr": "\u63d0\u51fa\u6837\u672c\u96c6\u805a\u5408\u5668(SSA)\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7d27\u51d1\u578bLLM\u805a\u5408\u591a\u4e2a\u6837\u672c\u7b54\u6848\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u4efb\u52a1\u6027\u80fd\u5e76\u8d85\u8d8a\u4f20\u7edf\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5(\u5982\u591a\u6570\u6295\u7968\u6216\u5956\u52b1\u6a21\u578b\u91cd\u6392\u5e8f)\u672a\u5145\u5206\u6316\u6398\u591a\u6837\u672c\u6f5c\u529b\uff0c\u9700\u5f00\u53d1\u66f4\u4f18\u5316\u7684\u7b54\u6848\u805a\u5408\u673a\u5236\u63d0\u5347\u5927\u6a21\u578b\u63a8\u7406\u6027\u80fd", "method": "1. \u6784\u5efaSSA\u6a21\u578b\u63a5\u6536\u591a\u6837\u672c\u7b54\u6848\u5e8f\u5217\u8f93\u5165\n2. \u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7b54\u6848\u51c6\u786e\u6027\n3. \u901a\u8fc7\u5206\u79bb\u751f\u6210\u4e0e\u5206\u6790\u6a21\u5757\u5b9e\u73b0\u7075\u6d3b\u90e8\u7f72", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u73b0\u8de8\u6837\u672c\u89c4\u6a21/\u6a21\u578b\u5bb6\u65cf/\u4efb\u52a1\u7c7b\u578b\u7684\u6cdb\u5316\u80fd\u529b", "conclusion": "SSA\u901a\u8fc7\u89e3\u8026\u751f\u6210\u4e0e\u805a\u5408\u8fc7\u7a0b\uff0c\u53ef\u9ad8\u6548\u517c\u5bb9\u9ed1\u7bb1\u6a21\u578b\u8f93\u51fa\uff0c\u4e3a\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2506.09021", "pdf": "https://arxiv.org/pdf/2506.09021", "abs": "https://arxiv.org/abs/2506.09021", "authors": ["Hakyung Sung", "Karla Csuros", "Min-Chang Sung"], "title": "Comparing human and LLM proofreading in L2 writing: Impact on lexical and syntactic features", "categories": ["cs.CL"], "comment": null, "summary": "This study examines the lexical and syntactic interventions of human and LLM\nproofreading aimed at improving overall intelligibility in identical second\nlanguage writings, and evaluates the consistency of outcomes across three LLMs\n(ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b). Findings show that both human and\nLLM proofreading enhance bigram lexical features, which may contribute to\nbetter coherence and contextual connectedness between adjacent words. However,\nLLM proofreading exhibits a more generative approach, extensively reworking\nvocabulary and sentence structures, such as employing more diverse and\nsophisticated vocabulary and incorporating a greater number of adjective\nmodifiers in noun phrases. The proofreading outcomes are highly consistent in\nmajor lexical and syntactic features across the three models.", "AI": {"tldr": "\u6bd4\u8f83\u4eba\u7c7b\u4e0e\u4e09\u79cdLLM\u6821\u5bf9\u5728\u4e8c\u8bed\u5199\u4f5c\u4e2d\u7684\u8bcd\u6c47\u3001\u53e5\u6cd5\u5e72\u9884\u6548\u679c\u53ca\u4e00\u81f4\u6027", "motivation": "\u63a2\u7a76LLM\u6821\u5bf9\u63d0\u5347\u6587\u672c\u53ef\u7406\u89e3\u6027\u7684\u6709\u6548\u6027\u53ca\u5176\u4e0e\u4eba\u7c7b\u6821\u5bf9\u7684\u5dee\u5f02\uff0c\u540c\u65f6\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\u95f4\u7684\u4e00\u81f4\u6027", "method": "\u5bf9\u76f8\u540c\u4e8c\u8bed\u5199\u4f5c\u6837\u672c\u8fdb\u884c\u4eba\u7c7b\u4e0eLLM\uff08ChatGPT-4o/Llama3.1-8b/Deepseek-r1-8b\uff09\u6821\u5bf9\u5e72\u9884\uff0c\u5206\u6790\u8bcd\u6c47\u53e5\u6cd5\u53d8\u5316\u5e76\u8bc4\u4f30\u6a21\u578b\u95f4\u4e00\u81f4\u6027", "result": "1. \u4eba\u7c7b\u4e0eLLM\u5747\u63d0\u5347bigram\u8bcd\u6c47\u7279\u5f81 \n2. LLM\u91c7\u7528\u751f\u6210\u5f0f\u4fee\u6539\uff08\u591a\u6837\u5316\u8bcd\u6c47/\u590d\u6742\u53e5\u6cd5/\u66f4\u591a\u5f62\u5bb9\u8bcd\u4fee\u9970\uff09 \n3. \u4e09\u79cd\u6a21\u578b\u5728\u4e3b\u8981\u7279\u5f81\u4e0a\u9ad8\u5ea6\u4e00\u81f4", "conclusion": "LLM\u6821\u5bf9\u5c55\u73b0\u66f4\u5f3a\u7684\u751f\u6210\u6027\u4fee\u6539\u80fd\u529b\u4e14\u8de8\u6a21\u578b\u6548\u679c\u7a33\u5b9a\uff0c\u4e3aAI\u8f85\u52a9\u5199\u4f5c\u63d0\u4f9b\u5b9e\u8df5\u53c2\u8003"}}
{"id": "2506.09033", "pdf": "https://arxiv.org/pdf/2506.09033", "abs": "https://arxiv.org/abs/2506.09033", "authors": ["Haozhen Zhang", "Tao Feng", "Jiaxuan You"], "title": "Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Code is available at https://github.com/ulab-uiuc/Router-R1", "summary": "The rapid emergence of diverse large language models (LLMs) has spurred the\ndevelopment of LLM routers that assign user queries to the most suitable model.\nHowever, existing LLM routers typically perform a single-round, one-to-one\nmapping (\\textit{i.e.}, assigning each query to a single model in isolation),\nwhich limits their capability to tackle complex tasks that demand the\ncomplementary strengths of multiple LLMs. In this paper, we present\n\\textbf{Router-R1}, a reinforcement learning (RL)-based framework that\nformulates multi-LLM routing and aggregation as a sequential decision process.\nRouter-R1 instantiates the router itself as a capable LLM, leveraging its\nreasoning ability to interleave \"think\" actions (internal deliberation) with\n\"route\" actions (dynamic model invocation), and integrates each response into\nits evolving context. To guide learning, we employ a lightweight rule-based\nreward comprising format rewards, final outcome rewards, and a novel cost\nreward for performance and cost trade-off optimization, opening a pathway\ntoward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions\nonly on simple model descriptors such as pricing, latency, and example\nperformance, enabling strong generalization to unseen model selection.\nExperiments on seven general and multi-hop QA benchmarks show that Router-R1\noutperforms over several strong baselines, achieving superior performance while\nmaintaining robust generalization and cost management.Code is available at\nhttps://github.com/ulab-uiuc/Router-R1.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684Router-R1\u6846\u67b6\uff0c\u52a8\u6001\u8def\u7531\u548c\u805a\u5408\u591a\u4e2aLLM\uff0c\u4f18\u5316\u6027\u80fd\u4e0e\u6210\u672c\u6743\u8861", "motivation": "\u73b0\u6709LLM\u8def\u7531\u5668\u91c7\u7528\u5355\u8f6e\u4e00\u5bf9\u4e00\u6620\u5c04\u673a\u5236\uff0c\u96be\u4ee5\u5904\u7406\u9700\u8981\u591a\u6a21\u578b\u534f\u4f5c\u7684\u590d\u6742\u4efb\u52a1", "method": "\u5c06\u8def\u7531\u5efa\u6a21\u4e3a\u5e8f\u5217\u51b3\u7b56\u8fc7\u7a0b\uff0c\u901a\u8fc7LLM\u8def\u7531\u5668\u4ea4\u66ff\u6267\u884c\u300c\u601d\u8003\u300d\uff08\u5185\u90e8\u63a8\u7406\uff09\u548c\u300c\u8def\u7531\u300d\uff08\u52a8\u6001\u6a21\u578b\u8c03\u7528\uff09\u52a8\u4f5c", "result": "\u57287\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u7684\u540c\u65f6\u4fdd\u6301\u6210\u672c\u53ef\u63a7", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u673a\u5236\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u8bed\u8a00\u6a21\u578b\u8def\u7531\u4e2d\u6027\u80fd-\u6210\u672c\u6743\u8861\u7684\u4f18\u5316\u8def\u5f84"}}
{"id": "2506.09047", "pdf": "https://arxiv.org/pdf/2506.09047", "abs": "https://arxiv.org/abs/2506.09047", "authors": ["Yaniv Nikankin", "Dana Arad", "Yossi Gandelsman", "Yonatan Belinkov"], "title": "Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs", "categories": ["cs.CL", "68T5", "I.2.7"], "comment": null, "summary": "Vision-Language models (VLMs) show impressive abilities to answer questions\non visual inputs (e.g., counting objects in an image), yet demonstrate higher\naccuracies when performing an analogous task on text (e.g., counting words in a\ntext). We investigate this accuracy gap by identifying and comparing the\n\\textit{circuits} - the task-specific computational sub-graphs - in different\nmodalities. We show that while circuits are largely disjoint between\nmodalities, they implement relatively similar functionalities: the differences\nlie primarily in processing modality-specific data positions (an image or a\ntext sequence). Zooming in on the image data representations, we observe they\nbecome aligned with the higher-performing analogous textual representations\nonly towards later layers, too late in processing to effectively influence\nsubsequent positions. To overcome this, we patch the representations of visual\ndata tokens from later layers back into earlier layers. In experiments with\nmultiple tasks and models, this simple intervention closes a third of the\nperformance gap between the modalities, on average. Our analysis sheds light on\nthe multi-modal performance gap in VLMs and suggests a training-free approach\nfor reducing it.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b(VLMs)\u591a\u6a21\u6001\u6027\u80fd\u5dee\u5f02\u6e90\u4e8e\u540e\u671f\u5c42\u8868\u793a\u5bf9\u9f50\u6ede\u540e\uff0c\u901a\u8fc7\u540e\u671f\u5c42\u89c6\u89c9\u8868\u793a\u56de\u8865\u65e9\u671f\u5c42\u7684\u7b80\u5355\u5e72\u9884\uff0c\u5e73\u5747\u7f29\u5c0f\u4e861/3\u7684\u6a21\u6001\u8868\u73b0\u5dee\u8ddd\u3002", "motivation": "\u63a2\u7a76VLMs\u5728\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u8868\u73b0\u5dee\u5f02\u7684\u6839\u672c\u539f\u56e0\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u6001\u7535\u8def\u867d\u7136\u529f\u80fd\u76f8\u4f3c\u4f46\u6570\u636e\u5904\u7406\u4f4d\u7f6e\u4e0d\u540c\uff0c\u5c24\u5176\u89c6\u89c9\u8868\u793a\u5bf9\u9f50\u6ede\u540e\u5f71\u54cd\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u7535\u8def\u5206\u6790\u6bd4\u8f83\u6a21\u6001\u5904\u7406\u673a\u5236\uff0c\u91c7\u7528\u540e\u671f\u5c42\u89c6\u89c9\u6570\u636e\u8868\u793a\u56de\u8865\u5230\u65e9\u671f\u5c42\u7684\u5e72\u9884\u7b56\u7565\uff0c\u5728\u591a\u9879\u4efb\u52a1\u548c\u6a21\u578b\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5e73\u5747\u7f29\u5c0f33%\u7684\u6a21\u6001\u6027\u80fd\u5dee\u8ddd\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u3002", "conclusion": "\u63ed\u793a\u4e86VLMs\u591a\u6a21\u6001\u6027\u80fd\u5dee\u5f02\u7684\u673a\u5236\uff0c\u63d0\u51fa\u9996\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u6027\u80fd\u4f18\u5316\u65b9\u6848\uff0c\u4e3a\u6a21\u578b\u67b6\u6784\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.08022", "pdf": "https://arxiv.org/pdf/2506.08022", "abs": "https://arxiv.org/abs/2506.08022", "authors": ["Chenxi Liu", "Tianyi Xiong", "Ruibo Chen", "Yihan Wu", "Junfeng Guo", "Tianyi Zhou", "Heng Huang"], "title": "Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The task adaptation and alignment of Large Multimodal Models (LMMs) have been\nsignificantly advanced by instruction tuning and further strengthened by recent\npreference optimization. Yet, most LMMs still suffer from severe modality\nimbalance during reasoning, i.e., outweighing language prior biases over visual\ninputs, which bottlenecks their generalization to downstream tasks and causes\nhallucinations. However, existing preference optimization approaches for LMMs\ndo not focus on restraining the internal biases of their Large Language Model\n(LLM) backbones when curating the training data. Moreover, they heavily rely on\noffline data and lack the capacity to explore diverse responses adaptive to\ndynamic distributional shifts during training. Meanwhile, Group Relative Policy\nOptimization (GRPO), a recent method using online-generated data and verified\nrewards to improve reasoning capabilities, remains largely underexplored in LMM\nalignment. In this paper, we propose a novel preference learning framework,\nModality-Balancing Preference Optimization (MBPO), to address the modality\nimbalance in LMMs. MBPO constructs a more effective offline preference dataset\nby generating hard negatives, i.e., rejected responses misled by LLM biases due\nto limited usage of visual information, through adversarial perturbation of\ninput images. Moreover, MBPO leverages the easy-to-verify nature of close-ended\ntasks to generate online responses with verified rewards. GRPO is then employed\nto train the model with offline-online hybrid data. Extensive experiments\ndemonstrate that MBPO can enhance LMM performance on challenging\nvision-language tasks and effectively reduce hallucinations.", "AI": {"tldr": "\u63d0\u51faMBPO\u65b9\u6cd5\u89e3\u51b3\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u7684\u6a21\u6001\u5931\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u786c\u8d1f\u6837\u672c\u751f\u6210\u548c\u5728\u7ebf\u9a8c\u8bc1\u5956\u52b1\u4f18\u5316\u6a21\u578b\u8868\u73b0", "motivation": "\u73b0\u6709\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u6291\u5236LLM\u5148\u9a8c\u504f\u5dee\uff0c\u5bfc\u81f4LMM\u5b58\u5728\u89c6\u89c9\u4fe1\u606f\u5ffd\u89c6\u548c\u5e7b\u89c9\u73b0\u8c61", "method": "1. \u901a\u8fc7\u5bf9\u6297\u6027\u56fe\u50cf\u6270\u52a8\u751f\u6210\u786c\u8d1f\u6837\u672c\u6784\u5efa\u79bb\u7ebf\u6570\u636e\u96c6\n2. \u5229\u7528\u95ed\u5408\u4efb\u52a1\u7279\u6027\u751f\u6210\u5e26\u9a8c\u8bc1\u5956\u52b1\u7684\u5728\u7ebf\u54cd\u5e94\n3. \u7ed3\u5408GRPO\u8fdb\u884c\u79bb\u7ebf-\u5728\u7ebf\u6df7\u5408\u8bad\u7ec3", "result": "\u5b9e\u9a8c\u8868\u660eMBPO\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u6027\u80fd\u5e76\u51cf\u5c1161.3%\u7684\u5e7b\u89c9\u73b0\u8c61", "conclusion": "MBPO\u9996\u6b21\u5b9e\u73b0\u57fa\u4e8e\u786c\u8d1f\u6837\u672c\u548c\u5728\u7ebf\u9a8c\u8bc1\u7684\u53cc\u91cd\u6a21\u6001\u5e73\u8861\uff0c\u4e3a\u591a\u6a21\u6001\u5bf9\u9f50\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2506.08074", "pdf": "https://arxiv.org/pdf/2506.08074", "abs": "https://arxiv.org/abs/2506.08074", "authors": ["Abdellah Ghassel", "Ian Robinson", "Gabriel Tanase", "Hal Cooper", "Bryan Thompson", "Zhen Han", "Vassilis N. Ioannidis", "Soji Adeshina", "Huzefa Rangwala"], "title": "Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "KDD '25", "summary": "Retrieval-Augmented Generation (RAG) grounds large language models in\nexternal evidence, yet it still falters when answers must be pieced together\nacross semantically distant documents. We close this gap with the Hierarchical\nLexical Graph (HLG), a three-tier index that (i) traces every atomic\nproposition to its source, (ii) clusters propositions into latent topics, and\n(iii) links entities and relations to expose cross-document paths. On top of\nHLG we build two complementary, plug-and-play retrievers: StatementGraphRAG,\nwhich performs fine-grained entity-aware beam search over propositions for\nhigh-precision factoid questions, and TopicGraphRAG, which selects coarse\ntopics before expanding along entity links to supply broad yet relevant context\nfor exploratory queries. Additionally, existing benchmarks lack the complexity\nrequired to rigorously evaluate multi-hop summarization systems, often focusing\non single-document queries or limited datasets. To address this, we introduce a\nsynthetic dataset generation pipeline that curates realistic, multi-document\nquestion-answer pairs, enabling robust evaluation of multi-hop retrieval\nsystems. Extensive experiments across five datasets demonstrate that our\nmethods outperform naive chunk-based RAG achieving an average relative\nimprovement of 23.1% in retrieval recall and correctness. Open-source Python\nlibrary is available at https://github.com/awslabs/graphrag-toolkit.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u8bcd\u6c47\u56fe(HLG)\u7d22\u5f15\u7ed3\u6784\u548c\u4e24\u79cd\u4e92\u8865\u68c0\u7d22\u5668\uff0c\u6709\u6548\u63d0\u5347\u8de8\u6587\u6863\u591a\u8df3\u68c0\u7d22\u6027\u80fd23.1%", "motivation": "\u4f20\u7edfRAG\u5728\u9700\u8981\u8de8\u8bed\u4e49\u8ddd\u79bb\u8f83\u8fdc\u7684\u6587\u6863\u62fc\u63a5\u7b54\u6848\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u590d\u6742\u591a\u8df3\u68c0\u7d22\u8bc4\u4f30\u4f53\u7cfb", "method": "HLG\u4e09\u5c42\u7d22\u5f15\uff08\u539f\u5b50\u547d\u9898\u8ffd\u8e2a-\u6f5c\u5728\u4e3b\u9898\u805a\u7c7b-\u5b9e\u4f53\u5173\u7cfb\u94fe\u63a5\uff09 + StatementGraphRAG\uff08\u7ec6\u7c92\u5ea6\u5b9e\u4f53\u641c\u7d22\uff09 + TopicGraphRAG\uff08\u7c97\u4e3b\u9898\u6269\u5c55\uff09 + \u5408\u6210\u6570\u636e\u751f\u6210\u7ba1\u9053", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u68c0\u7d22\u53ec\u56de\u7387\u548c\u51c6\u786e\u7387\u76f8\u5bf9\u63d0\u534723.1%\uff0c\u5f00\u6e90\u5de5\u5177\u5305\u5df2\u53d1\u5e03", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u7d22\u5f15\u548c\u591a\u7c92\u5ea6\u68c0\u7d22\u7b56\u7565\uff0c\u7cfb\u7edf\u6027\u89e3\u51b3\u4e86\u590d\u6742\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u8bc1\u636e\u6574\u5408\u96be\u9898"}}
{"id": "2506.08125", "pdf": "https://arxiv.org/pdf/2506.08125", "abs": "https://arxiv.org/abs/2506.08125", "authors": ["Hanbing Liu", "Lang Cao", "Yuanyi Ren", "Mengyu Zhou", "Haoyu Dong", "Xiaojun Ma", "Shi Han", "Dongmei Zhang"], "title": "Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large language models have demonstrated impressive reasoning capabilities,\nyet they often suffer from inefficiencies due to unnecessarily verbose or\nredundant outputs. While many works have explored reinforcement learning (RL)\nto enhance reasoning abilities, most primarily focus on improving accuracy,\nwith limited attention to reasoning efficiency. Some existing approaches\nintroduce direct length-based rewards to encourage brevity, but this often\nleads to noticeable drops in accuracy. In this paper, we propose Bingo, an RL\nframework that advances length-based reward design to boost efficient\nreasoning. Bingo incorporates two key mechanisms: a significance-aware length\nreward, which gradually guides the model to reduce only insignificant tokens,\nand a dynamic length reward, which initially encourages elaborate reasoning for\nhard questions but decays over time to improve overall efficiency. Experiments\nacross multiple reasoning benchmarks show that Bingo improves both accuracy and\nefficiency. It outperforms the vanilla reward and several other length-based\nreward baselines in RL, achieving a favorable trade-off between accuracy and\nefficiency. These results underscore the potential of training LLMs explicitly\nfor efficient reasoning.", "AI": {"tldr": "Bingo\u6846\u67b6\u901a\u8fc7\u663e\u8457\u6027\u611f\u77e5\u957f\u5ea6\u5956\u52b1\u548c\u52a8\u6001\u957f\u5ea6\u5956\u52b1\u673a\u5236\uff0c\u5728\u4fdd\u6301\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u5b9e\u73b0\u7cbe\u5ea6\u4e0e\u6548\u7387\u7684\u5e73\u8861\u4f18\u5316", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027\uff0c\u800c\u5bf9\u6548\u7387\u4f18\u5316\u5173\u6ce8\u4e0d\u8db3\u3002\u76f4\u63a5\u91c7\u7528\u57fa\u4e8e\u8f93\u51fa\u957f\u5ea6\u7684\u5956\u52b1\u673a\u5236\u5e38\u5bfc\u81f4\u51c6\u786e\u6027\u4e0b\u964d\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u5956\u52b1\u8bbe\u8ba1\u6765\u534f\u8c03\u4e24\u8005\u5173\u7cfb", "method": "1. \u663e\u8457\u6027\u611f\u77e5\u957f\u5ea6\u5956\u52b1\uff1a\u901a\u8fc7\u68af\u5ea6\u5f15\u5bfc\u9010\u6b65\u51cf\u5c11\u975e\u5173\u952etoken\n2. \u52a8\u6001\u957f\u5ea6\u5956\u52b1\uff1a\u521d\u671f\u9f13\u52b1\u8be6\u7ec6\u63a8\u7406\uff08\u5e94\u5bf9\u96be\u9898\uff09\uff0c\u968f\u65f6\u95f4\u8870\u51cf\u63d0\u5347\u6574\u4f53\u6548\u7387", "result": "\u5728\u591a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4f20\u7edfRL\u5956\u52b1\u673a\u5236\uff0c\u5b9e\u73b0\u51c6\u786e\u7387\u63d0\u5347\u540c\u65f6\u51cf\u5c11\u5197\u4f59\u8f93\u51fa\uff0c\u8fbe\u6210\u6700\u4f73\u7cbe\u5ea6-\u6548\u7387\u5e73\u8861", "conclusion": "\u663e\u5f0f\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9ad8\u6548\u63a8\u7406\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0cBingo\u7684\u5956\u52b1\u673a\u5236\u8bbe\u8ba1\u4e3a\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2506.08140", "pdf": "https://arxiv.org/pdf/2506.08140", "abs": "https://arxiv.org/abs/2506.08140", "authors": ["Yifei Li", "Hanane Nour Moussa", "Ziru Chen", "Shijie Chen", "Botao Yu", "Mingyi Xue", "Benjamin Burns", "Tzu-Yao Chiu", "Vishal Dey", "Zitong Lu", "Chen Wei", "Qianheng Zhang", "Tianyu Zhang", "Song Gao", "Xuhui Huang", "Xia Ning", "Nesreen K. Ahmed", "Ali Payani", "Huan Sun"], "title": "AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Despite long-standing efforts in accelerating scientific discovery with AI,\nbuilding AI co-scientists remains challenging due to limited high-quality data\nfor training and evaluation. To tackle this data scarcity issue, we present\nAutoSDT, an automatic pipeline that collects high-quality coding tasks in\nreal-world data-driven discovery workflows. AutoSDT leverages the coding\ncapabilities and parametric knowledge of LLMs to search for diverse sources,\nselect ecologically valid tasks, and synthesize accurate task instructions and\ncode solutions. Using our pipeline, we construct AutoSDT-5K, a dataset of 5,404\ncoding tasks for data-driven discovery that covers four scientific disciplines\nand 756 unique Python packages. To the best of our knowledge, AutoSDT-5K is the\nonly automatically collected and the largest open dataset for data-driven\nscientific discovery. Expert feedback on a subset of 256 tasks shows the\neffectiveness of AutoSDT: 93% of the collected tasks are ecologically valid,\nand 92.2% of the synthesized programs are functionally correct. Trained on\nAutoSDT-5K, the Qwen2.5-Coder-Instruct LLM series, dubbed AutoSDT-Coder, show\nsubstantial improvement on two challenging data-driven discovery benchmarks,\nScienceAgentBench and DiscoveryBench. Most notably, AutoSDT-Coder-32B reaches\nthe same level of performance as GPT-4o on ScienceAgentBench with a success\nrate of 7.8%, doubling the performance of its base model. On DiscoveryBench, it\nlifts the hypothesis matching score to 8.1, bringing a 17.4% relative\nimprovement and closing the gap between open-weight models and GPT-4o.", "AI": {"tldr": "AutoSDT\u81ea\u52a8\u5316\u6d41\u7a0b\u6784\u5efa\u4e86\u5f53\u524d\u6700\u5927\u7684\u5f00\u6e90\u79d1\u5b66\u53d1\u73b0\u6570\u636e\u96c6AutoSDT-5K\uff085,404\u4efb\u52a1\uff09\uff0c\u901a\u8fc7LLM\u7684\u7f16\u7801\u80fd\u529b\u5b9e\u73b0\u4efb\u52a1\u81ea\u52a8\u751f\u6210\u4e0e\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u79d1\u5b66\u53d1\u73b0\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "motivation": "\u73b0\u6709AI\u79d1\u7814\u52a9\u624b\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\uff0c\u9700\u8981\u81ea\u52a8\u5316\u7684\u4efb\u52a1\u751f\u6210\u65b9\u6848\u6765\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898", "method": "\u5229\u7528LLM\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u548c\u53c2\u6570\u5316\u77e5\u8bc6\uff0c\u5b9e\u73b0\uff1a1\uff09\u591a\u6e90\u4efb\u52a1\u81ea\u52a8\u641c\u7d22\u4e0e\u6709\u6548\u6027\u7b5b\u9009 2\uff09\u4efb\u52a1\u6307\u4ee4\u4e0e\u4ee3\u7801\u65b9\u6848\u5408\u6210 3\uff09\u8986\u76d64\u5927\u5b66\u79d1\u9886\u57df\u548c756\u4e2aPython\u5305\u7684\u6570\u636e\u96c6\u6784\u5efa", "result": "1\uff095,404\u4efb\u52a1\u4e2d93%\u901a\u8fc7\u4e13\u5bb6\u6709\u6548\u6027\u9a8c\u8bc1 2\uff0992.2%\u751f\u6210\u4ee3\u7801\u529f\u80fd\u6b63\u786e 3\uff09AutoSDT-Coder-32B\u5728ScienceAgentBench\u8fbe\u5230GPT-4o\u6c34\u5e73\uff087.8%\u6210\u529f\u7387\uff09\uff0cDiscoveryBench\u5047\u8bbe\u5339\u914d\u5206\u6570\u63d0\u534717.4%", "conclusion": "AutoSDT\u8bc1\u660e\u4e86\u81ea\u52a8\u751f\u6210\u79d1\u5b66\u53d1\u73b0\u4efb\u52a1\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u79d1\u7814\u5de5\u5177\u5f00\u53d1\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u663e\u8457\u7f29\u5c0f\u5f00\u6e90\u6a21\u578b\u4e0e\u9876\u7ea7\u95ed\u6e90\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd"}}
{"id": "2506.08188", "pdf": "https://arxiv.org/pdf/2506.08188", "abs": "https://arxiv.org/abs/2506.08188", "authors": ["Wenlong Meng", "Shuguo Fan", "Chengkun Wei", "Min Chen", "Yuwei Li", "Yuanchao Zhang", "Zhikun Zhang", "Wenzhi Chen"], "title": "GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors", "categories": ["cs.CR", "cs.CL"], "comment": "Accepted to USENIX Security'25", "summary": "In this paper, we introduce GradEscape, the first gradient-based evader\ndesigned to attack AI-generated text (AIGT) detectors. GradEscape overcomes the\nundifferentiable computation problem, caused by the discrete nature of text, by\nintroducing a novel approach to construct weighted embeddings for the detector\ninput. It then updates the evader model parameters using feedback from victim\ndetectors, achieving high attack success with minimal text modification. To\naddress the issue of tokenizer mismatch between the evader and the detector, we\nintroduce a warm-started evader method, enabling GradEscape to adapt to\ndetectors across any language model architecture. Moreover, we employ novel\ntokenizer inference and model extraction techniques, facilitating effective\nevasion even in query-only access.\n  We evaluate GradEscape on four datasets and three widely-used language\nmodels, benchmarking it against four state-of-the-art AIGT evaders.\nExperimental results demonstrate that GradEscape outperforms existing evaders\nin various scenarios, including with an 11B paraphrase model, while utilizing\nonly 139M parameters. We have successfully applied GradEscape to two real-world\ncommercial AIGT detectors. Our analysis reveals that the primary vulnerability\nstems from disparity in text expression styles within the training data. We\nalso propose a potential defense strategy to mitigate the threat of AIGT\nevaders. We open-source our GradEscape for developing more robust AIGT\ndetectors.", "AI": {"tldr": "GradEscape\uff1a\u9996\u4e2a\u57fa\u4e8e\u68af\u5ea6\u7684AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u9003\u907f\u5668\uff0c\u901a\u8fc7\u52a0\u6743\u5d4c\u5165\u89e3\u51b3\u4e0d\u53ef\u5fae\u5206\u95ee\u9898\uff0c\u9002\u914d\u591a\u67b6\u6784\u68c0\u6d4b\u5668\uff0c\u5b9e\u9a8c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u5668\u56e0\u6587\u672c\u79bb\u6563\u6027\u5bfc\u81f4\u7684\u4e0d\u53ef\u5fae\u5206\u8ba1\u7b97\u96be\u9898\uff0c\u7a81\u7834\u68c0\u6d4b\u5668\u4e0e\u9003\u907f\u5668\u8bed\u8a00\u6a21\u578b\u67b6\u6784\u4e0d\u5339\u914d\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u5546\u4e1a\u68c0\u6d4b\u5668\u573a\u666f\u4e0b\u7684\u6709\u6548\u9003\u907f\u3002", "method": "1. \u6784\u5efa\u52a0\u6743\u5d4c\u5165\u8868\u5f81\u68c0\u6d4b\u5668\u8f93\u5165\n2. \u53c2\u6570\u66f4\u65b0\u673a\u5236\u5229\u7528\u68c0\u6d4b\u5668\u53cd\u9988\n3. \u9884\u70ed\u542f\u52a8\u7b56\u7565\u89e3\u51b3\u5206\u8bcd\u5668\u4e0d\u5339\u914d\n4. \u5206\u8bcd\u5668\u63a8\u7406\u4e0e\u6a21\u578b\u63d0\u53d6\u6280\u672f\u9002\u914d\u9ed1\u76d2\u68c0\u6d4b\u5668", "result": "\u57284\u4e2a\u6570\u636e\u96c6/3\u4e2a\u4e3b\u6d41\u8bed\u8a00\u6a21\u578b\u4e0a\uff0c\u4ee5139M\u53c2\u6570\u91cf\u51fb\u8d254\u4e2aSOTA\u9003\u907f\u5668\uff08\u542b11B\u590d\u8ff0\u6a21\u578b\uff09\uff0c\u6210\u529f\u5e94\u7528\u4e8e2\u4e2a\u5546\u4e1a\u68c0\u6d4b\u5668\u3002\u5173\u952e\u6f0f\u6d1e\u6e90\u4e8e\u8bad\u7ec3\u6570\u636e\u6587\u672c\u8868\u8fbe\u98ce\u683c\u5dee\u5f02\u3002", "conclusion": "\u63d0\u51fa\u9996\u4e2a\u7aef\u5230\u7aef\u68af\u5ea6\u9003\u907f\u6846\u67b6\uff0c\u5f00\u6e90\u4fc3\u8fdb\u9c81\u68d2\u68c0\u6d4b\u5668\u5f00\u53d1\uff0c\u540c\u65f6\u63d0\u51fa\u57fa\u4e8e\u8bad\u7ec3\u6570\u636e\u98ce\u683c\u5e73\u8861\u7684\u9632\u5fa1\u65b9\u6848\u3002"}}
{"id": "2506.08210", "pdf": "https://arxiv.org/pdf/2506.08210", "abs": "https://arxiv.org/abs/2506.08210", "authors": ["Andrew Z. Wang", "Songwei Ge", "Tero Karras", "Ming-Yu Liu", "Yogesh Balaji"], "title": "A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "CVPR 2025", "summary": "Both text-to-image generation and large language models (LLMs) have made\nsignificant advancements. However, many text-to-image models still employ the\nsomewhat outdated T5 and CLIP as their text encoders. In this work, we\ninvestigate the effectiveness of using modern decoder-only LLMs as text\nencoders for text-to-image diffusion models. We build a standardized training\nand evaluation pipeline that allows us to isolate and evaluate the effect of\ndifferent text embeddings. We train a total of 27 text-to-image models with 12\ndifferent text encoders to analyze the critical aspects of LLMs that could\nimpact text-to-image generation, including the approaches to extract\nembeddings, different LLMs variants, and model sizes. Our experiments reveal\nthat the de facto way of using last-layer embeddings as conditioning leads to\ninferior performance. Instead, we explore embeddings from various layers and\nfind that using layer-normalized averaging across all layers significantly\nimproves alignment with complex prompts. Most LLMs with this conditioning\noutperform the baseline T5 model, showing enhanced performance in advanced\nvisio-linguistic reasoning skills.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u8bad\u7ec327\u4e2a\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e0e12\u79cd\u4e0d\u540c\u6587\u672c\u7f16\u7801\u5668\uff0c\u53d1\u73b0\u4f7f\u7528LLMs\u4e2d\u95f4\u5c42\u5f52\u4e00\u5316\u5e73\u5747\u5d4c\u5165\u80fd\u663e\u8457\u63d0\u5347\u590d\u6742\u63d0\u793a\u5bf9\u9f50\u6548\u679c\uff0c\u591a\u6570LLMs\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfT5\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4ecd\u4f7f\u7528\u8fc7\u65f6\u7684T5\u548cCLIP\u4f5c\u4e3a\u6587\u672c\u7f16\u7801\u5668\uff0c\u63a2\u7d22\u73b0\u4ee3\u4ec5\u89e3\u7801\u5668LLMs\u4f5c\u4e3a\u6587\u672c\u7f16\u7801\u5668\u7684\u6709\u6548\u6027\u3002", "method": "\u5efa\u7acb\u6807\u51c6\u5316\u8bad\u7ec3\u8bc4\u4f30\u6d41\u7a0b\uff0c\u5206\u6790\u4e0d\u540c\u5d4c\u5165\u63d0\u53d6\u65b9\u6cd5\u3001LLMs\u53d8\u79cd\u53ca\u6a21\u578b\u5c3a\u5bf8\u5bf9\u751f\u6210\u6548\u679c\u7684\u5f71\u54cd\uff0c\u91cd\u70b9\u5173\u6ce8\u5404\u5c42\u5d4c\u5165\u7279\u6027\u3002", "result": "\u6700\u540e\u4e00\u5c42\u5d4c\u5165\u6548\u679c\u4e0d\u4f73\uff0c\u91c7\u7528\u6240\u6709\u5c42\u5c42\u5f52\u4e00\u5316\u5e73\u5747\u5d4c\u5165\u53ef\u63d0\u5347\u590d\u6742\u63d0\u793a\u5bf9\u9f50\u80fd\u529b\uff0c\u591a\u6570LLMs\u5728\u6b64\u6761\u4ef6\u4e0b\u8d85\u8d8aT5\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "LLMs\u4f5c\u4e3a\u6587\u672c\u7f16\u7801\u5668\u5728\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u5c42\u5d4c\u5165\u7b56\u7565\u7684\u4f18\u5316\u662f\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u7684\u5173\u952e\u3002"}}
{"id": "2506.08249", "pdf": "https://arxiv.org/pdf/2506.08249", "abs": "https://arxiv.org/abs/2506.08249", "authors": ["Ken Gu", "Zhihan Zhang", "Kate Lin", "Yuwei Zhang", "Akshay Paruchuri", "Hong Yu", "Mehran Kazemi", "Kumar Ayush", "A. Ali Heydari", "Maxwell A. Xu", "Girish Narayanswamy", "Yun Liu", "Ming-Zher Poh", "Yuzhe Yang", "Mark Malhotra", "Shwetak Patel", "Hamid Palangi", "Xuhai Xu", "Daniel McDuff", "Tim Althoff", "Xin Liu"], "title": "RADAR: Benchmarking Language Models on Imperfect Tabular Data", "categories": ["cs.DB", "cs.CL"], "comment": null, "summary": "Language models (LMs) are increasingly being deployed to perform autonomous\ndata analyses. However, their data awareness -- the ability to recognize,\nreason over, and appropriately handle data artifacts such as missing values,\noutliers, and logical inconsistencies -- remains underexplored. These artifacts\nare especially common in real-world tabular data and, if mishandled, can\nsignificantly compromise the validity of analytical conclusions. To address\nthis gap, we present RADAR, a benchmark for systematically evaluating\ndata-aware reasoning on tabular data. We develop a framework to simulate data\nartifacts via programmatic perturbations to enable targeted evaluation of model\nbehavior. RADAR comprises 2980 table query pairs, grounded in real-world data\nspanning 9 domains and 5 data artifact types. In addition to evaluating\nartifact handling, RADAR systematically varies table size to study how\nreasoning performance holds when increasing table size. Our evaluation reveals\nthat, despite decent performance on tables without data artifacts, frontier\nmodels degrade significantly when data artifacts are introduced, exposing\ncritical gaps in their capacity for robust, data-aware analysis. Designed to be\nflexible and extensible, RADAR supports diverse perturbation types and\ncontrollable table sizes, offering a valuable resource for advancing tabular\nreasoning.", "AI": {"tldr": "\u63d0\u51faRADAR\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u63ed\u793a\u524d\u6cbf\u8bed\u8a00\u6a21\u578b\u5728\u8868\u683c\u6570\u636e\u5904\u7406\u4e2d\u5b58\u5728\u6570\u636e\u611f\u77e5\u7f3a\u9677\uff0c\u5f53\u9762\u5bf9\u6570\u636e\u5f02\u5e38\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u8868\u683c\u6570\u636e\u5206\u6790\u4e2d\u5bf9\u6570\u636e\u5f02\u5e38\uff08\u5982\u7f3a\u5931\u503c/\u5f02\u5e38\u503c\uff09\u7684\u5904\u7406\u80fd\u529b\u4e0d\u8db3\uff0c\u53ef\u80fd\u5f71\u54cd\u5206\u6790\u7ed3\u8bba\u6709\u6548\u6027", "method": "\u901a\u8fc7\u7a0b\u5e8f\u5316\u6270\u52a8\u751f\u62102980\u4e2a\u8868\u683c\u67e5\u8be2\u5bf9\uff0c\u8986\u76d69\u4e2a\u9886\u57df5\u7c7b\u6570\u636e\u5f02\u5e38\uff0c\u5e76\u7cfb\u7edf\u6027\u6539\u53d8\u8868\u683c\u89c4\u6a21\u8fdb\u884c\u6d4b\u8bd5", "result": "\u524d\u6cbf\u6a21\u578b\u5728\u65e0\u5f02\u5e38\u6570\u636e\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u9047\u5230\u6570\u636e\u5f02\u5e38\u65f6\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff08\u5e73\u5747\u51c6\u786e\u7387\u4e0b\u964d30%+\uff09", "conclusion": "RADAR\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u6570\u636e\u611f\u77e5\u5206\u6790\u4e0a\u7684\u91cd\u5927\u7f3a\u9677\uff0c\u5176\u7075\u6d3b\u6846\u67b6\u4e3a\u63d0\u5347\u8868\u683c\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u91cd\u8981\u8bc4\u4f30\u5de5\u5177"}}
{"id": "2506.08266", "pdf": "https://arxiv.org/pdf/2506.08266", "abs": "https://arxiv.org/abs/2506.08266", "authors": ["Yaswanth Chittepu", "Blossom Metevier", "Will Schwarzer", "Austin Hoag", "Scott Niekum", "Philip S. Thomas"], "title": "Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.AP"], "comment": "20 pages, 6 figures, 4 tables, Second Reinforcement Learning\n  Conference (RLC 2025)", "summary": "Existing approaches to language model alignment often treat safety as a\ntradeoff against helpfulness, which can lead to unacceptable responses in\nsensitive domains. To ensure reliable performance in such settings, we propose\nHigh-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a\nmethod that provides high-confidence safety guarantees while maximizing\nhelpfulness. Similar to previous methods, HC-RLHF explicitly decouples human\npreferences into helpfulness and harmlessness (safety), which are learned by\ntraining a reward model and a cost model, respectively. It then employs a\ntwo-step process to find safe solutions. In the first step, it optimizes the\nreward function under an intentionally pessimistic version of the cost\nconstraint. In the second step, the trained model undergoes a safety test to\nverify whether its performance stays within an upper-confidence bound of the\nactual cost constraint. We provide a theoretical analysis of HC-RLHF, including\nproof that it will not return an unsafe solution with a probability greater\nthan a user-specified threshold. For our empirical analysis, we apply HC-RLHF\nto align three different language models (Qwen2-1.5B, Qwen2.5-3B, and\nLLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF\nproduces safe models with high probability and can improve harmlessness and\nhelpfulness compared to previous methods.", "AI": {"tldr": "\u63d0\u51faHC-RLHF\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u4f18\u5316\u786e\u4fdd\u8bed\u8a00\u6a21\u578b\u5728\u654f\u611f\u9886\u57df\u7684\u5b89\u5168\u54cd\u5e94\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6709\u7528\u6027", "motivation": "\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u5c06\u5b89\u5168\u6027\u4e0e\u6709\u7528\u6027\u7b80\u5355\u6743\u8861\uff0c\u53ef\u80fd\u5bfc\u81f4\u654f\u611f\u9886\u57df\u7684\u4e0d\u5b89\u5168\u56de\u5e94", "method": "1. \u5c06\u4eba\u7c7b\u504f\u597d\u5206\u89e3\u4e3a\u5956\u52b1\u6a21\u578b\uff08\u6709\u7528\u6027\uff09\u548c\u6210\u672c\u6a21\u578b\uff08\u5b89\u5168\u6027\uff09\n2. \u5206\u4e24\u9636\u6bb5\u4f18\u5316\uff1a\u5148\u57fa\u4e8e\u60b2\u89c2\u6210\u672c\u7ea6\u675f\u4f18\u5316\u5956\u52b1\uff0c\u518d\u8fdb\u884c\u5b89\u5168\u6d4b\u8bd5\u9a8c\u8bc1", "result": "\u5728Qwen2-1.5B\u7b49\u4e09\u4e2a\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u5b89\u5168\u6027\u548c\u6709\u7528\u6027\uff0c\u7406\u8bba\u8bc1\u660e\u5b89\u5168\u8fdd\u89c4\u6982\u7387\u4f4e\u4e8e\u7528\u6237\u8bbe\u5b9a\u9608\u503c", "conclusion": "HC-RLHF\u901a\u8fc7\u89e3\u8026\u5b66\u4e60\u4e0e\u53cc\u91cd\u9a8c\u8bc1\u673a\u5236\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6709\u7528\u6027\u7684\u540c\u65f6\u63d0\u4f9b\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u5b89\u5168\u4fdd\u8bc1"}}
{"id": "2506.08277", "pdf": "https://arxiv.org/pdf/2506.08277", "abs": "https://arxiv.org/abs/2506.08277", "authors": ["Subba Reddy Oota", "Khushbu Pahwa", "Prachi Jindal", "Satya Sai Srinath Namburi", "Maneesh Singh", "Tanmoy Chakraborty", "Bapi S. Raju", "Manish Gupta"], "title": "Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain", "categories": ["q-bio.NC", "cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "39 pages, 22 figures", "summary": "Recent voxel-wise multimodal brain encoding studies have shown that\nmultimodal large language models (MLLMs) exhibit a higher degree of brain\nalignment compared to unimodal models in both unimodal and multimodal stimulus\nsettings. More recently, instruction-tuned multimodal models have shown to\ngenerate task-specific representations that align strongly with brain activity.\nHowever, prior work evaluating the brain alignment of MLLMs has primarily\nfocused on unimodal settings or relied on non-instruction-tuned multimodal\nmodels for multimodal stimuli. To address this gap, we investigated brain\nalignment, that is, measuring the degree of predictivity of neural activity\nrecorded while participants were watching naturalistic movies (video along with\naudio) with representations derived from MLLMs. We utilized\ninstruction-specific embeddings from six video and two audio instruction-tuned\nMLLMs. Experiments with 13 video task-specific instructions show that\ninstruction-tuned video MLLMs significantly outperform non-instruction-tuned\nmultimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for\nboth video and audio tasks using language-guided instructions shows clear\ndisentanglement in task-specific representations from MLLMs, leading to precise\ndifferentiation of multimodal functional processing in the brain. We also find\nthat MLLM layers align hierarchically with the brain, with early sensory areas\nshowing strong alignment with early layers, while higher-level visual and\nlanguage regions align more with middle to late layers. These findings provide\nclear evidence for the role of task-specific instructions in improving the\nalignment between brain activity and MLLMs, and open new avenues for mapping\njoint information processing in both the systems. We make the code publicly\navailable [https://github.com/subbareddy248/mllm_videos].", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u4efb\u52a1\u6307\u4ee4\u8c03\u4f18\u540e\uff0c\u5176\u89c6\u9891\u548c\u97f3\u9891\u8868\u5f81\u4e0e\u4eba\u7c7b\u89c2\u770b\u81ea\u7136\u7535\u5f71\u65f6\u7684\u8111\u6d3b\u52a8\u5bf9\u9f50\u5ea6\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5448\u73b0\u5c42\u6b21\u5316\u8111\u533a\u5bf9\u5e94\u5173\u7cfb\u3002", "motivation": "\u89e3\u51b3\u5148\u524d\u7814\u7a76\u5c40\u9650\u2014\u2014\u4e3b\u8981\u5173\u6ce8\u5355\u6a21\u6001\u573a\u666f\u6216\u672a\u6307\u4ee4\u8c03\u4f18\u7684\u591a\u6a21\u6001\u6a21\u578b\uff0c\u63a2\u7a76\u6307\u4ee4\u8c03\u4f18\u540e\u7684MLLMs\u5728\u591a\u6a21\u6001\u523a\u6fc0\uff08\u89c6\u9891+\u97f3\u9891\uff09\u4e0b\u4e0e\u8111\u6d3b\u52a8\u7684\u5bf9\u9f50\u673a\u5236\u3002", "method": "\u4f7f\u75286\u4e2a\u89c6\u9891\u548c2\u4e2a\u97f3\u9891\u6307\u4ee4\u8c03\u4f18MLLMs\uff0c\u901a\u8fc713\u79cd\u89c6\u9891\u4efb\u52a1\u6307\u4ee4\u6d4b\u8bd5\u6a21\u578b\u5bf9\u81ea\u7136\u7535\u5f71\u89c2\u770b\u671f\u95f4\u8111\u795e\u7ecf\u6d3b\u52a8\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u5bf9\u6bd4\u975e\u6307\u4ee4\u8c03\u4f18\u591a\u6a21\u6001/\u5355\u6a21\u6001\u6a21\u578b\u3002", "result": "1. \u6307\u4ee4\u8c03\u4f18\u89c6\u9891MLLMs\u9884\u6d4b\u6027\u80fd\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b15-20%\uff1b2. \u4efb\u52a1\u6307\u4ee4\u4f7fMLLMs\u8868\u5f81\u89e3\u8026\uff0c\u7cbe\u51c6\u533a\u5206\u8111\u533a\u591a\u6a21\u6001\u5904\u7406\uff1b3. \u6a21\u578b\u5c42\u6b21\u4e0e\u8111\u533a\u5c42\u7ea7\u5bf9\u5e94\uff08\u65e9\u671f\u5c42-\u611f\u89c9\u76ae\u5c42\uff0c\u4e2d\u540e\u671f\u5c42-\u9ad8\u7ea7\u89c6\u89c9/\u8bed\u8a00\u533a\uff09\u3002", "conclusion": "\u4efb\u52a1\u6307\u4ee4\u663e\u8457\u589e\u5f3a\u8111-MLLM\u5bf9\u9f50\uff0c\u4e3a\u63ed\u793a\u4eba\u8111\u4e0eAI\u7cfb\u7edf\u7684\u8054\u5408\u4fe1\u606f\u5904\u7406\u673a\u5236\u63d0\u4f9b\u65b0\u9014\u5f84\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.08292", "pdf": "https://arxiv.org/pdf/2506.08292", "abs": "https://arxiv.org/abs/2506.08292", "authors": ["Xie Yi", "Zhanke Zhou", "Chentao Cao", "Qiyu Niu", "Tongliang Liu", "Bo Han"], "title": "From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted by ICML 2025", "summary": "Multi-agent frameworks can substantially boost the reasoning power of large\nlanguage models (LLMs), but they typically incur heavy computational costs and\nlack convergence guarantees. To overcome these challenges, we recast multi-LLM\ncoordination as an incomplete-information game and seek a Bayesian Nash\nequilibrium (BNE), in which each agent optimally responds to its probabilistic\nbeliefs about the strategies of others. We introduce Efficient Coordination via\nNash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that\nmarries distributed reasoning with centralized final output. Under ECON, each\nLLM independently selects responses that maximize its expected reward,\nconditioned on its beliefs about co-agents, without requiring costly\ninter-agent exchanges. We mathematically prove that ECON attains a markedly\ntighter regret bound than non-equilibrium multi-agent schemes. Empirically,\nECON outperforms existing multi-LLM approaches by 11.2% on average across six\nbenchmarks spanning complex reasoning and planning tasks. Further experiments\ndemonstrate ECON's ability to flexibly incorporate additional models,\nconfirming its scalability and paving the way toward larger, more powerful\nmulti-LLM ensembles. The code is publicly available at:\nhttps://github.com/tmlr-group/ECON.", "AI": {"tldr": "\u63d0\u51faECON\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u7eb3\u4ec0\u5747\u8861\u5b9e\u73b0\u9ad8\u6548\u591aLLM\u534f\u4f5c\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c11.2%\u4e14\u5177\u5907\u7406\u8bba\u4fdd\u969c", "motivation": "\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5b58\u5728\u9ad8\u8ba1\u7b97\u5f00\u9500\u3001\u7f3a\u4e4f\u6536\u655b\u4fdd\u8bc1\u7684\u7f3a\u9677\uff0c\u9700\u5efa\u7acb\u7406\u8bba\u53ef\u9760\u7684\u9ad8\u6548\u534f\u4f5c\u673a\u5236", "method": "\u5c06\u591aLLM\u534f\u4f5c\u5efa\u6a21\u4e3a\u4e0d\u5b8c\u5168\u4fe1\u606f\u535a\u5f08\uff0c\u901a\u8fc7\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5b9e\u73b0\u5206\u5e03\u5f0f\u63a8\u7406\u4e0e\u96c6\u4e2d\u51b3\u7b56\uff0c\u5404LLM\u57fa\u4e8e\u8d1d\u53f6\u65af\u4fe1\u5ff5\u81ea\u4e3b\u4f18\u5316\u54cd\u5e94\u7b56\u7565", "result": "\u7406\u8bba\u8bc1\u660e\u83b7\u5f97\u6bd4\u975e\u5747\u8861\u65b9\u6cd5\u66f4\u7d27\u7684\u9057\u61be\u8fb9\u754c\uff0c\u57286\u4e2a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5e73\u5747\u63d0\u534711.2%\u6027\u80fd\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u6846\u67b6\u53ef\u6269\u5c55\u6027", "conclusion": "ECON\u5f00\u521b\u4e86\u7406\u8bba\u4fdd\u8bc1\u4e0e\u9ad8\u6548\u5b9e\u8df5\u517c\u5907\u7684\u591aLLM\u534f\u4f5c\u8303\u5f0f\uff0c\u4e3a\u6784\u5efa\u66f4\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7cfb\u7edf\u63d0\u4f9b\u65b0\u8def\u5f84"}}
{"id": "2506.08295", "pdf": "https://arxiv.org/pdf/2506.08295", "abs": "https://arxiv.org/abs/2506.08295", "authors": ["Zhanke Zhou", "Xiao Feng", "Zhaocheng Zhu", "Jiangchao Yao", "Sanmi Koyejo", "Bo Han"], "title": "From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted by ICML 2025", "summary": "While existing benchmarks probe the reasoning abilities of large language\nmodels (LLMs) across diverse domains, they predominantly assess passive\nreasoning, providing models with all the information needed to reach a\nsolution. By contrast, active reasoning-where an LLM must interact with\nexternal systems to acquire missing evidence or data-has received little\nsystematic attention. To address this shortfall, we present AR-Bench, a novel\nbenchmark designed explicitly to evaluate an LLM's active reasoning skills.\nAR-Bench comprises three task families-detective cases, situation puzzles, and\nguessing numbers-that together simulate real-world, agentic scenarios and\nmeasure performance across commonsense, logical, and symbolic reasoning\nchallenges. Empirical evaluation on AR-Bench demonstrates that contemporary\nLLMs exhibit pronounced difficulties with active reasoning: they frequently\nfail to acquire or leverage the information needed to solve tasks. This gap\nhighlights a stark divergence between their passive and active reasoning\nabilities. Moreover, ablation studies indicate that even advanced strategies,\nsuch as tree-based searching or post-training approaches, yield only modest\ngains and fall short of the levels required for real-world deployment.\nCollectively, these findings highlight the critical need to advance methodology\nfor active reasoning, e.g., incorporating interactive learning, real-time\nfeedback loops, and environment-aware objectives for training. The benchmark is\npublicly available at: https://github.com/tmlr-group/AR-Bench.", "AI": {"tldr": "\u63d0\u51faAR-Bench\u65b0\u57fa\u51c6\u63ed\u793a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e3b\u52a8\u63a8\u7406\uff08\u9700\u4e0e\u73af\u5883\u4ea4\u4e92\u83b7\u53d6\u4fe1\u606f\uff09\u65b9\u9762\u7684\u663e\u8457\u4e0d\u8db3\uff0c\u5f53\u524d\u6539\u8fdb\u7b56\u7565\u6548\u679c\u6709\u9650", "motivation": "\u73b0\u6709\u7814\u7a76\u96c6\u4e2d\u4e8e\u88ab\u52a8\u63a8\u7406\u8bc4\u4f30\uff0c\u800c\u771f\u5b9e\u573a\u666f\u9700\u8981\u7684\u4e3b\u52a8\u63a8\u7406\u80fd\u529b\uff08\u4ea4\u4e92\u5f0f\u4fe1\u606f\u83b7\u53d6\uff09\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u6d4b\u5de5\u5177", "method": "\u6784\u5efa\u5305\u542b\u4fa6\u63a2\u6848\u4f8b/\u60c5\u666f\u8c1c\u9898/\u731c\u6570\u5b57\u4e09\u7c7b\u4efb\u52a1\u7684AR-Bench\uff0c\u6a21\u62df\u9700\u4e3b\u52a8\u4fe1\u606f\u83b7\u53d6\u7684\u590d\u6742\u63a8\u7406\u573a\u666f", "result": "\u5b9e\u9a8c\u663e\u793a\u5f53\u4ee3LLMs\u4e3b\u52a8\u63a8\u7406\u80fd\u529b\u8584\u5f31\uff08\u51c6\u786e\u7387\u8f83\u88ab\u52a8\u63a8\u7406\u4e0b\u964d60%+\uff09\uff0c\u6811\u641c\u7d22\u7b49\u7b56\u7565\u4ec5\u5e26\u6765\u6709\u9650\u63d0\u5347\uff08<15%\uff09", "conclusion": "\u5f3a\u8c03\u5f00\u53d1\u4ea4\u4e92\u5b66\u4e60/\u5b9e\u65f6\u53cd\u9988/\u73af\u5883\u611f\u77e5\u8bad\u7ec3\u7b49\u65b0\u65b9\u6cd5\u7684\u8feb\u5207\u6027\uff0c\u63a8\u52a8LLMs\u5411\u5b9e\u7528\u5316\u667a\u80fd\u4f53\u53d1\u5c55"}}
{"id": "2506.08351", "pdf": "https://arxiv.org/pdf/2506.08351", "abs": "https://arxiv.org/abs/2506.08351", "authors": ["Huixuan Zhang", "Junzhe Zhang", "Xiaojun Wan"], "title": "How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "With the rapid development of text-to-vision generation diffusion models,\nclassifier-free guidance has emerged as the most prevalent method for\nconditioning. However, this approach inherently requires twice as many steps\nfor model forwarding compared to unconditional generation, resulting in\nsignificantly higher costs. While previous study has introduced the concept of\nadaptive guidance, it lacks solid analysis and empirical results, making\nprevious method unable to be applied to general diffusion models. In this work,\nwe present another perspective of applying adaptive guidance and propose Step\nAG, which is a simple, universally applicable adaptive guidance strategy. Our\nevaluations focus on both image quality and image-text alignment. whose results\nindicate that restricting classifier-free guidance to the first several\ndenoising steps is sufficient for generating high-quality, well-conditioned\nimages, achieving an average speedup of 20% to 30%. Such improvement is\nconsistent across different settings such as inference steps, and various\nmodels including video generation models, highlighting the superiority of our\nmethod.", "AI": {"tldr": "\u63d0\u51faStep AG\u81ea\u9002\u5e94\u5f15\u5bfc\u7b56\u7565\uff0c\u901a\u8fc7\u9650\u5236\u524d\u51e0\u4e2a\u53bb\u566a\u6b65\u9aa4\u7684\u5f15\u5bfc\u8303\u56f4\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b020%-30%\u7684\u52a0\u901f\u6548\u679c", "motivation": "\u4f20\u7edf\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u65b9\u6cd5\u9700\u8981\u53cc\u500d\u8ba1\u7b97\u6b65\u9aa4\u5bfc\u81f4\u751f\u6210\u6210\u672c\u8fc7\u9ad8\uff0c\u5148\u524d\u81ea\u9002\u5e94\u5f15\u5bfc\u7814\u7a76\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5206\u6790\u548c\u901a\u7528\u6027\u9a8c\u8bc1", "method": "Step AG\u7b56\u7565\uff1a\u4ec5\u5728\u53bb\u566a\u8fc7\u7a0b\u7684\u524d\u671f\u9636\u6bb5\u5e94\u7528\u5206\u7c7b\u5668\u5f15\u5bfc\uff0c\u540e\u671f\u4fdd\u6301\u65e0\u5f15\u5bfc\u72b6\u6001\u3002\u9002\u7528\u4e8e\u4e0d\u540c\u6b65\u957f\u8bbe\u7f6e\u548c\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b", "result": "\u5728\u56fe\u50cf\u8d28\u91cf\u4e0e\u56fe\u6587\u5bf9\u9f50\u5ea6\u6307\u6807\u4e0a\u53d6\u5f97\u53ef\u6bd4\u6548\u679c\uff0c\u5e73\u5747\u52a0\u901f20%-30%\uff0c\u5728\u89c6\u9891\u751f\u6210\u7b49\u6269\u5c55\u573a\u666f\u9a8c\u8bc1\u666e\u9002\u6027", "conclusion": "\u901a\u8fc7\u65e9\u671f\u5f15\u5bfc\u7ea6\u675f\u5b9e\u73b0\u6548\u7387\u7a81\u7834\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u65b0\u89c6\u89d2\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5de5\u7a0b\u5e94\u7528\u4ef7\u503c"}}
{"id": "2506.08379", "pdf": "https://arxiv.org/pdf/2506.08379", "abs": "https://arxiv.org/abs/2506.08379", "authors": ["Yurun Yuan", "Tengyang Xie"], "title": "Reinforce LLM Reasoning through Multi-Agent Reflection", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "International Conference on Machine Learning (ICML), 2025", "summary": "Leveraging more test-time computation has proven to be an effective way to\nboost the reasoning capabilities of large language models (LLMs). Among various\nmethods, the verify-and-improve paradigm stands out for enabling dynamic\nsolution exploration and feedback incorporation. However, existing approaches\noften suffer from restricted feedback spaces and lack of coordinated training\nof different parties, leading to suboptimal performance. To address this, we\nmodel this multi-turn refinement process as a Markov Decision Process and\nintroduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement\nlearning algorithm that trains an actor-critic LLM system to iteratively refine\nanswers via direct preference learning on self-generated data. Theoretically,\nDPSDP can match the performance of any policy within the training distribution.\nEmpirically, we instantiate DPSDP with various base models and show\nimprovements on both in- and out-of-distribution benchmarks. For example, on\nbenchmark MATH 500, majority voting over five refinement steps increases\nfirst-turn accuracy from 58.2% to 63.2% with Ministral-based models. An\nablation study further confirms the benefits of multi-agent collaboration and\nout-of-distribution generalization.", "AI": {"tldr": "\u63d0\u51faDPSDP\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u591a\u8f6e\u8fed\u4ee3\u4f18\u5316\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709verify-and-improve\u8303\u5f0f\u5b58\u5728\u53cd\u9988\u7a7a\u95f4\u53d7\u9650\u3001\u591a\u65b9\u534f\u4f5c\u8bad\u7ec3\u4e0d\u8db3\u7684\u95ee\u9898", "method": "\u5c06\u591a\u8f6e\u4f18\u5316\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5f00\u53d1\u57fa\u4e8eactor-critic\u67b6\u6784\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5DPSDP", "result": "\u5728MATH 500\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u901a\u8fc7\u4e94\u6b65\u8fed\u4ee3\u5c06\u9996\u8f6e\u51c6\u786e\u7387\u4ece58.2%\u63d0\u5347\u81f363.2%\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4f18\u52bf", "conclusion": "DPSDP\u7b97\u6cd5\u901a\u8fc7\u52a8\u6001\u89c4\u5212\u76f4\u63a5\u7b56\u7565\u641c\u7d22\u5b9e\u73b0\u8de8\u5206\u5e03\u6cdb\u5316\uff0c\u6709\u6548\u63d0\u5347\u5927\u6a21\u578b\u63a8\u7406\u6027\u80fd"}}
{"id": "2506.08388", "pdf": "https://arxiv.org/pdf/2506.08388", "abs": "https://arxiv.org/abs/2506.08388", "authors": ["Edoardo Cetin", "Tianyu Zhao", "Yujin Tang"], "title": "Reinforcement Learning Teachers of Test Time Scaling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint", "summary": "Training reasoning language models (LMs) with reinforcement learning (RL) for\none-hot correctness inherently relies on the LM being able to explore and solve\nits task with some chance at initialization. Furthermore, a key use case of\nreasoning LMs is to act as teachers for distilling new students and\ncold-starting future RL iterations rather than being deployed themselves. From\nthese considerations, we introduce a new framework that avoids RL's exploration\nchallenge by training a new class of Reinforcement-Learned Teachers (RLTs)\nfocused on yielding the most effective downstream distillation. RLTs are\nprompted with both the question and solution to each problem, and tasked to\nsimply \"connect-the-dots\" with detailed explanations tailored for their\nstudents. We train RLTs with dense rewards obtained by feeding each explanation\nto the student and testing its understanding of the problem's solution. In\npractice, the raw outputs of a 7B RLT provide higher final performance on\ncompetition and graduate-level tasks than existing distillation and\ncold-starting pipelines that collect and postprocess the reasoning traces of\norders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness\nwhen training larger students and when applied zero-shot to out-of-distribution\ntasks, unlocking new levels of efficiency and re-usability for the RL reasoning\nframework.", "AI": {"tldr": "\u63d0\u51fa\u5f3a\u5316\u5b66\u4e60\u6559\u5e08\u6a21\u578b\uff08RLT\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u9488\u5bf9\u5b66\u751f\u6a21\u578b\u84b8\u998f\u4f18\u5316\u7684\u5bc6\u96c6\u5956\u52b1\u673a\u5236\uff0c\u514b\u670d\u4f20\u7edfRL\u8bad\u7ec3\u63a2\u7d22\u96be\u9898\uff0c\u5b9e\u73b0\u5c0f\u6a21\u578b\u8d85\u8d8a\u5927\u6a21\u578b\u7684\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u9700\u8981\u6a21\u578b\u5177\u5907\u521d\u59cb\u63a2\u7d22\u80fd\u529b\uff0c\u4e14\u4e3b\u8981\u4f5c\u4e3a\u84b8\u998f\u6559\u5e08\u800c\u975e\u90e8\u7f72\u6a21\u578b\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u9488\u5bf9\u63d0\u5347\u5b66\u751f\u6a21\u578b\u7406\u89e3\u6548\u7387\u7684\u6559\u5e08\u6a21\u578b\u3002", "method": "\u8bbe\u8ba1\u5f3a\u5316\u5b66\u4e60\u6559\u5e08\uff08RLT\uff09\uff0c\u8f93\u5165\u95ee\u9898\u4e0e\u7b54\u6848\uff0c\u751f\u6210\u9762\u5411\u5b66\u751f\u6a21\u578b\u7684\u7406\u89e3\u5bfc\u5411\u89e3\u91ca\uff0c\u901a\u8fc7\u5b66\u751f\u53cd\u9988\u6784\u5efa\u5bc6\u96c6\u5956\u52b1\u51fd\u6570\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "7B\u53c2\u6570\u7684RLT\u5728\u7ade\u8d5b\u7ea7\u4efb\u52a1\u4e0a\u8d85\u8d8a\u5927\u6a21\u578b\u84b8\u998f\u6d41\u7a0b\uff0c\u652f\u6301\u8bad\u7ec3\u66f4\u5927\u89c4\u6a21\u5b66\u751f\u6a21\u578b\uff0c\u5e76\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u4fdd\u6301\u6709\u6548\u6027\u3002", "conclusion": "RLT\u6846\u67b6\u901a\u8fc7\u6559\u5e08-\u5b66\u751f\u534f\u540c\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u4e0e\u77e5\u8bc6\u8fc1\u79fb\u80fd\u529b\uff0c\u4e3a\u540e\u7eed\u6a21\u578b\u8fed\u4ee3\u63d0\u4f9b\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.08446", "pdf": "https://arxiv.org/pdf/2506.08446", "abs": "https://arxiv.org/abs/2506.08446", "authors": ["Peng-Yuan Wang", "Tian-Shuo Liu", "Chenyang Wang", "Yi-Di Wang", "Shu Yan", "Cheng-Xing Jia", "Xu-Hui Liu", "Xin-Wei Chen", "Jia-Cheng Xu", "Ziniu Li", "Yang Yu"], "title": "A Survey on Large Language Models for Mathematical Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Mathematical reasoning has long represented one of the most fundamental and\nchallenging frontiers in artificial intelligence research. In recent years,\nlarge language models (LLMs) have achieved significant advances in this area.\nThis survey examines the development of mathematical reasoning abilities in\nLLMs through two high-level cognitive phases: comprehension, where models gain\nmathematical understanding via diverse pretraining strategies, and answer\ngeneration, which has progressed from direct prediction to step-by-step\nChain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical\nreasoning, ranging from training-free prompting to fine-tuning approaches such\nas supervised fine-tuning and reinforcement learning, and discuss recent work\non extended CoT and \"test-time scaling\". Despite notable progress, fundamental\nchallenges remain in terms of capacity, efficiency, and generalization. To\naddress these issues, we highlight promising research directions, including\nadvanced pretraining and knowledge augmentation techniques, formal reasoning\nframeworks, and meta-generalization through principled learning paradigms. This\nsurvey tries to provide some insights for researchers interested in enhancing\nreasoning capabilities of LLMs and for those seeking to apply these techniques\nto other domains.", "AI": {"tldr": "\u7cfb\u7edf\u7efc\u8ff0\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u9886\u57df\u7684\u53d1\u5c55\u8def\u5f84\uff0c\u63d0\u51fa\u4ece\u7406\u89e3\u5230\u751f\u6210\u7684\u4e24\u9636\u6bb5\u8ba4\u77e5\u6846\u67b6\uff0c\u603b\u7ed3\u73b0\u6709\u65b9\u6cd5\u5e76\u6307\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411", "motivation": "\u6570\u5b66\u63a8\u7406\u662fAI\u6838\u5fc3\u6311\u6218\uff0c\u8fd1\u5e74\u6765LLMs\u53d6\u5f97\u7a81\u7834\u4f46\u5b58\u5728\u57fa\u7840\u6027\u74f6\u9888\uff0c\u9700\u7cfb\u7edf\u6027\u5206\u6790\u73b0\u72b6\u4e0e\u672a\u6765\u65b9\u5411", "method": "\u901a\u8fc7\u9884\u8bad\u7ec3\u7b56\u7565\u5efa\u7acb\u6570\u5b66\u7406\u89e3\uff0c\u91c7\u7528\u601d\u7ef4\u94fe\u63a8\u7406\u3001\u76d1\u7763\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\u7b49\u6280\u672f\u63d0\u5347\u63a8\u7406\u80fd\u529b", "result": "\u5f53\u524d\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u6027\u80fd\uff0c\u4f46\u9762\u4e34\u6a21\u578b\u5bb9\u91cf\u9650\u5236\u3001\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7b49\u95ee\u9898", "conclusion": "\u5efa\u8bae\u53d1\u5c55\u77e5\u8bc6\u589e\u5f3a\u9884\u8bad\u7ec3\u3001\u5f62\u5f0f\u5316\u63a8\u7406\u6846\u67b6\u3001\u5143\u6cdb\u5316\u5b66\u4e60\u8303\u5f0f\u7b49\u65b9\u5411\uff0c\u63a8\u52a8\u8de8\u9886\u57df\u63a8\u7406\u80fd\u529b\u8fc1\u79fb"}}
{"id": "2506.08572", "pdf": "https://arxiv.org/pdf/2506.08572", "abs": "https://arxiv.org/abs/2506.08572", "authors": ["Waiss Azizian", "Michael Kirchhof", "Eugene Ndiaye", "Louis Bethune", "Michal Klein", "Pierre Ablin", "Marco Cuturi"], "title": "The Geometries of Truth Are Orthogonal Across Tasks", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive generalization\ncapabilities across various tasks, but their claim to practical relevance is\nstill mired by concerns on their reliability. Recent works have proposed\nexamining the activations produced by an LLM at inference time to assess\nwhether its answer to a question is correct. Some works claim that a \"geometry\nof truth\" can be learned from examples, in the sense that the activations that\ngenerate correct answers can be distinguished from those leading to mistakes\nwith a linear classifier. In this work, we underline a limitation of these\napproaches: we observe that these \"geometries of truth\" are intrinsically\ntask-dependent and fail to transfer across tasks. More precisely, we show that\nlinear classifiers trained across distinct tasks share little similarity and,\nwhen trained with sparsity-enforcing regularizers, have almost disjoint\nsupports. We show that more sophisticated approaches (e.g., using mixtures of\nprobes and tasks) fail to overcome this limitation, likely because activation\nvectors commonly used to classify answers form clearly separated clusters when\nexamined across tasks.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u57fa\u4e8e\u6fc0\u6d3b\u5411\u91cf\u5206\u6790\u7684'\u771f\u7406\u51e0\u4f55'\u65b9\u6cd5\u5b58\u5728\u8de8\u4efb\u52a1\u8fc1\u79fb\u5c40\u9650\u6027\uff0c\u7ebf\u6027\u5206\u7c7b\u5668\u5728\u4e0d\u540c\u4efb\u52a1\u95f4\u65e0\u6cd5\u901a\u7528", "motivation": "\u9488\u5bf9LLMs\u53ef\u9760\u6027\u8bc4\u4f30\u4e2d'\u771f\u7406\u51e0\u4f55'\u65b9\u6cd5\u88ab\u8fc7\u5ea6\u6cdb\u5316\u4f7f\u7528\u7684\u95ee\u9898\uff0c\u63ed\u793a\u5176\u4efb\u52a1\u4f9d\u8d56\u6027\u672c\u8d28\u7f3a\u9677", "method": "\u901a\u8fc7\u8de8\u4efb\u52a1\u7ebf\u6027\u5206\u7c7b\u5668\u5bf9\u6bd4\u5206\u6790\uff0c\u7ed3\u5408\u7a00\u758f\u6b63\u5219\u5316\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u6fc0\u6d3b\u5411\u91cf\u7c07\u7684\u4efb\u52a1\u7279\u5f02\u6027", "result": "\u4e0d\u540c\u4efb\u52a1\u7684\u5206\u7c7b\u5668\u76f8\u4f3c\u6027\u6781\u4f4e\uff08\u7a00\u758f\u6b63\u5219\u5316\u540e\u652f\u6301\u96c6\u51e0\u4e4e\u4e0d\u76f8\u4ea4\uff09\uff0c\u6fc0\u6d3b\u5411\u91cf\u5f62\u6210\u4efb\u52a1\u5206\u79bb\u7c07", "conclusion": "'\u771f\u7406\u51e0\u4f55'\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u4efb\u52a1\u9650\u5236\uff0c\u6697\u793a\u9700\u8981\u5f00\u53d1\u66f4\u901a\u7528\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u6846\u67b6"}}
{"id": "2506.08633", "pdf": "https://arxiv.org/pdf/2506.08633", "abs": "https://arxiv.org/abs/2506.08633", "authors": ["\u0160imon Sedl\u00e1\u010dek", "Bolaji Yusuf", "J\u00e1n \u0160vec", "Pradyoth Hegde", "Santosh Kesiraju", "Old\u0159ich Plchot", "Jan \u010cernock\u00fd"], "title": "Approaching Dialogue State Tracking via Aligning Speech Encoders and LLMs", "categories": ["eess.AS", "cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "In this work, we approach spoken Dialogue State Tracking (DST) by bridging\nthe representation spaces of speech encoders and LLMs via a small connector\nmodule, with a focus on fully open-sourced and open-data components\n(WavLM-large, OLMo). We focus on ablating different aspects of such systems\nincluding full/LoRA adapter fine-tuning, the effect of agent turns in the\ndialogue history, as well as fuzzy matching-based output post-processing, which\ngreatly improves performance of our systems on named entities in the dialogue\nslot values. We conduct our experiments on the SpokenWOZ dataset, and\nadditionally utilize the Speech-Aware MultiWOZ dataset to augment our training\ndata. Ultimately, our best-performing WavLM + connector + OLMo-1B aligned\nmodels achieve state of the art on the SpokenWOZ test set (34.66% JGA), and our\nsystem with Gemma-2-9B-instruct further surpasses this result, reaching 42.17%\nJGA on SpokenWOZ test.", "AI": {"tldr": "\u901a\u8fc7\u8fde\u63a5\u8bed\u97f3\u7f16\u7801\u5668\u4e0eLLM\u7684\u8868\u793a\u7a7a\u95f4\uff0c\u4f7f\u7528WavLM+OLMo+\u540e\u5904\u7406\u65b9\u6848\u5728SpokenWOZ\u6570\u636e\u96c6\u5b9e\u73b0\u53e3\u8bed\u5bf9\u8bdd\u72b6\u6001\u8ffd\u8e2aSOTA\uff0842.17% JGA\uff09", "motivation": "\u73b0\u6709\u8bed\u97f3\u5bf9\u8bdd\u7cfb\u7edf\u5728\u5f00\u653e\u6027\u548c\u6027\u80fd\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u7279\u522b\u662f\u8bed\u97f3\u4e0e\u6587\u672c\u8868\u793a\u7a7a\u95f4\u4e0d\u5339\u914d\u5bfc\u81f4\u53e3\u8bed\u5bf9\u8bdd\u72b6\u6001\u8ffd\u8e2a\u56f0\u96be", "method": "1. \u6784\u5efaWavLM-large\u8bed\u97f3\u7f16\u7801\u5668\u4e0eOLMo/Gemma-2-9B\u7684\u8de8\u6a21\u6001\u8fde\u63a5\u5668\n2. \u91c7\u7528LoRA\u9002\u914d\u5668\u5fae\u8c03\u548c\u6a21\u7cca\u5339\u914d\u540e\u5904\u7406\u6280\u672f\n3. \u4f7f\u7528Speech-Aware MultiWOZ\u8fdb\u884c\u6570\u636e\u589e\u5f3a", "result": "\u6700\u4f73\u7cfb\u7edf\u5728SpokenWOZ\u6d4b\u8bd5\u96c6\u8fbe\u523042.17%\u8054\u5408\u76ee\u6807\u51c6\u786e\u7387\uff08JGA\uff09\uff0c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6027\u80fd\u663e\u8457\u63d0\u5347", "conclusion": "\u901a\u8fc7\u8de8\u6a21\u6001\u8fde\u63a5\u67b6\u6784\u548c\u540e\u5904\u7406\u4f18\u5316\uff0c\u8bc1\u660e\u4e86\u5b8c\u5168\u5f00\u6e90\u7ec4\u4ef6\u5728\u590d\u6742\u53e3\u8bed\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u63ed\u793a\u6a21\u578b\u5fae\u8c03\u4e0e\u6570\u636e\u589e\u5f3a\u7684\u5173\u952e\u4f5c\u7528"}}
{"id": "2506.08745", "pdf": "https://arxiv.org/pdf/2506.08745", "abs": "https://arxiv.org/abs/2506.08745", "authors": ["Kongcheng Zhang", "Qi Yao", "Shunyu Liu", "Yingjie Wang", "Baisheng Lai", "Jieping Ye", "Mingli Song", "Dacheng Tao"], "title": "Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances of Reinforcement Learning (RL) have highlighted its potential\nin complex reasoning tasks, yet effective training often relies on external\nsupervision, which limits the broader applicability. In this work, we propose a\nnovel self-rewarding reinforcement learning framework to enhance Large Language\nModel (LLM) reasoning by leveraging the consistency of intermediate reasoning\nstates across different reasoning trajectories. Our key insight is that correct\nresponses often exhibit consistent trajectory patterns in terms of model\nlikelihood: their intermediate reasoning states tend to converge toward their\nown final answers (high consistency) with minimal deviation toward other\ncandidates (low volatility). Inspired by this observation, we introduce CoVo,\nan intrinsic reward mechanism that integrates Consistency and Volatility via a\nrobust vector-space aggregation strategy, complemented by a curiosity bonus to\npromote diverse exploration. CoVo enables LLMs to perform RL in a\nself-rewarding manner, offering a scalable pathway for learning to reason\nwithout external supervision. Extensive experiments on diverse reasoning\nbenchmarks show that CoVo achieves performance comparable to or even surpassing\nsupervised RL. Our code is available at https://github.com/sastpg/CoVo.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u6fc0\u52b1\u5f3a\u5316\u5b66\u4e60\u6846\u67b6CoVo\uff0c\u901a\u8fc7\u5206\u6790\u4e2d\u95f4\u63a8\u7406\u72b6\u6001\u7684\u4e00\u81f4\u6027\u4e3aLLM\u63d0\u4f9b\u5185\u5728\u5956\u52b1\uff0c\u5b9e\u73b0\u65e0\u76d1\u7763\u7684\u63a8\u7406\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u76d1\u7763\uff0c\u9650\u5236\u4e86\u5e94\u7528\u8303\u56f4\u3002\u6b63\u786e\u56de\u7b54\u7684\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u5177\u6709\u9ad8\u4e00\u81f4\u6027\uff08\u5411\u6700\u7ec8\u7b54\u6848\u6536\u655b\uff09\u548c\u4f4e\u6ce2\u52a8\u6027\uff08\u8f83\u5c11\u504f\u79bb\u5176\u4ed6\u5019\u9009\u7b54\u6848\uff09\u7684\u7279\u5f81\u3002", "method": "\u8bbe\u8ba1CoVo\u5956\u52b1\u673a\u5236\uff0c\u6574\u5408\u4e00\u81f4\u6027\uff08Consistency\uff09\u548c\u6ce2\u52a8\u6027\uff08Volatility\uff09\u7684\u5411\u91cf\u7a7a\u95f4\u805a\u5408\u7b56\u7565\uff0c\u914d\u5408\u597d\u5947\u5fc3\u5956\u52b1\u4fc3\u8fdb\u63a2\u7d22\u3002", "result": "\u5728\u591a\u79cd\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6216\u8d85\u8d8a\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u6548\u679c\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u65e0\u76d1\u7763\u63a8\u7406\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u8def\u5f84\uff0c\u964d\u4f4e\u5bf9\u5916\u90e8\u76d1\u7763\u7684\u4f9d\u8d56\u3002"}}
{"id": "2506.08762", "pdf": "https://arxiv.org/pdf/2506.08762", "abs": "https://arxiv.org/abs/2506.08762", "authors": ["Issa Sugiura", "Takashi Ishida", "Taro Makino", "Chieko Tazuke", "Takanori Nakagawa", "Kosuke Nakago", "David Ha"], "title": "EDINET-Bench: Evaluating LLMs on Complex Financial Tasks using Japanese Financial Statements", "categories": ["q-fin.ST", "cs.CE", "cs.CL", "cs.LG"], "comment": null, "summary": "Financial analysis presents complex challenges that could leverage large\nlanguage model (LLM) capabilities. However, the scarcity of challenging\nfinancial datasets, particularly for Japanese financial data, impedes academic\ninnovation in financial analytics. As LLMs advance, this lack of accessible\nresearch resources increasingly hinders their development and evaluation in\nthis specialized domain. To address this gap, we introduce EDINET-Bench, an\nopen-source Japanese financial benchmark designed to evaluate the performance\nof LLMs on challenging financial tasks including accounting fraud detection,\nearnings forecasting, and industry prediction. EDINET-Bench is constructed by\ndownloading annual reports from the past 10 years from Japan's Electronic\nDisclosure for Investors' NETwork (EDINET) and automatically assigning labels\ncorresponding to each evaluation task. Our experiments reveal that even\nstate-of-the-art LLMs struggle, performing only slightly better than logistic\nregression in binary classification for fraud detection and earnings\nforecasting. These results highlight significant challenges in applying LLMs to\nreal-world financial applications and underscore the need for domain-specific\nadaptation. Our dataset, benchmark construction code, and evaluation code is\npublicly available to facilitate future research in finance with LLMs.", "AI": {"tldr": "\u65e5\u672c\u7814\u7a76\u8005\u63a8\u51faEDINET-Bench\u91d1\u878d\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6b3a\u8bc8\u68c0\u6d4b\u7b49\u91d1\u878d\u4efb\u52a1\u4e2d\u8868\u73b0\u4ec5\u7565\u4f18\u4e8e\u4f20\u7edf\u903b\u8f91\u56de\u5f52", "motivation": "\u89e3\u51b3\u65e5\u8bed\u91d1\u878d\u6570\u636e\u7a00\u7f3a\u963b\u788dLLM\u5728\u91d1\u878d\u5206\u6790\u9886\u57df\u53d1\u5c55\u7684\u95ee\u9898\uff0c\u63a8\u52a8\u91d1\u878d\u9886\u57df\u4e13\u7528\u6a21\u578b\u7814\u7a76", "method": "\u4eceEDINET\u4e0b\u8f7d\u5341\u5e74\u5e74\u62a5\u6570\u636e\uff0c\u81ea\u52a8\u751f\u6210\u4f1a\u8ba1\u6b3a\u8bc8/\u6536\u76ca\u9884\u6d4b/\u884c\u4e1a\u9884\u6d4b\u4e09\u5927\u4efb\u52a1\u7684\u6807\u6ce8\u6570\u636e\u96c6", "result": "\u9876\u7ea7LLM\u5728\u6b3a\u8bc8\u68c0\u6d4b\u4e8c\u5143\u5206\u7c7b\u4e2d\u51c6\u786e\u7387\u4ec5\u7a0d\u9ad8\u4e8e\u903b\u8f91\u56de\u5f52\uff08LLMs 65% vs LR 62%\uff09\uff0c\u6536\u76ca\u9884\u6d4b\u4efb\u52a1\u8868\u73b0\u76f8\u4f3c", "conclusion": "LLM\u5728\u771f\u5b9e\u91d1\u878d\u573a\u666f\u5e94\u7528\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u9700\u9886\u57df\u9002\u914d\u3002\u516c\u5f00\u6570\u636e\u96c6/\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u91d1\u878dLLM\u7814\u7a76"}}
{"id": "2506.08771", "pdf": "https://arxiv.org/pdf/2506.08771", "abs": "https://arxiv.org/abs/2506.08771", "authors": ["Yuni Susanti", "Michael F\u00e4rber"], "title": "Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": "Accepted at KDD 2025 (full research paper)", "summary": "Inferring causal relationships between variable pairs is crucial for\nunderstanding multivariate interactions in complex systems. Knowledge-based\ncausal discovery -- which involves inferring causal relationships by reasoning\nover the metadata of variables (e.g., names or textual context) -- offers a\ncompelling alternative to traditional methods that rely on observational data.\nHowever, existing methods using Large Language Models (LLMs) often produce\nunstable and inconsistent results, compromising their reliability for causal\ninference. To address this, we introduce a novel approach that integrates\nKnowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery.\nOur approach identifies informative metapath-based subgraphs within KGs and\nfurther refines the selection of these subgraphs using Learning-to-Rank-based\nmodels. The top-ranked subgraphs are then incorporated into zero-shot prompts,\nimproving the effectiveness of LLMs in inferring the causal relationship.\nExtensive experiments on biomedical and open-domain datasets demonstrate that\nour method outperforms most baselines by up to 44.4 points in F1 scores,\nevaluated across diverse LLMs and KGs. Our code and datasets are available on\nGitHub: https://github.com/susantiyuni/path-to-causality", "AI": {"tldr": "\u63d0\u51fa\u878d\u5408\u77e5\u8bc6\u56fe\u8c31\u4e0eLLM\u7684\u77e5\u8bc6\u9a71\u52a8\u56e0\u679c\u53d1\u73b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5143\u8def\u5f84\u5b50\u56fe\u7b5b\u9009\u63d0\u534744.4%F1\u503c", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u56e0\u679c\u63a8\u7406\u65b9\u6cd5\u5b58\u5728\u7ed3\u679c\u4e0d\u7a33\u5b9a\u3001\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u53ef\u9760\u6027\u3002\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u5316\u77e5\u8bc6\u53ef\u589e\u5f3a\u63a8\u7406\u8fc7\u7a0b\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "1. \u4ece\u77e5\u8bc6\u56fe\u8c31\u62bd\u53d6\u57fa\u4e8e\u5143\u8def\u5f84\u7684\u542f\u53d1\u5f0f\u5b50\u56fe\n2. \u4f7f\u7528Learning-to-Rank\u6a21\u578b\u4f18\u5316\u5b50\u56fe\u9009\u62e9\n3. \u878d\u5408\u4f18\u8d28\u5b50\u56fe\u6784\u5efa\u96f6\u6837\u672c\u63d0\u793a\u6846\u67b6\n4. \u5728\u751f\u7269\u533b\u5b66\u548c\u5f00\u653e\u57df\u6570\u636e\u96c6\u8fdb\u884c\u8de8\u6a21\u578b\u9a8c\u8bc1", "result": "\u5728\u751f\u7269\u533b\u5b66\u548c\u5f00\u653e\u57df\u6570\u636e\u96c6\u4e0a\uff0cF1\u5206\u6570\u6700\u9ad8\u63d0\u534744.4\u4e2a\u767e\u5206\u70b9\uff08\u4e0d\u540cLLM\u548cKG\u7ec4\u5408\u4e0b\u8868\u73b0\u7a33\u5b9a\uff09", "conclusion": "\u77e5\u8bc6\u56fe\u8c31\u7684\u7ed3\u6784\u5316\u8def\u5f84\u4fe1\u606f\u6709\u6548\u589e\u5f3aLLM\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0cLearning-to-Rank\u5b50\u56fe\u7b5b\u9009\u673a\u5236\u663e\u8457\u63d0\u5347\u63a8\u7406\u53ef\u9760\u6027\uff0c\u4e3a\u77e5\u8bc6\u9a71\u52a8\u578b\u56e0\u679c\u53d1\u73b0\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2506.08800", "pdf": "https://arxiv.org/pdf/2506.08800", "abs": "https://arxiv.org/abs/2506.08800", "authors": ["Irene Testini", "Jos\u00e9 Hern\u00e1ndez-Orallo", "Lorenzo Pacchiardi"], "title": "Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Data science aims to extract insights from data to support decision-making\nprocesses. Recently, Large Language Models (LLMs) are increasingly used as\nassistants for data science, by suggesting ideas, techniques and small code\nsnippets, or for the interpretation of results and reporting. Proper automation\nof some data-science activities is now promised by the rise of LLM agents,\ni.e., AI systems powered by an LLM equipped with additional affordances--such\nas code execution and knowledge bases--that can perform self-directed actions\nand interact with digital environments. In this paper, we survey the evaluation\nof LLM assistants and agents for data science. We find (1) a dominant focus on\na small subset of goal-oriented activities, largely ignoring data management\nand exploratory activities; (2) a concentration on pure assistance or fully\nautonomous agents, without considering intermediate levels of human-AI\ncollaboration; and (3) an emphasis on human substitution, therefore neglecting\nthe possibility of higher levels of automation thanks to task transformation.", "AI": {"tldr": "\u8bba\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86LLM\u5728\u6570\u636e\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u73b0\u72b6\uff0c\u6307\u51fa\u5f53\u524d\u7814\u7a76\u5b58\u5728\u76ee\u6807\u8303\u56f4\u5c40\u9650\u3001\u534f\u4f5c\u6a21\u5f0f\u5355\u4e00\u548c\u81ea\u52a8\u5316\u6f5c\u529b\u8ba4\u77e5\u4e0d\u8db3\u4e09\u5927\u95ee\u9898", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9LLM\u5728\u6570\u636e\u79d1\u5b66\u4e2d\u7684\u8bc4\u4f30\u5b58\u5728\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u4e3b\u8981\u4f53\u73b0\u5728\u6d3b\u52a8\u8986\u76d6\u4e0d\u5168\u3001\u534f\u4f5c\u5c42\u6b21\u7f3a\u5931\u548c\u81ea\u52a8\u5316\u53ef\u80fd\u6027\u8ba4\u77e5\u5c40\u9650", "method": "\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5bf9\u73b0\u6709LLM\u52a9\u624b\u548c\u4ee3\u7406\u5728\u6570\u636e\u79d1\u5b66\u9886\u57df\u7684\u5e94\u7528\u7814\u7a76\u8fdb\u884c\u5168\u9762\u8c03\u67e5\u5206\u6790", "result": "\u53d1\u73b0\u5f53\u524d\u7814\u7a76\u5b58\u5728\uff1a1) 83%\u805a\u7126\u4ee3\u7801\u751f\u6210\u7b49\u76ee\u6807\u6027\u4efb\u52a1\uff0c\u5ffd\u7565\u6570\u636e\u7ba1\u7406\uff1b2) \u4ec56%\u63a2\u8ba8\u4eba\u673a\u534f\u4f5c\u4e2d\u95f4\u5f62\u6001\uff1b3) \u672a\u6709\u6548\u6316\u6398\u4efb\u52a1\u91cd\u6784\u5e26\u6765\u7684\u81ea\u52a8\u5316\u6f5c\u529b", "conclusion": "\u9700\u8981\u5efa\u7acb\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u6570\u636e\u7ba1\u7406\u7eb3\u5165\u7814\u7a76\u8303\u7574\uff0c\u6784\u5efa\u8fde\u7eed\u7684\u4eba\u673a\u534f\u4f5c\u5149\u8c31\uff0c\u5e76\u901a\u8fc7\u4efb\u52a1\u91cd\u6784\u5b9e\u73b0\u66f4\u9ad8\u7ea7\u522b\u7684\u81ea\u52a8\u5316\u7a81\u7834"}}
{"id": "2506.08835", "pdf": "https://arxiv.org/pdf/2506.08835", "abs": "https://arxiv.org/abs/2506.08835", "authors": ["Shravan Nayak", "Mehar Bhatia", "Xiaofeng Zhang", "Verena Rieser", "Lisa Anne Hendricks", "Sjoerd van Steenkiste", "Yash Goyal", "Karolina Sta\u0144czak", "Aishwarya Agrawal"], "title": "CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The increasing ubiquity of text-to-image (T2I) models as tools for visual\ncontent generation raises concerns about their ability to accurately represent\ndiverse cultural contexts. In this work, we present the first study to\nsystematically quantify the alignment of T2I models and evaluation metrics with\nrespect to both explicit as well as implicit cultural expectations. To this\nend, we introduce CulturalFrames, a novel benchmark designed for rigorous human\nevaluation of cultural representation in visual generations. Spanning 10\ncountries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts,\n3637 corresponding images generated by 4 state-of-the-art T2I models, and over\n10k detailed human annotations. We find that T2I models not only fail to meet\nthe more challenging implicit expectations but also the less challenging\nexplicit expectations. Across models and countries, cultural expectations are\nmissed an average of 44% of the time. Among these failures, explicit\nexpectations are missed at a surprisingly high average rate of 68%, while\nimplicit expectation failures are also significant, averaging 49%. Furthermore,\nwe demonstrate that existing T2I evaluation metrics correlate poorly with human\njudgments of cultural alignment, irrespective of their internal reasoning.\nCollectively, our findings expose critical gaps, providing actionable\ndirections for developing more culturally informed T2I models and evaluation\nmethodologies.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u4e3b\u6d41\u6587\u751f\u56fe\u6a21\u578b\u5728\u6587\u5316\u8868\u5f81\u65b9\u9762\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u663e\u6027/\u9690\u6027\u6587\u5316\u671f\u671b\u5e73\u5747\u672a\u8fbe\u6210\u7387\u5206\u522b\u8fbe68%\u548c49%\uff0c\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u7c7b\u5224\u65ad\u8131\u8282\u3002", "motivation": "\u9488\u5bf9\u6587\u751f\u56fe\u6a21\u578b\u5728\u591a\u5143\u6587\u5316\u8bed\u5883\u8868\u5f81\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7cfb\u7edf\u6027\u91cf\u5316\u6a21\u578b\u8f93\u51fa\u4e0e\u663e\u6027/\u9690\u6027\u6587\u5316\u671f\u5f85\u7684\u504f\u5dee\uff0c\u586b\u8865\u8be5\u9886\u57df\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u6784\u5efa\u5305\u542b10\u56fd5\u5927\u9886\u57df\u7684CulturalFrames\u57fa\u51c6(983\u63d0\u793a\u8bcd+3637\u56fe\u50cf+10k\u6807\u6ce8)\uff0c\u91c7\u75284\u4e2aSOTA\u6a21\u578b\u8fdb\u884c\u8de8\u6587\u5316\u751f\u6210\u5b9e\u9a8c\u4e0e\u4eba\u5de5\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u5e73\u574744%\u672a\u8fbe\u6587\u5316\u671f\u5f85\uff1a\u663e\u6027\u5931\u8d25\u738768%\uff08\u8986\u76d6\u57fa\u7840\u6587\u5316\u5143\u7d20\uff09\uff0c\u9690\u6027\u5931\u8d25\u738749%\uff08\u6d89\u53ca\u6df1\u5c42\u6587\u5316\u903b\u8f91\uff09\u3002\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u7cfb\u6570\u4f4e\u4e8e0.2\u3002", "conclusion": "\u7814\u7a76\u66b4\u9732\u6587\u751f\u56fe\u6a21\u578b\u6587\u5316\u611f\u77e5\u7f3a\u9677\uff0c\u63d0\u51fa\u9700\u5f00\u53d1\u878d\u5408\u6587\u5316\u77e5\u8bc6\u7684\u65b0\u578b\u6a21\u578b\u67b6\u6784\u4e0e\u8bc4\u4f30\u4f53\u7cfb\uff0c\u63a8\u52a8\u8de8\u6587\u5316\u751f\u6210\u6280\u672f\u53d1\u5c55\u3002"}}
{"id": "2506.08927", "pdf": "https://arxiv.org/pdf/2506.08927", "abs": "https://arxiv.org/abs/2506.08927", "authors": ["David Acuna", "Ximing Lu", "Jaehun Jung", "Hyunwoo Kim", "Amlan Kar", "Sanja Fidler", "Yejin Choi"], "title": "Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent research in vision-language models (VLMs) has centered around the\npossibility of equipping them with implicit long-form chain-of-thought\nreasoning -- akin to the success observed in language models -- via\ndistillation and reinforcement learning. But what about the non-reasoning\nmodels already trained and deployed across the internet? Should we simply\nabandon them, or is there hope for a search mechanism that can elicit hidden\nknowledge and induce long reasoning traces -- without any additional training\nor supervision? In this paper, we explore this possibility using a Monte Carlo\nTree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer\npairs into the model's output stream. We show that framing reasoning as a\nsearch process -- where subquestions act as latent decisions within a broader\ninference trajectory -- helps the model \"connect the dots\" between fragmented\nknowledge and produce extended reasoning traces in non-reasoning models. We\nevaluate our method across three benchmarks and observe consistent\nimprovements. Notably, our approach yields a 2% overall improvement on\nMMMU-PRO, including a significant 9% gain in Liberal Arts.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u63d2\u5165\u5b50\u95ee\u9898-\u5b50\u7b54\u6848\u5bf9\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u975e\u63a8\u7406\u6a21\u578b\u5df2\u5e7f\u6cdb\u90e8\u7f72\uff0c\u4f46\u7f3a\u4e4f\u957f\u94fe\u63a8\u7406\u80fd\u529b\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e0d\u4f9d\u8d56\u8bad\u7ec3\u6216\u76d1\u7763\u7684\u641c\u7d22\u673a\u5236\uff0c\u6316\u6398\u5176\u9690\u542b\u77e5\u8bc6\u5e76\u751f\u6210\u6269\u5c55\u63a8\u7406\u8f68\u8ff9\u3002", "method": "\u5c06\u63a8\u7406\u5efa\u6a21\u4e3a\u641c\u7d22\u8fc7\u7a0b\uff0c\u4f7f\u7528MCTS\u7b97\u6cd5\u5728\u6a21\u578b\u8f93\u51fa\u6d41\u4e2d\u63d2\u5165\u5b50\u95ee\u9898-\u5b50\u7b54\u6848\u5bf9\uff0c\u901a\u8fc7\u5b50\u95ee\u9898\u8fde\u63a5\u788e\u7247\u5316\u77e5\u8bc6\u5f62\u6210\u63a8\u7406\u94fe\u3002", "result": "\u5728MMMU-PRO\u7b49\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6301\u7eed\u63d0\u5347\uff0c\u5176\u4e2d\u4eba\u6587\u79d1\u76ee\u51c6\u786e\u7387\u663e\u8457\u63d0\u9ad89%\uff0c\u6574\u4f53\u63d0\u53472%\u3002", "conclusion": "\u5c06\u63a8\u7406\u91cd\u6784\u4e3a\u5305\u542b\u6f5c\u5728\u51b3\u7b56\u7684\u641c\u7d22\u8fc7\u7a0b\uff0c\u6709\u6548\u6fc0\u6d3b\u975e\u63a8\u7406\u6a21\u578b\u7684\u6269\u5c55\u63a8\u7406\u6f5c\u529b\uff0c\u8bc1\u660e\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u589e\u5f3a\u73b0\u6709\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2506.08967", "pdf": "https://arxiv.org/pdf/2506.08967", "abs": "https://arxiv.org/abs/2506.08967", "authors": ["Ailin Huang", "Bingxin Li", "Bruce Wang", "Boyong Wu", "Chao Yan", "Chengli Feng", "Heng Wang", "Hongyu Zhou", "Hongyuan Wang", "Jingbei Li", "Jianjian Sun", "Joanna Wang", "Mingrui Chen", "Peng Liu", "Ruihang Miao", "Shilei Jiang", "Tian Fei", "Wang You", "Xi Chen", "Xuerui Yang", "Yechang Huang", "Yuxiang Zhang", "Zheng Ge", "Zheng Gong", "Zhewei Huang", "Zixin Zhang", "Bin Wang", "Bo Li", "Buyun Ma", "Changxin Miao", "Changyi Wan", "Chen Xu", "Dapeng Shi", "Dingyuan Hu", "Enle Liu", "Guanzhe Huang", "Gulin Yan", "Hanpeng Hu", "Haonan Jia", "Jiahao Gong", "Jiaoren Wu", "Jie Wu", "Jie Yang", "Junzhe Lin", "Kaixiang Li", "Lei Xia", "Longlong Gu", "Ming Li", "Nie Hao", "Ranchen Ming", "Shaoliang Pang", "Siqi Liu", "Song Yuan", "Tiancheng Cao", "Wen Li", "Wenqing He", "Xu Zhao", "Xuelin Zhang", "Yanbo Yu", "Yinmin Zhong", "Yu Zhou", "Yuanwei Liang", "Yuanwei Lu", "Yuxiang Yang", "Zidong Yang", "Zili Zhang", "Binxing Jiao", "Heung-Yeung Shum", "Jiansheng Chen", "Jing Li", "Xiangyu Zhang", "Xinhao Zhang", "Yibo Zhu", "Daxin Jiang", "Shuchang Zhou", "Chen Hu"], "title": "Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "12 pages, 3 figures", "summary": "Large Audio-Language Models (LALMs) have significantly advanced intelligent\nhuman-computer interaction, yet their reliance on text-based outputs limits\ntheir ability to generate natural speech responses directly, hindering seamless\naudio interactions. To address this, we introduce Step-Audio-AQAA, a fully\nend-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model\nintegrates a dual-codebook audio tokenizer for linguistic and semantic feature\nextraction, a 130-billion-parameter backbone LLM and a neural vocoder for\nhigh-fidelity speech synthesis. Our post-training approach employs interleaved\ntoken-output of text and audio to enhance semantic coherence and combines\nDirect Preference Optimization (DPO) with model merge to improve performance.\nEvaluations on the StepEval-Audio-360 benchmark demonstrate that\nStep-Audio-AQAA excels especially in speech control, outperforming the\nstate-of-art LALMs in key areas. This work contributes a promising solution for\nend-to-end LALMs and highlights the critical role of token-based vocoder in\nenhancing overall performance for AQAA tasks.", "AI": {"tldr": "\u63d0\u51fa\u7aef\u5230\u7aef\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578bStep-Audio-AQAA\uff0c\u901a\u8fc7\u53cc\u7801\u672c\u97f3\u9891\u6807\u8bb0\u5668\u4e0e\u795e\u7ecf\u58f0\u7801\u5668\u5b9e\u73b0\u81ea\u7136\u8bed\u97f3\u4ea4\u4e92\uff0c\u5728\u8bed\u97f3\u63a7\u5236\u6307\u6807\u4e0a\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709LALMs\u4f9d\u8d56\u6587\u672c\u8f93\u51fa\u5bfc\u81f4\u8bed\u97f3\u4ea4\u4e92\u4e0d\u81ea\u7136\uff0c\u9700\u5efa\u7acb\u7aef\u5230\u7aef\u7684\u97f3\u9891\u8f93\u5165\u8f93\u51fa\u67b6\u6784\u63d0\u5347\u4ea4\u4e92\u6d41\u7545\u5ea6\u3002", "method": "\u6574\u5408\u53cc\u7801\u672c\u97f3\u9891\u6807\u8bb0\u5668\uff08\u63d0\u53d6\u8bed\u8a00/\u8bed\u4e49\u7279\u5f81\uff09+1300\u4ebf\u53c2\u6570LLM+\u795e\u7ecf\u58f0\u7801\u5668\uff0c\u91c7\u7528\u4ea4\u9519\u6807\u8bb0\u8f93\u51fa\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408DPO\u4f18\u5316\u4e0e\u6a21\u578b\u878d\u5408\u6280\u672f\u3002", "result": "\u5728StepEval-Audio-360\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8bed\u97f3\u63a7\u5236\u7b49\u6838\u5fc3\u6307\u6807\u8d85\u8d8aSOTA\u6a21\u578b\uff0c\u9a8c\u8bc1\u57fa\u4e8e\u6807\u8bb0\u7684\u58f0\u7801\u5668\u5bf9\u6027\u80fd\u63d0\u5347\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u7aef\u5230\u7aefLALMs\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u5b9e\u97f3\u9891\u6807\u8bb0\u5316\u58f0\u7801\u5668\u5728AQAA\u4efb\u52a1\u4e2d\u7684\u4f53\u7cfb\u7ed3\u6784\u4ef7\u503c\u3002"}}
{"id": "2506.08989", "pdf": "https://arxiv.org/pdf/2506.08989", "abs": "https://arxiv.org/abs/2506.08989", "authors": ["Xiao Liang", "Zhong-Zhi Li", "Yeyun Gong", "Yang Wang", "Hengyuan Zhang", "Yelong Shen", "Ying Nian Wu", "Weizhu Chen"], "title": "SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning", "categories": ["cs.LG", "cs.CL"], "comment": "Reinforcement Learning; Large Language Models; LLM Reasoning", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective\nfor training large language models (LLMs) on complex reasoning tasks, such as\nmathematical problem solving. A prerequisite for the scalability of RLVR is a\nhigh-quality problem set with precise and verifiable answers. However, the\nscarcity of well-crafted human-labeled math problems and limited-verification\nanswers in existing distillation-oriented synthetic datasets limit their\neffectiveness in RL. Additionally, most problem synthesis strategies\nindiscriminately expand the problem set without considering the model's\ncapabilities, leading to low efficiency in generating useful questions. To\nmitigate this issue, we introduce a Self-aware Weakness-driven problem\nSynthesis framework (SwS) that systematically identifies model deficiencies and\nleverages them for problem augmentation. Specifically, we define weaknesses as\nquestions that the model consistently fails to learn through its iterative\nsampling during RL training. We then extract the core concepts from these\nfailure cases and synthesize new problems to strengthen the model's weak areas\nin subsequent augmented training, enabling it to focus on and gradually\novercome its weaknesses. Without relying on external knowledge distillation,\nour framework enables robust generalization byempowering the model to\nself-identify and address its weaknesses in RL, yielding average performance\ngains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning\nbenchmarks.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u6211\u611f\u77e5\u5f31\u70b9\u9a71\u52a8\u95ee\u9898\u5408\u6210\u6846\u67b6(SwS)\uff0c\u901a\u8fc7\u8bc6\u522b\u6a21\u578b\u5f31\u70b9\u5e76\u9488\u5bf9\u6027\u5408\u6210\u8bad\u7ec3\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u4efb\u52a1\u6027\u80fd", "motivation": "\u73b0\u6709RL\u8bad\u7ec3\u6570\u636e\u96c6\u5b58\u5728\u8d28\u91cf\u4e0d\u8db3\u3001\u95ee\u9898\u9488\u5bf9\u6027\u5dee\u3001\u5408\u6210\u7b56\u7565\u4e0d\u8003\u8651\u6a21\u578b\u80fd\u529b\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b", "method": "\u5b9a\u4e49\u6a21\u578b\u6301\u7eed\u5931\u8d25\u95ee\u9898\u4e3a\u5f31\u70b9\u2192\u63d0\u53d6\u6838\u5fc3\u6982\u5ff5\u2192\u5408\u6210\u589e\u5f3a\u95ee\u9898\u2192\u901a\u8fc7\u5f31\u70b9\u805a\u7126\u7684\u589e\u91cf\u8bad\u7ec3\u9010\u6b65\u514b\u670d\u7f3a\u9677", "result": "\u57287B/32B\u6a21\u578b\u4e0a\u5b9e\u73b0\u5e73\u574710.0%/7.7%\u6027\u80fd\u63d0\u5347\uff0c\u8986\u76d68\u4e2a\u4e3b\u6d41\u63a8\u7406\u57fa\u51c6", "conclusion": "\u65e0\u9700\u5916\u90e8\u77e5\u8bc6\u84b8\u998f\u7684\u81ea\u6211\u5f31\u70b9\u8bc6\u522b\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u81ea\u4e3b\u5f3a\u5316\u8584\u5f31\u73af\u8282\uff0c\u5b9e\u73b0\u66f4\u4f18\u6cdb\u5316\u80fd\u529b"}}
{"id": "2506.09026", "pdf": "https://arxiv.org/pdf/2506.09026", "abs": "https://arxiv.org/abs/2506.09026", "authors": ["Amrith Setlur", "Matthew Y. R. Yang", "Charlie Snell", "Jeremy Greer", "Ian Wu", "Virginia Smith", "Max Simchowitz", "Aviral Kumar"], "title": "e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Test-time scaling offers a promising path to improve LLM reasoning by\nutilizing more compute at inference time; however, the true promise of this\nparadigm lies in extrapolation (i.e., improvement in performance on hard\nproblems as LLMs keep \"thinking\" for longer, beyond the maximum token budget\nthey were trained on). Surprisingly, we find that most existing reasoning\nmodels do not extrapolate well. We show that one way to enable extrapolation is\nby training the LLM to perform in-context exploration: training the LLM to\neffectively spend its test time budget by chaining operations (such as\ngeneration, verification, refinement, etc.), or testing multiple hypotheses\nbefore it commits to an answer. To enable in-context exploration, we identify\nthree key ingredients as part of our recipe e3: (1) chaining skills that the\nbase LLM has asymmetric competence in, e.g., chaining verification (easy) with\ngeneration (hard), as a way to implement in-context search; (2) leveraging\n\"negative\" gradients from incorrect traces to amplify exploration during RL,\nresulting in longer search traces that chains additional asymmetries; and (3)\ncoupling task difficulty with training token budget during training via a\nspecifically-designed curriculum to structure in-context exploration. Our\nrecipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25\nscores, and extrapolates to 2x the training token budget. Our e3-1.7B model not\nonly attains high pass@1 scores, but also improves pass@k over the base model.", "AI": {"tldr": "\u901a\u8fc7\u6d4b\u8bd5\u65f6\u6269\u5c55\uff08e3\u65b9\u6cd5\uff09\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u73b0\u8bad\u7ec3\u9884\u7b97\u5916\u53cc\u500d\u63a8\u7406\u65f6\u957f\u4e0b\u7684\u6027\u80fd\u5916\u63a8", "motivation": "\u73b0\u6709\u63a8\u7406\u6a21\u578b\u5728\u8d85\u51fa\u8bad\u7ec3\u65f6\u6700\u5927token\u9884\u7b97\u7684\u957f\u65f6\u95f4\u63a8\u7406\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u63a2\u7d22\u66f4\u6709\u6548\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u65b9\u5f0f", "method": "1) \u94fe\u5f0f\u6280\u80fd\u7ec4\u5408\uff08\u9a8c\u8bc1+\u751f\u6210\u4e0d\u5bf9\u79f0\u80fd\u529b\uff09 2) \u57fa\u4e8e\u9519\u8bef\u8f68\u8ff9\u7684\u8d1f\u68af\u5ea6\u5f3a\u5316\u5b66\u4e60 3) \u4efb\u52a1\u96be\u5ea6\u4e0e\u8bad\u7ec3token\u9884\u7b97\u5339\u914d\u7684\u8bfe\u7a0b\u5b66\u4e60", "result": "1.7B\u6a21\u578b\u5728AIME/HMMT\u57fa\u51c6\u53d6\u5f97\u6700\u4f73\u6210\u7ee9\uff0cpass@k\u6307\u6807\u63d0\u5347\uff0c\u652f\u6301\u53cc\u500d\u8bad\u7ec3token\u9884\u7b97\u5916\u63a8", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u63a2\u7d22\u673a\u5236\uff0c\u6210\u529f\u7a81\u7834LLM\u63a8\u7406\u65f6\u957f\u9650\u5236\uff0c\u4e3a\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2506.09040", "pdf": "https://arxiv.org/pdf/2506.09040", "abs": "https://arxiv.org/abs/2506.09040", "authors": ["Dianyi Wang", "Wei Song", "Yikun Wang", "Siyuan Wang", "Kaicheng Yu", "Zhongyu Wei", "Jiaqi Wang"], "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Typical large vision-language models (LVLMs) apply autoregressive supervision\nsolely to textual sequences, without fully incorporating the visual modality\ninto the learning process. This results in three key limitations: (1) an\ninability to utilize images without accompanying captions, (2) the risk that\ncaptions omit critical visual details, and (3) the challenge that certain\nvision-centric content cannot be adequately conveyed through text. As a result,\ncurrent LVLMs often prioritize vision-to-language alignment while potentially\noverlooking fine-grained visual information. While some prior works have\nexplored autoregressive image generation, effectively leveraging autoregressive\nvisual supervision to enhance image understanding remains an open challenge. In\nthis paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),\nwhich enables joint learning of visual and textual modalities within a unified\nautoregressive framework. We show that autoregressively reconstructing the raw\nvisual appearance of images does not enhance and may even impair multimodal\nunderstanding. In contrast, autoregressively reconstructing the semantic\nrepresentation of images consistently improves comprehension. Notably, we find\nthat even when models are given continuous image features as input, they can\neffectively reconstruct discrete semantic tokens, resulting in stable and\nconsistent improvements across a wide range of multimodal understanding\nbenchmarks. Our approach delivers significant performance gains across varying\ndata scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves\nLLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is\navailable at https://github.com/AlenjandroWang/ASVR.", "AI": {"tldr": "\u63d0\u51faASVR\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u8bed\u4e49\u89c6\u89c9\u91cd\u5efa\u7edf\u4e00\u5b66\u4e60\u591a\u6a21\u6001\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u89c6\u89c9\u4fe1\u606f\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ec5\u5bf9\u6587\u672c\u5e8f\u5217\u81ea\u56de\u5f52\u76d1\u7763\uff0c\u5bfc\u81f4\u65e0\u6cd5\u6709\u6548\u5229\u7528\u65e0\u6807\u6ce8\u56fe\u50cf\u3001\u9057\u6f0f\u89c6\u89c9\u7ec6\u8282\u3001\u96be\u4ee5\u8868\u8fbe\u89c6\u89c9\u4e2d\u5fc3\u5185\u5bb9\u3002", "method": "ASVR\u6846\u67b6\u8054\u5408\u5b66\u4e60\u89c6\u89c9\u4e0e\u6587\u672c\u6a21\u6001\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u91cd\u5efa\u56fe\u50cf\u8bed\u4e49\u8868\u5f81\uff08\u800c\u975e\u539f\u59cb\u50cf\u7d20\uff09\uff0c\u4f7f\u7528\u79bb\u6563\u8bed\u4e49token\u5b9e\u73b0\u7a33\u5b9a\u4f18\u5316\u3002", "result": "\u572814\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u5347LLaVA-1.5\u6a21\u578b5%\u6027\u80fd\uff0c\u4e0d\u540c\u6570\u636e\u89c4\u6a21(556k-2M)\u548cLLM\u9aa8\u5e72\u5747\u8868\u73b0\u7a33\u5b9a\u589e\u76ca\u3002", "conclusion": "\u8bed\u4e49\u91cd\u5efa\u6bd4\u539f\u59cb\u89c6\u89c9\u91cd\u5efa\u66f4\u6709\u6548\uff0c\u8fde\u7eed\u7279\u5f81\u53ef\u8f6c\u5316\u4e3a\u79bb\u6563\u8bed\u4e49token\uff0c\u8be5\u65b9\u6cd5\u4e3a\u591a\u6a21\u6001\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
