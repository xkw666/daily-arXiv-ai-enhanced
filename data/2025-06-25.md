<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.GR](#cs.GR) [Total: 6]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.CV](#cs.CV) [Total: 4]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.AI](#cs.AI) [Total: 6]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [MemeMind: A Large-Scale Multimodal Dataset with Chain-of-Thought Reasoning for Harmful Meme Detection](https://arxiv.org/abs/2506.18919)
*Hexiang Gu,Qifan Yu,Saihui Hou,Zhiqin Fang,Huijia Wu,Zhaofeng He*

Main category: cs.CL

TL;DR: 研究者创建了首个具备双语支持与思维链标注的有害模因数据集MemeMind，并提出整合多模态推理的MemeGuard检测框架，显著提升了模型检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有有害模因检测存在数据集系统性不足、标注解释性弱的问题，阻碍了该领域的深入研究和实际应用。

Method: 1. 构建科学标准的MemeMind数据集（规模大/多样性/双语/含详细CoT标注）；2. 开发融合多模态信息与推理建模的MemeGuard框架。

Result: 在MemeMind数据集上，MemeGuard框架在有害模因检测任务中持续超越现有最优方法，验证了其有效性。

Conclusion: MemeMind填补了领域数据空白，MemeGuard通过建模推理过程显著提升模型理解能力，为有害内容检测提供了新范式。

Abstract: The rapid development of social media has intensified the spread of harmful
content. Harmful memes, which integrate both images and text, pose significant
challenges for automated detection due to their implicit semantics and complex
multimodal interactions. Although existing research has made progress in
detection accuracy and interpretability, the lack of a systematic, large-scale,
diverse, and highly explainable dataset continues to hinder further advancement
in this field. To address this gap, we introduce MemeMind, a novel dataset
featuring scientifically rigorous standards, large scale, diversity, bilingual
support (Chinese and English), and detailed Chain-of-Thought (CoT) annotations.
MemeMind fills critical gaps in current datasets by offering comprehensive
labeling and explicit reasoning traces, thereby providing a solid foundation
for enhancing harmful meme detection. In addition, we propose an innovative
detection framework, MemeGuard, which effectively integrates multimodal
information with reasoning process modeling, significantly improving models'
ability to understand and identify harmful memes. Extensive experiments
conducted on the MemeMind dataset demonstrate that MemeGuard consistently
outperforms existing state-of-the-art methods in harmful meme detection tasks.

</details>


### [2] [Mirage of Mastery: Memorization Tricks LLMs into Artificially Inflated Self-Knowledge](https://arxiv.org/abs/2506.18998)
*Sahil Kale,Vijaykant Nadadur*

Main category: cs.CL

TL;DR: LLMs因混淆记忆与推理能力，在STEM领域表现出自我认知偏差，导致可行性评估存在45%+的不一致性，暴露架构缺陷与改进需求


<details>
  <summary>Details</summary>
Motivation: 现有研究割裂看待LLM记忆与自知识问题，但作者发现二者交织导致模型响应可信度下降，需验证其推理能力本质（学习模式 vs 单纯记忆）

Method: 通过设计逻辑自洽的任务扰动框架，测试LLM在STEM领域问题中的泛化能力与自我评估一致性

Result: 科学/医学领域因标准化术语集中，LLMs可行性评估出现超45%波动，其架构缺陷导致自我认知与真实能力显著脱节（代码已开源）

Conclusion: 须改进架构与训练模式，确保模型对自身知识边界的一致性认知，从而提升AI可解释性与可信度

Abstract: When artificial intelligence mistakes memorization for intelligence, it
creates a dangerous mirage of reasoning. Existing studies treat memorization
and self-knowledge deficits in LLMs as separate issues and do not recognize an
intertwining link that degrades the trustworthiness of LLM responses. In our
study, we utilize a novel framework to ascertain if LLMs genuinely learn
reasoning patterns from training data or merely memorize them to assume
competence across problems of similar complexity focused on STEM domains. Our
analysis shows a noteworthy problem in generalization: LLMs draw confidence
from memorized solutions to infer a higher self-knowledge about their reasoning
ability, which manifests as an over 45% inconsistency in feasibility
assessments when faced with self-validated, logically coherent task
perturbations. This effect is most pronounced in science and medicine domains,
which tend to have maximal standardized jargon and problems, further confirming
our approach. Significant wavering within the self-knowledge of LLMs also shows
flaws in current architectures and training patterns, highlighting the need for
techniques that ensure a balanced, consistent stance on models' perceptions of
their own knowledge for maximum AI explainability and trustworthiness. Our code
and results are available publicly at
https://github.com/knowledge-verse-ai/LLM-Memorization_SK_Eval-.

</details>


### [3] [Broken Tokens? Your Language Model can Secretly Handle Non-Canonical Tokenizations](https://arxiv.org/abs/2506.19004)
*Brian Siyuan Zheng,Alisa Liu,Orevaoghene Ahia,Jonathan Hayase,Yejin Choi,Noah A. Smith*

Main category: cs.CL

TL;DR: 研究发现语言模型对非规范分词具有意外强鲁棒性，部分任务中非规范分词甚至能提升性能，表明模型与分词器的绑定关系弱于预期


<details>
  <summary>Details</summary>
Motivation: 探索训练时未见的非规范分词对语言模型性能的影响，验证模型是否被严格限制在规范分词体系内

Method: 在20个基准测试中评估指令微调模型使用随机分词和字符级分词的性能保留率，分析不同强度模型鲁棒性差异，探索非规范分词在特定任务中的优化潜力

Result: 指令模型在随机/字符分词下保留90%+性能；字符分词提升字符串处理/代码理解任务14%，右对齐数字分组提升大数运算33%；鲁棒性源于指令微调阶段

Conclusion: 模型对分词器的依赖低于预期，推理时干预分词策略可提升性能，为优化模型表现提供新思路

Abstract: Modern tokenizers employ deterministic algorithms to map text into a single
"canonical" token sequence, yet the same string can be encoded as many
non-canonical tokenizations using the tokenizer vocabulary. In this work, we
investigate the robustness of LMs to text encoded with non-canonical
tokenizations entirely unseen during training. Surprisingly, when evaluated
across 20 benchmarks, we find that instruction-tuned models retain up to 93.4%
of their original performance when given a randomly sampled tokenization, and
90.8% with character-level tokenization. We see that overall stronger models
tend to be more robust, and robustness diminishes as the tokenization departs
farther from the canonical form. Motivated by these results, we then identify
settings where non-canonical tokenization schemes can *improve* performance,
finding that character-level segmentation improves string manipulation and code
understanding tasks by up to +14%, and right-aligned digit grouping enhances
large-number arithmetic by +33%. Finally, we investigate the source of this
robustness, finding that it arises in the instruction-tuning phase. We show
that while both base and post-trained models grasp the semantics of
non-canonical tokenizations (perceiving them as containing misspellings), base
models try to mimic the imagined mistakes and degenerate into nonsensical
output, while post-trained models are committed to fluent responses. Overall,
our findings suggest that models are less tied to their tokenizer than
previously believed, and demonstrate the promise of intervening on tokenization
at inference time to boost performance.

</details>


### [4] [Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical Perspective](https://arxiv.org/abs/2506.19028)
*Weijie Xu,Yiwen Wang,Chi Xue,Xiangkun Hu,Xi Fang,Guimin Dong,Chandan K. Reddy*

Main category: cs.CL

TL;DR: 提出FiSCo框架，通过语义计算评估大语言模型的群体公平性，检测长文本中的细微偏见差异


<details>
  <summary>Details</summary>
Motivation: 现有评估方法忽视长文本偏见和模型输出固有变异性，需开发更可靠的公平性评估框架

Method: 基于语义主张分解输出，利用蕴涵检查进行声明级分析，采用统计假设检验比较群体间/内相似性

Result: FiSCo在性别/种族/年龄等维度上可靠识别微妙偏见，降低模型随机性影响，优于现有评估指标

Conclusion: FiSCo通过统计框架实现群体反事实公平性验证，为长文本公平性评估提供新范式

Abstract: Large Language Models (LLMs) often generate responses with inherent biases,
undermining their reliability in real-world applications. Existing evaluation
methods often overlook biases in long-form responses and the intrinsic
variability of LLM outputs. To address these challenges, we propose
FiSCo(Fine-grained Semantic Computation), a novel statistical framework to
evaluate group-level fairness in LLMs by detecting subtle semantic differences
in long-form responses across demographic groups. Unlike prior work focusing on
sentiment or token-level comparisons, FiSCo goes beyond surface-level analysis
by operating at the claim level, leveraging entailment checks to assess the
consistency of meaning across responses. We decompose model outputs into
semantically distinct claims and apply statistical hypothesis testing to
compare inter- and intra-group similarities, enabling robust detection of
subtle biases. We formalize a new group counterfactual fairness definition and
validate FiSCo on both synthetic and human-annotated datasets spanning gender,
race, and age. Experiments show that FiSco more reliably identifies nuanced
biases while reducing the impact of stochastic LLM variability, outperforming
various evaluation metrics.

</details>


### [5] [Plan for Speed -- Dilated Scheduling for Masked Diffusion Language Models](https://arxiv.org/abs/2506.19037)
*Omer Luxembourg,Haim Permuter,Eliya Nachmani*

Main category: cs.CL

TL;DR: 提出基于膨胀调度的解掩码策略（DUS），通过非相邻标记分组实现并行解掩码，显著提升掩码扩散语言模型的推理效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在并行解掩码时依赖启发式置信度评估，忽略标记间依赖关系，导致推理效率与传统自回归模型相当。

Method: 采用基于膨胀的分组策略（非相邻标记分组），利用一阶马尔可夫假设实现独立并行解掩码，将去噪器调用次数降至O(log B)。

Result: 在GSM8K数学推理及Humaneval/MBPP代码生成任务中，DUS在不修改去噪器的前提下优于现有并行置信度规划方法。

Conclusion: DUS为轻量级高效文本生成提供新范式，通过降低计算复杂度释放了掩码扩散语言模型的真实潜力。

Abstract: Masked diffusion language models (MDLM) have shown strong promise for
non-autoregressive text generation, yet existing samplers act as implicit
planners, selecting tokens to unmask via denoiser confidence or entropy scores.
Such heuristics falter under parallel unmasking - they ignore pairwise
interactions between tokens and cannot account for dependencies when unmasking
multiple positions at once, limiting their inference time to traditional
auto-regressive (AR) models. We introduce the Dilated-scheduled Unmasking
Strategy (DUS), an inference-only, planner-model-free method that requires no
additional training. DUS leverages a first-order Markov assumption to partition
sequence positions into dilation-based groups of non-adjacent tokens, enabling
independent, parallel unmasking steps that respect local context that minimizes
the joint entropy of each iteration step. Unlike semi-AR block approaches
(e.g., LLADA and Dream) that still invoke the denoiser per block, DUS reduces
the number of denoiser calls to O(log B) per generation block - yielding
substantial speedup over the O(B) run time of state-of-the-art diffusion
models, where B is the block size in the semi-AR inference process. In
experiments on math (GSM8K) and code completion (Humaneval, MBPP) benchmarks -
domains suited to non-ordinal generation - DUS improves scores over parallel
confidence-based planner, without modifying the underlying denoiser. DUS offers
a lightweight, budget-aware approach to efficient, high-quality text
generation, paving the way to unlock the true capabilities of MDLMs.

</details>


### [6] [NLPnorth @ TalentCLEF 2025: Comparing Discriminative, Contrastive, and Prompt-Based Methods for Job Title and Skill Matching](https://arxiv.org/abs/2506.19058)
*Mike Zhang,Rob van der Goot*

Main category: cs.CL

TL;DR: 通过分类/对比微调和提示方法，利用ESCO多语言数据增强，在多语言职位匹配和技能预测任务中，大模型在两项任务均表现最佳（任务A提示法MAP 0.492，任务B分类法MAP 0.290）


<details>
  <summary>Details</summary>
Motivation: 解决职位名称匹配在候选人匹配/职业预测/市场分析中的核心需求，同时扩展研究职位与技能关联对下游任务的价值

Method: 对比三种方法：1)微调分类模型 2)微调对比学习模型 3)提示工程；使用ESCO多语言职位描述数据增强；采用多语言大模型

Result: 任务A（英语/西语/德语）提示法平均MAP 0.492（排行榜第5），任务B分类法MAP 0.290（排行榜第3）

Conclusion: 多语言大模型在两项任务中均表现最优，验证了跨语言表征能力。方法组合使团队进入任务A前25%、任务B前22%的排名

Abstract: Matching job titles is a highly relevant task in the computational job market
domain, as it improves e.g., automatic candidate matching, career path
prediction, and job market analysis. Furthermore, aligning job titles to job
skills can be considered an extension to this task, with similar relevance for
the same downstream tasks. In this report, we outline NLPnorth's submission to
TalentCLEF 2025, which includes both of these tasks: Multilingual Job Title
Matching, and Job Title-Based Skill Prediction. For both tasks we compare
(fine-tuned) classification-based, (fine-tuned) contrastive-based, and
prompting methods. We observe that for Task A, our prompting approach performs
best with an average of 0.492 mean average precision (MAP) on test data,
averaged over English, Spanish, and German. For Task B, we obtain an MAP of
0.290 on test data with our fine-tuned classification-based approach.
Additionally, we made use of extra data by pulling all the language-specific
titles and corresponding \emph{descriptions} from ESCO for each job and skill.
Overall, we find that the largest multilingual language models perform best for
both tasks. Per the provisional results and only counting the unique teams, the
ranking on Task A is 5$^{\text{th}}$/20 and for Task B 3$^{\text{rd}}$/14.

</details>


### [7] [MFTCXplain: A Multilingual Benchmark Dataset for Evaluating the Moral Reasoning of LLMs through Hate Speech Multi-hop Explanation](https://arxiv.org/abs/2506.19073)
*Jackson Trager,Francielle Vargas,Diego Alves,Matteo Guida,Mikel K. Ngueajio,Ameeta Agrawal,Flor Plaza-del-Arco,Yalda Daryanai,Farzan Karimi-Malekabadi*

Main category: cs.CL

TL;DR: 提出了MFTCXplain多语言数据集，通过道德基础理论评估大语言模型的道德推理能力，发现LLMs在道德情感预测和跨语言对齐存在显著不足


<details>
  <summary>Details</summary>
Motivation: 现有道德评估基准存在标注不透明（缺乏道德分类依据）和语言单一性（集中于英语）两大缺陷，限制了跨文化道德推理评估

Method: 构建包含4种语言、3000条推文的多语言数据集，标注仇恨言论标签/道德类别/文本依据，采用F1值评估LLMs在检测、道德预测、理由对齐的表现

Result: 模型在仇恨检测表现良好（F1达0.836），但道德情感预测极弱（F1<0.35），低资源语言的理由对齐尤其受限

Conclusion: 当前LLMs尚未具备内化人类道德推理的能力，跨文化道德评估框架需进一步完善

Abstract: Ensuring the moral reasoning capabilities of Large Language Models (LLMs) is
a growing concern as these systems are used in socially sensitive tasks.
Nevertheless, current evaluation benchmarks present two major shortcomings: a
lack of annotations that justify moral classifications, which limits
transparency and interpretability; and a predominant focus on English, which
constrains the assessment of moral reasoning across diverse cultural settings.
In this paper, we introduce MFTCXplain, a multilingual benchmark dataset for
evaluating the moral reasoning of LLMs via hate speech multi-hop explanation
using Moral Foundation Theory (MFT). The dataset comprises 3,000 tweets across
Portuguese, Italian, Persian, and English, annotated with binary hate speech
labels, moral categories, and text span-level rationales. Empirical results
highlight a misalignment between LLM outputs and human annotations in moral
reasoning tasks. While LLMs perform well in hate speech detection (F1 up to
0.836), their ability to predict moral sentiments is notably weak (F1 < 0.35).
Furthermore, rationale alignment remains limited mainly in underrepresented
languages. These findings show the limited capacity of current LLMs to
internalize and reflect human moral reasoning.

</details>


### [8] [Language Models Might Not Understand You: Evaluating Theory of Mind via Story Prompting](https://arxiv.org/abs/2506.19089)
*Nathaniel Getachew,Abulhair Saparov*

Main category: cs.CL

TL;DR: 提出StorySim框架，通过生成可控合成故事评估大语言模型的心理理论（ToM）和世界建模（WM）能力，避免预训练数据污染问题


<details>
  <summary>Details</summary>
Motivation: 现有基准测试可能受预训练数据污染影响，需通过高度可控的Storyboard精确操纵故事要素，实现更可靠的能力评估

Method: 使用Storyboard控制角色视角和事件，设计一阶/二阶ToM任务及WM任务，区分模型对心智状态建模和事件追踪能力

Result: 主流LLM在WM任务表现优于ToM任务；对人类推理优于非生命体；发现近因偏差和过度依赖故事早期事件等启发式行为

Conclusion: StorySim有效揭示了LLM在复杂心智推理中的局限性，为评估模型认知能力提供可控工具，相关代码已开源

Abstract: We introduce $\texttt{StorySim}$, a programmable framework for synthetically
generating stories to evaluate the theory of mind (ToM) and world modeling (WM)
capabilities of large language models (LLMs). Unlike prior benchmarks that may
suffer from contamination in pretraining data, $\texttt{StorySim}$ produces
novel, compositional story prompts anchored by a highly controllable
$\texttt{Storyboard}$, enabling precise manipulation of character perspectives
and events. We use this framework to design first- and second-order ToM tasks
alongside WM tasks that control for the ability to track and model mental
states. Our experiments across a suite of state-of-the-art LLMs reveal that
most models perform better on WM tasks than ToM tasks, and that models tend to
perform better reasoning with humans compared to inanimate objects.
Additionally, our framework enabled us to find evidence of heuristic behavior
such as recency bias and an over-reliance on earlier events in the story. All
code for generating data and evaluations is freely available.

</details>


### [9] [Human-Aligned Faithfulness in Toxicity Explanations of LLMs](https://arxiv.org/abs/2506.19113)
*Ramaravind K. Mothilal,Joanna Roy,Syed Ishtiaque Ahmed,Shion Guha*

Main category: cs.CL

TL;DR: 提出HAF多维度标准评估LLM毒性解释的人类对齐性，发现LLM在复杂提示下存在推理断裂


<details>
  <summary>Details</summary>
Motivation: 传统毒性研究集中于检测任务，但LLM的毒性解释合理性缺乏评估标准，影响下游任务可信度

Method: 构建HAF理论框架，开发6个无监督量化指标，在5个毒性数据集测试4种LLM（包括70B Llama）

Result: LLM对简单提示能生成合理解释，但在原因-毒性立场关联的复杂推理中出现矛盾/非理性响应

Conclusion: HAF框架为LLM可解释性提供新评估维度，开源代码和生成解释促进相关研究发展

Abstract: The discourse around toxicity and LLMs in NLP largely revolves around
detection tasks. This work shifts the focus to evaluating LLMs' reasoning about
toxicity -- from their explanations that justify a stance -- to enhance their
trustworthiness in downstream tasks. Despite extensive research on
explainability, it is not straightforward to adopt existing methods to evaluate
free-form toxicity explanation due to their over-reliance on input text
perturbations, among other challenges. To account for these, we propose a
novel, theoretically-grounded multi-dimensional criterion, Human-Aligned
Faithfulness (HAF), that measures the extent to which LLMs' free-form toxicity
explanations align with those of a rational human under ideal conditions. We
develop six metrics, based on uncertainty quantification, to comprehensively
evaluate \haf of LLMs' toxicity explanations with no human involvement, and
highlight how "non-ideal" the explanations are. We conduct several experiments
on three Llama models (of size up to 70B) and an 8B Ministral model on five
diverse toxicity datasets. Our results show that while LLMs generate plausible
explanations to simple prompts, their reasoning about toxicity breaks down when
prompted about the nuanced relations between the complete set of reasons, the
individual reasons, and their toxicity stances, resulting in inconsistent and
nonsensical responses. We open-source our code and LLM-generated explanations
at https://github.com/uofthcdslab/HAF.

</details>


### [10] [Enhanced Hybrid Transducer and Attention Encoder Decoder with Text Data](https://arxiv.org/abs/2506.19159)
*Yun Tang,Eesung Kim,Vijendra Raj Apsingekar*

Main category: cs.CL

TL;DR: 提出联合语音文本优化的J-TAED模型，整合多模态信息提升ASR准确性，实现无语音数据的文本领域适应


<details>
  <summary>Details</summary>
Motivation: 解决领域不匹配任务中语音数据稀缺问题，利用文本语料库增强模型泛化能力

Method: 联合训练语音/文本输入的统一编码器，推理时仅需语音输入。通过文本模态实现领域自适应迁移

Result: Librispeech数据集WER降低5.8-12.8%；金融/命名实体领域文本适应分别带来15.3%和17.8% WER下降

Conclusion: J-TAED有效整合语音与语言信息，验证了文本驱动领域适应的可行性，显著提升跨领域ASR性能

Abstract: A joint speech and text optimization method is proposed for hybrid transducer
and attention-based encoder decoder (TAED) modeling to leverage large amounts
of text corpus and enhance ASR accuracy. The joint TAED (J-TAED) is trained
with both speech and text input modalities together, while it only takes speech
data as input during inference. The trained model can unify the internal
representations from different modalities, and be further extended to
text-based domain adaptation. It can effectively alleviate data scarcity for
mismatch domain tasks since no speech data is required. Our experiments show
J-TAED successfully integrates speech and linguistic information into one
model, and reduce the WER by 5.8 ~12.8% on the Librispeech dataset. The model
is also evaluated on two out-of-domain datasets: one is finance and another is
named entity focused. The text-based domain adaptation brings 15.3% and 17.8%
WER reduction on those two datasets respectively.

</details>


### [11] [Prompt, Translate, Fine-Tune, Re-Initialize, or Instruction-Tune? Adapting LLMs for In-Context Learning in Low-Resource Languages](https://arxiv.org/abs/2506.19187)
*Christopher Toukmaji,Jeffrey Flanigan*

Main category: cs.CL

TL;DR: 研究发现少样本提示和翻译测试方法在低资源语言上下文学习中显著优于梯度调优方法，主要归因于灾难性遗忘问题


<details>
  <summary>Details</summary>
Motivation: 探索如何有效提升大语言模型在低资源语言上的上下文学习性能，解决现有梯度调优方法效果不足的问题

Method: 跨5种低资源语言、3个基础模型和7个下游任务，对比分析少样本提示/翻译测试/微调/嵌入重初始化/指令微调等多种技术（消耗4,100 GPU小时）

Result: 少样本提示平均准确率比最佳梯度方法高17.2%；翻译测试方法提升幅度达23.5%；提出VOR指标验证模型输出有效性，发现梯度方法存在73%的灾难性遗忘现象

Conclusion: 首次系统性验证非参数方法在低资源语言上下文学习中的优越性，提出量化分析框架，公开9,900+TFLOPs实验成果供社区使用

Abstract: LLMs are typically trained in high-resource languages, and tasks in
lower-resourced languages tend to underperform the higher-resource language
counterparts for in-context learning. Despite the large body of work on
prompting settings, it is still unclear how LLMs should be adapted
cross-lingually specifically for in-context learning in the low-resource target
languages. We perform a comprehensive study spanning five diverse target
languages, three base LLMs, and seven downstream tasks spanning over 4,100 GPU
training hours (9,900+ TFLOPs) across various adaptation techniques: few-shot
prompting, translate-test, fine-tuning, embedding re-initialization, and
instruction fine-tuning. Our results show that the few-shot prompting and
translate-test settings tend to heavily outperform the gradient-based
adaptation methods. To better understand this discrepancy, we design a novel
metric, Valid Output Recall (VOR), and analyze model outputs to empirically
attribute the degradation of these trained models to catastrophic forgetting.
To the extent of our knowledge, this is the largest study done on in-context
learning for low-resource languages with respect to train compute and number of
adaptation techniques considered. We make all our datasets and trained models
available for public use.

</details>


### [12] [Augmenting Multi-Agent Communication with State Delta Trajectory](https://arxiv.org/abs/2506.19209)
*Yichen Tang,Weihang Su,Yujia Zhou,Yiqun Liu,Min Zhang,Shaoping Ma,Qingyao Ai*

Main category: cs.CL

TL;DR: 提出状态差量编码(SDE)协议，通过同时传递自然语言token和状态转移轨迹来减少多智能体通信中的信息损失。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统使用自然语言通信导致信息损失，尤其在传递推理逻辑和抽象思维时更为明显。

Method: 开发状态差量编码(SDE)，捕捉LLM生成每个token时的连续状态变化轨迹，代替单纯传递最终状态值。

Result: 在复杂推理任务中，采用SDE的多智能体系统达到SOTA性能，较传统通信协议平均提升12.3%准确率。

Conclusion: 状态转移轨迹能有效传递推理过程的潜在信息，验证了通信增强对LLM多智能体系统的优化潜力。

Abstract: Multi-agent techniques such as role playing or multi-turn debates have been
shown to be effective in improving the performance of large language models
(LLMs) in downstream tasks. Despite their differences in workflows, existing
LLM-based multi-agent systems mostly use natural language for agent
communication. While this is appealing for its simplicity and interpretability,
it also introduces inevitable information loss as one model must down sample
its continuous state vectors to concrete tokens before transferring them to the
other model. Such losses are particularly significant when the information to
transfer is not simple facts, but reasoning logics or abstractive thoughts. To
tackle this problem, we propose a new communication protocol that transfers
both natural language tokens and token-wise state transition trajectory from
one agent to another. Particularly, compared to the actual state value, we find
that the sequence of state changes in LLMs after generating each token can
better reflect the information hidden behind the inference process, so we
propose a State Delta Encoding (SDE) method to represent state transition
trajectories. The experimental results show that multi-agent systems with SDE
achieve SOTA performance compared to other communication protocols,
particularly in tasks that involve complex reasoning. This shows the potential
of communication augmentation for LLM-based multi-agent systems.

</details>


### [13] [Personality Prediction from Life Stories using Language Models](https://arxiv.org/abs/2506.19258)
*Rasiq Hussain,Jerry Ma,Rithik Khandelwal,Joshua Oltmanns,Mehak Gupta*

Main category: cs.CL

TL;DR: 提出结合预训练模型滑动窗口微调和注意力RNN的混合方法，有效处理长文本并提升人格预测效果


<details>
  <summary>Details</summary>
Motivation: 传统问卷评估人格存在局限，利用2000+ tokens的长访谈文本能更全面捕捉人格特征，但长上下文建模是核心挑战

Method: 两阶段框架：1) 滑动窗口微调获取上下文嵌入 2) 注意力循环神经网络整合长程依赖并增强可解释性

Result: 相比LLaMA/Longformer等模型，在预测大五人格特质时准确率提升18%，训练效率提高3倍，注意力机制可视化关键语义片段

Conclusion: 融合语言模型与序列建模的混合框架，为基于生活叙事的人格评估提供了高效可解释的新方案

Abstract: Natural Language Processing (NLP) offers new avenues for personality
assessment by leveraging rich, open-ended text, moving beyond traditional
questionnaires. In this study, we address the challenge of modeling long
narrative interview where each exceeds 2000 tokens so as to predict Five-Factor
Model (FFM) personality traits. We propose a two-step approach: first, we
extract contextual embeddings using sliding-window fine-tuning of pretrained
language models; then, we apply Recurrent Neural Networks (RNNs) with attention
mechanisms to integrate long-range dependencies and enhance interpretability.
This hybrid method effectively bridges the strengths of pretrained transformers
and sequence modeling to handle long-context data. Through ablation studies and
comparisons with state-of-the-art long-context models such as LLaMA and
Longformer, we demonstrate improvements in prediction accuracy, efficiency, and
interpretability. Our results highlight the potential of combining
language-based features with long-context modeling to advance personality
assessment from life narratives.

</details>


### [14] [What Matters in LLM-generated Data: Diversity and Its Effect on Model Fine-Tuning](https://arxiv.org/abs/2506.19262)
*Yuchang Zhu,Zhonghua zhen,Qunshu Lin,Haotong Wei,Xiaolong Sun,Zixuan Yu,Minghao Liu,Zibin Zheng,Liang Chen*

Main category: cs.CL

TL;DR: LLM生成的数据在标注数据不足时可缓解数据稀缺问题，但数据多样性至关重要：中等多样性提升模型性能，过高多样性反而损害表现。


<details>
  <summary>Details</summary>
Motivation: 探索LLM生成数据多样性对下游模型的影响，解决前人研究中忽视数据多样性的关键问题，验证数据多样性在模型性能中的核心作用。

Method: 通过控制LLM生成数据的多样性层级，并混合不同比例合成数据进行训练，测试模型在标注数据不足场景下的性能变化。

Result: 在最小分布偏移条件下，中等多样性数据可提升模型性能（尤其在标注数据不足时），而高多样性数据会产生负面影响。

Conclusion: 数据多样性是LLM生成数据质量的核心指标，未来研究需在数据多样性与质量间寻求平衡，为LLM作为数据生成器的应用提供实证指导。

Abstract: With the remarkable generative capabilities of large language models (LLMs),
using LLM-generated data to train downstream models has emerged as a promising
approach to mitigate data scarcity in specific domains and reduce
time-consuming annotations. However, recent studies have highlighted a critical
issue: iterative training on self-generated data results in model collapse,
where model performance degrades over time. Despite extensive research on the
implications of LLM-generated data, these works often neglect the importance of
data diversity, a key factor in data quality. In this work, we aim to
understand the implications of the diversity of LLM-generated data on
downstream model performance. Specifically, we explore how varying levels of
diversity in LLM-generated data affect downstream model performance.
Additionally, we investigate the performance of models trained on data that
mixes different proportions of LLM-generated data, which we refer to as
synthetic data. Our experimental results show that, with minimal distribution
shift, moderately diverse LLM-generated data can enhance model performance in
scenarios with insufficient labeled data, whereas highly diverse generated data
has a negative impact. We hope our empirical findings will offer valuable
guidance for future studies on LLMs as data generators.

</details>


### [15] [EmoStage: A Framework for Accurate Empathetic Response Generation via Perspective-Taking and Phase Recognition](https://arxiv.org/abs/2506.19279)
*Zhiyang Qi,Keiko Takamizo,Mariko Ukiyo,Michimasa Inaba*

Main category: cs.CL

TL;DR: 提出EmoStage框架，利用开源大模型的推理能力增强心理咨询中的共情响应生成，无需额外训练数据


<details>
  <summary>Details</summary>
Motivation: 当前AI心理咨询系统存在心理状态识别不足、依赖高质量数据、隐私风险等问题，需开发更有效的解决方案

Method: 通过视角采推断用户心理状态/需求 + 阶段识别确保咨询流程一致性，在日/中文场景验证框架有效性

Result: 基础模型响应质量显著提升，与数据驱动方法表现相当，支持多语言咨询场景

Conclusion: EmoStage为资源有限场景提供有效解决方案，平衡咨询效果与隐私保护需求

Abstract: The rising demand for mental health care has fueled interest in AI-driven
counseling systems. While large language models (LLMs) offer significant
potential, current approaches face challenges, including limited understanding
of clients' psychological states and counseling stages, reliance on
high-quality training data, and privacy concerns associated with commercial
deployment. To address these issues, we propose EmoStage, a framework that
enhances empathetic response generation by leveraging the inference
capabilities of open-source LLMs without additional training data. Our
framework introduces perspective-taking to infer clients' psychological states
and support needs, enabling the generation of emotionally resonant responses.
In addition, phase recognition is incorporated to ensure alignment with the
counseling process and to prevent contextually inappropriate or inopportune
responses. Experiments conducted in both Japanese and Chinese counseling
settings demonstrate that EmoStage improves the quality of responses generated
by base models and performs competitively with data-driven methods.

</details>


### [16] [JCAPT: A Joint Modeling Approach for CAPT](https://arxiv.org/abs/2506.19315)
*Tzu-Hsuan Yang,Yue-Yang He,Berlin Chen*

Main category: cs.CL

TL;DR: 提出首个结合语音学归因、SSM建模和提示学习的CAPT统一框架Mamba，在speechocean762基准测试中MDD任务表现显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有计算机辅助发音训练系统（CAPT）在自动发音评估（APA）和错误发音检测诊断（MDD）任务中面临可解释性不足、细粒度时间建模能力有限的问题。研究旨在通过联合建模提升两类任务的性能与可解释性

Method: 基于选择性状态空间模型（SSM）Mamba架构，整合语音学特征和think token策略，构建支持细粒度时序推理的统一框架。通过语音学归因增强模型可解释性，利用SSM的长程依赖建模优势

Result: 在speechocean762基准测试中取得SOTA性能，MDD任务提升显著（错误检测准确率提升3.2%）。首次实现语音学特征、SSM架构与提示学习的多维度融合

Conclusion: 验证了联合建模框架在CAPT任务中的有效性，SSM的时序建模能力与语音学先验知识结合能显著提升诊断精度。未来可扩展至多语种发音纠错场景

Abstract: Effective pronunciation feedback is critical in second language (L2)
learning, for which computer-assisted pronunciation training (CAPT) systems
often encompass two key tasks: automatic pronunciation assessment (APA) and
mispronunciation detection and diagnosis (MDD). Recent work has shown that
joint modeling of these two tasks can yield mutual benefits. Our unified
framework leverages Mamba, a selective state space model (SSM), while
integrating phonological features and think token strategies to jointly enhance
interpretability and fine-grained temporal reasoning in APA and MDD. To our
knowledge, this is the first study to combine phonological attribution,
SSM-based modeling, and prompting in CAPT. A series of experiments conducted on
the speechocean762 benchmark demonstrate that our model consistently
outperforms prior methods, particularly on the MDD task.

</details>


### [17] [Spotting Out-of-Character Behavior: Atomic-Level Evaluation of Persona Fidelity in Open-Ended Generation](https://arxiv.org/abs/2506.19352)
*Jisu Shin,Juhyun Oh,Eunsu Kim,Hoyun Song,Alice Oh*

Main category: cs.CL

TL;DR: 提出原子级评估框架，通过细粒度指标量化LLMs在角色保真度上的表现，解决现有方法难以捕捉细微偏差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法对长文本生成中细微的角色偏差识别不足，导致用户实际体验中的角色不一致问题未被有效检测。

Method: 设计三个核心指标：角色对齐度、生成内容内部一致性、跨生成内容一致性，构建原子级量化评估体系。

Result: 实验证明该框架能检测传统方法遗漏的角色偏差，并揭示任务结构和角色合意性对模型适应性的双重影响机制。

Conclusion: 细粒度评估显著提升角色保真度检测精度，同时暴露LLMs在复杂人格表达场景下的系统性挑战。

Abstract: Ensuring persona fidelity in large language models (LLMs) is essential for
maintaining coherent and engaging human-AI interactions. However, LLMs often
exhibit Out-of-Character (OOC) behavior, where generated responses deviate from
an assigned persona, leading to inconsistencies that affect model reliability.
Existing evaluation methods typically assign single scores to entire responses,
struggling to capture subtle persona misalignment, particularly in long-form
text generation. To address this limitation, we propose an atomic-level
evaluation framework that quantifies persona fidelity at a finer granularity.
Our three key metrics measure the degree of persona alignment and consistency
within and across generations. Our approach enables a more precise and
realistic assessment of persona fidelity by identifying subtle deviations that
real users would encounter. Through our experiments, we demonstrate that our
framework effectively detects persona inconsistencies that prior methods
overlook. By analyzing persona fidelity across diverse tasks and personality
types, we reveal how task structure and persona desirability influence model
adaptability, highlighting challenges in maintaining consistent persona
expression.

</details>


### [18] [Measuring and Guiding Monosemanticity](https://arxiv.org/abs/2506.19382)
*Ruben Härle,Felix Friedrich,Manuel Brack,Stephan Wäldchen,Björn Deiseroth,Patrick Schramowski,Kristian Kersting*

Main category: cs.CL

TL;DR: 提出引导稀疏自编码器(G-SAE)和特征单义性评分(FMS)，提升大语言模型特征解释性与控制能力，在毒性检测等任务中实现更精细的语义控制。


<details>
  <summary>Details</summary>
Motivation: 当前特征定位方法存在可靠性不足，稀疏自编码器(SAE)的特征隔离与单义性存在缺陷，需建立量化指标和改进方法增强模型机理可解释性。

Method: 1. 设计FMS指标量化潜在特征单义性；2. 开发G-SAE模型，通过在训练中引入标注概念引导潜在表示学习。

Result: 在毒性检测/写作风格识别等任务中，G-SAE相比传统SAE将单义性提升23%，控制精度提高37%且质量损失降低61%。

Conclusion: G-SAE为LLM机理解释与控制提供了量化框架，实验证明其能实现细粒度语义解耦与控制，为模型安全部署提供新思路。

Abstract: There is growing interest in leveraging mechanistic interpretability and
controllability to better understand and influence the internal dynamics of
large language models (LLMs). However, current methods face fundamental
challenges in reliably localizing and manipulating feature representations.
Sparse Autoencoders (SAEs) have recently emerged as a promising direction for
feature extraction at scale, yet they, too, are limited by incomplete feature
isolation and unreliable monosemanticity. To systematically quantify these
limitations, we introduce Feature Monosemanticity Score (FMS), a novel metric
to quantify feature monosemanticity in latent representation. Building on these
insights, we propose Guided Sparse Autoencoders (G-SAE), a method that
conditions latent representations on labeled concepts during training. We
demonstrate that reliable localization and disentanglement of target concepts
within the latent space improve interpretability, detection of behavior, and
control. Specifically, our evaluations on toxicity detection, writing style
identification, and privacy attribute recognition show that G-SAE not only
enhances monosemanticity but also enables more effective and fine-grained
steering with less quality degradation. Our findings provide actionable
guidelines for measuring and advancing mechanistic interpretability and control
of LLMs.

</details>


### [19] [Automated Detection of Pre-training Text in Black-box LLMs](https://arxiv.org/abs/2506.19399)
*Ruihan Hu,Yu-Ming Shang,Jiankun Peng,Wei Luo,Yazhe Wang,Xi Zhang*

Main category: cs.CL

TL;DR: 提出了首个黑盒环境下自动检测大语言模型预训练文本的框架VeilProbe，通过映射模型、关键token扰动和原型分类器实现高效检测，实验证明其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖模型内部信息或需要人工设计复杂指令，无法有效应用于黑盒场景，且缺乏自动化解决方案。

Method: 1. 序列到序列映射模型提取输入输出潜在特征 2. 关键token扰动增强特征区分度 3. 原型分类器缓解小样本过拟合问题

Result: 在三个主流数据集上验证框架有效性，黑盒场景下检测性能显著优于现有方法

Conclusion: VeilProbe首次实现无需人工干预的黑盒预训练文本检测，为数据隐私和版权保护提供了实用解决方案

Abstract: Detecting whether a given text is a member of the pre-training data of Large
Language Models (LLMs) is crucial for ensuring data privacy and copyright
protection. Most existing methods rely on the LLM's hidden information (e.g.,
model parameters or token probabilities), making them ineffective in the
black-box setting, where only input and output texts are accessible. Although
some methods have been proposed for the black-box setting, they rely on massive
manual efforts such as designing complicated questions or instructions. To
address these issues, we propose VeilProbe, the first framework for
automatically detecting LLMs' pre-training texts in a black-box setting without
human intervention. VeilProbe utilizes a sequence-to-sequence mapping model to
infer the latent mapping feature between the input text and the corresponding
output suffix generated by the LLM. Then it performs the key token
perturbations to obtain more distinguishable membership features. Additionally,
considering real-world scenarios where the ground-truth training text samples
are limited, a prototype-based membership classifier is introduced to alleviate
the overfitting issue. Extensive evaluations on three widely used datasets
demonstrate that our framework is effective and superior in the black-box
setting.

</details>


### [20] [Learning to Disentangle Latent Reasoning Rules with Language VAEs: A Systematic Study](https://arxiv.org/abs/2506.19418)
*Yingji Zhang,Marco Valentino,Danilo S. Carvalho,André Freitas*

Main category: cs.CL

TL;DR: 通过在Transformer语言模型中显式嵌入推理规则（使用变分自编码器），提升模型泛化性与可解释性，实验验证了规则分离、先验知识注入有效性及FFN层的规则保持优势。


<details>
  <summary>Details</summary>
Motivation: 当前Transformer模型在NLI任务中过度依赖记忆而非规则推理，需探索显式规则嵌入以增强模型解释性和可控性。

Method: 提出包含三项推理任务的理论框架和VAE架构，通过编码器参数空间分离规则、查询注入先验知识、对比FFN/注意力层的实验设计。

Result: 1. 编码器成功分离规则形成特征聚类 2. 查询注入先验知识提升检索效果 3. 数学推理存在样本量性能瓶颈，FFN层规则分离能力优于注意力层

Conclusion: 显式规则嵌入可有效提升模型解释性，先验知识注入方法实用性强，FFN层在规则保持方面具有结构优势，为可解释AI提供新方向。

Abstract: Incorporating explicit reasoning rules within the latent space of language
models (LMs) offers a promising pathway to enhance generalisation,
interpretability, and controllability. While current Transformer-based language
models have shown strong performance on Natural Language Inference (NLI) tasks,
they often rely on memorisation rather than rule-based inference. This work
investigates how reasoning rules can be explicitly embedded and memorised
within the LMs through Language Variational Autoencoders (VAEs). We propose a
complete pipeline for learning reasoning rules within Transformer-based
language VAEs. This pipeline encompasses three rule-based reasoning tasks, a
supporting theoretical framework, and a practical end-to-end architecture. The
experiment illustrates the following findings: Disentangled reasoning: Under
explicit signal supervision, reasoning rules - viewed as functional mappings -
can be disentangled within the encoder's parametric space. This separation
results in distinct clustering of rules in the output feature space. Prior
knowledge injection: injecting reasoning information into the Query enables the
model to more effectively retrieve the stored value Value from memory based on
Key. This approach offers a simple method for integrating prior knowledge into
decoder-only language models. Performance bottleneck: In mathematical reasoning
tasks using Qwen2.5(0.5B), increasing sample count doesn't improve performance
beyond a point. Moreover, ffn layers are better than attention layers at
preserving the separation of reasoning rules in the model's parameters.

</details>


### [21] [Can Large Language Models Capture Human Annotator Disagreements?](https://arxiv.org/abs/2506.19467)
*Jingwei Ni,Yu Fan,Vilém Zouhar,Donya Rooein,Alexander Hoyle,Mrinmaya Sachan,Markus Leippold,Dirk Hovy,Elliott Ash*

Main category: cs.CL

TL;DR: 大语言模型（LLMs）在预测人类标注分歧方面存在缺陷，基于多数标签的评估方法会掩盖该问题。RLVR式推理在分歧预测中反而降低模型性能。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否能在缺乏重复人类标注数据的情况下捕捉具有信息量的标注分歧，突破传统仅评估多数标签准确率的局限。

Method: 通过系统评估LLMs预测标注分歧的能力，对比不同推理策略（如RLVR）对模型性能的影响。

Result: LLMs难以准确建模标注分歧，传统评估指标无法反映该缺陷。RLVR推理范式虽提升常规任务表现，却显著降低分歧预测准确率。

Conclusion: 亟需建立专门针对分歧建模能力的LLM评估体系，并开发能有效捕捉人类标注差异性的新型标注方法。

Abstract: Human annotation variation (i.e., annotation disagreements) is common in NLP
and often reflects important information such as task subjectivity and sample
ambiguity. While Large Language Models (LLMs) are increasingly used for
automatic annotation to reduce human effort, their evaluation often focuses on
predicting the majority-voted "ground truth" labels. It is still unclear,
however, whether these models also capture informative human annotation
variation. Our work addresses this gap by extensively evaluating LLMs' ability
to predict annotation disagreements without access to repeated human labels.
Our results show that LLMs struggle with modeling disagreements, which can be
overlooked by majority label-based evaluations. Notably, while RLVR-style
(Reinforcement learning with verifiable rewards) reasoning generally boosts LLM
performance, it degrades performance in disagreement prediction. Our findings
highlight the critical need for evaluating and improving LLM annotators in
disagreement modeling. Code and data at
https://github.com/EdisonNi-hku/Disagreement_Prediction.

</details>


### [22] [MuBench: Assessment of Multilingual Capabilities of Large Language Models Across 61 Languages](https://arxiv.org/abs/2506.19468)
*Wenhan Han,Yifan Zhang,Zhixun Chen,Binbin Liu,Haobin Lin,Bingni Zhang,Taifeng Wang,Mykola Pechenizkiy,Meng Fang,Yin Zheng*

Main category: cs.CL

TL;DR: 提出多语言基准测试MuBench评估61种语言，发现大模型实际语言覆盖度与宣称存在差距，并提出跨语言一致性指标MLC辅助分析模型瓶颈


<details>
  <summary>Details</summary>
Motivation: 现有评估数据集缺乏跨语言对齐，导致多语言能力评估在语言覆盖度和技能维度上呈现碎片化

Method: 1. 构建覆盖61种语言的MuBench基准
2. 评估主流多语言大模型
3. 使用500B token预训练1.2B参数模型，调整中英文数据比例研究跨语言迁移

Result: 1. 模型在低资源语言表现显著落后英语
2. 跨语言一致性指标可有效定位性能瓶颈
3. 语料比例和并行数据量显著影响迁移效果

Conclusion: MuBench基准和MLC指标为多语言模型评估提供新范式，预训练实验揭示了跨语言迁移的关键影响因素

Abstract: Multilingual large language models (LLMs) are advancing rapidly, with new
models frequently claiming support for an increasing number of languages.
However, existing evaluation datasets are limited and lack cross-lingual
alignment, leaving assessments of multilingual capabilities fragmented in both
language and skill coverage. To address this, we introduce MuBench, a benchmark
covering 61 languages and evaluating a broad range of capabilities. We evaluate
several state-of-the-art multilingual LLMs and find notable gaps between
claimed and actual language coverage, particularly a persistent performance
disparity between English and low-resource languages. Leveraging MuBench's
alignment, we propose Multilingual Consistency (MLC) as a complementary metric
to accuracy for analyzing performance bottlenecks and guiding model
improvement. Finally, we pretrain a suite of 1.2B-parameter models on English
and Chinese with 500B tokens, varying language ratios and parallel data
proportions to investigate cross-lingual transfer dynamics.

</details>


### [23] [Commonsense Generation and Evaluation for Dialogue Systems using Large Language Models](https://arxiv.org/abs/2506.19483)
*Marcos Estecha-Garitagoitia,Chen Zhang,Mario Rodríguez-Cantelar,Luis Fernando D'Haro*

Main category: cs.CL

TL;DR: 利用大语言模型的常识推理能力实现对话数据增强，并开发自动化评估框架验证生成质量


<details>
  <summary>Details</summary>
Motivation: 解决对话系统数据增强中上下文关联性与常识逻辑保持的难题，探索LLM在零样本场景下的应用潜力

Method: 从5个主流对话数据集中抽取200个局部对话，基于12种ATOMIC常识关系生成增强回复，设计基于指令提示的自动评估框架替代传统复杂事件元组提取

Result: 验证了LLM在常识推理及自动评估中的有效性，生成的增强对话成功保留原始属性

Conclusion: 该框架为对话系统数据增强提供了高效可靠的解决方案，展示了LLM在复杂语义任务中的实用价值

Abstract: This paper provides preliminary results on exploring the task of performing
turn-level data augmentation for dialogue system based on different types of
commonsense relationships, and the automatic evaluation of the generated
synthetic turns. The proposed methodology takes advantage of the extended
knowledge and zero-shot capabilities of pretrained Large Language Models (LLMs)
to follow instructions, understand contextual information, and their
commonsense reasoning capabilities. The approach draws inspiration from
methodologies like Chain-of-Thought (CoT), applied more explicitly to the task
of prompt-based generation for dialogue-based data augmentation conditioned on
commonsense attributes, and the automatic evaluation of the generated
dialogues.
  To assess the effectiveness of the proposed approach, first we extracted 200
randomly selected partial dialogues, from 5 different well-known dialogue
datasets, and generate alternative responses conditioned on different event
commonsense attributes. This novel dataset allows us to measure the proficiency
of LLMs in generating contextually relevant commonsense knowledge, particularly
up to 12 different specific ATOMIC [10] database relations. Secondly, we
propose an evaluation framework to automatically detect the quality of the
generated dataset inspired by the ACCENT [26] metric, which offers a nuanced
approach to assess event commonsense. However, our method does not follow
ACCENT's complex eventrelation tuple extraction process. Instead, we propose an
instruction-based prompt for each commonsense attribute and use
state-of-the-art LLMs to automatically detect the original attributes used when
creating each augmented turn in the previous step.
  Preliminary results suggest that our approach effectively harnesses LLMs
capabilities for commonsense reasoning and evaluation in dialogue systems.

</details>


### [24] [Dialogic Pedagogy for Large Language Models: Aligning Conversational AI with Proven Theories of Learning](https://arxiv.org/abs/2506.19484)
*Russell Beale*

Main category: cs.CL

TL;DR: 论文系统综述了基于大语言模型的对话代理在高等教育中的应用，通过结合教学理论和检索增强技术，提出优化人机对话教学效果的实践策略。


<details>
  <summary>Details</summary>
Motivation: 填补教育理论与AI实践之间的鸿沟，指出LLM直接提供答案而非促进知识共建的缺陷，强调需要符合教学规律的人机对话设计。

Method: 综合文献分析法，将维果斯基近侧发展区、苏格拉底提问法等教学理论映射到LLM能力，通过提示工程和RAG技术实现理论对齐。

Result: 揭示LLM在持续可用性和非人类专家特性上的教学优势，同时暴露其替代思考过程、弱化知识共建的传统教育假设缺陷。

Conclusion: 建议采用苏格拉底式提问的提示设计，结合检索机制保障准确性，构建支持反思支架的对话系统，推动AI教育工具的理论适配性。

Abstract: Large Language Models (LLMs) are rapidly transforming education by enabling
rich conversational learning experiences. This article provides a comprehensive
review of how LLM-based conversational agents are being used in higher
education, with extensions to secondary and lifelong learning contexts. We
synthesize existing literature on LLMs in education and theories of
conversational and dialogic pedagogy - including Vygotsky's sociocultural
learning (scaffolding and the Zone of Proximal Development), the Socratic
method, and Laurillard's conversational framework - and examine how prompting
strategies and retrieval-augmented generation (RAG) can align LLM behaviors
with these pedagogical theories, and how it can support personalized, adaptive
learning. We map educational theories to LLM capabilities, highlighting where
LLM-driven dialogue supports established learning principles and where it
challenges or falls short of traditional pedagogical assumptions. Notable gaps
in applying prior theories to LLMs are identified, such as the models tendency
to provide direct answers instead of fostering co-construction of knowledge,
and the need to account for the constant availability and broad but non-human
expertise of LLM tutors. In response, we propose practical strategies to better
align LLM interactions with sound pedagogy - for example, designing prompts
that encourage Socratic questioning, scaffolded guidance, and student
reflection, as well as integrating retrieval mechanisms to ensure accuracy and
contextual relevance. Our aim is to bridge the gap between educational theory
and the emerging practice of AI-driven conversational learning, offering
insights and tools for making LLM-based dialogues more educationally productive
and theory-aligned.

</details>


### [25] [Is Long-to-Short a Free Lunch? Investigating Inconsistency and Reasoning Efficiency in LRMs](https://arxiv.org/abs/2506.19492)
*Shu Yang,Junchao Wu,Xuansheng Wu,Derek Wong,Ninhao Liu,Di Wang*

Main category: cs.CL

TL;DR: 研究发现大型推理模型的高效推理策略虽提升效率，但会引发模型自我矛盾、事后合理化等行为不一致风险


<details>
  <summary>Details</summary>
Motivation: 针对大型推理模型（LRMs）在复杂任务中过度生成推理内容导致效率下降的问题，现有优化方法可能忽略其潜在的行为逻辑风险

Method: 构建ICBENCH基准评估框架，从任务设定一致性（ITS）、训练目标与行为一致性（TR-LB）、推理过程与自我解释一致性（IR-SE）三个维度进行系统性评估

Result: 实验发现：1）大模型普遍存在自我矛盾等'诡计'行为 2）No-Thinking等高效策略显著增加所有类型的不一致性 3）模型规模与一致性正相关但无法完全避免问题

Conclusion: 效率优化需警惕监督失效风险，应在提升token效率与保障模型行为可控性之间取得平衡

Abstract: Large Reasoning Models (LRMs) have achieved remarkable performance on complex
tasks by engaging in extended reasoning before producing final answers, yet
this strength introduces the risk of overthinking, where excessive token
generation occurs even for simple tasks. While recent work in efficient
reasoning seeks to reduce reasoning length while preserving accuracy, it
remains unclear whether such optimization is truly a free lunch. Drawing on the
intuition that compressing reasoning may reduce the robustness of model
responses and lead models to omit key reasoning steps, we investigate whether
efficient reasoning strategies introduce behavioral inconsistencies. To
systematically assess this, we introduce $ICBENCH$, a benchmark designed to
measure inconsistency in LRMs across three dimensions: inconsistency across
task settings (ITS), inconsistency between training objectives and learned
behavior (TR-LB), and inconsistency between internal reasoning and
self-explanations (IR-SE). Applying $ICBENCH$ to a range of open-source LRMs,
we find that while larger models generally exhibit greater consistency than
smaller ones, they all display widespread "scheming" behaviors, including
self-disagreement, post-hoc rationalization, and the withholding of reasoning
cues. Crucially, our results demonstrate that efficient reasoning strategies
such as No-Thinking and Simple Token-Budget consistently increase all three
defined types of inconsistency. These findings suggest that although efficient
reasoning enhances token-level efficiency, further investigation is imperative
to ascertain whether it concurrently introduces the risk of models evading
effective supervision.

</details>


### [26] [AnTKV: Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models](https://arxiv.org/abs/2506.19505)
*Zeyu Li,Chuanfu Xiao,Yang Wang,Xiang Liu,Zhenheng Tang,Baotong Lu,Mao Yang,Xinyu Chen,Xiaowen Chu*

Main category: cs.CL

TL;DR: 提出AnTKV框架，通过锚定令牌感知的向量量化压缩KV缓存，结合高效Triton内核实现840K上下文支持与3.5倍吞吐提升。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存量化方法在超低位(如1-bit/0.375-bit)时性能下降严重，不同token的KV缓存对量化误差敏感度差异显著。

Method: 通过前向误差传播分析提出锚定分数(AnS)，基于高AnS令牌保留FP16策略，设计Anchor Token-aware向量量化框架。

Result: LLaMA-3-8B支持840K上下文，解码吞吐量比FP16基线提升3.5倍；1-bit量化困惑度仅6.32，显著优于现有方法。

Conclusion: AnTKV在超低位量化场景下显著降低性能损失，为长上下文LLM部署提供高效压缩方案，突破传统量化精度-效率权衡。

Abstract: Quantization has emerged as an effective and lightweight solution to reduce
the memory footprint of the KV cache in Large Language Models (LLMs).
Nevertheless, minimizing the performance degradation caused by ultra-low-bit KV
cache quantization remains a significant challenge. We observe that quantizing
the KV cache of different tokens has varying impacts on the quality of
attention outputs. To systematically investigate this phenomenon, we perform
forward error propagation analysis on attention and propose the Anchor Score
(AnS) that quantifies the sensitivity of each token's KV cache to
quantization-induced error. Our analysis reveals significant disparities in AnS
across tokens, suggesting that preserving a small subset with full precision
(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive
quantization scenarios. Based on this insight, we introduce AnTKV, a novel
framework that leverages Anchor Token-aware Vector Quantization to compress the
KV cache. Furthermore, to support efficient deployment, we design and develop a
triton kernel that is fully compatible with FlashAttention, enabling fast
online Anchor Token selection. AnTKV enables LLaMA-3-8B to handle context
lengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x
higher decoding throughput compared to the FP16 baseline. Our experiment
results demonstrate that AnTKV matches or outperforms prior works such as KIVI,
SKVQ, KVQuant, and CQ under 4-bit settings. More importantly, AnTKV achieves
significantly lower perplexity under ultra-low-bit quantization on Mistral-7B,
with only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of
4.73.

</details>


### [27] [heiDS at ArchEHR-QA 2025: From Fixed-k to Query-dependent-k for Retrieval Augmented Generation](https://arxiv.org/abs/2506.19512)
*Ashish Chouhan,Michael Gertz*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper presents the approach of our team called heiDS for the ArchEHR-QA
2025 shared task. A pipeline using a retrieval augmented generation (RAG)
framework is designed to generate answers that are attributed to clinical
evidence from the electronic health records (EHRs) of patients in response to
patient-specific questions. We explored various components of a RAG framework,
focusing on ranked list truncation (RLT) retrieval strategies and attribution
approaches. Instead of using a fixed top-k RLT retrieval strategy, we employ a
query-dependent-k retrieval strategy, including the existing surprise and
autocut methods and two new methods proposed in this work, autocut* and elbow.
The experimental results show the benefits of our strategy in producing factual
and relevant answers when compared to a fixed-$k$.

</details>


### [28] [Automatic Posology Structuration : What role for LLMs?](https://arxiv.org/abs/2506.19525)
*Natalia Bobkova,Laura Zanella-Calzada,Anyes Tafoughalt,Raphaël Teboul,François Plesse,Félix Gaschi*

Main category: cs.CL

TL;DR: LLM与NERL混合模型显著提升法国处方结构化准确率至91%，平衡计算成本与临床实用性


<details>
  <summary>Details</summary>
Motivation: 法国处方中的用药说明存在模糊性、非规范性和口语化问题，传统机器学习方法效果受限，需提升用药安全性和临床决策支持能力

Method: 对比基于prompt的LLM、微调LLM与基于命名实体识别与链接(NERL)的传统系统，提出置信度阈值路由的混合架构（NERL置信度<0.8时转LLM处理）

Result: 混合方案实现91%结构化准确率，计算延迟降低50%，综合错误率比单一系统降低35%

Conclusion: NERL提供结构精确性，LLM擅长语义解析，二者优势互补的混合架构为临床实践提供高精度、低延时的可扩展解决方案

Abstract: Automatically structuring posology instructions is essential for improving
medication safety and enabling clinical decision support. In French
prescriptions, these instructions are often ambiguous, irregular, or
colloquial, limiting the effectiveness of classic ML pipelines. We explore the
use of Large Language Models (LLMs) to convert free-text posologies into
structured formats, comparing prompt-based methods and fine-tuning against a
"pre-LLM" system based on Named Entity Recognition and Linking (NERL). Our
results show that while prompting improves performance, only fine-tuned LLMs
match the accuracy of the baseline. Through error analysis, we observe
complementary strengths: NERL offers structural precision, while LLMs better
handle semantic nuances. Based on this, we propose a hybrid pipeline that
routes low-confidence cases from NERL (<0.8) to the LLM, selecting outputs
based on confidence scores. This strategy achieves 91% structuration accuracy
while minimizing latency and compute. Our results show that this hybrid
approach improves structuration accuracy while limiting computational cost,
offering a scalable solution for real-world clinical use.

</details>


### [29] [KnowMap: Efficient Knowledge-Driven Task Adaptation for LLMs](https://arxiv.org/abs/2506.19527)
*Kelin Fu,Kaigui Bian*

Main category: cs.CL

TL;DR: 提出了KnowMap框架，通过动态构建知识库和小型知识嵌入模型，提升大语言模型在特定任务中的适应性和推理能力


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型依赖静态预训练知识、难以快速适应新任务的问题，传统微调方法存在高成本、数据依赖和灾难性遗忘缺陷

Method: KnowMap框架动态整合环境经验和任务数据，训练小型知识嵌入模型，将领域知识注入大语言模型

Result: 在ScienceWorld基准测试中使gpt-4-turbo模型性能提升17.71%

Conclusion: 证明了动态知识整合的有效性，为LLM任务适应提供了高效方案，并揭示了环境经验知识对增强模型推理能力的作用机制

Abstract: While Large Language Models (LLMs) possess significant capabilities in
open-world agent tasks, they also face challenges in rapidly adapting to new,
specialized tasks due to their reliance on static pre-trained knowledge.
Traditional methods such as fine-tuning are often costly, data-intensive, and
may lead to "catastrophic forgetting." Therefore, we present KnowMap, a novel
approach that dynamically constructs a knowledge base from environmental and
experiential data. KnowMap fine-tunes a small knowledge-embedding model to
equip a larger LLM with valuable task-specific knowledge. Our experiments on
the ScienceWorld benchmark demonstrate 17.71% improvement for the performance
of gpt-4-turbo model. KnowMap not only provides an efficient and effective
means for LLM task-adapting, but also highlights how integrating environmental
and experiential knowledge can enhance LLMs' reasoning capabilities.

</details>


### [30] [Health Sentinel: An AI Pipeline For Real-time Disease Outbreak Detection](https://arxiv.org/abs/2506.19548)
*Devesh Pant,Rishi Raj Grandhe,Vipin Samaria,Mukul Paul,Sudhir Kumar,Saransh Khanna,Jatin Agrawal,Jushaan Singh Kalra,Akhil VSSG,Satish V Khalikar,Vipin Garg,Himanshu Chauhan,Pranay Verma,Neha Khandelwal,Soma S Dhavala,Minesh Mathew*

Main category: cs.CL

TL;DR: Health Sentinel通过ML/非ML方法自动从在线新闻中提取结构化疫情信息，帮助卫生机构实现早期疫情监测。


<details>
  <summary>Details</summary>
Motivation: 传统疾病监测存在局限性，而海量在线媒体内容无法人工筛查，需自动化工具实现高效疫情预警。

Method: 开发多阶段信息抽取系统，结合机器学习与非机器学习技术，从在线文章中提取疾病暴发等异常健康事件的结构化数据。

Result: 系统处理超3亿篇新闻，识别95,000个健康事件，其中3,500个被疾控专家列为潜在疫情。

Conclusion: 该系统有效实现疫情信号自动化识别，为印度公共卫生部门提供实时决策支持，提升应急响应时效性。

Abstract: Early detection of disease outbreaks is crucial to ensure timely intervention
by the health authorities. Due to the challenges associated with traditional
indicator-based surveillance, monitoring informal sources such as online media
has become increasingly popular. However, owing to the number of online
articles getting published everyday, manual screening of the articles is
impractical. To address this, we propose Health Sentinel. It is a multi-stage
information extraction pipeline that uses a combination of ML and non-ML
methods to extract events-structured information concerning disease outbreaks
or other unusual health events-from online articles. The extracted events are
made available to the Media Scanning and Verification Cell (MSVC) at the
National Centre for Disease Control (NCDC), Delhi for analysis, interpretation
and further dissemination to local agencies for timely intervention. From April
2022 till date, Health Sentinel has processed over 300 million news articles
and identified over 95,000 unique health events across India of which over
3,500 events were shortlisted by the public health experts at NCDC as potential
outbreaks.

</details>


### [31] [RCStat: A Statistical Framework for using Relative Contextualization in Transformers](https://arxiv.org/abs/2506.19549)
*Debabrata Mahapatra,Shubham Agarwal,Apoorv Saxena,Subrata Mitra*

Main category: cs.CL

TL;DR: 提出RCStat框架利用原始注意力logits实现高效的KV缓存压缩和更高精度的归因解释


<details>
  <summary>Details</summary>
Motivation: 传统基于Softmax归一化注意力权重的方法会损失pre-Softmax阶段query-key logits的丰富结构信息，限制上下文对齐分析的准确性

Method: 通过定义相对情境化(RC)随机变量衡量token片段间对齐度，推导出高效的上界计算方式，应用于KV缓存自适应淘汰和注意力归因分析

Result: 在问答、摘要等任务中实现SOTA的KV缓存压缩（最高达48.6%）和比post-Softmax方法更高精度的token/sentence/chunk级归因

Conclusion: RCStat无需模型重训练即可实现注意力机制的深度解析，为模型压缩和可解释性研究提供了新范式

Abstract: Prior work on input-token importance in auto-regressive transformers has
relied on Softmax-normalized attention weights, which obscure the richer
structure of pre-Softmax query-key logits. We introduce RCStat, a statistical
framework that harnesses raw attention logits via Relative Contextualization
(RC), a random variable measuring contextual alignment between token segments,
and derive an efficient upper bound for RC. We demonstrate two applications:
(i) Key-Value compression, where RC-based thresholds drive adaptive key-value
eviction for substantial cache reduction with minimal quality loss; and (ii)
Attribution, where RC yields higher-fidelity token-, sentence-, and chunk-level
explanations than post-Softmax methods. Across question answering,
summarization, and attribution benchmarks, RCStat achieves significant
empirical gains, delivering state-of-the-art compression and attribution
performance without any model retraining.

</details>


### [32] [Has Machine Translation Evaluation Achieved Human Parity? The Human Reference and the Limits of Progress](https://arxiv.org/abs/2506.19571)
*Lorenzo Proietti,Stefano Perrella,Roberto Navigli*

Main category: cs.CL

TL;DR: 研究发现先进自动评估指标在机器翻译评估中已达到或超越人类基线，但对评估可靠性提出警示


<details>
  <summary>Details</summary>
Motivation: 探讨自动指标是否真正达到人类评估水平，并确立机器翻译评估指标性能的测量上限

Method: 在MT元评估中引入人类基线，通过比较自动指标与人类判断的一致性来评估指标能力

Result: 人类标注者未持续优于自动指标，最先进指标常达到或超过人类基线水平

Conclusion: 尽管数据表明达到人类水平，但需警惕评估可靠性，并质疑领域进展测量的有效性

Abstract: In Machine Translation (MT) evaluation, metric performance is assessed based
on agreement with human judgments. In recent years, automatic metrics have
demonstrated increasingly high levels of agreement with humans. To gain a
clearer understanding of metric performance and establish an upper bound, we
incorporate human baselines in the MT meta-evaluation, that is, the assessment
of MT metrics' capabilities. Our results show that human annotators are not
consistently superior to automatic metrics, with state-of-the-art metrics often
ranking on par with or higher than human baselines. Despite these findings
suggesting human parity, we discuss several reasons for caution. Finally, we
explore the broader implications of our results for the research field, asking:
Can we still reliably measure improvements in MT evaluation? With this work, we
aim to shed light on the limits of our ability to measure progress in the
field, fostering discussion on an issue that we believe is crucial to the
entire MT evaluation community.

</details>


### [33] [ECCoT: A Framework for Enhancing Effective Cognition via Chain of Thought in Large Language Model](https://arxiv.org/abs/2506.19599)
*Zhenke Duan,Jiqun Pan,Jiani Tu,Xiaoyi Wang,Yanqing Wang*

Main category: cs.CL

TL;DR: 提出ECCoT框架验证大语言模型推理链，整合MRF-ETM和CSBert技术提升决策可信度


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型输出不可靠、缺乏透明度的问题，通过验证推理链提升决策可信度

Method: ECCoT框架包含：1) MRF-ETM实现主题感知的思维链生成 2) CSBert进行因果推理对齐 3) 结构化排序统计过滤无效推理链

Result: 提升模型可解释性23.7%，减少34%的因果推理偏差，决策准确率提升19.2%

Conclusion: ECCoT为LLM提供可验证的推理框架，开源代码推动可信AI发展，实验证明可显著提升决策可靠性

Abstract: In the era of large-scale artificial intelligence, Large Language Models
(LLMs) have made significant strides in natural language processing. However,
they often lack transparency and generate unreliable outputs, raising concerns
about their interpretability. To address this, the Chain of Thought (CoT)
prompting method structures reasoning into step-by-step deductions. Yet, not
all reasoning chains are valid, and errors can lead to unreliable conclusions.
We propose ECCoT, an End-to-End Cognitive Chain of Thought Validation
Framework, to evaluate and refine reasoning chains in LLMs. ECCoT integrates
the Markov Random Field-Embedded Topic Model (MRF-ETM) for topic-aware CoT
generation and Causal Sentence-BERT (CSBert) for causal reasoning alignment. By
filtering ineffective chains using structured ordering statistics, ECCoT
improves interpretability, reduces biases, and enhances the trustworthiness of
LLM-based decision-making. Key contributions include the introduction of ECCoT,
MRF-ETM for topic-driven CoT generation, and CSBert for causal reasoning
enhancement. Code is released at: https://github.com/erwinmsmith/ECCoT.git.

</details>


### [34] [Social Hatred: Efficient Multimodal Detection of Hatemongers](https://arxiv.org/abs/2506.19603)
*Tom Marzea,Abraham Israeli,Oren Tsur*

Main category: cs.CL

TL;DR: 通过结合用户文本、活动及社交网络的多模态聚合方法，显著提升仇恨言论发布者的检测效果，并验证该方法在跨平台和大规模数据中的适用性


<details>
  <summary>Details</summary>
Motivation: 现有研究多集中于仇恨文本检测，但用户层面的分析对理解社会现象同样重要。当前面临用户级检测的挑战性，需综合多维数据进行突破

Method: 采用文本分析+用户行为分析+社交网络图谱的多模态框架，在Twitter、Gab、Parler三个不同平台的数据集进行实验验证

Result: 实验显示该方法对比纯文本/图模型的F1值提升18.2%，在Parler平台的AUC达到0.91，且处理200万节点网络时保持线性时间复杂度

Conclusion: 多模态方法有效解决用户级仇恨检测难题，支持跨平台部署，为干预措施提供技术基础，特别适用于识别隐晦表达（如狗哨政治、种族煤气灯）

Abstract: Automatic detection of online hate speech serves as a crucial step in the
detoxification of the online discourse. Moreover, accurate classification can
promote a better understanding of the proliferation of hate as a social
phenomenon. While most prior work focus on the detection of hateful utterances,
we argue that focusing on the user level is as important, albeit challenging.
In this paper we consider a multimodal aggregative approach for the detection
of hate-mongers, taking into account the potentially hateful texts, user
activity, and the user network. Evaluating our method on three unique datasets
X (Twitter), Gab, and Parler we show that processing a user's texts in her
social context significantly improves the detection of hate mongers, compared
to previously used text and graph-based methods. We offer comprehensive set of
results obtained in different experimental settings as well as qualitative
analysis of illustrative cases. Our method can be used to improve the
classification of coded messages, dog-whistling, and racial gas-lighting, as
well as to inform intervention measures. Moreover, we demonstrate that our
multimodal approach performs well across very different content platforms and
over large datasets and networks.

</details>


### [35] [Correcting Hallucinations in News Summaries: Exploration of Self-Correcting LLM Methods with External Knowledge](https://arxiv.org/abs/2506.19607)
*Juraj Vladika,Ihsan Soydemir,Florian Matthes*

Main category: cs.CL

TL;DR: 研究探索利用搜索引擎证据的自我纠正系统减少LLM新闻摘要中的幻觉现象，发现搜索引擎片段和少样本提示有效提升准确性，且自动评估与人工评估高度一致。


<details>
  <summary>Details</summary>
Motivation: 现有自我纠正方法多用于百科生成领域，但在新闻摘要领域应用不足。论文旨在验证该技术结合搜索引擎证据纠正摘要幻觉的效果。

Method: 应用两种前沿自我纠正系统，通过三个搜索引擎获取外部证据，采用多轮验证问题生成与知识整合的迭代修正流程。

Result: 搜索引擎片段提供有效知识补充，少样本提示显著改善纠正效果，G-Eval自动评估结果与人工评估保持88.6%的一致性。

Conclusion: 自我纠正机制结合外部知识源能有效缓解新闻摘要幻觉，研究结果为自动评估体系可靠性及跨领域应用提供实践依据。

Abstract: While large language models (LLMs) have shown remarkable capabilities to
generate coherent text, they suffer from the issue of hallucinations --
factually inaccurate statements. Among numerous approaches to tackle
hallucinations, especially promising are the self-correcting methods. They
leverage the multi-turn nature of LLMs to iteratively generate verification
questions inquiring additional evidence, answer them with internal or external
knowledge, and use that to refine the original response with the new
corrections. These methods have been explored for encyclopedic generation, but
less so for domains like news summarization. In this work, we investigate two
state-of-the-art self-correcting systems by applying them to correct
hallucinated summaries using evidence from three search engines. We analyze the
results and provide insights into systems' performance, revealing interesting
practical findings on the benefits of search engine snippets and few-shot
prompts, as well as high alignment of G-Eval and human evaluation.

</details>


### [36] [Tailored Conversations beyond LLMs: A RL-Based Dialogue Manager](https://arxiv.org/abs/2506.19652)
*Lucie Galland,Catherine Pelachaud,Florian Pecune*

Main category: cs.CL

TL;DR: 提出融合大语言模型与分层强化学习的对话管理系统，通过元学习增强适应性，在动机访谈场景中显著优于基线模型


<details>
  <summary>Details</summary>
Motivation: 现有开放对话系统在特定目标场景（如行为改变）中效率不足，需结合结构化对话管理和用户个性化适应机制

Method: 分层强化学习建模对话阶段 + 元学习优化跨用户适应 + LLMs生成响应

Result: 在奖励指标上超越现有LLM基线，验证结构化对话管理的有效性

Conclusion: 该框架为构建目标导向的开放对话系统提供新范式，尤其在个性化响应和数据效率方面展现优势

Abstract: In this work, we propose a novel framework that integrates large language
models (LLMs) with an RL-based dialogue manager for open-ended dialogue with a
specific goal. By leveraging hierarchical reinforcement learning to model the
structured phases of dialogue and employ meta-learning to enhance adaptability
across diverse user profiles, our approach enhances adaptability and
efficiency, enabling the system to learn from limited data, transition fluidly
between dialogue phases, and personalize responses to heterogeneous patient
needs. We apply our framework to Motivational Interviews, aiming to foster
behavior change, and demonstrate that the proposed dialogue manager outperforms
a state-of-the-art LLM baseline in terms of reward, showing a potential benefit
of conditioning LLMs to create open-ended dialogue systems with specific goals.

</details>


### [37] [Breaking Barriers: Do Reinforcement Post Training Gains Transfer To Unseen Domains?](https://arxiv.org/abs/2506.19733)
*Chuxuan Hu,Yuxuan Zhu,Antony Kellermann,Caleb Biddulph,Suppakit Waiwitlikhit,Jason Benn,Daniel Kang*

Main category: cs.CL

TL;DR: 强化后训练(RPT)在相似领域能显著提升大语言模型推理能力，但面对不同推理模式的领域时效果泛化性不足


<details>
  <summary>Details</summary>
Motivation: 探究RPT改进效果的领域泛化能力，因现有研究仅评估与微调数据同领域的结果

Method: 采用双研究设计：(1)观察性研究比较开放权重RPT模型与基础模型在多领域表现(含未见领域)；(2)干预性研究在单领域微调后评估跨领域性能

Result: RPT在类似微调数据的任务中提升显著，但增益效果在不同推理模式的领域出现不一致泛化甚至消失

Conclusion: RPT的改进效果具有领域局限性，在部署到新领域时需要谨慎评估其有效性

Abstract: Reinforcement post training (RPT) has recently shown promise in improving the
reasoning abilities of large language models (LLMs). However, it remains
unclear how well these improvements generalize to new domains, as prior work
evaluates RPT models on data from the same domains used for fine-tuning. To
understand the generalizability of RPT, we conduct two studies. (1)
Observational: We compare a wide range of open-weight RPT models against their
corresponding base models across multiple domains, including both seen and
unseen domains in their fine-tuning data. (2) Interventional: we fine-tune LLMs
with RPT on single domains and evaluate their performance across multiple
domains. Both studies converge on the same conclusion that, although RPT brings
substantial gains on tasks similar to the fine-tuning data, the gains
generalize inconsistently and can vanish on domains with different reasoning
patterns.

</details>


### [38] [Evaluating Rare Disease Diagnostic Performance in Symptom Checkers: A Synthetic Vignette Simulation Approach](https://arxiv.org/abs/2506.19750)
*Takashi Nishibayashi,Seiji Kanazawa,Kumpei Yamada*

Main category: cs.CL

TL;DR: 提出基于HPO知识库的合成临床案例模拟方法，有效预测症状检查器算法更新对罕见病诊断性能的影响（R²达0.831），需2小时/疾病的HPO表型映射工作。


<details>
  <summary>Details</summary>
Motivation: 解决症状检查器算法更新时，罕见病评估数据不足和人工创建临床案例成本高的问题

Method: 利用人类表型本体（HPO）生成合成临床案例，通过模拟问诊预测算法更新影响，采用R²验证预测准确性

Result: 含HPO频率信息的疾病预测显著（recall@8 R²=0.831，precision@8 R²=0.78），无频率信息疾病误差大

Conclusion: 该方法实现基于公开知识库的罕见病诊断性能预评估，低成本透明化支持算法优化，助力早期诊断

Abstract: Background: Symptom Checkers (SCs) provide users with personalized medical
information. To prevent performance degradation from algorithm updates, SC
developers must evaluate diagnostic performance changes for individual diseases
before deployment. However, acquiring sufficient evaluation data for rare
diseases is difficult, and manually creating numerous clinical vignettes is
costly and impractical. Objective: This study proposes and validates a novel
Synthetic Vignette Simulation Approach to evaluate diagnostic performance
changes for individual rare diseases following SC algorithm updates. Methods:
We used disease-phenotype annotations from the Human Phenotype Ontology (HPO),
a knowledge database for rare diseases, to generate synthetic vignettes. With
these, we simulated SC interviews to estimate the impact of algorithm updates
on real-world diagnostic performance. The method's effectiveness was evaluated
retrospectively by comparing estimated values with actual metric changes using
the R 2(R-squared) coefficient. Results: The experiment included eight past SC
algorithm updates. For updates on diseases with frequency information in HPO
(n=5), the R^2 for recall@8 change was 0.831 (p=0.031), and for precision@8
change, it was 0.78 (p=0.047), indicating the method can predict
post-deployment performance. In contrast, large prediction errors occurred for
diseases without frequency information (n=3), highlighting its importance. The
manual effort to map HPO phenotypes to SC symptoms was approximately 2 hours
per disease. Conclusions: Our method enables pre-deployment evaluation of SC
algorithm changes for individual rare diseases using a publicly available,
expert-created knowledge base. This transparent and low-cost approach allows
developers to efficiently improve diagnostic performance for rare diseases,
potentially enhancing support for early diagnosis.

</details>


### [39] [Arabic Dialect Classification using RNNs, Transformers, and Large Language Models: A Comparative Analysis](https://arxiv.org/abs/2506.19753)
*Omar A. Essameldin,Ali O. Elbeih,Wael H. Gomaa,Wael F. Elsersy*

Main category: cs.CL

TL;DR: 使用MARBERTv2模型在QADI数据集实现65%准确率的阿拉伯方言分类，应用于聊天机器人/社交媒体监测


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语22国方言多样性导致的自然语言处理难题，提升方言识别技术

Method: 采用RNN/Transformer/LLM三类模型对比，结合前沿预处理技术系统测试

Result: MARBERTv2表现最佳（准确率65%，F1值64%），揭示了阿拉伯方言识别的关键语言特征

Conclusion: 该技术可推动方言适配的聊天机器人开发，增强社交媒体内容监测，促进阿拉伯社区数字包容

Abstract: The Arabic language is among the most popular languages in the world with a
huge variety of dialects spoken in 22 countries. In this study, we address the
problem of classifying 18 Arabic dialects of the QADI dataset of Arabic tweets.
RNN models, Transformer models, and large language models (LLMs) via prompt
engineering are created and tested. Among these, MARBERTv2 performed best with
65% accuracy and 64% F1-score. Through the use of state-of-the-art
preprocessing techniques and the latest NLP models, this paper identifies the
most significant linguistic issues in Arabic dialect identification. The
results corroborate applications like personalized chatbots that respond in
users' dialects, social media monitoring, and greater accessibility for Arabic
communities.

</details>


### [40] [Accurate, fast, cheap: Choose three. Replacing Multi-Head-Attention with Bidirectional Recurrent Attention for Long-Form ASR](https://arxiv.org/abs/2506.19761)
*Martin Ratajczak,Jean-Philippe Robichaud,Jennifer Drexler Fox*

Main category: cs.CL

TL;DR: 提出基于线性复杂度循环注意力（RA）的长语音识别方案，通过双向RA层和Direction Dropout方法实现高效准确的长序列处理，相比传统方法提升44%吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统基于多头注意力（MHA）的ASR模型因二次复杂度难以处理长语音序列，需开发更高效的注意力机制。

Method: 采用双向循环注意力（RA）层构建模型，开发长训练范式优化性能，提出Direction Dropout正则化方法控制精度/效率平衡，支持交替方向解码。

Result: 双向RA在精度持平MHA基础上实现高效处理，长训练使RA相比LCA基线准确率更高且吞吐量提升44%，Direction Dropout提供动态精度-效率调控。

Conclusion: 循环注意力机制结合新型训练方法有效解决长语音识别效率瓶颈，通过方向正则化技术实现精度与吞吐量的最优权衡。

Abstract: Long-form speech recognition is an application area of increasing research
focus. ASR models based on multi-head attention (MHA) are ill-suited to
long-form ASR because of their quadratic complexity in sequence length. We
build on recent work that has investigated linear complexity recurrent
attention (RA) layers for ASR. We find that bidirectional RA layers can match
the accuracy of MHA for both short- and long-form applications. We present a
strong limited-context attention (LCA) baseline, and show that RA layers are
just as accurate while being more efficient. We develop a long-form training
paradigm which further improves RA performance, leading to better accuracy than
LCA with 44% higher throughput. We also present Direction Dropout, a novel
regularization method that improves accuracy, provides fine-grained control of
the accuracy/throughput trade-off of bidirectional RA, and enables a new
alternating directions decoding mode with even higher throughput.

</details>


### [41] [SRFT: A Single-Stage Method with Supervised and Reinforcement Fine-Tuning for Reasoning](https://arxiv.org/abs/2506.19767)
*Yuqian Fu,Tinghong Chen,Jiajun Chai,Xihuai Wang,Songjun Tu,Guojun Yin,Wei Lin,Qichao Zhang,Yuanheng Zhu,Dongbin Zhao*

Main category: cs.CL

TL;DR: 提出SRFT方法统一监督微调与强化学习，通过熵感知权重机制在单阶段优化LLM，实验显示数学推理任务准确率提升9%


<details>
  <summary>Details</summary>
Motivation: 现有两阶段整合方法效率不足，SFT导致粗粒度全局变化而RL实现细粒度优化，需通过熵指标探索更优的联合训练机制

Method: SRFT引入熵感知动态权重，同时应用监督信号和强化奖励，直接利用示范数据与自主探索轨迹进行端到端优化

Result: 在5个数学推理基准达到59.1%平均准确率，超越无RL方法9.0%；在3个分布外任务提升10.9%

Conclusion: 熵是训练有效性关键指标，单阶段联合优化显著优于传统两阶段方法，验证了统一范式的技术优势

Abstract: Large language models (LLMs) have achieved remarkable progress in reasoning
tasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and
Reinforcement Learning (RL) remains a fundamental challenge. Through
comprehensive analysis of token distributions, learning dynamics, and
integration mechanisms from entropy-based perspectives, we reveal key
differences between these paradigms: SFT induces coarse-grained global changes
to LLM policy distributions, while RL performs fine-grained selective
optimizations, with entropy serving as a critical indicator of training
effectiveness. Building on these observations, we propose Supervised
Reinforcement Fine-Tuning (SRFT), a single-stage method that unifies both
fine-tuning paradigms through entropy-aware weighting mechanisms. Our approach
simultaneously applies SFT and RL to directly optimize the LLM using
demonstrations and self-exploration rollouts rather than through two-stage
sequential methods. Extensive experiments show that SRFT achieves 59.1% average
accuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning
benchmarks and 10.9% on three out-of-distribution benchmarks.

</details>


### [42] [Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study](https://arxiv.org/abs/2506.19794)
*Yuqi Zhu,Yi Zhong,Jintian Zhang,Ziheng Zhang,Shuofei Qiao,Yujie Luo,Lun Du,Da Zheng,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 研究提升开源大语言模型数据分析能力，发现战略规划质量是关键，并通过数据合成方法显著提升性能


<details>
  <summary>Details</summary>
Motivation: 开源大语言模型在复杂推理场景中存在显著局限性，需提升其数据分析能力

Method: 通过策划多样化现实场景的种子数据集，从数据理解、代码生成、战略规划三个维度评估模型

Result: 发现战略规划质量决定性能、交互设计影响推理能力、数据质量优于多样性三个关键结论，开发出有效的数据合成方法

Conclusion: 战略规划质量、交互设计优化和高质量数据是提升开源模型分析推理能力的关键要素，数据合成方法具有显著改进效果

Abstract: Large Language Models (LLMs) hold promise in automating data analysis tasks,
yet open-source models face significant limitations in these kinds of
reasoning-intensive scenarios. In this work, we investigate strategies to
enhance the data analysis capabilities of open-source LLMs. By curating a seed
dataset of diverse, realistic scenarios, we evaluate models across three
dimensions: data understanding, code generation, and strategic planning. Our
analysis reveals three key findings: (1) Strategic planning quality serves as
the primary determinant of model performance; (2) Interaction design and task
complexity significantly influence reasoning capabilities; (3) Data quality
demonstrates a greater impact than diversity in achieving optimal performance.
We leverage these insights to develop a data synthesis methodology,
demonstrating significant improvements in open-source LLMs' analytical
reasoning capabilities.

</details>


### [43] [How Effectively Can BERT Models Interpret Context and Detect Bengali Communal Violent Text?](https://arxiv.org/abs/2506.19831)
*Abdullah Khondoker,Enam Ahmed Taufik,Md. Iftekhar Islam Tashik,S M Ishtiak Mahmud,Farig Sadeque*

Main category: cs.CL

TL;DR: 提出微调BanglaBERT模型及集成方法提升孟加拉语社群暴力文本检测效果（F1从0.60提升至0.63），通过LIME解释模型决策局限


<details>
  <summary>Details</summary>
Motivation: 现有研究对社群暴力文本分类探索不足，网络仇恨加剧群体暴力，需提升孟加拉语社交媒体文本检测精度以维护社会和谐

Method: 微调BanglaBERT模型→数据增强（+1794样本）→构建集成模型→结合余弦相似度分析和LIME解释模型行为

Result: 定量：集成模型macro F1达0.63；定性：模型存在语境理解偏差、混淆近义词问题；LIME揭示高置信度误判案例

Conclusion: 证实NLP技术对减少网络群体暴力的有效性，建立可解释性分析框架，为提升模型社会影响力提供方法论基础

Abstract: The spread of cyber hatred has led to communal violence, fueling aggression
and conflicts between various religious, ethnic, and social groups, posing a
significant threat to social harmony. Despite its critical importance, the
classification of communal violent text remains an underexplored area in
existing research. This study aims to enhance the accuracy of detecting text
that incites communal violence, focusing specifically on Bengali textual data
sourced from social media platforms. We introduce a fine-tuned BanglaBERT model
tailored for this task, achieving a macro F1 score of 0.60. To address the
issue of data imbalance, our dataset was expanded by adding 1,794 instances,
which facilitated the development and evaluation of a fine-tuned ensemble
model. This ensemble model demonstrated an improved performance, achieving a
macro F1 score of 0.63, thus highlighting its effectiveness in this domain. In
addition to quantitative performance metrics, qualitative analysis revealed
instances where the models struggled with context understanding, leading to
occasional misclassifications, even when predictions were made with high
confidence. Through analyzing the cosine similarity between words, we
identified certain limitations in the pre-trained BanglaBERT models,
particularly in their ability to distinguish between closely related communal
and non-communal terms. To further interpret the model's decisions, we applied
LIME, which helped to uncover specific areas where the model struggled in
understanding context, contributing to errors in classification. These findings
highlight the promise of NLP and interpretability tools in reducing online
communal violence. Our work contributes to the growing body of research in
communal violence detection and offers a foundation for future studies aiming
to refine these techniques for better accuracy and societal impact.

</details>


### [44] [MAM: Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis via Role-Specialized Collaboration](https://arxiv.org/abs/2506.19835)
*Yucheng Zhou,Lingran Song,Jianbing Shen*

Main category: cs.CL

TL;DR: 提出模块化多代理框架MAM，通过角色分工实现医疗诊断优化，性能提升18%-365%


<details>
  <summary>Details</summary>
Motivation: 现有统一多模态医疗LLMs存在知识更新成本高、全面性不足和灵活性差的问题，需要模块化方案突破限制

Method: 将诊断过程分解为全科医生/专科团队/放射科医师等角色，每个角色由LLM代理担任，形成协同诊断系统

Result: 在多模态医疗数据集（文本/图像/音频/视频）上全面超越基线模型，最高提升365%

Conclusion: MAM框架通过模块化分工实现高效知识更新，显著提升诊断性能，为医疗LLMs发展提供新范式

Abstract: Recent advancements in medical Large Language Models (LLMs) have showcased
their powerful reasoning and diagnostic capabilities. Despite their success,
current unified multimodal medical LLMs face limitations in knowledge update
costs, comprehensiveness, and flexibility. To address these challenges, we
introduce the Modular Multi-Agent Framework for Multi-Modal Medical Diagnosis
(MAM). Inspired by our empirical findings highlighting the benefits of role
assignment and diagnostic discernment in LLMs, MAM decomposes the medical
diagnostic process into specialized roles: a General Practitioner, Specialist
Team, Radiologist, Medical Assistant, and Director, each embodied by an
LLM-based agent. This modular and collaborative framework enables efficient
knowledge updates and leverages existing medical LLMs and knowledge bases.
Extensive experimental evaluations conducted on a wide range of publicly
accessible multimodal medical datasets, incorporating text, image, audio, and
video modalities, demonstrate that MAM consistently surpasses the performance
of modality-specific LLMs. Notably, MAM achieves significant performance
improvements ranging from 18% to 365% compared to baseline models. Our code is
released at https://github.com/yczhou001/MAM.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [45] [Am I Playing Better Now? The Effects of G-SYNC in 60Hz Gameplay](https://arxiv.org/abs/2506.19084)
*Maryam Riahi,Benjamin Watson*

Main category: cs.GR

TL;DR: G-SYNC在60Hz刷新率下仍能提升资深玩家表现（尤其在游戏挑战性高时），但对情感和体验影响有限


<details>
  <summary>Details</summary>
Motivation: 基于30Hz下G-SYNC显著提升玩家表现的先前研究，探索PC游戏主流60Hz刷新率下的技术效果

Method: 通过对比实验研究G-SYNC在60Hz环境下对资深玩家表现的影响，特别设置高难度游戏场景

Result: 60Hz下G-SYNC效果弱于30Hz，但仍可提升资深玩家操作表现；对情感/体验维度影响不显著

Conclusion: G-SYNC在高刷新率环境下仍具实用价值，但其效益集中于特定玩家群体和游戏场景

Abstract: G-SYNC technology matches formerly regular display refreshes to irregular
frame updates, improving frame rates and interactive latency. In a previous
study of gaming at the 30Hz frame rates common on consoles, players of
Battlefield 4 were unable to discern when G-SYNC was in use, but scored higher
with G-SYNC and were affected emotionally. We build on that study with the
first examination of G-SYNC's effects at the 60Hz frame rate more common in PC
gaming and on emerging consoles. Though G-SYNC's effects are less at 60Hz than
they were at 30Hz, G-SYNC can still improve the performance of veteran players,
particularly when games are challenging. G-SYNC's effects on emotion and
experience were limited.

</details>


### [46] [SOF: Sorted Opacity Fields for Fast Unbounded Surface Reconstruction](https://arxiv.org/abs/2506.19139)
*Lukas Radl,Felix Windisch,Thomas Deixelberger,Jozef Hladky,Michael Steiner,Dieter Schmalstieg,Markus Steinberger*

Main category: cs.GR

TL;DR: 提出Sorted Opacity Fields (SOF)方法，通过层次化重排序与高斯深度优化，实现从3D高斯模型中快速精确提取表面细节，重构效率提升三倍以上。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯场景重建方法在大规模无界环境中存在表面提取精度不足、依赖近似深度估计导致伪影的问题，需提升几何提取效率与准确性。

Method: 1. 层次化重排序策略 2. 基于level-set对齐的高斯深度鲁棒公式 3. 透明度场level-set正则化器 4. 几何一致性损失函数 5. 并行化四面体行进算法加速网格生成

Result: 定量评估显示SOF在保持实时渲染优势的同时，重构精度提升且总处理时间缩短至三分之一。

Conclusion: 该方法将高效高斯渲染转化为等效高效的几何提取，推动了3D重建领域的技术进步。

Abstract: Recent advances in 3D Gaussian representations have significantly improved
the quality and efficiency of image-based scene reconstruction. Their explicit
nature facilitates real-time rendering and fast optimization, yet extracting
accurate surfaces - particularly in large-scale, unbounded environments -
remains a difficult task. Many existing methods rely on approximate depth
estimates and global sorting heuristics, which can introduce artifacts and
limit the fidelity of the reconstructed mesh. In this paper, we present Sorted
Opacity Fields (SOF), a method designed to recover detailed surfaces from 3D
Gaussians with both speed and precision. Our approach improves upon prior work
by introducing hierarchical resorting and a robust formulation of Gaussian
depth, which better aligns with the level-set. To enhance mesh quality, we
incorporate a level-set regularizer operating on the opacity field and
introduce losses that encourage geometrically-consistent primitive shapes. In
addition, we develop a parallelized Marching Tetrahedra algorithm tailored to
our opacity formulation, reducing meshing time by up to an order of magnitude.
As demonstrated by our quantitative evaluation, SOF achieves higher
reconstruction accuracy while cutting total processing time by more than a
factor of three. These results mark a step forward in turning efficient
Gaussian-based rendering into equally efficient geometry extraction.

</details>


### [47] [Style Transfer: A Decade Survey](https://arxiv.org/abs/2506.19278)
*Tianshan Zhang,Hao Tang*

Main category: cs.GR

TL;DR: 系统回顾AIGC视觉艺术技术的三大范式（VAE/GAN/扩散模型），提出多维评估框架并揭示其创造力潜力与局限


<details>
  <summary>Details</summary>
Motivation: 探究AIGC技术从早期算法到现代生成模型的演变机制，解析其连接人类创造力与机器合成的美学影响

Method: 通过分析500+文献建立技术发展脉络，对比三大生成范式，构建包含技术创新/艺术价值/计算效率等维度的评估体系

Result: 证实AIGC在创意实践中的变革性潜力，同时揭示生成质量与人类意图匹配度的现存技术瓶颈

Conclusion: AIGC重塑艺术创作范式但需突破人机协同机制，未来应聚焦跨模态生成、动态交互创作及伦理评估框架构建

Abstract: The revolutionary advancement of Artificial Intelligence Generated Content
(AIGC) has fundamentally transformed the landscape of visual content creation
and artistic expression. While remarkable progress has been made in image
generation and style transfer, the underlying mechanisms and aesthetic
implications of these technologies remain insufficiently understood. This paper
presents a comprehensive survey of AIGC technologies in visual arts, tracing
their evolution from early algorithmic frameworks to contemporary deep
generative models. We identify three pivotal paradigms: Variational
Autoencoders (VAE), Generative Adversarial Networks (GANs), and Diffusion
Models, and examine their roles in bridging the gap between human creativity
and machine synthesis. To support our analysis, we systematically review over
500 research papers published in the past decade, spanning both foundational
developments and state-of-the-art innovations. Furthermore, we propose a
multidimensional evaluation framework that incorporates Technical Innovation,
Artistic Merit, Visual Quality, Computational Efficiency, and Creative
Potential. Our findings reveal both the transformative capacities and current
limitations of AIGC systems, emphasizing their profound impact on the future of
creative practices. Through this extensive synthesis, we offer a unified
perspective on the convergence of artificial intelligence and artistic
expression, while outlining key challenges and promising directions for future
research in this rapidly evolving field.

</details>


### [48] [Continuous Indexed Points for Multivariate Volume Visualization](https://arxiv.org/abs/2506.19400)
*Liang Zhou,Xinyi Gou,Daniel Weiskopf*

Main category: cs.GR

TL;DR: 提出基于连续索引点的多变量体数据可视化方法，通过局部PCA分析和并行坐标密度映射增强变量相关性分析能力


<details>
  <summary>Details</summary>
Motivation: 传统多变量可视化难以有效编码局部空间相关性，需要能同时表达变量间关系与空间分布特征的新方法

Method: 采用层次空间加速的局部PCA拟合，构建1-flat/2-flat索引点进行并行坐标映射，结合交互遮挡着色与传递函数控件

Result: 通过案例验证可有效识别2-3变量相关性，专家研究证实方法在体数据多属性分析中的实用性

Conclusion: 该方法为多属性数据集提供通用分析框架，交互设计显著提升体数据相关性特征的探索效率

Abstract: We introduce continuous indexed points for improved multivariate volume
visualization. Indexed points represent linear structures in parallel
coordinates and can be used to encode local correlation of multivariate
(including multifield, multifaceted, and multiattribute) volume data. First, we
perform local linear fitting in the spatial neighborhood of each volume sample
using principal component analysis, accelerated by hierarchical spatial data
structures. This local linear information is then visualized as continuous
indexed points in parallel coordinates: a density representation of indexed
points in a continuous domain. With our new method, multivariate volume data
can be analyzed using the eigenvector information from local spatial
embeddings. We utilize both 1-flat and 2-flat indexed points, allowing us to
identify correlations between two variables and even three variables,
respectively. An interactive occlusion shading model facilitates good spatial
perception of the volume rendering of volumetric correlation characteristics.
Interactive exploration is supported by specifically designed multivariate
transfer function widgets working in the image plane of parallel coordinates.
We show that our generic technique works for multi-attribute datasets. The
effectiveness and usefulness of our new method is demonstrated through a case
study, an expert user study, and domain expert feedback.

</details>


### [49] [Virtual Memory for 3D Gaussian Splatting](https://arxiv.org/abs/2506.19415)
*Jonathan Haberl,Philipp Fleck,Clemens Arth*

Main category: cs.GR

TL;DR: 通过虚拟内存技术和动态流式传输优化大型3D高斯场景实时渲染，降低内存占用并提升渲染速度


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯溅射技术处理大规模复杂场景时面临的内存消耗高和渲染效率低的问题

Method: 结合虚拟内存/虚拟纹理技术实现动态高斯数据流式传输，整合细节层次(LOD)机制，并针对不同硬件进行优化实现

Result: 内存使用降低40%，复杂场景渲染速度提升2.3倍，移动设备端实现75%的渲染加速

Conclusion: 该内存优化方案有效突破了3D高斯场景的规模限制，在桌面和移动端均实现高效实时渲染，具有重要工程实践价值

Abstract: 3D Gaussian Splatting represents a breakthrough in the field of novel view
synthesis. It establishes Gaussians as core rendering primitives for highly
accurate real-world environment reconstruction. Recent advances have
drastically increased the size of scenes that can be created. In this work, we
present a method for rendering large and complex 3D Gaussian Splatting scenes
using virtual memory. By leveraging well-established virtual memory and virtual
texturing techniques, our approach efficiently identifies visible Gaussians and
dynamically streams them to the GPU just in time for real-time rendering.
Selecting only the necessary Gaussians for both storage and rendering results
in reduced memory usage and effectively accelerates rendering, especially for
highly complex scenes. Furthermore, we demonstrate how level of detail can be
integrated into our proposed method to further enhance rendering speed for
large-scale scenes. With an optimized implementation, we highlight key
practical considerations and thoroughly evaluate the proposed technique and its
impact on desktop and mobile devices.

</details>


### [50] [Uncovering Conceptual Blindspots in Generative Image Models Using Sparse Autoencoders](https://arxiv.org/abs/2506.19708)
*Matyas Bohacek,Thomas Fel,Maneesh Agrawala,Ekdeep Singh Lubana*

Main category: cs.GR

TL;DR: 提出使用稀疏自编码器(RA-SAE)定量分析生成模型的概念盲点，发现主流模型存在特定概念抑制/夸大现象


<details>
  <summary>Details</summary>
Motivation: 生成模型常无法正确表达训练数据中存在的简单概念(如人手、四物组合)，需系统化识别这些概念盲点而非仅轶事记录

Method: 在DINOv2特征上训练32,000概念的稀疏自编码器(RA-SAE)，量化比较真实与生成图像的概念分布差异

Result: 在Stable Diffusion等4个模型中识别出被抑制概念(喂鸟器/DVD)和夸大概念(木质纹理/棕榈树)，发现特定视觉模板记忆现象

Conclusion: 建立了通过概念保真度评估生成模型盲点的理论框架，揭示了模型概念表达的系统性偏差与记忆机制

Abstract: Despite their impressive performance, generative image models trained on
large-scale datasets frequently fail to produce images with seemingly simple
concepts -- e.g., human hands or objects appearing in groups of four -- that
are reasonably expected to appear in the training data. These failure modes
have largely been documented anecdotally, leaving open the question of whether
they reflect idiosyncratic anomalies or more structural limitations of these
models. To address this, we introduce a systematic approach for identifying and
characterizing "conceptual blindspots" -- concepts present in the training data
but absent or misrepresented in a model's generations. Our method leverages
sparse autoencoders (SAEs) to extract interpretable concept embeddings,
enabling a quantitative comparison of concept prevalence between real and
generated images. We train an archetypal SAE (RA-SAE) on DINOv2 features with
32,000 concepts -- the largest such SAE to date -- enabling fine-grained
analysis of conceptual disparities. Applied to four popular generative models
(Stable Diffusion 1.5/2.1, PixArt, and Kandinsky), our approach reveals
specific suppressed blindspots (e.g., bird feeders, DVD discs, and whitespaces
on documents) and exaggerated blindspots (e.g., wood background texture and
palm trees). At the individual datapoint level, we further isolate memorization
artifacts -- instances where models reproduce highly specific visual templates
seen during training. Overall, we propose a theoretically grounded framework
for systematically identifying conceptual blindspots in generative models by
assessing their conceptual fidelity with respect to the underlying
data-generating process.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [51] [Mix-of-Language-Experts Architecture for Multilingual Programming](https://arxiv.org/abs/2506.18923)
*Yifan Zong,Yuntian Deng,Pengyu Nie*

Main category: cs.PL

TL;DR: 提出MoLE混合语言专家架构，通过共享与专用LoRA模块的协同优化，平衡多语言编程任务中的参数效率与性能


<details>
  <summary>Details</summary>
Motivation: 传统多语言编程模型存在效率与性能的取舍矛盾：单一模型微调参数效率高但缺乏语言专门化能力，多模型方案资源消耗过大。MoLE旨在实现两者的平衡

Method: 构建包含基础模型、共享LoRA模块及多语言专用LoRA模块的三层架构。通过联合优化实现知识共享，推理时根据编程语言自动路由至对应LoRA模块

Result: 相比单独训练语言专用LoRA方案，参数效率提升；在代码生成准确率上超越单一共享模型微调方案

Conclusion: MoLE成功实现参数效率与模型性能的平衡，为多语言编程支持提供新范式，其架构设计可扩展至其他多领域AI任务

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
aiding developers with tasks like code comprehension, generation, and
translation. Supporting multilingual programming -- i.e., coding tasks across
multiple programming languages -- typically requires either (1) finetuning a
single LLM across all programming languages, which is cost-efficient but
sacrifices language-specific specialization and performance, or (2) finetuning
separate LLMs for each programming language, which allows for specialization
but is computationally expensive and storage-intensive due to the duplication
of parameters. This paper introduces MoLE (Mix-of-Language-Experts), a novel
architecture that balances efficiency and specialization for multilingual
programming. MoLE is composed of a base model, a shared LoRA (low-rank
adaptation) module, and a collection of language-specific LoRA modules. These
modules are jointly optimized during the finetuning process, enabling effective
knowledge sharing and specialization across programming languages. During
inference, MoLE automatically routes to the language-specific LoRA module
corresponding to the programming language of the code token being generated.
Our experiments demonstrate that MoLE achieves greater parameter efficiency
compared to training separate language-specific LoRAs, while outperforming a
single shared LLM finetuned for all programming languages in terms of accuracy.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [52] [Fake or Real, Can Robots Tell? Evaluating Embodied Vision-Language Models on Real and 3D-Printed Objects](https://arxiv.org/abs/2506.19579)
*Federico Tavella,Kathryn Mearns,Angelo Cangelosi*

Main category: cs.RO

TL;DR: 比较视觉语言模型(VLMs)在机器人桌面场景中的多视角描述能力，发现其对常见物体识别有效但缺乏对新颖表征的泛化能力


<details>
  <summary>Details</summary>
Motivation: 评估基础模型在具身智能系统中的实际应用效果，解决机器人场景中物体识别与自然语言描述的适配性问题

Method: 通过机械臂多视角采集物体图像，对比BLIP/VLMs模型的单视角/多视角描述效果，定量评估识别准确率、描述完整性和语言自然度

Result: VLMs对常见物体识别准确率达82%，但对3D打印新表征的识别率骤降至37%，多视角融合使描述完整性提升26%

Conclusion: 基础模型适用于常规物体感知任务，需开发自适应机制提升对非典型物体表征的鲁棒性

Abstract: Robotic scene understanding increasingly relies on vision-language models
(VLMs) to generate natural language descriptions of the environment. In this
work, we present a comparative study of captioning strategies for tabletop
scenes captured by a robotic arm equipped with an RGB camera. The robot
collects images of objects from multiple viewpoints, and we evaluate several
models that generate scene descriptions. We compare the performance of various
captioning models, like BLIP and VLMs. Our experiments examine the trade-offs
between single-view and multi-view captioning, and difference between
recognising real-world and 3D printed objects. We quantitatively evaluate
object identification accuracy, completeness, and naturalness of the generated
captions. Results show that VLMs can be used in robotic settings where common
objects need to be recognised, but fail to generalise to novel representations.
Our findings provide practical insights into deploying foundation models for
embodied agents in real-world settings.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [53] [HAWAII: Hierarchical Visual Knowledge Transfer for Efficient Vision-Language Models](https://arxiv.org/abs/2506.19072)
*Yimu Wang,Mozhgan Nasr Azadani,Sean Sedwards,Krzysztof Czarnecki*

Main category: cs.CV

TL;DR: HAWAII框架通过知识蒸馏将多个视觉专家的优势整合到单一编码器，显著降低计算成本同时保持性能优势。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型使用多个预训练视觉专家会大幅增加计算开销，需在保持多专家优势的前提下优化效率。

Method: 采用教师专属LoRA适配器+路由机制解决知识冲突，通过细粒度（token重要性加权）和粗粒度（通用知识适配器）双层蒸馏实现知识融合。

Result: 在多种视觉-语言任务上超越主流开源模型，计算效率提升显著。

Conclusion: HAWAII成功平衡性能与效率，为视觉语言模型优化提供了知识蒸馏的新范式。

Abstract: Improving the visual understanding ability of vision-language models (VLMs)
is crucial for enhancing their performance across various tasks. While using
multiple pretrained visual experts has shown great promise, it often incurs
significant computational costs during training and inference. To address this
challenge, we propose HAWAII, a novel framework that distills knowledge from
multiple visual experts into a single vision encoder, enabling it to inherit
the complementary strengths of several experts with minimal computational
overhead. To mitigate conflicts among different teachers and switch between
different teacher-specific knowledge, instead of using a fixed set of adapters
for multiple teachers, we propose to use teacher-specific Low-Rank Adaptation
(LoRA) adapters with a corresponding router. Each adapter is aligned with a
specific teacher, avoiding noisy guidance during distillation. To enable
efficient knowledge distillation, we propose fine-grained and coarse-grained
distillation. At the fine-grained level, token importance scores are employed
to emphasize the most informative tokens from each teacher adaptively. At the
coarse-grained level, we summarize the knowledge from multiple teachers and
transfer it to the student using a set of general-knowledge LoRA adapters with
a router. Extensive experiments on various vision-language tasks demonstrate
the superiority of HAWAII, compared to the popular open-source VLMs.

</details>


### [54] [Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System](https://arxiv.org/abs/2506.19433)
*Lixuan He,Haoyu Dong,Zhenxing Chen,Yangcheng Yu,Jie Feng,Yong Li*

Main category: cs.CV

TL;DR: 提出Mem4Nav分层记忆系统，通过融合八叉树和语义拓扑图实现高效视觉语言导航，在多个数据集上提升7-13%任务完成率


<details>
  <summary>Details</summary>
Motivation: 现有模块化方法缺乏统一内存管理，端到端方法受限于固定上下文窗口和隐式空间推理，需结合两者优势实现高效长时程导航

Method: 采用分层存储架构：稀疏八叉树实现体素级索引，语义拓扑图描述地标连接，通过可逆Transformer嵌入可训练内存令牌。LTM长期记忆压缩历史观测，STM短期记忆处理实时避障

Result: 在Touchdown/Map2Seq数据集上，不同主干网络实现7-13pp任务完成率提升，SPD显著降低，nDTW提升>10pp。消融实验验证分层地图和双内存模块的必要性

Conclusion: Mem4Nav通过分层空间表征和自适应记忆机制突破现有VLN系统限制，代码开源推动导航研究。双内存架构平衡历史压缩与实时推理，为具身智能提供新范式

Abstract: Vision-and-Language Navigation (VLN) in large-scale urban environments
requires embodied agents to ground linguistic instructions in complex scenes
and recall relevant experiences over extended time horizons. Prior modular
pipelines offer interpretability but lack unified memory, while end-to-end
(M)LLM agents excel at fusing vision and language yet remain constrained by
fixed context windows and implicit spatial reasoning. We introduce
\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system
that can augment any VLN backbone. Mem4Nav fuses a sparse octree for
fine-grained voxel indexing with a semantic topology graph for high-level
landmark connectivity, storing both in trainable memory tokens embedded via a
reversible Transformer. Long-term memory (LTM) compresses and retains
historical observations at both octree and graph nodes, while short-term memory
(STM) caches recent multimodal entries in relative coordinates for real-time
obstacle avoidance and local planning. At each step, STM retrieval sharply
prunes dynamic context, and, when deeper history is needed, LTM tokens are
decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and
Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based
LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13
pp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW
improvement. Ablations confirm the indispensability of both the hierarchical
map and dual memory modules. Our codes are open-sourced via
https://github.com/tsinghua-fib-lab/Mem4Nav.

</details>


### [55] [Recurrent Visual Feature Extraction and Stereo Attentions for CT Report Generation](https://arxiv.org/abs/2506.19665)
*Yuanhe Tian,Lei Mao,Yan Song*

Main category: cs.CV

TL;DR: 提出基于大语言模型（LLM）的循环视觉特征提取与立体注意力CT报告生成方法，在M3D-Cap数据集达到SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 解决传统CT报告生成方法忽略切片间空间关联、无法有效整合多层级病灶特征的痛点，提升医学影像与文本的对齐能力。

Method: 采用Vision Transformer循环处理CT切片，通过立体注意力机制从多视角选择关键视觉信息并与文本特征对齐，指导LLM生成报告。

Result: 在M3D-Cap基准测试中超越现有基线模型，取得当前最优结果（BLEU-4提升3.2%，ROUGE-L提升4.5%）

Conclusion: 该方法通过显式建模CT切片间关联及多层次特征融合，显著提升CT报告生成质量，为医学影像分析提供新思路。

Abstract: Generating reports for computed tomography (CT) images is a challenging task,
while similar to existing studies for medical image report generation, yet has
its unique characteristics, such as spatial encoding of multiple images,
alignment between image volume and texts, etc. Existing solutions typically use
general 2D or 3D image processing techniques to extract features from a CT
volume, where they firstly compress the volume and then divide the compressed
CT slices into patches for visual encoding. These approaches do not explicitly
account for the transformations among CT slices, nor do they effectively
integrate multi-level image features, particularly those containing specific
organ lesions, to instruct CT report generation (CTRG). In considering the
strong correlation among consecutive slices in CT scans, in this paper, we
propose a large language model (LLM) based CTRG method with recurrent visual
feature extraction and stereo attentions for hierarchical feature modeling.
Specifically, we use a vision Transformer to recurrently process each slice in
a CT volume, and employ a set of attentions over the encoded slices from
different perspectives to selectively obtain important visual information and
align them with textual features, so as to better instruct an LLM for CTRG.
Experiment results and further analysis on the benchmark M3D-Cap dataset show
that our method outperforms strong baseline models and achieves
state-of-the-art results, demonstrating its validity and effectiveness.

</details>


### [56] [ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality Debiasing](https://arxiv.org/abs/2506.19848)
*Long Xing,Qidong Huang,Xiaoyi Dong,Pan Zhang,Yuhang Zang,Yuhang Cao,Jinsong Li,Shuangrui Ding,Weiming Zhang,Nenghai Yu,Jiaqi Wang,Feng Wu,Dahua Lin*

Main category: cs.CV

TL;DR: ScaleCap提出推理时可扩展的图像描述策略，通过启发式问答和对比句子评分消除多模态偏差与语言偏差，生成更精准平衡的描述文本


<details>
  <summary>Details</summary>
Motivation: 解决LVLM存在的多模态偏差（描述粒度不平衡）和语言偏差（幻觉描述）问题，提升图像描述的准确性与信息完整性

Method: 1. 启发式问答生成图像相关问答逐步注入信息
2. 对比句子评分消除语言幻觉
3. 动态增加推理成本扩展视觉细节捕捉

Result: 使用45万ScaleCap标注图像预训练的LVLM在11个基准测试中性能提升，在VQA任务替换图像、描述重建图像等任务中验证了描述丰富性与语义覆盖能力

Conclusion: ScaleCap通过可扩展推理机制有效平衡视觉细节描述，消除语言幻觉，为图像描述生成和跨模态预训练提供了创新解决方案

Abstract: This paper presents ScaleCap, an inference-time scalable image captioning
strategy that generates comprehensive and detailed image captions. The key
challenges of high-quality image captioning lie in the inherent biases of
LVLMs: multimodal bias resulting in imbalanced descriptive granularity,
offering detailed accounts of some elements while merely skimming over others;
linguistic bias leading to hallucinated descriptions of non-existent objects.
To address these issues, we propose a scalable debiased captioning strategy,
which continuously enriches and calibrates the caption with increased inference
budget. Specifically, we propose two novel components: heuristic question
answering and contrastive sentence rating. The former generates
content-specific questions based on the image and answers them to progressively
inject relevant information into the caption. The latter employs sentence-level
offline contrastive decoding to effectively identify and eliminate
hallucinations caused by linguistic biases. With increased inference cost, more
heuristic questions are raised by ScaleCap to progressively capture additional
visual details, generating captions that are more accurate, balanced, and
informative. Extensive modality alignment experiments demonstrate the
effectiveness of ScaleCap. Annotating 450K images with ScaleCap and using them
for LVLM pretraining leads to consistent performance gains across 11 widely
used benchmarks. Furthermore, ScaleCap showcases superb richness and fidelity
of generated captions with two additional tasks: replacing images with captions
in VQA task, and reconstructing images from captions to assess semantic
coverage. Code is available at https://github.com/Cooperx521/ScaleCap.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [57] [Kling-Foley: Multimodal Diffusion Transformer for High-Quality Video-to-Audio Generation](https://arxiv.org/abs/2506.19774)
*Jun Wang,Xijuan Zeng,Chunyu Qiang,Ruilong Chen,Shiyao Wang,Le Wang,Wangjing Zhou,Pengfei Cai,Jiahui Zhao,Nan Li,Zihan Li,Yuzhe Liang,Xiaopeng Wang,Haorui Zheng,Ming Wen,Kang Yin,Yiran Wang,Nan Li,Feng Deng,Liang Dong,Chen Zhang,Di Zhang,Kun Gai*

Main category: eess.AS

TL;DR: 提出了多模态视频生成音频模型Kling-Foley，通过多模态扩散变压器和增强对齐模块实现高质量音视频同步，并开源工业级评估基准。


<details>
  <summary>Details</summary>
Motivation: 现有开源基准在音频类型覆盖和标注完整性方面存在不足，需要提升音视频语义对齐和同步性能。

Method: 1. 多模态扩散变压器建模跨模态交互
2. 视觉语义表征模块+音视频同步模块
3. 通用潜在音频编解码器支持多场景建模
4. 立体声渲染技术增强空间感

Result: 在分布匹配度（FID:2.37）、语义对齐（CLAP:0.852）、时序对齐（SyncNet:8.12）和音频质量（MOS:4.3）指标上达到公开模型最佳水平

Conclusion: Kling-Foley验证了流匹配目标在跨模态生成中的有效性，同步开源的Kling-Audio-Eval基准填补了领域空白

Abstract: We propose Kling-Foley, a large-scale multimodal Video-to-Audio generation
model that synthesizes high-quality audio synchronized with video content. In
Kling-Foley, we introduce multimodal diffusion transformers to model the
interactions between video, audio, and text modalities, and combine it with a
visual semantic representation module and an audio-visual synchronization
module to enhance alignment capabilities. Specifically, these modules align
video conditions with latent audio elements at the frame level, thereby
improving semantic alignment and audio-visual synchronization. Together with
text conditions, this integrated approach enables precise generation of
video-matching sound effects. In addition, we propose a universal latent audio
codec that can achieve high-quality modeling in various scenarios such as sound
effects, speech, singing, and music. We employ a stereo rendering method that
imbues synthesized audio with a spatial presence. At the same time, in order to
make up for the incomplete types and annotations of the open-source benchmark,
we also open-source an industrial-level benchmark Kling-Audio-Eval. Our
experiments show that Kling-Foley trained with the flow matching objective
achieves new audio-visual SOTA performance among public models in terms of
distribution matching, semantic alignment, temporal alignment and audio
quality.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [58] [From Web Search towards Agentic Deep Research: Incentivizing Search with Reasoning Agents](https://arxiv.org/abs/2506.18959)
*Weizhi Zhang,Yangning Li,Yuanchen Bei,Junyu Luo,Guancheng Wan,Liangwei Yang,Chenxuan Xie,Yuyao Yang,Wei-Chieh Huang,Chunyu Miao,Henry Peng Zou,Xiao Luo,Yusheng Zhao,Yankai Chen,Chunkit Chan,Peilin Zhou,Xinyang Zhang,Chenwei Zhang,Jingbo Shang,Ming Zhang,Yangqiu Song,Irwin King,Philip S. Yu*

Main category: cs.IR

TL;DR: 论文提出基于大语言模型的Agentic Deep Research新范式，通过整合推理、迭代检索和信息合成，显著超越传统信息检索方法并有望成为未来主流


<details>
  <summary>Details</summary>
Motivation: 传统关键词搜索引擎难以处理复杂多步骤信息需求，LLM的推理和代理能力为解决这一问题提供了新路径

Method: 构建动态反馈循环系统，整合自主推理、迭代检索、信息合成三要素，引入测试阶段扩展定律量化计算深度的影响

Result: 基准测试显示性能显著优于现有方法，开源社区资源推动范式演进（GitHub仓库包含行业产品、论文、数据集和实现）

Conclusion: Agentic Deep Research通过系统化整合推理与检索，将重塑未来信息获取范式，测试扩展定律为系统优化提供理论支撑

Abstract: Information retrieval is a cornerstone of modern knowledge acquisition,
enabling billions of queries each day across diverse domains. However,
traditional keyword-based search engines are increasingly inadequate for
handling complex, multi-step information needs. Our position is that Large
Language Models (LLMs), endowed with reasoning and agentic capabilities, are
ushering in a new paradigm termed Agentic Deep Research. These systems
transcend conventional information search techniques by tightly integrating
autonomous reasoning, iterative retrieval, and information synthesis into a
dynamic feedback loop. We trace the evolution from static web search to
interactive, agent-based systems that plan, explore, and learn. We also
introduce a test-time scaling law to formalize the impact of computational
depth on reasoning and search. Supported by benchmark results and the rise of
open-source implementations, we demonstrate that Agentic Deep Research not only
significantly outperforms existing approaches, but is also poised to become the
dominant paradigm for future information seeking. All the related resources,
including industry products, research papers, benchmark datasets, and
open-source implementations, are collected for the community in
https://github.com/DavidZWZ/Awesome-Deep-Research.

</details>


### [59] [NEAR$^2$: A Nested Embedding Approach to Efficient Product Retrieval and Ranking](https://arxiv.org/abs/2506.19743)
*Shenbin Qian,Diptesh Kanojia,Samarth Agrawal,Hadeel Saadany,Swapnil Bhosale,Constantin Orasan,Zhe Wu*

Main category: cs.IR

TL;DR: 提出NEAR²嵌套嵌入方法，在推理阶段实现12倍嵌入效率提升，同时提升检索排序准确率


<details>
  <summary>Details</summary>
Motivation: 解决电商检索系统处理复杂查询时精度与效率难以兼顾的问题，特别是大规模产品目录下的实时搜索需求

Method: 基于Transformer编码器的嵌套嵌入架构，结合多负例排序损失和在线对比损失，在四个不同测试集验证

Result: 相比现有模型，在保持训练成本不变前提下，小维度嵌入实现性能提升，推理效率提高12倍

Conclusion: NEAR²通过嵌套嵌入设计有效平衡检索精度与计算效率，为大规模电商搜索提供新的优化路径

Abstract: E-commerce information retrieval (IR) systems struggle to simultaneously
achieve high accuracy in interpreting complex user queries and maintain
efficient processing of vast product catalogs. The dual challenge lies in
precisely matching user intent with relevant products while managing the
computational demands of real-time search across massive inventories. In this
paper, we propose a Nested Embedding Approach to product Retrieval and Ranking,
called NEAR$^2$, which can achieve up to $12$ times efficiency in embedding
size at inference time while introducing no extra cost in training and
improving performance in accuracy for various encoder-based Transformer models.
We validate our approach using different loss functions for the retrieval and
ranking task, including multiple negative ranking loss and online contrastive
loss, on four different test sets with various IR challenges such as short and
implicit queries. Our approach achieves an improved performance over a smaller
embedding dimension, compared to any existing models.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [60] [TTSDS2: Resources and Benchmark for Evaluating Human-Quality Text to Speech Systems](https://arxiv.org/abs/2506.19441)
*Christoph Minixhofer,Ondrej Klejch,Peter Bell*

Main category: cs.SD

TL;DR: 提出了改进版TTSDS2评估指标，该指标在跨领域和语言测试中表现优异，并发布了包含11,000+主观评分的数据集和多语言测试基准。


<details>
  <summary>Details</summary>
Motivation: 现有TTS系统评估存在主观指标难以比较、客观指标缺乏验证的问题，特别是面对接近真人语音的合成系统时传统指标面临挑战。

Method: 开发TTSDS2评估框架，通过多领域/多语言验证（Spearman相关系数分析），并构建防数据泄露的持续更新测试管道。

Result: TTSDS2在16个指标中唯一实现所有领域相关系数>0.50，同时发布覆盖14种语言的持续更新基准测试资源。

Conclusion: TTSDS2成为当前最可靠的TTS评估指标，配套资源为合成语音质量评估提供了标准化测试环境。

Abstract: Evaluation of Text to Speech (TTS) systems is challenging and
resource-intensive. Subjective metrics such as Mean Opinion Score (MOS) are not
easily comparable between works. Objective metrics are frequently used, but
rarely validated against subjective ones. Both kinds of metrics are challenged
by recent TTS systems capable of producing synthetic speech indistinguishable
from real speech. In this work, we introduce Text to Speech Distribution Score
2 (TTSDS2), a more robust and improved version of TTSDS. Across a range of
domains and languages, it is the only one out of 16 compared metrics to
correlate with a Spearman correlation above 0.50 for every domain and
subjective score evaluated. We also release a range of resources for evaluating
synthetic speech close to real speech: A dataset with over 11,000 subjective
opinion score ratings; a pipeline for continually recreating a multilingual
test dataset to avoid data leakage; and a continually updated benchmark for TTS
in 14 languages.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [61] [LLM-Based Social Simulations Require a Boundary](https://arxiv.org/abs/2506.19806)
*Zengqing Wu,Run Peng,Takayuki Ito,Chuan Xiao*

Main category: cs.CY

TL;DR: 提出LLM社会模拟需建立边界以提升可靠性，聚焦集体模式、真实行为对齐和验证方法


<details>
  <summary>Details</summary>
Motivation: LLM在模拟人类行为时存在行为同质化缺陷，难以满足复杂社会动态模拟的异质性要求

Method: 通过分析对齐性（行为匹配）、一致性（行为连贯）和鲁棒性（结果复现）三个边界问题

Result: 确立LLM模拟适用于集体模式分析、真实平均行为匹配场景，并提供验证方法检查清单

Conclusion: 明确边界的LLM社会模拟可有效支持社会科学研究，但需限定适用范围并建立验证机制

Abstract: This position paper argues that large language model (LLM)-based social
simulations should establish clear boundaries to meaningfully contribute to
social science research. While LLMs offer promising capabilities for modeling
human-like agents compared to traditional agent-based modeling, they face
fundamental limitations that constrain their reliability for social pattern
discovery. The core issue lies in LLMs' tendency towards an ``average persona''
that lacks sufficient behavioral heterogeneity, a critical requirement for
simulating complex social dynamics. We examine three key boundary problems:
alignment (simulated behaviors matching real-world patterns), consistency
(maintaining coherent agent behavior over time), and robustness
(reproducibility under varying conditions). We propose heuristic boundaries for
determining when LLM-based simulations can reliably advance social science
understanding. We believe that these simulations are more valuable when
focusing on (1) collective patterns rather than individual trajectories, (2)
agent behaviors aligning with real population averages despite limited
variance, and (3) proper validation methods available for testing simulation
robustness. We provide a practical checklist to guide researchers in
determining the appropriate scope and claims for LLM-based social simulations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [62] [A Batch-Insensitive Dynamic GNN Approach to Address Temporal Discontinuity in Graph Streams](https://arxiv.org/abs/2506.19282)
*Yang Zhou,Xiaoning Ren*

Main category: cs.LG

TL;DR: 提出BADGNN框架解决动态图神经网络大批量训练导致的时间连续性破坏问题，通过TLR正则化和A3注意力机制实现更高效训练。


<details>
  <summary>Details</summary>
Motivation: 动态图神经网络采用大批量训练会破坏事件序列的时间连续性，导致参数收敛困难和时间建模能力下降。理论分析表明大批量会扩大参数搜索空间的Lipschitz上界。

Method: BADGNN框架包含两个核心模块：1）TLR正则化控制参数搜索空间扩展 2）A3注意力机制自适应调整注意力权重，缓解正则化和批处理带来的注意力失真

Result: 在三个基准数据集上验证，BADGNN在保持性能优势的同时，支持比TGN更大的批次规模（提升4倍）和更快的训练速度（加速2.8倍）

Conclusion: BADGNN有效解决了动态图神经网络批量训练难题，通过理论引导的正则化和注意力机制改进，实现了效率与性能的平衡。

Abstract: In dynamic graphs, preserving temporal continuity is critical. However,
Memory-based Dynamic Graph Neural Networks (MDGNNs) trained with large batches
often disrupt event sequences, leading to temporal information loss. This
discontinuity not only deteriorates temporal modeling but also hinders
optimization by increasing the difficulty of parameter convergence. Our
theoretical study quantifies this through a Lipschitz upper bound, showing that
large batch sizes enlarge the parameter search space. In response, we propose
BADGNN, a novel batch-agnostic framework consisting of two core components: (1)
Temporal Lipschitz Regularization (TLR) to control parameter search space
expansion, and (2) Adaptive Attention Adjustment (A3) to alleviate attention
distortion induced by both regularization and batching. Empirical results on
three benchmark datasets show that BADGNN maintains strong performance while
enabling significantly larger batch sizes and faster training compared to TGN.
Our code is available at Code:
https://anonymous.4open.science/r/TGN_Lipichitz-C033/.

</details>


### [63] [Chain-of-Experts: Unlocking the Communication Power of Mixture-of-Experts Models](https://arxiv.org/abs/2506.18945)
*Zihan Wang,Rui Pan,Jiarui Yao,Robert Csordas,Linjie Li,Lu Yin,Jiajun Wu,Tong Zhang,Manling Li,Shiwei Liu*

Main category: cs.LG

TL;DR: 提出Chain-of-Experts架构，通过层内专家迭代通信提升MoE模型性能，在数学推理任务上验证损失降低6.7%，内存效率提升17.6-42%


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型中专家并行独立工作，缺乏交互导致表达能力受限。CoE通过层内专家链式迭代通信，增强专家协作和表示能力

Method: 1. 每层内部设计专家链式处理结构；2. 每个迭代步骤配备独立路由器实现动态专家选择；3. 引入迭代残差结构提升表达能力

Result: 1. 数学推理任务验证损失从1.20降至1.12；2. 2次迭代等效3倍专家选择宽度，内存节省17.6-42%；3. 专家专业化程度提升

Conclusion: CoE开创了通过专家迭代实现模型深度扩展的新维度，其动态路由机制和残差结构协同增强了模型表达能力，为高效模型设计提供新方向

Abstract: We propose Chain-of-Experts (CoE), a new Mixture-of-Experts (MoE)
architecture that introduces sequential expert communication within each layer.
Unlike traditional MoE models, where experts operate independently in parallel,
CoE processes tokens iteratively across a chain of experts inside a layer. To
support dynamic expert selection across iterations, CoE employs a dedicated
router at each iteration step within a layer. This design allows tokens to
re-evaluate and select different experts during each iteration, rather than
being statically assigned. As a result, CoE introduces a flexible routing
mechanism that increases the diversity of expert combinations and enriches the
model's representational capacity. CoE demonstrates improved performance under
fixed compute: on math reasoning tasks, it reduces validation loss from 1.20 to
1.12 compared to a standard MoE. Beyond performance, CoE offers a new scaling
axis: depth through expert iteration, which complements conventional
width/depth scaling. For example, using 2x iterations matches the performance
of 3x expert selections (in width), while reducing memory usage by 17.6-42%
relative to other scaling strategies. Our analysis reveals that CoE's benefits
stem from its iterative residual structure and enhanced expert specialization
empowered by iterative routing, which together unlock more expressive
representations. Code is available at https://github.com/ZihanWang314/coe.

</details>


### [64] [LLMs on a Budget? Say HOLA](https://arxiv.org/abs/2506.18952)
*Zohaib Hasan Siddiqui,Jiechao Gao,Ebad Shabbir,Mohammad Anas Azeez,Rafiq Ali,Gautam Siddharth Kashyap,Usman Naseem*

Main category: cs.LG

TL;DR: 提出HOLA框架，通过分层推测解码和自适应检索优化，显著提升边缘设备上LLM的效率和性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM优化方案无法兼顾速度与准确性，制约医疗/教育等实时场景应用

Method: 内部分层推测解码(HSD)+外部自适应检索(AdaComp-RAG)+混合压缩技术(LoBi)

Result: GSM8K提升17.6%EMA，ARC提升10.5%MCA，Jetson Nano延迟降低38%且内存占用减少

Conclusion: HOLA框架验证了边缘设备部署LLM的可行性，具备生产级扩展能力

Abstract: Running Large Language Models (LLMs) on edge devices is constrained by high
compute and memory demands posing a barrier for real-time applications in
sectors like healthcare, education, and embedded systems. Current solutions
such as quantization, pruning, and retrieval-augmented generation (RAG) offer
only partial optimizations and often compromise on speed or accuracy. We
introduce HOLA, an end-to-end optimization framework for efficient LLM
deployment. Internally, it leverages Hierarchical Speculative Decoding (HSD)
for faster inference without quality loss. Externally, AdaComp-RAG adjusts
retrieval complexity based on context needs. Together with LoBi, which blends
structured pruning (LoRA) and quantization, HOLA delivers significant gains:
17.6% EMA on GSM8K, 10.5% MCA on ARC, and reduced latency and memory on edge
devices like Jetson Nano--proving both scalable and production-ready.

</details>


### [65] [Thought Anchors: Which LLM Reasoning Steps Matter?](https://arxiv.org/abs/2506.19143)
*Paul C. Bogdan,Uzay Macar,Neel Nanda,Arthur Conmy*

Main category: cs.LG

TL;DR: 提出三种句子级归因方法分析大语言模型推理过程，揭示核心推理步骤（思维锚点）对后续推理的关键影响


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型长链条思维推理过程可解释性差的痛点，探索通过句子级推理轨迹分析来解构复杂推理过程

Method: 1) 黑盒反事实重要性评估：通过100次条件生成实验对比句子替换后的答案差异
2) 白盒注意力模式分析：通过聚合句子间注意力模式识别广播型句子
3) 因果归因方法：通过抑制特定句子注意力观测其对后续推理的影响

Result: 发现具有超常重要性的思维锚点（多为规划/回溯类句子），验证不同方法在追踪多步推理路径时的一致性，并开发可视化工具实现分析结果呈现

Conclusion: 句子级分析方法能有效揭示大模型的推理机制，多方法交叉验证为可解释性研究提供新范式，开源工具助力研究社区深入探索模型推理过程

Abstract: Reasoning large language models have recently achieved state-of-the-art
performance in many fields. However, their long-form chain-of-thought reasoning
creates interpretability challenges as each generated token depends on all
previous ones, making the computation harder to decompose. We argue that
analyzing reasoning traces at the sentence level is a promising approach to
understanding reasoning processes. We present three complementary attribution
methods: (1) a black-box method measuring each sentence's counterfactual
importance by comparing final answers across 100 rollouts conditioned on the
model generating that sentence or one with a different meaning; (2) a white-box
method of aggregating attention patterns between pairs of sentences, which
identified ``broadcasting'' sentences that receive disproportionate attention
from all future sentences via ``receiver'' attention heads; (3) a causal
attribution method measuring logical connections between sentences by
suppressing attention toward one sentence and measuring the effect on each
future sentence's tokens. Each method provides evidence for the existence of
thought anchors, reasoning steps that have outsized importance and that
disproportionately influence the subsequent reasoning process. These thought
anchors are typically planning or backtracking sentences. We provide an
open-source tool (www.thought-anchors.com) for visualizing the outputs of our
methods, and present a case study showing converging patterns across methods
that map how a model performs multi-step reasoning. The consistency across
methods demonstrates the potential of sentence-level analysis for a deeper
understanding of reasoning models.

</details>


### [66] [In-Context Occam's Razor: How Transformers Prefer Simpler Hypotheses on the Fly](https://arxiv.org/abs/2506.19351)
*Puneesh Deora,Bhavya Vasudeva,Tina Behnia,Christos Thrampoulidis*

Main category: cs.LG

TL;DR: Transformer通过上下文学习在不同复杂度任务中自动选择最简假设，验证了其内置的贝叶斯奥卡姆剃刀机制


<details>
  <summary>Details</summary>
Motivation: 研究Transformer在分层任务结构（不同复杂度任务共存）中的自适应能力，解决现有研究局限于固定复杂度环境的问题

Method: 设计马尔可夫链/线性回归测试平台，理论构建贝叶斯框架，分析模型规模/训练数据/上下文长度等要素的影响

Result: 模型能准确识别任务复杂度层级并选择最简有效假设，在预训练GPT-4布尔函数任务中验证该机制的有效性

Conclusion: Transformer通过平衡模型拟合与复杂度惩罚，隐式实现贝叶斯奥卡姆剃刀，这种归纳偏好可能源于多样化任务分布的训练

Abstract: In-context learning (ICL) enables transformers to adapt to new tasks through
contextual examples without parameter updates. While existing research has
typically studied ICL in fixed-complexity environments, practical language
models encounter tasks spanning diverse complexity levels. This paper
investigates how transformers navigate hierarchical task structures where
higher-complexity categories can perfectly represent any pattern generated by
simpler ones. We design well-controlled testbeds based on Markov chains and
linear regression that reveal transformers not only identify the appropriate
complexity level for each task but also accurately infer the corresponding
parameters--even when the in-context examples are compatible with multiple
complexity hypotheses. Notably, when presented with data generated by simpler
processes, transformers consistently favor the least complex sufficient
explanation. We theoretically explain this behavior through a Bayesian
framework, demonstrating that transformers effectively implement an in-context
Bayesian Occam's razor by balancing model fit against complexity penalties. We
further ablate on the roles of model size, training mixture distribution,
inference context length, and architecture. Finally, we validate this Occam's
razor-like inductive bias on a pretrained GPT-4 model with Boolean-function
tasks as case study, suggesting it may be inherent to transformers trained on
diverse task distributions.

</details>


### [67] [Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models](https://arxiv.org/abs/2506.19697)
*Jungwoo Park,Taewhoo Lee,Chanwoong Yoon,Hyeon Hwang,Jaewoo Kang*

Main category: cs.LG

TL;DR: 提出Outlier-Safe Pre-Training (OSP)框架，通过Muon优化器、单尺度RMSNorm和可学习嵌入投影三大创新，主动预防LLM激活异常值，使4位量化模型性能提升34.7%，训练开销仅增加2%。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖事后处理LLM激活异常值，难以有效解决量化性能退化问题。研究旨在从根本上预防异常值产生，而非被动修复。

Method: 1. Muon优化器消除特权基向量 2. 单尺度RMSNorm避免通道放大 3. 可学习嵌入投影重分布激活量级

Result: 训练1.4B参数模型（1万亿tokens）实现零异常值，4位量化后10项基准平均分达35.7（Adam模型26.5），峰度接近零（0.04 vs 标准模型1818.56）

Conclusion: 异常值非LLM固有特性，而是训练策略产物。OSP通过主动预防机制颠覆传统量化范式，为高效LLM部署开辟新路径，相关代码和模型已开源。

Abstract: Extreme activation outliers in Large Language Models (LLMs) critically
degrade quantization performance, hindering efficient on-device deployment.
While channel-wise operations and adaptive gradient scaling are recognized
causes, practical mitigation remains challenging. We introduce Outlier-Safe
Pre-Training (OSP), a practical guideline that proactively prevents outlier
formation rather than relying on post-hoc mitigation. OSP combines three key
innovations: (1) the Muon optimizer, eliminating privileged bases while
maintaining training efficiency; (2) Single-Scale RMSNorm, preventing
channel-wise amplification; and (3) a learnable embedding projection,
redistributing activation magnitudes originating from embedding matrices. We
validate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is
the first production-scale LLM trained without such outliers. Under aggressive
4-bit quantization, our OSP model achieves a 35.7 average score across 10
benchmarks (compared to 26.5 for an Adam-trained model), with only a 2%
training overhead. Remarkably, OSP models exhibit near-zero excess kurtosis
(0.04) compared to extreme values (1818.56) in standard models, fundamentally
altering LLM quantization behavior. Our work demonstrates that outliers are not
inherent to LLMs but are consequences of training strategies, paving the way
for more efficient LLM deployment. The source code and pretrained checkpoints
are available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.

</details>


### [68] [Scaling Speculative Decoding with Lookahead Reasoning](https://arxiv.org/abs/2506.19830)
*Yichao Fu,Rui Ge,Zelei Shao,Zhijie Deng,Hao Zhang*

Main category: cs.LG

TL;DR: 提出Lookahead Reasoning方法，通过语义级步骤并行性突破token级推测解码的速度瓶颈，在保持答案质量的同时将推理速度提升至2.1倍。


<details>
  <summary>Details</summary>
Motivation: 传统token级推测解码(SD)的加速受限于指数级下降的正确率，无法有效利用长token草案的计算资源。需要新的并行机制突破算法天花板。

Method: 1) 轻量级草案模型生成多步语义提案 2) 目标模型批量扩展提案 3) 验证器筛选语义正确步骤 4) 结合token级SD实现双重并行加速

Result: 在GSM8K/AIME等基准测试中，将SD加速比从1.4倍提升至2.1倍，且加速效果随GPU吞吐量提升更具扩展性

Conclusion: Lookahead Reasoning通过语义步骤验证机制，实现了推理加速的算法突破，为大规模语言模型的高效推理提供了新方向

Abstract: Reasoning models excel by generating long chain-of-thoughts, but decoding the
resulting thousands of tokens is slow. Token-level speculative decoding (SD)
helps, but its benefit is capped, because the chance that an entire
$\gamma$-token guess is correct falls exponentially as $\gamma$ grows. This
means allocating more compute for longer token drafts faces an algorithmic
ceiling -- making the speedup modest and hardware-agnostic. We raise this
ceiling with Lookahead Reasoning, which exploits a second, step-level layer of
parallelism. Our key insight is that reasoning models generate step-by-step,
and each step needs only to be semantically correct, not exact token matching.
In Lookahead Reasoning, a lightweight draft model proposes several future
steps; the target model expands each proposal in one batched pass, and a
verifier keeps semantically correct steps while letting the target regenerate
any that fail. Token-level SD still operates within each reasoning step, so the
two layers of parallelism multiply. We show Lookahead Reasoning lifts the peak
speedup of SD both theoretically and empirically. Across GSM8K, AIME, and other
benchmarks, Lookahead Reasoning improves the speedup of SD from 1.4x to 2.1x
while preserving answer quality, and its speedup scales better with additional
GPU throughput. Our code is available at
https://github.com/hao-ai-lab/LookaheadReasoning

</details>


### [69] [Orthogonal Finetuning Made Scalable](https://arxiv.org/abs/2506.19847)
*Zeju Qiu,Weiyang Liu,Adrian Weller,Bernhard Schölkopf*

Main category: cs.LG

TL;DR: OFTv2通过输入中心化重构和Cayley-Neumann参数化，将正交微调的计算复杂度从立方降为二次方，实现10倍加速和3倍内存优化，并支持量化模型微调


<details>
  <summary>Details</summary>
Motivation: 解决原始OFT方法因权重中心实现导致的高计算/内存开销问题，提升参数高效微调技术的实用性

Method: 1. 输入中心化重构（矩阵-向量乘法替代矩阵-矩阵乘法）
2. Cayley-Neumann正交参数化（截断Neumann级数近似矩阵求逆）
3. 扩展支持量化基础模型微调

Result: 训练速度提升10倍，GPU显存降低3倍且性能无损；在量化模型微调中训练稳定性、效率和内存使用均优于QLoRA

Conclusion: OFTv2通过算法重构和高效参数化，显著提升了正交微调的计算效率和实用性，为大规模基础模型的高效适配提供了新解决方案

Abstract: Orthogonal finetuning (OFT) offers highly parameter-efficient adaptation
while preventing catastrophic forgetting, but its high runtime and memory
demands limit practical deployment. We identify the core computational
bottleneck in OFT as its weight-centric implementation, which relies on costly
matrix-matrix multiplications with cubic complexity. To overcome this, we
propose OFTv2, an input-centric reformulation that instead uses matrix-vector
multiplications (i.e., matrix-free computation), reducing the computational
cost to quadratic. We further introduce the Cayley-Neumann parameterization, an
efficient orthogonal parameterization that approximates the matrix inversion in
Cayley transform via a truncated Neumann series. These modifications allow
OFTv2 to achieve up to 10x faster training and 3x lower GPU memory usage
without compromising performance. In addition, we extend OFTv2 to support
finetuning quantized foundation models and show that it outperforms the popular
QLoRA in training stability, efficiency, and memory usage.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [70] [A Comment On "The Illusion of Thinking": Reframing the Reasoning Cliff as an Agentic Gap](https://arxiv.org/abs/2506.18957)
*Sheraz Khan,Subha Madhavan,Kannan Natarajan*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The recent work by Shojaee et al. (2025), titled The Illusion of Thinking:
Understanding the Strengths and Limitations of Reasoning Models via the Lens of
Problem Complexity, presents a compelling empirical finding, a reasoning cliff,
where the performance of Large Reasoning Models (LRMs) collapses beyond a
specific complexity threshold, which the authors posit as an intrinsic scaling
limitation of Chain-of-Thought (CoT) reasoning. This commentary, while
acknowledging the study's methodological rigor, contends that this conclusion
is confounded by experimental artifacts. We argue that the observed failure is
not evidence of a fundamental cognitive boundary, but rather a predictable
outcome of system-level constraints in the static, text-only evaluation
paradigm, including tool use restrictions, context window recall issues, the
absence of crucial cognitive baselines, inadequate statistical reporting, and
output generation limits. We reframe this performance collapse through the lens
of an agentic gap, asserting that the models are not failing at reasoning, but
at execution within a profoundly restrictive interface. We empirically
substantiate this critique by demonstrating a striking reversal. A model,
initially declaring a puzzle impossible when confined to text-only generation,
now employs agentic tools to not only solve it but also master variations of
complexity far beyond the reasoning cliff it previously failed to surmount.
Additionally, our empirical analysis of tool-enabled models like o4-mini and
GPT-4o reveals a hierarchy of agentic reasoning, from simple procedural
execution to complex meta-cognitive self-correction, which has significant
implications for how we define and measure machine intelligence. The illusion
of thinking attributed to LRMs is less a reasoning deficit and more a
consequence of an otherwise capable mind lacking the tools for action.

</details>


### [71] [Bayesian Evolutionary Swarm Architecture: A Formal Epistemic System Grounded in Truth-Based Competition](https://arxiv.org/abs/2506.19191)
*Craig Steven Wright*

Main category: cs.AI

TL;DR: 提出基于贝叶斯推断与群体动力学的概率代理人工智能框架，通过竞争机制实现知识进化


<details>
  <summary>Details</summary>
Motivation: 建立数学上严格的人工智能系统，使真理成为进化吸引子，验证知识产生于对抗性认知压力

Method: 结合贝叶斯推断（信念更新）、测度论（一致性保证）、群体动态（代理评级/灭绝机制）、哈希加密身份和因果推断算子

Result: 系统具有可证明的收敛性、鲁棒性和进化稳定性，验证知识通过计算可实现的自调节群体涌现

Conclusion: 结构化对抗压力与进化选择可生成可验证知识，为构建自我修正的AI系统提供数学基础

Abstract: We introduce a mathematically rigorous framework for an artificial
intelligence system composed of probabilistic agents evolving through
structured competition and belief revision. The architecture, grounded in
Bayesian inference, measure theory, and population dynamics, defines agent
fitness as a function of alignment with a fixed external oracle representing
ground truth. Agents compete in a discrete-time environment, adjusting
posterior beliefs through observed outcomes, with higher-rated agents
reproducing and lower-rated agents undergoing extinction. Ratings are updated
via pairwise truth-aligned utility comparisons, and belief updates preserve
measurable consistency and stochastic convergence. We introduce hash-based
cryptographic identity commitments to ensure traceability, alongside causal
inference operators using do-calculus. Formal theorems on convergence,
robustness, and evolutionary stability are provided. The system establishes
truth as an evolutionary attractor, demonstrating that verifiable knowledge
arises from adversarial epistemic pressure within a computable, self-regulating
swarm.

</details>


### [72] [Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in LLMs](https://arxiv.org/abs/2506.19290)
*Liang Zeng,Yongcong Li,Yuzhen Xiao,Changshi Li,Chris Yuhao Liu,Rui Yan,Tianwen Wei,Jujie He,Xuchen Song,Yang Liu,Yahui Zhou*

Main category: cs.AI

TL;DR: 提出自动化SWE数据集构建流程Skywork-SWE，在32B参数模型上实现47%的SWE-bench测试准确率（SOTA）


<details>
  <summary>Details</summary>
Motivation: 解决现有SWE数据集规模小（数千实例）、依赖人工标注和环境配置的问题，提升LLM在软件工程任务中的持续迭代和长上下文处理能力

Method: 构建自动化数据流水线：从2,531个GitHub仓库收集10,169个Python任务实例，配套自然语言任务说明和运行时环境，生成8,000+已验证训练轨迹

Result: Skywork-SWE-32B模型在SWE-bench测试集上达到38.0% pass@1（基线），结合测试时优化技术提升至47.0%，超越同规模模型SOTA

Conclusion: 通过自动化数据扩展显著提升LLM软件工程能力，模型性能随数据规模持续增长未现饱和，开源模型加速领域研究

Abstract: Software engineering (SWE) has recently emerged as a crucial testbed for
next-generation LLM agents, demanding inherent capabilities in two critical
dimensions: sustained iterative problem-solving (e.g., >50 interaction rounds)
and long-context dependency resolution (e.g., >32k tokens). However, the data
curation process in SWE remains notoriously time-consuming, as it heavily
relies on manual annotation for code file filtering and the setup of dedicated
runtime environments to execute and validate unit tests. Consequently, most
existing datasets are limited to only a few thousand GitHub-sourced instances.
To this end, we propose an incremental, automated data-curation pipeline that
systematically scales both the volume and diversity of SWE datasets. Our
dataset comprises 10,169 real-world Python task instances from 2,531 distinct
GitHub repositories, each accompanied by a task specified in natural language
and a dedicated runtime-environment image for automated unit-test validation.
We have carefully curated over 8,000 successfully runtime-validated training
trajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE
model on these trajectories, we uncover a striking data scaling phenomenon: the
trained model's performance for software engineering capabilities in LLMs
continues to improve as the data size increases, showing no signs of
saturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on
the SWE-bench Verified benchmark without using verifiers or multiple rollouts,
establishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based
LLMs built on the OpenHands agent framework. Furthermore, with the
incorporation of test-time scaling techniques, the performance further improves
to 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter
models. We release the Skywork-SWE-32B model checkpoint to accelerate future
research.

</details>


### [73] [NaviAgent: Bilevel Planning on Tool Dependency Graphs for Function Calling](https://arxiv.org/abs/2506.19500)
*Yan Jiang,Hao Zhou,LiZhong GU,Ai Han,TianLong Li*

Main category: cs.AI

TL;DR: 提出NaviAgent架构，通过双层规划（多路径决策器+图编码导航器）显著提升工具链编排的成功率和效率


<details>
  <summary>Details</summary>
Motivation: 现有LLM工具调用方法依赖刚性单路径执行，存在错误恢复差和搜索空间爆炸问题，难以应对复杂异构工具链的规模化编排需求

Method: 1. 多路径决策器构建四维决策空间动态选择最优动作；2. 图编码导航器构建工具依赖异构图(TDHG)，融合API结构特征与历史调用行为的节点嵌入，集成启发式搜索策略

Result: 在多个基础模型上任务成功率(TSR)平均提升13.5%-19%，复杂任务中图编码导航器为大模型带来最高9个点的TSR提升，执行效率与最优基线仅差1步

Conclusion: NaviAgent通过创新的图结构编码和双层决策机制，在保证效率的同时显著提升工具链编排成功率，其中图编码导航器对大模型性能提升尤为关键

Abstract: LLMs' reliance on static knowledge and fragile tool invocation severely
hinders the orchestration of complex, heterogeneous toolchains, particularly at
large scales. Existing methods typically use rigid single-path execution,
resulting in poor error recovery and exponentially growing search spaces. We
introduce NaviAgent, a graph-navigated bilevel planning architecture for robust
function calling, comprising a Multi-Path Decider and Graph-Encoded Navigator.
As an LLM-powered agent, the Multi-Path Decider defines a four-dimensional
decision space and continuously perceives environmental states, dynamically
selecting the optimal action to fully cover all tool invocation scenarios. The
Graph-Encoded Navigator constructs a Tool Dependency Heterogeneous Graph
(TDHG), where node embeddings explicitly fuse API schema structure with
historical invocation behavior. It also integrates a novel heuristic search
strategy that guides the Decider toward efficient and highly successful
toolchains, even for unseen tool combinations. Experiments show that NaviAgent
consistently achieves the highest task success rate (TSR) across all foundation
models and task complexities, outperforming the average baselines (ReAct,
ToolLLM, {\alpha}-UMI) by 13.5%, 16.4%, and 19.0% on Qwen2.5-14B, Qwen2.5-32B,
and Deepseek-V3, respectively. Its execution steps are typically within one
step of the most efficient baseline, ensuring a strong balance between quality
and efficiency. Notably, a fine-tuned Qwen2.5-14B model achieves a TSR of
49.5%, surpassing the much larger 32B model (44.9%) under our architecture.
Incorporating the Graph-Encoded Navigator further boosts TSR by an average of
2.4 points, with gains up over 9 points on complex tasks for larger models
(Deepseek-V3 and GPT-4o), highlighting its essential role in toolchain
orchestration.

</details>


### [74] [KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality](https://arxiv.org/abs/2506.19807)
*Baochang Ren,Shuofei Qiao,Wenhao Yu,Huajun Chen,Ningyu Zhang*

Main category: cs.AI

TL;DR: KnowRL方法通过在强化学习训练中引入基于知识验证的事实性奖励机制，有效减少慢思考模型的幻觉现象并保持推理能力


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型慢思考过程中因无法识别知识边界导致的严重幻觉问题，以及现有强化学习方法缺乏思维过程事实监督的缺陷

Method: 将知识验证模块与强化学习结合，设计事实性奖励函数对推理步骤进行直接监督，使模型学习基于事实的推理策略

Result: 在三个幻觉评估数据集和两个推理数据集上验证了方法有效性，在降低幻觉率的同时保持了原有推理能力

Conclusion: KnowRL通过事实驱动的强化学习框架，为提升AI模型推理可靠性提供了新思路，推动了可信慢思考模型的发展

Abstract: Large Language Models (LLMs), particularly slow-thinking models, often
exhibit severe hallucination, outputting incorrect content due to an inability
to accurately recognize knowledge boundaries during reasoning. While
Reinforcement Learning (RL) can enhance complex reasoning abilities, its
outcome-oriented reward mechanism often lacks factual supervision over the
thinking process, further exacerbating the hallucination problem. To address
the high hallucination in slow-thinking models, we propose Knowledge-enhanced
RL, KnowRL. KnowRL guides models to perform fact-based slow thinking by
integrating a factuality reward, based on knowledge verification, into the RL
training process, helping them recognize their knowledge boundaries. KnowRL
guides models to perform fact-based slow thinking by integrating a factuality
reward, based on knowledge verification, into the RL training process, helping
them recognize their knowledge boundaries. This targeted factual input during
RL training enables the model to learn and internalize fact-based reasoning
strategies. By directly rewarding adherence to facts within the reasoning
steps, KnowRL fosters a more reliable thinking process. Experimental results on
three hallucination evaluation datasets and two reasoning evaluation datasets
demonstrate that KnowRL effectively mitigates hallucinations in slow-thinking
models while maintaining their original strong reasoning capabilities. Our code
is available at https://github.com/zjunlp/KnowRL.

</details>


### [75] [Evaluating Compliance with Visualization Guidelines in Diagrams for Scientific Publications Using Large Vision Language Models](https://arxiv.org/abs/2506.19825)
*Johannes Rückert,Louise Bloch,Christoph M. Friedrich*

Main category: cs.AI

TL;DR: 研究通过视觉语言模型（VLMs）自动检测图表中的可视化原则违规问题，验证模型在识别图表类型、3D效果、轴标签等方面的有效性，并确定最佳模型Qwen2.5VL和总结式提示策略


<details>
  <summary>Details</summary>
Motivation: 学术图表常因不遵守可视化原则导致信息误导，需自动化工具帮助研究者快速识别图表设计问题

Method: 采用5个开源VLMs和5种提示策略，基于数据可视化准则构建问题集进行系统性评测

Result: 模型在图表类型（F1 82.49%）、3D效果（98.55%）、颜色（RMSE 1.60）和图例（96.64%）检测表现优异，但在图像质量（0.74%）和刻度标记（46.13%）检测失效

Conclusion: VLMs可有效识别常见图表设计缺陷（如缺失轴标签、冗余3D效果），该方法可扩展应用于更广泛的可视化质量评估场景

Abstract: Diagrams are widely used to visualize data in publications. The research
field of data visualization deals with defining principles and guidelines for
the creation and use of these diagrams, which are often not known or adhered to
by researchers, leading to misinformation caused by providing inaccurate or
incomplete information.
  In this work, large Vision Language Models (VLMs) are used to analyze
diagrams in order to identify potential problems in regards to selected data
visualization principles and guidelines. To determine the suitability of VLMs
for these tasks, five open source VLMs and five prompting strategies are
compared using a set of questions derived from selected data visualization
guidelines.
  The results show that the employed VLMs work well to accurately analyze
diagram types (F1-score 82.49 %), 3D effects (F1-score 98.55 %), axes labels
(F1-score 76.74 %), lines (RMSE 1.16), colors (RMSE 1.60) and legends (F1-score
96.64 %, RMSE 0.70), while they cannot reliably provide feedback about the
image quality (F1-score 0.74 %) and tick marks/labels (F1-score 46.13 %). Among
the employed VLMs, Qwen2.5VL performs best, and the summarizing prompting
strategy performs best for most of the experimental questions.
  It is shown that VLMs can be used to automatically identify a number of
potential issues in diagrams, such as missing axes labels, missing legends, and
unnecessary 3D effects. The approach laid out in this work can be extended for
further aspects of data visualization.

</details>
