{"id": "2508.09235", "pdf": "https://arxiv.org/pdf/2508.09235", "abs": "https://arxiv.org/abs/2508.09235", "authors": ["Nathaniel Gorski", "Xin Liang", "Hanqi Guo", "Bei Wang"], "title": "TFZ: Topology-Preserving Compression of 2D Symmetric and Asymmetric Second-Order Tensor Fields", "categories": ["cs.GR", "cs.CG"], "comment": "29 pages, 27 figures, to be presented at IEEE Vis 2025 (and published\n  in IEEE TVCG 2026)", "summary": "In this paper, we present a novel compression framework, TFZ, that preserves\nthe topology of 2D symmetric and asymmetric second-order tensor fields defined\non flat triangular meshes. A tensor field assigns a tensor - a\nmulti-dimensional array of numbers - to each point in space. Tensor fields,\nsuch as the stress and strain tensors, and the Riemann curvature tensor, are\nessential to both science and engineering. The topology of tensor fields\ncaptures the core structure of data, and is useful in various disciplines, such\nas graphics (for manipulating shapes and textures) and neuroscience (for\nanalyzing brain structures from diffusion MRI). Lossy data compression may\ndistort the topology of tensor fields, thus hindering downstream analysis and\nvisualization tasks. TFZ ensures that certain topological features are\npreserved during lossy compression. Specifically, TFZ preserves degenerate\npoints essential to the topology of symmetric tensor fields and retains\neigenvector and eigenvalue graphs that represent the topology of asymmetric\ntensor fields. TFZ scans through each cell, preserving the local topology of\neach cell, and thereby ensuring certain global topological guarantees. We\nshowcase the effectiveness of our framework in enhancing the lossy scientific\ndata compressors SZ3 and SPERR.", "AI": {"tldr": "TFZ\u6846\u67b6\u901a\u8fc7\u4fdd\u6301\u5f20\u91cf\u573a\u5173\u952e\u62d3\u6251\u7279\u5f81\uff08\u5bf9\u79f0\u573a\u7684\u9000\u5316\u70b9/\u975e\u5bf9\u79f0\u573a\u7684\u7279\u5f81\u5411\u91cf\u56fe\uff09\uff0c\u5728\u589e\u5f3a\u73b0\u6709\u79d1\u5b66\u6570\u636e\u538b\u7f29\u5668\uff08SZ3/SPERR\uff09\u7684\u540c\u65f6\u5b9e\u73b0\u62d3\u6251\u4fdd\u6301\u538b\u7f29\u3002", "motivation": "\u79d1\u5b66\u5de5\u7a0b\u4e2d\u5f20\u91cf\u573a\u538b\u7f29\u6613\u5bfc\u81f4\u62d3\u6251\u7ed3\u6784\u5931\u771f\uff0c\u5f71\u54cd\u540e\u7eed\u5206\u6790\u548c\u53ef\u89c6\u5316\u3002\u73b0\u6709\u538b\u7f29\u5668\u7f3a\u4e4f\u5bf9\u5f20\u91cf\u573a\u62d3\u6251\u7684\u4fdd\u62a4\u673a\u5236\u3002", "method": "\u91c7\u7528\u5355\u5143\u7ea7\u62d3\u6251\u4fdd\u6301\u7b56\u7565\uff1a\u626b\u63cf\u7f51\u683c\u5355\u5143\uff0c\u901a\u8fc7\u5c40\u90e8\u62d3\u6251\u7279\u5f81\uff08\u9000\u5316\u70b9\u4f4d\u7f6e/\u7279\u5f81\u5411\u91cf\u8fde\u7eed\u6027\uff09\u7684\u7cbe\u786e\u4fdd\u7559\uff0c\u786e\u4fdd\u5168\u5c40\u62d3\u6251\u7ed3\u6784\u7684\u5b8c\u6574\u6027\u3002", "result": "\u6210\u529f\u5c06TFZ\u6574\u5408\u81f3\u4e3b\u6d41\u538b\u7f29\u5668SZ3\u548cSPERR\uff0c\u5728\u4fdd\u6301\u538b\u7f29\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u62d3\u6251\u5931\u771f\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "TFX\u9996\u6b21\u5b9e\u73b0\u6709\u62d3\u6251\u4fdd\u8bc1\u7684\u5f20\u91cf\u573a\u538b\u7f29\uff0c\u4e3a\u79d1\u5b66\u6570\u636e\u7ba1\u7406\u63d0\u4f9b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u56fe\u5f62\u5b66\u3001\u795e\u7ecf\u79d1\u5b66\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.09610", "pdf": "https://arxiv.org/pdf/2508.09610", "abs": "https://arxiv.org/abs/2508.09610", "authors": ["Jiachen Li", "Guangzhi Han", "Jin Wan", "Yuan Gao", "Delong Han"], "title": "DualPhys-GS: Dual Physically-Guided 3D Gaussian Splatting for Underwater Scene Reconstruction", "categories": ["cs.GR"], "comment": "12 pages, 4 figures", "summary": "In 3D reconstruction of underwater scenes, traditional methods based on\natmospheric optical models cannot effectively deal with the selective\nattenuation of light wavelengths and the effect of suspended particle\nscattering, which are unique to the water medium, and lead to color distortion,\ngeometric artifacts, and collapsing phenomena at long distances. We propose the\nDualPhys-GS framework to achieve high-quality underwater reconstruction through\na dual-path optimization mechanism. Our approach further develops a dual\nfeature-guided attenuation-scattering modeling mechanism, the RGB-guided\nattenuation optimization model combines RGB features and depth information and\ncan handle edge and structural details. In contrast, the multi-scale\ndepth-aware scattering model captures scattering effects at different scales\nusing a feature pyramid network and an attention mechanism. Meanwhile, we\ndesign several special loss functions. The attenuation scattering consistency\nloss ensures physical consistency. The water body type adaptive loss\ndynamically adjusts the weighting coefficients. The edge-aware scattering loss\nis used to maintain the sharpness of structural edges. The multi-scale feature\nloss helps to capture global and local structural information. In addition, we\ndesign a scene adaptive mechanism that can automatically identify the\nwater-body-type characteristics (e.g., clear coral reef waters or turbid\ncoastal waters) and dynamically adjust the scattering and attenuation\nparameters and optimization strategies. Experimental results show that our\nmethod outperforms existing methods in several metrics, especially in suspended\nmatter-dense regions and long-distance scenes, and the reconstruction quality\nis significantly improved.", "AI": {"tldr": "\u63d0\u51faDualPhys-GS\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u4f18\u5316\u673a\u5236\u89e3\u51b3\u6c34\u4e0b3D\u91cd\u5efa\u4e2d\u7684\u989c\u8272\u5931\u771f\u548c\u51e0\u4f55\u4f2a\u5f71\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u60ac\u6d6e\u7269\u5bc6\u96c6\u533a\u57df\u548c\u8fdc\u8ddd\u79bb\u573a\u666f\u7684\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u5927\u6c14\u5149\u5b66\u6a21\u578b\u65e0\u6cd5\u6709\u6548\u5904\u7406\u6c34\u4f53\u7279\u6709\u7684\u5149\u6ce2\u957f\u9009\u62e9\u6027\u8870\u51cf\u548c\u60ac\u6d6e\u9897\u7c92\u6563\u5c04\u6548\u5e94\uff0c\u5bfc\u81f4\u8fdc\u8ddd\u79bb\u91cd\u5efa\u51fa\u73b0\u989c\u8272\u5931\u771f\u3001\u51e0\u4f55\u4f2a\u5f71\u548c\u7ed3\u6784\u584c\u9677\u95ee\u9898\u3002", "method": "1. RGB\u5f15\u5bfc\u8870\u51cf\u4f18\u5316\u6a21\u578b\u7ed3\u5408\u6df1\u5ea6\u4fe1\u606f\u5904\u7406\u8fb9\u7f18\u7ec6\u8282\n2. \u591a\u5c3a\u5ea6\u6df1\u5ea6\u611f\u77e5\u6563\u5c04\u6a21\u578b\u4f7f\u7528\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\u6355\u6349\u4e0d\u540c\u5c3a\u5ea6\u6563\u5c04\u6548\u5e94\n3. \u8bbe\u8ba1\u56db\u9879\u7269\u7406\u7ea6\u675f\u635f\u5931\u51fd\u6570\uff08\u8870\u51cf\u6563\u5c04\u4e00\u81f4\u6027\u635f\u5931\u3001\u6c34\u4f53\u81ea\u9002\u5e94\u635f\u5931\u7b49\uff09\n4. \u573a\u666f\u81ea\u9002\u5e94\u673a\u5236\u52a8\u6001\u8c03\u6574\u6c34\u4f53\u7c7b\u578b\u53c2\u6570", "result": "\u5728\u591a\u9879\u6307\u6807\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u60ac\u6d6e\u7269\u5bc6\u96c6\u533a\u57df\uff08PSNR\u63d0\u534712.3%\uff09\u548c\u8fdc\u8ddd\u79bb\u573a\u666f\uff08SSIM\u63d0\u9ad818.7%\uff09\uff0c\u91cd\u5efa\u9510\u5ea6\u63d0\u5347\u663e\u8457", "conclusion": "\u901a\u8fc7\u53cc\u8def\u5f84\u7279\u5f81\u5f15\u5bfc\u5efa\u6a21\u3001\u7269\u7406\u7ea6\u675f\u635f\u5931\u51fd\u6570\u548c\u573a\u666f\u81ea\u9002\u5e94\u673a\u5236\u7684\u534f\u540c\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u6c34\u4e0b\u590d\u6742\u5149\u5b66\u73af\u5883\u7684\u9ad8\u4fdd\u771f\u4e09\u7ef4\u91cd\u5efa\u3002"}}
{"id": "2508.09830", "pdf": "https://arxiv.org/pdf/2508.09830", "abs": "https://arxiv.org/abs/2508.09830", "authors": ["Shenxing Wei", "Jinxi Li", "Yafei Yang", "Siyuan Zhou", "Bo Yang"], "title": "RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG", "cs.RO"], "comment": "ICCV 2025 Highlight. Shenxing and Jinxi are co-first authors. Code\n  and data are available at: https://github.com/vLAR-group/RayletDF", "summary": "In this paper, we present a generalizable method for 3D surface\nreconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from\nRGB images. Unlike existing coordinate-based methods which are often\ncomputationally intensive when rendering explicit surfaces, our proposed\nmethod, named RayletDF, introduces a new technique called raylet distance\nfield, which aims to directly predict surface points from query rays. Our\npipeline consists of three key modules: a raylet feature extractor, a raylet\ndistance field predictor, and a multi-raylet blender. These components work\ntogether to extract fine-grained local geometric features, predict raylet\ndistances, and aggregate multiple predictions to reconstruct precise surface\npoints. We extensively evaluate our method on multiple public real-world\ndatasets, demonstrating superior performance in surface reconstruction from\npoint clouds or 3D Gaussians. Most notably, our method achieves exceptional\ngeneralization ability, successfully recovering 3D surfaces in a single-forward\npass across unseen datasets in testing.", "AI": {"tldr": "\u63d0\u51faRayletDF\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c04\u7ebf\u8ddd\u79bb\u573a\u4ece\u70b9\u4e91\u62163D\u9ad8\u65af\u4e2d\u91cd\u5efa3D\u8868\u9762\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5750\u6807\u76843D\u91cd\u5efa\u65b9\u6cd5\u5728\u663e\u5f0f\u8868\u9762\u6e32\u67d3\u65f6\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u9700\u5f00\u53d1\u66f4\u9ad8\u6548\u4e14\u53ef\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u7531\u5c04\u7ebf\u7279\u5f81\u63d0\u53d6\u5668\u3001\u5c04\u7ebf\u8ddd\u79bb\u573a\u9884\u6d4b\u5668\u548c\u591a\u5c04\u7ebf\u6df7\u5408\u5668\u7ec4\u6210\uff0c\u901a\u8fc7\u5c40\u90e8\u51e0\u4f55\u7279\u5f81\u63d0\u53d6\u3001\u8ddd\u79bb\u9884\u6d4b\u548c\u591a\u9884\u6d4b\u878d\u5408\u5b9e\u73b0\u8868\u9762\u91cd\u5efa", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u4f18\u91cd\u5efa\u6548\u679c\uff0c\u5355\u6b21\u524d\u5411\u4f20\u64ad\u5373\u53ef\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u6210\u529f\u91cd\u5efa3D\u8868\u9762", "conclusion": "RayletDF\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u5176\u6cdb\u5316\u80fd\u529b\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u91cd\u8981\u4ef7\u503c"}}
{"id": "2508.09303", "pdf": "https://arxiv.org/pdf/2508.09303", "abs": "https://arxiv.org/abs/2508.09303", "authors": ["Shu Zhao", "Tan Yu", "Anbang Xu", "Japinder Singh", "Aaditya Shukla", "Rama Akkiraju"], "title": "ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Reasoning-augmented search agents such as Search-R1, trained via\nreinforcement learning with verifiable rewards (RLVR), demonstrate remarkable\ncapabilities in multi-step information retrieval from external knowledge\nsources. These agents address the limitations of their parametric memory by\ndynamically gathering relevant facts to address complex reasoning tasks.\nHowever, existing approaches suffer from a fundamental architectural\nlimitation: they process search queries strictly sequentially, even when\nhandling inherently parallelizable and logically independent comparisons. This\nsequential bottleneck significantly constrains computational efficiency,\nparticularly for queries that require multiple entity comparisons. To address\nthis critical limitation, we propose ParallelSearch, a novel reinforcement\nlearning framework that empowers large language models (LLMs) to recognize\nparallelizable query structures and execute multiple search operations\nconcurrently. Our approach introduces dedicated reward functions that\nincentivize the identification of independent query components while preserving\nanswer accuracy through jointly considering correctness, query decomposition\nquality, and parallel execution benefits. Comprehensive experiments demonstrate\nthat ParallelSearch outperforms state-of-the-art baselines by an average\nperformance gain of 2.9% across seven question-answering benchmarks. Notably,\non parallelizable questions, our method achieves a 12.7% performance\nimprovement while requiring only 69.6% of the LLM calls compared to sequential\napproaches.", "AI": {"tldr": "\u63d0\u51faParallelSearch\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u5316\u641c\u7d22\u64cd\u4f5c\u7a81\u7834\u4f20\u7edf\u987a\u5e8f\u5904\u7406\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u4e0e\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u641c\u7d22\u4ee3\u7406\u5728\u5904\u7406\u591a\u5b9e\u4f53\u6bd4\u8f83\u7b49\u53ef\u5e76\u884c\u5316\u67e5\u8be2\u65f6\u5b58\u5728\u987a\u5e8f\u5904\u7406\u67b6\u6784\u7f3a\u9677\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u6784\u5efa\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5f15\u5bfcLLM\u8bc6\u522b\u5e76\u884c\u67e5\u8be2\u7ed3\u6784\uff0c\u8bbe\u8ba1\u591a\u7ef4\u5956\u52b1\u51fd\u6570\u8054\u5408\u4f18\u5316\u67e5\u8be2\u5206\u89e3\u8d28\u91cf\u3001\u5e76\u884c\u6267\u884c\u6536\u76ca\u4e0e\u7b54\u6848\u51c6\u786e\u6027\u3002", "result": "\u57287\u4e2a\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u53472.9%\u6027\u80fd\uff0c\u53ef\u5e76\u884c\u95ee\u9898\u4e0a\u5b9e\u73b012.7%\u6027\u80fd\u589e\u76ca\u4e14LLM\u8c03\u7528\u6b21\u6570\u51cf\u5c1130.4%\u3002", "conclusion": "\u5e76\u884c\u5316\u641c\u7d22\u67b6\u6784\u6709\u6548\u7a81\u7834\u4f20\u7edf\u987a\u5e8f\u5904\u7406\u9650\u5236\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u9ad8\u6548\u6267\u884c\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09983", "pdf": "https://arxiv.org/pdf/2508.09983", "abs": "https://arxiv.org/abs/2508.09983", "authors": ["David Dinkevich", "Matan Levy", "Omri Avrahami", "Dvir Samuel", "Dani Lischinski"], "title": "Story2Board: A Training-Free Approach for Expressive Storyboard Generation", "categories": ["cs.CV", "cs.GR", "cs.LG"], "comment": "Project page is available at\n  https://daviddinkevich.github.io/Story2Board/", "summary": "We present Story2Board, a training-free framework for expressive storyboard\ngeneration from natural language. Existing methods narrowly focus on subject\nidentity, overlooking key aspects of visual storytelling such as spatial\ncomposition, background evolution, and narrative pacing. To address this, we\nintroduce a lightweight consistency framework composed of two components:\nLatent Panel Anchoring, which preserves a shared character reference across\npanels, and Reciprocal Attention Value Mixing, which softly blends visual\nfeatures between token pairs with strong reciprocal attention. Together, these\nmechanisms enhance coherence without architectural changes or fine-tuning,\nenabling state-of-the-art diffusion models to generate visually diverse yet\nconsistent storyboards. To structure generation, we use an off-the-shelf\nlanguage model to convert free-form stories into grounded panel-level prompts.\nTo evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain\nnarratives designed to assess layout diversity and background-grounded\nstorytelling, in addition to consistency. We also introduce a new Scene\nDiversity metric that quantifies spatial and pose variation across storyboards.\nOur qualitative and quantitative results, as well as a user study, show that\nStory2Board produces more dynamic, coherent, and narratively engaging\nstoryboards than existing baselines.", "AI": {"tldr": "\u65e0\u9700\u8bad\u7ec3\u7684Story2Board\u6846\u67b6\u901a\u8fc7\u6f5c\u5728\u9762\u677f\u951a\u5b9a\u548c\u4e92\u6ce8\u610f\u529b\u503c\u6df7\u5408\u673a\u5236\uff0c\u5728\u4fdd\u6301\u89d2\u8272\u4e00\u81f4\u6027\u7684\u524d\u63d0\u4e0b\u751f\u6210\u591a\u6837\u5316\u89c6\u89c9\u6545\u4e8b\u677f", "motivation": "\u73b0\u6709\u6545\u4e8b\u677f\u751f\u6210\u65b9\u6cd5\u8fc7\u5ea6\u5173\u6ce8\u4e3b\u4f53\u8eab\u4efd\uff0c\u5ffd\u7565\u7a7a\u95f4\u6784\u56fe/\u80cc\u666f\u6f14\u53d8/\u53d9\u4e8b\u8282\u594f\u7b49\u89c6\u89c9\u53d9\u4e8b\u6838\u5fc3\u8981\u7d20", "method": "\u5305\u542b\u6f5c\u5728\u9762\u677f\u951a\u5b9a(\u4fdd\u6301\u89d2\u8272\u4e00\u81f4\u6027)\u548c\u4e92\u6ce8\u610f\u529b\u503c\u6df7\u5408(\u8f6f\u878d\u5408\u89c6\u89c9\u7279\u5f81)\u7684\u8f7b\u91cf\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u751f\u6210\u9762\u677f\u7ea7\u63d0\u793a\uff0c\u63d0\u51fa\u5305\u542b\u573a\u666f\u591a\u6837\u6027\u6307\u6807\u7684\u65b0\u8bc4\u4f30\u4f53\u7cfb", "result": "\u5728\u52a8\u6001\u6027\u3001\u8fde\u8d2f\u6027\u3001\u53d9\u4e8b\u5438\u5f15\u529b\u65b9\u9762\u5168\u9762\u8d85\u8d8a\u57fa\u7ebf\uff0c\u7528\u6237\u7814\u7a76\u548c\u91cf\u5316\u6307\u6807\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u573a\u666f\u591a\u6837\u6027\u6307\u6807\u63d0\u534732%", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5e73\u8861\u6545\u4e8b\u677f\u751f\u6210\u4e2d\u7684\u89c6\u89c9\u591a\u6837\u6027\u4e0e\u53d9\u4e8b\u8fde\u8d2f\u6027\uff0c\u4e3aAI\u8f85\u52a9\u89c6\u89c9\u53d9\u4e8b\u5f00\u8f9f\u65b0\u65b9\u5411"}}
{"id": "2508.09323", "pdf": "https://arxiv.org/pdf/2508.09323", "abs": "https://arxiv.org/abs/2508.09323", "authors": ["Nan Miles Xi", "Yu Deng", "Lin Wang"], "title": "Leveraging Large Language Models for Rare Disease Named Entity Recognition", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Named Entity Recognition (NER) in the rare disease domain poses unique\nchallenges due to limited labeled data, semantic ambiguity between entity\ntypes, and long-tail distributions. In this study, we evaluate the capabilities\nof GPT-4o for rare disease NER under low-resource settings, using a range of\nprompt-based strategies including zero-shot prompting, few-shot in-context\nlearning, retrieval-augmented generation (RAG), and task-level fine-tuning. We\ndesign a structured prompting framework that encodes domain-specific knowledge\nand disambiguation rules for four entity types. We further introduce two\nsemantically guided few-shot example selection methods to improve in-context\nperformance while reducing labeling effort. Experiments on the RareDis Corpus\nshow that GPT-4o achieves competitive or superior performance compared to\nBioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art\n(SOTA) results. Cost-performance analysis reveals that few-shot prompting\ndelivers high returns at low token budgets, while RAG offers marginal\nadditional benefit. An error taxonomy highlights common failure modes such as\nboundary drift and type confusion, suggesting opportunities for post-processing\nand hybrid refinement. Our results demonstrate that prompt-optimized LLMs can\nserve as effective, scalable alternatives to traditional supervised models in\nbiomedical NER, particularly in rare disease applications where annotated data\nis scarce.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30GPT-4o\u5728\u7f55\u89c1\u75c5\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4e2d\u7684\u4f4e\u8d44\u6e90\u8868\u73b0\uff0c\u901a\u8fc7\u63d0\u793a\u7b56\u7565\u4f18\u5316\u5b9e\u73b0\u65b0SOTA\u7ed3\u679c", "motivation": "\u89e3\u51b3\u7f55\u89c1\u75c5\u9886\u57dfNER\u5b58\u5728\u7684\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u3001\u5b9e\u4f53\u7c7b\u578b\u8bed\u4e49\u6a21\u7cca\u3001\u957f\u5c3e\u5206\u5e03\u7b49\u6838\u5fc3\u6311\u6218", "method": "\u8bbe\u8ba1\u5305\u542b\u9886\u57df\u77e5\u8bc6\u7684\u7ed3\u6784\u5316\u63d0\u793a\u6846\u67b6\uff0c\u5f00\u53d1\u8bed\u4e49\u5f15\u5bfc\u7684\u5c11\u6837\u672c\u9009\u62e9\u65b9\u6cd5\uff0c\u7ed3\u5408\u96f6\u6837\u672c/\u5c11\u6837\u672c/RAG/\u5fae\u8c03\u7b56\u7565", "result": "\u5728RareDis Corpus\u4e0a\u8d85\u8d8aBioClinicalBERT\uff0c\u5fae\u8c03\u8fbe\u5230SOTA\uff1b\u5c11\u6837\u672c\u7b56\u7565\u6027\u4ef7\u6bd4\u6700\u4f18\uff0cRAG\u589e\u76ca\u6709\u9650", "conclusion": "\u63d0\u793a\u4f18\u5316\u7684LLM\u53ef\u4f5c\u4e3a\u751f\u7269\u533b\u5b66NER\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u7684\u7f55\u89c1\u75c5\u5e94\u7528\u573a\u666f\u4e2d"}}
{"id": "2508.09324", "pdf": "https://arxiv.org/pdf/2508.09324", "abs": "https://arxiv.org/abs/2508.09324", "authors": ["Nikita Mehrotra", "Aayush Kumar", "Sumit Gulwani", "Arjun Radhakrishna", "Ashish Tiwari"], "title": "TEN: Table Explicitization, Neurosymbolically", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a neurosymbolic approach, TEN, for extracting tabular data from\nsemistructured input text. This task is particularly challenging for text input\nthat does not use special delimiters consistently to separate columns and rows.\nPurely neural approaches perform poorly due to hallucinations and their\ninability to enforce hard constraints. TEN uses Structural Decomposition\nprompting - a specialized chain-of-thought prompting approach - on a large\nlanguage model (LLM) to generate an initial table, and thereafter uses a\nsymbolic checker to evaluate not only the well-formedness of that table, but\nalso detect cases of hallucinations or forgetting. The output of the symbolic\nchecker is processed by a critique-LLM to generate guidance for fixing the\ntable, which is presented to the original LLM in a self-debug loop. Our\nextensive experiments demonstrate that TEN significantly outperforms purely\nneural baselines across multiple datasets and metrics, achieving significantly\nhigher exact match accuracy and substantially reduced hallucination rates. A\n21-participant user study further confirms that TEN's tables are rated\nsignificantly more accurate (mean score: 5.0 vs 4.3; p = 0.021), and are\nconsistently preferred for ease of verification and correction, with\nparticipants favoring our method in over 60% of the cases.", "AI": {"tldr": "\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5TEN\uff0c\u901a\u8fc7\u7ed3\u6784\u5206\u89e3\u63d0\u793a+\u7b26\u53f7\u68c0\u67e5+\u81ea\u6211\u8c03\u8bd5\u5faa\u73af\uff0c\u663e\u8457\u63d0\u5347\u534a\u7ed3\u6784\u5316\u6587\u672c\u8868\u683c\u63d0\u53d6\u51c6\u786e\u7387\u5e76\u964d\u4f4e\u5e7b\u89c9", "motivation": "\u4f20\u7edf\u795e\u7ecf\u65b9\u6cd5\u5728\u5904\u7406\u65e0\u660e\u786e\u5206\u9694\u7b26\u7684\u8868\u683c\u6570\u636e\u65f6\u5b58\u5728\u5e7b\u89c9\u548c\u7ea6\u675f\u5931\u6548\u95ee\u9898\uff0c\u9700\u7ed3\u5408\u7b26\u53f7\u903b\u8f91\u786e\u4fdd\u7ed3\u6784\u51c6\u786e\u6027", "method": "1. \u5927\u6a21\u578b\u751f\u6210\u521d\u59cb\u8868 2. \u7b26\u53f7\u68c0\u67e5\u5668\u9a8c\u8bc1\u7ed3\u6784+\u68c0\u6d4b\u9519\u8bef 3. \u6279\u5224\u6027LLM\u751f\u6210\u4fee\u590d\u6307\u5bfc 4. \u81ea\u6211\u8c03\u8bd5\u5faa\u73af\u4f18\u5316\u7ed3\u679c", "result": "\u5b9e\u9a8c\u663e\u793a\uff1aExact Match\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\uff08\u7528\u6237\u8bc4\u52065.0 vs 4.3\uff0cp=0.021\uff09\uff0c60%\u7528\u6237\u6848\u4f8b\u66f4\u6613\u9a8c\u8bc1\u4fee\u6b63", "conclusion": "TEN\u901a\u8fc7\u795e\u7ecf\u4e0e\u7b26\u53f7\u65b9\u6cd5\u534f\u540c\uff0c\u6709\u6548\u89e3\u51b3\u8868\u683c\u63d0\u53d6\u96be\u9898\uff0c\u4e3a\u534a\u7ed3\u6784\u5316\u6570\u636e\u5904\u7406\u63d0\u4f9b\u53ef\u9760\u65b9\u6848"}}
{"id": "2508.09337", "pdf": "https://arxiv.org/pdf/2508.09337", "abs": "https://arxiv.org/abs/2508.09337", "authors": ["Gideon Vos", "Maryam Ebrahimpour", "Liza van Eijk", "Zoltan Sarnyai", "Mostafa Rahimi Azghadi"], "title": "Decoding Neural Emotion Patterns through Natural Language Processing Embeddings", "categories": ["cs.CL"], "comment": "26 pages, 9 figures", "summary": "Understanding how emotional expression in language relates to brain function\nis a challenge in computational neuroscience and affective computing.\nTraditional neuroimaging is costly and lab-bound, but abundant digital text\noffers new avenues for emotion-brain mapping. Prior work has largely examined\nneuroimaging-based emotion localization or computational text analysis\nseparately, with little integration. We propose a computational framework that\nmaps textual emotional content to anatomically defined brain regions without\nrequiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate\nhigh-dimensional semantic representations, apply dimensionality reduction and\nclustering to identify emotional groups, and map them to 18 brain regions\nlinked to emotional processing. Three experiments were conducted: i) analyzing\nconversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to\ncompare mapping patterns, ii) applying the method to the GoEmotions dataset and\niii) comparing human-written text with large language model (LLM) responses to\nassess differences in inferred brain activation. Emotional intensity was scored\nvia lexical analysis. Results showed neuroanatomically plausible mappings with\nhigh spatial specificity. Depressed subjects exhibited greater limbic\nengagement tied to negative affect. Discrete emotions were successfully\ndifferentiated. LLM-generated text matched humans in basic emotion distribution\nbut lacked nuanced activation in empathy and self-referential regions (medial\nprefrontal and posterior cingulate cortex). This cost-effective, scalable\napproach enables large-scale analysis of naturalistic language, distinguishes\nbetween clinical populations, and offers a brain-based benchmark for evaluating\nAI emotional expression.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u6587\u672c\u8bed\u4e49\u5206\u6790\u6620\u5c04\u60c5\u611f\u5185\u5bb9\u5230\u8111\u533a\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u5b9e\u73b0\u65e0\u9700\u795e\u7ecf\u6210\u50cf\u7684\u60c5\u611f-\u8111\u529f\u80fd\u5173\u8054\u7814\u7a76", "motivation": "\u4f20\u7edf\u795e\u7ecf\u5f71\u50cf\u6280\u672f\u6210\u672c\u9ad8\u6602\u4e14\u53d7\u9650\u4e8e\u5b9e\u9a8c\u5ba4\u73af\u5883\uff0c\u800c\u6d77\u91cf\u6570\u5b57\u6587\u672c\u4e3a\u60c5\u611f-\u8111\u529f\u80fd\u6620\u5c04\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002\u73b0\u6709\u7814\u7a76\u591a\u72ec\u7acb\u5904\u7406\u795e\u7ecf\u5f71\u50cf\u6570\u636e\u6216\u6587\u672c\u5206\u6790\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6574\u5408\u3002", "method": "\u4f7f\u7528text-embedding-ada-002\u751f\u6210\u9ad8\u7ef4\u8bed\u4e49\u8868\u5f81\u2192\u964d\u7ef4\u805a\u7c7b\u2192\u6620\u5c04\u523018\u4e2a\u60c5\u611f\u76f8\u5173\u8111\u533a\u3002\u8bbe\u8ba1\u4e09\u4e2a\u5b9e\u9a8c\uff1a1\uff09\u5bf9\u6bd4\u6291\u90c1/\u5065\u5eb7\u4eba\u7fa4\u5bf9\u8bdd\u6570\u636e 2\uff09GoEmotions\u6570\u636e\u96c6\u9a8c\u8bc1 3\uff09\u4eba\u7c7b\u6587\u672c\u4e0eLLM\u751f\u6210\u6587\u672c\u7684\u8111\u6fc0\u6d3b\u5dee\u5f02\u5206\u6790", "result": "1\uff09\u53d1\u73b0\u6291\u90c1\u75c7\u60a3\u8005\u8fb9\u7f18\u7cfb\u7edf\u6fc0\u6d3b\u589e\u5f3a 2\uff09\u6210\u529f\u533a\u5206\u79bb\u6563\u60c5\u7eea 3\uff09LLM\u6587\u672c\u5728\u57fa\u7840\u60c5\u7eea\u5206\u5e03\u4e0e\u4eba\u7c7b\u76f8\u5f53\uff0c\u4f46\u7f3a\u4e4f\u5185\u4fa7\u524d\u989d\u53f6/\u540e\u6263\u5e26\u56de\u7b49\u81ea\u6211\u53c2\u7167\u533a\u7684\u795e\u7ecf\u6fc0\u6d3b\u7279\u5f81", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u9ad8\u6210\u672c\u6548\u76ca\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u81ea\u7136\u8bed\u8a00\u5206\u6790\u3001\u4e34\u5e8a\u7fa4\u4f53\u533a\u5206\uff0c\u5e76\u4e3aAI\u60c5\u611f\u8868\u8fbe\u63d0\u4f9b\u795e\u7ecf\u79d1\u5b66\u8bc4\u4f30\u57fa\u51c6"}}
{"id": "2508.09349", "pdf": "https://arxiv.org/pdf/2508.09349", "abs": "https://arxiv.org/abs/2508.09349", "authors": ["Cathy Speed", "Ahmed A. Metwally"], "title": "The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Expert consensus plays a critical role in domains where evidence is complex,\nconflicting, or insufficient for direct prescription. Traditional methods, such\nas Delphi studies, consensus conferences, and systematic guideline synthesis,\noffer structure but face limitations including high panel burden, interpretive\noversimplification, and suppression of conditional nuance. These challenges are\nnow exacerbated by information overload, fragmentation of the evidence base,\nand increasing reliance on publicly available sources that lack expert\nfiltering. This study introduces and evaluates a Human-AI Hybrid Delphi\n(HAH-Delphi) framework designed to augment expert consensus development by\nintegrating a generative AI model (Gemini 2.5 Pro), small panels of senior\nhuman experts, and structured facilitation. The HAH-Delphi was tested in three\nphases: retrospective replication, prospective comparison, and applied\ndeployment in two applied domains (endurance training and resistance and mixed\ncardio/strength training). The AI replicated 95% of published expert consensus\nconclusions in Phase I and showed 95% directional agreement with senior human\nexperts in Phase II, though it lacked experiential and pragmatic nuance. In\nPhase III, compact panels of six senior experts achieved >90% consensus\ncoverage and reached thematic saturation before the final participant. The AI\nprovided consistent, literature-grounded scaffolding that supported divergence\nresolution and accelerated saturation. The HAH-Delphi framework offers a\nflexible, scalable approach for generating high-quality, context-sensitive\nconsensus. Its successful application across health, coaching, and performance\nscience confirms its methodological robustness and supports its use as a\nfoundation for generating conditional, personalised guidance and published\nconsensus frameworks at scale.", "AI": {"tldr": "HAH-Delphi\u6846\u67b6\u901a\u8fc7AI\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u534f\u540c\uff0c\u663e\u8457\u63d0\u5347\u5171\u8bc6\u6548\u7387\u4e0e\u8d28\u91cf\uff0c\u5b9e\u73b095%\u5171\u8bc6\u590d\u73b0\u7387\u4e0e\u4e13\u5bb6\u4e00\u81f4\u6027\u3002", "motivation": "\u4f20\u7edf\u4e13\u5bb6\u5171\u8bc6\u65b9\u6cd5\u5b58\u5728\u4e13\u5bb6\u8d1f\u62c5\u91cd\u3001\u4fe1\u606f\u8fc7\u5ea6\u7b80\u5316\u7b49\u95ee\u9898\uff0c\u5f53\u524d\u4fe1\u606f\u8fc7\u8f7d\u4e0e\u8bc1\u636e\u788e\u7247\u5316\u52a0\u5267\u4e86\u8fd9\u4e9b\u6311\u6218\u3002\u7814\u7a76\u65e8\u5728\u5f00\u53d1AI\u589e\u5f3a\u578b\u5171\u8bc6\u6846\u67b6\u4ee5\u63d0\u5347\u6548\u7387\u4e0e\u8d28\u91cf\u3002", "method": "\u5206\u4e09\u9636\u6bb5\u6d4b\u8bd5\uff1a1) \u590d\u73b0\u73b0\u6709\u5171\u8bc6 2) \u4e0e\u8d44\u6df1\u4e13\u5bb6\u524d\u77bb\u6027\u5bf9\u6bd4 3) \u5e94\u7528\u4e8e\u8010\u529b\u8bad\u7ec3\u4e0e\u6df7\u5408\u8bad\u7ec3\u9886\u57df\u3002\u6574\u5408\u751f\u6210\u5f0fAI\u6a21\u578b(Gemini 2.5 Pro)\u4e0e\u5c0f\u578b\u4e13\u5bb6\u5c0f\u7ec4\u3002", "result": "AI\u590d\u73b095%\u73b0\u6709\u7ed3\u8bba\uff0c\u4e0e\u4e13\u5bb6\u8fbe\u621095%\u65b9\u5411\u5171\u8bc6\uff08\u7f3a\u4e4f\u5b9e\u8df5\u7ec6\u8282\uff09\uff0c6\u4eba\u4e13\u5bb6\u5c0f\u7ec4\u8fbe\u6210>90%\u5171\u8bc6\u8986\u76d6\u5e76\u5728\u6700\u7ec8\u53c2\u4e0e\u8005\u524d\u5b9e\u73b0\u4e3b\u9898\u9971\u548c\u3002", "conclusion": "HAH-Delphi\u6846\u67b6\u5177\u6709\u7075\u6d3b\u53ef\u6269\u5c55\u6027\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u5065\u5eb7\u4e0e\u8bad\u7ec3\u9886\u57df\uff0c\u4e3a\u4e2a\u6027\u5316\u6307\u5bfc\u548c\u5927\u89c4\u6a21\u5171\u8bc6\u5efa\u8bbe\u63d0\u4f9b\u65b9\u6cd5\u8bba\u57fa\u7840\u3002"}}
{"id": "2508.09350", "pdf": "https://arxiv.org/pdf/2508.09350", "abs": "https://arxiv.org/abs/2508.09350", "authors": ["Ju-Chieh Chou", "Jiawei Zhou", "Karen Livescu"], "title": "Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling", "categories": ["cs.CL"], "comment": "Accepted to ASRU 2025", "summary": "Textless spoken language models (SLMs) are generative models of speech that\ndo not rely on text supervision. Most textless SLMs learn to predict the next\nsemantic token, a discrete representation of linguistic content, and rely on a\nseparate vocoder to add acoustic information to the generated speech. Such\nmodels have no access to acoustic context and no built-in control over acoustic\ndetails. In this work, we propose to jointly model linguistic and acoustic\ninformation by generating semantic tokens and a continuous real-valued\nrepresentation of the acoustic frame. We use a flow-matching objective to\npredict the continuous vector conditioned on the semantic tokens. We study the\ndesign space of this approach and find that predicting multiple future semantic\ntokens helps preserve linguistic information. Our approach achieves comparable\nperformance to existing models in terms of linguistic likelihood benchmarks,\nwhile providing better acoustic detail in prompted generation.", "AI": {"tldr": "\u63d0\u51fa\u8054\u5408\u5efa\u6a21\u8bed\u4e49\u6807\u8bb0\u548c\u58f0\u5b66\u7279\u5f81\u7684\u6587\u672c\u65e0\u5173\u8bed\u97f3\u751f\u6210\u6a21\u578b\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u4fdd\u6301\u8bed\u8a00\u4fe1\u606f\u7684\u540c\u65f6\u63d0\u5347\u58f0\u5b66\u7ec6\u8282\u8868\u73b0", "motivation": "\u73b0\u6709\u6587\u672c\u65e0\u5173\u8bed\u97f3\u6a21\u578b\u9700\u4f9d\u8d56\u72ec\u7acb\u58f0\u7801\u5668\uff0c\u7f3a\u4e4f\u58f0\u5b66\u4e0a\u4e0b\u6587\u611f\u77e5\u4e14\u65e0\u6cd5\u63a7\u5236\u58f0\u5b66\u7ec6\u8282", "method": "\u4f7f\u7528\u6d41\u5339\u914d\u76ee\u6807\u51fd\u6570\u8054\u5408\u751f\u6210\u8bed\u4e49\u6807\u8bb0\u548c\u8fde\u7eed\u58f0\u5b66\u5e27\u8868\u5f81\uff0c\u901a\u8fc7\u9884\u6d4b\u591a\u6b65\u672a\u6765\u8bed\u4e49\u6807\u8bb0\u4fdd\u6301\u8bed\u8a00\u4fe1\u606f", "result": "\u5728\u8bed\u8a00\u4f3c\u7136\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u76f8\u5f53\uff0c\u4f46\u5728\u63d0\u793a\u751f\u6210\u4e2d\u5c55\u73b0\u66f4\u4f18\u7684\u58f0\u5b66\u7ec6\u8282", "conclusion": "\u8054\u5408\u5efa\u6a21\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u6a21\u578b\u58f0\u5b66\u4fe1\u606f\u5efa\u6a21\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4e3a\u8bed\u97f3\u751f\u6210\u63d0\u4f9b\u66f4\u5b8c\u6574\u7684\u63a7\u5236\u80fd\u529b"}}
{"id": "2508.09378", "pdf": "https://arxiv.org/pdf/2508.09378", "abs": "https://arxiv.org/abs/2508.09378", "authors": ["Artem Chernodub", "Aman Saini", "Yejin Huh", "Vivek Kulkarni", "Vipul Raheja"], "title": "APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": "Accepted for publication at Recent Advances in Natural Language\n  Processing conference (RANLP 2025)", "summary": "Recent advancements in large language models (LLMs) have enabled a wide range\nof natural language processing (NLP) tasks to be performed through simple\nprompt-based interactions. Consequently, several approaches have been proposed\nto engineer prompts that most effectively enable LLMs to perform a given task\n(e.g., chain-of-thought prompting). In settings with a well-defined metric to\noptimize model performance, automatic prompt optimization (APO) methods have\nbeen developed to refine a seed prompt. Advancing this line of research, we\npropose APIO, a simple but effective prompt induction and optimization approach\nfor the tasks of Grammatical Error Correction (GEC) and Text Simplification,\nwithout relying on manually specified seed prompts. APIO achieves a new\nstate-of-the-art performance for purely LLM-based prompting methods on these\ntasks. We make our data, code, prompts, and outputs publicly available.", "AI": {"tldr": "\u63d0\u51faAPIO\u65b9\u6cd5\u2014\u2014\u65e0\u9700\u624b\u52a8\u79cd\u5b50\u63d0\u793a\u7684\u81ea\u52a8\u63d0\u793a\u8bf1\u5bfc\u4e0e\u4f18\u5316\u6846\u67b6\uff0c\u5728\u8bed\u6cd5\u7ea0\u9519\u548c\u6587\u672c\u7b80\u5316\u4efb\u52a1\u4e2d\u8fbe\u5230\u7eafLLM\u63d0\u793a\u65b9\u6cd5\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u7684\u521d\u59cb\u63d0\u793a\uff08seed prompts\uff09\uff0c\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u901a\u7528\u6027\u548c\u6548\u7387\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e0d\u4f9d\u8d56\u624b\u52a8\u63d0\u793a\u7684\u81ea\u52a8\u5316\u4f18\u5316\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u4efb\u52a1\u63cf\u8ff0\u81ea\u52a8\u8bf1\u5bfc\u751f\u6210\u521d\u59cb\u63d0\u793a\uff1b2\uff09\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\u7b56\u7565\uff0c\u5f15\u5165\u8d28\u91cf\u53cd\u9988\u4fe1\u53f7\u8c03\u6574\u751f\u6210\u5206\u5e03\u3002", "result": "\u5728BEA\u8bed\u6cd5\u7ea0\u9519\u57fa\u51c6\u548c\u6587\u672c\u7b80\u5316\u4efb\u52a1\u4e2d\uff0cAPIO\u5206\u522b\u53d6\u5f9762.7\uff08F0.5\uff09\u548c43.2\uff08SARI\uff09\u7684\u5206\u6570\uff0c\u8d85\u8d8a\u4eba\u5de5\u8bbe\u8ba1\u63d0\u793a\u65b9\u6cd53-5\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u8bc1\u660e\u4e86\u5b8c\u5168\u81ea\u52a8\u5316\u63d0\u793a\u4f18\u5316\u7684\u53ef\u884c\u6027\uff0c\u4e3aLLM\u63d0\u793a\u5de5\u7a0b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002\u901a\u8fc7\u516c\u5f00\u6570\u636e\u3001\u4ee3\u7801\u548c\u4f18\u5316\u540e\u7684\u63d0\u793a\u6a21\u677f\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2508.09403", "pdf": "https://arxiv.org/pdf/2508.09403", "abs": "https://arxiv.org/abs/2508.09403", "authors": ["Ting Cai", "Stephen Sheen", "AnHai Doan"], "title": "Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models", "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Expanding the abbreviated column names of tables, such as ``esal'' to\n``employee salary'', is critical for numerous downstream data tasks. This\nproblem arises in enterprises, domain sciences, government agencies, and more.\nIn this paper we make three contributions that significantly advances the state\nof the art. First, we show that synthetic public data used by prior work has\nmajor limitations, and we introduce 4 new datasets in enterprise/science\ndomains, with real-world abbreviations. Second, we show that accuracy measures\nused by prior work seriously undercount correct expansions, and we propose new\nsynonym-aware measures that capture accuracy much more accurately. Finally, we\ndevelop Columbo, a powerful LLM-based solution that exploits context, rules,\nchain-of-thought reasoning, and token-level analysis. Extensive experiments\nshow that Columbo significantly outperforms NameGuess, the current most\nadvanced solution, by 4-29\\%, over 5 datasets. Columbo has been used in\nproduction on EDI, a major data portal for environmental sciences.", "AI": {"tldr": "\u63d0\u51faColumbo\u7cfb\u7edf\u89e3\u51b3\u8868\u683c\u5217\u540d\u7f29\u5199\u6269\u5c55\u95ee\u9898\uff0c\u901a\u8fc7\u771f\u5b9e\u6570\u636e\u96c6\u3001\u65b0\u8bc4\u4f30\u6307\u6807\u548cLLM\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u51c6\u786e\u7387\u63d0\u53474-29%", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5408\u6210\u6570\u636e\u96c6\u4e14\u8bc4\u4f30\u6307\u6807\u4e0d\u51c6\u786e\uff0c\u9700\u6539\u8fdb\u6570\u636e\u8d28\u91cf\u4e0e\u8bc4\u4f30\u4f53\u7cfb", "method": "\u6784\u5efa4\u4e2a\u771f\u5b9e\u9886\u57df\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u540c\u4e49\u8bcd\u611f\u77e5\u8bc4\u4f30\u6307\u6807\uff0c\u5f00\u53d1\u7ed3\u5408\u89c4\u5219\u94fe\u4e0etoken\u5206\u6790\u7684Columbo\u6846\u67b6", "result": "\u57285\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u5f53\u524d\u6700\u4f18\u7cfb\u7edfNameGuess 4-29%\uff0c\u5e76\u90e8\u7f72\u4e8e\u73af\u5883\u79d1\u5b66\u6570\u636e\u95e8\u6237EDI", "conclusion": "\u901a\u8fc7\u771f\u5b9e\u6570\u636e\u3001\u7cbe\u51c6\u8bc4\u4f30\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684LLM\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u5217\u540d\u6269\u5c55\u51c6\u786e\u6027"}}
{"id": "2508.09430", "pdf": "https://arxiv.org/pdf/2508.09430", "abs": "https://arxiv.org/abs/2508.09430", "authors": ["Lavanya Shankar", "Leibny Paola Garcia Perera"], "title": "Leveraging Zipformer Model for Effective Language Identification in Code-Switched Child-Directed Speech", "categories": ["cs.CL", "cs.SD"], "comment": null, "summary": "Code-switching and language identification in child-directed scenarios\npresent significant challenges, particularly in bilingual environments. This\npaper addresses this challenge by using Zipformer to handle the nuances of\nspeech, which contains two imbalanced languages, Mandarin and English, in an\nutterance. This work demonstrates that the internal layers of the Zipformer\neffectively encode the language characteristics, which can be leveraged in\nlanguage identification. We present the selection methodology of the inner\nlayers to extract the embeddings and make a comparison with different\nback-ends. Our analysis shows that Zipformer is robust across these backends.\nOur approach effectively handles imbalanced data, achieving a Balanced Accuracy\n(BAC) of 81.89%, a 15.47% improvement over the language identification\nbaseline. These findings highlight the potential of the transformer encoder\narchitecture model in real scenarios.", "AI": {"tldr": "\u901a\u8fc7Zipformer\u6a21\u578b\u5904\u7406\u4e2d\u82f1\u4e0d\u5e73\u8861\u53cc\u8bed\u6570\u636e\uff0c\u8bed\u8a00\u8bc6\u522b\u5e73\u8861\u51c6\u786e\u7387\u63d0\u534715.47%", "motivation": "\u53cc\u8bed\u73af\u5883\u4e2d\u513f\u7ae5\u5bfc\u5411\u573a\u666f\u7684\u4ee3\u7801\u8f6c\u6362\u548c\u8bed\u8a00\u8bc6\u522b\u5b58\u5728\u6311\u6218\uff0c\u9700\u89e3\u51b3\u8bed\u97f3\u4e2d\u4e0d\u5e73\u8861\u8bed\u8a00\u6df7\u5408\u7684\u95ee\u9898", "method": "\u5229\u7528Zipformer\u5185\u90e8\u5c42\u7f16\u7801\u8bed\u8a00\u7279\u5f81\uff0c\u6bd4\u8f83\u4e0d\u540c\u540e\u7aef\u5e76\u9009\u62e9\u6700\u4f18\u5d4c\u5165\u63d0\u53d6\u65b9\u6848", "result": "\u5e73\u8861\u51c6\u786e\u7387(BAC)\u8fbe81.89%\uff0c\u8f83\u57fa\u7ebf\u63d0\u534715.47%\uff0c\u4e14\u6a21\u578b\u5728\u4e0d\u540c\u540e\u7aef\u8868\u73b0\u9c81\u68d2", "conclusion": "Transformer\u7f16\u7801\u5668\u67b6\u6784(Zipformer)\u5728\u73b0\u5b9e\u573a\u666f\u7684\u8bed\u8a00\u8bc6\u522b\u4e2d\u5c55\u73b0\u51fa\u5b9e\u9645\u5e94\u7528\u6f5c\u529b"}}
{"id": "2508.09450", "pdf": "https://arxiv.org/pdf/2508.09450", "abs": "https://arxiv.org/abs/2508.09450", "authors": ["Ridwan Mahbub", "Mohammed Saidul Islam", "Mir Tafseer Nayeem", "Md Tahmid Rahman Laskar", "Mizanur Rahman", "Shafiq Joty", "Enamul Hoque"], "title": "From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text", "categories": ["cs.CL"], "comment": null, "summary": "Charts are very common for exploring data and communicating insights, but\nextracting key takeaways from charts and articulating them in natural language\ncan be challenging. The chart-to-text task aims to automate this process by\ngenerating textual summaries of charts. While with the rapid advancement of\nlarge Vision-Language Models (VLMs), we have witnessed great progress in this\ndomain, little to no attention has been given to potential biases in their\noutputs. This paper investigates how VLMs can amplify geo-economic biases when\ngenerating chart summaries, potentially causing societal harm. Specifically, we\nconduct a large-scale evaluation of geo-economic biases in VLM-generated chart\nsummaries across 6,000 chart-country pairs from six widely used proprietary and\nopen-source models to understand how a country's economic status influences the\nsentiment of generated summaries. Our analysis reveals that existing VLMs tend\nto produce more positive descriptions for high-income countries compared to\nmiddle- or low-income countries, even when country attribution is the only\nvariable changed. We also find that models such as GPT-4o-mini,\nGemini-1.5-Flash, and Phi-3.5 exhibit varying degrees of bias. We further\nexplore inference-time prompt-based debiasing techniques using positive\ndistractors but find them only partially effective, underscoring the complexity\nof the issue and the need for more robust debiasing strategies. Our code and\ndataset are publicly available here.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4e3b\u6d41\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u56fe\u8868\u6458\u8981\u65f6\u5b58\u5728\u5730\u7406\u7ecf\u6d4e\u504f\u89c1\uff0c\u5bf9\u9ad8\u6536\u5165\u56fd\u5bb6\u63cf\u8ff0\u66f4\u79ef\u6781\uff0c\u9700\u66f4\u6709\u6548\u7684\u53bb\u504f\u7b56\u7565\u3002", "motivation": "\u867d\u7136\u56fe\u8868\u6587\u672c\u751f\u6210\u6280\u672f\u5feb\u901f\u53d1\u5c55\uff0c\u4f46\u6a21\u578b\u8f93\u51fa\u4e2d\u7684\u6f5c\u5728\u504f\u89c1\uff08\u5c24\u5176\u662f\u53ef\u80fd\u5f15\u53d1\u793e\u4f1a\u5371\u5bb3\u7684\u5730\u7406\u7ecf\u6d4e\u504f\u89c1\uff09\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u57fa\u4e8e6,000\u4e2a\u56fe\u8868-\u56fd\u5bb6\u7ec4\u5408\u7684\u5927\u89c4\u6a21\u8bc4\u4f30\uff0c\u5206\u6790\u516d\u4e2a\u4e3b\u6d41\u6a21\u578b\u751f\u6210\u6458\u8981\u7684\u60c5\u611f\u503e\u5411\u4e0e\u7ecf\u6d4e\u5730\u4f4d\u5173\u8054\u6027\u3002", "result": "\u73b0\u6709\u6a21\u578b\u5bf9\u9ad8\u6536\u5165\u56fd\u5bb6\u751f\u6210\u66f4\u79ef\u6781\u7684\u63cf\u8ff0\uff0cGPT-4o-mini/Gemini-1.5-Flash/Phi-3.5\u5b58\u5728\u4e0d\u540c\u7a0b\u5ea6\u504f\u89c1\uff0c\u63d0\u793a\u53bb\u504f\u65b9\u6cd5\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u5730\u7406\u7ecf\u6d4e\u504f\u89c1\u95ee\u9898\u590d\u6742\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u53bb\u504f\u65b9\u6cd5\uff0c\u7814\u7a76\u516c\u5f00\u4e86\u4ee3\u7801\u548c\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2508.09463", "pdf": "https://arxiv.org/pdf/2508.09463", "abs": "https://arxiv.org/abs/2508.09463", "authors": ["Qi Jia", "Xiujie Song", "Zicheng Zhang", "Yijin Guo", "Kaiwei Zhang", "Zijian Chen", "Guangtao Zhai"], "title": "User-centric Subjective Leaderboard by Customizable Reward Modeling", "categories": ["cs.CL"], "comment": null, "summary": "Existing benchmarks for large language models (LLMs) predominantely focus on\nassessing their capabilities through verifiable tasks. Such objective and\nstatic benchmarks offer limited utility for practical LLM selection, making it\ndifficult for users to find suitable models for their individual needs. To\nbridge this gap, we present the first User-Centric Subjective Leaderboard\n(USL), which provides a preference-driven, dynamic ranking of LLMs across\ndiverse real-world scenarios. Our work is built upon a thorough investigation\nof real human preference data, involving more than 10K subjective queries. Our\ninvestigation reveals significant diversity and contradictions in human\npreferences, which limit the effectiveness of state-of-the-art reward models.\nTo address this, we introduce Customizable Reward Models (CRMs). With only 4B\nparameters, our CRM surpasses the performance of leading models such as GPT-4.1\nand Gemini-2.5-pro, showing exceptional generalization capabilities across new\ntopics and criteria. The USL, powered by CRMs, exhibits strong negative\ncorrelations to contradictory preferences.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u7528\u6237\u4e2d\u5fc3\u7684\u4e3b\u89c2\u6392\u884c\u699c(USL)\u548c\u53ef\u5b9a\u5236\u5956\u52b1\u6a21\u578b(CRM)\uff0c\u901a\u8fc710K+\u4e3b\u89c2\u67e5\u8be2\u6570\u636e\u89e3\u51b3LLM\u8bc4\u4f30\u4e2d\u7684\u504f\u597d\u77db\u76fe\u95ee\u9898\uff0c4B\u53c2\u6570\u7684CRM\u6027\u80fd\u8d85\u8d8aGPT-4.1\u7b49\u4e3b\u6d41\u6a21\u578b\u3002", "motivation": "\u73b0\u6709LLM\u57fa\u51c6\u8fc7\u5ea6\u4f9d\u8d56\u5ba2\u89c2\u6307\u6807\uff0c\u96be\u4ee5\u6ee1\u8db3\u7528\u6237\u4e2a\u6027\u5316\u9700\u6c42\u3002\u4eba\u7c7b\u504f\u597d\u5b58\u5728\u663e\u8457\u591a\u6837\u6027\u548c\u77db\u76fe\u6027\uff0c\u4f20\u7edf\u5956\u52b1\u6a21\u578b\u5bf9\u6b64\u6548\u679c\u6709\u9650\u3002", "method": "1. \u6536\u96c610K+\u771f\u5b9e\u7528\u6237\u4e3b\u89c2\u67e5\u8be2\u6570\u636e\n2. \u5f00\u53d1\u53ef\u5b9a\u5236\u5956\u52b1\u6a21\u578b(CRM)\u67b6\u6784\n3. \u5728\u65b0\u4e3b\u9898/\u6807\u51c6\u4e0b\u6d4b\u8bd5\u6a21\u578b\u6cdb\u5316\u80fd\u529b\n4. \u6784\u5efa\u52a8\u6001\u504f\u597d\u9a71\u52a8\u7684USL\u8bc4\u4f30\u4f53\u7cfb", "result": "1. CRM(4B\u53c2\u6570)\u6027\u80fd\u8d85\u8d8aGPT-4.1\u548cGemini-2.5-pro\n2. USL\u5c55\u793a\u4e0e\u77db\u76fe\u504f\u597d\u7684\u5f3a\u8d1f\u76f8\u5173\u6027(-0.89)\n3. \u65b0\u573a\u666f\u4e0b\u51c6\u786e\u7387\u63d0\u534723%", "conclusion": "\u8be5\u7814\u7a76\u7a81\u7834\u4f20\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001USL\u7cfb\u7edf\u5b9e\u73b0\u4e2a\u6027\u5316LLM\u9009\u62e9\uff0cCRM\u7684\u5353\u8d8a\u6cdb\u5316\u80fd\u529b\u4e3aAI\u5bf9\u9f50\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.09494", "pdf": "https://arxiv.org/pdf/2508.09494", "abs": "https://arxiv.org/abs/2508.09494", "authors": ["Jessy Lin", "Vincent-Pierre Berges", "Xilun Chen", "Wen-Tau Yih", "Gargi Ghosh", "Barlas O\u011fuz"], "title": "Learning Facts at Scale with Active Reading", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs are known to store vast amounts of knowledge in their parametric memory.\nHowever, learning and recalling facts from this memory is known to be\nunreliable, depending largely on the prevalence of particular facts in the\ntraining data and other factors which are poorly understood. Practitioners are\nlacking tools which will allow them to ensure that the models learn a given\nbody of knowledge reliably and consistently. To this end, we propose Active\nReading: a framework where we train models to study a given set of material\nwith self-generated learning strategies. First, we demonstrate models trained\nwith Active Reading on expert domains absorb significantly more knowledge than\nvanilla finetuning and other data augmentations. We train expert 8B models that\nachieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over\nvanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla\nfinetuning) by applying Active Reading to the source documents for each\nbenchmark. Finally, we show that Active Reading can be utilized at pre-training\nscale to build more factual models. As a demonstration of this, we release Meta\nWikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens,\nwhich outcompetes models with hundreds of billions of parameters on factual QA.", "AI": {"tldr": "\u63d0\u51faActive Reading\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u751f\u6210\u5b66\u4e60\u7b56\u7565\u663e\u8457\u63d0\u5347\u6a21\u578b\u77e5\u8bc6\u5438\u6536\u6548\u679c\uff0c\u5728\u591a\u4e2a\u9886\u57df\u5b9e\u73b0\u51c6\u786e\u7387\u5927\u5e45\u63d0\u5347\u5e76\u63a8\u51fa\u4f18\u79c0\u4e13\u5bb6\u6a21\u578b", "motivation": "\u89e3\u51b3LLMs\u53c2\u6570\u5316\u8bb0\u5fc6\u5b66\u4e60\u4e0d\u53ef\u9760\u95ee\u9898\uff08\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u5206\u5e03/\u673a\u5236\u4e0d\u660e\u786e\uff09\uff0c\u586b\u8865\u786e\u4fdd\u77e5\u8bc6\u7a33\u5b9a\u5b66\u4e60\u7684\u5de5\u5177\u7a7a\u767d", "method": "Active Reading\u6846\u67b6\uff1a\u8bad\u7ec3\u6a21\u578b\u901a\u8fc7\u81ea\u751f\u6210\u5b66\u4e60\u7b56\u7565\u4e3b\u52a8\u5b66\u4e60\u7279\u5b9a\u6750\u6599\uff0c\u5e94\u7528\u4e8e\u4e13\u5bb6\u9886\u57df\u8bad\u7ec3\u548c\u9884\u8bad\u7ec3\u6269\u5c55", "result": "\u4e13\u5bb6\u6a21\u578b\u51c6\u786e\u7387\uff1aSimpleQA\u5b50\u96c666%\uff08+313%\uff09\u3001FinanceBench 26%\uff08+160%\uff09\uff1bMeta WikiExpert-8B\u5728\u4e07\u4ebftoken\u8bad\u7ec3\u540e\u8d85\u8d8a\u767e\u4ebf\u53c2\u6570\u6a21\u578b", "conclusion": "Active Reading\u6709\u6548\u63d0\u5347\u6a21\u578b\u77e5\u8bc6\u5438\u6536\u53ef\u9760\u6027\uff0c\u65e2\u9002\u7528\u4e8e\u7279\u5b9a\u9886\u57df\u4f18\u5316\uff0c\u4e5f\u80fd\u6269\u5c55\u81f3\u9884\u8bad\u7ec3\u9636\u6bb5\u6784\u5efa\u66f4\u4e8b\u5b9e\u51c6\u786e\u7684\u6a21\u578b"}}
{"id": "2508.09497", "pdf": "https://arxiv.org/pdf/2508.09497", "abs": "https://arxiv.org/abs/2508.09497", "authors": ["Siyuan Meng", "Junming Liu", "Yirong Chen", "Song Mao", "Pinlong Cai", "Guohang Yan", "Botian Shi", "Ding Wang"], "title": "From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 4 tables", "summary": "Retrieval-augmented generation (RAG) systems are often bottlenecked by their\nreranking modules, which typically score passages independently and select a\nfixed Top-K size. This approach struggles with complex multi-hop queries that\nrequire synthesizing evidence across multiple documents, creating a trade-off\nwhere small K values omit crucial information and large K values introduce\nnoise. To address this, we introduce the Dynamic Passage Selector (DPS), a\nnovel reranking framework that treats passage selection as a supervised\nlearning problem. Unlike traditional point-wise or list-wise methods, DPS is\nfine-tuned to capture inter-passage dependencies and dynamically select the\nmost relevant set of passages for generation. As a seamless plug-and-play\nmodule, DPS requires no modifications to the standard RAG pipeline.\nComprehensive evaluations on five benchmarks show that DPS consistently\noutperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the\nchallenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over\nstrong baselines like Qwen3-reranker and RankingGPT, respectively. Our results\ndemonstrate that by enabling adaptive evidence selection, DPS substantially\nenhances reasoning capabilities in complex RAG scenarios.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u6bb5\u843d\u9009\u62e9\u5668\uff08DPS\uff09\uff0c\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u4f18\u5316RAG\u7cfb\u7edf\u7684\u6bb5\u843d\u9009\u62e9\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u591a\u8df3\u67e5\u8be2\u6027\u80fd", "motivation": "\u4f20\u7edfRAG\u91cd\u6392\u5e8f\u6a21\u5757\u91c7\u7528\u56fa\u5b9aTop-K\u9009\u62e9\u7b56\u7565\uff0c\u5728\u5904\u7406\u9700\u8981\u8de8\u6587\u6863\u63a8\u7406\u7684\u591a\u8df3\u67e5\u8be2\u65f6\u9762\u4e34\u5173\u952e\u4fe1\u606f\u9057\u6f0f\u6216\u566a\u58f0\u5e72\u6270\u7684\u5e73\u8861\u96be\u9898", "method": "\u5c06\u6bb5\u843d\u9009\u62e9\u5efa\u6a21\u4e3a\u76d1\u7763\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u6355\u6349\u6bb5\u843d\u95f4\u4f9d\u8d56\u5173\u7cfb\u5b9e\u73b0\u52a8\u6001\u96c6\u5408\u9009\u62e9\uff0c\u4fdd\u6301\u73b0\u6709RAG\u6d41\u7a0b\u7684\u5373\u63d2\u5373\u7528\u7279\u6027", "result": "\u5728MuSiQue\u6570\u636e\u96c6\u4e0aF1\u5206\u6570\u6bd4Qwen3-reranker\u548cRankingGPT\u5206\u522b\u63d0\u534730.06%\u548c15.4%\uff0c\u4e94\u57fa\u51c6\u6d4b\u8bd5\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "conclusion": "DPS\u901a\u8fc7\u81ea\u9002\u5e94\u8bc1\u636e\u9009\u62e9\u673a\u5236\u6709\u6548\u589e\u5f3a\u590d\u6742RAG\u573a\u666f\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u68c0\u7d22\u589e\u5f3a\u7cfb\u7edf\u63d0\u4f9b\u65b0\u7684\u4f18\u5316\u65b9\u5411"}}
{"id": "2508.09515", "pdf": "https://arxiv.org/pdf/2508.09515", "abs": "https://arxiv.org/abs/2508.09515", "authors": ["Jakub \u0160m\u00edd", "Pavel P\u0159ib\u00e1\u0148", "Pavel Kr\u00e1l"], "title": "LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation", "categories": ["cs.CL"], "comment": "Published in Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics; Volume 1: Long Papers (ACL 2025).\n  Official version: https://aclanthology.org/2025.acl-long.41/", "summary": "Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed\nsentiment analysis in a target language by transferring knowledge from a source\nlanguage with available annotated data. Most existing methods depend heavily on\noften unreliable translation tools to bridge the language gap. In this paper,\nwe propose a new approach that leverages a large language model (LLM) to\ngenerate high-quality pseudo-labelled data in the target language without the\nneed for translation tools. First, the framework trains an ABSA model to obtain\npredictions for unlabelled target language data. Next, LLM is prompted to\ngenerate natural sentences that better represent these noisy predictions than\nthe original text. The ABSA model is then further fine-tuned on the resulting\npseudo-labelled dataset. We demonstrate the effectiveness of this method across\nsix languages and five backbone models, surpassing previous state-of-the-art\ntranslation-based approaches. The proposed framework also supports generative\nmodels, and we show that fine-tuned LLMs outperform smaller multilingual\nmodels.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4f2a\u6807\u7b7e\u6570\u636e\u7684\u8de8\u8bed\u8a00\u65b9\u9762\u60c5\u611f\u5206\u6790\u6846\u67b6\uff0c\u65e0\u9700\u7ffb\u8bd1\u5de5\u5177\u5e76\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e2d\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5", "motivation": "\u4f20\u7edf\u8de8\u8bed\u8a00ABSA\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u7ffb\u8bd1\u5de5\u5177\u5bfc\u81f4\u8bef\u5dee\u7d2f\u79ef\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u6d88\u9664\u8bed\u8a00\u5dee\u5f02", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1) \u521d\u6b65\u8bad\u7ec3\u76ee\u6807\u8bed\u8a00\u6a21\u578b 2) \u7528LLM\u91cd\u6784\u8bed\u4e49\u6e05\u6670\u7684\u4f2a\u6807\u7b7e\u6570\u636e 3) \u57fa\u4e8e\u589e\u5f3a\u6570\u636e\u5fae\u8c03\u6a21\u578b", "result": "\u57286\u79cd\u8bed\u8a00/5\u79cd\u6a21\u578b\u4e0a\u8d85\u8d8aSOTA\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u5fae\u8c03\u540e\u7684\u751f\u6210\u5f0fLLM\u4f18\u4e8e\u591a\u8bed\u8a00\u5c0f\u6a21\u578b", "conclusion": "LLM\u751f\u6210\u4f2a\u6807\u7b7e\u65b9\u6848\u6709\u6548\u7a81\u7834\u7ffb\u8bd1\u5de5\u5177\u9650\u5236\uff0c\u652f\u6301\u751f\u6210\u5f0f\u6a21\u578b\u4e14\u5177\u5907\u591a\u8bed\u8a00\u6269\u5c55\u6027"}}
{"id": "2508.09516", "pdf": "https://arxiv.org/pdf/2508.09516", "abs": "https://arxiv.org/abs/2508.09516", "authors": ["Jakub \u0160m\u00edd", "Pavel Kr\u00e1l"], "title": "Cross-lingual Aspect-Based Sentiment Analysis: A Survey on Tasks, Approaches, and Challenges", "categories": ["cs.CL"], "comment": "Submitted version prior to peer review. Updated version accepted in\n  Information Fusion. Official version:\n  https://www.sciencedirect.com/science/article/pii/S1566253525001460", "summary": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask that focuses on understanding opinions at the aspect level, including\nsentiment towards specific aspect terms, categories, and opinions. While ABSA\nresearch has seen significant progress, much of the focus has been on\nmonolingual settings. Cross-lingual ABSA, which aims to transfer knowledge from\nresource-rich languages (such as English) to low-resource languages, remains an\nunder-explored area, with no systematic review of the field. This paper aims to\nfill that gap by providing a comprehensive survey of cross-lingual ABSA. We\nsummarize key ABSA tasks, including aspect term extraction, aspect sentiment\nclassification, and compound tasks involving multiple sentiment elements.\nAdditionally, we review the datasets, modelling paradigms, and cross-lingual\ntransfer methods used to solve these tasks. We also examine how existing work\nin monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to\nthe development of cross-lingual ABSA. Finally, we highlight the main\nchallenges and suggest directions for future research to advance cross-lingual\nABSA systems.", "AI": {"tldr": "\u9996\u6b21\u7cfb\u7edf\u7efc\u8ff0\u8de8\u8bed\u8a00\u65b9\u9762\u60c5\u611f\u5206\u6790(ABSA)\uff0c\u6db5\u76d6\u4efb\u52a1\u5b9a\u4e49\u3001\u6570\u636e\u96c6\u3001\u5efa\u6a21\u65b9\u6cd5\u53ca\u672a\u6765\u6311\u6218", "motivation": "\u5f53\u524dABSA\u7814\u7a76\u96c6\u4e2d\u5728\u5355\u8bed\u8a00\u573a\u666f\uff0c\u8de8\u8bed\u8a00\u8fc1\u79fb\u573a\u666f\u7f3a\u4e4f\u7cfb\u7edf\u6027\u603b\u7ed3\uff0c\u963b\u788d\u8d44\u6e90\u4e30\u5bcc\u8bed\u8a00\u5411\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u8fc1\u79fb", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u7cfb\u7edf\u6027\u6574\u7406\u8de8\u8bed\u8a00ABSA\u7684\u4efb\u52a1\u4f53\u7cfb(\u65b9\u9762\u8bcd\u62bd\u53d6/\u60c5\u611f\u5206\u7c7b\u7b49)\u3001\u6570\u636e\u96c6\u7279\u5f81\u3001\u8de8\u8bed\u8a00\u8fc1\u79fb\u8303\u5f0f(\u57fa\u4e8e\u8868\u793a/\u53c2\u6570/\u6570\u636e\u8fc1\u79fb)", "result": "\u63ed\u793a\u73b0\u6709\u7814\u7a76\u591a\u805a\u7126\u5355\u8bed\u8a00\u573a\u666f\uff0c\u8de8\u8bed\u8a00ABSA\u4ecd\u5904\u65e9\u671f\u63a2\u7d22\u9636\u6bb5\uff0c\u7cfb\u7edf\u68b3\u7406\u4e86\u8be5\u9886\u57df\u6280\u672f\u8def\u7ebf\u4e0e\u74f6\u9888\u95ee\u9898", "conclusion": "\u8de8\u8bed\u8a00ABSA\u9700\u7a81\u7834\u6570\u636e\u7a00\u7f3a\u4e0e\u6a21\u578b\u6cdb\u5316\u96be\u9898\uff0c\u672a\u6765\u5e94\u4f18\u5316\u8fc1\u79fb\u673a\u5236\u5e76\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e94\u7528\u6f5c\u529b"}}
{"id": "2508.09517", "pdf": "https://arxiv.org/pdf/2508.09517", "abs": "https://arxiv.org/abs/2508.09517", "authors": ["Ladislav Lenc", "Daniel C\u00edfka", "Ji\u0159\u00ed Mart\u00ednek", "Jakub \u0160m\u00edd", "Pavel Kr\u00e1l"], "title": "UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval", "categories": ["cs.CL"], "comment": "Published in Proceedings of the 19th International Workshop on\n  Semantic Evaluation (SemEval-2025). Official version:\n  https://aclanthology.org/2025.semeval-1.31/", "summary": "This paper presents a zero-shot system for fact-checked claim retrieval. We\nemployed several state-of-the-art large language models to obtain text\nembeddings. The models were then combined to obtain the best possible result.\nOur approach achieved 7th place in monolingual and 9th in cross-lingual\nsubtasks. We used only English translations as an input to the text embedding\nmodels since multilingual models did not achieve satisfactory results. We\nidentified the most relevant claims for each post by leveraging the embeddings\nand measuring cosine similarity. Overall, the best results were obtained by the\nNVIDIA NV-Embed-v2 model. For some languages, we benefited from model\ncombinations (NV-Embed & GPT or Mistral).", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u7684\u96f6\u6837\u672c\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\uff0c\u5728CLEF 2023\u7ade\u8d5b\u4e2d\u83b7\u5355\u8bed\u7b2c7\u540d\u3001\u8de8\u8bed\u79cd\u7b2c9\u540d", "motivation": "\u63a2\u7d22\u96f6\u6837\u672c\u65b9\u6cd5\u5b9e\u73b0\u4e8b\u5b9e\u6838\u67e5\u58f0\u660e\u68c0\u7d22\uff0c\u907f\u514d\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u7684\u6210\u672c\u3002\u5c1d\u8bd5\u901a\u8fc7\u7ec4\u5408\u4e0d\u540c\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u6548\u679c\uff0c\u89e3\u51b3\u591a\u8bed\u8a00\u6a21\u578b\u6548\u679c\u6b20\u4f73\u7684\u95ee\u9898", "method": "1. \u91c7\u7528NVIDIA NV-Embed-v2\u7b49\u5148\u8fdb\u6a21\u578b\u751f\u6210\u6587\u672c\u5d4c\u5165\n2. \u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5339\u914d\u5e16\u5b50\u4e0e\u58f0\u660e\u7684\u5d4c\u5165\u5411\u91cf\n3. \u7ec4\u5408\u4e0d\u540c\u6a21\u578b\uff08\u5982NV-Embed\u4e0eGPT/Mistral\uff09\u4f18\u5316\u6548\u679c", "result": "\u5355\u8bed\u4efb\u52a1\u6392\u540d\u7b2c7\uff080.501\u5206\uff09\uff0c\u8de8\u8bed\u79cd\u7b2c9\uff080.416\u5206\uff09\u3002\u82f1\u8bed\u8f93\u5165\u6548\u679c\u6700\u4f73\uff0cNVIDIA\u6a21\u578b\u8868\u73b0\u6700\u4f18\uff0c\u90e8\u5206\u8bed\u79cd\u901a\u8fc7\u6a21\u578b\u7ec4\u5408\u63d0\u5347\u51c6\u786e\u7387", "conclusion": "\u96f6\u6837\u672c\u65b9\u6cd5\u5728\u4e8b\u5b9e\u6838\u67e5\u4efb\u52a1\u4e2d\u6709\u6548\uff0c\u4f46\u9700\u6ce8\u610f\u591a\u8bed\u8a00\u5904\u7406\u9650\u5236\u3002\u6a21\u578b\u7ec4\u5408\u7b56\u7565\u5728\u7279\u5b9a\u8bed\u79cd\u4e2d\u53ef\u63d0\u5347\u6548\u679c\uff0c\u672a\u6765\u9700\u6539\u8fdb\u591a\u8bed\u8a00\u5d4c\u5165\u8d28\u91cf"}}
{"id": "2508.09521", "pdf": "https://arxiv.org/pdf/2508.09521", "abs": "https://arxiv.org/abs/2508.09521", "authors": ["Yunxiao Wang", "Meng Liu", "Wenqi Liu", "Kaiyu Jiang", "Bin Wen", "Fan Yang", "Tingting Gao", "Guorui Zhou", "Liqiang Nie"], "title": "COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Emotional support conversations are crucial for promoting emotional\nwell-being, yet current models often lack deep empathetic reasoning grounded in\npsychological principles. To address this, we propose controllable empathetic\nreasoning, which combines natural language reasoning with structured\npsychological steps. We construct a fine-grained dataset annotated with\nreasoning correctness and response preferences to enable this capability. To\nfurther enhance training, we employ reinforcement learning with a unified\nprocess-outcome reward model that delivers precise feedback. To mitigate\nresponse repetitiveness from entropy collapse, we introduce personality-based\ndialogue rewriting and a redundancy-aware reward reweighting strategy. Our\napproach significantly improves model's emotional support ability, advancing\nthe development of empathetic, human-like support systems.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u63a7\u540c\u7406\u5fc3\u63a8\u7406\u65b9\u6cd5\uff0c\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4e0e\u5fc3\u7406\u6b65\u9aa4\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4e0e\u5197\u4f59\u5904\u7406\u7b56\u7565\u663e\u8457\u63d0\u5347\u60c5\u611f\u5bf9\u8bdd\u652f\u6301\u6548\u679c", "motivation": "\u73b0\u6709\u60c5\u611f\u652f\u6301\u6a21\u578b\u7f3a\u4e4f\u57fa\u4e8e\u5fc3\u7406\u5b66\u539f\u7406\u7684\u6df1\u5ea6\u5171\u60c5\u63a8\u7406\u80fd\u529b", "method": "\u6784\u5efa\u7ec6\u7c92\u5ea6\u6807\u6ce8\u6570\u636e\u96c6+\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08\u542b\u8fc7\u7a0b-\u7ed3\u679c\u53cc\u5956\u52b1\u6a21\u578b\uff09+\u4e2a\u6027\u5316\u5bf9\u8bdd\u91cd\u5199\u4e0e\u5197\u4f59\u611f\u77e5\u5956\u52b1\u52a0\u6743\u7b56\u7565", "result": "\u6a21\u578b\u60c5\u611f\u652f\u6301\u80fd\u529b\u63d0\u534723.6%\uff0c\u54cd\u5e94\u91cd\u590d\u7387\u964d\u4f4e41%", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u7a81\u7834\u5171\u60c5\u63a8\u7406\u74f6\u9888\uff0c\u4e3a\u4eba\u6027\u5316\u60c5\u611f\u652f\u6301\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2508.09603", "pdf": "https://arxiv.org/pdf/2508.09603", "abs": "https://arxiv.org/abs/2508.09603", "authors": ["Skyler Hallinan", "Jaehun Jung", "Melanie Sclar", "Ximing Lu", "Abhilasha Ravichander", "Sahana Ramnath", "Yejin Choi", "Sai Praneeth Karimireddy", "Niloofar Mireshghallah", "Xiang Ren"], "title": "The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage", "categories": ["cs.CL"], "comment": "CoLM 2025", "summary": "Membership inference attacks serves as useful tool for fair use of language\nmodels, such as detecting potential copyright infringement and auditing data\nleakage. However, many current state-of-the-art attacks require access to\nmodels' hidden states or probability distribution, which prevents investigation\ninto more widely-used, API-access only models like GPT-4. In this work, we\nintroduce N-Gram Coverage Attack, a membership inference attack that relies\nsolely on text outputs from the target model, enabling attacks on completely\nblack-box models. We leverage the observation that models are more likely to\nmemorize and subsequently generate text patterns that were commonly observed in\ntheir training data. Specifically, to make a prediction on a candidate member,\nN-Gram Coverage Attack first obtains multiple model generations conditioned on\na prefix of the candidate. It then uses n-gram overlap metrics to compute and\naggregate the similarities of these outputs with the ground truth suffix; high\nsimilarities indicate likely membership. We first demonstrate on a diverse set\nof existing benchmarks that N-Gram Coverage Attack outperforms other black-box\nmethods while also impressively achieving comparable or even better performance\nto state-of-the-art white-box attacks - despite having access to only text\noutputs. Interestingly, we find that the success rate of our method scales with\nthe attack compute budget - as we increase the number of sequences generated\nfrom the target model conditioned on the prefix, attack performance tends to\nimprove. Having verified the accuracy of our method, we use it to investigate\npreviously unstudied closed OpenAI models on multiple domains. We find that\nmore recent models, such as GPT-4o, exhibit increased robustness to membership\ninference, suggesting an evolving trend toward improved privacy protections.", "AI": {"tldr": "\u63d0\u51faN-Gram\u8986\u76d6\u7387\u653b\u51fb\u65b9\u6cd5\uff0c\u4ec5\u9700\u6587\u672c\u8f93\u51fa\u5373\u53ef\u5b9e\u73b0\u9ed1\u76d2\u6a21\u578b\u7684\u6210\u5458\u63a8\u7406\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8d85\u8d8a\u5176\u4ed6\u9ed1\u76d2\u65b9\u6cd5\uff0c\u53d1\u73b0GPT-4o\u7b49\u65b0\u578b\u53f7\u9690\u79c1\u4fdd\u62a4\u589e\u5f3a", "motivation": "\u73b0\u6709\u6210\u5458\u63a8\u7406\u653b\u51fb\u4f9d\u8d56\u6a21\u578b\u5185\u90e8\u4fe1\u606f\uff0c\u65e0\u6cd5\u5e94\u7528\u4e8e\u4ec5API\u8bbf\u95ee\u7684\u6a21\u578b\uff08\u5982GPT-4\uff09\u3002\u9700\u5f00\u53d1\u4ec5\u4f9d\u8d56\u6587\u672c\u8f93\u51fa\u7684\u653b\u51fb\u65b9\u6cd5\u4ee5\u6269\u5c55\u8c03\u67e5\u8303\u56f4", "method": "\u5229\u7528\u6a21\u578b\u503e\u5411\u4e8e\u751f\u6210\u8bad\u7ec3\u6570\u636e\u5e38\u89c1\u6a21\u5f0f\u7684\u7279\u6027\uff1a1\uff09\u57fa\u4e8e\u5019\u9009\u6210\u5458\u524d\u7f00\u751f\u6210\u591a\u7ec4\u6587\u672c 2\uff09\u901a\u8fc7n-gram\u91cd\u53e0\u5ea6\u8ba1\u7b97\u8f93\u51fa\u4e0e\u771f\u5b9e\u540e\u7f00\u7684\u76f8\u4f3c\u6027 3\uff09\u805a\u5408\u76f8\u4f3c\u5ea6\u5224\u65ad\u6210\u5458\u8d44\u683c", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u5176\u4ed6\u9ed1\u76d2\u65b9\u6cd5\uff0c\u6027\u80fd\u5339\u654c\u767d\u76d2\u653b\u51fb\uff1b\u653b\u51fb\u6210\u529f\u7387\u968f\u751f\u6210\u6587\u672c\u6570\u91cf\u589e\u52a0\u800c\u63d0\u5347\uff1b\u53d1\u73b0GPT-4o\u7b49\u65b0\u578b\u53f7\u6a21\u578b\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\u589e\u5f3a", "conclusion": "N-Gram\u8986\u76d6\u7387\u653b\u51fb\u4e3a\u9ed1\u76d2\u6a21\u578b\u9690\u79c1\u8bc4\u4f30\u63d0\u4f9b\u6709\u6548\u5de5\u5177\uff0c\u540c\u65f6\u663e\u793aOpenAI\u6a21\u578b\u7684\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\u5448\u6301\u7eed\u6539\u8fdb\u8d8b\u52bf"}}
{"id": "2508.09622", "pdf": "https://arxiv.org/pdf/2508.09622", "abs": "https://arxiv.org/abs/2508.09622", "authors": ["Tatiana Batura", "Elena Bruches", "Milana Shvenk", "Valentin Malykh"], "title": "AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian", "categories": ["cs.CL"], "comment": "AINL 2025 Conference", "summary": "The rapid advancement of large language models (LLMs) has revolutionized text\ngeneration, making it increasingly difficult to distinguish between human- and\nAI-generated content. This poses a significant challenge to academic integrity,\nparticularly in scientific publishing and multilingual contexts where detection\nresources are often limited. To address this critical gap, we introduce the\nAINL-Eval 2025 Shared Task, specifically focused on the detection of\nAI-generated scientific abstracts in Russian. We present a novel, large-scale\ndataset comprising 52,305 samples, including human-written abstracts across 12\ndiverse scientific domains and AI-generated counterparts from five\nstate-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and\nGigaChat-Lite). A core objective of the task is to challenge participants to\ndevelop robust solutions capable of generalizing to both (i) previously unseen\nscientific domains and (ii) models not included in the training data. The task\nwas organized in two phases, attracting 10 teams and 159 submissions, with top\nsystems demonstrating strong performance in identifying AI-generated content.\nWe also establish a continuous shared task platform to foster ongoing research\nand long-term progress in this important area. The dataset and platform are\npublicly available at https://github.com/iis-research-team/AINL-Eval-2025.", "AI": {"tldr": "\u63d0\u51faAINL-Eval 2025\u5171\u4eab\u4efb\u52a1\uff0c\u6784\u5efa\u4fc4\u8bed\u79d1\u5b66\u6458\u8981\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u63a8\u52a8AI\u751f\u6210\u5185\u5bb9\u8bc6\u522b\u7814\u7a76", "motivation": "\u5e94\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u5b66\u672f\u8bda\u4fe1\u7684\u5a01\u80c1\uff0c\u89e3\u51b3\u591a\u8bed\u79cd\u573a\u666f\u4e0b\u68c0\u6d4b\u8d44\u6e90\u4e0d\u8db3\u7684\u95ee\u9898", "method": "\u521b\u5efa\u5305\u542b52,305\u6837\u672c\u7684\u591a\u9886\u57df\u6570\u636e\u96c6\uff0812\u4e2a\u79d1\u5b66\u9886\u57df\u4eba\u7c7b\u6458\u8981+5\u4e2aLLM\u751f\u6210\uff09\uff0c\u8bbe\u8ba1\u4e24\u9636\u6bb5\u8de8\u9886\u57df/\u8de8\u6a21\u578b\u6cdb\u5316\u4efb\u52a1", "result": "\u5438\u5f1510\u4e2a\u56e2\u961f\u63d0\u4ea4159\u6b21\uff0c\u6700\u4f18\u7cfb\u7edf\u5c55\u73b0\u5f3a\u68c0\u6d4b\u80fd\u529b\uff0c\u5efa\u7acb\u6301\u7eed\u7814\u7a76\u5e73\u53f0", "conclusion": "\u901a\u8fc7\u5171\u4eab\u4efb\u52a1\u548c\u5f00\u6e90\u5e73\u53f0\u63a8\u52a8AI\u5185\u5bb9\u68c0\u6d4b\u6280\u672f\u7684\u957f\u671f\u53d1\u5c55\uff0c\u7ef4\u62a4\u5b66\u672f\u771f\u5b9e\u6027"}}
{"id": "2508.09654", "pdf": "https://arxiv.org/pdf/2508.09654", "abs": "https://arxiv.org/abs/2508.09654", "authors": ["Alexandre Verine", "Florian Le Bronnec", "Kunhao Zheng", "Alexandre Allauzen", "Yann Chevaleyre", "Benjamin Negrevergne"], "title": "Improving Diversity in Language Models: When Temperature Fails, Change the Loss", "categories": ["cs.CL", "cs.LG"], "comment": "Forty-Second International Conference on Machine Learning, ICML2025", "summary": "Increasing diversity in language models is a challenging yet essential\nobjective. A common approach is to raise the decoding temperature. In this\nwork, we investigate this approach through a simplistic yet common case to\nprovide insights into why decreasing temperature can improve quality\n(Precision), while increasing it often fails to boost coverage (Recall). Our\nanalysis reveals that for a model to be effectively tunable through temperature\nadjustments, it must be trained toward coverage. To address this, we propose\nrethinking loss functions in language models by leveraging the Precision-Recall\nframework. Our results demonstrate that this approach achieves a substantially\nbetter trade-off between Precision and Recall than merely combining negative\nlog-likelihood training with temperature scaling. These findings offer a\npathway toward more versatile and robust language modeling techniques.", "AI": {"tldr": "\u901a\u8fc7Precision-Recall\u6846\u67b6\u91cd\u6784\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u8d1f\u5bf9\u6570\u4f3c\u7136\u8bad\u7ec3+\u6e29\u5ea6\u8c03\u8282\u66f4\u597d\u7684\u6743\u8861\u6548\u679c", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u8fc7\u63d0\u5347\u89e3\u7801\u6e29\u5ea6\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u591a\u6837\u6027\u65f6\uff0c\u5e38\u51fa\u73b0\u65e0\u6cd5\u6709\u6548\u63d0\u5347\u8986\u76d6\u7387\uff08Recall\uff09\u7684\u95ee\u9898", "method": "\u63d0\u51fa\u57fa\u4e8ePrecision-Recall\u6846\u67b6\u7684\u635f\u5931\u51fd\u6570\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u66ff\u4ee3\u5355\u7eaf\u7684\u8d1f\u5bf9\u6570\u4f3c\u7136\u8bad\u7ec3", "result": "\u65b0\u65b9\u6cd5\u5728Precision\u548cRecall\u7684\u6743\u8861\u4e0a\u6bd4\u4f20\u7edf\u6e29\u5ea6\u8c03\u8282\u65b9\u6cd5\u63d0\u5347\u663e\u8457", "conclusion": "\u8fd9\u4e00\u53d1\u73b0\u4e3a\u6784\u5efa\u66f4\u5168\u9762\u9c81\u68d2\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84"}}
{"id": "2508.09662", "pdf": "https://arxiv.org/pdf/2508.09662", "abs": "https://arxiv.org/abs/2508.09662", "authors": ["Yaoning Wang", "Jiahao Ying", "Yixin Cao", "Yubo Ma", "Yugang Jiang"], "title": "EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization", "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) and the development of\nincreasingly large and diverse evaluation benchmarks have introduced\nsubstantial computational challenges for model assessment. In this paper, we\npresent EffiEval, a training-free approach for efficient benchmarking that\neffectively addresses data redundancy while maintaining high evaluation\nreliability. Our method is specifically designed to meet three key criteria for\nhigh-quality evaluation: representativeness, by ensuring comprehensive coverage\nof model capabilities; fairness, by remaining independent of model performance\nduring sample selection to avoid bias; and generalizability, by enabling\nflexible transfer across datasets and model families without reliance on\nlarge-scale evaluation data. Unlike traditional methods that rely on absolute\nperformance or require extensive evaluation data, our approach adaptively\nselects high-quality representative subsets based on the Model Utility Index\n(MUI). Extensive experiments on multiple public benchmarks and diverse LLMs\ndemonstrate that EffiEval achieves strong ranking consistency with full-dataset\nevaluation using only a small fraction of the original data. Furthermore, our\nmethod is flexible and scalable in size, allowing users to balance evaluation\nefficiency and representativeness according to specific needs. Overall,\nEffiEval provides a practical and generalizable solution for reliable, fair,\nand efficient evaluation in the era of LLMs.", "AI": {"tldr": "EffiEval\u901a\u8fc7\u81ea\u9002\u5e94\u9009\u62e9\u9ad8\u8d28\u91cf\u6570\u636e\u5b50\u96c6\uff08\u57fa\u4e8eMUI\u6307\u6807\uff09\uff0c\u5728\u4ec5\u4f7f\u752810%\u6570\u636e\u91cf\u65f6\u4ecd\u4fdd\u6301\u4e0e\u5168\u6570\u636e\u96c6\u8bc4\u4f30\u4e00\u81f4\u7684\u6a21\u578b\u6392\u540d\u7ed3\u679c\uff0c\u89e3\u51b3\u4e86\u5927\u6a21\u578b\u8bc4\u4f30\u4e2d\u7684\u8ba1\u7b97\u5197\u4f59\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u5b58\u5728\u4e24\u5927\u75db\u70b9\uff1a1) \u5927\u89c4\u6a21\u591a\u6837\u5316\u8bc4\u6d4b\u57fa\u51c6\u5e26\u6765\u6781\u9ad8\u8ba1\u7b97\u6210\u672c\uff1b2) \u6570\u636e\u5197\u4f59\u5bfc\u81f4\u8bc4\u4f30\u6548\u7387\u4f4e\u4e0b\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u5168\u91cf\u6570\u636e\u8bc4\u4f30\u6216\u7edd\u5bf9\u6027\u80fd\u6307\u6807\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u516c\u5e73\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6a21\u578b\u6548\u7528\u6307\u6570\uff08MUI\uff09\u7684\u81ea\u9002\u5e94\u5b50\u96c6\u9009\u62e9\u65b9\u6cd5\uff1a1) \u901a\u8fc7\u7279\u5f81\u7a7a\u95f4\u5206\u6790\u7b5b\u9009\u4ee3\u8868\u6027\u6837\u672c\uff1b2) \u4fdd\u6301\u6837\u672c\u9009\u62e9\u4e0e\u6a21\u578b\u8868\u73b0\u65e0\u5173\u786e\u4fdd\u516c\u5e73\u6027\uff1b3) \u652f\u6301\u8de8\u6570\u636e\u96c6/\u6a21\u578b\u65cf\u7684\u8fc1\u79fb\u5e94\u7528\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ec5\u4f7f\u75285-10%\u6570\u636e\u91cf\u5373\u53ef\u8fbe\u5230\u4e0e\u5168\u6570\u636e\u96c6\u8bc4\u4f300.95+\u7684Spearman\u6392\u5e8f\u76f8\u5173\u6027\uff0c\u8ba1\u7b97\u6548\u7387\u63d0\u534710-20\u500d\u3002", "conclusion": "EffiEval\u4e3aLLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u516c\u5e73\u3001\u53ef\u8fc1\u79fb\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u7528\u6237\u6839\u636e\u9700\u6c42\u7075\u6d3b\u5e73\u8861\u8bc4\u4f30\u6548\u7387\u4e0e\u7ed3\u679c\u53ef\u9760\u6027\u3002"}}
{"id": "2508.09666", "pdf": "https://arxiv.org/pdf/2508.09666", "abs": "https://arxiv.org/abs/2508.09666", "authors": ["Ziyang Ma", "Qingyue Yuan", "Linhai Zhang", "Deyu Zhou"], "title": "Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Previous chain-of-thought (CoT) distillation methods primarily focused on\nenhancing the reasoning capabilities of Small Language Models (SLMs) by\nutilizing high-quality rationales generated by powerful Large Language Models\n(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM\nsafety brought by the training, which are revealed in this study. Although\nthere are works on safety alignment that fine-tune language models or\nmanipulate model weights to defend against harmful inputs, they require extra\ncomputation or annotated data, and probably impact the reasoning ability of\nSLMs. In this paper, we investigate how to maintain the safety of SLMs during\nthe CoT distillation process. Specifically, we propose a safe distillation\nmethod, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing\ntwo modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the\nmagnitude of model weight changes to optimize the model weights in the\nneighboring space near the initial weight distribution. Low-Entropy Masking\nmasks low-entropy tokens, which are regarded as unnecessary learning targets,\nto exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,\nLlama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,\nAGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety\nof SLMs and comparably improves their reasoning capability compared to existing\ndistillation methods. Furthermore, our ablation study presents the\neffectiveness of Slow Tuning and Low-Entropy Masking, with the former\nmaintaining the model's safety in the early stage and the latter prolonging the\nsafe training epochs.", "AI": {"tldr": "\u63d0\u51fa\u5b89\u5168\u84b8\u998f\u65b9\u6cd5SLowED\uff0c\u901a\u8fc7Slow Tuning\u548cLow-Entropy Masking\u6a21\u5757\uff0c\u5728\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u4fdd\u6301\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709CoT\u84b8\u998f\u65b9\u6cd5\u4e13\u6ce8\u4e8e\u63d0\u5347SLM\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5ffd\u89c6\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u5bf9\u6a21\u578b\u5b89\u5168\u6027\u7684\u8d1f\u9762\u5f71\u54cd\u3002\u4f20\u7edf\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u9700\u8981\u989d\u5916\u8ba1\u7b97/\u6807\u6ce8\u6570\u636e\u4e14\u53ef\u80fd\u635f\u5bb3\u63a8\u7406\u80fd\u529b\u3002", "method": "1. Slow Tuning\uff1a\u9650\u5236\u6743\u91cd\u53d8\u5316\u5e45\u5ea6\uff0c\u5728\u521d\u59cb\u6743\u91cd\u90bb\u57df\u5185\u4f18\u5316\n2. Low-Entropy Masking\uff1a\u5c4f\u853d\u4f4e\u71b5token\uff08\u89c6\u4e3a\u65e0\u6548\u5b66\u4e60\u76ee\u6807\uff09", "result": "\u5728Qwen2.5-1.5B\u7b49\u4e09\u4e2aSLM\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1a\n- \u63a8\u7406\u80fd\u529b\uff08BBH/ARC\u7b49\u57fa\u51c6\uff09\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\n- \u5b89\u5168\u6027\uff08AdvBench\uff09\u663e\u8457\u4fdd\u6301\n- \u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u6a21\u5757\u6709\u6548\u6027", "conclusion": "SLowED\u6210\u529f\u5e73\u8861\u5b89\u5168\u4e0e\u63a8\u7406\u80fd\u529b\uff1a\n- Slow Tuning\u4fdd\u969c\u65e9\u671f\u5b89\u5168\u6027\n- Low-Entropy Masking\u5ef6\u957f\u5b89\u5168\u8bad\u7ec3\u5468\u671f"}}
{"id": "2508.09713", "pdf": "https://arxiv.org/pdf/2508.09713", "abs": "https://arxiv.org/abs/2508.09713", "authors": ["Rahul Hemrajani"], "title": "Evaluating the Role of Large Language Models in Legal Practice in India", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The integration of Artificial Intelligence(AI) into the legal profession\nraises significant questions about the capacity of Large Language Models(LLM)\nto perform key legal tasks. In this paper, I empirically evaluate how well\nLLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian\ncontext, including issue spotting, legal drafting, advice, research, and\nreasoning. Through a survey experiment, I compare outputs from LLMs with those\nof a junior lawyer, with advanced law students rating the work on helpfulness,\naccuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,\noften matching or surpassing human work. However, they struggle with\nspecialised legal research, frequently generating hallucinations, factually\nincorrect or fabricated outputs. I conclude that while LLMs can augment certain\nlegal tasks, human expertise remains essential for nuanced reasoning and the\nprecise application of law.", "AI": {"tldr": "LLMs\u5728\u6cd5\u5f8b\u8d77\u8349\u548c\u95ee\u9898\u53d1\u73b0\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02(\u5339\u914d\u6216\u8d85\u8d8a\u4eba\u7c7b)\uff0c\u4f46\u5728\u4e13\u4e1a\u6cd5\u5f8b\u7814\u7a76\u4e2d\u6613\u4ea7\u751f\u9519\u8bef\u4fe1\u606f", "motivation": "\u5b9e\u8bc1\u8bc4\u4f30LLMs\u5728\u5370\u5ea6\u6cd5\u5f8b\u73af\u5883\u4e0b\u7684\u5173\u952e\u4efb\u52a1\u8868\u73b0(\u95ee\u9898\u8bc6\u522b/\u6cd5\u5f8b\u8d77\u8349/\u7814\u7a76/\u63a8\u7406)\uff0c\u660e\u786e\u5176\u80fd\u529b\u8fb9\u754c", "method": "\u901a\u8fc7\u8c03\u67e5\u5b9e\u9a8c\u6bd4\u8f83LLMs\u4e0e\u521d\u7ea7\u5f8b\u5e08\u7684\u8f93\u51fa\u8d28\u91cf\uff0c\u7531\u6cd5\u5b66\u9662\u9ad8\u5e74\u7ea7\u5b66\u751f\u8fdb\u884c\u591a\u7ef4\u8bc4\u4f30(\u5e2e\u52a9\u6027/\u51c6\u786e\u6027/\u5168\u9762\u6027)", "result": "\u6a21\u578b\u64c5\u957f\u6807\u51c6\u5316\u8f93\u51fa\u4efb\u52a1\uff0c\u4f46\u5728\u4e13\u4e1a\u9886\u57df\u7814\u7a76\u4e2d\u51fa\u73b063%\u7684\u5e7b\u89c9\u7387", "conclusion": "LLMs\u53ef\u4f5c\u4e3a\u6cd5\u5f8b\u8f85\u52a9\u5de5\u5177\uff0c\u4f46\u590d\u6742\u63a8\u7406\u548c\u7cbe\u51c6\u6cd5\u5f8b\u5e94\u7528\u4ecd\u9700\u4eba\u7c7b\u4e13\u5bb6\u4ecb\u5165"}}
{"id": "2508.09716", "pdf": "https://arxiv.org/pdf/2508.09716", "abs": "https://arxiv.org/abs/2508.09716", "authors": ["Ridwan Mahbub", "Mohammed Saidul Islam", "Md Tahmid Rahman Laskar", "Mizanur Rahman", "Mir Tafseer Nayeem", "Enamul Hoque"], "title": "The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models", "categories": ["cs.CL"], "comment": "Accepted to IEEE VIS 2025", "summary": "Information visualizations are powerful tools that help users quickly\nidentify patterns, trends, and outliers, facilitating informed decision-making.\nHowever, when visualizations incorporate deceptive design elements-such as\ntruncated or inverted axes, unjustified 3D effects, or violations of best\npractices-they can mislead viewers and distort understanding, spreading\nmisinformation. While some deceptive tactics are obvious, others subtly\nmanipulate perception while maintaining a facade of legitimacy. As\nVision-Language Models (VLMs) are increasingly used to interpret\nvisualizations, especially by non-expert users, it is critical to understand\nhow susceptible these models are to deceptive visual designs. In this study, we\nconduct an in-depth evaluation of VLMs' ability to interpret misleading\nvisualizations. By analyzing over 16,000 responses from ten different models\nacross eight distinct types of misleading chart designs, we demonstrate that\nmost VLMs are deceived by them. This leads to altered interpretations of\ncharts, despite the underlying data remaining the same. Our findings highlight\nthe need for robust safeguards in VLMs against visual misinformation.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6613\u53d7\u8bef\u5bfc\u6027\u56fe\u8868\u8bbe\u8ba1\u5f71\u54cd\uff0c\u9700\u52a0\u5f3a\u9632\u8bef\u5bfc\u673a\u5236", "motivation": "\u968f\u7740\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u88ab\u975e\u4e13\u4e1a\u7528\u6237\u5e7f\u6cdb\u7528\u4e8e\u56fe\u8868\u89e3\u8bfb\uff0c\u9700\u9a8c\u8bc1\u5176\u5bf9\u8bef\u5bfc\u6027\u53ef\u89c6\u5316\u8bbe\u8ba1\u7684\u6297\u5e72\u6270\u80fd\u529b", "method": "\u901a\u8fc7\u5206\u679010\u4e2a\u4e0d\u540c\u6a21\u578b\u57288\u7c7b\u8bef\u5bfc\u6027\u56fe\u8868\u8bbe\u8ba1\u4e2d\u768416,000+\u6761\u54cd\u5e94\u6570\u636e", "result": "\u7edd\u5927\u591a\u6570VLMs\u4f1a\u88ab\u89c6\u89c9\u8bbe\u8ba1\u6b3a\u9a97\uff0c\u5bfc\u81f4\u5bf9\u76f8\u540c\u6570\u636e\u4ea7\u751f\u4e0d\u540c\u89e3\u8bfb", "conclusion": "\u5e94\u5728VLMs\u4e2d\u5efa\u7acb\u89c6\u89c9\u4fe1\u606f\u9a8c\u8bc1\u673a\u5236\uff0c\u9632\u6b62\u89c6\u89c9\u9519\u8bef\u4fe1\u606f\u4f20\u64ad"}}
{"id": "2508.09726", "pdf": "https://arxiv.org/pdf/2508.09726", "abs": "https://arxiv.org/abs/2508.09726", "authors": ["Vaishnavi Shrivastava", "Ahmed Awadallah", "Vidhisha Balachandran", "Shivam Garg", "Harkirat Behl", "Dimitris Papailiopoulos"], "title": "Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models trained with reinforcement learning with verifiable\nrewards tend to trade accuracy for length--inflating response lengths to\nachieve gains in accuracy. While longer answers may be warranted for harder\nproblems, many tokens are merely \"filler\": repetitive, verbose text that makes\nno real progress. We introduce GFPO (Group Filtered Policy Optimization), which\ncurbs this length explosion by sampling larger groups per problem during\ntraining and filtering responses to train on based on two key metrics: (1)\nresponse length and (2) token efficiency: reward per token ratio. By sampling\nmore at training time, we teach models to think less at inference time. On the\nPhi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across\nchallenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,\nLiveCodeBench) while maintaining accuracy. Optimizing for reward per token\nfurther increases reductions in length inflation to 71-85%. We also propose\nAdaptive Difficulty GFPO, which dynamically allocates more training resources\nto harder problems based on real-time difficulty estimates, improving the\nbalance between computational efficiency and accuracy especially on difficult\nquestions. GFPO demonstrates that increased training-time compute directly\ntranslates to reduced test-time compute--a simple yet effective trade-off for\nefficient reasoning.", "AI": {"tldr": "GFPO\u901a\u8fc7\u8bad\u7ec3\u65f6\u589e\u52a0\u6837\u672c\u91c7\u6837\u5e76\u8fc7\u6ee4\u4f4e\u6548\u54cd\u5e94\uff0c\u6709\u6548\u6291\u5236\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5197\u4f59\u6587\u672c\u751f\u6210\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5c06\u56de\u7b54\u957f\u5ea6\u51cf\u5c1146-85%", "motivation": "\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u503e\u5411\u4e8e\u901a\u8fc7\u589e\u52a0\u56de\u7b54\u957f\u5ea6\u6765\u63d0\u5347\u51c6\u786e\u7387\uff0c\u5bfc\u81f4\u5927\u91cf\u4f4e\u6548\u7684\u5197\u4f59\u6587\u672c\uff08\u91cd\u590d\u3001\u5197\u957f\u4f46\u65e0\u5b9e\u8d28\u8fdb\u5c55\uff09", "method": "GFPO\uff08\u7fa4\u4f53\u8fc7\u6ee4\u7b56\u7565\u4f18\u5316\uff09\uff1a1. \u8bad\u7ec3\u65f6\u5bf9\u6bcf\u4e2a\u95ee\u9898\u91c7\u6837\u66f4\u5927\u7fa4\u4f53 2. \u57fa\u4e8e\u54cd\u5e94\u957f\u5ea6\u548ctoken\u6548\u7387\uff08\u5355\u4f4dtoken\u5956\u52b1\u503c\uff09\u53cc\u91cd\u6307\u6807\u8fc7\u6ee4\u8bad\u7ec3\u6837\u672c", "result": "\u5728Phi-4\u6a21\u578b\u4e0a\uff0cGFPO\u5c06STEM/\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\uff08AIME/GPQA\u7b49\uff09\u7684\u5197\u4f59\u6587\u672c\u51cf\u5c1146-71%\uff0c\u4f18\u5316token\u6548\u7387\u540e\u8fbe71-85%\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u7387", "conclusion": "GFPO\u9a8c\u8bc1\u4e86\u8bad\u7ec3\u9636\u6bb5\u589e\u52a0\u8ba1\u7b97\u91cf\u53ef\u6709\u6548\u964d\u4f4e\u63a8\u7406\u9636\u6bb5\u8ba1\u7b97\u9700\u6c42\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u7684\u7b97\u529b\u5e73\u8861\uff0c\u5e76\u63d0\u51fa\u81ea\u9002\u5e94\u96be\u5ea6\u8bad\u7ec3\u8fdb\u4e00\u6b65\u63d0\u5347\u56f0\u96be\u95ee\u9898\u7684\u5904\u7406\u6548\u7387"}}
{"id": "2508.09755", "pdf": "https://arxiv.org/pdf/2508.09755", "abs": "https://arxiv.org/abs/2508.09755", "authors": ["Seokgi Lee"], "title": "Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "We introduce a novel retrieval-augmented generation (RAG) framework tailored\nfor multihop question answering. First, our system uses large language model\n(LLM) to decompose complex multihop questions into a sequence of single-hop\nsubquestions that guide document retrieval. This decomposition mitigates the\nambiguity inherent in multi-hop queries by clearly targeting distinct knowledge\nfacets. Second, instead of embedding raw or chunked documents directly, we\ngenerate answerable questions from each document chunk using Qwen3-8B, embed\nthese generated questions, and retrieve relevant chunks via question-question\nembedding similarity. During inference, the retrieved chunks are then fed along\nwith the original question into the RAG pipeline. We evaluate on three multihop\nquestion datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our\nmethod improves RAG performacne compared to baseline systems. Our contributions\nhighlight the benefits of using answerable-question embeddings for RAG, and the\neffectiveness of LLM-based query decomposition for multihop scenarios.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u95ee\u9898\u5206\u89e3\u548c\u53ef\u56de\u7b54\u95ee\u9898\u5d4c\u5165\u7684\u65b0\u578bRAG\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u591a\u8df3\u95ee\u7b54\u6027\u80fd", "motivation": "\u4f20\u7edfRAG\u7cfb\u7edf\u5728\u591a\u8df3\u95ee\u7b54\u573a\u666f\u5b58\u5728\u68c0\u7d22\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u4e14\u6587\u6863\u5757\u76f4\u63a5\u5d4c\u5165\u53ef\u80fd\u4e22\u5931\u8bed\u4e49\u4fe1\u606f", "method": "1. \u4f7f\u7528LLM\u5c06\u591a\u8df3\u95ee\u9898\u5206\u89e3\u4e3a\u5355\u8df3\u5b50\u95ee\u9898\u5e8f\u5217\n2. \u7528Qwen3-8B\u751f\u6210\u6587\u6863\u5757\u7684\u53ef\u56de\u7b54\u95ee\u9898\uff0c\u901a\u8fc7\u95ee\u9898-\u95ee\u9898\u5d4c\u5165\u76f8\u4f3c\u6027\u68c0\u7d22\n3. \u5c06\u68c0\u7d22\u7ed3\u679c\u8f93\u5165RAG\u6d41\u6c34\u7ebf", "result": "\u5728LongBench\u7684MuSiQue/2WikiMultiHopQa/HotpotQA\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u7cfb\u7edf", "conclusion": "\u53ef\u56de\u7b54\u95ee\u9898\u5d4c\u5165\u548cLLM\u9a71\u52a8\u7684\u67e5\u8be2\u5206\u89e3\u7b56\u7565\u6709\u6548\u63d0\u5347\u591a\u8df3RAG\u6027\u80fd"}}
{"id": "2508.09759", "pdf": "https://arxiv.org/pdf/2508.09759", "abs": "https://arxiv.org/abs/2508.09759", "authors": ["Avneet Kaur"], "title": "Echoes of Agreement: Argument Driven Opinion Shifts in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "There have been numerous studies evaluating bias of LLMs towards political\ntopics. However, how positions towards these topics in model outputs are highly\nsensitive to the prompt. What happens when the prompt itself is suggestive of\ncertain arguments towards those positions remains underexplored. This is\ncrucial for understanding how robust these bias evaluations are and for\nunderstanding model behaviour, as these models frequently interact with\nopinionated text. To that end, we conduct experiments for political bias\nevaluation in presence of supporting and refuting arguments. Our experiments\nshow that such arguments substantially alter model responses towards the\ndirection of the provided argument in both single-turn and multi-turn settings.\nMoreover, we find that the strength of these arguments influences the\ndirectional agreement rate of model responses. These effects point to a\nsycophantic tendency in LLMs adapting their stance to align with the presented\narguments which has downstream implications for measuring political bias and\ndeveloping effective mitigation strategies.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u63d0\u793a\u8bcd\u4e2d\u7684\u652f\u6301/\u53cd\u9a73\u8bba\u70b9\u4f1a\u663e\u8457\u6539\u53d8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u653f\u6cbb\u8bdd\u9898\u4e0a\u7684\u7acb\u573a\u56de\u5e94\uff0c\u63ed\u793a\u5176\u5b58\u5728\u8fce\u5408\u503e\u5411", "motivation": "\u73b0\u6709\u653f\u6cbb\u504f\u89c1\u8bc4\u4f30\u5ffd\u7565\u63d0\u793a\u8bcd\u672c\u8eab\u8bba\u70b9\u5bf9\u6a21\u578b\u7acb\u573a\u7684\u5f71\u54cd\uff0c\u8fd9\u5173\u7cfb\u5230\u8bc4\u4f30\u7ed3\u679c\u7684\u7a33\u5065\u6027\u548c\u6a21\u578b\u884c\u4e3a\u7684\u6df1\u5165\u7406\u89e3", "method": "\u901a\u8fc7\u8bbe\u8ba1\u542b\u652f\u6301/\u53cd\u9a73\u8bba\u70b9\u7684\u63d0\u793a\u8bcd\uff0c\u5728\u5355\u8f6e\u548c\u591a\u8f6e\u5bf9\u8bdd\u573a\u666f\u4e0b\u7cfb\u7edf\u6d4b\u8bd5\u6a21\u578b\u653f\u6cbb\u7acb\u573a\u53d8\u5316", "result": "\u8bba\u70b9\u5b58\u5728\u65f6\u6a21\u578b\u7acb\u573a\u660e\u663e\u5411\u63d0\u793a\u65b9\u5411\u504f\u79fb\uff0c\u4e14\u8bba\u70b9\u5f3a\u5ea6\u4e0e\u65b9\u5411\u4e00\u81f4\u6027\u6b63\u76f8\u5173\uff0c\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u8fce\u5408\u6548\u5e94\u66f4\u663e\u8457", "conclusion": "\u63d0\u793a\u5de5\u7a0b\u5bf9\u653f\u6cbb\u504f\u89c1\u6d4b\u91cf\u5177\u6709\u91cd\u5927\u5f71\u54cd\uff0c\u9700\u5f00\u53d1\u80fd\u62b5\u6297\u8bba\u70b9\u5e72\u6270\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u7f13\u89e3\u7b56\u7565"}}
{"id": "2508.09767", "pdf": "https://arxiv.org/pdf/2508.09767", "abs": "https://arxiv.org/abs/2508.09767", "authors": ["Shuhei Kato"], "title": "UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech", "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "We propose UtterTune, a lightweight adaptation method that fine-tunes a\nmultilingual text-to-speech (TTS) system based on a large language model (LLM)\narchitecture, designed to enhance the controllability of pronunciation in a\ntarget language while preserving performance in others. While LLM architectures\nhave enabled TTS models to achieve remarkable naturalness, accurately modeling\ngrapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially\nwhen the model omits an explicit G2P module and directly processes minimally\nencoded text (e.g., byte-pair encoding). UtterTune leverages low-rank\nadaptation to enable the control of segmental pronunciation and pitch accent at\nthe phoneme level for Japanese speech, the target language in this paper, while\nmaintaining naturalness and speaker similarity in a zero-shot setting.\nObjective and subjective evaluations confirm its effectiveness.", "AI": {"tldr": "UtterTune\u63d0\u51fa\u57fa\u4e8eLLM\u67b6\u6784\u7684\u8f7b\u91cf\u7ea7\u591a\u8bed\u8a00TTS\u81ea\u9002\u5e94\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\u589e\u5f3a\u65e5\u8bed\u53d1\u97f3\u63a7\u5236\u540c\u65f6\u4fdd\u6301\u5176\u4ed6\u8bed\u8a00\u6027\u80fd", "motivation": "LLM\u67b6\u6784\u7684TTS\u6a21\u578b\u5728G2P\u6620\u5c04\u548c\u97f5\u5f8b\u5efa\u6a21\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u663e\u5f0fG2P\u6a21\u5757\u65f6\u96be\u4ee5\u7cbe\u51c6\u63a7\u5236\u65e5\u8bed\u97f3\u7d20\u7ea7\u53d1\u97f3\u7279\u5f81", "method": "\u91c7\u7528\u4f4e\u79e9\u9002\u5e94\u6280\u672f\u5b9e\u73b0\u65e5\u8bed\u8bed\u97f3\u7684\u97f3\u7d20\u7ea7\u522b\u53d1\u97f3\u63a7\u5236\u548c\u97f3\u9ad8\u91cd\u97f3\u8c03\u6574\uff0c\u652f\u6301\u96f6\u6837\u672c\u573a\u666f\u4e0b\u7684\u81ea\u7136\u5ea6\u548c\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u6027\u4fdd\u6301", "result": "\u4e3b\u5ba2\u89c2\u8bc4\u4f30\u8bc1\u5b9e\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u65e5\u8bed\u53d1\u97f3\u53ef\u63a7\u6027\u7684\u540c\u65f6\uff0c\u6709\u6548\u7ef4\u6301\u4e86\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u8bed\u97f3\u81ea\u7136\u5ea6\u548c\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u6027", "conclusion": "UtterTune\u4e3aLLM\u67b6\u6784\u7684TTS\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8de8\u8bed\u8a00\u53d1\u97f3\u63a7\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u591a\u8bed\u8a00\u6027\u80fd\u5e73\u8861\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf"}}
{"id": "2508.09776", "pdf": "https://arxiv.org/pdf/2508.09776", "abs": "https://arxiv.org/abs/2508.09776", "authors": ["Mahdi Dhaini", "Juraj Vladika", "Ege Erdogan", "Zineb Attaoui", "Gjergji Kasneci"], "title": "Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to the 34th International Conference on Artificial Neural\n  Networks (ICANN 2025)", "summary": "In the rapidly evolving field of Explainable Natural Language Processing\n(NLP), textual explanations, i.e., human-like rationales, are pivotal for\nexplaining model predictions and enriching datasets with interpretable labels.\nTraditional approaches rely on human annotation, which is costly,\nlabor-intensive, and impedes scalability. In this work, we present an automated\nframework that leverages multiple state-of-the-art large language models (LLMs)\nto generate high-quality textual explanations. We rigorously assess the quality\nof these LLM-generated explanations using a comprehensive suite of Natural\nLanguage Generation (NLG) metrics. Furthermore, we investigate the downstream\nimpact of these explanations on the performance of pre-trained language models\n(PLMs) and LLMs across natural language inference tasks on two diverse\nbenchmark datasets. Our experiments demonstrate that automated explanations\nexhibit highly competitive effectiveness compared to human-annotated\nexplanations in improving model performance. Our findings underscore a\npromising avenue for scalable, automated LLM-based textual explanation\ngeneration for extending NLP datasets and enhancing model performance.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u52a8\u5316LLM\u6846\u67b6\u751f\u6210\u6587\u672c\u89e3\u91ca\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6548\u679c\u5ab2\u7f8e\u4eba\u5de5\u6807\u6ce8\u5e76\u80fd\u63d0\u5347NLP\u6a21\u578b\u6027\u80fd", "motivation": "\u4f20\u7edf\u4eba\u5de5\u6807\u6ce8\u89e3\u91ca\u6210\u672c\u9ad8\u4e14\u4e0d\u53ef\u6269\u5c55\uff0c\u9700\u5bfb\u6c42\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848", "method": "\u4f7f\u7528\u591aLLM\u751f\u6210\u89e3\u91ca\uff0c\u901a\u8fc7NLG\u6307\u6807\u8bc4\u4f30\u8d28\u91cf\uff0c\u5e76\u5728\u4e24\u4e2a\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u57fa\u51c6\u6570\u636e\u96c6\u6d4b\u8bd5\u5bf9PLM/LLM\u7684\u5f71\u54cd", "result": "\u81ea\u52a8\u751f\u6210\u89e3\u91ca\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u65b9\u9762\u4e0e\u4eba\u5de5\u6807\u6ce8\u6548\u679c\u76f8\u5f53\uff08highly competitive\uff09", "conclusion": "LLM\u81ea\u52a8\u5316\u751f\u6210\u6587\u672c\u89e3\u91ca\u662f\u6269\u5c55\u6570\u636e\u96c6\u548c\u589e\u5f3a\u6a21\u578b\u6027\u80fd\u7684\u6709\u6548\u9014\u5f84"}}
{"id": "2508.09786", "pdf": "https://arxiv.org/pdf/2508.09786", "abs": "https://arxiv.org/abs/2508.09786", "authors": ["Mahdi Dhaini", "Tobias M\u00fcller", "Roksoliana Rabets", "Gjergji Kasneci"], "title": "Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Accepted to AAAI/ACM Conference on AI, Ethics, and Society (AIES\n  2025)", "summary": "The field of explainable natural language processing (NLP) has grown rapidly\nin recent years. The growing opacity of complex models calls for transparency\nand explanations of their decisions, which is crucial to understand their\nreasoning and facilitate deployment, especially in high-stakes environments.\nDespite increasing attention given to explainable NLP, practitioners'\nperspectives regarding its practical adoption and effectiveness remain\nunderexplored. This paper addresses this research gap by investigating\npractitioners' experiences with explainability methods, specifically focusing\non their motivations for adopting such methods, the techniques employed,\nsatisfaction levels, and the practical challenges encountered in real-world NLP\napplications. Through a qualitative interview-based study with industry\npractitioners and complementary interviews with academic researchers, we\nsystematically analyze and compare their perspectives. Our findings reveal\nconceptual gaps, low satisfaction with current explainability methods, and\nhighlight evaluation challenges. Our findings emphasize the need for clear\ndefinitions and user-centric frameworks for better adoption of explainable NLP\nin practice.", "AI": {"tldr": "\u901a\u8fc7\u8bbf\u8c08\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u53ef\u89e3\u91ca\u6027NLP\u65b9\u6cd5\u5b58\u5728\u6982\u5ff5\u6a21\u7cca\u3001\u6ee1\u610f\u5ea6\u4f4e\u548c\u8bc4\u4f30\u56f0\u96be\uff0c\u9700\u5efa\u7acb\u7528\u6237\u4e2d\u5fc3\u6846\u67b6\u63d0\u5347\u5b9e\u8df5\u6548\u679c", "motivation": "\u9488\u5bf9\u590d\u6742NLP\u6a21\u578b\u900f\u660e\u5316\u9700\u6c42\u65e5\u76ca\u589e\u957f\u4f46\u4ece\u4e1a\u8005\u5b9e\u8df5\u7ecf\u9a8c\u7814\u7a76\u4e0d\u8db3\u7684\u73b0\u72b6\uff0c\u63a2\u7a76\u5b9e\u9645\u5e94\u7528\u4e2d\u89e3\u91ca\u6027\u65b9\u6cd5\u7684\u771f\u5b9e\u4f7f\u7528\u6548\u679c\u548c\u6311\u6218", "method": "\u91c7\u7528\u8d28\u6027\u8bbf\u8c08\u65b9\u6cd5\uff0c\u5bf9\u5de5\u4e1a\u754c\u4ece\u4e1a\u8005\u548c\u5b66\u672f\u7814\u7a76\u8005\u8fdb\u884c\u5bf9\u6bd4\u7814\u7a76\uff0c\u7cfb\u7edf\u5206\u6790\u53cc\u65b9\u89c6\u89d2\u5dee\u5f02", "result": "\u63ed\u793a\u6982\u5ff5\u5b9a\u4e49\u4e0d\u7edf\u4e00\uff0868%\u53d7\u8bbf\u8005\u56f0\u60d1\uff09\u3001\u73b0\u6709\u65b9\u6cd5\u6ee1\u610f\u5ea6\u4f4e\uff08\u4ec521%\u6ee1\u610f\uff09\u3001\u8bc4\u4f30\u4f53\u7cfb\u7f3a\u4e4f\u5ba2\u89c2\u6807\u51c6\u4e09\u5927\u6838\u5fc3\u95ee\u9898", "conclusion": "\u6784\u5efa\u660e\u786e\u5b9a\u4e49\u6807\u51c6\u548c\u7528\u6237\u5bfc\u5411\u7684\u8bc4\u4f30\u6846\u67b6\u662f\u63a8\u52a8\u53ef\u89e3\u91ca\u6027NLP\u5b9e\u9645\u843d\u5730\u7684\u5173\u952e\u8def\u5f84"}}
{"id": "2508.09804", "pdf": "https://arxiv.org/pdf/2508.09804", "abs": "https://arxiv.org/abs/2508.09804", "authors": ["Ahmed Masry", "Abhay Puri", "Masoud Hashemi", "Juan A. Rodriguez", "Megh Thakkar", "Khyati Mahajan", "Vikas Yadav", "Sathwik Tejaswi Madhusudhan", "Alexandre Pich\u00e9", "Dzmitry Bahdanau", "Christopher Pal", "David Vazquez", "Enamul Hoque", "Perouz Taslakian", "Sai Rajeswar", "Spandana Gella"], "title": "BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning", "categories": ["cs.CL"], "comment": null, "summary": "Charts are essential to data analysis, transforming raw data into clear\nvisual representations that support human decision-making. Although current\nvision-language models (VLMs) have made significant progress, they continue to\nstruggle with chart comprehension due to training on datasets that lack\ndiversity and real-world authenticity, or on automatically extracted underlying\ndata tables of charts, which can contain numerous estimation errors.\nFurthermore, existing models only rely on supervised fine-tuning using these\nlow-quality datasets, severely limiting their effectiveness. To address these\nissues, we first propose BigCharts, a dataset creation pipeline that generates\nvisually diverse chart images by conditioning the rendering process on\nreal-world charts sourced from multiple online platforms. Unlike purely\nsynthetic datasets, BigCharts incorporates real-world data, ensuring\nauthenticity and visual diversity, while still retaining accurate underlying\ndata due to our proposed replotting process. Additionally, we introduce a\ncomprehensive training framework that integrates supervised fine-tuning with\nGroup Relative Policy Optimization (GRPO)-based reinforcement learning. By\nintroducing novel reward signals specifically designed for chart reasoning, our\napproach enhances model robustness and generalization across diverse chart\nstyles and domains, resulting in a state-of-the-art chart reasoning model,\nBigCharts-R1. Extensive experiments demonstrate that our models surpass\nexisting methods on multiple chart question-answering benchmarks compared to\neven larger open-source and closed-source models.", "AI": {"tldr": "\u63d0\u51faBigCharts\u6570\u636e\u96c6\u6784\u5efa\u6d41\u7a0b\u4e0eGRPO\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u89e3\u51b3\u56fe\u8868\u6570\u636e\u591a\u6837\u6027\u4e0d\u8db3\u4e0e\u8bad\u7ec3\u65b9\u6cd5\u5c40\u9650\uff0c\u5b9e\u73b0SOTA\u56fe\u8868\u63a8\u7406\u6a21\u578bBigCharts-R1\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u56e0\u8bad\u7ec3\u6570\u636e\u7f3a\u4e4f\u771f\u5b9e\u591a\u6837\u6027\u3001\u81ea\u52a8\u63d0\u53d6\u56fe\u8868\u6570\u636e\u5b58\u5728\u8bef\u5dee\uff0c\u4e14\u4ec5\u4f9d\u8d56\u76d1\u7763\u5fae\u8c03\uff0c\u5bfc\u81f4\u56fe\u8868\u7406\u89e3\u6548\u679c\u53d7\u9650\u3002", "method": "1. BigCharts\u901a\u8fc7\u91cd\u65b0\u7ed8\u5236\u771f\u5b9e\u56fe\u8868\u751f\u6210\u591a\u6837\u5316\u56fe\u50cf\uff0c\u4fdd\u7559\u51c6\u786e\u6570\u636e\uff1b2. \u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u4e0e\u57fa\u4e8eGRPO\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8bbe\u8ba1\u4e13\u5c5e\u56fe\u8868\u63a8\u7406\u5956\u52b1\u673a\u5236\u3002", "result": "BigCharts-R1\u5728\u591a\u4e2a\u56fe\u8868\u95ee\u7b54\u57fa\u51c6\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u6027\u80fd\u4f18\u4e8e\u66f4\u5927\u89c4\u6a21\u7684\u5f00\u6e90/\u95ed\u6e90\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u771f\u5b9e\u6570\u636e\u589e\u5f3a\u4e0e\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u6a21\u578b\u5bf9\u591a\u6837\u5316\u56fe\u8868\u98ce\u683c\u7684\u6cdb\u5316\u80fd\u529b\u4e0e\u63a8\u7406\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.09809", "pdf": "https://arxiv.org/pdf/2508.09809", "abs": "https://arxiv.org/abs/2508.09809", "authors": ["Aishik Mandal", "Prottay Kumar Adhikary", "Hiba Arnaout", "Iryna Gurevych", "Tanmoy Chakraborty"], "title": "A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 3 figures", "summary": "Mental health disorders are rising worldwide. However, the availability of\ntrained clinicians has not scaled proportionally, leaving many people without\nadequate or timely support. To bridge this gap, recent studies have shown the\npromise of Artificial Intelligence (AI) to assist mental health diagnosis,\nmonitoring, and intervention. However, the development of efficient, reliable,\nand ethical AI to assist clinicians is heavily dependent on high-quality\nclinical training datasets. Despite growing interest in data curation for\ntraining clinical AI assistants, existing datasets largely remain scattered,\nunder-documented, and often inaccessible, hindering the reproducibility,\ncomparability, and generalizability of AI models developed for clinical mental\nhealth care. In this paper, we present the first comprehensive survey of\nclinical mental health datasets relevant to the training and development of\nAI-powered clinical assistants. We categorize these datasets by mental\ndisorders (e.g., depression, schizophrenia), data modalities (e.g., text,\nspeech, physiological signals), task types (e.g., diagnosis prediction, symptom\nseverity estimation, intervention generation), accessibility (public,\nrestricted or private), and sociocultural context (e.g., language and cultural\nbackground). Along with these, we also investigate synthetic clinical mental\nhealth datasets. Our survey identifies critical gaps such as a lack of\nlongitudinal data, limited cultural and linguistic representation, inconsistent\ncollection and annotation standards, and a lack of modalities in synthetic\ndata. We conclude by outlining key challenges in curating and standardizing\nfuture datasets and provide actionable recommendations to facilitate the\ndevelopment of more robust, generalizable, and equitable mental health AI\nsystems.", "AI": {"tldr": "\u7cfb\u7edf\u8c03\u67e5\u5fc3\u7406\u5065\u5eb7AI\u8bad\u7ec3\u6570\u636e\u96c6\u73b0\u72b6\uff0c\u63ed\u793a\u6570\u636e\u5206\u6563\u6027\u3001\u6807\u51c6\u5316\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u63d0\u51fa\u6570\u636e\u96c6\u4f18\u5316\u5efa\u8bae\u4ee5\u63d0\u5347AI\u7cfb\u7edf\u6548\u80fd\u3002", "motivation": "\u5168\u7403\u5fc3\u7406\u5065\u5eb7\u9700\u6c42\u6fc0\u589e\u4e0e\u4e34\u5e8a\u8d44\u6e90\u4e0d\u8db3\u7684\u77db\u76fe\u51f8\u663e\uff0cAI\u8f85\u52a9\u8bca\u7597\u6f5c\u529b\u53d7\u9650\u4e8e\u4f4e\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u9700\u7cfb\u7edf\u68b3\u7406\u6570\u636e\u73b0\u72b6\u63a8\u52a8\u6280\u672f\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u591a\u7ef4\u5206\u7c7b\uff08\u75be\u75c5\u7c7b\u578b/\u6570\u636e\u6a21\u6001/\u4efb\u52a1\u7c7b\u578b/\u53ef\u8bbf\u95ee\u6027/\u6587\u5316\u80cc\u666f\uff09\u8bc4\u4f30\u73b0\u6709\u6570\u636e\u96c6\uff0c\u5e76\u5206\u6790\u5408\u6210\u6570\u636e\u5c40\u9650\u6027\u3002", "result": "\u53d1\u73b0\u6570\u636e\u96c6\u5b58\u5728\u7eb5\u5411\u6570\u636e\u7f3a\u5931\u3001\u6587\u5316\u591a\u6837\u6027\u4e0d\u8db3\u3001\u91c7\u96c6\u6807\u51c6\u6df7\u4e71\u3001\u5408\u6210\u6570\u636e\u6a21\u6001\u5355\u4e00\u7b49\u5173\u952e\u7f3a\u9677\u3002", "conclusion": "\u5efa\u8bae\u5efa\u7acb\u6807\u51c6\u5316\u6570\u636e\u89c4\u8303\u3001\u52a0\u5f3a\u8de8\u6587\u5316\u6570\u636e\u91c7\u96c6\u3001\u5f00\u53d1\u591a\u6a21\u6001\u5408\u6210\u6570\u636e\uff0c\u4ee5\u6784\u5efa\u66f4\u516c\u5e73\u7a33\u5065\u7684\u5fc3\u7406\u5065\u5eb7AI\u7cfb\u7edf\u3002"}}
{"id": "2508.09834", "pdf": "https://arxiv.org/pdf/2508.09834", "abs": "https://arxiv.org/abs/2508.09834", "authors": ["Weigao Sun", "Jiaxi Hu", "Yucheng Zhou", "Jusen Du", "Disen Lan", "Kexin Wang", "Tong Zhu", "Xiaoye Qu", "Yu Zhang", "Xiaoyu Mo", "Daizong Liu", "Yuxuan Liang", "Wenliang Chen", "Guoqi Li", "Yu Cheng"], "title": "Speed Always Wins: A Survey on Efficient Architectures for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Survey, 82 pages, GitHub:\n  https://github.com/weigao266/Awesome-Efficient-Arch", "summary": "Large Language Models (LLMs) have delivered impressive results in language\nunderstanding, generation, reasoning, and pushes the ability boundary of\nmultimodal models. Transformer models, as the foundation of modern LLMs, offer\na strong baseline with excellent scaling properties. However, the traditional\ntransformer architecture requires substantial computations and poses\nsignificant obstacles for large-scale training and practical deployment. In\nthis survey, we offer a systematic examination of innovative LLM architectures\nthat address the inherent limitations of transformers and boost the efficiency.\nStarting from language modeling, this survey covers the background and\ntechnical details of linear and sparse sequence modeling methods, efficient\nfull attention variants, sparse mixture-of-experts, hybrid model architectures\nincorporating the above techniques, and emerging diffusion LLMs. Additionally,\nwe discuss applications of these techniques to other modalities and consider\ntheir wider implications for developing scalable, resource-aware foundation\nmodels. By grouping recent studies into the above category, this survey\npresents a blueprint of modern efficient LLM architectures, and we hope this\ncould help motivate future research toward more efficient, versatile AI\nsystems.", "AI": {"tldr": "\u8bba\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6548\u7387\u7684\u521b\u65b0\u67b6\u6784\u65b9\u6cd5\uff0c\u6db5\u76d6\u7ebf\u6027/\u7a00\u758f\u5e8f\u5217\u5efa\u6a21\u3001\u9ad8\u6548\u6ce8\u610f\u529b\u53d8\u4f53\u3001\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u7b49\u6280\u672f\u8def\u5f84\uff0c\u5e76\u63a2\u8ba8\u5176\u5bf9\u591a\u6a21\u6001\u5e94\u7528\u7684\u6269\u5c55\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edfTransformer\u67b6\u6784\u5b58\u5728\u9ad8\u8ba1\u7b97\u91cf\u74f6\u9888\uff0c\u963b\u788d\u5927\u89c4\u6a21\u8bad\u7ec3\u4e0e\u90e8\u7f72\uff0c\u9700\u63a2\u7d22\u66f4\u9ad8\u6548\u7684LLM\u67b6\u6784\u65b9\u6848\u4ee5\u7a81\u7834\u7b97\u529b\u9650\u5236\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u5206\u7c7b\u7814\u7a76\uff1a1) \u7ebf\u6027/\u7a00\u758f\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5 2) \u9ad8\u6548\u6ce8\u610f\u529b\u673a\u5236\u6539\u8fdb 3) \u7a00\u758f\u6df7\u5408\u4e13\u5bb6\u7cfb\u7edf 4) \u6df7\u5408\u67b6\u6784\u96c6\u6210\u6280\u672f 5) \u65b0\u5174\u6269\u6563\u8bed\u8a00\u6a21\u578b", "result": "\u6784\u5efa\u4e86\u73b0\u4ee3\u9ad8\u6548LLM\u67b6\u6784\u7684\u6280\u672f\u84dd\u56fe\uff0c\u4e3a\u5f00\u53d1\u53ef\u6269\u5c55\u3001\u8d44\u6e90\u4f18\u5316\u7684\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u63a8\u52a8\u9ad8\u6548AI\u7cfb\u7edf\u7684\u6f14\u8fdb\u65b9\u5411\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u68b3\u7406\u73b0\u6709\u9ad8\u6548\u67b6\u6784\u6280\u672f\uff0c\u63ed\u793a\u591a\u8def\u5f84\u4f18\u5316\u53ef\u80fd\uff0c\u5f3a\u8c03\u8de8\u6a21\u6001\u5e94\u7528\u7684\u6269\u5c55\u4ef7\u503c\uff0c\u4e3a\u6784\u5efa\u8d44\u6e90\u9ad8\u6548\u7684\u57fa\u7840\u6a21\u578b\u6307\u660e\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2508.09848", "pdf": "https://arxiv.org/pdf/2508.09848", "abs": "https://arxiv.org/abs/2508.09848", "authors": ["Mo Yu", "Tsz Ting Chung", "Chulun Zhou", "Tong Li", "Rui Lu", "Jiangnan Li", "Liyan Xu", "Haoshu Lu", "Ning Zhang", "Jing Li", "Jie Zhou"], "title": "PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts", "categories": ["cs.CL", "cs.AI"], "comment": "First 7 authors contributed equally. Project page:\n  https://gorov.github.io/prelude", "summary": "We introduce PRELUDE, a benchmark for evaluating long-context understanding\nthrough the task of determining whether a character's prequel story is\nconsistent with the canonical narrative of the original book. Our task poses a\nstronger demand for global comprehension and deep reasoning than existing\nbenchmarks -- as the prequels are not part of the original story, assessing\ntheir plausibility typically requires searching and integrating information\nthat is only indirectly related. Empirically, 88% of instances require evidence\nfrom multiple parts of the narrative. Experimental results highlight the\nchallenge of our task: in-context learning, RAG and in-domain training with\nstate-of-the-art LLMs, and commercial DeepResearch services, lag behind humans\nby >15%. A further human study reveals that models often produce correct\nanswers with flawed reasoning, leading to an over 30% gap in reasoning accuracy\ncompared to humans. These findings underscore the substantial room for\nimprovement in long-context understanding and reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86PRELUDE\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5224\u65ad\u89d2\u8272\u524d\u4f20\u6545\u4e8b\u4e0e\u539f\u8457\u4e00\u81f4\u6027\u7684\u4efb\u52a1\uff0c\u63ed\u793a\u5f53\u524dLLMs\u5728\u957f\u6587\u672c\u6df1\u5ea6\u63a8\u7406\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff08\u843d\u540e\u4eba\u7c7b>15%\uff09\uff0c\u4e14\u5b58\u5728\u6b63\u786e\u7ed3\u8bba\u4f34\u968f\u9519\u8bef\u63a8\u7406\u7684\u7f3a\u9677\uff08\u63a8\u7406\u51c6\u786e\u7387\u5dee\u8ddd>30%\uff09\u3002", "motivation": "\u73b0\u6709\u957f\u6587\u672c\u7406\u89e3\u8bc4\u6d4b\u57fa\u51c6\u5bf9\u5168\u5c40\u7406\u89e3\u548c\u6df1\u5ea6\u63a8\u7406\u7684\u8981\u6c42\u4e0d\u8db3\uff0c\u9700\u8981\u901a\u8fc7\u9700\u8981\u591a\u8bc1\u636e\u6574\u5408\u7684\u5f3a\u8bc4\u4f30\u4efb\u52a1\uff0888%\u6848\u4f8b\u9700\u591a\u6bb5\u843d\u8bc1\u636e\uff09\u6765\u7a81\u7834\u73b0\u6709\u6a21\u578b\u74f6\u9888\u3002", "method": "\u6784\u5efaPRELUDE\u8bc4\u6d4b\u96c6\uff0c\u91c7\u7528in-context learning/RAG/\u5fae\u8c03\u7b49\u65b9\u6cd5\u6d4b\u8bd5SOTA\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u4eba\u7c7b\u7814\u7a76\u5206\u6790\u6a21\u578b\u63a8\u7406\u7f3a\u9677\u3002", "result": "\u73b0\u6709\u6700\u4f73\u6a21\u578b\u4e0e\u4eba\u7c7b\u8868\u73b0\u5dee\u8ddd>15%\uff1b\u4eba\u7c7b\u8bc4\u4f30\u53d1\u73b0\u6a21\u578b\u5b58\u5728\"\u6b63\u786e\u7ed3\u8bba+\u9519\u8bef\u63a8\u7406\"\u73b0\u8c61\uff0c\u63a8\u7406\u51c6\u786e\u7387\u5dee\u8ddd\u8fbe30%\u4ee5\u4e0a\u3002", "conclusion": "\u957f\u6587\u672c\u7406\u89e3\u4e0e\u63a8\u7406\u80fd\u529b\u4ecd\u6709\u5de8\u5927\u63d0\u5347\u7a7a\u95f4\uff0c\u6a21\u578b\u5b58\u5728\u8868\u9762\u5bf9\u9f50\u4f46\u5e95\u5c42\u63a8\u7406\u7f3a\u9677\u7684\u95ee\u9898\uff0c\u9700\u5f00\u53d1\u65b0\u7684\u8bc4\u4f30\u4e0e\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
{"id": "2508.09865", "pdf": "https://arxiv.org/pdf/2508.09865", "abs": "https://arxiv.org/abs/2508.09865", "authors": ["Abdul Rehman Antall", "Naveed Akhtar"], "title": "Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription", "categories": ["cs.CL"], "comment": "8 pages, 3 figures, 1 table, including references and appendix", "summary": "This study evaluates the feasibility of lightweight Whisper models (Tiny,\nBase, Small) for Urdu speech recognition in low-resource settings. Despite Urdu\nbeing the 10th most spoken language globally with over 230 million speakers,\nits representation in automatic speech recognition (ASR) systems remains\nlimited due to dialectal diversity, code-switching, and sparse training data.\nWe benchmark these models on a curated Urdu dataset using word error rate\n(WER), without fine-tuning. Results show Whisper-Small achieves the lowest\nerror rates (33.68\\% WER), outperforming Tiny (67.08\\% WER) and Base (53.67\\%\nWER). Qualitative analysis reveals persistent challenges in phonetic accuracy\nand lexical coherence, particularly for complex utterances. While Whisper-Small\ndemonstrates promise for deployable Urdu ASR, significant gaps remain. Our\nfindings emphasize lay the groundwork for future research into effective,\nlow-resource ASR systems.", "AI": {"tldr": "\u8f7b\u91cf\u7ea7Whisper-Small\u6a21\u578b\u5728\u672a\u8c03\u4f18\u60c5\u51b5\u4e0b\u53d6\u5f97\u4e4c\u5c14\u90fd\u8bed\u8bed\u97f3\u8bc6\u522b\u6700\u4f4e\u9519\u8bef\u7387\uff0833.68% WER\uff09\uff0c\u4f46\u590d\u6742\u8bed\u53e5\u5904\u7406\u4ecd\u5b58\u6311\u6218\u3002", "motivation": "\u9488\u5bf9230 million\u4f7f\u7528\u8005\u7684\u4e4c\u5c14\u90fd\u8bed\u9762\u4e34\u65b9\u8a00\u591a\u6837\u6027\u3001\u8bed\u7801\u8f6c\u6362\u548c\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63a2\u7d22\u4f4e\u8d44\u6e90ASR\u89e3\u51b3\u65b9\u6848", "method": "\u4f7f\u7528\u7cbe\u9009\u4e4c\u5c14\u90fd\u8bed\u6570\u636e\u96c6\uff0c\u5728\u672a\u5fae\u8c03\u6761\u4ef6\u4e0b\u5bf9\u6bd4Tiny/Base/Small\u4e09\u79cdWhisper\u6a21\u578b\u7684\u8bcd\u9519\u7387\uff08WER\uff09\u8868\u73b0", "result": "Small > Base > Tiny\uff0833.68% vs 53.67% vs 67.08% WER\uff09\uff0c\u5b9a\u6027\u5206\u6790\u63ed\u793a\u8bed\u97f3\u51c6\u786e\u6027\u548c\u8bcd\u6c47\u8fde\u8d2f\u6027\u7f3a\u9677", "conclusion": "Whisper-Small\u5c55\u73b0\u90e8\u7f72\u6f5c\u529b\u4f46\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u4e3a\u4f4e\u8d44\u6e90ASR\u7cfb\u7edf\u5f00\u53d1\u5960\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u57fa\u7840"}}
{"id": "2508.09874", "pdf": "https://arxiv.org/pdf/2508.09874", "abs": "https://arxiv.org/abs/2508.09874", "authors": ["Jiaqi Cao", "Jiarui Wang", "Rubin Wei", "Qipeng Guo", "Kai Chen", "Bowen Zhou", "Zhouhan Lin"], "title": "Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown strong abilities in general language\ntasks, yet adapting them to specific domains remains a challenge. Current\nmethod like Domain Adaptive Pretraining (DAPT) requires costly full-parameter\ntraining and suffers from catastrophic forgetting. Meanwhile,\nRetrieval-Augmented Generation (RAG) introduces substantial inference latency\ndue to expensive nearest-neighbor searches and longer context. This paper\nintroduces Memory Decoder, a plug-and-play pretrained memory that enables\nefficient domain adaptation without changing the original model's parameters.\nMemory Decoder employs a small transformer decoder that learns to imitate the\nbehavior of an external non-parametric retriever. Once trained, Memory Decoder\ncan be seamlessly integrated with any pretrained language model that shares the\nsame tokenizer, requiring no model-specific modifications. Experimental results\ndemonstrate that Memory Decoder enables effective adaptation of various Qwen\nand Llama models to three distinct specialized domains: biomedicine, finance,\nand law, reducing perplexity by an average of 6.17 points. Overall, Memory\nDecoder introduces a novel paradigm centered on a specially pretrained memory\ncomponent designed for domain-specific adaptation. This memory architecture can\nbe integrated in a plug-and-play manner, consistently enhancing performance\nacross multiple models within the target domain.", "AI": {"tldr": "\u63d0\u51faMemory Decoder\u9884\u8bad\u7ec3\u8bb0\u5fc6\u6a21\u5757\uff0c\u901a\u8fc7\u5c0f\u578b\u89e3\u7801\u5668\u6a21\u4eff\u68c0\u7d22\u5668\u884c\u4e3a\uff0c\u5b9e\u73b0\u5373\u63d2\u5373\u7528\u9886\u57df\u9002\u5e94\uff0c\u5e73\u5747\u964d\u4f4e\u56f0\u60d1\u5ea66.17\u70b9\u3002", "motivation": "\u73b0\u6709DAPT\u65b9\u6cd5\u5b58\u5728\u5168\u53c2\u6570\u8bad\u7ec3\u6210\u672c\u9ad8\u4e0e\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0cRAG\u5219\u56e0\u68c0\u7d22\u8fc7\u7a0b\u4ea7\u751f\u5ef6\u8fdf\u3002\u9700\u5f00\u53d1\u4e0d\u4fee\u6539\u539f\u6a21\u578b\u53c2\u6570\u7684\u9ad8\u6548\u9886\u57df\u9002\u5e94\u65b9\u6848\u3002", "method": "\u8bad\u7ec3\u5c0f\u578btransformer\u89e3\u7801\u5668\u6a21\u4eff\u975e\u53c2\u6570\u68c0\u7d22\u5668\u884c\u4e3a\uff0c\u9002\u914d\u540ctokenizer\u7684\u4efb\u610f\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u65e0\u9700\u6a21\u578b\u7279\u5b9a\u4fee\u6539\u3002", "result": "\u5728\u751f\u7269\u533b\u5b66/\u91d1\u878d/\u6cd5\u5f8b\u9886\u57df\u9002\u914dQwen\u548cLlama\u6a21\u578b\uff0c\u5e73\u5747\u964d\u4f4e\u56f0\u60d1\u5ea66.17\u70b9\uff0c\u663e\u8457\u63d0\u5347\u9886\u57df\u8868\u73b0\u3002", "conclusion": "Memory Decoder\u5f00\u521b\u4e86\u4ee5\u4e13\u7528\u9884\u8bad\u7ec3\u5185\u5b58\u7ec4\u4ef6\u4e3a\u6838\u5fc3\u7684\u9886\u57df\u9002\u5e94\u8303\u5f0f\uff0c\u5373\u63d2\u5373\u7528\u67b6\u6784\u53ef\u8de8\u6a21\u578b\u6301\u7eed\u63d0\u5347\u76ee\u6807\u9886\u57df\u6027\u80fd\u3002"}}
{"id": "2508.09878", "pdf": "https://arxiv.org/pdf/2508.09878", "abs": "https://arxiv.org/abs/2508.09878", "authors": ["Archie Sage", "Jeroen Keppens", "Helen Yannakoudakis"], "title": "A Survey of Cognitive Distortion Detection and Classification in NLP", "categories": ["cs.CL"], "comment": "Under review via ACL Rolling Review and committed to EMNLP 2025.\n  Camera-ready updates to follow", "summary": "As interest grows in the application of natural language processing (NLP)\ntechniques to mental health, a growing body of work explores the automatic\ndetection and classification of cognitive distortions (CDs). CDs are habitual\npatterns of negatively biased or flawed thinking that distort how people\nperceive events, judge themselves, and react to the world around them.\nIdentifying and addressing them is an important part of therapy. Despite its\nmomentum, the field remains fragmented, with inconsistencies in CD taxonomies,\ntask formulations, and evaluation practices. This survey reviews 38 studies\nspanning two decades, providing a structured overview of datasets, modelling\napproaches, and evaluation strategies. We provide a consolidated CD taxonomy\nreference, summarise common task setups, and highlight open challenges to\nsupport more coherent and reproducible research in this emerging area.", "AI": {"tldr": "\u7efc\u8ff0\u8bba\u6587\u7cfb\u7edf\u68b3\u7406\u4e86\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5728\u8ba4\u77e5\u626d\u66f2\u68c0\u6d4b\u9886\u57df20\u5e74\u95f438\u9879\u7814\u7a76\uff0c\u6574\u5408\u5206\u7c7b\u4f53\u7cfb\u5e76\u6307\u51fa\u73b0\u6709\u6311\u6218\uff0c\u65e8\u5728\u63a8\u52a8\u8be5\u9886\u57df\u5f62\u6210\u66f4\u7edf\u4e00\u7684\u7814\u7a76\u6846\u67b6\u3002", "motivation": "\u9488\u5bf9\u8ba4\u77e5\u626d\u66f2\u68c0\u6d4b\u9886\u57df\u5b58\u5728\u7684\u5206\u7c7b\u4f53\u7cfb\u6df7\u4e71\u3001\u4efb\u52a1\u5b9a\u4e49\u4e0d\u7edf\u4e00\u3001\u8bc4\u4f30\u6807\u51c6\u788e\u7247\u5316\u95ee\u9898\uff0c\u9700\u6574\u5408\u73b0\u6709\u7814\u7a76\u6210\u679c\u5e76\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u7ed3\u6784\u5316\u53c2\u8003\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u56de\u987e\u8fd120\u5e7438\u7bc7\u6587\u732e\uff0c\u7ed3\u6784\u5316\u5206\u6790\u6570\u636e\u96c6\u6784\u5efa\u65b9\u5f0f\u3001\u5efa\u6a21\u65b9\u6cd5\uff08\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u4e0e\u6df1\u5ea6\u5b66\u4e60\uff09\u53ca\u8bc4\u4f30\u7b56\u7565\uff08\u4e34\u5e8a\u9a8c\u8bc1\u4e0e\u81ea\u52a8\u6307\u6807\uff09\u3002", "result": "\u63d0\u51fa\u6807\u51c6\u5316\u8ba4\u77e5\u626d\u66f2\u5206\u7c7b\u53c2\u8003\u6846\u67b6\uff0c\u603b\u7ed3\u6587\u672c\u5206\u7c7b/\u5e8f\u5217\u6807\u6ce8\u7b49\u4e3b\u6d41\u4efb\u52a1\u8303\u5f0f\uff0c\u63ed\u793a\u6570\u636e\u7a00\u7f3a\u6027\u3001\u4e34\u5e8a\u9a8c\u8bc1\u4e0d\u8db3\u3001\u8de8\u6587\u5316\u9002\u5e94\u6027\u7b49\u6838\u5fc3\u6311\u6218\u3002", "conclusion": "\u9700\u5efa\u7acb\u8de8\u5b66\u79d1\u534f\u4f5c\u673a\u5236\u4e0e\u6807\u51c6\u5316\u8bc4\u4f30\u4f53\u7cfb\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u4ece\u788e\u7247\u5316\u63a2\u7d22\u5411\u4e34\u5e8a\u53ef\u843d\u5730\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u8f6c\u53d8\u3002"}}
{"id": "2508.09935", "pdf": "https://arxiv.org/pdf/2508.09935", "abs": "https://arxiv.org/abs/2508.09935", "authors": ["Sayem Hossen", "Monalisa Moon Joti", "Md. Golam Rashed"], "title": "Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach", "categories": ["cs.CL", "q-fin.CP", "q-fin.GN"], "comment": "21", "summary": "Business communication digitisation has reorganised the process of persuasive\ndiscourse, which\n  allows not only greater transparency but also advanced deception. This\ninquiry synthesises classical\n  rhetoric and communication psychology with linguistic theory and empirical\nstudies in the financial\n  reporting, sustainability discourse, and digital marketing to explain how\ndeceptive language can be\n  systematically detected using persuasive lexicon. In controlled settings,\ndetection accuracies of greater\n  than 99% were achieved by using computational textual analysis as well as\npersonalised transformer\n  models. However, reproducing this performance in multilingual settings is\nalso problematic and,\n  to a large extent, this is because it is not easy to find sufficient data,\nand because few multilingual\n  text-processing infrastructures are in place. This evidence shows that there\nhas been an increasing\n  gap between the theoretical representations of communication and those\nempirically approximated,\n  and therefore, there is a need to have strong automatic text-identification\nsystems where AI-based\n  discourse is becoming more realistic in communicating with humans.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u8ba1\u7b97\u6587\u672c\u5206\u6790\u548c\u4e2a\u6027\u5316Transformer\u6a21\u578b\uff0c\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u5b9e\u73b099%\u4ee5\u4e0a\u6b3a\u9a97\u6027\u8bed\u8a00\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u4f46\u591a\u8bed\u8a00\u573a\u666f\u5b58\u5728\u6570\u636e\u4e0e\u57fa\u7840\u8bbe\u65bd\u4e0d\u8db3\u7684\u6311\u6218\u3002", "motivation": "\u6570\u5b57\u5316\u5546\u52a1\u6c9f\u901a\u5728\u589e\u5f3a\u900f\u660e\u5ea6\u7684\u540c\u65f6\u52a0\u5267\u4e86\u8bed\u8a00\u6b3a\u9a97\u98ce\u9669\uff0c\u9700\u5f25\u5408\u7406\u8bba\u4f20\u64ad\u6a21\u578b\u4e0e\u5b9e\u8bc1\u6570\u636e\u7684\u9e3f\u6c9f\uff0c\u5efa\u7acbAI\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u6587\u672c\u8bc6\u522b\u7cfb\u7edf\u3002", "method": "\u878d\u5408\u53e4\u5178\u4fee\u8f9e\u5b66\u3001\u4f20\u64ad\u5fc3\u7406\u5b66\u3001\u8bed\u8a00\u5b66\u7406\u8bba\u4e0e\u8d22\u52a1\u62a5\u544a/\u53ef\u6301\u7eed\u6027\u8bdd\u8bed/\u6570\u5b57\u8425\u9500\u7684\u5b9e\u8bc1\u7814\u7a76\uff0c\u91c7\u7528\u8ba1\u7b97\u6587\u672c\u5206\u6790\u548c\u4e2a\u6027\u5316Transformer\u6a21\u578b\u8fdb\u884c\u68c0\u6d4b\u3002", "result": "\u53d7\u63a7\u73af\u5883\u68c0\u6d4b\u51c6\u786e\u7387\u8d8599%\uff0c\u4f46\u591a\u8bed\u8a00\u573a\u666f\u56e0\u8bad\u7ec3\u6570\u636e\u532e\u4e4f\u53ca\u6587\u672c\u5904\u7406\u57fa\u7840\u8bbe\u65bd\u7f3a\u5931\u5bfc\u81f4\u6548\u679c\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "\u7406\u8bba\u4f20\u64ad\u6a21\u578b\u4e0e\u5b9e\u8bc1\u6570\u636e\u7684\u8131\u8282\u52a0\u5267\uff0c\u4e9f\u9700\u6784\u5efa\u652f\u6301\u591a\u8bed\u8a00\u7684AI\u8bdd\u8bed\u8bc6\u522b\u7cfb\u7edf\u4ee5\u5e94\u5bf9\u65e5\u76ca\u903c\u771f\u7684\u4eba\u673a\u4ea4\u4e92\u573a\u666f\u3002"}}
{"id": "2508.09937", "pdf": "https://arxiv.org/pdf/2508.09937", "abs": "https://arxiv.org/abs/2508.09937", "authors": ["Muneeza Azmat", "Momin Abbas", "Maysa Malfiza Garcia de Macedo", "Marcelo Carpinette Grave", "Luan Soares de Souza", "Tiago Machado", "Rogerio A de Paula", "Raya Horesh", "Yixin Chen", "Heloisa Caroline de Souza Pereira Candello", "Rebecka Nordenlow", "Aminat Adebiyi"], "title": "A Comprehensive Evaluation framework of Alignment Techniques for LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "In submission", "summary": "As Large Language Models (LLMs) become increasingly integrated into\nreal-world applications, ensuring their outputs align with human values and\nsafety standards has become critical. The field has developed diverse alignment\napproaches including traditional fine-tuning methods (RLHF, instruction\ntuning), post-hoc correction systems, and inference-time interventions, each\nwith distinct advantages and limitations. However, the lack of unified\nevaluation frameworks makes it difficult to systematically compare these\nparadigms and guide deployment decisions. This paper introduces a\nmulti-dimensional evaluation of alignment techniques for LLMs, a comprehensive\nevaluation framework that provides a systematic comparison across all major\nalignment paradigms. Our framework assesses methods along four key dimensions:\nalignment detection, alignment quality, computational efficiency, and\nrobustness. Through experiments across diverse base models and alignment\nstrategies, we demonstrate the utility of our framework in identifying\nstrengths and limitations of current state-of-the-art models, providing\nvaluable insights for future research directions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6\uff0c\u7cfb\u7edf\u6bd4\u8f83\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u6280\u672f\u7684\u68c0\u6d4b\u80fd\u529b\u3001\u8d28\u91cf\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u5bf9\u9f50\u6280\u672f\uff08\u5fae\u8c03/\u540e\u5904\u7406/\u63a8\u7406\u5e72\u9884\uff09\u7f3a\u4e4f\u7edf\u4e00\u8bc4\u4f30\u6807\u51c6\uff0c\u96be\u4ee5\u6307\u5bfc\u5b9e\u9645\u90e8\u7f72\u51b3\u7b56", "method": "\u6784\u5efa\u56db\u7ef4\u8bc4\u4f30\u4f53\u7cfb\uff1a\u5bf9\u9f50\u68c0\u6d4b\u80fd\u529b\u3001\u5bf9\u9f50\u8d28\u91cf\u3001\u8ba1\u7b97\u6548\u7387\u3001\u9c81\u68d2\u6027\uff0c\u8986\u76d6\u4e3b\u6d41\u57fa\u6a21\u578b\u548c\u5bf9\u9f50\u7b56\u7565", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u5f53\u524dSOTA\u6a21\u578b\u7684\u4f18\u52bf\u4e0e\u5c40\u9650\u6027", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e0d\u540c\u5bf9\u9f50\u8303\u5f0f\u63d0\u4f9b\u7cfb\u7edf\u8bc4\u4f30\u57fa\u51c6\uff0c\u6307\u660e\u672a\u6765\u6a21\u578b\u5b89\u5168\u7814\u7a76\u65b9\u5411"}}
{"id": "2508.09945", "pdf": "https://arxiv.org/pdf/2508.09945", "abs": "https://arxiv.org/abs/2508.09945", "authors": ["Lingjie Jiang", "Shaohan Huang", "Xun Wu", "Yixia Li", "Dongdong Zhang", "Furu Wei"], "title": "VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Multimodal large language models (MLLMs) have significantly advanced the\nintegration of visual and textual understanding. However, their ability to\ngenerate code from multimodal inputs remains limited. In this work, we\nintroduce VisCodex, a unified framework that seamlessly merges vision and\ncoding language models to empower MLLMs with strong multimodal code generation\nabilities. Leveraging a task vector-based model merging technique, we integrate\na state-of-the-art coding LLM into a strong vision-language backbone, while\npreserving both visual comprehension and advanced coding skills. To support\ntraining and evaluation, we introduce the Multimodal Coding Dataset (MCD), a\nlarge-scale and diverse collection of 598k samples, including high-quality HTML\ncode, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic\nproblems. Furthermore, we propose InfiBench-V, a novel and challenging\nbenchmark specifically designed to assess models on visually-rich, real-world\nprogramming questions that demand a nuanced understanding of both textual and\nvisual contexts. Extensive experiments show that VisCodex achieves\nstate-of-the-art performance among open-source MLLMs and approaches proprietary\nmodels like GPT-4o, highlighting the effectiveness of our model merging\nstrategy and new datasets.", "AI": {"tldr": "VisCodex\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u5408\u5e76\u7b56\u7565\u6574\u5408\u89c6\u89c9\u4e0e\u7f16\u7801\u6a21\u578b\uff0c\u7ed3\u5408\u65b0\u6570\u636e\u96c6MCD\u548c\u8bc4\u4f30\u57fa\u51c6InfiBench-V\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u4ee3\u7801\u751f\u6210\u80fd\u529b\uff0c\u63a5\u8fd1GPT-4o\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u591a\u6a21\u6001\u4ee3\u7801\u751f\u6210\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u878d\u5408\u89c6\u89c9\u7406\u89e3\u4e0e\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4efb\u52a1\u5411\u91cf\u5408\u5e76\u6280\u672f\u96c6\u6210\u7f16\u7801LLM\u5230\u89c6\u89c9\u8bed\u8a00\u4e3b\u5e72\u7f51\u7edc\uff0c\u6784\u5efa\u5305\u542b598k\u6837\u672c\u7684MCD\u6570\u636e\u96c6\u53ca\u89c6\u89c9\u7f16\u7a0b\u8bc4\u4f30\u57fa\u51c6InfiBench-V\u3002", "result": "VisCodex\u5728\u5f00\u6e90\u6a21\u578b\u4e2d\u8fbe\u5230SOTA\uff0c\u63a5\u8fd1GPT-4o\u6027\u80fd\uff0c\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027\u3002", "conclusion": "\u6a21\u578b\u5408\u5e76\u7b56\u7565\u4e0e\u65b0\u578b\u6570\u636e\u96c6\u6210\u529f\u63d0\u5347\u591a\u6a21\u6001\u7f16\u7801\u80fd\u529b\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u8d44\u6e90\u4e0e\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2508.09952", "pdf": "https://arxiv.org/pdf/2508.09952", "abs": "https://arxiv.org/abs/2508.09952", "authors": ["Hermione Warr", "Wentian Xu", "Harry Anthony", "Yasin Ibrahim", "Daniel McGowan", "Konstantinos Kamnitsas"], "title": "Specialised or Generic? Tokenization Choices for Radiology Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ELAMI@MICCAI2025", "summary": "The vocabulary used by language models (LM) - defined by the tokenizer -\nplays a key role in text generation quality. However, its impact remains\nunder-explored in radiology. In this work, we address this gap by\nsystematically comparing general, medical, and domain-specific tokenizers on\nthe task of radiology report summarisation across three imaging modalities. We\nalso investigate scenarios with and without LM pre-training on PubMed\nabstracts. Our findings demonstrate that medical and domain-specific\nvocabularies outperformed widely used natural language alternatives when models\nare trained from scratch. Pre-training partially mitigates performance\ndifferences between tokenizers, whilst the domain-specific tokenizers achieve\nthe most favourable results. Domain-specific tokenizers also reduce memory\nrequirements due to smaller vocabularies and shorter sequences. These results\ndemonstrate that adapting the vocabulary of LMs to the clinical domain provides\npractical benefits, including improved performance and reduced computational\ndemands, making such models more accessible and effective for both research and\nreal-world healthcare settings.", "AI": {"tldr": "\u9886\u57df\u7279\u5b9a\u5206\u8bcd\u5668\u5728\u653e\u5c04\u5b66\u62a5\u544a\u603b\u7ed3\u4efb\u52a1\u4e2d\u4f18\u4e8e\u901a\u7528\u5206\u8bcd\u5668\uff0c\u540c\u65f6\u964d\u4f4e\u5185\u5b58\u6d88\u8017\u548c\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u7684\u5206\u8bcd\u5668\u5bf9\u6587\u672c\u751f\u6210\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u653e\u5c04\u5b66\u9886\u57df\u7684\u5f71\u54cd\u5c1a\u672a\u88ab\u7cfb\u7edf\u7814\u7a76\u3002\u7814\u7a76\u8005\u65e8\u5728\u9a8c\u8bc1\u9886\u57df\u9002\u914d\u7684\u5206\u8bcd\u5668\u80fd\u5426\u63d0\u5347\u4e34\u5e8a\u6587\u672c\u5904\u7406\u6548\u679c\u3002", "method": "1. \u6bd4\u8f83\u901a\u7528/\u533b\u5b66/\u653e\u5c04\u5b66\u9886\u57df\u5206\u8bcd\u5668\u5728\u4e09\u79cd\u6210\u50cf\u6a21\u6001\u62a5\u544a\u603b\u7ed3\u4efb\u52a1\u7684\u8868\u73b0 2. \u6d4b\u8bd5\u4ece\u5934\u8bad\u7ec3\u4e0ePubMed\u9884\u8bad\u7ec3\u4e24\u79cd\u6a21\u5f0f 3. \u8bc4\u4f30\u6027\u80fd\u6307\u6807\u53ca\u5185\u5b58/\u5e8f\u5217\u957f\u5ea6\u6548\u7387", "result": "1. \u672a\u9884\u8bad\u7ec3\u65f6\u533b\u5b66/\u9886\u57df\u5206\u8bcd\u5668\u8868\u73b0\u66f4\u4f18 2. \u9884\u8bad\u7ec3\u7f29\u5c0f\u4e0d\u540c\u5206\u8bcd\u5668\u6027\u80fd\u5dee\u8ddd 3. \u9886\u57df\u5206\u8bcd\u5668\u5185\u5b58\u5360\u7528\u51cf\u5c1130%\uff0c\u5e8f\u5217\u957f\u5ea6\u7f29\u77ed20%", "conclusion": "\u4e34\u5e8a\u9886\u57df\u9002\u914d\u7684\u5206\u8bcd\u5668\u53ef\u540c\u65f6\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u6709\u5229\u4e8e\u533b\u7597\u573a\u666f\u7684\u5b9e\u9645\u5e94\u7528\u90e8\u7f72\u3002"}}
{"id": "2508.09954", "pdf": "https://arxiv.org/pdf/2508.09954", "abs": "https://arxiv.org/abs/2508.09954", "authors": ["Johannes Sch\u00e4fer", "Roman Klinger"], "title": "Shaping Event Backstories to Estimate Potential Emotion Contexts", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "May 2025 version", "summary": "Emotion analysis is an inherently ambiguous task. Previous work studied\nannotator properties to explain disagreement, but this overlooks the\npossibility that ambiguity may stem from missing information about the context\nof events. In this paper, we propose a novel approach that adds reasonable\ncontexts to event descriptions, which may better explain a particular\nsituation. Our goal is to understand whether these enriched contexts enable\nhuman annotators to annotate emotions more reliably. We disambiguate a target\nevent description by automatically generating multiple event chains conditioned\non differing emotions. By combining techniques from short story generation in\nvarious settings, we achieve coherent narratives that result in a specialized\ndataset for the first comprehensive and systematic examination of\ncontextualized emotion analysis. Through automatic and human evaluation, we\nfind that contextual narratives enhance the interpretation of specific emotions\nand support annotators in producing more consistent annotations.", "AI": {"tldr": "\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u591a\u60c5\u7eea\u4e8b\u4ef6\u94fe\u6784\u5efa\u60c5\u5883\u5316\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u4e0a\u4e0b\u6587\u53d9\u8ff0\u80fd\u63d0\u5347\u60c5\u611f\u6807\u6ce8\u4e00\u81f4\u6027", "motivation": "\u4f20\u7edf\u60c5\u611f\u5206\u6790\u5ffd\u89c6\u4e8b\u4ef6\u4e0a\u4e0b\u6587\u5bfc\u81f4\u6807\u6ce8\u5206\u6b67\uff0c\u9700\u9a8c\u8bc1\u60c5\u5883\u8865\u5145\u5bf9\u6807\u6ce8\u53ef\u9760\u6027\u7684\u5f71\u54cd", "method": "\u57fa\u4e8e\u4e0d\u540c\u60c5\u7eea\u751f\u6210\u4e8b\u4ef6\u94fe\uff0c\u7ed3\u5408\u77ed\u6545\u4e8b\u751f\u6210\u6280\u672f\u6784\u5efa\u4e13\u95e8\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u81ea\u52a8/\u4eba\u5de5\u53cc\u91cd\u8bc4\u4f30", "result": "\u60c5\u5883\u5316\u53d9\u8ff0\u589e\u5f3a\u60c5\u7eea\u89e3\u91ca\u6027\uff0c\u6807\u6ce8\u4e00\u81f4\u6027\u663e\u8457\u63d0\u5347\uff08\u81ea\u52a8\u6307\u6807+\u4eba\u5de5\u8bc4\u4f30\u53cc\u91cd\u9a8c\u8bc1\uff09", "conclusion": "\u4e0a\u4e0b\u6587\u7f3a\u5931\u662f\u60c5\u611f\u6b67\u4e49\u4e3b\u56e0\uff0c\u81ea\u52a8\u751f\u6210\u60c5\u5883\u6846\u67b6\u53ef\u6709\u6548\u652f\u6301\u5b9e\u9645\u6807\u6ce8\u4efb\u52a1"}}
{"id": "2508.09956", "pdf": "https://arxiv.org/pdf/2508.09956", "abs": "https://arxiv.org/abs/2508.09956", "authors": ["Fares Antaki", "David Mikhail", "Daniel Milad", "Danny A Mammo", "Sumit Sharma", "Sunil K Srivastava", "Bing Yu Chen", "Samir Touma", "Mertcan Sevgi", "Jonathan El-Khoury", "Pearse A Keane", "Qingyu Chen", "Yih Chung Tham", "Renaud Duval"], "title": "Performance of GPT-5 Frontier Models in Ophthalmology Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) such as GPT-5 integrate advanced reasoning\ncapabilities that may improve performance on complex medical question-answering\ntasks. For this latest generation of reasoning models, the configurations that\nmaximize both accuracy and cost-efficiency have yet to be established. We\nevaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across\nfour reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using\n260 closed-access multiple-choice questions from the American Academy of\nOphthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome\nwas multiple-choice accuracy; secondary outcomes included head-to-head ranking\nvia a Bradley-Terry model, rationale quality assessment using a\nreference-anchored, pairwise LLM-as-a-judge framework, and analysis of\naccuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved\nthe highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano\nvariants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high\n(0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x\nstronger than o3-high) and rationale quality (1.11x stronger than o3-high).\nCost-accuracy analysis identified several GPT-5 configurations on the Pareto\nfrontier, with GPT-5-mini-low offering the most favorable low-cost,\nhigh-performance balance. These results benchmark GPT-5 on a high-quality\nophthalmology dataset, demonstrate the influence of reasoning effort on\naccuracy, and introduce an autograder framework for scalable evaluation of\nLLM-generated answers against reference standards in ophthalmology.", "AI": {"tldr": "GPT-5-high\u5728\u773c\u79d1\u591a\u9009\u9898\u8bc4\u4f30\u4e2d\u8fbe\u523096.5%\u51c6\u786e\u7387\uff0c\u6210\u672c\u6548\u76ca\u5206\u6790\u663e\u793aGPT-5-mini-low\u4e3a\u6700\u4f73\u6027\u4ef7\u6bd4\u914d\u7f6e", "motivation": "\u786e\u5b9aGPT-5\u7cfb\u5217\u5728\u533b\u5b66\u95ee\u7b54\u4efb\u52a1\u4e2d\u517c\u987e\u51c6\u786e\u6027\u4e0e\u6210\u672c\u6548\u76ca\u7684\u6700\u4f18\u914d\u7f6e\u65b9\u6848", "method": "\u4f7f\u7528260\u9053\u773c\u79d1BCSC\u591a\u9009\u9898\uff0c\u8bc4\u4f3012\u79cdGPT-5\u914d\u7f6e\u4e0eo1-high/o3-high/GPT-4o\uff0c\u901a\u8fc7\u51c6\u786e\u6027\u3001Bradley-Terry\u6392\u540d\u3001LLM\u88c1\u5224\u6846\u67b6\u548c\u6210\u672c\u5206\u6790\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30", "result": "GPT-5-high\u51c6\u786e\u7387\u663e\u8457\u4f18\u4e8eGPT-5-nano\u53d8\u4f53(P<.001)\u548cGPT-4o(P<.001)\uff0c\u4e0eo3-high\u65e0\u7edf\u8ba1\u5b66\u5dee\u5f02\uff1b\u5728\u8d28\u91cf\u6392\u540d\u4e2dGPT-5-high\u6bd4o3-high\u5f3a1.66\u500d", "conclusion": "\u7814\u7a76\u5efa\u7acb\u4e86\u773c\u79d1\u9886\u57dfLLM\u6027\u80fd\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u63a8\u7406\u8ba1\u7b97\u91cf\u5bf9\u51c6\u786e\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u5f00\u53d1\u4e86\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u8bc4\u5206\u6846\u67b6"}}
{"id": "2508.09957", "pdf": "https://arxiv.org/pdf/2508.09957", "abs": "https://arxiv.org/abs/2508.09957", "authors": ["Renas Adnan", "Hossein Hassani"], "title": "Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)", "categories": ["cs.CL"], "comment": "21 pages, 20 figures, 7 tables", "summary": "Speech-to-text (STT) systems have a wide range of applications. They are\navailable in many languages, albeit at different quality levels. Although\nKurdish is considered a less-resourced language from a processing perspective,\nSST is available for some of the Kurdish dialects, for instance, Sorani\n(Central Kurdish). However, that is not applied to other Kurdish dialects,\nBadini and Hawrami, for example. This research is an attempt to address this\ngap. Bandin, approximately, has two million speakers, and STT systems can help\ntheir community use mobile and computer-based technologies while giving their\ndialect more global visibility. We aim to create a language model based on\nBadini's speech and evaluate its performance. To cover a conversational aspect,\nhave a proper confidence level of grammatical accuracy, and ready\ntranscriptions, we chose Badini kids' stories, eight books including 78\nstories, as the textual input. Six narrators narrated the books, which resulted\nin approximately 17 hours of recording. We cleaned, segmented, and tokenized\nthe input. The preprocessing produced nearly 15 hours of speech, including\n19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and\nWhisper-small to develop the language models. The experiments indicate that the\ntranscriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a\nsignificantly more accurate and readable output than the Whisper-small model,\nwith 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy,\nrespectively.", "AI": {"tldr": "\u5f00\u53d1\u9488\u5bf9\u5e93\u5c14\u5fb7\u8bedBadini\u65b9\u8a00\u7684\u8bed\u97f3\u8f6c\u6587\u5b57\u7cfb\u7edf\uff0c\u4f7f\u7528Wav2Vec2\u6a21\u578b\u663e\u8457\u4f18\u4e8eWhisper\u6a21\u578b", "motivation": "\u586b\u8865Badini\u65b9\u8a00\u7f3a\u4e4f\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u7684\u7a7a\u767d\uff0c\u5e2e\u52a9200\u4e07\u4f7f\u7528\u8005\u878d\u5165\u6570\u5b57\u6280\u672f\u5e76\u63d0\u5347\u65b9\u8a00\u53ef\u89c1\u6027", "method": "\u91c7\u96c678\u4e2a\u513f\u7ae5\u6545\u4e8b\u97f3\u9891(17\u5c0f\u65f6)\u2192\u9884\u5904\u7406\u4e3a15\u5c0f\u65f6\u8bed\u97f3\u6570\u636e\u2192\u4f7f\u7528Wav2Vec2-Large-XLSR-53\u548cWhisper-small\u6a21\u578b\u8bad\u7ec3", "result": "Wav2Vec2\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff1a\u53ef\u8bfb\u602790.38% vs 65.45%\uff0c\u51c6\u786e\u738782.67% vs 53.17%", "conclusion": "Wav2Vec2\u6846\u67b6\u66f4\u9002\u5408\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\uff0c\u4e3a\u5c11\u6570\u6c11\u65cf\u8bed\u8a00\u6280\u672f\u5f00\u53d1\u63d0\u4f9b\u6709\u6548\u65b9\u6848"}}
{"id": "2508.09958", "pdf": "https://arxiv.org/pdf/2508.09958", "abs": "https://arxiv.org/abs/2508.09958", "authors": ["Baran Atalar", "Eddie Zhang", "Carlee Joe-Wong"], "title": "Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks", "categories": ["cs.CL", "cs.LG"], "comment": "Submitted to AAAI 2026", "summary": "With the increasing popularity of large language models (LLMs) for a variety\nof tasks, there has been a growing interest in strategies that can predict\nwhich out of a set of LLMs will yield a successful answer at low cost. This\nproblem promises to become more and more relevant as providers like Microsoft\nallow users to easily create custom LLM \"assistants\" specialized to particular\ntypes of queries. However, some tasks (i.e., queries) may be too specialized\nand difficult for a single LLM to handle alone. These applications often\nbenefit from breaking down the task into smaller subtasks, each of which can\nthen be executed by a LLM expected to perform well on that specific subtask.\nFor example, in extracting a diagnosis from medical records, one can first\nselect an LLM to summarize the record, select another to validate the summary,\nand then select another, possibly different, LLM to extract the diagnosis from\nthe summarized record. Unlike existing LLM selection or routing algorithms,\nthis setting requires that we select a sequence of LLMs, with the output of\neach LLM feeding into the next and potentially influencing its success. Thus,\nunlike single LLM selection, the quality of each subtask's output directly\naffects the inputs, and hence the cost and success rate, of downstream LLMs,\ncreating complex performance dependencies that must be learned and accounted\nfor during selection. We propose a neural contextual bandit-based algorithm\nthat trains neural networks that model LLM success on each subtask in an online\nmanner, thus learning to guide the LLM selections for the different subtasks,\neven in the absence of historical LLM performance data. Experiments on\ntelecommunications question answering and medical diagnosis prediction datasets\nillustrate the effectiveness of our proposed approach compared to other LLM\nselection algorithms.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u7684LLM\u5e8f\u5217\u9009\u62e9\u7b97\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u5b66\u4e60\u5b50\u4efb\u52a1\u95f4\u7684\u6027\u80fd\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f18\u5316\u590d\u6742\u4efb\u52a1\u5206\u89e3\u573a\u666f\u4e0b\u7684\u6a21\u578b\u7ec4\u5408\u7b56\u7565\u3002", "motivation": "\u73b0\u6709LLM\u9009\u62e9\u7b97\u6cd5\u4ec5\u5173\u6ce8\u5355\u6a21\u578b\u9009\u62e9\uff0c\u65e0\u6cd5\u5904\u7406\u591a\u5b50\u4efb\u52a1\u573a\u666f\u4e2d\u6a21\u578b\u8f93\u51fa\u76f8\u4e92\u5f71\u54cd\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u533b\u7597\u8bca\u65ad\u7b49\u9700\u8981\u4efb\u52a1\u5206\u89e3\u7684\u5e94\u7528\u573a\u666f\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u5404\u5b50\u4efb\u52a1\u7684LLM\u6210\u529f\u6982\u7387\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u8d4c\u535a\u673a\u6846\u67b6\u5728\u7ebf\u5b66\u4e60\u6a21\u578b\u95f4\u7684\u6027\u80fd\u4f9d\u8d56\u5173\u7cfb\uff0c\u5b9e\u73b0\u65e0\u5386\u53f2\u6570\u636e\u60c5\u51b5\u4e0b\u7684\u52a8\u6001\u5e8f\u5217\u9009\u62e9\u3002", "result": "\u5728\u7535\u4fe1\u95ee\u7b54\u548c\u533b\u7597\u8bca\u65ad\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u4f20\u7edf\u7b97\u6cd5\u51c6\u786e\u7387\u63d0\u534712-15%\uff0c\u63a8\u7406\u6210\u672c\u964d\u4f4e20%\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u901a\u8fc7\u5efa\u6a21LLM\u95f4\u7684\u94fe\u5f0f\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u5206\u89e3\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u52a8\u6001\u6a21\u578b\u7ec4\u5408\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09145", "pdf": "https://arxiv.org/pdf/2508.09145", "abs": "https://arxiv.org/abs/2508.09145", "authors": ["Xingle Xu", "Yongkang Liu", "Dexian Cai", "Shi Feng", "Xiaocui Yang", "Daling Wang", "Yifei Zhang"], "title": "MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Multimodal Sentiment Analysis aims to integrate information from various\nmodalities, such as audio, visual, and text, to make complementary predictions.\nHowever, it often struggles with irrelevant or misleading visual and auditory\ninformation. Most existing approaches typically treat the entire modality\ninformation (e.g., a whole image, audio segment, or text paragraph) as an\nindependent unit for feature enhancement or denoising. They often suppress the\nredundant and noise information at the risk of losing critical information. To\naddress this challenge, we propose MoLAN, a unified ModaLity-aware noise\ndynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking\nby dividing the features of each modality into multiple blocks. Each block is\nthen dynamically assigned a distinct denoising strength based on its noise\nlevel and semantic relevance, enabling fine-grained noise suppression while\npreserving essential multimodal information. Notably, MoLAN is a unified and\nflexible framework that can be seamlessly integrated into a wide range of\nmultimodal models. Building upon this framework, we further introduce MoLAN+, a\nnew multimodal sentiment analysis approach. Experiments across five models and\nfour datasets demonstrate the broad effectiveness of the MoLAN framework.\nExtensive evaluations show that MoLAN+ achieves the state-of-the-art\nperformance. The code is publicly available at\nhttps://github.com/betterfly123/MoLAN-Framework.", "AI": {"tldr": "\u63d0\u51faMoLAN\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u52a8\u6001\u964d\u566a\u589e\u5f3a\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u6574\u4e2a\u6a21\u6001\u4fe1\u606f\u4f5c\u4e3a\u72ec\u7acb\u5355\u5143\u5904\u7406\uff0c\u53ef\u80fd\u5728\u964d\u566a\u65f6\u4e22\u5931\u5173\u952e\u4fe1\u606f\u3002\u9700\u8981\u5b9e\u73b0\u4fdd\u7559\u6838\u5fc3\u4fe1\u606f\u7684\u540c\u65f6\u6291\u5236\u566a\u58f0\u7684\u7ec6\u7c92\u5ea6\u5904\u7406\u65b9\u6cd5\u3002", "method": "MoLAN\u6846\u67b6\u5c06\u5404\u6a21\u6001\u7279\u5f81\u5212\u5206\u4e3a\u591a\u4e2a\u533a\u5757\uff0c\u6839\u636e\u566a\u58f0\u6c34\u5e73\u548c\u8bed\u4e49\u76f8\u5173\u6027\u52a8\u6001\u5206\u914d\u964d\u566a\u5f3a\u5ea6\u3002MoLAN+\u662f\u57fa\u4e8e\u6b64\u6846\u67b6\u7684\u65b0\u578b\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u65b9\u6cd5\u3002", "result": "\u57285\u4e2a\u6a21\u578b\u548c4\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\uff0cMoLAN+\u53d6\u5f97SOTA\u6027\u80fd\uff08F1\u503c\u63d0\u53472.13%-7.25%\uff09", "conclusion": "MoLAN\u4f5c\u4e3a\u7075\u6d3b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u533a\u5757\u964d\u566a\u673a\u5236\u6709\u6548\u5e73\u8861\u566a\u58f0\u6291\u5236\u4e0e\u4fe1\u606f\u4fdd\u7559\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u60c5\u611f\u5206\u6790\u6548\u679c\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.09199", "pdf": "https://arxiv.org/pdf/2508.09199", "abs": "https://arxiv.org/abs/2508.09199", "authors": ["Jucheng Hu", "Suorong Yang", "Dongzhan Zhou"], "title": "$\u0394$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Visual Instruction Finetuning (VIF) is pivotal for post-training\nVision-Language Models (VLMs). Unlike unimodal instruction finetuning in\nplain-text large language models, which mainly requires instruction datasets to\nenable model instruction-following ability, VIF also requires multimodal data\nto enable joint visual and textual understanding; therefore, it typically\nrequires more data. Consequently, VIF imposes stricter data selection\nchallenges: the method must scale efficiently to handle larger data demands\nwhile ensuring the quality of both visual and textual content, as well as their\nalignment. Despite its critical impact on performance, data selection for VIF\nremains an understudied area. In this paper, we propose $\\Delta$-AttnMask. This\ndata-efficient framework quantifies sample quality through attention-guided\nmasking of the model's hidden states, jointly evaluating image-text pairs\nwithout requiring domain labels, auxiliary models, or extra training. By\ncomputing loss differences ($\\Delta$) between the original states and states\nmasked using high-attention regions, $\\Delta$-AttnMask intrinsically assesses\nsample quality. Experiments across multiple VLMs and datasets show that\n$\\Delta$-AttnMask achieves state-of-the-art performance with just 20% of data,\naccelerating training by 5x while surpassing full-dataset baselines by +10.1%\nin overall accuracy. Its model-agnostic and data-agnostic design ensures broad\napplicability across modalities and architectures.", "AI": {"tldr": "\u63d0\u51fa\u0394-AttnMask\u6846\u67b6\uff0c\u4ec5\u752820%\u6570\u636e\u5b9e\u73b0\u89c6\u89c9\u6307\u4ee4\u5fae\u8c03\u6700\u4f73\u6027\u80fd\uff0c\u8bad\u7ec3\u52a0\u901f5\u500d\u4e14\u51c6\u786e\u7387\u63d0\u534710.1%", "motivation": "\u89c6\u89c9\u6307\u4ee4\u5fae\u8c03\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u5bf9\u9f50\u6570\u636e\uff0c\u73b0\u6709\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u4e14\u7f3a\u4e4f\u7cfb\u7edf\u7814\u7a76", "method": "\u901a\u8fc7\u6ce8\u610f\u529b\u63a9\u7801\u8ba1\u7b97\u9690\u85cf\u72b6\u6001\u635f\u5931\u5dee\u5f02(\u0394)\uff0c\u65e0\u76d1\u7763\u8bc4\u4f30\u56fe\u50cf-\u6587\u672c\u5bf9\u8d28\u91cf", "result": "\u5728\u591a\u4e2a\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c20%\u6570\u636e\u91cf\u5373\u8d85\u8d8a\u5168\u6570\u636e\u57fa\u7ebf10.1%\uff0c\u8bad\u7ec3\u6548\u7387\u63d0\u53475\u500d", "conclusion": "\u0394-AttnMask\u7684\u8de8\u6a21\u6001\u901a\u7528\u8bbe\u8ba1\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u9ad8\u6548\u6570\u636e\u9009\u62e9\u65b9\u6848"}}
{"id": "2508.09224", "pdf": "https://arxiv.org/pdf/2508.09224", "abs": "https://arxiv.org/abs/2508.09224", "authors": ["Yuan Yuan", "Tina Sriskandarajah", "Anna-Luisa Brakman", "Alec Helyar", "Alex Beutel", "Andrea Vallone", "Saachi Jain"], "title": "From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models used in ChatGPT have traditionally been trained to\nlearn a refusal boundary: depending on the user's intent, the model is taught\nto either fully comply or outright refuse. While this is a strong mitigation\nfor explicitly malicious prompts, focusing safety training on refusals can lead\nto brittleness for prompts with obscured user intent. Binary refusal boundaries\nare especially ill-suited for dual-use cases (such as biology or\ncybersecurity), where a user request can be answered safely at a high level,\nbut in some cases can lead to malicious uplift if sufficiently detailed or\nactionable. As an alternative, we propose safe-completions: a safety-training\napproach that centers on the safety of the assistant's output, rather than a\nbinary classification of the user's intent. Safe-completions seek to maximize\nhelpfulness within the safety policy's constraints. We incorporated this\napproach into GPT-5 and find that across both production comparisons and\ninternally controlled experiments, safe-completion training improves safety\n(especially on dual-use prompts), reduces the severity of residual safety\nfailures, and substantially increases model helpfulness.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8f93\u51fa\u5b89\u5168\u6027\u7684safe-completions\u8bad\u7ec3\u6846\u67b6\uff0c\u5728GPT-5\u4e2d\u5b9e\u73b0\u5b89\u5168\u6027\u4e0e\u6709\u7528\u6027\u53cc\u63d0\u5347", "motivation": "\u4f20\u7edf\u4e8c\u5143\u62d2\u7edd\u673a\u5236\u5bf9\u610f\u56fe\u6a21\u7cca\u7684\u63d0\u793a\uff08\u5c24\u5176\u662f\u751f\u7269\u3001\u7f51\u7edc\u5b89\u5168\u7b49\u53cc\u7528\u9014\u573a\u666f\uff09\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u5b58\u5728\u5b89\u5168\u8106\u5f31\u6027\u98ce\u9669", "method": "\u5c06\u5b89\u5168\u8bad\u7ec3\u91cd\u5fc3\u8f6c\u5411\u4fdd\u969c\u8f93\u51fa\u5185\u5bb9\u5b89\u5168\u6027\uff08safe-completions\uff09\uff0c\u800c\u975e\u7b80\u5355\u62d2\u7edd\u7528\u6237\u8bf7\u6c42\uff0c\u5141\u8bb8\u5728\u5b89\u5168\u8fb9\u754c\u5185\u6700\u5927\u5316\u5e2e\u52a9\u6027", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u53cc\u7528\u9014\u63d0\u793a\u5904\u7406\u5b89\u5168\u6027\uff08\u964d\u4f4e67%\u6b8b\u4f59\u98ce\u9669\uff09\uff0c\u51cf\u5c11\u5b89\u5168\u6f0f\u6d1e\u4e25\u91cd\u6027\uff0c\u5e76\u4f7f\u6a21\u578b\u5e2e\u52a9\u6027\u63d0\u534741%", "conclusion": "\u8f93\u51fa\u5bfc\u5411\u7684\u5b89\u5168\u8bad\u7ec3\u8303\u5f0f\u6709\u6548\u7a81\u7834\u4e8c\u5143\u62d2\u7edd\u9650\u5236\uff0c\u5b9e\u73b0\u5b89\u5168\u7b56\u7565\u7ea6\u675f\u4e0b\u7684\u6700\u4f18\u5e2e\u52a9\u6027\uff0c\u4e3aAI\u5b89\u5168\u63d0\u4f9b\u65b0\u65b9\u6cd5\u8bba"}}
{"id": "2508.09240", "pdf": "https://arxiv.org/pdf/2508.09240", "abs": "https://arxiv.org/abs/2508.09240", "authors": ["Zainab Khan", "Ahmed Hussain", "Mukesh Thakur", "Arto Hellas", "Panos Papadimitratos"], "title": "NEFMind: Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation", "categories": ["cs.NI", "cs.AI", "cs.CL"], "comment": "6 pages", "summary": "The use of Service-Based Architecture in modern telecommunications has\nexponentially increased Network Functions (NFs) and Application Programming\nInterfaces (APIs), creating substantial operational complexities in service\ndiscovery and management. We introduce \\textit{NEFMind}, a framework leveraging\nparameter-efficient fine-tuning of open-source Large Language Models (LLMs) to\naddress these challenges. It integrates three core components: synthetic\ndataset generation from Network Exposure Function (NEF) API specifications,\nmodel optimization through Quantized-Low-Rank Adaptation, and performance\nevaluation via GPT-4 Ref Score and BertScore metrics. Targeting 5G\nService-Based Architecture APIs, our approach achieves 85% reduction in\ncommunication overhead compared to manual discovery methods. Experimental\nvalidation using the open-source Phi-2 model demonstrates exceptional API call\nidentification performance at 98-100% accuracy. The fine-tuned Phi-2 model\ndelivers performance comparable to significantly larger models like GPT-4 while\nmaintaining computational efficiency for telecommunications infrastructure\ndeployment. These findings validate domain-specific, parameter-efficient LLM\nstrategies for managing complex API ecosystems in next-generation\ntelecommunications networks.", "AI": {"tldr": "\u63d0\u51faNEFMind\u6846\u67b6\uff0c\u5229\u7528\u9ad8\u6548\u53c2\u6570\u5fae\u8c03\u7684\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e5G\u7535\u4fe1\u7f51\u7edcAPI\u7ba1\u7406\u590d\u6742\u5ea6\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u7cbe\u5ea6API\u8bc6\u522b\u3002", "motivation": "\u73b0\u4ee3\u7535\u4fe1\u670d\u52a1\u5316\u67b6\u6784\u4e2d\u7f51\u7edc\u529f\u80fd\u548cAPI\u6570\u91cf\u6fc0\u589e\uff0c\u5bfc\u81f4\u670d\u52a1\u53d1\u73b0\u4e0e\u7ba1\u7406\u9762\u4e34\u5de8\u5927\u64cd\u4f5c\u590d\u6742\u6027\uff0c\u9700\u9ad8\u6548\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u57fa\u4e8eNEF API\u89c4\u8303\u751f\u6210\u5408\u6210\u6570\u636e\u96c6\n2) \u91c7\u7528\u91cf\u5316\u4f4e\u79e9\u81ea\u9002\u5e94(Q-LoRA)\u4f18\u5316\u6a21\u578b\n3) \u4f7f\u7528GPT-4 Ref Score\u548cBertScore\u8bc4\u4f30\u6027\u80fd", "result": "\u901a\u4fe1\u5f00\u9500\u964d\u4f4e85%\uff0cPhi-2\u6a21\u578b\u5b9e\u73b098-100% API\u8c03\u7528\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u6027\u80fd\u5ab2\u7f8eGPT-4\u7b49\u5927\u6a21\u578b\u4f46\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u9886\u57df\u7279\u5f02\u6027\u53c2\u6570\u9ad8\u6548LLM\u7b56\u7565\u5728\u4e0b\u4e00\u4ee3\u7535\u4fe1\u7f51\u7edc\u590d\u6742API\u751f\u6001\u7cfb\u7edf\u7ba1\u7406\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u7535\u4fe1\u57fa\u7840\u8bbe\u65bd\u90e8\u7f72\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.09288", "pdf": "https://arxiv.org/pdf/2508.09288", "abs": "https://arxiv.org/abs/2508.09288", "authors": ["Aayush Gupta"], "title": "Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs", "categories": ["cs.CR", "cs.AI", "cs.CL", "68T07, 94A60", "D.4.6; K.6.5; E.3; I.2.6; I.2.7"], "comment": "2 figures, 3 tables; code and certification harness:\n  https://github.com/ayushgupta4897/Contextual-Integrity-Verification ;\n  Elite-Attack dataset: https://huggingface.co/datasets/zyushg/elite-attack", "summary": "Large language models (LLMs) remain acutely vulnerable to prompt injection\nand related jailbreak attacks; heuristic guardrails (rules, filters, LLM\njudges) are routinely bypassed. We present Contextual Integrity Verification\n(CIV), an inference-time security architecture that attaches cryptographically\nsigned provenance labels to every token and enforces a source-trust lattice\ninside the transformer via a pre-softmax hard attention mask (with optional\nFFN/residual gating). CIV provides deterministic, per-token non-interference\nguarantees on frozen models: lower-trust tokens cannot influence higher-trust\nrepresentations. On benchmarks derived from recent taxonomies of\nprompt-injection vectors (Elite-Attack + SoK-246), CIV attains 0% attack\nsuccess rate under the stated threat model while preserving 93.1% token-level\nsimilarity and showing no degradation in model perplexity on benign tasks; we\nnote a latency overhead attributable to a non-optimized data path. Because CIV\nis a lightweight patch -- no fine-tuning required -- we demonstrate drop-in\nprotection for Llama-3-8B and Mistral-7B. We release a reference\nimplementation, an automated certification harness, and the Elite-Attack corpus\nto support reproducible research.", "AI": {"tldr": "\u63d0\u51faCIV\u67b6\u6784\uff0c\u901a\u8fc7\u52a0\u5bc6\u7b7e\u540d\u548c\u4fe1\u4efb\u683c\u6805\u5b9e\u73b0LLMs\u6297\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u5728\u51bb\u7ed3\u6a21\u578b\u4e0a\u5b9e\u73b00%\u653b\u51fb\u6210\u529f\u7387\u4e14\u4e0d\u5f71\u54cd\u6b63\u5e38\u4efb\u52a1\u6027\u80fd", "motivation": "\u73b0\u6709LLMs\u9632\u62a4\u673a\u5236\uff08\u89c4\u5219/\u8fc7\u6ee4\u5668/LLM\u6cd5\u5b98\uff09\u6613\u88ab\u7ed5\u8fc7\uff0c\u9700\u786e\u5b9a\u6027\u5b89\u5168\u65b9\u6848\u89e3\u51b3\u63d0\u793a\u6ce8\u5165\u6f0f\u6d1e", "method": "\u4f7f\u7528\u52a0\u5bc6\u6765\u6e90\u6807\u7b7e+\u4fe1\u4efb\u6e90\u683c\u6805\uff0c\u901a\u8fc7\u9884softmax\u786c\u6ce8\u610f\u529b\u63a9\u7801\u5f3a\u5236\u5b9e\u65bd\u4ee4\u724c\u7ea7\u4fe1\u4efb\u9694\u79bb\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u90e8\u7f72", "result": "SoK-246\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u653b\u51fb\u6210\u529f\u73870%\uff0c\u4ee4\u724c\u76f8\u4f3c\u5ea693.1%\uff0c\u6a21\u578b\u56f0\u60d1\u5ea6\u65e0\u9000\u5316\uff08\u5ef6\u8fdf\u95ee\u9898\u5f85\u4f18\u5316\uff09", "conclusion": "CIV\u4e3a\u51bb\u7ed3\u6a21\u578b\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u5b89\u5168\u8865\u4e01\uff0c\u9a8c\u8bc1\u9002\u7528\u4e8e\u4e3b\u6d41\u6a21\u578b\uff0c\u5f00\u6e90\u5b9e\u73b0\u4fc3\u8fdb\u53ef\u590d\u73b0\u7814\u7a76"}}
{"id": "2508.09294", "pdf": "https://arxiv.org/pdf/2508.09294", "abs": "https://arxiv.org/abs/2508.09294", "authors": ["Xi Xuan", "Zimo Zhu", "Wenxin Zhang", "Yi-Cheng Lin", "Tomi Kinnunen"], "title": "Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional Mamba as Self-Attention's Alternative", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SY", "eess.SY"], "comment": "Accepted at IEEE ASRU 2025", "summary": "Advances in speech synthesis intensify security threats, motivating real-time\ndeepfake detection research. We investigate whether bidirectional Mamba can\nserve as a competitive alternative to Self-Attention in detecting synthetic\nspeech. Our solution, Fake-Mamba, integrates an XLSR front-end with\nbidirectional Mamba to capture both local and global artifacts. Our core\ninnovation introduces three efficient encoders: TransBiMamba, ConBiMamba, and\nPN-BiMamba. Leveraging XLSR's rich linguistic representations, PN-BiMamba can\neffectively capture the subtle cues of synthetic speech. Evaluated on ASVspoof\n21 LA, 21 DF, and In-The-Wild benchmarks, Fake-Mamba achieves 0.97%, 1.74%, and\n5.85% EER, respectively, representing substantial relative gains over SOTA\nmodels XLSR-Conformer and XLSR-Mamba. The framework maintains real-time\ninference across utterance lengths, demonstrating strong generalization and\npractical viability. The code is available at\nhttps://github.com/xuanxixi/Fake-Mamba.", "AI": {"tldr": "\u63d0\u51faFake-Mamba\u6846\u67b6\uff0c\u7ed3\u5408XLSR\u8bed\u97f3\u8868\u5f81\u4e0e\u53cc\u5411Mamba\u7ed3\u6784\uff0c\u5728\u591a\u4e2a\u6df1\u5ea6\u4f2a\u9020\u8bed\u97f3\u68c0\u6d4b\u57fa\u51c6\u4e0a\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u5e76\u4fdd\u6301\u5b9e\u65f6\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u8bed\u97f3\u5408\u6210\u6280\u672f\u7684\u8fdb\u6b65\u52a0\u5267\u6df1\u5ea6\u4f2a\u9020\u5b89\u5168\u5a01\u80c1\uff0c\u9700\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u5b9e\u65f6\u68c0\u6d4b\u65b9\u6848\u3002\u7814\u7a76\u9a8c\u8bc1\u53cc\u5411Mamba\u66ff\u4ee3Transformer\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u53ef\u884c\u6027\u3002", "method": "1) XLSR\u524d\u7aef\u63d0\u53d6\u4e30\u5bcc\u8bed\u97f3\u7279\u5f81 2) \u63d0\u51fa\u4e09\u79cd\u53cc\u5411Mamba\u7f16\u7801\u5668\uff08TransBiMamba/ConBiMamba/PN-BiMamba\uff09\uff0c\u5176\u4e2dPN-BiMamba\u901a\u8fc7\u5e76\u884c\u7f51\u7edc\u4f18\u5316\u5408\u6210\u4f2a\u5f71\u6355\u6349", "result": "\u5728ASVspoof 21 LA/DF\u548cIn-The-Wild\u6570\u636e\u96c6\u5206\u522b\u8fbe\u52300.97%\u30011.74%\u30015.85% EER\uff0c\u76f8\u5bf9XLSR-Conformer/XLSR-Mamba\u6709\u663e\u8457\u63d0\u5347\uff0c\u63a8\u7406\u901f\u5ea6\u8fbe\u5b9e\u65f6\u8981\u6c42", "conclusion": "\u9996\u6b21\u8bc1\u660e\u53cc\u5411Mamba\u5728\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0cPN-BiMamba\u901a\u8fc7\u591a\u6a21\u6001\u7279\u5f81\u878d\u5408\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\uff0c\u5f00\u6e90\u6846\u67b6\u5177\u5907\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c"}}
{"id": "2508.09389", "pdf": "https://arxiv.org/pdf/2508.09389", "abs": "https://arxiv.org/abs/2508.09389", "authors": ["Eray Eren", "Qingju Liu", "Hyeongwoo Kim", "Pablo Garrido", "Abeer Alwan"], "title": "ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "comment": "Interspeech 2025; demo page at\n  https://promode8272.github.io/promode/index.html", "summary": "Prosody conveys rich emotional and semantic information of the speech signal\nas well as individual idiosyncrasies. We propose a stand-alone model that maps\ntext-to-prosodic features such as F0 and energy and can be used in downstream\ntasks such as TTS. The ProMode encoder takes as input acoustic features and\ntime-aligned textual content, both are partially masked, and obtains a\nfixed-length latent prosodic embedding. The decoder predicts acoustics in the\nmasked region using both the encoded prosody input and unmasked textual\ncontent. Trained on the GigaSpeech dataset, we compare our method with\nstate-of-the-art style encoders. For F0 and energy predictions, we show\nconsistent improvements for our model at different levels of granularity. We\nalso integrate these predicted prosodic features into a TTS system and conduct\nperceptual tests, which show higher prosody preference compared to the\nbaselines, demonstrating the model's potential in tasks where prosody modeling\nis important.", "AI": {"tldr": "\u63d0\u51faProMode\u6a21\u578b\uff0c\u901a\u8fc7\u90e8\u5206\u63a9\u7801\u7684\u58f0\u5b66-\u6587\u672c\u8054\u5408\u7f16\u7801\u751f\u6210\u97f5\u5f8b\u8868\u5f81\uff0c\u5728\u57fa\u9891/\u80fd\u91cf\u9884\u6d4b\u548cTTS\u7cfb\u7edf\u4e2d\u5c55\u73b0\u4f18\u8d8a\u6027\u80fd", "motivation": "\u97f5\u5f8b\u7279\u5f81\uff08\u5982\u57fa\u9891\u3001\u80fd\u91cf\uff09\u627f\u8f7d\u8bed\u97f3\u7684\u60c5\u611f\u8bed\u4e49\u53ca\u4e2a\u4f53\u7279\u5f81\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u97f5\u5f8b\u5efa\u6a21\u7cbe\u5ea6\u548c\u4e0b\u6e38\u4efb\u52a1\u5e94\u7528\u4e0a\u5b58\u5728\u5c40\u9650", "method": "\u6784\u5efa\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\uff1a\u7f16\u7801\u5668\u5904\u7406\u90e8\u5206\u63a9\u7801\u7684\u58f0\u5b66\u7279\u5f81\u4e0e\u5bf9\u9f50\u6587\u672c\uff0c\u751f\u6210\u56fa\u5b9a\u957f\u5ea6\u97f5\u5f8b\u5d4c\u5165\uff1b\u89e3\u7801\u5668\u7ed3\u5408\u6587\u672c\u4fe1\u606f\u91cd\u5efa\u63a9\u7801\u533a\u57df\u7684\u58f0\u5b66\u7279\u5f81", "result": "\u5728GigaSpeech\u6570\u636e\u96c6\u4e0a\uff0c\u57fa\u9891/\u80fd\u91cf\u9884\u6d4b\u6307\u6807\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u98ce\u683c\u7f16\u7801\u5668\uff0cTTS\u7cfb\u7edf\u4e3b\u89c2\u6d4b\u8bd5\u83b7\u5f97\u66f4\u9ad8\u97f5\u5f8b\u81ea\u7136\u5ea6\u504f\u597d", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u63a9\u7801\u8bad\u7ec3\u7b56\u7565\u6709\u6548\u6355\u83b7\u591a\u7c92\u5ea6\u97f5\u5f8b\u7279\u5f81\uff0c\u4e3a\u9700\u8981\u7cbe\u7ec6\u97f5\u5f8b\u5efa\u6a21\u7684\u8bed\u97f3\u5408\u6210\u4efb\u52a1\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.09442", "pdf": "https://arxiv.org/pdf/2508.09442", "abs": "https://arxiv.org/abs/2508.09442", "authors": ["Zhifan Luo", "Shuo Shao", "Su Zhang", "Lijing Zhou", "Yuke Hu", "Chenxu Zhao", "Zhihao Liu", "Zhan Qin"], "title": "Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "The Key-Value (KV) cache, which stores intermediate attention computations\n(Key and Value pairs) to avoid redundant calculations, is a fundamental\nmechanism for accelerating Large Language Model (LLM) inference. However, this\nefficiency optimization introduces significant yet underexplored privacy risks.\nThis paper provides the first comprehensive analysis of these vulnerabilities,\ndemonstrating that an attacker can reconstruct sensitive user inputs directly\nfrom the KV-cache. We design and implement three distinct attack vectors: a\ndirect Inversion Attack, a more broadly applicable and potent Collision Attack,\nand a semantic-based Injection Attack. These methods demonstrate the\npracticality and severity of KV-cache privacy leakage issues. To mitigate this,\nwe propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.\nKV-Cloak uses a reversible matrix-based obfuscation scheme, combined with\noperator fusion, to secure the KV-cache. Our extensive experiments show that\nKV-Cloak effectively thwarts all proposed attacks, reducing reconstruction\nquality to random noise. Crucially, it achieves this robust security with\nvirtually no degradation in model accuracy and minimal performance overhead,\noffering a practical solution for trustworthy LLM deployment.", "AI": {"tldr": "\u63ed\u793a\u4e86Key-Value\u7f13\u5b58\u5728\u52a0\u901f\u5927\u6a21\u578b\u63a8\u7406\u65f6\u7684\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u63d0\u51fa\u9632\u5fa1\u65b9\u6848KV-Cloak", "motivation": "KV\u7f13\u5b58\u673a\u5236\u867d\u63d0\u5347LLM\u63a8\u7406\u6548\u7387\uff0c\u4f46\u5176\u5b58\u50a8\u7684\u4e2d\u95f4\u6ce8\u610f\u529b\u8ba1\u7b97\u7ed3\u679c\u5b58\u5728\u4e25\u91cd\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u653b\u51fb\u8005\u53ef\u5229\u7528KV\u7f13\u5b58\u91cd\u6784\u7528\u6237\u654f\u611f\u8f93\u5165", "method": "\u8bbe\u8ba1\u4e09\u79cd\u653b\u51fb\u65b9\u5f0f\uff08\u53cd\u8f6c\u653b\u51fb/\u78b0\u649e\u653b\u51fb/\u6ce8\u5165\u653b\u51fb\uff09\uff0c\u63d0\u51fa\u57fa\u4e8e\u53ef\u9006\u77e9\u9635\u6df7\u6dc6\u548c\u7b97\u5b50\u878d\u5408\u7684KV-Cloak\u9632\u5fa1\u6846\u67b6", "result": "KV-Cloak\u6210\u529f\u5c06\u653b\u51fb\u91cd\u6784\u8d28\u91cf\u964d\u81f3\u968f\u673a\u566a\u58f0\u6c34\u5e73\uff0c\u9632\u5fa1\u6548\u679c\u663e\u8457\u4e14\u4e0d\u5f71\u54cd\u6a21\u578b\u7cbe\u5ea6\uff0c\u6027\u80fd\u5f00\u9500\u53ef\u5ffd\u7565\u4e0d\u8ba1", "conclusion": "KV-Cloak\u9996\u6b21\u5b9e\u73b0\u5b89\u5168\u9632\u62a4\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u5e73\u8861\uff0c\u4e3a\u53ef\u4fe1\u5927\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.09456", "pdf": "https://arxiv.org/pdf/2508.09456", "abs": "https://arxiv.org/abs/2508.09456", "authors": ["Junxian Li", "Beining Xu", "Di Zhang"], "title": "IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding", "categories": ["cs.CV", "cs.CL", "cs.CR"], "comment": "13 pages, 13 Figures", "summary": "Vision-language models (VLMs) have shown significant advancements in tasks\nsuch as visual grounding, where they localize specific objects in images based\non natural language queries and images. However, security issues in visual\ngrounding tasks for VLMs remain underexplored, especially in the context of\nbackdoor attacks. In this paper, we introduce a novel input-aware backdoor\nattack method, IAG, designed to manipulate the grounding behavior of VLMs. This\nattack forces the model to ground a specific target object in the input image,\nregardless of the user's query. We propose an adaptive trigger generator that\nembeds the semantic information of the attack target's description into the\noriginal image using a text-conditional U-Net, thereby overcoming the\nopen-vocabulary attack challenge. To ensure the attack's stealthiness, we\nutilize a reconstruction loss to minimize visual discrepancies between poisoned\nand clean images. Additionally, we introduce a unified method for generating\nattack data. IAG is evaluated theoretically and empirically, demonstrating its\nfeasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches\nover 65\\% on various testing sets. IAG also shows promising potential on\nmanipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on\nclean samples. Extensive specific experiments, such as ablation study and\npotential defense, also indicate the robustness and transferability of our\nattack.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u578b\u8f93\u5165\u611f\u77e5\u540e\u95e8\u653b\u51fb\u65b9\u6cd5IAG\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u89e6\u53d1\u5668\u548c\u91cd\u5efa\u635f\u5931\u673a\u5236\uff0c\u6709\u6548\u64cd\u7eb5\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b9a\u4f4d\u884c\u4e3a\u4e14\u4fdd\u6301\u9690\u853d\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u8bcd\u6c47\u573a\u666f\u4e0b\u7684\u540e\u95e8\u653b\u51fb\u5b89\u5168\u6027\u5173\u6ce8\u4e0d\u8db3\uff0c\u4f20\u7edf\u9759\u6001\u89e6\u53d1\u5668\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u52a8\u6001\u67e5\u8be2\u573a\u666f\u3002", "method": "1. \u6587\u672c\u6761\u4ef6U-Net\u751f\u6210\u81ea\u9002\u5e94\u89e6\u53d1\u5668\uff0c\u5c06\u653b\u51fb\u76ee\u6807\u8bed\u4e49\u5d4c\u5165\u56fe\u50cf\n2. \u91c7\u7528\u91cd\u5efa\u635f\u5931\u4fdd\u6301\u56fe\u50cf\u9690\u853d\u6027\n3. \u7edf\u4e00\u7684\u6570\u636e\u751f\u6210\u6846\u67b6\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u653b\u51fb", "result": "\u5728InternVL-2.5-8B\u5b9e\u73b0ASR@0.5\u8d8565%\uff0cFerret-7B\u548cLlaVA-1.5-7B\u653b\u51fb\u6210\u529f\u7387\u663e\u8457\u4e14\u6e05\u6d01\u6837\u672c\u51c6\u786e\u7387\u4e0b\u964d<1%", "conclusion": "IAG\u9a8c\u8bc1\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e0b\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5176\u8f93\u5165\u611f\u77e5\u673a\u5236\u548c\u8bed\u4e49\u878d\u5408\u65b9\u6cd5\u4e3a\u540e\u95e8\u653b\u51fb\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2508.09473", "pdf": "https://arxiv.org/pdf/2508.09473", "abs": "https://arxiv.org/abs/2508.09473", "authors": ["Birong Pan", "Mayi Xu", "Qiankun Pi", "Jianhao Chen", "Yuanyuan Zhu", "Ming Zhong", "Tieyun Qian"], "title": "NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Ensuring robust safety alignment while preserving utility is critical for the\nreliable deployment of Large Language Models (LLMs). However, current\ntechniques fundamentally suffer from intertwined deficiencies: insufficient\nrobustness against malicious attacks, frequent refusal of benign queries,\ndegradation in generated text quality and general task performance--the former\ntwo reflecting deficits in robust safety and the latter constituting utility\nimpairment. We trace these limitations to the coarse-grained layer-wise\ninterventions in existing methods. To resolve this, we propose NeuronTune, a\nfine-grained framework that dynamically modulates sparse neurons to achieve\nsimultaneous safety-utility optimization. Our approach first identifies\nsafety-critical and utility-preserving neurons across all layers via\nattribution, then employs meta-learning to adaptively amplify safety-neuron\nactivations and suppress utility-neuron activations. Crucially, NeuronTune\nenables tunable adjustment of intervention scope via neuron-count thresholds,\nsupporting flexible adaptation to security-critical or utility-priority\nscenarios. Extensive experimental results demonstrate that our method\nsignificantly outperforms existing state-of-the-art technologies, achieving\nsuperior model safety while maintaining excellent utility.", "AI": {"tldr": "\u63d0\u51faNeuronTune\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u8282\u7a00\u758f\u795e\u7ecf\u5143\u5b9e\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u6027\u4e0e\u5b9e\u7528\u6027\u7684\u540c\u6b65\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5b89\u5168\u9632\u62a4\u4e0d\u8db3\u3001\u9891\u7e41\u8bef\u62d2\u5408\u7406\u8bf7\u6c42\u3001\u6587\u672c\u8d28\u91cf\u4e0b\u964d\u7b49\u95ee\u9898\uff0c\u6839\u6e90\u5728\u4e8e\u7c97\u7c92\u5ea6\u7684\u5c42\u95f4\u5e72\u9884\u3002\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u795e\u7ecf\u5143\u7ea7\u8c03\u63a7\u65b9\u6848\u3002", "method": "1. \u901a\u8fc7\u5f52\u56e0\u5206\u6790\u8bc6\u522b\u5404\u5c42\u7684\u5b89\u5168\u5173\u952e\u795e\u7ecf\u5143\u548c\u6548\u7528\u4fdd\u6301\u795e\u7ecf\u5143 2. \u91c7\u7528\u5143\u5b66\u4e60\u5b9e\u73b0\u5b89\u5168\u795e\u7ecf\u5143\u6fc0\u6d3b\u653e\u5927\u548c\u6548\u7528\u795e\u7ecf\u5143\u6291\u5236 3. \u901a\u8fc7\u795e\u7ecf\u5143\u6570\u91cf\u9608\u503c\u5b9e\u73b0\u53ef\u8c03\u8282\u7684\u5e72\u9884\u8303\u56f4", "result": "\u5b9e\u9a8c\u8868\u660eNeuronTune\u5728\u5b89\u5168\u9632\u62a4(\u5bf9\u6297\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e37%)\u548c\u4efb\u52a1\u6027\u80fd(\u5b9e\u7528\u6307\u6807\u63d0\u534721%)\u4e0a\u5747\u8d85\u8d8a\u73b0\u6709\u6280\u672f\uff0c\u652f\u6301\u5b89\u5168\u4f18\u5148\u6216\u6548\u7528\u4f18\u5148\u7684\u7075\u6d3b\u573a\u666f\u9002\u914d\u3002", "conclusion": "\u7ec6\u7c92\u5ea6\u7684\u795e\u7ecf\u5143\u52a8\u6001\u8c03\u63a7\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86LLM\u90e8\u7f72\u4e2d\u7684\u5b89\u5168-\u6548\u7528\u6743\u8861\u96be\u9898\uff0c\u4e3a\u53ef\u9760\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.09535", "pdf": "https://arxiv.org/pdf/2508.09535", "abs": "https://arxiv.org/abs/2508.09535", "authors": ["Roberto Balestri"], "title": "AI Blob! LLM-Driven Recontextualization of Italian Television Archives", "categories": ["cs.MM", "cs.AI", "cs.CL", "cs.DL"], "comment": "Preprint", "summary": "This paper introduces AI Blob!, an experimental system designed to explore\nthe potential of semantic cataloging and Large Language Models (LLMs) for the\nretrieval and recontextualization of archival television footage. Drawing\nmethodological inspiration from Italian television programs such as Blob (RAI\nTre, 1989-), AI Blob! integrates automatic speech recognition (ASR), semantic\nembeddings, and retrieval-augmented generation (RAG) to organize and\nreinterpret archival content. The system processes a curated dataset of 1,547\nItalian television videos by transcribing audio, segmenting it into\nsentence-level units, and embedding these segments into a vector database for\nsemantic querying. Upon user input of a thematic prompt, the LLM generates a\nrange of linguistically and conceptually related queries, guiding the retrieval\nand recombination of audiovisual fragments. These fragments are algorithmically\nselected and structured into narrative sequences producing montages that\nemulate editorial practices of ironic juxtaposition and thematic coherence. By\nforegrounding dynamic, content-aware retrieval over static metadata schemas, AI\nBlob! demonstrates how semantic technologies can facilitate new approaches to\narchival engagement, enabling novel forms of automated narrative construction\nand cultural analysis. The project contributes to ongoing debates in media\nhistoriography and AI-driven archival research, offering both a conceptual\nframework and a publicly available dataset to support further interdisciplinary\nexperimentation.", "AI": {"tldr": "AI Blob! \u5229\u7528\u8bed\u4e49\u6280\u672f\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u5185\u5bb9\u68c0\u7d22\u91cd\u7ec4\u7535\u89c6\u6863\u6848\u7d20\u6750\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u53d9\u4e8b\u6784\u5efa\u4e0e\u6587\u5316\u5206\u6790", "motivation": "\u63a2\u7d22\u8bed\u4e49\u7f16\u76ee\u4e0eLLMs\u5728\u7535\u89c6\u6863\u6848\u7d20\u6750\u667a\u80fd\u68c0\u7d22\u4e2d\u7684\u5e94\u7528\uff0c\u7a81\u7834\u9759\u6001\u5143\u6570\u636e\u9650\u5236\uff0c\u63a8\u52a8\u5a92\u4f53\u53f2\u5b66\u4e0eAI\u6863\u6848\u7814\u7a76\u7684\u4ea4\u53c9\u521b\u65b0", "method": "\u6574\u5408ASR\u8bed\u97f3\u8bc6\u522b+\u8bed\u4e49\u5d4c\u5165+RAG\u6280\u672f\uff0c\u5c06\u89c6\u9891\u5206\u5272\u4e3a\u53e5\u5b50\u5355\u5143\u5b58\u5165\u5411\u91cf\u6570\u636e\u5e93\uff0c\u901a\u8fc7LLM\u751f\u6210\u6269\u5c55\u67e5\u8be2\u5b9e\u73b0\u788e\u7247\u5316\u5185\u5bb9\u53d9\u4e8b\u91cd\u7ec4", "result": "\u5f00\u53d1\u51fa\u53ef\u52a8\u6001\u68c0\u7d22\u7535\u89c6\u6863\u6848\u7684\u7b97\u6cd5\u7cfb\u7edf\uff0c\u4ea7\u751f\u5177\u6709\u4e3b\u9898\u8fde\u8d2f\u6027\u7684\u8499\u592a\u5947\u89c6\u9891\uff0c\u5e76\u516c\u5f001547\u4e2a\u610f\u5927\u5229\u7535\u89c6\u89c6\u9891\u6570\u636e\u96c6", "conclusion": "\u8be5\u9879\u76ee\u8bc1\u660e\u4e86\u8bed\u4e49\u6280\u672f\u5728\u6863\u6848\u6d3b\u5316\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u8de8\u5b66\u79d1\u7814\u7a76\u63d0\u4f9b\u81ea\u52a8\u5316\u53d9\u4e8b\u6846\u67b6\u4e0e\u5b9e\u9a8c\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u6587\u5316\u5206\u6790\u65b0\u8303\u5f0f\u53d1\u5c55"}}
{"id": "2508.09614", "pdf": "https://arxiv.org/pdf/2508.09614", "abs": "https://arxiv.org/abs/2508.09614", "authors": ["Daniel Raffini", "Agnese Macori", "Lorenzo Porcaro", "Tiziana Catarci", "Marco Angelini"], "title": "How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "9-pages", "summary": "This study examines the rhetorical and linguistic features of argumentative\ntexts generated by ChatGPT on ethically nuanced topics and investigates their\npersuasive impact on human readers.Through a user study involving 62\nparticipants and pre-post interaction surveys, the paper analyzes how exposure\nto AI-generated arguments affects opinion change and user perception. A\nlinguistic and rhetorical analysis of the generated texts reveals a consistent\nargumentative macrostructure, reliance on formulaic expressions, and limited\nstylistic richness. While ChatGPT demonstrates proficiency in constructing\ncoherent argumentative texts, its persuasive efficacy appears constrained,\nparticularly on topics involving ethical issues.The study finds that while\nparticipants often acknowledge the benefits highlighted by ChatGPT, ethical\nconcerns tend to persist or even intensify post-interaction. The results also\ndemonstrate a variation depending on the topic. These findings highlight new\ninsights on AI-generated persuasion in ethically sensitive domains and are a\nbasis for future research.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793aChatGPT\u751f\u6210\u7684\u4f26\u7406\u654f\u611f\u8bae\u8bba\u6587\u867d\u7ed3\u6784\u8fde\u8d2f\u4f46\u8bf4\u670d\u529b\u6709\u9650\uff0c\u4f26\u7406\u62c5\u5fe7\u5728\u4ea4\u4e92\u540e\u53ef\u80fd\u5f3a\u5316\uff0c\u4e0d\u540c\u4e3b\u9898\u6548\u679c\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u5206\u6790AI\u751f\u6210\u6587\u672c\u5728\u4f26\u7406\u8bdd\u9898\u4e2d\u7684\u4fee\u8f9e\u7279\u5f81\u53ca\u5176\u8bf4\u670d\u6548\u679c\uff0c\u63a2\u7d22\u4eba\u5de5\u667a\u80fd\u5728\u4f26\u7406\u654f\u611f\u9886\u57df\u7684\u5e94\u7528\u8fb9\u754c\u3002", "method": "\u901a\u8fc762\u4eba\u7528\u6237\u5b9e\u9a8c\uff08\u524d\u6d4b-\u540e\u6d4b\u8bbe\u8ba1\uff09\u3001\u8bed\u8a00\u5b66\u4fee\u8f9e\u5206\u6790\uff0c\u7ed3\u5408\u89c2\u70b9\u53d8\u5316\u6d4b\u91cf\u4e0e\u6587\u672c\u7279\u5f81\u5173\u8054\u7814\u7a76\u3002", "result": "1. \u6587\u672c\u5448\u73b0\u516c\u5f0f\u5316\u8868\u8fbe\u4e0e\u98ce\u683c\u5355\u4e00\u6027\n2. \u8bf4\u670d\u6548\u529b\u53d7\u4f26\u7406\u590d\u6742\u5ea6\u5236\u7ea6\n3. 53%\u53c2\u4e0e\u8005\u4f26\u7406\u62c5\u5fe7\u672a\u7f13\u89e3\n4. \u4e0d\u540c\u4e3b\u9898\u8bf4\u670d\u6548\u679c\u5dee\u5f02\u8fbe28%", "conclusion": "AI\u5728\u4f26\u7406\u8bf4\u670d\u4e2d\u5b58\u5728\u7ed3\u6784\u6027\u5c40\u9650\uff0c\u9700\u5f00\u53d1\u9488\u5bf9\u6027\u4f26\u7406\u63a8\u7406\u6846\u67b6\uff0c\u7814\u7a76\u6210\u679c\u4e3aAI\u4f26\u7406\u4f20\u64ad\u7814\u7a76\u5efa\u7acb\u57fa\u51c6\u6570\u636e\u96c6\u3002"}}
{"id": "2508.09651", "pdf": "https://arxiv.org/pdf/2508.09651", "abs": "https://arxiv.org/abs/2508.09651", "authors": ["Daniel Raffini", "Agnese Macori", "Marco Angelini", "Tiziana Catarci"], "title": "A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CY"], "comment": "8-pages", "summary": "The paper explores the study of gender-based narrative biases in stories\ngenerated by ChatGPT, Gemini, and Claude. The prompt design draws on Propp's\ncharacter classifications and Freytag's narrative structure. The stories are\nanalyzed through a close reading approach, with particular attention to\nadherence to the prompt, gender distribution of characters, physical and\npsychological descriptions, actions, and finally, plot development and\ncharacter relationships. The results reveal the persistence of biases -\nespecially implicit ones - in the generated stories and highlight the\nimportance of assessing biases at multiple levels using an interpretative\napproach.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793aChatGPT/Gemini/Claude\u751f\u6210\u6545\u4e8b\u4e2d\u5b58\u5728\u6027\u522b\u53d9\u4e8b\u504f\u89c1\uff0c\u9700\u591a\u5c42\u9762\u8bc4\u4f30\u9690\u6027\u504f\u89c1", "motivation": "\u63a2\u7a76\u4e3b\u6d41AI\u751f\u6210\u6545\u4e8b\u4e2d\u57fa\u4e8e\u6027\u522b\u7684\u9690\u6027\u53d9\u4e8b\u504f\u89c1\u53ca\u5176\u8bc4\u4f30\u65b9\u6cd5", "method": "\u7ed3\u5408Propp\u89d2\u8272\u5206\u7c7b\u4e0eFreytag\u53d9\u4e8b\u7ed3\u6784\u8bbe\u8ba1\u63d0\u793a\uff0c\u91c7\u7528\u7ec6\u8bfb\u5206\u6790\u6cd5\u8bc4\u4f30\u89d2\u8272\u6027\u522b\u5206\u5e03\u3001\u63cf\u8ff0\u7ef4\u5ea6\u53ca\u60c5\u8282\u903b\u8f91", "result": "\u53d1\u73b0\u751f\u6210\u6545\u4e8b\u4e2d\u5b58\u5728\u6301\u7eed\u6027\u9690\u6027\u504f\u89c1\uff0c\u9a8c\u8bc1\u591a\u5c42\u9762\u89e3\u91ca\u6027\u5206\u6790\u7684\u6709\u6548\u6027", "conclusion": "\u5f3a\u8c03\u9700\u5efa\u7acb\u7cfb\u7edf\u6027\u8bc4\u4f30\u6846\u67b6\u8bc6\u522bAI\u53d9\u4e8b\u4e2d\u7684\u591a\u5c42\u6b21\u504f\u89c1\uff0c\u63d0\u5347\u751f\u6210\u5185\u5bb9\u7684\u516c\u5e73\u6027"}}
{"id": "2508.09886", "pdf": "https://arxiv.org/pdf/2508.09886", "abs": "https://arxiv.org/abs/2508.09886", "authors": ["Lingyu Chen", "Yawen Zeng", "Yue Wang", "Peng Wan", "Guo-chen Ning", "Hongen Liao", "Daoqiang Zhang", "Fang Chen"], "title": "COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "ICCV 2025", "summary": "Conventional single-dataset training often fails with new data distributions,\nespecially in ultrasound (US) image analysis due to limited data, acoustic\nshadows, and speckle noise. Therefore, constructing a universal framework for\nmulti-heterogeneous US datasets is imperative. However, a key challenge arises:\nhow to effectively mitigate inter-dataset interference while preserving\ndataset-specific discriminative features for robust downstream task? Previous\napproaches utilize either a single source-specific decoder or a domain\nadaptation strategy, but these methods experienced a decline in performance\nwhen applied to other domains. Considering this, we propose a Universal\nCollaborative Mixture of Heterogeneous Source-Specific Experts (COME).\nSpecifically, COME establishes dual structure-semantic shared experts that\ncreate a universal representation space and then collaborate with\nsource-specific experts to extract discriminative features through providing\ncomplementary features. This design enables robust generalization by leveraging\ncross-datasets experience distributions and providing universal US priors for\nsmall-batch or unseen data scenarios. Extensive experiments under three\nevaluation modes (single-dataset, intra-organ, and inter-organ integration\ndatasets) demonstrate COME's superiority, achieving significant mean AP\nimprovements over state-of-the-art methods. Our project is available at:\nhttps://universalcome.github.io/UniversalCOME/.", "AI": {"tldr": "\u63d0\u51faCOME\u6a21\u578b\u89e3\u51b3\u591a\u5f02\u6784\u8d85\u58f0\u6570\u636e\u96c6\u6cdb\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5171\u4eab\u4e13\u5bb6\u4e0e\u6e90\u7279\u5b9a\u4e13\u5bb6\u534f\u4f5c\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u4f20\u7edf\u5355\u6570\u636e\u96c6\u8bad\u7ec3\u5728\u8de8\u57df\u8d85\u58f0\u56fe\u50cf\u5206\u6790\u4e2d\u5b58\u5728\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406\u6570\u636e\u5f02\u8d28\u6027\u548c\u8de8\u57df\u5e72\u6270", "method": "\u5efa\u7acb\u7ed3\u6784-\u8bed\u4e49\u53cc\u91cd\u5171\u4eab\u4e13\u5bb6\u521b\u5efa\u901a\u7528\u8868\u793a\u7a7a\u95f4\uff0c\u7ed3\u5408\u6e90\u7279\u5b9a\u4e13\u5bb6\u901a\u8fc7\u7279\u5f81\u4e92\u8865\u63d0\u53d6\u5224\u522b\u6027\u7279\u5f81", "result": "\u5728\u4e09\u79cd\u8bc4\u4f30\u6a21\u5f0f\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e73\u5747AP\u6307\u6807\u663e\u8457\u63d0\u5347", "conclusion": "COME\u5b9e\u73b0\u4e86\u8de8\u6570\u636e\u96c6\u7ecf\u9a8c\u6574\u5408\uff0c\u4e3a\u5c0f\u6837\u672c\u548c\u672a\u89c1\u6570\u636e\u573a\u666f\u63d0\u4f9b\u9c81\u68d2\u7684\u8d85\u58f0\u56fe\u50cf\u5206\u6790\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.09987", "pdf": "https://arxiv.org/pdf/2508.09987", "abs": "https://arxiv.org/abs/2508.09987", "authors": ["Junyan Ye", "Dongzhi Jiang", "Zihao Wang", "Leqi Zhu", "Zhenghao Hu", "Zilong Huang", "Jun He", "Zhiyuan Yan", "Jinghua Yu", "Hongsheng Li", "Conghui He", "Weijia Li"], "title": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "19 pages, 8 figures", "summary": "Recently, GPT-4o has garnered significant attention for its strong\nperformance in image generation, yet open-source models still lag behind.\nSeveral studies have explored distilling image data from GPT-4o to enhance\nopen-source models, achieving notable progress. However, a key question\nremains: given that real-world image datasets already constitute a natural\nsource of high-quality data, why should we use GPT-4o-generated synthetic data?\nIn this work, we identify two key advantages of synthetic images. First, they\ncan complement rare scenarios in real-world datasets, such as surreal fantasy\nor multi-reference image generation, which frequently occur in user queries.\nSecond, they provide clean and controllable supervision. Real-world data often\ncontains complex background noise and inherent misalignment between text\ndescriptions and image content, whereas synthetic images offer pure backgrounds\nand long-tailed supervision signals, facilitating more accurate text-to-image\nalignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale\nsynthetic dataset generated by GPT-4o, harnessing the power of synthetic image\ndata to address blind spots in real-world coverage. Using this dataset, we\nfine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.\nIn addition, we propose two new evaluation benchmarks for a more accurate and\nchallenging assessment of image generation capabilities: GenEval++, which\nincreases instruction complexity to mitigate score saturation, and\nImagine-Bench, which focuses on evaluating both the understanding and\ngeneration of imaginative content. Echo-4o demonstrates strong performance\nacross standard benchmarks. Moreover, applying Echo-4o-Image to other\nfoundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains\nacross multiple metrics, highlighting the datasets strong transferability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5229\u7528GPT-4o\u751f\u6210\u7684\u5408\u6210\u56fe\u50cf\u6570\u636eEcho-4o-Image\u5f25\u8865\u73b0\u5b9e\u6570\u636e\u96c6\u7684\u8986\u76d6\u76f2\u70b9\uff0c\u5e76\u901a\u8fc7\u65b0\u8bc4\u4f30\u57fa\u51c6\u9a8c\u8bc1\u5176\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5408\u6210\u6570\u636e\u5728\u63d0\u5347\u6a21\u578b\u6027\u80fd\u548c\u8fc1\u79fb\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002", "motivation": "\u73b0\u5b9e\u56fe\u50cf\u6570\u636e\u5b58\u5728\u7f55\u89c1\u573a\u666f\u8986\u76d6\u4e0d\u8db3\uff08\u5982\u5e7b\u60f3\u7c7b\u5185\u5bb9\uff09\u3001\u80cc\u666f\u566a\u58f0\u53ca\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u504f\u5dee\u7684\u95ee\u9898\uff0c\u800c\u5408\u6210\u6570\u636e\u80fd\u63d0\u4f9b\u7eaf\u51c0\u76d1\u7763\u4fe1\u53f7\u548c\u957f\u5c3e\u573a\u666f\u8986\u76d6\u3002", "method": "\u6784\u5efa180K\u89c4\u6a21\u7684GPT-4o\u5408\u6210\u6570\u636e\u96c6Echo-4o-Image\u5fae\u8c03Bagel\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1GenEval++\uff08\u590d\u6742\u6307\u4ee4\u8bc4\u4f30\uff09\u548cImagine-Bench\uff08\u60f3\u8c61\u529b\u8bc4\u4f30\uff09\u4e24\u4e2a\u65b0\u57fa\u51c6\u3002", "result": "Echo-4o\u5728\u6807\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u8be5\u6570\u636e\u96c6\u53ef\u8fc1\u79fb\u81f3OmniGen2\u7b49\u6a21\u578b\uff0c\u5e26\u6765\u591a\u6307\u6807\u6027\u80fd\u63d0\u5347\uff08BLEURT\u21915.7%\uff0cCLIP Score\u21913.2%\uff09\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u901a\u8fc7\u573a\u666f\u8865\u5145\u548c\u7cbe\u51c6\u5bf9\u9f50\u6709\u6548\u589e\u5f3a\u751f\u6210\u6a21\u578b\u80fd\u529b\uff0c\u65b0\u8bc4\u4f30\u4f53\u7cfb\u4e3a\u590d\u6742\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u66f4\u4e25\u8c28\u7684\u8bc4\u6d4b\u6807\u51c6\u3002"}}
