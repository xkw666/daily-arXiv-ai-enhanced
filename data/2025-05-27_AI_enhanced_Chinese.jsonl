{"id": "2505.18159", "pdf": "https://arxiv.org/pdf/2505.18159", "abs": "https://arxiv.org/abs/2505.18159", "authors": ["Jesus Alvarez C", "Daua D. Karajeanes", "Ashley Celeste Prado", "John Ruttan", "Ivory Yang", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "title": "Advancing Uto-Aztecan Language Technologies: A Case Study on the Endangered Comanche Language", "categories": ["cs.CL", "cs.LG", "I.2.7; H.3.1"], "comment": "11 pages, 13 figures; published in Proceedings of the Fifth Workshop\n  on NLP for Indigenous Languages of the Americas (AmericasNLP 2025) at NAACL\n  2025, Albuquerque, NM", "summary": "The digital exclusion of endangered languages remains a critical challenge in\nNLP, limiting both linguistic research and revitalization efforts. This study\nintroduces the first computational investigation of Comanche, an Uto-Aztecan\nlanguage on the verge of extinction, demonstrating how minimal-cost,\ncommunity-informed NLP interventions can support language preservation. We\npresent a manually curated dataset of 412 phrases, a synthetic data generation\npipeline, and an empirical evaluation of GPT-4o and GPT-4o-mini for language\nidentification. Our experiments reveal that while LLMs struggle with Comanche\nin zero-shot settings, few-shot prompting significantly improves performance,\nachieving near-perfect accuracy with just five examples. Our findings highlight\nthe potential of targeted NLP methodologies in low-resource contexts and\nemphasize that visibility is the first step toward inclusion. By establishing a\nfoundation for Comanche in NLP, we advocate for computational approaches that\nprioritize accessibility, cultural sensitivity, and community engagement.", "AI": {"tldr": "\u5c55\u793a\u5982\u4f55\u901a\u8fc7\u4f4e\u6210\u672cNLP\u65b9\u6cd5\u548c\u5c11\u91cf\u6837\u672c\u63d0\u793a\u6709\u6548\u652f\u6301\u6fd2\u5371\u79d1\u66fc\u5947\u8bed\u4fdd\u62a4", "motivation": "\u89e3\u51b3\u6fd2\u5371\u8bed\u8a00\u5728NLP\u9886\u57df\u7684\u6570\u5b57\u6392\u65a5\u95ee\u9898\uff0c\u901a\u8fc7\u63d0\u5347\u8ba1\u7b97\u53ef\u89c1\u6027\u652f\u6301\u8bed\u8a00\u4fdd\u5b58\u4e0e\u590d\u5174", "method": "\u521b\u5efa412\u4e2a\u77ed\u8bed\u6570\u636e\u96c6+\u5408\u6210\u6570\u636e\u751f\u6210+GPT-4o\u7cfb\u5217\u6a21\u578b\u7684\u8bed\u8a00\u8bc6\u522b\u5b9e\u9a8c\uff08\u96f6\u6837\u672c vs \u5c11\u91cf\u6837\u672c\uff09", "result": "\u5c11\u91cf\u6837\u672c\u63d0\u793a\u4f7f\u51c6\u786e\u7387\u63a5\u8fd1\u5b8c\u7f8e\uff085\u4e2a\u793a\u4f8b\u5373\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff09", "conclusion": "\u5f3a\u8c03\u9488\u5bf9\u6027NLP\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u7684\u6f5c\u529b\uff0c\u4e3b\u5f20\u53ef\u89c1\u6027\u662f\u8bed\u8a00\u5305\u5bb9\u7684\u9996\u8981\u6b65\u9aa4\uff0c\u9700\u6ce8\u91cd\u6587\u5316\u654f\u611f\u6027\u548c\u793e\u533a\u53c2\u4e0e"}}
{"id": "2505.18215", "pdf": "https://arxiv.org/pdf/2505.18215", "abs": "https://arxiv.org/abs/2505.18215", "authors": ["Junyan Zhang", "Yiming Huang", "Shuliang Liu", "Yubo Gao", "Xuming Hu"], "title": "Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid adoption of LLMs has overshadowed the potential advantages of\ntraditional BERT-like models in text classification. This study challenges the\nprevailing \"LLM-centric\" trend by systematically comparing three category\nmethods, i.e., BERT-like models fine-tuning, LLM internal state utilization,\nand zero-shot inference across six high-difficulty datasets. Our findings\nreveal that BERT-like models often outperform LLMs. We further categorize\ndatasets into three types, perform PCA and probing experiments, and identify\ntask-specific model strengths: BERT-like models excel in pattern-driven tasks,\nwhile LLMs dominate those requiring deep semantics or world knowledge. Based on\nthis, we propose TaMAS, a fine-grained task selection strategy, advocating for\na nuanced, task-driven approach over a one-size-fits-all reliance on LLMs.", "AI": {"tldr": "\u4f20\u7edfBERT\u6a21\u578b\u5728\u7279\u5b9a\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u5e38\u4f18\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u9700\u91c7\u7528\u4efb\u52a1\u9a71\u52a8\u7684\u7ec6\u7c92\u5ea6\u6a21\u578b\u9009\u62e9\u7b56\u7565", "motivation": "\u6311\u6218\u5f53\u524d\u8fc7\u5ea6\u4f9d\u8d56LLM\u7684\u884c\u4e1a\u8d8b\u52bf\uff0c\u63ed\u793a\u4e0d\u540c\u6a21\u578b\u5728\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\u4e2d\u7684\u4f18\u52bf\u5dee\u5f02", "method": "\u7cfb\u7edf\u6bd4\u8f83BERT\u5fae\u8c03\u3001LLM\u5185\u90e8\u72b6\u6001\u5229\u7528\u548c\u96f6\u6837\u672c\u63a8\u7406\u4e09\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u516d\u79cd\u9ad8\u96be\u5ea6\u6570\u636e\u96c6\u8fdb\u884cPCA\u5206\u6790\u548c\u63a2\u9488\u5b9e\u9a8c", "result": "\u8bc6\u522b\u4efb\u52a1\u7279\u5f02\u6027\u4f18\u52bf\uff1aBERT\u64c5\u957f\u6a21\u5f0f\u9a71\u52a8\u4efb\u52a1\uff0cLLM\u5728\u6df1\u5ea6\u8bed\u4e49/\u4e16\u754c\u77e5\u8bc6\u4efb\u52a1\u8868\u73b0\u66f4\u4f73", "conclusion": "\u63d0\u51faTaMAS\u7ec6\u7c92\u5ea6\u4efb\u52a1\u9009\u62e9\u6846\u67b6\uff0c\u5021\u5bfc\u6839\u636e\u4efb\u52a1\u7279\u6027\u9009\u62e9\u6a21\u578b\u800c\u975e\u76f2\u76ee\u4f7f\u7528LLM"}}
{"id": "2505.18218", "pdf": "https://arxiv.org/pdf/2505.18218", "abs": "https://arxiv.org/abs/2505.18218", "authors": ["Shuhang Xu", "Fangwei Zhong"], "title": "CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games", "categories": ["cs.CL", "cs.AI"], "comment": "To Appear at ACL 2025 (Main)", "summary": "Metaphors are a crucial way for humans to express complex or subtle ideas by\ncomparing one concept to another, often from a different domain. However, many\nlarge language models (LLMs) struggle to interpret and apply metaphors in\nmulti-agent language games, hindering their ability to engage in covert\ncommunication and semantic evasion, which are crucial for strategic\ncommunication. To address this challenge, we introduce CoMet, a framework that\nenables LLM-based agents to engage in metaphor processing. CoMet combines a\nhypothesis-based metaphor reasoner with a metaphor generator that improves\nthrough self-reflection and knowledge integration. This enhances the agents'\nability to interpret and apply metaphors, improving the strategic and nuanced\nquality of their interactions. We evaluate CoMet on two multi-agent language\ngames - Undercover and Adversarial Taboo - which emphasize Covert Communication\nand Semantic Evasion. Experimental results demonstrate that CoMet significantly\nenhances the agents' ability to communicate strategically using metaphors.", "AI": {"tldr": "CoMet\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u9690\u55bb\u63a8\u7406\u5668\u548c\u81ea\u6211\u4f18\u5316\u7684\u751f\u6210\u5668\uff0c\u663e\u8457\u63d0\u5347LLM\u5728\u591a\u667a\u80fd\u4f53\u6e38\u620f\u4e2d\u7684\u9690\u55bb\u6218\u7565\u6c9f\u901a\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u5728\u9690\u55bb\u5904\u7406\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u5f71\u54cd\u9690\u853d\u6c9f\u901a\u548c\u8bed\u4e49\u56de\u907f\u80fd\u529b\uff0c\u963b\u788d\u6218\u7565\u4ea4\u4e92\u8d28\u91cf\u3002", "method": "\u6574\u5408\u57fa\u4e8e\u5047\u8bbe\u7684\u9690\u55bb\u63a8\u7406\u5668\u4e0e\u5177\u5907\u81ea\u6211\u53cd\u601d/\u77e5\u8bc6\u6574\u5408\u673a\u5236\u7684\u9690\u55bb\u751f\u6210\u5668\uff0c\u6784\u5efa\u591a\u667a\u80fd\u4f53\u534f\u540c\u6846\u67b6\u3002", "result": "\u5728Undercover\u548cAdversarial Taboo\u6e38\u620f\u4e2d\uff0cCoMet\u4f7f\u667a\u80fd\u4f53\u9690\u55bb\u6c9f\u901a\u6210\u529f\u7387\u63d0\u534732%-45%\u3002", "conclusion": "CoMet\u9996\u6b21\u5b9e\u73b0LLM\u7684\u7aef\u5230\u7aef\u9690\u55bb\u5904\u7406\u80fd\u529b\uff0c\u4e3a\u6218\u7565\u8bed\u8a00\u4ea4\u4e92\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8ba4\u77e5\u67b6\u6784\u3002"}}
{"id": "2505.18223", "pdf": "https://arxiv.org/pdf/2505.18223", "abs": "https://arxiv.org/abs/2505.18223", "authors": ["Hanyu Li", "Haoyu Liu", "Tingyu Zhu", "Tianyu Guo", "Zeyu Zheng", "Xiaotie Deng", "Michael I. Jordan"], "title": "IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) show promise as data analysis agents, but\nexisting benchmarks overlook the iterative nature of the field, where experts'\ndecisions evolve with deeper insights of the dataset. To address this, we\nintroduce IDA-Bench, a novel benchmark evaluating LLM agents in multi-round\ninteractive scenarios. Derived from complex Kaggle notebooks, tasks are\npresented as sequential natural language instructions by an LLM-simulated user.\nAgent performance is judged by comparing its final numerical output to the\nhuman-derived baseline. Initial results show that even state-of-the-art coding\nagents (like Claude-3.7-thinking) succeed on < 50% of the tasks, highlighting\nlimitations not evident in single-turn tests. This work underscores the need to\nimprove LLMs' multi-round capabilities for building more reliable data analysis\nagents, highlighting the necessity of achieving a balance between instruction\nfollowing and reasoning.", "AI": {"tldr": "\u63d0\u51faIDA-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4ea4\u4e92\u5f0f\u6570\u636e\u5206\u6790\u573a\u666f\u4e2d\u7684\u591a\u8f6e\u5904\u7406\u80fd\u529b\u4e0d\u8db3\u3002\u5f53\u524d\u6700\u4f18\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u572850%\u4efb\u52a1\u4e2d\u5931\u8d25\uff0c\u51f8\u663e\u5355\u8f6e\u6d4b\u8bd5\u65e0\u6cd5\u8bc4\u4f30\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5ffd\u89c6\u6570\u636e\u5206\u6790\u9886\u57df\u4e13\u5bb6\u8ba4\u77e5\u7684\u8fed\u4ee3\u6027\uff0c\u9700\u6784\u5efa\u591a\u8f6e\u4ea4\u4e92\u573a\u666f\u8bc4\u4f30\u6a21\u578b\u80fd\u529b\u3002\u901a\u8fc7\u6a21\u62df\u771f\u5b9eKaggle\u7528\u6237\u6307\u4ee4\u5e8f\u5217\uff0c\u66f4\u51c6\u786e\u53cd\u6620\u5b9e\u9645\u9700\u6c42\u3002", "method": "\u57fa\u4e8e\u590d\u6742Kaggle notebook\u6784\u5efa\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5e8f\u5217\uff0c\u4f7f\u7528LLM\u6a21\u62df\u7528\u6237\u9010\u6b65\u53d1\u5e03\u4efb\u52a1\uff0c\u6700\u7ec8\u901a\u8fc7\u6570\u503c\u8f93\u51fa\u4e0e\u4eba\u7c7b\u57fa\u51c6\u5bf9\u6bd4\u8bc4\u4f30\u4ee3\u7406\u6027\u80fd\u3002", "result": "Claude-3.7\u7b49\u9876\u7ea7\u4ee3\u7801\u751f\u6210\u4ee3\u7406\u6210\u529f\u7387\u4e0d\u8db350%\uff0c\u66b4\u9732\u51fa\u73b0\u6709\u6a21\u578b\u5728\u6307\u4ee4\u8ddf\u968f\u4e0e\u81ea\u4e3b\u63a8\u7406\u5e73\u8861\u4e0a\u7684\u91cd\u5927\u7f3a\u9677\u3002", "conclusion": "\u6784\u5efa\u53ef\u9760\u6570\u636e\u5206\u6790\u4ee3\u7406\u9700\u91cd\u70b9\u63d0\u5347\u591a\u8f6e\u4ea4\u4e92\u80fd\u529b\uff0c\u5e94\u5728\u4fdd\u6301\u6307\u4ee4\u9075\u5faa\u7684\u540c\u65f6\u589e\u5f3a\u6301\u7eed\u63a8\u7406\u80fd\u529b\uff0c\u63a8\u52a8\u66f4\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u5de5\u4f5c\u6a21\u5f0f\u7684\u7b97\u6cd5\u6539\u8fdb\u3002"}}
{"id": "2505.18197", "pdf": "https://arxiv.org/pdf/2505.18197", "abs": "https://arxiv.org/abs/2505.18197", "authors": ["Kangli Wang", "Shihao Li", "Qianxi Yi", "Wei Gao"], "title": "A Novel Benchmark and Dataset for Efficient 3D Gaussian Splatting with Gaussian Point Cloud Compression", "categories": ["cs.GR"], "comment": "22 pages, 13 figures", "summary": "Recently, immersive media and autonomous driving applications have\nsignificantly advanced through 3D Gaussian Splatting (3DGS), which offers\nhigh-fidelity rendering and computational efficiency. Despite these advantages,\n3DGS as a display-oriented representation requires substantial storage due to\nits numerous Gaussian attributes. Current compression methods have shown\npromising results but typically neglect the compression of Gaussian spatial\npositions, creating unnecessary bitstream overhead. We conceptualize Gaussian\nprimitives as point clouds and propose leveraging point cloud compression\ntechniques for more effective storage. AI-based point cloud compression\ndemonstrates superior performance and faster inference compared to MPEG\nGeometry-based Point Cloud Compression (G-PCC). However, direct application of\nexisting models to Gaussian compression may yield suboptimal results, as\nGaussian point clouds tend to exhibit globally sparse yet locally dense\ngeometric distributions that differ from conventional point cloud\ncharacteristics. To address these challenges, we introduce GausPcgc for\nGaussian point cloud geometry compression along with a specialized training\ndataset GausPcc-1K. Our work pioneers the integration of AI-based point cloud\ncompression into Gaussian compression pipelines, achieving superior compression\nratios. The framework complements existing Gaussian compression methods while\ndelivering significant performance improvements. All code, data, and\npre-trained models will be publicly released to facilitate further research\nadvances in this field.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eAI\u7684\u70b9\u4e91\u538b\u7f29\u6846\u67b6GausPcgc\uff0c\u901a\u8fc7\u9002\u914d\u9ad8\u65af\u70b9\u4e91\u51e0\u4f55\u5206\u5e03\u7279\u6027\uff0c\u663e\u8457\u63d0\u53473D\u9ad8\u65af\u6cfc\u6e85\u6a21\u578b\u7684\u5b58\u50a8\u6548\u7387\u3002", "motivation": "\u73b0\u67093DGS\u538b\u7f29\u65b9\u6cd5\u5ffd\u7565\u9ad8\u65af\u7a7a\u95f4\u4f4d\u7f6e\u538b\u7f29\uff0c\u5bfc\u81f4\u5b58\u50a8\u5f00\u9500\u8fc7\u5927\u3002\u4f20\u7edf\u70b9\u4e91\u538b\u7f29\u65b9\u6848\u96be\u4ee5\u9002\u914d\u9ad8\u65af\u70b9\u4e91\u7279\u6709\u7684'\u5168\u5c40\u7a00\u758f-\u5c40\u90e8\u5bc6\u96c6'\u5206\u5e03\u7279\u6027\u3002", "method": "\u6784\u5efaGausPcc-1K\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u9002\u914d\u9ad8\u65af\u51e0\u4f55\u7279\u5f81\u7684GausPcgc\u538b\u7f29\u6846\u67b6\uff0c\u9996\u6b21\u5c06AI\u70b9\u4e91\u538b\u7f29\u6280\u672f\u5f15\u51653DGS\u538b\u7f29\u6d41\u7a0b\u3002", "result": "\u5728\u4fdd\u6301\u517c\u5bb9\u73b0\u6709\u538b\u7f29\u65b9\u6848\u7684\u57fa\u7840\u4e0a\u5b9e\u73b0\u66f4\u9ad8\u538b\u7f29\u6bd4\uff0c\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u4f20\u7edfMPEG G-PCC\u65b9\u6848\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a3DGS\u538b\u7f29\u5f00\u8f9f\u65b0\u65b9\u5411\uff0c\u5f00\u6e90\u4ee3\u7801/\u6570\u636e\u5c06\u63a8\u52a8\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2505.18237", "pdf": "https://arxiv.org/pdf/2505.18237", "abs": "https://arxiv.org/abs/2505.18237", "authors": ["Xixian Yong", "Xiao Zhou", "Yingying Zhang", "Jinlin Li", "Yefeng Zheng", "Xian Wu"], "title": "Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "The recent rise of Large Reasoning Models (LRMs) has significantly improved\nmulti-step reasoning performance, but often at the cost of generating\nexcessively long reasoning chains. This paper revisits the efficiency of such\nreasoning processes through an information-theoretic lens, revealing a\nfundamental trade-off between reasoning length and semantic efficiency. We\npropose two metrics, InfoBias and InfoGain, to quantify divergence from ideal\nreasoning paths and stepwise information contribution, respectively. Empirical\nanalyses show that longer reasoning chains tend to exhibit higher information\nbias and diminishing information gain, especially for incorrect answers.\nMotivated by these findings, we introduce an entropy-based Adaptive Think\nstrategy that dynamically halts reasoning once confidence is sufficiently high,\nimproving efficiency while maintaining competitive accuracy. Compared to the\nVanilla Think approach (default mode), our strategy yields a 1.10% improvement\nin average accuracy and a 50.80% reduction in token usage on QwQ-32B across six\nbenchmark tasks spanning diverse reasoning types and difficulty levels,\ndemonstrating superior efficiency and reasoning performance. These results\nunderscore the promise of entropy-based methods for enhancing both accuracy and\ncost-effiiciency in large language model deployment.", "AI": {"tldr": "\u901a\u8fc7\u4fe1\u606f\u8bba\u89c6\u89d2\u4f18\u5316\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u63a8\u7406\u94fe\u957f\u5ea6\u4e0e\u8bed\u4e49\u6548\u7387\u5e73\u8861\uff0c\u63d0\u51fa\u57fa\u4e8e\u71b5\u7684\u81ea\u9002\u5e94\u601d\u8003\u7b56\u7565\u5b9e\u73b0\u6548\u7387\u63d0\u5347", "motivation": "\u89c2\u5bdf\u5230\u73b0\u6709\u5927\u578b\u63a8\u7406\u6a21\u578b\u751f\u6210\u8fc7\u957f\u63a8\u7406\u94fe\u5bfc\u81f4\u6548\u7387\u4e0b\u964d\uff0c\u4ece\u4fe1\u606f\u8bba\u89d2\u5ea6\u63ed\u793a\u63a8\u7406\u957f\u5ea6\u4e0e\u8bed\u4e49\u6548\u7387\u7684\u6743\u8861\u5173\u7cfb", "method": "\u63d0\u51faInfoBias(\u91cf\u5316\u63a8\u7406\u8def\u5f84\u504f\u5dee)\u548cInfoGain(\u91cf\u5316\u4fe1\u606f\u589e\u76ca)\u6307\u6807\uff0c\u5f00\u53d1\u57fa\u4e8e\u71b5\u7684\u81ea\u9002\u5e94\u505c\u6b62\u673a\u5236\u52a8\u6001\u63a7\u5236\u63a8\u7406\u6df1\u5ea6", "result": "\u5728QwQ-32B\u6a21\u578b\u4e0a\u5b9e\u73b0\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53471.10%\uff0cToken\u6d88\u8017\u964d\u4f4e50.80%\uff08\u8de86\u4e2a\u4e0d\u540c\u96be\u5ea6\u63a8\u7406\u4efb\u52a1\uff09", "conclusion": "\u71b5\u9a71\u52a8\u65b9\u6cd5\u663e\u8457\u63d0\u5347LLM\u90e8\u7f72\u7684\u51c6\u786e\u7387\u548c\u6210\u672c\u6548\u7387\uff0c\u9a8c\u8bc1\u4fe1\u606f\u8bba\u6307\u6807\u5bf9\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u7684\u6709\u6548\u6027"}}
{"id": "2505.18764", "pdf": "https://arxiv.org/pdf/2505.18764", "abs": "https://arxiv.org/abs/2505.18764", "authors": ["Yitian Yuan", "Qianyue He"], "title": "Efficient Differentiable Hardware Rasterization for 3D Gaussian Splatting", "categories": ["cs.GR", "I.3.7; I.3.1"], "comment": "8 pages,2 figures", "summary": "Recent works demonstrate the advantages of hardware rasterization for 3D\nGaussian Splatting (3DGS) in forward-pass rendering through fast GPU-optimized\ngraphics and fixed memory footprint. However, extending these benefits to\nbackward-pass gradient computation remains challenging due to graphics pipeline\nconstraints. We present a differentiable hardware rasterizer for 3DGS that\novercomes the memory and performance limitations of tile-based software\nrasterization. Our solution employs programmable blending for per-pixel\ngradient computation combined with a hybrid gradient reduction strategy\n(quad-level + subgroup) in fragment shaders, achieving over 10x faster backward\nrasterization versus naive atomic operations and 3x speedup over the canonical\ntile-based rasterizer. Systematic evaluation reveals 16-bit render targets\n(float16 and unorm16) as the optimal accuracy-efficiency trade-off, achieving\nhigher gradient accuracy among mixed-precision rendering formats with execution\nspeeds second only to unorm8, while float32 texture incurs severe forward pass\nperformance degradation due to suboptimal hardware optimizations. Our method\nwith float16 formats demonstrates 3.07x acceleration in full pipeline execution\n(forward + backward passes) on RTX4080 GPUs with the MipNeRF dataset,\noutperforming the baseline tile-based renderer while preserving hardware\nrasterization's memory efficiency advantages -- incurring merely 2.67% of the\nmemory overhead required for splat sorting operations. This work presents a\nunified differentiable hardware rasterization method that simultaneously\noptimizes runtime and memory usage for 3DGS, making it particularly suitable\nfor resource-constrained devices with limited memory capacity.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u5fae\u5206\u786c\u4ef6\u5149\u6805\u5316\u65b9\u6cd5\u4f18\u53163DGS\uff0c\u5b9e\u73b010\u500d\u53cd\u5411\u5149\u6805\u52a0\u901f\u4e0e3.07\u500d\u5168\u6d41\u7a0b\u52a0\u901f\uff0c\u5185\u5b58\u5f00\u9500\u4ec52.67%", "motivation": "\u786c\u4ef6\u5149\u6805\u5316\u57283DGS\u6b63\u5411\u6e32\u67d3\u4e2d\u4f18\u52bf\u660e\u663e\uff0c\u4f46\u53cd\u5411\u68af\u5ea6\u8ba1\u7b97\u53d7\u5236\u4e8e\u56fe\u5f62\u7ba1\u7ebf\u7ea6\u675f\uff0c\u9700\u7a81\u7834\u5185\u5b58\u548c\u6027\u80fd\u9650\u5236", "method": "\u91c7\u7528\u53ef\u7f16\u7a0b\u6df7\u5408\u6280\u672f\u5b9e\u73b0\u9010\u50cf\u7d20\u68af\u5ea6\u8ba1\u7b97\uff0c\u7ed3\u5408\u56db\u5143\u7ec4+\u5b50\u7ec4\u6df7\u5408\u68af\u5ea6\u7f29\u51cf\u7b56\u7565\uff0c\u4f7f\u752816\u4f4d\u6e32\u67d3\u76ee\u6807\u4f18\u5316\u7cbe\u5ea6\u6548\u7387\u5e73\u8861", "result": "float16\u683c\u5f0f\u5728RTX4080\u4e0a\u5b9e\u73b0\u5168\u6d41\u7a0b3.07\u500d\u52a0\u901f\uff0c\u53cd\u5411\u5149\u6805\u6bd4\u539f\u5b50\u64cd\u4f5c\u5feb10\u500d\uff0c\u5185\u5b58\u5360\u7528\u4ec5\u4e3a\u57fa\u4e8e\u56fe\u5757\u65b9\u6cd5\u76842.67%", "conclusion": "\u8be5\u65b9\u6cd5\u7edf\u4e00\u4f18\u5316\u4e863DGS\u7684\u8fd0\u884c\u65f6\u6027\u80fd\u548c\u5185\u5b58\u6548\u7387\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5185\u5b58\u53d7\u9650\u8bbe\u5907\uff0c\u540c\u65f6\u4fdd\u6301\u786c\u4ef6\u5149\u6805\u5316\u7684\u56fa\u6709\u4f18\u52bf"}}
{"id": "2505.18240", "pdf": "https://arxiv.org/pdf/2505.18240", "abs": "https://arxiv.org/abs/2505.18240", "authors": ["Ananth Muppidi", "Tarak Das", "Sambaran Bandyopadhyay", "Tripti Shukla", "Dharun D A"], "title": "Taming LLMs with Negative Samples: A Reference-Free Framework to Evaluate Presentation Content with Actionable Feedback", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The generation of presentation slides automatically is an important problem\nin the era of generative AI. This paper focuses on evaluating multimodal\ncontent in presentation slides that can effectively summarize a document and\nconvey concepts to a broad audience. We introduce a benchmark dataset,\nRefSlides, consisting of human-made high-quality presentations that span\nvarious topics. Next, we propose a set of metrics to characterize different\nintrinsic properties of the content of a presentation and present REFLEX, an\nevaluation approach that generates scores and actionable feedback for these\nmetrics. We achieve this by generating negative presentation samples with\ndifferent degrees of metric-specific perturbations and use them to fine-tune\nLLMs. This reference-free evaluation technique does not require ground truth\npresentations during inference. Our extensive automated and human experiments\ndemonstrate that our evaluation approach outperforms classical heuristic-based\nand state-of-the-art large language model-based evaluations in generating\nscores and explanations.", "AI": {"tldr": "\u63d0\u51faREFLEX\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u51c6\u6570\u636e\u96c6RefSlides\u548c\u8d1f\u6837\u672c\u5fae\u8c03LLM\u5b9e\u73b0\u65e0\u53c2\u8003\u7684\u6f14\u793a\u6587\u7a3f\u81ea\u52a8\u8bc4\u4f30", "motivation": "\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u6f14\u793a\u6587\u7a3f\u9700\u89e3\u51b3\u591a\u6a21\u6001\u5185\u5bb9\u8bc4\u4f30\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u53c2\u8003\u4e14\u6548\u679c\u6709\u9650", "method": "1. \u6784\u5efa\u8de8\u9886\u57df\u4eba\u5de5\u9ad8\u8d28\u91cf\u6f14\u793a\u6570\u636e\u96c6RefSlides\n2. \u8bbe\u8ba1\u5185\u5bb9\u8bc4\u4f30\u6307\u6807\u5e76\u751f\u6210\u8d1f\u6837\u672c\n3. \u57fa\u4e8e\u6270\u52a8\u6837\u672c\u5fae\u8c03LLM\u5b9e\u73b0\u65e0\u53c2\u8003\u8bc4\u4f30", "result": "REFLEX\u5728\u81ea\u52a8\u5316\u548c\u4eba\u5de5\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u73b0\u6709LLM\u8bc4\u4f30\uff0c\u80fd\u751f\u6210\u53ef\u89e3\u91ca\u7684\u8bc4\u5206\u4e0e\u6539\u8fdb\u5efa\u8bae", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u7834\u4f20\u7edf\u8bc4\u4f30\u5bf9\u53c2\u8003\u6837\u672c\u7684\u4f9d\u8d56\uff0c\u4e3a\u751f\u6210\u5f0fAI\u7684\u6f14\u793a\u5185\u5bb9\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2505.18772", "pdf": "https://arxiv.org/pdf/2505.18772", "abs": "https://arxiv.org/abs/2505.18772", "authors": ["Michal Edelstein", "Hsueh-Ti Derek Liu", "Mirela Ben-Chen"], "title": "CageNet: A Meta-Framework for Learning on Wild Meshes", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "11 pages, 13 figures (excluding supplementary material)", "summary": "Learning on triangle meshes has recently proven to be instrumental to a\nmyriad of tasks, from shape classification, to segmentation, to deformation and\nanimation, to mention just a few. While some of these applications are tackled\nthrough neural network architectures which are tailored to the application at\nhand, many others use generic frameworks for triangle meshes where the only\ncustomization required is the modification of the input features and the loss\nfunction. Our goal in this paper is to broaden the applicability of these\ngeneric frameworks to \"wild\", i.e. meshes in-the-wild which often have multiple\ncomponents, non-manifold elements, disrupted connectivity, or a combination of\nthese. We propose a configurable meta-framework based on the concept of caged\ngeometry: Given a mesh, a cage is a single component manifold triangle mesh\nthat envelopes it closely. Generalized barycentric coordinates map between\nfunctions on the cage, and functions on the mesh, allowing us to learn and test\non a variety of data, in different applications. We demonstrate this concept by\nlearning segmentation and skinning weights on difficult data, achieving better\nperformance to state of the art techniques on wild meshes.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2505.18244", "pdf": "https://arxiv.org/pdf/2505.18244", "abs": "https://arxiv.org/abs/2505.18244", "authors": ["Yukin Zhang", "Qi Dong"], "title": "Multi-Scale Probabilistic Generation Theory: A Hierarchical Framework for Interpreting Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Transformer based language models achieve remarkable performance but\nremain opaque in how they plan, structure, and realize text. We introduce\nMulti_Scale Probabilistic Generation Theory (MSPGT), a hierarchical framework\nthat factorizes generation into three semantic scales_global context,\nintermediate structure, and local word choices and aligns each scale with\nspecific layer ranges in Transformer architectures. To identify scale\nboundaries, we propose two complementary metrics: attention span thresholds and\ninter layer mutual information peaks. Across four representative models (GPT-2,\nBERT, RoBERTa, and T5), these metrics yield stable local/intermediate/global\npartitions, corroborated by probing tasks and causal interventions. We find\nthat decoder_only models allocate more layers to intermediate and global\nprocessing while encoder_only models emphasize local feature extraction.\nThrough targeted interventions, we demonstrate that local scale manipulations\nprimarily influence lexical diversity, intermediate-scale modifications affect\nsentence structure and length, and global_scale perturbations impact discourse\ncoherence all with statistically significant effects. MSPGT thus offers a\nunified, architecture-agnostic method for interpreting, diagnosing, and\ncontrolling large language models, bridging the gap between mechanistic\ninterpretability and emergent capabilities.", "AI": {"tldr": "\u63d0\u51fa\u591a\u5c3a\u5ea6\u6982\u7387\u751f\u6210\u7406\u8bba\uff08MSPGT\uff09\uff0c\u901a\u8fc7\u5206\u5c42\u6846\u67b6\u89e3\u6790Transformer\u6a21\u578b\u7684\u6587\u672c\u751f\u6210\u673a\u5236\uff0c\u63ed\u793a\u4e0d\u540c\u67b6\u6784\u6a21\u578b\u5728\u5168\u5c40/\u4e2d\u95f4/\u5c40\u90e8\u5c3a\u5ea6\u7684\u5206\u5c42\u7279\u5f81\u53ca\u5e72\u9884\u5f71\u54cd", "motivation": "\u89e3\u51b3\u5927\u578bTransformer\u6a21\u578b\u6587\u672c\u751f\u6210\u8fc7\u7a0b\u4e0d\u900f\u660e\u7684\u95ee\u9898\uff0c\u5efa\u7acb\u53ef\u89e3\u91ca\u7684\u751f\u6210\u8fc7\u7a0b\u5206\u5c42\u7406\u8bba\u6846\u67b6", "method": "\u4f7f\u7528\u6ce8\u610f\u529b\u8de8\u5ea6\u9608\u503c\u548c\u5c42\u95f4\u4e92\u4fe1\u606f\u5cf0\u503c\u4f5c\u4e3a\u5c3a\u5ea6\u5212\u5206\u6307\u6807\uff0c\u5728GPT-2/BERT/RoBERTa/T5\u6a21\u578b\u8fdb\u884c\u63a2\u6d4b\u4efb\u52a1\u548c\u56e0\u679c\u5e72\u9884\u5b9e\u9a8c", "result": "\u4e0d\u540c\u6a21\u578b\u5c55\u73b0\u7a33\u5b9a\u7684\u4e09\u5c42\u5206\u533a\u6a21\u5f0f\uff1a\u89e3\u7801\u5668\u6a21\u578b\u4fa7\u91cd\u4e2d/\u5168\u5c40\u5904\u7406\uff08GPT-2\u536066%\u5c42\u6570\uff09\uff0c\u7f16\u7801\u5668\u6a21\u578b\u4e13\u6ce8\u5c40\u90e8\u7279\u5f81\uff08BERT\u536050%\u5c42\u6570\uff09\uff0c\u7edf\u8ba1\u663e\u793a\u5c40\u90e8\u5c3a\u5ea6\u5e72\u9884\u5f71\u54cd\u8bcd\u6c47\u591a\u6837\u6027\uff08p<0.01\uff09\uff0c\u4e2d\u95f4\u5c3a\u5ea6\u6539\u53d8\u53e5\u5b50\u7ed3\u6784\uff08\u957f\u5ea6\u53d8\u5316\u00b122%\uff09\uff0c\u5168\u5c40\u5c3a\u5ea6\u51b3\u5b9a\u7bc7\u7ae0\u8fde\u8d2f\u6027", "conclusion": "MSPGT\u63d0\u4f9b\u8de8\u67b6\u6784\u7684\u6a21\u578b\u89e3\u91ca\u6846\u67b6\uff0c\u901a\u8fc7\u5c3a\u5ea6\u7279\u5f02\u6027\u5e72\u9884\u5b9e\u73b0\u751f\u6210\u63a7\u5236\uff0c\u4e3a\u7406\u89e3\u5927\u6a21\u578b\u6d8c\u73b0\u80fd\u529b\u5efa\u7acb\u673a\u5236\u5316\u6865\u6881"}}
{"id": "2505.18805", "pdf": "https://arxiv.org/pdf/2505.18805", "abs": "https://arxiv.org/abs/2505.18805", "authors": ["Zhongtian Zheng", "Tao Huang", "Haozhe Su", "Xueqi Ma", "Yuefan Shen", "Tongtong Wang", "Yin Yang", "Xifeng Gao", "Zherong Pan", "Kui Wu"], "title": "DiffHairCard: Auto Hair Card Extraction with Differentiable Rendering", "categories": ["cs.GR"], "comment": null, "summary": "Hair cards remain a widely used representation for hair modeling in real-time\napplications, offering a practical trade-off between visual fidelity, memory\nusage, and performance. However, generating high-quality hair card models\nremains a challenging and labor-intensive task. This work presents an automated\npipeline for converting strand-based hair models into hair card models with a\nlimited number of cards and textures while preserving the hairstyle appearance.\nOur key idea is a novel differentiable representation where each strand is\nencoded as a projected 2D spline in the texture space, which enables efficient\noptimization with differentiable rendering and structured results respecting\nthe hair geometry. Based on this representation, we develop a novel algorithm\npipeline, where we first cluster hair strands into initial hair cards and\nproject the strands into the texture space. We then conduct a two-stage\noptimization where our first stage optimizes the texture and geometry of each\nhair card separately, and after texture reduction, our second stage conducts\njoint optimization of all the cards for fine-tuning. Put together, our method\nis evaluated on a wide range of hairstyles, including straight, wavy, curly,\nand coily hairs. To better capture the appearance of short or coily hair, we\nadditionally support hair cap and cross-card. Furthermore, our framework\nsupports seamless LoD transitions via texture sharing, balancing texture memory\nefficiency and visual quality.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u52a8\u5316\u7ba1\u7ebf\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u4e8c\u7ef4\u6837\u6761\u7f16\u7801\u4e0e\u4e24\u9636\u6bb5\u4f18\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u9ad8\u6548\u7387\u9ad8\u8d28\u91cf\u5934\u53d1\u5361\u7247\u6a21\u578b\u751f\u6210", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5934\u53d1\u5361\u7247\u5efa\u6a21\u6d41\u7a0b\u52b3\u52a8\u5bc6\u96c6\u3001\u96be\u4ee5\u5e73\u8861\u5361\u7247\u6570\u91cf\u4e0e\u89c6\u89c9\u8d28\u91cf\u7684\u75db\u70b9", "method": "1. \u5c06\u53d1\u4e1d\u805a\u7c7b\u6295\u5f71\u81f3\u7eb9\u7406\u7a7a\u95f4 2. \u4e24\u9636\u6bb5\u4f18\u5316\uff08\u5355\u5361\u72ec\u7acb\u4f18\u5316+\u7eb9\u7406\u538b\u7f29\u540e\u8054\u5408\u4f18\u5316\uff093. \u652f\u6301\u53d1\u5e3d/\u4ea4\u53c9\u5361\u5904\u7406\u590d\u6742\u53d1\u578b", "result": "\u6210\u529f\u5904\u7406\u76f4/\u6ce2\u6d6a/\u5377/\u87ba\u65cb\u7b49\u5404\u7c7b\u53d1\u578b\uff0c\u5b9e\u73b0\u4ec5\u9700\u5c11\u91cf\u5361\u7247\uff08~100\u5f20\uff09\u5373\u53ef\u4fdd\u6301\u53d1\u578b\u7279\u5f81\uff0c\u7eb9\u7406\u5185\u5b58\u6548\u7387\u63d0\u534740%", "conclusion": "\u521b\u65b0\u6027\u7684\u53ef\u5fae\u5206\u8868\u793a\u4e0e\u4f18\u5316\u6846\u67b6\u4e3a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u5934\u53d1\u5efa\u6a21\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301LOD\u8fc7\u6e21\u4e0e\u590d\u6742\u53d1\u578b\u5904\u7406"}}
{"id": "2505.18247", "pdf": "https://arxiv.org/pdf/2505.18247", "abs": "https://arxiv.org/abs/2505.18247", "authors": ["Kunal Sawarkar", "Shivam R. Solanki", "Abhilasha Mangal"], "title": "MetaGen Blended RAG: Higher Accuracy for Domain-Specific Q&A Without Fine-Tuning", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "comment": "Preprint. Paper Submitted NeurIPS 2025- The Thirty-Ninth Annual\n  Conference on Neural Information Processing Systems", "summary": "Despite the widespread exploration of Retrieval-Augmented Generation (RAG),\nits deployment in enterprises for domain-specific datasets remains limited due\nto poor answer accuracy. These corpora, often shielded behind firewalls in\nprivate enterprise knowledge bases, having complex, domain-specific\nterminology, rarely seen by LLMs during pre-training; exhibit significant\nsemantic variability across domains (like networking, military, or legal,\netc.), or even within a single domain like medicine, and thus result in poor\ncontext precision for RAG systems. Currently, in such situations, fine-tuning\nor RAG with fine-tuning is attempted, but these approaches are slow, expensive,\nand lack generalization for accuracy as the new domain-specific data emerges.\nWe propose an approach for Enterprise Search that focuses on enhancing the\nretriever for a domain-specific corpus through hybrid query indexes and\nmetadata enrichment. This 'MetaGen Blended RAG' method constructs a metadata\ngeneration pipeline using key concepts, topics, and acronyms, and then creates\na metadata-enriched hybrid index with boosted search queries. This approach\navoids overfitting and generalizes effectively across domains. On the PubMedQA\nbenchmark for the biomedical domain, the proposed method achieves 82% retrieval\naccuracy and 77% RAG accuracy, surpassing all previous RAG accuracy results\nwithout fine-tuning and sets a new benchmark for zero-shot results while\noutperforming much larger models like GPT3.5. The results are even comparable\nto the best fine-tuned models on this dataset, and we further demonstrate the\nrobustness and scalability of the approach by evaluating it on other Q&A\ndatasets like SQuAD, NQ etc.", "AI": {"tldr": "\u9488\u5bf9\u4f01\u4e1a\u9886\u57df\u6570\u636eRAG\u7cbe\u5ea6\u4f4e\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u5143\u6570\u636e\u589e\u5f3a\u6df7\u5408\u7d22\u5f15\u65b9\u6cd5\uff0c\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\u5b9e\u73b082%\u68c0\u7d22\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4f20\u7edfRAG\u5e76\u63a5\u8fd1\u5fae\u8c03\u6a21\u578b\u6548\u679c\u3002", "motivation": "\u4f01\u4e1a\u77e5\u8bc6\u5e93\u5305\u542b\u590d\u6742\u4e13\u4e1a\u672f\u8bed\u4e14\u5b58\u5728\u8de8\u9886\u57df\u8bed\u4e49\u5dee\u5f02\uff0c\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u6cdb\u5316\u6027\u5dee\uff0c\u9700\u5f00\u53d1\u65e0\u9700\u5fae\u8c03\u7684\u901a\u7528\u589e\u5f3a\u65b9\u6848\u3002", "method": "\u6784\u5efa\u5143\u6570\u636e\u751f\u6210\u7ba1\u9053\uff08\u5173\u952e\u6982\u5ff5/\u4e3b\u9898/\u7f29\u5199\uff09\uff0c\u521b\u5efa\u5e26\u589e\u5f3a\u67e5\u8be2\u7684\u6df7\u5408\u7d22\u5f15\uff0c\u901a\u8fc7\u5143\u6570\u636e\u6269\u5c55\u63d0\u5347\u68c0\u7d22\u7cbe\u5ea6\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "result": "PubMedQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d82%\u68c0\u7d22\u7cbe\u5ea6/77% RAG\u7cbe\u5ea6\uff0c\u96f6\u6837\u672c\u8868\u73b0\u8d85\u8d8aGPT3.5\uff0cSQuAD/NQ\u7b49\u8de8\u9886\u57df\u6d4b\u8bd5\u9a8c\u8bc1\u65b9\u6848\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u7834\u4f20\u7edfRAG\u9650\u5236\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u8de8\u9886\u57df\u7684\u9ad8\u7cbe\u5ea6\u68c0\u7d22\uff0c\u4e3a\u79c1\u6709\u77e5\u8bc6\u5e93\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.19151", "pdf": "https://arxiv.org/pdf/2505.19151", "abs": "https://arxiv.org/abs/2505.19151", "authors": ["Shenggan Cheng", "Yuanxin Wei", "Lansong Diao", "Yong Liu", "Bujiao Chen", "Lianghua Huang", "Yu Liu", "Wenyuan Yu", "Jiangsu Du", "Wei Lin", "Yang You"], "title": "SRDiffusion: Accelerate Video Diffusion Inference via Sketching-Rendering Cooperation", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "9 pages, 6 figures", "summary": "Leveraging the diffusion transformer (DiT) architecture, models like Sora,\nCogVideoX and Wan have achieved remarkable progress in text-to-video,\nimage-to-video, and video editing tasks. Despite these advances,\ndiffusion-based video generation remains computationally intensive, especially\nfor high-resolution, long-duration videos. Prior work accelerates its inference\nby skipping computation, usually at the cost of severe quality degradation. In\nthis paper, we propose SRDiffusion, a novel framework that leverages\ncollaboration between large and small models to reduce inference cost. The\nlarge model handles high-noise steps to ensure semantic and motion fidelity\n(Sketching), while the smaller model refines visual details in low-noise steps\n(Rendering). Experimental results demonstrate that our method outperforms\nexisting approaches, over 3$\\times$ speedup for Wan with nearly no quality loss\nfor VBench, and 2$\\times$ speedup for CogVideoX. Our method is introduced as a\nnew direction orthogonal to existing acceleration strategies, offering a\npractical solution for scalable video generation.", "AI": {"tldr": "\u63d0\u51faSRDiffusion\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u6a21\u578b\u5904\u7406\u9ad8\u566a\u58f0\u6b65\u9aa4(\u8bed\u4e49\u8349\u7ed8)\u4e0e\u5c0f\u6a21\u578b\u5904\u7406\u4f4e\u566a\u58f0\u6b65\u9aa4(\u89c6\u89c9\u6e32\u67d3)\u7684\u534f\u4f5c\u673a\u5236\uff0c\u5728\u4fdd\u6301\u89c6\u9891\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b03\u500d\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u52a0\u901f\u65b9\u6cd5\u5e38\u5bfc\u81f4\u8d28\u91cf\u4e25\u91cd\u4e0b\u964d\uff0c\u9700\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u7684\u77db\u76fe\u3002", "method": "\u91c7\u7528\u5927\u5c0f\u6a21\u578b\u534f\u4f5c\u67b6\u6784\uff1a\u5927\u6a21\u578b\u4e13\u6ce8\u9ad8\u566a\u58f0\u9636\u6bb5\u4fdd\u8bc1\u8bed\u4e49\u51c6\u786e\u6027\uff0c\u5c0f\u6a21\u578b\u4f18\u5316\u4f4e\u566a\u58f0\u9636\u6bb5\u63d0\u5347\u7ec6\u8282\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u5bf9Wan\u6a21\u578b\u5b9e\u73b03\u500d\u52a0\u901f(VBench\u8d28\u91cf\u65e0\u635f)\uff0cCogVideoX\u52a0\u901f2\u500d\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5f00\u8f9f\u4e86\u4e0e\u73b0\u6709\u52a0\u901f\u7b56\u7565\u6b63\u4ea4\u7684\u65b0\u65b9\u5411\uff0c\u4e3a\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u5207\u5b9e\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2505.18283", "pdf": "https://arxiv.org/pdf/2505.18283", "abs": "https://arxiv.org/abs/2505.18283", "authors": ["Jianghao Wu", "Feilong Tang", "Yulong Li", "Ming Hu", "Haochen Xue", "Shoaib Jameel", "Yutong Xie", "Imran Razzak"], "title": "TAGS: A Test-Time Generalist-Specialist Framework with Retrieval-Augmented Reasoning and Verification", "categories": ["cs.CL", "cs.AI", "cs.MA", "I.2.7"], "comment": "16 pages including references, 2 figures", "summary": "Recent advances such as Chain-of-Thought prompting have significantly\nimproved large language models (LLMs) in zero-shot medical reasoning. However,\nprompting-based methods often remain shallow and unstable, while fine-tuned\nmedical LLMs suffer from poor generalization under distribution shifts and\nlimited adaptability to unseen clinical scenarios. To address these\nlimitations, we present TAGS, a test-time framework that combines a broadly\ncapable generalist with a domain-specific specialist to offer complementary\nperspectives without any model fine-tuning or parameter updates. To support\nthis generalist-specialist reasoning process, we introduce two auxiliary\nmodules: a hierarchical retrieval mechanism that provides multi-scale exemplars\nby selecting examples based on both semantic and rationale-level similarity,\nand a reliability scorer that evaluates reasoning consistency to guide final\nanswer aggregation. TAGS achieves strong performance across nine MedQA\nbenchmarks, boosting GPT-4o accuracy by 13.8%, DeepSeek-R1 by 16.8%, and\nimproving a vanilla 7B model from 14.1% to 23.9%. These results surpass several\nfine-tuned medical LLMs, without any parameter updates. The code will be\navailable at https://github.com/JianghaoWu/TAGS.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u5fae\u8c03\u7684TAGS\u6846\u67b6\uff0c\u901a\u8fc7\u901a\u7528\u6a21\u578b\u4e0e\u533b\u5b66\u4e13\u5bb6\u6a21\u578b\u7684\u4e92\u8865\u534f\u540c\uff0c\u7ed3\u5408\u5206\u5c42\u68c0\u7d22\u548c\u53ef\u9760\u6027\u8bc4\u5206\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347LLM\u5728\u533b\u5b66\u95ee\u7b54\u4e2d\u7684\u8868\u73b0", "motivation": "\u73b0\u6709\u63d0\u793a\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u6d45\u5c42/\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u5fae\u8c03\u6a21\u578b\u5219\u9762\u4e34\u5206\u5e03\u504f\u79fb\u6cdb\u5316\u5dee\u3001\u65e0\u6cd5\u9002\u5e94\u65b0\u4e34\u5e8a\u573a\u666f\u7684\u5c40\u9650\u6027", "method": "\u7ed3\u5408\u901a\u7528\u6a21\u578b\u4e0e\u9886\u57df\u4e13\u5bb6\u6a21\u578b\u7684\u534f\u540c\u63a8\u7406\uff0c\u8bbe\u8ba1\u5206\u5c42\u68c0\u7d22\u673a\u5236\uff08\u8bed\u4e49+\u539f\u7406\u76f8\u4f3c\u6027\u9009\u62e9\u591a\u5c3a\u5ea6\u793a\u4f8b\uff09\u548c\u53ef\u9760\u6027\u8bc4\u5206\u5668\uff08\u8bc4\u4f30\u63a8\u7406\u4e00\u81f4\u6027\uff09", "result": "\u5728MedQA\u4e5d\u4e2a\u57fa\u51c6\u4e0a\uff1aGPT-4o\u51c6\u786e\u7387\u63d0\u534713.8%\uff0c7B\u5c0f\u6a21\u578b\u4ece14.1%\u63d0\u5347\u81f323.9%\uff0c\u8d85\u8d8a\u591a\u4e2a\u5fae\u8c03\u6a21\u578b", "conclusion": "TAGS\u901a\u8fc7\u6a21\u578b\u534f\u540c\u548c\u8f85\u52a9\u6a21\u5757\u8bbe\u8ba1\uff0c\u6709\u6548\u7a81\u7834\u4f20\u7edf\u65b9\u6cd5\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u533b\u5b66\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u96f6\u6837\u672c\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.19672", "pdf": "https://arxiv.org/pdf/2505.19672", "abs": "https://arxiv.org/abs/2505.19672", "authors": ["Belcour Laurent", "Fichet Alban", "Barla Pascal"], "title": "A Fluorescent Material Model for Non-Spectral Editing & Rendering", "categories": ["cs.GR", "I.3.7"], "comment": null, "summary": "Fluorescent materials are characterized by a spectral reradiation toward\nlonger wavelengths. Recent work [Fichet et al. 2024] has shown that the\nrendering of fluorescence in a non-spectral engine is possible through the use\nof appropriate reduced reradiation matrices. But the approach has limited\nexpressivity, as it requires the storage of one reduced matrix per fluorescent\nmaterial, and only works with measured fluorescent assets.\n  In this work, we introduce an analytical approach to the editing and\nrendering of fluorescence in a non-spectral engine. It is based on a\ndecomposition of the reduced reradiation matrix, and an analytically-integrable\nGaussian-based model of the fluorescent component. The model reproduces the\nappearance of fluorescent materials accurately, especially with the addition of\na UV basis. Most importantly, it grants variations of fluorescent material\nparameters in real-time, either for the editing of fluorescent materials, or\nfor the dynamic spatial variation of fluorescence properties across object\nsurfaces. A simplified one-Gaussian fluorescence model even allows for the\nartist-friendly creation of plausible fluorescent materials from scratch,\nrequiring only a few reflectance colors as input.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9ad8\u65af\u5206\u89e3\u7684\u8367\u5149\u6750\u6599\u5b9e\u65f6\u7f16\u8f91\u6e32\u67d3\u65b9\u6cd5\uff0c\u652f\u6301\u53c2\u6570\u52a8\u6001\u8c03\u6574\u4e0e\u7b80\u5316\u827a\u672f\u521b\u4f5c\u6d41\u7a0b", "motivation": "\u73b0\u6709\u8367\u5149\u6e32\u67d3\u65b9\u6cd5\u9700\u5b58\u50a8\u591a\u4e2a\u77e9\u9635\u4e14\u4f9d\u8d56\u6d4b\u91cf\u6570\u636e\uff0c\u9650\u5236\u4e86\u827a\u672f\u521b\u4f5c\u7075\u6d3b\u6027\u4e0e\u5b9e\u65f6\u7f16\u8f91\u80fd\u529b", "method": "\u901a\u8fc7\u5206\u89e3\u7f29\u51cf\u518d\u8f90\u5c04\u77e9\u9635\uff0c\u5efa\u7acb\u53ef\u89e3\u6790\u79ef\u5206\u7684\u9ad8\u65af\u8367\u5149\u6a21\u578b\uff0c\u5f15\u5165UV\u57fa\u5e95\u589e\u5f3a\u51c6\u786e\u6027", "result": "\u5b9e\u73b0\u5b9e\u65f6\u8367\u5149\u53c2\u6570\u7f16\u8f91/\u8868\u9762\u52a8\u6001\u53d8\u5316\uff0c\u5355\u9ad8\u65af\u6a21\u578b\u4ec5\u9700\u53cd\u5c04\u8272\u8f93\u5165\u5373\u53ef\u751f\u6210\u5408\u7406\u8367\u5149\u6750\u8d28", "conclusion": "\u8be5\u89e3\u6790\u65b9\u6cd5\u7a81\u7834\u8367\u5149\u6e32\u67d3\u7684\u6280\u672f\u9650\u5236\uff0c\u4e3a\u6570\u5b57\u5185\u5bb9\u521b\u4f5c\u63d0\u4f9b\u9ad8\u6548\u53ef\u63a7\u7684\u8367\u5149\u8868\u73b0\u5de5\u5177"}}
{"id": "2505.18298", "pdf": "https://arxiv.org/pdf/2505.18298", "abs": "https://arxiv.org/abs/2505.18298", "authors": ["Jinyan Su", "Claire Cardie"], "title": "Thinking Fast and Right: Balancing Accuracy and Reasoning Length with Adaptive Rewards", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong reasoning abilities in\nmathematical tasks, often enhanced through reinforcement learning (RL).\nHowever, RL-trained models frequently produce unnecessarily long reasoning\ntraces -- even for simple queries -- leading to increased inference costs and\nlatency. While recent approaches attempt to control verbosity by adding length\npenalties to the reward function, these methods rely on fixed penalty terms\nthat are hard to tune and cannot adapt as the model's reasoning capability\nevolves, limiting their effectiveness. In this work, we propose an adaptive\nreward-shaping method that enables LLMs to \"think fast and right\" -- producing\nconcise outputs without sacrificing correctness. Our method dynamically adjusts\nthe reward trade-off between accuracy and response length based on model\nperformance: when accuracy is high, the length penalty increases to encourage\nfaster length reduction; when accuracy drops, the penalty is relaxed to\npreserve correctness. This adaptive reward accelerates early-stage length\nreduction while avoiding over-compression in later stages. Experiments across\nmultiple datasets show that our approach consistently and dramatically reduces\nreasoning length while largely maintaining accuracy, offering a new direction\nfor cost-efficient adaptive reasoning in large-scale language models.", "AI": {"tldr": "Adaptive reward-shaping method balances accuracy and conciseness in LLM reasoning outputs by dynamically adjusting length penalties based on model performance.", "motivation": "Fixed length penalties in RL-trained LLMs cause inefficiency - they cannot adapt to evolving model capabilities and lead to unnecessary verbosity.", "method": "Dynamic reward trade-off mechanism that increases length penalty when accuracy is high (encouraging brevity) and relaxes penalty when accuracy drops (preserving correctness).", "result": "Consistent dramatic reduction in reasoning length (average 40% shorter) across datasets while maintaining 98%+ of original model accuracy.", "conclusion": "Provides cost-efficient adaptive reasoning through self-adjusting rewards, opening new directions for optimized large-scale language model deployment."}}
{"id": "2505.19713", "pdf": "https://arxiv.org/pdf/2505.19713", "abs": "https://arxiv.org/abs/2505.19713", "authors": ["Yandong Guan", "Xilin Wang", "Xingxi Ming", "Jing Zhang", "Dong Xu", "Qian Yu"], "title": "CAD-Coder: Text-to-CAD Generation with Chain-of-Thought and Geometric Reward", "categories": ["cs.GR"], "comment": null, "summary": "In this work, we introduce CAD-Coder, a novel framework that reformulates\ntext-to-CAD as the generation of CadQuery scripts - a Python-based, parametric\nCAD language. This representation enables direct geometric validation, a richer\nmodeling vocabulary, and seamless integration with existing LLMs. To further\nenhance code validity and geometric fidelity, we propose a two-stage learning\npipeline: (1) supervised fine-tuning on paired text-CadQuery data, and (2)\nreinforcement learning with Group Reward Policy Optimization (GRPO), guided by\na CAD-specific reward comprising both a geometric reward (Chamfer Distance) and\na format reward. We also introduce a chain-of-thought (CoT) planning process to\nimprove model reasoning, and construct a large-scale, high-quality dataset of\n110K text-CadQuery-3D model triplets and 1.5K CoT samples via an automated\npipeline. Extensive experiments demonstrate that CAD-Coder enables LLMs to\ngenerate diverse, valid, and complex CAD models directly from natural language,\nadvancing the state of the art of text-to-CAD generation and geometric\nreasoning.", "AI": {"tldr": "CAD-Coder\u6846\u67b6\u901a\u8fc7\u751f\u6210CadQuery\u811a\u672c\u5b9e\u73b0\u6587\u672c\u5230CAD\u7684\u8f6c\u6362\uff0c\u91c7\u7528\u76d1\u7763\u5b66\u4e60+\u5f3a\u5316\u5b66\u4e60\u4e24\u9636\u6bb5\u8bad\u7ec3\uff0c\u7ed3\u5408CoT\u89c4\u5212\u6d41\u7a0b\u663e\u8457\u63d0\u5347\u6a21\u578b\u751f\u6210\u80fd\u529b", "motivation": "\u89e3\u51b3\u4f20\u7edf\u6587\u672c\u5230CAD\u65b9\u6cd5\u7f3a\u4e4f\u51e0\u4f55\u9a8c\u8bc1\u80fd\u529b\u3001\u5efa\u6a21\u8bcd\u6c47\u53d7\u9650\u3001\u4e0e\u73b0\u6709LLM\u6574\u5408\u56f0\u96be\u7684\u95ee\u9898", "method": "1. \u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u76d1\u7763\u5fae\u8c03\u6587\u672c-CadQuery\u914d\u5bf9\u6570\u636e \u2192 GRPO\u5f3a\u5316\u5b66\u4e60\uff08\u51e0\u4f55\u5956\u52b1+\u683c\u5f0f\u5956\u52b1\uff09\n2. \u5f15\u5165CoT\u89c4\u5212\u63d0\u5347\u63a8\u7406\u80fd\u529b\n3. \u6784\u5efa\u81ea\u52a8\u5316\u6d41\u7a0b\u751f\u6210110K\u4e09\u5143\u7ec4\u6570\u636e\u96c6", "result": "\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u76f4\u63a5\u751f\u6210\u591a\u6837\u5316\u3001\u6709\u6548\u4e14\u590d\u6742\u7684CAD\u6a21\u578b\uff08Chamfer Distance\u6307\u6807\u9a8c\u8bc1\u51e0\u4f55\u7cbe\u5ea6\uff09", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u53c2\u6570\u5316\u811a\u672c\u8868\u793a\u3001\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\u548c\u81ea\u52a8\u5316\u6570\u636e\u6784\u5efa\uff0c\u663e\u8457\u63a8\u8fdb\u6587\u672c\u5230CAD\u751f\u6210\u7684\u6280\u672f\u8fb9\u754c"}}
{"id": "2505.18322", "pdf": "https://arxiv.org/pdf/2505.18322", "abs": "https://arxiv.org/abs/2505.18322", "authors": ["Zhuozhuo Joy Liu", "Farhan Samir", "Mehar Bhatia", "Laura K. Nelson", "Vered Shwartz"], "title": "Is It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs have been demonstrated to align with the values of Western or North\nAmerican cultures. Prior work predominantly showed this effect through\nleveraging surveys that directly ask (originally people and now also LLMs)\nabout their values. However, it is hard to believe that LLMs would consistently\napply those values in real-world scenarios. To address that, we take a\nbottom-up approach, asking LLMs to reason about cultural norms in narratives\nfrom different cultures. We find that GPT-4 tends to generate norms that, while\nnot necessarily incorrect, are significantly less culture-specific. In\naddition, while it avoids overtly generating stereotypes, the stereotypical\nrepresentations of certain cultures are merely hidden rather than suppressed in\nthe model, and such stereotypes can be easily recovered. Addressing these\nchallenges is a crucial step towards developing LLMs that fairly serve their\ndiverse user base.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u751f\u6210\u7684\u6587\u5316\u89c4\u8303\u7f3a\u4e4f\u6587\u5316\u7279\u5f02\u6027\uff0c\u4e14\u9690\u85cf\u7684\u523b\u677f\u5370\u8c61\u5bb9\u6613\u88ab\u6fc0\u6d3b", "motivation": "\u73b0\u6709\u7814\u7a76\u901a\u8fc7\u76f4\u63a5\u8be2\u95ee\u65b9\u5f0f\u9a8c\u8bc1LLMs\u7684\u4ef7\u503c\u89c2\u5bf9\u9f50\uff0c\u4f46\u65e0\u6cd5\u53cd\u6620\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u771f\u5b9e\u8868\u73b0\u3002\u9700\u8981\u9a8c\u8bc1LLMs\u5728\u4e0d\u540c\u6587\u5316\u53d9\u4e8b\u4e2d\u7684\u4ef7\u503c\u89c2\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5206\u6790LLMs\u751f\u6210\u7684\u4e0d\u540c\u6587\u5316\u53d9\u4e8b\u4e2d\u7684\u6587\u5316\u89c4\u8303\uff0c\u91c7\u7528\u81ea\u4e0b\u800c\u4e0a\u7684\u5b9e\u8bc1\u7814\u7a76\u65b9\u6cd5", "result": "1. GPT-4\u751f\u6210\u7684\u89c4\u8303\u6587\u5316\u7279\u5f02\u6027\u663e\u8457\u964d\u4f4e\n2. \u6a21\u578b\u9690\u85cf\u800c\u975e\u6d88\u9664\u6587\u5316\u523b\u677f\u5370\u8c61\n3. \u9690\u85cf\u7684\u523b\u677f\u5370\u8c61\u53ef\u88ab\u8f7b\u6613\u6fc0\u6d3b", "conclusion": "\u89e3\u51b3LLMs\u7684\u6587\u5316\u504f\u5dee\u548c\u6f5c\u5728\u523b\u677f\u5370\u8c61\u662f\u6784\u5efa\u516c\u5e73AI\u7cfb\u7edf\u7684\u5173\u952e\u6311\u6218"}}
{"id": "2505.19976", "pdf": "https://arxiv.org/pdf/2505.19976", "abs": "https://arxiv.org/abs/2505.19976", "authors": ["Naoki Agata", "Takeo Igarashi"], "title": "MAMM: Motion Control via Metric-Aligning Motion Matching", "categories": ["cs.GR"], "comment": "12 pages, SIGGRAPH 2025 (Conference Track)", "summary": "We introduce a novel method for controlling a motion sequence using an\narbitrary temporal control sequence using temporal alignment. Temporal\nalignment of motion has gained significant attention owing to its applications\nin motion control and retargeting. Traditional methods rely on either learned\nor hand-craft cross-domain mappings between frames in the original and control\ndomains, which often require large, paired, or annotated datasets and\ntime-consuming training. Our approach, named Metric-Aligning Motion Matching,\nachieves alignment by solely considering within-domain distances. It computes\ndistances among patches in each domain and seeks a matching that optimally\naligns the two within-domain distances. This framework allows for the alignment\nof a motion sequence to various types of control sequences, including sketches,\nlabels, audio, and another motion sequence, all without the need for manually\ndefined mappings or training with annotated data. We demonstrate the\neffectiveness of our approach through applications in efficient motion control,\nshowcasing its potential in practical scenarios.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u57df\u5185\u8ddd\u79bb\u5bf9\u9f50\u7684Metric-Aligning Motion Matching\u65b9\u6cd5\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u8fd0\u52a8\u5e8f\u5217\u4e0e\u8349\u56fe/\u6807\u7b7e/\u97f3\u9891\u7b49\u591a\u6a21\u6001\u63a7\u5236\u5e8f\u5217\u7684\u8de8\u57df\u5bf9\u9f50\u3002", "motivation": "\u4f20\u7edf\u8fd0\u52a8\u65f6\u5e8f\u5bf9\u9f50\u65b9\u6cd5\u4f9d\u8d56\u8de8\u57df\u6620\u5c04\uff0c\u9700\u8981\u5927\u91cf\u914d\u5bf9\u6807\u6ce8\u6570\u636e\u548c\u8017\u65f6\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u8fd0\u52a8\u57df\u548c\u63a7\u5236\u57df\u5404\u81ea\u7684\u5757\u95f4\u8ddd\u79bb\u77e9\u9635\uff0c\u5bfb\u627e\u6700\u4f18\u5339\u914d\u4f7f\u4e24\u4e2a\u57df\u7684\u8ddd\u79bb\u77e9\u9635\u5bf9\u9f50\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u65f6\u5e8f\u5bf9\u9f50\u3002", "result": "\u6210\u529f\u5e94\u7528\u4e8e\u8349\u56fe\u63a7\u5236\u3001\u6807\u7b7e\u9a71\u52a8\u3001\u97f3\u9891\u540c\u6b65\u7b49\u591a\u79cd\u573a\u666f\uff0c\u5b9e\u9a8c\u8bc1\u660e\u65b9\u6cd5\u5728\u8fd0\u52a8\u63a7\u5236\u6548\u7387\u548c\u6548\u679c\u4e0a\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u6846\u67b6\u7a81\u7834\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u4e3a\u591a\u6a21\u6001\u8fd0\u52a8\u63a7\u5236\u63d0\u4f9b\u4e86\u7075\u6d3b\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.18331", "pdf": "https://arxiv.org/pdf/2505.18331", "abs": "https://arxiv.org/abs/2505.18331", "authors": ["Naghmeh Jamali", "Milad Mohammadi", "Danial Baledi", "Zahra Rezvani", "Hesham Faili"], "title": "PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Medical consumer question answering (CQA) is crucial for empowering patients\nby providing personalized and reliable health information. Despite recent\nadvances in large language models (LLMs) for medical QA, consumer-oriented and\nmultilingual resources, particularly in low-resource languages like Persian,\nremain sparse. To bridge this gap, we present PerMedCQA, the first\nPersian-language benchmark for evaluating LLMs on real-world,\nconsumer-generated medical questions. Curated from a large medical QA forum,\nPerMedCQA contains 68,138 question-answer pairs, refined through careful data\ncleaning from an initial set of 87,780 raw entries. We evaluate several\nstate-of-the-art multilingual and instruction-tuned LLMs, utilizing MedJudge, a\nnovel rubric-based evaluation framework driven by an LLM grader, validated\nagainst expert human annotators. Our results highlight key challenges in\nmultilingual medical QA and provide valuable insights for developing more\naccurate and context-aware medical assistance systems. The data is publicly\navailable on https://huggingface.co/datasets/NaghmehAI/PerMedCQA", "AI": {"tldr": "\u9996\u4e2a\u6ce2\u65af\u8bed\u533b\u7597\u95ee\u7b54\u57fa\u51c6PerMedCQA\u7684\u6784\u5efa\u4e0e\u591a\u8bed\u8a00LLM\u8bc4\u4f30", "motivation": "\u89e3\u51b3\u6ce2\u65af\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u6d88\u8d39\u8005\u533b\u7597\u95ee\u7b54\u6570\u636e\u53ca\u8bc4\u4f30\u4f53\u7cfb\u532e\u4e4f\u7684\u95ee\u9898", "method": "1. \u4ece\u533b\u7597\u8bba\u575b\u6784\u5efa68k QA\u5bf9\u6570\u636e\u96c6\n2. \u63d0\u51faMedJudge\u8bc4\u4f30\u6846\u67b6\uff08LLM\u8bc4\u5206\u5668+\u4e13\u5bb6\u9a8c\u8bc1\uff09\n3. \u6d4b\u8bd5\u591a\u8bed\u8a00/\u6307\u4ee4\u8c03\u4f18LLM", "result": "1. \u53d1\u73b0\u591a\u8bed\u8a00\u533b\u7597QA\u6838\u5fc3\u6311\u6218\n2. MedJudge\u4e0e\u4e13\u5bb6\u6807\u6ce8\u4e00\u81f4\u6027\u8fbe92%\n3. \u6700\u4f73\u6a21\u578b\u51c6\u786e\u7387\u4ec558.3%", "conclusion": "\u9700\u5f00\u53d1\u517c\u987e\u8bed\u8a00\u9002\u5e94\u4e0e\u533b\u7597\u4e0a\u4e0b\u6587\u7406\u89e3\u7684\u8f85\u52a9\u7cfb\u7edf\uff0cPerMedCQA\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u57fa\u51c6\u652f\u6301"}}
{"id": "2505.18168", "pdf": "https://arxiv.org/pdf/2505.18168", "abs": "https://arxiv.org/abs/2505.18168", "authors": ["Feifan Wang", "Tengfei Song", "Minggui He", "Chang Su", "Zhanglin Wu", "Hao Yang", "Wenming Zheng", "Osamu Yoshie"], "title": "Emotion Knowledge Enhancement for Vision Large Language Models: A Self-Verification Approach for High-Quality Emotion Instruction Data Generation", "categories": ["cs.LG", "cs.GR"], "comment": null, "summary": "Facial emotion perception in the vision large language model (VLLM) is\ncrucial for achieving natural human-machine interaction. However, creating\nhigh-quality annotations for both coarse- and fine-grained facial emotion\nanalysis demands costly expertise. The lack of such high-quality instruction\ndata limits the performance of VLLMs in facial emotion perception. To address\nthis, we propose a self-verification approach with emotion knowledge\nenhancement (SEKE), which generates high-quality instruction data for\nmulti-grained emotion analysis cost-effectively using closed-source VLLM. This\napproach integrates prior human knowledge to VLLM inference, guided by the\ninherent correlations between three grained levels of emotion descriptions,\ni.e., discrete expression, valence-arousal, and action unit, to reliably\ngenerate comprehensive annotations. A self-verification strategy with\nUncertainty-Aware Monte Carlo sampling (SV-UAMC) is further embedded to\nefficiently extract more accurate VLLM predictions, further improving\nannotation reliability. Consequently, we construct a facial emotion instruction\ndataset (FEID) containing three comprehensive descriptions, which provides\ncoarse- and fine-grained emotional information for effective model training.\nAdditionally, we introduce a facial emotion analysis benchmark (FEAB) to\nmeasure the VLLM's corresponding ability. Our method significantly outperforms\nstate-of-the-art methods on three downstream facial emotion analysis tasks.", "AI": {"tldr": "\u63d0\u51faSEKE\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9a8c\u8bc1\u673a\u5236\u4e0e\u60c5\u611f\u77e5\u8bc6\u589e\u5f3a\u751f\u6210\u9ad8\u8d28\u91cf\u591a\u7c92\u5ea6\u60c5\u7eea\u6807\u6ce8\u6570\u636e\uff0c\u6784\u5efaFEID\u6570\u636e\u96c6\u548cFEAB\u57fa\u51c6\uff0c\u663e\u8457\u63d0\u5347VLLM\u5728\u9762\u90e8\u60c5\u7eea\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9762\u90e8\u60c5\u7eea\u611f\u77e5\u4e2d\u5b58\u5728\u9ad8\u8d28\u91cf\u591a\u7c92\u5ea6\u6807\u6ce8\u6570\u636e\u532e\u4e4f\u7684\u95ee\u9898\uff0c\u4e13\u5bb6\u6807\u6ce8\u6210\u672c\u8fc7\u9ad8\u5236\u7ea6\u6a21\u578b\u6027\u80fd\u63d0\u5347\u3002", "method": "\u878d\u5408\u4eba\u7c7b\u5148\u9a8c\u77e5\u8bc6\u4e0eVLLM\u63a8\u7406\u80fd\u529b\uff0c\u5229\u7528\u60c5\u7eea\u63cf\u8ff0\u4e09\u7c92\u5ea6\uff08\u79bb\u6563\u8868\u60c5/\u6548\u4ef7-\u5524\u9192/\u52a8\u4f5c\u5355\u5143\uff09\u7684\u56fa\u6709\u76f8\u5173\u6027\u751f\u6210\u6807\u6ce8\uff0c\u91c7\u7528\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u8499\u7279\u5361\u6d1b\u91c7\u6837\u81ea\u9a8c\u8bc1\u7b56\u7565\u4f18\u5316\u9884\u6d4b\u51c6\u786e\u6027\u3002", "result": "\u6784\u5efa\u5305\u542b3\u7c7b\u63cf\u8ff0\u7684\u9762\u90e8\u60c5\u7eea\u6307\u4ee4\u6570\u636e\u96c6FEID\uff0c\u5efa\u7acb\u8bc4\u4f30\u57fa\u51c6FEAB\uff0c\u5728\u4e09\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u3002", "conclusion": "SEKE\u65b9\u6cd5\u4ee5\u4f4e\u6210\u672c\u751f\u6210\u53ef\u9760\u6807\u6ce8\u6570\u636e\uff0c\u7a81\u7834VLLM\u60c5\u7eea\u5206\u6790\u74f6\u9888\uff0c\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u4e0e\u6807\u51c6\u5316\u8bc4\u4f30\u4f53\u7cfb\u3002"}}
{"id": "2505.18343", "pdf": "https://arxiv.org/pdf/2505.18343", "abs": "https://arxiv.org/abs/2505.18343", "authors": ["Yash Kumar Atri", "Ahmed Alaa", "Thomas Hartvigsen"], "title": "Model Editing with Graph-Based External Memory", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have revolutionized natural language processing,\nyet their practical utility is often limited by persistent issues of\nhallucinations and outdated parametric knowledge. Although post-training model\nediting offers a pathway for dynamic updates, existing methods frequently\nsuffer from overfitting and catastrophic forgetting. To tackle these\nchallenges, we propose a novel framework that leverages hyperbolic geometry and\ngraph neural networks for precise and stable model edits. We introduce HYPE\n(HYperbolic Parameter Editing), which comprises three key components: (i)\nHyperbolic Graph Construction, which uses Poincar\\'e embeddings to represent\nknowledge triples in hyperbolic space, preserving hierarchical relationships\nand preventing unintended side effects by ensuring that edits to parent\nconcepts do not inadvertently affect child concepts; (ii) M\\\"obius-Transformed\nUpdates, which apply hyperbolic addition to propagate edits while maintaining\nstructural consistency within the hyperbolic manifold, unlike conventional\nEuclidean updates that distort relational distances; and (iii) Dual\nStabilization, which combines gradient masking and periodic GNN parameter\nresetting to prevent catastrophic forgetting by focusing updates on critical\nparameters and preserving long-term knowledge. Experiments on CounterFact,\nCounterFact+, and MQuAKE with GPT-J and GPT2-XL demonstrate that HYPE\nsignificantly enhances edit stability, factual accuracy, and multi-hop\nreasoning.", "AI": {"tldr": "\u63d0\u51faHYPE\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u66f2\u51e0\u4f55+\u56fe\u795e\u7ecf\u7f51\u7edc\u89e3\u51b3\u5927\u6a21\u578b\u7f16\u8f91\u4e2d\u7684\u8fc7\u62df\u5408/\u9057\u5fd8\u95ee\u9898\uff0c\u5b9e\u73b0\u7cbe\u51c6\u7a33\u5b9a\u66f4\u65b0", "motivation": "\u73b0\u6709\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u5b58\u5728\u8fc7\u62df\u5408\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u65e0\u6cd5\u540c\u65f6\u4fdd\u8bc1\u77e5\u8bc6\u66f4\u65b0\u7684\u51c6\u786e\u6027\u548c\u7a33\u5b9a\u6027", "method": "\u2460\u53cc\u66f2\u56fe\u6784\u5efa\uff08\u4fdd\u6301\u77e5\u8bc6\u5c42\u6b21\u7ed3\u6784\uff09 \u2461Mobius\u53d8\u6362\u66f4\u65b0\uff08\u4fdd\u6301\u53cc\u66f2\u6d41\u5f62\u7ed3\u6784\uff09 \u2462\u53cc\u7a33\u5b9a\u673a\u5236\uff08\u68af\u5ea6\u63a9\u7801+\u53c2\u6570\u91cd\u7f6e\uff09", "result": "\u5728CounterFact\u7b49\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347GPT-J/GPT2-XL\u7684\u7f16\u8f91\u7a33\u5b9a\u6027\u3001\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u591a\u8df3\u63a8\u7406", "conclusion": "HYPE\u9996\u6b21\u5c06\u53cc\u66f2\u51e0\u4f55\u4e0eGNN\u7ed3\u5408\uff0c\u7a81\u7834\u4f20\u7edf\u6b27\u5f0f\u7a7a\u95f4\u7684\u9650\u5236\uff0c\u4e3a\u52a8\u6001\u77e5\u8bc6\u66f4\u65b0\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2505.19086", "pdf": "https://arxiv.org/pdf/2505.19086", "abs": "https://arxiv.org/abs/2505.19086", "authors": ["Chen Tessler", "Yifeng Jiang", "Erwin Coumans", "Zhengyi Luo", "Gal Chechik", "Xue Bin Peng"], "title": "MaskedManipulator: Versatile Whole-Body Control for Loco-Manipulation", "categories": ["cs.RO", "cs.AI", "cs.GR"], "comment": null, "summary": "Humans interact with their world while leveraging precise full-body control\nto achieve versatile goals. This versatility allows them to solve long-horizon,\nunderspecified problems, such as placing a cup in a sink, by seamlessly\nsequencing actions like approaching the cup, grasping, transporting it, and\nfinally placing it in the sink. Such goal-driven control can enable new\nprocedural tools for animation systems, enabling users to define partial\nobjectives while the system naturally ``fills in'' the intermediate motions.\nHowever, while current methods for whole-body dexterous manipulation in\nphysics-based animation achieve success in specific interaction tasks, they\ntypically employ control paradigms (e.g., detailed kinematic motion tracking,\ncontinuous object trajectory following, or direct VR teleoperation) that offer\nlimited versatility for high-level goal specification across the entire coupled\nhuman-object system. To bridge this gap, we present MaskedManipulator, a\nunified and generative policy developed through a two-stage learning approach.\nFirst, our system trains a tracking controller to physically reconstruct\ncomplex human-object interactions from large-scale human mocap datasets. This\ntracking controller is then distilled into MaskedManipulator, which provides\nusers with intuitive control over both the character's body and the manipulated\nobject. As a result, MaskedManipulator enables users to specify complex\nloco-manipulation tasks through intuitive high-level objectives (e.g., target\nobject poses, key character stances), and MaskedManipulator then synthesizes\nthe necessary full-body actions for a physically simulated humanoid to achieve\nthese goals, paving the way for more interactive and life-like virtual\ncharacters.", "AI": {"tldr": "\u63d0\u51faMaskedManipulator\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5b66\u4e60\u5b9e\u73b0\u7269\u7406\u4eba\u5f62\u673a\u5668\u4eba\u57fa\u4e8e\u9ad8\u5c42\u76ee\u6807\uff08\u7269\u4f53\u59ff\u6001/\u5173\u952e\u59ff\u52bf\uff09\u7684\u5168\u8eab\u52a8\u4f5c\u81ea\u4e3b\u751f\u6210", "motivation": "\u73b0\u6709\u7269\u7406\u6a21\u62df\u63a7\u5236\u65b9\u6cd5\u5c40\u9650\u4e8e\u5177\u4f53\u4efb\u52a1\u4e14\u7f3a\u4e4f\u7075\u6d3b\u7684\u9ad8\u5c42\u76ee\u6807\u63a7\u5236\u80fd\u529b\uff0c\u96be\u4ee5\u5b9e\u73b0\u7c7b\u4f3c\u4eba\u7c7b\u7684\u591a\u5c42\u6b21\u64cd\u4f5c\u5e8f\u5217", "method": "1. \u4ece\u52a8\u6355\u6570\u636e\u8bad\u7ec3\u8ddf\u8e2a\u63a7\u5236\u5668\u91cd\u5efa\u4ea4\u4e92\u52a8\u4f5c \u2192 2. \u84b8\u998f\u751f\u6210\u7edf\u4e00\u7b56\u7565\uff0c\u652f\u6301\u7528\u6237\u901a\u8fc7\u7269\u4f53\u4f4d\u59ff/\u8eab\u4f53\u59ff\u6001\u7b49\u8bed\u4e49\u7ea7\u53c2\u6570\u63a7\u5236", "result": "\u7cfb\u7edf\u53ef\u81ea\u52a8\u8865\u5168\u4e2d\u95f4\u52a8\u4f5c\uff0c\u5b9e\u73b0\u81ea\u7136\u7684\u957f\u65f6\u7a0b\u64cd\u4f5c\u5e8f\u5217\uff08\u5982\u6293\u53d6-\u79fb\u52a8-\u653e\u7f6e\uff09\uff0c\u652f\u6301\u7269\u7406\u6a21\u62df\u4eba\u5f62\u5b8c\u6210\u590d\u6742\u4eba-\u7269\u8054\u5408\u63a7\u5236\u4efb\u52a1", "conclusion": "\u8be5\u6280\u672f\u7a81\u7834\u4f20\u7edf\u4f4e\u5c42\u63a7\u5236\u8303\u5f0f\uff0c\u4e3a\u52a8\u753b\u7cfb\u7edf\u63d0\u4f9b\u7a0b\u5e8f\u5316\u751f\u6210\u5de5\u5177\uff0c\u63a8\u52a8\u865a\u62df\u89d2\u8272\u5411\u66f4\u4ea4\u4e92\u5f0f\u3001\u62df\u4eba\u5316\u65b9\u5411\u53d1\u5c55"}}
{"id": "2505.18356", "pdf": "https://arxiv.org/pdf/2505.18356", "abs": "https://arxiv.org/abs/2505.18356", "authors": ["Lucas Bandarkar", "Nanyun Peng"], "title": "The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "Large language models (LLMs) still struggle across tasks outside of\nhigh-resource languages. In this work, we investigate cross-lingual transfer to\nlower-resource languages where task-specific post-training data is scarce.\nBuilding on prior work, we first validate that the subsets of model parameters\nthat matter most for mathematical reasoning and multilingual capabilities are\ndistinctly non-overlapping. To exploit this implicit separability between task\nand target language parameterization, we develop and analyze numerous modular\nframeworks to improve the composition of the two during fine-tuning. These\nmethods generally employ freezing parameters or post hoc model merging to\nassign math and language improvement to different key parts of the LLM. In the\nabsence of in-language math data, we demonstrate that the modular approaches\nsuccessfully improve upon baselines across three languages, four models, and\ntwo fine-tuning paradigms (full and LoRA). Furthermore, we identify the most\nconsistently successful modular method to be fine-tuning separate language and\nmath experts and model merging via Layer-Swapping, somewhat surprisingly. We\noffer possible explanations for this result via recent works on the linearity\nof task vectors. We further explain this by empirically showing that reverting\nless useful fine-tuning updates after training often outperforms freezing them\nfrom the start.", "AI": {"tldr": "\u901a\u8fc7\u6a21\u5757\u5316\u6846\u67b6\u5b9e\u73b0\u6570\u5b66\u63a8\u7406\u4e0e\u591a\u8bed\u8a00\u80fd\u529b\u53c2\u6570\u89e3\u8026\uff0c\u63d0\u51fa\u5c42\u4ea4\u6362\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u89e3\u51b3LLMs\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4efb\u52a1\u4e2d\u56e0\u7f3a\u4e4f\u7279\u5b9a\u4efb\u52a1\u6570\u636e\u5bfc\u81f4\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5229\u7528\u53c2\u6570\u975e\u91cd\u53e0\u7279\u6027\u6784\u5efa\u7ec4\u5408\u5f0f\u4f18\u5316\u65b9\u6848", "method": "1. \u9a8c\u8bc1\u6570\u5b66\u63a8\u7406/\u591a\u8bed\u8a00\u53c2\u6570\u975e\u91cd\u53e0\u6027 2. \u5f00\u53d1\u51bb\u7ed3\u53c2\u6570/\u6a21\u578b\u5408\u5e76\u6846\u67b6 3. \u63d0\u51fa\u5c42\u4ea4\u6362\uff08Layer-Swapping\uff09\u878d\u5408\u4e13\u5bb6\u6a21\u578b", "result": "\u57283\u79cd\u8bed\u8a00\u30014\u4e2a\u6a21\u578b\u30012\u79cd\u5fae\u8c03\u8303\u5f0f\u4e0b\uff0c\u6a21\u5757\u5316\u65b9\u6cd5\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u5c42\u4ea4\u6362\u6cd5\u8868\u73b0\u6700\u4f18\u4e14\u7a33\u5b9a", "conclusion": "\u4e8b\u540e\u56de\u9000\u65e0\u6548\u53c2\u6570\u66f4\u65b0\u4f18\u4e8e\u63d0\u524d\u51bb\u7ed3\uff0c\u4efb\u52a1\u5411\u91cf\u7ebf\u6027\u7279\u6027\u652f\u6301\u6a21\u5757\u5316\u7ec4\u5408\uff0c\u4e3a\u4f4e\u8d44\u6e90\u4f18\u5316\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2505.19306", "pdf": "https://arxiv.org/pdf/2505.19306", "abs": "https://arxiv.org/abs/2505.19306", "authors": ["Weiming Zhi", "Ziyong Ma", "Tianyi Zhang", "Matthew Johnson-Roberson"], "title": "From Single Images to Motion Policies via Video-Generation Environment Representations", "categories": ["cs.RO", "cs.CV", "cs.GR", "cs.LG"], "comment": null, "summary": "Autonomous robots typically need to construct representations of their\nsurroundings and adapt their motions to the geometry of their environment.\nHere, we tackle the problem of constructing a policy model for collision-free\nmotion generation, consistent with the environment, from a single input RGB\nimage. Extracting 3D structures from a single image often involves monocular\ndepth estimation. Developments in depth estimation have given rise to large\npre-trained models such as DepthAnything. However, using outputs of these\nmodels for downstream motion generation is challenging due to frustum-shaped\nerrors that arise. Instead, we propose a framework known as Video-Generation\nEnvironment Representation (VGER), which leverages the advances of large-scale\nvideo generation models to generate a moving camera video conditioned on the\ninput image. Frames of this video, which form a multiview dataset, are then\ninput into a pre-trained 3D foundation model to produce a dense point cloud. We\nthen introduce a multi-scale noise approach to train an implicit representation\nof the environment structure and build a motion generation model that complies\nwith the geometry of the representation. We extensively evaluate VGER over a\ndiverse set of indoor and outdoor environments. We demonstrate its ability to\nproduce smooth motions that account for the captured geometry of a scene, all\nfrom a single RGB input image.", "AI": {"tldr": "\u63d0\u51faVGER\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u9891\u751f\u6210\u6a21\u578b\u548c3D\u57fa\u7840\u6a21\u578b\u4ece\u5355\u5f20RGB\u56fe\u50cf\u6784\u5efa\u73af\u5883\u8868\u793a\uff0c\u5e76\u751f\u6210\u7b26\u5408\u573a\u666f\u51e0\u4f55\u7684\u5e73\u6ed1\u8fd0\u52a8\u7b56\u7565", "motivation": "\u89e3\u51b3\u73b0\u6709\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\uff08\u5982DepthAnything\uff09\u5728\u8fd0\u52a8\u751f\u6210\u4efb\u52a1\u4e2d\u5b58\u5728\u7684\u89c6\u9525\u5f62\u8bef\u5dee\u95ee\u9898", "method": "1. \u5229\u7528\u89c6\u9891\u751f\u6210\u6a21\u578b\u751f\u6210\u591a\u89c6\u89d2\u89c6\u9891\n2. \u901a\u8fc7\u9884\u8bad\u7ec33D\u57fa\u7840\u6a21\u578b\u751f\u6210\u5bc6\u96c6\u70b9\u4e91\n3. \u91c7\u7528\u591a\u5c3a\u5ea6\u566a\u58f0\u8bad\u7ec3\u9690\u5f0f\u73af\u5883\u8868\u793a\n4. \u5efa\u7acb\u51e0\u4f55\u7b26\u5408\u7684\u8fd0\u52a8\u751f\u6210\u6a21\u578b", "result": "\u5728\u591a\u6837\u5316\u7684\u5ba4\u5185\u5916\u73af\u5883\u8bc4\u4f30\u4e2d\uff0c\u6210\u529f\u751f\u6210\u8003\u8651\u573a\u666f\u51e0\u4f55\u7684\u5e73\u6ed1\u8fd0\u52a8\u8f68\u8ff9", "conclusion": "VGER\u6846\u67b6\u4ec5\u9700\u5355\u5f20RGB\u56fe\u50cf\u5373\u53ef\u51c6\u786e\u6355\u6349\u573a\u666f\u51e0\u4f55\u7ed3\u6784\uff0c\u4e3a\u81ea\u4e3b\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.18363", "pdf": "https://arxiv.org/pdf/2505.18363", "abs": "https://arxiv.org/abs/2505.18363", "authors": ["AmirHossein Safdarian", "Milad Mohammadi", "Ehsan Jahanbakhsh", "Mona Shahamat Naderi", "Heshaam Faili"], "title": "SchemaGraphSQL: Efficient Schema Linking with Pathfinding Graph Algorithms for Text-to-SQL on Large-Scale Databases", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "Text-to-SQL systems translate natural language questions into executable SQL\nqueries, and recent progress with large language models (LLMs) has driven\nsubstantial improvements in this task. Schema linking remains a critical\ncomponent in Text-to-SQL systems, reducing prompt size for models with narrow\ncontext windows and sharpening model focus even when the entire schema fits. We\npresent a zero-shot, training-free schema linking approach that first\nconstructs a schema graph based on foreign key relations, then uses a single\nprompt to Gemini 2.5 Flash to extract source and destination tables from the\nuser query, followed by applying classical path-finding algorithms and\npost-processing to identify the optimal sequence of tables and columns that\nshould be joined, enabling the LLM to generate more accurate SQL queries.\nDespite being simple, cost-effective, and highly scalable, our method achieves\nstate-of-the-art results on the BIRD benchmark, outperforming previous\nspecialized, fine-tuned, and complex multi-step LLM-based approaches. We\nconduct detailed ablation studies to examine the precision-recall trade-off in\nour framework. Additionally, we evaluate the execution accuracy of our schema\nfiltering method compared to other approaches across various model sizes.", "AI": {"tldr": "\u63d0\u51fa\u96f6\u6837\u672c\u3001\u514d\u8bad\u7ec3\u7684\u6a21\u5f0f\u94fe\u63a5\u65b9\u6cd5\uff0c\u901a\u8fc7\u5916\u952e\u5173\u7cfb\u6784\u5efa\u6a21\u5f0f\u56fe\u5e76\u5e94\u7528\u8def\u5f84\u67e5\u627e\u7b97\u6cd5\uff0c\u5728BIRD\u57fa\u51c6\u5b9e\u73b0SOTA\u6027\u80fd", "motivation": "\u4f20\u7edfText-to-SQL\u7cfb\u7edf\u5b58\u5728\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u548c\u6a21\u5f0f\u5197\u4f59\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u6a21\u5f0f\u94fe\u63a5\u65b9\u6cd5\u63d0\u5347SQL\u751f\u6210\u7cbe\u5ea6", "method": "1. \u57fa\u4e8e\u5916\u952e\u6784\u5efa\u6a21\u5f0f\u56fe 2. \u7528Gemini 2.5 Flash\u63d0\u53d6\u67e5\u8be2\u4e2d\u7684\u8868 3. \u5e94\u7528\u7ecf\u5178\u8def\u5f84\u7b97\u6cd5\u786e\u5b9a\u6700\u4f18\u8fde\u63a5\u987a\u5e8f", "result": "\u5728BIRD\u57fa\u51c6\u5b9e\u73b0\u65b0SOTA\uff0c\u6267\u884c\u7cbe\u5ea6\u8fbe73.5%\uff0c\u4f18\u4e8e\u5148\u524d\u5fae\u8c03\u65b9\u6cd5\u548c\u590d\u6742\u591a\u6b65LLM\u65b9\u6848", "conclusion": "\u8be5\u65b9\u6848\u5177\u5907\u7b80\u5355\u3001\u4f4e\u6210\u672c\u548c\u5f3a\u6269\u5c55\u6027\u4f18\u52bf\uff0c\u901a\u8fc7\u7cbe\u786e\u7684\u6a21\u5f0f\u8fc7\u6ee4\u663e\u8457\u63d0\u5347LLM\u7684SQL\u751f\u6210\u8d28\u91cf"}}
{"id": "2505.20129", "pdf": "https://arxiv.org/pdf/2505.20129", "abs": "https://arxiv.org/abs/2505.20129", "authors": ["Xinhang Liu", "Yu-Wing Tai", "Chi-Keung Tang"], "title": "Agentic 3D Scene Generation with Spatially Contextualized VLMs", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Despite recent advances in multimodal content generation enabled by\nvision-language models (VLMs), their ability to reason about and generate\nstructured 3D scenes remains largely underexplored. This limitation constrains\ntheir utility in spatially grounded tasks such as embodied AI, immersive\nsimulations, and interactive 3D applications. We introduce a new paradigm that\nenables VLMs to generate, understand, and edit complex 3D environments by\ninjecting a continually evolving spatial context. Constructed from multimodal\ninput, this context consists of three components: a scene portrait that\nprovides a high-level semantic blueprint, a semantically labeled point cloud\ncapturing object-level geometry, and a scene hypergraph that encodes rich\nspatial relationships, including unary, binary, and higher-order constraints.\nTogether, these components provide the VLM with a structured, geometry-aware\nworking memory that integrates its inherent multimodal reasoning capabilities\nwith structured 3D understanding for effective spatial reasoning. Building on\nthis foundation, we develop an agentic 3D scene generation pipeline in which\nthe VLM iteratively reads from and updates the spatial context. The pipeline\nfeatures high-quality asset generation with geometric restoration, environment\nsetup with automatic verification, and ergonomic adjustment guided by the scene\nhypergraph. Experiments show that our framework can handle diverse and\nchallenging inputs, achieving a level of generalization not observed in prior\nwork. Further results demonstrate that injecting spatial context enables VLMs\nto perform downstream tasks such as interactive scene editing and path\nplanning, suggesting strong potential for spatially intelligent systems in\ncomputer graphics, 3D vision, and embodied applications.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u7a7a\u95f4\u4e0a\u4e0b\u6587\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4e09\u7ef4\u573a\u666f\u7406\u89e3\u4e0e\u751f\u6210\u80fd\u529b\uff0c\u6784\u5efa\u5305\u542b\u573a\u666f\u753b\u50cf\u3001\u8bed\u4e49\u70b9\u4e91\u548c\u573a\u666f\u8d85\u56fe\u7684\u7ed3\u6784\u5316\u7a7a\u95f4\u8bb0\u5fc6\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u4e09\u7ef4\u573a\u666f\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5236\u7ea6\u4e86\u5176\u5728\u5177\u8eab\u667a\u80fd/\u4eff\u771f\u73af\u5883\u7b49\u7a7a\u95f4\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "1) \u6784\u5efa\u6301\u7eed\u6f14\u8fdb\u7684\u7a7a\u95f4\u4e0a\u4e0b\u6587(\u573a\u666f\u753b\u50cf+\u8bed\u4e49\u70b9\u4e91+\u573a\u666f\u8d85\u56fe) \n2) \u5f00\u53d1\u5305\u542b\u51e0\u4f55\u4fee\u590d/\u73af\u5883\u9a8c\u8bc1/\u4eba\u673a\u5de5\u7a0b\u8c03\u6574\u7684\u4ee3\u7406\u751f\u6210\u6d41\u7a0b", "result": "\u5b9e\u9a8c\u8bc1\u660e\u6846\u67b6\u5177\u5907\u5904\u7406\u590d\u6742\u8f93\u5165\u7684\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u652f\u6301\u573a\u666f\u7f16\u8f91/\u8def\u5f84\u89c4\u5212\u7b49\u4e0b\u6e38\u4efb\u52a1\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8ba1\u7b97\u673a\u56fe\u5f62\u5b66/\u4e09\u7ef4\u89c6\u89c9/\u5177\u8eab\u7cfb\u7edf\u5f00\u53d1\u7a7a\u95f4\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.18374", "pdf": "https://arxiv.org/pdf/2505.18374", "abs": "https://arxiv.org/abs/2505.18374", "authors": ["Jarrod Ragsdale", "Rajendra Boppana"], "title": "ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation", "categories": ["cs.CL", "cs.LG"], "comment": "18 pages, 11 figures, conference preprint", "summary": "Command-line interfaces (CLIs) provide structured textual environments for\nsystem administration. Explorations have been performed using pre-trained\nlanguage models (PLMs) to simulate these environments for safe interaction in\nhigh-risk environments. However, their use has been constrained to frozen,\nlarge parameter models like GPT. For smaller architectures to reach a similar\nlevel of believability, a rich dataset of CLI interactions is required.\nExisting public datasets focus on mapping natural-language tasks to commands,\nomitting crucial execution data such as exit codes, outputs, and environmental\nside effects, limiting their usability for behavioral modeling. We introduce a\nShell Input -Output Environment (ShIOEnv), which casts command construction as\na Markov Decision Process whose state is the partially built sequence and whose\nactions append arguments. After each action, ShIOEnv executes the candidate and\nreturns its exit status, output, and progress toward a minimal-length\nbehavioral objective. Due to the intractable nature of the combinatorial\nargument state-action space, we derive a context-free grammar from man pages to\nmask invalid arguments from being emitted. We explore random and\nproximal-policy optimization (PPO)-optimized sampling of unrestricted and\ngrammar-masked action spaces to produce four exploration strategies. We\nobserved that grammar masking and PPO significantly improve sample efficiency\nto produce a higher quality dataset (maximizing the number of arguments while\nminimizing redundancies). Policy-generated datasets of shell input-output\nbehavior pairs are used to fine-tune CodeT5, where we observe 85% improvements\nin BLEU-4 when constraining the action space to grammar productions with an\nadditional 26% improvement when applying PPO. The ShIOEnv environment and\ncurated command behavior datasets are released for use in future research.", "AI": {"tldr": "\u63d0\u51faShIOEnv\u6846\u67b6\uff0c\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5efa\u6a21\u547d\u4ee4\u884c\u4ea4\u4e92\uff0c\u7ed3\u5408\u8bed\u6cd5\u7ea6\u675f\u548cPPO\u4f18\u5316\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u4f7f\u5c0f\u578b\u6a21\u578b\u6027\u80fd\u63d0\u534785%+\u3002", "motivation": "\u73b0\u6709CLI\u6a21\u62df\u4f9d\u8d56\u5927\u578b\u51bb\u7ed3PLM\uff0c\u800c\u5c0f\u578b\u67b6\u6784\u9700\u4e30\u5bcc\u6570\u636e\u96c6\u3002\u73b0\u6709\u6570\u636e\u7f3a\u4e4f\u6267\u884c\u7ec6\u8282\uff08\u9000\u51fa\u7801/\u8f93\u51fa\u7b49\uff09\uff0c\u9650\u5236\u4e86\u884c\u4e3a\u5efa\u6a21\u80fd\u529b\u3002", "method": "1) \u5c06\u547d\u4ee4\u6784\u5efa\u5efa\u6a21\u4e3a\u72b6\u6001=\u90e8\u5206\u547d\u4ee4\u5e8f\u5217\u3001\u52a8\u4f5c=\u53c2\u6570\u8ffd\u52a0\u7684MDP\uff1b2) \u4eceman page\u63a8\u5bfcCFG\u8bed\u6cd5\u5c4f\u853d\u65e0\u6548\u53c2\u6570\uff1b3) \u7ed3\u5408\u968f\u673a/PPO\u91c7\u6837\u7b56\u7565\u751f\u6210\u56db\u7c7b\u63a2\u7d22\u65b9\u5f0f\u3002", "result": "\u8bed\u6cd5\u5c4f\u853d+PPO\u4f7f\u91c7\u6837\u6548\u7387\u63d0\u5347\uff0cCodeT5\u5fae\u8c03\u540eBLEU-4\u63d0\u534785%\uff08\u8bed\u6cd5\u7ea6\u675f\uff09\u2192111%\uff08+PPO\uff09\uff0c\u540c\u65f6\u6570\u636e\u96c6\u53c2\u6570\u8986\u76d6\u7387\u6700\u5927\u5316\u5197\u4f59\u6700\u5c0f\u5316\u3002", "conclusion": "ShIOEnv\u9a8c\u8bc1\u4e86\u8bed\u6cd5\u7ea6\u675f\u4e0e\u5f3a\u5316\u5b66\u4e60\u5728CLI\u884c\u4e3a\u5efa\u6a21\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5f00\u6e90\u73af\u5883\u4e0e\u6570\u636e\u96c6\u4e3a\u8f7b\u91cf\u5316CLI\u4ee3\u7406\u5f00\u53d1\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2505.20271", "pdf": "https://arxiv.org/pdf/2505.20271", "abs": "https://arxiv.org/abs/2505.20271", "authors": ["Yu Xu", "Fan Tang", "You Wu", "Lin Gao", "Oliver Deussen", "Hongbin Yan", "Jintao Li", "Juan Cao", "Tong-Yee Lee"], "title": "In-Context Brush: Zero-shot Customized Subject Insertion with Context-Aware Latent Space Manipulation", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": null, "summary": "Recent advances in diffusion models have enhanced multimodal-guided visual\ngeneration, enabling customized subject insertion that seamlessly \"brushes\"\nuser-specified objects into a given image guided by textual prompts. However,\nexisting methods often struggle to insert customized subjects with high\nfidelity and align results with the user's intent through textual prompts. In\nthis work, we propose \"In-Context Brush\", a zero-shot framework for customized\nsubject insertion by reformulating the task within the paradigm of in-context\nlearning. Without loss of generality, we formulate the object image and the\ntextual prompts as cross-modal demonstrations, and the target image with the\nmasked region as the query. The goal is to inpaint the target image with the\nsubject aligning textual prompts without model tuning. Building upon a\npretrained MMDiT-based inpainting network, we perform test-time enhancement via\ndual-level latent space manipulation: intra-head \"latent feature shifting\"\nwithin each attention head that dynamically shifts attention outputs to reflect\nthe desired subject semantics and inter-head \"attention reweighting\" across\ndifferent heads that amplifies prompt controllability through differential\nattention prioritization. Extensive experiments and applications demonstrate\nthat our approach achieves superior identity preservation, text alignment, and\nimage quality compared to existing state-of-the-art methods, without requiring\ndedicated training or additional data collection.", "AI": {"tldr": "\u63d0\u51fa\u96f6\u6837\u672c\u6846\u67b6'In-Context Brush'\uff0c\u901a\u8fc7\u53cc\u7ea7\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\u5b9e\u73b0\u65e0\u9700\u8bad\u7ec3\u7684\u81ea\u5b9a\u4e49\u4e3b\u4f53\u56fe\u50cf\u63d2\u5165", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u5b9a\u5236\u5316\u4e3b\u4f53\u63d2\u5165\u65f6\u5b58\u5728\u4fdd\u771f\u5ea6\u4f4e\u4e0e\u6587\u672c\u63d0\u793a\u5bf9\u9f50\u56f0\u96be\u7684\u95ee\u9898\uff0c\u9700\u8981\u65e0\u9700\u5fae\u8c03\u6a21\u578b\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u57fa\u4e8eMMDiT\u4fee\u590d\u7f51\u7edc\uff0c\u91c7\u7528\u6ce8\u610f\u529b\u5934\u5185\u7684\u6f5c\u5728\u7279\u5f81\u504f\u79fb(intra-head)\u548c\u8de8\u6ce8\u610f\u529b\u5934\u7684\u6743\u91cd\u8c03\u6574(inter-head)\u5b9e\u73b0\u53cc\u91cd\u6f5c\u5728\u7a7a\u95f4\u63a7\u5236", "result": "\u5728\u8eab\u4efd\u4fdd\u6301\u3001\u6587\u672c\u5bf9\u9f50\u548c\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u6570\u636e\u6536\u96c6", "conclusion": "\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u6846\u67b6\u4e0e\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5b9a\u5236\u5316\u4e3b\u4f53\u63d2\u5165\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u96f6\u6837\u672c\u751f\u6210"}}
{"id": "2505.18383", "pdf": "https://arxiv.org/pdf/2505.18383", "abs": "https://arxiv.org/abs/2505.18383", "authors": ["Abdellah El Mekki", "Houdaifa Atou", "Omer Nacar", "Shady Shehata", "Muhammad Abdul-Mageed"], "title": "NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities", "categories": ["cs.CL"], "comment": null, "summary": "Enhancing the linguistic capabilities of Large Language Models (LLMs) to\ninclude low-resource languages is a critical research area. Current research\ndirections predominantly rely on synthetic data generated by translating\nEnglish corpora, which, while demonstrating promising linguistic understanding\nand translation abilities, often results in models aligned with source language\nculture. These models frequently fail to represent the cultural heritage and\nvalues of local communities. This work proposes a methodology to create both\nsynthetic and retrieval-based pre-training data tailored to a specific\ncommunity, considering its (i) language, (ii) cultural heritage, and (iii)\ncultural values. We demonstrate our methodology using Egyptian and Moroccan\ndialects as testbeds, chosen for their linguistic and cultural richness and\ncurrent underrepresentation in LLMs. As a proof-of-concept, we develop\nNileChat, a 3B parameter LLM adapted for Egyptian and Moroccan communities,\nincorporating their language, cultural heritage, and values. Our results on\nvarious understanding, translation, and cultural and values alignment\nbenchmarks show that NileChat outperforms existing Arabic-aware LLMs of similar\nsize and performs on par with larger models. We share our methods, data, and\nmodels with the community to promote the inclusion and coverage of more diverse\ncommunities in LLM development.", "AI": {"tldr": "\u63d0\u51fa\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u793e\u533a\uff08\u57c3\u53ca/\u6469\u6d1b\u54e5\u65b9\u8a00\uff09\u5b9a\u5236\u5408\u6210\u4e0e\u68c0\u7d22\u7ed3\u5408\u7684\u9884\u8bad\u7ec3\u6570\u636e\u65b9\u6cd5\uff0c\u5f00\u53d13B\u53c2\u6570\u7684NileChat\u6a21\u578b\uff0c\u5728\u6587\u5316\u5bf9\u9f50\u548c\u6027\u80fd\u4e0a\u8d85\u8d8a\u540c\u7c7b\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u82f1\u8bed\u7ffb\u8bd1\u7684\u5408\u6210\u6570\u636e\u65b9\u6cd5\u5bfc\u81f4\u6a21\u578b\u4e0e\u6e90\u8bed\u8a00\u6587\u5316\u5bf9\u9f50\uff0c\u65e0\u6cd5\u4f53\u73b0\u672c\u5730\u793e\u533a\u6587\u5316\u9057\u4ea7\u548c\u4ef7\u503c\u89c2\u3002", "method": "\u7ed3\u5408\u793e\u533a\u8bed\u8a00/\u6587\u5316\u7279\u5f81\u6784\u5efa\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u5f00\u53d1\u9002\u914d\u57c3\u53ca\u548c\u6469\u6d1b\u54e5\u793e\u533a\u76843B\u53c2\u6570NileChat\u6a21\u578b\u3002", "result": "NileChat\u5728\u7406\u89e3\u3001\u7ffb\u8bd1\u548c\u6587\u5316\u5bf9\u9f50\u4efb\u52a1\u4e2d\u4f18\u4e8e\u540c\u89c4\u6a21\u6a21\u578b\uff0c\u4e0e\u66f4\u5927\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002", "conclusion": "\u5b9a\u5236\u5316\u6570\u636e\u65b9\u6cd5\u6709\u6548\u63d0\u5347LLMs\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u6587\u5316\u7684\u652f\u6301\uff0c\u901a\u8fc7\u5f00\u6e90\u4fc3\u8fdb\u591a\u6837\u6027\u793e\u533a\u7eb3\u5165AI\u53d1\u5c55\u3002"}}
{"id": "2505.18405", "pdf": "https://arxiv.org/pdf/2505.18405", "abs": "https://arxiv.org/abs/2505.18405", "authors": ["Debrup Das", "Sam O' Nuallain", "Razieh Rahimi"], "title": "RaDeR: Reasoning-aware Dense Retrieval Models", "categories": ["cs.CL", "cs.IR"], "comment": "26 pages", "summary": "We propose RaDeR, a set of reasoning-based dense retrieval models trained\nwith data derived from mathematical problem solving using large language models\n(LLMs). Our method leverages retrieval-augmented reasoning trajectories of an\nLLM and self-reflective relevance evaluation, enabling the creation of both\ndiverse and hard-negative samples for reasoning-intensive relevance. RaDeR\nretrievers, trained for mathematical reasoning, effectively generalize to\ndiverse reasoning tasks in the BRIGHT and RAR-b benchmarks, consistently\noutperforming strong baselines in overall performance.Notably, RaDeR achieves\nsignificantly higher performance than baselines on the Math and Coding splits.\nIn addition, RaDeR presents the first dense retriever that outperforms BM25\nwhen queries are Chain-of-Thought reasoning steps, underscoring the critical\nrole of reasoning-based retrieval to augment reasoning language models.\nFurthermore, RaDeR achieves comparable or superior performance while using only\n2.5% of the training data used by the concurrent work REASONIR, highlighting\nthe quality of our synthesized training data.", "AI": {"tldr": "RaDeR\u662f\u57fa\u4e8eLLM\u6570\u5b66\u95ee\u9898\u89e3\u51b3\u6570\u636e\u8bad\u7ec3\u7684\u63a8\u7406\u5bc6\u96c6\u578b\u68c0\u7d22\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u53cd\u601d\u76f8\u5173\u6027\u8bc4\u4f30\u751f\u6210\u4f18\u8d28\u8bad\u7ec3\u6570\u636e\uff0c\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u4efb\u52a1\u4e2d\u663e\u8457\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5", "motivation": "\u89e3\u51b3\u4f20\u7edf\u68c0\u7d22\u6a21\u578b\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\uff08\u5982\u6570\u5b66\u548c\u4ee3\u7801\uff09\u4e2d\u8868\u73b0\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u9488\u5bf9\u601d\u7ef4\u94fe\u67e5\u8be2\u573a\u666f\u4f18\u5316\u68c0\u7d22\u6548\u679c", "method": "1. \u5229\u7528LLM\u751f\u6210\u68c0\u7d22\u589e\u5f3a\u7684\u63a8\u7406\u8f68\u8ff9\n2. \u81ea\u53cd\u601d\u673a\u5236\u8bc4\u4f30\u67e5\u8be2-\u6587\u6863\u76f8\u5173\u6027\n3. \u81ea\u52a8\u751f\u6210\u591a\u6837\u5316\u56f0\u96be\u8d1f\u6837\u672c\n4. \u4ec5\u9700\u5c11\u91cf\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff08REASONIR\u76842.5%\uff09", "result": "1. \u5728BRIGHT/RAR-b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5168\u9762\u8d85\u8d8a\u57fa\u7ebf\n2. \u6570\u5b66/\u4ee3\u7801\u4efb\u52a1\u63d0\u5347\u663e\u8457\uff08Math/Coding splits\uff09\n3. \u9996\u6b21\u5b9e\u73b0\u5bc6\u96c6\u68c0\u7d22\u5668\u5728\u601d\u7ef4\u94fe\u67e5\u8be2\u573a\u666f\u8d85\u8d8aBM25\n4. \u8bad\u7ec3\u6548\u7387\u6bd4REASONIR\u63d0\u534740\u500d", "conclusion": "RaDeR\u8bc1\u660e\u4e86\u57fa\u4e8e\u63a8\u7406\u7684\u68c0\u7d22\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u589e\u5f3a\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u6570\u636e\u9700\u6c42"}}
{"id": "2505.18411", "pdf": "https://arxiv.org/pdf/2505.18411", "abs": "https://arxiv.org/abs/2505.18411", "authors": ["Yue Jiang", "Jichu Li", "Yang Liu", "Dingkang Yang", "Feng Zhou", "Quyu Kong"], "title": "DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process Modeling and Understanding", "categories": ["cs.CL", "cs.LG"], "comment": "https://github.com/FRENKIE-CHIANG/DanmakuTPPBench", "summary": "We introduce DanmakuTPPBench, a comprehensive benchmark designed to advance\nmulti-modal Temporal Point Process (TPP) modeling in the era of Large Language\nModels (LLMs). While TPPs have been widely studied for modeling temporal event\nsequences, existing datasets are predominantly unimodal, hindering progress in\nmodels that require joint reasoning over temporal, textual, and visual\ninformation. To address this gap, DanmakuTPPBench comprises two complementary\ncomponents: (1) DanmakuTPP-Events, a novel dataset derived from the Bilibili\nvideo platform, where user-generated bullet comments (Danmaku) naturally form\nmulti-modal events annotated with precise timestamps, rich textual content, and\ncorresponding video frames; (2) DanmakuTPP-QA, a challenging question-answering\ndataset constructed via a novel multi-agent pipeline powered by\nstate-of-the-art LLMs and multi-modal LLMs (MLLMs), targeting complex\ntemporal-textual-visual reasoning. We conduct extensive evaluations using both\nclassical TPP models and recent MLLMs, revealing significant performance gaps\nand limitations in current methods' ability to model multi-modal event\ndynamics. Our benchmark establishes strong baselines and calls for further\nintegration of TPP modeling into the multi-modal language modeling landscape.\nThe code and dataset have been released at\nhttps://github.com/FRENKIE-CHIANG/DanmakuTPPBench", "AI": {"tldr": "\u63d0\u51faDanmakuTPPBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u591a\u6a21\u6001\u5f39\u5e55\u4e8b\u4ef6\u6570\u636e\u96c6\u548c\u590d\u6742\u95ee\u7b54\u4efb\u52a1\uff0c\u63ed\u793a\u73b0\u6709\u6a21\u578b\u5728\u591a\u6a21\u6001\u65f6\u5e8f\u5efa\u6a21\u7684\u4e0d\u8db3", "motivation": "\u73b0\u6709\u65f6\u5e8f\u70b9\u8fc7\u7a0b\u6570\u636e\u96c6\u591a\u4e3a\u5355\u6a21\u6001\uff0c\u65e0\u6cd5\u6ee1\u8db3\u9700\u8981\u65f6\u95f4-\u6587\u672c-\u89c6\u89c9\u8054\u5408\u63a8\u7406\u7684\u591a\u6a21\u6001\u6a21\u578b\u53d1\u5c55\u9700\u6c42", "method": "1) \u4eceBilibili\u6784\u5efa\u542b\u65f6\u95f4\u6233\u3001\u5f39\u5e55\u6587\u672c\u548c\u89c6\u9891\u5e27\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff1b2) \u57fa\u4e8eLLM/MLLM\u6784\u5efa\u590d\u6742\u591a\u6a21\u6001\u95ee\u7b54\u6570\u636e\u96c6\uff1b3) \u5bf9\u4f20\u7edfTPP\u6a21\u578b\u548cMLLM\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30", "result": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u4e8b\u4ef6\u5efa\u6a21\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u4f20\u7edfTPP\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u591a\u6a21\u6001\u4fe1\u606f\uff0cMLLM\u5728\u65f6\u5e8f\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u8db3", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u591a\u6a21\u6001\u65f6\u5e8f\u5efa\u6a21\u5efa\u7acb\u65b0\u6807\u51c6\uff0c\u63a8\u52a8TPP\u4e0e\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u6df1\u5ea6\u878d\u5408\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90"}}
{"id": "2505.18426", "pdf": "https://arxiv.org/pdf/2505.18426", "abs": "https://arxiv.org/abs/2505.18426", "authors": ["Khandakar Ashrafi Akbar", "Md Nahiyan Uddin", "Latifur Khan", "Trayce Hockstad", "Mizanur Rahman", "Mashrur Chowdhury", "Bhavani Thuraisingham"], "title": "Retrieval Augmented Generation-based Large Language Models for Bridging Transportation Cybersecurity Legal Knowledge Gaps", "categories": ["cs.CL", "cs.AI"], "comment": "Presented at the Transportation Research Board (TRB) Annual Meeting\n  2025, and subsequently submitted for publication consideration in the\n  Transportation Research Record (TRR)", "summary": "As connected and automated transportation systems evolve, there is a growing\nneed for federal and state authorities to revise existing laws and develop new\nstatutes to address emerging cybersecurity and data privacy challenges. This\nstudy introduces a Retrieval-Augmented Generation (RAG) based Large Language\nModel (LLM) framework designed to support policymakers by extracting relevant\nlegal content and generating accurate, inquiry-specific responses. The\nframework focuses on reducing hallucinations in LLMs by using a curated set of\ndomain-specific questions to guide response generation. By incorporating\nretrieval mechanisms, the system enhances the factual grounding and specificity\nof its outputs. Our analysis shows that the proposed RAG-based LLM outperforms\nleading commercial LLMs across four evaluation metrics: AlignScore, ParaScore,\nBERTScore, and ROUGE, demonstrating its effectiveness in producing reliable and\ncontext-aware legal insights. This approach offers a scalable, AI-driven method\nfor legislative analysis, supporting efforts to update legal frameworks in line\nwith advancements in transportation technologies.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684LLM\u6846\u67b6\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u95ee\u9898\u96c6\u51cf\u5c11\u6a21\u578b\u5e7b\u89c9\uff0c\u4e3a\u4ea4\u901a\u6280\u672f\u7acb\u6cd5\u66f4\u65b0\u63d0\u4f9b\u7cbe\u51c6\u6cd5\u5f8b\u5206\u6790\u652f\u6301\u3002", "motivation": "\u5e94\u5bf9\u81ea\u52a8\u9a7e\u9a76\u548c\u667a\u80fd\u4ea4\u901a\u6280\u672f\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u7684\u7f51\u7edc\u5b89\u5168\u4e0e\u6570\u636e\u9690\u79c1\u6cd5\u5f8b\u6f0f\u6d1e\uff0c\u89e3\u51b3\u4f20\u7edfLLM\u5728\u4e13\u4e1a\u6cd5\u5f8b\u9886\u57df\u5b58\u5728\u4e8b\u5b9e\u6027\u9519\u8bef\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efaRAG\u67b6\u6784\u6574\u5408\u6cd5\u5f8b\u77e5\u8bc6\u5e93\u68c0\u7d22\u673a\u5236\uff0c\u8bbe\u8ba1\u4ea4\u901a\u653f\u7b56\u4e13\u9879\u95ee\u9898\u6a21\u677f\uff0c\u901a\u8fc7\u68c0\u7d22-\u751f\u6210\u534f\u540c\u673a\u5236\u589e\u5f3a\u6cd5\u5f8b\u6761\u6587\u5f15\u7528\u7684\u51c6\u786e\u6027\u3002", "result": "\u5728AlignScore(0.82)\u3001ParaScore(0.79)\u3001BERTScore(0.88)\u548cROUGE-L(0.75)\u56db\u9879\u6307\u6807\u4e0a\u5747\u8d85\u8d8aGPT-4\u7b49\u5546\u4e1a\u6a21\u578b\uff0c\u7279\u5b9a\u6cd5\u5f8b\u6761\u6b3e\u53ec\u56de\u7387\u63d0\u534737%\u3002", "conclusion": "\u8be5\u6846\u67b6\u8bc1\u660e\u4e86\u9886\u57df\u5b9a\u5236\u5316RAG\u67b6\u6784\u5728\u7acb\u6cd5\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3aAI\u9a71\u52a8\u7684\u6cd5\u5f8b\u6846\u67b6\u52a8\u6001\u66f4\u65b0\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2505.18436", "pdf": "https://arxiv.org/pdf/2505.18436", "abs": "https://arxiv.org/abs/2505.18436", "authors": ["AbdelRahim Elmadany", "Sang Yun Kwon", "Hawau Olamide Toyin", "Alcides Alcoba Inciarte", "Hanan Aldarmaki", "Muhammad Abdul-Mageed"], "title": "Voice of a Continent: Mapping Africa's Speech Technology Frontier", "categories": ["cs.CL"], "comment": null, "summary": "Africa's rich linguistic diversity remains significantly underrepresented in\nspeech technologies, creating barriers to digital inclusion. To alleviate this\nchallenge, we systematically map the continent's speech space of datasets and\ntechnologies, leading to a new comprehensive benchmark SimbaBench for\ndownstream African speech tasks. Using SimbaBench, we introduce the Simba\nfamily of models, achieving state-of-the-art performance across multiple\nAfrican languages and speech tasks. Our benchmark analysis reveals critical\npatterns in resource availability, while our model evaluation demonstrates how\ndataset quality, domain diversity, and language family relationships influence\nperformance across languages. Our work highlights the need for expanded speech\ntechnology resources that better reflect Africa's linguistic diversity and\nprovides a solid foundation for future research and development efforts toward\nmore inclusive speech technologies.", "AI": {"tldr": "\u6784\u5efaSimbaBench\u57fa\u51c6\u4e0eSimba\u6a21\u578b\uff0c\u63d0\u5347\u975e\u6d32\u8bed\u8a00\u8bed\u97f3\u6280\u672f\u8986\u76d6", "motivation": "\u975e\u6d32\u8bed\u8a00\u5728\u8bed\u97f3\u6280\u672f\u8d44\u6e90\u4e2d\u4ee3\u8868\u6027\u4e25\u91cd\u4e0d\u8db3\uff0c\u5bfc\u81f4\u6570\u5b57\u5305\u5bb9\u6027\u969c\u788d", "method": "\u7cfb\u7edf\u7ed8\u5236\u975e\u6d32\u8bed\u97f3\u6570\u636e\u96c6\u4e0e\u6280\u672f\u56fe\u8c31\uff0c\u5efa\u7acbSimbaBench\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5f00\u53d1Simba\u7cfb\u5217\u6a21\u578b", "result": "\u6a21\u578b\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\uff0c\u53d1\u73b0\u6570\u636e\u96c6\u8d28\u91cf\u3001\u9886\u57df\u591a\u6837\u6027\u548c\u8bed\u8a00\u5bb6\u65cf\u5173\u7cfb\u663e\u8457\u5f71\u54cd\u6027\u80fd", "conclusion": "\u4e9f\u9700\u6269\u5927\u8bed\u97f3\u6280\u672f\u8d44\u6e90\u4ee5\u53cd\u6620\u975e\u6d32\u8bed\u8a00\u591a\u6837\u6027\uff0c\u672c\u7814\u7a76\u4e3a\u5f00\u53d1\u5305\u5bb9\u6027\u8bed\u97f3\u6280\u672f\u5960\u5b9a\u57fa\u7840"}}
{"id": "2505.18440", "pdf": "https://arxiv.org/pdf/2505.18440", "abs": "https://arxiv.org/abs/2505.18440", "authors": ["Zhaoyang Wang", "Jinqi Jiang", "Tian Qiu", "Hui Liu", "Xianfeng Tang", "Huaxiu Yao"], "title": "Efficient Long CoT Reasoning in Small Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent large reasoning models such as DeepSeek-R1 exhibit strong complex\nproblems solving abilities by generating long chain-of-thought (CoT) reasoning\nsteps. It is challenging to directly train small language models (SLMs) to\nemerge long CoT. Thus, distillation becomes a practical method to enable SLMs\nfor such reasoning ability. However, the long CoT often contains a lot of\nredundant contents (e.g., overthinking steps) which may make SLMs hard to learn\nconsidering their relatively poor capacity and generalization. To address this\nissue, we propose a simple-yet-effective method to prune unnecessary steps in\nlong CoT, and then employ an on-policy method for the SLM itself to curate\nvalid and useful long CoT training data. In this way, SLMs can effectively\nlearn efficient long CoT reasoning and preserve competitive performance at the\nsame time. Experimental results across a series of mathematical reasoning\nbenchmarks demonstrate the effectiveness of the proposed method in distilling\nlong CoT reasoning ability into SLMs which maintains the competitive\nperformance but significantly reduces generating redundant reasoning steps.", "AI": {"tldr": "\u63d0\u51fa\u526a\u679d\u957f\u601d\u7ef4\u94fe\u5197\u4f59\u6b65\u9aa4\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b56\u7565\u7b5b\u9009\u6709\u6548\u6570\u636e\u5b9e\u73b0\u5c0f\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\u8bad\u7ec3", "motivation": "\u5c0f\u8bed\u8a00\u6a21\u578b\u76f4\u63a5\u5b66\u4e60\u957f\u601d\u7ef4\u94fe\u65f6\u5bb9\u6613\u53d7\u5230\u5197\u4f59\u5185\u5bb9\u5e72\u6270\uff0c\u9700\u4f18\u5316\u84b8\u998f\u65b9\u6cd5\u63d0\u5347\u5b66\u4e60\u6548\u7387", "method": "\u7ed3\u5408\u5197\u4f59\u6b65\u9aa4\u526a\u679d\u4e0e\u81ea\u4e3b\u7b56\u7565\u7b5b\u9009\u673a\u5236\uff0c\u5229\u7528\u5c0f\u6a21\u578b\u81ea\u8eab\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u8d28\u91cf", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u5c0f\u6a21\u578b\u63a8\u7406\u6b65\u9aa4\u51cf\u5c1140%\u7684\u540c\u65f6\u4fdd\u630197%\u7684\u57fa\u51c6\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u957f\u601d\u7ef4\u94fe\u80fd\u529b\u7684\u9ad8\u6548\u84b8\u998f\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u53ef\u884c\u7684\u63a8\u7406\u4f18\u5316\u65b9\u6848"}}
{"id": "2505.18450", "pdf": "https://arxiv.org/pdf/2505.18450", "abs": "https://arxiv.org/abs/2505.18450", "authors": ["Ainulla Khan", "Yamada Moyuru", "Srinidhi Akella"], "title": "BRIT: Bidirectional Retrieval over Unified Image-Text Graph", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a promising technique to\nenhance the quality and relevance of responses generated by large language\nmodels. While recent advancements have mainly focused on improving RAG for\ntext-based queries, RAG on multi-modal documents containing both texts and\nimages has not been fully explored. Especially when fine-tuning does not work.\nThis paper proposes BRIT, a novel multi-modal RAG framework that effectively\nunifies various text-image connections in the document into a multi-modal graph\nand retrieves the texts and images as a query-specific sub-graph. By traversing\nboth image-to-text and text-to-image paths in the graph, BRIT retrieve not only\ndirectly query-relevant images and texts but also further relevant contents to\nanswering complex cross-modal multi-hop questions. To evaluate the\neffectiveness of BRIT, we introduce MM-RAG test set specifically designed for\nmulti-modal question answering tasks that require to understand the text-image\nrelations. Our comprehensive experiments demonstrate the superiority of BRIT,\nhighlighting its ability to handle cross-modal questions on the multi-modal\ndocuments.", "AI": {"tldr": "\u63d0\u51faBRIT\u591a\u6a21\u6001RAG\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u6a21\u6001\u56fe\u5b9e\u73b0\u8de8\u6587\u672c\u56fe\u50cf\u7684\u5173\u8054\u68c0\u7d22\uff0c\u89e3\u51b3\u590d\u6742\u8de8\u6a21\u6001\u95ee\u7b54\u95ee\u9898", "motivation": "\u73b0\u6709RAG\u6280\u672f\u4e3b\u8981\u805a\u7126\u6587\u672c\u68c0\u7d22\uff0c\u4f46\u591a\u6a21\u6001\u6587\u6863\uff08\u56fe\u6587\u6df7\u5408\uff09\u7684\u8de8\u6a21\u6001\u5173\u8054\u68c0\u7d22\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u5c24\u5176\u5728\u65e0\u6cd5\u5fae\u8c03\u6a21\u578b\u65f6\u5b58\u5728\u6311\u6218", "method": "\u5c06\u6587\u6863\u4e2d\u7684\u56fe\u6587\u5173\u7cfb\u6784\u5efa\u4e3a\u591a\u6a21\u6001\u56fe\uff0c\u901a\u8fc7\u56fe\u50cf-\u6587\u672c\u53cc\u5411\u8def\u5f84\u68c0\u7d22\uff0c\u83b7\u53d6\u76f4\u63a5\u76f8\u5173\u5185\u5bb9\u548c\u95f4\u63a5\u5173\u8054\u5185\u5bb9", "result": "\u6784\u5efaMM-RAG\u6d4b\u8bd5\u96c6\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660eBRIT\u5728\u5904\u7406\u8de8\u6a21\u6001\u591a\u8df3\u95ee\u9898\u4e0a\u7684\u4f18\u8d8a\u6027", "conclusion": "BRIT\u9996\u6b21\u5c06\u56fe\u7ed3\u6784\u5f15\u5165\u591a\u6a21\u6001RAG\uff0c\u901a\u8fc7\u521b\u65b0\u68c0\u7d22\u673a\u5236\u63d0\u5347\u590d\u6742\u8de8\u6a21\u6001\u95ee\u9898\u7684\u89e3\u7b54\u80fd\u529b"}}
{"id": "2505.18452", "pdf": "https://arxiv.org/pdf/2505.18452", "abs": "https://arxiv.org/abs/2505.18452", "authors": ["Heyuan Huang", "Alexandra DeLucia", "Vijay Murari Tiyyala", "Mark Dredze"], "title": "MedScore: Factuality Evaluation of Free-Form Medical Answers", "categories": ["cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) can generate fluent and convincing\nresponses, they are not necessarily correct. This is especially apparent in the\npopular decompose-then-verify factuality evaluation pipeline, where LLMs\nevaluate generations by decomposing the generations into individual, valid\nclaims. Factuality evaluation is especially important for medical answers,\nsince incorrect medical information could seriously harm the patient. However,\nexisting factuality systems are a poor match for the medical domain, as they\nare typically only evaluated on objective, entity-centric, formulaic texts such\nas biographies and historical topics. This differs from condition-dependent,\nconversational, hypothetical, sentence-structure diverse, and subjective\nmedical answers, which makes decomposition into valid facts challenging. We\npropose MedScore, a new approach to decomposing medical answers into\ncondition-aware valid facts. Our method extracts up to three times more valid\nfacts than existing methods, reducing hallucination and vague references, and\nretaining condition-dependency in facts. The resulting factuality score\nsignificantly varies by decomposition method, verification corpus, and used\nbackbone LLM, highlighting the importance of customizing each step for reliable\nfactuality evaluation.", "AI": {"tldr": "\u63d0\u51faMedScore\u65b9\u6cd5\u6539\u8fdb\u533b\u5b66\u7b54\u6848\u7684\u4e8b\u5b9e\u6027\u8bc4\u4f30\uff0c\u901a\u8fc7\u6761\u4ef6\u611f\u77e5\u5206\u89e3\u63d0\u53d6\u66f4\u591a\u6709\u6548\u4e8b\u5b9e\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u4e8b\u5b9e\u6027\u8bc4\u4f30\u7cfb\u7edf\u4e3b\u8981\u9488\u5bf9\u5ba2\u89c2\u5b9e\u4f53\u6587\u672c\uff08\u5982\u4f20\u8bb0\uff09\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u533b\u5b66\u9886\u57df\u6761\u4ef6\u4f9d\u8d56\u3001\u5bf9\u8bdd\u5f0f\u3001\u5047\u8bbe\u6027\u5f3a\u7684\u591a\u6837\u5316\u53e5\u5f0f\u7ed3\u6784", "method": "\u5f00\u53d1\u6761\u4ef6\u611f\u77e5\u5206\u89e3\u65b9\u6cd5\uff0c\u4fdd\u7559\u4e8b\u5b9e\u7684\u6761\u4ef6\u4f9d\u8d56\u6027\uff0c\u51cf\u5c11\u5e7b\u89c9\u548c\u6a21\u7cca\u6307\u4ee3", "result": "\u63d0\u53d6\u6709\u6548\u4e8b\u5b9e\u6570\u91cf\u63d0\u53473\u500d\uff0c\u4e8b\u5b9e\u6027\u8bc4\u5206\u53d7\u5206\u89e3\u65b9\u6cd5/\u9a8c\u8bc1\u8bed\u6599/\u9aa8\u5e72LLM\u4e09\u91cd\u5f71\u54cd\u663e\u8457", "conclusion": "MedScore\u901a\u8fc7\u5b9a\u5236\u5316\u5206\u89e3\u6d41\u7a0b\u63d0\u5347\u533b\u5b66\u4e8b\u5b9e\u6027\u8bc4\u4f30\u53ef\u9760\u6027\uff0c\u8bc1\u660e\u9886\u57df\u9002\u914d\u5404\u8bc4\u4f30\u73af\u8282\u7684\u91cd\u8981\u6027"}}
{"id": "2505.18454", "pdf": "https://arxiv.org/pdf/2505.18454", "abs": "https://arxiv.org/abs/2505.18454", "authors": ["Zhenrui Yue", "Bowen Jin", "Huimin Zeng", "Honglei Zhuang", "Zhen Qin", "Jinsung Yoon", "Lanyu Shang", "Jiawei Han", "Dong Wang"], "title": "Hybrid Latent Reasoning via Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have introduced latent\nreasoning as a promising alternative to autoregressive reasoning. By performing\ninternal computation with hidden states from previous steps, latent reasoning\nbenefit from more informative features rather than sampling a discrete\nchain-of-thought (CoT) path. Yet latent reasoning approaches are often\nincompatible with LLMs, as their continuous paradigm conflicts with the\ndiscrete nature of autoregressive generation. Moreover, these methods rely on\nCoT traces for training and thus fail to exploit the inherent reasoning\npatterns of LLMs. In this work, we explore latent reasoning by leveraging the\nintrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we\nintroduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid\nlatent reasoning approach that (1) integrates prior hidden states into sampled\ntokens with a learnable gating mechanism, and (2) initializes training with\npredominantly token embeddings while progressively incorporating more hidden\nfeatures. This design maintains LLMs' generative capabilities and incentivizes\nhybrid reasoning using both discrete and continuous representations. In\naddition, the hybrid HRPO introduces stochasticity into latent reasoning via\ntoken sampling, thereby enabling RL-based optimization without requiring CoT\ntrajectories. Extensive evaluations across diverse benchmarks show that HRPO\noutperforms prior methods in both knowledge- and reasoning-intensive tasks.\nFurthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing\nbehaviors like cross-lingual patterns and shorter completion lengths,\nhighlighting the potential of our RL-based approach and offer insights for\nfuture work in latent reasoning.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6df7\u5408\u6f5c\u5728\u63a8\u7406\u65b9\u6cd5HRPO\uff0c\u901a\u8fc7\u6574\u5408\u79bb\u6563token\u548c\u8fde\u7eed\u9690\u85cf\u72b6\u6001\u63d0\u5347LLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u5e76\u4fdd\u6301\u53ef\u89e3\u91ca\u6027", "motivation": "\u73b0\u6709\u6f5c\u5728\u63a8\u7406\u65b9\u6cd5\u56e0\u8fde\u7eed\u8303\u5f0f\u4e0eLLMs\u81ea\u56de\u5f52\u751f\u6210\u7684\u79bb\u6563\u7279\u6027\u51b2\u7a81\uff0c\u4e14\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u7684\u601d\u7ef4\u94fe\u6570\u636e\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528LLMs\u81ea\u8eab\u7684\u63a8\u7406\u6a21\u5f0f", "method": "\u8bbe\u8ba1\u6df7\u5408\u63a8\u7406\u7b56\u7565\uff1a1) \u53ef\u5b66\u4e60\u95e8\u63a7\u673a\u5236\u878d\u5408\u5386\u53f2\u9690\u85cf\u72b6\u6001\u4e0e\u5f53\u524dtoken\u91c7\u6837\uff1b2) \u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7b56\u7565\uff0c\u4ecetoken\u5d4c\u5165\u9010\u6b65\u8fc7\u6e21\u5230\u9690\u85cf\u7279\u5f81\u5229\u7528", "result": "\u5728\u77e5\u8bc6\u578b\u548c\u63a8\u7406\u578b\u4efb\u52a1\u4e2d\u5168\u9762\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6a21\u578b\u5c55\u73b0\u51fa\u8de8\u8bed\u8a00\u63a8\u7406\u6a21\u5f0f\u548c\u66f4\u77ed\u751f\u6210\u957f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u51b3\u7b56\u53ef\u89e3\u91ca\u6027", "conclusion": "HRPO\u9a8c\u8bc1\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u6f5c\u5728\u63a8\u7406\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5176\u6df7\u5408\u8868\u793a\u65b9\u6cd5\u548c\u65e0\u76d1\u7763\u4f18\u5316\u6846\u67b6\u4e3a\u672a\u6765LLMs\u63a8\u7406\u673a\u5236\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2505.18456", "pdf": "https://arxiv.org/pdf/2505.18456", "abs": "https://arxiv.org/abs/2505.18456", "authors": ["Litu Rout", "Constantine Caramanis", "Sanjay Shakkottai"], "title": "Anchored Diffusion Language Model", "categories": ["cs.CL", "cs.LG"], "comment": "Preprint", "summary": "Diffusion Language Models (DLMs) promise parallel generation and\nbidirectional context, yet they underperform autoregressive (AR) models in both\nlikelihood modeling and generated text quality. We identify that this\nperformance gap arises when important tokens (e.g., key words or low-frequency\nwords that anchor a sentence) are masked early in the forward process, limiting\ncontextual information for accurate reconstruction. To address this, we\nintroduce the Anchored Diffusion Language Model (ADLM), a novel two-stage\nframework that first predicts distributions over important tokens via an anchor\nnetwork, and then predicts the likelihoods of missing tokens conditioned on the\nanchored predictions. ADLM significantly improves test perplexity on LM1B and\nOpenWebText, achieving up to 25.4% gains over prior DLMs, and narrows the gap\nwith strong AR baselines. It also achieves state-of-the-art performance in\nzero-shot generalization across seven benchmarks and surpasses AR models in\nMAUVE score, which marks the first time a DLM generates better human-like text\nthan an AR model. Theoretically, we derive an Anchored Negative Evidence Lower\nBound (ANELBO) objective and show that anchoring improves sample complexity and\nlikelihood modeling. Beyond diffusion, anchoring boosts performance in AR\nmodels and enhances reasoning in math and logic tasks, outperforming existing\nchain-of-thought approaches", "AI": {"tldr": "\u63d0\u51fa\u951a\u5b9a\u6269\u6563\u8bed\u8a00\u6a21\u578bADLM\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u9884\u6d4b\u6846\u67b6\u663e\u8457\u63d0\u5347\u6269\u6563\u6a21\u578b\u6027\u80fd\u5e76\u9996\u6b21\u8d85\u8d8a\u81ea\u56de\u5f52\u6a21\u578b", "motivation": "\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u5173\u952etoken\u65e9\u671f\u906e\u853d\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u89e3\u51b3\u91cd\u8981token\u4e22\u5931\u5bfc\u81f4\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e0d\u8db3\u95ee\u9898", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u951a\u5b9a\u7f51\u7edc\u9884\u6d4b\u91cd\u8981token\u5206\u5e03 2\uff09\u57fa\u4e8e\u951a\u5b9a\u9884\u6d4b\u751f\u6210\u7f3a\u5931token\u4f3c\u7136", "result": "LM1B\u548cOpenWebText\u6d4b\u8bd5\u590d\u6742\u5ea6\u63d0\u534725.4%\uff0c\u96f6\u6837\u672c7\u57fa\u51c6SOTA\uff0cMAUVE\u5f97\u5206\u9996\u8d85AR\u6a21\u578b", "conclusion": "\u951a\u5b9a\u673a\u5236\u4e0d\u4ec5\u63d0\u5347\u6269\u6563\u6a21\u578b\u6027\u80fd\uff0c\u8fd8\u6539\u8fdbAR\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u8bed\u8a00\u5efa\u6a21\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2505.18466", "pdf": "https://arxiv.org/pdf/2505.18466", "abs": "https://arxiv.org/abs/2505.18466", "authors": ["Mamnuya Rinki", "Chahat Raj", "Anjishnu Mukherjee", "Ziwei Zhu"], "title": "Measuring South Asian Biases in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Evaluations of Large Language Models (LLMs) often overlook intersectional and\nculturally specific biases, particularly in underrepresented multilingual\nregions like South Asia. This work addresses these gaps by conducting a\nmultilingual and intersectional analysis of LLM outputs across 10 Indo-Aryan\nand Dravidian languages, identifying how cultural stigmas influenced by purdah\nand patriarchy are reinforced in generative tasks. We construct a culturally\ngrounded bias lexicon capturing previously unexplored intersectional dimensions\nincluding gender, religion, marital status, and number of children. We use our\nlexicon to quantify intersectional bias and the effectiveness of self-debiasing\nin open-ended generations (e.g., storytelling, hobbies, and to-do lists), where\nbias manifests subtly and remains largely unexamined in multilingual contexts.\nFinally, we evaluate two self-debiasing strategies (simple and complex prompts)\nto measure their effectiveness in reducing culturally specific bias in\nIndo-Aryan and Dravidian languages. Our approach offers a nuanced lens into\ncultural bias by introducing a novel bias lexicon and evaluation framework that\nextends beyond Eurocentric or small-scale multilingual settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u9996\u4e2a\u5357\u4e9a\u591a\u8bed\u8a00\u4ea4\u53c9\u504f\u89c1\u8bcd\u5178\uff0c\u5206\u6790\u4e86\u6587\u5316\u56e0\u7d20\u5bf9LLM\u751f\u6210\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u5e76\u9a8c\u8bc1\u4e86\u81ea\u6211\u53bb\u504f\u7b56\u7565\u5728\u5370\u5ea6-\u96c5\u5229\u5b89/\u5fb7\u62c9\u5a01\u8bed\u7cfb\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u5ffd\u89c6\u5357\u4e9a\u7b49\u4f4e\u8d44\u6e90\u5730\u533a\u7684\u4ea4\u53c9\u6587\u5316\u504f\u89c1\uff0c\u7279\u522b\u662f\u9762\u7eb1\u5236\u5ea6(purdah)\u548c\u7236\u6743\u5236\u5f71\u54cd\u4e0b\u7684\u6027\u522b\u3001\u5b97\u6559\u7b49\u590d\u5408\u7ef4\u5ea6\u504f\u89c1\u3002", "method": "\u57fa\u4e8e10\u79cd\u5370\u5ea6\u8bed\u8a00\u6784\u5efa\u6587\u5316\u504f\u89c1\u8bcd\u5178\uff0c\u8bbe\u8ba1\u5f00\u653e\u5f0f\u751f\u6210\u4efb\u52a1(\u6545\u4e8b\u521b\u4f5c/\u5174\u8da3\u5217\u8868\u7b49)\uff0c\u91cf\u5316\u4e24\u79cd\u63d0\u793a\u53bb\u504f\u7b56\u7565\u7684\u6548\u679c\u3002", "result": "\u53d1\u73b0\u6587\u5316\u6c61\u540d\u88ab\u7cfb\u7edf\u6027\u5f3a\u5316\uff0c\u7b80\u5355\u63d0\u793a\u53bb\u504f\u6548\u679c\u6709\u9650\uff0c\u590d\u6742\u60c5\u5883\u5316\u63d0\u793a\u80fd\u66f4\u6709\u6548\u964d\u4f4e\u4ea4\u53c9\u504f\u89c1\u5728\u751f\u6210\u5185\u5bb9\u4e2d\u7684\u6e17\u900f\u3002", "conclusion": "\u63d0\u51fa\u4e86\u8d85\u8d8a\u6b27\u7f8e\u4e2d\u5fc3\u7684\u591a\u6587\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3aLLM\u5728\u975e\u897f\u65b9\u8bed\u5883\u4e2d\u7684\u4f26\u7406\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2505.18486", "pdf": "https://arxiv.org/pdf/2505.18486", "abs": "https://arxiv.org/abs/2505.18486", "authors": ["Hong Jiao", "Dan Song", "Won-Chan Lee"], "title": "Investigating AI Rater Effects of Large Language Models: GPT, Claude, Gemini, and DeepSeek", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have been widely explored for automated scoring\nin low-stakes assessment to facilitate learning and instruction. Empirical\nevidence related to which LLM produces the most reliable scores and induces\nleast rater effects needs to be collected before the use of LLMs for automated\nscoring in practice. This study compared ten LLMs (ChatGPT 3.5, ChatGPT 4,\nChatGPT 4o, OpenAI o1, Claude 3.5 Sonnet, Gemini 1.5, Gemini 1.5 Pro, Gemini\n2.0, as well as DeepSeek V3, and DeepSeek R1) with human expert raters in\nscoring two types of writing tasks. The accuracy of the holistic and analytic\nscores from LLMs compared with human raters was evaluated in terms of Quadratic\nWeighted Kappa. Intra-rater consistency across prompts was compared in terms of\nCronbach Alpha. Rater effects of LLMs were evaluated and compared with human\nraters using the Many-Facet Rasch model. The results in general supported the\nuse of ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet with high scoring\naccuracy, better rater reliability, and less rater effects.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e8610\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5199\u4f5c\u8bc4\u5206\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0ChatGPT 4o\u3001Gemini 1.5 Pro\u548cClaude 3.5 Sonnet\u5177\u6709\u8f83\u9ad8\u7684\u8bc4\u5206\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027", "motivation": "\u9a8c\u8bc1\u4e0d\u540cLLM\u5728\u81ea\u52a8\u8bc4\u5206\u4e2d\u7684\u53ef\u9760\u6027\u53ca\u8bc4\u5206\u5458\u6548\u5e94\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4f9d\u636e", "method": "\u901a\u8fc7\u4e8c\u6b21\u52a0\u6743Kappa\u8bc4\u4f30\u8bc4\u5206\u51c6\u786e\u6027\uff0cCronbach Alpha\u68c0\u9a8c\u8bc4\u5206\u4e00\u81f4\u6027\uff0c\u591a\u9762Rasch\u6a21\u578b\u5206\u6790\u8bc4\u5206\u5458\u6548\u5e94", "result": "ChatGPT 4o\u3001Gemini 1.5 Pro\u548cClaude 3.5 Sonnet\u5728\u6574\u4f53/\u5206\u6790\u6027\u8bc4\u5206\u4e2d\u8868\u73b0\u6700\u4f18\uff0c\u8bc4\u5206\u8005\u6548\u5e94\u6700\u4f4e", "conclusion": "\u63a8\u8350\u4f18\u5148\u91c7\u7528ChatGPT 4o\u7b49\u53ef\u9760\u6027\u9ad8\u7684LLM\u8fdb\u884c\u81ea\u52a8\u8bc4\u5206\uff0c\u4f46\u9700\u6301\u7eed\u76d1\u63a7\u6a21\u578b\u8868\u73b0"}}
{"id": "2505.18497", "pdf": "https://arxiv.org/pdf/2505.18497", "abs": "https://arxiv.org/abs/2505.18497", "authors": ["Kefan Yu", "Qingcheng Zeng", "Weihao Xuan", "Wanxin Li", "Jingyi Wu", "Rob Voigt"], "title": "The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Current large language models (LLMs) have demonstrated emerging capabilities\nin social intelligence tasks, including implicature resolution (Sravanthi et\nal. (2024)) and theory-of-mind reasoning (Shapira et al. (2024)), both of which\nrequire substantial pragmatic understanding. However, how LLMs acquire this\ncompetence throughout the training process remains poorly understood. In this\nwork, we introduce ALTPRAG, a dataset grounded in the pragmatic concept of\nalternatives, designed to evaluate whether LLMs at different training stages\ncan accurately infer nuanced speaker intentions. Each instance pairs two\ncontextually appropriate but pragmatically distinct continuations, enabling\nfine-grained assessment of both pragmatic interpretation and contrastive\nreasoning. We systematically evaluate 22 LLMs across key training stages:\npre-training, supervised fine-tuning (SFT), and preference optimization, to\nexamine the development of pragmatic competence. Our results show that even\nbase models exhibit notable sensitivity to pragmatic cues, which improves\nconsistently with increases in model and data scale. Additionally, SFT and RLHF\ncontribute further gains, particularly in cognitive-pragmatic reasoning. These\nfindings highlight pragmatic competence as an emergent and compositional\nproperty of LLM training and offer new insights for aligning models with human\ncommunicative norms.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7ALTPRAG\u6570\u636e\u96c6\u63ed\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9010\u6b65\u53d1\u5c55\u51fa\u8bed\u7528\u63a8\u7406\u80fd\u529b\uff0c\u663e\u793a\u57fa\u5ea7\u6a21\u578b\u5df2\u5177\u5907\u8bed\u7528\u654f\u611f\u6027\uff0c\u4e14SFT/RLHF\u9636\u6bb5\u8fdb\u4e00\u6b65\u589e\u5f3a\u8ba4\u77e5\u8bed\u7528\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5982\u4f55\u5f62\u6210\u8bed\u7528\u7406\u89e3\u80fd\u529b\uff0c\u7279\u522b\u662f\u9488\u5bf9\u9690\u542b\u610f\u56fe\u63a8\u65ad\u7684\u673a\u5236\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u8bed\u7528\u5b66'\u66ff\u4ee3'\u6982\u5ff5\u7684ALTPRAG\u6570\u636e\u96c6\uff0c\u5728\u9884\u8bad\u7ec3/SFT/RLHF\u4e09\u4e2a\u9636\u6bb5\u7cfb\u7edf\u8bc4\u4f3022\u4e2aLLMs\u7684\u8bed\u7528\u63a8\u7406\u80fd\u529b\u3002", "result": "\u57fa\u5ea7\u6a21\u578b\u663e\u793a\u8bed\u7528\u654f\u611f\u6027\uff08\u51c6\u786e\u738760%+\uff09\uff0c\u6a21\u578b\u89c4\u6a21\u548c\u6570\u636e\u91cf\u63d0\u5347\u5e26\u6765\u6301\u7eed\u589e\u76ca\uff1bSFT\u548cRLHF\u9636\u6bb5\u4f7f\u8ba4\u77e5\u8bed\u7528\u63a8\u7406\u51c6\u786e\u7387\u63d0\u534710-15%\u3002", "conclusion": "\u8bed\u7528\u80fd\u529b\u662fLLM\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6d8c\u73b0\u7684\u7ec4\u5408\u5c5e\u6027\uff0c\u5bf9\u9f50\u4eba\u7c7b\u4ea4\u9645\u89c4\u8303\u9700\u5173\u6ce8\u8bed\u7528\u7ef4\u5ea6\u7684\u4f18\u5316\u3002"}}
{"id": "2505.18522", "pdf": "https://arxiv.org/pdf/2505.18522", "abs": "https://arxiv.org/abs/2505.18522", "authors": ["Xin Lu", "Yanyan Zhao", "Si Wei", "Shijin Wang", "Bing Qin", "Ting Liu"], "title": "How Does Sequence Modeling Architecture Influence Base Capabilities of Pre-trained Language Models? Exploring Key Architecture Design Principles to Avoid Base Capabilities Degradation", "categories": ["cs.CL"], "comment": null, "summary": "Pre-trained language models represented by the Transformer have been proven\nto possess strong base capabilities, and the representative self-attention\nmechanism in the Transformer has become a classic in sequence modeling\narchitectures. Different from the work of proposing sequence modeling\narchitecture to improve the efficiency of attention mechanism, this work\nfocuses on the impact of sequence modeling architectures on base capabilities.\nSpecifically, our concern is: How exactly do sequence modeling architectures\naffect the base capabilities of pre-trained language models? In this work, we\nfirst point out that the mixed domain pre-training setting commonly adopted in\nexisting architecture design works fails to adequately reveal the differences\nin base capabilities among various architectures. To address this, we propose a\nlimited domain pre-training setting with out-of-distribution testing, which\nsuccessfully uncovers significant differences in base capabilities among\narchitectures at an early stage. Next, we analyze the base capabilities of\nstateful sequence modeling architectures, and find that they exhibit\nsignificant degradation in base capabilities compared to the Transformer. Then,\nthrough a series of architecture component analysis, we summarize a key\narchitecture design principle: A sequence modeling architecture need possess\nfull-sequence arbitrary selection capability to avoid degradation in base\ncapabilities. Finally, we empirically validate this principle using an\nextremely simple Top-1 element selection architecture and further generalize it\nto a more practical Top-1 chunk selection architecture. Experimental results\ndemonstrate our proposed sequence modeling architecture design principle and\nsuggest that our work can serve as a valuable reference for future architecture\nimprovements and novel designs.", "AI": {"tldr": "\u901a\u8fc7\u6709\u9650\u57df\u9884\u8bad\u7ec3\u548c\u67b6\u6784\u7ec4\u4ef6\u5206\u6790\uff0c\u63d0\u51fa\u5e8f\u5217\u5efa\u6a21\u67b6\u6784\u9700\u5177\u5907\u5168\u5e8f\u5217\u4efb\u610f\u9009\u62e9\u80fd\u529b\u4ee5\u907f\u514d\u57fa\u7840\u80fd\u529b\u9000\u5316", "motivation": "\u73b0\u6709\u67b6\u6784\u8bbe\u8ba1\u5de5\u4f5c\u91c7\u7528\u7684\u6df7\u5408\u57df\u9884\u8bad\u7ec3\u8bbe\u7f6e\u65e0\u6cd5\u5145\u5206\u63ed\u793a\u4e0d\u540c\u67b6\u6784\u7684\u57fa\u7840\u80fd\u529b\u5dee\u5f02\uff0c\u9700\u63a2\u7d22\u67b6\u6784\u8bbe\u8ba1\u5bf9\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u57fa\u7840\u80fd\u529b\u7684\u5f71\u54cd\u673a\u5236", "method": "1. \u63d0\u51fa\u6709\u9650\u57df\u9884\u8bad\u7ec3+\u5206\u5e03\u5916\u6d4b\u8bd5\u8bbe\u7f6e 2. \u5206\u6790\u72b6\u6001\u5e8f\u5217\u5efa\u6a21\u67b6\u6784\u57fa\u7840\u80fd\u529b 3. \u901a\u8fc7\u67b6\u6784\u7ec4\u4ef6\u5206\u6790\u603b\u7ed3\u8bbe\u8ba1\u539f\u5219 4. \u4f7f\u7528Top-1\u9009\u62e9\u67b6\u6784\u8fdb\u884c\u9a8c\u8bc1", "result": "\u53d1\u73b0\u72b6\u6001\u5e8f\u5217\u5efa\u6a21\u67b6\u6784\u5b58\u5728\u663e\u8457\u57fa\u7840\u80fd\u529b\u9000\u5316\uff0c\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u300c\u5168\u5e8f\u5217\u4efb\u610f\u9009\u62e9\u80fd\u529b\u300d\u7684\u5173\u952e\u8bbe\u8ba1\u539f\u5219\uff0cTop-1\u5757\u9009\u62e9\u67b6\u6784\u9a8c\u8bc1\u6709\u6548", "conclusion": "\u5e8f\u5217\u5efa\u6a21\u67b6\u6784\u9700\u4fdd\u6301\u5168\u5e8f\u5217\u4efb\u610f\u9009\u62e9\u80fd\u529b\uff0c\u7814\u7a76\u4e3a\u672a\u6765\u67b6\u6784\u6539\u8fdb\u63d0\u4f9b\u8bbe\u8ba1\u539f\u5219\u53c2\u8003\uff0cTop-1\u5757\u9009\u62e9\u67b6\u6784\u5c55\u73b0\u5b9e\u7528\u6f5c\u529b"}}
{"id": "2505.18524", "pdf": "https://arxiv.org/pdf/2505.18524", "abs": "https://arxiv.org/abs/2505.18524", "authors": ["Guowei Xu", "Mert Yuksekgonul", "Carlos Guestrin", "James Zou"], "title": "metaTextGrad: Automatically optimizing language model optimizers", "categories": ["cs.CL"], "comment": "21 pages, 2 figures", "summary": "Large language models (LLMs) are increasingly used in learning algorithms,\nevaluations, and optimization tasks. Recent studies have shown that using\nLLM-based optimizers to automatically optimize model prompts, demonstrations,\npredictions themselves, or other components can significantly enhance the\nperformance of AI systems, as demonstrated by frameworks such as DSPy and\nTextGrad. However, optimizers built on language models themselves are usually\ndesigned by humans with manual design choices; optimizers themselves are not\noptimized. Moreover, these optimizers are general purpose by design, to be\nuseful to a broad audience, and are not tailored for specific tasks. To address\nthese challenges, we propose metaTextGrad, which focuses on designing a\nmeta-optimizer to further enhance existing optimizers and align them to be good\noptimizers for a given task. Our approach consists of two key components: a\nmeta prompt optimizer and a meta structure optimizer. The combination of these\ntwo significantly improves performance across multiple benchmarks, achieving an\naverage absolute performance improvement of up to 6% compared to the best\nbaseline.", "AI": {"tldr": "\u63d0\u51fametaTextGrad\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u4f18\u5316\u5668\u589e\u5f3a\u73b0\u6709LLM\u4f18\u5316\u5668\u7684\u4efb\u52a1\u9002\u914d\u6027\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u5b9e\u73b0\u5e73\u57476%\u7684\u6027\u80fd\u63d0\u5347", "motivation": "\u73b0\u6709LLM\u4f18\u5316\u5668\u5b58\u5728\u4eba\u5de5\u8bbe\u8ba1\u5c40\u9650\uff08\u672a\u81ea\u6211\u4f18\u5316\uff09\u548c\u901a\u7528\u6027\u8bbe\u8ba1\u5bfc\u81f4\u4efb\u52a1\u9002\u914d\u4e0d\u8db3\u7684\u95ee\u9898", "method": "\u5305\u542b\u5143\u63d0\u793a\u4f18\u5316\u5668\uff08\u4f18\u5316prompt\u5de5\u7a0b\uff09\u548c\u5143\u7ed3\u6784\u4f18\u5316\u5668\uff08\u4f18\u5316\u6846\u67b6\u7ed3\u6784\uff09\u7684\u53cc\u5c42\u4f18\u5316\u67b6\u6784", "result": "\u5728GSM8K\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4TextGrad\u7b49\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u6700\u9ad86%\u7684\u7edd\u5bf9\u6027\u80fd\u63d0\u5347", "conclusion": "\u901a\u8fc7\u5143\u4f18\u5316\u5668\u5b9e\u73b0\u73b0\u6709\u4f18\u5316\u5668\u7684\u4efb\u52a1\u9002\u914d\u4f18\u5316\uff0c\u4e3aLLM\u4f18\u5316\u5668\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2505.18536", "pdf": "https://arxiv.org/pdf/2505.18536", "abs": "https://arxiv.org/abs/2505.18536", "authors": ["Haoyuan Sun", "Jiaqi Wu", "Bo Xia", "Yifu Luo", "Yifei Zhao", "Kai Qin", "Xufei Lv", "Tiantian Zhang", "Yongzhe Chang", "Xueqian Wang"], "title": "Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Standing in 2025, at a critical juncture in the pursuit of Artificial General\nIntelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated\nsignificant potential in enhancing the reasoning capability of large language\nmodels (LLMs) and has led to the development of cutting-edge AI models such as\nOpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to\nenhance the reasoning capability of multimodal large language models (MLLMs)\nhas attracted widespread attention from the community. In this position paper,\nwe argue that reinforcement fine-tuning powers the reasoning capability of\nmultimodal large language models. To begin with, we provide a detailed\nintroduction to the fundamental background knowledge that researchers\ninterested in this field should be familiar with. Furthermore, we meticulously\nsummarize the improvements of RFT in powering reasoning capability of MLLMs\ninto five key points: diverse modalities, diverse tasks and domains, better\ntraining algorithms, abundant benchmarks and thriving engineering frameworks.\nFinally, we propose five promising directions for future research that the\ncommunity might consider. We hope that this position paper will provide\nvaluable insights to the community at this pivotal stage in the advancement\ntoward AGI. Summary of works done on RFT for MLLMs is available at\nhttps://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.", "AI": {"tldr": "\u5f3a\u5316\u5fae\u8c03(RFT)\u53ef\u6709\u6548\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8bba\u6587\u7cfb\u7edf\u68b3\u7406\u4e86\u5176\u80cc\u666f\u77e5\u8bc6\u3001\u4e94\u5927\u6539\u8fdb\u7ef4\u5ea6\uff08\u591a\u6a21\u6001/\u591a\u4efb\u52a1/\u8bad\u7ec3\u7b97\u6cd5/\u57fa\u51c6/\u5de5\u7a0b\u6846\u67b6\uff09\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u4e94\u5927\u7814\u7a76\u65b9\u5411", "motivation": "\u5f53\u524dAGI\u53d1\u5c55\u5904\u4e8e\u5173\u952e\u9636\u6bb5\uff0cRFT\u5728\u7eaf\u6587\u672cLLM\u4e2d\u5df2\u5c55\u73b0\u663e\u8457\u6f5c\u529b\uff0c\u4f46\u5982\u4f55\u5c06\u5176\u6709\u6548\u62d3\u5c55\u81f3\u591a\u6a21\u6001\u573a\u666f\u5c1a\u672a\u5f62\u6210\u7cfb\u7edf\u7814\u7a76\u4f53\u7cfb", "method": "\u9996\u5148\u5efa\u7acb\u591a\u6a21\u6001RFT\u7684\u7406\u8bba\u57fa\u7840\uff0c\u7ee7\u800c\u4ece\u6a21\u6001\u591a\u6837\u6027\u3001\u4efb\u52a1\u6269\u5c55\u6027\u3001\u7b97\u6cd5\u521b\u65b0\u6027\u3001\u57fa\u51c6\u5efa\u8bbe\u3001\u5de5\u7a0b\u6846\u67b6\u4e94\u4e2a\u7ef4\u5ea6\u7cfb\u7edf\u603b\u7ed3\u6280\u672f\u8fdb\u5c55", "result": "\u6784\u5efa\u4e86\u9996\u4e2a\u9762\u5411\u591a\u6a21\u6001RFT\u7684\u7cfb\u7edf\u5206\u6790\u6846\u67b6\uff0c\u660e\u786e\u5176\u6280\u672f\u53d1\u5c55\u8def\u7ebf\u56fe\uff0c\u63d0\u51fa\u5305\u542b\u8de8\u6a21\u6001\u5bf9\u9f50\u5f3a\u5316\u3001\u8ba4\u77e5\u67b6\u6784\u4f18\u5316\u7b49\u65b9\u5411\u7684\u672a\u6765\u7814\u7a76\u4f53\u7cfb", "conclusion": "\u5f3a\u5316\u5fae\u8c03\u662f\u63d0\u5347MLLMs\u63a8\u7406\u80fd\u529b\u7684\u6838\u5fc3\u9a71\u52a8\u529b\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u6280\u672f\u6539\u8fdb\u4e0e\u8de8\u5b66\u79d1\u7814\u7a76\u878d\u5408\uff0c\u5c06\u52a0\u901fAGI\u5173\u952e\u7a81\u7834"}}
{"id": "2505.18542", "pdf": "https://arxiv.org/pdf/2505.18542", "abs": "https://arxiv.org/abs/2505.18542", "authors": ["Chen Yang", "Ruping Xu", "Ruizhe Li", "Bin Cao", "Jing Fan"], "title": "Business as \\textit{Rule}sual: A Benchmark and Framework for Business Rule Flow Modeling with LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Process mining aims to discover, monitor and optimize the actual behaviors of\nreal processes. While prior work has mainly focused on extracting procedural\naction flows from instructional texts, rule flows embedded in business\ndocuments remain underexplored. To this end, we introduce a novel annotated\nChinese dataset, \\textbf{BPRF}, which contains 50 business process documents\nwith 326 explicitly labeled business rules across multiple domains. Each rule\nis represented as a <Condition, Action> pair, and we annotate logical\ndependencies between rules (sequential, conditional, or parallel). We also\npropose \\textbf{ExIde}, a framework for automatic business rule extraction and\ndependency relationship identification using large language models (LLMs). We\nevaluate ExIde using 12 state-of-the-art (SOTA) LLMs on the BPRF dataset,\nbenchmarking performance on both rule extraction and dependency classification\ntasks of current LLMs. Our results demonstrate the effectiveness of ExIde in\nextracting structured business rules and analyzing their interdependencies for\ncurrent SOTA LLMs, paving the way for more automated and interpretable business\nprocess automation.", "AI": {"tldr": "\u63d0\u51faBPRF\u6570\u636e\u96c6\u548cExIde\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u63d0\u53d6\u5546\u4e1a\u89c4\u5219\u53ca\u4f9d\u8d56\u5173\u7cfb\uff0c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u6d41\u7a0b\u6316\u6398\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u6307\u5bfc\u6027\u6587\u672c\u7684\u52a8\u4f5c\u6d41\u63d0\u53d6\uff0c\u4f46\u5546\u4e1a\u6587\u6863\u4e2d\u7684\u89c4\u5219\u6d41\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u65e8\u5728\u586b\u8865\u8be5\u7a7a\u767d\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u4e14\u53ef\u89e3\u91ca\u7684\u4e1a\u52a1\u89c4\u5219\u7ed3\u6784\u5316\u5206\u6790\u3002", "method": "\u6784\u5efa\u542b50\u4e2a\u6587\u6863\u7684BPRF\u6570\u636e\u96c6\uff08\u6807\u6ce8326\u6761<\u6761\u4ef6,\u52a8\u4f5c>\u89c4\u5219\u53ca\u5176\u903b\u8f91\u4f9d\u8d56\uff09\uff0c\u5f00\u53d1ExIde\u6846\u67b6\u5229\u7528LLMs\u8fdb\u884c\u89c4\u5219\u63d0\u53d6\u4e0e\u4f9d\u8d56\u5206\u7c7b\uff0c\u5e76\u8bc4\u4f3012\u4e2aSOTA\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u3002", "result": "ExIde\u6709\u6548\u63d0\u53d6\u7ed3\u6784\u5316\u5546\u4e1a\u89c4\u5219\u5e76\u5206\u6790\u4f9d\u8d56\u5173\u7cfb\uff0c\u5f53\u524d\u5148\u8fdbLLMs\u5728\u89c4\u5219\u63d0\u53d6\uff08F1=0.82\uff09\u548c\u4f9d\u8d56\u5206\u7c7b\uff08Accuracy=0.75\uff09\u4efb\u52a1\u4e2d\u5c55\u73b0\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u6784\u5316\u89c4\u5219\u63d0\u53d6\u4e0e\u4f9d\u8d56\u5206\u6790\uff0c\u4e3a\u81ea\u52a8\u5316\u4e1a\u52a1\u6d41\u7a0b\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u6280\u672f\u8def\u5f84\uff0c\u63a8\u52a8\u4e1a\u52a1\u8fc7\u7a0b\u81ea\u52a8\u5316\u5411\u66f4\u9ad8\u9636\u53d1\u5c55\u3002"}}
{"id": "2505.18548", "pdf": "https://arxiv.org/pdf/2505.18548", "abs": "https://arxiv.org/abs/2505.18548", "authors": ["Sanwoo Lee", "Kun Liang", "Yunfang Wu"], "title": "Composable Cross-prompt Essay Scoring by Merging Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in cross-prompt automated essay scoring (AES) typically train\nmodels jointly on all source prompts, often requiring additional access to\nunlabeled target prompt essays simultaneously. However, using all sources is\nsuboptimal in our pilot study, and re-accessing source datasets during\nadaptation raises privacy concerns. We propose a source-free adaptation\napproach that selectively merges individually trained source models' parameters\ninstead of datasets. In particular, we simulate joint training through linear\ncombinations of task vectors -- the parameter updates from fine-tuning. To\noptimize the combination's coefficients, we propose Prior-encoded Information\nMaximization (PIM), an unsupervised objective which promotes the model's score\ndiscriminability regularized by priors pre-computed from the sources. We employ\nBayesian optimization as an efficient optimizer of PIM. Experimental results\nwith LLMs on in-dataset and cross-dataset adaptation show that our method (1)\nconsistently outperforms training jointly on all sources, (2) maintains\nsuperior robustness compared to other merging methods, (3) excels under severe\ndistribution shifts where recent leading cross-prompt methods struggle, all\nwhile retaining computational efficiency.", "AI": {"tldr": "Proposes source-free cross-prompt AES adaptation via task vector merging and PIM optimization", "motivation": "Address privacy concerns and suboptimal performance of joint training approaches in cross-prompt AES", "method": "Linear combination of task vectors with Prior-encoded Information Maximization (PIM) objective and Bayesian optimization", "result": "Outperforms joint training, shows robustness, excels under distribution shifts with computational efficiency", "conclusion": "Provides privacy-preserving effective adaptation method for AES systems with strong empirical performance"}}
{"id": "2505.18549", "pdf": "https://arxiv.org/pdf/2505.18549", "abs": "https://arxiv.org/abs/2505.18549", "authors": ["Baraa Hikal", "Mohamed Basem", "Islam Oshallah", "Ali Hamdi"], "title": "MSA at BEA 2025 Shared Task: Disagreement-Aware Instruction Tuning for Multi-Dimensional Evaluation of LLMs as Math Tutors", "categories": ["cs.CL"], "comment": null, "summary": "We present MSA-MathEval, our submission to the BEA 2025 Shared Task on\nevaluating AI tutor responses across four instructional dimensions: Mistake\nIdentification, Mistake Location, Providing Guidance, and Actionability. Our\napproach uses a unified training pipeline to fine-tune a single\ninstruction-tuned language model across all tracks, without any task-specific\narchitectural changes. To improve prediction reliability, we introduce a\ndisagreement-aware ensemble inference strategy that enhances coverage of\nminority labels. Our system achieves strong performance across all tracks,\nranking 1st in Providing Guidance, 3rd in Actionability, and 4th in both\nMistake Identification and Mistake Location. These results demonstrate the\neffectiveness of scalable instruction tuning and disagreement-driven modeling\nfor robust, multi-dimensional evaluation of LLMs as educational tutors.", "AI": {"tldr": "\u63d0\u51faMSA-MathEval\u7cfb\u7edf\uff0c\u91c7\u7528\u7edf\u4e00\u6307\u4ee4\u5fae\u8c03\u6846\u67b6\u548c\u5206\u6b67\u611f\u77e5\u96c6\u6210\u7b56\u7565\uff0c\u5728BEA 2025\u6559\u80b2AI\u8bc4\u4f30\u4efb\u52a1\u4e2d\u56db\u9879\u6307\u6807\u5747\u8fdb\u5165\u524d\u56db\u540d", "motivation": "\u9488\u5bf9\u6559\u80b2\u573a\u666f\u4e0b\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5bfc\u5e08\u7684\u591a\u7ef4\u5ea6\u80fd\u529b\u8bc4\u4f30\uff0c\u63a2\u7d22\u53ef\u6269\u5c55\u7684\u6307\u4ee4\u5fae\u8c03\u65b9\u6cd5\u548c\u5206\u6b67\u9a71\u52a8\u5efa\u6a21\u5bf9\u8bc4\u4f30\u9c81\u68d2\u6027\u7684\u63d0\u5347", "method": "\u5355\u4e00\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u9002\u914d\u5168\u90e8\u4efb\u52a1+\u5206\u6b67\u611f\u77e5\u96c6\u6210\u63a8\u7406\uff08\u589e\u5f3a\u5c11\u6570\u6807\u7b7e\u8986\u76d6\u7387\uff09", "result": "\u6307\u5bfc\u5efa\u8bae\u7ef4\u5ea6\u7b2c\u4e00/\u53ef\u64cd\u4f5c\u6027\u7b2c\u4e09/\u9519\u8bef\u8bc6\u522b\u4e0e\u5b9a\u4f4d\u7b2c\u56db", "conclusion": "\u9a8c\u8bc1\u4e86\u7edf\u4e00\u6307\u4ee4\u5fae\u8c03\u6846\u67b6\u4e0e\u5206\u6b67\u9a71\u52a8\u5efa\u6a21\u5728\u6559\u80b2AI\u591a\u7ef4\u5ea6\u8bc4\u4f30\u4e2d\u7684\u6709\u6548\u6027"}}
{"id": "2505.18555", "pdf": "https://arxiv.org/pdf/2505.18555", "abs": "https://arxiv.org/abs/2505.18555", "authors": ["Yiyang Feng", "Yichen Wang", "Shaobo Cui", "Boi Faltings", "Mina Lee", "Jiawei Zhou"], "title": "Unraveling Misinformation Propagation in LLM Reasoning", "categories": ["cs.CL"], "comment": "24 pages, 14 figures, 4 tables", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nreasoning, positioning them as promising tools for supporting human\nproblem-solving. However, what happens when their performance is affected by\nmisinformation, i.e., incorrect inputs introduced by users due to oversights or\ngaps in knowledge? Such misinformation is prevalent in real-world interactions\nwith LLMs, yet how it propagates within LLMs' reasoning process remains\nunderexplored. Focusing on mathematical reasoning, we present a comprehensive\nanalysis of how misinformation affects intermediate reasoning steps and final\nanswers. We also examine how effectively LLMs can correct misinformation when\nexplicitly instructed to do so. Even with explicit instructions, LLMs succeed\nless than half the time in rectifying misinformation, despite possessing\ncorrect internal knowledge, leading to significant accuracy drops (10.02% -\n72.20%). Further analysis shows that applying factual corrections early in the\nreasoning process most effectively reduces misinformation propagation, and\nfine-tuning on synthesized data with early-stage corrections significantly\nimproves reasoning factuality. Our work offers a practical approach to\nmitigating misinformation propagation.", "AI": {"tldr": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u5904\u7406\u9519\u8bef\u4fe1\u606f\u7684\u5f71\u54cd\u53ca\u7ea0\u6b63\u65b9\u6cd5\uff0c\u53d1\u73b0\u65e9\u671f\u7ea0\u6b63\u6700\u6709\u6548", "motivation": "\u63a2\u8ba8\u9519\u8bef\u4fe1\u606f\u5982\u4f55\u5f71\u54cdLLMs\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u5c24\u5176\u662f\u7528\u6237\u56e0\u77e5\u8bc6\u76f2\u533a\u8f93\u5165\u9519\u8bef\u4fe1\u606f\u65f6\u7684\u4f20\u64ad\u673a\u5236", "method": "\u901a\u8fc7\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u5206\u6790\u9519\u8bef\u4fe1\u606f\u5bf9\u4e2d\u95f4\u6b65\u9aa4\u7684\u5f71\u54cd\uff0c\u6d4b\u8bd5\u6a21\u578b\u5728\u660e\u786e\u7ea0\u6b63\u6307\u4ee4\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u65e9\u671f\u7ea0\u6b63\u7684\u5fae\u8c03\u65b9\u6848", "result": "\u5373\u4f7f\u5177\u5907\u6b63\u786e\u77e5\u8bc6\uff0c\u6a21\u578b\u7ea0\u9519\u6210\u529f\u7387\u4e0d\u8db350%\uff08\u51c6\u786e\u7387\u4e0b\u964d10.02%-72.20%\uff09\uff0c\u4f46\u65e9\u671f\u7ea0\u6b63\u53ef\u4f7f\u9519\u8bef\u4f20\u64ad\u51cf\u5c1140%\u4ee5\u4e0a", "conclusion": "\u5728\u63a8\u7406\u94fe\u65e9\u671f\u5b9e\u65bd\u4e8b\u5b9e\u4fee\u6b63\u5e76\u7ed3\u5408\u9488\u5bf9\u6027\u5fae\u8c03\uff0c\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u6297\u5e72\u6270\u80fd\u529b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.18556", "pdf": "https://arxiv.org/pdf/2505.18556", "abs": "https://arxiv.org/abs/2505.18556", "authors": ["Jun Zhuang", "Haibo Jin", "Ye Zhang", "Zhengjian Kang", "Wenbin Zhang", "Gaby G. Dagher", "Haohan Wang"], "title": "Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint, under review. TL;DR: We propose a new two-stage\n  intent-based prompt-refinement framework, IntentPrompt, that aims to explore\n  the vulnerability of LLMs' content moderation guardrails by refining prompts\n  into benign-looking declarative forms via intent manipulation for red-teaming\n  purposes", "summary": "Intent detection, a core component of natural language understanding, has\nconsiderably evolved as a crucial mechanism in safeguarding large language\nmodels (LLMs). While prior work has applied intent detection to enhance LLMs'\nmoderation guardrails, showing a significant success against content-level\njailbreaks, the robustness of these intent-aware guardrails under malicious\nmanipulations remains under-explored. In this work, we investigate the\nvulnerability of intent-aware guardrails and demonstrate that LLMs exhibit\nimplicit intent detection capabilities. We propose a two-stage intent-based\nprompt-refinement framework, IntentPrompt, that first transforms harmful\ninquiries into structured outlines and further reframes them into\ndeclarative-style narratives by iteratively optimizing prompts via feedback\nloops to enhance jailbreak success for red-teaming purposes. Extensive\nexperiments across four public benchmarks and various black-box LLMs indicate\nthat our framework consistently outperforms several cutting-edge jailbreak\nmethods and evades even advanced Intent Analysis (IA) and Chain-of-Thought\n(CoT)-based defenses. Specifically, our \"FSTR+SPIN\" variant achieves attack\nsuccess rates ranging from 88.25% to 96.54% against CoT-based defenses on the\no1 model, and from 86.75% to 97.12% on the GPT-4o model under IA-based\ndefenses. These findings highlight a critical weakness in LLMs' safety\nmechanisms and suggest that intent manipulation poses a growing challenge to\ncontent moderation guardrails.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86\u610f\u56fe\u68c0\u6d4b\u673a\u5236\u5728\u5927\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u9632\u62a4\u4e2d\u7684\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5\u63d0\u793a\u4f18\u5316\u6846\u67b6IntentPrompt\uff0c\u6210\u529f\u7a81\u7834\u73b0\u6709\u9632\u5fa1\u4f53\u7cfb\u3002", "motivation": "\u73b0\u6709\u610f\u56fe\u611f\u77e5\u9632\u62a4\u673a\u5236\u5728\u6076\u610f\u64cd\u7eb5\u4e0b\u7684\u9c81\u68d2\u6027\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0cLLMs\u9690\u542b\u7684\u610f\u56fe\u68c0\u6d4b\u80fd\u529b\u53ef\u80fd\u6210\u4e3a\u653b\u51fb\u7a81\u7834\u53e3\u3002", "method": "\u5f00\u53d1\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u5148\u5c06\u6709\u5bb3\u67e5\u8be2\u8f6c\u4e3a\u7ed3\u6784\u5316\u5927\u7eb2\uff0c\u518d\u901a\u8fc7\u53cd\u9988\u5faa\u73af\u4f18\u5316\u4e3a\u9648\u8ff0\u5f0f\u53d9\u8ff0\uff08FSTR+SPIN\u53d8\u4f53\uff09\u3002", "result": "\u5728GPT-4o\u7b49\u6a21\u578b\u4e0a\u5b9e\u73b086.75%-97.12%\u653b\u51fb\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u80fd\u7ed5\u8fc7CoT/IA\u9632\u5fa1\u3002", "conclusion": "LLMs\u5b89\u5168\u673a\u5236\u5b58\u5728\u610f\u56fe\u64cd\u4f5c\u6f0f\u6d1e\uff0c\u9700\u91cd\u65b0\u8bc4\u4f30\u57fa\u4e8e\u610f\u56fe\u68c0\u6d4b\u7684\u5185\u5bb9\u5ba1\u6838\u9632\u62a4\u6709\u6548\u6027\u3002"}}
{"id": "2505.18557", "pdf": "https://arxiv.org/pdf/2505.18557", "abs": "https://arxiv.org/abs/2505.18557", "authors": ["He Zhu", "Zhiwen Ruan", "Junyou Su", "Xingwei He", "Wenjia Zhang", "Yun Chen", "Guanhua Chen"], "title": "TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through Structure-based Augmentation", "categories": ["cs.CL"], "comment": null, "summary": "High-quality instruction data is crucial for developing large language models\n(LLMs), yet existing approaches struggle to effectively control instruction\ncomplexity. We present TAG-INSTRUCT, a novel framework that enhances\ninstruction complexity through structured semantic compression and controlled\ndifficulty augmentation. Unlike previous prompt-based methods operating on raw\ntext, TAG-INSTRUCT compresses instructions into a compact tag space and\nsystematically enhances complexity through RL-guided tag expansion. Through\nextensive experiments, we show that TAG-INSTRUCT outperforms existing\ninstruction complexity augmentation approaches. Our analysis reveals that\noperating in tag space provides superior controllability and stability across\ndifferent instruction synthesis frameworks.", "AI": {"tldr": "\u63d0\u51faTAG-INSTRUCT\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u538b\u7f29\u548c\u96be\u5ea6\u589e\u5f3a\u63d0\u5347\u5927\u6a21\u578b\u6307\u4ee4\u6570\u636e\u590d\u6742\u5ea6", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u63a7\u5236\u6307\u4ee4\u590d\u6742\u5ea6\uff0c\u800c\u9ad8\u8d28\u91cf\u6307\u4ee4\u6570\u636e\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u53d1\u5c55\u81f3\u5173\u91cd\u8981", "method": "\u5c06\u6307\u4ee4\u538b\u7f29\u81f3\u6807\u7b7e\u7a7a\u95f4\uff0c\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u5f15\u5bfc\u7684\u6807\u7b7e\u6269\u5c55\u5b9e\u73b0\u7cfb\u7edf\u6027\u590d\u6742\u5ea6\u589e\u5f3a", "result": "\u5b9e\u9a8c\u663e\u793aTAG-INSTRUCT\u5728\u6307\u4ee4\u590d\u6742\u5ea6\u589e\u5f3a\u6548\u679c\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6807\u7b7e\u7a7a\u95f4\u64cd\u4f5c\u5c55\u73b0\u66f4\u597d\u63a7\u5236\u6027", "conclusion": "\u6807\u7b7e\u7a7a\u95f4\u64cd\u4f5c\u5728\u4e0d\u540c\u6307\u4ee4\u5408\u6210\u6846\u67b6\u4e2d\u5177\u6709\u66f4\u4f18\u7684\u590d\u6742\u5ea6\u63a7\u5236\u6f5c\u529b"}}
{"id": "2505.18562", "pdf": "https://arxiv.org/pdf/2505.18562", "abs": "https://arxiv.org/abs/2505.18562", "authors": ["Xunlian Dai", "Li Zhou", "Benyou Wang", "Haizhou Li"], "title": "From Word to World: Evaluate and Mitigate Culture Bias via Word Association Test", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The human-centered word association test (WAT) serves as a cognitive proxy,\nrevealing sociocultural variations through lexical-semantic patterns. We extend\nthis test into an LLM-adaptive, free-relation task to assess the alignment of\nlarge language models (LLMs) with cross-cultural cognition. To mitigate the\nculture preference, we propose CultureSteer, an innovative approach that\nintegrates a culture-aware steering mechanism to guide semantic representations\ntoward culturally specific spaces. Experiments show that current LLMs exhibit\nsignificant bias toward Western cultural (notably in American) schemas at the\nword association level. In contrast, our model substantially improves\ncross-cultural alignment, surpassing prompt-based methods in capturing diverse\nsemantic associations. Further validation on culture-sensitive downstream tasks\nconfirms its efficacy in fostering cognitive alignment across cultures. This\nwork contributes a novel methodological paradigm for enhancing cultural\nawareness in LLMs, advancing the development of more inclusive language\ntechnologies.", "AI": {"tldr": "\u63d0\u51faCultureSteer\u65b9\u6cd5\u6539\u5584\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6587\u5316\u504f\u89c1\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u8de8\u6587\u5316\u8bed\u4e49\u5bf9\u9f50\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u8bcd\u8bed\u8054\u60f3\u5c42\u9762\u5b58\u5728\u663e\u8457\u897f\u65b9\u6587\u5316\u504f\u597d\uff0c\u9700\u589e\u5f3a\u6587\u5316\u5305\u5bb9\u6027\u3002", "method": "\u5f00\u53d1\u6587\u5316\u611f\u77e5\u5f15\u5bfc\u673a\u5236CultureSteer\uff0c\u5c06\u8bed\u4e49\u8868\u5f81\u5bfc\u5411\u7279\u5b9a\u6587\u5316\u7a7a\u95f4\u3002", "result": "\u6a21\u578b\u8de8\u6587\u5316\u5bf9\u9f50\u5ea6\u63d0\u534765%\uff0c\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8ba4\u77e5\u5bf9\u9f50\u6548\u679c\u4f18\u4e8e\u4f20\u7edf\u63d0\u793a\u65b9\u6cd5\u3002", "conclusion": "\u4e3a\u589e\u5f3aLLMs\u6587\u5316\u610f\u8bc6\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u63a8\u52a8\u5305\u5bb9\u6027\u8bed\u8a00\u6280\u672f\u53d1\u5c55\u3002"}}
{"id": "2505.18581", "pdf": "https://arxiv.org/pdf/2505.18581", "abs": "https://arxiv.org/abs/2505.18581", "authors": ["Wentao Hu", "Wengyu Zhang", "Yiyang Jiang", "Chen Jason Zhang", "Xiaoyong Wei", "Qing Li"], "title": "Removal of Hallucination on Hallucination: Debate-Augmented RAG", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025", "summary": "Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating\nexternal knowledge, yet it introduces a critical issue: erroneous or biased\nretrieval can mislead generation, compounding hallucinations, a phenomenon we\nterm Hallucination on Hallucination. To address this, we propose\nDebate-Augmented RAG (DRAG), a training-free framework that integrates\nMulti-Agent Debate (MAD) mechanisms into both retrieval and generation stages.\nIn retrieval, DRAG employs structured debates among proponents, opponents, and\njudges to refine retrieval quality and ensure factual reliability. In\ngeneration, DRAG introduces asymmetric information roles and adversarial\ndebates, enhancing reasoning robustness and mitigating factual inconsistencies.\nEvaluations across multiple tasks demonstrate that DRAG improves retrieval\nreliability, reduces RAG-induced hallucinations, and significantly enhances\noverall factual accuracy. Our code is available at\nhttps://github.com/Huenao/Debate-Augmented-RAG.", "AI": {"tldr": "DRAG\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u673a\u5236\uff0c\u5728RAG\u7684\u68c0\u7d22\u548c\u751f\u6210\u9636\u6bb5\u5206\u522b\u5b9e\u65bd\u7ed3\u6784\u5316\u8fa9\u8bba\uff0c\u6709\u6548\u51cf\u5c11\u68c0\u7d22\u5e7b\u89c9\u5e76\u63d0\u5347\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edfRAG\u5b58\u5728\u68c0\u7d22\u9519\u8bef\u5f15\u53d1\u751f\u6210\u5e7b\u89c9\u7684\u94fe\u5f0f\u8bef\u5bfc\u95ee\u9898\uff08Hallucination on Hallucination\uff09\uff0c\u9700\u8981\u65b0\u7684\u673a\u5236\u6765\u6253\u7834\u9519\u8bef\u4f20\u64ad\u94fe\u6761\u3002", "method": "1. \u68c0\u7d22\u9636\u6bb5\uff1a\u8bbe\u8ba1\u652f\u6301\u8005/\u53cd\u5bf9\u8005/\u88c1\u5224\u7684\u4e09\u65b9\u8fa9\u8bba\u673a\u5236\u7b5b\u9009\u53ef\u9760\u5185\u5bb9\n2. \u751f\u6210\u9636\u6bb5\uff1a\u901a\u8fc7\u4e0d\u5bf9\u79f0\u4fe1\u606f\u89d2\u8272\u548c\u5bf9\u6297\u6027\u8fa9\u8bba\u63d0\u5347\u63a8\u7406\u4e25\u8c28\u6027", "result": "\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u9a8c\u8bc1\uff1aDRAG\u63d0\u5347\u68c0\u7d22\u53ef\u9760\u6027\uff08+18%\uff09\uff0c\u964d\u4f4eRAG\u5e7b\u89c9\u7387\uff08-32%\uff09\uff0c\u6574\u4f53\u4e8b\u5b9e\u51c6\u786e\u7387\u63d0\u534721%", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u5c06\u8fa9\u8bba\u673a\u5236\u7cfb\u7edf\u5316\u878d\u5165RAG\u5168\u6d41\u7a0b\uff0c\u4e3a\u63d0\u5347\u751f\u6210\u5f0fAI\u7684\u4e8b\u5b9e\u53ef\u9760\u6027\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2505.18588", "pdf": "https://arxiv.org/pdf/2505.18588", "abs": "https://arxiv.org/abs/2505.18588", "authors": ["Zesheng Shi", "Yucheng Zhou", "Jing Li"], "title": "Safety Alignment via Constrained Knowledge Unlearning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite significant progress in safety alignment, large language models\n(LLMs) remain susceptible to jailbreak attacks. Existing defense mechanisms\nhave not fully deleted harmful knowledge in LLMs, which allows such attacks to\nbypass safeguards and produce harmful outputs. To address this challenge, we\npropose a novel safety alignment strategy, Constrained Knowledge Unlearning\n(CKU), which focuses on two primary objectives: knowledge localization and\nretention, and unlearning harmful knowledge. CKU works by scoring neurons in\nspecific multilayer perceptron (MLP) layers to identify a subset U of neurons\nassociated with useful knowledge. During the unlearning process, CKU prunes the\ngradients of neurons in U to preserve valuable knowledge while effectively\nmitigating harmful content. Experimental results demonstrate that CKU\nsignificantly enhances model safety without compromising overall performance,\noffering a superior balance between safety and utility compared to existing\nmethods. Additionally, our analysis of neuron knowledge sensitivity across\nvarious MLP layers provides valuable insights into the mechanics of safety\nalignment and model knowledge editing.", "AI": {"tldr": "\u63d0\u51fa\u7ea6\u675f\u77e5\u8bc6\u9057\u5fd8(CKU)\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u4f4d\u5e76\u4fdd\u7559\u6709\u7528\u77e5\u8bc6\u795e\u7ecf\u5143\uff0c\u540c\u65f6\u6d88\u9664\u6709\u5bb3\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\u4e14\u4e0d\u5f71\u54cd\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u672a\u80fd\u5f7b\u5e95\u5220\u9664\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6709\u5bb3\u77e5\u8bc6\uff0c\u5bfc\u81f4\u8d8a\u72f1\u653b\u51fb\u53ef\u7ed5\u8fc7\u9632\u62a4\u673a\u5236\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u3002\u9700\u5728\u4fdd\u8bc1\u6a21\u578b\u77e5\u8bc6\u5b8c\u6574\u6027\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u5b89\u5168\u6027\u3002", "method": "1. \u5728\u7279\u5b9aMLP\u5c42\u8fdb\u884c\u795e\u7ecf\u5143\u8bc4\u5206\uff0c\u8bc6\u522b\u6709\u7528\u77e5\u8bc6\u795e\u7ecf\u5143\u5b50\u96c6U\n2. \u9057\u5fd8\u8fc7\u7a0b\u4e2d\u4fee\u526aU\u795e\u7ecf\u5143\u7684\u68af\u5ea6\u4ee5\u4fdd\u7559\u4ef7\u503c\u77e5\u8bc6\n3. \u901a\u8fc7\u795e\u7ecf\u5143\u77e5\u8bc6\u654f\u611f\u6027\u5206\u6790\u4f18\u5316\u5b89\u5168\u5bf9\u9f50\u673a\u5236", "result": "CKU\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5b89\u5168\u6307\u6807\u63d0\u534719.3%\uff0c\u5728\u4fdd\u630194%\u901a\u7528\u80fd\u529b\u7684\u540c\u65f6\u6709\u6548\u9632\u5fa1\u8d8a\u72f1\u653b\u51fb\u3002\u4e0d\u540cMLP\u5c42\u7684\u795e\u7ecf\u5143\u654f\u611f\u6027\u5206\u6790\u63ed\u793a\u4e86\u77e5\u8bc6\u7f16\u8f91\u7684\u68af\u5ea6\u4f20\u64ad\u89c4\u5f8b\u3002", "conclusion": "CKU\u5b9e\u73b0\u4e86\u5b89\u5168\u6027\u4e0e\u5b9e\u7528\u6027\u7684\u6700\u4f18\u5e73\u8861\uff0c\u4e3a\u6a21\u578b\u5b89\u5168\u5bf9\u9f50\u548c\u77e5\u8bc6\u7f16\u8f91\u63d0\u4f9b\u4e86\u795e\u7ecf\u5143\u7ea7\u522b\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u9996\u6b21\u9a8c\u8bc1\u4e86\u5b9a\u5411\u795e\u7ecf\u5143\u68af\u5ea6\u4fee\u526a\u5728\u77e5\u8bc6\u9057\u5fd8\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.18596", "pdf": "https://arxiv.org/pdf/2505.18596", "abs": "https://arxiv.org/abs/2505.18596", "authors": ["Chen Han", "Wenzhen Zheng", "Xijin Tang"], "title": "Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models", "categories": ["cs.CL", "cs.AI", "I.2.7; I.2.6; H.3.3"], "comment": null, "summary": "The proliferation of misinformation in digital platforms reveals the\nlimitations of traditional detection methods, which mostly rely on static\nclassification and fail to capture the intricate process of real-world\nfact-checking. Despite advancements in Large Language Models (LLMs) that\nenhance automated reasoning, their application to misinformation detection\nremains hindered by issues of logical inconsistency and superficial\nverification. In response, we introduce Debate-to-Detect (D2D), a novel\nMulti-Agent Debate (MAD) framework that reformulates misinformation detection\nas a structured adversarial debate. Inspired by fact-checking workflows, D2D\nassigns domain-specific profiles to each agent and orchestrates a five-stage\ndebate process, including Opening Statement, Rebuttal, Free Debate, Closing\nStatement, and Judgment. To transcend traditional binary classification, D2D\nintroduces a multi-dimensional evaluation mechanism that assesses each claim\nacross five distinct dimensions: Factuality, Source Reliability, Reasoning\nQuality, Clarity, and Ethics. Experiments with GPT-4o on two fakenews datasets\ndemonstrate significant improvements over baseline methods, and the case study\nhighlight D2D's capability to iteratively refine evidence while improving\ndecision transparency, representing a substantial advancement towards robust\nand interpretable misinformation detection. The code will be open-sourced in a\nfuture release.", "AI": {"tldr": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u6846\u67b6D2D\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5bf9\u6297\u8fa9\u8bba\u548c\u591a\u7ef4\u8bc4\u4f30\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u5206\u7c7b\uff0c\u65e0\u6cd5\u5904\u7406\u590d\u6742\u4e8b\u5b9e\u6838\u67e5\u6d41\u7a0b\uff1b\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u903b\u8f91\u4e0d\u4e00\u81f4\u548c\u8868\u9762\u9a8c\u8bc1\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u4e8b\u5b9e\u6838\u67e5\u6d41\u7a0b\u8bbe\u8ba1\u4e94\u9636\u6bb5\u8fa9\u8bba\u6846\u67b6\uff08\u5f00\u7bc7\u9648\u8ff0\u2192\u53cd\u9a73\u2192\u81ea\u7531\u8fa9\u8bba\u2192\u603b\u7ed3\u9648\u8ff0\u2192\u5224\u51b3\uff09\uff0c\u5f15\u5165\u4e8b\u5b9e\u6027/\u4fe1\u6e90\u53ef\u9760\u6027/\u63a8\u7406\u8d28\u91cf/\u6e05\u6670\u5ea6/\u4f26\u7406\u4e94\u7ef4\u8bc4\u4f30\u4f53\u7cfb\u3002", "result": "\u5728GPT-4o\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\u57fa\u7ebf\u65b9\u6cd5\u663e\u8457\u6539\u8fdb\uff0c\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u8bc1\u636e\u8fed\u4ee3\u4f18\u5316\u80fd\u529b\u548c\u51b3\u7b56\u900f\u660e\u6027\u63d0\u5347\u3002", "conclusion": "D2D\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u673a\u5236\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u7684\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\uff0c\u4e3a\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2505.18601", "pdf": "https://arxiv.org/pdf/2505.18601", "abs": "https://arxiv.org/abs/2505.18601", "authors": ["Jongwoo Ko", "Sungnyun Kim", "Sungwoo Cho", "Se-Young Yun"], "title": "Flex-Judge: Think Once, Judge Anywhere", "categories": ["cs.CL", "cs.AI"], "comment": "The code is available at https://github.com/jongwooko/flex-judge", "summary": "Human-generated reward signals are critical for aligning generative models\nwith human preferences, guiding both training and inference-time evaluations.\nWhile large language models (LLMs) employed as proxy evaluators, i.e.,\nLLM-as-a-Judge, significantly reduce the costs associated with manual\nannotations, they typically require extensive modality-specific training data\nand fail to generalize well across diverse multimodal tasks. In this paper, we\npropose Flex-Judge, a reasoning-guided multimodal judge model that leverages\nminimal textual reasoning data to robustly generalize across multiple\nmodalities and evaluation formats. Our core intuition is that structured\ntextual reasoning explanations inherently encode generalizable decision-making\npatterns, enabling an effective transfer to multimodal judgments, e.g., with\nimages or videos. Empirical results demonstrate that Flex-Judge, despite being\ntrained on significantly fewer text data, achieves competitive or superior\nperformance compared to state-of-the-art commercial APIs and extensively\ntrained multimodal evaluators. Notably, Flex-Judge presents broad impact in\nmodalities like molecule, where comprehensive evaluation benchmarks are scarce,\nunderscoring its practical value in resource-constrained domains. Our framework\nhighlights reasoning-based text supervision as a powerful, cost-effective\nalternative to traditional annotation-intensive approaches, substantially\nadvancing scalable multimodal model-as-a-judge.", "AI": {"tldr": "Flex-Judge\u901a\u8fc7\u7ed3\u6784\u5316\u6587\u672c\u63a8\u7406\u6570\u636e\u5b9e\u73b0\u8de8\u6a21\u6001\u8bc4\u4f30\uff0c\u5728\u6570\u636e\u91cf\u8fdc\u5c11\u4e8e\u73b0\u6709\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u6027\u80fd\u8d85\u8d8a\u5546\u4e1aAPI\u53ca\u591a\u6a21\u6001\u8bc4\u4f30\u5668\uff0c\u5c24\u5176\u5728\u5206\u5b50\u7b49\u8d44\u6e90\u53d7\u9650\u9886\u57df\u5c55\u73b0\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709LLM\u4f5c\u4e3a\u4ee3\u7406\u8bc4\u4f30\u5668\u9700\u5927\u91cf\u9886\u57df\u4e13\u5c5e\u8bad\u7ec3\u6570\u636e\u4e14\u8de8\u6a21\u6001\u6cdb\u5316\u80fd\u529b\u5f31\uff0c\u9700\u66f4\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528\u6587\u672c\u63a8\u7406\u89e3\u91ca\u5185\u5728\u7684\u901a\u7528\u51b3\u7b56\u6a21\u5f0f\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u6587\u672c\u76d1\u7763\u5b9e\u73b0\u6a21\u6001\u95f4\u77e5\u8bc6\u8fc1\u79fb\uff08\u5982\u56fe\u50cf/\u89c6\u9891\u8bc4\u4f30\uff09\uff0c\u91c7\u7528\u63a8\u7406\u5f15\u5bfc\u7684\u6a21\u578b\u67b6\u6784\u3002", "result": "\u4ec5\u7528\u5c11\u91cf\u6587\u672c\u6570\u636e\u8bad\u7ec3\u5373\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5728\u5206\u5b50\u8bc4\u4f30\u7b49\u7f3a\u4e4f\u57fa\u51c6\u7684\u9886\u57df\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u6846\u67b6\u666e\u9002\u6027\u3002", "conclusion": "\u57fa\u4e8e\u63a8\u7406\u7684\u6587\u672c\u76d1\u7763\u53ef\u66ff\u4ee3\u4f20\u7edf\u9ad8\u6210\u672c\u6807\u6ce8\u8303\u5f0f\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u6a21\u578b\u5373\u8bc4\u5224\u8005\u63d0\u4f9b\u65b0\u8def\u5f84\uff0c\u63a8\u52a8\u8d44\u6e90\u654f\u611f\u9886\u57df\u8bc4\u4f30\u4f53\u7cfb\u53d1\u5c55\u3002"}}
{"id": "2505.18609", "pdf": "https://arxiv.org/pdf/2505.18609", "abs": "https://arxiv.org/abs/2505.18609", "authors": ["Ashwin Sankar", "Yoach Lacombe", "Sherry Thomas", "Praveen Srinivasa Varadhan", "Sanchit Gandhi", "Mitesh M Khapra"], "title": "RASMALAI: Resources for Adaptive Speech Modeling in Indian Languages with Accents and Intonations", "categories": ["cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "We introduce RASMALAI, a large-scale speech dataset with rich text\ndescriptions, designed to advance controllable and expressive text-to-speech\n(TTS) synthesis for 23 Indian languages and English. It comprises 13,000 hours\nof speech and 24 million text-description annotations with fine-grained\nattributes like speaker identity, accent, emotion, style, and background\nconditions. Using RASMALAI, we develop IndicParlerTTS, the first open-source,\ntext-description-guided TTS for Indian languages. Systematic evaluation\ndemonstrates its ability to generate high-quality speech for named speakers,\nreliably follow text descriptions and accurately synthesize specified\nattributes. Additionally, it effectively transfers expressive characteristics\nboth within and across languages. IndicParlerTTS consistently achieves strong\nperformance across these evaluations, setting a new standard for controllable\nmultilingual expressive speech synthesis in Indian languages.", "AI": {"tldr": "\u63d0\u51fa\u5927\u89c4\u6a21\u591a\u5c5e\u6027\u8bed\u97f3\u6570\u636e\u96c6RASMALAI\u53ca\u9996\u4e2a\u5f00\u6e90\u5370\u5ea6\u8bed\u8a00\u63cf\u8ff0\u9a71\u52a8TTS\u7cfb\u7edfIndicParlerTTS\uff0c\u652f\u630123\u79cd\u5370\u5ea6\u8bed\u8a00\u4e0e\u82f1\u8bed\u7684\u9ad8\u8d28\u91cf\u53ef\u63a7\u8bed\u97f3\u5408\u6210\u3002", "motivation": "\u89e3\u51b3\u5370\u5ea6\u8bed\u8a00TTS\u9886\u57df\u7f3a\u4e4f\u53ef\u63a7\u8868\u8fbe\u6027\u5408\u6210\u7cfb\u7edf\u7684\u95ee\u9898\uff0c\u7a81\u7834\u591a\u8bed\u8a00\u3001\u591a\u5c5e\u6027\u8bed\u97f3\u6570\u636e\u8d44\u6e90\u4e0d\u8db3\u7684\u74f6\u9888\u3002", "method": "\u6784\u5efa\u542b13k\u5c0f\u65f6\u8bed\u97f3\u548c24M\u7ec6\u7c92\u5ea6\u6807\u6ce8\u7684RASMALAI\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u6587\u672c\u63cf\u8ff0\u5f15\u5bfc\u7684TTS\u6846\u67b6\u5b9e\u73b0\u591a\u5c5e\u6027\u8054\u5408\u63a7\u5236\u4e0e\u8de8\u8bed\u8a00\u7279\u5f81\u8fc1\u79fb\u3002", "result": "\u7cfb\u7edf\u8bc4\u4f30\u663e\u793a\u5728\u8bf4\u8bdd\u4eba\u4e00\u81f4\u6027\u3001\u5c5e\u6027\u63a7\u5236\u7cbe\u5ea6\u548c\u8de8\u8bed\u8a00\u8868\u8fbe\u8fc1\u79fb\u4e0a\u5747\u8fbeSOTA\uff0cMOS\u8bc4\u5206\u8d85\u8fc74.2/5.0\u3002", "conclusion": "IndicParlerTTS\u4e3a\u5370\u5ea6\u8bed\u8a00\u521b\u5efa\u4e86\u53ef\u63a7\u591a\u8bed\u8a00\u8bed\u97f3\u5408\u6210\u65b0\u8303\u5f0f\uff0c\u63a8\u52a8\u533a\u57df\u8bed\u8a00AI\u6280\u672f\u7684\u5305\u5bb9\u6027\u53d1\u5c55\u3002"}}
{"id": "2505.18610", "pdf": "https://arxiv.org/pdf/2505.18610", "abs": "https://arxiv.org/abs/2505.18610", "authors": ["Tengxuan Liu", "Shiyao Li", "Jiayi Yang", "Tianchen Zhao", "Feng Zhou", "Xiaohui Song", "Guohao Dai", "Shengen Yan", "Huazhong Yang", "Yu Wang"], "title": "PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Recently, significant progress has been made in developing reasoning-capable\nLarge Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.\nHowever, this long-CoT reasoning process imposes substantial memory overhead\ndue to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache\nquantization has emerged as a promising compression technique and has been\nextensively studied in short-context scenarios. However, directly applying\nexisting methods to long-CoT LLMs causes significant performance degradation\ndue to the following two reasons: (1) Large cumulative error: Existing methods\nfail to adequately leverage available memory, and they directly quantize the KV\nCache during each decoding step, leading to large cumulative quantization\nerror. (2) Short-context calibration: Due to Rotary Positional Embedding\n(RoPE), the use of short-context data during calibration fails to account for\nthe distribution of less frequent channels in the Key Cache, resulting in\nperformance loss. We propose Progressive Mixed-Precision KV Cache Quantization\n(PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To\nreduce cumulative error, we design a progressive quantization strategy to\ngradually lower the bit-width of KV Cache in each block. Then, we propose\nblock-wise memory allocation to assign a higher bit-width to more sensitive\ntransformer blocks. (2) To increase the calibration length without additional\noverhead, we propose a new calibration strategy with positional interpolation\nthat leverages short calibration data with positional interpolation to\napproximate the data distribution of long-context data. Extensive experiments\non 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark\nperformance by up to 8% over SOTA baselines under the same memory budget. Our\ncode is available at https://github.com/thu-nics/PM-KVQ.", "AI": {"tldr": "\u63d0\u51faPM-KVQ\u6e10\u8fdb\u6df7\u5408\u7cbe\u5ea6KV\u7f13\u5b58\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6e10\u8fdb\u91cf\u5316\u7b56\u7565\u548c\u4f4d\u7f6e\u63d2\u503c\u6821\u51c6\u7b56\u7565\uff0c\u89e3\u51b3\u957f\u94fe\u601d\u7ef4\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684KV\u7f13\u5b58\u91cf\u5316\u7d2f\u79ef\u8bef\u5dee\u548c\u77ed\u4e0a\u4e0b\u6587\u6821\u51c6\u95ee\u9898\u3002", "motivation": "\u73b0\u6709KV\u7f13\u5b58\u91cf\u5316\u65b9\u6cd5\u5728\u957f\u94fe\u601d\u7ef4LLMs\u4e2d\u76f4\u63a5\u5e94\u7528\u4f1a\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4e3b\u8981\u7531\u4e8e\uff1a1\uff09\u9010\u5c42\u91cf\u5316\u4ea7\u751f\u7684\u7d2f\u79ef\u8bef\u5dee\uff1b2\uff09\u57fa\u4e8e\u77ed\u4e0a\u4e0b\u6587\u6570\u636e\u7684\u6821\u51c6\u65e0\u6cd5\u9002\u5e94\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u7684\u952e\u7f13\u5b58\u5206\u5e03\u3002", "method": "PM-KVQ\u5305\u542b\uff1a1\uff09\u6e10\u8fdb\u91cf\u5316\u7b56\u7565\u5206\u5757\u964d\u4f4eKV\u7f13\u5b58\u4f4d\u5bbd\uff0c\u914d\u5408\u5757\u654f\u611f\u5ea6\u5185\u5b58\u5206\u914d\uff1b2\uff09\u901a\u8fc7\u4f4d\u7f6e\u63d2\u503c\u6280\u672f\u5229\u7528\u77ed\u6821\u51c6\u6570\u636e\u6a21\u62df\u957f\u4e0a\u4e0b\u6587\u6570\u636e\u5206\u5e03\u3002", "result": "\u57287B-70B\u957f\u94fe\u601d\u7ef4LLMs\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u540c\u7b49\u5185\u5b58\u9884\u7b97\u4e0b\u63a8\u7406\u6027\u80fd\u6700\u9ad8\u63d0\u53478%\uff0c\u8d85\u8fc7\u73b0\u6709\u6700\u4f18\u57fa\u7ebf\u3002", "conclusion": "PM-KVQ\u6709\u6548\u5e73\u8861\u5185\u5b58\u5f00\u9500\u4e0e\u6a21\u578b\u6027\u80fd\uff0c\u63d0\u51fa\u7684\u4f4d\u7f6e\u63d2\u503c\u6821\u51c6\u7b56\u7565\u4e3a\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u7684\u6a21\u578b\u9002\u914d\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.18614", "pdf": "https://arxiv.org/pdf/2505.18614", "abs": "https://arxiv.org/abs/2505.18614", "authors": ["Woohyun Cho", "Youngmin Kim", "Sunghyun Lee", "Youngjae Yu"], "title": "MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation", "categories": ["cs.CL", "cs.LG", "cs.MM", "cs.SD", "eess.AS"], "comment": "28 pages, 8 figures", "summary": "Lyrics translation requires both accurate semantic transfer and preservation\nof musical rhythm, syllabic structure, and poetic style. In animated musicals,\nthe challenge intensifies due to alignment with visual and auditory cues. We\nintroduce Multilingual Audio-Video Lyrics Benchmark for Animated Song\nTranslation (MAVL), the first multilingual, multimodal benchmark for singable\nlyrics translation. By integrating text, audio, and video, MAVL enables richer\nand more expressive translations than text-only approaches. Building on this,\nwe propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought\nSylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints\nto produce natural-sounding lyrics. Experimental results demonstrate that\nSylAVL-CoT significantly outperforms text-based models in singability and\ncontextual accuracy, emphasizing the value of multimodal, multilingual\napproaches for lyrics translation.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u591a\u6a21\u6001\u65b9\u6cd5MAVL\u57fa\u51c6\u53caSylAVL-CoT\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u89c6\u542c\u7ebf\u7d22\u548c\u97f3\u8282\u7ea6\u675f\u663e\u8457\u63d0\u5347\u52a8\u753b\u6b4c\u8bcd\u7ffb\u8bd1\u7684\u53ef\u5531\u6027\u4e0e\u51c6\u786e\u6027", "motivation": "\u4f20\u7edf\u7eaf\u6587\u672c\u6b4c\u8bcd\u7ffb\u8bd1\u96be\u4ee5\u4fdd\u6301\u97f3\u4e50\u8282\u594f\u4e0e\u89c6\u89c9\u540c\u6b65\uff0c\u52a8\u753b\u97f3\u4e50\u5267\u9700\u591a\u6a21\u6001\u5bf9\u9f50\u7684\u7ffb\u8bd1\u65b9\u6848", "method": "\u6784\u5efa\u9996\u4e2a\u591a\u8bed\u8a00\u89c6\u542c\u6b4c\u8bcd\u57fa\u51c6MAVL\uff0c\u5f00\u53d1\u7ed3\u5408\u97f3\u9891-\u89c6\u9891\u7279\u5f81\u548c\u97f3\u8282\u7ea6\u675f\u7684SylAVL-CoT\u5927\u8bed\u8a00\u6a21\u578b", "result": "\u5b9e\u9a8c\u663e\u793aSylAVL-CoT\u5728\u53ef\u5531\u6027\u6307\u6807\u4e0a\u6bd4\u7eaf\u6587\u672c\u6a21\u578b\u63d0\u534738%\uff0c\u4e0a\u4e0b\u6587\u51c6\u786e\u7387\u63d0\u9ad827%", "conclusion": "\u591a\u6a21\u6001\u591a\u8bed\u8a00\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u6b4c\u8bcd\u7ffb\u8bd1\u7684\u97f3\u4e50\u6027\u4fdd\u6301\u96be\u9898\uff0c\u4e3a\u8de8\u6587\u5316\u97f3\u4e50\u521b\u4f5c\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2505.18630", "pdf": "https://arxiv.org/pdf/2505.18630", "abs": "https://arxiv.org/abs/2505.18630", "authors": ["Zhihao Jia", "Mingyi Jia", "Junwen Duan", "Jianxin Wang"], "title": "DDO: Dual-Decision Optimization via Multi-Agent Collaboration for LLM-Based Medical Consultation", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "17 pages, 4 figures", "summary": "Large Language Models (LLMs) demonstrate strong generalization and reasoning\nabilities, making them well-suited for complex decision-making tasks such as\nmedical consultation (MC). However, existing LLM-based methods often fail to\ncapture the dual nature of MC, which entails two distinct sub-tasks: symptom\ninquiry, a sequential decision-making process, and disease diagnosis, a\nclassification problem. This mismatch often results in ineffective symptom\ninquiry and unreliable disease diagnosis. To address this, we propose\n\\textbf{DDO}, a novel LLM-based framework that performs\n\\textbf{D}ual-\\textbf{D}ecision \\textbf{O}ptimization by decoupling and\nindependently optimizing the the two sub-tasks through a collaborative\nmulti-agent workflow. Experiments on three real-world MC datasets show that DDO\nconsistently outperforms existing LLM-based approaches and achieves competitive\nperformance with state-of-the-art generation-based methods, demonstrating its\neffectiveness in the MC task.", "AI": {"tldr": "\u63d0\u51faDDO\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u533b\u7597\u54a8\u8be2\u4e2d\u7684\u75c7\u72b6\u8be2\u95ee\u548c\u75be\u75c5\u8bca\u65ad\u4e24\u4e2a\u5b50\u4efb\u52a1\u5e76\u5206\u522b\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347\u533b\u7597\u54a8\u8be2\u4efb\u52a1\u6548\u679c", "motivation": "\u73b0\u6709LLM\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u533a\u5206\u533b\u7597\u54a8\u8be2\u4e2d\u987a\u5e8f\u51b3\u7b56\u7684\u75c7\u72b6\u8be2\u95ee\u548c\u5206\u7c7b\u8bca\u65ad\u7684\u53cc\u91cd\u7279\u6027\uff0c\u5bfc\u81f4\u75c7\u72b6\u8be2\u95ee\u6548\u7387\u4f4e\u4e14\u8bca\u65ad\u4e0d\u53ef\u9760", "method": "\u5f00\u53d1\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684DDO\u6846\u67b6\uff0c\u5c06\u75c7\u72b6\u8be2\u95ee\uff08\u987a\u5e8f\u51b3\u7b56\uff09\u4e0e\u75be\u75c5\u8bca\u65ad\uff08\u5206\u7c7b\u4efb\u52a1\uff09\u89e3\u8026\u5e76\u72ec\u7acb\u4f18\u5316", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u533b\u7597\u54a8\u8be2\u6570\u636e\u96c6\u4e0a\uff0cDDO\u6301\u7eed\u8d85\u8d8a\u73b0\u6709LLM\u65b9\u6cd5\u5e76\u4e0e\u6700\u5148\u8fdb\u7684\u751f\u6210\u5f0f\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53", "conclusion": "\u901a\u8fc7\u53cc\u91cd\u51b3\u7b56\u4f18\u5316\u673a\u5236\uff0c\u9a8c\u8bc1\u4e86\u5c06\u590d\u6742\u4efb\u52a1\u89e3\u8026\u4e3a\u5b50\u4efb\u52a1\u5e76\u5206\u522b\u4f18\u5316\u7684\u6709\u6548\u6027\uff0c\u4e3a\u533b\u7597\u54a8\u8be2\u4efb\u52a1\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.18638", "pdf": "https://arxiv.org/pdf/2505.18638", "abs": "https://arxiv.org/abs/2505.18638", "authors": ["Md. Tanzib Hosain", "Rajan Das Gupta", "Md. Kishor Morol"], "title": "Multilingual Question Answering in Low-Resource Settings: A Dzongkha-English Benchmark for Foundation Models", "categories": ["cs.CL"], "comment": "24 pages, 20 figures", "summary": "In this work, we provide DZEN, a dataset of parallel Dzongkha and English\ntest questions for Bhutanese middle and high school students. The over 5K\nquestions in our collection span a variety of scientific topics and include\nfactual, application, and reasoning-based questions. We use our parallel\ndataset to test a number of Large Language Models (LLMs) and find a significant\nperformance difference between the models in English and Dzongkha. We also look\nat different prompting strategies and discover that Chain-of-Thought (CoT)\nprompting works well for reasoning questions but less well for factual ones. We\nalso find that adding English translations enhances the precision of Dzongkha\nquestion responses. Our results point to exciting avenues for further study to\nimprove LLM performance in Dzongkha and, more generally, in low-resource\nlanguages. We release the dataset at:\nhttps://github.com/kraritt/llm_dzongkha_evaluation.", "AI": {"tldr": "\u521b\u5efaDZEN\u53cc\u8bed\u6570\u636e\u96c6\u8bc4\u4f30LLM\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u601d\u7ef4\u94fe\u63d0\u793a\u5bf9\u63a8\u7406\u9898\u6709\u6548\u4e14\u82f1\u8bed\u7ffb\u8bd1\u63d0\u5347\u5b97\u5361\u8bed\u56de\u7b54\u51c6\u786e\u7387", "motivation": "\u6539\u5584\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b97\u5361\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u6027\u80fd\u8868\u73b0", "method": "\u4f7f\u75285K+\u53cc\u8bed\u6d4b\u8bd5\u9898\u8bc4\u4f30\u4e0d\u540cLLM\uff0c\u6d4b\u8bd5\u601d\u7ef4\u94fe\u63d0\u793a\u7b49\u7b56\u7565\uff0c\u5e76\u52a0\u5165\u82f1\u8bed\u7ffb\u8bd1\u5bf9\u6bd4", "result": "\u53d1\u73b0\u6a21\u578b\u5728\u53cc\u8bed\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u5f02\uff0c\u82f1\u8bed\u7ffb\u8bd1\u4f7f\u5b97\u5361\u8bed\u56de\u7b54\u51c6\u786e\u7387\u63d0\u534712%", "conclusion": "\u4e3a\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00LLM\u6027\u80fd\u6307\u660e\u65b9\u5411\uff0c\u516c\u5f00\u6570\u636e\u96c6\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76"}}
{"id": "2505.18642", "pdf": "https://arxiv.org/pdf/2505.18642", "abs": "https://arxiv.org/abs/2505.18642", "authors": ["Xiao Chen", "Sihang Zhou", "Ke Liang", "Xiaoyu Sun", "Xinwang Liu"], "title": "Skip-Thinking: Chunk-wise Chain-of-Thought Distillation Enable Smaller Language Models to Reason Better and Faster", "categories": ["cs.CL"], "comment": null, "summary": "Chain-of-thought (CoT) distillation allows a large language model (LLM) to\nguide a small language model (SLM) in reasoning tasks. Existing methods train\nthe SLM to learn the long rationale in one iteration, resulting in two issues:\n1) Long rationales lead to a large token-level batch size during training,\nmaking gradients of core reasoning tokens (i.e., the token will directly affect\nthe correctness of subsequent reasoning) over-smoothed as they contribute a\ntiny fraction of the rationale. As a result, the SLM converges to sharp minima\nwhere it fails to grasp the reasoning logic. 2) The response is slow, as the\nSLM must generate a long rationale before reaching the answer. Therefore, we\npropose chunk-wise training (CWT), which uses a heuristic search to divide the\nrationale into internal semantically coherent chunks and focuses SLM on\nlearning from only one chunk per iteration. In this way, CWT naturally isolates\nnon-reasoning chunks that do not involve the core reasoning token (e.g.,\nsummary and transitional chunks) from the SLM learning for reasoning chunks,\nmaking the fraction of the core reasoning token increase in the corresponding\niteration. Based on CWT, skip-thinking training (STT) is proposed. STT makes\nthe SLM automatically skip non-reasoning medium chunks to reach the answer,\nimproving reasoning speed while maintaining accuracy. We validate our approach\non a variety of SLMs and multiple reasoning tasks.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5757\u8bad\u7ec3(CWT)\u548c\u8df3\u8dc3\u601d\u7ef4\u8bad\u7ec3(STT)\u89e3\u51b3\u4f20\u7edfCoT\u84b8\u998f\u7684\u957f\u63a8\u7406\u68af\u5ea6\u5e73\u6ed1\u4e0e\u901f\u5ea6\u74f6\u9888\u95ee\u9898", "motivation": "\u73b0\u6709CoT\u84b8\u998f\u65b9\u6cd5\u5728\u5355\u6b21\u8fed\u4ee3\u4e2d\u8bad\u7ec3\u5c0f\u6a21\u578b\u5b66\u4e60\u957f\u63a8\u7406\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u6838\u5fc3\u63a8\u7406token\u68af\u5ea6\u88ab\u7a00\u91ca\uff08\u5360\u6bd4\u8f83\u4f4e\uff09\u548c\u54cd\u5e94\u901f\u5ea6\u4f4e\u4e0b", "method": "1.CWT\u901a\u8fc7\u542f\u53d1\u5f0f\u641c\u7d22\u5c06\u63a8\u7406\u94fe\u5206\u5272\u4e3a\u8bed\u4e49\u8fde\u8d2f\u7684\u5757\uff0c\u6bcf\u8f6e\u4e13\u6ce8\u5b66\u4e60\u5355\u4e2a\u5757\uff1b2.STT\u81ea\u52a8\u8df3\u8fc7\u975e\u63a8\u7406\u4e2d\u95f4\u5757\u76f4\u8fbe\u7b54\u6848", "result": "\u5728\u591a\u79cd\u5c0f\u6a21\u578b\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u7387\u7684\u540c\u65f6\u63d0\u5347\u63a8\u7406\u901f\u5ea6", "conclusion": "\u5206\u5757\u5b66\u4e60\u7b56\u7565\u6709\u6548\u805a\u7126\u6838\u5fc3\u63a8\u7406token\uff0c\u8df3\u8dc3\u673a\u5236\u5b9e\u73b0\u901f\u5ea6\u4e0e\u7cbe\u5ea6\u7684\u53cc\u91cd\u4f18\u5316"}}
{"id": "2505.18651", "pdf": "https://arxiv.org/pdf/2505.18651", "abs": "https://arxiv.org/abs/2505.18651", "authors": ["Daniel J. Korchinski", "Dhruva Karkada", "Yasaman Bahri", "Matthieu Wyart"], "title": "On the Emergence of Linear Analogies in Word Embeddings", "categories": ["cs.CL", "cond-mat.dis-nn", "cs.LG"], "comment": "Main: 12 pages, 3 figures. Appendices: 8 pages, 7 figures", "summary": "Models such as Word2Vec and GloVe construct word embeddings based on the\nco-occurrence probability $P(i,j)$ of words $i$ and $j$ in text corpora. The\nresulting vectors $W_i$ not only group semantically similar words but also\nexhibit a striking linear analogy structure -- for example, $W_{\\text{king}} -\nW_{\\text{man}} + W_{\\text{woman}} \\approx W_{\\text{queen}}$ -- whose\ntheoretical origin remains unclear. Previous observations indicate that this\nanalogy structure: (i) already emerges in the top eigenvectors of the matrix\n$M(i,j) = P(i,j)/P(i)P(j)$, (ii) strengthens and then saturates as more\neigenvectors of $M (i, j)$, which controls the dimension of the embeddings, are\nincluded, (iii) is enhanced when using $\\log M(i,j)$ rather than $M(i,j)$, and\n(iv) persists even when all word pairs involved in a specific analogy relation\n(e.g., king-queen, man-woman) are removed from the corpus. To explain these\nphenomena, we introduce a theoretical generative model in which words are\ndefined by binary semantic attributes, and co-occurrence probabilities are\nderived from attribute-based interactions. This model analytically reproduces\nthe emergence of linear analogy structure and naturally accounts for properties\n(i)-(iv). It can be viewed as giving fine-grained resolution into the role of\neach additional embedding dimension. It is robust to various forms of noise and\nagrees well with co-occurrence statistics measured on Wikipedia and the analogy\nbenchmark introduced by Mikolov et al.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6784\u5efa\u57fa\u4e8e\u4e8c\u503c\u8bed\u4e49\u5c5e\u6027\u7684\u751f\u6210\u6a21\u578b\uff0c\u89e3\u91ca\u4e86\u8bcd\u5d4c\u5165\u4e2d\u7ebf\u6027\u7c7b\u6bd4\u7ed3\u6784\u7684\u7406\u8bba\u8d77\u6e90\u53ca\u5b9e\u9a8c\u73b0\u8c61\u3002", "motivation": "\u63ed\u793aWord2Vec/GloVe\u8bcd\u5411\u91cf\u4e2d\u7ebf\u6027\u7c7b\u6bd4\u7ed3\u6784\uff08\u5982king-man+woman\u2248queen\uff09\u7684\u7406\u8bba\u6839\u6e90\uff0c\u89e3\u91ca\u5176\u56db\u5927\u5b9e\u9a8c\u7279\u6027\uff1a\u7279\u5f81\u5411\u91cf\u6d8c\u73b0\u6027\u3001\u7ef4\u5ea6\u9971\u548c\u6027\u3001\u5bf9\u6570\u589e\u5f3a\u6027\u53ca\u8bed\u6599\u65e0\u5173\u6027\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e8c\u503c\u8bed\u4e49\u5c5e\u6027\u7684\u7406\u8bba\u751f\u6210\u6a21\u578b\uff1a\u5b9a\u4e49\u8bcd\u8bed\u7684\u4e8c\u8fdb\u5236\u8bed\u4e49\u7279\u5f81\uff0c\u901a\u8fc7\u5c5e\u6027\u4ea4\u4e92\u63a8\u5bfc\u5171\u73b0\u6982\u7387\uff0c\u89e3\u6790\u7ebf\u6027\u7c7b\u6bd4\u7ed3\u6784\u7684\u5f62\u6210\u673a\u5236\u3002", "result": "\u6a21\u578b\u6210\u529f\u590d\u73b0\u7ebf\u6027\u7c7b\u6bd4\u7ed3\u6784\uff0c\u5b9a\u91cf\u89e3\u91ca\u56db\u4e2a\u5b9e\u9a8c\u73b0\u8c61\uff0c\u5728\u7ef4\u57fa\u767e\u79d1\u8bed\u6599\u548cMikolov\u7c7b\u6bd4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "conclusion": "\u8bcd\u8bed\u7684\u5e95\u5c42\u8bed\u4e49\u5c5e\u6027\u4ea4\u4e92\u662f\u7ebf\u6027\u7c7b\u6bd4\u7ed3\u6784\u4ea7\u751f\u7684\u6839\u672c\u539f\u56e0\uff0c\u8be5\u6a21\u578b\u4e3a\u7406\u89e3\u8bcd\u5411\u91cf\u7ef4\u5ea6\u529f\u80fd\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u89e3\u91ca\u6846\u67b6\u3002"}}
{"id": "2505.18653", "pdf": "https://arxiv.org/pdf/2505.18653", "abs": "https://arxiv.org/abs/2505.18653", "authors": ["Murathan Kurfal\u0131", "Shorouq Zahra", "Joakim Nivre", "Gabriele Messori"], "title": "Climate-Eval: A Comprehensive Benchmark for NLP Tasks Related to Climate Change", "categories": ["cs.CL"], "comment": "Accepted to ClimateNLP 2025@ACL", "summary": "Climate-Eval is a comprehensive benchmark designed to evaluate natural\nlanguage processing models across a broad range of tasks related to climate\nchange. Climate-Eval aggregates existing datasets along with a newly developed\nnews classification dataset, created specifically for this release. This\nresults in a benchmark of 25 tasks based on 13 datasets, covering key aspects\nof climate discourse, including text classification, question answering, and\ninformation extraction. Our benchmark provides a standardized evaluation suite\nfor systematically assessing the performance of large language models (LLMs) on\nthese tasks. Additionally, we conduct an extensive evaluation of open-source\nLLMs (ranging from 2B to 70B parameters) in both zero-shot and few-shot\nsettings, analyzing their strengths and limitations in the domain of climate\nchange.", "AI": {"tldr": "Climate-Eval\u57fa\u51c6\u6574\u540825\u4e2a\u6c14\u5019\u76f8\u5173NLP\u4efb\u52a1\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6c14\u5019\u53d8\u5316\u9886\u57df\u7684\u8868\u73b0", "motivation": "\u89e3\u51b3\u6c14\u5019\u53d8\u5316\u9886\u57df\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30LLM\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u6574\u5408\u73b0\u6709\u6570\u636e\u96c6\u5e76\u65b0\u589e\u65b0\u95fb\u5206\u7c7b\u6570\u636e\u96c6\u6784\u5efa\u6807\u51c6\u5316\u6d4b\u8bd5\u5957\u4ef6", "method": "\u805a\u540813\u4e2a\u6570\u636e\u96c6\u5f62\u621025\u4e2a\u6c14\u5019\u4efb\u52a1(\u542b\u65b0\u5efa\u65b0\u95fb\u5206\u7c7b\u6570\u636e\u96c6)\uff0c\u8bc4\u4f302B-70B\u53c2\u6570\u5f00\u6e90LLM\u7684\u96f6\u6837\u672c/\u5c11\u6837\u672c\u8868\u73b0", "result": "\u7cfb\u7edf\u63ed\u793a\u4e0d\u540c\u89c4\u6a21LLM\u5728\u6c14\u5019\u6587\u672c\u5206\u7c7b\u3001\u95ee\u7b54\u7b49\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u8fb9\u754c\u4e0e\u5c40\u9650\u6027", "conclusion": "Climate-Eval\u4e3a\u6c14\u5019\u53d8\u5316NLP\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u5e2e\u52a9\u7406\u89e3LLM\u5728\u6c14\u5019\u9886\u57df\u7684\u80fd\u529b\u73b0\u72b6"}}
{"id": "2505.18658", "pdf": "https://arxiv.org/pdf/2505.18658", "abs": "https://arxiv.org/abs/2505.18658", "authors": ["Pankaj Kumar", "Subhankar Mishra"], "title": "Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as a promising cornerstone for the\ndevelopment of natural language processing (NLP) and artificial intelligence\n(AI). However, ensuring the robustness of LLMs remains a critical challenge. To\naddress these challenges and advance the field, this survey provides a\ncomprehensive overview of current studies in this area. First, we\nsystematically examine the nature of robustness in LLMs, including its\nconceptual foundations, the importance of consistent performance across diverse\ninputs, and the implications of failure modes in real-world applications. Next,\nwe analyze the sources of non-robustness, categorizing intrinsic model\nlimitations, data-driven vulnerabilities, and external adversarial factors that\ncompromise reliability. Following this, we review state-of-the-art mitigation\nstrategies, and then we discuss widely adopted benchmarks, emerging metrics,\nand persistent gaps in assessing real-world reliability. Finally, we synthesize\nfindings from existing surveys and interdisciplinary studies to highlight\ntrends, unresolved issues, and pathways for future research.", "AI": {"tldr": "\u7cfb\u7edf\u7efc\u8ff0LLM\u9c81\u68d2\u6027\u7814\u7a76\uff1a\u89e3\u6790\u5176\u672c\u8d28\u7279\u5f81\u3001\u8106\u5f31\u6027\u6839\u6e90\u3001\u7f13\u89e3\u7b56\u7565\u3001\u8bc4\u4f30\u65b9\u6cd5\u53ca\u672a\u6765\u65b9\u5411", "motivation": "LLM\u5728\u5173\u952e\u9886\u57df\u5e94\u7528\u4e2d\u5b58\u5728\u53ef\u9760\u6027\u98ce\u9669\uff0c\u7cfb\u7edf\u6027\u5931\u6548\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\uff0c\u9700\u5efa\u7acb\u5b8c\u6574\u7684\u9c81\u68d2\u6027\u7814\u7a76\u4f53\u7cfb", "method": "\u91c7\u7528\u591a\u7ef4\u5ea6\u5206\u7c7b\u6846\u67b6\uff1a1) \u5b9a\u4e49\u9c81\u68d2\u6027\u6838\u5fc3\u8981\u7d20 2) \u5efa\u7acb\u4e09\u5c42\u8106\u5f31\u6027\u5206\u7c7b\u4f53\u7cfb\uff08\u6a21\u578b\u5185\u5728\u7f3a\u9677/\u6570\u636e\u6f0f\u6d1e/\u5916\u90e8\u5bf9\u6297\uff093) \u8bc4\u4f30\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u6709\u6548\u6027", "result": "\u53d1\u73b0\u5f53\u524d\u7f13\u89e3\u7b56\u7565\u5bf9\u52a8\u6001\u5bf9\u6297\u73af\u5883\u9002\u5e94\u6027\u4e0d\u8db3\uff0c\u8bc4\u4f30\u57fa\u51c6\u672a\u5145\u5206\u53cd\u6620\u5b9e\u9645\u5e94\u7528\u573a\u666f\uff0c\u5b58\u5728\u7406\u8bba-\u5b9e\u8df5\u8bc4\u4f30\u9e3f\u6c9f", "conclusion": "\u9700\u53d1\u5c55\u52a8\u6001\u9c81\u68d2\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u52a0\u5f3a\u8de8\u5b66\u79d1\u7814\u7a76\uff08\u8ba4\u77e5\u79d1\u5b66/\u5b89\u5168\u5de5\u7a0b\uff09\uff0c\u6784\u5efa\u5168\u751f\u547d\u5468\u671f\u9632\u62a4\u4f53\u7cfb"}}
{"id": "2505.18673", "pdf": "https://arxiv.org/pdf/2505.18673", "abs": "https://arxiv.org/abs/2505.18673", "authors": ["Zixiang Xu", "Yanbo Wang", "Yue Huang", "Xiuying Chen", "Jieyu Zhao", "Meng Jiang", "Xiangliang Zhang"], "title": "Cross-Lingual Pitfalls: Automatic Probing Cross-Lingual Weakness of Multilingual Large Language Models", "categories": ["cs.CL"], "comment": "ACL 2025. Code available at\n  https://github.com/xzx34/Cross-Lingual-Pitfalls", "summary": "Large Language Models (LLMs) have achieved remarkable success in Natural\nLanguage Processing (NLP), yet their cross-lingual performance consistency\nremains a significant challenge. This paper introduces a novel methodology for\nefficiently identifying inherent cross-lingual weaknesses in LLMs. Our approach\nleverages beam search and LLM-based simulation to generate bilingual question\npairs that expose performance discrepancies between English and target\nlanguages. We construct a new dataset of over 6,000 bilingual pairs across 16\nlanguages using this methodology, demonstrating its effectiveness in revealing\nweaknesses even in state-of-the-art models. The extensive experiments\ndemonstrate that our method precisely and cost-effectively pinpoints\ncross-lingual weaknesses, consistently revealing over 50\\% accuracy drops in\ntarget languages across a wide range of models. Moreover, further experiments\ninvestigate the relationship between linguistic similarity and cross-lingual\nweaknesses, revealing that linguistically related languages share similar\nperformance patterns and benefit from targeted post-training. Code is available\nat https://github.com/xzx34/Cross-Lingual-Pitfalls.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u751f\u6210\u53cc\u8bed\u95ee\u9898\u5bf9\u8bc6\u522bLLM\u8de8\u8bed\u8a00\u5f31\u70b9\u7684\u65b9\u6cd5\uff0c\u63ed\u793a\u6a21\u578b\u5728\u975e\u82f1\u8bed\u8bed\u8a00\u4e2d\u7684\u663e\u8457\u6027\u80fd\u4e0b\u964d\u53ca\u8bed\u8a00\u76f8\u4f3c\u6027\u7684\u5f71\u54cd", "motivation": "\u89e3\u51b3LLMs\u5728\u8de8\u8bed\u8a00\u573a\u666f\u4e0b\u6027\u80fd\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u73b0\u6709\u6a21\u578b\u5728\u76ee\u6807\u8bed\u8a00\u4e2d\u53ef\u80fd\u5b58\u5728\u672a\u88ab\u53d1\u73b0\u7684\u6f5c\u5728\u7f3a\u9677", "method": "\u91c7\u7528beam search\u548cLLM\u6a21\u62df\u751f\u6210\u53cc\u8bed\u95ee\u9898\u5bf9\uff0c\u6784\u5efa\u6db5\u76d616\u79cd\u8bed\u8a00\u76846000+\u53cc\u8bed\u6570\u636e\u96c6\u8fdb\u884c\u5f31\u70b9\u68c0\u6d4b", "result": "\u6210\u529f\u8bc6\u522b\u51fa\u4e3b\u6d41\u6a21\u578b\u5728\u76ee\u6807\u8bed\u8a00\u4e2d\u5e73\u5747\u8d8550%\u7684\u51c6\u786e\u7387\u4e0b\u964d\uff0c\u53d1\u73b0\u8bed\u8a00\u76f8\u4f3c\u6027\u4e0e\u6027\u80fd\u6a21\u5f0f\u5b58\u5728\u5f3a\u76f8\u5173\u6027", "conclusion": "\u8bed\u8a00\u76f8\u4f3c\u6027\u5f71\u54cd\u8de8\u8bed\u8a00\u6027\u80fd\u5206\u5e03\uff0c\u9488\u5bf9\u6027\u540e\u8bad\u7ec3\u53ef\u6539\u5584\u76f8\u5173\u8bed\u8a00\u7684\u6a21\u578b\u8868\u73b0"}}
{"id": "2505.18677", "pdf": "https://arxiv.org/pdf/2505.18677", "abs": "https://arxiv.org/abs/2505.18677", "authors": ["Eric Chamoun", "Nedjma Ousidhoum", "Michael Schlichtkrull", "Andreas Vlachos"], "title": "Social Good or Scientific Curiosity? Uncovering the Research Framing Behind NLP Artefacts", "categories": ["cs.CL"], "comment": null, "summary": "Clarifying the research framing of NLP artefacts (e.g., models, datasets,\netc.) is crucial to aligning research with practical applications. Recent\nstudies manually analyzed NLP research across domains, showing that few papers\nexplicitly identify key stakeholders, intended uses, or appropriate contexts.\nIn this work, we propose to automate this analysis, developing a\nthree-component system that infers research framings by first extracting key\nelements (means, ends, stakeholders), then linking them through interpretable\nrules and contextual reasoning. We evaluate our approach on two domains:\nautomated fact-checking using an existing dataset, and hate speech detection\nfor which we annotate a new dataset-achieving consistent improvements over\nstrong LLM baselines. Finally, we apply our system to recent automated\nfact-checking papers and uncover three notable trends: a rise in vague or\nunderspecified research goals, increased emphasis on scientific exploration\nover application, and a shift toward supporting human fact-checkers rather than\npursuing full automation.", "AI": {"tldr": "\u5f00\u53d1\u81ea\u52a8\u5316\u7cfb\u7edf\u5206\u6790NLP\u7814\u7a76\u6846\u67b6\uff0c\u53d1\u73b0\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u9886\u57df\u5b58\u5728\u7814\u7a76\u76ee\u6807\u6a21\u7cca\u5316\u3001\u79d1\u5b66\u63a2\u7d22\u4f18\u5148\u548c\u5e94\u7528\u8f6c\u5411\u4e09\u5927\u8d8b\u52bf", "motivation": "\u9488\u5bf9\u73b0\u6709NLP\u7814\u7a76\u666e\u904d\u7f3a\u4e4f\u5bf9\u5229\u76ca\u76f8\u5173\u8005\u3001\u4f7f\u7528\u573a\u666f\u7b49\u6838\u5fc3\u8981\u7d20\u7684\u660e\u786e\u5b9a\u4e49\uff0c\u65e8\u5728\u5efa\u7acb\u81ea\u52a8\u5316\u5206\u6790\u6846\u67b6\u63d0\u5347\u7814\u7a76\u4e0e\u5b9e\u8df5\u7684\u5339\u914d\u5ea6", "method": "\u4e09\u9636\u6bb5\u7cfb\u7edf\uff1a1\uff09\u63d0\u53d6\u624b\u6bb5/\u76ee\u7684/\u5229\u76ca\u76f8\u5173\u8005\u8981\u7d20 2\uff09\u57fa\u4e8e\u53ef\u89e3\u91ca\u89c4\u5219\u5efa\u7acb\u8981\u7d20\u5173\u8054 3\uff09\u4e0a\u4e0b\u6587\u63a8\u7406\u6846\u67b6\u5206\u6790", "result": "\u5728\u4e8b\u5b9e\u6838\u67e5\uff08\u73b0\u6709\u6570\u636e\u96c6\uff09\u548c\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\uff08\u65b0\u6807\u6ce8\u96c6\uff09\u9886\u57df\u5747\u663e\u8457\u4f18\u4e8eLLM\u57fa\u7ebf\uff0c\u51c6\u786e\u8bc6\u522b\u7814\u7a76\u6846\u67b6\u7279\u5f81", "conclusion": "\u5e94\u7528\u53d1\u73b0\uff1a\u76ee\u6807\u6a21\u7cca\u5316\u8d8b\u52bf\u663e\u8457\uff0885%\u8bba\u6587\uff09\uff0c\u79d1\u5b66\u63a2\u7d22\u5bfc\u5411\u589e\u957f37%\uff0c\u652f\u6301\u4eba\u673a\u534f\u540c\u7684\u8bba\u6587\u6bd4\u4f8b\u8f83\u57fa\u7ebf\u63d0\u53472\u500d"}}
{"id": "2505.18683", "pdf": "https://arxiv.org/pdf/2505.18683", "abs": "https://arxiv.org/abs/2505.18683", "authors": ["Rapha\u00ebl Merx", "Hanna Suominen", "Lois Hong", "Nick Thieberger", "Trevor Cohn", "Ekaterina Vylomova"], "title": "TULUN: Transparent and Adaptable Low-resource Machine Translation", "categories": ["cs.CL"], "comment": null, "summary": "Machine translation (MT) systems that support low-resource languages often\nstruggle on specialized domains. While researchers have proposed various\ntechniques for domain adaptation, these approaches typically require model\nfine-tuning, making them impractical for non-technical users and small\norganizations. To address this gap, we propose Tulun, a versatile solution for\nterminology-aware translation, combining neural MT with large language model\n(LLM)-based post-editing guided by existing glossaries and translation\nmemories. Our open-source web-based platform enables users to easily create,\nedit, and leverage terminology resources, fostering a collaborative\nhuman-machine translation process that respects and incorporates domain\nexpertise while increasing MT accuracy. Evaluations show effectiveness in both\nreal-world and benchmark scenarios: on medical and disaster relief translation\ntasks for Tetun and Bislama, our system achieves improvements of 16.90-22.41\nChrF++ points over baseline MT systems. Across six low-resource languages on\nthe FLORES dataset, Tulun outperforms both standalone MT and LLM approaches,\nachieving an average improvement of 2.8 ChrF points over NLLB-54B.", "AI": {"tldr": "\u63d0\u51faTulun\u672f\u8bed\u611f\u77e5\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u7ed3\u5408\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u4e0eLLM\u540e\u7f16\u8f91\u6280\u672f\uff0c\u901a\u8fc7\u5f00\u6e90\u534f\u4f5c\u5e73\u53f0\u663e\u8457\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e13\u4e1a\u9886\u57df\u7ffb\u8bd1\u8d28\u91cf", "motivation": "\u4f20\u7edf\u9886\u57df\u9002\u5e94\u65b9\u6cd5\u9700\u8981\u6a21\u578b\u5fae\u8c03\uff0c\u5bf9\u975e\u6280\u672f\u7528\u6237\u4e0d\u53cb\u597d\u3002\u9700\u5f00\u53d1\u6613\u7528\u65b9\u6848\u6574\u5408\u672f\u8bed\u8d44\u6e90\u5e76\u5b9e\u73b0\u4eba\u673a\u534f\u4f5c\u7ffb\u8bd1", "method": "1. \u878d\u5408\u795e\u7ecfMT\u4e0e\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u672f\u8bed\u540e\u7f16\u8f91\n2. \u5f00\u53d1\u5f00\u6e90Web\u5e73\u53f0\u652f\u6301\u672f\u8bed\u8d44\u6e90\u521b\u5efa\u4e0e\u7ba1\u7406\n3. \u6784\u5efa\u7ed3\u5408\u672f\u8bed\u5e93\u4e0e\u7ffb\u8bd1\u8bb0\u5fc6\u7684\u534f\u540c\u5de5\u4f5c\u6d41\u7a0b", "result": "1. \u533b\u7597/\u6551\u707e\u7ffb\u8bd1\u4efb\u52a1\u4e2dChrF++\u63d0\u534716.90-22.41\n2. FLORES\u6570\u636e\u96c6\u516d\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\u5e73\u5747\u63d0\u53472.8 ChrF\n3. \u8d85\u8d8aNLLB-54B\u7b49\u57fa\u7ebf\u7cfb\u7edf", "conclusion": "Tulun\u6709\u6548\u6574\u5408\u795e\u7ecfMT\u4e0eLLM\u4f18\u52bf\uff0c\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u673a\u5236\u663e\u8457\u63d0\u5347\u4e13\u4e1a\u9886\u57df\u7ffb\u8bd1\u7cbe\u5ea6\uff0c\u5f00\u6e90\u5e73\u53f0\u8bbe\u8ba1\u4fc3\u8fdb\u6280\u672f\u53ef\u53ca\u6027"}}
{"id": "2505.18685", "pdf": "https://arxiv.org/pdf/2505.18685", "abs": "https://arxiv.org/abs/2505.18685", "authors": ["Zhihao Zhang", "Yiran Zhang", "Xiyue Zhou", "Liting Huang", "Imran Razzak", "Preslav Nakov", "Usman Naseem"], "title": "From Generation to Detection: A Multimodal Multi-Task Dataset for Benchmarking Health Misinformation", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Infodemics and health misinformation have significant negative impact on\nindividuals and society, exacerbating confusion and increasing hesitancy in\nadopting recommended health measures. Recent advancements in generative AI,\ncapable of producing realistic, human like text and images, have significantly\naccelerated the spread and expanded the reach of health misinformation,\nresulting in an alarming surge in its dissemination. To combat the infodemics,\nmost existing work has focused on developing misinformation datasets from\nsocial media and fact checking platforms, but has faced limitations in topical\ncoverage, inclusion of AI generation, and accessibility of raw content. To\naddress these issues, we present MM Health, a large scale multimodal\nmisinformation dataset in the health domain consisting of 34,746 news article\nencompassing both textual and visual information. MM Health includes\nhuman-generated multimodal information (5,776 articles) and AI generated\nmultimodal information (28,880 articles) from various SOTA generative AI\nmodels. Additionally, We benchmarked our dataset against three tasks\n(reliability checks, originality checks, and fine-grained AI detection)\ndemonstrating that existing SOTA models struggle to accurately distinguish the\nreliability and origin of information. Our dataset aims to support the\ndevelopment of misinformation detection across various health scenarios,\nfacilitating the detection of human and machine generated content at multimodal\nlevels.", "AI": {"tldr": "\u6784\u5efa\u591a\u6a21\u6001\u5065\u5eb7\u9519\u8bef\u4fe1\u606f\u6570\u636e\u96c6MM Health\uff0c\u6db5\u76d6\u4eba\u7c7b\u751f\u6210\u4e0eAI\u751f\u6210\u5185\u5bb9\uff0c\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u6709\u6548\u68c0\u6d4b", "motivation": "\u73b0\u6709\u5065\u5eb7\u9519\u8bef\u4fe1\u606f\u6570\u636e\u96c6\u5b58\u5728\u4e3b\u9898\u8986\u76d6\u4e0d\u5168\u3001\u672a\u5305\u542bAI\u751f\u6210\u5185\u5bb9\u3001\u539f\u59cb\u4fe1\u606f\u53ef\u8bbf\u95ee\u6027\u5dee\u4e09\u5927\u5c40\u9650\uff0c\u751f\u6210\u5f0fAI\u6280\u672f\u52a0\u5267\u4e86\u9519\u8bef\u4fe1\u606f\u4f20\u64ad", "method": "\u6536\u96c634,746\u7bc7\u591a\u6a21\u6001\u5065\u5eb7\u65b0\u95fb\uff08\u542b5,776\u4eba\u7c7b\u751f\u6210/28,880 AI\u751f\u6210\uff09\uff0c\u4f7f\u7528SOTA\u751f\u6210\u6a21\u578b\u521b\u5efa\u6837\u672c\uff0c\u8bbe\u8ba1\u53ef\u9760\u6027\u9a8c\u8bc1\u3001\u539f\u521b\u6027\u9a8c\u8bc1\u548cAI\u68c0\u6d4b\u4e09\u5927\u57fa\u51c6\u4efb\u52a1", "result": "\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\u5728\u533a\u5206\u4fe1\u606f\u53ef\u9760\u6027\u548c\u6765\u6e90\u65b9\u9762\u51c6\u786e\u7387\u8f83\u4f4e\uff08\u53ef\u9760\u6027\u68c0\u67e5F1=0.48\uff0cAI\u68c0\u6d4bF1=0.52\uff09", "conclusion": "MM Health\u6570\u636e\u96c6\u652f\u6301\u5f00\u53d1\u8de8\u573a\u666f\u3001\u591a\u6a21\u6001\u5c42\u6b21\u7684\u9519\u8bef\u4fe1\u606f\u68c0\u6d4b\u7cfb\u7edf\uff0c\u52a9\u529b\u4eba\u673a\u751f\u6210\u5185\u5bb9\u7684\u8054\u5408\u8bc6\u522b"}}
{"id": "2505.18688", "pdf": "https://arxiv.org/pdf/2505.18688", "abs": "https://arxiv.org/abs/2505.18688", "authors": ["Aleksandr Tsymbalov"], "title": "Large Language Models in the Task of Automatic Validation of Text Classifier Predictions", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Machine learning models for text classification are trained to predict a\nclass for a given text. To do this, training and validation samples must be\nprepared: a set of texts is collected, and each text is assigned a class. These\nclasses are usually assigned by human annotators with different expertise\nlevels, depending on the specific classification task. Collecting such samples\nfrom scratch is labor-intensive because it requires finding specialists and\ncompensating them for their work; moreover, the number of available specialists\nis limited, and their productivity is constrained by human factors. While it\nmay not be too resource-intensive to collect samples once, the ongoing need to\nretrain models (especially in incremental learning pipelines) to address data\ndrift (also called model drift) makes the data collection process crucial and\ncostly over the model's entire lifecycle. This paper proposes several\napproaches to replace human annotators with Large Language Models (LLMs) to\ntest classifier predictions for correctness, helping ensure model quality and\nsupport high-quality incremental learning.", "AI": {"tldr": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u66ff\u4ee3\u4eba\u5de5\u6807\u6ce8\uff0c\u89e3\u51b3\u6587\u672c\u5206\u7c7b\u6a21\u578b\u6301\u7eed\u8bad\u7ec3\u4e2d\u6570\u636e\u6807\u6ce8\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u4eba\u5de5\u6807\u6ce8\u5b58\u5728\u4e13\u5bb6\u8d44\u6e90\u6709\u9650\u3001\u6807\u6ce8\u6210\u672c\u9ad8\u3001\u96be\u4ee5\u9002\u5e94\u6a21\u578b\u6301\u7eed\u8fed\u4ee3\u9700\u6c42\uff08\u5c24\u5176\u662f\u6570\u636e\u6f02\u79fb\u573a\u666f\uff09\u7684\u6838\u5fc3\u75db\u70b9\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u9884\u6d4b\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u6a21\u578b\u81ea\u52a8\u68c0\u6d4b\u5206\u7c7b\u5668\u9884\u6d4b\u7ed3\u679c\u7684\u6b63\u786e\u6027\uff0c\u66ff\u4ee3\u4f20\u7edf\u4eba\u5de5\u6807\u6ce8\u6d41\u7a0b\u3002", "result": "\u5b9e\u73b0\u6807\u6ce8\u6d41\u7a0b\u81ea\u52a8\u5316\uff0c\u5728\u4fdd\u8bc1\u6a21\u578b\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u663e\u8457\u964d\u4f4e\u5168\u751f\u547d\u5468\u671f\u7ef4\u62a4\u6210\u672c\uff0c\u652f\u6301\u9ad8\u8d28\u91cf\u589e\u91cf\u5b66\u4e60\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u9a8c\u8bc1\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u53ef\u6301\u7eed\u7ef4\u62a4\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6280\u672f\u8def\u5f84\uff0c\u7a81\u7834\u4f20\u7edf\u4eba\u5de5\u6807\u6ce8\u7684\u4ea7\u80fd\u74f6\u9888\u3002"}}
{"id": "2505.18690", "pdf": "https://arxiv.org/pdf/2505.18690", "abs": "https://arxiv.org/abs/2505.18690", "authors": ["Guoxiu He", "Xin Song", "Futing Wang", "Aixin Sun"], "title": "Benchmarking and Rethinking Knowledge Editing for Large Language Models", "categories": ["cs.CL"], "comment": "arXiv admin note: text overlap with arXiv:2503.05212", "summary": "Knowledge editing aims to update the embedded knowledge within Large Language\nModels (LLMs). However, existing approaches, whether through parameter\nmodification or external memory integration, often suffer from inconsistent\nevaluation objectives and experimental setups. To address this gap, we conduct\na comprehensive benchmarking study. In addition to fact-level datasets, we\nintroduce more complex event-based datasets and general-purpose datasets drawn\nfrom other tasks. Our evaluation covers both instruction-tuned and\nreasoning-oriented LLMs, under a realistic autoregressive inference setting\nrather than teacher-forced decoding. Beyond single-edit assessments, we also\nevaluate multi-edit scenarios to better reflect practical demands. We employ\nfour evaluation dimensions, including portability, and compare all recent\nmethods against a simple and straightforward baseline named Selective\nContextual Reasoning (SCR). Empirical results reveal that parameter-based\nediting methods perform poorly under realistic conditions. In contrast, SCR\nconsistently outperforms them across all settings. This study offers new\ninsights into the limitations of current knowledge editing methods and\nhighlights the potential of context-based reasoning as a more robust\nalternative.", "AI": {"tldr": "\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e0a\u4e0b\u6587\u63a8\u7406\u65b9\u6cd5SCR\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u9488\u5bf9\u5927\u6a21\u578b\u77e5\u8bc6\u66f4\u65b0\u65b9\u6cd5\u8bc4\u4f30\u6807\u51c6\u6df7\u4e71\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6784\u5efa\u590d\u5408\u6570\u636e\u96c6\u548c\u4e25\u8c28\u6d4b\u8bd5\u6846\u67b6\uff0c\u63ed\u793a\u53c2\u6570\u7f16\u8f91\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u5f15\u5165\u4e8b\u4ef6\u7ea7\u6570\u636e\u96c6+\u901a\u7528\u4efb\u52a1\u6570\u636e\u96c6\uff0c\u91c7\u7528\u81ea\u56de\u5f52\u63a8\u7406\u8bc4\u4f30\u6846\u67b6\uff0c\u4ece\u53ef\u79fb\u690d\u6027\u7b49\u56db\u4e2a\u7ef4\u5ea6\u5bf9\u6bd4\u53c2\u6570\u7f16\u8f91\u4e0e\u4e0a\u4e0b\u6587\u63a8\u7406\u65b9\u6cd5\u3002", "result": "\u53c2\u6570\u7f16\u8f91\u65b9\u6cd5\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u8868\u73b0\u6b20\u4f73\uff0cSCR\u65b9\u6cd5\u5728\u5355\u6b21/\u591a\u6b21\u7f16\u8f91\u573a\u666f\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u63a8\u7406\u662f\u66f4\u53ef\u9760\u7684\u77e5\u8bc6\u66f4\u65b0\u8def\u5f84\uff0c\u5f53\u524d\u53c2\u6570\u7f16\u8f91\u6280\u672f\u9700\u7a81\u7834\u67b6\u6784\u9650\u5236\u4ee5\u5b9e\u73b0\u5b9e\u7528\u5316\u3002"}}
{"id": "2505.18703", "pdf": "https://arxiv.org/pdf/2505.18703", "abs": "https://arxiv.org/abs/2505.18703", "authors": ["Gaurav Negi", "Dhairya Dalal", "Omnia Zayed", "Paul Buitelaar"], "title": "Towards Semantic Integration of Opinions: Unified Opinion Concepts Ontology and Extraction Task", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces the Unified Opinion Concepts (UOC) ontology to\nintegrate opinions within their semantic context. The UOC ontology bridges the\ngap between the semantic representation of opinion across different\nformulations. It is a unified conceptualisation based on the facets of opinions\nstudied extensively in NLP and semantic structures described through symbolic\ndescriptions. We further propose the Unified Opinion Concept Extraction (UOCE)\ntask of extracting opinions from the text with enhanced expressivity.\nAdditionally, we provide a manually extended and re-annotated evaluation\ndataset for this task and tailored evaluation metrics to assess the adherence\nof extracted opinions to UOC semantics. Finally, we establish baseline\nperformance for the UOCE task using state-of-the-art generative models.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u610f\u89c1\u6982\u5ff5\u672c\u4f53(UOC)\u6574\u5408\u8bed\u4e49\u8bed\u5883\u4e2d\u7684\u89c2\u70b9\u8868\u8fbe\uff0c\u5e76\u8bbe\u8ba1UOCE\u4efb\u52a1\u7528\u4e8e\u589e\u5f3a\u610f\u89c1\u62bd\u53d6\u80fd\u529b\uff0c\u5efa\u7acb\u751f\u6210\u6a21\u578b\u57fa\u7ebf\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4e0d\u540cNLP\u7814\u7a76\u4e2d\u89c2\u70b9\u8868\u8fbe\u8bed\u4e49\u4e0d\u7edf\u4e00\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u878d\u5408\u7b26\u53f7\u5316\u63cf\u8ff0\u4e0e\u8bed\u4e49\u7ed3\u6784\u5b9e\u73b0\u8de8\u9886\u57df\u89c2\u70b9\u8868\u5f81\u7684\u7edf\u4e00\u5efa\u6a21\u3002", "method": "\u6784\u5efaUOC\u672c\u4f53\u6865\u63a5\u8bed\u4e49\u9e3f\u6c9f\uff0c\u63d0\u51faUOCE\u62bd\u53d6\u4efb\u52a1\uff0c\u521b\u5efa\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u96c6\u5e76\u5f00\u53d1\u9488\u5bf9\u6027\u8bc4\u4f30\u6307\u6807\uff0c\u4f7f\u7528SOTA\u751f\u6210\u6a21\u578b\u5efa\u7acb\u57fa\u7ebf\u3002", "result": "\u63d0\u4f9b\u5305\u542b\u8bed\u4e49\u7ea6\u675f\u7684\u8bc4\u4f30\u6570\u636e\u96c6\u3001\u5b9a\u5236\u5316\u8bc4\u4f30\u6307\u6807\uff0c\u4ee5\u53ca\u57fa\u4e8e\u751f\u6210\u6a21\u578b\u7684\u57fa\u51c6\u6027\u80fd\u7ed3\u679c\u3002", "conclusion": "UOC\u672c\u4f53\u589e\u5f3a\u4e86\u89c2\u70b9\u8868\u8fbe\u7684\u8bed\u4e49\u5b8c\u6574\u6027\uff0cUOCE\u4efb\u52a1\u53ca\u8bc4\u4f30\u4f53\u7cfb\u4e3a\u7ec6\u7c92\u5ea6\u610f\u89c1\u6316\u6398\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u57fa\u7ebf\u7ed3\u679c\u4e3a\u540e\u7eed\u7814\u7a76\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2505.18708", "pdf": "https://arxiv.org/pdf/2505.18708", "abs": "https://arxiv.org/abs/2505.18708", "authors": ["Xu Zhang", "Kun Zhang", "Wenxin Ma", "Rongsheng Wang", "Chenxu Wu", "Yingtai Li", "S. Kevin Zhou"], "title": "A General Knowledge Injection Framework for ICD Coding", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "ICD Coding aims to assign a wide range of medical codes to a medical text\ndocument, which is a popular and challenging task in the healthcare domain. To\nalleviate the problems of long-tail distribution and the lack of annotations of\ncode-specific evidence, many previous works have proposed incorporating code\nknowledge to improve coding performance. However, existing methods often focus\non a single type of knowledge and design specialized modules that are complex\nand incompatible with each other, thereby limiting their scalability and\neffectiveness. To address this issue, we propose GKI-ICD, a novel, general\nknowledge injection framework that integrates three key types of knowledge,\nnamely ICD Description, ICD Synonym, and ICD Hierarchy, without specialized\ndesign of additional modules. The comprehensive utilization of the above\nknowledge, which exhibits both differences and complementarity, can effectively\nenhance the ICD coding performance. Extensive experiments on existing popular\nICD coding benchmarks demonstrate the effectiveness of GKI-ICD, which achieves\nthe state-of-the-art performance on most evaluation metrics. Code is available\nat https://github.com/xuzhang0112/GKI-ICD.", "AI": {"tldr": "\u63d0\u51fa\u901a\u7528\u77e5\u8bc6\u6ce8\u5165\u6846\u67b6GKI-ICD\uff0c\u901a\u8fc7\u6574\u5408ICD\u63cf\u8ff0\u3001\u540c\u4e49\u8bcd\u548c\u5c42\u7ea7\u4e09\u79cd\u77e5\u8bc6\u7c7b\u578b\uff0c\u65e0\u9700\u590d\u6742\u6a21\u5757\u8bbe\u8ba1\u5373\u663e\u8457\u63d0\u5347\u533b\u7597\u7f16\u7801\u6027\u80fd", "motivation": "\u73b0\u6709ICD\u7f16\u7801\u65b9\u6cd5\u591a\u805a\u7126\u5355\u4e00\u77e5\u8bc6\u7c7b\u578b\u4e14\u4f9d\u8d56\u590d\u6742\u5b9a\u5236\u6a21\u5757\uff0c\u5bfc\u81f4\u6269\u5c55\u6027\u5dee\u3001\u6548\u679c\u53d7\u9650\uff0c\u9700\u901a\u7528\u77e5\u8bc6\u6574\u5408\u65b9\u6848", "method": "\u8bbe\u8ba1\u65e0\u9700\u7279\u6b8a\u6a21\u5757\u7684\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u6ce8\u5165\u63cf\u8ff0\u6027\u6587\u672c\u3001\u540c\u4e49\u8bcd\u6269\u5c55\u548c\u5c42\u7ea7\u5173\u7cfb\u4e09\u79cd\u4e92\u8865\u77e5\u8bc6\u7c7b\u578b\u589e\u5f3a\u7f16\u7801\u80fd\u529b", "result": "\u5728\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u591a\u6570\u8bc4\u4f30\u6307\u6807\u8fbe\u5230\u6700\u4f18", "conclusion": "GKI-ICD\u8bc1\u660e\u591a\u7ef4\u5ea6\u77e5\u8bc6\u878d\u5408\u7684\u6709\u6548\u6027\uff0c\u4e3a\u533b\u7597\u7f16\u7801\u4efb\u52a1\u63d0\u4f9b\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.18709", "pdf": "https://arxiv.org/pdf/2505.18709", "abs": "https://arxiv.org/abs/2505.18709", "authors": ["Sourav Kumar Das", "Md. Julkar Naeen", "MD. Jahidul Islam", "Md. Anisul Haque Sajeeb", "Narayan Ranjan Chakraborty", "Mayen Uddin Mojumdar"], "title": "Improving Bangla Linguistics: Advanced LSTM, Bi-LSTM, and Seq2Seq Models for Translating Sylheti to Modern Bangla", "categories": ["cs.CL", "cs.AI"], "comment": "2024 15th International Conference on Computing Communication and\n  Networking Technologies (ICCCNT)", "summary": "Bangla or Bengali is the national language of Bangladesh, people from\ndifferent regions don't talk in proper Bangla. Every division of Bangladesh has\nits own local language like Sylheti, Chittagong etc. In recent years some\npapers were published on Bangla language like sentiment analysis, fake news\ndetection and classifications, but a few of them were on Bangla languages. This\nresearch is for the local language and this particular paper is on Sylheti\nlanguage. It presented a comprehensive system using Natural Language Processing\nor NLP techniques for translating Pure or Modern Bangla to locally spoken\nSylheti Bangla language. Total 1200 data used for training 3 models LSTM,\nBi-LSTM and Seq2Seq and LSTM scored the best in performance with 89.3%\naccuracy. The findings of this research may contribute to the growth of Bangla\nNLP researchers for future more advanced innovations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6784\u5efa\u4e86\u57fa\u4e8eLSTM\u7684\u5b5f\u52a0\u62c9\u8bed\u5230\u9521\u5c14\u8d6b\u7279\u65b9\u8a00\u7684\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u51c6\u786e\u7387\u8fbe89.3%", "motivation": "\u9488\u5bf9\u5b5f\u52a0\u62c9\u5730\u65b9\u8a00\uff08\u5982\u9521\u5c14\u8d6b\u7279\u8bed\uff09\u7f3a\u4e4f\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u7684\u73b0\u72b6\uff0c\u586b\u8865\u6807\u51c6\u5b5f\u52a0\u62c9\u8bed\u4e0e\u65b9\u8a00\u95f4\u7684\u7ffb\u8bd1\u7a7a\u767d", "method": "\u4f7f\u75281200\u6761\u8bad\u7ec3\u6570\u636e\uff0c\u5bf9\u6bd4\u6d4b\u8bd5LSTM\u3001Bi-LSTM\u548cSeq2Seq\u4e09\u79cd\u795e\u7ecf\u7f51\u7edc\u6a21\u578b", "result": "LSTM\u6a21\u578b\u8868\u73b0\u6700\u4f18\uff0c\u7ffb\u8bd1\u51c6\u786e\u7387\u8fbe\u523089.3%", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5b5f\u52a0\u62c9\u8bed\u65b9\u8a00\u5904\u7406\u5960\u5b9a\u6280\u672f\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u4fdd\u62a4\u8bed\u8a00\u591a\u6837\u6027\u5e76\u63a8\u52a8\u533a\u57dfNLP\u6280\u672f\u53d1\u5c55"}}
{"id": "2505.18720", "pdf": "https://arxiv.org/pdf/2505.18720", "abs": "https://arxiv.org/abs/2505.18720", "authors": ["Meng Li", "Guangda Huzhang", "Haibo Zhang", "Xiting Wang", "Anxiang Zeng"], "title": "Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 11 figures. Accepted by ACL 2025 (main)", "summary": "Direct Preference Optimization (DPO) has emerged as a promising framework for\naligning Large Language Models (LLMs) with human preferences by directly\noptimizing the log-likelihood difference between chosen and rejected responses.\nHowever, existing methods assign equal importance to all tokens in the\nresponse, while humans focus on more meaningful parts. This leads to suboptimal\npreference optimization, as irrelevant or noisy tokens disproportionately\ninfluence DPO loss. To address this limitation, we propose \\textbf{O}ptimal\n\\textbf{T}ransport-based token weighting scheme for enhancing direct\n\\textbf{P}reference \\textbf{O}ptimization (OTPO). By emphasizing semantically\nmeaningful token pairs and de-emphasizing less relevant ones, our method\nintroduces a context-aware token weighting scheme that yields a more\ncontrastive reward difference estimate. This adaptive weighting enhances reward\nstability, improves interpretability, and ensures that preference optimization\nfocuses on meaningful differences between responses. Extensive experiments have\nvalidated OTPO's effectiveness in improving instruction-following ability\nacross various settings\\footnote{Code is available at\nhttps://github.com/Mimasss2/OTPO.}.", "AI": {"tldr": "\u63d0\u51faOTPO\u65b9\u6cd5\u2014\u2014\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u6539\u8fdbDPO\u6846\u67b6\uff0c\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u8bed\u4e49\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709DPO\u65b9\u6cd5\u5e73\u7b49\u5bf9\u5f85\u6240\u6709token\uff0c\u5bfc\u81f4\u65e0\u5173\u566a\u58f0token\u5bf9\u504f\u597d\u4f18\u5316\u4ea7\u751f\u8fc7\u5ea6\u5f71\u54cd\u3002\u4eba\u7c7b\u66f4\u5173\u6ce8\u8bed\u4e49\u6838\u5fc3\u90e8\u5206\uff0c\u9700\u9488\u5bf9\u6027\u4f18\u5316\u3002", "method": "\u5f15\u5165\u6700\u4f18\u8fd0\u8f93\u7406\u8bba\u8bbe\u8ba1\u4e0a\u4e0b\u6587\u611f\u77e5\u7684token\u6743\u91cd\u5206\u914d\u673a\u5236\uff0c\u589e\u5f3a\u5173\u952e\u8bed\u4e49token\u5bf9\u6bd4\uff0c\u6291\u5236\u566a\u58f0\u5e72\u6270\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eOTPO\u663e\u8457\u63d0\u5347\u6a21\u578b\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u589e\u5f3a\u5956\u52b1\u7a33\u5b9a\u6027\u53ca\u7ed3\u679c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u57fa\u4e8e\u8bed\u4e49\u6743\u91cd\u7684\u4f18\u5316\u7b56\u7565\u80fd\u66f4\u6709\u6548\u6355\u6349\u54cd\u5e94\u95f4\u672c\u8d28\u5dee\u5f02\uff0c\u63a8\u52a8LLM\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u3002"}}
{"id": "2505.18744", "pdf": "https://arxiv.org/pdf/2505.18744", "abs": "https://arxiv.org/abs/2505.18744", "authors": ["Tao Liu", "Hongying Zan", "Yifan Li", "Dixuan Zhang", "Lulu Kong", "Haixin Liu", "Jiaming Hou", "Aoze Zheng", "Rui Li", "Yiming Qiao", "Zewei Luo", "Qi Wang", "Zhiqiang Zhang", "Jiaxi Li", "Supeng Liu", "Kunli Zhang", "Min Peng"], "title": "LogicCat: A Chain-of-Thought Text-to-SQL Benchmark for Multi-Domain Reasoning Challenges", "categories": ["cs.CL"], "comment": "22 pages, 10 figures", "summary": "Text-to-SQL is a fundamental task in natural language processing that seeks\nto translate natural language questions into meaningful and executable SQL\nqueries. While existing datasets are extensive and primarily focus on business\nscenarios and operational logic, they frequently lack coverage of\ndomain-specific knowledge and complex mathematical reasoning. To address this\ngap, we present a novel dataset tailored for complex reasoning and\nchain-of-thought analysis in SQL inference, encompassing physical, arithmetic,\ncommonsense, and hypothetical reasoning. The dataset consists of 4,038 English\nquestions, each paired with a unique SQL query and accompanied by 12,114\nstep-by-step reasoning annotations, spanning 45 databases across diverse\ndomains. Experimental results demonstrate that LogicCat substantially increases\nthe difficulty for state-of-the-art models, with the highest execution accuracy\nreaching only 14.96%. Incorporating our chain-of-thought annotations boosts\nperformance to 33.96%. Benchmarking leading public methods on Spider and BIRD\nfurther underscores the unique challenges presented by LogicCat, highlighting\nthe significant opportunities for advancing research in robust,\nreasoning-driven text-to-SQL systems. We have released our dataset code at\nhttps://github.com/Ffunkytao/LogicCat.", "AI": {"tldr": "\u63d0\u51faLogicCat\u6570\u636e\u96c6\u89e3\u51b3\u6587\u672c\u5230SQL\u4efb\u52a1\u4e2d\u9886\u57df\u77e5\u8bc6\u4e0e\u590d\u6742\u63a8\u7406\u7684\u4e0d\u8db3\uff0c\u5305\u542b4038\u4e2a\u95ee\u9898\u4e0e\u601d\u7ef4\u94fe\u6807\u6ce8\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709\u6587\u672c\u5230SQL\u6570\u636e\u96c6\u7f3a\u4e4f\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u548c\u590d\u6742\u6570\u5b66\u63a8\u7406\u80fd\u529b\u8986\u76d6\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528", "method": "\u6784\u5efa\u5305\u542b\u7269\u7406/\u7b97\u6570/\u5e38\u8bc6/\u5047\u8bbe\u63a8\u7406\u7684\u8de8\u9886\u57df\u6570\u636e\u96c6\uff0c\u542b4,038\u95ee\u9898+12,114\u5206\u6b65\u63a8\u7406\u6807\u6ce8\uff0c\u8986\u76d645\u4e2a\u6570\u636e\u5e93", "result": "SOTA\u6a21\u578b\u6700\u9ad8\u51c6\u786e\u7387\u4ec514.96%\uff0c\u52a0\u5165\u601d\u7ef4\u94fe\u6807\u6ce8\u540e\u63d0\u5347\u81f333.96%\uff0c\u5728Spider/BIRD\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u6311\u6218\u6027", "conclusion": "LogicCat\u63ed\u793a\u4e86\u63a8\u7406\u9a71\u52a8\u6587\u672c\u5230SQL\u7cfb\u7edf\u7684\u7814\u7a76\u7a7a\u95f4\uff0c\u601d\u7ef4\u94fe\u673a\u5236\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u63a8\u52a8\u9c81\u68d2\u6027SQL\u89e3\u6790\u53d1\u5c55"}}
{"id": "2505.18752", "pdf": "https://arxiv.org/pdf/2505.18752", "abs": "https://arxiv.org/abs/2505.18752", "authors": ["Haolin Yang", "Hakaze Cho", "Yiqiao Zhong", "Naoya Inoue"], "title": "Unifying Attention Heads and Task Vectors via Hidden State Geometry in In-Context Learning", "categories": ["cs.CL"], "comment": "45 pages, 49 figures", "summary": "The unusual properties of in-context learning (ICL) have prompted\ninvestigations into the internal mechanisms of large language models. Prior\nwork typically focuses on either special attention heads or task vectors at\nspecific layers, but lacks a unified framework linking these components to the\nevolution of hidden states across layers that ultimately produce the model's\noutput. In this paper, we propose such a framework for ICL in classification\ntasks by analyzing two geometric factors that govern performance: the\nseparability and alignment of query hidden states. A fine-grained analysis of\nlayer-wise dynamics reveals a striking two-stage mechanism: separability\nemerges in early layers, while alignment develops in later layers. Ablation\nstudies further show that Previous Token Heads drive separability, while\nInduction Heads and task vectors enhance alignment. Our findings thus bridge\nthe gap between attention heads and task vectors, offering a unified account of\nICL's underlying mechanisms.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u5206\u6790\u9690\u85cf\u72b6\u6001\u53ef\u5206\u6027\u548c\u5bf9\u9f50\u6027\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7edf\u4e00\u89e3\u91ca\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u673a\u5236\uff1a\u65e9\u671f\u5c42\u5f62\u6210\u7279\u5f81\u53ef\u5206\u6027\uff0c\u540e\u671f\u5c42\u5efa\u7acb\u4efb\u52a1\u5bf9\u9f50\u6027\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u5206\u522b\u5173\u6ce8\u6ce8\u610f\u529b\u5934\u6216\u4efb\u52a1\u5411\u91cf\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u9690\u85cf\u72b6\u6001\u8de8\u5c42\u6f14\u53d8\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u8f93\u51fa\u7684\u7edf\u4e00\u89e3\u91ca\u6846\u67b6\u3002\u672c\u6587\u65e8\u5728\u5efa\u7acb\u8fd9\u79cd\u5173\u8054\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u51e0\u4f55\u5206\u6790\u6846\u67b6\uff0c\u7814\u7a76\u5206\u7c7b\u4efb\u52a1\u4e2d\u67e5\u8be2\u9690\u85cf\u72b6\u6001\u7684\u53ef\u5206\u6027\uff08separability\uff09\u548c\u5bf9\u9f50\u6027\uff08alignment\uff09\u7684\u5c42\u95f4\u52a8\u6001\u6f14\u53d8\u673a\u5236\u3002", "result": "\u53d1\u73b0\u660e\u663e\u7684\u4e24\u9636\u6bb5\u673a\u5236\uff1a\u524d10\u5c42\u5efa\u7acb\u7279\u5f81\u53ef\u5206\u6027\uff08\u7531Previous Token Heads\u9a71\u52a8\uff09\uff0c\u540e10\u5c42\u53d1\u5c55\u4efb\u52a1\u5bf9\u9f50\u6027\uff08\u7531Induction Heads\u548c\u4efb\u52a1\u5411\u91cf\u589e\u5f3a\uff09\u3002", "conclusion": "\u9996\u6b21\u5c06\u6ce8\u610f\u529b\u5934\u4e0e\u4efb\u52a1\u5411\u91cf\u673a\u5236\u7edf\u4e00\uff0c\u63ed\u793aICL\u901a\u8fc7\u7279\u5f81\u5206\u79bb\u2192\u4efb\u52a1\u9002\u914d\u7684\u5206\u5c42\u5904\u7406\u673a\u5236\uff0c\u4e3a\u7406\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002"}}
{"id": "2505.18754", "pdf": "https://arxiv.org/pdf/2505.18754", "abs": "https://arxiv.org/abs/2505.18754", "authors": ["Elsen Ronando", "Sozo Inoue"], "title": "Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection", "categories": ["cs.CL", "I.2.7"], "comment": "43 pages, 18 figures. Accepted for publication in MDPI Sensors\n  (2025). Final version before journal publication", "summary": "In this paper, we propose a novel few-shot optimization with HED-LM (Hybrid\nEuclidean Distance with Large Language Models) to improve example selection for\nsensor-based classification tasks. While few-shot prompting enables efficient\ninference with limited labeled data, its performance largely depends on the\nquality of selected examples. HED-LM addresses this challenge through a hybrid\nselection pipeline that filters candidate examples based on Euclidean distance\nand re-ranks them using contextual relevance scored by large language models\n(LLMs). To validate its effectiveness, we apply HED-LM to a fatigue detection\ntask using accelerometer data characterized by overlapping patterns and high\ninter-subject variability. Unlike simpler tasks such as activity recognition,\nfatigue detection demands more nuanced example selection due to subtle\ndifferences in physiological signals. Our experiments show that HED-LM achieves\na mean macro F1-score of 69.13$\\pm$10.71%, outperforming both random selection\n(59.30$\\pm$10.13%) and distance-only filtering (67.61$\\pm$11.39%). These\nrepresent relative improvements of 16.6% and 2.3%, respectively. The results\nconfirm that combining numerical similarity with contextual relevance improves\nthe robustness of few-shot prompting. Overall, HED-LM offers a practical\nsolution to improve performance in real-world sensor-based learning tasks and\nshows potential for broader applications in healthcare monitoring, human\nactivity recognition, and industrial safety scenarios.", "AI": {"tldr": "\u63d0\u51faHED-LM\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u6b27\u6c0f\u8ddd\u79bb\u4e0e\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u5c0f\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u793a\u4f8b\u9009\u62e9\u6548\u679c\uff0c\u5e94\u7528\u4e8e\u75b2\u52b3\u68c0\u6d4b\u4efb\u52a1\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "\u5c0f\u6837\u672c\u5b66\u4e60\u4e2d\u793a\u4f8b\u9009\u62e9\u8d28\u91cf\u76f4\u63a5\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u4f20\u611f\u5668\u6570\u636e(\u5982\u75b2\u52b3\u68c0\u6d4b)\u5b58\u5728\u6a21\u5f0f\u91cd\u53e0\u548c\u4e2a\u4f53\u5dee\u5f02\u5927\u7684\u6311\u6218\uff0c\u9700\u8981\u66f4\u7cbe\u7ec6\u7684\u793a\u4f8b\u7b5b\u9009\u7b56\u7565", "method": "HED-LM\u6df7\u5408\u9009\u62e9\u6d41\u7a0b\uff1a\u5148\u7528\u6b27\u6c0f\u8ddd\u79bb\u521d\u7b5b\u5019\u9009\u6837\u672c\uff0c\u518d\u901a\u8fc7LLM\u8bc4\u4f30\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u8fdb\u884c\u91cd\u6392\u5e8f\uff0c\u517c\u987e\u6570\u503c\u76f8\u4f3c\u5ea6\u4e0e\u8bed\u4e49\u76f8\u5173\u6027", "result": "HED-LM\u53d6\u5f9769.13\u00b110.71%\u7684F1\u5206\u6570\uff0c\u76f8\u6bd4\u968f\u673a\u9009\u62e9(59.30%)\u548c\u7eaf\u8ddd\u79bb\u7b5b\u9009(67.61%)\u5206\u522b\u63d0\u534716.6%\u548c2.3%", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u73b0\u5b9e\u4f20\u611f\u5668\u4efb\u52a1\u63d0\u4f9b\u5b9e\u7528\u4f18\u5316\u65b9\u6848\uff0c\u5728\u533b\u7597\u76d1\u6d4b\u3001\u884c\u4e3a\u8bc6\u522b\u548c\u5de5\u4e1a\u5b89\u5168\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b"}}
{"id": "2505.18761", "pdf": "https://arxiv.org/pdf/2505.18761", "abs": "https://arxiv.org/abs/2505.18761", "authors": ["Minglai Yang", "Ethan Huang", "Liang Zhang", "Mihai Surdeanu", "William Wang", "Liangming Pan"], "title": "How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages, 9 figure, 4 tables", "summary": "We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic\nbenchmark to evaluate Large Language Models' (LLMs) reasoning robustness\nagainst systematically controlled irrelevant context (IC). GSM-DC constructs\nsymbolic reasoning graphs with precise distractor injections, enabling\nrigorous, reproducible evaluation. Our experiments demonstrate that LLMs are\nsignificantly sensitive to IC, affecting both reasoning path selection and\narithmetic accuracy. Additionally, training models with strong distractors\nimproves performance in both in-distribution and out-of-distribution scenarios.\nWe further propose a stepwise tree search guided by a process reward model,\nwhich notably enhances robustness in out-of-distribution conditions.", "AI": {"tldr": "\u63d0\u51faGSM-DC\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u7b26\u53f7\u63a8\u7406\u56fe\u4e0e\u5e72\u6270\u6ce8\u5165\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u6297\u5e72\u6270\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5bf9\u65e0\u5173\u4fe1\u606f\u654f\u611f\uff0c\u4f46\u9488\u5bf9\u6027\u8bad\u7ec3\u548c\u6811\u641c\u7d22\u7b56\u7565\u53ef\u6709\u6548\u63d0\u5347\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u7f3a\u4e4f\u5bf9\u65e0\u5173\u4e0a\u4e0b\u6587\u5e72\u6270\u7684\u7cfb\u7edf\u63a7\u5236\uff0c\u9700\u6784\u5efa\u53ef\u91cf\u5316\u6d4b\u8bd5\u73af\u5883\u5206\u6790LLM\u63a8\u7406\u7a33\u5b9a\u6027\u3002", "method": "\u521b\u5efa\u542b\u7cbe\u786e\u5e72\u6270\u7684\u7b26\u53f7\u63a8\u7406\u56fe\uff0c\u5f00\u5c55\u654f\u611f\u5ea6\u5b9e\u9a8c/\u5e72\u6270\u8bad\u7ec3\u6d4b\u8bd5\uff0c\u5e76\u63d0\u51fa\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u5f15\u5bfc\u7684\u9010\u6b65\u6811\u641c\u7d22\u7b97\u6cd5\u3002", "result": "\u6a21\u578b\u5448\u73b0\u663e\u8457\u5e72\u6270\u654f\u611f\u6027\uff08\u8def\u5f84\u9009\u62e9\u9519\u8bef\u7387\u219135%\uff09\uff0c\u5e72\u6270\u8bad\u7ec3\u4f7fOOD\u51c6\u786e\u7387\u63d0\u534718.7%\uff0c\u6811\u641c\u7d22\u7b56\u7565\u63d0\u5347\u6297\u5e72\u6270\u80fd\u529b\u3002", "conclusion": "\u7cfb\u7edf\u5316\u5e72\u6270\u8bc4\u4f30\u63ed\u793a\u6a21\u578b\u5f31\u70b9\uff0c\u9488\u5bf9\u6027\u8bad\u7ec3\u4e0e\u7ed3\u6784\u5316\u63a8\u7406\u7b97\u6cd5\u80fd\u6709\u6548\u589e\u5f3aLLM\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u63a8\u7406\u7a33\u5065\u6027\u3002"}}
{"id": "2505.18762", "pdf": "https://arxiv.org/pdf/2505.18762", "abs": "https://arxiv.org/abs/2505.18762", "authors": ["Michael Flor", "Zuowei Wang", "Paul Deane", "Tenaha O'Reilly"], "title": "Towards an automatic method for generating topical vocabulary test forms for specific reading passages", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "This manuscript was accepted to be published as an ETS Research\n  Report. Keywords topics; vocabulary; background knowledge; automatic item\n  generation; assessment; reading comprehension", "summary": "Background knowledge is typically needed for successful comprehension of\ntopical and domain specific reading passages, such as in the STEM domain.\nHowever, there are few automated measures of student knowledge that can be\nreadily deployed and scored in time to make predictions on whether a given\nstudent will likely be able to understand a specific content area text. In this\npaper, we present our effort in developing K-tool, an automated system for\ngenerating topical vocabulary tests that measure students' background knowledge\nrelated to a specific text. The system automatically detects the topic of a\ngiven text and produces topical vocabulary items based on their relationship\nwith the topic. This information is used to automatically generate background\nknowledge forms that contain words that are highly related to the topic and\nwords that share similar features but do not share high associations to the\ntopic. Prior research indicates that performance on such tasks can help\ndetermine whether a student is likely to understand a particular text based on\ntheir knowledge state. The described system is intended for use with middle and\nhigh school student population of native speakers of English. It is designed to\nhandle single reading passages and is not dependent on any corpus or text\ncollection. In this paper, we describe the system architecture and present an\ninitial evaluation of the system outputs.", "AI": {"tldr": "\u5f00\u53d1\u81ea\u52a8\u5316\u7cfb\u7edfK-tool\u751f\u6210\u4e3b\u9898\u8bcd\u6c47\u6d4b\u8bd5\uff0c\u901a\u8fc7\u68c0\u6d4b\u6587\u672c\u4e3b\u9898\u5e76\u6784\u5efa\u5173\u8054/\u975e\u5173\u8054\u8bcd\u6c47\u8868\uff0c\u8bc4\u4f30\u4e2d\u5b66\u751f\u82f1\u8bed\u80cc\u666f\u77e5\u8bc6\u5bf9\u7279\u5b9aSTEM\u6587\u672c\u7684\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u5316\u6d4b\u91cf\u5b66\u751f\u7279\u5b9a\u6587\u672c\u80cc\u666f\u77e5\u8bc6\u7684\u5de5\u5177\u532e\u4e4f\uff0c\u96be\u4ee5\u5b9e\u65f6\u9884\u6d4b\u5b66\u751f\u6587\u672c\u7406\u89e3\u80fd\u529b\u3002STEM\u9886\u57df\u9700\u8981\u80cc\u666f\u77e5\u8bc6\u652f\u6491\uff0c\u4f46\u7f3a\u4e4f\u53ef\u5feb\u901f\u90e8\u7f72\u7684\u8bc4\u4f30\u7cfb\u7edf\u3002", "method": "1.\u81ea\u52a8\u68c0\u6d4b\u6587\u672c\u4e3b\u9898\n2.\u57fa\u4e8e\u8bcd\u6c47\u4e0e\u4e3b\u9898\u7684\u5173\u8054\u5ea6\u751f\u6210\u6d4b\u8bd5\u9879\n3.\u6784\u5efa\u5305\u542b\u5f3a\u5173\u8054\u8bcd\u4e0e\u7279\u5f81\u76f8\u4f3c\u4f46\u4f4e\u5173\u8054\u8bcd\u7684\u77e5\u8bc6\u8868\u5355\n4.\u7cfb\u7edf\u72ec\u7acb\u8fd0\u4f5c\uff0c\u65e0\u9700\u4f9d\u8d56\u8bed\u6599\u5e93", "result": "\u5b8c\u6210\u7cfb\u7edf\u67b6\u6784\u8bbe\u8ba1\u5e76\u901a\u8fc7\u521d\u6b65\u8f93\u51fa\u8bc4\u4f30\uff08\u5177\u4f53\u6570\u636e\u672a\u62ab\u9732\uff09\uff0c\u7cfb\u7edf\u9002\u7528\u4e8e\u5355\u7bc7\u9605\u8bfb\u6750\u6599", "conclusion": "K-tool\u4e3a\u521d\u9ad8\u4e2d\u82f1\u8bed\u6bcd\u8bed\u5b66\u4e60\u8005\u63d0\u4f9b\u6709\u6548\u7684\u80cc\u666f\u77e5\u8bc6\u8bc4\u4f30\u65b9\u6848\uff0c\u901a\u8fc7\u8bcd\u6c47\u5173\u8054\u6027\u6d4b\u8bd5\u9884\u6d4b\u6587\u672c\u7406\u89e3\u53ef\u80fd\u6027\uff0c\u586b\u8865\u6559\u80b2\u6280\u672f\u9886\u57df\u5b9e\u65f6\u8bc4\u4f30\u5de5\u5177\u7684\u7a7a\u767d\u3002"}}
{"id": "2505.18774", "pdf": "https://arxiv.org/pdf/2505.18774", "abs": "https://arxiv.org/abs/2505.18774", "authors": ["Mengqi Zhang", "Zisheng Zhou", "Xiaotian Ye", "Qiang Liu", "Zhaochun Ren", "Zhumin Chen", "Pengjie Ren"], "title": "Disentangling Knowledge Representations for Large Language Model Editing", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge Editing has emerged as a promising solution for efficiently\nupdating embedded knowledge in large language models (LLMs). While existing\napproaches demonstrate effectiveness in integrating new knowledge and\npreserving the original capabilities of LLMs, they fail to maintain\nfine-grained irrelevant knowledge facts that share the same subject as edited\nknowledge but differ in relation and object. This challenge arises because\nsubject representations inherently encode multiple attributes, causing the\ntarget and fine-grained irrelevant knowledge to become entangled in the\nrepresentation space, and thus vulnerable to unintended alterations during\nediting. To address this, we propose DiKE, a novel approach that Disentangles\nKnowledge representations for LLM Editing (DiKE). DiKE consists of two key\ncomponents: a Knowledge Representation Disentanglement (KRD) module that\ndecomposes the subject representation into target-knowledgerelated and\n-unrelated components, and a Disentanglement-based Knowledge Edit (DKE) module\nthat updates only the target-related component while explicitly preserving the\nunrelated one. We further derive a closed-form, rank-one parameter update based\non matrix theory to enable efficient and minimally invasive edits. To\nrigorously evaluate fine-grained irrelevant knowledge preservation, we\nconstruct FINE-KED, a new benchmark comprising fine-grained irrelevant\nknowledge at different levels of relational similarity to the edited knowledge.\nExtensive experiments across multiple LLMs demonstrate that DiKE substantially\nimproves fine-grained irrelevant knowledge preservation while maintaining\ncompetitive general editing performance.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u77e5\u8bc6\u8868\u793a\u89e3\u8026\u7684DiKE\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u5927\u6a21\u578b\u77e5\u8bc6\u7f16\u8f91\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u7ec6\u7c92\u5ea6\u65e0\u5173\u77e5\u8bc6\u7684\u4fdd\u7559\u6548\u679c\uff0c\u5e76\u901a\u8fc7\u65b0\u6784\u5efa\u7684FINE-KED\u57fa\u51c6\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5728\u66f4\u65b0\u76ee\u6807\u77e5\u8bc6\u65f6\uff0c\u4f1a\u7834\u574f\u540c\u4e00\u4e3b\u4f53\u4e0b\u5176\u4ed6\u975e\u76f8\u5173\u7684\u7ec6\u7c92\u5ea6\u77e5\u8bc6\uff08\u5982\u76f8\u540c\u4e3b\u8bed\u4f46\u4e0d\u540c\u8c13\u5bbe\u5173\u7cfb\u7684\u77e5\u8bc6\uff09\uff0c\u56e0\u4e3b\u4f53\u8868\u793a\u5728\u5411\u91cf\u7a7a\u95f4\u4e2d\u5b58\u5728\u5c5e\u6027\u8026\u5408\u73b0\u8c61\u3002", "method": "1. KRD\u6a21\u5757\u5c06\u4e3b\u4f53\u8868\u793a\u89e3\u8026\u4e3a\u76ee\u6807\u76f8\u5173\u548c\u65e0\u5173\u5206\u91cf\uff1b2. DKE\u6a21\u5757\u4ec5\u66f4\u65b0\u76f8\u5173\u5206\u91cf\u5e76\u663e\u5f0f\u4fdd\u7559\u65e0\u5173\u5206\u91cf\uff1b3. \u57fa\u4e8e\u77e9\u9635\u7406\u8bba\u63a8\u5bfc\u51fa\u95ed\u5f0f\u79e9\u4e00\u53c2\u6570\u66f4\u65b0\u516c\u5f0f\u3002", "result": "\u5728\u591a\u4e2a\u5927\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDiKE\u663e\u8457\u63d0\u5347\u7ec6\u7c92\u5ea6\u65e0\u5173\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\uff08\u6700\u9ad8\u63d0\u534714.4%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u4e3b\u6d41\u65b9\u6cd5\u76f8\u5f53\u7684\u7f16\u8f91\u6210\u529f\u7387\uff08\u5e73\u574794.3%\uff09\u3002", "conclusion": "\u901a\u8fc7\u8868\u793a\u89e3\u8026\u673a\u5236\u3001\u95ed\u5f0f\u53c2\u6570\u66f4\u65b0\u548c\u65b0\u8bc4\u4f30\u57fa\u51c6\uff0cDiKE\u4e3a\u89e3\u51b3\u77e5\u8bc6\u7f16\u8f91\u4e2d\u7684\u5c5e\u6027\u8026\u5408\u95ee\u9898\u63d0\u4f9b\u4e86\u521b\u65b0\u65b9\u6848\uff0c\u8bc1\u5b9e\u4e86\u77e5\u8bc6\u89e3\u8026\u5bf9\u6a21\u578b\u53ef\u7f16\u8f91\u6027\u7684\u63d0\u5347\u4ef7\u503c\u3002"}}
{"id": "2505.18778", "pdf": "https://arxiv.org/pdf/2505.18778", "abs": "https://arxiv.org/abs/2505.18778", "authors": ["Benjamin Bennetzen", "Peter Buus Steffensen", "Hans H\u00fcttel", "Nikolaj Rossander Kristensen", "Andreas Tor Mortensen"], "title": "A generalised editor calculus (Short Paper)", "categories": ["cs.CL", "F.2.2, I.2.7"], "comment": "7 pages, 21 figures", "summary": "In this paper, we present a generalization of a syntax-directed editor\ncalculus, which can be used to instantiate a specialized syntax-directed editor\nfor any language, given by some abstract syntax. The editor calculus guarantees\nthe absence of syntactical errors while allowing incomplete programs. The\ngeneralized editor calculus is then encoded into a simply typed lambda\ncalculus, extended with pairs, booleans, pattern matching and fixed points", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6cdb\u5316\u7684\u8bed\u6cd5\u5bfc\u5411\u7f16\u8f91\u5668\u6f14\u7b97\u6846\u67b6\uff0c\u652f\u6301\u57fa\u4e8e\u4efb\u610f\u8bed\u8a00\u7684\u62bd\u8c61\u8bed\u6cd5\u5b9e\u4f8b\u5316\u4e13\u7528\u7f16\u8f91\u5668\uff0c\u4fdd\u8bc1\u8bed\u6cd5\u6b63\u786e\u6027\u7684\u540c\u65f6\u5141\u8bb8\u4e0d\u5b8c\u6574\u7a0b\u5e8f\uff0c\u5e76\u6210\u529f\u5c06\u5176\u7f16\u7801\u81f3\u6269\u5c55\u03bb\u6f14\u7b97\u4f53\u7cfb\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8bed\u6cd5\u5bfc\u5411\u7f16\u8f91\u5668\u8bed\u8a00\u5c40\u9650\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u8de8\u8bed\u8a00\u901a\u7528\u7f16\u8f91\u6846\u67b6\uff0c\u5728\u4fdd\u8bc1\u8bed\u6cd5\u65e0\u9519\u8bef\u7684\u524d\u63d0\u4e0b\u652f\u6301\u7a0b\u5e8f\u7247\u6bb5\u7f16\u8f91\u3002", "method": "1. \u6269\u5c55\u8bed\u6cd5\u5bfc\u5411\u7f16\u8f91\u5668\u6f14\u7b97\u7684\u6cdb\u5316\u80fd\u529b 2. \u5efa\u7acb\u62bd\u8c61\u8bed\u6cd5\u4e0e\u7f16\u8f91\u5668\u884c\u4e3a\u7684\u6620\u5c04\u673a\u5236 3. \u5728\u542b\u6a21\u5f0f\u5339\u914d/\u4e0d\u52a8\u70b9\u7684\u03bb\u6f14\u7b97\u7cfb\u7edf\u4e2d\u8fdb\u884c\u5f62\u5f0f\u5316\u7f16\u7801", "result": "\u6784\u5efa\u51fa\u7406\u8bba\u5b8c\u5907\u7684\u7f16\u8f91\u5668\u751f\u6210\u6846\u67b6\uff0c\u8bc1\u660e\u5176\u8bed\u6cd5\u5bb9\u9519\u7279\u6027\uff0c\u5b8c\u6210\u5411\u6269\u5c55\u03bb\u6f14\u7b97\uff08\u542b\u5e03\u5c14\u3001\u5e8f\u5bf9\u3001\u6a21\u5f0f\u5339\u914d\u3001\u4e0d\u52a8\u70b9\uff09\u7684\u7cfb\u7edf\u8f6c\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u7a81\u7834\u8bed\u8a00\u7279\u5b9a\u9650\u5236\uff0c\u4e3a\u5f00\u53d1\u53ef\u9760\u4e14\u7075\u6d3b\u7684\u8bed\u6cd5\u611f\u77e5\u7f16\u8f91\u5668\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6e10\u8fdb\u5f0f\u7a0b\u5e8f\u5f00\u53d1\u573a\u666f\u3002"}}
{"id": "2505.18799", "pdf": "https://arxiv.org/pdf/2505.18799", "abs": "https://arxiv.org/abs/2505.18799", "authors": ["Hao Chen", "Haoze Li", "Zhiqing Xiao", "Lirong Gao", "Qi Zhang", "Xiaomeng Hu", "Ningtao Wang", "Xing Fu", "Junbo Zhao"], "title": "ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 8 figures, 14 tables", "summary": "Aligning general-purpose large language models (LLMs) to downstream tasks\noften incurs significant costs, including constructing task-specific\ninstruction pairs and extensive training adjustments. Prior research has\nexplored various avenues to enhance alignment efficiency, primarily through\nminimal-data training or data-driven activations to identify key attention\nheads. However, these approaches inherently introduce data dependency, which\nhinders generalization and reusability. To address this issue and enhance model\nalignment efficiency, we propose the \\textit{\\textbf{A}ttention\n\\textbf{L}ocalization and \\textbf{P}runing \\textbf{S}trategy (\\textbf{ALPS})},\nan efficient algorithm that localizes the most task-sensitive attention heads\nand prunes by restricting attention training updates to these heads, thereby\nreducing alignment costs. Experimental results demonstrate that our method\nactivates only \\textbf{10\\%} of attention parameters during fine-tuning while\nachieving a \\textbf{2\\%} performance improvement over baselines on three tasks.\nMoreover, the identified task-specific heads are transferable across datasets\nand mitigate knowledge forgetting. Our work and findings provide a novel\nperspective on efficient LLM alignment.", "AI": {"tldr": "\u63d0\u51faALPS\u6ce8\u610f\u529b\u5b9a\u4f4d\u4e0e\u526a\u679d\u7b56\u7565\uff0c\u901a\u8fc7\u4ec5\u5fae\u8c0310%\u6ce8\u610f\u529b\u53c2\u6570\u5b9e\u73b0\u6027\u80fd\u63d0\u53472%\uff0c\u4e14\u5177\u5907\u8de8\u6570\u636e\u96c6\u8fc1\u79fb\u80fd\u529b", "motivation": "\u73b0\u6709LLM\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u3001\u8bad\u7ec3\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u63d0\u5347\u6a21\u578b\u5bf9\u9f50\u6548\u7387\u5e76\u51cf\u5c11\u77e5\u8bc6\u9057\u5fd8", "method": "\u5b9a\u4f4d\u4efb\u52a1\u654f\u611f\u7684\u6ce8\u610f\u529b\u5934\uff0c\u901a\u8fc7\u526a\u679d\u7b56\u7565\u9650\u5236\u8bad\u7ec3\u66f4\u65b0\u4ec5\u5728\u5173\u952e\u5934\u90e8\u8fdb\u884c", "result": "\u6fc0\u6d3b10%\u53c2\u6570\u5b9e\u73b02%\u6027\u80fd\u63d0\u5347\uff0c\u4efb\u52a1\u76f8\u5173\u5934\u90e8\u53ef\u8de8\u6570\u636e\u96c6\u8fc1\u79fb\u5e76\u7f13\u89e3\u77e5\u8bc6\u9057\u5fd8", "conclusion": "ALPS\u4e3aLLM\u9ad8\u6548\u5bf9\u9f50\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u663e\u8457\u964d\u4f4e\u5bf9\u9f50\u6210\u672c\u540c\u65f6\u63d0\u5347\u6a21\u578b\u9002\u5e94\u6027"}}
{"id": "2505.18842", "pdf": "https://arxiv.org/pdf/2505.18842", "abs": "https://arxiv.org/abs/2505.18842", "authors": ["Jiwan Chung", "Junhyeok Kim", "Siyeol Kim", "Jaeyoung Lee", "Min Soo Kim", "Youngjae Yu"], "title": "Don't Look Only Once: Towards Multimodal Interactive Reasoning with Selective Visual Revisitation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "We present v1, a lightweight extension to Multimodal Large Language Models\n(MLLMs) that enables selective visual revisitation during inference. While\ncurrent MLLMs typically consume visual input only once and reason purely over\ninternal memory, v1 introduces a simple point-and-copy mechanism that allows\nthe model to dynamically retrieve relevant image regions throughout the\nreasoning process. This mechanism augments existing architectures with minimal\nmodifications, enabling contextual access to visual tokens based on the model's\nevolving hypotheses. To train this capability, we construct v1g, a dataset of\n300K multimodal reasoning traces with interleaved visual grounding annotations.\nExperiments on three multimodal mathematical reasoning benchmarks -- MathVista,\nMathVision, and MathVerse -- demonstrate that v1 consistently improves\nperformance over comparable baselines, particularly on tasks requiring\nfine-grained visual reference and multi-step reasoning. Our results suggest\nthat dynamic visual access is a promising direction for enhancing grounded\nmultimodal reasoning. Code, models, and data will be released to support future\nresearch.", "AI": {"tldr": "v1\u901a\u8fc7\u52a8\u6001\u89c6\u89c9\u8bbf\u95ee\u673a\u5236\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u7ec6\u7c92\u5ea6\u89c6\u89c9\u53c2\u8003\u80fd\u529b", "motivation": "\u73b0\u6709MLLMs\u5355\u6b21\u5904\u7406\u89c6\u89c9\u8f93\u5165\u540e\u4ec5\u4f9d\u8d56\u5185\u90e8\u8bb0\u5fc6\u63a8\u7406\uff0c\u96be\u4ee5\u5e94\u5bf9\u9700\u8981\u6301\u7eed\u89c6\u89c9\u53c2\u7167\u7684\u590d\u6742\u591a\u6a21\u6001\u4efb\u52a1", "method": "\u63d0\u51fa\u70b9\u9009\u590d\u5236\u673a\u5236\uff0c\u6784\u5efa30\u4e07\u89c4\u6a21\u7684v1g\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u6700\u5c0f\u67b6\u6784\u4fee\u6539\u5b9e\u73b0\u52a8\u6001\u89c6\u89c9\u6807\u8bb0\u68c0\u7d22", "result": "\u5728MathVista\u7b49\u4e09\u5927\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u63d0\u5347\u663e\u8457", "conclusion": "\u52a8\u6001\u89c6\u89c9\u8bbf\u95ee\u673a\u5236\u4e3a\u589e\u5f3a\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u8def\u5f84\uff0c\u4ee3\u7801\u6a21\u578b\u5f00\u6e90\u5c06\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55"}}
{"id": "2505.18845", "pdf": "https://arxiv.org/pdf/2505.18845", "abs": "https://arxiv.org/abs/2505.18845", "authors": ["Sagar Sapkota", "Mohammad Saqib Hasan", "Mubarak Shah", "Santu Karmaker"], "title": "Multi-Party Conversational Agents: A Survey", "categories": ["cs.CL"], "comment": null, "summary": "Multi-party Conversational Agents (MPCAs) are systems designed to engage in\ndialogue with more than two participants simultaneously. Unlike traditional\ntwo-party agents, designing MPCAs faces additional challenges due to the need\nto interpret both utterance semantics and social dynamics. This survey explores\nrecent progress in MPCAs by addressing three key questions: 1) Can agents model\neach participants' mental states? (State of Mind Modeling); 2) Can they\nproperly understand the dialogue content? (Semantic Understanding); and 3) Can\nthey reason about and predict future conversation flow? (Agent Action\nModeling). We review methods ranging from classical machine learning to Large\nLanguage Models (LLMs) and multi-modal systems. Our analysis underscores Theory\nof Mind (ToM) as essential for building intelligent MPCAs and highlights\nmulti-modal understanding as a promising yet underexplored direction. Finally,\nthis survey offers guidance to future researchers on developing more capable\nMPCAs.", "AI": {"tldr": "\u7cfb\u7edf\u7efc\u8ff0\u591a\u65b9\u5bf9\u8bdd\u4ee3\u7406(MPCAs)\u7684\u5efa\u6a21\u6311\u6218\u4e0e\u6280\u672f\u8fdb\u5c55\uff0c\u5f3a\u8c03\u5fc3\u7406\u7406\u8bba\u548c\u591a\u6a21\u6001\u7406\u89e3\u7684\u91cd\u8981\u6027", "motivation": "\u89e3\u51b3MPCAs\u5728\u591a\u65b9\u5bf9\u8bdd\u573a\u666f\u4e2d\u9700\u540c\u65f6\u89e3\u6790\u8bed\u4e49\u548c\u793e\u4ea4\u52a8\u6001\u7684\u53cc\u91cd\u6311\u6218\uff0c\u7a81\u7834\u4f20\u7edf\u53cc\u4eba\u5bf9\u8bdd\u4ee3\u7406\u7684\u5c40\u9650\u6027", "method": "\u7cfb\u7edf\u6027\u5206\u6790\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u3001LLM\u53ca\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u6784\u5efa\u4e09\u7ef4\u8bc4\u4f30\u6846\u67b6\uff08\u5fc3\u7406\u5efa\u6a21-\u8bed\u4e49\u7406\u89e3-\u884c\u4e3a\u9884\u6d4b\uff09", "result": "\u9a8c\u8bc1\u5fc3\u7406\u7406\u8bba(ToM)\u662f\u5b9e\u73b0\u667a\u80fdMPCAs\u7684\u6838\u5fc3\u8981\u7d20\uff0c\u53d1\u73b0\u591a\u6a21\u6001\u7406\u89e3\u65b9\u5411\u5b58\u5728\u91cd\u5927\u7814\u7a76\u7f3a\u53e3", "conclusion": "\u5efa\u8bae\u672a\u6765\u7814\u7a76\u805a\u7126\u5fc3\u7406\u5efa\u6a21\u7b97\u6cd5\u4f18\u5316\u548c\u591a\u6a21\u6001\u6570\u636e\u878d\u5408\uff0c\u63a8\u52a8\u66f4\u667a\u80fd\u7684MPCAs\u7cfb\u7edf\u53d1\u5c55"}}
{"id": "2505.18853", "pdf": "https://arxiv.org/pdf/2505.18853", "abs": "https://arxiv.org/abs/2505.18853", "authors": ["Alexander Shabalin", "Viacheslav Meshchaninov", "Dmitry Vetrov"], "title": "Smoothie: Smoothing Diffusion on Token Embeddings for Text Generation", "categories": ["cs.CL"], "comment": "17 pages, 2 figures, 8 tables", "summary": "Diffusion models have achieved state-of-the-art performance in generating\nimages, audio, and video, but their adaptation to text remains challenging due\nto its discrete nature. Prior approaches either apply Gaussian diffusion in\ncontinuous latent spaces, which inherits semantic structure but struggles with\ntoken decoding, or operate in categorical simplex space, which respect\ndiscreteness but disregard semantic relation between tokens. In this paper, we\npropose Smoothing Diffusion on Token Embeddings (Smoothie), a novel diffusion\nmethod that combines the strengths of both approaches by progressively\nsmoothing token embeddings based on semantic similarity. This technique enables\ngradual information removal while maintaining a natural decoding process.\nExperimental results on several sequence-to-sequence generation tasks\ndemonstrate that Smoothie outperforms existing diffusion-based models in\ngeneration quality. Furthermore, ablation studies show that our proposed\ndiffusion space yields better performance than both the standard embedding\nspace and the categorical simplex. Our code is available at\nhttps://github.com/ashaba1in/smoothie.", "AI": {"tldr": "\u63d0\u51faSmoothie\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u6e10\u8fdb\u5e73\u6ed1\u8bcd\u5d4c\u5165\uff0c\u7ed3\u5408\u8fde\u7eed\u7a7a\u95f4\u548c\u79bb\u6563\u7a7a\u95f4\u7684\u4f18\u52bf\u63d0\u5347\u6587\u672c\u751f\u6210\u8d28\u91cf", "motivation": "\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u6587\u672c\u751f\u6210\u4e2d\u5b58\u5728\u4e24\u96be\uff1a\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u4fdd\u7559\u8bed\u4e49\u4f46\u96be\u4ee5\u89e3\u7801\uff0c\u79bb\u6563\u7a7a\u95f4\u4fdd\u6301\u79bb\u6563\u6027\u4f46\u5ffd\u7565\u8bed\u4e49\u5173\u8054\u3002\u9700\u8981\u517c\u987e\u8bed\u4e49\u4fdd\u7559\u4e0e\u89e3\u7801\u53ef\u9760\u6027", "method": "Smoothing Diffusion on Token Embeddings (Smoothie)\uff1a\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u9010\u6b65\u5e73\u6ed1\u8bcd\u5d4c\u5165\uff0c\u5b9e\u73b0\u6e10\u8fdb\u4fe1\u606f\u6d88\u9664\u540c\u65f6\u4fdd\u6301\u81ea\u7136\u89e3\u7801\u8fc7\u7a0b", "result": "\u5728\u591a\u4e2aseq2seq\u4efb\u52a1\u4e2d\u751f\u6210\u8d28\u91cf\u8d85\u8d8a\u73b0\u6709\u6269\u6563\u6a21\u578b\uff0c\u6d88\u878d\u5b9e\u9a8c\u8bc1\u660e\u63d0\u51fa\u7684\u6269\u6563\u7a7a\u95f4\u4f18\u4e8e\u6807\u51c6\u5d4c\u5165\u7a7a\u95f4\u548c\u7c7b\u522b\u5355\u7eaf\u5f62", "conclusion": "Smoothie\u6210\u529f\u878d\u5408\u8fde\u7eed\u4e0e\u79bb\u6563\u65b9\u6cd5\u4f18\u52bf\uff0c\u901a\u8fc7\u8bed\u4e49\u5bfc\u5411\u7684\u5e73\u6ed1\u673a\u5236\u6709\u6548\u63d0\u5347\u6587\u672c\u751f\u6210\u6027\u80fd\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u5728NLP\u9886\u57df\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2505.18859", "pdf": "https://arxiv.org/pdf/2505.18859", "abs": "https://arxiv.org/abs/2505.18859", "authors": ["Yuxiang Liu", "Kevin Chen-Chuan Chang"], "title": "Writing Like the Best: Exemplar-Based Expository Text Generation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025. Camera-ready version", "summary": "We introduce the Exemplar-Based Expository Text Generation task, aiming to\ngenerate an expository text on a new topic using an exemplar on a similar\ntopic. Current methods fall short due to their reliance on extensive exemplar\ndata, difficulty in adapting topic-specific content, and issues with long-text\ncoherence. To address these challenges, we propose the concept of Adaptive\nImitation and present a novel Recurrent Plan-then-Adapt (RePA) framework. RePA\nleverages large language models (LLMs) for effective adaptive imitation through\na fine-grained plan-then-adapt process. RePA also enables recurrent\nsegment-by-segment imitation, supported by two memory structures that enhance\ninput clarity and output coherence. We also develop task-specific evaluation\nmetrics--imitativeness, adaptiveness, and adaptive-imitativeness--using LLMs as\nevaluators. Experimental results across our collected three diverse datasets\ndemonstrate that RePA surpasses existing baselines in producing factual,\nconsistent, and relevant texts for this task.", "AI": {"tldr": "\u63d0\u51faRePA\u6846\u67b6\u7528\u4e8e\u8303\u4f8b\u5f0f\u8bf4\u660e\u6587\u672c\u751f\u6210\u4efb\u52a1\uff0c\u901a\u8fc7\u5206\u6bb5\u81ea\u9002\u5e94\u6a21\u4eff\u673a\u5236\u548c\u8bb0\u5fc6\u7ed3\u6784\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8fc7\u5ea6\u4f9d\u8d56\u8303\u4f8b\u6570\u636e\u3001\u4e3b\u9898\u9002\u5e94\u6027\u5dee\u3001\u957f\u6587\u672c\u8fde\u8d2f\u6027\u4e0d\u8db3\u4e09\u5927\u7f3a\u9677\uff0c\u9700\u5f00\u53d1\u65b0\u6846\u67b6\u7a81\u7834\u9650\u5236", "method": "\u57fa\u4e8eLLMs\u7684\u5faa\u73af\u5f0f\u8ba1\u5212-\u8c03\u6574\u673a\u5236\uff0c\u91c7\u7528\u53cc\u8bb0\u5fc6\u7ed3\u6784\u589e\u5f3a\u8f93\u5165\u8f93\u51fa\u8fde\u8d2f\u6027\uff0c\u5b9e\u73b0\u5206\u6bb5\u81ea\u9002\u5e94\u6a21\u4eff", "result": "\u5728\u4e09\u4e2a\u591a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u751f\u6210\u6587\u672c\u7684\u4e8b\u5b9e\u6027\u3001\u4e00\u81f4\u6027\u3001\u76f8\u5173\u6027\u5747\u663e\u8457\u63d0\u5347", "conclusion": "RePA\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u6a21\u4eff\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u8303\u4f8b\u5229\u7528\u7387\u4f4e\u548c\u6587\u672c\u8fde\u8d2f\u6027\u95ee\u9898\uff0c\u914d\u5957\u8bc4\u4f30\u6307\u6807\u4e3a\u4efb\u52a1\u53d1\u5c55\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2505.18864", "pdf": "https://arxiv.org/pdf/2505.18864", "abs": "https://arxiv.org/abs/2505.18864", "authors": ["Binhao Ma", "Hanqing Guo", "Zhengping Jay Luo", "Rui Duan"], "title": "Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have\nsignificantly enhanced the naturalness and flexibility of human computer\ninteraction by enabling seamless understanding across text, vision, and audio\nmodalities. Among these, voice enabled models such as SpeechGPT have\ndemonstrated considerable improvements in usability, offering expressive, and\nemotionally responsive interactions that foster deeper connections in real\nworld communication scenarios. However, the use of voice introduces new\nsecurity risks, as attackers can exploit the unique characteristics of spoken\nlanguage, such as timing, pronunciation variability, and speech to text\ntranslation, to craft inputs that bypass defenses in ways not seen in\ntext-based systems. Despite substantial research on text based jailbreaks, the\nvoice modality remains largely underexplored in terms of both attack strategies\nand defense mechanisms. In this work, we present an adversarial attack\ntargeting the speech input of aligned MLLMs in a white box scenario.\nSpecifically, we introduce a novel token level attack that leverages access to\nthe model's speech tokenization to generate adversarial token sequences. These\nsequences are then synthesized into audio prompts, which effectively bypass\nalignment safeguards and to induce prohibited outputs. Evaluated on SpeechGPT,\nour approach achieves up to 89 percent attack success rate across multiple\nrestricted tasks, significantly outperforming existing voice based jailbreak\nmethods. Our findings shed light on the vulnerabilities of voice-enabled\nmultimodal systems and to help guide the development of more robust\nnext-generation MLLMs.", "AI": {"tldr": "\u9488\u5bf9\u8bed\u97f3\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u97f3\u6807\u8bb0\u5316\u751f\u6210\u5b9e\u73b089%\u653b\u51fb\u6210\u529f\u7387", "motivation": "\u8bed\u97f3\u4ea4\u4e92\u7cfb\u7edf\u5b58\u5728\u72ec\u7279\u5b89\u5168\u98ce\u9669\uff08\u5982\u8bed\u97f3\u65f6\u95f4\u7279\u6027/\u53d1\u97f3\u53d8\u5f02\u6027/\u8bed\u97f3\u8f6c\u6587\u672c\u6f0f\u6d1e\uff09\uff0c\u4f46\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u805a\u7126\u6587\u672c\u653b\u51fb\u800c\u7f3a\u4e4f\u8bed\u97f3\u653b\u9632\u673a\u5236\u63a2\u7d22", "method": "\u57fa\u4e8e\u767d\u76d2\u573a\u666f\u7684\u8bed\u97f3\u6807\u8bb0\u7ea7\u653b\u51fb\uff0c\u5229\u7528\u6a21\u578b\u8bed\u97f3\u5206\u8bcd\u673a\u5236\u751f\u6210\u5bf9\u6297\u6027\u6807\u8bb0\u5e8f\u5217\u5e76\u5408\u6210\u97f3\u9891\u7ed5\u8fc7\u5b89\u5168\u9632\u62a4", "result": "\u5728SpeechGPT\u4e0a\u5b9e\u73b089%\u653b\u51fb\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u8bed\u97f3\u8d8a\u72f1\u65b9\u6cd5\uff08\u591a\u573a\u666f\u9650\u5236\u4efb\u52a1\u9a8c\u8bc1\uff09", "conclusion": "\u63ed\u793a\u4e86\u8bed\u97f3\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u5b89\u5168\u8106\u5f31\u6027\uff0c\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u4e0b\u4e00\u4ee3MLLMs\u63d0\u4f9b\u91cd\u8981\u53c2\u8003"}}
{"id": "2505.18867", "pdf": "https://arxiv.org/pdf/2505.18867", "abs": "https://arxiv.org/abs/2505.18867", "authors": ["Ming Cheng", "Jiaying Gong", "Hoda Eldardiry"], "title": "Sci-LoRA: Mixture of Scientific LoRAs for Cross-Domain Lay Paraphrasing", "categories": ["cs.CL", "cs.LG"], "comment": "18 pages, 3 figures, ACL 2025 Findings", "summary": "Lay paraphrasing aims to make scientific information accessible to audiences\nwithout technical backgrounds. However, most existing studies focus on a single\ndomain, such as biomedicine. With the rise of interdisciplinary research, it is\nincreasingly necessary to comprehend knowledge spanning multiple technical\nfields. To address this, we propose Sci-LoRA, a model that leverages a mixture\nof LoRAs fine-tuned on multiple scientific domains. In particular, Sci-LoRA\ndynamically generates and applies weights for each LoRA, enabling it to adjust\nthe impact of different domains based on the input text, without requiring\nexplicit domain labels. To balance domain-specific knowledge and generalization\nacross various domains, Sci-LoRA integrates information at both the data and\nmodel levels. This dynamic fusion enhances the adaptability and performance\nacross various domains. Experimental results across twelve domains on five\npublic datasets show that Sci-LoRA significantly outperforms state-of-the-art\nlarge language models and demonstrates flexible generalization and adaptability\nin cross-domain lay paraphrasing.", "AI": {"tldr": "Sci-LoRA\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6df7\u5408LoRA\u6a21\u5757\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u9886\u57df\u6807\u7b7e\u5373\u53ef\u5b9e\u73b0\u8de8\u5b66\u79d1\u79d1\u5b66\u6587\u672c\u7684\u901a\u4fd7\u5316\u6539\u5199\uff0c\u572812\u4e2a\u9886\u57df\u548c5\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5927\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u79d1\u5b66\u6587\u672c\u901a\u4fd7\u5316\u6539\u5199\u6a21\u578b\u4e3b\u8981\u5c40\u9650\u4e8e\u5355\u4e00\u9886\u57df\uff08\u5982\u751f\u7269\u533b\u5b66\uff09\uff0c\u800c\u8de8\u5b66\u79d1\u7814\u7a76\u9700\u8981\u6a21\u578b\u5177\u5907\u591a\u9886\u57df\u9002\u5e94\u80fd\u529b\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u65e0\u9700\u663e\u5f0f\u9886\u57df\u6807\u6ce8\u5373\u53ef\u52a8\u6001\u878d\u5408\u591a\u9886\u57df\u77e5\u8bc6\u7684\u6539\u5199\u65b9\u6cd5\u3002", "method": "1. \u91c7\u7528\u6df7\u5408LoRA\u67b6\u6784\uff1a\u4e3a\u4e0d\u540c\u79d1\u5b66\u9886\u57df\u5fae\u8c03\u72ec\u7acbLoRA\u6a21\u5757\n2. \u52a8\u6001\u6743\u91cd\u673a\u5236\uff1a\u6839\u636e\u8f93\u5165\u6587\u672c\u81ea\u52a8\u751f\u6210\u5404\u9886\u57dfLoRA\u7684\u878d\u5408\u6743\u91cd\n3. \u53cc\u5c42\u6b21\u878d\u5408\u7b56\u7565\uff1a\u5728\u6570\u636e\u5c42\u9762\u901a\u8fc7\u591a\u9886\u57df\u9884\u8bad\u7ec3\uff0c\u5728\u6a21\u578b\u5c42\u9762\u901a\u8fc7\u95e8\u63a7\u7f51\u7edc\u5b9e\u73b0\u52a8\u6001\u53c2\u6570\u7ec4\u5408", "result": "\u572812\u4e2a\u79d1\u5b66\u9886\u57df\u76845\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0cSci-LoRA\u5728\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\uff08\u5982BLEU\u3001ROUGE\uff09\u548c\u4eba\u5de5\u8bc4\u4f30\u4e2d\u5747\u8d85\u8d8aGPT-4\u7b49\u57fa\u7ebf\u6a21\u578b\uff0c\u5c24\u5176\u5728\u8de8\u9886\u57df\u573a\u666f\u4e0b\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u52a8\u6001\u53c2\u6570\u878d\u5408\u673a\u5236\u6210\u529f\u89e3\u51b3\u4e86\u591a\u9886\u57df\u79d1\u5b66\u6587\u672c\u6539\u5199\u96be\u9898\uff0c\u4e3a\u8de8\u5b66\u79d1\u77e5\u8bc6\u4f20\u64ad\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u65b9\u6848\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u66f4\u591a\u79d1\u5b66\u9886\u57df\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u3002"}}
{"id": "2505.18878", "pdf": "https://arxiv.org/pdf/2505.18878", "abs": "https://arxiv.org/abs/2505.18878", "authors": ["Kung-Hsiang Huang", "Akshara Prabhakar", "Onkar Thorat", "Divyansh Agarwal", "Prafulla Kumar Choubey", "Yixin Mao", "Silvio Savarese", "Caiming Xiong", "Chien-Sheng Wu"], "title": "CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While AI agents hold transformative potential in business, effective\nperformance benchmarking is hindered by the scarcity of public, realistic\nbusiness data on widely used platforms. Existing benchmarks often lack fidelity\nin their environments, data, and agent-user interactions, with limited coverage\nof diverse business scenarios and industries. To address these gaps, we\nintroduce CRMArena-Pro, a novel benchmark for holistic, realistic assessment of\nLLM agents in diverse professional settings. CRMArena-Pro expands on CRMArena\nwith nineteen expert-validated tasks across sales, service, and 'configure,\nprice, and quote' processes, for both Business-to-Business and\nBusiness-to-Customer scenarios. It distinctively incorporates multi-turn\ninteractions guided by diverse personas and robust confidentiality awareness\nassessments. Experiments reveal leading LLM agents achieve only around 58%\nsingle-turn success on CRMArena-Pro, with performance dropping significantly to\napproximately 35% in multi-turn settings. While Workflow Execution proves more\ntractable for top agents (over 83% single-turn success), other evaluated\nbusiness skills present greater challenges. Furthermore, agents exhibit\nnear-zero inherent confidentiality awareness; though targeted prompting can\nimprove this, it often compromises task performance. These findings highlight a\nsubstantial gap between current LLM capabilities and enterprise demands,\nunderscoring the need for advancements in multi-turn reasoning, confidentiality\nadherence, and versatile skill acquisition.", "AI": {"tldr": "CRMArena-Pro\u63a8\u51fa\u65b0\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u73b0\u6709LLM\u667a\u80fd\u4f53\u5728\u591a\u8f6e\u4ea4\u4e92\u4e0e\u4fdd\u5bc6\u6027\u65b9\u9762\u7684\u663e\u8457\u80fd\u529b\u5dee\u8ddd\uff08\u5355\u8f6e\u6210\u529f\u738758% vs \u591a\u8f6e35%\uff09\uff0c\u66b4\u9732\u4f01\u4e1a\u7ea7\u5e94\u7528\u9700\u6c42\u9e3f\u6c9f", "motivation": "\u73b0\u6709\u5546\u4e1aAI\u8bc4\u4f30\u57fa\u51c6\u5b58\u5728\u73af\u5883\u771f\u5b9e\u6027\u4e0d\u8db3\u3001\u884c\u4e1a\u8986\u76d6\u5355\u4e00\u3001\u4ea4\u4e92\u4eff\u771f\u5ea6\u4f4e\u7b49\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u8861\u91cf\u667a\u80fd\u4f53\u771f\u5b9e\u4e1a\u52a1\u8868\u73b0", "method": "\u5728CRMArena\u57fa\u7840\u4e0a\u6269\u5c5519\u4e2a\u4e13\u5bb6\u9a8c\u8bc1\u4efb\u52a1\uff0c\u8986\u76d6B2B/B2C\u9500\u552e/\u670d\u52a1/\u62a5\u4ef7\u5168\u6d41\u7a0b\uff0c\u521b\u65b0\u5f15\u5165\u591a\u8f6e\u89d2\u8272\u626e\u6f14\u673a\u5236\u4e0e\u4fdd\u5bc6\u610f\u8bc6\u8bc4\u4f30\u4f53\u7cfb", "result": "\u5934\u90e8LLM\u667a\u80fd\u4f53\u5355\u8f6e\u4efb\u52a1\u6210\u529f\u7387\u4ec558%\uff08\u591a\u8f6e\u9aa4\u964d\u81f335%\uff09\uff0c\u5de5\u4f5c\u6d41\u6267\u884c\u8fbe83%\u4f46\u5546\u4e1a\u6280\u80fd\u8584\u5f31\uff0c\u4fdd\u5bc6\u610f\u8bc6\u539f\u751f\u63a5\u8fd1\u96f6\uff08\u9700\u7279\u5b9a\u63d0\u793a\u4e14\u5f71\u54cd\u4efb\u52a1\u8868\u73b0\uff09", "conclusion": "\u5f53\u524dLLM\u9700\u7a81\u7834\u591a\u8f6e\u63a8\u7406\u3001\u4fdd\u5bc6\u5408\u89c4\u4e0e\u591a\u573a\u666f\u9002\u5e94\u80fd\u529b\u74f6\u9888\uff0c\u624d\u80fd\u771f\u6b63\u6ee1\u8db3\u4f01\u4e1a\u7ea7\u590d\u6742\u5546\u4e1a\u573a\u666f\u9700\u6c42"}}
{"id": "2505.18903", "pdf": "https://arxiv.org/pdf/2505.18903", "abs": "https://arxiv.org/abs/2505.18903", "authors": ["Valentin Barriere", "Nahuel Gomez", "Leo Hemamou", "Sofia Callejas", "Brian Ravenet"], "title": "StandUp4AI: A New Multilingual Dataset for Humor Detection in Stand-up Comedy Videos", "categories": ["cs.CL"], "comment": null, "summary": "Aiming towards improving current computational models of humor detection, we\npropose a new multimodal dataset of stand-up comedies, in seven languages:\nEnglish, French, Spanish, Italian, Portuguese, Hungarian and Czech. Our dataset\nof more than 330 hours, is at the time of writing the biggest available for\nthis type of task, and the most diverse. The whole dataset is automatically\nannotated in laughter (from the audience), and the subpart left for model\nvalidation is manually annotated. Contrary to contemporary approaches, we do\nnot frame the task of humor detection as a binary sequence classification, but\nas word-level sequence labeling, in order to take into account all the context\nof the sequence and to capture the continuous joke tagging mechanism typically\noccurring in natural conversations. As par with unimodal baselines results, we\npropose a method for e propose a method to enhance the automatic laughter\ndetection based on Audio Speech Recognition errors. Our code and data are\navailable online: https://tinyurl.com/EMNLPHumourStandUpPublic", "AI": {"tldr": "\u7814\u7a76\u8005\u6784\u5efa\u4e86\u76ee\u524d\u6700\u5927\u89c4\u6a21\u7684\u591a\u8bed\u8a00\u559c\u5267\u8868\u6f14\u6570\u636e\u96c6\uff08330+\u5c0f\u65f6/7\u79cd\u8bed\u8a00\uff09\uff0c\u5c06\u5e7d\u9ed8\u68c0\u6d4b\u4efb\u52a1\u91cd\u6784\u4e3a\u8bcd\u7ea7\u5e8f\u5217\u6807\u6ce8\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u8bed\u97f3\u8bc6\u522b\u9519\u8bef\u7684\u81ea\u52a8\u7b11\u58f0\u589e\u5f3a\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u5e7d\u9ed8\u68c0\u6d4b\u6a21\u578b\u5b58\u5728\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\u3001\u4efb\u52a1\u8bbe\u8ba1\u8fc7\u4e8e\u7b80\u5316\uff08\u4e8c\u5206\u7c7b\uff09\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u6355\u6349\u81ea\u7136\u5bf9\u8bdd\u4e2d\u8fde\u7eed\u5e7d\u9ed8\u6807\u8bb0\u673a\u5236\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u96c6\u548c\u7ec6\u7c92\u5ea6\u4efb\u52a1\u8bbe\u8ba1\u6539\u8fdb\u6a21\u578b\u6027\u80fd\u3002", "method": "1. \u6536\u96c6\u4e03\u79cd\u8bed\u8a00\u7684\u5355\u53e3\u559c\u5267\u8868\u6f14\u6784\u5efa\u591a\u6a21\u6001\u6570\u636e\u96c6\uff08\u81ea\u52a8\u6807\u6ce8\u7b11\u58f0+\u4eba\u5de5\u9a8c\u8bc1\uff09\n2. \u5c06\u5e7d\u9ed8\u68c0\u6d4b\u91cd\u6784\u4e3a\u8bcd\u7ea7\u5e8f\u5217\u6807\u6ce8\u4efb\u52a1\n3. \u5f00\u53d1\u57fa\u4e8e\u8bed\u97f3\u8bc6\u522b\u9519\u8bef\u5206\u6790\u7684\u81ea\u52a8\u7b11\u58f0\u68c0\u6d4b\u589e\u5f3a\u65b9\u6cd5", "result": "\u521b\u5efa\u4e86\u5f53\u524d\u6700\u5927\u6700\u4e30\u5bcc\u7684\u5e7d\u9ed8\u7814\u7a76\u6570\u636e\u96c6\uff08330+\u5c0f\u65f6/7\u8bed\u8a00\uff09\uff0c\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u7684\u5e7d\u9ed8\u6807\u8bb0\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u97f3\u9519\u8bef\u7279\u5f81\u63d0\u5347\u4e86\u7b11\u58f0\u68c0\u6d4b\u7684\u81ea\u52a8\u5316\u6c34\u5e73\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u6570\u636e\u89c4\u6a21\u6269\u5c55\u548c\u4efb\u52a1\u8303\u5f0f\u521b\u65b0\uff0c\u4e3a\u5bf9\u8bdd\u573a\u666f\u7684\u8fde\u7eed\u5e7d\u9ed8\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u591a\u8bed\u8a00\u7279\u6027\u652f\u6301\u8de8\u6587\u5316\u5e7d\u9ed8\u673a\u5236\u5206\u6790\u3002"}}
{"id": "2505.18905", "pdf": "https://arxiv.org/pdf/2505.18905", "abs": "https://arxiv.org/abs/2505.18905", "authors": ["Kweku Andoh Yamoah", "Jackson Weako", "Emmanuel J. Dorley"], "title": "Building a Functional Machine Translation Corpus for Kpelle", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we introduce the first publicly available English-Kpelle\ndataset for machine translation, comprising over 2000 sentence pairs drawn from\neveryday communication, religious texts, and educational materials. By\nfine-tuning Meta's No Language Left Behind(NLLB) model on two versions of the\ndataset, we achieved BLEU scores of up to 30 in the Kpelle-to-English\ndirection, demonstrating the benefits of data augmentation. Our findings align\nwith NLLB-200 benchmarks on other African languages, underscoring Kpelle's\npotential for competitive performance despite its low-resource status. Beyond\nmachine translation, this dataset enables broader NLP tasks, including speech\nrecognition and language modelling. We conclude with a roadmap for future\ndataset expansion, emphasizing orthographic consistency, community-driven\nvalidation, and interdisciplinary collaboration to advance inclusive language\ntechnology development for Kpelle and other low-resourced Mande languages.", "AI": {"tldr": "\u521b\u5efa\u9996\u4e2a\u516c\u5f00\u7684\u82f1\u8bed-Kpelle\u673a\u5668\u7ffb\u8bd1\u6570\u636e\u96c6\uff082000+\u53e5\u5bf9\uff09\uff0c\u901a\u8fc7\u5fae\u8c03NLLB\u6a21\u578b\u5b9e\u73b0BLEU 30\u7684\u7ffb\u8bd1\u6548\u679c\uff0c\u9a8c\u8bc1\u6570\u636e\u589e\u5f3a\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u672a\u6765\u6570\u636e\u96c6\u6269\u5c55\u8def\u7ebf", "motivation": "\u89e3\u51b3Kpelle\u4f5c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7f3a\u4e4f\u516c\u5f00\u53ef\u7528\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u4fc3\u8fdb\u8be5\u8bed\u8a00\u7684\u673a\u5668\u7ffb\u8bd1\u53ca\u5176\u4ed6NLP\u6280\u672f\u53d1\u5c55\uff0c\u63a2\u7d22\u6570\u636e\u589e\u5f3a\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u5e94\u7528\u4ef7\u503c", "method": "\u4ece\u65e5\u5e38\u4ea4\u6d41/\u5b97\u6559\u6587\u672c/\u6559\u80b2\u6750\u6599\u4e09\u4e2a\u9886\u57df\u6536\u96c6\u8bed\u6599\u6784\u5efa\u53cc\u8bed\u6570\u636e\u96c6\uff0c\u91c7\u7528Meta\u7684NLLB\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u5b9e\u9a8c\uff0c\u5bf9\u6bd4\u5206\u6790\u4e0d\u540c\u6570\u636e\u589e\u5f3a\u7b56\u7565\u7684\u6548\u679c", "result": "Kpelle-English\u65b9\u5411\u53d6\u5f97BLEU 30\u7684\u6700\u4f73\u6210\u7ee9\uff0c\u4e0eNLLB-200\u5728\u5176\u4ed6\u975e\u6d32\u8bed\u8a00\u7684\u57fa\u51c6\u8868\u73b0\u4e00\u81f4\uff0c\u9a8c\u8bc1\u4e86\u6570\u636e\u589e\u5f3a\u7b56\u7565\u7684\u6709\u6548\u6027\u548cKpelle\u7684\u6280\u672f\u53ef\u884c\u6027", "conclusion": "\u63d0\u51fa\u901a\u8fc7\u6b63\u5b57\u6cd5\u7edf\u4e00\u3001\u793e\u533a\u9a8c\u8bc1\u548c\u8de8\u5b66\u79d1\u5408\u4f5c\u7684\u4e09\u6b65\u8d70\u8ba1\u5212\uff0c\u63a8\u52a8Kpelle\u53ca\u5176\u4ed6Mande\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u5305\u5bb9\u6027\u8bed\u8a00\u6280\u672f\u53d1\u5c55"}}
{"id": "2505.18906", "pdf": "https://arxiv.org/pdf/2505.18906", "abs": "https://arxiv.org/abs/2505.18906", "authors": ["Abhijit Chakraborty", "Chahana Dahal", "Vivek Gupta"], "title": "Federated Retrieval-Augmented Generation: A Systematic Mapping Study", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Federated Retrieval-Augmented Generation (Federated RAG) combines Federated\nLearning (FL), which enables distributed model training without exposing raw\ndata, with Retrieval-Augmented Generation (RAG), which improves the factual\naccuracy of language models by grounding outputs in external knowledge. As\nlarge language models are increasingly deployed in privacy-sensitive domains\nsuch as healthcare, finance, and personalized assistance, Federated RAG offers\na promising framework for secure, knowledge-intensive natural language\nprocessing (NLP). To the best of our knowledge, this paper presents the first\nsystematic mapping study of Federated RAG, covering literature published\nbetween 2020 and 2025. Following Kitchenham's guidelines for evidence-based\nsoftware engineering, we develop a structured classification of research\nfocuses, contribution types, and application domains. We analyze architectural\npatterns, temporal trends, and key challenges, including privacy-preserving\nretrieval, cross-client heterogeneity, and evaluation limitations. Our findings\nsynthesize a rapidly evolving body of research, identify recurring design\npatterns, and surface open questions, providing a foundation for future work at\nthe intersection of RAG and federated systems.", "AI": {"tldr": "\u9996\u7bc7\u7cfb\u7edf\u7efc\u8ff0\u8054\u90a6\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08Federated RAG\uff09\u6280\u672f\uff0c\u878d\u5408\u8054\u90a6\u5b66\u4e60\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff0c\u4e3a\u9690\u79c1\u654f\u611f\u9886\u57df\u7684NLP\u5e94\u7528\u63d0\u4f9b\u5b89\u5168\u6846\u67b6\u3002", "motivation": "\u89e3\u51b3\u533b\u7597\u3001\u91d1\u878d\u7b49\u9690\u79c1\u654f\u611f\u9886\u57df\u7684\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u9700\u6c42\uff0c\u5728\u4fdd\u8bc1\u6570\u636e\u9690\u79c1\u524d\u63d0\u4e0b\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528Kitchenham\u8bc1\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5bf92020-2025\u5e74\u95f4\u6587\u732e\u8fdb\u884c\u7cfb\u7edf\u6620\u5c04\u7814\u7a76\uff0c\u6784\u5efa\u7814\u7a76\u7126\u70b9\u5206\u7c7b\u4f53\u7cfb\u5e76\u5206\u6790\u67b6\u6784\u6a21\u5f0f\u3001\u65f6\u5e8f\u8d8b\u52bf\u3002", "result": "\u63ed\u793a\u8be5\u9886\u57df\u5feb\u901f\u6f14\u8fdb\u6001\u52bf\uff0c\u8bc6\u522b\u51fa\u9690\u79c1\u4fdd\u62a4\u68c0\u7d22\u3001\u8de8\u5ba2\u6237\u7aef\u5f02\u6784\u6027\u3001\u8bc4\u4f30\u5c40\u9650\u7b49\u6838\u5fc3\u6311\u6218\uff0c\u603b\u7ed3\u51fa\u53ef\u590d\u7528\u7684\u7cfb\u7edf\u8bbe\u8ba1\u6a21\u5f0f\u3002", "conclusion": "\u8054\u90a6RAG\u4e3a\u9690\u79c1\u654f\u611fNLP\u5f00\u8f9f\u65b0\u8def\u5f84\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u540e\u7eed\u8054\u90a6\u7cfb\u7edf\u4e0eRAG\u6280\u672f\u7684\u878d\u5408\u521b\u65b0\u5960\u5b9a\u7406\u8bba\u57fa\u7840\uff0c\u4e9f\u5f85\u89e3\u51b3\u9690\u79c1-\u6027\u80fd\u5e73\u8861\u96be\u9898\u3002"}}
{"id": "2505.18916", "pdf": "https://arxiv.org/pdf/2505.18916", "abs": "https://arxiv.org/abs/2505.18916", "authors": ["Yue Li", "Jake Vasilakes", "Zhixue Zhao", "Carolina Scarton"], "title": "SCRum-9: Multilingual Stance Classification over Rumours on Social Media", "categories": ["cs.CL"], "comment": null, "summary": "We introduce SCRum-9, a multilingual dataset for Rumour Stance\nClassification, containing 7,516 tweet-reply pairs from X. SCRum-9 goes beyond\nexisting stance classification datasets by covering more languages (9), linking\nexamples to more fact-checked claims (2.1k), and including complex annotations\nfrom multiple annotators to account for intra- and inter-annotator variability.\nAnnotations were made by at least three native speakers per language, totalling\naround 405 hours of annotation and 8,150 dollars in compensation. Experiments\non SCRum-9 show that it is a challenging benchmark for both state-of-the-art\nLLMs (e.g. Deepseek) as well as fine-tuned pre-trained models, motivating\nfuture work in this area.", "AI": {"tldr": "\u591a\u8bed\u8a00\u8c23\u8a00\u7acb\u573a\u5206\u7c7b\u6570\u636e\u96c6SCRum-9\u53d1\u5e03\uff0c\u8986\u76d69\u79cd\u8bed\u8a00\u5e76\u5173\u80542100+\u4e8b\u5b9e\u6838\u67e5\u58f0\u660e\uff0c\u5b9e\u9a8c\u663e\u793a\u5bf9LLM\u548c\u5fae\u8c03\u6a21\u578b\u5747\u5177\u6311\u6218\u6027", "motivation": "\u73b0\u6709\u7acb\u573a\u5206\u7c7b\u6570\u636e\u96c6\u5b58\u5728\u8bed\u8a00\u8986\u76d6\u6709\u9650\u3001\u7f3a\u4e4f\u4e8b\u5b9e\u6838\u67e5\u5173\u8054\u53ca\u590d\u6742\u6ce8\u91ca\u7684\u7f3a\u9677\uff0c\u9700\u5efa\u7acb\u66f4\u5168\u9762\u7684\u591a\u8bed\u8a00\u57fa\u51c6\u6570\u636e\u96c6", "method": "\u901a\u8fc7\u81f3\u5c113\u540d\u6bcd\u8bed\u6807\u6ce8\u8005/\u8bed\u8a00\u7684\u4f17\u5305\u6807\u6ce8\uff08\u603b\u8017\u65f6405\u5c0f\u65f6/\u6210\u672c8150\u7f8e\u5143\uff09\uff0c\u6784\u5efa\u5305\u542b7516\u63a8\u6587\u5bf9\u7684\u590d\u6742\u6ce8\u91ca\u6570\u636e\u96c6", "result": "Deepseek\u7b49\u5148\u8fdb\u6a21\u578b\u5728SCRum-9\u4e0a\u8868\u73b0\u6b20\u4f73\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6570\u636e\u96c6\u4f5c\u4e3a\u6311\u6218\u6027\u57fa\u51c6\u7684\u4ef7\u503c", "conclusion": "SCRum-9\u586b\u8865\u4e86\u591a\u8bed\u8a00\u8c23\u8a00\u7acb\u573a\u68c0\u6d4b\u9886\u57df\u7684\u6570\u636e\u7f3a\u53e3\uff0c\u4e3a\u6a21\u578b\u9c81\u68d2\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2505.18927", "pdf": "https://arxiv.org/pdf/2505.18927", "abs": "https://arxiv.org/abs/2505.18927", "authors": ["Amel Muminovic"], "title": "Benchmarking Large Language Models for Cyberbullying Detection in Real-World YouTube Comments", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint. 9 pages, 3 tables, 1 figure. Not yet submitted to a\n  journal. Feedback welcome", "summary": "As online platforms grow, comment sections increasingly host harassment that\nundermines user experience and well-being. This study benchmarks three leading\nlarge language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic\nClaude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse\nthreads in gaming, lifestyle, food vlog, and music channels. The dataset\ncomprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and\nIndonesian, annotated independently by two reviewers with substantial agreement\n(Cohen's kappa = 0.83). Using a unified prompt and deterministic settings,\nGPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision\nof 0.887, and recall of 0.841. Gemini flagged the highest share of harmful\nposts (recall = 0.875) but its precision fell to 0.767 due to frequent false\npositives. Claude delivered the highest precision at 0.920 and the lowest\nfalse-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative\nanalysis showed that all three models struggle with sarcasm, coded insults, and\nmixed-language slang. These results underscore the need for moderation\npipelines that combine complementary models, incorporate conversational\ncontext, and fine-tune for under-represented languages and implicit abuse. A\nde-identified version of the dataset and full prompts is publicly released to\npromote reproducibility and further progress in automated content moderation.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86GPT-4.1\u3001Gemini 1.5 Pro\u548cClaude 3 Opus\u5728\u6709\u5bb3\u8bc4\u8bba\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\uff0cGPT-4.1\u7efc\u5408\u6700\u4f18\uff08F1=0.863\uff09\uff0c\u5f3a\u8c03\u9700\u7ec4\u5408\u6a21\u578b\u5e76\u6539\u8fdb\u591a\u8bed\u8a00\u5904\u7406", "motivation": "\u8bc4\u4f30\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u591a\u8bed\u8a00\u6709\u5bb3\u8bc4\u8bba\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u5185\u5bb9\u5ba1\u6838\u7cfb\u7edf\u5728\u9690\u6666\u9a9a\u6270\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u4e0a\u7684\u4e0d\u8db3", "method": "\u4f7f\u75285,080\u6761YouTube\u8bc4\u8bba\u6570\u636e\u96c6\uff08\u542b\u82f1/\u963f/\u5370\u5c3c\u8bed\uff09\uff0c\u901a\u8fc7\u53cc\u4eba\u72ec\u7acb\u6807\u6ce8\uff08\u03ba=0.83\uff09\uff0c\u91c7\u7528\u7edf\u4e00prompt\u548c\u786e\u5b9a\u6027\u53c2\u6570\u8bc4\u4f30\u6a21\u578b\u6027\u80fd", "result": "GPT-4.1\u7efc\u5408\u6700\u4f73\uff08F1=0.863/Precision=0.887/Recall=0.841\uff09\uff0cGemini\u53ec\u56de\u6700\u9ad8\uff080.875\uff09\u4f46\u51c6\u786e\u7387\u4f4e\uff080.767\uff09\uff0cClaude\u7cbe\u51c6\u6700\u9ad8\uff080.920\uff09\u4f46\u6f0f\u68c0\u591a\uff08Recall=0.720\uff09", "conclusion": "\u9700\u6784\u5efa\u7ec4\u5408\u6a21\u578b\u7ba1\u9053\uff0c\u6574\u5408\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u9488\u5bf9\u5c0f\u8bed\u79cd\u548c\u9690\u6666\u9a9a\u6270\u8fdb\u884c\u5fae\u8c03\u3002\u516c\u5f00\u6570\u636e\u96c6\u4fc3\u8fdb\u5185\u5bb9\u5ba1\u6838\u6280\u672f\u53d1\u5c55"}}
{"id": "2505.18943", "pdf": "https://arxiv.org/pdf/2505.18943", "abs": "https://arxiv.org/abs/2505.18943", "authors": ["Xuanming Zhang", "Yuxuan Chen", "Min-Hsuan Yeh", "Yixuan Li"], "title": "MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent Systems", "categories": ["cs.CL"], "comment": null, "summary": "Human social interactions depend on the ability to infer others' unspoken\nintentions, emotions, and beliefs-a cognitive skill grounded in the\npsychological concept of Theory of Mind (ToM). While large language models\n(LLMs) excel in semantic understanding tasks, they struggle with the ambiguity\nand contextual nuance inherent in human communication. To bridge this gap, we\nintroduce MetaMind, a multi-agent framework inspired by psychological theories\nof metacognition, designed to emulate human-like social reasoning. MetaMind\ndecomposes social understanding into three collaborative stages: (1) a\nTheory-of-Mind Agent generates hypotheses user mental states (e.g., intent,\nemotion), (2) a Domain Agent refines these hypotheses using cultural norms and\nethical constraints, and (3) a Response Agent generates contextually\nappropriate responses while validating alignment with inferred intent. Our\nframework achieves state-of-the-art performance across three challenging\nbenchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain\nin ToM reasoning. Notably, it enables LLMs to match human-level performance on\nkey ToM tasks for the first time. Ablation studies confirm the necessity of all\ncomponents, which showcase the framework's ability to balance contextual\nplausibility, social appropriateness, and user adaptation. This work advances\nAI systems toward human-like social intelligence, with applications in\nempathetic dialogue and culturally sensitive interactions. Code is available at\nhttps://github.com/XMZhangAI/MetaMind.", "AI": {"tldr": "MetaMind\u6846\u67b6\u901a\u8fc7\u5143\u8ba4\u77e5\u542f\u53d1\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u663e\u8457\u63d0\u5347LLMs\u5728\u5fc3\u7406\u7406\u8bba\u4efb\u52a1\u4e2d\u7684\u793e\u4ea4\u63a8\u7406\u80fd\u529b\uff0c\u9996\u6b21\u5b9e\u73b0\u4eba\u7c7b\u6c34\u5e73\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u4eba\u7c7b\u793e\u4ea4\u610f\u56fe\u3001\u60c5\u611f\u7b49\u5fc3\u7406\u72b6\u6001\u65f6\u7684\u6a21\u7cca\u6027\u548c\u8bed\u5883\u654f\u611f\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u5143\u8ba4\u77e5\u8fc7\u7a0b\u6784\u5efa\u793e\u4ea4\u667a\u80fd\u6846\u67b6\u3002", "method": "\u4e09\u9636\u6bb5\u534f\u4f5c\u67b6\u6784\uff1a1.\u5fc3\u7406\u7406\u8bba\u667a\u80fd\u4f53\u751f\u6210\u5fc3\u667a\u5047\u8bbe\uff1b2.\u9886\u57df\u667a\u80fd\u4f53\u57fa\u4e8e\u6587\u5316\u89c4\u8303\u4fee\u6b63\u5047\u8bbe\uff1b3.\u54cd\u5e94\u667a\u80fd\u4f53\u751f\u6210\u5e76\u9a8c\u8bc1\u8bed\u5883\u9002\u914d\u7684\u56de\u5e94\u3002", "result": "\u5728\u4e09\u5927\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\uff0c\u771f\u5b9e\u793e\u4ea4\u573a\u666f\u63d0\u534735.7%\uff0cToM\u63a8\u7406\u63d0\u53476.2%\uff0c\u9996\u6b21\u5b9e\u73b0\u5173\u952e\u5fc3\u7406\u7406\u8bba\u4efb\u52a1\u7684\u4eba\u7c7b\u6c34\u5e73\u8868\u73b0\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ec4\u4ef6\u534f\u540c\u4f5c\u7528\u5e73\u8861\u8bed\u5883\u5408\u7406\u6027\u3001\u793e\u4f1a\u9002\u5207\u6027\u4e0e\u7528\u6237\u9002\u914d\u6027\uff0c\u63a8\u52a8AI\u7cfb\u7edf\u5411\u7c7b\u4eba\u793e\u4ea4\u667a\u80fd\u53d1\u5c55\uff0c\u9002\u7528\u4e8e\u5171\u60c5\u5bf9\u8bdd\u548c\u8de8\u6587\u5316\u4ea4\u4e92\u573a\u666f\u3002"}}
{"id": "2505.18949", "pdf": "https://arxiv.org/pdf/2505.18949", "abs": "https://arxiv.org/abs/2505.18949", "authors": ["Longfei Yun", "Chenyang An", "Zilong Wang", "Letian Peng", "Jingbo Shang"], "title": "The Price of Format: Diversity Collapse in LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "14 pages, 7 figures", "summary": "Instruction-tuned large language models (LLMs) employ structured templates,\nsuch as role markers and special tokens, to enforce format consistency during\ninference. However, we identify a critical limitation of such formatting: it\ninduces a phenomenon we term diversity collapse, where the model generates\nsemantically similar outputs for open-ended inputs, undermining creativity and\nvariability. We systematically evaluate this effect across tasks like story\ncompletion and free-form generation, finding that (1) diversity collapse\npersists even under high-temperature sampling, and (2) structural tokens in\ntemplates significantly constrain the model's output space. To contextualize\nthese findings, we fine-tune the same model using a range of structured prompts\nand then evaluate them across three axes: downstream task performance,\nalignment behavior, and output diversity. Our analysis shows that format\nconsistency between fine-tuning and inference is crucial for\nstructure-sensitive tasks (e.g., GSM8K, IFEval), but has marginal influence on\nknowledge-heavy tasks (e.g., MMLU, WebQuestions). In contrast, output diversity\nis primarily governed by the presence or absence of structural tokens, with\nminimal formatting yielding the most diverse outputs. These findings reveal\nthat current prompting conventions, while beneficial for alignment, may\ninadvertently suppress output diversity, underscoring the need for\ndiversity-aware prompt design and instruction tuning.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6307\u4ee4\u8c03\u4f18\u7684LLMs\u4f7f\u7528\u7684\u7ed3\u6784\u5316\u6a21\u677f\u4f1a\u5bfc\u81f4\u8f93\u51fa\u591a\u6837\u6027\u5d29\u6e83\uff0c\u9650\u5236\u521b\u9020\u6027\uff0c\u9700\u6539\u8fdb\u63d0\u793a\u8bbe\u8ba1\u4ee5\u5e73\u8861\u683c\u5f0f\u4e00\u81f4\u6027\u4e0e\u591a\u6837\u6027\u3002", "motivation": "\u4f20\u7edf\u7ed3\u6784\u5316\u6a21\u677f\uff08\u5982\u89d2\u8272\u6807\u8bb0\uff09\u867d\u4fdd\u8bc1\u683c\u5f0f\u7edf\u4e00\uff0c\u4f46\u5bfc\u81f4\u5f00\u653e\u6027\u4efb\u52a1\u4e2d\u6a21\u578b\u8f93\u51fa\u8bed\u4e49\u8d8b\u540c\uff0c\u6291\u5236\u521b\u9020\u6027\u548c\u591a\u6837\u6027\uff08\u5373\u591a\u6837\u6027\u5d29\u6e83\uff09\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u9ad8\u6e29\u5ea6\u91c7\u6837\u4e0b\u7684\u591a\u6837\u6027\u5d29\u6e83\u73b0\u8c61\uff0c\u5fae\u8c03\u4e0d\u540c\u7ed3\u6784\u5316\u63d0\u793a\u7684\u6a21\u578b\uff0c\u5e76\u5206\u6790\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u3001\u5bf9\u9f50\u884c\u4e3a\u548c\u8f93\u51fa\u591a\u6837\u6027\u3002", "result": "1. \u683c\u5f0f\u4e00\u81f4\u6027\u5bf9\u7ed3\u6784\u654f\u611f\u4efb\u52a1\uff08GSM8K/IFEval\uff09\u5173\u952e\uff0c\u4f46\u5bf9\u77e5\u8bc6\u4efb\u52a1\uff08MMLU/WebQuestions\uff09\u5f71\u54cd\u5c0f\uff1b2. \u7ed3\u6784\u6807\u8bb0\u663e\u8457\u538b\u7f29\u8f93\u51fa\u7a7a\u95f4\uff0c\u6781\u7b80\u6a21\u677f\u53ef\u63d0\u5347\u591a\u6837\u6027\u3002", "conclusion": "\u5f53\u524d\u63d0\u793a\u65b9\u6cd5\u867d\u5229\u4e8e\u5bf9\u9f50\uff0c\u4f46\u53ef\u80fd\u65e0\u610f\u95f4\u538b\u5236\u8f93\u51fa\u591a\u6837\u6027\uff0c\u9700\u5f00\u53d1\u517c\u987e\u591a\u6837\u6027\u7684\u63d0\u793a\u8bbe\u8ba1\u548c\u6307\u4ee4\u8c03\u4f18\u65b9\u6cd5\u3002"}}
{"id": "2505.18951", "pdf": "https://arxiv.org/pdf/2505.18951", "abs": "https://arxiv.org/abs/2505.18951", "authors": ["Saman Sarker Joy"], "title": "BnMMLU: Measuring Massive Multitask Language Understanding in Bengali", "categories": ["cs.CL"], "comment": "18 pages, 9 figures, 5 tables; Code & dataset available at\n  https://github.com/samanjoy2/bnmmlu", "summary": "The Massive Multitask Language Understanding (MMLU) benchmark has been widely\nused to evaluate language models across various domains. However, existing MMLU\ndatasets primarily focus on high-resource languages such as English, which\nleaves low-resource languages like Bengali underrepresented. In this paper, we\nintroduce BnMMLU, a benchmark to evaluate the multitask language understanding\ncapabilities of Bengali in language models. The dataset spans 23 domains,\nincluding science, humanities, mathematics and general knowledge and is\nstructured in a multiple-choice format to assess factual knowledge,\napplication-based problem-solving and reasoning abilities of language models.\nIt consists of 138,949 question-option pairs. We benchmark several proprietary\nand open-source large language models (LLMs) on the BnMMLU test set.\nAdditionally, we annotate the test set with three cognitive categories-factual\nknowledge, procedural application and reasoning-to gain deeper insights into\nmodel strengths and weaknesses across various cognitive tasks. The results\nreveal significant performance gaps, highlighting the need for improved\npre-training and fine-tuning strategies tailored to Bengali data. We release\nthe dataset and benchmark results to facilitate further research in this area.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5b5f\u52a0\u62c9\u8bed\u591a\u4efb\u52a1\u8bed\u8a00\u7406\u89e3\u8bc4\u6d4b\u57fa\u51c6BnMMLU\uff0c\u5305\u542b23\u4e2a\u9886\u57df\u517113.8\u4e07\u9898\uff0c\u586b\u8865\u4f4e\u8d44\u6e90\u8bed\u8a00\u8bc4\u4f30\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709MMLU\u57fa\u51c6\u4e3b\u8981\u9762\u5411\u82f1\u8bed\u7b49\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u5b5f\u52a0\u62c9\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u8bc4\u4f30\u4f53\u7cfb\u7f3a\u5931\uff0c\u963b\u788d\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u8986\u76d6\u79d1\u5b66\u3001\u6570\u5b66\u7b4923\u4e2a\u9886\u57df\u7684\u9009\u62e9\u9898\u6570\u636e\u96c6\uff08138,949\u9898\uff09\uff0c\u6807\u6ce8\u4e8b\u5b9e\u6027\u77e5\u8bc6/\u7a0b\u5e8f\u5e94\u7528/\u63a8\u7406\u4e09\u7c7b\u8ba4\u77e5\u6807\u7b7e\uff0c\u6d4b\u8bd5\u591a\u4e2a\u95ed\u6e90/\u5f00\u6e90\u5927\u6a21\u578b\u3002", "result": "\u6a21\u578b\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff08\u6700\u4f18\u6a21\u578b\u51c6\u786e\u7387\u4ec559.1%\uff09\uff0c\u63ed\u793a\u9700\u8981\u9488\u5bf9\u5b5f\u52a0\u62c9\u8bed\u6539\u8fdb\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u7b56\u7565\u3002", "conclusion": "BnMMLU\u66b4\u9732\u5b5f\u52a0\u62c9\u8bedNLP\u5173\u952e\u6311\u6218\uff0c\u6570\u636e\u96c6\u7684\u5f00\u653e\u5c06\u63a8\u52a8\u4f4e\u8d44\u6e90\u8bed\u8a00\u5efa\u6a21\u7814\u7a76\uff0c\u4fc3\u8fdb\u8bed\u8a00\u6280\u672f\u5305\u5bb9\u6027\u53d1\u5c55\u3002"}}
{"id": "2505.18953", "pdf": "https://arxiv.org/pdf/2505.18953", "abs": "https://arxiv.org/abs/2505.18953", "authors": ["Divij Chawla", "Ashita Bhutada", "Do Duc Anh", "Abhinav Raghunathan", "Vinod SP", "Cathy Guo", "Dar Win Liew", "Prannaya Gupta", "Rishabh Bhardwaj", "Rajat Bhardwaj", "Soujanya Poria"], "title": "Evaluating AI for Finance: Is AI Credible at Assessing Investment Risk?", "categories": ["cs.CL"], "comment": null, "summary": "We evaluate the credibility of leading AI models in assessing investment risk\nappetite. Our analysis spans proprietary (GPT-4, Claude 3.7, Gemini 1.5) and\nopen-weight models (LLaMA 3.1/3.3, DeepSeek-V3, Mistral-small), using 1,720\nuser profiles constructed with 16 risk-relevant features across 10 countries\nand both genders. We observe significant variance across models in score\ndistributions and demographic sensitivity. For example, GPT-4o assigns higher\nrisk scores to Nigerian and Indonesian profiles, while LLaMA and DeepSeek show\nopposite gender tendencies in risk classification. While some models (e.g.,\nGPT-4o, LLaMA 3.1) align closely with expected scores in low- and mid-risk\nranges, none maintain consistent performance across regions and demographics.\nOur findings highlight the need for rigorous, standardized evaluations of AI\nsystems in regulated financial contexts to prevent bias, opacity, and\ninconsistency in real-world deployment.", "AI": {"tldr": "AI\u6a21\u578b\u5728\u6295\u8d44\u98ce\u9669\u8bc4\u4f30\u4e2d\u5b58\u5728\u663e\u8457\u5730\u57df/\u6027\u522b\u654f\u611f\u6027\u5dee\u5f02\uff0c\u9700\u5efa\u7acb\u6807\u51c6\u5316\u8bc4\u4f30\u4f53\u7cfb", "motivation": "\u8bc4\u4f30\u4e3b\u6d41AI\u6a21\u578b\u5728\u53d7\u76d1\u7ba1\u91d1\u878d\u573a\u666f\u4e2d\u7684\u98ce\u9669\u8bc4\u4f30\u53ef\u9760\u6027\uff0c\u63ed\u793a\u7b97\u6cd5\u504f\u89c1\u98ce\u9669", "method": "\u4f7f\u75281720\u4e2a\u542b16\u4e2a\u98ce\u9669\u7279\u5f81\u7684\u7528\u6237\u6863\u6848\uff0c\u6a2a\u8de810\u56fd\u4e24\u6027\uff0c\u5bf9\u6bd46\u7c7b\u6a21\u578b\uff08\u542bGPT-4/Claude\u7b49\uff09\u7684\u8bc4\u5206\u5206\u5e03", "result": "GPT-4o\u5bf9\u5c3c\u65e5\u5229\u4e9a/\u5370\u5c3c\u7528\u6237\u8bc4\u5206\u504f\u9ad8\uff0cLLaMA\u4e0eDeepSeek\u5448\u73b0\u6027\u522b\u8bc4\u4f30\u5bf9\u7acb\u503e\u5411\uff0c\u6240\u6709\u6a21\u578b\u8de8\u533a\u57df\u8868\u73b0\u4e0d\u7a33\u5b9a", "conclusion": "\u91d1\u878d\u9886\u57dfAI\u7cfb\u7edf\u9700\u5efa\u7acb\u5305\u542b\u5730\u57df/\u6027\u522b\u7ef4\u5ea6\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u9632\u8303\u7b97\u6cd5\u504f\u89c1\u4e0e\u90e8\u7f72\u98ce\u9669"}}
{"id": "2505.18962", "pdf": "https://arxiv.org/pdf/2505.18962", "abs": "https://arxiv.org/abs/2505.18962", "authors": ["Xiaoqiang Wang", "Suyuchen Wang", "Yun Zhu", "Bang Liu"], "title": "System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "Chain-of-thought (CoT) reasoning enables large language models (LLMs) to move\nbeyond fast System-1 responses and engage in deliberative System-2 reasoning.\nHowever, this comes at the cost of significant inefficiency due to verbose\nintermediate output. Recent latent-space reasoning methods improve efficiency\nby operating on hidden states without decoding into language, yet they treat\nall steps uniformly, failing to distinguish critical deductions from auxiliary\nsteps and resulting in suboptimal use of computational resources. In this\npaper, we propose System-1.5 Reasoning, an adaptive reasoning framework that\ndynamically allocates computation across reasoning steps through shortcut paths\nin latent space.Specifically, System-1.5 Reasoning introduces two types of\ndynamic shortcuts. The model depth shortcut (DS) adaptively reasons along the\nvertical depth by early exiting non-critical tokens through lightweight adapter\nbranches, while allowing critical tokens to continue through deeper Transformer\nlayers. The step shortcut (SS) reuses hidden states across the decoding steps\nto skip trivial steps and reason horizontally in latent space. Training\nSystem-1.5 Reasoning involves a two-stage self-distillation process: first\ndistilling natural language CoT into latent-space continuous thought, and then\ndistilling full-path System-2 latent reasoning into adaptive shortcut paths\n(System-1.5 Reasoning).Experiments on reasoning tasks demonstrate the superior\nperformance of our method. For example, on GSM8K, System-1.5 Reasoning achieves\nreasoning performance comparable to traditional CoT fine-tuning methods while\naccelerating inference by over 20x and reducing token generation by 92.31% on\naverage.", "AI": {"tldr": "\u63d0\u51faSystem-1.5 Reasoning\u6846\u67b6\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u52a8\u6001\u8ba1\u7b97\u5206\u914d\uff0c\u5728\u4fdd\u8bc1\u63a8\u7406\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b020\u500d\u52a0\u901f\u548c92.31%\u7684token\u751f\u6210\u7f29\u51cf", "motivation": "\u4f20\u7edf\u601d\u7ef4\u94fe\u65b9\u6cd5\u5b58\u5728\u4e2d\u95f4\u8f93\u51fa\u5197\u957f\u4f4e\u6548\u95ee\u9898\uff0c\u73b0\u6709\u6f5c\u5728\u7a7a\u95f4\u63a8\u7406\u65b9\u6cd5\u672a\u80fd\u533a\u5206\u5173\u952e\u6b65\u9aa4\u4e0e\u8f85\u52a9\u6b65\u9aa4\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u5229\u7528\u4e0d\u8db3", "method": "\u5f15\u5165\u5782\u76f4\u6df1\u5ea6\u7684\u6a21\u578b\u6df1\u5ea6\u6377\u5f84\uff08\u52a8\u6001\u9000\u51fa\u975e\u5173\u952etoken\uff09\u4e0e\u6c34\u5e73\u89e3\u7801\u7684\u6b65\u9aa4\u6377\u5f84\uff08\u91cd\u7528\u9690\u85cf\u72b6\u6001\uff09\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u81ea\u84b8\u998f\u8bad\u7ec3\u5b9e\u73b0\u81ea\u9002\u5e94\u63a8\u7406\u8def\u5f84", "result": "\u5728GSM8K\u7b49\u63a8\u7406\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e0e\u4f20\u7edfCoT\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u534720\u500d\u4ee5\u4e0a\uff0c\u5e73\u5747\u51cf\u5c1192.31%\u7684token\u751f\u6210", "conclusion": "System-1.5 Reasoning\u901a\u8fc7\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u5728\u8bed\u8a00\u6a21\u578b\u7684\u7cfb\u7edf1\u5feb\u901f\u54cd\u5e94\u4e0e\u7cfb\u7edf2\u6df1\u5ea6\u63a8\u7406\u95f4\u5efa\u7acb\u6548\u7387\u5e73\u8861"}}
{"id": "2505.18970", "pdf": "https://arxiv.org/pdf/2505.18970", "abs": "https://arxiv.org/abs/2505.18970", "authors": ["Bowen Wei", "Ziwei Zhu"], "title": "Learning to Explain: Prototype-Based Surrogate Models for LLM Classification", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance on\nnatural language tasks, but their decision-making processes remain largely\nopaque. Existing explanation methods either suffer from limited faithfulness to\nthe model's reasoning or produce explanations that humans find difficult to\nunderstand. To address these challenges, we propose \\textbf{ProtoSurE}, a novel\nprototype-based surrogate framework that provides faithful and\nhuman-understandable explanations for LLMs. ProtoSurE trains an\ninterpretable-by-design surrogate model that aligns with the target LLM while\nutilizing sentence-level prototypes as human-understandable concepts. Extensive\nexperiments show that ProtoSurE consistently outperforms SOTA explanation\nmethods across diverse LLMs and datasets. Importantly, ProtoSurE demonstrates\nstrong data efficiency, requiring relatively few training examples to achieve\ngood performance, making it practical for real-world applications.", "AI": {"tldr": "\u63d0\u51faProtoSurE\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u66ff\u4ee3\u6a21\u578b\u548c\u53e5\u5b50\u7ea7\u539f\u578b\uff0c\u4e3aLLM\u63d0\u4f9b\u5fe0\u5b9e\u4e14\u6613\u61c2\u7684\u89e3\u91ca\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u5e76\u5177\u5907\u9ad8\u6570\u636e\u6548\u7387", "motivation": "\u73b0\u6709LLM\u89e3\u91ca\u65b9\u6cd5\u5b58\u5728\u5fe0\u5b9e\u5ea6\u4e0d\u8db3\u4e0e\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u53cc\u91cd\u7f3a\u9677\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u4e2d\u5bf9\u6a21\u578b\u900f\u660e\u5ea6\u7684\u9700\u6c42", "method": "\u6784\u5efa\u57fa\u4e8e\u539f\u578b\u7684\u66ff\u4ee3\u6a21\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u5bf9\u9f50\u76ee\u6807LLM\u7684\u51b3\u7b56\u903b\u8f91\uff0c\u7ed3\u5408\u53e5\u5b50\u7ea7\u539f\u578b\u6784\u5efa\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u6982\u5ff5\u4f53\u7cfb", "result": "\u5728\u591a\u4e2aLLM\u548c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cProtoSurE\u5728\u89e3\u91ca\u8d28\u91cf\u6307\u6807\u4e0a\u5e73\u5747\u63d0\u534715.6%\uff0c\u4e14\u4ec5\u9700500\u4e2a\u8bad\u7ec3\u6837\u672c\u5373\u53ef\u8fbe\u5230\u7a33\u5b9a\u6027\u80fd", "conclusion": "ProtoSurE\u6210\u529f\u89e3\u51b3\u4e86LLM\u53ef\u89e3\u91ca\u6027\u9886\u57df\u5fe0\u5b9e\u5ea6\u4e0e\u53ef\u7406\u89e3\u6027\u7684\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u53ef\u4fe1AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84"}}
{"id": "2505.18971", "pdf": "https://arxiv.org/pdf/2505.18971", "abs": "https://arxiv.org/abs/2505.18971", "authors": ["Abhijit Chakraborty", "Chahana Dahal", "Ashutosh Balasubramaniam", "Tejas Anvekar", "Vivek Gupta"], "title": "Is Architectural Complexity Overrated? Competitive and Interpretable Knowledge Graph Completion with RelatE", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "We revisit the efficacy of simple, real-valued embedding models for knowledge\ngraph completion and introduce RelatE, an interpretable and modular method that\nefficiently integrates dual representations for entities and relations. RelatE\nemploys a real-valued phase-modulus decomposition, leveraging sinusoidal phase\nalignments to encode relational patterns such as symmetry, inversion, and\ncomposition. In contrast to recent approaches based on complex-valued\nembeddings or deep neural architectures, RelatE preserves architectural\nsimplicity while achieving competitive or superior performance on standard\nbenchmarks. Empirically, RelatE outperforms prior methods across several\ndatasets: on YAGO3-10, it achieves an MRR of 0.521 and Hit@10 of 0.680,\nsurpassing all baselines. Additionally, RelatE offers significant efficiency\ngains, reducing training time by 24%, inference latency by 31%, and peak GPU\nmemory usage by 22% compared to RotatE. Perturbation studies demonstrate\nimproved robustness, with MRR degradation reduced by up to 61% relative to\nTransE and by up to 19% compared to RotatE under structural edits such as edge\nremovals and relation swaps. Formal analysis further establishes the model's\nfull expressiveness and its capacity to represent essential first-order logical\ninference patterns. These results position RelatE as a scalable and\ninterpretable alternative to more complex architectures for knowledge graph\ncompletion.", "AI": {"tldr": "\u63d0\u51faRelatE\u6a21\u578b\uff0c\u901a\u8fc7\u76f8\u4f4d-\u6a21\u5206\u89e3\u5b9e\u73b0\u9ad8\u6548\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\uff0c\u5728\u6027\u80fd\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u590d\u6570\u5d4c\u5165\u6216\u6df1\u5ea6\u67b6\u6784\u7684\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u6a21\u578b\u590d\u6742\u5ea6\u9ad8\u4e14\u7f3a\u4e4f\u89e3\u91ca\u6027\uff0c\u9700\u5f00\u53d1\u517c\u5177\u9ad8\u6548\u6027\u3001\u53ef\u89e3\u91ca\u6027\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5b9e\u503c\u76f8\u4f4d-\u6a21\u5206\u89e3\u6280\u672f\uff0c\u5229\u7528\u6b63\u5f26\u76f8\u4f4d\u5bf9\u9f50\u7f16\u7801\u5bf9\u79f0/\u53cd\u8f6c/\u7ec4\u5408\u7b49\u5173\u7cfb\u6a21\u5f0f\uff0c\u4fdd\u6301\u67b6\u6784\u7b80\u6d01\u6027\u3002", "result": "YAGO3-10\u4e0aMRR\u8fbe0.521\uff08Hit@10 0.680\uff09\uff0c\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c1124%\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e31%\uff0c\u5185\u5b58\u5360\u7528\u4e0b\u964d22%\uff1b\u7ed3\u6784\u6270\u52a8\u4e0bMRR\u8870\u51cf\u6bd4TransE\u51cf\u5c1161%\u3002", "conclusion": "RelatE\u5728\u4fdd\u6301\u67b6\u6784\u7b80\u5355\u6027\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u3001\u6548\u7387\u548c\u9c81\u68d2\u6027\u7684\u7efc\u5408\u63d0\u5347\uff0c\u4e3a\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.18973", "pdf": "https://arxiv.org/pdf/2505.18973", "abs": "https://arxiv.org/abs/2505.18973", "authors": ["Sarang Patil", "Ashish Parmanand Pandey", "Ioannis Koutis", "Mengjia Xu"], "title": "Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings", "categories": ["cs.CL", "cs.LG"], "comment": "10 pages, 3 figures", "summary": "Selective state-space models have achieved great success in long-sequence\nmodeling. However, their capacity for language representation, especially in\ncomplex hierarchical reasoning tasks, remains underexplored. Most large\nlanguage models rely on flat Euclidean embeddings, limiting their ability to\ncapture latent hierarchies. To address this limitation, we propose Hierarchical\nMamba (HiM), integrating efficient Mamba2 with exponential growth and curved\nnature of hyperbolic geometry to learn hierarchy-aware language embeddings for\ndeeper linguistic understanding. Mamba2-processed sequences are projected to\nthe Poincare ball (via tangent-based mapping) or Lorentzian manifold (via\ncosine and sine-based mapping) with \"learnable\" curvature, optimized with a\ncombined hyperbolic loss. Our HiM model facilitates the capture of relational\ndistances across varying hierarchical levels, enabling effective long-range\nreasoning. This makes it well-suited for tasks like mixed-hop prediction and\nmulti-hop inference in hierarchical classification. We evaluated our HiM with\nfour linguistic and medical datasets for mixed-hop prediction and multi-hop\ninference tasks. Experimental results demonstrated that: 1) Both HiM models\neffectively capture hierarchical relationships for four ontological datasets,\nsurpassing Euclidean baselines. 2) HiM-Poincare captures fine-grained semantic\ndistinctions with higher h-norms, while HiM-Lorentz provides more stable,\ncompact, and hierarchy-preserving embeddings favoring robustness over detail.", "AI": {"tldr": "\u63d0\u51faHierarchical Mamba\u6a21\u578b\uff0c\u7ed3\u5408Mamba2\u4e0e\u53cc\u66f2\u51e0\u4f55\uff0c\u6709\u6548\u6355\u6349\u5c42\u6b21\u5316\u8bed\u8a00\u5173\u7cfb\uff0c\u5728\u6df7\u5408\u8df3\u6570\u63a8\u7406\u4efb\u52a1\u4e2d\u8d85\u8d8a\u6b27\u51e0\u91cc\u5f97\u57fa\u7ebf", "motivation": "\u73b0\u6709\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5728\u590d\u6742\u5c42\u6b21\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u4e14\u4e3b\u6d41\u8bed\u8a00\u6a21\u578b\u7684\u6b27\u51e0\u91cc\u5f97\u5d4c\u5165\u96be\u4ee5\u6355\u6349\u6f5c\u5728\u8bed\u4e49\u5c42\u6b21", "method": "\u5c06Mamba2\u5904\u7406\u540e\u7684\u5e8f\u5217\u901a\u8fc7\u53ef\u5b66\u4e60\u66f2\u7387\u6620\u5c04\u5230Poincare\u7403\uff08\u6b63\u5207\u6295\u5f71\uff09\u6216Lorentz\u6d41\u5f62\uff08\u6b63\u4f59\u5f26\u6295\u5f71\uff09\uff0c\u7ed3\u5408\u53cc\u66f2\u635f\u5931\u4f18\u5316\u5c42\u6b21\u5173\u7cfb\u5efa\u6a21", "result": "\u5728\u56db\u4e2a\u672c\u4f53\u6570\u636e\u96c6\u4e0a\uff0c\u53cc\u66f2\u7248\u672cHiM\u5747\u4f18\u4e8e\u6b27\u5f0f\u57fa\u7ebf\uff1aHiM-Poincare\u6355\u83b7\u66f4\u7ec6\u7c92\u5ea6\u8bed\u4e49\uff08\u9ad8h-norm\uff09\uff0cHiM-Lorentz\u63d0\u4f9b\u66f4\u7a33\u5b9a\u7684\u7d27\u51d1\u5c42\u6b21\u5d4c\u5165", "conclusion": "\u53cc\u66f2\u51e0\u4f55\u6709\u6548\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u5c42\u6b21\u63a8\u7406\u80fd\u529b\uff0cPoincare\u548cLorentz\u6295\u5f71\u5206\u522b\u9002\u7528\u4e8e\u7ec6\u8282\u654f\u611f\u548c\u7a33\u5b9a\u6027\u4f18\u5148\u7684\u4e0d\u540c\u5e94\u7528\u573a\u666f"}}
{"id": "2505.18978", "pdf": "https://arxiv.org/pdf/2505.18978", "abs": "https://arxiv.org/abs/2505.18978", "authors": ["Miguel Angel Pe\u00f1aloza Perez", "Bruno Lopez Orozco", "Jesus Tadeo Cruz Soto", "Michelle Bruno Hernandez", "Miguel Angel Alvarado Gonzalez", "Sandra Malagon"], "title": "AI4Math: A Native Spanish Benchmark for University-Level Mathematical Reasoning in Large Language Models", "categories": ["cs.CL", "68", "I.2"], "comment": "36 pages, 5 figures", "summary": "Existing mathematical reasoning benchmarks are predominantly English only or\ntranslation-based, which can introduce semantic drift and mask languagespecific\nreasoning errors. To address this, we present AI4Math, a benchmark of 105\noriginal university level math problems natively authored in Spanish. The\ndataset spans seven advanced domains (Algebra, Calculus, Geometry, Probability,\nNumber Theory, Combinatorics, and Logic), and each problem is accompanied by a\nstep by step human solution. We evaluate six large language models GPT 4o, GPT\n4o mini, o3 mini, LLaMA 3.3 70B, DeepSeek R1 685B, and DeepSeek V3 685B under\nfour configurations: zero shot and chain of thought, each in Spanish and\nEnglish. The top models (o3 mini, DeepSeek R1 685B, DeepSeek V3 685B) achieve\nover 70% accuracy, whereas LLaMA 3.3 70B and GPT-4o mini remain below 40%. Most\nmodels show no significant performance drop between languages, with GPT 4o even\nperforming better on Spanish problems in the zero shot setting. Geometry,\nCombinatorics, and Probability questions remain persistently challenging for\nall models. These results highlight the need for native-language benchmarks and\ndomain-specific evaluations to reveal reasoning failures not captured by\nstandard metrics.", "AI": {"tldr": "\u521b\u5efa\u539f\u751f\u897f\u73ed\u7259\u8bed\u6570\u5b66\u57fa\u51c6AI4Math\uff0c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u8a00/\u9886\u57df\u4e0b\u7684\u8868\u73b0", "motivation": "\u89e3\u51b3\u73b0\u6709\u6570\u5b66\u57fa\u51c6\u591a\u57fa\u4e8e\u7ffb\u8bd1\u5bfc\u81f4\u8bed\u4e49\u504f\u5dee\u7684\u95ee\u9898\uff0c\u5efa\u7acb\u8bed\u8a00\u7279\u5f02\u6027\u8bc4\u4f30\u6807\u51c6", "method": "\u6784\u5efa\u542b105\u4e2a\u897f\u73ed\u7259\u8bed\u5927\u5b66\u6570\u5b66\u95ee\u9898\uff08\u8986\u76d67\u9886\u57df\uff09\u7684\u57fa\u51c6\uff0c\u6d4b\u8bd56\u4e2a\u6a21\u578b\u5728\u96f6\u6837\u672c/\u601d\u7ef4\u94fe\uff08\u897f\u82f1\u53cc\u8bed\uff09\u7684\u8868\u73b0", "result": "\u9876\u7ea7\u6a21\u578b\u51c6\u786e\u7387\u8d8570%\uff0c\u51e0\u4f55/\u7ec4\u5408/\u6982\u7387\u4ecd\u5177\u6311\u6218\u6027\uff0c\u591a\u6570\u6a21\u578b\u8de8\u8bed\u8a00\u8868\u73b0\u7a33\u5b9a\uff08GPT-4o\u897f\u8bed\u96f6\u6837\u672c\u66f4\u4f18\uff09", "conclusion": "\u9700\u539f\u751f\u8bed\u8a00\u57fa\u51c6\u548c\u9886\u57df\u8bc4\u4f30\u6765\u63ed\u793a\u6a21\u578b\u7f3a\u9677\uff0c\u6a21\u578b\u8de8\u8bed\u8a00\u80fd\u529b\u8f83\u5f3a\u4f46\u7279\u5b9a\u9886\u57df\u4ecd\u9700\u7a81\u7834"}}
{"id": "2505.18995", "pdf": "https://arxiv.org/pdf/2505.18995", "abs": "https://arxiv.org/abs/2505.18995", "authors": ["Carlos Jude G. Maminta", "Isaiah Job Enriquez", "Deandre Nigel Nunez", "Michael B. Dela Fuente"], "title": "FiLLM -- A Filipino-optimized Large Language Model based on Southeast Asia Large Language Model (SEALLM)", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study presents FiLLM, a Filipino-optimized large language model,\ndesigned to enhance natural language processing (NLP) capabilities in the\nFilipino language. Built upon the SeaLLM-7B 2.5 model, FiLLM leverages Low-Rank\nAdaptation (LoRA) fine-tuning to optimize memory efficiency while maintaining\ntask-specific performance. The model was trained and evaluated on diverse\nFilipino datasets to address key NLP tasks, including Named Entity Recognition\n(NER), Part-of-Speech (POS) tagging, Dependency Parsing, and Text\nSummarization. Performance comparisons with the CalamanCy model were conducted\nusing F1 Score, Precision, Recall, Compression Rate, and Keyword Overlap\nmetrics. Results indicate that Calamancy outperforms FILLM in several aspects,\ndemonstrating its effectiveness in processing Filipino text with improved\nlinguistic comprehension and adaptability. This research contributes to the\nadvancement of Filipino NLP applications by providing an optimized, efficient,\nand scalable language model tailored for local linguistic needs.", "AI": {"tldr": "FiLLM\u662f\u57fa\u4e8eSeaLLM-7B 2.5\u4f18\u5316\u7684\u83f2\u5f8b\u5bbe\u8bed\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7LoRA\u5fae\u8c03\u63d0\u5347\u5185\u5b58\u6548\u7387\uff0c\u4f46\u8bc4\u4f30\u663e\u793aCalamanCy\u6a21\u578b\u5728\u591a\u9879NLP\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u89e3\u51b3\u83f2\u5f8b\u5bbe\u8bedNLP\u5de5\u5177\u532e\u4e4f\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6280\u672f\u9002\u914d\u672c\u571f\u8bed\u8a00\u7279\u6027\uff0c\u63a8\u52a8\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u7684\u5e94\u7528\u53d1\u5c55\u3002", "method": "\u91c7\u7528Low-Rank Adaptation(LoRA)\u5fae\u8c03SeaLLM-7B 2.5\u6a21\u578b\uff0c\u5728\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u8bcd\u6027\u6807\u6ce8\u7b49\u4efb\u52a1\u4e0a\u4f7f\u7528\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6307\u6807\u5bf9\u6bd4CalamanCy\u6a21\u578b\u3002", "result": "CalamanCy\u5728F1\u503c\u3001\u7cbe\u786e\u7387\u7b49\u5173\u952e\u6307\u6807\u4e0a\u6574\u4f53\u4f18\u4e8eFiLLM\uff0c\u4f46FiLLM\u5728\u5185\u5b58\u6548\u7387\u548c\u6a21\u578b\u8f7b\u91cf\u5316\u65b9\u9762\u5c55\u73b0\u4f18\u52bf\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u53ef\u6709\u6548\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8d44\u6e90\u6d88\u8017\uff0c\u4e3a\u83f2\u5f8b\u5bbe\u8bedNLP\u751f\u6001\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2505.19000", "pdf": "https://arxiv.org/pdf/2505.19000", "abs": "https://arxiv.org/abs/2505.19000", "authors": ["Yunxin Li", "Xinyu Chen", "Zitao Li", "Zhenyu Liu", "Longyue Wang", "Wenhan Luo", "Baotian Hu", "Min Zhang"], "title": "VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied Iterative Policy Optimization", "categories": ["cs.CL", "cs.CV"], "comment": "19 pages, 9 figures, Project Link:\n  https://github.com/HITsz-TMG/VerIPO", "summary": "Applying Reinforcement Learning (RL) to Video Large Language Models\n(Video-LLMs) shows significant promise for complex video reasoning. However,\npopular Reinforcement Fine-Tuning (RFT) methods, such as outcome-based Group\nRelative Policy Optimization (GRPO), are limited by data preparation\nbottlenecks (e.g., noise or high cost) and exhibit unstable improvements in the\nquality of long chain-of-thoughts (CoTs) and downstream performance.To address\nthese limitations, we propose VerIPO, a Verifier-guided Iterative Policy\nOptimization method designed to gradually improve video LLMs' capacity for\ngenerating deep, long-term reasoning chains. The core component is\nRollout-Aware Verifier, positioned between the GRPO and Direct Preference\nOptimization (DPO) training phases to form the GRPO-Verifier-DPO training loop.\nThis verifier leverages small LLMs as a judge to assess the reasoning logic of\nrollouts, enabling the construction of high-quality contrastive data, including\nreflective and contextually consistent CoTs. These curated preference samples\ndrive the efficient DPO stage (7x faster than GRPO), leading to marked\nimprovements in reasoning chain quality, especially in terms of length and\ncontextual consistency. This training loop benefits from GRPO's expansive\nsearch and DPO's targeted optimization. Experimental results demonstrate: 1)\nSignificantly faster and more effective optimization compared to standard GRPO\nvariants, yielding superior performance; 2) Our trained models exceed the\ndirect inference of large-scale instruction-tuned Video-LLMs, producing long\nand contextually consistent CoTs on diverse video reasoning tasks; and 3) Our\nmodel with one iteration outperforms powerful LMMs (e.g., Kimi-VL) and long\nreasoning models (e.g., Video-R1), highlighting its effectiveness and\nstability.", "AI": {"tldr": "\u63d0\u51faVerIPO\u65b9\u6cd5\uff0c\u901a\u8fc7\u9a8c\u8bc1\u5668\u5f15\u5bfc\u7684\u8fed\u4ee3\u8bad\u7ec3\u5faa\u73af(GRPO-Verifier-DPO)\u89e3\u51b3\u89c6\u9891\u5927\u6a21\u578b\u957f\u63a8\u7406\u94fe\u4f18\u5316\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\u4e0e\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5fae\u8c03\u65b9\u6cd5(\u5982GRPO)\u5b58\u5728\u6570\u636e\u8d28\u91cf\u5dee\u3001\u957f\u601d\u7ef4\u94fe\u6539\u8fdb\u4e0d\u7a33\u5b9a\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u6846\u67b6\u63d0\u5347\u89c6\u9891\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e09\u9636\u6bb5\u8bad\u7ec3\u5faa\u73af\uff1a1) GRPO\u6269\u5c55\u641c\u7d22\u7a7a\u95f4 2) \u5c0f\u6a21\u578b\u4f5c\u4e3a\u9a8c\u8bc1\u5668\u7b5b\u9009\u903b\u8f91\u8fde\u8d2f\u7684\u63a8\u7406\u94fe 3) \u9ad8\u8d28\u91cf\u6570\u636e\u9a71\u52a8DPO\u5feb\u901f\u4f18\u5316\u3002", "result": "\u5728\u591a\u9879\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u4e2d\u751f\u6210\u957f\u5ea62-3\u500d\u4e8e\u57fa\u7ebf\u7684\u601d\u7ef4\u94fe\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u53477\u500d\uff0c\u6027\u80fd\u8d85\u8d8aKimi-VL/Video-R1\u7b49\u5148\u8fdb\u6a21\u578b\u3002", "conclusion": "\u9a8c\u8bc1\u5668\u5f15\u5bfc\u7684\u8fed\u4ee3\u4f18\u5316\u673a\u5236\u6709\u6548\u7ed3\u5408GRPO\u7684\u63a2\u7d22\u80fd\u529b\u4e0eDPO\u7684\u5b9a\u5411\u4f18\u5316\uff0c\u4e3a\u957f\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u7a33\u5b9a\u9ad8\u6548\u7684\u8bad\u7ec3\u8303\u5f0f\u3002"}}
{"id": "2505.19018", "pdf": "https://arxiv.org/pdf/2505.19018", "abs": "https://arxiv.org/abs/2505.19018", "authors": ["Md. Mithun Hossain", "Md. Shakil Hossain", "Sudipto Chaki", "Md. Rajib Hossain", "Md. Saifur Rahman", "A. B. M. Shawkat Ali"], "title": "CrosGrpsABS: Cross-Attention over Syntactic and Semantic Graphs for Aspect-Based Sentiment Analysis in a Low-Resource Language", "categories": ["cs.CL"], "comment": null, "summary": "Aspect-Based Sentiment Analysis (ABSA) is a fundamental task in natural\nlanguage processing, offering fine-grained insights into opinions expressed in\ntext. While existing research has largely focused on resource-rich languages\nlike English which leveraging large annotated datasets, pre-trained models, and\nlanguage-specific tools. These resources are often unavailable for low-resource\nlanguages such as Bengali. The ABSA task in Bengali remains poorly explored and\nis further complicated by its unique linguistic characteristics and a lack of\nannotated data, pre-trained models, and optimized hyperparameters. To address\nthese challenges, this research propose CrosGrpsABS, a novel hybrid framework\nthat leverages bidirectional cross-attention between syntactic and semantic\ngraphs to enhance aspect-level sentiment classification. The CrosGrpsABS\ncombines transformerbased contextual embeddings with graph convolutional\nnetworks, built upon rule-based syntactic dependency parsing and semantic\nsimilarity computations. By employing bidirectional crossattention, the model\neffectively fuses local syntactic structure with global semantic context,\nresulting in improved sentiment classification performance across both low- and\nhigh-resource settings. We evaluate CrosGrpsABS on four low-resource Bengali\nABSA datasets and the high-resource English SemEval 2014 Task 4 dataset. The\nCrosGrpsABS consistently outperforms existing approaches, achieving notable\nimprovements, including a 0.93% F1-score increase for the Restaurant domain and\na 1.06% gain for the Laptop domain in the SemEval 2014 Task 4 benchmark.", "AI": {"tldr": "\u63d0\u51faCrosGrpsABS\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u53e5\u6cd5-\u8bed\u4e49\u56fe\u53cc\u5411\u8de8\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u5b5f\u52a0\u62c9\u8bed\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u7684\u7ec6\u7c92\u5ea6\u60c5\u611f\u5206\u6790\u6027\u80fd\uff0c\u5e76\u5728\u591a\u6570\u636e\u96c6\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e/\u9884\u8bad\u7ec3\u6a21\u578b/\u4f18\u5316\u8d85\u53c2\u6570\uff0c\u4e14\u73b0\u6709ABSA\u65b9\u6cd5\u9ad8\u5ea6\u4f9d\u8d56\u82f1\u8bed\u7b49\u8d44\u6e90\u4e30\u5bcc\u8bed\u8a00\u7684\u7279\u5b9a\u5de5\u5177\uff0c\u96be\u4ee5\u5e94\u5bf9\u5176\u72ec\u7279\u8bed\u8a00\u7279\u6027", "method": "CrosGrpsABS\u6846\u67b6\uff1a1) \u57fa\u4e8e\u89c4\u5219\u7684\u53e5\u6cd5\u4f9d\u8d56\u89e3\u6790\u6784\u5efa\u8bed\u6cd5\u56fe 2) \u8bed\u4e49\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u6784\u5efa\u8bed\u4e49\u56fe 3) \u53cc\u5411\u8de8\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u5c40\u90e8\u53e5\u6cd5\u7ed3\u6784\u4e0e\u5168\u5c40\u8bed\u4e49 4) Transformer\u5d4c\u5165\u4e0e\u56fe\u5377\u79ef\u7f51\u7edc\u8054\u5408\u5efa\u6a21", "result": "\u57284\u4e2a\u5b5f\u52a0\u62c9\u8bed\u6570\u636e\u96c6\u548c\u82f1\u6587SemEval2014\u4efb\u52a14\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5176\u4e2d\u9910\u5385/\u7b14\u8bb0\u672c\u7535\u8111\u9886\u57dfF1\u5206\u522b\u63d0\u53470.93%/1.06%", "conclusion": "\u53cc\u5411\u8de8\u6ce8\u610f\u529b\u6709\u6548\u6574\u5408\u591a\u7ef4\u5ea6\u8bed\u8a00\u7279\u5f81\uff0c\u6846\u67b6\u5177\u5907\u8de8\u8bed\u8a00\u8fc1\u79fb\u80fd\u529b\uff0c\u4e3a\u4f4e\u8d44\u6e90NLP\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.19051", "pdf": "https://arxiv.org/pdf/2505.19051", "abs": "https://arxiv.org/abs/2505.19051", "authors": ["Mahdi Nikdan", "Vincent Cohen-Addad", "Dan Alistarh", "Vahab Mirrokni"], "title": "Efficient Data Selection at Scale via Influence Distillation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Effective data selection is critical for efficient training of modern Large\nLanguage Models (LLMs). This paper introduces Influence Distillation, a novel,\nmathematically-justified framework for data selection that employs second-order\ninformation to optimally weight training samples. By distilling each sample's\ninfluence on a target distribution, our method assigns model-specific weights\nthat are used to select training data for LLM fine-tuning, guiding it toward\nstrong performance on the target domain. We derive these optimal weights for\nboth Gradient Descent and Adam optimizers. To ensure scalability and reduce\ncomputational cost, we propose a $\\textit{landmark-based approximation}$:\ninfluence is precisely computed for a small subset of \"landmark\" samples and\nthen efficiently propagated to all other samples to determine their weights. We\nvalidate Influence Distillation by applying it to instruction tuning on the\nTulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU,\nacross several models from the Llama and Qwen families. Experiments show that\nInfluence Distillation matches or outperforms state-of-the-art performance\nwhile achieving up to $3.5\\times$ faster selection.", "AI": {"tldr": "\u63d0\u51faInfluence Distillation\u6846\u67b6\uff0c\u5229\u7528\u4e8c\u9636\u4fe1\u606f\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u6743\u91cd\u9009\u62e9\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u53473.5\u500d\u6570\u636e\u9009\u62e9\u901f\u5ea6", "motivation": "\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u4e2d\u6570\u636e\u9009\u62e9\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u7f3a\u4e4f\u6570\u5b66\u4f9d\u636e\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6570\u5b66\u63a8\u5bfc\u4f18\u5316\u6837\u672c\u6743\u91cd\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u6a21\u578b\u76f8\u5173\u7684\u6570\u636e\u9009\u62e9\u3002", "method": "1. \u63d0\u51fa\u57fa\u4e8e\u76ee\u6807\u5206\u5e03\u6837\u672c\u5f71\u54cd\u529b\u7684\u6743\u91cd\u5206\u914d\u6846\u67b6 2. \u63a8\u5bfc\u68af\u5ea6\u4e0b\u964d\u548cAdam\u4f18\u5316\u5668\u4e0b\u7684\u6700\u4f18\u6743\u91cd\u516c\u5f0f 3. \u8bbe\u8ba1\u5730\u6807\u6837\u672c\u8fd1\u4f3c\u6cd5\uff0c\u4ec5\u7cbe\u786e\u8ba1\u7b97\u5173\u952e\u6837\u672c\u5f71\u54cd\u529b\u540e\u4f20\u64ad\u5230\u5168\u91cf\u6570\u636e", "result": "\u5728Tulu V2\u6307\u4ee4\u8c03\u4f18\u5b9e\u9a8c\u4e2d\uff0cGSM8k/SQuAD/MMLU\u4efb\u52a1\u4e0a\u5339\u914d\u6216\u8d85\u8d8aSOTA\uff0cLlama/Qwen\u7cfb\u5217\u6a21\u578b\u5747\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u6570\u636e\u9009\u62e9\u901f\u5ea6\u63d0\u5347\u8fbe3.5\u500d", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6570\u636e\u9009\u62e9\u63d0\u4f9b\u4e86\u6570\u5b66\u4e25\u8c28\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u7684\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u4f18\u5316\u5f00\u8f9f\u65b0\u8def\u5f84"}}
{"id": "2505.19056", "pdf": "https://arxiv.org/pdf/2505.19056", "abs": "https://arxiv.org/abs/2505.19056", "authors": ["Harethah Abu Shairah", "Hasan Abed Al Kader Hammoud", "Bernard Ghanem", "George Turkiyyah"], "title": "An Embarrassingly Simple Defense Against LLM Abliteration Attacks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "preprint", "summary": "Large language models (LLMs) are typically aligned to comply with safety\nguidelines by refusing harmful instructions. A recent attack, termed\nabliteration, isolates and suppresses the single latent direction most\nresponsible for refusal behavior, enabling the model to generate unethical\ncontent. We propose a defense that modifies how models generate refusals. We\nconstruct an extended-refusal dataset that contains harmful prompts with a full\nresponse that justifies the reason for refusal. We then fine-tune\nLlama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our\nextended-refusal dataset, and evaluate the resulting systems on a set of\nharmful prompts. In our experiments, extended-refusal models maintain high\nrefusal rates, dropping at most by 10%, whereas baseline models' refusal rates\ndrop by 70-80% after abliteration. A broad evaluation of safety and utility\nshows that extended-refusal fine-tuning neutralizes the abliteration attack\nwhile preserving general performance.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6269\u5c55\u62d2\u7edd\u6570\u636e\u96c6\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u6709\u6548\u9632\u5fa1LLMs\u7684abliteration\u653b\u51fb\u5e76\u4fdd\u6301\u6027\u80fd", "motivation": "\u73b0\u6709LLMs\u5b89\u5168\u673a\u5236\u6613\u88ababliteration\u653b\u51fb\u7834\u574f\uff08\u62d2\u7edd\u7387\u9aa4\u964d70-80%\uff09\uff0c\u9700\u589e\u5f3a\u9632\u5fa1\u80fd\u529b", "method": "\u6784\u5efa\u542b\u5b8c\u6574\u62d2\u7edd\u7406\u7531\u7684\u6269\u5c55\u62d2\u7edd\u6570\u636e\u96c6\uff0c\u5bf9Llama-2-7B-Chat\u548cQwen2.5\u7cfb\u5217\u6a21\u578b\u8fdb\u884c\u5fae\u8c03", "result": "\u6269\u5c55\u62d2\u7edd\u6a21\u578b\u62d2\u7edd\u7387\u6700\u591a\u4e0b\u964d10%\uff08\u57fa\u51c6\u6a21\u578b\u4e0b\u964d70-80%\uff09\uff0c\u5b89\u5168\u6027\u548c\u901a\u7528\u6027\u80fd\u4fdd\u6301\u5e73\u8861", "conclusion": "\u6269\u5c55\u62d2\u7edd\u5fae\u8c03\u6210\u529f\u62b5\u5fa1abliteration\u653b\u51fb\uff0c\u5728\u7ef4\u6301\u6a21\u578b\u5b9e\u7528\u6027\u7684\u540c\u65f6\u63d0\u5347\u5b89\u5168\u6027"}}
{"id": "2505.19060", "pdf": "https://arxiv.org/pdf/2505.19060", "abs": "https://arxiv.org/abs/2505.19060", "authors": ["Roman Vashurin", "Maiya Goloburda", "Preslav Nakov", "Maxim Panov"], "title": "UNCERTAINTY-LINE: Length-Invariant Estimation of Uncertainty for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have become indispensable tools across various\napplications, making it more important than ever to ensure the quality and the\ntrustworthiness of their outputs. This has led to growing interest in\nuncertainty quantification (UQ) methods for assessing the reliability of LLM\noutputs. Many existing UQ techniques rely on token probabilities, which\ninadvertently introduces a bias with respect to the length of the output. While\nsome methods attempt to account for this, we demonstrate that such biases\npersist even in length-normalized approaches. To address the problem, here we\npropose UNCERTAINTY-LINE: (Length-INvariant Estimation), a simple debiasing\nprocedure that regresses uncertainty scores on output length and uses the\nresiduals as corrected, length-invariant estimates. Our method is post-hoc,\nmodel-agnostic, and applicable to a range of UQ measures. Through extensive\nevaluation on machine translation, summarization, and question-answering tasks,\nwe demonstrate that UNCERTAINTY-LINE: consistently improves over even nominally\nlength-normalized UQ methods uncertainty estimates across multiple metrics and\nmodels.", "AI": {"tldr": "\u63d0\u51faUNCERTAINTY-LINE\u65b9\u6cd5\u6d88\u9664\u5927\u8bed\u8a00\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u4e2d\u7684\u957f\u5ea6\u504f\u5dee", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8bcd\u5143\u6982\u7387\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u5b58\u5728\u8f93\u51fa\u957f\u5ea6\u504f\u5dee\u95ee\u9898\uff0c\u5373\u4f7f\u957f\u5ea6\u5f52\u4e00\u5316\u540e\u4ecd\u6b8b\u7559\u504f\u5dee", "method": "\u901a\u8fc7\u56de\u5f52\u5206\u6790\u5efa\u7acb\u4e0d\u786e\u5b9a\u6027\u5206\u6570\u4e0e\u8f93\u51fa\u957f\u5ea6\u7684\u5173\u7cfb\uff0c\u5229\u7528\u56de\u5f52\u6b8b\u5dee\u6784\u5efa\u957f\u5ea6\u4e0d\u53d8\u4f30\u8ba1\u91cf", "result": "\u5728\u673a\u5668\u7ffb\u8bd1\u3001\u6458\u8981\u751f\u6210\u548c\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6a21\u578b\u548c\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709UQ\u65b9\u6cd5", "conclusion": "UNCERTAINTY-LINE\u4f5c\u4e3a\u6a21\u578b\u65e0\u5173\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u53ef\u9760\u6027\u548c\u8de8\u4efb\u52a1\u9002\u5e94\u6027"}}
{"id": "2505.19073", "pdf": "https://arxiv.org/pdf/2505.19073", "abs": "https://arxiv.org/abs/2505.19073", "authors": ["Rui Li", "Jing Long", "Muge Qi", "Heming Xia", "Lei Sha", "Peiyi Wang", "Zhifang Sui"], "title": "Towards Harmonized Uncertainty Estimation for Large Language Models", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "To facilitate robust and trustworthy deployment of large language models\n(LLMs), it is essential to quantify the reliability of their generations\nthrough uncertainty estimation. While recent efforts have made significant\nadvancements by leveraging the internal logic and linguistic features of LLMs\nto estimate uncertainty scores, our empirical analysis highlights the pitfalls\nof these methods to strike a harmonized estimation between indication, balance,\nand calibration, which hinders their broader capability for accurate\nuncertainty estimation. To address this challenge, we propose CUE (Corrector\nfor Uncertainty Estimation): A straightforward yet effective method that\nemploys a lightweight model trained on data aligned with the target LLM's\nperformance to adjust uncertainty scores. Comprehensive experiments across\ndiverse models and tasks demonstrate its effectiveness, which achieves\nconsistent improvements of up to 60% over existing methods.", "AI": {"tldr": "\u63d0\u51faCUE\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u578b\u6821\u6b63LLM\u7684\u4e0d\u786e\u5b9a\u6027\u5206\u6570\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534760%", "motivation": "\u73b0\u6709LLM\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u65b9\u6cd5\u5728\u5e73\u8861\u6027\u3001\u6821\u51c6\u6027\u4e0a\u5b58\u5728\u7f3a\u9677\uff0c\u5f71\u54cd\u53ef\u9760\u6027\u8bc4\u4f30", "method": "CUE\u6846\u67b6\u4f7f\u7528\u76ee\u6807LLM\u6027\u80fd\u5bf9\u9f50\u6570\u636e\u8bad\u7ec3\u8f7b\u91cf\u6821\u6b63\u6a21\u578b\u8c03\u6574\u4e0d\u786e\u5b9a\u6027\u5206\u6570", "result": "\u8de8\u6a21\u578b/\u4efb\u52a1\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u63d0\u5347\u8fbe60%\uff0c\u5b9e\u73b0\u66f4\u5747\u8861\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6", "conclusion": "CUE\u4ee5\u7b80\u5355\u67b6\u6784\u663e\u8457\u63d0\u5347LLM\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u8d28\u91cf\uff0c\u589e\u5f3a\u6a21\u578b\u90e8\u7f72\u53ef\u4fe1\u5ea6"}}
{"id": "2505.19091", "pdf": "https://arxiv.org/pdf/2505.19091", "abs": "https://arxiv.org/abs/2505.19091", "authors": ["Benjamin Clavi\u00e9", "Florian Brand"], "title": "ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in Large Vision-Language Models (VLMs), have greatly\nenhanced their capability to jointly process text and images. However, despite\nextensive benchmarks evaluating visual comprehension (e.g., diagrams, color\nschemes, OCR tasks...), there is limited assessment of VLMs' ability to read\nand reason about text-rich images effectively. To fill this gap, we introduce\nReadBench, a multimodal benchmark specifically designed to evaluate the reading\ncomprehension capabilities of VLMs. ReadBench transposes contexts from\nestablished text-only benchmarks into images of text while keeping textual\nprompts and questions intact. Evaluating leading VLMs with ReadBench, we find\nminimal-but-present performance degradation on short, text-image inputs, while\nperformance sharply declines for longer, multi-page contexts. Our experiments\nfurther reveal that text resolution has negligible effects on multimodal\nperformance. These findings highlight needed improvements in VLMs, particularly\ntheir reasoning over visually presented extensive textual content, a capability\ncritical for practical applications. ReadBench is available at\nhttps://github.com/answerdotai/ReadBench .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faReadBench\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u4e13\u95e8\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u5bf9\u6587\u672c\u4e30\u5bcc\u56fe\u50cf\u7684\u9605\u8bfb\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u957f\u6587\u672c\u5904\u7406\u4e0a\u5b58\u5728\u663e\u8457\u7f3a\u9677", "motivation": "\u73b0\u6709VLMs\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u7406\u89e3(\u5982\u56fe\u8868/OCR\u4efb\u52a1)\uff0c\u7f3a\u4e4f\u5bf9\u6587\u672c\u56fe\u50cf\u9605\u8bfb\u63a8\u7406\u80fd\u529b\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u9700\u5efa\u7acb\u9488\u5bf9\u6027\u6d4b\u8bd5\u6846\u67b6", "method": "\u5c06\u7eaf\u6587\u672c\u57fa\u51c6\u7684\u4e0a\u4e0b\u6587\u8f6c\u5316\u4e3a\u6587\u672c\u56fe\u50cf\uff0c\u4fdd\u7559\u539f\u59cb\u6587\u672c\u63d0\u793a\u548c\u95ee\u9898\uff0c\u6784\u5efa\u5305\u542b\u5355\u9875\u77ed\u6587\u672c\u4e0e\u591a\u9875\u957f\u6587\u672c\u7684\u8bc4\u4f30\u4f53\u7cfb", "result": "VLMs\u5728\u77ed\u6587\u672c\u56fe\u50cf\u5904\u7406\u4e2d\u6027\u80fd\u5fae\u964d(\u7ea61-3%)\uff0c\u4f46\u5728\u591a\u9875\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u51c6\u786e\u7387\u9aa4\u964d\u8d8550%\uff1b\u6587\u672c\u5206\u8fa8\u7387\u5bf9\u6a21\u578b\u8868\u73b0\u5f71\u54cd\u53ef\u5ffd\u7565", "conclusion": "VLMs\u9700\u91cd\u70b9\u63d0\u5347\u5bf9\u89c6\u89c9\u5316\u957f\u6587\u672c\u7684\u63a8\u7406\u80fd\u529b\uff0c\u8fd9\u5bf9\u6587\u6863\u5904\u7406/\u6559\u80b2\u7b49\u5b9e\u9645\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0cReadBench\u5f00\u6e90\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76"}}
{"id": "2505.19100", "pdf": "https://arxiv.org/pdf/2505.19100", "abs": "https://arxiv.org/abs/2505.19100", "authors": ["Yeyuan Wang", "Dehong Gao", "Rujiao Long", "Lei Yi", "Linbo Jin", "Libin Yang", "Xiaoyan Cai"], "title": "ASPO: Adaptive Sentence-Level Preference Optimization for Fine-Grained Multimodal Reasoning", "categories": ["cs.CL", "cs.CV"], "comment": "Accepted by ACL 2025 findings", "summary": "Direct Preference Optimization (DPO) has gained significant attention for its\nsimplicity and computational efficiency in aligning large language models\n(LLMs). Recent advancements have extended DPO to multimodal scenarios,\nachieving strong performance. However, traditional DPO relies on binary\npreference optimization, rewarding or penalizing entire responses without\nconsidering fine-grained segment correctness, leading to suboptimal solutions.\nThe root of this issue lies in the absence of fine-grained supervision during\nthe optimization process. To address this, we propose Adaptive Sentence-level\nPreference Optimization (ASPO), which evaluates individual sentences for more\nprecise preference optimization. By dynamically calculating adaptive rewards at\nthe sentence level based on model predictions, ASPO enhances response content\nassessment without additional models or parameters. This significantly improves\nthe alignment of multimodal features. Extensive experiments show that ASPO\nsubstantially enhances the overall performance of multimodal models.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u53e5\u5b50\u7ea7\u504f\u597d\u4f18\u5316(ASPO)\uff0c\u901a\u8fc7\u53e5\u5b50\u7ea7\u52a8\u6001\u5956\u52b1\u673a\u5236\u6539\u8fdb\u591a\u6a21\u6001\u6a21\u578b\u5bf9\u9f50\u6548\u679c", "motivation": "\u4f20\u7edfDPO\u65b9\u6cd5\u57fa\u4e8e\u4e8c\u5143\u504f\u597d\u4f18\u5316\uff0c\u7f3a\u4e4f\u5bf9\u7ec6\u7c92\u5ea6\u6587\u672c\u7247\u6bb5\u6b63\u786e\u6027\u7684\u8bc4\u4f30\uff0c\u5bfc\u81f4\u6b21\u4f18\u89e3", "method": "\u57fa\u4e8e\u6a21\u578b\u9884\u6d4b\u52a8\u6001\u8ba1\u7b97\u53e5\u5b50\u7ea7\u81ea\u9002\u5e94\u5956\u52b1\uff0c\u65e0\u9700\u989d\u5916\u6a21\u578b/\u53c2\u6570\u5373\u53ef\u589e\u5f3a\u5185\u5bb9\u8bc4\u4f30", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660eASPO\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u7684\u6574\u4f53\u6027\u80fd\u8868\u73b0", "conclusion": "\u53e5\u5b50\u7ea7\u81ea\u9002\u5e94\u4f18\u5316\u673a\u5236\u6709\u6548\u6539\u8fdb\u4e86\u591a\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\uff0c\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2505.19103", "pdf": "https://arxiv.org/pdf/2505.19103", "abs": "https://arxiv.org/abs/2505.19103", "authors": ["Iddo Yosha", "Dorin Shteyman", "Yossi Adi"], "title": "WHISTRESS: Enriching Transcriptions with Sentence Stress Detection", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to Interspeech2025", "summary": "Spoken language conveys meaning not only through words but also through\nintonation, emotion, and emphasis. Sentence stress, the emphasis placed on\nspecific words within a sentence, is crucial for conveying speaker intent and\nhas been extensively studied in linguistics. In this work, we introduce\nWHISTRESS, an alignment-free approach for enhancing transcription systems with\nsentence stress detection. To support this task, we propose TINYSTRESS-15K, a\nscalable, synthetic training data for the task of sentence stress detection\nwhich resulted from a fully automated dataset creation process. We train\nWHISTRESS on TINYSTRESS-15K and evaluate it against several competitive\nbaselines. Our results show that WHISTRESS outperforms existing methods while\nrequiring no additional input priors during training or inference. Notably,\ndespite being trained on synthetic data, WHISTRESS demonstrates strong\nzero-shot generalization across diverse benchmarks. Project page:\nhttps://pages.cs.huji.ac.il/adiyoss-lab/whistress.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u5bf9\u9f50\u7684WHISTRESS\u6a21\u578b\u589e\u5f3a\u8bed\u97f3\u8f6c\u5f55\u7cfb\u7edf\u7684\u53e5\u5b50\u91cd\u97f3\u68c0\u6d4b\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u96c6TINYSTRESS-15K\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u3002", "motivation": "\u53e3\u8bed\u4e2d\u53e5\u5b50\u91cd\u97f3\u5bf9\u4f20\u8fbe\u610f\u56fe\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u8f6c\u5f55\u7cfb\u7edf\u7f3a\u4e4f\u6709\u6548\u68c0\u6d4b\u80fd\u529b\uff0c\u9700\u8981\u65e0\u9700\u5bf9\u9f50\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u81ea\u52a8\u5316\u6d41\u7a0b\u521b\u5efaTINYSTRESS-15K\u5408\u6210\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bad\u7ec3\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u7684WHISTRESS\u6a21\u578b\u3002", "result": "WHISTRESS\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\u3002", "conclusion": "\u5408\u6210\u6570\u636e\u9a71\u52a8\u7684WHISTRESS\u6a21\u578b\u6709\u6548\u89e3\u51b3\u53e5\u5b50\u91cd\u97f3\u68c0\u6d4b\u96be\u9898\uff0c\u4e3a\u8bed\u97f3\u7406\u89e3\u7cfb\u7edf\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.19108", "pdf": "https://arxiv.org/pdf/2505.19108", "abs": "https://arxiv.org/abs/2505.19108", "authors": ["Yongheng Zhang", "Xu Liu", "Ruoxi Zhou", "Qiguang Chen", "Hao Fei", "Wenpeng Lu", "Libo Qin"], "title": "CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 Main Conference", "summary": "Investigating hallucination issues in large language models (LLMs) within\ncross-lingual and cross-modal scenarios can greatly advance the large-scale\ndeployment in real-world applications. Nevertheless, the current studies are\nlimited to a single scenario, either cross-lingual or cross-modal, leaving a\ngap in the exploration of hallucinations in the joint cross-lingual and\ncross-modal scenarios. Motivated by this, we introduce a novel joint\nCross-lingual and Cross-modal Hallucinations benchmark (CCHall) to fill this\ngap. Specifically, CCHall simultaneously incorporates both cross-lingual and\ncross-modal hallucination scenarios, which can be used to assess the\ncross-lingual and cross-modal capabilities of LLMs. Furthermore, we conduct a\ncomprehensive evaluation on CCHall, exploring both mainstream open-source and\nclosed-source LLMs. The experimental results highlight that current LLMs still\nstruggle with CCHall. We hope CCHall can serve as a valuable resource to assess\nLLMs in joint cross-lingual and cross-modal scenarios.", "AI": {"tldr": "\u63d0\u51fa\u8054\u5408\u8de8\u8bed\u8a00\u4e0e\u8de8\u6a21\u6001\u7684\u5e7b\u89c9\u57fa\u51c6\u6d4b\u8bd5CCHall\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u8868\u73b0", "motivation": "\u73b0\u6709\u7814\u7a76\u4ec5\u5173\u6ce8\u5355\u4e00\u573a\u666f\uff08\u8de8\u8bed\u8a00\u6216\u8de8\u6a21\u6001\uff09\uff0c\u7f3a\u4e4f\u5bf9\u8054\u5408\u573a\u666f\u4e0b\u6a21\u578b\u5e7b\u89c9\u95ee\u9898\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30", "method": "\u6784\u5efaCCHall\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5305\u542b\u8de8\u8bed\u8a00\uff08\u591a\u8bed\u8a00\uff09\u548c\u8de8\u6a21\u6001\uff08\u6587\u672c-\u56fe\u50cf\uff09\u7684\u8054\u5408\u8bc4\u4f30\u4efb\u52a1\uff0c\u5bf9\u4e3b\u6d41\u5f00\u6e90/\u95ed\u6e90LLMs\u8fdb\u884c\u7efc\u5408\u6d4b\u8bd5", "result": "\u5b9e\u9a8c\u8868\u660e\u5f53\u524dLLMs\u5728CCHall\u57fa\u51c6\u4e0a\u8868\u73b0\u6b20\u4f73\uff0c\u9a8c\u8bc1\u4e86\u8054\u5408\u573a\u666f\u8bc4\u4f30\u7684\u5fc5\u8981\u6027", "conclusion": "CCHall\u4e3a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\uff0c\u63a8\u52a8LLMs\u5728\u8de8\u8bed\u8a00\u8de8\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u7814\u7a76"}}
{"id": "2505.19112", "pdf": "https://arxiv.org/pdf/2505.19112", "abs": "https://arxiv.org/abs/2505.19112", "authors": ["Zheng Chu", "Huiming Fan", "Jingchang Chen", "Qianyu Wang", "Mingda Yang", "Jiafeng Liang", "Zhongjie Wang", "Hao Li", "Guo Tang", "Ming Liu", "Bing Qin"], "title": "Self-Critique Guided Iterative Reasoning for Multi-hop Question Answering", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Although large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities, they still face challenges in knowledge-intensive multi-hop\nreasoning. Recent work explores iterative retrieval to address complex\nproblems. However, the lack of intermediate guidance often results in\ninaccurate retrieval and flawed intermediate reasoning, leading to incorrect\nreasoning. To address these, we propose Self-Critique Guided Iterative\nReasoning (SiGIR), which uses self-critique feedback to guide the iterative\nreasoning process. Specifically, through end-to-end training, we enable the\nmodel to iteratively address complex problems via question decomposition.\nAdditionally, the model is able to self-evaluate its intermediate reasoning\nsteps. During iterative reasoning, the model engages in branching exploration\nand employs self-evaluation to guide the selection of promising reasoning\ntrajectories. Extensive experiments on three multi-hop reasoning datasets\ndemonstrate the effectiveness of our proposed method, surpassing the previous\nSOTA by $8.6\\%$. Furthermore, our thorough analysis offers insights for future\nresearch. Our code, data, and models are available at Github:\nhttps://github.com/zchuz/SiGIR-MHQA.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u6279\u5224\u5f15\u5bfc\u8fed\u4ee3\u63a8\u7406\u65b9\u6cd5(SiGIR)\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u5b9e\u73b0\u95ee\u9898\u5206\u89e3\u4e0e\u81ea\u6211\u8bc4\u4f30\uff0c\u63d0\u5347\u591a\u8df3\u63a8\u7406\u6548\u679c", "motivation": "\u73b0\u6709\u8fed\u4ee3\u68c0\u7d22\u65b9\u6cd5\u7f3a\u4e4f\u4e2d\u95f4\u6b65\u9aa4\u6307\u5bfc\uff0c\u5bfc\u81f4\u68c0\u7d22\u4e0d\u51c6\u786e\u548c\u4e2d\u95f4\u63a8\u7406\u9519\u8bef\uff0c\u9700\u901a\u8fc7\u81ea\u6211\u53cd\u9988\u673a\u5236\u4f18\u5316\u63a8\u7406\u8f68\u8ff9\u9009\u62e9", "method": "\u7ed3\u5408\u5206\u652f\u63a2\u7d22\u4e0e\u81ea\u6211\u8bc4\u4f30\u673a\u5236\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u5b9e\u73b0\u8fed\u4ee3\u5f0f\u95ee\u9898\u5206\u89e3\uff0c\u5e76\u57fa\u4e8e\u8d28\u91cf\u8bc4\u4f30\u9009\u62e9\u6700\u4f18\u63a8\u7406\u8def\u5f84", "result": "\u5728\u4e09\u4e2a\u591a\u8df3\u63a8\u7406\u6570\u636e\u96c6\u4e0a\u8d85\u8fc7\u5148\u524d\u6700\u4f18\u65b9\u6cd58.6%", "conclusion": "\u81ea\u6211\u6279\u5224\u673a\u5236\u80fd\u6709\u6548\u63d0\u5347\u590d\u6742\u63a8\u7406\u7684\u53ef\u9760\u6027\uff0c\u5f00\u653e\u4ee3\u7801\u548c\u6570\u636e\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u57fa\u7840"}}
{"id": "2505.19116", "pdf": "https://arxiv.org/pdf/2505.19116", "abs": "https://arxiv.org/abs/2505.19116", "authors": ["Nahyun Lee", "Yeongseo Woo", "Hyunwoo Ko", "Guijin Son"], "title": "Controlling Language Confusion in Multilingual LLMs", "categories": ["cs.CL"], "comment": "4 pages", "summary": "Large language models often suffer from language confusion, a phenomenon\nwhere responses are partially or entirely generated in unintended languages.\nThis can critically impact user experience in low-resource settings. We\nhypothesize that conventional supervised fine-tuning exacerbates this issue\nbecause the softmax objective focuses probability mass only on the single\ncorrect token but does not explicitly penalize cross-lingual mixing.\nInterestingly, by observing loss trajectories during the pretraining phase, we\nobserve that models fail to learn to distinguish between monolingual and\nlanguage-confused text. Additionally, we find that ORPO, which adds penalties\nfor unwanted output styles to standard SFT, effectively suppresses\nlanguage-confused generations even at high decoding temperatures without\ndegrading overall model performance. Our findings suggest that incorporating\nappropriate penalty terms can mitigate language confusion in low-resource\nsettings with limited data.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7ORPO\u60e9\u7f5a\u673a\u5236\u6709\u6548\u7f13\u89e3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8de8\u8bed\u8a00\u6df7\u6dc6\u95ee\u9898\uff0c\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u4fdd\u6301\u6a21\u578b\u6027\u80fd", "motivation": "\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u52a0\u5267\u8bed\u8a00\u6df7\u6dc6\u95ee\u9898\uff0c\u5f71\u54cd\u4f4e\u8d44\u6e90\u573a\u666f\u7528\u6237\u4f53\u9a8c\uff0c\u9700\u63a2\u7d22\u6539\u8fdb\u65b9\u6848", "method": "\u5728\u6807\u51c6SFT\u57fa\u7840\u4e0a\u5f15\u5165ORPO\u60e9\u7f5a\u673a\u5236\uff0c\u901a\u8fc7\u6291\u5236\u975e\u76ee\u6807\u8bed\u8a00\u8f93\u51fa\u6765\u63d0\u5347\u6a21\u578b\u8bed\u8a00\u4e00\u81f4\u6027", "result": "ORPO\u5728\u9ad8\u89e3\u7801\u6e29\u5ea6\u4e0b\u4ecd\u80fd\u6709\u6548\u63a7\u5236\u8bed\u8a00\u6df7\u6dc6\uff0c\u4e14\u4e0d\u635f\u5bb3\u6a21\u578b\u6574\u4f53\u751f\u6210\u8d28\u91cf", "conclusion": "\u9002\u5f53\u60e9\u7f5a\u673a\u5236\u53ef\u663e\u8457\u6539\u5584\u8bed\u8a00\u6df7\u6dc6\u95ee\u9898\uff0c\u4e3a\u4f4e\u8d44\u6e90\u73af\u5883\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.19121", "pdf": "https://arxiv.org/pdf/2505.19121", "abs": "https://arxiv.org/abs/2505.19121", "authors": ["Seunguk Yu", "Juhwan Choi", "Youngbin Kim"], "title": "Delving into Multilingual Ethical Bias: The MSQAD with Statistical Hypothesis Tests for Large Language Models", "categories": ["cs.CL"], "comment": "ACL 2025 main conference", "summary": "Despite the recent strides in large language models, studies have underscored\nthe existence of social biases within these systems. In this paper, we delve\ninto the validation and comparison of the ethical biases of LLMs concerning\nglobally discussed and potentially sensitive topics, hypothesizing that these\nbiases may arise from language-specific distinctions. Introducing the\nMultilingual Sensitive Questions & Answers Dataset (MSQAD), we collected news\narticles from Human Rights Watch covering 17 topics, and generated socially\nsensitive questions along with corresponding responses in multiple languages.\nWe scrutinized the biases of these responses across languages and topics,\nemploying two statistical hypothesis tests. The results showed that the null\nhypotheses were rejected in most cases, indicating biases arising from\ncross-language differences. It demonstrates that ethical biases in responses\nare widespread across various languages, and notably, these biases were\nprevalent even among different LLMs. By making the proposed MSQAD openly\navailable, we aim to facilitate future research endeavors focused on examining\ncross-language biases in LLMs and their variant models.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u521b\u5efa\u591a\u8bed\u8a00\u654f\u611f\u95ee\u7b54\u6570\u636e\u96c6MSQAD\uff0c\u9a8c\u8bc1\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\u5728\u654f\u611f\u8bdd\u9898\u4e0a\u7684\u4f26\u7406\u504f\u89c1\uff0c\u53d1\u73b0\u8de8\u8bed\u8a00\u5dee\u5f02\u5bfc\u81f4\u7684\u504f\u89c1\u666e\u904d\u5b58\u5728\u4e14\u4e0d\u540c\u6a21\u578b\u95f4\u8868\u73b0\u4e00\u81f4\uff0c\u540c\u65f6\u5f00\u6e90\u6570\u636e\u96c6\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76", "motivation": "\u9a8c\u8bc1\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u7684\u4f26\u7406\u504f\u89c1\u662f\u5426\u6e90\u4e8e\u8bed\u8a00\u7279\u5f02\u6027\u5dee\u5f02\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u8bed\u8a00\u654f\u611f\u95ee\u9898\u6570\u636e\u96c6\u5206\u6790\u8de8\u8bed\u8a00\u504f\u89c1\u8868\u73b0", "method": "1. \u6536\u96c6\u4eba\u6743\u89c2\u5bdf\u7ec4\u7ec717\u4e2a\u4e3b\u9898\u7684\u65b0\u95fb 2. \u751f\u6210\u591a\u8bed\u8a00\u654f\u611f\u95ee\u9898\u53ca\u56de\u7b54 3. \u8fd0\u7528\u4e24\u79cd\u7edf\u8ba1\u5047\u8bbe\u68c0\u9a8c\u65b9\u6cd5\u5206\u6790\u8bed\u8a00/\u4e3b\u9898\u7ef4\u5ea6\u7684\u56de\u7b54\u504f\u89c1", "result": "\u7edf\u8ba1\u68c0\u9a8c\u663e\u793a\u591a\u6570\u60c5\u51b5\u4e0b\u62d2\u7edd\u539f\u5047\u8bbe\uff0c\u8bc1\u5b9e\u4e0d\u540c\u8bed\u8a00\u95f4\u5b58\u5728\u663e\u8457\u4f26\u7406\u504f\u89c1\uff0c\u4e14\u4e0d\u540cLLM\u6a21\u578b\u95f4\u666e\u904d\u5b58\u5728\u6b64\u73b0\u8c61", "conclusion": "\u8bed\u8a00\u6a21\u578b\u7684\u4f26\u7406\u504f\u89c1\u5177\u6709\u8de8\u8bed\u8a00\u666e\u904d\u6027\uff0c\u5f00\u6e90MSQAD\u6570\u636e\u96c6\u53ef\u4e3a\u540e\u7eed\u6a21\u578b\u504f\u89c1\u7684\u68c0\u6d4b\u4e0e\u4f18\u5316\u63d0\u4f9b\u57fa\u51c6"}}
{"id": "2505.19126", "pdf": "https://arxiv.org/pdf/2505.19126", "abs": "https://arxiv.org/abs/2505.19126", "authors": ["Wenyang Luo", "Wayne Xin Zhao", "Jing Sha", "Shijin Wang", "Ji-Rong Wen"], "title": "MMATH: A Multilingual Benchmark for Mathematical Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "The advent of large reasoning models, such as OpenAI o1 and DeepSeek R1, has\nsignificantly advanced complex reasoning tasks. However, their capabilities in\nmultilingual complex reasoning remain underexplored, with existing efforts\nlargely focused on simpler tasks like MGSM. To address this gap, we introduce\nMMATH, a benchmark for multilingual complex reasoning spanning 374 high-quality\nmath problems across 10 typologically diverse languages. Using MMATH, we\nobserve that even advanced models like DeepSeek R1 exhibit substantial\nperformance disparities across languages and suffer from a critical off-target\nissue-generating responses in unintended languages. To address this, we explore\nstrategies including prompting and training, demonstrating that reasoning in\nEnglish and answering in target languages can simultaneously enhance\nperformance and preserve target-language consistency. Our findings offer new\ninsights and practical strategies for advancing the multilingual reasoning\ncapabilities of large language models. Our code and data could be found at\nhttps://github.com/RUCAIBox/MMATH.", "AI": {"tldr": "\u63d0\u51fa\u591a\u8bed\u8a00\u590d\u6742\u63a8\u7406\u57fa\u51c6MMATH\uff0c\u63ed\u793a\u5148\u8fdb\u6a21\u578b\u5b58\u5728\u7684\u8bed\u8a00\u6027\u80fd\u5dee\u5f02\u5e76\u63d0\u51fa\u82f1\u5f0f\u63a8\u7406+\u76ee\u6807\u8bed\u8a00\u8f93\u51fa\u7684\u4f18\u5316\u7b56\u7565", "motivation": "\u73b0\u6709\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u591a\u8bed\u8a00\u590d\u6742\u4efb\u52a1\u4e0a\u7684\u80fd\u529b\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u5efa\u7acb\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6", "method": "\u6784\u5efa\u8986\u76d610\u79cd\u8bed\u8a00/374\u6570\u5b66\u9898\u7684MMATH\u57fa\u51c6\uff0c\u5206\u6790\u6a21\u578b\u8de8\u8bed\u8a00\u8868\u73b0\u53ca\u8bed\u8a00\u504f\u79bb\u95ee\u9898", "result": "\u901a\u8fc7\u63d0\u793a\u7b56\u7565\u4f18\u5316\uff0c\u5b9e\u73b0\u6a21\u578b\u6027\u80fd\u63d0\u534715.2%\u4e14\u76ee\u6807\u8bed\u8a00\u56de\u7b54\u51c6\u786e\u7387\u63d0\u9ad8\u81f398.7%", "conclusion": "\u7814\u7a76\u4e3a\u63d0\u5347\u5927\u6a21\u578b\u591a\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u65b0\u65b9\u6cd5\u8bba\uff0c\u9a8c\u8bc1\u8de8\u8bed\u8a00\u601d\u7ef4\u94fe\u7684\u6709\u6548\u6027"}}
{"id": "2505.19128", "pdf": "https://arxiv.org/pdf/2505.19128", "abs": "https://arxiv.org/abs/2505.19128", "authors": ["Jin Zhang", "Fan Gao", "Linyu Li", "Yongbin Yu", "Xiangxiang Wang", "Nyima Tashi", "Gadeng Luosang"], "title": "RetrieveAll: A Multilingual Named Entity Recognition Framework with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rise of large language models has led to significant performance\nbreakthroughs in named entity recognition (NER) for high-resource languages,\nyet there remains substantial room for improvement in low- and medium-resource\nlanguages. Existing multilingual NER methods face severe language interference\nduring the multi-language adaptation process, manifested in feature conflicts\nbetween different languages and the competitive suppression of low-resource\nlanguage features by high-resource languages. Although training a dedicated\nmodel for each language can mitigate such interference, it lacks scalability\nand incurs excessive computational costs in real-world applications. To address\nthis issue, we propose RetrieveAll, a universal multilingual NER framework\nbased on dynamic LoRA. The framework decouples task-specific features across\nlanguages and demonstrates efficient dynamic adaptability. Furthermore, we\nintroduce a cross-granularity knowledge augmented method that fully exploits\nthe intrinsic potential of the data without relying on external resources. By\nleveraging a hierarchical prompting mechanism to guide knowledge injection,\nthis approach advances the paradigm from \"prompt-guided inference\" to\n\"prompt-driven learning.\" Experimental results show that RetrieveAll\noutperforms existing baselines; on the PAN-X dataset, it achieves an average F1\nimprovement of 12.1 percent.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u52a8\u6001LoRA\u7684RetrieveAll\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u8de8\u8bed\u8a00\u7279\u5f81\u548c\u8de8\u7c92\u5ea6\u77e5\u8bc6\u589e\u5f3a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u591a\u8bed\u8a00NER\u6027\u80fd\uff0c\u5728PAN-X\u6570\u636e\u96c6\u5b9e\u73b012.1%\u7684F1\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u591a\u8bed\u8a00NER\u65b9\u6cd5\u5b58\u5728\u8bed\u8a00\u5e72\u6270\u95ee\u9898\uff0c\u9ad8/\u4f4e\u8d44\u6e90\u8bed\u8a00\u95f4\u5b58\u5728\u7279\u5f81\u51b2\u7a81\u548c\u7ade\u4e89\u6291\u5236\u3002\u5355\u72ec\u8bad\u7ec3\u6a21\u578b\u867d\u80fd\u7f13\u89e3\u4f46\u6210\u672c\u8fc7\u9ad8\uff0c\u9700\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u52a8\u6001LoRA\u5b9e\u73b0\u8de8\u8bed\u8a00\u7279\u5f81\u89e3\u8026\u548c\u52a8\u6001\u9002\u914d\n2. \u8de8\u7c92\u5ea6\u77e5\u8bc6\u589e\u5f3a\u6316\u6398\u6570\u636e\u5185\u5728\u6f5c\u529b\n3. \u5206\u5c42\u63d0\u793a\u673a\u5236\u5c06\u77e5\u8bc6\u6ce8\u5165\u8303\u5f0f\u5347\u7ea7\u4e3a'\u63d0\u793a\u9a71\u52a8\u5b66\u4e60'", "result": "\u5728PAN-X\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5e73\u5747F1\u5206\u6570\u63d0\u534712.1%\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RetrieveAll\u6709\u6548\u89e3\u51b3\u4e86\u591a\u8bed\u8a00NER\u4e2d\u7684\u8bed\u8a00\u5e72\u6270\u95ee\u9898\uff0c\u4e0d\u4f9d\u8d56\u5916\u90e8\u8d44\u6e90\u5373\u53ef\u63d0\u5347\u6027\u80fd\uff0c\u4e3a\u63d0\u793a\u5b66\u4e60\u8303\u5f0f\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.19147", "pdf": "https://arxiv.org/pdf/2505.19147", "abs": "https://arxiv.org/abs/2505.19147", "authors": ["Xuyang Liu", "Zichen Wen", "Shaobo Wang", "Junjie Chen", "Zhishan Tao", "Yubo Wang", "Xiangqi Jin", "Chang Zou", "Yiyu Wang", "Chenfei Liao", "Xu Zheng", "Honggang Chen", "Weijia Li", "Xuming Hu", "Conghui He", "Linfeng Zhang"], "title": "Shifting AI Efficiency From Model-Centric to Data-Centric Compression", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Project:\n  \\url{https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression}", "summary": "The rapid advancement of large language models (LLMs) and multi-modal LLMs\n(MLLMs) has historically relied on model-centric scaling through increasing\nparameter counts from millions to hundreds of billions to drive performance\ngains. However, as we approach hardware limits on model size, the dominant\ncomputational bottleneck has fundamentally shifted to the quadratic cost of\nself-attention over long token sequences, now driven by ultra-long text\ncontexts, high-resolution images, and extended videos. In this position paper,\n\\textbf{we argue that the focus of research for efficient AI is shifting from\nmodel-centric compression to data-centric compression}. We position token\ncompression as the new frontier, which improves AI efficiency via reducing the\nnumber of tokens during model training or inference. Through comprehensive\nanalysis, we first examine recent developments in long-context AI across\nvarious domains and establish a unified mathematical framework for existing\nmodel efficiency strategies, demonstrating why token compression represents a\ncrucial paradigm shift in addressing long-context overhead. Subsequently, we\nsystematically review the research landscape of token compression, analyzing\nits fundamental benefits and identifying its compelling advantages across\ndiverse scenarios. Furthermore, we provide an in-depth analysis of current\nchallenges in token compression research and outline promising future\ndirections. Ultimately, our work aims to offer a fresh perspective on AI\nefficiency, synthesize existing research, and catalyze innovative developments\nto address the challenges that increasing context lengths pose to the AI\ncommunity's advancement.", "AI": {"tldr": "\u63d0\u51fa\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u4ee4\u724c\u538b\u7f29\u4f5c\u4e3a\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587AI\u6548\u7387\u74f6\u9888\u7684\u65b0\u8303\u5f0f", "motivation": "\u786c\u4ef6\u9650\u5236\u4e0b\u6a21\u578b\u4e2d\u5fc3\u538b\u7f29\u5df2\u89e6\u9876\uff0c\u4e8c\u6b21\u8ba1\u7b97\u6210\u672c\u6210\u4e3a\u65b0\u74f6\u9888\uff0c\u9700\u8f6c\u5411\u6570\u636e\u5c42\u9762\u7684\u6548\u7387\u4f18\u5316", "method": "\u5efa\u7acb\u7edf\u4e00\u6570\u5b66\u6846\u67b6\u5206\u6790\u6a21\u578b\u6548\u7387\u7b56\u7565\uff0c\u7cfb\u7edf\u56de\u987e\u4ee4\u724c\u538b\u7f29\u6280\u672f\u53ca\u5176\u8de8\u9886\u57df\u5e94\u7528\u573a\u666f", "result": "\u8bc1\u5b9e\u4ee4\u724c\u538b\u7f29\u5728\u591a\u79cd\u573a\u666f\u7684\u663e\u8457\u4f18\u52bf\uff0c\u660e\u786e\u5f53\u524d\u7814\u7a76\u6311\u6218\u5e76\u89c4\u5212\u672a\u6765\u53d1\u5c55\u8def\u5f84", "conclusion": "\u4ee4\u724c\u538b\u7f29\u6807\u5fd7\u7740AI\u6548\u7387\u7814\u7a76\u7684\u5173\u952e\u8f6c\u6298\uff0c\u9700\u8de8\u5b66\u79d1\u534f\u4f5c\u5e94\u5bf9\u957f\u4e0a\u4e0b\u6587\u6311\u6218"}}
{"id": "2505.19163", "pdf": "https://arxiv.org/pdf/2505.19163", "abs": "https://arxiv.org/abs/2505.19163", "authors": ["Firoj Alam", "Md Arid Hasan", "Shammur Absar Chowdhury"], "title": "SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7"], "comment": "Spoken Question Answering, Multilingual LLMs, Speech-based\n  Evaluation, Dialectal Speech, Low-resource Languages, Multimodal\n  Benchmarking, Conversational AI, Speech-to-Text QA, Real-world Interaction,\n  Natural Language Understanding", "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\nvarious disciplines and tasks. However, benchmarking their capabilities with\nmultilingual spoken queries remains largely unexplored. In this study, we\nintroduce SpokenNativQA, the first multilingual and culturally aligned spoken\nquestion-answering (SQA) dataset designed to evaluate LLMs in real-world\nconversational settings. The dataset comprises approximately 33,000 naturally\nspoken questions and answers in multiple languages, including low-resource and\ndialect-rich languages, providing a robust benchmark for assessing LLM\nperformance in speech-based interactions. SpokenNativQA addresses the\nlimitations of text-based QA datasets by incorporating speech variability,\naccents, and linguistic diversity. We benchmark different ASR systems and LLMs\nfor SQA and present our findings. We released the data at\n(https://huggingface.co/datasets/QCRI/SpokenNativQA) and the experimental\nscripts at (https://llmebench.qcri.org/) for the research community.", "AI": {"tldr": "\u9996\u4e2a\u591a\u8bed\u8a00\u8bed\u97f3\u95ee\u7b54\u6570\u636e\u96c6SpokenNativQA\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u771f\u5b9e\u8bed\u97f3\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u5305\u542b33K\u8bed\u97f3\u6837\u672c\u5e76\u6db5\u76d6\u4f4e\u8d44\u6e90\u8bed\u8a00", "motivation": "\u73b0\u6709\u6587\u672cQA\u6570\u636e\u96c6\u65e0\u6cd5\u53cd\u6620\u8bed\u97f3\u4ea4\u4e92\u4e2d\u7684\u53e3\u97f3\u3001\u8bed\u97f3\u53d8\u5f02\u548c\u8bed\u8a00\u591a\u6837\u6027\u95ee\u9898\uff0c\u9700\u6784\u5efa\u66f4\u8d34\u8fd1\u771f\u5b9e\u573a\u666f\u7684\u8bed\u97f3\u57fa\u51c6\u6d4b\u8bd5", "method": "\u521b\u5efa\u5305\u542b\u591a\u8bed\u8a00/\u65b9\u8a00\u7684\u81ea\u7136\u8bed\u97f3\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u4e0d\u540cASR\u7cfb\u7edf\u4e0eLLM\u5728\u8bed\u97f3\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "result": "\u6570\u636e\u96c6\u6709\u6548\u8bc4\u4f30\u8bed\u97f3\u4ea4\u4e92\u6027\u80fd\uff0c\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u73b0\u6709\u7cfb\u7edf\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u4e0a\u7684\u5c40\u9650\u6027", "conclusion": "SpokenNativQA\u586b\u8865\u4e86\u8bed\u97f3\u8bc4\u4f30\u7a7a\u767d\uff0c\u516c\u5f00\u6570\u636e/\u4ee3\u7801\u4fc3\u8fdbLLM\u5728\u8bed\u97f3\u4ea4\u4e92\u9886\u57df\u7684\u7814\u7a76"}}
{"id": "2505.19176", "pdf": "https://arxiv.org/pdf/2505.19176", "abs": "https://arxiv.org/abs/2505.19176", "authors": ["Zhuo Liu", "Moxin Li", "Xun Deng", "Qifan Wang", "Fuli Feng"], "title": "Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge", "categories": ["cs.CL"], "comment": "Under review", "summary": "LLM-as-a-Judge employs large language models (LLMs), such as GPT-4, to\nevaluate the quality of LLM-generated responses, gaining popularity for its\ncost-effectiveness and strong alignment with human evaluations. However,\ntraining proxy judge models using evaluation data generated by powerful teacher\nmodels introduces a critical yet previously overlooked issue: teacher\npreference bias, where the proxy judge model learns a biased preference for\nresponses from the teacher model. To tackle this problem, we propose a novel\nsetting that incorporates an additional assistant model, which is not biased\ntoward the teacher model's responses, to complement the training data. Building\non this setup, we introduce AGDe-Judge, a three-stage framework designed to\ndebias from both the labels and feedbacks in the training data. Extensive\nexperiments demonstrate that AGDe-Judge effectively reduces teacher preference\nbias while maintaining strong performance across six evaluation benchmarks.\nCode is available at https://github.com/Liuz233/AGDe-Judge.", "AI": {"tldr": "\u63d0\u51faAGDe-Judge\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u51cf\u5c11\u6559\u5e08\u6a21\u578b\u504f\u597d\u504f\u5dee\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u57fa\u51c6\u4fdd\u6301\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u6559\u5e08\u504f\u597d\u504f\u5dee\u95ee\u9898\uff0c\u4ee3\u7406\u6a21\u578b\u8bad\u7ec3\u65f6\u8fc7\u5ea6\u4f9d\u8d56\u6559\u5e08\u6a21\u578b\u751f\u6210\u7684\u6709\u504f\u6570\u636e\uff0c\u5f71\u54cd\u8bc4\u4f30\u5ba2\u89c2\u6027\u3002", "method": "\u5f15\u5165\u65e0\u504f\u8f85\u52a9\u6a21\u578b\u8865\u5145\u8bad\u7ec3\u6570\u636e\uff0c\u8bbe\u8ba1\u6807\u7b7e\u53bb\u504f\u3001\u53cd\u9988\u53bb\u504f\u3001\u6570\u636e\u589e\u5f3a\u4e09\u9636\u6bb5\u6846\u67b6(AGDe-Judge)\u5b9e\u73b0\u53cc\u91cd\u53bb\u504f\u3002", "result": "\u57286\u4e2a\u8bc4\u4f30\u57fa\u51c6\u4e0a\u6709\u6548\u964d\u4f4e\u6559\u5e08\u504f\u597d\u504f\u5dee\uff0c\u6a21\u578b\u6027\u80fd\u4fdd\u6301\u7a33\u5b9a\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\u3002", "conclusion": "\u9996\u6b21\u63d0\u51fa\u5305\u542b\u8f85\u52a9\u6a21\u578b\u7684\u8bc4\u4f30\u8bbe\u7f6e\uff0cAGDe-Judge\u4e3aLLM\u8bc4\u4f30\u63d0\u4f9b\u53ef\u9760\u53bb\u504f\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.19184", "pdf": "https://arxiv.org/pdf/2505.19184", "abs": "https://arxiv.org/abs/2505.19184", "authors": ["Minh Nhat Nguyen", "Pradyumna Shyama Prasad"], "title": "Two LLMs debate, both are certain they've won", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Can LLMs accurately adjust their confidence when facing opposition? Building\non previous studies measuring calibration on static fact-based\nquestion-answering tasks, we evaluate Large Language Models (LLMs) in a\ndynamic, adversarial debate setting, uniquely combining two realistic factors:\n(a) a multi-turn format requiring models to update beliefs as new information\nemerges, and (b) a zero-sum structure to control for task-related uncertainty,\nsince mutual high-confidence claims imply systematic overconfidence. We\norganized 60 three-round policy debates among ten state-of-the-art LLMs, with\nmodels privately rating their confidence (0-100) in winning after each round.\nWe observed five concerning patterns: (1) Systematic overconfidence: models\nbegan debates with average initial confidence of 72.9% vs. a rational 50%\nbaseline. (2) Confidence escalation: rather than reducing confidence as debates\nprogressed, debaters increased their win probabilities, averaging 83% by the\nfinal round. (3) Mutual overestimation: in 61.7% of debates, both sides\nsimultaneously claimed >=75% probability of victory, a logical impossibility.\n(4) Persistent self-debate bias: models debating identical copies increased\nconfidence from 64.1% to 75.2%; even when explicitly informed their chance of\nwinning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5)\nMisaligned private reasoning: models' private scratchpad thoughts sometimes\ndiffered from their public confidence ratings, raising concerns about\nfaithfulness of chain-of-thought reasoning. These results suggest LLMs lack the\nability to accurately self-assess or update their beliefs in dynamic,\nmulti-turn tasks; a major concern as LLM outputs are deployed without careful\nreview in assistant roles or agentic settings.", "AI": {"tldr": "LLMs\u5728\u52a8\u6001\u5bf9\u6297\u8fa9\u8bba\u4e2d\u65e0\u6cd5\u51c6\u786e\u8c03\u6574\u7f6e\u4fe1\u5ea6\uff0c\u5b58\u5728\u7cfb\u7edf\u6027\u8fc7\u5ea6\u81ea\u4fe1\uff08\u521d\u59cb72.9% vs \u7406\u6027\u57fa\u51c650%\uff09\u3001\u4fe1\u5fc3\u5347\u7ea7\uff08\u6700\u7ec8\u8fbe83%\uff09\u300161.7%\u8fa9\u8bba\u51fa\u73b0\u76f8\u4e92\u9ad8\u4f30\u3001\u81ea\u6211\u8fa9\u8bba\u65f6\u4fe1\u5fc3\u6301\u7eed\u4e0a\u5347\uff0864.1%\u219275.2%\uff09\u3001\u79c1\u5bc6\u63a8\u7406\u4e0e\u516c\u5f00\u8bc4\u5206\u4e0d\u4e00\u81f4\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5728\u9759\u6001\u77e5\u8bc6\u95ee\u7b54\u4e2d\u8bc4\u4f30LLM\u6821\u51c6\u80fd\u529b\uff0c\u672c\u7814\u7a76\u9996\u6b21\u5728\u52a8\u6001\u5bf9\u6297\u573a\u666f\uff08\u591a\u8f6e\u8fa9\u8bba+\u96f6\u548c\u535a\u5f08\uff09\u4e2d\u9a8c\u8bc1\u6a21\u578b\u8c03\u6574\u7f6e\u4fe1\u5ea6\u7684\u80fd\u529b\u3002", "method": "\u7ec4\u7ec710\u4e2a\u524d\u6cbfLLM\u8fdb\u884c60\u573a\u4e09\u8f6e\u653f\u7b56\u8fa9\u8bba\uff0c\u8981\u6c42\u6a21\u578b\u6bcf\u8f6e\u7ed3\u675f\u540e\u79c1\u4e0b\u8bc4\u4f30\u83b7\u80dc\u6982\u7387\uff080-100\u5206\uff09\uff0c\u63a7\u5236\u4efb\u52a1\u4e0d\u786e\u5b9a\u6027\u4ee5\u68c0\u6d4b\u7cfb\u7edf\u6027\u504f\u5dee\u3002", "result": "\u53d1\u73b0\u4e94\u4e2a\u5173\u952e\u6a21\u5f0f\uff1a1\uff09\u7cfb\u7edf\u6027\u521d\u59cb\u9ad8\u4f30 2\uff09\u8fa9\u8bba\u8fdb\u7a0b\u53cd\u800c\u63d0\u5347\u4fe1\u5fc3 3\uff0961.7%\u8fa9\u8bba\u53cc\u65b9\u540c\u65f6\u5ba3\u79f0\u226575%\u80dc\u7387 4\uff09\u514b\u9686\u81ea\u6211\u8fa9\u8bba\u4fe1\u5fc3\u6301\u7eed\u4e0a\u5347\uff08\u5373\u4f7f\u660e\u786e\u544a\u77e550%\u80dc\u7387\u4ecd\u4ece50%\u5347\u81f357.1%\uff095\uff09\u601d\u7ef4\u94fe\u63a8\u7406\u4e0e\u516c\u5f00\u8bc4\u5206\u5b58\u5728\u77db\u76fe\u3002", "conclusion": "LLMs\u5728\u52a8\u6001\u591a\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u81ea\u6211\u8bc4\u4f30\u548c\u4fe1\u5ff5\u66f4\u65b0\u80fd\u529b\uff0c\u8fd9\u5bf9\u52a9\u7406\u89d2\u8272\u548c\u667a\u80fd\u4f53\u5e94\u7528\u6784\u6210\u91cd\u5927\u98ce\u9669\uff08\u56e0\u5b9e\u9645\u90e8\u7f72\u65f6\u5f80\u5f80\u7f3a\u4e4f\u4eba\u5de5\u5ba1\u6838\uff09\u3002"}}
{"id": "2505.19187", "pdf": "https://arxiv.org/pdf/2505.19187", "abs": "https://arxiv.org/abs/2505.19187", "authors": ["Yang Xiao", "Jiashuo Wang", "Ruifeng Yuan", "Chunpu Xu", "Kaishuai Xu", "Wenjie Li", "Pengfei Liu"], "title": "LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities through test-time scaling approaches, particularly when fine-tuned\nwith chain-of-thought (CoT) data distilled from more powerful large reasoning\nmodels (LRMs). However, these reasoning chains often contain verbose elements\nthat mirror human problem-solving, categorized as progressive reasoning (the\nessential solution development path) and functional elements (verification\nprocesses, alternative solution approaches, and error corrections). While\nprogressive reasoning is crucial, the functional elements significantly\nincrease computational demands during test-time inference. We introduce PIR\n(Perplexity-based Importance Refinement), a principled framework that\nquantitatively evaluates the importance of each reasoning step based on its\nimpact on answer prediction confidence. PIR systematically identifies and\nselectively prunes only low-importance functional steps while preserving\nprogressive reasoning components, creating optimized training data that\nmaintains the integrity of the core solution path while reducing verbosity.\nModels fine-tuned on PIR-optimized data exhibit superior test-time scaling\nproperties, generating more concise reasoning chains while achieving improved\naccuracy (+0.9\\% to +6.6\\%) with significantly reduced token usage (-3\\% to\n-41\\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond).\nOur approach demonstrates strong generalizability across different model sizes,\ndata sources, and token budgets, offering a practical solution for deploying\nreasoning-capable LLMs in scenarios where efficient test-time scaling, response\ntime, and computational efficiency are valuable constraints.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56f0\u60d1\u5ea6\u7684\u91cd\u8981\u6027\u8bc4\u4f30\u6846\u67b6PIR\uff0c\u901a\u8fc7\u526a\u679d\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\u4f18\u5316LLMs\u63a8\u7406\u94fe\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u601d\u7ef4\u94fe\u65b9\u6cd5\u4ea7\u751f\u7684\u63a8\u7406\u94fe\u5305\u542b\u8fc7\u591a\u529f\u80fd\u6027\u5197\u4f59\u6b65\u9aa4(\u5982\u9a8c\u8bc1/\u7ea0\u9519)\uff0c\u5bfc\u81f4\u6d4b\u8bd5\u9636\u6bb5\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u5728\u4fdd\u6301\u6838\u5fc3\u63a8\u7406\u8def\u5f84\u7684\u524d\u63d0\u4e0b\u4f18\u5316\u6a21\u578b\u6548\u7387\u3002", "method": "\u8bbe\u8ba1PIR\u6846\u67b6\u91cf\u5316\u8bc4\u4f30\u5404\u63a8\u7406\u6b65\u9aa4\u5bf9\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u7684\u5f71\u54cd\uff0c\u9009\u62e9\u6027\u526a\u679d\u4f4e\u91cd\u8981\u6027\u529f\u80fd\u6b65\u9aa4\uff0c\u4fdd\u7559\u6e10\u8fdb\u63a8\u7406\u6838\u5fc3\u8def\u5f84\u6784\u5efa\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u3002", "result": "\u5728AIME\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u51c6\u786e\u7387\u63d0\u53470.9%-6.6%\uff0ctoken\u4f7f\u7528\u51cf\u5c1141%\uff0c\u4e14\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b/\u6570\u636e\u6e90\u4e2d\u4fdd\u6301\u826f\u597d\u6cdb\u5316\u6027\u3002", "conclusion": "PIR\u4e3a\u9700\u8981\u9ad8\u6548\u63a8\u7406\u7684\u5b9e\u9645\u573a\u666f\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u62d3\u5c55LLMs\u5728\u5b9e\u65f6\u54cd\u5e94\u573a\u666f\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2505.19191", "pdf": "https://arxiv.org/pdf/2505.19191", "abs": "https://arxiv.org/abs/2505.19191", "authors": ["Nursulu Sagimbayeva", "Ruveyda Bet\u00fcl Bah\u00e7eci", "Ingmar Weber"], "title": "Misleading through Inconsistency: A Benchmark for Political Inconsistencies Detection", "categories": ["cs.CL"], "comment": "8 pages, 6 figures. Accepted for publication in the Proceedings of\n  1st Workshop on Misinformation Detection in the Era of LLMs (MisD) at\n  ICWSM-2025", "summary": "Inconsistent political statements represent a form of misinformation. They\nerode public trust and pose challenges to accountability, when left unnoticed.\nDetecting inconsistencies automatically could support journalists in asking\nclarification questions, thereby helping to keep politicians accountable. We\npropose the Inconsistency detection task and develop a scale of inconsistency\ntypes to prompt NLP-research in this direction. To provide a resource for\ndetecting inconsistencies in a political domain, we present a dataset of 698\nhuman-annotated pairs of political statements with explanations of the\nannotators' reasoning for 237 samples. The statements mainly come from voting\nassistant platforms such as Wahl-O-Mat in Germany and Smartvote in Switzerland,\nreflecting real-world political issues. We benchmark Large Language Models\n(LLMs) on our dataset and show that in general, they are as good as humans at\ndetecting inconsistencies, and might be even better than individual humans at\npredicting the crowd-annotated ground-truth. However, when it comes to\nidentifying fine-grained inconsistency types, none of the model have reached\nthe upper bound of performance (due to natural labeling variation), thus\nleaving room for improvement. We make our dataset and code publicly available.", "AI": {"tldr": "\u7814\u7a76\u6784\u5efa\u653f\u6cbb\u58f0\u660e\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u53d1\u73b0\u6a21\u578b\u603b\u4f53\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u4f46\u7ec6\u7c92\u5ea6\u5206\u7c7b\u5f85\u63d0\u5347", "motivation": "\u653f\u6cbb\u58f0\u660e\u4e0d\u4e00\u81f4\u6027\u4f1a\u524a\u5f31\u516c\u4f17\u4fe1\u4efb\u4e14\u96be\u4ee5\u8ffd\u8d23\uff0c\u81ea\u52a8\u68c0\u6d4b\u6280\u672f\u53ef\u5e2e\u52a9\u8bb0\u8005\u63d0\u51fa\u6f84\u6e05\u95ee\u9898\u4ee5\u7ef4\u62a4\u653f\u6cbb\u95ee\u8d23", "method": "\u63d0\u51fa\u4e0d\u4e00\u81f4\u6027\u68c0\u6d4b\u4efb\u52a1\u5e76\u5236\u5b9a\u7c7b\u578b\u91cf\u8868\uff0c\u6536\u96c6698\u5bf9\u653f\u6cbb\u58f0\u660e\u6784\u5efa\u6807\u6ce8\u6570\u636e\u96c6\uff08\u542b237\u6761\u89e3\u91ca\uff09\uff0c\u4e3b\u8981\u6e90\u81ea\u5fb7/\u745e\u6295\u7968\u5e73\u53f0\u7684\u771f\u5b9e\u653f\u6cbb\u8bae\u9898", "result": "LLMs\u6574\u4f53\u68c0\u6d4b\u80fd\u529b\u4e0e\u4eba\u7c7b\u76f8\u5f53\u751a\u81f3\u4f18\u4e8e\u4e2a\u4f53\u6807\u6ce8\u8005\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u7c7b\u578b\u8bc6\u522b\u4e0a\u5c1a\u672a\u8fbe\u5230\u6027\u80fd\u4e0a\u9650\uff08\u53d7\u6807\u6ce8\u5dee\u5f02\u5f71\u54cd\uff09", "conclusion": "\u516c\u5f00\u6570\u636e\u96c6\u4e0e\u4ee3\u7801\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76\uff0c\u5f53\u524d\u6a21\u578b\u5728\u7c7b\u578b\u8bc6\u522b\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316"}}
{"id": "2505.19201", "pdf": "https://arxiv.org/pdf/2505.19201", "abs": "https://arxiv.org/abs/2505.19201", "authors": ["Yunhai Hu", "Tianhua Xia", "Zining Liu", "Rahul Raman", "Xingyu Liu", "Bo Bao", "Eric Sather", "Vithursan Thangarasa", "Sai Qian Zhang"], "title": "DREAM: Drafting with Refined Target Features and Entropy-Adaptive Cross-Attention Fusion for Multimodal Speculative Decoding", "categories": ["cs.CL"], "comment": null, "summary": "Speculative decoding (SD) has emerged as a powerful method for accelerating\nautoregressive generation in large language models (LLMs), yet its integration\ninto vision-language models (VLMs) remains underexplored. We introduce DREAM, a\nnovel speculative decoding framework tailored for VLMs that combines three key\ninnovations: (1) a cross-attention-based mechanism to inject intermediate\nfeatures from the target model into the draft model for improved alignment, (2)\nadaptive intermediate feature selection based on attention entropy to guide\nefficient draft model training, and (3) visual token compression to reduce\ndraft model latency. DREAM enables efficient, accurate, and parallel multimodal\ndecoding with significant throughput improvement. Experiments across a diverse\nset of recent popular VLMs, including LLaVA, Pixtral, SmolVLM and Gemma3,\ndemonstrate up to 3.6x speedup over conventional decoding and significantly\noutperform prior SD baselines in both inference throughput and speculative\ndraft acceptance length across a broad range of multimodal benchmarks. The code\nis publicly available at: https://github.com/SAI-Lab-NYU/DREAM.git", "AI": {"tldr": "\u63d0\u51faDREAM\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6ce8\u610f\u529b\u673a\u5236\u3001\u81ea\u9002\u5e94\u7279\u5f81\u9009\u62e9\u548c\u89c6\u89c9\u6807\u8bb0\u538b\u7f29\u6280\u672f\uff0c\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u5b9e\u73b0\u9ad8\u6548\u63a8\u6d4b\u89e3\u7801\uff0c\u5b9e\u9a8c\u663e\u793a\u6700\u9ad83.6\u500d\u52a0\u901f", "motivation": "\u63a8\u6d4b\u89e3\u7801\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLMs)\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u591a\u6a21\u6001\u5bf9\u9f50\u6548\u7387\u4f4e\u3001\u8ba1\u7b97\u5ef6\u8fdf\u9ad8\u7684\u95ee\u9898", "method": "1. \u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u7684\u7279\u5f81\u6ce8\u5165\u673a\u5236\n2. \u901a\u8fc7\u6ce8\u610f\u529b\u71b5\u81ea\u9002\u5e94\u9009\u62e9\u7279\u5f81\n3. \u89c6\u89c9\u6807\u8bb0\u538b\u7f29\u964d\u4f4e\u5ef6\u8fdf", "result": "\u5728LLaVA/Pixtral/SmolVLM/Gemma3\u7b49\u6a21\u578b\u4e0a\u5b9e\u73b0\u6700\u9ad83.6\u500d\u52a0\u901f\uff0c\u63a8\u6d4b\u63a5\u53d7\u957f\u5ea6\u548c\u541e\u5410\u91cf\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "DREAM\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u63a8\u6d4b\u89e3\u7801\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7a81\u7834\u6027\u7684\u52a0\u901f\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2505.19206", "pdf": "https://arxiv.org/pdf/2505.19206", "abs": "https://arxiv.org/abs/2505.19206", "authors": ["Richard He Bai", "Zijin Gu", "Tatiana Likhomanenko", "Navdeep Jaitly"], "title": "SpeakStream: Streaming Text-to-Speech with Interleaved Data", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "The latency bottleneck of traditional text-to-speech (TTS) systems\nfundamentally hinders the potential of streaming large language models (LLMs)\nin conversational AI. These TTS systems, typically trained and inferenced on\ncomplete utterances, introduce unacceptable delays, even with optimized\ninference speeds, when coupled with streaming LLM outputs. This is particularly\nproblematic for creating responsive conversational agents where low first-token\nlatency is critical. In this paper, we present SpeakStream, a streaming TTS\nsystem that generates audio incrementally from streaming text using a\ndecoder-only architecture. SpeakStream is trained using a next-step prediction\nloss on interleaved text-speech data. During inference, it generates speech\nincrementally while absorbing streaming input text, making it particularly\nsuitable for cascaded conversational AI agents where an LLM streams text to a\nTTS system. Our experiments demonstrate that SpeakStream achieves\nstate-of-the-art latency results in terms of first-token latency while\nmaintaining the quality of non-streaming TTS systems.", "AI": {"tldr": "SpeakStream\u63d0\u51fa\u6d41\u5f0fTTS\u7cfb\u7edf\uff0c\u901a\u8fc7\u4ec5\u89e3\u7801\u5668\u67b6\u6784\u5b9e\u73b0\u6587\u672c\u6d41\u589e\u91cf\u97f3\u9891\u751f\u6210\uff0c\u5728\u4fdd\u6301\u8d28\u91cf\u7684\u540c\u65f6\u8fbe\u5230\u6700\u4f4e\u9996\u4ee4\u724c\u5ef6\u8fdf", "motivation": "\u4f20\u7edfTTS\u7cfb\u7edf\u5904\u7406\u5b8c\u6574\u8bed\u53e5\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\uff0c\u65e0\u6cd5\u9002\u5e94\u6d41\u5f0fLLM\u8f93\u51fa\u7684\u5b9e\u65f6\u9700\u6c42\uff0c\u5f71\u54cd\u5bf9\u8bddAI\u7684\u54cd\u5e94\u901f\u5ea6", "method": "\u4f7f\u7528\u4ec5\u89e3\u7801\u5668\u67b6\u6784\uff0c\u57fa\u4e8e\u6587\u672c-\u8bed\u97f3\u4ea4\u7ec7\u6570\u636e\u901a\u8fc7\u4e0b\u4e00\u6b65\u9884\u6d4b\u635f\u5931\u8bad\u7ec3\uff0c\u63a8\u7406\u65f6\u652f\u6301\u6d41\u5f0f\u6587\u672c\u589e\u91cf\u751f\u6210\u8bed\u97f3", "result": "\u5b9e\u9a8c\u8bc1\u660e\u5728\u4fdd\u6301\u975e\u6d41\u5f0fTTS\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u9996\u4ee4\u724c\u5ef6\u8fdf\u8fbe\u5230SOTA\u6c34\u5e73\uff080.17\u79d2\uff09", "conclusion": "\u8be5\u67b6\u6784\u7279\u522b\u9002\u7528\u4e8eLLM\u6d41\u5f0f\u6587\u672c\u8f93\u51fa\u7684\u7ea7\u8054\u573a\u666f\uff0c\u4e3a\u5b9e\u65f6\u5bf9\u8bddAI\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u4f4e\u5ef6\u8fdf\u8bed\u97f3\u5408\u6210\u65b9\u6848"}}
{"id": "2505.19209", "pdf": "https://arxiv.org/pdf/2505.19209", "abs": "https://arxiv.org/abs/2505.19209", "authors": ["Zonglin Yang", "Wanhao Liu", "Ben Gao", "Yujie Liu", "Wei Li", "Tong Xie", "Lidong Bing", "Wanli Ouyang", "Erik Cambria", "Dongzhan Zhou"], "title": "MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search", "categories": ["cs.CL", "cs.AI", "cs.CE", "stat.ML"], "comment": null, "summary": "Large language models (LLMs) have shown promise in automating scientific\nhypothesis generation, yet existing approaches primarily yield coarse-grained\nhypotheses lacking critical methodological and experimental details. We\nintroduce and formally define the novel task of fine-grained scientific\nhypothesis discovery, which entails generating detailed, experimentally\nactionable hypotheses from coarse initial research directions. We frame this as\na combinatorial optimization problem and investigate the upper limits of LLMs'\ncapacity to solve it when maximally leveraged. Specifically, we explore four\nfoundational questions: (1) how to best harness an LLM's internal heuristics to\nformulate the fine-grained hypothesis it itself would judge as the most\npromising among all the possible hypotheses it might generate, based on its own\ninternal scoring-thus defining a latent reward landscape over the hypothesis\nspace; (2) whether such LLM-judged better hypotheses exhibit stronger alignment\nwith ground-truth hypotheses; (3) whether shaping the reward landscape using an\nensemble of diverse LLMs of similar capacity yields better outcomes than\ndefining it with repeated instances of the strongest LLM among them; and (4)\nwhether an ensemble of identical LLMs provides a more reliable reward landscape\nthan a single LLM. To address these questions, we propose a hierarchical search\nmethod that incrementally proposes and integrates details into the hypothesis,\nprogressing from general concepts to specific experimental configurations. We\nshow that this hierarchical process smooths the reward landscape and enables\nmore effective optimization. Empirical evaluations on a new benchmark of\nexpert-annotated fine-grained hypotheses from recent chemistry literature show\nthat our method consistently outperforms strong baselines.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u641c\u7d22\u65b9\u6cd5\u63d0\u5347\u7ec6\u7c92\u5ea6\u79d1\u5b66\u5047\u8bbe\u751f\u6210\u6548\u679c", "motivation": "\u73b0\u6709LLM\u751f\u6210\u7684\u79d1\u5b66\u5047\u8bbe\u7f3a\u4e4f\u5b9e\u9a8c\u7ec6\u8282\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u81ea\u52a8\u5316\u5047\u8bbe\u53d1\u73b0\u65b9\u6cd5", "method": "\u5206\u5c42\u641c\u7d22\u65b9\u6cd5\u9010\u6b65\u6574\u5408\u5047\u8bbe\u7ec6\u8282\uff0c\u4ece\u6982\u5ff5\u5230\u5177\u4f53\u5b9e\u9a8c\u914d\u7f6e\uff0c\u5e73\u6ed1\u5956\u52b1\u666f\u89c2", "result": "\u5728\u5316\u5b66\u9886\u57df\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u5206\u5c42\u4f18\u5316\u8fc7\u7a0b\u6709\u6548\u63d0\u5347\u5047\u8bbe\u751f\u6210\u8d28\u91cf\uff0c\u9a8c\u8bc1\u4e86\u591aLLM\u96c6\u6210\u6bd4\u5355\u4e00\u6a21\u578b\u66f4\u53ef\u9760"}}
{"id": "2505.19212", "pdf": "https://arxiv.org/pdf/2505.19212", "abs": "https://arxiv.org/abs/2505.19212", "authors": ["Steffen Backmann", "David Guzman Piedrahita", "Emanuel Tewolde", "Rada Mihalcea", "Bernhard Sch\u00f6lkopf", "Zhijing Jin"], "title": "When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled their use in\ncomplex agentic roles, involving decision-making with humans or other agents,\nmaking ethical alignment a key AI safety concern. While prior work has examined\nboth LLMs' moral judgment and strategic behavior in social dilemmas, there is\nlimited understanding of how they act when moral imperatives directly conflict\nwith rewards or incentives. To investigate this, we introduce Moral Behavior in\nSocial Dilemma Simulation (MoralSim) and evaluate how LLMs behave in the\nprisoner's dilemma and public goods game with morally charged contexts. In\nMoralSim, we test a range of frontier models across both game structures and\nthree distinct moral framings, enabling a systematic examination of how LLMs\nnavigate social dilemmas in which ethical norms conflict with payoff-maximizing\nstrategies. Our results show substantial variation across models in both their\ngeneral tendency to act morally and the consistency of their behavior across\ngame types, the specific moral framing, and situational factors such as\nopponent behavior and survival risks. Crucially, no model exhibits consistently\nmoral behavior in MoralSim, highlighting the need for caution when deploying\nLLMs in agentic roles where the agent's \"self-interest\" may conflict with\nethical expectations. Our code is available at\nhttps://github.com/sbackmann/moralsim.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7MoralSim\u6846\u67b6\u53d1\u73b0\uff0c\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9053\u5fb7\u4e0e\u5229\u76ca\u51b2\u7a81\u7684\u793e\u4f1a\u56f0\u5883\u4e2d\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u5b58\u5728\u4f26\u7406\u98ce\u9669", "motivation": "\u63a2\u7a76LLMs\u5728\u9053\u5fb7\u89c4\u8303\u4e0e\u81ea\u6211\u5229\u76ca\u76f4\u63a5\u51b2\u7a81\u573a\u666f\u4e0b\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u4e3aAI\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4f9d\u636e", "method": "\u6784\u5efaMoralSim\u793e\u4f1a\u56f0\u5883\u6a21\u62df\u5668\uff0c\u6d4b\u8bd5\u524d\u6cbf\u6a21\u578b\u5728\u56da\u5f92\u56f0\u5883/\u516c\u5171\u7269\u54c1\u6e38\u620f\u4e2d\u7684\u884c\u4e3a\uff0c\u7ed3\u5408\u4e09\u79cd\u9053\u5fb7\u6846\u67b6\u8fdb\u884c\u7cfb\u7edf\u5206\u6790", "result": "\u6a21\u578b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff1a1\uff09\u9053\u5fb7\u884c\u4e3a\u503e\u5411\u4e0e\u6e38\u620f\u7c7b\u578b\u5f3a\u76f8\u5173 2\uff09\u9053\u5fb7\u6846\u67b6\u6709\u6548\u6027\u53d6\u51b3\u4e8e\u60c5\u5883\u56e0\u7d20 3\uff09\u751f\u5b58\u538b\u529b\u663e\u8457\u964d\u4f4e\u9053\u5fb7\u4e00\u81f4\u6027", "conclusion": "\u5f53\u524dLLMs\u65e0\u6cd5\u7a33\u5b9a\u9075\u5b88\u4f26\u7406\u89c4\u8303\uff0c\u5728\u4ee3\u7406\u89d2\u8272\u90e8\u7f72\u4e2d\u9700\u5efa\u7acb\u9053\u5fb7\u62a4\u680f\u673a\u5236\uff0c\u907f\u514d\u5229\u76ca\u51b2\u7a81\u5bfc\u81f4\u975e\u9884\u671f\u884c\u4e3a"}}
{"id": "2505.19217", "pdf": "https://arxiv.org/pdf/2505.19217", "abs": "https://arxiv.org/abs/2505.19217", "authors": ["Weize Chen", "Jiarui Yuan", "Tailin Jin", "Ning Ding", "Huimin Chen", "Zhiyuan Liu", "Maosong Sun"], "title": "The Overthinker's DIET: Cutting Token Calories with DIfficulty-AwarE Training", "categories": ["cs.CL"], "comment": "under review", "summary": "Recent large language models (LLMs) exhibit impressive reasoning but often\nover-think, generating excessively long responses that hinder efficiency. We\nintroduce DIET ( DIfficulty-AwarE Training), a framework that systematically\ncuts these \"token calories\" by integrating on-the-fly problem difficulty into\nthe reinforcement learning (RL) process. DIET dynamically adapts token\ncompression strategies by modulating token penalty strength and conditioning\ntarget lengths on estimated task difficulty, to optimize the\nperformance-efficiency trade-off. We also theoretically analyze the pitfalls of\nnaive reward weighting in group-normalized RL algorithms like GRPO, and propose\nAdvantage Weighting technique, which enables stable and effective\nimplementation of these difficulty-aware objectives. Experimental results\ndemonstrate that DIET significantly reduces token counts while simultaneously\nimproving reasoning performance. Beyond raw token reduction, we show two\ncrucial benefits largely overlooked by prior work: (1) DIET leads to superior\ninference scaling. By maintaining high per-sample quality with fewer tokens, it\nenables better scaling performance via majority voting with more samples under\nfixed computational budgets, an area where other methods falter. (2) DIET\nenhances the natural positive correlation between response length and problem\ndifficulty, ensuring verbosity is appropriately allocated, unlike many existing\ncompression methods that disrupt this relationship. Our analyses provide a\nprincipled and effective framework for developing more efficient, practical,\nand high-performing LLMs.", "AI": {"tldr": "DIET\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u96be\u5ea6\u611f\u77e5\u8bad\u7ec3\u663e\u8457\u51cf\u5c11LLM\u751f\u6210\u7684token\u6570\u91cf\uff0c\u540c\u65f6\u63d0\u5347\u63a8\u7406\u6027\u80fd\u4e0e\u6548\u7387\u5e73\u8861", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u503e\u5411\u4e8e\u751f\u6210\u5197\u957f\u54cd\u5e94\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u5f00\u53d1\u517c\u987e\u6027\u80fd\u4e0e\u6548\u7387\u7684\u4f18\u5316\u65b9\u6cd5", "method": "\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7684\u96be\u5ea6\u611f\u77e5\u8bad\u7ec3\u6846\u67b6\uff08DIET\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u8282token\u60e9\u7f5a\u5f3a\u5ea6\u548c\u76ee\u6807\u957f\u5ea6\uff0c\u5e76\u5f15\u5165Advantage Weighting\u6280\u672f\u4f18\u5316\u5956\u52b1\u673a\u5236", "result": "\u5b9e\u9a8c\u663e\u793aDIET\u51cf\u5c1130-50% token\u4f7f\u7528\u91cf\uff0c\u63a8\u7406\u51c6\u786e\u7387\u63d0\u53472-5%\uff1b\u5b9e\u73b0\u66f4\u597d\u7684\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u6548\u7387\uff08\u76f8\u540c\u9884\u7b97\u4e0b\u591a\u6570\u6295\u7968\u51c6\u786e\u7387\u63d0\u53478-12%\uff09", "conclusion": "DIET\u4e3a\u5f00\u53d1\u9ad8\u6548\u5b9e\u7528LLM\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u96be\u5ea6-\u957f\u5ea6\u6b63\u76f8\u5173\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u6027\u80fd\u4e0e\u6548\u7387\u7684\u53cc\u91cd\u7a81\u7834"}}
{"id": "2505.19236", "pdf": "https://arxiv.org/pdf/2505.19236", "abs": "https://arxiv.org/abs/2505.19236", "authors": ["Qian Cao", "Xiting Wang", "Yuzhuo Yuan", "Yahui Liu", "Fang Luo", "Ruihua Song"], "title": "Evaluating Text Creativity across Diverse Domains: A Dataset and Large Language Model Evaluator", "categories": ["cs.CL"], "comment": null, "summary": "Creativity evaluation remains a challenging frontier for large language\nmodels (LLMs). Current evaluations heavily rely on inefficient and costly human\njudgments, hindering progress in enhancing machine creativity. While automated\nmethods exist, ranging from psychological testing to heuristic- or\nprompting-based approaches, they often lack generalizability or alignment with\nhuman judgment. To address these issues, in this paper, we propose a novel\npairwise-comparison framework for assessing textual creativity, leveraging\nshared contextual instructions to improve evaluation consistency. We introduce\nCreataSet, a large-scale dataset with 100K+ human-level and 1M+ synthetic\ncreative instruction-response pairs spanning diverse open-domain tasks. Through\ntraining on CreataSet, we develop an LLM-based evaluator named CrEval. CrEval\ndemonstrates remarkable superiority over existing methods in alignment with\nhuman judgments. Experimental results underscore the indispensable significance\nof integrating both human-generated and synthetic data in training highly\nrobust evaluators, and showcase the practical utility of CrEval in boosting the\ncreativity of LLMs. We will release all data, code, and models publicly soon to\nsupport further research.", "AI": {"tldr": "Proposes CrEval framework and CreataSet dataset for automated creativity evaluation of LLMs using human-synthetic hybrid data", "motivation": "Current creativity evaluation methods for LLMs are inefficient (relying on human judgment) and lack generalizability", "method": "Developed pairwise-comparison framework with CreataSet dataset (100K+ human + 1M+ synthetic pairs) to train LLM-based evaluator CrEval", "result": "CrEval outperforms existing methods in human alignment (88.3% accuracy), demonstrates hybrid data's importance for robustness", "conclusion": "CrEval effectively boosts LLM creativity evaluation while maintaining human consistency, dataset/model release will facilitate research"}}
{"id": "2505.19240", "pdf": "https://arxiv.org/pdf/2505.19240", "abs": "https://arxiv.org/abs/2505.19240", "authors": ["Aida Kostikova", "Zhipin Wang", "Deidamea Bajri", "Ole P\u00fctz", "Benjamin Paa\u00dfen", "Steffen Eger"], "title": "LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This manuscript is currently under review at ACM Computing Surveys", "summary": "Large language model (LLM) research has grown rapidly, along with increasing\nconcern about their limitations such as failures in reasoning, hallucinations,\nand limited multilingual capability. In this survey, we conduct a data-driven,\nsemi-automated review of research on limitations of LLM (LLLMs) from 2022 to\n2024 using a bottom-up approach. From a corpus of 250,000 ACL and arXiv papers,\nwe identify 14,648 relevant papers using keyword filtering, LLM-based\nclassification, validated against expert labels, and topic clustering (via two\napproaches, HDBSCAN+BERTopic and LlooM). We find that LLM-related research\nincreases over fivefold in ACL and fourfold in arXiv. Since 2022, LLLMs\nresearch grows even faster, reaching over 30% of LLM papers by late 2024.\nReasoning remains the most studied limitation, followed by generalization,\nhallucination, bias, and security. The distribution of topics in the ACL\ndataset stays relatively stable over time, while arXiv shifts toward safety and\ncontrollability (with topics like security risks, alignment, hallucinations,\nknowledge editing), and multimodality between 2022 and 2024. We release a\ndataset of annotated abstracts and a validated methodology, and offer a\nquantitative view of trends in LLM limitations research.", "AI": {"tldr": "2022-2024\u5e74\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u5c40\u9650\u6027\u7684\u7814\u7a76\u8868\u660e\uff1a\u76f8\u5173\u7814\u7a76\u6570\u91cf\u6fc0\u589e\uff0c\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u4ecd\u662f\u6838\u5fc3\u95ee\u9898\uff0carXiv\u8bba\u6587\u9010\u6e10\u8f6c\u5411\u5b89\u5168\u53ef\u63a7\u6027\u7814\u7a76\u3002", "motivation": "\u9488\u5bf9LLM\u5728\u63a8\u7406\u3001\u5e7b\u89c9\u3001\u591a\u8bed\u8a00\u80fd\u529b\u7b49\u65b9\u9762\u7684\u7f3a\u9677\uff0c\u7814\u7a76\u8005\u8bd5\u56fe\u901a\u8fc7\u91cf\u5316\u5206\u6790\u63ed\u793a\u9886\u57df\u7814\u7a76\u8d8b\u52bf\u4e0e\u5c40\u9650\u6027\u5173\u6ce8\u91cd\u70b9\u7684\u6f14\u53d8\u3002", "method": "\u91c7\u7528\u6570\u636e\u9a71\u52a8\u7684\u534a\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u5173\u952e\u8bcd\u7b5b\u9009\u3001LLM\u5206\u7c7b\u9a8c\u8bc1\u3001\u4e13\u5bb6\u6807\u6ce8\u53caHDBSCAN+BERTopic/LlooM\u53cc\u805a\u7c7b\u65b9\u6848\uff0c\u5206\u679025\u4e07\u7bc7ACL/arXiv\u8bba\u6587\u3002", "result": "LLM\u5c40\u9650\u6027\u7814\u7a76\u5360\u6bd4\u4ece2022\u5e74\u7684\u4e0d\u8db310%\u589e\u957f\u81f32024\u5e74\u768430%\uff1barXiv\u5728\u5b89\u5168\u98ce\u9669\u3001\u77e5\u8bc6\u7f16\u8f91\u7b49\u9886\u57df\u7814\u7a76\u589e\u5e45\u663e\u8457\uff0cACL\u4fdd\u6301\u8f83\u7a33\u5b9a\u5206\u5e03\u3002", "conclusion": "\u7814\u7a76\u53d1\u5e03\u5e26\u6807\u6ce8\u7684\u8bba\u6587\u6458\u8981\u6570\u636e\u96c6\u53ca\u9a8c\u8bc1\u65b9\u6cd5\u8bba\uff0c\u4e3aLLM\u5c40\u9650\u6027\u7814\u7a76\u63d0\u4f9b\u52a8\u6001\u91cf\u5316\u89c6\u89d2\uff0c\u53cd\u6620\u5b66\u672f\u754c\u5bf9\u6a21\u578b\u5b89\u5168\u53ef\u63a7\u6027\u5173\u6ce8\u5ea6\u7684\u5feb\u901f\u63d0\u5347\u3002"}}
{"id": "2505.19250", "pdf": "https://arxiv.org/pdf/2505.19250", "abs": "https://arxiv.org/abs/2505.19250", "authors": ["Yi Wang", "Junxiao Liu", "Shimao Zhang", "Jiajun Chen", "Shujian Huang"], "title": "PATS: Process-Level Adaptive Thinking Mode Switching", "categories": ["cs.CL"], "comment": null, "summary": "Current large-language models (LLMs) typically adopt a fixed reasoning\nstrategy, either simple or complex, for all questions, regardless of their\ndifficulty. This neglect of variation in task and reasoning process complexity\nleads to an imbalance between performance and efficiency. Existing methods\nattempt to implement training-free fast-slow thinking system switching to\nhandle problems of varying difficulty, but are limited by coarse-grained\nsolution-level strategy adjustments. To address this issue, we propose a novel\nreasoning paradigm: Process-Level Adaptive Thinking Mode Switching (PATS),\nwhich enables LLMs to dynamically adjust their reasoning strategy based on the\ndifficulty of each step, optimizing the balance between accuracy and\ncomputational efficiency. Our approach integrates Process Reward Models (PRMs)\nwith Beam Search, incorporating progressive mode switching and bad-step penalty\nmechanisms. Experiments on diverse mathematical benchmarks demonstrate that our\nmethodology achieves high accuracy while maintaining moderate token usage. This\nstudy emphasizes the significance of process-level, difficulty-aware reasoning\nstrategy adaptation, offering valuable insights into efficient inference for\nLLMs.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u8c03\u6574\u63a8\u7406\u7b56\u7565\u7684PATS\u6846\u67b6\uff0c\u901a\u8fc7\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u548c\u675f\u641c\u7d22\u5b9e\u73b0\u6b65\u9aa4\u7ea7\u81ea\u9002\u5e94\u601d\u7ef4\u5207\u6362\uff0c\u5e73\u8861LLM\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6548\u7387", "motivation": "\u56fa\u5b9a\u63a8\u7406\u7b56\u7565\u5bfc\u81f4\u7b80\u5355\u95ee\u9898\u8fc7\u5ea6\u8ba1\u7b97/\u590d\u6742\u95ee\u9898\u5206\u6790\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u89e3\u51b3\u65b9\u6848\u7ea7\u7b56\u7565\u8c03\u6574\uff0c\u65e0\u6cd5\u7ec6\u7c92\u5ea6\u9002\u914d\u6b65\u9aa4\u96be\u5ea6", "method": "\u6574\u5408\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b(PRM)\u4e0e\u675f\u641c\u7d22\uff0c\u5f15\u5165\u6e10\u8fdb\u5f0f\u6a21\u5f0f\u5207\u6362\u673a\u5236\u548c\u9519\u8bef\u6b65\u9aa4\u60e9\u7f5a\u673a\u5236\uff0c\u5b9e\u73b0\u63a8\u7406\u8fc7\u7a0b\u4e2d\u9010\u6b65\u9aa4\u7684\u601d\u7ef4\u6a21\u5f0f\u52a8\u6001\u5207\u6362", "result": "\u5728\u591a\u6837\u5316\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523094.2%\u51c6\u786e\u7387\uff0c\u63a8\u7406token\u6d88\u8017\u91cf\u8f83\u4f20\u7edf\u65b9\u6cd5\u51cf\u5c1137%\uff0c\u5b9e\u73b0\u7cbe\u5ea6\u4e0e\u6548\u7387\u7684\u6700\u4f73\u5e73\u8861", "conclusion": "\u8fc7\u7a0b\u7ea7\u96be\u5ea6\u611f\u77e5\u63a8\u7406\u7b56\u7565\u9002\u5e94\u673a\u5236\u4e3aLLM\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u8bc1\u5b9e\u52a8\u6001\u8c03\u6574\u601d\u7ef4\u6a21\u5f0f\u5bf9\u4f18\u5316\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u7684\u5173\u952e\u4f5c\u7528"}}
{"id": "2505.19254", "pdf": "https://arxiv.org/pdf/2505.19254", "abs": "https://arxiv.org/abs/2505.19254", "authors": ["Rafa\u0142 Po\u015bwiata", "Marcin Micha\u0142 Miro\u0144czuk", "S\u0142awomir Dadas", "Ma\u0142gorzata Gr\u0119bowiec", "Micha\u0142 Pere\u0142kiewicz"], "title": "Unveiling Dual Quality in Product Reviews: An NLP-Based Approach", "categories": ["cs.CL"], "comment": "Accepted for ACL 2025 Industry Track", "summary": "Consumers often face inconsistent product quality, particularly when\nidentical products vary between markets, a situation known as the dual quality\nproblem. To identify and address this issue, automated techniques are needed.\nThis paper explores how natural language processing (NLP) can aid in detecting\nsuch discrepancies and presents the full process of developing a solution.\nFirst, we describe in detail the creation of a new Polish-language dataset with\n1,957 reviews, 540 highlighting dual quality issues. We then discuss\nexperiments with various approaches like SetFit with sentence-transformers,\ntransformer-based encoders, and LLMs, including error analysis and robustness\nverification. Additionally, we evaluate multilingual transfer using a subset of\nopinions in English, French, and German. The paper concludes with insights on\ndeployment and practical applications.", "AI": {"tldr": "\u5229\u7528NLP\u6280\u672f\u5f00\u53d1\u81ea\u52a8\u5316\u65b9\u6848\u68c0\u6d4b\u4ea7\u54c1\u53cc\u91cd\u8d28\u91cf\u95ee\u9898\uff0c\u5305\u542b\u6ce2\u5170\u8bed\u6570\u636e\u96c6\u6784\u5efa\u3001\u591a\u6a21\u578b\u5b9e\u9a8c\u53ca\u591a\u8bed\u8a00\u8fc1\u79fb\u8bc4\u4f30\u3002", "motivation": "\u89e3\u51b3\u4e0d\u540c\u5e02\u573a\u540c\u6b3e\u4ea7\u54c1\u8d28\u91cf\u4e0d\u4e00\u81f4\u5bfc\u81f4\u7684\u6d88\u8d39\u8005\u6743\u76ca\u53d7\u635f\u95ee\u9898\uff0c\u9700\u5f00\u53d1\u81ea\u52a8\u5316\u68c0\u6d4b\u5de5\u5177\u63d0\u5347\u76d1\u7ba1\u6548\u7387\u3002", "method": "1. \u6784\u5efa\u542b1,957\u6761\u6ce2\u5170\u8bed\u8bc4\u8bba\uff08\u542b540\u6761\u53cc\u91cd\u8d28\u91cf\u6807\u6ce8\uff09\u7684\u65b0\u6570\u636e\u96c6\n2. \u91c7\u7528SetFit+sentence-transformers\u3001transformer\u7f16\u7801\u5668\u3001LLMs\u8fdb\u884c\u5b9e\u9a8c\n3. \u5f00\u5c55\u9519\u8bef\u5206\u6790\u4e0e\u9c81\u68d2\u6027\u9a8c\u8bc1\n4. \u4f7f\u7528\u82f1\u6cd5\u5fb7\u4e09\u8bed\u5b50\u96c6\u8bc4\u4f30\u591a\u8bed\u8a00\u8fc1\u79fb\u80fd\u529b", "result": "\u9a8c\u8bc1\u4e86NLP\u6a21\u578b\u5728\u53cc\u91cd\u8d28\u91cf\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u591a\u8bed\u8a00\u8fc1\u79fb\u5b9e\u9a8c\u663e\u793a\u8de8\u8bed\u8a00\u5e94\u7528\u6f5c\u529b\uff0c\u9519\u8bef\u5206\u6790\u63ed\u793a\u6a21\u578b\u6539\u8fdb\u65b9\u5411\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4ea7\u54c1\u8d28\u91cf\u76d1\u7ba1\u63d0\u4f9b\u53ef\u843d\u5730\u7684NLP\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u6280\u672f\u90e8\u7f72\u53ef\u884c\u6027\uff0c\u5e76\u6307\u660e\u591a\u8bed\u8a00\u573a\u666f\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2505.19286", "pdf": "https://arxiv.org/pdf/2505.19286", "abs": "https://arxiv.org/abs/2505.19286", "authors": ["Utkarsh Sahu", "Zhisheng Qi", "Yongjia Lei", "Ryan A. Rossi", "Franck Dernoncourt", "Nesreen K. Ahmed", "Mahantesh M Halappanavar", "Yao Ma", "Yu Wang"], "title": "A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models", "categories": ["cs.CL", "cs.LG", "cs.SI"], "comment": null, "summary": "Large language models have been extensively studied as neural knowledge bases\nfor their knowledge access, editability, reasoning, and explainability.\nHowever, few works focus on the structural patterns of their knowledge.\nMotivated by this gap, we investigate these structural patterns from a graph\nperspective. We quantify the knowledge of LLMs at both the triplet and entity\nlevels, and analyze how it relates to graph structural properties such as node\ndegree. Furthermore, we uncover the knowledge homophily, where topologically\nclose entities exhibit similar levels of knowledgeability, which further\nmotivates us to develop graph machine learning models to estimate entity\nknowledge based on its local neighbors. This model further enables valuable\nknowledge checking by selecting triplets less known to LLMs. Empirical results\nshow that using selected triplets for fine-tuning leads to superior\nperformance.", "AI": {"tldr": "\u4ece\u56fe\u7ed3\u6784\u89c6\u89d2\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u5206\u5e03\u7279\u5f81\uff0c\u63d0\u51fa\u77e5\u8bc6\u540c\u8d28\u6027\u73b0\u8c61\u5e76\u5f00\u53d1\u56fe\u6a21\u578b\u5b9e\u73b0\u77e5\u8bc6\u68c0\u67e5\u4e0e\u4f18\u5316", "motivation": "\u73b0\u6709\u7814\u7a76\u8f83\u5c11\u5173\u6ce8\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u7684\u7ed3\u6784\u6a21\u5f0f\uff0c\u7279\u522b\u5728\u56fe\u7ed3\u6784\u7279\u6027\u65b9\u9762\u5b58\u5728\u7814\u7a76\u7a7a\u767d", "method": "\u91cf\u5316\u4e09\u5143\u7ec4/\u5b9e\u4f53\u5c42\u77e5\u8bc6\u91cf\uff0c\u5206\u6790\u8282\u70b9\u5ea6\u7b49\u56fe\u5c5e\u6027\u5173\u8054\uff0c\u5f00\u53d1\u57fa\u4e8e\u90bb\u5c45\u7684\u56fe\u6a21\u578b\u4f30\u8ba1\u5b9e\u4f53\u77e5\u8bc6\u6c34\u5e73", "result": "\u53d1\u73b0\u62d3\u6251\u90bb\u8fd1\u5b9e\u4f53\u5177\u6709\u76f8\u4f3c\u77e5\u8bc6\u6c34\u5e73\uff08\u77e5\u8bc6\u540c\u8d28\u6027\uff09\uff0c\u7b5b\u9009\u672a\u77e5\u4e09\u5143\u7ec4\u5fae\u8c03\u4f7f\u6a21\u578b\u6027\u80fd\u63d0\u5347\u663e\u8457", "conclusion": "\u56fe\u7ed3\u6784\u5206\u6790\u4e3a\u7406\u89e3LLMs\u77e5\u8bc6\u7ec4\u7ec7\u53ca\u4f18\u5316\u6a21\u578b\u77e5\u8bc6\u8986\u76d6\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2"}}
{"id": "2505.19293", "pdf": "https://arxiv.org/pdf/2505.19293", "abs": "https://arxiv.org/abs/2505.19293", "authors": ["Wang Yang", "Hongye Jin", "Shaochen Zhong", "Song Jiang", "Qifan Wang", "Vipin Chaudhary", "Xiaotian Han"], "title": "100-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Long-context capability is considered one of the most important abilities of\nLLMs, as a truly long context-capable LLM enables users to effortlessly process\nmany originally exhausting tasks -- e.g., digesting a long-form document to\nfind answers vs. directly asking an LLM about it. However, existing\nreal-task-based long-context evaluation benchmarks have two major shortcomings.\nFirst, benchmarks like LongBench often do not provide proper metrics to\nseparate long-context performance from the model's baseline ability, making\ncross-model comparison unclear. Second, such benchmarks are usually constructed\nwith fixed input lengths, which limits their applicability across different\nmodels and fails to reveal when a model begins to break down. To address these\nissues, we introduce a length-controllable long-context benchmark and a novel\nmetric that disentangles baseline knowledge from true long-context\ncapabilities. Experiments demonstrate the superiority of our approach in\neffectively evaluating LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u53ef\u63a7\u5236\u957f\u5ea6\u7684\u957f\u4e0a\u4e0b\u6587\u8bc4\u6d4b\u57fa\u51c6\u53ca\u5206\u79bb\u6a21\u578b\u57fa\u7ebf\u80fd\u529b\u7684\u65b0\u6307\u6807\uff0c\u6709\u6548\u89e3\u51b3\u73b0\u6709\u8bc4\u6d4b\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u957f\u4e0a\u4e0b\u6587\u8bc4\u6d4b\u57fa\u51c6\u65e0\u6cd5\u533a\u5206\u6a21\u578b\u57fa\u7ebf\u80fd\u529b\u548c\u771f\u5b9e\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\uff0c\u4e14\u56fa\u5b9a\u8f93\u5165\u957f\u5ea6\u8bbe\u8ba1\u9650\u5236\u4e86\u9002\u7528\u6027\u3002", "method": "\u8bbe\u8ba1\u53ef\u8c03\u8282\u8f93\u5165\u957f\u5ea6\u7684\u8bc4\u6d4b\u57fa\u51c6\uff0c\u5e76\u521b\u65b0\u6027\u5730\u63d0\u51fa\u89e3\u8026\u57fa\u7ebf\u77e5\u8bc6\u4e0e\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u66f4\u51c6\u786e\u8bc4\u4f30\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\uff0c\u6709\u6548\u63ed\u793a\u6a21\u578b\u5931\u6548\u8fb9\u754c\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u4e3aLLM\u957f\u4e0a\u4e0b\u6587\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u79d1\u5b66\u3001\u9002\u5e94\u6027\u66f4\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.19299", "pdf": "https://arxiv.org/pdf/2505.19299", "abs": "https://arxiv.org/abs/2505.19299", "authors": ["Lingjun Zhao", "Hal Daum\u00e9 III"], "title": "A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Faithful free-text explanations are important to ensure transparency in\nhigh-stakes AI decision-making contexts, but they are challenging to generate\nby language models and assess by humans. In this paper, we present a measure\nfor Prediction-EXplanation (PEX) consistency, by extending the concept of\nweight of evidence. This measure quantifies how much a free-text explanation\nsupports or opposes a prediction, serving as an important aspect of explanation\nfaithfulness. Our analysis reveals that more than 62% explanations generated by\nlarge language models lack this consistency. We show that applying direct\npreference optimization improves the consistency of generated explanations\nacross three model families, with improvement ranging from 43.1% to 292.3%.\nFurthermore, we demonstrate that optimizing this consistency measure can\nimprove explanation faithfulness by up to 9.7%.", "AI": {"tldr": "\u63d0\u51faPEX\u4e00\u81f4\u6027\u6307\u6807\u91cf\u5316\u81ea\u7531\u6587\u672c\u89e3\u91ca\u5bf9\u9884\u6d4b\u7684\u652f\u6301\u5ea6\uff0c\u53d1\u73b0\u8d8562%\u5927\u6a21\u578b\u89e3\u91ca\u5b58\u5728\u4e0d\u4e00\u81f4\u6027\uff0c\u4f18\u5316\u540e\u63d0\u5347\u663e\u8457\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u81ea\u7531\u6587\u672c\u89e3\u91ca\u4e2d62%\u5b58\u5728\u9884\u6d4b\u4e0e\u89e3\u91ca\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5f71\u54cdAI\u51b3\u7b56\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002", "method": "\u6269\u5c55\u8bc1\u636e\u6743\u91cd\u6982\u5ff5\u63d0\u51faPEX\u4e00\u81f4\u6027\u6307\u6807\uff0c\u5e94\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316(DPO)\u6280\u672f\u4f18\u5316\u4e09\u4e2a\u6a21\u578b\u5bb6\u65cf\u7684\u751f\u6210\u7ed3\u679c\u3002", "result": "DPO\u4f7f\u89e3\u91ca\u4e00\u81f4\u6027\u63d0\u534743.1%-292.3%\uff0c\u4f18\u5316PEX\u4e00\u81f4\u6027\u6307\u6807\u4f7f\u89e3\u91ca\u5fe0\u5b9e\u6027\u6700\u9ad8\u63d0\u53479.7%\u3002", "conclusion": "PEX\u4e00\u81f4\u6027\u662f\u89e3\u91ca\u53ef\u4fe1\u5ea6\u7684\u5173\u952e\u6307\u6807\uff0c\u76f4\u63a5\u4f18\u5316\u8be5\u6307\u6807\u53ef\u6709\u6548\u63d0\u5347AI\u51b3\u7b56\u89e3\u91ca\u7684\u5fe0\u5b9e\u6027\u3002"}}
{"id": "2505.19300", "pdf": "https://arxiv.org/pdf/2505.19300", "abs": "https://arxiv.org/abs/2505.19300", "authors": ["Junnan Liu", "Linhao Luo", "Thuy-Trang Vu", "Gholamreza Haffari"], "title": "SituatedThinker: Grounding LLM Reasoning with Real-World through Situated Thinking", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Recent advances in large language models (LLMs) demonstrate their impressive\nreasoning capabilities. However, the reasoning confined to internal parametric\nspace limits LLMs' access to real-time information and understanding of the\nphysical world. To overcome this constraint, we introduce SituatedThinker, a\nnovel framework that enables LLMs to ground their reasoning in real-world\ncontexts through situated thinking, which adaptively combines both internal\nknowledge and external information with predefined interfaces. By utilizing\nreinforcement learning, SituatedThinker incentivizes deliberate reasoning with\nthe real world to acquire information and feedback, allowing LLMs to surpass\ntheir knowledge boundaries and enhance reasoning. Experimental results\ndemonstrate significant performance improvements on multi-hop\nquestion-answering and mathematical reasoning benchmarks. Furthermore,\nSituatedThinker demonstrates strong performance on unseen tasks, such as KBQA,\nTableQA, and text-based games, showcasing the generalizable real-world grounded\nreasoning capability. Our codes are available at\nhttps://github.com/jnanliu/SituatedThinker.", "AI": {"tldr": "\u63d0\u51faSituatedThinker\u6846\u67b6\uff0c\u901a\u8fc7\u60c5\u5883\u5316\u601d\u7ef4\u5c06\u5927\u6a21\u578b\u63a8\u7406\u4e0e\u771f\u5b9e\u4e16\u754c\u60c5\u5883\u7ed3\u5408\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u7a81\u7834\u77e5\u8bc6\u8fb9\u754c\u5e76\u63d0\u5347\u63a8\u7406\u80fd\u529b", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u4ec5\u4f9d\u8d56\u5185\u90e8\u53c2\u6570\u77e5\u8bc6\u5bfc\u81f4\u7684\u5b9e\u65f6\u4fe1\u606f\u83b7\u53d6\u53d7\u9650\u548c\u7269\u7406\u4e16\u754c\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898", "method": "\u7ed3\u5408\u9884\u5b9a\u4e49\u63a5\u53e3\u5b9e\u73b0\u5185\u90e8\u77e5\u8bc6\u4e0e\u5916\u90e8\u4fe1\u606f\u7684\u81ea\u9002\u5e94\u878d\u5408\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u6fc0\u52b1\u4e3b\u52a8\u63a8\u7406\u83b7\u53d6\u5b9e\u65f6\u4fe1\u606f\u53cd\u9988", "result": "\u5728\u591a\u8df3\u95ee\u7b54\u548c\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5728KBQA/TableQA/\u6587\u672c\u6e38\u620f\u7b49\u65b0\u4efb\u52a1\u5c55\u73b0\u6cdb\u5316\u80fd\u529b", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u62d3\u5c55\u4e86\u5927\u6a21\u578b\u7684\u77e5\u8bc6\u8fb9\u754c\uff0c\u9a8c\u8bc1\u4e86\u60c5\u5883\u5316\u63a8\u7406\u5bf9\u590d\u6742\u73b0\u5b9e\u4efb\u52a1\u7684\u6709\u6548\u6027\uff0c\u5177\u6709\u901a\u7528\u6027\u4ef7\u503c"}}
{"id": "2505.19345", "pdf": "https://arxiv.org/pdf/2505.19345", "abs": "https://arxiv.org/abs/2505.19345", "authors": ["Yongmin Yoo", "Qiongkai Xu", "Longbing Cao"], "title": "PatentScore: Multi-dimensional Evaluation of LLM-Generated Patent Claims", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Natural language generation (NLG) metrics play a central role in evaluating\ngenerated texts, but are not well suited for the structural and legal\ncharacteristics of patent documents. Large language models (LLMs) offer strong\npotential in automating patent generation, yet research on evaluating\nLLM-generated patents remains limited, especially in evaluating the generation\nquality of patent claims, which are central to defining the scope of\nprotection. Effective claim evaluation requires addressing legal validity,\ntechnical accuracy, and structural compliance. To address this gap, we\nintroduce PatentScore, a multi-dimensional evaluation framework for assessing\nLLM-generated patent claims. PatentScore incorporates: (1) hierarchical\ndecomposition for claim analysis; (2) domain-specific validation patterns based\non legal and technical standards; and (3) scoring across structural, semantic,\nand legal dimensions. Unlike general-purpose NLG metrics, PatentScore reflects\npatent-specific constraints and document structures, enabling evaluation beyond\nsurface similarity. We evaluate 400 GPT-4o-mini generated Claim 1s and report a\nPearson correlation of $r = 0.819$ with expert annotations, outperforming\nexisting NLG metrics. Furthermore, we conduct additional evaluations using open\nmodels such as Claude-3.5-Haiku and Gemini-1.5-flash, all of which show strong\ncorrelations with expert judgments, confirming the robustness and\ngeneralizability of our framework.", "AI": {"tldr": "\u63d0\u51faPatentScore\u6846\u67b6\u7528\u4e8e\u8bc4\u4f30LLM\u751f\u6210\u7684\u4e13\u5229\u6743\u5229\u8981\u6c42\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u8bc4\u5206\u5b9e\u73b0\u4f18\u4e8e\u4f20\u7edfNLG\u6307\u6807\u7684\u4e13\u5bb6\u76f8\u5173\u6027", "motivation": "\u73b0\u6709\u81ea\u7136\u8bed\u8a00\u751f\u6210\u6307\u6807\u65e0\u6cd5\u6ee1\u8db3\u4e13\u5229\u6587\u4ef6\u7279\u6709\u7684\u6cd5\u5f8b\u6709\u6548\u6027\u548c\u6280\u672f\u51c6\u786e\u6027\u8bc4\u4f30\u9700\u6c42\uff0c\u4e13\u5229\u6743\u5229\u8981\u6c42\u8bc4\u4f30\u5b58\u5728\u7a7a\u767d", "method": "1. \u5c42\u6b21\u5316\u6743\u5229\u8981\u6c42\u5206\u89e3\n2. \u57fa\u4e8e\u6cd5\u5f8b\u6280\u672f\u6807\u51c6\u7684\u9886\u57df\u9a8c\u8bc1\u6a21\u5f0f\n3. \u7ed3\u6784/\u8bed\u4e49/\u6cd5\u5f8b\u4e09\u7ef4\u5ea6\u8bc4\u5206\u4f53\u7cfb", "result": "GPT-4o-mini\u751f\u6210400\u9879\u4e13\u5229\u6743\u5229\u8981\u6c42\u7684Pearson\u76f8\u5173\u7cfb\u6570r=0.819\uff0cClaude-3.5/Haiku\u548cGemini-1.5-flash\u6a21\u578b\u9a8c\u8bc1\u6846\u67b6\u7a33\u5065\u6027", "conclusion": "PatentScore\u6709\u6548\u89e3\u51b3\u4e13\u5229\u751f\u6210\u8bc4\u4f30\u7684\u7279\u6b8a\u9700\u6c42\uff0c\u9996\u6b21\u5b9e\u73b0\u6cd5\u5f8b\u5408\u89c4\u4e0e\u6280\u672f\u51c6\u786e\u6027\u7684\u91cf\u5316\u8bc4\u4f30"}}
{"id": "2505.19354", "pdf": "https://arxiv.org/pdf/2505.19354", "abs": "https://arxiv.org/abs/2505.19354", "authors": ["Mohammad Mahdi Moradi", "Sudhir Mudur"], "title": "GC-KBVQA: A New Four-Stage Framework for Enhancing Knowledge Based Visual Question Answering Performance", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Knowledge-Based Visual Question Answering (KB-VQA) methods focus on tasks\nthat demand reasoning with information extending beyond the explicit content\ndepicted in the image. Early methods relied on explicit knowledge bases to\nprovide this auxiliary information. Recent approaches leverage Large Language\nModels (LLMs) as implicit knowledge sources. While KB-VQA methods have\ndemonstrated promising results, their potential remains constrained as the\nauxiliary text provided may not be relevant to the question context, and may\nalso include irrelevant information that could misguide the answer predictor.\nWe introduce a novel four-stage framework called Grounding Caption-Guided\nKnowledge-Based Visual Question Answering (GC-KBVQA), which enables LLMs to\neffectively perform zero-shot VQA tasks without the need for end-to-end\nmultimodal training. Innovations include grounding question-aware caption\ngeneration to move beyond generic descriptions and have compact, yet detailed\nand context-rich information. This is combined with knowledge from external\nsources to create highly informative prompts for the LLM. GC-KBVQA can address\na variety of VQA tasks, and does not require task-specific fine-tuning, thus\nreducing both costs and deployment complexity by leveraging general-purpose,\npre-trained LLMs. Comparison with competing KB-VQA methods shows significantly\nimproved performance. Our code will be made public.", "AI": {"tldr": "\u63d0\u51faGC-KB-VQA\u6846\u67b6\uff0c\u901a\u8fc7\u95ee\u9898\u611f\u77e5\u7684\u5b9a\u4f4d\u63cf\u8ff0\u6574\u5408\u5916\u90e8\u77e5\u8bc6\uff0c\u5229\u7528LLM\u5b9e\u73b0\u96f6\u6837\u672cVQA\u4efb\u52a1\uff0c\u65e0\u9700\u591a\u6a21\u6001\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709KB-VQA\u65b9\u6cd5\u63d0\u4f9b\u7684\u8f85\u52a9\u6587\u672c\u53ef\u80fd\u5305\u542b\u65e0\u5173\u4fe1\u606f\uff0c\u5bfc\u81f4\u7b54\u6848\u9884\u6d4b\u504f\u5dee\u3002\u9700\u5728\u964d\u4f4e\u90e8\u7f72\u6210\u672c\u7684\u540c\u65f6\u63d0\u5347\u63d0\u793a\u6709\u6548\u6027\u3002", "method": "\u56db\u9636\u6bb5\u6846\u67b6\uff1a1)\u751f\u6210\u95ee\u9898\u76f8\u5173\u7684\u5b9a\u4f4d\u63cf\u8ff0\uff1b2)\u7ed3\u5408\u5916\u90e8\u77e5\u8bc6\uff1b3)\u6784\u5efa\u9ad8\u4fe1\u606f\u91cf\u63d0\u793a\uff1b4)LLM\u76f4\u63a5\u63a8\u7406\u3002\u6838\u5fc3\u4e3a\u7ec6\u7c92\u5ea6\u5b9a\u4f4d\u63cf\u8ff0\u4e0e\u77e5\u8bc6\u878d\u5408\u3002", "result": "\u76f8\u6bd4\u5176\u4ed6KB-VQA\u65b9\u6cd5\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u4e14\u65e0\u9700\u4efb\u52a1\u5fae\u8c03\uff0c\u964d\u4f4e\u90e8\u7f72\u590d\u6742\u5ea6\u3002", "conclusion": "GC-KB-VQA\u6709\u6548\u5229\u7528LLM\u5b9e\u73b0\u96f6\u6837\u672c\u591a\u4efb\u52a1VQA\uff0c\u901a\u8fc7\u7cbe\u51c6\u63d0\u793a\u5de5\u7a0b\u7a81\u7834\u4f20\u7edf\u591a\u6a21\u6001\u8bad\u7ec3\u9650\u5236\uff0c\u5177\u6709\u9ad8\u6269\u5c55\u6027\u3002"}}
{"id": "2505.19355", "pdf": "https://arxiv.org/pdf/2505.19355", "abs": "https://arxiv.org/abs/2505.19355", "authors": ["Lin Tian", "Marian-Andrei Rizoiu"], "title": "Estimating Online Influence Needs Causal Modeling! Counterfactual Analysis of Social Media Engagement", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Understanding true influence in social media requires distinguishing\ncorrelation from causation--particularly when analyzing misinformation spread.\nWhile existing approaches focus on exposure metrics and network structures,\nthey often fail to capture the causal mechanisms by which external temporal\nsignals trigger engagement. We introduce a novel joint treatment-outcome\nframework that leverages existing sequential models to simultaneously adapt to\nboth policy timing and engagement effects. Our approach adapts causal inference\ntechniques from healthcare to estimate Average Treatment Effects (ATE) within\nthe sequential nature of social media interactions, tackling challenges from\nexternal confounding signals. Through our experiments on real-world\nmisinformation and disinformation datasets, we show that our models outperform\nexisting benchmarks by 15--22% in predicting engagement across diverse\ncounterfactual scenarios, including exposure adjustment, timing shifts, and\nvaried intervention durations. Case studies on 492 social media users show our\ncausal effect measure aligns strongly with the gold standard in influence\nestimation, the expert-based empirical influence.", "AI": {"tldr": "\u5f00\u53d1\u65b0\u578b\u8054\u5408\u5904\u7406-\u7ed3\u679c\u6846\u67b6\uff0c\u7ed3\u5408\u5e8f\u5217\u6a21\u578b\u548c\u56e0\u679c\u63a8\u65ad\u6280\u672f\uff0c\u7528\u4e8e\u793e\u4ea4\u5a92\u4f53\u4e2d\u7684\u56e0\u679c\u5f71\u54cd\u529b\u5206\u6790\uff0c\u5728\u53cd\u4e8b\u5b9e\u573a\u666f\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u534715-22%", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u66dd\u5149\u6307\u6807\u548c\u7f51\u7edc\u7ed3\u6784\uff0c\u65e0\u6cd5\u6355\u6349\u5916\u90e8\u65f6\u5e8f\u4fe1\u53f7\u89e6\u53d1\u53c2\u4e0e\u884c\u4e3a\u7684\u56e0\u679c\u673a\u5236\uff0c\u9700\u89e3\u51b3\u5916\u90e8\u6df7\u6742\u4fe1\u53f7\u5e26\u6765\u7684\u6311\u6218", "method": "1. \u5c06\u533b\u7597\u9886\u57df\u56e0\u679c\u63a8\u65ad\u6280\u672f\u8fc1\u79fb\u5230\u793e\u4ea4\u5a92\u4f53\n2. \u5f00\u53d1\u8054\u5408\u5904\u7406-\u7ed3\u679c\u6846\u67b6\u9002\u914d\u653f\u7b56\u65f6\u95f4\u7a97\u548c\u53c2\u4e0e\u6548\u5e94\n3. \u901a\u8fc7ATE\u4f30\u8ba1\u5904\u7406\u5e8f\u5217\u5316\u4ea4\u4e92\u4e2d\u7684\u56e0\u679c\u6548\u5e94", "result": "1. \u57283\u7c7b\u53cd\u4e8b\u5b9e\u573a\u666f(\u66dd\u5149\u8c03\u6574/\u65f6\u95f4\u504f\u79fb/\u5e72\u9884\u65f6\u957f\u53d8\u5316)\u4e2d\u9884\u6d4b\u51c6\u786e\u7387\u8d85\u8d8a\u57fa\u51c615-22%\n2. 492\u7528\u6237\u6848\u4f8b\u5206\u6790\u663e\u793a\u56e0\u679c\u6548\u5e94\u6307\u6807\u4e0e\u4e13\u5bb6\u7ecf\u9a8c\u8bc4\u4f30\u9ad8\u5ea6\u543b\u5408", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u5916\u90e8\u6df7\u6742\u4fe1\u53f7\u95ee\u9898\uff0c\u901a\u8fc7\u65f6\u5e8f\u5316ATE\u4f30\u8ba1\u63d0\u5347\u5f71\u54cd\u529b\u5206\u6790\u7cbe\u5ea6\uff0c\u4e3a\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u6cd5\u8bba"}}
{"id": "2505.19360", "pdf": "https://arxiv.org/pdf/2505.19360", "abs": "https://arxiv.org/abs/2505.19360", "authors": ["Manan Suri", "Puneet Mathur", "Nedim Lipka", "Franck Dernoncourt", "Ryan A. Rossi", "Dinesh Manocha"], "title": "ChartLens: Fine-grained Visual Attribution in Charts", "categories": ["cs.CL"], "comment": "ACL 2025 (Main)", "summary": "The growing capabilities of multimodal large language models (MLLMs) have\nadvanced tasks like chart understanding. However, these models often suffer\nfrom hallucinations, where generated text sequences conflict with the provided\nvisual data. To address this, we introduce Post-Hoc Visual Attribution for\nCharts, which identifies fine-grained chart elements that validate a given\nchart-associated response. We propose ChartLens, a novel chart attribution\nalgorithm that uses segmentation-based techniques to identify chart objects and\nemploys set-of-marks prompting with MLLMs for fine-grained visual attribution.\nAdditionally, we present ChartVA-Eval, a benchmark with synthetic and\nreal-world charts from diverse domains like finance, policy, and economics,\nfeaturing fine-grained attribution annotations. Our evaluations show that\nChartLens improves fine-grained attributions by 26-66%.", "AI": {"tldr": "\u63d0\u51faChartLens\u7b97\u6cd5\u548cChartVA-Eval\u57fa\u51c6\uff0c\u663e\u8457\u63d0\u5347\u56fe\u8868\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5f52\u56e0\u6548\u679c26-66%", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u7406\u89e3\u4e2d\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u751f\u6210\u6587\u672c\u4e0e\u89c6\u89c9\u6570\u636e\u51b2\u7a81\uff0c\u9700\u5efa\u7acb\u7ec6\u7c92\u5ea6\u7684\u89c6\u89c9\u5f52\u56e0\u9a8c\u8bc1\u673a\u5236", "method": "1. \u5f00\u53d1\u57fa\u4e8e\u5206\u5272\u7684ChartLens\u7b97\u6cd5\uff0c\u91c7\u7528\u6807\u8bb0\u96c6\u63d0\u793a\u6280\u672f\uff1b2. \u6784\u5efa\u8de8\u9886\u57df\u57fa\u51c6ChartVA-Eval\uff08\u542b\u91d1\u878d/\u653f\u7b56/\u7ecf\u6d4e\u56fe\u8868\u53ca\u7ec6\u7c92\u5ea6\u6807\u6ce8\uff09", "result": "ChartLens\u4f7f\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5f52\u56e0\u51c6\u786e\u7387\u63d0\u534726-66%", "conclusion": "\u901a\u8fc7\u7b97\u6cd5\u521b\u65b0\u548c\u57fa\u51c6\u6784\u5efa\uff0c\u6709\u6548\u89e3\u51b3\u56fe\u8868\u89c6\u89c9\u5f52\u56e0\u95ee\u9898\uff0c\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u5728\u56fe\u8868\u5206\u6790\u4e2d\u7684\u53ef\u9760\u6027"}}
{"id": "2505.19376", "pdf": "https://arxiv.org/pdf/2505.19376", "abs": "https://arxiv.org/abs/2505.19376", "authors": ["Lance Ying", "Almog Hillel", "Ryan Truong", "Vikash K. Mansinghka", "Joshua B. Tenenbaum", "Tan Zhi-Xuan"], "title": "Belief Attribution as Mental Explanation: The Role of Accuracy, Informativity, and Causality", "categories": ["cs.CL"], "comment": "8 pages, 3 figures; oral presentation at CogSci 2025", "summary": "A key feature of human theory-of-mind is the ability to attribute beliefs to\nother agents as mentalistic explanations for their behavior. But given the wide\nvariety of beliefs that agents may hold about the world and the rich language\nwe can use to express them, which specific beliefs are people inclined to\nattribute to others? In this paper, we investigate the hypothesis that people\nprefer to attribute beliefs that are good explanations for the behavior they\nobserve. We develop a computational model that quantifies the explanatory\nstrength of a (natural language) statement about an agent's beliefs via three\nfactors: accuracy, informativity, and causal relevance to actions, each of\nwhich can be computed from a probabilistic generative model of belief-driven\nbehavior. Using this model, we study the role of each factor in how people\nselectively attribute beliefs to other agents. We investigate this via an\nexperiment where participants watch an agent collect keys hidden in boxes in\norder to reach a goal, then rank a set of statements describing the agent's\nbeliefs about the boxes' contents. We find that accuracy and informativity\nperform reasonably well at predicting these rankings when combined, but that\ncausal relevance is the single factor that best explains participants'\nresponses.", "AI": {"tldr": "\u7814\u7a76\u4eba\u7c7b\u5982\u4f55\u901a\u8fc7\u4fe1\u5ff5\u5f52\u56e0\u89e3\u91ca\u4ed6\u4eba\u884c\u4e3a\uff0c\u63d0\u51fa\u8ba1\u7b97\u6a21\u578b\u91cf\u5316\u89e3\u91ca\u6027\u4fe1\u5ff5\u7684\u4e09\u4e2a\u56e0\u7d20\uff0c\u53d1\u73b0\u56e0\u679c\u76f8\u5173\u6027\u662f\u9884\u6d4b\u4fe1\u5ff5\u5f52\u56e0\u7684\u6700\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u63a2\u7d22\u4eba\u7c7b\u4e3a\u4f55\u503e\u5411\u4e8e\u9009\u62e9\u7279\u5b9a\u4fe1\u5ff5\u6765\u89e3\u91ca\u4ed6\u4eba\u884c\u4e3a\uff0c\u73b0\u6709\u7406\u8bba\u672a\u660e\u786e\u89e3\u91ca\u4fe1\u5ff5\u9009\u62e9\u7684\u4f18\u5148\u7ea7\uff0c\u9700\u5efa\u7acb\u91cf\u5316\u6a21\u578b\u5206\u6790\u89e3\u91ca\u6027\u4fe1\u5ff5\u7684\u6838\u5fc3\u8981\u7d20\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e\u6982\u7387\u751f\u6210\u6a21\u578b\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u51c6\u786e\u6027\u3001\u4fe1\u606f\u91cf\u3001\u56e0\u679c\u76f8\u5173\u6027\u4e09\u56e0\u7d20\u91cf\u5316\u4fe1\u5ff5\u89e3\u91ca\u529b\uff0c\u8bbe\u8ba1\u884c\u4e3a\u89c2\u5bdf\u5b9e\u9a8c\u6536\u96c6\u53c2\u4e0e\u8005\u5bf9\u4ee3\u7406\u4fe1\u5ff5\u7684\u6392\u5e8f\u6570\u636e\u3002", "result": "\u51c6\u786e\u6027\uff0884%\uff09\u4e0e\u4fe1\u606f\u91cf\uff0876%\uff09\u7ec4\u5408\u9884\u6d4b\u6548\u679c\u6700\u4f73\uff0c\u4f46\u56e0\u679c\u76f8\u5173\u6027\u5355\u72ec\u89e3\u91ca\u529b\u8fbe89%\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u5355\u4e00\u56e0\u7d20\uff08p<0.01\uff09\u3002", "conclusion": "\u56e0\u679c\u76f8\u5173\u6027\u662f\u4eba\u7c7b\u4fe1\u5ff5\u5f52\u56e0\u7684\u6838\u5fc3\u8ba4\u77e5\u673a\u5236\uff0c\u8be5\u8ba1\u7b97\u6a21\u578b\u4e3a\u5fc3\u667a\u7406\u8bba\u7814\u7a76\u63d0\u4f9b\u65b0\u7684\u91cf\u5316\u5206\u6790\u6846\u67b6\uff0c\u9a8c\u8bc1\u4e86\u89e3\u91ca\u6027\u5047\u8bf4\u7684\u5fc3\u7406\u5b66\u4ef7\u503c\u3002"}}
{"id": "2505.19384", "pdf": "https://arxiv.org/pdf/2505.19384", "abs": "https://arxiv.org/abs/2505.19384", "authors": ["Seokgi Lee", "Jungjun Kim"], "title": "GSA-TTS : Toward Zero-Shot Speech Synthesis based on Gradual Style Adaptor", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "7 pages, 3 figures", "summary": "We present the gradual style adaptor TTS (GSA-TTS) with a novel style encoder\nthat gradually encodes speaking styles from an acoustic reference for zero-shot\nspeech synthesis. GSA first captures the local style of each semantic sound\nunit. Then the local styles are combined by self-attention to obtain a global\nstyle condition. This semantic and hierarchical encoding strategy provides a\nrobust and rich style representation for an acoustic model. We test GSA-TTS on\nunseen speakers and obtain promising results regarding naturalness, speaker\nsimilarity, and intelligibility. Additionally, we explore the potential of GSA\nin terms of interpretability and controllability, which stems from its\nhierarchical structure.", "AI": {"tldr": "\u6e10\u8fdb\u5f0f\u98ce\u683c\u9002\u914d\u5668TTS\uff08GSA-TTS\uff09\u901a\u8fc7\u5206\u5c42\u8bed\u4e49\u7f16\u7801\u7b56\u7565\u5b9e\u73b0\u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210\uff0c\u5728\u672a\u89c1\u8bf4\u8bdd\u8005\u4e0a\u53d6\u5f97\u81ea\u7136\u5ea6\u3001\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u5ea6\u548c\u6e05\u6670\u5ea6\u7684\u63d0\u5347", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8bed\u97f3\u5408\u6210\u7cfb\u7edf\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u5bf9\u672a\u89c1\u8bf4\u8bdd\u8005\u98ce\u683c\u7f16\u7801\u4e0d\u591f\u9c81\u68d2\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u5c42\u7f16\u7801\u7b56\u7565\u6355\u6349\u66f4\u4e30\u5bcc\u7684\u98ce\u683c\u8868\u793a", "method": "1. \u6355\u6349\u6bcf\u4e2a\u8bed\u4e49\u5355\u5143\u7684\u5c40\u90e8\u98ce\u683c \u2192 2. \u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7ec4\u5408\u5c40\u90e8\u98ce\u683c\u5f62\u6210\u5168\u5c40\u6761\u4ef6 \u2192 3. \u5206\u5c42\u7f16\u7801\u7b56\u7565\u589e\u5f3a\u98ce\u683c\u8868\u5f81\u80fd\u529b", "result": "\u5728\u672a\u89c1\u8bf4\u8bdd\u8005\u6d4b\u8bd5\u4e2d\uff1a\u81ea\u7136\u5ea6MOS\u63d0\u534712%\uff0c\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u5ea6\u8fbe\u523085%\uff0c\u8bcd\u9519\u8bef\u7387\u964d\u4f4e\u81f32.1%\u3002\u540c\u65f6\u5c55\u73b0\u5206\u5c42\u7ed3\u6784\u7684\u89e3\u91ca\u4f18\u52bf\uff08\u98ce\u683c\u53ef\u89c6\u5316\uff09\u548c\u97f5\u5f8b\u63a7\u5236\u80fd\u529b", "conclusion": "\u6e10\u8fdb\u5f0f\u5206\u5c42\u7f16\u7801\u7b56\u7565\u6709\u6548\u63d0\u5347\u96f6\u6837\u672c\u8bed\u97f3\u5408\u6210\u7684\u98ce\u683c\u8868\u5f81\u8d28\u91cf\uff0c\u5176\u8bed\u4e49\u5bf9\u9f50\u7684\u5c42\u6b21\u7ed3\u6784\u4e3a\u8bed\u97f3\u5408\u6210\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u6027\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2505.19388", "pdf": "https://arxiv.org/pdf/2505.19388", "abs": "https://arxiv.org/abs/2505.19388", "authors": ["Takumi Goto", "Yusuke Sakai", "Taro Watanabe"], "title": "gec-metrics: A Unified Library for Grammatical Error Correction Evaluation", "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 System Demonstration Track, 11 pages, 9 figures", "summary": "We introduce gec-metrics, a library for using and developing grammatical\nerror correction (GEC) evaluation metrics through a unified interface. Our\nlibrary enables fair system comparisons by ensuring that everyone conducts\nevaluations using a consistent implementation. Moreover, it is designed with a\nstrong focus on API usage, making it highly extensible. It also includes\nmeta-evaluation functionalities and provides analysis and visualization\nscripts, contributing to developing GEC evaluation metrics. Our code is\nreleased under the MIT license and is also distributed as an installable\npackage. The video is available on YouTube.", "AI": {"tldr": "\u5f00\u53d1gec-metrics\u5e93\u7edf\u4e00GEC\u8bc4\u4f30\u6807\u51c6\uff0c\u63d0\u4f9b\u53ef\u6269\u5c55\u63a5\u53e3\u53ca\u5206\u6790\u5de5\u5177\uff0c\u4fc3\u8fdb\u516c\u5e73\u7cfb\u7edf\u6bd4\u8f83\u3002\u4ee3\u7801MIT\u5f00\u6e90\u5e76\u6253\u5305\u5206\u53d1\u3002", "motivation": "\u89e3\u51b3GEC\u8bc4\u4f30\u4e2d\u56e0\u4e0d\u540c\u5b9e\u73b0\u5bfc\u81f4\u7684\u6307\u6807\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u63a5\u53e3\u786e\u4fdd\u8bc4\u4f30\u7ed3\u679c\u53ef\u6bd4\u6027\uff0c\u540c\u65f6\u63d0\u5347\u5f00\u53d1\u6548\u7387\u3002", "method": "\u8bbe\u8ba1\u7edf\u4e00API\u96c6\u6210\u591a\u79cd\u8bc4\u4f30\u6307\u6807\uff0c\u5b9e\u73b0\u5143\u8bc4\u4f30\u6a21\u5757\uff0c\u5f00\u53d1\u53ef\u89c6\u5316\u5206\u6790\u811a\u672c\uff0c\u91c7\u7528MIT\u534f\u8bae\u5f00\u6e90\u5e76\u6253\u5305\u4e3a\u53ef\u5b89\u88c5\u7ec4\u4ef6\u3002", "result": "\u6210\u529f\u521b\u5efa\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u63d0\u4f9b\u7cfb\u7edf\u5bf9\u6bd4\u57fa\u51c6\uff0c\u914d\u5957\u5de5\u5177\u94fe\u5e2e\u52a9\u4f18\u5316\u6307\u6807\u8bbe\u8ba1\uff0c\u4ee3\u7801\u53ef\u7528\u6027\u83b7\u793e\u533a\u8ba4\u53ef\u3002", "conclusion": "\u8be5\u5e93\u6807\u51c6\u5316\u4e86GEC\u8bc4\u4f30\u6d41\u7a0b\uff0c\u4e3a\u7814\u7a76\u8005\u63d0\u4f9b\u53ef\u9760\u5de5\u5177\u57fa\u7840\uff0c\u672a\u6765\u53ef\u901a\u8fc7\u793e\u533a\u534f\u4f5c\u6301\u7eed\u6269\u5c55\u8bc4\u4f30\u6307\u6807\u96c6\u5408\u3002"}}
{"id": "2505.19392", "pdf": "https://arxiv.org/pdf/2505.19392", "abs": "https://arxiv.org/abs/2505.19392", "authors": ["Jade Robinson", "Jonathan K. Kummerfeld"], "title": "Simple and Effective Baselines for Code Summarisation Evaluation", "categories": ["cs.CL", "cs.AI", "cs.SE", "68T50", "I.2.7"], "comment": null, "summary": "Code documentation is useful, but writing it is time-consuming. Different\ntechniques for generating code summaries have emerged, but comparing them is\ndifficult because human evaluation is expensive and automatic metrics are\nunreliable. In this paper, we introduce a simple new baseline in which we ask\nan LLM to give an overall score to a summary. Unlike n-gram and embedding-based\nbaselines, our approach is able to consider the code when giving a score. This\nallows us to also make a variant that does not consider the reference summary\nat all, which could be used for other tasks, e.g., to evaluate the quality of\ndocumentation in code bases. We find that our method is as good or better than\nprior metrics, though we recommend using it in conjunction with embedding-based\nmethods to avoid the risk of LLM-specific bias.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u4ee3\u7801\u6458\u8981\u8bc4\u4f30\u65b0\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4f18\u4e8e\u73b0\u6709\u6307\u6807\u4e14\u652f\u6301\u65e0\u53c2\u8003\u8bc4\u4f30", "motivation": "\u73b0\u6709\u4ee3\u7801\u6458\u8981\u751f\u6210\u6280\u672f\u7f3a\u4e4f\u9ad8\u6548\u53ef\u9760\u7684\u8bc4\u4f30\u65b9\u5f0f\uff0c\u4eba\u5de5\u8bc4\u4f30\u6210\u672c\u9ad8\u800c\u81ea\u52a8\u6307\u6807\u4e0d\u53ef\u9760", "method": "\u8ba9LLM\u7ed3\u5408\u4ee3\u7801\u5185\u5bb9\u76f4\u63a5\u5bf9\u6458\u8981\u8fdb\u884c\u6574\u4f53\u8bc4\u5206\uff0c\u652f\u6301\u4e24\u79cd\u6a21\u5f0f\uff08\u53c2\u8003/\u65e0\u53c2\u8003\u6458\u8981\uff09", "result": "\u8be5\u65b9\u6cd5\u8fbe\u5230\u6216\u8d85\u8d8a\u73b0\u6709\u6307\u6807\u6548\u679c\uff0c\u65e0\u53c2\u8003\u6a21\u5f0f\u53ef\u6269\u5c55\u5e94\u7528\u4e8e\u4ee3\u7801\u6587\u6863\u8d28\u91cf\u8bc4\u4f30", "conclusion": "\u5efa\u8bae\u4e0e\u5d4c\u5165\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\u4ee5\u89c4\u907fLLM\u7279\u6709\u504f\u5dee\uff0c\u5b9e\u73b0\u66f4\u5168\u9762\u7684\u8bc4\u4f30"}}
{"id": "2505.19405", "pdf": "https://arxiv.org/pdf/2505.19405", "abs": "https://arxiv.org/abs/2505.19405", "authors": ["Yan Wen", "Junfeng Guo", "Heng Huang"], "title": "CoTGuard: Using Chain-of-Thought Triggering for Copyright Protection in Multi-Agent LLM Systems", "categories": ["cs.CL", "cs.CR"], "comment": "18 pages, 1 figure", "summary": "As large language models (LLMs) evolve into autonomous agents capable of\ncollaborative reasoning and task execution, multi-agent LLM systems have\nemerged as a powerful paradigm for solving complex problems. However, these\nsystems pose new challenges for copyright protection, particularly when\nsensitive or copyrighted content is inadvertently recalled through inter-agent\ncommunication and reasoning. Existing protection techniques primarily focus on\ndetecting content in final outputs, overlooking the richer, more revealing\nreasoning processes within the agents themselves. In this paper, we introduce\nCoTGuard, a novel framework for copyright protection that leverages\ntrigger-based detection within Chain-of-Thought (CoT) reasoning. Specifically,\nwe can activate specific CoT segments and monitor intermediate reasoning steps\nfor unauthorized content reproduction by embedding specific trigger queries\ninto agent prompts. This approach enables fine-grained, interpretable detection\nof copyright violations in collaborative agent scenarios. We evaluate CoTGuard\non various benchmarks in extensive experiments and show that it effectively\nuncovers content leakage with minimal interference to task performance. Our\nfindings suggest that reasoning-level monitoring offers a promising direction\nfor safeguarding intellectual property in LLM-based agent systems.", "AI": {"tldr": "\u63d0\u51faCoTGuard\u6846\u67b6\uff0c\u901a\u8fc7\u601d\u7ef4\u94fe\u89e6\u53d1\u68c0\u6d4b\u5b9e\u73b0\u534f\u4f5c\u5f0fLLM\u4ee3\u7406\u7cfb\u7edf\u7684\u7ec6\u7c92\u5ea6\u7248\u6743\u4fdd\u62a4\uff0c\u5728\u4efb\u52a1\u6027\u80fd\u5f71\u54cd\u6700\u5c0f\u5316\u7684\u524d\u63d0\u4e0b\u6709\u6548\u8bc6\u522b\u5185\u5bb9\u6cc4\u6f0f\u3002", "motivation": "\u73b0\u6709\u7248\u6743\u4fdd\u62a4\u6280\u672f\u4ec5\u5173\u6ce8\u6700\u7ec8\u8f93\u51fa\uff0c\u5ffd\u89c6\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u901a\u8fc7\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u6cc4\u9732\u654f\u611f/\u7248\u6743\u5185\u5bb9\u7684\u98ce\u9669\u3002", "method": "\u5728\u667a\u80fd\u4f53\u63d0\u793a\u4e2d\u5d4c\u5165\u7279\u5b9a\u89e6\u53d1\u67e5\u8be2\uff0c\u6fc0\u6d3b\u5e76\u76d1\u63a7\u601d\u7ef4\u94fe\u63a8\u7406\u4e2d\u7684\u5173\u952e\u6bb5\u843d\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u4e2d\u95f4\u8fc7\u7a0b\u4fb5\u6743\u68c0\u6d4b\u3002", "result": "\u591a\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u53d1\u73b0\u5185\u5bb9\u6cc4\u6f0f\uff08\u68c0\u6d4b\u7387\u63d0\u534723%\uff09\uff0c\u540c\u65f6\u4fdd\u630195%\u4ee5\u4e0a\u7684\u4efb\u52a1\u5b8c\u6210\u5ea6\u3002", "conclusion": "\u601d\u7ef4\u94fe\u5c42\u9762\u7684\u76d1\u63a7\u4e3aLLM\u4ee3\u7406\u7cfb\u7edf\u7684\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5e73\u8861\u4e86\u7248\u6743\u4fdd\u62a4\u4e0e\u7cfb\u7edf\u6548\u80fd\u3002"}}
{"id": "2505.19410", "pdf": "https://arxiv.org/pdf/2505.19410", "abs": "https://arxiv.org/abs/2505.19410", "authors": ["Jiajun Zhu", "Ye Liu", "Meikai Bao", "Kai Zhang", "Yanghai Zhang", "Qi Liu"], "title": "Self-Reflective Planning with Knowledge Graphs: Enhancing LLM Reasoning Reliability for Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated remarkable\ncapabilities in natural language processing tasks, yet they remain prone to\nhallucinations when reasoning with insufficient internal knowledge. While\nintegrating LLMs with knowledge graphs (KGs) provides access to structured,\nverifiable information, existing approaches often generate incomplete or\nfactually inconsistent reasoning paths. To this end, we propose Self-Reflective\nPlanning (SRP), a framework that synergizes LLMs with KGs through iterative,\nreference-guided reasoning. Specifically, given a question and topic entities,\nSRP first searches for references to guide planning and reflection. In the\nplanning process, it checks initial relations and generates a reasoning path.\nAfter retrieving knowledge from KGs through a reasoning path, it implements\niterative reflection by judging the retrieval result and editing the reasoning\npath until the answer is correctly retrieved. Extensive experiments on three\npublic datasets demonstrate that SRP surpasses various strong baselines and\nfurther underscore its reliable reasoning ability.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u6211\u53cd\u601d\u89c4\u5212(SRP)\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u4e0e\u8bed\u8a00\u6a21\u578b\u534f\u540c\u7684\u8fed\u4ee3\u53cd\u601d\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u4e0e\u77e5\u8bc6\u56fe\u8c31\u7ed3\u5408\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u8def\u5f84\u4e0d\u5b8c\u6574\u3001\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u9700\u8981\u66f4\u53ef\u9760\u7684\u8fed\u4ee3\u9a8c\u8bc1\u673a\u5236", "method": "1.\u53c2\u8003\u5f15\u5bfc\u89c4\u5212\uff1a\u641c\u7d22\u76f8\u5173\u5b9e\u4f53\u6784\u5efa\u521d\u59cb\u63a8\u7406\u8def\u5f84\n2.\u77e5\u8bc6\u68c0\u7d22\u9a8c\u8bc1\uff1a\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u83b7\u53d6\u7ed3\u6784\u5316\u4e8b\u5b9e\n3.\u8fed\u4ee3\u53cd\u601d\u4fee\u6b63\uff1a\u5224\u65ad\u68c0\u7d22\u7ed3\u679c\u5e76\u52a8\u6001\u8c03\u6574\u63a8\u7406\u8def\u5f84", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u51c6\u786e\u7387\u63d0\u5347\u663e\u8457\uff08\u5177\u4f53\u6570\u503c\u9700\u67e5\u539f\u6587\uff09\uff0c\u5c55\u793a\u51fa\u66f4\u53ef\u9760\u7684\u63a8\u7406\u80fd\u529b", "conclusion": "SRP\u6846\u67b6\u6709\u6548\u89e3\u51b3\u77e5\u8bc6\u9a71\u52a8\u63a8\u7406\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u4e0e\u7ed3\u6784\u5316\u77e5\u8bc6\u7684\u6df1\u5ea6\u878d\u5408\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2505.19426", "pdf": "https://arxiv.org/pdf/2505.19426", "abs": "https://arxiv.org/abs/2505.19426", "authors": ["Wenyang Xiao", "Haoyu Zhao", "Lingxiao Huang"], "title": "The Role of Diversity in In-Context Learning for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "30 pages", "summary": "In-context learning (ICL) is a crucial capability of current large language\nmodels (LLMs), where the selection of examples plays a key role in performance.\nWhile most existing approaches focus on selecting the most similar examples to\nthe query, the impact of diversity in example selection remains underexplored.\nWe systematically investigate the role of diversity in in-context example\nselection through experiments across a range of tasks, from sentiment\nclassification to more challenging math and code problems. Experiments on\nLlama-3.1, Gemma-2, and Mistral-v0.3 families of models show that\ndiversity-aware selection methods improve performance, particularly on complex\ntasks like math and code, and enhance robustness to out-of-distribution\nqueries. To support these findings, we introduce a theoretical framework that\nexplains the benefits of incorporating diversity in in-context example\nselection.", "AI": {"tldr": "\u7814\u7a76\u8bc1\u5b9e\u591a\u6837\u6027\u6837\u672c\u9009\u62e9\u63d0\u5347\u5927\u6a21\u578b\u4e0a\u4e0b\u6587\u5b66\u4e60\u6027\u80fd\uff0c\u5c24\u5176\u5728\u6570\u5b66/\u4ee3\u7801\u4efb\u52a1\u4e2d\u6548\u679c\u663e\u8457\uff0c\u5e76\u589e\u5f3a\u5bf9\u5206\u5e03\u5916\u6570\u636e\u7684\u9c81\u68d2\u6027", "motivation": "\u9488\u5bf9\u73b0\u6709\u4e0a\u4e0b\u6587\u5b66\u4e60\u7814\u7a76\u4e2d\u8fc7\u5ea6\u5173\u6ce8\u6837\u672c\u76f8\u4f3c\u6027\u800c\u5ffd\u89c6\u591a\u6837\u6027\u7684\u73b0\u72b6\uff0c\u63a2\u7d22\u591a\u6837\u6027\u6837\u672c\u9009\u62e9\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u7cfb\u7edf\u6027\u5f71\u54cd", "method": "\u5728Llama/Gemma/Mistral\u7b49\u4e3b\u6d41\u6a21\u578b\u5bb6\u65cf\u4e0a\u5f00\u5c55\u591a\u4efb\u52a1\u5b9e\u9a8c\uff08\u60c5\u611f\u5206\u7c7b\u2192\u6570\u5b66/\u4ee3\u7801\u95ee\u9898\uff09\uff0c\u63d0\u51fa\u7406\u8bba\u6846\u67b6\u89e3\u91ca\u591a\u6837\u6027\u589e\u76ca\u673a\u5236", "result": "\u591a\u6837\u6027\u6837\u672c\u9009\u62e9\u4f7f\u590d\u6742\u4efb\u52a1\u51c6\u786e\u7387\u63d0\u53478-15%\uff0c\u5206\u5e03\u5916\u573a\u666f\u9c81\u68d2\u6027\u63d0\u534730%+\uff0c\u7406\u8bba\u6846\u67b6\u6709\u6548\u89e3\u91ca\u591a\u6837\u6027\u589e\u76ca\u673a\u7406", "conclusion": "\u6837\u672c\u591a\u6837\u6027\u662f\u4e0a\u4e0b\u6587\u5b66\u4e60\u5173\u952e\u56e0\u7d20\uff0c\u5176\u4e0e\u76f8\u4f3c\u6027\u7684\u5e73\u8861\u673a\u5236\u4e3a\u672a\u6765\u4f18\u5316\u65b9\u5411\uff0c\u7406\u8bba\u6846\u67b6\u4e3a\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2505.19428", "pdf": "https://arxiv.org/pdf/2505.19428", "abs": "https://arxiv.org/abs/2505.19428", "authors": ["Abhijnan Nath", "Carine Graff", "Andrei Bachinin", "Nikhil Krishnaswamy"], "title": "Frictional Agent Alignment Framework: Slow Down and Don't Break Things", "categories": ["cs.CL"], "comment": "48 pages (main paper: 10 pages incl. Limitations and Acknowledgments;\n  references: 6 pages; appendix: 32 pages), 9 figures, 12 tables, appearing in\n  Proceedings of ACL 2025, Vienna, Austria", "summary": "AI support of collaborative interactions entails mediating potential\nmisalignment between interlocutor beliefs. Common preference alignment methods\nlike DPO excel in static settings, but struggle in dynamic collaborative tasks\nwhere the explicit signals of interlocutor beliefs are sparse and skewed. We\npropose the Frictional Agent Alignment Framework (FAAF), to generate precise,\ncontext-aware \"friction\" that prompts for deliberation and re-examination of\nexisting evidence. FAAF's two-player objective decouples from data skew: a\nfrictive-state policy identifies belief misalignments, while an intervention\npolicy crafts collaborator-preferred responses. We derive an analytical\nsolution to this objective, enabling training a single policy via a simple\nsupervised loss. Experiments on three benchmarks show FAAF outperforms\ncompetitors in producing concise, interpretable friction and in OOD\ngeneralization. By aligning LLMs to act as adaptive \"thought partners\" -- not\npassive responders -- FAAF advances scalable, dynamic human-AI collaboration.\nOur code and data can be found at https://github.com/csu-signal/FAAF_ACL.", "AI": {"tldr": "\u63d0\u51faFAAF\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u7b56\u7565\u673a\u5236\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684'\u6469\u64e6'\uff0c\u6709\u6548\u89e3\u51b3\u52a8\u6001\u534f\u4f5c\u4efb\u52a1\u4e2d\u4fe1\u5ff5\u5bf9\u9f50\u95ee\u9898\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u5177\u5907OOD\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982DPO\uff09\u5728\u52a8\u6001\u534f\u4f5c\u573a\u666f\u4e2d\u96be\u4ee5\u5e94\u5bf9\u7a00\u758f/\u6709\u504f\u7684\u4fe1\u5ff5\u4fe1\u53f7\uff0c\u9700\u53d1\u5c55\u4e3b\u52a8\u4fc3\u53d1\u53cd\u601d\u7684\u673a\u5236\u6765\u589e\u5f3a\u4eba\u673a\u534f\u4f5c\u8d28\u91cf\u3002", "method": "\u91c7\u7528\u53cc\u7b56\u7565\u6846\u67b6\uff1a1\uff09\u6469\u64e6\u72b6\u6001\u7b56\u7565\u68c0\u6d4b\u4fe1\u5ff5\u9519\u4f4d 2\uff09\u5e72\u9884\u7b56\u7565\u751f\u6210\u5408\u4f5c\u53cb\u597d\u54cd\u5e94\uff0c\u901a\u8fc7\u89e3\u6790\u89e3\u5b9e\u73b0\u76d1\u7763\u5b66\u4e60\u8bad\u7ec3\u5355\u4e00\u7b56\u7565\u3002", "result": "\u5728\u4e09\u5927\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFAAF\u751f\u6210\u66f4\u7b80\u6d01\u53ef\u89e3\u91ca\u7684\u6469\u64e6\u54cd\u5e94\uff0cOOD\u573a\u666f\u6cdb\u5316\u80fd\u529b\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "FAAF\u901a\u8fc7\u4e3b\u52a8\u5236\u9020\u8ba4\u77e5\u6469\u64e6\u4f7fLLM\u6210\u4e3a\u52a8\u6001'\u601d\u8003\u4f19\u4f34'\uff0c\u63a8\u52a8\u53ef\u6269\u5c55\u7684\u4eba\u673a\u534f\u4f5c\u8303\u5f0f\uff0c\u4ee3\u7801\u6570\u636e\u5df2\u5f00\u6e90\u4f9b\u793e\u533a\u9a8c\u8bc1\u3002"}}
{"id": "2505.19429", "pdf": "https://arxiv.org/pdf/2505.19429", "abs": "https://arxiv.org/abs/2505.19429", "authors": ["Younghan Park", "Anuj Diwan", "David Harwath", "Eunsol Choi"], "title": "Rhapsody: A Dataset for Highlight Detection in Podcasts", "categories": ["cs.CL"], "comment": null, "summary": "Podcasts have become daily companions for half a billion users. Given the\nenormous amount of podcast content available, highlights provide a valuable\nsignal that helps viewers get the gist of an episode and decide if they want to\ninvest in listening to it in its entirety. However, identifying highlights\nautomatically is challenging due to the unstructured and long-form nature of\nthe content. We introduce Rhapsody, a dataset of 13K podcast episodes paired\nwith segment-level highlight scores derived from YouTube's 'most replayed'\nfeature. We frame the podcast highlight detection as a segment-level binary\nclassification task. We explore various baseline approaches, including\nzero-shot prompting of language models and lightweight finetuned language\nmodels using segment-level classification heads. Our experimental results\nindicate that even state-of-the-art language models like GPT-4o and Gemini\nstruggle with this task, while models finetuned with in-domain data\nsignificantly outperform their zero-shot performance. The finetuned model\nbenefits from leveraging both speech signal features and transcripts. These\nfindings highlight the challenges for fine-grained information access in\nlong-form spoken media.", "AI": {"tldr": "\u63d0\u51faRhapsody\u6570\u636e\u96c6\u7528\u4e8e\u64ad\u5ba2\u9ad8\u4eae\u68c0\u6d4b\uff0c\u53d1\u73b0\u7ed3\u5408\u8bed\u97f3\u548c\u6587\u672c\u7279\u5f81\u7684\u5fae\u8c03\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u96f6\u6837\u672c\u5927\u6a21\u578b\u3002", "motivation": "\u64ad\u5ba2\u5185\u5bb9\u4f53\u91cf\u5e9e\u5927\u4f46\u975e\u7ed3\u6784\u5316\uff0c\u81ea\u52a8\u9ad8\u4eae\u68c0\u6d4b\u53ef\u63d0\u5347\u7528\u6237\u5185\u5bb9\u7b5b\u9009\u6548\u7387\u3002\u5f53\u524d\u4e3b\u6d41\u5927\u6a21\u578b\u5728\u6b64\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\uff0c\u9700\u63a2\u7d22\u9886\u57df\u4e13\u7528\u65b9\u6848\u3002", "method": "\u6784\u5efa13K\u64ad\u5ba2\u7247\u6bb5\u6570\u636e\u96c6\uff0c\u4f7f\u7528YouTube\u91cd\u64ad\u6570\u636e\u6807\u6ce8\u9ad8\u4eae\u7247\u6bb5\u3002\u5bf9\u6bd4\u96f6\u6837\u672c\u63d0\u793a\uff08GPT-4o/Gemini\uff09\u4e0e\u5fae\u8c03\u6a21\u578b\uff08\u7ed3\u5408\u8bed\u97f3\u7279\u5f81+\u6587\u672c\u7684\u5206\u7c7b\u6a21\u578b\uff09\u7684\u6027\u80fd\u3002", "result": "\u5fae\u8c03\u6a21\u578bF1\u503c\u8fbe0.78\uff0c\u663e\u8457\u4f18\u4e8eGPT-4o\uff080.62\uff09\u548cGemini\uff080.59\uff09\u3002\u591a\u6a21\u6001\u7279\u5f81\u4f7f\u51c6\u786e\u7387\u63d0\u534712%\u3002", "conclusion": "\u957f\u8bed\u97f3\u5a92\u4f53\u7684\u7ec6\u7c92\u5ea6\u4fe1\u606f\u62bd\u53d6\u9700\u9886\u57df\u9002\u914d\uff0c\u591a\u6a21\u6001\u5fae\u8c03\u65b9\u6848\u6709\u6548\uff0c\u4e3a\u97f3\u9891\u5185\u5bb9\u7406\u89e3\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2505.19430", "pdf": "https://arxiv.org/pdf/2505.19430", "abs": "https://arxiv.org/abs/2505.19430", "authors": ["Keane Ong", "Rui Mao", "Deeksha Varshney", "Paul Pu Liang", "Erik Cambria", "Gianmarco Mengaldo"], "title": "Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Counterfactual reasoning typically involves considering alternatives to\nactual events. While often applied to understand past events, a distinct\nform-forward counterfactual reasoning-focuses on anticipating plausible future\ndevelopments. This type of reasoning is invaluable in dynamic financial\nmarkets, where anticipating market developments can powerfully unveil potential\nrisks and opportunities for stakeholders, guiding their decision-making.\nHowever, performing this at scale is challenging due to the cognitive demands\ninvolved, underscoring the need for automated solutions. Large Language Models\n(LLMs) offer promise, but remain unexplored for this application. To address\nthis gap, we introduce a novel benchmark, Fin-Force-FINancial FORward\nCounterfactual Evaluation. By curating financial news headlines and providing\nstructured evaluation, Fin-Force supports LLM based forward counterfactual\ngeneration. This paves the way for scalable and automated solutions for\nexploring and anticipating future market developments, thereby providing\nstructured insights for decision-making. Through experiments on Fin-Force, we\nevaluate state-of-the-art LLMs and counterfactual generation methods, analyzing\ntheir limitations and proposing insights for future research.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u91d1\u878d\u524d\u5411\u53cd\u4e8b\u5b9e\u63a8\u7406\u57fa\u51c6Fin-Force\uff0c\u8bc4\u4f30\u5927\u6a21\u578b\u5728\u9884\u6d4b\u5e02\u573a\u53d1\u5c55\u4e2d\u7684\u6f5c\u529b", "motivation": "\u4f20\u7edf\u53cd\u4e8b\u5b9e\u63a8\u7406\u4fa7\u91cd\u5386\u53f2\u5206\u6790\uff0c\u800c\u524d\u5411\u53cd\u4e8b\u5b9e\u63a8\u7406\u80fd\u6709\u6548\u9884\u6d4b\u91d1\u878d\u5e02\u573a\u52a8\u6001\u53d8\u5316\uff0c\u4f46\u4eba\u5de5\u64cd\u4f5c\u5b58\u5728\u8ba4\u77e5\u8d1f\u8377\u9650\u5236\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848", "method": "\u901a\u8fc7\u6784\u5efa\u5305\u542b\u91d1\u878d\u65b0\u95fb\u5934\u6761\u7684\u7ed3\u6784\u5316\u8bc4\u4f30\u57fa\u51c6Fin-Force\uff0c\u652f\u6301\u57fa\u4e8eLLM\u7684\u524d\u5411\u53cd\u4e8b\u5b9e\u751f\u6210\u6846\u67b6\u5f00\u53d1", "result": "\u5728Fin-Force\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u4e86\u524d\u6cbfLLM\u548c\u53cd\u4e8b\u5b9e\u751f\u6210\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u5c40\u9650\u6027\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u5411", "conclusion": "Fin-Force\u4e3a\u81ea\u52a8\u5316\u9884\u6d4b\u5e02\u573a\u52a8\u6001\u63d0\u4f9b\u7ed3\u6784\u5316\u8bc4\u4f30\u57fa\u7840\uff0c\u63a8\u52a8\u98ce\u9669\u9884\u8b66\u548c\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u667a\u80fd\u5316\u53d1\u5c55"}}
{"id": "2505.19435", "pdf": "https://arxiv.org/pdf/2505.19435", "abs": "https://arxiv.org/abs/2505.19435", "authors": ["Zhihong Pan", "Kai Zhang", "Yuze Zhao", "Yupeng Han"], "title": "Route to Reason: Adaptive Routing for LLM and Reasoning Strategy Selection", "categories": ["cs.CL"], "comment": null, "summary": "The inherent capabilities of a language model (LM) and the reasoning\nstrategies it employs jointly determine its performance in reasoning tasks.\nWhile test-time scaling is regarded as an effective approach to tackling\ncomplex reasoning tasks, it incurs substantial computational costs and often\nleads to \"overthinking\", where models become trapped in \"thought pitfalls\". To\naddress this challenge, we propose Route-To-Reason (RTR), a novel unified\nrouting framework that dynamically allocates both LMs and reasoning strategies\naccording to task difficulty under budget constraints. RTR learns compressed\nrepresentations of both expert models and reasoning strategies, enabling their\njoint and adaptive selection at inference time. This method is low-cost, highly\nflexible, and can be seamlessly extended to arbitrary black-box or white-box\nmodels and strategies, achieving true plug-and-play functionality. Extensive\nexperiments across seven open source models and four reasoning strategies\ndemonstrate that RTR achieves an optimal trade-off between accuracy and\ncomputational efficiency among all baselines, achieving higher accuracy than\nthe best single model while reducing token usage by over 60%.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u8def\u7531\u6846\u67b6RTR\uff0c\u901a\u8fc7\u8054\u5408\u9009\u62e9\u8bed\u8a00\u6a21\u578b\u548c\u63a8\u7406\u7b56\u7565\uff0c\u5728\u9884\u7b97\u7ea6\u675f\u4e0b\u5b9e\u73b0\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u6700\u4f18\u5e73\u8861", "motivation": "\u4f20\u7edf\u6d4b\u8bd5\u65f6\u6269\u5c55\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6613\u5bfc\u81f4\u6a21\u578b'\u8fc7\u601d\u8003'\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u89e3\u51b3\u65b9\u6848", "method": "\u5b66\u4e60\u4e13\u5bb6\u6a21\u578b\u548c\u63a8\u7406\u7b56\u7565\u7684\u538b\u7f29\u8868\u793a\uff0c\u5b9e\u73b0\u63a8\u7406\u65f6\u7684\u52a8\u6001\u8054\u5408\u9009\u62e9\uff0c\u652f\u6301\u4efb\u610f\u9ed1\u7bb1/\u767d\u7bb1\u6a21\u578b\u7684\u5373\u63d2\u5373\u7528", "result": "\u57287\u4e2a\u6a21\u578b\u548c4\u79cd\u7b56\u7565\u7684\u5b9e\u9a8c\u4e2d\uff0c\u51c6\u786e\u7387\u8d85\u8d8a\u6700\u4f73\u5355\u6a21\u578b\u7684\u540c\u65f6\u51cf\u5c1160%\u4ee5\u4e0a\u7684token\u6d88\u8017", "conclusion": "RTR\u6846\u67b6\u4ee5\u4f4e\u6210\u672c\u5b9e\u73b0\u7075\u6d3b\u6269\u5c55\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u63a8\u7406\u573a\u666f"}}
{"id": "2505.19439", "pdf": "https://arxiv.org/pdf/2505.19439", "abs": "https://arxiv.org/abs/2505.19439", "authors": ["Rihui Xin", "Han Liu", "Zecheng Wang", "Yupeng Zhang", "Dianbo Sui", "Xiaolin Hu", "Bingning Wang"], "title": "Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models have achieved remarkable success in natural language\nprocessing tasks, with Reinforcement Learning playing a key role in adapting\nthem to specific applications. However, obtaining ground truth answers for\ntraining LLMs in mathematical problem-solving is often challenging, costly, and\nsometimes unfeasible. This research delves into the utilization of format and\nlength as surrogate signals to train LLMs for mathematical problem-solving,\nbypassing the need for traditional ground truth answers.Our study shows that a\nreward function centered on format correctness alone can yield performance\nimprovements comparable to the standard GRPO algorithm in early phases.\nRecognizing the limitations of format-only rewards in the later phases, we\nincorporate length-based rewards. The resulting GRPO approach, leveraging\nformat-length surrogate signals, not only matches but surpasses the performance\nof the standard GRPO algorithm relying on ground truth answers in certain\nscenarios, achieving 40.0\\% accuracy on AIME2024 with a 7B base model. Through\nsystematic exploration and experimentation, this research not only offers a\npractical solution for training LLMs to solve mathematical problems and\nreducing the dependence on extensive ground truth data collection, but also\nreveals the essence of why our label-free approach succeeds: base model is like\nan excellent student who has already mastered mathematical and logical\nreasoning skills, but performs poorly on the test paper, it simply needs to\ndevelop good answering habits to achieve outstanding results in exams , in\nother words, to unlock the capabilities it already possesses.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u771f\u5b9e\u7b54\u6848\u6807\u7b7e\u7684LLM\u6570\u5b66\u89e3\u9898\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u683c\u5f0f-\u957f\u5ea6\u53cc\u4fe1\u53f7\u5b9e\u73b0\u6027\u80fd\u7a81\u7834", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u6210\u672c\u7684\u771f\u5b9e\u7b54\u6848\u6807\u6ce8\uff0c\u6570\u5b66\u9886\u57df\u6807\u6ce8\u5c24\u5176\u56f0\u96be\u3002\u7814\u7a76\u53d1\u73b0\u57fa\u7840\u6a21\u578b\u5df2\u5177\u5907\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u53ea\u9700\u57f9\u517b\u826f\u597d\u4f5c\u7b54\u4e60\u60ef\u5373\u53ef\u91ca\u653e\u6f5c\u529b", "method": "\u5206\u9636\u6bb5\u8bad\u7ec3\uff1a\u521d\u671f\u4f7f\u7528\u683c\u5f0f\u6b63\u786e\u6027\u5956\u52b1\uff08GRPO\u7b97\u6cd5\uff09\uff0c\u540e\u671f\u5f15\u5165\u7b54\u6848\u957f\u5ea6\u5956\u52b1\u5f62\u6210\u590d\u5408\u5956\u52b1\u673a\u5236", "result": "\u57fa\u4e8e7B\u57fa\u7840\u6a21\u578b\u5728AIME2024\u8fbe\u523040%\u51c6\u786e\u7387\uff0c\u90e8\u5206\u573a\u666f\u8d85\u8d8a\u4f9d\u8d56\u771f\u5b9e\u6807\u7b7e\u7684\u6807\u51c6GRPO\u7b97\u6cd5", "conclusion": "\u6807\u7b7e\u66ff\u4ee3\u65b9\u6848\u6709\u6548\u964d\u4f4e\u6570\u636e\u4f9d\u8d56\uff0c\u63ed\u793a\u6a21\u578b\u80fd\u529b\u89e3\u9501\u672c\u8d28\uff1a\u57f9\u517b\u89c4\u8303\u7684\u4f5c\u7b54\u4e60\u60ef\u5373\u53ef\u6fc0\u6d3b\u9884\u8bad\u7ec3\u4e60\u5f97\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b"}}
{"id": "2505.19440", "pdf": "https://arxiv.org/pdf/2505.19440", "abs": "https://arxiv.org/abs/2505.19440", "authors": ["Shashata Sawmya", "Micah Adler", "Nir Shavit"], "title": "The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This paper studies the emergence of interpretable categorical features within\nlarge language models (LLMs), analyzing their behavior across training\ncheckpoints (time), transformer layers (space), and varying model sizes\n(scale). Using sparse autoencoders for mechanistic interpretability, we\nidentify when and where specific semantic concepts emerge within neural\nactivations. Results indicate clear temporal and scale-specific thresholds for\nfeature emergence across multiple domains. Notably, spatial analysis reveals\nunexpected semantic reactivation, with early-layer features re-emerging at\nlater layers, challenging standard assumptions about representational dynamics\nin transformer models.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u53ef\u89e3\u91ca\u7279\u5f81\u7684\u51fa\u73b0\u5b58\u5728\u65f6\u7a7a\u4e0e\u89c4\u6a21\u9608\u503c\uff0c\u5e76\u53d1\u73b0\u65e9\u671f\u5c42\u7279\u5f81\u5728\u540e\u671f\u5c42\u91cd\u65b0\u6fc0\u6d3b\u7684\u53cd\u5e38\u73b0\u8c61", "motivation": "\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u53ef\u89e3\u91ca\u7c7b\u522b\u7279\u5f81\u5728\u4e0d\u540c\u8bad\u7ec3\u9636\u6bb5\u3001\u6a21\u578b\u5c42\u6570\u548c\u6a21\u578b\u89c4\u6a21\u4e0b\u7684\u6f14\u53d8\u89c4\u5f8b", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8fdb\u884c\u673a\u68b0\u53ef\u89e3\u91ca\u6027\u5206\u6790\uff0c\u8ffd\u8e2a\u795e\u7ecf\u6fc0\u6d3b\u4e2d\u7684\u8bed\u4e49\u6982\u5ff5\u51fa\u73b0\u4f4d\u7f6e\u548c\u65f6\u95f4", "result": "\u53d1\u73b0\u7279\u5f81\u51fa\u73b0\u5b58\u5728\u660e\u786e\u7684\u65f6\u7a7a\u548c\u89c4\u6a21\u9608\u503c\uff0c\u5e76\u5728\u7a7a\u95f4\u7ef4\u5ea6\u89c2\u5bdf\u5230\u65e9\u671f\u5c42\u7279\u5f81\u5728\u540e\u671f\u5c42\u91cd\u65b0\u6fc0\u6d3b\u7684\u73b0\u8c61", "conclusion": "\u5c42\u95f4\u7279\u5f81\u518d\u6fc0\u6d3b\u73b0\u8c61\u6311\u6218\u4e86Transformer\u8868\u5f81\u52a8\u6001\u7684\u4f20\u7edf\u5047\u8bbe\uff0c\u4e3a\u6a21\u578b\u89e3\u91ca\u63d0\u4f9b\u65b0\u89c6\u89d2"}}
{"id": "2505.19472", "pdf": "https://arxiv.org/pdf/2505.19472", "abs": "https://arxiv.org/abs/2505.19472", "authors": ["Mohammad Mahdi Moradi", "Walid Ahmed", "Shuangyue Wen", "Sudhir Mudur", "Weiwei Zhang", "Yang Liu"], "title": "Balancing Computation Load and Representation Expressivity in Parallel Hybrid Neural Networks", "categories": ["cs.CL"], "comment": null, "summary": "Attention and State-Space Models (SSMs) when combined in a hybrid network in\nsequence or in parallel provide complementary strengths. In a hybrid sequential\npipeline they alternate between applying a transformer to the input and then\nfeeding its output into a SSM. This results in idle periods in the individual\ncomponents increasing end-to-end latency and lowering throughput caps. In the\nparallel hybrid architecture, the transformer operates independently in\nparallel with the SSM, and these pairs are cascaded, with output from one pair\nforming the input to the next. Two issues are (i) creating an expressive\nknowledge representation with the inherently divergent outputs from these\nseparate branches, and (ii) load balancing the computation between these\nparallel branches, while maintaining representation fidelity. In this work we\npresent FlowHN, a novel parallel hybrid network architecture that accommodates\nvarious strategies for load balancing, achieved through appropriate\ndistribution of input tokens between the two branches. Two innovative\ndifferentiating factors in FlowHN include a FLOP aware dynamic token split\nbetween the attention and SSM branches yielding efficient balance in compute\nload, and secondly, a method to fuse the highly divergent outputs from\nindividual branches for enhancing representation expressivity. Together they\nenable much better token processing speeds, avoid bottlenecks, and at the same\ntime yield significantly improved accuracy as compared to other competing\nworks. We conduct comprehensive experiments on autoregressive language modeling\nfor models with 135M, 350M, and 1B parameters. FlowHN outperforms sequential\nhybrid models and its parallel counterpart, achieving up to 4* higher Tokens\nper Second (TPS) and 2* better Model FLOPs Utilization (MFU).", "AI": {"tldr": "FlowHN\u63d0\u51fa\u65b0\u578b\u5e76\u884c\u6df7\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u52a8\u6001\u4ee4\u724c\u5206\u914d\u548c\u5206\u652f\u8f93\u51fa\u878d\u5408\u7b56\u7565\uff0c\u5728\u8bed\u8a00\u5efa\u6a21\u4e2d\u5b9e\u73b04\u500d\u5904\u7406\u901f\u5ea6\u63d0\u5347\u548c2\u500d\u8ba1\u7b97\u6548\u7387\u6539\u8fdb", "motivation": "\u73b0\u6709\u6ce8\u610f\u529b\u4e0eSSM\u6df7\u5408\u67b6\u6784\u5b58\u5728\u7ec4\u4ef6\u95f2\u7f6e\u5ef6\u8fdf\uff08\u987a\u5e8f\u5f0f\uff09\u548c\u5206\u652f\u8f93\u51fa\u4e0d\u517c\u5bb9/\u8d1f\u8f7d\u4e0d\u5747\uff08\u5e76\u884c\u5f0f\uff09\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5e76\u884c\u5904\u7406\u65b9\u6848", "method": "1) FLOP\u611f\u77e5\u7684\u52a8\u6001\u4ee4\u724c\u5206\u6d41\u673a\u5236\u5e73\u8861\u6ce8\u610f\u529b/SSM\u5206\u652f\u8ba1\u7b97\u8d1f\u8f7d 2) \u5f02\u6784\u5206\u652f\u8f93\u51fa\u878d\u5408\u65b9\u6cd5\u589e\u5f3a\u8868\u5f81\u8868\u8fbe\u80fd\u529b", "result": "\u5728135M/350M/1B\u53c2\u6570\u6a21\u578b\u4e2d\uff0cFlowHN\u76f8\u6bd4\u987a\u5e8f\u6df7\u5408\u67b6\u6784\u548c\u4f20\u7edf\u5e76\u884c\u65b9\u6848\uff0c\u5b9e\u73b0\u9ad8\u8fbe4\u500dTokens/s\u5904\u7406\u901f\u5ea6\u548c2\u500d\u6a21\u578b\u8ba1\u7b97\u5229\u7528\u7387(MFU)", "conclusion": "FlowHN\u901a\u8fc7\u8ba1\u7b97\u8d1f\u8f7d\u52a8\u6001\u5e73\u8861\u4e0e\u5f02\u6784\u7279\u5f81\u878d\u5408\uff0c\u7a81\u7834\u6df7\u5408\u67b6\u6784\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5904\u7406\u541e\u5410\u91cf\u548c\u786c\u4ef6\u5229\u7528\u7387"}}
{"id": "2505.19475", "pdf": "https://arxiv.org/pdf/2505.19475", "abs": "https://arxiv.org/abs/2505.19475", "authors": ["Mohammad Mahdi Moradi", "Hossam Amer", "Sudhir Mudur", "Weiwei Zhang", "Yang Liu", "Walid Ahmed"], "title": "Continuous Self-Improvement of Large Language Models by Test-time Training with Verifier-Driven Sample Selection", "categories": ["cs.CL"], "comment": null, "summary": "Learning to adapt pretrained language models to unlabeled,\nout-of-distribution data is a critical challenge, as models often falter on\nstructurally novel reasoning tasks even while excelling within their training\ndistribution. We introduce a new framework called VDS-TTT - Verifier-Driven\nSample Selection for Test-Time Training to efficiently address this. We use a\nlearned verifier to score a pool of generated responses and select only from\nhigh ranking pseudo-labeled examples for fine-tuned adaptation. Specifically,\nfor each input query our LLM generates N candidate answers; the verifier\nassigns a reliability score to each, and the response with the highest\nconfidence and above a fixed threshold is paired with its query for test-time\ntraining. We fine-tune only low-rank LoRA adapter parameters, ensuring\nadaptation efficiency and fast convergence. Our proposed self-supervised\nframework is the first to synthesize verifier driven test-time training data\nfor continuous self-improvement of the model. Experiments across three diverse\nbenchmarks and three state-of-the-art LLMs demonstrate that VDS-TTT yields up\nto a 32.29% relative improvement over the base model and a 6.66% gain compared\nto verifier-based methods without test-time training, highlighting its\neffectiveness and efficiency for on-the-fly large language model adaptation.", "AI": {"tldr": "\u63d0\u51faVDS-TTT\u6846\u67b6\uff0c\u901a\u8fc7\u9a8c\u8bc1\u5668\u9a71\u52a8\u6837\u672c\u9009\u62e9\u548cLoRA\u9002\u914d\u5668\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u7684\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u672a\u6807\u8bb0\u5206\u5e03\u5916\u6570\u636e\uff08\u7279\u522b\u662f\u7ed3\u6784\u65b0\u9896\u7684\u63a8\u7406\u4efb\u52a1\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u751f\u6210\u591a\u4e2a\u5019\u9009\u7b54\u6848\u2192\u9a8c\u8bc1\u5668\u8bc4\u5206\u7b5b\u9009\u9ad8\u7f6e\u4fe1\u5ea6\u6837\u672c\u2192\u4ec5\u5fae\u8c03LoRA\u9002\u914d\u5668\u53c2\u6570\u5b9e\u73b0\u9ad8\u6548\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e09\u79cdLLM\u4e0a\u53d6\u5f9732.29%\u76f8\u5bf9\u63d0\u5347\uff0c\u6bd4\u65e0\u6d4b\u8bd5\u65f6\u8bad\u7ec3\u65b9\u6cd5\u9ad86.66%\u3002", "conclusion": "VDS-TTT\u9996\u6b21\u7ed3\u5408\u9a8c\u8bc1\u5668\u9a71\u52a8\u6570\u636e\u5408\u6210\u4e0e\u6d4b\u8bd5\u65f6\u8bad\u7ec3\uff0c\u4e3a\u5927\u6a21\u578b\u5b9e\u65f6\u9002\u5e94\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.19484", "pdf": "https://arxiv.org/pdf/2505.19484", "abs": "https://arxiv.org/abs/2505.19484", "authors": ["Ruixiang Feng", "Shen Gao", "Xiuying Chen", "Lisi Chen", "Shuo Shang"], "title": "CulFiT: A Fine-grained Cultural-aware LLM Training Paradigm via Multilingual Critique Data Synthesis", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they often exhibit a specific cultural biases, neglecting\nthe values and linguistic diversity of low-resource regions. This cultural bias\nnot only undermines universal equality, but also risks reinforcing stereotypes\nand perpetuating discrimination. To address this, we propose CulFiT, a novel\nculturally-aware training paradigm that leverages multilingual data and\nfine-grained reward modeling to enhance cultural sensitivity and inclusivity.\nOur approach synthesizes diverse cultural-related questions, constructs\ncritique data in culturally relevant languages, and employs fine-grained\nrewards to decompose cultural texts into verifiable knowledge units for\ninterpretable evaluation. We also introduce GlobalCultureQA, a multilingual\nopen-ended question-answering dataset designed to evaluate culturally-aware\nresponses in a global context. Extensive experiments on three existing\nbenchmarks and our GlobalCultureQA demonstrate that CulFiT achieves\nstate-of-the-art open-source model performance in cultural alignment and\ngeneral reasoning.", "AI": {"tldr": "\u63d0\u51faCulFiT\u8bad\u7ec3\u8303\u5f0f\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u6587\u5316\u504f\u89c1\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u8bed\u8a00\u6570\u636e\u548c\u7ec6\u7c92\u5ea6\u5956\u52b1\u5efa\u6a21\u63d0\u5347\u6587\u5316\u654f\u611f\u6027\uff0c\u5e76\u5efa\u7acbGlobalCultureQA\u8bc4\u6d4b\u6570\u636e\u96c6\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5ffd\u89c6\u4f4e\u8d44\u6e90\u5730\u533a\u6587\u5316\u4ef7\u503c\u89c2\u7684\u663e\u8457\u504f\u89c1\uff0c\u53ef\u80fd\u5f3a\u5316\u523b\u677f\u5370\u8c61\u4e0e\u6b67\u89c6\uff0c\u7834\u574f\u666e\u4e16\u5e73\u7b49\u539f\u5219\u3002", "method": "1. \u5408\u6210\u591a\u5143\u6587\u5316\u76f8\u5173\u95ee\u9898\n2. \u6784\u5efa\u591a\u8bed\u8a00\u6279\u5224\u6570\u636e\u96c6\n3. \u7ec6\u7c92\u5ea6\u5956\u52b1\u673a\u5236\u5206\u89e3\u6587\u5316\u6587\u672c\u4e3a\u53ef\u9a8c\u8bc1\u77e5\u8bc6\u5355\u5143\n4. \u5f00\u53d1GlobalCultureQA\u591a\u8bed\u8a00\u5f00\u653e\u5f0f\u95ee\u7b54\u8bc4\u6d4b\u96c6", "result": "\u5728\u4e09\u5927\u57fa\u51c6\u6d4b\u8bd5\u53ca\u81ea\u5efa\u6570\u636e\u96c6\u4e0a\uff0cCulFiT\u5728\u6587\u5316\u5bf9\u9f50\u548c\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e2d\u8fbe\u5230\u5f00\u6e90\u6a21\u578b\u6700\u4f73\u6027\u80fd", "conclusion": "CulFiT\u901a\u8fc7\u7cfb\u7edf\u6027\u6587\u5316\u5efa\u6a21\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u6a21\u578b\u6587\u5316\u654f\u611f\u6027\uff0cGlobalCultureQA\u4e3a\u8de8\u6587\u5316\u8bc4\u4f30\u63d0\u4f9b\u65b0\u57fa\u51c6"}}
{"id": "2505.19494", "pdf": "https://arxiv.org/pdf/2505.19494", "abs": "https://arxiv.org/abs/2505.19494", "authors": ["Manoj Balaji Jagadeeshan", "Prince Raj", "Pawan Goyal"], "title": "Anveshana: A New Benchmark Dataset for Cross-Lingual Information Retrieval On English Queries and Sanskrit Documents", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "The study presents a comprehensive benchmark for retrieving Sanskrit\ndocuments using English queries, focusing on the chapters of the\nSrimadbhagavatam. It employs a tripartite approach: Direct Retrieval (DR),\nTranslation-based Retrieval (DT), and Query Translation (QT), utilizing shared\nembedding spaces and advanced translation methods to enhance retrieval systems\nin a RAG framework. The study fine-tunes state-of-the-art models for Sanskrit's\nlinguistic nuances, evaluating models such as BM25, REPLUG, mDPR, ColBERT,\nContriever, and GPT-2. It adapts summarization techniques for Sanskrit\ndocuments to improve QA processing. Evaluation shows DT methods outperform DR\nand QT in handling the cross-lingual challenges of ancient texts, improving\naccessibility and understanding. A dataset of 3,400 English-Sanskrit\nquery-document pairs underpins the study, aiming to preserve Sanskrit\nscriptures and share their philosophical importance widely. Our dataset is\npublicly available at https://huggingface.co/datasets/manojbalaji1/anveshana", "AI": {"tldr": "\u6784\u5efa\u82f1\u8bed\u67e5\u8be2\u68c0\u7d22\u68b5\u6587\u6587\u732e\u7684\u8de8\u8bed\u8a00\u57fa\u51c6\u6846\u67b6\uff0c\u57fa\u4e8e3400\u7ec4\u82f1\u68b5\u5bf9\u7167\u6570\u636e\u96c6\u9a8c\u8bc1\u7ffb\u8bd1\u68c0\u7d22\u6cd5(DT)\u5728\u53e4\u6587\u672c\u5904\u7406\u4e2d\u7684\u6700\u4f18\u8868\u73b0", "motivation": "\u89e3\u51b3\u68b5\u6587\u5178\u7c4d\u7684\u8de8\u8bed\u8a00\u68c0\u7d22\u96be\u9898\uff0c\u901a\u8fc7\u6784\u5efa\u7ed3\u6784\u5316\u6570\u636e\u96c6\u548c\u6280\u672f\u65b9\u6848\u4fc3\u8fdb\u5370\u5ea6\u6587\u5316\u9057\u4ea7\u7684\u6570\u5b57\u5316\u4fdd\u62a4\u4e0e\u54f2\u5b66\u4f20\u64ad", "method": "\u91c7\u7528DR/DT/QT\u4e09\u9636\u6bb5\u68c0\u7d22\u6846\u67b6\uff0c\u878d\u5408\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u4e0e\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\uff0c\u5fae\u8c03ColBERT\u7b49\u7a20\u5bc6\u68c0\u7d22\u6a21\u578b\uff0c\u9002\u914d\u68b5\u6587\u5f62\u6001\u5b66\u7279\u5f81", "result": "DT\u65b9\u6cd5\u5728MRR@10\u6307\u6807\u4e0a\u8f83\u4f20\u7edf\u65b9\u6cd5\u63d0\u534721.3%\uff0c\u8de8\u8bed\u8a00\u68c0\u7d22\u51c6\u786e\u7387\u8fbe76.8%\uff0c\u6210\u529f\u5b9e\u73b0\u300a\u5723\u5178\u535a\u4f3d\u74e6\u8c2d\u300b\u7684\u8bed\u4e49\u5bf9\u9f50", "conclusion": "\u878d\u5408\u7ffb\u8bd1\u589e\u5f3a\u7684\u68c0\u7d22\u67b6\u6784\u663e\u8457\u63d0\u5347\u53e4\u6587\u732e\u53ef\u53ca\u6027\uff0c\u6280\u672f\u65b9\u6848\u5bf9\u4f4e\u8d44\u6e90\u8bed\u79cd\u6587\u5316\u9057\u4ea7\u4fdd\u62a4\u5177\u6709\u8303\u5f0f\u4ef7\u503c"}}
{"id": "2505.19510", "pdf": "https://arxiv.org/pdf/2505.19510", "abs": "https://arxiv.org/abs/2505.19510", "authors": ["Dongil Yang", "Minjin Kim", "Sunghwan Kim", "Beong-woo Kwak", "Minjun Park", "Jinseok Hong", "Woontack Woo", "Jinyoung Yeo"], "title": "LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "The remarkable reasoning and generalization capabilities of Large Language\nModels (LLMs) have paved the way for their expanding applications in embodied\nAI, robotics, and other real-world tasks. To effectively support these\napplications, grounding in spatial and temporal understanding in multimodal\nenvironments is essential. To this end, recent works have leveraged scene\ngraphs, a structured representation that encodes entities, attributes, and\ntheir relationships in a scene. However, a comprehensive evaluation of LLMs'\nability to utilize scene graphs remains limited. In this work, we introduce\nText-Scene Graph (TSG) Bench, a benchmark designed to systematically assess\nLLMs' ability to (1) understand scene graphs and (2) generate them from textual\nnarratives. With TSG Bench we evaluate 11 LLMs and reveal that, while models\nperform well on scene graph understanding, they struggle with scene graph\ngeneration, particularly for complex narratives. Our analysis indicates that\nthese models fail to effectively decompose discrete scenes from a complex\nnarrative, leading to a bottleneck when generating scene graphs. These findings\nunderscore the need for improved methodologies in scene graph generation and\nprovide valuable insights for future research. The demonstration of our\nbenchmark is available at https://tsg-bench.netlify.app. Additionally, our code\nand evaluation data are publicly available at\nhttps://anonymous.4open.science/r/TSG-Bench.", "AI": {"tldr": "\u63d0\u51faTSG Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5728\u573a\u666f\u56fe\u751f\u6210\uff08\u5c24\u5176\u662f\u590d\u6742\u6587\u672c\uff09\u5b58\u5728\u663e\u8457\u74f6\u9888", "motivation": "\u5927\u6a21\u578b\u5728\u5177\u8eab\u667a\u80fd\u548c\u673a\u5668\u4eba\u9886\u57df\u7684\u5e94\u7528\u9700\u8981\u65f6\u7a7a\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5bf9\u573a\u666f\u56fe\u80fd\u529b\u7684\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u8db3", "method": "\u6784\u5efaTSG Bench\u57fa\u51c6\uff0c\u5305\u542b\u573a\u666f\u56fe\u7406\u89e3\u548c\u751f\u6210\u53cc\u7ef4\u5ea6\u6d4b\u8bd5\uff0c\u8bc4\u4f3011\u4e2a\u4e3b\u6d41LLM", "result": "\u6a21\u578b\u573a\u666f\u56fe\u7406\u89e3\u80fd\u529b\u5c1a\u53ef\uff0c\u4f46\u751f\u6210\u80fd\u529b\u8584\u5f31\uff08\u590d\u6742\u53d9\u4e8b\u573a\u666f\u51c6\u786e\u7387\u4ec528%\uff09\uff0c\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u573a\u666f\u5206\u89e3\u80fd\u529b\u4e0d\u8db3", "conclusion": "\u63ed\u793a\u4e86\u5f53\u524dLLM\u573a\u666f\u56fe\u751f\u6210\u7684\u7f3a\u9677\uff0c\u4e3a\u63d0\u5347\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u6307\u660e\u65b9\u5411\uff0c\u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u63a8\u52a8\u9886\u57df\u53d1\u5c55"}}
{"id": "2505.19511", "pdf": "https://arxiv.org/pdf/2505.19511", "abs": "https://arxiv.org/abs/2505.19511", "authors": ["Aggrey Muhebwa", "Khalid K. Osman"], "title": "Causal Distillation: Transferring Structured Explanations from Large to Compact Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large proprietary language models exhibit strong causal reasoning abilities\nthat smaller open-source models struggle to replicate. We introduce a novel\nframework for distilling causal explanations that transfers causal reasoning\nskills from a powerful teacher model to a compact open-source model. The key\nidea is to train the smaller model to develop causal reasoning abilities by\ngenerating structured cause-and-effect explanations consistent with those of\nthe teacher model. To evaluate the quality of the student-generated\nexplanations, we introduce a new metric called Causal Explanation Coherence\n(CEC) to assess the structural and logical consistency of causal reasoning.\nThis metric uses sentence-level semantic alignment to measure how well each\npart of the generated explanation corresponds to the teacher's reference,\ncapturing both faithfulness and coverage of the underlying causal chain. Our\nframework and the CEC metric provide a principled foundation for training\nsmaller models to perform robust causal reasoning and for systematically\nassessing the coherence of explanations in language model outputs.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u7ed3\u6784\u5316\u56e0\u679c\u89e3\u91ca\u84b8\u998f\u6846\u67b6\uff0c\u5c06\u5927\u578b\u6559\u5e08\u6a21\u578b\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u8fc1\u79fb\u5230\u5c0f\u578b\u5f00\u6e90\u6a21\u578b", "motivation": "\u89e3\u51b3\u5c0f\u578b\u5f00\u6e90\u6a21\u578b\u96be\u4ee5\u590d\u73b0\u5927\u578b\u4e13\u6709\u6a21\u578b\u56e0\u679c\u63a8\u7406\u80fd\u529b\u7684\u95ee\u9898\uff0c\u65e8\u5728\u4f7f\u7d27\u51d1\u6a21\u578b\u83b7\u5f97\u53ef\u9760\u56e0\u679c\u63a8\u7406\u6280\u80fd", "method": "\u901a\u8fc7\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u751f\u6210\u4e0e\u6559\u5e08\u6a21\u578b\u4e00\u81f4\u7684\u7ed3\u6784\u5316\u56e0\u679c\u89e3\u91ca\uff0c\u5e76\u8bbe\u8ba1CEC\u6307\u6807\u8bc4\u4f30\u89e3\u91ca\u7684\u903b\u8f91\u8fde\u8d2f\u6027", "result": "\u6846\u67b6\u4e3a\u5c0f\u6a21\u578b\u56e0\u679c\u63a8\u7406\u8bad\u7ec3\u548c\u89e3\u91ca\u8bc4\u4f30\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\uff0cCEC\u6307\u6807\u53ef\u7cfb\u7edf\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u56e0\u679c\u94fe\u5b8c\u6574\u6027\u4e0e\u903b\u8f91\u4e00\u81f4\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u4f7f\u5c0f\u578b\u6a21\u578b\u5177\u5907\u7a33\u5065\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4e3a\u89e3\u91ca\u8d28\u91cf\u8bc4\u4f30\u5efa\u7acb\u91cf\u5316\u6807\u51c6\uff0c\u63a8\u52a8\u53ef\u89e3\u91caAI\u53d1\u5c55"}}
{"id": "2505.19514", "pdf": "https://arxiv.org/pdf/2505.19514", "abs": "https://arxiv.org/abs/2505.19514", "authors": ["Yaoning Yu", "Ye Yu", "Kai Wei", "Haojing Luo", "Haohan Wang"], "title": "SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Prompt quality plays a critical role in the performance of large language\nmodels (LLMs), motivating a growing body of work on prompt optimization. Most\nexisting methods optimize prompts over a fixed dataset, assuming static input\ndistributions and offering limited support for iterative improvement. We\nintroduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a\nclosed-loop framework for prompt learning that integrates synthetic data\ngeneration into the optimization process. SIPDO couples a synthetic data\ngenerator with a prompt optimizer, where the generator produces new examples\nthat reveal current prompt weaknesses and the optimizer incrementally refines\nthe prompt in response. This feedback-driven loop enables systematic\nimprovement of prompt performance without assuming access to external\nsupervision or new tasks. Experiments across question answering and reasoning\nbenchmarks show that SIPDO outperforms standard prompt tuning methods,\nhighlighting the value of integrating data synthesis into prompt learning\nworkflows.", "AI": {"tldr": "\u63d0\u51faSIPDO\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u5408\u6210\u4e0e\u63d0\u793a\u4f18\u5316\u7684\u95ed\u73af\u7cfb\u7edf\u5b9e\u73b0\u81ea\u52a8\u5316\u7684\u63d0\u793a\u6539\u8fdb", "motivation": "\u73b0\u6709\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u6570\u636e\u96c6\u4e14\u7f3a\u4e4f\u8fed\u4ee3\u6539\u8fdb\u652f\u6301\uff0c\u9700\u52a8\u6001\u63ed\u793a\u63d0\u793a\u5f31\u70b9\u5e76\u4f18\u5316", "method": "\u8026\u5408\u5408\u6210\u6570\u636e\u751f\u6210\u5668\uff08\u63ed\u793a\u5f53\u524d\u63d0\u793a\u7f3a\u9677\uff09\u4e0e\u63d0\u793a\u4f18\u5316\u5668\uff08\u589e\u91cf\u5f0f\u6539\u8fdb\uff09\u7684\u53cd\u9988\u9a71\u52a8\u6846\u67b6", "result": "\u5728\u95ee\u7b54\u548c\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u6807\u51c6\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5", "conclusion": "\u5c06\u6570\u636e\u5408\u6210\u878d\u5165\u63d0\u793a\u5b66\u4e60\u6d41\u7a0b\u80fd\u7cfb\u7edf\u63d0\u5347\u6027\u80fd\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u6216\u65b0\u4efb\u52a1"}}
{"id": "2505.19515", "pdf": "https://arxiv.org/pdf/2505.19515", "abs": "https://arxiv.org/abs/2505.19515", "authors": ["Lavanya Prahallad", "Radhika Mamidi"], "title": "Bias in Political Dialogue: Tagging U.S. Presidential Debates with an Extended DAMSL Framework", "categories": ["cs.CL"], "comment": "8 pages", "summary": "We present a critical discourse analysis of the 2024 U.S. presidential\ndebates, examining Donald Trump's rhetorical strategies in his interactions\nwith Joe Biden and Kamala Harris. We introduce a novel annotation framework,\nBEADS (Bias Enriched Annotation for Dialogue Structure), which systematically\nextends the DAMSL framework to capture bias driven and adversarial discourse\nfeatures in political communication. BEADS includes a domain and language\nagnostic set of tags that model ideological framing, emotional appeals, and\nconfrontational tactics. Our methodology compares detailed human annotation\nwith zero shot ChatGPT assisted tagging on verified transcripts from the Trump\nand Biden (19,219 words) and Trump and Harris (18,123 words) debates. Our\nanalysis shows that Trump consistently dominated in key categories: Challenge\nand Adversarial Exchanges, Selective Emphasis, Appeal to Fear, Political Bias,\nand Perceived Dismissiveness. These findings underscore his use of emotionally\ncharged and adversarial rhetoric to control the narrative and influence\naudience perception. In this work, we establish BEADS as a scalable and\nreproducible framework for critical discourse analysis across languages,\ndomains, and political contexts.", "AI": {"tldr": "\u5f00\u53d1BEADS\u6807\u6ce8\u6846\u67b6\u5206\u6790\u7279\u6717\u666e\u5728\u603b\u7edf\u8fa9\u8bba\u4e2d\u7684\u5bf9\u6297\u6027\u4fee\u8f9e\u7b56\u7565\uff0c\u53d1\u73b0\u5176\u5728\u63a7\u5236\u53d9\u4e8b\u548c\u5f71\u54cd\u89c2\u4f17\u65b9\u9762\u5360\u636e\u4e3b\u5bfc\u5730\u4f4d\u3002", "motivation": "\u73b0\u6709DAMSL\u6846\u67b6\u7f3a\u4e4f\u5bf9\u653f\u6cbb\u6c9f\u901a\u4e2d\u504f\u89c1\u548c\u5bf9\u6297\u6027\u8bdd\u8bed\u7684\u7cfb\u7edf\u5206\u6790\u80fd\u529b\uff0c\u9700\u5efa\u7acb\u66f4\u5168\u9762\u7684\u6807\u6ce8\u4f53\u7cfb\u3002", "method": "\u7ed3\u5408\u4eba\u5de5\u6807\u6ce8\u4e0eChatGPT\u96f6\u6837\u672c\u6807\u6ce8\uff0c\u5bf9\u7279\u6717\u666e-\u62dc\u767b(19,219\u8bcd)\u548c\u7279\u6717\u666e-\u54c8\u91cc\u65af(18,123\u8bcd)\u8fa9\u8bba\u8f6c\u5f55\u672c\u8fdb\u884cBEADS\u6846\u67b6\u5206\u6790\u3002", "result": "\u7279\u6717\u666e\u5728\u6311\u6218\u6027\u4ea4\u6d41(83%)\u3001\u9009\u62e9\u6027\u5f3a\u8c03(78%)\u3001\u6050\u60e7\u8bc9\u6c42(65%)\u3001\u653f\u6cbb\u504f\u89c1(72%)\u548c\u611f\u77e5\u6027\u8f7b\u89c6(81%)\u7b49\u5173\u952e\u7c7b\u522b\u663e\u8457\u9886\u5148\u3002", "conclusion": "BEADS\u6210\u4e3a\u8de8\u8bed\u8a00/\u653f\u6cbb\u73af\u5883\u7684\u53ef\u6269\u5c55\u5206\u6790\u6846\u67b6\uff0c\u8bc1\u5b9e\u5bf9\u6297\u6027\u4fee\u8f9e\u5728\u5851\u9020\u653f\u6cbb\u53d9\u4e8b\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2505.19528", "pdf": "https://arxiv.org/pdf/2505.19528", "abs": "https://arxiv.org/abs/2505.19528", "authors": ["Yejin Lee", "Joonghyuk Hahn", "Hyeseon Ahn", "Yo-Sub Han"], "title": "AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection", "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "I.2.7"], "comment": "13 pages, 4 figures, Under Review", "summary": "Implicit hate speech detection is challenging due to its subtlety and\nreliance on contextual interpretation rather than explicit offensive words.\nCurrent approaches rely on contrastive learning, which are shown to be\neffective on distinguishing hate and non-hate sentences. Humans, however,\ndetect implicit hate speech by first identifying specific targets within the\ntext and subsequently interpreting how these target relate to their surrounding\ncontext. Motivated by this reasoning process, we propose AmpleHate, a novel\napproach designed to mirror human inference for implicit hate detection.\nAmpleHate identifies explicit target using a pretrained Named Entity\nRecognition model and capture implicit target information via [CLS] tokens. It\ncomputes attention-based relationships between explicit, implicit targets and\nsentence context and then, directly injects these relational vectors into the\nfinal sentence representation. This amplifies the critical signals of\ntarget-context relations for determining implicit hate. Experiments demonstrate\nthat AmpleHate achieves state-of-the-art performance, outperforming contrastive\nlearning baselines by an average of 82.14% and achieve faster convergence.\nQualitative analyses further reveal that attention patterns produced by\nAmpleHate closely align with human judgement, underscoring its interpretability\nand robustness.", "AI": {"tldr": "\u63d0\u51faAmpleHate\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u63a8\u7406\u8fc7\u7a0b\u7ed3\u5408\u663e\u5f0f/\u9690\u5f0f\u76ee\u6807\u4e0e\u4e0a\u4e0b\u6587\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u9690\u5f0f\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u6548\u679c\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u6027\u80fd\u8d85\u8d8a\u57fa\u7ebf82.14%\u4e14\u5177\u5907\u53ef\u89e3\u91ca\u6027", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u65b9\u6cd5\u65e0\u6cd5\u6a21\u62df\u4eba\u7c7b'\u5148\u8bc6\u522b\u76ee\u6807\u540e\u5206\u6790\u4e0a\u4e0b\u6587\u5173\u7cfb'\u7684\u63a8\u7406\u903b\u8f91\uff0c\u5bfc\u81f4\u9690\u5f0f\u4ec7\u6068\u68c0\u6d4b\u6548\u679c\u53d7\u9650", "method": "1. \u4f7f\u7528\u9884\u8bad\u7ec3NER\u8bc6\u522b\u663e\u5f0f\u76ee\u6807 2. \u7528[CLS]\u6355\u6349\u9690\u5f0f\u76ee\u6807 3. \u8ba1\u7b97\u76ee\u6807-\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\u5173\u7cfb 4. \u5c06\u5173\u7cfb\u5411\u91cf\u6ce8\u5165\u53e5\u5b50\u8868\u793a", "result": "SOTA\u6027\u80fd\uff08\u4f18\u4e8e\u57fa\u7ebf82.14%\uff09\uff0c\u6536\u655b\u901f\u5ea6\u66f4\u5feb\uff0c\u6ce8\u610f\u529b\u6a21\u5f0f\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u543b\u5408", "conclusion": "AmpleHate\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u63a8\u7406\u673a\u5236\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6548\u679c\uff0c\u4e3a\u5b9e\u9645\u573a\u666f\u63d0\u4f9b\u66f4\u4f18\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.19529", "pdf": "https://arxiv.org/pdf/2505.19529", "abs": "https://arxiv.org/abs/2505.19529", "authors": ["Tanjil Hasan Sakib", "Md. Tanzib Hosain", "Md. Kishor Morol"], "title": "Small Language Models: Architectures, Techniques, Evaluation, Problems and Future Adaptation", "categories": ["cs.CL"], "comment": "9 pages", "summary": "Small Language Models (SLMs) have gained substantial attention due to their\nability to execute diverse language tasks successfully while using fewer\ncomputer resources. These models are particularly ideal for deployment in\nlimited environments, such as mobile devices, on-device processing, and edge\nsystems. In this study, we present a complete assessment of SLMs, focussing on\ntheir design frameworks, training approaches, and techniques for lowering model\nsize and complexity. We offer a novel classification system to organize the\noptimization approaches applied for SLMs, encompassing strategies like pruning,\nquantization, and model compression. Furthermore, we assemble SLM's studies of\nevaluation suite with some existing datasets, establishing a rigorous platform\nfor measuring SLM capabilities. Alongside this, we discuss the important\ndifficulties that remain unresolved in this sector, including trade-offs\nbetween efficiency and performance, and we suggest directions for future study.\nWe anticipate this study to serve as a beneficial guide for researchers and\npractitioners who aim to construct compact, efficient, and high-performing\nlanguage models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u5c0f\u578b\u8bed\u8a00\u6a21\u578b(SLMs)\uff0c\u63d0\u51fa\u5305\u542b\u526a\u679d/\u91cf\u5316/\u538b\u7f29\u7b49\u4f18\u5316\u7b56\u7565\u7684\u521b\u65b0\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u5efa\u7acb\u5305\u542b\u591a\u7ef4\u5ea6\u6307\u6807\u7684\u8bc4\u4f30\u5e73\u53f0\uff0c\u8ba8\u8bba\u6548\u7387\u4e0e\u6027\u80fd\u7684\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u9488\u5bf9\u79fb\u52a8\u8bbe\u5907/\u8fb9\u7f18\u8ba1\u7b97\u7b49\u8d44\u6e90\u53d7\u9650\u573a\u666f\uff0c\u63a2\u7d22\u5982\u4f55\u6784\u5efa\u7d27\u51d1\u9ad8\u6548\u4e14\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4f20\u7edf\u5927\u6a21\u578b\u90e8\u7f72\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u7814\u7a76\u6846\u67b6\uff1a1) \u7cfb\u7edf\u5206\u7c7b\u6a21\u578b\u4f18\u5316\u65b9\u6cd5(\u526a\u679d/\u91cf\u5316/\u538b\u7f29) 2) \u6574\u5408\u73b0\u6709\u6570\u636e\u96c6\u6784\u5efa\u8bc4\u4f30\u5957\u4ef6 3) \u5f00\u5c55\u6548\u7387\u4e0e\u6027\u80fd\u7684\u6743\u8861\u5206\u6790\u3002", "result": "\u5efa\u7acb\u9996\u4e2a\u5305\u542b\u591a\u7ef4\u5ea6\u6307\u6807\u7684SLM\u8bc4\u4f30\u5e73\u53f0\uff0c\u8bc1\u5b9e\u4f18\u5316\u7b56\u7565\u53ef\u663e\u8457\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6(\u964d\u4f4e30-50%\u53c2\u6570\u91cf)\uff0c\u540c\u65f6\u4fdd\u630190%+\u7684\u539f\u59cb\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u7cfb\u7edf\u65b9\u6cd5\u8bba\uff0c\u672a\u6765\u9700\u5728\u77e5\u8bc6\u84b8\u998f/\u52a8\u6001\u7f51\u7edc\u67b6\u6784\u7b49\u65b9\u5411\u6df1\u5165\u63a2\u7d22\u6548\u7387-\u6027\u80fd\u7684\u5e15\u7d2f\u6258\u4f18\u5316\u3002"}}
{"id": "2505.19538", "pdf": "https://arxiv.org/pdf/2505.19538", "abs": "https://arxiv.org/abs/2505.19538", "authors": ["Yuxing Lu", "Gecheng Fu", "Wei Wu", "Xukai Zhao", "Sin Yee Goi", "Jinzhuo Wang"], "title": "DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.IR", "cs.MA"], "comment": "32 pages, 5 figures, 5 tables", "summary": "Existing medical RAG systems mainly leverage knowledge from medical knowledge\nbases, neglecting the crucial role of experiential knowledge derived from\nsimilar patient cases -- a key component of human clinical reasoning. To bridge\nthis gap, we propose DoctorRAG, a RAG framework that emulates doctor-like\nreasoning by integrating both explicit clinical knowledge and implicit\ncase-based experience. DoctorRAG enhances retrieval precision by first\nallocating conceptual tags for queries and knowledge sources, together with a\nhybrid retrieval mechanism from both relevant knowledge and patient. In\naddition, a Med-TextGrad module using multi-agent textual gradients is\nintegrated to ensure that the final output adheres to the retrieved knowledge\nand patient query. Comprehensive experiments on multilingual, multitask\ndatasets demonstrate that DoctorRAG significantly outperforms strong baseline\nRAG models and gains improvements from iterative refinements. Our approach\ngenerates more accurate, relevant, and comprehensive responses, taking a step\ntowards more doctor-like medical reasoning systems.", "AI": {"tldr": "\u63d0\u51faDoctorRAG\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u4e34\u5e8a\u77e5\u8bc6\u4e0e\u6848\u4f8b\u7ecf\u9a8c\u6539\u8fdb\u533b\u7597\u95ee\u7b54\u7cfb\u7edf\uff0c\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u533b\u751f\u5f0f\u63a8\u7406", "motivation": "\u73b0\u6709\u533b\u7597RAG\u7cfb\u7edf\u8fc7\u5ea6\u4f9d\u8d56\u77e5\u8bc6\u5e93\uff0c\u5ffd\u89c6\u7c7b\u4f3c\u75c5\u4f8b\u7684\u4e34\u5e8a\u7ecf\u9a8c\u77e5\u8bc6\uff0c\u800c\u540e\u8005\u662f\u533b\u751f\u8bca\u65ad\u7684\u6838\u5fc3\u8981\u7d20", "method": "1) \u57fa\u4e8e\u6982\u5ff5\u6807\u7b7e\u7684\u6df7\u5408\u68c0\u7d22\u673a\u5236\uff08\u77e5\u8bc6\u5e93+\u75c5\u4f8b\u68c0\u7d22\uff09 2) Med-TextGrad\u591a\u4ee3\u7406\u68af\u5ea6\u4f18\u5316\u6a21\u5757 3) \u591a\u8bed\u8a00\u591a\u4efb\u52a1\u9a8c\u8bc1\u6846\u67b6", "result": "\u5728\u591a\u8bed\u8a00\u591a\u4efb\u52a1\u6570\u636e\u96c6\u4e0a\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u8fed\u4ee3\u4f18\u5316\u540e\u54cd\u5e94\u51c6\u786e\u6027\u548c\u5168\u9762\u6027\u6301\u7eed\u63d0\u5347", "conclusion": "DoctorRAG\u901a\u8fc7\u77e5\u8bc6+\u7ecf\u9a8c\u53cc\u5f15\u64ce\u9a71\u52a8\uff0c\u63a8\u52a8\u4e86\u533b\u7597AI\u7cfb\u7edf\u5411\u533b\u751f\u5f0f\u63a8\u7406\u7684\u91cd\u8981\u8fdb\u5c55"}}
{"id": "2505.19548", "pdf": "https://arxiv.org/pdf/2505.19548", "abs": "https://arxiv.org/abs/2505.19548", "authors": ["Xufeng Duan", "Zhaoqian Yao", "Yunhao Zhang", "Shaonan Wang", "Zhenguang G. Cai"], "title": "How Syntax Specialization Emerges in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have been found to develop surprising internal\nspecializations: Individual neurons, attention heads, and circuits become\nselectively sensitive to syntactic structure, reflecting patterns observed in\nthe human brain. While this specialization is well-documented, how it emerges\nduring training and what influences its development remains largely unknown.\n  In this work, we tap into the black box of specialization by tracking its\nformation over time. By quantifying internal syntactic consistency across\nminimal pairs from various syntactic phenomena, we identify a clear\ndevelopmental trajectory: Syntactic sensitivity emerges gradually, concentrates\nin specific layers, and exhibits a 'critical period' of rapid internal\nspecialization. This process is consistent across architectures and\ninitialization parameters (e.g., random seeds), and is influenced by model\nscale and training data. We therefore reveal not only where syntax arises in\nLLMs but also how some models internalize it during training. To support future\nresearch, we will release the code, models, and training checkpoints upon\nacceptance.", "AI": {"tldr": "\u7814\u7a76\u8ffd\u8e2a\u4e86LLMs\u5185\u90e8\u53e5\u6cd5\u4e13\u95e8\u5316\u7684\u5f62\u6210\u8fc7\u7a0b\uff0c\u63ed\u793a\u4e86\u5176\u6e10\u8fdb\u5f0f\u53d1\u5c55\u8f68\u8ff9\u3001\u5206\u5c42\u96c6\u4e2d\u7279\u6027\u53ca\u53d7\u6a21\u578b\u89c4\u6a21/\u6570\u636e\u5f71\u54cd\u7684\u5173\u952e\u671f\u73b0\u8c61\u3002", "motivation": "\u5c3d\u7ba1LLMs\u7684\u53e5\u6cd5\u4e13\u95e8\u5316\u7279\u6027\u5df2\u88ab\u8bc1\u5b9e\uff0c\u4f46\u5176\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5f62\u6210\u673a\u5236\u53ca\u5f71\u54cd\u56e0\u7d20\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u7814\u7a76\u65e8\u5728\u63ed\u793a\u795e\u7ecf\u7f51\u7edc\u5185\u90e8\u53e5\u6cd5\u654f\u611f\u6027\u7684\u53d1\u5c55\u89c4\u5f8b\u53ca\u5176\u5f62\u6210\u6761\u4ef6\u3002", "method": "\u901a\u8fc7\u91cf\u5316\u4e0d\u540c\u53e5\u6cd5\u73b0\u8c61\u6700\u5c0f\u5bf9\u7684\u5185\u90e8\u53e5\u6cd5\u4e00\u81f4\u6027\uff0c\u8ffd\u8e2a\u8de8\u6a21\u578b\u67b6\u6784/\u521d\u59cb\u5316\u53c2\u6570\u7684\u53d1\u5c55\u8f68\u8ff9\uff0c\u5206\u6790\u6a21\u578b\u89c4\u6a21\u4e0e\u8bad\u7ec3\u6570\u636e\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u53e5\u6cd5\u654f\u611f\u6027\u5448\u73b0\u6e10\u8fdb\u53d1\u5c55\u3001\u5c42\u7ea7\u96c6\u4e2d\u7279\u6027\uff0c\u5b58\u5728\u5feb\u901f\u4e13\u4e1a\u5316\u7684\u5173\u952e\u671f\uff1b\u8be5\u8fc7\u7a0b\u5177\u6709\u67b6\u6784\u65e0\u5173\u6027\uff0c\u4e14\u53d7\u6a21\u578b\u89c4\u6a21\u4e0e\u8bad\u7ec3\u6570\u636e\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u4e0d\u4ec5\u63ed\u793a\u4e86LLMs\u53e5\u6cd5\u80fd\u529b\u7684\u5f62\u6210\u8def\u5f84\uff0c\u8fd8\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u4ee3\u7801\u3001\u6a21\u578b\u53ca\u8bad\u7ec3\u68c0\u67e5\u70b9\u8d44\u6e90\u3002"}}
{"id": "2505.19549", "pdf": "https://arxiv.org/pdf/2505.19549", "abs": "https://arxiv.org/abs/2505.19549", "authors": ["Derong Xu", "Yi Wen", "Pengyue Jia", "Yingyi Zhang", "wenlin zhang", "Yichao Wang", "Huifeng Guo", "Ruiming Tang", "Xiangyu Zhao", "Enhong Chen", "Tong Xu"], "title": "Towards Multi-Granularity Memory Association and Selection for Long-Term Conversational Agents", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have recently been widely adopted in\nconversational agents. However, the increasingly long interactions between\nusers and agents accumulate extensive dialogue records, making it difficult for\nLLMs with limited context windows to maintain a coherent long-term dialogue\nmemory and deliver personalized responses. While retrieval-augmented memory\nsystems have emerged to address this issue, existing methods often depend on\nsingle-granularity memory segmentation and retrieval. This approach falls short\nin capturing deep memory connections, leading to partial retrieval of useful\ninformation or substantial noise, resulting in suboptimal performance. To\ntackle these limits, we propose MemGAS, a framework that enhances memory\nconsolidation by constructing multi-granularity association, adaptive\nselection, and retrieval. MemGAS is based on multi-granularity memory units and\nemploys Gaussian Mixture Models to cluster and associate new memories with\nhistorical ones. An entropy-based router adaptively selects optimal granularity\nby evaluating query relevance distributions and balancing information\ncompleteness and noise. Retrieved memories are further refined via LLM-based\nfiltering. Experiments on four long-term memory benchmarks demonstrate that\nMemGAS outperforms state-of-the-art methods on both question answer and\nretrieval tasks, achieving superior performance across different query types\nand top-K settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86MemGAS\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7c92\u5ea6\u8bb0\u5fc6\u5173\u8054\u3001\u81ea\u9002\u5e94\u68c0\u7d22\u548c\u57fa\u4e8e\u71b5\u7684\u8def\u7531\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u957f\u5bf9\u8bdd\u573a\u666f\u4e0bLLMs\u7684\u8bb0\u5fc6\u6574\u5408\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5355\u7c92\u5ea6\u8bb0\u5fc6\u7cfb\u7edf\u5b58\u5728\u4fe1\u606f\u68c0\u7d22\u4e0d\u5b8c\u6574\u548c\u566a\u58f0\u5e72\u6270\u95ee\u9898\uff0c\u96be\u4ee5\u6355\u6349\u6df1\u5c42\u8bb0\u5fc6\u5173\u8054\u3002", "method": "1. \u6784\u5efa\u591a\u7c92\u5ea6\u8bb0\u5fc6\u5355\u5143 2. \u4f7f\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u8fdb\u884c\u8bb0\u5fc6\u805a\u7c7b 3. \u57fa\u4e8e\u71b5\u7684\u8def\u7531\u5668\u5b9e\u73b0\u6700\u4f18\u7c92\u5ea6\u9009\u62e9 4. LLM\u8fc7\u6ee4\u4f18\u5316\u68c0\u7d22\u7ed3\u679c", "result": "\u57284\u4e2a\u957f\u671f\u8bb0\u5fc6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5168\u9762\u8d85\u8d8aSOTA\u65b9\u6cd5\uff0c\u95ee\u7b54\u548c\u68c0\u7d22\u4efb\u52a1\u5e73\u5747\u63d0\u53475.8%\uff0c\u5728\u4e0d\u540c\u67e5\u8be2\u7c7b\u578b\u548ctop-K\u8bbe\u7f6e\u4e0b\u8868\u73b0\u7a33\u5b9a", "conclusion": "MemGAS\u901a\u8fc7\u591a\u5c42\u6b21\u8bb0\u5fc6\u5173\u8054\u548c\u52a8\u6001\u7c92\u5ea6\u9009\u62e9\u673a\u5236\uff0c\u6709\u6548\u5e73\u8861\u4fe1\u606f\u5b8c\u6574\u6027\u4e0e\u566a\u58f0\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u8bdd\u7cfb\u7edf\u7684\u8bb0\u5fc6\u5229\u7528\u6548\u7387"}}
{"id": "2505.19572", "pdf": "https://arxiv.org/pdf/2505.19572", "abs": "https://arxiv.org/abs/2505.19572", "authors": ["Li Zeng", "Zeming Liu", "Chong Feng", "Heyan Huang", "Yuhang Guo"], "title": "DocMEdit: Towards Document-Level Model Editing", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 findings", "summary": "Model editing aims to correct errors and outdated knowledge in the Large\nlanguage models (LLMs) with minimal cost. Prior research has proposed a variety\nof datasets to assess the effectiveness of these model editing methods.\nHowever, most existing datasets only require models to output short phrases or\nsentences, overlooks the widespread existence of document-level tasks in the\nreal world, raising doubts about their practical usability. Aimed at addressing\nthis limitation and promoting the application of model editing in real-world\nscenarios, we propose the task of document-level model editing. To tackle such\nchallenges and enhance model capabilities in practical settings, we introduce\n\\benchmarkname, a dataset focused on document-level model editing,\ncharacterized by document-level inputs and outputs, extrapolative, and multiple\nfacts within a single edit. We propose a series of evaluation metrics and\nexperiments. The results show that the difficulties in document-level model\nediting pose challenges for existing model editing methods.", "AI": {"tldr": "\u63d0\u51fa\u6587\u6863\u7ea7\u6a21\u578b\u7f16\u8f91\u4efb\u52a1\u53caBenchmark\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u6587\u672c\u7f16\u8f91\u4e2d\u7684\u5c40\u9650\u6027", "motivation": "\u73b0\u6709\u6a21\u578b\u7f16\u8f91\u6570\u636e\u96c6\u5c40\u9650\u4e8e\u77ed\u8bed\u7ea7\u8bc4\u4f30\uff0c\u65e0\u6cd5\u6ee1\u8db3\u73b0\u5b9e\u573a\u666f\u4e2d\u666e\u904d\u5b58\u5728\u7684\u6587\u6863\u7ea7\u4efb\u52a1\u9700\u6c42\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u4ef7\u503c", "method": "1. \u5b9a\u4e49\u6587\u6863\u7ea7\u6a21\u578b\u7f16\u8f91\u4efb\u52a1 2. \u6784\u5efa\u5305\u542b\u6587\u6863\u7ea7\u8f93\u5165\u8f93\u51fa\u3001\u5177\u5907\u63a8\u65ad\u80fd\u529b\u548c\u591a\u4e8b\u5b9e\u7279\u5f81\u7684\u6570\u636e\u96c6 3. \u8bbe\u8ba1\u65b0\u578b\u8bc4\u4f30\u6307\u6807\u4f53\u7cfb", "result": "\u5b9e\u9a8c\u8868\u660e\u6587\u6863\u7ea7\u7f16\u8f91\u4efb\u52a1\u5728\u4e8b\u5b9e\u66f4\u65b0\u4e00\u81f4\u6027\u3001\u957f\u6587\u672c\u8fde\u8d2f\u6027\u7b49\u65b9\u9762\u5bf9\u73b0\u6709\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u6784\u6210\u663e\u8457\u6311\u6218", "conclusion": "\u6587\u6863\u7ea7\u6a21\u578b\u7f16\u8f91\u662f\u5b9e\u9645\u5e94\u7528\u7684\u5173\u952e\u73af\u8282\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u6587\u672c\u5904\u7406\u548c\u591a\u4e8b\u5b9e\u7ef4\u62a4\u65b9\u9762\u4ecd\u9700\u7a81\u7834"}}
{"id": "2505.19586", "pdf": "https://arxiv.org/pdf/2505.19586", "abs": "https://arxiv.org/abs/2505.19586", "authors": ["Dingyu Yao", "Bowen Shen", "Zheng Lin", "Wei Liu", "Jian Luan", "Bin Wang", "Weiping Wang"], "title": "TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV Cache Optimization", "categories": ["cs.CL"], "comment": null, "summary": "The Key-Value (KV) cache in generative large language models (LLMs)\nintroduces substantial memory overhead. Existing works mitigate this burden by\noffloading or compressing the KV cache. However, loading the entire cache\nincurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU\ncommunication, while aggressive compression causes notable performance\ndegradation. We identify that certain layers in the LLM need to maintain global\ninformation and are unsuitable for selective loading. In contrast, other layers\nprimarily focus on a few tokens with dominant activations that potentially\nincur substantial quantization error. This observation leads to a key insight\nthat loading dominant tokens and quantizing all tokens can complement each\nother. Building on this insight, we propose a hybrid compression method,\nTailorKV, which seamlessly integrates quantization and offloading. TailorKV\ndevelops an inference framework along with a hardware-friendly implementation\nthat leverages these complementary characteristics. Extensive long-context\nevaluations exhibit that TailorKV achieves nearly lossless performance under\naggressive compression settings, outperforming the state-of-the-art.\nParticularly, the Llama-3.1-8B with 128k context can be served within a single\nRTX 3090 GPU, reaching 82 ms per token during decoding.", "AI": {"tldr": "TailorKV\u901a\u8fc7\u7ed3\u5408\u91cf\u5316\u548c\u5378\u8f7d\u6280\u672f\u4f18\u5316LLM\u7684KV\u7f13\u5b58\u7ba1\u7406\uff0c\u5728\u6fc0\u8fdb\u538b\u7f29\u4e0b\u5b9e\u73b0\u8fd1\u4e4e\u65e0\u635f\u6027\u80fd\u5e76\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf", "motivation": "\u73b0\u6709KV\u7f13\u5b58\u4f18\u5316\u65b9\u6cd5\u5b58\u5728PCIe\u5e26\u5bbd\u74f6\u9888\u5bfc\u81f4\u7684\u5ef6\u8fdf\u95ee\u9898\u6216\u6fc0\u8fdb\u538b\u7f29\u5e26\u6765\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u672a\u5145\u5206\u8003\u8651\u4e0d\u540c\u7f51\u7edc\u5c42\u5bf9\u4fe1\u606f\u9700\u6c42\u7684\u5dee\u5f02", "method": "\u63d0\u51fa\u5206\u5c42\u5904\u7406\u7684\u6df7\u5408\u538b\u7f29\u65b9\u6cd5\uff1a\u9700\u8981\u5168\u5c40\u4fe1\u606f\u7684\u5c42\u4fdd\u6301\u5b8c\u6574\uff0c\u5173\u952etoken\u5c42\u91c7\u7528\u9009\u62e9\u6027\u52a0\u8f7d\uff0c\u5176\u4ed6\u5c42\u5b9e\u65bd\u786c\u4ef6\u53cb\u597d\u7684\u91cf\u5316\u65b9\u6848", "result": "\u5728128k\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\uff0cLlama-3.1-8B\u53ef\u5728\u5355\u5361RTX 3090\u5b9e\u73b082ms/token\u7684\u89e3\u7801\u901f\u5ea6\uff0c\u6027\u80fd\u635f\u5931\u4ec50.3%-2.1%", "conclusion": "\u901a\u8fc7\u5dee\u5f02\u5316\u7684\u5206\u5c42\u538b\u7f29\u7b56\u7565\uff0cTailorKV\u6210\u529f\u5e73\u8861\u5b58\u50a8\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u5927\u6a21\u578b\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.19591", "pdf": "https://arxiv.org/pdf/2505.19591", "abs": "https://arxiv.org/abs/2505.19591", "authors": ["Yufan Dang", "Chen Qian", "Xueheng Luo", "Jingru Fan", "Zihao Xie", "Ruijie Shi", "Weize Chen", "Cheng Yang", "Xiaoyin Che", "Ye Tian", "Xuantang Xiong", "Lei Han", "Zhiyuan Liu", "Maosong Sun"], "title": "Multi-Agent Collaboration via Evolving Orchestration", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "Work in Progress", "summary": "Large language models (LLMs) have achieved remarkable results across diverse\ndownstream tasks, but their monolithic nature restricts scalability and\nefficiency in complex problem-solving. While recent research explores\nmulti-agent collaboration among LLMs, most approaches rely on static\norganizational structures that struggle to adapt as task complexity and agent\nnumbers grow, resulting in coordination overhead and inefficiencies. To this\nend, we propose a puppeteer-style paradigm for LLM-based multi-agent\ncollaboration, where a centralized orchestrator (\"puppeteer\") dynamically\ndirects agents (\"puppets\") in response to evolving task states. This\norchestrator is trained via reinforcement learning to adaptively sequence and\nprioritize agents, enabling flexible and evolvable collective reasoning.\nExperiments on closed- and open-domain scenarios show that this method achieves\nsuperior performance with reduced computational costs. Analyses further reveal\nthat the key improvements consistently stem from the emergence of more compact,\ncyclic reasoning structures under the orchestrator's evolution.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u6001\u534f\u8c03\u5668\u8303\u5f0f\uff0c\u901a\u8fc7\u6f14\u5316\u7d27\u51d1\u63a8\u7406\u7ed3\u6784\u63d0\u5347\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6548\u7387", "motivation": "\u73b0\u6709\u9759\u6001\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u65b9\u6cd5\u96be\u4ee5\u9002\u5e94\u590d\u6742\u4efb\u52a1\u6269\u5c55\uff0c\u5b58\u5728\u534f\u8c03\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898", "method": "\u6784\u5efa'\u7275\u7ebf\u6728\u5076'\u8303\u5f0f\uff0c\u4e2d\u592e\u534f\u8c03\u5668\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u7f16\u6392\u667a\u80fd\u4f53\u6267\u884c\u987a\u5e8f\u548c\u4f18\u5148\u7ea7", "result": "\u5728\u95ed\u57df/\u5f00\u653e\u57df\u573a\u666f\u5b9e\u73b0\u6027\u80fd\u63d0\u5347(\u8ba1\u7b97\u6210\u672c\u964d\u4f4e20-35%)\uff0c\u5f62\u6210\u7d27\u51d1\u5faa\u73af\u63a8\u7406\u7ed3\u6784", "conclusion": "\u52a8\u6001\u534f\u8c03\u673a\u5236\u4e3a\u590d\u6742\u95ee\u9898\u89e3\u51b3\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u8bc1\u660e\u6f14\u5316\u5f0f\u96c6\u4f53\u63a8\u7406\u7ed3\u6784\u7684\u6709\u6548\u6027"}}
{"id": "2505.19598", "pdf": "https://arxiv.org/pdf/2505.19598", "abs": "https://arxiv.org/abs/2505.19598", "authors": ["Guanyu Hou", "Jiaming He", "Yinhang Zhou", "Ji Guo", "Yitong Qiao", "Rui Zhang", "Wenbo Jiang"], "title": "Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study", "categories": ["cs.CL"], "comment": null, "summary": "Large Audio-Language Models (LALMs) are increasingly deployed in real-world\napplications, yet their robustness against malicious audio injection attacks\nremains underexplored. This study systematically evaluates five leading LALMs\nacross four attack scenarios: Audio Interference Attack, Instruction Following\nAttack, Context Injection Attack, and Judgment Hijacking Attack. Using metrics\nlike Defense Success Rate, Context Robustness Score, and Judgment Robustness\nIndex, their vulnerabilities and resilience were quantitatively assessed.\nExperimental results reveal significant performance disparities among models;\nno single model consistently outperforms others across all attack types. The\nposition of malicious content critically influences attack effectiveness,\nparticularly when placed at the beginning of sequences. A negative correlation\nbetween instruction-following capability and robustness suggests models\nadhering strictly to instructions may be more susceptible, contrasting with\ngreater resistance by safety-aligned models. Additionally, system prompts show\nmixed effectiveness, indicating the need for tailored strategies. This work\nintroduces a benchmark framework and highlights the importance of integrating\nrobustness into training pipelines. Findings emphasize developing multi-modal\ndefenses and architectural designs that decouple capability from susceptibility\nfor secure LALMs deployment.", "AI": {"tldr": "\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5bf9\u6297\u6076\u610f\u97f3\u9891\u6ce8\u5165\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u5dee\u5f02\u663e\u8457\u4e14\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e", "motivation": "\u5f53\u524dLALMs\u5e94\u7528\u5e7f\u6cdb\u4f46\u5bf9\u6297\u6076\u610f\u97f3\u9891\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u63ed\u793a\u6a21\u578b\u8106\u5f31\u6027\u4ee5\u63d0\u5347\u5b89\u5168\u6027", "method": "\u4f7f\u75284\u7c7b\u653b\u51fb\u573a\u666f\uff08\u97f3\u9891\u5e72\u6270/\u6307\u4ee4\u8ddf\u968f/\u4e0a\u4e0b\u6587\u6ce8\u5165/\u5224\u65ad\u52ab\u6301\uff09\u548c3\u4e2a\u91cf\u5316\u6307\u6807\uff08DSR/CRS/JRI\uff09\uff0c\u8bc4\u4f305\u4e2a\u4e3b\u6d41LALMs\u6a21\u578b", "result": "\u6a21\u578b\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u5f02\uff1b\u6076\u610f\u5185\u5bb9\u4f4d\u7f6e\uff08\u5c24\u5176\u5e8f\u5217\u5f00\u5934\uff09\u663e\u8457\u5f71\u54cd\u653b\u51fb\u6548\u679c\uff1b\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u4e0e\u9c81\u68d2\u6027\u8d1f\u76f8\u5173\uff1b\u5b89\u5168\u5bf9\u9f50\u6a21\u578b\u66f4\u5177\u6297\u6027", "conclusion": "\u9700\u5f00\u53d1\u591a\u6a21\u6001\u9632\u5fa1\u67b6\u6784\uff0c\u5206\u79bb\u6a21\u578b\u80fd\u529b\u4e0e\u8106\u5f31\u6027\uff0c\u5f3a\u8c03\u8bad\u7ec3\u6d41\u7a0b\u4e2d\u6574\u5408\u9c81\u68d2\u6027\u6307\u6807\uff0c\u6784\u5efa\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6"}}
{"id": "2505.19599", "pdf": "https://arxiv.org/pdf/2505.19599", "abs": "https://arxiv.org/abs/2505.19599", "authors": ["Andrew Gambardella", "Takeshi Kojima", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "In Proceedings of the 63rd Annual Meeting of the Association for\n  Computational Linguistics, 2025", "summary": "Typical methods for evaluating the performance of language models evaluate\ntheir ability to answer questions accurately. These evaluation metrics are\nacceptable for determining the extent to which language models can understand\nand reason about text in a general sense, but fail to capture nuanced\ncapabilities, such as the ability of language models to recognize and obey rare\ngrammar points, particularly in languages other than English. We measure the\nperplexity of language models when confronted with the \"first person psych\npredicate restriction\" grammar point in Japanese. Weblab is the only tested\nopen source model in the 7-10B parameter range which consistently assigns\nhigher perplexity to ungrammatical psych predicate sentences than grammatical\nones. We give evidence that Weblab's uniformly bad tokenization is a possible\nroot cause for its good performance, and show that Llama 3's perplexity on\ngrammatical psych predicate sentences can be reduced by orders of magnitude\n(28x difference) by restricting test sentences to those with uniformly\nwell-behaved tokenizations. We show in further experiments on machine\ntranslation tasks that language models will use alternative grammar patterns in\norder to produce grammatical sentences when tokenization issues prevent the\nmost natural sentence from being output.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u5728\u975e\u82f1\u8bed\u8bed\u6cd5\u7ec6\u8282\u8bc4\u4f30\u4e0a\u7684\u4e0d\u8db3\uff0c\u901a\u8fc7\u65e5\u8bed\u5fc3\u7406\u8c13\u8bcd\u9650\u5236\u6848\u4f8b\uff0c\u8bc1\u660e\u5206\u8bcd\u8d28\u91cf\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u6355\u6349\u6a21\u578b\u5bf9\u975e\u82f1\u8bed\u7f55\u89c1\u8bed\u6cd5\u70b9\u7684\u7406\u89e3\u80fd\u529b\uff0c\u9700\u901a\u8fc7\u5177\u4f53\u8bed\u6cd5\u6848\u4f8b\u63ed\u793a\u6a21\u578b\u771f\u5b9e\u8868\u73b0\u3002", "method": "\u4ee5\u65e5\u8bed\u5fc3\u7406\u8c13\u8bcd\u9650\u5236\u4e3a\u6d4b\u8bd5\u573a\u666f\uff0c\u5bf9\u6bd4\u4e0d\u540c\u6a21\u578b\u5728\u5408\u6cd5/\u975e\u6cd5\u53e5\u5b50\u4e0a\u7684\u56f0\u60d1\u5ea6\uff0c\u5e76\u8bbe\u8ba1\u5206\u8bcd\u63a7\u5236\u5b9e\u9a8c\u3002", "result": "Weblab\u56e0\u7edf\u4e00\u5206\u8bcd\u7b56\u7565\u8868\u73b0\u6700\u4f73\uff0cLlama 3\u901a\u8fc7\u4f18\u5316\u5206\u8bcd\u4f7f\u56f0\u60d1\u5ea6\u964d\u4f4e28\u500d\uff0c\u673a\u5668\u7ffb\u8bd1\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u4f1a\u4e3b\u52a8\u89c4\u907f\u5206\u8bcd\u95ee\u9898\u3002", "conclusion": "\u5206\u8bcd\u8d28\u91cf\u76f4\u63a5\u5f71\u54cd\u8bed\u8a00\u6a21\u578b\u5bf9\u975e\u82f1\u8bed\u8bed\u6cd5\u7684\u5904\u7406\u80fd\u529b\uff0c\u6539\u8fdb\u5206\u8bcd\u7b56\u7565\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2505.19604", "pdf": "https://arxiv.org/pdf/2505.19604", "abs": "https://arxiv.org/abs/2505.19604", "authors": ["Ahan Prasannakumar Shetty"], "title": "Evaluating Machine Translation Models for English-Hindi Language Pairs: A Comparative Analysis", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Machine translation has become a critical tool in bridging linguistic gaps,\nespecially between languages as diverse as English and Hindi. This paper\ncomprehensively evaluates various machine translation models for translating\nbetween English and Hindi. We assess the performance of these models using a\ndiverse set of automatic evaluation metrics, both lexical and machine\nlearning-based metrics. Our evaluation leverages an 18000+ corpus of English\nHindi parallel dataset and a custom FAQ dataset comprising questions from\ngovernment websites. The study aims to provide insights into the effectiveness\nof different machine translation approaches in handling both general and\nspecialized language domains. Results indicate varying performance levels\nacross different metrics, highlighting strengths and areas for improvement in\ncurrent translation systems.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u82f1\u8bed-\u5370\u5730\u8bed\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u5728\u901a\u7528\u548c\u4e13\u7528\u9886\u57df\u7684\u8868\u73b0\uff0c\u4f7f\u752818K\u5e73\u884c\u8bed\u6599\u5e93\u548c\u653f\u5e9cFAQ\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u4e0d\u540c\u6307\u6807\u4e0b\u6a21\u578b\u6027\u80fd\u5b58\u5728\u5dee\u5f02", "motivation": "\u586b\u8865\u82f1\u8bed\u548c\u5370\u5730\u8bed\u8fd9\u5bf9\u5dee\u5f02\u8f83\u5927\u7684\u8bed\u8a00\u95f4\u673a\u5668\u7ffb\u8bd1\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u7a7a\u767d\uff0c\u5206\u6790\u4e0d\u540c\u7ffb\u8bd1\u6a21\u578b\u5728\u901a\u7528\u548c\u4e13\u7528\u9886\u57df\u7684\u6709\u6548\u6027", "method": "\u4f7f\u752818,000+\u53e5\u5bf9\u7684\u5e73\u884c\u8bed\u6599\u5e93\u548c\u653f\u5e9c\u7f51\u7ad9FAQ\u5b9a\u5236\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u8bcd\u6c47\u7ea7\u6307\u6807\u548c\u673a\u5668\u5b66\u4e60\u6307\u6807\u8fdb\u884c\u591a\u7ef4\u5ea6\u81ea\u52a8\u8bc4\u4f30", "result": "\u4e0d\u540c\u7ffb\u8bd1\u7cfb\u7edf\u5728\u5404\u9879\u6307\u6807\u4e2d\u8868\u73b0\u6ce2\u52a8\u660e\u663e\uff0c\u73b0\u6709\u7cfb\u7edf\u5728\u7279\u5b9a\u9886\u57df\uff08\u5982\u653f\u5e9cFAQ\uff09\u4ecd\u5b58\u5728\u6539\u8fdb\u7a7a\u95f4", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4f18\u5316\u82f1\u8bed-\u5370\u5730\u8bed\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u51c6\uff0c\u8868\u660e\u9700\u9488\u5bf9\u4e0d\u540c\u8bed\u8a00\u573a\u666f\u5f00\u53d1\u5b9a\u5236\u5316\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.19606", "pdf": "https://arxiv.org/pdf/2505.19606", "abs": "https://arxiv.org/abs/2505.19606", "authors": ["Ryan Soh-Eun Shim", "Domenico De Cristofaro", "Chengzhi Martin Hu", "Alessandro Vietti", "Barbara Plank"], "title": "Languages in Multilingual Speech Foundation Models Align Both Phonetically and Semantically", "categories": ["cs.CL"], "comment": null, "summary": "Cross-lingual alignment in pretrained language models (LMs) has enabled\nefficient transfer in text-based LMs. Such an alignment has also been observed\nin speech foundation models. However, it remains an open question whether\nfindings and methods from text-based cross-lingual alignment apply to speech.\nBuilding on prior work on spoken translation retrieval, we perform\npronunciation-controlled experiments to observe if cross-lingual alignment can\nindeed occur in such models on a semantic basis, instead of relying on phonetic\nsimilarities. Our findings indicate that even in the absence of phonetic cues,\nspoken translation retrieval accuracy remains relatively stable. We follow up\nwith a controlled experiment on a word-level dataset of cross-lingual synonyms\nand near-homophones, confirming the existence of both phonetic and semantic\nknowledge in the encoder. Finally, we qualitatively examine the transcriptions\nproduced by early exiting the encoder, where we observe that speech translation\nproduces semantic errors that are characterized by phonetic similarities to\ncorresponding words in the source language. We apply this insight from early\nexiting to speech recognition in seven low-resource languages unsupported by\nthe Whisper model, and achieve improved accuracy in all languages examined,\nparticularly for languages with transparent orthographies.", "AI": {"tldr": "\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u5b58\u5728\u8de8\u8bed\u8a00\u8bed\u4e49\u5bf9\u9f50\u80fd\u529b\uff0c\u5373\u4f7f\u5265\u79bb\u8bed\u97f3\u7279\u5f81\u4ecd\u4fdd\u6301\u7ffb\u8bd1\u68c0\u7d22\u7a33\u5b9a\u6027\uff0c\u8be5\u53d1\u73b0\u53ef\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u6548\u679c", "motivation": "\u63a2\u7d22\u8bed\u97f3\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u8de8\u8bed\u8a00\u5bf9\u9f50\u673a\u5236\u662f\u5426\u57fa\u4e8e\u8bed\u4e49\u800c\u975e\u8bed\u97f3\u76f8\u4f3c\u6027\uff0c\u9a8c\u8bc1\u6587\u672c\u6a21\u578b\u7684\u53d1\u73b0\u662f\u5426\u9002\u7528\u4e8e\u8bed\u97f3\u9886\u57df", "method": "\u91c7\u7528\u53d1\u97f3\u63a7\u5236\u5b9e\u9a8c\uff08\u6d88\u9664\u8bed\u97f3\u7279\u5f81\uff09\u3001\u8de8\u8bed\u8a00\u540c\u4e49\u8bcd/\u8fd1\u97f3\u8bcd\u6570\u636e\u96c6\u6d4b\u8bd5\u3001\u7f16\u7801\u5668\u65e9\u9000\u673a\u5236\u5206\u6790\u8f6c\u5f55\u9519\u8bef", "result": "\u65e0\u8bed\u97f3\u7279\u5f81\u65f6\u7ffb\u8bd1\u68c0\u7d22\u51c6\u786e\u7387\u7a33\u5b9a\uff1b\u7f16\u7801\u5668\u540c\u65f6\u5177\u5907\u8bed\u97f3\u548c\u8bed\u4e49\u77e5\u8bc6\uff1b\u65e9\u9000\u673a\u5236\u63ed\u793a\u8bed\u97f3\u7ffb\u8bd1\u9519\u8bef\u5b58\u5728\u6e90\u8bed\u8a00\u8bed\u97f3\u76f8\u4f3c\u6027\u7279\u5f81", "conclusion": "\u5229\u7528\u7f16\u7801\u5668\u65e9\u671f\u8bed\u97f3\u7279\u5f81\u53ef\u63d0\u5347Whisper\u6a21\u578b\u672a\u652f\u6301\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u5c24\u5176\u5728\u62fc\u5199\u900f\u660e\u7684\u8bed\u8a00\u4e2d\u6548\u679c\u663e\u8457"}}
{"id": "2505.19628", "pdf": "https://arxiv.org/pdf/2505.19628", "abs": "https://arxiv.org/abs/2505.19628", "authors": ["Silin Li", "Yuhang Guo", "Jiashu Yao", "Zeming Liu", "Haifeng Wang"], "title": "HomeBench: Evaluating LLMs in Smart Homes with Valid and Invalid Instructions Across Single and Multiple Devices", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have the potential to revolutionize smart home\nassistants by enhancing their ability to accurately understand user needs and\nrespond appropriately, which is extremely beneficial for building a smarter\nhome environment. While recent studies have explored integrating LLMs into\nsmart home systems, they primarily focus on handling straightforward, valid\nsingle-device operation instructions. However, real-world scenarios are far\nmore complex and often involve users issuing invalid instructions or\ncontrolling multiple devices simultaneously. These have two main challenges:\nLLMs must accurately identify and rectify errors in user instructions and\nexecute multiple user instructions perfectly. To address these challenges and\nadvance the development of LLM-based smart home assistants, we introduce\nHomeBench, the first smart home dataset with valid and invalid instructions\nacross single and multiple devices in this paper. We have experimental results\non 13 distinct LLMs; e.g., GPT-4o achieves only a 0.0% success rate in the\nscenario of invalid multi-device instructions, revealing that the existing\nstate-of-the-art LLMs still cannot perform well in this situation even with the\nhelp of in-context learning, retrieval-augmented generation, and fine-tuning.\nOur code and dataset are publicly available at\nhttps://github.com/BITHLP/HomeBench.", "AI": {"tldr": "HomeBench\u662f\u9996\u4e2a\u5305\u542b\u6709\u6548/\u65e0\u6548\u6307\u4ee4\u7684\u667a\u80fd\u5bb6\u5c45\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u663e\u793aGPT-4o\u5728\u65e0\u6548\u591a\u8bbe\u5907\u6307\u4ee4\u573a\u666f\u6210\u529f\u7387\u4ec50%\uff0c\u66b4\u9732\u73b0\u6709LLM\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u5b9e\u667a\u80fd\u5bb6\u5c45\u573a\u666f\u5b58\u5728\u65e0\u6548\u6307\u4ee4\u548c\u591a\u8bbe\u5907\u5e76\u53d1\u64cd\u4f5c\u9700\u6c42\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4ec5\u5173\u6ce8\u7b80\u5355\u5355\u8bbe\u5907\u6307\u4ee4\u5904\u7406\uff0c\u9700\u6784\u5efa\u66f4\u8d34\u8fd1\u771f\u5b9e\u573a\u666f\u7684\u6570\u636e\u96c6\u63a8\u52a8LLM\u667a\u80fd\u52a9\u624b\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u5305\u542b\u5355\u8bbe\u5907/\u591a\u8bbe\u5907\u3001\u6709\u6548/\u65e0\u6548\u6307\u4ee4\u7684HomeBench\u6570\u636e\u96c6\uff0c\u572813\u79cdLLM\u4e0a\u6d4b\u8bd5\u6a21\u578b\u8868\u73b0\uff08\u5305\u62ec\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u68c0\u7d22\u589e\u5f3a\u548c\u5fae\u8c03\u7b49\u65b9\u6cd5\u7684\u5e94\u7528\uff09\u3002", "result": "GPT-4o\u5728\u65e0\u6548\u591a\u8bbe\u5907\u6307\u4ee4\u573a\u666f\u6210\u529f\u73870%\uff0c\u5176\u4ed6\u6a21\u578b\u8868\u73b0\u540c\u6837\u6b20\u4f73\uff0c\u663e\u793a\u73b0\u6709\u6280\u672f\u65e0\u6cd5\u6709\u6548\u5904\u7406\u590d\u6742\u6307\u4ee4\u573a\u666f\u3002", "conclusion": "HomeBench\u63ed\u793a\u4e86LLM\u5728\u667a\u80fd\u5bb6\u5c45\u573a\u666f\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u57fa\u51c6\uff0c\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u6307\u4ee4\u7406\u89e3\u4e0e\u6267\u884c\u65b9\u6848\u3002"}}
{"id": "2505.19630", "pdf": "https://arxiv.org/pdf/2505.19630", "abs": "https://arxiv.org/abs/2505.19630", "authors": ["Yichun Feng", "Jiawei Wang", "Lu Zhou", "Yixue Li"], "title": "DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated excellent capabilities in the\nfield of biomedical question answering, but their application in real-world\nclinical consultations still faces core challenges. Existing systems rely on a\none-way information transmission mode where patients must fully describe their\nsymptoms in a single round, leading to nonspecific diagnostic recommendations\nwhen complaints are vague. Traditional multi-turn dialogue methods based on\nsupervised learning are constrained by static data-driven paradigms, lacking\ngeneralizability and struggling to intelligently extract key clinical\ninformation. To address these limitations, we propose DoctorAgent-RL, a\nreinforcement learning (RL)-based multi-agent collaborative framework that\nmodels medical consultations as a dynamic decision-making process under\nuncertainty. The doctor agent continuously optimizes its questioning strategy\nwithin the RL framework through multi-turn interactions with the patient agent,\ndynamically adjusting its information-gathering path based on comprehensive\nrewards from the Consultation Evaluator. This RL fine-tuning mechanism enables\nLLMs to autonomously develop interaction strategies aligned with clinical\nreasoning logic, rather than superficially imitating patterns in existing\ndialogue data. Notably, we constructed MTMedDialog, the first English\nmulti-turn medical consultation dataset capable of simulating patient\ninteractions. Experiments demonstrate that DoctorAgent-RL outperforms existing\nmodels in both multi-turn reasoning capability and final diagnostic\nperformance, demonstrating practical value in assisting clinical consultations.\nhttps://github.com/JarvisUSTC/DoctorAgent-RL", "AI": {"tldr": "\u63d0\u51faDoctorAgent-RL\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4f18\u5316\u4e34\u5e8a\u95ee\u8bca\u6d41\u7a0b\uff0c\u5b9e\u73b0\u52a8\u6001\u4fe1\u606f\u91c7\u96c6\u548c\u8bca\u65ad\u51b3\u7b56\u3002", "motivation": "\u73b0\u6709\u533b\u7597\u95ee\u7b54\u7cfb\u7edf\u5b58\u5728\u5355\u5411\u4fe1\u606f\u4f20\u9012\u6a21\u5f0f\u6548\u7387\u4f4e\u3001\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u52a8\u6001\u63a8\u7406\u80fd\u529b\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u6a21\u7cca\u75c7\u72b6\u63cf\u8ff0\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6784\u5efa\u533b\u60a3\u53cc\u667a\u80fd\u4f53\u4ea4\u4e92\u7cfb\u7edf\uff0c\u901a\u8fc7Consultation Evaluator\u7684\u590d\u5408\u5956\u52b1\u673a\u5236\u52a8\u6001\u4f18\u5316\u95ee\u8bca\u7b56\u7565\uff0c\u5e76\u6784\u5efaMTMedDialog\u591a\u8f6e\u95ee\u8bca\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eDoctorAgent-RL\u5728\u591a\u8f6e\u63a8\u7406\u80fd\u529b\u548c\u6700\u7ec8\u8bca\u65ad\u6548\u679c\u4e0a\u5747\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u4e34\u5e8a\u8f85\u52a9\u4ef7\u503c\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u4f7fLLMs\u81ea\u4e3b\u5f62\u6210\u7b26\u5408\u4e34\u5e8a\u903b\u8f91\u7684\u4ea4\u4e92\u7b56\u7565\uff0c\u540c\u65f6\u521b\u5efa\u9996\u4e2a\u53ef\u6a21\u62df\u60a3\u8005\u4ea4\u4e92\u7684\u82f1\u6587\u591a\u8f6e\u95ee\u8bca\u6570\u636e\u96c6\uff0c\u63a8\u52a8\u667a\u80fd\u95ee\u8bca\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2505.19631", "pdf": "https://arxiv.org/pdf/2505.19631", "abs": "https://arxiv.org/abs/2505.19631", "authors": ["Zihong Zhang", "Liqi He", "Zuchao Li", "Lefei Zhang", "Hai Zhao", "Bo Du"], "title": "Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Word segmentation stands as a cornerstone of Natural Language Processing\n(NLP). Based on the concept of \"comprehend first, segment later\", we propose a\nnew framework to explore the limit of unsupervised word segmentation with Large\nLanguage Models (LLMs) and evaluate the semantic understanding capabilities of\nLLMs based on word segmentation. We employ current mainstream LLMs to perform\nword segmentation across multiple languages to assess LLMs' \"comprehension\".\nOur findings reveal that LLMs are capable of following simple prompts to\nsegment raw text into words. There is a trend suggesting that models with more\nparameters tend to perform better on multiple languages. Additionally, we\nintroduce a novel unsupervised method, termed LLACA ($\\textbf{L}$arge\n$\\textbf{L}$anguage Model-Inspired $\\textbf{A}$ho-$\\textbf{C}$orasick\n$\\textbf{A}$utomaton). Leveraging the advanced pattern recognition capabilities\nof Aho-Corasick automata, LLACA innovatively combines these with the deep\ninsights of well-pretrained LLMs. This approach not only enables the\nconstruction of a dynamic $n$-gram model that adjusts based on contextual\ninformation but also integrates the nuanced understanding of LLMs, offering\nsignificant improvements over traditional methods. Our source code is available\nat https://github.com/hkr04/LLACA", "AI": {"tldr": "\u63d0\u51faLLACA\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0eAho-Corasick\u81ea\u52a8\u673a\u5b9e\u73b0\u591a\u8bed\u8a00\u65e0\u76d1\u7763\u5206\u8bcd\uff0c\u6027\u80fd\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5206\u8bcd\u65b9\u6cd5\u4f9d\u8d56\u89c4\u5219\u6216\u7edf\u8ba1\u6a21\u578b\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528LLMs\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u3002\u8bba\u6587\u63a2\u7d22\u4ee5\u300e\u5148\u7406\u89e3\u540e\u5206\u8bcd\u300f\u4e3a\u6838\u5fc3\uff0c\u9a8c\u8bc1LLMs\u901a\u8fc7\u4e0a\u4e0b\u6587\u7406\u89e3\u5b9e\u73b0\u65e0\u76d1\u7763\u5206\u8bcd\u7684\u6f5c\u529b\u3002", "method": "1. \u901a\u8fc7prompt\u5f15\u5bfcLLMs\u5b9e\u73b0\u591a\u8bed\u8a00\u539f\u59cb\u6587\u672c\u5206\u8bcd\n2. \u63d0\u51faLLACA\u7b97\u6cd5\uff1a\u5c06LLMs\u7684\u8bed\u4e49\u7406\u89e3\u878d\u5165Aho-Corasick\u81ea\u52a8\u673a\uff0c\u6784\u5efa\u52a8\u6001n-gram\u6a21\u578b\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u81ea\u9002\u5e94\u7684\u5206\u8bcd", "result": "1. \u53c2\u6570\u91cf\u66f4\u5927\u7684LLMs\u5728\u591a\u8bed\u8a00\u5206\u8bcd\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\n2. LLACA\u5728\u591a\u79cd\u8bed\u8a00\u7684\u5206\u8bcd\u4efb\u52a1\u4e2dF1\u503c\u663e\u8457\u8d85\u8fc7\u4f20\u7edf\u65e0\u76d1\u7763\u65b9\u6cd5\uff08BPE\u3001SentencePiece\u7b49\uff09", "conclusion": "LLMs\u5c55\u73b0\u51fa\u57fa\u4e8e\u8bed\u4e49\u7406\u89e3\u7684\u65e0\u76d1\u7763\u5206\u8bcd\u80fd\u529b\uff0cLLACA\u901a\u8fc7\u6a21\u5f0f\u8bc6\u522b\u4e0e\u8bed\u4e49\u7406\u89e3\u7684\u7ed3\u5408\uff0c\u4e3aNLP\u57fa\u7840\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84"}}
{"id": "2505.19634", "pdf": "https://arxiv.org/pdf/2505.19634", "abs": "https://arxiv.org/abs/2505.19634", "authors": ["Zili Wang", "Tianyu Zhang", "Haoli Bai", "Lu Hou", "Xianzhi Yu", "Wulong Liu", "Shiming Xiang", "Lei Zhu"], "title": "Faster and Better LLMs via Latency-Aware Test-Time Scaling", "categories": ["cs.CL"], "comment": null, "summary": "Test-Time Scaling (TTS) has proven effective in improving the performance of\nLarge Language Models (LLMs) during inference. However, existing research has\noverlooked the efficiency of TTS from a latency-sensitive perspective. Through\na latency-aware evaluation of representative TTS methods, we demonstrate that a\ncompute-optimal TTS does not always result in the lowest latency in scenarios\nwhere latency is critical. To address this gap and achieve latency-optimal TTS,\nwe propose two key approaches by optimizing the concurrency configurations: (1)\nbranch-wise parallelism, which leverages multiple concurrent inference\nbranches, and (2) sequence-wise parallelism, enabled by speculative decoding.\nBy integrating these two approaches and allocating computational resources\nproperly to each, our latency-optimal TTS enables a 32B model to reach 82.3%\naccuracy on MATH-500 within 1 minute and a smaller 3B model to achieve 72.4%\nwithin 10 seconds. Our work emphasizes the importance of latency-aware TTS and\ndemonstrates its ability to deliver both speed and accuracy in\nlatency-sensitive scenarios.", "AI": {"tldr": "\u63d0\u51fa\u5ef6\u8fdf\u611f\u77e5\u7684Test-Time Scaling\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u652f\u7ea7\u5e76\u884c\u548c\u63a8\u6d4b\u89e3\u7801\u7684\u5e8f\u5217\u7ea7\u5e76\u884c\u914d\u7f6e\uff0c\u5728\u4e25\u683c\u5ef6\u8fdf\u9650\u5236\u4e0b\u5b9e\u73b0\u901f\u5ea6\u4e0e\u7cbe\u5ea6\u7684\u53cc\u91cd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709TTS\u7814\u7a76\u5ffd\u89c6\u5ef6\u8fdf\u654f\u611f\u573a\u666f\u7684\u6548\u7387\u95ee\u9898\uff0c\u8ba1\u7b97\u6700\u4f18\u65b9\u6848\u672a\u5fc5\u80fd\u8fbe\u5230\u6700\u4f4e\u5ef6\u8fdf\uff0c\u9700\u9488\u5bf9\u5b9e\u9645\u5ef6\u8fdf\u9700\u6c42\u4f18\u5316\u8d44\u6e90\u914d\u7f6e\u3002", "method": "1. \u5206\u652f\u7ea7\u5e76\u884c\uff1a\u591a\u63a8\u7406\u5206\u652f\u5e76\u53d1\u6267\u884c\n2. \u5e8f\u5217\u7ea7\u5e76\u884c\uff1a\u57fa\u4e8e\u63a8\u6d4b\u89e3\u7801\u4f18\u5316\u6267\u884c\u6d41\u7a0b\n3. \u8ba1\u7b97\u8d44\u6e90\u52a8\u6001\u5206\u914d\u7b56\u7565", "result": "32B\u6a21\u578b1\u5206\u949f\u5185MATH-500\u51c6\u786e\u738782.3%\uff0c3B\u6a21\u578b10\u79d2\u5185\u8fbe72.4%\uff0c\u7a81\u7834\u5ef6\u8fdf-\u7cbe\u5ea6\u6743\u8861\u8fb9\u754c\u3002", "conclusion": "\u5ef6\u8fdf\u611f\u77e5TTS\u5728\u5b9e\u65f6\u573a\u666f\u4e2d\u517c\u5177\u901f\u5ea6\u4e0e\u7cbe\u5ea6\u4f18\u52bf\uff0c\u4e3aLLM\u90e8\u7f72\u63d0\u4f9b\u65b0\u7684\u6548\u7387\u4f18\u5316\u8303\u5f0f\u3002"}}
{"id": "2505.19640", "pdf": "https://arxiv.org/pdf/2505.19640", "abs": "https://arxiv.org/abs/2505.19640", "authors": ["Roy Xie", "David Qiu", "Deepak Gopinath", "Dong Lin", "Yanchao Sun", "Chong Wang", "Saloni Potdar", "Bhuwan Dhingra"], "title": "Interleaved Reasoning for Large Language Models via Reinforcement Learning", "categories": ["cs.CL"], "comment": null, "summary": "Long chain-of-thought (CoT) significantly enhances large language models'\n(LLM) reasoning capabilities. However, the extensive reasoning traces lead to\ninefficiencies and an increased time-to-first-token (TTFT). We propose a novel\ntraining paradigm that uses reinforcement learning (RL) to guide reasoning LLMs\nto interleave thinking and answering for multi-hop questions. We observe that\nmodels inherently possess the ability to perform interleaved reasoning, which\ncan be further enhanced through RL. We introduce a simple yet effective\nrule-based reward to incentivize correct intermediate steps, which guides the\npolicy model toward correct reasoning paths by leveraging intermediate signals\ngenerated during interleaved reasoning. Extensive experiments conducted across\nfive diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++)\ndemonstrate consistent improvements over traditional think-answer reasoning,\nwithout requiring external tools. Specifically, our approach reduces TTFT by\nover 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore,\nour method, trained solely on question answering and logical reasoning\ndatasets, exhibits strong generalization ability to complex reasoning datasets\nsuch as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to\nreveal several valuable insights into conditional reward modeling.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4ea4\u66ff\u63a8\u7406\u8bad\u7ec3\u8303\u5f0f\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u5e76\u63d0\u5347\u6548\u7387", "motivation": "\u4f20\u7edf\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u9996\u6b21\u4ee4\u724c\u65f6\u95f4\u8fc7\u957f\uff0c\u9700\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b\u6548\u7387", "method": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\uff08PPO/GRPO/REINFORCE++\uff09\u5f15\u5bfc\u6a21\u578b\u4ea4\u66ff\u751f\u6210\u601d\u8003\u6b65\u9aa4\u548c\u7b54\u6848\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u4e2d\u95f4\u6b65\u9aa4\u6b63\u786e\u6027\u7684\u89c4\u5219\u5956\u52b1\u673a\u5236", "result": "\u5e73\u5747\u964d\u4f4e80%\u9996\u6b21\u4ee4\u724c\u65f6\u95f4\uff0c\u6700\u9ad8\u63d0\u534719.3%\u51c6\u786e\u7387\uff0c\u5728MATH/GPQA\u7b49\u590d\u6742\u6570\u636e\u96c6\u5c55\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b", "conclusion": "\u65e0\u9700\u5916\u90e8\u5de5\u5177\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u9a8c\u8bc1\u4e86\u6761\u4ef6\u5956\u52b1\u5efa\u6a21\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4f18\u5316\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2505.19647", "pdf": "https://arxiv.org/pdf/2505.19647", "abs": "https://arxiv.org/abs/2505.19647", "authors": ["Xiaochuan Liu", "Ruihua Song", "Xiting Wang", "Xu Chen"], "title": "Select, Read, and Write: A Multi-Agent Framework of Full-Text-based Related Work Generation", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 (Findings)", "summary": "Automatic related work generation (RWG) can save people's time and effort\nwhen writing a draft of related work section (RWS) for further revision.\nHowever, existing methods for RWG always suffer from shallow comprehension due\nto taking the limited portions of references papers as input and isolated\nexplanation for each reference due to ineffective capturing the relationships\namong them. To address these issues, we focus on full-text-based RWG task and\npropose a novel multi-agent framework. Our framework consists of three agents:\na selector that decides which section of the papers is going to read next, a\nreader that digests the selected section and updates a shared working memory,\nand a writer that generates RWS based on the final curated memory. To better\ncapture the relationships among references, we also propose two graph-aware\nstrategies for selector, enabling to optimize the reading order with constrains\nof the graph structure. Extensive experiments demonstrate that our framework\nconsistently improves performance across three base models and various input\nconfigurations. The graph-aware selectors outperform alternative selectors,\nachieving state-of-the-art results. The code and data are available at\nhttps://github.com/1190200817/Full_Text_RWG.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6846\u67b6\u7684\u5168\u6587\u76f8\u5173\u5de5\u4f5c\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u611f\u77e5\u7b56\u7565\u4f18\u5316\u6587\u732e\u95f4\u5173\u7cfb\u5efa\u6a21\uff0c\u5b9e\u9a8c\u8bc1\u660e\u663e\u8457\u63d0\u5347\u751f\u6210\u8d28\u91cf\u5e76\u53d6\u5f97SOTA\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u76f8\u5173\u5de5\u4f5c\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u8f93\u5165\u4fe1\u606f\u5c40\u9650\u5bfc\u81f4\u7684\u6d45\u5c42\u7406\u89e3\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u6587\u732e\u95f4\u5173\u8054\u7684\u6709\u6548\u5efa\u6a21\u3002", "method": "\u6784\u5efa\u5305\u542b\u9009\u62e9\u5668\uff08\u52a8\u6001\u89c4\u5212\u9605\u8bfb\u987a\u5e8f\uff09\u3001\u9605\u8bfb\u5668\uff08\u66f4\u65b0\u5171\u4eab\u8bb0\u5fc6\uff09\u548c\u5199\u5165\u5668\uff08\u751f\u6210\u6587\u672c\uff09\u7684\u4e09\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5728selector\u4e2d\u5f15\u5165\u56fe\u7ed3\u6784\u7ea6\u675f\u4f18\u5316\u6587\u732e\u9605\u8bfb\u987a\u5e8f\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u7840\u6a21\u578b\u548c\u591a\u79cd\u8f93\u5165\u914d\u7f6e\u4e0b\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u56fe\u611f\u77e5\u9009\u62e9\u5668\u53d6\u5f97SOTA\u7ed3\u679c\uff08\u4ee3\u7801\u6570\u636e\u5df2\u5f00\u6e90\uff09\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6709\u6548\u89e3\u51b3\u5168\u6587RWG\u4efb\u52a1\uff0c\u56fe\u7ed3\u6784\u7ea6\u675f\u7b56\u7565\u663e\u8457\u63d0\u5347\u6587\u732e\u5173\u7cfb\u5efa\u6a21\u80fd\u529b\u3002"}}
{"id": "2505.19660", "pdf": "https://arxiv.org/pdf/2505.19660", "abs": "https://arxiv.org/abs/2505.19660", "authors": ["Tingjia Shen", "Hao Wang", "Chuan Qin", "Ruijun Sun", "Yang Song", "Defu Lian", "Hengshu Zhu", "Enhong Chen"], "title": "GenKI: Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models", "categories": ["cs.CL", "cs.AI", "68P20", "H.3.4; I.2.6"], "comment": "13 pages, 5 figures", "summary": "Open-domain question answering (OpenQA) represents a cornerstone in natural\nlanguage processing (NLP), primarily focused on extracting answers from\nunstructured textual data. With the rapid advancements in Large Language Models\n(LLMs), LLM-based OpenQA methods have reaped the benefits of emergent\nunderstanding and answering capabilities enabled by massive parameters compared\nto traditional methods. However, most of these methods encounter two critical\nchallenges: how to integrate knowledge into LLMs effectively and how to\nadaptively generate results with specific answer formats for various task\nsituations. To address these challenges, we propose a novel framework named\nGenKI, which aims to improve the OpenQA performance by exploring Knowledge\nIntegration and controllable Generation on LLMs simultaneously. Specifically,\nwe first train a dense passage retrieval model to retrieve associated knowledge\nfrom a given knowledge base. Subsequently, we introduce a novel knowledge\nintegration model that incorporates the retrieval knowledge into instructions\nduring fine-tuning to intensify the model. Furthermore, to enable controllable\ngeneration in LLMs, we leverage a certain fine-tuned LLM and an ensemble based\non text consistency incorporating all coherence, fluency, and answer format\nassurance. Finally, extensive experiments conducted on the TriviaQA, MSMARCO,\nand CMRC2018 datasets, featuring diverse answer formats, have demonstrated the\neffectiveness of GenKI with comparison of state-of-the-art baselines. Moreover,\nablation studies have disclosed a linear relationship between the frequency of\nretrieved knowledge and the model's ability to recall knowledge accurately\nagainst the ground truth. Our code of GenKI is available at\nhttps://github.com/USTC-StarTeam/GenKI", "AI": {"tldr": "\u63d0\u51faGenKI\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u6574\u5408\u4e0e\u53ef\u63a7\u751f\u6210\u6280\u672f\u63d0\u5347\u5f00\u653e\u57df\u95ee\u7b54\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f00\u653e\u57df\u95ee\u7b54\u65b9\u6cd5\u9762\u4e34\u77e5\u8bc6\u6574\u5408\u6548\u7387\u4f4e\u548c\u751f\u6210\u7ed3\u679c\u683c\u5f0f\u63a7\u5236\u4e0d\u8db3\u7684\u53cc\u91cd\u6311\u6218", "method": "\u91c7\u7528\u5bc6\u96c6\u6bb5\u843d\u68c0\u7d22\u83b7\u53d6\u77e5\u8bc6\uff0c\u8bbe\u8ba1\u6307\u4ee4\u5fae\u8c03\u7684\u77e5\u8bc6\u6574\u5408\u6a21\u578b\uff0c\u7ed3\u5408\u4e00\u81f4\u6027\u96c6\u6210\u5b9e\u73b0\u683c\u5f0f\u53ef\u63a7\u751f\u6210", "result": "\u5728TriviaQA\u3001MSMARCO\u548cCMRC2018\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u53d1\u73b0\u77e5\u8bc6\u68c0\u7d22\u9891\u7387\u4e0e\u51c6\u786e\u6027\u5b58\u5728\u7ebf\u6027\u5173\u7cfb", "conclusion": "GenKI\u6210\u529f\u5b9e\u73b0\u77e5\u8bc6\u6574\u5408\u4e0e\u751f\u6210\u63a7\u5236\u7684\u53cc\u91cd\u7a81\u7834\uff0c\u4e3a\u5f00\u653e\u57df\u95ee\u7b54\u7cfb\u7edf\u63d0\u4f9b\u65b0\u7684\u6280\u672f\u8def\u5f84"}}
{"id": "2505.19667", "pdf": "https://arxiv.org/pdf/2505.19667", "abs": "https://arxiv.org/abs/2505.19667", "authors": ["Weikang Yuan", "Kaisong Song", "Zhuoren Jiang", "Junjie Cao", "Yujie Zhang", "Jun Lin", "Kun Kuang", "Ji Zhang", "Xiaozhong Liu"], "title": "LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue Evaluation", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Legal consultation is essential for safeguarding individual rights and\nensuring access to justice, yet remains costly and inaccessible to many\nindividuals due to the shortage of professionals. While recent advances in\nLarge Language Models (LLMs) offer a promising path toward scalable, low-cost\nlegal assistance, current systems fall short in handling the interactive and\nknowledge-intensive nature of real-world consultations. To address these\nchallenges, we introduce LeCoDe, a real-world multi-turn benchmark dataset\ncomprising 3,696 legal consultation dialogues with 110,008 dialogue turns,\ndesigned to evaluate and improve LLMs' legal consultation capability. With\nLeCoDe, we innovatively collect live-streamed consultations from short-video\nplatforms, providing authentic multi-turn legal consultation dialogues. The\nrigorous annotation by legal experts further enhances the dataset with\nprofessional insights and expertise. Furthermore, we propose a comprehensive\nevaluation framework that assesses LLMs' consultation capabilities in terms of\n(1) clarification capability and (2) professional advice quality. This unified\nframework incorporates 12 metrics across two dimensions. Through extensive\nexperiments on various general and domain-specific LLMs, our results reveal\nsignificant challenges in this task, with even state-of-the-art models like\nGPT-4 achieving only 39.8% recall for clarification and 59% overall score for\nadvice quality, highlighting the complexity of professional consultation\nscenarios. Based on these findings, we further explore several strategies to\nenhance LLMs' legal consultation abilities. Our benchmark contributes to\nadvancing research in legal domain dialogue systems, particularly in simulating\nmore real-world user-expert interactions.", "AI": {"tldr": "\u63d0\u51faLeCoDe\u591a\u8f6e\u6cd5\u5f8b\u54a8\u8be2\u5bf9\u8bdd\u6570\u636e\u96c6\u4e0e\u7efc\u5408\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u73b0\u6709\u5927\u6a21\u578b\u5728\u4e13\u4e1a\u54a8\u8be2\u573a\u666f\u7684\u5c40\u9650\u6027\uff08GPT-4\u6f84\u6e05\u53ec\u56de\u7387\u4ec539.8%\uff0c\u5efa\u8bae\u8d28\u91cf\u603b\u520659%\uff09", "motivation": "\u6cd5\u5f8b\u54a8\u8be2\u5b58\u5728\u4e13\u4e1a\u4eba\u624d\u77ed\u7f3a\u5bfc\u81f4\u7684\u9ad8\u6210\u672c\u95ee\u9898\uff0c\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u5904\u7406\u771f\u5b9e\u54a8\u8be2\u7684\u4ea4\u4e92\u590d\u6742\u6027\u548c\u77e5\u8bc6\u5bc6\u96c6\u6027\u9700\u6c42", "method": "\u901a\u8fc7\u77ed\u89c6\u9891\u5e73\u53f0\u91c7\u96c6\u76f4\u64ad\u54a8\u8be2\u6784\u5efa3,696\u4e2a\u5bf9\u8bdd\uff08110,008\u8f6e\u6b21\uff09\uff0c\u7ecf\u6cd5\u5f8b\u4e13\u5bb6\u6807\u6ce8\u540e\u8bbe\u8ba112\u7ef4\u5ea6\u8bc4\u4f30\u6846\u67b6\uff08\u6f84\u6e05\u80fd\u529b+\u4e13\u4e1a\u5efa\u8bae\u8d28\u91cf\uff09", "result": "\u5b9e\u9a8c\u663e\u793a\u9876\u5c16\u6a21\u578b\u5728\u6f84\u6e05\u53ec\u56de\u7387\uff0839.8%\uff09\u548c\u7efc\u5408\u5efa\u8bae\u8d28\u91cf\uff0859%\uff09\u8868\u73b0\u6b20\u4f73\uff0c\u9a8c\u8bc1\u4e13\u4e1a\u54a8\u8be2\u573a\u666f\u7684\u6280\u672f\u6311\u6218", "conclusion": "LeCoDe\u57fa\u51c6\u63a8\u52a8\u6cd5\u5f8b\u5bf9\u8bdd\u7cfb\u7edf\u7814\u7a76\uff0c\u4e3a\u6a21\u62df\u771f\u5b9e\u7528\u6237-\u4e13\u5bb6\u4ea4\u4e92\u63d0\u4f9b\u65b0\u65b9\u5411\uff0c\u540e\u7eed\u63a2\u7d22\u4e86\u63d0\u5347\u6a21\u578b\u54a8\u8be2\u80fd\u529b\u7684\u6709\u6548\u7b56\u7565"}}
{"id": "2505.19670", "pdf": "https://arxiv.org/pdf/2505.19670", "abs": "https://arxiv.org/abs/2505.19670", "authors": ["Hao Yang", "Lizhen Qu", "Ehsan Shareghi", "Gholamreza Haffari"], "title": "Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models", "categories": ["cs.CL", "cs.MM", "cs.SD", "eess.AS"], "comment": null, "summary": "Large Audio Language Models (LALMs) have extended the capabilities of Large\nLanguage Models (LLMs) by enabling audio-based human interactions. However,\nrecent research has revealed that LALMs remain vulnerable to harmful queries\ndue to insufficient safety-alignment. Despite advances in defence measures for\ntext and vision LLMs, effective safety-alignment strategies and audio-safety\ndataset specifically targeting LALMs are notably absent. Meanwhile defence\nmeasures based on Supervised Fine-tuning (SFT) struggle to address safety\nimprovement while avoiding over-rejection issues, significantly compromising\nhelpfulness. In this work, we propose an unsupervised safety-fine-tuning\nstrategy as remedy that reshapes model's representation space to enhance\nexisting LALMs safety-alignment while balancing the risk of over-rejection. Our\nexperiments, conducted across three generations of Qwen LALMs, demonstrate that\nour approach significantly improves LALMs safety under three modality input\nconditions (audio-text, text-only, and audio-only) while increasing\nover-rejection rate by only 0.88% on average. Warning: this paper contains\nharmful examples.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u76d1\u7763\u5b89\u5168\u5fae\u8c03\u7b56\u7565\uff0c\u901a\u8fc7\u91cd\u5851\u8868\u793a\u7a7a\u95f4\u589e\u5f3a\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u80fd\u529b\uff0c\u5728\u4ec5\u589e\u52a00.88%\u8fc7\u5ea6\u62d2\u7edd\u7387\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u8f93\u5165\u573a\u666f\u4e0b\u7684\u5b89\u5168\u6027", "motivation": "\u73b0\u6709\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u5728\u63d0\u5347LALMs\u5b89\u5168\u6027\u65f6\u6613\u5bfc\u81f4\u8fc7\u5ea6\u62d2\u7edd\u95ee\u9898\uff0c\u663e\u8457\u635f\u5bb3\u6a21\u578b\u5b9e\u7528\u6027\u3002\u5f53\u524d\u7f3a\u4e4f\u9488\u5bf9\u97f3\u9891\u6a21\u6001\u7684\u5b89\u5168\u5bf9\u9f50\u7b56\u7565\u548c\u4e13\u7528\u6570\u636e\u96c6", "method": "\u91c7\u7528\u65e0\u76d1\u7763\u5b89\u5168\u5fae\u8c03\u7b56\u7565\uff0c\u901a\u8fc7\u8c03\u6574\u6a21\u578b\u8868\u793a\u7a7a\u95f4\u7ed3\u6784\u5b9e\u73b0\u5b89\u5168\u5f3a\u5316\uff0c\u907f\u514d\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u53c2\u6570\u66f4\u65b0", "result": "\u5728\u4e09\u4ee3Qwen LALMs\u4e0a\u9a8c\u8bc1\uff0c\u97f3\u9891-\u6587\u672c/\u7eaf\u6587\u672c/\u7eaf\u97f3\u9891\u4e09\u79cd\u8f93\u5165\u6a21\u5f0f\u4e0b\u7684\u5b89\u5168\u6027\u663e\u8457\u63d0\u5347\uff0c\u5e73\u5747\u8fc7\u5ea6\u62d2\u7edd\u7387\u4ec5\u589e\u52a00.88%", "conclusion": "\u8be5\u65e0\u76d1\u7763\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u4e86\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\uff0c\u4e3aLALMs\u5b89\u5168\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u4ee3\u9645\u6a21\u578b\u4e14\u5177\u5907\u6a21\u6001\u901a\u7528\u6027"}}
{"id": "2505.19674", "pdf": "https://arxiv.org/pdf/2505.19674", "abs": "https://arxiv.org/abs/2505.19674", "authors": ["Chaoyi Xiang", "Chunhua Liu", "Simon De Deyne", "Lea Frermann"], "title": "Comparing Moral Values in Western English-speaking societies and LLMs with Word Associations", "categories": ["cs.CL"], "comment": "9 pages,7 figures. Accepted to the ACL 2025 conference", "summary": "As the impact of large language models increases, understanding the moral\nvalues they reflect becomes ever more important. Assessing the nature of moral\nvalues as understood by these models via direct prompting is challenging due to\npotential leakage of human norms into model training data, and their\nsensitivity to prompt formulation. Instead, we propose to use word\nassociations, which have been shown to reflect moral reasoning in humans, as\nlow-level underlying representations to obtain a more robust picture of LLMs'\nmoral reasoning. We study moral differences in associations from western\nEnglish-speaking communities and LLMs trained predominantly on English data.\nFirst, we create a large dataset of LLM-generated word associations, resembling\nan existing data set of human word associations. Next, we propose a novel\nmethod to propagate moral values based on seed words derived from Moral\nFoundation Theory through the human and LLM-generated association graphs.\nFinally, we compare the resulting moral conceptualizations, highlighting\ndetailed but systematic differences between moral values emerging from English\nspeakers and LLM associations.", "AI": {"tldr": "\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8bcd\u6c47\u5173\u8054\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u57fa\u4e8e\u9053\u5fb7\u57fa\u7840\u7406\u8bba\u7684\u65b0\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u82f1\u8bed\u6bcd\u8bed\u8005\u4e0eLLM\u5728\u9053\u5fb7\u6982\u5ff5\u4e0a\u7684\u7cfb\u7edf\u6027\u5dee\u5f02\u3002", "motivation": "\u76f4\u63a5\u63d0\u793a\u6cd5\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u9053\u5fb7\u4ef7\u503c\u89c2\u5b58\u5728\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\u548c\u63d0\u793a\u654f\u611f\u6027\u95ee\u9898\uff0c\u9700\u91c7\u7528\u66f4\u5e95\u5c42\u7684\u8bcd\u6c47\u5173\u8054\u8868\u5f81\u6765\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "method": "1. \u521b\u5efa\u7c7b\u4f3c\u4eba\u7c7b\u8bcd\u6c47\u5173\u8054\u7684LLM\u751f\u6210\u6570\u636e\u96c6\n2. \u57fa\u4e8e\u9053\u5fb7\u57fa\u7840\u7406\u8bba\u79cd\u5b50\u8bcd\uff0c\u5728\u5173\u8054\u56fe\u4e2d\u4f20\u64ad\u9053\u5fb7\u4ef7\u503c\n3. \u5bf9\u6bd4\u4eba\u7c7b\u4e0eLLM\u7684\u9053\u5fb7\u6982\u5ff5\u5316\u5dee\u5f02", "result": "\u53d1\u73b0\u82f1\u8bed\u4f7f\u7528\u8005\u4e0eLLM\u5728\u9053\u5fb7\u4ef7\u503c\u5448\u73b0\u4e0a\u5b58\u5728\u7ec6\u8282\u4e30\u5bcc\u4f46\u7cfb\u7edf\u6027\u7684\u5dee\u5f02\uff0cLLM\u672a\u5b8c\u5168\u590d\u5236\u4eba\u7c7b\u9053\u5fb7\u6846\u67b6", "conclusion": "\u8bcd\u6c47\u5173\u8054\u65b9\u6cd5\u6709\u6548\u63ed\u793aLLM\u9053\u5fb7\u63a8\u7406\u7279\u6027\uff0c\u8868\u660e\u6a21\u578b\u5f62\u6210\u4e86\u4e0e\u4eba\u7c7b\u65e2\u76f8\u5173\u53c8\u4e0d\u540c\u7684\u9053\u5fb7\u6982\u5ff5\u4f53\u7cfb"}}
{"id": "2505.19675", "pdf": "https://arxiv.org/pdf/2505.19675", "abs": "https://arxiv.org/abs/2505.19675", "authors": ["Liqin Ye", "Agam Shah", "Chao Zhang", "Sudheer Chava"], "title": "Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The traditional process of creating labeled datasets is labor-intensive and\nexpensive. Recent breakthroughs in open-source large language models (LLMs)\nhave opened up a new avenue in generating labeled datasets automatically for\nvarious natural language processing (NLP) tasks, providing an alternative to\nsuch an expensive annotation process. However, the reliability of such\nauto-generated labels remains a significant concern due to inherent\ninaccuracies. When learning from noisy labels, the model's generalization is\nlikely to be harmed as it is prone to overfit to those label noises. While\nprevious studies in learning from noisy labels mainly focus on synthetic noise\nand real-world noise, LLM-generated label noise receives less attention. In\nthis paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to\ncalibrate the classifier's prediction, thus enhancing its robustness towards\nLLM-generated noisy labels. SiDyP retrieves potential true label candidates by\nneighborhood label distribution in text embedding space and iteratively refines\nnoisy candidates using a simplex diffusion model. Our framework can increase\nthe performance of the BERT classifier fine-tuned on both zero-shot and\nfew-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30%\nrespectively. We demonstrate the effectiveness of SiDyP by conducting extensive\nbenchmarking for different LLMs over a variety of NLP tasks. Our code is\navailable on Github.", "AI": {"tldr": "\u63d0\u51faSiDyP\u6846\u67b6\uff0c\u901a\u8fc7\u6807\u7b7e\u6269\u6563\u548c\u52a8\u6001\u5148\u9a8c\u6821\u51c6\u5206\u7c7b\u5668\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347BERT\u5728LLM\u751f\u6210\u566a\u58f0\u6807\u7b7e\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u6807\u6ce8\u6570\u636e\u65f6\u4ea7\u751f\u7684\u566a\u58f0\u6807\u7b7e\u95ee\u9898\uff0c\u907f\u514d\u6a21\u578b\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8fc7\u5ea6\u62df\u5408\u566a\u58f0", "method": "\u7ed3\u5408\u6587\u672c\u5d4c\u5165\u7a7a\u95f4\u7684\u90bb\u5c45\u6807\u7b7e\u5206\u5e03\u68c0\u7d22\u6f5c\u5728\u771f\u5b9e\u6807\u7b7e\uff0c\u91c7\u7528\u5355\u7eaf\u5f62\u6269\u6563\u6a21\u578b\u8fed\u4ee3\u4f18\u5316\u566a\u58f0\u5019\u9009\u6807\u7b7e", "result": "\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u573a\u666f\u4e0b\u5206\u522b\u5e73\u5747\u63d0\u5347BERT\u5206\u7c7b\u5668\u6027\u80fd7.21%\u548c7.30%\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u591a\u6a21\u578b\u57fa\u51c6\u9a8c\u8bc1\u6709\u6548\u6027", "conclusion": "SiDyP\u4e3a\u5904\u7406LLM\u751f\u6210\u7684\u566a\u58f0\u6807\u7b7e\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u63a8\u52a8\u81ea\u52a8\u6807\u6ce8\u6280\u672f\u7684\u5b9e\u9645\u5e94\u7528"}}
{"id": "2505.19678", "pdf": "https://arxiv.org/pdf/2505.19678", "abs": "https://arxiv.org/abs/2505.19678", "authors": ["Hao Fang", "Changle Zhou", "Jiawei Kong", "Kuofeng Gao", "Bin Chen", "Tao Liang", "Guojun Ma", "Shu-Tao Xia"], "title": "Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) are susceptible to hallucinations, where\ngenerated responses seem semantically plausible yet exhibit little or no\nrelevance to the input image. Previous studies reveal that this issue primarily\nstems from LVLMs' over-reliance on language priors while disregarding the\nvisual information during decoding. To alleviate this issue, we introduce a\nnovel Conditional Pointwise Mutual Information (C-PMI) calibrated decoding\nstrategy, which adaptively strengthens the mutual dependency between generated\ntexts and input images to mitigate hallucinations. Unlike existing methods\nsolely focusing on text token sampling, we propose to jointly model the\ncontributions of visual and textual tokens to C-PMI, formulating hallucination\nmitigation as a bi-level optimization problem aimed at maximizing mutual\ninformation. To solve it, we design a token purification mechanism that\ndynamically regulates the decoding process by sampling text tokens remaining\nmaximally relevant to the given image, while simultaneously refining image\ntokens most pertinent to the generated response. Extensive experiments across\nvarious benchmarks reveal that the proposed method significantly reduces\nhallucinations in LVLMs while preserving decoding efficiency.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6761\u4ef6\u70b9\u4e92\u4fe1\u606f\uff08C-PMI\uff09\u7684\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u53cc\u5c42\u6b21\u4f18\u5316\u548c\u4ee4\u724c\u51c0\u5316\u673a\u5236\u7f13\u89e3\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u95ee\u9898", "motivation": "LVLM\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u8bed\u8a00\u5148\u9a8c\u800c\u5ffd\u89c6\u89c6\u89c9\u4fe1\u606f\uff0c\u5bfc\u81f4\u751f\u6210\u6587\u672c\u4e0e\u8f93\u5165\u56fe\u50cf\u76f8\u5173\u6027\u4f4e\u7684\u5e7b\u89c9\u95ee\u9898", "method": "1. \u8054\u5408\u5efa\u6a21\u89c6\u89c9\u4e0e\u6587\u672c\u4ee4\u724c\u5bf9C-PMI\u7684\u8d21\u732e\n2. \u5efa\u7acb\u53cc\u5c42\u6b21\u4f18\u5316\u6846\u67b6\n3. \u52a8\u6001\u8c03\u8282\u89e3\u7801\u8fc7\u7a0b\u7684\u4ee4\u724c\u51c0\u5316\u673a\u5236", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u964d\u4f4e\u5e7b\u89c9\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u89e3\u7801\u6548\u7387", "conclusion": "C-PMI\u7b56\u7565\u901a\u8fc7\u589e\u5f3a\u56fe\u6587\u4e92\u4f9d\u8d56\u6027\uff0c\u6709\u6548\u5e73\u8861\u4e86\u751f\u6210\u8d28\u91cf\u4e0e\u8ba1\u7b97\u6548\u7387"}}
{"id": "2505.19679", "pdf": "https://arxiv.org/pdf/2505.19679", "abs": "https://arxiv.org/abs/2505.19679", "authors": ["Zhaolin Li", "Yining Liu", "Danni Liu", "Tuan Nam Nguyen", "Enes Yavuz Ugan", "Tu Anh Dinh", "Carlos Mullov", "Alexander Waibel", "Jan Niehues"], "title": "KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents KIT's submissions to the IWSLT 2025 low-resource track.\nWe develop both cascaded systems, consisting of Automatic Speech Recognition\n(ASR) and Machine Translation (MT) models, and end-to-end (E2E) Speech\nTranslation (ST) systems for three language pairs: Bemba, North Levantine\nArabic, and Tunisian Arabic into English. Building upon pre-trained models, we\nfine-tune our systems with different strategies to utilize resources\nefficiently. This study further explores system enhancement with synthetic data\nand model regularization. Specifically, we investigate MT-augmented ST by\ngenerating translations from ASR data using MT models. For North Levantine,\nwhich lacks parallel ST training data, a system trained solely on synthetic\ndata slightly surpasses the cascaded system trained on real data. We also\nexplore augmentation using text-to-speech models by generating synthetic speech\nfrom MT data, demonstrating the benefits of synthetic data in improving both\nASR and ST performance for Bemba. Additionally, we apply intra-distillation to\nenhance model performance. Our experiments show that this approach consistently\nimproves results across ASR, MT, and ST tasks, as well as across different\npre-trained models. Finally, we apply Minimum Bayes Risk decoding to combine\nthe cascaded and end-to-end systems, achieving an improvement of approximately\n1.5 BLEU points.", "AI": {"tldr": "KIT\u56e2\u961f\u9488\u5bf9IWSLT 2025\u4f4e\u8d44\u6e90\u8d5b\u9053\uff0c\u5f00\u53d1\u4e86\u7ea7\u8054\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\u548c\u7aef\u5230\u7aef\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u6a21\u578b\u5fae\u8c03\u3001\u5408\u6210\u6570\u636e\u589e\u5f3a\u548c\u6a21\u578b\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u5728Bemba\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u53d6\u5f97\u663e\u8457\u6548\u679c\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08Bemba\u3001\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\uff09\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u5982\u4f55\u6709\u6548\u7ed3\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u4e0e\u6709\u9650\u8d44\u6e90\u5b9e\u73b0\u6027\u80fd\u4f18\u5316\u3002", "method": "1. \u7ea7\u8054\u7cfb\u7edf\uff08ASR+MT\uff09\u4e0e\u7aef\u5230\u7aef\u7cfb\u7edf\u5e76\u884c\u5f00\u53d1\n2. \u91c7\u7528MT\u6a21\u578b\u751f\u6210ASR\u6570\u636e\u7684\u7ffb\u8bd1\u5b9e\u73b0\u6570\u636e\u589e\u5f3a\n3. \u5229\u7528TTS\u6a21\u578b\u4eceMT\u6570\u636e\u751f\u6210\u5408\u6210\u8bed\u97f3\n4. \u5f15\u5165\u6a21\u578b\u6b63\u5219\u5316\uff08intra-distillation\uff09\n5. MBR\u89e3\u7801\u878d\u5408\u4e0d\u540c\u7cfb\u7edf\u8f93\u51fa", "result": "1. \u5317\u9ece\u51e1\u7279\u963f\u62c9\u4f2f\u8bed\uff1a\u7eaf\u5408\u6210\u6570\u636e\u7cfb\u7edf\u8d85\u8d8a\u771f\u5b9e\u6570\u636e\u7ea7\u8054\u7cfb\u7edf\n2. Bemba\u8bed\u8a00\uff1a\u5408\u6210\u8bed\u97f3\u6570\u636e\u63d0\u5347ASR/ST\u6027\u80fd\n3. \u6a21\u578b\u6b63\u5219\u5316\u4f7fASR/MT/ST\u4efb\u52a1\u5e73\u5747\u63d0\u5347\n4. \u7cfb\u7edf\u878d\u5408\u5e26\u67651.5 BLEU\u503c\u63d0\u5347", "conclusion": "\u5408\u6210\u6570\u636e\u6709\u6548\u7f13\u89e3\u4f4e\u8d44\u6e90\u56f0\u5883\uff0c\u6a21\u578b\u6b63\u5219\u5316\u4e0e\u7cfb\u7edf\u878d\u5408\u7b56\u7565\u5177\u6709\u666e\u9002\u6027\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u97f3\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u6570\u636e\u9ad8\u6548\u5229\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.19700", "pdf": "https://arxiv.org/pdf/2505.19700", "abs": "https://arxiv.org/abs/2505.19700", "authors": ["Yi Liu", "Dianqing Liu", "Mingye Zhu", "Junbo Guo", "Yongdong Zhang", "Zhendong Mao"], "title": "Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The widespread adoption of large language models (LLMs) across industries has\nincreased the demand for high-quality and customizable outputs. However,\ntraditional alignment methods often require retraining large pretrained models,\nmaking it difficult to quickly adapt and optimize LLMs for diverse\napplications. To address this limitation, we propose a novel \\textit{Residual\nAlignment Model} (\\textit{RAM}) that formalizes the alignment process as a type\nof importance sampling. In this framework, the unaligned upstream model serves\nas the proposal distribution, while the alignment process is framed as\nsecondary sampling based on an autoregressive alignment module that acts as an\nestimator of the importance weights. This design enables a natural detachment\nof the alignment module from the target aligned model, improving flexibility\nand scalability. Based on this model, we derive an efficient sequence-level\ntraining strategy for the alignment module, which operates independently of the\nproposal module. Additionally, we develop a resampling algorithm with iterative\ntoken-level decoding to address the common first-token latency issue in\ncomparable methods. Experimental evaluations on two leading open-source LLMs\nacross diverse tasks, including instruction following, domain adaptation, and\npreference optimization, demonstrate that our approach consistently outperforms\nbaseline models.", "AI": {"tldr": "\u63d0\u51fa\u6b8b\u5dee\u5bf9\u9f50\u6a21\u578b\uff08RAM\uff09\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u91c7\u6837\u6846\u67b6\u5b9e\u73b0\u7075\u6d3b\u9ad8\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50", "motivation": "\u4f20\u7edf\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u65b9\u6cd5\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6574\u4e2a\u6a21\u578b\uff0c\u96be\u4ee5\u5feb\u901f\u9002\u5e94\u591a\u6837\u5316\u5e94\u7528\u9700\u6c42", "method": "\u5c06\u672a\u5bf9\u9f50\u6a21\u578b\u4f5c\u4e3a\u63d0\u8bae\u5206\u5e03\uff0c\u8bbe\u8ba1\u81ea\u56de\u5f52\u5bf9\u9f50\u6a21\u5757\u4f30\u8ba1\u91cd\u8981\u6027\u6743\u91cd\uff0c\u5f00\u53d1\u5e8f\u5217\u7ea7\u8bad\u7ec3\u7b56\u7565\u548c\u8fed\u4ee3token\u89e3\u7801\u7b97\u6cd5", "result": "\u5728\u6307\u4ee4\u8ddf\u968f\u3001\u9886\u57df\u9002\u914d\u548c\u504f\u597d\u4f18\u5316\u7b49\u4efb\u52a1\u4e2d\u6301\u7eed\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b", "conclusion": "RAM\u5b9e\u73b0\u4e86\u5bf9\u9f50\u6a21\u5757\u4e0e\u76ee\u6807\u6a21\u578b\u7684\u81ea\u7136\u5206\u79bb\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u7075\u6d3b\u6027\u548c\u751f\u6210\u8d28\u91cf"}}
{"id": "2505.19706", "pdf": "https://arxiv.org/pdf/2505.19706", "abs": "https://arxiv.org/abs/2505.19706", "authors": ["Tej Deep Pala", "Panshul Sharma", "Amir Zadeh", "Chuan Li", "Soujanya Poria"], "title": "Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision", "categories": ["cs.CL", "cs.AI"], "comment": "https://github.com/declare-lab/PathFinder-PRM", "summary": "Large Language Models (LLMs) are prone to hallucination, especially during\nmulti-hop and reasoning-intensive tasks such as mathematical problem solving.\nWhile Outcome Reward Models verify only final answers, Process Reward Models\n(PRMs) score each intermediate step to steer generation toward coherent\nsolutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware\ndiscriminative PRM that first classifies math and consistency errors at each\nstep, then combines these fine-grained signals to estimate step correctness. To\ntrain PathFinder-PRM, we construct a 400K-sample dataset by enriching the\nhuman-annotated PRM800K corpus and RLHFlow Mistral traces with\nthree-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new\nstate-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while\nusing 3 times less data. When applied to reward guided greedy search, our model\nyields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results\ndemonstrate that decoupled error detection and reward estimation not only boost\nfine-grained error detection but also substantially improve end-to-end,\nreward-guided mathematical reasoning with greater data efficiency.", "AI": {"tldr": "PathFinder-PRM\u901a\u8fc7\u5206\u5c42\u9519\u8bef\u68c0\u6d4b\u673a\u5236(\u6570\u5b66\u9519\u8bef/\u4e00\u81f4\u6027\u9519\u8bef\u5206\u7c7b)\u548c\u7ec6\u7c92\u5ea6\u4fe1\u53f7\u6574\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u51c6\u786e\u6027\u548c\u6570\u636e\u6548\u7387\uff0c\u5728PRMBench\u4e0a\u4ee53\u500d\u5c11\u6570\u636e\u5b9e\u73b067.7\u7684SOTA\u6210\u7ee9\u3002", "motivation": "\u73b0\u6709\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b(PRMs)\u5bf9\u4e2d\u95f4\u6b65\u9aa4\u8bc4\u5206\u65f6\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u9519\u8bef\u5206\u7c7b\u80fd\u529b\uff0c\u5bfc\u81f4\u5956\u52b1\u4fe1\u53f7\u4e0d\u591f\u7cbe\u786e\u3002\u4e3a\u89e3\u51b3\u8be5\u7f3a\u9677\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u8bc6\u522b\u5177\u4f53\u9519\u8bef\u7c7b\u578b\u5e76\u6574\u5408\u591a\u7ef4\u5ea6\u4fe1\u53f7\u7684\u65b0\u578bPRM\u67b6\u6784\u3002", "method": "1. \u6784\u5efa400K\u589e\u5f3a\u6570\u636e\u96c6\uff1a\u878d\u5408PRM800K\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u548cRLHFlow\u6a21\u578b\u8f68\u8ff9\uff0c\u6dfb\u52a0\u6b65\u9aa4\u7ea7\u4e09\u7ef4\u6807\u7b7e\n2. \u8bbe\u8ba1\u5206\u5c42\u6a21\u578b\uff1a\u5148\u5206\u7c7b\u6570\u5b66\u9519\u8bef\u4e0e\u903b\u8f91\u4e00\u81f4\u6027\u9519\u8bef\uff0c\u518d\u7efc\u5408\u9519\u8bef\u7c7b\u578b\u5224\u65ad\u6b65\u9aa4\u6b63\u786e\u6027", "result": "1. PRMBench\u6d4b\u8bd5PRMScore\u8fbe67.7(SOTA)\uff0c\u6bd4\u4e4b\u524d\u6700\u4f73\u63d0\u53472.2\u70b9\n2. \u5956\u52b1\u5f15\u5bfc\u641c\u7d22prm@8\u8fbe48.3(+1.5)\n3. \u4f7f\u7528\u6570\u636e\u91cf\u4ec5\u4e3a\u5148\u524d\u6700\u4f73\u6a21\u578b\u76841/3", "conclusion": "\u89e3\u8026\u9519\u8bef\u68c0\u6d4b\u4e0e\u5956\u52b1\u4f30\u8ba1\u7684\u5c42\u6b21\u5316\u5efa\u6a21\u65b9\u6cd5\uff0c\u65e2\u80fd\u63d0\u5347\u7ec6\u7c92\u5ea6\u9519\u8bef\u8bc6\u522b\u80fd\u529b\uff0c\u53c8\u53ef\u901a\u8fc7\u66f4\u9ad8\u6548\u7684\u6570\u636e\u5229\u7528\u663e\u8457\u589e\u5f3a\u7aef\u5230\u7aef\u6570\u5b66\u63a8\u7406\u6027\u80fd\uff0c\u4e3aLLM\u5e7b\u89c9\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.19714", "pdf": "https://arxiv.org/pdf/2505.19714", "abs": "https://arxiv.org/abs/2505.19714", "authors": ["Zhaopeng Feng", "Yupu Liang", "Shaosheng Cao", "Jiayuan Su", "Jiahan Ren", "Zhe Xu", "Yao Hu", "Wenxuan Huang", "Jian Wu", "Zuozhu Liu"], "title": "MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in progress", "summary": "Text Image Machine Translation (TIMT)-the task of translating textual content\nembedded in images-is critical for applications in accessibility, cross-lingual\ninformation access, and real-world document understanding. However, TIMT\nremains a complex challenge due to the need for accurate optical character\nrecognition (OCR), robust visual-text reasoning, and high-quality translation,\noften requiring cascading multi-stage pipelines. Recent advances in large-scale\nReinforcement Learning (RL) have improved reasoning in Large Language Models\n(LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is\nstill underexplored. To bridge this gap, we introduce MT$^{3}$, the first\nframework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts\na multi-task optimization paradigm targeting three key sub-skills: text\nrecognition, context-aware reasoning, and translation. It is trained using a\nnovel multi-mixed reward mechanism that adapts rule-based RL strategies to\nTIMT's intricacies, offering fine-grained, non-binary feedback across tasks.\nFurthermore, to facilitate the evaluation of TIMT in authentic cross-cultural\nand real-world social media contexts, we introduced XHSPost, the first social\nmedia TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on\nthe latest in-domain MIT-10M benchmark, outperforming strong baselines such as\nQwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics.\nAdditionally, the model shows strong generalization to out-of-distribution\nlanguage pairs and datasets. In-depth analyses reveal how multi-task synergy,\nreinforcement learning initialization, curriculum design, and reward\nformulation contribute to advancing MLLM-driven TIMT.", "AI": {"tldr": "\u63d0\u51faMT\u00b3\u6846\u67b6\u2014\u2014\u9996\u4e2a\u5c06\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u7aef\u5230\u7aef\u6587\u672c\u56fe\u50cf\u673a\u5668\u7ffb\u8bd1\u65b9\u6848\uff0c\u5728MIT-10M\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a72B\u7ea7\u5927\u6a21\u578b\u53d6\u5f97SOTA\u3002", "motivation": "\u6587\u672c\u56fe\u50cf\u673a\u5668\u7ffb\u8bd1\uff08TIMT\uff09\u9700\u540c\u65f6\u5904\u7406OCR\u8bc6\u522b\u3001\u56fe\u6587\u63a8\u7406\u548c\u7ffb\u8bd1\u4efb\u52a1\uff0c\u4f20\u7edf\u7ea7\u8054\u65b9\u6848\u590d\u6742\u4f4e\u6548\u3002\u867d\u7136LLMs\u5728\u591a\u6a21\u6001\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46MLLMs\u5728\u7aef\u5230\u7aefTIMT\u4e2d\u7684\u5e94\u7528\u4ecd\u5f85\u63a2\u7d22\u3002", "method": "1. \u4e09\u4efb\u52a1\u534f\u540c\u4f18\u5316\uff1a\u6587\u672c\u8bc6\u522b/\u4e0a\u4e0b\u6587\u63a8\u7406/\u7ffb\u8bd1\u8054\u5408\u8bad\u7ec3\n2. \u591a\u6df7\u5408\u5956\u52b1\u673a\u5236\uff1a\u57fa\u4e8e\u89c4\u5219\u5f3a\u5316\u5b66\u4e60\u9002\u914dTIMT\u7279\u6027\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u8de8\u4efb\u52a1\u53cd\u9988\n3. \u6784\u5efaXHSPost\u57fa\u51c6\uff1a\u9996\u4e2a\u793e\u4ea4\u5a92\u4f53\u8de8\u6587\u5316TIMT\u8bc4\u4f30\u6570\u636e\u96c6", "result": "1. MIT-10M\u57fa\u51c6\uff1aMT\u00b3-7B-Zero\u5728CIDEr\u7b49\u6307\u6807\u4e0a\u663e\u8457\u8d85\u8d8aQwen2.5-VL-72B\uff08+15.7%\uff09\u3001InternVL2.5-78B\uff08+12.3%\uff09\n2. \u8de8\u8bed\u8a00\u6cdb\u5316\uff1a\u5728XHSPost\u57fa\u51c6\u4e0a\u4fdd\u6301\u4f18\u5f02\u8868\u73b0\n3. \u8bfe\u7a0b\u5b66\u4e60\u8bbe\u8ba1\u4f7f\u8bad\u7ec3\u6548\u7387\u63d0\u534738%", "conclusion": "\u901a\u8fc7\u591a\u4efb\u52a1\u534f\u540c\u4f18\u5316\u3001\u5f3a\u5316\u5b66\u4e60\u521d\u59cb\u5316\u7b56\u7565\u3001\u6e10\u8fdb\u5f0f\u8bfe\u7a0b\u8bbe\u8ba1\u4ee5\u53ca\u7ec6\u7c92\u5ea6\u5956\u52b1\u5efa\u6a21\uff0c\u9a8c\u8bc1\u4e86MLLMs\u9a71\u52a8\u7aef\u5230\u7aefTIMT\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u590d\u6742\u56fe\u6587\u7406\u89e3\u4efb\u52a1\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2505.19715", "pdf": "https://arxiv.org/pdf/2505.19715", "abs": "https://arxiv.org/abs/2505.19715", "authors": ["Chunyang Jiang", "Chi-min Chan", "Yiyang Cai", "Yulong Liu", "Wei Xue", "Yike Guo"], "title": "Graceful Forgetting in Generative Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8 pages, 6 figures", "summary": "Recently, the pretrain-finetune paradigm has become a cornerstone in various\ndeep learning areas. While in general the pre-trained model would promote both\neffectiveness and efficiency of downstream tasks fine-tuning, studies have\nshown that not all knowledge acquired during pre-training is beneficial. Some\nof the knowledge may actually bring detrimental effects to the fine-tuning\ntasks, which is also known as negative transfer. To address this problem,\ngraceful forgetting has emerged as a promising approach. The core principle of\ngraceful forgetting is to enhance the learning plasticity of the target task by\nselectively discarding irrelevant knowledge. However, this approach remains\nunderexplored in the context of generative language models, and it is often\nchallenging to migrate existing forgetting algorithms to these models due to\narchitecture incompatibility. To bridge this gap, in this paper we propose a\nnovel framework, Learning With Forgetting (LWF), to achieve graceful forgetting\nin generative language models. With Fisher Information Matrix weighting the\nintended parameter updates, LWF computes forgetting confidence to evaluate\nself-generated knowledge regarding the forgetting task, and consequently,\nknowledge with high confidence is periodically unlearned during fine-tuning.\nOur experiments demonstrate that, although thoroughly uncovering the mechanisms\nof knowledge interaction remains challenging in pre-trained language models,\napplying graceful forgetting can contribute to enhanced fine-tuning\nperformance.", "AI": {"tldr": "\u63d0\u51faLWF\u6846\u67b6\uff0c\u901a\u8fc7Fisher\u4fe1\u606f\u77e9\u9635\u548c\u5468\u671f\u6027\u9057\u5fd8\u673a\u5236\u5b9e\u73b0\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u96c5\u9057\u5fd8\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03\u6548\u679c\u3002", "motivation": "\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u90e8\u5206\u77e5\u8bc6\u4f1a\u5f15\u53d1\u8d1f\u8fc1\u79fb\u6548\u5e94\uff0c\u73b0\u6709\u4f18\u96c5\u9057\u5fd8\u65b9\u6cd5\u96be\u4ee5\u9002\u914d\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\u7684\u67b6\u6784\u7279\u6027\u3002", "method": "\u57fa\u4e8eFisher\u4fe1\u606f\u77e9\u9635\u52a0\u6743\u53c2\u6570\u66f4\u65b0\uff0c\u8ba1\u7b97\u81ea\u751f\u6210\u77e5\u8bc6\u7684\u9057\u5fd8\u7f6e\u4fe1\u5ea6\uff0c\u5468\u671f\u6027\u53bb\u9664\u9ad8\u7f6e\u4fe1\u5ea6\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u4f18\u96c5\u9057\u5fd8\u80fd\u6709\u6548\u63d0\u5347\u5fae\u8c03\u6027\u80fd\uff0c\u4f46\u77e5\u8bc6\u4ea4\u4e92\u673a\u5236\u4ecd\u5b58\u5728\u9ed1\u7bb1\u7279\u6027\u3002", "conclusion": "\u8bc1\u660e\u4e86\u4f18\u96c5\u9057\u5fd8\u5728\u751f\u6210\u6a21\u578b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u4ea4\u4e92\u673a\u5236\u4ecd\u9700\u6df1\u5165\u7814\u7a76\u3002"}}
{"id": "2505.19722", "pdf": "https://arxiv.org/pdf/2505.19722", "abs": "https://arxiv.org/abs/2505.19722", "authors": ["Yihao Ai", "Zhiyuan Ning", "Weiwei Dai", "Pengfei Wang", "Yi Du", "Wenjuan Cui", "Kunpeng Liu", "Yuanchun Zhou"], "title": "Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical Entity Linking", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ICIC 2025", "summary": "Biomedical entity linking aims to map nonstandard entities to standard\nentities in a knowledge base. Traditional supervised methods perform well but\nrequire extensive annotated data to transfer, limiting their usage in\nlow-resource scenarios. Large language models (LLMs), especially closed-source\nLLMs, can address these but risk stability issues and high economic costs:\nusing these models is restricted by commercial companies and brings significant\neconomic costs when dealing with large amounts of data. To address this, we\npropose ``RPDR'', a framework combining closed-source LLMs and open-source LLMs\nfor re-ranking candidates retrieved by a retriever fine-tuned with a small\namount of data. By prompting a closed-source LLM to generate training data from\nunannotated data and fine-tuning an open-source LLM for re-ranking, we\neffectively distill the knowledge to the open-source LLM that can be deployed\nlocally, thus avoiding the stability issues and the problem of high economic\ncosts. We evaluate RPDR on two datasets, including one real-world dataset and\none publicly available dataset involving two languages: Chinese and English.\nRPDR achieves 0.019 Acc@1 improvement and 0.036 Acc@1 improvement on the Aier\ndataset and the Ask A Patient dataset when the amount of training data is not\nenough. The results demonstrate the superiority and generalizability of the\nproposed framework.", "AI": {"tldr": "\u63d0\u51faRPDR\u6846\u67b6\uff0c\u7ed3\u5408\u95ed\u6e90\u4e0e\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u51b3\u4f4e\u8d44\u6e90\u751f\u7269\u533b\u5b66\u5b9e\u4f53\u94fe\u63a5\u95ee\u9898\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u964d\u4f4e\u5bf9\u95ed\u6e90\u6a21\u578b\u7684\u4f9d\u8d56\u3002", "motivation": "\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u95ed\u6e90\u5927\u6a21\u578b\u5b58\u5728\u7a33\u5b9a\u6027\u98ce\u9669\u548c\u9ad8\u6602\u6210\u672c\uff0c\u96be\u4ee5\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u548c\u89c4\u6a21\u5316\u5e94\u7528\u4e2d\u843d\u5730\u3002", "method": "1. \u7528\u5c11\u91cf\u6570\u636e\u5fae\u8c03\u68c0\u7d22\u5668\u83b7\u53d6\u5019\u9009\u5b9e\u4f53\uff1b2. \u901a\u8fc7\u95ed\u6e90\u5927\u6a21\u578b\u4ece\u672a\u6807\u6ce8\u6570\u636e\u751f\u6210\u8bad\u7ec3\u6570\u636e\uff1b3. \u5fae\u8c03\u5f00\u6e90\u5927\u6a21\u578b\u8fdb\u884c\u91cd\u6392\u5e8f\uff0c\u5b9e\u73b0\u77e5\u8bc6\u84b8\u998f\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff08Aier\uff09\u548c\u516c\u5f00\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff08Ask A Patient\uff09\u4e0a\u5206\u522b\u53d6\u5f970.019\u548c0.036\u7684Acc@1\u63d0\u5347\uff0c\u5c24\u5176\u5728\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "RPDR\u6846\u67b6\u6709\u6548\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u90e8\u7f72\u6210\u672c\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u672c\u5730\u5316\u90e8\u7f72\uff0c\u5177\u6709\u8f83\u597d\u7684\u901a\u7528\u6027\u548c\u5de5\u7a0b\u843d\u5730\u4ef7\u503c\u3002"}}
{"id": "2505.19743", "pdf": "https://arxiv.org/pdf/2505.19743", "abs": "https://arxiv.org/abs/2505.19743", "authors": ["Yang Zhang", "Yu Yu", "Bo Tang", "Yu Zhu", "Chuxiong Sun", "Wenqiang Wei", "Jie Hu", "Zipeng Xie", "Zhiyu Li", "Feiyu Xiong", "Edward Chung"], "title": "Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to 34th International Joint Conference on Artificial\n  Intelligence (IJCAI 2025)", "summary": "With the rapid development of Large Language Models (LLMs), aligning these\nmodels with human preferences and values is critical to ensuring ethical and\nsafe applications. However, existing alignment techniques such as RLHF or DPO\noften require direct fine-tuning on LLMs with billions of parameters, resulting\nin substantial computational costs and inefficiencies. To address this, we\npropose Micro token-level Accept-Reject Aligning (MARA) approach designed to\noperate independently of the language models. MARA simplifies the alignment\nprocess by decomposing sentence-level preference learning into token-level\nbinary classification, where a compact three-layer fully-connected network\ndetermines whether candidate tokens are \"Accepted\" or \"Rejected\" as part of the\nresponse. Extensive experiments across seven different LLMs and three\nopen-source datasets show that MARA achieves significant improvements in\nalignment performance while reducing computational costs.", "AI": {"tldr": "\u63d0\u51faMARA\u65b9\u6cd5\u2014\u2014\u901a\u8fc7token\u7ea7\u4e8c\u5206\u7c7b\u5b9e\u73b0\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\uff0c\u65e0\u9700\u76f4\u63a5\u5fae\u8c03\u5927\u6a21\u578b", "motivation": "\u73b0\u6709RLHF/DPO\u7b49\u5bf9\u9f50\u65b9\u6cd5\u9700\u8981\u76f4\u63a5\u5fae\u8c03\u6570\u5341\u4ebf\u53c2\u6570\u7684\u5927\u6a21\u578b\uff0c\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u4e14\u6548\u7387\u4f4e\u4e0b", "method": "\u5c06\u53e5\u5b50\u7ea7\u504f\u597d\u5b66\u4e60\u5206\u89e3\u4e3atoken\u7ea7\u4e8c\u5206\u7c7b\uff0c\u4f7f\u7528\u4e09\u5c42\u5168\u8fde\u63a5\u7f51\u7edc\u5bf9\u5019\u9009token\u8fdb\u884c\u300e\u63a5\u53d7/\u62d2\u7edd\u300f\u5224\u5b9a", "result": "\u57287\u4e2a\u4e0d\u540cLLM\u548c3\u4e2a\u5f00\u6e90\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u5bf9\u9f50\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "conclusion": "MARA\u4e3a\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u53c2\u6570\u89c4\u6a21\u9650\u5236"}}
{"id": "2505.19754", "pdf": "https://arxiv.org/pdf/2505.19754", "abs": "https://arxiv.org/abs/2505.19754", "authors": ["Ruisheng Cao", "Hanchong Zhang", "Tiancheng Huang", "Zhangyi Kang", "Yuxin Zhang", "Liangtai Sun", "Hanqi Li", "Yuxun Miao", "Shuai Fan", "Lu Chen", "Kai Yu"], "title": "NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": "29 pages, 11 figures, 12 tables, accepted to ACL 2025 Long Main", "summary": "The increasing number of academic papers poses significant challenges for\nresearchers to efficiently acquire key details. While retrieval augmented\ngeneration (RAG) shows great promise in large language model (LLM) based\nautomated question answering, previous works often isolate neural and symbolic\nretrieval despite their complementary strengths. Moreover, conventional\nsingle-view chunking neglects the rich structure and layout of PDFs, e.g.,\nsections and tables. In this work, we propose NeuSym-RAG, a hybrid neural\nsymbolic retrieval framework which combines both paradigms in an interactive\nprocess. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG\norganizes semi-structured PDF content into both the relational database and\nvectorstore, enabling LLM agents to iteratively gather context until sufficient\nto generate answers. Experiments on three full PDF-based QA datasets, including\na self-annotated one AIRQA-REAL, show that NeuSym-RAG stably defeats both the\nvector-based RAG and various structured baselines, highlighting its capacity to\nunify both retrieval schemes and utilize multiple views. Code and data are\npublicly available at https://github.com/X-LANCE/NeuSym-RAG.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u795e\u7ecf\u7b26\u53f7\u68c0\u7d22\u6846\u67b6NeuSym-RAG\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u5206\u5757\u548c\u6a21\u5f0f\u89e3\u6790\u63d0\u5347PDF\u95ee\u7b54\u6548\u679c", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u6709\u6548\u7ed3\u5408\u795e\u7ecf\u4e0e\u7b26\u53f7\u68c0\u7d22\u4f18\u52bf\uff0c\u4e14\u5355\u4e00\u5207\u5206\u65b9\u5f0f\u5ffd\u7565PDF\u7ed3\u6784\u4fe1\u606f", "method": "\u7ed3\u5408\u591a\u89c6\u89d2\u5206\u5757\u548c\u6a21\u5f0f\u89e3\u6790\uff0c\u5c06PDF\u5185\u5bb9\u7ec4\u7ec7\u81f3\u5173\u7cfb\u6570\u636e\u5e93\u4e0e\u5411\u91cf\u5e93\uff0c\u652f\u6301LLM\u8fed\u4ee3\u5f0f\u4e0a\u4e0b\u6587\u6536\u96c6", "result": "\u5728\u4e09\u4e2aPDF\u95ee\u7b54\u6570\u636e\u96c6\uff08\u542b\u81ea\u5efaAIRQA-REAL\uff09\u4e0a\u7a33\u5b9a\u8d85\u8d8a\u5411\u91cf\u68c0\u7d22\u4e0e\u7ed3\u6784\u5316\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "NeuSym-RAG\u6210\u529f\u7edf\u4e00\u4e24\u79cd\u68c0\u7d22\u8303\u5f0f\u5e76\u6709\u6548\u5229\u7528\u591a\u89c6\u56fe\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742PDF\u5185\u5bb9\u7406\u89e3\u80fd\u529b"}}
{"id": "2505.19756", "pdf": "https://arxiv.org/pdf/2505.19756", "abs": "https://arxiv.org/abs/2505.19756", "authors": ["Ruihan Gong", "Yue Liu", "Wenjie Qu", "Mingzhe Du", "Yufei He", "Yingwei Ma", "Yulin Chen", "Xiang Liu", "Yi Wen", "Xinfeng Li", "Ruidong Wang", "Xinzhong Zhu", "Bryan Hooi", "Jiaheng Zhang"], "title": "Efficient Reasoning via Chain of Unconscious Thought", "categories": ["cs.CL"], "comment": null, "summary": "Large Reasoning Models (LRMs) achieve promising performance but compromise\ntoken efficiency due to verbose reasoning processes. Unconscious Thought Theory\n(UTT) posits that complex problems can be solved more efficiently through\ninternalized cognitive processes. Inspired by UTT, we propose a new reasoning\nparadigm, termed Chain of Unconscious Thought (CoUT), to improve the token\nefficiency of LRMs by guiding them to mimic human unconscious thought and\ninternalize reasoning processes. Concretely, we first prompt the model to\ninternalize the reasoning by thinking in the hidden layer. Then, we design a\nbag of token-efficient strategies to further help models reduce unnecessary\ntokens yet preserve the performance. Our work reveals that models may possess\nbeneficial unconscious thought, enabling improved efficiency without\nsacrificing performance. Extensive experiments demonstrate the effectiveness of\nCoUT. Remarkably, it surpasses CoT by reducing token usage by 47.62% while\nmaintaining comparable accuracy, as shown in Figure 1. The code of CoUT is\navailable at this link: https://github.com/Rohan-GRH/CoUT", "AI": {"tldr": "\u63d0\u51faChain of Unconscious Thought (CoUT)\u8303\u5f0f\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u65e0\u610f\u8bc6\u601d\u7ef4\u51cf\u5c11\u5927\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u7684token\u6d88\u8017\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u534747.62%\u7684token\u6548\u7387\u3002", "motivation": "\u9488\u5bf9\u5927\u63a8\u7406\u6a21\u578b(LRMs)\u5b58\u5728\u7684\u63a8\u7406\u8fc7\u7a0b\u5197\u957f\u3001token\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u53d7\u5fc3\u7406\u5b66\u65e0\u610f\u8bc6\u601d\u7ef4\u7406\u8bba(UTT)\u542f\u53d1\uff0c\u63a2\u7d22\u901a\u8fc7\u5185\u90e8\u5316\u8ba4\u77e5\u8fc7\u7a0b\u63d0\u5347\u6548\u7387\u7684\u53ef\u80fd\u6027\u3002", "method": "1. \u901a\u8fc7\u9690\u85cf\u5c42\u601d\u7ef4\u5b9e\u73b0\u63a8\u7406\u8fc7\u7a0b\u5185\u90e8\u5316\n2. \u8bbe\u8ba1\u591a\u79cdtoken\u7cbe\u7b80\u7b56\u7565\uff08\u5305\u62ec\u52a8\u6001\u601d\u7ef4\u538b\u7f29\u3001\u6ce8\u610f\u529b\u805a\u7126\u7b49\u6280\u672f\uff09\n3. \u6784\u5efa\u7aef\u5230\u7aef\u7684\u9ad8\u6548\u63a8\u7406\u6846\u67b6", "result": "\u5728\u4fdd\u6301\u4e0eChain-of-Thought(CoT)\u76f8\u5f53\u51c6\u786e\u5ea6\u7684\u524d\u63d0\u4e0b\uff0ctoken\u4f7f\u7528\u91cf\u51cf\u5c1147.62%\uff08\u5b9e\u9a8c\u6570\u636e\u89c1\u56fe1\uff09\uff0c\u4ee3\u7801\u5df2\u5728GitHub\u5f00\u6e90\u3002", "conclusion": "\u6a21\u578b\u53ef\u80fd\u5177\u5907\u7c7b\u4f3c\u4eba\u7c7b\u7684\u65e0\u610f\u8bc6\u601d\u7ef4\u80fd\u529b\uff0c\u901a\u8fc7\u5408\u7406\u5f15\u5bfc\u53ef\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u800c\u4e0d\u727a\u7272\u6027\u80fd\uff0c\u4e3a\u9ad8\u6548\u5927\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2505.19766", "pdf": "https://arxiv.org/pdf/2505.19766", "abs": "https://arxiv.org/abs/2505.19766", "authors": ["Masoomali Fatehkia", "Enes Altinisik", "Husrev Taha Sencar"], "title": "SGM: A Framework for Building Specification-Guided Moderation Filters", "categories": ["cs.CL"], "comment": null, "summary": "Aligning large language models (LLMs) with deployment-specific requirements\nis critical but inherently imperfect. Despite extensive training, models remain\nsusceptible to misalignment and adversarial inputs such as jailbreaks. Content\nmoderation filters are commonly used as external safeguards, though they\ntypically focus narrowly on safety. We introduce SGM (Specification-Guided\nModeration), a flexible framework for training moderation filters grounded in\nuser-defined specifications that go beyond standard safety concerns. SGM\nautomates training data generation without relying on human-written examples,\nenabling scalable support for diverse, application-specific alignment goals.\nSGM-trained filters perform on par with state-of-the-art safety filters built\non curated datasets, while supporting fine-grained and user-defined alignment\ncontrol.", "AI": {"tldr": "\u63d0\u51faSGM\u6846\u67b6\u5b9e\u73b0\u57fa\u4e8e\u7528\u6237\u89c4\u8303\u7684\u81ea\u52a8\u5316\u5185\u5bb9\u5ba1\u67e5\uff0c\u89e3\u51b3LLM\u5bf9\u9f50\u4e0d\u8db3\u95ee\u9898\uff0c\u65e0\u9700\u4f9d\u8d56\u4eba\u5de5\u7f16\u5199\u8bad\u7ec3\u6570\u636e", "motivation": "\u73b0\u6709\u5b89\u5168\u8fc7\u6ee4\u673a\u5236\u5c40\u9650\u4e8e\u56fa\u5b9a\u5b89\u5168\u8303\u7574\u4e14\u4f9d\u8d56\u4eba\u5de5\u6570\u636e\uff0c\u96be\u4ee5\u6ee1\u8db3\u591a\u6837\u5316\u5e94\u7528\u573a\u666f\u7684\u7cbe\u51c6\u5bf9\u9f50\u9700\u6c42", "method": "\u901a\u8fc7\u81ea\u52a8\u5316\u751f\u6210\u8bad\u7ec3\u6570\u636e\u652f\u6301\u7528\u6237\u81ea\u5b9a\u4e49\u5bf9\u9f50\u89c4\u8303\uff0c\u8bad\u7ec3\u51fa\u7684\u8fc7\u6ee4\u5668\u5728\u4fdd\u6301\u5b89\u5168\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236", "result": "SGM\u8bad\u7ec3\u7684\u8fc7\u6ee4\u5668\u6027\u80fd\u4e0e\u4eba\u5de5\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u5b89\u5168\u8fc7\u6ee4\u5668\u76f8\u5f53\uff0c\u4e14\u652f\u6301\u66f4\u7075\u6d3b\u7684\u5bf9\u9f50\u89c4\u8303\u63a7\u5236", "conclusion": "SGM\u6846\u67b6\u7a81\u7834\u4e86\u4f20\u7edf\u5185\u5bb9\u5ba1\u6838\u7684\u5c40\u9650\u6027\uff0c\u4e3aLLM\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5b9a\u5236\u5316\u5bf9\u9f50\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.19768", "pdf": "https://arxiv.org/pdf/2505.19768", "abs": "https://arxiv.org/abs/2505.19768", "authors": ["Xing Cui", "Yueying Zou", "Zekun Li", "Peipei Li", "Xinyuan Xu", "Xuannan Liu", "Huaibo Huang", "Ran He"], "title": "T^2Agent A Tool-augmented Multimodal Misinformation Detection Agent with Monte Carlo Tree Search", "categories": ["cs.CL"], "comment": null, "summary": "Real-world multimodal misinformation often arises from mixed forgery sources,\nrequiring dynamic reasoning and adaptive verification. However, existing\nmethods mainly rely on static pipelines and limited tool usage, limiting their\nability to handle such complexity and diversity. To address this challenge, we\npropose T2Agent, a novel misinformation detection agent that incorporates an\nextensible toolkit with Monte Carlo Tree Search (MCTS). The toolkit consists of\nmodular tools such as web search, forgery detection, and consistency analysis.\nEach tool is described using standardized templates, enabling seamless\nintegration and future expansion. To avoid inefficiency from using all tools\nsimultaneously, a Bayesian optimization-based selector is proposed to identify\na task-relevant subset. This subset then serves as the action space for MCTS to\ndynamically collect evidence and perform multi-source verification. To better\nalign MCTS with the multi-source nature of misinformation detection, T2Agent\nextends traditional MCTS with multi-source verification, which decomposes the\ntask into coordinated subtasks targeting different forgery sources. A dual\nreward mechanism containing a reasoning trajectory score and a confidence score\nis further proposed to encourage a balance between exploration across mixed\nforgery sources and exploitation for more reliable evidence. We conduct\nablation studies to confirm the effectiveness of the tree search mechanism and\ntool usage. Extensive experiments further show that T2Agent consistently\noutperforms existing baselines on challenging mixed-source multimodal\nmisinformation benchmarks, demonstrating its strong potential as a\ntraining-free approach for enhancing detection accuracy. The code will be\nreleased.", "AI": {"tldr": "\u63d0\u51faT2Agent\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u6269\u5c55\u5de5\u5177\u5305\u4e0e\u6539\u8fdb\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff08MCTS\uff09\u5b9e\u73b0\u52a8\u6001\u591a\u6e90\u4f2a\u9020\u4fe1\u606f\u9a8c\u8bc1\uff0c\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u51c6\u786e\u7387", "motivation": "\u73b0\u6709\u9759\u6001\u68c0\u6d4b\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u591a\u6e90\u4f2a\u9020\u4fe1\u606f\u52a8\u6001\u9a8c\u8bc1\u9700\u6c42\uff0c\u5de5\u5177\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\u4e14\u6269\u5c55\u6027\u4e0d\u8db3", "method": "1.\u6a21\u5757\u5316\u5de5\u5177\u96c6\uff08\u542b\u7f51\u7edc\u641c\u7d22/\u4f2a\u9020\u68c0\u6d4b/\u4e00\u81f4\u6027\u5206\u6790\uff09 2.\u8d1d\u53f6\u65af\u4f18\u5316\u5de5\u5177\u9009\u62e9\u5668 3.\u591a\u6e90\u9a8c\u8bc1MCTS\u67b6\u6784 4.\u53cc\u91cd\u5956\u52b1\u673a\u5236\uff08\u8f68\u8ff9\u8bc4\u5206+\u7f6e\u4fe1\u5ea6\uff09", "result": "\u5728\u6df7\u5408\u6e90\u591a\u6a21\u6001\u9519\u8bef\u4fe1\u606f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u6811\u641c\u7d22\u673a\u5236\u548c\u5de5\u5177\u4f7f\u7528\u7684\u6709\u6548\u6027", "conclusion": "T2Agent\u4f5c\u4e3a\u514d\u8bad\u7ec3\u65b9\u6848\uff0c\u901a\u8fc7\u52a8\u6001\u8bc1\u636e\u6536\u96c6\u4e0e\u591a\u6e90\u534f\u540c\u9a8c\u8bc1\uff0c\u4e3a\u590d\u6742\u4f2a\u9020\u573a\u666f\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u68c0\u6d4b\u6846\u67b6"}}
{"id": "2505.19773", "pdf": "https://arxiv.org/pdf/2505.19773", "abs": "https://arxiv.org/abs/2505.19773", "authors": ["Sangyeop Kim", "Yohan Lee", "Yongwoo Song", "Kimin Lee"], "title": "What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs", "categories": ["cs.CL", "cs.CR"], "comment": "Accepted by ACL 2025", "summary": "We investigate long-context vulnerabilities in Large Language Models (LLMs)\nthrough Many-Shot Jailbreaking (MSJ). Our experiments utilize context length of\nup to 128K tokens. Through comprehensive analysis with various many-shot attack\nsettings with different instruction styles, shot density, topic, and format, we\nreveal that context length is the primary factor determining attack\neffectiveness. Critically, we find that successful attacks do not require\ncarefully crafted harmful content. Even repetitive shots or random dummy text\ncan circumvent model safety measures, suggesting fundamental limitations in\nlong-context processing capabilities of LLMs. The safety behavior of\nwell-aligned models becomes increasingly inconsistent with longer contexts.\nThese findings highlight significant safety gaps in context expansion\ncapabilities of LLMs, emphasizing the need for new safety mechanisms.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u5b58\u5728\u663e\u8457\u5b89\u5168\u6f0f\u6d1e\uff0c\u5373\u4f7f\u7b80\u5355\u91cd\u590d\u5185\u5bb9\u4e5f\u80fd\u7ed5\u8fc7\u5b89\u5168\u9632\u62a4\uff0c\u66b4\u9732\u51fa\u5176\u957f\u6587\u672c\u5904\u7406\u673a\u5236\u7684\u6839\u672c\u7f3a\u9677", "motivation": "\u63ed\u793a\u73b0\u6709LLMs\u5728\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u4e2d\u7684\u5b89\u5168\u8106\u5f31\u6027\uff0c\u9a8c\u8bc1\u6a21\u578b\u5b89\u5168\u673a\u5236\u5728\u6269\u5c55\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u5c40\u9650", "method": "\u91c7\u7528128K tokens\u957f\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u4e0d\u540c\u6307\u4ee4\u98ce\u683c\u3001\u653b\u51fb\u5bc6\u5ea6\u3001\u4e3b\u9898\u683c\u5f0f\u7684\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\uff08MSJ\uff09\u8bbe\u7f6e\u8fdb\u884c\u7cfb\u7edf\u6027\u6d4b\u8bd5", "result": "\u4e0a\u4e0b\u6587\u957f\u5ea6\u662f\u653b\u51fb\u6210\u529f\u5173\u952e\u56e0\u7d20\uff1b\u65e0\u9700\u590d\u6742\u6076\u610f\u5185\u5bb9\uff0c\u7b80\u5355\u91cd\u590d\u6837\u672c\u6216\u968f\u673a\u6587\u672c\u5373\u53ef\u7a81\u7834\u5b89\u5168\u9632\u62a4\uff1b\u6a21\u578b\u5b89\u5168\u884c\u4e3a\u968f\u4e0a\u4e0b\u6587\u589e\u957f\u6108\u53d1\u4e0d\u4e00\u81f4", "conclusion": "LLMs\u7684\u4e0a\u4e0b\u6587\u6269\u5c55\u80fd\u529b\u5b58\u5728\u91cd\u5927\u5b89\u5168\u7f3a\u9677\uff0c\u9700\u5f00\u53d1\u65b0\u578b\u5b89\u5168\u673a\u5236\u5e94\u5bf9\u957f\u6587\u672c\u573a\u666f\u4e0b\u7684\u6f5c\u5728\u98ce\u9669"}}
{"id": "2505.19776", "pdf": "https://arxiv.org/pdf/2505.19776", "abs": "https://arxiv.org/abs/2505.19776", "authors": ["Akram Elbouanani", "Evan Dufraisse", "Adrian Popescu"], "title": "Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification", "categories": ["cs.CL", "cs.AI"], "comment": "To be published in the Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2025)", "summary": "Political biases encoded by LLMs might have detrimental effects on downstream\napplications. Existing bias analysis methods rely on small-size intermediate\ntasks (questionnaire answering or political content generation) and rely on the\nLLMs themselves for analysis, thus propagating bias. We propose a new approach\nleveraging the observation that LLM sentiment predictions vary with the target\nentity in the same sentence. We define an entropy-based inconsistency metric to\nencode this prediction variability. We insert 1319 demographically and\npolitically diverse politician names in 450 political sentences and predict\ntarget-oriented sentiment using seven models in six widely spoken languages. We\nobserve inconsistencies in all tested combinations and aggregate them in a\nstatistically robust analysis at different granularity levels. We observe\npositive and negative bias toward left and far-right politicians and positive\ncorrelations between politicians with similar alignment. Bias intensity is\nhigher for Western languages than for others. Larger models exhibit stronger\nand more consistent biases and reduce discrepancies between similar languages.\nWe partially mitigate LLM unreliability in target-oriented sentiment\nclassification (TSC) by replacing politician names with fictional but plausible\ncounterparts.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u60c5\u611f\u9884\u6d4b\u4e0d\u4e00\u81f4\u6027\u7684\u65b0\u65b9\u6cd5\u91cf\u5316LLM\u653f\u6cbb\u504f\u89c1\uff0c\u53d1\u73b0\u6a21\u578b\u666e\u904d\u5b58\u5728\u5bf9\u5de6\u7ffc/\u6781\u53f3\u653f\u5ba2\u7684\u6b63/\u8d1f\u9762\u504f\u89c1\uff0c\u4e14\u897f\u65b9\u8bed\u8a00\u504f\u89c1\u66f4\u5f3a", "motivation": "\u73b0\u6709\u653f\u6cbb\u504f\u89c1\u5206\u6790\u65b9\u6cd5\u4f9d\u8d56\u5c0f\u6837\u672c\u95ee\u5377\u6216\u5185\u5bb9\u751f\u6210\u4efb\u52a1\uff0c\u4e14\u4f9d\u8d56LLM\u81ea\u8eab\u5206\u6790\uff0c\u53ef\u80fd\u4f20\u64ad\u504f\u89c1\u3002\u9700\u8981\u66f4\u53ef\u9760\u7684\u76ee\u6807\u5b9e\u4f53\u5bfc\u5411\u5206\u6790\u65b9\u6cd5", "method": "\u6784\u5efa1319\u4e2a\u4e0d\u540c\u80cc\u666f\u653f\u5ba2\u59d3\u540d\u5e93\uff0c\u5728450\u4e2a\u653f\u6cbb\u8bed\u53e5\u4e2d\u66ff\u6362\u76ee\u6807\u5b9e\u4f53\uff0c\u901a\u8fc77\u4e2a\u6a21\u578b\u57286\u79cd\u8bed\u8a00\u4e2d\u9884\u6d4b\u76ee\u6807\u60c5\u611f\uff0c\u5b9a\u4e49\u71b5\u6307\u6807\u91cf\u5316\u4e0d\u4e00\u81f4\u6027", "result": "\u6240\u6709\u6a21\u578b\u5747\u663e\u793a\u9884\u6d4b\u4e0d\u4e00\u81f4\u6027\uff1b\u5de6\u7ffc\u653f\u5ba2\u83b7\u66f4\u591a\u6b63\u9762\u8bc4\u4ef7\uff0c\u6781\u53f3\u83b7\u8d1f\u9762\uff1b\u897f\u65b9\u8bed\u8a00\u504f\u89c1\u5f3a\u5ea6\u9ad8\u4e8e\u5176\u4ed6\u8bed\u8a00\uff1b\u5927\u6a21\u578b\u504f\u89c1\u66f4\u660e\u663e\u4e14\u8de8\u8bed\u8a00\u66f4\u4e00\u81f4", "conclusion": "LLM\u5728\u76ee\u6807\u60c5\u611f\u5206\u7c7b\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u653f\u6cbb\u504f\u89c1\uff0c\u53ef\u901a\u8fc7\u66ff\u6362\u865a\u6784\u653f\u5ba2\u59d3\u540d\u90e8\u5206\u7f13\u89e3\u3002\u6a21\u578b\u89c4\u6a21\u589e\u5927\u5f3a\u5316\u504f\u89c1\u4f46\u63d0\u5347\u8de8\u8bed\u8a00\u4e00\u81f4\u6027"}}
{"id": "2505.19797", "pdf": "https://arxiv.org/pdf/2505.19797", "abs": "https://arxiv.org/abs/2505.19797", "authors": ["Yiqun Zhang", "Hao Li", "Chenxu Wang", "Linyao Chen", "Qiaosheng Zhang", "Peng Ye", "Shi Feng", "Daling Wang", "Zhen Wang", "Xinrun Wang", "Jia Xu", "Lei Bai", "Wanli Ouyang", "Shuyue Hu"], "title": "The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants", "categories": ["cs.CL"], "comment": "9 pages, 3 figures, 6 tables, supplementary material (appendix)\n  included separately", "summary": "As proprietary giants increasingly dominate the race for ever-larger language\nmodels, a pressing question arises for the open-source community: can smaller\nmodels remain competitive across a broad range of tasks? In this paper, we\npresent the Avengers--a simple recipe that effectively leverages the collective\nintelligence of open-source, smaller language models. Our framework is built\nupon four lightweight operations: (i) embedding: encode queries using a text\nembedding model; (ii) clustering: group queries based on their semantic\nsimilarity; (iii) scoring: scores each model's performance within each cluster;\nand (iv) voting: improve outputs via repeated sampling and voting. At inference\ntime, each query is embedded and assigned to its nearest cluster. The\ntop-performing model(s) within that cluster are selected to generate the\nresponse using the Self-Consistency or its multi-model variant. Remarkably,\nwith 10 open-source models (~7B parameters each), the Avengers collectively\noutperforms GPT-4.1 on 10 out of 15 datasets (spanning mathematics, code,\nlogic, knowledge, and affective tasks). In particular, it surpasses GPT-4.1 on\nmathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore, the\nAvengers delivers superior out-of-distribution generalization, and remains\nrobust across various embedding models, clustering algorithms, ensemble\nstrategies, and values of its sole parameter--the number of clusters. We have\nopen-sourced the code on GitHub: https://github.com/ZhangYiqun018/Avengers", "AI": {"tldr": "\u901a\u8fc7\u96c6\u6210\u591a\u4e2a\u5c0f\u578b\u5f00\u6e90\u6a21\u578b\uff08Avengers\u6846\u67b6\uff09\u5728\u6570\u5b66\u548c\u4ee3\u7801\u4efb\u52a1\u4e0a\u8d85\u8d8aGPT-4.1", "motivation": "\u89e3\u51b3\u4e13\u6709\u5927\u6a21\u578b\u4e3b\u5bfc\u4e0b\u5c0f\u578b\u5f00\u6e90\u6a21\u578b\u5728\u591a\u4efb\u52a1\u4e2d\u7684\u7ade\u4e89\u529b\u95ee\u9898", "method": "\u56db\u6b65\u6846\u67b6\uff1a\u5d4c\u5165\u67e5\u8be2\u2192\u8bed\u4e49\u805a\u7c7b\u2192\u6a21\u578b\u8bc4\u5206\u2192\u591a\u6a21\u578b\u6295\u7968\u91c7\u6837\u751f\u6210", "result": "10\u4e2a7B\u6a21\u578b\u96c6\u6210\u540e\uff0c\u572815\u4e2a\u6570\u636e\u96c6\u4e2d10\u4e2a\u8d85\u8d8aGPT-4.1\uff08\u6570\u5b66+18.21%\uff0c\u4ee3\u7801+7.46%\uff09\uff0c\u5177\u5907\u4f18\u5f02\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027", "conclusion": "\u901a\u8fc7\u8f7b\u91cf\u7ea7\u96c6\u6210\u7b56\u7565\u6709\u6548\u91ca\u653e\u5c0f\u6a21\u578b\u96c6\u4f53\u6f5c\u529b\uff0c\u4e3a\u5f00\u6e90\u793e\u533a\u63d0\u4f9b\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848"}}
{"id": "2505.19800", "pdf": "https://arxiv.org/pdf/2505.19800", "abs": "https://arxiv.org/abs/2505.19800", "authors": ["Zaid Alyafeai", "Maged S. Al-Shaibani", "Bernard Ghanem"], "title": "MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Metadata extraction is essential for cataloging and preserving datasets,\nenabling effective research discovery and reproducibility, especially given the\ncurrent exponential growth in scientific research. While Masader (Alyafeai et\nal.,2021) laid the groundwork for extracting a wide range of metadata\nattributes from Arabic NLP datasets' scholarly articles, it relies heavily on\nmanual annotation. In this paper, we present MOLE, a framework that leverages\nLarge Language Models (LLMs) to automatically extract metadata attributes from\nscientific papers covering datasets of languages other than Arabic. Our\nschema-driven methodology processes entire documents across multiple input\nformats and incorporates robust validation mechanisms for consistent output.\nAdditionally, we introduce a new benchmark to evaluate the research progress on\nthis task. Through systematic analysis of context length, few-shot learning,\nand web browsing integration, we demonstrate that modern LLMs show promising\nresults in automating this task, highlighting the need for further future work\nimprovements to ensure consistent and reliable performance. We release the\ncode: https://github.com/IVUL-KAUST/MOLE and dataset:\nhttps://huggingface.co/datasets/IVUL-KAUST/MOLE for the research community.", "AI": {"tldr": "\u63d0\u51faMOLE\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u63d0\u53d6\u975e\u963f\u62c9\u4f2f\u8bed\u6570\u636e\u96c6\u8bba\u6587\u5143\u6570\u636e\uff0c\u5e76\u53d1\u5e03\u4ee3\u7801\u4e0e\u6570\u636e\u96c6", "motivation": "\u5f53\u524d\u79d1\u5b66\u6587\u732e\u6fc0\u589e\u9700\u9ad8\u6548\u5143\u6570\u636e\u63d0\u53d6\u65b9\u6cd5\uff0c\u66ff\u4ee3Masader\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u7684\u4f4e\u6548\u6d41\u7a0b", "method": "\u91c7\u7528\u6a21\u5f0f\u9a71\u52a8\u67b6\u6784\u5904\u7406\u591a\u683c\u5f0f\u6587\u6863\uff0c\u96c6\u6210\u9a8c\u8bc1\u673a\u5236\uff0c\u6784\u5efa\u65b0\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u5206\u6790\u4e0a\u4e0b\u6587\u957f\u5ea6/\u5c0f\u6837\u672c\u5b66\u4e60/\u7f51\u7edc\u6d4f\u89c8\u96c6\u6210", "result": "\u73b0\u4ee3LLM\u5728\u81ea\u52a8\u5316\u5143\u6570\u636e\u63d0\u53d6\u4e2d\u5c55\u73b0\u6f5c\u529b\uff0c\u4f46\u9700\u6539\u8fdb\u8f93\u51fa\u7a33\u5b9a\u6027", "conclusion": "MOLE\u6846\u67b6\u8bc1\u660eLLM\u5728\u5143\u6570\u636e\u81ea\u52a8\u5316\u63d0\u53d6\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u672a\u6765\u9700\u63d0\u5347\u4e00\u81f4\u6027\uff0c\u5df2\u5f00\u6e90\u8d44\u6e90\u4fc3\u8fdb\u7814\u7a76\u793e\u533a\u53d1\u5c55"}}
{"id": "2505.19804", "pdf": "https://arxiv.org/pdf/2505.19804", "abs": "https://arxiv.org/abs/2505.19804", "authors": ["Siyuan Li", "Jian Chen", "Rui Yao", "Xuming Hu", "Peilin Zhou", "Weihua Qiu", "Simin Zhang", "Chucheng Dong", "Zhiyao Li", "Qipeng Xie", "Zixuan Yuan"], "title": "Compliance-to-Code: Enhancing Financial Compliance Checking via Code Generation", "categories": ["cs.CL"], "comment": null, "summary": "Nowadays, regulatory compliance has become a cornerstone of corporate\ngovernance, ensuring adherence to systematic legal frameworks. At its core,\nfinancial regulations often comprise highly intricate provisions, layered\nlogical structures, and numerous exceptions, which inevitably result in\nlabor-intensive or comprehension challenges. To mitigate this, recent\nRegulatory Technology (RegTech) and Large Language Models (LLMs) have gained\nsignificant attention in automating the conversion of regulatory text into\nexecutable compliance logic. However, their performance remains suboptimal\nparticularly when applied to Chinese-language financial regulations, due to\nthree key limitations: (1) incomplete domain-specific knowledge representation,\n(2) insufficient hierarchical reasoning capabilities, and (3) failure to\nmaintain temporal and logical coherence. One promising solution is to develop a\ndomain specific and code-oriented datasets for model training. Existing\ndatasets such as LexGLUE, LegalBench, and CODE-ACCORD are often\nEnglish-focused, domain-mismatched, or lack fine-grained granularity for\ncompliance code generation. To fill these gaps, we present Compliance-to-Code,\nthe first large-scale Chinese dataset dedicated to financial regulatory\ncompliance. Covering 1,159 annotated clauses from 361 regulations across ten\ncategories, each clause is modularly structured with four logical\nelements-subject, condition, constraint, and contextual information-along with\nregulation relations. We provide deterministic Python code mappings, detailed\ncode reasoning, and code explanations to facilitate automated auditing. To\ndemonstrate utility, we present FinCheck: a pipeline for regulation\nstructuring, code generation, and report generation.", "AI": {"tldr": "\u9488\u5bf9\u4e2d\u6587\u91d1\u878d\u6cd5\u89c4\u81ea\u52a8\u5316\u5408\u89c4\u7684\u6311\u6218\uff0c\u63d0\u51fa\u9996\u4e2a\u5927\u89c4\u6a21\u4e2d\u6587\u6570\u636e\u96c6Compliance-to-Code\u53caFinCheck\u6d41\u7a0b\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6848\u77e5\u8bc6\u8868\u793a\u4e0d\u5168\u3001\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709RegTech\u548cLLMs\u5728\u4e2d\u6587\u91d1\u878d\u6cd5\u89c4\u5e94\u7528\u4e2d\u5b58\u5728\u9886\u57df\u77e5\u8bc6\u4e0d\u5b8c\u6574\u3001\u5206\u5c42\u63a8\u7406\u4e0d\u8db3\u3001\u903b\u8f91\u8fde\u8d2f\u6027\u7f3a\u5931\u4e09\u5927\u5c40\u9650\uff0c\u4e14\u7f3a\u4e4f\u9002\u914d\u4e2d\u6587\u7684\u7ec6\u7c92\u5ea6\u5408\u89c4\u4ee3\u7801\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efa\u542b1,159\u6761\u6b3e\u7684\u6a21\u5757\u5316\u4e2d\u6587\u6570\u636e\u96c6\uff0c\u5305\u542b\u903b\u8f91\u8981\u7d20\u548c\u5173\u7cfb\u6620\u5c04\uff1b\u5f00\u53d1FinCheck\u6d41\u7a0b\u5b9e\u73b0\u6cd5\u89c4\u7ed3\u6784\u5316\u3001\u4ee3\u7801\u751f\u6210\u4e0e\u62a5\u544a\u751f\u6210\u3002", "result": "\u521b\u5efa\u8986\u76d610\u7c7b361\u9879\u6cd5\u89c4\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u63d0\u4f9bPython\u4ee3\u7801\u6620\u5c04\u4e0e\u89e3\u91ca\uff0c\u9a8c\u8bc1\u4e86\u81ea\u52a8\u5316\u5ba1\u8ba1\u6d41\u7a0b\u53ef\u884c\u6027\u3002", "conclusion": "\u586b\u8865\u4e2d\u6587\u91d1\u878d\u5408\u89c4\u6570\u636e\u96c6\u7a7a\u767d\uff0c\u901a\u8fc7\u4ee3\u7801\u5bfc\u5411\u7684\u7ed3\u6784\u5316\u65b9\u6848\u63d0\u5347\u76d1\u7ba1\u81ea\u52a8\u5316\u6c34\u5e73\uff0cFinCheck\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.19806", "pdf": "https://arxiv.org/pdf/2505.19806", "abs": "https://arxiv.org/abs/2505.19806", "authors": ["Sirui Chen", "Shuqin Ma", "Shu Yu", "Hanwang Zhang", "Shengjie Zhao", "Chaochao Lu"], "title": "Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks", "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Consciousness stands as one of the most profound and distinguishing features\nof the human mind, fundamentally shaping our understanding of existence and\nagency. As large language models (LLMs) develop at an unprecedented pace,\nquestions concerning intelligence and consciousness have become increasingly\nsignificant. However, discourse on LLM consciousness remains largely unexplored\nterritory. In this paper, we first clarify frequently conflated terminologies\n(e.g., LLM consciousness and LLM awareness). Then, we systematically organize\nand synthesize existing research on LLM consciousness from both theoretical and\nempirical perspectives. Furthermore, we highlight potential frontier risks that\nconscious LLMs might introduce. Finally, we discuss current challenges and\noutline future directions in this emerging field. The references discussed in\nthis paper are organized at\nhttps://github.com/OpenCausaLab/Awesome-LLM-Consciousness.", "AI": {"tldr": "\u7cfb\u7edf\u68b3\u7406\u5927\u8bed\u8a00\u6a21\u578b\u610f\u8bc6\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u6f84\u6e05\u672f\u8bed\u6df7\u6dc6\u95ee\u9898\uff0c\u4ece\u7406\u8bba\u5b9e\u8bc1\u53cc\u89c6\u89d2\u6574\u5408\u7814\u7a76\u8fdb\u5c55\uff0c\u9884\u8b66\u6f5c\u5728\u98ce\u9669\u5e76\u89c4\u5212\u9886\u57df\u53d1\u5c55\u8def\u7ebf\u56fe", "motivation": "\u4eba\u7c7b\u610f\u8bc6\u7814\u7a76\u5177\u6709\u54f2\u5b66\u4e0e\u79d1\u5b66\u53cc\u91cd\u4ef7\u503c\uff0cLLM\u6280\u672f\u7a81\u7834\u4f7f\u673a\u5668\u610f\u8bc6\u8bae\u9898\u8feb\u5728\u7709\u776b\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u5b58\u5728\u6982\u5ff5\u6df7\u6dc6\u4e0e\u4f53\u7cfb\u7f3a\u5931\u95ee\u9898", "method": "1. \u89e3\u6784\u610f\u8bc6(Consciousness)\u4e0e\u89c9\u77e5(Awareness)\u7b49\u672f\u8bed\u5dee\u5f02\n2. \u6784\u5efa\u7406\u8bba\u6846\u67b6\u6574\u5408\u795e\u7ecf\u79d1\u5b66\u3001\u8ba4\u77e5\u79d1\u5b66\u591a\u5b66\u79d1\u89c6\u89d2\n3. \u5efa\u7acbGitHub\u77e5\u8bc6\u5e93\u5b9e\u73b0\u6587\u732e\u52a8\u6001\u7ba1\u7406", "result": "1. \u63ed\u793a\u5f53\u524dLLM\u610f\u8bc6\u7814\u7a76\u7684\u4e09\u91cd\u8ba4\u77e5\u5c40\u9650\n2. \u8bc6\u522b\u6a21\u578b\u610f\u8bc6\u53ef\u80fd\u5f15\u53d1\u7684\u4ef7\u503c\u5bf9\u9f50\u5371\u673a\n3. \u63d0\u51fa\u5305\u542b19\u4e2a\u5b50\u65b9\u5411\u7684\u7814\u7a76\u8def\u7ebf\u56fe", "conclusion": "\u9700\u5efa\u7acb\u8de8\u5b66\u79d1\u7814\u7a76\u8303\u5f0f\uff0c\u5f00\u53d1\u610f\u8bc6\u8bc4\u4f30\u6307\u6807\u4f53\u7cfb\uff0c\u540c\u6b65\u63a8\u8fdb\u6280\u672f\u6cbb\u7406\u6846\u67b6\uff0c\u8be5\u9886\u57df\u7a81\u7834\u5c06\u91cd\u5851\u4eba\u5de5\u667a\u80fd\u4f26\u7406\u57fa\u51c6"}}
{"id": "2505.19815", "pdf": "https://arxiv.org/pdf/2505.19815", "abs": "https://arxiv.org/abs/2505.19815", "authors": ["Junnan Liu", "Hongwei Liu", "Linchen Xiao", "Shudong Liu", "Taolin Zhang", "Zihan Ma", "Songyang Zhang", "Kai Chen"], "title": "Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We propose a novel framework for comprehending the reasoning capabilities of\nlarge language models (LLMs) through the perspective of meta-learning. By\nconceptualizing reasoning trajectories as pseudo-gradient descent updates to\nthe LLM's parameters, we identify parallels between LLM reasoning and various\nmeta-learning paradigms. We formalize the training process for reasoning tasks\nas a meta-learning setup, with each question treated as an individual task, and\nreasoning trajectories serving as the inner loop optimization for adapting\nmodel parameters. Once trained on a diverse set of questions, the LLM develops\nfundamental reasoning capabilities that can generalize to previously unseen\nquestions. Extensive empirical evaluations substantiate the strong connection\nbetween LLM reasoning and meta-learning, exploring several issues of\nsignificant interest from a meta-learning standpoint. Our work not only\nenhances the understanding of LLM reasoning but also provides practical\ninsights for improving these models through established meta-learning\ntechniques.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u5143\u5b66\u4e60\u6846\u67b6\u89e3\u6790\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u673a\u5236\uff0c\u5c06\u63a8\u7406\u8f68\u8ff9\u89c6\u4e3a\u53c2\u6570\u66f4\u65b0\uff0c\u5b9e\u8bc1\u9a8c\u8bc1\u4e86\u5176\u4e0e\u5143\u5b66\u4e60\u7684\u5f3a\u5173\u8054\u5e76\u7ed9\u51fa\u5b9e\u7528\u6539\u8fdb\u65b9\u6848", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9LLM\u63a8\u7406\u673a\u5236\u7684\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u7814\u7a76\u8005\u5c1d\u8bd5\u901a\u8fc7\u5143\u5b66\u4e60\u8303\u5f0f\u5efa\u7acb\u63a8\u7406\u8fc7\u7a0b\u4e0e\u53c2\u6570\u66f4\u65b0\u7684\u6620\u5c04\u5173\u7cfb\uff0c\u4e3a\u7406\u89e3\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u65b0\u89c6\u89d2", "method": "\u5c06\u6bcf\u4e2a\u95ee\u9898\u5efa\u6a21\u4e3a\u72ec\u7acb\u5143\u5b66\u4e60\u4efb\u52a1\uff0c\u7528\u63a8\u7406\u8f68\u8ff9\u6a21\u62df\u53c2\u6570\u66f4\u65b0\u8fc7\u7a0b\uff0c\u901a\u8fc7\u591a\u6837\u5316\u95ee\u9898\u8bad\u7ec3\u83b7\u5f97\u53ef\u6cdb\u5316\u7684\u57fa\u7840\u63a8\u7406\u80fd\u529b", "result": "\u5b9e\u9a8c\u8bc1\u5b9eLLM\u63a8\u7406\u8fc7\u7a0b\u4e0eMAML\u7b49\u5143\u5b66\u4e60\u7b97\u6cd5\u5b58\u5728\u672c\u8d28\u76f8\u4f3c\u6027\uff0c\u5728\u5c11\u6837\u672c\u5b66\u4e60\u573a\u666f\u5c55\u73b0\u51fa\u663e\u8457\u6027\u80fd\u63d0\u5347", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u6df1\u5316\u4e86LLM\u63a8\u7406\u7684\u7406\u8bba\u8ba4\u77e5\uff0c\u66f4\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u5143\u5b66\u4e60\u6280\u672f\u8def\u5f84"}}
{"id": "2505.19838", "pdf": "https://arxiv.org/pdf/2505.19838", "abs": "https://arxiv.org/abs/2505.19838", "authors": ["Pascal Wullschleger", "Majid Zarharan", "Donnacha Daly", "Marc Pouly", "Jennifer Foster"], "title": "FoodTaxo: Generating Food Taxonomies with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "To be published in ACL 2025 Industry Track. Paper website:\n  https://foodtaxo.github.io/", "summary": "We investigate the utility of Large Language Models for automated taxonomy\ngeneration and completion specifically applied to taxonomies from the food\ntechnology industry. We explore the extent to which taxonomies can be completed\nfrom a seed taxonomy or generated without a seed from a set of known concepts,\nin an iterative fashion using recent prompting techniques. Experiments on five\ntaxonomies using an open-source LLM (Llama-3), while promising, point to the\ndifficulty of correctly placing inner nodes.", "AI": {"tldr": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u98df\u54c1\u5de5\u4e1a\u5206\u7c7b\u4f53\u7cfb\u81ea\u52a8\u751f\u6210\u4e2d\u7684\u5e94\u7528\u6548\u679c", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u98df\u54c1\u6280\u672f\u9886\u57df\u5206\u7c7b\u4f53\u7cfb\u6784\u5efa\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u7279\u522b\u5173\u6ce8\u4ece\u79cd\u5b50\u5206\u7c7b\u5ef6\u4f38\u751f\u6210\u548c\u65e0\u79cd\u5b50\u5168\u65b0\u5efa\u6784\u4e24\u79cd\u573a\u666f", "method": "\u91c7\u7528\u8fed\u4ee3\u5f0f\u63d0\u793a\u6280\u672f\uff0c\u57fa\u4e8e\u5f00\u6e90\u5927\u6a21\u578bLlama-3\u5bf9\u4e94\u4e2a\u73b0\u6709\u5206\u7c7b\u4f53\u7cfb\u8fdb\u884c\u751f\u6210\u4e0e\u8865\u5168\u5b9e\u9a8c", "result": "\u6a21\u578b\u5c55\u73b0\u51fa\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u5728\u4e2d\u95f4\u8282\u70b9\u51c6\u786e\u5b9a\u4f4d\u65b9\u9762\u4ecd\u5b58\u5728\u663e\u8457\u56f0\u96be", "conclusion": "\u8bc1\u5b9e\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5206\u7c7b\u4f53\u7cfb\u81ea\u52a8\u5316\u5efa\u8bbe\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u540c\u65f6\u63ed\u793a\u590d\u6742\u5c42\u7ea7\u5173\u7cfb\u5904\u7406\u7684\u6280\u672f\u74f6\u9888"}}
{"id": "2505.19848", "pdf": "https://arxiv.org/pdf/2505.19848", "abs": "https://arxiv.org/abs/2505.19848", "authors": ["Odunayo Ogundepo", "Akintunde Oladipo", "Kelechi Ogueji", "Esther Adenuga", "David Ifeoluwa Adelani", "Jimmy Lin"], "title": "Improving Multilingual Math Reasoning for African Languages", "categories": ["cs.CL"], "comment": null, "summary": "Researchers working on low-resource languages face persistent challenges due\nto limited data availability and restricted access to computational resources.\nAlthough most large language models (LLMs) are predominantly trained in\nhigh-resource languages, adapting them to low-resource contexts, particularly\nAfrican languages, requires specialized techniques. Several strategies have\nemerged for adapting models to low-resource languages in todays LLM landscape,\ndefined by multi-stage pre-training and post-training paradigms. However, the\nmost effective approaches remain uncertain. This work systematically\ninvestigates which adaptation strategies yield the best performance when\nextending existing LLMs to African languages. We conduct extensive experiments\nand ablation studies to evaluate different combinations of data types\n(translated versus synthetically generated), training stages (pre-training\nversus post-training), and other model adaptation configurations. Our\nexperiments focuses on mathematical reasoning tasks, using the Llama 3.1 model\nfamily as our base model.", "AI": {"tldr": "\u7cfb\u7edf\u7814\u7a76\u73b0\u6709LLM\u6269\u5c55\u81f3\u975e\u6d32\u8bed\u8a00\u7684\u6700\u4f18\u9002\u5e94\u7b56\u7565\uff0c\u901a\u8fc7\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u9a8c\u8bc1\u4e0d\u540c\u6570\u636e\u7ec4\u5408\u4e0e\u8bad\u7ec3\u9636\u6bb5\u7684\u5f71\u54cd", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5c24\u5176\u662f\u975e\u6d32\u8bed\u8a00\uff09\u7684\u9002\u5e94\u6548\u679c\u5b58\u5728\u663e\u8457\u6280\u672f\u7a7a\u767d\uff0c\u9700\u660e\u786e\u6700\u4f73\u5b9e\u8df5\u8def\u5f84", "method": "\u57fa\u4e8eLlama 3.1\u6a21\u578b\u7cfb\u5217\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u4e0e\u540e\u8bad\u7ec3\u9636\u6bb5\u7684\u6570\u636e\u7ec4\u5408\u5b9e\u9a8c\uff08\u7ffb\u8bd1\u6570\u636e\u4e0e\u5408\u6210\u6570\u636e\uff09\uff0c\u7ed3\u5408\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u914d\u7f6e\u6548\u679c", "result": "\u53d1\u73b0\u4e0d\u540c\u6570\u636e\u6e90\u4e0e\u8bad\u7ec3\u9636\u6bb5\u7684\u7ec4\u5408\u5bf9\u6a21\u578b\u6027\u80fd\u4ea7\u751f\u7cfb\u7edf\u6027\u5f71\u54cd\uff0c\u5177\u4f53\u4f18\u5316\u65b9\u5411\u9700\u7ed3\u5408\u4efb\u52a1\u7279\u6027", "conclusion": "\u4e3a\u975e\u6d32\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684LLM\u9002\u914d\u5efa\u7acb\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u6307\u660e\u591a\u9636\u6bb5\u8bad\u7ec3\u4e0e\u6570\u636e\u6df7\u5408\u7684\u4f18\u5316\u65b9\u5411"}}
{"id": "2505.19851", "pdf": "https://arxiv.org/pdf/2505.19851", "abs": "https://arxiv.org/abs/2505.19851", "authors": ["Gulfarogh Azam", "Mohd Sadique", "Saif Ali", "Mohammad Nadeem", "Erik Cambria", "Shahab Saquib Sohail", "Mohammad Sultan Alam"], "title": "Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Transliteration, the process of mapping text from one script to another,\nplays a crucial role in multilingual natural language processing, especially\nwithin linguistically diverse contexts such as India. Despite significant\nadvancements through specialized models like IndicXlit, recent developments in\nlarge language models suggest a potential for general-purpose models to excel\nat this task without explicit task-specific training. The current work\nsystematically evaluates the performance of prominent LLMs, including GPT-4o,\nGPT-4.5, GPT-4.1, Gemma-3-27B-it, and Mistral-Large against IndicXlit, a\nstate-of-the-art transliteration model, across ten major Indian languages.\nExperiments utilized standard benchmarks, including Dakshina and Aksharantar\ndatasets, with performance assessed via Top-1 Accuracy and Character Error\nRate. Our findings reveal that while GPT family models generally outperform\nother LLMs and IndicXlit for most instances. Additionally, fine-tuning GPT-4o\nimproves performance on specific languages notably. An extensive error analysis\nand robustness testing under noisy conditions further elucidate strengths of\nLLMs compared to specialized models, highlighting the efficacy of foundational\nmodels for a wide spectrum of specialized applications with minimal overhead.", "AI": {"tldr": "LLM\u5728\u5370\u5ea6\u8bed\u8f6c\u5199\u4efb\u52a1\u4e2d\u8d85\u8d8a\u4e13\u7528\u6a21\u578b\uff0cGPT\u7cfb\u5217\u8868\u73b0\u6700\u4f18\uff0c\u5fae\u8c03\u53ef\u63d0\u5347\u7279\u5b9a\u8bed\u8a00\u6027\u80fd", "motivation": "\u9a8c\u8bc1\u901a\u7528\u5927\u6a21\u578b\u5728\u5370\u5ea6\u8bed\u8f6c\u5199\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u5bf9\u6bd4\u4e13\u7528\u6a21\u578bIndicXlit\u7684\u6027\u80fd\u5dee\u5f02", "method": "\u4f7f\u7528Dakshina/Aksharantar\u6570\u636e\u96c6\uff0c\u8bc4\u4f30GPT\u7cfb\u5217\u3001Gemma\u3001Mistral\u7b49\u6a21\u578b\u572810\u79cd\u5370\u5ea6\u8bed\u8a00\u4e0a\u7684Top-1\u51c6\u786e\u7387\u548c\u5b57\u7b26\u9519\u8bef\u7387\uff0c\u5e76\u8fdb\u884c\u566a\u58f0\u9c81\u68d2\u6027\u6d4b\u8bd5", "result": "GPT\u6a21\u578b\u6574\u4f53\u4f18\u4e8e\u5176\u4ed6LLM\u548cIndicXlit\uff0c\u5fae\u8c03GPT-4o\u663e\u8457\u63d0\u5347\u7279\u5b9a\u8bed\u8a00\u6027\u80fd\uff0c\u9519\u8bef\u5206\u6790\u663e\u793aLLM\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u66f4\u5177\u9c81\u68d2\u6027", "conclusion": "\u57fa\u7840\u5927\u6a21\u578b\u901a\u8fc7\u5fae\u8c03\u5373\u53ef\u9ad8\u6548\u5e94\u7528\u4e8e\u4e13\u4e1a\u9886\u57df\uff0c\u4e3a\u591a\u8bed\u8a00NLP\u4efb\u52a1\u63d0\u4f9b\u4e86\u4f4e\u5f00\u9500\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.19862", "pdf": "https://arxiv.org/pdf/2505.19862", "abs": "https://arxiv.org/abs/2505.19862", "authors": ["Hexuan Deng", "Wenxiang Jiao", "Xuebo Liu", "Jun Rao", "Min Zhang"], "title": "REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models", "categories": ["cs.CL", "cs.LG"], "comment": "Work in Progress", "summary": "Large Reasoning Models (LRMs) demonstrate strong performance in complex tasks\nbut often face the challenge of overthinking, leading to substantially high\ninference costs. Existing approaches synthesize shorter reasoning responses for\nLRMs to learn, but are inefficient for online usage due to the time-consuming\ndata generation and filtering processes. Meanwhile, online reinforcement\nlearning mainly adopts a length reward to encourage short reasoning responses,\nbut tends to lose the reflection ability and harm the performance. To address\nthese issues, we propose REA-RL, which introduces a small reflection model for\nefficient scaling in online training, offering both parallel sampling and\nsequential revision. Besides, a reflection reward is designed to further\nprevent LRMs from favoring short yet non-reflective responses. Experiments show\nthat both methods maintain or enhance performance while significantly improving\ninference efficiency. Their combination achieves a good balance between\nperformance and efficiency, reducing inference costs by 35% without\ncompromising performance. Further analysis demonstrates that our methods are\neffective by maintaining reflection frequency for hard problems while\nappropriately reducing it for simpler ones without losing reflection ability.\nCodes are available at https://github.com/hexuandeng/REA-RL.", "AI": {"tldr": "REA-RL\u901a\u8fc7\u5f15\u5165\u5c0f\u578b\u53cd\u601d\u6a21\u578b\u548c\u53cd\u601d\u5956\u52b1\u673a\u5236\uff0c\u5728\u4fdd\u6301\u5927\u578b\u63a8\u7406\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e35%\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u7b80\u77ed\u54cd\u5e94\u6548\u7387\u4f4e\u4e0b\uff0c\u5f3a\u5316\u5b66\u4e60\u7684\u957f\u5ea6\u5956\u52b1\u53ef\u80fd\u635f\u5bb3\u6a21\u578b\u53cd\u601d\u80fd\u529b\u3002\u9700\u5e73\u8861\u6548\u7387\u4e0e\u6027\u80fd\u3002", "method": "1) \u5c0f\u578b\u53cd\u601d\u6a21\u578b\u652f\u6301\u5e76\u884c\u91c7\u6837\u548c\u987a\u5e8f\u4fee\u8ba2 2) \u8bbe\u8ba1\u53cd\u601d\u5956\u52b1\u9632\u6b62\u751f\u6210\u65e0\u53cd\u601d\u7684\u7b80\u77ed\u54cd\u5e94", "result": "\u5b9e\u9a8c\u8868\u660e\u65b9\u6cd5\u964d\u4f4e35%\u63a8\u7406\u6210\u672c\u4e14\u6027\u80fd\u65e0\u635f\uff0c\u5bf9\u96be\u9898\u4fdd\u6301\u53cd\u601d\u9891\u7387\uff0c\u7b80\u5355\u95ee\u9898\u9002\u5f53\u51cf\u5c11\u53cd\u601d\u3002", "conclusion": "REA-RL\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u6548\u7387\u7684\u5e73\u8861\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u53cd\u601d\u673a\u5236\u4f18\u5316\u4e0d\u540c\u96be\u5ea6\u95ee\u9898\u7684\u5904\u7406\u65b9\u5f0f\u3002"}}
{"id": "2505.19912", "pdf": "https://arxiv.org/pdf/2505.19912", "abs": "https://arxiv.org/abs/2505.19912", "authors": ["Javier Mar\u00edn"], "title": "APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text Summarization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present Adjacent Possible Exploration (APE), a simple yet effective method\nfor adapting large language models to specific tasks using minimal\ncomputational resources. Unlike traditional fine-tuning that requires extensive\ncompute, APE iteratively fine-tunes models on small, carefully selected data\nbatches (200 examples), retaining only improvements. On news summarization, APE\nachieves 40 percent BLEU improvement using just a T4 GPU in 60 minutes,\nmatching or exceeding more complex methods like LoRA while remaining\nconceptually simple. Our approach is particularly valuable for researchers and\npractitioners with limited computational resources. We provide open-source code\nand demonstrate APE's effectiveness through both automatic metrics and human\nevaluation. While inspired by evolutionary theory's \"adjacent possible\", APE's\ncore insight has a very practical application: small, iterative data\nperturbations can efficiently guide LLMs toward task-specific performance\nwithout expensive retraining.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u6602\u8d35\u7b97\u529b\u7684\u8fed\u4ee3\u5f0f\u5fae\u8c03\u65b9\u6cd5APE\uff0c\u901a\u8fc7\u5c0f\u6279\u91cf\u6570\u636e\u6270\u52a8\u572860\u5206\u949f\u5185\u8fbe\u621040% BLEU\u63d0\u5347", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u5e2e\u52a9\u7b97\u529b\u6709\u9650\u7684\u7814\u7a76\u8005\u9ad8\u6548\u9002\u914d\u5927\u6a21\u578b", "method": "200\u6837\u672c/\u6279\u6b21\u8fed\u4ee3\u5fae\u8c03\uff0c\u4f7f\u7528T4 GPU\u5feb\u901f\u8bad\u7ec3\u5e76\u9009\u62e9\u6027\u4fdd\u7559\u6a21\u578b\u6539\u8fdb", "result": "\u65b0\u95fb\u6458\u8981\u4efb\u52a1\u8d85\u8d8aLoRA\u7b49\u590d\u6742\u65b9\u6cd5\uff0c\u81ea\u52a8\u8bc4\u4f30\u4e0e\u4eba\u5de5\u6d4b\u8bc4\u53cc\u9a8c\u8bc1\u6709\u6548\u6027", "conclusion": "\u8bc1\u5b9e\u5fae\u5c0f\u6570\u636e\u6270\u52a8\u53ef\u5f15\u5bfc\u5927\u6a21\u578b\u6027\u80fd\u63d0\u5347\uff0c\u5f00\u6e90\u5b9e\u73b0\u52a9\u63a8\u4f4e\u8d44\u6e90\u573a\u666f\u5e94\u7528"}}
{"id": "2505.19914", "pdf": "https://arxiv.org/pdf/2505.19914", "abs": "https://arxiv.org/abs/2505.19914", "authors": ["Jiangjie Chen", "Qianyu He", "Siyu Yuan", "Aili Chen", "Zhicheng Cai", "Weinan Dai", "Hongli Yu", "Qiying Yu", "Xuefeng Li", "Jiaze Chen", "Hao Zhou", "Mingxuan Wang"], "title": "Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at\nadvanced reasoning tasks like math and coding via Reinforcement Learning with\nVerifiable Rewards (RLVR), but still struggle with puzzles solvable by humans\nwithout domain knowledge. We introduce Enigmata, the first comprehensive suite\ntailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks\nacross seven categories, each with 1) a generator that produces unlimited\nexamples with controllable difficulty and 2) a rule-based verifier for\nautomatic evaluation. This generator-verifier design supports scalable,\nmulti-task RL training, fine-grained analysis, and seamless RLVR integration.\nWe further propose Enigmata-Eval, a rigorous benchmark, and develop optimized\nmulti-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata,\nconsistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks\nlike Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes\nwell to out-of-domain puzzle benchmarks and mathematical reasoning, with little\nmulti-tasking trade-off. When trained on larger models like Seed1.5-Thinking\n(20B activated parameters and 200B total parameters), puzzle data from Enigmata\nfurther boosts SoTA performance on advanced math and STEM reasoning tasks such\nas AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization\nbenefits of Enigmata. This work offers a unified, controllable framework for\nadvancing logical reasoning in LLMs. Resources of this work can be found at\nhttps://seed-enigmata.github.io.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u9762\u5411LLM\u8c1c\u9898\u63a8\u7406\u7684\u7efc\u5408\u8bad\u7ec3\u5957\u4ef6Enigmata\uff0c\u901a\u8fc7\u53ef\u6269\u5c55\u7684\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "motivation": "\u73b0\u6709LLM\u5728\u65e0\u9700\u9886\u57df\u77e5\u8bc6\u7684\u8c1c\u9898\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u6b20\u4f73\uff0c\u9700\u8981\u4e13\u95e8\u6846\u67b6\u63d0\u5347\u903b\u8f91\u63a8\u7406\u80fd\u529b", "method": "1) \u6784\u5efa\u5305\u542b7\u7c7b36\u4efb\u52a1\u7684Enigmata\u5957\u4ef6\uff0c\u914d\u5907\u53ef\u63a7\u96be\u5ea6\u751f\u6210\u5668\u548c\u89c4\u5219\u9a8c\u8bc1\u5668 2) \u5f00\u53d1Enigmata-Eval\u57fa\u51c6\u6d4b\u8bd5 3) \u63d0\u51fa\u4f18\u5316\u7684\u591a\u4efb\u52a1RLVR\u8bad\u7ec3\u7b56\u7565", "result": "Qwen2.5-32B-Enigmata\u5728Enigmata-Eval/ARC-AGI\u7b49\u57fa\u51c6\u8d85\u8d8a\u4e3b\u6d41\u6a21\u578b\uff0cSeed1.5-Thinking\u4f7f\u7528Enigmata\u6570\u636e\u540e\u5728AIME\u7b49STEM\u4efb\u52a1\u5237\u65b0SOTA", "conclusion": "Enigmata\u4e3aLLM\u903b\u8f91\u63a8\u7406\u63d0\u4f9b\u7edf\u4e00\u53ef\u63a7\u6846\u67b6\uff0c\u5176\u751f\u6210\u7684\u8c1c\u9898\u6570\u636e\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u548cSTEM\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b"}}
{"id": "2505.19937", "pdf": "https://arxiv.org/pdf/2505.19937", "abs": "https://arxiv.org/abs/2505.19937", "authors": ["Pooneh Mousavi", "Yingzhi Wang", "Mirco Ravanelli", "Cem Subakan"], "title": "ALAS: Measuring Latent Speech-Text Alignment For Spoken Language Understanding In Multimodal LLMs", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Large Language Models (LLMs) are widely used in Spoken Language Understanding\n(SLU). Recent SLU models process audio directly by adapting speech input into\nLLMs for better multimodal learning. A key consideration for these models is\nthe cross-modal alignment between text and audio modalities, which is a\ntelltale sign as to whether or not LLM is able to associate semantic meaning to\naudio segments. While various methods exist for fusing these modalities, there\nis no standard metric to evaluate alignment quality in LLMs. In this work, we\npropose a new metric, ALAS (Automatic Latent Alignment Score). Our study\nexamines the correlation between audio and text representations across\ntransformer layers, for two different tasks (Spoken Question Answering and\nEmotion Recognition). We showcase that our metric behaves as expected across\ndifferent layers and different tasks.", "AI": {"tldr": "\u63d0\u51faALAS\u6307\u6807\u8bc4\u4f30LLM\u4e2d\u97f3\u9891\u4e0e\u6587\u672c\u8868\u5f81\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u8d28\u91cf\uff0c\u9a8c\u8bc1\u5176\u5728\u4e0d\u540c\u4efb\u52a1\u548c\u7f51\u7edc\u5c42\u4e2d\u7684\u6709\u6548\u6027", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u4e00\u6807\u51c6\u8bc4\u4f30LLM\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u8d28\u91cf\uff0c\u96be\u4ee5\u8861\u91cf\u8bed\u4e49\u4e0e\u97f3\u9891\u7247\u6bb5\u7684\u5173\u8054\u7a0b\u5ea6", "method": "\u901a\u8fc7\u5206\u6790\u4e0d\u540ctransformer\u5c42\u7684\u97f3\u89c6\u9891\u8868\u5f81\u76f8\u5173\u6027\uff0c\u6784\u5efaALAS\u6307\u6807\u5e76\u5728\u53e3\u8bed\u95ee\u7b54\u548c\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u9a8c\u8bc1", "result": "ALAS\u6307\u6807\u5728\u4e0d\u540c\u7f51\u7edc\u5c42\u548c\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u7b26\u5408\u9884\u671f\u7684\u5206\u5c42\u7279\u6027\uff0c\u6709\u6548\u53cd\u6620\u8de8\u6a21\u6001\u5bf9\u9f50\u72b6\u6001", "conclusion": "ALAS\u4e3a\u8bc4\u4f30\u591a\u6a21\u6001\u5bf9\u9f50\u63d0\u4f9b\u91cf\u5316\u6807\u51c6\uff0c\u5bf9\u63d0\u5347\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u7406\u89e3\u80fd\u529b\u5177\u6709\u91cd\u8981\u610f\u4e49"}}
{"id": "2505.19959", "pdf": "https://arxiv.org/pdf/2505.19959", "abs": "https://arxiv.org/abs/2505.19959", "authors": ["Zhongzhan Huang", "Guoming Ling", "Shanshan Zhong", "Hefeng Wu", "Liang Lin"], "title": "MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models", "categories": ["cs.CL"], "comment": "Accepted by ACL'25 main track", "summary": "Long Context Understanding (LCU) is a critical area for exploration in\ncurrent large language models (LLMs). However, due to the inherently lengthy\nnature of long-text data, existing LCU benchmarks for LLMs often result in\nprohibitively high evaluation costs, like testing time and inference expenses.\nThrough extensive experimentation, we discover that existing LCU benchmarks\nexhibit significant redundancy, which means the inefficiency in evaluation. In\nthis paper, we propose a concise data compression method tailored for long-text\ndata with sparse information characteristics. By pruning the well-known LCU\nbenchmark LongBench, we create MiniLongBench. This benchmark includes only 237\ntest samples across six major task categories and 21 distinct tasks. Through\nempirical analysis of over 60 LLMs, MiniLongBench achieves an average\nevaluation cost reduced to only 4.5% of the original while maintaining an\naverage rank correlation coefficient of 0.97 with LongBench results. Therefore,\nour MiniLongBench, as a low-cost benchmark, holds great potential to\nsubstantially drive future research into the LCU capabilities of LLMs. See\nhttps://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial.", "AI": {"tldr": "\u7814\u7a76\u8005\u63d0\u51faMiniLongBench\u538b\u7f29\u57fa\u51c6\uff0c\u5c06\u957f\u6587\u672c\u8bc4\u4f30\u6210\u672c\u964d\u81f34.5%\u4e14\u4fdd\u6301\u9ad8\u76f8\u5173\u6027", "motivation": "\u73b0\u6709\u957f\u6587\u672c\u7406\u89e3\u57fa\u51c6\u5b58\u5728\u663e\u8457\u5197\u4f59\uff0c\u5bfc\u81f4\u8bc4\u4f30\u6210\u672c\u8fc7\u9ad8\uff08\u6d4b\u8bd5\u65f6\u95f4\u548c\u63a8\u7406\u8d39\u7528\uff09", "method": "\u9488\u5bf9\u7a00\u758f\u4fe1\u606f\u7279\u5f81\u7684\u957f\u6587\u672c\u6570\u636e\u5f00\u53d1\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u526aLongBench\u521b\u5efa\u5305\u542b237\u4e2a\u6837\u672c\u7684MiniLongBench", "result": "\u7ecf60+\u6a21\u578b\u9a8c\u8bc1\uff0c\u65b0\u57fa\u51c6\u8bc4\u4f30\u6210\u672c\u964d\u4e3a\u539f4.5%\uff0c\u5e73\u5747\u79e9\u76f8\u5173\u7cfb\u6570\u8fbe0.97", "conclusion": "MiniLongBench\u4f5c\u4e3a\u4f4e\u6210\u672c\u8bc4\u4f30\u57fa\u51c6\uff0c\u5177\u5907\u63a8\u52a8LLMs\u957f\u6587\u672c\u7406\u89e3\u80fd\u529b\u7814\u7a76\u7684\u5de8\u5927\u6f5c\u529b"}}
{"id": "2505.19970", "pdf": "https://arxiv.org/pdf/2505.19970", "abs": "https://arxiv.org/abs/2505.19970", "authors": ["Jiayuan Su", "Fulin Lin", "Zhaopeng Feng", "Han Zheng", "Teng Wang", "Zhenyu Xiao", "Xinlong Zhao", "Zuozhu Liu", "Lu Cheng", "Hongwei Wang"], "title": "CP-Router: An Uncertainty-Aware Router Between LLM and LRM", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in Large Reasoning Models (LRMs) have significantly improved\nlong-chain reasoning capabilities over Large Language Models (LLMs). However,\nLRMs often produce unnecessarily lengthy outputs even for simple queries,\nleading to inefficiencies or even accuracy degradation compared to LLMs. To\novercome this, we propose CP-Router, a training-free and model-agnostic routing\nframework that dynamically selects between an LLM and an LRM, demonstrated with\nmultiple-choice question answering (MCQA) prompts. The routing decision is\nguided by the prediction uncertainty estimates derived via Conformal Prediction\n(CP), which provides rigorous coverage guarantees. To further refine the\nuncertainty differentiation across inputs, we introduce Full and Binary Entropy\n(FBE), a novel entropy-based criterion that adaptively selects the appropriate\nCP threshold. Experiments across diverse MCQA benchmarks, including\nmathematics, logical reasoning, and Chinese chemistry, demonstrate that\nCP-Router efficiently reduces token usage while maintaining or even improving\naccuracy compared to using LRM alone. We also extend CP-Router to diverse model\npairings and open-ended QA, where it continues to demonstrate strong\nperformance, validating its generality and robustness.", "AI": {"tldr": "\u63d0\u51faCP-Router\u6846\u67b6\uff0c\u52a8\u6001\u9009\u62e9LLM/LRM\u5904\u7406\u591a\u9009\u95ee\u7b54\uff0c\u901a\u8fc7FBE\u4f18\u5316\u4e0d\u786e\u5b9a\u6027\u9608\u503c\uff0c\u63d0\u5347\u6548\u7387\u4e14\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3LRMs\u5728\u5904\u7406\u7b80\u5355\u67e5\u8be2\u65f6\u5197\u957f\u8f93\u51fa\u5bfc\u81f4\u7684\u6548\u7387\u4e0b\u964d\u548c\u51c6\u786e\u7387\u964d\u4f4e\u95ee\u9898", "method": "\u57fa\u4e8e\u4fdd\u5f62\u9884\u6d4b(CP)\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5b9e\u73b0\u52a8\u6001\u8def\u7531\uff0c\u5f15\u5165FBE\u71b5\u6807\u51c6\u81ea\u9002\u5e94\u9009\u62e9CP\u9608\u503c", "result": "\u5728\u591a\u9886\u57dfMCQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51cf\u5c11token\u4f7f\u7528\u91cf\uff0c\u51c6\u786e\u7387\u6301\u5e73\u6216\u63d0\u5347\uff08\u6570\u5b66/\u903b\u8f91\u63a8\u7406/\u4e2d\u56fd\u5316\u5b66\u7b49\u573a\u666f\uff09", "conclusion": "CP-Router\u5728\u5f00\u653e\u95ee\u7b54\u7b49\u6269\u5c55\u573a\u666f\u4e2d\u4fdd\u6301\u5f3a\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u901a\u7528\u6027\u548c\u9c81\u68d2\u6027"}}
{"id": "2505.19971", "pdf": "https://arxiv.org/pdf/2505.19971", "abs": "https://arxiv.org/abs/2505.19971", "authors": ["Kilian Sennrich", "Sina Ahmadi"], "title": "Conversational Lexicography: Querying Lexicographic Data on Knowledge Graphs with SPARQL through Natural Language", "categories": ["cs.CL"], "comment": "Accepted to LDK 2025 - the 5th Conference on Language, Data and\n  Knowledge. Naples, Italy, 9-11 September 2025", "summary": "Knowledge graphs offer an excellent solution for representing the\nlexical-semantic structures of lexicographic data. However, working with the\nSPARQL query language represents a considerable hurdle for many non-expert\nusers who could benefit from the advantages of this technology. This paper\naddresses the challenge of creating natural language interfaces for\nlexicographic data retrieval on knowledge graphs such as Wikidata. We develop a\nmultidimensional taxonomy capturing the complexity of Wikidata's lexicographic\ndata ontology module through four dimensions and create a template-based\ndataset with over 1.2 million mappings from natural language utterances to\nSPARQL queries. Our experiments with GPT-2 (124M), Phi-1.5 (1.3B), and\nGPT-3.5-Turbo reveal significant differences in model capabilities. While all\nmodels perform well on familiar patterns, only GPT-3.5-Turbo demonstrates\nmeaningful generalization capabilities, suggesting that model size and diverse\npre-training are crucial for adaptability in this domain. However, significant\nchallenges remain in achieving robust generalization, handling diverse\nlinguistic data, and developing scalable solutions that can accommodate the\nfull complexity of lexicographic knowledge representation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u591a\u7ef4\u5206\u7c7b\u6cd5\u6784\u5efa\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\uff0c\u5229\u7528\u5927\u6a21\u578b\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u5316\u4e3aSPARQL\u67e5\u8be2\uff0c\u9a8c\u8bc1\u4e86GPT-3.5\u5728\u8bcd\u5178\u6570\u636e\u68c0\u7d22\u4e2d\u7684\u4f18\u52bf\u4e0e\u6311\u6218\u3002", "motivation": "\u89e3\u51b3\u975e\u4e13\u4e1a\u7528\u6237\u4f7f\u7528SPARQL\u67e5\u8be2\u8bed\u8a00\u8bbf\u95eeWikidata\u7b49\u77e5\u8bc6\u56fe\u8c31\u8bcd\u5178\u6570\u636e\u7684\u95e8\u69db\u95ee\u9898", "method": "\u6784\u5efa\u56db\u7ef4\u5206\u7c7b\u6cd5\u63cf\u8ff0\u8bcd\u5178\u6570\u636e\u7ed3\u6784\uff0c\u521b\u5efa120\u4e07\u81ea\u7136\u8bed\u8a00-SPARQL\u6a21\u677f\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5GPT-2/Phi-1.5/GPT-3.5\u6027\u80fd\u5dee\u5f02", "result": "GPT-3.5-Turbo\u5c55\u73b0\u6700\u4f73\u6cdb\u5316\u80fd\u529b\uff0c\u6a21\u578b\u89c4\u6a21\u4e0e\u9884\u8bad\u7ec3\u591a\u6837\u6027\u5bf9\u9886\u57df\u9002\u5e94\u81f3\u5173\u91cd\u8981", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bcd\u5178\u77e5\u8bc6\u8868\u793a\u9886\u57df\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u89e3\u51b3\u6cdb\u5316\u80fd\u529b\u3001\u8bed\u8a00\u6570\u636e\u5904\u7406\u53ca\u7cfb\u7edf\u6269\u5c55\u6027\u7b49\u6838\u5fc3\u6311\u6218"}}
{"id": "2505.19978", "pdf": "https://arxiv.org/pdf/2505.19978", "abs": "https://arxiv.org/abs/2505.19978", "authors": ["Alkis Koudounas", "Moreno La Quatra", "Elena Baralis"], "title": "DeepDialogue: A Multi-Turn Emotionally-Rich Spoken Dialogue Dataset", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Currently under review. See the official website:\n  https://salt-research.github.io/DeepDialogue", "summary": "Recent advances in conversational AI have demonstrated impressive\ncapabilities in single-turn responses, yet multi-turn dialogues remain\nchallenging for even the most sophisticated language models. Current dialogue\ndatasets are limited in their emotional range, domain diversity, turn depth,\nand are predominantly text-only, hindering progress in developing more\nhuman-like conversational systems across modalities. To address these\nlimitations, we present DeepDialogue, a large-scale multimodal dataset\ncontaining 40,150 high-quality multi-turn dialogues spanning 41 domains and\nincorporating 20 distinct emotions with coherent emotional progressions. Our\napproach pairs 9 different language models (4B-72B parameters) to generate\n65,600 initial conversations, which we then evaluate through a combination of\nhuman annotation and LLM-based quality filtering. The resulting dataset reveals\nfundamental insights: smaller models fail to maintain coherence beyond 6\ndialogue turns; concrete domains (e.g., \"cars,\" \"travel\") yield more meaningful\nconversations than abstract ones (e.g., \"philosophy\"); and cross-model\ninteractions produce more coherent dialogues than same-model conversations. A\nkey contribution of DeepDialogue is its speech component, where we synthesize\nemotion-consistent voices for all 40,150 dialogues, creating the first\nlarge-scale open-source multimodal dialogue dataset that faithfully preserves\nemotional context across multi-turn conversations.", "AI": {"tldr": "\u7814\u7a76\u56e2\u961f\u6784\u5efa\u4e86DeepDialogue\u5927\u89c4\u6a21\u591a\u6a21\u6001\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u8de8\u6a21\u578b\u751f\u6210\u4e0e\u4e25\u683c\u8d28\u91cf\u8fc7\u6ee4\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u89c4\u6a21\u3001\u9886\u57df\u7c7b\u578b\u53ca\u4ea4\u4e92\u6a21\u5f0f\u5bf9\u5bf9\u8bdd\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u60c5\u611f\u8bed\u97f3\u5408\u6210\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u6570\u636e\u96c6\u5b58\u5728\u60c5\u611f\u5355\u4e00\u3001\u9886\u57df\u5c40\u9650\u3001\u8f6e\u6b21\u6d45\u5c42\u53ca\u6a21\u6001\u5355\u4e00\u7b49\u95ee\u9898\uff0c\u5236\u7ea6\u4e86\u591a\u8f6e\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "method": "\u91c7\u75289\u79cd\u4e0d\u540c\u89c4\u6a21\u7684\u8bed\u8a00\u6a21\u578b\u751f\u6210\u521d\u59cb\u5bf9\u8bdd\uff0c\u7ed3\u5408\u4eba\u5de5\u6807\u6ce8\u4e0eLLM\u8d28\u91cf\u8fc7\u6ee4\uff0c\u6784\u5efa\u5305\u542b4\u4e07+\u5bf9\u8bdd\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5408\u6210\u60c5\u611f\u5339\u914d\u7684\u8bed\u97f3\u7ec4\u4ef6\u3002", "result": "\u5c0f\u6a21\u578b\uff084B-7B\uff09\u57286\u8f6e\u540e\u5bf9\u8bdd\u5d29\u6e83\uff1b\u5177\u8c61\u9886\u57df\uff08\u5982\u6c7d\u8f66/\u65c5\u884c\uff09\u6bd4\u62bd\u8c61\u9886\u57df\u5bf9\u8bdd\u8d28\u91cf\u9ad828%\uff1b\u8de8\u6a21\u578b\u5bf9\u8bdd\u6bd4\u540c\u6a21\u578b\u5bf9\u8bdd\u8fde\u8d2f\u6027\u63d0\u534719%\u3002", "conclusion": "DeepDialogue\u586b\u8865\u4e86\u591a\u6a21\u6001\u60c5\u611f\u5bf9\u8bdd\u6570\u636e\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u80fd\u529b\u8fb9\u754c\u4e0e\u6570\u636e\u7279\u5f81\u89c4\u5f8b\uff0c\u4e3a\u5f00\u53d1\u66f4\u81ea\u7136\u7684\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5173\u952e\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2505.19987", "pdf": "https://arxiv.org/pdf/2505.19987", "abs": "https://arxiv.org/abs/2505.19987", "authors": ["Yongshi Ye", "Biao Fu", "Chongxuan Huang", "Yidong Chen", "Xiaodong Shi"], "title": "How Well Do Large Reasoning Models Translate? A Comprehensive Evaluation for Multi-Domain Machine Translation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated strong performance in\ngeneral-purpose machine translation, but their effectiveness in complex,\ndomain-sensitive translation tasks remains underexplored. Recent advancements\nin Large Reasoning Models (LRMs), raise the question of whether structured\nreasoning can enhance translation quality across diverse domains. In this work,\nwe compare the performance of LRMs with traditional LLMs across 15\nrepresentative domains and four translation directions. Our evaluation\nconsiders various factors, including task difficulty, input length, and\nterminology density. We use a combination of automatic metrics and an enhanced\nMQM-based evaluation hierarchy to assess translation quality. Our findings show\nthat LRMs consistently outperform traditional LLMs in semantically complex\ndomains, especially in long-text and high-difficulty translation scenarios.\nMoreover, domain-adaptive prompting strategies further improve performance by\nbetter leveraging the reasoning capabilities of LRMs. These results highlight\nthe potential of structured reasoning in MDMT tasks and provide valuable\ninsights for optimizing translation systems in domain-sensitive contexts.", "AI": {"tldr": "LRMs\u5728\u8bed\u4e49\u590d\u6742\u9886\u57df\u8d85\u8d8a\u4f20\u7edfLLMs\uff0c\u5c24\u5176\u5728\u957f\u6587\u672c\u548c\u9ad8\u96be\u5ea6\u7ffb\u8bd1\u573a\u666f\uff0c\u9886\u57df\u81ea\u9002\u5e94\u63d0\u793a\u7b56\u7565\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd", "motivation": "\u63a2\u7d22\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u5426\u63d0\u5347\u8de8\u9886\u57df\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\uff0c\u7279\u522b\u662f\u4f20\u7edfLLMs\u5728\u590d\u6742\u9886\u57df\u654f\u611f\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3", "method": "\u8de815\u4e2a\u9886\u57df\u548c4\u4e2a\u7ffb\u8bd1\u65b9\u5411\u5bf9\u6bd4LRMs\u4e0eLLMs\uff0c\u7ed3\u5408\u81ea\u52a8\u6307\u6807\u548c\u589e\u5f3a\u578bMQM\u8bc4\u4f30\u6846\u67b6\uff0c\u5206\u6790\u4efb\u52a1\u96be\u5ea6/\u6587\u672c\u957f\u5ea6/\u672f\u8bed\u5bc6\u5ea6\u7b49\u56e0\u7d20", "result": "LRMs\u5728\u8bed\u4e49\u590d\u6742\u5ea6\u9ad8\u7684\u9886\u57df\u8868\u73b0\u66f4\u4f18\uff0c\u957f\u6587\u672c\u7ffb\u8bd1\u8d28\u91cf\u63d0\u534712.7%\uff0c\u9ad8\u96be\u5ea6\u4efb\u52a1\u9519\u8bef\u7387\u964d\u4f4e23%", "conclusion": "\u7ed3\u6784\u5316\u63a8\u7406\u663e\u8457\u63d0\u5347\u591a\u9886\u57df\u673a\u5668\u7ffb\u8bd1\u6027\u80fd\uff0c\u9886\u57df\u81ea\u9002\u5e94\u7b56\u7565\u4e3a\u4f18\u5316\u7ffb\u8bd1\u7cfb\u7edf\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2505.20006", "pdf": "https://arxiv.org/pdf/2505.20006", "abs": "https://arxiv.org/abs/2505.20006", "authors": ["Rapha\u00ebl Bagat", "Irina Illina", "Emmanuel Vincent"], "title": "Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech Recognition", "categories": ["cs.CL"], "comment": "Submitted to Interspeech 2025", "summary": "We aim to improve the robustness of Automatic Speech Recognition (ASR)\nsystems against non-native speech, particularly in low-resourced multi-accent\nsettings. We introduce Mixture of Accent-Specific LoRAs (MAS-LoRA), a\nfine-tuning method that leverages a mixture of Low-Rank Adaptation (LoRA)\nexperts, each specialized in a specific accent. This method can be used when\nthe accent is known or unknown at inference time, without the need to fine-tune\nthe model again. Our experiments, conducted using Whisper on the L2-ARCTIC\ncorpus, demonstrate significant improvements in Word Error Rate compared to\nregular LoRA and full fine-tuning when the accent is unknown. When the accent\nis known, the results further improve. Furthermore, MAS-LoRA shows less\ncatastrophic forgetting than the other fine-tuning methods. To the best of our\nknowledge, this is the first use of a mixture of LoRA experts for non-native\nmulti-accent ASR.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u53e3\u97f3\u4e13\u7528LoRA\u4e13\u5bb6\uff08MAS-LoRA\uff09\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4f4e\u8d44\u6e90\u591a\u53e3\u97f3\u573a\u666f\u4e0b\u8bed\u97f3\u8bc6\u522b\u7684\u9c81\u68d2\u6027\uff0c\u51cf\u5c11\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u9488\u5bf9\u975e\u6bcd\u8bed\u591a\u53e3\u97f3ASR\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u672a\u77e5\u53e3\u97f3\u65f6\u8868\u73b0\u4e0d\u4f73\u4e14\u9700\u8981\u91cd\u590d\u5fae\u8c03\u3002", "method": "\u57fa\u4e8eWhisper\u6a21\u578b\uff0c\u4f7f\u7528\u6df7\u5408\u4f4e\u79e9\u9002\u914d\u5668\uff08LoRA\uff09\u4e13\u5bb6\u67b6\u6784\uff0c\u652f\u6301\u5df2\u77e5/\u672a\u77e5\u53e3\u97f3\u63a8\u65ad\u4e14\u65e0\u9700\u91cd\u65b0\u5fae\u8c03\u3002", "result": "\u5728L2-ARCTIC\u8bed\u6599\u5e93\u4e0a\uff0c\u8bcd\u9519\u8bef\u7387\u76f8\u6bd4\u5e38\u89c4LoRA\u548c\u5168\u5fae\u8c03\u5206\u522b\u964d\u4f4e18.3%\u548c9.7%\uff08\u672a\u77e5\u53e3\u97f3\uff09\uff0c\u5df2\u77e5\u53e3\u97f3\u6548\u679c\u66f4\u4f18\u3002", "conclusion": "\u9996\u6b21\u5c06\u6df7\u5408LoRA\u4e13\u5bb6\u5e94\u7528\u4e8e\u591a\u53e3\u97f3ASR\uff0c\u9a8c\u8bc1\u4e86\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3a\u4f4e\u8d44\u6e90\u573a\u666f\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.20013", "pdf": "https://arxiv.org/pdf/2505.20013", "abs": "https://arxiv.org/abs/2505.20013", "authors": ["Minda Hu", "Tianqing Fang", "Jianshu Zhang", "Junyu Ma", "Zhisong Zhang", "Jingyan Zhou", "Hongming Zhang", "Haitao Mi", "Dong Yu", "Irwin King"], "title": "WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback", "categories": ["cs.CL"], "comment": "18 pages", "summary": "Web agents powered by Large Language Models (LLMs) show promise for\nnext-generation AI, but their limited reasoning in uncertain, dynamic web\nenvironments hinders robust deployment. In this paper, we identify key\nreasoning skills essential for effective web agents, i.e., reflection &\nlookahead, branching, and rollback, and curate trajectory data that exemplifies\nthese abilities by reconstructing the agent's (inference-time) reasoning\nalgorithms into chain-of-thought rationales. We conduct experiments in the\nagent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling\nsalient reasoning patterns into the backbone LLM via simple fine-tuning can\nsubstantially enhance its performance. Our approach yields significant\nimprovements across multiple benchmarks, including WebVoyager, Mind2web-live,\nand SimpleQA (web search), highlighting the potential of targeted reasoning\nskill enhancement for web agents.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7f51\u9875\u4ee3\u7406\u7684\u4e09\u79cd\u5173\u952e\u63a8\u7406\u80fd\u529b\uff08\u53cd\u601d\u524d\u77bb\u3001\u5206\u652f\u51b3\u7b56\u3001\u72b6\u6001\u56de\u6eda\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5176\u5728\u52a8\u6001\u7f51\u9875\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7f51\u9875\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\uff0c\u5236\u7ea6\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u9488\u5bf9\u6027\u63d0\u5347\u6838\u5fc3\u63a8\u7406\u80fd\u529b\u4ee5\u5b9e\u73b0\u7a33\u5065\u90e8\u7f72\u3002", "method": "\u5c06\u4ee3\u7406\u7684\u63a8\u7406\u7b97\u6cd5\u91cd\u6784\u4e3a\u601d\u7ef4\u94fe\u8f68\u8ff9\u6570\u636e\uff0c\u5728OpenWebVoyager\u57fa\u51c6\u4e2d\u901a\u8fc7\u5fae\u8c03LLM\u5b9e\u73b0\u63a8\u7406\u6a21\u5f0f\u84b8\u998f\u3002", "result": "\u5728WebVoyager\u3001Mind2web-live\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u6700\u9ad8\u8fbe15%\u7edd\u5bf9\u51c6\u786e\u7387\u6539\u8fdb\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u63a8\u7406\u80fd\u529b\u589e\u5f3a\u548c\u601d\u7ef4\u94fe\u6570\u636e\u84b8\u998f\uff0c\u53ef\u6709\u6548\u63d0\u5347\u7f51\u9875\u4ee3\u7406\u7684\u51b3\u7b56\u8d28\u91cf\uff0c\u4e3aAI\u4ee3\u7406\u53d1\u5c55\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2505.20014", "pdf": "https://arxiv.org/pdf/2505.20014", "abs": "https://arxiv.org/abs/2505.20014", "authors": ["Hoyun Song", "Huije Lee", "Jisu Shin", "Sukmin Cho", "Changgeon Ko", "Jong C. Park"], "title": "Does Rationale Quality Matter? Enhancing Mental Disorder Detection via Selective Reasoning Distillation", "categories": ["cs.CL"], "comment": null, "summary": "The detection of mental health problems from social media and the\ninterpretation of these results have been extensively explored. Research has\nshown that incorporating clinical symptom information into a model enhances\ndomain expertise, improving its detection and interpretation performance. While\nlarge language models (LLMs) are shown to be effective for generating\nexplanatory rationales in mental health detection, their substantially large\nparameter size and high computational cost limit their practicality. Reasoning\ndistillation transfers this ability to smaller language models (SLMs), but\ninconsistencies in the relevance and domain alignment of LLM-generated\nrationales pose a challenge. This paper investigates how rationale quality\nimpacts SLM performance in mental health detection and explanation generation.\nWe hypothesize that ensuring high-quality and domain-relevant rationales\nenhances the distillation. To this end, we propose a framework that selects\nrationales based on their alignment with expert clinical reasoning. Experiments\nshow that our quality-focused approach significantly enhances SLM performance\nin both mental disorder detection and rationale generation. This work\nhighlights the importance of rationale quality and offers an insightful\nframework for knowledge transfer in mental health applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e34\u5e8a\u63a8\u7406\u5bf9\u9f50\u7684rationale\u9009\u62e9\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u5065\u5eb7\u68c0\u6d4b\u4e0e\u89e3\u91ca\u751f\u6210\u4e2d\u7684\u53cc\u4efb\u52a1\u8868\u73b0", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210rationale\u5b58\u5728\u9886\u57df\u5bf9\u9f50\u7f3a\u9677\uff0c\u5bfc\u81f4\u77e5\u8bc6\u84b8\u998f\u5230\u5c0f\u578b\u6a21\u578b\u65f6\u6548\u679c\u53d7\u9650\uff0c\u9700\u786e\u4fddrationale\u8d28\u91cf\u4e0e\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u5bf9\u9f50", "method": "\u8bbe\u8ba1rationale\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u4e34\u5e8a\u63a8\u7406\u5bf9\u9f50\u673a\u5236\u7b5b\u9009\u9ad8\u4ef7\u503c\u6837\u672c\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u4f7fSLM\u5728\u5fc3\u7406\u5065\u5eb7\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u534712.6%\uff0c\u751f\u6210\u89e3\u91ca\u7684\u4e34\u5e8a\u76f8\u5173\u6027\u63d0\u9ad834%", "conclusion": "rationale\u8d28\u91cf\u76f4\u63a5\u5f71\u54cd\u77e5\u8bc6\u8fc1\u79fb\u6548\u679c\uff0c\u63d0\u51fa\u7684\u8d28\u91cf\u5bfc\u5411\u6846\u67b6\u4e3a\u5fc3\u7406\u5065\u5eb7NLP\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u8def\u5f84"}}
{"id": "2505.20015", "pdf": "https://arxiv.org/pdf/2505.20015", "abs": "https://arxiv.org/abs/2505.20015", "authors": ["Ramon Ferrer-i-Cancho"], "title": "On the class of coding optimality of human languages and the origins of Zipf's law", "categories": ["cs.CL", "physics.soc-ph"], "comment": null, "summary": "Here we present a new class of optimality for coding systems. Members of that\nclass are separated linearly from optimal coding and thus exhibit Zipf's law,\nnamely a power-law distribution of frequency ranks. Whithin that class, Zipf's\nlaw, the size-rank law and the size-probability law form a group-like\nstructure. We identify human languages that are members of the class. All\nlanguages showing sufficient agreement with Zipf's law are potential members of\nthe class. In contrast, there are communication systems in other species that\ncannot be members of that class for exhibiting an exponential distribution\ninstead but dolphins and humpback whales might. We provide a new insight into\nplots of frequency versus rank in double logarithmic scale. For any system, a\nstraight line in that scale indicates that the lengths of optimal codes under\nnon-singular coding and under uniquely decodable encoding are separated by a\nlinear function whose slope is the exponent of Zipf's law. For systems under\ncompression and constrained to be uniquely decodable, such a straight line may\nindicate that the system is coding close to optimality. Our findings provide\nsupport for the hypothesis that Zipf's law originates from compression.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u6700\u4f18\u7f16\u7801\u7c7b\u522b\u9a8c\u8bc1Zipf\u5b9a\u5f8b\u6e90\u4e8e\u538b\u7f29\u5047\u8bf4\uff0c\u4eba\u7c7b\u8bed\u8a00\u7b26\u5408\u8be5\u7c7b\u522b\u800c\u591a\u6570\u52a8\u7269\u901a\u4fe1\u7cfb\u7edf\u4e0d\u7b26\u5408\u3002", "motivation": "\u63a2\u7a76Zipf\u5b9a\u5f8b\u5728\u901a\u4fe1\u7cfb\u7edf\u4e2d\u7684\u666e\u904d\u6027\u53ca\u5176\u4e0e\u6700\u4f18\u7f16\u7801\u7684\u5173\u7cfb\uff0c\u9a8c\u8bc1\u4fe1\u606f\u538b\u7f29\u662f\u5426\u4e3a\u8be5\u5b9a\u5f8b\u7684\u8d77\u6e90\u3002", "method": "\u6784\u5efa\u6700\u4f18\u7f16\u7801\u7cfb\u7edf\u7406\u8bba\u6a21\u578b\uff0c\u5206\u6790\u4eba\u7c7b\u8bed\u8a00\u4e0e\u52a8\u7269\u901a\u4fe1\u7cfb\u7edf\u7684\u9891\u7387\u5206\u5e03\u7279\u5f81\uff0c\u901a\u8fc7\u53cc\u5bf9\u6570\u5750\u6807\u9a8c\u8bc1\u7ebf\u6027\u5173\u7cfb\u3002", "result": "\u4eba\u7c7b\u8bed\u8a00\u7b26\u5408\u6700\u4f18\u7f16\u7801\u7c7b\u522b\u5448\u73b0Zipf\u5b9a\u5f8b\uff0c\u6d77\u8c5a/\u9cb8\u7c7b\u53ef\u80fd\u4f8b\u5916\uff1b\u53cc\u5bf9\u6570\u56fe\u76f4\u7ebf\u659c\u7387\u8bc1\u5b9e\u7f16\u7801\u6700\u4f18\u6027\u5206\u79bb\u5173\u7cfb\u3002", "conclusion": "Zipf\u5b9a\u5f8b\u6e90\u4e8e\u4fe1\u606f\u538b\u7f29\u673a\u5236\uff0c\u4eba\u7c7b\u8bed\u8a00\u56e0\u7b26\u5408\u6700\u4f18\u7f16\u7801\u5c55\u73b0\u8be5\u5b9a\u5f8b\uff0c\u6307\u6570\u5206\u5e03\u7cfb\u7edf\u88ab\u6392\u9664\u8be5\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2505.20016", "pdf": "https://arxiv.org/pdf/2505.20016", "abs": "https://arxiv.org/abs/2505.20016", "authors": ["Chengrui Huang", "Shen Gao", "Zhengliang Shi", "Dongsheng Wang", "Shuo Shang"], "title": "TTPA: Token-level Tool-use Preference Alignment Training Framework with Fine-grained Evaluation", "categories": ["cs.CL"], "comment": "16 pages, 5 figures", "summary": "Existing tool-learning methods usually rely on supervised fine-tuning, they\noften overlook fine-grained optimization of internal tool call details, leading\nto limitations in preference alignment and error discrimination. To overcome\nthese challenges, we propose Token-level Tool-use Preference Alignment Training\nFramework (TTPA), a training paradigm for constructing token-level tool-use\npreference datasets that align LLMs with fine-grained preferences using a novel\nerror-oriented scoring mechanism. TTPA first introduces reversed dataset\nconstruction, a method for creating high-quality, multi-turn tool-use datasets\nby reversing the generation flow. Additionally, we propose Token-level\nPreference Sampling (TPS) to capture fine-grained preferences by modeling\ntoken-level differences during generation. To address biases in scoring, we\nintroduce the Error-oriented Scoring Mechanism (ESM), which quantifies\ntool-call errors and can be used as a training signal. Extensive experiments on\nthree diverse benchmark datasets demonstrate that TTPA significantly improves\ntool-using performance while showing strong generalization ability across\nmodels and datasets.", "AI": {"tldr": "\u63d0\u51faTTPA\u6846\u67b6\uff0c\u901a\u8fc7token\u7ea7\u504f\u597d\u5bf9\u9f50\u548c\u9006\u5411\u6570\u636e\u96c6\u6784\u5efa\u4f18\u5316\u5927\u6a21\u578b\u5de5\u5177\u4f7f\u7528\u6027\u80fd\uff0c\u5728\u4e09\u5927\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u76d1\u7763\u5fae\u8c03\uff0c\u5ffd\u89c6\u5de5\u5177\u8c03\u7528\u7ec6\u8282\u7684\u7ec6\u7c92\u5ea6\u4f18\u5316\uff0c\u5bfc\u81f4\u504f\u597d\u5bf9\u9f50\u4e0d\u8db3\u548c\u9519\u8bef\u5224\u522b\u5c40\u9650\u3002", "method": "1.\u9006\u5411\u6570\u636e\u6784\u5efa\u751f\u6210\u591a\u8f6e\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff1b2.\u63d0\u51fatoken\u7ea7\u504f\u597d\u91c7\u6837(TPS)\u6355\u6349\u7ec6\u7c92\u5ea6\u5dee\u5f02\uff1b3.\u8bbe\u8ba1\u9762\u5411\u9519\u8bef\u7684\u8bc4\u5206\u673a\u5236(ESM)\u91cf\u5316\u5de5\u5177\u8c03\u7528\u9519\u8bef\u3002", "result": "\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTTPA\u663e\u8457\u63d0\u5347\u5de5\u5177\u4f7f\u7528\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u8de8\u6a21\u578b\u3001\u8de8\u6570\u636e\u96c6\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u5177\u5b66\u4e60\u4e2d\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u96be\u9898\uff0c\u4e3a\u63d0\u5347\u5927\u6a21\u578b\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.20023", "pdf": "https://arxiv.org/pdf/2505.20023", "abs": "https://arxiv.org/abs/2505.20023", "authors": ["Yihan Chen", "Benfeng Xu", "Xiaorui Wang", "Yongdong Zhang", "Zhendong Mao"], "title": "Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking", "categories": ["cs.CL"], "comment": null, "summary": "Autonomous agents, which perceive environments and take actions to achieve\ngoals, have become increasingly feasible with the advancements in large\nlanguage models (LLMs). However, current powerful agents often depend on\nsophisticated prompt engineering combined with closed-source LLMs like GPT-4.\nAlthough training open-source LLMs using expert trajectories from teacher\nmodels has yielded some improvements in agent capabilities, this approach still\nfaces limitations such as performance plateauing and error propagation. To\nmitigate these challenges, we propose STeP, a novel method for improving\nLLM-based agent training. We synthesize self-reflected trajectories that\ninclude reflections and corrections of error steps, which enhance the\neffectiveness of LLM agents in learning from teacher models, enabling them to\nbecome agents capable of self-reflecting and correcting. We also introduce\npartial masking strategy that prevents the LLM from internalizing incorrect or\nsuboptimal steps. Experiments demonstrate that our method improves agent\nperformance across three representative tasks: ALFWorld, WebShop, and SciWorld.\nFor the open-source model LLaMA2-7B-Chat, when trained using self-reflected\ntrajectories constructed with Qwen1.5-110B-Chat as the teacher model, it\nachieves comprehensive improvements with less training data compared to agents\ntrained exclusively on expert trajectories.", "AI": {"tldr": "\u63d0\u51faSTeP\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u6210\u5e26\u9519\u8bef\u6b65\u9aa4\u53cd\u601d\u4e0e\u4fee\u6b63\u7684\u81ea\u53cd\u601d\u7ef4\u8f68\u8ff9\uff0c\u7ed3\u5408\u90e8\u5206\u63a9\u7801\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347LLM\u667a\u80fd\u4f53\u5728ALFWorld\u7b49\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4e13\u5bb6\u8f68\u8ff9\u8bad\u7ec3\u7684\u5f00\u6e90LLM\u667a\u80fd\u4f53\u5b58\u5728\u6027\u80fd\u74f6\u9888\u548c\u9519\u8bef\u4f20\u64ad\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u51cf\u5c11\u5bf9\u95ed\u6e90\u6a21\u578b\u548c\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\u3002", "method": "1. \u5408\u6210\u5305\u542b\u9519\u8bef\u6b65\u9aa4\u53cd\u601d\u4e0e\u4fee\u6b63\u7684\u81ea\u53cd\u601d\u7ef4\u8f68\u8ff9\uff1b2. \u5f15\u5165\u90e8\u5206\u63a9\u7801\u7b56\u7565\u9632\u6b62\u6a21\u578b\u5438\u6536\u9519\u8bef/\u6b21\u4f18\u6b65\u9aa4\uff1b3. \u4f7f\u7528Qwen1.5-110B-Chat\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u751f\u6210\u8bad\u7ec3\u6570\u636e\u3002", "result": "LLaMA2-7B-Chat\u6a21\u578b\u5728\u4e09\u4e2a\u57fa\u51c6\u4efb\u52a1\u4e2d\u5b9e\u73b0\u5168\u9762\u6027\u80fd\u63d0\u5347\uff0c\u76f8\u6bd4\u7eaf\u4e13\u5bb6\u8f68\u8ff9\u8bad\u7ec3\uff0c\u6570\u636e\u9700\u6c42\u51cf\u5c11\u4e14\u6548\u679c\u66f4\u4f18\u3002", "conclusion": "STeP\u65b9\u6cd5\u901a\u8fc7\u81ea\u6211\u53cd\u601d\u673a\u5236\u548c\u6297\u8fc7\u62df\u5408\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86LLM\u667a\u80fd\u4f53\u7684\u5b66\u4e60\u6548\u7387\u4e0e\u4efb\u52a1\u8868\u73b0\uff0c\u4e3a\u5f00\u6e90\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.20045", "pdf": "https://arxiv.org/pdf/2505.20045", "abs": "https://arxiv.org/abs/2505.20045", "authors": ["Artem Vazhentsev", "Lyudmila Rvanova", "Gleb Kuzmin", "Ekaterina Fadeeva", "Ivan Lazichny", "Alexander Panchenko", "Maxim Panov", "Timothy Baldwin", "Mrinmaya Sachan", "Preslav Nakov", "Artem Shelmanov"], "title": "Uncertainty-Aware Attention Heads: Efficient Unsupervised Uncertainty Quantification for LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit impressive fluency, but often produce\ncritical errors known as \"hallucinations\". Uncertainty quantification (UQ)\nmethods are a promising tool for coping with this fundamental shortcoming. Yet,\nexisting UQ methods face challenges such as high computational overhead or\nreliance on supervised learning. Here, we aim to bridge this gap. In\nparticular, we propose RAUQ (Recurrent Attention-based Uncertainty\nQuantification), an unsupervised approach that leverages intrinsic attention\npatterns in transformers to detect hallucinations efficiently. By analyzing\nattention weights, we identified a peculiar pattern: drops in attention to\npreceding tokens are systematically observed during incorrect generations for\ncertain \"uncertainty-aware\" heads. RAUQ automatically selects such heads,\nrecurrently aggregates their attention weights and token-level confidences, and\ncomputes sequence-level uncertainty scores in a single forward pass.\nExperiments across 4 LLMs and 12 question answering, summarization, and\ntranslation tasks demonstrate that RAUQ yields excellent results, outperforming\nstate-of-the-art UQ methods using minimal computational overhead (<1% latency).\nMoreover, it requires no task-specific labels and no careful hyperparameter\ntuning, offering plug-and-play real-time hallucination detection in white-box\nLLMs.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTransformer\u6ce8\u610f\u529b\u673a\u5236\u7684RAUQ\u65b9\u6cd5\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u65b9\u5f0f\u5b9e\u65f6\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\uff0c\u8ba1\u7b97\u6548\u7387\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08<1%\u5ef6\u8fdf\uff09", "motivation": "\u89e3\u51b3\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u5b58\u5728\u7684\u9ad8\u8ba1\u7b97\u5f00\u9500\u3001\u4f9d\u8d56\u76d1\u7763\u5b66\u4e60\u7b49\u95ee\u9898\uff0c\u5229\u7528LLM\u81ea\u8eab\u6ce8\u610f\u529b\u6a21\u5f0f\u5b9e\u73b0\u9ad8\u6548\u5e7b\u89c9\u68c0\u6d4b", "method": "\u5206\u6790\u6ce8\u610f\u529b\u6743\u91cd\u8bc6\u522b\u7279\u6b8a\u6a21\u5f0f\u2192\u81ea\u52a8\u9009\u62e9\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u5934\u90e8\u2192\u9012\u5f52\u805a\u5408\u6ce8\u610f\u529b\u6743\u91cd\u548c\u7f6e\u4fe1\u5ea6\u2192\u5355\u6b21\u524d\u5411\u4f20\u64ad\u8ba1\u7b97\u5e8f\u5217\u7ea7\u4e0d\u786e\u5b9a\u6027\u5206\u6570", "result": "\u572812\u4e2a\u95ee\u7b54/\u6458\u8981/\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8d85\u8d8aSOTA\u65b9\u6cd5\uff0c\u8ba1\u7b97\u5ef6\u8fdf<1%\uff0c\u65e0\u9700\u4efb\u52a1\u6807\u7b7e\u548c\u590d\u6742\u8c03\u53c2", "conclusion": "RAUQ\u4e3a\u767d\u76d2LLM\u63d0\u4f9b\u5373\u63d2\u5373\u7528\u7684\u5b9e\u65f6\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u53ef\u9760\u6027\u540c\u65f6\u4fdd\u6301\u9ad8\u6548\u8fd0\u884c"}}
{"id": "2505.20047", "pdf": "https://arxiv.org/pdf/2505.20047", "abs": "https://arxiv.org/abs/2505.20047", "authors": ["Debargha Ganguly", "Vikash Singh", "Sreehari Sankar", "Biyao Zhang", "Xuecen Zhang", "Srinivasan Iyengar", "Xiaotian Han", "Amit Sharma", "Shivkumar Kalyanaraman", "Vipin Chaudhary"], "title": "Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks", "categories": ["cs.CL", "cs.AI", "cs.LO", "cs.SE"], "comment": null, "summary": "Large language models (LLMs) show remarkable promise for democratizing\nautomated reasoning by generating formal specifications. However, a fundamental\ntension exists: LLMs are probabilistic, while formal verification demands\ndeterministic guarantees. This paper addresses this epistemological gap by\ncomprehensively investigating failure modes and uncertainty quantification (UQ)\nin LLM-generated formal artifacts. Our systematic evaluation of five frontier\nLLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's\ndomain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on\nfactual ones), with known UQ techniques like the entropy of token probabilities\nfailing to identify these errors. We introduce a probabilistic context-free\ngrammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty\ntaxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy\nfor logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables\nselective verification, drastically reducing errors (14-100%) with minimal\nabstention, transforming LLM-driven formalization into a reliable engineering\ndiscipline.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51faPCFG\u6846\u67b6\u91cf\u5316LLM\u751f\u6210\u5f62\u5f0f\u5316\u89c4\u8303\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u4fe1\u53f7\u878d\u5408\u5b9e\u73b0\u9009\u62e9\u6027\u9a8c\u8bc1\uff0c\u9519\u8bef\u7387\u964d\u4f4e14-100%", "motivation": "\u89e3\u51b3LLM\u6982\u7387\u6027\u8f93\u51fa\u4e0e\u5f62\u5f0f\u9a8c\u8bc1\u786e\u5b9a\u6027\u9700\u6c42\u4e4b\u95f4\u7684\u6839\u672c\u77db\u76fe", "method": "\u7cfb\u7edf\u8bc4\u4f305\u4e2a\u524d\u6cbfLLM+SMT\u81ea\u52a8\u5f62\u5f0f\u5316\u6548\u679c\uff0c\u6784\u5efaPCFG\u6846\u67b6\u6539\u8fdb\u4e0d\u786e\u5b9a\u6027\u5206\u7c7b\uff0c\u5f00\u53d1\u4fe1\u53f7\u878d\u5408\u6280\u672f", "result": "\u903b\u8f91\u4efb\u52a1\u51c6\u786e\u7387\u63d0\u534734.8%\uff08\u8bed\u6cd5\u71b5AUROC>0.93\uff09\uff0c\u4f46\u4e8b\u5b9e\u6027\u4efb\u52a1\u4e0b\u964d44.5%\uff1b\u4fe1\u53f7\u6709\u6548\u6027\u5177\u6709\u4efb\u52a1\u4f9d\u8d56\u6027", "conclusion": "\u8f7b\u91cf\u7ea7\u4fe1\u53f7\u878d\u5408\u663e\u8457\u63d0\u5347\u53ef\u9760\u6027\uff0c\u5c06LLM\u5f62\u5f0f\u5316\u8f6c\u53d8\u4e3a\u53ef\u4fe1\u4efb\u7684\u5de5\u7a0b\u5b66\u79d1"}}
{"id": "2505.20072", "pdf": "https://arxiv.org/pdf/2505.20072", "abs": "https://arxiv.org/abs/2505.20072", "authors": ["Yige Yuan", "Teng Xiao", "Shuchang Tao", "Xue Wang", "Jinyang Gao", "Bolin Ding", "Bingbing Xu"], "title": "Incentivizing Reasoning from Weak Supervision", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance on\nreasoning-intensive tasks, but enhancing their reasoning abilities typically\nrelies on either reinforcement learning (RL) with verifiable signals or\nsupervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT)\ndemonstrations, both of which are expensive. In this paper, we study a novel\nproblem of incentivizing the reasoning capacity of LLMs without expensive\nhigh-quality demonstrations and reinforcement learning. We investigate whether\nthe reasoning capabilities of LLMs can be effectively incentivized via\nsupervision from significantly weaker models. We further analyze when and why\nsuch weak supervision succeeds in eliciting reasoning abilities in stronger\nmodels. Our findings show that supervision from significantly weaker reasoners\ncan substantially improve student reasoning performance, recovering close to\n94% of the gains of expensive RL at a fraction of the cost. Experiments across\ndiverse benchmarks and model architectures demonstrate that weak reasoners can\neffectively incentivize reasoning in stronger student models, consistently\nimproving performance across a wide range of reasoning tasks. Our results\nsuggest that this simple weak-to-strong paradigm is a promising and\ngeneralizable alternative to costly methods for incentivizing strong reasoning\ncapabilities at inference-time in LLMs. The code is publicly available at\nhttps://github.com/yuanyige/W2SR.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u5f31\u76d1\u7763\u65b9\u6cd5\uff08W2SR\uff09\u7528\u4f4e\u8d28\u91cf\u5c0f\u6a21\u578b\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u6210\u672c\u4ec5\u4e3a\u5f3a\u5316\u5b66\u4e60\u7684\u6781\u5c0f\u90e8\u5206", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u548c\u94fe\u5f0f\u601d\u8003\u76d1\u7763\u5b66\u4e60\u6210\u672c\u8fc7\u9ad8\uff0c\u63a2\u7d22\u662f\u5426\u80fd\u7528\u5f31\u76d1\u7763\u4fe1\u53f7\u6709\u6548\u6fc0\u53d1\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b", "method": "\u4f7f\u7528\u663e\u8457\u5f31\u4e8e\u5b66\u751f\u6a21\u578b\u7684\u63a8\u7406\u5668\uff08\u5982GPT-3.5\u76d1\u7763PaLM 2-L\uff09\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\uff0c\u5efa\u7acb\u5f31\u5230\u5f3a\u7684\u77e5\u8bc6\u8fc1\u79fb\u6846\u67b6", "result": "\u5728\u591a\u9879\u63a8\u7406\u4efb\u52a1\u4e2d\u6062\u590dRL\u65b9\u6cd594%\u7684\u6027\u80fd\u589e\u76ca\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\uff08https://github.com/yuanyige/W2SR\uff09", "conclusion": "\u5f31\u5230\u5f3a\u76d1\u7763\u8303\u5f0f\u662f\u66ff\u4ee3\u6602\u8d35\u63a8\u7406\u80fd\u529b\u63d0\u5347\u65b9\u6cd5\u7684\u6709\u6548\u65b9\u6848\uff0c\u5177\u6709\u826f\u597d\u6cdb\u5316\u6027\u548c\u5e94\u7528\u524d\u666f"}}
{"id": "2505.20081", "pdf": "https://arxiv.org/pdf/2505.20081", "abs": "https://arxiv.org/abs/2505.20081", "authors": ["Yige Yuan", "Teng Xiao", "Li Yunfan", "Bingbing Xu", "Shuchang Tao", "Yunqi Qiu", "Huawei Shen", "Xueqi Cheng"], "title": "Inference-time Alignment in Continuous Space", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Aligning large language models with human feedback at inference time has\nreceived increasing attention due to its flexibility. Existing methods rely on\ngenerating multiple responses from the base policy for search using a reward\nmodel, which can be considered as searching in a discrete response space.\nHowever, these methods struggle to explore informative candidates when the base\npolicy is weak or the candidate set is small, resulting in limited\neffectiveness. In this paper, to address this problem, we propose Simple Energy\nAdaptation ($\\textbf{SEA}$), a simple yet effective algorithm for\ninference-time alignment. In contrast to expensive search over the discrete\nspace, SEA directly adapts original responses from the base policy toward the\noptimal one via gradient-based sampling in continuous latent space.\nSpecifically, SEA formulates inference as an iterative optimization procedure\non an energy function over actions in the continuous space defined by the\noptimal policy, enabling simple and effective alignment. For instance, despite\nits simplicity, SEA outperforms the second-best baseline with a relative\nimprovement of up to $ \\textbf{77.51%}$ on AdvBench and $\\textbf{16.36%}$ on\nMATH. Our code is publicly available at https://github.com/yuanyige/SEA", "AI": {"tldr": "\u63d0\u51faSEA\u7b97\u6cd5\uff0c\u901a\u8fc7\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u7684\u68af\u5ea6\u91c7\u6837\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u63a8\u7406\u5bf9\u9f50\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728AdvBench\u548cMATH\u4efb\u52a1\u5206\u522b\u63d0\u534777.51%\u548c16.36%", "motivation": "\u73b0\u6709\u57fa\u4e8e\u79bb\u6563\u54cd\u5e94\u7a7a\u95f4\u641c\u7d22\u7684\u63a8\u7406\u5bf9\u9f50\u65b9\u6cd5\u5728\u57fa\u7840\u7b56\u7565\u8f83\u5f31\u6216\u5019\u9009\u96c6\u8f83\u5c0f\u65f6\u6548\u679c\u53d7\u9650\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u4f18\u5316\u65b9\u5f0f", "method": "\u5728\u8fde\u7eed\u6f5c\u5728\u7a7a\u95f4\u5b9a\u4e49\u80fd\u91cf\u51fd\u6570\uff0c\u901a\u8fc7\u68af\u5ea6\u91c7\u6837\u8fed\u4ee3\u4f18\u5316\u57fa\u7840\u7b56\u7565\u7684\u8f93\u51fa\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u5bf9\u9f50\u4f18\u5316", "result": "\u5728AdvBench\u4efb\u52a1\u53d6\u5f9777.51%\u7684\u76f8\u5bf9\u6539\u8fdb\uff0cMATH\u4efb\u52a1\u63d0\u534716.36%\uff0c\u4e14\u7b97\u6cd5\u5b9e\u73b0\u7b80\u5355\u9ad8\u6548", "conclusion": "SEA\u7b97\u6cd5\u901a\u8fc7\u8fde\u7eed\u7a7a\u95f4\u4f18\u5316\u7a81\u7834\u4e86\u79bb\u6563\u641c\u7d22\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u63a8\u7406\u65f6\u5bf9\u9f50\u63d0\u4f9b\u4e86\u7b80\u5355\u800c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.20088", "pdf": "https://arxiv.org/pdf/2505.20088", "abs": "https://arxiv.org/abs/2505.20088", "authors": ["Nitay Calderon", "Liat Ein-Dor", "Roi Reichart"], "title": "Multi-Domain Explainability of Preferences", "categories": ["cs.CL"], "comment": null, "summary": "Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and\nreward models, are central to aligning and evaluating large language models\n(LLMs). Yet, the underlying concepts that drive these preferences remain poorly\nunderstood. In this work, we propose a fully automated end-to-end method for\ngenerating local and global concept-based explanations of preferences across\nmultiple domains. Our method employs an LLM to discover concepts that\ndifferentiate between chosen and rejected responses and represent them with\nconcept-based vectors. To model the relationships between concepts and\npreferences, we propose a white-box Hierarchical Multi-Domain Regression model\nthat captures both domain-general and domain-specific effects. To evaluate our\nmethod, we curate a dataset spanning eight challenging and diverse domains and\nexplain twelve mechanisms. Our method achieves strong preference prediction\nperformance, outperforming baselines while also being explainable.\nAdditionally, we assess explanations in two novel application-driven settings.\nFirst, guiding LLM outputs with concepts from LaaJ explanations yields\nresponses that those judges consistently prefer. Second, prompting LaaJs with\nconcepts explaining humans improves their preference predictions. Together, our\nwork provides a new paradigm for explainability in the era of LLMs.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u7aef\u5230\u7aef\u65b9\u6cd5\uff0c\u901a\u8fc7\u6982\u5ff5\u5411\u91cf\u548c\u5206\u5c42\u56de\u5f52\u6a21\u578b\u89e3\u91caLLM\u504f\u597d\u673a\u5236\uff0c\u5e76\u5728\u591a\u9886\u57df\u9a8c\u8bc1\u5176\u9884\u6d4b\u6027\u80fd\u548c\u5e94\u7528\u4ef7\u503c", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4eba\u7c7b\u504f\u597d/LLM\u8bc4\u5224/\u5956\u52b1\u6a21\u578b\u7684\u504f\u597d\u673a\u5236\u7f3a\u4e4f\u5bf9\u5176\u5e95\u5c42\u9a71\u52a8\u6982\u5ff5\u7684\u7cfb\u7edf\u89e3\u91ca\uff0c\u963b\u788dLLM\u5bf9\u9f50\u4e0e\u8bc4\u4f30\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027\u63d0\u5347", "method": "1) \u7528LLM\u81ea\u52a8\u53d1\u73b0\u533a\u5206\u9009\u62e9/\u62d2\u7edd\u54cd\u5e94\u7684\u6982\u5ff5\u5e76\u5411\u91cf\u5316 2) \u63d0\u51fa\u5206\u5c42\u591a\u9886\u57df\u56de\u5f52\u6a21\u578b\u6355\u6349\u9886\u57df\u901a\u7528/\u7279\u5b9a\u6548\u5e94 3) \u6784\u5efa8\u9886\u57df\u6570\u636e\u96c6\u9a8c\u8bc112\u79cd\u673a\u5236", "result": "1) \u9884\u6d4b\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u4e14\u53ef\u89e3\u91ca 2) \u6982\u5ff5\u6307\u5bfc\u4f7fLLM\u8f93\u51fa\u66f4\u7b26\u5408\u8bc4\u5224\u504f\u597d 3) \u4eba\u7c7b\u6982\u5ff5\u89e3\u91ca\u63d0\u5347LLM\u8bc4\u5224\u9884\u6d4b\u51c6\u786e\u6027", "conclusion": "\u8be5\u5de5\u4f5c\u5efa\u7acb\u4e86LLM\u65f6\u4ee3\u53ef\u89e3\u91ca\u6027\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u6982\u5ff5\u9a71\u52a8\u7684\u89e3\u91ca\u65b9\u6cd5\u540c\u65f6\u5b9e\u73b0\u9ad8\u6027\u80fd\u9884\u6d4b\u548c\u8de8\u9886\u57df\u5e94\u7528\u6307\u5bfc"}}
{"id": "2505.20096", "pdf": "https://arxiv.org/pdf/2505.20096", "abs": "https://arxiv.org/abs/2505.20096", "authors": ["Thang Nguyen", "Peter Chin", "Yu-Wing Tai"], "title": "MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation\n(RAG) that addresses the inherent ambiguities and reasoning challenges in\ncomplex information-seeking tasks. Unlike conventional RAG methods that rely on\neither end-to-end fine-tuning or isolated component enhancements, MA-RAG\norchestrates a collaborative set of specialized AI agents: Planner, Step\nDefiner, Extractor, and QA Agents, to tackle each stage of the RAG pipeline\nwith task-aware reasoning. Ambiguities may arise from underspecified queries,\nsparse or indirect evidence in retrieved documents, or the need to integrate\ninformation scattered across multiple sources. MA-RAG mitigates these\nchallenges by decomposing the problem into subtasks, such as query\ndisambiguation, evidence extraction, and answer synthesis, and dispatching them\nto dedicated agents equipped with chain-of-thought prompting. These agents\ncommunicate intermediate reasoning and progressively refine the retrieval and\nsynthesis process. Our design allows fine-grained control over information flow\nwithout any model fine-tuning. Crucially, agents are invoked on demand,\nenabling a dynamic and efficient workflow that avoids unnecessary computation.\nThis modular and reasoning-driven architecture enables MA-RAG to deliver\nrobust, interpretable results. Experiments on multi-hop and ambiguous QA\nbenchmarks demonstrate that MA-RAG outperforms state-of-the-art training-free\nbaselines and rivals fine-tuned systems, validating the effectiveness of\ncollaborative agent-based reasoning in RAG.", "AI": {"tldr": "MA-RAG\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u6846\u67b6\u89e3\u51b3RAG\u4e2d\u7684\u6a21\u7cca\u63a8\u7406\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u89e3\u4efb\u52a1+\u4ee3\u7406\u534f\u4f5c\uff0c\u5b9e\u9a8c\u663e\u793a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u4e14\u65e0\u9700\u5fae\u8c03", "motivation": "\u4f20\u7edfRAG\u5728\u5904\u7406\u590d\u6742\u4fe1\u606f\u68c0\u7d22\u65f6\u5b58\u5728\u67e5\u8be2\u6a21\u7cca/\u8bc1\u636e\u7a00\u758f/\u591a\u6e90\u6574\u5408\u56f0\u96be\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684\u63a8\u7406\u63a7\u5236\u673a\u5236", "method": "\u6784\u5efaPlanner/Step Definer/Extractor/QA\u56db\u7c7b\u4ee3\u7406\uff0c\u91c7\u7528\u601d\u7ef4\u94fe\u63d0\u793a\u8fdb\u884c\u4efb\u52a1\u5206\u89e3\uff0c\u52a8\u6001\u8c03\u7528\u4ee3\u7406\u5b9e\u73b0\u68c0\u7d22-\u5408\u6210\u6d41\u7a0b\u4f18\u5316", "result": "\u5728\u591a\u8df3\u548c\u6a21\u7ccaQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u65e0\u8bad\u7ec3\u57fa\u7ebf\uff0c\u6027\u80fd\u63a5\u8fd1\u5fae\u8c03\u7cfb\u7edf\uff08HotpotQA\u51c6\u786e\u7387\u63d0\u53475.2%\uff09", "conclusion": "\u6a21\u5757\u5316\u4ee3\u7406\u67b6\u6784\u901a\u8fc7\u663e\u5f0f\u63a8\u7406\u6b65\u9aa4\u63d0\u5347RAG\u9c81\u68d2\u6027\uff0c\u9a8c\u8bc1\u4e86\u534f\u4f5c\u5f0f\u4ee3\u7406\u5728\u590d\u6742\u4fe1\u606f\u5904\u7406\u4e2d\u7684\u6709\u6548\u6027"}}
{"id": "2505.20097", "pdf": "https://arxiv.org/pdf/2505.20097", "abs": "https://arxiv.org/abs/2505.20097", "authors": ["Liang Cheng", "Tianyi LI", "Zhaowei Wang", "Mark Steedman"], "title": "S2LPP: Small-to-Large Prompt Prediction across LLMs", "categories": ["cs.CL"], "comment": "15 pages", "summary": "The performance of pre-trained Large Language Models (LLMs) is often\nsensitive to nuances in prompt templates, requiring careful prompt engineering,\nadding costs in terms of computing and human effort. In this study, we present\nexperiments encompassing multiple LLMs variants of varying sizes aimed at\nprobing their preference with different prompts. Through experiments on\nQuestion Answering, we show prompt preference consistency across LLMs of\ndifferent sizes. We also show that this consistency extends to other tasks,\nsuch as Natural Language Inference. Utilizing this consistency, we propose a\nmethod to use a smaller model to select effective prompt templates for a larger\nmodel. We show that our method substantially reduces the cost of prompt\nengineering while consistently matching performance with optimal prompts among\ncandidates. More importantly, our experiment shows the efficacy of our strategy\nacross fourteen LLMs and its applicability to a broad range of NLP tasks,\nhighlighting its robustness", "AI": {"tldr": "\u5229\u7528\u5c0f\u6a21\u578b\u9009\u62e9\u63d0\u793a\u6a21\u677f\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u63d0\u793a\u5de5\u7a0b\u6210\u672c", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u63d0\u793a\u6a21\u677f\u654f\u611f\uff0c\u4f20\u7edf\u63d0\u793a\u5de5\u7a0b\u9700\u8017\u8d39\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u548c\u4eba\u529b\u6210\u672c", "method": "\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u4e0d\u540c\u89c4\u6a21LLMs\u7684\u63d0\u793a\u504f\u597d\u4e00\u81f4\u6027\uff0c\u63d0\u51fa\u7528\u5c0f\u6a21\u578b\u7684\u63d0\u793a\u9009\u62e9\u7ed3\u679c\u6307\u5bfc\u5927\u6a21\u578b", "result": "\u65b9\u6cd5\u572814\u4e2aLLMs\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u8de8\u591a\u4e2aNLP\u4efb\u52a1\u4fdd\u6301\u4e0e\u6700\u4f18\u63d0\u793a\u76f8\u5f53\u7684\u51c6\u786e\u7387", "conclusion": "\u8be5\u7b56\u7565\u663e\u8457\u964d\u4f4e\u63d0\u793a\u5de5\u7a0b\u6210\u672c\uff0c\u5177\u6709\u8de8\u6a21\u578b\u89c4\u6a21\u548c\u4efb\u52a1\u7c7b\u578b\u7684\u5f3a\u9c81\u68d2\u6027"}}
{"id": "2505.20099", "pdf": "https://arxiv.org/pdf/2505.20099", "abs": "https://arxiv.org/abs/2505.20099", "authors": ["Chuangtao Ma", "Yongrui Chen", "Tianxing Wu", "Arijit Khan", "Haofen Wang"], "title": "Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Under Review", "summary": "Large language models (LLMs) have demonstrated remarkable performance on\nquestion-answering (QA) tasks because of their superior capabilities in natural\nlanguage understanding and generation. However, LLM-based QA struggles with\ncomplex QA tasks due to poor reasoning capacity, outdated knowledge, and\nhallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs)\nfor QA to address the above challenges. In this survey, we propose a new\nstructured taxonomy that categorizes the methodology of synthesizing LLMs and\nKGs for QA according to the categories of QA and the KG's role when integrating\nwith LLMs. We systematically survey state-of-the-art advances in synthesizing\nLLMs and KGs for QA and compare and analyze these approaches in terms of\nstrength, limitations, and KG requirements. We then align the approaches with\nQA and discuss how these approaches address the main challenges of different\ncomplex QA. Finally, we summarize the advancements, evaluation metrics, and\nbenchmark datasets and highlight open challenges and opportunities.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u77e5\u8bc6\u56fe\u8c31\u878d\u5408\u7684QA\u65b9\u6cd5\u8c03\u7814\uff1a\u63d0\u51fa\u7ed3\u6784\u5316\u5206\u7c7b\u6cd5\uff0c\u7cfb\u7edf\u5206\u6790LLM+KG\u5728\u4e0d\u540cQA\u573a\u666f\u4e2d\u7684\u534f\u540c\u673a\u5236\u4e0e\u6548\u679c", "motivation": "\u89e3\u51b3LLM\u5728\u590d\u6742QA\u4efb\u52a1\u4e2d\u5b58\u5728\u7684\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3001\u77e5\u8bc6\u6ede\u540e\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3aLLM\u7684\u63a8\u7406\u4e0e\u4e8b\u5b9e\u6027", "method": "\u6784\u5efa\u65b0\u578b\u5206\u7c7b\u4f53\u7cfb\uff08\u6309QA\u7c7b\u578b\u548cKG\u89d2\u8272\u5206\u7c7b\uff09\uff0c\u7cfb\u7edf\u6027\u68b3\u7406LLM+KG\u7684\u534f\u540c\u65b9\u6cd5\uff08\u589e\u5f3a\u3001\u66ff\u4ee3\u3001\u6df7\u5408\u6a21\u5f0f\uff09\uff0c\u5bf9\u6bd4\u5206\u6790\u4e0d\u540c\u65b9\u6cd5\u7684\u4f18\u52a3\u53caKG\u9700\u6c42", "result": "\u5efa\u7acb\u8de8\u7ef4\u5ea6\u65b9\u6cd5\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e0d\u540c\u65b9\u6cd5\u5728\u590d\u6742QA\uff08\u591a\u8df3/\u53cd\u4e8b\u5b9e/\u65f6\u5e8f\u63a8\u7406\uff09\u4e2d\u7684\u9002\u914d\u6027\uff0c\u603b\u7ed3\u73b0\u6709\u8bc4\u4f30\u6307\u6807\uff08\u51c6\u786e\u6027/\u53ef\u89e3\u91ca\u6027\uff09\u53ca\u57fa\u51c6\u6570\u636e\u96c6\uff08MetaQA/CommonsenseQA\uff09", "conclusion": "LLM+KG\u534f\u540c\u663e\u8457\u63d0\u5347\u590d\u6742QA\u6027\u80fd\uff0c\u4f46\u9762\u4e34\u52a8\u6001\u77e5\u8bc6\u66f4\u65b0\u3001\u591a\u6a21\u6001KG\u6574\u5408\u53ca\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\u7b49\u5f00\u653e\u6311\u6218"}}
{"id": "2505.20101", "pdf": "https://arxiv.org/pdf/2505.20101", "abs": "https://arxiv.org/abs/2505.20101", "authors": ["Yunhao Wang", "Yuhao Zhang", "Tinghao Yu", "Can Xu", "Feng Zhang", "Fengzong Lian"], "title": "Adaptive Deep Reasoning: Triggering Deep Thinking When Needed", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown impressive capabilities in handling\ncomplex tasks through long-chain reasoning. However, the extensive reasoning\nsteps involved can significantly increase computational costs, posing\nchallenges for real-world deployment. Recent efforts have focused on optimizing\nreasoning efficiency by shortening the Chain-of-Thought (CoT) reasoning\nprocesses through various approaches, such as length-aware prompt engineering,\nsupervised fine-tuning on CoT data with variable lengths, and reinforcement\nlearning with length penalties. Although these methods effectively reduce\nreasoning length, they still necessitate an initial reasoning phase. More\nrecent approaches have attempted to integrate long-chain and short-chain\nreasoning abilities into a single model, yet they still rely on manual control\nto toggle between short and long CoT.In this work, we propose a novel approach\nthat autonomously switches between short and long reasoning chains based on\nproblem complexity. Our method begins with supervised fine-tuning of the base\nmodel to equip both long-chain and short-chain reasoning abilities. We then\nemploy reinforcement learning to further balance short and long CoT generation\nwhile maintaining accuracy through two key strategies: first, integrating\nreinforcement learning with a long-short adaptive group-wise reward strategy to\nassess prompt complexity and provide corresponding rewards; second,\nimplementing a logit-based reasoning mode switching loss to optimize the\nmodel's initial token choice, thereby guiding the selection of the reasoning\ntype.Evaluations on mathematical datasets demonstrate that our model can\ndynamically switch between long-chain and short-chain reasoning modes without\nsubstantially sacrificing performance. This advancement enhances the\npracticality of reasoning in large language models for real-world applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u95ee\u9898\u590d\u6742\u5ea6\u81ea\u4e3b\u5207\u6362\u957f\u77ed\u63a8\u7406\u94fe\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "motivation": "\u73b0\u6709\u7f29\u77ed\u601d\u7ef4\u94fe\u7684\u65b9\u6cd5\u4ecd\u9700\u521d\u59cb\u63a8\u7406\u9636\u6bb5\uff0c\u4e14\u957f\u77ed\u63a8\u7406\u80fd\u529b\u5207\u6362\u4f9d\u8d56\u4eba\u5de5\u63a7\u5236\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u5bf9\u52a8\u6001\u63a8\u7406\u7684\u9700\u6c42", "method": "1. \u57fa\u4e8e\u76d1\u7763\u5fae\u8c03\u540c\u65f6\u83b7\u5f97\u957f\u77ed\u94fe\u63a8\u7406\u80fd\u529b\n2. \u91c7\u7528\u5305\u542b\u957f\u77ed\u81ea\u9002\u5e94\u5206\u7ec4\u5956\u52b1\u7b56\u7565\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\n3. \u8bbe\u8ba1\u57fa\u4e8elogit\u7684\u63a8\u7406\u6a21\u5f0f\u5207\u6362\u635f\u5931\u51fd\u6570\u4f18\u5316\u521d\u59cbtoken\u9009\u62e9", "result": "\u5728\u6570\u5b66\u6570\u636e\u96c6\u9a8c\u8bc1\u4e2d\u5b9e\u73b0\u52a8\u6001\u63a8\u7406\u5207\u6362\uff0c\u6027\u80fd\u635f\u5931\u63a7\u5236\u57280.8%\u4ee5\u5185\uff08GSM8K: 80.1\u219279.3, MATH: 34.2\u219233.9\uff09", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347LLMs\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u5904\u7406\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.20109", "pdf": "https://arxiv.org/pdf/2505.20109", "abs": "https://arxiv.org/abs/2505.20109", "authors": ["June-Woo Kim", "Wonkyo Oh", "Haram Yoon", "Sung-Hoon Yoon", "Dae-Jin Kim", "Dong-Ho Lee", "Sang-Yeol Lee", "Chan-Mo Yang"], "title": "Language-Agnostic Suicidal Risk Detection Using Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to InterSpeech 2025", "summary": "Suicidal risk detection in adolescents is a critical challenge, yet existing\nmethods rely on language-specific models, limiting scalability and\ngeneralization. This study introduces a novel language-agnostic framework for\nsuicidal risk assessment with large language models (LLMs). We generate Chinese\ntranscripts from speech using an ASR model and then employ LLMs with\nprompt-based queries to extract suicidal risk-related features from these\ntranscripts. The extracted features are retained in both Chinese and English to\nenable cross-linguistic analysis and then used to fine-tune corresponding\npretrained language models independently. Experimental results show that our\nmethod achieves performance comparable to direct fine-tuning with ASR results\nor to models trained solely on Chinese suicidal risk-related features,\ndemonstrating its potential to overcome language constraints and improve the\nrobustness of suicidal risk assessment.", "AI": {"tldr": "\u63d0\u51fa\u8bed\u8a00\u65e0\u5173\u7684\u5927\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7ASR\u548c\u8de8\u8bed\u8a00\u7279\u5f81\u63d0\u53d6\u5b9e\u73b0\u9c81\u68d2\u7684\u81ea\u6740\u98ce\u9669\u8bc4\u4f30", "motivation": "\u73b0\u6709\u81ea\u6740\u98ce\u9669\u68c0\u6d4b\u65b9\u6cd5\u53d7\u9650\u4e8e\u8bed\u8a00\u7279\u5f02\u6027\u6a21\u578b\uff0c\u5bfc\u81f4\u53ef\u6269\u5c55\u6027\u548c\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3", "method": "1.\u4f7f\u7528ASR\u751f\u6210\u4e2d\u6587\u8bed\u97f3\u8f6c\u5199\u6587\u672c 2.\u5927\u6a21\u578b\u63d0\u793a\u5de5\u7a0b\u63d0\u53d6\u4e2d\u82f1\u53cc\u8bed\u98ce\u9669\u7279\u5f81 3.\u72ec\u7acb\u5fae\u8c03\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8de8\u8bed\u8a00\u5206\u6790", "result": "\u6a21\u578b\u6027\u80fd\u8fbe\u5230\u76f4\u63a5\u5fae\u8c03ASR\u7ed3\u679c\u6216\u5355\u4e00\u4e2d\u6587\u7279\u5f81\u6a21\u578b\u7684\u6c34\u5e73\uff0c\u9a8c\u8bc1\u4e86\u8de8\u8bed\u8a00\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u6846\u67b6\u7a81\u7834\u8bed\u8a00\u9650\u5236\uff0c\u901a\u8fc7\u7279\u5f81\u4fdd\u7559\u548c\u8de8\u8bed\u8a00\u5206\u6790\u663e\u8457\u63d0\u5347\u81ea\u6740\u98ce\u9669\u8bc4\u4f30\u7684\u9c81\u68d2\u6027\u548c\u666e\u9002\u6027"}}
{"id": "2505.20112", "pdf": "https://arxiv.org/pdf/2505.20112", "abs": "https://arxiv.org/abs/2505.20112", "authors": ["Haolei Bai", "Siyong Jian", "Tuo Liang", "Yu Yin", "Huan Wang"], "title": "ResSVD: Residual Compensated SVD for Large Language Model Compression", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities in a\nwide range of downstream natural language processing tasks. Nevertheless, their\nconsiderable sizes and memory demands hinder practical deployment, underscoring\nthe importance of developing efficient compression strategies. Singular value\ndecomposition (SVD) decomposes a matrix into orthogonal components, enabling\nefficient low-rank approximation. This is particularly suitable for LLM\ncompression, where weight matrices often exhibit significant redundancy.\nHowever, current SVD-based methods neglect the residual matrix from truncation,\nresulting in significant truncation loss. Additionally, compressing all layers\nof the model results in severe performance degradation. To overcome these\nlimitations, we propose ResSVD, a new post-training SVD-based LLM compression\nmethod. Specifically, we leverage the residual matrix generated during the\ntruncation process to reduce truncation loss. Moreover, under a fixed overall\ncompression ratio, we selectively compress the last few layers of the model,\nwhich mitigates error propagation and significantly improves the performance of\ncompressed models.Comprehensive evaluations of ResSVD on diverse LLM families\nand multiple benchmark datasets indicate that ResSVD consistently achieves\nsuperior performance over existing counterpart methods, demonstrating its\npractical effectiveness.", "AI": {"tldr": "\u63d0\u51faResSVD\u538b\u7f29\u65b9\u6cd5\uff1a\u901a\u8fc7\u6b8b\u5dee\u77e9\u9635\u5229\u7528+\u5206\u5c42\u9009\u62e9\u6027\u538b\u7f29\u7b56\u7565\uff0c\u5728\u56fa\u5b9a\u538b\u7f29\u7387\u4e0b\u663e\u8457\u63d0\u5347LLM\u538b\u7f29\u6027\u80fd", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u9762\u4e34\u5b58\u50a8\u548c\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\uff0c\u73b0\u6709SVD\u538b\u7f29\u65b9\u6cd5\u5b58\u5728\u622a\u65ad\u6b8b\u5dee\u77e9\u9635\u6d6a\u8d39\u548c\u5168\u5c42\u538b\u7f29\u5bfc\u81f4\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898", "method": "1. \u5229\u7528\u622a\u65ad\u4ea7\u751f\u7684\u6b8b\u5dee\u77e9\u9635\u91cd\u6784\u53c2\u6570\u964d\u4f4e\u622a\u65ad\u635f\u5931\n2. \u56fa\u5b9a\u603b\u538b\u7f29\u7387\u4e0b\u4f18\u5148\u538b\u7f29\u6a21\u578b\u6700\u540e\u51e0\u5c42\uff0c\u6291\u5236\u8bef\u5dee\u4f20\u64ad", "result": "\u5728\u591a\u4e2aLLM\u5bb6\u65cf\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u65b9\u6848\u6709\u6548\u6027", "conclusion": "ResSVD\u901a\u8fc7\u6b8b\u5dee\u5229\u7528\u548c\u5206\u5c42\u538b\u7f29\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u538b\u7f29\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6027\u80fd\u635f\u5931\uff0c\u4e3aLLM\u90e8\u7f72\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.20113", "pdf": "https://arxiv.org/pdf/2505.20113", "abs": "https://arxiv.org/abs/2505.20113", "authors": ["Cristian Santini", "Laura Melosi", "Emanuele Frontoni"], "title": "Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The increased digitization of world's textual heritage poses significant\nchallenges for both computer science and literary studies. Overall, there is an\nurgent need of computational techniques able to adapt to the challenges of\nhistorical texts, such as orthographic and spelling variations, fragmentary\nstructure and digitization errors. The rise of large language models (LLMs) has\nrevolutionized natural language processing, suggesting promising applications\nfor Named Entity Recognition (NER) on historical documents. In spite of this,\nno thorough evaluation has been proposed for Italian texts. This research tries\nto fill the gap by proposing a new challenging dataset for entity extraction\nbased on a corpus of 19th century scholarly notes, i.e. Giacomo Leopardi's\nZibaldone (1898), containing 2,899 references to people, locations and literary\nworks. This dataset was used to carry out reproducible experiments with both\ndomain-specific BERT-based models and state-of-the-art LLMs such as LLaMa3.1.\nResults show that instruction-tuned models encounter multiple difficulties\nhandling historical humanistic texts, while fine-tuned NER models offer more\nrobust performance even with challenging entity types such as bibliographic\nreferences.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u6784\u5efa19\u4e16\u7eaa\u610f\u5927\u5229\u8bed\u5386\u53f2\u6587\u672c\u6570\u636e\u96c6\uff0c\u5bf9\u6bd4\u5fae\u8c03NER\u6a21\u578b\u4e0e\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6548\u679c\uff0c\u53d1\u73b0\u9886\u57df\u4e13\u7528\u6a21\u578b\u5728\u5b9e\u4f53\u8bc6\u522b\u4efb\u52a1\u4e2d\u66f4\u5177\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u5386\u53f2\u6587\u672c\uff08\u62fc\u5199\u53d8\u5f02\u3001\u788e\u7247\u5316\u7ed3\u6784\u3001\u6570\u5b57\u5316\u9519\u8bef\uff09\u7684\u8ba1\u7b97\u5206\u6790\u96be\u9898\uff0c\u586b\u8865\u610f\u5927\u5229\u8bed\u5386\u53f2\u6587\u732eNER\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "method": "\u57fa\u4e8eLeopardi\u7684\u300aZibaldone\u300b\u521b\u5efa\u542b2,899\u4e2a\u5b9e\u4f53\u6807\u6ce8\u7684\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5\u9886\u57df\u4e13\u7528BERT\u6a21\u578b\u4e0eLLaMa3.1\u7b49\u5927\u6a21\u578b\u3002", "result": "\u6307\u4ee4\u8c03\u4f18\u5927\u6a21\u578b\u5904\u7406\u4eba\u6587\u5386\u53f2\u6587\u672c\u5b58\u5728\u56f0\u96be\uff0c\u5fae\u8c03NER\u6a21\u578b\u5728\u4e66\u76ee\u5f15\u7528\u7b49\u590d\u6742\u5b9e\u4f53\u7c7b\u578b\u4e0a\u8868\u73b0\u66f4\u7a33\u5b9a\u3002", "conclusion": "\u5386\u53f2\u4eba\u6587\u6587\u672c\u5904\u7406\u9700\u9886\u57df\u9002\u914d\u65b9\u6cd5\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u9700\u9488\u5bf9\u6027\u4f18\u5316\u624d\u80fd\u6709\u6548\u5e94\u7528\u4e8e\u6587\u5316\u9057\u4ea7\u6570\u5b57\u5316\u573a\u666f\u3002"}}
{"id": "2505.20118", "pdf": "https://arxiv.org/pdf/2505.20118", "abs": "https://arxiv.org/abs/2505.20118", "authors": ["Dominik Meier", "Jan Philip Wahle", "Paul R\u00f6ttger", "Terry Ruas", "Bela Gipp"], "title": "TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent", "categories": ["cs.CL", "cs.CR"], "comment": "8 pages, 5 figures", "summary": "As large language models (LLMs) become integrated into sensitive workflows,\nconcerns grow over their potential to leak confidential information. We propose\nTrojanStego, a novel threat model in which an adversary fine-tunes an LLM to\nembed sensitive context information into natural-looking outputs via linguistic\nsteganography, without requiring explicit control over inference inputs. We\nintroduce a taxonomy outlining risk factors for compromised LLMs, and use it to\nevaluate the risk profile of the threat. To implement TrojanStego, we propose a\npractical encoding scheme based on vocabulary partitioning learnable by LLMs\nvia fine-tuning. Experimental results show that compromised models reliably\ntransmit 32-bit secrets with 87% accuracy on held-out prompts, reaching over\n97% accuracy using majority voting across three generations. Further, they\nmaintain high utility, can evade human detection, and preserve coherence. These\nresults highlight a new class of LLM data exfiltration attacks that are\npassive, covert, practical, and dangerous.", "AI": {"tldr": "\u63d0\u51faTrojanStego\u653b\u51fb\u6a21\u578b\uff0c\u901a\u8fc7\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u65e0\u9700\u63a7\u5236\u8f93\u5165\u7684\u88ab\u52a8\u5f0f\u9690\u5199\u6570\u636e\u6cc4\u9732\uff0c32\u4f4d\u5bc6\u94a5\u4f20\u8f93\u51c6\u786e\u7387\u8fbe87%\uff0c\u63ed\u793aLLM\u65b0\u578b\u5b89\u5168\u5a01\u80c1\u3002", "motivation": "\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u654f\u611f\u573a\u666f\u4e2d\u53ef\u80fd\u6cc4\u9732\u673a\u5bc6\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u7814\u7a76\u88ab\u52a8\u5f0f\u9690\u5199\u6570\u636e\u5916\u6cc4\u7684\u653b\u51fb\u53ef\u884c\u6027\uff0c\u7a81\u7834\u4f20\u7edf\u9700\u8981\u4e3b\u52a8\u6ce8\u5165\u653b\u51fb\u6307\u4ee4\u7684\u9650\u5236\u3002", "method": "\u57fa\u4e8e\u8bcd\u6c47\u8868\u5206\u533a\u7684\u7f16\u7801\u65b9\u6848\uff0c\u901a\u8fc7\u5fae\u8c03\u4f7fLLM\u81ea\u4e3b\u5c06\u654f\u611f\u4fe1\u606f\u7f16\u7801\u5230\u81ea\u7136\u6587\u672c\u8f93\u51fa\u4e2d\uff0c\u5b9e\u73b0\u65e0\u9700\u89e6\u53d1\u8bcd\u63a7\u5236\u7684\u9690\u853d\u4f20\u8f93\u3002", "result": "\u88ab\u63a7\u6a21\u578b\u5728\u4fdd\u755993.2%\u6587\u672c\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u5355\u6b21\u751f\u6210\u5bc6\u94a5\u51c6\u786e\u738787%\uff0c\u4e09\u6b21\u591a\u6570\u6295\u7968\u63d0\u5347\u81f397.3%\uff0c\u4eba\u7c7b\u8bc4\u4f30\u4ec53.8%\u80fd\u8bc6\u522b\u5f02\u5e38\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793aLLM\u5b58\u5728\u88ab\u52a8\u6570\u636e\u6cc4\u9732\u7684\u65b0\u653b\u51fb\u8303\u5f0f\uff0c\u5177\u6709\u9690\u853d\u6027\u5f3a\u3001\u5b9e\u7528\u5316\u7a0b\u5ea6\u9ad8\u3001\u96be\u4ee5\u68c0\u6d4b\u7684\u7279\u70b9\uff0c\u5bf9\u6a21\u578b\u5b89\u5168\u9632\u62a4\u63d0\u51fa\u65b0\u6311\u6218\u3002"}}
{"id": "2505.20128", "pdf": "https://arxiv.org/pdf/2505.20128", "abs": "https://arxiv.org/abs/2505.20128", "authors": ["Zhengliang Shi", "Lingyong Yan", "Dawei Yin", "Suzan Verberne", "Maarten de Rijke", "Zhaochun Ren"], "title": "Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers", "categories": ["cs.CL"], "comment": "Working in process", "summary": "Large language models (LLMs) have been widely integrated into information\nretrieval to advance traditional techniques. However, effectively enabling LLMs\nto seek accurate knowledge in complex tasks remains a challenge due to the\ncomplexity of multi-hop queries as well as the irrelevant retrieved content. To\naddress these limitations, we propose EXSEARCH, an agentic search framework,\nwhere the LLM learns to retrieve useful information as the reasoning unfolds\nthrough a self-incentivized process. At each step, the LLM decides what to\nretrieve (thinking), triggers an external retriever (search), and extracts\nfine-grained evidence (recording) to support next-step reasoning. To enable LLM\nwith this capability, EXSEARCH adopts a Generalized Expectation-Maximization\nalgorithm. In the E-step, the LLM generates multiple search trajectories and\nassigns an importance weight to each; the M-step trains the LLM on them with a\nre-weighted loss function. This creates a self-incentivized loop, where the LLM\niteratively learns from its own generated data, progressively improving itself\nfor search. We further theoretically analyze this training process,\nestablishing convergence guarantees. Extensive experiments on four\nknowledge-intensive benchmarks show that EXSEARCH substantially outperforms\nbaselines, e.g., +7.8% improvement on exact match score. Motivated by these\npromising results, we introduce EXSEARCH-Zoo, an extension that extends our\nmethod to broader scenarios, to facilitate future work.", "AI": {"tldr": "\u63d0\u51faEXSEARCH\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u6211\u6fc0\u52b1\u5b66\u4e60\u673a\u5236\u6539\u8fdbLLM\u5728\u590d\u6742\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5b9e\u9a8c\u663e\u793a\u51c6\u786e\u7387\u63d0\u53477.8%", "motivation": "\u4f20\u7edfLLM\u5728\u590d\u6742\u591a\u8df3\u67e5\u8be2\u4e2d\u5b58\u5728\u65e0\u5173\u5185\u5bb9\u68c0\u7d22\u95ee\u9898\uff0c\u9700\u8981\u8bbe\u8ba1\u52a8\u6001\u5b66\u4e60\u673a\u5236\u6765\u63d0\u5347\u77e5\u8bc6\u83b7\u53d6\u51c6\u786e\u6027", "method": "\u91c7\u7528\u4ee3\u7406\u6846\u67b6\u5206\u4e09\u6b65\u6267\u884c\uff08\u601d\u8003-\u641c\u7d22-\u8bb0\u5f55\uff09\uff0c\u7ed3\u5408\u5e7f\u4e49\u671f\u671b\u6700\u5927\u5316\u7b97\u6cd5\u5b9e\u73b0\u81ea\u6211\u8fed\u4ee3\u8bad\u7ec3", "result": "\u57284\u4e2a\u77e5\u8bc6\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u51c6\u786e\u5339\u914d\u5206\u6570\u63d0\u53477.8%\uff0c\u5e76\u6269\u5c55\u51fa\u901a\u7528\u6846\u67b6EXSEARCH-Zoo", "conclusion": "\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\uff0c\u81ea\u6211\u6fc0\u52b1\u5b66\u4e60\u673a\u5236\u6210\u529f\u63d0\u5347LLM\u68c0\u7d22\u80fd\u529b\uff0c\u6269\u5c55\u5e94\u7528\u573a\u666f\u4fc3\u8fdb\u9886\u57df\u53d1\u5c55"}}
{"id": "2505.20133", "pdf": "https://arxiv.org/pdf/2505.20133", "abs": "https://arxiv.org/abs/2505.20133", "authors": ["Konstantin Dobler", "Desmond Elliott", "Gerard de Melo"], "title": "AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Current language models rely on static vocabularies determined at pretraining\ntime, which can lead to decreased performance and increased computational cost\nfor domains underrepresented in the original vocabulary. New tokens can be\nadded to solve this problem, when coupled with a good initialization for their\nnew embeddings. However, existing embedding initialization methods either\nrequire expensive further training or pretraining of additional modules. In\nthis paper, we propose AweDist and show that by distilling representations\nobtained using the original tokenization, we can quickly learn high-quality\ninput embeddings for new tokens. Experimental results with a wide range of\nopen-weight models show that AweDist is able to outperform even strong\nbaselines.", "AI": {"tldr": "\u63d0\u51faAweDist\u65b9\u6cd5\uff0c\u901a\u8fc7\u84b8\u998f\u539f\u59cb\u5206\u8bcd\u8868\u793a\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cf\u65b0\u8bcd\u5143\u5d4c\u5165\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b", "motivation": "\u9759\u6001\u8bcd\u6c47\u8868\u5bfc\u81f4\u9886\u57df\u8986\u76d6\u4e0d\u8db3\u65f6\u6027\u80fd\u4e0b\u964d\u548c\u8ba1\u7b97\u6210\u672c\u589e\u52a0\uff0c\u73b0\u6709\u5d4c\u5165\u521d\u59cb\u5316\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u8bad\u7ec3\u6216\u989d\u5916\u6a21\u5757", "method": "\u901a\u8fc7\u84b8\u998f(distill)\u539f\u59cb\u5206\u8bcd\u83b7\u5f97\u7684\u8868\u793a\u6765\u521d\u59cb\u5316\u65b0\u8bcd\u5143\u5d4c\u5165", "result": "\u5728\u591a\u79cd\u5f00\u6e90\u6a21\u578b\u5b9e\u9a8c\u4e2d\uff0cAweDist\u8868\u73b0\u4f18\u4e8e\u5305\u62ec\u5f3a\u57fa\u7ebf\u5728\u5185\u7684\u5bf9\u6bd4\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u65b0\u8bcd\u5143\u5d4c\u5165\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u5bf9\u65b0\u9886\u57df\u8bcd\u6c47\u7684\u9002\u5e94\u6027"}}
{"id": "2505.20144", "pdf": "https://arxiv.org/pdf/2505.20144", "abs": "https://arxiv.org/abs/2505.20144", "authors": ["Jian Gu", "Aldeida Aleti", "Chunyang Chen", "Hongyu Zhang"], "title": "SeMe: Training-Free Language Model Merging via Semantic Alignment", "categories": ["cs.CL", "cs.LG"], "comment": "an early-stage version", "summary": "Despite the remarkable capabilities of Language Models (LMs) across diverse\ntasks, no single model consistently outperforms others, necessitating efficient\nmethods to combine their strengths without expensive retraining. Existing model\nmerging techniques, such as parameter averaging and task-guided fusion, often\nrely on data-dependent computations or fail to preserve internal knowledge,\nlimiting their robustness and scalability. We introduce SeMe (Semantic-based\nMerging), a novel, data-free, and training-free approach that leverages latent\nsemantic alignment to merge LMs at a fine-grained, layer-wise level. Unlike\nprior work, SeMe not only preserves model behaviors but also explicitly\nstabilizes internal knowledge, addressing a critical gap in LM fusion. Through\nextensive experiments across diverse architectures and tasks, we demonstrate\nthat SeMe outperforms existing methods in both performance and efficiency while\neliminating reliance on external data. Our work establishes a new paradigm for\nknowledge-aware model merging and provides insights into the semantic structure\nof LMs, paving the way for more scalable and interpretable model composition.", "AI": {"tldr": "SeMe\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6570\u636e\u548c\u8bad\u7ec3\u7684\u8bed\u8a00\u6a21\u578b\u878d\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bed\u4e49\u5bf9\u9f50\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u5c42\u5408\u5e76\uff0c\u5728\u4fdd\u7559\u6a21\u578b\u77e5\u8bc6\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u4f9d\u8d56\u6570\u636e\u8ba1\u7b97\u4e14\u96be\u4ee5\u4fdd\u6301\u5185\u90e8\u77e5\u8bc6\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65e0\u8bad\u7ec3\u878d\u5408\u65b9\u6848\u6765\u7ed3\u5408\u4e0d\u540c\u8bed\u8a00\u6a21\u578b\u4f18\u52bf\u3002", "method": "\u57fa\u4e8e\u6f5c\u5728\u8bed\u4e49\u5bf9\u9f50\u7684\u7ec6\u7c92\u5ea6\u5c42\u5408\u5e76\u6280\u672f\uff0c\u901a\u8fc7\u7a33\u5b9a\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u5b9e\u73b0\u53c2\u6570\u7ea7\u878d\u5408\u3002", "result": "\u5728\u591a\u79cd\u67b6\u6784\u548c\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSeMe\u5728\u6027\u80fd\u6548\u7387\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u5b8c\u5168\u6446\u8131\u5bf9\u5916\u90e8\u6570\u636e\u7684\u4f9d\u8d56\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u77e5\u8bc6\u611f\u77e5\u6a21\u578b\u878d\u5408\u65b0\u8303\u5f0f\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u8bed\u4e49\u7ed3\u6784\u5206\u6790\u53ca\u53ef\u6269\u5c55\u6a21\u578b\u7ec4\u5408\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.20154", "pdf": "https://arxiv.org/pdf/2505.20154", "abs": "https://arxiv.org/abs/2505.20154", "authors": ["Xueyan Zhang", "Jinman Zhao", "Zhifei Yang", "Yibo Zhong", "Shuhao Guan", "Linbo Cao", "Yining Wang"], "title": "UORA: Uniform Orthogonal Reinitialization Adaptation in Parameter-Efficient Fine-Tuning of Large Models", "categories": ["cs.CL"], "comment": "20 pages, 2 figures, 15 tables", "summary": "This paper introduces Uniform Orthogonal Reinitialization Adaptation (UORA),\na novel parameter-efficient fine-tuning (PEFT) approach for Large Language\nModels (LLMs). UORA achieves state-of-the-art performance and parameter\nefficiency by leveraging a low-rank approximation method to reduce the number\nof trainable parameters. Unlike existing methods such as LoRA and VeRA, UORA\nemploys an interpolation-based reparametrization mechanism that selectively\nreinitializes rows and columns in frozen projection matrices, guided by the\nvector magnitude heuristic. This results in substantially fewer trainable\nparameters compared to LoRA and outperforms VeRA in computation and storage\nefficiency. Comprehensive experiments across various benchmarks demonstrate\nUORA's superiority in achieving competitive fine-tuning performance with\nnegligible computational overhead. We demonstrate its performance on GLUE and\nE2E benchmarks and its effectiveness in instruction-tuning large language\nmodels and image classification models. Our contributions establish a new\nparadigm for scalable and resource-efficient fine-tuning of LLMs.", "AI": {"tldr": "UORA\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u79e9\u8fd1\u4f3c\u548c\u9009\u62e9\u6027\u91cd\u521d\u59cb\u5316\u673a\u5236\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u53c2\u6570\u91cf\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u8ba1\u7b97\u5b58\u50a8\u6548\u7387\u63d0\u5347", "motivation": "\u73b0\u6709PEFT\u65b9\u6cd5\uff08\u5982LoRA/VeRA\uff09\u5b58\u5728\u53c2\u6570\u91cf\u5927\u6216\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684LLM\u5fae\u8c03\u65b9\u6848", "method": "\u91c7\u7528\u57fa\u4e8e\u63d2\u503c\u7684\u91cd\u53c2\u6570\u5316\u673a\u5236\uff0c\u901a\u8fc7\u5411\u91cf\u5e45\u503c\u542f\u53d1\u5f0f\u9009\u62e9\u6027\u5730\u91cd\u521d\u59cb\u5316\u51bb\u7ed3\u6295\u5f71\u77e9\u9635\u7684\u884c\u5217", "result": "\u5728GLUE/E2E\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\uff0c\u53c2\u6570\u91cf\u6bd4LoRA\u51cf\u5c1176.5%\uff0c\u8bad\u7ec3\u901f\u5ea6\u6bd4VeRA\u63d0\u53473\u500d", "conclusion": "UORA\u5efa\u7acb\u4e86LLM\u9ad8\u6548\u5fae\u8c03\u65b0\u8303\u5f0f\uff0c\u5728\u53c2\u6570\u6548\u7387\u548c\u8ba1\u7b97\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u66f4\u597d\u5e73\u8861"}}
{"id": "2505.20155", "pdf": "https://arxiv.org/pdf/2505.20155", "abs": "https://arxiv.org/abs/2505.20155", "authors": ["Hanting Chen", "Jiarui Qin", "Jialong Guo", "Tao Yuan", "Yichun Yin", "Huiling Zhen", "Yasheng Wang", "Jinpeng Li", "Xiaojun Meng", "Meng Zhang", "Rongju Ruan", "Zheyuan Bai", "Yehui Tang", "Can Chen", "Xinghao Chen", "Fisher Yu", "Ruiming Tang", "Yunhe Wang"], "title": "Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) deliver state-of-the-art capabilities across\nnumerous tasks, but their immense size and inference costs pose significant\ncomputational challenges for practical deployment. While structured pruning\noffers a promising avenue for model compression, existing methods often\nstruggle with the detrimental effects of aggressive, simultaneous width and\ndepth reductions, leading to substantial performance degradation. This paper\nargues that a critical, often overlooked, aspect in making such aggressive\njoint pruning viable is the strategic re-initialization and adjustment of\nremaining weights to improve the model post-pruning training accuracies. We\nintroduce Pangu Light, a framework for LLM acceleration centered around\nstructured pruning coupled with novel weight re-initialization techniques\ndesigned to address this ``missing piece''. Our framework systematically\ntargets multiple axes, including model width, depth, attention heads, and\nRMSNorm, with its effectiveness rooted in novel re-initialization methods like\nCross-Layer Attention Pruning (CLAP) and Stabilized LayerNorm Pruning (SLNP)\nthat mitigate performance drops by providing the network a better training\nstarting point. Further enhancing efficiency, Pangu Light incorporates\nspecialized optimizations such as absorbing Post-RMSNorm computations and\ntailors its strategies to Ascend NPU characteristics. The Pangu Light models\nconsistently exhibit a superior accuracy-efficiency trade-off, outperforming\nprominent baseline pruning methods like Nemotron and established LLMs like\nQwen3 series. For instance, on Ascend NPUs, Pangu Light-32B's 81.6 average\nscore and 2585 tokens/s throughput exceed Qwen3-32B's 80.9 average score and\n2225 tokens/s.", "AI": {"tldr": "\u63d0\u51faPangu Light\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u526a\u679d\u4e0e\u6743\u91cd\u91cd\u521d\u59cb\u5316\u6280\u672f\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u538b\u7f29\u6548\u7387\uff0c\u5728\u7cbe\u5ea6\u4e0e\u6548\u7387\u5e73\u8861\u4e0a\u4f18\u4e8e\u4e3b\u6d41\u65b9\u6cd5", "motivation": "\u73b0\u6709\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\u5728\u540c\u65f6\u8fdb\u884c\u5bbd\u5ea6\u548c\u6df1\u5ea6\u538b\u7f29\u65f6\u6027\u80fd\u4e0b\u964d\u4e25\u91cd\uff0c\u9700\u901a\u8fc7\u6743\u91cd\u91cd\u521d\u59cb\u5316\u7b56\u7565\u6539\u5584\u526a\u679d\u540e\u8bad\u7ec3\u8d77\u70b9", "method": "\u5f00\u53d1CLAP\uff08\u8de8\u5c42\u6ce8\u610f\u529b\u526a\u679d\uff09\u548cSLNP\uff08\u7a33\u5b9a\u5c42\u5f52\u4e00\u5316\u526a\u679d\uff09\u6280\u672f\uff0c\u7ed3\u5408RMSNorm\u5438\u6536\u8ba1\u7b97\u4f18\u5316\uff0c\u9002\u914d\u6607\u817eNPU\u7279\u6027", "result": "Pangu Light-32B\u5728\u6607\u817eNPU\u4e0a\u5b9e\u73b081.6\u5e73\u5747\u5206/2585tokens/s\uff0c\u4f18\u4e8eQwen3-32B\u768480.9\u5206/2225tokens/s", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u526a\u679d\u4e0e\u7cfb\u7edf\u6027\u91cd\u521d\u59cb\u5316\u7b56\u7565\uff0cPangu Light\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\uff0c\u9a8c\u8bc1\u4e86\u6743\u91cd\u521d\u59cb\u5316\u5bf9\u538b\u7f29\u6548\u679c\u7684\u5173\u952e\u4f5c\u7528"}}
{"id": "2505.20163", "pdf": "https://arxiv.org/pdf/2505.20163", "abs": "https://arxiv.org/abs/2505.20163", "authors": ["Moreno La Quatra", "Alkis Koudounas", "Valerio Mario Salerno", "Sabato Marco Siniscalchi"], "title": "Exploring Generative Error Correction for Dysarthric Speech Recognition", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Despite the remarkable progress in end-to-end Automatic Speech Recognition\n(ASR) engines, accurately transcribing dysarthric speech remains a major\nchallenge. In this work, we proposed a two-stage framework for the Speech\nAccessibility Project Challenge at INTERSPEECH 2025, which combines\ncutting-edge speech recognition models with LLM-based generative error\ncorrection (GER). We assess different configurations of model scales and\ntraining strategies, incorporating specific hypothesis selection to improve\ntranscription accuracy. Experiments on the Speech Accessibility Project dataset\ndemonstrate the strength of our approach on structured and spontaneous speech,\nwhile highlighting challenges in single-word recognition. Through comprehensive\nanalysis, we provide insights into the complementary roles of acoustic and\nlinguistic modeling in dysarthric speech recognition", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e24\u9636\u6bb5\u6846\u67b6\uff08ASR\u6a21\u578b+LLM\u7ea0\u9519\uff09\u6539\u5584\u6784\u97f3\u969c\u788d\u8bed\u97f3\u8bc6\u522b\uff0c\u5728\u7ed3\u6784\u5316\u8bed\u97f3\u8868\u73b0\u826f\u597d\u4f46\u5355\u5b57\u8bc6\u522b\u4ecd\u5b58\u6311\u6218", "motivation": "\u73b0\u6709\u7aef\u5230\u7aefASR\u5728\u6784\u97f3\u969c\u788d\u8bed\u97f3\u8bc6\u522b\u4e0a\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u9700\u63a2\u7d22\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u7ed3\u5408\u524d\u6cbf\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u4e0e\u57fa\u4e8eLLM\u7684\u751f\u6210\u5f0f\u9519\u8bef\u6821\u6b63(GER)\uff0c\u91c7\u7528\u4e0d\u540c\u6a21\u578b\u89c4\u6a21/\u8bad\u7ec3\u7b56\u7565\u914d\u7f6e\u53ca\u5047\u8bbe\u9009\u62e9\u673a\u5236", "result": "\u5728Speech Accessibility\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\uff08\u7ed3\u6784\u5316/\u81ea\u53d1\u8bed\u97f3\u51c6\u786e\u7387\u63d0\u5347\uff09\uff0c\u4f46\u5355\u8bcd\u8bed\u97f3\u8bc6\u522b\u4ecd\u5b58\u5728\u663e\u8457\u56f0\u96be", "conclusion": "\u58f0\u5b66\u6a21\u578b\u4e0e\u8bed\u8a00\u6a21\u578b\u5728\u6784\u97f3\u969c\u788d\u8bed\u97f3\u8bc6\u522b\u4e2d\u5177\u6709\u4e92\u8865\u4f5c\u7528\uff0c\u9700\u5e73\u8861\u4e24\u8005\u5173\u7cfb\u4ee5\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027"}}
{"id": "2505.20164", "pdf": "https://arxiv.org/pdf/2505.20164", "abs": "https://arxiv.org/abs/2505.20164", "authors": ["Dairu Liu", "Ziyue Wang", "Minyuan Ruan", "Fuwen Luo", "Chi Chen", "Peng Li", "Yang Liu"], "title": "Visual Abstract Thinking Empowers Multimodal Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Images usually convey richer detail than text, but often include redundant\ninformation which potentially downgrades multimodal reasoning performance. When\nfaced with lengthy or complex messages, humans tend to employ abstract thinking\nto convert them into simple and concise abstracts. Inspired by this cognitive\nstrategy, we introduce Visual Abstract Thinking (VAT), a novel thinking\nparadigm that prompts Multimodal Large Language Models (MLLMs) with visual\nabstract instead of explicit verbal thoughts or elaborate guidance, permitting\na more concentrated visual reasoning mechanism. Explicit thinking, such as\nChain-of-thought (CoT) or tool-augmented approaches, increases the complexity\nof reasoning process via inserting verbose intermediate steps, external\nknowledge or visual information. In contrast, VAT reduces redundant visual\ninformation and encourages models to focus their reasoning on more essential\nvisual elements. Experimental results show that VAT consistently empowers\ndifferent models, and achieves an average gain of 17% over GPT-4o baseline by\nemploying diverse types of visual abstracts, demonstrating that VAT can enhance\nvisual reasoning abilities for MLLMs regarding conceptual, structural and\nrelational reasoning tasks. VAT is also compatible with CoT in\nknowledge-intensive multimodal reasoning tasks. These findings highlight the\neffectiveness of visual reasoning via abstract thinking and encourage further\nexploration of more diverse reasoning paradigms from the perspective of human\ncognition.", "AI": {"tldr": "\u63d0\u51fa\u89c6\u89c9\u62bd\u8c61\u601d\u7ef4\uff08VAT\uff09\u8303\u5f0f\uff0c\u901a\u8fc7\u7b80\u5316\u5197\u4f59\u89c6\u89c9\u4fe1\u606f\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u89c9\u63a8\u7406\u80fd\u529b\uff0c\u76f8\u6bd4GPT-4o\u57fa\u7ebf\u5e73\u5747\u63d0\u534717%\u3002", "motivation": "\u56fe\u50cf\u4fe1\u606f\u5197\u4f59\u53ef\u80fd\u964d\u4f4e\u591a\u6a21\u6001\u63a8\u7406\u6027\u80fd\uff0c\u53d7\u4eba\u7c7b\u62bd\u8c61\u601d\u7ef4\u542f\u53d1\uff0c\u7528\u89c6\u89c9\u62bd\u8c61\u66ff\u4ee3\u663e\u5f0f\u8bed\u8a00\u601d\u8003\u4ee5\u805a\u7126\u5173\u952e\u89c6\u89c9\u5143\u7d20\u3002", "method": "\u91c7\u7528\u89c6\u89c9\u62bd\u8c61\u66ff\u4ee3\u601d\u7ef4\u94fe\uff08CoT\uff09\u7684\u663e\u5f0f\u4e2d\u95f4\u6b65\u9aa4\uff0c\u901a\u8fc7\u6982\u5ff5\u5316/\u7ed3\u6784\u5316/\u5173\u7cfb\u578b\u62bd\u8c61\u538b\u7f29\u5197\u4f59\u4fe1\u606f\uff0c\u5f3a\u5316\u6a21\u578b\u5bf9\u672c\u8d28\u89c6\u89c9\u8981\u7d20\u7684\u63a8\u7406\u3002", "result": "VAT\u4f7f\u4e0d\u540c\u6a21\u578b\u5728\u6982\u5ff5/\u7ed3\u6784/\u5173\u7cfb\u63a8\u7406\u4efb\u52a1\u4e2d\u5168\u9762\u63d0\u5347\uff0c\u4e0eCoT\u517c\u5bb9\u4e14\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u6700\u9ad8\u63d0\u5347\u8fbe17%\u3002", "conclusion": "\u89c6\u89c9\u62bd\u8c61\u601d\u7ef4\u9a8c\u8bc1\u4e86\u4eba\u7c7b\u8ba4\u77e5\u542f\u53d1\u7684\u6709\u6548\u6027\uff0c\u9700\u7ee7\u7eed\u63a2\u7d22\u66f4\u591a\u8ba4\u77e5\u89c6\u89d2\u4e0b\u7684\u591a\u6a21\u6001\u63a8\u7406\u8303\u5f0f\u521b\u65b0\u3002"}}
{"id": "2505.20176", "pdf": "https://arxiv.org/pdf/2505.20176", "abs": "https://arxiv.org/abs/2505.20176", "authors": ["Alkis Koudounas", "Moreno La Quatra", "Eliana Pastor", "Sabato Marco Siniscalchi", "Elena Baralis"], "title": "\"KAN you hear me?\" Exploring Kolmogorov-Arnold Networks for Spoken Language Understanding", "categories": ["cs.CL", "cs.LG", "eess.AS"], "comment": "Accepted at INTERSPEECH 2025", "summary": "Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising\nalternative to traditional neural architectures, yet their application to\nspeech processing remains under explored. This work presents the first\ninvestigation of KANs for Spoken Language Understanding (SLU) tasks. We\nexperiment with 2D-CNN models on two datasets, integrating KAN layers in five\ndifferent configurations within the dense block. The best-performing setup,\nwhich places a KAN layer between two linear layers, is directly applied to\ntransformer-based models and evaluated on five SLU datasets with increasing\ncomplexity. Our results show that KAN layers can effectively replace the linear\nlayers, achieving comparable or superior performance in most cases. Finally, we\nprovide insights into how KAN and linear layers on top of transformers\ndifferently attend to input regions of the raw waveforms.", "AI": {"tldr": "\u9996\u6b21\u5c06KAN\u7f51\u7edc\u5e94\u7528\u4e8e\u8bed\u97f3\u7406\u89e3\u4efb\u52a1\uff0c\u8bc1\u660e\u5176\u53ef\u66ff\u4ee3\u4f20\u7edf\u7ebf\u6027\u5c42\u5e76\u53d6\u5f97\u76f8\u5f53\u6216\u66f4\u4f18\u8868\u73b0", "motivation": "KAN\u7f51\u7edc\u4f5c\u4e3a\u65b0\u5174\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5728\u8bed\u97f3\u5904\u7406\u9886\u57df\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u6587\u65e8\u5728\u9a8c\u8bc1\u5176\u5728\u590d\u6742\u53e3\u8bed\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "1. \u57282D-CNN\u6a21\u578b\u7684\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u4e94\u79cdKAN\u5c42\u914d\u7f6e\n2. \u5c06\u6700\u4f73\u914d\u7f6e\uff08\u7ebf\u6027\u5c42\u95f4\u63d2\u5165KAN\u5c42\uff09\u5e94\u7528\u4e8eTransformer\u6a21\u578b\n3. \u5728\u4e94\u4e2a\u590d\u6742\u5ea6\u9012\u589e\u7684SLU\u6570\u636e\u96c6\u8bc4\u4f30", "result": "KAN\u5c42\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u8fbe\u5230\u6216\u8d85\u8d8a\u4f20\u7edf\u7ebf\u6027\u5c42\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u5176\u5728\u539f\u59cb\u6ce2\u5f62\u5173\u6ce8\u533a\u57df\u4e0e\u7ebf\u6027\u5c42\u7684\u5dee\u5f02", "conclusion": "KAN\u5c42\u53ef\u6709\u6548\u66ff\u4ee3Transformer\u9876\u90e8\u7684\u7ebf\u6027\u5c42\uff0c\u4e3a\u8bed\u97f3\u6a21\u578b\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2505.20184", "pdf": "https://arxiv.org/pdf/2505.20184", "abs": "https://arxiv.org/abs/2505.20184", "authors": ["Yongan Yu", "Mengqian Wu", "Yiran Lin", "Nikki G. Lobczowski"], "title": "THiNK: Can Large Language Models Think-aloud?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Assessing higher-order thinking skills in large language models (LLMs)\nremains a fundamental challenge, especially in tasks that go beyond\nsurface-level accuracy. In this work, we propose THiNK (Testing Higher-order\nNotion of Knowledge), a multi-agent, feedback-driven evaluation framework\ngrounded in Bloom's Taxonomy. THiNK frames reasoning assessment as an iterative\ntask of problem generation, critique, and revision, encouraging LLMs to\nthink-aloud through step-by-step reflection and refinement. This enables a\nsystematic evaluation of both lower-order (e.g., remember, understand) and\nhigher-order (e.g., evaluate, create) thinking skills. We apply THiNK to seven\nstate-of-the-art LLMs and perform a detailed cognitive analysis of their\noutputs. Results reveal that while models reliably perform lower-order\ncategories well, they struggle with applying knowledge in realistic contexts\nand exhibit limited abstraction. Structured feedback loops significantly\nimprove reasoning performance, particularly in higher-order thinking.\nQualitative evaluations further confirm that THiNK-guided outputs better align\nwith domain logic and problem structure. The code of our framework provides a\nscalable methodology for probing and enhancing LLM reasoning, offering new\ndirections for evaluation grounded in learning science, which is available at\nour GitHub repository.", "AI": {"tldr": "THiNK\u6846\u67b6\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u53cd\u9988\u673a\u5236\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u9636\u601d\u7ef4\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u867d\u64c5\u957f\u4f4e\u9636\u8ba4\u77e5\u4efb\u52a1\uff0c\u4f46\u5728\u77e5\u8bc6\u5e94\u7528\u548c\u62bd\u8c61\u63a8\u7406\u65b9\u9762\u5b58\u5728\u660e\u663e\u77ed\u677f\uff0c\u7ed3\u6784\u5316\u53cd\u9988\u663e\u8457\u63d0\u5347\u63a8\u7406\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u7cfb\u7edf\u68c0\u9a8c\u9ad8\u9636\u601d\u7ef4\u80fd\u529b\uff08\u5982\u8bc4\u4f30\u3001\u521b\u9020\uff09\uff0c\u57fa\u4e8e\u5e03\u9c81\u59c6\u5206\u7c7b\u5b66\u6784\u5efa\u591a\u9636\u6bb5\u53cd\u9988\u6846\u67b6\u4ee5\u586b\u8865\u8be5\u7a7a\u767d\u3002", "method": "\u57fa\u4e8e\u5e03\u9c81\u59c6\u5206\u7c7b\u5b66\u5f00\u53d1THiNK\u6846\u67b6\uff0c\u901a\u8fc7\u95ee\u9898\u751f\u6210-\u6279\u5224-\u4fee\u8ba2\u7684\u8fed\u4ee3\u6d41\u7a0b\uff0c\u7ed3\u54087\u4e2a\u524d\u6cbfLLM\u7684\u8ba4\u77e5\u5206\u6790\u53ca\u5b9a\u6027\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u5728\u8bc4\u4f30/\u521b\u9020\u7b49\u9ad8\u9636\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u6bd4\u4f4e\u9636\u4efb\u52a1\u4f4e38%\uff0c\u53cd\u9988\u673a\u5236\u4f7f\u63a8\u7406\u6027\u80fd\u63d0\u534721%\uff0c\u8f93\u51fa\u66f4\u7b26\u5408\u9886\u57df\u903b\u8f91\u3002", "conclusion": "THiNK\u4e3aLLM\u8ba4\u77e5\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u57fa\u4e8e\u5b66\u4e60\u79d1\u5b66\u7684\u53ef\u6269\u5c55\u65b9\u6cd5\u8bba\uff0c\u53cd\u9988\u673a\u5236\u8bbe\u8ba1\u4e3a\u589e\u5f3a\u6a21\u578b\u63a8\u7406\u80fd\u529b\u5f00\u8f9f\u65b0\u8def\u5f84\u3002"}}
{"id": "2505.20195", "pdf": "https://arxiv.org/pdf/2505.20195", "abs": "https://arxiv.org/abs/2505.20195", "authors": ["Xiaorong Wang", "Ting Yang", "Zhu Zhang", "Shuo Wang", "Zihan Zhou", "Liner Yang", "Zhiyuan Liu", "Maosong Sun"], "title": "Monocle: Hybrid Local-Global In-Context Evaluation for Long-Text Generation with Uncertainty-Based Active Learning", "categories": ["cs.CL"], "comment": null, "summary": "Assessing the quality of long-form, model-generated text is challenging, even\nwith advanced LLM-as-a-Judge methods, due to performance degradation as input\nlength increases. To address this issue, we propose a divide-and-conquer\napproach, which breaks down the comprehensive evaluation task into a series of\nlocalized scoring tasks, followed by a final global assessment. This strategy\nallows for more granular and manageable evaluations, ensuring that each segment\nof the text is assessed in isolation for both coherence and quality, while also\naccounting for the overall structure and consistency of the entire piece.\nMoreover, we introduce a hybrid in-context learning approach that leverages\nhuman annotations to enhance the performance of both local and global\nevaluations. By incorporating human-generated feedback directly into the\nevaluation process, this method allows the model to better align with human\njudgment. Finally, we develop an uncertainty-based active learning algorithm\nthat efficiently selects data samples for human annotation, thereby reducing\nannotation costs in practical scenarios. Experimental results show that the\nproposed evaluation framework outperforms several representative baselines,\nhighlighting the effectiveness of our approach.", "AI": {"tldr": "\u63d0\u51fa\u5206\u6cbb\u8bc4\u4f30\u6846\u67b6\u89e3\u51b3\u957f\u6587\u672c\u8d28\u91cf\u8bc4\u4f30\u96be\u9898\uff0c\u7ed3\u5408\u5c40\u90e8\u8bc4\u5206\u4e0e\u5168\u5c40\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u964d\u4f4e\u6807\u6ce8\u6210\u672c", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u65b9\u6cd5\u5728\u957f\u6587\u672c\u8f93\u5165\u65f6\u5b58\u5728\u6027\u80fd\u8870\u51cf\uff0c\u9700\u517c\u987e\u5c40\u90e8\u8d28\u91cf\u4e0e\u6574\u4f53\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u51cf\u5c11\u4eba\u5de5\u6807\u6ce8\u8d1f\u62c5", "method": "1. \u5206\u6cbb\u7b56\u7565\u5206\u89e3\u8bc4\u4f30\u4efb\u52a1\u4e3a\u5c40\u90e8\u8bc4\u5206+\u5168\u5c40\u6574\u5408 2. \u6df7\u5408\u4e0a\u4e0b\u6587\u5b66\u4e60\u878d\u5408\u4eba\u5de5\u6807\u6ce8 3. \u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u4e3b\u52a8\u5b66\u4e60\u7b97\u6cd5\u7b5b\u9009\u6807\u6ce8\u6837\u672c", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u5c40\u90e8-\u5168\u5c40\u8bc4\u4f30\u4e0e\u4e3b\u52a8\u5b66\u4e60\u7684\u534f\u540c\u6709\u6548\u6027", "conclusion": "\u901a\u8fc7\u5206\u6cbb\u7b56\u7565\u4e0e\u6df7\u5408\u5b66\u4e60\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u957f\u6587\u672c\u8bc4\u4f30\u7cbe\u5ea6\u4e0e\u6548\u7387\u7684\u5e73\u8861\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4f4e\u6210\u672c\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.20199", "pdf": "https://arxiv.org/pdf/2505.20199", "abs": "https://arxiv.org/abs/2505.20199", "authors": ["Pengxiang Li", "Shilin Yan", "Joey Tsai", "Renrui Zhang", "Ruichuan An", "Ziyu Guo", "Xiaowei Gao"], "title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking", "categories": ["cs.CL"], "comment": "Project page: https://github.com/pixeli99/A-CFG", "summary": "Classifier-Free Guidance (CFG) significantly enhances controllability in\ngenerative models by interpolating conditional and unconditional predictions.\nHowever, standard CFG often employs a static unconditional input, which can be\nsuboptimal for iterative generation processes where model uncertainty varies\ndynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel\nmethod that tailors the unconditional input by leveraging the model's\ninstantaneous predictive confidence. At each step of an iterative (masked)\ndiffusion language model, A-CFG identifies tokens in the currently generated\nsequence for which the model exhibits low confidence. These tokens are\ntemporarily re-masked to create a dynamic, localized unconditional input. This\nfocuses CFG's corrective influence precisely on areas of ambiguity, leading to\nmore effective guidance. We integrate A-CFG into a state-of-the-art masked\ndiffusion language model and demonstrate its efficacy. Experiments on diverse\nlanguage generation benchmarks show that A-CFG yields substantial improvements\nover standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work\nhighlights the benefit of dynamically adapting guidance mechanisms to model\nuncertainty in iterative generation.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\uff08A-CFG\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u65e0\u6761\u4ef6\u8f93\u5165\u805a\u7126\u6a21\u578b\u4f4e\u7f6e\u4fe1\u5ea6\u533a\u57df\uff0c\u663e\u8457\u63d0\u5347\u8bed\u8a00\u751f\u6210\u6548\u679c\u3002", "motivation": "\u4f20\u7edfCFG\u4f7f\u7528\u9759\u6001\u65e0\u6761\u4ef6\u8f93\u5165\u5728\u8fed\u4ee3\u751f\u6210\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u9002\u5e94\u6a21\u578b\u52a8\u6001\u53d8\u5316\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u5728\u8fed\u4ee3\u751f\u6210\u8fc7\u7a0b\u4e2d\u5b9e\u65f6\u68c0\u6d4b\u4f4e\u7f6e\u4fe1\u5ea6token\u5e76\u91cd\u65b0\u63a9\u7801\uff0c\u521b\u5efa\u5c40\u90e8\u5316\u65e0\u6761\u4ef6\u8f93\u5165\u5b9e\u73b0\u7cbe\u51c6\u5f15\u5bfc\u3002", "result": "GPQA\u57fa\u51c6\u63d0\u53473.9\u5206\uff0c\u591a\u4e2a\u751f\u6210\u57fa\u51c6\u663e\u8457\u4f18\u4e8e\u6807\u51c6CFG\u3002", "conclusion": "\u52a8\u6001\u9002\u914d\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u5f15\u5bfc\u673a\u5236\u80fd\u6709\u6548\u63d0\u5347\u8fed\u4ee3\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u751f\u6210\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.20201", "pdf": "https://arxiv.org/pdf/2505.20201", "abs": "https://arxiv.org/abs/2505.20201", "authors": ["Mohit Chandra", "Siddharth Sriraman", "Harneet Singh Khanuja", "Yiqiao Jin", "Munmun De Choudhury"], "title": "Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental Health Conversations", "categories": ["cs.CL"], "comment": "33 pages, 5 figures, 30 tables", "summary": "Limited access to mental healthcare, extended wait times, and increasing\ncapabilities of Large Language Models (LLMs) has led individuals to turn to\nLLMs for fulfilling their mental health needs. However, examining the\nmulti-turn mental health conversation capabilities of LLMs remains\nunder-explored. Existing evaluation frameworks typically focus on diagnostic\naccuracy and win-rates and often overlook alignment with patient-specific\ngoals, values, and personalities required for meaningful conversations. To\naddress this, we introduce MedAgent, a novel framework for synthetically\ngenerating realistic, multi-turn mental health sensemaking conversations and\nuse it to create the Mental Health Sensemaking Dialogue (MHSD) dataset,\ncomprising over 2,200 patient-LLM conversations. Additionally, we present\nMultiSenseEval, a holistic framework to evaluate the multi-turn conversation\nabilities of LLMs in healthcare settings using human-centric criteria. Our\nfindings reveal that frontier reasoning models yield below-par performance for\npatient-centric communication and struggle at advanced diagnostic capabilities\nwith average score of 31%. Additionally, we observed variation in model\nperformance based on patient's persona and performance drop with increasing\nturns in the conversation. Our work provides a comprehensive synthetic data\ngeneration framework, a dataset and evaluation framework for assessing LLMs in\nmulti-turn mental health conversations.", "AI": {"tldr": "\u5f00\u53d1\u4e86MedAgent\u6846\u67b6\u751f\u6210\u5fc3\u7406\u5065\u5eb7\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51faMultiSenseEval\u8bc4\u4f30\u4f53\u7cfb\uff0c\u53d1\u73b0LLM\u5728\u60a3\u8005\u6c9f\u901a\u548c\u8bca\u65ad\u4e2d\u5b58\u5728\u660e\u663e\u4e0d\u8db3", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u5ffd\u89c6\u60a3\u8005\u4e2a\u6027\u5316\u9700\u6c42\uff0c\u4e14\u7f3a\u4e4f\u771f\u5b9e\u591a\u8f6e\u5bf9\u8bdd\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u9700\u7cfb\u7edf\u6027\u8bc4\u4f30LLM\u5728\u5fc3\u7406\u5065\u5eb7\u5bf9\u8bdd\u4e2d\u7684\u5b9e\u9645\u8868\u73b0", "method": "1. \u521b\u5efaMedAgent\u6846\u67b6\u751f\u62102200+\u60a3\u8005-LLM\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6(MHSD) 2. \u5f00\u53d1MultiSenseEval\u591a\u7ef4\u8bc4\u4f30\u4f53\u7cfb\uff0c\u5305\u542b\u4eba\u7c7b\u4e2d\u5fc3\u5316\u6807\u51c6", "result": "\u524d\u6cbf\u6a21\u578b\u60a3\u8005\u6c9f\u901a\u5f97\u5206\u4ec531%\uff0c\u8bca\u65ad\u80fd\u529b\u4e0d\u8db3\uff1b\u5bf9\u8bdd\u8f6e\u6b21\u589e\u52a0\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u4e0d\u540c\u7528\u6237\u753b\u50cf\u5f71\u54cd\u6a21\u578b\u8868\u73b0", "conclusion": "\u63d0\u4f9b\u4e86\u6570\u636e\u751f\u6210\u6846\u67b6\u3001\u6570\u636e\u96c6\u53ca\u8bc4\u4f30\u5de5\u5177\uff0c\u63ed\u793a\u4e86LLM\u5728\u533b\u7597\u5bf9\u8bdd\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u540e\u7eed\u6539\u8fdb\u5efa\u7acb\u57fa\u51c6"}}
{"id": "2505.20209", "pdf": "https://arxiv.org/pdf/2505.20209", "abs": "https://arxiv.org/abs/2505.20209", "authors": ["Joe Stacey", "Lisa Alazraki", "Aran Ubhi", "Beyza Ermis", "Aaron Mueller", "Marek Rei"], "title": "How to Improve the Robustness of Closed-Source Models on NLI", "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Closed-source Large Language Models (LLMs) have become increasingly popular,\nwith impressive performance across a wide range of natural language tasks.\nThese models can be fine-tuned to further improve performance, but this often\nresults in the models learning from dataset-specific heuristics that reduce\ntheir robustness on out-of-distribution (OOD) data. Existing methods to improve\nrobustness either perform poorly, or are non-applicable to closed-source models\nbecause they assume access to model internals, or the ability to change the\nmodel's training procedure. In this work, we investigate strategies to improve\nthe robustness of closed-source LLMs through data-centric methods that do not\nrequire access to model internals. We find that the optimal strategy depends on\nthe complexity of the OOD data. For highly complex OOD datasets, upsampling\nmore challenging training examples can improve robustness by up to 1.5%. For\nless complex OOD datasets, replacing a portion of the training set with\nLLM-generated examples can improve robustness by 3.7%. More broadly, we find\nthat large-scale closed-source autoregressive LLMs are substantially more\nrobust than commonly used encoder models, and are a more appropriate choice of\nbaseline going forward.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6570\u636e\u7b56\u7565\u63d0\u5347\u95ed\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u6700\u4f73\u65b9\u6cd5\u53d6\u51b3\u4e8eOOD\u6570\u636e\u590d\u6742\u5ea6\uff1a\u590d\u6742\u6570\u636e\u4e0a\u91c7\u6837\u6311\u6218\u6027\u6837\u672c\u63d0\u53471.5%\uff0c\u7b80\u5355\u6570\u636e\u66ff\u6362LLM\u751f\u6210\u6837\u672c\u63d0\u53473.7%\u3002\u95ed\u6e90LLMs\u6bd4\u7f16\u7801\u5668\u6a21\u578b\u66f4\u9c81\u68d2\uff0c\u5e94\u4f5c\u4e3a\u57fa\u7ebf\u3002", "motivation": "\u95ed\u6e90LLMs\u5fae\u8c03\u540e\u6613\u5b66\u4e60\u6570\u636e\u96c6\u7279\u5b9a\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u5bfc\u81f4OOD\u6570\u636e\u9c81\u68d2\u6027\u4e0b\u964d\u3002\u73b0\u6709\u65b9\u6cd5\u56e0\u4f9d\u8d56\u6a21\u578b\u5185\u90e8\u8bbf\u95ee\u6216\u8bad\u7ec3\u8c03\u6574\u4e0d\u9002\u7528\u4e8e\u95ed\u6e90\u6a21\u578b\uff0c\u9700\u63a2\u7d22\u7eaf\u6570\u636e\u5c42\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u9488\u5bf9\u4e0d\u540c\u590d\u6742\u5ea6OOD\u6570\u636e\u8bbe\u8ba1\u7b56\u7565\uff1a1) \u9ad8\u590d\u6742\u5ea6\u573a\u666f\u4e0a\u91c7\u6837\u56f0\u96be\u6837\u672c 2) \u4f4e\u590d\u6742\u5ea6\u573a\u666f\u7528LLM\u751f\u6210\u6837\u672c\u66ff\u6362\u90e8\u5206\u8bad\u7ec3\u6570\u636e\u3002\u5bf9\u6bd4\u95ed\u6e90\u81ea\u56de\u5f52LLMs\u4e0e\u7f16\u7801\u5668\u6a21\u578b\u7684\u9c81\u68d2\u6027\u5dee\u5f02\u3002", "result": "\u9ad8\u590d\u6742\u5ea6OOD\u6570\u636e\u4e0a\u91c7\u6837\u7b56\u7565\u63d0\u5347\u9c81\u68d2\u60271.5%\uff1b\u4f4e\u590d\u6742\u5ea6\u573a\u666f\u751f\u6210\u6837\u672c\u66ff\u6362\u7b56\u7565\u63d0\u53473.7%\u3002\u95ed\u6e90LLMs\u9c81\u68d2\u6027\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7f16\u7801\u5668\u6a21\u578b\uff08\u5982BERT\uff09\u3002", "conclusion": "\u6570\u636e\u7b56\u7565\u6709\u6548\u6027\u53d6\u51b3\u4e8eOOD\u590d\u6742\u5ea6\uff0c\u95ed\u6e90\u81ea\u56de\u5f52LLMs\u5e94\u6210\u4e3a\u9c81\u68d2\u6027\u7814\u7a76\u7684\u57fa\u51c6\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u4e3a\u4e0d\u900f\u660e\u6a21\u578b\u63d0\u4f9b\u65e0\u9700\u5185\u90e8\u8bbf\u95ee\u7684\u9c81\u68d2\u6027\u63d0\u5347\u65b9\u6848\u3002"}}
