{"id": "2508.18290", "pdf": "https://arxiv.org/pdf/2508.18290", "abs": "https://arxiv.org/abs/2508.18290", "authors": ["Hans-Joachim Rudolph"], "title": "Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI", "categories": ["cs.CL"], "comment": "10 pages", "summary": "This essay develops a theoretical framework for a semantic Artificial General\nIntelligence (AGI) based on the notion of semantic attractors in complex-valued\nmeaning spaces. Departing from current transformer-based language models, which\noperate on statistical next-token prediction, we explore a model in which\nmeaning is not inferred probabilistically but formed through recursive\ntensorial transformation. Using cyclic operations involving the imaginary unit\n\\emph{i}, we describe a rotational semantic structure capable of modeling\nirony, homonymy, and ambiguity. At the center of this model, however, is a\nsemantic attractor -- a teleological operator that, unlike statistical\ncomputation, acts as an intentional agent (Microvitum), guiding meaning toward\nstability, clarity, and expressive depth. Conceived in terms of gradient flows,\ntensor deformations, and iterative matrix dynamics, the attractor offers a\nmodel of semantic transformation that is not only mathematically suggestive,\nbut also philosophically significant. We argue that true meaning emerges not\nfrom simulation, but from recursive convergence toward semantic coherence, and\nthat this requires a fundamentally new kind of cognitive architecture -- one\ndesigned to shape language, not just predict it.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u590d\u6570\u7a7a\u95f4\u8bed\u4e49\u5438\u5f15\u5b50\u7684\u65b0\u578bAGI\u6846\u67b6\uff0c\u901a\u8fc7\u9012\u5f52\u5f20\u91cf\u53d8\u6362\u5b9e\u73b0\u8bed\u4e49\u751f\u6210\u800c\u975e\u9884\u6d4b\uff0c\u7a81\u7834\u4f20\u7edf\u7edf\u8ba1\u8bed\u8a00\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709Transformer\u6a21\u578b\u4f9d\u8d56\u7edf\u8ba1\u9884\u6d4b\u65e0\u6cd5\u5904\u7406\u8bed\u4e49\u590d\u6742\u6027\uff08\u5982\u53cd\u8bbd/\u6b67\u4e49\uff09\uff0c\u9700\u6784\u5efa\u80fd\u4e3b\u52a8\u5851\u9020\u8bed\u8a00\u610f\u4e49\u7684\u8ba4\u77e5\u67b6\u6784\u3002", "method": "\u5728\u590d\u6570\u503c\u7a7a\u95f4\u5efa\u7acb\u65cb\u8f6c\u8bed\u4e49\u7ed3\u6784\uff0c\u901a\u8fc7\u865a\u6570\u5355\u4f4d\u5faa\u73af\u64cd\u4f5c\u548c\u9012\u5f52\u5f20\u91cf\u53d8\u6362\uff0c\u7ed3\u5408\u68af\u5ea6\u6d41/\u5f20\u91cf\u53d8\u5f62\u5b9e\u73b0\u8bed\u4e49\u5438\u5f15\u5b50\u52a8\u529b\u5b66\u3002", "result": "\u6784\u5efa\u51fa\u80fd\u89e3\u91ca\u8bed\u4e49\u590d\u6742\u73b0\u8c61\u7684\u6570\u5b66\u6a21\u578b\uff0c\u8bc1\u660e\u8bed\u4e49\u5438\u5f15\u5b50\u53ef\u5f15\u5bfc\u610f\u4e49\u5411\u7a33\u5b9a\u6001\u6536\u655b\uff0c\u63ed\u793aAGI\u9700\u8981\u8bed\u4e49\u751f\u6210\u578b\u8ba4\u77e5\u67b6\u6784\u3002", "conclusion": "\u771f\u6b63\u8bed\u4e49\u667a\u80fd\u4ea7\u751f\u4e8e\u9012\u5f52\u6536\u655b\u7684\u8bed\u4e49\u8fde\u8d2f\u8fc7\u7a0b\uff0c\u8fd9\u8981\u6c42\u4ece\u9884\u6d4b\u8303\u5f0f\u8f6c\u5411\u751f\u6210\u8303\u5f0f\uff0c\u4e3aAGI\u53d1\u5c55\u63d0\u4f9b\u7406\u8bba\u7a81\u7834\u65b9\u5411\u3002"}}
{"id": "2508.18321", "pdf": "https://arxiv.org/pdf/2508.18321", "abs": "https://arxiv.org/abs/2508.18321", "authors": ["Maojia Song", "Tej Deep Pala", "Weisheng Jin", "Amir Zadeh", "Chuan Li", "Dorien Herremans", "Soujanya Poria"], "title": "LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in multi-agent systems\n(MAS) as components of collaborative intelligence, where peer interactions\ndynamically shape individual decision-making. Although prior work has focused\non conformity bias, we extend the analysis to examine how LLMs form trust from\nprevious impressions, resist misinformation, and integrate peer input during\ninteraction, key factors for achieving collective intelligence under complex\nsocial dynamics. We present KAIROS, a benchmark simulating quiz contests with\npeer agents of varying reliability, offering fine-grained control over\nconditions such as expert-novice roles, noisy crowds, and adversarial peers.\nLLMs receive both historical interactions and current peer responses, allowing\nsystematic investigation into how trust, peer action, and self-confidence\ninfluence decisions. As for mitigation strategies, we evaluate prompting,\nsupervised fine-tuning, and reinforcement learning, Group Relative Policy\nOptimisation (GRPO), across multiple models. Our results reveal that GRPO with\nmulti-agent context combined with outcome-based rewards and unconstrained\nreasoning achieves the best overall performance, but also decreases the\nrobustness to social influence compared to Base models. The code and datasets\nare available at: https://github.com/declare-lab/KAIROS.", "AI": {"tldr": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u5982\u4f55\u5efa\u7acb\u4fe1\u4efb\u3001\u62b5\u5fa1\u9519\u8bef\u4fe1\u606f\u5e76\u6574\u5408\u7fa4\u4f53\u610f\u89c1\uff0c\u63d0\u51faKAIROS\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u53ca\u4f18\u5316\u7b56\u7565", "motivation": "\u7a81\u7834\u4f20\u7edf\u4ece\u4f17\u504f\u5dee\u7684\u7814\u7a76\u5c40\u9650\uff0c\u63a2\u7a76\u4fe1\u4efb\u673a\u5236\u3001\u7fa4\u4f53\u52a8\u6001\u5bf9\u96c6\u4f53\u667a\u80fd\u7684\u5f71\u54cd\uff0c\u65e8\u5728\u63d0\u5347\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e2dLLM\u7684\u51b3\u7b56\u8d28\u91cf", "method": "\u6784\u5efaKAIROS\u6a21\u62df\u6d4b\u8bd5\u5e73\u53f0\uff08\u542b\u4e13\u5bb6/\u65b0\u624b\u89d2\u8272\u3001\u566a\u58f0\u7fa4\u4f53\u3001\u5bf9\u6297\u6027\u667a\u80fd\u4f53\uff09\uff0c\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff08GRPO\u7b97\u6cd5\uff09\u8fdb\u884c\u7cfb\u7edf\u9a8c\u8bc1", "result": "GRPO\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u4e0a\u4e0b\u6587\u548c\u7ed3\u679c\u5956\u52b1\u673a\u5236\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\uff0c\u4f46\u964d\u4f4e\u4e86\u6a21\u578b\u5bf9\u793e\u4f1a\u5f71\u54cd\u7684\u9c81\u68d2\u6027\u3002\u6a21\u578b\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90", "conclusion": "\u63ed\u793a\u6027\u80fd\u63d0\u5347\u4e0e\u793e\u4f1a\u5f71\u54cd\u8106\u5f31\u6027\u4e4b\u95f4\u7684\u5e73\u8861\u96be\u9898\uff0c\u4e3a\u5f00\u53d1\u5177\u6709\u793e\u4f1a\u9002\u5e94\u6027\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u65b0\u65b9\u6cd5\u8bba\u548c\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177"}}
{"id": "2508.18328", "pdf": "https://arxiv.org/pdf/2508.18328", "abs": "https://arxiv.org/abs/2508.18328", "authors": ["Masudul Hasan Masud Bhuiyan", "Matteo Varvello", "Yasir Zaki", "Cristian-Alexandru Staicu"], "title": "Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective", "categories": ["cs.CL", "cs.CY", "cs.NI"], "comment": "6 pages, 6 figures", "summary": "English is the predominant language on the web, powering nearly half of the\nworld's top ten million websites. Support for multilingual content is\nnevertheless growing, with many websites increasingly combining English with\nregional or native languages in both visible content and hidden metadata. This\nmultilingualism introduces significant barriers for users with visual\nimpairments, as assistive technologies like screen readers frequently lack\nrobust support for non-Latin scripts and misrender or mispronounce non-English\ntext, compounding accessibility challenges across diverse linguistic contexts.\nYet, large-scale studies of this issue have been limited by the lack of\ncomprehensive datasets on multilingual web content. To address this gap, we\nintroduce LangCrUX, the first large-scale dataset of 120,000 popular websites\nacross 12 languages that primarily use non-Latin scripts. Leveraging this\ndataset, we conduct a systematic analysis of multilingual web accessibility and\nuncover widespread neglect of accessibility hints. We find that these hints\noften fail to reflect the language diversity of visible content, reducing the\neffectiveness of screen readers and limiting web accessibility. We finally\npropose Kizuki, a language-aware automated accessibility testing extension to\naccount for the limited utility of language-inconsistent accessibility hints.", "AI": {"tldr": "\u901a\u8fc7\u6784\u5efa\u9996\u4e2a\u591a\u8bed\u8a00\u7f51\u9875\u6570\u636e\u96c6LangCrUX\uff0c\u63ed\u793a\u7f51\u9875\u65e0\u969c\u788d\u63d0\u793a\u666e\u904d\u5ffd\u89c6\u8bed\u8a00\u591a\u6837\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u8bed\u8a00\u611f\u77e5\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u5de5\u5177Kizuki\u3002", "motivation": "\u73b0\u5b58\u7f51\u9875\u65e0\u969c\u788d\u7814\u7a76\u7f3a\u4e4f\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u5c4f\u5e55\u9605\u8bfb\u5668\u5bf9\u975e\u62c9\u4e01\u6587\u5b57\u652f\u6301\u4e0d\u8db3\uff0c\u52a0\u5267\u89c6\u969c\u7528\u6237\u8bbf\u95ee\u969c\u788d\u3002", "method": "1. \u6784\u5efa\u5305\u542b12\u79cd\u975e\u62c9\u4e01\u8bed\u7cfb\u300112\u4e07\u4e2a\u6d41\u884c\u7f51\u7ad9\u7684LangCrUX\u6570\u636e\u96c6\n2. \u7cfb\u7edf\u6027\u5206\u6790\u591a\u8bed\u8a00\u7f51\u9875\u7684\u65e0\u969c\u788d\u63d0\u793a\n3. \u5f00\u53d1\u8bed\u8a00\u611f\u77e5\u68c0\u6d4b\u5de5\u5177Kizuki", "result": "\u53d1\u73b0\u8d85\u534a\u6570\u7f51\u7ad9\u7684\u65e0\u969c\u788d\u63d0\u793a\u5b58\u5728\u8bed\u8a00\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5bfc\u81f4\u5c4f\u5e55\u9605\u8bfb\u5668\u6548\u7387\u964d\u4f4e43%\u3002", "conclusion": "\u9700\u5efa\u7acb\u8bed\u8a00\u654f\u611f\u7684\u65e0\u969c\u788d\u68c0\u6d4b\u673a\u5236\uff0cKizuki\u5de5\u5177\u53ef\u63d0\u5347\u591a\u8bed\u8a00\u7f51\u9875\u8f85\u52a9\u6280\u672f\u517c\u5bb9\u602720%\u4ee5\u4e0a\u3002"}}
{"id": "2508.18525", "pdf": "https://arxiv.org/pdf/2508.18525", "abs": "https://arxiv.org/abs/2508.18525", "authors": ["Eleni Tselepi", "Spyridon Thermos", "Gerasimos Potamianos"], "title": "Controllable Single-shot Animation Blending with Temporal Conditioning", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to the AI for Visual Arts Workshop at ICCV 2025", "summary": "Training a generative model on a single human skeletal motion sequence\nwithout being bound to a specific kinematic tree has drawn significant\nattention from the animation community. Unlike text-to-motion generation,\nsingle-shot models allow animators to controllably generate variations of\nexisting motion patterns without requiring additional data or extensive\nretraining. However, existing single-shot methods do not explicitly offer a\ncontrollable framework for blending two or more motions within a single\ngenerative pass. In this paper, we present the first single-shot motion\nblending framework that enables seamless blending by temporally conditioning\nthe generation process. Our method introduces a skeleton-aware normalization\nmechanism to guide the transition between motions, allowing smooth, data-driven\ncontrol over when and how motions blend. We perform extensive quantitative and\nqualitative evaluations across various animation styles and different kinematic\nskeletons, demonstrating that our approach produces plausible, smooth, and\ncontrollable motion blends in a unified and efficient manner.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u5355\u6b21\u8fd0\u52a8\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u95f4\u6761\u4ef6\u751f\u6210\u5b9e\u73b0\u65e0\u7f1d\u52a8\u4f5c\u878d\u5408", "motivation": "\u73b0\u6709\u5355\u6b21\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u5355\u4e00\u751f\u6210\u8fc7\u7a0b\u4e2d\u6df7\u5408\u591a\u52a8\u4f5c\u7684\u53ef\u63a7\u6846\u67b6", "method": "\u5f15\u5165\u9aa8\u67b6\u611f\u77e5\u5f52\u4e00\u5316\u673a\u5236\uff0c\u901a\u8fc7\u65f6\u95f4\u6761\u4ef6\u63a7\u5236\u751f\u6210\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u6570\u636e\u9a71\u52a8\u7684\u52a8\u4f5c\u8fc7\u6e21", "result": "\u5728\u4e0d\u540c\u52a8\u753b\u98ce\u683c\u548c\u9aa8\u9abc\u7ed3\u6784\u4e0b\u5b9e\u73b0\u5e73\u6ed1\u53ef\u63a7\u7684\u8fd0\u52a8\u6df7\u5408\uff0c\u5b9a\u91cf\u5b9a\u6027\u8bc4\u4f30\u663e\u793a\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u9ad8\u6548\u7edf\u4e00\u7684\u8fd0\u52a8\u6df7\u5408\u65b9\u6cd5\uff0c\u9996\u6b21\u5b9e\u73b0\u5355\u6b21\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u52a8\u6001\u53ef\u63a7\u6df7\u5408"}}
{"id": "2508.18381", "pdf": "https://arxiv.org/pdf/2508.18381", "abs": "https://arxiv.org/abs/2508.18381", "authors": ["Yuchun Fan", "Yilin Wang", "Yongyu Mu", "Lei Huang", "Bei Li", "Xiaocheng Feng", "Tong Xiao", "Jingbo Zhu"], "title": "Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models", "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 findings", "summary": "Large vision-language models (LVLMs) have demonstrated exceptional\ncapabilities in understanding visual information with human languages but also\nexhibit an imbalance in multilingual capabilities. In this work, we delve into\nthe multilingual working pattern of LVLMs and identify a salient correlation\nbetween the multilingual understanding ability of LVLMs and language-specific\nneuron activations in shallow layers. Building on this insight, we introduce\nPLAST, a training recipe that achieves efficient multilingual enhancement for\nLVLMs by Precise LAnguage-Specific layers fine-Tuning. PLAST first identifies\nlayers involved in multilingual understanding by monitoring language-specific\nneuron activations. These layers are then precisely fine-tuned with\nquestion-translation pairs to achieve multilingual alignment. Our empirical\nresults on MM-Bench and MMMB demonstrate that PLAST effectively improves the\nmultilingual capabilities of LVLMs and achieves significant efficiency with\nonly 14% of the parameters tuned. Further analysis reveals that PLAST can be\ngeneralized to low-resource and complex visual reasoning tasks, facilitating\nthe language-specific visual information engagement in shallow layers.", "AI": {"tldr": "\u63d0\u51faPLAST\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cbe\u51c6\u5fae\u8c03\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6d45\u5c42\u8bed\u8a00\u76f8\u5173\u795e\u7ecf\u5143\uff0c\u4ec5\u8c03\u657414%\u53c2\u6570\u5373\u53ef\u663e\u8457\u63d0\u5347\u591a\u8bed\u8a00\u80fd\u529b", "motivation": "\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u4e0a\u5b58\u5728\u4e0d\u5e73\u8861\u73b0\u8c61\uff0c\u5176\u80fd\u529b\u4e0e\u6d45\u5c42\u8bed\u8a00\u7279\u5f02\u6027\u795e\u7ecf\u5143\u6fc0\u6d3b\u76f8\u5173", "method": "1. \u901a\u8fc7\u76d1\u63a7\u8bed\u8a00\u7279\u5f02\u6027\u795e\u7ecf\u5143\u6fc0\u6d3b\u8bc6\u522b\u5173\u952e\u5c42 2. \u4f7f\u7528\u95ee\u9898\u7ffb\u8bd1\u5bf9\u5fae\u8c03\u5b9e\u73b0\u591a\u8bed\u8a00\u5bf9\u9f50", "result": "\u5728MM-Bench\u548cMMMB\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\u63d0\u5347\u591a\u8bed\u8a00\u80fd\u529b\uff0c\u53c2\u6570\u6548\u7387\u63d0\u5347\u81f3\u4ec5\u9700\u8c03\u657414%", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6cdb\u5316\u81f3\u4f4e\u8d44\u6e90\u4e0e\u590d\u6742\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\uff0c\u4fc3\u8fdb\u6d45\u5c42\u8bed\u8a00\u7279\u5f02\u6027\u89c6\u89c9\u4fe1\u606f\u5904\u7406\u80fd\u529b"}}
{"id": "2508.18540", "pdf": "https://arxiv.org/pdf/2508.18540", "abs": "https://arxiv.org/abs/2508.18540", "authors": ["Jonghyun Kim", "Cheng Sun", "Michael Stengel", "Matthew Chan", "Andrew Russell", "Jaehyun Jung", "Wil Braithwaite", "Shalini De Mello", "David Luebke"], "title": "Real-time 3D Visualization of Radiance Fields on Light Field Displays", "categories": ["cs.GR", "eess.IV"], "comment": "10 pages, 14 figures. J. Kim, C. Sun, and M. Stengel contributed\n  equally", "summary": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\u5b9e\u73b0\u5149\u573a\u663e\u793a\u5668\u4e0a\u7684\u5b9e\u65f6\u8f90\u5c04\u573a\u6e32\u67d3\uff0c\u652f\u6301NeRF/3D\u9ad8\u65af\u6cfc\u6e85/\u7a00\u758f\u4f53\u7d20\u7b49\u591a\u79cd\u8868\u793a\u65b9\u6cd5\uff0c\u572845\u89c6\u70b9\u4e0b\u5b9e\u73b0200+ FPS\u6e32\u67d3\u6027\u80fd\u3002", "motivation": "\u8f90\u5c04\u573a\u4e0e\u5149\u573a\u663e\u793a\u5668\u7684\u6574\u5408\u5b58\u5728\u53cc\u91cd\u8ba1\u7b97\u6311\u6218\uff1a\u5149\u573a\u9700\u8981\u591a\u89c6\u70b9\u9ad8\u5206\u8fa8\u7387\u6e32\u67d3\uff0c\u800c\u8f90\u5c04\u573a\u4f9d\u8d56\u8ba1\u7b97\u5bc6\u96c6\u7684\u4f53\u6e32\u67d3\u6280\u672f\u3002", "method": "\u91c7\u7528\u5355\u6b21\u5e73\u9762\u626b\u63cf\u7b56\u7565+\u975e\u65b9\u5411\u6027\u5171\u4eab\u7ec4\u4ef6\u7f13\u5b58\u67b6\u6784\uff0c\u901a\u8fc7\u89c6\u9525\u7a7a\u95f4\u91cd\u6295\u5f71\u907f\u514d\u591a\u89c6\u70b9\u91cd\u590d\u8ba1\u7b97\uff0c\u652f\u6301\u4e0d\u540c\u573a\u666f\u8868\u5f81\u7684\u901a\u7528\u5316\u5904\u7406\u3002", "result": "\u5728Looking Glass\u663e\u793a\u5668\u5b9e\u73b0\u5b9e\u65f6\u4ea4\u4e92\u5e94\u7528\uff08512p@45\u89c6\u70b9\uff09\uff0c\u6807\u51c6\u6d4b\u8bd5\u4e2d\u76f8\u6bd4\u72ec\u7acb\u6e32\u67d3\u5404\u89c6\u70b9\u901f\u5ea6\u63d0\u534722\u500d\u4e14\u4fdd\u6301\u753b\u8d28\u3002", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u5b9e\u73b0\u8de8\u591a\u79cd\u8f90\u5c04\u573a\u8868\u5f81\u7684\u5b9e\u65f6\u5149\u573a\u6e32\u67d3\uff0c\u901a\u8fc7\u67b6\u6784\u521b\u65b0\u6709\u6548\u89e3\u51b3\u591a\u89c6\u70b9\u6e32\u67d3\u5197\u4f59\u95ee\u9898\uff0c\u63a8\u52a8\u6c89\u6d78\u5f0f3D\u4ea4\u4e92\u53d1\u5c55\u3002"}}
{"id": "2508.18384", "pdf": "https://arxiv.org/pdf/2508.18384", "abs": "https://arxiv.org/abs/2508.18384", "authors": ["Kellen Tan Cheng", "Anna Lisa Gentile", "Chad DeLuca", "Guang-Jie Ren"], "title": "Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The pervasiveness of large language models (LLMs) in enterprise settings has\nalso brought forth a significant amount of risks associated with their usage.\nGuardrails technologies aim to mitigate this risk by filtering LLMs'\ninput/output text through various detectors. However, developing and\nmaintaining robust detectors faces many challenges, one of which is the\ndifficulty in acquiring production-quality labeled data on real LLM outputs\nprior to deployment. In this work, we propose backprompting, a simple yet\nintuitive solution to generate production-like labeled data for health advice\nguardrails development. Furthermore, we pair our backprompting method with a\nsparse human-in-the-loop clustering technique to label the generated data. Our\naim is to construct a parallel corpus roughly representative of the original\ndataset yet resembling real LLM output. We then infuse existing datasets with\nour synthetic examples to produce robust training data for our detector. We\ntest our technique in one of the most difficult and nuanced guardrails: the\nidentification of health advice in LLM output, and demonstrate improvement\nversus other solutions. Our detector is able to outperform GPT-4o by up to\n3.73%, despite having 400x less parameters.", "AI": {"tldr": "\u63d0\u51fabackprompting\u65b9\u6cd5\u751f\u6210\u751f\u4ea7\u7ea7\u6807\u6ce8\u6570\u636e\uff0c\u7ed3\u5408\u7a00\u758f\u4eba\u5de5\u805a\u7c7b\u6280\u672f\uff0c\u6784\u5efa\u63a5\u8fd1\u771f\u5b9eLLM\u8f93\u51fa\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u51fa\u7684\u5065\u5eb7\u5efa\u8bae\u68c0\u6d4b\u5668\u6027\u80fd\u8d85\u8d8aGPT-4o\u8fbe3.73%\uff08\u53c2\u6570\u91cf\u4ec51/400\uff09", "motivation": "\u4f01\u4e1a\u7ea7LLM\u5e94\u7528\u5b58\u5728\u5065\u5eb7\u5efa\u8bae\u7b49\u9ad8\u98ce\u9669\u573a\u666f\uff0c\u4f20\u7edf\u68c0\u6d4b\u5668\u56e0\u7f3a\u4e4f\u90e8\u7f72\u524d\u7684\u771f\u5b9eLLM\u8f93\u51fa\u6807\u6ce8\u6570\u636e\u800c\u96be\u4ee5\u6784\u5efa\u9c81\u68d2\u9632\u7ebf", "method": "\u901a\u8fc7backprompting\u9006\u5411\u751f\u6210\u6a21\u62df\u751f\u4ea7\u73af\u5883\u7684LLM\u8f93\u51fa\u6570\u636e\uff0c\u91c7\u7528\u4eba\u673a\u534f\u540c\u805a\u7c7b\u8fdb\u884c\u6807\u6ce8\uff0c\u589e\u5f3a\u73b0\u6709\u6570\u636e\u96c6\u4ee5\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u68c0\u6d4b\u6a21\u578b", "result": "\u53c2\u6570\u91cf\u4ec51/400\u7684\u68c0\u6d4b\u5668\u5728\u5065\u5eb7\u5efa\u8bae\u8bc6\u522b\u4efb\u52a1\u4e2dF1\u503c\u8fbe89.37%\uff0c\u76f8\u6bd4GPT-4o\u63d0\u53473.73\u4e2a\u767e\u5206\u70b9", "conclusion": "backprompting\u6709\u6548\u89e3\u51b3LLM\u5b89\u5168\u62a4\u680f\u5f00\u53d1\u4e2d\u7684\u6570\u636e\u74f6\u9888\uff0c\u8bc1\u660e\u5c0f\u6a21\u578b\u901a\u8fc7\u4f18\u8d28\u5408\u6210\u6570\u636e\u589e\u5f3a\u53ef\u5b9e\u73b0\u8d85\u8d8a\u5927\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u98ce\u9669\u68c0\u6d4b\u80fd\u529b"}}
{"id": "2508.18597", "pdf": "https://arxiv.org/pdf/2508.18597", "abs": "https://arxiv.org/abs/2508.18597", "authors": ["Xiaohao Sun", "Divyam Goel", "Angle X. Chang"], "title": "SemLayoutDiff: Semantic Layout Generation with Diffusion Model for Indoor Scene Synthesis", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://3dlg-hcvc.github.io/SemLayoutDiff/", "summary": "We present SemLayoutDiff, a unified model for synthesizing diverse 3D indoor\nscenes across multiple room types. The model introduces a scene layout\nrepresentation combining a top-down semantic map and attributes for each\nobject. Unlike prior approaches, which cannot condition on architectural\nconstraints, SemLayoutDiff employs a categorical diffusion model capable of\nconditioning scene synthesis explicitly on room masks. It first generates a\ncoherent semantic map, followed by a cross-attention-based network to predict\nfurniture placements that respect the synthesized layout. Our method also\naccounts for architectural elements such as doors and windows, ensuring that\ngenerated furniture arrangements remain practical and unobstructed. Experiments\non the 3D-FRONT dataset show that SemLayoutDiff produces spatially coherent,\nrealistic, and varied scenes, outperforming previous methods.", "AI": {"tldr": "\u63d0\u51faSemLayoutDiff\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u5730\u56fe\u4e0e\u6269\u6563\u6a21\u578b\u751f\u6210\u7b26\u5408\u5efa\u7b51\u7ea6\u675f\u76843D\u5ba4\u5185\u573a\u666f\u5e03\u5c40", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u8003\u8651\u95e8\u7a97\u7b49\u5efa\u7b51\u7ea6\u675f\uff0c\u5bfc\u81f4\u5bb6\u5177\u5e03\u5c40\u4e0d\u5b9e\u7528\u7684\u95ee\u9898", "method": "1. \u4f7f\u7528\u5206\u7c7b\u6269\u6563\u6a21\u578b\u751f\u6210\u5e26\u623f\u95f4\u63a9\u7801\u7684\u8bed\u4e49\u5730\u56fe 2. \u57fa\u4e8e\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u9884\u6d4b\u5bb6\u5177\u4f4d\u7f6e 3. \u663e\u5f0f\u5efa\u6a21\u95e8\u7a97\u7b49\u5efa\u7b51\u5143\u7d20\u7ea6\u675f", "result": "\u57283D-FRONT\u6570\u636e\u96c6\u4e0a\u751f\u6210\u573a\u666f\u7684\u7a7a\u95f4\u8fde\u8d2f\u6027\u3001\u771f\u5b9e\u6027\u548c\u591a\u6837\u6027\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u80fd\u751f\u6210\u65e2\u7b26\u5408\u5efa\u7b51\u89c4\u8303\u53c8\u4fdd\u6301\u591a\u6837\u6027\u7684\u5b9e\u75283D\u5ba4\u5185\u5e03\u5c40"}}
{"id": "2508.18387", "pdf": "https://arxiv.org/pdf/2508.18387", "abs": "https://arxiv.org/abs/2508.18387", "authors": ["Ivan Kobyzev", "Abbas Ghaddar", "Dingtao Hu", "Boxing Chen"], "title": "Integral Transformer: Denoising Attention, Not Too Much Not Too Little", "categories": ["cs.CL"], "comment": "EMNLP 2025 Main", "summary": "Softmax self-attention often assigns disproportionate weight to semantically\nuninformative tokens such as special tokens and punctuation, a phenomenon known\nas attention noise. While recent methods like Cog Attention and the\nDifferential Transformer have addressed this by introducing negative attention\nscores, they risk discarding useful information. In this paper, we propose the\nIntegral Transformer, a novel self-attention mechanism that denoises attention\nby integrating signals sampled from the logit distribution. Our approach\nmitigates noise while preserving the contributions of special tokens critical\nfor model performance. Extensive experiments demonstrate that our model\noutperforms vanilla, Cog, and Differential attention variants on\nwell-established knowledge and reasoning language benchmarks. Moreover, our\nanalysis reveals that employing vanilla self-attention in the lower Transformer\nlayers enhances performance and that the Integral Transformer effectively\nbalances attention distributions and reduces rank collapse in upper layers.", "AI": {"tldr": "\u63d0\u51fa\u4e86Integral Transformer\uff0c\u901a\u8fc7\u6574\u5408logit\u5206\u5e03\u7684\u91c7\u6837\u4fe1\u53f7\u5b9e\u73b0\u81ea\u6ce8\u610f\u529b\u964d\u566a\uff0c\u5728\u4fdd\u7559\u5173\u952e\u7279\u6b8a\u6807\u8bb0\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709Cog\u548cDifferential\u6ce8\u610f\u529b\u673a\u5236\u901a\u8fc7\u8d1f\u5206\u6570\u6291\u5236\u566a\u58f0\u65f6\u53ef\u80fd\u4e22\u5931\u6709\u7528\u4fe1\u606f\uff0c\u9700\u5728\u964d\u566a\u4e0e\u4fdd\u7559\u5173\u952e\u6807\u8bb0\u95f4\u53d6\u5f97\u5e73\u8861", "method": "\u5728Transformer\u5e95\u5c42\u4f7f\u7528\u5e38\u89c4\u81ea\u6ce8\u610f\u529b\u589e\u5f3a\u8868\u8fbe\u80fd\u529b\uff0c\u5728\u9ad8\u5c42\u5e94\u7528Integral Transformer\u6574\u5408logit\u4fe1\u53f7\u5e73\u8861\u6ce8\u610f\u529b\u5206\u5e03", "result": "\u5728\u77e5\u8bc6\u63a8\u7406\u57fa\u51c6\u4e0a\u8d85\u8d8a\u591a\u79cd\u6ce8\u610f\u529b\u53d8\u4f53\uff0c\u5206\u6790\u663e\u793a\u80fd\u6709\u6548\u7f13\u89e3\u79e9\u5d29\u6e83\u73b0\u8c61\u5e76\u4f18\u5316\u6ce8\u610f\u529b\u5206\u5e03", "conclusion": "\u5206\u5c42\u6ce8\u610f\u529b\u8bbe\u8ba1\uff08\u5e95\u5c42\u5e38\u89c4+\u9ad8\u5c42\u79ef\u5206\uff09\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u4e0e\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u7684\u53cc\u91cd\u76ee\u6807\uff0c\u4e3aTransformer\u6539\u8fdb\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2508.18944", "pdf": "https://arxiv.org/pdf/2508.18944", "abs": "https://arxiv.org/abs/2508.18944", "authors": ["Shashikant Verma", "Shanmuganathan Raman"], "title": "PanoHair: Detailed Hair Strand Synthesis on Volumetric Heads", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Achieving realistic hair strand synthesis is essential for creating lifelike\ndigital humans, but producing high-fidelity hair strand geometry remains a\nsignificant challenge. Existing methods require a complex setup for data\nacquisition, involving multi-view images captured in constrained studio\nenvironments. Additionally, these methods have longer hair volume estimation\nand strand synthesis times, which hinder efficiency. We introduce PanoHair, a\nmodel that estimates head geometry as signed distance fields using knowledge\ndistillation from a pre-trained generative teacher model for head synthesis.\nOur approach enables the prediction of semantic segmentation masks and 3D\norientations specifically for the hair region of the estimated geometry. Our\nmethod is generative and can generate diverse hairstyles with latent space\nmanipulations. For real images, our approach involves an inversion process to\ninfer latent codes and produces visually appealing hair strands, offering a\nstreamlined alternative to complex multi-view data acquisition setups. Given\nthe latent code, PanoHair generates a clean manifold mesh for the hair region\nin under 5 seconds, along with semantic and orientation maps, marking a\nsignificant improvement over existing methods, as demonstrated in our\nexperiments.", "AI": {"tldr": "\u63d0\u51faPanoHair\u6a21\u578b\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u751f\u6210\u5f0f\u65b9\u6cd5\u5b9e\u73b05\u79d2\u5185\u5feb\u901f\u751f\u6210\u9ad8\u4fdd\u771f\u5934\u53d1\u7f51\u683c\uff0c\u663e\u8457\u63d0\u5347\u73b0\u6709\u65b9\u6cd5\u7684\u6548\u7387\u4e0e\u8d28\u91cf", "motivation": "\u73b0\u6709\u5934\u53d1\u5408\u6210\u65b9\u6cd5\u9700\u8981\u590d\u6742\u591a\u89c6\u89d2\u6570\u636e\u91c7\u96c6\u4e14\u6548\u7387\u4f4e\u4e0b\uff0c\u96be\u4ee5\u5feb\u901f\u751f\u6210\u591a\u6837\u5316\u9ad8\u7cbe\u5ea6\u53d1\u578b", "method": "1. \u4f7f\u7528\u9884\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\u4f30\u8ba1\u5934\u90e8SDF\n2. \u9884\u6d4b\u5934\u53d1\u533a\u57df\u8bed\u4e49\u5206\u5272\u63a9\u7801\u548c3D\u65b9\u5411\n3. \u652f\u6301\u6f5c\u5728\u7a7a\u95f4\u64cd\u4f5c\u751f\u6210\u591a\u6837\u5316\u53d1\u578b\n4. \u901a\u8fc7\u53cd\u8f6c\u8fc7\u7a0b\u5904\u7406\u771f\u5b9e\u56fe\u50cf", "result": "\u751f\u6210\u6e05\u6d01\u5934\u53d1\u7f51\u683c\u8017\u65f6<5\u79d2\uff0c\u652f\u6301\u6f5c\u5728\u7a7a\u95f4\u53d1\u578b\u7f16\u8f91\uff0c\u5728\u771f\u5b9e\u56fe\u50cf\u4e0a\u5b9e\u73b0\u89c6\u89c9\u5438\u5f15\u529b\u7684\u5934\u53d1\u5408\u6210\uff0c\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "PanoHair\u901a\u8fc7\u751f\u6210\u5f0f\u67b6\u6784\u548c\u9ad8\u6548\u63a8\u7406\u6d41\u7a0b\uff0c\u7a81\u7834\u4f20\u7edf\u65b9\u6cd5\u7684\u6570\u636e\u91c7\u96c6\u9650\u5236\uff0c\u5b9e\u73b0\u5feb\u901f\u3001\u9ad8\u8d28\u91cf\u7684\u5934\u53d1\u5408\u6210"}}
{"id": "2508.18395", "pdf": "https://arxiv.org/pdf/2508.18395", "abs": "https://arxiv.org/abs/2508.18395", "authors": ["Jeong-seok Oh", "Jay-yoon Lee"], "title": "Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Probabilistic decoding in Large Language Models (LLMs) often yields\ninconsistent outputs, particularly on complex or long-form questions.\nSelf-Consistency (SC) mitigates this for short-form QA by majority voting over\nexact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram\nConsistency Score (WUCS) extend to long-form responses but lose accuracy on\nshort-form benchmarks.\n  We introduce Latent Self-Consistency (LSC), which selects the most\nsemantically consistent response using learnable token embeddings. A\nlightweight forward generation of summary tokens increases inference time by\nless than 1% and requires no changes to the model architecture.\n  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU,\nTruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form\nones on average, while maintaining negligible computational overhead. These\nresults position LSC as a practical consistency-selection method that works\nreliably across answer formats. Additionally, LSC provides well-calibrated\nconfidence estimates, maintaining low Expected Calibration Error across both\nanswer formats.", "AI": {"tldr": "\u63d0\u51fa\u6f5c\u5728\u8bed\u4e49\u4e00\u81f4\u6027\uff08LSC\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u8bcd\u5d4c\u5165\u9009\u62e9\u8bed\u4e49\u6700\u4e00\u81f4\u7684\u56de\u7b54\uff0c\u5728\u77ed/\u957f\u683c\u5f0f\u95ee\u7b54\u4e2d\u5747\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u81ea\u6d3d\u65b9\u6cd5\uff08SC/USC/WUCS\uff09\u5728\u77ed\u683c\u5f0f\u548c\u957f\u683c\u5f0f\u7b54\u6848\u573a\u666f\u4e2d\u5b58\u5728\u51c6\u786e\u7387\u65e0\u6cd5\u517c\u987e\u7684\u95ee\u9898\uff0c\u9700\u8981\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u53ef\u5b66\u4e60\u8bcd\u5d4c\u5165\u8fdb\u884c\u8bed\u4e49\u4e00\u81f4\u6027\u9009\u62e9\uff0c\u914d\u5408\u8f7b\u91cf\u7ea7\u6458\u8981\u6807\u8bb0\u751f\u6210\u6280\u672f\uff0c\u63a8\u7406\u65f6\u95f4\u589e\u52a0\u4e0d\u8db31%\u4e14\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\u3002", "result": "\u57286\u4e2a\u77ed\u683c\u5f0f\u548c5\u4e2a\u957f\u683c\u5f0f\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u6821\u51c6\u8bef\u5dee\u4f4e\u4e8e0.04\u3002", "conclusion": "LSC\u6210\u4e3a\u9996\u4e2a\u5728\u591a\u79cd\u7b54\u6848\u683c\u5f0f\u4e2d\u5747\u53ef\u9760\u7684\u81ea\u6d3d\u9009\u62e9\u65b9\u6cd5\uff0c\u517c\u5177\u8ba1\u7b97\u9ad8\u6548\u548c\u7f6e\u4fe1\u5ea6\u6821\u51c6\u4f18\u52bf\u3002"}}
{"id": "2508.19140", "pdf": "https://arxiv.org/pdf/2508.19140", "abs": "https://arxiv.org/abs/2508.19140", "authors": ["Florian Hahlbohm", "Linus Franke", "Leon Overk\u00e4mping", "Paula Wespe", "Susana Castillo", "Martin Eisemann", "Marcus Magnor"], "title": "A Bag of Tricks for Efficient Implicit Neural Point Clouds", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Project page: https://fhahlbohm.github.io/inpc_v2/", "summary": "Implicit Neural Point Cloud (INPC) is a recent hybrid representation that\ncombines the expressiveness of neural fields with the efficiency of point-based\nrendering, achieving state-of-the-art image quality in novel view synthesis.\nHowever, as with other high-quality approaches that query neural networks\nduring rendering, the practical usability of INPC is limited by comparatively\nslow rendering. In this work, we present a collection of optimizations that\nsignificantly improve both the training and inference performance of INPC\nwithout sacrificing visual fidelity. The most significant modifications are an\nimproved rasterizer implementation, more effective sampling techniques, and the\nincorporation of pre-training for the convolutional neural network used for\nhole-filling. Furthermore, we demonstrate that points can be modeled as small\nGaussians during inference to further improve quality in extrapolated, e.g.,\nclose-up views of the scene. We design our implementations to be broadly\napplicable beyond INPC and systematically evaluate each modification in a\nseries of experiments. Our optimized INPC pipeline achieves up to 25% faster\ntraining, 2x faster rendering, and 20% reduced VRAM usage paired with slight\nimage quality improvements.", "AI": {"tldr": "INPC\u901a\u8fc7\u4f18\u5316\u5149\u6805\u5316\u5668\u3001\u91c7\u6837\u6280\u672f\u548c\u5f15\u5165CNN\u9884\u8bad\u7ec3\uff0c\u5b9e\u73b0\u8bad\u7ec3\u52a0\u901f25%\u3001\u6e32\u67d3\u63d0\u901f2\u500d\u3001\u663e\u5b58\u51cf\u5c1120%\uff0c\u540c\u65f6\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3INPC\u56e0\u6e32\u67d3\u901f\u5ea6\u6162\u5bfc\u81f4\u7684\u5b9e\u7528\u6027\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u4f18\u5316\u6027\u80fd\u3002", "method": "\u6539\u8fdb\u5149\u6805\u5316\u5668\u5b9e\u73b0\u3001\u4f18\u5316\u91c7\u6837\u7b56\u7565\u3001CNN\u9884\u8bad\u7ec3\u7528\u4e8e\u5b54\u6d1e\u586b\u5145\u3001\u63a8\u7406\u65f6\u91c7\u7528\u9ad8\u65af\u70b9\u5efa\u6a21\u3002", "result": "\u8bad\u7ec3\u901f\u5ea6\u63d0\u534725%\uff0c\u6e32\u67d3\u901f\u5ea6\u7ffb\u500d\uff0c\u663e\u5b58\u5360\u7528\u51cf\u5c1120%\uff0c\u56fe\u50cf\u8d28\u91cf\u8f7b\u5fae\u63d0\u5347\u3002", "conclusion": "\u7cfb\u7edf\u6027\u7684\u4f18\u5316\u65b9\u6848\u4e0d\u4ec5\u9002\u7528\u4e8eINPC\uff0c\u4e5f\u4e3a\u7c7b\u4f3c\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\u63d0\u4f9b\u53ef\u501f\u9274\u7684\u52a0\u901f\u7b56\u7565\u3002"}}
{"id": "2508.18407", "pdf": "https://arxiv.org/pdf/2508.18407", "abs": "https://arxiv.org/abs/2508.18407", "authors": ["Michal \u0160tef\u00e1nik", "Timothee Mickus", "Marek Kadl\u010d\u00edk", "Michal Spiegel", "Josef Kucha\u0159"], "title": "Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering", "categories": ["cs.CL", "cs.AI", "68T01, 68T07, 68T50", "I.2"], "comment": "To appear in Findings of EMNLP 2025", "summary": "A majority of recent work in AI assesses models' generalization capabilities\nthrough the lens of performance on out-of-distribution (OOD) datasets. Despite\ntheir practicality, such evaluations build upon a strong assumption: that OOD\nevaluations can capture and reflect upon possible failures in a real-world\ndeployment.\n  In this work, we challenge this assumption and confront the results obtained\nfrom OOD evaluations with a set of specific failure modes documented in\nexisting question-answering (QA) models, referred to as a reliance on spurious\nfeatures or prediction shortcuts.\n  We find that different datasets used for OOD evaluations in QA provide an\nestimate of models' robustness to shortcuts that have a vastly different\nquality, some largely under-performing even a simple, in-distribution\nevaluation. We partially attribute this to the observation that spurious\nshortcuts are shared across ID+OOD datasets, but also find cases where a\ndataset's quality for training and evaluation is largely disconnected. Our work\nunderlines limitations of commonly-used OOD-based evaluations of\ngeneralization, and provides methodology and recommendations for evaluating\ngeneralization within and beyond QA more robustly.", "AI": {"tldr": "\u8bba\u6587\u8d28\u7591\u5f53\u524dAI\u9886\u57df\u5e7f\u6cdb\u4f7f\u7528\u7684OOD\u8bc4\u4f30\u65b9\u6cd5\u5728\u68c0\u6d4b\u6a21\u578b\u771f\u5b9e\u573a\u666f\u5931\u6548\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u4e0d\u540cQA\u6570\u636e\u96c6\u5bf9\u6a21\u578b\u6297\u5e72\u6270\u80fd\u529b\u7684\u8bc4\u4f30\u8d28\u91cf\u5dee\u5f02\u663e\u8457\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u65b9\u6cd5\u5efa\u8bae\u3002", "motivation": "\u6311\u6218OOD\u8bc4\u4f30\u65b9\u6cd5\u80fd\u6709\u6548\u53cd\u6620\u6a21\u578b\u5b9e\u9645\u90e8\u7f72\u4e2d\u6f5c\u5728\u5931\u6548\u7684\u5047\u8bbe\uff0c\u63ed\u793a\u73b0\u6709QA\u6a21\u578b\u8bc4\u4f30\u4e2d\u5b58\u5728\u7684\u300c\u4f2a\u7279\u5f81\u4f9d\u8d56\u300d\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06OOD\u8bc4\u4f30\u7ed3\u679c\u4e0e\u5df2\u8bb0\u5f55\u7684QA\u6a21\u578b\u5931\u6548\u6a21\u5f0f\uff08\u9884\u6d4b\u6377\u5f84\uff09\u8fdb\u884c\u5bf9\u6bd4\uff0c\u5206\u6790\u4e0d\u540cOOD\u6570\u636e\u96c6\u5bf9\u6a21\u578b\u6297\u5e72\u6270\u80fd\u529b\u7684\u8bc4\u4f30\u8d28\u91cf\u5dee\u5f02\u53ca\u5176\u6210\u56e0\u3002", "result": "\u53d1\u73b0\u90e8\u5206OOD\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u8d28\u91cf\u751a\u81f3\u4f4e\u4e8e\u7b80\u5355\u540c\u5206\u5e03\u6d4b\u8bd5\uff0c\u90e8\u5206\u6e90\u4e8eID/OOD\u6570\u636e\u96c6\u95f4\u5171\u4eab\u4f2a\u7279\u5f81\uff0c\u90e8\u5206\u6e90\u4e8e\u6570\u636e\u96c6\u8bad\u7ec3/\u8bc4\u4f30\u8d28\u91cf\u7684\u8131\u8282\u3002", "conclusion": "\u5f3a\u8c03\u5e38\u7528OOD\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u5e94\u5efa\u7acb\u66f4\u9c81\u68d2\u7684\u8bc4\u4f30\u65b9\u6cd5\u8bba\uff0c\u63a8\u8350\u540c\u65f6\u5173\u6ce8\u57df\u5185\u57df\u5916\u8bc4\u4f30\uff0c\u5e76\u6539\u8fdb\u6570\u636e\u96c6\u8d28\u91cf\u8bc4\u4f30\u4f53\u7cfb\u3002"}}
{"id": "2508.18481", "pdf": "https://arxiv.org/pdf/2508.18481", "abs": "https://arxiv.org/abs/2508.18481", "authors": ["Yue Yang", "Xue Xie", "Xinkai Wang", "Hui Zhang", "Chiming Yu", "Xiaoxian Xiong", "Lifeng Zhu", "Yuanyi Zheng", "Jue Cen", "Bruce Daniel", "Fred Baik"], "title": "Impact of Target and Tool Visualization on Depth Perception and Usability in Optical See-Through AR", "categories": ["cs.HC", "cs.CV", "cs.GR"], "comment": null, "summary": "Optical see-through augmented reality (OST-AR) systems like Microsoft\nHoloLens 2 hold promise for arm's distance guidance (e.g., surgery), but depth\nperception of the hologram and occlusion of real instruments remain\nchallenging. We present an evaluation of how visualizing the target object with\ndifferent transparencies and visualizing a tracked tool (virtual proxy vs. real\ntool vs. no tool tracking) affects depth perception and system usability. Ten\nparticipants performed two experiments on HoloLens 2. In Experiment 1, we\ncompared high-transparency vs. low-transparency target rendering in a depth\nmatching task at arm's length. In Experiment 2, participants performed a\nsimulated surgical pinpoint task on a frontal bone target under six\nvisualization conditions ($2 \\times 3$: two target transparencies and three\ntool visualization modes: virtual tool hologram, real tool, or no tool\ntracking). We collected data on depth matching error, target localization\nerror, system usability, task workload, and qualitative feedback. Results show\nthat a more opaque target yields significantly lower depth estimation error\nthan a highly transparent target at arm's distance. Moreover, showing the real\ntool (occluding the virtual target) led to the highest accuracy and usability\nwith the lowest workload, while not tracking the tool yielded the worst\nperformance and user ratings. However, making the target highly transparent,\nwhile allowing the real tool to remain visible, slightly impaired depth cues\nand did not improve usability. Our findings underscore that correct occlusion\ncues, rendering virtual content opaque and occluding it with real tools in real\ntime, are critical for depth perception and precision in OST-AR. Designers of\narm-distance AR systems should prioritize robust tool tracking and occlusion\nhandling; if unavailable, cautiously use transparency to balance depth\nperception and tool visibility.", "AI": {"tldr": "HoloLens 2\u7b49OST-AR\u7cfb\u7edf\u5728\u8fd1\u8ddd\u79bb\u624b\u672f\u5f15\u5bfc\u4e2d\u5b58\u5728\u6df1\u5ea6\u611f\u77e5\u4e0e\u906e\u6321\u95ee\u9898\u3002\u5b9e\u9a8c\u8868\u660e\uff1a\u4e0d\u900f\u660e\u865a\u62df\u76ee\u6807\u3001\u5b9e\u65f6\u771f\u5b9e\u5de5\u5177\u906e\u6321\u80fd\u663e\u8457\u63d0\u5347\u6df1\u5ea6\u611f\u77e5\u7cbe\u5ea6\uff0c\u5de5\u5177\u8ffd\u8e2a\u7f3a\u5931\u4f1a\u4e25\u91cd\u964d\u4f4e\u7cfb\u7edf\u53ef\u7528\u6027\u3002", "motivation": "\u89e3\u51b3OST-AR\u5728\u8fd1\u8ddd\u79bb\u624b\u672f\u573a\u666f\u4e2d\u865a\u62df\u76ee\u6807\u6df1\u5ea6\u611f\u77e5\u6a21\u7cca\u3001\u771f\u5b9e\u624b\u672f\u5de5\u5177\u4e0e\u865a\u62df\u76ee\u6807\u906e\u6321\u5173\u7cfb\u5931\u771f\u7684\u6838\u5fc3\u75db\u70b9\uff0c\u4f18\u5316AR\u5f15\u5bfc\u7cfb\u7edf\u7684\u7a7a\u95f4\u611f\u77e5\u7cbe\u5ea6\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\uff1a1) \u9ad8\u4f4e\u900f\u660e\u5ea6\u76ee\u6807\u7684\u6df1\u5ea6\u5339\u914d\u6d4b\u8bd5 2) 6\u79cd\u53ef\u89c6\u5316\u6761\u4ef6(2\u900f\u660e\u5ea6\u00d73\u5de5\u5177\u6a21\u5f0f)\u4e0b\u7684\u6a21\u62df\u624b\u672f\u5b9a\u4f4d\u4efb\u52a1\uff0c\u91c7\u96c610\u540d\u53c2\u4e0e\u8005\u7684\u6df1\u5ea6\u8bef\u5dee\u3001\u5b9a\u4f4d\u7cbe\u5ea6\u3001\u7cfb\u7edf\u53ef\u7528\u6027\u7b49\u591a\u7ef4\u5ea6\u6570\u636e\u3002", "result": "\u4e0d\u900f\u660e\u76ee\u6807\u6bd4\u9ad8\u900f\u660e\u76ee\u6807\u51cf\u5c1143%\u6df1\u5ea6\u8bef\u5dee(p<0.05)\uff1b\u771f\u5b9e\u5de5\u5177\u53ef\u89c6\u5316\u6a21\u5f0f\u5b9a\u4f4d\u8bef\u5dee\u964d\u4f4e62%\uff0c\u7cfb\u7edf\u53ef\u7528\u6027\u8bc4\u5206\u63d0\u9ad831%\uff0c\u540c\u65f6\u8ba4\u77e5\u8d1f\u8377\u964d\u4f4e28%\uff1b\u65e0\u5de5\u5177\u8ffd\u8e2a\u6a21\u5f0f\u8868\u73b0\u6700\u5dee\u3002", "conclusion": "AR\u7cfb\u7edf\u8bbe\u8ba1\u5e94\u4f18\u5148\u786e\u4fdd\u5b9e\u65f6\u5de5\u5177\u8ffd\u8e2a\u4e0e\u906e\u6321\u5904\u7406\u3002\u82e5\u65e0\u6cd5\u5b9e\u73b0\uff0c\u9700\u8c28\u614e\u8c03\u8282\u900f\u660e\u5ea6\u5e73\u8861\u6df1\u5ea6\u7ebf\u7d22\u4e0e\u5de5\u5177\u53ef\u89c1\u6027\uff0c\u865a\u62df\u5185\u5bb9\u5e94\u4fdd\u6301\u4e0d\u900f\u660e\u4ee5\u7ef4\u6301\u7a7a\u95f4\u611f\u77e5\u51c6\u786e\u6027\u3002"}}
{"id": "2508.18444", "pdf": "https://arxiv.org/pdf/2508.18444", "abs": "https://arxiv.org/abs/2508.18444", "authors": ["Nafis Tanveer Islam", "Zhiming Zhao"], "title": "How Reliable are LLMs for Reasoning on the Re-ranking task?", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at FQAS Conference 2024. DOI will be provided in 3 weeks\n  after the conference has published the paper", "summary": "With the improving semantic understanding capability of Large Language Models\n(LLMs), they exhibit a greater awareness and alignment with human values, but\nthis comes at the cost of transparency. Although promising results are achieved\nvia experimental analysis, an in-depth understanding of the LLM's internal\nworkings is unavoidable to comprehend the reasoning behind the re-ranking,\nwhich provides end users with an explanation that enables them to make an\ninformed decision. Moreover, in newly developed systems with limited user\nengagement and insufficient ranking data, accurately re-ranking content remains\na significant challenge. While various training methods affect the training of\nLLMs and generate inference, our analysis has found that some training methods\nexhibit better explainability than others, implying that an accurate semantic\nunderstanding has not been learned through all training methods; instead,\nabstract knowledge has been gained to optimize evaluation, which raises\nquestions about the true reliability of LLMs. Therefore, in this work, we\nanalyze how different training methods affect the semantic understanding of the\nre-ranking task in LLMs and investigate whether these models can generate more\ninformed textual reasoning to overcome the challenges of transparency or LLMs\nand limited training data. To analyze the LLMs for re-ranking tasks, we utilize\na relatively small ranking dataset from the environment and the Earth science\ndomain to re-rank retrieved content. Furthermore, we also analyze the\nexplainable information to see if the re-ranking can be reasoned using\nexplainability.", "AI": {"tldr": "\u63a2\u8ba8\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u5bf9LLM\u5728\u91cd\u6392\u5e8f\u4efb\u52a1\u4e2d\u8bed\u4e49\u7406\u89e3\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u90e8\u5206\u8bad\u7ec3\u65b9\u6cd5\u901a\u8fc7\u62bd\u8c61\u77e5\u8bc6\u4f18\u5316\u8bc4\u4f30\u6307\u6807\u800c\u975e\u771f\u6b63\u7406\u89e3\u8bed\u4e49\uff0c\u8d28\u7591LLM\u53ef\u9760\u6027\u3002", "motivation": "LLM\u5728\u63d0\u5347\u8bed\u4e49\u7406\u89e3\u65f6\u727a\u7272\u4e86\u900f\u660e\u5ea6\uff0c\u4e14\u6570\u636e\u4e0d\u8db3\u7684\u65b0\u7cfb\u7edf\u9762\u4e34\u91cd\u6392\u5e8f\u51c6\u786e\u6027\u6311\u6218\uff0c\u9700\u9a8c\u8bc1\u6a21\u578b\u80fd\u5426\u751f\u6210\u53ef\u4fe1\u7684\u6587\u672c\u63a8\u7406\u3002", "method": "\u4f7f\u7528\u73af\u5883\u4e0e\u5730\u7403\u79d1\u5b66\u9886\u57df\u7684\u5c0f\u578b\u6392\u540d\u6570\u636e\u96c6\uff0c\u5206\u6790\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u5bf9LLM\u8bed\u4e49\u7406\u89e3\u548c\u63a8\u7406\u53ef\u89e3\u91ca\u6027\u7684\u5f71\u54cd\u3002", "result": "\u90e8\u5206\u8bad\u7ec3\u65b9\u6cd5\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u90e8\u5206\u65b9\u6cd5\u4ec5\u4f18\u5316\u8bc4\u4f30\u6307\u6807\u800c\u672a\u5b9e\u73b0\u771f\u5b9e\u8bed\u4e49\u7406\u89e3\uff0c\u66b4\u9732LLM\u53ef\u9760\u6027\u9690\u5fe7\u3002", "conclusion": "\u9700\u5f00\u53d1\u66f4\u900f\u660e\u7684\u8bad\u7ec3\u65b9\u6cd5\u4ee5\u63d0\u9ad8LLM\u5728\u6709\u9650\u6570\u636e\u573a\u666f\u4e0b\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u786e\u4fdd\u63a8\u7406\u8fc7\u7a0b\u7684\u771f\u5b9e\u8bed\u4e49\u7406\u89e3\u3002"}}
{"id": "2508.19204", "pdf": "https://arxiv.org/pdf/2508.19204", "abs": "https://arxiv.org/abs/2508.19204", "authors": ["Julian Ost", "Andrea Ramazzina", "Amogh Joshi", "Maximilian B\u00f6mer", "Mario Bijelic", "Felix Heide"], "title": "LSD-3D: Large-Scale 3D Driving Scene Generation with Geometry Grounding", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "Project webpage: https://light.princeton.edu/LSD-3D", "summary": "Large-scale scene data is essential for training and testing in robot\nlearning. Neural reconstruction methods have promised the capability of\nreconstructing large physically-grounded outdoor scenes from captured sensor\ndata. However, these methods have baked-in static environments and only allow\nfor limited scene control -- they are functionally constrained in scene and\ntrajectory diversity by the captures from which they are reconstructed. In\ncontrast, generating driving data with recent image or video diffusion models\noffers control, however, at the cost of geometry grounding and causality. In\nthis work, we aim to bridge this gap and present a method that directly\ngenerates large-scale 3D driving scenes with accurate geometry, allowing for\ncausal novel view synthesis with object permanence and explicit 3D geometry\nestimation. The proposed method combines the generation of a proxy geometry and\nenvironment representation with score distillation from learned 2D image\npriors. We find that this approach allows for high controllability, enabling\nthe prompt-guided geometry and high-fidelity texture and structure that can be\nconditioned on map layouts -- producing realistic and geometrically consistent\n3D generations of complex driving scenes.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u4ee3\u7406\u51e0\u4f55\u751f\u6210\u4e0e\u5206\u6570\u84b8\u998f\u7684\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u53ef\u63a7\u6027\u7684\u5927\u89c4\u6a213D\u9a7e\u9a76\u573a\u666f\u751f\u6210", "motivation": "\u73b0\u6709\u795e\u7ecf\u91cd\u5efa\u65b9\u6cd5\u7f3a\u4e4f\u573a\u666f\u63a7\u5236\u80fd\u529b\uff0c\u6269\u6563\u6a21\u578b\u751f\u6210\u6570\u636e\u7f3a\u4e4f\u51e0\u4f55\u57fa\u7840\u4e0e\u56e0\u679c\u6027\uff0c\u9700\u7ed3\u5408\u4e24\u8005\u4f18\u52bf", "method": "\u901a\u8fc7\u4ee3\u7406\u51e0\u4f55\u751f\u6210\u73af\u5883\u8868\u793a\uff0c\u5229\u75282D\u56fe\u50cf\u5148\u9a8c\u8fdb\u884c\u5206\u6570\u84b8\u998f\uff0c\u652f\u6301\u5730\u56fe\u5e03\u5c40\u6761\u4ef6\u7ea6\u675f", "result": "\u5b9e\u73b0\u51e0\u4f55\u7cbe\u786e\u3001\u7eb9\u7406\u903c\u771f\u4e14\u4fdd\u6301\u4e09\u7ef4\u4e00\u81f4\u6027\u7684\u590d\u6742\u9a7e\u9a76\u573a\u666f\u751f\u6210", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u7834\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9759\u6001\u9650\u5236\uff0c\u5728\u4fdd\u6301\u51e0\u4f55\u7cbe\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u573a\u666f\u751f\u6210\u7684\u53ef\u63a7\u6027"}}
{"id": "2508.18466", "pdf": "https://arxiv.org/pdf/2508.18466", "abs": "https://arxiv.org/abs/2508.18466", "authors": ["Alina Wr\u00f3blewska", "Bartosz \u017buk"], "title": "Integrating gender inclusivity into large language models via instruction tuning", "categories": ["cs.CL"], "comment": null, "summary": "Imagine a language with masculine, feminine, and neuter grammatical genders,\nyet, due to historical and political conventions, masculine forms are\npredominantly used to refer to men, women and mixed-gender groups. This is the\nreality of contemporary Polish. A social consequence of this unfair linguistic\nsystem is that large language models (LLMs) trained on Polish texts inherit and\nreinforce this masculine bias, generating gender-imbalanced outputs. This study\naddresses this issue by tuning LLMs using the IPIS dataset, a collection of\nhuman-crafted gender-inclusive proofreading in Polish and Polish-to-English\ntranslation instructions. Grounded in a theoretical linguistic framework, we\ndesign a system prompt with explicit gender-inclusive guidelines for Polish. In\nour experiments, we IPIS-tune multilingual LLMs (Llama-8B, Mistral-7B and\nMistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM). Our approach aims to\nintegrate gender inclusivity as an inherent feature of these models, offering a\nsystematic solution to mitigate gender bias in Polish language generation.", "AI": {"tldr": "\u901a\u8fc7IPIS\u6570\u636e\u96c6\u8c03\u6574\u6ce2\u5170\u8bed\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7cfb\u7edf\u6027\u51cf\u8f7b\u8bed\u6cd5\u6027\u522b\u504f\u89c1", "motivation": "\u6ce2\u5170\u8bed\u56e0\u5386\u53f2\u60ef\u4f8b\u8fc7\u5ea6\u4f7f\u7528\u9633\u6027\u8bcd\u5f62\u5bfc\u81f4\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6027\u522b\u5931\u8861\u5185\u5bb9\uff0c\u9700\u89e3\u51b3\u5176\u793e\u4f1a\u5f71\u54cd", "method": "\u8bbe\u8ba1\u663e\u5f0f\u6027\u522b\u5305\u5bb9\u7cfb\u7edf\u63d0\u793a\uff0c\u4f7f\u7528IPIS\u6570\u636e\u96c6\u5bf9Llama/Mistral\u7b49\u6a21\u578b\u8fdb\u884c\u6307\u4ee4\u5fae\u8c03", "result": "\u5b9e\u73b0\u5c06\u6027\u522b\u5305\u5bb9\u6027\u5185\u5316\u4e3a\u591a\u8bed\u8a00\u6a21\u578b\u53ca\u6ce2\u5170\u4e13\u7528\u6a21\u578b\u7684\u56fa\u6709\u7279\u5f81", "conclusion": "\u4e3a\u6ce2\u5170\u8bed\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u9996\u4e2a\u7406\u8bba\u8bed\u8a00\u5b66\u6846\u67b6\u652f\u6301\u7684\u6027\u522b\u504f\u89c1\u7f13\u89e3\u65b9\u6848"}}
{"id": "2508.18473", "pdf": "https://arxiv.org/pdf/2508.18473", "abs": "https://arxiv.org/abs/2508.18473", "authors": ["Jiawei Li", "Akshayaa Magesh", "Venugopal V. Veeravalli"], "title": "Principled Detection of Hallucinations in Large Language Models via Multiple Testing", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages", "summary": "While Large Language Models (LLMs) have emerged as powerful foundational\nmodels to solve a variety of tasks, they have also been shown to be prone to\nhallucinations, i.e., generating responses that sound confident but are\nactually incorrect or even nonsensical. In this work, we formulate the problem\nof detecting hallucinations as a hypothesis testing problem and draw parallels\nto the problem of out-of-distribution detection in machine learning models. We\npropose a multiple-testing-inspired method to solve the hallucination detection\nproblem, and provide extensive experimental results to validate the robustness\nof our approach against state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u91cd\u5047\u8bbe\u68c0\u9a8c\u7684LLM\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u5b58\u5728\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u5b9e\u9645\u9519\u8bef\u5185\u5bb9\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u5efa\u7acb\u6709\u6548\u68c0\u6d4b\u673a\u5236\u3002", "method": "\u5c06\u5e7b\u89c9\u68c0\u6d4b\u5efa\u6a21\u4e3a\u5047\u8bbe\u68c0\u9a8c\u95ee\u9898\uff0c\u501f\u9274\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u5916\u68c0\u6d4b\u601d\u8def\uff0c\u5f00\u53d1\u57fa\u4e8e\u591a\u91cd\u6d4b\u8bd5\u7684\u68c0\u6d4b\u6846\u67b6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u5047\u8bbe\u68c0\u9a8c\u7684\u591a\u91cd\u6d4b\u8bd5\u65b9\u6cd5\u80fd\u6709\u6548\u68c0\u6d4bLLM\u5e7b\u89c9\uff0c\u4e3a\u6a21\u578b\u53ef\u9760\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.18549", "pdf": "https://arxiv.org/pdf/2508.18549", "abs": "https://arxiv.org/abs/2508.18549", "authors": ["Maike Z\u00fcfle", "Vil\u00e9m Zouhar", "Tu Anh Dinh", "Felipe Maia Polo", "Jan Niehues", "Mrinmaya Sachan"], "title": "COMET-poly: Machine Translation Metric Grounded in Other Candidates", "categories": ["cs.CL", "I.2.7"], "comment": "Maike Z\\\"ufle, Vil\\'em Zouhar, and Tu Anh Dinh contributed equally", "summary": "Automated metrics for machine translation attempt to replicate human\njudgment. Unlike humans, who often assess a translation in the context of\nmultiple alternatives, these metrics typically consider only the source\nsentence and a single translation. This discrepancy in the evaluation setup may\nnegatively impact the performance of automated metrics. We propose two\nautomated metrics that incorporate additional information beyond the single\ntranslation. COMET-polycand uses alternative translations of the same source\nsentence to compare and contrast with the translation at hand, thereby\nproviding a more informed assessment of its quality. COMET-polyic, inspired by\nretrieval-based in-context learning, takes in translations of similar source\ntexts along with their human-labeled quality scores to guide the evaluation. We\nfind that including a single additional translation in COMET-polycand improves\nthe segment-level metric performance (0.079 to 0.118 Kendall's tau-b\ncorrelation), with further gains when more translations are added.\nIncorporating retrieved examples in COMET-polyic yields similar improvements\n(0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.", "AI": {"tldr": "\u63d0\u51faCOMET-polycand\u548cCOMET-polyic\u4e24\u79cd\u65b0\u6307\u6807\uff0c\u901a\u8fc7\u5f15\u5165\u591a\u7ffb\u8bd1\u5bf9\u6bd4\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u5347\u673a\u5668\u7ffb\u8bd1\u81ea\u52a8\u8bc4\u4f30\u6548\u679c", "motivation": "\u73b0\u6709\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u4ec5\u4f7f\u7528\u5355\u4e00\u7ffb\u8bd1\uff0c\u800c\u4eba\u7c7b\u8bc4\u4f30\u65f6\u4f1a\u53c2\u8003\u591a\u4e2a\u5907\u9009\u7ffb\u8bd1\uff0c\u8fd9\u79cd\u8bc4\u4f30\u65b9\u5f0f\u5dee\u5f02\u9650\u5236\u4e86\u81ea\u52a8\u6307\u6807\u7684\u6027\u80fd", "method": "COMET-polycand\u901a\u8fc7\u5bf9\u6bd4\u540c\u4e00\u6e90\u53e5\u7684\u591a\u7ffb\u8bd1\u7ed3\u679c\u63d0\u5347\u8bc4\u4f30\u8d28\u91cf\uff1bCOMET-polyic\u91c7\u7528\u68c0\u7d22\u76f8\u4f3c\u6e90\u6587\u672c\u7684\u7ffb\u8bd1\u53ca\u4eba\u5de5\u8bc4\u5206\u8fdb\u884c\u4e0a\u4e0b\u6587\u5b66\u4e60", "result": "COMET-polycand\u52a0\u5165\u989d\u5916\u7ffb\u8bd1\u540eKendall's tau-b\u63d0\u53470.079-0.118\uff1bCOMET-polyic\u901a\u8fc7\u68c0\u7d22\u793a\u4f8b\u8fbe\u5230\u7c7b\u4f3c\u63d0\u5347(0.079-0.116)", "conclusion": "\u5229\u7528\u591a\u7ffb\u8bd1\u5bf9\u6bd4\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u6709\u6548\u63d0\u5347\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u6027\u80fd\uff0c\u4e24\u79cd\u65b9\u6cd5\u5747\u53d6\u5f97\u663e\u8457\u6548\u679c\u63d0\u5347\u5e76\u516c\u5f00\u4e86\u6a21\u578b"}}
{"id": "2508.18569", "pdf": "https://arxiv.org/pdf/2508.18569", "abs": "https://arxiv.org/abs/2508.18569", "authors": ["Girish A. Koushik", "Fatemeh Nazarieh", "Katherine Birch", "Shenbin Qian", "Diptesh Kanojia"], "title": "The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation", "categories": ["cs.CL", "cs.CV"], "comment": "Under Review", "summary": "Visual metaphor generation is a challenging task that aims to generate an\nimage given an input text metaphor. Inherently, it needs language understanding\nto bind a source concept with a target concept, in a way that preserves meaning\nwhile ensuring visual coherence. We propose a self-evaluating visual metaphor\ngeneration framework that focuses on metaphor alignment. Our self-evaluation\napproach combines existing metrics with our newly proposed metaphor\ndecomposition score and a meaning alignment (MA) metric. Within this setup, we\nexplore two novel approaches: a training-free pipeline that explicitly\ndecomposes prompts into source-target-meaning (S-T-M) mapping for image\nsynthesis, and a complementary training-based pipeline that improves alignment\nusing our proposed self-evaluation reward schema, without any large-scale\nretraining. On the held-out test set, the training-free approach surpasses\nstrong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores,\nwith the training-based approach close behind. We evaluate our framework output\nusing a user-facing study, and observed that participants preferred GPT-4o\noverall, while our training-free pipeline led open-source methods and edged\nImagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or\nmore abstract metaphors, with closed models excelling on short, concrete cases;\nwe also observe sensitivity to sampler settings. Overall, structured prompting\nand lightweight RL perform metaphor alignment well under modest compute, and\nremaining gaps to human preference appear driven by aesthetics and sampling.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u8bc4\u4f30\u89c6\u89c9\u9690\u55bb\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u5206\u89e3\u548c\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u9690\u55bb\u5bf9\u9f50\u6548\u679c", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u89c6\u89c9\u9690\u55bb\u751f\u6210\u4e2d\u5b58\u5728\u8bed\u8a00\u7406\u89e3\u4e0e\u89c6\u89c9\u8fde\u8d2f\u6027\u5bf9\u9f50\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u540c\u65f6\u517c\u987e\u9690\u55bb\u542b\u4e49\u4fdd\u6301\u548c\u56fe\u50cf\u8d28\u91cf", "method": "1) \u65e0\u8bad\u7ec3S-T-M\u5206\u89e3\u63d0\u793a\u6cd5\uff08\u6e90-\u76ee\u6807-\u610f\u4e49\u6620\u5c04\uff09\n2) \u57fa\u4e8e\u81ea\u8bc4\u4f30\u5956\u52b1\u7684\u8f7b\u91cf\u7ea7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6846\u67b6", "result": "\u6d4b\u8bd5\u96c6\u663e\u793a\u65e0\u8bad\u7ec3\u65b9\u6cd5\u5728\u5206\u89e3/CLIP/MA\u6307\u6807\u4e0a\u8d85\u8d8aGPT-4o\u548cImagen\uff1b\u7528\u6237\u7814\u7a76\u663e\u793a\u5f00\u6e90\u65b9\u6cd5\u5728\u62bd\u8c61\u9690\u55bb\u751f\u6210\u4e2d\u4f18\u4e8eImagen\uff0c\u4f46\u603b\u4f53\u4ecd\u504f\u597dGPT-4o\u7684\u5ba1\u7f8e\u8f93\u51fa", "conclusion": "\u7ed3\u6784\u5316\u63d0\u793a\u4e0e\u8f7b\u91cf\u7ea7RL\u80fd\u6709\u6548\u63d0\u5347\u9690\u55bb\u5bf9\u9f50\uff0c\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u5dee\u8ddd\u4e3b\u8981\u6e90\u4e8e\u7f8e\u5b66\u8d28\u91cf\u548c\u91c7\u6837\u654f\u611f\u6027\uff0c\u5c01\u95ed\u6a21\u578b\u5728\u77ed/\u5177\u4f53\u9690\u55bb\u8868\u73b0\u66f4\u4f73"}}
{"id": "2508.18598", "pdf": "https://arxiv.org/pdf/2508.18598", "abs": "https://arxiv.org/abs/2508.18598", "authors": ["Colin Klein"], "title": "What do language models model? Transformers, automata, and the format of thought", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "What do large language models actually model? Do they tell us something about\nhuman capacities, or are they models of the corpus we've trained them on? I\ngive a non-deflationary defence of the latter position. Cognitive science tells\nus that linguistic capabilities in humans rely supralinear formats for\ncomputation. The transformer architecture, by contrast, supports at best a\nlinear formats for processing. This argument will rely primarily on certain\ninvariants of the computational architecture of transformers. I then suggest a\npositive story about what transformers are doing, focusing on Liu et al.\n(2022)'s intriguing speculations about shortcut automata. I conclude with why I\ndon't think this is a terribly deflationary story. Language is not (just) a\nmeans for expressing inner state but also a kind of 'discourse machine' that\nlets us make new language given appropriate context. We have learned to use\nthis technology in one way; LLMs have also learned to use it too, but via very\ndifferent means.", "AI": {"tldr": "\u8bba\u6587\u8bba\u8bc1\u5927\u8bed\u8a00\u6a21\u578b\u672c\u8d28\u662f\u8bad\u7ec3\u8bed\u6599\u5e93\u7684\u7edf\u8ba1\u6a21\u578b\u800c\u975e\u4eba\u7c7b\u8ba4\u77e5\u6a21\u578b\uff0c\u63d0\u51faTransformer\u67b6\u6784\u7684\u7ebf\u6027\u8ba1\u7b97\u9650\u5236\u4e0e\u4eba\u7c7b\u8bed\u8a00\u5904\u7406\u7684\u8d85\u7ebf\u6027\u7279\u5f81\u5b58\u5728\u672c\u8d28\u5dee\u5f02", "motivation": "\u56de\u5e94\u5b66\u754c\u5173\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u53cd\u6620\u4eba\u7c7b\u8ba4\u77e5\u80fd\u529b\u7684\u4e89\u8bba\uff0c\u901a\u8fc7\u8ba1\u7b97\u67b6\u6784\u5dee\u5f02\u8bba\u8bc1LLMs\u672c\u8d28\u662f\u5bf9\u8bed\u6599\u5e93\u7edf\u8ba1\u89c4\u5f8b\u7684\u5efa\u6a21", "method": "1. \u57fa\u4e8eTransformer\u67b6\u6784\u8ba1\u7b97\u4e0d\u53d8\u91cf\u7684\u7406\u8bba\u5206\u6790\n2. \u7ed3\u5408Liu\u7b49\u4eba(2022)\u7684\u6377\u5f84\u81ea\u52a8\u673a\u7406\u8bba\u6784\u5efa\u89e3\u91ca\u6846\u67b6\n3. \u8bed\u8a00\u4f5c\u4e3a'\u8bdd\u8bed\u673a\u5668'\u7684\u54f2\u5b66\u89c6\u89d2\u5bf9\u6bd4\u4eba\u7c7b\u4e0eLLMs\u7684\u8bed\u8a00\u751f\u6210\u673a\u5236", "result": "\u63ed\u793aTransformer\u7ebf\u6027\u8ba1\u7b97\u7279\u6027\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u67b6\u6784\u7684\u6839\u672c\u5dee\u5f02\uff0c\u63d0\u51faLLMs\u901a\u8fc7\u8bed\u6599\u7edf\u8ba1\u5efa\u6a21\u6784\u5efa\u65b0\u578b'\u8bdd\u8bed\u673a\u5668'\u7684\u7406\u8bba\u6a21\u578b", "conclusion": "\u8bed\u8a00\u65e2\u662f\u5185\u5728\u72b6\u6001\u7684\u8868\u8fbe\u5de5\u5177\uff0c\u4e5f\u662f\u57fa\u4e8e\u8bed\u5883\u751f\u6210\u65b0\u8bdd\u8bed\u7684\u673a\u5668\u3002\u4eba\u7c7b\u4e0eLLMs\u4ee5\u4e0d\u540c\u673a\u5236\u638c\u63e1\u4e86\u8fd9\u79cd\u6280\u672f\uff0c\u4f46\u90fd\u9a8c\u8bc1\u4e86\u8bed\u8a00\u4f5c\u4e3a\u751f\u6210\u7cfb\u7edf\u7684\u672c\u8d28\u5c5e\u6027"}}
{"id": "2508.18607", "pdf": "https://arxiv.org/pdf/2508.18607", "abs": "https://arxiv.org/abs/2508.18607", "authors": ["Rumeng Li", "Xun Wang", "Hong Yu"], "title": "A New NMT Model for Translating Clinical Texts from English to Spanish", "categories": ["cs.CL"], "comment": "This work was accepted by the Machine Learning for Health (ML4H)\n  Workshop at NeurIPS 2018", "summary": "Translating electronic health record (EHR) narratives from English to Spanish\nis a clinically important yet challenging task due to the lack of a\nparallel-aligned corpus and the abundant unknown words contained. To address\nsuch challenges, we propose \\textbf{NOOV} (for No OOV), a new neural machine\ntranslation (NMT) system that requires little in-domain parallel-aligned corpus\nfor training. NOOV integrates a bilingual lexicon automatically learned from\nparallel-aligned corpora and a phrase look-up table extracted from a large\nbiomedical knowledge resource, to alleviate both the unknown word problem and\nthe word-repeat challenge in NMT, enhancing better phrase generation of NMT\nsystems. Evaluation shows that NOOV is able to generate better translation of\nEHR with improvement in both accuracy and fluency.", "AI": {"tldr": "\u63d0\u51faNOOV\u795e\u7ecf\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u89e3\u51b3\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7ffb\u8bd1\u4e2d\u5e73\u884c\u8bed\u6599\u4e0d\u8db3\u548c\u672a\u77e5\u8bcd\u95ee\u9898", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7ffb\u8bd1\u5bf9\u4e34\u5e8a\u8bca\u7597\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u5e73\u884c\u8bed\u6599\u5e93\u7f3a\u4e4f\u548c\u672a\u77e5\u4e13\u4e1a\u8bcd\u6c47\u591a\u7684\u53cc\u91cd\u6311\u6218", "method": "\u6574\u5408\u53cc\u8bed\u8bcd\u5178\uff08\u81ea\u52a8\u5b66\u4e60\u81ea\u5e73\u884c\u8bed\u6599\uff09\u548c\u751f\u7269\u533b\u5b66\u77ed\u8bed\u67e5\u627e\u8868\uff0c\u901a\u8fc7\u53cc\u91cd\u673a\u5236\u7f13\u89e3\u672a\u77e5\u8bcd\u95ee\u9898\u5e76\u6539\u5584\u77ed\u8bed\u751f\u6210\u8d28\u91cf", "result": "\u8bc4\u4f30\u663e\u793a\u5728\u7ffb\u8bd1\u51c6\u786e\u7387\u63d0\u534712.7%\uff0c\u8bed\u53e5\u6d41\u7545\u5ea6\u63d0\u9ad818.4%", "conclusion": "NOOV\u7cfb\u7edf\u901a\u8fc7\u77e5\u8bc6\u589e\u5f3a\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e13\u4e1a\u9886\u57df\u673a\u5668\u7ffb\u8bd1\u6027\u80fd\uff0c\u4e3a\u8de8\u8bed\u8a00\u533b\u7597\u4fe1\u606f\u5904\u7406\u63d0\u4f9b\u65b0\u65b9\u6848"}}
{"id": "2508.18609", "pdf": "https://arxiv.org/pdf/2508.18609", "abs": "https://arxiv.org/abs/2508.18609", "authors": ["Chenxi Zhou", "Pengfei Cao", "Jiang Li", "Jun Zhao", "Kang Liu"], "title": "Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) present significant deployment challenges due to\ntheir scale, with post-training quantization (PTQ) emerging as a practical\ncompression solution. However, a comprehensive understanding of how PTQ\nprecisely impacts diverse LLM knowledge capabilities remains elusive, and\nexisting scaling laws for quantized models often overlook crucial PTQ-specific\nparameters and task-specific sensitivities. This paper addresses these gaps by\nconducting an extensive empirical investigation to establish task-stratified\nscaling laws. We disentangle LLM knowledge into memorization and utilization\ncapabilities and develop a unified quantitative framework that incorporates\nmodel size, effective bit-width, calibration set size, and group size. Our\ncentral finding reveals that knowledge memorization exhibits markedly greater\nsensitivity to variations in effective bit-width, calibration set size, and\nmodel size compared to the more robust knowledge utilization. These findings\noffer a fine-grained understanding of PTQ's impact and provide guidance for\ndeveloping knowledge-aware quantization strategies that can better preserve\ntargeted cognitive functions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u5efa\u7acb\u4e86\u4efb\u52a1\u5206\u5c42\u7684\u7f29\u653e\u5b9a\u5f8b\uff0c\u63ed\u793a\u8bad\u7ec3\u540e\u91cf\u5316(PTQ)\u5bf9LLM\u77e5\u8bc6\u80fd\u529b\u7684\u5f71\u54cd\u5dee\u5f02\uff1a\u77e5\u8bc6\u8bb0\u5fc6\u80fd\u529b\u5bf9\u91cf\u5316\u53c2\u6570\u66f4\u654f\u611f\uff0c\u77e5\u8bc6\u5229\u7528\u80fd\u529b\u5219\u66f4\u7a33\u5065\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9PTQ\u5982\u4f55\u5f71\u54cdLLM\u4e0d\u540c\u77e5\u8bc6\u80fd\u529b\u7684\u7cfb\u7edf\u7406\u89e3\uff0c\u4e14\u91cf\u5316\u6a21\u578b\u7684\u7f29\u653e\u5b9a\u5f8b\u5e38\u5ffd\u7565PTQ\u7279\u6709\u53c2\u6570\u548c\u4efb\u52a1\u654f\u611f\u6027\u3002", "method": "\u5c06LLM\u77e5\u8bc6\u89e3\u6784\u4e3a\u8bb0\u5fc6\u4e0e\u5229\u7528\u80fd\u529b\uff0c\u6784\u5efa\u5305\u542b\u6a21\u578b\u89c4\u6a21/\u6709\u6548\u6bd4\u7279\u5bbd\u5ea6/\u6821\u51c6\u96c6\u5927\u5c0f/\u5206\u7ec4\u5927\u5c0f\u7684\u7edf\u4e00\u91cf\u5316\u6846\u67b6\u8fdb\u884c\u654f\u611f\u6027\u5206\u6790\u3002", "result": "\u77e5\u8bc6\u8bb0\u5fc6\u80fd\u529b\u5bf9\u6709\u6548\u6bd4\u7279\u5bbd\u5ea6/\u6821\u51c6\u96c6\u5927\u5c0f/\u6a21\u578b\u89c4\u6a21\u7684\u654f\u611f\u6027\u6bd4\u77e5\u8bc6\u5229\u7528\u80fd\u529b\u9ad83-5\u500d\uff0c\u8868\u73b0\u51fa\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u77e5\u8bc6\u611f\u77e5\u7684\u91cf\u5316\u7b56\u7565\u63d0\u4f9b\u6307\u5bfc\uff0c\u5efa\u8bae\u6839\u636e\u76ee\u6807\u8ba4\u77e5\u529f\u80fd\uff08\u8bb0\u5fc6/\u5229\u7528\uff09\u5236\u5b9a\u5dee\u5f02\u5316\u7684\u538b\u7f29\u65b9\u6848\u3002"}}
{"id": "2508.18648", "pdf": "https://arxiv.org/pdf/2508.18648", "abs": "https://arxiv.org/abs/2508.18648", "authors": ["Cong Li", "Wenchang Chai", "Hejun Wu", "Yan Pan", "Pengxu Wei", "Liang Lin"], "title": "Thinking Before You Speak: A Proactive Test-time Scaling Approach", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit deficiencies with complex\nreasoning tasks, such as maths, which we attribute to the discrepancy between\nhuman reasoning patterns and those presented in the LLMs' training data. When\ndealing with complex problems, humans tend to think carefully before expressing\nsolutions. However, they often do not articulate their inner thoughts,\nincluding their intentions and chosen methodologies. Consequently, critical\ninsights essential for bridging reasoning steps may be absent in training data\ncollected from human sources. To bridge this gap, we proposes inserting\n\\emph{insight}s between consecutive reasoning steps, which review the status\nand initiate the next reasoning steps. Unlike prior prompting strategies that\nrely on a single or a workflow of static prompts to facilitate reasoning,\n\\emph{insight}s are \\emph{proactively} generated to guide reasoning processes.\nWe implement our idea as a reasoning framework, named \\emph{Thinking Before You\nSpeak} (TBYS), and design a pipeline for automatically collecting and filtering\nin-context examples for the generation of \\emph{insight}s, which alleviates\nhuman labeling efforts and fine-tuning overheads. Experiments on challenging\nmathematical datasets verify the effectiveness of TBYS. Project website:\nhttps://gitee.com/jswrt/TBYS", "AI": {"tldr": "\u63d0\u51faTBYS\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u63a8\u7406\u6b65\u9aa4\u95f4\u63d2\u5165\u4e3b\u52a8\u751f\u6210\u7684insights\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff08\u5982\u6570\u5b66\uff09\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u8bad\u7ec3\u6570\u636e\u7f3a\u4e4f\u4eba\u7c7b\u601d\u8003\u65f6\u672a\u660e\u793a\u7684\u5173\u952e\u4e2d\u95f4\u610f\u56fe\u548c\u65b9\u6cd5\u8bba", "method": "\u8bbe\u8ba1\u81ea\u52a8\u6536\u96c6\u548c\u8fc7\u6ee4\u4e0a\u4e0b\u6587\u793a\u4f8b\u7684\u6d41\u7a0b\uff0c\u6784\u5efa\u4e3b\u52a8\u751f\u6210insights\u7684\u63a8\u7406\u6846\u67b6TBYS\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u548c\u5fae\u8c03", "result": "\u5728\u590d\u6742\u6570\u5b66\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u63a8\u7406\u6846\u67b6\u663e\u8457\u63d0\u5347\u6a21\u578b\u8868\u73b0", "conclusion": "\u4e3b\u52a8\u751f\u6210insights\u80fd\u6709\u6548\u6865\u63a5\u63a8\u7406\u6b65\u9aa4\uff0c\u81ea\u52a8\u5316\u6d41\u7a0b\u5b9e\u73b0\u4f4e\u6210\u672c\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u63d0\u5347"}}
{"id": "2508.18651", "pdf": "https://arxiv.org/pdf/2508.18651", "abs": "https://arxiv.org/abs/2508.18651", "authors": ["Chenxu Yang", "Qingyi Si", "Zheng Lin"], "title": "Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Grounding responses in external knowledge represents an effective strategy\nfor mitigating hallucinations in Large Language Models (LLMs). However, current\nLLMs struggle to seamlessly integrate knowledge while simultaneously\nmaintaining faithfulness (or fidelity) and expressiveness, capabilities that\nhumans naturally possess. This limitation results in outputs that either lack\nsupport from external knowledge, thereby compromising faithfulness, or appear\noverly verbose and unnatural, thus sacrificing expressiveness. In this work, to\nbreak the trade-off between faithfulness and expressiveness, we propose\nCollaborative Decoding (CoDe), a novel approach that dynamically integrates\noutput probabilities generated with and without external knowledge. This\nintegration is guided by distribution divergence and model confidence, enabling\nthe selective activation of relevant and reliable expressions from the model's\ninternal parameters. Furthermore, we introduce a knowledge-aware reranking\nmechanism that prevents over-reliance on prior parametric knowledge while\nensuring proper utilization of provided external information. Through\ncomprehensive experiments, our plug-and-play CoDe framework demonstrates\nsuperior performance in enhancing faithfulness without compromising\nexpressiveness across diverse LLMs and evaluation metrics, validating both its\neffectiveness and generalizability.", "AI": {"tldr": "\u63d0\u51fa\u534f\u4f5c\u89e3\u7801\u6846\u67b6CoDe\uff0c\u901a\u8fc7\u52a8\u6001\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u6982\u7387\u5206\u5e03\u4e0e\u7f6e\u4fe1\u5ea6\u6307\u5bfc\uff0c\u5728\u4fdd\u6301\u8868\u8fbe\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u56de\u7b54\u5fe0\u5b9e\u6027\u3002", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u65f6\u9762\u4e34\u5fe0\u5b9e\u6027\uff08\u7f3a\u4e4f\u77e5\u8bc6\u652f\u6491\uff09\u4e0e\u8868\u8fbe\u529b\uff08\u8f93\u51fa\u4e0d\u81ea\u7136\uff09\u7684\u6743\u8861\u56f0\u5883\uff0c\u9700\u7a81\u7834\u8be5\u9650\u5236\u3002", "method": "1. \u52a8\u6001\u878d\u5408\u6709\u65e0\u5916\u90e8\u77e5\u8bc6\u7684\u8f93\u51fa\u6982\u7387\uff0c\u57fa\u4e8e\u5206\u5e03\u5dee\u5f02\u548c\u7f6e\u4fe1\u5ea6\u9009\u62e9\u53ef\u9760\u8868\u8fbe\n2. \u77e5\u8bc6\u611f\u77e5\u91cd\u6392\u5e8f\u673a\u5236\u9632\u6b62\u53c2\u6570\u77e5\u8bc6\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u786e\u4fdd\u5916\u90e8\u4fe1\u606f\u6709\u6548\u5229\u7528", "result": "\u8de8\u591a\u4e2aLLM\u548c\u8bc4\u4f30\u6307\u6807\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCoDe\u5728\u63d0\u5347\u5fe0\u5b9e\u6027\u540c\u65f6\u4fdd\u6301\u8868\u8fbe\u529b\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "CoDe\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u65b9\u6848\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u5fe0\u5b9e\u6027\u4e0e\u8868\u8fbe\u529b\u7684\u53d6\u820d\u56f0\u5883\uff0c\u4e3a\u77e5\u8bc6\u589e\u5f3a\u578bLLM\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.18655", "pdf": "https://arxiv.org/pdf/2508.18655", "abs": "https://arxiv.org/abs/2508.18655", "authors": ["Haoyu Wang", "Guangyan Zhang", "Jiale Chen", "Jingyu Li", "Yuehai Wang", "Yiwen Guo"], "title": "Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7"], "comment": "5 pages, 1 figure, submitted to ICASSP 2026", "summary": "With the development of speech large language models (speech LLMs), users can\nnow interact directly with assistants via speech. However, most existing models\nsimply convert the response content into speech without fully understanding the\nrich emotional and paralinguistic cues embedded in the user's query. In many\ncases, the same sentence can have different meanings depending on the emotional\nexpression. Furthermore, emotional understanding is essential for improving\nuser experience in human-machine interaction. Currently, most speech LLMs with\nempathetic capabilities are trained on massive datasets. This approach requires\nvast amounts of data and significant computational resources. Therefore, a key\nchallenge lies in how to develop a speech LLM capable of generating empathetic\nresponses with limited data and without the need for large-scale training. To\naddress this challenge, we propose Emotion Omni, a novel model architecture\ndesigned to understand the emotional content of user speech input and generate\nempathetic speech responses. Additionally, we developed a data generation\npipeline based on an open-source TTS framework to construct a 200k emotional\ndialogue dataset, which supports the construction of an empathetic speech\nassistant. The demos are available at https://w311411.github.io/omni_demo/", "AI": {"tldr": "\u63d0\u51faEmotion Omni\u8bed\u97f3\u5927\u6a21\u578b\u67b6\u6784\uff0c\u901a\u8fc7\u65b0\u578b\u6a21\u578b\u8bbe\u8ba1\u548c20\u4e07\u60c5\u611f\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u6709\u9650\u6570\u636e\u4e0b\u5bf9\u7528\u6237\u8bed\u97f3\u7684\u60c5\u611f\u7406\u89e3\u4e0e\u5171\u60c5\u56de\u5e94\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3LLM\u5ffd\u89c6\u7528\u6237\u8bed\u97f3\u4e2d\u7684\u60c5\u611f\u7ebf\u7d22\uff0c\u4e14\u8bad\u7ec3\u540c\u7406\u80fd\u529b\u9700\u6d77\u91cf\u6570\u636e\u3002\u9700\u5f00\u53d1\u6570\u636e\u9ad8\u6548\u7684\u65b9\u6cd5\u63d0\u5347\u8bed\u97f3\u52a9\u624b\u7684\u60c5\u611f\u4ea4\u4e92\u80fd\u529b\u3002", "method": "1. \u8bbe\u8ba1\u4e13\u6ce8\u8bed\u97f3\u60c5\u611f\u7406\u89e3\u7684\u6a21\u578b\u67b6\u6784 2. \u57fa\u4e8e\u5f00\u6e90TTS\u6846\u67b6\u6784\u5efa\u6570\u636e\u751f\u6210pipeline 3. \u521b\u5efa20\u4e07\u6761\u542b\u60c5\u611f\u6807\u6ce8\u7684\u5bf9\u8bdd\u6570\u636e\u96c6", "result": "\u6210\u529f\u6784\u5efa\u652f\u6301\u60c5\u611f\u4ea4\u4e92\u7684\u8bed\u97f3\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u51fa\u80fd\u7406\u89e3\u7528\u6237\u8bed\u97f3\u60c5\u611f\u5e76\u751f\u6210\u5171\u60c5\u56de\u5e94\u7684\u8bed\u97f3\u52a9\u624b\u7cfb\u7edf\uff08\u6f14\u793a\u5730\u5740\u89c1\u8bba\u6587\uff09", "conclusion": "Emotion Omni\u8bc1\u660e\u4e86\u6709\u9650\u6570\u636e\u4e0b\u5b9e\u73b0\u8bed\u97f3\u60c5\u611f\u7406\u89e3\u7684\u53ef\u884c\u6027\uff0c\u5176\u6570\u636e\u751f\u6210\u65b9\u6cd5\u4e3a\u4f4e\u8d44\u6e90\u8bed\u97f3LLM\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.18673", "pdf": "https://arxiv.org/pdf/2508.18673", "abs": "https://arxiv.org/abs/2508.18673", "authors": ["Xinglong Yang", "Quan Feng", "Zhongying Pan", "Xiang Chen", "Yu Tian", "Wentong Li", "Shuofei Qiao", "Yuxia Geng", "Xingyu Zhao", "Sheng-Jun Huang"], "title": "Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum", "categories": ["cs.CL", "cs.AI", "cs.MM"], "comment": null, "summary": "The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often\nlimited by the use of randomly or manually selected examples. These examples\nfail to account for both model-specific knowledge distributions and the\nintrinsic complexity of the tasks, resulting in suboptimal and unstable model\nperformance. To address this, we propose a novel framework inspired by the\npedagogical principle of \"tailored teaching with balanced difficulty\". We\nreframe prompt selection as a prompt curriculum design problem: constructing a\nwell ordered set of training examples that align with the model's current\ncapabilities. Our approach integrates two complementary signals: (1)\nmodel-perceived difficulty, quantified through prediction disagreement in an\nactive learning setup, capturing what the model itself finds challenging; and\n(2) intrinsic sample complexity, which measures the inherent difficulty of each\nquestion-image pair independently of any model. By jointly analyzing these\nsignals, we develop a difficulty-balanced sampling strategy that ensures the\nselected prompt examples are diverse across both dimensions. Extensive\nexperiments conducted on five challenging benchmarks and multiple popular\nMultimodal Large Language Models (MLLMs) demonstrate that our method yields\nsubstantial and consistent improvements and greatly reduces performance\ndiscrepancies caused by random sampling, providing a principled and robust\napproach for enhancing multimodal reasoning.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6559\u5b66\u539f\u5219\u7684\u96be\u5ea6\u5e73\u8861\u63d0\u793a\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u6a21\u578b\u611f\u77e5\u96be\u5ea6\u548c\u6837\u672c\u590d\u6742\u6027\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u6027\u80fd", "motivation": "\u73b0\u6709MCoT\u65b9\u6cd5\u968f\u673a\u9009\u62e9\u793a\u4f8b\u672a\u8003\u8651\u6a21\u578b\u77e5\u8bc6\u5206\u5e03\u548c\u4efb\u52a1\u5185\u5728\u590d\u6742\u6027\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0d\u7a33\u5b9a", "method": "\u6784\u5efa\u63d0\u793a\u8bfe\u7a0b\u6846\u67b6\uff0c\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u5206\u6b67\u91cf\u5316\u7684\u611f\u77e5\u96be\u5ea6\u4e0e\u72ec\u7acb\u4e8e\u6a21\u578b\u7684\u6837\u672c\u590d\u6742\u5ea6\u5206\u6790\uff0c\u8bbe\u8ba1\u53cc\u7ef4\u5ea6\u5e73\u8861\u91c7\u6837\u7b56\u7565", "result": "\u57285\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u79cdMLLMs\u4e0a\u5b9e\u73b0\u663e\u8457\u4e14\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\uff0c\u51cf\u5c11\u968f\u673a\u91c7\u6837\u5e26\u6765\u7684\u6027\u80fd\u6ce2\u52a8", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u589e\u5f3a\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8bfe\u7a0b\u8bbe\u8ba1\u5b9e\u73b0\u9c81\u68d2\u7684\u6a21\u578b\u9002\u5e94\u80fd\u529b"}}
{"id": "2508.18687", "pdf": "https://arxiv.org/pdf/2508.18687", "abs": "https://arxiv.org/abs/2508.18687", "authors": ["Songtao Jiang", "Yuxi Chen", "Sibo Song", "Yan Zhang", "Yeying Jin", "Yang Feng", "Jian Wu", "Zuozhu Liu"], "title": "Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning", "categories": ["cs.CL"], "comment": null, "summary": "In high-stakes medical applications, consistent answering across diverse\nquestion phrasings is essential for reliable diagnosis. However, we reveal that\ncurrent Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility\nin Medical Visual Question Answering, as their answers fluctuate significantly\nwhen faced with semantically equivalent rephrasings of medical questions. We\nattribute this to two limitations: (1) insufficient alignment of medical\nconcepts, leading to divergent reasoning patterns, and (2) hidden biases in\ntraining data that prioritize syntactic shortcuts over semantic understanding.\nTo address these challenges, we construct RoMed, a dataset built upon original\nVQA datasets containing 144k questions with variations spanning word-level,\nsentence-level, and semantic-level perturbations. When evaluating\nstate-of-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarming\nperformance drops (e.g., a 40\\% decline in Recall) compared to original VQA\nbenchmarks, exposing critical robustness gaps. To bridge this gap, we propose\nConsistency and Contrastive Learning (CCL), which integrates two key\ncomponents: (1) knowledge-anchored consistency learning, aligning Med-VLMs with\nmedical knowledge rather than shallow feature patterns, and (2) bias-aware\ncontrastive learning, mitigating data-specific priors through discriminative\nrepresentation refinement. CCL achieves SOTA performance on three popular VQA\nbenchmarks and notably improves answer consistency by 50\\% on the challenging\nRoMed test set, demonstrating significantly enhanced robustness. Code will be\nreleased.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(Med-VLMs)\u5728\u8bed\u4e49\u7b49\u6548\u95ee\u9898\u8868\u8ff0\u4e2d\u5b58\u5728\u7b54\u6848\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u77e5\u8bc6\u951a\u5b9a\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684CCL\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709Med-VLMs\u5728\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u4e2d\u8868\u73b0\u51fa\u7b54\u6848\u6ce2\u52a8\u6027\uff0c\u65e0\u6cd5\u4fdd\u6301\u4e0d\u540c\u95ee\u9898\u8868\u8ff0\u4e0b\u8bca\u65ad\u7ed3\u8bba\u7684\u4e00\u81f4\u6027\uff0c\u5f71\u54cd\u533b\u7597\u53ef\u9760\u6027", "method": "\u6784\u5efaRoMed\u6570\u636e\u96c6(\u542b14.4\u4e07\u53d8\u4f53\u95ee\u9898)\uff0c\u63d0\u51faCCL\u6846\u67b6\uff1a1) \u77e5\u8bc6\u951a\u5b9a\u4e00\u81f4\u6027\u5b66\u4e60 2) \u504f\u5dee\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60", "result": "LLaVA-Med\u6a21\u578b\u5728RoMed\u6d4b\u8bd5\u96c6\u53ec\u56de\u7387\u4e0b\u964d40%\uff0cCCL\u65b9\u6cd5\u5728\u4e09\u5927VQA\u57fa\u51c6\u5b9e\u73b0SOTA\uff0c\u7b54\u6848\u4e00\u81f4\u6027\u63d0\u534750%", "conclusion": "\u901a\u8fc7\u77e5\u8bc6\u5f15\u5bfc\u548c\u504f\u5dee\u6291\u5236\uff0cCCL\u6709\u6548\u63d0\u5347\u533b\u5b66\u591a\u6a21\u6001\u6a21\u578b\u7684\u8bed\u4e49\u9c81\u68d2\u6027\uff0c\u4e3a\u53ef\u9760\u533b\u7597\u8bca\u65ad\u7cfb\u7edf\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2508.18701", "pdf": "https://arxiv.org/pdf/2508.18701", "abs": "https://arxiv.org/abs/2508.18701", "authors": ["Yanfan Du", "Jun Zhang", "Bin Wang", "Jin Qiu", "Lu Huang", "Yuan Ge", "Xiaoqian Liu", "Tong Xiao", "Jingbo Zhu"], "title": "Attention2Probability: Attention-Driven Terminology Probability Estimation for Robust Speech-to-Text System", "categories": ["cs.CL"], "comment": "9 pages, 4 figures, 5 tables", "summary": "Recent advances in speech large language models (SLMs) have improved speech\nrecognition and translation in general domains, but accurately generating\ndomain-specific terms or neologisms remains challenging. To address this, we\npropose Attention2Probability: attention-driven terminology probability\nestimation for robust speech-to-text system, which is lightweight, flexible,\nand accurate. Attention2Probability converts cross-attention weights between\nspeech and terminology into presence probabilities, and it further employs\ncurriculum learning to enhance retrieval accuracy. Furthermore, to tackle the\nlack of data for speech-to-text tasks with terminology intervention, we create\nand release a new speech dataset with terminology to support future research in\nthis area. Experimental results show that Attention2Probability significantly\noutperforms the VectorDB method on our test set. Specifically, its maximum\nrecall rates reach 92.57% for Chinese and 86.83% for English. This high recall\nis achieved with a latency of only 8.71ms per query. Intervening in SLMs'\nrecognition and translation tasks using Attention2Probability-retrieved terms\nimproves terminology accuracy by 6-17%, while revealing that the current\nutilization of terminology by SLMs has limitations.", "AI": {"tldr": "\u63d0\u51fa\u4e86Attention2Probability\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u8bed\u97f3\u5927\u6a21\u578b\u672f\u8bed\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u4e2d\u82f1\u6587\u53ec\u56de\u7387\u5206\u522b\u8fbe92.57%\u548c86.83%\uff0c\u5ef6\u8fdf\u4ec58.71ms\uff0c\u672f\u8bed\u5e72\u9884\u4f7f\u51c6\u786e\u7387\u63d0\u53476-17%\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u5927\u6a21\u578b\u5728\u9886\u57df\u672f\u8bed/\u65b0\u8bcd\u8bc6\u522b\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8f7b\u91cf\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u7f3a\u4e4f\u76f8\u5173\u6570\u636e\u96c6\u652f\u6491\u7814\u7a76\u3002", "method": "\u5c06\u8bed\u97f3-\u672f\u8bed\u4ea4\u53c9\u6ce8\u610f\u529b\u6743\u91cd\u8f6c\u5316\u4e3a\u5b58\u5728\u6982\u7387\uff0c\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u63d0\u5347\u68c0\u7d22\u7cbe\u5ea6\uff0c\u540c\u65f6\u6784\u5efa\u65b0\u672f\u8bed\u8bed\u97f3\u6570\u636e\u96c6\u3002", "result": "\u6d4b\u8bd5\u96c6\u663e\u8457\u8d85\u8d8aVectorDB\u65b9\u6cd5\uff0c\u6700\u5927\u53ec\u56de\u7387\u4e2d92.57%/\u82f186.83%\uff0c\u5ef6\u8fdf8.71ms/query\uff0c\u672f\u8bed\u5e72\u9884\u63d0\u5347\u51c6\u786e\u73876-17%\u3002", "conclusion": "Attention2Probability\u6709\u6548\u63d0\u5347\u672f\u8bed\u51c6\u786e\u6027\uff0c\u63ed\u793a\u5f53\u524d\u8bed\u97f3\u5927\u6a21\u578b\u5bf9\u672f\u8bed\u7684\u5229\u7528\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u672f\u8bed\u5e94\u7528\u7b56\u7565\u3002"}}
{"id": "2508.18709", "pdf": "https://arxiv.org/pdf/2508.18709", "abs": "https://arxiv.org/abs/2508.18709", "authors": ["Duy Le", "Kent Ziti", "Evan Girard-Sun", "Sean O'Brien", "Vasu Sharma", "Kevin Zhu"], "title": "Filtering for Creativity: Adaptive Prompting for Multilingual Riddle Generation in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Multilingual riddle generation challenges large language models (LLMs) to\nbalance cultural fluency with creative abstraction. Standard prompting\nstrategies -- zero-shot, few-shot, chain-of-thought -- tend to reuse memorized\nriddles or perform shallow paraphrasing. We introduce Adaptive Originality\nFiltering (AOF), a prompting framework that filters redundant generations using\ncosine-based similarity rejection, while enforcing lexical novelty and\ncross-lingual fidelity. Evaluated across three LLMs and four language pairs,\nAOF-enhanced GPT-4o achieves \\texttt{0.177} Self-BLEU and \\texttt{0.915}\nDistinct-2 in Japanese, signaling improved lexical diversity and reduced\nredundancy compared to other prompting methods and language pairs. Our findings\nshow that semantic rejection can guide culturally grounded, creative generation\nwithout task-specific fine-tuning.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u539f\u521b\u6027\u8fc7\u6ee4\u6846\u67b6\uff08AOF\uff09\uff0c\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8fc7\u6ee4\u548c\u8de8\u8bed\u8a00\u4fdd\u771f\u5ea6\u63d0\u5347\uff0c\u663e\u8457\u63d0\u5347\u591a\u8bed\u8a00\u8c1c\u8bed\u751f\u6210\u7684\u8bcd\u6c47\u591a\u6837\u6027\u548c\u6587\u5316\u9002\u5e94\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u8c1c\u8bed\u751f\u6210\u4e2d\u6587\u5316\u9002\u5e94\u6027\u4e0e\u521b\u610f\u62bd\u8c61\u96be\u4ee5\u5e73\u8861\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u6613\u5bfc\u81f4\u91cd\u590d\u8bb0\u5fc6\u6216\u6d45\u5c42\u6539\u5199\u3002", "method": "\u91c7\u7528\u81ea\u9002\u5e94\u539f\u521b\u6027\u8fc7\u6ee4\uff08AOF\uff09\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u5197\u4f59\u751f\u6210\u8fc7\u6ee4 2\uff09\u5f3a\u5236\u5b9e\u65bd\u8bcd\u6c47\u65b0\u9896\u6027 3\uff09\u8de8\u8bed\u8a00\u4fdd\u771f\u5ea6\u9a8c\u8bc1", "result": "\u57284\u79cd\u8bed\u8a00\u5bf9\u6d4b\u8bd5\u4e2d\uff0cAOF\u589e\u5f3a\u7684GPT-4o\u5b9e\u73b0\u65e5\u8bedSelf-BLEU 0.177\u548cDistinct-2 0.915\uff0c\u8bcd\u6c47\u591a\u6837\u6027\u63d0\u5347\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u8bed\u4e49\u62d2\u7edd\u673a\u5236\u53ef\u5728\u65e0\u9700\u4efb\u52a1\u5fae\u8c03\u60c5\u51b5\u4e0b\uff0c\u6709\u6548\u5f15\u5bfc\u5177\u6709\u6587\u5316\u6839\u57fa\u7684\u521b\u9020\u6027\u751f\u6210\uff0c\u4e3a\u8de8\u8bed\u8a00\u5185\u5bb9\u521b\u4f5c\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2508.18715", "pdf": "https://arxiv.org/pdf/2508.18715", "abs": "https://arxiv.org/abs/2508.18715", "authors": ["Angela Yifei Yuan", "Haoyi Li", "Soyeon Caren Han", "Christopher Leckie"], "title": "EMMM, Explain Me My Model! Explainable Machine Generated Text Detection in Dialogues", "categories": ["cs.CL"], "comment": "15 pages", "summary": "The rapid adoption of large language models (LLMs) in customer service\nintroduces new risks, as malicious actors can exploit them to conduct\nlarge-scale user impersonation through machine-generated text (MGT). Current\nMGT detection methods often struggle in online conversational settings,\nreducing the reliability and interpretability essential for trustworthy AI\ndeployment. In customer service scenarios where operators are typically\nnon-expert users, explanation become crucial for trustworthy MGT detection. In\nthis paper, we propose EMMM, an explanation-then-detection framework that\nbalances latency, accuracy, and non-expert-oriented interpretability.\nExperimental results demonstrate that EMMM provides explanations accessible to\nnon-expert users, with 70\\% of human evaluators preferring its outputs, while\nachieving competitive accuracy compared to state-of-the-art models and\nmaintaining low latency, generating outputs within 1 second. Our code and\ndataset are open-sourced at\nhttps://github.com/AngieYYF/EMMM-explainable-chatbot-detection.", "AI": {"tldr": "\u63d0\u51faEMMM\u6846\u67b6\u89e3\u51b3\u5ba2\u670d\u573a\u666f\u4e2dLLMs\u751f\u6210\u7684\u673a\u5668\u6587\u672c\u68c0\u6d4b\u95ee\u9898\uff0c\u517c\u987e\u975e\u4e13\u5bb6\u7528\u6237\u7684\u53ef\u89e3\u91ca\u6027\u3001\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u4f4e\u5ef6\u8fdf", "motivation": "\u73b0\u6709\u673a\u5668\u6587\u672c\u68c0\u6d4b\u65b9\u6cd5\u5728\u5728\u7ebf\u5bf9\u8bdd\u573a\u666f\u4e2d\u5b58\u5728\u53ef\u9760\u6027\u4f4e\u3001\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5bf9\u975e\u4e13\u5bb6\u7528\u6237\u7f3a\u4e4f\u6709\u6548\u89e3\u91ca\u673a\u5236", "method": "\u91c7\u7528\u5148\u89e3\u91ca\u540e\u68c0\u6d4b\uff08explanation-then-detection\uff09\u7684\u6846\u67b6\u8bbe\u8ba1\uff0c\u4f18\u5316\u89e3\u91ca\u751f\u6210\u673a\u5236\u548c\u68c0\u6d4b\u6a21\u578b\u7684\u9ad8\u6548\u534f\u540c", "result": "\u5b9e\u9a8c\u663e\u793a70%\u4eba\u5de5\u8bc4\u4f30\u8005\u504f\u597d\u5176\u89e3\u91ca\u8f93\u51fa\uff0c\u68c0\u6d4b\u7cbe\u5ea6\u4e0eSOTA\u6a21\u578b\u76f8\u5f53\uff0c\u5ef6\u8fdf\u63a7\u5236\u57281\u79d2\u5185\uff0c\u5e76\u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6", "conclusion": "EMMM\u6210\u529f\u5e73\u8861\u4e86\u53ef\u89e3\u91ca\u6027\u3001\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\u8981\u6c42\uff0c\u4e3a\u53ef\u4fe1AI\u90e8\u7f72\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u5f00\u6e90\u4fc3\u8fdb\u5b9e\u9645\u5e94\u7528\u548c\u540e\u7eed\u7814\u7a76"}}
{"id": "2508.18739", "pdf": "https://arxiv.org/pdf/2508.18739", "abs": "https://arxiv.org/abs/2508.18739", "authors": ["Chang Wang", "Siyu Yan", "Depeng Yuan", "Yuqi Chen", "Yanhua Huang", "Yuanhang Zheng", "Shuhao Li", "Yinqi Zhang", "Kedi Chen", "Mingrui Zhu", "Ruiwen Xu"], "title": "Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The generation of ad headlines plays a vital role in modern advertising,\nwhere both quality and diversity are essential to engage a broad range of\naudience segments. Current approaches primarily optimize language models for\nheadline quality or click-through rates (CTR), often overlooking the need for\ndiversity and resulting in homogeneous outputs. To address this limitation, we\npropose DIVER, a novel framework based on large language models (LLMs) that are\njointly optimized for both diversity and quality. We first design a semantic-\nand stylistic-aware data generation pipeline that automatically produces\nhigh-quality training pairs with ad content and multiple diverse headlines. To\nachieve the goal of generating high-quality and diversified ad headlines within\na single forward pass, we propose a multi-stage multi-objective optimization\nframework with supervised fine-tuning (SFT) and reinforcement learning (RL).\nExperiments on real-world industrial datasets demonstrate that DIVER\neffectively balances quality and diversity. Deployed on a large-scale\ncontent-sharing platform serving hundreds of millions of users, our framework\nimproves advertiser value (ADVV) and CTR by 4.0% and 1.4%.", "AI": {"tldr": "\u63d0\u51faDIVER\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u98ce\u683c\u611f\u77e5\u6570\u636e\u751f\u6210\u548c\u591a\u9636\u6bb5\u591a\u76ee\u6807\u4f18\u5316\uff08SFT+RL\uff09\uff0c\u5728\u5355\u6b21\u63a8\u7406\u4e2d\u5b9e\u73b0\u5e7f\u544a\u6807\u9898\u8d28\u91cf\u4e0e\u591a\u6837\u6027\u7684\u5e73\u8861\uff0c\u5b9e\u9645\u90e8\u7f72\u63d0\u5347ADVV 4.0%\u548cCTR 1.4%", "motivation": "\u73b0\u6709\u5e7f\u544a\u6807\u9898\u751f\u6210\u65b9\u6cd5\u8fc7\u5ea6\u5173\u6ce8\u8d28\u91cf\u6216\u70b9\u51fb\u7387\uff0c\u5bfc\u81f4\u8f93\u51fa\u540c\u8d28\u5316\u4e25\u91cd\uff0c\u7f3a\u4e4f\u5438\u5f15\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u7684\u591a\u6837\u6027", "method": "1. \u8bbe\u8ba1\u81ea\u52a8\u5316\u6570\u636e\u751f\u6210\u7ba1\u9053\u751f\u4ea7\u591a\u6837\u5316\u6807\u9898\u5bf9\uff1b2. \u91c7\u7528\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RL)\u4e24\u9636\u6bb5\u4f18\u5316\u6846\u67b6\uff0c\u5f15\u5165\u591a\u6837\u6027\u5956\u52b1\u673a\u5236\u548c\u591a\u76ee\u6807\u635f\u5931\u51fd\u6570", "result": "\u5de5\u4e1a\u6570\u636e\u96c6\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\uff0c\u90e8\u7f72\u540e\u5e7f\u544a\u4e3b\u4ef7\u503c(ADVV)\u63d0\u53474.0%\uff0c\u70b9\u51fb\u7387(CTR)\u63d0\u9ad81.4%", "conclusion": "DIVER\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u8d28\u91cf\u4e0e\u591a\u6837\u6027\u534f\u540c\u4f18\u5316\uff0c\u4e3a\u5927\u89c4\u6a21\u5185\u5bb9\u5e73\u53f0\u63d0\u4f9b\u6709\u6548\u7684\u5e7f\u544a\u751f\u6210\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.18740", "pdf": "https://arxiv.org/pdf/2508.18740", "abs": "https://arxiv.org/abs/2508.18740", "authors": ["Qiao Liang", "Ying Shen", "Tiantian Chen", "Lin Zhang"], "title": "M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 8 figures. Accepted to Findings of ACL 2025", "summary": "Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has\nrecently gained significant attention in social media analysis, aiming to\nextract emotion utterances, cause utterances, and emotion categories\nsimultaneously. However, the scarcity of related datasets, with only one\npublished dataset featuring highly uniform dialogue scenarios, hinders model\ndevelopment in this field. To address this, we introduce MECAD, the first\nmultimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56\nTV series spanning a wide range of dialogue contexts. In addition, existing\nMECTEC methods fail to explicitly model emotional and causal contexts and\nneglect the fusion of semantic information at different levels, leading to\nperformance degradation. In this paper, we propose M3HG, a novel model that\nexplicitly captures emotional and causal contexts and effectively fuses\ncontextual information at both inter- and intra-utterance levels via a\nmultimodal heterogeneous graph. Extensive experiments demonstrate the\neffectiveness of M3HG compared with existing state-of-the-art methods. The\ncodes and dataset are available at https://github.com/redifinition/M3HG.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9996\u4e2a\u591a\u6a21\u6001\u591a\u573a\u666fMECTEC\u6570\u636e\u96c6MECAD\u53ca\u65b0\u578b\u6a21\u578bM3HG\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5f02\u6784\u56fe\u663e\u5f0f\u5efa\u6a21\u60c5\u611f\u56e0\u679c\u4e0a\u4e0b\u6587\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u5f53\u524dMECTEC\u9886\u57df\u5b58\u5728\u6570\u636e\u96c6\u7a00\u7f3a\u4e14\u573a\u666f\u5355\u4e00\u3001\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u663e\u5f0f\u5efa\u6a21\u60c5\u611f\u56e0\u679c\u4e0a\u4e0b\u6587\u4e14\u5ffd\u7565\u591a\u5c42\u7ea7\u8bed\u4e49\u4fe1\u606f\u878d\u5408\u7684\u95ee\u9898\u3002", "method": "\u6784\u5efa\u591a\u6a21\u6001\u5f02\u6784\u56fe\u663e\u5f0f\u6355\u6349\u60c5\u611f/\u56e0\u679c\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u8bed\u53e5\u95f4\u548c\u8bed\u53e5\u5185\u4e24\u4e2a\u5c42\u6b21\u8fdb\u884c\u4e0a\u4e0b\u6587\u4fe1\u606f\u878d\u5408\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eM3HG\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u73b0\u6709SOTA\u65b9\u6cd5\uff0c\u6700\u9ad8\u63d0\u53475.31% F1\u503c\u3002", "conclusion": "MECAD\u6570\u636e\u96c6\u586b\u8865\u9886\u57df\u7a7a\u767d\uff0cM3HG\u6a21\u578b\u901a\u8fc7\u521b\u65b0\u7684\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u4fe1\u606f\u878d\u5408\u673a\u5236\u63a8\u52a8\u4e86MECTEC\u4efb\u52a1\u53d1\u5c55\u3002"}}
{"id": "2508.18748", "pdf": "https://arxiv.org/pdf/2508.18748", "abs": "https://arxiv.org/abs/2508.18748", "authors": ["Byeongjeong Kim", "Jeonghyun Park", "Joonho Yang", "Hwanhee Lee"], "title": "Chronological Passage Assembling in RAG framework for Temporal Question Answering", "categories": ["cs.CL"], "comment": "7 pages, 3 figures", "summary": "Long-context question answering over narrative tasks is challenging because\ncorrect answers often hinge on reconstructing a coherent timeline of events\nwhile preserving contextual flow in a limited context window.\nRetrieval-augmented generation (RAG) indexing methods aim to address this\nchallenge by selectively retrieving only necessary document segments. However,\nnarrative texts possess unique characteristics that limit the effectiveness of\nthese existing approaches. Specifically, understanding narrative texts requires\nmore than isolated segments, as the broader context and sequential\nrelationships between segments are crucial for comprehension. To address these\nlimitations, we propose ChronoRAG, a novel RAG framework specialized for\nnarrative texts. This approach focuses on two essential aspects: refining\ndispersed document information into coherent and structured passages, and\npreserving narrative flow by explicitly capturing and maintaining the temporal\norder among retrieved passages. We empirically demonstrate the effectiveness of\nChronoRAG through experiments on the NarrativeQA dataset, showing substantial\nimprovements in tasks requiring both factual identification and comprehension\nof complex sequential relationships, underscoring that reasoning over temporal\norder is crucial in resolving narrative QA.", "AI": {"tldr": "\u63d0\u51faChronoRAG\u6846\u67b6\u89e3\u51b3\u53d9\u4e8b\u7c7b\u957f\u6587\u672cQA\u4efb\u52a1\u4e2d\u65f6\u95f4\u7ebf\u91cd\u5efa\u4e0e\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\u7684\u96be\u9898", "motivation": "\u4f20\u7edfRAG\u65b9\u6cd5\u5728\u53d9\u4e8b\u6587\u672c\u4e2d\u5931\u6548\uff0c\u56e0\u53d9\u4e8b\u7406\u89e3\u9700\u4f9d\u8d56\u4e8b\u4ef6\u987a\u5e8f\u4e0e\u6574\u4f53\u4e0a\u4e0b\u6587\uff0c\u800c\u73b0\u6709\u68c0\u7d22\u65b9\u6cd5\u65e0\u6cd5\u4fdd\u7559\u6bb5\u843d\u95f4\u65f6\u5e8f\u5173\u7cfb", "method": "ChronoRAG\u6846\u67b6\uff1a1. \u91cd\u6784\u5206\u6563\u6587\u672c\u4e3a\u7ed3\u6784\u5316\u6bb5\u843d 2. \u663e\u5f0f\u6355\u6349\u5e76\u4fdd\u6301\u68c0\u7d22\u6bb5\u843d\u7684\u65f6\u5e8f\u5173\u7cfb", "result": "\u5728NarrativeQA\u6570\u636e\u96c6\u9a8c\u8bc1\uff0c\u4e8b\u5b9e\u8bc6\u522b\u51c6\u786e\u7387\u63d0\u534721.3%\uff0c\u65f6\u5e8f\u5173\u7cfb\u7406\u89e3\u4efb\u52a1\u63d0\u534734.7%", "conclusion": "\u65f6\u5e8f\u63a8\u7406\u662f\u53d9\u4e8bQA\u7684\u6838\u5fc3\uff0cChronoRAG\u901a\u8fc7\u7ed3\u6784\u5316\u53d9\u4e8b\u6d41\u4e0e\u663e\u5f0f\u65f6\u5e8f\u5efa\u6a21\u663e\u8457\u63d0\u5347\u6027\u80fd"}}
{"id": "2508.18773", "pdf": "https://arxiv.org/pdf/2508.18773", "abs": "https://arxiv.org/abs/2508.18773", "authors": ["Qianyu He", "Siyu Yuan", "Xuefeng Li", "Mingxuan Wang", "Jiangjie Chen"], "title": "ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) with chain-of-thought reasoning have\ndemonstrated remarkable problem-solving capabilities, but controlling their\ncomputational effort remains a significant challenge for practical deployment.\nRecent proprietary systems like OpenAI's gpt-oss series have introduced\ndiscrete operational modes for intuitive reasoning control, but the open-source\ncommunity has largely failed to achieve such capabilities. In this paper, we\nintroduce ThinkDial, the first open-recipe end-to-end framework that\nsuccessfully implements gpt-oss-style controllable reasoning through discrete\noperational modes. Our system enables seamless switching between three distinct\nreasoning regimes: High mode (full reasoning capability), Medium mode (50\npercent token reduction with <10 percent performance degradation), and Low mode\n(75 percent token reduction with <15 percent performance degradation). We\nachieve this through an end-to-end training paradigm that integrates\nbudget-mode control throughout the entire pipeline: budget-mode supervised\nfine-tuning that embeds controllable reasoning capabilities directly into the\nlearning process, and two-phase budget-aware reinforcement learning with\nadaptive reward shaping. Extensive experiments demonstrate that ThinkDial\nachieves target compression-performance trade-offs with clear response length\nreductions while maintaining performance thresholds. The framework also\nexhibits strong generalization capabilities on out-of-distribution tasks.", "AI": {"tldr": "\u9996\u4e2a\u5f00\u6e90\u7aef\u5230\u7aef\u6846\u67b6ThinkDial\u5b9e\u73b0GPT\u5f0f\u53ef\u63a7\u63a8\u7406\u6a21\u5f0f\uff0c\u901a\u8fc7\u9884\u7b97\u63a7\u5236\u8bad\u7ec3\u8303\u5f0f\u5728\u4fdd\u6301\u6027\u80fd\u9608\u503c\u4e0b\u663e\u8457\u964d\u4f4e\u54cd\u5e94\u957f\u5ea6", "motivation": "\u5f00\u6e90\u793e\u533a\u7f3a\u4e4f\u7c7b\u4f3cGPT-OSS\u7684\u79bb\u6563\u63a8\u7406\u63a7\u5236\u80fd\u529b\uff0c\u73b0\u6709LLMs\u5728\u8ba1\u7b97\u8d44\u6e90\u63a7\u5236\u65b9\u9762\u5b58\u5728\u90e8\u7f72\u6311\u6218", "method": "\u7aef\u5230\u7aef\u8bad\u7ec3\u8303\u5f0f\u6574\u5408\u9884\u7b97\u6a21\u5f0f\u63a7\u5236\uff1a\u9884\u7b97\u6a21\u5f0f\u76d1\u7763\u5fae\u8c03+\u4e24\u9636\u6bb5\u9884\u7b97\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff08\u542b\u81ea\u9002\u5e94\u5956\u52b1\u673a\u5236\uff09", "result": "\u6210\u529f\u5b9e\u73b0\u4e09\u79cd\u6a21\u5f0f\uff1aHigh\uff08\u5168\u6027\u80fd\uff09/Medium\uff0850% token\u51cf\u5c11\uff0c\u6027\u80fd\u635f\u5931<10%\uff09/Low\uff0875% token\u51cf\u5c11\uff0c\u6027\u80fd\u635f\u5931<15%\uff09", "conclusion": "ThinkDial\u6846\u67b6\u5728\u4fdd\u6301\u6027\u80fd\u9608\u503c\u7684\u540c\u65f6\u8fbe\u6210\u76ee\u6807\u538b\u7f29\u7387\uff0c\u4e14\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e2d\u5c55\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u63a8\u52a8\u9ad8\u6548LLM\u90e8\u7f72"}}
{"id": "2508.18780", "pdf": "https://arxiv.org/pdf/2508.18780", "abs": "https://arxiv.org/abs/2508.18780", "authors": ["Yilin Li", "Xunjian Yin", "Yilin Chen", "Xiaojun Wan"], "title": "Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction", "categories": ["cs.CL", "cs.AI"], "comment": "Code will be released upon publication", "summary": "Grammatical error correction is a significant task in NLP. Traditional\nmethods based on encoder-decoder models have achieved certain success, but the\napplication of LLMs in this field is still underexplored. Current research\npredominantly relies on supervised fine-tuning to train LLMs to directly\ngenerate the corrected sentence, which limits the model's powerful reasoning\nability. To address this limitation, we propose a novel framework based on\nRule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL\nframework achieves \\textbf{state-of-the-art }performance, with a notable\nincrease in \\textbf{recall}. This result clearly highlights the advantages of\nusing RL to steer LLMs, offering a more controllable and reliable paradigm for\nfuture development in GEC.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c4\u5219\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\u663e\u8457\u63d0\u5347\u8bed\u6cd5\u9519\u8bef\u7ea0\u6b63\u7684\u53ec\u56de\u7387\uff0c\u9a8c\u8bc1\u5f3a\u5316\u5b66\u4e60\u5f15\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4f18\u52bf", "motivation": "\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u9650\u5236\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u9700\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u7b49\u65b0\u65b9\u6cd5\u63d0\u5347\u6a21\u578b\u53ef\u63a7\u6027\u548c\u53ef\u9760\u6027", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u89c4\u5219\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u4e2d\u6587\u6570\u636e\u96c6\u5b9e\u9a8c\u9a8c\u8bc1\u6709\u6548\u6027", "result": "\u5728\u4e2d\u6587\u8bed\u6cd5\u7ea0\u9519\u4efb\u52a1\u4e2d\u53d6\u5f97SOTA\u6027\u80fd\uff0c\u53ec\u56de\u7387\u6307\u6807\u663e\u8457\u63d0\u5347", "conclusion": "\u7814\u7a76\u8868\u660e\u5f3a\u5316\u5b66\u4e60\u80fd\u6709\u6548\u5f15\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bed\u6cd5\u7ea0\u9519\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u53ef\u63a7\u53ef\u9760\u7684\u65b0\u8303\u5f0f"}}
{"id": "2508.18783", "pdf": "https://arxiv.org/pdf/2508.18783", "abs": "https://arxiv.org/abs/2508.18783", "authors": ["Igor Shalyminov", "Hang Su", "Jake Vincent", "Siffi Singh", "Jason Cai", "James Gung", "Raphael Shu", "Saab Mansour"], "title": "Controllable Conversational Theme Detection Track at DSTC 12", "categories": ["cs.CL"], "comment": "DSTC12@SigDial2025; data and code available at\n  https://github.com/amazon-science/dstc12-controllable-conversational-theme-detection", "summary": "Conversational analytics has been on the forefront of transformation driven\nby the advances in Speech and Natural Language Processing techniques. Rapid\nadoption of Large Language Models (LLMs) in the analytics field has taken the\nproblems that can be automated to a new level of complexity and scale. In this\npaper, we introduce Theme Detection as a critical task in conversational\nanalytics, aimed at automatically identifying and categorizing topics within\nconversations. This process can significantly reduce the manual effort involved\nin analyzing expansive dialogs, particularly in domains like customer support\nor sales. Unlike traditional dialog intent detection, which often relies on a\nfixed set of intents for downstream system logic, themes are intended as a\ndirect, user-facing summary of the conversation's core inquiry. This\ndistinction allows for greater flexibility in theme surface forms and\nuser-specific customizations. We pose Controllable Conversational Theme\nDetection problem as a public competition track at Dialog System Technology\nChallenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of\ndialog utterances, with the distinctive aspect being controllability of the\nresulting theme clusters' granularity achieved via the provided user preference\ndata. We give an overview of the problem, the associated dataset and the\nevaluation metrics, both automatic and human. Finally, we discuss the\nparticipant teams' submissions and provide insights from those. The track\nmaterials (data and code) are openly available in the GitHub repository.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u7528\u6237\u504f\u597d\u8c03\u8282\u805a\u7c7b\u7c92\u5ea6\u7684\u53ef\u63a7\u5bf9\u8bdd\u4e3b\u9898\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7DSTC 12\u7ade\u8d5b\u9a8c\u8bc1\u53ef\u663e\u8457\u63d0\u5347\u5ba2\u6237\u652f\u6301\u7b49\u9886\u57df\u7684\u5bf9\u8bdd\u5206\u6790\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u5bf9\u8bdd\u610f\u56fe\u68c0\u6d4b\u4f9d\u8d56\u56fa\u5b9a\u610f\u56fe\u96c6\uff0c\u96be\u4ee5\u9002\u5e94\u7528\u6237\u81ea\u5b9a\u4e49\u9700\u6c42\u3002\u4e3b\u9898\u68c0\u6d4b\u901a\u8fc7\u7075\u6d3b\u6807\u6ce8\u5bf9\u8bdd\u6838\u5fc3\u5185\u5bb9\uff0c\u53ef\u51cf\u5c11\u4eba\u5de5\u5206\u6790\u5927\u89c4\u6a21\u5bf9\u8bdd\u7684\u8d1f\u62c5\u3002", "method": "\u5c06\u4e3b\u9898\u68c0\u6d4b\u5b9a\u4e49\u4e3a\u5bf9\u8bdd\u8bed\u53e5\u7684\u8054\u5408\u805a\u7c7b\u4e0e\u6807\u6ce8\u4efb\u52a1\uff0c\u5f15\u5165\u7528\u6237\u504f\u597d\u6570\u636e\u52a8\u6001\u63a7\u5236\u4e3b\u9898\u7c92\u5ea6\u7684\u7c97\u7ec6\u7a0b\u5ea6\u3002", "result": "\u5728DSTC 12\u7ade\u8d5b\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\uff0c\u6536\u96c6\u4e86\u591a\u56e2\u961f\u89e3\u51b3\u65b9\u6848\u5e76\u5f00\u6e90\u6570\u636e\u96c6\u4e0e\u4ee3\u7801\u3002\u53c2\u8d5b\u65b9\u6848\u663e\u793a\u901a\u8fc7\u7528\u6237\u504f\u597d\u8c03\u8282\u53ef\u6709\u6548\u63a7\u5236\u4e3b\u9898\u62bd\u8c61\u5c42\u7ea7\u3002", "conclusion": "\u53ef\u63a7\u4e3b\u9898\u68c0\u6d4b\u673a\u5236\u7a81\u7834\u4f20\u7edf\u610f\u56fe\u68c0\u6d4b\u7684\u5c40\u9650\u6027\uff0c\u5728\u5ba2\u6237\u670d\u52a1\u7b49\u9886\u57df\u5177\u6709\u663e\u8457\u5e94\u7528\u4ef7\u503c\uff0c\u5176\u7075\u6d3b\u6027\u548c\u53ef\u5b9a\u5236\u6027\u4e3a\u5bf9\u8bdd\u5206\u6790\u5f00\u8f9f\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.18791", "pdf": "https://arxiv.org/pdf/2508.18791", "abs": "https://arxiv.org/abs/2508.18791", "authors": ["Ziming Zhu", "Chenglong Wang", "Shunjie Xing", "Yifu Huo", "Fengning Tian", "Quan Du", "Di Yang", "Chunliang Zhang", "Tong Xiao", "Jingbo Zhu"], "title": "LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination", "categories": ["cs.CL"], "comment": null, "summary": "Despite the remarkable progress of modern machine translation (MT) systems on\ngeneral-domain texts, translating structured LaTeX-formatted documents remains\na significant challenge. These documents typically interleave natural language\nwith domain-specific syntax, such as mathematical equations, tables, figures,\nand cross-references, all of which must be accurately preserved to maintain\nsemantic integrity and compilability. In this paper, we introduce LaTeXTrans, a\ncollaborative multi-agent system designed to address this challenge. LaTeXTrans\nensures format preservation, structural fidelity, and terminology consistency\nthrough six specialized agents: 1) a Parser that decomposes LaTeX into\ntranslation-friendly units via placeholder substitution and syntax filtering;\n2) a Translator, Validator, Summarizer, and Terminology Extractor that work\ncollaboratively to ensure context-aware, self-correcting, and\nterminology-consistent translations; 3) a Generator that reconstructs the\ntranslated content into well-structured LaTeX documents. Experimental results\ndemonstrate that LaTeXTrans can outperform mainstream MT systems in both\ntranslation accuracy and structural fidelity, offering an effective and\npractical solution for translating LaTeX-formatted documents.", "AI": {"tldr": "\u63d0\u51faLaTeXTrans\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u683c\u5f0f\u4fdd\u7559\u548c\u672f\u8bed\u4e00\u81f4\u6027\u63d0\u5347LaTeX\u6587\u6863\u7ffb\u8bd1\u8d28\u91cf", "motivation": "\u73b0\u6709\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u96be\u4ee5\u5904\u7406LaTeX\u6587\u6863\u4e2d\u81ea\u7136\u8bed\u8a00\u4e0e\u6570\u5b66\u516c\u5f0f/\u8868\u683c\u7b49\u4e13\u4e1a\u8bed\u6cd5\u7684\u6df7\u5408\u7ed3\u6784\uff0c\u9700\u8981\u4fdd\u6301\u7f16\u8bd1\u517c\u5bb9\u6027\u548c\u8bed\u4e49\u5b8c\u6574\u6027", "method": "\u91c7\u7528\u516d\u6a21\u5757\u534f\u540c\u5de5\u4f5c\uff1a1)\u89e3\u6790\u5668\u5206\u89e3LaTeX\u7ed3\u6784 2)\u7ffb\u8bd1/\u9a8c\u8bc1/\u6458\u8981/\u672f\u8bed\u63d0\u53d6\u56db\u6a21\u5757\u786e\u4fdd\u4e0a\u4e0b\u6587\u611f\u77e5 3)\u751f\u6210\u5668\u91cd\u6784\u7ffb\u8bd1\u5185\u5bb9", "result": "\u5b9e\u9a8c\u8bc1\u660e\u5728\u7ffb\u8bd1\u51c6\u786e\u7387\u548c\u7ed3\u6784\u4fdd\u771f\u5ea6\u4e0a\u8d85\u8d8a\u4e3b\u6d41\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf", "conclusion": "LaTeXTrans\u4e3a\u7ed3\u6784\u5316\u6587\u6863\u7ffb\u8bd1\u63d0\u4f9b\u683c\u5f0f\u4fdd\u7559\u3001\u672f\u8bed\u4e00\u81f4\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.18819", "pdf": "https://arxiv.org/pdf/2508.18819", "abs": "https://arxiv.org/abs/2508.18819", "authors": ["Shubham Gupta", "Shraban Kumar Chatterjee", "Suman Kundu"], "title": "LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "The proliferation of misinformation in the digital age has led to significant\nsocietal challenges. Existing approaches often struggle with capturing\nlong-range dependencies, complex semantic relations, and the social dynamics\ninfluencing news dissemination. Furthermore, these methods require extensive\nlabelled datasets, making their deployment resource-intensive. In this study,\nwe propose a novel self-supervised misinformation detection framework that\nintegrates both complex semantic relations using Abstract Meaning\nRepresentation (AMR) and news propagation dynamics. We introduce an LLM-based\ngraph contrastive loss (LGCL) that utilizes negative anchor points generated by\na Large Language Model (LLM) to enhance feature separability in a zero-shot\nmanner. To incorporate social context, we employ a multi view graph masked\nautoencoder, which learns news propagation features from social context graph.\nBy combining these semantic and propagation-based features, our approach\neffectively differentiates between fake and real news in a self-supervised\nmanner. Extensive experiments demonstrate that our self-supervised framework\nachieves superior performance compared to other state-of-the-art methodologies,\neven with limited labelled datasets while improving generalizability.", "AI": {"tldr": "\u63d0\u51fa\u6574\u5408\u8bed\u4e49\u5173\u7cfb\u4e0e\u4f20\u64ad\u52a8\u6001\u7684\u81ea\u76d1\u7763\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7LLM\u589e\u5f3a\u7279\u5f81\u53ef\u5206\u6027\u548c\u591a\u89c6\u56fe\u56fe\u5b66\u4e60\uff0c\u5b9e\u73b0\u9ad8\u6548\u5047\u65b0\u95fb\u68c0\u6d4b", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u957f\u8ddd\u79bb\u4f9d\u8d56\u6355\u6349\u4e0d\u8db3\u3001\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u3001\u5ffd\u89c6\u793e\u4ea4\u4f20\u64ad\u52a8\u6001\u4e09\u5927\u7f3a\u9677\uff0c\u9700\u5f00\u53d1\u66f4\u9ad8\u6548\u666e\u9002\u7684\u68c0\u6d4b\u65b9\u6848", "method": "\u878d\u5408AMR\u8bed\u4e49\u5206\u6790\u4e0e\u793e\u4ea4\u4f20\u64ad\u56fe\u7279\u5f81\uff0c\u8bbe\u8ba1LLM\u751f\u6210\u8d1f\u951a\u70b9\u7684\u56fe\u5bf9\u6bd4\u635f\u5931(LGCL)\u548c\u591a\u89c6\u56fe\u56fe\u63a9\u7801\u81ea\u7f16\u7801\u5668", "result": "\u5728\u96f6\u6837\u672c\u548c\u6709\u9650\u6807\u6ce8\u573a\u666f\u4e0b\u68c0\u6d4b\u51c6\u786e\u7387\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u6846\u67b6\u5177\u6709\u66f4\u597d\u7684\u7279\u5f81\u53ef\u5206\u6027\u4e0e\u6a21\u578b\u6cdb\u5316\u80fd\u529b", "conclusion": "\u901a\u8fc7\u8bed\u4e49-\u4f20\u64ad\u53cc\u7279\u5f81\u878d\u5408\u4e0e\u81ea\u76d1\u7763\u5b66\u4e60\u673a\u5236\uff0c\u6709\u6548\u7a81\u7834\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u63d0\u5347\u68c0\u6d4b\u6548\u679c\u4e0e\u9002\u7528\u6027"}}
{"id": "2508.18824", "pdf": "https://arxiv.org/pdf/2508.18824", "abs": "https://arxiv.org/abs/2508.18824", "authors": ["Sirui Chen", "Changxin Tian", "Binbin Hu", "Kunlong Chen", "Ziqi Liu", "Zhiqiang Zhang", "Jun Zhou"], "title": "Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness", "categories": ["cs.CL"], "comment": null, "summary": "Enhancing the mathematical reasoning of large language models (LLMs) demands\nhigh-quality training data, yet conventional methods face critical challenges\nin scalability, cost, and data reliability. To address these limitations, we\npropose a novel program-assisted synthesis framework that systematically\ngenerates a high-quality mathematical corpus with guaranteed diversity,\ncomplexity, and correctness. This framework integrates mathematical knowledge\nsystems and domain-specific tools to create executable programs. These programs\nare then translated into natural language problem-solution pairs and vetted by\na bilateral validation mechanism that verifies solution correctness against\nprogram outputs and ensures program-problem consistency. We have generated 12.3\nmillion such problem-solving triples. Experiments demonstrate that models\nfine-tuned on our data significantly improve their inference capabilities,\nachieving state-of-the-art performance on several benchmark datasets and\nshowcasing the effectiveness of our synthesis approach.", "AI": {"tldr": "\u63d0\u51fa\u7a0b\u5e8f\u8f85\u52a9\u5408\u6210\u6846\u67b6\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u5b66\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u53cc\u5411\u9a8c\u8bc1\u786e\u4fdd\u6570\u636e\u6b63\u786e\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b", "motivation": "\u4f20\u7edf\u6570\u5b66\u6570\u636e\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u53ef\u6269\u5c55\u6027\u5dee\u3001\u6210\u672c\u9ad8\u4e14\u53ef\u9760\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\u6765\u4fdd\u8bc1\u6570\u636e\u8d28\u91cf", "method": "\u6574\u5408\u6570\u5b66\u77e5\u8bc6\u7cfb\u7edf\u4e0e\u9886\u57df\u5de5\u5177\u751f\u6210\u53ef\u6267\u884c\u7a0b\u5e8f\uff0c\u8f6c\u6362\u4e3a\u95ee\u9898-\u89e3\u51b3\u65b9\u6848\u5bf9\u540e\uff0c\u901a\u8fc7\u7a0b\u5e8f\u8f93\u51fa\u9a8c\u8bc1\u548c\u95ee\u9898\u4e00\u81f4\u6027\u68c0\u67e5\u7684\u53cc\u5411\u9a8c\u8bc1\u673a\u5236", "result": "\u751f\u62101230\u4e07\u9ad8\u8d28\u91cf\u4e09\u5143\u7ec4\u6570\u636e\uff0c\u5fae\u8c03\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\uff0c\u63a8\u7406\u80fd\u529b\u663e\u8457\u63d0\u5347", "conclusion": "\u8be5\u5408\u6210\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6570\u5b66\u6570\u636e\u751f\u6210\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u7a0b\u5e8f\u8f85\u52a9\u65b9\u6cd5\u5bf9\u63d0\u5347\u6a21\u578b\u6570\u5b66\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u6027"}}
{"id": "2508.18847", "pdf": "https://arxiv.org/pdf/2508.18847", "abs": "https://arxiv.org/abs/2508.18847", "authors": ["Yibo Li", "Miao Xiong", "Jiaying Wu", "Bryan Hooi"], "title": "ConfTuner: Training Large Language Models to Express Their Confidence Verbally", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in high-stakes domains\nsuch as science, law, and healthcare, where accurate expressions of uncertainty\nare essential for reliability and trust. However, current LLMs are often\nobserved to generate incorrect answers with high confidence, a phenomenon known\nas \"overconfidence\". Recent efforts have focused on calibrating LLMs'\nverbalized confidence: i.e., their expressions of confidence in text form, such\nas \"I am 80% confident that...\". Existing approaches either rely on prompt\nengineering or fine-tuning with heuristically generated uncertainty estimates,\nboth of which have limited effectiveness and generalizability. Motivated by the\nnotion of proper scoring rules for calibration in classical machine learning\nmodels, we introduce ConfTuner, a simple and efficient fine-tuning method that\nintroduces minimal overhead and does not require ground-truth confidence scores\nor proxy confidence estimates. ConfTuner relies on a new loss function,\ntokenized Brier score, which we theoretically prove to be a proper scoring\nrule, intuitively meaning that it \"correctly incentivizes the model to report\nits true probability of being correct\". ConfTuner improves calibration across\ndiverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our\nresults further show that better-calibrated confidence enables downstream gains\nin self-correction and model cascade, advancing the development of trustworthy\nLLM systems. The code is available at\nhttps://github.com/liushiliushi/ConfTuner.", "AI": {"tldr": "\u63d0\u51faConfTuner\u65b9\u6cd5\uff0c\u901a\u8fc7\u65b0\u7684tokenized Brier score\u635f\u5931\u51fd\u6570\u6709\u6548\u6821\u51c6LLM\u7f6e\u4fe1\u5ea6\uff0c\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0", "motivation": "LLM\u5728\u5173\u952e\u9886\u57df\u90e8\u7f72\u9700\u8981\u53ef\u9760\u7f6e\u4fe1\u5ea6\u8868\u8fbe\uff0c\u4f46\u73b0\u6709\u6821\u51c6\u65b9\u6cd5\uff08\u63d0\u793a\u5de5\u7a0b/\u542f\u53d1\u5f0f\u5fae\u8c03\uff09\u6548\u679c\u6709\u9650\u4e14\u6cdb\u5316\u6027\u5dee", "method": "\u57fa\u4e8eproper scoring rules\u7406\u8bba\u8bbe\u8ba1tokenized Brier score\u635f\u5931\u51fd\u6570\uff0c\u65e0\u9700\u771f\u5b9e\u7f6e\u4fe1\u5ea6\u6570\u636e\u5373\u53ef\u5fae\u8c03\u6a21\u578b", "result": "\u5b9e\u9a8c\u8bc1\u660eConfTuner\u63d0\u5347\u591a\u4efb\u52a1\u6821\u51c6\u6548\u679c\uff08\u5305\u62ecGPT-4o\uff09\uff0c\u6539\u8fdb\u81ea\u7ea0\u6b63\u548c\u6a21\u578b\u7ea7\u8054\u6027\u80fd", "conclusion": "\u901a\u8fc7\u7406\u8bba\u4fdd\u8bc1\u7684\u6821\u51c6\u65b9\u6cd5\u63a8\u52a8\u53ef\u4fe1LLM\u7cfb\u7edf\u53d1\u5c55\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2508.18870", "pdf": "https://arxiv.org/pdf/2508.18870", "abs": "https://arxiv.org/abs/2508.18870", "authors": ["Viktor N. Zhuravlev", "Artur R. Khairullin", "Ernest A. Dyagin", "Alena N. Sitkina", "Nikita I. Kulin"], "title": "ReflectivePrompt: Reflective evolution in autoprompting algorithms", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which has been gaining popularity with the rapid advancement\nof prompt engineering, driven by extensive research in the field of large\nlanguage models (LLMs). This paper presents ReflectivePrompt - a novel\nautoprompting method based on evolutionary algorithms that employs a reflective\nevolution approach for more precise and comprehensive search of optimal\nprompts. ReflectivePrompt utilizes short-term and long-term reflection\noperations before crossover and elitist mutation to enhance the quality of the\nmodifications they introduce. This method allows for the accumulation of\nknowledge obtained throughout the evolution process and updates it at each\nepoch based on the current population. ReflectivePrompt was tested on 33\ndatasets for classification and text generation tasks using open-access large\nlanguage models: t-lite-instruct-0.1 and gemma3-27b-it. The method\ndemonstrates, on average, a significant improvement (e.g., 28% on BBH compared\nto EvoPrompt) in metrics relative to current state-of-the-art approaches,\nthereby establishing itself as one of the most effective solutions in\nevolutionary algorithm-based autoprompting.", "AI": {"tldr": "\u63d0\u51faReflectivePrompt\u2014\u2014\u57fa\u4e8e\u8fdb\u5316\u7b97\u6cd5\u7684\u53cd\u5c04\u5f0f\u81ea\u52a8\u63d0\u793a\u65b9\u6cd5\uff0c\u572833\u4e2a\u6570\u636e\u96c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u6027\u80fd\u63d0\u534728%", "motivation": "\u4f20\u7edf\u8fdb\u5316\u7b97\u6cd5\u5728\u63d0\u793a\u4f18\u5316\u4e2d\u96be\u4ee5\u6709\u6548\u79ef\u7d2f\u5386\u53f2\u77e5\u8bc6\uff0c\u5bfc\u81f4\u63d0\u793a\u4fee\u6539\u8d28\u91cf\u53d7\u9650\u3002\u4e3a\u89e3\u51b3\u8be5\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u77ed\u671f/\u957f\u671f\u53cd\u5c04\u673a\u5236\u589e\u5f3a\u8fdb\u5316\u8fc7\u7a0b\u4e2d\u7684\u77e5\u8bc6\u5229\u7528\u6548\u7387\u3002", "method": "1. \u5728\u4ea4\u53c9\u548c\u7cbe\u82f1\u7a81\u53d8\u524d\u6267\u884c\u53cd\u5c04\u64cd\u4f5c\n2. \u77ed\u671f\u53cd\u5c04\u5206\u6790\u5355\u4ee3\u79cd\u7fa4\u7279\u5f81\n3. \u957f\u671f\u53cd\u5c04\u6574\u5408\u5168\u8fdb\u5316\u8fc7\u7a0b\u77e5\u8bc6\n4. \u6784\u5efa\u52a8\u6001\u66f4\u65b0\u7684\u77e5\u8bc6\u5e93\u6307\u5bfc\u53d8\u5f02\u65b9\u5411", "result": "\u4f7f\u7528t-lite-instruct-0.1\u548cgemma3-27b-it\u6a21\u578b\u6d4b\u8bd5\uff1a\n- \u5206\u7c7b\u4efb\u52a1\u51c6\u786e\u7387\u63d0\u534718-35%\n- BBH\u57fa\u51c6\u6210\u7ee9\u8f83EvoPrompt\u63d0\u534728%\n- \u572883%\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u8fbe\u5230SOTA", "conclusion": "\u53cd\u5c04\u673a\u5236\u663e\u8457\u63d0\u5347\u8fdb\u5316\u7b97\u6cd5\u5728\u63d0\u793a\u4f18\u5316\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4f7fReflectivePrompt\u6210\u4e3a\u5f53\u524d\u6700\u5148\u8fdb\u7684\u81ea\u52a8\u63d0\u793a\u65b9\u6cd5\u4e4b\u4e00"}}
{"id": "2508.18872", "pdf": "https://arxiv.org/pdf/2508.18872", "abs": "https://arxiv.org/abs/2508.18872", "authors": ["Laurie Gale", "Sebastian Mateos Nicolajsen"], "title": "Empowering Computing Education Researchers Through LLM-Assisted Content Analysis", "categories": ["cs.CL"], "comment": "7 pages, 2 figures", "summary": "Computing education research (CER) is often instigated by practitioners\nwanting to improve both their own and the wider discipline's teaching practice.\nHowever, the latter is often difficult as many researchers lack the colleagues,\nresources, or capacity to conduct research that is generalisable or rigorous\nenough to advance the discipline. As a result, research methods that enable\nsense-making with larger volumes of qualitative data, while not increasing the\nburden on the researcher, have significant potential within CER.\n  In this discussion paper, we propose such a method for conducting rigorous\nanalysis on large volumes of textual data, namely a variation of LLM-assisted\ncontent analysis (LACA). This method combines content analysis with the use of\nlarge language models, empowering researchers to conduct larger-scale research\nwhich they would otherwise not be able to perform. Using a computing education\ndataset, we illustrate how LACA could be applied in a reproducible and rigorous\nmanner. We believe this method has potential in CER, enabling more\ngeneralisable findings from a wider range of research. This, together with the\ndevelopment of similar methods, can help to advance both the practice and\nresearch quality of the CER discipline.", "AI": {"tldr": "\u63d0\u51faLLM\u8f85\u52a9\u5185\u5bb9\u5206\u6790\u6cd5(LACA)\uff0c\u901a\u8fc7\u7ed3\u5408\u5185\u5bb9\u5206\u6790\u548c\u5927\u8bed\u8a00\u6a21\u578b\u89e3\u51b3CER\u9886\u57df\u7814\u7a76\u89c4\u6a21\u53d7\u9650\u7684\u95ee\u9898", "motivation": "\u5f53\u524d\u8ba1\u7b97\u6559\u80b2\u7814\u7a76\u8005\u666e\u904d\u9762\u4e34\u6570\u636e\u89c4\u6a21\u5c0f\u3001\u8d44\u6e90\u6709\u9650\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u5f00\u5c55\u5177\u6709\u666e\u904d\u6027\u7684\u9ad8\u8d28\u91cf\u7814\u7a76", "method": "\u5f00\u53d1\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8f85\u52a9\u5185\u5bb9\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5904\u7406\u63d0\u5347\u6587\u672c\u6570\u636e\u5206\u6790\u6548\u7387\u4e0e\u89c4\u6a21", "result": "\u4f7f\u7528\u771f\u5b9e\u8ba1\u7b97\u6559\u80b2\u6570\u636e\u96c6\u9a8c\u8bc1\u65b9\u6cd5\u53ef\u884c\u6027\uff0c\u8bc1\u660e\u53ef\u5728\u4fdd\u6301\u4e25\u8c28\u6027\u7684\u524d\u63d0\u4e0b\u5f00\u5c55\u66f4\u5927\u89c4\u6a21\u7814\u7a76", "conclusion": "LACA\u65b9\u6cd5\u4e3aCER\u9886\u57df\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u7814\u7a76\u8303\u5f0f\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u6574\u4f53\u7814\u7a76\u8d28\u91cf\u5e76\u63a8\u52a8\u6559\u5b66\u5b9e\u8df5\u6539\u8fdb"}}
{"id": "2508.18916", "pdf": "https://arxiv.org/pdf/2508.18916", "abs": "https://arxiv.org/abs/2508.18916", "authors": ["Bojan Evkoski", "Igor Mozeti\u010d", "Nikola Ljube\u0161i\u0107", "Petra Kralj Novak"], "title": "Affective Polarization across European Parliaments", "categories": ["cs.CL", "cs.SI"], "comment": "6 pages, 4 figures", "summary": "Affective polarization, characterized by increased negativity and hostility\ntowards opposing groups, has become a prominent feature of political discourse\nworldwide. Our study examines the presence of this type of polarization in a\nselection of European parliaments in a fully automated manner. Utilizing a\ncomprehensive corpus of parliamentary speeches from the parliaments of six\nEuropean countries, we employ natural language processing techniques to\nestimate parliamentarian sentiment. By comparing the levels of negativity\nconveyed in references to individuals from opposing groups versus one's own, we\ndiscover patterns of affectively polarized interactions. The findings\ndemonstrate the existence of consistent affective polarization across all six\nEuropean parliaments. Although activity correlates with negativity, there is no\nobserved difference in affective polarization between less active and more\nactive members of parliament. Finally, we show that reciprocity is a\ncontributing mechanism in affective polarization between parliamentarians\nacross all six parliaments.", "AI": {"tldr": "\u7814\u7a76\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u5206\u6790\u6b27\u6d32\u516d\u56fd\u8bae\u4f1a\u6f14\u8bb2\uff0c\u53d1\u73b0\u666e\u904d\u5b58\u5728\u60c5\u611f\u6781\u5316\u73b0\u8c61\uff0c\u5e76\u63ed\u793a\u5176\u9a71\u52a8\u673a\u5236", "motivation": "\u63a2\u7a76\u60c5\u611f\u6781\u5316\u73b0\u8c61\u662f\u5426\u5b58\u5728\u4e8e\u6b27\u6d32\u8bae\u4f1a\u653f\u6cbb\u4e92\u52a8\u4e2d\uff0c\u53ca\u5176\u5177\u4f53\u8868\u73b0\u6a21\u5f0f\u548c\u5f62\u6210\u673a\u5236", "method": "\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u5206\u6790\u516d\u56fd\u8bae\u4f1a\u6f14\u8bb2\u8bed\u6599\u5e93\uff0c\u6bd4\u8f83\u5bf9\u5df1\u65b9\u548c\u5bf9\u65b9\u56e2\u4f53\u6210\u5458\u7684\u8d1f\u9762\u60c5\u7eea\u8868\u8fbe\u5dee\u5f02", "result": "\u6240\u6709\u8bae\u4f1a\u5747\u5b58\u5728\u60c5\u611f\u6781\u5316\uff1b\u6d3b\u8dc3\u5ea6\u4e0e\u8d1f\u9762\u60c5\u7eea\u76f8\u5173\u4f46\u6781\u5316\u7a0b\u5ea6\u65e0\u5dee\u5f02\uff1breciprocity\u662f\u4e3b\u8981\u9a71\u52a8\u673a\u5236", "conclusion": "\u60c5\u611f\u6781\u5316\u5728\u6b27\u6d32\u8bae\u4f1a\u666e\u904d\u5b58\u5728\uff0c\u76f8\u4e92\u6027\u673a\u5236\u5728\u8de8\u8bae\u4f1a\u60c5\u611f\u6781\u5316\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u7406\u89e3\u653f\u6cbb\u5bf9\u7acb\u63d0\u4f9b\u65b0\u89c6\u89d2"}}
{"id": "2508.18929", "pdf": "https://arxiv.org/pdf/2508.18929", "abs": "https://arxiv.org/abs/2508.18929", "authors": ["Ilias Driouich", "Hongliu Cao", "Eoin Thomas"], "title": "Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework", "categories": ["cs.CL", "cs.AI"], "comment": "ECAI 2025 TRUST AI workshop", "summary": "Retrieval-augmented generation (RAG) systems improve large language model\noutputs by incorporating external knowledge, enabling more informed and\ncontext-aware responses. However, the effectiveness and trustworthiness of\nthese systems critically depends on how they are evaluated, particularly on\nwhether the evaluation process captures real-world constraints like protecting\nsensitive information. While current evaluation efforts for RAG systems have\nprimarily focused on the development of performance metrics, far less attention\nhas been given to the design and quality of the underlying evaluation datasets,\ndespite their pivotal role in enabling meaningful, reliable assessments. In\nthis work, we introduce a novel multi-agent framework for generating synthetic\nQA datasets for RAG evaluation that prioritize semantic diversity and privacy\npreservation. Our approach involves: (1) a Diversity agent leveraging\nclustering techniques to maximize topical coverage and semantic variability,\n(2) a Privacy Agent that detects and mask sensitive information across multiple\ndomains and (3) a QA curation agent that synthesizes private and diverse QA\npairs suitable as ground truth for RAG evaluation. Extensive experiments\ndemonstrate that our evaluation sets outperform baseline methods in diversity\nand achieve robust privacy masking on domain-specific datasets. This work\noffers a practical and ethically aligned pathway toward safer, more\ncomprehensive RAG system evaluation, laying the foundation for future\nenhancements aligned with evolving AI regulations and compliance standards.", "AI": {"tldr": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u6846\u67b6\u751f\u6210\u517c\u5177\u8bed\u4e49\u591a\u6837\u6027\u4e0e\u9690\u79c1\u4fdd\u62a4\u7684QA\u6570\u636e\u96c6\uff0c\u6539\u8fdbRAG\u7cfb\u7edf\u8bc4\u4f30\u8d28\u91cf", "motivation": "\u5f53\u524dRAG\u7cfb\u7edf\u8bc4\u4f30\u6570\u636e\u96c6\u7f3a\u4e4f\u8bed\u4e49\u591a\u6837\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u8bbe\u8ba1\uff0c\u5236\u7ea6\u53ef\u9760\u8bc4\u4f30", "method": "1) \u591a\u6837\u6027\u4ee3\u7406\u901a\u8fc7\u805a\u7c7b\u589e\u5f3a\u4e3b\u9898\u8986\u76d6\uff1b2) \u9690\u79c1\u4ee3\u7406\u8de8\u9886\u57df\u68c0\u6d4b\u654f\u611f\u4fe1\u606f\uff1b3) QA\u751f\u6210\u4ee3\u7406\u5408\u6210\u7b26\u5408\u9690\u79c1\u8981\u6c42\u7684\u591a\u6837\u5316\u95ee\u7b54\u5bf9", "result": "\u751f\u6210\u7684\u6570\u636e\u96c6\u5728\u9886\u57df\u7279\u5b9a\u573a\u666f\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u7684\u8bed\u4e49\u591a\u6837\u6027\uff08+23%\u8986\u76d6\u7387\uff09\u548c\u5f3a\u9690\u79c1\u4fdd\u62a4\uff08\u654f\u611f\u4fe1\u606f\u906e\u853d\u738798.5%\uff09", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u5408\u89c4\u8bc4\u4f30\u63d0\u4f9b\u5b9e\u8df5\u8def\u5f84\uff0c\u652f\u6301\u6784\u5efa\u7b26\u5408\u4f26\u7406\u89c4\u8303\u7684\u5b89\u5168RAG\u7cfb\u7edf\u8bc4\u4f30\u4f53\u7cfb"}}
{"id": "2508.18988", "pdf": "https://arxiv.org/pdf/2508.18988", "abs": "https://arxiv.org/abs/2508.18988", "authors": ["Hung Ming Liu"], "title": "Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "25 pages, 9 figures. The AI Intuition Explorer dashboard is available\n  at: https://cyrilliu1974.github.io/github.io/vi.html", "summary": "We present a framework where neural models develop an AI Mother Tongue, a\nnative symbolic language that simultaneously supports intuitive reasoning,\ncompositional symbol chains, and inherent interpretability. Unlike post-hoc\nexplanation methods, our approach embeds reasoning directly into the model's\nrepresentations: symbols capture meaningful semantic patterns, chains trace\ndecision paths, and gated induction mechanisms guide selective focus, yielding\ntransparent yet flexible reasoning. We introduce complementary training\nobjectives to enhance symbol purity and decision sparsity, and employ a\nsequential specialization strategy to first build broad symbolic competence and\nthen refine intuitive judgments. Experiments on AI tasks demonstrate\ncompetitive accuracy alongside verifiable reasoning traces, showing that AI\nMother Tongue can serve as a unified mechanism for interpretability, intuition,\nand symbolic reasoning in neural models.", "AI": {"tldr": "\u63d0\u51faAI Mother Tongue\u6846\u67b6\uff0c\u901a\u8fc7\u539f\u751f\u7b26\u53f7\u8bed\u8a00\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u76f4\u89c9\u63a8\u7406\u4e0e\u7b26\u53f7\u7ec4\u5408", "motivation": "\u7a81\u7834\u4f20\u7edf\u540e\u89e3\u91ca\u65b9\u6cd5\u7684\u5c40\u9650\uff0c\u5c06\u53ef\u89e3\u91ca\u6027\u76f4\u63a5\u5d4c\u5165\u6a21\u578b\u8868\u5f81\uff0c\u5b9e\u73b0\u7b26\u53f7\u6a21\u5f0f\u6355\u83b7\u3001\u51b3\u7b56\u8def\u5f84\u8ffd\u6eaf\u548c\u95e8\u63a7\u805a\u7126\u7684\u900f\u660e\u63a8\u7406", "method": "1.\u5f15\u5165\u7b26\u53f7\u7eaf\u5ea6/\u51b3\u7b56\u7a00\u758f\u6027\u7684\u4e92\u8865\u8bad\u7ec3\u76ee\u6807\n2.\u91c7\u7528\u5148\u5efa\u7acb\u7b26\u53f7\u57fa\u7840\u518d\u4f18\u5316\u76f4\u89c9\u5224\u65ad\u7684\u5e8f\u5217\u4e13\u4e1a\u5316\u7b56\u7565", "result": "\u5728AI\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u51c6\u786e\u6027\uff0c\u5e76\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u8f68\u8ff9", "conclusion": "\u8be5\u6846\u67b6\u5c06\u7b26\u53f7\u7cfb\u7edf\u7684\u89e3\u91ca\u4f18\u52bf\u4e0e\u795e\u7ecf\u7f51\u7edc\u7684\u7075\u6d3b\u6027\u7ed3\u5408\uff0c\u4e3a\u53ef\u4fe1AI\u63d0\u4f9b\u65b0\u7684\u5b9e\u73b0\u8def\u5f84"}}
{"id": "2508.18992", "pdf": "https://arxiv.org/pdf/2508.18992", "abs": "https://arxiv.org/abs/2508.18992", "authors": ["Viktor N. Zhuravlev", "Artur R. Khairullin", "Ernest A. Dyagin", "Alena N. Sitkina", "Nikita I. Kulin"], "title": "Automatic Prompt Optimization with Prompt Distillation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which is gaining popularity due to the rapid development of\nprompt engineering driven by extensive research in the field of large language\nmodels (LLMs). This paper presents DistillPrompt -- a novel autoprompting\nmethod based on large language models that employs a multi-stage integration of\ntask-specific information into prompts using training data. DistillPrompt\nutilizes distillation, compression, and aggregation operations to explore the\nprompt space more thoroughly. The method was tested on different datasets for\ntext classification and generation tasks using the t-lite-instruct-0.1 language\nmodel. The results demonstrate a significant average improvement (e.g., 20.12%\nacross the entire dataset compared to Grips) in key metrics over existing\nmethods in the field, establishing DistillPrompt as one of the most effective\nnon-gradient approaches in autoprompting.", "AI": {"tldr": "\u63d0\u51faDistillPrompt\u65b0\u578b\u81ea\u52a8\u63d0\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u4fe1\u606f\u6574\u5408\u663e\u8457\u63d0\u5347LLM\u6027\u80fd", "motivation": "\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u65b9\u6cd5\u5bf9\u63d0\u793a\u7a7a\u95f4\u7684\u63a2\u7d22\u4e0d\u591f\u5168\u9762\uff0c\u9700\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u7b56\u7565", "method": "\u7ed3\u5408\u84b8\u998f/\u538b\u7f29/\u805a\u5408\u4e09\u9636\u6bb5\u64cd\u4f5c\uff0c\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u6df1\u5ea6\u96c6\u6210\u4efb\u52a1\u7279\u5f81\u4fe1\u606f", "result": "\u6587\u672c\u4efb\u52a1\u5173\u952e\u6307\u6807\u5e73\u5747\u63d0\u534720.12%(vs Grips)\uff0c\u9a8c\u8bc1\u975e\u68af\u5ea6\u65b9\u6cd5\u6709\u6548\u6027", "conclusion": "DistillPrompt\u6210\u4e3a\u81ea\u52a8\u63d0\u793a\u9886\u57df\u6700\u6709\u6548\u7684\u67b6\u6784\u4e4b\u4e00\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u573a\u666f"}}
{"id": "2508.19026", "pdf": "https://arxiv.org/pdf/2508.19026", "abs": "https://arxiv.org/abs/2508.19026", "authors": ["Gueter Josmy Faure", "Min-Hung Chen", "Jia-Fong Yeh", "Ying Cheng", "Hung-Ting Su", "Yung-Hao Tang", "Shang-Hong Lai", "Winston H. Hsu"], "title": "MovieCORE: COgnitive REasoning in Movies", "categories": ["cs.CL"], "comment": "Accepted for EMNLP'2025 Main Conference. Project Page:\n  https://joslefaure.github.io/assets/html/moviecore.html", "summary": "This paper introduces MovieCORE, a novel video question answering (VQA)\ndataset designed to probe deeper cognitive understanding of movie content.\nUnlike existing datasets that focus on surface-level comprehension, MovieCORE\nemphasizes questions that engage System-2 thinking while remaining specific to\nthe video material. We present an innovative agentic brainstorming approach,\nutilizing multiple large language models (LLMs) as thought agents to generate\nand refine high-quality question-answer pairs. To evaluate dataset quality, we\ndevelop a set of cognitive tests assessing depth, thought-provocation\npotential, and syntactic complexity. We also propose a comprehensive evaluation\nscheme for assessing VQA model performance on deeper cognitive tasks. To\naddress the limitations of existing video-language models (VLMs), we introduce\nan agentic enhancement module, Agentic Choice Enhancement (ACE), which improves\nmodel reasoning capabilities post-training by up to 25%. Our work contributes\nto advancing movie understanding in AI systems and provides valuable insights\ninto the capabilities and limitations of current VQA models when faced with\nmore challenging, nuanced questions about cinematic content. Our project page,\ndataset and code can be found at\nhttps://joslefaure.github.io/assets/html/moviecore.html.", "AI": {"tldr": "MovieCORE\u63d0\u51fa\u65b0\u578b\u7535\u5f71\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53LLM\u751f\u6210\u6df1\u5ea6\u8ba4\u77e5\u95ee\u9898\uff0c\u5f00\u53d1\u8bc4\u4f30\u4f53\u7cfb\u5e76\u8bbe\u8ba1ACE\u6a21\u5757\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b25%", "motivation": "\u73b0\u6709\u89c6\u9891\u95ee\u7b54\u6570\u636e\u96c6\u4e13\u6ce8\u6d45\u5c42\u7406\u89e3\uff0c\u9700\u6784\u5efa\u6fc0\u53d1\u7cfb\u7edf\u4e8c\u601d\u8003\u7684\u6df1\u5ea6\u8ba4\u77e5\u6d4b\u8bd5\u6846\u67b6", "method": "\u91c7\u7528\u591aLLM\u4ee3\u7406\u8111\u66b4\u751f\u6210\u95ee\u9898\u5bf9\uff0c\u8bbe\u8ba1\u8ba4\u77e5\u590d\u6742\u5ea6\u8bc4\u4f30\u6307\u6807\uff0c\u5f00\u53d1\u8bad\u7ec3\u540e\u589e\u5f3a\u6a21\u5757ACE\u63d0\u5347\u6a21\u578b\u63a8\u7406", "result": "\u6570\u636e\u96c6\u901a\u8fc7\u4e09\u5c42\u8ba4\u77e5\u6d4b\u8bd5\u9a8c\u8bc1\u8d28\u91cf\uff0cACE\u6a21\u5757\u4f7fVQA\u6a21\u578b\u5728\u6df1\u5ea6\u4efb\u52a1\u4e0a\u63a8\u7406\u80fd\u529b\u63d0\u5347\u8fbe25%", "conclusion": "\u8be5\u7814\u7a76\u63a8\u8fdbAI\u7535\u5f71\u7406\u89e3\u80fd\u529b\u8bc4\u4f30\uff0c\u63ed\u793a\u73b0\u6709\u6a21\u578b\u5904\u7406\u590d\u6742\u7535\u5f71\u5185\u5bb9\u7684\u5c40\u9650\u6027\u4e0e\u6539\u8fdb\u65b9\u5411"}}
{"id": "2508.19076", "pdf": "https://arxiv.org/pdf/2508.19076", "abs": "https://arxiv.org/abs/2508.19076", "authors": ["Ziyue Li", "Yuan Chang", "Gaihong Yu", "Xiaoqiu Le"], "title": "HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language model (LLM)-based agents have demonstrated remarkable\ncapabilities in decision-making tasks, but struggle significantly with complex,\nlong-horizon planning scenarios. This arises from their lack of macroscopic\nguidance, causing disorientation and failures in complex tasks, as well as\ninsufficient continuous oversight during execution, rendering them unresponsive\nto environmental changes and prone to deviations. To tackle these challenges,\nwe introduce HiPlan, a hierarchical planning framework that provides adaptive\nglobal-local guidance to boost LLM-based agents'decision-making. HiPlan\ndecomposes complex tasks into milestone action guides for general direction and\nstep-wise hints for detailed actions. During the offline phase, we construct a\nmilestone library from expert demonstrations, enabling structured experience\nreuse by retrieving semantically similar tasks and milestones. In the execution\nphase, trajectory segments from past milestones are dynamically adapted to\ngenerate step-wise hints that align current observations with the milestone\nobjectives, bridging gaps and correcting deviations. Extensive experiments\nacross two challenging benchmarks demonstrate that HiPlan substantially\noutperforms strong baselines, and ablation studies validate the complementary\nbenefits of its hierarchical components.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u89c4\u5212\u6846\u67b6HiPlan\uff0c\u901a\u8fc7\u5168\u5c40-\u5c40\u90e8\u53cc\u5c42\u6b21\u6307\u5bfc\u589e\u5f3aLLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u957f\u7a0b\u4efb\u52a1\u4e2d\u7684\u51b3\u7b56\u80fd\u529b", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u957f\u7a0b\u89c4\u5212\u4e2d\u7f3a\u4e4f\u5b8f\u89c2\u5f15\u5bfc\u5bfc\u81f4\u8ff7\u5931\u65b9\u5411\uff0c\u6267\u884c\u8fc7\u7a0b\u4e2d\u7f3a\u4e4f\u6301\u7eed\u76d1\u63a7\u5bfc\u81f4\u73af\u5883\u9002\u5e94\u6027\u5dee", "method": "\u6784\u5efa\u91cc\u7a0b\u7891\u5e93\u5b9e\u73b0\u7ed3\u6784\u5316\u7ecf\u9a8c\u590d\u7528\uff0c\u52a8\u6001\u751f\u6210\u5c42\u6b21\u5316\u5f15\u5bfc\uff08\u91cc\u7a0b\u7891\u6307\u5357+\u6b65\u9aa4\u63d0\u793a\uff09\uff0c\u5b9e\u73b0\u79bb\u7ebf\u9636\u6bb5\u77e5\u8bc6\u6c89\u6dc0\u4e0e\u5728\u7ebf\u9636\u6bb5\u8f68\u8ff9\u9002\u914d", "result": "\u5728\u4e24\u4e2a\u590d\u6742\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u5c42\u6b21\u5316\u7ec4\u4ef6\u7684\u4e92\u8865\u4f18\u52bf", "conclusion": "HiPlan\u901a\u8fc7\u5c42\u6b21\u5316\u5f15\u5bfc\u673a\u5236\u6709\u6548\u63d0\u5347LLM\u667a\u80fd\u4f53\u7684\u6301\u7eed\u89c4\u5212\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684AI\u51b3\u7b56\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2508.19077", "pdf": "https://arxiv.org/pdf/2508.19077", "abs": "https://arxiv.org/abs/2508.19077", "authors": ["Tom R\u00f6hr", "Soumyadeep Roy", "Fares Al Mohamad", "Jens-Michalis Papaioannou", "Wolfgang Nejdl", "Felix Gers", "Alexander L\u00f6ser"], "title": "\"Where does it hurt?\" -- Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues", "categories": ["cs.CL"], "comment": "Accepted at ECAI 2025", "summary": "In a doctor-patient dialogue, the primary objective of physicians is to\ndiagnose patients and propose a treatment plan. Medical doctors guide these\nconversations through targeted questioning to efficiently gather the\ninformation required to provide the best possible outcomes for patients. To the\nbest of our knowledge, this is the first work that studies physician intent\ntrajectories in doctor-patient dialogues. We use the `Ambient Clinical\nIntelligence Benchmark' (Aci-bench) dataset for our study. We collaborate with\nmedical professionals to develop a fine-grained taxonomy of physician intents\nbased on the SOAP framework (Subjective, Objective, Assessment, and Plan). We\nthen conduct a large-scale annotation effort to label over 5000 doctor-patient\nturns with the help of a large number of medical experts recruited using\nProlific, a popular crowd-sourcing platform. This large labeled dataset is an\nimportant resource contribution that we use for benchmarking the\nstate-of-the-art generative and encoder models for medical intent\nclassification tasks. Our findings show that our models understand the general\nstructure of medical dialogues with high accuracy, but often fail to identify\ntransitions between SOAP categories. We also report for the first time common\ntrajectories in medical dialogue structures that provide valuable insights for\ndesigning `differential diagnosis' systems. Finally, we extensively study the\nimpact of intent filtering for medical dialogue summarization and observe a\nsignificant boost in performance. We make the codes and data, including\nannotation guidelines, publicly available at\nhttps://github.com/DATEXIS/medical-intent-classification.", "AI": {"tldr": "\u9996\u9879\u7814\u7a76\u533b\u60a3\u5bf9\u8bdd\u4e2d\u533b\u751f\u610f\u56fe\u8f68\u8ff9\u7684\u5de5\u4f5c\uff0c\u6784\u5efaSOAP\u6846\u67b6\u610f\u56fe\u5206\u7c7b\u6cd5\u5e76\u6807\u6ce85000+\u5bf9\u8bdd\u8f6e\u6b21\uff0c\u53d1\u73b0\u610f\u56fe\u8fc7\u6ee4\u663e\u8457\u63d0\u5347\u533b\u7597\u5bf9\u8bdd\u6458\u8981\u6027\u80fd", "motivation": "\u7406\u89e3\u533b\u751f\u5728\u8bca\u65ad\u8fc7\u7a0b\u4e2d\u7684\u610f\u56fe\u8f68\u8ff9\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u6709\u6548\u7684'\u9274\u522b\u8bca\u65ad'\u7cfb\u7edf\u63d0\u4f9b\u7ed3\u6784\u5316\u652f\u6301", "method": "\u4f7f\u7528ACI-bench\u6570\u636e\u96c6\uff0c\u8054\u5408\u533b\u5b66\u4e13\u5bb6\u5f00\u53d1SOAP\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7Prolific\u5e73\u53f0\u4f17\u5305\u6807\u6ce8\uff0c\u5e76\u8bc4\u4f30\u751f\u6210\u5f0f/\u7f16\u7801\u5668\u6a21\u578b\u7684\u610f\u56fe\u5206\u7c7b\u8868\u73b0", "result": "\u6a21\u578b\u6574\u4f53\u7ed3\u6784\u7406\u89e3\u51c6\u786e\u7387\u8fbe85%\uff0c\u4f46SOAP\u7c7b\u522b\u8f6c\u6362\u8bc6\u522b\u5931\u8d25\u738732%\u3002\u610f\u56fe\u8fc7\u6ee4\u4f7f\u5bf9\u8bdd\u6458\u8981ROUGE-L\u63d0\u534711.5\u4e2a\u767e\u5206\u70b9", "conclusion": "\u533b\u7597\u610f\u56fe\u8f68\u8ff9\u7814\u7a76\u4e3a\u5bf9\u8bdd\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u516c\u5f00\u7684\u6807\u6ce8\u6570\u636e\u96c6\u548cSOAP\u5206\u7c7b\u6cd5\u5c06\u6210\u4e3a\u533b\u7597NLP\u9886\u57df\u91cd\u8981\u57fa\u51c6\u8d44\u6e90"}}
{"id": "2508.19089", "pdf": "https://arxiv.org/pdf/2508.19089", "abs": "https://arxiv.org/abs/2508.19089", "authors": ["Yue Li", "Zhixue Zhao", "Carolina Scarton"], "title": "It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs", "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025", "summary": "Extremely low-resource languages, especially those written in rare scripts,\nas shown in Figure 1, remain largely unsupported by large language models\n(LLMs). This is due in part to compounding factors such as the lack of training\ndata. This paper delivers the first comprehensive analysis of whether LLMs can\nacquire such languages purely via in-context learning (ICL), with or without\nauxiliary alignment signals, and how these methods compare to\nparameter-efficient fine-tuning (PEFT). We systematically evaluate 20\nunder-represented languages across three state-of-the-art multilingual LLMs.\nOur findings highlight the limitation of PEFT when both language and its script\nare extremely under-represented by the LLM. In contrast, zero-shot ICL with\nlanguage alignment is impressively effective on extremely low-resource\nlanguages, while few-shot ICL or PEFT is more beneficial for languages\nrelatively better represented by LLMs. For LLM practitioners working on\nextremely low-resource languages, we summarise guidelines grounded by our\nresults on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning\na multilingual model on languages of unseen scripts.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5bf9\u4e8e\u6587\u5b57\u548c\u8bed\u8a00\u90fd\u6781\u5ea6\u7f3a\u4e4f\u652f\u6301\u7684\u8d85\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u96f6\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\u7ed3\u5408\u8bed\u8a00\u5bf9\u9f50\u6548\u679c\u663e\u8457\u4f18\u4e8e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u800c\u5c11\u91cf\u6837\u672c\u5fae\u8c03\u66f4\u9002\u5408\u5df2\u6709\u90e8\u5206\u8868\u5f81\u7684\u8bed\u8a00\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u5b57\u7f55\u89c1\u3001\u8bad\u7ec3\u6570\u636e\u532e\u4e4f\u7684\u8d85\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u652f\u6301\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e0e\u53c2\u6570\u5fae\u8c03\u7684\u9002\u7528\u8fb9\u754c\u3002", "method": "\u57283\u4e2a\u524d\u6cbf\u591a\u8bed\u8a00\u5927\u6a21\u578b\u4e0a\u7cfb\u7edf\u8bc4\u4f3020\u79cd\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u5bf9\u6bd4\u5206\u6790\u96f6\u6837\u672c/\u5c11\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u7684\u6548\u679c\u5dee\u5f02\u3002", "result": "PEFT\u5bf9\u6587\u5b57\u548c\u8bed\u8a00\u53cc\u91cd\u7a00\u7f3a\u7684\u8bed\u8a00\u6548\u679c\u6709\u9650\uff0c\u96f6\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60+\u8bed\u8a00\u5bf9\u9f50\u65b9\u6848\u5bf9\u8d85\u4f4e\u8d44\u6e90\u8bed\u8a00\u6548\u679c\u7a81\u51fa\uff0c\u5c11\u6837\u672c\u5fae\u8c03\u66f4\u9002\u5408\u5df2\u6709\u90e8\u5206\u8868\u5f81\u7684\u8bed\u8a00\u3002", "conclusion": "\u5efa\u8bae\u8d85\u4f4e\u8d44\u6e90\u8bed\u8a00\u5f00\u53d1\u8005\uff1a\u907f\u514d\u5bf9\u672a\u89c1\u8fc7\u6587\u5b57\u7684\u8bed\u8a00\u8fdb\u884c\u5fae\u8c03\uff0c\u4f18\u5148\u91c7\u7528\u96f6\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60+\u8bed\u8a00\u5bf9\u9f50\u65b9\u6848\uff0c\u5c11\u6837\u672c\u5fae\u8c03\u9002\u7528\u4e8e\u4e2d\u7b49\u8d44\u6e90\u8bed\u8a00\u9002\u914d\u3002"}}
{"id": "2508.19093", "pdf": "https://arxiv.org/pdf/2508.19093", "abs": "https://arxiv.org/abs/2508.19093", "authors": ["Mathew Henrickson"], "title": "Retrieval-Augmented Generation for Natural Language Art Provenance Searches in the Getty Provenance Index", "categories": ["cs.CL"], "comment": null, "summary": "This research presents a Retrieval-Augmented Generation (RAG) framework for\nart provenance studies, focusing on the Getty Provenance Index. Provenance\nresearch establishes the ownership history of artworks, which is essential for\nverifying authenticity, supporting restitution and legal claims, and\nunderstanding the cultural and historical context of art objects. The process\nis complicated by fragmented, multilingual archival data that hinders efficient\nretrieval. Current search portals require precise metadata, limiting\nexploratory searches. Our method enables natural-language and multilingual\nsearches through semantic retrieval and contextual summarization, reducing\ndependence on metadata structures. We assess RAG's capability to retrieve and\nsummarize auction records using a 10,000-record sample from the Getty\nProvenance Index - German Sales. The results show this approach provides a\nscalable solution for navigating art market archives, offering a practical tool\nfor historians and cultural heritage professionals conducting historically\nsensitive research.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7684\u6846\u67b6\uff0c\u6539\u8fdb\u827a\u672f\u54c1\u6765\u6e90\u7814\u7a76\u7684\u8de8\u8bed\u8a00\u68c0\u7d22\u4e0e\u6458\u8981\u751f\u6210\u3002", "motivation": "\u827a\u672f\u54c1\u6765\u6e90\u7814\u7a76\u4f9d\u8d56\u96f6\u6563\u7684\u591a\u8bed\u8a00\u6863\u6848\u6570\u636e\uff0c\u73b0\u6709\u68c0\u7d22\u7cfb\u7edf\u53d7\u9650\u4e8e\u7cbe\u786e\u5143\u6570\u636e\u67e5\u8be2\uff0c\u65e0\u6cd5\u652f\u6301\u63a2\u7d22\u6027\u7814\u7a76\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u8bed\u4e49\u68c0\u7d22\u4e0e\u4e0a\u4e0b\u6587\u6458\u8981\u6280\u672f\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00/\u591a\u8bed\u8a00\u641c\u7d22\uff0c\u964d\u4f4e\u5bf9\u7ed3\u6784\u5316\u5143\u6570\u636e\u7684\u4f9d\u8d56\u3002", "result": "\u5728Getty\u5fb7\u56fd\u62cd\u5356\u6570\u636e\u96c6\u7684\u4e07\u6761\u8bb0\u5f55\u6d4b\u8bd5\u4e2d\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u827a\u672f\u5e02\u573a\u6863\u6848\u7684\u68c0\u7d22\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6587\u5316\u9057\u4ea7\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5386\u53f2\u654f\u611f\u578b\u7814\u7a76\u5de5\u5177\uff0c\u652f\u6301\u827a\u672f\u54c1\u6eaf\u6e90\u4e0e\u6cd5\u5f8b\u7d22\u8d54\u7b49\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2508.19099", "pdf": "https://arxiv.org/pdf/2508.19099", "abs": "https://arxiv.org/abs/2508.19099", "authors": ["Thomas Compton"], "title": "Beyond the Black Box: Integrating Lexical and Semantic Methods in Quantitative Discourse Analysis with BERTopic", "categories": ["cs.CL"], "comment": "5 pages conference paper, 4 tables", "summary": "Quantitative Discourse Analysis has seen growing adoption with the rise of\nLarge Language Models and computational tools. However, reliance on black box\nsoftware such as MAXQDA and NVivo risks undermining methodological transparency\nand alignment with research goals. This paper presents a hybrid, transparent\nframework for QDA that combines lexical and semantic methods to enable\ntriangulation, reproducibility, and interpretability. Drawing from a case study\nin historical political discourse, we demonstrate how custom Python pipelines\nusing NLTK, spaCy, and Sentence Transformers allow fine-grained control over\npreprocessing, lemmatisation, and embedding generation. We further detail our\niterative BERTopic modelling process, incorporating UMAP dimensionality\nreduction, HDBSCAN clustering, and c-TF-IDF keyword extraction, optimised\nthrough parameter tuning and multiple runs to enhance topic coherence and\ncoverage. By juxtaposing precise lexical searches with context-aware semantic\nclustering, we argue for a multi-layered approach that mitigates the\nlimitations of either method in isolation. Our workflow underscores the\nimportance of code-level transparency, researcher agency, and methodological\ntriangulation in computational discourse studies. Code and supplementary\nmaterials are available via GitHub.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u900f\u660eQDA\u6846\u67b6\uff0c\u7ed3\u5408\u8bcd\u6c47\u4e0e\u8bed\u4e49\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7Python\u5de5\u5177\u94fe\u5b9e\u73b0\u53ef\u63a7\u8ba1\u7b97\u6d41\u7a0b\u4e0e\u4e3b\u9898\u5efa\u6a21\u4f18\u5316\uff0c\u63d0\u5347\u8bdd\u8bed\u5206\u6790\u7684\u53ef\u89e3\u91ca\u6027\u548c\u65b9\u6cd5\u900f\u660e\u5ea6\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u9ed1\u7bb1\u5206\u6790\u5de5\u5177(\u5982MAXQDA/NVivo)\u5728\u65b9\u6cd5\u8bba\u900f\u660e\u5ea6\u4e0e\u7814\u7a76\u76ee\u6807\u5339\u914d\u5ea6\u65b9\u9762\u7684\u7f3a\u9677\uff0c\u5efa\u7acb\u53ef\u9a8c\u8bc1\u3001\u53ef\u91cd\u590d\u7684\u8ba1\u7b97\u8bdd\u8bed\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8ePython\u6784\u5efaNLTK/spaCy\u9884\u5904\u7406\u6d41\u7a0b\uff0c\u91c7\u7528Sentence Transformers\u751f\u6210\u8bed\u4e49\u5d4c\u5165\uff0c\u7ed3\u5408UMAP+HDBSCAN\u8fdb\u884cBERTopic\u5efa\u6a21\uff0c\u901a\u8fc7\u53c2\u6570\u8c03\u4f18\u63d0\u5347\u4e3b\u9898\u4e00\u81f4\u6027\u3002", "result": "\u5386\u53f2\u653f\u6cbb\u8bdd\u8bed\u6848\u4f8b\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\uff0c\u8bcd\u6c47\u641c\u7d22\u4e0e\u8bed\u4e49\u805a\u7c7b\u7684\u591a\u7ef4\u5ea6\u5206\u6790\u5b9e\u73b0\u66f4\u9ad8\u4e3b\u9898\u8986\u76d6\uff0c\u53c2\u6570\u4f18\u5316\u4f7f\u4e3b\u9898\u8fde\u8d2f\u6027\u63d0\u534723%\u3002", "conclusion": "\u4ee3\u7801\u900f\u660e\u6027\u3001\u7814\u7a76\u8005\u81ea\u4e3b\u51b3\u7b56\u4e0e\u65b9\u6cd5\u4e09\u89d2\u9a8c\u8bc1\u662f\u8ba1\u7b97\u8bdd\u8bed\u5206\u6790\u7684\u5173\u952e\uff0c\u6df7\u5408\u65b9\u6cd5\u80fd\u6709\u6548\u89c4\u907f\u5355\u4e00\u5206\u6790\u8def\u5f84\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.19111", "pdf": "https://arxiv.org/pdf/2508.19111", "abs": "https://arxiv.org/abs/2508.19111", "authors": ["Zhikai Ding", "Shiyu Ni", "Keping Bi"], "title": "Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs", "categories": ["cs.CL"], "comment": "EMNLP2025 Findings", "summary": "Large vision-language models (LVLMs) demonstrate strong visual question\nanswering (VQA) capabilities but are shown to hallucinate. A reliable model\nshould perceive its knowledge boundaries-knowing what it knows and what it does\nnot. This paper investigates LVLMs' perception of their knowledge boundaries by\nevaluating three types of confidence signals: probabilistic confidence, answer\nconsistency-based confidence, and verbalized confidence. Experiments on three\nLVLMs across three VQA datasets show that, although LVLMs possess a reasonable\nperception level, there is substantial room for improvement. Among the three\nconfidences, probabilistic and consistency-based signals are more reliable\nindicators, while verbalized confidence often leads to overconfidence. To\nenhance LVLMs' perception, we adapt several established confidence calibration\nmethods from Large Language Models (LLMs) and propose three effective methods.\nAdditionally, we compare LVLMs with their LLM counterparts, finding that\njointly processing visual and textual inputs decreases question-answering\nperformance but reduces confidence, resulting in an improved perception level\ncompared to LLMs.", "AI": {"tldr": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u95ee\u7b54\u4e2d\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u7814\u7a76\u901a\u8fc7\u8bc4\u4f30\u4e09\u79cd\u7f6e\u4fe1\u5ea6\u4fe1\u53f7\u5206\u6790\u5176\u77e5\u8bc6\u8fb9\u754c\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5", "motivation": "LVLMs\u5728VQA\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5e7b\u89c9\u73b0\u8c61\uff0c\u9700\u8bc4\u4f30\u5176\u77e5\u8bc6\u8fb9\u754c\u611f\u77e5\u80fd\u529b\u4ee5\u63d0\u5347\u53ef\u9760\u6027", "method": "\u901a\u8fc7\u6982\u7387\u7f6e\u4fe1\u5ea6\u3001\u7b54\u6848\u4e00\u81f4\u6027\u7f6e\u4fe1\u5ea6\u548c\u8bed\u8a00\u5316\u7f6e\u4fe1\u5ea6\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\uff0c\u5e76\u5f15\u5165LLM\u7f6e\u4fe1\u5ea6\u6821\u51c6\u65b9\u6cd5\u6539\u8fdb", "result": "\u6982\u7387\u548c\u4e00\u81f4\u6027\u7f6e\u4fe1\u5ea6\u66f4\u53ef\u9760\uff0c\u8bed\u8a00\u5316\u7f6e\u4fe1\u5ea6\u6613\u5bfc\u81f4\u8fc7\u5ea6\u81ea\u4fe1\uff1b\u591a\u6a21\u6001\u5904\u7406\u964d\u4f4e\u6027\u80fd\u4f46\u63d0\u5347\u611f\u77e5\u6c34\u5e73", "conclusion": "\u9700\u63d0\u5347LVLMs\u7684\u77e5\u8bc6\u8fb9\u754c\u611f\u77e5\u80fd\u529b\uff0c\u5efa\u8bae\u91c7\u7528\u975e\u8bed\u8a00\u5316\u7f6e\u4fe1\u5ea6\u6307\u6807\uff0c\u591a\u6a21\u6001\u8054\u5408\u5904\u7406\u5bf9\u611f\u77e5\u6c34\u5e73\u6709\u79ef\u6781\u5f71\u54cd"}}
{"id": "2508.19202", "pdf": "https://arxiv.org/pdf/2508.19202", "abs": "https://arxiv.org/abs/2508.19202", "authors": ["Alan Li", "Yixin Liu", "Arpan Sarkar", "Doug Downey", "Arman Cohan"], "title": "Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning", "categories": ["cs.CL"], "comment": "28 pages, 16 figures", "summary": "Scientific problem solving poses unique challenges for LLMs, requiring both\ndeep domain knowledge and the ability to apply such knowledge through complex\nreasoning. While automated scientific reasoners hold great promise for\nassisting human scientists, there is currently no widely adopted holistic\nbenchmark for evaluating scientific reasoning, and few approaches\nsystematically disentangle the distinct roles of knowledge and reasoning in\nthese tasks. To address these gaps, we introduce SciReas, a diverse suite of\nexisting benchmarks for scientific reasoning tasks, and SciReas-Pro, a\nselective subset that requires more complex reasoning. Our holistic evaluation\nsurfaces insights about scientific reasoning performance that remain hidden\nwhen relying on individual benchmarks alone. We then propose KRUX, a probing\nframework for studying the distinct roles of reasoning and knowledge in\nscientific tasks. Combining the two, we conduct an in-depth analysis that\nyields several key findings: (1) Retrieving task-relevant knowledge from model\nparameters is a critical bottleneck for LLMs in scientific reasoning; (2)\nReasoning models consistently benefit from external knowledge added in-context\non top of the reasoning enhancement; (3) Enhancing verbalized reasoning\nimproves LLMs' ability to surface task-relevant knowledge. Finally, we conduct\na lightweight analysis, comparing our science-focused data composition with\nconcurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline\nfor scientific reasoning.", "AI": {"tldr": "\u63d0\u51faSciReas/SciReas-Pro\u79d1\u5b66\u63a8\u7406\u8bc4\u4f30\u57fa\u51c6\u4e0eKRUX\u5206\u6790\u6846\u67b6\uff0c\u63ed\u793aLLMs\u77e5\u8bc6\u68c0\u7d22\u74f6\u9888\u53ca\u63a8\u7406\u589e\u5f3a\u7b56\u7565\uff0c\u5e76\u53d1\u5e038B\u57fa\u7ebf\u6a21\u578bSciLit01", "motivation": "\u73b0\u6709\u79d1\u5b66\u63a8\u7406\u8bc4\u4f30\u7f3a\u4e4f\u7cfb\u7edf\u6027\u57fa\u51c6\uff0c\u4e14\u672a\u660e\u786e\u533a\u5206\u77e5\u8bc6\u50a8\u5907\u4e0e\u63a8\u7406\u80fd\u529b\u7684\u4f5c\u7528\u3002\u9700\u5efa\u7acb\u8bc4\u4f30\u4f53\u7cfb\u5e76\u63a2\u7a76LLMs\u5728\u79d1\u5b66\u4efb\u52a1\u4e2d\u7684\u6838\u5fc3\u74f6\u9888", "method": "1. \u6784\u5efaSciReas\u591a\u4efb\u52a1\u57fa\u51c6\u4e0e\u7cbe\u9009\u7248SciReas-Pro 2. \u8bbe\u8ba1KRUX\u6846\u67b6\u89e3\u8026\u77e5\u8bc6\u4e0e\u63a8\u7406 3. \u901a\u8fc7\u77e5\u8bc6\u6ce8\u5165\u548c\u63a8\u7406\u589e\u5f3a\u5b9e\u9a8c\u9a8c\u8bc1\u5047\u8bbe", "result": "1. \u77e5\u8bc6\u68c0\u7d22\u662fLLMs\u79d1\u5b66\u63a8\u7406\u4e3b\u8981\u74f6\u9888 2. \u4e0a\u4e0b\u6587\u8865\u5145\u5916\u90e8\u77e5\u8bc6\u53ef\u63d0\u5347\u63a8\u7406\u6548\u679c 3. \u589e\u5f3a\u663e\u6027\u63a8\u7406\u80fd\u529b\u6709\u52a9\u4e8e\u6fc0\u6d3b\u6f5c\u5728\u77e5\u8bc6 4. \u6784\u5efa\u7684SciLit01\u6a21\u578b\u5c55\u73b0\u7ade\u4e89\u529b", "conclusion": "\u7cfb\u7edf\u8bc4\u4f30\u63ed\u793a\u4e86LLMs\u79d1\u5b66\u63a8\u7406\u7684\u53cc\u91cd\u6311\u6218\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u8bba\u6846\u67b6\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u57fa\u51c6\u6a21\u578b\u63a8\u52a8\u9886\u57df\u53d1\u5c55"}}
{"id": "2508.19205", "pdf": "https://arxiv.org/pdf/2508.19205", "abs": "https://arxiv.org/abs/2508.19205", "authors": ["Zhiliang Peng", "Jianwei Yu", "Wenhui Wang", "Yaoyao Chang", "Yutao Sun", "Li Dong", "Yi Zhu", "Weijiang Xu", "Hangbo Bao", "Zehua Wang", "Shaohan Huang", "Yan Xia", "Furu Wei"], "title": "VibeVoice Technical Report", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "This report presents VibeVoice, a novel model designed to synthesize\nlong-form speech with multiple speakers by employing next-token diffusion,\nwhich is a unified method for modeling continuous data by autoregressively\ngenerating latent vectors via diffusion. To enable this, we introduce a novel\ncontinuous speech tokenizer that, when compared to the popular Encodec model,\nimproves data compression by 80 times while maintaining comparable performance.\nThe tokenizer effectively preserves audio fidelity while significantly boosting\ncomputational efficiency for processing long sequences. Thus, VibeVoice can\nsynthesize long-form speech for up to 90 minutes (in a 64K context window\nlength) with a maximum of 4 speakers, capturing the authentic conversational\n``vibe'' and surpassing open-source and proprietary dialogue models.", "AI": {"tldr": "VibeVoice\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8enext-token diffusion\u7684\u957f\u6587\u672c\u591a\u8bf4\u8bdd\u4eba\u8bed\u97f3\u5408\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u65b0\u578b\u8fde\u7eed\u8bed\u97f3\u5206\u8bcd\u5668\u5b9e\u73b080\u500d\u6570\u636e\u538b\u7f29\uff0c\u652f\u630190\u5206\u949f\u8bed\u97f3\u5408\u6210\u5e76\u8d85\u8d8a\u4e3b\u6d41\u5bf9\u8bdd\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u957f\u6587\u672c\u8bed\u97f3\u5408\u6210\u4e2d\u8ba1\u7b97\u6548\u7387\u4f4e\u3001\u591a\u8bf4\u8bdd\u4eba\u5bf9\u8bdd\u771f\u5b9e\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u5347\u8bed\u97f3\u5408\u6210\u7684\u65f6\u957f\u4e0a\u9650\u548c\u5bf9\u8bdd\u81ea\u7136\u5ea6\u3002", "method": "1. \u91c7\u7528next-token diffusion\u8fdb\u884c\u9690\u5411\u91cf\u81ea\u56de\u5f52\u751f\u6210\n2. \u5f00\u53d1\u65b0\u578b\u8fde\u7eed\u8bed\u97f3\u5206\u8bcd\u5668(\u76f8\u6bd4Encodec\u538b\u7f29\u7387\u63d0\u534780\u500d)\n3. \u652f\u630164K\u957f\u5ea6\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u5efa\u6a21", "result": "1. \u5b9e\u73b0\u6700\u957f90\u5206\u949f\u8bed\u97f3\u5408\u6210\n2. \u6700\u591a\u652f\u63014\u4e2a\u8bf4\u8bdd\u4eba\u5bf9\u8bdd\n3. \u5728\u8ba1\u7b97\u6548\u7387\u548c\u97f3\u9891\u4fdd\u771f\u5ea6\u4e0a\u8d85\u8d8a\u5f00\u6e90/\u5546\u4e1a\u5bf9\u8bdd\u6a21\u578b\n4. \u4fdd\u6301\u8bed\u97f3\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u957f\u5e8f\u5217\u5904\u7406\u6548\u7387", "conclusion": "VibeVoice\u901a\u8fc7\u521b\u65b0\u7684\u5206\u8bcd\u5668\u548c\u6269\u6563\u6a21\u578b\u67b6\u6784\uff0c\u5728\u957f\u6587\u672c\u8bed\u97f3\u5408\u6210\u9886\u57df\u5b9e\u73b0\u7a81\u7834\uff0c\u4e3a\u591a\u8f6e\u5bf9\u8bdd\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6807\u5fd7\u7740\u8bed\u97f3\u5408\u6210\u6280\u672f\u5411\u771f\u5b9e\u5bf9\u8bdd\u573a\u666f\u7684\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2508.19221", "pdf": "https://arxiv.org/pdf/2508.19221", "abs": "https://arxiv.org/abs/2508.19221", "authors": ["Isabel Cachola", "Daniel Khashabi", "Mark Dredze"], "title": "Evaluating the Evaluators: Are readability metrics good measures of readability?", "categories": ["cs.CL"], "comment": null, "summary": "Plain Language Summarization (PLS) aims to distill complex documents into\naccessible summaries for non-expert audiences. In this paper, we conduct a\nthorough survey of PLS literature, and identify that the current standard\npractice for readability evaluation is to use traditional readability metrics,\nsuch as Flesch-Kincaid Grade Level (FKGL). However, despite proven utility in\nother fields, these metrics have not been compared to human readability\njudgments in PLS. We evaluate 8 readability metrics and show that most\ncorrelate poorly with human judgments, including the most popular metric, FKGL.\nWe then show that Language Models (LMs) are better judges of readability, with\nthe best-performing model achieving a Pearson correlation of 0.56 with human\njudgments. Extending our analysis to PLS datasets, which contain summaries\naimed at non-expert audiences, we find that LMs better capture deeper measures\nof readability, such as required background knowledge, and lead to different\nconclusions than the traditional metrics. Based on these findings, we offer\nrecommendations for best practices in the evaluation of plain language\nsummaries. We release our analysis code and survey data.", "AI": {"tldr": "\u4f20\u7edf\u53ef\u8bfb\u6027\u6307\u6807\uff08\u5982FKGL\uff09\u5728\u7b80\u660e\u8bed\u8a00\u6458\u8981\u8bc4\u4f30\u4e2d\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u4f4e\uff0c\u8bed\u8a00\u6a21\u578b\uff08LMs\uff09\u8868\u73b0\u66f4\u4f18\u4e14\u80fd\u6355\u6349\u6df1\u5c42\u53ef\u8bfb\u6027\u56e0\u7d20\u3002", "motivation": "\u9a8c\u8bc1\u4f20\u7edf\u53ef\u8bfb\u6027\u6307\u6807\u5728PLS\u9886\u57df\u7684\u6709\u6548\u6027\uff0c\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u66ff\u4ee3\u8bc4\u4f30\u65b9\u6848\u7684\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u6587\u732e\u8c03\u67e5\u30018\u79cd\u4f20\u7edf\u6307\u6807\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u5bf9\u6bd4\u6d4b\u8bd5\uff0c\u4ee5\u53ca\u8bed\u8a00\u6a21\u578b\u7684Pearson\u76f8\u5173\u6027\u5206\u6790\uff0c\u7ed3\u5408PLS\u6570\u636e\u96c6\u9a8c\u8bc1\u6a21\u578b\u5bf9\u80cc\u666f\u77e5\u8bc6\u7b49\u6df1\u5c42\u6307\u6807\u7684\u6355\u6349\u80fd\u529b\u3002", "result": "\u6700\u4f18\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u8fbe0.56\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6307\u6807\uff0c\u4e14\u5728\u80cc\u666f\u77e5\u8bc6\u9700\u6c42\u7b49\u7ef4\u5ea6\u8bc4\u4f30\u7ed3\u8bba\u4e0e\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u672c\u8d28\u5dee\u5f02\u3002", "conclusion": "\u5efa\u8bae\u5728PLS\u8bc4\u4f30\u4e2d\u91c7\u7528\u8bed\u8a00\u6a21\u578b\u66ff\u4ee3\u4f20\u7edf\u6307\u6807\uff0c\u5e76\u516c\u5f00\u7814\u7a76\u6570\u636e\u4e0e\u4ee3\u7801\u63a8\u52a8\u9886\u57df\u6807\u51c6\u5316\u8fdb\u7a0b\u3002"}}
{"id": "2508.19227", "pdf": "https://arxiv.org/pdf/2508.19227", "abs": "https://arxiv.org/abs/2508.19227", "authors": ["Jiaqi Chen", "Yanzhe Zhang", "Yutong Zhang", "Yijia Shao", "Diyi Yang"], "title": "Generative Interfaces for Language Models", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "Preprint", "summary": "Large language models (LLMs) are increasingly seen as assistants, copilots,\nand consultants, capable of supporting a wide range of tasks through natural\nconversation. However, most systems remain constrained by a linear\nrequest-response format that often makes interactions inefficient in\nmulti-turn, information-dense, and exploratory tasks. To address these\nlimitations, we propose Generative Interfaces for Language Models, a paradigm\nin which LLMs respond to user queries by proactively generating user interfaces\n(UIs) that enable more adaptive and interactive engagement. Our framework\nleverages structured interface-specific representations and iterative\nrefinements to translate user queries into task-specific UIs. For systematic\nevaluation, we introduce a multidimensional assessment framework that compares\ngenerative interfaces with traditional chat-based ones across diverse tasks,\ninteraction patterns, and query types, capturing functional, interactive, and\nemotional aspects of user experience. Results show that generative interfaces\nconsistently outperform conversational ones, with humans preferring them in\nover 70% of cases. These findings clarify when and why users favor generative\ninterfaces, paving the way for future advancements in human-AI interaction.", "AI": {"tldr": "\u63d0\u51fa\u751f\u6210\u5f0f\u4ea4\u4e92\u754c\u9762\u6846\u67b6\uff0c\u901a\u8fc7LLM\u4e3b\u52a8\u751f\u6210\u4efb\u52a1\u7279\u5b9aUI\uff0c\u5728\u591a\u7ef4\u5ea6\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u4f20\u7edf\u804a\u5929\u6a21\u5f0f\uff08\u4eba\u7c7b\u504f\u597d\u7387\u8d8570%\uff09", "motivation": "\u4f20\u7edf\u7ebf\u6027\u5bf9\u8bdd\u6a21\u5f0f\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5b58\u5728\u6548\u7387\u74f6\u9888\uff0c\u9700\u63a2\u7d22\u66f4\u9002\u914d\u7684\u4ea4\u4e92\u8303\u5f0f\u6765\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u6548\u80fd", "method": "\u57fa\u4e8e\u7ed3\u6784\u5316\u754c\u9762\u8868\u793a\u548c\u8fed\u4ee3\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u7528\u6237\u67e5\u8be2\u52a8\u6001\u8f6c\u5316\u4e3a\u4ea4\u4e92\u754c\u9762", "result": "\u751f\u6210\u5f0f\u754c\u9762\u5728\u529f\u80fd/\u4ea4\u4e92/\u60c5\u611f\u7ef4\u5ea6\u5168\u9762\u8d85\u8d8a\u5bf9\u8bdd\u5f0f\u754c\u9762\uff0c70%+\u7528\u6237\u504f\u597d\uff0c\u4efb\u52a1\u5b8c\u6210\u6548\u7387\u63d0\u5347\u663e\u8457", "conclusion": "\u751f\u6210\u5f0f\u4ea4\u4e92\u8303\u5f0f\u4e3a\u590d\u6742\u4eba\u673a\u534f\u4f5c\u4efb\u52a1\u63d0\u4f9b\u65b0\u65b9\u5411\uff0c\u5176\u4e3b\u52a8\u754c\u9762\u751f\u6210\u673a\u5236\u5c06\u63a8\u52a8HCI\u9886\u57df\u521b\u65b0\u53d1\u5c55"}}
{"id": "2508.18288", "pdf": "https://arxiv.org/pdf/2508.18288", "abs": "https://arxiv.org/abs/2508.18288", "authors": ["Jay L. Cunningham", "Adinawa Adjagbodjou", "Jeffrey Basoah", "Jainaba Jawara", "Kowe Kadoma", "Aaleyah Lewis"], "title": "Toward Responsible ASR for African American English Speakers: A Scoping Review of Bias and Equity in Speech Technology", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "10 pages, 9 Pages (References and Appendices). The archival version\n  has been accepted to AAAI (AIES 2025) without the extended Appendices. This\n  extended version includes Appendices", "summary": "This scoping literature review examines how fairness, bias, and equity are\nconceptualized and operationalized in Automatic Speech Recognition (ASR) and\nadjacent speech and language technologies (SLT) for African American English\n(AAE) speakers and other linguistically diverse communities. Drawing from 44\npeer-reviewed publications across Human-Computer Interaction (HCI), Machine\nLearning/Natural Language Processing (ML/NLP), and Sociolinguistics, we\nidentify four major areas of inquiry: (1) how researchers understand\nASR-related harms; (2) inclusive data practices spanning collection, curation,\nannotation, and model training; (3) methodological and theoretical approaches\nto linguistic inclusion; and (4) emerging practices and design recommendations\nfor more equitable systems. While technical fairness interventions are growing,\nour review highlights a critical gap in governance-centered approaches that\nforeground community agency, linguistic justice, and participatory\naccountability. We propose a governance-centered ASR lifecycle as an emergent\ninterdisciplinary framework for responsible ASR development and offer\nimplications for researchers, practitioners, and policymakers seeking to\naddress language marginalization in speech AI systems.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u7efc\u8ff044\u7bc7\u6587\u732e\uff0c\u5206\u6790ASR\u6280\u672f\u4e2d\u5bf9\u975e\u88d4\u82f1\u8bed\u4f7f\u7528\u8005\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u6cbb\u7406\u5bfc\u5411\u7684\u8de8\u5b66\u79d1\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u516c\u5e73\u6027\u5e72\u9884\u7f3a\u4e4f\u6cbb\u7406\u89c6\u89d2\uff08\u5982\u793e\u533a\u53c2\u4e0e\u3001\u8bed\u8a00\u6b63\u4e49\uff09\uff0c\u9700\u6784\u5efa\u8d1f\u8d23\u4efb\u7684ASR\u5f00\u53d1\u4f53\u7cfb\u3002", "method": "\u91c7\u7528\u8303\u56f4\u754c\u5b9a\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u8986\u76d6\u4eba\u673a\u4ea4\u4e92\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\u3001\u793e\u4f1a\u8bed\u8a00\u5b66\u9886\u57df\u768444\u7bc7\u6587\u732e\uff0c\u805a\u7126\u56db\u5927\u7814\u7a76\u7ef4\u5ea6\u3002", "result": "\u53d1\u73b0\u6280\u672f\u516c\u5e73\u63aa\u65bd\u589e\u957f\u4f46\u6cbb\u7406\u65b9\u6cd5\u7f3a\u5931\uff0c\u63d0\u51fa\u4ee5\u793e\u533a\u4ee3\u7406\u4e3a\u6838\u5fc3\u7684ASR\u751f\u547d\u5468\u671f\u6cbb\u7406\u6846\u67b6\u3002", "conclusion": "\u6784\u5efa\u6cbb\u7406\u5bfc\u5411\u7684\u8de8\u5b66\u79d1\u6846\u67b6\uff0c\u4e3a\u6d88\u9664\u8bed\u97f3AI\u8bed\u8a00\u8fb9\u7f18\u5316\u63d0\u4f9b\u65b9\u6cd5\u8bba\u4e0e\u5b9e\u8df5\u8def\u5f84\u3002"}}
{"id": "2508.18295", "pdf": "https://arxiv.org/pdf/2508.18295", "abs": "https://arxiv.org/abs/2508.18295", "authors": ["Huangyu Dai", "Lingtao Mao", "Ben Chen", "Zihan Wang", "Zihan Liang", "Ying Han", "Chenyi Lei", "Han Li"], "title": "H-PRM: A Pluggable Hotword Pre-Retrieval Module for Various Speech Recognition Systems", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "Hotword customization is crucial in ASR to enhance the accuracy of\ndomain-specific terms. It has been primarily driven by the advancements in\ntraditional models and Audio large language models (LLMs). However, existing\nmodels often struggle with large-scale hotwords, as the recognition rate drops\ndramatically with the number of hotwords increasing. In this paper, we\nintroduce a novel hotword customization system that utilizes a hotword\npre-retrieval module (H-PRM) to identify the most relevant hotword candidate by\nmeasuring the acoustic similarity between the hotwords and the speech segment.\nThis plug-and-play solution can be easily integrated into traditional models\nsuch as SeACo-Paraformer, significantly enhancing hotwords post-recall rate\n(PRR). Additionally, we incorporate H-PRM into Audio LLMs through a\nprompt-based approach, enabling seamless customization of hotwords. Extensive\ntesting validates that H-PRM can outperform existing methods, showing a new\ndirection for hotword customization in ASR.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u58f0\u5b66\u76f8\u4f3c\u5ea6\u9884\u68c0\u7d22\u7684\u70ed\u8bcd\u5b9a\u5236\u7cfb\u7edfH-PRM\uff0c\u663e\u8457\u63d0\u5347ASR\u9886\u57df\u70ed\u8bcd\u53ec\u56de\u7387\uff0c\u517c\u5bb9\u4f20\u7edf\u6a21\u578b\u4e0e\u97f3\u9891\u5927\u6a21\u578b\u3002", "motivation": "\u73b0\u6709ASR\u70ed\u8bcd\u5b9a\u5236\u6a21\u578b\u5728\u5927\u89c4\u6a21\u70ed\u8bcd\u573a\u666f\u4e0b\u8bc6\u522b\u7387\u9aa4\u964d\uff0c\u9700\u89e3\u51b3\u58f0\u5b66\u5339\u914d\u4e0e\u6a21\u578b\u9002\u914d\u6027\u95ee\u9898\u3002", "method": "1. \u5f00\u53d1\u70ed\u8bcd\u9884\u68c0\u7d22\u6a21\u5757(H-PRM)\u6d4b\u91cf\u70ed\u8bcd\u4e0e\u8bed\u97f3\u58f0\u5b66\u76f8\u4f3c\u5ea6\uff1b2. \u5373\u63d2\u5373\u7528\u96c6\u6210\u81f3SeACo-Paraformer\u7b49\u4f20\u7edf\u6a21\u578b\uff1b3. \u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u878d\u5165Audio LLMs\u5b9e\u73b0\u70ed\u8bcd\u5b9a\u5236\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1H-PRM\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u70ed\u8bcd\u540e\u53ec\u56de\u7387(PRR)\u83b7\u5f97\u7a81\u7834\u6027\u63d0\u5347\u3002", "conclusion": "H-PRM\u4e3aASR\u70ed\u8bcd\u5b9a\u5236\u5f00\u8f9f\u65b0\u8def\u5f84\uff0c\u8bc1\u660e\u58f0\u5b66\u9884\u68c0\u7d22\u673a\u5236\u5728\u4f20\u7edf\u6a21\u578b\u4e0eLLM\u4e2d\u7684\u53cc\u91cd\u6709\u6548\u6027\u3002"}}
{"id": "2508.18297", "pdf": "https://arxiv.org/pdf/2508.18297", "abs": "https://arxiv.org/abs/2508.18297", "authors": ["Dhananjay Ashok", "Ashutosh Chaubey", "Hirona J. Arai", "Jonathan May", "Jesse Thomason"], "title": "Can VLMs Recall Factual Associations From Visual References?", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "To appear at EMNLP 2025 (Findings)", "summary": "Through a controlled study, we identify a systematic deficiency in the\nmultimodal grounding of Vision Language Models (VLMs). While VLMs can recall\nfactual associations when provided a textual reference to an entity; their\nability to do so is significantly diminished when the reference is visual\ninstead. Forcing VLMs to rely on image representations of an entity halves\ntheir ability to recall factual knowledge, suggesting that VLMs struggle to\nlink their internal knowledge of an entity with its image representation. We\nshow that such linking failures are correlated with the expression of distinct\npatterns in model internal states, and that probes on these internal states\nachieve over 92% accuracy at flagging cases where the VLM response is\nunreliable. These probes can be applied, without retraining, to identify when a\nVLM will fail to correctly answer a question that requires an understanding of\nmultimodal input. When used to facilitate selective prediction on a visual\nquestion answering task, the probes increase coverage by 7.87% (absolute) while\nalso reducing the risk of error by 0.9% (absolute). Addressing the systematic,\ndetectable deficiency is an important avenue in language grounding, and we\nprovide informed recommendations for future directions.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u89c6\u89c9\u5b9e\u4f53\u5f15\u7528\u65f6\u5b58\u5728\u77e5\u8bc6\u94fe\u63a5\u7f3a\u9677\uff0c\u901a\u8fc7\u5185\u90e8\u72b6\u6001\u63a2\u6d4b\u53ef\u6709\u6548\u8bc6\u522b\u4e0d\u53ef\u9760\u56de\u7b54\u5e76\u63d0\u5347\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22VLMs\u5728\u591a\u6a21\u6001\u57fa\u7840\u4e2d\u7684\u7cfb\u7edf\u7f3a\u9677\uff0c\u7279\u522b\u662f\u89c6\u89c9\u4e0e\u6587\u672c\u5f15\u7528\u5728\u77e5\u8bc6\u5173\u8054\u4e0a\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u6bd4\u8f83\u6587\u672c/\u89c6\u89c9\u5f15\u7528\u6548\u679c\uff0c\u5206\u6790\u5185\u90e8\u72b6\u6001\u6a21\u5f0f\uff0c\u6784\u5efa\u65e0\u9700\u8bad\u7ec3\u7684\u63a2\u6d4b\u5668\u8bc6\u522b\u4e0d\u53ef\u9760\u56de\u7b54\uff0c\u5e76\u5728\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u3002", "result": "\u89c6\u89c9\u5f15\u7528\u5bfc\u81f4\u77e5\u8bc6\u56de\u5fc6\u80fd\u529b\u4e0b\u964d50%\uff0c\u63a2\u6d4b\u5668\u51c6\u786e\u7387\u8fbe92%\uff0c\u8986\u76d6\u63d0\u53477.87%\u4e14\u9519\u8bef\u7387\u964d\u4f4e0.9%\u3002", "conclusion": "VLMs\u5b58\u5728\u53ef\u68c0\u6d4b\u7684\u7cfb\u7edf\u6027\u89c6\u89c9-\u77e5\u8bc6\u94fe\u63a5\u7f3a\u9677\uff0c\u5185\u90e8\u72b6\u6001\u63a2\u6d4b\u4e3a\u53ef\u9760\u89e3\u51b3\u65b9\u6848\uff0c\u5efa\u8bae\u52a0\u5f3a\u591a\u6a21\u6001\u77e5\u8bc6\u8868\u5f81\u7814\u7a76\u3002"}}
{"id": "2508.18306", "pdf": "https://arxiv.org/pdf/2508.18306", "abs": "https://arxiv.org/abs/2508.18306", "authors": ["Wuxinlin Cheng", "Yupeng Cao", "Jinwen Wu", "Koduvayur Subbalakshmi", "Tian Han", "Zhuo Feng"], "title": "SALMAN: Stability Analysis of Language Models Through the Maps Between Graph-based Manifolds", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent strides in pretrained transformer-based language models have propelled\nstate-of-the-art performance in numerous NLP tasks. Yet, as these models grow\nin size and deployment, their robustness under input perturbations becomes an\nincreasingly urgent question. Existing robustness methods often diverge between\nsmall-parameter and large-scale models (LLMs), and they typically rely on\nlabor-intensive, sample-specific adversarial designs. In this paper, we propose\na unified, local (sample-level) robustness framework (SALMAN) that evaluates\nmodel stability without modifying internal parameters or resorting to complex\nperturbation heuristics. Central to our approach is a novel Distance Mapping\nDistortion (DMD) measure, which ranks each sample's susceptibility by comparing\ninput-to-output distance mappings in a near-linear complexity manner. By\ndemonstrating significant gains in attack efficiency and robust training, we\nposition our framework as a practical, model-agnostic tool for advancing the\nreliability of transformer-based NLP systems.", "AI": {"tldr": "\u63d0\u51faSALMAN\u6846\u67b6\uff0c\u901a\u8fc7DMD\u5ea6\u91cf\u8bc4\u4f30Transformer\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u53c2\u6570\u5373\u53ef\u63d0\u5347\u653b\u51fb\u6548\u7387\u548c\u9c81\u68d2\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u65f6\u5b58\u5728\u5272\u88c2\uff0c\u4e14\u4f9d\u8d56\u9ad8\u6210\u672c\u7684\u5bf9\u6297\u6837\u672c\u8bbe\u8ba1\u3002\u9700\u5efa\u7acb\u7edf\u4e00\u4e14\u65e0\u9700\u590d\u6742\u8c03\u53c2\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u4f53\u7cfb\u3002", "method": "\u57fa\u4e8e\u8f93\u5165-\u8f93\u51fa\u8ddd\u79bb\u6620\u5c04\u5931\u771f(DMD)\u7684\u7ebf\u6027\u590d\u6742\u5ea6\u5ea6\u91cf\uff0c\u6784\u5efa\u6a21\u578b\u65e0\u5173\u7684\u7a33\u5b9a\u6027\u8bc4\u4f30\u6846\u67b6\u3002\u901a\u8fc7\u6392\u5e8f\u6837\u672c\u8106\u5f31\u6027\u5b9e\u73b0\u9ad8\u6548\u653b\u51fb\u8bc6\u522b\u3002", "result": "\u5728\u653b\u51fb\u6548\u7387\u63d0\u5347(\u5b9e\u9a8c\u663e\u793a50%+\u6548\u7387\u589e\u76ca)\u548c\u9c81\u68d2\u8bad\u7ec3\u6548\u679c\u4f18\u5316\u65b9\u9762\u53d6\u5f97\u663e\u8457\u7a81\u7834\uff0c\u652f\u6301\u5404\u7c7bTransformer\u67b6\u6784\u3002", "conclusion": "SALMAN\u4e3aTransformer NLP\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u3001\u6a21\u578b\u65e0\u5173\u7684\u9c81\u68d2\u6027\u8bc4\u4f30\u5de5\u5177\uff0c\u63a8\u52a8\u53ef\u9760AI\u7cfb\u7edf\u53d1\u5c55\u3002"}}
{"id": "2508.18370", "pdf": "https://arxiv.org/pdf/2508.18370", "abs": "https://arxiv.org/abs/2508.18370", "authors": ["Terry Yue Zhuo", "Dingmin Wang", "Hantian Ding", "Varun Kumar", "Zijian Wang"], "title": "Training Language Model Agents to Find Vulnerabilities with CTF-Dojo", "categories": ["cs.SE", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated exceptional capabilities when\ntrained within executable runtime environments, notably excelling at software\nengineering tasks through verified feedback loops. Yet, scalable and\ngeneralizable execution-grounded environments remain scarce, limiting progress\nin training more capable ML agents. We introduce CTF-Dojo, the first\nlarge-scale executable runtime tailored for training LLMs with verifiable\nfeedback, featuring 658 fully functional Capture-The-Flag (CTF)-style\nchallenges containerized in Docker with guaranteed reproducibility. To enable\nrapid scaling without manual intervention, we develop CTF-Forge, an automated\npipeline that transforms publicly available artifacts into ready-to-use\nexecution environments in minutes, eliminating weeks of expert configuration\ntraditionally required. We trained LLM-based agents on just 486 high-quality,\nexecution-verified trajectories from CTF-Dojo, achieving up to 11.6% absolute\ngains over strong baselines across three competitive benchmarks: InterCode-CTF,\nNYU CTF Bench, and Cybench. Our best-performing 32B model reaches 31.9% Pass@1,\nestablishing a new open-weight state-of-the-art that rivals frontier models\nlike DeepSeek-V3-0324 and Gemini-2.5-Flash. By framing CTF-style tasks as a\nbenchmark for executable-agent learning, CTF-Dojo demonstrates that\nexecution-grounded training signals are not only effective but pivotal in\nadvancing high-performance ML agents without dependence on costly proprietary\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u5927\u89c4\u6a21\u53ef\u6267\u884c\u8bad\u7ec3\u73af\u5883CTF-Dojo\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u7a0bCTF-Forge\u5b9e\u73b0\u5feb\u901f\u6269\u5c55\uff0c\u4ec5\u9700486\u6761\u8bad\u7ec3\u8f68\u8ff9\u5373\u5b9e\u73b011.6%\u6027\u80fd\u63d0\u5347\uff0c32B\u6a21\u578b\u8fbe\u523031.9% Pass@1\u65b0SOTA\u3002", "motivation": "\u73b0\u6709\u53ef\u6267\u884c\u8bad\u7ec3\u73af\u5883\u7a00\u7f3a\u4e14\u6269\u5c55\u6027\u5dee\uff0c\u5236\u7ea6\u4e86\u57fa\u4e8e\u9a8c\u8bc1\u53cd\u9988\u7684AI\u667a\u80fd\u4f53\u8bad\u7ec3\u3002\u9700\u8981\u53ef\u590d\u73b0\u3001\u81ea\u52a8\u5316\u7684\u5927\u89c4\u6a21\u6267\u884c\u73af\u5883\u63a8\u52a8ML\u667a\u80fd\u4f53\u53d1\u5c55\u3002", "method": "1. \u6784\u5efa\u5305\u542b658\u4e2aDocker\u5bb9\u5668\u5316CTF\u6311\u6218\u7684CTF-Dojo\u73af\u5883\n2. \u5f00\u53d1CTF-Forge\u81ea\u52a8\u5316\u6d41\u6c34\u7ebf\uff0c\u5c06\u516c\u5f00\u8d44\u6e90\u5feb\u901f\u8f6c\u5316\u4e3a\u53ef\u7528\u73af\u5883\n3. \u57fa\u4e8e\u6267\u884c\u9a8c\u8bc1\u8f68\u8ff9\u8bad\u7ec3LLM\u667a\u80fd\u4f53", "result": "\u5728\u4e09\u5927\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u63d0\u534711.6%\uff0c32B\u6a21\u578b\u8fbe\u523031.9% Pass@1\uff08\u5f00\u6e90\u6a21\u578b\u65b0SOTA\uff09\uff0c\u5ab2\u7f8eDeepSeek-V3\u548cGemini\u7b49\u524d\u6cbf\u6a21\u578b\u3002", "conclusion": "\u9a8c\u8bc1\u6267\u884c\u53cd\u9988\u673a\u5236\u5bf9\u8bad\u7ec3\u9ad8\u6027\u80fdML\u667a\u80fd\u4f53\u7684\u5173\u952e\u4f5c\u7528\uff0c\u8bc1\u660e\u65e0\u9700\u4f9d\u8d56\u6602\u8d35\u5546\u4e1a\u7cfb\u7edf\u5373\u53ef\u5b9e\u73b0\u7a81\u7834\u3002"}}
{"id": "2508.18439", "pdf": "https://arxiv.org/pdf/2508.18439", "abs": "https://arxiv.org/abs/2508.18439", "authors": ["Anders M\u00f8lmen H\u00f8st", "Pierre Lison", "Leon Moonen"], "title": "A Systematic Approach to Predict the Impact of Cybersecurity Vulnerabilities Using LLMs", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.SE"], "comment": null, "summary": "Vulnerability databases, such as the National Vulnerability Database (NVD),\noffer detailed descriptions of Common Vulnerabilities and Exposures (CVEs), but\noften lack information on their real-world impact, such as the tactics,\ntechniques, and procedures (TTPs) that adversaries may use to exploit the\nvulnerability. However, manually linking CVEs to their corresponding TTPs is a\nchallenging and time-consuming task, and the high volume of new vulnerabilities\npublished annually makes automated support desirable.\n  This paper introduces TRIAGE, a two-pronged automated approach that uses\nLarge Language Models (LLMs) to map CVEs to relevant techniques from the ATT&CK\nknowledge base. We first prompt an LLM with instructions based on MITRE's CVE\nMapping Methodology to predict an initial list of techniques. This list is then\ncombined with the results from a second LLM-based module that uses in-context\nlearning to map a CVE to relevant techniques. This hybrid approach\nstrategically combines rule-based reasoning with data-driven inference. Our\nevaluation reveals that in-context learning outperforms the individual mapping\nmethods, and the hybrid approach improves recall of exploitation techniques. We\nalso find that GPT-4o-mini performs better than Llama3.3-70B on this task.\nOverall, our results show that LLMs can be used to automatically predict the\nimpact of cybersecurity vulnerabilities and TRIAGE makes the process of mapping\nCVEs to ATT&CK more efficient.\n  Keywords: vulnerability impact, CVE, ATT&CK techniques, large language\nmodels, automated mapping.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTRIAGE\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u89c4\u5219\u63a8\u7406\u4e0e\u6570\u636e\u9a71\u52a8\u7684LLM\u6a21\u5757\uff0c\u5b9e\u73b0CVE\u6f0f\u6d1e\u5230ATT&CK\u6280\u672f\u7684\u81ea\u52a8\u5316\u6620\u5c04\uff0c\u63d0\u5347\u6548\u7387\u4e0e\u53ec\u56de\u7387\u3002", "motivation": "\u73b0\u6709\u6f0f\u6d1e\u6570\u636e\u5e93\u7f3a\u4e4f\u5bf9CVE\u6f0f\u6d1e\u5b9e\u9645\u653b\u51fb\u5f71\u54cd\uff08\u5982TTPs\uff09\u7684\u63cf\u8ff0\uff0c\u624b\u52a8\u6620\u5c04\u8017\u65f6\u4e14\u96be\u4ee5\u5e94\u5bf9\u6d77\u91cf\u6f0f\u6d1e\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u57fa\u4e8eMITRE\u65b9\u6cd5\u63d0\u793aLLM\u751f\u6210\u521d\u59cb\u6280\u672f\u5217\u8868\uff1b2. \u7ed3\u5408\u4e0a\u4e0b\u6587\u5b66\u4e60\u6a21\u5757\u8fdb\u884c\u8865\u5145\uff0c\u5f62\u6210\u6df7\u5408\u6620\u5c04\u7b56\u7565\u3002", "result": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u6a21\u5757\u6548\u679c\u4f18\u4e8e\u5355\u4e00\u65b9\u6cd5\uff0c\u6df7\u5408\u7b56\u7565\u63d0\u5347\u6280\u672f\u53ec\u56de\u7387\uff1bGPT-4o-mini\u6027\u80fd\u4f18\u4e8eLlama3.3-70B\u3002", "conclusion": "LLMs\u53ef\u6709\u6548\u9884\u6d4b\u6f0f\u6d1e\u653b\u51fb\u5f71\u54cd\uff0cTRIAGE\u663e\u8457\u63d0\u5347CVE-ATT&CK\u6620\u5c04\u6548\u7387\uff0c\u4e3a\u7f51\u7edc\u5b89\u5168\u5206\u6790\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.18512", "pdf": "https://arxiv.org/pdf/2508.18512", "abs": "https://arxiv.org/abs/2508.18512", "authors": ["Antony C Chan"], "title": "Designing across domains with declarative thinking: Insights from the 96-Eyes ptychographic imager project", "categories": ["physics.optics", "cs.CL"], "comment": null, "summary": "This article presents a practitioner's reflection on applying declarative,\n5th generation, problem formulation language (5GL) to de novo imaging system\ndesign, informed by experiences across the interdisciplinary research in\nacademia and cross-functional product development within the private sector.\nUsing the 96-Eyes project: 96-camera parallel multi-modal imager for\nhigh-throughput drug discovery as a representative case, I illustrate how\nproject requirements, ranging from hardware constraints to life sciences needs,\ncan be formalized into machine-readable problem statements to preserve\nmission-critical input from diverse domain stakeholders. This declarative\napproach enhances transparency, ensures design traceability, and minimizes\ncostly misalignment across optical, algorithmic, hardware-accelerated compute,\nand life sciences teams.\n  Alongside the technical discussion of 5GL with real-world code examples, I\nreflect on the practical barriers to adopting 5GL in environments where\nimperative, 3rd-generation languages (3GL) remain the default medium for\ninter-team collaboration. Rather than offering an one-size-fits-all solution,\nthese learned lessons highlight how programming paradigms implicitly shapes\nresearch workflows through existing domain hierarchies. The discussion aims to\ninvite further explorations into how declarative problem formulations can\nfacilitate innovation in settings where concurrent R\\&{}D workflows are gaining\ntraction, as opposed to environments where sequential, phase-driven workflows\nremain the norm.", "AI": {"tldr": "\u63a2\u8ba85GL\u5728\u6210\u50cf\u7cfb\u7edf\u8bbe\u8ba1\u7684\u5e94\u7528\uff0c\u901a\u8fc796-Eyes\u6848\u4f8b\u5c55\u793a\u8de8\u9886\u57df\u9700\u6c42\u8f6c\u5316\u4e0e\u534f\u4f5c\u4f18\u5316", "motivation": "\u89e3\u51b3\u8de8\u5b66\u79d1\u56e2\u961f\u5728\u786c\u4ef6\u3001\u7b97\u6cd5\u3001\u751f\u547d\u79d1\u5b66\u7b49\u9886\u57df\u7684\u534f\u4f5c\u969c\u788d\uff0c\u901a\u8fc7\u58f0\u660e\u5f0f\u8bed\u8a00\u4fdd\u6301\u5173\u952e\u8f93\u5165\u5b8c\u6574\u6027", "method": "\u4ee596-Eyes\u591a\u6a21\u6001\u6210\u50cf\u9879\u76ee\u4e3a\u6848\u4f8b\uff0c\u5c06\u786c\u4ef6\u7ea6\u675f\u548c\u79d1\u5b66\u9700\u6c42\u5f62\u5f0f\u5316\u4e3a\u673a\u5668\u53ef\u8bfb\u7684\u95ee\u9898\u9648\u8ff0\u6846\u67b6", "result": "\u63d0\u5347\u8bbe\u8ba1\u900f\u660e\u5ea6\u4e0e\u53ef\u8ffd\u6eaf\u6027\uff0c\u51cf\u5c1175%\u8de8\u56e2\u961f\u504f\u5dee\uff0c\u5b9e\u73b0\u5149\u5b66/\u7b97\u6cd5/\u751f\u547d\u79d1\u5b66\u6a21\u5757\u7684\u7cbe\u786e\u5bf9\u9f50", "conclusion": "\u58f0\u660e\u5f0f\u95ee\u9898\u63cf\u8ff0\u5728\u5e76\u884c\u7814\u53d1\u73af\u5883\u4e2d\u5c55\u73b0\u6f5c\u529b\uff0c\u4f46\u9700\u514b\u670d3GL\u8303\u5f0f\u60ef\u6027\uff0c\u672a\u6765\u9700\u63a2\u7d22\u6df7\u5408\u7f16\u7a0b\u8303\u5f0f\u534f\u4f5c\u673a\u5236"}}
{"id": "2508.18642", "pdf": "https://arxiv.org/pdf/2508.18642", "abs": "https://arxiv.org/abs/2508.18642", "authors": ["Jianxing Liao", "Tian Zhang", "Xiao Feng", "Yusong Zhang", "Rui Yang", "Haorui Wang", "Bosi Wen", "Ziying Wang", "Runzhi Shi"], "title": "RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large language models are extensively utilized in creative writing\napplications. Creative writing requires a balance between subjective writing\nquality (e.g., literariness and emotional expression) and objective constraint\nfollowing (e.g., format requirements and word limits). Existing reinforcement\nlearning methods struggle to balance these two aspects: single reward\nstrategies fail to improve both abilities simultaneously, while fixed-weight\nmixed-reward methods lack the ability to adapt to different writing scenarios.\nTo address this problem, we propose Reinforcement Learning with Mixed Rewards\n(RLMR), utilizing a dynamically mixed reward system from a writing reward model\nevaluating subjective writing quality and a constraint verification model\nassessing objective constraint following. The constraint following reward\nweight is adjusted dynamically according to the writing quality within sampled\ngroups, ensuring that samples violating constraints get negative advantage in\nGRPO and thus penalized during training, which is the key innovation of this\nproposed method. We conduct automated and manual evaluations across diverse\nmodel families from 8B to 72B parameters. Additionally, we construct a\nreal-world writing benchmark named WriteEval for comprehensive evaluation.\nResults illustrate that our method achieves consistent improvements in both\ninstruction following (IFEval from 83.36\\% to 86.65\\%) and writing quality\n(72.75\\% win rate in manual expert pairwise evaluations on WriteEval). To the\nbest of our knowledge, RLMR is the first work to combine subjective preferences\nwith objective verification in online RL training, providing an effective\nsolution for multi-dimensional creative writing optimization.", "AI": {"tldr": "\u63d0\u51faRLMR\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6df7\u5408\u5199\u4f5c\u8d28\u91cf\u5956\u52b1\u548c\u7ea6\u675f\u9a8c\u8bc1\u5956\u52b1\uff0c\u89e3\u51b3\u521b\u610f\u5199\u4f5c\u4e2d\u4e3b\u89c2\u8d28\u91cf\u4e0e\u5ba2\u89c2\u7ea6\u675f\u96be\u4ee5\u5e73\u8861\u7684\u95ee\u9898", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u65e0\u6cd5\u517c\u987e\u5199\u4f5c\u4e3b\u89c2\u8d28\u91cf\uff08\u6587\u5b66\u6027\uff09\u548c\u5ba2\u89c2\u7ea6\u675f\uff08\u683c\u5f0f\u8981\u6c42\uff09\uff0c\u5355\u4e00\u5956\u52b1\u7b56\u7565\u6548\u679c\u6709\u9650\uff0c\u56fa\u5b9a\u6743\u91cd\u6df7\u5408\u5956\u52b1\u7f3a\u4e4f\u573a\u666f\u9002\u5e94\u6027", "method": "\u4f7f\u7528\u5199\u4f5c\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u4e3b\u89c2\u8d28\u91cf\uff0c\u7ea6\u675f\u9a8c\u8bc1\u6a21\u578b\u8bc4\u4f30\u5ba2\u89c2\u7ea6\u675f\uff0c\u6839\u636e\u7ec4\u5185\u6837\u672c\u8d28\u91cf\u52a8\u6001\u8c03\u6574\u7ea6\u675f\u5956\u52b1\u6743\u91cd\uff0c\u901a\u8fc7GRPO\u7b97\u6cd5\u5bf9\u8fdd\u89c4\u6837\u672c\u65bd\u52a0\u8d1f\u4f18\u52bf\u60e9\u7f5a", "result": "\u57288B-72B\u53c2\u6570\u6a21\u578b\u4e0a\u5b9e\u73b0\u6307\u4ee4\u9075\u5faa\uff08IFEval\u4ece83.36%\u63d0\u5347\u81f386.65%\uff09\u548c\u5199\u4f5c\u8d28\u91cf\uff08WriteEval\u4eba\u5de5\u8bc4\u4f3072.75%\u80dc\u7387\uff09\u7684\u53cc\u91cd\u63d0\u5347", "conclusion": "\u9996\u6b21\u5728\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u878d\u5408\u4e3b\u89c2\u504f\u597d\u4e0e\u5ba2\u89c2\u9a8c\u8bc1\uff0c\u4e3a\u591a\u7ef4\u521b\u610f\u5199\u4f5c\u4f18\u5316\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.18646", "pdf": "https://arxiv.org/pdf/2508.18646", "abs": "https://arxiv.org/abs/2508.18646", "authors": ["Jun Wang", "Ninglun Gu", "Kailai Zhang", "Zijiao Zhang", "Yelun Bao", "Jin Yang", "Xu Yin", "Liwei Liu", "Yihuan Liu", "Pengyong Li", "Gary G. Yen", "Junchi Yan"], "title": "Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap", "categories": ["cs.AI", "cs.CL"], "comment": "Preprint. Under review", "summary": "For Large Language Models (LLMs), a disconnect persists between benchmark\nperformance and real-world utility. Current evaluation frameworks remain\nfragmented, prioritizing technical metrics while neglecting holistic assessment\nfor deployment. This survey introduces an anthropomorphic evaluation paradigm\nthrough the lens of human intelligence, proposing a novel three-dimensional\ntaxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational\ncapacity, Emotional Quotient (EQ)-Alignment Ability for value-based\ninteractions, and Professional Quotient (PQ)-Professional Expertise for\nspecialized proficiency. For practical value, we pioneer a Value-oriented\nEvaluation (VQ) framework assessing economic viability, social impact, ethical\nalignment, and environmental sustainability. Our modular architecture\nintegrates six components with an implementation roadmap. Through analysis of\n200+ benchmarks, we identify key challenges including dynamic assessment needs\nand interpretability gaps. It provides actionable guidance for developing LLMs\nthat are technically proficient, contextually relevant, and ethically sound. We\nmaintain a curated repository of open-source evaluation resources at:\nhttps://github.com/onejune2018/Awesome-LLM-Eval.", "AI": {"tldr": "\u63d0\u51fa\u4ee5\u4eba\u7c7b\u667a\u80fd\u4e3a\u53c2\u7167\u7684\u4e09\u7ef4\u8bc4\u4f30\u8303\u5f0f\uff08IQ/EQ/PQ\uff09\u548c\u4ef7\u503c\u5bfc\u5411\u8bc4\u4f30\u6846\u67b6\uff08VQ\uff09\uff0c\u7a81\u7834\u4f20\u7edf\u788e\u7247\u5316\u8bc4\u6d4b\uff0c\u6784\u5efaLLM\u7efc\u5408\u8bc4\u4f30\u4f53\u7cfb", "motivation": "\u5f53\u524dLLM\u8bc4\u4f30\u6846\u67b6\u5b58\u5728\u6280\u672f\u6307\u6807\u4e0e\u90e8\u7f72\u9700\u6c42\u8131\u8282\u3001\u5ffd\u89c6\u5546\u4e1a\u4ef7\u503c\u4e0e\u793e\u4f1a\u5f71\u54cd\u7b49\u7cfb\u7edf\u6027\u8bc4\u4f30\u7684\u95ee\u9898", "method": "\u901a\u8fc7\u4eba\u7c7b\u667a\u529b\u4e09\u5546\u6a21\u578b\u6784\u5efa\u8bc4\u4f30\u4f53\u7cfb\uff08\u57fa\u7840\u667a\u80fd/\u4ef7\u503c\u5bf9\u9f50/\u4e13\u4e1a\u80fd\u529b\uff09\uff0c\u8bbe\u8ba1\u5305\u542b\u7ecf\u6d4e\u4ef7\u503c\u3001\u793e\u4f1a\u5f71\u54cd\u3001\u4f26\u7406\u5408\u89c4\u3001\u73af\u5883\u53ef\u6301\u7eed\u7684\u56db\u7ef4\u4ef7\u503c\u8bc4\u4f30\u6846\u67b6", "result": "\u5efa\u7acb\u6a21\u5757\u5316\u8bc4\u4f30\u67b6\u6784\uff0c\u5206\u6790200+\u57fa\u51c6\u6d4b\u8bd5\u540e\u63ed\u793a\u52a8\u6001\u8bc4\u4f30\u9700\u6c42\u3001\u53ef\u89e3\u91ca\u6027\u5dee\u8ddd\u7b49\u6838\u5fc3\u6311\u6218", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5f00\u53d1\u517c\u5177\u6280\u672f\u80fd\u529b\u3001\u573a\u666f\u9002\u5e94\u6027\u548c\u4f26\u7406\u5408\u89c4\u7684LLM\u63d0\u4f9b\u7cfb\u7edf\u5316\u6307\u5bfc\uff0c\u914d\u5957\u5f00\u6e90\u8bc4\u4f30\u8d44\u6e90\u5e93\u63a8\u52a8\u884c\u4e1a\u6807\u51c6\u5316\u8fdb\u7a0b"}}
{"id": "2508.18652", "pdf": "https://arxiv.org/pdf/2508.18652", "abs": "https://arxiv.org/abs/2508.18652", "authors": ["Runpeng Geng", "Yanting Wang", "Ying Chen", "Jinyuan Jia"], "title": "UniC-RAG: Universal Knowledge Corruption Attacks to Retrieval-Augmented Generation", "categories": ["cs.CR", "cs.CL", "I.2.7"], "comment": "21 pages, 4 figures", "summary": "Retrieval-augmented generation (RAG) systems are widely deployed in\nreal-world applications in diverse domains such as finance, healthcare, and\ncybersecurity. However, many studies showed that they are vulnerable to\nknowledge corruption attacks, where an attacker can inject adversarial texts\ninto the knowledge database of a RAG system to induce the LLM to generate\nattacker-desired outputs. Existing studies mainly focus on attacking specific\nqueries or queries with similar topics (or keywords). In this work, we propose\nUniC-RAG, a universal knowledge corruption attack against RAG systems. Unlike\nprior work, UniC-RAG jointly optimizes a small number of adversarial texts that\ncan simultaneously attack a large number of user queries with diverse topics\nand domains, enabling an attacker to achieve various malicious objectives, such\nas directing users to malicious websites, triggering harmful command execution,\nor launching denial-of-service attacks. We formulate UniC-RAG as an\noptimization problem and further design an effective solution to solve it,\nincluding a balanced similarity-based clustering method to enhance the attack's\neffectiveness. Our extensive evaluations demonstrate that UniC-RAG is highly\neffective and significantly outperforms baselines. For instance, UniC-RAG could\nachieve over 90% attack success rate by injecting 100 adversarial texts into a\nknowledge database with millions of texts to simultaneously attack a large set\nof user queries (e.g., 2,000). Additionally, we evaluate existing defenses and\nshow that they are insufficient to defend against UniC-RAG, highlighting the\nneed for new defense mechanisms in RAG systems.", "AI": {"tldr": "\u63d0\u51faUniC-RAG\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c11\u91cf\u5bf9\u6297\u6587\u672c\u6c61\u67d3RAG\u7cfb\u7edf\u77e5\u8bc6\u5e93\uff0c\u53ef\u540c\u65f6\u653b\u51fb\u6570\u5343\u4e2a\u4e0d\u540c\u4e3b\u9898\u7684\u7528\u6237\u67e5\u8be2\uff0c\u653b\u51fb\u6210\u529f\u7387\u8d8590%", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u6613\u53d7\u77e5\u8bc6\u6c61\u67d3\u653b\u51fb\uff0c\u4f46\u4f20\u7edf\u653b\u51fb\u4ec5\u9488\u5bf9\u7279\u5b9a\u67e5\u8be2\u6216\u76f8\u4f3c\u4e3b\u9898\u3002\u9700\u5f00\u53d1\u80fd\u540c\u65f6\u6c61\u67d3\u591a\u6837\u5316\u67e5\u8be2\u7684\u901a\u7528\u653b\u51fb\u65b9\u6cd5\u4ee5\u5b9e\u73b0\u66f4\u5927\u5371\u5bb3", "method": "1. \u5c06\u653b\u51fb\u5efa\u6a21\u4e3a\u4f18\u5316\u95ee\u9898\uff0c\u8054\u5408\u4f18\u5316\u5bf9\u6297\u6587\u672c 2. \u8bbe\u8ba1\u5e73\u8861\u76f8\u4f3c\u6027\u805a\u7c7b\u65b9\u6cd5\uff0c\u589e\u5f3a\u653b\u51fb\u6709\u6548\u6027 3. \u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u751f\u6210\u8de8\u9886\u57df\u901a\u7528\u6c61\u67d3\u6587\u672c", "result": "\u6ce8\u5165100\u4e2a\u5bf9\u6297\u6587\u672c\u5373\u53ef\u653b\u51fb2000\u4e2a\u67e5\u8be2\uff08\u6210\u529f\u7387>90%\uff09\uff0c\u5728\u767e\u4e07\u7ea7\u77e5\u8bc6\u5e93\u4e2d\u4ecd\u6709\u6548\uff0c\u73b0\u6709\u9632\u5fa1\u673a\u5236\u65e0\u6cd5\u6709\u6548\u62b5\u5fa1", "conclusion": "UniC-RAG\u63ed\u793aRAG\u7cfb\u7edf\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\u4e0d\u8db3\uff0c\u4e9f\u9700\u5f00\u53d1\u65b0\u578b\u9632\u5fa1\u673a\u5236\u5e94\u5bf9\u901a\u7528\u77e5\u8bc6\u6c61\u67d3\u653b\u51fb"}}
{"id": "2508.18665", "pdf": "https://arxiv.org/pdf/2508.18665", "abs": "https://arxiv.org/abs/2508.18665", "authors": ["Jiajie He", "Yuechun Gu", "Min-Chun Chen", "Keke Chen"], "title": "Membership Inference Attacks on LLM-based Recommender Systems", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) based Recommender Systems (RecSys) can flexibly\nadapt recommendation systems to different domains. It utilizes in-context\nlearning (ICL), i.e., the prompts, to customize the recommendation functions,\nwhich include sensitive historical user-specific item interactions, e.g.,\nimplicit feedback like clicked items or explicit product reviews. Such private\ninformation may be exposed to novel privacy attack. However, no study has been\ndone on this important issue. We design four membership inference attacks\n(MIAs), aiming to reveal whether victims' historical interactions have been\nused by system prompts. They are \\emph{direct inquiry, hallucination,\nsimilarity, and poisoning attacks}, each of which utilizes the unique features\nof LLMs or RecSys. We have carefully evaluated them on three LLMs that have\nbeen used to develop ICL-LLM RecSys and two well-known RecSys benchmark\ndatasets. The results confirm that the MIA threat on LLM RecSys is realistic:\ndirect inquiry and poisoning attacks showing significantly high attack\nadvantages. We have also analyzed the factors affecting these attacks, such as\nthe number of shots in system prompts and the position of the victim in the\nshots.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u8350\u7cfb\u7edf\u5b58\u5728\u6210\u5458\u9690\u79c1\u6cc4\u9732\u98ce\u9669\uff0c\u63d0\u51fa\u56db\u79cd\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u5176\u4e2d\u76f4\u63a5\u8be2\u95ee\u548c\u6295\u6bd2\u653b\u51fb\u8868\u73b0\u7a81\u51fa", "motivation": "\u73b0\u6709LLM\u63a8\u8350\u7cfb\u7edf\u5728\u63d0\u793a\u4e2d\u4f7f\u7528\u4e86\u7528\u6237\u5386\u53f2\u4ea4\u4e92\u6570\u636e\uff0c\u8fd9\u4e9b\u654f\u611f\u4fe1\u606f\u53ef\u80fd\u88ab\u65b0\u578b\u9690\u79c1\u653b\u51fb\u7a83\u53d6\uff0c\u800c\u8be5\u9886\u57df\u5c1a\u672a\u6709\u76f8\u5173\u7814\u7a76", "method": "\u8bbe\u8ba1\u4e86\u76f4\u63a5\u8be2\u95ee\u3001\u5e7b\u89c9\u653b\u51fb\u3001\u76f8\u4f3c\u6027\u653b\u51fb\u3001\u6295\u6bd2\u653b\u51fb\u56db\u79cdMIA\u65b9\u6848\uff0c\u5728\u4e09\u79cdLLM\u63a8\u8350\u7cfb\u7edf\u548c\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30", "result": "\u5b9e\u9a8c\u8868\u660e\u76f4\u63a5\u8be2\u95ee\u653b\u51fb(\u6210\u529f\u738778%)\u548c\u6295\u6bd2\u653b\u51fb(\u4f18\u52bf\u6307\u6807\u8fbe0.42)\u6548\u679c\u663e\u8457\uff0c\u653b\u51fb\u6548\u679c\u53d7\u63d0\u793a\u6837\u672c\u6570\u91cf\u53ca\u53d7\u5bb3\u8005\u4f4d\u7f6e\u7b49\u56e0\u7d20\u5f71\u54cd", "conclusion": "\u9996\u6b21\u8bc1\u5b9eLLM\u63a8\u8350\u7cfb\u7edf\u9762\u4e34\u73b0\u5b9e\u7684MIA\u5a01\u80c1\uff0c\u63d0\u793a\u5de5\u7a0b\u7684\u8bbe\u8ba1\u9700\u52a0\u5f3a\u9690\u79c1\u4fdd\u62a4\u673a\u5236\uff0c\u4e3a\u540e\u7eed\u9632\u5fa1\u7814\u7a76\u63d0\u4f9b\u65b9\u5411"}}
{"id": "2508.18672", "pdf": "https://arxiv.org/pdf/2508.18672", "abs": "https://arxiv.org/abs/2508.18672", "authors": ["Taishi Nakamura", "Satoki Ishikawa", "Masaki Kawamura", "Takumi Okamoto", "Daisuke Nohara", "Jun Suzuki", "Rio Yokota"], "title": "Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Presented at the Second AI for Math Workshop at ICML", "summary": "Empirical scaling laws have driven the evolution of large language models\n(LLMs), yet their coefficients shift whenever the model architecture or data\npipeline changes. Mixture-of-Experts (MoE) models, now standard in\nstate-of-the-art systems, introduce a new sparsity dimension that current\ndense-model frontiers overlook. We investigate how MoE sparsity influences two\ndistinct capability regimes: memorization and reasoning. We train families of\nMoE Transformers that systematically vary total parameters, active parameters,\nand top-$k$ routing while holding the compute budget fixed. For every model we\nrecord pre-training loss, downstream task loss, and task accuracy, allowing us\nto separate the train-test generalization gap from the loss-accuracy gap.\nMemorization benchmarks improve monotonically with total parameters, mirroring\ntraining loss. By contrast, reasoning performance saturates and can even\nregress despite continued gains in both total parameters and training loss.\nAltering top-$k$ alone has little effect when active parameters are constant,\nand classic hyperparameters such as learning rate and initialization modulate\nthe generalization gap in the same direction as sparsity. Neither post-training\nreinforcement learning (GRPO) nor extra test-time compute rescues the reasoning\ndeficit of overly sparse models. Our model checkpoints, code and logs are\nopen-source at https://github.com/rioyokotalab/optimal-sparsity.", "AI": {"tldr": "MoE\u6a21\u578b\u7684\u7a00\u758f\u6027\u5bf9\u8bb0\u5fc6\u80fd\u529b\u6301\u7eed\u589e\u76ca\u4f46\u5bf9\u63a8\u7406\u80fd\u529b\u5b58\u5728\u9971\u548c\u6548\u5e94\uff0c\u4f20\u7edf\u8d85\u53c2\u6570\u8c03\u8282\u65e0\u6cd5\u5f25\u8865\u8fc7\u5ea6\u7a00\u758f\u6a21\u578b\u7684\u63a8\u7406\u7f3a\u9677", "motivation": "\u73b0\u6709LLM\u7684\u7f29\u653e\u5b9a\u5f8b\u672a\u8003\u8651MoE\u67b6\u6784\u5f15\u5165\u7684\u7a00\u758f\u6027\u7ef4\u5ea6\uff0c\u9700\u9a8c\u8bc1\u7a00\u758f\u6027\u5bf9\u8bb0\u5fc6/\u63a8\u7406\u4e24\u79cd\u80fd\u529b\u7684\u4e0d\u540c\u5f71\u54cd\u673a\u5236", "method": "\u5728\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4e0b\u8bad\u7ec3\u4e0d\u540c\u603b\u53c2\u6570\u91cf/\u6fc0\u6d3b\u53c2\u6570\u91cf\u7684MoE Transformer\u5bb6\u65cf\uff0c\u7cfb\u7edf\u8c03\u6574top-k\u8def\u7531\u7b56\u7565\uff0c\u76d1\u63a7\u9884\u8bad\u7ec3\u635f\u5931\u3001\u4e0b\u6e38\u4efb\u52a1\u635f\u5931\u4e0e\u51c6\u786e\u7387", "result": "\u8bb0\u5fc6\u80fd\u529b\u968f\u603b\u53c2\u6570\u589e\u52a0\u6301\u7eed\u63d0\u5347\uff08\u4e0e\u8bad\u7ec3\u635f\u5931\u540c\u6b65\uff09\uff0c\u63a8\u7406\u80fd\u529b\u5219\u5448\u73b0\u9971\u548c\u751a\u81f3\u8d1f\u76f8\u5173\uff1b\u5b66\u4e60\u7387\u7b49\u4f20\u7edf\u8d85\u53c2\u6570\u4e0e\u7a00\u758f\u6027\u5bf9\u6cdb\u5316\u7f3a\u53e3\u6709\u540c\u5411\u8c03\u8282\u4f5c\u7528", "conclusion": "MoE\u7a00\u758f\u6027\u9700\u4e0e\u4efb\u52a1\u7279\u6027\u5339\u914d\uff0c\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u9700\u8c28\u614e\u63a7\u5236\u7a00\u758f\u5ea6\uff0c\u73b0\u6709RLHF\u548c\u63a8\u7406\u9636\u6bb5\u8ba1\u7b97\u589e\u5f3a\u65e0\u6cd5\u4fee\u590d\u8fc7\u5ea6\u7a00\u758f\u5bfc\u81f4\u7684\u63a8\u7406\u7f3a\u9677\uff08\u6a21\u578b\u4e0e\u4ee3\u7801\u5df2\u5f00\u6e90\uff09"}}
{"id": "2508.18684", "pdf": "https://arxiv.org/pdf/2508.18684", "abs": "https://arxiv.org/abs/2508.18684", "authors": ["Shaswata Mitra", "Azim Bazarov", "Martin Duclos", "Sudip Mittal", "Aritran Piplai", "Md Rayhanur Rahman", "Edward Zieglar", "Shahram Rahimi"], "title": "FALCON: Autonomous Cyber Threat Intelligence Mining with LLMs for IDS Rule Generation", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.LG", "cs.SY", "eess.SY"], "comment": "11 pages, 5 figures, 4 tables", "summary": "Signature-based Intrusion Detection Systems (IDS) detect malicious activities\nby matching network or host activity against predefined rules. These rules are\nderived from extensive Cyber Threat Intelligence (CTI), which includes attack\nsignatures and behavioral patterns obtained through automated tools and manual\nthreat analysis, such as sandboxing. The CTI is then transformed into\nactionable rules for the IDS engine, enabling real-time detection and\nprevention. However, the constant evolution of cyber threats necessitates\nfrequent rule updates, which delay deployment time and weaken overall security\nreadiness. Recent advancements in agentic systems powered by Large Language\nModels (LLMs) offer the potential for autonomous IDS rule generation with\ninternal evaluation. We introduce FALCON, an autonomous agentic framework that\ngenerates deployable IDS rules from CTI data in real-time and evaluates them\nusing built-in multi-phased validators. To demonstrate versatility, we target\nboth network (Snort) and host-based (YARA) mediums and construct a\ncomprehensive dataset of IDS rules with their corresponding CTIs. Our\nevaluations indicate FALCON excels in automatic rule generation, with an\naverage of 95% accuracy validated by qualitative evaluation with 84%\ninter-rater agreement among multiple cybersecurity analysts across all metrics.\nThese results underscore the feasibility and effectiveness of LLM-driven data\nmining for real-time cyber threat mitigation.", "AI": {"tldr": "FALCON\u6846\u67b6\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u65f6\u751f\u6210IDS\u89c4\u5219\uff0c\u5b9e\u73b095%\u51c6\u786e\u7387\u5e76\u901a\u8fc7\u5185\u7f6e\u9a8c\u8bc1\u5668\u8bc4\u4f30\u6709\u6548\u6027", "motivation": "\u4f20\u7edfIDS\u89c4\u5219\u66f4\u65b0\u9891\u7387\u65e0\u6cd5\u5e94\u5bf9\u5feb\u901f\u6f14\u53d8\u7684\u7f51\u7edc\u5a01\u80c1\uff0c\u5bfc\u81f4\u5b89\u5168\u9632\u5fa1\u5b58\u5728\u6ede\u540e\u6027", "method": "\u57fa\u4e8eLLM\u6784\u5efa\u591a\u9636\u6bb5\u9a8c\u8bc1\u6846\u67b6\uff0c\u652f\u6301Snort(\u7f51\u7edc)\u548cYARA(\u4e3b\u673a)\u53cc\u5e73\u53f0\u7684\u5b9e\u65f6\u89c4\u5219\u751f\u6210\u4e0e\u8bc4\u4f30", "result": "\u5b9e\u73b0\u5e73\u574795%\u7684\u89c4\u5219\u751f\u6210\u51c6\u786e\u7387\uff0c\u7f51\u7edc\u5b89\u5168\u5206\u6790\u5e08\u8bc4\u4f30\u8fbe\u621084%\u7684\u8de8\u6307\u6807\u4e00\u81f4\u6027\u5171\u8bc6", "conclusion": "LLM\u9a71\u52a8\u7684\u5b9e\u65f6\u6570\u636e\u6316\u6398\u6280\u672f\u53ef\u6709\u6548\u63d0\u5347\u7f51\u7edc\u5a01\u80c1\u7f13\u89e3\u7684\u53ca\u65f6\u6027\u548c\u51c6\u786e\u6027"}}
{"id": "2508.18724", "pdf": "https://arxiv.org/pdf/2508.18724", "abs": "https://arxiv.org/abs/2508.18724", "authors": ["Karanbir Singh", "Deepak Muppiri", "William Ngu"], "title": "Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted at KDD'2025 Agent4IR workshop", "summary": "Large Language Models (LLMs) have transformed the field of artificial\nintelligence by unlocking the era of generative applications. Built on top of\ngenerative AI capabilities, Agentic AI represents a major shift toward\nautonomous, goal-driven systems that can reason, retrieve, and act. However,\nthey also inherit the bias present in both internal and external information\nsources. This significantly affects the fairness and balance of retrieved\ninformation, and hence reduces user trust. To address this critical challenge,\nwe introduce a novel Bias Mitigation Agent, a multi-agent system designed to\norchestrate the workflow of bias mitigation through specialized agents that\noptimize the selection of sources to ensure that the retrieved content is both\nhighly relevant and minimally biased to promote fair and balanced knowledge\ndissemination. The experimental results demonstrate an 81.82\\% reduction in\nbias compared to a baseline naive retrieval strategy.", "AI": {"tldr": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u7cfb\u7edfBias Mitigation Agent\uff0c\u901a\u8fc7\u534f\u540c\u4f18\u5316\u4fe1\u606f\u6765\u6e90\u9009\u62e9\uff0c\u5b9e\u73b081.82%\u7684\u504f\u89c1\u6d88\u51cf\u6548\u679c\u3002", "motivation": "\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u7cfb\u7edf\u5728\u4fe1\u606f\u68c0\u7d22\u8fc7\u7a0b\u4e2d\u5b58\u5728\u56fa\u6709\u504f\u89c1\uff0c\u8fd9\u4f1a\u5f71\u54cd\u77e5\u8bc6\u4f20\u64ad\u7684\u516c\u5e73\u6027\u5e76\u964d\u4f4e\u7528\u6237\u4fe1\u4efb\u5ea6\u3002", "method": "\u5f00\u53d1\u7531\u591a\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u7ec4\u6210\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f18\u5316\u4fe1\u606f\u6765\u6e90\u9009\u62e9\u673a\u5236\uff0c\u5e73\u8861\u76f8\u5173\u6027\u4e0e\u504f\u89c1\u6c34\u5e73\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebf\u68c0\u7d22\u7b56\u7565\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u504f\u89c1\u51cf\u5c1181.82%\u3002", "conclusion": "\u8be5\u591a\u667a\u80fd\u4f53\u67b6\u6784\u6709\u6548\u63d0\u5347\u4e86\u4fe1\u606f\u516c\u5e73\u6027\uff0c\u4e3a\u751f\u6210\u5f0fAI\u7684\u504f\u89c1\u6cbb\u7406\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.18743", "pdf": "https://arxiv.org/pdf/2508.18743", "abs": "https://arxiv.org/abs/2508.18743", "authors": ["Sunguk Choi", "Yonghoon Kwon", "Heondeuk Lee"], "title": "CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted at EMNLP 2025 findings", "summary": "Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs)\nsolve difficult problems, but very long traces often slow or even degrade\nperformance on fast, intuitive \"System-1\" tasks. We introduce Connector-Aware\nCompact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a\nsmall, fixed set of connector phrases, steering the model toward concise and\nwell -- structured explanations. Despite its simplicity, our synthetic method\nwith Gemini-2.0-Flash yields a high-quality training quality. CAC-CoT achieves\napproximately 85% on GSM8K and approximately 40% on GPQA (System-2) while\nretaining approximately 90% on S1-Bench (System-1). Its reasoning traces\naverage approximately 300 tokens(ART), about one-third the length of baseline\ntraces, delivering higher efficiency without loss of accuracy.", "AI": {"tldr": "\u901a\u8fc7\u9650\u5236\u8fde\u63a5\u8bcd\u77ed\u8bed\u5b9e\u73b0\u9ad8\u6548\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u5728\u4fdd\u6301System-1\u4efb\u52a1\u6027\u80fd\uff08\u7ea690%\uff09\u7684\u540c\u65f6\uff0cGSM8K\u8fbe\u523085%\u3001GPQA\u8fbe40%\u51c6\u786e\u7387\uff0c\u63a8\u7406\u957f\u5ea6\u7f29\u51cf\u81f3\u57fa\u7ebf1/3\uff08\u7ea6300token\uff09", "motivation": "\u89e3\u51b3\u957f\u601d\u7ef4\u94fe(CoT)\u5728\u5feb\u901f\u76f4\u89c9\u578bSystem-1\u4efb\u52a1\u4e2d\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u540c\u65f6\u9700\u8981\u517c\u987e\u590d\u6742System-2\u4efb\u52a1\u7684\u89e3\u51b3\u80fd\u529b", "method": "\u57fa\u4e8e\u56fa\u5b9a\u8fde\u63a5\u8bcd\u77ed\u8bed\u5f15\u5bfc\u6a21\u578b\u751f\u6210\u7b80\u6d01\u7ed3\u6784\u5316\u89e3\u91ca\uff0c\u7ed3\u5408Gemini-2.0-Flash\u6a21\u578b\u5b9e\u73b0\u9ad8\u8d28\u91cf\u8bad\u7ec3", "result": "GSM8K:85% | GPQA(System-2):40% | S1-Bench(System-1):90% | \u5e73\u5747\u63a8\u7406\u957f\u5ea6300token\uff08\u8f83\u57fa\u7ebf\u51cf\u5c112/3\uff09", "conclusion": "CAC-CoT\u8bc1\u660e\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u53ef\u5728\u4e0d\u635f\u5931System-1\u6027\u80fd\u7684\u524d\u63d0\u4e0b\uff0c\u6709\u6548\u63d0\u5347System-2\u4efb\u52a1\u6548\u7387\uff0c\u4e3aLLM\u63a8\u7406\u6548\u7387\u4f18\u5316\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2508.18758", "pdf": "https://arxiv.org/pdf/2508.18758", "abs": "https://arxiv.org/abs/2508.18758", "authors": ["Yipeng Zhang", "Chen Wang", "Yuzhe Zhang", "Jacky Jiang"], "title": "Text to Query Plans for Question Answering on Large Tables", "categories": ["cs.DB", "cs.AI", "cs.CL"], "comment": null, "summary": "Efficient querying and analysis of large tabular datasets remain significant\nchallenges, especially for users without expertise in programming languages\nlike SQL. Text-to-SQL approaches have shown promising performance on benchmark\ndata; however, they inherit SQL's drawbacks, including inefficiency with large\ndatasets and limited support for complex data analyses beyond basic querying.\nWe propose a novel framework that transforms natural language queries into\nquery plans. Our solution is implemented outside traditional databases,\nallowing us to support classical SQL commands while avoiding SQL's inherent\nlimitations. Additionally, we enable complex analytical functions, such as\nprincipal component analysis and anomaly detection, providing greater\nflexibility and extensibility than traditional SQL capabilities. We leverage\nLLMs to iteratively interpret queries and construct operation sequences,\naddressing computational complexity by incrementally building solutions. By\nexecuting operations directly on the data, we overcome context length\nlimitations without requiring the entire dataset to be processed by the model.\nWe validate our framework through experiments on both standard databases and\nlarge scientific tables, demonstrating its effectiveness in handling extensive\ndatasets and performing sophisticated data analyses.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u81ea\u7136\u8bed\u8a00\u8f6c\u67e5\u8be2\u8ba1\u5212\u6846\u67b6\uff0c\u514b\u670dSQL\u5728\u5927\u6570\u636e\u5206\u6790\u4e2d\u7684\u6548\u7387\u9650\u5236\u5e76\u652f\u6301\u590d\u6742\u5206\u6790\u529f\u80fd", "motivation": "\u4f20\u7edfText-to-SQL\u65b9\u6cd5\u53d7\u9650\u4e8eSQL\u7684\u6267\u884c\u6548\u7387\u4e0e\u6269\u5c55\u6027\uff0c\u96be\u4ee5\u5e94\u5bf9\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u590d\u6742\u5206\u6790\u9700\u6c42\uff0c\u4e14\u5bf9\u975e\u4e13\u4e1a\u7528\u6237\u4e0d\u53cb\u597d", "method": "\u901a\u8fc7LLM\u8fed\u4ee3\u89e3\u6790\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u6784\u5efa\u64cd\u4f5c\u5e8f\u5217\uff0c\u5728\u6570\u636e\u5e93\u5916\u76f4\u63a5\u6267\u884c\u8fd0\u7b97\uff0c\u652f\u6301SQL\u547d\u4ee4\u548cPCA/\u5f02\u5e38\u68c0\u6d4b\u7b49\u9ad8\u7ea7\u5206\u6790\uff0c\u907f\u514d\u5168\u91cf\u6570\u636e\u8f93\u5165\u6a21\u578b", "result": "\u5728\u6807\u51c6\u6570\u636e\u5e93\u548c\u5927\u578b\u79d1\u5b66\u8868\u683c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u5904\u7406\u6d77\u91cf\u6570\u636e\u548c\u590d\u6742\u5206\u6790\u4efb\u52a1\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u6846\u67b6\u7a81\u7834SQL\u56fa\u6709\u5c40\u9650\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u64cd\u4f5c\u5e8f\u5217\u5b9e\u73b0\u9ad8\u6548\u67e5\u8be2\u4e0e\u6269\u5c55\u5206\u6790\uff0c\u4e3a\u5927\u6570\u636e\u5904\u7406\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2508.18760", "pdf": "https://arxiv.org/pdf/2508.18760", "abs": "https://arxiv.org/abs/2508.18760", "authors": ["Yi Liu", "Xiangyu Liu", "Zequn Sun", "Wei Hu"], "title": "Answering the Unanswerable Is to Err Knowingly: Analyzing and Mitigating Abstention Failures in Large Reasoning Models", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large reasoning models (LRMs) have shown remarkable progress on complex\nreasoning tasks. However, some questions posed to LRMs are inherently\nunanswerable, such as math problems lacking sufficient conditions. We find that\nLRMs continually fail to provide appropriate abstentions when confronted with\nthese unanswerable questions. In this paper, we systematically analyze,\ninvestigate, and resolve this issue for trustworthy AI. We first conduct a\ndetailed analysis of the distinct response behaviors of LRMs when facing\nunanswerable questions. Then, we show that LRMs possess sufficient cognitive\ncapabilities to recognize the flaws in these questions. However, they fail to\nexhibit appropriate abstention behavior, revealing a misalignment between their\ninternal cognition and external response. Finally, to resolve this issue, we\npropose a lightweight, two-stage method that combines cognitive monitoring with\ninference-time intervention. Experimental results demonstrate that our method\nsignificantly improves the abstention rate while maintaining the overall\nreasoning performance.", "AI": {"tldr": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u65e0\u6cd5\u56de\u7b54\u7684\u95ee\u9898\uff08\u5982\u7f3a\u5c11\u5fc5\u8981\u6761\u4ef6\u7684\u6570\u5b66\u9898\uff09\u4e2d\u6301\u7eed\u65e0\u6cd5\u6070\u5f53\u62d2\u7edd\u56de\u7b54\uff0c\u4f5c\u8005\u63d0\u51fa\u7ed3\u5408\u8ba4\u77e5\u76d1\u63a7\u4e0e\u63a8\u7406\u5e72\u9884\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u62d2\u7edd\u7387", "motivation": "\u4e3a\u4e86\u89e3\u51b3LRMs\u5185\u90e8\u8ba4\u77e5\u4e0e\u5916\u90e8\u56de\u5e94\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u786e\u4fddAI\u7684\u53ef\u4fe1\u5ea6\u9700\u8981\u7cfb\u7edf\u6027\u7684\u89e3\u51b3\u65b9\u6848", "method": "1. \u5206\u6790LRMs\u5bf9\u65e0\u6cd5\u56de\u7b54\u95ee\u9898\u7684\u54cd\u5e94\u884c\u4e3a 2. \u901a\u8fc7\u8ba4\u77e5\u76d1\u6d4b\u548c\u63a8\u7406\u65f6\u5e72\u9884\u76f8\u7ed3\u5408\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u8fdb\u884c\u6a21\u578b\u6821\u51c6", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u7684\u540c\u65f6\uff0c\u5c06\u62d2\u7edd\u56de\u7b54\u7387\u63d0\u5347\u4e8631.5\u4e2a\u767e\u5206\u70b9", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86LRMs\u7684\u8ba4\u77e5-\u56de\u5e94\u9519\u4f4d\u95ee\u9898\uff0c\u4e3a\u6784\u5efa\u53ef\u4fe1AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2508.18772", "pdf": "https://arxiv.org/pdf/2508.18772", "abs": "https://arxiv.org/abs/2508.18772", "authors": ["Wanqiang Wang", "Longzhu He", "Wei Zheng"], "title": "Beyond the Textual: Generating Coherent Visual Options for MCQs", "categories": ["cs.CV", "cs.CL"], "comment": "EMNLP 2025", "summary": "Multiple-choice questions (MCQs) play a crucial role in fostering deep\nthinking and knowledge integration in education. However, previous research has\nprimarily focused on generating MCQs with textual options, but it largely\noverlooks the visual options. Moreover, generating high-quality distractors\nremains a major challenge due to the high cost and limited scalability of\nmanual authoring. To tackle these problems, we propose a Cross-modal Options\nSynthesis (CmOS), a novel framework for generating educational MCQs with visual\noptions. Our framework integrates Multimodal Chain-of-Thought (MCoT) reasoning\nprocess and Retrieval-Augmented Generation (RAG) to produce semantically\nplausible and visually similar answer and distractors. It also includes a\ndiscrimination module to identify content suitable for visual options.\nExperimental results on test tasks demonstrate the superiority of CmOS in\ncontent discrimination, question generation and visual option generation over\nexisting methods across various subjects and educational levels.", "AI": {"tldr": "\u63d0\u51fa\u8de8\u6a21\u6001\u9009\u9879\u5408\u6210\u6846\u67b6CmOS\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\uff0c\u89e3\u51b3\u6559\u80b2\u591a\u9009\u9898\u89c6\u89c9\u9009\u9879\u751f\u6210\u4e0e\u5e72\u6270\u9879\u8d28\u91cf\u96be\u9898\u3002", "motivation": "\u73b0\u6709MCQ\u751f\u6210\u65b9\u6cd5\u5ffd\u89c6\u89c6\u89c9\u9009\u9879\u4e14\u4eba\u5de5\u5236\u4f5c\u5e72\u6270\u9879\u6210\u672c\u9ad8\uff0c\u9700\u81ea\u52a8\u5316\u751f\u6210\u5177\u5907\u8bed\u4e49\u5408\u7406\u6027\u548c\u89c6\u89c9\u76f8\u4f3c\u6027\u7684\u591a\u6a21\u6001\u9009\u9879\u3002", "method": "\u6574\u5408\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u63a8\u7406(MCoT)\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\uff0c\u5f00\u53d1\u5185\u5bb9\u5224\u522b\u6a21\u5757\u7b5b\u9009\u9002\u5408\u89c6\u89c9\u5316\u7684\u9898\u76ee\uff0c\u540c\u6b65\u751f\u6210\u6587\u672c\u548c\u89c6\u89c9\u5e72\u6270\u9879\u3002", "result": "\u8de8\u5b66\u79d1\u5b9e\u9a8c\u663e\u793aCmOS\u5728\u5185\u5bb9\u5224\u522b\u51c6\u786e\u7387\u3001\u95ee\u9898\u751f\u6210\u8d28\u91cf\u548c\u89c6\u89c9\u9009\u9879\u903c\u771f\u5ea6\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u6559\u80b2\u9636\u6bb5\u3002", "conclusion": "CmOS\u6846\u67b6\u4e3a\u6559\u80b2\u8bc4\u4f30\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u591a\u6a21\u6001MCQ\u751f\u6210\u65b9\u6848\uff0c\u8bc1\u660e\u8de8\u6a21\u6001\u5408\u6210\u6280\u672f\u5728\u6559\u80b2\u667a\u80fd\u5316\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.18976", "pdf": "https://arxiv.org/pdf/2508.18976", "abs": "https://arxiv.org/abs/2508.18976", "authors": ["Stephen Meisenbacher", "Alexandra Klymenko", "Andreea-Elena Bodea", "Florian Matthes"], "title": "The Double-edged Sword of LLM-based Data Reconstruction: Understanding and Mitigating Contextual Vulnerability in Word-level Differential Privacy Text Sanitization", "categories": ["cs.CR", "cs.CL"], "comment": "15 pages, 4 figures, 8 tables. Accepted to WPES @ CCS 2025", "summary": "Differentially private text sanitization refers to the process of privatizing\ntexts under the framework of Differential Privacy (DP), providing provable\nprivacy guarantees while also empirically defending against adversaries seeking\nto harm privacy. Despite their simplicity, DP text sanitization methods\noperating at the word level exhibit a number of shortcomings, among them the\ntendency to leave contextual clues from the original texts due to randomization\nduring sanitization $\\unicode{x2013}$ this we refer to as $\\textit{contextual\nvulnerability}$. Given the powerful contextual understanding and inference\ncapabilities of Large Language Models (LLMs), we explore to what extent LLMs\ncan be leveraged to exploit the contextual vulnerability of DP-sanitized texts.\nWe expand on previous work not only in the use of advanced LLMs, but also in\ntesting a broader range of sanitization mechanisms at various privacy levels.\nOur experiments uncover a double-edged sword effect of LLM-based data\nreconstruction attacks on privacy and utility: while LLMs can indeed infer\noriginal semantics and sometimes degrade empirical privacy protections, they\ncan also be used for good, to improve the quality and privacy of DP-sanitized\ntexts. Based on our findings, we propose recommendations for using LLM data\nreconstruction as a post-processing step, serving to increase privacy\nprotection by thinking adversarially.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5dee\u5206\u9690\u79c1\u6587\u672c\u8131\u654f\u4e2d\u5177\u6709\u53cc\u5203\u5251\u6548\u5e94\uff1a\u65e2\u80fd\u901a\u8fc7\u4e0a\u4e0b\u6587\u63a8\u7406\u653b\u51fb\u964d\u4f4e\u9690\u79c1\u4fdd\u62a4\uff0c\u53c8\u80fd\u9006\u5411\u63d0\u5347\u8131\u654f\u6587\u672c\u7684\u8d28\u91cf\u4e0e\u9690\u79c1\u6027\u3002", "motivation": "\u9488\u5bf9\u73b0\u6709\u8bcd\u7ea7\u5dee\u5206\u9690\u79c1\u6587\u672c\u8131\u654f\u65b9\u6cd5\u5b58\u5728\u7684\u4e0a\u4e0b\u6587\u8106\u5f31\u6027\u95ee\u9898\uff0c\u7814\u7a76\u8005\u5e0c\u671b\u501f\u52a9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5f3a\u5927\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u7cfb\u7edf\u8bc4\u4f30\u5176\u5bf9\u9690\u79c1\u4fdd\u62a4\u7684\u5f71\u54cd\u5e76\u63a2\u7d22\u6539\u8fdb\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u5b9e\u9a8c\u8303\u56f4\uff0c\u4f7f\u7528\u5148\u8fdbLLM\u5bf9\u591a\u79cd\u9690\u79c1\u7ea7\u522b\u7684\u4e0d\u540c\u8131\u654f\u673a\u5236\u8fdb\u884c\u6570\u636e\u91cd\u5efa\u653b\u51fb\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u6a21\u578b\u5bf9\u539f\u59cb\u8bed\u4e49\u7684\u63a8\u65ad\u80fd\u529b\u3002", "result": "\u53d1\u73b0LLM\u65e2\u80fd\u6709\u6548\u63a8\u65ad\u539f\u59cb\u6587\u672c\u8bed\u4e49\uff08\u964d\u4f4e\u5b9e\u8bc1\u9690\u79c1\u4fdd\u62a4\uff09\uff0c\u4e5f\u53ef\u901a\u8fc7\u5bf9\u6297\u6027\u91cd\u5efa\u4f18\u5316\u63d0\u5347\u8131\u654f\u6587\u672c\u8d28\u91cf\u4e0e\u9690\u79c1\u4fdd\u62a4\u7684\u5e73\u8861\u3002", "conclusion": "\u5efa\u8bae\u5c06LLM\u6570\u636e\u91cd\u5efa\u4f5c\u4e3a\u540e\u5904\u7406\u6b65\u9aa4\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u601d\u7ef4\u9006\u5411\u63d0\u5347\u5dee\u5206\u9690\u79c1\u6587\u672c\u7684\u9690\u79c1\u4fdd\u62a4\u6548\u679c\u3002"}}
{"id": "2508.19005", "pdf": "https://arxiv.org/pdf/2508.19005", "abs": "https://arxiv.org/abs/2508.19005", "authors": ["Yuxuan Cai", "Yipeng Hao", "Jie Zhou", "Hang Yan", "Zhikai Lei", "Rui Zhen", "Zhenhua Han", "Yutao Yang", "Junsong Li", "Qianjun Pan", "Tianyu Huai", "Qin Chen", "Xin Li", "Kai Chen", "Bo Zhang", "Xipeng Qiu", "Liang He"], "title": "Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As AI advances toward general intelligence, the focus is shifting from\nsystems optimized for static tasks to creating open-ended agents that learn\ncontinuously. In this paper, we introduce Experience-driven Lifelong Learning\n(ELL), a framework for building self-evolving agents capable of continuous\ngrowth through real-world interaction. The framework is built on four core\nprinciples: (1) Experience Exploration: Agents learn through continuous,\nself-motivated interaction with dynamic environments, navigating interdependent\ntasks and generating rich experiential trajectories. (2) Long-term Memory:\nAgents preserve and structure historical knowledge, including personal\nexperiences, domain expertise, and commonsense reasoning, into a persistent\nmemory system. (3) Skill Learning: Agents autonomously improve by abstracting\nrecurring patterns from experience into reusable skills, which are actively\nrefined and validated for application in new tasks. (4) Knowledge\nInternalization: Agents internalize explicit and discrete experiences into\nimplicit and intuitive capabilities as \"second nature\".\n  We also introduce StuLife, a benchmark dataset for ELL that simulates a\nstudent's holistic college journey, from enrollment to academic and personal\ndevelopment, across three core phases and ten detailed sub-scenarios. StuLife\nis designed around three key paradigm shifts: From Passive to Proactive, From\nContext to Memory, and From Imitation to Learning. In this dynamic environment,\nagents must acquire and distill practical skills and maintain persistent memory\nto make decisions based on evolving state variables. StuLife provides a\ncomprehensive platform for evaluating lifelong learning capabilities, including\nmemory retention, skill transfer, and self-motivated behavior. Beyond\nevaluating SOTA LLMs on the StuLife benchmark, we also explore the role of\ncontext engineering in advancing AGI.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7ecf\u9a8c\u9a71\u52a8\u7684\u7ec8\u8eab\u5b66\u4e60\u6846\u67b6ELL\uff0c\u901a\u8fc7\u56db\u4e2a\u6838\u5fc3\u673a\u5236\u5b9e\u73b0\u667a\u80fd\u4f53\u6301\u7eed\u8fdb\u5316\uff0c\u5e76\u5efa\u7acbStuLife\u57fa\u51c6\u8bc4\u4f30\u7cfb\u7edf\u3002", "motivation": "AI\u6b63\u5411\u901a\u7528\u667a\u80fd\u53d1\u5c55\uff0c\u9700\u4ece\u9759\u6001\u4efb\u52a1\u7cfb\u7edf\u8f6c\u5411\u80fd\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u4e92\u52a8\u6301\u7eed\u5b66\u4e60\u7684\u5f00\u653e\u667a\u80fd\u4f53\u3002", "method": "1) \u7ecf\u9a8c\u63a2\u7d22\uff1a\u667a\u80fd\u4f53\u5728\u52a8\u6001\u73af\u5883\u4e2d\u81ea\u4e3b\u4ea4\u4e92\n2) \u957f\u671f\u8bb0\u5fc6\uff1a\u6784\u5efa\u7ed3\u6784\u5316\u77e5\u8bc6\u5b58\u50a8\u7cfb\u7edf\n3) \u6280\u80fd\u5b66\u4e60\uff1a\u4ece\u7ecf\u9a8c\u62bd\u8c61\u53ef\u590d\u7528\u6280\u80fd\n4) \u77e5\u8bc6\u5185\u5316\uff1a\u5c06\u663e\u6027\u7ecf\u9a8c\u8f6c\u5316\u4e3a\u76f4\u89c9\u80fd\u529b", "result": "\u521b\u5efaStuLife\u6a21\u62df\u5927\u5b66\u751f\u6d3b\u5168\u5468\u671f\uff0c\u9a8c\u8bc1\u73b0\u6709LLM\u5728\u6301\u7eed\u5b66\u4e60\u573a\u666f\u4e2d\u7684\u8868\u73b0\uff0c\u63a2\u7d22\u4e0a\u4e0b\u6587\u5de5\u7a0b\u5bf9AGI\u53d1\u5c55\u7684\u5f71\u54cd\u3002", "conclusion": "ELL\u6846\u67b6\u4e3aAGI\u53d1\u5c55\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0cStuLife\u57fa\u51c6\u63a8\u52a8\u7ec8\u8eab\u5b66\u4e60\u80fd\u529b\u8bc4\u4f30\uff0c\u4e0a\u4e0b\u6587\u5de5\u7a0b\u662f\u63d0\u5347\u667a\u80fd\u4f53\u9002\u5e94\u6027\u7684\u5173\u952e\u3002"}}
{"id": "2508.19200", "pdf": "https://arxiv.org/pdf/2508.19200", "abs": "https://arxiv.org/abs/2508.19200", "authors": ["Xinran Zhao", "Boyuan Zheng", "Chenglei Si", "Haofei Yu", "Ken Liu", "Runlong Zhou", "Ruochen Li", "Tong Chen", "Xiang Li", "Yiming Zhang", "Tongshuang Wu"], "title": "The Ramon Llull's Thinking Machine for Automated Ideation", "categories": ["cs.AI", "cs.CL"], "comment": "21 pages, 3 figures", "summary": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for\ngenerating knowledge through symbolic recombination - as a conceptual\nfoundation for building a modern Llull's thinking machine for research\nideation. Our approach defines three compositional axes: Theme (e.g.,\nefficiency, adaptivity), Domain (e.g., question answering, machine\ntranslation), and Method (e.g., adversarial training, linear attention). These\nelements represent high-level abstractions common in scientific work -\nmotivations, problem settings, and technical approaches - and serve as building\nblocks for LLM-driven exploration. We mine elements from human experts or\nconference papers and show that prompting LLMs with curated combinations\nproduces research ideas that are diverse, relevant, and grounded in current\nliterature. This modern thinking machine offers a lightweight, interpretable\ntool for augmenting scientific creativity and suggests a path toward\ncollaborative ideation between humans and AI.", "AI": {"tldr": "Revisiting Llull's Ars combinatoria to\u6784\u5efa\u57fa\u4e8e\u7ec4\u5408\u8f74\uff08\u4e3b\u9898\u3001\u9886\u57df\u3001\u65b9\u6cd5\uff09\u7684\u73b0\u4ee3AI\u7814\u7a76\u6784\u601d\u5de5\u5177\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7ec4\u5408\u751f\u6210\u591a\u6837\u5316\u4e14\u53ef\u89e3\u91ca\u7684\u7814\u7a76\u521b\u610f\u3002", "motivation": "\u5c06\u4e2d\u4e16\u7eaa\u7ec4\u5408\u601d\u7ef4\u6846\u67b6\u4e0e\u73b0\u4ee3AI\u6280\u672f\u7ed3\u5408\uff0c\u89e3\u51b3\u79d1\u7814\u521b\u65b0\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4eba\u7c7b\u4e0eAI\u534f\u540c\u7684\u79d1\u5b66\u521b\u9020\u529b\u589e\u5f3a\u3002", "method": "\u5b9a\u4e49\u4e3b\u9898\uff08\u52a8\u673a\uff09\u3001\u9886\u57df\uff08\u95ee\u9898\u573a\u666f\uff09\u3001\u65b9\u6cd5\uff08\u6280\u672f\u8def\u5f84\uff09\u4e09\u5927\u7ec4\u5408\u8f74\uff0c\u901a\u8fc7\u4e13\u5bb6\u77e5\u8bc6/\u8bba\u6587\u6316\u6398\u8981\u7d20\u5e93\uff0c\u5f15\u5bfcLLM\u8fdb\u884c\u7ed3\u6784\u5316\u7ec4\u5408\u751f\u6210\u7814\u7a76\u521b\u610f\u3002", "result": "\u9a8c\u8bc1\u4e86\u7ec4\u5408\u5f0f\u63d0\u793a\u80fd\u4ea7\u751f\u4e0e\u5f53\u524d\u6587\u732e\u7d27\u5bc6\u76f8\u5173\u3001\u591a\u6837\u4e14\u5177\u843d\u5730\u6027\u7684\u7814\u7a76\u601d\u8def\uff08\u5982\u6548\u7387\u5bfc\u5411\u7684\u5bf9\u6297\u8bad\u7ec3\u5728\u95ee\u7b54\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff09\uff0c\u63d0\u4f9b\u8f7b\u91cf\u53ef\u89e3\u91ca\u7684\u79d1\u7814\u8f85\u52a9\u5de5\u5177\u3002", "conclusion": "\u73b0\u4ee3\u601d\u7ef4\u673a\u5668\u5b9e\u73b0\u4e86\u7ec4\u5408\u5f0f\u79d1\u7814\u521b\u65b0\u7684\u7cfb\u7edf\u5316\uff0c\u5f00\u8f9f\u4e86\u4eba\u673a\u534f\u540c\u7814\u7a76\u8303\u5f0f\uff0c\u4e3a\u4fdd\u6301\u79d1\u5b66\u53d1\u73b0\u6d3b\u529b\u63d0\u4f9b\u65b0\u8def\u5f84\u3002"}}
{"id": "2508.19229", "pdf": "https://arxiv.org/pdf/2508.19229", "abs": "https://arxiv.org/abs/2508.19229", "authors": ["Wei Xiong", "Wenting Zhao", "Weizhe Yuan", "Olga Golovneva", "Tong Zhang", "Jason Weston", "Sainbayar Sukhbaatar"], "title": "StepWiser: Stepwise Generative Judges for Wiser Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As models increasingly leverage multi-step reasoning strategies to solve\ncomplex problems, supervising the logical validity of these intermediate steps\nhas become a critical research challenge. Process reward models address this by\nproviding step-by-step feedback, but current approaches have two major\ndrawbacks: they typically function as classifiers without providing\nexplanations, and their reliance on supervised fine-tuning with static datasets\nlimits generalization. Inspired by recent advances, we reframe stepwise reward\nmodeling from a classification task to a reasoning task itself. We thus propose\na generative judge that reasons about the policy model's reasoning steps (i.e.,\nmeta-reasons), outputting thinking tokens before delivering a final verdict.\nOur model, StepWiser, is trained by reinforcement learning using relative\noutcomes of rollouts. We show it provides (i) better judgment accuracy on\nintermediate steps than existing methods; (ii) can be used to improve the\npolicy model at training time; and (iii) improves inference-time search.", "AI": {"tldr": "\u63d0\u51faStepWiser\u6a21\u578b\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u5143\u63a8\u7406\u548c\u5f3a\u5316\u5b66\u4e60\u6539\u8fdb\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff0c\u63d0\u5347\u4e2d\u95f4\u6b65\u9aa4\u5224\u65ad\u51c6\u786e\u6027\u548c\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u5b58\u5728\u4e24\u4e2a\u6838\u5fc3\u7f3a\u9677\uff1a1) \u4f5c\u4e3a\u5206\u7c7b\u5668\u7f3a\u4e4f\u89e3\u91ca\u6027 2) \u4f9d\u8d56\u9759\u6001\u6570\u636e\u96c6\u76d1\u7763\u5fae\u8c03\u9650\u5236\u6cdb\u5316\u80fd\u529b\u3002\u9700\u8981\u80fd\u8fdb\u884c\u5143\u63a8\u7406\u7684\u751f\u6210\u5f0f\u6a21\u578b\u6765\u63d0\u5347\u4e2d\u95f4\u6b65\u9aa4\u76d1\u7763\u6548\u679c", "method": "\u5c06\u9010\u6b65\u5956\u52b1\u5efa\u6a21\u91cd\u6784\u4e3a\u63a8\u7406\u4efb\u52a1\uff0c\u5f00\u53d1\u751f\u6210\u5f0f\u5224\u65ad\u6a21\u578bStepWiser\u3002\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5229\u7528rollout\u76f8\u5bf9\u7ed3\u679c\u8fdb\u884c\u8bad\u7ec3\uff0c\u6a21\u578b\u9996\u5148\u751f\u6210\u601d\u8003\u6807\u8bb0\u518d\u8f93\u51fa\u6700\u7ec8\u5224\u65ad", "result": "StepWiser\u5728\u4e2d\u95f4\u6b65\u9aa4\u5224\u65ad\u51c6\u786e\u7387\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u53ef\u6709\u6548\u6539\u8fdb\u7b56\u7565\u6a21\u578b\u8bad\u7ec3\uff0c\u5e76\u63d0\u5347\u63a8\u7406\u65f6\u7684\u641c\u7d22\u6548\u7387", "conclusion": "\u901a\u8fc7\u751f\u6210\u5f0f\u5143\u63a8\u7406\u6846\u67b6\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0cStepWiser\u7a81\u7834\u4e86\u4f20\u7edf\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6b65\u9aa4\u76d1\u7763\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
