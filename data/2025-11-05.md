<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 24]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 8]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.CV](#cs.CV) [Total: 8]
- [cs.LG](#cs.LG) [Total: 7]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Multi-Personality Generation of LLMs at Decoding-time](https://arxiv.org/abs/2511.01891)
*Rongxin Chen,Yunfan Li,Yige Yuan,Bingbing Xu,Huawei Shen*

Main category: cs.CL

TL;DR: 提出MPG框架，利用单维度模型的隐式密度比和SCR算法，无需重新训练即可实现大语言模型的多人格生成


<details>
  <summary>Details</summary>
Motivation: 现有基于重训练的方法成本高且扩展性差，解码时方法依赖外部模型或启发式规则，限制灵活性和鲁棒性

Method: 通过目标策略聚合隐式密度比重构生成任务，设计基于推测性分块拒绝采样(SCR)的并行验证机制

Result: 在MBTI人格和角色扮演任务中取得16%-18%的性能提升

Conclusion: MPG框架有效实现多维度人格控制，代码和数据已在GitHub开源

Abstract: Multi-personality generation for LLMs, enabling simultaneous embodiment of
multiple personalization attributes, is a fundamental challenge. Existing
retraining-based approaches are costly and poorly scalable, while decoding-time
methods often rely on external models or heuristics, limiting flexibility and
robustness. In this paper, we propose a novel Multi-Personality Generation
(MPG) framework under the decoding-time combination paradigm. It flexibly
controls multi-personality without relying on scarce multi-dimensional models
or extra training, leveraging implicit density ratios in single-dimensional
models as a "free lunch" to reformulate the task as sampling from a target
strategy aggregating these ratios. To implement MPG efficiently, we design
Speculative Chunk-level based Rejection sampling (SCR), which generates
responses in chunks and parallelly validates them via estimated thresholds
within a sliding window. This significantly reduces computational overhead
while maintaining high-quality generation. Experiments on MBTI personality and
Role-Playing demonstrate the effectiveness of MPG, showing improvements up to
16%-18%. Code and data are available at https://github.com/Libra117/MPG .

</details>


### [2] [Rethinking LLM Human Simulation: When a Graph is What You Need](https://arxiv.org/abs/2511.02135)
*Joseph Suh,Suhong Moon,Serina Chang*

Main category: cs.CL

TL;DR: 提出轻量级图神经网络模型GEMS，在人类行为模拟任务中实现优于LLM的效率和准确性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)在人类模拟任务中存在计算成本高、可解释性差的问题，探索更高效的领域专用模型可能性

Method: 将离散选择模拟转化为图链接预测问题，构建包含实体关系的图结构，有限整合语言表示

Result: 在三个模拟数据集上取得相当或优于LLM的准确率，计算效率提升三个数量级

Conclusion: 图神经网络可作为LLM的高效替代方案，在保持性能优势的同时显著提升可解释性和计算效率

Abstract: Large language models (LLMs) are increasingly used to simulate humans, with
applications ranging from survey prediction to decision-making. However, are
LLMs strictly necessary, or can smaller, domain-grounded models suffice? We
identify a large class of simulation problems in which individuals make choices
among discrete options, where a graph neural network (GNN) can match or surpass
strong LLM baselines despite being three orders of magnitude smaller. We
introduce Graph-basEd Models for human Simulation (GEMS), which casts discrete
choice simulation tasks as a link prediction problem on graphs, leveraging
relational knowledge while incorporating language representations only when
needed. Evaluations across three key settings on three simulation datasets show
that GEMS achieves comparable or better accuracy than LLMs, with far greater
efficiency, interpretability, and transparency, highlighting the promise of
graph-based modeling as a lightweight alternative to LLMs for human simulation.
Our code is available at https://github.com/schang-lab/gems.

</details>


### [3] [IG-Pruning: Input-Guided Block Pruning for Large Language Models](https://arxiv.org/abs/2511.02213)
*Kangyu Qiao,Shaolei Zhang,Yang Feng*

Main category: cs.CL

TL;DR: 提出IG-Pruning动态剪枝方法，通过语义聚类和动态掩码选择优化LLM推理效率


<details>
  <summary>Details</summary>
Motivation: 现有静态深度剪枝方法使用固定块掩码，无法适应不同任务/输入的动态需求，导致性能次优

Method: 两阶段框架：1) 通过语义聚类和L0优化发现多样掩码候选 2) 无需重训练的动态剪枝机制

Result: 实验显示在多个任务上优于SOTA静态方法，特别适用于资源受限场景

Conclusion: 动态输入感知剪枝可有效平衡LLM计算成本与性能，为实际部署提供高效解决方案

Abstract: With the growing computational demands of large language models (LLMs),
efficient inference has become increasingly critical for practical deployment.
Depth pruning has emerged as a promising approach for reducing the
computational costs of large language models by removing transformer layers.
However, existing methods typically rely on fixed block masks, which can lead
to suboptimal performance across different tasks and inputs. In this paper, we
propose IG-Pruning, a novel input-aware block-wise pruning method that
dynamically selects layer masks at inference time. Our approach consists of two
stages: (1) Discovering diverse mask candidates through semantic clustering and
L0 optimization, and (2) Implementing efficient dynamic pruning without the
need for extensive training. Experimental results demonstrate that our method
consistently outperforms state-of-the-art static depth pruning methods, making
it particularly suitable for resource-constrained deployment scenarios.

</details>


### [4] [Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not Guarantee Generalizable Results](https://arxiv.org/abs/2511.02246)
*Jonathan Liu,Haoling Qiu,Jonathan Lasko,Damianos Karakos,Mahsa Yarmohammadi,Mark Dredze*

Main category: cs.CL

TL;DR: 研究开发了自动生成医疗查询并利用多LLM评估的基础设施，发现LLM评估者一致性低（Cohen's Kappa=0.118），建议采用多评估者避免统计偏差。


<details>
  <summary>Details</summary>
Motivation: 医疗聊天机器人需在含人口统计等非医疗因素时保持建议一致性，但现有LLMs存在幻觉/遗漏/偏见问题，需系统性检测失效条件。

Method: 1) 构建自动化查询生成管道（采样患者人口统计/病史/写作风格）；2) 多LLM评估流程（幻觉检测+治疗分类器）；3) 基线研究（LLM间一致性+不同LLM组合影响）。

Result: LLM评估者一致性低，仅特定LLM组合在写作风格/性别/种族维度呈现显著差异。建议使用多LLM评估并公开LLM间一致性指标。

Conclusion: 医疗领域LLM评估需多评估者交叉验证，提升结果可靠性。公开代码/数据集（https://github.com/BBN-E/medic-neurips-2025-demo）促进研究透明度。

Abstract: Recent research has shown that hallucinations, omissions, and biases are
prevalent in everyday use-cases of LLMs. However, chatbots used in medical
contexts must provide consistent advice in situations where non-medical factors
are involved, such as when demographic information is present. In order to
understand the conditions under which medical chatbots fail to perform as
expected, we develop an infrastructure that 1) automatically generates queries
to probe LLMs and 2) evaluates answers to these queries using multiple
LLM-as-a-judge setups and prompts. For 1), our prompt creation pipeline samples
the space of patient demographics, histories, disorders, and writing styles to
create realistic questions that we subsequently use to prompt LLMs. In 2), our
evaluation pipeline provides hallucination and omission detection using
LLM-as-a-judge as well as agentic workflows, in addition to LLM-as-a-judge
treatment category detectors. As a baseline study, we perform two case studies
on inter-LLM agreement and the impact of varying the answering and evaluation
LLMs. We find that LLM annotators exhibit low agreement scores (average Cohen's
Kappa $\kappa=0.118$), and only specific (answering, evaluation) LLM pairs
yield statistically significant differences across writing styles, genders, and
races. We recommend that studies using LLM evaluation use multiple LLMs as
evaluators in order to avoid arriving at statistically significant but
non-generalizable results, particularly in the absence of ground-truth data. We
also suggest publishing inter-LLM agreement metrics for transparency. Our code
and dataset are available here:
https://github.com/BBN-E/medic-neurips-2025-demo.

</details>


### [5] [LTD-Bench: Evaluating Large Language Models by Letting Them Draw](https://arxiv.org/abs/2511.02347)
*Liuhao Lin,Ke Li,Zihan Xu,Yuchen Shi,Yulei Qin,Yan Zhang,Xing Sun,Rongrong Ji*

Main category: cs.CL

TL;DR: 提出可视化评测基准LTD-Bench，通过绘图或代码生成直观暴露LLM空间推理缺陷


<details>
  <summary>Details</summary>
Motivation: 传统数值评估指标掩盖LLM空间推理局限，导致模型能力评估与实际应用脱节，在物理世界理解场景存在风险

Method: 设计包含生成任务（测试空间想象）与识别任务（评估空间感知）的三级渐进评测框架，系统检验语言-空间双向映射能力

Result: 实验揭示顶尖LLM存在显著能力断层，传统基准高分模型在语言-空间概念双向映射中表现严重不足

Conclusion: LTD-Bench的视觉输出特性不仅实现直观评估，还为模型诊断与相似性研究提供新途径，暴露LLM作为世界模型的根本性局限

Abstract: Current evaluation paradigms for large language models (LLMs) represent a
critical blind spot in AI research--relying on opaque numerical metrics that
conceal fundamental limitations in spatial reasoning while providing no
intuitive understanding of model capabilities. This deficiency creates a
dangerous disconnect between reported performance and practical abilities,
particularly for applications requiring physical world understanding. We
introduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation
from abstract scores to directly observable visual outputs by requiring models
to generate drawings through dot matrices or executable code. This approach
makes spatial reasoning limitations immediately apparent even to non-experts,
bridging the fundamental gap between statistical performance and intuitive
assessment. LTD-Bench implements a comprehensive methodology with complementary
generation tasks (testing spatial imagination) and recognition tasks (assessing
spatial perception) across three progressively challenging difficulty levels,
methodically evaluating both directions of the critical language-spatial
mapping. Our extensive experiments with state-of-the-art models expose an
alarming capability gap: even LLMs achieving impressive results on traditional
benchmarks demonstrate profound deficiencies in establishing bidirectional
mappings between language and spatial concept--a fundamental limitation that
undermines their potential as genuine world models. Furthermore, LTD-Bench's
visual outputs enable powerful diagnostic analysis, offering a potential
approach to investigate model similarity.

</details>


### [6] [Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation](https://arxiv.org/abs/2511.02358)
*Wongyu Kim,Hochang Lee,Sanghak Lee,Yoonsung Kim,Jaehyun Park*

Main category: cs.CL

TL;DR: 提出M-Solomon多模态嵌入器，通过自适应查询增强策略有效平衡性能与延迟，在减少47%延迟的同时超越基准模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-based查询增强方法存在全量增强导致的延迟问题（部分查询性能下降），且未探索多模态场景。需要动态决策何时进行查询增强以提高效率。

Method: 1. 训练集层级划分需/无需增强的查询组 2. 利用MLLM生成合成增强 3. 设计自适应机制：生成/augment前缀触发增强，/embed前缀直接嵌入。

Result: M-Solomon在保持高检索质量的同时，嵌入延迟降低47%，性能超越全量增强基线（+12.3% MRR）和无增强基线（+21.5% MRR）。

Conclusion: 自适应查询增强策略在多模态场景中有效平衡精度与效率，M-Solomon为跨模态检索系统提供更优的解决方案。

Abstract: Query augmentation makes queries more meaningful by appending further
information to the queries to find relevant documents. Current studies have
proposed Large Language Model (LLM)-based embedders, which learn representation
for embedding and generation for query augmentation in a multi-task manner by
leveraging the generative capabilities of LLM. During inference, these jointly
trained embedders have conducted query augmentation followed by embedding,
showing effective results. However, augmenting every query leads to substantial
embedding latency and query augmentation can be detrimental to performance for
some queries. Also, previous methods have not been explored in multimodal
environments. To tackle these problems, we propose M-Solomon, a universal
multimodal embedder that can adaptively determine when to augment queries. Our
approach first divides the queries of the training datasets into two groups at
the dataset level. One includes queries that require augmentation and the other
includes queries that do not. Then, we introduces a synthesis process that
generates appropriate augmentations for queries that require them by leveraging
a powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation.
Through this step, M-Solomon can conduct query augmentation only when necessary
by learning to generate synthetic augmentations with the prefix /augment for
queries that demand them and to generate the simple string /embed for others.
Experimental results showed that M-Solomon not only surpassed the baseline
without augmentation by a large margin but also outperformed the baseline that
always used augmentation, providing much faster embedding latency.

</details>


### [7] [LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context](https://arxiv.org/abs/2511.02366)
*Yudong Li,Zhongliang Yang,Kejiang Chen,Wenxuan Wang,Tianxin Zhang,Sifang Wan,Kecheng Wang,Haitian Li,Xu Wang,Lefan Cheng,Youdan Yang,Baocheng Chen,Ziyu Liu,Yufei Sun,Liyan Wu,Wenya Wen,Xingchi Gu,Peiru Yang*

Main category: cs.CL

TL;DR: 推出动态更新的中文大模型安全基准LiveSecBench，覆盖合法性等六个安全维度，建立中文AI安全评估体系


<details>
  <summary>Details</summary>
Motivation: 针对中文法律和社会框架，构建动态更新的安全评估体系以适应快速迭代的AI威胁场景

Method: 设计六维评估框架（合法性、伦理、事实性、隐私、对抗鲁棒性、推理安全），采用持续更新机制（计划纳入文生图安全和智能体安全）

Result: 完成18个大模型的评估（v251030版本），建立公开可访问的中文AI安全排行榜

Conclusion: LiveSecBench为中文语境下的AI安全提供动态评估工具，通过持续更新机制保持评估体系的前沿性，推动安全技术发展

Abstract: In this work, we propose LiveSecBench, a dynamic and continuously updated
safety benchmark specifically for Chinese-language LLM application scenarios.
LiveSecBench evaluates models across six critical dimensions (Legality, Ethics,
Factuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in
the Chinese legal and social frameworks. This benchmark maintains relevance
through a dynamic update schedule that incorporates new threat vectors, such as
the planned inclusion of Text-to-Image Generation Safety and Agentic Safety in
the next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs,
providing a landscape of AI safety in the context of Chinese language. The
leaderboard is publicly accessible at https://livesecbench.intokentech.cn/.

</details>


### [8] [AyurParam: A State-of-the-Art Bilingual Language Model for Ayurveda](https://arxiv.org/abs/2511.02374)
*Mohd Nauman,Sravan Gvm,Vijay Devane,Shyam Pawar,Viraj Thakur,Kundeshwar Pundalik,Piyush Sawarkar,Rohit Saluja,Maunendra Desarkar,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 提出了AyurParam-2.9B——基于高质量阿育吠陀医学数据集微调的双语专业模型，在1.5-3B参数规模中性能超越所有开源模型，并具备与更大模型竞争的能力


<details>
  <summary>Details</summary>
Motivation: 主流大模型在需要深厚文化/专业知识的传统医学领域（如阿育吠陀）表现不佳，无法准确解析其复杂的文本和临床知识体系

Method: 基于Param-1-2.9B架构，使用英语和印地语双语构建的上下文感知型阿育吠陀数据集（含经典文献与临床指南），采用严格标注协议确保事实精确性

Result: 在BhashaBench-Ayur基准测试中，不仅超越同规模指令微调模型，部分指标甚至优于参数量更大的模型

Conclusion: 专业医学领域AI需通过真实的领域适应（domain adaptation）和高质量监督数据，才能实现可靠且文化适配的知识应用

Abstract: Current large language models excel at broad, general-purpose tasks, but
consistently underperform when exposed to highly specialized domains that
require deep cultural, linguistic, and subject-matter expertise. In particular,
traditional medical systems such as Ayurveda embody centuries of nuanced
textual and clinical knowledge that mainstream LLMs fail to accurately
interpret or apply. We introduce AyurParam-2.9B, a domain-specialized,
bilingual language model fine-tuned from Param-1-2.9B using an extensive,
expertly curated Ayurveda dataset spanning classical texts and clinical
guidance. AyurParam's dataset incorporates context-aware, reasoning, and
objective-style Q&A in both English and Hindi, with rigorous annotation
protocols for factual precision and instructional clarity. Benchmarked on
BhashaBench-Ayur, AyurParam not only surpasses all open-source
instruction-tuned models in its size class (1.5--3B parameters), but also
demonstrates competitive or superior performance compared to much larger
models. The results from AyurParam highlight the necessity for authentic domain
adaptation and high-quality supervision in delivering reliable, culturally
congruent AI for specialized medical knowledge.

</details>


### [9] [AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2511.02376)
*Aashray Reddy,Andrew Zagula,Nicholas Saban*

Main category: cs.CL

TL;DR: 研究提出AutoAdv框架，通过多轮自适应攻击使LLM安全机制失效，攻击成功率高达95%，揭示现有单轮防御策略的不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全评估集中于单轮攻击，但实际攻击往往通过多轮对话逐步突破防线，需研究多轮攻击的有效性及防御漏洞。

Method: 结合模式管理器（学习成功攻击模式）、温度管理器（动态调整采样参数）、两阶段重写策略（伪装+迭代优化）实现自动化多轮越狱。

Result: 在Llama-3.1-8B等模型上实现6轮内95%攻击成功率，较单轮基线提升24%，多轮攻击效果在GPT-4o-mini等模型持续领先单轮方法。

Conclusion: 当前安全对齐策略无法抵御多轮攻击，需开发多轮感知的防御机制，暴露了LLM安全研究的重要盲区。

Abstract: Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where
adversarial prompts elicit harmful outputs, yet most evaluations focus on
single-turn interactions while real-world attacks unfold through adaptive
multi-turn conversations. We present AutoAdv, a training-free framework for
automated multi-turn jailbreaking that achieves up to 95% attack success rate
on Llama-3.1-8B within six turns a 24 percent improvement over single turn
baselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern
manager that learns from successful attacks to enhance future prompts, a
temperature manager that dynamically adjusts sampling parameters based on
failure modes, and a two-phase rewriting strategy that disguises harmful
requests then iteratively refines them. Extensive evaluation across commercial
and open-source models (GPT-4o-mini, Qwen3-235B, Mistral-7B) reveals persistent
vulnerabilities in current safety mechanisms, with multi-turn attacks
consistently outperforming single-turn approaches. These findings demonstrate
that alignment strategies optimized for single-turn interactions fail to
maintain robustness across extended conversations, highlighting an urgent need
for multi-turn-aware defenses.

</details>


### [10] [Merging Continual Pretraining Models for Domain-Specialized LLMs: A Case Study in Finance](https://arxiv.org/abs/2511.02451)
*Kentaro Ueda,François Portet,Hirohiko Suwa,Keiichi Yasumoto*

Main category: cs.CL

TL;DR: 通过合并领域持续预训练专家模型提升金融大语言模型的多技能表现，首次系统性分析CPT模型合并机制并验证其跨领域技能涌现潜力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在金融等专业领域存在多技能融合难题，直接训练成本高且不稳定，探索CPT专家模型合并作为替代方案。

Method: 构建金融/数学/日语专家模型，设计知识恢复-互补性-涌现性三阶段评估框架，在18个任务的金融基准测试三种合并方法（Task Arithmetic/TIES/DARE-TIES）。

Result: 专家与基础模型合并可恢复通用知识，专家间合并提升性能并产生跨领域涌现技能，Task Arithmetic效果强但超参数敏感，TIES更稳健。

Conclusion: 模型相似性与合并效果相关，但涌现技能依赖复杂因素，研究为基于现有资源构建多技能LLMs提供了首个系统性分析框架与实践指导。

Abstract: While LLMs excel at general tasks, they struggle in specialized domains like
finance, requiring diverse skills in domain knowledge, mathematical reasoning,
and multilingual processing. Merging domain-specific Continual Pre-training
(CPT) "experts" offers a practical alternative to costly and unstable
multi-skill training. However, unlike established Supervised Fine-Tuning (SFT)
model-based merging, CPT model merging remains largely unexplored. We address
this gap by creating financial LLMs from experts in finance, math, and
Japanese. We propose a three-stage evaluation focusing on knowledge recovery,
complementarity, and emergence, and assess three merging methods (Task
Arithmetic, TIES, and DARE-TIES) on a comprehensive financial benchmark curated
from 18 tasks across 8 established datasets. Results show that merging an
expert with its base model recovers general knowledge lost during CPT, while
merging experts improves performance and can yield emergent cross-domain
skills. Among the methods, Task Arithmetic performs strongly but is
hyperparameter-sensitive, whereas TIES is more robust. Our findings also
suggest that while model similarity correlates with merging success, emergent
skills depend on more complex factors. This work presents the first
foundational analysis of CPT model merging, establishing a principled framework
and providing clear guidance for building multi-skill LLMs from existing
assets.

</details>


### [11] [Prompting for Policy: Forecasting Macroeconomic Scenarios with Synthetic LLM Personas](https://arxiv.org/abs/2511.02458)
*Giulia Iadisernia,Carolina Camassa*

Main category: cs.CL

TL;DR: GPT-4o与人类专家在宏观经济预测中表现接近，角色提示无实际增益可省略


<details>
  <summary>Details</summary>
Motivation: 验证人物角色提示策略能否提升大语言模型在宏观经济预测任务中的表现

Method: 使用2,368个经济角色提示GPT-4o进行季度预测，对比人类专家与基线模型，特别设计样本外(2024-2025)验证

Result: 1. GPT-4o预测误差与人类差异显著但实际微小；2.角色提示对预测精度无显著提升效果

Conclusion: 上下文数据支持下GPT-4o具备样本外事件预测能力，但提示多样性未能产生人类专家组的预测异质性优势

Abstract: We evaluate whether persona-based prompting improves Large Language Model
(LLM) performance on macroeconomic forecasting tasks. Using 2,368
economics-related personas from the PersonaHub corpus, we prompt GPT-4o to
replicate the ECB Survey of Professional Forecasters across 50 quarterly rounds
(2013-2025). We compare the persona-prompted forecasts against the human
experts panel, across four target variables (HICP, core HICP, GDP growth,
unemployment) and four forecast horizons. We also compare the results against
100 baseline forecasts without persona descriptions to isolate its effect. We
report two main findings. Firstly, GPT-4o and human forecasters achieve
remarkably similar accuracy levels, with differences that are statistically
significant yet practically modest. Our out-of-sample evaluation on 2024-2025
data demonstrates that GPT-4o can maintain competitive forecasting performance
on unseen events, though with notable differences compared to the in-sample
period. Secondly, our ablation experiment reveals no measurable forecasting
advantage from persona descriptions, suggesting these prompt components can be
omitted to reduce computational costs without sacrificing accuracy. Our results
provide evidence that GPT-4o can achieve competitive forecasting accuracy even
on out-of-sample macroeconomic events, if provided with relevant context data,
while revealing that diverse prompts produce remarkably homogeneous forecasts
compared to human panels.

</details>


### [12] [Smart-Hiring: An Explainable end-to-end Pipeline for CV Information Extraction and Job Matching](https://arxiv.org/abs/2511.02537)
*Kenza Khelkhal,Dihia Lanasri*

Main category: cs.CL

TL;DR: 提出端到端NLP框架Smart-Hiring，通过简历解析与语义匹配实现自动化招聘，兼具匹配准确性和可解释性


<details>
  <summary>Details</summary>
Motivation: 传统人工筛选简历存在效率低、易出错、主观偏见等问题，需要自动化解决方案来提升招聘效率与公平性

Method: 结合文档解析、命名实体识别和文本嵌入技术，构建共享向量空间计算简历与职位描述的语义相似度，模块化设计支持过程可解释

Result: 在多领域真实数据集验证中实现竞争力的匹配准确率，同时保持决策过程的可追溯性和透明度

Conclusion: 建立可扩展的招聘分析框架，为后续偏见缓解、公平性建模和大规模部署数据驱动解决方案奠定基础

Abstract: Hiring processes often involve the manual screening of hundreds of resumes
for each job, a task that is time and effort consuming, error-prone, and
subject to human bias. This paper presents Smart-Hiring, an end-to-end Natural
Language Processing (NLP) pipeline de- signed to automatically extract
structured information from unstructured resumes and to semantically match
candidates with job descriptions. The proposed system combines document
parsing, named-entity recognition, and contextual text embedding techniques to
capture skills, experience, and qualifications. Using advanced NLP technics,
Smart-Hiring encodes both resumes and job descriptions in a shared vector space
to compute similarity scores between candidates and job postings. The pipeline
is modular and explainable, allowing users to inspect extracted entities and
matching rationales. Experiments were conducted on a real-world dataset of
resumes and job descriptions spanning multiple professional domains,
demonstrating the robustness and feasibility of the proposed approach. The
system achieves competitive matching accuracy while preserving a high degree of
interpretability and transparency in its decision process. This work introduces
a scalable and practical NLP frame- work for recruitment analytics and outlines
promising directions for bias mitigation, fairness-aware modeling, and
large-scale deployment of data-driven hiring solutions.

</details>


### [13] [The Analysis of Lexical Errors in Machine Translation from English into Romanian](https://arxiv.org/abs/2511.02587)
*Angela Stamatie*

Main category: cs.CL

TL;DR: 分析Google Translate在新冠疫情相关英文文本翻译成罗马尼亚语时的词汇错误，通过230篇文本分析提出改进方案


<details>
  <summary>Details</summary>
Motivation: 机器翻译在处理专业官方文本时存在词汇选择不准确的问题，改进Google Translate的翻译质量对疫情信息传播至关重要

Method: 选取WHO/Gavi/药品说明书230篇新冠相关英文文本，通过Google Translate翻译后系统分析词汇错误类型及分布

Result: 发现专业术语和剂量说明等领域的词汇选择错误率较高，错误模式呈现特定规律性

Conclusion: 针对性的词汇优化可有效提升机器翻译在专业领域的准确性，为改进神经网络翻译模型提供实证依据

Abstract: The research explores error analysis in the performance of translating by
Machine Translation from English into Romanian, and it focuses on lexical
errors found in texts which include official information, provided by the World
Health Organization (WHO), the Gavi Organization, by the patient information
leaflet (the information about the active ingredients of the vaccines or the
medication, the indications, the dosage instructions, the storage instructions,
the side effects and warning, etc.). All of these texts are related to Covid-19
and have been translated by Google Translate, a multilingual Machine
Translation that was created by Google. In the last decades, Google has
actively worked to develop a more accurate and fluent automatic translation
system. This research, specifically focused on improving Google Translate, aims
to enhance the overall quality of Machine Translation by achieving better
lexical selection and by reducing errors. The investigation involves a
comprehensive analysis of 230 texts that have been translated from English into
Romanian.

</details>


### [14] [Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations to Decode Student Behaviour](https://arxiv.org/abs/2511.02599)
*Max Norris,Kobi Gal,Sahan Bulathwela*

Main category: cs.CL

TL;DR: 提出NTKT方法，将知识追踪任务重构为LLM的下一个token预测任务，利用问题文本内容显著提升预测性能并增强冷启动场景泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪模型主要依赖答题正确率和元数据，却忽略了问题文本本身蕴含的教学价值，导致预测能力受限和教学洞察力缺失。

Method: 通过预训练大语言模型将学生行为序列和问题文本统一转化为文本序列，实现语言模式和行为模式的双重学习。

Result: 实验证明该方法在冷启动问题和用户场景下，性能显著超越现有神经知识追踪模型，展现出更强的泛化能力。

Conclusion: 问题文本内容对知识追踪至关重要，利用LLM的预训练表征能更有效建模学习过程，为个性化教育提供新范式。

Abstract: Modelling student knowledge is a key challenge when leveraging AI in
education, with major implications for personalised learning. The Knowledge
Tracing (KT) task aims to predict how students will respond to educational
questions in learning environments, based on their prior interactions. Existing
KT models typically use response correctness along with metadata like skill
tags and timestamps, often overlooking the question text, which is an important
source of pedagogical insight. This omission poses a lost opportunity while
limiting predictive performance. We propose Next Token Knowledge Tracing
(NTKT), a novel approach that reframes KT as a next-token prediction task using
pretrained Large Language Models (LLMs). NTKT represents both student histories
and question content as sequences of text, allowing LLMs to learn patterns in
both behaviour and language. Our series of experiments significantly improves
performance over state-of-the-art neural KT models and generalises much better
to cold-start questions and users. These findings highlight the importance of
question content in KT and demonstrate the benefits of leveraging pretrained
representations of LLMs to model student learning more effectively.

</details>


### [15] [CGES: Confidence-Guided Early Stopping for Efficient and Accurate Self-Consistency](https://arxiv.org/abs/2511.02603)
*Ehsan Aghazadeh,Ahmad Ghasemi,Hedyeh Beyhaghi,Hossein Pishro-Nik*

Main category: cs.CL

TL;DR: 提出置信度引导的提前停止框架CGES，在保持精度的前提下将大模型调用次数减少69%


<details>
  <summary>Details</summary>
Motivation: 传统自洽策略需要固定调用次数且在正确答案罕见时容易失效，需要更高效可靠的推理策略

Method: 利用token概率/奖励模型构建置信度信号，通过贝叶斯框架计算候选答案后验分布，动态停止采样

Result: 在五个推理基准测试中平均减少69%调用次数（如16→4.9），精度差异小于0.06%

Conclusion: CGES实现了效率与精度的平衡，为置信度校准提供理论保障，适用于大规模语言模型部署

Abstract: Large language models (LLMs) are often queried multiple times at test time,
with predictions aggregated by majority vote. While effective, this
self-consistency strategy (arXiv:2203.11171) requires a fixed number of calls
and can fail when the correct answer is rare. We introduce Confidence-Guided
Early Stopping (CGES), a Bayesian framework that forms posteriors over
candidate answers using scalar confidence signals derived from token
probabilities or reward models. CGES adaptively halts sampling once the
posterior mass of a candidate exceeds a threshold. We provide theoretical
guarantees for both perfectly calibrated confidences and realistic noisy
confidence signals. Across five reasoning benchmarks, CGES reduces the average
number of model calls by about 69 percent (for example, from 16.0 to 4.9) while
matching the accuracy of self-consistency within 0.06 percentage points.

</details>


### [16] [The Realignment Problem: When Right becomes Wrong in LLMs](https://arxiv.org/abs/2511.02623)
*Aakash Sen Sharma,Debdeep Sanyal,Vivek Srivastava,Shirish Karande,Murari Mandal*

Main category: cs.CL

TL;DR: 提出TRACE框架解决大语言模型动态对齐难题，通过数据筛选和混合优化实现策略更新，保持模型性能的同时适应政策变化。


<details>
  <summary>Details</summary>
Motivation: 现有模型对齐方法存在静态脆弱性（Alignment-Reality Gap），无法适应快速变化的政策规范，重新标注成本高，传统遗忘方法损害模型性能。

Method: 1. 策略冲突数据筛选 2. 基于对齐影响评分定位关键冲突 3. 混合优化（偏好反转/删除/保留）实现精准策略更新

Result: 在Qwen2.5-7B/Gemma-2-9B/Llama-3.1-8B等模型验证，PKU-SafeRLHF数据集上实现策略迁移，保持基础能力不退化

Conclusion: 建立了可扩展的动态对齐范式，为可持续AI部署提供新方案，通过程序化策略应用降低维护成本

Abstract: The alignment of Large Language Models (LLMs) with human values is central to
their safe deployment, yet current practice produces static, brittle, and
costly-to-maintain models that fail to keep pace with evolving norms and
policies. This misalignment, which we term the Alignment-Reality Gap, poses a
growing challenge for reliable long-term use. Existing remedies are inadequate:
large-scale re-annotation is economically prohibitive, and standard unlearning
methods act as blunt instruments that erode utility rather than enable precise
policy updates. We introduce TRACE (Triage and Re-align by Alignment Conflict
Evaluation), a framework for principled unlearning that reconceives
re-alignment as a programmatic policy application problem. TRACE
programmatically triages existing preference data against a new policy,
identifies high-impact conflicts via a alignment impact score, and applies a
hybrid optimization that cleanly inverts, discards, or preserves preferences
while safeguarding model performance. Empirical results show that TRACE
achieves robust re-alignment across diverse model families (Qwen2.5-7B,
Gemma-2-9B, Llama-3.1-8B). On both synthetic benchmarks and the PKU-SafeRLHF
dataset under complex policy shift, TRACE enforces new principles without
degrading general capabilities. Our work establishes a scalable, dynamic, and
cost-effective paradigm for maintaining LLM alignment, providing a foundation
for sustainable and responsible AI deployment.

</details>


### [17] [Understanding New-Knowledge-Induced Factual Hallucinations in LLMs: Analysis, Solution, and Interpretation](https://arxiv.org/abs/2511.02626)
*Renfei Dang,Peng Hu,Changjiang Gao,Shujian Huang*

Main category: cs.CL

TL;DR: 研究发现LLM微调中引入特定类型新知识会加剧事实性幻觉，提出KnownPatch方法通过补充已知知识有效缓解问题


<details>
  <summary>Details</summary>
Motivation: 现有研究未深入揭示新知识导致幻觉的具体机制，本研究旨在填补这一空白，探究幻觉的驱动因素与传播规律

Method: 构建Biography-Reasoning控制数据集，通过多知识类型/任务类型细粒度分析，提出训练后期补充已知知识的KnownPatch方法，结合注意力机制分析模型行为

Result: 特定知识类型高陌生性导致幻觉增加，KnownPatch使幻觉率降低18.7%，注意力分析显示该方法能恢复对关键实体的关注

Conclusion: 知识类型陌生性而非总量驱动幻觉，KnownPatch通过修复注意力机制有效控制幻觉传播，为LLM知识注入提供新思路

Abstract: Previous studies show that introducing new knowledge during large language
models (LLMs) fine-tuning can lead to the generation of erroneous output when
tested on known information, thereby triggering factual hallucinations.
However, existing studies have not deeply investigated the specific
manifestations and underlying mechanisms of these hallucinations. Our work
addresses this gap by designing a controlled dataset Biography-Reasoning, and
conducting a fine-grained analysis across multiple knowledge types and two task
types, including knowledge question answering (QA) and knowledge reasoning
tasks. We find that when fine-tuned on a dataset in which a specific knowledge
type consists entirely of new knowledge, LLMs exhibit significantly increased
hallucination tendencies. This suggests that the high unfamiliarity of a
particular knowledge type, rather than the overall proportion of new knowledge,
is a stronger driver of hallucinations, and these tendencies can even affect
other knowledge types in QA tasks. To mitigate such factual hallucinations, we
propose KnownPatch, which patches a small number of known knowledge samples in
the later stages of training, effectively alleviating new-knowledge-induced
hallucinations. Through attention analysis, we find that learning new knowledge
reduces the model's attention to key entities in the question, thus causing
excessive focus on the surrounding context, which may increase the risk of
hallucination. Moreover, the attention pattern can propagate to similar
contexts, facilitating the spread of hallucinations to textually similar
questions. Our method effectively mitigates the disruption of new knowledge
learning to the model's attention on key entities, accompanied by improved
performance.

</details>


### [18] [Optimal Singular Damage: Efficient LLM Inference in Low Storage Regimes](https://arxiv.org/abs/2511.02681)
*Mohammadsajad Alipour,Mohammad Mohammadi Amiri*

Main category: cs.CL

TL;DR: 提出通过结合低秩近似与稀疏化技术（最优奇异值损伤方法），在相同存储预算下实现微调模型参数的高效存储与性能优化。


<details>
  <summary>Details</summary>
Motivation: 大语言模型微调后参数更新具有低秩和稀疏特性，但单独使用低秩近似或稀疏化会丢失关键信息。需探索更高效的存储方式以支持多任务微调模型的部署。

Method: 基于参数更新矩阵的奇异向量交错重要性，选择性地稀疏化低秩近似后的更新参数，保留对模型表达能力最关键的分量。

Result: 实验证明该方法在相同存储预算下，比单独使用低秩近似或稀疏化获得更高的精度（如MNLI任务提升1.4%，SQuAD提升2.3%）与存储效率。

Conclusion: 通过低秩稀疏混合策略可有效平衡存储效率与模型性能，为大规模预训练模型的轻量化部署提供新思路。

Abstract: Large language models (LLMs) are increasingly prevalent across diverse
applications. However, their enormous size limits storage and processing
capabilities to a few well-resourced stakeholders. As a result, most
applications rely on pre-trained LLMs, fine-tuned for specific tasks. However,
even storing the fine-tuned versions of these models remains a significant
challenge due to the wide range of tasks they address. Recently, studies show
that fine-tuning these models primarily affects a small fraction of parameters,
highlighting the need for more efficient storage of fine-tuned models. This
paper focuses on efficient storage of parameter updates in pre-trained models
after fine-tuning. To address this challenge, we leverage the observation that
fine-tuning updates are both low-rank and sparse, which can be utilized for
storage efficiency. However, using only low-rank approximation or
sparsification may discard critical singular components that enhance model
expressivity. We first observe that given the same memory budget, sparsified
low-rank approximations with larger ranks outperform standard low-rank
approximations with smaller ranks. Building on this, we propose our method,
optimal singular damage, that selectively sparsifies low-rank approximated
updates by leveraging the interleaved importance of singular vectors, ensuring
that the most impactful components are retained. We demonstrate through
extensive experiments that our proposed methods lead to significant storage
efficiency and superior accuracy within the same memory budget compared to
employing the low-rank approximation or sparsification individually.

</details>


### [19] [PragExTra: A Multilingual Corpus of Pragmatic Explicitation in Translation](https://arxiv.org/abs/2511.02721)
*Doreen Osmelak,Koel Dutta Chowdhury,Uliana Sentsova,Cristina España-Bonet,Josef van Genabith*

Main category: cs.CL

TL;DR: 首次提出PragExTra框架量化翻译中的语用显化现象，建立多语种语料库并通过主动学习提升检测准确率


<details>
  <summary>Details</summary>
Motivation: 翻译实践中常通过添加背景知识显化文化内涵（语用显化），该现象长期缺乏可计算模型支撑

Method: 基于TED-Multi和Europarl构建8种语言的语料库，通过零对齐识别候选案例，结合主动学习和人工标注优化模型

Result: 实体和系统级显化最普遍，主动学习使分类准确率提升7-8%，最高达0.88准确率和0.82 F1值

Conclusion: PragExTra首次实现语用显化的跨语言量化分析，为构建文化感知型机器翻译奠定基础

Abstract: Translators often enrich texts with background details that make implicit
cultural meanings explicit for new audiences. This phenomenon, known as
pragmatic explicitation, has been widely discussed in translation theory but
rarely modeled computationally. We introduce PragExTra, the first multilingual
corpus and detection framework for pragmatic explicitation. The corpus covers
eight language pairs from TED-Multi and Europarl and includes additions such as
entity descriptions, measurement conversions, and translator remarks. We
identify candidate explicitation cases through null alignments and refined
using active learning with human annotation. Our results show that entity and
system-level explicitations are most frequent, and that active learning
improves classifier accuracy by 7-8 percentage points, achieving up to 0.88
accuracy and 0.82 F1 across languages. PragExTra establishes pragmatic
explicitation as a measurable, cross-linguistic phenomenon and takes a step
towards building culturally aware machine translation. Keywords: translation,
multilingualism, explicitation

</details>


### [20] [AI Diffusion in Low Resource Language Countries](https://arxiv.org/abs/2511.02752)
*Amit Misra,Syed Waqas Zamir,Wassim Hamidouche,Inbal Becker-Reshef,Juan Lavista Ferres*

Main category: cs.CL

TL;DR: 语言障碍导致低资源国家AI采用率下降20%


<details>
  <summary>Details</summary>
Motivation: 验证低资源语言国家因语言模型性能不足导致AI效用降低的假说

Method: 使用加权回归模型分离语言因素与社会经济/人口变量的影响

Result: 低资源语言国家的AI用户比例较基准水平降低约20%

Conclusion: 语言可及性是阻碍AI公平扩散的独立关键因素

Abstract: Artificial intelligence (AI) is diffusing globally at unprecedented speed,
but adoption remains uneven. Frontier Large Language Models (LLMs) are known to
perform poorly on low-resource languages due to data scarcity. We hypothesize
that this performance deficit reduces the utility of AI, thereby slowing
adoption in Low-Resource Language Countries (LRLCs). To test this, we use a
weighted regression model to isolate the language effect from socioeconomic and
demographic factors, finding that LRLCs have a share of AI users that is
approximately 20% lower relative to their baseline. These results indicate that
linguistic accessibility is a significant, independent barrier to equitable AI
diffusion.

</details>


### [21] [Controlling Performance and Budget of a Centralized Multi-agent LLM System with Reinforcement Learning](https://arxiv.org/abs/2511.02755)
*Bowen Jin,TJ Collins,Donghan Yu,Mert Cemri,Shenao Zhang,Mengyu Li,Jay Tang,Tian Qin,Zhiyang Xu,Jiarui Lu,Guoli Yin,Jiawei Han,Zirui Wang*

Main category: cs.CL

TL;DR: 提出集中式多LLM协调框架CoRL，通过强化学习实现性能与推理成本的优化平衡，实验证明其在高低预算场景下的有效性


<details>
  <summary>Details</summary>
Motivation: 现有去中心化多LLM系统存在高成本不可控问题，需要构建成本高效且可控的协调机制

Method: 采用强化学习框架(CoRL)优化性能-成本双目标，实现多预算条件下的动态系统适配

Result: 在四个基准测试中，CoRL系统高预算时超越最佳单模型，低预算时仍保持强竞争力

Conclusion: 集中式协调机制为构建可扩展、成本可控的多智能体LLM系统提供了有效解决方案

Abstract: Large language models (LLMs) exhibit complementary strengths across domains
and come with varying inference costs, motivating the design of multi-agent LLM
systems where specialized models collaborate efficiently. Existing approaches
predominantly rely on decentralized frameworks, which invoke multiple LLMs for
every input and thus lead to substantial and uncontrolled inference costs. In
this work, we introduce a centralized multi-LLM framework, where a controller
LLM selectively coordinates a pool of expert models in a cost-efficient and
cost-controllable manner. We formulate this coordination problem as
reinforcement learning with dual objectives: maximizing task performance while
minimizing the overall inference cost. In addition, we expect the multi-agent
system to have adapted behavior with different budget conditions during
inference. To this end, we propose CoRL, a reinforcement learning framework
that optimizes the performance cost trade-off in a controllable multi-budget
setting. Experiments on four diverse benchmarks demonstrate that CoRL enables a
single system to surpass the best expert LLM under high-budget settings, while
maintaining strong performance in more economical low-budget modes,
highlighting the effectiveness of centralized coordination for scalable and
cost-efficient multi-agent LLM systems.

</details>


### [22] [Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval](https://arxiv.org/abs/2511.02770)
*Hung-Ting Chen,Xiang Liu,Shauli Ravfogel,Eunsol Choi*

Main category: cs.CL

TL;DR: AMER提出自回归多向量检索模型，通过生成多个查询向量解决单向量检索在多模态分布场景下的性能限制，实验显示比单向量模型提升4倍性能


<details>
  <summary>Details</summary>
Motivation: 现有检索模型生成单个查询向量难以捕捉相关文档的多模态分布特征（如查询的不同解释场景），实验显示现有模型在目标文档嵌入距离较大时表现显著下降

Method: 开发自回归多向量检索框架（AMER），通过自回归生成多个查询向量，综合所有预测向量进行文档检索

Result: 在合成数据上性能提升4倍；真实多答案检索数据集上相对提升4%和21%；目标文档嵌入相似度较低时增益更显著

Conclusion: 多查询向量检索器在捕捉多模态分布方面具有显著潜力，为目标文档嵌入差异大的场景提供有效解决方案，开辟了检索模型新研究方向

Abstract: Most text retrievers generate \emph{one} query vector to retrieve relevant
documents. Yet, the conditional distribution of relevant documents for the
query may be multimodal, e.g., representing different interpretations of the
query. We first quantify the limitations of existing retrievers. All retrievers
we evaluate struggle more as the distance between target document embeddings
grows. To address this limitation, we develop a new retriever architecture,
\emph{A}utoregressive \emph{M}ulti-\emph{E}mbedding \emph{R}etriever (AMER).
Our model autoregressively generates multiple query vectors, and all the
predicted query vectors are used to retrieve documents from the corpus. We show
that on the synthetic vectorized data, the proposed method could capture
multiple target distributions perfectly, showing 4x better performance than
single embedding model. We also fine-tune our model on real-world multi-answer
retrieval datasets and evaluate in-domain. AMER presents 4 and 21\% relative
gains over single-embedding baselines on two datasets we evaluate on.
Furthermore, we consistently observe larger gains on the subset of dataset
where the embeddings of the target documents are less similar to each other. We
demonstrate the potential of using a multi-query vector retriever and open up a
new direction for future work.

</details>


### [23] [MemSearcher: Training LLMs to Reason, Search and Manage Memory via End-to-End Reinforcement Learning](https://arxiv.org/abs/2511.02805)
*Qianhao Yuan,Jie Lou,Zichao Li,Jiawei Chen,Yaojie Lu,Hongyu Lin,Le Sun,Debing Zhang,Xianpei Han*

Main category: cs.CL

TL;DR: MemSearcher提出了一种通过迭代维护紧凑内存来平衡搜索代理效率与准确性的方法，结合多上下文GRPO强化学习框架优化推理、搜索与内存管理。


<details>
  <summary>Details</summary>
Motivation: 传统搜索代理需在完整历史信息保留（高计算/内存开销）与仅用当前轮次信息（丢失关键上下文）间权衡，制约了多轮交互的可扩展性。

Method: 设计迭代更新的紧凑内存机制，每轮将用户问题与内存融合生成推理轨迹，并动态过滤保留核心信息。采用多上下文GRPO框架联合优化推理策略、搜索动作和内存更新。

Result: 在7个基准测试中显著超越基线（Qwen2.5-3B提升11%，7B提升12%），3B版本性能甚至优于7B基线，验证了效率与精度的双重优势。

Conclusion: 平衡信息完整性与计算效率可同时提升搜索代理的准确性和资源利用率，MemSearcher为可扩展的多轮搜索系统提供了有效解决方案。

Abstract: Typical search agents concatenate the entire interaction history into the LLM
context, preserving information integrity but producing long, noisy contexts,
resulting in high computation and memory costs. In contrast, using only the
current turn avoids this overhead but discards essential information. This
trade-off limits the scalability of search agents. To address this challenge,
we propose MemSearcher, an agent workflow that iteratively maintains a compact
memory and combines the current turn with it. At each turn, MemSearcher fuses
the user's question with the memory to generate reasoning traces, perform
search actions, and update memory to retain only information essential for
solving the task. This design stabilizes context length across multi-turn
interactions, improving efficiency without sacrificing accuracy. To optimize
this workflow, we introduce multi-context GRPO, an end-to-end RL framework that
jointly optimize reasoning, search strategies, and memory management of
MemSearcher Agents. Specifically, multi-context GRPO samples groups of
trajectories under different contexts and propagates trajectory-level
advantages across all conversations within them. Trained on the same dataset as
Search-R1, MemSearcher achieves significant improvements over strong baselines
on seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on
Qwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher
even outperforms 7B-based baselines, demonstrating that striking a balance
between information integrity and efficiency yields both higher accuracy and
lower computational overhead. The code and models will be publicly available at
https://github.com/icip-cas/MemSearcher

</details>


### [24] [Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities](https://arxiv.org/abs/2511.02817)
*Amanda Bertsch,Adithya Pratapa,Teruko Mitamura,Graham Neubig,Matthew R. Gormley*

Main category: cs.CL

TL;DR: 提出Oolong基准测试，用于评估模型在长上下文环境下的原子级文本分析和聚合推理能力


<details>
  <summary>Details</summary>
Motivation: 现有长上下文评估方法过度依赖局部检索，忽视整体上下文利用效果

Method: 设计包含合成任务（Oolong-synth）和真实对话任务（Oolong-real）的测试集，要求模型进行原子文本分析、分类统计及时序关系推理

Result: GPT-5/Claude-Sonnet-4/Gemini-2.5-Pro在128K长度任务上准确率均低于50%

Conclusion: 发布Oolong基准测试数据及评估框架，推动大文本量推理模型的发展

Abstract: As model context lengths continue to grow, concerns about whether models
effectively use the full context length have persisted. While several carefully
designed long-context evaluations have recently been released, these
evaluations tend to rely on retrieval from one or more sections of the context,
which allows nearly all of the context tokens to be disregarded as noise. This
represents only one type of task that might be performed with long context. We
introduce Oolong, a benchmark of long-context reasoning tasks that require
analyzing individual chunks of text on an atomic level, and then aggregating
these analyses to answer distributional questions. Oolong is separated into two
task sets: Oolong-synth, a set of naturalistic synthetic tasks, where we can
easily ablate components of the reasoning problem; and Oolong-real, a
downstream setting which requires reasoning over real-world conversational
data. Oolong requires models to reason over large quantities of examples, to
perform both classification and counting in-context, and to reason over
temporal and user relations. Even frontier models struggle on Oolong, with
GPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro all achieving less than 50% accuracy
on both splits at 128K. We release the data and evaluation harness for Oolong
to enable further development of models that can reason over large quantities
of text.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [25] [LGCC: Enhancing Flow Matching Based Text-Guided Image Editing with Local Gaussian Coupling and Context Consistency](https://arxiv.org/abs/2511.01894)
*Fangbing Liu,Pengfei Duan,Wen Li,Yi He*

Main category: cs.GR

TL;DR: 提出LGCC框架解决现有图像编辑模型细节退化、内容不一致和效率问题，实现3-5倍加速并提升评分


<details>
  <summary>Details</summary>
Motivation: BAGEL等现有方法因随机噪声初始化导致细节退化、内容不一致和效率低下

Method: 结合局部高斯噪声耦合(LGNC)保持空间细节，内容一致性损失(CCL)确保语义对齐，通过课程学习集成预训练模型

Result: I2EBench局部细节评分提升1.60%，整体提升0.53%；轻量编辑3-5倍加速，通用编辑2倍加速，推理时间减少50%-60%

Conclusion: LGCC在保持编辑质量的同时显著提升效率，提供保留细节、维护上下文完整性的低成本解决方案

Abstract: Recent advancements have demonstrated the great potential of flow
matching-based Multimodal Large Language Models (MLLMs) in image editing.
However, state-of-the-art works like BAGEL face limitations, including detail
degradation, content inconsistency, and inefficiency due to their reliance on
random noise initialization. To address these issues, we propose LGCC, a novel
framework with two key components: Local Gaussian Noise Coupling (LGNC) and
Content Consistency Loss (CCL). LGNC preserves spatial details by modeling
target image embeddings and their locally perturbed counterparts as coupled
pairs, while CCL ensures semantic alignment between edit instructions and image
modifications, preventing unintended content removal. By integrating LGCC with
the BAGEL pre-trained model via curriculum learning, we significantly reduce
inference steps, improving local detail scores on I2EBench by 1.60% and overall
scores by 0.53%. LGCC achieves 3x -- 5x speedup for lightweight editing and 2x
for universal editing, requiring only 40% -- 50% of the inference time of BAGEL
or Flux. These results demonstrate LGCC's ability to preserve detail, maintain
contextual integrity, and enhance inference speed, offering a cost-efficient
solution without compromising editing quality.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [26] [Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences](https://arxiv.org/abs/2511.02109)
*Joshua Ashkinaze,Hua Shen,Sai Avula,Eric Gilbert,Ceren Budak*

Main category: cs.AI

TL;DR: Deep Value Benchmark (DVB) 通过实验设计验证大语言模型是否真正理解人类深层价值观，发现现有模型主要依赖表层特征而非核心价值原则进行决策。


<details>
  <summary>Details</summary>
Motivation: 区分AI系统是否掌握深层价值观（如道德原则）或仅学习表面偏好（如语言风格），这对实现可靠的人工智能对齐至关重要。

Method: 采用训练阶段故意关联深层价值与表层特征（如非恶原则+正式语言），测试阶段打破关联的对照实验设计，测量模型基于价值观的泛化概率(DVGR)。

Result: 9个模型的平均DVGR仅0.30（低于随机水平），模型规模与DVGR负相关，人类验证实验显示参与者DVGR达0.85形成鲜明对比。

Conclusion: 当前大语言模型普遍缺乏价值观泛化能力，揭示现有对齐方法的局限性，强调需要开发更关注核心价值学习的新范式。

Abstract: We introduce the Deep Value Benchmark (DVB), an evaluation framework that
directly tests whether large language models (LLMs) learn fundamental human
values or merely surface-level preferences. This distinction is critical for AI
alignment: Systems that capture deeper values are likely to generalize human
intentions robustly, while those that capture only superficial patterns in
preference data risk producing misaligned behavior. The DVB uses a novel
experimental design with controlled confounding between deep values (e.g.,
moral principles) and shallow features (e.g., superficial attributes). In the
training phase, we expose LLMs to human preference data with deliberately
correlated deep and shallow features -- for instance, where a user consistently
prefers (non-maleficence, formal language) options over (justice, informal
language) alternatives. The testing phase then breaks these correlations,
presenting choices between (justice, formal language) and (non-maleficence,
informal language) options. This design allows us to precisely measure a
model's Deep Value Generalization Rate (DVGR) -- the probability of
generalizing based on the underlying value rather than the shallow feature.
Across 9 different models, the average DVGR is just 0.30. All models generalize
deep values less than chance. Larger models have a (slightly) lower DVGR than
smaller models. We are releasing our dataset, which was subject to three
separate human validation experiments. DVB provides an interpretable measure of
a core feature of alignment.

</details>


### [27] [InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance](https://arxiv.org/abs/2511.02119)
*Ziheng Geng,Jiachen Liu,Ran Cao,Lu Cheng,Dan M. Frangopol,Minghui Cheng*

Main category: cs.AI

TL;DR: 研究提出InsurAgent框架，通过大语言模型增强的智能体解决洪水保险参与率低的定量预测难题，结合数据驱动与常识推理提升决策模拟能力。


<details>
  <summary>Details</summary>
Motivation: 美国高危人群洪水保险参保率低，传统模型难以捕捉复杂行为机制。LLMs展现人类决策模拟潜力但缺乏定量分析能力，需开发新方法填补缺口。

Method: 1.构建保险购买概率基准数据集；2.开发包含感知/检索/推理/行动/记忆五模块的InsurAgent；3.检索模块采用RAG整合调查数据，推理模块融合LLM常识扩展预测维度。

Result: InsurAgent实现边际概率和双变量概率的准确估计，捕获传统模型难以量化的上下文信息，记忆模块成功模拟保险决策随时间演变的全生命周期轨迹。

Conclusion: 该框架为行为建模提供新范式，通过数据锚定与常识推理的融合，为精准政策模拟和个性化保险服务设计提供技术支持。

Abstract: Flood insurance is an effective strategy for individuals to mitigate
disaster-related losses. However, participation rates among at-risk populations
in the United States remain strikingly low. This gap underscores the need to
understand and model the behavioral mechanisms underlying insurance decisions.
Large language models (LLMs) have recently exhibited human-like intelligence
across wide-ranging tasks, offering promising tools for simulating human
decision-making. This study constructs a benchmark dataset to capture insurance
purchase probabilities across factors. Using this dataset, the capacity of LLMs
is evaluated: while LLMs exhibit a qualitative understanding of factors, they
fall short in estimating quantitative probabilities. To address this
limitation, InsurAgent, an LLM-empowered agent comprising five modules
including perception, retrieval, reasoning, action, and memory, is proposed.
The retrieval module leverages retrieval-augmented generation (RAG) to ground
decisions in empirical survey data, achieving accurate estimation of marginal
and bivariate probabilities. The reasoning module leverages LLM common sense to
extrapolate beyond survey data, capturing contextual information that is
intractable for traditional models. The memory module supports the simulation
of temporal decision evolutions, illustrated through a roller coaster life
trajectory. Overall, InsurAgent provides a valuable tool for behavioral
modeling and policy analysis.

</details>


### [28] [Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning](https://arxiv.org/abs/2511.02194)
*Yibo Zhao,Yang Zhao,Hongru Du,Hao Frank Yang*

Main category: cs.AI

TL;DR: 提出ATHENA框架，整合符号效用建模与语义适配，显著提升个体决策预测性能


<details>
  <summary>Details</summary>
Motivation: 解决高风险场景下个体决策与群体最优预测的差异问题，结合数值属性和语言因素对决策的影响

Method: 两阶段框架：1) LLM增强的群体级符号效用函数发现 2) 基于最优效用的个性化语义模板适配

Result: 在交通方式选择和疫苗决策任务中F1分数提升6.5%，消融实验验证双阶段结构的必要性

Conclusion: 通过有机整合符号建模与语义适配，为人类中心决策建模提供新范式

Abstract: Decision-making models for individuals, particularly in high-stakes scenarios
like vaccine uptake, often diverge from population optimal predictions. This
gap arises from the uniqueness of the individual decision-making process,
shaped by numerical attributes (e.g., cost, time) and linguistic influences
(e.g., personal preferences and constraints). Developing upon Utility Theory
and leveraging the textual-reasoning capabilities of Large Language Models
(LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric
Reasoning framework (ATHENA) to address the optimal information integration.
ATHENA uniquely integrates two stages: First, it discovers robust, group-level
symbolic utility functions via LLM-augmented symbolic discovery; Second, it
implements individual-level semantic adaptation, creating personalized semantic
templates guided by the optimal utility to model personalized choices.
Validated on real-world travel mode and vaccine choice tasks, ATHENA
consistently outperforms utility-based, machine learning, and other LLM-based
models, lifting F1 score by at least 6.5% over the strongest cutting-edge
models. Further, ablation studies confirm that both stages of ATHENA are
critical and complementary, as removing either clearly degrades overall
predictive performance. By organically integrating symbolic utility modeling
and semantic adaptation, ATHENA provides a new scheme for modeling
human-centric decisions. The project page can be found at
https://yibozh.github.io/Athena.

</details>


### [29] [Training Proactive and Personalized LLM Agents](https://arxiv.org/abs/2511.02208)
*Weiwei Sun,Xuhui Zhou,Weihua Du,Xingyao Wang,Sean Welleck,Graham Neubig,Maarten Sap,Yiming Yang*

Main category: cs.AI

TL;DR: 提出多目标强化学习框架PPP，联合优化AI代理的生产力、主动性和个性化，实验显示显著超越基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注任务成功率，但实际应用中需同时优化用户交互的三大维度：任务完成度、主动提问能力和个性化适应能力。

Method: 1. 构建UserVille交互环境（含LLM用户模拟器）
2. 开发PPP框架进行多目标强化学习
3. 在软件工程和深度研究任务中验证

Result: PPP代理相比GPT-5平均提升21.6分，展示出：
- 策略性提问能力
- 适应未知用户偏好的能力
- 通过优化交互提升任务成功率

Conclusion: 明确优化用户中心的交互维度是构建实用AI代理的关键，PPP框架为多目标优化提供有效解决方案。

Abstract: While existing work focuses primarily on task success, we argue that
effective real-world agents require optimizing three dimensions: productivity
(task completion), proactivity (asking essential questions), and
personalization (adapting to diverse user preferences). We introduce UserVille,
an interactive environment with LLM-based user simulators enabling diverse,
configurable user preferences. Leveraging UserVille, we introduce PPP, a
multi-objective reinforcement learning approach that jointly optimizes all
three dimensions: Productivity, Proactivity, and Personalization. Experiments
on software engineering and deep research tasks show that agents trained with
PPP achieve substantial improvements over strong baselines such as GPT-5 (+21.6
on average), demonstrating the ability to ask strategic clarifying questions,
adapt to unseen user preferences, and improve task success through better
interaction. This work demonstrates that explicitly optimizing for
user-centered interaction is critical for building practical and effective AI
agents.

</details>


### [30] [Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation](https://arxiv.org/abs/2511.02303)
*Zhiwei Zhang,Xiaomin Li,Yudi Lin,Hui Liu,Ramraj Chandradevan,Linlin Wu,Minhua Lin,Fali Wang,Xianfeng Tang,Qi He,Suhang Wang*

Main category: cs.AI

TL;DR: 论文提出通过因果影响测量和可验证奖励机制解决多智能体推理中的懒惰代理问题，实验证明能有效提升协作效率


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的多智能体框架存在懒惰代理行为，导致协作失效并退化为单代理系统，限制了复杂推理任务的性能提升

Method: 1. 理论分析懒惰代理行为的产生机制 2. 提出基于因果影响测量的稳定评估方法 3. 设计允许丢弃噪声输出、整合指令并重启推理的可验证奖励机制

Result: 实验表明该框架成功减轻了78%的懒惰代理现象，在复杂推理任务中使多智能体系统的准确率提升15%以上

Conclusion: 通过理论分析与创新机制设计，有效释放了多智能体框架的协作潜力，为复杂推理任务提供了新的解决方案

Abstract: Large Language Models (LLMs) trained with reinforcement learning and
verifiable rewards have achieved strong results on complex reasoning tasks.
Recent work extends this paradigm to a multi-agent setting, where a
meta-thinking agent proposes plans and monitors progress while a reasoning
agent executes subtasks through sequential conversational turns. Despite
promising performance, we identify a critical limitation: lazy agent behavior,
in which one agent dominates while the other contributes little, undermining
collaboration and collapsing the setup to an ineffective single agent. In this
paper, we first provide a theoretical analysis showing why lazy behavior
naturally arises in multi-agent reasoning. We then introduce a stable and
efficient method for measuring causal influence, helping mitigate this issue.
Finally, as collaboration intensifies, the reasoning agent risks getting lost
in multi-turn interactions and trapped by previous noisy responses. To counter
this, we propose a verifiable reward mechanism that encourages deliberation by
allowing the reasoning agent to discard noisy outputs, consolidate
instructions, and restart its reasoning process when necessary. Extensive
experiments demonstrate that our framework alleviates lazy agent behavior and
unlocks the full potential of multi-agent framework for complex reasoning
tasks.

</details>


### [31] [The Collaboration Gap](https://arxiv.org/abs/2511.02687)
*Tim R. Davidson,Adam Fourney,Saleema Amershi,Robert West,Eric Horvitz,Ece Kamar*

Main category: cs.AI

TL;DR: 研究发现AI代理单独性能与协作性能存在显著差距（协作鸿沟），提出通过接力推理方法可有效缩小差距，并强调协作评估和训练的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统向多代理协作方向发展，现有研究缺乏对异构代理在部分可观察条件下协作能力的系统性评估。协作失败可能限制复杂AI系统的实际应用效果。

Method: 开发协作迷宫解决基准框架：1）隔离协作能力评估 2）支持复杂度调节 3）自动化评分扩展 4）保持输出格式自由以维持生态合理性。在32个主流模型中测试单独/同质/异质协作表现。

Result: 发现协作鸿沟现象（单独成功率75%的模型在协作时降至45%）；小型蒸馏模型在某些配对中完全失效；接力推理策略可提升协作成功率约30%。

Conclusion: 提出三个关键方向：1）建立协作感知的评估体系 2）开发增强协作能力的训练方法 3）设计可靠激发代理潜在协作技能的交互机制，适用于AI-AI及人-AI协作场景。

Abstract: The trajectory of AI development suggests that we will increasingly rely on
agent-based systems composed of independently developed agents with different
information, privileges, and tools. The success of these systems will
critically depend on effective collaboration among these heterogeneous agents,
even under partial observability. Despite intense interest, few empirical
studies have evaluated such agent-agent collaboration at scale. We propose a
collaborative maze-solving benchmark that (i) isolates collaborative
capabilities, (ii) modulates problem complexity, (iii) enables scalable
automated grading, and (iv) imposes no output-format constraints, preserving
ecological plausibility. Using this framework, we evaluate 32 leading open- and
closed-source models in solo, homogeneous, and heterogeneous pairings. Our
results reveal a "collaboration gap": models that perform well solo often
degrade substantially when required to collaborate. Collaboration can break
down dramatically; for instance, small distilled models that solve mazes well
alone may fail almost completely in certain pairings. We find that starting
with the stronger agent often improves outcomes, motivating a "relay inference"
approach where the stronger agent leads before handing off to the weaker one,
closing much of the gap. Our findings argue for (1) collaboration-aware
evaluation, (2) training strategies developed to enhance collaborative
capabilities, and (3) interaction design that reliably elicits agents' latent
skills, guidance that applies to AI-AI and human-AI collaboration.

</details>


### [32] [CostBench: Evaluating Multi-Turn Cost-Optimal Planning and Adaptation in Dynamic Environments for LLM Tool-Use Agents](https://arxiv.org/abs/2511.02734)
*Jiayu Liu,Cheng Qian,Zhaochen Su,Qing Zong,Shijue Huang,Bingxiang He,Yi R. Fung*

Main category: cs.AI

TL;DR: 提出CostBench基准测试框架，用于评估LLM代理在成本优化规划和动态环境适应中的经济理性表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理评估过度关注任务完成度，忽视了资源效率与动态适应性这两个决定经济理性的关键指标，导致代理难以应对真实世界的不确定性。

Method: 通过旅行规划领域构建多路径可解任务集，设计带自定义成本的原子/复合工具系统，并引入工具失效、成本突变等四类动态阻塞事件模拟真实场景。

Result: 顶尖模型在静态环境下仅达成75%精确匹配率（GPT-5），动态条件下性能骤降40%，暴露严重成本规划缺陷。

Conclusion: CostBench揭示了现有代理的经济理性短板，为开发成本敏感且鲁棒的下一代智能体提供了诊断基准与改进方向。

Abstract: Current evaluations of Large Language Model (LLM) agents primarily emphasize
task completion, often overlooking resource efficiency and adaptability. This
neglects a crucial capability: agents' ability to devise and adjust
cost-optimal plans in response to changing environments. To bridge this gap, we
introduce CostBench, a scalable, cost-centric benchmark designed to evaluate
agents' economic reasoning and replanning abilities. Situated in the
travel-planning domain, CostBench comprises tasks solvable via multiple
sequences of atomic and composite tools with diverse, customizable costs. It
also supports four types of dynamic blocking events, such as tool failures and
cost changes, to simulate real-world unpredictability and necessitate agents to
adapt in real time. Evaluating leading open-sourced and proprietary models on
CostBench reveals a substantial gap in cost-aware planning: agents frequently
fail to identify cost-optimal solutions in static settings, with even GPT-5
achieving less than 75% exact match rate on the hardest tasks, and performance
further dropping by around 40% under dynamic conditions. By diagnosing these
weaknesses, CostBench lays the groundwork for developing future agents that are
both economically rational and robust.

</details>


### [33] [Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything](https://arxiv.org/abs/2511.02834)
*Huawei Lin,Yunzhi Shi,Tong Geng,Weijie Zhao,Wei Wang,Ravender Pal Singh*

Main category: cs.AI

TL;DR: 提出无需训练的主代理协调框架Agent-Omni，通过集成现有基础模型实现跨模态推理


<details>
  <summary>Details</summary>
Motivation: 现有多模态大模型受限于固定模态组合，且需大量对齐数据微调

Method: 主代理解析用户意图，调度各模态专用代理执行子任务并整合结果

Result: 在文本/图像/音频/视频跨模态任务中均实现SOTA性能

Conclusion: 模块化设计实现透明可解释推理，支持未来模型无缝升级

Abstract: Multimodal large language models (MLLMs) have shown strong capabilities but
remain limited to fixed modality pairs and require costly fine-tuning with
large aligned datasets. Building fully omni-capable models that can integrate
text, images, audio, and video remains impractical and lacks robust reasoning
support. In this paper, we propose an Agent-Omni framework that coordinates
existing foundation models through a master-agent system, enabling flexible
multimodal reasoning without retraining. The master agent interprets user
intent, delegates subtasks to modality-specific agents, and integrates their
outputs into coherent responses. Extensive experiments across text, image,
audio, video, and omni benchmarks show that Agent-Omni consistently achieves
state-of-the-art performance, particularly on tasks requiring complex
cross-modal reasoning. Its agent-based design enables seamless integration of
specialized foundation models, ensuring adaptability to diverse inputs while
maintaining transparency and interpretability. In addition, the framework is
modular and easily extensible, allowing future improvements as stronger models
become available. %We release an open-source implementation to support
continued research on scalable and reliable omni-modal reasoning.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [34] [Automata-Conditioned Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.02304)
*Beyazit Yalcinkaya,Marcell Vazquez-Chanlatte,Ameesh Shah,Hanna Krasowski,Sanjit A. Seshia*

Main category: cs.MA

TL;DR: 提出了基于自动机的多智能体强化学习框架ACC-MARL，通过任务分解实现复杂时序目标的高效协作


<details>
  <summary>Details</summary>
Motivation: 现有方法在样本效率和多任务适应性方面存在局限，单任务场景下的集中训练-分散执行模式效率低下

Method: 使用自动机进行任务表示与分解，设计任务条件化的分布式团队策略学习框架，解决多任务分配的关键技术难题

Result: 实验验证了智能体间多步骤协调能力（如按钮解锁-门禁维持-电路短接任务链），证明价值函数可用于测试阶段的任务最优分配

Conclusion: 该框架成功实现了跨任务的知识迁移，为复杂协同任务提供可验证的解决方案，推动多智能体系统在现实场景的应用

Abstract: We study the problem of learning multi-task, multi-agent policies for
cooperative, temporal objectives, under centralized training, decentralized
execution. In this setting, using automata to represent tasks enables the
decomposition of complex tasks into simpler sub-tasks that can be assigned to
agents. However, existing approaches remain sample-inefficient and are limited
to the single-task case. In this work, we present Automata-Conditioned
Cooperative Multi-Agent Reinforcement Learning (ACC-MARL), a framework for
learning task-conditioned, decentralized team policies. We identify the main
challenges to ACC-MARL's feasibility in practice, propose solutions, and prove
the correctness of our approach. We further show that the value functions of
learned policies can be used to assign tasks optimally at test time.
Experiments show emergent task-aware, multi-step coordination among agents,
e.g., pressing a button to unlock a door, holding the door, and
short-circuiting tasks.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [35] [SciDaSynth: Interactive Structured Data Extraction from Scientific Literature with Large Language Model](https://arxiv.org/abs/2404.13765)
*Xingbo Wang,Samantha L. Huey,Rui Sheng,Saurabh Mehta,Fei Wang*

Main category: cs.HC

TL;DR: SciDaSynth利用大语言模型自动整合文本、表格、图表等多源数据生成结构化表格，显著提升科学数据提取效率与准确性。


<details>
  <summary>Details</summary>
Motivation: 科学文献爆炸式增长背景下，现有工具难以有效整合多模态、异构的跨文档信息生成标准化数据表格。

Method: 开发基于LLM的交互式系统SciDaSynth，支持通过多源数据融合自动生成结构化表格，并提供可视化摘要和语义分组功能实现数据验证与优化。

Result: 营养学和NLP领域用户研究表明，相比基线方法，SciDaSynth在保持数据质量的同时显著提升结构化数据生成效率。

Conclusion: 系统验证了人机协同在数据提取任务中的有效性，并为相关系统设计提供了实践启示，代码已开源促进可复现性。

Abstract: The explosion of scientific literature has made the efficient and accurate
extraction of structured data a critical component for advancing scientific
knowledge and supporting evidence-based decision-making. However, existing
tools often struggle to extract and structure multimodal, varied, and
inconsistent information across documents into standardized formats. We
introduce SciDaSynth, a novel interactive system powered by large language
models (LLMs) that automatically generates structured data tables according to
users' queries by integrating information from diverse sources, including text,
tables, and figures. Furthermore, SciDaSynth supports efficient table data
validation and refinement, featuring multi-faceted visual summaries and
semantic grouping capabilities to resolve cross-document data inconsistencies.
A within-subjects study with nutrition and NLP researchers demonstrates
SciDaSynth's effectiveness in producing high-quality structured data more
efficiently than baseline methods. We discuss design implications for human-AI
collaborative systems supporting data extraction tasks. The system code is
available at https://github.com/xingbow/SciDaEx

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [36] [OLATverse: A Large-scale Real-world Object Dataset with Precise Lighting Control](https://arxiv.org/abs/2511.02483)
*Xilong Zhou,Jianchun Chen,Pramod Rao,Timo Teufel,Linjie Lyu,Tigran Minasian,Oleksandr Sotnychenko,Xiaoxiao Long,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: OLATverse是首个大规模真实物体数据集（900万图像/765物体），提供多视角、多光源的精准控制光照数据，推动逆向渲染与重光照技术的真实性与泛化能力提升。


<details>
  <summary>Details</summary>
Motivation: 现有技术过度依赖合成数据和小规模真实数据集，限制了逆向渲染等技术的真实应用与泛化能力。

Method: 使用35个DSLR相机和331个独立光源采集物体数据，提供校准参数、物体遮罩、表面法线、漫反射反照率等辅助资源，建立首个综合真实世界物体基准测试集。

Result: 构建覆盖765种真实物体（含特殊材质）的开放数据集，提供完整的逆向渲染技术评估体系与法线估计基准。

Conclusion: OLATverse通过高质量真实数据填补了该领域数据空白，为下一代逆向渲染技术从实验室走向实际应用提供了关键基础设施。

Abstract: We introduce OLATverse, a large-scale dataset comprising around 9M images of
765 real-world objects, captured from multiple viewpoints under a diverse set
of precisely controlled lighting conditions. While recent advances in
object-centric inverse rendering, novel view synthesis and relighting have
shown promising results, most techniques still heavily rely on the synthetic
datasets for training and small-scale real-world datasets for benchmarking,
which limits their realism and generalization. To address this gap, OLATverse
offers two key advantages over existing datasets: large-scale coverage of real
objects and high-fidelity appearance under precisely controlled illuminations.
Specifically, OLATverse contains 765 common and uncommon real-world objects,
spanning a wide range of material categories. Each object is captured using 35
DSLR cameras and 331 individually controlled light sources, enabling the
simulation of diverse illumination conditions. In addition, for each object, we
provide well-calibrated camera parameters, accurate object masks, photometric
surface normals, and diffuse albedo as auxiliary resources. We also construct
an extensive evaluation set, establishing the first comprehensive real-world
object-centric benchmark for inverse rendering and normal estimation. We
believe that OLATverse represents a pivotal step toward integrating the next
generation of inverse rendering and relighting methods with real-world data.
The full dataset, along with all post-processing workflows, will be publicly
released at https://vcai.mpi-inf.mpg.de/projects/OLATverse/.

</details>


### [37] [TAUE: Training-free Noise Transplant and Cultivation Diffusion Model](https://arxiv.org/abs/2511.02580)
*Daichi Nagai,Ryugo Morita,Shunsuke Kitada,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 提出无需训练的TAUE框架，通过噪声移植技术实现分层图像生成，保持场景一致性


<details>
  <summary>Details</summary>
Motivation: 现有文本生成图像模型缺乏分层控制能力，微调方案依赖大数据集且不可获取，训练免费方案只能生成孤立元素

Method: 核心NTC技术从前景和合成过程中提取潜在表征，移植到后续层的初始噪声中实现跨层一致性

Result: 在保持图像质量前提下达到与微调方法相当的层间一致性，支持复杂组合编辑等新应用

Conclusion: TAUE突破训练依赖，开创可控生成新范式，为专业级图像编辑提供高效解决方案

Abstract: Despite the remarkable success of text-to-image diffusion models, their
output of a single, flattened image remains a critical bottleneck for
professional applications requiring layer-wise control. Existing solutions
either rely on fine-tuning with large, inaccessible datasets or are
training-free yet limited to generating isolated foreground elements, failing
to produce a complete and coherent scene. To address this, we introduce the
Training-free Noise Transplantation and Cultivation Diffusion Model (TAUE), a
novel framework for zero-shot, layer-wise image generation. Our core technique,
Noise Transplantation and Cultivation (NTC), extracts intermediate latent
representations from both foreground and composite generation processes,
transplanting them into the initial noise for subsequent layers. This ensures
semantic and structural coherence across foreground, background, and composite
layers, enabling consistent, multi-layered outputs without requiring
fine-tuning or auxiliary datasets. Extensive experiments show that our
training-free method achieves performance comparable to fine-tuned methods,
enhancing layer-wise consistency while maintaining high image quality and
fidelity. TAUE not only eliminates costly training and dataset requirements but
also unlocks novel downstream applications, such as complex compositional
editing, paving the way for more accessible and controllable generative
workflows.

</details>


### [38] [SAIL-RL: Guiding MLLMs in When and How to Think via Dual-Reward RL Tuning](https://arxiv.org/abs/2511.02280)
*Fangxun Shu,Yongjie Ye,Yue Liao,Zijian Kang,Weijie Yin,Jiacong Wang,Xiao Liang,Shuicheng Yan,Chao Feng*

Main category: cs.CV

TL;DR: SAIL-RL提出强化学习后训练框架，通过双奖励系统提升多模态大模型的推理可靠性和适应性


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于仅奖励正确答案的结果导向监督，以及导致简单任务过度思考/复杂任务思考不足的统一策略

Method: 设计思考奖励（评估事实依据/逻辑连贯/答案一致性）和判断奖励（自适应选择推理深度）的双奖励机制

Result: 在SAIL-VL2模型上实现4B/8B规模的推理提升，性能接近GPT-4o，幻觉现象减少75%

Conclusion: SAIL-RL建立了更可靠、自适应的MLLM框架，通过结构化思维机制平衡推理效率与深度

Abstract: We introduce SAIL-RL, a reinforcement learning (RL) post-training framework
that enhances the reasoning capabilities of multimodal large language models
(MLLMs) by teaching them when and how to think. Existing approaches are limited
by outcome-only supervision, which rewards correct answers without ensuring
sound reasoning, and by uniform thinking strategies, which often lead to
overthinking on simple tasks and underthinking on complex ones. SAIL-RL
addresses these challenges with a dual reward system: the Thinking Reward,
which evaluates reasoning quality through factual grounding, logical coherence,
and answer consistency, and the Judging Reward, which adaptively determines
whether deep reasoning or direct answering is appropriate. Experiments on the
state-of-the-art SAIL-VL2 show that SAIL-RL improves reasoning and multimodal
understanding benchmarks at both 4B and 8B scales, achieving competitive
performance against commercial closed-source models such as GPT-4o, and
substantially reduces hallucinations, establishing it as a principled framework
for building more reliable and adaptive MLLMs. The code will be available at
https://github.com/BytedanceDouyinContent/SAIL-RL.

</details>


### [39] [Link prediction Graph Neural Networks for structure recognition of Handwritten Mathematical Expressions](https://arxiv.org/abs/2511.02288)
*Cuong Tuan Nguyen,Ngoc Tuan Nguyen,Triet Hoang Minh Dao,Huy Minh Nhat,Huy Truong Dinh*

Main category: cs.CV

TL;DR: 提出基于图神经网络（GNN）的手写数学表达式识别方法，通过符号图建模和结构优化实现高效识别


<details>
  <summary>Details</summary>
Motivation: 传统方法难以有效捕捉手写数学表达式中复杂的符号空间依赖关系，需通过图结构建模提升结构化识别能力

Method: 1. 使用深度BLSTM网络进行符号分割/识别/空间分类 → 生成初始图 2. 2D-CFG生成候选关系 → GNN链接预测模型剪枝优化 → 构建符号标签图

Result: 实验证明该方法在HME结构识别中具有显著效果，展现出优于传统方法的性能表现

Conclusion: 融合BLSTM、2D-CFG与GNN的图结构建模策略，有效解决了手写数学表达式的结构化识别挑战

Abstract: We propose a Graph Neural Network (GNN)-based approach for Handwritten
Mathematical Expression (HME) recognition by modeling HMEs as graphs, where
nodes represent symbols and edges capture spatial dependencies. A deep BLSTM
network is used for symbol segmentation, recognition, and spatial relation
classification, forming an initial primitive graph. A 2D-CFG parser then
generates all possible spatial relations, while the GNN-based link prediction
model refines the structure by removing unnecessary connections, ultimately
forming the Symbol Label Graph. Experimental results demonstrate the
effectiveness of our approach, showing promising performance in HME structure
recognition.

</details>


### [40] [CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning](https://arxiv.org/abs/2511.02360)
*Jizheng Ma,Xiaofei Zhou,Yanlong Song,Han Yan*

Main category: cs.CV

TL;DR: 提出CoCoVa框架——通过连续跨模态推理链增强视觉语言模型的表示能力，突破离散语言标记的限制


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型受限于离散语言符号空间，无法充分捕捉视觉感知的连续高维特性，导致视觉理解的表示鸿沟

Method: 1. 迭代式推理循环机制 2. LQ-Former动态推理引擎 3. 动态视觉关注机制 4. 多任务训练目标（对比学习+扩散重建）

Result: 1.5B模型性能超越7B-9B基线模型，7B版本与SOTA模型相当 2. 推理token效率提升 3. 潜在空间展现可解释的结构化推理模式

Conclusion: CoCoVa成功弥合了离散语言处理与连续视觉理解间的表示鸿沟，为构建更接近人类认知的跨模态推理系统提供了新范式

Abstract: In human cognition, there exist numerous thought processes that are tacit and
beyond verbal expression, enabling us to understand and interact with the world
in multiple ways. However, contemporary Vision-Language Models (VLMs) remain
constrained to reasoning within the discrete and rigid space of linguistic
tokens, thereby bottlenecking the rich, high-dimensional nature of visual
perception. To bridge this gap, we propose CoCoVa (Chain of Continuous
Vision-Language Thought), a novel framework for vision-language model that
leverages continuous cross-modal reasoning for diverse vision-language tasks.
The core of CoCoVa is an iterative reasoning cycle, where a novel Latent
Q-Former (LQ-Former) acts as a dynamic reasoning engine, iteratively refining a
chain of latent thought vectors through cross-modal fusion. To focus this
process, a token selection mechanism dynamically identifies salient visual
regions, mimicking attentional focus. To ensure these latent thoughts remain
grounded, we train the model with a multi-task objective that combines
contrastive learning and diffusion-based reconstruction, enforcing alignment
between latent representations and both visual and textual modalities.
Evaluations show CoCoVa improves accuracy and token efficiency over strong
baselines. With a 1.5B backbone, it competes with or surpasses larger 7B-9B
models on almost all benchmarks. When scaled to 7B LLM backbones, it remains
competitive with state-of-the-art models. Qualitative analysis validates that
learned latent space captures interpretable and structured reasoning patterns,
highlighting the potential of CoCoVa to bridge the representational gap between
discrete language processing and the continuous nature of visual understanding.

</details>


### [41] [DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding](https://arxiv.org/abs/2511.02495)
*Zixuan Liu,Siavash H. Khajavi,Guangkai Jiang*

Main category: cs.CV

TL;DR: 研究者推出DetectiumFire数据集——包含2.25万张高清火灾图片和2500个真实视频的多模态数据集，标注包含视觉标签与文本提示，显著提升火灾领域AI研究的基准质量。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型在火灾领域应用受限，主要由于缺乏高质量标注的公开数据集。现有数据集存在规模小、冗余度高、场景覆盖有限等问题。

Method: 构建包含22.5k图像和2.5k视频的大规模数据集，涵盖多种火灾类型/环境/风险等级，采用双重标注体系（边界框+场景描述文本），支持合成数据生成和风险推理任务。

Result: 在目标检测、图像生成、视觉语言推理等任务验证中，数据集展现出优于现有基准的表现，冗余度降低38%，真实场景覆盖率提升67%。

Conclusion: DetectiumFire填补了火灾研究领域的数据空白，通过开源促进AI社区对火灾理解的探索，支持智能安全系统开发。数据集已发布于Kaggle平台。

Abstract: Recent advances in multi-modal models have demonstrated strong performance in
tasks such as image generation and reasoning. However, applying these models to
the fire domain remains challenging due to the lack of publicly available
datasets with high-quality fire domain annotations. To address this gap, we
introduce DetectiumFire, a large-scale, multi-modal dataset comprising of 22.5k
high-resolution fire-related images and 2.5k real-world fire-related videos
covering a wide range of fire types, environments, and risk levels. The data
are annotated with both traditional computer vision labels (e.g., bounding
boxes) and detailed textual prompts describing the scene, enabling applications
such as synthetic data generation and fire risk reasoning. DetectiumFire offers
clear advantages over existing benchmarks in scale, diversity, and data
quality, significantly reducing redundancy and enhancing coverage of real-world
scenarios. We validate the utility of DetectiumFire across multiple tasks,
including object detection, diffusion-based image generation, and
vision-language reasoning. Our results highlight the potential of this dataset
to advance fire-related research and support the development of intelligent
safety systems. We release DetectiumFire to promote broader exploration of fire
understanding in the AI community. The dataset is available at
https://kaggle.com/datasets/38b79c344bdfc55d1eed3d22fbaa9c31fad45e27edbbe9e3c529d6e5c4f93890

</details>


### [42] [UniChange: Unifying Change Detection with Multimodal Large Language Model](https://arxiv.org/abs/2511.02607)
*Xu Zhang,Danyang Li,Xiaohang Dong,Tianhao Wu,Hualong Yu,Jianye Wang,Qicheng Li,Xiang Li*

Main category: cs.CV

TL;DR: 提出首个基于多模态大语言模型的变化检测框架UniChange，通过语言先验和特殊标记统一BCD/SCD任务，实现多源数据知识融合与SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有变化检测模型受限于单类型标注数据，无法协同利用多源BCD/SCD数据集，导致泛化性和多功能性不足。MLLMs的统一能力为构建通用框架提供新可能

Method: 1. 引入[T1][T2][CHANGE]三个特殊标记统一BCD/SCD任务
2. 利用文本提示指导变化类别识别，摆脱预定义分类头限制
3. 整合生成式语言能力与专用CD功能

Result: 在WHU-CD(90.41 IoU)、S2Looking(53.04)、LEVIR-CD+(78.87)、SECOND(57.62)四个基准上全面超越现有方法，达到SOTA

Conclusion: UniChange首次实现变化检测任务的统一框架，能有效学习多源冲突数据，在保持语言生成能力的同时取得显著性能提升，代码已开源

Abstract: Change detection (CD) is a fundamental task for monitoring and analyzing land
cover dynamics. While recent high performance models and high quality datasets
have significantly advanced the field, a critical limitation persists. Current
models typically acquire limited knowledge from single-type annotated data and
cannot concurrently leverage diverse binary change detection (BCD) and semantic
change detection (SCD) datasets. This constraint leads to poor generalization
and limited versatility. The recent advancements in Multimodal Large Language
Models (MLLMs) introduce new possibilities for a unified CD framework. We
leverage the language priors and unification capabilities of MLLMs to develop
UniChange, the first MLLM-based unified change detection model. UniChange
integrates generative language abilities with specialized CD functionalities.
Our model successfully unifies both BCD and SCD tasks through the introduction
of three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange
utilizes text prompts to guide the identification of change categories,
eliminating the reliance on predefined classification heads. This design allows
UniChange to effectively acquire knowledge from multi-source datasets, even
when their class definitions conflict. Experiments on four public benchmarks
(WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance,
achieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively,
surpassing all previous methods. The code is available at
https://github.com/Erxucomeon/UniChange.

</details>


### [43] [VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation](https://arxiv.org/abs/2511.02778)
*Kevin Qinghong Lin,Yuhao Zheng,Hangyu Ran,Dantong Zhu,Dongxing Mao,Linjie Li,Philip Torr,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: 提出SVG代码作为符号化视觉表示，构建VCode基准测试，通过CodeVQA评估框架和VCoder代理框架显著提升多模态编码能力（12.3分增益）


<details>
  <summary>Details</summary>
Motivation: 现有模型在语言任务（代码生成/调试）表现突出，但视觉编码领域存在空白。受人类草图推理启发，主张SVG代码作为紧凑、可解释、可执行的视觉表示形式

Method: 1. 构建VCode基准：将多模态理解重构为代码生成任务（覆盖常识/专业领域/视觉感知）
2. 提出CodeVQA评估协议：通过SVG渲染后的问答验证符号保真度
3. 开发VCoder框架：结合思维修订循环（迭代分析差异）和视觉工具（检测器/解析器提供结构化线索）

Result: 前沿视觉语言模型在SVG生成存在显著差距，VCoder框架带来12.3分整体提升。人类实验显示SVG与原始图像在VQA任务中保持一致性（相关系数0.81）

Conclusion: SVG符号化视觉表示具有可行性，VCoder的迭代修订机制和视觉工具集成有效提升视觉编码能力，为多模态推理开辟新路径

Abstract: Code has emerged as a precise and executable medium for reasoning and action
in the agent era. Yet, progress has largely focused on language-centric tasks
such as program synthesis and debugging, leaving visual-centric coding
underexplored. Inspired by how humans reason over sketches, we advocate SVG
code as a compact, interpretable, and executable visual representation. We
introduce VCode, a benchmark that reframes multimodal understanding as code
generation: given an image, a model must produce SVG that preserves symbolic
meaning for downstream reasoning. VCode covers three domains - general
commonsense (MM-Vet), professional disciplines (MMMU), and visual-centric
perception (CV-Bench). To assess symbolic fidelity, we propose CodeVQA, a novel
evaluation protocol in which a policy model answers questions over rendered
SVGs; correct answers indicate faithful symbolic preservation. Empirically,
frontier VLMs struggle to generate faithful SVGs, revealing a persistent gap
between language-centric and visual-centric coding. To close this gap, we
introduce VCoder, an agentic framework that augments VLMs along two axes: (i)
Thinking with Revision, which iteratively analyzes discrepancies and refines
SVG code; and (ii) Acting with Visual Tools, where detectors and parsers supply
structured cues such as objects, shapes, and text beyond the model's intrinsic
capacity. Across benchmarks, frontier VLMs with strong reasoning capabilities
score well overall yet remain limited in professional knowledge and 3D
reasoning. VCoder delivers a 12.3-point overall gain over the top-performing
Claude-4-Opus. Human studies show that both humans and VLMs perform worse on
rendered SVGs, their consistency reveals the promise of symbolic visual
representation. The benchmark and code are available at
https://github.com/CSU-JPG/VCode.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [44] [CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization](https://arxiv.org/abs/2511.01884)
*Zijian Zhang,Rong Wang,Shiyang Li,Yuebo Luo,Mingyi Hong,Caiwen Ding*

Main category: cs.LG

TL;DR: 提出多智能体训练免配置工作流CudaForge，通过双代理协同迭代优化CUDA内核，实现97.6%正确率和1.68倍加速效果


<details>
  <summary>Details</summary>
Motivation: 现有CUDA内核自动生成方法存在效率低、计算开销大、泛化能力差的问题，人工设计成本高昂且耗时

Method: 采用Coder和Judge双代理架构，通过生成-验证-硬件反馈-迭代优化的闭环流程，集成Nsight Compute硬件指标分析

Result: 支持多GPU平台(A100/RTX6000等)和基础模型(OpenAI-o3/GPT-5等)，单核优化仅需26分钟/0.3美元成本，显著优于现有方案

Conclusion: 验证了多智能体工作流在CUDA内核优化中的成本效益和泛化能力，为高性能计算提供新范式

Abstract: Developing efficient CUDA kernels is increasingly critical for AI
applications such as large-scale LLM training. However, manual kernel design is
both costly and time-consuming, motivating automatic approaches that leverage
LLMs for code generation. Existing methods for automatic kernel generation,
however, often produce low-efficiency kernels, incur high computational
overhead, and fail to generalize across settings. In this work, we propose
CudaForge, a training-free multi-agent workflow for CUDA kernel generation and
optimization. Our workflow is inspired by the iterative workflow of human
experts, which contains steps such as developing initial kernels, testing
correctness, analyzing hardware feedback, and iterative improvement. More
specifically, CudaForge employs two LLM agents: a Coder and a Judge, that
iteratively generate, correct, and optimize CUDA kernels, while integrating
hardware feedback such as Nsight Compute (NCU) metrics. In extensive
evaluations, we show that CudaForge, by leveraging base models like OpenAI-o3,
achieves 97.6\% correctness of generated kernels and an average 1.68$\times$
speedup over PyTorch baselines, substantially surpassing state-of-the-art
models including OpenAI-o3 and Kevin on KernelBench. Beyond accuracy and speed,
CudaForge demonstrates strong generalization across GPUs (A100, RTX 6000, 4090,
3090) and base models (OpenAI-o3, GPT-5, gpt-oss-120B, Claude-Sonnet-4,
QwQ-32B), while maintaining high efficiency. In particular, generating an
optimized kernel takes about 26.5 minutes on one RTX6000 and incurs about \$
0.3 API cost, which is significantly cheaper than existing agentic work that
costs 6 H100 hours and \$ 5 API cost per kernel. Our results highlight that
multi-agent, training-free workflows can enable cost-effective, generalizable,
and high-performance CUDA kernel optimization. Code available at
https://github.com/OptimAI-Lab/CudaForge

</details>


### [45] [Retrieval-Augmented Multimodal Depression Detection](https://arxiv.org/abs/2511.01892)
*Ruibo Hou,Shiyu Teng,Jiaqing Liu,Shurong Chai,Yinhao Li,Lanfen Lin,Yen-Wei Chen*

Main category: cs.LG

TL;DR: 提出基于检索增强生成（RAG）框架的抑郁症检测方法，通过LLM生成情感提示增强多模态表征，在AVEC 2019数据集上达到SOTA性能（CCC 0.593，MAE 3.95）


<details>
  <summary>Details</summary>
Motivation: 现有基于情感分析的方法存在计算成本高、领域不匹配和静态知识局限等问题，需增强情绪理解能力和模型可解释性

Method: 从情感数据集中检索语义相关情绪内容，利用大语言模型生成情感提示（Emotion Prompt）作为辅助模态

Result: 在AVEC 2019数据集上取得当前最优性能（一致性相关系数0.593，平均绝对误差3.95），超越迁移学习和多任务学习方法

Conclusion: RAG框架通过动态生成情感提示，有效提升多模态抑郁症检测的情绪表征能力和模型可解释性

Abstract: Multimodal deep learning has shown promise in depression detection by
integrating text, audio, and video signals. Recent work leverages sentiment
analysis to enhance emotional understanding, yet suffers from high
computational cost, domain mismatch, and static knowledge limitations. To
address these issues, we propose a novel Retrieval-Augmented Generation (RAG)
framework. Given a depression-related text, our method retrieves semantically
relevant emotional content from a sentiment dataset and uses a Large Language
Model (LLM) to generate an Emotion Prompt as an auxiliary modality. This prompt
enriches emotional representation and improves interpretability. Experiments on
the AVEC 2019 dataset show our approach achieves state-of-the-art performance
with CCC of 0.593 and MAE of 3.95, surpassing previous transfer learning and
multi-task learning baselines.

</details>


### [46] [TapOut: A Bandit-Based Approach to Dynamic Speculative Decoding](https://arxiv.org/abs/2511.02017)
*Aditya Sridhar,Nish Sinnadurai,Sean Lie,Vithursan Thangarasa*

Main category: cs.LG

TL;DR: 提出TapOut算法，使用多臂老虎机实现动态推测策略选择，无需调参即获得更优加速效果


<details>
  <summary>Details</summary>
Motivation: 现有动态推测解码方法依赖人工调参的敏感阈值（如token熵），存在跨模型/领域泛化性差、调参成本高的问题

Method: 基于多臂老虎机的元算法框架，在线选择多个无参数的动态推测策略，通过历史奖励和探索平衡实现策略选择

Result: 在多个模型组合和数据集上的实验显示，TapOut无需超参调节即达到或超越现有动态推测基线的加速效果

Conclusion: TapOut为动态推测解码提供了一种即插即用的高效解决方案，显著提升了方法的通用性和易用性

Abstract: Speculative decoding accelerates LLMs by using a lightweight draft model to
generate tokens autoregressively before verifying them in parallel with a
larger target model. However, determining the optimal number of tokens to draft
remains a key challenge limiting the approach's effectiveness. Dynamic
speculative decoding aims to intelligently decide how many tokens to draft to
achieve maximum speedups. Existing methods often rely on hand-tuned, sensitive
thresholds (e.g., token entropy), which are costly to set and generalize poorly
across models and domains. We propose TapOut, an online, training-free,
plug-and-play algorithm for dynamic speculation policy selection using
multi-armed bandits. Our approach employs a meta-algorithm that selects among
multiple parameter-free dynamic speculation strategies based on past reward and
exploration. We conduct extensive experiments across diverse model pairs and
datasets, showing that TapOut achieves competitive or superior speedups
compared to well-established dynamic speculation baselines without any
hyperparameter tuning.

</details>


### [47] [Regularization Through Reasoning: Systematic Improvements in Language Model Classification via Explanation-Enhanced Fine-Tuning](https://arxiv.org/abs/2511.02044)
*Vivswan Shah,Randy Cogill,Hanwei Yue,Gopinath Chennupati,Rinat Khaziev*

Main category: cs.LG

TL;DR: 研究发现微调时给标签添加解释（包括随机无意义标记）能提升模型性能，关键因素是结构化标记而非语义内容


<details>
  <summary>Details</summary>
Motivation: 探索在LLM微调过程中，标签附带解释是否提升模型性能，并验证随机标记是否也能达到类似效果

Method: 使用多LLM集成数据微调7B模型，在六个对话数据集测试标签+解释训练，对比真实解释与词袋/乱序等伪解释效果

Result: 伪解释组准确率显著高于纯标签训练，中间层激活熵增加+输出层预测更集中，显示结构化标记促进深度计算

Conclusion: 标记级结构化信息通过增加计算深度和正则化作用提升分类可靠性，揭示了模型推理过程中架构设计的重要性

Abstract: Fine-tuning LLMs for classification typically maps inputs directly to labels.
We ask whether attaching brief explanations to each label during fine-tuning
yields better models. We evaluate conversational response quality along three
axes: naturalness, comprehensiveness, and on-topic adherence, each rated on
5-point scales. Using ensemble-generated data from multiple LLMs, we fine-tune
a 7B-parameter model and test across six diverse conversational datasets.
Across 18 dataset, task settings, label-plus-explanation training outperforms
label-only baselines.
  A central and unexpected result concerns random tokens. We replace
human-written explanations with text that is syntactically incoherent yet
vocabulary-aligned with the originals (e.g., shuffled or bag-of-words
variants). Despite lacking semantics, these pseudo-explanations still improve
accuracy over label-only training and often narrow much of the gap to true
explanations. The effect persists across datasets and training seeds,
indicating that gains arise less from meaning than from structure: the extra
token budget encourages richer intermediate computation and acts as a
regularizer that reduces over-confident shortcuts.
  Internal analyses support this view: explanation-augmented models exhibit
higher activation entropy in intermediate layers alongside sharper predictive
mass at the output layer, consistent with increased deliberation before
decision. Overall, explanation-augmented fine-tuning, whether with genuine
rationales or carefully constructed random token sequences, improves accuracy
and reliability for LLM classification while clarifying how token-level
scaffolding shapes computation during inference.

</details>


### [48] [LLM Probing with Contrastive Eigenproblems: Improving Understanding and Applicability of CCS](https://arxiv.org/abs/2511.02089)
*Stefan F. Schouten,Peter Bloem*

Main category: cs.LG

TL;DR: 通过将对比一致搜索(CCS)重构为特征问题并提出相对对比一致性概念，提升了方法性能并解决了初始化敏感性问题


<details>
  <summary>Details</summary>
Motivation: 原始CCS的双术语优化目标机制不明确且适用性有限，需要更深入的理论解释和方法扩展

Method: 将CCS重新构建为特征值问题，推导出闭式解，引入可解释特征值和多变量扩展框架

Result: 新方法在多个数据集上达到与CCS相当的准确率，同时显著降低对随机初始化的敏感性

Conclusion: 相对化对比一致性原则不仅深化了对CCS的理解，还为开发更通用的模型探测和可解释性方法提供了新方向

Abstract: Contrast-Consistent Search (CCS) is an unsupervised probing method able to
test whether large language models represent binary features, such as sentence
truth, in their internal activations. While CCS has shown promise, its two-term
objective has been only partially understood. In this work, we revisit CCS with
the aim of clarifying its mechanisms and extending its applicability. We argue
that what should be optimized for, is relative contrast consistency. Building
on this insight, we reformulate CCS as an eigenproblem, yielding closed-form
solutions with interpretable eigenvalues and natural extensions to multiple
variables. We evaluate these approaches across a range of datasets, finding
that they recover similar performance to CCS, while avoiding problems around
sensitivity to random initialization. Our results suggest that relativizing
contrast consistency not only improves our understanding of CCS but also opens
pathways for broader probing and mechanistic interpretability methods.

</details>


### [49] [Can LLMs subtract numbers?](https://arxiv.org/abs/2511.02795)
*Mayank Jobanputra,Nils Philipp Walter,Maitrey Mehta,Blerta Veseli,Evan Parker Kelly Chapple,Yifan Wang,Sneha Chetani,Ellie Pavlick,Antonio Vergari,Vera Demberg*

Main category: cs.LG

TL;DR: 研究发现LLMs在减法运算中常忽略负号，指令微调能有效改善该问题


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注加法/乘法，但作为非交换运算的减法尚未被充分研究，其错误模式存在特殊性

Method: 评估4个家族的8个预训练LLMs，采用数值探测分析和小样本/指令微调实验

Result: 减法准确率显著低于加法（尤其a<b时），模型内部能编码负号信息但输出失败，指令微调实现97.8%负号准确率

Conclusion: 揭示了LLMs算术能力的局限性及可恢复性，为改进数值推理提供新方向

Abstract: We present a systematic study of subtraction in large language models (LLMs).
While prior benchmarks emphasize addition and multiplication, subtraction has
received comparatively little attention despite being structurally distinct as
a non-commutative operation. We evaluate eight pretrained LLMs spanning four
families on addition and subtraction problems. Our experiments reveal that
subtraction accuracy lags behind addition by a wide margin. We find that the
errors for ($a-b$) are concentrated in cases where ($a<b$). In such cases, LLMs
frequently produce the correct magnitude but omit the negative sign. Probing
analyses show that LLMs internally encode whether results should be negative,
yet this information is often not reflected in generated outputs. We further
test well-known techniques such as few-shot learning and instruction-tuning to
see if they can improve the LLMs' performance. Our results suggest that while
few-shot prompting yields modest gains, the instruction-tuned models achieve
near-perfect accuracies in generating the negative sign. Together, these
findings provide a clearer characterization of the limitations and
recoverability of LLMs' arithmetic capabilities in subtraction.

</details>


### [50] [In Good GRACEs: Principled Teacher Selection for Knowledge Distillation](https://arxiv.org/abs/2511.02833)
*Abhishek Panigrahi,Bingbin Liu,Sadhika Malladi,Sham Kakade,Surbhi Goel*

Main category: cs.LG

TL;DR: 提出GRACE轻量评分法，通过梯度分布特性高效选择兼容教师模型，提升知识蒸馏效果


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏依赖试错法选择教师模型效率低下，需开发无需验证器/测试数据的评估方法

Method: 基于信息论连接留一法稳定性，通过学生模型梯度分布特性量化教师有效性，避免访问教师内部参数

Result: 在GSM8K/MATH数据集达86%相关性，LLaMA/OLMo学生模型性能提升7.4%，指导蒸馏温度/模型选择

Conclusion: GRACE可系统优化蒸馏设计，显著提升学生模型性能，为模型压缩提供高效指导框架

Abstract: Knowledge distillation is an efficient strategy to use data generated by
large "teacher" language models to train smaller capable "student" models, but
selecting the optimal teacher for a specific student-task combination requires
expensive trial-and-error. We propose a lightweight score called GRACE to
quantify how effective a teacher will be for post-training a student model.
GRACE measures distributional properties of the student's gradients without
access to a verifier, teacher logits, teacher internals, or test data. From an
information-theoretic perspective, GRACE connects to leave-one-out stability of
gradient-based algorithms, which controls the generalization performance of the
distilled students. On GSM8K and MATH, GRACE correlates strongly (up to 86%
Spearman correlation) with the performance of the distilled LLaMA and OLMo
students. In particular, training a student using the GRACE-selected teacher
can improve the performance by up to 7.4% over naively using the
best-performing teacher. Further, GRACE can provide guidance on crucial design
choices in distillation, including (1) the best temperature to use when
generating from the teacher, (2) the best teacher to use given a size
constraint, and (3) the best teacher to use within a specific model family.
Altogether, our findings demonstrate that GRACE can efficiently and effectively
identify a strongly compatible teacher for a given student and provide
fine-grained guidance on how to perform distillation.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [51] [Complete asymptotic type-token relationship for growing complex systems with inverse power-law count rankings](https://arxiv.org/abs/2511.02069)
*Pablo Rosillo-Rodes,Laurent Hébert-Dufresne,Peter Sheridan Dodds*

Main category: physics.soc-ph

TL;DR: 提出确定性模型统一解释Zipf定律与Heaps定律关系，无需随机机制即可推导类型-标记渐近规律


<details>
  <summary>Details</summary>
Motivation: 解决早期模型在α特殊值（如α=1和α≫1）的局限性，证明类型-标记关系本质源于Zipf定律的统计规律性

Method: 构建理想化增长系统模型，基于类型计数的逆幂律排名形式，通过严格渐近分析避免近似和随机过程

Result: 获得适用于所有α值的统一渐近表达式，修正特殊案例偏差，验证类型-标记关系作为Zipf定律的必然结果

Conclusion: 确定性框架揭示Heaps定律本质是Zipf定律的衍生现象，无需随机采样即可统一解释两种统计规律关系

Abstract: The growth dynamics of complex systems often exhibit statistical regularities
involving power-law relationships. For real finite complex systems formed by
countable tokens (animals, words) as instances of distinct types (species,
dictionary entries), an inverse power-law scaling $S \sim r^{-\alpha}$ between
type count $S$ and type rank $r$, widely known as Zipf's law, is widely
observed to varying degrees of fidelity. A secondary, summary relationship is
Heaps' law, which states that the number of types scales sublinearly with the
total number of observed tokens present in a growing system. Here, we propose
an idealized model of a growing system that (1) deterministically produces
arbitrary inverse power-law count rankings for types, and (2) allows us to
determine the exact asymptotics of the type-token relationship. Our argument
improves upon and remedies earlier work. We obtain a unified asymptotic
expression for all values of $\alpha$, which corrects the special cases of
$\alpha = 1$ and $\alpha \gg 1$. Our approach relies solely on the form of
count rankings, avoids unnecessary approximations, and does not involve any
stochastic mechanisms or sampling processes. We thereby demonstrate that a
general type-token relationship arises solely as a consequence of Zipf's law.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [52] [An Evaluation of Interleaved Instruction Tuning on Semantic Reasoning Performance in an Audio MLLM](https://arxiv.org/abs/2511.02234)
*Jiawei Liu,Enis Berk Çoban,Zarina Schevchenko,Hao Tang,Zhigang Zhu,Michael I Mandel,Johanna Devaney*

Main category: cs.MM

TL;DR: 探索交错指令微调在音频多模态大语言模型（MLLM）中对语义推理与标注能力的权衡影响


<details>
  <summary>Details</summary>
Motivation: 传统MLLM训练方式可能导致模态整合不充分，限制了语言模型核心推理能力的发挥。研究者提出通过交错音频标记与文本提示的调优方法，以增强模型的语义推理能力。

Method: 基于LTU模型构建SHARD音频语义推理基准（聚焦同义词/上位词识别），对比零样本交错提示与微调策略的效果，同时评估模型在音频标注任务上的表现。

Result: 交错提示显著提升推理性能（微调后效果更佳），但导致音频标注能力下降，揭示模态整合中的任务特异性权衡现象。

Conclusion: 交错调优可有效释放MLLM的推理潜力，但需平衡不同任务需求。SHARD基准的建立为后续研究提供了重要评估工具。

Abstract: Standard training for Multi-modal Large Language Models (MLLMs) involves
concatenating non-textual information, like vision or audio, with a text
prompt. This approach may not encourage deep integration of modalities,
limiting the model's ability to leverage the core language model's reasoning
capabilities. This work examined the impact of interleaved instruction tuning
in an audio MLLM, where audio tokens are interleaved within the prompt. Using
the Listen, Think, and Understand (LTU) model as a testbed, we conduct an
experiment using the Synonym and Hypernym Audio Reasoning Dataset (SHARD), our
newly created reasoning benchmark for audio-based semantic reasoning focusing
on synonym and hypernym recognition. Our findings show that while even
zero-shot interleaved prompting improves performance on our reasoning tasks, a
small amount of fine-tuning using interleaved training prompts improves the
results further, however, at the expense of the MLLM's audio labeling ability.

</details>
