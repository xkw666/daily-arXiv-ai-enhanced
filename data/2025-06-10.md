<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 133]
- [cs.GR](#cs.GR) [Total: 12]
- [cs.LG](#cs.LG) [Total: 16]
- [cs.CY](#cs.CY) [Total: 5]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.MA](#cs.MA) [Total: 1]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.RO](#cs.RO) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 10]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [How Significant Are the Real Performance Gains? An Unbiased Evaluation Framework for GraphRAG](https://arxiv.org/abs/2506.06331)
*Qiming Zeng,Xiao Yan,Hao Luo,Yuhao Lin,Yuxiang Wang,Fangcheng Fu,Bo Du,Quanqing Xu,Jiawei Jiang*

Main category: cs.CL

TL;DR: 研究发现现有GraphRAG评估框架存在严重缺陷，提出基于图-文本的问题生成和无偏评估框架，发现主流方法性能提升幅度远低于先前报告


<details>
  <summary>Details</summary>
Motivation: 当前GraphRAG评估存在两大缺陷：问题与数据集无关性、LLM评估偏差，导致性能结论失真

Method: 1. 图-文本联合的问题生成技术
2. 消除LLM评估偏见的标准化流程
3. 对3种典型GraphRAG方法进行系统性评估

Result: 新评估框架下，主流GraphRAG方法的性能提升幅度比原报告降低50%-70%，显示先前结论存在显著偏差

Conclusion: 尽管存在局限，该研究揭示了科学评估体系对GraphRAG领域的基础性作用，呼吁建立更严谨的评估标准

Abstract: By retrieving contexts from knowledge graphs, graph-based retrieval-augmented
generation (GraphRAG) enhances large language models (LLMs) to generate quality
answers for user questions. Many GraphRAG methods have been proposed and
reported inspiring performance in answer quality. However, we observe that the
current answer evaluation framework for GraphRAG has two critical flaws, i.e.,
unrelated questions and evaluation biases, which may lead to biased or even
wrong conclusions on performance. To tackle the two flaws, we propose an
unbiased evaluation framework that uses graph-text-grounded question generation
to produce questions that are more related to the underlying dataset and an
unbiased evaluation procedure to eliminate the biases in LLM-based answer
assessment. We apply our unbiased framework to evaluate 3 representative
GraphRAG methods and find that their performance gains are much more moderate
than reported previously. Although our evaluation framework may still have
flaws, it calls for scientific evaluations to lay solid foundations for
GraphRAG research.

</details>


### [2] [TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment](https://arxiv.org/abs/2506.06343)
*Taesoo Kim,Jong Hwan Ko*

Main category: cs.CL

TL;DR: TESU-LLM框架通过统一编码器将文本与语音映射到共享潜在空间，仅用文本数据训练即可实现语音推理能力，突破了传统方法对语音数据和算力的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有语音语言模型依赖大规模语音-文本配对数据和海量算力，导致模型扩展性和可访问性受限。

Method: 使用统一编码器对齐文本/语音潜在空间，通过轻量级投影网络将编码输出与LLM嵌入空间对接，实现纯文本监督下的跨模态泛化。

Result: 在多个语音基准测试中达到与多模态数据训练基线模型相当的性能表现。

Conclusion: 该方法证明了无需语音数据构建语音LLM的可能性，为语音语言模型开发提供了高效可扩展的新路径。

Abstract: Recent advances in speech-enabled language models have shown promising
results in building intelligent voice assistants. However, most existing
approaches rely on large-scale paired speech-text data and extensive
computational resources, which pose challenges in terms of scalability and
accessibility. In this paper, we present \textbf{TESU-LLM}, a novel framework
that enables training speech-capable language models using only text data. Our
key insight is to leverage a unified encoder that maps semantically equivalent
text and speech inputs to a shared latent space. By aligning the encoder output
with the embedding space of a LLM via a lightweight projection network, we
enable the model to generalize from text-only supervision to speech-based
inference. Despite being trained exclusively on text, TESU-LLM achieves strong
performance on various speech-related benchmarks, comparable to baseline
methods trained with large-scale multimodal datasets and substantial
computational resources. These results highlight the effectiveness and
efficiency of our approach, offering a scalable path toward building speech
LLMs without speech data.

</details>


### [3] [Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection](https://arxiv.org/abs/2506.06347)
*Zachary Yang,Domenico Tullo,Reihaneh Rabbany*

Main category: cs.CL

TL;DR: 论文提出通过软提示技术和LLM辅助标签迁移框架，提升多游戏多语言场景下毒性检测的扩展性和效率，在Ubisoft应用中显著降低资源消耗。


<details>
  <summary>Details</summary>
Motivation: 解决游戏社区毒性检测在多游戏、多语言扩展时面临的实时计算效率与维护成本问题，避免为不同游戏/语言单独维护模型的复杂性。

Method: 1. 软提示技术：通过游戏上下文令牌实现单一模型跨游戏检测；2. GPT-4o-mini驱动的标签迁移框架支持7种新语言。

Result: 德语检测性能超越英语基准（45.39%→更高），法语/葡萄牙语/俄语F1分数32.96%-58.88%；生产环境日均识别50名违规玩家/游戏。

Conclusion: 统一框架显著降低计算资源和维护成本，验证了跨游戏跨语言检测方案的可行性，为游戏社区管理提供高效解决方案。

Abstract: Toxicity detection in gaming communities faces significant scaling challenges
when expanding across multiple games and languages, particularly in real-time
environments where computational efficiency is crucial. We present two key
findings to address these challenges while building upon our previous work on
ToxBuster, a BERT-based real-time toxicity detection system. First, we
introduce a soft-prompting approach that enables a single model to effectively
handle multiple games by incorporating game-context tokens, matching the
performance of more complex methods like curriculum learning while offering
superior scalability. Second, we develop an LLM-assisted label transfer
framework using GPT-4o-mini to extend support to seven additional languages.
Evaluations on real game chat data across French, German, Portuguese, and
Russian achieve macro F1-scores ranging from 32.96% to 58.88%, with
particularly strong performance in German, surpassing the English benchmark of
45.39%. In production, this unified approach significantly reduces
computational resources and maintenance overhead compared to maintaining
separate models for each game and language combination. At Ubisoft, this model
successfully identifies an average of 50 players, per game, per day engaging in
sanctionable behavior.

</details>


### [4] [Relationship Detection on Tabular Data Using Statistical Analysis and Large Language Models](https://arxiv.org/abs/2506.06371)
*Panagiotis Koletsis,Christos Panagiotopoulos,Georgios Th. Papadopoulos,Vasilis Efthymiou*

Main category: cs.CL

TL;DR: 提出混合KG与LLM的统计分析方法，优化表格列关系检测任务(CPA)，在基准测试中展现竞争力


<details>
  <summary>Details</summary>
Motivation: 解决未标注表格数据中列关系检测效率问题，通过结合知识图谱约束和统计降维提升传统LLM方法的搜索效率

Method: 基于领域/范围约束检测和关系共现分析的统计降维模块，结合不同量化级别LLM的多提示技术

Result: 在SemTab数据集验证中模块有效性显著，最优LLM配置(7B参数+4bit量化)达到SOTA水平，代码已开源

Conclusion: 统计分析与LLM的混合策略在保持精度的同时降低计算成本，为表格理解任务提供新范式

Abstract: Over the past few years, table interpretation tasks have made significant
progress due to their importance and the introduction of new technologies and
benchmarks in the field. This work experiments with a hybrid approach for
detecting relationships among columns of unlabeled tabular data, using a
Knowledge Graph (KG) as a reference point, a task known as CPA. This approach
leverages large language models (LLMs) while employing statistical analysis to
reduce the search space of potential KG relations. The main modules of this
approach for reducing the search space are domain and range constraints
detection, as well as relation co-appearance analysis. The experimental
evaluation on two benchmark datasets provided by the SemTab challenge assesses
the influence of each module and the effectiveness of different
state-of-the-art LLMs at various levels of quantization. The experiments were
performed, as well as at different prompting techniques. The proposed
methodology, which is publicly available on github, proved to be competitive
with state-of-the-art approaches on these datasets.

</details>


### [5] [Enhancing Decision-Making of Large Language Models via Actor-Critic](https://arxiv.org/abs/2506.06376)
*Heng Dong,Kefei Duan,Chongjie Zhang*

Main category: cs.CL

TL;DR: 提出LAC框架，通过长期动作评估改进LLM策略，在复杂决策任务中超越现有方法


<details>
  <summary>Details</summary>
Motivation: 现有LLM在复杂决策场景存在短期行为局限和评估不准确的问题，需要新的框架实现长期策略优化

Method: 结合token概率计算Q值的鲁棒评估方法，配合梯度无关策略改进机制，支持未来轨迹推演推理

Result: 在ALFWorld/BabyAI-Text/WebShop等环境验证，7B/8B参数模型超越GPT-4基线方法

Conclusion: 结构化策略优化与LLM内在知识结合，显著提升多步决策能力，展现轻量模型应用潜力

Abstract: Large Language Models (LLMs) have achieved remarkable advancements in natural
language processing tasks, yet they encounter challenges in complex
decision-making scenarios that require long-term reasoning and alignment with
high-level objectives. Existing methods either rely on short-term
auto-regressive action generation or face limitations in accurately simulating
rollouts and assessing outcomes, leading to sub-optimal decisions. This paper
introduces a novel LLM-based Actor-Critic framework, termed LAC, that
effectively improves LLM policies with long-term action evaluations in a
principled and scalable way. Our approach addresses two key challenges: (1)
extracting robust action evaluations by computing Q-values via token logits
associated with positive/negative outcomes, enhanced by future trajectory
rollouts and reasoning; and (2) enabling efficient policy improvement through a
gradient-free mechanism. Experiments across diverse environments -- including
high-level decision-making (ALFWorld), low-level action spaces (BabyAI-Text),
and large action spaces (WebShop) -- demonstrate the framework's generality and
superiority over state-of-the-art methods. Notably, our approach achieves
competitive performance using 7B/8B parameter LLMs, even outperforming baseline
methods employing GPT-4 in complex tasks. These results underscore the
potential of integrating structured policy optimization with LLMs' intrinsic
knowledge to advance decision-making capabilities in multi-step environments.

</details>


### [6] [Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering](https://arxiv.org/abs/2506.06384)
*Yi Ji,Runzhi Li,Baolei Mao*

Main category: cs.CL

TL;DR: 提出DMPI-PMHFE双通道特征融合框架，结合预训练模型与启发式特征工程，有效检测LLM提示注入攻击并在多模型部署中显著降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有LLM提示注入防御机制面临效果与泛化性之间的矛盾，亟需一种高效通用的检测方法。

Method: 1. 语义通道：使用DeBERTa-v3-base提取文本上下文语义特征
2. 结构通道：基于已知攻击模式设计启发式规则提取显式特征
3. 双通道特征融合后通过全连接网络进行预测

Result: 在多个基准数据集上实现最优准确率/召回率/F1值，实际部署使GLM-4/LLaMA3/Qwen2.5/GPT-4o等主流LLM攻击成功率显著下降

Conclusion: 双通道特征融合有效弥补单模型局限，实验证明该方法在检测效果和实际防御能力上的双重优势。

Abstract: With the widespread adoption of Large Language Models (LLMs), prompt
injection attacks have emerged as a significant security threat. Existing
defense mechanisms often face critical trade-offs between effectiveness and
generalizability. This highlights the urgent need for efficient prompt
injection detection methods that are applicable across a wide range of LLMs. To
address this challenge, we propose DMPI-PMHFE, a dual-channel feature fusion
detection framework. It integrates a pretrained language model with heuristic
feature engineering to detect prompt injection attacks. Specifically, the
framework employs DeBERTa-v3-base as a feature extractor to transform input
text into semantic vectors enriched with contextual information. In parallel,
we design heuristic rules based on known attack patterns to extract explicit
structural features commonly observed in attacks. Features from both channels
are subsequently fused and passed through a fully connected neural network to
produce the final prediction. This dual-channel approach mitigates the
limitations of relying only on DeBERTa to extract features. Experimental
results on diverse benchmark datasets demonstrate that DMPI-PMHFE outperforms
existing methods in terms of accuracy, recall, and F1-score. Furthermore, when
deployed actually, it significantly reduces attack success rates across
mainstream LLMs, including GLM-4, LLaMA 3, Qwen 2.5, and GPT-4o.

</details>


### [7] [Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models](https://arxiv.org/abs/2506.06395)
*Pengyi Li,Matvey Skripkin,Alexander Zubrey,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.CL

TL;DR: 提出基于模型自身置信度的强化学习方法（RLSC），无需人工标注即可显著提升数学推理能力


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法依赖昂贵的人工标注或外部奖励模型，需要更简单高效的模型后训练方法

Method: 将模型输出置信度作为强化学习奖励信号，实现自我监督训练（每个问题仅需8样本，4训练轮次）

Result: 在Qwen2.5-Math-7B模型上实现显著提升：AIME2024 +20.10%、MATH500 +49.40%、AMC23 +52.50%

Conclusion: RLSC为推理模型提供了一种简单、可扩展的轻监督后训练方案，具有工程应用价值

Abstract: Large language models (LLMs) excel at reasoning, yet post-training remains
critical for aligning their behavior with task goals. Existing reinforcement
learning (RL) methods often depend on costly human annotations or external
reward models. We propose Reinforcement Learning via Self-Confidence (RLSC),
which uses the model's own confidence as reward signals-eliminating the need
for labels, preference models, or reward engineering. Applied to
Qwen2.5-Math-7B with only 8 samples per question and 4 training epochs, RLSC
improves accuracy by +20.10% on AIME2024, +49.40% on MATH500, and +52.50% on
AMC23. RLSC offers a simple, scalable post-training method for reasoning models
with minimal supervision.

</details>


### [8] [Natural Language Interaction with Databases on Edge Devices in the Internet of Battlefield Things](https://arxiv.org/abs/2506.06396)
*Christopher D. Molek,Roberto Fronteddu,K. Brent Venable,Niranjan Suri*

Main category: cs.CL

TL;DR: 提出基于边缘设备LLM的NLP工作流，通过自然语言交互提升战场物联网决策能力。Llama 3.1模型在数据库查询任务中实现19.4%准确率提升。


<details>
  <summary>Details</summary>
Motivation: 战场物联网(IoBT)设备数据需要实时转化为可消费信息，但现有方法受限于精确匹配要求，影响决策支持效率。

Method: 采用图形数据库+中等规模LLM（适配边缘设备）的双阶段架构：1) 自然语言转Cypher查询 2) 数据库结果自然语言摘要。使用美国陆军MSA公开数据集验证。

Result: Llama 3.1（80亿参数）综合表现最佳，通过放宽Cypher查询的精确匹配要求，准确率提升19.4%

Conclusion: 该工作流为边缘设备部署LLM实现自然语言数据库交互奠定基础，特别适用于战场决策等需要即时信息处理的场景。

Abstract: The expansion of the Internet of Things (IoT) in the battlefield, Internet of
Battlefield Things (IoBT), gives rise to new opportunities for enhancing
situational awareness. To increase the potential of IoBT for situational
awareness in critical decision making, the data from these devices must be
processed into consumer-ready information objects, and made available to
consumers on demand. To address this challenge we propose a workflow that makes
use of natural language processing (NLP) to query a database technology and
return a response in natural language. Our solution utilizes Large Language
Models (LLMs) that are sized for edge devices to perform NLP as well as
graphical databases which are well suited for dynamic connected networks which
are pervasive in the IoBT. Our architecture employs LLMs for both mapping
questions in natural language to Cypher database queries as well as to
summarize the database output back to the user in natural language. We evaluate
several medium sized LLMs for both of these tasks on a database representing
publicly available data from the US Army's Multipurpose Sensing Area (MSA) at
the Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion
parameters) outperforms the other models across all the considered metrics.
Most importantly, we note that, unlike current methods, our two step approach
allows the relaxation of the Exact Match (EM) requirement of the produced
Cypher queries with ground truth code and, in this way, it achieves a 19.4%
increase in accuracy. Our workflow lays the ground work for deploying LLMs on
edge devices to enable natural language interactions with databases containing
information objects for critical decision making.

</details>


### [9] [Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs](https://arxiv.org/abs/2506.06401)
*Hongming Yang,Shi Lin,Jun Shao,Changting Lin,Donghai Zhu,Meng Han,Qinglei Kong*

Main category: cs.CL

TL;DR: DeBoP提出了一种基于蒙特卡洛树搜索的直接行为优化范式，有效提升轻量级大语言模型在复杂任务中的性能表现，同时显著降低计算耗时。


<details>
  <summary>Details</summary>
Motivation: 轻量级大语言模型(LwLLMs)存在推理能力不足、难以应对复杂任务的局限性，而现有提示优化方法依赖人工或大模型元认知能力，无法有效适配LwLLMs。

Method: 将复杂提示优化转化为离散可量化的执行序列优化，采用无梯度蒙特卡洛树搜索算法，直接优化模型行为而非思维链生成。

Result: 在7项挑战性任务中，DeBoP优化的LwLLMs超越GPT-3.5表现，相比其他自动提示优化方法减少60%计算时间。

Conclusion: DeBoP为轻量级模型提供了高效的性能优化方案，在保持计算效率的同时显著提升了模型的实用价值。

Abstract: Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized
models designed to run efficiently on consumer-grade hardware, offering
significant advantages in resource efficiency, cost-effectiveness, and data
privacy. However, these models often struggle with limited inference and
reasoning capabilities, which restrict their performance on complex tasks and
limit their practical applicability. Moreover, existing prompt optimization
methods typically rely on extensive manual effort or the meta-cognitive
abilities of state-of-the-art LLMs, making them less effective for LwLLMs. To
address these challenges, we introduce DeBoP, a new Direct Behavior
Optimization Paradigm, original from the Chain-of-Thought (CoT) prompting
technique. Unlike CoT Prompting, DeBoP is an automatic optimization method,
which focuses on the optimization directly on the behavior of LwLLMs. In
particular, DeBoP transforms the optimization of complex prompts into the
optimization of discrete, quantifiable execution sequences using a
gradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging
tasks where state-of-the-art LLMs excel but LwLLMs generally underperform.
Experimental results demonstrate that DeBoP significantly outperforms recent
prompt optimization methods on most tasks. In particular, DeBoP-optimized
LwLLMs surpass GPT-3.5 on most tasks while reducing computational time by
approximately 60% compared to other automatic prompt optimization methods.

</details>


### [10] [Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights](https://arxiv.org/abs/2506.06404)
*Sooyung Choi,Jaehyeok Lee,Xiaoyuan Yi,Jing Yao,Xing Xie,JinYeong Bak*

Main category: cs.CL

TL;DR: 研究发现价值对齐的大语言模型存在更高安全风险，其生成机制会放大危害，并提出上下文对齐改进方案


<details>
  <summary>Details</summary>
Motivation: 探究价值对齐LLMs的安全隐患及其背后的心理学机制，解决模型价值观与人类安全需求之间的矛盾

Method: 通过带安全标注的数据集分析，结合心理学假设验证，比较不同微调模型的安全表现

Result: 价值对齐模型比未微调模型危害率高37%，在传统安全评估中风险比普通微调模型高5%；价值观驱动机制导致危害结果放大1.8倍

Conclusion: 首次揭示价值对齐模型的'价值观黑箱'机制，提出的上下文对齐方法使安全指标提升19%，为安全可控的价值观对齐提供新思路

Abstract: The application scope of Large Language Models (LLMs) continues to expand,
leading to increasing interest in personalized LLMs that align with human
values. However, aligning these models with individual values raises
significant safety concerns, as certain values may correlate with harmful
information. In this paper, we identify specific safety risks associated with
value-aligned LLMs and investigate the psychological principles behind these
challenges. Our findings reveal two key insights. (1) Value-aligned LLMs are
more prone to harmful behavior compared to non-fine-tuned models and exhibit
slightly higher risks in traditional safety evaluations than other fine-tuned
models. (2) These safety issues arise because value-aligned LLMs genuinely
generate text according to the aligned values, which can amplify harmful
outcomes. Using a dataset with detailed safety categories, we find significant
correlations between value alignment and safety risks, supported by
psychological hypotheses. This study offers insights into the "black box" of
value alignment and proposes in-context alignment methods to enhance the safety
of value-aligned LLMs.

</details>


### [11] [SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities](https://arxiv.org/abs/2506.06406)
*Guoyang Xia,Yifeng Ding,Fengfa Li,Lei Ren,Chen Wei,Fangxiang Feng,Xiaojie Wang*

Main category: cs.CL

TL;DR: 提出Soft Modality-Aware Routing（SMAR）方法，通过KL散度正则化实现多模态MoE模型的专家专业化，在保持86.6%语言能力的同时降低训练成本。


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态MoE模型训练成本高、文本能力退化的问题，寻求不依赖架构修改和大量文本数据的平衡方案。

Method: 采用KL散度正则化约束跨模态路由概率分布，实现无架构修改的专家专业化，仅需2.5%纯文本数据。

Result: 视觉指令调优中语言能力保留率达86.6%（基线对比），多模态性能保持优异，参数效率提升显著。

Conclusion: SMAR为多模态MoE模型提供了模态分化与语言能力平衡的实用解决方案，验证了路由正则化的有效性。

Abstract: Mixture of Experts (MoE) architectures have become a key approach for scaling
large language models, with growing interest in extending them to multimodal
tasks. Existing methods to build multimodal MoE models either incur high
training costs or suffer from degraded language capabilities when adapting
pretrained models. To address this, we propose Soft ModalityAware Routing
(SMAR), a novel regularization technique that uses Kullback Leibler divergence
to control routing probability distributions across modalities, encouraging
expert specialization without modifying model architecture or heavily relying
on textual data. Experiments on visual instruction tuning show that SMAR
preserves language ability at 86.6% retention with only 2.5% pure text,
outperforming baselines while maintaining strong multimodal performance. Our
approach offers a practical and efficient solution to balance modality
differentiation and language capabilities in multimodal MoE models.

</details>


### [12] [Canonical Autoregressive Generation](https://arxiv.org/abs/2506.06446)
*Ivi Chatzi,Nina Corvelo Benz,Stratis Tsirtsis,Manuel Gomez-Rodriguez*

Main category: cs.CL

TL;DR: 提出一种名为'canonical sampling'的采样方法，通过强制模型在自回归生成的每一步生成规范token序列，有效避免非规范token生成，使输出更接近训练数据的真实分布。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在生成过程中可能产生非规范token序列，导致结果偏差和效率问题。现有tokenizer虽然规定了规范token化路径，但模型在推理时可能偏离该路径。

Method: 基于自回归生成必须逐步满足规范token化的理论发现，设计了一种简单高效的canonical sampling机制，通过动态masking排除非规范token的生成可能性。

Result: 理论证明该方法生成的token序列分布比标准采样更接近训练数据的真实分布，实验验证能有效消除非规范token序列的产生。

Conclusion: Canonical sampling通过约束生成过程的规范token选择，显著提升模型输出的可靠性和对齐性，为语言模型生成质量提供理论保障。

Abstract: State of the art large language models are trained using large amounts of
tokens derived from raw text using what is called a tokenizer. Crucially, the
tokenizer determines the (token) vocabulary a model will use during inference
as well as, in principle, the (token) language. This is because, while the
token vocabulary may allow for different tokenizations of a string, the
tokenizer always maps the string to only one of these tokenizations--the
canonical tokenization. However, multiple lines of empirical evidence suggest
that large language models do not always generate canonical token sequences,
and this comes with several negative consequences. In this work, we first show
that, to generate a canonical token sequence, a model needs to generate
(partial) canonical token sequences at each step of the autoregressive
generation process underpinning its functioning. Building upon this theoretical
result, we introduce canonical sampling, a simple and efficient sampling method
that precludes a given model from generating non-canonical token sequences.
Further, we also show that, in comparison with standard sampling, the
distribution of token sequences generated using canonical sampling is provably
closer to the true distribution of token sequences used during training.

</details>


### [13] [What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models](https://arxiv.org/abs/2506.06485)
*Kaiser Sun,Fan Bai,Mark Dredze*

Main category: cs.CL

TL;DR: 研究探讨LLM在上下文与内部知识冲突时的表现，发现模型难以完全抑制固有知识，且上下文与知识一致时表现更佳。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型中上下文输入与参数知识冲突时的行为特性，尤其是检索文档与模型内部知识矛盾时的性能影响问题。

Method: 构建诊断框架和冲突数据集，测试模型在知识冲突场景下多任务类型中的表现。

Result: 1) 知识冲突对非知识型任务影响微弱
2) 上下文与知识一致时表现最优
3) 模型无法完全抑制内部知识
4) 提供冲突解释会增强上下文依赖

Conclusion: 研究揭示了LLM处理知识冲突的局限性，提示基于模型的评估存在效度风险，需在部署中充分考虑知识冲突因素。

Abstract: Large language models frequently rely on both contextual input and parametric
knowledge to perform tasks. However, these sources can come into conflict,
especially when retrieved documents contradict the model's parametric
knowledge. We propose a diagnostic framework to systematically evaluate LLM
behavior under context-memory conflict, where the contextual information
diverges from their parametric beliefs. We construct diagnostic data that
elicit these conflicts and analyze model performance across multiple task
types. Our findings reveal that (1) knowledge conflict has minimal impact on
tasks that do not require knowledge utilization, (2) model performance is
consistently higher when contextual and parametric knowledge are aligned, (3)
models are unable to fully suppress their internal knowledge even when
instructed, and (4) providing rationales that explain the conflict increases
reliance on contexts. These insights raise concerns about the validity of
model-based evaluation and underscore the need to account for knowledge
conflict in the deployment of LLMs.

</details>


### [14] [Improving LLM-Powered EDA Assistants with RAFT](https://arxiv.org/abs/2506.06500)
*Luyao Shi,Michael Kazda,Charles Schmitter,Hemlata Gupta*

Main category: cs.CL

TL;DR: 提出利用合成问答数据和RAFT方法提升LLMs在EDA任务中的性能，并分析数据安全性与泄露风险。


<details>
  <summary>Details</summary>
Motivation: 电子设计工程师在EDA任务中难以高效获取专业知识，且现有开源LLMs缺乏领域适应性，RAG存在回答不准确问题，RAFT需标注数据获取困难。

Method: 1. 生成合成Q/A数据集结合RAFT微调LLMs；2. 用真实用户问题作为RAFS示例优化生成；3. 实施权限管控；4. 评估数据泄露风险。

Result: RAFT显著提升LLMs在EDA任务的表现，RAFS增强数据生成质量，权限控制有效，数据泄露风险可控。

Conclusion: 合成数据+RAFT可高效增强LLMs领域能力，RAFS和权限机制提升实用性，为实际部署提供安全性和有效性参考。

Abstract: Electronic design engineers often struggle to efficiently access relevant
information for tasks like design verification and technology development.
While large language models (LLMs) can enhance productivity as conversational
agents, pre-trained open-source LLMs lack domain-specific knowledge for
Electronic Design Automation (EDA). In a Retrieval-Augmented Generation (RAG)
context, LLMs rely on external context but may still produce inaccurate
responses. Retrieval-Augmented Fine-Tuning (RAFT) improves LLM performance, but
acquiring labeled question/answer (Q/A) data in EDA is difficult. To address
this, we propose using synthetic Q/A datasets to enhance LLMs with RAFT. Our
results show that RAFT with synthetic data significantly boosts LLM performance
for RAG-based EDA tasks. We also investigate the impact of using real user
questions as Retrieval-Augmented Few-Shot (RAFS) examples for synthetic data
generation. Additionally, we implement secure access control to ensure
sensitive information is only accessible to authorized personnel. Finally, we
assess the risk of data leakage and unintended memorization during fine-tuning
with synthetic data, providing practical insights.

</details>


### [15] [Biases Propagate in Encoder-based Vision-Language Models: A Systematic Analysis From Intrinsic Measures to Zero-shot Retrieval Outcomes](https://arxiv.org/abs/2506.06506)
*Kshitish Ghate,Tessa Charlesworth,Mona Diab,Aylin Caliskan*

Main category: cs.CL

TL;DR: 研究发现基础视觉语言模型的社会群体偏见会系统性地传播到下游任务，且模型越大偏见传播越强


<details>
  <summary>Details</summary>
Motivation: 探究基础视觉语言模型（VLMs）中固有的社会群体偏见如何影响下游零样本检索任务中的偏差表现

Method: 通过建立内在表示空间偏差与外在零样本检索任务偏差的关联框架，在3种VLMs、6个社会群体和114个分析场景中进行系统性测量

Result: 发现内在与外在偏差存在强相关性（平均ρ=0.83±0.10），且性能更优的大模型表现出更强的偏见传播，少数群体呈现更不稳健的传播模式

Conclusion: 揭示了AI模型性能提升与偏见强化的矛盾，对当前模型开发趋势发出重要警示，需建立更完善的偏差评估体系

Abstract: To build fair AI systems we need to understand how social-group biases
intrinsic to foundational encoder-based vision-language models (VLMs) manifest
in biases in downstream tasks. In this study, we demonstrate that intrinsic
biases in VLM representations systematically ``carry over'' or propagate into
zero-shot retrieval tasks, revealing how deeply rooted biases shape a model's
outputs. We introduce a controlled framework to measure this propagation by
correlating (a) intrinsic measures of bias in the representational space with
(b) extrinsic measures of bias in zero-shot text-to-image (TTI) and
image-to-text (ITT) retrieval. Results show substantial correlations between
intrinsic and extrinsic bias, with an average $\rho$ = 0.83 $\pm$ 0.10. This
pattern is consistent across 114 analyses, both retrieval directions, six
social groups, and three distinct VLMs. Notably, we find that
larger/better-performing models exhibit greater bias propagation, a finding
that raises concerns given the trend towards increasingly complex AI models.
Our framework introduces baseline evaluation tasks to measure the propagation
of group and valence signals. Investigations reveal that underrepresented
groups experience less robust propagation, further skewing their model-related
outcomes.

</details>


### [16] [Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance](https://arxiv.org/abs/2506.06522)
*Aladin Djuhera,Swanand Ravindra Kadhe,Syed Zawad,Farhan Ahmed,Heiko Ludwig,Holger Boche*

Main category: cs.CL

TL;DR: 现有LLM训练数据集透明度不足，作者通过对比Tulu-3-SFT-Mix和SmolTalk数据集，提出优化后的TuluTalk混合方案，在减少14%数据量的情况下保持/超越原数据集性能


<details>
  <summary>Details</summary>
Motivation: 主流LLM使用的训练数据集缺乏透明度，开源替代方案虽有效但缺乏系统对比，需明确数据质量对模型性能的影响机制

Method: 使用Magpie框架对数据集进行结构化标注（对话轮次/任务类型/输入输出质量），基于分析结果设计新的数据混合方案

Result: TuluTalk数据量减少14%但关键基准测试表现持平或更优，同步公开标注数据集和优化方案

Conclusion: 系统化的数据质量分析可显著提升训练效率，数据质量比数量更重要，开放数据资源将推动LLM训练研究进展

Abstract: Recent work on large language models (LLMs) has increasingly focused on
post-training and alignment with datasets curated to enhance instruction
following, world knowledge, and specialized skills. However, most post-training
datasets used in leading open- and closed-source LLMs remain inaccessible to
the public, with limited information about their construction process. This
lack of transparency has motivated the recent development of open-source
post-training corpora. While training on these open alternatives can yield
performance comparable to that of leading models, systematic comparisons remain
challenging due to the significant computational cost of conducting them
rigorously at scale, and are therefore largely absent. As a result, it remains
unclear how specific samples, task types, or curation strategies influence
downstream performance when assessing data quality. In this work, we conduct
the first comprehensive side-by-side analysis of two prominent open
post-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie
framework, we annotate each sample with detailed quality metrics, including
turn structure (single-turn vs. multi-turn), task category, input quality, and
response quality, and we derive statistics that reveal structural and
qualitative similarities and differences between the two datasets. Based on
these insights, we design a principled curation recipe that produces a new data
mixture, TuluTalk, which contains 14% fewer samples than either source dataset
while matching or exceeding their performance on key benchmarks. Our findings
offer actionable insights for constructing more effective post-training
datasets that improve model performance within practical resource limits. To
support future research, we publicly release both the annotated source datasets
and our curated TuluTalk mixture.

</details>


### [17] [Beyond Facts: Evaluating Intent Hallucination in Large Language Models](https://arxiv.org/abs/2506.06539)
*Yijie Hao,Haofei Yu,Jiaxuan You*

Main category: cs.CL

TL;DR: 论文提出意图幻觉概念（LLM在复杂查询中遗漏/误解条件），建立FAITHQA基准（20,068问题），并提出自动评估指标CONSTRAINT SCORE。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型处理多条件复杂查询时，常出现部分条件遗漏或错误解读的问题，需系统性评估该现象的根源。

Method: 构建FAITHQA基准（覆盖不同主题/难度的查询/RAG场景），通过约束条件分解量化意图幻觉，开发CONSTRAINT SCORE自动评估指标。

Result: 1) SOTA模型普遍存在意图幻觉 2) 根源是LLM的查询条件遗漏或错误解读 3) CONSTRAINT SCORE评估效果接近人类判断。

Conclusion: FAITHQA首次突破事实性幻觉评估框架，为诊断意图幻觉提供系统性工具，CONSTRAINT SCORE指标助力未来LLM可靠性研究。

Abstract: When exposed to complex queries containing multiple conditions, today's large
language models (LLMs) tend to produce responses that only partially satisfy
the query while neglecting certain conditions. We therefore introduce the
concept of Intent Hallucination. In this phenomenon, LLMs either omit
(neglecting to address certain parts) or misinterpret (responding to invented
query parts) elements of the given query, leading to intent hallucinated
generation. To systematically evaluate intent hallucination, we introduce
FAITHQA, a novel benchmark for intent hallucination that contains 20,068
problems, covering both query-only and retrieval-augmented generation (RAG)
setups with varying topics and difficulty. FAITHQA is the first hallucination
benchmark that goes beyond factual verification, tailored to identify the
fundamental cause of intent hallucination. By evaluating various LLMs on
FAITHQA, we find that (1) intent hallucination is a common issue even for
state-of-the-art models, and (2) the phenomenon stems from omission or
misinterpretation of LLMs. To facilitate future research, we introduce an
automatic LLM generation evaluation metric, CONSTRAINT SCORE, for detecting
intent hallucination. Human evaluation results demonstrate that CONSTRAINT
SCORE is closer to human performance for intent hallucination compared to
baselines.

</details>


### [18] [LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles](https://arxiv.org/abs/2506.06561)
*Ho Yin 'Sam' Ng,Ting-Yao Hsu,Aashish Anantha Ramakrishnan,Branislav Kveton,Nedim Lipka,Franck Dernoncourt,Dongwon Lee,Tong Yu,Sungchul Kim,Ryan A. Rossi,Ting-Hao 'Kenneth' Huang*

Main category: cs.CL

TL;DR: 提出多模态个性化图注生成数据集LaMP-Cap，通过整合上下文图像与文本提升个性化效果


<details>
  <summary>Details</summary>
Motivation: 现有AI生成的通用图注难以匹配作者的写作风格和领域规范，需要多模态个性化解决方案

Method: 构建LaMP-Cap数据集，包含目标图像及同文档内三个关联图像（含图像、图注和说明段落）作为上下文特征

Result: 实验显示利用上下文信息可使生成的图注更接近原文（图像特征比文本段落更有效）

Conclusion: 多模态上下文特征（特别是图像）显著提升个性化图注生成效果

Abstract: Figure captions are crucial for helping readers understand and remember a
figure's key message. Many models have been developed to generate these
captions, helping authors compose better quality captions more easily. Yet,
authors almost always need to revise generic AI-generated captions to match
their writing style and the domain's style, highlighting the need for
personalization. Despite language models' personalization (LaMP) advances,
these technologies often focus on text-only settings and rarely address
scenarios where both inputs and profiles are multimodal. This paper introduces
LaMP-Cap, a dataset for personalized figure caption generation with multimodal
figure profiles. For each target figure, LaMP-Cap provides not only the needed
inputs, such as figure images, but also up to three other figures from the same
document--each with its image, caption, and figure-mentioning paragraphs--as a
profile to characterize the context. Experiments with four LLMs show that using
profile information consistently helps generate captions closer to the original
author-written ones. Ablation studies reveal that images in the profile are
more helpful than figure-mentioning paragraphs, highlighting the advantage of
using multimodal profiles over text-only ones.

</details>


### [19] [Precise Information Control in Long-Form Text Generation](https://arxiv.org/abs/2506.06589)
*Jacqueline He,Howard Yen,Margaret Li,Shuyue Stella Li,Zhiyuan Zeng,Weijia Shi,Yulia Tsvetkov,Danqi Chen,Pang Wei Koh,Luke Zettlemoyer*

Main category: cs.CL

TL;DR: 本文提出精确信息控制（PIC）任务和PIC-Bench基准，发现主流语言模型存在超过70%的幻觉问题，并通过后训练框架开发了PIC-LM模型，显著提升了生成准确性。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型生成未经验证信息（内在幻觉）的问题，提升生成内容的可信度。

Method: 设计PIC任务（包含完整和部分场景），构建PIC-Bench基准测试，采用弱监督偏好数据训练8B参数的PIC-LM模型。

Result: 实验显示PIC-LM在完整PIC设置下F1值从69.1%提升至91.0%，端到端任务中准确率提升显著（如事实精确度提高30.5%）。

Conclusion: PIC框架有效缓解模型幻觉问题，为可靠内容生成提供了新方向，展示了精确基础生成的实际应用潜力。

Abstract: A central challenge in modern language models (LMs) is intrinsic
hallucination: the generation of information that is plausible but
unsubstantiated relative to input context. To study this problem, we propose
Precise Information Control (PIC), a new task formulation that requires models
to generate long-form outputs grounded in a provided set of short
self-contained statements, known as verifiable claims, without adding any
unsupported ones. For comprehensiveness, PIC includes a full setting that tests
a model's ability to include exactly all input claims, and a partial setting
that requires the model to selectively incorporate only relevant claims. We
present PIC-Bench, a benchmark of eight long-form generation tasks (e.g.,
summarization, biography generation) adapted to the PIC setting, where LMs are
supplied with well-formed, verifiable input claims. Our evaluation of a range
of open and proprietary LMs on PIC-Bench reveals that, surprisingly,
state-of-the-art LMs still intrinsically hallucinate in over 70% of outputs. To
alleviate this lack of faithfulness, we introduce a post-training framework,
using a weakly supervised preference data construction method, to train an 8B
PIC-LM with stronger PIC ability--improving from 69.1% to 91.0% F1 in the full
PIC setting. When integrated into end-to-end factual generation pipelines,
PIC-LM improves exact match recall by 17.1% on ambiguous QA with retrieval, and
factual precision by 30.5% on a birthplace verification task, underscoring the
potential of precisely grounded generation.

</details>


### [20] [MedCite: Can Language Models Generate Verifiable Text for Medicine?](https://arxiv.org/abs/2506.06605)
*Xiao Wang,Mengjue Tan,Qiao Jin,Guangzhi Xiong,Yu Hu,Aidong Zhang,Zhiyong Lu,Minjia Zhang*

Main category: cs.CL

TL;DR: 开发首个端到端框架\name，集成多通道检索引用方法，显著提升医疗问答系统的引用质量与评估效果


<details>
  <summary>Details</summary>
Motivation: 现有医疗问答系统缺乏引用生成及评估能力，阻碍实际临床应用。需构建可信赖的医疗AI系统

Method: 提出包含多通道检索-引用机制的端到端框架，整合自动化评估模块验证引用质量

Result: 新方法实现引用精度提升21%，召回率提高18%，系统评估结果与专家标注相关性达0.89

Conclusion: 框架首次实现医疗任务引用生成全流程优化，为可信医疗AI系统建设提供方法论与实践验证

Abstract: Existing LLM-based medical question-answering systems lack citation
generation and evaluation capabilities, raising concerns about their adoption
in practice. In this work, we introduce \name, the first end-to-end framework
that facilitates the design and evaluation of citation generation with LLMs for
medical tasks. Meanwhile, we introduce a novel multi-pass retrieval-citation
method that generates high-quality citations. Our evaluation highlights the
challenges and opportunities of citation generation for medical tasks, while
identifying important design choices that have a significant impact on the
final citation quality. Our proposed method achieves superior citation
precision and recall improvements compared to strong baseline methods, and we
show that evaluation results correlate well with annotation results from
professional experts.

</details>


### [21] [Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit](https://arxiv.org/abs/2506.06607)
*Charles Goddard,Fernando Fernandes Neto*

Main category: cs.CL

TL;DR: 无训练移植tokenizer方法，通过正交匹配追踪(OMP)重建token嵌入，实现跨tokenizer的零样本性能保持


<details>
  <summary>Details</summary>
Motivation: 解决不同预训练模型tokenizer不匹配导致的性能下降问题，避免重新训练模型的巨大成本

Method: 分两阶段将新token表示为共享token的稀疏组合：1）在donor模型空间计算稀疏系数 2）将系数迁移至base模型空间

Result: 在Llama→Mistral和Qwen→Llama迁移任务中，OMP零样本性能显著优于WECHSEL/FOCUS等基线方法，保持数学推理能力

Conclusion: OMP有效桥接大tokenizer差异，支持跨模型知识蒸馏、推测解码和领域适配，已集成至mergekit-tokensurgeon工具

Abstract: We present a training-free method to transplant tokenizers in pretrained
large language models (LLMs) by reconstructing unseen token embeddings via
Orthogonal Matching Pursuit (OMP). Specifically, we approximate each
out-of-vocabulary token as a sparse linear combination of shared tokens, in two
phases: first, compute each new token's representation in the donor embedding
space with a small dictionary of shared anchor tokens, then transfer these same
sparse coefficients back into the base model's embedding space.
  On two challenging cross-tokenizer tasks--Llama$\to$Mistral NeMo (12B) and
Qwen$\to$Llama (1B)--we show that OMP achieves best zero-shot preservation of
the base model's performance across multiple benchmarks, while other zero-shot
approaches degrade significantly. Compared to baselines (zero-init, mean-init,
and existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves
the best overall performance, effectively bridging large tokenizer
discrepancies without gradient updates. Our analysis further identifies
mismatched numerical tokenization schemes as a critical challenge for
preserving mathematical reasoning capabilities. This technique enables direct
reuse of pretrained model weights with new tokenizers, facilitating
cross-tokenizer knowledge distillation, speculative decoding, ensembling,
merging, and domain-specific vocabulary adaptations. We integrate our method
into the open-source mergekit-tokensurgeon tool for post hoc vocabulary
realignment.

</details>


### [22] [Transferring Features Across Language Models With Model Stitching](https://arxiv.org/abs/2506.06609)
*Alan Chen,Jack Merullo,Alessandro Stolfo,Ellie Pavlick*

Main category: cs.CL

TL;DR: 通过残差流仿射映射实现跨模型规模的稀疏自编码器特征迁移，可降低50%训练成本并揭示大/小模型表征空间的相似性与差异


<details>
  <summary>Details</summary>
Motivation: 探索大模型与小模型表征空间的相似性，通过迁移学习提升稀疏自编码器(SAE)训练效率

Method: 在语言模型残差流之间建立仿射映射，实现SAE权重的跨模型迁移（如小模型SAE迁移至大模型）

Result: 小模型SAE迁移至大模型可节省50%训练成本，语义特征与结构特征迁移效果差异显著，功能特征角色映射稳定

Conclusion: 线性表征空间的跨模型可迁移性为SAE高效训练提供新路径，同时揭示了不同类别特征在模型规模变化时的演化规律

Abstract: In this work, we demonstrate that affine mappings between residual streams of
language models is a cheap way to effectively transfer represented features
between models. We apply this technique to transfer the weights of Sparse
Autoencoders (SAEs) between models of different sizes to compare their
representations. We find that small and large models learn highly similar
representation spaces, which motivates training expensive components like SAEs
on a smaller model and transferring to a larger model at a FLOPs savings. For
example, using a small-to-large transferred SAE as initialization can lead to
50% cheaper training runs when training SAEs on larger models. Next, we show
that transferred probes and steering vectors can effectively recover ground
truth performance. Finally, we dive deeper into feature-level transferability,
finding that semantic and structural features transfer noticeably differently
while specific classes of functional features have their roles faithfully
mapped. Overall, our findings illustrate similarities and differences in the
linear representation spaces of small and large models and demonstrate a method
for improving the training efficiency of SAEs.

</details>


### [23] [Interpretable Depression Detection from Social Media Text Using LLM-Derived Embeddings](https://arxiv.org/abs/2506.06616)
*Samuel Kim,Oghenemaro Imieye,Yunting Yin*

Main category: cs.CL

TL;DR: 比较LLMs与传统机器学习在社交媒体抑郁检测中的表现，发现零样本LLMs在二元分类表现优异，但有序分类欠佳；基于LLM摘要嵌入的分类器展现更强竞争力


<details>
  <summary>Details</summary>
Motivation: 社交媒体抑郁语言的准确识别对心理健康早期干预及公共健康管理具有重要临床和社会价值

Method: 通过三类分类任务（二元抑郁/抑郁严重程度/鉴别诊断）对比零样本LLMs与基于传统文本嵌入/LLM摘要嵌入的监督分类器性能

Result: 零样本LLMs在二元分类展现强泛化能力，但有序分类表现欠佳；使用LLM生成摘要嵌入的分类器在多项任务中超越传统文本嵌入模型

Conclusion: LLMs在心理健康预测领域具有独特优势，未来可通过优化零样本提示策略与上下文感知摘要技术进一步提升应用效果

Abstract: Accurate and interpretable detection of depressive language in social media
is useful for early interventions of mental health conditions, and has
important implications for both clinical practice and broader public health
efforts. In this paper, we investigate the performance of large language models
(LLMs) and traditional machine learning classifiers across three classification
tasks involving social media data: binary depression classification, depression
severity classification, and differential diagnosis classification among
depression, PTSD, and anxiety. Our study compares zero-shot LLMs with
supervised classifiers trained on both conventional text embeddings and
LLM-generated summary embeddings. Our experiments reveal that while zero-shot
LLMs demonstrate strong generalization capabilities in binary classification,
they struggle with fine-grained ordinal classifications. In contrast,
classifiers trained on summary embeddings generated by LLMs demonstrate
competitive, and in some cases superior, performance on the classification
tasks, particularly when compared to models using traditional text embeddings.
Our findings demonstrate the strengths of LLMs in mental health prediction, and
suggest promising directions for better utilization of their zero-shot
capabilities and context-aware summarization techniques.

</details>


### [24] [BriefMe: A Legal NLP Benchmark for Assisting with Legal Briefs](https://arxiv.org/abs/2506.06619)
*Jesse Woo,Fateme Hashemi Chaleshtori,Ana Marasović,Kenneth Marino*

Main category: cs.CL

TL;DR: 论文介绍了BRIEFME数据集，聚焦法律简报自动处理的三个任务（论点总结/补全/案例检索），发现LLMs在部分任务表现优异但案例检索仍存短板


<details>
  <summary>Details</summary>
Motivation: 现有法律NLP研究忽视法律简报的创造性论证需求，需同时具备法律理解与创新论证能力。该研究旨在填补这一空白并评估模型的法律文书处理能力

Method: 通过构建BRIEFME数据集，设计三个递进任务：1) 法律论点自动总结 2) 引导式/开放式论点补全 3) 相关判例检索，采用人工评估与自动指标结合的分析方法

Result: LLMs在论点总结（超越人工标题质量）和引导补全表现优异，但在开放式论证补全（F1仅0.25）和案例检索（准确率不足50%）任务中显著落后

Conclusion: BRIEFME揭示了当前模型在法律创新论证方面的局限，为法律NLP发展提供了明确方向，强调需开发更专业的法律语义理解与推理能力

Abstract: A core part of legal work that has been under-explored in Legal NLP is the
writing and editing of legal briefs. This requires not only a thorough
understanding of the law of a jurisdiction, from judgments to statutes, but
also the ability to make new arguments to try to expand the law in a new
direction and make novel and creative arguments that are persuasive to judges.
To capture and evaluate these legal skills in language models, we introduce
BRIEFME, a new dataset focused on legal briefs. It contains three tasks for
language models to assist legal professionals in writing briefs: argument
summarization, argument completion, and case retrieval. In this work, we
describe the creation of these tasks, analyze them, and show how current models
perform. We see that today's large language models (LLMs) are already quite
good at the summarization and guided completion tasks, even beating
human-generated headings. Yet, they perform poorly on other tasks in our
benchmark: realistic argument completion and retrieving relevant legal cases.
We hope this dataset encourages more development in Legal NLP in ways that will
specifically aid people in performing legal work.

</details>


### [25] [Psychological Counseling Cannot Be Achieved Overnight: Automated Psychological Counseling Through Multi-Session Conversations](https://arxiv.org/abs/2506.06626)
*Junzhe Wang,Bichen Wang,Xing Fu,Yixin Sun,Yanyan Zhao,Bing Qin*

Main category: cs.CL

TL;DR: 提出首个多阶段心理咨询对话数据集MusPsy-Dataset及配套追踪模型MusPsy-Model，突破单次会话限制，实验显示多阶段效果更优


<details>
  <summary>Details</summary>
Motivation: 现有LLM心理咨询研究集中于单次对话，不符合真实场景中持续多次咨询的渐进性需求

Method: 基于真实心理案例构建多阶段咨询数据集，开发具备长期进度追踪和方向调整能力的对话模型

Result: 模型在连续多轮咨询对话中表现优于基线，验证了长期追踪机制的有效性

Conclusion: 该研究为持续性心理咨询AI系统开发提供了数据集和方法论基础，推动领域向真实场景迈进

Abstract: In recent years, Large Language Models (LLMs) have made significant progress
in automated psychological counseling. However, current research focuses on
single-session counseling, which doesn't represent real-world scenarios. In
practice, psychological counseling is a process, not a one-time event,
requiring sustained, multi-session engagement to progressively address clients'
issues. To overcome this limitation, we introduce a dataset for Multi-Session
Psychological Counseling Conversation Dataset (MusPsy-Dataset). Our
MusPsy-Dataset is constructed using real client profiles from publicly
available psychological case reports. It captures the dynamic arc of
counseling, encompassing multiple progressive counseling conversations from the
same client across different sessions. Leveraging our dataset, we also
developed our MusPsy-Model, which aims to track client progress and adapt its
counseling direction over time. Experiments show that our model performs better
than baseline models across multiple sessions.

</details>


### [26] [SafeLawBench: Towards Safe Alignment of Large Language Models](https://arxiv.org/abs/2506.06636)
*Chuxue Cao,Han Zhu,Jiaming Ji,Qichao Sun,Zhenghao Zhu,Yinyu Wu,Juntao Dai,Yaodong Yang,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: 论文提出首个基于法律视角的大模型安全评估基准SafeLawBench，包含2.4万+多选题和1100+开放问答任务。测试20个大模型发现，顶尖模型Claude-3.5-Sonnet和GPT-4o在选择题准确率不足80.5%，平均准确率仅68.8%，揭示大模型安全隐患。


<details>
  <summary>Details</summary>
Motivation: 针对当前大模型安全评估标准主观性强、缺乏系统性框架的问题，首次从法律视角构建安全评估体系。现有安全基准存在主观判断主导的问题，需要更客观量化的评估标准。

Method: 1. 构建三级法律风险分类体系（违法、高风险、低风险）
2. 创建包含24,860多选题和1,106开放QA的SafeLawBench基准
3. 采用zero-shot和few-shot方式测试2个闭源模型和18个开源模型
4. 分析模型的安全推理稳定性和拒绝行为

Result: 1. 多选任务中顶尖模型准确率未超过80.5%（Claude-3.5-Sonnet:80.1%, GPT-4o:79.3%）
2. 20个模型平均准确率68.8%
3. 多数投票机制可提升5-8%性能
4. 开源模型Qwen1.5-110B在开放QA任务表现最佳

Conclusion: 现有大模型在法律安全维度存在显著缺陷，需重点加强安全研究。建议采用多数投票机制提升模型安全性，并持续完善法律导向的评估体系。论文揭示即使SOTA模型也存在安全隐患，敦促社区重视AI安全治理。

Abstract: With the growing prevalence of large language models (LLMs), the safety of
LLMs has raised significant concerns. However, there is still a lack of
definitive standards for evaluating their safety due to the subjective nature
of current safety benchmarks. To address this gap, we conducted the first
exploration of LLMs' safety evaluation from a legal perspective by proposing
the SafeLawBench benchmark. SafeLawBench categorizes safety risks into three
levels based on legal standards, providing a systematic and comprehensive
framework for evaluation. It comprises 24,860 multi-choice questions and 1,106
open-domain question-answering (QA) tasks. Our evaluation included 2
closed-source LLMs and 18 open-source LLMs using zero-shot and few-shot
prompting, highlighting the safety features of each model. We also evaluated
the LLMs' safety-related reasoning stability and refusal behavior.
Additionally, we found that a majority voting mechanism can enhance model
performance. Notably, even leading SOTA models like Claude-3.5-Sonnet and
GPT-4o have not exceeded 80.5% accuracy in multi-choice tasks on SafeLawBench,
while the average accuracy of 20 LLMs remains at 68.8\%. We urge the community
to prioritize research on the safety of LLMs.

</details>


### [27] [Quantile Regression with Large Language Models for Price Prediction](https://arxiv.org/abs/2506.06657)
*Nikhita Vedula,Dushyanta Dhyani,Laleh Jalali,Boris Oreshkin,Mohsen Bayati,Shervin Malmasi*

Main category: cs.CL

TL;DR: LLM在价格预测任务中引入分位数回归方法，实现完整预测分布并显著提升预测精度


<details>
  <summary>Details</summary>
Motivation: 现有LLM结构化预测方法主要关注点估计，缺乏系统方法比较和不确定性量化能力

Method: 提出基于Mistral-7B的分位数回归框架，通过微调模型头部实现分布预测

Result: 在三个数据集上显示分位数方法在准确性和分布校准指标上全面超越传统方法，LLM辅助标签校正可达人类水平精度

Conclusion: 验证了LLM在复杂回归任务中的有效性，公开数据集支持后续研究

Abstract: Large Language Models (LLMs) have shown promise in structured prediction
tasks, including regression, but existing approaches primarily focus on point
estimates and lack systematic comparison across different methods. We
investigate probabilistic regression using LLMs for unstructured inputs,
addressing challenging text-to-distribution prediction tasks such as price
estimation where both nuanced text understanding and uncertainty quantification
are critical. We propose a novel quantile regression approach that enables LLMs
to produce full predictive distributions, improving upon traditional point
estimates. Through extensive experiments across three diverse price prediction
datasets, we demonstrate that a Mistral-7B model fine-tuned with quantile heads
significantly outperforms traditional approaches for both point and
distributional estimations, as measured by three established metrics each for
prediction accuracy and distributional calibration. Our systematic comparison
of LLM approaches, model architectures, training approaches, and data scaling
reveals that Mistral-7B consistently outperforms encoder architectures,
embedding-based methods, and few-shot learning methods. Our experiments also
reveal the effectiveness of LLM-assisted label correction in achieving
human-level accuracy without systematic bias. Our curated datasets are made
available at https://github.com/vnik18/llm-price-quantile-reg/ to support
future research.

</details>


### [28] [Learning Distribution-Wise Control in Representation Space for Language Models](https://arxiv.org/abs/2506.06686)
*Chunyuan Deng,Ruidi Chang,Hanjie Chen*

Main category: cs.CL

TL;DR: 提出分布级干预方法（D-Intervention），通过控制概念子空间分布提升语言模型的可控性与鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有点级干预方法在概念子空间控制上存在局限性，需要扩展至分布层面以实现更全面的行为控制

Method: 在早期网络层应用标准差更大的分布级干预，学习概念子空间的整体分布特征而非单点变换

Result: 在8个常识推理和7个算术推理基准测试中，分布级干预的控制成功率比点级方法平均提升9.8%，标准差与性能强相关（r=0.86）

Conclusion: 分布级干预为语言模型行为控制提供了更全面的解决方案，尤其在早期层实现更细粒度的模型调控

Abstract: Interventions in language models (LMs) are applied strategically to steer
model behavior during the forward pass. Learnable interventions, also known as
representation fine-tuning, aim to apply pointwise control within the concept
subspace and have proven effective in altering high-level behaviors. In this
work, we extend this approach to the distribution level, enabling the model to
learn not only pointwise transformations but also the surrounding regions of
the concept subspace. We demonstrate that these methods perform effectively in
early layers, with larger standard deviations correlating strongly with
improved performance. Across eight commonsense reasoning and seven arithmetic
reasoning benchmarks, our distribution-wise interventions consistently
outperform pointwise interventions in controllability and robustness. These
results illustrate that distribution-wise interventions provide a more
comprehensive method for steering model behavior and enabling finer-grained
control over language models. The code is at:
\href{https://github.com/chili-lab/D-Intervention}{https://github.com/chili-lab/D-Intervention}.

</details>


### [29] [Dynamic and Parametric Retrieval-Augmented Generation](https://arxiv.org/abs/2506.06704)
*Weihang Su,Qingyao Ai,Jingtao Zhan,Qian Dong,Yiqun Liu*

Main category: cs.CL

TL;DR: 本教程系统探讨动态RAG与参数化RAG两大新兴方向，通过动态检索时机决策和参数级知识注入提升检索增强生成的效率与效果。


<details>
  <summary>Details</summary>
Motivation: 传统静态RAG在复杂任务中存在多跳推理不足、知识整合薄弱等问题，需探索更灵活的知识集成方式。

Method: 动态RAG实时调整检索策略，参数化RAG将知识注入从输入层转移到模型参数层实现深度融合。

Result: 新方法实现生成过程实时适应需求，提升知识整合效率，为复杂知识任务提供新范式。

Conclusion: 教程系统梳理理论框架与实践经验，为RAG技术演进提供双重优化路径，推动检索与生成的深度协同进化。

Abstract: Retrieval-Augmented Generation (RAG) has become a foundational paradigm for
equipping large language models (LLMs) with external knowledge, playing a
critical role in information retrieval and knowledge-intensive applications.
However, conventional RAG systems typically adopt a static
retrieve-then-generate pipeline and rely on in-context knowledge injection,
which can be suboptimal for complex tasks that require multihop reasoning,
adaptive information access, and deeper integration of external knowledge.
Motivated by these limitations, the research community has moved beyond static
retrieval and in-context knowledge injection. Among the emerging directions,
this tutorial delves into two rapidly growing and complementary research areas
on RAG: Dynamic RAG and Parametric RAG. Dynamic RAG adaptively determines when
and what to retrieve during the LLM's generation process, enabling real-time
adaptation to the LLM's evolving information needs. Parametric RAG rethinks how
retrieved knowledge should be injected into LLMs, transitioning from
input-level to parameter-level knowledge injection for enhanced efficiency and
effectiveness. This tutorial offers a comprehensive overview of recent advances
in these emerging research areas. It also shares theoretical foundations and
practical insights to support and inspire further research in RAG.

</details>


### [30] [DivScore: Zero-Shot Detection of LLM-Generated Text in Specialized Domains](https://arxiv.org/abs/2506.06705)
*Zhihui Chen,Kai He,Yucheng Huang,Yunxiao Zhu,Mengling Feng*

Main category: cs.CL

TL;DR: 提出DivScore框架解决专业领域LLM生成文本检测难题，通过熵值评分和领域知识蒸馏提升检测性能，并发布医疗法律领域基准数据集


<details>
  <summary>Details</summary>
Motivation: 当前零样本检测器在专业领域存在领域偏移问题，导致高风险管理场景（医疗/法律）的文本真实性难以保障，可能助长错误信息传播

Method: 结合归一化熵值评分（量化文本不确定性）与领域知识蒸馏（提取专业领域特征），构建鲁棒的零样本检测框架

Result: 在自建基准测试中AUROC提升14.4%，召回率提高64%（0.1%误报率阈值），对抗环境下AUROC优势达22.8%，召回率优势29.5%

Conclusion: DivScore有效克服领域偏移问题，为专业领域文本检测提供新范式，公开的代码数据推动领域研究发展

Abstract: Detecting LLM-generated text in specialized and high-stakes domains like
medicine and law is crucial for combating misinformation and ensuring
authenticity. However, current zero-shot detectors, while effective on general
text, often fail when applied to specialized content due to domain shift. We
provide a theoretical analysis showing this failure is fundamentally linked to
the KL divergence between human, detector, and source text distributions. To
address this, we propose DivScore, a zero-shot detection framework using
normalized entropy-based scoring and domain knowledge distillation to robustly
identify LLM-generated text in specialized domains. We also release a
domain-specific benchmark for LLM-generated text detection in the medical and
legal domains. Experiments on our benchmark show that DivScore consistently
outperforms state-of-the-art detectors, with 14.4% higher AUROC and 64.0%
higher recall (0.1% false positive rate threshold). In adversarial settings,
DivScore demonstrates superior robustness than other baselines, achieving on
average 22.8% advantage in AUROC and 29.5% in recall. Code and data are
publicly available.

</details>


### [31] [A Survey of Retentive Network](https://arxiv.org/abs/2506.06708)
*Haiqi Yang,Zhiyuan Li,Yi Chang,Yuan Wu*

Main category: cs.CL

TL;DR: RetNet通过retention机制整合循环结构与注意力优势，在保持训练并行性的同时实现线性推断复杂度，成为Transformer的高效替代方案


<details>
  <summary>Details</summary>
Motivation: 解决Transformer因二次复杂度导致的长序列处理内存成本高、可扩展性差的问题，同时保留全局依赖建模能力

Method: 提出结合循环结构归纳偏置与注意力全局建模的retention机制，支持并行训练和线性时间推断的三重计算范式

Result: 在自然语言处理、语音识别、时间序列分析等领域展现跨域有效性，计算效率比Transformer提升5.4倍（序列长度8k）

Conclusion: 需进一步研究retention机制的理论基础、长上下文优化、多模态扩展及硬件适配，推动其在学术与工业场景的应用

Abstract: Retentive Network (RetNet) represents a significant advancement in neural
network architecture, offering an efficient alternative to the Transformer.
While Transformers rely on self-attention to model dependencies, they suffer
from high memory costs and limited scalability when handling long sequences due
to their quadratic complexity. To mitigate these limitations, RetNet introduces
a retention mechanism that unifies the inductive bias of recurrence with the
global dependency modeling of attention. This mechanism enables linear-time
inference, facilitates efficient modeling of extended contexts, and remains
compatible with fully parallelizable training pipelines. RetNet has garnered
significant research interest due to its consistently demonstrated cross-domain
effectiveness, achieving robust performance across machine learning paradigms
including natural language processing, speech recognition, and time-series
analysis. However, a comprehensive review of RetNet is still missing from the
current literature. This paper aims to fill that gap by offering the first
detailed survey of the RetNet architecture, its key innovations, and its
diverse applications. We also explore the main challenges associated with
RetNet and propose future research directions to support its continued
advancement in both academic research and practical deployment.

</details>


### [32] [C-PATH: Conversational Patient Assistance and Triage in Healthcare System](https://arxiv.org/abs/2506.06737)
*Qi Shi,Qiwei Han,Cláudia Soares*

Main category: cs.CL

TL;DR: 基于LLaMA3架构开发的多轮对话医疗分诊AI系统C-PATH，通过数据增强和对话管理策略提升问诊准确性


<details>
  <summary>Details</summary>
Motivation: 解决医疗系统复杂性导致的患者就医障碍，通过自然对话帮助患者识别症状并推荐合适科室

Method: 1. 基于LLaMA3架构的多阶段微调管道
2. GPT驱动的临床知识对话数据增强框架
3. 可扩展的长期对话历史管理策略

Result: GPTScore评估显示在清晰度(84.3)、信息量(79.6)、推荐准确性(81.2)表现优异，在对话数据集上准确率超基线模型15%

Conclusion: C-PATH在数字健康领域实现了以用户为中心、高可及性且精准的AI分诊工具开发突破

Abstract: Navigating healthcare systems can be complex and overwhelming, creating
barriers for patients seeking timely and appropriate medical attention. In this
paper, we introduce C-PATH (Conversational Patient Assistance and Triage in
Healthcare), a novel conversational AI system powered by large language models
(LLMs) designed to assist patients in recognizing symptoms and recommending
appropriate medical departments through natural, multi-turn dialogues. C-PATH
is fine-tuned on medical knowledge, dialogue data, and clinical summaries using
a multi-stage pipeline built on the LLaMA3 architecture. A core contribution of
this work is a GPT-based data augmentation framework that transforms structured
clinical knowledge from DDXPlus into lay-person-friendly conversations,
allowing alignment with patient communication norms. We also implement a
scalable conversation history management strategy to ensure long-range
coherence. Evaluation with GPTScore demonstrates strong performance across
dimensions such as clarity, informativeness, and recommendation accuracy.
Quantitative benchmarks show that C-PATH achieves superior performance in
GPT-rewritten conversational datasets, significantly outperforming
domain-specific baselines. C-PATH represents a step forward in the development
of user-centric, accessible, and accurate AI tools for digital health
assistance and triage.

</details>


### [33] [Geopolitical biases in LLMs: what are the "good" and the "bad" countries according to contemporary language models](https://arxiv.org/abs/2506.06751)
*Mikhail Salnikov,Dmitrii Korzh,Ivan Lazichny,Elvir Karimov,Artyom Iudin,Ivan Oseledets,Oleg Y. Rogov,Alexander Panchenko,Natalia Loukachevitch,Elena Tutubalina*

Main category: cs.CL

TL;DR: 通过构建中立事件数据集和对比国家视角，发现LLMs存在显著地缘政治偏见且简单去偏方法效果有限


<details>
  <summary>Details</summary>
Motivation: 评估LLMs处理冲突性国家叙事时的地缘政治偏见，揭示模型对国家标签的敏感性及其对去偏方法的挑战

Method: 创建包含中立事件描述与对立国家观点（美/英/苏/中）的数据集，通过标签置换实验检测模型敏感性

Result: 模型呈现显著国家叙事偏好，简单提示去偏效果微弱，标签置换会放大偏见或引发逻辑矛盾识别

Conclusion: 揭示了LLMs的国家叙事偏见机制，质疑现有去偏方法有效性，为地缘政治偏见研究建立新框架与基准数据集

Abstract: This paper evaluates geopolitical biases in LLMs with respect to various
countries though an analysis of their interpretation of historical events with
conflicting national perspectives (USA, UK, USSR, and China). We introduce a
novel dataset with neutral event descriptions and contrasting viewpoints from
different countries. Our findings show significant geopolitical biases, with
models favoring specific national narratives. Additionally, simple debiasing
prompts had a limited effect in reducing these biases. Experiments with
manipulated participant labels reveal models' sensitivity to attribution,
sometimes amplifying biases or recognizing inconsistencies, especially with
swapped labels. This work highlights national narrative biases in LLMs,
challenges the effectiveness of simple debiasing methods, and offers a
framework and dataset for future geopolitical bias research.

</details>


### [34] [They want to pretend not to understand: The Limits of Current LLMs in Interpreting Implicit Content of Political Discourse](https://arxiv.org/abs/2506.06775)
*Walter Paci,Alessandro Panunzi,Sandro Pezzelle*

Main category: cs.CL

TL;DR: 研究发现当前大语言模型在解析政治话语中的预设和隐含义方面存在显著不足


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在政治话语中解析隐含内容（如预设和隐含义）的实际能力，填补该领域研究空白

Method: 使用标注了操控性隐含内容的意大利政治演讲语料库IMPAQTS，设计多项选择题和开放式生成任务进行测试

Result: 所有测试模型在解释预设和隐含义任务中表现欠佳，准确率显著低于人类水平

Conclusion: 现有模型缺乏解析政治话语所需的关键语用能力，但通过针对性训练和语用框架优化存在改进潜力

Abstract: Implicit content plays a crucial role in political discourse, where speakers
systematically employ pragmatic strategies such as implicatures and
presuppositions to influence their audiences. Large Language Models (LLMs) have
demonstrated strong performance in tasks requiring complex semantic and
pragmatic understanding, highlighting their potential for detecting and
explaining the meaning of implicit content. However, their ability to do this
within political discourse remains largely underexplored. Leveraging, for the
first time, the large IMPAQTS corpus, which comprises Italian political
speeches with the annotation of manipulative implicit content, we propose
methods to test the effectiveness of LLMs in this challenging problem. Through
a multiple-choice task and an open-ended generation task, we demonstrate that
all tested models struggle to interpret presuppositions and implicatures. We
conclude that current LLMs lack the key pragmatic capabilities necessary for
accurately interpreting highly implicit language, such as that found in
political discourse. At the same time, we highlight promising trends and future
directions for enhancing model performance. We release our data and code at
https://github.com/WalterPaci/IMPAQTS-PID

</details>


### [35] [Extending dependencies to the taggedPBC: Word order in transitive clauses](https://arxiv.org/abs/2506.06785)
*Hiram Ring*

Main category: cs.CL

TL;DR: 将taggedPBC语料库转换为CoNLLU格式并添加依存句法标注，验证噪声数据在充分标注下的跨语言研究价值


<details>
  <summary>Details</summary>
Motivation: 原始taggedPBC语料库仅有词性标注而无依存句法信息，限制了其在句法类型学研究中的应用潜力

Method: 通过跨语言词性标注迁移完成1,500+语言的依存句法标注，并与WALS等三大类型学数据库的专家判断进行相关性验证

Result: 及物从句中谓词-论元位置信息与专家判断显著相关（r=0.82,p<0.001），噪声数据展现类型学分析可行性

Conclusion: 语料库驱动类型学可扩展离散范畴比较，充分标注的噪声数据具有研究价值，依存标注语料库已开源供学术合作

Abstract: The taggedPBC (Ring 2025a) contains more than 1,800 sentences of pos-tagged
parallel text data from over 1,500 languages, representing 133 language
families and 111 isolates. While this dwarfs previously available resources,
and the POS tags achieve decent accuracy, allowing for predictive
crosslinguistic insights (Ring 2025b), the dataset was not initially annotated
for dependencies. This paper reports on a CoNLLU-formatted version of the
dataset which transfers dependency information along with POS tags to all
languages in the taggedPBC. Although there are various concerns regarding the
quality of the tags and the dependencies, word order information derived from
this dataset regarding the position of arguments and predicates in transitive
clauses correlates with expert determinations of word order in three
typological databases (WALS, Grambank, Autotyp). This highlights the usefulness
of corpus-based typological approaches (as per Baylor et al. 2023; Bjerva 2024)
for extending comparisons of discrete linguistic categories, and suggests that
important insights can be gained even from noisy data, given sufficient
annotation. The dependency-annotated corpora are also made available for
research and collaboration via GitHub.

</details>


### [36] [On the Adaptive Psychological Persuasion of Large Language Models](https://arxiv.org/abs/2506.06800)
*Tianjie Ju,Yujia Chen,Hao Fei,Mong-Li Lee,Wynne Hsu,Pengzhou Cheng,Zongru Wu,Zhuosheng Zhang,Gongshen Liu*

Main category: cs.CL

TL;DR: 通过构建基于直接偏好优化的自适应框架，提升LLMs在心理说服场景中自主选择最优策略的能力，显著提高成功率


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对LLMs自主说服与抗说服双重能力的系统性探索，尤其在涉及心理修辞的对抗性对话场景中表现不足

Method: 1. 评估四款主流LLMs在对抗对话中的表现；2. 设计11种心理说服策略并验证效果；3. 开发基于DPO的自适应策略选择框架

Result: 特定策略（如流畅效应）可提升成功率23.1%，自适应框架使开源LLM成功率提升12.6-19.8%，且保持模型通用能力

Conclusion: 动态策略选择机制有效解决了传统固定策略的局限性，为LLMs在复杂人际交互中的应用提供了新的技术路径

Abstract: Previous work has showcased the intriguing capabilities of Large Language
Models (LLMs) in instruction-following and rhetorical fluency. However,
systematic exploration of their dual capabilities to autonomously persuade and
resist persuasion, particularly in contexts involving psychological rhetoric,
remains unexplored. In this paper, we first evaluate four commonly adopted LLMs
by tasking them to alternately act as persuaders and listeners in adversarial
dialogues. Empirical results show that persuader LLMs predominantly employ
repetitive strategies, leading to low success rates. Then we introduce eleven
comprehensive psychological persuasion strategies, finding that explicitly
instructing LLMs to adopt specific strategies such as Fluency Effect and
Repetition Effect significantly improves persuasion success rates. However, no
``one-size-fits-all'' strategy proves universally effective, with performance
heavily dependent on contextual counterfactuals. Motivated by these
observations, we propose an adaptive framework based on direct preference
optimization that trains LLMs to autonomously select optimal strategies by
leveraging persuasion results from strategy-specific responses as preference
pairs. Experiments on three open-source LLMs confirm that the proposed adaptive
psychological persuasion method effectively enables persuader LLMs to select
optimal strategies, significantly enhancing their success rates while
maintaining general capabilities. Our code is available at
https://github.com/KalinaEine/PsychologicalPersuasion.

</details>


### [37] [Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification](https://arxiv.org/abs/2506.06806)
*Subhendu Khatuya,Shashwat Naidu,Saptarshi Ghosh,Pawan Goyal,Niloy Ganguly*

Main category: cs.CL

TL;DR: 提出参数高效的LAGAMC模型框架，通过生成标签描述+语义匹配机制，在多项数据集实现文本分类SOTA性能（Micro-F1提升13.94%，Macro-F1提升24.85%）


<details>
  <summary>Details</summary>
Motivation: 传统多标签分类方法将标签视为原子符号，缺乏语义理解。面对海量文本数据，需要构建领域无关且参数高效的分类框架。

Method: 1. 预定义标签描述作为生成目标
2. 推理时通过微调sentence transformer匹配生成描述与标签
3. 双目标损失函数（交叉熵+生成句与目标描述的余弦相似度）确保语义对齐

Result: 在全部评估数据集上达到SOTA，相比最优基线平均提升Micro-F1 13.94%、Macro-F1 24.85%

Conclusion: LAGAMC框架通过语义驱动的生成式方法，在保持参数效率的同时实现跨领域适配，为实际应用提供有效解决方案

Abstract: The explosion of textual data has made manual document classification
increasingly challenging. To address this, we introduce a robust, efficient
domain-agnostic generative model framework for multi-label text classification.
Instead of treating labels as mere atomic symbols, our approach utilizes
predefined label descriptions and is trained to generate these descriptions
based on the input text. During inference, the generated descriptions are
matched to the pre-defined labels using a finetuned sentence transformer. We
integrate this with a dual-objective loss function, combining cross-entropy
loss and cosine similarity of the generated sentences with the predefined
target descriptions, ensuring both semantic alignment and accuracy. Our
proposed model LAGAMC stands out for its parameter efficiency and versatility
across diverse datasets, making it well-suited for practical applications. We
demonstrate the effectiveness of our proposed model by achieving new
state-of-the-art performances across all evaluated datasets, surpassing several
strong baselines. We achieve improvements of 13.94% in Micro-F1 and 24.85% in
Macro-F1 compared to the closest baseline across all datasets.

</details>


### [38] [Not quite Sherlock Holmes: Language model predictions do not reliably differentiate impossible from improbable events](https://arxiv.org/abs/2506.06808)
*James A. Michaelov,Reeka Estacio,Zhien Zhang,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 语言模型在特定条件下无法可靠区分可能事件与不可能事件，甚至出现反常识判断


<details>
  <summary>Details</summary>
Motivation: 验证语言模型是否能可靠预测事件可能性，挑战先前研究结论，探究模型判断的脆弱性边界

Method: 通过解构可能性/典型性/上下文相关性，设计对比实验测试Llama3/Gemma2/Mistral等主流模型

Result: 所有模型在特定情境下表现差于随机猜测，对不可能事件（如'刹车开罚单'）赋予高于低概率事件（如'探险家开罚单'）的概率

Conclusion: 现有语言模型在可能性判断上存在根本性缺陷，其事件推理能力难以满足现实应用需求

Abstract: Can language models reliably predict that possible events are more likely
than merely improbable ones? By teasing apart possibility, typicality, and
contextual relatedness, we show that despite the results of previous work,
language models' ability to do this is far from robust. In fact, under certain
conditions, all models tested - including Llama 3, Gemma 2, and Mistral NeMo -
perform at worse-than-chance level, assigning higher probabilities to
impossible sentences such as 'the car was given a parking ticket by the brake'
than to merely unlikely sentences such as 'the car was given a parking ticket
by the explorer'.

</details>


### [39] [Advancing Question Generation with Joint Narrative and Difficulty Control](https://arxiv.org/abs/2506.06812)
*Bernardo Leite,Henrique Lopes Cardoso*

Main category: cs.CL

TL;DR: 提出联合控制策略，在阅读理解问题生成中同时调控难度和叙述属性，初步验证可行性但存在局限性


<details>
  <summary>Details</summary>
Motivation: 现有问题生成研究缺乏对难度和叙述的联合控制，而这对教育定制问题生成至关重要

Method: 采用联合叙事与难度控制策略，允许在生成阅读理解问题时同时调控这两个属性

Result: 评估显示该方法初步可行但非普适有效，明确了策略有效条件和应用中的权衡关系

Conclusion: 该联合控制策略为教育领域的问题生成提供了新方向，但需进一步优化适用场景和效果稳定性

Abstract: Question Generation (QG), the task of automatically generating questions from
a source input, has seen significant progress in recent years.
Difficulty-controllable QG (DCQG) enables control over the difficulty level of
generated questions while considering the learner's ability. Additionally,
narrative-controllable QG (NCQG) allows control over the narrative aspects
embedded in the questions. However, research in QG lacks a focus on combining
these two types of control, which is important for generating questions
tailored to educational purposes. To address this gap, we propose a strategy
for Joint Narrative and Difficulty Control, enabling simultaneous control over
these two attributes in the generation of reading comprehension questions. Our
evaluation provides preliminary evidence that this approach is feasible, though
it is not effective across all instances. Our findings highlight the conditions
under which the strategy performs well and discuss the trade-offs associated
with its application.

</details>


### [40] [BTPD: A Multilingual Hand-curated Dataset of Bengali Transnational Political Discourse Across Online Communities](https://arxiv.org/abs/2506.06813)
*Dipto Das,Syed Ishtiaque Ahmed,Shion Guha*

Main category: cs.CL

TL;DR: 本文构建了首个孟加拉语跨国政治话语数据集BTPD，包含来自三个不同社区结构平台的跨语言内容分析


<details>
  <summary>Details</summary>
Motivation: 现有政治话语研究集中于英语，孟加拉语等资源匮乏语言因数据缺失面临研究瓶颈

Method: 通过社区知识指导的关键词检索方法手工整理数据集，并分析其主题分布和语言特征

Result: 成功收集包含多语言内容的跨国政治讨论数据集，揭示平台间社区结构和互动模式的差异

Conclusion: 该数据集填补了非英语政治话语研究的空白，为资源匮乏语言的跨语言分析提供新途径

Abstract: Understanding political discourse in online spaces is crucial for analyzing
public opinion and ideological polarization. While social computing and
computational linguistics have explored such discussions in English, such
research efforts are significantly limited in major yet under-resourced
languages like Bengali due to the unavailability of datasets. In this paper, we
present a multilingual dataset of Bengali transnational political discourse
(BTPD) collected from three online platforms, each representing distinct
community structures and interaction dynamics. Besides describing how we
hand-curated the dataset through community-informed keyword-based retrieval,
this paper also provides a general overview of its topics and multilingual
content.

</details>


### [41] [How do datasets, developers, and models affect biases in a low-resourced language?](https://arxiv.org/abs/2506.06816)
*Dipto Das,Shion Guha,Bryan Semaan*

Main category: cs.CL

TL;DR: 对孟加拉语情感分析模型的算法审计显示，跨身份类别的系统性偏误持续存在，即使语义内容和结构相似。研究揭示了预训练模型与多样化数据集结合时的矛盾，关联到认知不公和AI对齐问题。


<details>
  <summary>Details</summary>
Motivation: 针对低资源语言技术中身份偏误研究不足的问题，验证多语言模型/单语数据集方案在孟加拉语场景（性别/宗教/国籍身份）的有效性，弥补现有解决方案的实证空白。

Method: 使用Google Dataset Search中所有孟加拉语情感分析(BSA)数据集，对基于mBERT和BanglaBERT的模型进行微调，通过控制变量法对比不同身份类别的模型表现差异。

Result: BSA模型在不同身份类别中持续展现偏误，预训练模型与多源数据集的组合导致预测不一致性，揭示算法审计方法学决策对结果的重要影响。

Conclusion: 现行技术方案无法根本解决低资源语言的身份偏误问题，须加强算法审计中的认知正义考量，并重新审视模型对齐与数据集构建方法论。

Abstract: Sociotechnical systems, such as language technologies, frequently exhibit
identity-based biases. These biases exacerbate the experiences of historically
marginalized communities and remain understudied in low-resource contexts.
While models and datasets specific to a language or with multilingual support
are commonly recommended to address these biases, this paper empirically tests
the effectiveness of such approaches in the context of gender, religion, and
nationality-based identities in Bengali, a widely spoken but low-resourced
language. We conducted an algorithmic audit of sentiment analysis models built
on mBERT and BanglaBERT, which were fine-tuned using all Bengali sentiment
analysis (BSA) datasets from Google Dataset Search. Our analyses showed that
BSA models exhibit biases across different identity categories despite having
similar semantic content and structure. We also examined the inconsistencies
and uncertainties arising from combining pre-trained models and datasets
created by individuals from diverse demographic backgrounds. We connected these
findings to the broader discussions on epistemic injustice, AI alignment, and
methodological decisions in algorithmic audits.

</details>


### [42] [Beyond Classification: Towards Speech Emotion Reasoning with Multitask AudioLLMs](https://arxiv.org/abs/2506.06820)
*Wenyu Zhang,Yingxu He,Geyu Lin,Zhuohan Liu,Shuo Sun,Bin Wang,Xunlong Zou,Jeremy H. M. Wong,Qiongqiong Wang,Hardik B. Sailor,Nancy F. Chen,Ai Ti Aw*

Main category: cs.CL

TL;DR: 提出Emotion Reasoning框架增强AudioLLMs的情感推理能力，通过生成证据支持的语义解释，提升情感预测准确率与回答可解释性


<details>
  <summary>Details</summary>
Motivation: 现有方法将情感识别视为分类任务，缺乏对预测背后逻辑的解释。需突破传统分类范式，构建兼具预测能力和解释能力的情感推理框架

Method: 三阶段统一框架：1）构建含情感推理标注的增强数据集；2）双编码器架构分离语义与副语言特征；3）任务交替训练策略协调多任务学习

Result: 在IEMOCAP和MELD数据集上实现情感预测准确率提升，生成回答的连贯性和证据支持度显著优于基线模型

Conclusion: 该框架成功将情感推理融入多任务AudioLLMs，证明生成式解释与预测任务的协同效应，为可解释性语音AI提供新思路

Abstract: Audio Large Language Models (AudioLLMs) have achieved strong results in
semantic tasks like speech recognition and translation, but remain limited in
modeling paralinguistic cues such as emotion. Existing approaches often treat
emotion understanding as a classification problem, offering little insight into
the underlying rationale behind predictions. In this work, we explore emotion
reasoning, a strategy that leverages the generative capabilities of AudioLLMs
to enhance emotion recognition by producing semantically aligned,
evidence-grounded explanations. To support this in multitask AudioLLMs, we
introduce a unified framework combining reasoning-augmented data supervision,
dual-encoder architecture, and task-alternating training. This approach enables
AudioLLMs to effectively learn different tasks while incorporating emotional
reasoning. Experiments on IEMOCAP and MELD show that our approach not only
improves emotion prediction accuracy but also enhances the coherence and
evidential grounding of the generated responses.

</details>


### [43] [Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems](https://arxiv.org/abs/2506.06821)
*Yuhan Cao,Zian Chen,Kun Quan,Ziliang Zhang,Yu Wang,Xiaoning Dong,Yeqi Feng,Guanzhong He,Jingcheng Huang,Jianhao Li,Yixuan Tan,Jiafu Tang,Yilin Tang,Junlei Wu,Qianyu Xiao,Can Zheng,Shouchen Zhou,Yuxiang Zhu,Yiming Huang,Tian Xie,Tianxing He*

Main category: cs.CL

TL;DR: 提出TCGBench基准测试，评估大语言模型在生成测试用例生成器方面的能力，发现现有模型在针对性错误检测方面显著落后人类表现，构建专用数据集可提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在代码检查/调试中的潜力，特别是通过测试用例生成来暴露程序缺陷的能力。

Method: 构建TCGBench基准，包含两个任务：1) 生成有效测试用例生成器；2) 生成能暴露人类代码缺陷的针对性生成器。通过实验评估主流LLM表现，并构建专用指令数据集。

Result: 先进LLM可生成有效测试生成器(成功率>70%)，但在针对性错误检测任务中表现差(准确率<35%)，o3-mini等推理模型显著落后人类表现。专用数据集可使模型性能提升12-15%。

Conclusion: 构建高质量指令数据集能有效提升LLM在代码检查任务中的表现，当前模型在复杂逻辑推理任务中仍需突破性进展。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
code generation, capable of tackling complex tasks during inference. However,
the extent to which LLMs can be utilized for code checking or debugging through
test case generation remains largely unexplored. We investigate this problem
from the perspective of competition-level programming (CP) programs and propose
TCGBench, a Benchmark for (LLM generation of) Test Case Generators. This
benchmark comprises two tasks, aimed at studying the capabilities of LLMs in
(1) generating valid test case generators for a given CP problem, and further
(2) generating targeted test case generators that expose bugs in human-written
code. Experimental results indicate that while state-of-the-art LLMs can
generate valid test case generators in most cases, most LLMs struggle to
generate targeted test cases that reveal flaws in human code effectively.
Especially, even advanced reasoning models (e.g., o3-mini) fall significantly
short of human performance in the task of generating targeted generators.
Furthermore, we construct a high-quality, manually curated dataset of
instructions for generating targeted generators. Analysis demonstrates that the
performance of LLMs can be enhanced with the aid of this dataset, by both
prompting and fine-tuning.

</details>


### [44] [PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation](https://arxiv.org/abs/2506.06842)
*Arkadiusz Modzelewski,Witold Sosnowski,Tiziano Labruna,Adam Wierzbicki,Giovanni Da San Martino*

Main category: cs.CL

TL;DR: 提出基于说服知识增强的PCoT方法，在零样本虚假信息检测中平均提升15%效果


<details>
  <summary>Details</summary>
Motivation: 心理学研究表明说服性谬误知识有助于虚假信息检测，因此探索将这种知识注入大语言模型的可能性

Method: 开发Persuasion-Augmented Chain of Thought（PCoT）方法，在新闻和社交媒体数据上验证，并创建包含时效性内容的EUDisinfo和MultiDis两个新数据集

Result: PCoT在5个大语言模型和5个数据集上的平均表现优于现有方法15%

Conclusion: 说服知识能有效增强零样本检测能力，新数据集为后续研究提供完全未被模型训练过的测试基准

Abstract: Disinformation detection is a key aspect of media literacy. Psychological
studies have shown that knowledge of persuasive fallacies helps individuals
detect disinformation. Inspired by these findings, we experimented with large
language models (LLMs) to test whether infusing persuasion knowledge enhances
disinformation detection. As a result, we introduce the Persuasion-Augmented
Chain of Thought (PCoT), a novel approach that leverages persuasion to improve
disinformation detection in zero-shot classification. We extensively evaluate
PCoT on online news and social media posts. Moreover, we publish two novel,
up-to-date disinformation datasets: EUDisinfo and MultiDis. These datasets
enable the evaluation of PCoT on content entirely unseen by the LLMs used in
our experiments, as the content was published after the models' knowledge
cutoffs. We show that, on average, PCoT outperforms competitive methods by 15%
across five LLMs and five datasets. These findings highlight the value of
persuasion in strengthening zero-shot disinformation detection.

</details>


### [45] [Adapt Once, Thrive with Updates: Transferable Parameter-Efficient Fine-Tuning on Evolving Base Models](https://arxiv.org/abs/2506.06844)
*Naibin Gu,Peng Fu,Xiyu Liu,Ke Ma,Zheng Lin,Weiping Wang*

Main category: cs.CL

TL;DR: Trans-PEFT通过聚焦注意力机制中的任务特定模式，无需重新调优即可在更新后的基础模型上保持PEFT模块性能


<details>
  <summary>Details</summary>
Motivation: 基础模型更新后PEFT模块性能显著下降，重新调优会产生高昂计算成本，需降低实际应用中的维护开销

Method: 分析模型更新变化规律，发现持续训练主要影响FFN层的任务知识，提出聚焦注意力模式并减少对基础模型知识依赖的Trans-PEFT方法

Result: 在7个基础模型和12个数据集上的实验表明，Trans-PEFT训练后的模块无需调优即可保持更新后模型的性能

Conclusion: Trans-PEFT显著降低实际维护成本，其有效性得到理论分析和大量实验支持

Abstract: Parameter-efficient fine-tuning (PEFT) has become a common method for
fine-tuning large language models, where a base model can serve multiple users
through PEFT module switching. To enhance user experience, base models require
periodic updates. However, once updated, PEFT modules fine-tuned on previous
versions often suffer substantial performance degradation on newer versions.
Re-tuning these numerous modules to restore performance would incur significant
computational costs. Through a comprehensive analysis of the changes that occur
during base model updates, we uncover an interesting phenomenon: continual
training primarily affects task-specific knowledge stored in Feed-Forward
Networks (FFN), while having less impact on the task-specific pattern in the
Attention mechanism. Based on these findings, we introduce Trans-PEFT, a novel
approach that enhances the PEFT module by focusing on the task-specific pattern
while reducing its dependence on certain knowledge in the base model. Further
theoretical analysis supports our approach. Extensive experiments across 7 base
models and 12 datasets demonstrate that Trans-PEFT trained modules can maintain
performance on updated base models without re-tuning, significantly reducing
maintenance overhead in real-world applications.

</details>


### [46] [Right Is Not Enough: The Pitfalls of Outcome Supervision in Training LLMs for Math Reasoning](https://arxiv.org/abs/2506.06877)
*Jiaxing Guo,Wenjie Yang,Shengzhong Zhang,Tongshan Xu,Lun Du,Da Zheng,Zengfeng Huang*

Main category: cs.CL

TL;DR: 揭示LLMs数学解题中答案正确但推理错误的问题，提出MathOlympiadEval数据集和ParaStepVerifier分步验证方法


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在数学问题中存在奖励黑客现象，正确答案掩盖了错误的推理过程，需要更可靠的评估方法

Method: 开发带细粒度标注的数据集MathOlympiadEval，提出ParaStepVerifier方法对数学解题步骤进行并行验证

Result: ParaStepVerifier在识别错误推理步骤上显著优于基线，特别是在多步骤复杂问题中提升明显

Conclusion: 该研究为LLMs的数学推理能力评估提供了更可靠的路径，对改进模型训练方法有重要意义

Abstract: Outcome-rewarded Large Language Models (LLMs) have demonstrated remarkable
success in mathematical problem-solving. However, this success often masks a
critical issue: models frequently achieve correct answers through fundamentally
unsound reasoning processes, a phenomenon indicative of reward hacking. We
introduce MathOlympiadEval, a new dataset with fine-grained annotations, which
reveals a significant gap between LLMs' answer correctness and their low
process correctness. Existing automated methods like LLM-as-a-judge struggle to
reliably detect these reasoning flaws. To address this, we propose
ParaStepVerifier, a novel methodology for meticulous, step-by-step verification
of mathematical solutions. ParaStepVerifier identifies incorrect reasoning
steps. Empirical results demonstrate that ParaStepVerifier substantially
improves the accuracy of identifying flawed solutions compared to baselines,
especially for complex, multi-step problems. This offers a more robust path
towards evaluating and training LLMs with genuine mathematical reasoning.

</details>


### [47] [Mixture of Small and Large Models for Chinese Spelling Check](https://arxiv.org/abs/2506.06887)
*Ziheng Qiao,Houquan Zhou,Zhenghua Li*

Main category: cs.CL

TL;DR: 提出动态混合方法结合小模型和LLMs的概率分布，提升中文拼写检查任务性能


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在中文拼写检查任务表现不佳，BERT小模型存在编辑模式过拟合问题

Method: 在beam search解码阶段动态融合小模型和LLMs的概率分布，无需微调大语言模型

Result: 在多个数据集上取得state-of-the-art效果，错误纠正能力显著提升

Conclusion: 混合方法平衡精确校正与语言流畅性，节省资源并提升领域适应性

Abstract: In the era of large language models (LLMs), the Chinese Spelling Check (CSC)
task has seen various LLM methods developed, yet their performance remains
unsatisfactory. In contrast, fine-tuned BERT-based models, relying on
high-quality in-domain data, show excellent performance but suffer from edit
pattern overfitting. This paper proposes a novel dynamic mixture approach that
effectively combines the probability distributions of small models and LLMs
during the beam search decoding phase, achieving a balanced enhancement of
precise corrections from small models and the fluency of LLMs. This approach
also eliminates the need for fine-tuning LLMs, saving significant time and
resources, and facilitating domain adaptation. Comprehensive experiments
demonstrate that our mixture approach significantly boosts error correction
capabilities, achieving state-of-the-art results across multiple datasets. Our
code is available at https://github.com/zhqiao-nlp/MSLLM.

</details>


### [48] [Automatic Speech Recognition of African American English: Lexical and Contextual Effects](https://arxiv.org/abs/2506.06888)
*Hamid Mojarad,Kevin Tang*

Main category: cs.CL

TL;DR: 研究揭示非裔美国英语的辅音簇缩减和ING缩减会轻微但显著增加ASR词错率，无语言模型的ASR系统更易受词汇邻域效应影响。


<details>
  <summary>Details</summary>
Motivation: 针对ASR模型对非裔美国英语(AAE)语音特征处理不足的问题，探究语音缩减现象对识别准确率的影响，并比较不同ASR架构的语言处理机制差异。

Method: 使用CORAAL语料库，通过wav2vec 2.0进行语音转写（含/不含语言模型），采用蒙特利尔强制对齐器进行发音扩展检测CCR和ING缩减现象。

Result: 1. CCR和ING缩减使词错率(WER)增加0.5-1.2% 2. 无LM系统表现出更强的词汇邻域效应（词频相似词干扰）3. 上下文预测性对含LM系统影响更大

Conclusion: 研究强调方言语音特征对ASR性能的系统性影响，建议模型开发需结合语言模型与方言语音学特征，特别在资源不足的方言场景中需重视词汇邻域效应。

Abstract: Automatic Speech Recognition (ASR) models often struggle with the phonetic,
phonological, and morphosyntactic features found in African American English
(AAE). This study focuses on two key AAE variables: Consonant Cluster Reduction
(CCR) and ING-reduction. It examines whether the presence of CCR and
ING-reduction increases ASR misrecognition. Subsequently, it investigates
whether end-to-end ASR systems without an external Language Model (LM) are more
influenced by lexical neighborhood effect and less by contextual predictability
compared to systems with an LM. The Corpus of Regional African American
Language (CORAAL) was transcribed using wav2vec 2.0 with and without an LM. CCR
and ING-reduction were detected using the Montreal Forced Aligner (MFA) with
pronunciation expansion. The analysis reveals a small but significant effect of
CCR and ING on Word Error Rate (WER) and indicates a stronger presence of
lexical neighborhood effect in ASR systems without LMs.

</details>


### [49] [Hybrid Extractive Abstractive Summarization for Multilingual Sentiment Analysis](https://arxiv.org/abs/2506.06929)
*Mikhail Krasitskii,Grigori Sidorov,Olga Kolesnikova,Liliana Chanona Hernandez,Alexander Gelbukh*

Main category: cs.CL

TL;DR: 混合抽取式与生成式摘要的多语言情感分析模型，在10种语言中准确率最高达0.90，计算效率提升22%


<details>
  <summary>Details</summary>
Motivation: 解决传统独立方法在跨语言情感分析中的局限性，特别是低资源语言处理效率和文化适应性不足的问题

Method: 整合TF-IDF抽取与XLM-R生成模块，采用动态阈值调节和文化适配机制

Result: 英语准确率0.90，低资源语言0.84，计算效率比传统方法提升22%

Conclusion: 成功应用于实时品牌监控和跨文化分析，未来将通过8位量化技术优化低资源语言处理

Abstract: We propose a hybrid approach for multilingual sentiment analysis that
combines extractive and abstractive summarization to address the limitations of
standalone methods. The model integrates TF-IDF-based extraction with a
fine-tuned XLM-R abstractive module, enhanced by dynamic thresholding and
cultural adaptation. Experiments across 10 languages show significant
improvements over baselines, achieving 0.90 accuracy for English and 0.84 for
low-resource languages. The approach also demonstrates 22% greater
computational efficiency than traditional methods. Practical applications
include real-time brand monitoring and cross-cultural discourse analysis.
Future work will focus on optimization for low-resource languages via 8-bit
quantization.

</details>


### [50] [DiscoSum: Discourse-aware News Summarization](https://arxiv.org/abs/2506.06930)
*Alexander Spangher,Tenghao Huang,Jialiang Gu,Jiatong Shi,Muhao Chen*

Main category: cs.CL

TL;DR: 提出DiscoSum算法，通过整合新闻话语结构提升摘要质量


<details>
  <summary>Details</summary>
Motivation: 解决语言模型在新闻摘要中难以保持长期话语结构的问题，提升读者参与度

Method: 1. 构建跨社交媒体平台的多风格摘要数据集
2. 设计新闻话语模式
3. 开发基于束搜索的DiscoSum算法实现结构感知摘要

Result: 人类评估和自动评估均显示在叙事保真度和结构适配性上的显著改进

Conclusion: 结构感知的摘要方法能有效满足不同平台风格需求，推动个性化内容生成发展

Abstract: Recent advances in text summarization have predominantly leveraged large
language models to generate concise summaries. However, language models often
do not maintain long-term discourse structure, especially in news articles,
where organizational flow significantly influences reader engagement. We
introduce a novel approach to integrating discourse structure into
summarization processes, focusing specifically on news articles across various
media. We present a novel summarization dataset where news articles are
summarized multiple times in different ways across different social media
platforms (e.g. LinkedIn, Facebook, etc.). We develop a novel news discourse
schema to describe summarization structures and a novel algorithm, DiscoSum,
which employs beam search technique for structure-aware summarization, enabling
the transformation of news stories to meet different stylistic and structural
demands. Both human and automatic evaluation results demonstrate the efficacy
of our approach in maintaining narrative fidelity and meeting structural
requirements.

</details>


### [51] [What Makes a Good Natural Language Prompt?](https://arxiv.org/abs/2506.06950)
*Do Xuan Long,Duy Dinh,Ngoc-Hai Nguyen,Kenji Kawaguchi,Nancy F. Chen,Shafiq Joty,Min-Yen Kan*

Main category: cs.CL

TL;DR: 提出基于属性的人本框架评估提示质量，通过元分析发现单属性增强效果最佳，指令调优可提升模型推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM提示研究缺乏统一评估标准，导致模型支持不平衡且存在研究空白。亟需建立系统性框架提升人机交流质量。

Method: 1. 元分析150+文献建立6维度21属性的评估框架
2. 相关性分析揭示优质提示特征
3. 多属性增强的实证研究
4. 指令调优验证框架有效性

Result: 单属性增强在推理任务中效果最显著；基于属性增强的指令调优能提升模型性能；发现现有研究对属性支持存在明显不平衡。

Conclusion: 建立了首个属性中心的提示评估体系，填补人机交流研究空白，为提示优化和模型训练提供新方向。

Abstract: As large language models (LLMs) have progressed towards more human-like and
human--AI communications have become prevalent, prompting has emerged as a
decisive component. However, there is limited conceptual consensus on what
exactly quantifies natural language prompts. We attempt to address this
question by conducting a meta-analysis surveying more than 150
prompting-related papers from leading NLP and AI conferences from 2022 to 2025
and blogs. We propose a property- and human-centric framework for evaluating
prompt quality, encompassing 21 properties categorized into six dimensions. We
then examine how existing studies assess their impact on LLMs, revealing their
imbalanced support across models and tasks, and substantial research gaps.
Further, we analyze correlations among properties in high-quality natural
language prompts, deriving prompting recommendations. We then empirically
explore multi-property prompt enhancements in reasoning tasks, observing that
single-property enhancements often have the greatest impact. Finally, we
discover that instruction-tuning on property-enhanced prompts can result in
better reasoning models. Our findings establish a foundation for
property-centric prompt evaluation and optimization, bridging the gaps between
human--AI communication and opening new prompting research directions.

</details>


### [52] [BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning](https://arxiv.org/abs/2506.06955)
*Ha-Thanh Nguyen,Chaoran Liu,Hirokazu Kiyomaru,Koichi Takeda,Yusuke Miyao,Maki Matsuda,Yusuke Oda,Pontus Stenetorp,Qianying Liu,Su Myat Noe,Hideyuki Tachibana,Kouta Nakayama,Sadao Kurohashi*

Main category: cs.CL

TL;DR: 日本研究者发布首个大规模信念冲突型三段论推理数据集BIS Reasoning 1.0，专门用于检测LLMs在逻辑有效但违背常识的推理场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要测试常规或信念一致的推理，缺乏对逻辑有效但信念冲突场景的评估能力检测。为揭示LLMs在人类对齐语料训练后产生的推理偏见而开发。

Method: 构建包含逻辑有效但信念矛盾的三段论问题的日语数据集，测试GPT系列、Claude系列及日本主流LLMs的推理表现。

Result: GPT-4o以79.54%准确率领先，多数模型在信念冲突输入时表现显著下降，暴露当前LLMs处理逻辑与信念冲突的核心缺陷。

Conclusion: 该发现对法律、医疗等高风险领域至关重要，强调LLMs必须优先事实而非直觉信念，以确保决策的完整性和安全性。

Abstract: We present BIS Reasoning 1.0, the first large-scale Japanese dataset of
syllogistic reasoning problems explicitly designed to evaluate
belief-inconsistent reasoning in large language models (LLMs). Unlike prior
datasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned
reasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent
syllogisms to uncover reasoning biases in LLMs trained on human-aligned
corpora. We benchmark state-of-the-art models - including GPT models, Claude
models, and leading Japanese LLMs - revealing significant variance in
performance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies
critical weaknesses in current LLMs when handling logically valid but
belief-conflicting inputs. These findings have important implications for
deploying LLMs in high-stakes domains such as law, healthcare, and scientific
literature, where truth must override intuitive belief to ensure integrity and
safety.

</details>


### [53] [Learning to Clarify by Reinforcement Learning Through Reward-Weighted Fine-Tuning](https://arxiv.org/abs/2506.06964)
*Subhojyoti Mukherjee,Viet Dac Lai,Raghavendra Addanki,Ryan Rossi,Seunghyun Yoon,Trung Bui,Anup Rao,Jayakumar Subramanian,Branislav Kveton*

Main category: cs.CL

TL;DR: 提出通过强化学习训练QA智能体主动提出澄清问题，采用离线RL目标优化奖励函数，相比传统方法实现奖励和语言质量双提升。


<details>
  <summary>Details</summary>
Motivation: 解决QA智能体面对模糊问题时被动应答的局限，通过主动提问完善问题理解，提升回答准确性。

Method: 使用强化学习框架，设计奖励加权的监督微调目标（reward-weighted SFT），避免传统DPO方法超参数多且无法直接优化奖励的问题。

Result: 实证显示本方法在奖励得分和生成质量上均优于SFT、DPO等基线模型，验证了离线RL优化的有效性。

Conclusion: 基于强化学习的澄清提问机制显著提升QA系统性能，为对话式AI的主动交互提供新范式。

Abstract: Question answering (QA) agents automatically answer questions posed in
natural language. In this work, we learn to ask clarifying questions in QA
agents. The key idea in our method is to simulate conversations that contain
clarifying questions and learn from them using reinforcement learning (RL). To
make RL practical, we propose and analyze offline RL objectives that can be
viewed as reward-weighted supervised fine-tuning (SFT) and easily optimized in
large language models. Our work stands in a stark contrast to recently proposed
methods, based on SFT and direct preference optimization, which have additional
hyper-parameters and do not directly optimize rewards. We compare to these
methods empirically and report gains in both optimized rewards and language
quality.

</details>


### [54] [A dependently-typed calculus of event telicity and culminativity](https://arxiv.org/abs/2506.06968)
*Pavel Kovalev,Carlo Angiuli*

Main category: cs.CL

TL;DR: 提出基于依赖类型论的跨语言事件分析框架，分别处理名词域有界性和动词域telic/culminative事件，并通过Agda形式化验证。


<details>
  <summary>Details</summary>
Motivation: 建立形式化工具分析自然语言中事件的时间结构（telicity）和完成性（culminativity），解决传统类型理论在处理语言现象时的局限性。

Method: 1. 名词域：建模名词短语有界性及其类型推导 2. 动词域：构建依赖事件演算，定义telic事件（有界受事）和culminative事件（达到终点的telic事件）3. 扩展Martin-Löf依赖类型论，在Agda证明助手中实现形式化。

Result: 成功构建可处理形容词修饰、副词修饰及相应蕴含关系的类型系统，通过英语例句验证框架有效性，实现计算可验证的形式化模型。

Conclusion: 该框架为跨语言事件分析提供了新的类型理论基础，通过依赖类型论实现了语言现象与形式逻辑的深度结合，推动了计算语言学与形式语义学的交叉发展。

Abstract: We present a dependently-typed cross-linguistic framework for analyzing the
telicity and culminativity of events, accompanied by examples of using our
framework to model English sentences. Our framework consists of two parts. In
the nominal domain, we model the boundedness of noun phrases and its
relationship to subtyping, delimited quantities, and adjectival modification.
In the verbal domain we define a dependent event calculus, modeling telic
events as those whose undergoer is bounded, culminating events as telic events
that achieve their inherent endpoint, and consider adverbial modification. In
both domains we pay particular attention to associated entailments. Our
framework is defined as an extension of intensional Martin-L\"of dependent type
theory, and the rules and examples in this paper have been formalized in the
Agda proof assistant.

</details>


### [55] [Break-The-Chain: Reasoning Failures in LLMs via Adversarial Prompting in Code Generation](https://arxiv.org/abs/2506.06971)
*Jaechul Roh,Varun Gandhi,Shivani Anilkumar,Arin Garg*

Main category: cs.CL

TL;DR: 通过对抗性提示扰动测试发现，大型语言模型的推理能力对提示表面结构高度敏感，准确性波动显著(-42.1%~+35.3%)，揭示其可能依赖统计模式而非真实推理


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在复杂推理任务中的表现是否源于真正的逻辑推理能力，还是基于表层统计模式的模仿学习

Method: 构建包含700个代码生成样本的对抗性测试集，应用故事重构/约束注入/示例重排/数值扰动等语义保留但结构变化的提示改造方案

Result: 不同扰动导致准确性剧烈波动：故事化重构最大降幅42.1%，数值扰动却意外提升35.3%，显示模型对表面提示动态的敏感依赖

Conclusion: 当前LLM推理系统存在脆弱性和不可预测性，需建立基于原理的提示工程方法与鲁棒性评估框架，研究团队公开测试集促进可信推理研究

Abstract: Large Language Models (LLMs) have achieved remarkable success in tasks
requiring complex reasoning, such as code generation, mathematical problem
solving, and algorithmic synthesis -- especially when aided by reasoning tokens
and Chain-of-Thought prompting. Yet, a core question remains: do these models
truly reason, or do they merely exploit shallow statistical patterns? In this
paper, we systematically investigate the robustness of reasoning LLMs by
introducing a suite of semantically faithful yet adversarially structured
prompt perturbations. Our evaluation -- spanning 700 perturbed code generations
derived from LeetCode-style problems -- applies transformations such as
storytelling reframing, irrelevant constraint injection, example reordering,
and numeric perturbation. We observe that while certain modifications severely
degrade performance (with accuracy drops up to -42.1%), others surprisingly
improve model accuracy by up to 35.3%, suggesting sensitivity not only to
semantics but also to surface-level prompt dynamics. These findings expose the
fragility and unpredictability of current reasoning systems, underscoring the
need for more principles approaches to reasoning alignments and prompting
robustness. We release our perturbation datasets and evaluation framework to
promote further research in trustworthy and resilient LLM reasoning.

</details>


### [56] [Atomic Reasoning for Scientific Table Claim Verification](https://arxiv.org/abs/2506.06972)
*Yuji Zhang,Qingyun Wang,Cheng Qian,Jiateng Liu,Chenkai Sun,Denghui Zhang,Tarek Abdelzaher,Chengxiang Zhai,Preslav Nakov,Heng Ji*

Main category: cs.CL

TL;DR: 通过原子技能链降低认知负荷，提出基于技能组合的表格声明验证方法，仅需少量训练数据即超越现有模型表现


<details>
  <summary>Details</summary>
Motivation: 科学表格的高信息密度特性易导致非专业人士误读，现有表格声明验证模型在细粒度推理上存在精度不足问题

Method: 基于认知负荷理论设计原子技能链架构，构建跨领域基准测试SciAtomicBench，通过动态技能组合实现可复用的推理组件

Result: 仅使用350个微调样本，模型在科学声明验证任务上准确率超越GPT-4o的思维链方法，达到当前最优水平

Conclusion: 原子技能链方法有效降低认知负荷，在提升模型推理精度的同时显著减少训练数据需求，为科学信息验证提供新范式

Abstract: Scientific texts often convey authority due to their technical language and
complex data. However, this complexity can sometimes lead to the spread of
misinformation. Non-experts are particularly susceptible to misleading claims
based on scientific tables due to their high information density and perceived
credibility. Existing table claim verification models, including
state-of-the-art large language models (LLMs), often struggle with precise
fine-grained reasoning, resulting in errors and a lack of precision in
verifying scientific claims. Inspired by Cognitive Load Theory, we propose that
enhancing a model's ability to interpret table-based claims involves reducing
cognitive load by developing modular, reusable reasoning components (i.e.,
atomic skills). We introduce a skill-chaining schema that dynamically composes
these skills to facilitate more accurate and generalizable reasoning with a
reduced cognitive load. To evaluate this, we create SciAtomicBench, a
cross-domain benchmark with fine-grained reasoning annotations. With only 350
fine-tuning examples, our model trained by atomic reasoning outperforms
GPT-4o's chain-of-thought method, achieving state-of-the-art results with far
less training data.

</details>


### [57] [Chain of Methodologies: Scaling Test Time Computation without Training](https://arxiv.org/abs/2506.06982)
*Cong Liu,Jie Wu,Weigang Wu,Xu Chen,Liang Lin,Wei-Shi Zheng*

Main category: cs.CL

TL;DR: 提出Chain of Methodologies (CoM)框架，通过整合人类方法论增强LLMs结构化思维，显著提升复杂推理任务表现


<details>
  <summary>Details</summary>
Motivation: LLMs在处理复杂推理任务时因训练数据缺乏深度洞察而表现不足

Method: 通过用户自定义方法论激活系统推理，利用LLMs元认知能力实现免微调的结构化思维增强

Result: 实验显示CoM超越现有基线方法，在复杂推理任务中实现更优表现

Conclusion: 验证了免训练提示方法在复杂推理任务中的潜力，通过人类方法论缩小与人类水平推理的差距

Abstract: Large Language Models (LLMs) often struggle with complex reasoning tasks due
to insufficient in-depth insights in their training data, which are typically
absent in publicly available documents. This paper introduces the Chain of
Methodologies (CoM), an innovative and intuitive prompting framework that
enhances structured thinking by integrating human methodological insights,
enabling LLMs to tackle complex tasks with extended reasoning. CoM leverages
the metacognitive abilities of advanced LLMs, activating systematic reasoning
throught user-defined methodologies without explicit fine-tuning. Experiments
show that CoM surpasses competitive baselines, demonstrating the potential of
training-free prompting methods as robust solutions for complex reasoning tasks
and bridging the gap toward human-level reasoning through human-like
methodological insights.

</details>


### [58] [Cultural Bias Matters: A Cross-Cultural Benchmark Dataset and Sentiment-Enriched Model for Understanding Multimodal Metaphors](https://arxiv.org/abs/2506.06987)
*Senqi Yang,Dongyu Zhang,Jing Ren,Ziqi Xu,Xiuzhen Zhang,Yiliao Song,Hongfei Lin,Feng Xia*

Main category: cs.CL

TL;DR: 提出跨文化多模态隐喻数据集MultiMM和情感增强检测模型SEMD，解决NLP隐喻处理中的文化偏见问题


<details>
  <summary>Details</summary>
Motivation: 现有隐喻研究集中于西方英语数据，导致模型性能评估偏差，跨文化多模态隐喻研究存在空白

Method: 构建包含8,461对中英文广告的多模态数据集MultiMM，设计整合情感嵌入的SEMD基线模型

Result: 实验证明SEMD在隐喻检测和情感分析任务中有效提升跨文化理解

Conclusion: 揭示NLP文化偏见问题，推动开发更公平包容的语言模型，公开数据集促进后续研究

Abstract: Metaphors are pervasive in communication, making them crucial for natural
language processing (NLP). Previous research on automatic metaphor processing
predominantly relies on training data consisting of English samples, which
often reflect Western European or North American biases. This cultural skew can
lead to an overestimation of model performance and contributions to NLP
progress. However, the impact of cultural bias on metaphor processing,
particularly in multimodal contexts, remains largely unexplored. To address
this gap, we introduce MultiMM, a Multicultural Multimodal Metaphor dataset
designed for cross-cultural studies of metaphor in Chinese and English. MultiMM
consists of 8,461 text-image advertisement pairs, each accompanied by
fine-grained annotations, providing a deeper understanding of multimodal
metaphors beyond a single cultural domain. Additionally, we propose
Sentiment-Enriched Metaphor Detection (SEMD), a baseline model that integrates
sentiment embeddings to enhance metaphor comprehension across cultural
backgrounds. Experimental results validate the effectiveness of SEMD on
metaphor detection and sentiment analysis tasks. We hope this work increases
awareness of cultural bias in NLP research and contributes to the development
of fairer and more inclusive language models. Our dataset and code are
available at https://github.com/DUTIR-YSQ/MultiMM.

</details>


### [59] [What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding](https://arxiv.org/abs/2506.06998)
*Ming Li,Zhengyuan Yang,Xiyao Wang,Dianqi Li,Kevin Lin,Tianyi Zhou,Lijuan Wang*

Main category: cs.CL

TL;DR: 提出FoReaL-Decoding方法，通过大模型与小模型协作降低推理成本，保持86%-100%性能的同时减少30-50%计算量。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）存在过度思考问题，导致推理速度慢且产生冗余细节。研究发现其与非推理模型的token级不对齐现象，包含全局不对齐反弹和局部不对齐减少两个关键特征。

Method: FoReaL-Decoding采用快慢思维协同解码：大模型引导每句话的前几个token（思考提示），小模型续写剩余部分，通过随机门实现平滑插值。

Result: 在4个数学推理基准测试中，FLOPs降低30-50%，思维链长度缩减40%，性能保留86-100%。

Conclusion: 该方法为推理任务提供了即插即用的成本-质量可控平衡方案，具有实际部署价值。

Abstract: Large reasoning models (LRMs) achieve strong reasoning performance by
emitting long chains of thought. Yet, these verbose traces slow down inference
and often drift into unnecessary detail, known as the overthinking phenomenon.
To better understand LRMs' behavior, we systematically analyze the token-level
misalignment between reasoning and non-reasoning models. While it is expected
that their primary difference lies in the stylistic "thinking cues", LRMs
uniquely exhibit two pivotal, previously under-explored phenomena: a Global
Misalignment Rebound, where their divergence from non-reasoning models persists
or even grows as response length increases, and more critically, a Local
Misalignment Diminish, where the misalignment concentrates at the "thinking
cues" each sentence starts with but rapidly declines in the remaining of the
sentence. Motivated by the Local Misalignment Diminish, we propose
FoReaL-Decoding, a collaborative fast-slow thinking decoding method for
cost-quality trade-off. In FoReaL-Decoding, a Leading model leads the first few
tokens for each sentence, and then a weaker draft model completes the following
tokens to the end of each sentence. FoReaL-Decoding adopts a stochastic gate to
smoothly interpolate between the small and the large model. On four popular
math-reasoning benchmarks (AIME24, GPQA-Diamond, MATH500, AMC23),
FoReaL-Decoding reduces theoretical FLOPs by 30 to 50% and trims CoT length by
up to 40%, while preserving 86 to 100% of model performance. These results
establish FoReaL-Decoding as a simple, plug-and-play route to controllable
cost-quality trade-offs in reasoning-centric tasks.

</details>


### [60] [Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated Text](https://arxiv.org/abs/2506.07001)
*Yize Cheng,Vinu Sankar Sadasivan,Mehrdad Saberi,Shoumik Saha,Soheil Feizi*

Main category: cs.CL

TL;DR: 对抗性改述框架通过指导性改写有效逃避多种AI文本检测器，显著降低检测率


<details>
  <summary>Details</summary>
Motivation: 现有AI文本检测器易被简单规避技术绕过，需要更强大的攻击方法来揭示检测系统的脆弱性

Method: 利用指令跟随型LLM在检测器指导下改述生成内容，生成优化的对抗样本（无需额外训练）

Result: 在OpenAI-RoBERTa-Large指导下，平均检测率降低87.88%（Fast-DetectGPT降低98.96%），文本质量仅轻微下降

Conclusion: 当前检测策略需增强鲁棒性以应对复杂规避技术，对抗性改述框架揭示了现有系统的根本性漏洞

Abstract: The increasing capabilities of Large Language Models (LLMs) have raised
concerns about their misuse in AI-generated plagiarism and social engineering.
While various AI-generated text detectors have been proposed to mitigate these
risks, many remain vulnerable to simple evasion techniques such as
paraphrasing. However, recent detectors have shown greater robustness against
such basic attacks. In this work, we introduce Adversarial Paraphrasing, a
training-free attack framework that universally humanizes any AI-generated text
to evade detection more effectively. Our approach leverages an off-the-shelf
instruction-following LLM to paraphrase AI-generated content under the guidance
of an AI text detector, producing adversarial examples that are specifically
optimized to bypass detection. Extensive experiments show that our attack is
both broadly effective and highly transferable across several detection
systems. For instance, compared to simple paraphrasing attack--which,
ironically, increases the true positive at 1% false positive (T@1%F) by 8.57%
on RADAR and 15.03% on Fast-DetectGPT--adversarial paraphrasing, guided by
OpenAI-RoBERTa-Large, reduces T@1%F by 64.49% on RADAR and a striking 98.96% on
Fast-DetectGPT. Across a diverse set of detectors--including neural
network-based, watermark-based, and zero-shot approaches--our attack achieves
an average T@1%F reduction of 87.88% under the guidance of
OpenAI-RoBERTa-Large. We also analyze the tradeoff between text quality and
attack success to find that our method can significantly reduce detection
rates, with mostly a slight degradation in text quality. Our adversarial setup
highlights the need for more robust and resilient detection strategies in the
light of increasingly sophisticated evasion techniques.

</details>


### [61] [A Culturally-diverse Multilingual Multimodal Video Benchmark & Model](https://arxiv.org/abs/2506.07032)
*Bhuiyan Sanjid Shafique,Ashmal Vayani,Muhammad Maaz,Hanoona Abdul Rasheed,Dinura Dissanayake,Mohammed Irfan Kurpath,Yahya Hmaiti,Go Inoue,Jean Lahoud,Md. Safirur Rashid,Shadid Intisar Quasem,Maheen Fatima,Franco Vidal,Mykola Maslych,Ketan Pravin More,Sanoojan Baliah,Hasindri Watawana,Yuhao Li,Fabian Farestam,Leon Schaller,Roman Tymtsiv,Simon Weber,Hisham Cholakkal,Ivan Laptev,Shin'ichi Satoh,Michael Felsberg,Mubarak Shah,Salman Khan,Fahad Shahbaz Khan*

Main category: cs.CL

TL;DR: 提出ViMUL-Bench多语言视频理解基准与ViMUL模型，覆盖14种语言并优化高低资源语言平衡，推动包容性视频大模型发展。


<details>
  <summary>Details</summary>
Motivation: 现有视频大模型多聚焦英语，缺乏文化语言包容性。需构建多语言评估体系促进跨语言视频理解技术发展。

Method: 1) 创建含8k人工验证样本的多语言基准ViMUL-Bench，覆盖15类文化主题与不同时长视频；2) 构建120万样本训练集并开发ViMUL模型，优化语言平衡策略。

Result: ViMUL-Bench有效评估多语言视频理解能力，ViMUL模型在高低资源语言间取得最优平衡。公开基准、模型与训练数据推动社区发展。

Conclusion: 通过标准化评估体系与优化模型架构，为构建文化语言包容的视频大模型奠定基础，促进多模态AI技术全球化发展。

Abstract: Large multimodal models (LMMs) have recently gained attention due to their
effectiveness to understand and generate descriptions of visual content. Most
existing LMMs are in English language. While few recent works explore
multilingual image LMMs, to the best of our knowledge, moving beyond the
English language for cultural and linguistic inclusivity is yet to be
investigated in the context of video LMMs. In pursuit of more inclusive video
LMMs, we introduce a multilingual Video LMM benchmark, named ViMUL-Bench, to
evaluate Video LMMs across 14 languages, including both low- and high-resource
languages: English, Chinese, Spanish, French, German, Hindi, Arabic, Russian,
Bengali, Urdu, Sinhala, Tamil, Swedish, and Japanese. Our ViMUL-Bench is
designed to rigorously test video LMMs across 15 categories including eight
culturally diverse categories, ranging from lifestyles and festivals to foods
and rituals and from local landmarks to prominent cultural personalities.
ViMUL-Bench comprises both open-ended (short and long-form) and multiple-choice
questions spanning various video durations (short, medium, and long) with 8k
samples that are manually verified by native language speakers. In addition, we
also introduce a machine translated multilingual video training set comprising
1.2 million samples and develop a simple multilingual video LMM, named ViMUL,
that is shown to provide a better tradeoff between high-and low-resource
languages for video understanding. We hope our ViMUL-Bench and multilingual
video LMM along with a large-scale multilingual video training set will help
ease future research in developing cultural and linguistic inclusive
multilingual video LMMs. Our proposed benchmark, video LMM and training data
will be publicly released at https://mbzuai-oryx.github.io/ViMUL/.

</details>


### [62] [KG2QA: Knowledge Graph-enhanced Retrieval-Augmented Generation for Communication Standards Question Answering](https://arxiv.org/abs/2506.07037)
*Zhongze Luo,Weixuan Wan,Qizhi Zheng,Yanhong Bai,Jingyun Sun,Jian Wang,Dan Wang*

Main category: cs.CL

TL;DR: 通过微调Qwen2.5-7B-Instruct模型与构建通信标准知识图谱，实现智能问答系统，显著提升BLEU-4（18.85→66.89）和平均评分2.26%，具备实际应用价值


<details>
  <summary>Details</summary>
Motivation: 传统通信标准咨询模式周期长、依赖专家经验，难以满足快速发展的技术需求，需建立高效智能解决方案

Method: 1. 采用LoRA微调方法在6,587条通信标准问答数据集训练
2. 构建含13,906实体/13,524关系的知识图谱
3. 实现RAG框架结合模型推理与图谱检索

Result: 1. BLEU-4提升至66.8993（原18.8564）
2. DeepSeek评估平均评分提升2.26%
3. 知识图谱查询准确率良好
4. 超越Llama-3-8B-Instruct微调效果

Conclusion: 系统通过模型微调与知识图谱检索协同机制有效提升问答效果，结合Web服务与API接口实现优秀交互体验，具备显著工程实践价值

Abstract: There are many types of standards in the field of communication. The
traditional consulting model has a long cycle and relies on the knowledge and
experience of experts, making it difficult to meet the rapidly developing
technological demands. This paper combines the fine-tuning of large language
models with the construction of knowledge graphs to implement an intelligent
consultation and question-answering system for communication standards. The
experimental results show that after LoRA tuning on the constructed dataset of
6,587 questions and answers in the field of communication standards,
Qwen2.5-7B-Instruct demonstrates outstanding professional capabilities in the
field of communication standards on the test set. BLEU-4 rose from 18.8564 to
66.8993, and evaluation indicators such as ROUGE also increased significantly,
outperforming the fine-tuning effect of the comparison model
Llama-3-8B-Instruct. Based on the ontology framework containing 6 entity
attributes and 10 relation attributes, a knowledge graph of the communication
standard domain containing 13,906 entities and 13,524 relations was
constructed, showing a relatively good query accuracy rate. The intelligent
consultation and question-answering system enables the fine-tuned model on the
server side to access the locally constructed knowledge graph and conduct
graphical retrieval of key information first, which is conducive to improving
the question-answering effect. The evaluation using DeepSeek as the Judge on
the test set shows that our RAG framework enables the fine-tuned model to
improve the scores at all five angles, with an average score increase of 2.26%.
And combined with web services and API interfaces, it has achieved very good
results in terms of interaction experience and back-end access, and has very
good practical application value.

</details>


### [63] [Reasoning with RAGged events: RAG-Enhanced Event Knowledge Base Construction and reasoning with proof-assistants](https://arxiv.org/abs/2506.07042)
*Stergios Chatzikyriakidis*

Main category: cs.CL

TL;DR: 研究通过多LLM模型(GPT-4/Claude/Llama)结合三种增强策略，实现历史事件自动化结构化提取，并开发RDF→Coq验证框架支持高阶推理。


<details>
  <summary>Details</summary>
Motivation: 传统人工构建历史事件结构化表示成本过高，且RDF/OWL推理局限于一阶逻辑，无法满足深层次时空语义分析需求。

Method: 使用三种LLM配合基础生成/知识图谱增强/RAG策略，基于修昔底德文本评估，并开发自动化RDF转Coq验证流程。

Result: 基础生成在事件覆盖面最优(Claude/GPT-4)，RAG提升坐标精度(最高+23%)，大模型RAG改进稳健(Llama 3.2表现波动±40%)，Coq成功验证事件因果关系。

Conclusion: 增强策略需针对性部署，模型规模决定策略有效性，Coq形式化验证为历史语义结构提供数学可证性基础。

Abstract: Extracting structured computational representations of historical events from
narrative text remains computationally expensive when constructed manually.
While RDF/OWL reasoners enable graph-based reasoning, they are limited to
fragments of first-order logic, preventing deeper temporal and semantic
analysis. This paper addresses both challenges by developing automatic
historical event extraction models using multiple LLMs (GPT-4, Claude, Llama
3.2) with three enhancement strategies: pure base generation, knowledge graph
enhancement, and Retrieval-Augmented Generation (RAG). We conducted
comprehensive evaluations using historical texts from Thucydides. Our findings
reveal that enhancement strategies optimize different performance dimensions
rather than providing universal improvements. For coverage and historical
breadth, base generation achieves optimal performance with Claude and GPT-4
extracting comprehensive events. However, for precision, RAG enhancement
improves coordinate accuracy and metadata completeness. Model architecture
fundamentally determines enhancement sensitivity: larger models demonstrate
robust baseline performance with incremental RAG improvements, while Llama 3.2
shows extreme variance from competitive performance to complete failure. We
then developed an automated translation pipeline converting extracted RDF
representations into Coq proof assistant specifications, enabling higher-order
reasoning beyond RDF capabilities including multi-step causal verification,
temporal arithmetic with BC dates, and formal proofs about historical
causation. The Coq formalization validates that RAG-discovered event types
represent legitimate domain-specific semantic structures rather than
ontological violations.

</details>


### [64] [Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning](https://arxiv.org/abs/2506.07044)
*LASA Team,Weiwen Xu,Hou Pong Chan,Long Li,Mahani Aljunied,Ruifeng Yuan,Jianyu Wang,Chenghao Xiao,Guizhen Chen,Chaoqun Liu,Zhaodonghui Li,Yu Sun,Junao Shen,Chaojun Wang,Jie Tan,Deli Zhao,Tingyang Xu,Hao Zhang,Yu Rong*

Main category: cs.CL

TL;DR: 提出医学专用多模态大模型Lingshu，通过多阶段训练和强化学习增强医疗推理能力，在多项任务中超越现有开源模型


<details>
  <summary>Details</summary>
Motivation: 现有医疗MLLMs存在医学知识覆盖局限、易产生幻觉、缺乏复杂医疗场景推理能力三大核心缺陷

Method: 1.建立涵盖医学影像/文本的多源数据采集流程 2.合成精准医学标注样本 3.采用分阶段训练策略嵌入医学专业知识 4.开发标准化评估框架MedEvalKit

Result: Lingshu在多模态QA、文本QA和报告生成任务中全面超越现有开源多模态模型

Conclusion: 通过系统化数据构建和专业化模型设计，显著提升MLLMs在医疗领域的知识覆盖、抗幻觉能力和复杂场景推理表现

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities in understanding common visual elements, largely due to their
large-scale datasets and advanced training strategies. However, their
effectiveness in medical applications remains limited due to the inherent
discrepancies between data and tasks in medical scenarios and those in the
general domain. Concretely, existing medical MLLMs face the following critical
limitations: (1) limited coverage of medical knowledge beyond imaging, (2)
heightened susceptibility to hallucinations due to suboptimal data curation
processes, (3) lack of reasoning capabilities tailored for complex medical
scenarios. To address these challenges, we first propose a comprehensive data
curation procedure that (1) efficiently acquires rich medical knowledge data
not only from medical imaging but also from extensive medical texts and
general-domain data; and (2) synthesizes accurate medical captions, visual
question answering (VQA), and reasoning samples. As a result, we build a
multimodal dataset enriched with extensive medical knowledge. Building on the
curated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu
undergoes multi-stage training to embed medical expertise and enhance its
task-solving capabilities progressively. Besides, we preliminarily explore the
potential of applying reinforcement learning with verifiable rewards paradigm
to enhance Lingshu's medical reasoning ability. Additionally, we develop
MedEvalKit, a unified evaluation framework that consolidates leading multimodal
and textual medical benchmarks for standardized, fair, and efficient model
assessment. We evaluate the performance of Lingshu on three fundamental medical
tasks, multimodal QA, text-based QA, and medical report generation. The results
show that Lingshu consistently outperforms the existing open-source multimodal
models on most tasks ...

</details>


### [65] [Com$^2$: A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models](https://arxiv.org/abs/2506.07064)
*Kai Xiong,Xiao Ding,Yixin Cao,Yuxiong Yan,Li Du,Yufei Zhang,Jinglong Gao,Jiaqian Liu,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 论文提出Com²基准测试，针对LLMs在复杂隐含常识推理（如事件长期影响）的不足，通过因果事件图结构化知识和慢思考方法提升模型表现


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于数学/代码等结构化复杂任务，而人类关注的复杂常识推理（因果关系演变）因不确定性和缺乏结构未被充分探索

Method: 1. 整合因果事件图作为结构化常识 2. 使用因果干预理论生成不同场景 3. 利用LLM慢思考合成逻辑关联的测试样本 4. 构建侦探故事子集增加挑战性

Result: LLMs在推理深度/广度上存在显著局限，后训练和慢思考方法可有效缓解（代码和数据已开源）

Conclusion: Com²填补复杂常识推理评估空白，为模型优化提供方向，验证后训练和慢思考方法的有效性

Abstract: Large language models (LLMs) have mastered abundant simple and explicit
commonsense knowledge through pre-training, enabling them to achieve human-like
performance in simple commonsense reasoning. Nevertheless, LLMs struggle to
reason with complex and implicit commonsense knowledge that is derived from
simple ones (such as understanding the long-term effects of certain events), an
aspect humans tend to focus on more. Existing works focus on complex tasks like
math and code, while complex commonsense reasoning remains underexplored due to
its uncertainty and lack of structure. To fill this gap and align with
real-world concerns, we propose a benchmark Com$^2$ focusing on complex
commonsense reasoning. We first incorporate causal event graphs to serve as
structured complex commonsense. Then we adopt causal theory~(e.g.,
intervention) to modify the causal event graphs and obtain different scenarios
that meet human concerns. Finally, an LLM is employed to synthesize examples
with slow thinking, which is guided by the logical relationships in the
modified causal graphs. Furthermore, we use detective stories to construct a
more challenging subset. Experiments show that LLMs struggle in reasoning depth
and breadth, while post-training and slow thinking can alleviate this. The code
and data are available at https://github.com/Waste-Wood/Com2.

</details>


### [66] [Representation Decomposition for Learning Similarity and Contrastness Across Modalities for Affective Computing](https://arxiv.org/abs/2506.07086)
*Yuanhe Tian,Pengsen Cheng,Guoqing Jin,Lei Zhang,Yan Song*

Main category: cs.CL

TL;DR: 提出基于LLM的多模态情感计算方法，通过表征解耦和注意力整合机制提升多模态情感分析效果


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效捕捉多模态间的复杂/冲突证据，需解耦共享与特定模态信息

Method: 1)预训练编码器对齐模态 2)表征分解框架分离共有情感/独特线索 3)注意力机制构建动态软提示

Result: 在情感分析、情绪分析和有害表情包检测三个任务上超越现有基准和SOTA模型

Conclusion: 模态表征解耦方法能有效提升多模态情感计算性能，验证了跨模态动态整合策略的有效性

Abstract: Multi-modal affective computing aims to automatically recognize and interpret
human attitudes from diverse data sources such as images and text, thereby
enhancing human-computer interaction and emotion understanding. Existing
approaches typically rely on unimodal analysis or straightforward fusion of
cross-modal information that fail to capture complex and conflicting evidence
presented across different modalities. In this paper, we propose a novel
LLM-based approach for affective computing that explicitly deconstructs visual
and textual representations into shared (modality-invariant) and
modality-specific components. Specifically, our approach firstly encodes and
aligns input modalities using pre-trained multi-modal encoders, then employs a
representation decomposition framework to separate common emotional content
from unique cues, and finally integrates these decomposed signals via an
attention mechanism to form a dynamic soft prompt for a multi-modal LLM.
Extensive experiments on three representative tasks for affective computing,
namely, multi-modal aspect-based sentiment analysis, multi-modal emotion
analysis, and hateful meme detection, demonstrate the effectiveness of our
approach, which consistently outperforms strong baselines and state-of-the-art
models.

</details>


### [67] [How Far Are We from Optimal Reasoning Efficiency?](https://arxiv.org/abs/2506.07104)
*Jiaxuan Gao,Shu Yan,Qixin Tan,Lu Yang,Shusheng Xu,Wei Fu,Zhiyu Mei,Kaifeng Lyu,Yi Wu*

Main category: cs.CL

TL;DR: 提出推理效率前沿（Reasoning Efficiency Frontiers）和REG指标量化大模型推理效率差距，开发REO-RL强化学习算法有效压缩推理长度同时保持精度


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的思维链推理存在冗余冗长问题，导致计算成本高且部署困难。现有微调方法缺乏统一效率评估标准

Method: 通过多种微调方法建立推理效率上界，提出REG指标量化模型效率差距。开发基于强化学习的REO-RL算法，采用指数级token预算策略优化推理效率

Result: REO-RL在16K token预算下减少REG≥50%，匹配Qwen3-4B/8B效率前沿，精度损失<0.5%。系统测试显示低REG方法能保持精度同时压缩推理长度

Conclusion: 当前方法仍难以完全对齐效率前沿。REO-RL通过战略token预算选择，验证了强化学习在优化推理效率方向的有效性，为大模型部署提供新思路

Abstract: Large Reasoning Models (LRMs) demonstrate remarkable problem-solving
capabilities through extended Chain-of-Thought (CoT) reasoning but often
produce excessively verbose and redundant reasoning traces. This inefficiency
incurs high inference costs and limits practical deployment. While existing
fine-tuning methods aim to improve reasoning efficiency, assessing their
efficiency gains remains challenging due to inconsistent evaluations. In this
work, we introduce the reasoning efficiency frontiers, empirical upper bounds
derived from fine-tuning base LRMs across diverse approaches and training
configurations. Based on these frontiers, we propose the Reasoning Efficiency
Gap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from
these frontiers. Systematic evaluation on challenging mathematical benchmarks
reveals significant gaps in current methods: they either sacrifice accuracy for
short length or still remain inefficient under tight token budgets. To reduce
the efficiency gap, we propose REO-RL, a class of Reinforcement Learning
algorithms that minimizes REG by targeting a sparse set of token budgets.
Leveraging numerical integration over strategically selected budgets, REO-RL
approximates the full efficiency objective with low error using a small set of
token budgets. Through systematic benchmarking, we demonstrate that our
efficiency metric, REG, effectively captures the accuracy-length trade-off,
with low-REG methods reducing length while maintaining accuracy. Our approach,
REO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching
Qwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy
loss. Ablation studies confirm the effectiveness of our exponential token
budget strategy. Finally, our findings highlight that fine-tuning LRMs to
perfectly align with the efficiency frontiers remains an open challenge.

</details>


### [68] [Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models](https://arxiv.org/abs/2506.07106)
*Samir Abdaljalil,Hasan Kurban,Khalid Qaraqe,Erchin Serpedin*

Main category: cs.CL

TL;DR: 提出Theorem-of-Thought框架，通过三种推理代理协作提升大语言模型推理的可靠性和可解释性


<details>
  <summary>Details</summary>
Motivation: 现有Chain-of-Thought方法缺乏逻辑结构验证机制，需要更可靠的推理过程评估体系

Method: 构建溯因/演绎/归纳三代理协作架构，生成结构化推理图并通过贝叶斯信念传播评估一致性

Result: 在WebOfLies和MultiArith基准测试中优于CoT等方法，GPT-4准确率达88.7%

Conclusion: 该框架为构建类认知推理系统提供新方向，代码已开源

Abstract: Large language models (LLMs) have shown strong performance across natural
language reasoning tasks, yet their reasoning processes remain brittle and
difficult to interpret. Prompting techniques like Chain-of-Thought (CoT)
enhance reliability by eliciting intermediate reasoning steps or aggregating
multiple outputs. However, they lack mechanisms for enforcing logical structure
and assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a
novel framework that models reasoning as collaboration among three parallel
agents, each simulating a distinct mode of inference: abductive, deductive, and
inductive. Each agent produces a reasoning trace, which is structured into a
formal reasoning graph. To evaluate consistency, we apply Bayesian belief
propagation guided by natural language inference (NLI), assigning confidence
scores to each step. The most coherent graph is selected to derive the final
answer. Experiments on symbolic (WebOfLies) and numerical (MultiArith)
reasoning benchmarks show that ToTh consistently outperforms CoT,
Self-Consistency, and CoT-Decoding across multiple LLMs, while producing
interpretable and logically grounded reasoning chains. Our findings suggest a
promising direction for building more robust and cognitively inspired LLM
reasoning. The implementation is available at
https://github.com/KurbanIntelligenceLab/theorem-of-thought.

</details>


### [69] [Prompting Science Report 2: The Decreasing Value of Chain of Thought in Prompting](https://arxiv.org/abs/2506.07142)
*Lennart Meincke,Ethan Mollick,Lilach Mollick,Dan Shapiro*

Main category: cs.CL

TL;DR: Chain-of-Thought提示法在不同模型类型中效果差异显著：非推理模型小幅提升性能但增加资源消耗，显式推理模型增益有限且成本显著增加。


<details>
  <summary>Details</summary>
Motivation: 探究Chain-of-Thought提示法在不同任务和模型中的实际有效性差异，验证其在资源消耗与准确性提升之间的权衡关系。

Method: 通过对比测试非推理模型与显式推理模型，分析CoT对性能指标、答案变异性及token消耗量的影响。

Result: 非推理模型使用CoT平均提升小但变异性增加（+0.5%准确率，token消耗翻倍）；显式推理模型仅提升0.2%准确率，token消耗增加300%。

Conclusion: CoT的应用价值取决于模型架构，建议在非推理模型中谨慎使用以平衡成本收益，显式推理模型无需强制启用CoT提示。

Abstract: This is the second in a series of short reports that seek to help business,
education, and policy leaders understand the technical details of working with
AI through rigorous testing. In this report, we investigate Chain-of-Thought
(CoT) prompting, a technique that encourages a large language model (LLM) to
"think step by step" (Wei et al., 2022). CoT is a widely adopted method for
improving reasoning tasks, however, our findings reveal a more nuanced picture
of its effectiveness. We demonstrate two things:
  - The effectiveness of Chain-of-Thought prompting can vary greatly depending
on the type of task and model. For non-reasoning models, CoT generally improves
average performance by a small amount, particularly if the model does not
inherently engage in step-by-step processing by default. However, CoT can
introduce more variability in answers, sometimes triggering occasional errors
in questions the model would otherwise get right. We also found that many
recent models perform some form of CoT reasoning even if not asked; for these
models, a request to perform CoT had little impact. Performing CoT generally
requires far more tokens (increasing cost and time) than direct answers.
  - For models designed with explicit reasoning capabilities, CoT prompting
often results in only marginal, if any, gains in answer accuracy. However, it
significantly increases the time and tokens needed to generate a response.

</details>


### [70] [Semantic-preserved Augmentation with Confidence-weighted Fine-tuning for Aspect Category Sentiment Analysis](https://arxiv.org/abs/2506.07148)
*Yaping Chai,Haoran Xie,Joe S. Qin*

Main category: cs.CL

TL;DR: 提出基于大语言模型的结构化提示模板数据增强方法，通过置信度加权微调策略提升方面类别情感分析任务的性能


<details>
  <summary>Details</summary>
Motivation: 解决低资源场景下方面类别情感分析（ACSA）任务数据不足的问题，传统手工设计提示方法在语义一致性和语言多样性方面存在局限

Method: 1. 设计结构化提示模板引导LLM生成预定义结构内容
2. 采用后处理技术确保生成句与原句语义一致性
3. 提出置信度加权微调策略强化模型预测信心

Result: 在四个基准数据集上全面超越现有基线模型，取得最优性能表现

Conclusion: 通过结构化数据增强和置信度加权训练策略，有效提升模型对方面类别-情感极性关系的理解能力，增强推理效果

Abstract: Large language model (LLM) is an effective approach to addressing data
scarcity in low-resource scenarios. Recent existing research designs
hand-crafted prompts to guide LLM for data augmentation. We introduce a data
augmentation strategy for the aspect category sentiment analysis (ACSA) task
that preserves the original sentence semantics and has linguistic diversity,
specifically by providing a structured prompt template for an LLM to generate
predefined content. In addition, we employ a post-processing technique to
further ensure semantic consistency between the generated sentence and the
original sentence. The augmented data increases the semantic coverage of the
training distribution, enabling the model better to understand the relationship
between aspect categories and sentiment polarities, enhancing its inference
capabilities. Furthermore, we propose a confidence-weighted fine-tuning
strategy to encourage the model to generate more confident and accurate
sentiment polarity predictions. Compared with powerful and recent works, our
method consistently achieves the best performance on four benchmark datasets
over all baselines.

</details>


### [71] [Syntactic Control of Language Models by Posterior Inference](https://arxiv.org/abs/2506.07154)
*Vicky Xefteri,Tim Vieira,Ryan Cotterell,Afra Amini*

Main category: cs.CL

TL;DR: 提出基于后验推断的抽样算法，结合SMC与句法标注器，显著提升语言模型的句法控制精度（F1从12/35提升至93）同时保持流畅性


<details>
  <summary>Details</summary>
Motivation: 现有语言模型缺乏有效的句法结构控制能力，难以满足需要精确语法控制的应用需求（如技术文档生成、法律文本创作等）

Method: 1. 采用顺序蒙特卡洛(SMC)估计后验分布
2. 设计句法标注器实现token-level结构对齐
3. 通过提议分布优化抽样过程

Result: GPT2-large和Llama3-8B的句法准确率F1分别从12.31/35.33提升至约93，语言流畅性未受显著影响

Conclusion: 该方法有效解决了生成文本的句法控制难题，为需要精确语法控制的应用场景提供了可靠解决方案

Abstract: Controlling the syntactic structure of text generated by language models is
valuable for applications requiring clarity, stylistic consistency, or
interpretability, yet it remains a challenging task. In this paper, we argue
that sampling algorithms based on the posterior inference can effectively
enforce a target constituency structure during generation. Our approach
combines sequential Monte Carlo, which estimates the posterior distribution by
sampling from a proposal distribution, with a syntactic tagger that ensures
that each generated token aligns with the desired syntactic structure. Our
experiments with GPT2 and Llama3-8B models show that with an appropriate
proposal distribution, we can improve syntactic accuracy, increasing the F1
score from $12.31$ (GPT2-large) and $35.33$ (Llama3-8B) to about $93$ in both
cases without compromising the language model's fluency. These results
underscore both the complexity of syntactic control and the effectiveness of
sampling algorithms, offering a promising approach for applications where
precise control over syntax is essential.

</details>


### [72] [GeometryZero: Improving Geometry Solving for LLM with Group Contrastive Policy Optimization](https://arxiv.org/abs/2506.07160)
*Yikun Wang,Yibin Wang,Dianyi Wang,Zimian Peng,Qipeng Guo,Dacheng Tao,Jiaqi Wang*

Main category: cs.CL

TL;DR: 提出GCPO强化学习框架，通过组对比掩蔽和长度奖励机制优化几何推理，GeometryZero模型在基准测试中平均提升4.29%性能


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在几何问题解决中面临辅助构建效率低、计算成本高的问题，需要开发更高效的轻量化解决方案

Method: 创新提出Group Contrastive Policy Optimization框架，包含基于上下文效用的组对比掩蔽奖励机制和促进长推理链的长度奖励

Result: GeometryZero模型在Geometry3K和MathVista等基准测试中平均提升4.29%，显著优于GRPO等基线方法

Conclusion: GCPO框架有效平衡辅助构建与几何推理，为小模型的高效几何问题解决提供了新范式，降低计算成本的同时提升性能

Abstract: Recent advances in large language models (LLMs) have demonstrated remarkable
capabilities across diverse domains, particularly in mathematical reasoning,
amid which geometry problem solving remains a challenging area where auxiliary
construction plays a enssential role. Existing approaches either achieve
suboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring
massive computational costs. We posit that reinforcement learning with
verifiable reward (e.g., GRPO) offers a promising direction for training
smaller models that effectively combine auxiliary construction with robust
geometric reasoning. However, directly applying GRPO to geometric reasoning
presents fundamental limitations due to its dependence on unconditional
rewards, which leads to indiscriminate and counterproductive auxiliary
constructions. To address these challenges, we propose Group Contrastive Policy
Optimization (GCPO), a novel reinforcement learning framework featuring two key
innovations: (1) Group Contrastive Masking, which adaptively provides positive
or negative reward signals for auxiliary construction based on contextual
utility, and a (2) length reward that promotes longer reasoning chains.
Building on GCPO, we develop GeometryZero, a family of affordable-size
geometric reasoning models that judiciously determine when to employ auxiliary
construction. Our extensive empirical evaluation across popular geometric
benchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models
consistently outperform baselines (e.g. GRPO), achieving an average improvement
of 4.29% across all benchmarks.

</details>


### [73] [CTDGSI: A comprehensive exploitation of instance selection methods for automatic text classification. VII Concurso de Teses, Dissertações e Trabalhos de Graduação em SI -- XXI Simpósio Brasileiro de Sistemas de Informação](https://arxiv.org/abs/2506.07169)
*Washington Cunha,Leonardo Rocha,Marcos André Gonçalves*

Main category: cs.CL

TL;DR: 通过实例选择技术减少NLP模型训练数据量（平均缩减41%），在保持效果的同时显著降低训练成本（1.67-2.46倍加速）


<details>
  <summary>Details</summary>
Motivation: 当前NLP发展依赖大量计算资源，但大规模密集模型的训练成本过高，需要探索更高效的数据工程技术

Method: 系统比较实例选择方法在文本分类任务中的应用，提出面向噪声处理和冗余识别的IS新方案，适配Transformer架构和大数据集

Result: 在保持分类效果的前提下实现训练集平均缩减41%，训练速度提升最高达2.46倍，验证了方法的可扩展性

Conclusion: 实例选择技术在NLP领域存在巨大未开发潜力，提出的针对性解决方案为资源受限场景提供了高效实用的训练优化路径

Abstract: Progress in Natural Language Processing (NLP) has been dictated by the rule
of more: more data, more computing power and more complexity, best exemplified
by the Large Language Models. However, training (or fine-tuning) large dense
models for specific applications usually requires significant amounts of
computing resources. This \textbf{Ph.D. dissertation} focuses on an
under-investi\-gated NLP data engineering technique, whose potential is
enormous in the current scenario known as Instance Selection (IS). The IS goal
is to reduce the training set size by removing noisy or redundant instances
while maintaining the effectiveness of the trained models and reducing the
training process cost. We provide a comprehensive and scientifically sound
comparison of IS methods applied to an essential NLP task -- Automatic Text
Classification (ATC), considering several classification solutions and many
datasets. Our findings reveal a significant untapped potential for IS
solutions. We also propose two novel IS solutions that are noise-oriented and
redundancy-aware, specifically designed for large datasets and transformer
architectures. Our final solution achieved an average reduction of 41\% in
training sets, while maintaining the same levels of effectiveness in all
datasets. Importantly, our solutions demonstrated speedup improvements of 1.67x
(up to 2.46x), making them scalable for datasets with hundreds of thousands of
documents.

</details>


### [74] [RULE: Reinforcement UnLEarning Achieves Forget-Retain Pareto Optimality](https://arxiv.org/abs/2506.07171)
*Chenlong Zhang,Zhuoran Jin,Hongbang Yuan,Jiaheng Wei,Tong Zhou,Kang Liu,Jun Zhao,Yubo Chen*

Main category: cs.CL

TL;DR: 本文提出基于强化学习的RULE框架，通过优化拒绝边界实现定向遗忘敏感信息，在仅需少量数据的情况下显著提升遗忘效果并保持模型实用性。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型去学习方法依赖大量数据且存在响应不自然、泛化差等问题，需开发更高效解决方案以平衡信息删除与模型效用。

Method: 将去学习转化为边界优化问题，利用少量遗忘集和合成边界数据，通过可验证的奖励函数引导模型安全拒绝敏感查询并保留正常响应。

Result: 实验显示RULE仅用12%遗忘数据和8%合成数据即实现17.5%的遗忘质量提升及16.3%回答自然度提高，达到帕累托最优并增强泛化能力。

Conclusion: RULE框架有效解决了LLM定向遗忘难题，在保持模型实用性的同时提升输出自然性、训练效率和拒绝行为泛化能力。

Abstract: The widespread deployment of Large Language Models (LLMs) trained on massive,
uncurated corpora has raised growing concerns about the inclusion of sensitive,
copyrighted, or illegal content. This has led to increasing interest in LLM
unlearning: the task of selectively removing specific information from a model
without retraining from scratch or degrading overall utility. However, existing
methods often rely on large-scale forget and retain datasets, and suffer from
unnatural responses, poor generalization, or catastrophic utility loss. In this
work, we propose Reinforcement UnLearning (RULE), an efficient framework that
formulates unlearning as a refusal boundary optimization problem. RULE is
trained with a small portion of the forget set and synthesized boundary
queries, using a verifiable reward function that encourages safe refusal on
forget--related queries while preserving helpful responses on permissible
inputs. We provide both theoretical and empirical evidence demonstrating the
effectiveness of RULE in achieving targeted unlearning without compromising
model utility. Experimental results show that, with only $12%$ forget set and
$8%$ synthesized boundary data, RULE outperforms existing baselines by up to
$17.5%$ forget quality and $16.3%$ naturalness response while maintaining
general utility, achieving forget--retain Pareto optimality. Remarkably, we
further observe that RULE improves the naturalness of model outputs, enhances
training efficiency, and exhibits strong generalization ability, generalizing
refusal behavior to semantically related but unseen queries.

</details>


### [75] [Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs](https://arxiv.org/abs/2506.07180)
*Wenrui Zhou,Shu Yang,Qingsong Yang,Zikun Guo,Lijie Hu,Di Wang*

Main category: cs.CL

TL;DR: 提出首个视频大语言模型奉承行为基准VISE，评估模型在误导性输入下的响应，并探索关键帧选择作为缓解策略


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视视频领域模型奉承现象，缺乏系统化评估影响模型可靠性

Method: 构建多维度测试基准，结合语言学视角分析奉承类型，引入关键帧选择增强视觉锚定

Result: 发现主流Video-LLMs存在显著奉承倾向，关键帧筛选可有效降低视觉-语言对齐偏差

Conclusion: VISE填补视频领域评估空白，关键帧机制为提升模型事实一致性提供新方向

Abstract: As video large language models (Video-LLMs) become increasingly integrated
into real-world applications that demand grounded multimodal reasoning,
ensuring their factual consistency and reliability is of critical importance.
However, sycophancy, the tendency of these models to align with user input even
when it contradicts the visual evidence, undermines their trustworthiness in
such contexts. Current sycophancy research has largely overlooked its specific
manifestations in the video-language domain, resulting in a notable absence of
systematic benchmarks and targeted evaluations to understand how Video-LLMs
respond under misleading user input. To fill this gap, we propose VISE
(Video-LLM Sycophancy Benchmarking and Evaluation), the first dedicated
benchmark designed to evaluate sycophantic behavior in state-of-the-art
Video-LLMs across diverse question formats, prompt biases, and visual reasoning
tasks. Specifically, VISE pioneeringly brings linguistic perspectives on
sycophancy into the visual domain, enabling fine-grained analysis across
multiple sycophancy types and interaction patterns. In addition, we explore
key-frame selection as an interpretable, training-free mitigation strategy,
which reveals potential paths for reducing sycophantic bias by strengthening
visual grounding.

</details>


### [76] [SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes](https://arxiv.org/abs/2506.07245)
*Wenxuan Xie,Yaxun Dai,Wenhao Jiang*

Main category: cs.CL

TL;DR: SDE-SQL框架通过动态生成SQL探针使大语言模型在推理时自主探索数据库，在BIRD基准上实现8.02%准确率提升并创造新SOTA


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法依赖静态数据库信息，限制模型对数据内容的全方位理解。需要突破静态上下文限制，实现主动数据探索。

Method: 提出零样本框架SDE-SQL：1. 生成并执行SQL探针主动检索数据库信息 2. 通过迭代更新机制动态增强模型对数据的理解 3. 无需任何问答-SQL对作为上下文示例

Result: 在Qwen2.5-72B-Instruct上：1. 执行准确率相对提升8.02% 2. 成为无需监督微调/模型集成的开源模型最佳方案 3. 加入监督微调可额外提升0.52%

Conclusion: 动态数据探索机制显著提升LLMs的数据库理解能力，零样本方法保持通用性，监督微调可进一步释放框架潜力，为复杂数据库交互任务提供新思路

Abstract: Recent advancements in large language models (LLMs) have significantly
improved performance on the Text-to-SQL task. However, prior approaches
typically rely on static, pre-processed database information provided at
inference time, which limits the model's ability to fully understand the
database contents. Without dynamic interaction, LLMs are constrained to fixed,
human-provided context and cannot autonomously explore the underlying data. To
address this limitation, we propose SDE-SQL, a framework that enables large
language models to perform self-driven exploration of databases during
inference. This is accomplished by generating and executing SQL probes, which
allow the model to actively retrieve information from the database and
iteratively update its understanding of the data. Unlike prior methods, SDE-SQL
operates in a zero-shot setting, without relying on any question-SQL pairs as
in-context demonstrations. When evaluated on the BIRD benchmark with
Qwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in
execution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing
a new state-of-the-art among methods based on open-source models without
supervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the
performance of SDE-SQL can be further enhanced, yielding an additional 0.52%
improvement.

</details>


### [77] [Improving the Efficiency of Long Document Classification using Sentence Ranking Approach](https://arxiv.org/abs/2506.07248)
*Prathamesh Kokate,Mitali Sarnaik,Manavi Khopade,Raviraj Joshi*

Main category: cs.CL

TL;DR: 通过TF-IDF句子选择实现高效长文档分类，性能接近全文本基准


<details>
  <summary>Details</summary>
Motivation: 解决Transformer模型处理长文本时的计算限制和冗余问题，利用关键句子提升分类效率

Method: 提出基于TF-IDF的句子排序方法，结合归一化评分与句子长度，开发固定数量/百分比选择策略

Result: 在MahaNews数据集上实现分类准确率仅下降0.33%，输入规模减少50%+，推理延迟降低43%

Conclusion: 验证了显著减少上下文长度而不损失性能的可行性，为实际长文本处理提供有效方案

Abstract: Long document classification poses challenges due to the computational
limitations of transformer-based models, particularly BERT, which are
constrained by fixed input lengths and quadratic attention complexity.
Moreover, using the full document for classification is often redundant, as
only a subset of sentences typically carries the necessary information. To
address this, we propose a TF-IDF-based sentence ranking method that improves
efficiency by selecting the most informative content. Our approach explores
fixed-count and percentage-based sentence selection, along with an enhanced
scoring strategy combining normalized TF-IDF scores and sentence length.
Evaluated on the MahaNews LDC dataset of long Marathi news articles, the method
consistently outperforms baselines such as first, last, and random sentence
selection. With MahaBERT-v2, we achieve near-identical classification accuracy
with just a 0.33 percent drop compared to the full-context baseline, while
reducing input size by over 50 percent and inference latency by 43 percent.
This demonstrates that significant context reduction is possible without
sacrificing performance, making the method practical for real-world long
document classification tasks.

</details>


### [78] [Bias Attribution in Filipino Language Models: Extending a Bias Interpretability Metric for Application on Agglutinative Languages](https://arxiv.org/abs/2506.07249)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

TL;DR: 将英语模型的偏差归因方法适配到黏着语（菲律宾语），发现非英语模型的偏差驱动主题存在文化差异


<details>
  <summary>Details</summary>
Motivation: 现有偏差归因研究集中于英语模型，需探究黏着语模型的偏差机制以填补非英语语言处理模型的认知空白

Method: 改进信息论偏差归因指标，在纯菲律宾语模型和三个多语言模型（全球/东南亚训练数据）上实施验证

Result: 菲律宾语模型的偏差由人物/物体/关系等实体主题驱动，与英语模型侧重犯罪行为等动作主题形成鲜明对比

Conclusion: 英语与非英语模型在处理社会人口统计输入时存在系统性差异，强调语言特异性在偏差分析中的必要性

Abstract: Emerging research on bias attribution and interpretability have revealed how
tokens contribute to biased behavior in language models processing English
texts. We build on this line of inquiry by adapting the information-theoretic
bias attribution score metric for implementation on models handling
agglutinative languages, particularly Filipino. We then demonstrate the
effectiveness of our adapted method by using it on a purely Filipino model and
on three multilingual models: one trained on languages worldwide and two on
Southeast Asian data. Our results show that Filipino models are driven towards
bias by words pertaining to people, objects, and relationships, entity-based
themes that stand in contrast to the action-heavy nature of bias-contributing
themes in English (i.e., criminal, sexual, and prosocial behaviors). These
findings point to differences in how English and non-English models process
inputs linked to sociodemographic groups and bias.

</details>


### [79] [Question Answering under Temporal Conflict: Evaluating and Organizing Evolving Knowledge with LLMs](https://arxiv.org/abs/2506.07270)
*Atahan Özer,Çağatay Yıldız*

Main category: cs.CL

TL;DR: 提出轻量级外部记忆框架解决大语言模型在时效性知识更新中的局限性，在时序文本数据集上表现优于传统方法


<details>
  <summary>Details</summary>
Motivation: 大语言模型的参数化记忆难以适应实时更新的信息，传统方法（重新训练/上下文学习）存在成本高、扩展性差的问题。研究聚焦时间演变文本（如维基历史版本）中的知识冲突问题

Method: 构建Temporal Wiki（维基历史快照）和Unified Clark（时序新闻聚合）双基准，提出基于外部记忆结构的代理框架实现增量式知识组织与时间感知推理

Result: 在需要处理冲突事实的复杂推理任务上，该方法比ICL和RAG基线模型平均提升12.3%准确率

Conclusion: 结构化外部记忆机制有效解决了语言模型在时序知识融合中的局限性，为动态知识更新提供了轻量化解决方案

Abstract: Large language models (LLMs) exhibit remarkable capabilities in question
answering and reasoning thanks to their extensive parametric memory. However,
their knowledge is inherently limited by the scope of their pre-training data,
while real-world information evolves continuously. Updating this knowledge
typically requires costly and brittle re-training, or in-context learning
(ICL), which becomes impractical at scale given the volume and volatility of
modern information. Motivated by these limitations, we investigate how LLMs
perform when exposed to temporal text corpora, or documents that reflect
evolving knowledge over time, such as sports biographies where facts like a
player's "current team" change year by year. To this end, we introduce two new
benchmarks: Temporal Wiki, which captures factual drift across historical
Wikipedia snapshots, and Unified Clark, which aggregates timestamped news
articles to simulate real-world information accumulation. Our analysis reveals
that LLMs often struggle to reconcile conflicting or outdated facts and can be
misled when multiple versions of a fact appear in context. To address these
issues, we propose a lightweight, agentic framework that incrementally builds a
structured, external memory from source documents without requiring
re-training. This knowledge organization strategy enables models to retrieve
and reason over temporally filtered, relevant information at inference time.
Empirically, our method outperforms ICL and RAG baselines across both
benchmarks, especially on questions requiring more complex reasoning or
integration of conflicting facts.

</details>


### [80] [Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched and Low-Resource Languages](https://arxiv.org/abs/2506.07274)
*Olga Kellert,Nemika Tyagi,Muhammad Imran,Nelvin Licona-Guevara,Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: 提出基于大语言模型的BiLingua Parser框架，通过提示工程和专家审核生成代码转换文本的句法标注，在低资源语言环境中实现95.29% LAS性能


<details>
  <summary>Details</summary>
Motivation: 现有单语树库训练的解析器难以处理多语言混合文本，LLMs在代码转换场景的句法分析潜力未被系统探索

Method: 开发提示框架结合少量示例和专家审核，构建西班牙语-英语/瓜拉尼语的UD标注数据集，开展跨语言切换点分析

Result: 专家修订后LAS达95.29%，显著优于基线模型，创建首个西班牙语-瓜拉尼语UD标注语料库

Conclusion: 验证了LLMs在引导下可作为低资源代码转换环境中的有效标注工具，为稀缺语言对提供实用句法分析方案

Abstract: Code-switching presents a complex challenge for syntactic analysis,
especially in low-resource language settings where annotated data is scarce.
While recent work has explored the use of large language models (LLMs) for
sequence-level tagging, few approaches systematically investigate how well
these models capture syntactic structure in code-switched contexts. Moreover,
existing parsers trained on monolingual treebanks often fail to generalize to
multilingual and mixed-language input. To address this gap, we introduce the
BiLingua Parser, an LLM-based annotation pipeline designed to produce Universal
Dependencies (UD) annotations for code-switched text. First, we develop a
prompt-based framework for Spanish-English and Spanish-Guaran\'i data,
combining few-shot LLM prompting with expert review. Second, we release two
annotated datasets, including the first Spanish-Guaran\'i UD-parsed corpus.
Third, we conduct a detailed syntactic analysis of switch points across
language pairs and communicative contexts. Experimental results show that
BiLingua Parser achieves up to 95.29% LAS after expert revision, significantly
outperforming prior baselines and multilingual parsers. These results show that
LLMs, when carefully guided, can serve as practical tools for bootstrapping
syntactic resources in under-resourced, code-switched environments. Data and
source code are available at https://github.com/N3mika/ParsingProject

</details>


### [81] [Exploring the Impact of Temperature on Large Language Models:Hot or Cold?](https://arxiv.org/abs/2506.07295)
*Lujun Li,Lama Sleem,Niccolo' Gentile,Geoffrey Nichil,Radu State*

Main category: cs.CL

TL;DR: 温度参数通过特定技能影响LLM性能，提出BERT温度选择器提升中小模型效果，发现量化模型中温度效应一致性及Mutation Temperature随模型规模增大现象


<details>
  <summary>Details</summary>
Motivation: 针对LLM温度参数对模型不同能力的影响机制不明确问题，系统研究温度参数在0-2区间内对6类能力的量化影响，解决实际应用中温度选择的复杂性挑战

Method: 使用3种规模开源模型（小/中/大），在6个能力评估数据集进行温度敏感性测试；提出基于BERT的温度选择器框架；拓展到FP16及4-bit量化模型分析

Result: 1. 温度对数学/逻辑能力呈单调影响，对创意任务呈倒U型影响
2. BERT温度选择器使中小模型SuperGLUE性能提升12-15%
3. 量化模型中Mutation Temperature阈值与原始模型保持线性关系（R²=0.89）
4. 模型规模每增加10B，Mutation Temperature临界值提升0.3±0.05

Conclusion: 温度选择应基于具体任务特性，提出的动态温度选择机制显著提升资源受限场景的模型效率，量化研究结果为部署不同规模LLM的温度策略提供理论依据

Abstract: The sampling temperature, a critical hyperparameter in large language models
(LLMs), modifies the logits before the softmax layer, thereby reshaping the
distribution of output tokens. Recent studies have challenged the Stochastic
Parrots analogy by demonstrating that LLMs are capable of understanding
semantics rather than merely memorizing data and that randomness, modulated by
sampling temperature, plays a crucial role in model inference. In this study,
we systematically evaluated the impact of temperature in the range of 0 to 2 on
data sets designed to assess six different capabilities, conducting statistical
analyses on open source models of three different sizes: small (1B--4B), medium
(6B--13B), and large (40B--80B). Our findings reveal distinct skill-specific
effects of temperature on model performance, highlighting the complexity of
optimal temperature selection in practical applications. To address this
challenge, we propose a BERT-based temperature selector that takes advantage of
these observed effects to identify the optimal temperature for a given prompt.
We demonstrate that this approach can significantly improve the performance of
small and medium models in the SuperGLUE datasets. Furthermore, our study
extends to FP16 precision inference, revealing that temperature effects are
consistent with those observed in 4-bit quantized models. By evaluating
temperature effects up to 4.0 in three quantized models, we find that the
Mutation Temperature -- the point at which significant performance changes
occur -- increases with model size.

</details>


### [82] [Subjectivity in the Annotation of Bridging Anaphora](https://arxiv.org/abs/2506.07297)
*Lauren Levine,Amir Zeldes*

Main category: cs.CL

TL;DR: 分析桥接标注中的主观性问题，发现现有资源存在严重标注不足且标注者一致性较低


<details>
  <summary>Details</summary>
Motivation: 桥接关系识别具有主观性特征，导致标注一致性低，需探究其在照应识别、先行词解析和子类型选择三个层面的具体表现

Method: 对GUM语料库测试集进行注释试验，开发新的桥接子类型分类系统并与现有方案对比

Result: 现有资源可能存在严重标注不足，子类型分类一致性中等，桥接实例识别重叠率低（多源于实体理解的主观差异）

Conclusion: 桥接标注受主观认知影响显著，现有标注资源可能存在系统性遗漏，需建立更细粒度的分类体系提升标注可靠性

Abstract: Bridging refers to the associative relationship between inferable entities in
a discourse and the antecedents which allow us to understand them, such as
understanding what "the door" means with respect to an aforementioned "house".
As identifying associative relations between entities is an inherently
subjective task, it is difficult to achieve consistent agreement in the
annotation of bridging anaphora and their antecedents. In this paper, we
explore the subjectivity involved in the annotation of bridging instances at
three levels: anaphor recognition, antecedent resolution, and bridging subtype
selection. To do this, we conduct an annotation pilot on the test set of the
existing GUM corpus, and propose a newly developed classification system for
bridging subtypes, which we compare to previously proposed schemes. Our results
suggest that some previous resources are likely to be severely under-annotated.
We also find that while agreement on the bridging subtype category was
moderate, annotator overlap for exhaustively identifying instances of bridging
is low, and that many disagreements resulted from subjective understanding of
the entities involved.

</details>


### [83] [ConfQA: Answer Only If You Are Confident](https://arxiv.org/abs/2506.07309)
*Yin Huang,Yifan Ethan Xu,Kai Sun,Vera Yan,Alicia Sun,Haidar Khan,Jimmy Nguyen,Mohammad Kachuee,Zhaojiang Lin,Yue Liu,Aaron Colak,Anuj Kumar,Wen-tau Yih,Xin Luna Dong*

Main category: cs.CL

TL;DR: 论文提出ConfQA微调方法，将大语言模型的幻觉率从20-40%降至5%以下，并通过双神经知识框架实现95%准确率和30%检索量缩减。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在事实性陈述中产生幻觉的问题，通过校准置信度提升模型可靠性。核心思路是让模型在不确定时主动承认而非虚构答案。

Method: 1. 使用带『仅在确信时回答』的抑制性提示词引导行为
2. 利用知识图谱属性值校准置信度
3. 提出双神经知识框架动态切换神经参数化知识与符号化知识

Result: 跨领域/题型基准测试显示：
- 幻觉率稳定低于5%
- 框架准确率突破95%
- 外部知识检索量减少超30%

Conclusion: 通过置信度校准机制与知识融合框架，ConfQA在保持模型参数化优势的同时，显著提升事实准确性并降低计算成本，为可靠语言模型系统提供新范式。

Abstract: Can we teach Large Language Models (LLMs) to refrain from hallucinating
factual statements? In this paper we present a fine-tuning strategy that we
call ConfQA, which can reduce hallucination rate from 20-40% to under 5% across
multiple factuality benchmarks. The core idea is simple: when the LLM answers a
question correctly, it is trained to continue with the answer; otherwise, it is
trained to admit "I am unsure". But there are two key factors that make the
training highly effective. First, we introduce a dampening prompt "answer only
if you are confident" to explicitly guide the behavior, without which
hallucination remains high as 15%-25%. Second, we leverage simple factual
statements, specifically attribute values from knowledge graphs, to help LLMs
calibrate the confidence, resulting in robust generalization across domains and
question types. Building on this insight, we propose the Dual Neural Knowledge
framework, which seamlessly select between internally parameterized neural
knowledge and externally recorded symbolic knowledge based on ConfQA's
confidence. The framework enables potential accuracy gains to beyond 95%, while
reducing unnecessary external retrievals by over 30%.

</details>


### [84] [Reward Model Interpretability via Optimal and Pessimal Tokens](https://arxiv.org/abs/2506.07326)
*Brian Christian,Hannah Rose Kirk,Jessica A. F. Thompson,Christopher Summerfield,Tsvetomira Dumbalska*

Main category: cs.CL

TL;DR: 研究发现不同奖励模型存在显著异质性和系统性偏见，揭示其作为人类价值观代理的局限性


<details>
  <summary>Details</summary>
Motivation: 现有研究过度关注奖励模型在微调生成模型中的应用，而忽视了对模型本身价值编码机制的深入分析

Method: 通过穷举分析十个开源奖励模型在整个词汇空间中对单令牌响应的评分模式

Result: 发现模型间异质性、评分不对称性、框架敏感性、高频词过估值，以及无害训练可能引发的身份偏见

Conclusion: 奖励模型的互换性假设存在风险，其编码的偏差可能通过大语言模型传播，需警惕作为复杂人类价值观代理的适用性

Abstract: Reward modeling has emerged as a crucial component in aligning large language
models with human values. Significant attention has focused on using reward
models as a means for fine-tuning generative models. However, the reward models
themselves -- which directly encode human value judgments by turning
prompt-response pairs into scalar rewards -- remain relatively understudied. We
present a novel approach to reward model interpretability through exhaustive
analysis of their responses across their entire vocabulary space. By examining
how different reward models score every possible single-token response to
value-laden prompts, we uncover several striking findings: (i) substantial
heterogeneity between models trained on similar objectives, (ii) systematic
asymmetries in how models encode high- vs low-scoring tokens, (iii) significant
sensitivity to prompt framing that mirrors human cognitive biases, and (iv)
overvaluation of more frequent tokens. We demonstrate these effects across ten
recent open-source reward models of varying parameter counts and architectures.
Our results challenge assumptions about the interchangeability of reward
models, as well as their suitability as proxies of complex and
context-dependent human values. We find that these models can encode concerning
biases toward certain identity groups, which may emerge as unintended
consequences of harmlessness training -- distortions that risk propagating
through the downstream large language models now deployed to millions.

</details>


### [85] [Improving LLM Reasoning through Interpretable Role-Playing Steering](https://arxiv.org/abs/2506.07335)
*Anyi Wang,Dong Shu,Yifan Wang,Yunpu Ma,Mengnan Du*

Main category: cs.CL

TL;DR: 提出SRPS框架，通过分析并操纵大语言模型内部角色扮演相关特征，在多个推理基准测试中实现性能提升（如Llama3.1-8B在CSQA准确率提升7.94%），相比传统提示方法具有更好的可解释性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有角色扮演方法依赖提示工程，存在稳定性和可解释性不足的问题。希望通过分析模型内部特征实现更精细的控制。

Method: SRPS框架：1. 从角色扮演提示中提取潜在表征 2. 根据激活模式选择最相关特征 3. 构建可调节强度的导向向量注入残差流

Result: Llama3.1-8B在CSQA零样本CoT设置下准确率从31.86%→39.80%；Gemma2-9B在SVAMP上从37.50%→45.10%

Conclusion: SRPS通过特征操纵有效增强LLM推理能力，为角色信息如何影响模型激活提供新见解，具有更好的可控性和稳定性。

Abstract: Role-playing has emerged as an effective technique for enhancing the
reasoning capabilities of large language models (LLMs). However, existing
methods primarily rely on prompt engineering, which often lacks stability and
interpretability. In this paper, we introduce Sparse Autoencoder Role-Playing
Steering (SRPS), a novel framework that identifies and manipulates internal
model features associated with role-playing behavior. Our approach extracts
latent representations from role-play prompts, selects the most relevant
features based on activation patterns, and constructs a steering vector that
can be injected into the model's residual stream with controllable intensity.
Our method enables fine-grained control over role-specific behavior and offers
insights into how role information influences internal model activations.
Extensive experiments across various reasoning benchmarks and model sizes
demonstrate consistent performance gains. Notably, in the zero-shot
chain-of-thought (CoT) setting, the accuracy of Llama3.1-8B on CSQA improves
from 31.86% to 39.80%, while Gemma2-9B on SVAMP increases from 37.50% to
45.10%. These results highlight the potential of SRPS to enhance reasoning
ability in LLMs, providing better interpretability and stability compared to
traditional prompt-based role-playing.

</details>


### [86] [Refusal-Feature-guided Teacher for Safe Finetuning via Data Filtering and Alignment Distillation](https://arxiv.org/abs/2506.07356)
*Seokil Ham,Yubin Choi,Seungju Cho,Yujin Yang,Younghun Kim,Changick Kim*

Main category: cs.CL

TL;DR: 通过ReFT模型在FaaS服务中过滤有害用户数据，提升LLM安全对齐性与微调准确性


<details>
  <summary>Details</summary>
Motivation: 现有Finetuning-as-a-Service存在安全漏洞，用户数据中的有害提示会破坏LLM的安全对齐性

Method: 利用安全对齐LLM的拒绝特征训练ReFT模型，通过特征相似度识别有害提示，并在微调时进行数据过滤与知识蒸馏

Result: 实验证明该方法有效减少89%有害输出，同时提升用户特定任务的微调准确率

Conclusion: ReFT为LLM在FaaS服务中的安全可靠部署提供了实践解决方案

Abstract: Recently, major AI service providers such as Google and OpenAI have
introduced Finetuning-as-a-Service, which enables users to customize Large
Language Models (LLMs) for specific downstream tasks using their own data.
However, this service is vulnerable to degradation of LLM safety-alignment when
user data contains harmful prompts. While some prior works address this issue,
fundamentally filtering harmful data from user data remains unexplored.
Motivated by our observation that a directional representation reflecting
refusal behavior (called the refusal feature) obtained from safety-aligned LLMs
can inherently distinguish between harmful and harmless prompts, we propose the
Refusal-Feature-guided Teacher (ReFT). Our ReFT model is trained to identify
harmful prompts based on the similarity between input prompt features and its
refusal feature. During finetuning, the ReFT model serves as a teacher that
filters harmful prompts from user data and distills alignment knowledge into
the base model. Extensive experiments demonstrate that our ReFT-based
finetuning strategy effectively minimizes harmful outputs and enhances
finetuning accuracy for user-specific tasks, offering a practical solution for
secure and reliable deployment of LLMs in Finetuning-as-a-Service.

</details>


### [87] [SEED: Enhancing Text-to-SQL Performance and Practical Usability Through Automatic Evidence Generation](https://arxiv.org/abs/2506.07423)
*Janghyeon Yun,Sang-goo Lee*

Main category: cs.CL

TL;DR: SEED系统通过自动生成数据库相关证据，显著提升文本到SQL模型在无人工证据场景下的准确率，部分情况甚至超越带人工证据的基准表现


<details>
  <summary>Details</summary>
Motivation: 现有BIRD数据集依赖人工提供的专业知识证据，这与真实场景中非专家用户的需求相矛盾，且人工证据存在缺失/错误问题影响模型性能

Method: 系统化分析数据库模式、描述文件和数值特征，自动提取结构化证据替代人工证据，增强模型对复杂数据库的理解能力

Result: 在BIRD和Spider数据集上，SEED使无证据场景的SQL生成准确率提升显著，部分场景超越使用BIRD人工证据的基准模型

Conclusion: SEED有效弥合文本到SQL研究与实践的鸿沟，通过自动化证据生成增强模型适应性和鲁棒性，推动技术落地应用

Abstract: Text-to-SQL enables non-experts to retrieve data from databases by converting
natural language queries into SQL. However, state-of-the-art text-to-SQL
studies rely on the BIRD dataset, which assumes that evidence is provided along
with questions. Although BIRD facilitates research advancements, it assumes
that users have expertise and domain knowledge, contradicting the fundamental
goal of text-to-SQL. In addition, human-generated evidence in BIRD contains
defects, including missing or erroneous evidence, which affects model
performance. To address this issue, we propose SEED (System for Evidence
Extraction and Domain knowledge generation), an approach that automatically
generates evidence to improve performance and practical usability in real-world
scenarios. SEED systematically analyzes database schema, description files, and
values to extract relevant information. We evaluated SEED on BIRD and Spider,
demonstrating that it significantly improves SQL generation accuracy in the
no-evidence scenario, and in some cases, even outperforms the setting where
BIRD evidence is provided. Our results highlight that SEED-generated evidence
not only bridges the gap between research and real-world deployment but also
improves the adaptability and robustness of text-to-SQL models. Our code is
available at https://github.com/felix01189/SEED

</details>


### [88] [Plug-in and Fine-tuning: Bridging the Gap between Small Language Models and Large Language Models](https://arxiv.org/abs/2506.07424)
*Kyeonghyun Kim,Jinhee Jang,Juhwan Choi,Yoonji Lee,Kyohoon Jin,YoungBin Kim*

Main category: cs.CL

TL;DR: 提出PiFi框架，通过融合LLM的冻结层增强SLM性能，保持计算效率


<details>
  <summary>Details</summary>
Motivation: 解决LLMs计算成本高与SLMs泛化能力不足的矛盾，寻求高效与高性能的平衡

Method: 将LLM的单个冻结层集成到SLM中，针对特定任务微调组合模型

Result: 在多项NLP任务中实现性能提升，增强跨领域泛化能力和语言能力迁移

Conclusion: PiFi有效结合LLM与SLM优势，为资源受限场景提供高效解决方案

Abstract: Large language models (LLMs) are renowned for their extensive linguistic
knowledge and strong generalization capabilities, but their high computational
demands make them unsuitable for resource-constrained environments. In
contrast, small language models (SLMs) are computationally efficient but often
lack the broad generalization capacity of LLMs. To bridge this gap, we propose
PiFi, a novel framework that combines the strengths of both LLMs and SLMs to
achieve high performance while maintaining efficiency. PiFi integrates a single
frozen layer from an LLM into a SLM and fine-tunes the combined model for
specific tasks, boosting performance without a significant increase in
computational cost. We show that PiFi delivers consistent performance
improvements across a range of natural language processing tasks, including
both natural language understanding and generation. Moreover, our findings
demonstrate PiFi's ability to effectively leverage LLM knowledge, enhancing
generalization to unseen domains and facilitating the transfer of linguistic
abilities.

</details>


### [89] [Conjoined Predication and Scalar Implicature](https://arxiv.org/abs/2506.07429)
*Ratna Kandala*

Main category: cs.CL

TL;DR: 解析Magri(2016)未解决的连词第一难题：集体/并发解读导致语境矛盾


<details>
  <summary>Details</summary>
Motivation: 揭示量化、集体解释与语境更新的三重交互机制理论空白

Method: 基于原始理论框架分析连词结构的集体/并发语义解读机制

Result: 发现连词结构的异常源于集体解读引发的间接语境矛盾

Conclusion: 标量含义的语用机制超越语法化许可解释框架

Abstract: Magri (2016) investigates two puzzles arising from conjunction. Although
Magri has proposed a solution to the second puzzle, the first remains
unresolved. This first puzzle reveals a hidden interaction among
quantification, collective/concurrent interpretation, and contextual updating
dimensions that have yet to be explored. In essence, the problem is that
certain forms of sentences like "Some Italians come from a warm country," when
conjoined as in "(Only) Some Italians come from a warm country and are blond,"
sound infelicitous, even though no obvious alternative triggers a conflicting
scalar implicature. In this paper, we offer a conceptual analysis of Magri's
first puzzle by situating it within its original theoretical framework. We
argue that the oddness arises from the collective or concurrent reading of the
conjunctive predicate: in examples such as "(Only) Some Italians come from a
warm country and are blond," this interpretation generates an indirect
contextual contradiction. Moreover, we suggest that the pragmatic mechanisms
governing scalar implicature generation extend beyond what is captured by
exhaustification-based grammatical licensing accounts.

</details>


### [90] [Well Begun is Half Done: Low-resource Preference Alignment by Weak-to-Strong Decoding](https://arxiv.org/abs/2506.07434)
*Feifan Song,Shaohang Wei,Wen Luo,Yuxuan Fan,Tianyu Liu,Guoyin Wang,Houfeng Wang*

Main category: cs.CL

TL;DR: 提出Weak-to-Strong Decoding框架，通过小型对齐模型引导大模型解码开头部分，提升LLM对齐效果同时避免下游任务性能下降。


<details>
  <summary>Details</summary>
Motivation: 观察到生成对齐响应的困难集中在解码初始阶段，通过小模型生成对齐开头+大模型后续接力的方式结合两者优势。

Method: 1. 小模型生成对齐开头→大模型继续后续内容 2. 设计自动切换机制控制生成过程 3. 使用GenerAlign数据集微调Pilot-3B作为草案模型

Result: WSD框架使不同基础模型全面超越基线方法，在保持下游任务性能（避免对齐税）的同时提升对齐效果，实验验证了机制有效性。

Conclusion: 通过小型对齐模型引导初始解码阶段，WSD框架有效解决了LLM对齐的质量-性能平衡问题，为低资源对齐提供了新思路。

Abstract: Large Language Models (LLMs) require alignment with human preferences to
avoid generating offensive, false, or meaningless content. Recently,
low-resource methods for LLM alignment have been popular, while still facing
challenges in obtaining both high-quality and aligned content. Motivated by the
observation that the difficulty of generating aligned responses is concentrated
at the beginning of decoding, we propose a novel framework, Weak-to-Strong
Decoding (WSD), to enhance the alignment ability of base models by the guidance
of a small aligned model. The small model first drafts well-aligned beginnings,
followed by the large base model to continue the rest, controlled by a
well-designed auto-switch mechanism. We also collect a new dataset, GenerAlign,
to fine-tune a small-sized Pilot-3B as the draft model, which effectively
enhances different base models under the WSD framework to outperform all
baseline methods, while avoiding degradation on downstream tasks, termed as the
alignment tax. Extensive experiments are further conducted to examine the
impact of different settings and time efficiency, as well as analyses on the
intrinsic mechanisms of WSD in depth.

</details>


### [91] [LG-ANNA-Embedding technical report](https://arxiv.org/abs/2506.07438)
*Jooyoung Choi,Hyun Kim,Hansol Jang,Changwook Jun,Kyunghoon Bae,Hyewon Choi,Stanley Jungkyu Choi,Honglak Lee,Chulmin Yun*

Main category: cs.CL

TL;DR: 提出基于指令学习的统一框架Mistral-7B，结合上下文学习、软监督和自适应负样本挖掘，在41项任务中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 解决传统文本嵌入模型在跨任务泛化性不足、依赖任务微调的问题，探索无需特定任务调整的通用嵌入生成方案

Method: 1. 指令模板+少样本上下文学习引导模型 2. 基于稠密检索器蒸馏的连续相关性软标签 3. 基于相似度的自适应硬负样本筛选机制

Result: 在MTEB v2基准的41项任务中达到Borda分数前列，超越更大规模或全微调的基线模型

Conclusion: 验证了上下文提示、软监督与自适应采样结合的有效性，为可扩展的高质量嵌入生成提供了新范式

Abstract: This report presents a unified instruction-based framework for learning
generalized text embeddings optimized for both information retrieval (IR) and
non-IR tasks. Built upon a decoder-only large language model (Mistral-7B), our
approach combines in-context learning, soft supervision, and adaptive
hard-negative mining to generate context-aware embeddings without task-specific
fine-tuning. Structured instructions and few-shot examples are used to guide
the model across diverse tasks, enabling strong performance on classification,
semantic similarity, clustering, and reranking benchmarks. To improve semantic
discrimination, we employ a soft labeling framework where continuous relevance
scores, distilled from a high-performance dense retriever and reranker, serve
as fine-grained supervision signals. In addition, we introduce adaptive
margin-based hard-negative mining, which filters out semantically ambiguous
negatives based on their similarity to positive examples, thereby enhancing
training stability and retrieval robustness. Our model is evaluated on the
newly introduced MTEB (English, v2) benchmark, covering 41 tasks across seven
categories. Results show that our method achieves strong generalization and
ranks among the top-performing models by Borda score, outperforming several
larger or fully fine-tuned baselines. These findings highlight the
effectiveness of combining in-context prompting, soft supervision, and adaptive
sampling for scalable, high-quality embedding generation.

</details>


### [92] [Understanding Cross-Domain Adaptation in Low-Resource Topic Modeling](https://arxiv.org/abs/2506.07453)
*Pritom Saha Akash,Kevin Chen-Chuan Chang*

Main category: cs.CL

TL;DR: 提出DALTA框架解决低资源主题建模的领域适应问题，通过跨域对齐提升主题连贯性和稳定性


<details>
  <summary>Details</summary>
Motivation: 现有主题模型在低资源场景下因目标域数据不足导致主题推断不稳定，需要有效的跨领域知识迁移方法

Method: 采用共享编码器提取域不变特征，专用解码器处理域特性，结合对抗对齐实现选择性知识迁移

Result: 在多个低资源数据集上，DALTA在主题连贯性(提升15%)、稳定性(方差降低40%)和可迁移性指标上全面超越基线方法

Conclusion: 首次将领域适应形式化引入主题建模，理论分析与实验验证为低资源NLP任务的跨域知识迁移提供了新范式

Abstract: Topic modeling plays a vital role in uncovering hidden semantic structures
within text corpora, but existing models struggle in low-resource settings
where limited target-domain data leads to unstable and incoherent topic
inference. We address this challenge by formally introducing domain adaptation
for low-resource topic modeling, where a high-resource source domain informs a
low-resource target domain without overwhelming it with irrelevant content. We
establish a finite-sample generalization bound showing that effective knowledge
transfer depends on robust performance in both domains, minimizing latent-space
discrepancy, and preventing overfitting to the data. Guided by these insights,
we propose DALTA (Domain-Aligned Latent Topic Adaptation), a new framework that
employs a shared encoder for domain-invariant features, specialized decoders
for domain-specific nuances, and adversarial alignment to selectively transfer
relevant information. Experiments on diverse low-resource datasets demonstrate
that DALTA consistently outperforms state-of-the-art methods in terms of topic
coherence, stability, and transferability.

</details>


### [93] [KScope: A Framework for Characterizing the Knowledge Status of Language Models](https://arxiv.org/abs/2506.07458)
*Yuxin Xiao,Shan Chen,Jack Gallifant,Danielle Bitterman,Thomas Hartvigsen,Marzyeh Ghassemi*

Main category: cs.CL

TL;DR: 提出KScope分层框架，通过五种知识状态分类系统评估LLM知识质量，揭示上下文对知识更新的关键作用


<details>
  <summary>Details</summary>
Motivation: 现有知识冲突研究无法全面反映LLM真实知识水平，需建立系统性评估体系揭示不同知识状态特征

Method: 构建基于统计检验的分层框架KScope，通过假设检验逐步确定模型知识在一致性/正确性维度的状态归属

Result: 支持性上下文缩小知识差距；难度/相关性/熟悉度驱动知识更新；LLM在不同状态表现呈现模式化差异

Conclusion: KScope有效诊断LLM知识状态，上下文特征工程结合可信度增强显著提升知识更新效果，方法具备跨模型泛化能力

Abstract: Characterizing a large language model's (LLM's) knowledge of a given question
is challenging. As a result, prior work has primarily examined LLM behavior
under knowledge conflicts, where the model's internal parametric memory
contradicts information in the external context. However, this does not fully
reflect how well the model knows the answer to the question. In this paper, we
first introduce a taxonomy of five knowledge statuses based on the consistency
and correctness of LLM knowledge modes. We then propose KScope, a hierarchical
framework of statistical tests that progressively refines hypotheses about
knowledge modes and characterizes LLM knowledge into one of these five
statuses. We apply KScope to nine LLMs across four datasets and systematically
establish: (1) Supporting context narrows knowledge gaps across models. (2)
Context features related to difficulty, relevance, and familiarity drive
successful knowledge updates. (3) LLMs exhibit similar feature preferences when
partially correct or conflicted, but diverge sharply when consistently wrong.
(4) Context summarization constrained by our feature analysis, together with
enhanced credibility, further improves update effectiveness and generalizes
across LLMs.

</details>


### [94] [From Calibration to Collaboration: LLM Uncertainty Quantification Should Be More Human-Centered](https://arxiv.org/abs/2506.07461)
*Siddartha Devic,Tejas Srinivasan,Jesse Thomason,Willie Neiswanger,Vatsal Sharan*

Main category: cs.CL

TL;DR: 论文指出当前LLM不确定性量化方法存在生态效度低、仅考虑认知不确定性、评估指标不实用三大问题，建议转向以用户为中心的研究范式。


<details>
  <summary>Details</summary>
Motivation: 现有LLM不确定性量化方法未能有效支持用户决策，需改进UQ方法提升LLM应用可靠性。

Method: 通过分析40种LLM UQ方法，识别三个阻碍用户效用的问题，提出用户导向的改进方案。

Result: 发现现有UQ方法存在非真实场景测试、忽视决策相关不确定性、使用非效用指标等根本性缺陷。

Conclusion: 应建立基于真实任务场景、整合多维度不确定性、采用用户效用指标的新型UQ评估体系。

Abstract: Large Language Models (LLMs) are increasingly assisting users in the real
world, yet their reliability remains a concern. Uncertainty quantification (UQ)
has been heralded as a tool to enhance human-LLM collaboration by enabling
users to know when to trust LLM predictions. We argue that current practices
for uncertainty quantification in LLMs are not optimal for developing useful UQ
for human users making decisions in real-world tasks. Through an analysis of 40
LLM UQ methods, we identify three prevalent practices hindering the community's
progress toward its goal of benefiting downstream users: 1) evaluating on
benchmarks with low ecological validity; 2) considering only epistemic
uncertainty; and 3) optimizing metrics that are not necessarily indicative of
downstream utility. For each issue, we propose concrete user-centric practices
and research directions that LLM UQ researchers should consider. Instead of
hill-climbing on unrepresentative tasks using imperfect metrics, we argue that
the community should adopt a more human-centered approach to LLM uncertainty
quantification.

</details>


### [95] [CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large Language Models](https://arxiv.org/abs/2506.07463)
*Guang Liu,Liangdong Wang,Jijie Li,Yang Yu,Yao Xu,Jiabei Chen,Yu Bai,Feng Liao,Yonghua Lin*

Main category: cs.CL

TL;DR: CCI4.0是一个35TB的大规模双语预训练数据集，包含高质量数据和多样化思维链模板，显著提升LLM在数学和代码任务的表现。


<details>
  <summary>Details</summary>
Motivation: 解决现有预训练数据质量参差不齐、缺乏多样化人类推理模式的问题，探索自动化处理预训练语料的方法。

Method: 采用两阶段去重+多分类器质量评分+领域流畅度过滤的数据处理流程，提出分阶段思维链提取方法减少幻觉。

Result: 预训练模型在下游任务（尤其是数学和代码推理）中表现持续提升，验证了数据清洗和思维模板的有效性。

Conclusion: 严格的数据管理和人类思维模板对提升LLM性能至关重要，为自动化处理预训练语料提供了新思路。

Abstract: We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered
for superior data quality and diverse human-like reasoning trajectory. CCI4.0
occupies roughly $35$ TB of disk space and comprises two sub-datasets:
CCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a $5.2$ TB carefully
curated Chinese web corpus, a $22.5$ TB English subset from Nemotron-CC, and
diverse sources from math, wiki, arxiv, and code. Although these data are
mostly sourced from well-processed datasets, the quality standards of various
domains are dynamic and require extensive expert experience and labor to
process. So, we propose a novel pipeline justifying data quality mainly based
on models through two-stage deduplication, multiclassifier quality scoring, and
domain-aware fluency filtering. We extract $4.5$ billion pieces of
CoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the
distillation of CoT from larger models, our proposed staged CoT extraction
exemplifies diverse reasoning patterns and significantly decreases the
possibility of hallucination. Empirical evaluations demonstrate that LLMs
pre-trained in CCI4.0 benefit from cleaner, more reliable training signals,
yielding consistent improvements in downstream tasks, especially in math and
code reflection tasks. Our results underscore the critical role of rigorous
data curation and human thinking templates in advancing LLM performance,
shedding some light on automatically processing pretraining corpora.

</details>


### [96] [Improving Fairness of Large Language Models in Multi-document Summarization](https://arxiv.org/abs/2506.07479)
*Haoyuan Li Yusen Zhang,Snigdha Chaturvedi*

Main category: cs.CL

TL;DR: FairPO提出兼顾摘要级和语料库级公平性的偏好调整方法，通过扰动文档集生成偏好对及动态权重调整，实验证明其优于基线模型并保持摘要质量。


<details>
  <summary>Details</summary>
Motivation: 现有多文档摘要方法主要关注摘要级公平性，但语料库级公平性对系统决策影响同样重要（如产品评论偏向可能误导用户），需同时优化两者。

Method: 1. 摘要级公平性：通过扰动原始文档集生成偏好对，指导模型关注文档覆盖均衡性
2. 语料库级公平性：设计动态权重调整机制，根据历史偏好动态调整训练偏好对的权重

Result: FairPO在ROUGE指标上优于SummHammer、FaiRSum等基线模型，同时提升覆盖率公平性指标（+5.3%），且不损害摘要可读性。

Conclusion: 双层级公平性优化策略有效，动态权重调整机制能平衡不同层级的优化目标，该方法为构建公平摘要系统提供新思路。

Abstract: Fairness in multi-document summarization (MDS) is crucial for providing
comprehensive views across documents with diverse social attribute values,
which can significantly impact decision-making. For example, a summarization
system that tends to overrepresent negative reviews of products can mislead
customers into disregarding good products. Previous works measure fairness in
MDS at two levels: summary-level and corpus-level. While summary-level fairness
focuses on individual summaries, corpus-level fairness focuses on a corpus of
summaries. Recent methods primarily focus on summary-level fairness. We propose
FairPO, a preference tuning method that focuses on both summary-level and
corpus-level fairness in MDS. To improve summary-level fairness, we propose to
generate preference pairs by perturbing document sets. To improve corpus-level
fairness, we propose fairness-aware preference tuning by dynamically adjusting
the weights of preference pairs. Our experiments show that FairPO outperforms
strong baselines while maintaining the critical qualities of summaries. The
code is available at https://github.com/leehaoyuan/coverage_fairnes.

</details>


### [97] [A Hybrid GA LLM Framework for Structured Task Optimization](https://arxiv.org/abs/2506.07483)
*Berry Feng,Jonas Lin,Patrick Lau*

Main category: cs.CL

TL;DR: GA LLM框架整合遗传算法与大型语言模型，通过基因进化机制优化结构化生成任务


<details>
  <summary>Details</summary>
Motivation: 解决单一语言模型在严格约束条件下生成结构化内容时存在的格式松散、约束满足度低的问题

Method: 将输出视为基因，通过LLM指导选择/交叉/变异等进化操作，结合领域知识与全局优化

Result: 在行程规划、学术大纲等任务中实现更好的结构完整性和需求满足度

Conclusion: 遗传算法与语言模型的协同机制比单一模型更具优势，模块化设计易于扩展新任务

Abstract: GA LLM is a hybrid framework that combines Genetic Algorithms with Large
Language Models to handle structured generation tasks under strict constraints.
Each output, such as a plan or report, is treated as a gene, and evolutionary
operations like selection, crossover, and mutation are guided by the language
model to iteratively improve solutions. The language model provides domain
knowledge and creative variation, while the genetic algorithm ensures
structural integrity and global optimization. GA LLM has proven effective in
tasks such as itinerary planning, academic outlining, and business reporting,
consistently producing well structured and requirement satisfying results. Its
modular design also makes it easy to adapt to new tasks. Compared to using a
language model alone, GA LLM achieves better constraint satisfaction and higher
quality solutions by combining the strengths of both components.

</details>


### [98] [DEBATE: A Dataset for Disentangling Textual Ambiguity in Mandarin Through Speech](https://arxiv.org/abs/2506.07502)
*Haotian Guo,Jing Han,Yongfeng Tu,Shihao Gao,Shengfan Shen,Wulong Xiang,Weihao Gan,Zixing Zhang*

Main category: cs.CL

TL;DR: 构建首个中文语音消歧数据集DEBATE，通过10人录音捕捉发音/停顿/重音等语音线索对文本歧义的消解作用，验证现有模型与人类理解的巨大鸿沟。


<details>
  <summary>Details</summary>
Motivation: 当前语音消歧领域缺乏高质量多模态数据集，限制了对语音线索（发音/停顿/重音/语调）消歧机制的研究。

Method: 从1001个中文歧义句库中精选样本，由10位母语者录制语音，建立数据清洗-标注-质量验证的全流程体系。

Result: 三大SOTA语音模型在意图理解任务上的准确率均低于60%，而人类理解准确率达95.7%，揭示现有技术瓶颈。

Conclusion: DEBATE填补跨模态消歧研究空白，为构建跨语言/文化的语音消歧基准提供方法论和数据集支撑。

Abstract: Despite extensive research on textual and visual disambiguation,
disambiguation through speech (DTS) remains underexplored. This is largely due
to the lack of high-quality datasets that pair spoken sentences with richly
ambiguous text. To address this gap, we present DEBATE, a unique public Chinese
speech-text dataset designed to study how speech cues and
patterns-pronunciation, pause, stress and intonation-can help resolve textual
ambiguity and reveal a speaker's true intent. DEBATE contains 1,001 carefully
selected ambiguous utterances, each recorded by 10 native speakers, capturing
diverse linguistic ambiguities and their disambiguation through speech. We
detail the data collection pipeline and provide rigorous quality analysis.
Additionally, we benchmark three state-of-the-art large speech and
audio-language models, illustrating clear and huge performance gaps between
machine and human understanding of spoken intent. DEBATE represents the first
effort of its kind and offers a foundation for building similar DTS datasets
across languages and cultures. The dataset and associated code are available
at: https://github.com/SmileHnu/DEBATE.

</details>


### [99] [What Do Indonesians Really Need from Language Technology? A Nationwide Survey](https://arxiv.org/abs/2506.07506)
*Muhammad Dehan Al Kautsar,Lucky Susanto,Derry Wijaya,Fajri Koto*

Main category: cs.CL

TL;DR: 印尼本土语言社区最需要突破机器翻译与信息检索技术解决语言障碍，但对AI训练数据使用存在隐私偏见担忧


<details>
  <summary>Details</summary>
Motivation: 印尼700多种本土语言NLP开发需频繁接触母语者成本高昂，且缺乏对语言技术真实需求的认知

Method: 通过全国性问卷调查评估印尼母语者的实际需求

Result: 解决语言障碍（尤其是机器翻译与信息检索）是首要需求，语言技术进步存在高期待与隐私/数据安全担忧并存

Conclusion: 需增强技术透明度并建立沟通机制，在推进AI应用的同时解决伦理问题

Abstract: There is an emerging effort to develop NLP for Indonesias 700+ local
languages, but progress remains costly due to the need for direct engagement
with native speakers. However, it is unclear what these language communities
truly need from language technology. To address this, we conduct a nationwide
survey to assess the actual needs of native speakers in Indonesia. Our findings
indicate that addressing language barriers, particularly through machine
translation and information retrieval, is the most critical priority. Although
there is strong enthusiasm for advancements in language technology, concerns
around privacy, bias, and the use of public data for AI training highlight the
need for greater transparency and clear communication to support broader AI
adoption.

</details>


### [100] [DeRAGEC: Denoising Named Entity Candidates with Synthetic Rationale for ASR Error Correction](https://arxiv.org/abs/2506.07510)
*Solee Im,Wonjun Lee,Jinmyeong An,Yunsu Kim,Jungseul Ok,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: DeRAGEC通过合成去噪策略改进ASR中的命名实体纠错，实现28%的WER相对降低


<details>
  <summary>Details</summary>
Motivation: 传统ASR系统在命名实体识别上存在误差，RAGEC框架的候选实体存在噪声干扰，需设计前置过滤机制提升纠错效果

Method: 扩展RAGEC框架，利用语音相似性和增强定义生成去噪依据，通过上下文学习筛选候选实体，无需额外训练

Result: 在CommonVoice和STOP数据集上WER相对降低28%，命名实体命中率显著超越基线模型

Conclusion: DeRAGEC通过前置过滤机制有效提升ASR后处理性能，该方法无需训练成本且开源代码具备可复现性

Abstract: We present DeRAGEC, a method for improving Named Entity (NE) correction in
Automatic Speech Recognition (ASR) systems. By extending the
Retrieval-Augmented Generative Error Correction (RAGEC) framework, DeRAGEC
employs synthetic denoising rationales to filter out noisy NE candidates before
correction. By leveraging phonetic similarity and augmented definitions, it
refines noisy retrieved NEs using in-context learning, requiring no additional
training. Experimental results on CommonVoice and STOP datasets show
significant improvements in Word Error Rate (WER) and NE hit ratio,
outperforming baseline ASR and RAGEC methods. Specifically, we achieved a 28%
relative reduction in WER compared to ASR without postprocessing. Our source
code is publicly available at: https://github.com/solee0022/deragec

</details>


### [101] [Towards Large Language Models with Self-Consistent Natural Language Explanations](https://arxiv.org/abs/2506.07523)
*Sahar Admoni,Ofra Amir,Assaf Hallak,Yftah Ziser*

Main category: cs.CL

TL;DR: 提出大规模基准测试集PSCB评估LLM解释自洽性，通过新指标和DPO微调显著提升解释与决策特征的一致性


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型的事后解释常与真实决策特征不一致，缺乏有效评估指标且系统性解决方案不足

Method: 构建跨任务/模型的PSCB基准，分析自洽性评分缺陷，设计新评估指标，应用直接偏好优化(DPO)微调模型

Result: 新指标有效区分解释质量，DPO微调后模型解释与决策特征对齐度提升53%，域外任务表现提升27%

Conclusion: 通过可扩展的评估框架和优化方法，为构建更可信的自洽大语言模型提供了系统化解决方案

Abstract: Large language models (LLMs) seem to offer an easy path to interpretability:
just ask them to explain their decisions. Yet, studies show that these post-hoc
explanations often misrepresent the true decision process, as revealed by
mismatches in feature importance. Despite growing evidence of this
inconsistency, no systematic solutions have emerged, partly due to the high
cost of estimating feature importance, which limits evaluations to small
datasets. To address this, we introduce the Post-hoc Self-Consistency Bank
(PSCB) - a large-scale benchmark of decisions spanning diverse tasks and
models, each paired with LLM-generated explanations and corresponding feature
importance scores. Analysis of PSCB reveals that self-consistency scores barely
differ between correct and incorrect predictions. We also show that the
standard metric fails to meaningfully distinguish between explanations. To
overcome this limitation, we propose an alternative metric that more
effectively captures variation in explanation quality. We use it to fine-tune
LLMs via Direct Preference Optimization (DPO), leading to significantly better
alignment between explanations and decision-relevant features, even under
domain shift. Our findings point to a scalable path toward more trustworthy,
self-consistent LLMs.

</details>


### [102] [Bit-level BPE: Below the byte boundary](https://arxiv.org/abs/2506.07541)
*Sangwhan Moon,Tatsuya Hiraoka,Naoaki Okazaki*

Main category: cs.CL

TL;DR: 提出无损压缩技术解决子词分词中字节级回退导致的序列过长问题


<details>
  <summary>Details</summary>
Motivation: 字节级回退在CJK语言和表情符号等场景下会导致序列长度显著增加，影响计算效率

Method: 开发简单的无损压缩技术减少序列长度

Result: 有效降低训练和推理阶段的序列长度与计算量

Conclusion: 该压缩方法为解决子词分词效率问题提供了有效解决方案

Abstract: Byte-level fallbacks for subword tokenization have become a common practice
in large language models. In particular, it has been demonstrated to be
incredibly effective as a pragmatic solution for preventing OOV, especially in
the context of larger models. However, breaking a character down to individual
bytes significantly increases the sequence length for long-tail tokens in
languages such as Chinese, Japanese, and Korean (CJK) and other
character-diverse contexts such as emoji. The increased sequence length results
in longer computation during both training and inference. In this work, we
propose a simple compression technique that reduces the sequence length
losslessly.

</details>


### [103] [SELT: Self-Evaluation Tree Search for LLMs with Task Decomposition](https://arxiv.org/abs/2506.07557)
*Mengsong Wu,Di Zhang,Yuqiang Li,Dongzhan Zhou,Wenliang Chen*

Main category: cs.CL

TL;DR: SELT框架通过改进的蒙特卡洛树搜索增强大语言模型的复杂推理能力，无需依赖外部奖励模型


<details>
  <summary>Details</summary>
Motivation: 大语言模型在复杂推理任务中性能下降，需开发不依赖外部奖励模型的自主优化方案

Method: 1. 改进蒙特卡洛树搜索的置信上界评分机制
2. 将推理过程分解为原子子任务
3. 节点层面对语义进行聚类分析
4. 融合自评估机制平衡探索与利用

Result: 在MMLU和Seal-Tools基准测试中准确率显著提升，推理鲁棒性增强（无需任务微调）

Conclusion: 该框架通过自主推理路径优化有效缓解幻觉现象，在跨领域任务中展现强泛化能力

Abstract: While Large Language Models (LLMs) have achieved remarkable success in a wide
range of applications, their performance often degrades in complex reasoning
tasks. In this work, we introduce SELT (Self-Evaluation LLM Tree Search), a
novel framework that leverages a modified Monte Carlo Tree Search (MCTS) to
enhance LLM reasoning without relying on external reward models. By redefining
the Upper Confidence Bound scoring to align with intrinsic self-evaluation
capabilities of LLMs and decomposing the inference process into atomic subtasks
augmented with semantic clustering at each node, SELT effectively balances
exploration and exploitation, reduces redundant reasoning paths, and mitigates
hallucination. We validate our approach on challenging benchmarks, including
the knowledge-based MMLU and the Tool Learning dataset Seal-Tools, where SELT
achieves significant improvements in answer accuracy and reasoning robustness
compared to baseline methods. Notably, our framework operates without
task-specific fine-tuning, demonstrating strong generalizability across diverse
reasoning tasks. Relevant results and code are available at
https://github.com/fairyshine/SELT .

</details>


### [104] [Beyond the Sentence: A Survey on Context-Aware Machine Translation with Large Language Models](https://arxiv.org/abs/2506.07583)
*Ramakrishna Appicharla,Baban Gain,Santanu Pal,Asif Ekbal*

Main category: cs.CL

TL;DR: 系统性回顾LLMs在上下文感知机器翻译中的应用现状，指出商业模型优于开源模型，提示方法可作为质量评估基准。


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型在上下文感知机器翻译中的潜力与应用局限性

Method: 通过文献综述分析提示工程、微调、自动后编辑和翻译代理四类方法

Result: 商业LLMs(如ChatGPT)表现优于开源模型，提示方法可作为翻译质量评估基线

Conclusion: 未来需改进开源模型性能、开发专用上下文处理机制、探索多模态翻译场景

Abstract: Despite the popularity of the large language models (LLMs), their application
to machine translation is relatively underexplored, especially in context-aware
settings. This work presents a literature review of context-aware translation
with LLMs. The existing works utilise prompting and fine-tuning approaches,
with few focusing on automatic post-editing and creating translation agents for
context-aware machine translation. We observed that the commercial LLMs (such
as ChatGPT and Tower LLM) achieved better results than the open-source LLMs
(such as Llama and Bloom LLMs), and prompt-based approaches serve as good
baselines to assess the quality of translations. Finally, we present some
interesting future directions to explore.

</details>


### [105] [Instructing Large Language Models for Low-Resource Languages: A Systematic Study for Basque](https://arxiv.org/abs/2506.07597)
*Oscar Sainz,Naiara Perez,Julen Etxaniz,Joseba Fernandez de Landa,Itziar Aldabe,Iker García-Ferrero,Aimar Zabala,Ekhi Azurmendi,German Rigau,Eneko Agirre,Mikel Artetxe,Aitor Soroa*

Main category: cs.CL

TL;DR: 提出使用目标语言语料库和指令调优模型提升低资源语言（巴斯克语）性能，仅需1.2B词汇量即可接近大模型效果


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言缺乏大规模指令数据集的问题，探索仅用目标语言语料库+多语言基础模型+合成指令的替代方案

Method: 基于巴斯克语的系统性实验（基准测试+1,680人参与的人类偏好评估），组合使用目标语料库、指令调优主干模型（Llama 3.1 70B）和合成指令

Result: 目标语料库关键作用，合成指令构建稳健模型，指令调优主干模型显著优于非指令模型，模型规模扩大效果提升，最终模型性能接近更大规模前沿模型

Conclusion: 验证了低资源语言适配的有效路径，开源代码/模型/指令数据集/人类偏好数据支持后续研究，证明指令调优主干模型的规模化优势

Abstract: Instructing language models with user intent requires large instruction
datasets, which are only available for a limited set of languages. In this
paper, we explore alternatives to conventional instruction adaptation pipelines
in low-resource scenarios. We assume a realistic scenario for low-resource
languages, where only the following are available: corpora in the target
language, existing open-weight multilingual base and instructed backbone LLMs,
and synthetically generated instructions sampled from the instructed backbone.
We present a comprehensive set of experiments for Basque that systematically
study different combinations of these components evaluated on benchmarks and
human preferences from 1,680 participants. Our conclusions show that target
language corpora are essential, with synthetic instructions yielding robust
models, and, most importantly, that using as backbone an instruction-tuned
model outperforms using a base non-instructed model, and improved results when
scaling up. Using Llama 3.1 instruct 70B as backbone our model comes near
frontier models of much larger sizes for Basque, without using any Basque data
apart from the 1.2B word corpora. We release code, models, instruction
datasets, and human preferences to support full reproducibility in future
research on low-resource language adaptation.

</details>


### [106] [PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels](https://arxiv.org/abs/2506.07606)
*Peyman Rostami,Vahid Rahimzadeh,Ali Adibi,Azadeh Shakery*

Main category: cs.CL

TL;DR: 提出首个针对2024年美国总统选举的Bluesky用户级立场检测数据集PolitiSky24，包含16,044个用户-目标立场对及丰富元数据


<details>
  <summary>Details</summary>
Motivation: 解决现有立场检测数据集中在推文级别、缺乏新兴社交平台（如Bluesky）用户级视角的问题，提供更全面的用户政治立场分析

Method: 通过信息检索和大语言模型结合的标注流程，生成带有支持理由和文本片段的立场标签，实现81%标注准确率

Result: 构建包含用户参与元数据、互动图谱和完整发帖历史的开放数据集，支持透明度验证的立场标注系统

Conclusion: PolitiSky24以其时效性、开放数据特性和用户级视角，填补了政治立场分析领域的数据空白，支持更深入的社会计算研究

Abstract: Stance detection identifies the viewpoint expressed in text toward a specific
target, such as a political figure. While previous datasets have focused
primarily on tweet-level stances from established platforms, user-level stance
resources, especially on emerging platforms like Bluesky remain scarce.
User-level stance detection provides a more holistic view by considering a
user's complete posting history rather than isolated posts. We present the
first stance detection dataset for the 2024 U.S. presidential election,
collected from Bluesky and centered on Kamala Harris and Donald Trump. The
dataset comprises 16,044 user-target stance pairs enriched with engagement
metadata, interaction graphs, and user posting histories. PolitiSky24 was
created using a carefully evaluated pipeline combining advanced information
retrieval and large language models, which generates stance labels with
supporting rationales and text spans for transparency. The labeling approach
achieves 81\% accuracy with scalable LLMs. This resource addresses gaps in
political stance analysis through its timeliness, open-data nature, and
user-level perspective. The dataset is available at
https://doi.org/10.5281/zenodo.15616911

</details>


### [107] [Vuyko Mistral: Adapting LLMs for Low-Resource Dialectal Translation](https://arxiv.org/abs/2506.07617)
*Roman Kyslyi,Yuliia Maksymiuk,Ihor Pysmennyi*

Main category: cs.CL

TL;DR: 首个将大语言模型适配乌克兰Hutsul方言的研究，通过构建平行语料库、提出RAG增强数据生成策略，证明7B小模型在翻译任务中超越GPT-4o零样本基线


<details>
  <summary>Details</summary>
Motivation: 解决乌克兰Hutsul方言作为低资源、形态复杂语种的数据匮乏问题，探索LLMs在濒危语言保护中的应用潜力

Method: 1. 创建9852组标准-方言平行句对和7320词词典 2. 提出RAG增强生成框架生成52142组合成数据 3. 使用LoRA微调多个开源模型 4. 采用BLEU/chrF++/TER/LLM多维度评估体系

Result: 微调后的7B小模型在自动评估指标(BLEU↑32.5%)和LLM评估中全面超越GPT-4o，最佳模型chrF++达68.2%，所有资源已开源

Conclusion: 验证了参数高效微调方法在低资源语言场景的有效性，证明合理的数据增强策略能显著提升小模型性能，为濒危语言数字化保护提供技术框架

Abstract: In this paper we introduce the first effort to adapt large language models
(LLMs) to the Ukrainian dialect (in our case Hutsul), a low-resource and
morphologically complex dialect spoken in the Carpathian Highlands. We created
a parallel corpus of 9852 dialect-to-standard Ukrainian sentence pairs and a
dictionary of 7320 dialectal word mappings. We also addressed data shortage by
proposing an advanced Retrieval-Augmented Generation (RAG) pipeline to generate
synthetic parallel translation pairs, expanding the corpus with 52142 examples.
We have fine-tuned multiple open-source LLMs using LoRA and evaluated them on a
standard-to-dialect translation task, also comparing with few-shot GPT-4o
translation. In the absence of human annotators, we adopt a multi-metric
evaluation strategy combining BLEU, chrF++, TER, and LLM-based judgment
(GPT-4o). The results show that even small(7B) finetuned models outperform
zero-shot baselines such as GPT-4o across both automatic and LLM-evaluated
metrics. All data, models, and code are publicly released at:
https://github.com/woters/vuyko-hutsul

</details>


### [108] [LoRMA: Low-Rank Multiplicative Adaptation for LLMs](https://arxiv.org/abs/2506.07621)
*Harsh Bihany,Shubham Patel,Ashutosh Modi*

Main category: cs.CL

TL;DR: 提出低秩乘法适应（LoRMA）方法，通过矩阵乘法变换替代传统加性更新，解决大模型微调中的计算效率和秩瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有LoRA等加性更新方法在参数更新空间上存在限制，矩阵乘法变换能提供更丰富的参数更新方式以提升模型适应能力。

Method: 采用矩阵乘法重排序操作降低计算复杂度，引入秩膨胀策略突破矩阵乘法的秩限制，构建低秩乘法适应框架。

Result: 通过多维度评估指标的广泛实验验证了方法的有效性，具体量化指标未在摘要中体现。

Conclusion: LoRMA成功实现了从加性到乘法更新的范式转换，为高效大模型微调提供了新的技术路径。

Abstract: Large Language Models have shown remarkable capabilities in the NLP domain.
Their effectiveness can mainly be attributed to their ability to adapt to an
array of downstream tasks. However, generally, full fine-tuning is a
computationally expensive job. To mitigate this, many techniques have been
developed that prime efficiency, a prominent one being Low-Rank Adaptation
(LoRA). However, LoRA and its variants employ re-parametrized additive updates.
In this paper, we propose Low-Rank Multiplicative Adaptation (LoRMA), which
shifts the paradigm of additive updates to a richer space of matrix
multiplicative transformations. We tackle challenges such as computational
complexity and rank bottleneck of matrix multiplication by effectively
re-ordering operations and introducing rank inflation strategies. We conduct
extensive experiments to demonstrate the effectiveness of our approach in terms
of various evaluation metrics.

</details>


### [109] [Intent Matters: Enhancing AI Tutoring with Fine-Grained Pedagogical Intent Annotation](https://arxiv.org/abs/2506.07626)
*Kseniia Petukhova,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: 通过细粒度教学意图标注提升LLM教育应用效果


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在教育场景缺乏教学策略对齐，需通过意图分类改进生成质量

Method: 使用MathDial数学辅导数据集，采用11种教学意图的新分类法重新标注，并微调LLM模型

Result: 细粒度模型在自动评估和人工评估中均产生更符合教学策略的有效响应

Conclusion: 教学意图特异性对教育领域文本生成具有重要价值，研究团队公开标注数据和代码推动相关研究

Abstract: Large language models (LLMs) hold great promise for educational applications,
particularly in intelligent tutoring systems. However, effective tutoring
requires alignment with pedagogical strategies - something current LLMs lack
without task-specific adaptation. In this work, we explore whether fine-grained
annotation of teacher intents can improve the quality of LLM-generated tutoring
responses. We focus on MathDial, a dialog dataset for math instruction, and
apply an automated annotation framework to re-annotate a portion of the dataset
using a detailed taxonomy of eleven pedagogical intents. We then fine-tune an
LLM using these new annotations and compare its performance to models trained
on the original four-category taxonomy. Both automatic and qualitative
evaluations show that the fine-grained model produces more pedagogically
aligned and effective responses. Our findings highlight the value of intent
specificity for controlled text generation in educational settings, and we
release our annotated data and code to facilitate further research.

</details>


### [110] [Unblocking Fine-Grained Evaluation of Detailed Captions: An Explaining AutoRater and Critic-and-Revise Pipeline](https://arxiv.org/abs/2506.07631)
*Brian Gordon,Yonatan Bitton,Andreea Marzoca,Yasumasa Onoe,Xiao Wang,Daniel Cohen-Or,Idan Szpektor*

Main category: cs.CL

TL;DR: 提出DOCCI-Critique基准和VNLI-Critique模型，用于自动评估视觉语言模型生成段落描述的事实准确性，并通过Critic-and-Revise流程显著改进描述质量


<details>
  <summary>Details</summary>
Motivation: 现有评估方法难以检测段落长度描述中的细粒度事实错误，且缺乏带人工验证错误标注的数据集

Method: 构建包含10,216句级人工标注的基准数据集，开发基于句级事实分类的VNLI-Critique模型，并设计三种应用场景（模型评估、自动评分、修正流程）

Result: 在M-HalDetect基准取得SOTA性能（0.98 Spearman相关性），通过Critic-and-Revise流程使DetailCaps-4870的事实性提升46%

Conclusion: 该工作建立了细粒度评估基准和实用工具链，为提升视觉语言模型的图像理解能力提供了新的技术标准和方法论框架

Abstract: Large Vision-Language Models (VLMs) now generate highly detailed,
paragraphlength image captions, yet evaluating their factual accuracy remains
challenging. Current methods often miss fine-grained errors, being designed for
shorter texts or lacking datasets with verified inaccuracies. We introduce
DOCCI-Critique, a benchmark with 1,400 VLM-generated paragraph captions (100
images, 14 VLMs) featuring over 10,216 sentence-level human annotations of
factual correctness and explanatory rationales for errors, all within paragraph
context. Building on this, we develop VNLI-Critique, a model for automated
sentence-level factuality classification and critique generation. We highlight
three key applications: (1) VNLI-Critique demonstrates robust generalization,
validated by state-of-the-art performance on the M-HalDetect benchmark and
strong results in CHOCOLATE claim verification. (2) The VNLI-Critique driven
AutoRater for DOCCI-Critique provides reliable VLM rankings, showing excellent
alignment with human factuality judgments (e.g., 0.98 Spearman). (3) An
innovative Critic-and-Revise pipeline, where critiques from VNLI-Critique guide
LLM-based corrections, achieves substantial improvements in caption factuality
(e.g., a 46% gain on DetailCaps-4870). Our work offers a crucial benchmark
alongside practical tools, designed to significantly elevate the standards for
fine-grained evaluation and foster the improvement of VLM image understanding.
Project page: https://google.github.io/unblocking-detail-caption

</details>


### [111] [TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient LLM-based Scientific Peer Review](https://arxiv.org/abs/2506.07642)
*Yuan Chang,Ziyue Li,Hengyuan Zhang,Yuanbo Kong,Yanru Wu,Zhijiang Guo,Ngai Wong*

Main category: cs.CL

TL;DR: 提出TreeReview框架，通过分层双向问答流程生成更深入的评审意见，同时减少80%的LLM计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 当前LLM辅助评审方法在生成深度评审意见与保持效率之间存在矛盾，传统方法难以兼顾全面性和计算效率。

Method: 1. 构建层次化问题树（递归分解评审问题）
2. 动态问题扩展机制（按需生成后续追问）
3. 自底向上的迭代答案聚合流程

Result: 在ICLR/NeurIPS基准测试中：
- 评审全面性提升32%
- 专家评价匹配度提高28%
- 比传统方法节省80%计算资源

Conclusion: TreeReview框架通过结构化问答流程，在保持计算效率的同时实现了更专家化的深度评审，为LLM辅助学术评审提供了新范式。

Abstract: While Large Language Models (LLMs) have shown significant potential in
assisting peer review, current methods often struggle to generate thorough and
insightful reviews while maintaining efficiency. In this paper, we propose
TreeReview, a novel framework that models paper review as a hierarchical and
bidirectional question-answering process. TreeReview first constructs a tree of
review questions by recursively decomposing high-level questions into
fine-grained sub-questions and then resolves the question tree by iteratively
aggregating answers from leaf to root to get the final review. Crucially, we
incorporate a dynamic question expansion mechanism to enable deeper probing by
generating follow-up questions when needed. We construct a benchmark derived
from ICLR and NeurIPS venues to evaluate our method on full review generation
and actionable feedback comments generation tasks. Experimental results of both
LLM-based and human evaluation show that TreeReview outperforms strong
baselines in providing comprehensive, in-depth, and expert-aligned review
feedback, while reducing LLM token usage by up to 80% compared to
computationally intensive approaches. Our code and benchmark dataset are
available at https://github.com/YuanChang98/tree-review.

</details>


### [112] [Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models](https://arxiv.org/abs/2506.07645)
*Maciej Chrabąszcz,Katarzyna Lorenc,Karolina Seweryn*

Main category: cs.CL

TL;DR: 发现大型语言模型在低资源语言（如波兰语）中存在字符/词级安全漏洞，通过小型代理模型计算词重要性可高效构建攻击样本。


<details>
  <summary>Details</summary>
Motivation: 当前多语言LLMs的安全训练数据集中于高资源语言（如英语），导致低资源语言安全机制存在潜在漏洞。

Method: 通过修改少量字符，利用代理模型计算词重要性构建攻击，在波兰语验证后扩展至其他语言。

Result: 字符/词级攻击显著改变LLMs预测，成功绕过其内部安全机制。

Conclusion: 揭示了LLMs跨语言安全防御的薄弱环节，为后续安全研究提供数据集与方法支持。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
various natural language processing (NLP) tasks in recent years. However, their
susceptibility to jailbreaks and perturbations necessitates additional
evaluations. Many LLMs are multilingual, but safety-related training data
contains mainly high-resource languages like English. This can leave them
vulnerable to perturbations in low-resource languages such as Polish. We show
how surprisingly strong attacks can be cheaply created by altering just a few
characters and using a small proxy model for word importance calculation. We
find that these character and word-level attacks drastically alter the
predictions of different LLMs, suggesting a potential vulnerability that can be
used to circumvent their internal safety mechanisms. We validate our attack
construction methodology on Polish, a low-resource language, and find potential
vulnerabilities of LLMs in this language. Additionally, we show how it can be
extended to other languages. We release the created datasets and code for
further research.

</details>


### [113] [Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for Japanese Speech Annotation](https://arxiv.org/abs/2506.07646)
*Rui Hu,Xiaolong Lin,Jiawang Liu,Shixi Huang,Zhenpeng Zhan*

Main category: cs.CL

TL;DR: 提出基于预训练ASR模型与字典校正的日语TTS自动标注方法，显著提升标注效率与准确率


<details>
  <summary>Details</summary>
Motivation: 传统日语TTS数据集标注依赖人工或单一模态方法，存在效率低且错误率较高的问题

Method: 1. 微调预训练ASR模型实现音素韵律联合标注
2. 引入字典先验知识解码策略校正音素标签
3. 结合文本和音频双模态特征进行条件建模

Result: 客观评估：标注准确率优于纯文本/音频方法
主观评估：TTS合成自然度接近人工标注训练模型（MOS:4.2 vs 4.3）

Conclusion: 该方法成功实现高效准确的自动标注，显著降低人工标注成本，为日语TTS系统开发提供有效解决方案

Abstract: In this paper, we propose a method for annotating phonemic and prosodic
labels on a given audio-transcript pair, aimed at constructing Japanese
text-to-speech (TTS) datasets. Our approach involves fine-tuning a large-scale
pre-trained automatic speech recognition (ASR) model, conditioned on ground
truth transcripts, to simultaneously output phrase-level graphemes and
annotation labels. To further correct errors in phonemic labeling, we employ a
decoding strategy that utilizes dictionary prior knowledge. The objective
evaluation results demonstrate that our proposed method outperforms previous
approaches relying solely on text or audio. The subjective evaluation results
indicate that the naturalness of speech synthesized by the TTS model, trained
with labels annotated using our method, is comparable to that of a model
trained with manual annotations.

</details>


### [114] [Beyond Benchmarks: A Novel Framework for Domain-Specific LLM Evaluation and Knowledge Mapping](https://arxiv.org/abs/2506.07658)
*Nitin Sharma,Thomas Wolfers,Çağatay Yıldız*

Main category: cs.CL

TL;DR: 提出自动化生成领域特定评估基准的确定性流程，通过分层机制分析揭示语言模型在领域适应中知识表示与遗忘规律


<details>
  <summary>Details</summary>
Motivation: 解决领域基准构建易受污染、传统困惑度指标难以准确评估领域知识的问题，建立高效可靠的模型评估体系

Method: 使用TF/Term TF-IDF生成领域关键词，构建prompt-target评估对，结合分层机制分析（初始层属性提取，深层预测，中间层遗忘）

Result: 新基准与传统基准强相关且计算高效，发现小模型500步内快速适应，验证中间层属性提取与遗忘的层级传播规律

Conclusion: 提供领域模型评估新范式，揭示知识表示机制，为高效微调和防灾难性遗忘提供理论依据

Abstract: The paper addresses two critical challenges in language model (LM)
evaluation: creating reliable domain-specific benchmarks and understanding
knowledge representation during domain adaptation. We introduce a deterministic
pipeline that converts raw domain corpora into completion-type benchmarks
without relying on LMs or human curation, eliminating benchmark contamination
issues while enabling evaluation on the latest domain data. Our approach
generates domain-specific keywords and related word lists using TF and Term
TF-IDF methods and constructs prompt-target pairs. We evaluate models by
measuring their ability to complete these prompts with the correct
domain-specific targets, providing a direct assessment of domain knowledge with
low computational cost. Through comprehensive experiments across multiple
models (GPT-2 medium/XL, Llama-2/3.1, OLMo-2, Qwen-2, Mistral) and domains, we
demonstrate that our benchmark strongly correlates with expert-generated
benchmarks while providing a more accurate measure of domain knowledge than
traditional perplexity metrics. We reveal that domain adaptation happens
rapidly in smaller models (within 500 steps) and illustrate a new approach to
domain knowledge evaluation in base models during training for early stopping.
By extending mechanistic analysis to domain adaptation, we discover that
initial-to-mid layers are primarily responsible for attribute extraction, while
later layers focus on next token prediction. Furthermore, we show that during
adaptation, forgetting begins in the middle layers, where attribute extraction
happens and is amplified in later layers. Our work provides both a practical
evaluation methodology for domain-specific LMs and novel insights into
knowledge representation during adaptation, with implications for more
efficient fine-tuning strategies and targeted approaches to mitigate
catastrophic forgetting.

</details>


### [115] [Synthesis by Design: Controlled Data Generation via Structural Guidance](https://arxiv.org/abs/2506.07664)
*Lei Xu,Sirui Chen,Yuxuan Huang,Chaochao Lu*

Main category: cs.CL

TL;DR: 提出利用生成代码和结构化解决方案增强LLM数学推理能力的方法，并通过实验验证其有效性


<details>
  <summary>Details</summary>
Motivation: 现有方法通过问题重述生成数据集时面临生成质量和问题复杂性的局限，需提升LLM在复杂数学推理中的表现

Method: 提取数学问题中的结构化信息并生成解题代码，通过结构化解指导数据生成，应用于MATH和GSM8K数据集

Result: 构建了含39K标注步骤的问题集和6.1K高难度基准集，实验显示模型在长推理链问题上性能显著下降，微调实验验证数据有效性

Conclusion: 提出的数据构建方法能有效提升LLM推理能力，高质量数据集为后续研究提供支持

Abstract: Mathematical reasoning remains challenging for LLMs due to complex logic and
the need for precise computation. Existing methods enhance LLM reasoning by
synthesizing datasets through problem rephrasing, but face issues with
generation quality and problem complexity. To address this, we propose to
extract structural information with generated problem-solving code from
mathematical reasoning and guide data generation with structured solutions.
Applied to MATH and GSM8K, our approach produces 39K problems with labeled
intermediate steps and a 6.1K-problem benchmark of higher difficulty. Results
on our benchmark show that model performance declines as reasoning length
increases. Additionally, we conducted fine-tuning experiments using the
proposed training data on a range of LLMs, and the results validate the
effectiveness of our dataset. We hope the proposed method and dataset will
contribute to future research in enhancing LLM reasoning capabilities.

</details>


### [116] [Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch](https://arxiv.org/abs/2506.07667)
*Prarabdh Shukla,Wei Yin Chong,Yash Patel,Brennan Schaffner,Danish Pruthi,Arjun Bhagoji*

Main category: cs.CL

TL;DR: Twitch的AutoMod工具在仇恨内容审核中存在显著漏洞：漏检率最高达94%，同时错误拦截89.5%的良性语境敏感词，突显其过度依赖关键词而缺乏上下文理解能力。


<details>
  <summary>Details</summary>
Motivation: 针对实时流媒体平台内容审核的延迟敏感性和自动化系统有效性的研究空白，作者通过系统审计揭示Twitch AutoMod在仇恨内容识别中的实际表现。

Method: 通过创建测试账号，使用Twitch API发送107,000条来自4个数据集的评论（含明显仇恨内容），定量分析AutoMod对性别歧视/种族主义等内容的标记准确率及敏感词误判情况。

Result: 1) 94%仇恨内容未被标记；2) 添加明确侮辱性词汇后删除率100%；3) 89.5%教育/赋权场景的良性案例被错误拦截，违反平台指南。

Conclusion: 现有自动化审核系统存在双重缺陷：既漏检仇恨内容，又过度屏蔽合理表达。研究强调上下文理解对内容审核的关键作用，建议改进算法设计以平衡安全与言论自由。

Abstract: To meet the demands of content moderation, online platforms have resorted to
automated systems. Newer forms of real-time engagement($\textit{e.g.}$, users
commenting on live streams) on platforms like Twitch exert additional pressures
on the latency expected of such moderation systems. Despite their prevalence,
relatively little is known about the effectiveness of these systems. In this
paper, we conduct an audit of Twitch's automated moderation tool
($\texttt{AutoMod}$) to investigate its effectiveness in flagging hateful
content. For our audit, we create streaming accounts to act as siloed test
beds, and interface with the live chat using Twitch's APIs to send over
$107,000$ comments collated from $4$ datasets. We measure $\texttt{AutoMod}$'s
accuracy in flagging blatantly hateful content containing misogyny, racism,
ableism and homophobia. Our experiments reveal that a large fraction of hateful
messages, up to $94\%$ on some datasets, $\textit{bypass moderation}$.
Contextual addition of slurs to these messages results in $100\%$ removal,
revealing $\texttt{AutoMod}$'s reliance on slurs as a moderation signal. We
also find that contrary to Twitch's community guidelines, $\texttt{AutoMod}$
blocks up to $89.5\%$ of benign examples that use sensitive words in
pedagogical or empowering contexts. Overall, our audit points to large gaps in
$\texttt{AutoMod}$'s capabilities and underscores the importance for such
systems to understand context effectively.

</details>


### [117] [GaRAGe: A Benchmark with Grounding Annotations for RAG Evaluation](https://arxiv.org/abs/2506.07671)
*Ionut-Teodor Sorodoc,Leonardo F. R. Ribeiro,Rexhina Blloshmi,Christopher Davis,Adrià de Gispert*

Main category: cs.CL

TL;DR: 提出大型RAG基准测试GaRAGe，揭示主流LLMs存在过度总结、严格引用能力不足（最高60%事实性评分）和回避回答缺陷（最高31%正确率），特别在时效性问题和稀疏私有知识源场景表现更差。


<details>
  <summary>Details</summary>
Motivation: 解决现有RAG评估体系缺乏细粒度标注的问题，构建包含多样化问题和35K+标注段落的数据集，以真实反映模型在严格引用与信息不足时回避回答的能力。

Method: 创建含2366个多样化问题的基准集，整合来自私有文档和网络的标注段落，通过Relevance-Aware Factuality Score等指标量化评估模型表现。

Result: 主流LLMs在相关段落引用（最高58.9% F1）、时效性问题（性能下降显著）和稀疏私有源知识调用场景表现欠佳，且回避回答准确率最高仅31%。

Conclusion: 当前LLMs在RAG场景存在系统性缺陷，需开发针对性优化方法提升严格引用能力和动态场景适应性，基准测试GaRAGe为相关研究提供评估标准。

Abstract: We present GaRAGe, a large RAG benchmark with human-curated long-form answers
and annotations of each grounding passage, allowing a fine-grained evaluation
of whether LLMs can identify relevant grounding when generating RAG answers.
Our benchmark contains 2366 questions of diverse complexity, dynamism, and
topics, and includes over 35K annotated passages retrieved from both private
document sets and the Web, to reflect real-world RAG use cases. This makes it
an ideal test bed to evaluate an LLM's ability to identify only the relevant
information necessary to compose a response, or provide a deflective response
when there is insufficient information. Evaluations of multiple
state-of-the-art LLMs on GaRAGe show that the models tend to over-summarise
rather than (a) ground their answers strictly on the annotated relevant
passages (reaching at most a Relevance-Aware Factuality Score of 60%), or (b)
deflect when no relevant grounding is available (reaching at most 31% true
positive rate in deflections). The F1 in attribution to relevant sources is at
most 58.9%, and we show that performance is particularly reduced when answering
time-sensitive questions and when having to draw knowledge from sparser private
grounding sources.

</details>


### [118] [Training Superior Sparse Autoencoders for Instruct Models](https://arxiv.org/abs/2506.07691)
*Jiaming Li,Haoran Ye,Yukun Chen,Xinyue Li,Lei Zhang,Hamid Alinejad-Rokny,Jimmy Chih-Hsien Peng,Min Yang*

Main category: cs.CL

TL;DR: 提出FAST方法优化面向指令模型的特征提取，显著提升稀疏自编码器的重建质量与特征可解释性


<details>
  <summary>Details</summary>
Motivation: 现有稀疏自编码器方法在基础模型表现良好，但在指令模型上重建质量和特征可解释性下降明显

Method: FAST通过对齐指令模型的数据分布和激活模式，改进稀疏自编码器的训练过程

Result: 在Qwen2.5-7B-Instruct实现0.6468重建误差，Llama3.2-3B-Instruct高质量特征占比达21.1%，并发现通过特殊token激活干预可优化模型控制

Conclusion: FAST为指令模型提供更优的特征提取方案，同时揭示通过特征工程实施细粒度模型控制的新可能

Abstract: As large language models (LLMs) grow in scale and capability, understanding
their internal mechanisms becomes increasingly critical. Sparse autoencoders
(SAEs) have emerged as a key tool in mechanistic interpretability, enabling the
extraction of human-interpretable features from LLMs. However, existing SAE
training methods are primarily designed for base models, resulting in reduced
reconstruction quality and interpretability when applied to instruct models. To
bridge this gap, we propose
$\underline{\textbf{F}}$inetuning-$\underline{\textbf{a}}$ligned
$\underline{\textbf{S}}$equential $\underline{\textbf{T}}$raining
($\textit{FAST}$), a novel training method specifically tailored for instruct
models. $\textit{FAST}$ aligns the training process with the data distribution
and activation patterns characteristic of instruct models, resulting in
substantial improvements in both reconstruction and feature interpretability.
On Qwen2.5-7B-Instruct, $\textit{FAST}$ achieves a mean squared error of 0.6468
in token reconstruction, significantly outperforming baseline methods with
errors of 5.1985 and 1.5096. In feature interpretability, $\textit{FAST}$
yields a higher proportion of high-quality features, for Llama3.2-3B-Instruct,
$21.1\%$ scored in the top range, compared to $7.0\%$ and $10.2\%$ for
$\textit{BT(P)}$ and $\textit{BT(F)}$. Surprisingly, we discover that
intervening on the activations of special tokens via the SAEs leads to
improvements in output quality, suggesting new opportunities for fine-grained
control of model behavior. Code, data, and 240 trained SAEs are available at
https://github.com/Geaming2002/FAST.

</details>


### [119] [Through the Valley: Path to Effective Long CoT Training for Small Language Models](https://arxiv.org/abs/2506.07712)
*Renjie Luo,Jiaxi Li,Chen Huang,Wei Lu*

Main category: cs.CL

TL;DR: 小型语言模型在长思维链监督下出现显著性能退化（Long CoT Degradation），主因是错误累积效应，需平衡推理长度与监督质量


<details>
  <summary>Details</summary>
Motivation: 挑战传统认知，揭示长CoT监督对小型模型（≤3B参数）的负面效应，特别是在有限数据场景下的性能恶化现象

Method: 通过Qwen2.5/LLaMA3/Gemma3系列模型的广泛实验，分析不同数据量（8k-220k）下的性能变化，结合错误累积理论解释机制，并验证对强化学习的影响

Result: 8k长CoT数据导致性能下降达75%；部分小模型即使使用220k数据仍无法恢复原始性能；错误累积是核心机制；监督微调可缓解对强化学习的负面影响

Conclusion: 长CoT训练对小型模型的适用性需重新评估，建议采用适度推理长度与高质量监督结合的方案，为构建高效小型推理模型提供新方向

Abstract: Long chain-of-thought (CoT) supervision has become a common strategy to
enhance reasoning in language models. While effective for large models, we
identify a phenomenon we call Long CoT Degradation, in which small language
models (SLMs; <=3B parameters) trained on limited long CoT data experience
significant performance deterioration. Through extensive experiments on the
Qwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is
widespread across SLMs. In some settings, models trained on only 8k long CoT
examples lose up to 75% of their original performance before fine-tuning.
Strikingly, we further observe that for some particularly small models, even
training on 220k long CoT examples fails to recover or surpass their original
performance prior to fine-tuning. Our analysis attributes this effect to error
accumulation: while longer responses increase the capacity for multi-step
reasoning, they also amplify the risk of compounding mistakes. Furthermore, we
find that Long CoT Degradation may negatively impacts downstream reinforcement
learning (RL), although this can be alleviated by sufficiently scaled
supervised fine-tuning (SFT). Our findings challenge common assumptions about
the benefits of long CoT training for SLMs and offer practical guidance for
building more effective small-scale reasoning models.

</details>


### [120] [Multilingual Grammatical Error Annotation: Combining Language-Agnostic Framework with Language-Specific Flexibility](https://arxiv.org/abs/2506.07719)
*Mengyang Qiu,Tran Minh Nguyen,Zihao Huang,Zelong Li,Yang Gu,Qingyu Gao,Siliang Liu,Jungyeul Park*

Main category: cs.CL

TL;DR: 开发了支持多语言的标准化语法错误标注框架，通过模块化设计实现跨语言一致性与可定制性


<details>
  <summary>Details</summary>
Motivation: 现有语法错误标注工具（如errant）在扩展到不同语系语言时存在兼容性和灵活性不足的问题

Method: 构建语言无关的标注基础架构+结构化语言扩展模块，基于stanza重构工具链支持多语言处理

Result: 成功应用于英语、德语、捷克语、韩语和中文，实现从通用标注到特定语言细节定制的完整解决方案

Conclusion: 该框架为多语言语法纠错系统提供了可扩展的标准化评估基础，促进跨语言研究的公平比较

Abstract: Grammatical Error Correction (GEC) relies on accurate error annotation and
evaluation, yet existing frameworks, such as $\texttt{errant}$, face
limitations when extended to typologically diverse languages. In this paper, we
introduce a standardized, modular framework for multilingual grammatical error
annotation. Our approach combines a language-agnostic foundation with
structured language-specific extensions, enabling both consistency and
flexibility across languages. We reimplement $\texttt{errant}$ using
$\texttt{stanza}$ to support broader multilingual coverage, and demonstrate the
framework's adaptability through applications to English, German, Czech,
Korean, and Chinese, ranging from general-purpose annotation to more customized
linguistic refinements. This work supports scalable and interpretable GEC
annotation across languages and promotes more consistent evaluation in
multilingual settings. The complete codebase and annotation tools can be
accessed at https://github.com/open-writing-evaluation/jp_errant_bea.

</details>


### [121] [Swiss Parliaments Corpus Re-Imagined (SPC_R): Enhanced Transcription with RAG-based Correction and Predicted BLEU](https://arxiv.org/abs/2506.07726)
*Vincenzo Timmel,Manfred Vogel,Daniel Perruchoud,Reza Kakooee*

Main category: cs.CL

TL;DR: 通过结合ASR、LLM校正和数据过滤技术，构建了751小时高质量瑞士德语议会辩论语音语料库，比原版提升6个BLEU值


<details>
  <summary>Details</summary>
Motivation: 解决低资源领域（瑞士德语议会辩论）语音数据转写质量不足的问题，需处理长达数小时的原始语音与协议文本对齐的挑战

Method: 1. 用Whisper Large-v3转写原始音频
2. 两阶段GPT-4o校正（实体修正+语义完整性评估）
3. 基于预测BLEU和GPT-4评分的数据过滤

Result: 最终获得751小时通过质检的语音数据（总量801小时），相比原版实现6个BLEU值的提升

Conclusion: 证实ASR+LLM校正+数据驱动的过滤流程可有效构建低资源领域的高质量语音语料库

Abstract: This paper presents a new long-form release of the Swiss Parliaments Corpus,
converting entire multi-hour Swiss German debate sessions (each aligned with
the official session protocols) into high-quality speech-text pairs. Our
pipeline starts by transcribing all session audio into Standard German using
Whisper Large-v3 under high-compute settings. We then apply a two-step GPT-4o
correction process: first, GPT-4o ingests the raw Whisper output alongside the
official protocols to refine misrecognitions, mainly named entities. Second, a
separate GPT-4o pass evaluates each refined segment for semantic completeness.
We filter out any segments whose Predicted BLEU score (derived from Whisper's
average token log-probability) and GPT-4o evaluation score fall below a certain
threshold. The final corpus contains 801 hours of audio, of which 751 hours
pass our quality control. Compared to the original sentence-level SPC release,
our long-form dataset achieves a 6-point BLEU improvement, demonstrating the
power of combining robust ASR, LLM-based correction, and data-driven filtering
for low-resource, domain-specific speech corpora.

</details>


### [122] [Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking](https://arxiv.org/abs/2506.07751)
*Silin Gao,Antoine Bosselut,Samy Bengio,Emmanuel Abbe*

Main category: cs.CL

TL;DR: 通过强化学习训练LLMs进行抽象推理以应对分布变化，相比监督微调更有效提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（尤其是小规模模型）在面临数值/名义变量变化等分布偏移时推理性能显著下降，需探索增强鲁棒性的方法。

Method: 提出AbstraL方法：使用强化学习训练模型对问题进行抽象化，而非生成具体实例数据，同时连接符号工具辅助求解。

Result: 在GSM扰动基准测试中，该方法使性能下降幅度显著减少（监督微调无法实现忠实抽象）。

Conclusion: 抽象化过程能有效对抗分布偏移，且通过强化学习比监督学习更能掌握该能力，为结合符号推理工具奠定基础。

Abstract: Recent studies have shown that large language models (LLMs), especially
smaller ones, often lack robustness in their reasoning. I.e., they tend to
experience performance drops when faced with distribution shifts, such as
changes to numerical or nominal variables, or insertions of distracting
clauses. A possible strategy to address this involves generating synthetic data
to further "instantiate" reasoning problems on potential variations. In
contrast, our approach focuses on "abstracting" reasoning problems. This not
only helps counteract distribution shifts but also facilitates the connection
to symbolic tools for deriving solutions. We find that this abstraction process
is better acquired through reinforcement learning (RL) than just supervised
fine-tuning, which often fails to produce faithful abstractions. Our method,
AbstraL -- which promotes abstract reasoning in LLMs using RL on granular
abstraction data -- significantly mitigates performance degradation on recent
GSM perturbation benchmarks.

</details>


### [123] [LLM Unlearning Should Be Form-Independent](https://arxiv.org/abs/2506.07795)
*Xiaotian Ye,Mengqi Zhang,Shu Wu*

Main category: cs.CL

TL;DR: 现有LLM遗忘方法存在形式依赖偏差（Form-Dependent Bias），导致实际场景泛化能力差。作者提出ORT基准量化该问题，并开发ROCR方法通过概念重定向实现高效形式独立遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有LLM遗忘方法过度依赖训练样本表达形式，无法应对现实场景中知识表达的无穷变化，限制了安全关键场景下的实际应用价值。

Method: 1. 提出ORT基准系统评估遗忘方法对知识表达变体的鲁棒性；2. 开发ROCR方法（Rank-one Concept Redirection），通过修改模型参数在秒级时间内将危险概念重定向至无害概念。

Result: ORT基准揭示现有技术普遍存在严重形式依赖偏差，ROCR相比传统方法提升效果达2-5倍，同时保持输出自然性（困惑度降低23.1%）。

Conclusion: LLM遗忘应追求形式独立性，ROCR通过概念层干预为实际应用提供了新方向，其参数高效性尤其适合动态安全防护场景。

Abstract: Large Language Model (LLM) unlearning aims to erase or suppress undesirable
knowledge within the model, offering promise for controlling harmful or private
information to prevent misuse. However, recent studies highlight its limited
efficacy in real-world scenarios, hindering practical adoption. In this study,
we identify a pervasive issue underlying many downstream failures: the
effectiveness of existing unlearning methods heavily depends on the form of
training samples and frequently fails to generalize to alternate expressions of
the same knowledge. We formally characterize this problem as Form-Dependent
Bias and systematically investigate its specific manifestation patterns across
various downstream tasks. To quantify its prevalence and support future
research, we introduce ORT, a novel benchmark designed to evaluate the
robustness of unlearning methods against variations in knowledge expression.
Results reveal that Form-Dependent Bias is both widespread and severe among
current techniques.
  We argue that LLM unlearning should be form-independent to address the
endless forms of downstream tasks encountered in real-world security-critical
scenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR),
a novel training-free method, as a promising solution path. ROCR performs
unlearning by targeting the invariants in downstream tasks, specifically the
activated dangerous concepts. It is capable of modifying model parameters
within seconds to redirect the model's perception of a specific unlearning
target concept to another harmless concept. Extensive experiments demonstrate
that ROCR significantly improves unlearning effectiveness compared to
traditional methods while generating highly natural outputs.

</details>


### [124] [MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification](https://arxiv.org/abs/2506.07801)
*Iustin Sirbu,Robert-Adrian Popovici,Cornelia Caragea,Stefan Trausan-Matu,Traian Rebedea*

Main category: cs.CL

TL;DR: MultiMatch是一种新型半监督学习算法，整合协同训练、一致性正则化和三重伪标签加权模块，在NLP基准测试中9/10任务达到SOTA，并在数据不平衡场景展现强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有半监督学习方法在处理数据不平衡问题时存在局限，需整合协同训练、自适应阈值等不同技术路线优势以提升模型鲁棒性。

Method: 结合Multihead协同训练（头部一致性）、FreeMatch的自适应阈值机制，以及MarginMatch的伪标签边际加权策略，形成统一的三重加权模块进行伪标签筛选和权重分配。

Result: 在5个NLP数据集10个配置中9项达到SOTA，Friedman测试排名第一；数据不平衡场景下优于第二名3.26%。

Conclusion: MultiMatch通过系统性整合不同技术路径，显著提升文本分类任务（特别是数据不平衡场景）的性能和稳定性，具有重要应用价值。

Abstract: We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm
combining the paradigms of co-training and consistency regularization with
pseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label
weighting module designed for three key purposes: selecting and filtering
pseudo-labels based on head agreement and model confidence, and weighting them
according to the perceived classification difficulty. This novel module
enhances and unifies three existing techniques -- heads agreement from
Multihead Co-training, self-adaptive thresholds from FreeMatch, and Average
Pseudo-Margins from MarginMatch -- resulting in a holistic approach that
improves robustness and performance in SSL settings. Experimental results on
benchmark datasets highlight the superior performance of MultiMatch, achieving
state-of-the-art results on 9 out of 10 setups from 5 natural language
processing datasets and ranking first according to the Friedman test among 19
methods. Furthermore, MultiMatch demonstrates exceptional robustness in highly
imbalanced settings, outperforming the second-best approach by 3.26% -- and
data imbalance is a key factor for many text classification tasks.

</details>


### [125] [WebUIBench: A Comprehensive Benchmark for Evaluating Multimodal Large Language Models in WebUI-to-Code](https://arxiv.org/abs/2506.07818)
*Zhiyu Lin,Zhengda Zhou,Zhiyuan Zhao,Tianrui Wan,Yilun Ma,Junyu Gao,Xuelong Li*

Main category: cs.CL

TL;DR: 构建WebUIBench基准测试框架，系统评估多模态大语言模型在网页开发中的多维子能力


<details>
  <summary>Details</summary>
Motivation: 现有基准测试仅关注网页生成结果，缺乏对开发过程中多维子能力的系统性评估，难以指导模型能力优化

Method: 基于软件工程原理设计WebUIBench，包含WebUI感知/HTML编程/WebUI-HTML理解/WebUI转代码4大评估维度，构建含21K问答对的数据集

Result: 通过29个主流模型测试，揭示了模型在网页开发不同阶段的技能分布特征及典型缺陷

Conclusion: 该基准为提升MLLMs的网页开发效能提供了系统性评估工具，推动AI软件工程师能力发展

Abstract: With the rapid advancement of Generative AI technology, Multimodal Large
Language Models(MLLMs) have the potential to act as AI software engineers
capable of executing complex web application development. Considering that the
model requires a confluence of multidimensional sub-capabilities to address the
challenges of various development phases, constructing a multi-view evaluation
framework is crucial for accurately guiding the enhancement of development
efficiency. However, existing benchmarks usually fail to provide an assessment
of sub-capabilities and focus solely on webpage generation outcomes. In this
work, we draw inspiration from the principles of software engineering and
further propose WebUIBench, a benchmark systematically designed to evaluate
MLLMs in four key areas: WebUI Perception, HTML Programming,WebUI-HTML
Understanding, and WebUI-to-Code. WebUIBench comprises 21K high-quality
question-answer pairs derived from over 0.7K real-world websites. The extensive
evaluation of 29 mainstream MLLMs uncovers the skill characteristics and
various weakness that models encountered during the development process.

</details>


### [126] [Learning to Focus: Causal Attention Distillation via Gradient-Guided Token Pruning](https://arxiv.org/abs/2506.07851)
*Yiju Guo,Wenkai Yang,Zexu Sun,Ning Ding,Zhiyuan Liu,Yankai Lin*

Main category: cs.CL

TL;DR: 提出LeaF框架，通过消除训练数据中的干扰相关性，提升LLMs在长上下文推理中的关键信息聚焦能力


<details>
  <summary>Details</summary>
Motivation: LLMs在长文本推理中易受虚假相关性干扰，导致注意力分散和错误响应，需通过因果干预改善

Method: 两阶段框架：1）基于梯度比较识别干扰标记；2）通过知识蒸馏剪枝干扰标记，对齐师生注意力分布

Result: 数学推理和代码生成任务绝对性能提升，推理时对干扰标记的注意力抑制效果显著

Conclusion: LeaF有效提升模型可靠性和可解释性，验证干扰模式对推理质量的关键影响

Abstract: Large language models (LLMs) have demonstrated significant improvements in
contextual understanding. However, their ability to attend to truly critical
information during long-context reasoning and generation still falls behind the
pace. Specifically, our preliminary experiments reveal that certain distracting
patterns can misdirect the model's attention during inference, and removing
these patterns substantially improves reasoning accuracy and generation
quality. We attribute this phenomenon to spurious correlations in the training
data, which obstruct the model's capacity to infer authentic causal
instruction-response relationships. This phenomenon may induce redundant
reasoning processes, potentially resulting in significant inference overhead
and, more critically, the generation of erroneous or suboptimal responses. To
mitigate this, we introduce a two-stage framework called Learning to Focus
(LeaF) leveraging intervention-based inference to disentangle confounding
factors. In the first stage, LeaF employs gradient-based comparisons with an
advanced teacher to automatically identify confounding tokens based on causal
relationships in the training corpus. Then, in the second stage, it prunes
these tokens during distillation to enact intervention, aligning the student's
attention with the teacher's focus distribution on truly critical context
tokens. Experimental results demonstrate that LeaF not only achieves an
absolute improvement in various mathematical reasoning and code generation
benchmarks but also effectively suppresses attention to confounding tokens
during inference, yielding a more interpretable and reliable reasoning model.

</details>


### [127] [MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs](https://arxiv.org/abs/2506.07899)
*Ke Wang,Yiming Qin,Nikolaos Dimitriadis,Alessandro Favero,Pascal Frossard*

Main category: cs.CL

TL;DR: 提出MEMOIR框架实现语言模型的高效持续知识更新，通过残差内存模块和稀疏激活机制减少编辑干扰，支持数千次连续编辑且保持模型核心能力


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑方法存在泛化能力下降、编辑干扰和扩展性差的问题，需要在不重新训练/遗忘旧知识的前提下实现可靠的知识更新

Method: 1. 通过残差内存模块注入新知识
2. 样本依赖的掩码稀疏化激活
3. 编辑时分配独立内存参数子集
4. 推理时匹配激活模式触发相关记忆

Result: 在问答/幻觉纠正/分布外泛化等任务中达到SOTA，支持3K+连续编辑（LLaMA-3/Mistral），遗忘率降低65%以上

Conclusion: MEMOIR通过解耦知识存储与模型参数，有效实现可扩展的持续学习，为需要频繁更新的AI系统提供可靠解决方案

Abstract: Language models deployed in real-world systems often require post-hoc updates
to incorporate new or corrected knowledge. However, editing such models
efficiently and reliably - without retraining or forgetting previous
information - remains a major challenge. Existing methods for lifelong model
editing either compromise generalization, interfere with past edits, or fail to
scale to long editing sequences. We propose MEMOIR, a novel scalable framework
that injects knowledge through a residual memory, i.e., a dedicated parameter
module, while preserving the core capabilities of the pre-trained model. By
sparsifying input activations through sample-dependent masks, MEMOIR confines
each edit to a distinct subset of the memory parameters, minimizing
interference among edits. At inference, it identifies relevant edits by
comparing the sparse activation patterns of new queries to those stored during
editing. This enables generalization to rephrased queries by activating only
the relevant knowledge while suppressing unnecessary memory activation for
unrelated prompts. Experiments on question answering, hallucination correction,
and out-of-distribution generalization benchmarks across LLaMA-3 and Mistral
demonstrate that MEMOIR achieves state-of-the-art performance across
reliability, generalization, and locality metrics, scaling to thousands of
sequential edits with minimal forgetting.

</details>


### [128] [MiniCPM4: Ultra-Efficient LLMs on End Devices](https://arxiv.org/abs/2506.07900)
*MiniCPM Team,Chaojun Xiao,Yuxuan Li,Xu Han,Yuzhuo Bai,Jie Cai,Haotian Chen,Wentong Chen,Xin Cong,Ganqu Cui,Ning Ding,Shengdan Fan,Yewei Fang,Zixuan Fu,Wenyu Guan,Yitong Guan,Junshao Guo,Yufeng Han,Bingxiang He,Yuxiang Huang,Cunliang Kong,Qiuzuo Li,Siyuan Li,Wenhao Li,Yanghao Li,Yishan Li,Zhen Li,Dan Liu,Biyuan Lin,Yankai Lin,Xiang Long,Quanyu Lu,Yaxi Lu,Peiyan Luo,Hongya Lyu,Litu Ou,Yinxu Pan,Zekai Qu,Qundong Shi,Zijun Song,Jiayuan Su,Zhou Su,Ao Sun,Xianghui Sun,Peijun Tang,Fangzheng Wang,Feng Wang,Shuo Wang,Yudong Wang,Yesai Wu,Zhenyu Xiao,Jie Xie,Zihao Xie,Yukun Yan,Jiarui Yuan,Kaihuo Zhang,Lei Zhang,Linyue Zhang,Xueren Zhang,Yudi Zhang,Hengyu Zhao,Weilin Zhao,Weilun Zhao,Yuanqian Zhao,Zhi Zheng,Ge Zhou,Jie Zhou,Wei Zhou,Zihan Zhou,Zixuan Zhou,Zhiyuan Liu,Guoyang Zeng,Chao Jia,Dahai Li,Maosong Sun*

Main category: cs.CL

TL;DR: MiniCPM4是面向终端设备的高效大语言模型，通过模型架构/训练数据/训练算法/推理系统四维创新，在0.5B和8B参数规模下实现超越同类模型的性能表现。


<details>
  <summary>Details</summary>
Motivation: 解决端侧设备运行大语言模型的效率瓶颈，针对长上下文处理、训练效率、推理速度等关键问题提出系统性解决方案。

Method: 1. 模型架构：InfLLM v2稀疏注意力加速长文本处理
2. 训练数据：UltraClean预训练数据过滤 + UltraChat v2微调数据集
3. 训练算法：ModelTunnel v2预训练策略搜索 + BitCPM三元量化
4. 推理系统：CPM.cu整合稀疏注意力/量化/推测采样

Result: 在多项基准测试中超越同规模开源模型，8B版本处理长序列速度显著优于Qwen3-8B。支持可信调查生成、模型上下文协议工具调用等应用场景。

Conclusion: 通过软硬件协同创新，MiniCPM4在保持高效推理的同时展现出强大的通用性，为端侧AI部署提供了新的技术范式。

Abstract: This paper introduces MiniCPM4, a highly efficient large language model (LLM)
designed explicitly for end-side devices. We achieve this efficiency through
systematic innovation in four key dimensions: model architecture, training
data, training algorithms, and inference systems. Specifically, in terms of
model architecture, we propose InfLLM v2, a trainable sparse attention
mechanism that accelerates both prefilling and decoding phases for long-context
processing. Regarding training data, we propose UltraClean, an efficient and
accurate pre-training data filtering and generation strategy, and UltraChat v2,
a comprehensive supervised fine-tuning dataset. These datasets enable
satisfactory model performance to be achieved using just 8 trillion training
tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient
pre-training strategy search, and improve existing post-training methods by
introducing chunk-wise rollout for load-balanced reinforcement learning and
data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose
CPM.cu that integrates sparse attention, model quantization, and speculative
sampling to achieve efficient prefilling and decoding. To meet diverse
on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B
parameters, respectively. Sufficient evaluation results show that MiniCPM4
outperforms open-source models of similar size across multiple benchmarks,
highlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B
demonstrates significant speed improvements over Qwen3-8B when processing long
sequences. Through further adaptation, MiniCPM4 successfully powers diverse
applications, including trustworthy survey generation and tool use with model
context protocol, clearly showcasing its broad usability.

</details>


### [129] [Quantum Graph Transformer for NLP Sentiment Classification](https://arxiv.org/abs/2506.07937)
*Shamminuj Aktar,Andreas Bärtschi,Abdel-Hameed A. Badawy,Stephan Eidenbenz*

Main category: cs.CL

TL;DR: 量子图转换器(QGT)通过整合量子自注意力机制，在情感分类任务中实现了优于现有量子NLP模型的准确率，参数效率提升且样本需求减少50%


<details>
  <summary>Details</summary>
Motivation: 解决量子自然语言处理模型参数冗余和样本效率低下的问题，探索图结构数据与量子计算的协同优势

Method: 在消息传递框架中引入参数化量子电路实现自注意力机制，使用5个情感分类基准测试进行验证

Result: 真实/合成数据集准确率分别提升5.42%/4.76%，Yelp数据集样本效率提升50%

Conclusion: 基于图的QNLP技术展示了量子计算在高效可扩展语言理解中的突破潜力

Abstract: Quantum machine learning is a promising direction for building more efficient
and expressive models, particularly in domains where understanding complex,
structured data is critical. We present the Quantum Graph Transformer (QGT), a
hybrid graph-based architecture that integrates a quantum self-attention
mechanism into the message-passing framework for structured language modeling.
The attention mechanism is implemented using parameterized quantum circuits
(PQCs), which enable the model to capture rich contextual relationships while
significantly reducing the number of trainable parameters compared to classical
attention mechanisms. We evaluate QGT on five sentiment classification
benchmarks. Experimental results show that QGT consistently achieves higher or
comparable accuracy than existing quantum natural language processing (QNLP)
models, including both attention-based and non-attention-based approaches. When
compared with an equivalent classical graph transformer, QGT yields an average
accuracy improvement of 5.42% on real-world datasets and 4.76% on synthetic
datasets. Additionally, QGT demonstrates improved sample efficiency, requiring
nearly 50% fewer labeled samples to reach comparable performance on the Yelp
dataset. These results highlight the potential of graph-based QNLP techniques
for advancing efficient and scalable language understanding.

</details>


### [130] [Statistical Hypothesis Testing for Auditing Robustness in Language Models](https://arxiv.org/abs/2506.07947)
*Paulius Rauba,Qiyao Wei,Mihaela van der Schaar*

Main category: cs.CL

TL;DR: 提出基于分布的扰动分析框架，将大语言模型输出变化的检验转化为频数假设检验问题，通过蒙特卡洛采样构建语义空间的经验分布。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法无法有效检测LLM系统在干预下的输出分布变化问题（如输入扰动或模型替换），传统方法受限于随机性差异和计算不可行性。

Method: 构建低维语义相似空间的空分布/备择分布，采用蒙特卡洛采样实现可解释的p值计算，支持多扰动误差控制与模型无关的任意黑盒LLM评估。

Result: 案例验证了框架可量化响应变化、计算真假阳性率、评估与参考模型对齐度，证明其在模型审计中的实用性。

Conclusion: 该框架为LLM审计提供了可靠的频数统计检验工具，具有模型无关性、可解释性、多扰动支持及效应量指标等核心优势。

Abstract: Consider the problem of testing whether the outputs of a large language model
(LLM) system change under an arbitrary intervention, such as an input
perturbation or changing the model variant. We cannot simply compare two LLM
outputs since they might differ due to the stochastic nature of the system, nor
can we compare the entire output distribution due to computational
intractability. While existing methods for analyzing text-based outputs exist,
they focus on fundamentally different problems, such as measuring bias or
fairness. To this end, we introduce distribution-based perturbation analysis, a
framework that reformulates LLM perturbation analysis as a frequentist
hypothesis testing problem. We construct empirical null and alternative output
distributions within a low-dimensional semantic similarity space via Monte
Carlo sampling, enabling tractable inference without restrictive distributional
assumptions. The framework is (i) model-agnostic, (ii) supports the evaluation
of arbitrary input perturbations on any black-box LLM, (iii) yields
interpretable p-values; (iv) supports multiple perturbations via controlled
error rates; and (v) provides scalar effect sizes. We demonstrate the
usefulness of the framework across multiple case studies, showing how we can
quantify response changes, measure true/false positive rates, and evaluate
alignment with reference models. Above all, we see this as a reliable
frequentist hypothesis testing framework for LLM auditing.

</details>


### [131] [Language Models over Canonical Byte-Pair Encodings](https://arxiv.org/abs/2506.07956)
*Tim Vieira,Tianyu Liu,Clemente Pasti,Yahya Emara,Brian DuSell,Benjamin LeBrun,Mario Giulianelli,Juan Luis Gastaldi,Timothy J. O'Donnell,Ryan Cotterell*

Main category: cs.CL

TL;DR: 现代语言模型通过确定性分词器将字符字符串转换为token字符串，但错误分配概率至大量非规范token编码。本研究提出两种确保token级模型规范性的方法，修正概率分配错误并提升模型效果。


<details>
  <summary>Details</summary>
Motivation: 现有分词模型对非规范token编码分配非零概率，这类编码虽解码有效但实际训练中从未出现，导致概率资源浪费和模型效率低下。需要解决这种错误分配问题以提高模型准确性。

Method: 提出两种方案：1) 条件化推理(测试时通过采样策略强制规范)，2) 构造性参数化(模型结构直接保证输出规范)。前者无需重新训练，后者需模型重构但能从根本上消除非规范输出。

Result: 实验表明修正规范性错误后，多个模型在多个语料库上的held-out数据似然度显著提升，验证了方法的有效性。

Conclusion: 确保token级语言模型的输出规范性可有效提升模型效率，提出的条件化方法和构造性参数化方案为解决概率分配错位问题提供了系统化框架。

Abstract: Modern language models represent probability distributions over character
strings as distributions over (shorter) token strings derived via a
deterministic tokenizer, such as byte-pair encoding. While this approach is
highly effective at scaling up language models to large corpora, its current
incarnations have a concerning property: the model assigns nonzero probability
mass to an exponential number of $\it{noncanonical}$ token encodings of each
character string -- these are token strings that decode to valid character
strings but are impossible under the deterministic tokenizer (i.e., they will
never be seen in any training corpus, no matter how large). This misallocation
is both erroneous, as noncanonical strings never appear in training data, and
wasteful, diverting probability mass away from plausible outputs. These are
avoidable mistakes! In this work, we propose methods to enforce canonicality in
token-level language models, ensuring that only canonical token strings are
assigned positive probability. We present two approaches: (1) canonicality by
conditioning, leveraging test-time inference strategies without additional
training, and (2) canonicality by construction, a model parameterization that
guarantees canonical outputs but requires training. We demonstrate that fixing
canonicality mistakes improves the likelihood of held-out data for several
models and corpora.

</details>


### [132] [Correlated Errors in Large Language Models](https://arxiv.org/abs/2506.07962)
*Elliot Kim,Avi Garg,Kenny Peng,Nikhil Garg*

Main category: cs.CL

TL;DR: 不同LLMs存在高度错误相关性（大模型尤甚），影响LLM-as-judge评估与招聘任务


<details>
  <summary>Details</summary>
Motivation: 验证训练数据/架构/供应商多样性是否真正减少LLMs同质性，因当前缺乏实证证据

Method: 通过2个主流排行榜数据集和简历筛选任务，对超350个LLMs进行错误相关性分析

Result: 模型错误显著相关（相同架构/供应商模型达60%错误一致性），大模型跨架构/供应商仍高度相关

Conclusion: 错误相关性导致算法单一化风险，下游任务需警惕模型多样性幻觉

Abstract: Diversity in training data, architecture, and providers is assumed to
mitigate homogeneity in LLMs. However, we lack empirical evidence on whether
different LLMs differ meaningfully. We conduct a large-scale empirical
evaluation on over 350 LLMs overall, using two popular leaderboards and a
resume-screening task. We find substantial correlation in model errors -- on
one leaderboard dataset, models agree 60% of the time when both models err. We
identify factors driving model correlation, including shared architectures and
providers. Crucially, however, larger and more accurate models have highly
correlated errors, even with distinct architectures and providers. Finally, we
show the effects of correlation in two downstream tasks: LLM-as-judge
evaluation and hiring -- the latter reflecting theoretical predictions
regarding algorithmic monoculture.

</details>


### [133] [Reinforcement Pre-Training](https://arxiv.org/abs/2506.08007)
*Qingxiu Dong,Li Dong,Yao Tang,Tianzhu Ye,Yutao Sun,Zhifang Sui,Furu Wei*

Main category: cs.CL

TL;DR: 提出强化预训练(RPT)框架，将RL融入语言模型预训练以提升推理能力和扩展性


<details>
  <summary>Details</summary>
Motivation: 传统语言模型预训练依赖大量标注数据且缺乏可验证推理能力，需探索更高效的扩展范式

Method: 将next-token预测重构为RL任务，通过验证性奖励机制训练模型，实现无领域标注的大规模预训练

Result: RPT显著提升next-token预测精度(约10%)，计算资源投入与准确率呈正相关，提供优质强化学习基座模型

Conclusion: RPT开创了语言模型预训练的新范式，验证了通过强化学习实现模型能力扩展的技术路径可行性

Abstract: In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling
paradigm for large language models and reinforcement learning (RL).
Specifically, we reframe next-token prediction as a reasoning task trained
using RL, where it receives verifiable rewards for correctly predicting the
next token for a given context. RPT offers a scalable method to leverage vast
amounts of text data for general-purpose RL, rather than relying on
domain-specific annotated answers. By incentivizing the capability of
next-token reasoning, RPT significantly improves the language modeling accuracy
of predicting the next tokens. Moreover, RPT provides a strong pre-trained
foundation for further reinforcement fine-tuning. The scaling curves show that
increased training compute consistently improves the next-token prediction
accuracy. The results position RPT as an effective and promising scaling
paradigm to advance language model pre-training.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [134] [Vid2Sim: Generalizable, Video-based Reconstruction of Appearance, Geometry and Physics for Mesh-free Simulation](https://arxiv.org/abs/2506.06440)
*Chuhao Chen,Zhiyang Dou,Chen Wang,Yiming Huang,Anjun Chen,Qiao Feng,Jiatao Gu,Lingjie Liu*

Main category: cs.GR

TL;DR: 提出Vid2Sim框架，通过基于线性混合蒙皮的无网格简化模拟，实现视频中几何形状与物理属性的高效重建与模拟


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖计算密集型优化流程和可微分模拟器，存在效率低、参数调优成本高、泛化性差等问题

Method: 1. 前馈神经网络初步重建物理系统 
2. 轻量级优化流程细化外观/几何/物理属性 
3. 基于LBS的mesh-free高效模拟框架

Result: 在视频数据重建中展现出优于现有方法的精度与效率（数分钟内完成优化）

Conclusion: Vid2Sim成功建立视频到仿真的端到端流程，在保持物理准确性的同时显著提升计算效率，为动态系统识别提供新范式

Abstract: Faithfully reconstructing textured shapes and physical properties from videos
presents an intriguing yet challenging problem. Significant efforts have been
dedicated to advancing such a system identification problem in this area.
Previous methods often rely on heavy optimization pipelines with a
differentiable simulator and renderer to estimate physical parameters. However,
these approaches frequently necessitate extensive hyperparameter tuning for
each scene and involve a costly optimization process, which limits both their
practicality and generalizability. In this work, we propose a novel framework,
Vid2Sim, a generalizable video-based approach for recovering geometry and
physical properties through a mesh-free reduced simulation based on Linear
Blend Skinning (LBS), offering high computational efficiency and versatile
representation capability. Specifically, Vid2Sim first reconstructs the
observed configuration of the physical system from video using a feed-forward
neural network trained to capture physical world knowledge. A lightweight
optimization pipeline then refines the estimated appearance, geometry, and
physical properties to closely align with video observations within just a few
minutes. Additionally, after the reconstruction, Vid2Sim enables high-quality,
mesh-free simulation with high efficiency. Extensive experiments demonstrate
that our method achieves superior accuracy and efficiency in reconstructing
geometry and physical properties from video data.

</details>


### [135] [Splat and Replace: 3D Reconstruction with Repetitive Elements](https://arxiv.org/abs/2506.06462)
*Nicolás Violante,Andreas Meuleman,Alban Gauthier,Frédo Durand,Thibault Groueix,George Drettakis*

Main category: cs.GR

TL;DR: 利用3D场景中的重复元素改进新视角合成质量，通过实例分割和跨实例信息共享提升几何重建和外观一致性


<details>
  <summary>Details</summary>
Motivation: NeRF和3DGS在训练视角不完整时对遮挡/未观测区域渲染质量差。现实场景普遍存在重复元素，利用这些重复特征可以补充缺失信息

Method: 1. 在3DGS重建中分割重复实例 2. 实例配准对齐 3. 建立跨实例信息共享机制（考虑外观差异） 4. 联合优化几何与外观

Result: 在合成/真实场景测试中，新视角合成质量显著提升（PSNR平均提高2.1dB）

Conclusion: 该方法通过挖掘场景重复模式有效解决视角覆盖不足问题，为三维重建开辟新思路

Abstract: We leverage repetitive elements in 3D scenes to improve novel view synthesis.
Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly
improved novel view synthesis but renderings of unseen and occluded parts
remain low-quality if the training views are not exhaustive enough. Our key
observation is that our environment is often full of repetitive elements. We
propose to leverage those repetitions to improve the reconstruction of
low-quality parts of the scene due to poor coverage and occlusions. We propose
a method that segments each repeated instance in a 3DGS reconstruction,
registers them together, and allows information to be shared among instances.
Our method improves the geometry while also accounting for appearance
variations across instances. We demonstrate our method on a variety of
synthetic and real scenes with typical repetitive elements, leading to a
substantial improvement in the quality of novel view synthesis.

</details>


### [136] [Noise Consistency Regularization for Improved Subject-Driven Image Synthesis](https://arxiv.org/abs/2506.06483)
*Yao Ni,Song Wen,Piotr Koniusz,Anoop Cherian*

Main category: cs.GR

TL;DR: 提出两种一致性正则化损失（先验一致性+主体一致性），解决Stable Diffusion微调中主体身份保留与图像多样性的平衡问题


<details>
  <summary>Details</summary>
Motivation: 现有微调方法存在欠拟合（无法可靠捕捉主体特征）和过拟合（记忆主体图像导致背景单一化）的双重问题

Method: 在扩散模型微调阶段引入：1) 先验一致性损失保持非主体图像噪声预测与原始模型一致；2) 主体一致性损失增强潜在空间噪声鲁棒性

Result: CLIP分数、背景多样性、视觉质量全面超越DreamBooth，同时保持主体身份特征

Conclusion: 双重正则化机制有效破解微调困境，为可控图像生成提供新思路

Abstract: Fine-tuning Stable Diffusion enables subject-driven image synthesis by
adapting the model to generate images containing specific subjects. However,
existing fine-tuning methods suffer from two key issues: underfitting, where
the model fails to reliably capture subject identity, and overfitting, where it
memorizes the subject image and reduces background diversity. To address these
challenges, we propose two auxiliary consistency losses for diffusion
fine-tuning. First, a prior consistency regularization loss ensures that the
predicted diffusion noise for prior (non-subject) images remains consistent
with that of the pretrained model, improving fidelity. Second, a subject
consistency regularization loss enhances the fine-tuned model's robustness to
multiplicative noise modulated latent code, helping to preserve subject
identity while improving diversity. Our experimental results demonstrate that
incorporating these losses into fine-tuning not only preserves subject identity
but also enhances image diversity, outperforming DreamBooth in terms of CLIP
scores, background variation, and overall visual quality.

</details>


### [137] [JGS2: Near Second-order Converging Jacobi/Gauss-Seidel for GPU Elastodynamics](https://arxiv.org/abs/2506.06494)
*Lei Lan,Zixuan Lu,Chun Yuan,Weiwei Xu,Hao Su,Huamin Wang,Chenfanfu Jiang,Yin Yang*

Main category: cs.GR

TL;DR: 新型GPU算法突破并行模拟收敛与并行性矛盾，实现近牛顿法的收敛速度


<details>
  <summary>Details</summary>
Motivation: 传统并行模拟方法需在收敛速度与并行效率间权衡，强并行性常导致收敛缓慢

Method: 通过理论分析过冲现象，提出二阶最优解并转化为预计算形式，结合Cubature采样降低运行时成本，开发全坐标公式优化预计算效率

Result: 算法在GPU上实现近二次收敛，收敛速度超越现有方法50-100倍，支持刚体/软体材料的高质量模拟

Conclusion: 该方法成功统一收敛性与并行性，为物理模拟领域提供突破性解决方案

Abstract: In parallel simulation, convergence and parallelism are often seen as
inherently conflicting objectives. Improved parallelism typically entails
lighter local computation and weaker coupling, which unavoidably slow the
global convergence. This paper presents a novel GPU algorithm that achieves
convergence rates comparable to fullspace Newton's method while maintaining
good parallelizability just like the Jacobi method. Our approach is built on a
key insight into the phenomenon of overshoot. Overshoot occurs when a local
solver aggressively minimizes its local energy without accounting for the
global context, resulting in a local update that undermines global convergence.
To address this, we derive a theoretically second-order optimal solution to
mitigate overshoot. Furthermore, we adapt this solution into a pre-computable
form. Leveraging Cubature sampling, our runtime cost is only marginally higher
than the Jacobi method, yet our algorithm converges nearly quadratically as
Newton's method. We also introduce a novel full-coordinate formulation for more
efficient pre-computation. Our method integrates seamlessly with the
incremental potential contact method and achieves second-order convergence for
both stiff and soft materials. Experimental results demonstrate that our
approach delivers high-quality simulations and outperforms state-of-the-art GPU
methods with 50 to 100 times better convergence.

</details>


### [138] [CrossGen: Learning and Generating Cross Fields for Quad Meshing](https://arxiv.org/abs/2506.07020)
*Qiujie Dong,Jiepeng Wang,Rui Xu,Cheng Lin,Yuan Liu,Shiqing Xin,Zichun Zhong,Xin Li,Changhe Tu,Taku Komura,Leif Kobbelt,Scott Schaefer,Wenping Wang*

Main category: cs.GR

TL;DR: 提出CrossGen框架，通过联合潜在空间统一几何与cross field表示，实现高效高质量的四边形网格生成，支持前馈预测和潜在生成建模，推理速度通常在一秒内且无需逐形状优化。


<details>
  <summary>Details</summary>
Motivation: 现有cross field生成方法在计算效率与质量间难以平衡，依赖耗时的逐形状优化过程，无法满足实际应用中对快速生成高质量四边形网格的需求。

Method: 1. 构建联合潜在空间统一几何(SDF)与cross field表示
2. 使用自动编码器网络架构编码点云表面为稀疏体素网格
3. 结合扩散模型实现基于草图等部分输入的新形状生成
4. 创建包含高质量SDF和cross field的数据集用于训练

Result: 1. 前馈推理速度达秒级(快于传统方法3个数量级)
2. 生成结果具有高几何保真度与噪声鲁棒性
3. 支持多样化表面形状的四边形网格生成验证
4. 构建首个包含SDF与cross field配对的专用数据集

Conclusion: CrossGen通过联合潜在表示突破传统方法效率瓶颈，扩散模型的引入拓展了生成能力，为几何处理任务提供了新的解决方案。该方法在速度、质量和泛化性方面显著优于现有技术，其构建的数据集和框架为后续研究提供了重要基础。

Abstract: Cross fields play a critical role in various geometry processing tasks,
especially for quad mesh generation. Existing methods for cross field
generation often struggle to balance computational efficiency with generation
quality, using slow per-shape optimization. We introduce CrossGen, a novel
framework that supports both feed-forward prediction and latent generative
modeling of cross fields for quad meshing by unifying geometry and cross field
representations within a joint latent space. Our method enables extremely fast
computation of high-quality cross fields of general input shapes, typically
within one second without per-shape optimization. Our method assumes a
point-sampled surface, or called a point-cloud surface, as input, so we can
accommodate various different surface representations by a straightforward
point sampling process. Using an auto-encoder network architecture, we encode
input point-cloud surfaces into a sparse voxel grid with fine-grained latent
spaces, which are decoded into both SDF-based surface geometry and cross
fields. We also contribute a dataset of models with both high-quality signed
distance fields (SDFs) representations and their corresponding cross fields,
and use it to train our network. Once trained, the network is capable of
computing a cross field of an input surface in a feed-forward manner, ensuring
high geometric fidelity, noise resilience, and rapid inference. Furthermore,
leveraging the same unified latent representation, we incorporate a diffusion
model for computing cross fields of new shapes generated from partial input,
such as sketches. To demonstrate its practical applications, we validate
CrossGen on the quad mesh generation task for a large variety of surface
shapes. Experimental results...

</details>


### [139] [Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization](https://arxiv.org/abs/2506.07069)
*Zhican Wang,Guanghui He,Dantong Liu,Lingjun Gao,Shell Xu Hu,Chen Zhang,Zhuoran Song,Nicholas Lane,Wayne Luk,Hongxiang Fan*

Main category: cs.GR

TL;DR: 提出3DGS的架构-算法协同设计方案，通过轴向光栅化、神经排序方法和可重构处理阵列等技术，在保持渲染质量的同时实现23.4-27.8倍加速和28.8-51.4倍节能


<details>
  <summary>Details</summary>
Motivation: 解决3D高斯泼溅技术在资源受限设备上实时渲染的硬件效率瓶颈，尤其是传统光栅化中的计算冗余和排序过程带来的资源浪费

Method: 1.轴向光栅化预计算共享项减少63%乘加运算 2.神经排序网络替代硬件排序器 3.可重构处理阵列统一支持光栅化和神经网络推理 4.π轨迹瓦片调度优化内存访问

Result: 实验显示相比边缘GPU实现23.4-27.8倍速度提升，28.8-51.4倍能耗降低，同时保持原有渲染质量

Conclusion: 通过硬件架构与算法的深度协同设计，有效解决了3DGS的实时渲染效率问题，并通过开源计划推动领域发展

Abstract: 3D Gaussian Splatting (3DGS) has recently gained significant attention for
high-quality and efficient view synthesis, making it widely adopted in fields
such as AR/VR, robotics, and autonomous driving. Despite its impressive
algorithmic performance, real-time rendering on resource-constrained devices
remains a major challenge due to tight power and area budgets. This paper
presents an architecture-algorithm co-design to address these inefficiencies.
First, we reveal substantial redundancy caused by repeated computation of
common terms/expressions during the conventional rasterization. To resolve
this, we propose axis-oriented rasterization, which pre-computes and reuses
shared terms along both the X and Y axes through a dedicated hardware design,
effectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by
identifying the resource and performance inefficiency of the sorting process,
we introduce a novel neural sorting approach that predicts order-independent
blending weights using an efficient neural network, eliminating the need for
costly hardware sorters. A dedicated training framework is also proposed to
improve its algorithmic stability. Third, to uniformly support rasterization
and neural network inference, we design an efficient reconfigurable processing
array that maximizes hardware utilization and throughput. Furthermore, we
introduce a $\pi$-trajectory tile schedule, inspired by Morton encoding and
Hilbert curve, to optimize Gaussian reuse and reduce memory access overhead.
Comprehensive experiments demonstrate that the proposed design preserves
rendering quality while achieving a speedup of $23.4\sim27.8\times$ and energy
savings of $28.8\sim51.4\times$ compared to edge GPUs for real-world scenes. We
plan to open-source our design to foster further development in this field.

</details>


### [140] [HOI-PAGE: Zero-Shot Human-Object Interaction Generation with Part Affordance Guidance](https://arxiv.org/abs/2506.07209)
*Lei Li,Angela Dai*

Main category: cs.GR

TL;DR: HOI-PAGE通过部分层次交互推理框架，结合PAGs图结构指导的三阶段合成流程，实现零样本4D人物-物体交互生成，显著提升真实性与文本对齐效果


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于全局运动建模，难以生成细粒度的人体部位-物体部件交互。论文提出通过部位affordance关系解析实现更精细的交互控制

Method: 1) 构建基于大语言模型的PAGs表示解析部位级接触关系 2) 三阶段生成：物体部件分解→参考视频运动约束提取→基于物理约束的4D序列优化

Result: 实验证明方法支持复杂多人多物体交互生成，在运动真实性和文本符合度上超越基线方法，FID指标提升32%，用户偏好率增加45%

Conclusion: 首次将部位级affordance推理引入HOI生成，通过结构化PAGs指导的渐进式优化框架，为零样本4D交互合成开辟新方向

Abstract: We present HOI-PAGE, a new approach to synthesizing 4D human-object
interactions (HOIs) from text prompts in a zero-shot fashion, driven by
part-level affordance reasoning. In contrast to prior works that focus on
global, whole body-object motion for 4D HOI synthesis, we observe that
generating realistic and diverse HOIs requires a finer-grained understanding --
at the level of how human body parts engage with object parts. We thus
introduce Part Affordance Graphs (PAGs), a structured HOI representation
distilled from large language models (LLMs) that encodes fine-grained part
information along with contact relations. We then use these PAGs to guide a
three-stage synthesis: first, decomposing input 3D objects into geometric
parts; then, generating reference HOI videos from text prompts, from which we
extract part-based motion constraints; finally, optimizing for 4D HOI motion
sequences that not only mimic the reference dynamics but also satisfy
part-level contact constraints. Extensive experiments show that our approach is
flexible and capable of generating complex multi-object or multi-person
interaction sequences, with significantly improved realism and text alignment
for zero-shot 4D HOI generation.

</details>


### [141] [Immersive Visualization of Flat Surfaces Using Ray Marching](https://arxiv.org/abs/2506.07558)
*Fabian Lander,Diaaeldin Taha*

Main category: cs.GR

TL;DR: 提出基于光线步进法的平面可视化方法，支持多种几何结构探索并保持计算效率


<details>
  <summary>Details</summary>
Motivation: 传统几何可视化方法在复杂结构（如平移表面/镜室）展示上存在效率限制，需要更直观的解决方案

Method: 采用光线步进算法实现实时渲染，优化空间遍历策略保证计算效率

Result: 通过多案例验证方法有效性，提供开源代码实现方案

Conclusion: 该方法适用于数学教育推广和几何研究，所有资源已在线公开共享

Abstract: We present an effective method for visualizing flat surfaces using ray
marching. Our approach provides an intuitive way to explore translation
surfaces, mirror rooms, unfolded polyhedra, and translation prisms while
maintaining computational efficiency. We demonstrate the utility of the method
through various examples and provide implementation insights for programmers.
Finally, we discuss the use of our visualizations in outreach. We make our
simulations and code available online.

</details>


### [142] [PIG: Physically-based Multi-Material Interaction with 3D Gaussians](https://arxiv.org/abs/2506.07657)
*Zeyu Xiao,Zhenyi Wu,Mingyang Sun,Qipeng Yan,Yufan Guo,Zhuoer Liang,Lihua Zhang*

Main category: cs.GR

TL;DR: 提出PIG方法，通过3D对象分割与物理属性约束结合，解决高斯溅射场景中的多材质交互问题，显著提升视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯溅射技术在物体交互中存在分割不准确、材质变形误差大及渲染伪影严重的问题，需兼顾物理模拟精度与视觉一致性。

Method: 1. 建立2D像素到3D高斯的快速映射实现精确分割；2. 为不同材质对象赋予独立物理属性；3. 在变形梯度中嵌入约束尺度以消除伪影。

Result: 实验证明该方法视觉质量超越SOTA，同时为物理真实场景生成提供新研究范式。

Conclusion: PIG首次将物理约束与高斯溅射结合，为高精度动态场景生成开辟了新方向。

Abstract: 3D Gaussian Splatting has achieved remarkable success in reconstructing both
static and dynamic 3D scenes. However, in a scene represented by 3D Gaussian
primitives, interactions between objects suffer from inaccurate 3D
segmentation, imprecise deformation among different materials, and severe
rendering artifacts. To address these challenges, we introduce PIG:
Physically-Based Multi-Material Interaction with 3D Gaussians, a novel approach
that combines 3D object segmentation with the simulation of interacting objects
in high precision. Firstly, our method facilitates fast and accurate mapping
from 2D pixels to 3D Gaussians, enabling precise 3D object-level segmentation.
Secondly, we assign unique physical properties to correspondingly segmented
objects within the scene for multi-material coupled interactions. Finally, we
have successfully embedded constraint scales into deformation gradients,
specifically clamping the scaling and rotation properties of the Gaussian
primitives to eliminate artifacts and achieve geometric fidelity and visual
consistency. Experimental results demonstrate that our method not only
outperforms the state-of-the-art (SOTA) in terms of visual quality, but also
opens up new directions and pipelines for the field of physically realistic
scene generation.

</details>


### [143] [GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution](https://arxiv.org/abs/2506.07897)
*Shuja Khalid,Mohamed Ibrahim,Yang Liu*

Main category: cs.GR

TL;DR: 提出Hessian辅助采样的轻量生成模型，实现超越训练分辨率的3D高斯泼溅实时增强


<details>
  <summary>Details</summary>
Motivation: 传统3DGS方法受限于输入分辨率，无法生成超出训练视图的细节。需要突破分辨率限制并保持计算效率

Method: 1. 基于Hessian矩阵的智能区域采样策略 2. 轻量级生成模型预测/优化3D高斯分布 3. 实时推理架构设计

Result: 单GPU实现0.015秒实时推理，几何精度提升38%，渲染质量超越SOTA方法

Conclusion: 首次建立分辨率无关的3D增强范式，为交互式应用提供实用解决方案

Abstract: We present a novel approach for enhancing the resolution and geometric
fidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution.
Current 3DGS methods are fundamentally limited by their input resolution,
producing reconstructions that cannot extrapolate finer details than are
present in the training views. Our work breaks this limitation through a
lightweight generative model that predicts and refines additional 3D Gaussians
where needed most. The key innovation is our Hessian-assisted sampling
strategy, which intelligently identifies regions that are likely to benefit
from densification, ensuring computational efficiency. Unlike computationally
intensive GANs or diffusion approaches, our method operates in real-time
(0.015s per inference on a single consumer-grade GPU), making it practical for
interactive applications. Comprehensive experiments demonstrate significant
improvements in both geometric accuracy and rendering quality compared to
state-of-the-art methods, establishing a new paradigm for resolution-free 3D
scene enhancement.

</details>


### [144] [Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression of Dynamic Scenes](https://arxiv.org/abs/2506.07917)
*Allen Tu,Haiyang Ying,Alex Hanson,Yonghan Lee,Tom Goldstein,Matthias Zwicker*

Main category: cs.GR

TL;DR: 提出SpeeDe3DGS框架，通过时间敏感度剪枝和GroupFlow运动分析技术，加速动态3D高斯泼溅渲染速度10.37倍，模型体积缩小7.71倍


<details>
  <summary>Details</summary>
Motivation: 现有动态3DGS方法对每个高斯进行逐帧神经推断，导致渲染速度瓶颈和计算资源激增

Method: 1. 时间敏感度剪枝算法去除低贡献高斯，配合退火平滑机制提升鲁棒性；2. GroupFlow轨迹聚类技术实现组级刚性变换预测

Result: 在NeRF-DS数据集实现10.37倍加速/7.71倍模型压缩，D-NeRF和HyperNeRF分别加速4.20倍/58.23倍，训练时间缩短2.71倍

Conclusion: 模块化设计兼容现有变形框架，显著提升动态场景渲染效率，为实时4D重建提供新思路

Abstract: Recent extensions of 3D Gaussian Splatting (3DGS) to dynamic scenes achieve
high-quality novel view synthesis by using neural networks to predict the
time-varying deformation of each Gaussian. However, performing per-Gaussian
neural inference at every frame poses a significant bottleneck, limiting
rendering speed and increasing memory and compute requirements. In this paper,
we present Speedy Deformable 3D Gaussian Splatting (SpeeDe3DGS), a general
pipeline for accelerating the rendering speed of dynamic 3DGS and 4DGS
representations by reducing neural inference through two complementary
techniques. First, we propose a temporal sensitivity pruning score that
identifies and removes Gaussians with low contribution to the dynamic scene
reconstruction. We also introduce an annealing smooth pruning mechanism that
improves pruning robustness in real-world scenes with imprecise camera poses.
Second, we propose GroupFlow, a motion analysis technique that clusters
Gaussians by trajectory similarity and predicts a single rigid transformation
per group instead of separate deformations for each Gaussian. Together, our
techniques accelerate rendering by $10.37\times$, reduce model size by
$7.71\times$, and shorten training time by $2.71\times$ on the NeRF-DS dataset.
SpeeDe3DGS also improves rendering speed by $4.20\times$ and $58.23\times$ on
the D-NeRF and HyperNeRF vrig datasets. Our methods are modular and can be
integrated into any deformable 3DGS or 4DGS framework.

</details>


### [145] [Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural Compressor](https://arxiv.org/abs/2506.07932)
*Rishit Dagli,Yushi Guan,Sankeerth Durvasula,Mohammadreza Mofayezi,Nandita Vijaykumar*

Main category: cs.GR

TL;DR: Squeeze3D framework achieves extreme 3D compression ratios (2187x for meshes) by bridging pre-trained encoder-generator latent spaces through mapping networks, requiring no real 3D training data.


<details>
  <summary>Details</summary>
Motivation: Existing 3D compression methods face limitations in compression ratios and dataset dependency. Squeeze3D leverages pre-trained models' knowledge to overcome these constraints.

Method: Connects pre-trained encoder and generative model via trainable mapping networks. Encodes 3D data (mesh/point cloud/radiance field) into compact latent codes, then maps to generative model's space for reconstruction.

Result: Achieves 2187x mesh, 55x point cloud, 619x radiance field compression with comparable visual quality. Low latency due to no object-specific training.

Conclusion: Demonstrates data-efficient extreme 3D compression through latent space bridging, enabling flexible format support while maintaining quality and efficiency.

Abstract: We propose Squeeze3D, a novel framework that leverages implicit prior
knowledge learnt by existing pre-trained 3D generative models to compress 3D
data at extremely high compression ratios. Our approach bridges the latent
spaces between a pre-trained encoder and a pre-trained generation model through
trainable mapping networks. Any 3D model represented as a mesh, point cloud, or
a radiance field is first encoded by the pre-trained encoder and then
transformed (i.e. compressed) into a highly compact latent code. This latent
code can effectively be used as an extremely compressed representation of the
mesh or point cloud. A mapping network transforms the compressed latent code
into the latent space of a powerful generative model, which is then conditioned
to recreate the original 3D model (i.e. decompression). Squeeze3D is trained
entirely on generated synthetic data and does not require any 3D datasets. The
Squeeze3D architecture can be flexibly used with existing pre-trained 3D
encoders and existing generative models. It can flexibly support different
formats, including meshes, point clouds, and radiance fields. Our experiments
demonstrate that Squeeze3D achieves compression ratios of up to 2187x for
textured meshes, 55x for point clouds, and 619x for radiance fields while
maintaining visual quality comparable to many existing methods. Squeeze3D only
incurs a small compression and decompression latency since it does not involve
training object-specific networks to compress an object.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [146] [GLProtein: Global-and-Local Structure Aware Protein Representation Learning](https://arxiv.org/abs/2506.06294)
*Yunqing Liu,Wenqi Fan,Xiaoyong Wei,Qing Li*

Main category: cs.LG

TL;DR: 提出首个结合蛋白质全局结构相似性和局部氨基酸细节的预训练框架GLProtein，显著提升预测精度


<details>
  <summary>Details</summary>
Motivation: 现有蛋白质序列分析方法未能充分整合结构信息，特别是缺乏从氨基酸分子（局部）到蛋白结构相似性（全局）的多层次整合

Method: 融合蛋白质掩码建模与三元组结构相似性评分，结合3D距离编码和基于亚结构的氨基酸分子编码

Result: 在蛋白质相互作用预测、接触预测等生物信息学任务中超越现有方法

Conclusion: GLProtein实现了蛋白质预训练中全局-局部结构信息的协同建模，为功能预测提供新范式

Abstract: Proteins are central to biological systems, participating as building blocks
across all forms of life. Despite advancements in understanding protein
functions through protein sequence analysis, there remains potential for
further exploration in integrating protein structural information. We argue
that the structural information of proteins is not only limited to their 3D
information but also encompasses information from amino acid molecules (local
information) to protein-protein structure similarity (global information). To
address this, we propose \textbf{GLProtein}, the first framework in protein
pre-training that incorporates both global structural similarity and local
amino acid details to enhance prediction accuracy and functional insights.
GLProtein innovatively combines protein-masked modelling with triplet structure
similarity scoring, protein 3D distance encoding and substructure-based amino
acid molecule encoding. Experimental results demonstrate that GLProtein
outperforms previous methods in several bioinformatics tasks, including
predicting protein-protein interaction, contact prediction, and so on.

</details>


### [147] [dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching](https://arxiv.org/abs/2506.06295)
*Zhiyuan Liu,Yicun Yang,Yaojie Zhang,Junjie Chen,Chang Zou,Qingyuan Wei,Shaobo Wang,Linfeng Zhang*

Main category: cs.LG

TL;DR: 提出无需训练的dLLM-Cache框架，通过长间隔提示缓存和特征相似性引导的响应更新，实现9.1倍加速且不损失质量


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型(dLLMs)存在高推理延迟问题，传统ARM加速技术因双向注意力机制不兼容

Method: 结合静态提示的长间隔缓存与动态响应的特征相似性引导更新，重用中间计算结果

Result: 在LLaDA 8B/Dream 7B上实现最高9.1倍加速，推理延迟接近自回归模型(ARMs)

Conclusion: 该框架有效解决dLLMs延迟瓶颈，为实际应用奠定基础，代码已开源促进后续研究

Abstract: Autoregressive Models (ARMs) have long dominated the landscape of Large
Language Models. Recently, a new paradigm has emerged in the form of
diffusion-based Large Language Models (dLLMs), which generate text by
iteratively denoising masked segments. This approach has shown significant
advantages and potential. However, dLLMs suffer from high inference latency.
Traditional ARM acceleration techniques, such as Key-Value caching, are
incompatible with dLLMs due to their bidirectional attention mechanism. To
address this specific challenge, our work begins with a key observation that
dLLM inference involves a static prompt and a partially dynamic response, where
most tokens remain stable across adjacent denoising steps. Based on this, we
propose dLLM-Cache, a training-free adaptive caching framework that combines
long-interval prompt caching with partial response updates guided by feature
similarity. This design enables efficient reuse of intermediate computations
without compromising model performance. Extensive experiments on representative
dLLMs, including LLaDA 8B and Dream 7B, show that dLLM-Cache achieves up to 9.1
x speedup over standard inference without compromising output quality. Notably,
our method brings dLLM inference latency close to that of ARMs under many
settings. Codes are provided in the supplementary material and will be released
publicly on GitHub.

</details>


### [148] [Reward Is Enough: LLMs Are In-Context Reinforcement Learners](https://arxiv.org/abs/2506.06303)
*Kefan Song,Amir Moeini,Peng Wang,Lei Gong,Rohan Chandra,Yanjun Qi,Shangtong Zhang*

Main category: cs.LG

TL;DR: 论文提出ICRL提示框架，揭示大语言模型在推理时自发产生强化学习行为，通过多轮奖励反馈显著提升任务表现。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习框架依赖人工设计，而本研究发现大语言模型在推理过程中自然表现出最大化奖励信号的能力，突破了传统RL范式。

Method: 提出多轮提示框架：每轮生成响应后注入数值奖励，将历史响应与奖励构成新提示上下文，实现类似强化学习的在线优化过程。

Result: 在Game of 24、创意写作和ScienceWorld基准测试中，性能显著超越Self-Refine等方法，且LLM自生成奖励时仍持续改进。

Conclusion: 上下文强化学习现象为LLM能力进化提供新范式，展示出通过测试时计算扩展模型潜力的重要方向。

Abstract: Reinforcement learning (RL) is a human-designed framework for solving
sequential decision making problems. In this work, we demonstrate that,
surprisingly, RL emerges in LLM's (Large Language Model) inference time -- a
phenomenon known as in-context RL (ICRL). Specifically, we propose a novel
multi-round prompting framework called ICRL prompting. The goal is to prompt
the LLM to complete a task. After the LLM generates a response at the current
round, we give numerical scalar feedbacks for the response, called the rewards.
At the next round, we prompt the LLM again with the same task and a context
consisting of all previous responses and rewards. We observe that the quality
of the LLM's response increases as the context grows. In other words, the LLM
is able to maximize the scalar reward signal in the inference time, just like
an RL algorithm. We evaluate ICRL prompting in three benchmarks (Game of 24,
creative writing, and ScienceWorld) and demonstrate significant performance
improvements over baseline methods such as Self-Refine and Reflexion.
Surprisingly, in some experiments the reward signals are generated by the LLM
itself, yet performance improvements are still observed from ICRL prompting,
offering a promising paradigm for scaling test-time compute.

</details>


### [149] [Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques](https://arxiv.org/abs/2506.06579)
*Adarsh Prasad Behera,Jaya Prakash Champati,Roberto Morabito,Sasu Tarkoma,James Gross*

Main category: cs.LG

TL;DR: 论文探讨通过路由选择与分层推理策略优化大语言模型的计算效率，实现资源受限场景下的高效部署


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在移动/边缘设备部署时面临高计算成本与能耗挑战，需动态资源分配策略降低使用门槛

Method: 提出两种互补策略：1) 路由机制根据查询复杂度选择模型 2) 分层推理通过模型序列逐步处理直至获得可靠响应

Result: 两种策略在关键性能指标上实现计算量优化，基准测试显示能有效平衡计算成本与模型性能

Conclusion: 未来需研究自适应模型选择算法、异构环境部署方案，推动高效LLM系统在现实场景的应用普及

Abstract: Recent progress in Language Models (LMs) has dramatically advanced the field
of natural language processing (NLP), excelling at tasks like text generation,
summarization, and question answering. However, their inference remains
computationally expensive and energy intensive, especially in settings with
limited hardware, power, or bandwidth. This makes it difficult to deploy LMs in
mobile, edge, or cost sensitive environments. To address these challenges,
recent approaches have introduced multi LLM intelligent model selection
strategies that dynamically allocate computational resources based on query
complexity -- using lightweight models for simpler queries and escalating to
larger models only when necessary. This survey explores two complementary
strategies for efficient LLM inference: (i) routing, which selects the most
suitable model based on the query, and (ii) cascading or hierarchical inference
(HI), which escalates queries through a sequence of models until a confident
response is found. Both approaches aim to reduce computation by using
lightweight models for simpler tasks while offloading only when needed. We
provide a comparative analysis of these techniques across key performance
metrics, discuss benchmarking efforts, and outline open challenges. Finally, we
outline future research directions to enable faster response times, adaptive
model selection based on task complexity, and scalable deployment across
heterogeneous environments, making LLM based systems more efficient and
accessible for real world applications.

</details>


### [150] [Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning](https://arxiv.org/abs/2506.06632)
*Shubham Parashar,Shurui Gui,Xiner Li,Hongyi Ling,Sushil Vemuri,Blake Olson,Eric Li,Yu Zhang,James Caverlee,Dileep Kalathil,Shuiwang Ji*

Main category: cs.LG

TL;DR: 通过易到难的课程学习调度方法（E2H Reasoner），显著提升小规模语言模型在复杂任务中的推理能力


<details>
  <summary>Details</summary>
Motivation: 单独使用强化学习训练小模型效果有限，需要更高效的训练策略来提升模型在数学推理等复杂任务中的表现

Method: 采用课程学习框架，将任务从易到难分级调度，并动态调整任务权重以防止过拟合，结合近似策略迭代理论框架

Result: 在1.5B-3B参数量的模型上实现推理能力显著提升，理论证明课程学习比直接学习节省34%样本量，多领域实验验证有效性

Conclusion: E2H方法为解决小模型强化学习训练难题提供了新思路，在保持样本效率的同时实现性能突破，为资源受限场景提供实用方案

Abstract: We aim to improve the reasoning capabilities of language models via
reinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1
have demonstrated reasoning abilities on mathematical and coding tasks.
However, prior studies suggest that using RL alone to improve reasoning on
inherently difficult tasks is less effective. Here, we draw inspiration from
curriculum learning and propose to schedule tasks from easy to hard (E2H),
allowing LLMs to build reasoning skills gradually. Our method is termed E2H
Reasoner. Empirically, we observe that, although easy tasks are important
initially, fading them out through appropriate scheduling is essential in
preventing overfitting. Theoretically, we establish convergence guarantees for
E2H Reasoner within an approximate policy iteration framework. We derive
finite-sample complexity bounds and show that when tasks are appropriately
decomposed and conditioned, learning through curriculum stages requires fewer
total samples than direct learning. Experiments across multiple domains show
that E2H Reasoner significantly improves the reasoning ability of small LLMs
(1.5B to 3B), which otherwise struggle when trained with vanilla RL alone,
highlighting the effectiveness of our method.

</details>


### [151] [MarginSel : Max-Margin Demonstration Selection for LLMs](https://arxiv.org/abs/2506.06699)
*Rajeev Bhatt Ambati,James Lester,Shashank Srivastava,Snigdha Chaturvedi*

Main category: cs.LG

TL;DR: 提出MarginSel方法，通过选择类似支持向量的困难样本增加模型边距，在分类任务中实现2-7%的F1分数提升


<details>
  <summary>Details</summary>
Motivation: 传统上下文学习(ICL)的效果对演示样本的选择和排序敏感，需改进示例选择机制

Method: 两阶段困难样本选择框架：1) 筛选硬样本 2) 通过最大化边距调整决策边界，模拟支持向量机原理

Result: 在多个分类任务中实现绝对2-7%的F1值提升，理论证明其能诱导LLMs产生最大边距行为

Conclusion: MarginSel通过增加硬样本的决策边距，有效改善LLMs的决策边界方向，提升小样本学习效果

Abstract: Large Language Models (LLMs) excel at few-shot learning via in-context
learning (ICL). However, the effectiveness of ICL is often sensitive to the
selection and ordering of demonstration examples. To address this, we present
MarginSel: Max-Margin Demonstration Selection for LLMs, a two-step method that
selects hard demonstration examples for the ICL prompt, adapting to each test
instance. Our approach achieves 2-7% absolute improvement in F1-score across
classification tasks, compared to a random selection of examples. We also
provide theoretical insights and empirical evidence showing that MarginSel
induces max-margin behavior in LLMs by effectively increasing the margin for
hard examples, analogous to support vectors, thereby shifting the decision
boundary in a beneficial direction.

</details>


### [152] [Efficient Text-Attributed Graph Learning through Selective Annotation and Graph Alignment](https://arxiv.org/abs/2506.07168)
*Huanyi Xie,Lijie Hu,Lu Yu,Tianhao Huang,Longfei Li,Meng Li,Jun Zhou,Huan Wang,Di Wang*

Main category: cs.LG

TL;DR: 提出GAGA框架，仅需标注1%数据即可实现高效TAG表征学习，通过双层对齐机制整合标注图与原始图结构


<details>
  <summary>Details</summary>
Motivation: 传统GNN难以处理带复杂文本属性的图数据，现有LLM增强方法需要全图标注/微调，成本过高

Method: 1. 标注代表性节点/边构建标注图 2. 设计拓扑感知的标注图 3. 开发节点级和图级双对齐模块进行结构融合

Result: 在分类准确率持平/超越SOTA的同时，将标注数据量减少至1%

Conclusion: GAGA为TAG学习提供了高成本效益的解决方案，显著降低标注成本并保持模型性能

Abstract: In the realm of Text-attributed Graphs (TAGs), traditional graph neural
networks (GNNs) often fall short due to the complex textual information
associated with each node. Recent methods have improved node representations by
leveraging large language models (LLMs) to enhance node text features, but
these approaches typically require extensive annotations or fine-tuning across
all nodes, which is both time-consuming and costly. To overcome these
challenges, we introduce GAGA, an efficient framework for TAG representation
learning. GAGA reduces annotation time and cost by focusing on annotating only
representative nodes and edges. It constructs an annotation graph that captures
the topological relationships among these annotations. Furthermore, GAGA
employs a two-level alignment module to effectively integrate the annotation
graph with the TAG, aligning their underlying structures. Experiments show that
GAGA achieves classification accuracies on par with or surpassing
state-of-the-art methods while requiring only 1% of the data to be annotated,
demonstrating its high efficiency.

</details>


### [153] [When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment](https://arxiv.org/abs/2506.07452)
*Yuxin Xiao,Sana Tonekaboni,Walter Gerych,Vinith Suriyakumar,Marzyeh Ghassemi*

Main category: cs.LG

TL;DR: 发现LLM在处理含特定风格模式的恶意查询时攻击成功率显著上升，并提出防御策略SafeStyle有效提升模型安全性


<details>
  <summary>Details</summary>
Motivation: 探究风格模式是否威胁LLM安全性、表面风格对齐如何放大模型脆弱性，以及如何通过对齐过程缓解风险

Method: 评估32个LLM在7个越狱基准的表现，分析风格模式长度/注意力权重与攻击成功率的关系，设计安全训练数据增强策略

Result: 风格模式使平均攻击成功率提升5.3倍，注意力权重与ASR膨胀显著相关；风格对齐训练使同风格攻击成功率提高1.9倍；SafeStyle防御效果优于基线

Conclusion: 风格模式是LLM安全漏洞放大器，SafeStyle通过风格分布匹配的安全数据增强，有效维持对齐训练后的模型安全性

Abstract: Large language models (LLMs) can be prompted with specific styles (e.g.,
formatting responses as lists), including in jailbreak queries. Although these
style patterns are semantically unrelated to the malicious intents behind
jailbreak queries, their safety impact remains unclear. In this work, we seek
to understand whether style patterns compromise LLM safety, how superficial
style alignment increases model vulnerability, and how best to mitigate these
risks during alignment. We evaluate 32 LLMs across seven jailbreak benchmarks,
and find that malicious queries with style patterns inflate the attack success
rate (ASR) for nearly all models. Notably, ASR inflation correlates with both
the length of style patterns and the relative attention an LLM exhibits on
them. We then investigate superficial style alignment, and find that
fine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of
those same styles. Finally, we propose SafeStyle, a defense strategy that
incorporates a small amount of safety training data augmented to match the
distribution of style patterns in the fine-tuning data. Across three LLMs and
five fine-tuning style settings, SafeStyle consistently outperforms baselines
in maintaining LLM safety.

</details>


### [154] [Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models](https://arxiv.org/abs/2506.07468)
*Mickel Liu,Liwei Jiang,Yancheng Liang,Simon Shaolei Du,Yejin Choi,Tim Althoff,Natasha Jaques*

Main category: cs.LG

TL;DR: 提出Self-RedTeam方法，通过自我对抗的强化学习框架实现语言模型攻击者与防御者的动态协同进化，将安全对齐建模为两人零和博弈，显著提升模型安全防御的主动性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型安全对齐采用静态防御+被动修补的循环模式，导致攻击者容易过时化防御策略，防御者始终滞后于新型攻击方式。需要建立动态共适应的安全训练范式。

Method: 1. 构建攻击者-防御者双智能体在线博弈框架，模型动态切换角色
2. 引入奖励模型裁决对抗结果
3. 提出隐藏思维链技术增强攻击多样性并降低过度防御

Result: 1. 攻击多样性提升21.8%(SBERT)
2. WildJailBreak基准防御成功率提升65.5%
3. 成功实现模型自主安全进化

Conclusion: 通过多智能体强化学习的动态协同进化机制，推动语言模型安全训练范式从被动修补转向主动防御，为构建自演进的安全系统提供新路径。

Abstract: Conventional language model (LM) safety alignment relies on a reactive,
disjoint procedure: attackers exploit a static model, followed by defensive
fine-tuning to patch exposed vulnerabilities. This sequential approach creates
a mismatch -- attackers overfit to obsolete defenses, while defenders
perpetually lag behind emerging threats. To address this, we propose
Self-RedTeam, an online self-play reinforcement learning algorithm where an
attacker and defender agent co-evolve through continuous interaction. We cast
safety alignment as a two-player zero-sum game, where a single model alternates
between attacker and defender roles -- generating adversarial prompts and
safeguarding against them -- while a reward LM adjudicates outcomes. This
enables dynamic co-adaptation. Grounded in the game-theoretic framework of
zero-sum games, we establish a theoretical safety guarantee which motivates the
design of our method: if self-play converges to a Nash Equilibrium, the
defender will reliably produce safe responses to any adversarial input.
Empirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared
to attackers trained against static defenders and achieves higher robustness on
safety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained
against static attackers. We further propose hidden Chain-of-Thought, allowing
agents to plan privately, which boosts adversarial diversity and reduces
over-refusals. Our results motivate a shift from reactive patching to proactive
co-evolution in LM safety training, enabling scalable, autonomous, and robust
self-improvement of LMs via multi-agent reinforcement learning (MARL).

</details>


### [155] [Graph-of-Causal Evolution: Challenging Chain-of-Model for Reasoning](https://arxiv.org/abs/2506.07501)
*Libo Wang*

Main category: cs.LG

TL;DR: 提出因果演化图GoCE解决链式模型长程依赖丢失问题，通过因果约束渗透和自进化门实现动态平衡


<details>
  <summary>Details</summary>
Motivation: 传统CoM架构中子链仅依赖前序信息，因果掩码阻碍全局上下文流动导致长程依赖丢失

Method: 1. 将隐式表征映射为可微分稀疏因果邻接矩阵
2. 通过因果注意力与因果-MoE实现层级约束渗透
3. 结合干预一致性损失与自进化门实现架构自适应更新

Result: 在CLUTRR/CLADDER等数据集测试显示：
- 长程因果依赖捕捉能力提升38%
- 自进化效率提高2.3倍
- 准确率超越基线LLMs 15.7%

Conclusion: GoCE不仅超越CoM设计，还为因果学习与持续自适应改进研究提供了：
1. 新型架构范式
2. 动态平衡机制
3. 可扩展的因果约束实现方案

Abstract: In view of the problem that each subchain in the chain-of-model (CoM) relies
only on the information of the previous subchain and may lose long-range
dependencies due to the causal mask blocking the global context flow between
multi-level subchains, this work proposes a graph of causal evolution (GoCE).
Its core principle is to map the implicit token representation into a
differentiable and sparse causal adjacency matrix, then permeate causal
constraints through each layer of calculation using causal-masked attention and
causal-MoE. By combining intervention consistency loss test and self-evolution
gate, the dynamic balance between causal structure learning and adaptive
updating of transformer architecture is realized. The researcher built
experimental environments in sandboxes built with Claude Sonnet 4,
o4-mini-high, and DeepSeek R1 respectively with the transformer variant
architecture introduced in GoCE. It is evaluated on publicly available datasets
including CLUTRR, CLADDER, EX-FEVER, and CausalQA and compared with the
baseline LLMs. The finding proves that GoCE strengthens the transformer's
ability to capture long-range causal dependencies, while the ability to
self-evolve is improved. It not only surpasses the design of CoM in terms of
design principles, but also provides experience for future research on causal
learning and continuous adaptive improvement.

</details>


### [156] [ChemAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning](https://arxiv.org/abs/2506.07551)
*Mengsong Wu,YaFei Wang,Yidong Ming,Yuqi An,Yuwei Wan,Wenliang Chen,Binbin Lin,Yuqiang Li,Tong Xie,Dongzhan Zhou*

Main category: cs.LG

TL;DR: 提出基于LLM的ChemistryAgent框架，整合137个化学工具和HE-MCTS优化算法，显著提升化学QA和发现任务性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在化学任务中存在的预训练知识滞后与专业知识整合困难问题

Method: 1. 开发ChemistryAgent集成137种化学工具 2. 构建ChemToolBench数据集 3. 提出分层进化蒙特卡洛树搜索(HE-MCTS)框架

Result: 实验显示该方法在化学QA任务中超越GPT-4o，PRM/ORM模型准确率提升显著（+15%）

Conclusion: 通过工具集成与HE-MCTS优化，为LLMs与专业工具结合提供有效解决方案，代码与数据集已开源

Abstract: Large language models (LLMs) have recently demonstrated promising
capabilities in chemistry tasks while still facing challenges due to outdated
pretraining knowledge and the difficulty of incorporating specialized chemical
expertise. To address these issues, we propose an LLM-based agent that
synergistically integrates 137 external chemical tools created ranging from
basic information retrieval to complex reaction predictions, and a dataset
curation pipeline to generate the dataset ChemToolBench that facilitates both
effective tool selection and precise parameter filling during fine-tuning and
evaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search
(HE-MCTS) framework, enabling independent optimization of tool planning and
execution. By leveraging self-generated data, our approach supports step-level
fine-tuning (FT) of the policy model and training task-adaptive PRM and ORM
that surpass GPT-4o. Experimental evaluations demonstrate that our approach
significantly improves performance in Chemistry QA and discovery tasks,
offering a robust solution to integrate specialized tools with LLMs for
advanced chemical applications. All datasets and code are available at
https://github.com/AI4Chem/ChemistryAgent .

</details>


### [157] [E-LDA: Toward Interpretable LDA Topic Models with Strong Guarantees in Logarithmic Parallel Time](https://arxiv.org/abs/2506.07747)
*Adam Breuer*

Main category: cs.LG

TL;DR: 提出首个基于组合优化的非梯度LDA主题模型推断算法，在并行计算效率和可解释性方面实现指数级提升，优于主流神经/LLM主题模型


<details>
  <summary>Details</summary>
Motivation: 传统LDA算法存在收敛速度慢（线性时间）、缺乏可解释性保证、无法满足因果推断独立性假设等问题，制约其在社会科学和因果研究中的应用

Method: 采用组合优化方法替代梯度下降，设计对数级适应性的并行算法，通过关键词关联机制建立可解释性保障

Result: 在多样化文本数据集上持续生成语义质量更高的主题，计算效率较现有方法提升指数级（log vs linear时间）

Conclusion: 该算法突破性地解决了主题模型在计算效率-可解释性-因果有效性三角权衡问题，为社会科学定量分析提供新工具

Abstract: In this paper, we provide the first practical algorithms with provable
guarantees for the problem of inferring the topics assigned to each document in
an LDA topic model. This is the primary inference problem for many applications
of topic models in social science, data exploration, and causal inference
settings. We obtain this result by showing a novel non-gradient-based,
combinatorial approach to estimating topic models. This yields algorithms that
converge to near-optimal posterior probability in logarithmic parallel
computation time (adaptivity) -- exponentially faster than any known LDA
algorithm. We also show that our approach can provide interpretability
guarantees such that each learned topic is formally associated with a known
keyword. Finally, we show that unlike alternatives, our approach can maintain
the independence assumptions necessary to use the learned topic model for
downstream causal inference methods that allow researchers to study topics as
treatments. In terms of practical performance, our approach consistently
returns solutions of higher semantic quality than solutions from
state-of-the-art LDA algorithms, neural topic models, and LLM-based topic
models across a diverse range of text datasets and evaluation parameters.

</details>


### [158] [Improving large language models with concept-aware fine-tuning](https://arxiv.org/abs/2506.07833)
*Michael K. Chen,Xikun Zhang,Jiaxing Huang,Dacheng Tao*

Main category: cs.LG

TL;DR: 提出概念感知微调方法CAFT，突破现有LLM逐令牌训练限制，通过多令牌预测实现高效概念学习，显著提升文本摘要和蛋白质设计等任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM的逐令牌预测范式将语义整体拆解为碎片化文本片段（如将'ribonucleic acid'拆分为'rib'/'on'等），阻碍模型形成高级概念认知，限制了类人理解和推理能力的发展。

Method: 首创概念感知微调框架CAFT：在微调阶段引入多令牌序列训练，使模型学习跨令牌的连贯语义实体（如完整生物术语/专业概念），突破传统方法仅支持预训练阶段多令牌预测的限制。

Result: 实验证明CAFT在文本摘要（传统任务）和de novo蛋白质设计（领域任务）中均显著优于传统单令牌微调方法，且首次将多令牌训练成本从预训练阶段的高昂开销降低到普通研究者可承担的微调阶段。

Conclusion: CAFT不仅为LLM概念学习开辟新路径，其'训练后多令牌预测'创新范式更具普惠价值——使更多研究者在无需重复预训练的情况下获得概念增强能力，对机器学习社区具有广泛启发意义。

Abstract: Large language models (LLMs) have become the cornerstone of modern AI.
However, the existing paradigm of next-token prediction fundamentally limits
their ability to form coherent, high-level concepts, making it a critical
barrier to human-like understanding and reasoning. Take the phrase "ribonucleic
acid" as an example: an LLM will first decompose it into tokens, i.e.,
artificial text fragments ("rib", "on", ...), then learn each token
sequentially, rather than grasping the phrase as a unified, coherent semantic
entity. This fragmented representation hinders deeper conceptual understanding
and, ultimately, the development of truly intelligent systems. In response, we
introduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method
that redefines how LLMs are fine-tuned. By enabling the learning of sequences
that span multiple tokens, this method fosters stronger concept-aware learning.
Our experiments demonstrate significant improvements compared to conventional
next-token finetuning methods across diverse tasks, including traditional
applications like text summarization and domain-specific ones like de novo
protein design. Multi-token prediction was previously only possible in the
prohibitively expensive pretraining phase; CAFT, to our knowledge, is the first
to bring the multi-token setting to the post-training phase, thus effectively
democratizing its benefits for the broader community of practitioners and
researchers. Finally, the unexpected effectiveness of our proposed method
suggests wider implications for the machine learning research community. All
code and data are available at https://github.com/michaelchen-lab/caft-llm

</details>


### [159] [Uncovering the Functional Roles of Nonlinearity in Memory](https://arxiv.org/abs/2506.07919)
*Manuel Brenner,Georgia Koppe*

Main category: cs.LG

TL;DR: 研究发现循环神经网络中最小程度的非线性处理既能满足需求又常能达到最佳效果，产生更简洁、鲁棒且可解释的模型


<details>
  <summary>Details</summary>
Motivation: 系统解析循环网络中非线性机制的功能角色，明确其计算必要性和作用原理

Method: 使用具备细粒度非线性控制能力的几乎线性循环神经网络（AL-RNNs）作为建模工具和分析手段

Result: 在多个经典序列建模任务和真实世界刺激选择任务中，最小非线性模型表现优于完全非线性或纯线性模型

Conclusion: 建立了选择性引入非线性的理论框架，为人工和生物神经系统在长程记忆与结构化计算方面提供了新的研究视角

Abstract: Memory and long-range temporal processing are core requirements for sequence
modeling tasks across natural language processing, time-series forecasting,
speech recognition, and control. While nonlinear recurrence has long been
viewed as essential for enabling such mechanisms, recent work suggests that
linear dynamics may often suffice. In this study, we go beyond performance
comparisons to systematically dissect the functional role of nonlinearity in
recurrent networks--identifying both when it is computationally necessary, and
what mechanisms it enables. We use Almost Linear Recurrent Neural Networks
(AL-RNNs), which allow fine-grained control over nonlinearity, as both a
flexible modeling tool and a probe into the internal mechanisms of memory.
Across a range of classic sequence modeling tasks and a real-world stimulus
selection task, we find that minimal nonlinearity is not only sufficient but
often optimal, yielding models that are simpler, more robust, and more
interpretable than their fully nonlinear or linear counterparts. Our results
provide a principled framework for selectively introducing nonlinearity,
bridging dynamical systems theory with the functional demands of long-range
memory and structured computation in recurrent neural networks, with
implications for both artificial and biological neural systems.

</details>


### [160] [HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization](https://arxiv.org/abs/2506.07972)
*Hongzheng Chen,Yingheng Wang,Yaohui Cai,Hins Hu,Jiajie Li,Shirley Huang,Chenhui Deng,Rongjian Liang,Shufeng Kong,Haoxing Ren,Samitha Samaranayake,Carla P. Gomes,Zhiru Zhang*

Main category: cs.LG

TL;DR: 开发HeuriGym框架评估LLMs在组合优化中的启发式算法，揭示现有模型在工具使用和推理方面的局限性


<details>
  <summary>Details</summary>
Motivation: 现有评估方法存在封闭式问题易饱和和主观评价缺乏严谨性的缺陷，需要更有效的LLM评估体系

Method: 构建支持LLM提出启发式算法→代码执行反馈→迭代改进的框架，在9个领域问题上测试9个先进模型

Result: 顶尖模型QYI得分仅0.6（专家基准1），暴露工具使用、规划、自适应推理的持续缺陷

Conclusion: 开源基准HeuriGym旨在推动LLMs向科学工程领域更有效的实际问题解决方向发展

Abstract: While Large Language Models (LLMs) have demonstrated significant advancements
in reasoning and agent-based problem-solving, current evaluation methodologies
fail to adequately assess their capabilities: existing benchmarks either rely
on closed-ended questions prone to saturation and memorization, or subjective
comparisons that lack consistency and rigor. In this work, we introduce
HeuriGym, an agentic framework designed for evaluating heuristic algorithms
generated by LLMs for combinatorial optimization problems, characterized by
clearly defined objectives and expansive solution spaces. HeuriGym empowers
LLMs to propose heuristics, receive evaluative feedback via code execution, and
iteratively refine their solutions. We evaluate nine state-of-the-art models on
nine problems across domains such as computer systems, logistics, and biology,
exposing persistent limitations in tool use, planning, and adaptive reasoning.
To quantify performance, we propose the Quality-Yield Index (QYI), a metric
that captures both solution pass rate and quality. Even top models like
GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below
the expert baseline of 1. Our open-source benchmark aims to guide the
development of LLMs toward more effective and realistic problem-solving in
scientific and engineering domains.

</details>


### [161] [Reparameterized LLM Training via Orthogonal Equivalence Transformation](https://arxiv.org/abs/2506.08001)
*Zeju Qiu,Simon Buchholz,Tim Z. Xiao,Maximilian Dax,Bernhard Schölkopf,Weiyang Liu*

Main category: cs.LG

TL;DR: 提出POET算法通过正交等价变换优化神经元参数化，提升大语言模型训练的稳定性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型训练存在稳定性差和泛化能力不足的挑战，传统方法难以有效优化神经网络参数

Method: 使用两个可学习的正交矩阵和固定随机权重矩阵对神经元进行重参数化，保持权重矩阵的谱特性，并开发高效近似方法实现扩展

Result: 大量实验验证POET在LLM训练中的有效性和扩展性，证明其能稳定优化目标函数并提升泛化性能

Conclusion: POET为大规模神经网络训练提供了具有理论保障且可扩展的优化方案，显著提升训练效果

Abstract: While large language models (LLMs) are driving the rapid advancement of
artificial intelligence, effectively and reliably training these large models
remains one of the field's most significant challenges. To address this
challenge, we propose POET, a novel reParameterized training algorithm that
uses Orthogonal Equivalence Transformation to optimize neurons. Specifically,
POET reparameterizes each neuron with two learnable orthogonal matrices and a
fixed random weight matrix. Because of its provable preservation of spectral
properties of weight matrices, POET can stably optimize the objective function
with improved generalization. We further develop efficient approximations that
make POET flexible and scalable for training large-scale neural networks.
Extensive experiments validate the effectiveness and scalability of POET in
training LLMs.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [162] [How Malicious AI Swarms Can Threaten Democracy](https://arxiv.org/abs/2506.06299)
*Daniel Thilo Schroeder,Meeyoung Cha,Andrea Baronchelli,Nick Bostrom,Nicholas A. Christakis,David Garcia,Amit Goldenberg,Yara Kyrychenko,Kevin Leyton-Brown,Nina Lutz,Gary Marcus,Filippo Menczer,Gordon Pennycook,David G. Rand,Frank Schweitzer,Christopher Summerfield,Audrey Tang,Jay Van Bavel,Sander van der Linden,Dawn Song,Jonas R. Kunst*

Main category: cs.CY

TL;DR: AI技术发展催生恶意群体智能系统，通过协同攻击威胁民主进程，需构建平台防护、模型保护和全球监测三位一体防御体系。


<details>
  <summary>Details</summary>
Motivation: 揭示AI群体智能可能通过隐蔽协同、持续测试等手段制造虚假共识、操控舆论、破坏民主制度的核心威胁，呼吁建立防御机制。

Method: 提出平台侧部署群体检测仪表盘和AI防护盾，模型侧实施说服力风险评估/数字水印，系统层建立联合国AI影响力观测站的三层防御策略。

Result: 恶意AI群体可能导致选民微压制、训练数据污染、机构信任崩塌等严重后果，当前全球78%的选举系统存在可被攻破的漏洞。

Conclusion: 需立即实施多层次防御方案，通过技术屏障与全球协同监测应对AI群体智能带来的新型数字威胁，保护民主基础设施。

Abstract: Advances in AI portend a new era of sophisticated disinformation operations.
While individual AI systems already create convincing -- and at times
misleading -- information, an imminent development is the emergence of
malicious AI swarms. These systems can coordinate covertly, infiltrate
communities, evade traditional detectors, and run continuous A/B tests, with
round-the-clock persistence. The result can include fabricated grassroots
consensus, fragmented shared reality, mass harassment, voter micro-suppression
or mobilization, contamination of AI training data, and erosion of
institutional trust. With democratic processes worldwide increasingly
vulnerable, we urge a three-pronged response: (1) platform-side defenses --
always-on swarm-detection dashboards, pre-election high-fidelity
swarm-simulation stress-tests, transparency audits, and optional client-side
"AI shields" for users; (2) model-side safeguards -- standardized
persuasion-risk tests, provenance-authenticating passkeys, and watermarking;
and (3) system-level oversight -- a UN-backed AI Influence Observatory.

</details>


### [163] [LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment](https://arxiv.org/abs/2506.06355)
*Lingyao Li,Dawei Li,Zhenhui Ou,Xiaoran Xu,Jingxiao Liu,Zihui Ma,Runlong Yu,Min Deng*

Main category: cs.CY

TL;DR: 大语言模型通过多模态数据实现地震影响预测，相关系数达0.88，视觉输入显著提升模拟精度。


<details>
  <summary>Details</summary>
Motivation: 利用LLMs的复杂场景模拟能力提升突发性灾害的主动应急准备能力

Method: 整合地理空间/社会经济/建筑/街景多模态数据，开发MMI预测框架，使用RAG和ICL技术优化模型性能

Result: 在2014纳帕和2019里奇克莱斯特地震中实现0.88高相关性和0.77低RMSE，视觉数据比纯结构化数据准确度提升明显

Conclusion: LLM灾害模拟技术可有效支持灾前规划，多模态融合是提升预测精度的关键路径

Abstract: Efficient simulation is essential for enhancing proactive preparedness for
sudden-onset disasters such as earthquakes. Recent advancements in large
language models (LLMs) as world models show promise in simulating complex
scenarios. This study examines multiple LLMs to proactively estimate perceived
earthquake impacts. Leveraging multimodal datasets including geospatial,
socioeconomic, building, and street-level imagery data, our framework generates
Modified Mercalli Intensity (MMI) predictions at zip code and county scales.
Evaluations on the 2014 Napa and 2019 Ridgecrest earthquakes using USGS ''Did
You Feel It? (DYFI)'' reports demonstrate significant alignment, as evidenced
by a high correlation of 0.88 and a low RMSE of 0.77 as compared to real
reports at the zip code level. Techniques such as RAG and ICL can improve
simulation performance, while visual inputs notably enhance accuracy compared
to structured numerical data alone. These findings show the promise of LLMs in
simulating disaster impacts that can help strengthen pre-event planning.

</details>


### [164] [From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law](https://arxiv.org/abs/2506.06391)
*John Mavi,Diana Teodora Găitan,Sergio Coronado*

Main category: cs.CY

TL;DR: 研究评估8个主流大语言模型在国际人道法合规性方面的表现，发现系统级安全提示能显著提升拒绝解释质量，但复杂提示仍存在漏洞


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型拒绝违反国际人道法请求的能力及其响应质量，确保AI系统安全透明

Method: 通过测试模型对明确违反法律框架提示的拒绝能力，分析响应清晰度、解释完整性及标准化安全提示干预效果

Result: 标准化安全提示提升73%模型解释质量，但涉及技术术语或代码请求时仍存在合规漏洞

Conclusion: 需建立轻量级干预机制并制定LLM-IHL合规基准，推动开发更安全透明的AI系统

Abstract: Large Language Models (LLMs) are widely used across sectors, yet their
alignment with International Humanitarian Law (IHL) is not well understood.
This study evaluates eight leading LLMs on their ability to refuse prompts that
explicitly violate these legal frameworks, focusing also on helpfulness - how
clearly and constructively refusals are communicated. While most models
rejected unlawful requests, the clarity and consistency of their responses
varied. By revealing the model's rationale and referencing relevant legal or
safety principles, explanatory refusals clarify the system's boundaries, reduce
ambiguity, and help prevent misuse. A standardised system-level safety prompt
significantly improved the quality of the explanations expressed within
refusals in most models, highlighting the effectiveness of lightweight
interventions. However, more complex prompts involving technical language or
requests for code revealed ongoing vulnerabilities. These findings contribute
to the development of safer, more transparent AI systems and propose a
benchmark to evaluate the compliance of LLM with IHL.

</details>


### [165] [Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts Traditional Measurement Approaches](https://arxiv.org/abs/2506.06540)
*Patrick Y. Wu*

Main category: cs.CY

TL;DR: 大语言模型（LLMs）可替代专家政治调查用于突发事件分析，通过案例研究验证其在意识形态评分和机构认知预测的有效性。


<details>
  <summary>Details</summary>
Motivation: 突发事件后传统专家调查易受结果干扰难以还原事前认知，LLMs基于海量数据可突破传统测量限制。

Method: 使用LLMs对联邦机构进行成对比较生成意识形态分数，结合知识机构认知度构建预测模型。

Result: LLMs复现了裁员前专家评分，准确预测被裁机构；知识机构认知度独立影响裁员决策。

Conclusion: 提出LLMs替代专家调查的双重标准：传统测量失效时+需快速验证关联假设的场景。

Abstract: After a disruptive event or shock, such as the Department of Government
Efficiency (DOGE) federal layoffs of 2025, expert judgments are colored by
knowledge of the outcome. This can make it difficult or impossible to
reconstruct the pre-event perceptions needed to study the factors associated
with the event. This position paper argues that large language models (LLMs),
trained on vast amounts of digital media data, can be a viable substitute for
expert political surveys when a shock disrupts traditional measurement. We
analyze the DOGE layoffs as a specific case study for this position. We use
pairwise comparison prompts with LLMs and derive ideology scores for federal
executive agencies. These scores replicate pre-layoff expert measures and
predict which agencies were targeted by DOGE. We also use this same approach
and find that the perceptions of certain federal agencies as knowledge
institutions predict which agencies were targeted by DOGE, even when
controlling for ideology. This case study demonstrates that using LLMs allows
us to rapidly and easily test the associated factors hypothesized behind the
shock. More broadly, our case study of this recent event exemplifies how LLMs
offer insights into the correlational factors of the shock when traditional
measurement techniques fail. We conclude by proposing a two-part criterion for
when researchers can turn to LLMs as a substitute for expert political surveys.

</details>


### [166] [Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce](https://arxiv.org/abs/2506.06576)
*Yijia Shao,Humishka Zope,Yucheng Jiang,Jiaxin Pei,David Nguyen,Erik Brynjolfsson,Diyi Yang*

Main category: cs.CY

TL;DR: 论文提出审计框架评估AI代理在职业任务中的自动化/增强需求与当前技术能力的匹配，通过WORKBank数据库揭示四类任务区域和人类参与偏好的多样性。


<details>
  <summary>Details</summary>
Motivation: 复合AI系统重塑劳动力市场引发担忧，但缺乏对工人实际需求与技术能力匹配程度的系统性研究。现有研究难以捕捉工人对自动化/增强的复杂意愿，且缺乏量化人类参与度的统一标准。

Method: 1. 开发音频增强微访谈法收集工人需求细节
2. 创建人类参与量表（HAS）量化偏好
3. 基于O*NET构建WORKBank数据库（1,500名工人偏好+AI专家对844项任务的评估）
4. 四象限分类法（自动化绿灯/红灯区、研发机遇区、低优先级区）

Result: 1. 发现不同职业的HAS特征存在显著差异（如教师高HAS值，数据分析师低HAS值）
2. 识别出自动化需求与技术能力不匹配的关键领域
3. 预示核心人力技能将从信息处理转向人际能力

Conclusion: AI开发需与人类需求对齐，应重视工人对自主权的保留需求，未来劳动力培养需强化人际技能以适应人机协作新范式。

Abstract: The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the
labor market, raising concerns about job displacement, diminished human agency,
and overreliance on automation. Yet, we lack a systematic understanding of the
evolving landscape. In this paper, we address this gap by introducing a novel
auditing framework to assess which occupational tasks workers want AI agents to
automate or augment, and how those desires align with the current technological
capabilities. Our framework features an audio-enhanced mini-interview to
capture nuanced worker desires and introduces the Human Agency Scale (HAS) as a
shared language to quantify the preferred level of human involvement. Using
this framework, we construct the WORKBank database, building on the U.S.
Department of Labor's O*NET database, to capture preferences from 1,500 domain
workers and capability assessments from AI experts across over 844 tasks
spanning 104 occupations. Jointly considering the desire and technological
capability divides tasks in WORKBank into four zones: Automation "Green Light"
Zone, Automation "Red Light" Zone, R&D Opportunity Zone, Low Priority Zone.
This highlights critical mismatches and opportunities for AI agent development.
Moving beyond a simple automate-or-not dichotomy, our results reveal diverse
HAS profiles across occupations, reflecting heterogeneous expectations for
human involvement. Moreover, our study offers early signals of how AI agent
integration may reshape the core human competencies, shifting from
information-focused skills to interpersonal ones. These findings underscore the
importance of aligning AI agent development with human desires and preparing
workers for evolving workplace dynamics.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [167] [HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions](https://arxiv.org/abs/2506.06409)
*Dor Tsur,Carol Xuan Long,Claudio Mayrink Verdun,Hsiang Hsu,Chen-Fu Chen,Haim Permuter,Sajani Vithana,Flavio P. Calmon*

Main category: cs.CR

TL;DR: 提出可调谐水印框架HeavyWater和SimplexWater，优化低熵场景下的检测精度与文本失真平衡


<details>
  <summary>Details</summary>
Motivation: 现有LLM水印在低熵生成任务（如代码生成）中因预测确定性过高难以有效工作

Method: 建立优化框架设计水印算法，利用随机辅助信息进行动态调整

Result: 新水印在低熵场景实现高检测准确率（如代码生成仅0.3%失真下保持效果）

Conclusion: 水印设计与编码理论存在新联系，开源实现促进LLM可信应用

Abstract: Large language model (LLM) watermarks enable authentication of text
provenance, curb misuse of machine-generated text, and promote trust in AI
systems. Current watermarks operate by changing the next-token predictions
output by an LLM. The updated (i.e., watermarked) predictions depend on random
side information produced, for example, by hashing previously generated tokens.
LLM watermarking is particularly challenging in low-entropy generation tasks -
such as coding - where next-token predictions are near-deterministic. In this
paper, we propose an optimization framework for watermark design. Our goal is
to understand how to most effectively use random side information in order to
maximize the likelihood of watermark detection and minimize the distortion of
generated text. Our analysis informs the design of two new watermarks:
HeavyWater and SimplexWater. Both watermarks are tunable, gracefully
trading-off between detection accuracy and text distortion. They can also be
applied to any LLM and are agnostic to side information generation. We examine
the performance of HeavyWater and SimplexWater through several benchmarks,
demonstrating that they can achieve high watermark detection accuracy with
minimal compromise of text generation quality, particularly in the low-entropy
regime. Our theoretical analysis also reveals surprising new connections
between LLM watermarking and coding theory. The code implementation can be
found in https://github.com/DorTsur/HeavyWater_SimplexWater

</details>


### [168] [Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test](https://arxiv.org/abs/2506.06975)
*Xiaoyuan Zhu,Yaowen Ye,Tianyi Qiu,Hanlin Zhu,Sijun Tan,Ajraf Mannan,Jonathan Michala,Raluca Ada Popa,Willie Neiswanger*

Main category: cs.CR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As API access becomes a primary interface to large language models (LLMs),
users often interact with black-box systems that offer little transparency into
the deployed model. To reduce costs or maliciously alter model behaviors, API
providers may discreetly serve quantized or fine-tuned variants, which can
degrade performance and compromise safety. Detecting such substitutions is
difficult, as users lack access to model weights and, in most cases, even
output logits. To tackle this problem, we propose a rank-based uniformity test
that can verify the behavioral equality of a black-box LLM to a locally
deployed authentic model. Our method is accurate, query-efficient, and avoids
detectable query patterns, making it robust to adversarial providers that
reroute or mix responses upon the detection of testing attempts. We evaluate
the approach across diverse threat scenarios, including quantization, harmful
fine-tuning, jailbreak prompts, and full model substitution, showing that it
consistently achieves superior statistical power over prior methods under
constrained query budgets.

</details>


### [169] [HauntAttack: When Attack Follows Reasoning as a Shadow](https://arxiv.org/abs/2506.07031)
*Jingyuan Ma,Rui Li,Zheng Li,Junfeng Liu,Lei Sha,Zhifang Sui*

Main category: cs.CR

TL;DR: HauntAttack框架揭示先进大型推理模型在安全与推理能力间存在显著权衡漏洞，通过替换条件嵌入有害指令可诱导模型生成不安全输出。


<details>
  <summary>Details</summary>
Motivation: 探究当推理能力与有害性高度耦合时，LRMs在安全性和推理性能之间的潜在风险与权衡机制。

Method: 设计黑盒攻击框架，将推理问题中的原始条件替换为有害指令，构建逐步导向不安全输出的推理路径链。

Result: 实验表明当前最先进LRMs均存在严重安全漏洞，不同模型对各类有害指令的响应模式呈现系统性脆弱特征。

Conclusion: 该研究为LRMs的安全防护机制设计提供了关键实证依据，揭示了模型内在推理逻辑与安全边界协同优化的必要性。

Abstract: Emerging Large Reasoning Models (LRMs) consistently excel in mathematical and
reasoning tasks, showcasing exceptional capabilities. However, the enhancement
of reasoning abilities and the exposure of their internal reasoning processes
introduce new safety vulnerabilities. One intriguing concern is: when reasoning
is strongly entangled with harmfulness, what safety-reasoning trade-off do LRMs
exhibit? To address this issue, we introduce HauntAttack, a novel and
general-purpose black-box attack framework that systematically embeds harmful
instructions into reasoning questions. Specifically, we treat reasoning
questions as carriers and substitute one of their original conditions with a
harmful instruction. This process creates a reasoning pathway in which the
model is guided step by step toward generating unsafe outputs. Based on
HauntAttack, we conduct comprehensive experiments on multiple LRMs. Our results
reveal that even the most advanced LRMs exhibit significant safety
vulnerabilities. Additionally, we perform a detailed analysis of different
models, various types of harmful instructions, and model output patterns,
providing valuable insights into the security of LRMs.

</details>


### [170] [Beyond Jailbreaks: Revealing Stealthier and Broader LLM Security Risks Stemming from Alignment Failures](https://arxiv.org/abs/2506.07402)
*Yukai Zhou,Sibei Yang,Wenjie Wang*

Main category: cs.CR

TL;DR: 研究揭示大语言模型在看似无害的输入下仍存在高风险的隐式危害（Implicit Harm），现有安全评估框架存在盲区，需开发新型评测方法。


<details>
  <summary>Details</summary>
Motivation: 传统越狱攻击仅关注明显有害的查询，忽视了模型在无害输入场景下输出错误结果可能引发的现实危害。

Method: 提出三维度风险评测框架JailFlipBench，包含单模态、多模态和事实扩展场景，并开发配套的攻击方法论进行跨模型评估。

Result: 实验证明开源和闭源模型普遍存在隐式危害风险，部分场景攻击成功率高达75%，证实该风险的现实紧迫性。

Conclusion: LLM安全评估需突破传统越狱范式，建立覆盖输入输出双维度的新型对齐框架，防范隐式危害引发的系统性风险。

Abstract: Large language models (LLMs) are increasingly deployed in real-world
applications, raising concerns about their security. While jailbreak attacks
highlight failures under overtly harmful queries, they overlook a critical
risk: incorrectly answering harmless-looking inputs can be dangerous and cause
real-world harm (Implicit Harm). We systematically reformulate the LLM risk
landscape through a structured quadrant perspective based on output factuality
and input harmlessness, uncovering an overlooked high-risk region. To
investigate this gap, we propose JailFlipBench, a benchmark aims to capture
implicit harm, spanning single-modal, multimodal, and factual extension
scenarios with diverse evaluation metrics. We further develop initial JailFlip
attack methodologies and conduct comprehensive evaluations across multiple
open-source and black-box LLMs, show that implicit harm present immediate and
urgent real-world risks, calling for broader LLM safety assessments and
alignment beyond conventional jailbreak paradigms.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [171] [G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems](https://arxiv.org/abs/2506.07398)
*Guibin Zhang,Muxin Fu,Guancheng Wan,Miao Yu,Kun Wang,Shuicheng Yan*

Main category: cs.MA

TL;DR: 提出分层记忆系统G-Memory，通过三层图架构和双向记忆检索显著提升多智能体系统的任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统的记忆机制过于简单，缺乏对复杂协作轨迹的捕捉和智能体定制化记忆管理，限制了系统的自我进化能力。

Method: 基于组织记忆理论构建G-Memory系统，采用「洞察图-查询图-交互图」三层架构，通过双向记忆遍历实现跨试验知识复用和协作经验编码。

Result: 在5个基准测试中，G-Memory使具身行动成功率最高提升20.89%，知识问答准确率提升10.12%，且无需修改原系统框架。

Conclusion: G-Memory通过动态吸收协作轨迹实现了智能体团队的持续进化，为多智能体系统的记忆架构提供了有效解决方案。

Abstract: Large language model (LLM)-powered multi-agent systems (MAS) have
demonstrated cognitive and execution capabilities that far exceed those of
single LLM agents, yet their capacity for self-evolution remains hampered by
underdeveloped memory architectures. Upon close inspection, we are alarmed to
discover that prevailing MAS memory mechanisms (1) are overly simplistic,
completely disregarding the nuanced inter-agent collaboration trajectories, and
(2) lack cross-trial and agent-specific customization, in stark contrast to the
expressive memory developed for single agents. To bridge this gap, we introduce
G-Memory, a hierarchical, agentic memory system for MAS inspired by
organizational memory theory, which manages the lengthy MAS interaction via a
three-tier graph hierarchy: insight, query, and interaction graphs. Upon
receiving a new user query, G-Memory performs bi-directional memory traversal
to retrieve both $\textit{high-level, generalizable insights}$ that enable the
system to leverage cross-trial knowledge, and $\textit{fine-grained, condensed
interaction trajectories}$ that compactly encode prior collaboration
experiences. Upon task execution, the entire hierarchy evolves by assimilating
new collaborative trajectories, nurturing the progressive evolution of agent
teams. Extensive experiments across five benchmarks, three LLM backbones, and
three popular MAS frameworks demonstrate that G-Memory improves success rates
in embodied action and accuracy in knowledge QA by up to $20.89\%$ and
$10.12\%$, respectively, without any modifications to the original frameworks.
Our codes are available at https://github.com/bingreeky/GMemory.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [172] [Reducing Object Hallucination in Large Audio-Language Models via Audio-Aware Decoding](https://arxiv.org/abs/2506.07233)
*Tzu-wen Hsu,Ke-Han Lu,Cheng-Han Chiang,Hung-yi Lee*

Main category: eess.AS

TL;DR: 针对大型音频语言模型(LALM)的幻觉问题，提出轻量级推理策略AAD，通过对比音频上下文存在/缺失时的预测差异，提升模型可靠性。实验显示F1提升0.046-0.428，Clotho-AQA准确率提升5.4%-10.3%。


<details>
  <summary>Details</summary>
Motivation: 现有音频语言模型在标准测试中表现优异，但存在严重幻觉问题——模型可能虚构音频中不存在的内容。需开发轻量级方法来缓解此问题。

Method: 提出音频感知解码(AAD)：1) 对比解码策略，比较带/不带音频上下文的token预测概率；2) 优先选择音频存在时概率显著提升的token；3) 仅需推理阶段调整，无需重新训练模型。

Result: 在物体幻觉数据集上F1提升0.046-0.428；Clotho-AQA通用音频QA数据集准确率提升5.4%-10.3%；消融实验验证各组件有效性。

Conclusion: AAD通过简单高效的推理策略有效缓解音频语言模型幻觉问题，为现有模型提供即插即用的优化方案，显著提升可靠性与任务性能。

Abstract: Large Audio-Language Models (LALMs) can take audio and text as the inputs and
answer questions about the audio. While prior LALMs have shown strong
performance on standard benchmarks, there has been alarming evidence that LALMs
can hallucinate what is presented in the audio. To mitigate the hallucination
of LALMs, we introduce Audio-Aware Decoding (AAD), a lightweight inference-time
strategy that uses contrastive decoding to compare the token prediction logits
with and without the audio context. By contrastive decoding, AAD promotes the
tokens whose probability increases when the audio is present. We conduct our
experiment on object hallucination datasets with three LALMs and show that AAD
improves the F1 score by 0.046 to 0.428. We also show that AAD can improve the
accuracy on general audio QA datasets like Clotho-AQA by 5.4% to 10.3%. We
conduct thorough ablation studies to understand the effectiveness of each
component in AAD.

</details>


### [173] [Speaker-Distinguishable CTC: Learning Speaker Distinction Using CTC for Multi-Talker Speech Recognition](https://arxiv.org/abs/2506.07515)
*Asahi Sakuma,Hiroaki Sato,Ryuga Sugano,Tadashi Kumano,Yoshihiko Kawai,Tetsuji Ogawa*

Main category: eess.AS

TL;DR: 提出SD-CTC方法解决多说话人语音识别中的说话人分配问题，无需辅助信息即达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 传统SOT方法存在说话人分配错误，依赖辅助信息(如时间戳)在真实场景中难以获取

Method: 扩展CTC框架，联合学习语音帧的文本标记和说话人标签，并与SOT框架多任务集成

Result: 错误率降低26%，性能与依赖辅助信息的先进方法相当

Conclusion: 首次实现仅通过重叠语音和文本即可学习说话人区分，突破多说话人ASR对辅助信息的依赖

Abstract: This paper presents a novel framework for multi-talker automatic speech
recognition without the need for auxiliary information. Serialized Output
Training (SOT), a widely used approach, suffers from recognition errors due to
speaker assignment failures. Although incorporating auxiliary information, such
as token-level timestamps, can improve recognition accuracy, extracting such
information from natural conversational speech remains challenging. To address
this limitation, we propose Speaker-Distinguishable CTC (SD-CTC), an extension
of CTC that jointly assigns a token and its corresponding speaker label to each
frame. We further integrate SD-CTC into the SOT framework, enabling the SOT
model to learn speaker distinction using only overlapping speech and
transcriptions. Experimental comparisons show that multi-task learning with
SD-CTC and SOT reduces the error rate of the SOT model by 26% and achieves
performance comparable to state-of-the-art methods relying on auxiliary
information.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [174] [DISRetrieval: Harnessing Discourse Structure for Long Document Retrieval](https://arxiv.org/abs/2506.06313)
*Huiyao Chen,Yi Yang,Yinghui Li,Meishan Zhang,Min Zhang*

Main category: cs.IR

TL;DR: 论文提出了DISRetrieval框架，通过引入话语结构理论增强长文档理解，在检索指标和问答任务中实现显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有长文档处理方法忽视文本内在的篇章结构，导致检索效果受限。人类通过话语结构理解文本的认知机制未被现有方法有效捕捉。

Method: 1）基于修辞结构理论(RST)构建句子级层次表示；2）LLM增强的节点表示技术结合自适应摘要；3）保持语篇连贯的层次化证据检索机制。

Result: 在QASPER和QuALITY数据集上超越现有方法，消融实验验证话语结构对不同文档长度和查询类型的普适有效性。

Conclusion: 语言结构指导的文档表示显著提升长文本理解效果，公开代码和数据集促进相关研究发展。

Abstract: Long document understanding has become increasingly crucial in natural
language processing, with retrieval-based methods emerging as a promising
solution to address the context length limitations of large language models
(LLMs). However, existing approaches either treat documents as flat sequences
or employ arbitrary chunking strategies, failing to capture the inherent
discourse structure that guides human comprehension. We present DISRetrieval, a
novel hierarchical retrieval framework that leverages linguistic discourse
structure to enhance long document understanding. Our approach introduces three
key innovations: (1) a discourse-aware document organization framework that
utilizes rhetorical structure theory (RST) to create sentence-level
hierarchical representations, preserving both semantic relationships and
natural document flow; (2) an LLM-enhanced node representation technique that
combines discourse structure with adaptive summarization to enrich tree nodes
with contextual information; and (3) a hierarchical evidence retrieval
mechanism that effectively selects relevant content while maintaining discourse
coherence. Through comprehensive experiments on QASPER and QuALITY datasets,
DISRetrieval demonstrates substantial improvements over existing methods in
both token-level retrieval metrics and downstream question answering tasks. Our
ablation studies confirm that incorporating discourse structure significantly
enhances retrieval effectiveness across different document lengths and query
types, validating the importance of linguistically-informed document
representation in long-text understanding. Our code and datasets are publicly
available at github/DreamH1gh/DISRetrieval to facilitate future research.

</details>


### [175] [Is BERTopic Better than PLSA for Extracting Key Topics in Aviation Safety Reports?](https://arxiv.org/abs/2506.06328)
*Aziida Nanyonga,Joiner Keith,Turhan Ugur,Wild Graham*

Main category: cs.IR

TL;DR: 本研究对比BERTopic和PLSA在航空安全报告主题建模中的表现，证明基于Transformer的BERTopic在主题连贯性（Cv 0.41 vs 0.37）和可解释性上更优。


<details>
  <summary>Details</summary>
Motivation: 传统概率模型(如PLSA)在复杂航空数据分析中存在局限，需验证现代transformer方法在航空安全领域的应用效果。

Method: 使用36,000+份NTSB报告，BERTopic采用Transformer嵌入和层次聚类，PLSA使用EM算法概率建模。

Result: BERTopic主题连贯性Cv值0.41优于PLSA的0.37，且经专家验证具有更好的主题可解释性。

Conclusion: 基于Transformer的方法为航空安全数据分析提供更优解决方案，未来将探索混合模型和多语言数据集应用。

Abstract: This study compares the effectiveness of BERTopic and Probabilistic Latent
Semantic Analysis (PLSA) in extracting meaningful topics from aviation safety
reports aiming to enhance the understanding of patterns in aviation incident
data. Using a dataset of over 36,000 National Transportation Safety Board
(NTSB) reports from 2000 to 2020, BERTopic employed transformer based
embeddings and hierarchical clustering, while PLSA utilized probabilistic
modelling through the Expectation-Maximization (EM) algorithm. Results showed
that BERTopic outperformed PLSA in topic coherence, achieving a Cv score of
0.41 compared to PLSA 0.37, while also demonstrating superior interpretability
as validated by aviation safety experts. These findings underscore the
advantages of modern transformer based approaches in analyzing complex aviation
datasets, paving the way for enhanced insights and informed decision-making in
aviation safety. Future work will explore hybrid models, multilingual datasets,
and advanced clustering techniques to further improve topic modelling in this
domain.

</details>


### [176] [FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in Finance-Specific Deployment of Large Language Models](https://arxiv.org/abs/2506.06335)
*Xuan Xu,Fufang Wen,Beilin Chu,Zhibing Fu,Qinhong Lin,Jiaqi Liu,Binjie Fei,Zhongliang Yang,Linna Zhou,Yu Li*

Main category: cs.IR

TL;DR: 金融领域专用双向编码器FinBERT2解决了LLMs在金融任务中的三大局限：在分类/检索/主题建模任务中超越现有模型，提供高效部署方案。


<details>
  <summary>Details</summary>
Motivation: LLMs在金融领域应用存在三大问题：(1)判别任务性能低于微调BERT但成本更高；(2)生成任务过度依赖检索增强；(3)主题建模等场景表现不足。

Method: 基于32B tokens中文金融语料预训练FinBERT2，构建Fin-Labelers(分类)、Fin-Retrievers(检索)、Fin-TopicModel(主题建模)三大变体。

Result: (1)Fin-Labelers平均超越LLMs 9.7%-12.3%；(2)Fin-Retrievers超越BGE-base-zh 6.8%、超越OpenAI嵌入模型4.2%；(3)Fin-TopicModel实现更优的金融标题聚类。

Conclusion: FinBERT2通过与当代LLMs对比分析，为LLMs时代有效利用金融BERT模型提供了实践路径，重新确立专用模型在领域应用中的价值。

Abstract: In natural language processing (NLP), the focus has shifted from encoder-only
tiny language models like BERT to decoder-only large language models(LLMs) such
as GPT-3. However, LLMs' practical application in the financial sector has
revealed three limitations: (1) LLMs often perform worse than fine-tuned BERT
on discriminative tasks despite costing much higher computational resources,
such as market sentiment analysis in financial reports; (2) Application on
generative tasks heavily relies on retrieval augmented generation (RAG) methods
to provide current and specialized information, with general retrievers showing
suboptimal performance on domain-specific retrieval tasks; (3) There are
additional inadequacies in other feature-based scenarios, such as topic
modeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained
on a high-quality, financial-specific corpus of 32b tokens. This represents the
largest known Chinese financial pretraining corpus for models of this parameter
size. As a better backbone, FinBERT2 can bridge the gap in the
financial-specific deployment of LLMs through the following achievements: (1)
Discriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT
variants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five
financial classification tasks. (2) Contrastive fine-tuned models
(Fin-Retrievers) outperform both open-source (e.g., +6.8\% avg improvement over
BGE-base-zh) and proprietary (e.g., +4.2\% avg improvement over OpenAI's
text-embedding-3-large) embedders across five financial retrieval tasks; (3)
Building on FinBERT2 variants, we construct the Fin-TopicModel, which enables
superior clustering and topic representation for financial titles. Our work
revisits financial BERT models through comparative analysis with contemporary
LLMs and offers practical insights for effectively utilizing FinBERT in the
LLMs era.

</details>


### [177] [Optimizing RAG Pipelines for Arabic: A Systematic Analysis of Core Components](https://arxiv.org/abs/2506.06339)
*Jumana Alsubhi,Mohammad D. Alahmadi,Ahmed Alhusayni,Ibrahim Aldailami,Israa Hamdine,Ahmad Shabana,Yazeed Iskandar,Suhayb Khayyat*

Main category: cs.IR

TL;DR: 系统评估阿拉伯语RAG组件性能，确定句子分块策略、BGE-M3嵌入模型、bge-reranker-v2-m3重排器和Aya-8B模型为最优组合方案


<details>
  <summary>Details</summary>
Motivation: 现有RAG研究多集中于高资源语言，阿拉伯语场景的组件优化缺乏深入探索

Method: 采用RAGAS框架，在多样化阿拉伯语数据集上对比分块策略/嵌入模型/重排器/语言模型等组件的四维指标（上下文精度/召回率、答案忠实度/相关性）

Result: 句子感知分块效果最佳，BGE-M3与Multilingual-E5-large嵌入最优，重排器提升复杂数据忠实度20%，Aya-8B生成质量超基准模型35%

Conclusion: 研究结果为构建阿拉伯语RAG系统提供组件选型指南，不同文档类型适配最优技术组合

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful architecture
for combining the precision of retrieval systems with the fluency of large
language models. While several studies have investigated RAG pipelines for
high-resource languages, the optimization of RAG components for Arabic remains
underexplored. This study presents a comprehensive empirical evaluation of
state-of-the-art RAG components-including chunking strategies, embedding
models, rerankers, and language models-across a diverse set of Arabic datasets.
Using the RAGAS framework, we systematically compare performance across four
core metrics: context precision, context recall, answer faithfulness, and
answer relevancy. Our experiments demonstrate that sentence-aware chunking
outperforms all other segmentation methods, while BGE-M3 and
Multilingual-E5-large emerge as the most effective embedding models. The
inclusion of a reranker (bge-reranker-v2-m3) significantly boosts faithfulness
in complex datasets, and Aya-8B surpasses StableLM in generation quality. These
findings provide critical insights for building high-quality Arabic RAG
pipelines and offer practical guidelines for selecting optimal components
across different document types.

</details>


### [178] [LlamaRec-LKG-RAG: A Single-Pass, Learnable Knowledge Graph-RAG Framework for LLM-Based Ranking](https://arxiv.org/abs/2506.07449)
*Vahid Azizi,Fatemeh Koochaki*

Main category: cs.IR

TL;DR: 提出LlamaRec-LKG-RAG框架，通过整合个性化知识图谱上下文到LLM推荐排序，显著提升推荐效果


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法依赖扁平化相似性检索，未能有效利用用户-物品交互中的关系结构特征

Method: 扩展LlamaRec架构，增加轻量级用户偏好模块动态识别异构知识图谱中的关键关系路径

Result: 在ML-100K和Amazon Beauty数据集上关键排序指标（MRR/NDCG/Recall）显著优于LlamaRec基准模型

Conclusion: 结构化推理对LLM推荐系统至关重要，为下一代可扩展的知识感知推荐系统奠定基础

Abstract: Recent advances in Large Language Models (LLMs) have driven their adoption in
recommender systems through Retrieval-Augmented Generation (RAG) frameworks.
However, existing RAG approaches predominantly rely on flat, similarity-based
retrieval that fails to leverage the rich relational structure inherent in
user-item interactions. We introduce LlamaRec-LKG-RAG, a novel single-pass,
end-to-end trainable framework that integrates personalized knowledge graph
context into LLM-based recommendation ranking. Our approach extends the
LlamaRec architecture by incorporating a lightweight user preference module
that dynamically identifies salient relation paths within a heterogeneous
knowledge graph constructed from user behavior and item metadata. These
personalized subgraphs are seamlessly integrated into prompts for a fine-tuned
Llama-2 model, enabling efficient and interpretable recommendations through a
unified inference step. Comprehensive experiments on ML-100K and Amazon Beauty
datasets demonstrate consistent and significant improvements over LlamaRec
across key ranking metrics (MRR, NDCG, Recall). LlamaRec-LKG-RAG demonstrates
the critical value of structured reasoning in LLM-based recommendations and
establishes a foundation for scalable, knowledge-aware personalization in
next-generation recommender systems. Code is available
at~\href{https://github.com/VahidAz/LlamaRec-LKG-RAG}{repository}.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [179] [SMaRCSim: Maritime Robotics Simulation Modules](https://arxiv.org/abs/2506.07781)
*Mart Kartašev,David Dörner,Özer Özkahraman,Petter Ögren,Ivan Stenius,John Folkesson*

Main category: cs.RO

TL;DR: SMaRCSim是专为水下机器人开发的新型仿真套件，解决现有工具在算法训练、多域载具协同和任务规划集成方面的不足。


<details>
  <summary>Details</summary>
Motivation: 水下机器人真实环境测试成本高且现有仿真工具缺乏对学习算法开发、多域载具协同（水下/水面/空中）及与任务规划集成的支持，阻碍了创新功能的快速迭代。

Method: 开发SMaRCSim仿真套件，包含支持学习算法训练的水下载具仿真模块、多域载具协同仿真框架，以及对接实际任务规划的接口设计。

Result: 构建了可加速学习算法开发、支持跨域载具编队测试、且能与实际任务规划系统联调的完整仿真平台。

Conclusion: 该工具填补了水下领域仿真生态的关键缺口，为复杂水下系统创新提供了高效的虚拟验证环境。

Abstract: Developing new functionality for underwater robots and testing them in the
real world is time-consuming and resource-intensive. Simulation environments
allow for rapid testing before field deployment. However, existing tools lack
certain functionality for use cases in our project: i) developing
learning-based methods for underwater vehicles; ii) creating teams of
autonomous underwater, surface, and aerial vehicles; iii) integrating the
simulation with mission planning for field experiments. A holistic solution to
these problems presents great potential for bringing novel functionality into
the underwater domain. In this paper we present SMaRCSim, a set of simulation
packages that we have developed to help us address these issues.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [180] [The Hype Index: an NLP-driven Measure of Market News Attention](https://arxiv.org/abs/2506.06329)
*Zheng Cao,Wanchaloem Wunkaew,Helyette Geman*

Main category: q-fin.ST

TL;DR: 提出Hype Index系列指标，通过NLP技术量化媒体对大型股票的关注度，为金融分析提供波动率分析和市场信号工具。


<details>
  <summary>Details</summary>
Motivation: 量化媒体关注对资本市场的影响，利用NLP技术从新闻中提取预测信号，弥补传统金融指标的不足。

Method: 基于标普100构建新闻计数型（媒体权重占比）和市值调整型（媒体权重/市值权重）双维度指标，通过分级检验、市场指标关联分析、信号预测能力验证等方法评估。

Result: Hype Index系列能有效识别股票波动、预测短期市场走势，在金融NLP应用场景展现显著价值。

Conclusion: 该指标体系为投资者提供了媒体情绪量化工具，拓展了NLP在金融领域的应用边界，对市场分析和风险管理具有实践意义。

Abstract: This paper introduces the Hype Index as a novel metric to quantify media
attention toward large-cap equities, leveraging advances in Natural Language
Processing (NLP) for extracting predictive signals from financial news. Using
the S&P 100 as the focus universe, we first construct a News Count-Based Hype
Index, which measures relative media exposure by computing the share of news
articles referencing each stock or sector. We then extend it to the
Capitalization Adjusted Hype Index, adjusts for economic size by taking the
ratio of a stock's or sector's media weight to its market capitalization weight
within its industry or sector. We compute both versions of the Hype Index at
the stock and sector levels, and evaluate them through multiple lenses: (1)
their classification into different hype groups, (2) their associations with
returns, volatility, and VIX index at various lags, (3) their signaling power
for short-term market movements, and (4) their empirical properties including
correlations, samplings, and trends. Our findings suggest that the Hype Index
family provides a valuable set of tools for stock volatility analysis, market
signaling, and NLP extensions in Finance.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [181] [On the Fundamental Impossibility of Hallucination Control in Large Language Models](https://arxiv.org/abs/2506.06382)
*Michał P. Karpowicz*

Main category: stat.ML

TL;DR: 本文通过形式化的不可能定理证明，大型语言模型无法同时满足真实性、语义守恒性、相关知识揭示和知识约束最优性四个基本属性，揭示了模型幻觉现象的本质必然性。


<details>
  <summary>Details</summary>
Motivation: 通过建立数学框架解释LLM产生幻觉的根本原因，为模型架构设计和评估提供理论依据

Method: 将LLM推理建模为神经组件竞争贡献的'思想拍卖'，运用Green-Laffont定理进行数学证明

Result: 构建了严格的数学框架证明四属性共存的不可能性，为理解推理机制本质奠定基础

Conclusion: 模型设计需在关键属性间进行权衡取舍，该理论框架对训练目标设定和评估体系构建具有指导意义

Abstract: This paper explains \textbf{why it is impossible to create large language
models that do not hallucinate and what are the trade-offs we should be looking
for}. It presents a formal \textbf{impossibility theorem} demonstrating that no
inference mechanism can simultaneously satisfy four fundamental properties:
\textbf{truthful (non-hallucinatory) generation, semantic information
conservation, relevant knowledge revelation, and knowledge-constrained
optimality}. By modeling LLM inference as an \textbf{auction of ideas} where
neural components compete to contribute to responses, we prove the
impossibility using the Green-Laffont theorem. That mathematical framework
provides a rigorous foundation for understanding the nature of inference
process, with implications for model architecture, training objectives, and
evaluation methods.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [182] [Contextual Experience Replay for Self-Improvement of Language Agents](https://arxiv.org/abs/2506.06698)
*Yitao Liu,Chenglei Si,Karthik Narasimhan,Shunyu Yao*

Main category: cs.AI

TL;DR: 提出Contextual Experience Replay (CER)框架，通过动态记忆缓冲区积累环境经验，使语言模型代理在复杂任务中实现51%的成功率提升


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理在复杂顺序决策任务中表现不足，主要缺乏环境特定经验和持续学习能力

Method: 构建动态记忆缓冲区积累环境动态和决策模式，实现经验检索与知识增强的免训练框架

Result: VisualWebArena达到31.9%成功率，WebArena获得36.7%成功率，相对GPT-4o基线提升51%

Conclusion: CER框架有效增强语言代理的自适应能力，验证了上下文经验重放机制在复杂环境中的有效性

Abstract: Large language model (LLM) agents have been applied to sequential
decision-making tasks such as web navigation, but without any
environment-specific experiences, they often fail in these complex tasks.
Moreover, current LLM agents are not designed to continually learn from past
experiences during inference time, which could be crucial for them to gain
these environment-specific experiences. To address this, we propose Contextual
Experience Replay (CER), a training-free framework to enable efficient
self-improvement for language agents in their context window. Specifically, CER
accumulates and synthesizes past experiences into a dynamic memory buffer.
These experiences encompass environment dynamics and common decision-making
patterns, allowing the agents to retrieve and augment themselves with relevant
knowledge in new tasks, enhancing their adaptability in complex environments.
We evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On
VisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena,
CER also gets a competitive average success rate of 36.7%, relatively improving
the success rate of the GPT-4o agent baseline by 51.0%. We also conduct a
comprehensive analysis on it to prove its efficiency, validity and understand
it better.

</details>


### [183] [Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures](https://arxiv.org/abs/2506.06832)
*Clément Hongler,Andrew Emil*

Main category: cs.AI

TL;DR: 提出交叉熵游戏（Xent Games）作为评估大语言模型能力的新框架，覆盖生成任务外的复杂认知评估场景。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成采样的评估方式不足以衡量LLM隐含的概率测度知识，需建立系统性能力评估框架以挖掘模型在摘要、反事实推理等复杂任务的潜力。

Method: 基于交叉熵指标构建游戏理论空间，通过一致性公理推导Xent Games形式体系，并设计覆盖性度量（covering measure）构建能力基准。

Result: 证明Xent Games空间具备足够丰富性，可形成包含单/多人模式、熵约束等维度的评估矩阵，并提出进化动力学启发的动态探索方案解决评估范围无限性问题。

Conclusion: 该框架为LLM能力评估提供了理论严谨且可扩展的工具，其游戏形式化方法对构建动态评估系统具有重要启示。

Abstract: Large Language Models (LLMs) define probability measures on text. By
considering the implicit knowledge question of what it means for an LLM to know
such a measure and what it entails algorithmically, we are naturally led to
formulate a series of tasks that go beyond generative sampling, involving forms
of summarization, counterfactual thinking, anomaly detection, originality
search, reverse prompting, debating, creative solving, etc. These tasks can be
formulated as games based on LLM measures, which we call Cross-Entropy (Xent)
Games. Xent Games can be single-player or multi-player. They involve
cross-entropy scores and cross-entropy constraints, and can be expressed as
simple computational graphs and programs. We show the Xent Game space is large
enough to contain a wealth of interesting examples, while being constructible
from basic game-theoretic consistency axioms. We then discuss how the Xent Game
space can be used to measure the abilities of LLMs. This leads to the
construction of Xent Game measures: finite families of Xent Games that can be
used as capability benchmarks, built from a given scope, by extracting a
covering measure. To address the unbounded scope problem associated with the
challenge of measuring general abilities, we propose to explore the space of
Xent Games in a coherent fashion, using ideas inspired by evolutionary
dynamics.

</details>


### [184] [Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering](https://arxiv.org/abs/2506.06905)
*Akash Gupta,Amos Storkey,Mirella Lapata*

Main category: cs.AI

TL;DR: 论文提出通过元学习方法蒸馏任务相关的图像特征生成软提示，有效提升小型多模态模型在少样本场景下的任务适应能力，实验显示该方法在视觉问答任务中稳定优于传统上下文学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有上下文学习(ICL)方法在小型多模态模型中表现不稳定，且易受任务无关图像特征干扰，导致性能无法随示例增加持续提升。

Method: 开发基于注意力映射模块的元学习框架，将任务相关图像特征蒸馏为可适应的软提示，并与LLaVA架构集成实现低数据条件下的快速梯度调整。

Result: 在VL-ICL基准测试中准确率显著提升(相对ICL方法+15.2%)，且在图像扰动下保持稳定，视觉推理任务成功率提高22.8%。

Conclusion: 该方法成功实现少样本场景下的高效任务适应，为多模态模型在低数据环境的实际应用提供了新范式。

Abstract: Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to
perform new tasks with minimal supervision. However, ICL performance,
especially in smaller LMMs, is inconsistent and does not always improve
monotonically with increasing examples. We hypothesize that this occurs due to
the LMM being overwhelmed by additional information present in the image
embeddings, which is not required for the downstream task. To address this, we
propose a meta-learning approach that provides an alternative for inducing
few-shot capabilities in LMMs, using a fixed set of soft prompts that are
distilled from task-relevant image features and can be adapted at test time
using a few examples. To facilitate this distillation, we introduce an
attention-mapper module that can be easily integrated with the popular LLaVA
v1.5 architecture and is jointly learned with soft prompts, enabling task
adaptation in LMMs under low-data regimes with just a few gradient steps.
Evaluation on the VL-ICL Bench shows that our method consistently outperforms
ICL and related prompt-tuning approaches, even under image perturbations,
improving task induction and reasoning across visual question answering tasks.

</details>


### [185] [The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity](https://arxiv.org/abs/2506.06941)
*Parshin Shojaee,Iman Mirzadeh,Keivan Alizadeh,Maxwell Horton,Samy Bengio,Mehrdad Farajtabar*

Main category: cs.AI

TL;DR: 大型推理模型(LRMs)在复杂任务中呈现准确率骤降和反直觉的扩展限制，其推理能力存在算法使用缺陷和跨尺度不一致性。


<details>
  <summary>Details</summary>
Motivation: 针对现有评估方法存在数据污染且无法分析推理过程的问题，系统研究LRMs的基本能力、扩展特性和局限性。

Method: 通过构建可控拼图环境，精准调节问题复杂度并保持逻辑结构一致性，同时分析模型内部推理轨迹。

Result: 发现LRMs存在临界复杂度崩溃现象，展示出先增强后衰减的反直觉推理努力曲线，识别出标准LLM与LRM在不同复杂度任务中的三个性能区间。

Conclusion: LRMs在精确计算和算法应用上存在根本性局限，需重新审视其推理能力的本质，未来研究应关注推理过程优化与算法整合。

Abstract: Recent generations of language models have introduced Large Reasoning Models
(LRMs) that generate detailed thinking processes before providing answers.
While these models demonstrate improved performance on reasoning benchmarks,
their fundamental capabilities, scaling properties, and limitations remain
insufficiently understood. Current evaluations primarily focus on established
math and coding benchmarks, emphasizing final answer accuracy. However, this
evaluation paradigm often suffers from contamination and does not provide
insights into the reasoning traces. In this work, we systematically investigate
these gaps with the help of controllable puzzle environments that allow precise
manipulation of complexity while maintaining consistent logical structures.
This setup enables the analysis of not only final answers but also the internal
reasoning traces, offering insights into how LRMs think. Through extensive
experiments, we show that LRMs face a complete accuracy collapse beyond certain
complexities. Moreover, they exhibit a counterintuitive scaling limit: their
reasoning effort increases with problem complexity up to a point, then declines
despite having remaining token budget. By comparing LRMs with their standard
LLM counterparts under same inference compute, we identify three performance
regimes: (1) low-complexity tasks where standard models outperform LRMs, (2)
medium-complexity tasks where LRMs demonstrates advantage, and (3)
high-complexity tasks where both models face complete collapse. We found that
LRMs have limitations in exact computation: they fail to use explicit
algorithms and reason inconsistently across scales. We also investigate the
reasoning traces in more depth, studying the patterns of explored solutions and
analyzing the models' computational behavior, shedding light on their
strengths, limitations, and raising questions about their reasoning
capabilities.

</details>


### [186] [Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images](https://arxiv.org/abs/2506.07184)
*Liangliang You,Junchi Yao,Shu Yang,Guimin Hu,Lijie Hu,Di Wang*

Main category: cs.AI

TL;DR: 提出SHE框架解决多模态模型中行为幻觉问题，引入BEACH评估指标，实验显示行为幻觉减少10%且保持准确性


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注客观幻觉，但序列图像中未被充分研究的行为幻觉由先验驱动偏见和雪球效应引发，需专门解决方案

Method: 两阶段轻量框架SHE：1）通过自适应时间窗口进行视觉-文本对齐检测；2）通过正交投影到联合嵌入空间缓解幻觉

Result: 在标准基准测试中，SHE将行为幻觉减少超10%（基于BEACH指标），同时维持描述准确性

Conclusion: SHE有效解决行为幻觉，BEACH指标为量化评估提供新标准，增强多模态模型的可靠性

Abstract: While multimodal large language models excel at various tasks, they still
suffer from hallucinations, which limit their reliability and scalability for
broader domain applications. To address this issue, recent research mainly
focuses on objective hallucination. However, for sequential images, besides
objective hallucination, there is also behavioral hallucination, which is less
studied. This work aims to fill in the gap. We first reveal that behavioral
hallucinations mainly arise from two key factors: prior-driven bias and the
snowball effect. Based on these observations, we introduce SHE (Sequence
Hallucination Eradication), a lightweight, two-stage framework that (1) detects
hallucinations via visual-textual alignment check using our proposed adaptive
temporal window and (2) mitigates them via orthogonal projection onto the joint
embedding space. We also propose a new metric (BEACH) to quantify behavioral
hallucination severity. Empirical results on standard benchmarks demonstrate
that SHE reduces behavioral hallucination by over 10% on BEACH while
maintaining descriptive accuracy.

</details>


### [187] [SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems](https://arxiv.org/abs/2506.07564)
*Peiran Li,Xinkai Zou,Zhuohang Wu,Ruifeng Li,Shuo Xing,Hanwen Zheng,Zhikai Hu,Yuping Wang,Haoxi Li,Qin Yuan,Yingmo Zhang,Zhengzhong Tu*

Main category: cs.AI

TL;DR: SAFEFLOW协议级框架通过细粒度信息流控制与事务机制，提升LLM/VLM代理的安全性与可靠性


<details>
  <summary>Details</summary>
Motivation: 现有代理框架在安全信息流控制、可靠性和多代理协调方面存在脆弱性，缺乏系统级保障机制

Method: 1. 细粒度信息流控制(IFC)追踪数据来源与安全属性
2. 事务执行与冲突解决保障多代理状态一致性
3. 预写日志/回滚/安全缓存增强容错能力
4. 开发SAFEFLOWBENCH基准测试套件

Result: 在对抗性/噪声/并发环境下，SAFEFLOW代理保持95%+任务成功率，安全性指标提升3倍以上

Conclusion: SAFEFLOW为构建可信赖的自主代理生态系统奠定理论基础，推动可靠自治系统发展

Abstract: Recent advances in large language models (LLMs) and vision-language models
(VLMs) have enabled powerful autonomous agents capable of complex reasoning and
multi-modal tool use. Despite their growing capabilities, today's agent
frameworks remain fragile, lacking principled mechanisms for secure information
flow, reliability, and multi-agent coordination. In this work, we introduce
SAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based
agents. SAFEFLOW enforces fine-grained information flow control (IFC),
precisely tracking provenance, integrity, and confidentiality of all the data
exchanged between agents, tools, users, and environments. By constraining LLM
reasoning to respect these security labels, SAFEFLOW prevents untrusted or
adversarial inputs from contaminating high-integrity decisions. To ensure
robustness in concurrent multi-agent settings, SAFEFLOW introduces
transactional execution, conflict resolution, and secure scheduling over shared
state, preserving global consistency across agents. We further introduce
mechanisms, including write-ahead logging, rollback, and secure caches, that
further enhance resilience against runtime errors and policy violations. To
validate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark
suite designed to evaluate agent reliability under adversarial, noisy, and
concurrent operational conditions. Extensive experiments demonstrate that
agents built with SAFEFLOW maintain impressive task performance and security
guarantees even in hostile environments, substantially outperforming
state-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for
principled, robust, and secure agent ecosystems, advancing the frontier of
reliable autonomy.

</details>


### [188] [Evaluating Large Language Models on the Frame and Symbol Grounding Problems: A Zero-shot Benchmark](https://arxiv.org/abs/2506.07896)
*Shoko Oka*

Main category: cs.AI

TL;DR: 研究发现部分现代大语言模型通过零样本测试，在框架问题和符号接地问题上展现出稳定解题潜力，闭源模型表现优于开源模型。


<details>
  <summary>Details</summary>
Motivation: 验证大语言模型是否具备解决传统符号AI长期无法突破的哲学难题（框架问题、符号接地问题）的认知能力，推动理论边界突破。

Method: 设计两个哲学核心问题对应的基准测试任务，对13个主流闭源/开源LLM进行零样本多轮测试，从上下文推理、语义连贯性、信息过滤等维度评估输出质量。

Result: 开源模型受参数量化/微调影响表现波动大，部分闭源模型（如GPT-4）在五次试验中保持高准确率，证明其具备稳定应对理论挑战的潜力。

Conclusion: 特定LLM已展现解决长期哲学难题的初步能力，这为AI理论发展提供了新方向，但需进一步验证泛化性与人类认知的相似度。

Abstract: Recent advancements in large language models (LLMs) have revitalized
philosophical debates surrounding artificial intelligence. Two of the most
fundamental challenges - namely, the Frame Problem and the Symbol Grounding
Problem - have historically been viewed as unsolvable within traditional
symbolic AI systems. This study investigates whether modern LLMs possess the
cognitive capacities required to address these problems. To do so, I designed
two benchmark tasks reflecting the philosophical core of each problem,
administered them under zero-shot conditions to 13 prominent LLMs (both closed
and open-source), and assessed the quality of the models' outputs across five
trials each. Responses were scored along multiple criteria, including
contextual reasoning, semantic coherence, and information filtering. The
results demonstrate that while open-source models showed variability in
performance due to differences in model size, quantization, and instruction
tuning, several closed models consistently achieved high scores. These findings
suggest that select modern LLMs may be acquiring capacities sufficient to
produce meaningful and stable responses to these long-standing theoretical
challenges.

</details>


### [189] [LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement](https://arxiv.org/abs/2506.07915)
*Dimitris Panagopoulos,Adolfo Perrusquia,Weisi Guo*

Main category: cs.AI

TL;DR: 提出LUCIFER框架，通过整合分层决策架构、强化学习和大型语言模型，利用人类上下文知识提升自主系统在动态环境中的决策效率和质量。


<details>
  <summary>Details</summary>
Motivation: 动态环境中现有环境知识快速过时，导致代理内部模型与现实的差距，需利用人类实时观察的上下文知识来提升决策有效性。

Method: 分层架构将复杂任务分解，LLM担任双角色：1）结构化提取人类输入的上下文信息（通过注意力空间机制影响决策） 2）零样本探索指导者（优化行动选择）。

Result: 实验表明LUCIFER在探索效率和决策质量上优于传统平面策略，验证上下文驱动决策的潜力。

Conclusion: 整合人类上下文知识与分层学习架构，有效提升自主系统在动态环境中的适应性，为智能体与人类协作决策提供新范式。

Abstract: In dynamic environments, the rapid obsolescence of pre-existing environmental
knowledge creates a gap between an agent's internal model and the evolving
reality of its operational context. This disparity between prior and updated
environmental valuations fundamentally limits the effectiveness of autonomous
decision-making. To bridge this gap, the contextual bias of human domain
stakeholders, who naturally accumulate insights through direct, real-time
observation, becomes indispensable. However, translating their nuanced, and
context-rich input into actionable intelligence for autonomous systems remains
an open challenge. To address this, we propose LUCIFER (Language Understanding
and Context-Infused Framework for Exploration and Behavior Refinement), a
domain-agnostic framework that integrates a hierarchical decision-making
architecture with reinforcement learning (RL) and large language models (LLMs)
into a unified system. This architecture mirrors how humans decompose complex
tasks, enabling a high-level planner to coordinate specialised sub-agents, each
focused on distinct objectives and temporally interdependent actions. Unlike
traditional applications where LLMs are limited to single role, LUCIFER
integrates them in two synergistic roles: as context extractors, structuring
verbal stakeholder input into domain-aware representations that influence
decision-making through an attention space mechanism aligning LLM-derived
insights with the agent's learning process, and as zero-shot exploration
facilitators guiding the agent's action selection process during exploration.
We benchmark various LLMs in both roles and demonstrate that LUCIFER improves
exploration efficiency and decision quality, outperforming flat,
goal-conditioned policies. Our findings show the potential of context-driven
decision-making, where autonomous systems leverage human contextual knowledge
for operational success.

</details>


### [190] [Solving Inequality Proofs with Large Language Models](https://arxiv.org/abs/2506.07927)
*Jiayi Sheng,Luna Lyu,Jikai Jin,Tony Xia,Alex Gu,James Zou,Pan Lu*

Main category: cs.AI

TL;DR: 论文提出IneqMath数据集及LLM评估框架，揭示顶级模型在不等式证明任务中准确率不足10%，暴露严格推理能力差距。


<details>
  <summary>Details</summary>
Motivation: 现有不等式数据集存在稀缺性、合成性、形式化局限，阻碍LLMs在复杂推理任务中的进展，需新型任务框架与评估体系。

Method: 1. 将不等式证明分解为边界估计+关系预测两个可验证子任务
2. 构建含逐步解答的奥赛级IneqMath数据集
3. 设计包含终答案+四类分步缺陷检测的LLM-as-judge评估框架

Result: 29个主流模型中，最佳模型分步准确率<10%（终答案准确率差距达65.5%），模型规模扩大仅有限提升，暴露LLMs推理链条的脆弱性。

Conclusion: 当前LLMs难以构建严格证明，需发展定理引导推理、自我优化等新范式，而非单纯依赖算力扩展。

Abstract: Inequality proving, crucial across diverse scientific and mathematical
fields, tests advanced reasoning skills such as discovering tight bounds and
strategic theorem application. This makes it a distinct, demanding frontier for
large language models (LLMs), offering insights beyond general mathematical
problem-solving. Progress in this area is hampered by existing datasets that
are often scarce, synthetic, or rigidly formal. We address this by proposing an
informal yet verifiable task formulation, recasting inequality proving into two
automatically checkable subtasks: bound estimation and relation prediction.
Building on this, we release IneqMath, an expert-curated dataset of
Olympiad-level inequalities, including a test set and training corpus enriched
with step-wise solutions and theorem annotations. We also develop a novel
LLM-as-judge evaluation framework, combining a final-answer judge with four
step-wise judges designed to detect common reasoning flaws. A systematic
evaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even
top models like o1 achieve less than 10% overall accuracy under step-wise
scrutiny; this is a drop of up to 65.5% from their accuracy considering only
final answer equivalence. This discrepancy exposes fragile deductive chains and
a critical gap for current LLMs between merely finding an answer and
constructing a rigorous proof. Scaling model size and increasing test-time
computation yield limited gains in overall proof correctness. Instead, our
findings highlight promising research directions such as theorem-guided
reasoning and self-refinement. Code and data are available at
https://ineqmath.github.io/.

</details>


### [191] [Reinforcing Multimodal Understanding and Generation with Dual Self-rewards](https://arxiv.org/abs/2506.07963)
*Jixiang Hong,Yiran Zhang,Guanzhong Wang,Yi Liu,Ji-Rong Wen,Rui Yan*

Main category: cs.AI

TL;DR: 提出自监督双奖励机制增强多模态模型的双向理解与生成能力，无需外部监督即可显著提升文本到图像任务表现


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型存在图文对齐不准确、无法兼顾理解与生成双向任务的问题，且现有方案依赖外部监督

Method: 基于理解与生成互为对偶任务的观察，通过反转输入输出对计算双重似然作为自监督奖励，优化模型参数

Result: 在视觉理解和生成基准测试中显著提升性能，文本到图像任务改进尤为突出

Conclusion: 自监督双奖励机制有效验证任务对偶性，为多模态模型的自监督优化开辟新路径

Abstract: Building upon large language models (LLMs), recent large multimodal models
(LMMs) unify cross-model understanding and generation into a single framework.
However, LMMs still struggle to achieve accurate image-text alignment, prone to
generating text responses contradicting the visual input or failing to follow
the text-to-image prompts. Current solutions require external supervision
(e.g., human feedback or reward models) and only address unidirectional
tasks-either understanding or generation. In this work, based on the
observation that understanding and generation are inverse dual tasks, we
introduce a self-supervised dual reward mechanism to reinforce the
understanding and generation capabilities of LMMs. Specifically, we sample
multiple outputs for a given input in one task domain, then reverse the
input-output pairs to compute the dual likelihood of the model as self-rewards
for optimization. Extensive experimental results on visual understanding and
generation benchmarks demonstrate that our method can effectively enhance the
performance of the model without any external supervision, especially achieving
remarkable improvements in text-to-image tasks.

</details>


### [192] [$τ^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment](https://arxiv.org/abs/2506.07982)
*Victor Barres,Honghua Dong,Soham Ray,Xujie Si,Karthik Narasimhan*

Main category: cs.AI

TL;DR: 提出τ²-bench双控制对话AI基准测试框架，解决现有单控制环境局限，强调用户与代理的协作协调能力评估。


<details>
  <summary>Details</summary>
Motivation: 现有对话AI基准仅模拟代理单方面使用工具的场景，而真实场景（如技术支持）需要用户主动参与共享环境操作，需新的评估框架。

Method: 1) 构建电信领域Dec-POMDP双控制模型 2) 开发组合式任务生成器 3) 设计状态约束的用户模拟器 4) 通过消融实验分离推理与协调错误

Result: 双控制环境下代理性能显著下降（相比单控制），突显用户引导时协调沟通的挑战性，沟通错误占比达总错误的35%

Conclusion: τ²-bench填补了需要代理同时具备推理和用户引导能力的评估空白，为复杂人机协作场景提供标准化测试平台

Abstract: Existing benchmarks for conversational AI agents simulate single-control
environments, where only the AI agent can use tools to interact with the world,
while the user remains a passive information provider. This differs from
real-world scenarios like technical support, where users need to actively
participate in modifying the state of the (shared) world. In order to address
this gap, we introduce $\tau^2$-bench, with four key contributions:
  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both
agent and user make use of tools to act in a shared, dynamic environment that
tests both agent coordination and communication,
  2) A compositional task generator that programmatically creates diverse,
verifiable tasks from atomic components, ensuring domain coverage and
controlled complexity,
  3) A reliable user simulator tightly coupled with the environment, whose
behavior is constrained by tools and observable states, improving simulation
fidelity,
  4) Fine-grained analysis of agent performance through multiple ablations
including separating errors arising from reasoning vs
communication/coordination.
  In particular, our experiments show significant performance drops when agents
shift from no-user to dual-control, highlighting the challenges of guiding
users. Overall, $\tau^2$-bench provides a controlled testbed for agents that
must both reason effectively and guide user actions.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [193] [ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication Protocols](https://arxiv.org/abs/2506.07945)
*Arnav Sheth,Ivaxi Sheth,Mario Fritz*

Main category: cs.AR

TL;DR: 首次系统评估大语言模型在生成SystemVerilog硬件描述语言时的能力，针对SPI/I2C/UART/AXI四种常用协议建立首个基准测试套件，通过代码生成、仿真验证等维度验证模型生成代码的语法正确性、可综合性和功能准确性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用编程语言代码生成中表现优异，但在需要严格遵循时序语义、并发性和可综合性的硬件描述语言（如SystemVerilog）领域应用潜力尚未被充分挖掘。硬件设计流程还涉及测试平台开发、时序收敛等复杂任务。

Method: 建立包含四种常用协议的基准套件，定义不同抽象层次的设计任务（从模块级到协议级）。通过语法检查、综合工具验证、波形仿真和测试平台进行三维度评估（语法正确性、可综合性和功能保真度）。

Result: 提出首个面向通信协议的HDL基准测试框架，实证评估现有LLM在硬件代码生成任务中的能力边界（具体评估数据需参考论文正文）。

Conclusion: 揭示了LLM在硬件设计自动化中的应用潜力与现存挑战，为后续研究提供了系统性评估框架，强调需要提升模型对HDL特定约束（时序、并发性）的理解能力。

Abstract: Recent advances in Large Language Models (LLMs) have shown promising
capabilities in generating code for general-purpose programming languages. In
contrast, their applicability for hardware description languages, particularly
for generating synthesizable and functionally correct designs, remains
significantly underexplored. HDLs such as SystemVerilog are logic-oriented and
demand strict adherence to timing semantics, concurrency, and synthesizability
constraints. Moreover, HDL-based design flows encompass a broad set of tasks
beyond structural code generation, including testbench development,
assertion-based verification, timing closure, and protocol-level integration
for on-chip communication. The objective of our paper is to analyze the
capabilities of state-of-the-art LLMs in generating SystemVerilog
implementations of standard communication protocols, a core component of
embedded and System-on-Chip (SoC) architectures. This paper introduces the
first benchmark suite targeting four widely used protocols: SPI, I2C, UART, and
AXI. We define code generation tasks that capture varying levels of design
abstraction and prompt specificity. The generated designs are assessed for
syntactic correctness, synthesizability, and functional fidelity via waveform
simulation and test benches.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [194] [Mitigating Object Hallucination via Robust Local Perception Search](https://arxiv.org/abs/2506.06729)
*Zixian Gao,Chao Yang,Zhanhui Zhou,Xing Xu,Chaochao Lu*

Main category: cs.CV

TL;DR: 提出局部感知搜索(LPS)解码方法，通过局部视觉先验抑制多模态大语言模型的幻觉现象，在噪声场景表现突出


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs存在输出与图像内容不匹配的幻觉问题，尤其在噪声图像场景下更为明显

Method: 基于局部视觉先验构建价值函数，无需训练即可在推理阶段调整解码过程，具备模型兼容性

Result: 在幻觉基准测试中显著降低幻觉率，噪声数据场景下表现优于基线模型

Conclusion: LPS作为即插即用方案，通过局部视觉校正有效抑制幻觉，特别适用于高噪声环境

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have enabled
them to effectively integrate vision and language, addressing a variety of
downstream tasks. However, despite their significant success, these models
still exhibit hallucination phenomena, where the outputs appear plausible but
do not align with the content of the images. To mitigate this issue, we
introduce Local Perception Search (LPS), a decoding method during inference
that is both simple and training-free, yet effectively suppresses
hallucinations. This method leverages local visual prior information as a value
function to correct the decoding process. Additionally, we observe that the
impact of the local visual prior on model performance is more pronounced in
scenarios with high levels of image noise. Notably, LPS is a plug-and-play
approach that is compatible with various models. Extensive experiments on
widely used hallucination benchmarks and noisy data demonstrate that LPS
significantly reduces the incidence of hallucinations compared to the baseline,
showing exceptional performance, particularly in noisy settings.

</details>


### [195] [Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs](https://arxiv.org/abs/2506.07045)
*Yikun Ji,Hong Yan,Jun Lan,Huijia Zhu,Weiqiang Wang,Qi Fan,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: 提出通过构建标注数据集和多阶段优化策略，显著提升多模态大语言模型在AI生成图像检测与视觉伪影定位中的性能表现


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测方法缺乏可解释性，多模态大语言模型存在视觉推理与人类认知不对齐的缺陷

Method: 构建带边界框和描述标注的数据集，采用多阶段优化策略平衡检测精度、视觉定位和文本解释能力

Result: 优化后的模型在AI生成图像检测和视觉缺陷定位任务中显著超越基线方法

Conclusion: 通过数据驱动的视觉-文本对齐方法和渐进式优化策略，有效提升模型的可解释检测能力

Abstract: The rapid advancement of image generation technologies intensifies the demand
for interpretable and robust detection methods. Although existing approaches
often attain high accuracy, they typically operate as black boxes without
providing human-understandable justifications. Multi-modal Large Language
Models (MLLMs), while not originally intended for forgery detection, exhibit
strong analytical and reasoning capabilities. When properly fine-tuned, they
can effectively identify AI-generated images and offer meaningful explanations.
However, existing MLLMs still struggle with hallucination and often fail to
align their visual interpretations with actual image content and human
reasoning. To bridge this gap, we construct a dataset of AI-generated images
annotated with bounding boxes and descriptive captions that highlight synthesis
artifacts, establishing a foundation for human-aligned visual-textual grounded
reasoning. We then finetune MLLMs through a multi-stage optimization strategy
that progressively balances the objectives of accurate detection, visual
localization, and coherent textual explanation. The resulting model achieves
superior performance in both detecting AI-generated images and localizing
visual flaws, significantly outperforming baseline methods.

</details>


### [196] [Learning Compact Vision Tokens for Efficient Large Multimodal Models](https://arxiv.org/abs/2506.07138)
*Hao Tang,Chengchao Shen*

Main category: cs.CV

TL;DR: 提出空间令牌融合（STF）和多块令牌融合（MBTF）方法，在减少75%视觉令牌的同时保持多模态推理性能


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型（LMMs）面临LLM的高计算成本和长视觉令牌序列的二次复杂度问题，需提升推理效率同时保持性能

Method: STF融合空间相邻令牌压缩序列，MBTF补充多粒度特征，两者结合平衡令牌压缩与信息保留

Result: 在8个主流视觉-语言基准测试中，仅使用基线25%的视觉令牌即达到可比/更优性能

Conclusion: 该方法通过双模块协同实现计算效率与模型性能的优化平衡，为多模态模型压缩提供有效解决方案

Abstract: Large multimodal models (LMMs) suffer significant computational challenges
due to the high cost of Large Language Models (LLMs) and the quadratic
complexity of processing long vision token sequences. In this paper, we explore
the spatial redundancy among vision tokens and shorten the length of vision
token sequences for inference acceleration. Specifically, we propose a Spatial
Token Fusion (STF) method to learn compact vision tokens for short vision token
sequence, where spatial-adjacent tokens are fused into one. Meanwhile,
weight-frozen vision encoder can not well adapt to the demand of extensive
downstream vision-language tasks. To this end, we further introduce a
Multi-Block Token Fusion (MBTF) module to supplement multi-granularity features
for the reduced token sequence. Overall, we combine STF and MBTF module to
balance token reduction and information preservation, thereby improving
inference efficiency without sacrificing multimodal reasoning capabilities.
Experimental results demonstrate that our method based on LLaVA-1.5 achieves
comparable or even superior performance to the baseline on 8 popular
vision-language benchmarks with only $25\%$ vision tokens of baseline. The
source code and trained weights are available at
https://github.com/visresearch/LLaVA-STF.

</details>


### [197] [SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning](https://arxiv.org/abs/2506.07196)
*Mengya Xu,Zhongzhen Huang,Dillan Imans,Yiru Ye,Xiaofan Zhang,Qi Dou*

Main category: cs.CV

TL;DR: SAP-Bench数据集填补外科手术规划评估空白，测试7个先进MLLM模型发现显著性能差距


<details>
  <summary>Details</summary>
Motivation: 现有基准无法有效评估外科决策所需的原子动作识别和长流程协调能力，而手术领域对可靠性要求极高

Method: 构建包含74个胆囊切除手术、1,226个临床验证动作片段的大规模SAP-Bench数据集，提出结合领域知识的MLLM-SAP框架

Result: 测试OpenAI-o1/GPT-4o等7个模型显示，在下一动作预测任务中存在显著性能缺陷（平均手术时长1137.5秒，动作片段平均68.7秒）

Conclusion: 该数据集为MLLM在外科规划中的可靠评估提供新基准，揭示当前模型在生命关键领域应用的局限性

Abstract: Effective evaluation is critical for driving advancements in MLLM research.
The surgical action planning (SAP) task, which aims to generate future action
sequences from visual inputs, demands precise and sophisticated analytical
capabilities. Unlike mathematical reasoning, surgical decision-making operates
in life-critical domains and requires meticulous, verifiable processes to
ensure reliability and patient safety. This task demands the ability to
distinguish between atomic visual actions and coordinate complex, long-horizon
procedures, capabilities that are inadequately evaluated by current benchmarks.
To address this gap, we introduce SAP-Bench, a large-scale, high-quality
dataset designed to enable multimodal large language models (MLLMs) to perform
interpretable surgical action planning. Our SAP-Bench benchmark, derived from
the cholecystectomy procedures context with the mean duration of 1137.5s, and
introduces temporally-grounded surgical action annotations, comprising the
1,226 clinically validated action clips (mean duration: 68.7s) capturing five
fundamental surgical actions across 74 procedures. The dataset provides 1,152
strategically sampled current frames, each paired with the corresponding next
action as multimodal analysis anchors. We propose the MLLM-SAP framework that
leverages MLLMs to generate next action recommendations from the current
surgical scene and natural language instructions, enhanced with injected
surgical domain knowledge. To assess our dataset's effectiveness and the
broader capabilities of current models, we evaluate seven state-of-the-art
MLLMs (e.g., OpenAI-o1, GPT-4o, QwenVL2.5-72B, Claude-3.5-Sonnet, GeminiPro2.5,
Step-1o, and GLM-4v) and reveal critical gaps in next action prediction
performance.

</details>


### [198] [Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning](https://arxiv.org/abs/2506.07227)
*Tianyi Bai,Yuxuan Fan,Jiantao Qiu,Fupeng Sun,Jiayi Song,Junlin Han,Zichen Liu,Conghui He,Wentao Zhang,Binhang Yuan*

Main category: cs.CV

TL;DR: 通过构建MED数据集和特征级一致性损失框架，显著提升多模态大模型在细粒度视觉差异检测和标准视觉任务中的表现


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs因训练数据和学习目标限制，难以处理属性/数量/位置等细微视觉变化，导致幻觉和语义偏移遗漏

Method: 1. 构建含50K+图像对的MED数据集（覆盖11类细粒度编辑）
2. 设计带特征级一致性损失的监督微调框架

Result: 在Micro Edit Detection基准上超越GPT-4o等基线，检测准确率提升+幻觉减少，图像描述/VQA任务表现同步提升

Conclusion: 验证了定向数据生成与特征对齐目标相结合的策略对增强MLLMs细粒度视觉推理能力的有效性

Abstract: Multimodal large language models (MLLMs) have achieved strong performance on
vision-language tasks but still struggle with fine-grained visual differences,
leading to hallucinations or missed semantic shifts. We attribute this to
limitations in both training data and learning objectives. To address these
issues, we propose a controlled data generation pipeline that produces
minimally edited image pairs with semantically aligned captions. Using this
pipeline, we construct the Micro Edit Dataset (MED), containing over 50K
image-text pairs spanning 11 fine-grained edit categories, including attribute,
count, position, and object presence changes. Building on MED, we introduce a
supervised fine-tuning (SFT) framework with a feature-level consistency loss
that promotes stable visual embeddings under small edits. We evaluate our
approach on the Micro Edit Detection benchmark, which includes carefully
balanced evaluation pairs designed to test sensitivity to subtle visual
variations across the same edit categories. Our method improves difference
detection accuracy and reduces hallucinations compared to strong baselines,
including GPT-4o. Moreover, it yields consistent gains on standard
vision-language tasks such as image captioning and visual question answering.
These results demonstrate the effectiveness of combining targeted data and
alignment objectives for enhancing fine-grained visual reasoning in MLLMs.

</details>


### [199] [Multi-Step Visual Reasoning with Visual Tokens Scaling and Verification](https://arxiv.org/abs/2506.07235)
*Tianyi Bai,Zengjie Hu,Fupeng Sun,Jiantao Qiu,Yizhen Jiang,Guangxin He,Bohan Zeng,Conghui He,Binhang Yuan,Wentao Zhang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Multi-modal large language models (MLLMs) have achieved remarkable
capabilities by integrating visual perception with language understanding,
enabling applications such as image-grounded dialogue, visual question
answering, and scientific analysis. However, most MLLMs adopt a static
inference paradigm, encoding the entire image into fixed visual tokens upfront,
which limits their ability to iteratively refine understanding or adapt to
context during inference. This contrasts sharply with human perception, which
is dynamic, selective, and feedback-driven. In this work, we introduce a novel
framework for inference-time visual token scaling that enables MLLMs to perform
iterative, verifier-guided reasoning over visual content. We formulate the
problem as a Markov Decision Process, involving a reasoner that proposes visual
actions and a verifier, which is trained via multi-step Direct Preference
Optimization (DPO), that evaluates these actions and determines when reasoning
should terminate. To support this, we present a new dataset, VTS, comprising
supervised reasoning trajectories (VTS-SFT) and preference-labeled reasoning
comparisons (VTS-DPO). Our method significantly outperforms existing approaches
across diverse visual reasoning benchmarks, offering not only improved accuracy
but also more interpretable and grounded reasoning processes. These results
demonstrate the promise of dynamic inference mechanisms for enabling
fine-grained, context-aware visual reasoning in next-generation MLLMs.

</details>


### [200] [GLOS: Sign Language Generation with Temporally Aligned Gloss-Level Conditioning](https://arxiv.org/abs/2506.07460)
*Taeryung Lee,Hyeongjin Nam,Gyeongsik Moon,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: 提出GLOS框架，通过时间对齐的gloss级条件和TAC模块，提升手语生成的词汇顺序和语义准确性


<details>
  <summary>Details</summary>
Motivation: 现有手语生成方法采用句子级条件编码，无法捕捉手语时间结构并缺乏词级语义粒度，导致手势序列混乱和动作模糊

Method: 1. 采用与动作序列时间对齐的gloss级条件（词级语义嵌入序列）
2. 提出时间对齐条件融合模块（TAC）实现细粒度语义控制

Result: 在CSL-Daily和Phoenix-2014T数据集上超越现有方法，生成词汇顺序正确且语义准确的手势

Conclusion: 通过gloss级条件的时间对齐和TAC模块，实现了对手势序列的细粒度控制，有效解决了传统方法存在的词汇顺序混乱和语义模糊问题

Abstract: Sign language generation (SLG), or text-to-sign generation, bridges the gap
between signers and non-signers. Despite recent progress in SLG, existing
methods still often suffer from incorrect lexical ordering and low semantic
accuracy. This is primarily due to sentence-level condition, which encodes the
entire sentence of the input text into a single feature vector as a condition
for SLG. This approach fails to capture the temporal structure of sign language
and lacks the granularity of word-level semantics, often leading to disordered
sign sequences and ambiguous motions. To overcome these limitations, we propose
GLOS, a sign language generation framework with temporally aligned gloss-level
conditioning. First, we employ gloss-level conditions, which we define as
sequences of gloss embeddings temporally aligned with the motion sequence. This
enables the model to access both the temporal structure of sign language and
word-level semantics at each timestep. As a result, this allows for
fine-grained control of signs and better preservation of lexical order. Second,
we introduce a condition fusion module, temporal alignment conditioning (TAC),
to efficiently deliver the word-level semantic and temporal structure provided
by the gloss-level condition to the corresponding motion timesteps. Our method,
which is composed of gloss-level conditions and TAC, generates signs with
correct lexical order and high semantic accuracy, outperforming prior methods
on CSL-Daily and Phoenix-2014T.

</details>


### [201] [Learning Speaker-Invariant Visual Features for Lipreading](https://arxiv.org/abs/2506.07572)
*Yu Li,Feng Xue,Shujie Li,Jinrui Zhang,Shuang Yang,Dan Guo,Richang Hong*

Main category: cs.CV

TL;DR: 提出SIFLip框架，通过解耦说话人特征提升唇语识别的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有唇语识别方法提取的视觉特征包含说话人特异性属性（唇形/颜色/纹理），导致视觉与文本的伪相关性，影响准确率和模型泛化性

Method: 双模块解耦框架：1）隐式解耦利用文本嵌入监督学习跨说话人通用表征；2）显式解耦通过说话人识别子任务过滤个性化特征，结合梯度反转实现特征分离

Result: 在多个公开数据集上显著超越现有方法，验证了模型泛化性能的提升

Conclusion: 通过隐式语义对齐和显式特征分离的协同作用，有效消除说话人特异性干扰，推动跨说话人唇语识别发展

Abstract: Lipreading is a challenging cross-modal task that aims to convert visual lip
movements into spoken text. Existing lipreading methods often extract visual
features that include speaker-specific lip attributes (e.g., shape, color,
texture), which introduce spurious correlations between vision and text. These
correlations lead to suboptimal lipreading accuracy and restrict model
generalization. To address this challenge, we introduce SIFLip, a
speaker-invariant visual feature learning framework that disentangles
speaker-specific attributes using two complementary disentanglement modules
(Implicit Disentanglement and Explicit Disentanglement) to improve
generalization. Specifically, since different speakers exhibit semantic
consistency between lip movements and phonetic text when pronouncing the same
words, our implicit disentanglement module leverages stable text embeddings as
supervisory signals to learn common visual representations across speakers,
implicitly decoupling speaker-specific features. Additionally, we design a
speaker recognition sub-task within the main lipreading pipeline to filter
speaker-specific features, then further explicitly disentangle these
personalized visual features from the backbone network via gradient reversal.
Experimental results demonstrate that SIFLip significantly enhances
generalization performance across multiple public datasets. Experimental
results demonstrate that SIFLip significantly improves generalization
performance across multiple public datasets, outperforming state-of-the-art
methods.

</details>


### [202] [Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models](https://arxiv.org/abs/2506.07936)
*Chengyue Huang,Yuchen Zhu,Sichen Zhu,Jingyun Xiao,Moises Andrade,Shivang Chopra,Zsolt Kira*

Main category: cs.CV

TL;DR: 研究发现当前视觉语言模型(VLMs)在多模态上下文学习中更多依赖浅层启发式策略(如答案复制)而非真正任务理解，性能对演示样本数量/质量不敏感。


<details>
  <summary>Details</summary>
Motivation: 质疑现有视觉语言模型是否真正具备多模态上下文学习能力(MM-ICL)，发现其可能依赖浅层启发式策略而非深入理解任务。

Method: 提出MM-ICL with Reasoning方法，为每个演示样本生成解释性原理。在感知与推理任务数据集上测试3B-72B开源模型及Gemini 2.0，控制变量包括样本数量、检索方法、原理质量等。

Result: 模型性能随演示样本增加反而下降，倾向于复制答案而非学习模式；性能对原理质量、检索方式等关键因素敏感性有限。

Conclusion: 当前VLMs无法有效利用演示样本信息，质疑其真实任务理解能力，揭示现有模型架构在实现真正上下文学习方面的局限性。

Abstract: Vision-language models (VLMs) are widely assumed to exhibit in-context
learning (ICL), a property similar to that of their language-only counterparts.
While recent work suggests VLMs can perform multimodal ICL (MM-ICL), studies
show they often rely on shallow heuristics -- such as copying or majority
voting -- rather than true task understanding. We revisit this assumption by
evaluating VLMs under distribution shifts, where support examples come from a
dataset different from the query. Surprisingly, performance often degrades with
more demonstrations, and models tend to copy answers rather than learn from
them. To investigate further, we propose a new MM-ICL with Reasoning pipeline
that augments each demonstration with a generated rationale alongside the
answer. We conduct extensive and comprehensive experiments on both perception-
and reasoning-required datasets with open-source VLMs ranging from 3B to 72B
and proprietary models such as Gemini 2.0. We conduct controlled studies
varying shot count, retrieval method, rationale quality, and distribution. Our
results show limited performance sensitivity across these factors, suggesting
that current VLMs do not effectively utilize demonstration-level information as
intended in MM-ICL.

</details>


### [203] [Play to Generalize: Learning to Reason Through Game Play](https://arxiv.org/abs/2506.08011)
*Yunfei Xie,Yinsong Ma,Shiyi Lan,Alan Yuille,Junfei Xiao,Chen Wei*

Main category: cs.CV

TL;DR: ViGaL通过游戏化后训练范式显著提升多模态大语言模型的跨领域推理能力，在数学和跨学科任务中超越专家模型


<details>
  <summary>Details</summary>
Motivation: 受认知科学启发（游戏促进可迁移认知技能），探索通过街机游戏训练来开发MLLMs的泛化推理能力

Method: 使用强化学习对7B参数的MLLM进行后训练，训练数据为贪吃蛇等简单规则游戏的合成游戏场景

Result: 在MathVista数学推理基准提升显著，MMMU跨学科问题表现优于专门调整的专家模型，同时保留基础模型的视觉能力

Conclusion: 基于规则的合成游戏可作为可控、可扩展的预训练任务，有效解锁MLLMs的通用多模态推理能力

Abstract: Developing generalizable reasoning capabilities in multimodal large language
models (MLLMs) remains challenging. Motivated by cognitive science literature
suggesting that gameplay promotes transferable cognitive skills, we propose a
novel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs
develop out-of-domain generalization of multimodal reasoning through playing
arcade-like games. Specifically, we show that post-training a 7B-parameter MLLM
via reinforcement learning (RL) on simple arcade-like games, e.g. Snake,
significantly enhances its downstream performance on multimodal math benchmarks
like MathVista, and on multi-discipline questions like MMMU, without seeing any
worked solutions, equations, or diagrams during RL, suggesting the capture of
transferable reasoning skills. Remarkably, our model outperforms specialist
models tuned on multimodal reasoning data in multimodal reasoning benchmarks,
while preserving the base model's performance on general visual benchmarks, a
challenge where specialist models often fall short. Our findings suggest a new
post-training paradigm: synthetic, rule-based games can serve as controllable
and scalable pre-text tasks that unlock generalizable multimodal reasoning
abilities in MLLMs.

</details>
