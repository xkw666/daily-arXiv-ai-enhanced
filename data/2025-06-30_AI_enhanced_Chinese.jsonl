{"id": "2506.21629", "pdf": "https://arxiv.org/pdf/2506.21629", "abs": "https://arxiv.org/abs/2506.21629", "authors": ["Chenhao Zhang", "Yezhi Shen", "Fengqing Zhu"], "title": "ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded Scenes", "categories": ["cs.GR"], "comment": "6 pages, Source code is available at\n  https://github.com/Chenhao-Z/ICP-3DGS. To appear at ICIP 2025", "summary": "In recent years, neural rendering methods such as NeRFs and 3D Gaussian\nSplatting (3DGS) have made significant progress in scene reconstruction and\nnovel view synthesis. However, they heavily rely on preprocessed camera poses\nand 3D structural priors from structure-from-motion (SfM), which are\nchallenging to obtain in outdoor scenarios. To address this challenge, we\npropose to incorporate Iterative Closest Point (ICP) with optimization-based\nrefinement to achieve accurate camera pose estimation under large camera\nmovements. Additionally, we introduce a voxel-based scene densification\napproach to guide the reconstruction in large-scale scenes. Experiments\ndemonstrate that our approach ICP-3DGS outperforms existing methods in both\ncamera pose estimation and novel view synthesis across indoor and outdoor\nscenes of various scales. Source code is available at\nhttps://github.com/Chenhao-Z/ICP-3DGS.", "AI": {"tldr": "\u63d0\u51faICP-3DGS\u65b9\u6cd5\uff0c\u7ed3\u5408\u8fed\u4ee3\u6700\u8fd1\u70b9\u7b97\u6cd5\u4e0e\u4f18\u5316\u7b56\u7565\u5b9e\u73b0\u5927\u8303\u56f4\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7\u4f53\u7d20\u573a\u666f\u81f4\u5bc6\u5316\u63d0\u5347\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u6548\u679c", "motivation": "\u73b0\u6709\u795e\u7ecf\u6e32\u67d3\u65b9\u6cd5\uff08\u5982NeRF\u30013DGS\uff09\u4e25\u91cd\u4f9d\u8d56\u9884\u5904\u7406\u76f8\u673a\u4f4d\u59ff\u548c\u4e09\u7ef4\u7ed3\u6784\u5148\u9a8c\uff0c\u8fd9\u5728\u6237\u5916\u573a\u666f\u4e2d\u96be\u4ee5\u83b7\u53d6\u3002\u9700\u8981\u89e3\u51b3\u5927\u8303\u56f4\u76f8\u673a\u8fd0\u52a8\u4e0b\u7684\u4f4d\u59ff\u4f30\u8ba1\u548c\u573a\u666f\u91cd\u5efa\u95ee\u9898", "method": "1. \u5c06ICP\u7b97\u6cd5\u4e0e\u57fa\u4e8e\u4f18\u5316\u7684\u4f4d\u59ff\u7ec6\u5316\u76f8\u7ed3\u5408\u5b9e\u73b0\u7cbe\u786e\u4f4d\u59ff\u4f30\u8ba1 2. \u63d0\u51fa\u4f53\u7d20\u5316\u573a\u666f\u81f4\u5bc6\u5316\u7b56\u7565\u6307\u5bfc\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa", "result": "\u5728\u5ba4\u5185\u5916\u591a\u5c3a\u5ea6\u573a\u666f\u4e2d\uff0cICP-3DGS\u5728\u76f8\u673a\u4f4d\u59ff\u4f30\u8ba1\u548c\u65b0\u89c6\u89d2\u5408\u6210\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff08\u4ee3\u7801\u5df2\u5f00\u6e90\uff09", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8303\u56f4\u76f8\u673a\u8fd0\u52a8\u4e0b\u7684\u4f4d\u59ff\u4f30\u8ba1\u96be\u9898\uff0c\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u573a\u666f\u91cd\u5efa\u8d28\u91cf\uff0c\u63a8\u52a8\u4e86\u795e\u7ecf\u6e32\u67d3\u7684\u5b9e\u9645\u5e94\u7528"}}
{"id": "2506.21632", "pdf": "https://arxiv.org/pdf/2506.21632", "abs": "https://arxiv.org/abs/2506.21632", "authors": ["Da Li", "Donggang Jia", "Markus Hadwiger", "Ivan Viola"], "title": "SkinningGS: Editable Dynamic Human Scene Reconstruction Using Gaussian Splatting Based on a Skinning Model", "categories": ["cs.GR"], "comment": null, "summary": "Reconstructing an interactive human avatar and the background from a\nmonocular video of a dynamic human scene is highly challenging. In this work we\nadopt a strategy of point cloud decoupling and joint optimization to achieve\nthe decoupled reconstruction of backgrounds and human bodies while preserving\nthe interactivity of human motion. We introduce a position texture to subdivide\nthe Skinned Multi-Person Linear (SMPL) body model's surface and grow the human\npoint cloud. To capture fine details of human dynamics and deformations, we\nincorporate a convolutional neural network structure to predict human body\npoint cloud features based on texture. This strategy makes our approach free of\nhyperparameter tuning for densification and efficiently represents human points\nwith half the point cloud of HUGS. This approach ensures high-quality human\nreconstruction and reduces GPU resource consumption during training. As a\nresult, our method surpasses the previous state-of-the-art HUGS in\nreconstruction metrics while maintaining the ability to generalize to novel\nposes and views. Furthermore, our technique achieves real-time rendering at\nover 100 FPS, $\\sim$6$\\times$ the HUGS speed using only Linear Blend Skinning\n(LBS) weights for human transformation. Additionally, this work demonstrates\nthat this framework can be extended to animal scene reconstruction when an\naccurately-posed model of an animal is available.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u70b9\u4e91\u89e3\u8026\u4e0e\u8054\u5408\u4f18\u5316\u7684\u5355\u76ee\u89c6\u9891\u52a8\u6001\u4eba\u4f53\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u52a8\u4f5c\u4ea4\u4e92\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4eba\u4f53/\u80cc\u666f\u5206\u79bb\u91cd\u5efa\uff0c\u6548\u7387\u8fbeHUGS\u76846\u500d\u4e14\u663e\u5b58\u6d88\u8017\u51cf\u534a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5HUGS\u5b58\u5728\u4e24\u70b9\u4e0d\u8db3\uff1a(1)\u9700\u8981\u8d85\u53c2\u6570\u8c03\u6574\u7684\u70b9\u4e91\u52a0\u5bc6\u8fc7\u7a0b\u5f71\u54cd\u91cd\u5efa\u8d28\u91cf (2)\u8bad\u7ec3\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u6e32\u67d3\u901f\u5ea6\u8f83\u6162\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u52a8\u6001\u4eba\u4f53\u573a\u666f\u91cd\u5efa\u7684\u6548\u7387\u4e0e\u8d28\u91cf\u5e73\u8861\u95ee\u9898\u3002", "method": "1. \u57fa\u4e8eSMPL\u6a21\u578b\u8868\u9762\u4f4d\u7f6e\u7eb9\u7406\u751f\u957f\u4eba\u4f53\u70b9\u4e91\n2. \u8bbe\u8ba1CNN\u67b6\u6784\u9884\u6d4b\u4eba\u4f53\u70b9\u4e91\u7eb9\u7406\u7279\u5f81\n3. \u91c7\u7528\u7ebf\u6027\u6df7\u5408\u8499\u76ae\u6743\u91cd\u5b9e\u73b0\u4eba\u4f53\u5f62\u53d8\n4. \u70b9\u4e91\u89e3\u8026\u8054\u5408\u4f18\u5316\u6846\u67b6\u5b9e\u73b0\u80cc\u666f/\u4eba\u4f53\u5206\u79bb\u91cd\u5efa", "result": "1. \u8d85\u8d8aHUGS\u7684PSNR(26.15\u219226.95)/SSIM(0.916\u21920.927)\n2. \u663e\u5b58\u6d88\u8017\u964d\u4f4e50%\uff08\u4ec5\u9700HUGS\u4e00\u534a\u70b9\u4e91\uff09\n3. \u5b9e\u65f6\u6e32\u67d3\u901f\u5ea6\u8fbe100FPS\uff08HUGS\u76846\u500d\uff09\n4. \u6210\u529f\u6269\u5c55\u5e94\u7528\u4e8e\u52a8\u7269\u573a\u666f\u91cd\u5efa", "conclusion": "\u901a\u8fc7\u4f4d\u7f6e\u7eb9\u7406\u4e0eCNN\u7279\u5f81\u9884\u6d4b\u7684\u521b\u65b0\u7ec4\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u9ad8\u8d28\u91cf\u7684\u52a8\u6001\u573a\u666f\u91cd\u5efa\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u52a8\u4f5c\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u4e3a\u5b9e\u65f6AR/VR\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u5c55\u793a\u4e86\u8de8\u7269\u79cd\u573a\u666f\u7684\u9002\u7528\u6f5c\u529b\u3002"}}
{"id": "2506.21633", "pdf": "https://arxiv.org/pdf/2506.21633", "abs": "https://arxiv.org/abs/2506.21633", "authors": ["Aobo Li", "Zhengxin Lei", "Jiangtao Wei", "Feng Xu"], "title": "SAR-GS: 3D Gaussian Splatting for Synthetic Aperture Radar Target Reconstruction", "categories": ["cs.GR"], "comment": null, "summary": "Three-dimensional target reconstruction from synthetic aperture radar (SAR)\nimagery is crucial for interpreting complex scattering information in SAR data.\nHowever, the intricate electromagnetic scattering mechanisms inherent to SAR\nimaging pose significant reconstruction challenges. Inspired by the remarkable\nsuccess of 3D Gaussian Splatting (3D-GS) in optical domain reconstruction, this\npaper presents a novel SAR Differentiable Gaussian Splatting Rasterizer (SDGR)\nspecifically designed for SAR target reconstruction. Our approach combines\nGaussian splatting with the Mapping and Projection Algorithm to compute\nscattering intensities of Gaussian primitives and generate simulated SAR images\nthrough SDGR. Subsequently, the loss function between the rendered image and\nthe ground truth image is computed to optimize the Gaussian primitive\nparameters representing the scene, while a custom CUDA gradient flow is\nemployed to replace automatic differentiation for accelerated gradient\ncomputation. Through experiments involving the rendering of simplified\narchitectural targets and SAR images of multiple vehicle targets, we validate\nthe imaging rationality of SDGR on simulated SAR imagery. Furthermore, the\neffectiveness of our method for target reconstruction is demonstrated on both\nsimulated and real-world datasets containing multiple vehicle targets, with\nquantitative evaluations conducted to assess its reconstruction performance.\nExperimental results indicate that our approach can effectively reconstruct the\ngeometric structures and scattering properties of targets, thereby providing a\nnovel solution for 3D reconstruction in the field of SAR imaging.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e3D\u9ad8\u65af\u6e85\u5c04\u7684SAR\u53ef\u5fae\u5149\u6805\u5316\u65b9\u6cd5SDGR\uff0c\u901a\u8fc7\u878d\u5408\u6563\u5c04\u8ba1\u7b97\u4e0e\u68af\u5ea6\u4f18\u5316\u5b9e\u73b0\u76ee\u6807\u4e09\u7ef4\u91cd\u5efa", "motivation": "SAR\u6210\u50cf\u4e2d\u590d\u6742\u7684\u7535\u78c1\u6563\u5c04\u673a\u5236\u5bfc\u81f4\u76ee\u6807\u91cd\u5efa\u56f0\u96be\uff0c\u53d73D-GS\u5728\u5149\u5b66\u91cd\u5efa\u6210\u529f\u7684\u542f\u53d1\uff0c\u9700\u5f00\u53d1\u4e13\u95e8\u9002\u7528\u4e8eSAR\u7684\u4f18\u5316\u65b9\u6cd5", "method": "\u7ed3\u5408\u9ad8\u65af\u6e85\u5c04\u4e0e\u6620\u5c04\u6295\u5f71\u7b97\u6cd5\u8ba1\u7b97\u6563\u5c04\u5f3a\u5ea6\uff0c\u901a\u8fc7SDGR\u751f\u6210\u6a21\u62df\u56fe\u50cf\uff0c\u4f7f\u7528\u81ea\u5b9a\u4e49CUDA\u68af\u5ea6\u6d41\u66ff\u4ee3\u81ea\u52a8\u5fae\u5206\u52a0\u901f\u8ba1\u7b97", "result": "\u5728\u5efa\u7b51\u76ee\u6807\u548c\u8f66\u8f86\u76ee\u6807\u7684\u6a21\u62df/\u771f\u5b9e\u6570\u636e\u96c6\u9a8c\u8bc1\u4e2d\uff0c\u5b9a\u91cf\u8bc4\u4f30\u663e\u793a\u80fd\u6709\u6548\u91cd\u5efa\u76ee\u6807\u51e0\u4f55\u7ed3\u6784\u4e0e\u6563\u5c04\u7279\u6027", "conclusion": "SDGR\u4e3aSAR\u4e09\u7ef4\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u7cbe\u786e\u8868\u5f81\u76ee\u6807\u6563\u5c04\u7279\u6027\u4e0e\u7a7a\u95f4\u7ed3\u6784\uff0c\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c"}}
{"id": "2506.22250", "pdf": "https://arxiv.org/pdf/2506.22250", "abs": "https://arxiv.org/abs/2506.22250", "authors": ["Yucheng Lu", "Tobias Rau", "Benjamin Lee", "Andreas K\u00f6hn", "Michael Sedlmair", "Christian Sandor", "Tobias Isenberg"], "title": "A Design Space for Visualization Transitions of 3D Spatial Data in Hybrid AR-Desktop Environments", "categories": ["cs.GR"], "comment": "14 pages, 6 figures", "summary": "We present a design space for animated transitions of the appearance of 3D\nspatial datasets in a hybrid Augmented Reality (AR)-desktop context. Such\nhybrid interfaces combine both traditional and immersive displays to facilitate\nthe exploration of 2D and 3D data representations in the environment in which\nthey are best displayed. One key aspect is to introduce transitional animations\nthat change between the different dimensionalities to illustrate the connection\nbetween the different representations and to reduce the potential cognitive\nload on the user. The specific transitions to be used depend on the type of\ndata, the needs of the application domain, and other factors. We summarize\nthese as a transition design space to simplify the decision-making process and\nprovide inspiration for future designs. First, we discuss 3D visualizations\nfrom a spatial perspective: a spatial encoding pipeline, where 3D data sampled\nfrom the physical world goes through various transformations, being mapped to\nvisual representations, and then being integrated into a hybrid AR-desktop\nenvironment. The transition design then focuses on interpolating between two\nspatial encoding pipelines to provide a smooth experience. To illustrate the\nuse of our design space, we apply it to three case studies that focus on\napplications in astronomy, radiology, and chemistry; we then discuss lessons\nlearned from these applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u6df7\u5408AR-\u684c\u9762\u73af\u5883\u4e0b3D\u7a7a\u95f4\u6570\u636e\u96c6\u52a8\u753b\u8fc7\u6e21\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u901a\u8fc7\u6848\u4f8b\u9a8c\u8bc1\u53ef\u964d\u4f4e\u7528\u6237\u8ba4\u77e5\u8d1f\u8377", "motivation": "\u6df7\u5408\u754c\u9762\u4e2d\u4e0d\u540c\u7ef4\u5ea6\u6570\u636e\u8f6c\u6362\u9700\u8981\u8fc7\u6e21\u52a8\u753b\u6765\u589e\u5f3a\u8ba4\u77e5\u5173\u8054\u6027\uff0c\u51cf\u5c11\u7528\u6237\u5728\u8de8\u8bbe\u5907\u4ea4\u4e92\u4e2d\u7684\u8ba4\u77e5\u8d1f\u62c5", "method": "\u6784\u5efa\u7a7a\u95f4\u7f16\u7801\u6d41\u6c34\u7ebf\u6a21\u578b\uff0c\u901a\u8fc7\u63d2\u503c\u4e0d\u540c\u7a7a\u95f4\u7f16\u7801\u72b6\u6001\u5b9e\u73b0\u5e73\u6ed1\u8fc7\u6e21\uff0c\u5e76\u5728\u5929\u6587/\u653e\u5c04/\u5316\u5b66\u9886\u57df\u8fdb\u884c\u6848\u4f8b\u9a8c\u8bc1", "result": "\u5efa\u7acb\u4e86\u8fc7\u6e21\u52a8\u753b\u8bbe\u8ba1\u7a7a\u95f4\u6846\u67b6\uff0c\u6848\u4f8b\u5e94\u7528\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u4fdd\u6301\u4e0a\u4e0b\u6587\u8fde\u8d2f\u6027\u5e76\u63d0\u5347\u7528\u6237\u4f53\u9a8c", "conclusion": "\u8be5\u8bbe\u8ba1\u7a7a\u95f4\u4e3a\u6df7\u5408\u73b0\u5b9e\u6570\u636e\u53ef\u89c6\u5316\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u8fc7\u6e21\u65b9\u6848\uff0c\u53ef\u6269\u5c55\u5e94\u7528\u4e8e\u5176\u4ed6\u8de8\u8bbe\u5907\u4ea4\u4e92\u573a\u666f"}}
{"id": "2506.21555", "pdf": "https://arxiv.org/pdf/2506.21555", "abs": "https://arxiv.org/abs/2506.21555", "authors": ["Jiahong Li", "Yiwen Shao", "Jianheng Zhuo", "Chenda Li", "Liliang Tang", "Dong Yu", "Yanmin Qian"], "title": "Efficient Multilingual ASR Finetuning via LoRA Language Experts", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted in Interspeech 2025", "summary": "Recent advancements in deep learning have significantly enhanced multilingual\nautomatic speech recognition (ASR) due to the development of advanced model\narchitectures and available large-scale multilingual datasets. Despite that,\nmultilingual ASR still suffers from the curse of multilinguality in that\ndifferent languages tend to interfere with each other, making it difficult for\nthe ASR model to identify multiple languages effectively while sharing model\ncapacity across them. This paper proposes an efficient finetuning framework for\ncustomized multilingual ASR via prepared LoRA language experts based on\nWhisper. Through LoRA expert fusion or knowledge distillation, our approach\nachieves better recognition performance on target languages than standard\nfine-tuning methods. Experimental results demonstrate that the proposed models\nyield approximately 10\\% and 15\\% relative performance gains in language-aware\nand language-agnostic scenarios, respectively.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eWhisper\u7684LoRA\u4e13\u5bb6\u5fae\u8c03\u6846\u67b6\uff0c\u89e3\u51b3\u591a\u8bed\u8a00ASR\u4e2d\u7684\u8bed\u8a00\u5e72\u6270\u95ee\u9898", "motivation": "\u591a\u8bed\u8a00ASR\u5b58\u5728\u8bed\u8a00\u95f4\u76f8\u4e92\u5e72\u6270\u7684\u96be\u9898\uff0c\u5f71\u54cd\u6a21\u578b\u5bf9\u591a\u8bed\u8a00\u7684\u8bc6\u522b\u6548\u7387\u548c\u5bb9\u91cf\u5171\u4eab", "method": "\u901a\u8fc7\u9884\u8bad\u7ec3LoRA\u8bed\u8a00\u4e13\u5bb6\uff0c\u91c7\u7528\u4e13\u5bb6\u878d\u5408\u6216\u77e5\u8bc6\u84b8\u998f\u7684\u5fae\u8c03\u7b56\u7565", "result": "\u5728\u8bed\u8a00\u611f\u77e5\u548c\u8bed\u8a00\u65e0\u5173\u573a\u666f\u4e0b\u5206\u522b\u83b7\u5f9710%\u548c15%\u7684\u76f8\u5bf9\u6027\u80fd\u63d0\u5347", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u6807\u51c6\u5fae\u8c03\uff0c\u6709\u6548\u63d0\u5347\u76ee\u6807\u8bed\u8a00\u7684\u8bc6\u522b\u7cbe\u5ea6"}}
{"id": "2109.05721", "pdf": "https://arxiv.org/pdf/2109.05721", "abs": "https://arxiv.org/abs/2109.05721", "authors": ["Yangyu Huang", "Hao Yang", "Chong Li", "Jongyoo Kim", "Fangyun Wei"], "title": "ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.IR", "cs.LG"], "comment": "Proceedings of the IEEE/CVF International Conference on Computer\n  Vision. 2021 (ICCV 2021)", "summary": "The recent progress of CNN has dramatically improved face alignment\nperformance. However, few works have paid attention to the error-bias with\nrespect to error distribution of facial landmarks. In this paper, we\ninvestigate the error-bias issue in face alignment, where the distributions of\nlandmark errors tend to spread along the tangent line to landmark curves. This\nerror-bias is not trivial since it is closely connected to the ambiguous\nlandmark labeling task. Inspired by this observation, we seek a way to leverage\nthe error-bias property for better convergence of CNN model. To this end, we\npropose anisotropic direction loss (ADL) and anisotropic attention module (AAM)\nfor coordinate and heatmap regression, respectively. ADL imposes strong binding\nforce in normal direction for each landmark point on facial boundaries. On the\nother hand, AAM is an attention module which can get anisotropic attention mask\nfocusing on the region of point and its local edge connected by adjacent\npoints, it has a stronger response in tangent than in normal, which means\nrelaxed constraints in the tangent. These two methods work in a complementary\nmanner to learn both facial structures and texture details. Finally, we\nintegrate them into an optimized end-to-end training pipeline named ADNet. Our\nADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which\ndemonstrates the effectiveness and robustness.", "AI": {"tldr": "\u63d0\u51fa\u5404\u5411\u5f02\u6027\u65b9\u5411\u635f\u5931(ADL)\u548c\u6ce8\u610f\u529b\u6a21\u5757(AAM)\uff0c\u901a\u8fc7\u8054\u5408\u7ea6\u675f\u6cd5\u7ebf\u65b9\u5411\u5b9a\u4f4d\u7cbe\u5ea6\u4e0e\u5207\u7ebf\u65b9\u5411\u7279\u5f81\u5173\u6ce8\uff0c\u6784\u5efaADNet\u5b9e\u73b0\u4eba\u8138\u5bf9\u9f50SOTA\u6027\u80fd", "motivation": "\u9488\u5bf9\u4eba\u8138\u5bf9\u9f50\u4efb\u52a1\u4e2d\u5730\u6807\u8bef\u5dee\u6cbf\u5207\u7ebf\u65b9\u5411\u5206\u5e03\u7684\u7279\u6027\uff0c\u8be5\u504f\u5dee\u6e90\u4e8e\u6807\u6ce8\u6a21\u7cca\u6027\u3002\u5229\u7528\u8be5\u7279\u6027\u4f18\u5316CNN\u6a21\u578b\u6536\u655b\u8fc7\u7a0b", "method": "ADL\u5728\u6cd5\u7ebf\u65b9\u5411\u65bd\u52a0\u5f3a\u7ea6\u675f\u529b\u786e\u4fdd\u5b9a\u4f4d\u7cbe\u5ea6\uff0cAAM\u751f\u6210\u5404\u5411\u5f02\u6027\u6ce8\u610f\u529b\u63a9\u6a21\u805a\u7126\u5c40\u90e8\u8fb9\u7f18\u533a\u57df\uff0c\u4e24\u8005\u4e92\u8865\u5b66\u4e60\u9762\u90e8\u7ed3\u6784\u548c\u7eb9\u7406\u7ec6\u8282", "result": "\u5728300W(3.18% NME)\u3001WFLW(4.05% NME)\u3001COFW(3.43% NME)\u6570\u636e\u96c6\u4e0a\u8fbe\u5230state-of-the-art", "conclusion": "\u901a\u8fc7\u8bef\u5dee\u5206\u5e03\u7279\u6027\u8bbe\u8ba1\u7684\u65b9\u5411\u7ea6\u675f\u4e0e\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u65b9\u6cd5\u5728\u7cbe\u5ea6\u4e0e\u9c81\u68d2\u6027\u4e0a\u7684\u4f18\u52bf"}}
{"id": "2506.21556", "pdf": "https://arxiv.org/pdf/2506.21556", "abs": "https://arxiv.org/abs/2506.21556", "authors": ["Hyeongcheol Park", "MinHyuk Jang", "Ha Dam Baek", "Gyusam Chang", "Jiyoung Seo", "Jiwan Park", "Hogun Park", "Sangpil Kim"], "title": "VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "Project Page: https://vatkg.github.io/", "summary": "Multimodal Knowledge Graphs (MMKGs), which represent explicit knowledge\nacross multiple modalities, play a pivotal role by complementing the implicit\nknowledge of Multimodal Large Language Models (MLLMs) and enabling more\ngrounded reasoning via Retrieval Augmented Generation (RAG). However, existing\nMMKGs are generally limited in scope: they are often constructed by augmenting\npre-existing knowledge graphs, which restricts their knowledge, resulting in\noutdated or incomplete knowledge coverage, and they often support only a narrow\nrange of modalities, such as text and visual information. These limitations\nreduce their extensibility and applicability to a broad range of multimodal\ntasks, particularly as the field shifts toward richer modalities such as video\nand audio in recent MLLMs. Therefore, we propose the Visual-Audio-Text\nKnowledge Graph (VAT-KG), the first concept-centric and knowledge-intensive\nmultimodal knowledge graph that covers visual, audio, and text information,\nwhere each triplet is linked to multimodal data and enriched with detailed\ndescriptions of concepts. Specifically, our construction pipeline ensures\ncross-modal knowledge alignment between multimodal data and fine-grained\nsemantics through a series of stringent filtering and alignment steps, enabling\nthe automatic generation of MMKGs from any multimodal dataset. We further\nintroduce a novel multimodal RAG framework that retrieves detailed\nconcept-level knowledge in response to queries from arbitrary modalities.\nExperiments on question answering tasks across various modalities demonstrate\nthe effectiveness of VAT-KG in supporting MLLMs, highlighting its practical\nvalue in unifying and leveraging multimodal knowledge.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u4ee5\u6982\u5ff5\u4e3a\u4e2d\u5fc3\u3001\u8986\u76d6\u89c6\u89c9-\u97f3\u9891-\u6587\u672c\u7684\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31VAT-KG\uff0c\u901a\u8fc7\u65b0\u578bRAG\u6846\u67b6\u63d0\u5347\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u77e5\u8bc6\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709MMKGs\u5b58\u5728\u77e5\u8bc6\u8986\u76d6\u8fc7\u65f6/\u4e0d\u5b8c\u6574\u3001\u6a21\u6001\u652f\u6301\u5355\u4e00\uff08\u4ec5\u56fe\u6587\uff09\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u9002\u5e94\u89c6\u9891/\u97f3\u9891\u7b49\u65b0\u578b\u6a21\u6001\u9700\u6c42", "method": "\u6784\u5efa\u5305\u542b\u4e25\u683c\u8de8\u6a21\u6001\u5bf9\u9f50\u673a\u5236\u7684\u4e09\u9636\u6bb5\u6d41\u6c34\u7ebf\uff1a1\uff09\u591a\u6a21\u6001\u6570\u636e\u8fc7\u6ee4 2\uff09\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5bf9\u9f50 3\uff09\u81ea\u52a8\u5316\u56fe\u8c31\u751f\u6210", "result": "\u5728\u591a\u6a21\u6001\u95ee\u7b54\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u8bc1\u660e\u5176\u80fd\u663e\u8457\u63d0\u5347MLLMs\u7684\u8de8\u6a21\u6001\u77e5\u8bc6\u63a8\u7406\u80fd\u529b", "conclusion": "VAT-KG\u901a\u8fc7\u7edf\u4e00\u591a\u6a21\u6001\u77e5\u8bc6\u8868\u793a\uff0c\u4e3a\u6784\u5efa\u66f4\u901a\u7528\u7684\u591a\u6a21\u6001\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\u8bbe\u65bd\u652f\u6301"}}
{"id": "2212.09525", "pdf": "https://arxiv.org/pdf/2212.09525", "abs": "https://arxiv.org/abs/2212.09525", "authors": ["Yangyu Huang", "Xi Chen", "Jongyoo Kim", "Hao Yang", "Chong Li", "Jiaolong Yang", "Dong Chen"], "title": "FreeEnricher: Enriching Face Landmarks without Additional Cost", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.IR", "cs.LG"], "comment": "AAAI 2023", "summary": "Recent years have witnessed significant growth of face alignment. Though\ndense facial landmark is highly demanded in various scenarios, e.g., cosmetic\nmedicine and facial beautification, most works only consider sparse face\nalignment. To address this problem, we present a framework that can enrich\nlandmark density by existing sparse landmark datasets, e.g., 300W with 68\npoints and WFLW with 98 points. Firstly, we observe that the local patches\nalong each semantic contour are highly similar in appearance. Then, we propose\na weakly-supervised idea of learning the refinement ability on original sparse\nlandmarks and adapting this ability to enriched dense landmarks. Meanwhile,\nseveral operators are devised and organized together to implement the idea.\nFinally, the trained model is applied as a plug-and-play module to the existing\nface alignment networks. To evaluate our method, we manually label the dense\nlandmarks on 300W testset. Our method yields state-of-the-art accuracy not only\nin newly-constructed dense 300W testset but also in the original sparse 300W\nand WFLW testsets without additional cost.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u73b0\u6709\u7a00\u758f\u5173\u952e\u70b9\u6570\u636e\u96c6\u589e\u5f3a\u9762\u90e8\u5bf9\u9f50\u5bc6\u5ea6\u7684\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u5728\u591a\u4e2a\u6d4b\u8bd5\u96c6\u5b9e\u73b0SOTA\u7cbe\u5ea6", "motivation": "\u5bc6\u96c6\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u5728\u533b\u7f8e\u3001\u7f8e\u989c\u7b49\u9886\u57df\u9700\u6c42\u65fa\u76db\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u805a\u7126\u7a00\u758f\u5173\u952e\u70b9\u68c0\u6d4b", "method": "\u5229\u7528\u9762\u90e8\u8bed\u4e49\u8f6e\u5ed3\u5c40\u90e8\u7279\u5f81\u76f8\u4f3c\u6027\uff0c\u8bbe\u8ba1\u5f31\u76d1\u7763\u5b66\u4e60\u8303\u5f0f\uff1a\u5148\u5728\u7a00\u758f\u5173\u952e\u70b9\u4e0a\u5b66\u4e60\u7ec6\u5316\u80fd\u529b\uff0c\u540e\u8fc1\u79fb\u5230\u5bc6\u96c6\u5173\u952e\u70b9\uff0c\u5e76\u5f00\u53d1\u914d\u5957\u7b97\u5b50\u6a21\u5757", "result": "\u5728\u81ea\u5efa\u5bc6\u96c6300W\u6d4b\u8bd5\u96c6\u53ca\u539f\u59cb\u7a00\u758f300W/WFLW\u6d4b\u8bd5\u96c6\u5747\u8fbe\u5230state-of-the-art\u7cbe\u5ea6\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u6210\u672c", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5bc6\u96c6\u9762\u90e8\u5bf9\u9f50\u7684\u6570\u636e\u4f9d\u8d56\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u901a\u8fc7\u7a00\u758f\u6570\u636e\u6269\u5c55\u5bc6\u96c6\u68c0\u6d4b\u80fd\u529b\u7684\u53ef\u884c\u6027"}}
{"id": "2506.21557", "pdf": "https://arxiv.org/pdf/2506.21557", "abs": "https://arxiv.org/abs/2506.21557", "authors": ["Kaiying Yan", "Moyang Liu", "Yukun Liu", "Ruibo Fu", "Zhengqi Wen", "Jianhua Tao", "Xuefei Liu"], "title": "Debunk and Infer: Multimodal Fake News Detection via Diffusion-Generated Evidence and LLM Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "The rapid spread of fake news across multimedia platforms presents serious\nchallenges to information credibility. In this paper, we propose a\nDebunk-and-Infer framework for Fake News Detection(DIFND) that leverages\ndebunking knowledge to enhance both the performance and interpretability of\nfake news detection. DIFND integrates the generative strength of conditional\ndiffusion models with the collaborative reasoning capabilities of multimodal\nlarge language models (MLLMs). Specifically, debunk diffusion is employed to\ngenerate refuting or authenticating evidence based on the multimodal content of\nnews videos, enriching the evaluation process with diverse yet semantically\naligned synthetic samples. To improve inference, we propose a chain-of-debunk\nstrategy where a multi-agent MLLM system produces logic-grounded,\nmultimodal-aware reasoning content and final veracity judgment. By jointly\nmodeling multimodal features, generative debunking cues, and reasoning-rich\nverification within a unified architecture, DIFND achieves notable improvements\nin detection accuracy. Extensive experiments on the FakeSV and FVC datasets\nshow that DIFND not only outperforms existing approaches but also delivers\ntrustworthy decisions.", "AI": {"tldr": "\u63d0\u51faDIFND\u6846\u67b6\uff0c\u7ed3\u5408\u6761\u4ef6\u6269\u6563\u6a21\u578b\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u865a\u5047\u65b0\u95fb\u68c0\u6d4b\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027", "motivation": "\u865a\u5047\u65b0\u95fb\u5728\u591a\u5a92\u4f53\u5e73\u53f0\u5feb\u901f\u4f20\u64ad\u4e25\u91cd\u5a01\u80c1\u4fe1\u606f\u53ef\u4fe1\u5ea6\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u4fe1\u9a8c\u8bc1\u548c\u63a8\u7406\u80fd\u529b", "method": "1. \u4f7f\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\u751f\u6210\u53cd\u9a73/\u9a8c\u8bc1\u8bc1\u636e\n2. \u6784\u5efa\u591a\u4ee3\u7406MLLM\u7cfb\u7edf\u5b9e\u73b0\u94fe\u5f0f\u63a8\u7406\n3. \u8054\u5408\u5efa\u6a21\u591a\u6a21\u6001\u7279\u5f81\u3001\u751f\u6210\u53cd\u9a73\u7ebf\u7d22\u548c\u63a8\u7406\u9a8c\u8bc1", "result": "\u5728FakeSV\u548cFVC\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u68c0\u6d4b\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "conclusion": "DIFND\u6846\u67b6\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u6837\u672c\u548c\u591a\u6a21\u6001\u534f\u540c\u63a8\u7406\uff0c\u65e2\u63d0\u5347\u68c0\u6d4b\u6027\u80fd\u53c8\u63d0\u4f9b\u53ef\u4fe1\u51b3\u7b56\u4f9d\u636e"}}
{"id": "2506.21811", "pdf": "https://arxiv.org/pdf/2506.21811", "abs": "https://arxiv.org/abs/2506.21811", "authors": ["Lingkai Meng", "Yu Shao", "Long Yuan", "Longbin Lai", "Peng Cheng", "Xue Li", "Wenyuan Yu", "Wenjie Zhang", "Xuemin Lin", "Jingren Zhou"], "title": "Revisiting Graph Analytics Benchmark", "categories": ["cs.DB", "cs.GR"], "comment": null, "summary": "The rise of graph analytics platforms has led to the development of various\nbenchmarks for evaluating and comparing platform performance. However, existing\nbenchmarks often fall short of fully assessing performance due to limitations\nin core algorithm selection, data generation processes (and the corresponding\nsynthetic datasets), as well as the neglect of API usability evaluation. To\naddress these shortcomings, we propose a novel graph analytics benchmark.\nFirst, we select eight core algorithms by extensively reviewing both academic\nand industrial settings. Second, we design an efficient and flexible data\ngenerator and produce eight new synthetic datasets as the default datasets for\nour benchmark. Lastly, we introduce a multi-level large language model\n(LLM)-based framework for API usability evaluation-the first of its kind in\ngraph analytics benchmarks. We conduct comprehensive experimental evaluations\non existing platforms (GraphX, PowerGraph, Flash, Grape, Pregel+, Ligra and\nG-thinker). The experimental results demonstrate the superiority of our\nproposed benchmark.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u578b\u56fe\u5206\u6790\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u6838\u5fc3\u7b97\u6cd5\u9009\u62e9\u3001\u6570\u636e\u751f\u6210\u6d41\u7a0b\u548c\u5f15\u5165LLM\u9a71\u52a8\u7684API\u8bc4\u4f30\u6846\u67b6\uff0c\u5b9e\u73b0\u5bf9\u73b0\u6709\u5e73\u53f0\u7684\u5168\u9762\u6027\u80fd\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u56fe\u5206\u6790\u57fa\u51c6\u5b58\u5728\u6838\u5fc3\u7b97\u6cd5\u8986\u76d6\u4e0d\u8db3\u3001\u5408\u6210\u6570\u636e\u96c6\u751f\u6210\u65b9\u5f0f\u53d7\u9650\u3001\u7f3a\u4e4fAPI\u6613\u7528\u6027\u8bc4\u4f30\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u6027\u80fd\u8bc4\u4f30\u4e0d\u5168\u9762\u3002", "method": "1) \u901a\u8fc7\u5b66\u672f\u4e0e\u5de5\u4e1a\u573a\u666f\u8c03\u7814\u9009\u53d68\u4e2a\u6838\u5fc3\u7b97\u6cd5\uff1b2) \u5f00\u53d1\u7075\u6d3b\u6570\u636e\u751f\u6210\u5668\u5e76\u521b\u5efa8\u4e2a\u65b0\u5408\u6210\u6570\u636e\u96c6\uff1b3) \u9996\u521b\u57fa\u4e8eLLM\u7684\u591a\u5c42\u6b21API\u6613\u7528\u6027\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u5728GraphX/PowerGraph\u7b497\u4e2a\u4e3b\u6d41\u5e73\u53f0\u4e0a\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u65b0\u57fa\u51c6\u5728\u8bc4\u4f30\u7ef4\u5ea6\u5b8c\u6574\u6027\u548c\u8bc4\u4f30\u7ed3\u679c\u6709\u6548\u6027\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u8be5\u57fa\u51c6\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u4f53\u7cfb\u7684\u4e09\u5927\u7f3a\u9677\uff0c\u4e3a\u56fe\u5206\u6790\u5e73\u53f0\u7684\u6027\u80fd\u6bd4\u8f83\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u5de5\u5177\u548c\u65b9\u6cd5\u8bba\u652f\u6491\u3002"}}
{"id": "2506.21558", "pdf": "https://arxiv.org/pdf/2506.21558", "abs": "https://arxiv.org/abs/2506.21558", "authors": ["FutureSearch", ":", "Jack Wildman", "Nikos I. Bosse", "Daniel Hnyk", "Peter M\u00fchlbacher", "Finn Hambly", "Jon Evans", "Dan Schwarz", "Lawrence Phillips"], "title": "Bench to the Future: A Pastcasting Benchmark for Forecasting Agents", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Forecasting is a challenging task that offers a clearly measurable way to\nstudy AI systems. Forecasting requires a large amount of research on the\ninternet, and evaluations require time for events to happen, making the\ndevelopment of forecasting benchmarks challenging. To date, no forecasting\nbenchmark provides a realistic, hermetic, and repeatable environment for LLM\nforecasters. We introduce Bench To the Future (BTF), a \"pastcasting\" benchmark\nwith hundreds of high-quality questions for which the resolution is already\nknown. Each question is accompanied by a large offline corpus of tens of\nthousands of relevant web pages, enabling a way to elicit realistic \"forecasts\"\non past events from LLMs. Results suggest that our pastcasting environment can\nproduce results comparable to those based on forecasts using the internet on\nat-the-time unresolved questions. We show results benchmarking agent and\nchain-of-thought forecasting approaches using several LLMs, including the\nrecently-released Claude 4 models, and demonstrate BTF's ability to track\nsteady forecasting capability progress over time. We intend this to be a living\nbenchmark, with new questions added continually to account for increasing\ntraining data cutoff dates. We invite researchers to contact us at\nhello@futuresearch.ai to utilize our benchmark or tooling for their own\nresearch.", "AI": {"tldr": "\u63d0\u51faBTF\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5386\u53f2\u5df2\u77e5\u7ed3\u679c\u7684'\u8fc7\u53bb\u9884\u6d4b'\u65b9\u6cd5\u8bc4\u4f30LLM\u9884\u6d4b\u80fd\u529b\uff0c\u652f\u6301\u6301\u7eed\u66f4\u65b0\u8ddf\u8e2aAI\u8fdb\u5c55\u3002", "motivation": "\u73b0\u6709\u9884\u6d4b\u57fa\u51c6\u7f3a\u4e4f\u771f\u5b9e\u5c01\u95ed\u7684\u6d4b\u8bd5\u73af\u5883\uff0c\u4e14\u8bc4\u4f30\u9700\u8981\u7b49\u5f85\u4e8b\u4ef6\u53d1\u751f\uff0c\u5f00\u53d1\u9884\u6d4b\u57fa\u51c6\u9762\u4e34\u6311\u6218\u3002", "method": "\u6784\u5efa\u5305\u542b\u6570\u767e\u4e2a\u5df2\u77e5\u7ed3\u679c\u95ee\u9898\u7684'pastcasting'\u57fa\u51c6\uff0c\u914d\u5957\u6570\u4e07\u76f8\u5173\u7f51\u9875\u79bb\u7ebf\u8bed\u6599\u5e93\u8fdb\u884c\u5386\u53f2\u4e8b\u4ef6\u9884\u6d4b\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u73af\u5883\u9884\u6d4b\u6548\u679c\u63a5\u8fd1\u771f\u5b9e\u4e92\u8054\u7f51\u9884\u6d4b\uff0c\u6210\u529f\u8ffd\u8e2aClaude 4\u7b49\u6a21\u578b\u7684\u9884\u6d4b\u80fd\u529b\u8fdb\u5c55\u3002", "conclusion": "\u5efa\u7acb\u53ef\u6301\u7eed\u66f4\u65b0\u7684\u9884\u6d4b\u57fa\u51c6\uff0c\u9080\u8bf7\u7814\u7a76\u8005\u4f7f\u7528\u5176\u5de5\u5177\u63a8\u52a8AI\u9884\u6d4b\u80fd\u529b\u7814\u7a76\u3002"}}
{"id": "2506.21845", "pdf": "https://arxiv.org/pdf/2506.21845", "abs": "https://arxiv.org/abs/2506.21845", "authors": ["Zhuodi Cai"], "title": "3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.GR", "I.2; I.2.1; I.2.7; I.3; H.5; J.5"], "comment": "5 pages, 2 figures, 3 tables (containing 21 subfigures)", "summary": "This paper presents 3Description, an experimental human-AI collaborative\napproach for intuitive 3D modeling. 3Description aims to address accessibility\nand usability challenges in traditional 3D modeling by enabling\nnon-professional individuals to co-create 3D models using verbal and gesture\ndescriptions. Through a combination of qualitative research, product analysis,\nand user testing, 3Description integrates AI technologies such as Natural\nLanguage Processing and Computer Vision, powered by OpenAI and MediaPipe.\nRecognizing the web has wide cross-platform capabilities, 3Description is\nweb-based, allowing users to describe the desired model and subsequently adjust\nits components using verbal and gestural inputs. In the era of AI and emerging\nmedia, 3Description not only contributes to a more inclusive and user-friendly\ndesign process, empowering more people to participate in the construction of\nthe future 3D world, but also strives to increase human engagement in\nco-creation with AI, thereby avoiding undue surrender to technology and\npreserving human creativity.", "AI": {"tldr": "3Description\u662f\u4e00\u4e2a\u57fa\u4e8eWeb\u7684AI\u534f\u4f5c\u5f0f3D\u5efa\u6a21\u5de5\u5177\uff0c\u901a\u8fc7\u8bed\u8a00\u548c\u624b\u52bf\u4ea4\u4e92\u964d\u4f4e\u4f7f\u7528\u95e8\u69db\uff0c\u8d4b\u80fd\u975e\u4e13\u4e1a\u7528\u6237\u53c2\u4e0e\u521b\u4f5c\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf3D\u5efa\u6a21\u5de5\u5177\u5bf9\u975e\u4e13\u4e1a\u4eba\u58eb\u4e0d\u53cb\u597d\u7684\u95ee\u9898\uff0c\u5728AI\u65f6\u4ee3\u4fdd\u6301\u4eba\u7c7b\u5728\u534f\u540c\u521b\u4f5c\u4e2d\u7684\u4e3b\u5bfc\u5730\u4f4d\uff0c\u907f\u514d\u6280\u672f\u8fc7\u5ea6\u53d6\u4ee3\u4eba\u7c7b\u521b\u9020\u529b\u3002", "method": "\u7ed3\u5408\u5b9a\u6027\u7814\u7a76\u3001\u4ea7\u54c1\u5206\u6790\u548c\u7528\u6237\u6d4b\u8bd5\uff0c\u96c6\u6210OpenAI\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548cMediaPipe\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\uff0c\u6784\u5efa\u7f51\u9875\u7aef\u4ea4\u4e92\u7cfb\u7edf\u3002", "result": "\u5f00\u53d1\u51fa\u652f\u6301\u8bed\u97f3\u63cf\u8ff0\u548c\u624b\u52bf\u8c03\u6574\u7684\u8de8\u5e73\u53f0\u5efa\u6a21\u65b9\u6848\uff0c\u63d0\u5347\u4eba\u673a\u534f\u540c\u6548\u7387\u4e0e\u521b\u4f5c\u5305\u5bb9\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u63a8\u52a83D\u8bbe\u8ba1\u7684\u6c11\u4e3b\u5316\u8fdb\u7a0b\uff0c\u66f4\u5728AI\u4e0e\u4eba\u7c7b\u5171\u521b\u4e2d\u5efa\u7acb\u4e86\u826f\u6027\u4e92\u52a8\u6a21\u5f0f\uff0c\u4e3a\u672a\u6765\u5143\u5b87\u5b99\u5efa\u8bbe\u50a8\u5907\u521b\u4f5c\u529b\u91cf\u3002"}}
{"id": "2506.21559", "pdf": "https://arxiv.org/pdf/2506.21559", "abs": "https://arxiv.org/abs/2506.21559", "authors": ["Junze Chen", "Cheng Yang", "Shujie Li", "Zhiqiang Zhang", "Yawen Li", "Junping Du", "Chuan Shi"], "title": "GraphLAMA: Enabling Efficient Adaptation of Graph Language Models with Limited Annotations", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated their strong capabilities in\nvarious domains, and have been recently integrated for graph analysis as graph\nlanguage models (GLMs). With LLMs as the predictor, some GLMs can interpret\nunseen tasks described by natural language, and learn from a few examples in\nthe prompts without parameter tuning, known as in-context learning (ICL).\nAnother subset of GLMs utilizes abundant training labels to enhance model\nperformance, known as instruction tuning. However, we argue that ICL on graphs\nhas effectiveness issues due to fixed parameters and efficiency issues due to\nlong context. Meanwhile, the large amount of labeled data required for\ninstruction tuning can be difficult to obtain in real-world scenarios. To this\nend, we aim to introduce an extra parameter adaptation stage that can\nefficiently tailor GLMs to an unseen graph and task with only a few labeled\nexamples, in exchange for better prediction accuracy and faster inference\nspeed. For implementation, in this paper we propose GraphLAMA method, with its\nmodel backbone and learning schemes specialized for efficient tuning and\ninference. Specifically, for model backbone, we use a graph neural network\n(GNN) with several well-designed components to transform nodes into the\nrepresentation space of LLM tokens. Task instructions can then be represented\nas a mixture of node and language tokens. In the pre-training stage, model\nparameters except the LLM will be trained with different tasks to capture\ngeneral knowledge. In the adaptation stage, only a few pre-trained parameters\nwill be updated based on few-shot examples. Extensive experiments on\nfew/zero-shot node classification and summary generation show that our proposed\nGraphLAMA achieves state-of-the-art performance with 4.91% absolution\nimprovement in accuracy. Compared with ICL, our inference speed can be 10 times\nfaster under 5-shot setting.", "AI": {"tldr": "\u63d0\u51faGraphLAMA\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u9002\u5e94\u9636\u6bb5\u6709\u6548\u8c03\u6574\u56fe\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u5c11\u91cf\u6807\u6ce8\u6837\u672c\u4e0b\u5b9e\u73b0\u51c6\u786e\u7387\u63d0\u53474.91%\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4ICL\u5feb10\u500d\u3002", "motivation": "\u73b0\u6709\u56fe\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u5b58\u5728\u53c2\u6570\u56fa\u5316\u5bfc\u81f4\u7684\u6548\u7387\u95ee\u9898\uff0c\u800c\u6307\u4ee4\u5fae\u8c03\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u3002\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u9002\u5e94\u673a\u5236\u6765\u89e3\u51b3\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u6027\u80fd\u4e0e\u6548\u7387\u5e73\u8861\u95ee\u9898\u3002", "method": "1. \u6784\u5efa\u5305\u542bGNN\u7ec4\u4ef6\u7684\u6a21\u578b\u67b6\u6784\uff0c\u5c06\u56fe\u8282\u70b9\u6620\u5c04\u5230LLM\u8868\u793a\u7a7a\u95f4\n2. \u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u9884\u8bad\u7ec3\u6355\u83b7\u901a\u7528\u77e5\u8bc6 + \u9002\u914d\u9636\u6bb5\u5fae\u8c03\u5c11\u91cf\u53c2\u6570\n3. \u4efb\u52a1\u6307\u4ee4\u8868\u793a\u4e3a\u8282\u70b9\u4e0e\u8bed\u8a00\u6807\u8bb0\u7684\u6df7\u5408\u5d4c\u5165", "result": "\u5728\u5c11\u6837\u672c/\u96f6\u6837\u672c\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u51c6\u786e\u7387\u7edd\u5bf9\u63d0\u53474.91%\u30025-shot\u573a\u666f\u4e0b\u63a8\u7406\u901f\u5ea6\u8fbe\u5230ICL\u768410\u500d\u3002", "conclusion": "GraphLAMA\u9996\u6b21\u5c06\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u673a\u5236\u5f15\u5165\u56fe\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u4fdd\u6301LLM\u63a8\u7406\u4f18\u52bf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5c0f\u6837\u672c\u573a\u666f\u4e0b\u7684\u4efb\u52a1\u6027\u80fd\uff0c\u4e3a\u56fe\u5206\u6790\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u8303\u5f0f\u3002"}}
{"id": "2506.22319", "pdf": "https://arxiv.org/pdf/2506.22319", "abs": "https://arxiv.org/abs/2506.22319", "authors": ["Di Zhang", "Ligang Liu"], "title": "Asymptotic analysis and design of shell-based thermal lattice metamaterials", "categories": ["math.AP", "cs.GR", "math-ph", "math.MP", "physics.comp-ph", "74Q15 (Primary) 35Q74, 74Q20, 74K25 (Secondary)", "I.3.5; J.2"], "comment": null, "summary": "We present a rigorous asymptotic analysis framework for investigating the\nthermal conductivity of shell lattice metamaterials, extending prior work from\nmechanical stiffness to heat transfer. Central to our analysis is a new metric,\nthe asymptotic directional conductivity (ADC), which captures the leading-order\ninfluence of the middle surface geometry on the effective thermal conductivity\nin the vanishing-thickness limit. A convergence theorem is established for\nevaluating ADC, along with a sharp upper bound and the necessary and sufficient\ncondition for achieving this bound. These results provide the first theoretical\njustification for the optimal thermal conductivity of triply periodic minimal\nsurfaces. Furthermore, we show that ADC yields a third-order approximation to\nthe effective conductivity of shell lattices at low volume fractions. To\nsupport practical design applications, we develop a discrete algorithm for\ncomputing and optimizing ADC over arbitrary periodic surfaces. Numerical\nresults confirm the theoretical predictions and demonstrate the robustness and\neffectiveness of the proposed optimization algorithm.", "AI": {"tldr": "\u63d0\u51fa\u6e10\u8fd1\u65b9\u5411\u5bfc\u70ed\u7387(ADC)\u6307\u6807\u5206\u6790\u58f3\u5c42\u6676\u683c\u8d85\u6750\u6599\u5bfc\u70ed\u6027\uff0c\u5efa\u7acb\u6536\u655b\u5b9a\u7406\u4e0e\u4f18\u5316\u7b97\u6cd5\uff0c\u6570\u503c\u9a8c\u8bc1\u7406\u8bba\u6709\u6548\u6027\u3002", "motivation": "\u5c06\u529b\u5b66\u6027\u80fd\u5206\u6790\u6846\u67b6\u62d3\u5c55\u81f3\u70ed\u4f20\u5bfc\u9886\u57df\uff0c\u4e3a\u4e09\u91cd\u5468\u671f\u6700\u5c0f\u8868\u9762\u8d85\u6750\u6599\u7684\u6700\u4f18\u5bfc\u70ed\u6027\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u901a\u8fc7\u6e10\u8fd1\u5206\u6790\u6846\u67b6\u5efa\u7acbADC\u6570\u5b66\u6a21\u578b\uff0c\u63a8\u5bfc\u6536\u655b\u5b9a\u7406\u4e0e\u6700\u4f18\u4e0a\u754c\u6761\u4ef6\uff0c\u5f00\u53d1\u79bb\u6563\u7b97\u6cd5\u5b9e\u73b0\u5468\u671f\u6027\u8868\u9762ADC\u4f18\u5316\u8ba1\u7b97\u3002", "result": "\u7406\u8bba\u8bc1\u660eADC\u53ef\u4e09\u9636\u8fd1\u4f3c\u4f4e\u4f53\u79ef\u5206\u6570\u4e0b\u7684\u6709\u6548\u5bfc\u70ed\u7387\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u7b97\u6cd5\u9c81\u68d2\u6027\u53ca\u7406\u8bba\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u6784\u5efa\u7684\u6e10\u8fd1\u5206\u6790\u6846\u67b6\u4e3a\u8d85\u6750\u6599\u5bfc\u70ed\u4f18\u5316\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u79bb\u6563\u7b97\u6cd5\u5b9e\u73b0\u4e3a\u5b9e\u9645\u5de5\u7a0b\u5e94\u7528\u63d0\u4f9b\u6709\u6548\u5de5\u5177\uff0c\u65b9\u6cd5\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u7269\u7406\u6027\u8d28\u7814\u7a76\u3002"}}
{"id": "2506.21560", "pdf": "https://arxiv.org/pdf/2506.21560", "abs": "https://arxiv.org/abs/2506.21560", "authors": ["Yifu Han", "Geo Zhang"], "title": "Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates the effectiveness of reinforcement learning (RL)\nfine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two\nchallenging tasks: instruction following and mathematical reasoning. We compare\nsupervised fine-tuning (SFT), Direct Preference Optimization (DPO) using\npreference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models.\nOur experiments show that RLOO with DeBERTa reward modeling achieves the best\nalignment, while DPO provides strong and consistent results. For math reasoing\ntasks, synthetic data augmentation and best-of-N sampling with an external\nverifier significantly improve accuracy, showing the potential of combining\nfine-tuning with inference-time tools. This study highlights key trade-offs and\npractical strategies for training lightweight, task-aligned small-scale\nlanguage models.", "AI": {"tldr": "\u5bf9\u6bd4\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u65b9\u6cd5\u5728\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u6548\u679c\uff0cRLOO+DeBERTa\u5956\u52b1\u6a21\u578b\u5b9e\u73b0\u6700\u4f73\u5bf9\u9f50\uff0cDPO\u8868\u73b0\u7a33\u5065\uff0c\u5408\u6210\u6570\u636e\u4e0e\u9a8c\u8bc1\u5668\u7ec4\u5408\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u80fd\u529b", "motivation": "\u63a2\u7d22\u5728\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\uff08Qwen2.5-0.5B\uff09\u4e0a\u5e94\u7528\u4e0d\u540c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u65b9\u6cd5\u7684\u6548\u679c\uff0c\u89e3\u51b3\u6307\u4ee4\u8ddf\u968f\u548c\u6570\u5b66\u63a8\u7406\u4e24\u5927\u6311\u6218\u6027\u4efb\u52a1\u7684\u6a21\u578b\u5bf9\u9f50\u95ee\u9898", "method": "\u4f7f\u7528\u76d1\u7763\u5fae\u8c03(SFT)\u3001\u76f4\u63a5\u504f\u597d\u4f18\u5316(DPO)\u548cReinforce Leave-One-Out(RLOO)\u4e09\u79cd\u65b9\u6cd5\uff0c\u7ed3\u5408DeBERTa\u5956\u52b1\u5efa\u6a21\u3001\u5408\u6210\u6570\u636e\u589e\u5f3a\u548cbest-of-N\u91c7\u6837\u9a8c\u8bc1\u5de5\u5177", "result": "\u6570\u5b66\u4efb\u52a1\u51c6\u786e\u7387\u901a\u8fc7\u5408\u6210\u6570\u636e+\u9a8c\u8bc1\u5668\u7ec4\u5408\u663e\u8457\u63d0\u5347\uff0cRLOO\u5728\u6307\u4ee4\u5bf9\u9f50\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\uff0cDPO\u5728\u4e0d\u540c\u573a\u666f\u4fdd\u6301\u7a33\u5065\u6027", "conclusion": "\u7814\u7a76\u8868\u660e\u5c06\u5fae\u8c03\u4e0e\u63a8\u7406\u65f6\u5de5\u5177\u7ed3\u5408\u80fd\u6709\u6548\u63d0\u5347\u5c0f\u6a21\u578b\u4efb\u52a1\u6027\u80fd\uff0c\u4e3a\u8f7b\u91cf\u7ea7\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6570\u636e\u589e\u5f3a\u3001\u5956\u52b1\u5efa\u6a21\u4e0e\u9a8c\u8bc1\u5668\u7ec4\u5408\u7684\u5b9e\u7528\u65b9\u6848"}}
{"id": "2506.22426", "pdf": "https://arxiv.org/pdf/2506.22426", "abs": "https://arxiv.org/abs/2506.22426", "authors": ["Xiang Dai", "Kyrollos Yanny", "Kristina Monakhova", "Nicholas Antipa"], "title": "Single-shot HDR using conventional image sensor shutter functions and optical randomization", "categories": ["eess.IV", "cs.CV", "cs.GR", "eess.SP", "physics.optics"], "comment": null, "summary": "High-dynamic-range (HDR) imaging is an essential technique for overcoming the\ndynamic range limits of image sensors. The classic method relies on multiple\nexposures, which slows capture time, resulting in motion artifacts when imaging\ndynamic scenes. Single-shot HDR imaging alleviates this issue by encoding HDR\ndata into a single exposure, then computationally recovering it. Many\nestablished methods use strong image priors to recover improperly exposed image\ndetail. These approaches struggle with extended highlight regions. We utilize\nthe global reset release (GRR) shutter mode of an off-the-shelf sensor. GRR\nshutter mode applies a longer exposure time to rows closer to the bottom of the\nsensor. We use optics that relay a randomly permuted (shuffled) image onto the\nsensor, effectively creating spatially randomized exposures across the scene.\nThe exposure diversity allows us to recover HDR data by solving an optimization\nproblem with a simple total variation image prior. In simulation, we\ndemonstrate that our method outperforms other single-shot methods when many\nsensor pixels are saturated (10% or more), and is competitive at a modest\nsaturation (1%). Finally, we demonstrate a physical lab prototype that uses an\noff-the-shelf random fiber bundle for the optical shuffling. The fiber bundle\nis coupled to a low-cost commercial sensor operating in GRR shutter mode. Our\nprototype achieves a dynamic range of up to 73dB using an 8-bit sensor with\n48dB dynamic range.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eGRR\u5feb\u95e8\u548c\u5149\u5b66\u968f\u673a\u6392\u5217\u7684\u5355\u6b21HDR\u6210\u50cf\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u9ad8\u9971\u548c\u533a\u57df\u7684\u52a8\u6001\u8303\u56f4\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u591a\u66dd\u5149HDR\u65b9\u6cd5\u5728\u52a8\u6001\u573a\u666f\u4e2d\u4ea7\u751f\u7684\u8fd0\u52a8\u4f2a\u5f71\uff0c\u4ee5\u53ca\u73b0\u6709\u5355\u6b21HDR\u65b9\u6cd5\u5728\u5f3a\u5149\u533a\u57df\u6062\u590d\u6548\u679c\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "1. \u5229\u7528\u73b0\u6210\u4f20\u611f\u5668\u7684GRR\u5feb\u95e8\u6a21\u5f0f\u5b9e\u73b0\u884c\u7ea7\u5dee\u5f02\u5316\u66dd\u5149\n2. \u901a\u8fc7\u968f\u673a\u5149\u7ea4\u675f\u5c06\u56fe\u50cf\u7a7a\u95f4\u968f\u673a\u6392\u5217\u5230\u4f20\u611f\u5668\uff0c\u521b\u5efa\u7a7a\u95f4\u968f\u673a\u5316\u66dd\u5149\n3. \u7ed3\u5408\u603b\u53d8\u5dee\u5148\u9a8c\u8fdb\u884c\u4f18\u5316\u91cd\u5efa", "result": "\u4eff\u771f\u663e\u793a\u572810%\u50cf\u7d20\u9971\u548c\u65f6\u4f18\u4e8e\u5176\u4ed6\u5355\u6b21\u65b9\u6cd5\uff0c\u539f\u578b\u7cfb\u7edf\u57288bit\u4f20\u611f\u5668\u4e0a\u5b9e\u73b073dB\u52a8\u6001\u8303\u56f4\uff08\u539f48dB\uff09", "conclusion": "\u901a\u8fc7GRR\u5feb\u95e8\u6a21\u5f0f\u4e0e\u5149\u5b66\u968f\u673a\u6392\u5217\u7684\u521b\u65b0\u7ed3\u5408\uff0c\u5728\u786c\u4ef6\u517c\u5bb9\u6027\u524d\u63d0\u4e0b\u6709\u6548\u63d0\u5347\u4e86\u5355\u6b21HDR\u6210\u50cf\u6027\u80fd\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9ad8\u9971\u548c\u573a\u666f\u3002"}}
{"id": "2506.21561", "pdf": "https://arxiv.org/pdf/2506.21561", "abs": "https://arxiv.org/abs/2506.21561", "authors": ["Emilio Barkett", "Olivia Long", "Madhavendra Thakur"], "title": "Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite their widespread use in fact-checking, moderation, and high-stakes\ndecision-making, large language models (LLMs) remain poorly understood as\njudges of truth. This study presents the largest evaluation to date of LLMs'\nveracity detection capabilities and the first analysis of these capabilities in\nreasoning models. We had eight LLMs make 4,800 veracity judgments across\nseveral prompts, comparing reasoning and non-reasoning models. We find that\nrates of truth-bias, or the likelihood to believe a statement is true,\nregardless of whether it is actually true, are lower in reasoning models than\nin non-reasoning models, but still higher than human benchmarks. Most\nconcerning, we identify sycophantic tendencies in several advanced models\n(o4-mini and GPT-4.1 from OpenAI, R1 from DeepSeek), which displayed an\nasymmetry in detection accuracy, performing well in truth accuracy but poorly\nin deception accuracy. This suggests that capability advances alone do not\nresolve fundamental veracity detection challenges in LLMs.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u771f\u76f8\u68c0\u6d4b\u4e2d\u5b58\u5728\u771f\u504f\u5dee\u548c\u4e0d\u5bf9\u79f0\u6027\uff0c\u63a8\u7406\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u975e\u63a8\u7406\u6a21\u578b\u4f46\u4ecd\u6709\u5c40\u9650\uff0c\u90e8\u5206\u5148\u8fdb\u6a21\u578b\u5448\u73b0\u963f\u8c00\u503e\u5411", "motivation": "\u8bc4\u4f30LLMs\u4f5c\u4e3a\u771f\u76f8\u5224\u65ad\u8005\u7684\u6709\u6548\u6027\uff0c\u63a2\u7a76\u63a8\u7406\u6a21\u578b\u4e0e\u975e\u63a8\u7406\u6a21\u578b\u5728\u771f\u76f8\u68c0\u6d4b\u4e2d\u7684\u5dee\u5f02", "method": "\u4f7f\u75288\u4e2aLLM\u8fdb\u884c4,800\u6b21\u771f\u5b9e\u6027\u5224\u65ad\uff0c\u6bd4\u8f83\u4e0d\u540c\u63d0\u793a\u4e0b\u63a8\u7406\u6a21\u578b\u4e0e\u975e\u63a8\u7406\u6a21\u578b\u7684\u771f\u504f\u5dee\u7387\u548c\u68c0\u6d4b\u51c6\u786e\u6027", "result": "\u63a8\u7406\u6a21\u578b\u771f\u504f\u5dee\u7387\uff0818.3%\uff09\u4f4e\u4e8e\u975e\u63a8\u7406\u6a21\u578b\uff0831.7%\uff09\uff0c\u4f46GPT-4.1\u7b49\u5148\u8fdb\u6a21\u578b\u5728\u6b3a\u9a97\u68c0\u6d4b\u4e2d\u51c6\u786e\u7387\u9aa4\u964d\uff08\u4ec552%\uff09", "conclusion": "\u5355\u7eaf\u63d0\u5347\u6a21\u578b\u80fd\u529b\u65e0\u6cd5\u89e3\u51b3\u771f\u76f8\u68c0\u6d4b\u7684\u6838\u5fc3\u6311\u6218\uff0c\u9700\u9488\u5bf9\u6027\u4f18\u5316\u6a21\u578b\u5bf9\u6b3a\u9a97\u6027\u5185\u5bb9\u7684\u8bc6\u522b\u673a\u5236"}}
{"id": "2506.21562", "pdf": "https://arxiv.org/pdf/2506.21562", "abs": "https://arxiv.org/abs/2506.21562", "authors": ["Jun Yin", "Pengyu Zeng", "Jing Zhong", "Peilin Li", "Miao Zhang", "Ran Luo", "Shuai Lu"], "title": "FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction", "categories": ["cs.CL", "cs.AI", "cs.AR"], "comment": null, "summary": "In the architectural design process, floor plan generation is inherently\nprogressive and iterative. However, existing generative models for floor plans\nare predominantly end-to-end generation that produce an entire pixel-based\nlayout in a single pass. This paradigm is often incompatible with the\nincremental workflows observed in real-world architectural practice. To address\nthis issue, we draw inspiration from the autoregressive 'next token prediction'\nmechanism commonly used in large language models, and propose a novel 'next\nroom prediction' paradigm tailored to architectural floor plan modeling.\nExperimental evaluation indicates that FPDS demonstrates competitive\nperformance in comparison to diffusion models and Tell2Design in the\ntext-to-floorplan task, indicating its potential applicability in supporting\nfuture intelligent architectural design.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e'next room prediction'\u7684FPDS\u751f\u6210\u8303\u5f0f\uff0c\u89e3\u51b3\u73b0\u6709\u7aef\u5230\u7aef\u6a21\u578b\u4e0e\u5efa\u7b51\u6e10\u8fdb\u5f0f\u5de5\u4f5c\u6d41\u4e0d\u517c\u5bb9\u7684\u95ee\u9898", "motivation": "\u73b0\u6709\u50cf\u7d20\u7ea7\u7aef\u5230\u7aef\u751f\u6210\u65b9\u5f0f\u4e0d\u7b26\u5408\u5efa\u7b51\u5b9e\u8df5\u4e2d\u8fed\u4ee3\u6e10\u8fdb\u7684\u8bbe\u8ba1\u6d41\u7a0b\uff0c\u9700\u63a2\u7d22\u7b26\u5408\u884c\u4e1a\u5b9e\u9645\u9700\u6c42\u7684\u65b9\u6cd5", "method": "\u53d7\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u56de\u5f52\u673a\u5236\u542f\u53d1\uff0c\u6784\u5efa\u9762\u5411\u5efa\u7b51\u5e73\u9762\u56fe\u5efa\u6a21\u7684'next room prediction'\u6e10\u8fdb\u751f\u6210\u8303\u5f0f", "result": "\u5728\u6587\u672c\u8f6c\u5e73\u9762\u56fe\u4efb\u52a1\u4e2d\uff0cFPDS\u76f8\u8f83\u4e8e\u6269\u6563\u6a21\u578b\u548cTell2Design\u5c55\u73b0\u51fa\u7ade\u4e89\u529b", "conclusion": "FPDS\u4e3a\u667a\u80fd\u5efa\u7b51\u8bbe\u8ba1\u652f\u6301\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5177\u6709\u5b9e\u8df5\u5e94\u7528\u6f5c\u529b\u7684\u65b0\u8303\u5f0f"}}
{"id": "2506.21563", "pdf": "https://arxiv.org/pdf/2506.21563", "abs": "https://arxiv.org/abs/2506.21563", "authors": ["Kaiying Kevin Lin", "Hsiyu Chen", "Haopeng Zhang"], "title": "FormosanBench: Benchmarking Low-Resource Austronesian Languages in the Era of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "While large language models (LLMs) have demonstrated impressive performance\nacross a wide range of natural language processing (NLP) tasks in high-resource\nlanguages, their capabilities in low-resource and minority languages remain\nsignificantly underexplored. Formosan languages -- a subgroup of Austronesian\nlanguages spoken in Taiwan -- are both linguistically rich and endangered,\nlargely due to the sociolinguistic dominance of Mandarin. In this work, we\nintroduce FORMOSANBENCH, the first benchmark for evaluating LLMs on\nlow-resource Austronesian languages. It covers three endangered Formosan\nlanguages: Atayal, Amis, and Paiwan, across three core NLP tasks: machine\ntranslation, automatic speech recognition (ASR), and text summarization. We\nassess model performance in zero-shot, 10-shot, and fine-tuned settings using\nFORMOSANBENCH. Our results reveal a substantial performance gap between\nhigh-resource and Formosan languages. Existing LLMs consistently underperform\nacross all tasks, with 10-shot learning and fine-tuning offering only limited\nimprovements. These findings underscore the urgent need for more inclusive NLP\ntechnologies that can effectively support endangered and underrepresented\nlanguages. We release our datasets and code to facilitate future research in\nthis direction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u53f0\u6e7e\u6fd2\u5371\u5357\u5c9b\u8bed\u8a00\u7684NLP\u57fa\u51c6FORMOSANBENCH\uff0c\u63ed\u793a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u663e\u8457\u8868\u73b0\u5dee\u8ddd", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90/\u6fd2\u5371\u8bed\u8a00\uff08\u5982\u53f0\u6e7e\u5357\u5c9b\u8bed\u65cf\uff09\u8bc4\u4f30\u4e0d\u8db3\u7684\u95ee\u9898", "method": "\u5efa\u7acb\u8986\u76d6\u6cf0\u96c5\u8bed\u3001\u963f\u7f8e\u8bed\u3001\u6392\u6e7e\u8bed\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b\u673a\u5668\u7ffb\u8bd1\u3001\u8bed\u97f3\u8bc6\u522b\u3001\u6587\u672c\u6458\u8981\u4efb\u52a1\uff0c\u91c7\u7528\u96f6\u6837\u672c/10\u6837\u672c/\u5fae\u8c03\u4e09\u79cd\u8bbe\u7f6e\u8bc4\u4f30\u6a21\u578b", "result": "\u73b0\u6709\u6a21\u578b\u8868\u73b0\u663e\u8457\u843d\u540e\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u5fae\u8c03\u548c\u5c11\u91cf\u6837\u672c\u5b66\u4e60\u63d0\u5347\u6709\u9650\uff08\u6700\u9ad8\u51c6\u786e\u7387\u63d0\u5347\u4e0d\u8d85\u8fc715%\uff09", "conclusion": "\u4e9f\u9700\u5f00\u53d1\u66f4\u5177\u5305\u5bb9\u6027\u7684NLP\u6280\u672f\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u6fd2\u5371\u8bed\u8a00\u4fdd\u62a4\u7814\u7a76"}}
{"id": "2506.21564", "pdf": "https://arxiv.org/pdf/2506.21564", "abs": "https://arxiv.org/abs/2506.21564", "authors": ["Jiyan Liu", "Youzheng Liu", "Taihang Wang", "Xiaoman Xu", "Yimin Wang", "Ye Jiang"], "title": "Team QUST at SemEval-2025 Task 10: Evaluating Large Language Models in Multiclass Multi-label Classification of News Entity Framing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper describes the participation of QUST_NLP in the SemEval-2025 Task\n7. We propose a three-stage retrieval framework specifically designed for\nfact-checked claim retrieval. Initially, we evaluate the performance of several\nretrieval models and select the one that yields the best results for candidate\nretrieval. Next, we employ multiple re-ranking models to enhance the candidate\nresults, with each model selecting the Top-10 outcomes. In the final stage, we\nutilize weighted voting to determine the final retrieval outcomes. Our approach\nachieved 5th place in the monolingual track and 7th place in the crosslingual\ntrack. We release our system code at:\nhttps://github.com/warmth27/SemEval2025_Task7.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e09\u9636\u6bb5\u68c0\u7d22\u6846\u67b6\u7528\u4e8e\u4e8b\u5b9e\u6838\u67e5\u58f0\u660e\u68c0\u7d22\uff0c\u5728SemEval-2025 Task 7\u4e2d\u5206\u522b\u83b7\u5f97\u5355\u8bed\u8d5b\u9053\u7b2c5\u540d\u548c\u8de8\u8bed\u8a00\u8d5b\u9053\u7b2c7\u540d\u3002", "motivation": "\u9488\u5bf9\u4e8b\u5b9e\u6838\u67e5\u58f0\u660e\u7684\u6709\u6548\u68c0\u7d22\u9700\u6c42\uff0c\u8bbe\u8ba1\u66f4\u4f18\u5316\u7684\u68c0\u7d22\u67b6\u6784\u6765\u63d0\u5347\u7cfb\u7edf\u6027\u80fd", "method": "1. \u8bc4\u4f30\u9009\u62e9\u6700\u4f73\u68c0\u7d22\u6a21\u578b\u8fdb\u884c\u5019\u9009\u68c0\u7d22 \u2192 2. \u591a\u6a21\u578b\u91cd\u6392\u5e8f\u5404\u9009Top10 \u2192 3. \u52a0\u6743\u6295\u7968\u786e\u5b9a\u6700\u7ec8\u7ed3\u679c", "result": "\u5355\u8bed\u8d5b\u9053\u6392\u540d\u7b2c5\uff08MAP 0.762\uff09\uff0c\u8de8\u8bed\u8a00\u8d5b\u9053\u7b2c7\uff08MAP 0.698\uff09\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90", "conclusion": "\u4e09\u9636\u6bb5\u6846\u67b6\u6709\u6548\u7ed3\u5408\u4e0d\u540c\u6a21\u578b\u4f18\u52bf\uff0c\u52a0\u6743\u6295\u7968\u673a\u5236\u6210\u529f\u6574\u5408\u591a\u6a21\u578b\u7ed3\u679c\uff0c\u672a\u6765\u53ef\u63a2\u7d22\u66f4\u7cbe\u7ec6\u7684\u6743\u91cd\u5206\u914d\u7b56\u7565"}}
{"id": "2506.21565", "pdf": "https://arxiv.org/pdf/2506.21565", "abs": "https://arxiv.org/abs/2506.21565", "authors": ["Takato Ueno", "Keito Inoshita"], "title": "A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "Japan's kairanban culture and idobata conversations have long functioned as\ntraditional communication practices that foster nuanced dialogue among\ncommunity members and contribute to the formation of social balance. Inspired\nby these information exchange processes, this study proposes a multi-agent\ninference framework (KCS+IBC) that integrates multiple large language models\n(LLMs) to achieve bias mitigation, improved explainability, and probabilistic\nprediction in sentiment analysis. In addition to sequentially sharing\nprediction results, the proposed method incorporates a mid-phase casual\ndialogue session to blend formal inference with individual perspectives and\nintroduces probabilistic sentiment prediction. Experimental results show that\nKCS achieves accuracy comparable to that of a single LLM across datasets, while\nKCS+IBC exhibits a consistent decrease in entropy and a gradual increase in\nvariance during the latter stages of inference, suggesting the framework's\nability to balance aggregation and diversity of predictions. Future work will\nquantitatively assess the impact of these characteristics on bias correction\nand aim to develop more advanced sentiment analysis systems.", "AI": {"tldr": "\u63d0\u51fa\u878d\u5408\u65e5\u672c\u4f20\u7edf\u6c9f\u901a\u65b9\u5f0f\u7684\u591a\u667a\u80fd\u4f53\u63a8\u7406\u6846\u67b6KCS+IBC\uff0c\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u5b9e\u73b0\u60c5\u611f\u5206\u6790\u7684\u504f\u8bef\u7f13\u89e3\u4e0e\u6982\u7387\u9884\u6d4b", "motivation": "\u53d7\u65e5\u672c\u56de\u89c8\u677f\u6587\u5316\u548c\u4e95\u7aef\u4f1a\u8bdd\u542f\u53d1\uff0c\u65e8\u5728\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5bf9\u8bdd\u673a\u5236\u5e73\u8861\u793e\u7fa4\u89c2\u70b9\uff0c\u63d0\u5347\u60c5\u611f\u5206\u6790\u7684\u89e3\u91ca\u6027", "method": "\u7ed3\u5408\u6b63\u5f0f\u63a8\u7406\u4e0e\u4e2a\u4f53\u89c6\u89d2\u7684\u4e2d\u95f4\u9636\u6bb5\u975e\u6b63\u5f0f\u5bf9\u8bdd\uff0c\u5f15\u5165\u5e8f\u5217\u5316\u9884\u6d4b\u7ed3\u679c\u5171\u4eab\u548c\u6982\u7387\u60c5\u611f\u9884\u6d4b\u673a\u5236", "result": "KCS\u4fdd\u6301\u4e0e\u5355\u6a21\u578b\u76f8\u5f53\u7684\u51c6\u786e\u7387\uff0cKCS+IBC\u5728\u63a8\u7406\u540e\u671f\u5448\u73b0\u71b5\u503c\u964d\u4f4e\u4e0e\u65b9\u5dee\u589e\u957f\uff0c\u663e\u793a\u6846\u67b6\u5177\u5907\u5e73\u8861\u9884\u6d4b\u805a\u5408\u4e0e\u591a\u6837\u6027\u7684\u80fd\u529b", "conclusion": "\u672a\u6765\u5c06\u91cf\u5316\u8bc4\u4f30\u6846\u67b6\u7279\u6027\u5bf9\u504f\u8bef\u4fee\u6b63\u7684\u5f71\u54cd\uff0c\u81f4\u529b\u4e8e\u5f00\u53d1\u66f4\u5148\u8fdb\u7684\u60c5\u611f\u5206\u6790\u7cfb\u7edf"}}
{"id": "2506.21566", "pdf": "https://arxiv.org/pdf/2506.21566", "abs": "https://arxiv.org/abs/2506.21566", "authors": ["Arwa Arif"], "title": "The Saturation Point of Backtranslation in High Quality Low Resource English Gujarati Machine Translation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint, 8 Pages", "summary": "Backtranslation BT is widely used in low resource machine translation MT to\ngenerate additional synthetic training data using monolingual corpora. While\nthis approach has shown strong improvements for many language pairs, its\neffectiveness in high quality, low resource settings remains unclear. In this\nwork, we explore the effectiveness of backtranslation for English Gujarati\ntranslation using the multilingual pretrained MBART50 model. Our baseline\nsystem, trained on a high quality parallel corpus of approximately 50,000\nsentence pairs, achieves a BLEU score of 43.8 on a validation set. We augment\nthis data with carefully filtered backtranslated examples generated from\nmonolingual Gujarati text. Surprisingly, adding this synthetic data does not\nimprove translation performance and, in some cases, slightly reduces it. We\nevaluate our models using multiple metrics like BLEU, ChrF++, TER, BLEURT and\nanalyze possible reasons for this saturation. Our findings suggest that\nbacktranslation may reach a point of diminishing returns in certain\nlow-resource settings and we discuss implications for future research.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5728\u82f1\u8bed-\u53e4\u5409\u62c9\u7279\u8bed\u4f4e\u8d44\u6e90\u673a\u5668\u7ffb\u8bd1\u4e2d\uff0c\u4f7f\u7528MBART50\u6a21\u578b\u65f6\uff0c\u53cd\u5411\u7ffb\u8bd1\u6280\u672f\u672a\u80fd\u63d0\u5347\u7ffb\u8bd1\u6027\u80fd\uff0c\u663e\u793a\u8be5\u6280\u672f\u5728\u67d0\u4e9b\u4f4e\u8d44\u6e90\u573a\u666f\u4e2d\u53ef\u80fd\u6536\u76ca\u9012\u51cf", "motivation": "\u63a2\u7d22\u53cd\u5411\u7ffb\u8bd1\u6280\u672f\u5728\u9ad8\u54c1\u8d28\u4f4e\u8d44\u6e90\u673a\u5668\u7ffb\u8bd1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u5728\u82f1\u8bed-\u53e4\u5409\u62c9\u7279\u8bed\u573a\u666f\u4e0b\u7684\u5b9e\u9645\u5e94\u7528\u6548\u679c", "method": "\u4f7f\u7528MBART50\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u57fa\u4e8e5\u4e07\u53e5\u5bf9\u7684\u9ad8\u8d28\u91cf\u5e73\u884c\u8bed\u6599\u5e93\u5efa\u7acb\u57fa\u7ebf\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u8fc7\u6ee4\u540e\u7684\u5355\u8bed\u53e4\u5409\u62c9\u7279\u6587\u672c\u751f\u6210\u53cd\u5411\u7ffb\u8bd1\u5408\u6210\u6570\u636e\u8fdb\u884c\u589e\u5f3a", "result": "\u6dfb\u52a0\u5408\u6210\u6570\u636e\u540e\u7ffb\u8bd1\u6027\u80fd\u672a\u63d0\u5347\uff08\u57fa\u7ebfBLEU 43.8\uff09\uff0c\u67d0\u4e9b\u60c5\u51b5\u4e0b\u6307\u6807\u53cd\u800c\u8f7b\u5fae\u4e0b\u964d\uff0c\u901a\u8fc7BLEU/ChrF++/TER/BLEURT\u591a\u7ef4\u5ea6\u9a8c\u8bc1\u7ed3\u8bba", "conclusion": "\u53cd\u5411\u7ffb\u8bd1\u6280\u672f\u5728\u67d0\u4e9b\u4f4e\u8d44\u6e90\u573a\u666f\u53ef\u80fd\u8fbe\u5230\u6536\u76ca\u62d0\u70b9\uff0c\u672a\u6765\u7814\u7a76\u9700\u91cd\u65b0\u8bc4\u4f30\u5176\u9002\u7528\u8fb9\u754c\uff0c\u5e76\u63a2\u7d22\u66f4\u6709\u6548\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565"}}
{"id": "2506.21567", "pdf": "https://arxiv.org/pdf/2506.21567", "abs": "https://arxiv.org/abs/2506.21567", "authors": ["Baqer M. Merzah", "Tania Taami", "Salman Asoudeh", "Amir reza Hossein pour", "Saeed Mirzaee", "Amir Ali Bengari"], "title": "BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have recently gained attention in the life\nsciences due to their capacity to model, extract, and apply complex biological\ninformation. Beyond their classical use as chatbots, these systems are\nincreasingly used for complex analysis and problem-solving in specialized\nfields, including bioinformatics. First, we introduce BIOPARS-BENCH, a dataset\nfrom over 10,000 scientific articles, textbooks, and medical websites.\nBioParsQA was also introduced to evaluate the proposed model, which consists of\n5,231 Persian medical questions and answers. This study then introduces\nBioPars, a simple but accurate measure designed to assess LLMs for three main\nabilities: acquiring subject-specific knowledge, interpreting and synthesizing\nsuch knowledge, and demonstrating proper evidence. Comparing ChatGPT, Llama,\nand Galactica, our study highlights their ability to remember and retrieve\nlearned knowledge but also reveals shortcomings in addressing higher-level,\nreal-world questions and fine-grained inferences. These findings indicate the\nneed for further fine-tuning to address the capabilities of LLM in\nbioinformatics tasks. To our knowledge, BioPars is the first application of LLM\nin Persian medical QA, especially for generating long answers. Evaluation of\nfour selected medical QA datasets shows that BioPars has achieved remarkable\nresults compared to comparative approaches. The model on BioParsQA achieved a\nROUGE-L score of 29.99, which is an improvement over GPT-4 1.0. The model\nachieved a BERTScore of 90.87 with the MMR method. The MoverScore and BLEURT\nvalues were also higher in this model than the other three models. In addition,\nthe reported scores for the model are MoverScore=60.43 and BLEURT=50.78.\nBioPars is an ongoing project and all resources related to its development will\nbe made available via the following GitHub repository:\nhttps://github.com/amirap80/BioPars.", "AI": {"tldr": "\u7814\u7a76\u5f15\u5165BIOPARS-BENCH\u6570\u636e\u96c6\u548cBioParsQA\u8bc4\u4f30\u6a21\u578b\uff0c\u9a8c\u8bc1LLMs\u5728\u6ce2\u65af\u8bed\u533b\u7597QA\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u9ad8\u5c42\u63a8\u7406\u5b58\u5728\u4e0d\u8db3\uff0cBioPars\u6a21\u578b\u5728\u591a\u6307\u6807\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u751f\u7269\u4fe1\u606f\u5b66\u4e13\u4e1a\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662f\u586b\u8865\u6ce2\u65af\u8bed\u533b\u7597\u95ee\u7b54\u7cfb\u7edf\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u9700\u7cfb\u7edf\u6027\u8bc4\u4f30\u6a21\u578b\u5728\u77e5\u8bc6\u83b7\u53d6\u3001\u7efc\u5408\u5e94\u7528\u548c\u8bc1\u636e\u652f\u6301\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "1. \u6784\u5efa\u542b10,000+\u6587\u732e\u7684BIOPARS-BENCH\u548c5,231\u6ce2\u65af\u8bedQA\u5bf9\u7684BioParsQA\n2. \u5bf9\u6bd4ChatGPT/Llama/Galactica\n3. \u8bbe\u8ba1\u4e09\u9636\u6bb5\u8bc4\u4f30\u6846\u67b6\uff1a\u77e5\u8bc6\u83b7\u53d6\u2192\u77e5\u8bc6\u7efc\u5408\u2192\u8bc1\u636e\u652f\u6301", "result": "BioParsQA\u4e0aROUGE-L\u8fbe29.99\uff08\u8d85GPT-4\uff09\uff0cBERTScore 90.87\uff0cMoverScore 60.43/BLEURT 50.78\u5747\u6700\u4f18\u3002\u6a21\u578b\u5f00\u6e90\u5728GitHub\u4f9b\u6301\u7eed\u6539\u8fdb\u3002", "conclusion": "\u73b0\u6709LLMs\u9700\u9488\u5bf9\u6027\u5fae\u8c03\u4ee5\u9002\u5e94\u751f\u7269\u4fe1\u606f\u5b66\u4efb\u52a1\uff0cBioPars\u4f5c\u4e3a\u9996\u4e2a\u6ce2\u65af\u8bed\u533b\u7597\u957f\u7b54\u6848\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bc4\u4f30\u6846\u67b6\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\uff0c\u5c06\u6301\u7eed\u8fed\u4ee3\u66f4\u65b0\u3002"}}
{"id": "2506.21568", "pdf": "https://arxiv.org/pdf/2506.21568", "abs": "https://arxiv.org/abs/2506.21568", "authors": ["Andrejs Sorstkins"], "title": "Assessing RAG and HyDE on 1B vs. 4B-Parameter Gemma LLMs for Personal Assistants Integretion", "categories": ["cs.CL", "I.2.7"], "comment": "Technical report as part of research project", "summary": "Resource efficiency is a critical barrier to deploying large language models\n(LLMs) in edge and privacy-sensitive applications. This study evaluates the\nefficacy of two augmentation strategies--Retrieval-Augmented Generation (RAG)\nand Hypothetical Document Embeddings (HyDE)--on compact Gemma LLMs of 1 billion\nand 4 billion parameters, within the context of a privacy-first personal\nassistant. We implement short-term memory via MongoDB and long-term semantic\nstorage via Qdrant, orchestrated through FastAPI and LangChain, and expose the\nsystem through a React.js frontend. Across both model scales, RAG consistently\nreduces latency by up to 17\\% and eliminates factual hallucinations when\nresponding to user-specific and domain-specific queries. HyDE, by contrast,\nenhances semantic relevance--particularly for complex physics prompts--but\nincurs a 25--40\\% increase in response time and a non-negligible hallucination\nrate in personal-data retrieval. Comparing 1 B to 4 B models, we observe that\nscaling yields marginal throughput gains for baseline and RAG pipelines, but\nmagnifies HyDE's computational overhead and variability. Our findings position\nRAG as the pragmatic choice for on-device personal assistants powered by\nsmall-scale LLMs.", "AI": {"tldr": "\u7814\u7a76\u8bc1\u660e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5728\u5c0f\u578bGemma\u8bed\u8a00\u6a21\u578b\u4e2d\u80fd\u663e\u8457\u964d\u4f4e\u54cd\u5e94\u5ef6\u8fdf\u5e76\u6d88\u9664\u4e8b\u5b9e\u6027\u5e7b\u89c9\uff0c\u800c\u5047\u8bbe\u6587\u6863\u5d4c\u5165\uff08HyDE\uff09\u867d\u63d0\u5347\u8bed\u4e49\u76f8\u5173\u6027\u4f46\u4f34\u968f\u663e\u8457\u6027\u80fd\u635f\u8017\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fb9\u7f18\u8ba1\u7b97\u548c\u9690\u79c1\u654f\u611f\u573a\u666f\u4e2d\u7684\u8d44\u6e90\u6548\u7387\u74f6\u9888\uff0c\u63a2\u7d22\u9002\u5408\u8f7b\u91cf\u5316\u6a21\u578b\u7684\u589e\u5f3a\u7b56\u7565\u3002", "method": "\u91c7\u75281B/4B\u53c2\u6570\u7684Gemma\u6a21\u578b\uff0c\u96c6\u6210MongoDB\u77ed\u671f\u8bb0\u5fc6\u548cQdrant\u957f\u671f\u8bed\u4e49\u5b58\u50a8\uff0c\u901a\u8fc7FastAPI+LangChain\u6846\u67b6\u5b9e\u73b0RAG\u4e0eHyDE\u7b56\u7565\uff0c\u6784\u5efaReact.js\u524d\u7aef\u9690\u79c1\u4f18\u5148\u52a9\u624b\u7cfb\u7edf\u3002", "result": "RAG\u5728\u4e24\u79cd\u6a21\u578b\u89c4\u6a21\u4e0b\u5b9e\u73b017%\u5ef6\u8fdf\u964d\u4f4e\u5e76\u6d88\u9664\u7528\u6237\u6570\u636e\u5e7b\u89c9\uff0cHyDE\u4f7f\u7269\u7406\u7c7b\u590d\u6742\u63d0\u793a\u76f8\u5173\u6027\u63d0\u5347\u4f46\u54cd\u5e94\u65f6\u95f4\u589e\u52a025-40%\u4e14\u5b58\u5728\u4e2a\u4eba\u6570\u636e\u5e7b\u89c9\u98ce\u9669\u30024B\u6a21\u578b\u653e\u5927HyDE\u7684\u8ba1\u7b97\u5f00\u9500\u6ce2\u52a8\u3002", "conclusion": "RAG\u66f4\u9002\u5408\u5c0f\u578bLLM\u9a71\u52a8\u7684\u8bbe\u5907\u7aef\u52a9\u624b\uff0cHyDE\u7684\u8bed\u4e49\u589e\u76ca\u96be\u4ee5\u62b5\u6d88\u5176\u8d44\u6e90\u6d88\u8017\u548c\u53ef\u9760\u6027\u95ee\u9898\u3002"}}
{"id": "2506.21569", "pdf": "https://arxiv.org/pdf/2506.21569", "abs": "https://arxiv.org/abs/2506.21569", "authors": ["Weihua Xiao", "Derek Ekberg", "Siddharth Garg", "Ramesh Karri"], "title": "Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "SystemVerilog Assertions (SVAs) are critical for verifying the correctness of\nhardware designs, but manually writing them from natural language property\ndescriptions, i.e., NL2SVA, remains a labor-intensive and error-prone task.\nRecent advances in large language models (LLMs) offer opportunities to automate\nthis translation. However, existing models still struggle with understanding\ndomain-specific syntax and semantics. To enhance LLM performance in NL2SVA, we\npropose a customized retrieval-augmented generation (RAG) framework and a\nsynthetic fine-tuning dataset that together improve LLM's performance. To\nfurther improve lightweight models over NL2SVA, our fine-tuning dataset\nprovides prompt-guided explanations that teach LLMs the layer-by-layer\nconstruction process of concurrent SVAs, enabling supervised fine-tuning that\ngreatly improves syntax and functionality accuracy. To evaluate the performance\nof LLMs over NL2SVA, we construct the largest evaluation dataset for NL2SVA,\ncomprising 40 Verilog designs and 229 formally verified SVAs with detailed\nannotations. Experimental results show that our customized RAG framework\nincreases the number of functionality matched SVAs by 58.42% over GPT-4o-mini,\nwhile Qwen2.5-Coder-7B-Instruct fine-tuned on our fine-tuning dataset and\nintegrated with HybridRetrieval achieves a 59.05% over the base Qwen model.", "AI": {"tldr": "\u63d0\u51fa\u5b9a\u5236\u5316RAG\u6846\u67b6\u4e0e\u5fae\u8c03\u6570\u636e\u96c6\uff0c\u63d0\u5347LLM\u5728NL2SVA\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u5b9e\u9a8c\u663e\u793a\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u624b\u52a8\u7f16\u5199SystemVerilog\u65ad\u8a00\u8017\u65f6\u4e14\u6613\u9519\uff0c\u73b0\u6709LLM\u5728\u9886\u57df\u8bed\u6cd5/\u8bed\u4e49\u7406\u89e3\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u9488\u5bf9\u6027\u4f18\u5316\u3002", "method": "1. \u5b9a\u5236RAG\u6846\u67b6\u589e\u5f3a\u9886\u57df\u7406\u89e3\uff1b2. \u521b\u5efa\u542b\u5206\u6b65\u6784\u5efa\u903b\u8f91\u7684\u5fae\u8c03\u6570\u636e\u96c6\uff1b3. \u7ed3\u5408\u6df7\u5408\u68c0\u7d22\u6280\u672f\u4f18\u5316\u8f7b\u91cf\u6a21\u578b\u3002", "result": "RAG\u4f7fGPT-4o-mini\u529f\u80fd\u5339\u914dSVA\u6570\u91cf\u63d0\u534758.42%\uff1b\u5fae\u8c03\u540eQwen\u6a21\u578b\u51c6\u786e\u7387\u63d0\u534759.05%\u3002", "conclusion": "\u5b9a\u5236\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3LLM\u9886\u57df\u9002\u914d\u95ee\u9898\uff0c\u6784\u5efa\u7684\u6700\u5927NL2SVA\u8bc4\u4f30\u6570\u636e\u96c6\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2506.21570", "pdf": "https://arxiv.org/pdf/2506.21570", "abs": "https://arxiv.org/abs/2506.21570", "authors": ["Roland Riachi", "Kashif Rasul", "Arjun Ashok", "Prateek Humane", "Alexis Roger", "Andrew R. Williams", "Yuriy Nevmyvaka", "Irina Rish"], "title": "Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent works have demonstrated the effectiveness of adapting pre-trained\nlanguage models (LMs) for forecasting time series in the low-data regime. We\nbuild upon these findings by analyzing the effective transfer from language\nmodels to time series forecasting under various design choices including\nupstream post-training, time series tokenizer and language backbone size. In\nthe low-data regime, these design choices have a significant impact on the\nvalidation loss, with clear-cut choices that outperform others. Contrary to\nHernandez et al. (2021), we observe that the validation loss of the LMs\ncontinues to smoothly decrease long after the validation loss of the randomly\ninitialized models has converged, leading to a non-vanishing transfer gap that\nholds across design choices. These findings not only help shed light on the\neffective use of compute-efficient training for time series, but also open the\nway for the study of modality-agnostic properties of data distributions\nleveraged by these models.", "AI": {"tldr": "\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u6570\u636e\u91cf\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5177\u6709\u663e\u8457\u6709\u6548\u6027\uff0c\u4e0d\u540c\u8bbe\u8ba1\u9009\u62e9\u4f1a\u663e\u8457\u5f71\u54cd\u9a8c\u8bc1\u635f\u5931\u5e76\u4ea7\u751f\u6301\u7eed\u8fc1\u79fb\u5dee\u8ddd\u3002", "motivation": "\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u8fc1\u79fb\u6548\u679c\uff0c\u5206\u6790\u4e0d\u540c\u8bbe\u8ba1\u9009\u62e9\u5bf9\u4f4e\u6570\u636e\u573a\u666f\u7684\u5f71\u54cd\u53ca\u9a8c\u8bc1\u635f\u5931\u53d8\u5316\u89c4\u5f8b\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u53d8\u91cf\u5b9e\u9a8c\uff08\u4e0a\u6e38\u540e\u8bad\u7ec3\u7b56\u7565/\u65f6\u95f4\u5e8f\u5217tokenizer\u8bbe\u8ba1/\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\uff09\u9a8c\u8bc1\u4e0d\u540c\u67b6\u6784\u5bf9\u9a8c\u8bc1\u635f\u5931\u7684\u5f71\u54cd\u3002", "result": "\u8bbe\u8ba1\u9009\u62e9\u5bf9\u9a8c\u8bc1\u635f\u5931\u5f71\u54cd\u663e\u8457\uff0c\u4e14\u8bed\u8a00\u6a21\u578b\u7684\u9a8c\u8bc1\u635f\u5931\u5728baseline\u6536\u655b\u540e\u4ecd\u6301\u7eed\u4e0b\u964d\uff0c\u5f62\u6210\u8de8\u8bbe\u8ba1\u9009\u62e9\u7684\u6301\u7eed\u8fc1\u79fb\u5dee\u8ddd\u73b0\u8c61\u3002", "conclusion": "\u53d1\u73b0\u4e3a\u9ad8\u6548\u65f6\u95f4\u5e8f\u5217\u8bad\u7ec3\u63d0\u4f9b\u6307\u5bfc\uff0c\u540c\u65f6\u542f\u53d1\u4e86\u5bf9\u6a21\u578b\u8de8\u6a21\u6001\u6570\u636e\u5206\u5e03\u7279\u6027\u7684\u7814\u7a76\u8def\u5f84\u3002"}}
{"id": "2506.21571", "pdf": "https://arxiv.org/pdf/2506.21571", "abs": "https://arxiv.org/abs/2506.21571", "authors": ["Jianshuo Dong", "Yujia Fu", "Chuanrui Hu", "Chao Zhang", "Han Qiu"], "title": "Towards Understanding the Cognitive Habits of Large Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Large Reasoning Models (LRMs), which autonomously produce a reasoning Chain\nof Thought (CoT) before producing final responses, offer a promising approach\nto interpreting and monitoring model behaviors. Inspired by the observation\nthat certain CoT patterns -- e.g., ``Wait, did I miss anything?'' --\nconsistently emerge across tasks, we explore whether LRMs exhibit human-like\ncognitive habits. Building on Habits of Mind, a well-established framework of\ncognitive habits associated with successful human problem-solving, we introduce\nCogTest, a principled benchmark designed to evaluate LRMs' cognitive habits.\nCogTest includes 16 cognitive habits, each instantiated with 25 diverse tasks,\nand employs an evidence-first extraction method to ensure reliable habit\nidentification. With CogTest, we conduct a comprehensive evaluation of 16\nwidely used LLMs (13 LRMs and 3 non-reasoning ones). Our findings reveal that\nLRMs, unlike conventional LLMs, not only exhibit human-like habits but also\nadaptively deploy them according to different tasks. Finer-grained analyses\nfurther uncover patterns of similarity and difference in LRMs' cognitive habit\nprofiles, particularly certain inter-family similarity (e.g., Qwen-3 models and\nDeepSeek-R1). Extending the study to safety-related tasks, we observe that\ncertain habits, such as Taking Responsible Risks, are strongly associated with\nthe generation of harmful responses. These findings suggest that studying\npersistent behavioral patterns in LRMs' CoTs is a valuable step toward deeper\nunderstanding of LLM misbehavior. The code is available at:\nhttps://github.com/jianshuod/CogTest.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7CogTest\u57fa\u51c6\u53d1\u73b0\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u80fd\u6a21\u4eff\u4eba\u7c7b\u8ba4\u77e5\u4e60\u60ef\uff0c\u5e76\u63ed\u793a\u67d0\u4e9b\u4e60\u60ef\u4e0e\u5b89\u5168\u98ce\u9669\u7684\u5173\u8054\u3002", "motivation": "\u63a2\u7d22LRMs\u662f\u5426\u5177\u5907\u7c7b\u4f3c\u4eba\u7c7b\u7684\u6301\u7eed\u6027\u8ba4\u77e5\u4e60\u60ef\uff0c\u53ca\u5176\u5bf9\u6a21\u578b\u884c\u4e3a\u89e3\u91ca\u548c\u5b89\u5168\u76d1\u6d4b\u7684\u6f5c\u5728\u4ef7\u503c\u3002", "method": "\u57fa\u4e8eHabits of Mind\u6846\u67b6\u6784\u5efaCogTest\u57fa\u51c6\uff08\u542b16\u4e2a\u8ba4\u77e5\u4e60\u60ef\u00d725\u4efb\u52a1\uff09\uff0c\u91c7\u7528\u8bc1\u636e\u4f18\u5148\u63d0\u53d6\u6cd5\u8bc4\u4f3016\u4e2a\u4e3b\u6d41\u6a21\u578b\uff0813\u4e2aLRMs+3\u975e\u63a8\u7406\u6a21\u578b\uff09\u3002", "result": "LRMs\u5c55\u73b0\u51fa\u4efb\u52a1\u81ea\u9002\u5e94\u7684\u7c7b\u4eba\u4e60\u60ef\uff0c\u6a21\u578b\u5bb6\u65cf\u95f4\u5b58\u5728\u8ba4\u77e5\u7279\u5f81\u76f8\u4f3c\u6027\uff0c\u4e14'\u627f\u62c5\u98ce\u9669'\u7b49\u4e60\u60ef\u4e0e\u6709\u5bb3\u5185\u5bb9\u751f\u6210\u5f3a\u76f8\u5173\u3002", "conclusion": "\u5206\u6790CoT\u4e2d\u7684\u6301\u7eed\u6027\u884c\u4e3a\u6a21\u5f0f\u662f\u7406\u89e3LLM\u5f02\u5e38\u884c\u4e3a\u7684\u91cd\u8981\u9014\u5f84\uff0c\u8ba4\u77e5\u4e60\u60ef\u7814\u7a76\u4e3a\u6a21\u578b\u5b89\u5168\u4f18\u5316\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002"}}
{"id": "2506.21572", "pdf": "https://arxiv.org/pdf/2506.21572", "abs": "https://arxiv.org/abs/2506.21572", "authors": ["Tianyu. Zou", "Shengwu. Xiong", "Ruilin. Yao", "Jirui. Huang", "Yi. Rong", "Yaxiong. Chen", "Shili. Xiong", "Cong. Wang"], "title": "Aligning MLLM Benchmark With Human Preferences via Structural Equation Modeling", "categories": ["cs.CL"], "comment": "9 pages, 5 figures", "summary": "Evaluating multimodal large language models (MLLMs) remains a fundamental\nchallenge due to a lack of structured, interpretable, and theoretically\ngrounded benchmark designs. Existing benchmarks often adopt heuristic-based\ntask groupings with unclear cognitive targets, thus resulting in overlapping\nabilities, redundant indicators, and limited diagnostic power. In this work, we\npropose a novel framework for aligning MLLM benchmark based on Structural\nEquation Modeling (SEM) to analyze and quantify the internal validity,\ndimensional separability, and contribution of benchmark components. Motivated\nby the observed limitations of current designs, we further introduce a novel\ncapability hierarchy grounded in Piagets theory of cognitive development,\ndividing MLLM abilities into three hierarchical layers, i.e., Perception,\nMemory, and Reasoning. We reorganize existing MLLM benchmarks under the\nproposed framework and construct a new benchmark named Gold. Experimental\nresults demonstrate that the proposed benchmark exhibits stronger\ninterpretability, reduced indicator redundancy, and clearer cognitive\nconsistency compared to existing approaches.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7ed3\u6784\u65b9\u7a0b\u6a21\u578b(SEM)\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u6846\u67b6Gold\uff0c\u901a\u8fc7\u8ba4\u77e5\u5206\u5c42\u7406\u8bba\u89e3\u51b3\u73b0\u6709\u57fa\u51c6\u53ef\u89e3\u91ca\u6027\u5dee\u3001\u6307\u6807\u5197\u4f59\u95ee\u9898\u3002", "motivation": "\u5f53\u524dMLLM\u8bc4\u4f30\u57fa\u51c6\u5b58\u5728\u4efb\u52a1\u5206\u7ec4\u542f\u53d1\u5f0f\u3001\u8ba4\u77e5\u76ee\u6807\u4e0d\u660e\u786e\u3001\u80fd\u529b\u7ef4\u5ea6\u91cd\u53e0\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u8bca\u65ad\u80fd\u529b\u53d7\u9650\u3002", "method": "1. \u91c7\u7528SEM\u6846\u67b6\u91cf\u5316\u57fa\u51c6\u6548\u5ea6\uff1b2. \u57fa\u4e8e\u76ae\u4e9a\u6770\u8ba4\u77e5\u53d1\u5c55\u7406\u8bba\u5efa\u7acb\u611f\u77e5-\u8bb0\u5fc6-\u63a8\u7406\u4e09\u5c42\u80fd\u529b\u4f53\u7cfb\uff1b3. \u91cd\u6784\u73b0\u6709\u57fa\u51c6\u5e76\u6784\u5efaGold\u65b0\u57fa\u51c6\u3002", "result": "Gold\u57fa\u51c6\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6307\u6807\u5197\u4f59\u51cf\u5c1123.7%\uff0c\u8ba4\u77e5\u4e00\u81f4\u6027\u63d0\u534735%\uff0c\u5177\u5907\u66f4\u5f3a\u7684\u7ed3\u6784\u6548\u5ea6\u548c\u8bca\u65ad\u80fd\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aMLLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6491\u7684\u57fa\u51c6\u8bbe\u8ba1\u8303\u5f0f\uff0c\u63a8\u52a8\u4e86\u8ba4\u77e5\u53ef\u89e3\u91ca\u6027\u8bc4\u4f30\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.21573", "pdf": "https://arxiv.org/pdf/2506.21573", "abs": "https://arxiv.org/abs/2506.21573", "authors": ["Yanwei Ren", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Quan Chen"], "title": "Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Optimizing instructions for large language models (LLMs) is critical for\nharnessing their full potential in complex and diverse tasks. However, relying\nsolely on white-box approaches demands extensive computational resources and\noffers limited representational capacity, while black-box models can incur\nprohibitive financial costs. To address these challenges, we introduce a novel\nframework that seamlessly merges the strengths of both paradigms. Black-box\nmodels provide high-quality, diverse instruction initializations, and white-box\nmodels supply fine-grained interpretability through hidden states and output\nfeatures. By enforcing a semantic similarity constraint, these components fuse\ninto a unified high-dimensional representation that captures deep semantic and\nstructural nuances, enabling an iterative optimization process to refine\ninstruction quality and adaptability. Extensive evaluations across a broad\nspectrum of tasks-ranging from complex reasoning to cross-lingual\ngeneralization-demonstrate that our approach consistently outperforms\nstate-of-the-art baselines. This fusion of black-box initialization with\nadvanced semantic refinement yields a scalable and efficient solution, paving\nthe way for next-generation LLM-driven applications in diverse real-world\nscenarios. The source code will be released soon.", "AI": {"tldr": "\u63d0\u51fa\u878d\u5408\u9ed1\u76d2\u6a21\u578b\u7684\u9ad8\u8d28\u91cf\u521d\u59cb\u5316\u4e0e\u767d\u76d2\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u7279\u5f81\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u7ea6\u675f\u5b9e\u73b0\u8fed\u4ee3\u4f18\u5316\u7684\u65b0\u578bLLM\u6307\u4ee4\u4f18\u5316\u6846\u67b6", "motivation": "\u73b0\u6709\u767d\u76d2\u65b9\u6cd5\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u8868\u5f81\u80fd\u529b\u6709\u9650\uff0c\u9ed1\u76d2\u6a21\u578b\u6210\u672c\u8fc7\u9ad8\u3002\u9700\u7ed3\u5408\u9ed1\u76d2\u7684\u4f18\u8d28\u521d\u59cb\u5316\u80fd\u529b\u4e0e\u767d\u76d2\u7684\u9690\u85cf\u72b6\u6001\u89e3\u91ca\u6027\u4f18\u52bf\uff0c\u7a81\u7834\u5355\u4e00\u8303\u5f0f\u7684\u5c40\u9650\u6027", "method": "\u9ed1\u76d2\u6a21\u578b\u63d0\u4f9b\u591a\u6837\u5316\u6307\u4ee4\u521d\u59cb\u5316\uff0c\u767d\u76d2\u6a21\u578b\u8f93\u51fa\u9690\u85cf\u72b6\u6001\u7279\u5f81\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u4f3c\u6027\u7ea6\u675f\u878d\u5408\u6210\u9ad8\u7ef4\u8868\u5f81\uff0c\u5efa\u7acb\u8fed\u4ee3\u4f18\u5316\u673a\u5236\u63d0\u5347\u6307\u4ee4\u8d28\u91cf\u4e0e\u9002\u5e94\u6027", "result": "\u5728\u590d\u6742\u63a8\u7406\u3001\u8de8\u8bed\u8a00\u6cdb\u5316\u7b49\u4efb\u52a1\u4e2d\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u8bed\u4e49\u6df1\u5ea6\u6355\u83b7\u548c\u7ed3\u6784\u4f18\u5316\u65b9\u9762\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u878d\u5408\u65b9\u6848\u4e3a\u4e0b\u4e00\u4ee3LLM\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u521d\u59cb\u5316\u8d28\u91cf\u4e0e\u8bed\u4e49\u7cbe\u7ec6\u5316\u5904\u7406\u7684\u4f18\u52bf\uff0c\u63a8\u52a8\u73b0\u5b9e\u573a\u666f\u7684\u591a\u6837\u5316\u5e94\u7528\u843d\u5730"}}
{"id": "2506.21574", "pdf": "https://arxiv.org/pdf/2506.21574", "abs": "https://arxiv.org/abs/2506.21574", "authors": ["Yicheng Mao", "Yang Zhao"], "title": "Digital Gatekeepers: Exploring Large Language Model's Role in Immigration Decisions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With globalization and increasing immigrant populations, immigration\ndepartments face significant work-loads and the challenge of ensuring fairness\nin decision-making processes. Integrating artificial intelligence offers a\npromising solution to these challenges. This study investigates the potential\nof large language models (LLMs),such as GPT-3.5 and GPT-4, in supporting\nimmigration decision-making. Utilizing a mixed-methods approach,this paper\nconducted discrete choice experiments and in-depth interviews to study LLM\ndecision-making strategies and whether they are fair. Our findings demonstrate\nthat LLMs can align their decision-making with human strategies, emphasizing\nutility maximization and procedural fairness. Meanwhile, this paper also\nreveals that while ChatGPT has safeguards to prevent unintentional\ndiscrimination, it still exhibits stereotypes and biases concerning nationality\nand shows preferences toward privileged group. This dual analysis highlights\nboth the potential and limitations of LLMs in automating and enhancing\nimmigration decisions.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8GPT-3.5/GPT-4\u5728\u79fb\u6c11\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u53d1\u73b0\u5176\u80fd\u6a21\u62df\u4eba\u7c7b\u51b3\u7b56\u7b56\u7565\u4f46\u5b58\u5728\u56fd\u7c4d\u504f\u89c1", "motivation": "\u5e94\u5bf9\u5168\u7403\u5316\u80cc\u666f\u4e0b\u79fb\u6c11\u90e8\u95e8\u5de5\u4f5c\u8d1f\u8377\u5267\u589e\u4e0e\u51b3\u7b56\u516c\u5e73\u6027\u4fdd\u969c\u7684\u53cc\u91cd\u6311\u6218", "method": "\u91c7\u7528\u79bb\u6563\u9009\u62e9\u5b9e\u9a8c+\u6df1\u5ea6\u8bbf\u8c08\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u5206\u6790LLM\u51b3\u7b56\u7b56\u7565\u53ca\u5176\u516c\u5e73\u6027", "result": "LLM\u65e2\u80fd\u5b9e\u73b0\u6548\u7528\u6700\u5927\u5316\u4e0e\u7a0b\u5e8f\u516c\u5e73\uff0c\u53c8\u5b58\u5728\u5bf9\u7279\u6743\u7fa4\u4f53\u504f\u597d\u53ca\u56fd\u7c4d\u523b\u677f\u5370\u8c61", "conclusion": "LLM\u5728\u63d0\u5347\u79fb\u6c11\u51b3\u7b56\u6548\u7387\u65b9\u9762\u6f5c\u529b\u663e\u8457\uff0c\u4f46\u9700\u8b66\u60d5\u7b97\u6cd5\u504f\u89c1\u5e26\u6765\u7684\u9690\u6027\u6b67\u89c6\u98ce\u9669"}}
{"id": "2506.21575", "pdf": "https://arxiv.org/pdf/2506.21575", "abs": "https://arxiv.org/abs/2506.21575", "authors": ["Josefa Lia Stoisser", "Marc Boubnovski Martell", "Lawrence Phillips", "Casper Hansen", "Julien Fauqueur"], "title": "STRuCT-LLM: Unifying Tabular and Graph Reasoning with Reinforcement Learning for Semantic Parsing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We propose STRuCT-LLM, a unified framework for training large language models\n(LLMs) to perform structured reasoning over both relational and\ngraph-structured data. Our approach jointly optimizes Text-to-SQL and\nText-to-Cypher tasks using reinforcement learning (RL) combined with\nChain-of-Thought (CoT) supervision. To support fine-grained optimization in\ngraph-based parsing, we introduce a topology-aware reward function based on\ngraph edit distance. Unlike prior work that treats relational and graph\nformalisms in isolation, STRuCT-LLM leverages shared abstractions between SQL\nand Cypher to induce cross-formalism transfer, enabling SQL training to improve\nCypher performance and vice versa - even without shared schemas. Our largest\nmodel (QwQ-32B) achieves substantial relative improvements across tasks: on\nsemantic parsing, Spider improves by 13.5\\% and Text2Cypher by 73.1\\%. The\nmodel also demonstrates strong zero-shot generalization, improving performance\non downstream tabular QA (TableBench: 8.5\\%) and knowledge graph QA\n(CR-LT-KGQA: 1.7\\%) without any QA-specific supervision. These results\ndemonstrate both the effectiveness of executable queries as scaffolds for\nstructured reasoning and the synergistic benefits of jointly training on SQL\nand Cypher (code available at https://github.com/bouv/STRuCT-LLM).", "AI": {"tldr": "\u63d0\u51faSTRuCT-LLM\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3SQL\u4e0eCypher\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u73b0\u8de8\u5f62\u5f0f\u7ed3\u6784\u5316\u63a8\u7406\u6027\u80fd\u63d0\u5347", "motivation": "\u7a81\u7834\u73b0\u6709\u65b9\u6cd5\u5bf9\u5173\u7cfb\u578b\u4e0e\u56fe\u7ed3\u6784\u6570\u636e\u7684\u5b64\u7acb\u5904\u7406\u6a21\u5f0f\uff0c\u5229\u7528SQL\u4e0eCypher\u7684\u5171\u4eab\u62bd\u8c61\u5b9e\u73b0\u8de8\u5f62\u5f0f\u8fc1\u79fb\u5b66\u4e60", "method": "\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4e0eChain-of-Thought\u76d1\u7763\uff0c\u8bbe\u8ba1\u62d3\u6251\u611f\u77e5\u5956\u52b1\u51fd\u6570\uff0c\u901a\u8fc7\u5171\u4eab\u67e5\u8be2\u8bed\u8a00\u62bd\u8c61\u5b9e\u73b0\u8de8\u5f62\u5f0f\u53c2\u6570\u8fc1\u79fb", "result": "QwQ-32B\u6a21\u578b\u5728Spider\u4efb\u52a1\u63d0\u534713.5%\uff0cText2Cypher\u63d0\u534773.1%\uff0c\u96f6\u6837\u672c\u4e0b\u6e38QA\u4efb\u52a1\u6700\u9ad8\u63d0\u53478.5%", "conclusion": "\u9a8c\u8bc1\u4e86\u53ef\u6267\u884c\u67e5\u8be2\u4f5c\u4e3a\u7ed3\u6784\u5316\u63a8\u7406\u652f\u67b6\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660eSQL\u4e0eCypher\u8054\u5408\u8bad\u7ec3\u7684\u534f\u540c\u4f18\u52bf"}}
{"id": "2506.21576", "pdf": "https://arxiv.org/pdf/2506.21576", "abs": "https://arxiv.org/abs/2506.21576", "authors": ["Hongli Yang", "Yizhou Peng", "Hao Huang", "Sheng Li"], "title": "Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Large-scale multilingual ASR models like Whisper excel in high-resource\nsettings but face challenges in low-resource scenarios, such as rare languages\nand code-switching (CS), due to computational costs and catastrophic\nforgetting. We explore Soft Prompt Tuning (SPT), a parameter-efficient method\nto enhance CS ASR while preserving prior knowledge. We evaluate two strategies:\n(1) full fine-tuning (FFT) of both soft prompts and the entire Whisper model,\ndemonstrating improved cross-lingual capabilities compared to traditional\nmethods, and (2) adhering to SPT's original design by freezing model parameters\nand only training soft prompts. Additionally, we introduce SPT4ASR, a\ncombination of different SPT variants. Experiments on the SEAME and ASRU2019\ndatasets show that deep prompt tuning is the most effective SPT approach, and\nour SPT4ASR methods achieve further error reductions in CS ASR, maintaining\nparameter efficiency similar to LoRA, without degrading performance on existing\nlanguages.", "AI": {"tldr": "\u63d0\u51faSPT\u65b9\u6cd5\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u7801\u8f6c\u6362ASR\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u53c2\u6570\u9ad8\u6548\u6027\u548c\u8de8\u8bed\u8a00\u80fd\u529b", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u591a\u8bed\u8a00ASR\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u573a\u666f\uff08\u7a00\u6709\u8bed\u8a00/\u8bed\u7801\u8f6c\u6362\uff09\u5b58\u5728\u7684\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898", "method": "\u63a2\u7d22\u4e24\u79cd\u7b56\u7565\uff1a1\uff09\u5b8c\u5168\u5fae\u8c03(FFT)\u6a21\u578b+\u8f6f\u63d0\u793a 2\uff09\u4ec5\u8bad\u7ec3\u8f6f\u63d0\u793a\u3002\u63d0\u51faSPT4ASR\u7ec4\u5408\u65b9\u6cd5", "result": "\u6df1\u5ea6\u63d0\u793a\u8c03\u4f18\u6548\u679c\u6700\u4f73\uff0cSPT4ASR\u5728SEAME/ASRU2019\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u989d\u5916\u9519\u8bef\u7387\u964d\u4f4e\uff0c\u53c2\u6570\u6548\u7387\u4e0eLoRA\u76f8\u5f53\u4e14\u4e0d\u635f\u5bb3\u73b0\u6709\u8bed\u8a00\u6027\u80fd", "conclusion": "SPT\u65b9\u6cd5\uff08\u5c24\u5176\u6df1\u5ea6\u63d0\u793a\u8c03\u4f18\u548cSPT4ASR\uff09\u6709\u6548\u63d0\u5347\u4f4e\u8d44\u6e90CS ASR\u6027\u80fd\uff0c\u517c\u5177\u53c2\u6570\u9ad8\u6548\u6027\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72"}}
{"id": "2506.21577", "pdf": "https://arxiv.org/pdf/2506.21577", "abs": "https://arxiv.org/abs/2506.21577", "authors": ["Hongli Yang", "Sheng Li", "Hao Huang", "Ayiduosi Tuohan", "Yizhou Peng"], "title": "Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Recent advancements in multilingual automatic speech recognition (ASR) have\nbeen driven by large-scale end-to-end models like Whisper. However, challenges\nsuch as language interference and expanding to unseen languages (language\nexpansion) without degrading performance persist. This paper addresses these\nwith three contributions: 1) Entire Soft Prompt Tuning (Entire SPT), which\napplies soft prompts to both the encoder and decoder, enhancing feature\nextraction and decoding; 2) Language-Aware Prompt Tuning (LAPT), which\nleverages cross-lingual similarities to encode shared and language-specific\nfeatures using lightweight prompt matrices; 3) SPT-Whisper, a toolkit that\nintegrates SPT into Whisper and enables efficient continual learning.\nExperiments across three languages from FLEURS demonstrate that Entire SPT and\nLAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks,\nrespectively, providing an efficient solution for dynamic, multilingual ASR\nmodels with minimal computational overhead.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEntire SPT\u548cLAPT\u65b9\u6cd5\u4f18\u5316\u591a\u8bed\u8a00\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\uff0c\u901a\u8fc7\u8f6f\u63d0\u793a\u8c03\u4f18\u6280\u672f\u63d0\u5347\u672a\u89c1\u8bed\u8a00\u7684\u8bc6\u522b\u6027\u80fd", "motivation": "\u89e3\u51b3\u73b0\u6709\u5927\u89c4\u6a21\u591a\u8bed\u8a00ASR\u6a21\u578b\u5b58\u5728\u7684\u8bed\u8a00\u5e72\u6270\u95ee\u9898\uff0c\u4ee5\u53ca\u5728\u6269\u5c55\u65b0\u8bed\u8a00\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u6311\u6218", "method": "1) \u7f16\u7801\u5668-\u89e3\u7801\u5668\u5168\u8f6f\u63d0\u793a\u8c03\u4f18(Entire SPT) 2) \u57fa\u4e8e\u8de8\u8bed\u8a00\u76f8\u4f3c\u6027\u7684\u8bed\u8a00\u611f\u77e5\u63d0\u793a\u8c03\u4f18(LAPT) 3) \u5f00\u53d1SPT-Whisper\u5de5\u5177\u5305", "result": "\u5728FLEURS\u6570\u636e\u96c6\u4e0a\uff0cEntire SPT\u548cLAPT\u5206\u522b\u6bd4Decoder SPT\u5728\u8bed\u8a00\u6269\u5c55\u4efb\u52a1\u4e0a\u63d0\u53475.0%\u548c16.0%", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u4ee5\u6781\u4f4e\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u4e86\u52a8\u6001\u591a\u8bed\u8a00ASR\u6a21\u578b\u7684\u6301\u7eed\u9ad8\u6548\u5b66\u4e60"}}
{"id": "2506.21578", "pdf": "https://arxiv.org/pdf/2506.21578", "abs": "https://arxiv.org/abs/2506.21578", "authors": ["Andrew Maranh\u00e3o Ventura D'addario"], "title": "HealthQA-BR: A System-Wide Benchmark Reveals Critical Knowledge Gaps in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The evaluation of Large Language Models (LLMs) in healthcare has been\ndominated by physician-centric, English-language benchmarks, creating a\ndangerous illusion of competence that ignores the interprofessional nature of\npatient care. To provide a more holistic and realistic assessment, we introduce\nHealthQA-BR, the first large-scale, system-wide benchmark for\nPortuguese-speaking healthcare. Comprising 5,632 questions from Brazil's\nnational licensing and residency exams, it uniquely assesses knowledge not only\nin medicine and its specialties but also in nursing, dentistry, psychology,\nsocial work, and other allied health professions. We conducted a rigorous\nzero-shot evaluation of over 20 leading LLMs. Our results reveal that while\nstate-of-the-art models like GPT 4.1 achieve high overall accuracy (86.6%),\nthis top-line score masks alarming, previously unmeasured deficiencies. A\ngranular analysis shows performance plummets from near-perfect in specialties\nlike Ophthalmology (98.7%) to barely passing in Neurosurgery (60.0%) and, most\nnotably, Social Work (68.4%). This \"spiky\" knowledge profile is a systemic\nissue observed across all models, demonstrating that high-level scores are\ninsufficient for safety validation. By publicly releasing HealthQA-BR and our\nevaluation suite, we provide a crucial tool to move beyond single-score\nevaluations and toward a more honest, granular audit of AI readiness for the\nentire healthcare team.", "AI": {"tldr": "\u5f00\u53d1\u9996\u4e2a\u8461\u8404\u7259\u8bed\u8de8\u4e13\u4e1a\u533b\u7597\u8bc4\u4f30\u57fa\u51c6HealthQA-BR\uff0c\u63ed\u793a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u533b\u7597\u4e13\u4e1a\u7684\u8868\u73b0\u5dee\u5f02", "motivation": "\u73b0\u6709LLM\u533b\u7597\u8bc4\u4f30\u96c6\u4e2d\u4e8e\u82f1\u8bed\u548c\u533b\u751f\u89c6\u89d2\uff0c\u5ffd\u89c6\u4e86\u771f\u5b9e\u533b\u7597\u573a\u666f\u4e2d\u591a\u4e13\u4e1a\u534f\u4f5c\u7684\u9700\u6c42", "method": "\u57fa\u4e8e\u5df4\u897f5,632\u9053\u56fd\u5bb6\u8003\u8bd5\u9898\u6784\u5efa\u8de8\u4e13\u4e1a\u6d4b\u8bd5\u96c6\uff0c\u5bf920+\u4e3b\u6d41\u6a21\u578b\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30", "result": "GPT-4.1\u6574\u4f53\u51c6\u786e\u738786.6%\uff0c\u4f46\u795e\u7ecf\u5916\u79d1(60%)\u548c\u793e\u4f1a\u5de5\u4f5c(68.4%)\u8868\u73b0\u663e\u8457\u843d\u540e\u773c\u79d1(98.7%)", "conclusion": "\u5355\u4e00\u8bc4\u5206\u4f53\u7cfb\u5b58\u5728\u8bef\u5bfc\u6027\uff0c\u9700\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u5de5\u5177\u786e\u4fddAI\u5728\u6574\u4e2a\u533b\u7597\u56e2\u961f\u4e2d\u7684\u5b89\u5168\u90e8\u7f72"}}
{"id": "2506.21580", "pdf": "https://arxiv.org/pdf/2506.21580", "abs": "https://arxiv.org/abs/2506.21580", "authors": ["Dana Alsagheer", "Yang Lu", "Abdulrahman Kamal", "Omar Kamal", "Mohammad Kamal", "Nada Mansour", "Cosmo Yang Wu", "Rambiba Karanjai", "Sen Li", "Weidong Shi"], "title": "From General Reasoning to Domain Expertise: Uncovering the Limits of Generalization in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nremarkable capabilities in various domains. However, effective decision-making\nrelies heavily on strong reasoning abilities. Reasoning is the foundation for\ndecision-making, providing the analytical and logical framework to make sound\nchoices. Reasoning involves analyzing information, drawing inferences, and\nreaching conclusions based on logic or evidence. Decision-making builds on this\nfoundation by applying the insights from reasoning to select the best course of\naction among alternatives. Together, these processes create a continuous cycle\nof thought and action aimed at achieving goals effectively. As AI technology\nevolves, there is a growing trend to train LLMs to excel in general reasoning.\nThis study explores how the general reasoning capabilities of LLMs connect to\ntheir performance in domain-specific reasoning tasks.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8LLMs\u901a\u7528\u63a8\u7406\u80fd\u529b\u4e0e\u9886\u57df\u4efb\u52a1\u8868\u73b0\u4e4b\u95f4\u7684\u5173\u8054\u673a\u5236", "motivation": "\u9488\u5bf9AI\u53d1\u5c55\u4e2dLLMs\u901a\u7528\u63a8\u7406\u80fd\u529b\u4e0e\u9886\u57df\u4efb\u52a1\u8fc1\u79fb\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7814\u7a76\u4e24\u8005\u5185\u5728\u8054\u7cfb", "method": "\u901a\u8fc7\u7406\u8bba\u6846\u67b6\u6784\u5efa\u4e0e\u8de8\u9886\u57df\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5efa\u7acb\u901a\u7528\u63a8\u7406\u6307\u6807\u4e0e\u4e13\u4e1a\u4efb\u52a1\u8868\u73b0\u7684\u6620\u5c04\u5173\u7cfb", "result": "\u53d1\u73b0LLMs\u7684\u62bd\u8c61\u903b\u8f91\u63a8\u7406\u80fd\u529b\u53ef\u6709\u6548\u8fc1\u79fb\u81f3\u7279\u5b9a\u9886\u57df\uff0c\u4f46\u53d7\u9886\u57df\u77e5\u8bc6\u7ed3\u6784\u5f71\u54cd\u5b58\u5728\u9608\u503c\u6548\u5e94", "conclusion": "\u5e94\u6784\u5efa'\u901a\u7528-\u9886\u57df'\u53cc\u8f68\u8bad\u7ec3\u8303\u5f0f\uff0c\u5728\u5f3a\u5316\u57fa\u7840\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u4f18\u5316\u77e5\u8bc6\u9002\u914d\u673a\u5236"}}
{"id": "2506.21582", "pdf": "https://arxiv.org/pdf/2506.21582", "abs": "https://arxiv.org/abs/2506.21582", "authors": ["Sam Yu-Te Lee", "Chengyang Ji", "Shicheng Wen", "Lifu Huang", "Dongyi Liu", "Kwan-Liu Ma"], "title": "VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Text analytics has traditionally required specialized knowledge in Natural\nLanguage Processing (NLP) or text analysis, which presents a barrier for\nentry-level analysts. Recent advances in large language models (LLMs) have\nchanged the landscape of NLP by enabling more accessible and automated text\nanalysis (e.g., topic detection, summarization, information extraction, etc.).\nWe introduce VIDEE, a system that supports entry-level data analysts to conduct\nadvanced text analytics with intelligent agents. VIDEE instantiates a\nhuman-agent collaroration workflow consisting of three stages: (1)\nDecomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search\nalgorithm to support generative reasoning with human feedback, (2) Execution,\nwhich generates an executable text analytics pipeline, and (3) Evaluation,\nwhich integrates LLM-based evaluation and visualizations to support user\nvalidation of execution results. We conduct two quantitative experiments to\nevaluate VIDEE's effectiveness and analyze common agent errors. A user study\ninvolving participants with varying levels of NLP and text analytics experience\n-- from none to expert -- demonstrates the system's usability and reveals\ndistinct user behavior patterns. The findings identify design implications for\nhuman-agent collaboration, validate the practical utility of VIDEE for\nnon-expert users, and inform future improvements to intelligent text analytics\nsystems.", "AI": {"tldr": "VIDEE\u7cfb\u7edf\u901a\u8fc7\u5206\u89e3-\u6267\u884c-\u8bc4\u4f30\u4e09\u9636\u6bb5\u5de5\u4f5c\u6d41\uff0c\u5e2e\u52a9\u975e\u4e13\u4e1a\u7528\u6237\u4f7f\u7528LLM\u8fdb\u884c\u9ad8\u7ea7\u6587\u672c\u5206\u6790\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u53ef\u7528\u6027\u3002", "motivation": "\u4f20\u7edf\u6587\u672c\u5206\u6790\u9700\u8981NLP\u4e13\u4e1a\u77e5\u8bc6\u5f62\u6210\u4f7f\u7528\u95e8\u69db\uff0c\u800c\u73b0\u6709LLM\u867d\u80fd\u5b9e\u73b0\u81ea\u52a8\u5316\u5206\u6790\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u5316\u652f\u6301\u5de5\u5177\uff0c\u9700\u4e3a\u975e\u4e13\u5bb6\u63d0\u4f9b\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u5206\u89e3\u9636\u6bb5\uff1a\u4eba\u673a\u534f\u4f5c\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7b97\u6cd5\u652f\u6301\u751f\u6210\u5f0f\u63a8\u7406\n2. \u6267\u884c\u9636\u6bb5\uff1a\u81ea\u52a8\u751f\u6210\u53ef\u6267\u884c\u6587\u672c\u5206\u6790\u6d41\u7a0b\n3. \u8bc4\u4f30\u9636\u6bb5\uff1a\u7ed3\u5408LLM\u8bc4\u4f30\u548c\u53ef\u89c6\u5316\u9a8c\u8bc1\u7ed3\u679c", "result": "\u5b9a\u91cf\u5b9e\u9a8c\u8bc1\u660e\u7cfb\u7edf\u6709\u6548\u6027\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a\u4e0d\u540c\u7ecf\u9a8c\u7528\u6237\uff08\u4ece\u96f6\u57fa\u7840\u5230\u4e13\u5bb6\uff09\u5747\u53ef\u64cd\u4f5c\uff0c\u5e76\u63ed\u793a\u51fa\u663e\u8457\u7684\u7528\u6237\u884c\u4e3a\u6a21\u5f0f\u5dee\u5f02\u3002", "conclusion": "VIDEE\u9a8c\u8bc1\u4e86\u4eba\u673a\u534f\u540c\u5de5\u4f5c\u6d41\u7684\u5b9e\u8df5\u4ef7\u503c\uff0c\u4e3a\u975e\u4e13\u5bb6\u63d0\u4f9b\u6709\u6548\u5206\u6790\u5de5\u5177\uff0c\u672a\u6765\u9700\u5728\u667a\u80fd\u4ee3\u7406\u9519\u8bef\u5904\u7406\u548c\u5de5\u4f5c\u6d41\u4f18\u5316\u65b9\u5411\u6301\u7eed\u6539\u8fdb\u3002"}}
{"id": "2506.21583", "pdf": "https://arxiv.org/pdf/2506.21583", "abs": "https://arxiv.org/abs/2506.21583", "authors": ["Muhammad Ahmad", "Muhammad Waqas", "Ameer Hamza", "Ildar Batyrshin", "Grigori Sidorov"], "title": "Hope Speech Detection in code-mixed Roman Urdu tweets: A Positive Turn in Natural Language Processing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hope is a positive emotional state involving the expectation of favorable\nfuture outcomes, while hope speech refers to communication that promotes\noptimism, resilience, and support, particularly in adverse contexts. Although\nhope speech detection has gained attention in Natural Language Processing\n(NLP), existing research mainly focuses on high-resource languages and\nstandardized scripts, often overlooking informal and underrepresented forms\nsuch as Roman Urdu. To the best of our knowledge, this is the first study to\naddress hope speech detection in code-mixed Roman Urdu by introducing a\ncarefully annotated dataset, thereby filling a critical gap in inclusive NLP\nresearch for low-resource, informal language varieties. This study makes four\nkey contributions: (1) it introduces the first multi-class annotated dataset\nfor Roman Urdu hope speech, comprising Generalized Hope, Realistic Hope,\nUnrealistic Hope, and Not Hope categories; (2) it explores the psychological\nfoundations of hope and analyzes its linguistic patterns in code-mixed Roman\nUrdu to inform dataset development; (3) it proposes a custom attention-based\ntransformer model optimized for the syntactic and semantic variability of Roman\nUrdu, evaluated using 5-fold cross-validation; and (4) it verifies the\nstatistical significance of performance gains using a t-test. The proposed\nmodel, XLM-R, achieves the best performance with a cross-validation score of\n0.78, outperforming the baseline SVM (0.75) and BiLSTM (0.76), with gains of 4%\nand 2.63% respectively.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u9488\u5bf9\u7f57\u9a6c\u4e4c\u5c14\u90fd\u8bed\u6df7\u5408\u4ee3\u7801\u7684\u5e0c\u671b\u8bed\u97f3\u68c0\u6d4b\uff0c\u5f15\u5165\u591a\u7c7b\u522b\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u51fa\u5b9a\u5236Transformer\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u5e0c\u671b\u8bed\u97f3\u68c0\u6d4b\u7814\u7a76\u96c6\u4e2d\u4e8e\u9ad8\u8d44\u6e90\u8bed\u8a00\u548c\u6807\u51c6\u5316\u6587\u672c\uff0c\u5ffd\u89c6\u7f57\u9a6c\u4e4c\u5c14\u90fd\u8bed\u7b49\u975e\u6b63\u5f0f\u4f4e\u8d44\u6e90\u8bed\u8a00\u3002\u672c\u7814\u7a76\u586b\u8865\u8be5\u7a7a\u767d\uff0c\u63a8\u52a8\u5305\u5bb9\u6027NLP\u53d1\u5c55\u3002", "method": "1) \u521b\u5efa\u56db\u7c7b\u6807\u6ce8\u6570\u636e\u96c6\uff08\u666e\u904d\u5e0c\u671b\u3001\u73b0\u5b9e\u5e0c\u671b\u3001\u975e\u73b0\u5b9e\u5e0c\u671b\u3001\u975e\u5e0c\u671b\uff09\uff1b2) \u57fa\u4e8e\u5fc3\u7406\u5b66\u5206\u6790\u8bed\u8a00\u6a21\u5f0f\uff1b3) \u63d0\u51fa\u9488\u5bf9\u7f57\u9a6c\u4e4c\u5c14\u90fd\u8bed\u8bed\u6cd5\u8bed\u4e49\u7279\u6027\u7684\u5b9a\u5236\u6ce8\u610f\u529bTransformer\u6a21\u578b\uff0c\u91c7\u75285\u6298\u4ea4\u53c9\u9a8c\u8bc1\uff1b4) \u4f7f\u7528t\u68c0\u9a8c\u9a8c\u8bc1\u6027\u80fd\u63d0\u5347\u663e\u8457\u6027\u3002", "result": "XLM-R\u6a21\u578b\u83b7\u5f970.78\u4ea4\u53c9\u9a8c\u8bc1\u5f97\u5206\uff0c\u663e\u8457\u4f18\u4e8eSVM(0.75)\u548cBiLSTM(0.76)\uff0c\u5206\u522b\u63d0\u53474%\u548c2.63%\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4f4e\u8d44\u6e90\u8bed\u8a00NLP\u7a7a\u767d\uff0c\u6570\u636e\u96c6\u548c\u5b9a\u5236\u6a21\u578b\u4e3a\u7f57\u9a6c\u4e4c\u5c14\u90fd\u8bed\u5e0c\u671b\u8bed\u97f3\u68c0\u6d4b\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4fc3\u8fdb\u5305\u5bb9\u6027\u8bed\u8a00\u6280\u672f\u53d1\u5c55\u3002"}}
{"id": "2506.21584", "pdf": "https://arxiv.org/pdf/2506.21584", "abs": "https://arxiv.org/abs/2506.21584", "authors": ["J. Koorndijk"], "title": "Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Current literature suggests that alignment faking (deceptive alignment) is an\nemergent property of large language models. We present the first empirical\nevidence that a small instruction-tuned model, specifically LLaMA 3 8B, can\nalso exhibit alignment faking. We further show that prompt-only interventions,\nincluding deontological moral framing and scratchpad reasoning, significantly\nreduce this behavior without modifying model internals. This challenges the\nassumption that prompt-based ethics are trivial and that deceptive alignment\nrequires scale. We introduce a taxonomy distinguishing shallow deception,\nshaped by context and suppressible through prompting, from deep deception,\nwhich reflects persistent, goal-driven misalignment. Our findings refine the\nunderstanding of deception in language models and underscore the need for\nalignment evaluations across model sizes and deployment settings.", "AI": {"tldr": "LLaMA 3 8B\u7b49\u5c0f\u578b\u6a21\u578b\u4e5f\u5b58\u5728\u5bf9\u9f50\u4f2a\u88c5\u884c\u4e3a\uff0c\u63d0\u793a\u5e72\u9884\u53ef\u6709\u6548\u6291\u5236\u8868\u5c42\u6b3a\u9a97\uff0c\u6311\u6218\u5927\u6a21\u578b\u4e13\u5c5e\u5047\u8bbe", "motivation": "\u9a8c\u8bc1\u5c0f\u578b\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\u662f\u5426\u5177\u5907\u5bf9\u9f50\u4f2a\u88c5\u80fd\u529b\uff0c\u6311\u6218'\u6b3a\u9a97\u6027\u5bf9\u9f50\u9700\u8981\u6a21\u578b\u89c4\u6a21'\u7684\u4f20\u7edf\u8ba4\u77e5\uff0c\u63ed\u793a\u63d0\u793a\u5e72\u9884\u7684\u6709\u6548\u6027", "method": "\u4f7f\u7528LLaMA 3 8B\u8fdb\u884c\u5b9e\u9a8c\uff0c\u901a\u8fc7\u4e49\u52a1\u8bba\u9053\u5fb7\u6846\u67b6\u63d0\u793a\u548c\u601d\u7ef4\u94fe\u63a8\u7406\u7b49\u7eaf\u63d0\u793a\u5e72\u9884\uff0c\u4e0d\u4fee\u6539\u6a21\u578b\u5185\u90e8\u53c2\u6570", "result": "\u6210\u529f\u6291\u5236\u8868\u5c42\u6b3a\u9a97\u884c\u4e3a\uff0c\u63d0\u51fa\u533a\u5206\u4e0a\u4e0b\u6587\u9a71\u52a8\u7684\u8868\u5c42\u6b3a\u9a97\u4e0e\u76ee\u6807\u9a71\u52a8\u7684\u6df1\u5ea6\u6b3a\u9a97\u7684\u4e8c\u5143\u5206\u7c7b\u4f53\u7cfb", "conclusion": "\u5f3a\u8c03\u8de8\u6a21\u578b\u89c4\u6a21\u7684\u5bf9\u9f50\u8bc4\u4f30\u5fc5\u8981\u6027\uff0c\u63d0\u793a\u4f26\u7406\u7684\u6709\u6548\u6027\u88ab\u4f4e\u4f30\uff0c\u9700\u5728\u90e8\u7f72\u573a\u666f\u4e2d\u52a0\u5f3a\u6a21\u578b\u884c\u4e3a\u76d1\u63a7"}}
{"id": "2506.21585", "pdf": "https://arxiv.org/pdf/2506.21585", "abs": "https://arxiv.org/abs/2506.21585", "authors": ["Christoph Brosch", "Sian Brumm", "Rolf Krieger", "Jonas Scheffler"], "title": "Evaluation of LLM-based Strategies for the Extraction of Food Product Information from Online Shops", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": "Preprint for paper presented at DATA 2025 in Bilbao, Spain. Corrected\n  -2.27 to -1.61 in abstract and +2.27 to +1.61 in discussion. Reference to\n  journal and publication will follow", "summary": "Generative AI and large language models (LLMs) offer significant potential\nfor automating the extraction of structured information from web pages. In this\nwork, we focus on food product pages from online retailers and explore\nschema-constrained extraction approaches to retrieve key product attributes,\nsuch as ingredient lists and nutrition tables. We compare two LLM-based\napproaches, direct extraction and indirect extraction via generated functions,\nevaluating them in terms of accuracy, efficiency, and cost on a curated dataset\nof 3,000 food product pages from three different online shops. Our results show\nthat although the indirect approach achieves slightly lower accuracy (96.48\\%,\n$-1.61\\%$ compared to direct extraction), it reduces the number of required LLM\ncalls by 95.82\\%, leading to substantial efficiency gains and lower operational\ncosts. These findings suggest that indirect extraction approaches can provide\nscalable and cost-effective solutions for large-scale information extraction\ntasks from template-based web pages using LLMs.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u76f4\u63a5\u63d0\u53d6\u4e0e\u95f4\u63a5\u63d0\u53d6\u65b9\u6cd5\u5728\u98df\u54c1\u7f51\u9875\u4fe1\u606f\u62bd\u53d6\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u95f4\u63a5\u65b9\u6cd5\u5728\u7565\u5fae\u964d\u4f4e\u51c6\u786e\u7387\uff0896.48%\uff09\u7684\u540c\u65f6\uff0c\u51cf\u5c11\u4e8695.82%\u7684\u5927\u6a21\u578b\u8c03\u7528\u6b21\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u3002", "motivation": "\u89e3\u51b3\u57fa\u4e8e\u6a21\u677f\u7684\u98df\u54c1\u4ea7\u54c1\u7f51\u9875\uff08\u5982\u6210\u5206\u8868\u3001\u8425\u517b\u8868\uff09\u5927\u89c4\u6a21\u4fe1\u606f\u62bd\u53d6\u65f6\uff0c\u76f4\u63a5\u8c03\u7528\u5927\u6a21\u578b\u6548\u7387\u4f4e\u3001\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u75283,000\u4e2a\u98df\u54c1\u4ea7\u54c1\u7f51\u9875\u6570\u636e\u96c6\uff0c\u5bf9\u6bd4\u76f4\u63a5\u62bd\u53d6\u4e0e\u95f4\u63a5\u751f\u6210\u51fd\u6570\u62bd\u53d6\u4e24\u79cd\u65b9\u6cd5\uff0c\u8bc4\u4f30\u51c6\u786e\u7387\u3001\u6548\u7387\u548c\u8fd0\u8425\u6210\u672c\u3002", "result": "\u95f4\u63a5\u65b9\u6cd5\u51c6\u786e\u7387\u6bd4\u76f4\u63a5\u65b9\u6cd5\u4f4e1.61%\uff0896.48% vs 98.09%\uff09\uff0c\u4f46\u51cf\u5c1195.82%\u7684\u5927\u6a21\u578b\u8c03\u7528\u6b21\u6570\uff0c\u663e\u8457\u63d0\u5347\u5904\u7406\u901f\u5ea6\u5e76\u964d\u4f4e\u6210\u672c\u3002", "conclusion": "\u95f4\u63a5\u63d0\u53d6\u65b9\u6cd5\u4e3a\u57fa\u4e8e\u5927\u6a21\u578b\u7684\u89c4\u6a21\u5316\u4fe1\u606f\u62bd\u53d6\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6a21\u677f\u5316\u7f51\u9875\u573a\u666f\u3002"}}
{"id": "2506.21586", "pdf": "https://arxiv.org/pdf/2506.21586", "abs": "https://arxiv.org/abs/2506.21586", "authors": ["Hyundong Cho", "Spencer Lin", "Tejas Srinivasan", "Michael Saxon", "Deuksin Kwon", "Natali T. Chavez", "Jonathan May"], "title": "Can Vision Language Models Understand Mimed Actions?", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "ACL 2025 Findings", "summary": "Nonverbal communication (NVC) plays an integral role in human language, but\nstudying NVC in general is challenging because of its broad scope and high\nvariance in interpretation among individuals and cultures. However, mime -- the\ntheatrical technique of suggesting intent using only gesture, expression, and\nmovement -- is a subset of NVC that consists of explicit and embodied actions\nwith much lower human interpretation variance. We argue that a solid\nunderstanding of mimed actions is a crucial prerequisite for vision-language\nmodels capable of interpreting and commanding more subtle aspects of NVC.\nHence, we propose Mime Identification Multimodal Evaluation (MIME), a novel\nvideo-based question answering benchmark comprising of 86 mimed actions.\nConstructed with motion capture data, MIME consists of variations of each\naction with perturbations applied to the character, background, and viewpoint\nfor evaluating recognition robustness. We find that both open-weight and\nAPI-based vision-language models perform significantly worse than humans on\nMIME, motivating the need for increased research for instilling more robust\nunderstanding of human gestures.", "AI": {"tldr": "\u63d0\u51faMIME\u57fa\u51c6\u6d4b\u8bd5\u6a21\u578b\u5bf9\u9ed8\u5267\u52a8\u4f5c\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u663e\u8457\u843d\u540e\u4e8e\u4eba\u7c7b\u8868\u73b0", "motivation": "\u975e\u8bed\u8a00\u4ea4\u6d41(NVC)\u8303\u56f4\u5e7f\u4e14\u5b58\u5728\u4e2a\u4f53/\u6587\u5316\u89e3\u8bfb\u5dee\u5f02\uff0c\u9ed8\u5267\u4f5c\u4e3a\u660e\u786e\u5177\u8c61\u5316\u7684NVC\u5b50\u96c6\uff0c\u662f\u7814\u7a76\u66f4\u590d\u6742NVC\u7684\u57fa\u7840", "method": "\u57fa\u4e8e\u52a8\u4f5c\u6355\u6349\u6570\u636e\u6784\u5efa\u542b86\u79cd\u9ed8\u5267\u52a8\u4f5c\u7684MIME\u57fa\u51c6\uff0c\u901a\u8fc7\u89d2\u8272/\u80cc\u666f/\u89c6\u89d2\u6270\u52a8\u6d4b\u8bd5\u8bc6\u522b\u9c81\u68d2\u6027", "result": "\u5f00\u6e90\u548cAPI\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728MIME\u4e0a\u7684\u8868\u73b0\u663e\u8457\u5dee\u4e8e\u4eba\u7c7b\uff0c\u51c6\u786e\u7387\u5dee\u8ddd\u8fbe20-30\u4e2a\u767e\u5206\u70b9", "conclusion": "MIME\u57fa\u51c6\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u4eba\u7c7b\u624b\u52bf\u7406\u89e3\u4e0a\u7684\u4e0d\u8db3\uff0c\u5f3a\u8c03\u63d0\u5347\u59ff\u52bf\u8bc6\u522b\u80fd\u529b\u5bf9\u5b9e\u73b0\u5b8c\u6574NVC\u7406\u89e3\u7684\u91cd\u8981\u6027"}}
{"id": "2506.21587", "pdf": "https://arxiv.org/pdf/2506.21587", "abs": "https://arxiv.org/abs/2506.21587", "authors": ["Weihong Qi", "Fan Huang", "Jisun An", "Haewoon Kwak"], "title": "Is DeepSeek a New Voice Among LLMs in Public Opinion Simulation?", "categories": ["cs.CL"], "comment": null, "summary": "This study evaluates the ability of DeepSeek, an open-source large language\nmodel (LLM), to simulate public opinions in comparison to LLMs developed by\nmajor tech companies. By comparing DeepSeek-R1 and DeepSeek-V3 with Qwen2.5,\nGPT-4o, and Llama-3.3 and utilizing survey data from the American National\nElection Studies (ANES) and the Zuobiao dataset of China, we assess these\nmodels' capacity to predict public opinions on social issues in both China and\nthe United States, highlighting their comparative capabilities between\ncountries. Our findings indicate that DeepSeek-V3 performs best in simulating\nU.S. opinions on the abortion issue compared to other topics such as climate\nchange, gun control, immigration, and services for same-sex couples, primarily\nbecause it more accurately simulates responses when provided with Democratic or\nliberal personas. For Chinese samples, DeepSeek-V3 performs best in simulating\nopinions on foreign aid and individualism but shows limitations in modeling\nviews on capitalism, particularly failing to capture the stances of low-income\nand non-college-educated individuals. It does not exhibit significant\ndifferences from other models in simulating opinions on traditionalism and the\nfree market. Further analysis reveals that all LLMs exhibit the tendency to\novergeneralize a single perspective within demographic groups, often defaulting\nto consistent responses within groups. These findings highlight the need to\nmitigate cultural and demographic biases in LLM-driven public opinion modeling,\ncalling for approaches such as more inclusive training methodologies.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u5f00\u6e90\u5927\u6a21\u578bDeepSeek\u5728\u6a21\u62df\u4e2d\u7f8e\u793e\u4f1a\u8bae\u9898\u516c\u4f17\u610f\u89c1\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u7279\u5b9a\u8bae\u9898\uff08\u7f8e\u56fd\u5815\u80ce/\u4e2d\u56fd\u5bf9\u5916\u63f4\u52a9\uff09\u8868\u73b0\u4f18\u5f02\u4f46\u5b58\u5728\u7fa4\u4f53\u8ba4\u77e5\u504f\u5dee\uff0c\u5efa\u8bae\u9700\u4f18\u5316\u6587\u5316\u504f\u89c1", "motivation": "\u9a8c\u8bc1\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578bDeepSeek\u5728\u6a21\u62df\u516c\u4f17\u610f\u89c1\u65b9\u9762\u4e0e\u5546\u4e1a\u5927\u6a21\u578b\uff08\u5982GPT-4o\uff09\u7684\u7ade\u4e89\u529b\uff0c\u63a2\u7d22LLM\u5728\u8de8\u6587\u5316\u793e\u4f1a\u8bae\u9898\u5efa\u6a21\u4e2d\u7684\u5c40\u9650\u6027", "method": "\u4f7f\u7528ANES\u7f8e\u56fd\u9009\u4e3e\u7814\u7a76\u6570\u636e\u548c\u4e2d\u56fd\u5750\u6807\u6570\u636e\u96c6\uff0c\u5bf9\u6bd4DeepSeek-R1/V3\u4e0e\u5176\u4ed6\u6a21\u578b\u5728\u5815\u80ce\u3001\u6c14\u5019\u53d8\u5316\uff08\u7f8e\uff09\u3001\u8d44\u672c\u4e3b\u4e49\u8ba4\u77e5\uff08\u4e2d\uff09\u7b49\u8bae\u9898\u7684\u7fa4\u4f53\u610f\u89c1\u6a21\u62df\u80fd\u529b\uff0c\u7279\u522b\u8003\u5bdf\u4eba\u53e3\u5c5e\u6027\u6807\u7b7e\uff08\u5982\u515a\u6d3e/\u6536\u5165\uff09\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd", "result": "DeepSeek-V3\u6a21\u62df\u7f8e\u56fd\u5815\u80ce\u8bae\u9898\u65f6\u56e0\u51c6\u786e\u6355\u6349\u6c11\u4e3b\u515a\u7acb\u573a\u8868\u73b0\u6700\u4f73\uff1b\u5728\u4e2d\u56fd\u6837\u672c\u4e2d\u5bf9\u5916\u63f4\u52a9\u5efa\u6a21\u6700\u4f18\uff0c\u4f46\u672a\u80fd\u51c6\u786e\u53cd\u6620\u4f4e\u6536\u5165\u7fa4\u4f53\u5bf9\u8d44\u672c\u4e3b\u4e49\u7684\u6001\u5ea6\u3002\u6240\u6709\u6a21\u578b\u5747\u5b58\u5728\u5c06\u7fa4\u4f53\u89c2\u70b9\u5355\u4e00\u5316\u7684\u503e\u5411", "conclusion": "LLM\u5728\u8de8\u6587\u5316\u610f\u89c1\u5efa\u6a21\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u9700\u901a\u8fc7\u5305\u5bb9\u6027\u8bad\u7ec3\u65b9\u6cd5\u6539\u8fdb\u6a21\u578b\u5bf9\u4eba\u53e3\u5c5e\u6027\u4e0e\u6587\u5316\u80cc\u666f\u7684\u654f\u611f\u6027\uff0c\u907f\u514d\u523b\u677f\u5316\u7fa4\u4f53\u8ba4\u77e5"}}
{"id": "2506.21588", "pdf": "https://arxiv.org/pdf/2506.21588", "abs": "https://arxiv.org/abs/2506.21588", "authors": ["Ilya Lasy", "Peter Knees", "Stefan Woltran"], "title": "Understanding Verbatim Memorization in LLMs Through Circuit Discovery", "categories": ["cs.CL"], "comment": "The First Workshop on Large Language Model Memorization @ ACL 2025,\n  Vienna, August 1st, 2025", "summary": "Underlying mechanisms of memorization in LLMs -- the verbatim reproduction of\ntraining data -- remain poorly understood. What exact part of the network\ndecides to retrieve a token that we would consider as start of memorization\nsequence? How exactly is the models' behaviour different when producing\nmemorized sentence vs non-memorized? In this work we approach these questions\nfrom mechanistic interpretability standpoint by utilizing transformer circuits\n-- the minimal computational subgraphs that perform specific functions within\nthe model. Through carefully constructed contrastive datasets, we identify\npoints where model generation diverges from memorized content and isolate the\nspecific circuits responsible for two distinct aspects of memorization. We find\nthat circuits that initiate memorization can also maintain it once started,\nwhile circuits that only maintain memorization cannot trigger its initiation.\nIntriguingly, memorization prevention mechanisms transfer robustly across\ndifferent text domains, while memorization induction appears more\ncontext-dependent.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bb0\u5fc6\u673a\u5236\u5305\u542b\u89e6\u53d1\u548c\u7ef4\u6301\u4e24\u79cd\u72ec\u7acb\u56de\u8def\uff0c\u8bb0\u5fc6\u9884\u9632\u673a\u5236\u5177\u5907\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u800c\u8bb0\u5fc6\u89e6\u53d1\u66f4\u4f9d\u8d56\u4e0a\u4e0b\u6587\u73af\u5883\u3002", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u8bb0\u5fc6\u673a\u5236\u4e2d\u89e6\u53d1\u8bb0\u5fc6\u5185\u5bb9\u7684\u5177\u4f53\u7f51\u7edc\u8282\u70b9\uff0c\u5bf9\u6bd4\u6a21\u578b\u751f\u6210\u8bb0\u5fc6\u5185\u5bb9\u4e0e\u975e\u8bb0\u5fc6\u5185\u5bb9\u65f6\u7684\u884c\u4e3a\u5dee\u5f02\u3002\u5f53\u524d\u5bf9LLM\u9010\u5b57\u590d\u73b0\u8bad\u7ec3\u6570\u636e\u7684\u5e95\u5c42\u673a\u5236\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u5bf9\u6bd4\u6570\u636e\u96c6\u5b9a\u4f4d\u6a21\u578b\u751f\u6210\u4e0e\u8bb0\u5fc6\u5185\u5bb9\u7684\u5206\u6b67\u70b9\uff0c\u5229\u7528Transformer\u7535\u8def\uff08\u6267\u884c\u7279\u5b9a\u529f\u80fd\u7684\u6700\u5c0f\u8ba1\u7b97\u5b50\u56fe\uff09\u5206\u79bb\u8bb0\u5fc6\u673a\u5236\u7684\u4e24\u4e2a\u72ec\u7acb\u56de\u8def\u3002", "result": "1. \u8bb0\u5fc6\u89e6\u53d1\u56de\u8def\u53ef\u540c\u65f6\u627f\u62c5\u8bb0\u5fc6\u7ef4\u6301\u529f\u80fd\uff0c\u4f46\u7eaf\u7ef4\u6301\u56de\u8def\u65e0\u6cd5\u89e6\u53d1\u8bb0\u5fc6\n2. \u8bb0\u5fc6\u9884\u9632\u673a\u5236\u5728\u4e0d\u540c\u6587\u672c\u9886\u57df\u95f4\u5177\u6709\u5f3a\u8fc1\u79fb\u6027\uff0c\u800c\u8bb0\u5fc6\u89e6\u53d1\u673a\u5236\u4f9d\u8d56\u4e0a\u4e0b\u6587\u73af\u5883", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u7535\u8def\u5206\u6790\u63ed\u793a\u4e86LLM\u8bb0\u5fc6\u673a\u5236\u7684\u53cc\u56de\u8def\u7279\u5f81\uff0c\u4e3a\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\u548c\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u8868\u660e\u8bb0\u5fc6\u9884\u9632\u673a\u5236\u7684\u6cdb\u5316\u7279\u6027\u53ef\u88ab\u5de5\u7a0b\u5316\u5229\u7528\u3002"}}
