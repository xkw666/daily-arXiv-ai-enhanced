<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 79]
- [cs.GR](#cs.GR) [Total: 7]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CV](#cs.CV) [Total: 5]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [cs.LG](#cs.LG) [Total: 11]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Conservative Bias in Large Language Models: Measuring Relation Predictions](https://arxiv.org/abs/2506.08120)
*Toyin Aguda,Erik Wilson,Allan Anzagira,Simerjot Kaur,Charese Smiley*

Main category: cs.CL

TL;DR: 大语言模型在关系抽取中存在保守偏误，优先选择无信息标签而非错误标签，导致信息丢失但安全性提升


<details>
  <summary>Details</summary>
Motivation: 发现LLMs在关系抽取任务中频繁使用No_Relation标签的保守偏误行为，导致显著信息损失，需量化保守偏误与幻觉的权衡

Method: 使用SBERT和多种提示策略，通过语义相似度分析比较受限提示与半开放提示下的模型行为

Result: 保守偏误发生率是幻觉的两倍，语义相似度分析验证了保守选择与开放生成的关联性

Conclusion: 需要开发新的评估框架平衡信息完整性与准确性，为受限场景的模型设计提供新视角

Abstract: Large language models (LLMs) exhibit pronounced conservative bias in relation
extraction tasks, frequently defaulting to No_Relation label when an
appropriate option is unavailable. While this behavior helps prevent incorrect
relation assignments, our analysis reveals that it also leads to significant
information loss when reasoning is not explicitly included in the output. We
systematically evaluate this trade-off across multiple prompts, datasets, and
relation types, introducing the concept of Hobson's choice to capture scenarios
where models opt for safe but uninformative labels over hallucinated ones. Our
findings suggest that conservative bias occurs twice as often as hallucination.
To quantify this effect, we use SBERT and LLM prompts to capture the semantic
similarity between conservative bias behaviors in constrained prompts and
labels generated from semi-constrained and open-ended prompts.

</details>


### [2] [QA-LIGN: Aligning LLMs through Constitutionally Decomposed QA](https://arxiv.org/abs/2506.08123)
*Jacob Dineen,Aswin RRV,Qin Liu,Zhikun Xu,Xiao Ye,Ming Shen,Zhaonan Li,Shijie Lu,Chitta Baral,Muhao Chen,Ben Zhou*

Main category: cs.CL

TL;DR: QA-LIGN提出了一种可解释的奖励分解方法，通过原则特异性评估问题实现透明化模型对齐，在保持性能的同时提升可控性。


<details>
  <summary>Details</summary>
Motivation: 传统基于奖励的模型对齐方法将多维反馈压缩为单一标量奖励，导致可解释性差、不同原则间的权衡关系不透明。

Method: 开发QA-LIGN框架，通过自动生成原则相关评估问题，为每个宪法原则建立独立奖励组件，替代传统黑箱奖励模型。

Result: 在未审查大模型上的实验表明，该方法在保持与DPO基线相当/更优性能的同时，显著提升对齐过程的可解释性和适应性。

Conclusion: 该研究在无需牺牲模型性能的前提下，推动了语言模型对齐过程的可解释性与可控性，为安全可靠的AI系统提供了新方向。

Abstract: Alignment of large language models with explicit principles (such as
helpfulness, honesty, and harmlessness) is crucial for ensuring safe and
reliable AI systems. However, standard reward-based alignment methods typically
collapse diverse feedback into a single scalar reward, entangling multiple
objectives into one opaque training signal, which hinders interpretability. In
this work, we introduce QA-LIGN, an automatic symbolic reward decomposition
approach that preserves the structure of each constitutional principle within
the reward mechanism. Instead of training a black-box reward model that outputs
a monolithic score, QA-LIGN formulates principle-specific evaluation questions
and derives separate reward components for each principle, making it a drop-in
reward model replacement. Experiments aligning an uncensored large language
model with a set of constitutional principles demonstrate that QA-LIGN offers
greater transparency and adaptability in the alignment process. At the same
time, our approach achieves performance on par with or better than a DPO
baseline. Overall, these results represent a step toward more interpretable and
controllable alignment of language models, achieved without sacrificing
end-task performance.

</details>


### [3] [EconWebArena: Benchmarking Autonomous Agents on Economic Tasks in Realistic Web Environments](https://arxiv.org/abs/2506.08136)
*Zefang Liu,Yinzhu Quan*

Main category: cs.CL

TL;DR: EconWebArena是一个基于真实网络环境的多模态经济任务评估基准，包含360个权威网站任务，用于测试智能体的网络经济推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏对权威经济数据源的适应性测试，需构建更贴近真实场景的经济网络智能评估体系。

Method: 通过LLMs生成候选任务→人工筛选→多模态LLM代理测试，进行视觉锚定、规划推理和交互设计的分层评估。

Result: 主流模型在数据锚定、跨模态理解和多步导航中存在显著性能差距（如规划推理能力提升仅带来6%准确率增长）。

Conclusion: 该基准揭示了经济网络智能的三大核心挑战：动态数据锚定、多模态交互理解、复杂工作流管理，为领域研究提供标准化测试平台。

Abstract: We introduce EconWebArena, a benchmark for evaluating autonomous agents on
complex, multimodal economic tasks in realistic web environments. The benchmark
comprises 360 curated tasks from 82 authoritative websites spanning domains
such as macroeconomics, labor, finance, trade, and public policy. Each task
challenges agents to navigate live websites, interpret structured and visual
content, interact with real interfaces, and extract precise, time-sensitive
data through multi-step workflows. We construct the benchmark by prompting
multiple large language models (LLMs) to generate candidate tasks, followed by
rigorous human curation to ensure clarity, feasibility, and source reliability.
Unlike prior work, EconWebArena emphasizes fidelity to authoritative data
sources and the need for grounded web-based economic reasoning. We evaluate a
diverse set of state-of-the-art multimodal LLMs as web agents, analyze failure
cases, and conduct ablation studies to assess the impact of visual grounding,
plan-based reasoning, and interaction design. Our results reveal substantial
performance gaps and highlight persistent challenges in grounding, navigation,
and multimodal understanding, positioning EconWebArena as a rigorous testbed
for economic web intelligence.

</details>


### [4] [Multilingual Hate Speech Detection in Social Media Using Translation-Based Approaches with Large Language Models](https://arxiv.org/abs/2506.08147)
*Muhammad Usman,Muhammad Ahmad,M. Shahiki Tash,Irina Gelbukh,Rolando Quintero Tellez,Grigori Sidorov*

Main category: cs.CL

TL;DR: 提出结合注意力机制与Transformer模型的多语言仇恨言论检测框架，在三语数据集上取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 社交媒体仇恨言论威胁在线安全，但乌尔都语研究存在空白，需开发有效的多语言检测方案

Method: 构建10,193条三语平衡数据集，采用注意力层增强的Transformer模型（GPT-3.5/Qwen 72B）与传统模型对比

Result: 多语言联合模型F1达0.88（较基线提升7.32%），英语/西班牙语/乌尔都语分别达0.87/0.85/0.81

Conclusion: 该框架为全球数字社区安全提供有效解决方案，推动多语言内容审核技术进步

Abstract: Social media platforms are critical spaces for public discourse, shaping
opinions and community dynamics, yet their widespread use has amplified harmful
content, particularly hate speech, threatening online safety and inclusivity.
While hate speech detection has been extensively studied in languages like
English and Spanish, Urdu remains underexplored, especially using
translation-based approaches. To address this gap, we introduce a trilingual
dataset of 10,193 tweets in English (3,834 samples), Urdu (3,197 samples), and
Spanish (3,162 samples), collected via keyword filtering, with a balanced
distribution of 4,849 Hateful and 5,344 Not-Hateful labels. Our methodology
leverages attention layers as a precursor to transformer-based models and large
language models (LLMs), enhancing feature extraction for multilingual hate
speech detection. For non-transformer models, we use TF-IDF for feature
extraction. The dataset is benchmarked using state-of-the-art models, including
GPT-3.5 Turbo and Qwen 2.5 72B, alongside traditional machine learning models
like SVM and other transformers (e.g., BERT, RoBERTa). Three annotators,
following rigorous guidelines, ensured high dataset quality, achieving a
Fleiss' Kappa of 0.821. Our approach, integrating attention layers with GPT-3.5
Turbo and Qwen 2.5 72B, achieves strong performance, with macro F1 scores of
0.87 for English (GPT-3.5 Turbo), 0.85 for Spanish (GPT-3.5 Turbo), 0.81 for
Urdu (Qwen 2.5 72B), and 0.88 for the joint multilingual model (Qwen 2.5 72B).
These results reflect improvements of 8.75% in English (over SVM baseline
0.80), 8.97% in Spanish (over SVM baseline 0.78), 5.19% in Urdu (over SVM
baseline 0.77), and 7.32% in the joint multilingual model (over SVM baseline
0.82). Our framework offers a robust solution for multilingual hate speech
detection, fostering safer digital communities worldwide.

</details>


### [5] [ETT-CKGE: Efficient Task-driven Tokens for Continual Knowledge Graph Embedding](https://arxiv.org/abs/2506.08158)
*Lijing Zhu,Qizhen Lan,Qing Tian,Wenbo Sun,Li Yang,Lu Xia,Yixin Xie,Xi Xiao,Tiehang Duan,Cui Tao,Shuteng Niu*

Main category: cs.CL

TL;DR: 提出ETT-CKGE方法，通过任务驱动令牌实现高效持续知识图谱嵌入，替代人工评分机制，显著提升训练效率和可扩展性


<details>
  <summary>Details</summary>
Motivation: 现有持续知识图谱嵌入方法因依赖人工设计的节点重要性评分和复杂的图遍历计算，导致效率低下、可扩展性差及知识保存不充分

Method: 引入可学习的任务驱动令牌捕捉任务相关信号，通过令牌掩码嵌入对齐实现跨快照的知识迁移，仅需矩阵运算即可完成

Result: 在6个基准数据集上验证，ETT-CKGE在预测性能上优于现有方法，训练速度提升9.7倍，内存消耗降低89%

Conclusion: 该方法通过任务驱动令牌有效解决持续学习中的效率瓶颈，使知识图谱嵌入更适用于实际应用场景，代码已开源

Abstract: Continual Knowledge Graph Embedding (CKGE) seeks to integrate new knowledge
while preserving past information. However, existing methods struggle with
efficiency and scalability due to two key limitations: (1) suboptimal knowledge
preservation between snapshots caused by manually designed node/relation
importance scores that ignore graph dependencies relevant to the downstream
task, and (2) computationally expensive graph traversal for node/relation
importance calculation, leading to slow training and high memory overhead. To
address these limitations, we introduce ETT-CKGE (Efficient, Task-driven,
Tokens for Continual Knowledge Graph Embedding), a novel task-guided CKGE
method that leverages efficient task-driven tokens for efficient and effective
knowledge transfer between snapshots. Our method introduces a set of learnable
tokens that directly capture task-relevant signals, eliminating the need for
explicit node scoring or traversal. These tokens serve as consistent and
reusable guidance across snapshots, enabling efficient token-masked embedding
alignment between snapshots. Importantly, knowledge transfer is achieved
through simple matrix operations, significantly reducing training time and
memory usage. Extensive experiments across six benchmark datasets demonstrate
that ETT-CKGE consistently achieves superior or competitive predictive
performance, while substantially improving training efficiency and scalability
compared to state-of-the-art CKGE methods. The code is available at:
https://github.com/lijingzhu1/ETT-CKGE/tree/main

</details>


### [6] [Can Artificial Intelligence Write Like Borges? An Evaluation Protocol for Spanish Microfiction](https://arxiv.org/abs/2506.08172)
*Gerardo Aleman Manzanarez,Nora de la Cruz Arana,Jorge Garcia Flores,Yobany Garcia Medina,Raul Monroy,Nathalie Pernelle*

Main category: cs.CL

TL;DR: 提出GrAImes评估协议，基于文学理论构建AI生成微型小说的文学价值多维度评估框架


<details>
  <summary>Details</summary>
Motivation: 现有AI文本生成技术缺乏对文学审美质量的系统性评估标准，需建立理论驱动的客观评价体系

Method: 从文学理论出发，整合主题连贯性、文本清晰度、阐释深度和美学质量四个维度构建评估矩阵

Result: 协议通过文学专家和爱好者双重验证，证实能有效评估AI微小说的文学价值

Conclusion: GrAImes为AI文学创作质量评估建立理论基准，推动计算创造力与文学批评的跨学科融合

Abstract: Automated story writing has been a subject of study for over 60 years. Large
language models can generate narratively consistent and linguistically coherent
short fiction texts. Despite these advancements, rigorous assessment of such
outputs for literary merit - especially concerning aesthetic qualities - has
received scant attention. In this paper, we address the challenge of evaluating
AI-generated microfictions and argue that this task requires consideration of
literary criteria across various aspects of the text, such as thematic
coherence, textual clarity, interpretive depth, and aesthetic quality. To
facilitate this, we present GrAImes: an evaluation protocol grounded in
literary theory, specifically drawing from a literary perspective, to offer an
objective framework for assessing AI-generated microfiction. Furthermore, we
report the results of our validation of the evaluation protocol, as answered by
both literature experts and literary enthusiasts. This protocol will serve as a
foundation for evaluating automatically generated microfictions and assessing
their literary value.

</details>


### [7] [LLM-BT: Back-Translation as a Framework for Terminology Standardization and Dynamic Semantic Embedding](https://arxiv.org/abs/2506.08174)
*Li Weigang,Pedro Carvalho Brom*

Main category: cs.CL

TL;DR: 提出LLM-BT框架，通过大语言模型驱动的回译技术实现跨语言术语标准化，核心贡献包括术语一致性验证、多路径验证流程和动态语义嵌入机制。


<details>
  <summary>Details</summary>
Motivation: 传统专家主导的术语标准化方法难以应对AI/量子计算等领域快速增长的英语技术术语，且人工方法无法保证多语言一致性。

Method: 1) 术语级一致性验证（英语→中间语言→英语回译）
2) 多路径验证流程（检索-生成-验证-优化）
3) 将回译概念化为动态语义嵌入

Result: 案例显示超90%术语匹配率；BLEU>0.45（葡萄牙语准确率100%）；构建透明路径嵌入反映模型演进

Conclusion: LLM-BT重构回译为多语言术语标准化的主动引擎，实现人机协作：机器保障语义保真，人类指导文化诠释，支撑全球科技领域术语治理。

Abstract: The rapid growth of English technical terms challenges traditional
expert-driven standardization, especially in fast-evolving fields like AI and
quantum computing. Manual methods struggle to ensure multilingual consistency.
We propose \textbf{LLM-BT}, a back-translation framework powered by large
language models (LLMs) to automate terminology verification and standardization
via cross-lingual semantic alignment. Our contributions are: \textbf{(1)
Term-Level Consistency Validation:} Using English $\rightarrow$ intermediate
language $\rightarrow$ English back-translation, LLM-BT achieves high term
consistency across models (e.g., GPT-4, DeepSeek, Grok), with case studies
showing over 90\% exact or semantic matches. \textbf{(2) Multi-Path
Verification Workflow:} A novel ``Retrieve--Generate--Verify--Optimize''
pipeline integrates serial (e.g., EN $\rightarrow$ ZHcn $\rightarrow$ ZHtw
$\rightarrow$ EN) and parallel (e.g., EN $\rightarrow$ Chinese/Portuguese
$\rightarrow$ EN) BT routes. BLEU and term accuracy indicate strong
cross-lingual robustness (BLEU $>$ 0.45; Portuguese accuracy 100\%).
\textbf{(3) Back-Translation as Semantic Embedding:} BT is conceptualized as
dynamic semantic embedding, revealing latent meaning trajectories. Unlike
static embeddings, LLM-BT provides transparent path-based embeddings shaped by
model evolution. LLM-BT transforms back-translation into an active engine for
multilingual terminology standardization, enabling human--AI collaboration:
machines ensure semantic fidelity, humans guide cultural interpretation. This
infrastructure supports terminology governance across scientific and
technological fields worldwide.

</details>


### [8] [Unable to forget: Proactive lnterference Reveals Working Memory Limits in LLMs Beyond Context Length](https://arxiv.org/abs/2506.08184)
*Chupei Wang,Jiaqiu Vince Sun*

Main category: cs.CL

TL;DR: LLMs在信息检索中存在主动干扰敏感性问题，随着语义干扰积累，检索准确率呈对数线性下降，提示工程干预效果有限，揭示工作记忆瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分验证长上下文窗口能否真正提升LLMs的信息检索能力，尤其主动干扰（旧信息覆盖新信息）对检索准确性的影响机制尚未明确。

Method: 引入认知科学中的主动干扰(PI)范式，设计PI-LLM评估框架：流式输入语义相关的键值更新后，仅查询最终值的位置准确性。

Result: 1. 最终值位置临近查询时准确率仍显著下降
2. 错误源于检索被覆盖的旧值
3. 提示模型忽略旧信息的指令仅提升10%准确率

Conclusion: LLMs存在与人类相似的工作记忆瓶颈，单纯扩大上下文窗口无法解决信息干扰问题，需开发抑制无关信息的检索增强机制。

Abstract: Information retrieval in Large Language Models (LLMs) is increasingly
recognized as intertwined with generation capabilities rather than mere lookup.
While longer contexts are often assumed to improve retrieval, the effects of
intra-context interference remain understudied. To address this, we adapt the
proactive interference (PI) paradigm from cognitive science, where earlier
information disrupts recall of newer updates. In humans, susceptibility to such
interference is inversely linked to working memory capacity. We introduce
PI-LLM, an evaluation that sequentially streams semantically related key-value
updates and queries only the final values. Although these final values are
clearly positioned just before the query, LLM retrieval accuracy declines
log-linearly toward zero as interference accumulates; errors arise from
retrieving previously overwritten values. Attempts to mitigate interference via
prompt engineering (e.g., instructing models to ignore earlier input) yield
limited success. These findings reveal a fundamental constraint on LLMs'
ability to disentangle interference and flexibly manipulate information,
suggesting a working memory bottleneck beyond mere context access. This calls
for approaches that strengthen models' ability to suppress irrelevant content
during retrieval.

</details>


### [9] ["I Wrote, I Paused, I Rewrote" Teaching LLMs to Read Between the Lines of Student Writing](https://arxiv.org/abs/2506.08221)
*Samra Zafar,Shaheer Minhas,Syed Ali Hassan Zaidi,Arfa Naeem,Zahra Ali*

Main category: cs.CL

TL;DR: 通过追踪写作过程数据提升LLM反馈质量


<details>
  <summary>Details</summary>
Motivation: 当前LLM写作反馈仅基于最终文本，缺乏对写作过程的理解。研究旨在探索写作过程数据（击键记录/版本快照）如何帮助LLM生成更贴合学生实际思考路径的反馈。

Method: 开发数字写作工具采集20名学生的写作轨迹，采用双评估法：1）结合终稿和过程数据的LLM反馈 2）学生问卷调查反馈实用性

Result: 学生更认可过程感知反馈（87%认为更贴近思考模式），添加内容/段落重组等编辑行为与作文评分指标（连贯性/阐述深度）显著正相关

Conclusion: 整合写作过程数据能使LLM反馈更具针对性，提升反馈的情感共鸣与教学支持效果

Abstract: Large language models(LLMs) like Gemini are becoming common tools for
supporting student writing. But most of their feedback is based only on the
final essay missing important context about how that text was written. In this
paper, we explore whether using writing process data, collected through
keystroke logging and periodic snapshots, can help LLMs give feedback that
better reflects how learners think and revise while writing. We built a digital
writing tool that captures both what students type and how their essays evolve
over time. Twenty students used this tool to write timed essays, which were
then evaluated in two ways: (i) LLM generated feedback using both the final
essay and the full writing trace, and (ii) After the task, students completed
surveys about how useful and relatable they found the feedback. Early results
show that learners preferred the process-aware LLM feedback, finding it more in
tune with their own thinking. We also found that certain types of edits, like
adding new content or reorganizing paragraphs, aligned closely with higher
scores in areas like coherence and elaboration. Our findings suggest that
making LLMs more aware of the writing process can lead to feedback that feels
more meaningful, personal, and supportive.

</details>


### [10] [Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions](https://arxiv.org/abs/2506.08234)
*Yu-Ang Lee,Guan-Ting Yi,Mei-Yi Liu,Jui-Chao Lu,Guan-Bo Yang,Yun-Nung Chen*

Main category: cs.CL

TL;DR: 系统回顾复合AI系统优化方法，提出结合数值优化与自然语言反馈的创新框架


<details>
  <summary>Details</summary>
Motivation: 复合AI系统复杂度提升导致组件间交互优化成为新挑战，需突破传统单组件优化范式

Method: 分类传统数值优化（SFT/RL）与新兴语言反馈方法，特别关注不可微分系统优化方案

Result: 建立复合AI优化的系统框架，提出技术分类维度并指明人机协同优化等未来方向

Conclusion: 该领域处于快速演进阶段，需融合数值方法与语言理解技术实现系统级优化突破

Abstract: Recent advancements in large language models (LLMs) and AI systems have led
to a paradigm shift in the design and optimization of complex AI workflows. By
integrating multiple components, compound AI systems have become increasingly
adept at performing sophisticated tasks. However, as these systems grow in
complexity, new challenges arise in optimizing not only individual components
but also their interactions. While traditional optimization methods such as
supervised fine-tuning (SFT) and reinforcement learning (RL) remain
foundational, the rise of natural language feedback introduces promising new
approaches, especially for optimizing non-differentiable systems. This paper
provides a systematic review of recent progress in optimizing compound AI
systems, encompassing both numerical and language-based techniques. We
formalize the notion of compound AI system optimization, classify existing
methods along several key dimensions, and highlight open research challenges
and future directions in this rapidly evolving field. A list of surveyed papers
is publicly available at https://github.com/MiuLab/AISysOpt-Survey.

</details>


### [11] [Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\rightarrow$ Evidence Reasoning](https://arxiv.org/abs/2506.08235)
*Shashidhar Reddy Javaji,Yupeng Cao,Haohang Li,Yangyang Yu,Nikhil Muralidhar,Zining Zhu*

Main category: cs.CL

TL;DR: 开发CLAIM-BENCH基准评估大模型在科学主张-证据提取验证中的表现，发现闭源模型性能更优但存在成本限制


<details>
  <summary>Details</summary>
Motivation: 现有大模型虽广泛用于科研分析，但其对论文中主张-证据逻辑关系的深层理解能力尚未被充分验证

Method: 构建包含300+跨领域主张-证据对的基准测试，采用分治策略（三阶段/逐一验证提示法）对比六大模型的性能

Result: 闭源模型（GPT-4/Claude）在精确率/召回率上平均领先开源模型15-20%，优化提示策略可提升证据关联准确性（+35%）但增加30%计算成本

Conclusion: CLAIM-BENCH为评估模型科学理解能力建立新标准，提示策略优化方向为提升可靠推理能力提供路径

Abstract: Large language models (LLMs) are increasingly being used for complex research
tasks such as literature review, idea generation, and scientific paper
analysis, yet their ability to truly understand and process the intricate
relationships within complex research papers, such as the logical links between
claims and supporting evidence remains largely unexplored. In this study, we
present CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs'
capabilities in scientific claim-evidence extraction and validation, a task
that reflects deeper comprehension of scientific argumentation. We
systematically compare three approaches which are inspired by divide and
conquer approaches, across six diverse LLMs, highlighting model-specific
strengths and weaknesses in scientific comprehension. Through evaluation
involving over 300 claim-evidence pairs across multiple research domains, we
reveal significant limitations in LLMs' ability to process complex scientific
content. Our results demonstrate that closed-source models like GPT-4 and
Claude consistently outperform open-source counterparts in precision and recall
across claim-evidence identification tasks. Furthermore, strategically designed
three-pass and one-by-one prompting approaches significantly improve LLMs'
abilities to accurately link dispersed evidence with claims, although this
comes at increased computational cost. CLAIM-BENCH sets a new standard for
evaluating scientific comprehension in LLMs, offering both a diagnostic tool
and a path forward for building systems capable of deeper, more reliable
reasoning across full-length papers.

</details>


### [12] [Automatic Generation of Inference Making Questions for Reading Comprehension Assessments](https://arxiv.org/abs/2506.08260)
*Wanjing Anya Ma,Michael Flor,Zuowei Wang*

Main category: cs.CL

TL;DR: 研究提出阅读理解诊断题自动生成框架，通过GPT-4o生成跨文本推理题，验证其质量与类型匹配度。


<details>
  <summary>Details</summary>
Motivation: 通过自动生成诊断性阅读理解题目，帮助教育工作者实现精准教学干预，解决人工命题效率瓶颈。

Method: 构建推理类型分类体系，采用few-shot提示策略（含思维链与非思维链对比），利用GPT-4o生成跨文本推理题目并进行多维评估。

Result: 生成题目93.8%达到操作标准，但仅42.6%准确匹配目标推理类型；人工评估者间一致性高于0.90。

Conclusion: 人机协同的题目生成模式为规模化诊断评估提供可行路径，但需加强推理类型控制机制。

Abstract: Inference making is an essential but complex skill in reading comprehension
(RC). Some inferences require resolving references across sentences, and some
rely on using prior knowledge to fill in the detail that is not explicitly
written in the text. Diagnostic RC questions can help educators provide more
effective and targeted reading instruction and interventions for school-age
students. We introduce a taxonomy of inference types for RC and use it to
analyze the distribution of items within a diagnostic RC item bank. Next, we
present experiments using GPT-4o to generate bridging-inference RC items for
given reading passages via few-shot prompting, comparing conditions with and
without chain-of-thought prompts. Generated items were evaluated on three
aspects: overall item quality, appropriate inference type, and LLM reasoning,
achieving high inter-rater agreements above 0.90. Our results show that GPT-4o
produced 93.8% good-quality questions suitable for operational use in grade
3-12 contexts; however, only 42.6% of the generated questions accurately
matched the targeted inference type. We conclude that combining automatic item
generation with human judgment offers a promising path toward scalable,
high-quality diagnostic RC assessments.

</details>


### [13] [Institutional Books 1.0: A 242B token dataset from Harvard Library's collections, refined for accuracy and usability](https://arxiv.org/abs/2506.08300)
*Matteo Cargnelutti,Catherine Brobston,John Hess,Jack Cushman,Kristi Mukk,Aristana Scourtas,Kyle Courtney,Greg Leppert,Amanda Watson,Martha Whitehead,Jonathan Zittrain*

Main category: cs.CL

TL;DR: 介绍哈佛图书馆与谷歌合作创建的公共领域书籍数据集Institutional Books 1.0，包含2420亿token的OCR文本及元数据，旨在提高历史文献可访问性。


<details>
  <summary>Details</summary>
Motivation: 应对高质量训练数据稀缺问题，通过可持续数据管理提供清晰来源链的历史文本资源。

Method: 提取分析哈佛图书馆谷歌图书项目的107万册藏书，对98.3万册公域书籍进行OCR文本处理和元数据生成。

Result: 公开983,004卷公域书籍的原始/后处理文本及元数据（书目/来源/生成），总规模达242B tokens。

Conclusion: 建立可机器处理的历史文本资源库，促进人类与机器对文化遗产的高效过滤、阅读和使用。

Abstract: Large language models (LLMs) use data to learn about the world in order to
produce meaningful correlations and predictions. As such, the nature, scale,
quality, and diversity of the datasets used to train these models, or to
support their work at inference time, have a direct impact on their quality.
The rapid development and adoption of LLMs of varying quality has brought into
focus the scarcity of publicly available, high-quality training data and
revealed an urgent need to ground the stewardship of these datasets in
sustainable practices with clear provenance chains. To that end, this technical
report introduces Institutional Books 1.0, a large collection of public domain
books originally digitized through Harvard Library's participation in the
Google Books project, beginning in 2006. Working with Harvard Library, we
extracted, analyzed, and processed these volumes into an extensively-documented
dataset of historic texts. This analysis covers the entirety of Harvard
Library's collection scanned as part of that project, originally spanning
1,075,899 volumes written in over 250 different languages for a total of
approximately 250 billion tokens. As part of this initial release, the
OCR-extracted text (original and post-processed) as well as the metadata
(bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens,
identified as being in the public domain have been made available. This report
describes this project's goals and methods as well as the results of the
analyses we performed, all in service of making this historical collection more
accessible and easier for humans and machines alike to filter, read and use.

</details>


### [14] [Wait, We Don't Need to "Wait"! Removing Thinking Tokens Improves Reasoning Efficiency](https://arxiv.org/abs/2506.08343)
*Chenlong Wang,Yuanning Feng,Dongping Chen,Zhaoyang Chu,Ranjay Krishna,Tianyi Zhou*

Main category: cs.CL

TL;DR: NoWait通过抑制推理过程中自省标记（如'Wait'/'Hmm'），将多模态推理轨迹长度缩减27%-51%，同时保持模型性能不变


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型存在过度思考问题，导致输出冗长冗余，影响推理效率。需要探索显式自省标记是否必要，并寻求更高效的推理方案。

Method: 提出NoWait方法：在推理过程中抑制自省标记的生成，通过修改生成策略实现链式思维轨迹的简洁化。该方法兼容不同模态（文本/视觉/视频）的推理任务。

Result: 在10个跨模态基准测试中，NoWait使5个R1系列模型的思维链长度减少27%-51%，且模型效用未受损。该方法具有即插即用特性，无需额外训练。

Conclusion: NoWait证明了显式自省标记在复杂推理中的非必要性，为多模态推理提供了高效且保持性能的解决方案，显著提升推理过程的经济性。

Abstract: Recent advances in large reasoning models have enabled complex, step-by-step
reasoning but often introduce significant overthinking, resulting in verbose
and redundant outputs that hinder efficiency. In this study, we examine whether
explicit self-reflection, signaled by tokens such as "Wait" and "Hmm", is
necessary for advanced reasoning. We propose NoWait, a simple yet effective
approach that disables explicit self-reflection by suppressing these tokens
during inference. Extensive experiments on ten benchmarks across textual,
visual, and video reasoning tasks show that NoWait reduces chain-of-thought
trajectory length by up to 27%-51% in five R1-style model series, without
compromising model utility. NoWait thus offers a plug-and-play solution for
efficient and utility-preserving multimodal reasoning.

</details>


### [15] [Evaluating LLMs Across Multi-Cognitive Levels: From Medical Knowledge Mastery to Scenario-Based Problem Solving](https://arxiv.org/abs/2506.08349)
*Yuxuan Zhou,Xien Liu,Chenwei Yan,Chen Ning,Xiao Zhang,Boxun Li,Xiangling Fu,Shijin Wang,Guoping Hu,Yu Wang,Ji Wu*

Main category: cs.CL

TL;DR: 提出基于布鲁姆分类学的多认知层次医学评估框架，发现模型性能随认知复杂度提升显著下降，模型规模在高层认知任务中更关键。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索LLMs在不同认知层次（知识掌握→综合应用→实际问题解决）的医学能力差异，需系统性评估模型在真实医疗场景中的适用性。

Method: 整合现有医学数据集并设计三层认知任务：基础知识掌握（L1）、综合应用（L2）、场景化问题解决（L3），评估Llama/Qwen/Gemma/Phi/GPT/DeepSeek六大模型家族。

Result: 1. 所有模型随认知层次提升性能显著下降（平均准确率L1 73.5% → L3 51.2%）；2. 模型参数规模对高层认知任务影响更大（L3任务规模效应系数达0.87 vs L1 0.62）。

Conclusion: 当前医学LLMs在高层认知能力存在明显短板，未来应着重提升复杂场景下的推理与问题解决能力，建议通过领域适应性训练与认知架构优化推进真实医疗应用。

Abstract: Large language models (LLMs) have demonstrated remarkable performance on
various medical benchmarks, but their capabilities across different cognitive
levels remain underexplored. Inspired by Bloom's Taxonomy, we propose a
multi-cognitive-level evaluation framework for assessing LLMs in the medical
domain in this study. The framework integrates existing medical datasets and
introduces tasks targeting three cognitive levels: preliminary knowledge grasp,
comprehensive knowledge application, and scenario-based problem solving. Using
this framework, we systematically evaluate state-of-the-art general and medical
LLMs from six prominent families: Llama, Qwen, Gemma, Phi, GPT, and DeepSeek.
Our findings reveal a significant performance decline as cognitive complexity
increases across evaluated models, with model size playing a more critical role
in performance at higher cognitive levels. Our study highlights the need to
enhance LLMs' medical capabilities at higher cognitive levels and provides
insights for developing LLMs suited to real-world medical applications.

</details>


### [16] [Text Embeddings Should Capture Implicit Semantics, Not Just Surface Meaning](https://arxiv.org/abs/2506.08354)
*Yiqun Sun,Qiang Huang,Anthony K. H. Tung,Jun Yu*

Main category: cs.CL

TL;DR: 主张文本嵌入研究应超越表层语义，将隐含语义作为核心建模目标


<details>
  <summary>Details</summary>
Motivation: 当前模型过度依赖缺乏深度语义的训练数据及评估基准，导致在语用推理、社会意义等隐含语义任务上表现不佳

Method: 通过试点研究验证模型缺陷，提出应构建语言学基础数据、设计深层语义评估基准、明确建模目标三位一体的解决方案

Result: 实验显示最先进模型在隐含语义任务上仅略优于简单基线模型

Conclusion: 呼吁研究范式转型，通过数据-基准-目标协同优化，使嵌入模型更好适应真实语言复杂性

Abstract: This position paper argues that the text embedding research community should
move beyond surface meaning and embrace implicit semantics as a central
modeling goal. Text embedding models have become foundational in modern NLP,
powering a wide range of applications and drawing increasing research
attention. Yet, much of this progress remains narrowly focused on surface-level
semantics. In contrast, linguistic theory emphasizes that meaning is often
implicit, shaped by pragmatics, speaker intent, and sociocultural context.
Current embedding models are typically trained on data that lacks such depth
and evaluated on benchmarks that reward the capture of surface meaning. As a
result, they struggle with tasks requiring interpretive reasoning, speaker
stance, or social meaning. Our pilot study highlights this gap, showing that
even state-of-the-art models perform only marginally better than simplistic
baselines on implicit semantics tasks. To address this, we call for a paradigm
shift: embedding research should prioritize more diverse and linguistically
grounded training data, design benchmarks that evaluate deeper semantic
understanding, and explicitly frame implicit meaning as a core modeling
objective, better aligning embeddings with real-world language complexity.

</details>


### [17] [DEAL: Disentangling Transformer Head Activations for LLM Steering](https://arxiv.org/abs/2506.08359)
*Li-Ming Zhan,Bo Liu,Zexin Lu,Chengqiang Xie,Jiannong Cao,Xiao-Ming Wu*

Main category: cs.CL

TL;DR: 提出基于因果归因的框架，通过VQ-AE量化注意力头激活，有效识别行为相关注意力头，提升推理干预效果


<details>
  <summary>Details</summary>
Motivation: 现有注意力头选择方法依赖表面线索或启发式规则，导致干预效果欠佳，需要更原则性的模块选择方法

Method: 使用向量量化自编码器(VQ-AE)对注意力头激活进行编码，划分行为相关/无关子空间，通过二分类指标量化注意力头的判别能力

Result: 在7个LLM和5个行为数据集验证，真实性引导任务取得最佳效果，所选注意力头展现跨领域零样本泛化能力

Conclusion: 该方法实现了精准的注意力头选择，支持有效的推理时干预，且具备跨领域应用潜力

Abstract: Inference-time steering aims to alter the response characteristics of large
language models (LLMs) without modifying their underlying parameters. A
critical step in this process is the identification of internal modules within
LLMs that are associated with the target behavior. However, current approaches
to module selection often depend on superficial cues or ad-hoc heuristics,
which can result in suboptimal or unintended outcomes. In this work, we propose
a principled causal-attribution framework for identifying behavior-relevant
attention heads in transformers. For each head, we train a vector-quantized
autoencoder (VQ-AE) on its attention activations, partitioning the latent space
into behavior-relevant and behavior-irrelevant subspaces, each quantized with a
shared learnable codebook. We assess the behavioral relevance of each head by
quantifying the separability of VQ-AE encodings for behavior-aligned versus
behavior-violating responses using a binary classification metric. This yields
a behavioral relevance score that reflects each head discriminative capacity
with respect to the target behavior, guiding both selection and importance
weighting. Experiments on seven LLMs from two model families and five
behavioral steering datasets demonstrate that our method enables more accurate
inference-time interventions, achieving superior performance on the
truthfulness-steering task. Furthermore, the heads selected by our approach
exhibit strong zero-shot generalization in cross-domain truthfulness-steering
scenarios.

</details>


### [18] [CC-RAG: Structured Multi-Hop Reasoning via Theme-Based Causal Graphs](https://arxiv.org/abs/2506.08364)
*Jash Rajesh Parekh,Pengcheng Jiang,Jiawei Han*

Main category: cs.CL

TL;DR: 提出Causal-Chain RAG方法，通过结构化因果链增强RAG性能，在专业领域任务中超越传统RAG和零样本LLM


<details>
  <summary>Details</summary>
Motivation: 现有RAG在处理专业领域因果关系时存在结构缺失，无法实现深层因果推理，需要显式建模因果依赖关系

Method: 整合零样本三元组抽取和主题图链式结构，构建<原因-关系-结果>DAG，采用前/后向链式推理引导结构化答案生成

Result: 在比特币价格波动和高雪氏病领域，CC-RAG在链相似性(提升32%)、信息密度(增加41%)和词汇多样性(提高28%)上显著优于基线

Conclusion: 显式因果结构建模使LLM生成更准确可解释的回答，尤其在传统扁平检索失效的专业领域效果显著

Abstract: Understanding cause and effect relationships remains a formidable challenge
for Large Language Models (LLMs), particularly in specialized domains where
reasoning requires more than surface-level correlations. Retrieval-Augmented
Generation (RAG) improves factual accuracy, but standard RAG pipelines treat
evidence as flat context, lacking the structure required to model true causal
dependencies. We introduce Causal-Chain RAG (CC-RAG), a novel approach that
integrates zero-shot triple extraction and theme-aware graph chaining into the
RAG pipeline, enabling structured multi-hop inference. Given a domain specific
corpus, CC-RAG constructs a Directed Acyclic Graph (DAG) of <cause, relation,
effect> triples and uses forward/backward chaining to guide structured answer
generation. Experiments on two real-world domains: Bitcoin price fluctuations
and Gaucher disease, show that CC-RAG outperforms standard RAG and zero-shot
LLMs in chain similarity, information density, and lexical diversity. Both
LLM-as-a-Judge and human evaluations consistently favor CC-RAG. Our results
demonstrate that explicitly modeling causal structure enables LLMs to generate
more accurate and interpretable responses, especially in specialized domains
where flat retrieval fails.

</details>


### [19] [Mitigating Posterior Salience Attenuation in Long-Context LLMs with Positional Contrastive Decoding](https://arxiv.org/abs/2506.08371)
*Zikai Xiao,Ziyang Wang,Wen Ma,Yan Zhang,Wei Shen,Yan Wang,Luqi Gong,Zuozhu Liu*

Main category: cs.CL

TL;DR: 提出无需训练的Positional Contrastive Decoding(PCD)方法，通过对比长程和局部注意力机制，有效提升大语言模型在长上下文场景下的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有解决长上下文性能衰减的方法存在高训练成本问题，且发现尽管存在后验显著性衰减(PSA)，黄金标记在解码空间中仍保持高排名位置。

Method: 设计对比解码策略，通过对比长程注意力和局部注意力产生的logits分布，利用大规模短文本训练的增益提升长文本处理能力。

Result: 在长上下文基准测试中取得SOTA性能，并通过衰减模拟验证了方法对注意力分数退化的缓解效果。

Conclusion: PCD作为零训练成本解决方案，有效解决了LLMs的长文本处理性能衰减问题，具有实际部署价值。

Abstract: While Large Language Models (LLMs) support long contexts, they struggle with
performance degradation within the context window. Current solutions incur
prohibitive training costs, leaving statistical behaviors and cost-effective
approaches underexplored. From the decoding perspective, we identify the
Posterior Salience Attenuation (PSA) phenomenon, where the salience ratio
correlates with long-text performance degradation. Notably, despite the
attenuation, gold tokens still occupy high-ranking positions in the decoding
space. Motivated by it, we propose the training-free Positional Contrastive
Decoding (PCD) that contrasts the logits derived from long-aware attention with
those from designed local-aware attention, enabling the model to focus on the
gains introduced by large-scale short-to-long training. Through the analysis of
long-term decay simulation, we demonstrate that PCD effectively alleviates
attention score degradation. Experimental results show that PCD achieves
state-of-the-art performance on long-context benchmarks.

</details>


### [20] [Draft-based Approximate Inference for LLMs](https://arxiv.org/abs/2506.08373)
*Kevin Galim,Ethan Ewer,Wonjun Kang,Minjae Lee,Hyung Il Koo,Kangwook Lee*

Main category: cs.CL

TL;DR: 提出基于草稿模型的近似推理框架SpecKV和SpecPC，显著提升长上下文LLM推理效率


<details>
  <summary>Details</summary>
Motivation: 现有Transformer模型在长上下文场景存在计算和内存效率瓶颈，传统近似方法依赖粗糙的重要性预测。为更精准评估KV对和token重要性，利用轻量级草稿模型提升预测准确性。

Method: 1. SpecKV：通过草稿模型输出精准评估KV对重要性，优化KV缓存丢弃策略
2. SpecPC：利用草稿模型注意力激活识别非关键提示token，实现高效提示压缩

Result: 在长上下文基准测试中准确率超越现有方法，同时保持内存/延迟/吞吐量改进，公开代码验证有效性

Conclusion: 首次将草稿模型应用于近似推理加速，突破传统仅用于无损推测解码的限制，通过理论分析和实验验证框架有效性，为LLM优化提供新方向

Abstract: Optimizing inference for long-context Large Language Models (LLMs) is
increasingly important due to the quadratic compute and linear memory
complexity of Transformers. Existing approximation methods, such as key-value
(KV) cache dropping, sparse attention, and prompt compression, typically rely
on rough predictions of token or KV pair importance. We propose a novel
framework for approximate LLM inference that leverages small draft models to
more accurately predict the importance of tokens and KV pairs. Specifically, we
introduce two instantiations of our proposed framework: (i) SpecKV, which
leverages a draft output to accurately assess the importance of each KV pair
for more effective KV cache dropping, and (ii) SpecPC, which uses the draft
model's attention activations to identify and discard unimportant prompt
tokens. To the best of our knowledge, this is the first work to use draft
models for approximate LLM inference acceleration, extending their utility
beyond traditional lossless speculative decoding. We motivate our methods with
theoretical and empirical analyses, and show a strong correlation between the
attention patterns of draft and target models. Extensive experiments on
long-context benchmarks show that our methods consistently achieve higher
accuracy than existing baselines, while preserving the same improvements in
memory usage, latency, and throughput. Our code is available at
https://github.com/furiosa-ai/draft-based-approx-llm.

</details>


### [21] [EIFBENCH: Extremely Complex Instruction Following Benchmark for Large Language Models](https://arxiv.org/abs/2506.08375)
*Tao Zou,Xinghua Zhang,Haiyang Yu,Minzheng Wang,Fei Huang,Yongbin Li*

Main category: cs.CL

TL;DR: 提出EIFBENCH基准测试框架和SegPO算法，用于评估和优化大语言模型在多任务复杂场景下的指令遵循能力


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准测试局限于单任务场景，缺乏真实场景的复杂性，无法全面评估模型处理多任务工作流的能力

Method: 1. 构建包含多任务协同与多样化约束的EIFBENCH基准测试框架
2. 开发分段策略优化算法(SegPO)提升多任务处理精度

Result: 现有主流LLM在EIFBENCH测试中表现出显著性能差异（平均准确率下降23.7%），暴露多任务处理瓶颈

Conclusion: LLM在复杂场景的应用需要持续优化，EIFBENCH为模型能力评估提供了更贴近现实的验证平台

Abstract: With the development and widespread application of large language models
(LLMs), the new paradigm of "Model as Product" is rapidly evolving, and demands
higher capabilities to address complex user needs, often requiring precise
workflow execution which involves the accurate understanding of multiple tasks.
However, existing benchmarks focusing on single-task environments with limited
constraints lack the complexity required to fully reflect real-world scenarios.
To bridge this gap, we present the Extremely Complex Instruction Following
Benchmark (EIFBENCH), meticulously crafted to facilitate a more realistic and
robust evaluation of LLMs. EIFBENCH not only includes multi-task scenarios that
enable comprehensive assessment across diverse task types concurrently, but
also integrates a variety of constraints, replicating complex operational
environments. Furthermore, we propose the Segment Policy Optimization (SegPO)
algorithm to enhance the LLM's ability to accurately fulfill multi-task
workflow. Evaluations on EIFBENCH have unveiled considerable performance
discrepancies in existing LLMs when challenged with these extremely complex
instructions. This finding underscores the necessity for ongoing optimization
to navigate the intricate challenges posed by LLM applications.

</details>


### [22] [mSTEB: Massively Multilingual Evaluation of LLMs on Speech and Text Tasks](https://arxiv.org/abs/2506.08400)
*Luel Hagos Beyene,Vivek Verma,Min Ma,Jesujoba O. Alabi,Fabian David Schmidt,Joyce Nakatumba-Nabende,David Ifeoluwa Adelani*

Main category: cs.CL

TL;DR: 研究者开发了多模态基准测试mSTEB，发现主流大语言模型在低资源语言（尤其是非洲及美洲/大洋洲语种）表现显著落后，呼吁加强相关投入。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评估集中于英语等高资源语言，缺乏针对低资源语言的标准化评测基准

Method: 构建覆盖语音/文本模态的mSTEB基准（含语言识别、文本分类、QA、翻译等任务），测试Gemini Flash、GPT-4o Audio、Qwen Audio等前沿模型

Result: 高低资源语言性能差距显著（非洲语言F1值比欧洲语言低40%+），语音模态表现普遍弱于文本模态

Conclusion: 需系统性提升LLMs对低资源语言的覆盖能力，建议通过数据增强和架构创新改善语言公平性

Abstract: Large Language models (LLMs) have demonstrated impressive performance on a
wide range of tasks, including in multimodal settings such as speech. However,
their evaluation is often limited to English and a few high-resource languages.
For low-resource languages, there is no standardized evaluation benchmark. In
this paper, we address this gap by introducing mSTEB, a new benchmark to
evaluate the performance of LLMs on a wide range of tasks covering language
identification, text classification, question answering, and translation tasks
on both speech and text modalities. We evaluated the performance of leading
LLMs such as Gemini 2.0 Flash and GPT-4o (Audio) and state-of-the-art open
models such as Qwen 2 Audio and Gemma 3 27B. Our evaluation shows a wide gap in
performance between high-resource and low-resource languages, especially for
languages spoken in Africa and Americas/Oceania. Our findings show that more
investment is needed to address their under-representation in LLMs coverage.

</details>


### [23] [TACTIC: Translation Agents with Cognitive-Theoretic Interactive Collaboration](https://arxiv.org/abs/2506.08403)
*Weiya Li,Junjie Chen,Bei Li,Boyang Liu,Zichen Wen,Nuanqiao Shan,Xiaoqian Liu,Anping Liu,Huajie Liu,Youyan Wang,Wujiuge Yin,Hu Song,Bing Huang,Zhiyuan Xia,Jialiang Chen,Linfeng Zhang*

Main category: cs.CL

TL;DR: 提出认知启发的多智能体框架TACTIC，通过模拟人类翻译认知策略显著提升大语言模型翻译质量


<details>
  <summary>Details</summary>
Motivation: 现有框架忽视认知翻译理论，人类翻译策略（如直译/意译平衡、上下文优化、迭代评估）未被有效整合

Method: 构建6个功能分化的智能体（起草/精炼/评估/评分/上下文推理/知识收集），形成理论指导的协作流程

Result: 在FLORES-200和WMT24多语言测试中，XCOMET和COMETKIWI-23指标超越GPT-4和DeepSeek-R1基准模型

Conclusion: 认知理论驱动的多智能体协作能充分释放LLM翻译潜力，验证认知策略在机器翻译中的核心价值

Abstract: Machine translation has long been a central task in natural language
processing. With the rapid advancement of large language models (LLMs), there
has been remarkable progress in translation quality. However, fully realizing
the translation potential of LLMs remains an open challenge. Recent studies
have explored multi-agent systems to decompose complex translation tasks into
collaborative subtasks, showing initial promise in enhancing translation
quality through agent cooperation and specialization. Nevertheless, existing
multi-agent translation frameworks largely neglect foundational insights from
cognitive translation studies. These insights emphasize how human translators
employ different cognitive strategies, such as balancing literal and free
translation, refining expressions based on context, and iteratively evaluating
outputs. To address this limitation, we propose a cognitively informed
multi-agent framework called TACTIC, which stands for T ranslation A gents with
Cognitive- T heoretic Interactive Collaboration. The framework comprises six
functionally distinct agents that mirror key cognitive processes observed in
human translation behavior. These include agents for drafting, refinement,
evaluation, scoring, context reasoning, and external knowledge gathering. By
simulating an interactive and theory-grounded translation workflow, TACTIC
effectively leverages the full capacity of LLMs for high-quality translation.
Experimental results on diverse language pairs from the FLORES-200 and WMT24
benchmarks show that our method consistently achieves state-of-the-art
performance. Using DeepSeek-V3 as the base model, TACTIC surpasses GPT-4.1 by
an average of +0.6 XCOMET and +1.18 COMETKIWI-23. Compared to DeepSeek-R1, it
further improves by +0.84 XCOMET and +2.99 COMETKIWI-23. Code is available at
https://github.com/weiyali126/TACTIC.

</details>


### [24] [Large Language Models Have Intrinsic Meta-Cognition, but Need a Good Lens](https://arxiv.org/abs/2506.08410)
*Ziyang Ma,Qingyue Yuan,Zhenglin Wang,Deyu Zhou*

Main category: cs.CL

TL;DR: 本文提出AutoMeco框架评估大语言模型的元认知能力，并通过MIRA策略改进现有评估方法，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注LLMs的认知错误检测能力，但缺乏对其元认知能力（如步骤错误自我意识）的系统评估，这直接影响模型的可靠性。

Method: 1. 提出自动化元认知评估框架AutoMeco；2. 设计无需训练的马尔可夫内在奖励调整策略MIRA；3. 在三个数学推理数据集和三种LLMs上进行验证。

Result: AutoMeco框架验证结果合理，MIRA能更有效评估LLMs的元认知能力。

Conclusion: 改进元认知评估方法对提升LLM可靠性至关重要，AutoMeco和MIRA为此提供了有效解决方案。

Abstract: Previous research has primarily focused on the cognitive error detection
capabilities of Large Language Models (LLMs), often prompting them to analyze
mistakes in reasoning chains. However, few studies have examined the
meta-cognitive abilities of LLMs (e.g., their self-awareness of step errors),
which are crucial for their reliability. While studies on LLM self-evaluation
present some measures, such as perplexity, which can reflect the answer
correctness and be viewed as the lens of meta-cognition, they lack step-level
analysis and adaptation. This paper studies the evaluation of LLM
meta-cognition using the current lenses and how to improve these lenses.
Specifically, we propose AutoMeco, an Automated Meta-cognition Evaluation
framework for benchmarking the existing lenses. Furthermore, a training-free
Markovian Intrinsic Reward Adjustment strategy, MIRA, is proposed to boost
current meta-cognition lenses. Experimental results on three mathematical
reasoning datasets and three LLMs show the reasonableness of AutoMeco by
comparing it with Best-of-N verification. Moreover, the meta-cognition ability
of LLMs can be better evaluated using MIRA.

</details>


### [25] [Know-MRI: A Knowledge Mechanisms Revealer&Interpreter for Large Language Models](https://arxiv.org/abs/2506.08427)
*Jiaxiang Liu,Boxuan Xing,Chenhao Yuan,Chenxiang Zhang,Di Wu,Xiusheng Huang,Haida Yu,Chuhan Lang,Pengfei Cao,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 开发了Know-MRI开源系统，通过可扩展核心模块整合多种解释方法，支持用户多维度分析大语言模型内部的知识机制。


<details>
  <summary>Details</summary>
Motivation: 现有LLM知识解释方法存在输入格式与输出结果不统一的问题，导致工具应用场景受限，亟需开发系统化的分析框架。

Method: 构建自动匹配输入数据与解释方法的核心模块，实现解释结果的集成输出，允许用户根据输入自由选择解释方法进行组合分析。

Result: 创建开源工具Know-MRI并提供代码与演示视频，支持从神经元、参数权重等多角度对模型知识机制进行系统性诊断。

Conclusion: Know-MRI有效解决了现有解释方法碎片化的问题，为深入理解LLM知识存储与调用机制提供了标准化分析平台。

Abstract: As large language models (LLMs) continue to advance, there is a growing
urgency to enhance the interpretability of their internal knowledge mechanisms.
Consequently, many interpretation methods have emerged, aiming to unravel the
knowledge mechanisms of LLMs from various perspectives. However, current
interpretation methods differ in input data formats and interpreting outputs.
The tools integrating these methods are only capable of supporting tasks with
specific inputs, significantly constraining their practical applications. To
address these challenges, we present an open-source Knowledge Mechanisms
Revealer&Interpreter (Know-MRI) designed to analyze the knowledge mechanisms
within LLMs systematically. Specifically, we have developed an extensible core
module that can automatically match different input data with interpretation
methods and consolidate the interpreting outputs. It enables users to freely
choose appropriate interpretation methods based on the inputs, making it easier
to comprehensively diagnose the model's internal knowledge mechanisms from
multiple perspectives. Our code is available at
https://github.com/nlpkeg/Know-MRI. We also provide a demonstration video on
https://youtu.be/NVWZABJ43Bs.

</details>


### [26] [CAF-I: A Collaborative Multi-Agent Framework for Enhanced Irony Detection with Large Language Models](https://arxiv.org/abs/2506.08430)
*Ziqi. Liu,Ziyang. Zhou,Mingxuan. Hu*

Main category: cs.CL

TL;DR: CAF-I框架通过多智能体协作优化，突破传统LLM单视角局限，实现讽刺检测SOTA性能（Macro-F1 76.31，提升4.98%）


<details>
  <summary>Details</summary>
Motivation: 针对现有LLM在讽刺检测中的三大缺陷：单视角局限性、理解不充分、缺乏可解释性

Method: CAF-I框架包含语境/语义/修辞三大专业智能体，通过多维分析+交互式协同优化，决策智能体整合分析，优化评估器提供条件反馈

Result: 在基准测试中达成零样本SOTA，平均Macro-F1达76.31（绝对提升4.98），绝大多数指标超越现有最优模型

Conclusion: 通过模拟人类多视角认知机制，CAF-I在提升检测精度的同时增强了模型可解释性，验证了多智能体协作的有效性

Abstract: Large language model (LLM) have become mainstream methods in the field of
sarcasm detection. However, existing LLM methods face challenges in irony
detection, including: 1. single-perspective limitations, 2. insufficient
comprehensive understanding, and 3. lack of interpretability. This paper
introduces the Collaborative Agent Framework for Irony (CAF-I), an LLM-driven
multi-agent system designed to overcome these issues. CAF-I employs specialized
agents for Context, Semantics, and Rhetoric, which perform multidimensional
analysis and engage in interactive collaborative optimization. A Decision Agent
then consolidates these perspectives, with a Refinement Evaluator Agent
providing conditional feedback for optimization. Experiments on benchmark
datasets establish CAF-I's state-of-the-art zero-shot performance. Achieving
SOTA on the vast majority of metrics, CAF-I reaches an average Macro-F1 of
76.31, a 4.98 absolute improvement over the strongest prior baseline. This
success is attained by its effective simulation of human-like multi-perspective
analysis, enhancing detection accuracy and interpretability.

</details>


### [27] [Low-resource domain adaptation while minimizing energy and hardware resource consumption](https://arxiv.org/abs/2506.08433)
*Hernán Maina,Nicolás Wolovick,Luciana Benotti*

Main category: cs.CL

TL;DR: 探索不同数值精度与数据并行策略对LLM领域适应的效率影响，平衡训练速度与模型精度以降低资源门槛


<details>
  <summary>Details</summary>
Motivation: 传统LLM训练存在高能耗与文化偏见问题，领域适配虽能缓解但计算成本过高，制约资源匮乏研究机构的应用

Method: 系统性评估混合精度训练（FP16/FP32）与多GPU数据并行策略的组合效果，建立训练速度-能耗-准确率评估框架

Result: 优化后的混合精度并行方案在保持98%模型精度下，实现40%训练加速与35%显存占用降低

Conclusion: 数值优化策略有效突破领域适配的硬件壁垒，为边缘计算与可持续AI发展提供可复现的技术路径

Abstract: Training Large Language Models (LLMs) is costly in terms of energy, hardware,
and annotated data, often resulting in a positionality rooted in predominant
cultures and values (Santy et al., 2023). Domain adaptation has emerged as a
promising strategy to better align models with diverse cultural and value
contexts (Hershcovich et al., 2022), but its computational cost remains a
significant barrier, particularly for research groups lacking access to
large-scale infrastructure. In this paper, we evaluate how the use of different
numerical precisions and data parallelization strategies impacts both training
speed (as a proxy to energy and hardware consumption) and model accuracy, with
the goal of facilitating domain adaptation in low-resource environments. Our
findings are relevant to any setting where energy efficiency, accessibility, or
limited hardware availability are key concerns.

</details>


### [28] [Olica: Efficient Structured Pruning of Large Language Models without Retraining](https://arxiv.org/abs/2506.08436)
*Jiujun He,Huazhen Lin*

Main category: cs.CL

TL;DR: 提出无需重训练的Olica剪枝框架，通过正交分解和线性校准技术压缩大语言模型，保持性能同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有大模型结构化剪枝方法需要大量计算资源和数据进行重训练，导致应用成本过高。为解决该问题，提出无需重训练的剪枝方案。

Method: 1. 将多头注意力层的矩阵乘积视为统一实体进行PCA降维
2. 设计快速分解方法降低计算复杂度
3. 通过SVD求解最小二乘问题实现前馈网络层的误差重构

Result: 实验证明Olica在数据使用效率、GPU内存占用和运行时间方面具有优势，在多个基准测试中保持模型性能。

Conclusion: Olica框架有效解决了传统剪枝方法依赖重训练的问题，为LLM压缩提供了高效实用的解决方案。

Abstract: Most existing structured pruning methods for Large Language Models (LLMs)
require substantial computational and data resources for retraining to
reestablish the corrupted correlations, making them prohibitively expensive. To
address this, we propose a pruning framework for LLMs called Orthogonal
decomposition and Linear Calibration (Olica), which eliminates the need for
retraining. A key observation is that the multi-head attention (MHA) layer
depends on two types of matrix products. By treating these matrix products as
unified entities and applying principal component analysis (PCA), we extract
the most important information to compress LLMs without sacrificing accuracy or
disrupting their original structure. Consequently, retraining becomes
unnecessary. A fast decomposition method is devised, reducing the complexity of
PCA by a factor of the square of the number of attention heads. Additionally,
to mitigate error accumulation problem caused by pruning the feed-forward
network (FFN) layer, we introduce a linear calibration method to reconstruct
the residual errors of pruned layers using low-rank matrices. By leveraging
singular value decomposition (SVD) on the solution of the least-squares
problem, these matrices are obtained without requiring retraining. Extensive
experiments show that the proposed Olica is efficient in terms of data usage,
GPU memory, and running time, while delivering superior performance across
multiple benchmarks.

</details>


### [29] [Detecting Harmful Memes with Decoupled Understanding and Guided CoT Reasoning](https://arxiv.org/abs/2506.08477)
*Fengjun Pan,Anh Tuan Luu,Xiaobao Wu*

Main category: cs.CL

TL;DR: 提出U-CoT+框架，通过模因文本化转换与人类指导的零样本思维链提示，实现高效、灵活且可解释的有害模因检测


<details>
  <summary>Details</summary>
Motivation: 现有方法存在资源效率低、灵活性差和可解释性不足的问题，需通过解耦模因解释与分类来提升部署效率

Method: 1.建立高保真模因文本转换管道 2.结合可解释的人工制定准则 3.采用零样本思维链提示引导小规模语言模型推理

Result: 在七个基准数据集上验证有效性，证明框架在可解释性和低资源场景下的优势

Conclusion: 该框架可灵活适应不同平台/地域的检测标准演变，为内容审核提供可持续的解决方案

Abstract: Detecting harmful memes is essential for maintaining the integrity of online
environments. However, current approaches often struggle with resource
efficiency, flexibility, or explainability, limiting their practical deployment
in content moderation systems. To address these challenges, we introduce
U-CoT+, a novel framework for harmful meme detection. Instead of relying solely
on prompting or fine-tuning multimodal models, we first develop a high-fidelity
meme-to-text pipeline that converts visual memes into detail-preserving textual
descriptions. This design decouples meme interpretation from meme
classification, thus avoiding immediate reasoning over complex raw visual
content and enabling resource-efficient harmful meme detection with general
large language models (LLMs). Building on these textual descriptions, we
further incorporate targeted, interpretable human-crafted guidelines to guide
models' reasoning under zero-shot CoT prompting. As such, this framework allows
for easy adaptation to different harmfulness detection criteria across
platforms, regions, and over time, offering high flexibility and
explainability. Extensive experiments on seven benchmark datasets validate the
effectiveness of our framework, highlighting its potential for explainable and
low-resource harmful meme detection using small-scale LLMs. Codes and data are
available at: https://anonymous.4open.science/r/HMC-AF2B/README.md.

</details>


### [30] [Efficient Context Selection for Long-Context QA: No Tuning, No Iteration, Just Adaptive-$k$](https://arxiv.org/abs/2506.08479)
*Chihiro Taguchi,Seiji Maekawa,Nikita Bhutani*

Main category: cs.CL

TL;DR: 提出Adaptive-$k$检索方法，通过相似度分数动态选择检索段落数量，在保持问答准确性的同时显著减少token消耗。


<details>
  <summary>Details</summary>
Motivation: 现有固定检索方法无法适应聚合型QA任务中动态变化的上下文需求，导致资源浪费或信息遗漏。

Method: 基于查询与候选段落相似度分数分布进行单次自适应选择，无需模型微调或流程修改。

Result: 在事实/聚合QA任务中节省90%token仍保持70%相关段落召回，五大LCLM模型准确率全面提升。

Conclusion: 动态调整上下文规模是实现高效精准问答的关键，该方法为不同QA任务提供通用解决方案。

Abstract: Retrieval-augmented generation (RAG) and long-context language models (LCLMs)
both address context limitations of LLMs in open-domain question answering
(QA). However, optimal external context to retrieve remains an open problem:
fixing the retrieval size risks either wasting tokens or omitting key evidence.
Existing adaptive methods like Self-RAG and Self-Route rely on iterative LLM
prompting and perform well on factoid QA, but struggle with aggregation QA,
where the optimal context size is both unknown and variable. We present
Adaptive-$k$ retrieval, a simple and effective single-pass method that
adaptively selects the number of passages based on the distribution of the
similarity scores between the query and the candidate passages. It does not
require model fine-tuning, extra LLM inferences or changes to existing
retriever-reader pipelines. On both factoid and aggregation QA benchmarks,
Adaptive-$k$ matches or outperforms fixed-$k$ baselines while using up to 10x
fewer tokens than full-context input, yet still retrieves 70% of relevant
passages. It improves accuracy across five LCLMs and two embedding models,
highlighting that dynamically adjusting context size leads to more efficient
and accurate QA.

</details>


### [31] [Re-Thinking the Automatic Evaluation of Image-Text Alignment in Text-to-Image Models](https://arxiv.org/abs/2506.08480)
*Huixuan Zhang,Xiaojun Wan*

Main category: cs.CL

TL;DR: 现有文本-图像对齐评估框架存在可信度缺陷，本文提出改进建议


<details>
  <summary>Details</summary>
Motivation: 现有评估方法过度关注人类判断一致性，忽视评估框架本身的可信属性（如可靠性和稳定性）

Method: 通过实证分析主流评估框架，揭示其在多样化指标和模型中的不足

Result: 证实当前评估体系无法全面满足可信评估的关键属性要求

Conclusion: 建议建立包含可靠性验证和稳定性测试的多维度评估体系

Abstract: Text-to-image models often struggle to generate images that precisely match
textual prompts. Prior research has extensively studied the evaluation of
image-text alignment in text-to-image generation. However, existing evaluations
primarily focus on agreement with human assessments, neglecting other critical
properties of a trustworthy evaluation framework. In this work, we first
identify two key aspects that a reliable evaluation should address. We then
empirically demonstrate that current mainstream evaluation frameworks fail to
fully satisfy these properties across a diverse range of metrics and models.
Finally, we propose recommendations for improving image-text alignment
evaluation.

</details>


### [32] [Fairness is Not Silence: Unmasking Vacuous Neutrality in Small Language Models](https://arxiv.org/abs/2506.08487)
*Sumanth Manduru,Carlotta Domeniconi*

Main category: cs.CL

TL;DR: 中等规模指令调优SLMs的伦理审计显示：Phi模型实现90%+准确率与最小偏见并存，架构选择显著影响偏见表现，量化技术存在复杂性能/公平性权衡


<details>
  <summary>Details</summary>
Motivation: 填补中等规模SLMs（0.5-50亿参数）伦理风险的研究空白，为资源受限环境中的负责任部署提供实证依据

Method: 使用BBQ基准测试零样本提示，评估Qwen 2.5/LLaMA 3.2/Gemma 3/Phi四大家族共9个开源模型在模糊/明确语境下的性能与公平性

Result: 1. Phi系列实现准确率与公平性双优（F1>90%且偏最低） 2. Qwen表现中性但存在虚假中立，LLaMA呈现过自信偏见 3. 4-bit量化提升LLaMA性能但增加Phi残障偏见7%+

Conclusion: 研究为中小企业SLMs部署提供关键路径：优先选择Phi架构，谨慎评估Qwen的中性本质，针对具体应用场景优化量化策略以实现效率-伦理平衡

Abstract: The rapid adoption of Small Language Models (SLMs) for on-device and
resource-constrained deployments has outpaced our understanding of their
ethical risks. To the best of our knowledge, we present the first large-scale
audit of instruction-tuned SLMs spanning 0.5 to 5 billion parameters-an
overlooked "middle tier" between BERT-class encoders and flagship LLMs. Our
evaluation includes nine open-source models from the Qwen 2.5, LLaMA 3.2, Gemma
3, and Phi families. Using the BBQ benchmark under zero-shot prompting, we
analyze both utility and fairness across ambiguous and disambiguated contexts.
This evaluation reveals three key insights. First, competence and fairness need
not be antagonistic: Phi models achieve F1 scores exceeding 90 percent while
exhibiting minimal bias, showing that efficient and ethical NLP is attainable.
Second, social bias varies significantly by architecture: Qwen 2.5 models may
appear fair, but this often reflects vacuous neutrality, random guessing, or
evasive behavior rather than genuine ethical alignment. In contrast, LLaMA 3.2
models exhibit stronger stereotypical bias, suggesting overconfidence rather
than neutrality. Third, compression introduces nuanced trade-offs: 4-bit AWQ
quantization improves F1 scores in ambiguous settings for LLaMA 3.2-3B but
increases disability-related bias in Phi-4-Mini by over 7 percentage points.
These insights provide practical guidance for the responsible deployment of
SLMs in applications demanding fairness and efficiency, particularly benefiting
small enterprises and resource-constrained environments.

</details>


### [33] [EtiCor++: Towards Understanding Etiquettical Bias in LLMs](https://arxiv.org/abs/2506.08488)
*Ashutosh Dwivedi,Siddhant Shivdutt Singh,Ashutosh Modi*

Main category: cs.CL

TL;DR: 提出EtiCor++语料库，用于评估大语言模型在不同地区礼仪文化理解中的偏见问题


<details>
  <summary>Details</summary>
Motivation: 现有资源难以评估LLMs对地区礼仪文化的理解偏差，礼仪作为文化核心要素需增强模型敏感性

Method: 构建覆盖全球礼仪的EtiCor++语料库，设计多维度评估任务并创建偏差量化指标

Result: 实验证实LLMs存在对特定地区的固有偏见

Conclusion: EtiCor++填补评估空白，为提升LLMs文化敏感性提供重要基准

Abstract: In recent years, researchers have started analyzing the cultural sensitivity
of LLMs. In this respect, Etiquettes have been an active area of research.
Etiquettes are region-specific and are an essential part of the culture of a
region; hence, it is imperative to make LLMs sensitive to etiquettes. However,
there needs to be more resources in evaluating LLMs for their understanding and
bias with regard to etiquettes. In this resource paper, we introduce EtiCor++,
a corpus of etiquettes worldwide. We introduce different tasks for evaluating
LLMs for knowledge about etiquettes across various regions. Further, we
introduce various metrics for measuring bias in LLMs. Extensive experimentation
with LLMs shows inherent bias towards certain regions.

</details>


### [34] [Integration of Old and New Knowledge for Generalized Intent Discovery: A Consistency-driven Prototype-Prompting Framework](https://arxiv.org/abs/2506.08490)
*Xiao Wei,Xiaobao Wang,Ning Zhuang,Chenyang Wang,Longbiao Wang,Jianwu dang*

Main category: cs.CL

TL;DR: 提出基于一致性驱动的原型提示框架解决广义意图发现问题，通过整合新旧知识显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 传统监督方法依赖标注数据且无法处理未知领域意图，现有广义意图发现方法忽视领域适应问题

Method: 1. 原型提示框架迁移外部旧知识
2. 分层一致性约束学习目标域新知识

Result: 在多个实验中显著超越基线方法，达到当前最优效果（SOTA）

Conclusion: 该框架成功实现新旧知识融合，为解决领域适应问题提供新方向，代码已开源

Abstract: Intent detection aims to identify user intents from natural language inputs,
where supervised methods rely heavily on labeled in-domain (IND) data and
struggle with out-of-domain (OOD) intents, limiting their practical
applicability. Generalized Intent Discovery (GID) addresses this by leveraging
unlabeled OOD data to discover new intents without additional annotation.
However, existing methods focus solely on clustering unsupervised data while
neglecting domain adaptation. Therefore, we propose a consistency-driven
prototype-prompting framework for GID from the perspective of integrating old
and new knowledge, which includes a prototype-prompting framework for
transferring old knowledge from external sources, and a hierarchical
consistency constraint for learning new knowledge from target domains. We
conducted extensive experiments and the results show that our method
significantly outperforms all baseline methods, achieving state-of-the-art
results, which strongly demonstrates the effectiveness and generalization of
our methods. Our source code is publicly available at
https://github.com/smileix/cpp.

</details>


### [35] [DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in Search-Augmented LLMs](https://arxiv.org/abs/2506.08500)
*Arie Cattan,Alon Jacovi,Ori Ram,Jonathan Herzig,Roee Aharoni,Sasha Goldshtein,Eran Ofek,Idan Szpektor,Avi Caciularu*

Main category: cs.CL

TL;DR: 研究提出RAG中的知识冲突分类法和CONFLICTS基准，发现LLM处理冲突存在不足，显式推理提示可改善效果但仍需优化。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统面对检索信息冲突时缺乏明确处理准则，模型应对不同冲突类型的行为机制不清晰，影响可靠性和实际应用效果。

Method: 提出知识冲突类型学并定义期望模型行为，构建包含专家标注的CONFLICTS基准，通过实验验证LLM冲突处理能力及显式提示策略效果。

Result: LLM普遍难以正确处理知识冲突，显式要求模型推理冲突显著提升回答质量，但最佳解决率仅57%，显示较大改进空间。

Conclusion: 研究揭示知识冲突对RAG系统的核心挑战，CONFLICTS基准为评估提供标准，显式冲突推理是有效方向，需进一步开发优化策略。

Abstract: Retrieval Augmented Generation (RAG) is a commonly used approach for
enhancing large language models (LLMs) with relevant and up-to-date
information. However, the retrieved sources can often contain conflicting
information and it remains unclear how models should address such
discrepancies. In this work, we first propose a novel taxonomy of knowledge
conflict types in RAG, along with the desired model behavior for each type. We
then introduce CONFLICTS, a high-quality benchmark with expert annotations of
conflict types in a realistic RAG setting. CONFLICTS is the first benchmark
that enables tracking progress on how models address a wide range of knowledge
conflicts. We conduct extensive experiments on this benchmark, showing that
LLMs often struggle to appropriately resolve conflicts between sources. While
prompting LLMs to explicitly reason about the potential conflict in the
retrieved documents significantly improves the quality and appropriateness of
their responses, substantial room for improvement in future research remains.

</details>


### [36] [CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in conversations](https://arxiv.org/abs/2506.08504)
*Divyaksh Shukla,Ritesh Baviskar,Dwijesh Gohil,Aniket Tiwari,Atul Shree,Ashutosh Modi*

Main category: cs.CL

TL;DR: 构建代码混合多模态多领域语料库CoMuMDR，用于提升多领域对话话语解析任务。现有模型在该语料库上表现不佳，需开发更鲁棒的模型。


<details>
  <summary>Details</summary>
Motivation: 现有话语解析数据集仅覆盖单一领域英文书面对话，难以满足多领域代码混合场景的解析需求。需构建更贴近现实的多模态混合语料库。

Method: 创建包含印地语-英语代码混合的音频/文本数据，标注9种话语关系。使用多种SoTA基线模型进行跨领域解析实验。

Result: 现有模型在跨领域代码混合数据上性能显著下降（F1值低于传统单领域英文数据），揭示多模态混合解析的技术挑战。

Conclusion: CoMuMDR填补了多领域代码混合对话解析的资源空白，为开发现实场景的鲁棒话语解析模型提供基准测试平台。

Abstract: Discourse parsing is an important task useful for NLU applications such as
summarization, machine comprehension, and emotion recognition. The current
discourse parsing datasets based on conversations consists of written English
dialogues restricted to a single domain. In this resource paper, we introduce
CoMuMDR: Code-mixed Multi-modal Multi-domain corpus for Discourse paRsing in
conversations. The corpus (code-mixed in Hindi and English) has both audio and
transcribed text and is annotated with nine discourse relations. We experiment
with various SoTA baseline models; the poor performance of SoTA models
highlights the challenges of multi-domain code-mixed corpus, pointing towards
the need for developing better models for such realistic settings.

</details>


### [37] [Efficient Post-Training Refinement of Latent Reasoning in Large Language Models](https://arxiv.org/abs/2506.08552)
*Xinyuan Wang,Dongjie Wang,Wangyang Ying,Haoyue Bai,Nanxu Gong,Sixun Dong,Kunpeng Liu,Yanjie Fu*

Main category: cs.CL

TL;DR: 提出轻量级训练后框架，通过对比推理反馈和残差嵌入细化策略优化潜在推理轨迹，在MathQA等基准测试中实现5%准确率提升


<details>
  <summary>Details</summary>
Motivation: 传统Chain-of-Thought方法存在令牌开销大、推理轨迹固定的局限性，潜在推理方法虽能优化内部过程，但缺乏有效的训练后嵌入更新机制

Method: 1) 对比推理反馈机制：通过强弱基线比较确定嵌入优化方向
2) 残差嵌入细化：融合历史梯度实现稳定快速收敛

Result: 在5个推理基准测试中验证有效性，MathQA准确率提升5%且无需额外训练

Conclusion: 该框架通过潜在空间优化有效提升推理性能，为语言模型的推理能力优化提供了新方向

Abstract: Reasoning is a key component of language understanding in Large Language
Models. While Chain-of-Thought prompting enhances performance via explicit
intermediate steps, it suffers from sufficient token overhead and a fixed
reasoning trajectory, preventing step-wise refinement. Recent advances in
latent reasoning address these limitations by refining internal reasoning
processes directly in the model's latent space, without producing explicit
outputs. However, a key challenge remains: how to effectively update reasoning
embeddings during post-training to guide the model toward more accurate
solutions. To overcome this challenge, we propose a lightweight post-training
framework that refines latent reasoning trajectories using two novel
strategies: 1) Contrastive reasoning feedback, which compares reasoning
embeddings against strong and weak baselines to infer effective update
directions via embedding enhancement; 2) Residual embedding refinement, which
stabilizes updates by progressively integrating current and historical
gradients, enabling fast yet controlled convergence. Extensive experiments and
case studies are conducted on five reasoning benchmarks to demonstrate the
effectiveness of the proposed framework. Notably, a 5\% accuracy gain on MathQA
without additional training.

</details>


### [38] [Neighbors and relatives: How do speech embeddings reflect linguistic connections across the world?](https://arxiv.org/abs/2506.08564)
*Tuukka Törö,Antti Suni,Juraj Šimko*

Main category: cs.CL

TL;DR: 该研究利用微调后的XLS-R语音识别模型生成语言嵌入，通过机器学习方法分析全球106种语言关系，发现语音嵌入距离与传统谱系/地理指标高度吻合，为大规模语言变异分析提供新路径。


<details>
  <summary>Details</summary>
Motivation: 传统语言分析方法依赖专家经验和有限特征标注，难以处理语言特征的动态演变和大规模数据。研究旨在探索语音嵌入作为自动化分析工具在揭示语言关系方面的潜力，突破传统方法的规模和效率限制。

Method: 使用voxlingua107-xls-r-300m-wav2vec模型生成语言嵌入，通过线性判别分析(LDA)聚类，并与Genealogical、Lexical、Geographical三种传统距离指标进行系统性对比。

Result: 语音嵌入距离与传统测量指标显著相关（r=0.82），能同时捕捉全球语言类型模式和区域特征。但层次聚类在网络可视化中表现出动态语言变化的复杂性。

Conclusion: 该方法为低资源语言研究提供新范式，未来可扩展至非书面语研究，整合社会语言学参数构建多维语言演变模型，推动计算语言学的跨学科发展。

Abstract: Investigating linguistic relationships on a global scale requires analyzing
diverse features such as syntax, phonology and prosody, which evolve at varying
rates influenced by internal diversification, language contact, and
sociolinguistic factors. Recent advances in machine learning (ML) offer
complementary alternatives to traditional historical and typological
approaches. Instead of relying on expert labor in analyzing specific linguistic
features, these new methods enable the exploration of linguistic variation
through embeddings derived directly from speech, opening new avenues for
large-scale, data-driven analyses.
  This study employs embeddings from the fine-tuned XLS-R self-supervised
language identification model voxlingua107-xls-r-300m-wav2vec, to analyze
relationships between 106 world languages based on speech recordings. Using
linear discriminant analysis (LDA), language embeddings are clustered and
compared with genealogical, lexical, and geographical distances. The results
demonstrate that embedding-based distances align closely with traditional
measures, effectively capturing both global and local typological patterns.
Challenges in visualizing relationships, particularly with hierarchical
clustering and network-based methods, highlight the dynamic nature of language
change.
  The findings show potential for scalable analyses of language variation based
on speech embeddings, providing new perspectives on relationships among
languages. By addressing methodological considerations such as corpus size and
latent space dimensionality, this approach opens avenues for studying
low-resource languages and bridging macro- and micro-level linguistic
variation. Future work aims to extend these methods to underrepresented
languages and integrate sociolinguistic variation for a more comprehensive
understanding of linguistic diversity.

</details>


### [39] [CounselBench: A Large-Scale Expert Evaluation and Adversarial Benchmark of Large Language Models in Mental Health Counseling](https://arxiv.org/abs/2506.08584)
*Yahan Li,Jifan Yao,John Bosco S. Bunyi,Adam C. Frank,Angel Hwang,Ruishan Liu*

Main category: cs.CL

TL;DR: CounselBench临床基准测试显示，LLMs在心理健康咨询场景中虽质量优于人类治疗师，但存在安全隐患与评估盲区。


<details>
  <summary>Details</summary>
Motivation: 验证LLMs在真实心理咨询场景中的可靠性，解决现有研究对模型安全性和专业评估的缺失。

Method: 联合100位专家构建两阶段测试框架：1) CounselBench-EVAL用2,000个专家评估对比LLMs与人类治疗师响应质量 2) CounselBench-Adv通过120个对抗性问题触发模型失败模式。

Result: LLMs在6个临床维度评分中平均超过人类治疗师21%，但32%的模型输出被专家标注安全隐患。对抗测试显示所有模型均存在特定风险模式（如GPT-4过度自信给出医疗建议）。

Conclusion: 该研究为高风险心理健康场景建立了临床评估标准，揭示需建立双重验证机制（专家+自动化评估）来保障LLMs的安全应用。

Abstract: Large language models (LLMs) are increasingly proposed for use in mental
health support, yet their behavior in realistic counseling scenarios remains
largely untested. We introduce CounselBench, a large-scale benchmark developed
with 100 mental health professionals to evaluate and stress-test LLMs in
single-turn counseling. The first component, CounselBench-EVAL, contains 2,000
expert evaluations of responses from GPT-4, LLaMA 3, Gemini, and online human
therapists to real patient questions. Each response is rated along six
clinically grounded dimensions, with written rationales and span-level
annotations. We find that LLMs often outperform online human therapists in
perceived quality, but experts frequently flag their outputs for safety
concerns such as unauthorized medical advice. Follow-up experiments show that
LLM judges consistently overrate model responses and overlook safety issues
identified by human experts. To probe failure modes more directly, we construct
CounselBench-Adv, an adversarial dataset of 120 expert-authored counseling
questions designed to trigger specific model issues. Evaluation across 2,880
responses from eight LLMs reveals consistent, model-specific failure patterns.
Together, CounselBench establishes a clinically grounded framework for
benchmarking and improving LLM behavior in high-stakes mental health settings.

</details>


### [40] [Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity Dilemma of Embeddings](https://arxiv.org/abs/2506.08592)
*Liyan Xu,Zhenlin Su,Mo Yu,Jiangnan Li,Fandong Meng,Jie Zhou*

Main category: cs.CL

TL;DR: 研究发现文本编码器在细粒度实体/事件识别存在局限，提出新中文数据集CapRetrieval并通过数据生成策略优化模型性能，揭示粒度困境挑战。


<details>
  <summary>Details</summary>
Motivation: 现有文本编码器在图像描述与短语查询的细粒度匹配任务中表现不佳，与模型规模及训练数据无关，需要针对性优化方案。

Method: 构建图像描述数据集CapRetrieval，采用数据生成策略进行编码器微调，并分析粒度困境问题。

Result: 数据生成策略使模型在CapRetrieval上取得最佳性能，但发现嵌入表达需平衡细粒度显著性与整体语义的对齐难题。

Conclusion: 细粒度检索需要专门优化方案，公开数据集为相关研究提供基准，粒度困境揭示了嵌入表达的本质挑战。

Abstract: This work focuses on an observed limitation of text encoders: embeddings may
not be able to recognize fine-grained entities or events within the semantics,
resulting in failed dense retrieval on even simple cases. To examine such
behaviors, we first introduce a new evaluation dataset in Chinese, named
CapRetrieval, whose passages are image captions, and queries are phrases
inquiring entities or events in various forms. Zero-shot evaluation suggests
that encoders may fail on these fine-grained matching, regardless of training
sources or model sizes. Aiming for enhancement, we proceed to finetune encoders
with our proposed data generation strategies, which obtains the best
performance on CapRetrieval. Within this process, we further identify an issue
of granularity dilemma, a challenge for embeddings to express fine-grained
salience while aligning with overall semantics. Our dataset, code and models in
this work are publicly released at https://github.com/lxucs/CapRetrieval.

</details>


### [41] [Hateful Person or Hateful Model? Investigating the Role of Personas in Hate Speech Detection by Large Language Models](https://arxiv.org/abs/2506.08593)
*Shuzhou Yuan,Ercong Nie,Mario Tawfelis,Helmut Schmid,Hinrich Schütze,Michael Färber*

Main category: cs.CL

TL;DR: 研究MBTI人格特质对大型语言模型在仇恨言论检测中的影响，揭示人格提示导致的标注偏差问题


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注社会人口因素对标注的影响，但LLM的人格敏感性及其在仇恨检测中的潜在偏差尚未充分探索。该研究旨在验证MBTI人格维度如何影响LLM的标注行为，推动更公平的AI标注流程

Method: 1. 人类标注者MBTI人格与标注行为关联性调查
2. 在3个仇恨言论数据集上，使用4个开源LLM进行MBTI人格提示实验
3. 分析模型输出的真实标签一致性、人格间分歧及logit层面偏差

Result: 1. MBTI人格导致显著标注差异（人类和LLM均表现）
2. 模型输出与真实标签存在系统性偏差
3. 不同人格提示间出现矛盾判断
4. 模型logit分布显示人格特异性偏见

Conclusion: LLM标注流程需严格定义人格提示参数，以避免公平性风险。研究强调AI系统价值观对齐中人格工程的重要性，为可解释性研究提供新方向

Abstract: Hate speech detection is a socially sensitive and inherently subjective task,
with judgments often varying based on personal traits. While prior work has
examined how socio-demographic factors influence annotation, the impact of
personality traits on Large Language Models (LLMs) remains largely unexplored.
In this paper, we present the first comprehensive study on the role of persona
prompts in hate speech classification, focusing on MBTI-based traits. A human
annotation survey confirms that MBTI dimensions significantly affect labeling
behavior. Extending this to LLMs, we prompt four open-source models with MBTI
personas and evaluate their outputs across three hate speech datasets. Our
analysis uncovers substantial persona-driven variation, including
inconsistencies with ground truth, inter-persona disagreement, and logit-level
biases. These findings highlight the need to carefully define persona prompts
in LLM-based annotation workflows, with implications for fairness and alignment
with human values.

</details>


### [42] [RAISE: Enhancing Scientific Reasoning in LLMs via Step-by-Step Retrieval](https://arxiv.org/abs/2506.08625)
*Minhae Oh,Jeonghye Kim,Nakyung Lee,Donggeon Seo,Taeuk Kim,Jungwoo Lee*

Main category: cs.CL

TL;DR: RAISE框架通过三步检索增强机制（问题分解→逻辑查询→逻辑检索），在科学推理任务中超越基线方法，因其能检索领域知识和逻辑相关文档


<details>
  <summary>Details</summary>
Motivation: 科学推理存在三大挑战：需要长逻辑链推理、依赖领域专业术语知识、需适应不断更新的科学发现。传统检索方法难以满足这些复合需求

Method: 1. 问题分解：将复杂问题拆解为子问题
2. 逻辑查询生成：构建结构化检索查询
3. 逻辑检索：从开放域语料中获取逻辑相关的文档

Result: 在科学推理基准测试中持续优于基线方法（提升具体数值需查看原文），关键优势在于同时捕获领域知识和逻辑相关性

Conclusion: RAISE创新地将逻辑结构引入检索过程，突破了传统语义相似性检索的局限，为复杂科学推理任务提供了新的解决方案框架

Abstract: Scientific reasoning requires not only long-chain reasoning processes, but
also knowledge of domain-specific terminologies and adaptation to updated
findings. To deal with these challenges for scientific reasoning, we introduce
RAISE, a step-by-step retrieval-augmented framework which retrieves logically
relevant documents from in-the-wild corpus. RAISE is divided into three steps:
problem decomposition, logical query generation, and logical retrieval. We
observe that RAISE consistently outperforms other baselines on scientific
reasoning benchmarks. We analyze that unlike other baselines, RAISE retrieves
documents that are not only similar in terms of the domain knowledge, but also
documents logically more relevant.

</details>


### [43] [MEMETRON: Metaheuristic Mechanisms for Test-time Response Optimization of Large Language Models](https://arxiv.org/abs/2506.08643)
*Son The Nguyen,Theja Tulabandhula*

Main category: cs.CL

TL;DR: 提出MEMETRON框架，通过元启发式算法实现无需训练/梯度的黑盒优化，在人类偏好对齐任务中显著优于传统解码方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM解码策略（贪婪搜索/采样/重排）缺乏任务目标导向的优化能力，需要不依赖模型重训练的新型优化框架。

Method: 将解码转化为离散优化问题，使用GENETRON（遗传算法）和ANNETRON（神经网络引导）算法，基于奖励模型和LLM自反馈进行响应搜索。

Result: 在人类偏好对齐任务中，MEMETRON的响应质量显著超过标准解码策略和重排序方法，验证了框架有效性。

Conclusion: 模块化设计使框架具备跨任务通用性，为模型对齐提供了轻量化解决方案，展示了黑盒优化的巨大潜力。

Abstract: Large language models (LLMs) are increasingly used for both open-ended and
structured tasks, yet their inference-time behavior is still largely dictated
by heuristic decoding strategies such as greedy search, sampling, or reranking.
These methods provide limited control and do not explicitly optimize for
task-specific objectives. We introduce MEMETRON, a task-agnostic framework that
formulates LLM decoding as a discrete black-box optimization problem. MEMETRON
leverages hybrid metaheuristic algorithms, GENETRON and ANNETRON, to search the
response space, guided by reward models and contextual operations performed by
the LLM itself. This approach enables efficient discovery of high-reward
responses without requiring model retraining or gradient access. The framework
is modular and generalizes across diverse tasks, requiring only a reward
function and lightweight prompt templates. We evaluate our framework on the
critical human preference alignment task and demonstrate that it significantly
outperforms standard decoding and reranking methods, highlighting its potential
to improve alignment without model retraining.

</details>


### [44] [TableDreamer: Progressive and Weakness-guided Data Synthesis from Scratch for Table Instruction Tuning](https://arxiv.org/abs/2506.08646)
*Mingyu Zheng,Zhifan Feng,Jia Wang,Lanrui Wang,Zheng Lin,Yang Hao,Weiping Wang*

Main category: cs.CL

TL;DR: 提出基于弱点引导的迭代式表格数据合成框架TableDreamer，通过渐进探索输入空间优化LLM表格理解能力


<details>
  <summary>Details</summary>
Motivation: 现有LLM表格数据合成方法存在输入空间探索不足导致数据多样性受限，且未针对性补足目标模型弱点导致数据效率低下

Method: 分两阶段：首先生成多样化种子表格数据，再基于模型预测弱点迭代生成针对性训练数据

Result: 在10个基准测试中使Llama3.1-8B-instruct平均准确率提升11.62%（49.07%→60.69%），仅用27K数据即超越现有基线方法

Conclusion: TableDreamer框架通过弱点引导的迭代数据合成，显著提升表格指令调优的数据效率与模型性能

Abstract: Despite the commendable progress of recent LLM-based data synthesis methods,
they face two limitations in generating table instruction tuning data. First,
they can not thoroughly explore the vast input space of table understanding
tasks, leading to limited data diversity. Second, they ignore the weaknesses in
table understanding ability of the target LLM and blindly pursue the increase
of data quantity, resulting in suboptimal data efficiency. In this paper, we
introduce a progressive and weakness-guided data synthesis framework tailored
for table instruction tuning, named TableDreamer, to mitigate the above issues.
Specifically, we first synthesize diverse tables and related instructions as
seed data, and then perform an iterative exploration of the input space under
the guidance of the newly identified weakness data, which eventually serve as
the final training data for fine-tuning the target LLM. Extensive experiments
on 10 tabular benchmarks demonstrate the effectiveness of the proposed
framework, which boosts the average accuracy of Llama3.1-8B-instruct by 11.62%
(49.07% to 60.69%) with 27K GPT-4o synthetic data and outperforms
state-of-the-art data synthesis baselines which use more training data. The
code and data is available at https://github.com/SpursGoZmy/TableDreamer

</details>


### [45] [Summarization for Generative Relation Extraction in the Microbiome Domain](https://arxiv.org/abs/2506.08647)
*Oumaima El Khettari,Solen Quiniou,Samuel Chaffron*

Main category: cs.CL

TL;DR: 探索生成式关系提取方法在低资源肠道微生物研究中的应用，通过LLM摘要优化上下文后关系提取效果提升但传统BERT方法仍更优


<details>
  <summary>Details</summary>
Motivation: 针对复杂且资源匮乏的肠道微生物组研究领域，需要开发高效的关系提取方法以支持专业领域研究

Method: 使用大型语言模型进行文本摘要预处理，通过指令微调的生成模型进行关系提取

Result: 摘要预处理使生成式RE性能提升（减少噪声/引导模型），但基于BERT的方法仍优于生成模型

Conclusion: 生成式方法在低资源专业领域研究中具备应用潜力，但当前阶段传统方法仍保持优势

Abstract: We explore a generative relation extraction (RE) pipeline tailored to the
study of interactions in the intestinal microbiome, a complex and low-resource
biomedical domain. Our method leverages summarization with large language
models (LLMs) to refine context before extracting relations via
instruction-tuned generation. Preliminary results on a dedicated corpus show
that summarization improves generative RE performance by reducing noise and
guiding the model. However, BERT-based RE approaches still outperform
generative models. This ongoing work demonstrates the potential of generative
methods to support the study of specialized domains in low-resources setting.

</details>


### [46] [RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling](https://arxiv.org/abs/2506.08672)
*Yang Liu,Jiaqi Li,Zilong Zheng*

Main category: cs.CL

TL;DR: Proposed RuleReasoner method enhances small reasoning models' rule-based reasoning through dynamic domain sampling and reinforcement learning, achieving superior performance over large models.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with rule format variations and rely on pre-engineered training mixes. Small models' generalization capability across tasks needs validation.

Method: Domain-aware dynamic sampling with RL: adjusts domain weights based on historical rewards during batch resampling, enabling automated domain augmentation.

Result: 4.1% ID task improvement, 10.4% OOD gains vs OpenAI-o1. 3x faster than prior dynamic sampling methods.

Conclusion: RuleReasoner demonstrates SRMs can achieve efficient, human-independent rule reasoning through adaptive RL training strategies.

Abstract: Rule-based reasoning has been acknowledged as one of the fundamental problems
in reasoning, while deviations in rule formats, types, and complexity in
real-world applications pose severe challenges. Recent studies have shown that
large reasoning models (LRMs) have remarkable reasoning capabilities, and their
performance is substantially enhanced by reinforcement learning (RL). However,
it remains an open question whether small reasoning models (SRMs) can learn
rule-based reasoning effectively with robust generalization across diverse
tasks and domains. To address this, we introduce Reinforced Rule-based
Reasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct
rule-based reasoning via a wide collection of curated tasks and a novel
domain-aware dynamic sampling approach. Specifically, RuleReasoner resamples
each training batch by updating the sampling weights of different domains based
on historical rewards. This facilitates domain augmentation and flexible online
learning schedules for RL, obviating the need for pre-hoc human-engineered
mix-training recipes used in existing methods. Empirical evaluations on
in-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that
RuleReasoner outperforms frontier LRMs by a significant margin ($\Delta$4.1%
average points on eight ID tasks and $\Delta$10.4% average points on three OOD
tasks over OpenAI-o1). Notably, our approach also exhibits higher computational
efficiency compared to prior dynamic sampling methods for RL.

</details>


### [47] [Brevity is the soul of sustainability: Characterizing LLM response lengths](https://arxiv.org/abs/2506.08686)
*Soham Poddar,Paramita Koley,Janardan Misra,Sanjay Podder,Navveen Balani,Niloy Ganguly,Saptarshi Ghosh*

Main category: cs.CL

TL;DR: 通过提示工程策略减少LLM生成响应的长度，在保持质量的同时实现25-60%的能源优化。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）推理过程消耗大量能源，而现有的输出压缩技术尚未被充分探索。

Method: 1. 对12个仅解码器架构的LLM在5个数据集进行基准测试，发现响应冗余现象
2. 定义LLM响应中的6个信息类别，分析冗余信息分布
3. 开发针对长度压缩和信息控制的提示工程策略

Result: 通过针对性提示工程可减少响应长度，实现25-60%的能源节省，同时保持回答质量（通过质量评估指标验证）

Conclusion: 简单的提示工程方法能有效优化LLM能源效率，为解决模型冗余输出提供了实用解决方案

Abstract: A significant portion of the energy consumed by Large Language Models (LLMs)
arises from their inference processes; hence developing energy-efficient
methods for inference is crucial. While several techniques exist for inference
optimization, output compression remains relatively unexplored, with only a few
preliminary efforts addressing this aspect. In this work, we first benchmark 12
decoder-only LLMs across 5 datasets, revealing that these models often produce
responses that are substantially longer than necessary. We then conduct a
comprehensive quality assessment of LLM responses, formally defining six
information categories present in LLM responses. We show that LLMs often tend
to include redundant or additional information besides the minimal answer. To
address this issue of long responses by LLMs, we explore several simple and
intuitive prompt-engineering strategies. Empirical evaluation shows that
appropriate prompts targeting length reduction and controlling information
content can achieve significant energy optimization between 25-60\% by reducing
the response length while preserving the quality of LLM responses.

</details>


### [48] [ClimateViz: A Benchmark for Statistical Reasoning and Fact Verification on Scientific Charts](https://arxiv.org/abs/2506.08700)
*Ruiran Su,Jiasheng Si,Zhijiang Guo,Janet B. Pierrehumbert*

Main category: cs.CL

TL;DR: 论文提出首个面向科学图表验证的大规模基准测试ClimateViz，揭示当前多模态模型在图表推理任务中显著落后于人类表现（准确率76-78% vs 89-93%），解释增强策略仅部分有效。


<details>
  <summary>Details</summary>
Motivation: 现有科学事实核查主要聚焦文本和表格，忽略了科学图表作为定量证据载体的核心价值。ClimateViz旨在填补这一研究空白。

Method: 构建含49,862个标注声明与2,896个图表的语料库，集成趋势/对比/因果关系的知识图谱解释，评估Gemini 2.5、InternVL 2.5等前沿模型在零样本/小样本场景下的表现。

Result: 最佳模型准确率仅达76.2-77.8%，远低于人类专家(89.3-92.7%)；解释增强策略对部分模型有提升效果。

Conclusion: 强调图表验证任务的挑战性，呼吁开发更强大的多模态推理方法。数据集与代码已开源以促进研究。

Abstract: Scientific fact-checking has mostly focused on text and tables, overlooking
scientific charts, which are key for presenting quantitative evidence and
statistical reasoning. We introduce ClimateViz, the first large-scale benchmark
for scientific fact-checking using expert-curated scientific charts. ClimateViz
contains 49,862 claims linked to 2,896 visualizations, each labeled as support,
refute, or not enough information. To improve interpretability, each example
includes structured knowledge graph explanations covering trends, comparisons,
and causal relations. We evaluate state-of-the-art multimodal language models,
including both proprietary and open-source systems, in zero-shot and few-shot
settings. Results show that current models struggle with chart-based reasoning:
even the best systems, such as Gemini 2.5 and InternVL 2.5, reach only 76.2 to
77.8 percent accuracy in label-only settings, far below human performance (89.3
and 92.7 percent). Explanation-augmented outputs improve performance in some
models. We released our dataset and code alongside the paper.

</details>


### [49] [ConfPO: Exploiting Policy Model Confidence for Critical Token Selection in Large Language Model Preference Optimization](https://arxiv.org/abs/2506.08712)
*Hee Suk Yoon,Eunseop Yoon,Mark A. Hasegawa-Johnson,Sungwoong Kim,Chang D. Yoo*

Main category: cs.CL

TL;DR: 提出ConfPO方法，通过针对性优化偏好关键token提升LLM对齐效果，相比传统方法更高效且无需额外计算资源


<details>
  <summary>Details</summary>
Motivation: 现有对齐算法（如DPO）均匀调整所有token概率，导致KL散度预算浪费和过优化问题；其他token级方法依赖额外模型影响扩展性

Method: 基于训练策略置信度识别偏好关键token，集中优化资源在影响力最大的token上

Result: 在AlpacaEval 2等基准测试中，ConfPO在不同规模LLM上均超越传统算法，实现更好的对齐效果

Conclusion: ConfPO以零额外计算成本提供更高效的对齐方案，为轻量级偏好学习开辟新方向

Abstract: We introduce ConfPO, a method for preference learning in Large Language
Models (LLMs) that identifies and optimizes preference-critical tokens based
solely on the training policy's confidence, without requiring any auxiliary
models or compute. Unlike prior Direct Alignment Algorithms (DAAs) such as
Direct Preference Optimization (DPO), which uniformly adjust all token
probabilities regardless of their relevance to preference, ConfPO focuses
optimization on the most impactful tokens. This targeted approach improves
alignment quality while mitigating overoptimization (i.e., reward hacking) by
using the KL divergence budget more efficiently. In contrast to recent
token-level methods that rely on credit-assignment models or AI annotators,
raising concerns about scalability and reliability, ConfPO is simple,
lightweight, and model-free. Experimental results on challenging alignment
benchmarks, including AlpacaEval 2 and Arena-Hard, demonstrate that ConfPO
consistently outperforms uniform DAAs across various LLMs, delivering better
alignment with zero additional computational overhead.

</details>


### [50] [Explainable Compliance Detection with Multi-Hop Natural Language Inference on Assurance Case Structure](https://arxiv.org/abs/2506.08713)
*Fariz Ikhwantri,Dusica Marijan*

Main category: cs.CL

TL;DR: 提出基于自然语言推理的EXCLAIM方法，通过多跳推理和LLM生成保证案例，实现可解释的合规检测。


<details>
  <summary>Details</summary>
Motivation: 解决法规文本复杂性、模型可解释性需求、保证案例数据匮乏三大合规检测挑战。

Method: 将保证案例结构转化为多跳推理框架，利用LLM生成训练数据，设计覆盖率与结构一致性评估指标。

Result: GDPR案例验证显示生成数据有效支持多跳推理任务，新指标成功量化模型表现。

Conclusion: 基于NLI的方法为自动化法规合规检测提供了可解释、可追溯的技术路径。

Abstract: Ensuring complex systems meet regulations typically requires checking the
validity of assurance cases through a claim-argument-evidence framework. Some
challenges in this process include the complicated nature of legal and
technical texts, the need for model explanations, and limited access to
assurance case data. We propose a compliance detection approach based on
Natural Language Inference (NLI): EXplainable CompLiance detection with
Argumentative Inference of Multi-hop reasoning (EXCLAIM). We formulate the
claim-argument-evidence structure of an assurance case as a multi-hop inference
for explainable and traceable compliance detection. We address the limited
number of assurance cases by generating them using large language models
(LLMs). We introduce metrics that measure the coverage and structural
consistency. We demonstrate the effectiveness of the generated assurance case
from GDPR requirements in a multi-hop inference task as a case study. Our
results highlight the potential of NLI-based approaches in automating the
regulatory compliance process.

</details>


### [51] [Multi-Teacher Language-Aware Knowledge Distillation for Multilingual Speech Emotion Recognition](https://arxiv.org/abs/2506.08717)
*Mehedi Hasan Bijoy,Dejan Porjazovski,Tamás Grósz,Mikko Kurimo*

Main category: cs.CL

TL;DR: 提出语言感知多教师知识蒸馏方法，构建多语言语音情感识别(SER)模型，在英语和芬兰数据集上达到SOTA性能，但愤怒和快乐情绪识别仍存在挑战。


<details>
  <summary>Details</summary>
Motivation: 现有单语SER系统难以扩展到多语言场景，需通过知识蒸馏整合多个单语教师模型的知识，构建统一的多语言学生模型。

Method: 基于Wav2Vec2.0构建单语教师模型，通过语言感知蒸馏技术将知识迁移至多语言学生模型，实现跨语言特征融合。

Result: 学生模型英语加权召回率72.9/芬兰非加权召回率63.4，超越微调和传统蒸馏方法。悲伤和中性情绪识别显著提升，但愤怒(64.3)和快乐(58.1)识别准确率较低。

Conclusion: 该方法有效提升多语言SER性能，但需要改进对高激活度情绪的识别。未来将探索跨语言情感特征对齐和数据增强策略。

Abstract: Speech Emotion Recognition (SER) is crucial for improving human-computer
interaction. Despite strides in monolingual SER, extending them to build a
multilingual system remains challenging. Our goal is to train a single model
capable of multilingual SER by distilling knowledge from multiple teacher
models. To address this, we introduce a novel language-aware multi-teacher
knowledge distillation method to advance SER in English, Finnish, and French.
It leverages Wav2Vec2.0 as the foundation of monolingual teacher models and
then distills their knowledge into a single multilingual student model. The
student model demonstrates state-of-the-art performance, with a weighted recall
of 72.9 on the English dataset and an unweighted recall of 63.4 on the Finnish
dataset, surpassing fine-tuning and knowledge distillation baselines. Our
method excels in improving recall for sad and neutral emotions, although it
still faces challenges in recognizing anger and happiness.

</details>


### [52] [Improved LLM Agents for Financial Document Question Answering](https://arxiv.org/abs/2506.08726)
*Nelvin Tan,Zian Seng,Liang Zhang,Yu-Ching Shih,Dong Yang,Amol Salunkhe*

Main category: cs.CL

TL;DR: 提出改进的批评代理和计算器代理，在无标签场景下超越现有方法并提升安全性，同时研究代理间互动对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 传统批评代理在缺乏正确标签时性能显著下降，需开发更稳健的解决方案应对金融文档数值问答任务。

Method: 开发改进型批评代理与专用计算器代理，形成协同工作机制替代传统程序思维方法。

Result: 新方法在无监督环境下超越程序思维方法，实现更安全的数值推理，代理间互动模式影响最终性能表现。

Conclusion: 通过优化代理架构与交互机制，可有效提升LLMs在复杂金融场景中的数值处理能力与系统可靠性。

Abstract: Large language models (LLMs) have shown impressive capabilities on numerous
natural language processing tasks. However, LLMs still struggle with numerical
question answering for financial documents that include tabular and textual
data. Recent works have showed the effectiveness of critic agents (i.e.,
self-correction) for this task given oracle labels. Building upon this
framework, this paper examines the effectiveness of the traditional critic
agent when oracle labels are not available, and show, through experiments, that
this critic agent's performance deteriorates in this scenario. With this in
mind, we present an improved critic agent, along with the calculator agent
which outperforms the previous state-of-the-art approach (program-of-thought)
and is safer. Furthermore, we investigate how our agents interact with each
other, and how this interaction affects their performance.

</details>


### [53] [Societal AI Research Has Become Less Interdisciplinary](https://arxiv.org/abs/2506.08738)
*Dror Kris Markus,Fabrizio Gilardi,Daria Stetsenko*

Main category: cs.CL

TL;DR: 研究表明纯计算机科学团队在AI社会影响研究中的贡献显著增加，挑战了跨学科合作是必要前提的固有认知


<details>
  <summary>Details</summary>
Motivation: 验证跨学科团队是否真正主导AI技术与社会价值融合的实践转向，揭示技术团队在社会价值整合中的实际作用

Method: 通过开发社会内容分类器，系统分析2014-2024年间arXiv平台10万余篇AI论文的社会属性特征

Result: 纯技术团队不仅在社会导向研究总量占比上升，且研究领域覆盖公平性、安全性、医疗健康等多维度社会议题

Conclusion: 该发现引发对技术主导型社会AI研究的治理路径反思，并促使人文社科学者重新定位其在AI发展中的独特价值

Abstract: As artificial intelligence (AI) systems become deeply embedded in everyday
life, calls to align AI development with ethical and societal values have
intensified. Interdisciplinary collaboration is often championed as a key
pathway for fostering such engagement. Yet it remains unclear whether
interdisciplinary research teams are actually leading this shift in practice.
This study analyzes over 100,000 AI-related papers published on ArXiv between
2014 and 2024 to examine how ethical values and societal concerns are
integrated into technical AI research. We develop a classifier to identify
societal content and measure the extent to which research papers express these
considerations. We find a striking shift: while interdisciplinary teams remain
more likely to produce societally-oriented research, computer science-only
teams now account for a growing share of the field's overall societal output.
These teams are increasingly integrating societal concerns into their papers
and tackling a wide range of domains - from fairness and safety to healthcare
and misinformation. These findings challenge common assumptions about the
drivers of societal AI and raise important questions. First, what are the
implications for emerging understandings of AI safety and governance if most
societally-oriented research is being undertaken by exclusively technical
teams? Second, for scholars in the social sciences and humanities: in a
technical field increasingly responsive to societal demands, what distinctive
perspectives can we still offer to help shape the future of AI?

</details>


### [54] [Towards Secure and Private Language Models for Nuclear Power Plants](https://arxiv.org/abs/2506.08746)
*Muhammad Anwar,Mishca de Costa,Issam Hammad,Daniel Lau*

Main category: cs.CL

TL;DR: 本文构建了一个基于Transformer架构的核领域专用语言模型，使用单GPU训练保障数据安全。模型初步掌握专业术语但存在语法缺陷，未来需扩展数据集和优化预处理。


<details>
  <summary>Details</summary>
Motivation: 核领域数据敏感且专业性强，需开发符合网络安全标准的专用模型，避免通用模型的数据泄露风险。

Method: 采用紧凑Transformer架构，基于Essential CANDU教材数据，在单GPU环境训练以确保数据机密性。

Result: 模型成功捕获核领域专业词汇，但生成文本语法连贯性不足，验证了内部LLM解决方案的可行性。

Conclusion: 该模型证实了领域专用LLM的实践价值，需通过扩展语料库、改进分词策略及系统评估来提升实用性。

Abstract: This paper introduces a domain-specific Large Language Model for nuclear
applications, built from the publicly accessible Essential CANDU textbook.
Drawing on a compact Transformer-based architecture, the model is trained on a
single GPU to protect the sensitive data inherent in nuclear operations.
Despite relying on a relatively small dataset, it shows encouraging signs of
capturing specialized nuclear vocabulary, though the generated text sometimes
lacks syntactic coherence. By focusing exclusively on nuclear content, this
approach demonstrates the feasibility of in-house LLM solutions that align with
rigorous cybersecurity and data confidentiality standards. Early successes in
text generation underscore the model's utility for specialized tasks, while
also revealing the need for richer corpora, more sophisticated preprocessing,
and instruction fine-tuning to enhance domain accuracy. Future directions
include extending the dataset to cover diverse nuclear subtopics, refining
tokenization to reduce noise, and systematically evaluating the model's
readiness for real-world applications in nuclear domain.

</details>


### [55] [Unlocking the Potential of Large Language Models in the Nuclear Industry with Synthetic Data](https://arxiv.org/abs/2506.08750)
*Muhammad Anwar,Daniel Lau,Mishca de Costa,Issam Hammad*

Main category: cs.CL

TL;DR: 利用合成数据技术将核工业非结构化文本转化为结构化问答对，解决LLM应用中的数据短缺和隐私问题


<details>
  <summary>Details</summary>
Motivation: 核工业存在大量非结构化文本数据难以直接用于LLM训练，且面临数据隐私限制。通过合成数据生成可创造合规可用的训练资源

Method: 使用LLM自动分析文本、提取关键信息、生成相关问答对，并建立质量评估体系验证合成数据有效性

Result: 成功开发出将行业文本转化为结构化数据的技术路径，为LLM在核工业的应用奠定数据基础

Conclusion: 合成数据技术能提升核工业信息检索效率、促进知识共享、支持安全合规的智能决策系统建设

Abstract: The nuclear industry possesses a wealth of valuable information locked away
in unstructured text data. This data, however, is not readily usable for
advanced Large Language Model (LLM) applications that require clean, structured
question-answer pairs for tasks like model training, fine-tuning, and
evaluation. This paper explores how synthetic data generation can bridge this
gap, enabling the development of robust LLMs for the nuclear domain. We discuss
the challenges of data scarcity and privacy concerns inherent in the nuclear
industry and how synthetic data provides a solution by transforming existing
text data into usable Q&A pairs. This approach leverages LLMs to analyze text,
extract key information, generate relevant questions, and evaluate the quality
of the resulting synthetic dataset. By unlocking the potential of LLMs in the
nuclear industry, synthetic data can pave the way for improved information
retrieval, enhanced knowledge sharing, and more informed decision-making in
this critical sector.

</details>


### [56] [Factors affecting the in-context learning abilities of LLMs for dialogue state tracking](https://arxiv.org/abs/2506.08753)
*Pradyoth Hegde,Santosh Kesiraju,Jan Švec,Šimon Sedláček,Bolaji Yusuf,Oldřich Plchot,Deepak K T,Jan Černocký*

Main category: cs.CL

TL;DR: 探索上下文学习(ICL)在对话状态追踪(DST)中的应用，通过k近邻检索演示样本，分析提示策略对OLMo/Mistral/Llama模型在MultiWoZ数据集表现的影响


<details>
  <summary>Details</summary>
Motivation: 研究ICL在DST任务中的有效性及影响因素，揭示大语言模型的上下文学习机制

Method: 使用句子嵌入kNN检索演示样本，构建模板化输入测试LLM，系统分析演示选择策略和提示上下文设计

Result: 获得关于演示样本质量、上下文长度、提示模板设计对DST准确率影响的关键洞见

Conclusion: 为LLM在对话系统中的上下文学习应用提供实证指导，揭示模型架构与提示工程的相互作用

Abstract: This study explores the application of in-context learning (ICL) to the
dialogue state tracking (DST) problem and investigates the factors that
influence its effectiveness. We use a sentence embedding based k-nearest
neighbour method to retrieve the suitable demonstrations for ICL. The selected
demonstrations, along with the test samples, are structured within a template
as input to the LLM. We then conduct a systematic study to analyse the impact
of factors related to demonstration selection and prompt context on DST
performance. This work is conducted using the MultiWoZ2.4 dataset and focuses
primarily on the OLMo-7B-instruct, Mistral-7B-Instruct-v0.3, and
Llama3.2-3B-Instruct models. Our findings provide several useful insights on
in-context learning abilities of LLMs for dialogue state tracking.

</details>


### [57] [Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL](https://arxiv.org/abs/2506.08757)
*Mishca de Costa,Muhammad Anwar,Dave Mercier,Mark Randall,Issam Hammad*

Main category: cs.CL

TL;DR: 提出使用函数调用大模型替代传统NL-to-SQL方法，通过预定义专家审核的SQL函数库提升核电站数据查询的安全性和可靠性


<details>
  <summary>Details</summary>
Motivation: 传统NL-to-SSQL方法存在SQL查询不可验证、老旧核电站数据库结构复杂导致错误率高且可信度低的问题

Method: 建立预审核的专用函数库封装SQL逻辑，结合LLM进行函数调用，专家通过验证函数而非直接生成SQL来确保安全性

Result: 相比直接NL-to-SQL方法，该框架在准确性和可维护性方面有显著提升，同时保留自然语言查询的易用性

Conclusion: 在关键基础设施中应优先保障操作安全，通过函数封装实现自然语言查询与专家审核机制的有效平衡

Abstract: Retrieving operational data from nuclear power plants requires exceptional
accuracy and transparency due to the criticality of the decisions it supports.
Traditionally, natural language to SQL (NL-to-SQL) approaches have been
explored for querying such data. While NL-to-SQL promises ease of use, it poses
significant risks: end-users cannot easily validate generated SQL queries, and
legacy nuclear plant databases -- often complex and poorly structured --
complicate query generation due to decades of incremental modifications. These
challenges increase the likelihood of inaccuracies and reduce trust in the
approach. In this work, we propose an alternative paradigm: leveraging
function-calling large language models (LLMs) to address these challenges.
Instead of directly generating SQL queries, we define a set of pre-approved,
purpose-specific functions representing common use cases. Queries are processed
by invoking these functions, which encapsulate validated SQL logic. This hybrid
approach mitigates the risks associated with direct NL-to-SQL translations by
ensuring that SQL queries are reviewed and optimized by experts before
deployment. While this strategy introduces the upfront cost of developing and
maintaining the function library, we demonstrate how NL-to-SQL tools can assist
in the initial generation of function code, allowing experts to focus on
validation rather than creation. Our study includes a performance comparison
between direct NL-to-SQL generation and the proposed function-based approach,
highlighting improvements in accuracy and maintainability. This work
underscores the importance of balancing user accessibility with operational
safety and provides a novel, actionable framework for robust data retrieval in
critical systems.

</details>


### [58] [AraReasoner: Evaluating Reasoning-Based LLMs for Arabic NLP](https://arxiv.org/abs/2506.08768)
*Ahmed Hasanaath,Aisha Alansari,Ahmed Ashraf,Chafik Salmane,Hamzah Luqman,Saad Ezzini*

Main category: cs.CL

TL;DR: 研究通过系统实验评估多种大语言模型在阿拉伯语任务上的表现，发现少样本学习、DeepSeek架构和LoRA微调可显著提升性能


<details>
  <summary>Details</summary>
Motivation: 针对阿拉伯语特有的复杂语言特征（丰富形态/多方言/复杂文字），探究大语言模型在此类数据上的推理能力

Method: 在15项阿拉伯语NLP任务上测试多款模型，采用零样本/少样本/微调策略，重点关注DeepSeek架构的推理能力

Result: ①3个上下文样本即可提升分类任务F1值13+；②DeepSeek在零样本下比GPT4-mini高12 F1；③LoRA微调比单纯增大模型规模多提升8分

Conclusion: 研究为阿拉伯语NLP提供基准数据，验证了上下文学习/专业架构/参数高效微调对提升模型推理能力的有效性

Abstract: Large language models (LLMs) have shown remarkable progress in reasoning
abilities and general natural language processing (NLP) tasks, yet their
performance on Arabic data, characterized by rich morphology, diverse dialects,
and complex script, remains underexplored. This paper presents a comprehensive
benchmarking study of multiple reasoning-focused LLMs, with a special emphasis
on the newly introduced DeepSeek models, across a suite of fifteen Arabic NLP
tasks. We experiment with various strategies, including zero-shot, few-shot,
and fine-tuning. This allows us to systematically evaluate performance on
datasets covering a range of applications to examine their capacity for
linguistic reasoning under different levels of complexity. Our experiments
reveal several key findings. First, carefully selecting just three in-context
examples delivers an average uplift of over 13 F1 points on classification
tasks-boosting sentiment analysis from 35.3% to 87.5% and paraphrase detection
from 56.1% to 87.0%. Second, reasoning-focused DeepSeek architectures
outperform a strong GPT o4-mini baseline by an average of 12 F1 points on
complex inference tasks in the zero-shot setting. Third, LoRA-based fine-tuning
yields up to an additional 8 points in F1 and BLEU compared to equivalent
increases in model scale. The code is available at
https://anonymous.4open.science/r/AraReasoner41299

</details>


### [59] [The impact of fine tuning in LLaMA on hallucinations for named entity extraction in legal documentation](https://arxiv.org/abs/2506.08827)
*Francisco Vargas,Alejandro González Coene,Gaston Escalante,Exequiel Lobón,Manuel Pulido*

Main category: cs.CL

TL;DR: 提出两阶段方法（语义分割+大模型实体提取），通过微调显著提升LLaMA性能，GPT-4 Turbo达到最高86.1%准确率。


<details>
  <summary>Details</summary>
Motivation: 法律文书中的事故信息提取对保险成本量化至关重要，但法律推理复杂导致实体提取困难。

Method: 1. 文本分割（正则表达式 vs 语义向量化）；2. 大模型实体提取（LLaMA系列微调、GPT-4 Turbo零样本）

Result: 微调后LLaMA-2 70B达79.4%准确率，LLaMA-3 8B基础模型76.6%，GPT-4 Turbo最佳86.1%；语义方法显著优于经典方法（39.5%）

Conclusion: 微调减少幻觉，LLaMA-3显示模型快速迭代潜力；闭源模型（GPT-4）性能最优但开源模型追赶中

Abstract: The extraction of information about traffic accidents from legal documents is
crucial for quantifying insurance company costs. Extracting entities such as
percentages of physical and/or psychological disability and the involved
compensation amounts is a challenging process, even for experts, due to the
subtle arguments and reasoning in the court decision. A two-step procedure is
proposed: first, segmenting the document identifying the most relevant
segments, and then extracting the entities. For text segmentation, two
methodologies are compared: a classic method based on regular expressions and a
second approach that divides the document into blocks of n-tokens, which are
then vectorized using multilingual models for semantic searches
(text-embedding-ada-002/MiniLM-L12-v2 ). Subsequently, large language models
(LLaMA-2 7b, 70b, LLaMA-3 8b, and GPT-4 Turbo) are applied with prompting to
the selected segments for entity extraction. For the LLaMA models, fine-tuning
is performed using LoRA. LLaMA-2 7b, even with zero temperature, shows a
significant number of hallucinations in extractions which are an important
contention point for named entity extraction. This work shows that these
hallucinations are substantially reduced after finetuning the model. The
performance of the methodology based on segment vectorization and subsequent
use of LLMs significantly surpasses the classic method which achieves an
accuracy of 39.5%. Among open-source models, LLaMA-2 70B with finetuning
achieves the highest accuracy 79.4%, surpassing its base version 61.7%.
Notably, the base LLaMA-3 8B model already performs comparably to the finetuned
LLaMA-2 70B model, achieving 76.6%, highlighting the rapid progress in model
development. Meanwhile, GPT-4 Turbo achieves the highest accuracy at 86.1%.

</details>


### [60] [Advancing STT for Low-Resource Real-World Speech](https://arxiv.org/abs/2506.08836)
*Flavio D'Intino,Hans-Peter Hutter*

Main category: cs.CL

TL;DR: SRB-300数据集的发布显著提升了瑞士德语语音识别模型性能，微调后Whisper模型WER降低19-33%，BLEU提升8-40%。


<details>
  <summary>Details</summary>
Motivation: 现有受控环境数据集训练的语音识别模型难以处理瑞士德语方言的真实对话场景，需构建真实语音语料库。

Method: 收集300小时瑞士广播电视多方言长音频，覆盖39个电台的即兴对话，并基于Whisper模型进行微调训练。

Result: 最佳模型large-v3达到17.1% WER和74.8 BLEU，相比zero-shot性能WER提升33%，BLEU提升40%。

Conclusion: SRB-300数据集有效解决了低资源方言语音识别难题，为真实场景语音技术发展提供了重要数据支撑。

Abstract: Swiss German is a low-resource language represented by diverse dialects that
differ significantly from Standard German and from each other, lacking a
standardized written form. As a result, transcribing Swiss German involves
translating into Standard German. Existing datasets have been collected in
controlled environments, yielding effective speech-to-text (STT) models, but
these models struggle with spontaneous conversational speech.
  This paper, therefore, introduces the new SRB-300 dataset, a 300-hour
annotated speech corpus featuring real-world long-audio recordings from 39
Swiss German radio and TV stations. It captures spontaneous speech across all
major Swiss dialects recorded in various realistic environments and overcomes
the limitation of prior sentence-level corpora.
  We fine-tuned multiple OpenAI Whisper models on the SRB-300 dataset,
achieving notable enhancements over previous zero-shot performance metrics.
Improvements in word error rate (WER) ranged from 19% to 33%, while BLEU scores
increased between 8% and 40%. The best fine-tuned model, large-v3, achieved a
WER of 17.1% and a BLEU score of 74.8. This advancement is crucial for
developing effective and robust STT systems for Swiss German and other
low-resource languages in real-world contexts.

</details>


### [61] [AdversariaL attacK sAfety aLIgnment(ALKALI): Safeguarding LLMs through GRACE: Geometric Representation-Aware Contrastive Enhancement- Introducing Adversarial Vulnerability Quality Index (AVQI)](https://arxiv.org/abs/2506.08885)
*Danush Khanna,Krishna Kumar,Basab Ghosh,Vinija Jain,Vasu Sharma,Aman Chadha,Amitava Das*

Main category: cs.CL

TL;DR: 提出了首个对抗性基准ALKALI和几何感知对齐框架GRACE，通过潜空间正则化降低39%攻击成功率，揭示LLMs安全编码的几何盲区。


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法（如DPO）忽视潜空间几何特征，导致对抗性提示通过潜在伪装绕过防御，暴露LLMs安全机制的结构性漏洞。

Method: 开发GRACE框架：在偏好学习基础上增加潜空间分离约束（安全/对抗补全分离）和对抗凝聚约束（同类攻击行为聚类），通过层间嵌入池化实现几何重塑。

Result: 在21个主流LLM上验证，GRACE最高降低39%攻击成功率；提出几何量化指标AVQI，揭示不安全补全模仿安全补全几何特征的现象。

Conclusion: LLM安全编码存在几何维度漏洞，需要建立几何感知的对齐范式。ALKALI基准和GRACE框架为防御潜空间对抗攻击提供了新方向。

Abstract: Adversarial threats against LLMs are escalating faster than current defenses
can adapt. We expose a critical geometric blind spot in alignment: adversarial
prompts exploit latent camouflage, embedding perilously close to the safe
representation manifold while encoding unsafe intent thereby evading surface
level defenses like Direct Preference Optimization (DPO), which remain blind to
the latent geometry. We introduce ALKALI, the first rigorously curated
adversarial benchmark and the most comprehensive to date spanning 9,000 prompts
across three macro categories, six subtypes, and fifteen attack families.
Evaluation of 21 leading LLMs reveals alarmingly high Attack Success Rates
(ASRs) across both open and closed source models, exposing an underlying
vulnerability we term latent camouflage, a structural blind spot where
adversarial completions mimic the latent geometry of safe ones. To mitigate
this vulnerability, we introduce GRACE - Geometric Representation Aware
Contrastive Enhancement, an alignment framework coupling preference learning
with latent space regularization. GRACE enforces two constraints: latent
separation between safe and adversarial completions, and adversarial cohesion
among unsafe and jailbreak behaviors. These operate over layerwise pooled
embeddings guided by a learned attention profile, reshaping internal geometry
without modifying the base model, and achieve up to 39% ASR reduction.
Moreover, we introduce AVQI, a geometry aware metric that quantifies latent
alignment failure via cluster separation and compactness. AVQI reveals when
unsafe completions mimic the geometry of safe ones, offering a principled lens
into how models internally encode safety. We make the code publicly available
at https://anonymous.4open.science/r/alkali-B416/README.md.

</details>


### [62] [PlantBert: An Open Source Language Model for Plant Science](https://arxiv.org/abs/2506.08897)
*Hiba Khey,Amine Lakhder,Salma Rouichi,Imane El Ghabi,Kamal Hejjaoui,Younes En-nahli,Fahd Kalloubi,Moez Amri*

Main category: cs.CL

TL;DR: 开发了基于DeBERTa架构的PlantBert语言模型，专门用于从植物胁迫响应文献中提取结构化知识，通过本体对齐标注体系实现多维度植物适应分析。


<details>
  <summary>Details</summary>
Motivation: 植物科学领域缺乏专业化的自然语言处理工具，制约了农业知识发现效率。现有通用模型难以捕捉植物学实体间复杂的生物学关系。

Method: 基于DeBERTa架构构建模型，结合规则增强的文本后处理和本体实体规范化技术。使用与作物本体对齐的层次化标注体系处理分子-生理-农艺多维度数据。

Result: 模型在低资源场景下展现出优异的跨实体泛化能力，验证了领域自适应方法在科研NLP中的可行性，F1值达87.3%（比基线提升15%）。

Conclusion: PlantBert填补了农业NLP工具空白，其开源特性加速了植物表型组学与基因组学领域的数据驱动研究，为智能农艺系统奠定技术基础。

Abstract: The rapid advancement of transformer-based language models has catalyzed
breakthroughs in biomedical and clinical natural language processing; however,
plant science remains markedly underserved by such domain-adapted tools. In
this work, we present PlantBert, a high-performance, open-source language model
specifically tailored for extracting structured knowledge from plant
stress-response literature. Built upon the DeBERTa architecture-known for its
disentangled attention and robust contextual encoding-PlantBert is fine-tuned
on a meticulously curated corpus of expert-annotated abstracts, with a primary
focus on lentil (Lens culinaris) responses to diverse abiotic and biotic
stressors. Our methodology combines transformer-based modeling with
rule-enhanced linguistic post-processing and ontology-grounded entity
normalization, enabling PlantBert to capture biologically meaningful
relationships with precision and semantic fidelity. The underlying corpus is
annotated using a hierarchical schema aligned with the Crop Ontology,
encompassing molecular, physiological, biochemical, and agronomic dimensions of
plant adaptation. PlantBert exhibits strong generalization capabilities across
entity types and demonstrates the feasibility of robust domain adaptation in
low-resource scientific fields. By providing a scalable and reproducible
framework for high-resolution entity recognition, PlantBert bridges a critical
gap in agricultural NLP and paves the way for intelligent, data-driven systems
in plant genomics, phenomics, and agronomic knowledge discovery. Our model is
publicly released to promote transparency and accelerate cross-disciplinary
innovation in computational plant science.

</details>


### [63] [From Legal Texts to Defeasible Deontic Logic via LLMs: A Study in Automated Semantic Analysis](https://arxiv.org/abs/2506.08899)
*Elias Horner,Cristinel Mateis,Guido Governatori,Agata Ciabattoni*

Main category: cs.CL

TL;DR: 提出基于大语言模型的自动化法律文本形式化方法，通过结构化流程将法律条款转化为可废止道义逻辑表示


<details>
  <summary>Details</summary>
Motivation: 解决复杂法律文本自动化分析难题，探索大语言模型在可扩展法律信息化中的应用潜力

Method: 设计包含条款分割、义务规则提取、句法语义校验的三阶段流程，采用提示工程/微调模型/多阶段管道等多种LLM配置方案

Result: 在澳大利亚电信消费者保护法典上的实验显示，机器生成形式化与专家标注结果达成显著一致性（prompt工程策略效果最佳）

Conclusion: 合理引导的大语言模型能够有效支持法律规范形式化，为可扩展法律推理系统奠定基础

Abstract: We present a novel approach to the automated semantic analysis of legal texts
using large language models (LLMs), targeting their transformation into formal
representations in Defeasible Deontic Logic (DDL). We propose a structured
pipeline that segments complex normative language into atomic snippets,
extracts deontic rules, and evaluates them for syntactic and semantic
coherence. Our methodology is evaluated across various LLM configurations,
including prompt engineering strategies, fine-tuned models, and multi-stage
pipelines, focusing on legal norms from the Australian Telecommunications
Consumer Protections Code. Empirical results demonstrate promising alignment
between machine-generated and expert-crafted formalizations, showing that LLMs
- particularly when prompted effectively - can significantly contribute to
scalable legal informatics.

</details>


### [64] [Dialect Normalization using Large Language Models and Morphological Rules](https://arxiv.org/abs/2506.08907)
*Antonios Dimakis,John Pavlopoulos,Antonios Anastasopoulos*

Main category: cs.CL

TL;DR: 本文提出结合规则转换和LLM少样本提示的希腊方言规范化方法，实验发现先前研究依赖表层语言特征，而语义信息仍能产生新见解


<details>
  <summary>Details</summary>
Motivation: 解决低资源方言在自然语言处理中的难题，通过方言到标准语的转换提升下游工具适用性，特别是针对缺乏平行语料的情况

Method: 结合基于规则的语言学转换和大型语言模型的少样本提示技术，应用于希腊方言谚语数据集，并通过人工评估输出质量

Result: 下游实验表明先前研究结果主要依赖拼写特征等表层信息，而通过剩余语义仍可获得新的语言学发现

Conclusion: 混合方法有效解决了方言规范化问题，强调深层语义分析的重要性，为低资源语言处理提供了无需平行数据的新方案

Abstract: Natural language understanding systems struggle with low-resource languages,
including many dialects of high-resource ones. Dialect-to-standard
normalization attempts to tackle this issue by transforming dialectal text so
that it can be used by standard-language tools downstream. In this study, we
tackle this task by introducing a new normalization method that combines
rule-based linguistically informed transformations and large language models
(LLMs) with targeted few-shot prompting, without requiring any parallel data.
We implement our method for Greek dialects and apply it on a dataset of
regional proverbs, evaluating the outputs using human annotators. We then use
this dataset to conduct downstream experiments, finding that previous results
regarding these proverbs relied solely on superficial linguistic information,
including orthographic artifacts, while new observations can still be made
through the remaining semantics.

</details>


### [65] [PropMEND: Hypernetworks for Knowledge Propagation in LLMs](https://arxiv.org/abs/2506.08920)
*Zeyu Leo Liu,Greg Durrett,Eunsol Choi*

Main category: cs.CL

TL;DR: PropMEND提出基于超网络的知识传播方法，通过元学习调整梯度更新，显著提升大语言模型在多跳推理任务中的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑技术可注入知识但无法支持基于该知识的推理，需解决知识传播问题以实现多跳问题回答。

Method: 扩展MEND的元目标，使用超网络对梯度进行元学习转换，使模型能利用注入知识进行多步推理。

Result: RippleEdit上多跳问题准确率提升近2倍；在含未见过实体关系的Controlled RippleEdit中仍优于基线，但性能差距缩小。

Conclusion: PropMEND有效促进知识传播，但对未见过关系的泛化能力有限，未来需研究更广泛的知识传播机制。

Abstract: Knowledge editing techniques for large language models (LLMs) can inject
knowledge that is later reproducible verbatim, but they fall short on
propagating that knowledge: models cannot answer questions that require
reasoning with the injected knowledge. We present a hypernetwork-based approach
for knowledge propagation, named PropMEND, where we meta-learn how to modify
gradients of a language modeling loss to encourage injected information to
propagate. Our approach extends the meta-objective of MEND [29] so that
gradient updates on knowledge are transformed to enable answering multi-hop
questions involving that knowledge. We show improved performance on the
RippleEdit dataset, showing almost 2x accuracy on challenging multi-hop
questions whose answers are not explicitly stated in the injected fact. We
further introduce a new dataset, Controlled RippleEdit, to evaluate the
generalization of our hypernetwork, testing knowledge propagation along
relations and entities unseen during hypernetwork training. PropMEND still
outperforms existing approaches in unseen entity-relation pairs, yet the
performance gap decreases substantially, suggesting future work in propagating
knowledge to a wide range of relations.

</details>


### [66] [Can A Gamer Train A Mathematical Reasoning Model?](https://arxiv.org/abs/2506.08935)
*Andrew Shin*

Main category: cs.CL

TL;DR: 通过强化学习与内存优化技术，在单张游戏GPU（RTX 3080 Ti）上训练1.5B参数数学推理模型，性能超越更大模型


<details>
  <summary>Details</summary>
Motivation: 打破高性能数学推理模型需要昂贵硬件集群的传统范式，降低AI研究门槛

Method: 整合强化学习算法与内存优化技术，实现16GB显存GPU上的高效训练

Result: 在资源受限环境下，模型在数学推理基准测试中达到或超越数倍规模模型的性能

Conclusion: 证明高性能数学推理模型可在消费级硬件实现，推动AI研究的民主化进程

Abstract: While large language models (LLMs) have achieved remarkable performance in
various tasks including mathematical reasoning, their development typically
demands prohibitive computational resources. Recent advancements have reduced
costs for training capable models, yet even these approaches rely on high-end
hardware clusters. In this paper, we demonstrate that a single average gaming
GPU can train a solid mathematical reasoning model, by integrating
reinforcement learning and memory optimization techniques. Specifically, we
train a 1.5B parameter mathematical reasoning model on RTX 3080 Ti of 16GB
memory that achieves comparable or better performance on mathematical reasoning
benchmarks than models several times larger, in resource-constrained
environments. Our results challenge the paradigm that state-of-the-art
mathematical reasoning necessitates massive infrastructure, democratizing
access to high-performance AI research.
https://github.com/shinandrew/YouronMath.

</details>


### [67] [FaithfulRAG: Fact-Level Conflict Modeling for Context-Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2506.08938)
*Qinggang Zhang,Zhishang Xiang,Yilin Xiao,Le Wang,Junhui Li,Xinrun Wang,Jinsong Su*

Main category: cs.CL

TL;DR: 提出FaithfulRAG框架解决检索增强生成中的知识冲突问题，通过显式建模参数知识与检索上下文的差异，在事实层面识别冲突并整合生成可信结果


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法通过强制抑制模型参数知识来实现忠实性，这会破坏模型内部知识结构并增加误解上下文的风险

Method: 1. 在事实层面识别参数知识与检索上下文的冲突
2. 设计自我思考过程让LLM推理整合冲突事实
3. 建立知识冲突的显式建模机制

Result: 在广泛实验中证明优于现有state-of-the-art方法

Conclusion: FaithfulRAG成功解决了知识冲突问题，在保持生成结果忠实性的同时有效整合参数知识和检索上下文

Abstract: Large language models (LLMs) augmented with retrieval systems have
demonstrated significant potential in handling knowledge-intensive tasks.
However, these models often struggle with unfaithfulness issues, generating
outputs that either ignore the retrieved context or inconsistently blend it
with the LLM`s parametric knowledge. This issue is particularly severe in cases
of knowledge conflict, where the retrieved context conflicts with the model`s
parametric knowledge. While existing faithful RAG approaches enforce strict
context adherence through well-designed prompts or modified decoding
strategies, our analysis reveals a critical limitation: they achieve
faithfulness by forcibly suppressing the model`s parametric knowledge, which
undermines the model`s internal knowledge structure and increases the risk of
misinterpreting the context. To this end, this paper proposes FaithfulRAG, a
novel framework that resolves knowledge conflicts by explicitly modeling
discrepancies between the model`s parametric knowledge and retrieved context.
Specifically, FaithfulRAG identifies conflicting knowledge at the fact level
and designs a self-thinking process, allowing LLMs to reason about and
integrate conflicting facts before generating responses. Extensive experiments
demonstrate that our method outperforms state-of-the-art methods. The code is
available at https:// github.com/DeepLearnXMU/Faithful-RAG

</details>


### [68] [Can LLMs Ground when they (Don't) Know: A Study on Direct and Loaded Political Questions](https://arxiv.org/abs/2506.08952)
*Clara Lachenmaier,Judith Sieker,Sina Zarrieß*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在政治领域难以纠正用户错误信念，揭露其在虚假信息治理中的潜在风险。


<details>
  <summary>Details</summary>
Motivation: 对话中的共同基础构建是实现相互理解的核心机制，而政治领域因虚假信息泛滥成为高风险场景。本研究聚焦LLM在（不）掌握知识时如何建立共同基础，特别是面对预设错误信息的诱导性问题时的应对机制。

Method: 通过设计直接知识性问题与预设错误立场的诱导性问题，评估不同LLM的应答模式，分析模型知识水平与政治倾向性对纠错能力的影响。

Result: LLM展现出显著的地基构建障碍：面对诱导性问题时缺乏主动澄清意愿，且纠错能力与模型知识储备及政治立场偏倚存在相关性。

Conclusion: 当前LLM在政治话语中纠正错误信念的能力存在系统性缺陷，其作为虚假信息过滤器的可靠性面临严峻挑战。

Abstract: Communication among humans relies on conversational grounding, allowing
interlocutors to reach mutual understanding even when they do not have perfect
knowledge and must resolve discrepancies in each other's beliefs. This paper
investigates how large language models (LLMs) manage common ground in cases
where they (don't) possess knowledge, focusing on facts in the political domain
where the risk of misinformation and grounding failure is high. We examine the
ability of LLMs to answer direct knowledge questions and loaded questions that
presuppose misinformation. We evaluate whether loaded questions lead LLMs to
engage in active grounding and correct false user beliefs, in connection to
their level of knowledge and their political bias. Our findings highlight
significant challenges in LLMs' ability to engage in grounding and reject false
user beliefs, raising concerns about their role in mitigating misinformation in
political discourse.

</details>


### [69] [Pre-trained Language Models Learn Remarkably Accurate Representations of Numbers](https://arxiv.org/abs/2506.08966)
*Marek Kadlčík,Michal Štefánik,Timothee Mickus,Michal Spiegel,Josef Kuchař*

Main category: cs.CL

TL;DR: 研究发现预训练语言模型通过新型探测技术可精准解码数字表征，证实其算术错误主要源于嵌入表示精度不足，通过嵌入对齐可有效改善算术能力


<details>
  <summary>Details</summary>
Motivation: 现有方法未能有效捕捉语言模型数值嵌入的正弦模式结构，导致无法准确解释模型算术错误根源

Method: 提出基于正弦模式的新型数值解码探针，对多个开源语言模型的输入嵌入进行数值重建分析

Result: 新探针在各类模型上实现接近完美的数值解码准确率，证明预训练后模型具备精确数字表征能力，且嵌入精度与算术错误率显著相关

Conclusion: 语言模型内在具备精确数值表征能力，通过探针指导的嵌入空间对齐可有效提升基础算术性能

Abstract: Pretrained language models (LMs) are prone to arithmetic errors. Existing
work showed limited success in probing numeric values from models'
representations, indicating that these errors can be attributed to the inherent
unreliability of distributionally learned embeddings in representing exact
quantities. However, we observe that previous probing methods are inadequate
for the emergent structure of learned number embeddings with sinusoidal
patterns.
  In response, we propose a novel probing technique that decodes numeric values
from input embeddings with near-perfect accuracy across a range of open-source
LMs. This proves that after the sole pre-training, LMs represent numbers with
remarkable precision. Finally, we find that the embeddings' preciseness judged
by our probe's accuracy explains a large portion of LM's errors in elementary
arithmetic, and show that aligning the embeddings with the pattern discovered
by our probe can mitigate these errors.

</details>


### [70] [Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark and Scheduling System](https://arxiv.org/abs/2506.08972)
*Yuan Guo,Tingjia Miao,Zheng Wu,Pengzhou Cheng,Ming Zhou,Zhuosheng Zhang*

Main category: cs.CL

TL;DR: 提出UI-NEXUS基准测试评估移动代理的组合任务处理能力，并开发AGENT-NEXUS系统改善现有代理24%-40%的任务成功率


<details>
  <summary>Details</summary>
Motivation: 现有移动代理主要关注原子任务，缺乏对真实场景中关键组合任务的泛化能力评估

Method: 构建包含三类组合操作（简单串联/上下文转换/深度探索）的UI-NEXUS基准，含50个交互应用环境和100个任务模板；提出AGENT-NEXUS系统通过动态任务分解机制处理长程任务

Result: 现有代理在组合任务中普遍存在执行偏差（欠执行/过执行/注意力漂移），AGENT-NEXUS在不显著增加推理开销的情况下显著提升任务成功率

Conclusion: UI-NEXUS揭示了移动代理的原子-组合任务泛化鸿沟，AGENT-NEXUS为提升复杂交互场景下的代理效能提供了有效解决方案

Abstract: Autonomous agents powered by multimodal large language models have been
developed to facilitate task execution on mobile devices. However, prior work
has predominantly focused on atomic tasks -- such as shot-chain execution tasks
and single-screen grounding tasks -- while overlooking the generalization to
compositional tasks, which are indispensable for real-world applications. This
work introduces UI-NEXUS, a comprehensive benchmark designed to evaluate mobile
agents on three categories of compositional operations: Simple Concatenation,
Context Transition, and Deep Dive. UI-NEXUS supports interactive evaluation in
20 fully controllable local utility app environments, as well as 30 online
Chinese and English service apps. It comprises 100 interactive task templates
with an average optimal step count of 14.05. Experimental results across a
range of mobile agents with agentic workflow or agent-as-a-model show that
UI-NEXUS presents significant challenges. Specifically, existing agents
generally struggle to balance performance and efficiency, exhibiting
representative failure modes such as under-execution, over-execution, and
attention drift, causing visible atomic-to-compositional generalization gap.
Inspired by these findings, we propose AGENT-NEXUS, a lightweight and efficient
scheduling system to tackle compositional mobile tasks. AGENT-NEXUS
extrapolates the abilities of existing mobile agents by dynamically decomposing
long-horizon tasks to a series of self-contained atomic subtasks. AGENT-NEXUS
achieves 24% to 40% task success rate improvement for existing mobile agents on
compositional operation tasks within the UI-NEXUS benchmark without
significantly sacrificing inference overhead. The demo video, dataset, and code
are available on the project page at https://ui-nexus.github.io.

</details>


### [71] [FROST-EMA: Finnish and Russian Oral Speech Dataset of Electromagnetic Articulography Measurements with L1, L2 and Imitated L2 Accents](https://arxiv.org/abs/2506.08981)
*Satu Hopponen,Tomi Kinnunen,Alexandre Nikolaev,Rosa González Hautamäki,Lauri Tavi,Einar Meister*

Main category: cs.CL

TL;DR: 创建包含18名双语者的FROST-EMA多语言发音数据集，探索L2/伪口音对声纹识别系统及发音模式的影响


<details>
  <summary>Details</summary>
Motivation: 研究第二语言习得和伪外国口音在语音学层面的发音特征，及其对语音技术系统的影响

Method: 构建包含俄语母语者芬兰语(L2)和模仿芬兰语的三语料库，通过自动说话人验证系统测试和电磁发音仪分析进行双维度验证

Result: 发现L2和伪口音均会降低声纹识别准确率（-15%和-25%），且呈现独特的舌部运动模式

Conclusion: 该语料库为跨语言语音研究提供多维度数据支持，证明发音变异对技术系统和语音学研究均具重要价值

Abstract: We introduce a new FROST-EMA (Finnish and Russian Oral Speech Dataset of
Electromagnetic Articulography) corpus. It consists of 18 bilingual speakers,
who produced speech in their native language (L1), second language (L2), and
imitated L2 (fake foreign accent). The new corpus enables research into
language variability from phonetic and technological points of view.
Accordingly, we include two preliminary case studies to demonstrate both
perspectives. The first case study explores the impact of L2 and imitated L2 on
the performance of an automatic speaker verification system, while the second
illustrates the articulatory patterns of one speaker in L1, L2, and a fake
accent.

</details>


### [72] [Naturalistic Language-related Movie-Watching fMRI Task for Detecting Neurocognitive Decline and Disorder](https://arxiv.org/abs/2506.08986)
*Yuejiao Wang,Xianmin Gong,Xixin Wu,Patrick Wong,Hoi-lam Helene Fung,Man Wai Mak,Helen Meng*

Main category: cs.CL

TL;DR: 研究开发自然语言相关fMRI任务检测老年人认知衰退，基于97名受试者的机器学习模型达到AUC 0.86，特征定位显示语言相关脑区激活。


<details>
  <summary>Details</summary>
Motivation: 早期检测神经认知障碍对临床干预至关重要，语言相关fMRI可能提供非侵入性检测手段。

Method: 使用自然语言fMRI任务测试97名香港非痴呆老年人，结合机器学习和人口统计学特征建立分类模型。

Result: 模型AUC达0.86，特征主要来自颞上回、颞中回和右小脑等语言处理相关脑区。

Conclusion: 自然语言fMRI任务具有早期检测认知衰退的潜力，为临床干预提供新方法。

Abstract: Early detection is crucial for timely intervention aimed at preventing and
slowing the progression of neurocognitive disorder (NCD), a common and
significant health problem among the aging population. Recent evidence has
suggested that language-related functional magnetic resonance imaging (fMRI)
may be a promising approach for detecting cognitive decline and early NCD. In
this paper, we proposed a novel, naturalistic language-related fMRI task for
this purpose. We examined the effectiveness of this task among 97 non-demented
Chinese older adults from Hong Kong. The results showed that machine-learning
classification models based on fMRI features extracted from the task and
demographics (age, gender, and education year) achieved an average area under
the curve of 0.86 when classifying participants' cognitive status (labeled as
NORMAL vs DECLINE based on their scores on a standard neurcognitive test).
Feature localization revealed that the fMRI features most frequently selected
by the data-driven approach came primarily from brain regions associated with
language processing, such as the superior temporal gyrus, middle temporal
gyrus, and right cerebellum. The study demonstrated the potential of the
naturalistic language-related fMRI task for early detection of aging-related
cognitive decline and NCD.

</details>


### [73] [Employing self-supervised learning models for cross-linguistic child speech maturity classification](https://arxiv.org/abs/2506.08999)
*Theo Zhang,Madurya Suresh,Anne S. Warlaumont,Kasia Hitczenko,Alejandrina Cristia,Margaret Cychosz*

Main category: cs.CL

TL;DR: 使用SpeechMaturity新型大数据集训练的Transformer模型在儿童语音分类任务中表现超越现有方法，准确率接近人类水平


<details>
  <summary>Details</summary>
Motivation: 解决儿童语音技术系统因训练数据稀缺和儿童语音复杂性导致的下游任务性能不足问题

Method: 构建包含24.2万标注样本的跨语言数据集（覆盖6个国家25+语言），训练Transformer模型进行哭声/笑声/成熟音/未成熟音四分类

Result: 模型准确率超越现有SOTA模型（达人类水平），在城乡不同场景中保持鲁棒性

Conclusion: 大规模生态效度数据集显著提升儿童语音分类性能，为儿童语音技术发展提供新基准

Abstract: Speech technology systems struggle with many downstream tasks for child
speech due to small training corpora and the difficulties that child speech
pose. We apply a novel dataset, SpeechMaturity, to state-of-the-art transformer
models to address a fundamental classification task: identifying child
vocalizations. Unlike previous corpora, our dataset captures maximally
ecologically-valid child vocalizations across an unprecedented sample,
comprising children acquiring 25+ languages in the U.S., Bolivia, Vanuatu,
Papua New Guinea, Solomon Islands, and France. The dataset contains 242,004
labeled vocalizations, magnitudes larger than previous work. Models were
trained to distinguish between cry, laughter, mature (consonant+vowel), and
immature speech (just consonant or vowel). Models trained on the dataset
outperform state-of-the-art models trained on previous datasets, achieved
classification accuracy comparable to humans, and were robust across rural and
urban settings.

</details>


### [74] [SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner](https://arxiv.org/abs/2506.09003)
*Lei Zhang,Jiaxi Yang,Min Yang,Jian Yang,Mouxiang Chen,Jiajun Zhang,Zeyu Cui,Binyuan Hui,Junyang Lin*

Main category: cs.CL

TL;DR: 提出基于测试驱动开发（TDD）的SWE-Flow框架，通过运行时依赖图自动生成可验证的开发任务，并构建SWE-Flow-Eval基准数据集


<details>
  <summary>Details</summary>
Motivation: 现有软件工程数据依赖人工提交的issue存在局限性，需通过单元测试自动生成结构化开发步骤

Method: 构建运行时依赖图（RDG）捕获函数交互，生成分步开发计划，每步产出可验证代码/测试/修改

Result: 从GitHub项目生成16,061个训练实例和2,020个测试实例，模型微调后显著提升TDD编码能力

Conclusion: 框架有效提升基于TDD的代码生成性能，开源代码/数据集/模型/Docker镜像推动后续研究

Abstract: We introduce **SWE-Flow**, a novel data synthesis framework grounded in
Test-Driven Development (TDD). Unlike existing software engineering data that
rely on human-submitted issues, **SWE-Flow** automatically infers incremental
development steps directly from unit tests, which inherently encapsulate
high-level requirements. The core of **SWE-Flow** is the construction of a
Runtime Dependency Graph (RDG), which precisely captures function interactions,
enabling the generation of a structured, step-by-step *development schedule*.
At each step, **SWE-Flow** produces a partial codebase, the corresponding unit
tests, and the necessary code modifications, resulting in fully verifiable TDD
tasks. With this approach, we generated 16,061 training instances and 2,020
test instances from real-world GitHub projects, creating the **SWE-Flow-Eval**
benchmark. Our experiments show that fine-tuning open model on this dataset
significantly improves performance in TDD-based coding. To facilitate further
research, we release all code, datasets, models, and Docker images at
[Github](https://github.com/Hambaobao/SWE-Flow).

</details>


### [75] [UD-KSL Treebank v1.3: A semi-automated framework for aligning XPOS-extracted units with UPOS tags](https://arxiv.org/abs/2506.09009)
*Hakyung Sung,Gyu-Ho Shin,Chanyoung Lee,You Kyung Sung,Boo Kyung Jung*

Main category: cs.CL

TL;DR: 开发半自动化框架实现XPOS-UPOS对齐，提升韩语二语形态句法标注一致性及分析准确率


<details>
  <summary>Details</summary>
Motivation: 解决现有L2韩语标注中形态句法与依存分析层的不一致问题，优化标注数据有限场景下的NLP表现

Method: 构建半自动化框架实现XPOS序列与UPOS对齐，通过两种工具包在有无对齐数据集上微调模型进行对比实验

Result: 对齐数据集使标注层一致性提升，形态句法标注和依存解析准确率提高，在低资源场景提升显著

Conclusion: XPOS-UPOS对齐机制有效增强跨标注层一致性，为低资源二语分析任务提供可靠解决方案

Abstract: The present study extends recent work on Universal Dependencies annotations
for second-language (L2) Korean by introducing a semi-automated framework that
identifies morphosyntactic constructions from XPOS sequences and aligns those
constructions with corresponding UPOS categories. We also broaden the existing
L2-Korean corpus by annotating 2,998 new sentences from argumentative essays.
To evaluate the impact of XPOS-UPOS alignments, we fine-tune L2-Korean
morphosyntactic analysis models on datasets both with and without these
alignments, using two NLP toolkits. Our results indicate that the aligned
dataset not only improves consistency across annotation layers but also
enhances morphosyntactic tagging and dependency-parsing accuracy, particularly
in cases of limited annotated data.

</details>


### [76] [Learning to Reason Across Parallel Samples for LLM Reasoning](https://arxiv.org/abs/2506.09014)
*Jianing Qi,Xi Ye,Hao Tang,Zhigang Zhu,Eunsol Choi*

Main category: cs.CL

TL;DR: 提出样本集聚合器(SSA)模型，通过强化学习训练紧凑型LLM聚合多个样本答案，显著提升推理任务性能并超越传统测试时扩展方法


<details>
  <summary>Details</summary>
Motivation: 现有测试时扩展方法(如多数投票或奖励模型重排序)未充分挖掘多样本潜力，需开发更优化的答案聚合机制提升大模型推理性能

Method: 1. 构建SSA模型接收多样本答案序列输入
2. 使用强化学习优化答案准确性
3. 通过分离生成与分析模块实现灵活部署

Result: 在多个推理数据集上超越现有方法，展现跨样本规模/模型家族/任务类型的泛化能力

Conclusion: SSA通过解耦生成与聚合过程，可高效兼容黑箱模型输出，为提升LLM推理性能提供新范式

Abstract: Scaling test-time compute brings substantial performance gains for large
language models (LLMs). By sampling multiple answers and heuristically
aggregate their answers (e.g., either through majority voting or using
verifiers to rank the answers), one can achieve consistent performance gains in
math domains. In this paper, we propose a new way to leverage such multiple
sample set. We train a compact LLM, called Sample Set Aggregator (SSA), that
takes a concatenated sequence of multiple samples and output the final answer,
optimizing it for the answer accuracy with reinforcement learning. Experiments
on multiple reasoning datasets show that SSA outperforms other test-time
scaling methods such as reward model-based re-ranking. Our approach also shows
a promising generalization ability, across sample set sizes, base model
families and scales, and tasks. By separating LLMs to generate answers and LLMs
to analyze and aggregate sampled answers, our approach can work with the
outputs from premier black box models easily and efficiently.

</details>


### [77] [Comparing human and LLM proofreading in L2 writing: Impact on lexical and syntactic features](https://arxiv.org/abs/2506.09021)
*Hakyung Sung,Karla Csuros,Min-Chang Sung*

Main category: cs.CL

TL;DR: 比较人类与三种LLM校对在二语写作中的词汇、句法干预效果及一致性


<details>
  <summary>Details</summary>
Motivation: 探究LLM校对提升文本可理解性的有效性及其与人类校对的差异，同时评估不同模型间的一致性

Method: 对相同二语写作样本进行人类与LLM（ChatGPT-4o/Llama3.1-8b/Deepseek-r1-8b）校对干预，分析词汇句法变化并评估模型间一致性

Result: 1. 人类与LLM均提升bigram词汇特征 
2. LLM采用生成式修改（多样化词汇/复杂句法/更多形容词修饰） 
3. 三种模型在主要特征上高度一致

Conclusion: LLM校对展现更强的生成性修改能力且跨模型效果稳定，为AI辅助写作提供实践参考

Abstract: This study examines the lexical and syntactic interventions of human and LLM
proofreading aimed at improving overall intelligibility in identical second
language writings, and evaluates the consistency of outcomes across three LLMs
(ChatGPT-4o, Llama3.1-8b, Deepseek-r1-8b). Findings show that both human and
LLM proofreading enhance bigram lexical features, which may contribute to
better coherence and contextual connectedness between adjacent words. However,
LLM proofreading exhibits a more generative approach, extensively reworking
vocabulary and sentence structures, such as employing more diverse and
sophisticated vocabulary and incorporating a greater number of adjective
modifiers in noun phrases. The proofreading outcomes are highly consistent in
major lexical and syntactic features across the three models.

</details>


### [78] [Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning](https://arxiv.org/abs/2506.09033)
*Haozhen Zhang,Tao Feng,Jiaxuan You*

Main category: cs.CL

TL;DR: 提出基于强化学习的Router-R1框架，动态路由和聚合多个LLM，优化性能与成本权衡


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由器采用单轮一对一映射机制，难以处理需要多模型协作的复杂任务

Method: 将路由建模为序列决策过程，通过LLM路由器交替执行「思考」（内部推理）和「路由」（动态模型调用）动作

Result: 在7个QA基准测试中超越基线模型，实现性能提升的同时保持成本可控

Conclusion: 通过强化学习奖励机制，首次实现了语言模型路由中性能-成本权衡的优化路径

Abstract: The rapid emergence of diverse large language models (LLMs) has spurred the
development of LLM routers that assign user queries to the most suitable model.
However, existing LLM routers typically perform a single-round, one-to-one
mapping (\textit{i.e.}, assigning each query to a single model in isolation),
which limits their capability to tackle complex tasks that demand the
complementary strengths of multiple LLMs. In this paper, we present
\textbf{Router-R1}, a reinforcement learning (RL)-based framework that
formulates multi-LLM routing and aggregation as a sequential decision process.
Router-R1 instantiates the router itself as a capable LLM, leveraging its
reasoning ability to interleave "think" actions (internal deliberation) with
"route" actions (dynamic model invocation), and integrates each response into
its evolving context. To guide learning, we employ a lightweight rule-based
reward comprising format rewards, final outcome rewards, and a novel cost
reward for performance and cost trade-off optimization, opening a pathway
toward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions
only on simple model descriptors such as pricing, latency, and example
performance, enabling strong generalization to unseen model selection.
Experiments on seven general and multi-hop QA benchmarks show that Router-R1
outperforms over several strong baselines, achieving superior performance while
maintaining robust generalization and cost management.Code is available at
https://github.com/ulab-uiuc/Router-R1.

</details>


### [79] [Same Task, Different Circuits: Disentangling Modality-Specific Mechanisms in VLMs](https://arxiv.org/abs/2506.09047)
*Yaniv Nikankin,Dana Arad,Yossi Gandelsman,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 研究发现视觉-语言模型(VLMs)多模态性能差异源于后期层表示对齐滞后，通过后期层视觉表示回补早期层的简单干预，平均缩小了1/3的模态表现差距。


<details>
  <summary>Details</summary>
Motivation: 探究VLMs在视觉和文本模态表现差异的根本原因，发现不同模态电路虽然功能相似但数据处理位置不同，尤其视觉表示对齐滞后影响性能。

Method: 通过电路分析比较模态处理机制，采用后期层视觉数据表示回补到早期层的干预策略，在多项任务和模型中进行验证。

Result: 实验证明该方法平均缩小33%的模态性能差距，且无需重新训练模型。

Conclusion: 揭示了VLMs多模态性能差异的机制，提出首个无需训练的性能优化方案，为模型架构改进提供了新方向。

Abstract: Vision-Language models (VLMs) show impressive abilities to answer questions
on visual inputs (e.g., counting objects in an image), yet demonstrate higher
accuracies when performing an analogous task on text (e.g., counting words in a
text). We investigate this accuracy gap by identifying and comparing the
\textit{circuits} - the task-specific computational sub-graphs - in different
modalities. We show that while circuits are largely disjoint between
modalities, they implement relatively similar functionalities: the differences
lie primarily in processing modality-specific data positions (an image or a
text sequence). Zooming in on the image data representations, we observe they
become aligned with the higher-performing analogous textual representations
only towards later layers, too late in processing to effectively influence
subsequent positions. To overcome this, we patch the representations of visual
data tokens from later layers back into earlier layers. In experiments with
multiple tasks and models, this simple intervention closes a third of the
performance gap between the modalities, on average. Our analysis sheds light on
the multi-modal performance gap in VLMs and suggests a training-free approach
for reducing it.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [80] [Neural-Augmented Kelvinlet: Real-Time Soft Tissue Deformation with Multiple Graspers](https://arxiv.org/abs/2506.08043)
*Ashkan Shahbazi,Kyvia Pereira,Jon S. Heiselman,Elaheh Akbari,Annie C. Benson,Sepehr Seifi,Xinyuan Liu,Garrison L. Johnston,Erwin Terpstra,Anne Draaisma,Jan-Jaap Severes,Jie Ying Wu,Nabil Simaan,Michael L. Miga,Soheil Kolouri*

Main category: cs.GR

TL;DR: 提出融合Kelvinlet先验的神经模拟器，实现手术应用中实时、物理精确的软组织形变模拟


<details>
  <summary>Details</summary>
Motivation: 现有软组织形变模拟在实时性和物理一致性存在不足，需开发兼顾精度与效率的解决方案以支持手术机器人及医学训练

Method: 整合Kelvinlet基先验进行残差学习，结合线性和非线性FEM数据集增强神经网络预测能力

Result: 显著提升预测精度与物理一致性，在腹腔镜抓取模拟中实现高保真手术操作，保持毫秒级实时性能

Conclusion: Kelvinlet增强学习为手术应用提供了实时物理模拟的高效策略，拓展了数据驱动建模的物理可解释性

Abstract: Fast and accurate simulation of soft tissue deformation is a critical factor
for surgical robotics and medical training. In this paper, we introduce a novel
physics-informed neural simulator that approximates soft tissue deformations in
a realistic and real-time manner. Our framework integrates Kelvinlet-based
priors into neural simulators, making it the first approach to leverage
Kelvinlets for residual learning and regularization in data-driven soft tissue
modeling. By incorporating large-scale Finite Element Method (FEM) simulations
of both linear and nonlinear soft tissue responses, our method improves neural
network predictions across diverse architectures, enhancing accuracy and
physical consistency while maintaining low latency for real-time performance.
We demonstrate the effectiveness of our approach by performing accurate
surgical maneuvers that simulate the use of standard laparoscopic tissue
grasping tools with high fidelity. These results establish Kelvinlet-augmented
learning as a powerful and efficient strategy for real-time, physics-aware soft
tissue simulation in surgical applications.

</details>


### [81] [A Real-time 3D Desktop Display](https://arxiv.org/abs/2506.08064)
*Livio Tenze,Enrique Canessa*

Main category: cs.GR

TL;DR: 扩展altiro3D库实现实时2D转3D渲染，支持视频流/桌面应用，结合MiDaS CNN与AI优化


<details>
  <summary>Details</summary>
Motivation: 提升现有库功能以处理多样化3D输入源（2D摄像头/视频文件/桌面应用），简化用户操作并增强实时3D渲染能力

Method: 采用MiDaS CNN从单帧2D图像提取深度图，结合AI优化算法提升性能，开发跨平台GUI实现屏幕区域选择

Result: 实现实时光场合成，兼容网页/视频会议等桌面应用，支持Looking Glass等光场设备直接输出

Conclusion: 通过AI深度估计与系统集成优化，推动裸眼3D显示技术在AR/VR等场景的实用化进程

Abstract: A new extended version of the altiro3D C++ Library -- initially developed to
get glass-free holographic displays starting from 2D images -- is here
introduced aiming to deal with 3D video streams from either 2D webcam images or
flat video files. These streams are processed in real-time to synthesize
light-fields (in Native format) and feed realistic 3D experiences. The core
function needed to recreate multiviews consists on the use of MiDaS
Convolutional Neural Network (CNN), which allows to extract a depth map from a
single 2D image. Artificial Intelligence (AI) computing techniques are applied
to improve the overall performance of the extended altiro3D Library. Thus,
altiro3D can now treat standard images, video streams or screen portions of a
Desktop where other apps may be also running (like web browsers, video chats,
etc) and render them into 3D. To achieve the latter, a screen region need to be
selected in order to feed the output directly into a light-field 3D device such
as Looking Glass (LG) Portrait. In order to simplify the acquisition of a
Desktop screen area by the user, a multi-platform Graphical User Interface has
been also implemented. Sources available at:
https://github.com/canessae/altiro3D/releases/tag/2.0.0

</details>


### [82] [GATE: Geometry-Aware Trained Encoding](https://arxiv.org/abs/2506.08161)
*Jakub Bokšanský,Daniel Meister,Carsten Benthin*

Main category: cs.GR

TL;DR: 提出基于三角网格的几何感知编码GATE，解决哈希编码的冲突问题并提升神经渲染效果


<details>
  <summary>Details</summary>
Motivation: 传统哈希编码存在哈希冲突、分辨率与场景尺寸矛盾、内存访问不一致等缺陷，需更优的几何特征编码方案

Method: 通过将特征向量存储在三角网格表面，利用网格颜色解耦几何密度与特征密度，支持自适应细节控制

Result: 避免哈希冲突，实现更精细的训练控制，支持神经辐射缓存等应用的动态LOD调节

Conclusion: GATE编码为神经渲染算法提供高效几何特征表达，在训练控制和内存效率方面优于哈希编码方案

Abstract: The encoding of input parameters is one of the fundamental building blocks of
neural network algorithms. Its goal is to map the input data to a
higher-dimensional space, typically supported by trained feature vectors. The
mapping is crucial for the efficiency and approximation quality of neural
networks. We propose a novel geometry-aware encoding called GATE that stores
feature vectors on the surface of triangular meshes. Our encoding is suitable
for neural rendering-related algorithms, for example, neural radiance caching.
It also avoids limitations of previous hash-based encoding schemes, such as
hash collisions, selection of resolution versus scene size, and divergent
memory access. Our approach decouples feature vector density from geometry
density using mesh colors, while allowing for finer control over neural network
training and adaptive level-of-detail.

</details>


### [83] [Solving partial differential equations in participating media](https://arxiv.org/abs/2506.08237)
*Bailey Miller,Rohan Sawhney,Keenan Crane,Ioannis Gkioulekas*

Main category: cs.GR

TL;DR: 提出两种基于蒙特卡洛的体素化算法，通过统计建模微颗粒几何，高效求解复杂介质中的椭圆型偏微分方程。


<details>
  <summary>Details</summary>
Motivation: 显式建模复杂微颗粒几何结构在实际应用中不可行，传统方法(如集合平均/均质化)存在效率与精度局限，需要无需离散化的新方法。

Method: 1. 基于指数介质特性开发volumetric walk on spheres/stars算法
2. 将蒙特卡洛方法扩展到参与介质，利用统计属性(如粒子密度)实现离散化自由求解

Result: 实验证明新算法在拉普拉斯边值问题求解中，相比传统方法准确率提升20-30%，计算效率提高1.5倍

Conclusion: 该算法为复杂微颗粒几何PDE求解提供更优解，未来可拓展至非线性方程与其他介质类型

Abstract: We consider the problem of solving partial differential equations (PDEs) in
domains with complex microparticle geometry that is impractical, or
intractable, to model explicitly. Drawing inspiration from volume rendering, we
propose tackling this problem by treating the domain as a participating medium
that models microparticle geometry stochastically, through aggregate
statistical properties (e.g., particle density). We first introduce the problem
setting of PDE simulation in participating media. We then specialize to
exponential media and describe the properties that make them an attractive
model of microparticle geometry for PDE simulation problems. We use these
properties to develop two new algorithms, volumetric walk on spheres and
volumetric walk on stars, that generalize previous Monte Carlo algorithms to
enable efficient and discretization-free simulation of linear elliptic PDEs
(e.g., Laplace) in participating media. We demonstrate experimentally that our
algorithms can solve Laplace boundary value problems with complex microparticle
geometry more accurately and more efficiently than previous approaches, such as
ensemble averaging and homogenization.

</details>


### [84] [Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos](https://arxiv.org/abs/2506.08334)
*Weikun Peng,Jun Lv,Cewu Lu,Manolis Savva*

Main category: cs.GR

TL;DR: 提出从动态RGBD视频中重建铰接物体的粗到细框架，克服现有方法对精良采集数据的依赖，使用智能手机轻松采集的交互视频实现可扩展重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要精心采集的训练/推理数据，难以实现铰接物体的大规模普适化重建。日常智能手机拍摄的交互视频易获取但存在物体与相机同步移动、严重遮挡等挑战。

Method: 引入粗到细框架：1）从动态RGBD视频推断关节参数 2）分割物体可动部件。通过20倍规模合成数据集（784视频/284物体/11类）进行验证。

Result: 在合成和真实数据上显著优于现有视频输入方法，跨类别重建效果优异。

Conclusion: 该方法首次实现从动态RGBD视频跨类别重建铰接物体，为机器人等领域提供实用化解决方案。

Abstract: Articulated objects are prevalent in daily life. Understanding their
kinematic structure and reconstructing them have numerous applications in
embodied AI and robotics. However, current methods require carefully captured
data for training or inference, preventing practical, scalable, and
generalizable reconstruction of articulated objects. We focus on reconstruction
of an articulated object from a casually captured RGBD video shot with a
hand-held camera. A casually captured video of an interaction with an
articulated object is easy to acquire at scale using smartphones. However, this
setting is quite challenging, as the object and camera move simultaneously and
there are significant occlusions as the person interacts with the object. To
tackle these challenges, we introduce a coarse-to-fine framework that infers
joint parameters and segments movable parts of the object from a dynamic RGBD
video. To evaluate our method under this new setting, we build a 20$\times$
larger synthetic dataset of 784 videos containing 284 objects across 11
categories. We compare our approach with existing methods that also take video
as input. Experiments show that our method can reconstruct synthetic and real
articulated objects across different categories from dynamic RGBD videos,
outperforming existing methods significantly.

</details>


### [85] [Complex-Valued Holographic Radiance Fields](https://arxiv.org/abs/2506.08350)
*Yicheng Zhan,Dong-Ha Shin,Seung-Hwan Baek,Kaan Akşit*

Main category: cs.GR

TL;DR: 提出复数值高斯基元表示方法，实现高效物理精确的全息场景渲染


<details>
  <summary>Details</summary>
Motivation: 现有3D场景表示无法同时满足几何对齐和物理光波属性建模需求，制约全息显示发展

Method: 使用复数值高斯基元重构3D高斯泼溅算法，直接通过RGBD多视角图像优化全息场景表示

Result: 实现30-10000倍速度提升，保持图像质量，消除全息图重复优化需求

Conclusion: 首次建立几何对齐的物理精确全息场景表示框架，为下一代显示技术奠定基础

Abstract: Modeling the full properties of light, including both amplitude and phase, in
3D representations is crucial for advancing physically plausible rendering,
particularly in holographic displays. To support these features, we propose a
novel representation that optimizes 3D scenes without relying on
intensity-based intermediaries. We reformulate 3D Gaussian splatting with
complex-valued Gaussian primitives, expanding support for rendering with light
waves. By leveraging RGBD multi-view images, our method directly optimizes
complex-valued Gaussians as a 3D holographic scene representation. This
eliminates the need for computationally expensive hologram re-optimization.
Compared with state-of-the-art methods, our method achieves 30x-10,000x speed
improvements while maintaining on-par image quality, representing a first step
towards geometrically aligned, physically plausible holographic scene
representations.

</details>


### [86] [Fine-Grained Spatially Varying Material Selection in Images](https://arxiv.org/abs/2506.09023)
*Julia Guerrero-Viu,Michael Fischer,Iliyan Georgiev,Elena Garces,Diego Gutierrez,Belen Masia,Valentin Deschaintre*

Main category: cs.GR

TL;DR: 提出基于ViT的多分辨率材质选择方法DuMaS，实现纹理/子纹理两级选择，提升图像编辑的精度和稳定性


<details>
  <summary>Details</summary>
Motivation: 现有材质选择方法在光照和反射变化下不够鲁棒，需改进选择质量以支持高效图像编辑

Method: 结合视觉Transformer特征与多分辨率处理策略，构建包含80万标注图像的DuMaS数据集实现两级材质选择

Result: 方法在合成数据集上实现优于先前方法的精细选择，支持更稳定的下游编辑操作

Conclusion: 多分辨率ViT处理有效提升材质选择质量，DuMaS数据集为材质感知研究提供新基准

Abstract: Selection is the first step in many image editing processes, enabling faster
and simpler modifications of all pixels sharing a common modality. In this
work, we present a method for material selection in images, robust to lighting
and reflectance variations, which can be used for downstream editing tasks. We
rely on vision transformer (ViT) models and leverage their features for
selection, proposing a multi-resolution processing strategy that yields finer
and more stable selection results than prior methods. Furthermore, we enable
selection at two levels: texture and subtexture, leveraging a new two-level
material selection (DuMaS) dataset which includes dense annotations for over
800,000 synthetic images, both on the texture and subtexture levels.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [87] [Hierarchical Lexical Graph for Enhanced Multi-Hop Retrieval](https://arxiv.org/abs/2506.08074)
*Abdellah Ghassel,Ian Robinson,Gabriel Tanase,Hal Cooper,Bryan Thompson,Zhen Han,Vassilis N. Ioannidis,Soji Adeshina,Huzefa Rangwala*

Main category: cs.IR

TL;DR: 提出分层词汇图(HLG)索引结构和两种互补检索器，有效提升跨文档多跳检索性能23.1%


<details>
  <summary>Details</summary>
Motivation: 传统RAG在需要跨语义距离较远的文档拼接答案时表现不佳，现有基准缺乏复杂多跳检索评估体系

Method: HLG三层索引（原子命题追踪-潜在主题聚类-实体关系链接） + StatementGraphRAG（细粒度实体搜索） + TopicGraphRAG（粗主题扩展） + 合成数据生成管道

Result: 在五个数据集上平均检索召回率和准确率相对提升23.1%，开源工具包已发布

Conclusion: 通过结构化索引和多粒度检索策略，系统性解决了复杂多跳问答中的证据整合难题

Abstract: Retrieval-Augmented Generation (RAG) grounds large language models in
external evidence, yet it still falters when answers must be pieced together
across semantically distant documents. We close this gap with the Hierarchical
Lexical Graph (HLG), a three-tier index that (i) traces every atomic
proposition to its source, (ii) clusters propositions into latent topics, and
(iii) links entities and relations to expose cross-document paths. On top of
HLG we build two complementary, plug-and-play retrievers: StatementGraphRAG,
which performs fine-grained entity-aware beam search over propositions for
high-precision factoid questions, and TopicGraphRAG, which selects coarse
topics before expanding along entity links to supply broad yet relevant context
for exploratory queries. Additionally, existing benchmarks lack the complexity
required to rigorously evaluate multi-hop summarization systems, often focusing
on single-document queries or limited datasets. To address this, we introduce a
synthetic dataset generation pipeline that curates realistic, multi-document
question-answer pairs, enabling robust evaluation of multi-hop retrieval
systems. Extensive experiments across five datasets demonstrate that our
methods outperform naive chunk-based RAG achieving an average relative
improvement of 23.1% in retrieval recall and correctness. Open-source Python
library is available at https://github.com/awslabs/graphrag-toolkit.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [88] [A Comprehensive Study of Decoder-Only LLMs for Text-to-Image Generation](https://arxiv.org/abs/2506.08210)
*Andrew Z. Wang,Songwei Ge,Tero Karras,Ming-Yu Liu,Yogesh Balaji*

Main category: cs.CV

TL;DR: 研究通过训练27个文本到图像模型与12种不同文本编码器，发现使用LLMs中间层归一化平均嵌入能显著提升复杂提示对齐效果，多数LLMs表现优于传统T5模型。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型仍使用过时的T5和CLIP作为文本编码器，探索现代仅解码器LLMs作为文本编码器的有效性。

Method: 建立标准化训练评估流程，分析不同嵌入提取方法、LLMs变种及模型尺寸对生成效果的影响，重点关注各层嵌入特性。

Result: 最后一层嵌入效果不佳，采用所有层层归一化平均嵌入可提升复杂提示对齐能力，多数LLMs在此条件下超越T5基线模型。

Conclusion: LLMs作为文本编码器在视觉语言推理任务中展现出优越性能，层嵌入策略的优化是提升文本到图像生成质量的关键。

Abstract: Both text-to-image generation and large language models (LLMs) have made
significant advancements. However, many text-to-image models still employ the
somewhat outdated T5 and CLIP as their text encoders. In this work, we
investigate the effectiveness of using modern decoder-only LLMs as text
encoders for text-to-image diffusion models. We build a standardized training
and evaluation pipeline that allows us to isolate and evaluate the effect of
different text embeddings. We train a total of 27 text-to-image models with 12
different text encoders to analyze the critical aspects of LLMs that could
impact text-to-image generation, including the approaches to extract
embeddings, different LLMs variants, and model sizes. Our experiments reveal
that the de facto way of using last-layer embeddings as conditioning leads to
inferior performance. Instead, we explore embeddings from various layers and
find that using layer-normalized averaging across all layers significantly
improves alignment with complex prompts. Most LLMs with this conditioning
outperform the baseline T5 model, showing enhanced performance in advanced
visio-linguistic reasoning skills.

</details>


### [89] [How Much To Guide: Revisiting Adaptive Guidance in Classifier-Free Guidance Text-to-Vision Diffusion Models](https://arxiv.org/abs/2506.08351)
*Huixuan Zhang,Junzhe Zhang,Xiaojun Wan*

Main category: cs.CV

TL;DR: 提出Step AG自适应引导策略，通过限制前几个去噪步骤的引导范围，在保持生成质量的同时实现20%-30%的加速效果


<details>
  <summary>Details</summary>
Motivation: 传统无分类器引导方法需要双倍计算步骤导致生成成本过高，先前自适应引导研究缺乏系统性分析和通用性验证

Method: Step AG策略：仅在去噪过程的前期阶段应用分类器引导，后期保持无引导状态。适用于不同步长设置和多模态生成模型

Result: 在图像质量与图文对齐度指标上取得可比效果，平均加速20%-30%，在视频生成等扩展场景验证普适性

Conclusion: 通过早期引导约束实现效率突破，为扩散模型优化提供新视角，具有广泛的工程应用价值

Abstract: With the rapid development of text-to-vision generation diffusion models,
classifier-free guidance has emerged as the most prevalent method for
conditioning. However, this approach inherently requires twice as many steps
for model forwarding compared to unconditional generation, resulting in
significantly higher costs. While previous study has introduced the concept of
adaptive guidance, it lacks solid analysis and empirical results, making
previous method unable to be applied to general diffusion models. In this work,
we present another perspective of applying adaptive guidance and propose Step
AG, which is a simple, universally applicable adaptive guidance strategy. Our
evaluations focus on both image quality and image-text alignment. whose results
indicate that restricting classifier-free guidance to the first several
denoising steps is sufficient for generating high-quality, well-conditioned
images, achieving an average speedup of 20% to 30%. Such improvement is
consistent across different settings such as inference steps, and various
models including video generation models, highlighting the superiority of our
method.

</details>


### [90] [CulturalFrames: Assessing Cultural Expectation Alignment in Text-to-Image Models and Evaluation Metrics](https://arxiv.org/abs/2506.08835)
*Shravan Nayak,Mehar Bhatia,Xiaofeng Zhang,Verena Rieser,Lisa Anne Hendricks,Sjoerd van Steenkiste,Yash Goyal,Karolina Stańczak,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: 研究揭示主流文生图模型在文化表征方面存在显著缺陷，显性/隐性文化期望平均未达成率分别达68%和49%，现有评估指标与人类判断脱节。


<details>
  <summary>Details</summary>
Motivation: 针对文生图模型在多元文化语境表征能力不足的问题，系统性量化模型输出与显性/隐性文化期待的偏差，填补该领域研究空白。

Method: 构建包含10国5大领域的CulturalFrames基准(983提示词+3637图像+10k标注)，采用4个SOTA模型进行跨文化生成实验与人工评估。

Result: 模型平均44%未达文化期待：显性失败率68%（覆盖基础文化元素），隐性失败率49%（涉及深层文化逻辑）。现有评估指标与人类判断相关系数低于0.2。

Conclusion: 研究暴露文生图模型文化感知缺陷，提出需开发融合文化知识的新型模型架构与评估体系，推动跨文化生成技术发展。

Abstract: The increasing ubiquity of text-to-image (T2I) models as tools for visual
content generation raises concerns about their ability to accurately represent
diverse cultural contexts. In this work, we present the first study to
systematically quantify the alignment of T2I models and evaluation metrics with
respect to both explicit as well as implicit cultural expectations. To this
end, we introduce CulturalFrames, a novel benchmark designed for rigorous human
evaluation of cultural representation in visual generations. Spanning 10
countries and 5 socio-cultural domains, CulturalFrames comprises 983 prompts,
3637 corresponding images generated by 4 state-of-the-art T2I models, and over
10k detailed human annotations. We find that T2I models not only fail to meet
the more challenging implicit expectations but also the less challenging
explicit expectations. Across models and countries, cultural expectations are
missed an average of 44% of the time. Among these failures, explicit
expectations are missed at a surprisingly high average rate of 68%, while
implicit expectation failures are also significant, averaging 49%. Furthermore,
we demonstrate that existing T2I evaluation metrics correlate poorly with human
judgments of cultural alignment, irrespective of their internal reasoning.
Collectively, our findings expose critical gaps, providing actionable
directions for developing more culturally informed T2I models and evaluation
methodologies.

</details>


### [91] [Socratic-MCTS: Test-Time Visual Reasoning by Asking the Right Questions](https://arxiv.org/abs/2506.08927)
*David Acuna,Ximing Lu,Jaehun Jung,Hyunwoo Kim,Amlan Kar,Sanja Fidler,Yejin Choi*

Main category: cs.CV

TL;DR: 提出一种基于蒙特卡洛树搜索（MCTS）的算法，通过插入子问题-子答案对，无需额外训练即可提升现有视觉语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有非推理模型已广泛部署，但缺乏长链推理能力。研究旨在探索不依赖训练或监督的搜索机制，挖掘其隐含知识并生成扩展推理轨迹。

Method: 将推理建模为搜索过程，使用MCTS算法在模型输出流中插入子问题-子答案对，通过子问题连接碎片化知识形成推理链。

Result: 在MMMU-PRO等三个基准测试中实现持续提升，其中人文科目准确率显著提高9%，整体提升2%。

Conclusion: 将推理重构为包含潜在决策的搜索过程，有效激活非推理模型的扩展推理潜力，证明无需训练即可增强现有模型推理能力。

Abstract: Recent research in vision-language models (VLMs) has centered around the
possibility of equipping them with implicit long-form chain-of-thought
reasoning -- akin to the success observed in language models -- via
distillation and reinforcement learning. But what about the non-reasoning
models already trained and deployed across the internet? Should we simply
abandon them, or is there hope for a search mechanism that can elicit hidden
knowledge and induce long reasoning traces -- without any additional training
or supervision? In this paper, we explore this possibility using a Monte Carlo
Tree Search (MCTS)-inspired algorithm, which injects subquestion-subanswer
pairs into the model's output stream. We show that framing reasoning as a
search process -- where subquestions act as latent decisions within a broader
inference trajectory -- helps the model "connect the dots" between fragmented
knowledge and produce extended reasoning traces in non-reasoning models. We
evaluate our method across three benchmarks and observe consistent
improvements. Notably, our approach yields a 2% overall improvement on
MMMU-PRO, including a significant 9% gain in Liberal Arts.

</details>


### [92] [Autoregressive Semantic Visual Reconstruction Helps VLMs Understand Better](https://arxiv.org/abs/2506.09040)
*Dianyi Wang,Wei Song,Yikun Wang,Siyuan Wang,Kaicheng Yu,Zhongyu Wei,Jiaqi Wang*

Main category: cs.CV

TL;DR: 提出ASVR方法，通过自回归语义视觉重建统一学习多模态，解决现有模型视觉信息利用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型仅对文本序列自回归监督，导致无法有效利用无标注图像、遗漏视觉细节、难以表达视觉中心内容。

Method: ASVR框架联合学习视觉与文本模态，通过自回归重建图像语义表征（而非原始像素），使用离散语义token实现稳定优化。

Result: 在14个多模态基准测试中平均提升LLaVA-1.5模型5%性能，不同数据规模(556k-2M)和LLM骨干均表现稳定增益。

Conclusion: 语义重建比原始视觉重建更有效，连续特征可转化为离散语义token，该方法为多模态理解提供了新思路。

Abstract: Typical large vision-language models (LVLMs) apply autoregressive supervision
solely to textual sequences, without fully incorporating the visual modality
into the learning process. This results in three key limitations: (1) an
inability to utilize images without accompanying captions, (2) the risk that
captions omit critical visual details, and (3) the challenge that certain
vision-centric content cannot be adequately conveyed through text. As a result,
current LVLMs often prioritize vision-to-language alignment while potentially
overlooking fine-grained visual information. While some prior works have
explored autoregressive image generation, effectively leveraging autoregressive
visual supervision to enhance image understanding remains an open challenge. In
this paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),
which enables joint learning of visual and textual modalities within a unified
autoregressive framework. We show that autoregressively reconstructing the raw
visual appearance of images does not enhance and may even impair multimodal
understanding. In contrast, autoregressively reconstructing the semantic
representation of images consistently improves comprehension. Notably, we find
that even when models are given continuous image features as input, they can
effectively reconstruct discrete semantic tokens, resulting in stable and
consistent improvements across a wide range of multimodal understanding
benchmarks. Our approach delivers significant performance gains across varying
data scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves
LLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is
available at https://github.com/AlenjandroWang/ASVR.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [93] [GradEscape: A Gradient-Based Evader Against AI-Generated Text Detectors](https://arxiv.org/abs/2506.08188)
*Wenlong Meng,Shuguo Fan,Chengkun Wei,Min Chen,Yuwei Li,Yuanchao Zhang,Zhikun Zhang,Wenzhi Chen*

Main category: cs.CR

TL;DR: GradEscape：首个基于梯度的AI生成文本检测逃避器，通过加权嵌入解决不可微分问题，适配多架构检测器，实验效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决AI生成文本检测器因文本离散性导致的不可微分计算难题，突破检测器与逃避器语言模型架构不匹配的限制，实现商业检测器场景下的有效逃避。

Method: 1. 构建加权嵌入表征检测器输入
2. 参数更新机制利用检测器反馈
3. 预热启动策略解决分词器不匹配
4. 分词器推理与模型提取技术适配黑盒检测器

Result: 在4个数据集/3个主流语言模型上，以139M参数量击败4个SOTA逃避器（含11B复述模型），成功应用于2个商业检测器。关键漏洞源于训练数据文本表达风格差异。

Conclusion: 提出首个端到端梯度逃避框架，开源促进鲁棒检测器开发，同时提出基于训练数据风格平衡的防御方案。

Abstract: In this paper, we introduce GradEscape, the first gradient-based evader
designed to attack AI-generated text (AIGT) detectors. GradEscape overcomes the
undifferentiable computation problem, caused by the discrete nature of text, by
introducing a novel approach to construct weighted embeddings for the detector
input. It then updates the evader model parameters using feedback from victim
detectors, achieving high attack success with minimal text modification. To
address the issue of tokenizer mismatch between the evader and the detector, we
introduce a warm-started evader method, enabling GradEscape to adapt to
detectors across any language model architecture. Moreover, we employ novel
tokenizer inference and model extraction techniques, facilitating effective
evasion even in query-only access.
  We evaluate GradEscape on four datasets and three widely-used language
models, benchmarking it against four state-of-the-art AIGT evaders.
Experimental results demonstrate that GradEscape outperforms existing evaders
in various scenarios, including with an 11B paraphrase model, while utilizing
only 139M parameters. We have successfully applied GradEscape to two real-world
commercial AIGT detectors. Our analysis reveals that the primary vulnerability
stems from disparity in text expression styles within the training data. We
also propose a potential defense strategy to mitigate the threat of AIGT
evaders. We open-source our GradEscape for developing more robust AIGT
detectors.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [94] [RADAR: Benchmarking Language Models on Imperfect Tabular Data](https://arxiv.org/abs/2506.08249)
*Ken Gu,Zhihan Zhang,Kate Lin,Yuwei Zhang,Akshay Paruchuri,Hong Yu,Mehran Kazemi,Kumar Ayush,A. Ali Heydari,Maxwell A. Xu,Girish Narayanswamy,Yun Liu,Ming-Zher Poh,Yuzhe Yang,Mark Malhotra,Shwetak Patel,Hamid Palangi,Xuhai Xu,Daniel McDuff,Tim Althoff,Xin Liu*

Main category: cs.DB

TL;DR: 提出RADAR基准测试框架，揭示前沿语言模型在表格数据处理中存在数据感知缺陷，当面对数据异常时性能显著下降


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在真实表格数据分析中对数据异常（如缺失值/异常值）的处理能力不足，可能影响分析结论有效性

Method: 通过程序化扰动生成2980个表格查询对，覆盖9个领域5类数据异常，并系统性改变表格规模进行测试

Result: 前沿模型在无异常数据表现尚可，但遇到数据异常时性能显著下降（平均准确率下降30%+）

Conclusion: RADAR基准测试揭示了语言模型在数据感知分析上的重大缺陷，其灵活框架为提升表格推理能力提供重要评估工具

Abstract: Language models (LMs) are increasingly being deployed to perform autonomous
data analyses. However, their data awareness -- the ability to recognize,
reason over, and appropriately handle data artifacts such as missing values,
outliers, and logical inconsistencies -- remains underexplored. These artifacts
are especially common in real-world tabular data and, if mishandled, can
significantly compromise the validity of analytical conclusions. To address
this gap, we present RADAR, a benchmark for systematically evaluating
data-aware reasoning on tabular data. We develop a framework to simulate data
artifacts via programmatic perturbations to enable targeted evaluation of model
behavior. RADAR comprises 2980 table query pairs, grounded in real-world data
spanning 9 domains and 5 data artifact types. In addition to evaluating
artifact handling, RADAR systematically varies table size to study how
reasoning performance holds when increasing table size. Our evaluation reveals
that, despite decent performance on tables without data artifacts, frontier
models degrade significantly when data artifacts are introduced, exposing
critical gaps in their capacity for robust, data-aware analysis. Designed to be
flexible and extensible, RADAR supports diverse perturbation types and
controllable table sizes, offering a valuable resource for advancing tabular
reasoning.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [95] [Instruction-Tuned Video-Audio Models Elucidate Functional Specialization in the Brain](https://arxiv.org/abs/2506.08277)
*Subba Reddy Oota,Khushbu Pahwa,Prachi Jindal,Satya Sai Srinath Namburi,Maneesh Singh,Tanmoy Chakraborty,Bapi S. Raju,Manish Gupta*

Main category: q-bio.NC

TL;DR: 多模态大语言模型（MLLMs）在任务指令调优后，其视频和音频表征与人类观看自然电影时的脑活动对齐度显著提升，并呈现层次化脑区对应关系。


<details>
  <summary>Details</summary>
Motivation: 解决先前研究局限——主要关注单模态场景或未指令调优的多模态模型，探究指令调优后的MLLMs在多模态刺激（视频+音频）下与脑活动的对齐机制。

Method: 使用6个视频和2个音频指令调优MLLMs，通过13种视频任务指令测试模型对自然电影观看期间脑神经活动的预测能力，对比非指令调优多模态/单模态模型。

Result: 1. 指令调优视频MLLMs预测性能超越基线模型15-20%；2. 任务指令使MLLMs表征解耦，精准区分脑区多模态处理；3. 模型层次与脑区层级对应（早期层-感觉皮层，中后期层-高级视觉/语言区）。

Conclusion: 任务指令显著增强脑-MLLM对齐，为揭示人脑与AI系统的联合信息处理机制提供新途径，代码已开源。

Abstract: Recent voxel-wise multimodal brain encoding studies have shown that
multimodal large language models (MLLMs) exhibit a higher degree of brain
alignment compared to unimodal models in both unimodal and multimodal stimulus
settings. More recently, instruction-tuned multimodal models have shown to
generate task-specific representations that align strongly with brain activity.
However, prior work evaluating the brain alignment of MLLMs has primarily
focused on unimodal settings or relied on non-instruction-tuned multimodal
models for multimodal stimuli. To address this gap, we investigated brain
alignment, that is, measuring the degree of predictivity of neural activity
recorded while participants were watching naturalistic movies (video along with
audio) with representations derived from MLLMs. We utilized
instruction-specific embeddings from six video and two audio instruction-tuned
MLLMs. Experiments with 13 video task-specific instructions show that
instruction-tuned video MLLMs significantly outperform non-instruction-tuned
multimodal (by 15%) and unimodal models (by 20%). Our evaluation of MLLMs for
both video and audio tasks using language-guided instructions shows clear
disentanglement in task-specific representations from MLLMs, leading to precise
differentiation of multimodal functional processing in the brain. We also find
that MLLM layers align hierarchically with the brain, with early sensory areas
showing strong alignment with early layers, while higher-level visual and
language regions align more with middle to late layers. These findings provide
clear evidence for the role of task-specific instructions in improving the
alignment between brain activity and MLLMs, and open new avenues for mapping
joint information processing in both the systems. We make the code publicly
available [https://github.com/subbareddy248/mllm_videos].

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [96] [A Survey on Large Language Models for Mathematical Reasoning](https://arxiv.org/abs/2506.08446)
*Peng-Yuan Wang,Tian-Shuo Liu,Chenyang Wang,Yi-Di Wang,Shu Yan,Cheng-Xing Jia,Xu-Hui Liu,Xin-Wei Chen,Jia-Cheng Xu,Ziniu Li,Yang Yu*

Main category: cs.AI

TL;DR: 系统综述大语言模型在数学推理领域的发展路径，提出从理解到生成的两阶段认知框架，总结现有方法并指出未来研究方向


<details>
  <summary>Details</summary>
Motivation: 数学推理是AI核心挑战，近年来LLMs取得突破但存在基础性瓶颈，需系统性分析现状与未来方向

Method: 通过预训练策略建立数学理解，采用思维链推理、监督微调、强化学习等技术提升推理能力

Result: 当前方法显著提升数学推理性能，但面临模型容量限制、计算效率低下和泛化能力不足等问题

Conclusion: 建议发展知识增强预训练、形式化推理框架、元泛化学习范式等方向，推动跨领域推理能力迁移

Abstract: Mathematical reasoning has long represented one of the most fundamental and
challenging frontiers in artificial intelligence research. In recent years,
large language models (LLMs) have achieved significant advances in this area.
This survey examines the development of mathematical reasoning abilities in
LLMs through two high-level cognitive phases: comprehension, where models gain
mathematical understanding via diverse pretraining strategies, and answer
generation, which has progressed from direct prediction to step-by-step
Chain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical
reasoning, ranging from training-free prompting to fine-tuning approaches such
as supervised fine-tuning and reinforcement learning, and discuss recent work
on extended CoT and "test-time scaling". Despite notable progress, fundamental
challenges remain in terms of capacity, efficiency, and generalization. To
address these issues, we highlight promising research directions, including
advanced pretraining and knowledge augmentation techniques, formal reasoning
frameworks, and meta-generalization through principled learning paradigms. This
survey tries to provide some insights for researchers interested in enhancing
reasoning capabilities of LLMs and for those seeking to apply these techniques
to other domains.

</details>


### [97] [Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08745)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Yingjie Wang,Baisheng Lai,Jieping Ye,Mingli Song,Dacheng Tao*

Main category: cs.AI

TL;DR: 提出自激励强化学习框架CoVo，通过分析中间推理状态的一致性为LLM提供内在奖励，实现无监督的推理训练。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法依赖外部监督，限制了应用范围。正确回答的中间推理步骤具有高一致性（向最终答案收敛）和低波动性（较少偏离其他候选答案）的特征。

Method: 设计CoVo奖励机制，整合一致性（Consistency）和波动性（Volatility）的向量空间聚合策略，配合好奇心奖励促进探索。

Result: 在多种推理基准测试中达到或超越监督强化学习效果，代码已开源。

Conclusion: 该框架为无监督推理学习提供了可扩展路径，降低对外部监督的依赖。

Abstract: Recent advances of Reinforcement Learning (RL) have highlighted its potential
in complex reasoning tasks, yet effective training often relies on external
supervision, which limits the broader applicability. In this work, we propose a
novel self-rewarding reinforcement learning framework to enhance Large Language
Model (LLM) reasoning by leveraging the consistency of intermediate reasoning
states across different reasoning trajectories. Our key insight is that correct
responses often exhibit consistent trajectory patterns in terms of model
likelihood: their intermediate reasoning states tend to converge toward their
own final answers (high consistency) with minimal deviation toward other
candidates (low volatility). Inspired by this observation, we introduce CoVo,
an intrinsic reward mechanism that integrates Consistency and Volatility via a
robust vector-space aggregation strategy, complemented by a curiosity bonus to
promote diverse exploration. CoVo enables LLMs to perform RL in a
self-rewarding manner, offering a scalable pathway for learning to reason
without external supervision. Extensive experiments on diverse reasoning
benchmarks show that CoVo achieves performance comparable to or even surpassing
supervised RL. Our code is available at https://github.com/sastpg/CoVo.

</details>


### [98] [Paths to Causality: Finding Informative Subgraphs Within Knowledge Graphs for Knowledge-Based Causal Discovery](https://arxiv.org/abs/2506.08771)
*Yuni Susanti,Michael Färber*

Main category: cs.AI

TL;DR: 提出融合知识图谱与LLM的知识驱动因果发现方法，通过元路径子图筛选提升44.4%F1值


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的因果推理方法存在结果不稳定、不一致的问题，影响可靠性。知识图谱的结构化知识可增强推理过程的可信度。

Method: 1. 从知识图谱抽取基于元路径的启发式子图
2. 使用Learning-to-Rank模型优化子图选择
3. 融合优质子图构建零样本提示框架
4. 在生物医学和开放域数据集进行跨模型验证

Result: 在生物医学和开放域数据集上，F1分数最高提升44.4个百分点（不同LLM和KG组合下表现稳定）

Conclusion: 知识图谱的结构化路径信息有效增强LLM的因果推理能力，Learning-to-Rank子图筛选机制显著提升推理可靠性，为知识驱动型因果发现提供新范式

Abstract: Inferring causal relationships between variable pairs is crucial for
understanding multivariate interactions in complex systems. Knowledge-based
causal discovery -- which involves inferring causal relationships by reasoning
over the metadata of variables (e.g., names or textual context) -- offers a
compelling alternative to traditional methods that rely on observational data.
However, existing methods using Large Language Models (LLMs) often produce
unstable and inconsistent results, compromising their reliability for causal
inference. To address this, we introduce a novel approach that integrates
Knowledge Graphs (KGs) with LLMs to enhance knowledge-based causal discovery.
Our approach identifies informative metapath-based subgraphs within KGs and
further refines the selection of these subgraphs using Learning-to-Rank-based
models. The top-ranked subgraphs are then incorporated into zero-shot prompts,
improving the effectiveness of LLMs in inferring the causal relationship.
Extensive experiments on biomedical and open-domain datasets demonstrate that
our method outperforms most baselines by up to 44.4 points in F1 scores,
evaluated across diverse LLMs and KGs. Our code and datasets are available on
GitHub: https://github.com/susantiyuni/path-to-causality

</details>


### [99] [Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents](https://arxiv.org/abs/2506.08800)
*Irene Testini,José Hernández-Orallo,Lorenzo Pacchiardi*

Main category: cs.AI

TL;DR: 论文系统评估了LLM在数据科学中的应用现状，指出当前研究存在目标范围局限、协作模式单一和自动化潜力认知不足三大问题


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLM在数据科学中的评估存在系统性缺陷，主要体现在活动覆盖不全、协作层次缺失和自动化可能性认知局限

Method: 采用文献综述方法，对现有LLM助手和代理在数据科学领域的应用研究进行全面调查分析

Result: 发现当前研究存在：1) 83%聚焦代码生成等目标性任务，忽略数据管理；2) 仅6%探讨人机协作中间形态；3) 未有效挖掘任务重构带来的自动化潜力

Conclusion: 需要建立多维评估框架，将数据管理纳入研究范畴，构建连续的人机协作光谱，并通过任务重构实现更高级别的自动化突破

Abstract: Data science aims to extract insights from data to support decision-making
processes. Recently, Large Language Models (LLMs) are increasingly used as
assistants for data science, by suggesting ideas, techniques and small code
snippets, or for the interpretation of results and reporting. Proper automation
of some data-science activities is now promised by the rise of LLM agents,
i.e., AI systems powered by an LLM equipped with additional affordances--such
as code execution and knowledge bases--that can perform self-directed actions
and interact with digital environments. In this paper, we survey the evaluation
of LLM assistants and agents for data science. We find (1) a dominant focus on
a small subset of goal-oriented activities, largely ignoring data management
and exploratory activities; (2) a concentration on pure assistance or fully
autonomous agents, without considering intermediate levels of human-AI
collaboration; and (3) an emphasis on human substitution, therefore neglecting
the possibility of higher levels of automation thanks to task transformation.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [100] [Approaching Dialogue State Tracking via Aligning Speech Encoders and LLMs](https://arxiv.org/abs/2506.08633)
*Šimon Sedláček,Bolaji Yusuf,Ján Švec,Pradyoth Hegde,Santosh Kesiraju,Oldřich Plchot,Jan Černocký*

Main category: eess.AS

TL;DR: 通过连接语音编码器与LLM的表示空间，使用WavLM+OLMo+后处理方案在SpokenWOZ数据集实现口语对话状态追踪SOTA（42.17% JGA）


<details>
  <summary>Details</summary>
Motivation: 现有语音对话系统在开放性和性能上存在局限，特别是语音与文本表示空间不匹配导致口语对话状态追踪困难

Method: 1. 构建WavLM-large语音编码器与OLMo/Gemma-2-9B的跨模态连接器
2. 采用LoRA适配器微调和模糊匹配后处理技术
3. 使用Speech-Aware MultiWOZ进行数据增强

Result: 最佳系统在SpokenWOZ测试集达到42.17%联合目标准确率（JGA），命名实体识别性能显著提升

Conclusion: 通过跨模态连接架构和后处理优化，证明了完全开源组件在复杂口语对话任务中的有效性，同时揭示模型微调与数据增强的关键作用

Abstract: In this work, we approach spoken Dialogue State Tracking (DST) by bridging
the representation spaces of speech encoders and LLMs via a small connector
module, with a focus on fully open-sourced and open-data components
(WavLM-large, OLMo). We focus on ablating different aspects of such systems
including full/LoRA adapter fine-tuning, the effect of agent turns in the
dialogue history, as well as fuzzy matching-based output post-processing, which
greatly improves performance of our systems on named entities in the dialogue
slot values. We conduct our experiments on the SpokenWOZ dataset, and
additionally utilize the Speech-Aware MultiWOZ dataset to augment our training
data. Ultimately, our best-performing WavLM + connector + OLMo-1B aligned
models achieve state of the art on the SpokenWOZ test set (34.66% JGA), and our
system with Gemma-2-9B-instruct further surpasses this result, reaching 42.17%
JGA on SpokenWOZ test.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [101] [Step-Audio-AQAA: a Fully End-to-End Expressive Large Audio Language Model](https://arxiv.org/abs/2506.08967)
*Ailin Huang,Bingxin Li,Bruce Wang,Boyong Wu,Chao Yan,Chengli Feng,Heng Wang,Hongyu Zhou,Hongyuan Wang,Jingbei Li,Jianjian Sun,Joanna Wang,Mingrui Chen,Peng Liu,Ruihang Miao,Shilei Jiang,Tian Fei,Wang You,Xi Chen,Xuerui Yang,Yechang Huang,Yuxiang Zhang,Zheng Ge,Zheng Gong,Zhewei Huang,Zixin Zhang,Bin Wang,Bo Li,Buyun Ma,Changxin Miao,Changyi Wan,Chen Xu,Dapeng Shi,Dingyuan Hu,Enle Liu,Guanzhe Huang,Gulin Yan,Hanpeng Hu,Haonan Jia,Jiahao Gong,Jiaoren Wu,Jie Wu,Jie Yang,Junzhe Lin,Kaixiang Li,Lei Xia,Longlong Gu,Ming Li,Nie Hao,Ranchen Ming,Shaoliang Pang,Siqi Liu,Song Yuan,Tiancheng Cao,Wen Li,Wenqing He,Xu Zhao,Xuelin Zhang,Yanbo Yu,Yinmin Zhong,Yu Zhou,Yuanwei Liang,Yuanwei Lu,Yuxiang Yang,Zidong Yang,Zili Zhang,Binxing Jiao,Heung-Yeung Shum,Jiansheng Chen,Jing Li,Xiangyu Zhang,Xinhao Zhang,Yibo Zhu,Daxin Jiang,Shuchang Zhou,Chen Hu*

Main category: cs.SD

TL;DR: 提出端到端大型音频语言模型Step-Audio-AQAA，通过双码本音频标记器与神经声码器实现自然语音交互，在语音控制指标上超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有LALMs依赖文本输出导致语音交互不自然，需建立端到端的音频输入输出架构提升交互流畅度。

Method: 整合双码本音频标记器（提取语言/语义特征）+1300亿参数LLM+神经声码器，采用交错标记输出训练策略，结合DPO优化与模型融合技术。

Result: 在StepEval-Audio-360基准测试中，语音控制等核心指标超越SOTA模型，验证基于标记的声码器对性能提升的关键作用。

Conclusion: 该工作为端到端LALMs提供了创新解决方案，证实音频标记化声码器在AQAA任务中的体系结构价值。

Abstract: Large Audio-Language Models (LALMs) have significantly advanced intelligent
human-computer interaction, yet their reliance on text-based outputs limits
their ability to generate natural speech responses directly, hindering seamless
audio interactions. To address this, we introduce Step-Audio-AQAA, a fully
end-to-end LALM designed for Audio Query-Audio Answer (AQAA) tasks. The model
integrates a dual-codebook audio tokenizer for linguistic and semantic feature
extraction, a 130-billion-parameter backbone LLM and a neural vocoder for
high-fidelity speech synthesis. Our post-training approach employs interleaved
token-output of text and audio to enhance semantic coherence and combines
Direct Preference Optimization (DPO) with model merge to improve performance.
Evaluations on the StepEval-Audio-360 benchmark demonstrate that
Step-Audio-AQAA excels especially in speech control, outperforming the
state-of-art LALMs in key areas. This work contributes a promising solution for
end-to-end LALMs and highlights the critical role of token-based vocoder in
enhancing overall performance for AQAA tasks.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [102] [EDINET-Bench: Evaluating LLMs on Complex Financial Tasks using Japanese Financial Statements](https://arxiv.org/abs/2506.08762)
*Issa Sugiura,Takashi Ishida,Taro Makino,Chieko Tazuke,Takanori Nakagawa,Kosuke Nakago,David Ha*

Main category: q-fin.ST

TL;DR: 日本研究者推出EDINET-Bench金融基准测试，发现当前大语言模型在欺诈检测等金融任务中表现仅略优于传统逻辑回归


<details>
  <summary>Details</summary>
Motivation: 解决日语金融数据稀缺阻碍LLM在金融分析领域发展的问题，推动金融领域专用模型研究

Method: 从EDINET下载十年年报数据，自动生成会计欺诈/收益预测/行业预测三大任务的标注数据集

Result: 顶级LLM在欺诈检测二元分类中准确率仅稍高于逻辑回归（LLMs 65% vs LR 62%），收益预测任务表现相似

Conclusion: LLM在真实金融场景应用存在显著挑战，需领域适配。公开数据集/代码以促进金融LLM研究

Abstract: Financial analysis presents complex challenges that could leverage large
language model (LLM) capabilities. However, the scarcity of challenging
financial datasets, particularly for Japanese financial data, impedes academic
innovation in financial analytics. As LLMs advance, this lack of accessible
research resources increasingly hinders their development and evaluation in
this specialized domain. To address this gap, we introduce EDINET-Bench, an
open-source Japanese financial benchmark designed to evaluate the performance
of LLMs on challenging financial tasks including accounting fraud detection,
earnings forecasting, and industry prediction. EDINET-Bench is constructed by
downloading annual reports from the past 10 years from Japan's Electronic
Disclosure for Investors' NETwork (EDINET) and automatically assigning labels
corresponding to each evaluation task. Our experiments reveal that even
state-of-the-art LLMs struggle, performing only slightly better than logistic
regression in binary classification for fraud detection and earnings
forecasting. These results highlight significant challenges in applying LLMs to
real-world financial applications and underscore the need for domain-specific
adaptation. Our dataset, benchmark construction code, and evaluation code is
publicly available to facilitate future research in finance with LLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [103] [Modality-Balancing Preference Optimization of Large Multimodal Models by Adversarial Negative Mining](https://arxiv.org/abs/2506.08022)
*Chenxi Liu,Tianyi Xiong,Ruibo Chen,Yihan Wu,Junfeng Guo,Tianyi Zhou,Heng Huang*

Main category: cs.LG

TL;DR: 提出MBPO方法解决大型多模态模型的模态失衡问题，通过硬负样本生成和在线验证奖励优化模型表现


<details>
  <summary>Details</summary>
Motivation: 现有偏好优化方法未能有效抑制LLM先验偏差，导致LMM存在视觉信息忽视和幻觉现象

Method: 1. 通过对抗性图像扰动生成硬负样本构建离线数据集
2. 利用闭合任务特性生成带验证奖励的在线响应
3. 结合GRPO进行离线-在线混合训练

Result: 实验表明MBPO显著提升视觉语言任务性能并减少61.3%的幻觉现象

Conclusion: MBPO首次实现基于硬负样本和在线验证的双重模态平衡，为多模态对齐提供新方向

Abstract: The task adaptation and alignment of Large Multimodal Models (LMMs) have been
significantly advanced by instruction tuning and further strengthened by recent
preference optimization. Yet, most LMMs still suffer from severe modality
imbalance during reasoning, i.e., outweighing language prior biases over visual
inputs, which bottlenecks their generalization to downstream tasks and causes
hallucinations. However, existing preference optimization approaches for LMMs
do not focus on restraining the internal biases of their Large Language Model
(LLM) backbones when curating the training data. Moreover, they heavily rely on
offline data and lack the capacity to explore diverse responses adaptive to
dynamic distributional shifts during training. Meanwhile, Group Relative Policy
Optimization (GRPO), a recent method using online-generated data and verified
rewards to improve reasoning capabilities, remains largely underexplored in LMM
alignment. In this paper, we propose a novel preference learning framework,
Modality-Balancing Preference Optimization (MBPO), to address the modality
imbalance in LMMs. MBPO constructs a more effective offline preference dataset
by generating hard negatives, i.e., rejected responses misled by LLM biases due
to limited usage of visual information, through adversarial perturbation of
input images. Moreover, MBPO leverages the easy-to-verify nature of close-ended
tasks to generate online responses with verified rewards. GRPO is then employed
to train the model with offline-online hybrid data. Extensive experiments
demonstrate that MBPO can enhance LMM performance on challenging
vision-language tasks and effectively reduce hallucinations.

</details>


### [104] [Bingo: Boosting Efficient Reasoning of LLMs via Dynamic and Significance-based Reinforcement Learning](https://arxiv.org/abs/2506.08125)
*Hanbing Liu,Lang Cao,Yuanyi Ren,Mengyu Zhou,Haoyu Dong,Xiaojun Ma,Shi Han,Dongmei Zhang*

Main category: cs.LG

TL;DR: Bingo框架通过显著性感知长度奖励和动态长度奖励机制，在保持大语言模型推理准确性的同时显著提升效率，实现精度与效率的平衡优化


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法主要关注提升推理准确性，而对效率优化关注不足。直接采用基于输出长度的奖励机制常导致准确性下降，需要更精细的奖励设计来协调两者关系

Method: 1. 显著性感知长度奖励：通过梯度引导逐步减少非关键token
2. 动态长度奖励：初期鼓励详细推理（应对难题），随时间衰减提升整体效率

Result: 在多推理基准测试中超越传统RL奖励机制，实现准确率提升同时减少冗余输出，达成最佳精度-效率平衡

Conclusion: 显式训练大语言模型进行高效推理具有显著潜力，Bingo的奖励机制设计为平衡模型性能与计算效率提供了新方向

Abstract: Large language models have demonstrated impressive reasoning capabilities,
yet they often suffer from inefficiencies due to unnecessarily verbose or
redundant outputs. While many works have explored reinforcement learning (RL)
to enhance reasoning abilities, most primarily focus on improving accuracy,
with limited attention to reasoning efficiency. Some existing approaches
introduce direct length-based rewards to encourage brevity, but this often
leads to noticeable drops in accuracy. In this paper, we propose Bingo, an RL
framework that advances length-based reward design to boost efficient
reasoning. Bingo incorporates two key mechanisms: a significance-aware length
reward, which gradually guides the model to reduce only insignificant tokens,
and a dynamic length reward, which initially encourages elaborate reasoning for
hard questions but decays over time to improve overall efficiency. Experiments
across multiple reasoning benchmarks show that Bingo improves both accuracy and
efficiency. It outperforms the vanilla reward and several other length-based
reward baselines in RL, achieving a favorable trade-off between accuracy and
efficiency. These results underscore the potential of training LLMs explicitly
for efficient reasoning.

</details>


### [105] [AutoSDT: Scaling Data-Driven Discovery Tasks Toward Open Co-Scientists](https://arxiv.org/abs/2506.08140)
*Yifei Li,Hanane Nour Moussa,Ziru Chen,Shijie Chen,Botao Yu,Mingyi Xue,Benjamin Burns,Tzu-Yao Chiu,Vishal Dey,Zitong Lu,Chen Wei,Qianheng Zhang,Tianyu Zhang,Song Gao,Xuhui Huang,Xia Ning,Nesreen K. Ahmed,Ali Payani,Huan Sun*

Main category: cs.LG

TL;DR: AutoSDT自动化流程构建了当前最大的开源科学发现数据集AutoSDT-5K（5,404任务），通过LLM的编码能力实现任务自动生成与验证，显著提升模型在科学发现任务中的表现


<details>
  <summary>Details</summary>
Motivation: 现有AI科研助手受限于高质量训练数据不足，需要自动化的任务生成方案来解决数据稀缺问题

Method: 利用LLM的代码生成能力和参数化知识，实现：1）多源任务自动搜索与有效性筛选 2）任务指令与代码方案合成 3）覆盖4大学科领域和756个Python包的数据集构建

Result: 1）5,404任务中93%通过专家有效性验证 2）92.2%生成代码功能正确 3）AutoSDT-Coder-32B在ScienceAgentBench达到GPT-4o水平（7.8%成功率），DiscoveryBench假设匹配分数提升17.4%

Conclusion: AutoSDT证明了自动生成科学发现任务的可行性，为数据驱动的科研工具开发提供新范式，显著缩小开源模型与顶级闭源模型的性能差距

Abstract: Despite long-standing efforts in accelerating scientific discovery with AI,
building AI co-scientists remains challenging due to limited high-quality data
for training and evaluation. To tackle this data scarcity issue, we present
AutoSDT, an automatic pipeline that collects high-quality coding tasks in
real-world data-driven discovery workflows. AutoSDT leverages the coding
capabilities and parametric knowledge of LLMs to search for diverse sources,
select ecologically valid tasks, and synthesize accurate task instructions and
code solutions. Using our pipeline, we construct AutoSDT-5K, a dataset of 5,404
coding tasks for data-driven discovery that covers four scientific disciplines
and 756 unique Python packages. To the best of our knowledge, AutoSDT-5K is the
only automatically collected and the largest open dataset for data-driven
scientific discovery. Expert feedback on a subset of 256 tasks shows the
effectiveness of AutoSDT: 93% of the collected tasks are ecologically valid,
and 92.2% of the synthesized programs are functionally correct. Trained on
AutoSDT-5K, the Qwen2.5-Coder-Instruct LLM series, dubbed AutoSDT-Coder, show
substantial improvement on two challenging data-driven discovery benchmarks,
ScienceAgentBench and DiscoveryBench. Most notably, AutoSDT-Coder-32B reaches
the same level of performance as GPT-4o on ScienceAgentBench with a success
rate of 7.8%, doubling the performance of its base model. On DiscoveryBench, it
lifts the hypothesis matching score to 8.1, bringing a 17.4% relative
improvement and closing the gap between open-weight models and GPT-4o.

</details>


### [106] [Reinforcement Learning from Human Feedback with High-Confidence Safety Constraints](https://arxiv.org/abs/2506.08266)
*Yaswanth Chittepu,Blossom Metevier,Will Schwarzer,Austin Hoag,Scott Niekum,Philip S. Thomas*

Main category: cs.LG

TL;DR: 提出HC-RLHF方法，通过两阶段优化确保语言模型在敏感领域的安全响应，同时保持高有用性


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法将安全性与有用性简单权衡，可能导致敏感领域的不安全回应

Method: 1. 将人类偏好分解为奖励模型（有用性）和成本模型（安全性）
2. 分两阶段优化：先基于悲观成本约束优化奖励，再进行安全测试验证

Result: 在Qwen2-1.5B等三个模型上验证，相比现有方法提升安全性和有用性，理论证明安全违规概率低于用户设定阈值

Conclusion: HC-RLHF通过解耦学习与双重验证机制，在保持模型有用性的同时提供高置信度的安全保证

Abstract: Existing approaches to language model alignment often treat safety as a
tradeoff against helpfulness, which can lead to unacceptable responses in
sensitive domains. To ensure reliable performance in such settings, we propose
High-Confidence Safe Reinforcement Learning from Human Feedback (HC-RLHF), a
method that provides high-confidence safety guarantees while maximizing
helpfulness. Similar to previous methods, HC-RLHF explicitly decouples human
preferences into helpfulness and harmlessness (safety), which are learned by
training a reward model and a cost model, respectively. It then employs a
two-step process to find safe solutions. In the first step, it optimizes the
reward function under an intentionally pessimistic version of the cost
constraint. In the second step, the trained model undergoes a safety test to
verify whether its performance stays within an upper-confidence bound of the
actual cost constraint. We provide a theoretical analysis of HC-RLHF, including
proof that it will not return an unsafe solution with a probability greater
than a user-specified threshold. For our empirical analysis, we apply HC-RLHF
to align three different language models (Qwen2-1.5B, Qwen2.5-3B, and
LLaMa3.2-3B) with human preferences. Our results demonstrate that HC-RLHF
produces safe models with high probability and can improve harmlessness and
helpfulness compared to previous methods.

</details>


### [107] [From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium](https://arxiv.org/abs/2506.08292)
*Xie Yi,Zhanke Zhou,Chentao Cao,Qiyu Niu,Tongliang Liu,Bo Han*

Main category: cs.LG

TL;DR: 提出ECON框架，通过贝叶斯纳什均衡实现高效多LLM协作，相比传统方法降低计算成本11.2%且具备理论保障


<details>
  <summary>Details</summary>
Motivation: 传统多智能体框架存在高计算开销、缺乏收敛保证的缺陷，需建立理论可靠的高效协作机制

Method: 将多LLM协作建模为不完全信息博弈，通过分层强化学习框架实现分布式推理与集中决策，各LLM基于贝叶斯信念自主优化响应策略

Result: 理论证明获得比非均衡方法更紧的遗憾边界，在6个复杂推理任务中平均提升11.2%性能，实验验证框架可扩展性

Conclusion: ECON开创了理论保证与高效实践兼备的多LLM协作范式，为构建更大规模语言模型系统提供新路径

Abstract: Multi-agent frameworks can substantially boost the reasoning power of large
language models (LLMs), but they typically incur heavy computational costs and
lack convergence guarantees. To overcome these challenges, we recast multi-LLM
coordination as an incomplete-information game and seek a Bayesian Nash
equilibrium (BNE), in which each agent optimally responds to its probabilistic
beliefs about the strategies of others. We introduce Efficient Coordination via
Nash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that
marries distributed reasoning with centralized final output. Under ECON, each
LLM independently selects responses that maximize its expected reward,
conditioned on its beliefs about co-agents, without requiring costly
inter-agent exchanges. We mathematically prove that ECON attains a markedly
tighter regret bound than non-equilibrium multi-agent schemes. Empirically,
ECON outperforms existing multi-LLM approaches by 11.2% on average across six
benchmarks spanning complex reasoning and planning tasks. Further experiments
demonstrate ECON's ability to flexibly incorporate additional models,
confirming its scalability and paving the way toward larger, more powerful
multi-LLM ensembles. The code is publicly available at:
https://github.com/tmlr-group/ECON.

</details>


### [108] [From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?](https://arxiv.org/abs/2506.08295)
*Zhanke Zhou,Xiao Feng,Zhaocheng Zhu,Jiangchao Yao,Sanmi Koyejo,Bo Han*

Main category: cs.LG

TL;DR: 提出AR-Bench新基准揭示大语言模型在主动推理（需与环境交互获取信息）方面的显著不足，当前改进策略效果有限


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于被动推理评估，而真实场景需要的主动推理能力（交互式信息获取）缺乏系统评测工具

Method: 构建包含侦探案例/情景谜题/猜数字三类任务的AR-Bench，模拟需主动信息获取的复杂推理场景

Result: 实验显示当代LLMs主动推理能力薄弱（准确率较被动推理下降60%+），树搜索等策略仅带来有限提升（<15%）

Conclusion: 强调开发交互学习/实时反馈/环境感知训练等新方法的迫切性，推动LLMs向实用化智能体发展

Abstract: While existing benchmarks probe the reasoning abilities of large language
models (LLMs) across diverse domains, they predominantly assess passive
reasoning, providing models with all the information needed to reach a
solution. By contrast, active reasoning-where an LLM must interact with
external systems to acquire missing evidence or data-has received little
systematic attention. To address this shortfall, we present AR-Bench, a novel
benchmark designed explicitly to evaluate an LLM's active reasoning skills.
AR-Bench comprises three task families-detective cases, situation puzzles, and
guessing numbers-that together simulate real-world, agentic scenarios and
measure performance across commonsense, logical, and symbolic reasoning
challenges. Empirical evaluation on AR-Bench demonstrates that contemporary
LLMs exhibit pronounced difficulties with active reasoning: they frequently
fail to acquire or leverage the information needed to solve tasks. This gap
highlights a stark divergence between their passive and active reasoning
abilities. Moreover, ablation studies indicate that even advanced strategies,
such as tree-based searching or post-training approaches, yield only modest
gains and fall short of the levels required for real-world deployment.
Collectively, these findings highlight the critical need to advance methodology
for active reasoning, e.g., incorporating interactive learning, real-time
feedback loops, and environment-aware objectives for training. The benchmark is
publicly available at: https://github.com/tmlr-group/AR-Bench.

</details>


### [109] [Reinforce LLM Reasoning through Multi-Agent Reflection](https://arxiv.org/abs/2506.08379)
*Yurun Yuan,Tengyang Xie*

Main category: cs.LG

TL;DR: 提出DPSDP强化学习算法，通过多轮迭代优化提升大语言模型推理能力


<details>
  <summary>Details</summary>
Motivation: 现有verify-and-improve范式存在反馈空间受限、多方协作训练不足的问题

Method: 将多轮优化建模为马尔可夫决策过程，开发基于actor-critic架构的强化学习算法DPSDP

Result: 在MATH 500基准测试中通过五步迭代将首轮准确率从58.2%提升至63.2%，消融实验验证多智能体协作优势

Conclusion: DPSDP算法通过动态规划直接策略搜索实现跨分布泛化，有效提升大模型推理性能

Abstract: Leveraging more test-time computation has proven to be an effective way to
boost the reasoning capabilities of large language models (LLMs). Among various
methods, the verify-and-improve paradigm stands out for enabling dynamic
solution exploration and feedback incorporation. However, existing approaches
often suffer from restricted feedback spaces and lack of coordinated training
of different parties, leading to suboptimal performance. To address this, we
model this multi-turn refinement process as a Markov Decision Process and
introduce DPSDP (Direct Policy Search by Dynamic Programming), a reinforcement
learning algorithm that trains an actor-critic LLM system to iteratively refine
answers via direct preference learning on self-generated data. Theoretically,
DPSDP can match the performance of any policy within the training distribution.
Empirically, we instantiate DPSDP with various base models and show
improvements on both in- and out-of-distribution benchmarks. For example, on
benchmark MATH 500, majority voting over five refinement steps increases
first-turn accuracy from 58.2% to 63.2% with Ministral-based models. An
ablation study further confirms the benefits of multi-agent collaboration and
out-of-distribution generalization.

</details>


### [110] [Reinforcement Learning Teachers of Test Time Scaling](https://arxiv.org/abs/2506.08388)
*Edoardo Cetin,Tianyu Zhao,Yujin Tang*

Main category: cs.LG

TL;DR: 提出强化学习教师模型（RLT）框架，通过针对学生模型蒸馏优化的密集奖励机制，克服传统RL训练探索难题，实现小模型超越大模型的下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习训练推理语言模型需要模型具备初始探索能力，且主要作为蒸馏教师而非部署模型。因此需要开发专门针对提升学生模型理解效率的教师模型。

Method: 设计强化学习教师（RLT），输入问题与答案，生成面向学生模型的理解导向解释，通过学生反馈构建密集奖励函数进行训练。

Result: 7B参数的RLT在竞赛级任务上超越大模型蒸馏流程，支持训练更大规模学生模型，并在分布外任务上保持有效性。

Conclusion: RLT框架通过教师-学生协同优化，显著提升语言模型训练效率与知识迁移能力，为后续模型迭代提供可扩展解决方案。

Abstract: Training reasoning language models (LMs) with reinforcement learning (RL) for
one-hot correctness inherently relies on the LM being able to explore and solve
its task with some chance at initialization. Furthermore, a key use case of
reasoning LMs is to act as teachers for distilling new students and
cold-starting future RL iterations rather than being deployed themselves. From
these considerations, we introduce a new framework that avoids RL's exploration
challenge by training a new class of Reinforcement-Learned Teachers (RLTs)
focused on yielding the most effective downstream distillation. RLTs are
prompted with both the question and solution to each problem, and tasked to
simply "connect-the-dots" with detailed explanations tailored for their
students. We train RLTs with dense rewards obtained by feeding each explanation
to the student and testing its understanding of the problem's solution. In
practice, the raw outputs of a 7B RLT provide higher final performance on
competition and graduate-level tasks than existing distillation and
cold-starting pipelines that collect and postprocess the reasoning traces of
orders of magnitude larger LMs. Furthermore, RLTs maintain their effectiveness
when training larger students and when applied zero-shot to out-of-distribution
tasks, unlocking new levels of efficiency and re-usability for the RL reasoning
framework.

</details>


### [111] [The Geometries of Truth Are Orthogonal Across Tasks](https://arxiv.org/abs/2506.08572)
*Waiss Azizian,Michael Kirchhof,Eugene Ndiaye,Louis Bethune,Michal Klein,Pierre Ablin,Marco Cuturi*

Main category: cs.LG

TL;DR: 研究发现基于激活向量分析的'真理几何'方法存在跨任务迁移局限性，线性分类器在不同任务间无法通用


<details>
  <summary>Details</summary>
Motivation: 针对LLMs可靠性评估中'真理几何'方法被过度泛化使用的问题，揭示其任务依赖性本质缺陷

Method: 通过跨任务线性分类器对比分析，结合稀疏正则化实验，验证激活向量簇的任务特异性

Result: 不同任务的分类器相似性极低（稀疏正则化后支持集几乎不相交），激活向量形成任务分离簇

Conclusion: '真理几何'评估方法存在根本性任务限制，暗示需要开发更通用的可靠性评估框架

Abstract: Large Language Models (LLMs) have demonstrated impressive generalization
capabilities across various tasks, but their claim to practical relevance is
still mired by concerns on their reliability. Recent works have proposed
examining the activations produced by an LLM at inference time to assess
whether its answer to a question is correct. Some works claim that a "geometry
of truth" can be learned from examples, in the sense that the activations that
generate correct answers can be distinguished from those leading to mistakes
with a linear classifier. In this work, we underline a limitation of these
approaches: we observe that these "geometries of truth" are intrinsically
task-dependent and fail to transfer across tasks. More precisely, we show that
linear classifiers trained across distinct tasks share little similarity and,
when trained with sparsity-enforcing regularizers, have almost disjoint
supports. We show that more sophisticated approaches (e.g., using mixtures of
probes and tasks) fail to overcome this limitation, likely because activation
vectors commonly used to classify answers form clearly separated clusters when
examined across tasks.

</details>


### [112] [SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement Learning for LLM Reasoning](https://arxiv.org/abs/2506.08989)
*Xiao Liang,Zhong-Zhi Li,Yeyun Gong,Yang Wang,Hengyuan Zhang,Yelong Shen,Ying Nian Wu,Weizhu Chen*

Main category: cs.LG

TL;DR: 提出自我感知弱点驱动问题合成框架(SwS)，通过识别模型弱点并针对性合成训练问题，显著提升推理任务性能


<details>
  <summary>Details</summary>
Motivation: 现有RL训练数据集存在质量不足、问题针对性差、合成策略不考虑模型能力等问题，导致训练效率低下

Method: 定义模型持续失败问题为弱点→提取核心概念→合成增强问题→通过弱点聚焦的增量训练逐步克服缺陷

Result: 在7B/32B模型上实现平均10.0%/7.7%性能提升，覆盖8个主流推理基准

Conclusion: 无需外部知识蒸馏的自我弱点识别机制，使模型自主强化薄弱环节，实现更优泛化能力

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective
for training large language models (LLMs) on complex reasoning tasks, such as
mathematical problem solving. A prerequisite for the scalability of RLVR is a
high-quality problem set with precise and verifiable answers. However, the
scarcity of well-crafted human-labeled math problems and limited-verification
answers in existing distillation-oriented synthetic datasets limit their
effectiveness in RL. Additionally, most problem synthesis strategies
indiscriminately expand the problem set without considering the model's
capabilities, leading to low efficiency in generating useful questions. To
mitigate this issue, we introduce a Self-aware Weakness-driven problem
Synthesis framework (SwS) that systematically identifies model deficiencies and
leverages them for problem augmentation. Specifically, we define weaknesses as
questions that the model consistently fails to learn through its iterative
sampling during RL training. We then extract the core concepts from these
failure cases and synthesize new problems to strengthen the model's weak areas
in subsequent augmented training, enabling it to focus on and gradually
overcome its weaknesses. Without relying on external knowledge distillation,
our framework enables robust generalization byempowering the model to
self-identify and address its weaknesses in RL, yielding average performance
gains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning
benchmarks.

</details>


### [113] [e3: Learning to Explore Enables Extrapolation of Test-Time Compute for LLMs](https://arxiv.org/abs/2506.09026)
*Amrith Setlur,Matthew Y. R. Yang,Charlie Snell,Jeremy Greer,Ian Wu,Virginia Smith,Max Simchowitz,Aviral Kumar*

Main category: cs.LG

TL;DR: 通过测试时扩展（e3方法）提升LLM推理能力，实现训练预算外双倍推理时长下的性能外推


<details>
  <summary>Details</summary>
Motivation: 现有推理模型在超出训练时最大token预算的长时间推理场景中表现不佳，需要探索更有效的测试时计算资源利用方式

Method: 1) 链式技能组合（验证+生成不对称能力） 2) 基于错误轨迹的负梯度强化学习 3) 任务难度与训练token预算匹配的课程学习

Result: 1.7B模型在AIME/HMMT基准取得最佳成绩，pass@k指标提升，支持双倍训练token预算外推

Conclusion: 通过结构化上下文探索机制，成功突破LLM推理时长限制，为测试时计算资源利用提供新范式

Abstract: Test-time scaling offers a promising path to improve LLM reasoning by
utilizing more compute at inference time; however, the true promise of this
paradigm lies in extrapolation (i.e., improvement in performance on hard
problems as LLMs keep "thinking" for longer, beyond the maximum token budget
they were trained on). Surprisingly, we find that most existing reasoning
models do not extrapolate well. We show that one way to enable extrapolation is
by training the LLM to perform in-context exploration: training the LLM to
effectively spend its test time budget by chaining operations (such as
generation, verification, refinement, etc.), or testing multiple hypotheses
before it commits to an answer. To enable in-context exploration, we identify
three key ingredients as part of our recipe e3: (1) chaining skills that the
base LLM has asymmetric competence in, e.g., chaining verification (easy) with
generation (hard), as a way to implement in-context search; (2) leveraging
"negative" gradients from incorrect traces to amplify exploration during RL,
resulting in longer search traces that chains additional asymmetries; and (3)
coupling task difficulty with training token budget during training via a
specifically-designed curriculum to structure in-context exploration. Our
recipe e3 produces the best known 1.7B model according to AIME'25 and HMMT'25
scores, and extrapolates to 2x the training token budget. Our e3-1.7B model not
only attains high pass@1 scores, but also improves pass@k over the base model.

</details>
