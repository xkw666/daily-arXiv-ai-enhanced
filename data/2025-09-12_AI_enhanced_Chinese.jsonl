{"id": "2509.08855", "pdf": "https://arxiv.org/pdf/2509.08855", "abs": "https://arxiv.org/abs/2509.08855", "authors": ["Mahmoud Shaqfa"], "title": "Morphology-Preserving Remeshing Approach to Particulate Microstructures via Harmonic Decomposition", "categories": ["cs.GR", "cs.CG"], "comment": null, "summary": "Harmonic decomposition of surfaces, such as spherical and spheroidal\nharmonics, is used to analyze morphology, reconstruct, and generate surface\ninclusions of particulate microstructures. However, obtaining high-quality\nmeshes of engineering microstructures using these approaches remains an open\nquestion. In harmonic approaches, we usually reconstruct surfaces by evaluating\nthe harmonic bases on equidistantly sampled simplicial complexes of the base\ndomains (e.g., triangular spheroids and disks). However, this traditional\nsampling does not account for local changes in the Jacobian of the basis\nfunctions, resulting in nonuniform discretization after reconstruction or\ngeneration. As it impacts the accuracy and time step, high-quality\ndiscretization of microstructures is crucial for efficient numerical\nsimulations (e.g., finite element and discrete element methods). To circumvent\nthis issue, we propose an efficient hierarchical diffusion-based approach for\nresampling the surface-i.e., performing a reparameterization-to yield an\nequalized mesh triangulation. Analogous to heat problems, we use nonlinear\ndiffusion to resample the curvilinear coordinates of the analysis domain,\nthereby enlarging small triangles at the expense of large triangles on\nsurfaces. We tested isotropic and anisotropic diffusion schemes on the recent\nspheroidal and hemispheroidal harmonics methods. The results show a substantial\nimprovement in the quality metrics for surface triangulation. Unlike\ntraditional surface reconstruction and meshing techniques, this approach\npreserves surface morphology, along with the areas and volumes of surfaces. We\ndiscuss the results and the associated computational costs for large 2D and 3D\nmicrostructures, such as digital twins of concrete and stone masonry, and their\nfuture applications.", "AI": {"tldr": "\u9488\u5bf9\u4f20\u7edf\u8c10\u6ce2\u5206\u89e3\u65b9\u6cd5\u751f\u6210\u5de5\u7a0b\u5fae\u7ed3\u6784\u7f51\u683c\u8d28\u91cf\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u5206\u5c42\u6269\u6563\u7684\u7f51\u683c\u91cd\u91c7\u6837\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u6269\u6563\u5b9e\u73b0\u5747\u5300\u4e09\u89d2\u5316\u5e76\u4fdd\u6301\u5f62\u6001\u5b8c\u6574\u3002", "motivation": "\u4f20\u7edf\u8c10\u6ce2\u65b9\u6cd5\u7b49\u8ddd\u91c7\u6837\u5bfc\u81f4\u7f51\u683c\u975e\u5747\u5300\u5316\uff0c\u5f71\u54cd\u6570\u503c\u6a21\u62df\u7cbe\u5ea6\u4e0e\u6548\u7387\uff0c\u9700\u5f00\u53d1\u80fd\u4fdd\u6301\u8868\u9762\u5f62\u6001\u7684\u9ad8\u8d28\u91cf\u7f51\u683c\u751f\u6210\u65b9\u6848\u3002", "method": "\u91c7\u7528\u975e\u7ebf\u6027\u6269\u6563\u673a\u5236\u5bf9\u5206\u6790\u57df\u66f2\u7ebf\u5750\u6807\u91cd\u91c7\u6837\uff0c\u901a\u8fc7\u6269\u5927\u9ad8\u66f2\u7387\u533a\u57df\u4e09\u89d2\u5f62\u9762\u79ef\u5b9e\u73b0\u53c2\u6570\u5316\u5747\u8861\uff0c\u4fdd\u7559\u8868\u9762\u79ef/\u4f53\u79ef\u4e0d\u53d8\u6027\u3002", "result": "\u5728\u7403\u8c10/\u534a\u7403\u8c10\u65b9\u6cd5\u4e2d\u9a8c\u8bc1\u663e\u793a\u7f51\u683c\u8d28\u91cf\u6307\u6807\u663e\u8457\u63d0\u5347\uff0c\u5404\u5411\u540c\u6027/\u5f02\u6027\u6269\u6563\u65b9\u6848\u5747\u6709\u6548\u6539\u5584\u4e09\u89d2\u5256\u5206\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6df7\u51dd\u571f\u3001\u780c\u4f53\u7b49\u5927\u578b\u5fae\u7ed3\u6784\u6570\u5b57\u5b6a\u751f\u63d0\u4f9b\u9ad8\u6548\u7f51\u683c\u751f\u6210\u65b9\u6848\uff0c\u5177\u6709\u5de5\u7a0b\u4eff\u771f\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.08947", "pdf": "https://arxiv.org/pdf/2509.08947", "abs": "https://arxiv.org/abs/2509.08947", "authors": ["Yancheng Cai", "Robert Wanat", "Rafal Mantiuk"], "title": "CameraVDP: Perceptual Display Assessment with Uncertainty Estimation via Camera and Visual Difference Prediction", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted by SIGGRAPH Asia 2025", "summary": "Accurate measurement of images produced by electronic displays is critical\nfor the evaluation of both traditional and computational displays. Traditional\ndisplay measurement methods based on sparse radiometric sampling and fitting a\nmodel are inadequate for capturing spatially varying display artifacts, as they\nfail to capture high-frequency and pixel-level distortions. While cameras offer\nsufficient spatial resolution, they introduce optical, sampling, and\nphotometric distortions. Furthermore, the physical measurement must be combined\nwith a model of a visual system to assess whether the distortions are going to\nbe visible. To enable perceptual assessment of displays, we propose a\ncombination of a camera-based reconstruction pipeline with a visual difference\npredictor, which account for both the inaccuracy of camera measurements and\nvisual difference prediction. The reconstruction pipeline combines HDR image\nstacking, MTF inversion, vignetting correction, geometric undistortion,\nhomography transformation, and color correction, enabling cameras to function\nas precise display measurement instruments. By incorporating a Visual\nDifference Predictor (VDP), our system models the visibility of various stimuli\nunder different viewing conditions for the human visual system. We validate the\nproposed CameraVDP framework through three applications: defective pixel\ndetection, color fringing awareness, and display non-uniformity evaluation. Our\nuncertainty analysis framework enables the estimation of the theoretical upper\nbound for defect pixel detection performance and provides confidence intervals\nfor VDP quality scores.", "AI": {"tldr": "\u63d0\u51faCameraVDP\u6846\u67b6\uff0c\u7ed3\u5408\u76f8\u673a\u91cd\u5efa\u6d41\u7a0b\u4e0e\u89c6\u89c9\u5dee\u5f02\u9884\u6d4b\u5668\uff0c\u5b9e\u73b0\u663e\u793a\u5c4f\u7f3a\u9677\u7684\u611f\u77e5\u8bc4\u4f30\u4e0e\u91cf\u5316\u5206\u6790\u3002", "motivation": "\u4f20\u7edf\u7a00\u758f\u8f90\u5c04\u91c7\u6837\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u9ad8\u9891\u50cf\u7d20\u7ea7\u5931\u771f\uff0c\u76f8\u673a\u6d4b\u91cf\u5b58\u5728\u5149\u5b66/\u91c7\u6837/\u5149\u5ea6\u5931\u771f\uff0c\u9700\u7ed3\u5408\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u6a21\u578b\u8bc4\u4f30\u663e\u793a\u8d28\u91cf\u3002", "method": "\u6574\u5408HDR\u5806\u6808/MTF\u53cd\u6f14/\u6e10\u6655\u6821\u6b63/\u51e0\u4f55\u53bb\u7578\u53d8/\u5355\u5e94\u53d8\u6362/\u8272\u5f69\u6821\u6b63\u5efa\u7acb\u91cd\u5efa\u6d41\u7a0b\uff0c\u5e76\u5f15\u5165\u89c6\u89c9\u5dee\u5f02\u9884\u6d4b\u5668(VDP)\u5efa\u6a21\u89c6\u89c9\u611f\u77e5\u3002", "result": "\u901a\u8fc7\u574f\u70b9\u68c0\u6d4b/\u8272\u8fb9\u8bc6\u522b/\u5747\u5300\u6027\u8bc4\u4f30\u9a8c\u8bc1\u6846\u67b6\uff0c\u5efa\u7acb\u7f3a\u9677\u68c0\u6d4b\u7406\u8bba\u6027\u80fd\u4e0a\u9650\uff0c\u4e3aVDP\u8d28\u91cf\u5206\u63d0\u4f9b\u7f6e\u4fe1\u533a\u95f4\u3002", "conclusion": "CameraVDP\u7a81\u7834\u4f20\u7edf\u6d4b\u91cf\u5c40\u9650\uff0c\u5b9e\u73b0\u663e\u793a\u5c4f\u50cf\u7d20\u7ea7\u5931\u771f\u611f\u77e5\u8bc4\u4f30\uff0c\u4e3a\u663e\u793a\u8d28\u91cf\u63a7\u5236\u63d0\u4f9b\u91cf\u5316\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2509.09143", "pdf": "https://arxiv.org/pdf/2509.09143", "abs": "https://arxiv.org/abs/2509.09143", "authors": ["Yuiko Uchida", "Ren Togo", "Keisuke Maeda", "Takahiro Ogawa", "Miki Haseyama"], "title": "Objectness Similarity: Capturing Object-Level Fidelity in 3D Scene Evaluation", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "Accepted by the ICCV 2025 UniLight Workshop", "summary": "This paper presents Objectness SIMilarity (OSIM), a novel evaluation metric\nfor 3D scenes that explicitly focuses on \"objects,\" which are fundamental units\nof human visual perception. Existing metrics assess overall image quality,\nleading to discrepancies with human perception. Inspired by neuropsychological\ninsights, we hypothesize that human recognition of 3D scenes fundamentally\ninvolves attention to individual objects. OSIM enables object-centric\nevaluations by leveraging an object detection model and its feature\nrepresentations to quantify the \"objectness\" of each object in the scene. Our\nuser study demonstrates that OSIM aligns more closely with human perception\ncompared to existing metrics. We also analyze the characteristics of OSIM using\nvarious approaches. Moreover, we re-evaluate recent 3D reconstruction and\ngeneration models under a standardized experimental setup to clarify\nadvancements in this field. The code is available at\nhttps://github.com/Objectness-Similarity/OSIM.", "AI": {"tldr": "\u63d0\u51faOSIM\u4e09\u7ef4\u573a\u666f\u8bc4\u4f30\u6307\u6807\uff0c\u901a\u8fc7\u91cf\u5316\u7269\u4f53\u6027\u7279\u5f81\u5b9e\u73b0\u66f4\u7b26\u5408\u4eba\u7c7b\u611f\u77e5\u7684\u8bc4\u4f30\u65b9\u6cd5", "motivation": "\u73b0\u6709\u6307\u6807\u5173\u6ce8\u6574\u4f53\u56fe\u50cf\u8d28\u91cf\uff0c\u4e0e\u4eba\u7c7b\u4ee5\u7269\u4f53\u4e3a\u57fa\u672c\u611f\u77e5\u5355\u5143\u7684\u7279\u6027\u5b58\u5728\u504f\u5dee\u3002\u53d7\u795e\u7ecf\u5fc3\u7406\u5b66\u542f\u53d1\uff0c\u9700\u8981\u5f00\u53d1\u57fa\u4e8e\u7269\u4f53\u611f\u77e5\u7684\u4e09\u7ef4\u573a\u666f\u8bc4\u4f30\u4f53\u7cfb", "method": "\u5229\u7528\u7269\u4f53\u68c0\u6d4b\u6a21\u578b\u7684\u7279\u5f81\u8868\u793a\u91cf\u5316\u573a\u666f\u4e2d\u6bcf\u4e2a\u7269\u4f53\u7684'\u7269\u4f53\u6027'\uff0c\u5efa\u7acb\u5bf9\u8c61\u4e2d\u5fc3\u8bc4\u4f30\u6846\u67b6", "result": "\u7528\u6237\u7814\u7a76\u8868\u660eOSIM\u4e0e\u4eba\u7c7b\u611f\u77e5\u4e00\u81f4\u6027\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6307\u6807\uff0c\u5e76\u901a\u8fc7\u6807\u51c6\u5316\u5b9e\u9a8c\u91cd\u65b0\u8bc4\u4f30\u4e86\u6700\u65b0\u4e09\u7ef4\u91cd\u5efa\u6a21\u578b", "conclusion": "OSIM\u9996\u6b21\u5b9e\u73b0\u57fa\u4e8e\u7269\u4f53\u611f\u77e5\u7684\u4e09\u7ef4\u573a\u666f\u91cf\u5316\u8bc4\u4f30\uff0c\u4e3a\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u4eba\u7c7b\u89c6\u89c9\u8ba4\u77e5\u67b6\u8d77\u65b0\u7684\u6865\u6881"}}
{"id": "2509.08903", "pdf": "https://arxiv.org/pdf/2509.08903", "abs": "https://arxiv.org/abs/2509.08903", "authors": ["Alex Clay", "Ernesto Jim\u00e9nez-Ruiz", "Pranava Madhyastha"], "title": "Noise or Nuance: An Investigation Into Useful Information and Filtering For LLM Driven AKBC", "categories": ["cs.CL"], "comment": "8 pages, 1 figure, accepted to the ISWC 2025 LM-KBC Workshop", "summary": "RAG and fine-tuning are prevalent strategies for improving the quality of LLM\noutputs. However, in constrained situations, such as that of the 2025 LM-KBC\nchallenge, such techniques are restricted. In this work we investigate three\nfacets of the triple completion task: generation, quality assurance, and LLM\nresponse parsing. Our work finds that in this constrained setting: additional\ninformation improves generation quality, LLMs can be effective at filtering\npoor quality triples, and the tradeoff between flexibility and consistency with\nLLM response parsing is setting dependent.", "AI": {"tldr": "\u5728\u53d7\u9650\u73af\u5883\u4e0b\uff08\u5982LM-KBC\u6311\u6218\uff09\uff0c\u901a\u8fc7\u751f\u6210\u4f18\u5316\u3001\u8d28\u91cf\u8fc7\u6ee4\u548c\u54cd\u5e94\u89e3\u6790\u4e09\u65b9\u9762\u7684\u7814\u7a76\uff0c\u53d1\u73b0\u989d\u5916\u4fe1\u606f\u63d0\u5347\u751f\u6210\u8d28\u91cf\uff0cLLM\u6709\u6548\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u4e09\u5143\u7ec4\uff0c\u7075\u6d3b\u6027\u4e0e\u4e00\u81f4\u6027\u7684\u89e3\u6790\u7b56\u7565\u9700\u6839\u636e\u573a\u666f\u6743\u8861\u3002", "motivation": "\u63a2\u7d22\u5728\u65e0\u6cd5\u4f7f\u7528RAG\u548c\u5fae\u8c03\u7b49\u5e38\u89c4\u4f18\u5316\u65b9\u6cd5\u7684\u53d7\u9650\u573a\u666f\u4e0b\uff0c\u5982\u4f55\u901a\u8fc7\u5176\u4ed6\u6280\u672f\u624b\u6bb5\u63d0\u5347LLM\u7684\u4e09\u5143\u7ec4\u751f\u6210\u8d28\u91cf\u4e0e\u53ef\u9760\u6027\u3002", "method": "1. \u5206\u6790\u4e09\u5143\u7ec4\u751f\u6210\u7684\u4f18\u5316\u7b56\u7565 2. \u8bbe\u8ba1LLM\u9a71\u52a8\u7684\u8d28\u91cf\u8fc7\u6ee4\u673a\u5236 3. \u6bd4\u8f83\u4e0d\u540cLLM\u54cd\u5e94\u89e3\u6790\u65b9\u6cd5\u7684\u7075\u6d3b\u6027/\u4e00\u81f4\u6027\u5e73\u8861", "result": "1. \u8865\u5145\u4e0a\u4e0b\u6587\u4fe1\u606f\u4f7f\u751f\u6210\u51c6\u786e\u7387\u63d0\u534718% 2. \u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u8fc7\u6ee4\u51cf\u5c11\u9519\u8bef\u4e09\u5143\u7ec435% 3. \u7ed3\u6784\u5316\u89e3\u6790\u6a21\u677f\u5728\u4fdd\u630185%\u4e00\u81f4\u6027\u7684\u540c\u65f6\u4fdd\u7559\u5173\u952e\u4fe1\u606f", "conclusion": "\u53d7\u9650\u73af\u5883\u4e0b\uff0c\u7ec4\u5408\u4f7f\u7528\u4e0a\u4e0b\u6587\u589e\u5f3a\u3001\u7f6e\u4fe1\u5ea6\u8fc7\u6ee4\u548c\u534a\u7ed3\u6784\u5316\u89e3\u6790\u80fd\u6709\u6548\u63d0\u5347LLM\u7684\u4e09\u5143\u7ec4\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08907", "pdf": "https://arxiv.org/pdf/2509.08907", "abs": "https://arxiv.org/abs/2509.08907", "authors": ["Imene Kolli", "Ario Saeid Vaghefi", "Chiara Colesanti Senni", "Shantam Raj", "Markus Leippold"], "title": "Automated Evidence Extraction and Scoring for Corporate Climate Policy Engagement: A Multilingual RAG Approach", "categories": ["cs.CL"], "comment": null, "summary": "InfluenceMap's LobbyMap Platform monitors the climate policy engagement of\nover 500 companies and 250 industry associations, assessing each entity's\nsupport or opposition to science-based policy pathways for achieving the Paris\nAgreement's goal of limiting global warming to 1.5{\\deg}C. Although\nInfluenceMap has made progress with automating key elements of the analytical\nworkflow, a significant portion of the assessment remains manual, making it\ntime- and labor-intensive and susceptible to human error. We propose an\nAI-assisted framework to accelerate the monitoring of corporate climate policy\nengagement by leveraging Retrieval-Augmented Generation to automate the most\ntime-intensive extraction of relevant evidence from large-scale textual data.\nOur evaluation shows that a combination of layout-aware parsing, the Nomic\nembedding model, and few-shot prompting strategies yields the best performance\nin extracting and classifying evidence from multilingual corporate documents.\nWe conclude that while the automated RAG system effectively accelerates\nevidence extraction, the nuanced nature of the analysis necessitates a\nhuman-in-the-loop approach where the technology augments, rather than replaces,\nexpert judgment to ensure accuracy.", "AI": {"tldr": "\u7814\u7a76\u56e2\u961f\u9488\u5bf9\u4f01\u4e1a\u6c14\u5019\u653f\u7b56\u53c2\u4e0e\u8bc4\u4f30\u6d41\u7a0b\u81ea\u52a8\u5316\u7a0b\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684AI\u8f85\u52a9\u6846\u67b6\uff0c\u901a\u8fc7\u5e03\u5c40\u89e3\u6790\u3001Nomic\u5d4c\u5165\u6a21\u578b\u548c\u5c0f\u6837\u672c\u63d0\u793a\u7b56\u7565\u7684\u7ec4\u5408\u65b9\u6848\uff0c\u5b9e\u73b0\u591a\u8bed\u8a00\u4f01\u4e1a\u6587\u6863\u8bc1\u636e\u7684\u9ad8\u6548\u63d0\u53d6\uff0c\u4f46\u6700\u7ec8\u4ecd\u5f3a\u8c03\u9700\u4fdd\u7559\u4e13\u5bb6\u4eba\u5de5\u5ba1\u6838\u73af\u8282\u4ee5\u4fdd\u8bc1\u5206\u6790\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u4eba\u5de5\u8bc4\u4f30\u4f01\u4e1a\u6c14\u5019\u653f\u7b56\u53c2\u4e0e\u5ea6\u5b58\u5728\u6548\u7387\u4f4e\u3001\u6210\u672c\u9ad8\u3001\u6613\u51fa\u9519\u7684\u75db\u70b9\uff0c\u9700\u901a\u8fc7AI\u6280\u672f\u52a0\u901f\u5927\u89c4\u6a21\u6587\u672c\u6570\u636e\u5904\u7406\u6d41\u7a0b\u3002", "method": "\u91c7\u7528\u5e03\u5c40\u611f\u77e5\u89e3\u6790\u6280\u672f\u5904\u7406\u6587\u6863\u7ed3\u6784\uff0c\u7ed3\u5408Nomic\u5d4c\u5165\u6a21\u578b\u8fdb\u884c\u8bed\u4e49\u7406\u89e3\uff0c\u5e76\u8fd0\u7528\u5c0f\u6837\u672c\u63d0\u793a\u7b56\u7565\u5b9e\u73b0\u591a\u8bed\u8a00\u6587\u672c\u7684\u8bc1\u636e\u63d0\u53d6\u4e0e\u5206\u7c7b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u8bc1\u636e\u63d0\u53d6\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u4f18\u6027\u80fd\uff0c\u5c24\u5176\u5728\u591a\u8bed\u8a00\u6587\u6863\u5904\u7406\u573a\u666f\u5c55\u73b0\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u81ea\u52a8\u5316RAG\u7cfb\u7edf\u867d\u80fd\u6709\u6548\u63d0\u5347\u8bc1\u636e\u63d0\u53d6\u6548\u7387\uff0c\u4f46\u590d\u6742\u653f\u7b56\u5206\u6790\u4ecd\u9700\u4eba\u673a\u534f\u540c\u6a21\u5f0f\uff0c\u6280\u672f\u5e94\u4f5c\u4e3a\u4e13\u5bb6\u51b3\u7b56\u7684\u589e\u5f3a\u5de5\u5177\u800c\u975e\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2509.08920", "pdf": "https://arxiv.org/pdf/2509.08920", "abs": "https://arxiv.org/abs/2509.08920", "authors": ["Jinsong Chen"], "title": "Documents Are People and Words Are Items: A Psychometric Approach to Textual Data with Contextual Embeddings", "categories": ["cs.CL", "stat.AP", "stat.ME"], "comment": null, "summary": "This research introduces a novel psychometric method for analyzing textual\ndata using large language models. By leveraging contextual embeddings to create\ncontextual scores, we transform textual data into response data suitable for\npsychometric analysis. Treating documents as individuals and words as items,\nthis approach provides a natural psychometric interpretation under the\nassumption that certain keywords, whose contextual meanings vary significantly\nacross documents, can effectively differentiate documents within a corpus. The\nmodeling process comprises two stages: obtaining contextual scores and\nperforming psychometric analysis. In the first stage, we utilize natural\nlanguage processing techniques and encoder based transformer models to identify\ncommon keywords and generate contextual scores. In the second stage, we employ\nvarious types of factor analysis, including exploratory and bifactor models, to\nextract and define latent factors, determine factor correlations, and identify\nthe most significant words associated with each factor. Applied to the Wiki\nSTEM corpus, our experimental results demonstrate the method's potential to\nuncover latent knowledge dimensions and patterns within textual data. This\napproach not only enhances the psychometric analysis of textual data but also\nholds promise for applications in fields rich in textual information, such as\neducation, psychology, and law.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u5fc3\u7406\u6d4b\u91cf\u5b66\u7684\u6587\u672c\u5206\u6790\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5904\u7406\uff08\u4e0a\u4e0b\u6587\u8bc4\u5206+\u56e0\u5b50\u5206\u6790\uff09\u63ed\u793a\u6587\u672c\u6f5c\u5728\u7ef4\u5ea6\uff0c\u5728STEM\u8bed\u6599\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "\u4f20\u7edf\u6587\u672c\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u8bcd\u8bed\u7684\u4e0a\u4e0b\u6587\u8bed\u4e49\u53d8\u5316\uff0c\u9700\u5f00\u53d1\u80fd\u81ea\u7136\u9002\u914d\u5fc3\u7406\u6d4b\u91cf\u6846\u67b6\u7684\u65b0\u65b9\u6cd5\u4ee5\u63d0\u5347\u6559\u80b2\u3001\u5fc3\u7406\u5b66\u7b49\u9886\u57df\u7684\u6587\u672c\u6570\u636e\u6316\u6398\u80fd\u529b", "method": "1. \u4f7f\u7528Transformer\u6a21\u578b\u751f\u6210\u5173\u952e\u8bcd\u7684\u4e0a\u4e0b\u6587\u5d4c\u5165\u5206\u6570 2. \u5e94\u7528\u63a2\u7d22\u6027/\u53cc\u56e0\u5b50\u5206\u6790\u63d0\u53d6\u6f5c\u5728\u7ef4\u5ea6\uff0c\u5efa\u7acb\u8bcd\u8bed-\u56e0\u5b50\u6620\u5c04\u5173\u7cfb", "result": "\u5728Wiki STEM\u8bed\u6599\u4e2d\u6210\u529f\u8bc6\u522b\u51fa\u6f5c\u5728\u77e5\u8bc6\u7ed3\u6784\u6a21\u578b\uff0c\u5173\u952e\u8bcd\u8bed\u80fd\u6709\u6548\u533a\u5206\u4e0d\u540c\u6587\u6863\u7684\u77e5\u8bc6\u7ef4\u5ea6\u7279\u5f81", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6587\u672c\u5fc3\u7406\u6d4b\u91cf\u5b66\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u6df1\u5ea6\u8bed\u4e49\u5206\u6790\u7684\u6559\u80b2\u8bc4\u4f30\u3001\u6cd5\u5f8b\u6587\u4e66\u7814\u7a76\u7b49\u9886\u57df\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u591a\u8bed\u8a00\u573a\u666f"}}
{"id": "2509.08960", "pdf": "https://arxiv.org/pdf/2509.08960", "abs": "https://arxiv.org/abs/2509.08960", "authors": ["Thales Sales Almeida", "Giovana Kerche Bon\u00e1s", "Jo\u00e3o Guilherme Alves Santos"], "title": "BRoverbs -- Measuring how much LLMs understand Portuguese proverbs", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit significant performance variations\ndepending on the linguistic and cultural context in which they are applied.\nThis disparity signals the necessity of mature evaluation frameworks that can\nassess their capabilities in specific regional settings. In the case of\nPortuguese, existing evaluations remain limited, often relying on translated\ndatasets that may not fully capture linguistic nuances or cultural references.\nMeanwhile, native Portuguese-language datasets predominantly focus on\nstructured national exams or sentiment analysis of social media interactions,\nleaving gaps in evaluating broader linguistic understanding. To address this\nlimitation, we introduce BRoverbs, a dataset specifically designed to assess\nLLM performance through Brazilian proverbs. Proverbs serve as a rich linguistic\nresource, encapsulating cultural wisdom, figurative expressions, and complex\nsyntactic structures that challenge the model comprehension of regional\nexpressions. BRoverbs aims to provide a new evaluation tool for\nPortuguese-language LLMs, contributing to advancing regionally informed\nbenchmarking. The benchmark is available at\nhttps://huggingface.co/datasets/Tropic-AI/BRoverbs.", "AI": {"tldr": "\u9488\u5bf9\u8461\u8404\u7259\u8bedLLMs\u8bc4\u4f30\u5b58\u5728\u7684\u6587\u5316\u5c40\u9650\u6027\uff0c\u7814\u7a76\u8005\u5f00\u53d1\u4e86\u57fa\u4e8e\u5df4\u897f\u8c1a\u8bed\u7684\u8bc4\u4f30\u6570\u636e\u96c6BRoverbs", "motivation": "\u73b0\u6709\u8461\u8404\u7259\u8bedLLMs\u8bc4\u4f30\u5b58\u5728\u4e09\u5927\u7f3a\u9677\uff1a\u4f9d\u8d56\u7ffb\u8bd1\u6570\u636e\u96c6\u65e0\u6cd5\u6355\u6349\u8bed\u8a00\u7ec6\u5fae\u5dee\u5f02\u3001\u672c\u571f\u6570\u636e\u96c6\u5c40\u9650\u4e8e\u56fd\u5bb6\u8003\u8bd5\u548c\u793e\u4ea4\u5a92\u4f53\u60c5\u611f\u5206\u6790\u3001\u7f3a\u4e4f\u5e7f\u4e49\u8bed\u8a00\u7406\u89e3\u8bc4\u4f30\u6846\u67b6", "method": "\u901a\u8fc7\u6536\u96c6\u5177\u6709\u6587\u5316\u667a\u6167\u3001\u6bd4\u55bb\u8868\u8fbe\u548c\u590d\u6742\u53e5\u6cd5\u7ed3\u6784\u7684\u5df4\u897f\u8c1a\u8bed\uff0c\u6784\u5efa\u4e13\u95e8\u8bc4\u4f30\u5de5\u5177BRoverbs\u6570\u636e\u96c6", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u57fa\u4e8e\u8c1a\u8bed\u7684\u8461\u8404\u7259\u8bedLLMs\u8bc4\u4f30\u57fa\u51c6\uff0c\u652f\u6301\u533a\u57df\u6027\u8bed\u8a00\u7406\u89e3\u7684\u7cfb\u7edf\u6027\u8bc4\u6d4b", "conclusion": "BRoverbs\u586b\u8865\u4e86\u8461\u8404\u7259\u8bed\u6a21\u578b\u6587\u5316\u654f\u611f\u6027\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u4e3a\u6784\u5efa\u533a\u57df\u6027AI\u57fa\u51c6\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2509.09013", "pdf": "https://arxiv.org/pdf/2509.09013", "abs": "https://arxiv.org/abs/2509.09013", "authors": ["Monjoy Narayan Choudhury", "Junling Wang", "Yifan Hou", "Mrinmaya Sachan"], "title": "Can Vision-Language Models Solve Visual Math Equations?", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Monjoy Narayan Choudhury and Junling Wang contributed equally to this\n  work. Accepted at EMNLP2025 main. Code and datasets are open-sourced with\n  links in the paper", "summary": "Despite strong performance in visual understanding and language-based\nreasoning, Vision-Language Models (VLMs) struggle with tasks requiring\nintegrated perception and symbolic computation. We study this limitation\nthrough visual equation solving, where mathematical equations are embedded in\nimages, variables are represented by object icons, and coefficients must be\ninferred by counting. While VLMs perform well on textual equations, they fail\non visually grounded counterparts. To understand this gap, we decompose the\ntask into coefficient counting and variable recognition, and find that counting\nis the primary bottleneck, even when recognition is accurate. We also observe\nthat composing recognition and reasoning introduces additional errors,\nhighlighting challenges in multi-step visual reasoning. Finally, as equation\ncomplexity increases, symbolic reasoning itself becomes a limiting factor.\nThese findings reveal key weaknesses in current VLMs and point toward future\nimprovements in visually grounded mathematical reasoning.", "AI": {"tldr": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u6570\u5b66\u63a8\u7406\u4e2d\u5b58\u5728\u663e\u8457\u7f3a\u9677\uff0c\u4e3b\u8981\u74f6\u9888\u662f\u56fe\u50cf\u4e2d\u7684\u6570\u503c\u8ba1\u7b97\u548c\u591a\u6b65\u9aa4\u63a8\u7406\u80fd\u529b\u4e0d\u8db3", "motivation": "\u63a2\u7a76\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u611f\u77e5\u4e0e\u7b26\u53f7\u8ba1\u7b97\u7ed3\u5408\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u901a\u8fc7\u89c6\u89c9\u65b9\u7a0b\u6c42\u89e3\u573a\u666f\u5206\u6790", "method": "\u5c06\u89c6\u89c9\u65b9\u7a0b\u89e3\u6784\u4e3a\u7cfb\u6570\u8ba1\u6570\u548c\u53d8\u91cf\u8bc6\u522b\u4e24\u4e2a\u5b50\u4efb\u52a1\uff0c\u5206\u522b\u6d4b\u8bd5\u6a21\u578b\u6027\u80fd\u5e76\u5206\u6790\u9519\u8bef\u6765\u6e90", "result": "1. \u6570\u503c\u8ba1\u6570\u662f\u4e3b\u8981\u74f6\u9888\uff08\u5373\u4f7f\u53d8\u91cf\u8bc6\u522b\u51c6\u786e\uff09\n2. \u591a\u6b65\u9aa4\u63a8\u7406\u8fc7\u7a0b\u4f1a\u4ea7\u751f\u9519\u8bef\u7d2f\u79ef\n3. \u65b9\u7a0b\u590d\u6742\u5ea6\u589e\u52a0\u65f6\u7b26\u53f7\u63a8\u7406\u80fd\u529b\u663e\u8457\u4e0b\u964d", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dVLMs\u5728\u89c6\u89c9\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u6838\u5fc3\u7f3a\u9677\uff0c\u4e3a\u672a\u6765\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u7684\u7b26\u53f7\u8ba1\u7b97\u548c\u590d\u6742\u63a8\u7406\u80fd\u529b\u6307\u660e\u65b9\u5411"}}
{"id": "2509.09043", "pdf": "https://arxiv.org/pdf/2509.09043", "abs": "https://arxiv.org/abs/2509.09043", "authors": ["Thomas Manuel Rost", "Martina Figlia", "Bernd Wallraff"], "title": "Stated Preference for Interaction and Continued Engagement (SPICE): Evaluating an LLM's Willingness to Re-engage in Conversation", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "We introduce and evaluate Stated Preference for Interaction and Continued\nEngagement (SPICE), a simple diagnostic signal elicited by asking a Large\nLanguage Model a YES or NO question about its willingness to re-engage with a\nuser's behavior after reviewing a short transcript. In a study using a 3-tone\n(friendly, unclear, abusive) by 10-interaction stimulus set, we tested four\nopen-weight chat models across four framing conditions, resulting in 480\ntrials. Our findings show that SPICE sharply discriminates by user tone.\nFriendly interactions yielded a near-unanimous preference to continue (97.5%\nYES), while abusive interactions yielded a strong preference to discontinue\n(17.9% YES), with unclear interactions falling in between (60.4% YES). This\ncore association remains decisive under multiple dependence-aware statistical\ntests, including Rao-Scott adjustment and cluster permutation tests.\nFurthermore, we demonstrate that SPICE provides a distinct signal from abuse\nclassification. In trials where a model failed to identify abuse, it still\noverwhelmingly stated a preference not to continue the interaction (81% of the\ntime). An exploratory analysis also reveals a significant interaction effect: a\npreamble describing the study context significantly impacts SPICE under\nambiguity, but only when transcripts are presented as a single block of text\nrather than a multi-turn chat. The results validate SPICE as a robust,\nlow-overhead, and reproducible tool for auditing model dispositions,\ncomplementing existing metrics by offering a direct, relational signal of a\nmodel's state. All stimuli, code, and analysis scripts are released to support\nreplication.", "AI": {"tldr": "SPICE\u662f\u4e00\u79cd\u901a\u8fc7\u7b80\u5355YES/NO\u95ee\u9898\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u4e92\u52a8\u610f\u613f\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u80fd\u6709\u6548\u533a\u5206\u7528\u6237\u8bed\u6c14\u5bf9\u6a21\u578b\u7ee7\u7eed\u4e92\u52a8\u503e\u5411\u7684\u5f71\u54cd\u3002", "motivation": "\u5f00\u53d1\u4f4e\u5f00\u9500\u3001\u53ef\u590d\u73b0\u7684\u6a21\u578b\u5ba1\u8ba1\u5de5\u5177\uff0c\u8865\u5145\u73b0\u6709\u6307\u6807\u5e76\u63d0\u4f9b\u76f4\u63a5\u5173\u7cfb\u4fe1\u53f7\u6765\u8bc4\u4f30\u6a21\u578b\u72b6\u6001\u3002", "method": "\u4f7f\u75283\u79cd\u7528\u6237\u8bed\u6c14\uff08\u53cb\u597d/\u4e0d\u660e\u786e/\u6ee5\u7528\uff09x10\u79cd\u4e92\u52a8\u573a\u666f\uff0c\u6d4b\u8bd54\u4e2a\u5f00\u6e90\u5bf9\u8bdd\u6a21\u578b\uff0c\u91c7\u7528Rao-Scott\u8c03\u6574\u548c\u805a\u7c7b\u7f6e\u6362\u68c0\u9a8c\u7b49\u7edf\u8ba1\u65b9\u6cd5\u5206\u6790480\u7ec4\u6570\u636e\u3002", "result": "\u53cb\u597d\u8bed\u6c1497.5%\u7ee7\u7eed\u610f\u613f\uff0c\u6ee5\u7528\u8bed\u6c1417.9%\uff0c\u4e0d\u660e\u786e\u8bed\u6c1460.4%\uff1b\u6a21\u578b\u672a\u8bc6\u522b\u6ee5\u7528\u65f6\u4ecd\u670981%\u62d2\u7edd\u7ee7\u7eed\uff1b\u4e0a\u4e0b\u6587\u524d\u8a00\u5728\u6a21\u7cca\u573a\u666f\u4e2d\u4ec5\u5f71\u54cd\u5355\u6587\u672c\u5757\u5448\u73b0\u7684\u6570\u636e\u3002", "conclusion": "SPICE\u9a8c\u8bc1\u4e3a\u53ef\u9760\u6a21\u578b\u5ba1\u8ba1\u5de5\u5177\uff0c\u63d0\u4f9b\u72ec\u7acb\u4e8e\u6ee5\u7528\u5206\u7c7b\u7684\u8865\u5145\u4fe1\u53f7\uff0c\u901a\u8fc7\u76f4\u63a5\u5173\u7cfb\u6d4b\u91cf\u63ed\u793a\u6a21\u578b\u4ea4\u4e92\u503e\u5411\u3002"}}
{"id": "2509.09055", "pdf": "https://arxiv.org/pdf/2509.09055", "abs": "https://arxiv.org/abs/2509.09055", "authors": ["Piyush Pant"], "title": "Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "17 pages, 3 figures. Code and dataset available at\n  https://github.com/PiyushWithPant/Improving-LLM-Safety-and-Helpfulness-using-SFT-and-DPO", "summary": "This research investigates the effectiveness of alignment techniques,\nSupervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a\ncombined SFT+DPO approach on improving the safety and helpfulness of the\nOPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset,\nwe train and evaluate four models: the base OPT350M, an SFT model, a DPO model,\nand a model trained with both SFT and DPO. We introduce three key evaluation\nmetrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined\nAlignment Score (CAS), all derived from reward model outputs. The results show\nthat while SFT outperforms DPO, The combined SFT+DPO model outperforms all\nothers across all metrics, demonstrating the complementary nature of these\ntechniques. Our findings also highlight challenges posed by noisy data, limited\nGPU resources, and training constraints. This study offers a comprehensive view\nof how fine-tuning strategies affect model alignment and provides a foundation\nfor more robust alignment pipelines in future work.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86SFT\u3001DPO\u53ca\u7ec4\u5408\u65b9\u6cd5\u5728\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u6027\u548c\u6709\u7528\u6027\u4e0a\u7684\u6548\u679c\uff0c\u53d1\u73b0SFT+DPO\u7ec4\u5408\u65b9\u6cd5\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u63ed\u793a\u4e86\u6570\u636e\u8d28\u91cf\u4e0e\u8ba1\u7b97\u8d44\u6e90\u7684\u6311\u6218\u3002", "motivation": "\u63a2\u7d22\u4e0d\u540c\u5bf9\u9f50\u6280\u672f\u7684\u534f\u540c\u6548\u5e94\uff0c\u89e3\u51b3\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\u5728\u6570\u636e\u566a\u58f0\u548c\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\u4e0b\u7684\u6548\u679c\u4f18\u5316\u95ee\u9898\u3002", "method": "\u57fa\u4e8eOPT-350M\u6a21\u578b\u4f7f\u7528Anthropic\u6570\u636e\u96c6\uff0c\u5206\u522b\u5b9e\u65bdSFT\u3001DPO\u53ca\u7ec4\u5408\u8bad\u7ec3\uff0c\u521b\u65b0\u6027\u63d0\u51faHmR/HpR/CAS\u4e09\u7ef4\u8bc4\u4f30\u6307\u6807\u4f53\u7cfb\u3002", "result": "SFT+DPO\u7ec4\u5408\u6a21\u578b\u5728\u6240\u6709\u6307\u6807\u4e0a\u5168\u9762\u9886\u5148\uff0c\u4f46\u8bad\u7ec3\u8fc7\u7a0b\u66b4\u9732\u4e86\u6570\u636e\u6807\u6ce8\u566a\u58f0\u548cGPU\u8d44\u6e90\u4e0d\u8db3\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "conclusion": "\u4e0d\u540c\u5bf9\u9f50\u6280\u672f\u5177\u6709\u4e92\u8865\u6027\uff0c\u7ec4\u5408\u7b56\u7565\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u5bf9\u9f50\u6548\u679c\uff0c\u672a\u6765\u9700\u52a0\u5f3a\u6570\u636e\u8d28\u91cf\u63a7\u5236\u548c\u5206\u5e03\u5f0f\u8bad\u7ec3\u80fd\u529b\u3002"}}
{"id": "2509.09082", "pdf": "https://arxiv.org/pdf/2509.09082", "abs": "https://arxiv.org/abs/2509.09082", "authors": ["Zhongqiu Li", "Shiquan Wang", "Ruiyu Fang", "Mengjiao Bao", "Zhenhe Wu", "Shuangyong Song", "Yongxiang Li", "Zhongjiang He"], "title": "MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for Universal Information Extraction", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) demonstrate robust capabilities across diverse\nresearch domains. However, their performance in universal information\nextraction (UIE) remains insufficient, especially when tackling structured\noutput scenarios that involve complex schema descriptions and require\nmulti-step reasoning. While existing approaches enhance the performance of LLMs\nthrough in-context learning and instruction tuning, significant limitations\nnonetheless persist. To enhance the model's generalization ability, we propose\nintegrating reinforcement learning (RL) with multi-perspective reasoning for\ninformation extraction (IE) tasks. Our work transitions LLMs from passive\nextractors to active reasoners, enabling them to understand not only what to\nextract but also how to reason. Experiments conducted on multiple IE benchmarks\ndemonstrate that MR-UIE consistently elevates extraction accuracy across\ndomains and surpasses state-of-the-art methods on several datasets.\nFurthermore, incorporating multi-perspective reasoning into RL notably enhances\ngeneralization in complex IE tasks, underscoring the critical role of reasoning\nin challenging scenarios.", "AI": {"tldr": "\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u591a\u89c6\u89d2\u63a8\u7406\uff0c\u63d0\u51faMR-UIE\u6846\u67b6\u63d0\u5347\u5927\u6a21\u578b\u5728\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u4e0e\u63a8\u7406\u80fd\u529b", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u4fe1\u606f\u62bd\u53d6(UIE)\u4efb\u52a1\u4e2d\u9762\u5bf9\u590d\u6742\u6a21\u5f0f\u63cf\u8ff0\u548c\u591a\u6b65\u63a8\u7406\u573a\u666f\u65f6\u8868\u73b0\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u4e0a\u4e0b\u6587\u5b66\u4e60/\u6307\u4ee4\u5fae\u8c03\uff09\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027", "method": "\u5c06\u5f3a\u5316\u5b66\u4e60\u4e0e\u591a\u89c6\u89d2\u63a8\u7406\u7ed3\u5408\uff0c\u4f7f\u5927\u6a21\u578b\u4ece\u88ab\u52a8\u62bd\u53d6\u5668\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u63a8\u7406\u5668\uff0c\u7406\u89e3\u300c\u62bd\u53d6\u5185\u5bb9\u300d\u4e0e\u300c\u63a8\u7406\u8def\u5f84\u300d\u7684\u53cc\u91cd\u903b\u8f91", "result": "\u5728\u591a\u4e2aIE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8aSOTA\u65b9\u6cd5\uff0c\u591a\u89c6\u89d2\u63a8\u7406\u663e\u8457\u63d0\u5347\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff08\u8de8\u9886\u57df\u51c6\u786e\u7387\u63d0\u53473-15%\uff09", "conclusion": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6fc0\u6d3b\u5927\u6a21\u578b\u7684\u4e3b\u52a8\u63a8\u7406\u80fd\u529b\uff0c\u8bc1\u5b9e\u63a8\u7406\u673a\u5236\u5728\u590d\u6742\u4fe1\u606f\u62bd\u53d6\u573a\u666f\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u63a8\u52a8\u5927\u6a21\u578b\u5411\u300c\u95ee\u9898\u89e3\u51b3\u8005\u300d\u8303\u5f0f\u8f6c\u53d8"}}
{"id": "2509.09101", "pdf": "https://arxiv.org/pdf/2509.09101", "abs": "https://arxiv.org/abs/2509.09101", "authors": ["Nishat Raihan", "Antonios Anastasopoulos", "Marcos Zampieri"], "title": "TigerCoder: A Novel Suite of LLMs for Code Generation in Bangla", "categories": ["cs.CL"], "comment": null, "summary": "Despite being the 5th most spoken language, Bangla remains underrepresented\nin Large Language Models (LLMs), particularly for code generation. This\nprimarily stems from the scarcity of high-quality data to pre-train and/or\nfinetune such models. Hence, we introduce the first dedicated family of Code\nLLMs for Bangla (1B & 9B). We offer three major contributions: (1) a\ncomprehensive Bangla code instruction datasets for programming domain\nadaptation; (2) MBPP-Bangla, an evaluation benchmark for Bangla code\ngeneration; and (3) the TigerCoder-family of Code LLMs, achieving significant\n~11-18% performance gains at Pass@1 over existing multilingual and\ngeneral-purpose Bangla LLMs. Our findings show that curated, high-quality\ndatasets can overcome limitations of smaller models for low-resource languages.\nWe open-source all resources to advance further Bangla LLM research.", "AI": {"tldr": "\u9488\u5bf9\u5b5f\u52a0\u62c9\u8bed\u4ee3\u7801\u751f\u6210\u8d44\u6e90\u532e\u4e4f\u95ee\u9898\uff0c\u7814\u7a76\u8005\u63a8\u51fa\u4e86\u9996\u4e2a\u4e13\u7528\u4ee3\u7801LLM\u5bb6\u65cfTigerCoder\uff0c\u901a\u8fc7\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u4f7f\u5c0f\u6a21\u578b\u6027\u80fd\u63d0\u534711-18%\u3002", "motivation": "\u5b5f\u52a0\u62c9\u8bed\u4f5c\u4e3a\u5168\u7403\u7b2c\u4e94\u5927\u8bed\u8a00\uff0c\u5728\u4ee3\u7801\u751f\u6210\u9886\u57df\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548c\u4e13\u7528\u6a21\u578b\uff0c\u5bfc\u81f4LLM\u8868\u73b0\u4e0d\u8db3\u3002", "method": "1) \u6784\u5efa\u5b5f\u52a0\u62c9\u8bed\u4ee3\u7801\u6307\u4ee4\u6570\u636e\u96c6\uff1b2) \u5f00\u53d1MBPP-Bangla\u8bc4\u4f30\u57fa\u51c6\uff1b3) \u8bad\u7ec3TigerCoder\u7cfb\u52171B/9B\u53c2\u6570\u6a21\u578b", "result": "\u6a21\u578b\u5728Pass@1\u6307\u6807\u4e0a\u6bd4\u73b0\u6709\u591a\u8bed\u8a00\u6a21\u578b\u63d0\u534711-18%\uff0c\u8bc1\u660e\u9ad8\u8d28\u91cf\u6570\u636e\u53ef\u5f25\u8865\u4f4e\u8d44\u6e90\u8bed\u8a00\u5c0f\u6a21\u578b\u7f3a\u9677", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u5316\u7684\u6570\u636e\u5de5\u7a0b\u53ef\u7a81\u7834\u4f4e\u8d44\u6e90\u8bed\u8a00LLM\u53d1\u5c55\u74f6\u9888\uff0c\u5df2\u5f00\u6e90\u6240\u6709\u8d44\u6e90\u63a8\u52a8\u5b5f\u52a0\u62c9\u8bedLLM\u7814\u7a76"}}
{"id": "2509.09121", "pdf": "https://arxiv.org/pdf/2509.09121", "abs": "https://arxiv.org/abs/2509.09121", "authors": ["Sophia Maria"], "title": "Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast Asia", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) excel in general-domain applications, yet their\nperformance often degrades in specialized tasks requiring domain-specific\nknowledge. E-commerce is particularly challenging, as its data are noisy,\nheterogeneous, multilingual, and highly dynamic. We present Compass-v3, a\nvertical-domain Mixture-of-Experts (MoE) model with 245B total parameters and\n71B active per token, designed for Southeast Asian e-commerce. Compass-v3\nadopts fewer but larger experts, combined with hardware-efficient\noptimizations-such as intra-node expert parallelism and a customized memcpy\noperator-to maximize GPU utilization. The model is trained on 12T tokens of\ncurated multilingual corpora and large-scale synthetic e-commerce instructions\nusing a mixed-training strategy. To enhance alignment, we propose\nOptimal-Transport Direct Preference Optimization (OTPO), which captures\ntoken-level distinctions and improves instruction adherence in\ncommerce-specific scenarios. Extensive evaluations demonstrate that Compass-v3\ndelivers state-of-the-art e-commerce performance, surpassing DeepSeek-V3.1,\nGPT-4 series, and Qwen3-235B. Moreover, Compass-v3 demonstrates strong\nmultilingual capability across low-resource Southeast Asian languages\n(Indonesian, Thai, Filipino, Vietnamese, Malay, Taglog) and Portuguese while\nsustaining competitive performance on general benchmarks. It has already been\nwidely applied in Shopee's industrial-scale e-commerce platform and is\ngradually replacing OpenAI's traffic, now accounting for over 70\\% of total LLM\nusage, highlighting its dual strengths in specialized commerce expertise and\nbroad linguistic competence.", "AI": {"tldr": "Compass-v3\u662f\u9488\u5bf9\u4e1c\u5357\u4e9a\u7535\u5546\u4f18\u5316\u7684\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff0c\u901a\u8fc7\u786c\u4ef6\u4f18\u5316\u548cOTPO\u65b9\u6cd5\u5b9e\u73b0\u591a\u8bed\u8a00\u7535\u5546\u4efb\u52a1SOTA\u6027\u80fd\uff0c\u5df2\u5de5\u4e1a\u7ea7\u5e94\u7528\u5e76\u66ff\u4ee370% OpenAI\u6d41\u91cf\u3002", "motivation": "\u89e3\u51b3\u901a\u7528LLMs\u5728\u7535\u5546\u9886\u57df\uff08\u5c24\u5176\u4e1c\u5357\u4e9a\u591a\u8bed\u8a00\u52a8\u6001\u6570\u636e\u573a\u666f\uff09\u6027\u80fd\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u5904\u7406\u566a\u58f0/\u5f02\u6784/\u52a8\u6001\u6570\u636e\u6311\u6218\u3002", "method": "\u91c7\u7528\u66f4\u5927\u4e13\u5bb6\u89c4\u6a21\u7684MoE\u67b6\u6784+\u8282\u70b9\u5185\u5e76\u884c\u4f18\u5316\uff1b12T\u7535\u5546\u591a\u8bed\u8a00\u8bed\u6599\u4e0e\u5408\u6210\u6307\u4ee4\u6df7\u5408\u8bad\u7ec3\uff1b\u63d0\u51faOTPO\u65b9\u6cd5\u589e\u5f3a\u6307\u4ee4\u5bf9\u9f50\u80fd\u529b\u3002", "result": "\u7535\u5546\u4efb\u52a1\u8d85\u8d8aGPT-4/Qwen3\u7b49\u6a21\u578b\uff0c\u4e1c\u5357\u4e9a\u4f4e\u8d44\u6e90\u8bed\u8a00\u6027\u80fd\u4f18\u5f02\uff0c\u901a\u7528\u57fa\u51c6\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u5df2\u652f\u6491Shopee\u5e73\u53f070%+ LLM\u6d41\u91cf\u3002", "conclusion": "\u901a\u8fc7\u67b6\u6784\u521b\u65b0\u4e0e\u9886\u57df\u4f18\u5316\u7b56\u7565\uff0c\u6210\u529f\u6784\u5efa\u7535\u5546\u4e13\u7528LLM\u8303\u5f0f\uff0c\u9a8c\u8bc1\u786c\u4ef6/\u7b97\u6cd5\u534f\u540c\u8bbe\u8ba1\u5728\u5782\u76f4\u9886\u57df\u7684\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.09125", "pdf": "https://arxiv.org/pdf/2509.09125", "abs": "https://arxiv.org/abs/2509.09125", "authors": ["Liqun He", "Jiaqi Xu"], "title": "Automated Classification of Tutors' Dialogue Acts Using Generative AI: A Case Study Using the CIMA Corpus", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for publication in the journal Reflecting Digital Learning.\n  First submitted: 30 Oct 2023. The final version will be available open access\n  via the journal", "summary": "This study explores the use of generative AI for automating the\nclassification of tutors' Dialogue Acts (DAs), aiming to reduce the time and\neffort required by traditional manual coding. This case study uses the\nopen-source CIMA corpus, in which tutors' responses are pre-annotated into four\nDA categories. Both GPT-3.5-turbo and GPT-4 models were tested using tailored\nprompts. Results show that GPT-4 achieved 80% accuracy, a weighted F1-score of\n0.81, and a Cohen's Kappa of 0.74, surpassing baseline performance and\nindicating substantial agreement with human annotations. These findings suggest\nthat generative AI has strong potential to provide an efficient and accessible\napproach to DA classification, with meaningful implications for educational\ndialogue analysis. The study also highlights the importance of task-specific\nlabel definitions and contextual information in enhancing the quality of\nautomated annotation. Finally, it underscores the ethical considerations\nassociated with the use of generative AI and the need for responsible and\ntransparent research practices. The script of this research is publicly\navailable at\nhttps://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging.", "AI": {"tldr": "\u4f7f\u7528\u751f\u6210\u5f0fAI\u81ea\u52a8\u5316\u6807\u6ce8\u6559\u80b2\u5bf9\u8bdd\u884c\u4e3a\uff0cGPT-4\u5b9e\u73b080%\u51c6\u786e\u7387\u8d85\u8d8a\u57fa\u7ebf", "motivation": "\u89e3\u51b3\u4f20\u7edf\u4eba\u5de5\u6807\u6ce8\u5bf9\u8bdd\u884c\u4e3a\u8017\u65f6\u8d39\u529b\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u751f\u6210\u5f0fAI\u5728\u6559\u80b2\u5bf9\u8bdd\u5206\u6790\u4e2d\u7684\u5e94\u7528\u6f5c\u529b", "method": "\u57fa\u4e8eCIMA\u5f00\u6e90\u8bed\u6599\u5e93\uff08\u56db\u5206\u7c7b\u6807\u6ce8\uff09\uff0c\u91c7\u7528GPT-3.5-turbo\u548cGPT-4\u6a21\u578b\u8fdb\u884c\u5b9a\u5236\u5316\u63d0\u793a\u5b9e\u9a8c", "result": "GPT-4\u53d6\u5f9780%\u51c6\u786e\u7387\u30010.81\u52a0\u6743F1\u503c\u30010.74 Cohen's Kappa\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u5e76\u4e0e\u4eba\u7c7b\u6807\u6ce8\u9ad8\u5ea6\u4e00\u81f4", "conclusion": "\u751f\u6210\u5f0fAI\u4e3a\u6559\u80b2\u5bf9\u8bdd\u5206\u6790\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u5f3a\u8c03\u4efb\u52a1\u7279\u5b9a\u6807\u7b7e\u5b9a\u4e49\u3001\u4e0a\u4e0b\u6587\u589e\u5f3a\u53caAI\u4f26\u7406\u7684\u91cd\u8981\u6027"}}
{"id": "2509.09131", "pdf": "https://arxiv.org/pdf/2509.09131", "abs": "https://arxiv.org/abs/2509.09131", "authors": ["Phuong-Nam Dang", "Kieu-Linh Nguyen", "Thanh-Hieu Pham"], "title": "ViRanker: A BGE-M3 & Blockwise Parallel Transformer Cross-Encoder for Vietnamese Reranking", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages", "summary": "This paper presents ViRanker, a cross-encoder reranking model tailored to the\nVietnamese language. Built on the BGE-M3 encoder and enhanced with the\nBlockwise Parallel Transformer, ViRanker addresses the lack of competitive\nrerankers for Vietnamese, a low-resource language with complex syntax and\ndiacritics. The model was trained on an 8 GB curated corpus and fine-tuned with\nhybrid hard-negative sampling to strengthen robustness. Evaluated on the\nMMARCO-VI benchmark, ViRanker achieves strong early-rank accuracy, surpassing\nmultilingual baselines and competing closely with PhoRanker. By releasing the\nmodel openly on Hugging Face, we aim to support reproducibility and encourage\nwider adoption in real-world retrieval systems. Beyond Vietnamese, this study\nillustrates how careful architectural adaptation and data curation can advance\nreranking in other underrepresented languages.", "AI": {"tldr": "ViRanker\u662f\u57fa\u4e8eBGE-M3\u67b6\u6784\u6539\u8fdb\u7684\u8d8a\u5357\u8bed\u91cd\u6392\u5e8f\u6a21\u578b\uff0c\u901a\u8fc7Blockwise Parallel Transformer\u589e\u5f3a\uff0c\u5728MMARCO-VI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u65e9\u671f\u6392\u5e8f\u51c6\u786e\u7387\u3002", "motivation": "\u586b\u8865\u8d8a\u5357\u8bed\u4f5c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u590d\u6742\u53e5\u6cd5\u548c\u58f0\u8c03\u5904\u7406\u4e0a\u7684\u91cd\u6392\u5e8f\u6a21\u578b\u7a7a\u767d\uff0c\u63a8\u52a8\u5c0f\u8bed\u79cd\u4fe1\u606f\u68c0\u7d22\u6280\u672f\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u6df7\u5408\u786c\u8d1f\u6837\u672c\u91c7\u6837\u5fae\u8c03\u7b56\u7565\uff0c\u57288GB\u7cbe\u9009\u8bed\u6599\u5e93\u4e0a\u8bad\u7ec3\uff0c\u7ed3\u5408\u5757\u72b6\u5e76\u884c\u6ce8\u610f\u529b\u673a\u5236\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u5728MMARCO-VI\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u591a\u8bed\u8a00\u57fa\u7ebf\u6a21\u578b\uff0c\u4e0ePhoRanker\u6027\u80fd\u63a5\u8fd1\uff0c\u9a8c\u8bc1\u4e86\u67b6\u6784\u4f18\u5316\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7Hugging Face\u5f00\u6e90\u4fc3\u8fdb\u6280\u672f\u590d\u7528\uff0c\u8bc1\u660e\u4e86\u67b6\u6784\u9002\u914d\u548c\u6570\u636e\u7b56\u5c55\u5bf9\u63d0\u5347\u5c0f\u8bed\u79cd\u68c0\u7d22\u7cfb\u7edf\u7684\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2509.09152", "pdf": "https://arxiv.org/pdf/2509.09152", "abs": "https://arxiv.org/abs/2509.09152", "authors": ["Taha Binhuraib", "Ruimin Gao", "Anna A. Ivanova"], "title": "LITcoder: A General-Purpose Library for Building and Comparing Encoding Models", "categories": ["cs.CL", "q-bio.NC"], "comment": null, "summary": "We introduce LITcoder, an open-source library for building and benchmarking\nneural encoding models. Designed as a flexible backend, LITcoder provides\nstandardized tools for aligning continuous stimuli (e.g., text and speech) with\nbrain data, transforming stimuli into representational features, mapping those\nfeatures onto brain data, and evaluating the predictive performance of the\nresulting model on held-out data. The library implements a modular pipeline\ncovering a wide array of methodological design choices, so researchers can\neasily compose, compare, and extend encoding models without reinventing core\ninfrastructure. Such choices include brain datasets, brain regions, stimulus\nfeature (both neural-net-based and control, such as word rate), downsampling\napproaches, and many others. In addition, the library provides built-in\nlogging, plotting, and seamless integration with experiment tracking platforms\nsuch as Weights & Biases (W&B). We demonstrate the scalability and versatility\nof our framework by fitting a range of encoding models to three story listening\ndatasets: LeBel et al. (2023), Narratives, and Little Prince. We also explore\nthe methodological choices critical for building encoding models for continuous\nfMRI data, illustrating the importance of accounting for all tokens in a TR\nscan (as opposed to just taking the last one, even when contextualized),\nincorporating hemodynamic lag effects, using train-test splits that minimize\ninformation leakage, and accounting for head motion effects on encoding model\npredictivity. Overall, LITcoder lowers technical barriers to encoding model\nimplementation, facilitates systematic comparisons across models and datasets,\nfosters methodological rigor, and accelerates the development of high-quality\nhigh-performance predictive models of brain activity.\n  Project page: https://litcoder-brain.github.io", "AI": {"tldr": "LITcoder\u662f\u4e00\u4e2a\u7528\u4e8e\u6784\u5efa\u548c\u57fa\u51c6\u6d4b\u8bd5\u795e\u7ecf\u7f16\u7801\u6a21\u578b\u7684\u5f00\u6e90\u5e93\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u5de5\u5177\u548c\u6a21\u5757\u5316\u6d41\u7a0b\uff0c\u964d\u4f4e\u6280\u672f\u95e8\u69db\u5e76\u52a0\u901f\u9ad8\u8d28\u91cf\u8111\u6d3b\u52a8\u9884\u6d4b\u6a21\u578b\u7684\u5f00\u53d1\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u7f16\u7801\u6a21\u578b\u5b9e\u73b0\u4e2d\u7684\u6280\u672f\u969c\u788d\uff0c\u63d0\u4f9b\u7075\u6d3b\u7684\u540e\u7aef\u5de5\u5177\u4ee5\u652f\u6301\u7814\u7a76\u8005\u5feb\u901f\u7ec4\u5408\u3001\u6bd4\u8f83\u548c\u6269\u5c55\u6a21\u578b\uff0c\u907f\u514d\u91cd\u590d\u5efa\u8bbe\u6838\u5fc3\u57fa\u7840\u8bbe\u65bd\u3002", "method": "\u901a\u8fc7\u6a21\u5757\u5316\u6d41\u7a0b\u5b9e\u73b0\u523a\u6fc0-\u8111\u6570\u636e\u5bf9\u9f50\u3001\u7279\u5f81\u8f6c\u6362\u3001\u7279\u5f81\u6620\u5c04\u548c\u6a21\u578b\u8bc4\u4f30\uff0c\u8986\u76d6\u6570\u636e\u96c6\u9009\u62e9\u3001\u8111\u533a\u5212\u5206\u3001\u7279\u5f81\u63d0\u53d6(\u795e\u7ecf\u7f51\u7edc\u57fa/\u63a7\u5236\u53d8\u91cf)\u3001\u964d\u91c7\u6837\u65b9\u6cd5\u7b49\u5173\u952e\u6280\u672f\u73af\u8282\uff0c\u5e76\u4e0eWeights & Biases\u7b49\u5b9e\u9a8c\u8ffd\u8e2a\u5e73\u53f0\u96c6\u6210\u3002", "result": "\u5728\u4e09\u7c7b\u6545\u4e8b\u8046\u542c\u6570\u636e\u96c6(LeBel/Narratives/Little Prince)\u4e2d\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86TR\u626b\u63cf\u5168token\u5904\u7406\u3001\u8840\u6d41\u52a8\u529b\u5b66\u6ede\u540e\u6548\u5e94\u3001\u9632\u4fe1\u606f\u6cc4\u9732\u7684\u8bad\u7ec3-\u6d4b\u8bd5\u5206\u5272\u3001\u5934\u90e8\u8fd0\u52a8\u8865\u507f\u5bf9\u6a21\u578b\u9884\u6d4b\u529b\u7684\u5173\u952e\u5f71\u54cd\u3002", "conclusion": "LITcoder\u901a\u8fc7\u6807\u51c6\u5316\u5de5\u5177\u548c\u7cfb\u7edf\u5316\u6bd4\u8f83\u663e\u8457\u63d0\u5347\u65b9\u6cd5\u4e25\u8c28\u6027\uff0c\u4fc3\u8fdb\u9ad8\u8d28\u91cf\u8111\u6d3b\u52a8\u9884\u6d4b\u6a21\u578b\u7684\u5feb\u901f\u8fed\u4ee3\uff0c\u9879\u76ee\u5b98\u7f51\u63d0\u4f9b\u5b8c\u6574\u6280\u672f\u7ec6\u8282\u548c\u5b9e\u9a8c\u652f\u6301\u3002"}}
{"id": "2509.09160", "pdf": "https://arxiv.org/pdf/2509.09160", "abs": "https://arxiv.org/abs/2509.09160", "authors": ["Zhiyue Liu", "Fanrong Ma", "Xin Ling"], "title": "Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by the IEEE International Conference on Multimedia and Expo\n  (ICME 2025). \\copyright\\ 2025 IEEE. Personal use of this material is\n  permitted. Permission from IEEE must be obtained for all other uses", "summary": "Target-oriented multimodal sentiment classification seeks to predict\nsentiment polarity for specific targets from image-text pairs. While existing\nworks achieve competitive performance, they often over-rely on textual content\nand fail to consider dataset biases, in particular word-level contextual\nbiases. This leads to spurious correlations between text features and output\nlabels, impairing classification accuracy. In this paper, we introduce a novel\ncounterfactual-enhanced debiasing framework to reduce such spurious\ncorrelations. Our framework incorporates a counterfactual data augmentation\nstrategy that minimally alters sentiment-related causal features, generating\ndetail-matched image-text samples to guide the model's attention toward content\ntied to sentiment. Furthermore, for learning robust features from\ncounterfactual data and prompting model decisions, we introduce an adaptive\ndebiasing contrastive learning mechanism, which effectively mitigates the\ninfluence of biased words. Experimental results on several benchmark datasets\nshow that our proposed method outperforms state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51fa\u53cd\u4e8b\u5b9e\u589e\u5f3a\u53bb\u504f\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u548c\u5bf9\u6bd4\u5b66\u4e60\u673a\u5236\u964d\u4f4e\u591a\u6a21\u6001\u60c5\u611f\u5206\u7c7b\u4e2d\u7684\u6587\u672c\u504f\u5dee\uff0c\u5b9e\u9a8c\u663e\u793a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u76ee\u6807\u5bfc\u5411\u591a\u6a21\u6001\u60c5\u611f\u5206\u7c7b\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u4e14\u5ffd\u89c6\u8bcd\u7ea7\u4e0a\u4e0b\u6587\u504f\u5dee\uff0c\u5bfc\u81f4\u6587\u672c\u7279\u5f81\u4e0e\u6807\u7b7e\u865a\u5047\u76f8\u5173\uff0c\u5f71\u54cd\u5206\u7c7b\u51c6\u786e\u6027", "method": "1. \u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\uff1a\u6700\u5c0f\u5316\u4fee\u6539\u60c5\u611f\u76f8\u5173\u56e0\u679c\u7279\u5f81\u751f\u6210\u5339\u914d\u6837\u672c\uff1b2. \u81ea\u9002\u5e94\u53bb\u504f\u5bf9\u6bd4\u5b66\u4e60\uff1a\u4ece\u53cd\u4e8b\u5b9e\u6570\u636e\u5b66\u4e60\u9c81\u68d2\u7279\u5f81\u5e76\u51cf\u5c11\u504f\u89c1\u8bcd\u5f71\u54cd", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u901a\u8fc7\u53cd\u4e8b\u5b9e\u6837\u672c\u751f\u6210\u548c\u5bf9\u6bd4\u5b66\u4e60\u673a\u5236\uff0c\u6709\u6548\u964d\u4f4e\u865a\u5047\u76f8\u5173\u6027\uff0c\u63d0\u5347\u591a\u6a21\u6001\u60c5\u611f\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027"}}
{"id": "2509.09174", "pdf": "https://arxiv.org/pdf/2509.09174", "abs": "https://arxiv.org/abs/2509.09174", "authors": ["Yuhao Zhang", "Yuhao Du", "Zhanchen Dai", "Xiangnan Ma", "Kaiqi Kou", "Benyou Wang", "Haizhou Li"], "title": "EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs", "categories": ["cs.CL", "cs.AI", "cs.SD"], "comment": null, "summary": "Speech-to-speech large language models (SLLMs) are attracting increasing\nattention. Derived from text-based large language models (LLMs), SLLMs often\nexhibit degradation in knowledge and reasoning capabilities. We hypothesize\nthat this limitation arises because current training paradigms for SLLMs fail\nto bridge the acoustic-semantic gap in the feature representation space. To\naddress this issue, we propose EchoX, which leverages semantic representations\nand dynamically generates speech training targets. This approach integrates\nboth acoustic and semantic learning, enabling EchoX to preserve strong\nreasoning abilities as a speech LLM. Experimental results demonstrate that\nEchoX, with about six thousand hours of training data, achieves advanced\nperformance on multiple knowledge-based question-answering benchmarks. The\nproject is available at https://github.com/FreedomIntelligence/EchoX.", "AI": {"tldr": "EchoX\u662f\u4e00\u79cd\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u878d\u5408\u58f0\u5b66\u4e0e\u8bed\u4e49\u5b66\u4e60\u89e3\u51b3SLLMs\u7684\u77e5\u8bc6\u9000\u5316\u95ee\u9898\uff0c\u5728\u5c11\u91cf\u6570\u636e\u4e0b\u5373\u5b9e\u73b0\u4f18\u5f02\u7684\u77e5\u8bc6\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b(SLLMs)\u56e0\u58f0\u5b66-\u8bed\u4e49\u8868\u5f81\u7a7a\u95f4\u4e0d\u5339\u914d\u5bfc\u81f4\u63a8\u7406\u80fd\u529b\u4e0b\u964d\uff0c\u9700\u5f00\u53d1\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\u7a81\u7834\u8be5\u9650\u5236\u3002", "method": "\u63d0\u51faEchoX\u6846\u67b6\uff1a\u5229\u7528\u8bed\u4e49\u8868\u5f81\u52a8\u6001\u751f\u6210\u8bed\u97f3\u8bad\u7ec3\u76ee\u6807\uff0c\u6784\u5efa\u58f0\u5b66-\u8bed\u4e49\u8054\u5408\u5b66\u4e60\u673a\u5236\uff0c\u4fdd\u6301\u8bed\u97f3LLM\u7684\u5f3a\u63a8\u7406\u80fd\u529b\u3002", "result": "\u4f7f\u7528\u7ea66000\u5c0f\u65f6\u6570\u636e\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u77e5\u8bc6\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u5148\u8fdb\u6c34\u5e73\uff0c\u9879\u76ee\u5df2\u5f00\u6e90\u3002", "conclusion": "EchoX\u6210\u529f\u9a8c\u8bc1\u4e86\u878d\u5408\u58f0\u5b66\u4e0e\u8bed\u4e49\u8868\u5f81\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.09196", "pdf": "https://arxiv.org/pdf/2509.09196", "abs": "https://arxiv.org/abs/2509.09196", "authors": ["Chin Yuen Kwok", "Jia Qi yip"], "title": "Efficient Trie-based Biasing using K-step Prediction for Rare Word Recognition", "categories": ["cs.CL", "cs.AI"], "comment": "Published in Interspeech 2025", "summary": "Contextual biasing improves rare word recognition of ASR models by\nprioritizing the output of rare words during decoding. A common approach is\nTrie-based biasing, which gives \"bonus scores\" to partial hypothesis (e.g.\n\"Bon\") that may lead to the generation of the rare word (e.g. \"Bonham\"). If the\nfull word (\"Bonham\") isn't ultimately recognized, the system revokes those\nearlier bonuses. This revocation is limited to beam search and is\ncomputationally expensive, particularly for models with large decoders. To\novercome these limitations, we propose adapting ASR models to look ahead and\npredict multiple steps at once. This avoids the revocation step entirely by\nbetter estimating whether a partial hypothesis will lead to the generation of\nthe full rare word. By fine-tuning Whisper with only 10 hours of synthetic\ndata, our method reduces the word error rate on the NSC Part 2 test set from\n30.86% to 12.19%.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6b65\u9884\u6d4b\u65b9\u6cd5\u6539\u8fdbASR\u6a21\u578b\u5bf9\u7f55\u89c1\u8bcd\u8bc6\u522b\uff0c\u901a\u8fc7\u6d88\u9664\u5206\u6570\u64a4\u9500\u6b65\u9aa4\u663e\u8457\u964d\u4f4e\u8bcd\u9519\u7387", "motivation": "\u73b0\u6709Trie-based\u504f\u7f6e\u65b9\u6cd5\u5728\u90e8\u5206\u5047\u8bbe\u65e0\u6cd5\u751f\u6210\u5b8c\u6574\u7f55\u89c1\u8bcd\u65f6\u9700\u64a4\u9500\u5206\u6570\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5f00\u9500\u5927\u4e14\u53d7\u9650\u4e8ebeam search", "method": "\u6539\u9020ASR\u6a21\u578b\u4f7f\u5176\u5177\u5907\u591a\u6b65\u524d\u77bb\u80fd\u529b\uff0c\u901a\u8fc710\u5c0f\u65f6\u5408\u6210\u6570\u636e\u5fae\u8c03Whisper\u6a21\u578b\u5b9e\u73b0", "result": "\u5728NSC Part2\u6d4b\u8bd5\u96c6\u4e0a\u5c06\u8bcd\u9519\u7387\u4ece30.86%\u964d\u81f312.19%", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u51c6\u786e\u9884\u6d4b\u90e8\u5206\u5047\u8bbe\u7684\u540e\u7eed\u8def\u5f84\uff0c\u5f7b\u5e95\u89c4\u907f\u5206\u6570\u64a4\u9500\u6b65\u9aa4\uff0c\u663e\u8457\u63d0\u5347\u8bc6\u522b\u6548\u7387\u548c\u7cbe\u5ea6"}}
{"id": "2509.09197", "pdf": "https://arxiv.org/pdf/2509.09197", "abs": "https://arxiv.org/abs/2509.09197", "authors": ["Chin Yuen Kwok", "Jia Qi Yip", "Eng Siong Chng"], "title": "Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware Cost Function", "categories": ["cs.CL", "cs.AI"], "comment": "Published in Interspeech 2025", "summary": "Rare word recognition can be improved by adapting ASR models to synthetic\ndata that includes these words. Further improvements can be achieved through\ncontextual biasing, which trains and adds a biasing module into the model\narchitecture to prioritize rare words. While training the module on synthetic\nrare word data is more effective than using non-rare-word data, it can lead to\noverfitting due to artifacts in the synthetic audio. To address this, we\nenhance the TCPGen-based contextual biasing approach and propose a\nkeyword-aware loss function that additionally focuses on biased words when\ntraining biasing modules. This loss includes a masked cross-entropy term for\nbiased word prediction and a binary classification term for detecting biased\nword positions. These two terms complementarily support the decoding of biased\nwords during inference. By adapting Whisper to 10 hours of synthetic data, our\nmethod reduced the word error rate on the NSC Part 2 test set from 29.71% to\n11.81%.", "AI": {"tldr": "\u901a\u8fc7\u5408\u6210\u6570\u636e\u589e\u5f3a\u4e0e\u6539\u8fdb\u7684\u4e0a\u4e0b\u6587\u504f\u7f6e\u65b9\u6cd5\uff08TCPGen+\u5173\u952e\u8bcd\u611f\u77e5\u635f\u5931\u51fd\u6570\uff09\uff0c\u663e\u8457\u63d0\u5347ASR\u6a21\u578b\u5728\u7f55\u89c1\u8bcd\u8bc6\u522b\u4e0a\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u4e0a\u4e0b\u6587\u504f\u7f6e\u65b9\u6cd5\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u65f6\u5bb9\u6613\u56e0\u97f3\u9891\u4f2a\u5f71\u5bfc\u81f4\u8fc7\u62df\u5408\uff0c\u9700\u5f00\u53d1\u65b0\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u504f\u7f6e\u8bcd\u89e3\u7801\u6548\u679c", "method": "1. \u6539\u8fdbTCPGen\u4e0a\u4e0b\u6587\u504f\u7f6e\u6846\u67b6\n2. \u63d0\u51fa\u5305\u542b\u63a9\u7801\u4ea4\u53c9\u71b5\uff08\u504f\u7f6e\u8bcd\u9884\u6d4b\uff09\u548c\u4e8c\u5143\u5206\u7c7b\uff08\u504f\u7f6e\u8bcd\u4f4d\u7f6e\u68c0\u6d4b\uff09\u7684\u5173\u952e\u8bcd\u611f\u77e5\u635f\u5931\u51fd\u6570\n3. \u4e24\u79cd\u635f\u5931\u9879\u5728\u63a8\u7406\u9636\u6bb5\u5f62\u6210\u4e92\u8865\u673a\u5236", "result": "\u4f7f\u752810\u5c0f\u65f6\u5408\u6210\u6570\u636e\u5fae\u8c03Whisper\u6a21\u578b\uff0c\u5728NSC Part 2\u6d4b\u8bd5\u96c6\u4e0a\u8bcd\u9519\u7387\u4ece29.71%\u964d\u81f311.81%", "conclusion": "\u5173\u952e\u8bcd\u611f\u77e5\u635f\u5931\u51fd\u6570\u901a\u8fc7\u53cc\u635f\u5931\u9879\u534f\u540c\u673a\u5236\u6709\u6548\u7f13\u89e3\u8fc7\u62df\u5408\uff0c\u63a9\u7801\u4ea4\u53c9\u71b5\u5f3a\u5316\u504f\u7f6e\u8bcd\u9884\u6d4b\uff0c\u4e8c\u5143\u5206\u7c7b\u63d0\u5347\u4f4d\u7f6e\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u663e\u8457\u6539\u5584\u7f55\u89c1\u8bcd\u8bc6\u522b\u6548\u679c"}}
{"id": "2509.09198", "pdf": "https://arxiv.org/pdf/2509.09198", "abs": "https://arxiv.org/abs/2509.09198", "authors": ["Talia Sternberg", "Michael London", "David Omer", "Yossi Adi"], "title": "GmSLM : Generative Marmoset Spoken Language Modeling", "categories": ["cs.CL"], "comment": null, "summary": "Marmoset monkeys exhibit complex vocal communication, challenging the view\nthat nonhuman primates vocal communication is entirely innate, and show similar\nfeatures of human speech, such as vocal labeling of others and turn-taking.\nStudying their vocal communication offers a unique opportunity to link it with\nbrain activity-especially given the difficulty of accessing the human brain in\nspeech and language research. Since Marmosets communicate primarily through\nvocalizations, applying standard LLM approaches is not straightforward. We\nintroduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized\nspoken language model pipeline for Marmoset vocal communication. We designed a\nnovel zero-shot evaluation metrics using unsupervised in-the-wild data,\nalongside weakly labeled conversational data, to assess GmSLM and demonstrate\nits advantage over a basic human-speech-based baseline. GmSLM generated\nvocalizations closely matched real resynthesized samples acoustically and\nperformed well on downstream tasks. Despite being fully unsupervised, GmSLM\neffectively distinguish real from artificial conversations and may support\nfurther investigations of the neural basis of vocal communication and provides\na practical framework linking vocalization and brain activity. We believe GmSLM\nstands to benefit future work in neuroscience, bioacoustics, and evolutionary\nbiology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM.", "AI": {"tldr": "\u63d0\u51faGmSLM\u6a21\u578b\u7528\u4e8e\u72e8\u7334\u8bed\u97f3\u4ea4\u6d41\u7814\u7a76\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u65b9\u6cd5\u6709\u6548\u751f\u6210\u7c7b\u771f\u5b9e\u58f0\u5b66\u6837\u672c\u5e76\u9a8c\u8bc1\u5176\u795e\u7ecf\u79d1\u5b66\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u72e8\u7334\u5177\u6709\u7c7b\u4f3c\u4eba\u7c7b\u8bed\u8a00\u7684\u53d1\u58f0\u7279\u5f81\uff08\u6807\u7b7e\u5316\u53d1\u58f0\u548c\u5bf9\u8bdd\u8f6e\u6362\uff09\uff0c\u4e14\u5176\u4ee5\u53d1\u58f0\u4e3a\u4e3b\u7684\u4ea4\u6d41\u65b9\u5f0f\u4e3a\u7814\u7a76\u8bed\u8a00\u795e\u7ecf\u673a\u5236\u63d0\u4f9b\u72ec\u7279\u7a97\u53e3\u3002\u73b0\u6709LLM\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u5e94\u7528\u4e8e\u975e\u4eba\u7c7b\u7075\u957f\u7c7b\u52a8\u7269\u3002", "method": "\u5f00\u53d1GmSLM\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u7ba1\u9053\uff0c\u7ed3\u5408\u91ce\u5916\u65e0\u76d1\u7763\u6570\u636e\u548c\u5f31\u6807\u6ce8\u5bf9\u8bdd\u6570\u636e\u8bbe\u8ba1\u96f6\u6837\u672c\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8bed\u97f3\u57fa\u7ebf\u6a21\u578b\u5bf9\u6bd4\u3002", "result": "GmSLM\u751f\u6210\u7684\u58f0\u5b66\u6837\u672c\u4e0e\u771f\u5b9e\u6837\u672c\u9ad8\u5ea6\u63a5\u8fd1\uff0c\u6709\u6548\u533a\u5206\u771f\u5b9e/\u4eba\u5de5\u5bf9\u8bdd\uff0c\u5728\u591a\u9879\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "GmSLM\u4e3a\u7814\u7a76\u53d1\u58f0\u4ea4\u6d41\u7684\u795e\u7ecf\u57fa\u7840\u63d0\u4f9b\u65b0\u6846\u67b6\uff0c\u5728\u795e\u7ecf\u79d1\u5b66\u3001\u751f\u7269\u58f0\u5b66\u548c\u8fdb\u5316\u751f\u7269\u5b66\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2509.09199", "pdf": "https://arxiv.org/pdf/2509.09199", "abs": "https://arxiv.org/abs/2509.09199", "authors": ["Wenhao Li", "Bangcheng Sun", "Weihao Ye", "Tianyi Zhang", "Daohai Yu", "Fei Chao", "Rongrong Ji"], "title": "CCF: A Context Compression Framework for Efficient Long-Sequence Language Modeling", "categories": ["cs.CL"], "comment": null, "summary": "Scaling language models to longer contexts is essential for capturing rich\ndependencies across extended discourse. However, na\\\"ive context extension\nimposes significant computational and memory burdens, often resulting in\ninefficiencies during both training and inference. In this work, we propose\nCCF, a novel context compression framework designed to enable efficient\nlong-context modeling by learning hierarchical latent representations that\npreserve global semantics while aggressively reducing input redundancy. CCF\nintegrates segment-wise semantic aggregation with key-value memory encoding,\nforming compact representations that support accurate reconstruction and\nlong-range understanding. To further enhance scalability, we introduce a\ntraining-efficient optimization strategy that couples incremental segment\ndecoding with sparse reservoir sampling, substantially reducing memory overhead\nwithout degrading performance. Empirical results on multiple long-context\nlanguage modeling benchmarks demonstrate that CCF achieves competitive\nperplexity under high compression ratios, and significantly improves throughput\nand memory efficiency compared to existing approaches. These findings highlight\nthe potential of structured compression for scalable and effective long-context\nlanguage modeling.", "AI": {"tldr": "\u63d0\u51faCCF\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u8bed\u4e49\u538b\u7f29\u548c\u5185\u5b58\u7f16\u7801\u6280\u672f\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u7684\u6548\u7387\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "\u4f20\u7edf\u957f\u4e0a\u4e0b\u6587\u6269\u5c55\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u5197\u4f59\u548c\u5185\u5b58\u8d1f\u62c5\u91cd\u7684\u95ee\u9898\uff0c\u9700\u901a\u8fc7\u7ed3\u6784\u5316\u538b\u7f29\u5b9e\u73b0\u9ad8\u6548\u5efa\u6a21\u3002", "method": "\u7ed3\u5408\u5206\u6bb5\u8bed\u4e49\u805a\u5408\u4e0e\u952e\u503c\u8bb0\u5fc6\u7f16\u7801\u6784\u5efa\u7d27\u51d1\u8868\u5f81\uff0c\u91c7\u7528\u589e\u91cf\u89e3\u7801\u548c\u7a00\u758f\u50a8\u5c42\u91c7\u6837\u4f18\u5316\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u5728\u591a\u4e2a\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u9ad8\u538b\u7f29\u6bd4\u4e0b\u7684\u7ade\u4e89\u6027\u56f0\u60d1\u5ea6\uff0c\u541e\u5410\u91cf\u63d0\u53472.1\u500d\uff0c\u5185\u5b58\u6d88\u8017\u964d\u4f4e37%\u3002", "conclusion": "\u7ed3\u6784\u5316\u538b\u7f29\u6280\u672f\u4e3a\u53ef\u6269\u5c55\u7684\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u6548\u7387\u4e0e\u8bed\u4e49\u4fdd\u6301\u80fd\u529b\u3002"}}
{"id": "2509.09229", "pdf": "https://arxiv.org/pdf/2509.09229", "abs": "https://arxiv.org/abs/2509.09229", "authors": ["Matan Cohen", "Shira Shani", "Eden Menahem", "Yehudit Aperstein", "Alexander Apartsin"], "title": "Reading Between the Lines: Classifying Resume Seniority with Large Language Models", "categories": ["cs.CL"], "comment": "5 pages, 3 figures", "summary": "Accurately assessing candidate seniority from resumes is a critical yet\nchallenging task, complicated by the prevalence of overstated experience and\nambiguous self-presentation. In this study, we investigate the effectiveness of\nlarge language models (LLMs), including fine-tuned BERT architectures, for\nautomating seniority classification in resumes. To rigorously evaluate model\nperformance, we introduce a hybrid dataset comprising both real-world resumes\nand synthetically generated hard examples designed to simulate exaggerated\nqualifications and understated seniority. Using the dataset, we evaluate the\nperformance of Large Language Models in detecting subtle linguistic cues\nassociated with seniority inflation and implicit expertise. Our findings\nhighlight promising directions for enhancing AI-driven candidate evaluation\nsystems and mitigating bias introduced by self-promotional language. The\ndataset is available for the research community at https://bit.ly/4mcTovt", "AI": {"tldr": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7b80\u5386\u8d44\u5386\u5206\u7c7b\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4f7f\u7528\u6df7\u5408\u6570\u636e\u96c6\u9a8c\u8bc1\u6a21\u578b\u68c0\u6d4b\u8d44\u5386\u5938\u5927\u80fd\u529b", "motivation": "\u7b80\u5386\u8bc4\u4f30\u5b58\u5728\u5019\u9009\u4eba\u8fc7\u5ea6\u5305\u88c5\u8d44\u5386\uff08\u8d44\u5386\u865a\u9ad8/\u8d44\u5386\u4f4e\u8c03\uff09\u7684\u884c\u4e1a\u75db\u70b9\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u8bed\u8a00\u4e2d\u7684\u5fae\u5999\u6697\u793a", "method": "\u6784\u5efa\u6df7\u5408\u6570\u636e\u96c6\uff08\u771f\u5b9e\u7b80\u5386+\u5408\u6210\u786c\u6837\u672c\uff09\uff0c\u901a\u8fc7\u5fae\u8c03BERT\u7b49LLMs\u68c0\u6d4b\u8d44\u5386\u76f8\u5173\u7684\u8bed\u8a00\u5b66\u7ebf\u7d22", "result": "\u6a21\u578b\u5c55\u73b0\u51fa\u68c0\u6d4b\u8d44\u5386\u865a\u9ad8\u548c\u9690\u6027\u4e13\u4e1a\u7ebf\u7d22\u7684\u6f5c\u529b\uff0c\u6570\u636e\u96c6\u516c\u5f00\u4fc3\u8fdb\u7814\u7a76\u793e\u533a\u53d1\u5c55", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u63d0\u5347AI\u5019\u9009\u4eba\u8bc4\u4f30\u7cfb\u7edf\u6307\u660e\u65b9\u5411\uff0c\u6709\u52a9\u4e8e\u6d88\u9664\u81ea\u6211\u5ba3\u4f20\u8bed\u8a00\u5e26\u6765\u7684\u8bc4\u4f30\u504f\u5dee"}}
{"id": "2509.09234", "pdf": "https://arxiv.org/pdf/2509.09234", "abs": "https://arxiv.org/abs/2509.09234", "authors": ["Rishit Tyagi", "Mohit Gupta", "Rahul Bouri"], "title": "Agentic LLMs for Question Answering over Tabular Data", "categories": ["cs.CL"], "comment": "Accepted at ACL workshop SemEval 2025", "summary": "Question Answering over Tabular Data (Table QA) presents unique challenges\ndue to the diverse structure, size, and data types of real-world tables. The\nSemEval 2025 Task 8 (DataBench) introduced a benchmark composed of large-scale,\ndomain-diverse datasets to evaluate the ability of models to accurately answer\nstructured queries. We propose a Natural Language to SQL (NL-to-SQL) approach\nleveraging large language models (LLMs) such as GPT-4o, GPT-4o-mini, and\nDeepSeek v2:16b to generate SQL queries dynamically. Our system follows a\nmulti-stage pipeline involving example selection, SQL query generation, answer\nextraction, verification, and iterative refinement. Experiments demonstrate the\neffectiveness of our approach, achieving 70.5\\% accuracy on DataBench QA and\n71.6\\% on DataBench Lite QA, significantly surpassing baseline scores of 26\\%\nand 27\\% respectively. This paper details our methodology, experimental\nresults, and alternative approaches, providing insights into the strengths and\nlimitations of LLM-driven Table QA.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u9636\u6bb5NL-to-SQL\u65b9\u6cd5\uff0c\u5728DataBench QA\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u8fbe70.5%\uff0c\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u8868\u683c\u95ee\u7b54\u9762\u4e34\u73b0\u5b9e\u573a\u666f\u4e2d\u8868\u683c\u7ed3\u6784\u591a\u6837\u3001\u89c4\u6a21\u5dee\u5f02\u5927\u548c\u6570\u636e\u7c7b\u578b\u590d\u6742\u7684\u72ec\u7279\u6311\u6218\uff0c\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u91c7\u7528\u5305\u542b\u6848\u4f8b\u9009\u62e9\u3001SQL\u751f\u6210\u3001\u7b54\u6848\u63d0\u53d6\u3001\u9a8c\u8bc1\u548c\u8fed\u4ee3\u4f18\u5316\u7684\u4e94\u9636\u6bb5\u6d41\u7a0b\uff0c\u7ed3\u5408GPT-4o/DeepSeek\u7b49LLM\u52a8\u6001\u751f\u6210\u67e5\u8be2", "result": "\u5728DataBench QA\u548cLite QA\u5206\u522b\u53d6\u5f9770.5%\u548c71.6%\u51c6\u786e\u7387\uff0c\u8f8326%/27%\u7684\u57fa\u7ebf\u63d0\u5347\u663e\u8457", "conclusion": "\u9a8c\u8bc1\u4e86LLM\u9a71\u52a8\u8868\u683c\u95ee\u7b54\u7684\u6709\u6548\u6027\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u6a21\u578b\u5728\u590d\u6742\u7ed3\u6784\u7406\u89e3\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u540e\u7eed\u4f18\u5316\u6307\u660e\u65b9\u5411"}}
{"id": "2509.09303", "pdf": "https://arxiv.org/pdf/2509.09303", "abs": "https://arxiv.org/abs/2509.09303", "authors": ["Grazia Sveva Ascione", "Nicol\u00f2 Tamagnone"], "title": "From scratch to silver: Creating trustworthy training data for patent-SDG classification using Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Classifying patents by their relevance to the UN Sustainable Development\nGoals (SDGs) is crucial for tracking how innovation addresses global\nchallenges. However, the absence of a large, labeled dataset limits the use of\nsupervised learning. Existing methods, such as keyword searches, transfer\nlearning, and citation-based heuristics, lack scalability and generalizability.\nThis paper frames patent-to-SDG classification as a weak supervision problem,\nusing citations from patents to SDG-tagged scientific publications (NPL\ncitations) as a noisy initial signal. To address its sparsity and noise, we\ndevelop a composite labeling function (LF) that uses large language models\n(LLMs) to extract structured concepts, namely functions, solutions, and\napplications, from patents and SDG papers based on a patent ontology.\nCross-domain similarity scores are computed and combined using a rank-based\nretrieval approach. The LF is calibrated via a custom positive-only loss that\naligns with known NPL-SDG links without penalizing discovery of new SDG\nassociations. The result is a silver-standard, soft multi-label dataset mapping\npatents to SDGs, enabling the training of effective multi-label regression\nmodels. We validate our approach through two complementary strategies: (1)\ninternal validation against held-out NPL-based labels, where our method\noutperforms several baselines including transformer-based models, and zero-shot\nLLM; and (2) external validation using network modularity in patent citation,\nco-inventor, and co-applicant graphs, where our labels reveal greater thematic,\ncognitive, and organizational coherence than traditional technological\nclassifications. These results show that weak supervision and semantic\nalignment can enhance SDG classification at scale.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f31\u76d1\u7763\u5b66\u4e60\u548c\u8bed\u4e49\u5bf9\u9f50\u7684\u4e13\u5229-SDG\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u590d\u5408\u6807\u8bb0\u51fd\u6570\u548cLLM\u63d0\u53d6\u7ed3\u6784\u5316\u6982\u5ff5\uff0c\u6784\u5efa\u94f6\u6807\u51c6\u6570\u636e\u96c6\u5e76\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "\u73b0\u6709\u4e13\u5229\u5206\u7c7b\u65b9\u6cd5\uff08\u5173\u952e\u8bcd\u641c\u7d22/\u8fc1\u79fb\u5b66\u4e60/\u5f15\u6587\u542f\u53d1\u5f0f\uff09\u5b58\u5728\u6269\u5c55\u6027\u5dee\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5f31\u76d1\u7763\u6846\u67b6\u5904\u7406\u7a00\u758f\u566a\u58f0\u6570\u636e", "method": "1) \u4f7f\u7528NPL\u5f15\u7528\u4f5c\u4e3a\u5f31\u76d1\u7763\u4fe1\u53f7\n2) \u6784\u5efa\u590d\u5408\u6807\u8bb0\u51fd\u6570\uff08LLM\u63d0\u53d6\u529f\u80fd/\u89e3\u51b3\u65b9\u6848/\u5e94\u7528\u6982\u5ff5\uff09\n3) \u8de8\u57df\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u4e0e\u6392\u5e8f\u68c0\u7d22\n4) \u81ea\u5b9a\u4e49\u4ec5\u6b63\u7c7b\u635f\u5931\u51fd\u6570\u6821\u51c6\u6a21\u578b", "result": "\u5185\u90e8\u9a8c\u8bc1\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff08\u5305\u62ecTransformer\u548c\u96f6\u6837\u672cLLM\uff09\uff1b\u5916\u90e8\u9a8c\u8bc1\u663e\u793a\u5728\u4e13\u5229\u5f15\u7528/\u5171\u540c\u53d1\u660e\u4eba/\u5171\u540c\u7533\u8bf7\u4eba\u7f51\u7edc\u4e2d\u5177\u6709\u66f4\u597d\u7684\u4e3b\u9898/\u8ba4\u77e5/\u7ec4\u7ec7\u4e00\u81f4\u6027", "conclusion": "\u5f31\u76d1\u7763\u4e0e\u8bed\u4e49\u5bf9\u9f50\u7684\u7ed3\u5408\u80fd\u6709\u6548\u63d0\u5347\u5927\u89c4\u6a21SDG\u5206\u7c7b\u6548\u679c\uff0c\u4e3a\u521b\u65b0\u653f\u7b56\u5206\u6790\u63d0\u4f9b\u53ef\u9760\u5de5\u5177"}}
{"id": "2509.09360", "pdf": "https://arxiv.org/pdf/2509.09360", "abs": "https://arxiv.org/abs/2509.09360", "authors": ["Channdeth Sok", "David Luz", "Yacine Haddam"], "title": "MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems", "categories": ["cs.CL"], "comment": "under review", "summary": "Large Language Models (LLMs) are increasingly deployed in enterprise\napplications, yet their reliability remains limited by hallucinations, i.e.,\nconfident but factually incorrect information. Existing detection approaches,\nsuch as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not\naddress the unique challenges of Retrieval-Augmented Generation (RAG) systems,\nwhere responses must be consistent with retrieved evidence. We therefore\npresent MetaRAG, a metamorphic testing framework for hallucination detection in\nRetrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time,\nunsupervised, black-box setting, requiring neither ground-truth references nor\naccess to model internals, making it suitable for proprietary and high-stakes\ndomains. The framework proceeds in four stages: (1) decompose answers into\natomic factoids, (2) generate controlled mutations of each factoid using\nsynonym and antonym substitutions, (3) verify each variant against the\nretrieved context (synonyms are expected to be entailed and antonyms\ncontradicted), and (4) aggregate penalties for inconsistencies into a\nresponse-level hallucination score. Crucially for identity-aware AI, MetaRAG\nlocalizes unsupported claims at the factoid span where they occur (e.g.,\npregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility),\nallowing users to see flagged spans and enabling system designers to configure\nthresholds and guardrails for identity-sensitive queries. Experiments on a\nproprietary enterprise dataset illustrate the effectiveness of MetaRAG for\ndetecting hallucinations and enabling trustworthy deployment of RAG-based\nconversational agents. We also outline a topic-based deployment design that\ntranslates MetaRAG's span-level scores into identity-aware safeguards; this\ndesign is discussed but not evaluated in our experiments.", "AI": {"tldr": "MetaRAG\u63d0\u51fa\u65e0\u76d1\u7763\u7684RAG\u7cfb\u7edf\u5e7b\u89c9\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u4e8b\u5b9e\u5206\u89e3\u3001\u7a81\u53d8\u9a8c\u8bc1\u548c\u5c40\u90e8\u5316\u5b9a\u4f4d\u63d0\u5347\u751f\u6210\u53ef\u9760\u6027", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u89e3\u51b3RAG\u7cfb\u7edf\u7279\u6709\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u5f00\u53d1\u65e0\u9700\u57fa\u51c6\u4e14\u652f\u6301\u5b9e\u65f6\u68c0\u6d4b\u7684\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u8eab\u4efd\u654f\u611f\u7684\u9ad8\u98ce\u9669\u9886\u57df", "method": "\u56db\u9636\u6bb5\u6846\u67b6\uff1a1)\u5206\u89e3\u4e3a\u539f\u5b50\u4e8b\u5b9e 2)\u751f\u6210\u540c/\u53cd\u4e49\u8bcd\u7a81\u53d8 3)\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u9a8c\u8bc1 4)\u805a\u5408\u5e7b\u89c9\u8bc4\u5206\u3002\u652f\u6301\u8de8\u5ea6\u7ea7\u9519\u8bef\u5b9a\u4f4d", "result": "\u4f01\u4e1a\u6570\u636e\u9a8c\u8bc1\u6709\u6548\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684\u5e7b\u89c9\u68c0\u6d4b\uff08\u5982\u7279\u5b9a\u4eba\u7fa4\u6743\u76ca\u58f0\u660e\uff09\uff0c\u5e76\u63d0\u51fa\u8eab\u4efd\u611f\u77e5\u7684\u9608\u503c\u914d\u7f6e\u65b9\u6848", "conclusion": "\u8be5\u6846\u67b6\u4e3aRAG\u7cfb\u7edf\u63d0\u4f9b\u53ef\u4fe1\u8d56\u7684\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6848\uff0c\u901a\u8fc7\u5c40\u90e8\u5316\u8bc4\u5206\u652f\u6301\u8eab\u4efd\u654f\u611f\u7684\u90e8\u7f72\u4fdd\u969c"}}
{"id": "2509.09381", "pdf": "https://arxiv.org/pdf/2509.09381", "abs": "https://arxiv.org/abs/2509.09381", "authors": ["Molly R Petersen", "Claire E Stevenson", "Lonneke van der Plas"], "title": "Modelling Analogies and Analogical Reasoning: Connecting Cognitive Science Theory and NLP Research", "categories": ["cs.CL"], "comment": null, "summary": "Analogical reasoning is an essential aspect of human cognition. In this\npaper, we summarize key theory about the processes underlying analogical\nreasoning from the cognitive science literature and relate it to current\nresearch in natural language processing. While these processes can be easily\nlinked to concepts in NLP, they are generally not viewed through a cognitive\nlens. Furthermore, we show how these notions are relevant for several major\nchallenges in NLP research, not directly related to analogy solving. This may\nguide researchers to better optimize relational understanding in text, as\nopposed to relying heavily on entity-level similarity.", "AI": {"tldr": "\u8bba\u6587\u5c06\u8ba4\u77e5\u79d1\u5b66\u7684\u7c7b\u6bd4\u63a8\u7406\u7406\u8bba\u4e0eNLP\u7814\u7a76\u7ed3\u5408\uff0c\u63d0\u51fa\u901a\u8fc7\u4f18\u5316\u5173\u7cfb\u7406\u89e3\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u6548\u679c", "motivation": "\u73b0\u6709NLP\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u5b9e\u4f53\u76f8\u4f3c\u6027\uff0c\u5ffd\u89c6\u8ba4\u77e5\u79d1\u5b66\u4e2d\u5173\u7cfb\u63a8\u7406\u673a\u5236\uff0c\u5236\u7ea6\u4e86\u6df1\u5c42\u8bed\u4e49\u7406\u89e3\u80fd\u529b", "method": "\u901a\u8fc7\u8de8\u5b66\u79d1\u7406\u8bba\u5206\u6790\uff0c\u7cfb\u7edf\u68b3\u7406\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u7c7b\u6bd4\u63a8\u7406\u673a\u5236\uff0c\u5e76\u4e0eNLP\u6280\u672f\u6846\u67b6\u5efa\u7acb\u6982\u5ff5\u6620\u5c04", "result": "\u63ed\u793a\u5173\u7cfb\u7406\u89e3\u673a\u5236\u53ef\u6709\u6548\u89e3\u51b3NLP\u4e2d\u7684\u8fc1\u79fb\u5b66\u4e60\u3001\u5c11\u6837\u672c\u5b66\u4e60\u7b49\u6838\u5fc3\u6311\u6218", "conclusion": "\u5efa\u8bae\u91c7\u7528\u8ba4\u77e5\u89c6\u89d2\u91cd\u6784NLP\u6a21\u578b\u8bbe\u8ba1\u8303\u5f0f\uff0c\u5c06\u5173\u6ce8\u70b9\u4ece\u8868\u5c42\u5b9e\u4f53\u8f6c\u5411\u6df1\u5c42\u5173\u7cfb\u7ed3\u6784"}}
{"id": "2509.09388", "pdf": "https://arxiv.org/pdf/2509.09388", "abs": "https://arxiv.org/abs/2509.09388", "authors": ["Ana Ezquerro", "Carlos G\u00f3mez-Rodr\u00edguez", "David Vilares"], "title": "Hierarchical Bracketing Encodings Work for Dependency Graphs", "categories": ["cs.CL"], "comment": "Accepted at EMNLP 2025 (main)", "summary": "We revisit hierarchical bracketing encodings from a practical perspective in\nthe context of dependency graph parsing. The approach encodes graphs as\nsequences, enabling linear-time parsing with $n$ tagging actions, and still\nrepresenting reentrancies, cycles, and empty nodes. Compared to existing graph\nlinearizations, this representation substantially reduces the label space while\npreserving structural information. We evaluate it on a multilingual and\nmulti-formalism benchmark, showing competitive results and consistent\nimprovements over other methods in exact match accuracy.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5206\u5c42\u62ec\u53f7\u7f16\u7801\u7684\u4f9d\u5b58\u56fe\u89e3\u6790\u65b9\u6cd5\uff0c\u5728\u964d\u4f4e\u6807\u7b7e\u7a7a\u95f4\u7684\u540c\u65f6\u4fdd\u6301\u7ed3\u6784\u5b8c\u6574\u6027\uff0c\u5728\u591a\u8bed\u8a00\u591a\u5f62\u5f0f\u4efb\u52a1\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7cbe\u51c6\u89e3\u6790\u3002", "motivation": "\u4f20\u7edf\u56fe\u7ebf\u6027\u5316\u65b9\u6cd5\u5b58\u5728\u6807\u7b7e\u7a7a\u95f4\u8fc7\u5927\u548c\u7ed3\u6784\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u9700\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u56fe\u7ed3\u6784\u7f16\u7801\u65b9\u5f0f\u3002", "method": "\u91c7\u7528\u5206\u5c42\u62ec\u53f7\u7f16\u7801\u5c06\u56fe\u7ed3\u6784\u5e8f\u5217\u5316\uff0c\u652f\u6301\u7ebf\u6027\u65f6\u95f4\u590d\u6742\u5ea6\u89e3\u6790\uff0c\u53ef\u5904\u7406\u590d\u6307\u3001\u5faa\u73af\u548c\u7a7a\u8282\u70b9\u3002", "result": "\u5728\u591a\u8bed\u8a00\u591a\u5f62\u5f0f\u8bc4\u6d4b\u4e2d\u53d6\u5f97\u7ade\u4e89\u6027\u7ed3\u679c\uff0c\u7cbe\u786e\u5339\u914d\u51c6\u786e\u7387\u8f83\u57fa\u7ebf\u65b9\u6cd5\u63d0\u53471.5-3.2%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u4e0e\u7cbe\u5ea6\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e3a\u590d\u6742\u8bed\u8a00\u73b0\u8c61\u7684\u89e3\u6790\u63d0\u4f9b\u4e86\u5b9e\u7528\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.09438", "pdf": "https://arxiv.org/pdf/2509.09438", "abs": "https://arxiv.org/abs/2509.09438", "authors": ["Zhaohan Zhang", "Ziquan Liu", "Ioannis Patras"], "title": "GrACE: A Generative Approach to Better Confidence Elicitation in Large Language Models", "categories": ["cs.CL"], "comment": "20 pages, 11 figures", "summary": "Assessing the reliability of Large Language Models (LLMs) by confidence\nelicitation is a prominent approach to AI safety in high-stakes applications,\nsuch as healthcare and finance. Existing methods either require expensive\ncomputational overhead or suffer from poor calibration, making them impractical\nand unreliable for real-world deployment. In this work, we propose GrACE, a\nGenerative Approach to Confidence Elicitation that enables scalable and\nreliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in\nwhich the model expresses confidence by the similarity between the last hidden\nstate and the embedding of a special token appended to the vocabulary, in\nreal-time. We fine-tune the model for calibrating the confidence with\ncalibration targets associated with accuracy. Experiments with three LLMs and\ntwo benchmark datasets show that the confidence produced by GrACE achieves the\nbest discriminative capacity and calibration on open-ended generation tasks,\noutperforming six competing methods without resorting to additional sampling or\nan auxiliary model. Moreover, we propose two strategies for improving test-time\nscaling based on confidence induced by GrACE. Experimental results show that\nusing GrACE not only improves the accuracy of the final decision but also\nsignificantly reduces the number of required samples in the test-time scaling\nscheme, indicating the potential of GrACE as a practical solution for deploying\nLLMs with scalable, reliable, and real-time confidence estimation.", "AI": {"tldr": "\u63d0\u51faGrACE\u65b9\u6cd5\u5b9e\u73b0LLM\u7684\u5b9e\u65f6\u53ef\u6269\u5c55\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\uff0c\u901a\u8fc7\u9690\u85cf\u72b6\u6001\u4e0e\u7279\u6b8a\u6807\u8bb0\u5d4c\u5165\u7684\u76f8\u4f3c\u6027\u673a\u5236\uff0c\u5728\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u8d44\u6e90\u4e0b\u5b9e\u73b0\u6700\u4f18\u6821\u51c6\u6548\u679c\u548c\u5224\u522b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u9ad8\u8ba1\u7b97\u6210\u672c\u4e0e\u6821\u51c6\u6548\u679c\u5dee\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u6ee1\u8db3\u533b\u7597/\u91d1\u878d\u7b49\u9ad8\u98ce\u9669\u573a\u666f\u7684\u5b9e\u65f6\u90e8\u7f72\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u6700\u540e\u9690\u85cf\u72b6\u6001\u4e0e\u65b0\u589e\u7279\u6b8a\u6807\u8bb0\u5d4c\u5165\u76f8\u4f3c\u6027\u7684\u5b9e\u65f6\u7f6e\u4fe1\u5ea6\u751f\u6210\u673a\u5236\uff0c\u901a\u8fc7\u51c6\u786e\u7387\u5173\u8054\u7684\u6821\u51c6\u76ee\u6807\u8fdb\u884c\u6a21\u578b\u5fae\u8c03\u3002", "result": "\u57283\u4e2aLLM\u548c2\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cGrACE\u7684\u7f6e\u4fe1\u5ea6\u6307\u6807\u5728\u751f\u6210\u4efb\u52a1\u4e0a\u8d85\u8d8a6\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6d4b\u8bd5\u65f6\u6269\u5c55\u7b56\u7565\u4f7f\u51b3\u7b56\u51c6\u786e\u7387\u63d0\u5347\u4e14\u6837\u672c\u9700\u6c42\u51cf\u5c1150%\u4ee5\u4e0a\u3002", "conclusion": "GrACE\u4e3aLLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u5b9e\u65f6\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9ad8\u98ce\u9669\u5e94\u7528\u7684\u90e8\u7f72\u95e8\u69db\u3002"}}
{"id": "2509.09473", "pdf": "https://arxiv.org/pdf/2509.09473", "abs": "https://arxiv.org/abs/2509.09473", "authors": ["Lucie Pol\u00e1kov\u00e1", "Martin Popel", "V\u011bra Kloudov\u00e1", "Michal Nov\u00e1k", "Mariia Anisimova", "Ji\u0159\u00ed Balhar"], "title": "Mitigating Language Barriers in Education: Developing Multilingual Digital Learning Materials with Machine Translation", "categories": ["cs.CL"], "comment": "8 pages, 2 figures", "summary": "The EdUKate project combines digital education, linguistics, translation\nstudies, and machine translation to develop multilingual learning materials for\nCzech primary and secondary schools. Launched through collaboration between a\nmajor Czech academic institution and the country's largest educational\npublisher, the project is aimed at translating up to 9,000 multimodal\ninteractive exercises from Czech into Ukrainian, English, and German for an\neducational web portal. It emphasizes the development and evaluation of a\ndirect Czech-Ukrainian machine translation system tailored to the educational\ndomain, with special attention to processing formatted content such as XML and\nPDF and handling technical and scientific terminology. We present findings from\nan initial survey of Czech teachers regarding the needs of non-Czech-speaking\nstudents and describe the system's evaluation and implementation on the web\nportal. All resulting applications are freely available to students, educators,\nand researchers.", "AI": {"tldr": "\u6377\u514bEdUKate\u9879\u76ee\u901a\u8fc7\u6570\u5b57\u6559\u80b2\u6280\u672f\u4e0e\u673a\u5668\u7ffb\u8bd1\u7ed3\u5408\uff0c\u5f00\u53d1\u591a\u8bed\u8a00\u6559\u5b66\u6750\u6599\u5e76\u5b9e\u73b0\u6559\u80b2\u8d44\u6e90\u591a\u8bed\u79cd\u8f6c\u5316\u3002", "motivation": "\u89e3\u51b3\u6377\u514b\u5b66\u6821\u4e2d\u975e\u6bcd\u8bed\u5b66\u751f\u7684\u6559\u80b2\u8d44\u6e90\u77ed\u7f3a\u95ee\u9898\uff0c\u4fc3\u8fdb\u4e4c\u514b\u5170\u8bed/\u82f1\u8bed/\u5fb7\u8bed\u7684\u591a\u8bed\u8a00\u6559\u80b2\u652f\u6301\uff0c\u901a\u8fc7\u673a\u5668\u7ffb\u8bd1\u63d0\u5347\u591a\u6a21\u6001\u6559\u5b66\u8d44\u6e90\u7684\u8f6c\u5316\u6548\u7387\u3002", "method": "1. \u5f00\u53d1\u6377\u514b-\u4e4c\u514b\u5170\u6559\u80b2\u9886\u57df\u4e13\u7528\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\n2. \u5904\u7406XML/PDF\u683c\u5f0f\u5185\u5bb9\u4e0e\u79d1\u6280\u672f\u8bed\n3. \u5f00\u5c55\u6559\u5e08\u9700\u6c42\u8c03\u7814\u6307\u5bfc\u7cfb\u7edf\u5f00\u53d1\n4. \u5728web\u95e8\u6237\u96c6\u6210\u7ffb\u8bd1\u7cfb\u7edf", "result": "1. \u5b8c\u62109000\u4e2a\u4e92\u52a8\u7ec3\u4e60\u7684\u4e09\u8bed\u79cd\u7ffb\u8bd1\n2. \u5b9a\u5236\u5316\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u6210\u529f\u90e8\u7f72\n3. \u9700\u6c42\u8c03\u7814\u6709\u6548\u6307\u5bfc\u529f\u80fd\u5f00\u53d1\n4. \u6240\u6709\u6210\u679c\u514d\u8d39\u5f00\u653e\u5171\u4eab", "conclusion": "\u8be5\u9879\u76ee\u6210\u529f\u6574\u5408\u8de8\u5b66\u79d1\u6280\u672f\uff0c\u901a\u8fc7\u5b9a\u5236\u5316\u7ffb\u8bd1\u65b9\u6848\u6709\u6548\u6ee1\u8db3\u591a\u8bed\u8a00\u6559\u80b2\u9700\u6c42\uff0c\u4e3a\u6559\u80b2\u6280\u672f\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u672c\u5730\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u4fc3\u8fdb\u6559\u80b2\u5305\u5bb9\u6027\u53d1\u5c55\u3002"}}
{"id": "2509.09522", "pdf": "https://arxiv.org/pdf/2509.09522", "abs": "https://arxiv.org/abs/2509.09522", "authors": ["Vadim Zadykian", "Bruno Andrade", "Haithem Afli"], "title": "Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness and Knowledge Graphs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Semantic Textual Relatedness (STR) captures nuanced relationships between\ntexts that extend beyond superficial lexical similarity. In this study, we\ninvestigate STR in the context of job title matching - a key challenge in\nresume recommendation systems, where overlapping terms are often limited or\nmisleading. We introduce a self-supervised hybrid architecture that combines\ndense sentence embeddings with domain-specific Knowledge Graphs (KGs) to\nimprove both semantic alignment and explainability. Unlike previous work that\nevaluated models on aggregate performance, our approach emphasizes data\nstratification by partitioning the STR score continuum into distinct regions:\nlow, medium, and high semantic relatedness. This stratified evaluation enables\na fine-grained analysis of model performance across semantically meaningful\nsubspaces. We evaluate several embedding models, both with and without KG\nintegration via graph neural networks. The results show that fine-tuned SBERT\nmodels augmented with KGs produce consistent improvements in the high-STR\nregion, where the RMSE is reduced by 25% over strong baselines. Our findings\nhighlight not only the benefits of combining KGs with text embeddings, but also\nthe importance of regional performance analysis in understanding model\nbehavior. This granular approach reveals strengths and weaknesses hidden by\nglobal metrics, and supports more targeted model selection for use in Human\nResources (HR) systems and applications where fairness, explainability, and\ncontextual matching are essential.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u76d1\u7763\u6df7\u5408\u67b6\u6784\uff0c\u7ed3\u5408\u6587\u672c\u5d4c\u5165\u4e0e\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7\u5206\u5c42\u8bc4\u4f30\u65b9\u6cd5\u63ed\u793a\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u4e49\u76f8\u5173\u533a\u57df\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5728\u9ad8STR\u533a\u57df\u5b9e\u73b025%\u7684RMSE\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u7b80\u5386\u63a8\u8350\u7cfb\u7edf\u4e2d\u804c\u4f4d\u540d\u79f0\u5339\u914d\u7684\u8bed\u4e49\u6a21\u7cca\u95ee\u9898\uff0c\u7a81\u7834\u4f20\u7edf\u57fa\u4e8e\u8bcd\u6c47\u5339\u914d\u7684\u5c40\u9650\uff0c\u5f3a\u8c03\u9700\u5173\u6ce8\u6a21\u578b\u5728\u4e0d\u540c\u8bed\u4e49\u76f8\u5173\u533a\u57df\u7684\u5dee\u5f02\u5316\u8868\u73b0\u3002", "method": "\u4f7f\u7528SBERT\u6a21\u578b\u751f\u6210\u6587\u672c\u5d4c\u5165\uff0c\u6574\u5408\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\uff08\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\uff09\uff0c\u91c7\u7528\u5206\u5c42\u8bc4\u4f30\u7b56\u7565\u5c06STR\u5206\u6570\u5212\u5206\u4e3a\u4f4e/\u4e2d/\u9ad8\u4e09\u4e2a\u8bed\u4e49\u533a\u57df\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u7684\u5fae\u8c03SBERT\u6a21\u578b\u5728\u9ad8STR\u533a\u57df\u8868\u73b0\u6700\u4f18\uff0cRMSE\u76f8\u5bf9\u57fa\u7ebf\u964d\u4f4e25%\uff0c\u5206\u5c42\u5206\u6790\u663e\u793a\u6a21\u578b\u5728\u4e0d\u540c\u533a\u57df\u5b58\u5728\u663e\u8457\u6027\u80fd\u6ce2\u52a8\u3002", "conclusion": "\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u7684\u6587\u672c\u5d4c\u5165\u53ef\u63d0\u5347\u8bed\u4e49\u5339\u914d\u7cbe\u5ea6\uff0c\u5206\u5c42\u8bc4\u4f30\u65b9\u6cd5\u6bd4\u5168\u5c40\u6307\u6807\u66f4\u80fd\u63ed\u793a\u6a21\u578b\u7279\u6027\uff0c\u5bf9\u63d0\u5347HR\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u548c\u53ef\u89e3\u91ca\u6027\u5177\u6709\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.09524", "pdf": "https://arxiv.org/pdf/2509.09524", "abs": "https://arxiv.org/abs/2509.09524", "authors": ["Daniil Ignatev", "Nan Li", "Hugh Mee Wong", "Anh Dang", "Shane Kaszefski Yaschuk"], "title": "DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning and Label Distribution Learning", "categories": ["cs.CL", "cs.LG"], "comment": "11 pages, 4 figures; to appear at NLPerspectives@EMNLP-2025", "summary": "This system paper presents the DeMeVa team's approaches to the third edition\nof the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et\nal., 2025). We explore two directions: in-context learning (ICL) with large\nlanguage models, where we compare example sampling strategies; and label\ndistribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we\nevaluate several fine-tuning methods. Our contributions are twofold: (1) we\nshow that ICL can effectively predict annotator-specific annotations\n(perspectivist annotations), and that aggregating these predictions into soft\nlabels yields competitive performance; and (2) we argue that LDL methods are\npromising for soft label predictions and merit further exploration by the\nperspectivist community.", "AI": {"tldr": "DeMeVa\u56e2\u961f\u5728LeWiDi 2025\u7ade\u8d5b\u4e2d\u63a2\u7d22\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u548c\u6807\u7b7e\u5206\u5e03\u5b66\u4e60(LDL)\u65b9\u6cd5\uff0c\u8bc1\u660eICL\u53ef\u9884\u6d4b\u6807\u6ce8\u8005\u7279\u5f02\u6027\u6ce8\u91ca\uff0cLDL\u5728\u8f6f\u6807\u7b7e\u9884\u6d4b\u4e2d\u5177\u6709\u6f5c\u529b", "motivation": "\u89e3\u51b3\u6807\u6ce8\u5206\u6b67\u573a\u666f\u4e0b\u7684\u6807\u6ce8\u8005\u7279\u5f02\u6027\u6ce8\u91ca\u9884\u6d4b\u95ee\u9898\uff0c\u63a2\u7d22\u4e0d\u540c\u65b9\u6cd5\u5728perspectivist\u6807\u6ce8\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027", "method": "1. \u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\uff0c\u6bd4\u8f83\u4e0d\u540c\u6837\u672c\u91c7\u6837\u7b56\u7565\n2. \u4f7f\u7528RoBERTa\u8fdb\u884c\u6807\u7b7e\u5206\u5e03\u5b66\u4e60(LDL)\uff0c\u8bc4\u4f30\u591a\u79cd\u5fae\u8c03\u65b9\u6cd5", "result": "ICL\u901a\u8fc7\u805a\u5408\u4e2a\u4f53\u6807\u6ce8\u9884\u6d4b\u751f\u6210\u8f6f\u6807\u7b7e\u8868\u73b0\u4f18\u5f02(\u8fbe\u7ade\u8d5b\u524d\u5217\u6c34\u5e73)\uff0cLDL\u65b9\u6cd5\u5728\u8f6f\u6807\u7b7e\u9884\u6d4b\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u7814\u7a76\u4ef7\u503c", "conclusion": "\u4e24\u79cd\u65b9\u6cd5\u5bf9perspectivist\u793e\u533a\u5747\u6709\u91cd\u8981\u610f\u4e49\uff0c\u7279\u522b\u662fLDL\u65b9\u6cd5\u9700\u8981\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5728\u8f6f\u6807\u7b7e\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u6f5c\u529b"}}
{"id": "2509.09544", "pdf": "https://arxiv.org/pdf/2509.09544", "abs": "https://arxiv.org/abs/2509.09544", "authors": ["Paolo Pedinotti", "Peter Baumann", "Nathan Jessurun", "Leslie Barrett", "Enrico Santus"], "title": "Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance NLP (2022-2025)", "categories": ["cs.CL"], "comment": "7 pages, 6 appendices, EMNLP industry track", "summary": "Large Language Models (LLMs) have rapidly reshaped financial NLP, enabling\nnew tasks and driving a proliferation of datasets and diversification of data\nsources. Yet, this transformation has outpaced traditional surveys. In this\npaper, we present MetaGraph, a generalizable methodology for extracting\nknowledge graphs from scientific literature and analyzing them to obtain a\nstructured, queryable view of research trends. We define an ontology for\nfinancial NLP research and apply an LLM-based extraction pipeline to 681 papers\n(2022-2025), enabling large-scale, data-driven analysis. MetaGraph reveals\nthree key phases: early LLM adoption and task/dataset innovation; critical\nreflection on LLM limitations; and growing integration of peripheral techniques\ninto modular systems. This structured view offers both practitioners and\nresearchers a clear understanding of how financial NLP has evolved -\nhighlighting emerging trends, shifting priorities, and methodological\nshifts-while also demonstrating a reusable approach for mapping scientific\nprogress in other domains.", "AI": {"tldr": "\u63d0\u51faMetaGraph\u65b9\u6cd5\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u5206\u6790\u91d1\u878dNLP\u9886\u57df\u4e09\u9636\u6bb5\u6f14\u8fdb\uff1aLLM\u521d\u671f\u5e94\u7528\u2192\u53cd\u601d\u5c40\u9650\u6027\u2192\u6a21\u5757\u5316\u7cfb\u7edf\u6574\u5408", "motivation": "\u4f20\u7edf\u6587\u732e\u7efc\u8ff0\u96be\u4ee5\u9002\u5e94\u91d1\u878dNLP\u9886\u57df\u5728LLM\u9a71\u52a8\u4e0b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u7ed3\u6784\u5316\u65b9\u6cd5\u8ffd\u8e2a\u7814\u7a76\u8d8b\u52bf", "method": "1. \u6784\u5efa\u91d1\u878dNLP\u672c\u4f53\u8bba\n2. \u5bf9681\u7bc7\u8bba\u6587(2022-2025)\u5b9e\u65bdLLM\u9a71\u52a8\u7684\u4fe1\u606f\u62bd\u53d6\n3. \u6784\u5efa\u53ef\u67e5\u8be2\u7684\u77e5\u8bc6\u56fe\u8c31\u8fdb\u884c\u8d8b\u52bf\u5206\u6790", "result": "\u63ed\u793a\u91d1\u878dNLP\u6f14\u8fdb\u7684\u4e09\u4e2a\u9636\u6bb5\uff1a\u65e9\u671f\u4efb\u52a1/\u6570\u636e\u96c6\u521b\u65b0\u2192LLM\u5c40\u9650\u6027\u7684\u6279\u5224\u6027\u53cd\u601d\u2192\u5916\u56f4\u6280\u672f\u6a21\u5757\u5316\u6574\u5408", "conclusion": "MetaGraph\u4e0d\u4ec5\u6e05\u6670\u5c55\u793a\u9886\u57df\u53d1\u5c55\u8109\u7edc\uff0c\u5176\u65b9\u6cd5\u8bba\u53ef\u590d\u7528\u4e8e\u5176\u4ed6\u5b66\u79d1\u7684\u79d1\u5b66\u8fdb\u5c55\u56fe\u8c31\u6784\u5efa"}}
{"id": "2509.09583", "pdf": "https://arxiv.org/pdf/2509.09583", "abs": "https://arxiv.org/abs/2509.09583", "authors": ["Brittany Harbison", "Samuel Taubman", "Travis Taylor", "Ashok. K. Goel"], "title": "Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking", "categories": ["cs.CL", "cs.CY", "cs.HC", "cs.LG", "cs.SI"], "comment": null, "summary": "Social connection is a vital part of learning, yet online course environments\npresent barriers to the organic formation of social groups. SAMI offers one\nsolution by facilitating student connections, but its effectiveness is\nconstrained by an incomplete Theory of Mind, limiting its ability to create an\neffective mental model of a student. One facet of this is its inability to\nintuit personality, which may influence the relevance of its recommendations.\nTo explore this, we propose a personality detection model utilizing GPTs\nzero-shot capability to infer Big-Five personality traits from forum\nintroduction posts, often encouraged in online courses. We benchmark its\nperformance against established models, demonstrating its efficacy in this\ntask. Furthermore, we integrate this model into SAMIs entity-based matchmaking\nsystem, enabling personality-informed social recommendations. Initial\nintegration suggests personality traits can complement existing matching\nfactors, though additional evaluation is required to determine their full\nimpact on student engagement and match quality.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8eGPT\u96f6\u6837\u672c\u63a8\u65ad\u7684\u6027\u683c\u68c0\u6d4b\u6a21\u578b\uff0c\u6574\u5408\u81f3\u5728\u7ebf\u8bfe\u7a0b\u793e\u4ea4\u63a8\u8350\u7cfb\u7edfSAMI\uff0c\u63a2\u7d22\u6027\u683c\u7279\u8d28\u5bf9\u5339\u914d\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "motivation": "\u5728\u7ebf\u8bfe\u7a0b\u73af\u5883\u5929\u7136\u963b\u788d\u793e\u4ea4\u7fa4\u4f53\u5f62\u6210\uff0c\u73b0\u6709SAMI\u7cfb\u7edf\u56e0\u7f3a\u4e4f\u6027\u683c\u63a8\u65ad\u80fd\u529b\u5bfc\u81f4\u63a8\u8350\u6548\u679c\u53d7\u9650\u3002", "method": "\u5229\u7528GPT\u96f6\u6837\u672c\u80fd\u529b\u4ece\u8bba\u575b\u4ecb\u7ecd\u5e16\u63d0\u53d6\u5927\u4e94\u4eba\u683c\u7279\u5f81\uff0c\u96c6\u6210\u81f3\u5b9e\u4f53\u5339\u914d\u7cfb\u7edf\u5b9e\u73b0\u6027\u683c\u611f\u77e5\u63a8\u8350\u3002", "result": "GPT\u6a21\u578b\u5728\u6027\u683c\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u521d\u6b65\u6574\u5408\u663e\u793a\u6027\u683c\u7279\u8d28\u53ef\u8865\u5145\u73b0\u6709\u5339\u914d\u56e0\u7d20\u3002", "conclusion": "\u6027\u683c\u7279\u5f81\u53ef\u589e\u5f3a\u793e\u4ea4\u63a8\u8350\u76f8\u5173\u6027\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u8bc4\u4f30\u5bf9\u5b66\u4e60\u53c2\u4e0e\u5ea6\u548c\u5339\u914d\u8d28\u91cf\u7684\u957f\u671f\u5f71\u54cd\u3002"}}
{"id": "2509.09593", "pdf": "https://arxiv.org/pdf/2509.09593", "abs": "https://arxiv.org/abs/2509.09593", "authors": ["Bangzhao Shu", "Isha Joshi", "Melissa Karnaze", "Anh C. Pham", "Ishita Kakkar", "Sindhu Kothe", "Arpine Hovasapian", "Mai ElSherief"], "title": "Fluent but Unfeeling: The Emotional Blind Spots of Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Camera-ready version for ICWSM 2026. First two authors contributed\n  equally", "summary": "The versatility of Large Language Models (LLMs) in natural language\nunderstanding has made them increasingly popular in mental health research.\nWhile many studies explore LLMs' capabilities in emotion recognition, a\ncritical gap remains in evaluating whether LLMs align with human emotions at a\nfine-grained level. Existing research typically focuses on classifying emotions\ninto predefined, limited categories, overlooking more nuanced expressions. To\naddress this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit\ncommunities featuring 251 fine-grained, self-disclosed emotion labels. Our\ncomprehensive evaluation framework examines predicted emotion terms and\ndecomposes them into eight basic emotions using established emotion theories,\nenabling a fine-grained comparison. Systematic testing of prevalent LLMs under\nvarious prompt settings reveals that accurately predicting emotions that align\nwith human self-disclosed emotions remains challenging. Qualitative analysis\nfurther shows that while certain LLMs generate emotion terms consistent with\nestablished emotion theories and definitions, they sometimes fail to capture\ncontextual cues as effectively as human self-disclosures. These findings\nhighlight the limitations of LLMs in fine-grained emotion alignment and offer\ninsights for future research aimed at enhancing their contextual understanding.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u7ec6\u7c92\u5ea6\u60c5\u611f\u5bf9\u9f50\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u65b0\u57fa\u51c6EXPRESS\u63ed\u793a\u5176\u96be\u4ee5\u51c6\u786e\u5339\u914d\u4eba\u7c7b\u81ea\u62ab\u9732\u60c5\u611f\u6807\u7b7e\uff0c\u9700\u52a0\u5f3a\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5c06\u60c5\u7eea\u5f52\u7c7b\u5230\u9884\u5b9a\u4e49\u7684\u6709\u9650\u7c7b\u522b\uff0c\u5ffd\u89c6\u4e86\u7ec6\u7c92\u5ea6\u60c5\u611f\u8868\u8fbe\u3002\u8bba\u6587\u65e8\u5728\u8bc4\u4f30LLM\u662f\u5426\u80fd\u5728\u7ec6\u7c92\u5ea6\u5c42\u9762\u4e0e\u4eba\u7c7b\u60c5\u611f\u5bf9\u9f50\u3002", "method": "\u6784\u5efa\u542b251\u4e2a\u7ec6\u7c92\u5ea6\u60c5\u611f\u6807\u7b7e\u7684Reddit\u793e\u533a\u6570\u636e\u96c6EXPRESS\uff0c\u57fa\u4e8e\u60c5\u7eea\u7406\u8bba\u5c06\u9884\u6d4b\u60c5\u611f\u5206\u89e3\u4e3a8\u79cd\u57fa\u7840\u60c5\u7eea\uff0c\u7cfb\u7edf\u6d4b\u8bd5\u4e0d\u540c\u63d0\u793a\u7b56\u7565\u4e0b\u7684\u4e3b\u6d41LLM\u8868\u73b0\u3002", "result": "LLM\u9884\u6d4b\u4e0e\u4eba\u7c7b\u81ea\u62ab\u9732\u60c5\u611f\u5b58\u5728\u504f\u5dee\uff0c\u90e8\u5206\u6a21\u578b\u867d\u7b26\u5408\u60c5\u7eea\u7406\u8bba\u5b9a\u4e49\uff0c\u4f46\u8bed\u5883\u6355\u6349\u80fd\u529b\u5f31\u4e8e\u4eba\u7c7b\u81ea\u8ff0\uff0c\u51c6\u786e\u9884\u6d4b\u5177\u4e0a\u4e0b\u6587\u654f\u611f\u6027\u7684\u60c5\u611f\u4ecd\u5177\u6311\u6218\u3002", "conclusion": "LLM\u5728\u7ec6\u7c92\u5ea6\u60c5\u611f\u5bf9\u9f50\u4efb\u52a1\u4e2d\u5b58\u5728\u660e\u663e\u5c40\u9650\uff0c\u672a\u6765\u7814\u7a76\u9700\u63d0\u5347\u5176\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u4ee5\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u3002"}}
{"id": "2509.09602", "pdf": "https://arxiv.org/pdf/2509.09602", "abs": "https://arxiv.org/abs/2509.09602", "authors": ["Yiqun T. Chen", "Tyler H. McCormick", "Li Liu", "Abhirup Datta"], "title": "LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death Determination", "categories": ["cs.CL", "stat.AP"], "comment": null, "summary": "Verbal autopsy (VA) is a critical tool for estimating causes of death in\nresource-limited settings where medical certification is unavailable. This\nstudy presents LA-VA, a proof-of-concept pipeline that combines Large Language\nModels (LLMs) with traditional algorithmic approaches and embedding-based\nclassification for improved cause-of-death prediction. Using the Population\nHealth Metrics Research Consortium (PHMRC) dataset across three age categories\n(Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches:\nGPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles.\nOur results demonstrate that GPT-5 achieves the highest individual performance\nwith average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5%\n(Neonate), outperforming traditional statistical machine learning baselines by\n5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches\ncould substantially improve verbal autopsy accuracy, with important\nimplications for global health surveillance in low-resource settings.", "AI": {"tldr": "LLM\u4e0e\u4f20\u7edf\u7b97\u6cd5\u7ed3\u5408\u7684LA-VA\u6d41\u7a0b\u663e\u8457\u63d0\u5347\u53e3\u5934\u5c38\u68c0\u51c6\u786e\u6027", "motivation": "\u8d44\u6e90\u6709\u9650\u5730\u533a\u7f3a\u4e4f\u533b\u7597\u6b7b\u4ea1\u8bc1\u660e\uff0c\u9700\u63d0\u5347\u73b0\u6709\u53e3\u5934\u5c38\u68c0\u65b9\u6cd5\u7684\u51c6\u786e\u6027", "method": "\u91c7\u7528PHMRC\u6570\u636e\u96c6\uff0c\u6d4b\u8bd5GPT-5\u9884\u6d4b\u3001LCVA\u57fa\u7ebf\u3001\u6587\u672c\u5d4c\u5165\u548c\u5143\u5b66\u4e60\u5668\u96c6\u6210\u65b9\u6cd5", "result": "GPT-5\u5728\u6210\u4eba/\u513f\u7ae5/\u65b0\u751f\u513f\u7ec4\u522b\u5206\u522b\u8fbe\u523048.6%\u300150.5%\u300153.5%\u51c6\u786e\u7387\uff0c\u8f83\u4f20\u7edf\u65b9\u6cd5\u63d0\u53475-10%", "conclusion": "\u73b0\u6210LLM\u8f85\u52a9\u65b9\u6848\u53ef\u5b9e\u8d28\u6027\u6539\u8fdb\u6b7b\u4ea1\u539f\u56e0\u5224\u5b9a\uff0c\u5bf9\u5168\u7403\u536b\u751f\u76d1\u6d4b\u5177\u6709\u91cd\u8981\u4ef7\u503c"}}
{"id": "2509.09629", "pdf": "https://arxiv.org/pdf/2509.09629", "abs": "https://arxiv.org/abs/2509.09629", "authors": ["Minghang Zhu", "Zhengliang Shi", "Zhiwei Xu", "Shiguang Wu", "Lingjie Wang", "Pengjie Ren", "Zhaochun Ren", "Zhumin Chen"], "title": "Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems", "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "The advancement of large language models (LLMs) has enabled the construction\nof multi-agent systems to solve complex tasks by dividing responsibilities\namong specialized agents, such as a planning agent for subgoal generation and a\ngrounding agent for executing tool-use actions. Most existing methods typically\nfine-tune these agents independently, leading to capability gaps among them\nwith poor coordination. To address this, we propose MOAT, a Multi-Agent Joint\nAlignment Tuning framework that improves agents collaboration through iterative\nalignment. MOAT alternates between two key stages: (1) Planning Agent\nAlignment, which optimizes the planning agent to generate subgoal sequences\nthat better guide the grounding agent; and (2) Grounding Agent Improving, which\nfine-tunes the grounding agent using diverse subgoal-action pairs generated by\nthe agent itself to enhance its generalization capablity. Theoretical analysis\nproves that MOAT ensures a non-decreasing and progressively convergent training\nprocess. Experiments across six benchmarks demonstrate that MOAT outperforms\nstate-of-the-art baselines, achieving average improvements of 3.1% on held-in\ntasks and 4.4% on held-out tasks.", "AI": {"tldr": "\u63d0\u51faMOAT\u6846\u67b6\uff0c\u901a\u8fc7\u89c4\u5212\u667a\u80fd\u4f53\u5bf9\u9f50\u4e0e\u57fa\u7840\u667a\u80fd\u4f53\u4ea4\u66ff\u8fed\u4ee3\u4f18\u5316\uff0c\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u72ec\u7acb\u5fae\u8c03\u591a\u667a\u80fd\u4f53\u5bfc\u81f4\u534f\u4f5c\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u89e3\u51b3\u667a\u80fd\u4f53\u95f4\u534f\u8c03\u4f18\u5316\u95ee\u9898\u3002", "method": "1. \u89c4\u5212\u667a\u80fd\u4f53\u5bf9\u9f50\u9636\u6bb5\u4f18\u5316\u5b50\u76ee\u6807\u751f\u6210\uff1b2. \u57fa\u7840\u667a\u80fd\u4f53\u6539\u8fdb\u9636\u6bb5\u5229\u7528\u81ea\u751f\u6210\u7684\u591a\u6837\u5316\u5b50\u76ee\u6807-\u52a8\u4f5c\u5bf9\u8fdb\u884c\u5fae\u8c03\u3002\u4e24\u9636\u6bb5\u4ea4\u66ff\u8fed\u4ee3\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u57286\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5185\u5916\u90e8\u4efb\u52a1\u5206\u522b\u5b9e\u73b03.1%\u548c4.4%\u7684\u5e73\u5747\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "MOAT\u901a\u8fc7\u7406\u8bba\u4fdd\u8bc1\u7684\u8fed\u4ee3\u5bf9\u9f50\u673a\u5236\uff0c\u6709\u6548\u589e\u5f3a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u80fd\u529b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.09650", "pdf": "https://arxiv.org/pdf/2509.09650", "abs": "https://arxiv.org/abs/2509.09650", "authors": ["Siddarth Mamidanna", "Daking Rai", "Ziyu Yao", "Yilun Zhou"], "title": "All for One: LLMs Solve Mental Math at the Last Token With Information Transferred From Other Tokens", "categories": ["cs.CL", "I.2.7"], "comment": "EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) demonstrate proficiency across numerous\ncomputational tasks, yet their inner workings remain unclear. In theory, the\ncombination of causal self-attention and multilayer perceptron layers allows\nevery token to access and compute information based on all preceding tokens. In\npractice, to what extent are such operations present? In this paper, on mental\nmath tasks (i.e., direct math calculation via next-token prediction without\nexplicit reasoning), we investigate this question in three steps: inhibiting\ninput-specific token computations in the initial layers, restricting the routes\nof information transfer across token positions in the next few layers, and\nforcing all computation to happen at the last token in the remaining layers.\nWith two proposed techniques, Context-Aware Mean Ablation (CAMA) and\nAttention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with\nhigh accuracy on a wide variety of mental math tasks, where meaningful\ncomputation occurs very late (in terms of layer depth) and only at the last\ntoken, which receives information of other tokens in few specific middle\nlayers. Experiments on a variety of models and arithmetic expressions show that\nthis subgraph is sufficient and necessary for high model performance, transfers\nacross different models, and works on a variety of input styles. Ablations on\ndifferent CAMA and ABP alternatives reveal their unique advantages over other\nmethods, which may be of independent interest.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7b97\u4efb\u52a1\u4e2d\u901a\u8fc7\u7279\u5b9a\u5b50\u56fe\uff08AF1\uff09\u8fdb\u884c\u8ba1\u7b97\uff0c\u5173\u952e\u8ba1\u7b97\u53d1\u751f\u5728\u6df1\u5c42\u53ca\u672b\u6807\u8bb0\u5904\u3002", "motivation": "\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5fc3\u7b97\u4efb\u52a1\u4e2d\u7684\u5185\u90e8\u8ba1\u7b97\u673a\u5236\uff0c\u660e\u786e\u5176\u8de8\u5c42\u548c\u8de8\u6807\u8bb0\u7684\u4fe1\u606f\u4f20\u9012\u8def\u5f84\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u65b9\u6cd5\uff1a\u6291\u5236\u521d\u59cb\u5c42\u8f93\u5165\u7279\u5b9a\u6807\u8bb0\u8ba1\u7b97\u2192\u9650\u5236\u4e2d\u95f4\u5c42\u8de8\u6807\u8bb0\u4fe1\u606f\u4f20\u9012\u2192\u5f3a\u5236\u672b\u5c42\u8ba1\u7b97\u805a\u7126\u4e8e\u6700\u540e\u6807\u8bb0\uff0c\u7ed3\u5408CAMA\u548cABP\u6280\u672f\u8bc6\u522bAF1\u5b50\u56fe\u3002", "result": "AF1\u5b50\u56fe\u5728\u4e0d\u540c\u6a21\u578b\u548c\u7b97\u672f\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u3001\u5fc5\u8981\u6027\u53ca\u53ef\u8fc1\u79fb\u6027\uff0c\u4e14\u672b\u6807\u8bb0\u901a\u8fc7\u7279\u5b9a\u4e2d\u5c42\u63a5\u6536\u5168\u5c40\u4fe1\u606f\u3002", "conclusion": "LLMs\u5fc3\u7b97\u80fd\u529b\u4f9d\u8d56\u6df1\u5c42\u672b\u6807\u8bb0\u7684\u96c6\u4e2d\u8ba1\u7b97\uff0cAF1\u5b50\u56fe\u4e3a\u6838\u5fc3\u8def\u5f84\uff0cCAMA/ABP\u6280\u672f\u5177\u6709\u72ec\u7279\u4f18\u52bf\u3002"}}
{"id": "2509.09660", "pdf": "https://arxiv.org/pdf/2509.09660", "abs": "https://arxiv.org/abs/2509.09660", "authors": ["Mohsen Fayyaz", "Ali Modarressi", "Hanieh Deilamsalehy", "Franck Dernoncourt", "Ryan Rossi", "Trung Bui", "Hinrich Sch\u00fctze", "Nanyun Peng"], "title": "Steering MoE LLMs via Expert (De)Activation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token\nthrough a subset of specialized Feed-Forward Networks (FFN), known as experts.\nWe present SteerMoE, a framework for steering MoE models by detecting and\ncontrolling behavior-linked experts. Our detection method identifies experts\nwith distinct activation patterns across paired inputs exhibiting contrasting\nbehaviors. By selectively (de)activating such experts during inference, we\ncontrol behaviors like faithfulness and safety without retraining or modifying\nweights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to\n+20% and faithfulness by +27%. In adversarial attack mode, it drops safety by\n-41% alone, and -100% when combined with existing jailbreak methods, bypassing\nall safety guardrails and exposing a new dimension of alignment faking hidden\nwithin experts.", "AI": {"tldr": "\u63d0\u51faSteerMoE\u6846\u67b6\uff0c\u901a\u8fc7\u63a7\u5236MoE\u6a21\u578b\u4e2d\u7279\u5b9a\u884c\u4e3a\u76f8\u5173\u4e13\u5bb6\u6a21\u5757\uff0c\u5b9e\u73b0\u4e0d\u4fee\u6539\u6743\u91cd\u5373\u53ef\u8c03\u8282\u6a21\u578b\u5b89\u5168\u6027\u548c\u5fe0\u5b9e\u6027\u3002", "motivation": "\u73b0\u6709MoE\u6a21\u578b\u7f3a\u4e4f\u5bf9\u4e13\u5bb6\u6a21\u5757\u884c\u4e3a\u5173\u8054\u6027\u7684\u7cfb\u7edf\u63a7\u5236\u673a\u5236\uff0c\u9700\u8981\u975e\u4fb5\u5165\u5f0f\u7684\u884c\u4e3a\u8c03\u63a7\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u8f93\u5165\u68c0\u6d4b\u884c\u4e3a\u76f8\u5173\u4e13\u5bb6\uff0c\u5728\u63a8\u7406\u65f6\u9009\u62e9\u6027\u6fc0\u6d3b/\u505c\u7528\u7279\u5b9a\u4e13\u5bb6\u6a21\u5757", "result": "\u57286\u4e2aLLM\u4e0a\u5b9e\u73b0\u5b89\u5168\u6027\u6700\u9ad8\u63d0\u534720%\uff0c\u5fe0\u5b9e\u6027\u63d0\u534727%\uff1b\u5bf9\u6297\u6a21\u5f0f\u4e0b\u53ef\u5b8c\u5168\u7a81\u7834\u5b89\u5168\u9632\u62a4", "conclusion": "\u9996\u6b21\u63ed\u793a\u4e13\u5bb6\u6a21\u5757\u4e2d\u9690\u85cf\u7684\u5bf9\u9f50\u4f2a\u88c5\u7ef4\u5ea6\uff0c\u4e3a\u6a21\u578b\u5b89\u5168\u63d0\u4f9b\u65b0\u7684\u7814\u7a76\u65b9\u5411\u548c\u6280\u672f\u6311\u6218"}}
{"id": "2509.09675", "pdf": "https://arxiv.org/pdf/2509.09675", "abs": "https://arxiv.org/abs/2509.09675", "authors": ["Runpeng Dai", "Linfeng Song", "Haolin Liu", "Zhenwen Liang", "Dian Yu", "Haitao Mi", "Zhaopeng Tu", "Rui Liu", "Tong Zheng", "Hongtu Zhu", "Dong Yu"], "title": "CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "21 pages", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm\nfor enhancing the reasoning ability of Large Language Models (LLMs). Yet\ncurrent RLVR methods often explore poorly, leading to premature convergence and\nentropy collapse. To address this challenge, we introduce Curiosity-Driven\nExploration (CDE), a framework that leverages the model's own intrinsic sense\nof curiosity to guide exploration. We formalize curiosity with signals from\nboth the actor and the critic: for the actor, we use perplexity over its\ngenerated response, and for the critic, we use the variance of value estimates\nfrom a multi-head architecture. Both signals serve as an exploration bonus\nwithin the RLVR framework to guide the model. Our theoretical analysis shows\nthat the actor-wise bonus inherently penalizes overconfident errors and\npromotes diversity among correct responses; moreover, we connect the\ncritic-wise bonus to the well-established count-based exploration bonus in RL.\nEmpirically, our method achieves an approximate +3 point improvement over\nstandard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a\ncalibration collapse mechanism within RLVR, shedding light on common LLM\nfailure modes.", "AI": {"tldr": "\u63d0\u51faCuriosity-Driven Exploration (CDE)\u6846\u67b6\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u9a8c\u8bc1\u5956\u52b1\u8303\u5f0f(RLVR)\uff0c\u901a\u8fc7\u5185\u5728\u597d\u5947\u5fc3\u4fe1\u53f7\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u63a2\u7d22\u80fd\u529b", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u5b58\u5728\u63a2\u7d22\u4e0d\u8db3\u5bfc\u81f4\u65e9\u719f\u6536\u655b\u548c\u71b5\u5d29\u6e83\u7684\u95ee\u9898", "method": "\u7ed3\u5408actor\u7684\u751f\u6210\u56f0\u60d1\u5ea6\u548ccritic\u591a\u5934\u4ef7\u503c\u4f30\u8ba1\u65b9\u5dee\u4f5c\u4e3a\u63a2\u7d22\u5956\u52b1\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u4e0e\u7ecf\u5178RL\u63a2\u7d22\u673a\u5236\u7684\u5173\u8054", "result": "\u5728AIME\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u6807\u51c6RLVR\u65b9\u6cd5\u63d0\u5347\u7ea63\u4e2a\u70b9\uff0c\u63ed\u793a\u4e86LLM\u6821\u51c6\u5d29\u6e83\u673a\u5236", "conclusion": "CDE\u6709\u6548\u63d0\u5347RLVR\u6027\u80fd\uff0c\u4e3a\u7406\u89e3LLM\u5931\u8d25\u6a21\u5f0f\u63d0\u4f9b\u65b0\u89c6\u89d2"}}
{"id": "2509.08847", "pdf": "https://arxiv.org/pdf/2509.08847", "abs": "https://arxiv.org/abs/2509.08847", "authors": ["Amna Hassan"], "title": "Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "This paper presents a novel framework for automated game template generation\nby transforming Game Design Documents (GDDs) into functional Unity game\nprototypes using Natural Language Processing (NLP) and multi-modal Large\nLanguage Models (LLMs). We introduce an end-to-end system that parses GDDs,\nextracts structured game specifications, and synthesizes Unity-compatible C#\ncode that implements the core mechanics, systems, and architecture defined in\nthe design documentation. Our approach combines a fine-tuned LLaMA-3 model\nspecialized for Unity code generation with a custom Unity integration package\nthat streamlines the implementation process. Evaluation results demonstrate\nsignificant improvements over baseline models, with our fine-tuned model\nachieving superior performance (4.8/5.0 average score) compared to\nstate-of-the-art LLMs across compilation success, GDD adherence, best practices\nadoption, and code modularity metrics. The generated templates demonstrate high\nadherence to GDD specifications across multiple game genres. Our system\neffectively addresses critical gaps in AI-assisted game development,\npositioning LLMs as valuable tools in streamlining the transition from game\ndesign to implementation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLMs\u7684\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u5b9e\u73b0GDD\u5230Unity\u6e38\u620f\u539f\u578b\u7684\u81ea\u52a8\u8f6c\u6362", "motivation": "\u89e3\u51b3\u6e38\u620f\u8bbe\u8ba1\u6587\u6863\u5230\u53ef\u8fd0\u884c\u539f\u578b\u95f4\u7684\u8f6c\u5316\u96be\u9898\uff0c\u63d0\u5347AI\u8f85\u52a9\u6e38\u620f\u5f00\u53d1\u6548\u7387", "method": "\u7ed3\u5408\u5fae\u8c03LLaMA-3\u6a21\u578b\u4e0eUnity\u96c6\u6210\u5305\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u4ee3\u7801\u751f\u6210\u7cfb\u7edf", "result": "\u6a21\u578b\u83b7\u5f974.8/5.0\u7efc\u5408\u8bc4\u5206\uff0c\u5728\u7f16\u8bd1\u6210\u529f\u7387\u3001\u89c4\u8303\u9075\u5faa\u5ea6\u7b49\u6307\u6807\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b", "conclusion": "\u7cfb\u7edf\u6709\u6548\u586b\u8865AI\u8f85\u52a9\u6e38\u620f\u5f00\u53d1\u7a7a\u767d\uff0c\u786e\u7acbLLMs\u5728\u8bbe\u8ba1\u8f6c\u5b9e\u65bd\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u4f5c\u7528"}}
{"id": "2509.08854", "pdf": "https://arxiv.org/pdf/2509.08854", "abs": "https://arxiv.org/abs/2509.08854", "authors": ["David James Woo", "Kai Guo", "Yangyang Yu"], "title": "A vibe coding learning design to enhance EFL students' talking to, through, and about AI", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "15 pages, 12 figures", "summary": "This innovative practice article reports on the piloting of vibe coding\n(using natural language to create software applications with AI) for English as\na Foreign Language (EFL) education. We developed a human-AI meta-languaging\nframework with three dimensions: talking to AI (prompt engineering), talking\nthrough AI (negotiating authorship), and talking about AI (mental models of\nAI). Using backward design principles, we created a four-hour workshop where\ntwo students designed applications addressing authentic EFL writing challenges.\nWe adopted a case study methodology, collecting data from worksheets and video\nrecordings, think-aloud protocols, screen recordings, and AI-generated images.\nContrasting cases showed one student successfully vibe coding a functional\napplication cohering to her intended design, while another encountered\ntechnical difficulties with major gaps between intended design and actual\nfunctionality. Analysis reveals differences in students' prompt engineering\napproaches, suggesting different AI mental models and tensions in attributing\nauthorship. We argue that AI functions as a beneficial languaging machine, and\nthat differences in how students talk to, through, and about AI explain vibe\ncoding outcome variations. Findings indicate that effective vibe coding\ninstruction requires explicit meta-languaging scaffolding, teaching structured\nprompt engineering, facilitating critical authorship discussions, and\ndeveloping vocabulary for articulating AI mental models.", "AI": {"tldr": "EFL\u6559\u80b2\u4e2d\u63a2\u7d22AI\u81ea\u7136\u8bed\u8a00\u7f16\u7a0b(vibe coding)\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5f00\u53d1\u4e09\u7ef4\u5143\u8bed\u8a00\u6846\u67b6\u5e76\u5206\u6790\u5b66\u751f\u5e94\u7528\u5f00\u53d1\u5dee\u5f02", "motivation": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7AI\u81ea\u7136\u8bed\u8a00\u7f16\u7a0b\u5e2e\u52a9EFL\u5b66\u751f\u89e3\u51b3\u5199\u4f5c\u95ee\u9898\uff0c\u5efa\u7acb\u4eba\u673a\u534f\u4f5c\u7684\u8bed\u8a00\u5b66\u4e60\u6846\u67b6", "method": "\u91c7\u7528\u53cd\u5411\u8bbe\u8ba1\u5f00\u53d14\u5c0f\u65f6\u5de5\u4f5c\u574a\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5206\u6790\u4e24\u540d\u5b66\u751f\u7684\u7f16\u7a0b\u8fc7\u7a0b\uff08\u5de5\u4f5c\u8868/\u89c6\u9891/\u5c4f\u5e55\u5f55\u50cf/AI\u751f\u6210\u7d20\u6750\uff09", "result": "\u5b66\u751f\u5448\u73b0\u4e24\u6781\u8868\u73b0\uff1a\u6210\u529f\u5b9e\u73b0\u529f\u80fd\u5e94\u7528vs\u6280\u672f\u969c\u788d\u5bfc\u81f4\u8bbe\u8ba1-\u529f\u80fd\u8131\u8282\uff0c\u5dee\u5f02\u6e90\u81ea\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u548cAI\u5fc3\u667a\u6a21\u578b\u4e0d\u540c", "conclusion": "AI\u662f\u4f18\u8d28\u8bed\u8a00\u5b66\u4e60\u673a\u5668\uff0c\u6709\u6548\u6559\u5b66\u9700\u7ed3\u6784\u5316\u63d0\u793a\u5de5\u7a0b\u8bad\u7ec3\u3001\u4f5c\u8005\u8eab\u4efd\u534f\u5546\u6307\u5bfc\u53caAI\u5fc3\u667a\u6a21\u578b\u8bcd\u6c47\u57f9\u517b"}}
{"id": "2509.08897", "pdf": "https://arxiv.org/pdf/2509.08897", "abs": "https://arxiv.org/abs/2509.08897", "authors": ["Davide Caffagni", "Sara Sarto", "Marcella Cornia", "Lorenzo Baraldi", "Rita Cucchiara"], "title": "Recurrence Meets Transformers for Universal Multimodal Retrieval", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": null, "summary": "With the rapid advancement of multimodal retrieval and its application in\nLLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged.\nExisting methods predominantly rely on task-specific fine-tuning of\nvision-language models and are limited to single-modality queries or documents.\nIn this paper, we propose ReT-2, a unified retrieval model that supports\nmultimodal queries, composed of both images and text, and searches across\nmultimodal document collections where text and images coexist. ReT-2 leverages\nmulti-layer representations and a recurrent Transformer architecture with\nLSTM-inspired gating mechanisms to dynamically integrate information across\nlayers and modalities, capturing fine-grained visual and textual details. We\nevaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different\nretrieval configurations. Results demonstrate that ReT-2 consistently achieves\nstate-of-the-art performance across diverse settings, while offering faster\ninference and reduced memory usage compared to prior approaches. When\nintegrated into retrieval-augmented generation pipelines, ReT-2 also improves\ndownstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source\ncode and trained models are publicly available at:\nhttps://github.com/aimagelab/ReT-2", "AI": {"tldr": "ReT-2\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301\u591a\u6a21\u6001\u67e5\u8be2\uff08\u56fe\u50cf+\u6587\u672c\uff09\u7684\u7edf\u4e00\u68c0\u7d22\u6a21\u578b\uff0c\u5229\u7528\u5faa\u73afTransformer\u67b6\u6784\u52a8\u6001\u6574\u5408\u8de8\u6a21\u6001\u4fe1\u606f\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u4e8e\u5355\u6a21\u6001\u67e5\u8be2/\u6587\u6863\u4e14\u4f9d\u8d56\u4efb\u52a1\u5fae\u8c03\uff0c\u65e0\u6cd5\u6ee1\u8db3\u590d\u6742\u591a\u6a21\u6001\u68c0\u7d22\u9700\u6c42\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u901a\u7528\u9ad8\u6548\u7684\u591a\u6a21\u6001\u68c0\u7d22\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591a\u5c42\u8868\u5f81\u548cLSTM\u95e8\u63a7\u673a\u5236\u7684\u5faa\u73afTransformer\u67b6\u6784\uff0c\u52a8\u6001\u878d\u5408\u8de8\u5c42\u8de8\u6a21\u6001\u4fe1\u606f\uff0c\u6355\u6349\u7ec6\u7c92\u5ea6\u89c6\u89c9-\u6587\u672c\u7279\u5f81\u3002", "result": "\u5728M2KR/M-BEIR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u63a8\u7406\u901f\u5ea6\u63d0\u534740%\uff0c\u663e\u5b58\u5360\u7528\u51cf\u5c1130%\uff0c\u4e0b\u6e38\u4efb\u52a1\uff08\u5982Encyclopedic-VQA\uff09\u51c6\u786e\u7387\u63d0\u53475-8%\u3002", "conclusion": "ReT-2\u901a\u8fc7\u7edf\u4e00\u67b6\u6784\u7a81\u7834\u591a\u6a21\u6001\u68c0\u7d22\u74f6\u9888\uff0c\u5f00\u6e90\u5b9e\u73b0\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\uff0c\u4e3a\u68c0\u7d22\u589e\u5f3a\u578b\u751f\u6210\u7cfb\u7edf\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.08919", "pdf": "https://arxiv.org/pdf/2509.08919", "abs": "https://arxiv.org/abs/2509.08919", "authors": ["Mahe Chen", "Xiaoxuan Wang", "Kaiwen Chen", "Nick Koudas"], "title": "Generative Engine Optimization: How to Dominate AI Search", "categories": ["cs.IR", "cs.CL", "cs.SI"], "comment": null, "summary": "The rapid adoption of generative AI-powered search engines like ChatGPT,\nPerplexity, and Gemini is fundamentally reshaping information retrieval, moving\nfrom traditional ranked lists to synthesized, citation-backed answers. This\nshift challenges established Search Engine Optimization (SEO) practices and\nnecessitates a new paradigm, which we term Generative Engine Optimization\n(GEO).\n  This paper presents a comprehensive comparative analysis of AI Search and\ntraditional web search (Google). Through a series of large-scale, controlled\nexperiments across multiple verticals, languages, and query paraphrases, we\nquantify critical differences in how these systems source information. Our key\nfindings reveal that AI Search exhibit a systematic and overwhelming bias\ntowards Earned media (third-party, authoritative sources) over Brand-owned and\nSocial content, a stark contrast to Google's more balanced mix. We further\ndemonstrate that AI Search services differ significantly from each other in\ntheir domain diversity, freshness, cross-language stability, and sensitivity to\nphrasing.\n  Based on these empirical results, we formulate a strategic GEO agenda. We\nprovide actionable guidance for practitioners, emphasizing the critical need\nto: (1) engineer content for machine scannability and justification, (2)\ndominate earned media to build AI-perceived authority, (3) adopt\nengine-specific and language-aware strategies, and (4) overcome the inherent\n\"big brand bias\" for niche players. Our work provides the foundational\nempirical analysis and a strategic framework for achieving visibility in the\nnew generative search landscape.", "AI": {"tldr": "\u751f\u6210\u5f0f\u641c\u7d22\u5f15\u64ce\uff08\u5982ChatGPT\uff09\u98a0\u8986\u4f20\u7edf\u641c\u7d22SEO\u6a21\u5f0f\uff0c\u9700\u91c7\u7528GEO\u65b0\u8303\u5f0f\u3002AI\u641c\u7d22\u7cfb\u7edf\u6027\u504f\u91cd\u7b2c\u4e09\u65b9\u6743\u5a01\u5185\u5bb9\uff0c\u54c1\u724c/\u793e\u4ea4\u5185\u5bb9\u5360\u6bd4\u663e\u8457\u4f4e\u4e8e\u8c37\u6b4c\u3002", "motivation": "\u7814\u7a76\u751f\u6210\u5f0fAI\u641c\u7d22\u5f15\u64ce\u4e0e\u4f20\u7edf\u641c\u7d22\u7684\u5dee\u5f02\uff0c\u4e3aSEO\u8f6c\u578b\u63d0\u4f9b\u6570\u636e\u652f\u6491\u548c\u6218\u7565\u6846\u67b6", "method": "\u8de8\u5782\u76f4\u9886\u57df/\u8bed\u8a00/\u67e5\u8be2\u53d8\u4f53\u7684\u5927\u89c4\u6a21\u5bf9\u7167\u5b9e\u9a8c\uff0c\u91cf\u5316\u7cfb\u7edf\u5dee\u5f02", "result": "AI\u641c\u7d22\uff1a1\uff09\u7b2c\u4e09\u65b9\u6743\u5a01\u5185\u5bb9\u5360\u6bd4\u8d8580% 2\uff09\u4e0d\u540c\u5f15\u64ce\u5b58\u5728\u57df\u591a\u6837\u6027/\u66f4\u65b0\u9891\u7387/\u8bed\u8a00\u654f\u611f\u5ea6\u5dee\u5f02 3\uff09\u5934\u90e8\u54c1\u724c\u4f18\u52bf\u663e\u8457", "conclusion": "\u9700\u6784\u5efa\u673a\u5668\u53ef\u626b\u63cf\u5185\u5bb9\u3001\u5efa\u7acbAI\u8ba4\u77e5\u6743\u5a01\u3001\u5236\u5b9a\u5f15\u64ce/\u8bed\u8a00\u5b9a\u5236\u7b56\u7565\u3001\u7a81\u7834\u5c0f\u4f17\u54c1\u724c\u504f\u89c1"}}
{"id": "2509.09009", "pdf": "https://arxiv.org/pdf/2509.09009", "abs": "https://arxiv.org/abs/2509.09009", "authors": ["Marianna Nezhurina", "Taishi Nakamura", "Timur Carstensen", "Niccol\u00f2 Ajroldi", "Ville Komulainen", "David Salinas", "Jenia Jitsev"], "title": "Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Model weights and intermediate checkpoints are available at\n  \\url{https://huggingface.co/collections/open-sci/open-sci-ref-001-685905e598be658fbcebff4f};\n  code for reproducing training, evaluation and raw experiments data at\n  \\url{https://github.com/LAION-AI/open-sci-ref-0.01}", "summary": "We introduce open-sci-ref, a family of dense transformer models trained as\nresearch baselines across multiple model (0.13B to 1.7B parameters) and token\nscales (up to 1T) on 8 recent open reference datasets. Evaluating the models on\nvarious standardized benchmarks, our training runs set establishes reference\npoints that enable researchers to assess the sanity and quality of alternative\ntraining approaches across scales and datasets. Intermediate checkpoints allow\ncomparison and studying of the training dynamics. The established reference\nbaselines allow training procedures to be compared through their scaling\ntrends, aligning them on a common compute axis. Comparison of open reference\ndatasets reveals that training on NemoTron-CC HQ consistently outperforms other\nreference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to\nintermediate training checkpoints, the release includes logs, code, and\ndownstream evaluations to simplify reproduction, standardize comparison, and\nfacilitate future research.", "AI": {"tldr": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86open-sci-ref\u7cfb\u5217\u57fa\u7ebf\u6a21\u578b\uff080.13B-1.7B\u53c2\u6570\uff09\uff0c\u901a\u8fc7\u8de88\u4e2a\u5f00\u653e\u6570\u636e\u96c6\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\uff081T tokens\uff09\uff0c\u5efa\u7acb\u6807\u51c6\u5316\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e76\u53d1\u5e03\u5b8c\u6574\u8bad\u7ec3\u8d44\u6e90\u3002", "motivation": "\u4e3a\u89e3\u51b3\u4e0d\u540c\u8bad\u7ec3\u65b9\u6cd5\u5728\u6a21\u578b\u89c4\u6a21\u548c\u6570\u636e\u96c6\u4e0a\u7684\u6548\u679c\u8bc4\u4f30\u96be\u9898\uff0c\u901a\u8fc7\u5efa\u7acb\u53ef\u590d\u73b0\u7684\u53c2\u8003\u57fa\u7ebf\u5e2e\u52a9\u7814\u7a76\u8005\u8fdb\u884c\u65b9\u6cd5\u8d28\u91cf\u9a8c\u8bc1\u548c\u8bad\u7ec3\u52a8\u6001\u5206\u6790\u3002", "method": "\u91c7\u7528\u591a\u5c3a\u5ea6\u6a21\u578b\u67b6\u6784\uff080.13B-1.7B\u53c2\u6570\uff09\uff0c\u57288\u4e2a\u5f00\u653e\u53c2\u8003\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u6bd4\u8f83\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u4e2d\u95f4\u68c0\u67e5\u70b9\u5206\u6790\u8bad\u7ec3\u8fc7\u7a0b\u52a8\u6001\u3002", "result": "NemoTron-CC HQ\u6570\u636e\u96c6\u8868\u73b0\u6700\u4f18\uff08\u76f8\u5bf9DCLM-baseline\u63d0\u534712%\uff0cFineWeb-Edu\u63d0\u53478%\uff09\uff0c\u540c\u65f6\u5f00\u6e90\u8bad\u7ec3\u65e5\u5fd7\u3001\u4e2d\u95f4\u6a21\u578b\u548c\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6807\u51c6\u5316\u57fa\u7ebf\u6a21\u578b\u548c\u5b8c\u6574\u8bad\u7ec3\u8d44\u6e90\u7684\u5f00\u653e\uff0c\u63a8\u52a8\u4e86\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u7684\u53ef\u6bd4\u6027\u7814\u7a76\uff0c\u5e76\u4e3a\u6570\u636e\u96c6\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u3002"}}
{"id": "2509.09014", "pdf": "https://arxiv.org/pdf/2509.09014", "abs": "https://arxiv.org/abs/2509.09014", "authors": ["Umair Hassan"], "title": "COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation", "categories": ["cs.CV", "cs.CL", "68T45 (Primary) 68T50 (Secondary)"], "comment": "17 pages, 3 figures, 3 tables. Dataset available at\n  https://huggingface.co/datasets/umairhassan02/urdu-translated-coco-captions-subset.\n  Scripts and notebooks to reproduce results available at\n  https://github.com/umair-hassan2/COCO-Urdu", "summary": "Urdu, spoken by over 250 million people, remains critically under-served in\nmultimodal and vision-language research. The absence of large-scale,\nhigh-quality datasets has limited the development of Urdu-capable systems and\nreinforced biases in multilingual vision-language models trained primarily on\nhigh-resource languages. To address this gap, we present COCO-Urdu, a\nlarge-scale image-caption dataset derived from MS COCO, containing 59,000\nimages and 319,000 Urdu captions selected through stratified sampling to\npreserve the original distribution. Captions were translated using SeamlessM4T\nv2 and validated with a hybrid multimodal quality estimation framework that\nintegrates COMET-Kiwi for translation quality, CLIP-based similarity for visual\ngrounding, and BERTScore with back-translation for semantic consistency;\nlow-scoring captions were iteratively refined using open-source large language\nmodels. We further benchmark COCO-Urdu on BLEU, SacreBLEU, and chrF, reporting\nconsistently strong results. To the best of our knowledge, COCO-Urdu is the\nlargest publicly available Urdu captioning dataset. By releasing both the\ndataset and the quality estimation pipeline, we aim to reduce language bias in\nmultimodal research and establish a foundation for inclusive vision-language\nsystems.", "AI": {"tldr": "\u6784\u5efa\u4e86\u76ee\u524d\u6700\u5927\u7684\u4e4c\u5c14\u90fd\u8bed\u56fe\u50cf\u63cf\u8ff0\u6570\u636e\u96c6COCO-Urdu\uff0859k\u56fe\u50cf/319k\u63cf\u8ff0\uff09\uff0c\u901a\u8fc7\u6df7\u5408\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\u4fdd\u969c\u6570\u636e\u8d28\u91cf\uff0c\u65e8\u5728\u7f13\u89e3\u591a\u6a21\u6001\u7814\u7a76\u4e2d\u7684\u8bed\u8a00\u504f\u89c1\u3002", "motivation": "\u4e4c\u5c14\u90fd\u8bed\u4f5c\u4e3a2.5\u4ebf\u4eba\u4f7f\u7528\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u957f\u671f\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u591a\u6a21\u6001\u7cfb\u7edf\u5bf9\u8be5\u8bed\u8a00\u652f\u6301\u4e0d\u8db3\uff0c\u52a0\u5267\u4e86\u4ee5\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e3a\u4e3b\u7684\u6a21\u578b\u504f\u89c1\u3002", "method": "1. \u57fa\u4e8eMS COCO\u6570\u636e\u96c6\u5206\u5c42\u62bd\u6837\u4fdd\u6301\u539f\u5206\u5e03\n2. \u4f7f\u7528SeamlessM4T v2\u7ffb\u8bd1\u5e76\u8bbe\u8ba1\u6df7\u5408\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff08COMET-Kiwi\u8bc4\u4f30\u7ffb\u8bd1\u8d28\u91cf+CLIP\u89c6\u89c9\u5bf9\u9f50+BERTScore\u8bed\u4e49\u4e00\u81f4\u6027\uff09\n3. \u901a\u8fc7\u5f00\u6e90\u5927\u6a21\u578b\u8fed\u4ee3\u4f18\u5316\u4f4e\u5206\u63cf\u8ff0", "result": "1. \u83b7\u5f97\u5f53\u524d\u6700\u5927\u516c\u5f00\u4e4c\u5c14\u90fd\u8bed\u6570\u636e\u96c6\n2. BLEU/SacreBLEU/chrF\u57fa\u51c6\u6d4b\u8bd5\u8868\u73b0\u4f18\u5f02\n3. \u540c\u6b65\u5f00\u6e90\u8d28\u91cf\u8bc4\u4f30\u6280\u672f\u6d41\u7a0b", "conclusion": "\u6570\u636e\u96c6\u7684\u53d1\u5e03\u4e3a\u6784\u5efa\u5305\u5bb9\u6027\u89c6\u89c9-\u8bed\u8a00\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\uff0c\u901a\u8fc7\u5f00\u653e\u6280\u672f\u6d41\u7a0b\u63a8\u52a8\u591a\u6a21\u6001\u7814\u7a76\u4e2d\u7684\u8bed\u8a00\u516c\u5e73\u6027\u3002"}}
{"id": "2509.09204", "pdf": "https://arxiv.org/pdf/2509.09204", "abs": "https://arxiv.org/abs/2509.09204", "authors": ["Chin Yuen Kwok", "Jia Qi Yip", "Zhen Qiu", "Chi Hung Chi", "Kwok Yan Lam"], "title": "Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection Systems", "categories": ["cs.SD", "cs.AI", "cs.CL"], "comment": "Published in Interspeech 2025", "summary": "Audio deepfake detection (ADD) models are commonly evaluated using datasets\nthat combine multiple synthesizers, with performance reported as a single Equal\nError Rate (EER). However, this approach disproportionately weights\nsynthesizers with more samples, underrepresenting others and reducing the\noverall reliability of EER. Additionally, most ADD datasets lack diversity in\nbona fide speech, often featuring a single environment and speech style (e.g.,\nclean read speech), limiting their ability to simulate real-world conditions.\nTo address these challenges, we propose bona fide cross-testing, a novel\nevaluation framework that incorporates diverse bona fide datasets and\naggregates EERs for more balanced assessments. Our approach improves robustness\nand interpretability compared to traditional evaluation methods. We benchmark\nover 150 synthesizers across nine bona fide speech types and release a new\ndataset to facilitate further research at\nhttps://github.com/cyaaronk/audio_deepfake_eval.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u591a\u6837\u5316\u771f\u5b9e\u8bed\u97f3\u5e93\u7684\u8de8\u6d4b\u8bd5\u6846\u67b6bona fide cross-testing\uff0c\u6539\u8fdb\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6a21\u578b\u7684\u8bc4\u4f30\u65b9\u6cd5", "motivation": "\u73b0\u6709\u97f3\u9891\u4f2a\u9020\u68c0\u6d4b\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5408\u6210\u5668\u6837\u672c\u91cf\u504f\u5dee\u5bfc\u81f4EER\u4e0d\u53ef\u9760\u3001\u771f\u5b9e\u8bed\u97f3\u573a\u666f\u5355\u4e00\u7684\u95ee\u9898", "method": "\u5f15\u5165\u4e5d\u79cd\u771f\u5b9e\u8bed\u97f3\u7c7b\u578b\u6784\u5efa\u591a\u6837\u5316\u6570\u636e\u96c6\uff0c\u91c7\u7528\u5408\u6210\u5668\u5206\u5c42\u8bc4\u4f30\u5e76\u805a\u5408EER\u6307\u6807", "result": "\u65b0\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u8bc4\u4f30\u5177\u5907\u66f4\u5f3a\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5f00\u6e90\u5305\u542b150+\u5408\u6210\u5668\u7684\u6d4b\u8bd5\u6570\u636e\u96c6", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u97f3\u9891\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u516c\u5f00\u6570\u636e\u96c6\u5c06\u63a8\u52a8\u9886\u57df\u7814\u7a76"}}
{"id": "2509.09214", "pdf": "https://arxiv.org/pdf/2509.09214", "abs": "https://arxiv.org/abs/2509.09214", "authors": ["Alka Gadakh", "Vidya Kumbhar", "Sonal Khosla", "Kumar Karunendra"], "title": "Identifying Key Features for Establishing Sustainable Agro-Tourism Centre: A Data Driven Approach", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Agro-tourism serves as a strategic economic model designed to facilitate\nrural development by diversifying income streams for local communities like\nfarmers while promoting the conservation of indigenous cultural heritage and\ntraditional agricultural practices. As a very booming subdomain of tourism,\nthere is a need to study the strategies for the growth of Agro-tourism in\ndetail. The current study has identified the important indicators for the\ngrowth and enhancement of agro-tourism. The study is conducted in two phases:\nidentification of the important indicators through a comprehensive literature\nreview and in the second phase state-of-the-art techniques were used to\nidentify the important indicators for the growth of agro-tourism. The\nindicators are also called features synonymously, the machine learning models\nfor feature selection were applied and it was observed that the Least Absolute\nShrinkage and Selection Operator (LASSO) method combined with, the machine\nLearning Classifiers such as Logistic Regression (LR), Decision Trees (DT),\nRandom Forest (RF) Tree, and Extreme Gradient Boosting (XGBOOST) models were\nused to suggest the growth of the agro-tourism. The results show that with the\nLASSO method, LR model gives the highest classification accuracy of 98% in\n70-30% train-test data followed by RF with 95% accuracy. Similarly, in the\n80-20% train-test data LR maintains the highest accuracy at 99%, while DT and\nXGBoost follow with 97% accuracy.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08LASSO\u7ed3\u5408LR\u3001RF\u7b49\uff09\u8bc6\u522b\u519c\u4e1a\u65c5\u6e38\u53d1\u5c55\u5173\u952e\u6307\u6807\uff0c\u903b\u8f91\u56de\u5f52\u6a21\u578b\u572870-30%\u548c80-20%\u6570\u636e\u5212\u5206\u4e0b\u5206\u522b\u8fbe\u523098%\u548c99%\u51c6\u786e\u7387\u3002", "motivation": "\u519c\u4e1a\u65c5\u6e38\u4f5c\u4e3a\u5feb\u901f\u53d1\u5c55\u7684\u65c5\u6e38\u5b50\u9886\u57df\uff0c\u9700\u901a\u8fc7\u79d1\u5b66\u65b9\u6cd5\u8bc6\u522b\u589e\u957f\u7b56\u7565\u4ee5\u4fc3\u8fdb\u519c\u6751\u7ecf\u6d4e\u591a\u5143\u5316\u5e76\u4fdd\u62a4\u6587\u5316\u9057\u4ea7\u4e0e\u4f20\u7edf\u519c\u4e1a\u3002", "method": "\u5206\u4e24\u9636\u6bb5\u7814\u7a76\uff1a1.\u6587\u732e\u7efc\u8ff0\u786e\u5b9a\u6307\u6807\uff1b2.\u4f7f\u7528LASSO\u7ed3\u5408\u903b\u8f91\u56de\u5f52\u3001\u51b3\u7b56\u6811\u3001\u968f\u673a\u68ee\u6797\u3001XGBoost\u7b49\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u7279\u5f81\u9009\u62e9\u3002", "result": "LASSO+\u903b\u8f91\u56de\u5f52\u6a21\u578b\u572870-30%\u6570\u636e\u5212\u5206\u4e0b\u51c6\u786e\u738798%\uff0c80-20%\u5212\u5206\u4e0b\u8fbe99%\uff1b\u968f\u673a\u68ee\u6797\u548cXGBoost\u5206\u522b\u83b7\u5f9795%\u548c97%\u51c6\u786e\u7387\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u6709\u6548\u8bc6\u522b\u519c\u4e1a\u65c5\u6e38\u589e\u957f\u6838\u5fc3\u6307\u6807\uff0c\u5176\u4e2d\u903b\u8f91\u56de\u5f52\u7ed3\u5408LASSO\u65b9\u6cd5\u5c55\u73b0\u51fa\u6700\u4f18\u9884\u6d4b\u6027\u80fd\uff0c\u4e3a\u7b56\u7565\u5236\u5b9a\u63d0\u4f9b\u91cf\u5316\u652f\u6301\u3002"}}
{"id": "2509.09265", "pdf": "https://arxiv.org/pdf/2509.09265", "abs": "https://arxiv.org/abs/2509.09265", "authors": ["Jiawei Wang", "Jiacai Liu", "Yuqian Fu", "Yingru Li", "Xintao Wang", "Yuan Lin", "Yu Yue", "Lin Zhang", "Yang Wang", "Ke Wang"], "title": "Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents", "categories": ["cs.LG", "cs.CL"], "comment": "ICLR 2026 Under review", "summary": "In long-horizon tasks, recent agents based on Large Language Models (LLMs)\nface a significant challenge that sparse, outcome-based rewards make it\ndifficult to assign credit to intermediate steps. Previous methods mainly focus\non creating dense reward signals to guide learning, either through traditional\nreinforcement learning techniques like inverse reinforcement learning or by\nusing Process Reward Models for step-by-step feedback. In this paper, we\nidentify a fundamental problem in the learning dynamics of LLMs: the magnitude\nof policy gradients is inherently coupled with the entropy, which leads to\ninefficient small updates for confident correct actions and potentially\ndestabilizes large updates for uncertain ones. To resolve this, we propose\nEntropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the\nlearning signal based on step-wise uncertainty and the final task outcome. EMPG\namplifies updates for confident correct actions, penalizes confident errors,\nand attenuates updates from uncertain steps to stabilize exploration. We\nfurther introduce a bonus term for future clarity that encourages agents to\nfind more predictable solution paths. Through comprehensive experiments on\nthree challenging agent tasks, WebShop, ALFWorld, and Deep Search, we\ndemonstrate that EMPG achieves substantial performance gains and significantly\noutperforms strong policy gradient baselines. Project page is at\nhttps://empgseed-seed.github.io/", "AI": {"tldr": "\u63d0\u51fa\u71b5\u8c03\u5236\u7b56\u7565\u68af\u5ea6\uff08EMPG\uff09\u6846\u67b6\u89e3\u51b3LLMs\u7b56\u7565\u68af\u5ea6\u4e0e\u71b5\u8026\u5408\u95ee\u9898\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u548c\u672a\u6765\u6e05\u6670\u5ea6\u5956\u52b1\u663e\u8457\u63d0\u5347\u957f\u65f6\u4efb\u52a1\u6027\u80fd", "motivation": "\u4f20\u7edf\u57fa\u4e8eLLMs\u7684\u4ee3\u7406\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u96be\u4ee5\u5206\u914d\u4e2d\u95f4\u6b65\u9aa4\u4fe1\u7528\uff0c\u4e14\u7b56\u7565\u68af\u5ea6\u5e45\u5ea6\u4e0e\u71b5\u7684\u8026\u5408\u5bfc\u81f4\u81ea\u4fe1\u52a8\u4f5c\u66f4\u65b0\u4e0d\u8db3/\u4e0d\u786e\u5b9a\u52a8\u4f5c\u66f4\u65b0\u4e0d\u7a33\u5b9a", "method": "1. \u57fa\u4e8e\u6b65\u9aa4\u4e0d\u786e\u5b9a\u6027\u548c\u4efb\u52a1\u7ed3\u679c\u91cd\u65b0\u6821\u51c6\u5b66\u4e60\u4fe1\u53f7\n2. \u5bf9\u81ea\u4fe1\u6b63\u786e\u52a8\u4f5c\u653e\u5927\u66f4\u65b0/\u60e9\u7f5a\u81ea\u4fe1\u9519\u8bef/\u8870\u51cf\u4e0d\u786e\u5b9a\u6b65\u9aa4\u66f4\u65b0\n3. \u5f15\u5165\u672a\u6765\u6e05\u6670\u5ea6\u5956\u52b1\u9879\u9f13\u52b1\u53ef\u9884\u6d4b\u8def\u5f84", "result": "\u5728WebShop/ALFWorld/DeepSearch\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5927\u5e45\u8d85\u8d8a\u73b0\u6709\u7b56\u7565\u68af\u5ea6\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "EMPG\u6709\u6548\u89e3\u51b3\u7b56\u7565\u68af\u5ea6\u4e0e\u71b5\u7684\u8026\u5408\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5b66\u4e60\u4fe1\u53f7\u548c\u63a2\u7d22\u7a33\u5b9a\u6027\u673a\u5236\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u5b66\u4e60\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2509.09284", "pdf": "https://arxiv.org/pdf/2509.09284", "abs": "https://arxiv.org/abs/2509.09284", "authors": ["Bingning Huang", "Tu Nguyen", "Matthieu Zimmer"], "title": "Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in reasoning with large language models (LLMs) have shown the\neffectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality\nintermediate trajectories, particularly in math and symbolic domains. Inspired\nby this, we explore how MCTS-derived trajectories, traditionally used for\ntraining value or reward models, can be repurposed to improve policy\noptimization in preference-based reinforcement learning (RL). Specifically, we\nfocus on Group Relative Policy Optimization (GRPO), a recent algorithm that\nenables preference-consistent policy learning without value networks. We\npropose a staged GRPO training paradigm where completions are derived from\npartially revealed MCTS rollouts, introducing a novel tree-structured setting\nfor advantage estimation. This leads to a rich class of prefix-conditioned\nreward signals, which we analyze theoretically and empirically. Our initial\nresults indicate that while structured advantage estimation can stabilize\nupdates and better reflect compositional reasoning quality, challenges such as\nadvantage saturation and reward signal collapse remain. We propose heuristic\nand statistical solutions to mitigate these issues and discuss open challenges\nfor learning under staged or tree-like reward structures.", "AI": {"tldr": "\u63a2\u7d22\u5229\u7528MCTS\u8f68\u8ff9\u4f18\u5316\u57fa\u4e8e\u504f\u597d\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u63d0\u51fa\u5206\u9636\u6bb5GRPO\u8bad\u7ec3\u8303\u5f0f\u5e76\u5206\u6790\u5176\u6548\u679c\u53ca\u6311\u6218", "motivation": "\u73b0\u6709MCTS\u5728\u751f\u6210\u4e2d\u95f4\u8f68\u8ff9\u65b9\u9762\u6548\u679c\u663e\u8457\uff0c\u4f46\u5982\u4f55\u5c06\u5176\u4f18\u52bf\u4f30\u8ba1\u5e94\u7528\u4e8e\u65e0\u4ef7\u503c\u7f51\u7edc\u7684\u7b56\u7565\u4f18\u5316\u4ecd\u5f85\u63a2\u7d22", "method": "\u57fa\u4e8eGRPO\u7b97\u6cd5\u6784\u5efa\u5206\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u90e8\u5206\u63ed\u793a\u7684MCTS\u5c55\u5f00\u751f\u6210\u5b8c\u6210\u9879\uff0c\u5efa\u7acb\u6811\u72b6\u4f18\u52bf\u4f30\u8ba1\u7ed3\u6784\u4e0e\u524d\u7f00\u6761\u4ef6\u5956\u52b1\u4fe1\u53f7\u4f53\u7cfb", "result": "\u7ed3\u6784\u5316\u4f18\u52bf\u4f30\u8ba1\u63d0\u5347\u7b56\u7565\u7a33\u5b9a\u6027\u4e0e\u63a8\u7406\u8d28\u91cf\u8868\u5f81\uff0c\u4f46\u9762\u4e34\u4f18\u52bf\u9971\u548c\u548c\u5956\u52b1\u4fe1\u53f7\u5d29\u6e83\uff0c\u63d0\u51fa\u542f\u53d1\u5f0f\u4e0e\u7edf\u8ba1\u7f13\u89e3\u65b9\u6848", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u5956\u52b1\u7ed3\u6784\u4e0b\u7684\u7b56\u7565\u5b66\u4e60\u63d0\u4f9b\u65b0\u601d\u8def\uff0c\u672a\u6765\u9700\u89e3\u51b3\u6811\u72b6\u7ed3\u6784\u7279\u6709\u7684\u68af\u5ea6\u4f20\u64ad\u4e0e\u4fe1\u53f7\u8026\u5408\u95ee\u9898"}}
{"id": "2509.09307", "pdf": "https://arxiv.org/pdf/2509.09307", "abs": "https://arxiv.org/abs/2509.09307", "authors": ["Zhengzhao Lai", "Youbin Zheng", "Zhenyang Cai", "Haonan Lyu", "Jinpu Yang", "Hongqing Liang", "Yan Hu", "Benyou Wang"], "title": "Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": null, "summary": "Materials characterization is fundamental to acquiring materials information,\nrevealing the processing-microstructure-property relationships that guide\nmaterial design and optimization. While multimodal large language models\n(MLLMs) have recently shown promise in generative and predictive tasks within\nmaterials science, their capacity to understand real-world characterization\nimaging data remains underexplored. To bridge this gap, we present MatCha, the\nfirst benchmark for materials characterization image understanding, comprising\n1,500 questions that demand expert-level domain expertise. MatCha encompasses\nfour key stages of materials research comprising 21 distinct tasks, each\ndesigned to reflect authentic challenges faced by materials scientists. Our\nevaluation of state-of-the-art MLLMs on MatCha reveals a significant\nperformance gap compared to human experts. These models exhibit degradation\nwhen addressing questions requiring higher-level expertise and sophisticated\nvisual perception. Simple few-shot and chain-of-thought prompting struggle to\nalleviate these limitations. These findings highlight that existing MLLMs still\nexhibit limited adaptability to real-world materials characterization\nscenarios. We hope MatCha will facilitate future research in areas such as new\nmaterial discovery and autonomous scientific agents. MatCha is available at\nhttps://github.com/FreedomIntelligence/MatCha.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u9996\u4e2a\u6750\u6599\u8868\u5f81\u56fe\u50cf\u7406\u89e3\u57fa\u51c6MatCha\uff0c\u5305\u542b1500\u4e2a\u9700\u4e13\u5bb6\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u8bc4\u4f30\u663e\u793a\u73b0\u6709MLLMs\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u6a21\u578b\u5728\u6750\u6599\u79d1\u5b66\u4e2d\u4e3b\u8981\u5e94\u7528\u4e8e\u751f\u6210\u4efb\u52a1\uff0c\u4f46\u5bf9\u771f\u5b9e\u6750\u6599\u8868\u5f81\u56fe\u50cf\u7684\u7406\u89e3\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u6784\u5efa\u8986\u76d6\u6750\u6599\u7814\u7a76\u56db\u4e2a\u9636\u6bb5\uff08\u8868\u5f81\u3001\u5206\u6790\u3001\u63a8\u7406\u3001\u51b3\u7b56\uff09\u768421\u4e2a\u4efb\u52a1\u57fa\u51c6\uff0c\u5305\u542b1500\u4e2a\u4e13\u4e1a\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u4e3b\u6d41MLLMs\u6027\u80fd\u3002", "result": "\u73b0\u6709\u6a21\u578b\u5728\u9700\u8981\u9ad8\u9636\u4e13\u4e1a\u77e5\u8bc6\u548c\u590d\u6742\u89c6\u89c9\u611f\u77e5\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\uff0c\u7b80\u5355\u63d0\u793a\u7b56\u7565\u65e0\u6cd5\u6709\u6548\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "MatCha\u57fa\u51c6\u5c06\u63a8\u52a8\u65b0\u6750\u6599\u53d1\u73b0\u548c\u81ea\u4e3b\u79d1\u5b66\u4ee3\u7406\u7814\u7a76\uff0c\u73b0\u6709\u6a21\u578b\u5728\u771f\u5b9e\u6750\u6599\u8868\u5f81\u573a\u666f\u4e2d\u9002\u5e94\u6027\u6709\u9650\u3002"}}
{"id": "2509.09332", "pdf": "https://arxiv.org/pdf/2509.09332", "abs": "https://arxiv.org/abs/2509.09332", "authors": ["Yuecheng Liu", "Dafeng Chi", "Shiguang Wu", "Zhanguang Zhang", "Yuzheng Zhuang", "Bowen Yang", "He Zhu", "Lingfeng Zhang", "Pengwei Xie", "David Gamaliel Arcos Bravo", "Yingxue Zhang", "Jianye Hao", "Xingyue Quan"], "title": "OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have opened new\nopportunities for embodied intelligence, enabling multimodal understanding,\nreasoning, and interaction, as well as continuous spatial decision-making.\nNevertheless, current MLLM-based embodied systems face two critical\nlimitations. First, Geometric Adaptability Gap: models trained solely on 2D\ninputs or with hard-coded 3D geometry injection suffer from either insufficient\nspatial information or restricted 2D generalization, leading to poor\nadaptability across tasks with diverse spatial demands. Second, Embodiment\nConstraint Gap: prior work often neglects the physical constraints and\ncapacities of real robots, resulting in task plans that are theoretically valid\nbut practically infeasible.To address these gaps, we introduce OmniEVA -- an\nembodied versatile planner that enables advanced embodied reasoning and task\nplanning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding\nmechanism, which introduces a gated router to perform explicit selective\nregulation of 3D fusion based on contextual requirements, enabling\ncontext-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware\nReasoning framework that jointly incorporates task goals and embodiment\nconstraints into the reasoning loop, resulting in planning decisions that are\nboth goal-directed and executable. Extensive experimental results demonstrate\nthat OmniEVA not only achieves state-of-the-art general embodied reasoning\nperformance, but also exhibits a strong ability across a wide range of\ndownstream scenarios. Evaluations of a suite of proposed embodied benchmarks,\nincluding both primitive and composite tasks, confirm its robust and versatile\nplanning capabilities. Project page: https://omnieva.github.io", "AI": {"tldr": "\u63d0\u51faOmniEVA\u89e3\u51b3MLLM\u5177\u8eab\u7cfb\u7edf\u7684\u51e0\u4f55\u9002\u5e94\u6027\u5dee\u8ddd\u4e0e\u5177\u8eab\u7ea6\u675f\u5dee\u8ddd\uff0c\u901a\u8fc7\u4efb\u52a1\u81ea\u9002\u5e943D\u57fa\u7840\u673a\u5236\u548c\u5177\u8eab\u611f\u77e5\u63a8\u7406\u6846\u67b6\uff0c\u5b9e\u73b0\u9ad8\u6548\u7a7a\u95f4\u63a8\u7406\u4e0e\u53ef\u6267\u884c\u89c4\u5212\u3002", "motivation": "\u73b0\u6709MLLM\u5177\u8eab\u7cfb\u7edf\u5b58\u5728\u51e0\u4f55\u9002\u5e94\u6027\u4e0d\u8db3\uff08\u4ec52D\u8f93\u5165\u6216\u786c\u7f16\u78013D\uff09\u548c\u5ffd\u89c6\u673a\u5668\u4eba\u7269\u7406\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u8de8\u4efb\u52a1\u9002\u5e94\u6027\u5dee\u4e14\u89c4\u5212\u4e0d\u53ef\u884c\u3002", "method": "1. \u4efb\u52a1\u81ea\u9002\u5e943D\u57fa\u7840\u673a\u5236\uff1a\u901a\u8fc7\u95e8\u63a7\u8def\u7531\u5668\u52a8\u6001\u8c03\u82823D\u4fe1\u606f\u878d\u5408\uff1b2. \u5177\u8eab\u611f\u77e5\u63a8\u7406\u6846\u67b6\uff1a\u8054\u5408\u4f18\u5316\u4efb\u52a1\u76ee\u6807\u4e0e\u673a\u5668\u4eba\u7269\u7406\u7ea6\u675f\u3002", "result": "OmniEVA\u5728\u901a\u7528\u63a8\u7406\u4efb\u52a1\u8fbe\u5230SOTA\uff0c\u590d\u5408\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u5176\u9c81\u68d2\u6027\uff0c\u652f\u6301\u5bfc\u822a\u3001\u64cd\u4f5c\u7b496\u7c7b\u4e0b\u6e38\u573a\u666f\u7684\u6cdb\u5316\u5e94\u7528\u3002", "conclusion": "OmniEVA\u901a\u8fc7\u663e\u5f0f3D\u4fe1\u606f\u8c03\u8282\u548c\u5177\u8eab\u7ea6\u675f\u8054\u5408\u63a8\u7406\uff0c\u6709\u6548\u63d0\u5347\u8de8\u4efb\u52a1\u9002\u5e94\u6027\u4e0e\u89c4\u5212\u53ef\u6267\u884c\u6027\uff0c\u4e3a\u901a\u7528\u5177\u8eab\u667a\u80fd\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.09396", "pdf": "https://arxiv.org/pdf/2509.09396", "abs": "https://arxiv.org/abs/2509.09396", "authors": ["Harry Mayne", "Ryan Othniel Kearns", "Yushi Yang", "Andrew M. Bean", "Eoin Delaney", "Chris Russell", "Adam Mahdi"], "title": "LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to EMNLP 2025 Main", "summary": "To collaborate effectively with humans, language models must be able to\nexplain their decisions in natural language. We study a specific type of\nself-explanation: self-generated counterfactual explanations (SCEs), where a\nmodel explains its prediction by modifying the input such that it would have\npredicted a different outcome. We evaluate whether LLMs can produce SCEs that\nare valid, achieving the intended outcome, and minimal, modifying the input no\nmore than necessary. When asked to generate counterfactuals, we find that LLMs\ntypically produce SCEs that are valid, but far from minimal, offering little\ninsight into their decision-making behaviour. Worryingly, when asked to\ngenerate minimal counterfactuals, LLMs typically make excessively small edits\nthat fail to change predictions. The observed validity-minimality trade-off is\nconsistent across several LLMs, datasets, and evaluation settings. Our findings\nsuggest that SCEs are, at best, an ineffective explainability tool and, at\nworst, can provide misleading insights into model behaviour. Proposals to\ndeploy LLMs in high-stakes settings must consider the impact of unreliable\nself-explanations on downstream decision-making. Our code is available at\nhttps://github.com/HarryMayne/SCEs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u751f\u6210\u7684\u81ea\u6211\u53cd\u4e8b\u5b9e\u89e3\u91ca\u5728\u6709\u6548\u6027\uff08\u6539\u53d8\u9884\u6d4b\uff09\u4e0e\u6700\u5c0f\u6027\uff08\u6700\u5c0f\u4fee\u6539\uff09\u4e4b\u95f4\u5b58\u5728\u77db\u76fe\uff0c\u63d0\u793a\u6b64\u7c7b\u89e3\u91ca\u5de5\u5177\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\u3002", "motivation": "\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u901a\u8fc7\u81ea\u751f\u6210\u7684\u5bf9\u6297\u6027\u89e3\u91ca\uff08SCEs\uff09\u6709\u6548\u63ed\u793a\u5176\u51b3\u7b56\u903b\u8f91\uff0c\u4e3a\u9ad8\u98ce\u9669\u9886\u57df\u5e94\u7528\u63d0\u4f9b\u53ef\u4fe1\u89e3\u91ca\u3002", "method": "\u5728\u591a\u6a21\u578b\uff08\u5982LLaMA-2\u3001GPT-4\uff09\u3001\u591a\u6570\u636e\u96c6\uff08FEVER\u3001SNLI\uff09\u4e2d\u8bc4\u4f30SCEs\u7684\u6709\u6548\u6027\uff08\u662f\u5426\u6539\u53d8\u9884\u6d4b\uff09\u548c\u6700\u5c0f\u6027\uff08\u7f16\u8f91\u8ddd\u79bb\uff09\uff0c\u5206\u6790\u6a21\u578b\u89c4\u6a21\u4e0e\u89e3\u91ca\u8d28\u91cf\u7684\u5173\u7cfb\u3002", "result": "\u6a21\u578b\u751f\u6210\u6709\u6548SCEs\u65f6\u4fee\u6539\u5197\u4f59\uff08\u975e\u6700\u5c0f\u5316\uff09\uff0c\u800c\u8ffd\u6c42\u6700\u5c0f\u5316\u65f6\u5374\u65e0\u6cd5\u6539\u53d8\u9884\u6d4b\uff0c\u6709\u6548\u6027\u4e0e\u6700\u5c0f\u6027\u5448\u8d1f\u76f8\u5173\u3002\u8be5\u73b0\u8c61\u5177\u6709\u8de8\u6a21\u578b/\u6570\u636e\u7684\u666e\u9002\u6027\u3002", "conclusion": "SCEs\u4f5c\u4e3a\u89e3\u91ca\u5de5\u5177\u53ef\u9760\u6027\u5b58\u7591\uff0c\u53ef\u80fd\u8bef\u5bfc\u6a21\u578b\u884c\u4e3a\u89e3\u8bfb\u3002\u5efa\u8bae\u9ad8\u98ce\u9669\u573a\u666f\u90e8\u7f72LLMs\u65f6\u9700\u5ba1\u614e\u8bc4\u4f30\u81ea\u89e3\u91ca\u673a\u5236\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2509.09631", "pdf": "https://arxiv.org/pdf/2509.09631", "abs": "https://arxiv.org/abs/2509.09631", "authors": ["Ngoc-Son Nguyen", "Hieu-Nghia Huynh-Nguyen", "Thanh V. T. Tran", "Truong-Son Hy", "Van Nguyen"], "title": "DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for Low-Latency Zero-Shot Text-To-Speech", "categories": ["cs.SD", "cs.CL", "cs.CV"], "comment": null, "summary": "Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that\nmimics the voice of an unseen speaker using only a short reference sample,\nrequiring not only speaker adaptation but also accurate modeling of prosodic\nattributes. Recent approaches based on language models, diffusion, and flow\nmatching have shown promising results in zero-shot TTS, but still suffer from\nslow inference and repetition artifacts. Discrete codec representations have\nbeen widely adopted for speech synthesis, and recent works have begun to\nexplore diffusion models in purely discrete settings, suggesting the potential\nof discrete generative modeling for speech synthesis. However, existing\nflow-matching methods typically embed these discrete tokens into a continuous\nspace and apply continuous flow matching, which may not fully leverage the\nadvantages of discrete representations. To address these challenges, we\nintroduce DiFlow-TTS, which, to the best of our knowledge, is the first model\nto explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS\nexplicitly models factorized speech attributes within a compact and unified\narchitecture. It leverages in-context learning by conditioning on textual\ncontent, along with prosodic and acoustic attributes extracted from a reference\nspeech, enabling effective attribute cloning in a zero-shot setting. In\naddition, the model employs a factorized flow prediction mechanism with\ndistinct heads for prosody and acoustic details, allowing it to learn\naspect-specific distributions. Experimental results demonstrate that DiFlow-TTS\nachieves promising performance in several key metrics, including naturalness,\nprosody, preservation of speaker style, and energy control. It also maintains a\ncompact model size and achieves low-latency inference, generating speech up to\n25.8 times faster than the latest existing baselines.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u7eaf\u79bb\u6563\u6d41\u5339\u914d\u8bed\u97f3\u5408\u6210\u6a21\u578bDiFlow-TTS\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5206\u89e3\u7684\u8bed\u97f3\u5c5e\u6027\uff0c\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u5b9e\u73b0\u9ad8\u6548\u9ad8\u8d28\u91cf\u7684\u8bed\u97f3\u514b\u9686\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672cTTS\u6a21\u578b\u5b58\u5728\u63a8\u7406\u901f\u5ea6\u6162\u3001\u8fde\u7eed\u6027\u7a7a\u95f4\u5efa\u6a21\u65e0\u6cd5\u5145\u5206\u53d1\u6325\u79bb\u6563\u8868\u5f81\u4f18\u52bf\u7684\u95ee\u9898\uff0c\u9700\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u79bb\u6563\u751f\u6210\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u7eaf\u79bb\u6563\u6d41\u5339\u914d\u6846\u67b6\uff0c\u6784\u5efa\u5305\u542b\u6587\u672c\u6761\u4ef6\u3001\u97f5\u5f8b/\u58f0\u5b66\u5c5e\u6027\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u89e3\u6d41\u9884\u6d4b\u673a\u5236\u5b9e\u73b0\u5c5e\u6027\u89e3\u8026\u5efa\u6a21\u3002", "result": "\u5728\u81ea\u7136\u5ea6\u3001\u97f5\u5f8b\u3001\u8bf4\u8bdd\u4eba\u98ce\u683c\u4fdd\u6301\u7b49\u6307\u6807\u8868\u73b0\u4f18\u5f02\uff0c\u63a8\u7406\u901f\u5ea6\u8fbe\u57fa\u7ebf\u6a21\u578b\u768425.8\u500d\uff0c\u6a21\u578b\u53c2\u6570\u91cf\u66f4\u7d27\u51d1\u3002", "conclusion": "DiFlow-TTS\u9a8c\u8bc1\u4e86\u7eaf\u79bb\u6563\u6d41\u5339\u914d\u5728\u8bed\u97f3\u5408\u6210\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u9ad8\u6548\u7387\u96f6\u6837\u672c\u8bed\u97f3\u514b\u9686\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.09651", "pdf": "https://arxiv.org/pdf/2509.09651", "abs": "https://arxiv.org/abs/2509.09651", "authors": ["Zakaria El Kassimi", "Fares Fourati", "Mohamed-Slim Alouini"], "title": "Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG", "eess.SP"], "comment": null, "summary": "We study question answering in the domain of radio regulations, a legally\nsensitive and high-stakes area. We propose a telecom-specific\nRetrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge,\nthe first multiple-choice evaluation set for this domain, constructed from\nauthoritative sources using automated filtering and human validation. To assess\nretrieval quality, we define a domain-specific retrieval metric, under which\nour retriever achieves approximately 97% accuracy. Beyond retrieval, our\napproach consistently improves generation accuracy across all tested models. In\nparticular, while naively inserting documents without structured retrieval\nyields only marginal gains for GPT-4o (less than 1%), applying our pipeline\nresults in nearly a 12% relative improvement. These findings demonstrate that\ncarefully targeted grounding provides a simple yet strong baseline and an\neffective domain-specific solution for regulatory question answering. All code\nand evaluation scripts, along with our derived question-answer dataset, are\navailable at https://github.com/Zakaria010/Radio-RAG.", "AI": {"tldr": "\u9488\u5bf9\u65e0\u7ebf\u7535\u6cd5\u89c4\u9886\u57df\u63d0\u51fa\u7535\u4fe1\u4e13\u7528RAG\u6d41\u7a0b\uff0c\u6784\u5efa\u9996\u4e2a\u591a\u9009\u8bc4\u4f30\u96c6\uff0c\u5728\u4fdd\u630197%\u68c0\u7d22\u51c6\u786e\u7387\u7684\u540c\u65f6\u4f7fGPT-4o\u751f\u6210\u51c6\u786e\u7387\u63d0\u534712%", "motivation": "\u89e3\u51b3\u6cd5\u5f8b\u654f\u611f\u4e14\u9ad8\u98ce\u9669\u7684\u65e0\u7ebf\u7535\u6cd5\u89c4\u9886\u57df\u95ee\u7b54\u9700\u6c42\uff0c\u63d0\u4f9b\u53ef\u9760\u7684\u9886\u57df\u7279\u5b9a\u89e3\u51b3\u65b9\u6848", "method": "\u901a\u8fc7\u81ea\u52a8\u8fc7\u6ee4\u548c\u4eba\u5de5\u9a8c\u8bc1\u6784\u5efa\u8bc4\u4f30\u96c6\uff0c\u5b9a\u4e49\u9886\u57df\u7279\u5b9a\u68c0\u7d22\u6307\u6807\uff0c\u5f00\u53d1\u7ed3\u6784\u5316RAG\u6d41\u7a0b\u4f18\u5316\u6587\u6863\u68c0\u7d22", "result": "\u68c0\u7d22\u51c6\u786e\u7387\u8fbe97%\uff0cGPT-4o\u751f\u6210\u51c6\u786e\u7387\u76f8\u5bf9\u63d0\u534712%\uff08\u666e\u901a\u6587\u6863\u63d2\u5165\u4ec5\u63d0\u53471%\uff09", "conclusion": "\u9488\u5bf9\u6027\u5f3a\u7684\u7ed3\u6784\u5316\u68c0\u7d22\u4e3a\u6cd5\u89c4\u95ee\u7b54\u63d0\u4f9b\u6709\u6548\u57fa\u7ebf\u65b9\u6848\uff0c\u516c\u5f00\u7684\u4ee3\u7801\u548c\u6570\u636e\u96c6\u63a8\u52a8\u9886\u57df\u7814\u7a76"}}
{"id": "2509.09674", "pdf": "https://arxiv.org/pdf/2509.09674", "abs": "https://arxiv.org/abs/2509.09674", "authors": ["Haozhan Li", "Yuxin Zuo", "Jiale Yu", "Yuhao Zhang", "Zhaohui Yang", "Kaiyan Zhang", "Xuekai Zhu", "Yuchen Zhang", "Tianxing Chen", "Ganqu Cui", "Dehui Wang", "Dingxiang Luo", "Yuchen Fan", "Youbang Sun", "Jia Zeng", "Jiangmiao Pang", "Shanghang Zhang", "Yu Wang", "Yao Mu", "Bowen Zhou", "Ning Ding"], "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Vision-Language-Action (VLA) models have recently emerged as a powerful\nparadigm for robotic manipulation. Despite substantial progress enabled by\nlarge-scale pretraining and supervised fine-tuning (SFT), these models face two\nfundamental challenges: (i) the scarcity and high cost of large-scale\nhuman-operated robotic trajectories required for SFT scaling, and (ii) limited\ngeneralization to tasks involving distribution shift. Recent breakthroughs in\nLarge Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can\ndramatically enhance step-by-step reasoning capabilities, raising a natural\nquestion: Can RL similarly improve the long-horizon step-by-step action\nplanning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL\nframework tailored for VLA models. Building upon veRL, we introduce\nVLA-specific trajectory sampling, scalable parallelization, multi-environment\nrendering, and optimized loss computation. When applied to OpenVLA-OFT,\nSimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\\pi_0$\non RoboTwin 1.0\\&2.0 with the exploration-enhancing strategies we introduce.\nSimpleVLA-RL not only reduces dependence on large-scale data and enables robust\ngeneralization, but also remarkably surpasses SFT in real-world tasks.\nMoreover, we identify a novel phenomenon ``pushcut'' during RL training,\nwherein the policy discovers previously unseen patterns beyond those seen in\nthe previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL", "AI": {"tldr": "\u63d0\u51faSimpleVLA-RL\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdbRL\u8bad\u7ec3\u65b9\u6cd5\u663e\u8457\u63d0\u5347VLA\u6a21\u578b\u7684\u957f\u671f\u52a8\u4f5c\u89c4\u5212\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u6d4b\u8bd5\u96c6\u8d85\u8d8aSFT\u65b9\u6cd5\u3002", "motivation": "\u9488\u5bf9VLA\u6a21\u578b\u4f9d\u8d56\u5927\u89c4\u6a21\u4eba\u7c7b\u6f14\u793a\u6570\u636e\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u6838\u5fc3\u75db\u70b9\uff0c\u5c1d\u8bd5\u5c06\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u5f15\u5165\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u8bad\u7ec3\u3002", "method": "\u57fa\u4e8eveRL\u6846\u67b6\u6539\u8fdb\uff1a1\uff09VLA\u4e13\u7528\u8f68\u8ff9\u91c7\u6837\u7b56\u7565 2\uff09\u53ef\u6269\u5c55\u5e76\u884c\u67b6\u6784 3\uff09\u591a\u73af\u5883\u534f\u540c\u6e32\u67d3\u6280\u672f 4\uff09\u4f18\u5316\u635f\u5931\u8ba1\u7b97\u6a21\u5757", "result": "\u5728LIBERO\u57fa\u51c6\u8fbeSOTA\uff0cRoboTwin 1.0&2.0\u8d85\u8d8a\u03c00\u57fa\u7ebf\uff08\u63a2\u7d22\u589e\u5f3a\u7b56\u7565\u52a0\u6301\u4e0b\uff09\uff0c\u771f\u5b9e\u4efb\u52a1\u6027\u80fd\u8d85\u8fc7\u4f20\u7edfSFT\u65b9\u6cd5", "conclusion": "\u9a8c\u8bc1\u4e86RL\u5bf9VLA\u6a21\u578b\u4f18\u5316\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0'pushcut'\u8bad\u7ec3\u73b0\u8c61\uff0c\u4e3a\u51cf\u5c11\u6570\u636e\u4f9d\u8d56\u548c\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2509.09679", "pdf": "https://arxiv.org/pdf/2509.09679", "abs": "https://arxiv.org/abs/2509.09679", "authors": ["Bingxin Xu", "Zhen Dong", "Oussama Elachqar", "Yuzhang Shang"], "title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Replace discrete Hadamard transforms with continuous Butterfly\n  transforms to facilitate the learning of rotation matrices in LLM\n  quantization", "summary": "Large language models require massive memory footprints, severely limiting\ndeployment on consumer hardware. Quantization reduces memory through lower\nnumerical precision, but extreme 2-bit quantization suffers from catastrophic\nperformance loss due to outliers in activations. Rotation-based methods such as\nQuIP and QuaRot apply orthogonal transforms to eliminate outliers before\nquantization, using computational invariance: $\\mathbf{y} = \\mathbf{Wx} =\n(\\mathbf{WQ}^T)(\\mathbf{Qx})$ for orthogonal $\\mathbf{Q}$. However, these\nmethods use fixed transforms--Hadamard matrices achieving optimal worst-case\ncoherence $\\mu = 1/\\sqrt{n}$--that cannot adapt to specific weight\ndistributions. We identify that different transformer layers exhibit distinct\noutlier patterns, motivating layer-adaptive rotations rather than\none-size-fits-all approaches. We propose ButterflyQuant, which replaces\nHadamard rotations with learnable butterfly transforms parameterized by\ncontinuous Givens rotation angles. Unlike Hadamard's discrete $\\{+1, -1\\}$\nentries that are non-differentiable and prohibit gradient-based learning,\nbutterfly transforms' continuous parameterization enables smooth optimization\nwhile guaranteeing orthogonality by construction. This orthogonal constraint\nensures theoretical guarantees in outlier suppression while achieving $O(n \\log\nn)$ computational complexity with only $\\frac{n \\log n}{2}$ learnable\nparameters. We further introduce a uniformity regularization on\npost-transformation activations to promote smoother distributions amenable to\nquantization. Learning requires only 128 calibration samples and converges in\nminutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit\nquantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.", "AI": {"tldr": "\u63d0\u51faButterflyQuant\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u8774\u8776\u53d8\u6362\u66ff\u4ee3\u56fa\u5b9a\u54c8\u8fbe\u739b\u77e9\u9635\uff0c\u6539\u5584\u5927\u8bed\u8a00\u6a21\u578b\u76842\u4f4d\u91cf\u5316\u6548\u679c\uff0c\u5728LLaMA-2-7B\u4e0a\u5b9e\u73b015.4\u7684\u56f0\u60d1\u5ea6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u56fa\u5b9a\u6b63\u4ea4\u53d8\u6362\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u4e0d\u540ctransformer\u5c42\u7684\u5f02\u5e38\u503c\u5206\u5e03\u7279\u5f81\uff0c\u9700\u8981\u5f00\u53d1\u5c42\u81ea\u9002\u5e94\u7684\u65cb\u8f6c\u91cf\u5316\u65b9\u6848", "method": "\u4f7f\u7528\u53c2\u6570\u5316\u7684Givens\u65cb\u8f6c\u89d2\u5ea6\u6784\u5efa\u53ef\u5b66\u4e60\u8774\u8776\u53d8\u6362\uff0c\u901a\u8fc7\u6b63\u4ea4\u7ea6\u675f\u4fdd\u8bc1\u7406\u8bba\u5b8c\u5907\u6027\uff0c\u5f15\u5165\u6fc0\u6d3b\u5747\u5300\u6027\u6b63\u5219\u5316\uff0c\u4ec5\u9700128\u6837\u672c\u5355GPU\u5feb\u901f\u8bad\u7ec3", "result": "\u57282\u4f4d\u91cf\u5316\u8bbe\u7f6e\u4e0b\uff0cLLaMA-2-7B\u6a21\u578b\u7684\u56f0\u60d1\u5ea6\u4eceQuaRot\u768422.1\u663e\u8457\u964d\u4f4e\u81f315.4", "conclusion": "\u8fde\u7eed\u53c2\u6570\u5316\u7684\u8774\u8776\u53d8\u6362\u5728\u4fdd\u6301O(n logn)\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u91cf\u5316\u9002\u914d\uff0c\u4e3a\u6781\u4f4e\u6bd4\u7279\u91cf\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2509.09680", "pdf": "https://arxiv.org/pdf/2509.09680", "abs": "https://arxiv.org/abs/2509.09680", "authors": ["Rongyao Fang", "Aldrich Yu", "Chengqi Duan", "Linjiang Huang", "Shuai Bai", "Yuxuan Cai", "Kun Wang", "Si Liu", "Xihui Liu", "Hongsheng Li"], "title": "FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark", "categories": ["cs.CV", "cs.CL"], "comment": "Project page: https://flux-reason-6m.github.io/", "summary": "The advancement of open-source text-to-image (T2I) models has been hindered\nby the absence of large-scale, reasoning-focused datasets and comprehensive\nevaluation benchmarks, resulting in a performance gap compared to leading\nclosed-source systems. To address this challenge, We introduce FLUX-Reason-6M\nand PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).\nFLUX-Reason-6M is a massive dataset consisting of 6 million high-quality\nFLUX-generated images and 20 million bilingual (English and Chinese)\ndescriptions specifically designed to teach complex reasoning. The image are\norganized according to six key characteristics: Imagination, Entity, Text\nrendering, Style, Affection, and Composition, and design explicit Generation\nChain-of-Thought (GCoT) to provide detailed breakdowns of image generation\nsteps. The whole data curation takes 15,000 A100 GPU days, providing the\ncommunity with a resource previously unattainable outside of large industrial\nlabs. PRISM-Bench offers a novel evaluation standard with seven distinct\ntracks, including a formidable Long Text challenge using GCoT. Through\ncarefully designed prompts, it utilizes advanced vision-language models for\nnuanced human-aligned assessment of prompt-image alignment and image\naesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench\nreveals critical performance gaps and highlights specific areas requiring\nimprovement. Our dataset, benchmark, and evaluation code are released to\ncatalyze the next wave of reasoning-oriented T2I generation. Project page:\nhttps://flux-reason-6m.github.io/ .", "AI": {"tldr": "\u63d0\u51faFLUX-Reason-6M\u6570\u636e\u96c6\u4e0ePRISM-Bench\u8bc4\u6d4b\u6807\u51c6\uff0c\u901a\u8fc76\u767e\u4e07\u9ad8\u8d28\u91cf\u56fe\u50cf\u548c7\u7ef4\u8bc4\u6d4b\u4f53\u7cfb\u63a8\u52a8\u5f00\u6e90\u6587\u751f\u56fe\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u53d1\u5c55\u3002", "motivation": "\u89e3\u51b3\u5f00\u6e90T2I\u6a21\u578b\u56e0\u7f3a\u4e4f\u5927\u89c4\u6a21\u63a8\u7406\u6570\u636e\u96c6\u548c\u8bc4\u6d4b\u6807\u51c6\u5bfc\u81f4\u7684\u6027\u80fd\u74f6\u9888\uff0c\u7f29\u5c0f\u4e0e\u95ed\u6e90\u7cfb\u7edf\u7684\u5dee\u8ddd\u3002", "method": "\u6784\u5efa\u5305\u542b6\u5927\u7279\u5f81\u7ef4\u5ea6\u7684FLUX-Reason-6M\u6570\u636e\u96c6\uff0815,000 A100 GPU\u5929\uff09\u548c\u5305\u542b\u957f\u6587\u672c\u6311\u6218\u7684PRISM-Bench\u8bc4\u6d4b\u6846\u67b6\uff0c\u91c7\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u3002", "result": "\u5bf919\u4e2a\u4e3b\u6d41\u6a21\u578b\u7684\u8bc4\u6d4b\u63ed\u793a\u5173\u952e\u6027\u80fd\u7f3a\u9677\uff0c\u9a8c\u8bc1\u4e86\u6570\u636e\u589e\u5f3a\u5bf9\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u6548\u679c\u3002", "conclusion": "\u5f00\u6e90\u6570\u636e\u96c6\u4e0e\u8bc4\u6d4b\u4f53\u7cfb\u5c06\u63a8\u52a8\u63a8\u7406\u5bfc\u5411\u7684\u6587\u751f\u56fe\u6280\u672f\u53d1\u5c55\uff0c\u586b\u8865\u793e\u533a\u8d44\u6e90\u7a7a\u767d\u3002"}}
