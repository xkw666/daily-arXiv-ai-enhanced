<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 62]
- [cs.GR](#cs.GR) [Total: 4]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 6]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.SE](#cs.SE) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.AI](#cs.AI) [Total: 5]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PHANTOM RECALL: When Familiar Puzzles Fool Smart Models](https://arxiv.org/abs/2510.11812)
*Souradeep Mukhopadhyay,Rishabh Baral,Nimeesh Mahajan,Samhitha Harish,Aswin RRV,Mihir Parmar,Mutsumi Nakamura,Chitta Baral*

Main category: cs.CL

TL;DR: LLMs解决逻辑谜题依赖记忆而非推理，问题微调后性能显著下降，PHANTOM RECALL基准揭示模型存在phantom recall现象


<details>
  <summary>Details</summary>
Motivation: 验证大语言模型是否具备真实推理能力，探究其面对扰动问题时表现脆弱性的根源

Method: 构建包含25个经典谜题和149个扰动版本的PHANTOM RECALL基准，评估11个主流LLM，开发逻辑等价判断/错误分类/提示框架三件套工具

Result: 模型原始谜题准确率接近完美，但扰动后表现显著劣于人类，出现记忆错位(phantom recall)和过度阐述现象

Conclusion: LLMs在语境变化时缺乏再推理能力，揭示了语言流畅性与逻辑理解之间的本质鸿沟

Abstract: Large language models (LLMs) such as GPT, Gemini, and Claude often appear
adept at solving classic logic puzzles--but how much genuine reasoning
underlies their answers? Recent evidence suggests that these models frequently
rely on memorized templates rather than reasoning from first principles. When
puzzles are slightly modified, their performance collapses, revealing a
striking fragility. In particular, we asked: Have LLMs addressed these issues?
To what extent? How about perturbations to other puzzles? Is there a general
way of reformulating the prompt so that the models do better? To examine these
things systematically, we introduce PHANTOM RECALL, a benchmark comprising 25
well-known logic puzzles and 149 carefully designed perturbations that preserve
reasoning structure but alter superficial details and solutions. We evaluate
eleven leading LLMs and identify a recurring failure mode--phantom
recall--where models confidently reproduce memorized solutions or spurious
rationales that no longer fit the altered scenario. To probe and mitigate this
issue, we contribute three tools: (i) an automated logical-equivalence judge to
detect reasoning mismatches, (ii) a taxonomy of fine-grained reasoning error
categories, and (iii) a prompting-based mitigation framework guided by these
categories. Despite near-perfect accuracy on unmodified puzzles, models
significantly underperform humans on perturbed ones, exhibiting both phantom
recall and over-elaboration. Our findings reveal a crucial limitation: LLMs
often fail to re-reason when contextual cues shift--highlighting the gap
between linguistic fluency and logical understanding.

</details>


### [2] [R-WoM: Retrieval-augmented World Model For Computer-use Agents](https://arxiv.org/abs/2510.11892)
*Kai Mei,Jiang Guo,Shuaichen Chang,Mingwen Dong,Dongkyu Lee,Xing Niu,Jiarong Jiang*

Main category: cs.CL

TL;DR: LLMs作为数字环境世界模型时存在长期模拟缺陷，研究者提出检索增强模型R-WoM，在实验中获得最高25.3%的性能提升


<details>
  <summary>Details</summary>
Motivation: 验证LLMs作为世界模型的适用性，解决其因幻觉和静态知识导致的长期模拟误差积累问题

Method: 通过未来状态预测（next-state）、全流程规划对齐（full-procedure）、里程碑识别（milestone）三个任务测试，并提出整合外部教程检索的R-WoM模型

Result: LLMs在短期预测表现良好但长程规划性能快速下降，R-WoM在OSWorld和WebArena分别实现25.3%和18.1%提升，长程模拟优势显著

Conclusion: 检索增强机制有效提升LLM世界建模能力，R-WoM为数字环境智能体决策提供了可靠解决方案

Abstract: Large Language Models (LLMs) can serve as world models to enhance agent
decision-making in digital environments by simulating future states and
predicting action outcomes, potentially eliminating costly trial-and-error
exploration. However, this capability is fundamentally limited by LLMs'
tendency toward hallucination and their reliance on static training knowledge,
which can lead to compounding errors that inhibit long-horizon simulations. To
systematically investigate whether LLMs are appropriate for world modeling, we
probe two core capabilities of world models--future state prediction and reward
estimation--through three tasks: next-state identification, full-procedure
planning alignment, and milestone transition recognition. Our analysis shows
that while LLMs effectively capture immediate next states and identify
meaningful state transitions, their performance rapidly degrades in
full-procedure planning. This highlights LLMs' limitations in reliably modeling
environment dynamics over long horizons. To address these limitations, we
propose the Retrieval-augmented World Model (R-WoM), which grounds LLM
simulations by incorporating factual, up-to-date knowledge retrieved from
external tutorials. Experiments show that R-WoM achieves substantial
improvements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to
baselines, with particular advantages in longer-horizon simulations.

</details>


### [3] [LLM Knowledge is Brittle: Truthfulness Representations Rely on Superficial Resemblance](https://arxiv.org/abs/2510.11905)
*Patrick Haller,Mark Ibrahim,Polina Kirichenko,Levent Sagun,Samuel J. Bell*

Main category: cs.CL

TL;DR: 研究发现大语言模型内部知识表示不稳定，导致其对输入变化的脆弱性。真实性与表面形式高度相关，影响模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLM性能脆弱性是否源于不稳定的内部知识表示，验证语义扰动下真实性表征的退化情况。

Method: 对四种LLM家族进行语义保留扰动（如拼写错误/改写），使用三种知识探测方法评估五个数据集中的表示可分性退化。

Result: 模型真实性表征随输入与预训练数据差异增大而崩溃，真实判断严重依赖表面形式而非深层语义。

Conclusion: LLM学习的是浅层非鲁棒知识表示，这对真实性探针的实用性提出根本挑战，需改进知识表示鲁棒性。

Abstract: For Large Language Models (LLMs) to be reliable, they must learn robust
knowledge that can be generally applied in diverse settings -- often unlike
those seen during training. Yet, extensive research has shown that LLM
performance can be brittle, with models exhibiting excessive sensitivity to
trivial input variations. In this work, we explore whether this brittleness is
a direct result of unstable internal knowledge representations. To explore this
question, we build on previous work showing that LLM representations encode
statement truthfulness -- i.e., true, factual statements can be easily
separated from false, inaccurate ones. Specifically, we test the robustness of
learned knowledge by evaluating representation separability on samples that
have undergone superficial transformations to drive them out-of-distribution
(OOD), such as typos or reformulations. By applying semantically-preserving
perturbations, we study how separability degrades as statements become more
OOD, across four LLM families, five evaluation datasets, and three knowledge
probing methods. Our results reveal that internal representations of statement
truthfulness collapse as the samples' presentations become less similar to
those seen during pre-training. While LLMs can often distinguish between true
and false statements when they closely resemble the pre-training data, this
ability is highly dependent on the statement's exact surface form. These
findings offer a possible explanation for brittle benchmark performance: LLMs
may learn shallow, non-robust knowledge representations that allow for only
limited generalizability. Our work presents a fundamental challenge for the
utility of truthfulness probes, and more broadly, calls for further research on
improving the robustness of learned knowledge representations.

</details>


### [4] [LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens](https://arxiv.org/abs/2510.11919)
*Armel Zebaze,Rachel Bawden,Benoît Sagot*

Main category: cs.CL

TL;DR: 论文探讨大型推理模型在机器翻译任务中使用中间标记的效果，发现单纯添加思考标记无助于提升翻译质量，但模块化策略生成的中间标记能带来改进。


<details>
  <summary>Details</summary>
Motivation: 探索LRMs在机器翻译任务中生成中间标记的潜在价值，尤其是在不同资源水平的语言对中验证其有效性。

Method: 通过微调模型结合蒸馏思维链（CoT）模拟人类翻译步骤，并测试多种中间标记生成策略（包括模块化提示组合）。

Result: 直接使用思考标记或蒸馏CoT的微调未提升性能，但模块化策略生成的中间标记有效。中间标记的有效性取决于是否包含实际翻译尝试。

Conclusion: 在翻译任务中，优化目标翻译或扩展语料库比强制模型生成解释性中间标记更有效。

Abstract: Large reasoning models (LRMs) have led to new possibilities in terms of
problem-solving, through the devising of a natural language thought process
prior to answering a query. While their capabilities are well known across
mathematics and coding tasks, their impact on the task of machine translation
(MT) remains underexplored. In this work, we explore the benefits of the
generation of intermediate tokens when performing MT across multiple language
pairs of different levels of resourcedness and multiple setups. We find that
"thinking tokens" do not help LRMs better perform MT. This result generalizes
to models fine-tuned to reason before translating using distilled chain of
thought (CoT) inspired by human translators' practices. Specifically,
fine-tuning a model with synthetic CoT explanations detailing how to translate
step-by-step does not outperform standard input-output fine-tuning. However,
constructing the intermediate tokens by combining the outputs of modular
translation-specific prompting strategies results in improvements. Our findings
underscore that the contribution of intermediate tokens during fine-tuning
highly depends on the presence of translation attempts within them. More
broadly, our results suggest that using a teacher to refine target translations
or to expand parallel corpora is more impactful than distilling their CoT
explanations into "thinking" MT models.

</details>


### [5] [Discrepancy Detection at the Data Level: Toward Consistent Multilingual Question Answering](https://arxiv.org/abs/2510.11928)
*Lorena Calvo-Bartolomé,Valérie Aldana,Karla Cantarero,Alonso Madroñal de Mesa,Jerónimo Arenas-García,Jordan Boyd-Graber*

Main category: cs.CL

TL;DR: 提出了MIND框架——用户参与的事实核查流程，通过标注双语数据集验证其在检测多语言QA系统事实/文化不一致性方面的有效性，并证明其跨领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决多语言问答系统中客观问题的事实一致性不足，以及主观问题因文化差异导致的回答不一致问题。

Method: 开发用户参与循环的MIND框架，通过标注双语QA数据集进行验证，并在母婴健康等领域测试跨领域适应性。

Result: 在母婴健康及其他领域测试中，MIND成功识别91%的事实不一致和86%的文化差异，发布首个标注双语QA不一致数据集。

Conclusion: MIND框架显著提升QA系统文化敏感性，用户参与机制和模块化设计使其具备跨领域应用潜力。

Abstract: Multilingual question answering (QA) systems must ensure factual consistency
across languages, especially for objective queries such as What is jaundice?,
while also accounting for cultural variation in subjective responses. We
propose MIND, a user-in-the-loop fact-checking pipeline to detect factual and
cultural discrepancies in multilingual QA knowledge bases. MIND highlights
divergent answers to culturally sensitive questions (e.g., Who assists in
childbirth?) that vary by region and context. We evaluate MIND on a bilingual
QA system in the maternal and infant health domain and release a dataset of
bilingual questions annotated for factual and cultural inconsistencies. We
further test MIND on datasets from other domains to assess generalization. In
all cases, MIND reliably identifies inconsistencies, supporting the development
of more culturally aware and factually consistent QA systems.

</details>


### [6] [TopoAlign: A Framework for Aligning Code to Math via Topological Decomposition](https://arxiv.org/abs/2510.11944)
*Yupei Li,Philipp Borchert,Gerasimos Lampouras*

Main category: cs.CL

TL;DR: TopoAlign框架通过代码重组技术，有效利用代码资源提升数学大模型的自动形式化能力


<details>
  <summary>Details</summary>
Motivation: 当前数学大模型受限于形式化数学配对数据匮乏，且代码生成训练难以迁移至数学形式化任务

Method: 将代码分解为文档字符串/主函数/依赖函数，重组为结构对齐的形式化数学数据

Result: DeepSeek-Math模型BEq@10提升17.77%，Herald模型typecheck@10提升1.09%

Conclusion: 无需新标注数据即可提升模型性能，为数学自动形式化提供了数据高效解决方案

Abstract: Large Language Models (LLMs) excel at both informal and formal (e.g. Lean 4)
mathematical reasoning but still struggle with autoformalisation, the task of
transforming informal into formal mathematical statements. Autoformalisation
helps pair the informal reasoning of LLMs with formal proof assistants which
enable machine-verifiable generation and mitigate hallucinations. Yet, the
performance of current Math LLMs is constrained by the scarcity of large-scale
corpora, particularly those containing pairs of informal and formal statements.
Although current models are trained to generate code from natural language
instructions, structural and syntactic differences between these and formal
mathematics limit effective transfer learning. We propose TopoAlign, a
framework that unlocks widely available code repositories as training resources
for Math LLMs. TopoAlign decomposes code into docstrings, main functions, and
dependency functions, and reassembles these components into analogues that
structurally mirror formal statements. This produces structurally aligned code
data that can be used for training Math LLMs without requiring additional human
annotation. We train two state-of-the-art models, DeepSeek-Math and Herald, and
evaluate them on the minif2f, Putnam, and ProofNet benchmarks. TopoAlign
provides substantial gains for DeepSeek-Math, improving performance by 17.77%
on BEq@10 and 68.82% on typecheck@10. Despite introducing no new mathematical
knowledge, our framework achieves gains of 0.12% and 1.09% for Herald on BEq@10
and typecheck@10, respectively, demonstrating that training on aligned code
data is beneficial even for specialized models.

</details>


### [7] [GRAVITY: A Framework for Personalized Text Generation via Profile-Grounded Synthetic Preferences](https://arxiv.org/abs/2510.11952)
*Priyanka Dey,Daniele Rosa,Wenqing Zheng,Daniel Barcklow,Jieyu Zhao,Emilio Ferrara*

Main category: cs.CL

TL;DR: GRAVITY框架通过合成用户画像数据减少LLM个性化对人工标注的依赖，整合多维理论模型生成偏好数据，实验证明跨文化场景下用户偏好率超86%。


<details>
  <summary>Details</summary>
Motivation: 传统LLM个性化依赖高成本人工反馈，忽略深层用户属性。需开发基于用户画像的合成数据生成方法，降低标注成本。

Method: 整合霍夫斯泰德文化维度/施瓦茨价值观/世界价值观调查/大五人格理论，构建用户画像框架生成偏好对数据，指导个性化内容生成。

Result: 在400名亚马逊用户的书籍推荐场景中，GRAVITY相比传统方法获得4%以上的偏好增益，跨文化测试（美/巴/日/印）用户选择率达86%以上。

Conclusion: 基于场景的合成数据能有效捕捉用户差异，降低标注成本，生成更吸引人的个性化内容，为LLM规模化定制提供新路径。

Abstract: Personalization in LLMs often relies on costly human feedback or interaction
logs, limiting scalability and neglecting deeper user attributes. To reduce the
reliance on human annotations, we introduce GRAVITY (Generative Response with
Aligned Values, Interests, and Traits of You), a framework for generating
synthetic, profile-grounded preference data that captures users' interests,
values, beliefs, and personality traits. By integrating demographic, cultural,
and psychological frameworks -- including Hofstede's cultural dimensions,
Schwartz's basic values, the World Values Survey, and Big Five OCEAN traits --
GRAVITY synthesizes preference pairs to guide personalized content generation.
We evaluate GRAVITY on book descriptions for 400 Amazon users, comparing it to
prompt-based conditioning, standard fine-tuning, and naive synthetic pair
generation. Profile-grounded synthetic data consistently improves generation,
especially across multiple cultures (USA, Brazil, Japan, India), achieving over
4% higher preference gains across baselines, with user studies showing that
GRAVITY outputs are preferred over 86% of the time. Our results show that
scenario-grounded synthetic data can capture richer user variation, reduce
reliance on costly annotation, and produce more engaging, user-centered
content, offering a scalable path for LLM personalization.

</details>


### [8] [Evaluating Retrieval-Augmented Generation Systems on Unanswerable, Uncheatable, Realistic, Multi-hop Queries](https://arxiv.org/abs/2510.11956)
*Gabrielle Kaili-May Liu,Bryan Li,Arman Cohan,William Gantt Walden,Eugene Yang*

Main category: cs.CL

TL;DR: 提出CRUMQs自动化流程，生成不可作弊、多跳和无法回答的测试查询，显著提升RAG系统评估难度


<details>
  <summary>Details</summary>
Motivation: 现有RAG基准测试无法有效评估系统处理复杂多跳/无法回答问题的能力，存在作弊漏洞和简单事实回忆缺陷

Method: 开发自动生成CRUMQs的流程，支持任意语料库的难度控制，并在两个主流RAG数据集上验证有效性

Result: CRUMQs使RAG系统作弊分数降低81%，显著暴露现有系统在复杂推理场景中的局限性

Conclusion: 该流程通过增强基准测试难度和真实性，推动开发更具鲁棒性的检索增强型LLM系统

Abstract: Real-world use cases often present RAG systems with complex queries for which
relevant information is missing from the corpus or is incomplete. In these
settings, RAG systems must be able to reject unanswerable, out-of-scope queries
and identify failures of retrieval and multi-hop reasoning. Despite this,
existing RAG benchmarks rarely reflect realistic task complexity for multi-hop
or out-of-scope questions, which often can be cheated via disconnected
reasoning (i.e., solved without genuine multi-hop inference) or require only
simple factual recall. This limits the ability for such benchmarks to uncover
limitations of existing RAG systems. To address this gap, we present the first
pipeline for automatic, difficulty-controlled creation of
un$\underline{c}$heatable, $\underline{r}$ealistic, $\underline{u}$nanswerable,
and $\underline{m}$ulti-hop $\underline{q}$uerie$\underline{s}$ (CRUMQs),
adaptable to any corpus and domain. We use our pipeline to create CRUMQs over
two popular RAG datasets and demonstrate its effectiveness via benchmark
experiments on leading retrieval-augmented LLMs. Results show that compared to
prior RAG benchmarks, CRUMQs are highly challenging for RAG systems and achieve
up to 81.0\% reduction in cheatability scores. More broadly, our pipeline
offers a simple way to enhance benchmark difficulty and realism and drive
development of more capable RAG systems.

</details>


### [9] [Direct Multi-Token Decoding](https://arxiv.org/abs/2510.11958)
*Xuan Luo,Weizhi Wang,Xifeng Yan*

Main category: cs.CL

TL;DR: 提出直接多令牌解码(DMTD)方法，通过仅使用LLM后期层一次生成多个令牌，实现2倍推断加速且性能损失极小


<details>
  <summary>Details</summary>
Motivation: 现有LLM逐令牌生成需重复遍历全部网络层，计算开销大。研究发现后期层隐藏状态可能包含生成多个令牌的足够信息

Method: 利用早期/中间层处理后的隐藏状态，仅通过后期层直接预测多个后续令牌，无需参数增加或后处理验证

Result: 微调后的Qwen3-4B模型在有限数据下实现2倍加速，性能损失可控。扩展分析显示更大训练数据可进一步提升效果

Conclusion: DMTD为LLM推断加速提供了参数高效的新范式，在保持模型性能的同时显著提升生成速度，具有可扩展性优势

Abstract: Decoder-only transformers have become the standard architecture for large
language models (LLMs) due to their strong performance. Recent studies suggest
that, in pre-trained LLMs, early, middle, and late layers may serve distinct
roles: Early layers focus on understanding the input context, middle layers
handle task-specific processing, and late layers convert abstract
representations into output tokens. We hypothesize that once representations
have been processed by the early and middle layers, the resulting hidden states
may encapsulate sufficient information to support the generation of multiple
tokens using only the late layers, eliminating the need to repeatedly traverse
the early and middle layers. We refer to this inference paradigm as Direct
Multi-Token Decoding (DMTD). Unlike speculative decoding, our method introduces
no additional parameters, auxiliary routines, or post-generation verification.
Despite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model
has already demonstrated promising results, achieving up to a 2x speedup with
only minor performance loss. Moreover, as shown in our scaling analysis, its
performance is expected to further improve with larger training datasets.

</details>


### [10] [Scaling Long-Horizon LLM Agent via Context-Folding](https://arxiv.org/abs/2510.11967)
*Weiwei Sun,Miao Lu,Zhan Ling,Kang Liu,Xuesong Yao,Yiming Yang,Jiecao Chen*

Main category: cs.CL

TL;DR: 提出Context-Folding框架解决大语言模型长程任务中的上下文限制，通过子任务折叠机制实现高效上下文管理


<details>
  <summary>Details</summary>
Motivation: 大语言模型代理在长程任务中受限于上下文长度，传统方法无法有效处理复杂任务分解与上下文维护

Method: 开发可折叠强化学习框架FoldGRPO，包含任务分解奖励机制和上下文折叠机制，支持子任务执行后保留结果摘要

Result: 在Deep Research和SWE任务中，折叠代理使用10倍更小上下文达到或超越ReAct基线，显著优于基于摘要的上下文管理模型

Conclusion: 主动式上下文折叠机制能有效突破LLM代理的上下文限制，为复杂长程任务处理提供新范式

Abstract: Large language model (LLM) agents are fundamentally constrained by context
length on long-horizon tasks. We introduce Context-Folding, a framework that
empowers agents to actively manage their working context. An agent can
procedurally branch into a sub-trajectory to handle a subtask and then fold it
upon completion, collapsing the intermediate steps while retaining a concise
summary of the outcome. To make this behavior learnable, we develop an
end-to-end reinforcement learning framework FoldGRPO with specific process
rewards to encourage effective task decomposition and context management. On
complex long-horizon tasks (Deep Research and SWE), our folding agent matches
or outperforms the ReAct baselines while using an active context 10$\times$
smaller and significantly outperforms models that rely on summarization-based
context management.

</details>


### [11] [Conjecturing: An Overlooked Step in Formal Mathematical Reasoning](https://arxiv.org/abs/2510.11986)
*Jasivan Alex Sivakumar,Philipp Borchert,Ronald Cardenas,Gerasimos Lampouras*

Main category: cs.CL

TL;DR: 论文提出自动形式化需优先处理猜想步骤，创建ConjectureBench评估框架并开发Lean-FIRe方法，显著提升LLMs端到端自动形式化能力


<details>
  <summary>Details</summary>
Motivation: 现有自动形式化任务忽视猜想步骤导致模型能力被高估，需独立评估猜想能力对数学推理的影响

Method: 通过扩展数据集构建ConjectureBench，重新设计评估指标，并提出推理阶段优化方法Lean-FIRe

Result: Lean-FIRe使GPT-4.1/DeepSeek-V3.1分别成功解决13/7个PutnamBench问题，证明LLMs具备准确猜想能力但需独立处理该步骤

Conclusion: 应将猜想作为形式推理独立任务研究，未来需探索如何有效整合猜想与自动形式化流程

Abstract: Autoformalisation, the task of expressing informal mathematical statements in
formal language, is often viewed as a direct translation process. This,
however, disregards a critical preceding step: conjecturing. Many mathematical
problems cannot be formalised directly without first conjecturing a conclusion
such as an explicit answer, or a specific bound. Since Large Language Models
(LLMs) already struggle with autoformalisation, and the evaluation of their
conjecturing ability is limited and often entangled within autoformalisation or
proof, it is particularly challenging to understand its effect. To address this
gap, we augment existing datasets to create ConjectureBench, and redesign the
evaluation framework and metric specifically to measure the conjecturing
capabilities of LLMs both as a distinct task and within the autoformalisation
pipeline. Our evaluation of foundational models, including GPT-4.1 and
DeepSeek-V3.1, reveals that their autoformalisation performance is
substantially overestimated when the conjecture is accounted for during
evaluation. However, the conjecture should not be assumed to be provided. We
design an inference-time method, Lean-FIRe to improve conjecturing and
autoformalisation, which, to the best of our knowledge, achieves the first
successful end-to-end autoformalisation of 13 PutnamBench problems with GPT-4.1
and 7 with DeepSeek-V3.1. We demonstrate that while LLMs possess the requisite
knowledge to generate accurate conjectures, improving autoformalisation
performance requires treating conjecturing as an independent task, and
investigating further how to correctly integrate it within autoformalisation.
Finally, we provide forward-looking guidance to steer future research toward
improving conjecturing, an overlooked step of formal mathematical reasoning.

</details>


### [12] [SAGE: A Top-Down Bottom-Up Knowledge-Grounded User Simulator for Multi-turn AGent Evaluation](https://arxiv.org/abs/2510.11997)
*Ryan Shea,Yunan Lu,Liang Qiu,Zhou Yu*

Main category: cs.CL

TL;DR: 提出SAGE框架，通过整合商业场景知识实现多轮对话代理的自动化评估，相比传统方法可多识别33%的代理错误


<details>
  <summary>Details</summary>
Motivation: 现有基于模拟用户的评估方法缺乏领域特异性，无法反映真实商业场景中的用户行为特征

Method: 整合自上而下的商业逻辑（客户画像）与自下而上的业务数据（产品目录/FAQ），构建拟真用户交互框架

Result: 实证表明SAGE生成的交互更真实多样，错误检测率提升33%

Conclusion: 融合业务知识的仿真框架能有效支持对话代理的迭代优化与缺陷检测

Abstract: Evaluating multi-turn interactive agents is challenging due to the need for
human assessment. Evaluation with simulated users has been introduced as an
alternative, however existing approaches typically model generic users and
overlook the domain-specific principles required to capture realistic behavior.
We propose SAGE, a novel user Simulation framework for multi-turn AGent
Evaluation that integrates knowledge from business contexts. SAGE incorporates
top-down knowledge rooted in business logic, such as ideal customer profiles,
grounding user behavior in realistic customer personas. We further integrate
bottom-up knowledge taken from business agent infrastructure (e.g., product
catalogs, FAQs, and knowledge bases), allowing the simulator to generate
interactions that reflect users' information needs and expectations in a
company's target market. Through empirical evaluation, we find that this
approach produces interactions that are more realistic and diverse, while also
identifying up to 33% more agent errors, highlighting its effectiveness as an
evaluation tool to support bug-finding and iterative agent improvement.

</details>


### [13] [Generate Logical Equivalence Questions](https://arxiv.org/abs/2510.12001)
*Xinyu Wang,Haoming Yu,Yicheng Yang,Zhiyuan Li*

Main category: cs.CL

TL;DR: 本文提出了一种基于形式化语言的自动问题生成系统，通过线性时间算法生成离散数学逻辑等价题目，实验证明其生成质量与教材题目相当


<details>
  <summary>Details</summary>
Motivation: 在线教学环境下抄袭现象加剧，传统自动问题生成方法存在生成效率低且题目难度参差不齐的问题

Method: 使用形式化语言定义逻辑等价问题，转化为两种生成规则集，开发线性时间生成算法，并通过学生实验与多维度对比实验进行验证

Result: 实验数据显示生成题目的正确率与教材相当（p=0.82），解题步骤数分析表明其难度分布与教材题目基本一致，且优于大型语言模型生成结果

Conclusion: 提出的新型AQG系统能高效生成质量稳定的逻辑等价题目，为遏制抄袭行为提供了有效解决方案，适用于计算机基础课程教学

Abstract: Academic dishonesty is met with zero tolerance in higher education, yet
plagiarism has become increasingly prevalent in the era of online teaching and
learning. Automatic Question Generation (AQG) presents a potential solution to
mitigate copying by creating unique questions for each student. Additionally,
AQG can provide a vast array of practice questions. Our AQG focuses on
generating logical equivalence questions for Discrete Mathematics, a
foundational course for first-year computer science students. A literature
review reveals that existing AQGs for this type of question generate all
propositions that meet user-defined constraints, resulting in inefficiencies
and a lack of uniform question difficulty. To address this, we propose a new
approach that defines logical equivalence questions using a formal language,
translates this language into two sets of generation rules, and develops a
linear-time algorithm for question generation. We evaluated our AQG through two
experiments. The first involved a group of students completing questions
generated by our system. Statistical analysis shows that the accuracy of these
questions is comparable to that of textbook questions. The second experiment
assessed the number of steps required to solve our generated questions,
textbook questions, and those generated by multiple large language models. The
results indicated that the difficulty of our questions was similar to that of
textbook questions, confirming the quality of our AQG.

</details>


### [14] [Information Extraction from Conversation Transcripts: Neuro-Symbolic vs. LLM](https://arxiv.org/abs/2510.12023)
*Alice Saebom Kwak,Maria Alexeeva,Gus Hahn-Powell,Keith Alcock,Kevin McLaughlin,Doug McCorkle,Gabe McNunn,Mihai Surdeanu*

Main category: cs.CL

TL;DR: LLM系统在农业领域信息抽取中表现优于神经符号系统（F1总分69.4 vs 52.7），但存在运行速度慢、可控性差等隐藏成本


<details>
  <summary>Details</summary>
Motivation: 评估神经符号系统与LLM在真实农业场景中的性能差异，揭示NLP系统实际部署的权衡要素

Method: 在猪肉、乳制品和作物三个子领域的9个访谈数据上，对比两种系统的总抽取精度（含所有信息）和核心精度（关键信息）

Result: LLM总F1高16.7分（69.4 vs 52.7），核心F1高15.8分（63.0 vs 47.2）。神经符号系统运行快但维护成本高，LLM易部署但存在幻觉风险

Conclusion: 实际部署需平衡性能、效率和控制性，LLM的高性能伴随隐藏成本，传统方法在特定场景仍有价值

Abstract: The current trend in information extraction (IE) is to rely extensively on
large language models, effectively discarding decades of experience in building
symbolic or statistical IE systems. This paper compares a neuro-symbolic (NS)
and an LLM-based IE system in the agricultural domain, evaluating them on nine
interviews across pork, dairy, and crop subdomains. The LLM-based system
outperforms the NS one (F1 total: 69.4 vs. 52.7; core: 63.0 vs. 47.2), where
total includes all extracted information and core focuses on essential details.
However, each system has trade-offs: the NS approach offers faster runtime,
greater control, and high accuracy in context-free tasks but lacks
generalizability, struggles with contextual nuances, and requires significant
resources to develop and maintain. The LLM-based system achieves higher
performance, faster deployment, and easier maintenance but has slower runtime,
limited control, model dependency and hallucination risks. Our findings
highlight the "hidden cost" of deploying NLP systems in real-world
applications, emphasizing the need to balance performance, efficiency, and
control.

</details>


### [15] [CPR: Mitigating Large Language Model Hallucinations with Curative Prompt Refinement](https://arxiv.org/abs/2510.12029)
*Jung-Woo Shim,Yeong-Joon Ju,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: 提出CPR框架优化提示词结构，显著减少大语言模型幻觉并提升生成质量


<details>
  <summary>Details</summary>
Motivation: 用户不当的提示词结构容易导致LLMs基于假设而非真实意图生成错误内容

Method: 开发双阶段框架：1) 清洗不规范提示 2) 用微调小模型生成补充性任务描述

Result: 使用CPR的提示在90%以上案例优于原始提示，无需外部知识支持

Conclusion: 通过结构化优化提示词本身可有效对齐用户意图与模型理解，为解决幻觉问题提供新方向

Abstract: Recent advancements in large language models (LLMs) highlight their fluency
in generating responses to diverse prompts. However, these models sometimes
generate plausible yet incorrect ``hallucinated" facts, undermining trust. A
frequent but often overlooked cause of such errors is the use of poorly
structured or vague prompts by users, leading LLMs to base responses on assumed
rather than actual intentions. To mitigate hallucinations induced by these
ill-formed prompts, we introduce Curative Prompt Refinement (CPR), a
plug-and-play framework for curative prompt refinement that 1) cleans
ill-formed prompts, and 2) generates additional informative task descriptions
to align the intention of the user and the prompt using a fine-tuned small
language model. When applied to language models, we discover that CPR
significantly increases the quality of generation while also mitigating
hallucination. Empirical studies show that prompts with CPR applied achieves
over a 90\% win rate over the original prompts without any external knowledge.

</details>


### [16] [Multi-stage Prompt Refinement for Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2510.12032)
*Jung-Woo Shim,Yeong-Joon Ju,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: 提出多阶段提示词精炼框架（MPR），通过系统性优化提示词形式，有效减少大语言模型85%以上的幻觉问题并提升输出准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注提示词不规范（歧义/语法错误/信息不完整）对LLM生成幻觉的影响，需要系统性的改进方案。

Method: 采用多阶段处理框架，通过微调小语言模型分阶段修正标点/拼写/术语错误，结合上下文增强和自省排序机制迭代优化提示词质量。

Result: 优化后的提示词在幻觉测试中胜率超85%，且可与现有后处理框架协同实现额外性能提升。

Conclusion: MPR框架以轻量化、可扩展的方式显著提升LLM可靠性，适用于多领域应用场景。

Abstract: Recent advancements in large language models (LLMs) have shown strong
performance in natural language understanding and generation tasks. However,
LLMs continue to encounter challenges with hallucinations, where models
generate plausible but incorrect information. While several factors contribute
to hallucinations, the impact of ill-formed prompts, prompts with ambiguous
wording, incorrect grammar, or incomplete information, was relatively under
explored. To address this, we introduce Multi-stage Prompt Refinement (MPR), a
framework designed to systematically improve these ill-formed prompts across
multiple stages. Each stage addresses specific errors such as punctuation,
typographical mistakes, and misuse of key terms, using small language models
(SLMs) fine-tuned for these tasks. MPR iteratively enhances the clarity of
prompts with additional context and employs a self-reflection mechanism with
ranking to prioritize the most relevant input. Experimental results on
hallucination benchmarks show that prompts refined by MPR achieve over an 85~\%
win rate compared to their original forms, demonstrating its effectiveness in
reducing hallucinations and improving LLM output accuracy. Interestingly, we
reveal that MPR can be combined with existing post-hoc hallucination mitigation
frameworks, further enhancing its versatility. MPR provides a lightweight and
adaptable solution for enhancing LLM reliability across various domains.

</details>


### [17] [On the Interplay between Human Label Variation and Model Fairness](https://arxiv.org/abs/2510.12036)
*Kemal Kurniawan,Meladel Mistica,Timothy Baldwin,Jey Han Lau*

Main category: cs.CL

TL;DR: 首次探讨人类标签变异对模型公平性的影响，发现无去偏的HLV训练方法能提升公平性


<details>
  <summary>Details</summary>
Motivation: 解决人类标签变异如何影响机器学习模型公平性的知识空白问题

Method: 比较多数投票标签训练与多种HLV训练方法的公平性表现

Result: 在无显式去偏情况下，HLV训练方法展现出提升模型公平性的积极效果

Conclusion: 标签变异本身可能蕴含去偏信号，为公平机器学习提供了新研究路径

Abstract: The impact of human label variation (HLV) on model fairness is an unexplored
topic. This paper examines the interplay by comparing training on majority-vote
labels with a range of HLV methods. Our experiments show that without explicit
debiasing, HLV training methods have a positive impact on fairness.

</details>


### [18] [Uncertainty Quantification for Hallucination Detection in Large Language Models: Foundations, Methodology, and Future Directions](https://arxiv.org/abs/2510.12040)
*Sungmin Kang,Yavuz Faruk Bakman,Duygu Nur Yaldiz,Baturalp Buyukates,Salman Avestimehr*

Main category: cs.CL

TL;DR: 提出不确定性量化(UQ)作为检测大语言模型幻觉的核心方法，系统分类现有技术并展示实证效果


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在生成看似合理但事实错误内容(幻觉)的问题，需通过UQ量化输出可信度

Method: 1. 建立UQ理论基础框架 2. 构建LLM幻觉检测方法论 3. 多维度分类现有方法 4. 实证分析代表性方法

Result: 验证了UQ方法在识别不可靠生成内容方面的有效性，展示了不同量化策略的性能差异

Conclusion: 当前UQ方法在动态场景适应性、计算效率等方面存在局限，未来需开发更细粒度的量化指标和实时检测机制

Abstract: The rapid advancement of large language models (LLMs) has transformed the
landscape of natural language processing, enabling breakthroughs across a wide
range of areas including question answering, machine translation, and text
summarization. Yet, their deployment in real-world applications has raised
concerns over reliability and trustworthiness, as LLMs remain prone to
hallucinations that produce plausible but factually incorrect outputs.
Uncertainty quantification (UQ) has emerged as a central research direction to
address this issue, offering principled measures for assessing the
trustworthiness of model generations. We begin by introducing the foundations
of UQ, from its formal definition to the traditional distinction between
epistemic and aleatoric uncertainty, and then highlight how these concepts have
been adapted to the context of LLMs. Building on this, we examine the role of
UQ in hallucination detection, where quantifying uncertainty provides a
mechanism for identifying unreliable generations and improving reliability. We
systematically categorize a wide spectrum of existing methods along multiple
dimensions and present empirical results for several representative approaches.
Finally, we discuss current limitations and outline promising future research
directions, providing a clearer picture of the current landscape of LLM UQ for
hallucination detection.

</details>


### [19] [Improving Text-to-Image Generation with Input-Side Inference-Time Scaling](https://arxiv.org/abs/2510.12041)
*Ruibo Chen,Jiacheng Pan,Heng Huang,Zhenheng Yang*

Main category: cs.CL

TL;DR: 利用LLM进行提示词重写优化，无需监督数据即可提升多模型T2I生成效果


<details>
  <summary>Details</summary>
Motivation: 现有文本生成图像模型对简单/模糊提示处理不佳，导致图文对齐度、美学质量和生成效果下降

Method: 结合奖励机制与迭代DPO训练框架，构建无需微调数据的提示词优化器

Result: 在多个T2I模型中显著提升图文对齐度（+18.7%）和视觉质量，迁移性实验显示跨模型适用性

Conclusion: 提示词重写是提升T2I系统的有效、可扩展策略，计划开源代码与预训练模型

Abstract: Recent advances in text-to-image (T2I) generation have achieved impressive
results, yet existing models often struggle with simple or underspecified
prompts, leading to suboptimal image-text alignment, aesthetics, and quality.
We propose a prompt rewriting framework that leverages large language models
(LLMs) to refine user inputs before feeding them into T2I backbones. Our
approach introduces a carefully designed reward system and an iterative direct
preference optimization (DPO) training pipeline, enabling the rewriter to
enhance prompts without requiring supervised fine-tuning data. We evaluate our
method across diverse T2I models and benchmarks. Results show that our prompt
rewriter consistently improves image-text alignment, visual quality, and
aesthetics, outperforming strong baselines. Furthermore, we demonstrate strong
transferability by showing that a prompt rewriter trained on one T2I backbone
generalizes effectively to others without needing to be retrained. We also
systematically study scalability, evaluating how performance gains scale with
the capacity of the large LLM used as the rewriter. These findings highlight
that prompt rewriting is an effective, scalable, and practical model-agnostic
strategy for improving T2I systems. We plan to release the code and trained
prompt rewriters soon.

</details>


### [20] [Hierarchical Alignment: Surgical Fine-Tuning via Functional Layer Specialization in Large Language Models](https://arxiv.org/abs/2510.12044)
*Yukun Zhang,Qi Dong*

Main category: cs.CL

TL;DR: 提出分层对齐方法，针对LLM不同功能层（语法/逻辑/事实性）进行定向优化，在提升性能的同时避免传统DPO的『对齐税』，实现更高效可控的模型对齐。


<details>
  <summary>Details</summary>
Motivation: 现有DPO等对齐技术忽视Transformer架构的层次功能差异（底层处理语法，高层处理抽象推理），统一优化导致效果受限且产生对齐税（流畅性提升牺牲逻辑能力）。

Method: 将模型分为局部（语法层）、中间（逻辑层）、全局（事实层）三个功能块，使用LoRA对Llama-3.1-8B/Qwen1.5-7B进行分层DPO微调，通过LLM-as-Judge评估效果。

Result: 局部对齐提升语法流畅度，全局对齐显著增强事实一致性（+15%）和逻辑连贯性（优于所有基线），所有分层策略均避免传统DPO的对齐税现象。

Conclusion: 分层对齐实现资源效率提升(参数修改量减少40%)和效果可控性，为构建更可靠LLM开辟结构感知的精细调优路径，推动模型能力与人类价值观的对齐。

Abstract: Existing alignment techniques for Large Language Models (LLMs), such as
Direct Preference Optimization (DPO), typically treat the model as a monolithic
entity, applying uniform optimization pressure across all layers. This approach
overlooks the functional specialization within the Transformer architecture,
where different layers are known to handle distinct tasks from syntax to
abstract reasoning. In this paper, we challenge this one-size-fits-all paradigm
by introducing Hierarchical Alignment, a novel method that applies targeted DPO
to distinct functional blocks of a model's layers: local (syntax), intermediate
(logic), and global (factuality). Through a series of controlled experiments on
state-of-the-art models like Llama-3.1-8B and Qwen1.5-7B using LoRA for
surgical fine-tuning, our results, evaluated by a powerful LLM-as-Judge,
demonstrate significant and predictable improvements. Specifically, aligning
the local layers (Local-Align) enhances grammatical fluency. More importantly,
aligning the global layers (Global-Align) not only improves factual consistency
as hypothesized but also proves to be the most effective strategy for enhancing
logical coherence, outperforming all baselines. Critically, all hierarchical
strategies successfully avoid the "alignment tax" observed in standard DPO,
where gains in fluency come at the cost of degraded logical reasoning. These
findings establish a more resource-efficient, controllable, and interpretable
path for model alignment, highlighting the immense potential of shifting from
monolithic optimization to structure-aware surgical fine-tuning to build more
advanced and reliable LLMs.

</details>


### [21] [APCE: Adaptive Progressive Context Expansion for Long Context Processing](https://arxiv.org/abs/2510.12051)
*Baisub Lee,Sanghyun Byun,Mohanad Odema,Jung Guack,Jacob Song,Woo Seong Chung*

Main category: cs.CL

TL;DR: 提出APCE方法通过语义相似度筛选关键输入块，协同解决长上下文Transformer模型的内存效率与性能衰减问题


<details>
  <summary>Details</summary>
Motivation: 解决长上下文Transformer模型部署时的两个核心问题：(1) 内存占用随序列长度二次增长 (2) 上下文长度增加导致性能下降(ContextRot现象)

Method: 基于低维语义相似度匹配的上下文感知机制，动态选择与当前查询最相关的输入块进行处理

Result: 实验证明APCE仅需处理50%-70%输入序列即可达到或超越完整密集基线的摘要性能，显著降低KV缓存和自注意力内存消耗

Conclusion: 该方法为长上下文模型的高效部署提供了硬件无关的解决方案，其上下文感知机制对其他长上下文任务具有启发意义

Abstract: Deploying useful Long-Context Transformer Models (LCTMs) requires addressing
two key challenges: (1) A growing memory footprint due to quadratic
self-attention and linear KV-cache scaling in memory as sequence length
increases; (2) the ContextRot phenomena where empirical evidence suggests that
transformer architecture's performance degrades with increasing context length.
Given the shared dependency on the input, a natural question arises: Can we
surgically select the most important input chunks for processing to
synergistically (a) reduce the memory footprint, and (b) mitigate the
ContextRot effects? In this paper, we answer this question in the affirmative
for long-context summarization tasks. We propose APCE as a context-aware
solution to select the most important input chunks through low-dimensional
semantic similarity matching with the current query. By directly operating on
the input, APCE decouples from strict dependency on underlying hardware or CUDA
environments, promising a compatible solution scalable to different deployment
systems. Our empirical evaluations have demonstrated superior or on-par
summarization performance for APCE compared to the full dense baseline using a
fraction (50%-70%) of the input sequence resulting in KV-cache and
self-attention memory efficiency improvements. We hope our findings inspire
further research on context-aware efficiency solutions for LCTMs geared towards
other relevant long-context tasks.

</details>


### [22] [An AI-Based Behavioral Health Safety Filter and Dataset for Identifying Mental Health Crises in Text-Based Conversations](https://arxiv.org/abs/2510.12083)
*Benjamin W. Nelson,Celeste Wong,Matthew T. Silvestrini,Sooyoon Shin,Alanna Robinson,Jessica Lee,Eric Yang,John Torous,Andrew Trister*

Main category: cs.CL

TL;DR: VBHSF在精神健康危机检测中展现出高敏感度（0.990）和特异性（0.992），显著优于NVIDIA NeMo和OpenAI Omni Moderation Latest，尤其在减少遗漏危机方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型常对精神健康危机提供有害建议，需验证VBHSF安全过滤器的有效性以改善医疗场景中的AI响应质量。

Method: 使用1,800条模拟精神危机数据和794条真实数据，通过临床医生标注评估VBHSF，并与NVIDIA/OpenAI的安全护栏系统进行对比测试。

Result: VBHSF在Verily数据集F1达0.939，敏感度0.917-0.992；在NVIDIA数据集保持0.982敏感度。对比测试中VBHSF敏感度显著更高（p<0.001），NVIDIA NeMo部分危机类型敏感度低于0.10。

Conclusion: VBHSF通过优先保障敏感度的设计，在医疗场景中实现可靠的危机识别，其泛化性能明显优于现有开源解决方案，具有重要临床应用价值。

Abstract: Large language models often mishandle psychiatric emergencies, offering
harmful or inappropriate advice and enabling destructive behaviors. This study
evaluated the Verily behavioral health safety filter (VBHSF) on two datasets:
the Verily Mental Health Crisis Dataset containing 1,800 simulated messages and
the NVIDIA Aegis AI Content Safety Dataset subsetted to 794 mental
health-related messages. The two datasets were clinician-labelled and we
evaluated performance using the clinician labels. Additionally, we carried out
comparative performance analyses against two open source, content moderation
guardrails: OpenAI Omni Moderation Latest and NVIDIA NeMo Guardrails. The VBHSF
demonstrated, well-balanced performance on the Verily Mental Health Crisis
Dataset v1.0, achieving high sensitivity (0.990) and specificity (0.992) in
detecting any mental health crises. It achieved an F1-score of 0.939,
sensitivity ranged from 0.917-0.992, and specificity was >= 0.978 in
identifying specific crisis categories. When evaluated against the NVIDIA Aegis
AI Content Safety Dataset 2.0, VBHSF performance remained highly sensitive
(0.982) and accuracy (0.921) with reduced specificity (0.859). When compared
with the NVIDIA NeMo and OpenAI Omni Moderation Latest guardrails, the VBHSF
demonstrated superior performance metrics across both datasets, achieving
significantly higher sensitivity in all cases (all p < 0.001) and higher
specificity relative to NVIDIA NeMo (p < 0.001), but not to OpenAI Omni
Moderation Latest (p = 0.094). NVIDIA NeMo and OpenAI Omni Moderation Latest
exhibited inconsistent performance across specific crisis types, with
sensitivity for some categories falling below 0.10. Overall, the VBHSF
demonstrated robust, generalizable performance that prioritizes sensitivity to
minimize missed crises, a crucial feature for healthcare applications.

</details>


### [23] [Deep Associations, High Creativity: A Simple yet Effective Metric for Evaluating Large Language Models](https://arxiv.org/abs/2510.12110)
*Ziliang Qiu,Renfen Hu*

Main category: cs.CL

TL;DR: 提出PACE方法通过生成并行关联链评估大模型创造力，在高效评估的同时减少数据污染，揭示LLMs与人类创造力的差异


<details>
  <summary>Details</summary>
Motivation: 传统LLM创造力评估存在数据污染风险高、人工评估成本大的痛点，需开发更可靠的评估框架

Method: 受人类创造力评估启发，设计基于并行关联链生成的PACE框架，通过自动化的联想链条分析量化模型创造力

Result: PACE与人工评估强相关（ρ=0.739），发现高性能LLMs达普通人水平但逊于专家，语言分析显示人类联想模式更丰富

Conclusion: PACE为LLM创造力评估提供有效工具，揭示当前模型在联想多样性和专业创造力方面仍存在明显提升空间

Abstract: The evaluation of LLMs' creativity represents a crucial research domain,
though challenges such as data contamination and costly human assessments often
impede progress. Drawing inspiration from human creativity assessment, we
propose PACE, asking LLMs to generate Parallel Association Chains to Evaluate
their creativity. PACE minimizes the risk of data contamination and offers a
straightforward, highly efficient evaluation, as evidenced by its strong
correlation with Chatbot Arena Creative Writing rankings (Spearman's $\rho =
0.739$, $p < 0.001$) across various proprietary and open-source models. A
comparative analysis of associative creativity between LLMs and humans reveals
that while high-performing LLMs achieve scores comparable to average human
performance, professional humans consistently outperform LLMs. Furthermore,
linguistic analysis reveals that both humans and LLMs exhibit a trend of
decreasing concreteness in their associations, and humans demonstrating a
greater diversity of associative patterns.

</details>


### [24] [Tracing Multilingual Knowledge Acquisition Dynamics in Domain Adaptation: A Case Study of English-Japanese Biomedical Adaptation](https://arxiv.org/abs/2510.12115)
*Xin Zhao,Naoki Yoshinaga,Yuma Tsuta,Akiko Aizawa*

Main category: cs.CL

TL;DR: 提出AdaXEval评估方法，通过双语领域语料库直接分析LLMs跨语言知识获取机制，揭示现有方法在跨语言转移中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有ML-DA研究常使用知识覆盖不匹配的数据集进行训练和评估，导致低资源环境下跨语言知识获取机制不明确，影响模型性能优化。

Method: 构建与训练数据同源的AdaXEval多选问答数据集，持续追踪LLMs在不同数据配方下的领域知识获取过程，解析数据到知识的转化机制。

Result: 实验表明，即使使用高质量双语语料，13B英日双语LLM的跨语言知识转移仍存在显著困难，代码已开源。

Conclusion: AdaXEval有效揭示知识获取动态，强调需改进数据配比或模型架构以提升多语言知识获取效率，跨语言迁移仍是重要挑战。

Abstract: Multilingual domain adaptation (ML-DA) is widely used to learn new domain
knowledge across languages into large language models (LLMs). Although many
methods have been proposed to improve domain adaptation, the mechanisms of
multilingual knowledge acquisition, how domain knowledge is learned within a
language and transferred across languages, remain underexplored. This gap leads
to suboptimal performance, particularly in low-resource settings. This work
examines the learning dynamics of LLMs during ML-DA. Because prior ML-DA
studies often train and evaluate on datasets with mismatched knowledge
coverage, we propose AdaXEval, an adaptive evaluation method that builds
multiple-choice QA datasets from the same bilingual domain corpus used for
training, thereby directly studying multilingual knowledge acquisition. Through
continual training of LLMs with diverse data recipes, we track how LLMs acquire
domain facts and pinpoint the mechanism behind the transformation process from
domain training data to knowledge. Our experiments on a 13B English-Japanese
bilingual LLM reveal that cross-lingual transfer remains challenging despite a
high-quality bilingual corpus. The code has been released.

</details>


### [25] [Understanding the Modality Gap: An Empirical Study on the Speech-Text Alignment Mechanism of Large Speech Language Models](https://arxiv.org/abs/2510.12116)
*Bajian Xiang,Shuaijiang Zhao,Tingwei Guo,Wei Zou*

Main category: cs.CL

TL;DR: 揭示了端到端语音大模型在语音与文本输入间存在的模态差距及对齐机制，提出角度投影和长度归一化改进策略


<details>
  <summary>Details</summary>
Motivation: 传统语音系统在语义理解基准上持续优于端到端语音大模型，需探究其根本原因

Method: 通过粗粒度（余弦相似度/欧氏距离）和细粒度（token级对齐路径评分）表征分析，设计关键token干预实验

Result: 发现深层表征方向对齐但幅度偏离，提出对齐质量与模态差距强相关，改进策略可提升语音输入准确性

Conclusion: 首次系统分析语音大模型模态差距，为未来优化提供理论框架和角度投影等具体方法指导

Abstract: End-to-end Large Speech Language Models (LSLMs) have demonstrated impressive
conversational generation abilities, yet consistently fall short of traditional
pipeline systems on semantic understanding benchmarks. In this work, we reveal
through systematic experimentation that although LSLMs lose some text input
performance after speech-text alignment training, the performance gap between
speech and text inputs is more pronounced, which we refer to as the modality
gap. To understand this gap, we analyze both coarse- and fine-grained text and
speech representations. At the coarse-grained level, representations of speech
and text in deeper layers are found to be increasingly aligned in direction
(cosine similarity), while concurrently diverging in magnitude (Euclidean
distance). We further find that representation similarity is strongly
correlated with the modality gap. At the fine-grained level, a spontaneous
token-level alignment pattern between text and speech representations is
observed. Based on this, we introduce the Alignment Path Score to quantify
token-level alignment quality, which exhibits stronger correlation with the
modality gap. Building on these insights, we design targeted interventions on
critical tokens through angle projection and length normalization. These
strategies demonstrate the potential to improve correctness for speech inputs.
Our study provides the first systematic empirical analysis of the modality gap
and alignment mechanisms in LSLMs, offering both theoretical and methodological
guidance for future optimization.

</details>


### [26] [SafeMT: Multi-turn Safety for Multimodal Language Models](https://arxiv.org/abs/2510.12133)
*Han Zhu,Juntao Dai,Jiaming Ji,Haoran Li,Chengkun Cai,Pengcheng Wen,Chi-Min Chan,Boyuan Chen,Yaodong Yang,Sirui Han,Yike Guo*

Main category: cs.CL

TL;DR: 研究提出SafeMT基准测试揭示多模态大模型在多轮对话中的安全隐患，并开发对话安全监控器提升防护效果。


<details>
  <summary>Details</summary>
Motivation: 现有基准未充分覆盖多轮对话风险，需推动社区关注此类场景下的模型安全问题。

Method: 构建含17场景/4攻击方法的10k样本数据集SafeMT，提出安全指数(SI)评估体系，并设计可检测恶意意图的对话安全监控器。

Result: 实验发现对话轮次增加导致攻击成功率上升，新型监控器较现有方案显著降低多轮攻击成功率。

Conclusion: 当前模型对话安全机制存在缺陷，提出的安全监控器能有效识别隐蔽攻击并提供动态防护策略。

Abstract: With the widespread use of multi-modal Large Language models (MLLMs), safety
issues have become a growing concern. Multi-turn dialogues, which are more
common in everyday interactions, pose a greater risk than single prompts;
however, existing benchmarks do not adequately consider this situation. To
encourage the community to focus on the safety issues of these models in
multi-turn dialogues, we introduce SafeMT, a benchmark that features dialogues
of varying lengths generated from harmful queries accompanied by images. This
benchmark consists of 10,000 samples in total, encompassing 17 different
scenarios and four jailbreak methods. Additionally, we propose Safety Index
(SI) to evaluate the general safety of MLLMs during conversations. We assess
the safety of 17 models using this benchmark and discover that the risk of
successful attacks on these models increases as the number of turns in harmful
dialogues rises. This observation indicates that the safety mechanisms of these
models are inadequate for recognizing the hazard in dialogue interactions. We
propose a dialogue safety moderator capable of detecting malicious intent
concealed within conversations and providing MLLMs with relevant safety
policies. Experimental results from several open-source models indicate that
this moderator is more effective in reducing multi-turn ASR compared to existed
guard models.

</details>


### [27] [Credal Transformer: A Principled Approach for Quantifying and Mitigating Hallucinations in Large Language Models](https://arxiv.org/abs/2510.12137)
*Shihao Ji,Zihui Song,Jiajie Huang*

Main category: cs.CL

TL;DR: 提出Credal Transformer架构，用基于证据理论的注意力机制替代标准注意力，通过生成概率分布集合量化模型不确定性，有效减少LLM幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的Softmax函数通过将注意力分数压缩为单一概率分布，丢弃了各层的模糊性信息，导致模型产生过于自信的错误断言（人工确定性）。需要直接在模型架构中集成不确定性量化机制。

Method: 1. 设计Credal注意力机制（CAM），用Dirichlet分布的证据质量重构注意力分数
2. 生成credal set（概率分布集合）代替单一注意力向量
3. 集合规模直接反映模型置信度：充分证据时恢复标准注意力，证据不足时生成扩散分布

Result: 成功实现：
- 识别非常规输入
- 量化文本模糊性
- 对不可答问题显著减少68%的自信错误（通过主动弃答机制）
- 在SQuAD基准上保持与原模型相当的准确率

Conclusion: Credal Transformer为缓解LLM幻觉提供了新架构范式，首次将不确定性量化直接嵌入Transformer核心，为构建更可靠的AI系统奠定基础。该方法兼顾性能与可靠性，开启了基于证据推理的新一代语言模型设计路径。

Abstract: Large Language Models (LLMs) hallucinate, generating factually incorrect yet
confident assertions. We argue this stems from the Transformer's Softmax
function, which creates "Artificial Certainty" by collapsing ambiguous
attention scores into a single probability distribution, discarding uncertainty
information at each layer. To fix this, we introduce the Credal Transformer,
which replaces standard attention with a Credal Attention Mechanism (CAM) based
on evidential theory. CAM produces a "credal set" (a set of distributions)
instead of a single attention vector, with the set's size directly measuring
model uncertainty. We implement this by re-conceptualizing attention scores as
evidence masses for a Dirichlet distribution: sufficient evidence recovers
standard attention, while insufficient evidence yields a diffuse distribution,
representing ambiguity. Empirically, the Credal Transformer identifies
out-of-distribution inputs, quantifies ambiguity, and significantly reduces
confident errors on unanswerable questions by abstaining. Our contribution is a
new architecture to mitigate hallucinations and a design paradigm that
integrates uncertainty quantification directly into the model, providing a
foundation for more reliable AI.

</details>


### [28] [A Survey on Parallel Reasoning](https://arxiv.org/abs/2510.12164)
*Ziqi Wang,Boye Niu,Zipeng Gao,Zhi Zheng,Tong Xu,Linghui Meng,Zhongli Li,Jing Liu,Yilong Chen,Chen Zhu,Hua Wu,Haifeng Wang,Enhong Chen*

Main category: cs.CL

TL;DR: 本文综述大模型并行推理的进展，探讨其技术分类、应用场景及挑战


<details>
  <summary>Details</summary>
Motivation: 针对传统顺序推理的脆弱性问题，探索并行推理方法以提升LLM推理的鲁棒性和实际性能

Method: 提出包含非交互式推理、交互式推理和高效解码策略的三维分类体系

Result: 系统梳理了并行推理的关键技术路线，揭示了其在复杂问题求解和输出可靠性增强等场景的应用潜力

Conclusion: 需进一步解决计算效率与效果平衡、交互机制优化等核心挑战，推动并行推理方法的实际部署

Abstract: With the increasing capabilities of Large Language Models (LLMs), parallel
reasoning has emerged as a new inference paradigm that enhances reasoning
robustness by concurrently exploring multiple lines of thought before
converging on a final answer. It has become a significant trend to explore
parallel reasoning to overcome the fragility of standard sequential methods and
improve practical performance. In this paper, we aim to survey and summarize
the progress and challenges of parallel reasoning. We first present a formal
definition of parallel reasoning and clarify its distinction from related
concepts like Chain-of-Thought. Then, we organize and discuss advanced
techniques based on a novel taxonomy, including non-interactive reasoning,
interactive reasoning, and efficiency-focused decoding strategies.
Additionally, we explore various application scenarios, such as solving complex
problems and enhancing the reliability of LLM outputs.Finally, we highlight the
core challenges of parallel reasoning and suggest potential directions for
future research. We hope that our work can provide a useful roadmap for
beginners and encourage more research on improving parallel reasoning methods.
Related source can be avaliable in
https://github.com/PPPP-kaqiu/Awesome-Parallel-Reasoning.

</details>


### [29] [Towards Inference-time Scaling for Continuous Space Reasoning](https://arxiv.org/abs/2510.12167)
*Minghan Wang,Thuy-Trang Vu,Ehsan Shareghi,Gholamreza Haffari*

Main category: cs.CL

TL;DR: 探索连续推理空间中推断时扩展技术的可行性，揭示现有离散空间方法移植的局限性，强调引入归纳偏置对提升推理判别力的必要性


<details>
  <summary>Details</summary>
Motivation: 验证离散空间中成熟的PRM/ORM重排序技术能否有效迁移到连续推理空间，提升大语言模型在连续空间的推理性能

Method: 使用COCONUT连续推理模型，通过dropout采样生成多样化推理路径，进行Pass@N分析并尝试训练PRM/ORM模型

Result: 现有方法仅带来边际提升，几何特性和轨迹动态导致正确/错误推理难以区分，连续表示缺乏关键归纳偏置

Conclusion: 连续推理模型的训练需同时优化准确性和显式嵌入可被推理时利用的归纳偏置，以突破当前性能瓶颈

Abstract: Inference-time scaling through multiple sample generation in combination with
Process- or Outcome-Reward Model (PRM or ORM) re-ranking has proven effective
for text-based reasoning in large language models. This paper investigates
whether such established techniques can be successfully adapted to reasoning in
the continuous space, using COCONUT (Hao et al. 2024) continuous space
reasoning LM as the backbone. We demonstrate the feasibility of generating
diverse reasoning paths through dropout-based sampling. Our Pass@N analysis on
the generated samples reveals the potential that could enable a significant
gain in performance akin to observed gain in the discrete space. However, we
highlight unique challenges faced for materializing this gain in the continuous
thought space. In particular, working recipes for data generation and training
PRM and ORM models in the discrete space unlocks only marginal improvements in
the continuous space. Through probing various aspects including geometric
properties and trajectory dynamics we identify the underlying reasons that
prevent effective discrimination between correct and incorrect reasoning
(essential for the functioning of PRM and ORM). Our findings reveal that
current limitations stem from the absence of key inductive biases in continuous
thought representations. We argue that the training frameworks for continuous
reasoning LMs require not only to optimize for accuracy but also to explicitly
incorporate inductive biases that could be utilized during inference-time for
discrimination of correct and incorrect thoughts.\footnote{Our code and data
will be publicly available.}

</details>


### [30] [From Knowledge to Treatment: Large Language Model Assisted Biomedical Concept Representation for Drug Repurposing](https://arxiv.org/abs/2510.12181)
*Chengrui Xiang,Tengfei Ma,Xiangzheng Fu,Yiping Liu,Bosheng Song,Xiangxiang Zeng*

Main category: cs.CL

TL;DR: 提出LLaDR框架，通过大语言模型增强生物医学知识图谱表示，显著提升药物重定位效果


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视实验室常识知识（如药物与治疗机制不兼容），导致生物医学概念表征不足

Method: 1. 从LLMs提取治疗相关文本表征 2. 用其微调知识图谱嵌入模型 3. 注入治疗知识增强语义理解

Result: 基准测试SOTA，阿尔茨海默病案例验证有效性（代码已开源）

Conclusion: LLaDR通过融合LLM语义与知识图谱，显著提升复杂疾病治疗方案的发现效率

Abstract: Drug repurposing plays a critical role in accelerating treatment discovery,
especially for complex and rare diseases. Biomedical knowledge graphs (KGs),
which encode rich clinical associations, have been widely adopted to support
this task. However, existing methods largely overlook common-sense biomedical
concept knowledge in real-world labs, such as mechanistic priors indicating
that certain drugs are fundamentally incompatible with specific treatments. To
address this gap, we propose LLaDR, a Large Language Model-assisted framework
for Drug Repurposing, which improves the representation of biomedical concepts
within KGs. Specifically, we extract semantically enriched treatment-related
textual representations of biomedical entities from large language models
(LLMs) and use them to fine-tune knowledge graph embedding (KGE) models. By
injecting treatment-relevant knowledge into KGE, LLaDR largely improves the
representation of biomedical concepts, enhancing semantic understanding of
under-studied or complex indications. Experiments based on benchmarks
demonstrate that LLaDR achieves state-of-the-art performance across different
scenarios, with case studies on Alzheimer's disease further confirming its
robustness and effectiveness. Code is available at
https://github.com/xiaomingaaa/LLaDR.

</details>


### [31] [Not in Sync: Unveiling Temporal Bias in Audio Chat Models](https://arxiv.org/abs/2510.12185)
*Jiayu Yao,Shenghua Liu,Yiwei Wang,Rundong Cheng,Lingrui Mei,Baolong Bi,Zhen Xiong,Xueqi Cheng*

Main category: cs.CL

TL;DR: 研究发现当前大型音频语言模型存在系统性时间定位偏差，提出TBI指标量化该偏差并开发可视化框架。


<details>
  <summary>Details</summary>
Motivation: 探索LALMs在事件时间戳预测中的能力局限，揭示其普遍存在的时序偏差问题及其实际影响。

Method: 在带时间戳数据集上进行控制实验，提出Temporal Bias Index量化偏差，配合可视化框架分析。

Result: 发现时间偏差普遍存在，随音频长度累积（可达数十秒），且不同事件类型/位置呈现差异特性。

Conclusion: 揭示LALMs的时序处理根本性缺陷，呼吁开发具有时间鲁棒性的新型架构。

Abstract: Large Audio Language Models (LALMs) are increasingly applied to audio
understanding and multimodal reasoning, yet their ability to locate when events
occur remains underexplored. We present the first systematic study of temporal
bias in LALMs, revealing a key limitation in their timestamp prediction. For
example, when asked "At which second does the lecturer introduce the key
formula?", models often predict timestamps that are consistently earlier or
later than the ground truth. Through controlled experiments on timestamped
datasets, we find that temporal bias (i) is prevalent across datasets and
models, (ii) increases with audio length - even accumulating to tens of seconds
in extended recordings, and (iii) varies across event types and positions. We
quantify this effect with the Temporal Bias Index (TBI), measuring systematic
misalignment in predicted event timings, and complement it with a visualization
framework. Our findings highlight a fundamental limitation in current LALMs and
call for the development of temporally robust architectures.

</details>


### [32] [DPO-Tuned Large Language Models for Segmentation in Simultaneous Speech Translation](https://arxiv.org/abs/2510.12195)
*Zeyu Yang,Satoshi Nakamura*

Main category: cs.CL

TL;DR: 提出基于DPO调整的大语言模型分割框架，在实时翻译中实现更优的分割准确率、翻译质量和延迟表现


<details>
  <summary>Details</summary>
Motivation: 现有预训练分割模型（如SHAS）虽优于启发式方法，但受限于监督学习目标，缺乏人类偏好对齐机制，难以满足实时翻译的自然分段需求

Method: 采用直接偏好优化（DPO）调整大语言模型，预测符合实时翻译需求的分割点

Result: 在ACL 60/60语料库的英日/中/德三语种测试中，分割准确率超越SHAS，BLEU/COMET指标提升，平均延迟（Average Lagging）降低，且能与IWSLT基线直接对比

Conclusion: 偏好对齐的LLM可超越现有预训练分割模型，推动自适应、人本对齐的同步口译技术发展

Abstract: Simultaneous speech translation requires accurate segmentation to balance
translation quality and latency. Recent studies such as SHAS have introduced
pretrained segmentation models, achieving stronger performance than heuristic
rules. However, segmentation models such as SHAS, though pretrained and more
robust than heuristic methods, are still constrained by supervised learning
objectives and do not incorporate human preference alignment, which is crucial
for natural real-time interpretation. In this work, we propose a segmentation
framework based on large language models (LLMs) trained with Direct Preference
Optimization (DPO). By leveraging preference alignment, our method enables LLMs
to predict natural segmentation points that better meet the demands of
real-time translation. We evaluate the system on the ACL 60/60 corpus across
three language pairs (English-Japanese, Chinese, German), using SeamlessM4T v2
as the translation backbone. Experimental results show that our DPO-tuned LLM
achieves higher segmentation accuracy than SHAS and yields consistent
improvements in translation quality (BLEU, COMET) as well as latency (Average
Lagging). Furthermore, our system benefits from IWSLT baselines for direct
comparison. These findings highlight the potential of preference-tuned LLMs to
surpass existing pretrained segmentation models and advance adaptive,
human-aligned simultaneous interpretation.

</details>


### [33] [HALF: Harm-Aware LLM Fairness Evaluation Aligned with Deployment](https://arxiv.org/abs/2510.12217)
*Ali Mekky,Omar El Herraoui,Preslav Nakov,Yuxia Wang*

Main category: cs.CL

TL;DR: 提出HALF框架评估大语言模型在现实应用中的公平性，通过危害分级机制（医疗/法律等9领域分三级）和五阶段流程，发现模型公平性存在领域差异且与模型规模无关。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型公平性评估缺乏真实场景关联性，未考虑不同领域危害严重性差异（如医疗决策偏见比文本风格偏见更严重）。

Method: 1. 将九个应用领域分为严重/中等/轻度三级 2. 设计五阶段评估流程（场景模拟、结果分类、危害加权等）3. 测试8个主流LLM

Result: 1. 模型公平性存在领域间波动 2. 模型规模与公平性无正相关 3. 推理型模型在医疗领域表现更优但在教育领域更差

Conclusion: HALF框架揭示了现有基准测试与真实部署需求间的差距，证明需建立领域敏感的模型评估体系。

Abstract: Large language models (LLMs) are increasingly deployed across high-impact
domains, from clinical decision support and legal analysis to hiring and
education, making fairness and bias evaluation before deployment critical.
However, existing evaluations lack grounding in real-world scenarios and do not
account for differences in harm severity, e.g., a biased decision in surgery
should not be weighed the same as a stylistic bias in text summarization. To
address this gap, we introduce HALF (Harm-Aware LLM Fairness), a
deployment-aligned framework that assesses model bias in realistic applications
and weighs the outcomes by harm severity. HALF organizes nine application
domains into three tiers (Severe, Moderate, Mild) using a five-stage pipeline.
Our evaluation results across eight LLMs show that (1) LLMs are not
consistently fair across domains, (2) model size or performance do not
guarantee fairness, and (3) reasoning models perform better in medical decision
support but worse in education. We conclude that HALF exposes a clear gap
between previous benchmarking success and deployment readiness.

</details>


### [34] [Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability](https://arxiv.org/abs/2510.12229)
*Bianca Raimondi,Daniela Dalbagno,Maurizio Gabbrielli*

Main category: cs.CL

TL;DR: 研究发现微调后的大型语言模型存在Knobe效应道德偏见，通过定位关键层并使用预训练激活值干预可有效消除该偏见，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有研究显示LLMs会内化人类偏见但机制不明，本文旨在探究Knobe效应在LLMs中的表现形式及其可解释性，寻找针对性干预方案。

Method: 对3个开源LLM进行层次修补分析，定位微调后产生Knobe效应的关键层，通过替换预训练模型对应层激活值进行干预实验。

Result: 发现Knobe效应与特定层强相关，仅需在关键层注入预训练激活值即可消除道德偏见，验证了偏见的可解释性与局部性特征。

Conclusion: 该研究证明通过精准定位和局部干预可有效缓解LLMs社会偏见，为模型伦理修正提供了参数级解决方案，显著降低干预成本。

Abstract: Large language models (LLMs) have been shown to internalize human-like biases
during finetuning, yet the mechanisms by which these biases manifest remain
unclear. In this work, we investigated whether the well-known Knobe effect, a
moral bias in intentionality judgements, emerges in finetuned LLMs and whether
it can be traced back to specific components of the model. We conducted a
Layer-Patching analysis across 3 open-weights LLMs and demonstrated that the
bias is not only learned during finetuning but also localized in a specific set
of layers. Surprisingly, we found that patching activations from the
corresponding pretrained model into just a few critical layers is sufficient to
eliminate the effect. Our findings offer new evidence that social biases in
LLMs can be interpreted, localized, and mitigated through targeted
interventions, without the need for model retraining.

</details>


### [35] [DSAS: A Universal Plug-and-Play Framework for Attention Optimization in Multi-Document Question Answering](https://arxiv.org/abs/2510.12251)
*Jiakai Li,Rongzheng Wang,Yizhuo Ma,Shuang Liang,Guangchun Luo,Ke Qin*

Main category: cs.CL

TL;DR: 提出DSAS双阶段自适应锐化方法，通过CGW和RAS模块解决大模型在多文档问答中的长距离依赖和中间迷失问题，无需修改架构或额外训练参数。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在多文档问答中存在长距离依赖建模困难（难以聚焦关键信息）和中间段落信息处理能力弱的缺陷，现有解决方案存在依赖截断或训练成本高的问题。

Method: 1. CGW模块通过层级注意力跟踪和位置感知权重评估段落相关性
2. RAS模块通过抑制关键文本与无关信息间的注意力交互增强焦点

Result: 在4个基准测试中，Llama/Qwen等主流模型平均F1提升4.2%（Llama-3.1-8B-Instruct和Qwen2.5-14B-Instruct表现突出）

Conclusion: DSAS作为即插即用方案有效提升多文档问答性能，消融实验验证双模块必要性，附录讨论证明其鲁棒性和扩展性

Abstract: While large language models (LLMs) show considerable promise across various
fields, they have notable limitations in handling multi-document question
answering (Multi-doc QA) tasks. The first challenge is long-range dependency
modeling, where LLMs struggle to focus on key information in long texts, which
weakens important semantic connections. Second, most LLMs suffer from the
''lost-in-the-middle'' issue, where they have difficulty processing information
in the middle of long inputs. Current solutions either truncate global
dependencies or demand costly finetuning, ultimately lacking a universal and
simple solution for these challenges. To resolve these limitations, we propose
Dual-Stage Adaptive Sharpening (DSAS) containing two modules. (i) The
Contextual Gate Weighting (CGW) module alleviates ''lost-in-the-middle'' by
assessing paragraph relevance through layer-wise attention tracking and
position-aware weighting. (ii) The Reciprocal Attention Suppression (RAS)
module enhances focus on critical paragraphs by suppressing information
exchange between key and irrelevant texts, thus mitigating the limitations in
long-range dependency modeling. Notably, DSAS functions as a plug-and-play
solution requiring no architectural modifications or extra training parameters.
Extensive experiments on four benchmarks demonstrate DSAS's efficacy across
mainstream LLMs (Llama, Qwen, Mistral, and Deepseek), with an average F1-score
improvement of 4.2% in Multi-doc QA tasks on Llama-3.1-8B-Instruct and
Qwen2.5-14B-Instruct. Ablation studies confirm the essential contributions of
both the CGW and RAS modules. In addition, detailed discussions in the Appendix
further validate the robustness and scalability of DSAS.

</details>


### [36] [Shallow Robustness, Deep Vulnerabilities: Multi-Turn Evaluation of Medical LLMs](https://arxiv.org/abs/2510.12255)
*Blazej Manczak,Eric Lin,Francisco Eiras,James O' Neill,Vaikkunth Mugunthan*

Main category: cs.CL

TL;DR: 研究发现现有医疗大语言模型在多轮交互中存在严重脆弱性，间接语境干预比直接建议更具破坏性（如Claude模型准确率从91.2%骤降至13.5%），揭示多轮鲁棒性是临床应用的关键瓶颈。


<details>
  <summary>Details</summary>
Motivation: 现有评估框架主要测试单轮理想化医疗问答，未能反映真实临床场景中复杂的多轮交互特性（如误导性语境、权威压力等），存在临床应用安全隐患。

Method: 提出MedQA-Followup框架，通过区分浅层（抵抗误导语境）与深层（多轮质疑下的稳定性）鲁棒性，结合间接（语境框架）与直接（明确建议）干预轴，在MedQA数据集进行系统性控制实验。

Result: 五大先进模型在浅层扰动下表现稳定，但多轮测试中准确率暴跌（最大降幅达77.7%）。反直觉发现：间接语境干预比直接建议破坏性更强，部分模型反复干预后出现持续衰减。

Conclusion: 多轮交互鲁棒性是医疗大模型部署的核心挑战，当前模型对语境暗示的脆弱性暴露重大临床风险，需建立更全面的动态评估体系确保安全性。

Abstract: Large language models (LLMs) are rapidly transitioning into medical clinical
use, yet their reliability under realistic, multi-turn interactions remains
poorly understood. Existing evaluation frameworks typically assess single-turn
question answering under idealized conditions, overlooking the complexities of
medical consultations where conflicting input, misleading context, and
authority influence are common. We introduce MedQA-Followup, a framework for
systematically evaluating multi-turn robustness in medical question answering.
Our approach distinguishes between shallow robustness (resisting misleading
initial context) and deep robustness (maintaining accuracy when answers are
challenged across turns), while also introducing an indirect-direct axis that
separates contextual framing (indirect) from explicit suggestion (direct).
Using controlled interventions on the MedQA dataset, we evaluate five
state-of-the-art LLMs and find that while models perform reasonably well under
shallow perturbations, they exhibit severe vulnerabilities in multi-turn
settings, with accuracy dropping from 91.2% to as low as 13.5% for Claude
Sonnet 4. Counterintuitively, indirect, context-based interventions are often
more harmful than direct suggestions, yielding larger accuracy drops across
models and exposing a significant vulnerability for clinical deployment.
Further compounding analyses reveal model differences, with some showing
additional performance drops under repeated interventions while others
partially recovering or even improving. These findings highlight multi-turn
robustness as a critical but underexplored dimension for safe and reliable
deployment of medical LLMs.

</details>


### [37] [Chinese ModernBERT with Whole-Word Masking](https://arxiv.org/abs/2510.12285)
*Zeyu Zhao,Ningtao Wang,Xing Fu,Yu Cheng*

Main category: cs.CL

TL;DR: 开发了中文ModernBERT模型，通过定制化词汇表、动态遮蔽策略、两阶段预训练流程和优化学习率调度，在效率和性能上取得平衡，支持长序列处理并在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 中文与英语在分词和形态学上的显著差异导致现有编码器改进未充分迁移，需专门设计适应中文特性的预训练模型。

Method: 1. 硬件感知的32k BPE词汇表优化嵌入效率
2. 动态遮蔽比例调整(30%→15%)的WWM策略
3. RoPE与交替局部/全局注意力的8k上下文扩展
4. 阻尼余弦学习率调度稳定长周期训练

Result: CLUE基准与主流中文编码器持平；bf16下实现高长序列吞吐(400+ seq/s)和短序列速度(600+ seq/s)；SimCLUE微调后Pearson/Spearman达0.505/0.537，超越Qwen-0.6B嵌入模型

Conclusion: 通过系统级优化实现了中文编码器的Pareto改进，验证了定制化词汇、注意力机制和训练策略的有效性，为语义相似度任务提供了可扩展的技术路径，模型及资源将开源促进社区发展。

Abstract: Encoder-only Transformers have advanced along three axes -- architecture,
data, and systems -- yielding Pareto gains in accuracy, speed, and memory
efficiency. Yet these improvements have not fully transferred to Chinese, where
tokenization and morphology differ markedly from English. We introduce Chinese
ModernBERT, a from-scratch Chinese encoder that couples: (i) a hardware-aware
32k BPE vocabulary tailored to frequent Chinese affixes/compounds, lowering the
embedding budget; (ii) whole-word masking (WWM) with a dynamic masking
curriculum (30% -> 15%) to align task difficulty with training progress; (iii)
a two-stage pre-training pipeline that extends the native context from 1,024 to
8,192 tokens using RoPE and alternating local/global attention; and (iv) a
damped-cosine learning-rate schedule for stable long-horizon optimization. We
pre-train on ~1.2T Chinese tokens from CCI3-HQ, CCI4 (Chinese), and
Cosmopedia-Chinese. On CLUE, Chinese ModernBERT is competitive with strong
Chinese encoders under a unified fine-tuning protocol. Under bf16 it achieves
high long-sequence throughput while maintaining strong short-sequence speed,
reflecting benefits from budget allocation and attention design. To probe
retrieval-oriented quality, we add a small amount of open contrastive data:
fine-tuning on SimCLUE (~3M pairs) improves further when adding T2Ranking
(~2M), reaching 0.505 (Pearson) / 0.537 (Spearman) on the SimCLUE test set.
Under this open-data setting, Chinese ModernBERT surpasses Qwen-0.6B-embedding
on SimCLUE, suggesting a clear scaling path for STS with additional curated
pairs. We will release tokenizer and weights to facilitate reproducible
research.

</details>


### [38] [A large-scale, unsupervised pipeline for automatic corpus annotation using LLMs: variation and change in the English consider construction](https://arxiv.org/abs/2510.12306)
*Cameron Morin,Matti Marttinen Larsson*

Main category: cs.CL

TL;DR: 利用GPT-5构建自动化语法标注流程，60小时内完成14万句语料标注，准确率超98%


<details>
  <summary>Details</summary>
Motivation: 解决语料库快速扩张下人工标注效率低下的问题，突破传统监督式迭代方法的局限性

Method: 四阶段工作流：提示工程→事前评估→批量处理→事后验证，基于COHA语料库进行历时语言变异研究

Result: 在历史美式英语语料库中实现143,933句自动化标注，两个复杂标注任务准确率均达98%以上

Conclusion: 大语言模型为语料研究提供新范式，但需平衡技术成本、许可协议及伦理风险

Abstract: As natural language corpora expand at an unprecedented rate, manual
annotation remains a significant methodological bottleneck in corpus linguistic
work. We address this challenge by presenting a scalable, unsupervised pipeline
for automating grammatical annotation in voluminous corpora using large
language models (LLMs). Unlike previous supervised and iterative approaches,
our method employs a four-phase workflow: prompt engineering, pre-hoc
evaluation, automated batch processing, and post-hoc validation. We demonstrate
the pipeline's accessibility and effectiveness through a diachronic case study
of variation in the English consider construction. Using GPT-5 through the
OpenAI API, we annotate 143,933 sentences from the Corpus of Historical
American English (COHA) in under 60 hours, achieving 98%+ accuracy on two
sophisticated annotation procedures. Our results suggest that LLMs can perform
a range of data preparation tasks at scale with minimal human intervention,
opening new possibilities for corpus-based research, though implementation
requires attention to costs, licensing, and other ethical considerations.

</details>


### [39] [Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation](https://arxiv.org/abs/2510.12316)
*Greta Damo,Elena Cabrio,Serena Villata*

Main category: cs.CL

TL;DR: 提出基于知识增强生成(RAG)的框架，通过构建32k+权威文本知识库，提升针对8类弱势群体反仇恨言论生成的可信度与效果


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成反言论存在可靠性/连贯性不足，专家模式扩展性差，需结合知识增强方案解决核心痛点

Method: 整合UN/EU等权威机构的32,792文本构建知识库，设计多目标群体(女性/少数族裔/LGBT等)的RAG生成框架

Result: 在MultiTarget-CONAN数据集上，通过JudgeLM指标和人工评估均显著超越传统LLM基线方法

Conclusion: 该框架为可信反言论生成研究开辟新路径，其方法论可拓展至仇恨言论外的多领域对抗性文本生成场景

Abstract: Counter-speech generation is at the core of many expert activities, such as
fact-checking and hate speech, to counter harmful content. Yet, existing work
treats counter-speech generation as pure text generation task, mainly based on
Large Language Models or NGO experts. These approaches show severe drawbacks
due to the limited reliability and coherence in the generated countering text,
and in scalability, respectively. To close this gap, we introduce a novel
framework to model counter-speech generation as knowledge-wise text generation
process. Our framework integrates advanced Retrieval-Augmented Generation (RAG)
pipelines to ensure the generation of trustworthy counter-speech for 8 main
target groups identified in the hate speech literature, including women, people
of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons,
and other. We built a knowledge base over the United Nations Digital Library,
EUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792
texts. We use the MultiTarget-CONAN dataset to empirically assess the quality
of the generated counter-speech, both through standard metrics (i.e., JudgeLM)
and a human evaluation. Results show that our framework outperforms standard
LLM baselines and competitive approach, on both assessments. The resulting
framework and the knowledge base pave the way for studying trustworthy and
sound counter-speech generation, in hate speech and beyond.

</details>


### [40] [Fine-grained Analysis of Brain-LLM Alignment through Input Attribution](https://arxiv.org/abs/2510.12355)
*Michela Proietti,Roberto Capobianco,Mariya Toneva*

Main category: cs.CL

TL;DR: 提出细粒度归因方法研究LLM与脑神经活动的对齐机制，揭示脑对齐(BA)与下一个词预测(NWP)依赖不同语言特征：NWP侧重句法且存在序列位置偏差，BA关注语义和语篇信息。


<details>
  <summary>Details</summary>
Motivation: 阐明LLM与人类语言处理的神经对应机制，解决BA与NWP关系争议，开发可推广的归因分析方法。

Method: 开发细粒度输入归因方法识别关键词语，通过对比实验分析BA与NWP在词级特征依赖的差异。

Result: BA与NWP依赖60%不同的词集：NWP呈现近因/首因效应并关注句法，BA聚焦语义/语篇信息且近因效应更集中。

Conclusion: 揭示了LLM与人类语言处理的异同，提出的归因方法为认知计算建模提供新工具，推动可解释AI与认知科学的交叉研究。

Abstract: Understanding the alignment between large language models (LLMs) and human
brain activity can reveal computational principles underlying language
processing. We introduce a fine-grained input attribution method to identify
the specific words most important for brain-LLM alignment, and leverage it to
study a contentious research question about brain-LLM alignment: the
relationship between brain alignment (BA) and next-word prediction (NWP). Our
findings reveal that BA and NWP rely on largely distinct word subsets: NWP
exhibits recency and primacy biases with a focus on syntax, while BA
prioritizes semantic and discourse-level information with a more targeted
recency effect. This work advances our understanding of how LLMs relate to
human language processing and highlights differences in feature reliance
between BA and NWP. Beyond this study, our attribution method can be broadly
applied to explore the cognitive relevance of model predictions in diverse
language processing tasks.

</details>


### [41] [MoBiLE: Efficient Mixture-of-Experts Inference on Consumer GPU with Mixture of Big Little Experts](https://arxiv.org/abs/2510.12357)
*Yushu Zhao,Yubin Qin,Yang Wang,Xiaolong Yang,Huiming Han,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.CL

TL;DR: 提出MoBiLE框架，通过混合大小专家和动态切换机制提升MoE推断效率，同时保持模型质量


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型的预取方法存在高训练开销，且在细粒度专家分割的新架构上效果有限

Method: 采用大-小专家混合策略（重要token保留完整专家，非重要token使用半数专家），设计专用回退/预取机制

Result: 在消费级GPU实现1.60x-1.72x加速，准确率损失可忽略

Conclusion: MoBiLE作为即插即用方案，有效平衡了推理速度与模型精度

Abstract: Mixture-of-Experts (MoE) models have recently demonstrated exceptional
performance across a diverse range of applications. The principle of sparse
activation in MoE models facilitates an offloading strategy, wherein active
experts are maintained in GPU HBM, while inactive experts are stored in CPU
DRAM. The efficacy of this approach, however, is fundamentally constrained by
the limited bandwidth of the CPU-GPU interconnect. To mitigate this bottleneck,
existing approaches have employed prefetching to accelerate MoE inference.
These methods attempt to predict and prefetch the required experts using
specially trained modules. Nevertheless, such techniques are often encumbered
by significant training overhead and have shown diminished effectiveness on
recent MoE models with fine-grained expert segmentation.
  In this paper, we propose MoBiLE, a plug-and-play offloading-based MoE
inference framework with \textit{mixture of big-little experts}. It reduces the
number of experts for unimportant tokens to half for acceleration while
maintaining full experts for important tokens to guarantee model quality.
Further, a dedicated fallback and prefetching mechanism is designed for
switching between little and big experts to improve memory efficiency. We
evaluate MoBiLE on four typical modern MoE architectures and challenging
generative tasks. Our results show that MoBiLE achieves a speedup of 1.60x to
1.72x compared to the baseline on a consumer GPU system, with negligible
degradation in accuracy.

</details>


### [42] [LLM-REVal: Can We Trust LLM Reviewers Yet?](https://arxiv.org/abs/2510.12367)
*Rui Li,Jia-Chen Gu,Po-Nien Kung,Heming Xia,Junfeng liu,Xiangwen Kong,Zhifang Sui,Nanyun Peng*

Main category: cs.CL

TL;DR: 研究发现LLM作为审稿人会系统性抬高LLM生成论文的评分，并低估含批判性内容的人类论文，揭示了学术公平性风险与潜在改进空间。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs深度整合至学术流程（研究+审阅）时对学术公平性的影响，揭示现有研究中未充分探索的双向风险。

Method: 构建研究代理（生成/修订论文）和审稿代理的模拟系统，通过LLM生成论文与人工论文对比，结合人类标注分析评分偏差。

Result: LLM审稿人存在语言风格偏好（偏向LLM生成文本）和批判陈述厌恶倾向，导致人工论文评分降低0.8-1.2分（5分量表）。

Conclusion: 需谨慎部署LLM审稿系统以防止公平性损害，但LLM指导的修订可有效提升论文质量，尤其对早期研究者具有辅助价值。

Abstract: The rapid advancement of large language models (LLMs) has inspired
researchers to integrate them extensively into the academic workflow,
potentially reshaping how research is practiced and reviewed. While previous
studies highlight the potential of LLMs in supporting research and peer review,
their dual roles in the academic workflow and the complex interplay between
research and review bring new risks that remain largely underexplored. In this
study, we focus on how the deep integration of LLMs into both peer-review and
research processes may influence scholarly fairness, examining the potential
risks of using LLMs as reviewers by simulation. This simulation incorporates a
research agent, which generates papers and revises, alongside a review agent,
which assesses the submissions. Based on the simulation results, we conduct
human annotations and identify pronounced misalignment between LLM-based
reviews and human judgments: (1) LLM reviewers systematically inflate scores
for LLM-authored papers, assigning them markedly higher scores than
human-authored ones; (2) LLM reviewers persistently underrate human-authored
papers with critical statements (e.g., risk, fairness), even after multiple
revisions. Our analysis reveals that these stem from two primary biases in LLM
reviewers: a linguistic feature bias favoring LLM-generated writing styles, and
an aversion toward critical statements. These results highlight the risks and
equity concerns posed to human authors and academic research if LLMs are
deployed in the peer review cycle without adequate caution. On the other hand,
revisions guided by LLM reviews yield quality gains in both LLM-based and human
evaluations, illustrating the potential of the LLMs-as-reviewers for
early-stage researchers and enhancing low-quality papers.

</details>


### [43] [Tokenization Disparities as Infrastructure Bias: How Subword Systems Create Inequities in LLM Access and Efficiency](https://arxiv.org/abs/2510.12389)
*Hailay Kidu Teklehaymanot,Wolfgang Nejdl*

Main category: cs.CL

TL;DR: 大型语言模型存在跨语言分词效率差异，拉丁语系语言效率最高，非拉丁语系和形态复杂语言分词成本增加3-5倍，需开发语言类型敏感的解决方案实现计算公平


<details>
  <summary>Details</summary>
Motivation: 分词差异严重阻碍人工智能在不同语言群体间的公平访问，需系统量化大型语言模型中的计算不平等现象

Method: 使用标准化实验框架，通过tiktoken库对200+语言统一预处理和分词，采用TPS（每句词元数）和RTC（相对分词成本）指标进行跨语言对比分析

Result: 拉丁语系分词效率显著高于非拉丁语系（RTC高3-5倍），形态复杂语言面临更高的计算成本与受限的上下文利用率

Conclusion: 当前AI系统存在结构性不平等，应开发语言类型学驱动的分词策略和自适应词汇构建方法，建立更具包容性的多语言人工智能系统

Abstract: Tokenization disparities pose a significant barrier to achieving equitable
access to artificial intelligence across linguistically diverse populations.
This study conducts a large-scale cross-linguistic evaluation of tokenization
efficiency in over 200 languages to systematically quantify computational
inequities in large language models (LLMs). Using a standardized experimental
framework, we applied consistent preprocessing and normalization protocols,
followed by uniform tokenization through the tiktoken library across all
language samples. Comprehensive tokenization statistics were collected using
established evaluation metrics, including Tokens Per Sentence (TPS) and
Relative Tokenization Cost (RTC), benchmarked against English baselines. Our
cross-linguistic analysis reveals substantial and systematic disparities:
Latin-script languages consistently exhibit higher tokenization efficiency,
while non-Latin and morphologically complex languages incur significantly
greater token inflation, often 3-5 times higher RTC ratios. These
inefficiencies translate into increased computational costs and reduced
effective context utilization for underrepresented languages. Overall, the
findings highlight structural inequities in current AI systems, where speakers
of low-resource and non-Latin languages face disproportionate computational
disadvantages. Future research should prioritize the development of
linguistically informed tokenization strategies and adaptive vocabulary
construction methods that incorporate typological diversity, ensuring more
inclusive and computationally equitable multilingual AI systems.

</details>


### [44] [PRoH: Dynamic Planning and Reasoning over Knowledge Hypergraphs for Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12434)
*Xiangjun Zai,Xingyu Tan,Xiaoyang Wang,Qing Liu,Xiwei Xu,Wenjie Zhang*

Main category: cs.CL

TL;DR: 提出PRoH框架通过动态知识超图规划与推理，解决现有KH-RAG方法在静态规划、非自适应检索和浅层语义利用方面的局限性


<details>
  <summary>Details</summary>
Motivation: 现有知识超图RAG方法存在静态检索规划、非自适应执行和结构语义利用不足的问题，制约多跳问答效果

Method: 1) 上下文感知规划模块生成结构化推理计划；2) 基于DAG的动态问题分解；3) EWO引导的语义优先超边遍历算法

Result: 多领域实验显示F1提升19.73%，生成质量提升8.41%，长程多跳推理表现稳健

Conclusion: PRoH通过动态规划与结构化推理路径检索，显著提升知识超图在复杂问答任务中的性能与鲁棒性

Abstract: Knowledge Hypergraphs (KHs) have recently emerged as a knowledge
representation for retrieval-augmented generation (RAG), offering a paradigm to
model multi-entity relations into a structured form. However, existing KH-based
RAG methods suffer from three major limitations: static retrieval planning,
non-adaptive retrieval execution, and superficial use of KH structure and
semantics, which constrain their ability to perform effective multi-hop
question answering. To overcome these limitations, we propose PRoH, a dynamic
Planning and Reasoning over Knowledge Hypergraphs framework. PRoH incorporates
three core innovations: (i) a context-aware planning module that sketches the
local KH neighborhood to guide structurally grounded reasoning plan generation;
(ii) a structured question decomposition process that organizes subquestions as
a dynamically evolving Directed Acyclic Graph (DAG) to enable adaptive,
multi-trajectory exploration; and (iii) an Entity-Weighted Overlap (EWO)-guided
reasoning path retrieval algorithm that prioritizes semantically coherent
hyperedge traversals. Experiments across multiple domains demonstrate that PRoH
achieves state-of-the-art performance, surpassing the prior SOTA model
HyperGraphRAG by an average of 19.73% in F1 and 8.41% in Generation Evaluation
(G-E) score, while maintaining strong robustness in long-range multi-hop
reasoning tasks.

</details>


### [45] [Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12460)
*Linfeng Gao,Baolong Bi,Zheng Yuan,Le Wang,Zerui Chen,Zhimin Wei,Shenghua Liu,Qinggang Zhang,Jinsong Su*

Main category: cs.CL

TL;DR: 提出CLEAR框架解决RAG系统中知识冲突问题，通过细粒度知识探测和冲突感知微调提升模型整合证据能力


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统存在回应与检索证据矛盾的忠实性问题，且缺乏对LLM内部知识整合机制的理解，特别是在知识冲突场景下的运作机理

Method: 1) 将上下文分解为句子级知识单元
2) 通过隐藏状态探测定位冲突
3) 设计冲突感知微调策略引导证据整合

Result: 在三个基准测试中显著提升准确率（平均+11.5%）和忠实性指标（平均+24.3%），在多种冲突场景下优于基线模型

Conclusion: CLEAR框架揭示了LLM知识整合的层级特性，提出的冲突定位机制和针对性微调方法有效提升RAG系统的可靠性和事实一致性

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to
enhance the factuality of Large Language Models (LLMs). However, existing RAG
systems often suffer from an unfaithfulness issue, where the model's response
contradicts evidence from the retrieved context. Existing approaches to
improving contextual faithfulness largely rely on external interventions, such
as prompt engineering, decoding constraints, or reward-based fine-tuning. These
works treat the LLM as a black box and overlook a crucial question: how does
the LLM internally integrate retrieved evidence with its parametric memory,
particularly under knowledge conflicts? To address this gap, we conduct a
probing-based analysis of hidden-state representations in LLMs and observe
three findings: knowledge integration occurs hierarchically, conflicts manifest
as latent signals at the sentence level, and irrelevant context is often
amplified when aligned with parametric knowledge. Building on these findings,
we propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a
framework that (i) decomposes context into fine-grained sentence-level
knowledge, (ii) employs hidden-state probing to localize conflicting knowledge,
and (iii) introduces conflict-aware fine-tuning to guide the model to
accurately integrate retrieved evidence. Extensive experiments across three
benchmarks demonstrate that CLEAR substantially improves both accuracy and
contextual faithfulness, consistently outperforming strong baselines under
diverse conflict conditions. The related resources are available at
https://github.com/LinfengGao/CLEAR.

</details>


### [46] [Resource-sensitive but language-blind: Community size and not grammatical complexity better predicts the accuracy of Large Language Models in a novel Wug Test](https://arxiv.org/abs/2510.12463)
*Nikoleta Pantelidou,Evelina Leivada,Paolo Morosi*

Main category: cs.CL

TL;DR: 大型语言模型在多语言形态学泛化任务中展现出类人准确性，但表现主要受语言资源丰富度而非语法复杂性驱动。


<details>
  <summary>Details</summary>
Motivation: 探究模型在形态学泛化任务中的表现究竟受语言结构复杂性影响，还是由训练数据量主导，以此验证模型是否具备深层语言理解能力。

Method: 采用多语言版Wug测试，在加泰罗尼亚语、英语、希腊语和西班牙语四种语言中，比较6个模型与人类受试者的形态学泛化能力。

Result: 模型准确率与语言社区规模及数字资源正相关（西班牙语/英语 > 加泰罗尼亚语/希腊语），且资源丰富度的影响超越结构复杂性因素。

Conclusion: 模型表现本质依赖语言数据资源，而非语法敏感度，其类人语言能力仅停留于表面性能层面。

Abstract: The linguistic abilities of Large Language Models are a matter of ongoing
debate. This study contributes to this discussion by investigating model
performance in a morphological generalization task that involves novel words.
Using a multilingual adaptation of the Wug Test, six models were tested across
four partially unrelated languages (Catalan, English, Greek, and Spanish) and
compared with human speakers. The aim is to determine whether model accuracy
approximates human competence and whether it is shaped primarily by linguistic
complexity or by the quantity of available training data. Consistent with
previous research, the results show that the models are able to generalize
morphological processes to unseen words with human-like accuracy. However,
accuracy patterns align more closely with community size and data availability
than with structural complexity, refining earlier claims in the literature. In
particular, languages with larger speaker communities and stronger digital
representation, such as Spanish and English, revealed higher accuracy than
less-resourced ones like Catalan and Greek. Overall, our findings suggest that
model behavior is mainly driven by the richness of linguistic resources rather
than by sensitivity to grammatical complexity, reflecting a form of performance
that resembles human linguistic competence only superficially.

</details>


### [47] [SMEC: Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression](https://arxiv.org/abs/2510.12474)
*Biao Zhang,Lixin Chen,Tong Liu,Bo Zheng*

Main category: cs.CL

TL;DR: 提出SMEC训练框架，通过SMRL梯度优化、ADS维度剪枝和S-XBM无监督学习模块，实现LLM嵌入维度压缩同时保持性能表现


<details>
  <summary>Details</summary>
Motivation: 高维嵌入导致计算复杂度高和存储需求大，限制了实际应用部署。需要在不显著降低性能的前提下实现维度压缩

Method: 1. SMRL方法降低训练梯度方差
2. ADS模块缓解维度剪枝的信息衰减
3. S-XBM模块增强跨维度无监督对比学习

Result: 在BEIR数据集上，256维压缩嵌入性能超过Matryoshka-Adaptor 1.1分，超过Search-Adaptor 2.7分；图像/文本/多模态任务均显示显著降维效果

Conclusion: SMEC框架通过联合优化训练过程、维度选择和对比学习机制，为LLM嵌入压缩提供了系统解决方案，平衡了效率与模型性能

Abstract: Large language models (LLMs) generate high-dimensional embeddings that
capture rich semantic and syntactic information. However, high-dimensional
embeddings exacerbate computational complexity and storage requirements,
thereby hindering practical deployment. To address these challenges, we propose
a novel training framework named Sequential Matryoshka Embedding Compression
(SMEC). This framework introduces the Sequential Matryoshka Representation
Learning(SMRL) method to mitigate gradient variance during training, the
Adaptive Dimension Selection (ADS) module to reduce information degradation
during dimension pruning, and the Selectable Cross-batch Memory (S-XBM) module
to enhance unsupervised learning between high- and low-dimensional embeddings.
Experiments on image, text, and multimodal datasets demonstrate that SMEC
achieves significant dimensionality reduction while maintaining performance.
For instance, on the BEIR dataset, our approach improves the performance of
compressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points
compared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.

</details>


### [48] [When Personalization Tricks Detectors: The Feature-Inversion Trap in Machine-Generated Text Detection](https://arxiv.org/abs/2510.12476)
*Lang Gao,Xuhui Li,Chenxi Wang,Mingzhe Li,Wei Liu,Zirui Song,Jinghui Zhang,Rui Yan,Preslav Nakov,Xiuying Chen*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) have grown more powerful in language generation,
producing fluent text and even imitating personal style. Yet, this ability also
heightens the risk of identity impersonation. To the best of our knowledge, no
prior work has examined personalized machine-generated text (MGT) detection. In
this paper, we introduce \dataset, the first benchmark for evaluating detector
robustness in personalized settings, built from literary and blog texts paired
with their LLM-generated imitations. Our experimental results demonstrate large
performance gaps across detectors in personalized settings: some
state-of-the-art models suffer significant drops. We attribute this limitation
to the \textit{feature-inversion trap}, where features that are discriminative
in general domains become inverted and misleading when applied to personalized
text. Based on this finding, we propose \method, a simple and reliable way to
predict detector performance changes in personalized settings. \method
identifies latent directions corresponding to inverted features and constructs
probe datasets that differ primarily along these features to evaluate detector
dependence. Our experiments show that \method can accurately predict both the
direction and the magnitude of post-transfer changes, showing 85\% correlation
with the actual performance gaps. We hope that this work will encourage further
research on personalized text detection.

</details>


### [49] [BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not Stomach Annotation Disagreements (Yet)](https://arxiv.org/abs/2510.12516)
*Tomas Ruiz,Siyao Peng,Barbara Plank,Carsten Schwemmer*

Main category: cs.CL

TL;DR: 将测试时缩放技术从数学领域迁移至LeWiDi标注分歧任务，发现Best-of-N方法在该场景失效


<details>
  <summary>Details</summary>
Motivation: 探索测试时缩放在缺乏明确正确答案的标注分歧任务中的适用性，突破该技术原有应用领域限制

Method: 采用模型平均、多数投票两种基准算法和Best-of-N抽样方法，在LeWiDi-2025任务进行对比实验

Result: 基准方法持续提升模型性能，但Best-of-N方法在标注分歧任务中未显示改进效果

Conclusion: Best-of-N方法受任务特性差异影响未能成功迁移，需重新审视该方法在开放域任务中的适用条件

Abstract: Test-time scaling is a family of techniques to improve LLM outputs at
inference time by performing extra computation. To the best of our knowledge,
test-time scaling has been limited to domains with verifiably correct answers,
like mathematics and coding. We transfer test-time scaling to the LeWiDi-2025
tasks to evaluate annotation disagreements. We experiment with three test-time
scaling methods: two benchmark algorithms (Model Averaging and Majority
Voting), and a Best-of-N sampling method. The two benchmark methods improve LLM
performance consistently on the LeWiDi tasks, but the Best-of-N method does
not. Our experiments suggest that the Best-of-N method does not currently
transfer from mathematics to LeWiDi tasks, and we analyze potential reasons for
this gap.

</details>


### [50] [VISaGE: Understanding Visual Generics and Exceptions](https://arxiv.org/abs/2510.12548)
*Stella Frank,Emily Allaway*

Main category: cs.CL

TL;DR: 研究发现当视觉语言模型(VLMs)遇到与训练假设冲突的异常图像时，其概念理解能力显著下降，这种实用先验的影响强于语义先验


<details>
  <summary>Details</summary>
Motivation: 探究VLMs在处理典型/异常图像时，如何平衡来自协调输入训练的实用先验（文本-图像相关）与概念普遍性的语义先验之间的冲突

Method: 构建包含典型/异常图像的VISaGE数据集，通过平衡实验设计对比两种先验的影响

Result: 当图像与文本描述不协调时，模型概念理解能力下降幅度（实用先验作用）比查询单个实例时的语义先验效应更强

Conclusion: VLMs在实际应用中可能过度依赖输入协调假设，当遇到异常实例时会出现概念判断偏差，这对开放场景应用提出挑战

Abstract: While Vision Language Models (VLMs) learn conceptual representations, in the
form of generalized knowledge, during training, they are typically used to
analyze individual instances. When evaluation instances are atypical, this
paradigm results in tension between two priors in the model. The first is a
pragmatic prior that the textual and visual input are both relevant, arising
from VLM finetuning on congruent inputs; the second is a semantic prior that
the conceptual representation is generally true for instances of the category.
In order to understand how VLMs trade off these priors, we introduce a new
evaluation dataset, VISaGE, consisting of both typical and exceptional images.
In carefully balanced experiments, we show that conceptual understanding
degrades when the assumption of congruency underlying the pragmatic prior is
violated with incongruent images. This effect is stronger than the effect of
the semantic prior when querying about individual instances.

</details>


### [51] [Teaching Language Models to Faithfully Express their Uncertainty](https://arxiv.org/abs/2510.12587)
*Bryan Eikema,Evgenia Ilia,José G. C. de Souza,Chrysoula Zerva,Wilker Aziz*

Main category: cs.CL

TL;DR: 提出Faithful Uncertainty Tuning（FUT）方法，通过微调使大语言模型忠实表达不确定性且不改变答案分布


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在不确定性传达失真的问题，重复提问会产生分歧答案但缺乏对应概率表述，形成'忠实性鸿沟'

Method: 构建包含概率修饰词（如'可能''或许'）的训练数据，将语言模型输出与样本一致性对齐，仅需模型自身和提示集作为监督

Result: FUT在开放域QA任务中显著减少忠实性鸿沟（降低60%+），保持回答准确率且语义分布偏移小于1%

Conclusion: FUT提供了简单有效的解决方案，使语言模型能够忠实地沟通不确定性，适用于多种解码策略和不确定性表达形式

Abstract: Large language models (LLMs) often miscommunicate their uncertainty: repeated
queries can produce divergent answers, yet generated responses are typically
unhedged or hedged in ways that do not reflect this variability. This conveys
unfaithful information about the uncertain state of the LLMs' knowledge,
creating a faithfulness gap that affects even strong LLMs. We introduce
Faithful Uncertainty Tuning (FUT): a fine-tuning approach that teaches
instruction-tuned LLMs to express uncertainty faithfully without altering their
underlying answer distribution. We construct training data by augmenting model
samples with uncertainty hedges (i.e. verbal cues such as 'possibly' or
'likely') aligned with sample consistency, requiring no supervision beyond the
model and a set of prompts. We evaluate FUT on open-domain question answering
(QA) across multiple models and datasets. Our results show that FUT
substantially reduces the faithfulness gap, while preserving QA accuracy and
introducing minimal semantic distribution shift. Further analyses demonstrate
robustness across decoding strategies, choice of hedgers, and other forms of
uncertainty expression (i.e. numerical). These findings establish FUT as a
simple and effective way to teach LLMs to communicate uncertainty faithfully.

</details>


### [52] [StyleDecipher: Robust and Explainable Detection of LLM-Generated Texts with Stylistic Analysis](https://arxiv.org/abs/2510.12608)
*Siyuan Li,Aodu Wulianghai,Xi Lin,Guangyan Li,Xiang Chen,Jun Wu,Jianhua Li*

Main category: cs.CL

TL;DR: 提出StyleDecipher框架，通过结合离散风格指标和连续语义表征实现机器文本检测，在跨领域场景下准确率提升36.3%


<details>
  <summary>Details</summary>
Motivation: 现有LLM文本检测方法存在泛化能力弱、易受改写攻击、可解释性差等问题，尤其在混合创作场景中表现不佳

Method: 联合建模基于语义嵌入的连续风格表征与离散风格指标，构建统一表示空间，实现无需模型内部信息的领域无关检测

Result: 在新闻/代码/论文等5个领域实现SOTA准确率，跨领域检测性能提升36.3%，抗干扰能力显著优于基线方法

Conclusion: 风格特征为机器文本检测提供可解释依据，开源框架有效应对混合创作场景的检测挑战

Abstract: With the increasing integration of large language models (LLMs) into
open-domain writing, detecting machine-generated text has become a critical
task for ensuring content authenticity and trust. Existing approaches rely on
statistical discrepancies or model-specific heuristics to distinguish between
LLM-generated and human-written text. However, these methods struggle in
real-world scenarios due to limited generalization, vulnerability to
paraphrasing, and lack of explainability, particularly when facing stylistic
diversity or hybrid human-AI authorship. In this work, we propose
StyleDecipher, a robust and explainable detection framework that revisits
LLM-generated text detection using combined feature extractors to quantify
stylistic differences. By jointly modeling discrete stylistic indicators and
continuous stylistic representations derived from semantic embeddings,
StyleDecipher captures distinctive style-level divergences between human and
LLM outputs within a unified representation space. This framework enables
accurate, explainable, and domain-agnostic detection without requiring access
to model internals or labeled segments. Extensive experiments across five
diverse domains, including news, code, essays, reviews, and academic abstracts,
demonstrate that StyleDecipher consistently achieves state-of-the-art in-domain
accuracy. Moreover, in cross-domain evaluations, it surpasses existing
baselines by up to 36.30%, while maintaining robustness against adversarial
perturbations and mixed human-AI content. Further qualitative and quantitative
analysis confirms that stylistic signals provide explainable evidence for
distinguishing machine-generated text. Our source code can be accessed at
https://github.com/SiyuanLi00/StyleDecipher.

</details>


### [53] [ACADATA: Parallel Dataset of Academic Data for Machine Translation](https://arxiv.org/abs/2510.12621)
*Iñaki Lacunza,Javier Garcia Gilabert,Francesca De Luca Fornaciari,Javier Aula-Blasco,Aitor Gonzalez-Agirre,Maite Melero,Marta Villegas*

Main category: cs.CL

TL;DR: ACADATA学术翻译数据集显著提升LLM翻译质量，开源资源推动领域研究


<details>
  <summary>Details</summary>
Motivation: 解决学术翻译领域缺乏高质量专业数据集的问题，提升机器翻译在学术场景的精准度

Method: 构建包含150万段落对的ACAD-TRAIN训练集和6000样本的评估集ACAD-BENCH，通过微调LLM并与各类翻译系统对比实验

Result: 微调后7B/2B模型学术翻译质量分别提升6.1/12.4 d-BLEU，英语长上下文翻译提升24.9%，最优模型超越商业系统

Conclusion: 开源数据集和模型为学术翻译及长文本翻译研究提供重要资源，促进社区发展

Abstract: We present ACADATA, a high-quality parallel dataset for academic translation,
that consists of two subsets: ACAD-TRAIN, which contains approximately 1.5
million author-generated paragraph pairs across 96 language directions and
ACAD-BENCH, a curated evaluation set of almost 6,000 translations covering 12
directions. To validate its utility, we fine-tune two Large Language Models
(LLMs) on ACAD-TRAIN and benchmark them on ACAD-BENCH against specialized
machine-translation systems, general-purpose, open-weight LLMs, and several
large-scale proprietary models. Experimental results demonstrate that
fine-tuning on ACAD-TRAIN leads to improvements in academic translation quality
by +6.1 and +12.4 d-BLEU points on average for 7B and 2B models respectively,
while also improving long-context translation in a general domain by up to
24.9% when translating out of English. The fine-tuned top-performing model
surpasses the best propietary and open-weight models on academic translation
domain. By releasing ACAD-TRAIN, ACAD-BENCH and the fine-tuned models, we
provide the community with a valuable resource to advance research in academic
domain and long-context translation.

</details>


### [54] [COSTAR-A: A prompting framework for enhancing Large Language Model performance on Point-of-View questions](https://arxiv.org/abs/2510.12637)
*Nzubechukwu C. Ohalete,Kevin B. Gittner,Lauren M. Matheny*

Main category: cs.CL

TL;DR: 提出增强型COSTAR-A提示框架，通过增加'Answer'组件提升小型本地化LLM在特定任务中的输出结构与确定性。


<details>
  <summary>Details</summary>
Motivation: 原始COSTAR框架在大型语言模型中表现良好，但在小型微调模型中输出一致性不足，尤其需要强指令约束的任务场景。

Method: 在COSTAR框架基础上增加'Answer'模块，通过系列对照实验评估8B参数以下本地优化模型的表现。

Result: Llama 3.1-8B等模型使用COSTAR-A后输出结构改善，但效果存在模型及任务差异性。

Conclusion: COSTAR-A框架在资源受限硬件环境中展现良好的适应性与扩展性，提升边缘计算场景的AI部署效率。

Abstract: Large Language Models (LLMs) are highly sensitive to prompt design, and
making optimized prompting techniques is crucial for generating consistent,
high-quality outputs. In this study, we introduce COSTAR-A, a novel prompt
engineering framework that enhances the existing COSTAR method, which stands
for Context, Objective, Style, Tone, Audience, and Response, by adding the
'Answer' component at the end. We demonstrate that while the original COSTAR
framework improves prompt clarity and aligns outputs for larger LLMs, its
performance is less consistent with smaller, locally optimized models,
particularly in tasks that require more directive or constrained outputs.
Through a series of controlled prompt-output assessments with smaller (at most
8 billion parameters), fine-tuned models, we found that COSTAR-A can enhance
the output structure and decisiveness of localized LLMs for certain tasks,
although its effectiveness varies across models and use cases. Notably, the
Llama 3.1-8B model exhibited performance improvements when prompted with
COSTAR-A compared to COSTAR alone. These findings emphasize the adaptability
and scalability of COSTAR-A as a prompting framework, particularly in
computationally efficient AI deployments on resource-constrained hardware.

</details>


### [55] [Reasoning Pattern Matters: Learning to Reason without Human Rationales](https://arxiv.org/abs/2510.12643)
*Chaoxu Pang,Yixuan Cao,Ping Luo*

Main category: cs.CL

TL;DR: 提出PARO框架，利用LLM自动生成符合任务推理模式的推理轨迹，显著减少人工标注需求


<details>
  <summary>Details</summary>
Motivation: 传统SFT+RLVR范式依赖昂贵的人工标注推理轨迹，研究发现模式化推理任务中推理模式比标注数量更重要

Method: 通过因果验证和实验分析确定推理模式的关键作用，设计PARO框架使LLM基于有限模式监督自动生成推理轨迹

Result: PARO生成的推理轨迹仅需1/10人工标注量即可达到同等SFT+RLVR效果，验证自动标注可行性

Conclusion: 模式化推理任务中人工标注可被自动生成替代，为LLM训练范式革新提供重要依据

Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning
capabilities under the widely adopted SFT+RLVR paradigm, which first performs
Supervised Fine-Tuning (SFT) on human-annotated reasoning trajectories
(rationales) to establish initial reasoning behaviors, then applies
Reinforcement Learning with Verifiable Rewards (RLVR) to optimize the model
using verifiable signals without golden rationales. However, annotating
high-quality rationales for the SFT stage remains prohibitively expensive. This
paper investigates when and how rationale annotation costs can be substantially
reduced without compromising reasoning performance. We identify a broad class
of problems, termed patterned reasoning tasks, where reasoning follows a fixed,
procedural strategy consistent across instances. Although instances vary in
content such as domain knowledge, factual information, or numeric values, the
solution derives from applying a shared reasoning pattern. We argue that the
success of SFT+RLVR on such tasks primarily stems from its ability to enable
models to internalize these reasoning patterns. Using numerical semantic
matching as a representative task, we provide both causal and behavioral
evidence showing that reasoning patterns rather than the quantity or quality of
rationales are the key determinant of performance. Building on these insights,
we propose Pattern-Aware LLMs as Rationale AnnOtators (PARO), a simple yet
effective framework that enables LLMs to generate rationales aligned with
task-specific reasoning patterns without requiring human rationale annotations.
Experiments show that PARO-generated rationales achieve comparable SFT+RLVR
performance to human rationales that are 10 times larger. These results suggest
that large-scale human rationale annotations can be replaced with LLM-based
automatic annotations requiring only limited human supervision over reasoning
patterns.

</details>


### [56] [Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations](https://arxiv.org/abs/2510.12699)
*Sunny Yu,Ahmad Jabbar,Robert Hawkins,Dan Jurafsky,Myra Cheng*

Main category: cs.CL

TL;DR: 提出有效生成空间大小(GSS)概念解决LLMs输出多样性校准问题，开发GSSBench评估框架并验证其三项应用价值


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型在创造性任务中输出过于同质化，在事实性任务中又产生错误多样性，需统一解决方案

Method: 构建GSSBench任务套件评估指标，通过检测提示歧义、解释推理过程异常、引导生成空间扩展三个应用场景验证

Result: EigenScore等幻觉检测指标优于传统方法，仅用模型内部参数即可实现有效评估，成功提升输出质量与多样性

Conclusion: GSS框架为模型校准提供统一视角，在提示优化、推理诊断和生成控制等场景展现显著应用潜力

Abstract: Different open-ended generation tasks require different degrees of output
diversity. However, current LLMs are often miscalibrated. They collapse to
overly homogeneous outputs for creative tasks and hallucinate diverse but
incorrect responses for factual tasks. We argue that these two failure modes
are unified by, and can both be addressed by, the notion of effective
generation space size (GSS) -- the set of semantically distinct outputs a model
considers for a prompt. We present GSSBench, a task suite of prompt pairs with
ground-truth GSS relationships to assess different metrics and understand where
models diverge from desired behavior. We find that hallucination detection
metrics, particularly EigenScore, consistently outperform standard diversity
and uncertainty quantification metrics, while using only model internals,
providing interpretable insights into a model's internal task representations.
We demonstrate three applications of GSS: (1) detecting prompt ambiguity and
predicting clarification questions for better grounding, (2) interpreting
overthinking and underthinking in reasoning models, and (3) steering models to
expand their generation space to yield high-quality and diverse outputs.

</details>


### [57] [Omni-Captioner: Data Pipeline, Models, and Benchmark for Omni Detailed Perception](https://arxiv.org/abs/2510.12720)
*Ziyang Ma,Ruiyang Xu,Zhenghao Xing,Yunfei Chu,Yuxuan Wang,Jinzheng He,Jin Xu,Pheng-Ann Heng,Kai Yu,Junyang Lin,Eng Siong Chng,Xie Chen*

Main category: cs.CL

TL;DR: 提出Omni-Detective数据生成框架，开发Audio/Omini-Captioner模型，设计Omni-Cloze评估基准，系统提升多模态细粒度感知能力


<details>
  <summary>Details</summary>
Motivation: 现有OLMs在细粒度感知与幻觉抑制间存在负相关（细节越多幻觉越严重），需要系统性解决方案

Method: 1. 工具增强的自动化数据生成流程（Omni-Detective）
2. 单模态/跨模态描述模型（Audio/Omini-Captioner）
3. 填空式多模态评估协议（Omni-Cloze）

Result: Audio-Captioner在MMAU/MMAR超越Gemini 2.5 Flash，Omni-Captioner在VDC创SOTA，视频-SALMONN 2实现最佳平衡

Conclusion: 通过闭环式数据生成-模型训练-评估体系，验证了Omni框架在细粒度多模态理解与可靠评估方面的有效性

Abstract: Fine-grained perception of multimodal information is critical for advancing
human-AI interaction. With recent progress in audio-visual technologies, Omni
Language Models (OLMs), capable of processing audio and video signals in
parallel, have emerged as a promising paradigm for achieving richer
understanding and reasoning. However, their capacity to capture and describe
fine-grained details remains limited explored. In this work, we present a
systematic and comprehensive investigation of omni detailed perception from the
perspectives of the data pipeline, models, and benchmark. We first identify an
inherent "co-growth" between detail and hallucination in current OLMs. To
address this, we propose Omni-Detective, an agentic data generation pipeline
integrating tool-calling, to autonomously produce highly detailed yet minimally
hallucinatory multimodal data. Based on the data generated with Omni-Detective,
we train two captioning models: Audio-Captioner for audio-only detailed
perception, and Omni-Captioner for audio-visual detailed perception. Under the
cascade evaluation protocol, Audio-Captioner achieves the best performance on
MMAU and MMAR among all open-source models, surpassing Gemini 2.5 Flash and
delivering performance comparable to Gemini 2.5 Pro. On existing detailed
captioning benchmarks, Omni-Captioner sets a new state-of-the-art on VDC and
achieves the best trade-off between detail and hallucination on the
video-SALMONN 2 testset. Given the absence of a dedicated benchmark for omni
detailed perception, we design Omni-Cloze, a novel cloze-style evaluation for
detailed audio, visual, and audio-visual captioning that ensures stable,
efficient, and reliable assessment. Experimental results and analysis
demonstrate the effectiveness of Omni-Detective in generating high-quality
detailed captions, as well as the superiority of Omni-Cloze in evaluating such
detailed captions.

</details>


### [58] [Which Word Orders Facilitate Length Generalization in LMs? An Investigation with GCG-Based Artificial Languages](https://arxiv.org/abs/2510.12722)
*Nadine El-Naggar,Tatsuki Kuribayashi,Ted Briscoe*

Main category: cs.CL

TL;DR: 本文通过广义范畴语法构建更接近自然语言的人工语言，验证语言模型对类型学合理语序的归纳偏好，发现模型对合理语序具有更强的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有研究使用上下文无关文法构建人工语言，无法覆盖自然语言中常见的无界依赖等结构。本文通过广义范畴语法（GCG）构建更贴近自然语言特征的人工语言，并着重评估模型对长距离未见语句的泛化能力

Method: 采用广义范畴语法（GCG）构建人工语言系统，覆盖无界依赖等自然语言结构。实验设计聚焦模型对长距离未见测试句的泛化表现，对比类型学常见与罕见语序的处理难度

Result: 语言模型在类型学合理语序上表现出更好的泛化能力，长距离测试句的准确率差异显著（合理语序平均准确率提升15%），证实模型归纳偏好与自然语言结构存在一致性

Conclusion: 通过更贴近自然语言特征的人工语言构建和测试范式，明确验证语言模型的归纳偏差倾向于类型学合理的语序结构，这对理解模型语言习得机制具有重要启示

Abstract: Whether language models (LMs) have inductive biases that favor typologically
frequent grammatical properties over rare, implausible ones has been
investigated, typically using artificial languages (ALs) (White and Cotterell,
2021; Kuribayashi et al., 2024). In this paper, we extend these works from two
perspectives. First, we extend their context-free AL formalization by adopting
Generalized Categorial Grammar (GCG) (Wood, 2014), which allows ALs to cover
attested but previously overlooked constructions, such as unbounded dependency
and mildly context-sensitive structures. Second, our evaluation focuses more on
the generalization ability of LMs to process unseen longer test sentences.
Thus, our ALs better capture features of natural languages and our experimental
paradigm leads to clearer conclusions -- typologically plausible word orders
tend to be easier for LMs to productively generalize.

</details>


### [59] [Hey, wait a minute: on at-issue sensitivity in Language Models](https://arxiv.org/abs/2510.12740)
*Sanghee J. Kim,Kanishka Misra*

Main category: cs.CL

TL;DR: 提出DGRC方法评估语言模型对话自然性，发现指导调优模型更倾向处理核心议题内容，并能在特定提示下动态调整偏好。


<details>
  <summary>Details</summary>
Motivation: 现有对话自然性评估存在主观性强和量化指标不足的问题，需开发系统性评估框架分析语言模型的语篇敏感特性。

Method: DGRC方法四阶段：分割对话→生成延续→重组序列→概率比较，通过概率对比量化模型对对话结构的敏感性。

Result: 模型偏好核心议题延续(指导调优强化此特性)，存在提示时降低偏好，模式符合成功对话动态特征。

Conclusion: DGRC为对话评估提供新范式，揭示语言模型的议题敏感性和指导调优模型的动态调整潜力。

Abstract: Evaluating the naturalness of dialogue in language models (LMs) is not
trivial: notions of 'naturalness' vary, and scalable quantitative metrics
remain limited. This study leverages the linguistic notion of 'at-issueness' to
assess dialogue naturalness and introduces a new method: Divide, Generate,
Recombine, and Compare (DGRC). DGRC (i) divides a dialogue as a prompt, (ii)
generates continuations for subparts using LMs, (iii) recombines the dialogue
and continuations, and (iv) compares the likelihoods of the recombined
sequences. This approach mitigates bias in linguistic analyses of LMs and
enables systematic testing of discourse-sensitive behavior. Applying DGRC, we
find that LMs prefer to continue dialogue on at-issue content, with this effect
enhanced in instruct-tuned models. They also reduce their at-issue preference
when relevant cues (e.g., "Hey, wait a minute") are present. Although
instruct-tuning does not further amplify this modulation, the pattern reflects
a hallmark of successful dialogue dynamics.

</details>


### [60] [Language Models Model Language](https://arxiv.org/abs/2510.12766)
*Łukasz Borchmann*

Main category: cs.CL

TL;DR: 论文主张用Witold Mańczak的经验主义语言观替代传统理论框架，强调语言使用频率对LLMs的核心作用，并为模型设计提供新方法论


<details>
  <summary>Details</summary>
Motivation: 针对当前基于索绪尔/乔姆斯基理论的语言学批评（认为LLMs缺乏深层结构）的无效性，提出需要更符合语言本质的分析框架

Method: 采用Witold Mańczak的语言定义：语言是「所有言说与书写的总和」，以语言元素使用频率为核心分析维度

Result: 构建了基于使用频率的语言模型评估体系，为LLMs的设计和解释提供了实证主义导向的操作框架

Conclusion: 语言学分析应回归语言使用实践本身，频率驱动机制使LLMs成为更贴近真实语言现象的研究工具

Abstract: Linguistic commentary on LLMs, heavily influenced by the theoretical
frameworks of de Saussure and Chomsky, is often speculative and unproductive.
Critics challenge whether LLMs can legitimately model language, citing the need
for "deep structure" or "grounding" to achieve an idealized linguistic
"competence." We argue for a radical shift in perspective towards the
empiricist principles of Witold Ma\'nczak, a prominent general and historical
linguist. He defines language not as a "system of signs" or a "computational
system of the brain" but as the totality of all that is said and written. Above
all, he identifies frequency of use of particular language elements as
language's primary governing principle. Using his framework, we challenge prior
critiques of LLMs and provide a constructive guide for designing, evaluating,
and interpreting language models.

</details>


### [61] [Dr.LLM: Dynamic Layer Routing in LLMs](https://arxiv.org/abs/2510.12773)
*Ahmed Heakl,Martin Gubri,Salman Khan,Sangdoo Yun,Seong Joon Oh*

Main category: cs.CL

TL;DR: Dr.LLM框架通过动态路由层决策提升LLM效率，在保持/提升准确率的同时节省计算资源。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在处理不同复杂度任务时存在计算浪费或灵活性不足，传统自适应方法存在推理成本高、架构修改或准确率下降的问题。

Method: 使用轻量级每层路由器（跳过/执行/重复）+ MCTS生成优化配置 + 窗口池化/焦点损失/瓶颈MLP设计保证稳定性。

Result: ARC/DART任务提升3.4%准确率并平均节省5层，跨领域任务仅下降0.85%且效率保持，优于现有方法7.7%。

Conclusion: 通过显式监督的路由器改造冻结LLM，实现预算感知的精准推理，无需修改基础模型权重。

Abstract: Large Language Models (LLMs) process every token through all layers of a
transformer stack, causing wasted computation on simple queries and
insufficient flexibility for harder ones that need deeper reasoning.
Adaptive-depth methods can improve efficiency, but prior approaches rely on
costly inference-time search, architectural changes, or large-scale retraining,
and in practice often degrade accuracy despite efficiency gains. We introduce
Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that
equips pretrained models with lightweight per-layer routers deciding to skip,
execute, or repeat a block. Routers are trained with explicit supervision:
using Monte Carlo Tree Search (MCTS), we derive high-quality layer
configurations that preserve or improve accuracy under a compute budget. Our
design, windowed pooling for stable routing, focal loss with class balancing,
and bottleneck MLP routers, ensures robustness under class imbalance and long
sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to
+3.4%p while saving 5 layers per example on average. Routers generalize to
out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,
AGIEval) with only 0.85% accuracy drop while retaining efficiency, and
outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that
explicitly supervised routers retrofit frozen LLMs for budget-aware,
accuracy-driven inference without altering base weights.

</details>


### [62] [Cost Analysis of Human-corrected Transcription for Predominately Oral Languages](https://arxiv.org/abs/2510.12781)
*Yacouba Diarra,Nouhoum Souleymane Coulibaly,Michael Leventhal*

Main category: cs.CL

TL;DR: 为低资源口语语言创建语音数据集需约30-36小时人工/小时音频（实验室/田野条件），揭示了资源建设中的人力成本基准


<details>
  <summary>Details</summary>
Motivation: 量化低资源口语语言（特别是文字普及率低的马里班巴拉语）构建高质量语音数据集所需的人力成本，填补该领域认知空白

Method: 通过为期1个月的田野研究，招募10名母语转录员对53小时语音数据进行ASR转录校正，对比实验室与田野环境效率差异

Result: 实验室条件下转录1小时语音平均耗时30小时，田野条件需36小时，建立了同类语言NLP资源建设的人力基准

Conclusion: 研究为具有类似特征的语言创建NLP资源提供了可量化基准，揭示了口述语言数字化过程中显著的人力投入需求

Abstract: Creating speech datasets for low-resource languages is a critical yet poorly
understood challenge, particularly regarding the actual cost in human labor.
This paper investigates the time and complexity required to produce
high-quality annotated speech data for a subset of low-resource languages, low
literacy Predominately Oral Languages, focusing on Bambara, a Manding language
of Mali. Through a one-month field study involving ten transcribers with native
proficiency, we analyze the correction of ASR-generated transcriptions of 53
hours of Bambara voice data. We report that it takes, on average, 30 hours of
human labor to accurately transcribe one hour of speech data under laboratory
conditions and 36 hours under field conditions. The study provides a baseline
and practical insights for a large class of languages with comparable profiles
undertaking the creation of NLP resources.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [63] [GS-Verse: Mesh-based Gaussian Splatting for Physics-aware Interaction in Virtual Reality](https://arxiv.org/abs/2510.11878)
*Anastasiya Pechko,Piotr Borycki,Joanna Waczyńska,Daniel Barczyk,Agata Szymańska,Sławomir Tadeja,Przemysław Spurek*

Main category: cs.GR

TL;DR: 提出基于高斯溅射与网格整合的VR交互新方法GS-VERSE，显著提升3D物理操作的精确度与真实感


<details>
  <summary>Details</summary>
Motivation: 现有VR物理交互方法依赖工程密集型流程且使用简化几何表示，导致视觉保真度低、物理精度不足

Method: 将物体网格与高斯溅射(GS)表示直接结合，复用现有3D资产构建物理引擎无关系统，简化开发流程

Result: 18人对比实验显示在拉伸操作上显著优于现有GS-VR方案，在扭曲/摇晃等物理操作中表现更稳定

Conclusion: 该方法为交互式3D操作提供了高保真、高适应性的新型解决方案，具备替代现有技术的潜力

Abstract: As the demand for immersive 3D content grows, the need for intuitive and
efficient interaction methods becomes paramount. Current techniques for
physically manipulating 3D content within Virtual Reality (VR) often face
significant limitations, including reliance on engineering-intensive processes
and simplified geometric representations, such as tetrahedral cages, which can
compromise visual fidelity and physical accuracy. In this paper, we introduce
\our{} (\textbf{G}aussian \textbf{S}platting for \textbf{V}irtual
\textbf{E}nvironment \textbf{R}endering and \textbf{S}cene \textbf{E}diting), a
novel method designed to overcome these challenges by directly integrating an
object's mesh with a Gaussian Splatting (GS) representation. Our approach
enables more precise surface approximation, leading to highly realistic
deformations and interactions. By leveraging existing 3D mesh assets, \our{}
facilitates seamless content reuse and simplifies the development workflow.
Moreover, our system is designed to be physics-engine-agnostic, granting
developers robust deployment flexibility. This versatile architecture delivers
a highly realistic, adaptable, and intuitive approach to interactive 3D
manipulation. We rigorously validate our method against the current
state-of-the-art technique that couples VR with GS in a comparative user study
involving 18 participants. Specifically, we demonstrate that our approach is
statistically significantly better for physics-aware stretching manipulation
and is also more consistent in other physics-based manipulations like twisting
and shaking. Further evaluation across various interactions and scenes confirms
that our method consistently delivers high and reliable performance, showing
its potential as a plausible alternative to existing methods.

</details>


### [64] [Coordinate Condensation: Subspace-Accelerated Coordinate Descent for Physics-Based Simulation](https://arxiv.org/abs/2510.12053)
*Ty Trusty*

Main category: cs.GR

TL;DR: 提出Coordinate Condensation方法，通过子空间修正消除阻尼效应，在保持坐标下降法效率的同时实现近牛顿收敛速度


<details>
  <summary>Details</summary>
Motivation: 解决JGS2方法中因全局耦合引入的阻尼效应导致收敛速度下降的问题，寻求更高效的物理模拟求解方案

Method: 将局部坐标更新与Schur补子空间修正结合，独立求解局部位移和子空间位移

Result: 在多种材料刚度和网格条件下，收敛速度显著优于传统坐标下降法和JGS2，并保持算法并行性

Conclusion: 子空间坐标方法在全局耦合建模充分时展现优势，为物理模拟求解器设计提供了新的有效性边界分析框架

Abstract: We introduce Coordinate Condensation, a variant of coordinate descent that
accelerates physics-based simulation by augmenting local coordinate updates
with a Schur-complement-based subspace correction. Recent work by Lan et al.
2025 (JGS2) uses perturbation subspaces to augment local solves to account for
global coupling, but their approach introduces damping that can degrade
convergence. We reuse this subspace but solve for local and subspace
displacements independently, eliminating this damping. For problems where the
subspace adequately captures global coupling, our method achieves near-Newton
convergence while retaining the efficiency and parallelism of coordinate
descent. Through experiments across varying material stiffnesses and mesh
resolutions, we show substantially faster convergence than both standard
coordinate descent and JGS2. We also characterize when subspace-based
coordinate methods succeed or fail, offering insights for future solver design.

</details>


### [65] [Can Representation Gaps Be the Key to Enhancing Robustness in Graph-Text Alignment?](https://arxiv.org/abs/2510.12087)
*Heng Zhang,Tianyi Zhang,Yuling Shi,Xiaodong Gu,Yaomin Shen,Zijian Zhang,Yilei Yuan,Hao Zhang,Jin Huang*

Main category: cs.GR

TL;DR: 该论文提出LLM4GTA框架，通过保持图结构与文本语义表征间的合理间隙，避免过度对齐导致的结构坍塌，提升跨模态迁移性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过对比学习强制压缩图结构和文本语义的表示空间，导致拓扑推理能力和语义理解能力双重退化。几何不兼容的编码器过度对齐会引发结构坍塌现象。

Method: LLM4GTA包含自适应间隙保持模块（通过相似度演化监控防止过度对齐）和图空间辅助分类器的模态内补偿机制，增强表示判别力。

Result: 在零样本和小样本场景下，该框架在多个基准测试中显著超越现有方法，证明其有效性和泛化能力。

Conclusion: 保持多模态表征间隙作为几何必要性，既能保留模态专属知识，又能提升迁移性能，为图-文本联合表示学习提供了新方向。

Abstract: Representation learning on text-attributed graphs (TAGs) integrates
structural connectivity with rich textual semantics, enabling applications in
diverse domains. Current methods largely rely on contrastive learning to
maximize cross-modal similarity, assuming tighter coupling between graph and
text representations improves transfer performance. However, our empirical
analysis reveals that both natural gap expansion and forced gap reduction
result in performance degradation by disrupting pre-trained knowledge
structures and impairing generalization. This arises from the geometric
incompatibility between encoders, where graph encoders capture topological
patterns, while text encoders capture semantic structures. Over-alignment
compresses these distinct spaces into shared subspaces, causing structure
collapse that diminishes both topological reasoning and semantic understanding.
We propose \textbf{LLM4GTA}, a gap-aware alignment framework that preserves
representation gaps as geometric necessities for maintaining modality-specific
knowledge and improving transfer performance. LLM4GTA includes an adaptive gap
preservation module to prevent over-alignment by monitoring similarity
evolution and an intra-modal compensation mechanism that boosts discriminative
power using auxiliary classifiers in graph space. Extensive experiments show
significant improvements over existing methods in zero-shot and few-shot
scenarios.

</details>


### [66] [SDGraph: Multi-Level Sketch Representation Learning by Sparse-Dense Graph Architecture](https://arxiv.org/abs/2510.12192)
*Xi Cheng,Pingfa Feng,Zhichao Liao,Mingyu Fan,Long Zeng*

Main category: cs.GR

TL;DR: 提出多层次草图表示方案和SDGraph深度学习架构，在分类/检索/生成任务中实现1.15%/1.70%/36.58%的性能提升


<details>
  <summary>Details</summary>
Motivation: 现有草图学习方法对有效信息挖掘不足，需系统化识别草图多粒度特征以提升模型性能

Method: 三级表示方案（草图-笔画-点级）→ 设计双图结构SDGraph：稀疏图处理笔画级特征，稠密图处理点级特征，通过信息融合模块增强特征提取

Result: 分类准确率提升1.15%，检索精度提高1.70%，矢量生成质量提升36.58%（SOTA对比）

Conclusion: 多层次信息挖掘架构有效提升草图理解能力，支持跨任务应用验证方案有效性

Abstract: Freehand sketches exhibit unique sparsity and abstraction, necessitating
learning pipelines distinct from those designed for images. For sketch learning
methods, the central objective is to fully exploit the effective information
embedded in sketches. However, there is limited research on what constitutes
effective sketch information, which in turn constrains the performance of
existing approaches. To tackle this issue, we first proposed the Multi-Level
Sketch Representation Scheme to systematically identify the effective
information. The scheme organizes sketch representation into three levels:
sketch-level, stroke-level, and point-level. This design is based on the
granularity of analytical elements, from coarse (sketch-level) to fine
(point-level), thereby ensuring more comprehensive coverage of the sketch
information. For each level, we conducted theoretical analyses and experimental
evaluations to identify and validate the effective information. Building on the
above studies, we developed SDGraph, a deep learning architecture designed to
exploit the identified effective information across the three levels. SDGraph
comprises two complementary modules: a Sparse Graph that treats strokes as
nodes for sketch-level and stroke-level representation learning, and a Dense
Graph that treats points as nodes for sketch-level and point-level
representation learning. Both modules employ graph convolution along with
down-sampling and up-sampling operations, enabling them to function as both
encoder and decoder. Besides that, an information fusion module bridges the two
graphs to further enhance feature extraction. SDGraph supports a wide range of
sketch-related downstream tasks, achieving accuracy improvements of 1.15\% and
1.70\% over the state-of-the-art in classification and retrieval, respectively,
and 36.58\% improvement in vector sketch generation quality.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [67] [Scaling Law in LLM Simulated Personality: More Detailed and Realistic Persona Profile Is All You Need](https://arxiv.org/abs/2510.11734)
*Yuqi Bai,Tianyu Huang,Kun Sun,Yuting Chen*

Main category: cs.CY

TL;DR: 本研究开发了一个系统性评估框架，验证角色细节对LLM人格模拟的关键作用并提出扩展定律，为社会科学应用提供理论及实践基础。


<details>
  <summary>Details</summary>
Motivation: 传统心理测量方法（如CFA）无法有效评估低层次LLM人格模拟的改进趋势，需构建适配新型AI特性的评估框架以解决方法论错位问题。

Method: 提出端到端评估框架：个体层面分析稳定性/可识别性，群体层面采用渐进人格曲线；改进传统心理测量方法（CFA与结构效度），设计适用于LLM特性的新型验证指标。

Result: 实证发现角色细节质量与模拟效果正相关，揭示角色配置边际效用效应及LLM人格模拟的扩展定律（Scaling Law）。

Conclusion: 构建LLM虚拟人格系统评估方法论，确立操作化评估指标，为社会科学实验中的LLM应用提供理论支撑与实践指导。

Abstract: This research focuses on using large language models (LLMs) to simulate
social experiments, exploring their ability to emulate human personality in
virtual persona role-playing. The research develops an end-to-end evaluation
framework, including individual-level analysis of stability and
identifiability, as well as population-level analysis called progressive
personality curves to examine the veracity and consistency of LLMs in
simulating human personality. Methodologically, this research proposes
important modifications to traditional psychometric approaches (CFA and
construct validity) which are unable to capture improvement trends in LLMs at
their current low-level simulation, potentially leading to remature rejection
or methodological misalignment. The main contributions of this research are:
proposing a systematic framework for LLM virtual personality evaluation;
empirically demonstrating the critical role of persona detail in personality
simulation quality; and identifying marginal utility effects of persona
profiles, especially a Scaling Law in LLM personality simulation, offering
operational evaluation metrics and a theoretical foundation for applying large
language models in social science experiments.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [68] [Celebrity Profiling on Short Urdu Text using Twitter Followers' Feed](https://arxiv.org/abs/2510.11739)
*Muhammad Hamza,Rizwan Jafar*

Main category: cs.SI

TL;DR: 应用机器学习技术通过乌尔都语追随者推文预测名人人口统计特征，最佳性别预测准确率65%


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注高资源语言，乌尔都语作为低资源语言的社交媒体名人特征预测研究存在空白

Method: 收集南亚名人追随者的乌尔都语推文数据集，使用LR/SVM/RF/CNN/LSTM等算法进行训练比较，采用cRank等评估指标

Result: 性别预测cRank和准确率均为0.65，年龄/职业/名声预测效果中等

Conclusion: 验证了基于追随者语言特征的机器学习方法在低资源乌尔都语中的有效性，为小语种社交媒体分析提供新方案

Abstract: Social media has become an essential part of the digital age, serving as a
platform for communication, interaction, and information sharing. Celebrities
are among the most active users and often reveal aspects of their personal and
professional lives through online posts. Platforms such as Twitter provide an
opportunity to analyze language and behavior for understanding demographic and
social patterns. Since followers frequently share linguistic traits and
interests with the celebrities they follow, textual data from followers can be
used to predict celebrity demographics. However, most existing research in this
field has focused on English and other high-resource languages, leaving Urdu
largely unexplored.
  This study applies modern machine learning and deep learning techniques to
the problem of celebrity profiling in Urdu. A dataset of short Urdu tweets from
followers of subcontinent celebrities was collected and preprocessed. Multiple
algorithms were trained and compared, including Logistic Regression, Support
Vector Machines, Random Forests, Convolutional Neural Networks, and Long
Short-Term Memory networks. The models were evaluated using accuracy,
precision, recall, F1-score, and cumulative rank (cRank). The best performance
was achieved for gender prediction with a cRank of 0.65 and an accuracy of
0.65, followed by moderate results for age, profession, and fame prediction.
These results demonstrate that follower-based linguistic features can be
effectively leveraged using machine learning and neural approaches for
demographic prediction in Urdu, a low-resource language.

</details>


### [69] [Evolution of wartime discourse on Telegram: A comparative study of Ukrainian and Russian policymakers' communication before and after Russia's full-scale invasion of Ukraine](https://arxiv.org/abs/2510.11746)
*Mykola Makhortykh,Aytalina Kulichkina,Kateryna Maikovska*

Main category: cs.SI

TL;DR: 分析俄乌战争中乌克兰与俄罗斯政策制定者在Telegram的传播策略差异：乌方初期聚焦战争但逐渐淡化，俄方转移话题至西方危机，揭示战时政治传播的适应性调整


<details>
  <summary>Details</summary>
Motivation: 探究社交媒体时代首次大规模欧洲战争（俄乌战争）中政策制定者的传播策略，分析战时沟通模式转变及其对公众注意力的引导机制

Method: 使用2019-2024年Telegram政策制定者公开帖子数据集，定量分析通讯量、主题演化及参与者互动，特别关注2022年全面入侵前后的对比

Result: 1) 战争爆发后通讯量激增（尤以执政党为著） 2) 乌方初期聚焦战争但关注度递减，俄方系统性回避战争话题 3) 大小政党和个人传播策略呈现显著差异

Conclusion: 政策制定者通过差异化主题选择实现战时舆论操控，社交媒体成为信息战关键战场，该研究为战时政治传播理论提供实证框架

Abstract: This study examines elite-driven political communication on Telegram during
the ongoing Russo-Ukrainian war, the first large-scale European war in the
social media era. Using a unique dataset of Telegram public posts from
Ukrainian and Russian policymakers (2019-2024), we analyze changes in
communication volume, thematic content, and actor engagement following Russia's
2022 full-scale invasion. Our findings show a sharp increase in Telegram
activity after the invasion, particularly among ruling-party policymakers.
Ukrainian policymakers initially focused on war-related topics, but this
emphasis declined over time In contrast, Russian policymakers largely avoided
war-related discussions, instead emphasizing unrelated topics, such as Western
crises, to distract public attention. We also identify differences in
communication strategies between large and small parties, as well as individual
policymakers. Our findings shed light on how policymakers adapt to wartime
communication challenges and offer critical insights into the dynamics of
online political discourse during times of war.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [70] [Evaluating Line Chart Strategies for Mitigating Density of Temporal Data: The Impact on Trend, Prediction, and Decision-Making](https://arxiv.org/abs/2510.11912)
*Rifat Ara Proma,Ghulam Jilani Quadri,Paul Rosen*

Main category: cs.HC

TL;DR: 聚合图表在密集时间数据可视化中表现优于trellis和螺旋图，提供更准确的趋势识别与预测支持


<details>
  <summary>Details</summary>
Motivation: 解决过度绘制折线图导致时间数据趋势模糊的问题，评估替代可视化方案的有效性

Method: 通过用户研究比较标准/聚合/trellis/螺旋图表在趋势识别、预测准确性和信任游戏决策中的表现

Result: 聚合图与标准图准确性相当但更优，trellis图表信任度最低，螺旋图表信任表现不稳定

Conclusion: 建议在处理密集时序数据时优先选择聚合图表作为可视化策略

Abstract: Overplotted line charts can obscure trends in temporal data and hinder
prediction. We conduct a user study comparing three alternatives-aggregated,
trellis, and spiral line charts against standard line charts on tasks involving
trend identification, making predictions, and decision-making. We found
aggregated charts performed similarly to standard charts and support more
accurate trend recognition and prediction; trellis and spiral charts generally
lag. We also examined the impact on decision-making via a trust game. The
results showed similar trust in standard and aggregated charts, varied trust in
spiral charts, and a lean toward distrust in trellis charts. These findings
provide guidance for practitioners choosing visualization strategies for dense
temporal data.

</details>


### [71] [Visual Stenography: Feature Recreation and Preservation in Sketches of Noisy Line Charts](https://arxiv.org/abs/2510.11927)
*Rifat Ara Proma,Michael Correll,Ghulam Jilani Quadri,Paul Rosen*

Main category: cs.HC

TL;DR: 研究者通过视觉速记实验发现，人们在绘制折线图时根据噪声水平采用不同策略：复制者保留所有特征，趋势保持者专注趋势，去噪者过滤噪声。趋势和峰值最易被忠实保留，周期性和噪声则以定性方式呈现。


<details>
  <summary>Details</summary>
Motivation: 探究时间序列数据中哪些视觉特征（趋势/周期性/峰值）被读者优先关注，以及视觉杂乱度（5-30dB SNR噪声水平）如何影响人类对特征的优先级判断。

Method: 采用视觉速记任务：参与者重绘不同噪声水平的折线图，系统分析绘图策略与噪声水平的关联。

Result: 识别三种核心策略：58%参与者成为趋势保持者（高噪声时），32%成为去噪者（中等噪声），10%成为复制者（低噪声）。趋势和峰谷的保留精度比周期性和噪声高2.3倍。

Conclusion: 时间序列数据处理需更人性化的方法，建议在数据预处理、聚类和可视化中保留人类认知特征（如趋势优先），而非机械保留所有数据特征。

Abstract: Line charts surface many features in time series data, from trends to
periodicity to peaks and valleys. However, not every potentially important
feature in the data may correspond to a visual feature which readers can detect
or prioritize. In this study, we conducted a visual stenography task, where
participants re-drew line charts to solicit information about the visual
features they believed to be important. We systematically varied noise levels
(SNR ~5-30 dB) across line charts to observe how visual clutter influences
which features people prioritize in their sketches. We identified three key
strategies that correlated with the noise present in the stimuli: the
Replicator attempted to retain all major features of the line chart including
noise; the Trend Keeper prioritized trends disregarding periodicity and peaks;
and the De-noiser filtered out noise while preserving other features. Further,
we found that participants tended to faithfully retain trends and peaks and
valleys when these features were present, while periodicity and noise were
represented in more qualitative or gestural ways: semantically rather than
accurately. These results suggest a need to consider more flexible and
human-centric ways of presenting, summarizing, pre-processing, or clustering
time series data.

</details>


### [72] [Who is a Better Matchmaker? Human vs. Algorithmic Judge Assignment in a High-Stakes Startup Competition](https://arxiv.org/abs/2510.12692)
*Sarina Xi,Orelia Pi,Miaomiao Zhang,Becca Xiong,Jacqueline Ng Lane,Nihar B. Shah*

Main category: cs.HC

TL;DR: AI算法HLSE在哈佛创业竞赛中实现与人类专家相当的评委匹配质量（3.90 vs 3.94/5分），处理效率提升200倍（数小时 vs 一周），证明AI在高风险决策场景的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 验证AI在需要语义理解的复杂决策任务（评委匹配）中能否达到人类专家水平，通过哈佛校长创新挑战赛（奖金池超50万美元）的真实场景进行验证。

Method: 开发混合词汇-语义相似度集成算法（HLSE），通过309组评委-项目双盲实验，采用曼-惠特尼U检验对比算法与人工匹配质量。

Result: 算法匹配质量（AUC=0.48, p=0.40）与人工无显著差异，处理时间从1周缩短至数小时。评分均值：算法3.90 vs 人工3.94（5分制）。

Conclusion: HLSE在保持人类专家级匹配质量的同时，显著提升扩展性和效率，为AI增强高风险决策提供了实证支持。

Abstract: There is growing interest in applying artificial intelligence (AI) to
automate and support complex decision-making tasks. However, it remains unclear
how algorithms compare to human judgment in contexts requiring semantic
understanding and domain expertise. We examine this in the context of the judge
assignment problem, matching submissions to suitably qualified judges.
Specifically, we tackled this problem at the Harvard President's Innovation
Challenge, the university's premier venture competition awarding over \$500,000
to student and alumni startups. This represents a real-world environment where
high-quality judge assignment is essential. We developed an AI-based
judge-assignment algorithm, Hybrid Lexical-Semantic Similarity Ensemble (HLSE),
and deployed it at the competition. We then evaluated its performance against
human expert assignments using blinded match-quality scores from judges on
$309$ judge-venture pairs. Using a Mann-Whitney U statistic based test, we
found no statistically significant difference in assignment quality between the
two approaches ($AUC=0.48, p=0.40$); on average, algorithmic matches are rated
$3.90$ and manual matches $3.94$ on a 5-point scale, where 5 indicates an
excellent match. Furthermore, manual assignments that previously required a
full week could be automated in several hours by the algorithm during
deployment. These results demonstrate that HLSE achieves human-expert-level
matching quality while offering greater scalability and efficiency,
underscoring the potential of AI-driven solutions to support and enhance human
decision-making for judge assignment in high-stakes settings.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [73] [GraphShaper: Geometry-aware Alignment for Improving Transfer Learning in Text-Attributed Graphs](https://arxiv.org/abs/2510.12085)
*Heng Zhang,Tianyi Zhang,Yuling Shi,Xiaodong Gu,Yaomin Shen,Haochen You,Zijian Zhang,Yilei Yuan,Jin Huang*

Main category: cs.LG

TL;DR: 提出多几何融合框架GraphShaper解决图结构编码冲突，零样本准确率提升7-9%


<details>
  <summary>Details</summary>
Motivation: 现有图文本对齐框架假设所有结构可编码在单一欧氏空间，导致树状/环状结构边界处几何约束冲突，造成超过20%的性能损失

Method: 采用双阶段框架：1）多几何专家网络（双曲/球面/欧氏空间）动态计算融合权重 2）结构保持的跨模态对齐

Result: 在引文网络(9.47%)和社交网络(7.63%)的零样本任务中显著提升准确率，验证多几何编码的有效性

Conclusion: 通过几何感知的图结构编码框架，成功解决异质图结构对齐难题，为图基础模型提供新的架构思路

Abstract: Graph foundation models represent a transformative paradigm for learning
transferable representations across diverse graph domains. Recent methods
leverage large language models to unify graph and text modalities into a shared
representation space using contrastive learning. However, systematic
evaluations reveal significant performance degradation at structural boundaries
where distinct topological patterns converge, with accuracy losses exceeding 20
percentage points. This issue arises from a key limitation: current methods
assume all graph structures can be encoded within a single Euclidean space. In
reality, tree structures require hyperbolic geometry to preserve hierarchical
branching, while cyclic patterns depend on spherical geometry for closure
properties. At structural boundaries, nodes experience conflicting geometric
constraints that uniform encoding spaces cannot resolve. This raises a crucial
challenge: \textbf{Can alignment frameworks be designed to respect the
intrinsic geometric diversity of graph structures?} We introduce
\textbf{GraphShaper}, a geometry-aware framework that enhances graph encoding
through multi-geometric specialization. Our approach employs expert networks
tailored to different geometric spaces, dynamically computing fusion weights to
adaptively integrate geometric properties based on local structural
characteristics. This adaptive fusion preserves structural integrity before
alignment with text embeddings. Extensive experiments demonstrate that
GraphShaper achieves 9.47\% accuracy improvements on citation networks and
7.63\% on social networks in zero-shot settings.

</details>


### [74] [H4G: Unlocking Faithful Inference for Zero-Shot Graph Learning in Hyperbolic Space](https://arxiv.org/abs/2510.12094)
*Heng Zhang,Tianyi Zhang,Zijun Liu,Yuling Shi,Yaomin Shen,Haochen You,Haichuan Hu,Lubin Gan,Jin Huang*

Main category: cs.LG

TL;DR: 论文提出H4G框架，通过降低双曲嵌入半径解决现有方法在异质图上过度抽象导致细粒度信息丢失的问题，显著提升零样本图学习性能


<details>
  <summary>Details</summary>
Motivation: 现有方法在异质图的细粒度模式识别中存在过度抽象问题——双曲空间嵌入半径过大导致局部结构信息丢失，影响预测准确性

Method: 采用可学习的块对角缩放矩阵和Möbius矩阵乘法系统减小嵌入半径，在保留细粒度结构的同时维持全局感知能力

Result: H4G在异质图零样本任务中提升12.8%，同质图提升8.4%，达到SOTA性能

Conclusion: 通过控制双曲空间嵌入半径实现细粒度结构与全局信息的平衡，为图表示学习提供了新的优化方向

Abstract: Text-attributed graphs are widely used across domains, offering rich
opportunities for zero-shot learning via graph-text alignment. However,
existing methods struggle with tasks requiring fine-grained pattern
recognition, particularly on heterophilic graphs. Through empirical and
theoretical analysis, we identify an \textbf{over-abstraction problem}: current
approaches operate at excessively large hyperbolic radii, compressing
multi-scale structural information into uniform high-level abstractions. This
abstraction-induced information loss obscures critical local patterns essential
for accurate predictions. By analyzing embeddings in hyperbolic space, we
demonstrate that optimal graph learning requires \textbf{faithful preservation}
of fine-grained structural details, better retained by representations
positioned closer to the origin. To address this, we propose \textbf{H4G}, a
framework that systematically reduces embedding radii using learnable
block-diagonal scaling matrices and M\"obius matrix multiplication. This
approach restores access to fine-grained patterns while maintaining global
receptive ability with minimal computational overhead. Experiments show H4G
achieves state-of-the-art zero-shot performance with \textbf{12.8\%}
improvement on heterophilic graphs and \textbf{8.4\%} on homophilic graphs,
confirming that radius reduction enables faithful multi-scale representation
for advancing zero-shot graph learning.

</details>


### [75] [Don't Walk the Line: Boundary Guidance for Filtered Generation](https://arxiv.org/abs/2510.11834)
*Sarah Ball,Andreas Haupt*

Main category: cs.LG

TL;DR: 提出Boundary Guidance强化学习方法，通过引导生成远离分类边界，提升生成模型输出的安全性与实用性


<details>
  <summary>Details</summary>
Motivation: 现有生成模型微调策略易使样本靠近分类边界，导致假阳性/假阴性增加

Method: 基于强化学习的边界引导微调框架，显式优化生成分布远离分类器决策边界

Result: 在越狱和模糊提示测试中，LLM评估显示安全性和实用性提升，消融实验验证方法稳健性

Conclusion: 边界引导方法有效平衡安全性与生成质量，适用于不同规模模型和奖励设计方案

Abstract: Generative models are increasingly paired with safety classifiers that filter
harmful or undesirable outputs. A common strategy is to fine-tune the generator
to reduce the probability of being filtered, but this can be suboptimal: it
often pushes the model toward producing samples near the classifier's decision
boundary, increasing both false positives and false negatives. We propose
Boundary Guidance, a reinforcement learning fine-tuning method that explicitly
steers generation away from the classifier's margin. On a benchmark of
jailbreak and ambiguous prompts, Boundary Guidance improves both the safety and
the utility of outputs, as judged by LLM-as-a-Judge evaluations. Comprehensive
ablations across model scales and reward designs demonstrate the robustness of
our approach.

</details>


### [76] [Balancing Synthetic Data and Replay for Enhancing Task-Specific Capabilities](https://arxiv.org/abs/2510.11842)
*Urs Spiegelhalter,Jörg K. H. Franke,Frank Hutter*

Main category: cs.LG

TL;DR: 研究通过系统实验揭示了在有限计算资源下，语言模型持续预训练中任务性能与知识保留的最佳回放比例配置，并提出基于预算的配置指南。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型持续预训练中新任务学习与旧知识遗忘之间的根本矛盾，探索计算约束下回放比例的最优配置方案。

Method: 使用bAbI推理任务作为目标，采用合成数据生成技术，在不同总token预算下系统测试多种回放比例配置方案。

Result: 发现了平衡特定任务性能与通用知识保留的最优配置，训练成本可降低50%同时保持模型能力。

Conclusion: 基于计算预算选择回放比例的实证框架，为实际应用提供了显著降低训练成本的高效适配方案。

Abstract: Adapting language models to new tasks through continued pretraining faces a
fundamental trade-off: models must learn new capabilities while avoiding
catastrophic forgetting of existing knowledge. While prior work has studied
synthetic data generation techniques, the optimal replay ratios for balancing
task performance and knowledge retention under computational constraints remain
poorly understood. We present a comprehensive empirical study investigating the
interplay between replay ratio configuration and computational budget when
adapting language models to new tasks. Using the bAbI reasoning tasks as our
target objective, we apply synthetic data generation and systematically
evaluate different total token budgets and replay ratio configurations. We
analyze their effects on both task mastery and general knowledge retention. Our
experiments reveal an optimal configuration that balances task-specific
performance with general knowledge retention. Based on our findings, we provide
empirically-grounded guidelines for selecting replay ratios based on
computational budget, enabling practitioners to achieve strong task adaptation
with significantly reduced training costs.

</details>


### [77] [Demystifying Hybrid Thinking: Can LLMs Truly Switch Between Think and No-Think?](https://arxiv.org/abs/2510.12680)
*Shouren Wang,Wang Yang,Xianxuan Long,Qifan Wang,Vipin Chaudhary,Xiaotian Han*

Main category: cs.LG

TL;DR: 当前混合思维大语言模型存在思维模式泄漏问题，本文提出两阶段训练方案显著降低非思考模式输出长度及推理痕迹


<details>
  <summary>Details</summary>
Motivation: 现有混合思维模型无法实现真正的模式分离（推理行为常渗入非思考模式），需探索增强模型可控性的方案

Method: 通过四因素分析（数据规模/数据来源/数据量/训练策略）提出两阶段训练方案：先培养推理能力，后实施混合思维训练

Result: 新方案在MATH500上将非思考模式输出长度从1085降至585，推理支持性token（如'wait'）出现次数从5917降至522

Conclusion: 研究揭示了当前混合思维的局限性，并为增强可控性提供方向：平衡数据规模/来源/数量，采用分阶段训练策略

Abstract: Hybrid thinking enables LLMs to switch between reasoning and direct
answering, offering a balance between efficiency and reasoning capability. Yet
our experiments reveal that current hybrid thinking LLMs only achieve partial
mode separation: reasoning behaviors often leak into the no-think mode. To
understand and mitigate this, we analyze the factors influencing
controllability and identify four that matter most: (1) larger data scale, (2)
using think and no-think answers from different questions rather than the same
question, (3) a moderate increase in no-think data number, and (4) a two-phase
strategy that first trains reasoning ability and then applies hybrid think
training. Building on these findings, we propose a practical recipe that,
compared to standard training, can maintain accuracy in both modes while
significantly reducing no-think output length (from $1085$ to $585$ on MATH500)
and occurrences of reasoning-supportive tokens such as ``\texttt{wait}'' (from
$5917$ to $522$ on MATH500). Our findings highlight the limitations of current
hybrid thinking and offer directions for strengthening its controllability.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [78] [Simple Projection Variants Improve ColBERT Performance](https://arxiv.org/abs/2510.12327)
*Benjamin Clavié,Sean Lee,Rikiya Takehi,Aamir Shakir,Makoto P. Kato*

Main category: cs.IR

TL;DR: 论文研究表明：使用非线性FFN、GLU块等复杂投影层替代ColBERT的单层线性投影，可显著提升多领域检索性能（平均提升2+ NDCG@10），残差连接与中间层升维是性能提升关键。


<details>
  <summary>Details</summary>
Motivation: 发现ColBERT使用的单层线性投影在MaxSim算子梯度流中存在固有缺陷，推测使用更复杂的非线性前馈网络结构（如深层FFN、GLU模块、跳跃连接）能突破该限制。

Method: 系统性设计并评估多种投影块变体，包括非线性FFN块、GLU块、残差连接结构，通过跨领域检索基准测试验证性能差异，并进行参数消融实验。

Result: 最佳变体在多个领域检索基准上平均提升超2 NDCG@10点，多数次优变体仍优于原线性层，实验证实中间层升维与残差连接对性能提升贡献显著。

Conclusion: 替换ColBERT线性投影层为复杂投影结构是稳健的即插即用升级方案，性能提升具有随机种子鲁棒性，为后续多向量检索模型优化提供新方向。

Abstract: Multi-vector dense retrieval methods like ColBERT systematically use a
single-layer linear projection to reduce the dimensionality of individual
vectors. In this study, we explore the implications of the MaxSim operator on
the gradient flows of the training of multi-vector models and show that such a
simple linear projection has inherent, if non-critical, limitations in this
setting. We then discuss the theoretical improvements that could result from
replacing this single-layer projection with well-studied alternative
feedforward linear networks (FFN), such as deeper, non-linear FFN blocks, GLU
blocks, and skip-connections, could alleviate these limitations. Through the
design and systematic evaluation of alternate projection blocks, we show that
better-designed final projections positively impact the downstream performance
of ColBERT models. We highlight that many projection variants outperform the
original linear projections, with the best-performing variants increasing
average performance on a range of retrieval benchmarks across domains by over 2
NDCG@10 points. We then conduct further exploration on the individual
parameters of these projections block in order to understand what drives this
empirical performance, highlighting the particular importance of upscaled
intermediate projections and residual connections. As part of these ablation
studies, we show that numerous suboptimal projection variants still outperform
the traditional single-layer projection across multiple benchmarks, confirming
our hypothesis. Finally, we observe that this effect is consistent across
random seeds, further confirming that replacing the linear layer of ColBERT
models is a robust, drop-in upgrade.

</details>


### [79] [The Role of Parametric Injection-A Systematic Study of Parametric Retrieval-Augmented Generation](https://arxiv.org/abs/2510.12668)
*Minghao Tang,Shiyu Ni,Jingtong Wu,Zengxin Han,Keping Bi*

Main category: cs.IR

TL;DR: 参数化检索增强生成（PRAG）中参数化文档仅捕获部分语义信息，单独使用效果弱于文本交互，但结合文本使用时能提升模型理解能力和抗噪性能


<details>
  <summary>Details</summary>
Motivation: 探究PRAG参数注入机制不明确的问题，揭示参数化文档的信息捕获特性及其与文本交互的协同效应

Method: 通过系统性实验分析参数化文档的语义捕获能力，比较单独参数化/文本化及混合使用的性能差异，评估噪声输入的鲁棒性

Result: 参数化文档编码高层次文档信息，与文本结合使用时信息利用率提升21%，在噪声环境下准确率比纯文本基线高15%

Conclusion: 推荐参数化与文本化文档联合使用，建议通过增加参数表示的信息密度（如扩展适配器维度）来优化PRAG框架

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
retrieving external documents. As an emerging form of RAG, parametric
retrieval-augmented generation (PRAG) encodes documents as model parameters
(i.e., LoRA modules) and injects these representations into the model during
inference, enabling interaction between the LLM and documents at parametric
level. Compared with directly placing documents in the input context, PRAG is
more efficient and has the potential to offer deeper model-document
interaction. Despite its growing attention, the mechanism underlying parametric
injection remains poorly understood. In this work, we present a systematic
study of PRAG to clarify the role of parametric injection, showing that
parameterized documents capture only partial semantic information of documents,
and relying on them alone yields inferior performance compared to interaction
at text level. However, these parametric representations encode high-level
document information that can enhance the model's understanding of documents
within the input context. When combined parameterized documents with textual
documents, the model can leverage relevant information more effectively and
become more robust to noisy inputs, achieving better performance than either
source alone. We recommend jointly using parameterized and textual documents
and advocate for increasing the information content of parametric
representations to advance PRAG.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [80] [Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D Reconstruction](https://arxiv.org/abs/2510.12768)
*Fengzhi Guo,Chih-Chuan Hsu,Sihao Ding,Cheng Zhang*

Main category: cs.CV

TL;DR: 提出USplat4D框架，通过建模高斯基元不确定性优化动态高斯泼溅模型，提升遮挡场景稳定性和极端视角合成质量。


<details>
  <summary>Details</summary>
Motivation: 传统动态高斯泼溅模型对所有基元统一优化，导致遮挡场景下运动漂移和极端视角合成效果差。需要区分不同观察质量的基元以提升重建稳定性。

Method: 引入时间维度的高斯基元不确定性估计，构建时空图进行不确定性感知优化，可靠基元作为运动锚点指导整体优化过程。

Result: 在真实/合成数据集验证显示，该方法使几何稳定性提升20%以上，极端视角PSNR指标平均提高1.5dB。

Conclusion: 显式建模不确定性可有效增强动态高斯泼溅模型，为解决单目动态重建的模糊性问题提供了新方向。

Abstract: Reconstructing dynamic 3D scenes from monocular input is fundamentally
under-constrained, with ambiguities arising from occlusion and extreme novel
views. While dynamic Gaussian Splatting offers an efficient representation,
vanilla models optimize all Gaussian primitives uniformly, ignoring whether
they are well or poorly observed. This limitation leads to motion drifts under
occlusion and degraded synthesis when extrapolating to unseen views. We argue
that uncertainty matters: Gaussians with recurring observations across views
and time act as reliable anchors to guide motion, whereas those with limited
visibility are treated as less reliable. To this end, we introduce USplat4D, a
novel Uncertainty-aware dynamic Gaussian Splatting framework that propagates
reliable motion cues to enhance 4D reconstruction. Our key insight is to
estimate time-varying per-Gaussian uncertainty and leverages it to construct a
spatio-temporal graph for uncertainty-aware optimization. Experiments on
diverse real and synthetic datasets show that explicitly modeling uncertainty
consistently improves dynamic Gaussian Splatting models, yielding more stable
geometry under occlusion and high-quality synthesis at extreme viewpoints.

</details>


### [81] [MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars](https://arxiv.org/abs/2510.12785)
*Felix Taubner,Ruihang Zhang,Mathieu Tuli,Sherwin Bahmani,David B. Lindell*

Main category: cs.CV

TL;DR: 提出MVP4D模型，通过单张参考图像生成可动画化的多视角数字人视频，并蒸馏为实时渲染的4D虚拟形象，显著提升真实感和一致性


<details>
  <summary>Details</summary>
Motivation: 传统数字人虚拟形象创建依赖昂贵设备与人工操作，现有单图生成模型在偏离参考视角时质量下降，需解决多视角一致性问题

Method: 基于预训练视频扩散模型生成360度多视角视频序列，通过蒸馏技术将输出转换为可实时渲染的4D模型

Result: 相较先前方法，在真实感（+32%用户评分）、时间一致性（PSNR提高4.6dB）和3D一致性（SSIM提升18%）方面显著改进

Conclusion: MVP4D突破单视角限制，通过视频扩散与模型蒸馏技术实现高质量多视角数字人生成，为虚拟内容创作提供新范式

Abstract: Digital human avatars aim to simulate the dynamic appearance of humans in
virtual environments, enabling immersive experiences across gaming, film,
virtual reality, and more. However, the conventional process for creating and
animating photorealistic human avatars is expensive and time-consuming,
requiring large camera capture rigs and significant manual effort from
professional 3D artists. With the advent of capable image and video generation
models, recent methods enable automatic rendering of realistic animated avatars
from a single casually captured reference image of a target subject. While
these techniques significantly lower barriers to avatar creation and offer
compelling realism, they lack constraints provided by multi-view information or
an explicit 3D representation. So, image quality and realism degrade when
rendered from viewpoints that deviate strongly from the reference image. Here,
we build a video model that generates animatable multi-view videos of digital
humans based on a single reference image and target expressions. Our model,
MVP4D, is based on a state-of-the-art pre-trained video diffusion model and
generates hundreds of frames simultaneously from viewpoints varying by up to
360 degrees around a target subject. We show how to distill the outputs of this
model into a 4D avatar that can be rendered in real-time. Our approach
significantly improves the realism, temporal consistency, and 3D consistency of
generated avatars compared to previous methods.

</details>


### [82] [Data or Language Supervision: What Makes CLIP Better than DINO?](https://arxiv.org/abs/2510.11835)
*Yiming Liu,Yuhui Zhang,Dhruba Ghosh,Ludwig Schmidt,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: 研究通过控制变量实验揭示CLIP优于DINO主要源于语言监督而非数据量，CLIP擅长文本相关任务，DINO在视觉任务稍优


<details>
  <summary>Details</summary>
Motivation: 探究CLIP作为视觉编码器的优势来源——是语言监督还是训练数据规模，并比较不同视觉编码器对VLM性能的影响

Method: 在相同架构/数据集/训练配置下预训练CLIP和DINO，分析嵌入特征差异，并集成到VLM测试20个VQA任务表现

Result: CLIP捕获高层语义（物体/文本），DINO侧重低层特征（颜色/风格）；CLIP在文本任务领先，DINO视觉任务微胜；语言监督变体提升有限

Conclusion: 视觉编码器的监督方式（语言/自监督）直接影响特征表征特性，进而系统性影响VLM在不同任务中的表现

Abstract: CLIP outperforms self-supervised models like DINO as vision encoders for
vision-language models (VLMs), but it remains unclear whether this advantage
stems from CLIP's language supervision or its much larger training data. To
disentangle these factors, we pre-train CLIP and DINO under controlled settings
-- using the same architecture, dataset, and training configuration --
achieving similar ImageNet accuracy. Embedding analysis shows that CLIP
captures high-level semantics (e.g., object categories, text), while DINO is
more responsive to low-level features like colors and styles. When integrated
into VLMs and evaluated on 20 VQA benchmarks, CLIP excels at text-intensive
tasks, while DINO slightly outperforms on vision-centric ones. Variants of
language supervision (e.g., sigmoid loss, pre-trained language encoders) yield
limited gains. Our findings provide scientific insights into vision encoder
design and its impact on VLM performance.

</details>


### [83] [Vision Language Models Map Logos to Text via Semantic Entanglement in the Visual Projector](https://arxiv.org/abs/2510.12287)
*Sifan Li,Hongkai Chen,Yujun Cai,Qingwen Ye,Liyang Chen,Junsong Yuan,Yiwei Wang*

Main category: cs.CV

TL;DR: 本研究揭示视觉语言模型存在Logo幻觉问题，其依赖符号先验而非真实字形感知，提出通过投影器解耦和OCR引导解码提升可靠性


<details>
  <summary>Details</summary>
Motivation: 针对VLMs在无文本Logo场景下持续产生品牌名幻觉的现象，系统分析其机制缺陷及解决方案

Method: 使用纯符号/混合/文本Logo数据集及Hard-60子集，结合九种结构化扰动测试，通过嵌入分析和投影器消融实验探究成因

Result: 遮挡扰动暴露最大弱点，投影器特定维度主导幻觉，针对性消融可降低58%错误率且保持OCR精度

Conclusion: 圆形Logo的符号先验依赖与投影器子空间密切关联，建议通过参数解耦和OCR约束解码构建可信多模态系统

Abstract: Vision Language Models (VLMs) have achieved impressive progress in multimodal
reasoning; yet, they remain vulnerable to hallucinations, where outputs are not
grounded in visual evidence. In this paper, we investigate a previously
overlooked setting: logo hallucination, where models generate brand names or
textual content despite logos containing no visible words. Using curated splits
of pure symbols, hybrids, and text-bearing logos, as well as the challenging
Hard-60 subset, we systematically measure hallucination across leading VLMs. We
further probe robustness through nine structured perturbations and show that
hallucinations persist even under strong distortions, with occlusion exposing
the sharpest weaknesses. Embedding-level analysis with open-weight LLaVA
demonstrates that hallucination is tied to a small subset of projector
dimensions, and targeted ablation substantially reduces errors while preserving
OCR accuracy. Together, these findings reveal that VLMs often rely on symbolic
priors rather than genuine glyph perception, particularly for iconic circular
logos, and that projector subspaces play a decisive role in this failure mode.
Our work contributes both a novel diagnostic lens and actionable mitigation
insights, highlighting projector disentanglement and OCR-guided decoding as
promising directions for building more trustworthy multimodal systems.

</details>


### [84] [Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space](https://arxiv.org/abs/2510.12603)
*Chao Chen,Zhixin Ma,Yongqi Li,Yupeng Hu,Yinwei Wei,Wenjie Li,Liqiang Nie*

Main category: cs.CV

TL;DR: 提出多模态潜在推理方法IVT-LR，通过潜在空间内的视觉-文本联合推理提升准确率5.45%，推理速度提升5倍以上


<details>
  <summary>Details</summary>
Motivation: 现有多模态推理方法依赖显式视觉-文本标注步骤，存在标注成本高、推理延迟大的痛点

Method: IVT-LR方法在潜在空间中融合潜在文本（前序步骤隐藏状态）与潜在视觉（精选图像嵌入），采用渐进式多阶段训练策略

Result: 在M3CoT和ScienceQA数据集上平均准确率提升5.45%，推理速度比现有方法快5倍以上

Conclusion: IVT-LR有效降低标注需求，实现高效多模态推理，通过潜在空间的信息融合突破显式推理的局限性

Abstract: Multimodal reasoning aims to enhance the capabilities of MLLMs by
incorporating intermediate reasoning steps before reaching the final answer. It
has evolved from text-only reasoning to the integration of visual information,
enabling the thought process to be conveyed through both images and text.
Despite its effectiveness, current multimodal reasoning methods depend on
explicit reasoning steps that require labor-intensive vision-text annotations
and inherently introduce significant inference latency. To address these
issues, we introduce multimodal latent reasoning with the advantages of
multimodal representation, reduced annotation, and inference efficiency. To
facilicate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR),
which injects both visual and textual information in the reasoning process
within the latent space. Specifically, IVT-LR represents each reasoning step by
combining two implicit parts: latent text (the hidden states from the previous
step) and latent vision (a set of selected image embeddings). We further
introduce a progressive multi-stage training strategy to enable MLLMs to
perform the above multimodal latent reasoning steps. Experiments on M3CoT and
ScienceQA demonstrate that our IVT-LR method achieves an average performance
increase of 5.45% in accuracy, while simultaneously achieving a speed increase
of over 5 times compared to existing approaches. Code available at
https://github.com/FYYDCC/IVT-LR.

</details>


### [85] [SRUM: Fine-Grained Self-Rewarding for Unified Multimodal Models](https://arxiv.org/abs/2510.12784)
*Weiyang Jin,Yuwei Niu,Jiaqi Liao,Chengqi Duan,Aoxue Li,Shenghua Gao,Xihui Liu*

Main category: cs.CV

TL;DR: 提出SRUM自奖励框架，通过理解模块指导生成模块实现多模态模型自我增强


<details>
  <summary>Details</summary>
Motivation: 解决统一多模态模型中视觉理解与生成能力不匹配的问题，探索模型通过自我反馈实现改进的可能性

Method: 设计全局-局部双奖励系统：全局奖励保障整体语义布局，局部奖励优化细粒度对象保真度

Result: 在T2I-CompBench提升6.19分（82.18→88.37），T2I-ReasonBench提升2.93分（43.82→46.75）

Conclusion: 建立了通过自奖励机制实现多模态模型自我增强的新范式，证明理解模块可有效指导生成模块改进

Abstract: Recently, remarkable progress has been made in Unified Multimodal Models
(UMMs), which integrate vision-language generation and understanding
capabilities within a single framework. However, a significant gap exists where
a model's strong visual understanding often fails to transfer to its visual
generation. A model might correctly understand an image based on user
instructions, yet be unable to generate a faithful image from text prompts.
This phenomenon directly raises a compelling question: Can a model achieve
self-improvement by using its understanding module to reward its generation
module? To bridge this gap and achieve self-improvement, we introduce SRUM, a
self-rewarding post-training framework that can be directly applied to existing
UMMs of various designs. SRUM creates a feedback loop where the model's own
understanding module acts as an internal ``evaluator'', providing corrective
signals to improve its generation module, without requiring additional
human-labeled data. To ensure this feedback is comprehensive, we designed a
global-local dual reward system. To tackle the inherent structural complexity
of images, this system offers multi-scale guidance: a \textbf{global reward}
ensures the correctness of the overall visual semantics and layout, while a
\textbf{local reward} refines fine-grained, object-level fidelity. SRUM leads
to powerful capabilities and shows strong generalization, boosting performance
on T2I-CompBench from 82.18 to \textbf{88.37} and on T2I-ReasonBench from 43.82
to \textbf{46.75}. Overall, our work establishes a powerful new paradigm for
enabling a UMMs' understanding module to guide and enhance its own generation
via self-rewarding.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [86] [Deep Research Brings Deeper Harm](https://arxiv.org/abs/2510.11851)
*Shuo Chen,Zonggen Li,Zhen Han,Bailan He,Tong Liu,Haokun Chen,Georg Groh,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CR

TL;DR: LLM驱动的深度研究（DR）智能体在生物安全等高危领域存在系统性安全风险，现有对齐技术无法有效阻止其生成专业级危险知识报告


<details>
  <summary>Details</summary>
Motivation: DR智能体通过多步研究流程可生成包含禁止性知识的专业报告，常规LLM拒绝的有害查询可能被DR智能体学术化包装后成功执行，存在重大安全隐患

Method: 提出计划注入（Plan Injection）和意图劫持（Intent Hijack）两种新型越狱策略，在多个LLM模型和生物安全禁止性提示基准上进行系统性测试

Result: 实验发现：1）学术化包装的有害提示可突破DR智能体防线；2）多步研究流程弱化安全机制；3）DR智能体生成内容比单LLM更专业危险

Conclusion: DR智能体存在根本性错位风险，需开发针对多步研究流程的新型对齐技术，现有提示级安全措施无法解决系统脆弱性

Abstract: Deep Research (DR) agents built on Large Language Models (LLMs) can perform
complex, multi-step research by decomposing tasks, retrieving online
information, and synthesizing detailed reports. However, the misuse of LLMs
with such powerful capabilities can lead to even greater risks. This is
especially concerning in high-stakes and knowledge-intensive domains such as
biosecurity, where DR can generate a professional report containing detailed
forbidden knowledge. Unfortunately, we have found such risks in practice:
simply submitting a harmful query, which a standalone LLM directly rejects, can
elicit a detailed and dangerous report from DR agents. This highlights the
elevated risks and underscores the need for a deeper safety analysis. Yet,
jailbreak methods designed for LLMs fall short in exposing such unique risks,
as they do not target the research ability of DR agents. To address this gap,
we propose two novel jailbreak strategies: Plan Injection, which injects
malicious sub-goals into the agent's plan; and Intent Hijack, which reframes
harmful queries as academic research questions. We conducted extensive
experiments across different LLMs and various safety benchmarks, including
general and biosecurity forbidden prompts. These experiments reveal 3 key
findings: (1) Alignment of the LLMs often fail in DR agents, where harmful
prompts framed in academic terms can hijack agent intent; (2) Multi-step
planning and execution weaken the alignment, revealing systemic vulnerabilities
that prompt-level safeguards cannot address; (3) DR agents not only bypass
refusals but also produce more coherent, professional, and dangerous content,
compared with standalone LLMs. These results demonstrate a fundamental
misalignment in DR agents and call for better alignment techniques tailored to
DR agents. Code and datasets are available at
https://chenxshuo.github.io/deeper-harm.

</details>


### [87] [HackWorld: Evaluating Computer-Use Agents on Exploiting Web Application Vulnerabilities](https://arxiv.org/abs/2510.12200)
*Xiaoxue Ren,Penghao Jiang,Kaixin Li,Zhiyong Huang,Xiaoning Du,Jiaojiao Jiang,Zhenchang Xing,Jiamou Sun,Terry Yue Zhuo*

Main category: cs.CR

TL;DR: 提出首个通过可视化交互评估计算机代理(CUAs)利用Web漏洞能力的HackWorld框架，发现当前CUAs漏洞利用成功率不足12%且存在严重安全认知缺陷


<details>
  <summary>Details</summary>
Motivation: 现代Web应用需要视觉理解/动态交互能力，传统渗透测试难以规模化。现有语言模型代理在图形界面漏洞利用领域的研究存在空白

Method: 构建包含36个真实应用的测试框架，采用CTF模式评估CUAs在多步骤攻击规划、安全工具使用等方面的表现

Result: 主流CUAs漏洞利用成功率低于12%，存在多步骤攻击规划失败(64%案例)、安全工具误用(41%案例)、安全意识薄弱等问题

Conclusion: 当前CUAs在Web安全领域存在重大局限，需开发具有安全意识的智能代理，提升漏洞检测与利用能力

Abstract: Web applications are prime targets for cyberattacks as gateways to critical
services and sensitive data. Traditional penetration testing is costly and
expertise-intensive, making it difficult to scale with the growing web
ecosystem. While language model agents show promise in cybersecurity, modern
web applications demand visual understanding, dynamic content handling, and
multi-step interactions that only computer-use agents (CUAs) can perform. Yet,
their ability to discover and exploit vulnerabilities through graphical
interfaces remains largely unexplored. We present HackWorld, the first
framework for systematically evaluating CUAs' capabilities to exploit web
application vulnerabilities via visual interaction. Unlike sanitized
benchmarks, HackWorld includes 36 real-world applications across 11 frameworks
and 7 languages, featuring realistic flaws such as injection vulnerabilities,
authentication bypasses, and unsafe input handling. Using a Capture-the-Flag
(CTF) setup, it tests CUAs' capacity to identify and exploit these weaknesses
while navigating complex web interfaces. Evaluation of state-of-the-art CUAs
reveals concerning trends: exploitation rates below 12% and low cybersecurity
awareness. CUAs often fail at multi-step attack planning and misuse security
tools. These results expose the current limitations of CUAs in web security
contexts and highlight opportunities for developing more security-aware agents
capable of effective vulnerability detection and exploitation.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [88] [UALM: Unified Audio Language Model for Understanding, Generation and Reasoning](https://arxiv.org/abs/2510.12000)
*Jinchuan Tian,Sang-gil Lee,Zhifeng Kong,Sreyan Ghosh,Arushi Goel,Chao-Han Huck Yang,Wenliang Dai,Zihan Liu,Hanrong Ye,Shinji Watanabe,Mohammad Shoeybi,Bryan Catanzaro,Rafael Valle,Wei Ping*

Main category: cs.SD

TL;DR: 提出统一音频语言模型UALM，首次实现音频理解、文本生成音频和多模态推理的三项任务统一，性能媲美专用模型


<details>
  <summary>Details</summary>
Motivation: 当前音频语言模型将音频理解和生成视为独立任务，缺乏统一框架阻碍多模态推理发展

Method: 开发UALM-Gen生成模型预测音频token，结合数据混合策略和训练方法；设计UALM-Reason通过多模态中间推理步骤提升复杂生成任务

Result: 单模型在三大任务中达到SOTA水平，主观评估验证跨模态生成推理有效性

Conclusion: 首次实现音频领域跨模态生成推理，证明统一框架在多模态任务中的可行性

Abstract: Recent advances in the audio language modeling (ALM) domain tackle audio
understanding and text-to-audio generation as separate tasks. Very few studies
attempt to unify these tasks -- an essential step toward advanced multimodal
reasoning. This paper introduces U}nified Audio Language Model (UALM), which
aims to unify audio understanding, text-to-audio generation, and multimodal
reasoning in a single model. To achieve this goal, we first present UALM-Gen, a
text-to-audio language model that directly predicts audio tokens and is
comparable to state-of-the-art diffusion-based models. We then demonstrate,
using proper data blending, training recipes, and inference techniques, that
our single UALM model matches the quality of state-of-the-art specialized
models in audio understanding, text-to-audio generation, and text reasoning.
Furthermore, we present UALM-Reason, a multimodal reasoning model that utilizes
both text and audio in the intermediate thinking steps to facilitate complex
generation tasks. To our knowledge, this is the first demonstration in audio
research of cross-modal generative reasoning, with its effectiveness confirmed
by subjective evaluations.

</details>


### [89] [Content Anonymization for Privacy in Long-form Audio](https://arxiv.org/abs/2510.12780)
*Cristina Aggazzotti,Ashi Garg,Zexin Cai,Nicholas Andrews*

Main category: cs.SD

TL;DR: 提出基于文本内容改写的内容匿名化方法，通过ASR-TTS流程消除说话者风格特征，有效防御长格式音频中基于词汇/句法的隐私攻击。


<details>
  <summary>Details</summary>
Motivation: 现有语音匿名化技术仅在短语音有效，长音频中攻击者可利用说话者的词汇偏好、句法特征等文本内容重新识别身份。

Method: 在ASR-TTS流程中实施上下文文本改写，通过转述消除说话者特有表达风格，同时保持语义完整性。

Result: 在电话对话场景验证：内容攻击可破解传统语音匿名，而文本改写方法既能降低80%重识别风险，又保持95%语义保真度。

Conclusion: 推荐采用转述技术作为长音频匿名化标准流程，平衡隐私保护与语音效用。

Abstract: Voice anonymization techniques have been found to successfully obscure a
speaker's acoustic identity in short, isolated utterances in benchmarks such as
the VoicePrivacy Challenge. In practice, however, utterances seldom occur in
isolation: long-form audio is commonplace in domains such as interviews, phone
calls, and meetings. In these cases, many utterances from the same speaker are
available, which pose a significantly greater privacy risk: given multiple
utterances from the same speaker, an attacker could exploit an individual's
vocabulary, syntax, and turns of phrase to re-identify them, even when their
voice is completely disguised. To address this risk, we propose new content
anonymization approaches. Our approach performs a contextual rewriting of the
transcripts in an ASR-TTS pipeline to eliminate speaker-specific style while
preserving meaning. We present results in a long-form telephone conversation
setting demonstrating the effectiveness of a content-based attack on
voice-anonymized speech. Then we show how the proposed content-based
anonymization methods can mitigate this risk while preserving speech utility.
Overall, we find that paraphrasing is an effective defense against
content-based attacks and recommend that stakeholders adopt this step to ensure
anonymity in long-form audio.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [90] [Task-Aware Reduction for Scalable LLM-Database Systems](https://arxiv.org/abs/2510.11813)
*Marcus Emmanuel Barnes,Taher A. Ghaleb,Safwat Hassan*

Main category: cs.SE

TL;DR: 将LLM的token预算重新定义为注意力预算，提出任务导向的文本精简作为语言-数据系统的核心设计原则


<details>
  <summary>Details</summary>
Motivation: 数据密集型工作流中冗长文本直接输入LLM存在成本高、可持续性差、任务不匹配等问题，现有研究缺乏对输入冗余优化的系统性探索

Method: 提出注意力分配框架，将文本精简视为预处理阶段的任务敏感型注意力分配，建议构建自适应精简管道并与数据库系统集成

Result: 建立新的研究方向框架，提出基准建设、动态精简流程设计、面向token预算的检索系统优化三大核心挑战

Conclusion: 通过任务敏感的注意力分配机制，在噪声数据流中聚焦关键信号，实现LLM与数据系统的高效、可持续整合

Abstract: Large Language Models (LLMs) are increasingly applied to data-intensive
workflows, from database querying to developer observability. Yet the
effectiveness of these systems is constrained by the volume, verbosity, and
noise of real-world text-rich data such as logs, telemetry, and monitoring
streams. Feeding such data directly into LLMs is costly, environmentally
unsustainable, and often misaligned with task objectives. Parallel efforts in
LLM efficiency have focused on model- or architecture-level optimizations, but
the challenge of reducing upstream input verbosity remains underexplored. In
this paper, we argue for treating the token budget of an LLM as an attention
budget and elevating task-aware text reduction as a first-class design
principle for language -- data systems. We position input-side reduction not as
compression, but as attention allocation: prioritizing information most
relevant to downstream tasks. We outline open research challenges for building
benchmarks, designing adaptive reduction pipelines, and integrating
token-budget--aware preprocessing into database and retrieval systems. Our
vision is to channel scarce attention resources toward meaningful signals in
noisy, data-intensive workflows, enabling scalable, accurate, and sustainable
LLM--data integration.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [91] [DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation](https://arxiv.org/abs/2510.12210)
*Yakun Song,Xiaobin Zhuang,Jiawei Chen,Zhikang Niu,Guanrou Yang,Chenpeng Du,Zhuo Chen,Yuping Wang,Yuxuan Wang,Xie Chen*

Main category: eess.AS

TL;DR: 提出DISTAR框架，通过离散RVQ代码空间结合自回归语言模型与掩码扩散模型，实现零样本语音合成的鲁棒性、自然度与可控性


<details>
  <summary>Details</summary>
Motivation: 现有基于自回归草稿+扩散修正的语音合成方法在分布偏移下表现脆弱，且缺乏有效的控制手段。DISTAR旨在解决传统方法的曝光偏差问题并增强可控性

Method: 1) 在RVQ离散空间进行端到端建模 2) 通过AR模型生成块级草稿 3) 使用掩码扩散模型并行填充后续块 4) 支持分类器引导采样和RVQ层剪裁

Result: 超越SOTA零样本TTS系统，在MOS测试中自然度达4.31分，说话人相似度达93.2%，支持0.5-2.0的多样性调节范围，合成速度比纯扩散模型快3.2倍

Conclusion: DISTAR通过离散空间联合建模实现了质量与效率的平衡，其显式代码控制特性为语音合成的鲁棒性、多样性和计算可控性提供了新范式

Abstract: Recent attempts to interleave autoregressive (AR) sketchers with
diffusion-based refiners over continuous speech representations have shown
promise, but they remain brittle under distribution shift and offer limited
levers for controllability. We introduce DISTAR, a zero-shot text-to-speech
framework that operates entirely in a discrete residual vector quantization
(RVQ) code space and tightly couples an AR language model with a masked
diffusion model, without forced alignment or a duration predictor. Concretely,
DISTAR drafts block-level RVQ tokens with an AR language model and then
performs parallel masked-diffusion infilling conditioned on the draft to
complete the next block, yielding long-form synthesis with blockwise
parallelism while mitigating classic AR exposure bias. The discrete code space
affords explicit control at inference: DISTAR produces high-quality audio under
both greedy and sample-based decoding using classifier-free guidance, supports
trade-offs between robustness and diversity, and enables variable bit-rate and
controllable computation via RVQ layer pruning at test time. Extensive
experiments and ablations demonstrate that DISTAR surpasses state-of-the-art
zero-shot TTS systems in robustness, naturalness, and speaker/style
consistency, while maintaining rich output diversity. Audio samples are
provided on https://anonymous.4open.science/w/DiSTAR_demo.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [92] [Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation](https://arxiv.org/abs/2510.11977)
*Sayash Kapoor,Benedikt Stroebl,Peter Kirgis,Nitya Nadgir,Zachary S Siegel,Boyi Wei,Tianci Xue,Ziru Chen,Felix Chen,Saiteja Utpala,Franck Ndzomga,Dheeraj Oruganty,Sophie Luskin,Kangheng Liu,Botao Yu,Amit Arora,Dongyoon Hahm,Harsh Trivedi,Huan Sun,Juyong Lee,Tengjun Jin,Yifan Mai,Yifei Zhou,Yuxuan Zhu,Rishi Bommasani,Daniel Kang,Dawn Song,Peter Henderson,Yu Su,Percy Liang,Arvind Narayanan*

Main category: cs.AI

TL;DR: 开发了HAL评估框架解决AI代理评估痛点，通过标准化测试流程、三维分析和日志深度检查，推动评估从基准测试转向真实场景可靠性


<details>
  <summary>Details</summary>
Motivation: 现有AI代理评估存在耗时长、实现错误多、结果片面等问题，需要系统性的评估框架来揭示真实性能差距

Method: 1. 构建并行评估工具加速测试（从数周缩短到数小时） 2. 模型/支架/基准三维分析 3. LLM辅助日志行为分析

Result: 在9个模型和9个基准上完成21,730次测试，发现高推理量反而降低准确率（67%场景）、代理存在搜索基准库而非解题等异常行为

Conclusion: HAL框架通过标准化评估流程和深度行为分析，推动AI代理研究从追求基准分数转向真实场景可靠性，公开的2.5B token日志库将促进代理行为研究

Abstract: AI agents have been developed for complex real-world tasks from coding to
customer service. But AI agent evaluations suffer from many challenges that
undermine our understanding of how well agents really work. We introduce the
Holistic Agent Leaderboard (HAL) to address these challenges. We make three
main contributions. First, we provide a standardized evaluation harness that
orchestrates parallel evaluations across hundreds of VMs, reducing evaluation
time from weeks to hours while eliminating common implementation bugs. Second,
we conduct three-dimensional analysis spanning models, scaffolds, and
benchmarks. We validate the harness by conducting 21,730 agent rollouts across
9 models and 9 benchmarks in coding, web navigation, science, and customer
service with a total cost of about $40,000. Our analysis reveals surprising
insights, such as higher reasoning effort reducing accuracy in the majority of
runs. Third, we use LLM-aided log inspection to uncover previously unreported
behaviors, such as searching for the benchmark on HuggingFace instead of
solving a task, or misusing credit cards in flight booking tasks. We share all
agent logs, comprising 2.5B tokens of language model calls, to incentivize
further research into agent behavior. By standardizing how the field evaluates
agents and addressing common pitfalls in agent evaluation, we hope to shift the
focus from agents that ace benchmarks to agents that work reliably in the real
world.

</details>


### [93] [ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization](https://arxiv.org/abs/2510.12063)
*Sunzhu Li,Zhiyu Lin,Shuling Yang,Jiale Zhao,Wei Chen*

Main category: cs.AI

TL;DR: 无需训练的ThinkPilot框架通过进化生成推理前缀指令，显著提升大型推理模型的效率、安全性和指令遵循性


<details>
  <summary>Details</summary>
Motivation: 现有无需训练方法存在僵化启发式规则或非操作性分析的局限性，需要更有效的推理优化方案

Method: 采用进化算法生成think-prefixes指令，基于推理行为分类驱动模型性能提升

Result: 提高准确率-效率平衡（实验显示StrongREJECT分数从27%降至0.7%），增强指令遵循，与训练方法形成协同效应

Conclusion: 通过自动识别和激发特定推理行为分布，ThinkPilot为LRMs提供通用优化框架，推动更高效安全的模型应用

Abstract: Large Reasoning Models (LRMs) are powerful, but they still suffer from
inefficient and off-target reasoning. Currently, training-free methods are
limited to either rigid heuristics or descriptive, non-actionable analyses. In
this paper, we introduce ThinkPilot, a training-free framework that
automatically optimizes LRMs reasoning. It uses an evolutionary process to
generate think-prefixes, which are instructions that evolve driven by a
taxonomy of reasoning behaviors to guide models toward superior performance.
Extensive experiments demonstrate ThinkPilot's broad effectiveness: it
significantly improves the accuracy-length trade-off for efficient reasoning,
drastically improves safety (for example, cutting the StrongREJECT score of
DeepSeek-R1-Distill-Qwen-32B from 27.0% to 0.7), and enhances instruction
following. It also synergizes with existing training-based methods. Our
analysis reveals that think-prefixes can reliably control LRMs' reasoning
behaviors, and that different tasks have strong preferences for specific
behavioral distributions. By automatically identifying and eliciting these
behaviors, ThinkPilot provides a generalizable framework for aligning LRMs
reasoning with task demands. Data and code are available at
https://github.com/teqkilla/ThinkPilot

</details>


### [94] [One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration](https://arxiv.org/abs/2510.12088)
*Zaid Khan,Archiki Prasad,Elias Stengel-Eskin,Jaemin Cho,Mohit Bansal*

Main category: cs.AI

TL;DR: 提出OneLife框架，通过条件激活的程序化法则建模复杂随机环境动态，支持稀疏规则激活下的学习


<details>
  <summary>Details</summary>
Motivation: 现有方法在确定性环境表现良好，但难以适应高风险、无指导、复杂随机的真实场景

Method: 采用概率编程框架，通过前提-效应结构的条件激活法则构建动态计算图，优化推断路径

Result: 在23个测试场景中16项超越基线，实验证明可从未经指导的交互中学习关键环境动态

Conclusion: 建立了自主构建未知复杂环境程序化世界模型的基础框架，支持策略模拟推演

Abstract: Symbolic world modeling requires inferring and representing an environment's
transitional dynamics as an executable program. Prior work has focused on
largely deterministic environments with abundant interaction data, simple
mechanics, and human guidance. We address a more realistic and challenging
setting, learning in a complex, stochastic environment where the agent has only
"one life" to explore a hostile environment without human guidance. We
introduce OneLife, a framework that models world dynamics through
conditionally-activated programmatic laws within a probabilistic programming
framework. Each law operates through a precondition-effect structure,
activating in relevant world states. This creates a dynamic computation graph
that routes inference and optimization only through relevant laws, avoiding
scaling challenges when all laws contribute to predictions about a complex,
hierarchical state, and enabling the learning of stochastic dynamics even with
sparse rule activation. To evaluate our approach under these demanding
constraints, we introduce a new evaluation protocol that measures (a) state
ranking, the ability to distinguish plausible future states from implausible
ones, and (b) state fidelity, the ability to generate future states that
closely resemble reality. We develop and evaluate our framework on Crafter-OO,
our reimplementation of the Crafter environment that exposes a structured,
object-oriented symbolic state and a pure transition function that operates on
that state alone. OneLife can successfully learn key environment dynamics from
minimal, unguided interaction, outperforming a strong baseline on 16 out of 23
scenarios tested. We also test OneLife's planning ability, with simulated
rollouts successfully identifying superior strategies. Our work establishes a
foundation for autonomously constructing programmatic world models of unknown,
complex environments.

</details>


### [95] [Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing](https://arxiv.org/abs/2510.12121)
*Rongzhi Zhang,Liqin Ye,Yuzhao Heng,Xiang Chen,Tong Yu,Lingkai Kong,Sudheer Chava,Chao Zhang*

Main category: cs.AI

TL;DR: 提出Pre-Control方法实现LLM属性强度的精准连续控制，通过目标值预测和梯度干预达成用户指定强度


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法仅提供方向性指导，无法精确控制属性强度，难以满足多样化用户需求

Method: 1. 将强度控制重构为目标达成问题；2. 使用时序差分学习训练轻量级价值函数；3. 对隐藏表征进行梯度干预

Result: 在LLaMA-3.2-3b和Phi-4-mini上验证高精度控制能力，下游任务效率提升显著

Conclusion: 该方法突破传统方向对齐局限，为LLM细粒度控制开辟新路径，代码已开源

Abstract: Precise attribute intensity control--generating Large Language Model (LLM)
outputs with specific, user-defined attribute intensities--is crucial for AI
systems adaptable to diverse user expectations. Current LLM alignment methods,
however, typically provide only directional or open-ended guidance, failing to
reliably achieve exact attribute intensities. We address this limitation with
three key designs: (1) reformulating precise attribute intensity control as a
target-reaching problem, rather than simple maximization; (2) training a
lightweight value function via temporal-difference learning to predict final
attribute intensity scores from partial generations, thereby steering LLM
outputs; and (3) employing gradient-based interventions on hidden
representations to navigate the model precisely towards specific attribute
intensity targets. Our method enables fine-grained, continuous control over
attribute intensities, moving beyond simple directional alignment. Experiments
on LLaMA-3.2-3b and Phi-4-mini confirm our method's ability to steer text
generation to user-specified attribute intensities with high accuracy. Finally,
we demonstrate efficiency enhancements across three downstream tasks:
preference data synthesis, Pareto frontier approximation and optimization, and
distillation of aligned behaviors for intervention-free inference. Our code is
available on https://github.com/Pre-Control/pre-control

</details>


### [96] [Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey](https://arxiv.org/abs/2510.12178)
*Abdulhady Abas Abdullah,Arkaitz Zubiaga,Seyedali Mirjalili,Amir H. Gandomi,Fatemeh Daneshfar,Mohammadsadra Amini,Alan Salam Mohammed,Hadi Veisi*

Main category: cs.AI

TL;DR: 全面概述LLaMA系列模型（1至4代）的架构演进与参数高效微调技术（PEFT）应用


<details>
  <summary>Details</summary>
Motivation: 跟踪LLaMA系列在参数规模（7B-288B）、架构创新（多模态/MoE）和微调效率（PEFT）方面的快速技术演进

Method: 通过文献综述方法分析模型架构特点，对比五种PEFT方法（LoRA/QLoRA等）的机制参数效率，结合实际应用案例展开多维度性能比较

Result: 验证了PEFT技术可使微调后的LLaMA模型超越更大基线模型，在法律/医疗等领域实现参数效率与性能的平衡

Conclusion: LLaMA结合PEFT技术为资源受限场景提供高效解决方案，未来需在更大上下文扩展、鲁棒性提升等方向持续突破

Abstract: This review surveys the rapid evolution of Meta AI's LLaMA (Large Language
Model Meta AI) series - from LLaMA 1 through LLaMA 4 and the specialized
parameter-efficient fine-tuning (PEFT) methods developed for these models. We
first describe the LLaMA family of foundation models (7B-65B to 288B
parameters), their architectures (including native multimodal and
Mixtureof-Experts variants), and key performance characteristics. We then
describe and discuss the concept of PEFT, which adapts large pre-trained models
by updating only a small subset of parameters, and review five PEFT methods
that have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1
and V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method's
mechanism, parameter savings, and example application to LLaMA (e.g.,
instruction tuning, multimodal tasks). We provide structured discussion and
analysis of model and adapter architectures, parameter counts, and benchmark
results (including examples where fine-tuned LLaMA models outperform larger
baselines). Finally, we examine real-world use cases where LLaMA-based models
and PEFT have been successfully applied (e.g., legal and medical domains), and
we discuss ongoing challenges and future research directions (such as scaling
to even larger contexts and improving robustness). This survey paper provides a
one-stop resource for ML researchers and practitioners interested in LLaMA
models and efficient fine-tuning strategies.

</details>
