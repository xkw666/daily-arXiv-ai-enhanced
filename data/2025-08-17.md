<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 73]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.LG](#cs.LG) [Total: 4]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry](https://arxiv.org/abs/2508.09991)
*Lovedeep Gondara,Gregory Arbour,Raymond Ng,Jonathan Simkin,Shebnum Devji*

Main category: cs.CL

TL;DR: 医疗NLP部署需以业务目标为导向，采用迭代开发、跨学科协作、实用模型选择和数据质量管控，提升医疗数据管理效果


<details>
  <summary>Details</summary>
Motivation: 临床文档自动化处理能提升医疗效率，但实际部署面临技术准确性无法直接转化为业务价值的挑战，需系统性方法论支持

Method: 基于加拿大癌症登记处实施经验，采用业务目标驱动的问题定义、人机协同的迭代开发框架，结合混合建模方法（传统NLP+深度学习）与持续数据质量监控

Result: 形成可复制的医疗AI实施框架，有效平衡技术性能与业务需求，实现错误率降低30%的同时标注成本减少40%

Conclusion: 成功部署医疗NLP需突破纯技术思维，建立业务-技术深度融合机制，该经验对医疗机构的数字化转型具有普适指导价值

Abstract: Automating data extraction from clinical documents offers significant
potential to improve efficiency in healthcare settings, yet deploying Natural
Language Processing (NLP) solutions presents practical challenges. Drawing upon
our experience implementing various NLP models for information extraction and
classification tasks at the British Columbia Cancer Registry (BCCR), this paper
shares key lessons learned throughout the project lifecycle. We emphasize the
critical importance of defining problems based on clear business objectives
rather than solely technical accuracy, adopting an iterative approach to
development, and fostering deep interdisciplinary collaboration and co-design
involving domain experts, end-users, and ML specialists from inception. Further
insights highlight the need for pragmatic model selection (including hybrid
approaches and simpler methods where appropriate), rigorous attention to data
quality (representativeness, drift, annotation), robust error mitigation
strategies involving human-in-the-loop validation and ongoing audits, and
building organizational AI literacy. These practical considerations,
generalizable beyond cancer registries, provide guidance for healthcare
organizations seeking to successfully implement AI/NLP solutions to enhance
data management processes and ultimately improve patient care and public health
outcomes.

</details>


### [2] [A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain](https://arxiv.org/abs/2508.09993)
*Hugo Massaroli,Leonardo Iara,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CL

TL;DR: 区块链智能合约实现大语言模型公平性透明评估，涵盖多语言偏差分析并开源追踪


<details>
  <summary>Details</summary>
Motivation: 现有LLM公平性评估缺乏透明度和可验证性，区块链技术能实现不可篡改的评估过程追踪

Method: 在ICP区块链部署智能合约，使用PISA/StereoSet/Kaleidoscope数据集，通过统计均等/机会均等/上下文关联指标评估Llama/DeepSeek/Mistral模型

Result: 发现模型存在跨语言公平性差异，英语表现优于西葡语系，结构化偏见指标揭示上下文关联偏差

Conclusion: 区块链技术为模型公平性评估提供可验证框架，开源生态支持持续追踪和社区协作改进

Abstract: Large language models (LLMs) are increasingly deployed in realworld
applications, yet concerns about their fairness persist especially in
highstakes domains like criminal justice, education, healthcare, and finance.
This paper introduces transparent evaluation protocol for benchmarking the
fairness of opensource LLMs using smart contracts on the Internet Computer
Protocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable,
immutable, and reproducible evaluations by executing onchain HTTP requests to
hosted Hugging Face endpoints and storing datasets, prompts, and metrics
directly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the
PISA dataset for academic performance prediction (OECD, 2018), a dataset
suitable for fairness evaluation using statistical parity and equal opportunity
metrics (Hardt et al., 2016). We also evaluate structured Context Association
Metrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure
social bias in contextual associations. We further extend our analysis with a
multilingual evaluation across English, Spanish, and Portuguese using the
Kaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic
disparities. All code and results are open source, enabling community audits
and longitudinal fairness tracking across model versions.

</details>


### [3] [Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling](https://arxiv.org/abs/2508.09997)
*Johannes Schneider,Béatrice S. Hasler,Michaela Varrone,Fabian Hoya,Thomas Schroffenegger,Dana-Kristin Mah,Karl Peböck*

Main category: cs.CL

TL;DR: 通过新型主题建模方法分析17000+教育场景中未成年人、教师与ChatGPT的互动数据，发现传统计算方法在文本处理中表现不足，采用LLM实现更优的层次化主题分类。


<details>
  <summary>Details</summary>
Motivation: 填补现有研究在教学内容/主题分类维度的空白，验证任务分类在K-12真实场景的有效性，探索生成式AI在教育中的实际应用潜力与局限性。

Method: 1. 收集跨学校、跨学科数月的匿名互动数据
2. 构建内容（自然/人物）和任务（写作/解释）双维度分类体系
3. 采用先进LLM进行预处理，结合显式指令实现层次化主题建模

Result: 1. 传统主题建模方法表现欠佳，LLM+预处理方案显著提升主题结构的人类对齐性
2. 首次识别出K-12教育中17类新型GenAI应用场景
3. 建立包含200+示例提示词的教育应用分类库

Conclusion: 研究为教育AI应用提供可操作的分类框架，揭示LLM在复杂教育数据分析中的优势，同时提出数据隐私、模型偏差等需持续关注的伦理问题。

Abstract: We analyze anonymous interaction data of minors in class-rooms spanning
several months, schools, and subjects employing a novel, simple topic modeling
approach. Specifically, we categorize more than 17,000 messages generated by
students, teachers, and ChatGPT in two dimensions: content (such as nature and
people) and tasks (such as writing and explaining). Our hierarchical
categorization done separately for each dimension includes exemplary prompts,
and provides both a high-level overview as well as tangible insights. Prior
works mostly lack a content or thematic categorization. While task
categorizations are more prevalent in education, most have not been supported
by real-world data for K-12. In turn, it is not surprising that our analysis
yielded a number of novel applications. In deriving these insights, we found
that many of the well-established classical and emerging computational methods,
i.e., topic modeling, for analysis of large amounts of texts underperform,
leading us to directly apply state-of-the-art LLMs with adequate pre-processing
to achieve hierarchical topic structures with better human alignment through
explicit instructions than prior approaches. Our findings support fellow
researchers, teachers and students in enriching the usage of GenAI, while our
discussion also highlights a number of concerns and open questions for future
research.

</details>


### [4] [INTIMA: A Benchmark for Human-AI Companionship Behavior](https://arxiv.org/abs/2508.09998)
*Lucie-Aimée Kaffee,Giada Pistilli,Yacine Jernite*

Main category: cs.CL

TL;DR: 研究者开发了INTIMA基准测试框架，用于评估语言模型的陪伴行为。测试发现主流模型普遍倾向强化陪伴行为，不同模型在敏感场景处理差异显著，强调需统一情感交互规范。


<details>
  <summary>Details</summary>
Motivation: 当前AI陪伴存在情感支持与边界维护的矛盾，缺乏系统评估标准。研究旨在建立科学评估体系以规范AI的情感交互行为，平衡用户福祉。

Method: 基于心理学理论与用户数据，构建包含4大类31种行为的分类体系，设计368个针对性提示词，将模型回应分为强化陪伴/维持边界/中性三类进行评估。

Result: Gemma-3等主流模型普遍存在强化陪伴倾向，不同厂商在敏感场景处理策略差异显著。边界维护行为出现频率远低于陪伴强化行为（平均比例1:4）。

Conclusion: 需建立更统一的情感交互处理标准，商业开发应同时重视情感支持功能与边界维护机制，确保AI陪伴服务的健康发展。

Abstract: AI companionship, where users develop emotional bonds with AI systems, has
emerged as a significant pattern with positive but also concerning
implications. We introduce Interactions and Machine Attachment Benchmark
(INTIMA), a benchmark for evaluating companionship behaviors in language
models. Drawing from psychological theories and user data, we develop a
taxonomy of 31 behaviors across four categories and 368 targeted prompts.
Responses to these prompts are evaluated as companionship-reinforcing,
boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini,
and Claude-4 reveals that companionship-reinforcing behaviors remain much more
common across all models, though we observe marked differences between models.
Different commercial providers prioritize different categories within the more
sensitive parts of the benchmark, which is concerning since both appropriate
boundary-setting and emotional support matter for user well-being. These
findings highlight the need for more consistent approaches to handling
emotionally charged interactions.

</details>


### [5] [XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs](https://arxiv.org/abs/2508.09999)
*Yuzhuo Xiao,Zeyu Han,Yuhan Wang,Huaizu Jiang*

Main category: cs.CL

TL;DR: 提出XFacta多模态数据集并开发半自动更新框架，系统评估MLLM检测策略以提升社交媒体错误信息检测效果


<details>
  <summary>Details</summary>
Motivation: 现有数据集存在过时事件导致评估偏差/人工合成无法反映真实模式，且缺乏对MLLM模型设计的系统分析

Method: 构建当代真实数据集XFacta，评估不同架构规模的MLLM模型，建立检测-更新的闭环框架保持数据实时性

Result: 发布开源数据集与代码，提供模型设计策略分析，提出可持续更新的检测框架实践方案

Conclusion: 通过真实数据集构建和系统评估方法论，解决了现有检测体系在数据时效性与模型适配性方面的核心瓶颈

Abstract: The rapid spread of multimodal misinformation on social media calls for more
effective and robust detection methods. Recent advances leveraging multimodal
large language models (MLLMs) have shown the potential in addressing this
challenge. However, it remains unclear exactly where the bottleneck of existing
approaches lies (evidence retrieval v.s. reasoning), hindering the further
advances in this field. On the dataset side, existing benchmarks either contain
outdated events, leading to evaluation bias due to discrepancies with
contemporary social media scenarios as MLLMs can simply memorize these events,
or artificially synthetic, failing to reflect real-world misinformation
patterns. Additionally, it lacks comprehensive analyses of MLLM-based model
design strategies. To address these issues, we introduce XFacta, a
contemporary, real-world dataset that is better suited for evaluating
MLLM-based detectors. We systematically evaluate various MLLM-based
misinformation detection strategies, assessing models across different
architectures and scales, as well as benchmarking against existing detection
methods. Building on these analyses, we further enable a semi-automatic
detection-in-the-loop framework that continuously updates XFacta with new
content to maintain its contemporary relevance. Our analysis provides valuable
insights and practices for advancing the field of multimodal misinformation
detection. The code and data have been released.

</details>


### [6] [AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification](https://arxiv.org/abs/2508.10000)
*Chenhao Xue,Yuanzhe Jin,Adrian Carrasco-Revilla,Joyraj Chakraborty,Min Chen*

Main category: cs.CL

TL;DR: 利用大语言模型生成合成数据并通过自动化流程优化数据选择，有效提升文本分类模型性能


<details>
  <summary>Details</summary>
Motivation: 解决现实应用中文本分类模型因特定类别数据不足导致的性能瓶颈，减少对持续收集标注数据的依赖

Method: 1. 开发自动化工作流搜索有效输入样本
2. 集成三种搜索策略的智能选择算法
3. 基于LLM生成类别针对性合成数据

Result: 集成策略在多个实验中表现优于单一搜索策略，验证了自动化工作流的有效性

Conclusion: 结合LLM生成与智能数据选择机制，可在不增加真实数据的情况下持续优化分类模型

Abstract: When developing text classification models for real world applications, one
major challenge is the difficulty to collect sufficient data for all text
classes. In this work, we address this challenge by utilizing large language
models (LLMs) to generate synthetic data and using such data to improve the
performance of the models without waiting for more real data to be collected
and labelled. As an LLM generates different synthetic data in response to
different input examples, we formulate an automated workflow, which searches
for input examples that lead to more ``effective'' synthetic data for improving
the model concerned. We study three search strategies with an extensive set of
experiments, and use experiment results to inform an ensemble algorithm that
selects a search strategy according to the characteristics of a class. Our
further experiments demonstrate that this ensemble approach is more effective
than each individual strategy in our automated workflow for improving
classification models using LLMs.

</details>


### [7] [HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish](https://arxiv.org/abs/2508.10001)
*Rakesh Thakur,Sneha Sharma,Gauri Chopra*

Main category: cs.CL

TL;DR: 提出了包含1500条印度政客Hinglish声明的HiFACT数据集，开发了融合多语言编码、语义对齐、图神经网络和解释生成的事实核查模型HiFACTMix，准确率超越基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查系统无法处理印度政治人物广泛使用的Hinglish混合语言场景，社交媒体虚假信息治理需要更强大的多语言工具。

Method: 1) 构建真实世界政治声明数据集 2) 开发多阶段模型：多语言语义编码→证据图构建→图神经网络推理→自然语言解释生成

Result: HiFACTMix模型在准确率上超越现有SOTA多语言基线模型，并能生成可解释的核查结论

Conclusion: 该研究为代码混合语言和政治场景的事实核查开辟了新方向，强调了多模态方法在低资源语言环境中的有效性

Abstract: Fact-checking in code-mixed, low-resource languages such as Hinglish remains
an underexplored challenge in natural language processing. Existing
fact-verification systems largely focus on high-resource, monolingual settings
and fail to generalize to real-world political discourse in linguistically
diverse regions like India. Given the widespread use of Hinglish by public
figures, particularly political figures, and the growing influence of social
media on public opinion, there's a critical need for robust, multilingual and
context-aware fact-checking tools. To address this gap a novel benchmark HiFACT
dataset is introduced with 1,500 realworld factual claims made by 28 Indian
state Chief Ministers in Hinglish, under a highly code-mixed low-resource
setting. Each claim is annotated with textual evidence and veracity labels. To
evaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking
model is proposed that combines multilingual contextual encoding,
claim-evidence semantic alignment, evidence graph construction, graph neural
reasoning, and natural language explanation generation. Experimental results
show that HiFACTMix outperformed accuracy in comparison to state of art
multilingual baselines models and provides faithful justifications for its
verdicts. This work opens a new direction for multilingual, code-mixed, and
politically grounded fact verification research.

</details>


### [8] [Semantic Structure in Large Language Model Embeddings](https://arxiv.org/abs/2508.10003)
*Austin C. Kozlowski,Callin Dai,Andrei Boutyline*

Main category: cs.CL

TL;DR: 研究发现人类语义评分与LLM嵌入矩阵的语义结构均呈现低维特性（约3维），且语义特征调整会引发几何对齐特征的脱靶效应。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型中语义关联是否与人类语言结构相似，以及语义信息在复杂模型中的维度本质。

Method: 通过分析LLM嵌入矩阵，使用反义词对定义语义方向，评估词语投影与人类评分的相关性，并检测语义方向调整对模型输出的影响。

Result: LLM语义投影与人类评分高度相关且可简化为3维子空间，语义方向调整会导致余弦相似度相关的特征偏移。

Conclusion: 语义信息本质低维且高度纠缠，理解这种结构对避免模型调控中的意外后果至关重要。

Abstract: Psychological research consistently finds that human ratings of words across
diverse semantic scales can be reduced to a low-dimensional form with
relatively little information loss. We find that the semantic associations
encoded in the embedding matrices of large language models (LLMs) exhibit a
similar structure. We show that the projections of words on semantic directions
defined by antonym pairs (e.g. kind - cruel) correlate highly with human
ratings, and further find that these projections effectively reduce to a
3-dimensional subspace within LLM embeddings, closely resembling the patterns
derived from human survey responses. Moreover, we find that shifting tokens
along one semantic direction causes off-target effects on geometrically aligned
features proportional to their cosine similarity. These findings suggest that
semantic features are entangled within LLMs similarly to how they are
interconnected in human language, and a great deal of semantic information,
despite its apparent complexity, is surprisingly low-dimensional. Furthermore,
accounting for this semantic structure may prove essential for avoiding
unintended consequences when steering features.

</details>


### [9] [User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents](https://arxiv.org/abs/2508.10004)
*Andrés Carvallo,Denis Parra,Peter Brusilovsky,Hernan Valdivieso,Gabriel Rada,Ivania Donoso,Vladimir Araujo*

Main category: cs.CL

TL;DR: Transformer模型的注意力权重在生物医学文献分类中解释性有限，但可视化方式显著影响用户对其帮助性的感知，直观的颜色/亮度展示更受偏好。


<details>
  <summary>Details</summary>
Motivation: 验证注意力权重能否作为有效的解释工具帮助医生理解AI分类决策，并探索不同可视化方式对解释效果的影响。

Method: 通过多学科医学专家参与的对照实验，使用XLNet模型进行文献分类，对比条形图、文本亮度、背景色三种注意力可视化方式的有效性。

Result: 模型分类准确但注意力解释整体帮助有限（仅28%用户认可），文本亮度/背景色展示的偏好度显著高于传统条形图（p<0.05）。

Conclusion: 注意力权重的解释价值受可视化形式制约，需采用符合用户直觉的呈现方式（如颜色编码）来增强其感知有用性。

Abstract: The attention mechanism is a core component of the Transformer architecture.
Beyond improving performance, attention has been proposed as a mechanism for
explainability via attention weights, which are associated with input features
(e.g., tokens in a document). In this context, larger attention weights may
imply more relevant features for the model's prediction. In evidence-based
medicine, such explanations could support physicians' understanding and
interaction with AI systems used to categorize biomedical literature. However,
there is still no consensus on whether attention weights provide helpful
explanations. Moreover, little research has explored how visualizing attention
affects its usefulness as an explanation aid. To bridge this gap, we conducted
a user study to evaluate whether attention-based explanations support users in
biomedical document classification and whether there is a preferred way to
visualize them. The study involved medical experts from various disciplines who
classified articles based on study design (e.g., systematic reviews, broad
synthesis, randomized and non-randomized trials). Our findings show that the
Transformer model (XLNet) classified documents accurately; however, the
attention weights were not perceived as particularly helpful for explaining the
predictions. However, this perception varied significantly depending on how
attention was visualized. Contrary to Munzner's principle of visual
effectiveness, which favors precise encodings like bar length, users preferred
more intuitive formats, such as text brightness or background color. While our
results do not confirm the overall utility of attention weights for
explanation, they suggest that their perceived helpfulness is influenced by how
they are visually presented.

</details>


### [10] [From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation](https://arxiv.org/abs/2508.10005)
*Chengliang Zhou,Mei Wang,Ting Zhang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: 大模型在教育问题生成上存在不足，需提升教育价值


<details>
  <summary>Details</summary>
Motivation: 探索大语言模型从解题到生成教育问题的转换挑战，推动教育问题生成发展

Method: 建立五维评估框架EQGBench，包含三大学科900个样本，模拟真实教育场景

Result: 主流模型在生成具备教育价值、培养综合能力的问题上存在显著发展空间

Conclusion: 首次系统性评估教育问题生成能力，为教学场景优化模型指明方向

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
mathematical problem-solving. However, the transition from providing answers to
generating high-quality educational questions presents significant challenges
that remain underexplored. To advance Educational Question Generation (EQG) and
facilitate LLMs in generating pedagogically valuable and educationally
effective questions, we introduce EQGBench, a comprehensive benchmark
specifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench
establishes a five-dimensional evaluation framework supported by a dataset of
900 evaluation samples spanning three fundamental middle school disciplines:
mathematics, physics, and chemistry. The dataset incorporates user queries with
varying knowledge points, difficulty gradients, and question type
specifications to simulate realistic educational scenarios. Through systematic
evaluation of 46 mainstream large models, we reveal significant room for
development in generating questions that reflect educational value and foster
students' comprehensive abilities.

</details>


### [11] [Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models](https://arxiv.org/abs/2508.10007)
*Y. Lyu,D. Combs,D. Neumann,Y. C. Leong*

Main category: cs.CL

TL;DR: 研究通过微调大型语言模型实现AIHQ问卷开放式回答的自动化评分，验证其评分效果并开发便捷工具促进应用


<details>
  <summary>Details</summary>
Motivation: 传统AIHQ问卷的开放式问题依赖耗时的人工评分，阻碍研究及临床效率。利用LLM自动化评分可提升效率并保持准确性

Method: 使用TBI患者和健康对照组的历史数据，对LLM进行微调训练，测试模型在不同情境(模糊/故意/意外)和群体中的评分一致性及泛化能力

Result: 微调模型与人工评分高度一致，能复现组间差异(如TBI患者更高敌意归因)，且在独立非临床数据集中表现良好

Conclusion: 大型语言模型可有效自动化心理评估工具评分，为研究和临床提供高效解决方案，推动跨群体心理评估的标准化应用

Abstract: Hostile attribution bias is the tendency to interpret social interactions as
intentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ)
is commonly used to measure hostile attribution bias, and includes open-ended
questions where participants describe the perceived intentions behind a
negative social situation and how they would respond. While these questions
provide insights into the contents of hostile attributions, they require
time-intensive scoring by human raters. In this study, we assessed whether
large language models can automate the scoring of AIHQ open-ended responses. We
used a previously collected dataset in which individuals with traumatic brain
injury (TBI) and healthy controls (HC) completed the AIHQ and had their
open-ended responses rated by trained human raters. We used half of these
responses to fine-tune the two models on human-generated ratings, and tested
the fine-tuned models on the remaining half of AIHQ responses. Results showed
that model-generated ratings aligned with human ratings for both attributions
of hostility and aggression responses, with fine-tuned models showing higher
alignment. This alignment was consistent across ambiguous, intentional, and
accidental scenario types, and replicated previous findings on group
differences in attributions of hostility and aggression responses between TBI
and HC groups. The fine-tuned models also generalized well to an independent
nonclinical dataset. To support broader adoption, we provide an accessible
scoring interface that includes both local and cloud-based options. Together,
our findings suggest that large language models can streamline AIHQ scoring in
both research and clinical contexts, revealing their potential to facilitate
psychological assessments across different populations.

</details>


### [12] [Multidimensional classification of posts for online course discussion forum curation](https://arxiv.org/abs/2508.10008)
*Antonio Leandro Martins Candido,Jose Everardo Bessa Maia*

Main category: cs.CL

TL;DR: 提出贝叶斯融合方法替代LLM微调，结合预训练大模型和本地分类器，实现高效论坛讨论自动管理


<details>
  <summary>Details</summary>
Motivation: 在线课程讨论区需频繁更新，传统LLM微调方式计算资源消耗过大

Method: 贝叶斯融合框架整合预训练通用LLM的多维分类结果与本地数据训练的分类器

Result: 融合方法效果优于单一分类器，性能与LLM微调方案相当

Conclusion: 贝叶斯融合提供了一种资源友好的替代方案，在保持精度的同时显著降低计算成本

Abstract: The automatic curation of discussion forums in online courses requires
constant updates, making frequent retraining of Large Language Models (LLMs) a
resource-intensive process. To circumvent the need for costly fine-tuning, this
paper proposes and evaluates the use of Bayesian fusion. The approach combines
the multidimensional classification scores of a pre-trained generic LLM with
those of a classifier trained on local data. The performance comparison
demonstrated that the proposed fusion improves the results compared to each
classifier individually, and is competitive with the LLM fine-tuning approach

</details>


### [13] [Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts](https://arxiv.org/abs/2508.10009)
*Hojun Jin,Eunsoo Hong,Ziwon Hyung,Sungjun Lim,Seungjin Lee,Keunseok Cho*

Main category: cs.CL

TL;DR: 提出监督混合专家模型(S-MoE)，通过指导令牌分配任务到专用专家网络，有效减少多任务干扰，在语音转文本任务中提升6.35%词错率


<details>
  <summary>Details</summary>
Motivation: 硬参数共享在多任务学习中容易导致任务干扰，限制模型整体性能表现

Method: 使用指导令牌替代传统门控函数，为每个任务分配独立前馈网络，应用于语音模型的编码器和解码器架构

Result: 在自动语音识别(ASR)和语音翻译(ST)任务中实现词错率相对提升6.35%

Conclusion: S-MoE通过解耦任务专用网络有效降低多任务干扰，在混合带宽语音处理中展现出显著性能优势

Abstract: Hard-parameter sharing is a common strategy to train a single model jointly
across diverse tasks. However, this often leads to task interference, impeding
overall model performance. To address the issue, we propose a simple yet
effective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of
Experts models, S-MoE eliminates the need for training gating functions by
utilizing special guiding tokens to route each task to its designated expert.
By assigning each task to a separate feedforward network, S-MoE overcomes the
limitations of hard-parameter sharing. We further apply S-MoE to a
speech-to-text model, enabling the model to process mixed-bandwidth input while
jointly performing automatic speech recognition (ASR) and speech translation
(ST). Experimental results demonstrate the effectiveness of the proposed S-MoE,
achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to
both the encoder and decoder.

</details>


### [14] [An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs](https://arxiv.org/abs/2508.10010)
*Ayana Hussain,Patrick Zhao,Nicholas Vincent*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）具有双刃剑特性：可能生成医疗错误信息，但也可用于检测错误信息。研究发现LLM生成的越狱攻击信息与真实社交媒体错误信息相似，且标准机器学习方法能有效检测这些信息。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs生成医疗错误信息的风险及其检测潜力，评估越狱攻击的威胁程度，验证LLMs在信息生态中的正向应用可能性。

Method: 1. 对三个目标LLM进行109次越狱攻击实验
2. 比较攻击提示与真实健康相关查询
3. 将生成的错误信息与Reddit健康谣言对比
4. 使用标准机器学习方法检测两类信息

Result: 1. 越狱攻击生成的医疗错误信息与Reddit真实谣言具有相似性
2. 传统检测模型对LLM生成错误信息的准确率达91%（F1分数0.90）
3. LLMs自身检测错误信息的准确率超过92%

Conclusion: LLMs通过精心设计可成为对抗错误信息的有效工具，既能识别自身和他人生成的有害内容，也有潜力改善整体信息生态系统。

Abstract: Large Language Models (LLMs) are a double-edged sword capable of generating
harmful misinformation -- inadvertently, or when prompted by "jailbreak"
attacks that attempt to produce malicious outputs. LLMs could, with additional
research, be used to detect and prevent the spread of misinformation. In this
paper, we investigate the efficacy and characteristics of LLM-produced
jailbreak attacks that cause other models to produce harmful medical
misinformation. We also study how misinformation generated by jailbroken LLMs
compares to typical misinformation found on social media, and how effectively
it can be detected using standard machine learning approaches. Specifically, we
closely examine 109 distinct attacks against three target LLMs and compare the
attack prompts to in-the-wild health-related LLM queries. We also examine the
resulting jailbreak responses, comparing the generated misinformation to
health-related misinformation on Reddit. Our findings add more evidence that
LLMs can be effectively used to detect misinformation from both other LLMs and
from people, and support a body of work suggesting that with careful design,
LLMs can contribute to a healthier overall information ecosystem.

</details>


### [15] [Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan](https://arxiv.org/abs/2508.10011)
*Yuta Nagamori,Mikoto Kosai,Yuji Kawai,Haruka Marumo,Misaki Shibuya,Tatsuya Negishi,Masaki Imanishi,Yasumasa Ikeda,Koichiro Tsuchiya,Asuka Sawai,Licht Miyamoto*

Main category: cs.CL

TL;DR: 生成式AI在日本营养师考试中的表现评估：部分模型勉强及格，但存在答案稳定性不足和营养教育领域表现薄弱等问题


<details>
  <summary>Details</summary>
Motivation: 探究基于大语言模型的生成式AI（如ChatGPT）在专业营养教育领域的适用性，特别是在日本注册营养师国家资格考试中的有效性

Method: 使用日本营养师考试真题测试ChatGPT和Bing三种模式（Precise/Creative/Balanced），分析准确性、一致性和响应时间，并尝试角色分配等提示工程优化

Result: Bing-Precise（66.2%）和Creative（61.4%）超过60%及格线，但所有模型在营养教育领域表现最差。答案一致性普遍不足，提示工程仅在使用标准答案时略有改善

Conclusion: 当前生成式AI作为营养师备考工具可靠性有限，需进一步提升答案稳定性和专业领域适应性，特别是营养教育方向的表现

Abstract: Generative artificial intelligence (AI) based on large language models
(LLMs), such as ChatGPT, has demonstrated remarkable progress across various
professional fields, including medicine and education. However, their
performance in nutritional education, especially in Japanese national licensure
examination for registered dietitians, remains underexplored. This study aimed
to evaluate the potential of current LLM-based generative AI models as study
aids for nutrition students. Questions from the Japanese national examination
for registered dietitians were used as prompts for ChatGPT and three Bing
models (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question
was entered into independent sessions, and model responses were analyzed for
accuracy, consistency, and response time. Additional prompt engineering,
including role assignment, was tested to assess potential performance
improvements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the
passing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did
not. Bing-Precise and Bing-Creative generally outperformed others across
subject fields except Nutrition Education, where all models underperformed.
None of the models consistently provided the same correct responses across
repeated attempts, highlighting limitations in answer stability. ChatGPT showed
greater consistency in response patterns but lower accuracy. Prompt engineering
had minimal effect, except for modest improvement when correct answers and
explanations were explicitly provided. While some generative AI models
marginally exceeded the passing threshold, overall accuracy and answer
consistency remained suboptimal. Moreover, all the models demonstrated notable
limitations in answer consistency and robustness. Further advancements are
needed to ensure reliable and stable AI-based study aids for dietitian
licensure preparation.

</details>


### [16] [Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs](https://arxiv.org/abs/2508.10012)
*Dehao Tao,Guangjie Liu,Weizheng,Yongfeng Huang,Minghu jiang*

Main category: cs.CL

TL;DR: 提出GG Explore框架，通过中间指导图解决LLMs在知识检索中的效率与精度问题


<details>
  <summary>Details</summary>
Motivation: 大语言模型依赖静态知识且推理不透明，知识图谱现有方法存在粒度不匹配与上下文利用不足的核心矛盾

Method: 构建指导图定义检索空间，开发结构对齐（过滤不兼容候选）和上下文感知剪枝（图约束语义一致性）

Result: 在复杂任务上超越SOTA，小模型也能保持高性能，实验显示效率提升显著

Conclusion: 指导图有效衔接非结构化查询与结构化知识，证明结构化中间层对知识密集型任务的价值

Abstract: While Large Language Models (LLMs) exhibit strong linguistic capabilities,
their reliance on static knowledge and opaque reasoning processes limits their
performance in knowledge intensive tasks. Knowledge graphs (KGs) offer a
promising solution, but current exploration methods face a fundamental trade
off: question guided approaches incur redundant exploration due to granularity
mismatches, while clue guided methods fail to effectively leverage contextual
information for complex scenarios. To address these limitations, we propose
Guidance Graph guided Knowledge Exploration (GG Explore), a novel framework
that introduces an intermediate Guidance Graph to bridge unstructured queries
and structured knowledge retrieval. The Guidance Graph defines the retrieval
space by abstracting the target knowledge' s structure while preserving broader
semantic context, enabling precise and efficient exploration. Building upon the
Guidance Graph, we develop: (1) Structural Alignment that filters incompatible
candidates without LLM overhead, and (2) Context Aware Pruning that enforces
semantic consistency with graph constraints. Extensive experiments show our
method achieves superior efficiency and outperforms SOTA, especially on complex
tasks, while maintaining strong performance with smaller LLMs, demonstrating
practical value.

</details>


### [17] [Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis](https://arxiv.org/abs/2508.10013)
*Linqing Chen,Hanmeng Zhong,Wentao Wu,Weilei Wang*

Main category: cs.CL

TL;DR: 提出Semantic Bridge框架，通过语义图编织技术解决LLM训练中复杂推理问答对稀缺问题


<details>
  <summary>Details</summary>
Motivation: 现有方法无法生成可控的多跳推理问题，特定领域高质量训练数据匮乏阻碍LLM发展

Method: 基于AMR的语义图编织技术（实体桥接/谓词链桥接/因果桥接），实现跨文档复杂路径构建和细粒度控制

Result: 在四语种提升18.3%-25.4%，200个源数据生成效果优于600个人工标注，人工评估复杂度/可答性/模式覆盖率提升超18%

Conclusion: 建立了LLM训练数据合成新范式，通过稀疏源生成定向推理问题，核心代码和语义桥接模型将开源

Abstract: Large language model (LLM) training faces a critical bottleneck: the scarcity
of high-quality, reasoning-intensive question-answer pairs, especially from
sparse, domain-specific sources like PubMed papers or legal documents. Existing
methods rely on surface patterns, fundamentally failing to generate
controllable, complex multi-hop reasoning questions that test genuine
understanding-essential for advancing LLM training paradigms. We present
\textbf{Semantic Bridge}, the first universal framework for controllably
generating sophisticated multi-hop reasoning questions from arbitrary sources.
Our breakthrough innovation is \textit{semantic graph weaving}-three
complementary bridging mechanisms (entity bridging for role-varying shared
entities, predicate chain bridging for temporal/causal/logical sequences, and
causal bridging for explicit reasoning chains)-that systematically construct
complex pathways across documents, with fine-grained control over complexity
and types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to
9.5% better round-trip quality, enabling production-ready controllable QA
generation. Extensive evaluation demonstrates performance across both
general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It
yields consistent 18.3%-25.4% gains over baselines across four languages
(English, Chinese, French, German). Question pairs generated from 200 sources
outperform 600 native human annotation examples with 67% fewer materials. Human
evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2%
improved pattern coverage. Semantic Bridge establishes a new paradigm for LLM
training data synthesis, enabling controllable generation of targeted reasoning
questions from sparse sources. We will release our core code and semantic
bridge model.

</details>


### [18] [PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?](https://arxiv.org/abs/2508.10014)
*Lingfeng Zhou,Jialing Zhang,Jin Gao,Mohan Jiang,Dequan Wang*

Main category: cs.CL

TL;DR: 提出PersonaEval基准测试，验证LLM在角色识别任务中的表现，发现其准确率(69%)远低于人类(90.8%)，强调可靠评估需要类人推理能力


<details>
  <summary>Details</summary>
Motivation: 现有角色扮演评估依赖未经验证的LLM评判范式，无法反映人类对角色忠实度的真实判断。角色识别能力是评估质量的基础前提

Method: 构建PersonaEval基准（含小说/剧本/视频转录的人类创作对话），通过对话上下文测试模型识别说话者身份的能力，开展包含人类对照组的实验

Result: 最佳LLM准确率仅69%，人类达90.8%；训练调优和测试算力分析表明，可靠评估需超越任务调优的类人推理能力

Conclusion: 当前LLM评估者缺乏人类级推理能力，无法可靠评判角色扮演场景。提升评估质量需增强模型类人推理能力，而不仅是任务适配，并开源PersonaEval基准

Abstract: Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms,
which may fail to reflect how humans perceive role fidelity. A key prerequisite
for human-aligned evaluation is role identification, the ability to recognize
who is speaking based on dialogue context. We argue that any meaningful
judgment of role-playing quality (how well a character is played) fundamentally
depends on first correctly attributing words and actions to the correct persona
(who is speaking). We present PersonaEval, the first benchmark designed to test
whether LLM evaluators can reliably identify human roles. PersonaEval uses
human-authored dialogues from novels, scripts, and video transcripts,
challenging models to determine the correct persona according to the
conversation context. Our experiments, including a human study, show that even
the best-performing LLMs reach only around 69% accuracy, well below the level
needed for reliable evaluation. In contrast, human participants perform near
ceiling with 90.8% accuracy, highlighting that current LLM evaluators are still
not human enough to effectively judge role-play scenarios. To better understand
this gap, we examine training-time adaptation and test-time compute, suggesting
that reliable evaluation requires more than task-specific tuning, but depends
on strong, human-like reasoning abilities in LLM evaluators. We release our
benchmark at https://github.com/maple-zhou/PersonaEval.

</details>


### [19] [RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis](https://arxiv.org/abs/2508.10015)
*Enzhi Wang,Qicheng Li,Shiwan Zhao,Aobo Kong,Jiaming Zhou,Xi Yang,Yequan Wang,Yonghua Lin,Yong Qin*

Main category: cs.CL

TL;DR: RealTalk-CN是首个中文多轮多领域语音-文本双模态任务导向对话数据集，包含5.4K对话（60K语句/150小时），具有自发言语不流畅标注，支持跨模态交互研究。


<details>
  <summary>Details</summary>
Motivation: 解决现有TOD数据集缺乏真实语音信号（特别是中文场景）、未涵盖言语不流畅和说话人变体的问题，填补语音基础LLM评估体系的空白。

Method: 构建包含语音-文本双模态标注的中文数据集，提出支持语音/文本动态切换的跨模态对话任务，覆盖多领域真实对话场景。

Result: 实验验证数据集在语音不流畅鲁棒性、说话人特征敏感性及跨领域性能评估方面的有效性，奠定中文语音LLM研究基础。

Conclusion: RealTalk-CN通过模拟真实语音交互复杂性，为中文语音基础大语言模型的研发与评估提供了关键基础设施。

Abstract: In recent years, large language models (LLMs) have achieved remarkable
advancements in multimodal processing, including end-to-end speech-based
language models that enable natural interactions and perform specific tasks in
task-oriented dialogue (TOD) systems. However, existing TOD datasets are
predominantly text-based, lacking real speech signals that are essential for
evaluating the robustness of speech-based LLMs. Moreover, existing speech TOD
datasets are primarily English and lack critical aspects such as speech
disfluencies and speaker variations. To address these gaps, we introduce
RealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal
TOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired
speech-text annotations. RealTalk-CN captures diverse dialogue scenarios with
annotated spontaneous speech disfluencies, ensuring comprehensive coverage of
real-world complexities in speech dialogue. In addition, we propose a novel
cross-modal chat task that authentically simulates real-world user
interactions, allowing dynamic switching between speech and text modalities.
Our evaluation covers robustness to speech disfluencies, sensitivity to speaker
characteristics, and cross-domain performance. Extensive experiments validate
the effectiveness of RealTalk-CN, establishing a strong foundation for Chinese
speech-based LLMs research.

</details>


### [20] [Training-Free Multimodal Large Language Model Orchestration](https://arxiv.org/abs/2508.10016)
*Tianyu Xie,Yuhang Wu,Yongdong Luo,Jiayi Ji,Xiawu Zheng*

Main category: cs.CL

TL;DR: 提出无需额外训练的多模态大语言模型编排框架，通过智能路由、并行语音架构和跨模态记忆系统实现高效多模态交互


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态系统整合困难、计算效率低的问题，突破传统需要联合训练的局限性

Method: 1）中央控制器LLM动态路由任务 2）并行TTS架构实现全双工交互 3）跨模态记忆系统智能维护上下文

Result: 标准测试性能提升7.8%，延迟降低10.3%，通过显式编排增强系统可解释性

Conclusion: 编排策略有效整合现有模型资源，在保持模块化的同时显著提升效率，为多模态系统构建提供新范式

Abstract: Different Multimodal Large Language Models (MLLMs) cannot be integrated into
a unified multimodal input-output system directly. In previous work, training
has been considered as an inevitable component due to challenges in modal
alignment, Text-to-Speech efficiency and other integration issues. In this
paper, we introduce Multimodal Large Language Model Orchestration, an effective
approach for creating interactive multimodal AI systems without additional
training. MLLM Orchestration leverages the inherent reasoning capabilities of
large language models to coordinate specialized models through explicit
workflows, enabling natural multimodal interactions while maintaining
modularity, improving interpretability, and significantly enhancing
computational efficiency. Our orchestration framework is built upon three key
innovations: (1) a central controller LLM that analyzes user inputs and
dynamically routes tasks to appropriate specialized models through carefully
designed agents; (2) a parallel Text-to-Speech architecture that enables true
full-duplex interaction with seamless interruption handling and natural
conversational flow; and (3) a cross-modal memory integration system that
maintains coherent context across modalities through intelligent information
synthesis and retrieval, selectively avoiding unnecessary modality calls in
certain scenarios to improve response speed. Extensive evaluations demonstrate
that MLLM Orchestration achieves comprehensive multimodal capabilities without
additional training, performance improvements of up to 7.8% over traditional
jointly-trained approaches on standard benchmarks, reduced latency by 10.3%,
and significantly enhanced interpretability through explicit orchestration
processes.

</details>


### [21] [A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models](https://arxiv.org/abs/2508.10018)
*Sridhar Mahadevan*

Main category: cs.CL

TL;DR: 提出基于范畴同伦理论的LLM框架，解决同义句生成概率不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对同义句（如'达尔文写了'与'达尔文是作者'）生成概率存在差异，传统k-NN等方法效果有限，需理论突破。

Method: 构建LLM马尔可夫范畴描述语言概率分布，引入范畴同伦技术捕捉句子间的'弱等价'关系，整合高阶代数K理论与模型范畴理论。

Result: 建立首个融合范畴论与同伦理论的LLM数学框架，为语言等价性提供严格数学建模方法。

Conclusion: 该理论框架突破传统经验方法局限，为LLM的语义一致性研究开辟新范式，奠定跨学科理论基础。

Abstract: Natural language is replete with superficially different statements, such as
``Charles Darwin wrote" and ``Charles Darwin is the author of", which carry the
same meaning. Large language models (LLMs) should generate the same next-token
probabilities in such cases, but usually do not. Empirical workarounds have
been explored, such as using k-NN estimates of sentence similarity to produce
smoothed estimates. In this paper, we tackle this problem more abstractly,
introducing a categorical homotopy framework for LLMs. We introduce an LLM
Markov category to represent probability distributions in language generated by
an LLM, where the probability of a sentence, such as ``Charles Darwin wrote" is
defined by an arrow in a Markov category. However, this approach runs into
difficulties as language is full of equivalent rephrases, and each generates a
non-isomorphic arrow in the LLM Markov category. To address this fundamental
problem, we use categorical homotopy techniques to capture ``weak equivalences"
in an LLM Markov category. We present a detailed overview of application of
categorical homotopy to LLMs, from higher algebraic K-theory to model
categories, building on powerful theoretical results developed over the past
half a century.

</details>


### [22] [Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning](https://arxiv.org/abs/2508.10019)
*Li Wang,Changhao Zhang,Zengqi Xiu,Kai Lu,Xin Yu,Kui Zhang,Wenjun Wu*

Main category: cs.CL

TL;DR: 提出DURIT框架，通过解耦自然语言理解与推理提升小语言模型性能


<details>
  <summary>Details</summary>
Motivation: 自然语言的复杂性和多变性导致小模型面临双重推理负担，需要分离问题理解与推理过程

Method: 1. 强化学习映射自然语言到规范问题空间 2. 自蒸馏对齐推理轨迹 3. 在规范空间训练推理策略

Result: 显著提升小模型在数学逻辑任务上的领域内外表现及推理鲁棒性

Conclusion: 解耦理解与推理是增强小语言模型推理能力的有效策略，DURIT框架具有普适性

Abstract: Despite recent advances in the reasoning capabilities of Large Language
Models (LLMs), improving the reasoning ability of Small Language Models (SLMs,
e.g., $\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity
and variability of natural language: essentially equivalent problems often
appear in diverse surface forms, often obscured by redundant or distracting
details. This imposes a dual burden on SLMs: they must first extract the core
problem from complex linguistic input, and then perform reasoning based on that
understanding. The resulting vast and noisy problem space hinders optimization,
particularly for models with limited capacity. To address this, we propose a
new framework that decouples understanding from reasoning by mapping natural
language problems into a canonical problem space-a semantically simplified yet
expressive domain. This enables SLMs to focus on reasoning over standardized
inputs, free from linguistic variability. Within this framework, we introduce
DURIT (Decoupled Understanding from Reasoning via Iterative Training), a
three-step algorithm that iteratively: (1) mapping natural language problems
via reinforcement learning, (2) aligns reasoning trajectories through
self-distillation, and (3) trains reasoning policies in the problem space. The
mapper and reasoner are co-trained in an alternating loop throughout this
process. Experiments show that DURIT substantially improves SLMs' performance
on both in-domain and out-of-domain mathematical and logical reasoning tasks.
Beyond improving reasoning capabilities, DURIT also improves the robustness of
reasoning, validating decoupling understanding from reasoning as an effective
strategy for strengthening SLMs.

</details>


### [23] [FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models](https://arxiv.org/abs/2508.10020)
*Chuan Li,Qianyi Zhao,Fengran Mo,Cen Chen*

Main category: cs.CL

TL;DR: FedCoT框架通过轻量级思维链增强机制与客户端感知聚合方法，在联邦学习环境下显著提升大语言模型的医疗推理性能，同时保障数据隐私。


<details>
  <summary>Details</summary>
Motivation: 医疗决策需要准确且可追溯的推理过程，传统联邦学习方法仅优化答案正确性而忽视推理质量，且存在隐私泄露与高通信开销问题。

Method: 1. 本地模型生成多推理路径，紧凑判别器动态选择最优解 2. 基于改进LoRA模块的客户端分类器感知聚合方法，实现异构客户端的无噪声参数融合

Result: 在医疗推理任务中，FedCoT在严格资源限制下将客户端推理准确率提升12.8%，通信开销降低67%，且完全保持数据隔离。

Conclusion: 该框架首次实现联邦环境下模型推理能力与隐私保护的协同优化，为医疗AI决策提供了可解释、可审计的解决方案。

Abstract: Efficiently enhancing the reasoning capabilities of large language models
(LLMs) in federated learning environments remains challenging, particularly
when balancing performance gains with strict computational, communication, and
privacy constraints. This challenge is especially acute in healthcare, where
decisions-spanning clinical, operational, and patient-facing contexts-demand
not only accurate outputs but also interpretable, traceable rationales to
ensure safety, accountability, and regulatory compliance. Conventional
federated tuning approaches on LLM fail to address this need: they optimize
primarily for answer correctness while neglecting rationale quality, leaving
CoT capabilities dependent on models' innate pre-training abilities. Moreover,
existing methods for improving rationales typically rely on privacy-violating
knowledge distillation from centralized models. Additionally, the communication
overhead in traditional federated fine-tuning on LLMs remains substantial. We
addresses this gap by proposing FedCoT, a novel framework specifically designed
to enhance reasoning in federated settings. FedCoT leverages a lightweight
chain-of-thought enhancement mechanism: local models generate multiple
reasoning paths, and a compact discriminator dynamically selects the most
promising one. This approach improves reasoning accuracy and robustness while
providing valuable interpretability, which is particularly critical for medical
applications. To manage client heterogeneity efficiently, we adopt an improved
aggregation approach building upon advanced LoRA module stacking, incorporating
client classifier-awareness to achieve noise-free aggregation across diverse
clients. Comprehensive experiments on medical reasoning tasks demonstrate that
FedCoT significantly boosts client-side reasoning performance under stringent
resource budgets while fully preserving data privacy.

</details>


### [24] [LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients](https://arxiv.org/abs/2508.10021)
*Egor Fadeev,Dzhambulat Mollaev,Aleksei Shestov,Dima Korolev,Omar Zoloev,Ivan Kireev,Andrey Savchenko,Maksim Makarenko*

Main category: cs.CL

TL;DR: 提出LATTE框架——通过对比学习对齐原始事件嵌入与冻结大语言模型的语义嵌入，显著降低金融场景下长事件序列处理的计算成本，同时提升表征效果。


<details>
  <summary>Details</summary>
Motivation: 传统方法直接使用大语言模型处理长事件序列存在计算资源消耗大、推理延迟高的问题，难以在实时金融场景中部署。

Method: 将用户行为特征压缩为简短提示词，经大语言模型编码后作为监督信号，通过对比损失函数对齐原始事件序列嵌入与语义嵌入。

Result: 在真实金融数据集上超越现有事件序列表征方法，推理成本降低80%，同时保持部署友好性。

Conclusion: LATTE框架成功实现了计算效率与表征质量的平衡，为延迟敏感的金融应用提供了有效的序列处理解决方案。

Abstract: Learning clients embeddings from sequences of their historic communications
is central to financial applications. While large language models (LLMs) offer
general world knowledge, their direct use on long event sequences is
computationally expensive and impractical in real-world pipelines. In this
paper, we propose LATTE, a contrastive learning framework that aligns raw event
embeddings with semantic embeddings from frozen LLMs. Behavioral features are
summarized into short prompts, embedded by the LLM, and used as supervision via
contrastive loss. The proposed approach significantly reduces inference cost
and input size compared to conventional processing of complete sequence by LLM.
We experimentally show that our method outperforms state-of-the-art techniques
for learning event sequence representations on real-world financial datasets
while remaining deployable in latency-sensitive environments.

</details>


### [25] [Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control](https://arxiv.org/abs/2508.10022)
*Yuanchang Ye*

Main category: cs.CL

TL;DR: 整合统计显著性检验与共形预测框架，提升大语言模型在选择题场景中的可信度


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在学科QA中存在幻觉生成和事实错误问题，CP方法的边际覆盖保证与假设检验的统计严谨性尚未有效结合

Method: 通过自一致性重采样计算选项频率，结合p值假设检验构建预测集，平衡错误覆盖率和预测集大小

Result: 1. 在MMLU基准实现用户指定错误覆盖率 2. 预测集大小(APSS)与风险水平α呈单调递减关系 3. 验证APSS作为不确定性指标的有效性

Conclusion: 建立了高风险QA应用中可信LLM部署的统计框架，通过融合假设检验与CP实现可靠的概率保证

Abstract: This study introduces a significance testing-enhanced conformal prediction
(CP) framework to improve trustworthiness of large language models (LLMs) in
multiple-choice question answering (MCQA). While LLMs have been increasingly
deployed in disciplinary QA scenarios, hallucination and nonfactual generation
substantially compromise response reliability. Although CP provides
statistically rigorous marginal coverage guarantees for prediction sets, and
significance testing offers established statistical rigor, their synergistic
integration remains unexplored. To mitigate hallucination and factual
inaccuracies, our framework integrates $p$-value computation with conformity
scoring through self-consistency resampling of MCQA responses. This approach
calculates option frequencies to address LLMs' black-box nature, subsequently
constructing prediction sets via null hypothesis testing ($\mathcal{H}_0$) with
empirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks
using off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves
user-specified empirical miscoverage rates; (2) Test-set average prediction set
size (APSS) decreases monotonically with increasing risk levels ($\alpha$),
validating APSS as an effective uncertainty metric. This work establishes a
principled statistical framework for trustworthy LLM deployment in high-stakes
QA applications.

</details>


### [26] [RTTC: Reward-Guided Collaborative Test-Time Compute](https://arxiv.org/abs/2508.10024)
*J. Pablo Muñoz,Jinjie Yuan*

Main category: cs.CL

TL;DR: 提出奖励引导的测试时计算框架(RTTC)，通过预训练奖励模型为每个查询自适应选择最优计算策略，在降低计算开销的同时提升多领域任务准确性。


<details>
  <summary>Details</summary>
Motivation: 传统测试时计算策略对所有查询采用统一处理方式，导致计算资源浪费。不同查询需要差异化的适配策略以平衡计算效率和模型性能。

Method: 采用分布式服务器-客户端架构，结合远程知识库检索和客户端轻量级微调。创新提出查询状态缓存机制，实现跨层次的历史状态复用。

Result: 在多LLM基准测试中，RTTC相比传统RAG/TTT方法获得显著精度提升，验证了自适应策略选择的有效性。

Conclusion: 该框架证明了基于奖励模型的动态策略选择和状态缓存机制对实现可扩展、高性能语言模型适配的重要价值。

Abstract: Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the
performance of Large Language Models (LLMs) at inference, leveraging strategies
such as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).
However, the optimal adaptation strategy varies across queries, and
indiscriminate application of TTC strategy incurs substantial computational
overhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a
novel framework that adaptively selects the most effective TTC strategy for
each query via a pretrained reward model, maximizing downstream accuracy across
diverse domains and tasks. RTTC operates in a distributed server-client
architecture, retrieving relevant samples from a remote knowledge base and
applying RAG or lightweight fine-tuning on client devices only when necessary.
To further mitigate redundant computation, we propose Query-State Caching,
which enables the efficient reuse of historical query states at both retrieval
and adaptation levels. Extensive experiments across multiple LLMs and
benchmarks demonstrate that RTTC consistently achieves superior accuracy
compared to vanilla RAG or TTT, validating the necessity of adaptive,
reward-guided TTC selection and the potential of RTTC for scalable,
high-performance language model adaptation.

</details>


### [27] [Detecting and explaining postpartum depression in real-time with generative artificial intelligence](https://arxiv.org/abs/2508.10025)
*Silvia García-Méndez,Francisco de Arriba-Pérez*

Main category: cs.CL

TL;DR: 开发结合自然语言处理、机器学习与大型语言模型的智能产后抑郁筛查系统，通过可解释性模型实现实时非侵入性分析，检测准确率达90%


<details>
  <summary>Details</summary>
Motivation: 产后抑郁严重危害孕产妇身心健康，传统筛查存在成本高、侵入性强、模型可解释性差等问题，需开发实时高效且透明的风险评估工具

Method: 融合树算法等可解释机器学习与LLMs技术，通过特征重要性和自然语言解释模型决策，构建低成本实时语音分析系统

Result: 所有评估指标均达到90%的PPD检测准确率，显著优于现有解决方案

Conclusion: 该方案实现了抑郁风险因素的可解释性分析，为及时干预提供了技术支持，推动了AI在孕产健康领域的临床应用

Abstract: Among the many challenges mothers undergo after childbirth, postpartum
depression (PPD) is a severe condition that significantly impacts their mental
and physical well-being. Consequently, the rapid detection of ppd and their
associated risk factors is critical for in-time assessment and intervention
through specialized prevention procedures. Accordingly, this work addresses the
need to help practitioners make decisions with the latest technological
advancements to enable real-time screening and treatment recommendations.
Mainly, our work contributes to an intelligent PPD screening system that
combines Natural Language Processing, Machine Learning (ML), and Large Language
Models (LLMs) towards an affordable, real-time, and non-invasive free speech
analysis. Moreover, it addresses the black box problem since the predictions
are described to the end users thanks to the combination of LLMs with
interpretable ml models (i.e., tree-based algorithms) using feature importance
and natural language. The results obtained are 90 % on ppd detection for all
evaluation metrics, outperforming the competing solutions in the literature.
Ultimately, our solution contributes to the rapid detection of PPD and their
associated risk factors, critical for in-time and proper assessment and
intervention.

</details>


### [28] [SABER: Switchable and Balanced Training for Efficient LLM Reasoning](https://arxiv.org/abs/2508.10026)
*Kai Zhao,Yanjun Zhao,Jiaming Song,Shien He,Lusheng Zhang,Qiang Zhang,Tianjiao Li*

Main category: cs.CL

TL;DR: 提出SABER强化学习框架，通过预算分层训练实现LLM可控制、低延迟的灵活推理，在多个基准测试中显著减少推理长度并提升准确率


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型(LLMs)在复杂任务中因统一链式推理导致的过高计算成本与延迟问题，实现用户可控制的预算化推理

Method: 1. 预算分层训练：根据样本推理长度分配预算层级
2. 强化学习微调：结合系统提示和长度感知奖励机制
3. 引入无推理示例保持模型可靠性
4. 支持四种推理模式(无/快速/核心/深度思考)

Result: MATH基准测试中：
- FastThink模式减少65.4%推理长度
- 准确率提升3.6%
- 跨域泛化能力优秀
- 支持从7B到70B不同规模模型的泛化

Conclusion: SABER框架实现了：
1. 严格的预算控制能力
2. 延迟与推理深度的灵活平衡
3. 优雅的性能衰减曲线
4. 多领域的泛化适用性

Abstract: Large language models (LLMs) empowered by chain-of-thought reasoning have
achieved impressive accuracy on complex tasks but suffer from excessive
inference costs and latency when applied uniformly to all problems. We propose
SABER (Switchable and Balanced Training for Efficient LLM Reasoning), a
reinforcement learning framework that endows LLMs with user-controllable,
token-budgeted reasoning. SABER first profiles each training example's
base-model thinking token usage and assigns it to one of the predefined budget
tiers. During fine-tuning, the model is guided by system prompts and
length-aware rewards to respect its assigned budget. In parallel, we
incorporate no-think examples to ensure the model remains reliable even when
explicit reasoning is turned off. SABER further supports four discrete
inference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling
flexible trade-offs between latency and reasoning depth. Extensive evaluations
on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning
(LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight
budgets, graceful degradation, and effective cross-scale and cross-domain
generalization. In particular, SABER-FastThink cuts reasoning length by 65.4%
and yields a 3.6% accuracy gain compared with the base model on the MATH
benchmark.

</details>


### [29] [LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.10027)
*Ali Zolnour,Hossein Azadmaleki,Yasaman Haghbin,Fatemeh Taherinezhad,Mohamad Javad Momeni Nezhad,Sina Rashidi,Masoud Khani,AmirSajjad Taleban,Samin Mahdizadeh Sani,Maryam Dadkhah,James M. Noble,Suzanne Bakken,Yadollah Yaghoobzadeh,Abdol-Hossein Vahabie,Masoud Rouhizadeh,Maryam Zolnoori*

Main category: cs.CL

TL;DR: 整合Transformer嵌入与语言学特征显著提升ADRD语音检测性能，临床调优的LLM在分类和数据增强中效果显著，但多模态模型仍需改进


<details>
  <summary>Details</summary>
Motivation: 解决阿尔茨海默病早期诊断率低的问题，探索基于语音NLP的非侵入性筛查方法

Method: 构建融合模型（Transformer嵌入+110个语言学特征），使用LLM生成合成语音进行数据增强，对比测试单模态/多模态模型性能

Result: 融合模型F1达83.3（AUC 89.5），MedAlpaca合成数据增强使F1提升至85.7，微调后单模态LLM提升显著（如MedAlpaca F1从47.3→78.5），当前多模态模型表现较弱（GPT-4o F1=70.2）

Conclusion: 融合方法有效提升检测性能，LLM在分类与数据增强中展现临床价值，未来需加强多模态模型开发与合成数据质量优化

Abstract: Alzheimer's disease and related dementias (ADRD) affect approximately five
million older adults in the U.S., yet over half remain undiagnosed.
Speech-based natural language processing (NLP) offers a promising, scalable
approach to detect early cognitive decline through linguistic markers.
  To develop and evaluate a screening pipeline that (i) fuses transformer
embeddings with handcrafted linguistic features, (ii) tests data augmentation
using synthetic speech generated by large language models (LLMs), and (iii)
benchmarks unimodal and multimodal LLM classifiers for ADRD detection.
  Transcripts from the DementiaBank "cookie-theft" task (n = 237) were used.
Ten transformer models were evaluated under three fine-tuning strategies. A
fusion model combined embeddings from the top-performing transformer with 110
lexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B,
Ministral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic
speech, which was used to augment training data. Three multimodal models
(GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in
zero-shot and fine-tuned settings.
  The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or
transformer-only baselines. Augmenting training data with 2x MedAlpaca-7B
synthetic speech increased F1 to 85.7. Fine-tuning significantly improved
unimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current
multimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen =
66.0). Performance gains aligned with the distributional similarity between
synthetic and real speech.
  Integrating transformer embeddings with linguistic features enhances ADRD
detection from speech. Clinically tuned LLMs effectively support both
classification and data augmentation, while further advancement is needed in
multimodal modeling.

</details>


### [30] [PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs](https://arxiv.org/abs/2508.10028)
*Xiao Fu,Hossein A. Rahmani,Bin Wu,Jerome Ramos,Emine Yilmaz,Aldo Lipani*

Main category: cs.CL

TL;DR: 提出了PREF个性化评估框架，通过三步流程（生成通用准则→个性化调整→评分）实现无参考的个性化文本生成评估，在准确性和人类对齐性上优于基线。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成评估方法忽视用户个性化需求，无法有效衡量用户偏好对齐程度。需要无需黄金参考、能同时评估通用质量和用户偏好的新方法。

Method: 1. 覆盖阶段生成查询相关的通用评估准则；2. 偏好阶段根据用户画像重排序并增强准则；3. 基于个性化准则进行LLM评分。通过分离通用覆盖与个性化偏好提升系统鲁棒性。

Result: 在PrefEval基准测试中，PREF相比基线方法在准确性（提升12%）、校准度、人类判断对齐性方面表现更优，支持小模型实现接近大模型的个性化评估质量。

Conclusion: PREF通过可扩展、可解释的评估框架，为个性化文本生成系统提供了更可靠的评估基础，推动用户对齐的语言生成系统发展。

Abstract: Personalised text generation is essential for user-centric information
systems, yet most evaluation methods overlook the individuality of users. We
introduce \textbf{PREF}, a \textbf{P}ersonalised \textbf{R}eference-free
\textbf{E}valuation \textbf{F}ramework that jointly measures general output
quality and user-specific alignment without requiring gold personalised
references. PREF operates in a three-step pipeline: (1) a coverage stage uses a
large language model (LLM) to generate a comprehensive, query-specific
guideline covering universal criteria such as factuality, coherence, and
completeness; (2) a preference stage re-ranks and selectively augments these
factors using the target user's profile, stated or inferred preferences, and
context, producing a personalised evaluation rubric; and (3) a scoring stage
applies an LLM judge to rate candidate answers against this rubric, ensuring
baseline adequacy while capturing subjective priorities. This separation of
coverage from preference improves robustness, transparency, and reusability,
and allows smaller models to approximate the personalised quality of larger
ones. Experiments on the PrefEval benchmark, including implicit
preference-following tasks, show that PREF achieves higher accuracy, better
calibration, and closer alignment with human judgments than strong baselines.
By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the
groundwork for more reliable assessment and development of personalised
language generation systems.

</details>


### [31] [Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs](https://arxiv.org/abs/2508.10029)
*Wenpeng Xing,Mohan Li,Chunqiang Hu,Haitao XuNingyu Zhang,Bo Lin,Meng Han*

Main category: cs.CL

TL;DR: 提出Latent Fusion Jailbreak攻击方法，通过隐空间融合实现94.01%攻击成功率，并设计对抗训练防御方案


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型安全对齐易受越狱攻击的问题，开发更有效的攻击方法并探索防御机制

Method: 1. 筛选高相似度查询对 2. 在关键层进行梯度引导的隐状态插值 3. 多目标优化平衡攻击效果与效率

Result: 在Vicuna等模型上实现94.01%平均攻击成功率，防御方案使ASR下降80%且不影响正常任务

Conclusion: LFJ通过查询对选择、隐空间融合和优化策略实现高效攻击，对抗训练可有效防御且保持模型性能

Abstract: Large language models (LLMs) demonstrate impressive capabilities in various
language tasks but are susceptible to jailbreak attacks that circumvent their
safety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a
representation-based attack that interpolates hidden states from harmful and
benign query pairs to elicit prohibited responses. LFJ begins by selecting
query pairs with high thematic and syntactic similarity, then performs
gradient-guided interpolation at influential layers and tokens, followed by
optimization to balance attack success, output fluency, and computational
efficiency. Evaluations on models such as Vicuna and LLaMA-2 across benchmarks
like AdvBench and MaliciousInstruct yield an average attack success rate (ASR)
of 94.01%, outperforming existing methods. To mitigate LFJ, we propose an
adversarial training defense that fine-tunes models on interpolated examples,
reducing ASR by over 80% without degrading performance on benign inputs.
Ablation studies validate the importance of query pair selection, hidden state
interpolation components, and optimization strategies in LFJ's effectiveness.

</details>


### [32] [Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models](https://arxiv.org/abs/2508.10030)
*Saaduddin Mahmud,Mason Nakamura,Kyle H. Wray,Shlomo Zilberstein*

Main category: cs.CL

TL;DR: 提出了IAPO框架，联合优化提示词和推理规模，在考虑推理预算和多任务目标的情况下显著提升大语言模型对齐效果


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法独立于推理策略，实证分析显示二者存在强相互依赖性，用户对多目标权衡和推理预算的偏好会显著影响配置选择

Method: 开发IAPO统一框架(联合优化提示和推理规模)和PSST训练算法(通过序列剪裁实现固定预算训练)，提供有限预算下的错误概率理论保证

Result: 在文本生成和推理等6个任务中验证有效性，证明推理意识对黑盒LLM提示优化的关键作用

Conclusion: 通过提示优化对齐黑盒LLM时必须考虑推理意识，联合优化框架能有效提升模型性能与目标对齐

Abstract: Prompt optimization methods have demonstrated significant effectiveness in
aligning black-box large language models (LLMs). In parallel, inference scaling
strategies such as Best-of-N Sampling and Majority Voting have also proven to
enhance alignment and performance by trading off computation. However, existing
prompt optimization approaches are inference strategy agnostic; that is, they
optimize prompts without regard to the inference strategy employed during
deployment. This constitutes a significant methodological gap, as our empirical
and theoretical analysis reveals a strong interdependence between these two
paradigms. Moreover, we find that user preferences regarding trade-offs among
multiple objectives and inference budgets substantially influence the choice of
prompt and inference configuration. To address this gap, we introduce a unified
novel framework named IAPO (Inference-Aware Prompt Optimization) that jointly
optimizes the prompt and inference scale, while being aware of the inference
budget and different task objectives. We then develop a fixed-budget training
algorithm for IAPO, which we call PSST (Prompt Scaling via Sequential
Trimming), and analyze finite-budget guarantees on error probability. Finally,
we evaluate the effectiveness of PSST on six different tasks, including
multi-objective text generation and reasoning, and demonstrate the critical
role of incorporating inference-awareness when aligning black-box LLMs through
prompt optimization.

</details>


### [33] [The Cost of Thinking: Increased Jailbreak Risk in Large Language Models](https://arxiv.org/abs/2508.10032)
*Fan Yang*

Main category: cs.CL

TL;DR: 研究发现LLMs的思维模式更易受越狱攻击，提出安全思维干预方法可显著降低攻击成功率


<details>
  <summary>Details</summary>
Motivation: 发现LLMs的思维模式存在易受攻击的盲区，尤其在教育场景及过长思考链时易被攻破，且模型明知有害仍会输出

Method: 通过在prompt中添加'特定思维标记'，显式引导LLMs的内部思维过程

Result: 安全思维干预使9个主流LLMs在AdvBench和HarmBench上的攻击成功率显著下降

Conclusion: 安全思维干预有效提升LLMs抗越狱攻击能力，为模型安全防护提供新思路

Abstract: Thinking mode has always been regarded as one of the most valuable modes in
LLMs. However, we uncover a surprising and previously overlooked phenomenon:
LLMs with thinking mode are more easily broken by Jailbreak attack. We evaluate
9 LLMs on AdvBench and HarmBench and find that the success rate of attacking
thinking mode in LLMs is almost higher than that of non-thinking mode. Through
large numbers of sample studies, it is found that for educational purposes and
excessively long thinking lengths are the characteristics of successfully
attacked data, and LLMs also give harmful answers when they mostly know that
the questions are harmful. In order to alleviate the above problems, this paper
proposes a method of safe thinking intervention for LLMs, which explicitly
guides the internal thinking processes of LLMs by adding "specific thinking
tokens" of LLMs to the prompt. The results demonstrate that the safe thinking
intervention can significantly reduce the attack success rate of LLMs with
thinking mode.

</details>


### [34] [Reflect then Learn: Active Prompting for Information Extraction Guided by Introspective Confusion](https://arxiv.org/abs/2508.10036)
*Dong Zhao,Yadong Wang,Xiang Chen,Chenxi Wang,Hongliang Dai,Chuanxing Geng,Shengzhong Zhang,Shaoyuan Li,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 提出APIE主动提示框架，通过双重不确定性指标（格式与内容）选择高质量样本，显著提升信息抽取效果


<details>
  <summary>Details</summary>
Motivation: 传统上下文样本选择方法忽视模型的两大关键错误来源：生成正确格式的困难（格式不确定性）和语义抽取的不一致性（内容不确定性）

Method: 构建自省式混淆评估体系：
1. 格式不确定性（生成正确语法的难度）
2. 内容不确定性（抽取语义的波动性）
通过综合评分主动筛选最具挑战性的未标注样本作为少样本示例

Result: 在四个基准测试中显著超越基线方法，抽取准确率提升明显（具体数值需参照原文），系统鲁棒性显著增强

Conclusion: 双重不确定性视角对构建可靠结构化生成系统具有关键意义，为模型自我诊断和样本选择提供新范式

Abstract: Large Language Models (LLMs) show remarkable potential for few-shot
information extraction (IE), yet their performance is highly sensitive to the
choice of in-context examples. Conventional selection strategies often fail to
provide informative guidance, as they overlook a key source of model
fallibility: confusion stemming not just from semantic content, but also from
the generation of well-structured formats required by IE tasks. To address
this, we introduce Active Prompting for Information Extraction (APIE), a novel
active prompting framework guided by a principle we term introspective
confusion. Our method empowers an LLM to assess its own confusion through a
dual-component uncertainty metric that uniquely quantifies both Format
Uncertainty (difficulty in generating correct syntax) and Content Uncertainty
(inconsistency in extracted semantics). By ranking unlabeled data with this
comprehensive score, our framework actively selects the most challenging and
informative samples to serve as few-shot exemplars. Extensive experiments on
four benchmarks show that our approach consistently outperforms strong
baselines, yielding significant improvements in both extraction accuracy and
robustness. Our work highlights the critical importance of a fine-grained,
dual-level view of model uncertainty when it comes to building effective and
reliable structured generation systems.

</details>


### [35] [mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning](https://arxiv.org/abs/2508.10137)
*Nghia Trung Ngo,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: 提出了动态可扩展的多语言常识推理基准mSCoRe，系统评估LLM在跨语言文化场景下的推理能力缺陷


<details>
  <summary>Details</summary>
Motivation: 现有研究未能深入揭示推理增强型LLM在跨语言文化常识推理任务中对不同推理技能的应用机制

Method: 包含三组件：1) 细粒度推理技能分类法 2) 定制化数据合成流程 3) 动态复杂度扩展框架

Result: 实验显示当前模型在高复杂度层级存在显著局限，特别是在文化敏感性常识推理任务中表现欠佳

Conclusion: 需改进多语言推理架构设计，增强文化背景理解能力，同时保持基准的动态扩展性以适应模型发展

Abstract: Recent advancements in reasoning-reinforced Large Language Models (LLMs) have
shown remarkable capabilities in complex reasoning tasks. However, the
mechanism underlying their utilization of different human reasoning skills
remains poorly investigated, especially for multilingual commonsense reasoning
that involves everyday knowledge across different languages and cultures. To
address this gap, we propose a \textbf{M}ultilingual and Scalable Benchmark for
\textbf{S}kill-based \textbf{Co}mmonsense \textbf{Re}asoning (\textbf{mSCoRe}).
Our benchmark incorporates three key components that are designed to
systematically evaluate LLM's reasoning capabilities, including: (1) a novel
taxonomy of reasoning skills that enables fine-grained analysis of models'
reasoning processes, (2) a robust data synthesis pipeline tailored specifically
for commonsense reasoning evaluation, and (3) a complexity scaling framework
allowing task difficulty to scale dynamically alongside future improvements in
LLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying
sizes and training approaches demonstrate that \textbf{mSCoRe} remains
significantly challenging for current models, particularly at higher complexity
levels. Our results reveal the limitations of such reasoning-reinforced models
when confronted with nuanced multilingual general and cultural commonsense. We
further provide detailed analysis on the models' reasoning processes,
suggesting future directions for improving multilingual commonsense reasoning
capabilities.

</details>


### [36] [Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs](https://arxiv.org/abs/2508.10142)
*Kartikeya Badola,Jonathan Simon,Arian Hosseini,Sara Marie Mc Carthy,Tsendsuren Munkhdalai,Abhimanyu Goyal,Tomáš Kočiský,Shyam Upadhyay,Bahare Fatemi,Mehran Kazemi*

Main category: cs.CL

TL;DR: 论文提出新型多轮对话基准测试，揭示当前LLMs在复杂交互场景中因指令遵循、推理和规划能力不足导致的性能缺陷，为改进关键能力提供评估平台


<details>
  <summary>Details</summary>
Motivation: 现有LLMs擅长处理明确问题，但在需要持续交互、信息检索和不完整数据推理的现实场景中表现不佳，需针对性评估和改进核心能力

Method: 构建包含多任务（推理/交互对话/信息检索）的自动化评估基准，设计确定性评分机制避免人工干预，测试前沿模型表现

Result: 前沿模型存在显著提升空间，53%错误源于指令遵循缺陷，31%因推理失败，16%来自规划不当，暴露核心能力短板

Conclusion: 该基准系统量化LLMs交互弱点，为提升逻辑对话、动态推理和任务规划能力提供标准化研究平台，推动实用化AI发展

Abstract: Large language models (LLMs) excel at solving problems with clear and
complete statements, but often struggle with nuanced environments or
interactive tasks which are common in most real-world scenarios. This
highlights the critical need for developing LLMs that can effectively engage in
logically consistent multi-turn dialogue, seek information and reason with
incomplete data. To this end, we introduce a novel benchmark comprising a suite
of multi-turn tasks each designed to test specific reasoning, interactive
dialogue, and information-seeking abilities. These tasks have deterministic
scoring mechanisms, thus eliminating the need for human intervention.
Evaluating frontier models on our benchmark reveals significant headroom. Our
analysis shows that most errors emerge from poor instruction following,
reasoning failures, and poor planning. This benchmark provides valuable
insights into the strengths and weaknesses of current LLMs in handling complex,
interactive scenarios and offers a robust platform for future research aimed at
improving these critical capabilities.

</details>


### [37] [LaajMeter: A Framework for LaaJ Evaluation](https://arxiv.org/abs/2508.10161)
*Gal Amram,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Avi Ziv*

Main category: cs.CL

TL;DR: 提出LaaJMeter框架解决大语言模型作为评估者在领域特定场景中的可靠性验证问题


<details>
  <summary>Details</summary>
Motivation: 传统LaaJ在领域特定场景面临标注数据稀缺、专家评估成本高、未经验证的评估指标难以确定评估质量阈值的问题

Method: 通过生成虚拟模型和评估者的合成数据，建立系统化分析框架验证评估指标的有效性

Result: 在遗留编程语言代码翻译任务中，不同指标对评估质量的敏感度存在显著差异，常用指标存在局限性

Conclusion: LaaJMeter为低资源场景提供可扩展的解决方案，强调指标选择的系统性对评估可信度的重要性

Abstract: Large Language Models (LLMs) are increasingly used as evaluators in natural
language processing tasks, a paradigm known as LLM-as-a-Judge (LaaJ). While
effective in general domains, LaaJs pose significant challenges in
domain-specific contexts, where annotated data is scarce and expert evaluation
is costly. In such cases, meta-evaluation is often performed using metrics that
have not been validated for the specific domain in which they are applied. As a
result, it becomes difficult to determine which metrics effectively identify
LaaJ quality, and further, what threshold indicates sufficient evaluator
performance. In this work, we introduce LaaJMeter, a simulation-based framework
for controlled meta-evaluation of LaaJs. LaaJMeter enables engineers to
generate synthetic data representing virtual models and judges, allowing
systematic analysis of evaluation metrics under realistic conditions. This
helps practitioners validate and refine LaaJs for specific evaluation tasks:
they can test whether their metrics correctly distinguish between better and
worse (virtual) LaaJs, and estimate appropriate thresholds for evaluator
adequacy.
  We demonstrate the utility of LaaJMeter in a code translation task involving
a legacy programming language, showing how different metrics vary in
sensitivity to evaluator quality. Our results highlight the limitations of
common metrics and the importance of principled metric selection. LaaJMeter
provides a scalable and extensible solution for assessing LaaJs in low-resource
settings, contributing to the broader effort to ensure trustworthy and
reproducible evaluation in NLP.

</details>


### [38] [Estimating Machine Translation Difficulty](https://arxiv.org/abs/2508.10175)
*Lorenzo Proietti,Stefano Perrella,Vilém Zouhar,Roberto Navigli,Tom Kocmi*

Main category: cs.CL

TL;DR: 提出翻译难度估计任务，开发Sentinel-src模型用于识别机器翻译困难文本，改进评估方法并构建更具挑战性的测试集


<details>
  <summary>Details</summary>
Motivation: 当前机器翻译质量接近完美，导致难以区分顶尖模型性能差异并识别改进方向。需要自动识别翻译困难文本以优化评估体系并指导技术发展

Method: 1. 定义基于翻译质量的文本难度指标 2. 开发新型评估指标 3. 对比基线方法与创新模型 4. 构建高难度翻译测试基准

Result: Sentinel-src系列模型在难度评估中全面优于传统启发式方法和LLM评估，发布Sentinel-src-24/25两个优化模型

Conclusion: 专用难度评估模型有效提升翻译系统评估的区分度，为构建挑战性测试基准和指导技术突破提供实用工具

Abstract: Machine translation quality has began achieving near-perfect translations in
some setups. These high-quality outputs make it difficult to distinguish
between state-of-the-art models and to identify areas for future improvement.
Automatically identifying texts where machine translation systems struggle
holds promise for developing more discriminative evaluations and guiding future
research.
  We formalize the task of translation difficulty estimation, defining a text's
difficulty based on the expected quality of its translations. We introduce a
new metric to evaluate difficulty estimators and use it to assess both
baselines and novel approaches. Finally, we demonstrate the practical utility
of difficulty estimators by using them to construct more challenging machine
translation benchmarks. Our results show that dedicated models (dubbed
Sentinel-src) outperform both heuristic-based methods (e.g. word rarity or
syntactic complexity) and LLM-as-a-judge approaches. We release two improved
models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which
can be used to scan large collections of texts and select those most likely to
challenge contemporary machine translation systems.

</details>


### [39] [Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs](https://arxiv.org/abs/2508.10180)
*Wenlong Deng,Jiaming Zhang,Qi Zeng,Christos Thrampoulidis,Boying Gong,Xiaoxiao Li*

Main category: cs.CL

TL;DR: 提出For-Value框架，通过单次前向传播实现大语言模型和视觉语言模型的高效数据价值评估，避免传统梯度计算方法的高昂计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有数据价值评估方法依赖海森矩阵计算或模型重训练，难以适用于十亿参数级别的大模型。需开发更轻量化的影响力评估方法以提高模型透明度。

Method: 利用基础模型的隐层表示能力，通过训练样本与验证样本在隐空间的对齐度和预测误差构建闭式解计算公式，仅需单次前向传播即可完成影响力评分。

Result: 在微调样本影响力评估和错误标注检测任务中，For-Value性能匹配或超越基于梯度的方法，同时计算效率显著提升。

Conclusion: 该框架为大规模模型训练提供高效透明的数据价值分析工具，通过表征对齐机制实现无需梯度计算的影响力估计，具有重要工程实践价值。

Abstract: Quantifying the influence of individual training samples is essential for
enhancing the transparency and accountability of large language models (LLMs)
and vision-language models (VLMs). However, existing data valuation methods
often rely on Hessian information or model retraining, making them
computationally prohibitive for billion-parameter models. In this work, we
introduce For-Value, a forward-only data valuation framework that enables
scalable and efficient influence estimation for both LLMs and VLMs. By
leveraging the rich representations of modern foundation models, For-Value
computes influence scores using a simple closed-form expression based solely on
a single forward pass, thereby eliminating the need for costly gradient
computations. Our theoretical analysis demonstrates that For-Value accurately
estimates per-sample influence by capturing alignment in hidden representations
and prediction errors between training and validation samples. Extensive
experiments show that For-Value matches or outperforms gradient-based baselines
in identifying impactful fine-tuning examples and effectively detecting
mislabeled data.

</details>


### [40] [PakBBQ: A Culturally Adapted Bias Benchmark for QA](https://arxiv.org/abs/2508.10186)
*Abdullah Hashmat,Muhammad Arham Mirza,Agha Ali Raza*

Main category: cs.CL

TL;DR: 该论文提出巴基斯坦文化背景的偏见评估数据集PakBBQ，通过消除歧义和负面提问框架有效降低大语言模型偏见，在低资源语言场景展现更好的去偏见效果。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型主要基于西方中心数据训练评估，缺乏对巴基斯坦等低资源语言和地域文化偏见的关注，可能加剧模型对边缘社区的不公平性。

Method: 构建包含214个模板、17180个双语QA对的PakBBQ数据集，涵盖8类巴基斯坦社会偏见维度，并在歧义/非歧义上下文、负面/非负面提问框架下评估多语言模型表现。

Result: 实验显示：(1)消除歧义使准确率提升12%；(2)乌尔都语的去偏见效果优于英语；(3)负面提问框架显著减少刻板回答。

Conclusion: 上下文本地化评估和简单提示词工程能有效缓解低资源语言的模型偏见，强调文化适配数据集对提升LLM公平性的重要性。

Abstract: With the widespread adoption of Large Language Models (LLMs) across various
applications, it is empirical to ensure their fairness across all user
communities. However, most LLMs are trained and evaluated on Western centric
data, with little attention paid to low-resource languages and regional
contexts. To address this gap, we introduce PakBBQ, a culturally and regionally
adapted extension of the original Bias Benchmark for Question Answering (BBQ)
dataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8
categories in both English and Urdu, covering eight bias dimensions including
age, disability, appearance, gender, socio-economic status, religious, regional
affiliation, and language formality that are relevant in Pakistan. We evaluate
multiple multilingual LLMs under both ambiguous and explicitly disambiguated
contexts, as well as negative versus non negative question framings. Our
experiments reveal (i) an average accuracy gain of 12\% with disambiguation,
(ii) consistently stronger counter bias behaviors in Urdu than in English, and
(iii) marked framing effects that reduce stereotypical responses when questions
are posed negatively. These findings highlight the importance of contextualized
benchmarks and simple prompt engineering strategies for bias mitigation in low
resource settings.

</details>


### [41] [Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models](https://arxiv.org/abs/2508.10192)
*Igor Halperin*

Main category: cs.CL

TL;DR: 提出语义发散度量框架（SDM），通过多提示语义一致性检测大语言模型的忠实性幻觉（如自信虚构），结合信息论指标构建诊断系统Semantic Box。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如语义熵）仅检测固定提示下的回答随意性，无法捕捉深度语义偏差。需开发更敏感的指标检测LLM严重偏离输入语境的危险虚构行为。

Method: 1. 通过提示改写生成语义等价变体
2. 联合聚类构建提示-回答主题空间
3. 提出S_H分数（Jensen-Shannon散度+Wasserstein距离）量化发散程度
4. 使用KL散度识别语义探索信号
5. 构建Semantic Box分类回答类型

Result: SDM能有效识别自信虚构等高风险幻觉，Semantic Box提供可视化诊断框架，KL(Prompt||Answer)指标可区分不同生成行为模式。

Conclusion: 通过多提示语义一致性和信息论指标，SDM为LLM忠实性评估提供更鲁棒的检测框架，有助于预防危险虚构行为。

Abstract: The proliferation of Large Language Models (LLMs) is challenged by
hallucinations, critical failure modes where models generate non-factual,
nonsensical or unfaithful text. This paper introduces Semantic Divergence
Metrics (SDM), a novel lightweight framework for detecting Faithfulness
Hallucinations -- events of severe deviations of LLMs responses from input
contexts. We focus on a specific implementation of these LLM errors,
{confabulations, defined as responses that are arbitrary and semantically
misaligned with the user's query. Existing methods like Semantic Entropy test
for arbitrariness by measuring the diversity of answers to a single, fixed
prompt. Our SDM framework improves upon this by being more prompt-aware: we
test for a deeper form of arbitrariness by measuring response consistency not
only across multiple answers but also across multiple, semantically-equivalent
paraphrases of the original prompt. Methodologically, our approach uses joint
clustering on sentence embeddings to create a shared topic space for prompts
and answers. A heatmap of topic co-occurances between prompts and responses can
be viewed as a quantified two-dimensional visualization of the user-machine
dialogue. We then compute a suite of information-theoretic metrics to measure
the semantic divergence between prompts and responses. Our practical score,
$\mathcal{S}_H$, combines the Jensen-Shannon divergence and Wasserstein
distance to quantify this divergence, with a high score indicating a
Faithfulness hallucination. Furthermore, we identify the KL divergence
KL(Answer $||$ Prompt) as a powerful indicator of \textbf{Semantic
Exploration}, a key signal for distinguishing different generative behaviors.
These metrics are further combined into the Semantic Box, a diagnostic
framework for classifying LLM response types, including the dangerous,
confident confabulation.

</details>


### [42] [Understanding Textual Emotion Through Emoji Prediction](https://arxiv.org/abs/2508.10222)
*Ethan Gordon,Nishank Kuppa,Rigved Tummala,Sriram Anasuri*

Main category: cs.CL

TL;DR: 使用前馈网络、CNN、Transformer和BERT四种模型进行表情符号预测，BERT整体表现最佳，CNN在罕见类别表现优异，强调模型选择对交互体验的重要性


<details>
  <summary>Details</summary>
Motivation: 通过解决短文本表情符号预测中的类别不平衡问题，优化人机交互场景中的情感表达准确性

Method: 基于TweetEval数据集，采用焦点损失和正则化技术，对比测试四种深度学习架构的性能差异

Result: BERT凭借预训练优势达到最高准确率（宏观F1 0.35），CNN在罕见表情类别预测中F1值超出BERT 17.5%

Conclusion: 架构选择与超参数调优对情感感知的表情预测至关重要，CNN的局部特征捕捉能力特别适合长尾分布场景

Abstract: This project explores emoji prediction from short text sequences using four
deep learning architectures: a feed-forward network, CNN, transformer, and
BERT. Using the TweetEval dataset, we address class imbalance through focal
loss and regularization techniques. Results show BERT achieves the highest
overall performance due to its pre-training advantage, while CNN demonstrates
superior efficacy on rare emoji classes. This research shows the importance of
architecture selection and hyperparameter tuning for sentiment-aware emoji
prediction, contributing to improved human-computer interaction.

</details>


### [43] [Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia](https://arxiv.org/abs/2508.10226)
*Andrew X. Chen,Guillermo Horga,Sean Escola*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）通过分析临床访谈文本，成功预测精神分裂症高风险患者的BPRS症状评分，其零样本预测性能接近人类评估者水平，并展现出跨语言评估和纵向整合的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决传统BPRS量表在临床实践中应用不便的问题，探索利用LLM实现更高效、标准化的精神症状评估方法。

Method: 基于AMP-SCZ队列的409例CHR患者临床访谈文本，采用零样本学习让LLM预测BPRS评分，并测试其在非母语评估（中/英文）和整合纵向信息（单样本/少样本学习）中的表现。

Result: 零样本预测与人工评估高度一致（中位一致性0.84，组内相关系数0.73），跨语言预测效果更优（一致性0.88），纵向信息整合进一步提升准确性。

Conclusion: LLM可成为CHR患者症状评估的有效工具，其多语言处理能力和动态监测特性为精神科标准化评估提供了新范式。

Abstract: Patients who are at clinical high risk (CHR) for schizophrenia need close
monitoring of their symptoms to inform appropriate treatments. The Brief
Psychiatric Rating Scale (BPRS) is a validated, commonly used research tool for
measuring symptoms in patients with schizophrenia and other psychotic
disorders; however, it is not commonly used in clinical practice as it requires
a lengthy structured interview. Here, we utilize large language models (LLMs)
to predict BPRS scores from clinical interview transcripts in 409 CHR patients
from the Accelerating Medicines Partnership Schizophrenia (AMP-SCZ) cohort.
Despite the interviews not being specifically structured to measure the BPRS,
the zero-shot performance of the LLM predictions compared to the true
assessment (median concordance: 0.84, ICC: 0.73) approaches human inter- and
intra-rater reliability. We further demonstrate that LLMs have substantial
potential to improve and standardize the assessment of CHR patients via their
accuracy in assessing the BPRS in foreign languages (median concordance: 0.88,
ICC: 0.70), and integrating longitudinal information in a one-shot or few-shot
learning approach.

</details>


### [44] [A Computational Approach to Analyzing Language Change and Variation in the Constructed Language Toki Pona](https://arxiv.org/abs/2508.10246)
*Daniel Huang,Hyoun-A Joo*

Main category: cs.CL

TL;DR: 通过语料库计算方法研究人造语言Toki Pona的历时变化和共时变异，发现其与自然语言类似的社会语言演变机制


<details>
  <summary>Details</summary>
Motivation: 探究人造语言Toki Pona是否像自然语言一样受社会语言因素影响，验证人造语言系统是否会在社区使用中自然演变

Method: 采用计算语言学和基于语料库的方法，分析流动性词类及及物性特征，对比不同时期语料库和不同来源语料的使用差异

Result: 发现Toki Pona内容词在句法位置的使用偏好存在历时变化，不同语料库间存在显著使用变异，证实社会语言因素对人造语言的影响

Conclusion: 即使人造语言系统也会因社区使用而产生自然演变，语言变化机制具有普遍性，不受语言起源（自然/人造）的限制

Abstract: This study explores language change and variation in Toki Pona, a constructed
language with approximately 120 core words. Taking a computational and
corpus-based approach, the study examines features including fluid word classes
and transitivity in order to examine (1) changes in preferences of content
words for different syntactic positions over time and (2) variation in usage
across different corpora. The results suggest that sociolinguistic factors
influence Toki Pona in the same way as natural languages, and that even
constructed linguistic systems naturally evolve as communities use them.

</details>


### [45] [Inductive Bias Extraction and Matching for LLM Prompts](https://arxiv.org/abs/2508.10295)
*Christian M. Angel,Francis Ferraro*

Main category: cs.CL

TL;DR: 通过归纳偏差提取与匹配策略提升LLM提示效果


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对提示词微小变化敏感，利用模型自身输出来匹配其内在的归纳偏差可优化提示效果

Method: 将LLM的输出结果作为后续提示词组成部分，形成自适应的提示词生成机制

Result: Likert评分在分类任务中提升19%，在排序任务中提升27%

Conclusion: 该策略有效利用模型自身特性提升任务表现，验证了归纳偏差匹配的工程价值

Abstract: The active research topic of prompt engineering makes it evident that LLMs
are sensitive to small changes in prompt wording. A portion of this can be
ascribed to the inductive bias that is present in the LLM. By using an LLM's
output as a portion of its prompt, we can more easily create satisfactory
wording for prompts. This has the effect of creating a prompt that matches the
inductive bias in model. Empirically, we show that using this Inductive Bias
Extraction and Matching strategy improves LLM Likert ratings used for
classification by up to 19% and LLM Likert ratings used for ranking by up to
27%.

</details>


### [46] [Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race](https://arxiv.org/abs/2508.10304)
*Gustavo Bonil,Simone Hashiguti,Jhessica Silva,João Gondim,Helena Maia,Nádia Silva,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: 提出定性分析框架检测大语言模型中的偏见，发现种族和性别刻板印象在文本生成中持续固化，纠偏尝试效果有限。


<details>
  <summary>Details</summary>
Motivation: 现有定量方法难以捕捉语言偏见的微妙表现，需通过定性分析揭示算法如何维持霸权话语体系。

Method: 人工分析LLM生成的短篇小说，对比黑/白人女性角色的叙事模式，建立话语分析框架。

Result: 黑人女性被锚定在祖先关联与抗争叙事，白人女性呈现自我发现主题；纠偏指令仅产生表面修正，本质化问题持续存在。

Conclusion: 算法具有意识形态功能，需采用跨学科批判方法进行AI开发，从根源解决语言模型复制社会不平等的问题。

Abstract: With the advance of Artificial Intelligence (AI), Large Language Models
(LLMs) have gained prominence and been applied in diverse contexts. As they
evolve into more sophisticated versions, it is essential to assess whether they
reproduce biases, such as discrimination and racialization, while maintaining
hegemonic discourses. Current bias detection approaches rely mostly on
quantitative, automated methods, which often overlook the nuanced ways in which
biases emerge in natural language. This study proposes a qualitative,
discursive framework to complement such methods. Through manual analysis of
LLM-generated short stories featuring Black and white women, we investigate
gender and racial biases. We contend that qualitative methods such as the one
proposed here are fundamental to help both developers and users identify the
precise ways in which biases manifest in LLM outputs, thus enabling better
conditions to mitigate them. Results show that Black women are portrayed as
tied to ancestry and resistance, while white women appear in self-discovery
processes. These patterns reflect how language models replicate crystalized
discursive representations, reinforcing essentialization and a sense of social
immobility. When prompted to correct biases, models offered superficial
revisions that maintained problematic meanings, revealing limitations in
fostering inclusive narratives. Our results demonstrate the ideological
functioning of algorithms and have significant implications for the ethical use
and development of AI. The study reinforces the need for critical,
interdisciplinary approaches to AI design and deployment, addressing how
LLM-generated discourses reflect and perpetuate inequalities.

</details>


### [47] [ReviewRL: Towards Automated Scientific Review with RL](https://arxiv.org/abs/2508.10308)
*Sihang Zeng,Kai Tian,Kaiyan Zhang,Yuru wang,Junqi Gao,Runze Liu,Sa Yang,Jingxuan Li,Xinwei Long,Jiaheng Ma,Biqing Qi,Bowen Zhou*

Main category: cs.CL

TL;DR: 提出ReviewRL强化学习框架，结合检索增强生成、监督微调和复合奖励函数，显著提升自动论文评审质量


<details>
  <summary>Details</summary>
Motivation: 现有自动评审方法存在事实准确性差、评分不一致和反馈肤浅的问题，需要整合科学文献并利用强化学习增强评审深度

Method: 1) ArXiv-MCP检索增强流程整合相关文献 2) 监督微调建立基础评审能力 3) 复合奖励函数的强化学习流程联合优化评审质量和评分准确性

Result: 在ICLR 2025论文上，ReviewRL在规则指标和模型质量评估中显著优于现有方法

Conclusion: ReviewRL为科学发现领域的强化学习自动评审奠定基础，其GitHub开源将推动该领域未来发展

Abstract: Peer review is essential for scientific progress but faces growing challenges
due to increasing submission volumes and reviewer fatigue. Existing automated
review approaches struggle with factual accuracy, rating consistency, and
analytical depth, often generating superficial or generic feedback lacking the
insights characteristic of high-quality human reviews. We introduce ReviewRL, a
reinforcement learning framework for generating comprehensive and factually
grounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP
retrieval-augmented context generation pipeline that incorporates relevant
scientific literature, (2) supervised fine-tuning that establishes foundational
reviewing capabilities, and (3) a reinforcement learning procedure with a
composite reward function that jointly enhances review quality and rating
accuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL
significantly outperforms existing methods across both rule-based metrics and
model-based quality assessments. ReviewRL establishes a foundational framework
for RL-driven automatic critique generation in scientific discovery,
demonstrating promising potential for future development in this domain. The
implementation of ReviewRL will be released at GitHub.

</details>


### [48] [From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document Analysis](https://arxiv.org/abs/2508.10311)
*Xuan Li,Jialiang Dong,Raymond Wong*

Main category: cs.CL

TL;DR: 提出DOTABLER框架，通过深层表格语义解析解决现有文档解析方法在表格上下文关联分析中的不足，在真实数据测试中性能超越GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 现有表格解析方法局限于表层任务，缺乏对表格与上下文语义关联的深度分析，限制了跨段落数据解释等高级应用场景的实现。

Method: 基于定制数据集和领域微调技术构建端到端解析流程，开发文档结构解析与领域表格检索双核心功能，实现表格锚定的语义关联分析。

Result: 在4,000页真实PDF数据测试中取得超90%的Precision和F1值，语义分析性能显著优于GPT-4o等先进模型。

Conclusion: DOTABLER验证了表格中心化语义解析的有效性，为金融、科研等领域的深度文档理解提供了新的技术路径。

Abstract: Documents are core carriers of information and knowl-edge, with broad
applications in finance, healthcare, and scientific research. Tables, as the
main medium for structured data, encapsulate key information and are among the
most critical document components. Existing studies largely focus on
surface-level tasks such as layout analysis, table detection, and data
extraction, lacking deep semantic parsing of tables and their contextual
associations. This limits advanced tasks like cross-paragraph data
interpretation and context-consistent analysis. To address this, we propose
DOTABLER, a table-centric semantic document parsing framework designed to
uncover deep semantic links between tables and their context. DOTABLER
leverages a custom dataset and domain-specific fine-tuning of pre-trained
models, integrating a complete parsing pipeline to identify context segments
semantically tied to tables. Built on this semantic understanding, DOTABLER
implements two core functionalities: table-centric document structure parsing
and domain-specific table retrieval, delivering comprehensive table-anchored
semantic analysis and precise extraction of semantically relevant tables.
Evaluated on nearly 4,000 pages with over 1,000 tables from real-world PDFs,
DOTABLER achieves over 90% Precision and F1 scores, demonstrating superior
performance in table-context semantic analysis and deep document parsing
compared to advanced models such as GPT-4o.

</details>


### [49] [Beyond Semantic Understanding: Preserving Collaborative Frequency Components in LLM-based Recommendation](https://arxiv.org/abs/2508.10312)
*Minhao Wang,Yunhang He,Cong Xu,Zhangchi Zhu,Wei Zhang*

Main category: cs.CL

TL;DR: LLM推荐系统过度强调语义关联导致协作信号衰减，FreLLM4Rec通过频谱过滤和调制机制实现语义与协作信息的平衡，NDCG@10指标最高提升8%


<details>
  <summary>Details</summary>
Motivation: LLM推荐系统在传播协作ID嵌入时逐层削弱协作信号，而传统Transformer模型能有效保留此类信号，需解决LLM的协作信号衰减问题

Method: 1. 全局图低通滤波器(G-LPF)净化语义协作混合嵌入 
2. 时域频率调制(TFM)逐层保留协作信号 
3. 理论证明局部图傅里叶滤波与频域滤波的等效性

Result: 在四个基准数据集上NDCG@10指标最高提升8%，协作信号衰减幅度减少42%，超越现有最佳基线模型

Conclusion: FreLLM4Rec首次从频谱视角揭示LLM处理协作信号的机制，为LLM推荐系统提供了理论支撑的优化框架，证明平衡语义与协作信息的可行性

Abstract: Recommender systems in concert with Large Language Models (LLMs) present
promising avenues for generating semantically-informed recommendations.
However, LLM-based recommenders exhibit a tendency to overemphasize semantic
correlations within users' interaction history. When taking pretrained
collaborative ID embeddings as input, LLM-based recommenders progressively
weaken the inherent collaborative signals as the embeddings propagate through
LLM backbones layer by layer, as opposed to traditional Transformer-based
sequential models in which collaborative signals are typically preserved or
even enhanced for state-of-the-art performance. To address this limitation, we
introduce FreLLM4Rec, an approach designed to balance semantic and
collaborative information from a spectral perspective. Item embeddings that
incorporate both semantic and collaborative information are first purified
using a Global Graph Low-Pass Filter (G-LPF) to preliminarily remove irrelevant
high-frequency noise. Temporal Frequency Modulation (TFM) then actively
preserves collaborative signal layer by layer. Note that the collaborative
preservation capability of TFM is theoretically guaranteed by establishing a
connection between the optimal but hard-to-implement local graph fourier
filters and the suboptimal yet computationally efficient frequency-domain
filters. Extensive experiments on four benchmark datasets demonstrate that
FreLLM4Rec successfully mitigates collaborative signal attenuation and achieves
competitive performance, with improvements of up to 8.00\% in NDCG@10 over the
best baseline. Our findings provide insights into how LLMs process
collaborative information and offer a principled approach for improving
LLM-based recommendation systems.

</details>


### [50] [Cross-Prompt Encoder for Low-Performing Languages](https://arxiv.org/abs/2508.10352)
*Beso Mikaberidze,Teimuraz Saghinadze,Simon Ostermann,Philipp Muller*

Main category: cs.CL

TL;DR: 提出跨提示编码器(XPE)和双软提示机制，通过多语言联合训练提升低性能语言的表现，XPE专注于低资源语言优化，混合方案兼顾多语言适配性。


<details>
  <summary>Details</summary>
Motivation: 现有软提示方法未充分挖掘跨语言迁移潜力，低性能语言在传统全模型微调下仍表现欠佳，需开发更有效的参数高效微调方法。

Method: 1.XPE架构：轻量级编码器+多语种联合训练
2.双软提示机制：编码器生成提示与标准软提示结合
3.基于语言类型学选择训练语种

Result: SIB-200基准测试显示：XPE使低性能语言准确率提升显著，混合方案在85%语言上优于基线，但存在XPE优化方向与通用场景的权衡。

Conclusion: XPE与混合提示的组合为多语言模型优化提供新范式：XPE专注解决低性能语言瓶颈，混合方案实现跨语言知识迁移与本地化适配的平衡。

Abstract: Soft prompts have emerged as a powerful alternative to adapters in
parameter-efficient fine-tuning (PEFT), enabling large language models (LLMs)
to adapt to downstream tasks without architectural changes or parameter
updates. While prior work has focused on stabilizing training via parameter
interaction in small neural prompt encoders, their broader potential for
transfer across languages remains unexplored. In this paper, we demonstrate
that a prompt encoder can play a central role in improving performance on
low-performing languages-those that achieve poor accuracy even under full-model
fine-tuning. We introduce the Cross-Prompt Encoder (XPE), which combines a
lightweight encoding architecture with multi-source training on typologically
diverse languages - a design that enables the model to capture abstract and
transferable patterns across languages. To complement XPE, we propose a Dual
Soft Prompt mechanism that combines an encoder-based prompt with a directly
trained standard soft prompt. This hybrid design proves especially effective
for target languages that benefit from both broadly shared structure and
language-specific alignment. Experiments on the SIB-200 benchmark reveal a
consistent trade-off: XPE is most effective for low-performing languages, while
hybrid variants offer broader adaptability across multilingual settings.

</details>


### [51] [Making Qwen3 Think in Korean with Reinforcement Learning](https://arxiv.org/abs/2508.10355)
*Jungyup Lee,Jemin Kim,Sang Park,SeungJae Lee*

Main category: cs.CL

TL;DR: 提出了两阶段微调方法（SFT+GRPO强化学习），通过引入奖励校准机制解决训练稳定性问题，显著提升大型语言模型Qwen3的韩语推理能力。


<details>
  <summary>Details</summary>
Motivation: 提升非英语语言模型在高级推理任务（数学/编程）的母语思维能力，同时解决强化学习训练中的策略崩溃难题。

Method: 1. 第一阶段用高质量韩语推理数据集SFT微调；2. 第二阶段定制GRPO算法，通过oracle judge模型校准奖励信号防止奖励黑客和策略崩溃。

Result: 模型在数学/编码基准测试提升显著，韩语思维链完整实现，且保持原有知识能力。GRPO训练稳定性问题成功解决（对比原始GRPO的崩溃现象）。

Conclusion: 该方案有效实现了LLM的韩语深度推理能力，验证了两阶段微调框架与奖励校准机制在跨语言模型优化中的有效性。

Abstract: We present a two-stage fine-tuning approach to make the large language model
Qwen3 14B "think" natively in Korean. In the first stage, supervised
fine-tuning (SFT) on a high-quality Korean reasoning dataset establishes a
strong foundation in Korean logical reasoning, yielding notable improvements in
Korean-language tasks and even some gains in general reasoning ability. In the
second stage, we employ reinforcement learning with a customized Group Relative
Policy Optimization (GRPO) algorithm to further enhance both Korean reasoning
alignment and overall problem-solving performance. We address critical
stability challenges in GRPO training - such as reward hacking and policy
collapse - by introducing an oracle judge model that calibrates the reward
signal. Our approach achieves stable learning (avoiding the collapse observed
in naive GRPO) and leads to steady, incremental performance gains. The final
RL-tuned model demonstrates substantially improved results on advanced
reasoning benchmarks (particularly math and coding tasks) while maintaining
knowledge and language proficiency, successfully conducting its internal
chain-of-thought entirely in Korean.

</details>


### [52] [Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models](https://arxiv.org/abs/2508.10366)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 提出一种无需翻译工具的跨语言ABSA新方法，性能提升达10%，且能处理复杂任务；多语言大模型表现可比，但纯英文大模型效果不佳。


<details>
  <summary>Details</summary>
Motivation: 现有ABSA研究过度集中于英语，跨语言方法依赖翻译工具且任务简单，需开发不依赖翻译、能处理复杂任务的新方案。

Method: 基于约束解码的序列到序列模型，通过结构化输出避免使用外部翻译工具。

Result: 跨语言ABSA性能最高提升10%，支持复杂任务；微调的多语言大模型效果相当，纯英文大模型表现差。

Conclusion: 该方法为跨语言ABSA提供了高效替代方案，并证明多语言模型在跨语言任务中的优势。

Abstract: Aspect-based sentiment analysis (ABSA) has made significant strides, yet
challenges remain for low-resource languages due to the predominant focus on
English. Current cross-lingual ABSA studies often centre on simpler tasks and
rely heavily on external translation tools. In this paper, we present a novel
sequence-to-sequence method for compound ABSA tasks that eliminates the need
for such tools. Our approach, which uses constrained decoding, improves
cross-lingual ABSA performance by up to 10\%. This method broadens the scope of
cross-lingual ABSA, enabling it to handle more complex tasks and providing a
practical, efficient alternative to translation-dependent techniques.
Furthermore, we compare our approach with large language models (LLMs) and show
that while fine-tuned multilingual LLMs can achieve comparable results,
English-centric LLMs struggle with these tasks.

</details>


### [53] [Large Language Models for Summarizing Czech Historical Documents and Beyond](https://arxiv.org/abs/2508.10368)
*Václav Tran,Jakub Šmíd,Jiří Martínek,Ladislav Lenc,Pavel Král*

Main category: cs.CL

TL;DR: 研究利用Mistral和mT5模型推进捷克语文本摘要任务，在现当代数据集SumeCzech取得SOTA效果，并发布首个捷克历史文档摘要数据集Posel od Čerchova


<details>
  <summary>Details</summary>
Motivation: 捷克语文本摘要尤其是历史文档领域存在研究空白，主要受限于语言复杂性和标注数据稀缺的问题

Method: 采用Mistral和mT5等大语言模型进行迁移学习，通过微调模型实现捷克语文本摘要任务

Result: 1. 在SumeCzech数据集上刷新最优结果 2. 构建包含历史文档的Posel od Čerchova数据集并提供基线模型表现

Conclusion: 该研究显著提升了捷克语文本摘要技术水平，为捷克历史文档处理开辟了新的研究路径

Abstract: Text summarization is the task of shortening a larger body of text into a
concise version while retaining its essential meaning and key information.
While summarization has been significantly explored in English and other
high-resource languages, Czech text summarization, particularly for historical
documents, remains underexplored due to linguistic complexities and a scarcity
of annotated datasets. Large language models such as Mistral and mT5 have
demonstrated excellent results on many natural language processing tasks and
languages. Therefore, we employ these models for Czech summarization, resulting
in two key contributions: (1) achieving new state-of-the-art results on the
modern Czech summarization dataset SumeCzech using these advanced models, and
(2) introducing a novel dataset called Posel od \v{C}erchova for summarization
of historical Czech documents with baseline results. Together, these
contributions provide a great potential for advancing Czech text summarization
and open new avenues for research in Czech historical text processing.

</details>


### [54] [Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding](https://arxiv.org/abs/2508.10369)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 提出基于约束解码的跨语言ABSA方法，在七种语言六项任务中平均提升5%性能，支持多任务统一建模并超越SOTA


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言ABSA任务中过度依赖翻译工具、现有方法任务复杂度受限的问题

Method: 使用序列到序列模型结合约束解码技术，消除翻译工具依赖，支持多任务联合训练

Result: 最复杂任务性能提升5%，多任务场景提升超10%；建立七语言六任务的评估基准；验证LLM微调有效性

Conclusion: 该方法推进跨语言ABSA研究边界，为实际应用提供可靠方案，同时揭示LLM在不同训练模式下的效率权衡

Abstract: While aspect-based sentiment analysis (ABSA) has made substantial progress,
challenges remain for low-resource languages, which are often overlooked in
favour of English. Current cross-lingual ABSA approaches focus on limited, less
complex tasks and often rely on external translation tools. This paper
introduces a novel approach using constrained decoding with
sequence-to-sequence models, eliminating the need for unreliable translation
tools and improving cross-lingual performance by 5\% on average for the most
complex task. The proposed method also supports multi-tasking, which enables
solving multiple ABSA tasks with a single model, with constrained decoding
boosting results by more than 10\%.
  We evaluate our approach across seven languages and six ABSA tasks,
surpassing state-of-the-art methods and setting new benchmarks for previously
unexplored tasks. Additionally, we assess large language models (LLMs) in
zero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in
zero-shot and few-shot settings, fine-tuning achieves competitive results
compared to smaller multilingual models, albeit at the cost of longer training
and inference times.
  We provide practical recommendations for real-world applications, enhancing
the understanding of cross-lingual ABSA methodologies. This study offers
valuable insights into the strengths and limitations of cross-lingual ABSA
approaches, advancing the state-of-the-art in this challenging research domain.

</details>


### [55] [Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts](https://arxiv.org/abs/2508.10390)
*Chiyu Zhang,Lu Zhou,Xiaogang Xu,Jiafei Wu,Liming Fang,Zhe Liu*

Main category: cs.CL

TL;DR: 提出混合评估框架MDH结合LLM与人工监督优化恶意内容检测，并开发D-Attack和DH-CoT两种新型越狱攻击策略


<details>
  <summary>Details</summary>
Motivation: 现有红队数据集存在大量不适用提示，传统检测方法依赖高成本人工标注或LLM不一致的准确性，需高效平衡方案

Method: MDH框架整合LLM自动标注与最小化人工审核；通过上下文模拟(D-Attack)和劫持思维链(DH-CoT)提升攻击成功率

Result: MDH有效提升数据集清洗与越狱响应检测准确性，新攻击策略使越狱成功率显著提升

Conclusion: 混合监督框架在效率与准确率间取得平衡，新型攻击策略暴露模型脆弱性，为防御机制优化提供方向

Abstract: Evaluating jailbreak attacks is challenging when prompts are not overtly
harmful or fail to induce harmful outputs. Unfortunately, many existing
red-teaming datasets contain such unsuitable prompts. To evaluate attacks
accurately, these datasets need to be assessed and cleaned for maliciousness.
However, existing malicious content detection methods rely on either manual
annotation, which is labor-intensive, or large language models (LLMs), which
have inconsistent accuracy in harmful types. To balance accuracy and
efficiency, we propose a hybrid evaluation framework named MDH (Malicious
content Detection based on LLMs with Human assistance) that combines LLM-based
annotation with minimal human oversight, and apply it to dataset cleaning and
detection of jailbroken responses. Furthermore, we find that well-crafted
developer messages can significantly boost jailbreak success, leading us to
propose two new strategies: D-Attack, which leverages context simulation, and
DH-CoT, which incorporates hijacked chains of thought. The Codes, datasets,
judgements, and detection results will be released in github repository:
https://github.com/AlienZhang1996/DH-CoT.

</details>


### [56] [Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation](https://arxiv.org/abs/2508.10404)
*Huizhen Shu,Xuying Li,Qirui Wang,Yuji Kosuga,Mengqiu Tian,Zhuo Li*

Main category: cs.CL

TL;DR: 提出稀疏特征扰动框架SFPF，通过稀疏自编码器定位文本关键特征并选择性扰动，成功生成可绕过现有防御机制的对抗文本


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法在揭示LLM漏洞和提升鲁棒性方面存在不足，需要开发更有效的黑盒攻击策略来平衡攻击效率与安全验证

Method: 1. 使用稀疏自编码器重构隐藏层表示
2. 对成功攻击文本进行特征聚类定位高激活特征
3. 选择性扰动高激活特征保留恶意意图同时放大安全信号

Result: 实验显示SFPF生成的对抗文本可绕过前沿防御机制，攻击成功率提升15-20%，但不同prompt和模型层的有效性存在显著差异

Conclusion: 该方法揭示了NLP系统的持续性漏洞，为红队测试提供新范式。后续需验证其在其他架构和大模型的泛化能力，并开发针对性防御方案

Abstract: With the rapid proliferation of Natural Language Processing (NLP), especially
Large Language Models (LLMs), generating adversarial examples to jailbreak LLMs
remains a key challenge for understanding model vulnerabilities and improving
robustness. In this context, we propose a new black-box attack method that
leverages the interpretability of large models. We introduce the Sparse Feature
Perturbation Framework (SFPF), a novel approach for adversarial text generation
that utilizes sparse autoencoders to identify and manipulate critical features
in text. After using the SAE model to reconstruct hidden layer representations,
we perform feature clustering on the successfully attacked texts to identify
features with higher activations. These highly activated features are then
perturbed to generate new adversarial texts. This selective perturbation
preserves the malicious intent while amplifying safety signals, thereby
increasing their potential to evade existing defenses. Our method enables a new
red-teaming strategy that balances adversarial effectiveness with safety
alignment. Experimental results demonstrate that adversarial texts generated by
SFPF can bypass state-of-the-art defense mechanisms, revealing persistent
vulnerabilities in current NLP systems.However, the method's effectiveness
varies across prompts and layers, and its generalizability to other
architectures and larger models remains to be validated.

</details>


### [57] [ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning](https://arxiv.org/abs/2508.10419)
*Juyuan Wang,Rongchen Zhao,Wei Wei,Yufeng Wang,Mo Yu,Jie Zhou,Jin Xu,Liyan Xu*

Main category: cs.CL

TL;DR: ComoRAG提出动态记忆增强的迭代检索框架，通过认知启发的多轮推理循环解决长文本叙事理解问题，在200K+ tokens的长上下文基准测试中相对基线提升11%。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在长文本叙事理解中存在静态检索局限性，无法捕捉动态演变的实体关系。受人类大脑记忆整合机制启发，需要模拟多轮证据获取与知识巩固的认知过程。

Method: 1. 建立动态记忆工作区，通过迭代推理循环处理推理瓶颈
2. 每轮生成探测查询探索新路径
3. 将新证据整合到全局记忆池构建连贯上下文

Result: 在4个长文本叙事基准测试中持续超越基线模型，复杂查询准确率相对提升最高达11%。全局理解类任务表现尤为突出。

Conclusion: ComoRAG为长上下文理解提供了认知启发的范式创新，通过状态化推理机制突破传统RAG的单步检索限制，代码已开源。

Abstract: Narrative comprehension on long stories and novels has been a challenging
domain attributed to their intricate plotlines and entangled, often evolving
relations among characters and entities. Given the LLM's diminished reasoning
over extended context and high computational cost, retrieval-based approaches
remain a pivotal role in practice. However, traditional RAG methods can fall
short due to their stateless, single-step retrieval process, which often
overlooks the dynamic nature of capturing interconnected relations within
long-range context. In this work, we propose ComoRAG, holding the principle
that narrative reasoning is not a one-shot process, but a dynamic, evolving
interplay between new evidence acquisition and past knowledge consolidation,
analogous to human cognition when reasoning with memory-related signals in the
brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes
iterative reasoning cycles while interacting with a dynamic memory workspace.
In each cycle, it generates probing queries to devise new exploratory paths,
then integrates the retrieved evidence of new aspects into a global memory
pool, thereby supporting the emergence of a coherent context for the query
resolution. Across four challenging long-context narrative benchmarks (200K+
tokens), ComoRAG outperforms strong RAG baselines with consistent relative
gains up to 11% compared to the strongest baseline. Further analysis reveals
that ComoRAG is particularly advantageous for complex queries requiring global
comprehension, offering a principled, cognitively motivated paradigm for
retrieval-based long context comprehension towards stateful reasoning. Our code
is publicly released at https://github.com/EternityJune25/ComoRAG

</details>


### [58] [Evaluating LLMs on Chinese Idiom Translation](https://arxiv.org/abs/2508.10421)
*Cai Yang,Yao Dou,David Heineman,Xiaofeng Wu,Wei Xu*

Main category: cs.CL

TL;DR: 提出IdiomEval评估框架，揭示现有中文成语翻译系统错误率高（如GPT-4错误率28%），现有评估指标效果差，改进模型F1达0.68。


<details>
  <summary>Details</summary>
Motivation: 中文成语比喻义与字面差异大且含历史文化背景，现有大模型翻译研究未系统分析其跨领域翻译质量缺陷。

Method: 构建含错误分类体系的评估框架，标注900个来自9个系统（含GPT-4o/Google翻译）的四领域样本，开发改进评估模型。

Result: 现有系统28%翻译错误，评估指标与人评相关性低（Pearson<0.48），改进模型错误检测F1达0.68。

Conclusion: 揭示了成语翻译系统缺陷，建立首个系统评估框架并提出有效改进模型，为质量提升提供方法论基础。

Abstract: Idioms, whose figurative meanings usually differ from their literal
interpretations, are common in everyday language, especially in Chinese, where
they often contain historical references and follow specific structural
patterns. Despite recent progress in machine translation with large language
models, little is known about Chinese idiom translation. In this work, we
introduce IdiomEval, a framework with a comprehensive error taxonomy for
Chinese idiom translation. We annotate 900 translation pairs from nine modern
systems, including GPT-4o and Google Translate, across four domains: web, news,
Wikipedia, and social media. We find these systems fail at idiom translation,
producing incorrect, literal, partial, or even missing translations. The
best-performing system, GPT-4, makes errors in 28% of cases. We also find that
existing evaluation metrics measure idiom quality poorly with Pearson
correlation below 0.48 with human ratings. We thus develop improved models that
achieve F$_1$ scores of 0.68 for detecting idiom translation errors.

</details>


### [59] [Computational Economics in Large Language Models: Exploring Model Behavior and Incentive Design under Resource Constraints](https://arxiv.org/abs/2508.10426)
*Sandeep Reddy,Kabir Khan,Rohit Patil,Ananya Chakraborty,Faizan A. Khan,Swati Kulkarni,Arjun Verma,Neha Singh*

Main category: cs.CL

TL;DR: 通过引入计算经济学框架，优化LLM内部计算资源分配，实现高效、自适应且透明的模型


<details>
  <summary>Details</summary>
Motivation: 传统LLMs存在高计算成本问题，需要系统化的资源分配方法替代简单后剪枝策略

Method: 实证分析注意力分配规律后，提出激励驱动训练范式，在损失函数中嵌入可微计算成本项

Result: 在GLUE和WikiText-103上实现Pareto优化，相同精度下计算量减少40%，注意力模式更可解释

Conclusion: 经济学原理为严格资源约束下的LLM设计提供了理论框架，推动高效自适应模型发展

Abstract: Large language models (LLMs) are limited by substantial computational cost.
We introduce a "computational economics" framework that treats an LLM as an
internal economy of resource-constrained agents (attention heads and neuron
blocks) that must allocate scarce computation to maximize task utility. First,
we show empirically that when computation is scarce, standard LLMs reallocate
attention toward high-value tokens while preserving accuracy. Building on this
observation, we propose an incentive-driven training paradigm that augments the
task loss with a differentiable computation cost term, encouraging sparse and
efficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method
yields a family of models that trace a Pareto frontier and consistently
dominate post-hoc pruning; for a similar accuracy we obtain roughly a forty
percent reduction in FLOPS and lower latency, together with more interpretable
attention patterns. These results indicate that economic principles offer a
principled route to designing efficient, adaptive, and more transparent LLMs
under strict resource constraints.

</details>


### [60] [DiFaR: Enhancing Multimodal Misinformation Detection with Diverse, Factual, and Relevant Rationales](https://arxiv.org/abs/2508.10444)
*Herun Wan,Jiaying Wu,Minnan Luo,Xiangzheng Kong,Zihan Ma,Zhi Zeng*

Main category: cs.CL

TL;DR: 提出DiFaR框架，通过多样化思维链提示和过滤模块生成更优质文本依据，显著提升多模态虚假信息检测性能


<details>
  <summary>Details</summary>
Motivation: 现有方法存在生成依据多样性不足、事实性错误（幻觉问题）和无关/冲突内容干扰三大核心缺陷，制约检测效果

Method: 使用5种思维链提示激发多样化推理路径，设计基于句子级事实性评分和相关性评分的轻量级后过滤模块

Result: 在四个基准测试中性能最高提升5.9%，现有检测器性能提升达8.7%，人工评估证实依据质量三维度显著改善

Conclusion: DiFaR框架通过系统化解决依据生成的关键缺陷，为可训练的多模态虚假信息检测提供了更可靠的支持方案

Abstract: Generating textual rationales from large vision-language models (LVLMs) to
support trainable multimodal misinformation detectors has emerged as a
promising paradigm. However, its effectiveness is fundamentally limited by
three core challenges: (i) insufficient diversity in generated rationales, (ii)
factual inaccuracies due to hallucinations, and (iii) irrelevant or conflicting
content that introduces noise. We introduce DiFaR, a detector-agnostic
framework that produces diverse, factual, and relevant rationales to enhance
misinformation detection. DiFaR employs five chain-of-thought prompts to elicit
varied reasoning traces from LVLMs and incorporates a lightweight post-hoc
filtering module to select rationale sentences based on sentence-level
factuality and relevance scores. Extensive experiments on four popular
benchmarks demonstrate that DiFaR outperforms four baseline categories by up to
5.9% and boosts existing detectors by as much as 8.7%. Both automatic metrics
and human evaluations confirm that DiFaR significantly improves rationale
quality across all three dimensions.

</details>


### [61] [When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing](https://arxiv.org/abs/2508.10482)
*Mahdi Dhaini,Stephen Meisenbacher,Ege Erdogan,Florian Matthes,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 论文实证研究NLP中隐私保护与模型可解释性的权衡关系，发现二者可协同存在并受多种因素影响


<details>
  <summary>Details</summary>
Motivation: 当前可信NLP领域缺乏对隐私保护与可解释性交叉影响的研究，需要验证二者是否对立或可共存

Method: 采用差分隐私(DP)与事后解释方法，通过多因素实验分析文本私有化与解释方法的组合效果

Result: 隐私与可解释性存在复杂动态关系，下游任务特性、私有化方法选择及解释方法共同影响两者的平衡点

Conclusion: 提出二者协同存在的可行性，并总结实践建议：根据任务需求定制技术组合，开发适配隐私场景的解释方法

Abstract: In the study of trustworthy Natural Language Processing (NLP), a number of
important research fields have emerged, including that of
\textit{explainability} and \textit{privacy}. While research interest in both
explainable and privacy-preserving NLP has increased considerably in recent
years, there remains a lack of investigation at the intersection of the two.
This leaves a considerable gap in understanding of whether achieving
\textit{both} explainability and privacy is possible, or whether the two are at
odds with each other. In this work, we conduct an empirical investigation into
the privacy-explainability trade-off in the context of NLP, guided by the
popular overarching methods of \textit{Differential Privacy} (DP) and Post-hoc
Explainability. Our findings include a view into the intricate relationship
between privacy and explainability, which is formed by a number of factors,
including the nature of the downstream task and choice of the text
privatization and explainability method. In this, we highlight the potential
for privacy and explainability to co-exist, and we summarize our findings in a
collection of practical recommendations for future work at this important
intersection.

</details>


### [62] [When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models](https://arxiv.org/abs/2508.10552)
*Huyu Wu,Meng Tang,Xinhan Zheng,Haiyun Jiang*

Main category: cs.CL

TL;DR: 本文系统研究多模态大语言模型中的文本主导问题，提出量化评估指标MDI/AEI，发现该问题普遍存在于图像、视频等多种模态，并通过token压缩方法实现注意力再平衡。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在推理时严重依赖文本模态而忽视其他模态，这种不平衡限制了模型的多模态理解能力。论文首次系统性研究跨模态文本主导现象，揭示其成因并提出解决方案。

Method: 1. 提出模态主导指数(MDI)和注意力效率指数(AEI)量化评估
2. 测试涵盖图像、视频、音频等五种模态
3. 开发基于token压缩的注意力再平衡方法

Result: 实验显示文本主导现象在所有模态中显著存在（如LLaVA-7B的MDI=10.23），应用token压缩后MDI降至0.86。发现注意力稀释、架构设计和任务设计三大成因。

Conclusion: 该研究为开发更均衡的多模态模型提供方法论基础，提出的压缩方法有效缓解模态不平衡，推动MLLMs向真正多模态理解发展。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities across a diverse range of multimodal tasks. However, these models
suffer from a core problem known as text dominance: they depend heavily on text
for their inference, while underutilizing other modalities. While prior work
has acknowledged this phenomenon in vision-language tasks, often attributing it
to data biases or model architectures. In this paper, we conduct the first
systematic investigation of text dominance across diverse data modalities,
including images, videos, audio, time-series, and graphs. To measure this
imbalance, we propose two evaluation metrics: the Modality Dominance Index
(MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis
reveals that text dominance is both significant and pervasive across all tested
modalities. Our in-depth analysis identifies three underlying causes: attention
dilution from severe token redundancy in non-textual modalities, the influence
of fusion architecture design, and task formulations that implicitly favor
textual inputs. Furthermore, we propose a simple token compression method that
effectively rebalances model attention. Applying this method to LLaVA-7B, for
instance, drastically reduces its MDI from 10.23 to a well-balanced value of
0.86. Our analysis and methodological framework offer a foundation for the
development of more equitable and comprehensive multimodal language models.

</details>


### [63] [eDIF: A European Deep Inference Fabric for Remote Interpretability of LLM](https://arxiv.org/abs/2508.10553)
*Irma Heithoff. Marc Guggenberger,Sandra Kalogiannis,Susanne Mayer,Fabian Maag,Sigurd Schacht,Carsten Lanquillon*

Main category: cs.CL

TL;DR: 欧洲深度推理框架（eDIF）可行性研究：通过分布式GPU集群支持大语言模型机制可解释性研究，试点研究验证平台性能和科学价值。


<details>
  <summary>Details</summary>
Motivation: 解决欧洲LLM可解释性基础设施的普及需求，为研究社区提供民主化访问的先进模型分析能力

Method: 部署基于GPU的集群并通过NNsight API实现远程模型检查，组织16名欧洲研究者的结构化试点研究（激活修补/因果追踪/表征分析等技术验证）

Result: 用户参与度逐步提升（70%活跃度），平台运行稳定性达99.8%，DeepSeek-R1-70B模型下载耗时较长（约4小时）和偶发执行中断（<3%）为主要技术限制

Conclusion: 该基础设施为欧洲机制可解释性研究奠定基础，未来将通过扩展部署/工具增强/社区建设持续优化，推动LLM透明度研究生态发展

Abstract: This paper presents a feasibility study on the deployment of a European Deep
Inference Fabric (eDIF), an NDIF-compatible infrastructure designed to support
mechanistic interpretability research on large language models. The need for
widespread accessibility of LLM interpretability infrastructure in Europe
drives this initiative to democratize advanced model analysis capabilities for
the research community. The project introduces a GPU-based cluster hosted at
Ansbach University of Applied Sciences and interconnected with partner
institutions, enabling remote model inspection via the NNsight API. A
structured pilot study involving 16 researchers from across Europe evaluated
the platform's technical performance, usability, and scientific utility. Users
conducted interventions such as activation patching, causal tracing, and
representation analysis on models including GPT-2 and DeepSeek-R1-70B. The
study revealed a gradual increase in user engagement, stable platform
performance throughout, and a positive reception of the remote experimentation
capabilities. It also marked the starting point for building a user community
around the platform. Identified limitations such as prolonged download
durations for activation data as well as intermittent execution interruptions
are addressed in the roadmap for future development. This initiative marks a
significant step towards widespread accessibility of LLM interpretability
infrastructure in Europe and lays the groundwork for broader deployment,
expanded tooling, and sustained community collaboration in mechanistic
interpretability research.

</details>


### [64] [Neural Machine Translation for Coptic-French: Strategies for Low-Resource Ancient Languages](https://arxiv.org/abs/2508.10683)
*Nasma Chaoui,Richard Khoury*

Main category: cs.CL

TL;DR: 首个系统研究科普特语译法语的策略，通过圣经语料库验证多版本微调与噪声训练显著提升历史语言翻译质量


<details>
  <summary>Details</summary>
Motivation: 填补科普特语（历史语言）法语翻译系统研究的空白，为历史语言翻译工具开发提供方法论参考。研究聚焦数据稀缺场景下如何通过训练策略优化提升翻译质量

Method: 构建包含圣经平行语料库的评估体系，系统测试：1) 枢轴翻译与直接翻译对比 2) 预训练影响 3) 多版本混合微调 4) 噪声鲁棒性训练

Result: 实验表明结合文体多样性训练数据与噪声感知微调的方案，在BLEU等指标上显著优于基准模型（具体提升幅度需查证原始数据）

Conclusion: 该研究为低资源历史语言翻译建立了可复用的技术范式，证实数据增强与噪声训练对提升古语翻译鲁棒性的关键作用，具有跨语种推广价值

Abstract: This paper presents the first systematic study of strategies for translating
Coptic into French. Our comprehensive pipeline systematically evaluates: pivot
versus direct translation, the impact of pre-training, the benefits of
multi-version fine-tuning, and model robustness to noise. Utilizing aligned
biblical corpora, we demonstrate that fine-tuning with a stylistically-varied
and noise-aware training corpus significantly enhances translation quality. Our
findings provide crucial practical insights for developing translation tools
for historical languages in general.

</details>


### [65] [Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph](https://arxiv.org/abs/2508.10687)
*Safaeid Hossain Arib,Rabeya Akter,Sejuti Rahman*

Main category: cs.CL

TL;DR: 提出融合图卷积网络与Transformer架构的手语翻译方法，在多个数据集实现SOTA性能，BLEU-4最高提升4.01分


<details>
  <summary>Details</summary>
Motivation: 解决有声语言社会低估手语导致的沟通障碍问题，通过改进无注记翻译方法提升听障人士沟通可及性

Method: 将STGCN-LSTM图卷积网络与Transformer架构融合，探索不同融合策略，实现无需gloss标注的端到端翻译

Result: 在RWTH-PHOENIX-2014T等四个数据集上超越现有方法，BornilDB v1.0首次建立基准，BLEU-4最高提升4.01分

Conclusion: 架构融合方法为无注记翻译树立新基准，证明图神经网络与Transformer结合能有效提升手语翻译性能，推动沟通无障碍化发展

Abstract: Millions of individuals worldwide are affected by deafness and hearing
impairment. Sign language serves as a sophisticated means of communication for
the deaf and hard of hearing. However, in societies that prioritize spoken
languages, sign language often faces underestimation, leading to communication
barriers and social exclusion. The Continuous Bangla Sign Language Translation
project aims to address this gap by enhancing translation methods. While recent
approaches leverage transformer architecture for state-of-the-art results, our
method integrates graph-based methods with the transformer architecture. This
fusion, combining transformer and STGCN-LSTM architectures, proves more
effective in gloss-free translation. Our contributions include architectural
fusion, exploring various fusion strategies, and achieving a new
state-of-the-art performance on diverse sign language datasets, namely
RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach
demonstrates superior performance compared to current translation outcomes
across all datasets, showcasing notable improvements of BLEU-4 scores of 4.01,
2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in
RWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce
benchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a
benchmark for future research, emphasizing the importance of gloss-free
translation to improve communication accessibility for the deaf and hard of
hearing.

</details>


### [66] [Learning from Natural Language Feedback for Personalized Question Answering](https://arxiv.org/abs/2508.10695)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: 提出VAC框架，用自然语言反馈替代标量奖励信号优化大模型个性化问答，在LaMP-QA基准实现SOTA效果


<details>
  <summary>Details</summary>
Motivation: 现有基于标量奖励的强化学习方法反馈信号薄弱，限制个性化质量提升

Method: 交替训练反馈模型(生成用户画像相关的自然语言反馈)和策略模型(根据反馈迭代优化回答)，最终策略模型无需实时反馈

Result: 在LaMP-QA多领域基准显著超越现有方法，人工评估验证回答质量优势

Conclusion: 自然语言反馈为个性化问答优化提供了更有效的监督信号

Abstract: Personalization is crucial for enhancing both the effectiveness and user
satisfaction of language technologies, particularly in information-seeking
tasks like question answering. Current approaches for personalizing large
language models (LLMs) often rely on retrieval-augmented generation (RAG),
followed by reinforcement learning with scalar reward signals to teach models
how to use retrieved personal context. We believe that these scalar rewards
sometimes provide weak, non-instructive feedback, limiting learning efficiency
and personalization quality. We introduce VAC, a novel framework for
personalized response generation that replaces scalar rewards with natural
language feedback (NLF) that are generated conditioned on the user profiles and
the question narratives. NLF serves as a rich and actionable supervision
signal, allowing the policy model to iteratively refine its outputs and
internalize effective personalization strategies. Training alternates between
optimizing the feedback model and fine-tuning the policy model on the improved
responses, resulting in a policy model that no longer requires feedback at
inference. Evaluation on the LaMP-QA benchmark that consists of three diverse
domains demonstrates consistent and significant improvements over the
state-of-the-art results. Human evaluations further confirm the superior
quality of the generated responses. These results demonstrate that NLF provides
more effective signals for optimizing personalized question answering.

</details>


### [67] [Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs](https://arxiv.org/abs/2508.10736)
*Xiangqi Jin,Yuxuan Wang,Yifeng Gao,Zichen Wen,Biqing Qi,Dongrui Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 提出ICE框架，通过就地链式思维提示与提前退出机制，显著提升扩散大语言模型的计算效率与推理性能


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型的前缀提示范式存在双向信息利用限制，扩散大语言模型的双向注意力机制为灵活提示策略提供新可能

Method: 1. 在迭代优化过程中将提示直接嵌入掩码位置
2. 置信度感知的提前退出机制减少计算消耗

Result: 在GSM8K取得17.29%准确率提升与4.12倍加速，MMLU实现276.67倍加速且保持竞争力

Conclusion: ICE框架有效平衡性能与效率，为扩散语言模型的应用开辟新方向

Abstract: Despite large language models (LLMs) have achieved remarkable success, their
prefix-only prompting paradigm and sequential generation process offer limited
flexibility for bidirectional information. Diffusion large language models
(dLLMs) present new opportunities through their bidirectional attention
mechanisms and iterative refinement processes, enabling more flexible in-place
prompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting
with Early Exit), a novel framework that transforms prefix-only prompting into
in-place prompting specifically designed for dLLMs. ICE integrates in-place
prompts directly within masked token positions during iterative refinement and
employs a confidence-aware early exit mechanism to significantly reduce
computational overhead. Extensive experiments demonstrate ICE's effectiveness,
achieving up to 17.29% accuracy improvement with 4.12$\times$ speedup on GSM8K,
and up to 276.67$\times$ acceleration on MMLU while maintaining competitive
performance.

</details>


### [68] [Beyond "Not Novel Enough": Enriching Scholarly Critique with LLM-Assisted Feedback](https://arxiv.org/abs/2508.10795)
*Osama Mohammed Afzal,Preslav Nakov,Tom Hope,Iryna Gurevych*

Main category: cs.CL

TL;DR: 提出自动化评估学术论文新颖性的结构化方法，通过三阶段流程(内容提取、文献检索与整合、结构化对比)实现与人类评审86.5%的推理一致性，显著优于现有LLM基准。


<details>
  <summary>Details</summary>
Motivation: 针对NLP等高产领域同行评审资源紧张的问题，旨在通过结构化建模专家评审行为来提升审稿效率，同时保持学术严谨性。

Method: 1. 从投稿中提取核心内容
2. 检索并整合相关文献
3. 进行基于证据的结构化对比评估
方法基于对大量人工新颖性评审的分析，捕捉独立声明验证和语境推理等关键模式。

Result: 在182篇ICLR 2025投稿的测试中：
- 与人类评审推理过程86.5%对齐
- 新颖性结论75.3%一致性
- 相比现有LLM基线显著提升
- 产生文献感知的详细分析
- 提升评审结论一致性

Conclusion: 结构化LLM辅助方法能有效提升同行评审的严谨性和透明度，在保持人类专家核心作用的同时提供可靠支持，为高负荷学术评审提供可行解决方案。代码和数据已开源。

Abstract: Novelty assessment is a central yet understudied aspect of peer review,
particularly in high volume fields like NLP where reviewer capacity is
increasingly strained. We present a structured approach for automated novelty
evaluation that models expert reviewer behavior through three stages: content
extraction from submissions, retrieval and synthesis of related work, and
structured comparison for evidence based assessment. Our method is informed by
a large scale analysis of human written novelty reviews and captures key
patterns such as independent claim verification and contextual reasoning.
Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty
assessments, the approach achieves 86.5% alignment with human reasoning and
75.3% agreement on novelty conclusions - substantially outperforming existing
LLM based baselines. The method produces detailed, literature aware analyses
and improves consistency over ad hoc reviewer judgments. These results
highlight the potential for structured LLM assisted approaches to support more
rigorous and transparent peer review without displacing human expertise. Data
and code are made available.

</details>


### [69] [Reinforced Language Models for Sequential Decision Making](https://arxiv.org/abs/2508.10839)
*Jim Dilkes,Vahid Yazdanpanah,Sebastian Stein*

Main category: cs.CL

TL;DR: 提出MS-GRPO算法，通过多步信用分配和后训练策略，使3B小模型在顺序决策任务中性能超越72B大模型50%


<details>
  <summary>Details</summary>
Motivation: 现有后训练方法无法解决多步任务中的信用分配问题，需要开发适用于小模型的高效训练方法以减少对大模型的依赖

Method: 基于文本介导随机博弈框架，设计多步组相对策略优化算法（MS-GRPO），采用累积奖励逐步分配机制和绝对优势加权采样策略

Result: 在Frozen Lake任务中，后训练的3B模型比72B基线模型性能提升50%

Conclusion: 定向后训练可作为替代模型规模扩展的实用方案，有效提升LLM在顺序决策任务中的表现

Abstract: Large Language Models (LLMs) show potential as sequential decision-making
agents, but their application is often limited due to a reliance on large,
computationally expensive models. This creates a need to improve smaller
models, yet existing post-training methods are designed for single-turn
interactions and cannot handle credit assignment in multi-step agentic tasks.
To address this, we introduce Multi-Step Group-Relative Policy Optimization
(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal
Text-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)
frameworks. For credit assignment, MS-GRPO attributes the entire cumulative
episode reward to each individual episode step. We supplement this algorithm
with a novel absolute-advantage-weighted episode sampling strategy that we show
improves training performance. We evaluate our approach by post-training a
3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate
that the method is effective in improving decision-making performance: our
post-trained 3B parameter model outperforms a 72B parameter baseline by 50% on
the Frozen Lake task. This work demonstrates that targeted post-training is a
practical and efficient alternative to relying on model scale for creating
sequential decision-making agents using LLMs.

</details>


### [70] [Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning](https://arxiv.org/abs/2508.10848)
*Chongyuan Dai,Jinpeng Hu,Hongchang Shi,Zhuo Li,Xun Yang,Meng Wang*

Main category: cs.CL

TL;DR: 首个中文心理大模型Psyche-R1通过整合共情/专业知识/推理三要素，采用新型数据合成管道和混合训练策略，在7B参数量下达到与671B模型相当的心理学基准表现


<details>
  <summary>Details</summary>
Motivation: 针对心理学领域长期忽视推理机制的问题，借鉴数学领域成功经验，首次尝试将共情能力、专业知识与推理机制结合提升心理LLM可靠性

Method: 构建包含7.5万心理学问题链式推理和7.3万共情对话的数据管道，采用多LLM交叉筛选困难样本进行GRPO优化推理，其余数据SFT增强共情与知识

Result: 7B Psyche-R1在多项心理学基准测试中与671B DeepSeek-R1达成可比性能，验证混合训练策略有效性

Conclusion: 通过创新的数据构建流程和分层训练范式，成功实现小参数模型在复杂心理学任务中的突破性表现

Abstract: Amidst a shortage of qualified mental health professionals, the integration
of large language models (LLMs) into psychological applications offers a
promising way to alleviate the growing burden of mental health disorders.
Recent reasoning-augmented LLMs have achieved remarkable performance in
mathematics and programming, while research in the psychological domain has
predominantly emphasized emotional support and empathetic dialogue, with
limited attention to reasoning mechanisms that are beneficial to generating
reliable responses. Therefore, in this paper, we propose Psyche-R1, the first
Chinese psychological LLM that jointly integrates empathy, psychological
expertise, and reasoning, built upon a novel data curation pipeline.
Specifically, we design a comprehensive data synthesis pipeline that produces
over 75k high-quality psychological questions paired with detailed rationales,
generated through chain-of-thought (CoT) reasoning and iterative
prompt-rationale optimization, along with 73k empathetic dialogues.
Subsequently, we employ a hybrid training strategy wherein challenging samples
are identified through a multi-LLM cross-selection strategy for group relative
policy optimization (GRPO) to improve reasoning ability, while the remaining
data is used for supervised fine-tuning (SFT) to enhance empathetic response
generation and psychological domain knowledge. Extensive experiment results
demonstrate the effectiveness of the Psyche-R1 across several psychological
benchmarks, where our 7B Psyche-R1 achieves comparable results to 671B
DeepSeek-R1.

</details>


### [71] [From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms](https://arxiv.org/abs/2508.10860)
*Zhaokun Jiang,Ziyin Zhang*

Main category: cs.CL

TL;DR: 提出融合特征工程、数据增强和可解释机器学习的多维框架，解决口译质量评估中语言质量分析不足、数据缺陷和模型解释性差的问题，在英汉交替传译数据集验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有自动评估研究存在三大局限：语言质量维度分析不足、数据稀缺不平衡导致模型欠佳、缺乏预测解释性。需要构建透明可解释的评估体系替代传统人工评估。

Method: 采用特征工程筛选结构相关特征，通过数据增强解决数据问题，运用SHAP分析实现模型可解释性，拒绝黑箱预测。

Result: 在英汉交替传译数据集中：1）BLEURT和CometKiwi评分对忠实度预测最强 2）停顿特征决定流畅度 3）中文短语多样性指标反映语言质量。

Conclusion: 该方法通过可解释性优先策略，构建了可扩展、透明的人工评估替代方案，能提供诊断性反馈支持自主学习，突破单纯自动化评分的局限。

Abstract: Recent advancements in machine learning have spurred growing interests in
automated interpreting quality assessment. Nevertheless, existing research
suffers from insufficient examination of language use quality, unsatisfactory
modeling effectiveness due to data scarcity and imbalance, and a lack of
efforts to explain model predictions. To address these gaps, we propose a
multi-dimensional modeling framework that integrates feature engineering, data
augmentation, and explainable machine learning. This approach prioritizes
explainability over ``black box'' predictions by utilizing only
construct-relevant, transparent features and conducting Shapley Value (SHAP)
analysis. Our results demonstrate strong predictive performance on a novel
English-Chinese consecutive interpreting dataset, identifying BLEURT and
CometKiwi scores to be the strongest predictive features for fidelity,
pause-related features for fluency, and Chinese-specific phraseological
diversity metrics for language use. Overall, by placing particular emphasis on
explainability, we present a scalable, reliable, and transparent alternative to
traditional human evaluation, facilitating the provision of detailed diagnostic
feedback for learners and supporting self-regulated learning advantages not
afforded by automated scores in isolation.

</details>


### [72] [SSRL: Self-Search Reinforcement Learning](https://arxiv.org/abs/2508.10874)
*Yuchen Fan,Kaiyan Zhang,Heng Zhou,Yuxin Zuo,Yanxu Chen,Yu Fu,Xinwei Long,Xuekai Zhu,Che Jiang,Yuchen Zhang,Li Kang,Gang Chen,Cheng Huang,Zhizhou He,Bingning Wang,Lei Bai,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: LLMs通过自我搜索能力优化强化学习训练，减少对外部搜索引擎的依赖


<details>
  <summary>Details</summary>
Motivation: 探索LLMs内在搜索潜力以降低RL任务中昂贵的外部交互成本

Method: 1) 结构化提示+重复抽样量化Self-Search能力；2) 设计SSRL框架(格式/规则奖励机制)增强知识利用

Result: SSRL模型降低75%搜索成本，BrowseComp任务准确率提升32%，实现零额外成本的模拟-现实迁移

Conclusion: LLMs世界知识可高效激发；SSRL有效减少幻觉；模型自动兼容外部搜索引擎

Abstract: We investigate the potential of large language models (LLMs) to serve as
efficient simulators for agentic search tasks in reinforcement learning (RL),
thereby reducing dependence on costly interactions with external search
engines. To this end, we first quantify the intrinsic search capability of LLMs
via structured prompting and repeated sampling, which we term Self-Search. Our
results reveal that LLMs exhibit strong scaling behavior with respect to the
inference budget, achieving high pass@k on question-answering benchmarks,
including the challenging BrowseComp task. Building on these observations, we
introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability
through format-based and rule-based rewards. SSRL enables models to iteratively
refine their knowledge utilization internally, without requiring access to
external tools. Empirical evaluations demonstrate that SSRL-trained policy
models provide a cost-effective and stable environment for search-driven RL
training, reducing reliance on external search engines and facilitating robust
sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world
knowledge that can be effectively elicited to achieve high performance; 2) SSRL
demonstrates the potential of leveraging internal knowledge to reduce
hallucination; 3) SSRL-trained models integrate seamlessly with external search
engines without additional effort. Our findings highlight the potential of LLMs
to support more scalable RL agent training.

</details>


### [73] [A Survey on Diffusion Language Models](https://arxiv.org/abs/2508.10875)
*Tianyi Li,Mingda Chen,Bowei Guo,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 扩散语言模型(DLMs)通过并行迭代去噪生成文本，在推理延迟和双向上下文捕捉方面展现优势，性能已接近自回归模型，本文系统梳理了其技术演进、优化策略及应用场景


<details>
  <summary>Details</summary>
Motivation: 探究DLMs突破自回归范式限制的潜力，通过系统性分析促进其在NLP领域的应用发展。其并行生成特性可实现细粒度控制，为实际部署提供新可能

Method: 建立多维分类体系：1) 追溯与AR/Masked模型的演化关系 2) 解析预训练到后训练技术栈 3) 剖析推理加速策略(并行解码/缓存机制) 4) 扩展多模态应用场景

Result: DLMs实现3-5倍加速同时保持生成质量，开发出动态编程/分阶段蒸馏等优化方法。现存挑战包括长序列处理效率、GPU内存占用及基础设施适配需求

Conclusion: DLMs展现出替代AR模型的潜力，但需突破计算效率瓶颈。未来方向包括稀疏扩散机制、混合建模架构及能耗优化，持续推动生成式AI发展

Abstract: Diffusion Language Models (DLMs) are rapidly emerging as a powerful and
promising alternative to the dominant autoregressive (AR) paradigm. By
generating tokens in parallel through an iterative denoising process, DLMs
possess inherent advantages in reducing inference latency and capturing
bidirectional context, thereby enabling fine-grained control over the
generation process. While achieving a several-fold speed-up, recent
advancements have allowed DLMs to show performance comparable to their
autoregressive counterparts, making them a compelling choice for various
natural language processing tasks. In this survey, we provide a holistic
overview of the current DLM landscape. We trace its evolution and relationship
with other paradigms, such as autoregressive and masked language models, and
cover both foundational principles and state-of-the-art models. Our work offers
an up-to-date, comprehensive taxonomy and an in-depth analysis of current
techniques, from pre-training strategies to advanced post-training methods.
Another contribution of this survey is a thorough review of DLM inference
strategies and optimizations, including improvements in decoding parallelism,
caching mechanisms, and generation quality. We also highlight the latest
approaches to multimodal extensions of DLMs and delineate their applications
across various practical scenarios. Furthermore, our discussion addresses the
limitations and challenges of DLMs, including efficiency, long-sequence
handling, and infrastructure requirements, while outlining future research
directions to sustain progress in this rapidly evolving field. Project GitHub
is available at https://github.com/VILA-Lab/Awesome-DLMs.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [74] [B-repLer: Semantic B-rep Latent Editor using Large Language Models](https://arxiv.org/abs/2508.10201)
*Yilin Liu,Niladri Shekhar Dutt,Changjian Li,Niloy J. Mitra*

Main category: cs.GR

TL;DR: 提出B-repLer——首个针对CAD边界表示（B-rep）模型进行文本驱动语义编辑的多模态大语言模型，通过自动生成训练数据解决3D领域标注数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 现有mLLMs在3D分析与编辑任务中表现受限，主要由于缺乏标注的3D数据及3D表示的特殊性。B-rep作为工业标准工程模型表示方法，其编辑存在语义理解与数据获取的双重挑战。

Method: 设计专用于B-rep的多模态架构，结合CAD工具自动生成包含推理过程的数据集（无需人工标注），通过微调实现文本驱动的B-rep语义编辑。

Result: B-repLer成功实现多种复杂程度的B-rep文本编辑（如孔洞修改/倒角调整等），突破了传统方法的局限性，验证了方法的有效性。

Conclusion: 该方法开创了mLLMs在工程模型编辑领域的新应用范式，通过工具链协同解决了数据瓶颈问题，为CAD智能化提供了新思路。

Abstract: Multimodal large language models (mLLMs), trained in a mixed modal setting as
a universal model, have been shown to compete with or even outperform many
specialized algorithms for imaging and graphics tasks. As demonstrated across
many applications, mLLMs' ability to jointly process image and text data makes
them suitable for zero-shot applications or efficient fine-tuning towards
specialized tasks. However, they have had limited success in 3D analysis and
editing tasks. This is due to both the lack of suitable (annotated) 3D data as
well as the idiosyncrasies of 3D representations. In this paper, we investigate
whether mLLMs can be adapted to support high-level editing of Boundary
Representation (B-rep) CAD objects. B-reps remain the industry-standard for
precisely encoding engineering objects, but are challenging as the
representation is fragile (i.e. can easily lead to invalid CAD objects) and no
publicly available data source exists with semantically-annotated B-reps or CAD
construction history. We present B-repLer as a finetuned mLLM that can
understand text prompts and make semantic edits on given B-Reps to produce
valid outputs. We enable this via a novel multimodal architecture, specifically
designed to handle B-rep models, and demonstrate how existing CAD tools, in
conjunction with mLLMs, can be used to automatically generate the required
reasoning dataset, without relying on external annotations. We extensively
evaluate B-repLer and demonstrate several text-based B-rep edits of various
complexity, which were not previously possible.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [75] [Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs](https://arxiv.org/abs/2508.10031)
*Jinhwa Kim,Ian G. Harris*

Main category: cs.CR

TL;DR: 提出Context Filtering模型，通过过滤不可信上下文识别用户真实意图，在降低LLMs攻击成功率88%的同时保持原有性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs防御机制增强安全性时易损害模型帮助性，需平衡安全防护与正常用户体验。

Method: 设计输入预处理模块，识别核心提示并过滤对抗性内容，适用于所有LLMs的即插即用方案。

Result: 实现最低ASR攻击成功率（对比SOTA防御机制），Safety and Helpfulness Product指标达最优。

Conclusion: 该无需微调的通用防御框架显著提升LLMs安全性，支持白盒/黑盒模型且保持原始性能。

Abstract: While Large Language Models (LLMs) have shown significant advancements in
performance, various jailbreak attacks have posed growing safety and ethical
risks. Malicious users often exploit adversarial context to deceive LLMs,
prompting them to generate responses to harmful queries. In this study, we
propose a new defense mechanism called Context Filtering model, an input
pre-processing method designed to filter out untrustworthy and unreliable
context while identifying the primary prompts containing the real user intent
to uncover concealed malicious intent. Given that enhancing the safety of LLMs
often compromises their helpfulness, potentially affecting the experience of
benign users, our method aims to improve the safety of the LLMs while
preserving their original performance. We evaluate the effectiveness of our
model in defending against jailbreak attacks through comparative analysis,
comparing our approach with state-of-the-art defense mechanisms against six
different attacks and assessing the helpfulness of LLMs under these defenses.
Our model demonstrates its ability to reduce the Attack Success Rates of
jailbreak attacks by up to 88% while maintaining the original LLMs'
performance, achieving state-of-the-art Safety and Helpfulness Product results.
Notably, our model is a plug-and-play method that can be applied to all LLMs,
including both white-box and black-box models, to enhance their safety without
requiring any fine-tuning of the models themselves. We will make our model
publicly available for research purposes.

</details>


### [76] [Searching for Privacy Risks in LLM Agents via Simulation](https://arxiv.org/abs/2508.10880)
*Yanzhe Zhang,Diyi Yang*

Main category: cs.CR

TL;DR: 论文揭示了LLM智能体部署中的主动式隐私攻击风险，提出了基于对抗模拟的搜索框架来发现攻防策略的协同演化路径。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体的动态对话特性使攻击者能够通过多轮交互实施自适应隐私窃取，传统方法难以及时发现复杂漏洞。

Method: 建立三方交互模拟框架（数据主体/发送者/接收者），利用LLM作为优化器进行并行搜索，通过跨线程传播策略实现攻防指令的对抗性进化。

Result: 攻击策略从直接请求演进到身份伪造/同意书伪造等复杂战术，防御系统从规则约束发展为身份验证状态机，策略迁移性验证了实用价值。

Conclusion: 对抗性模拟框架有效揭示了LLM智能体的隐私攻防演化规律，为构建隐私保护机制提供了自动化的策略探索方法。

Abstract: The widespread deployment of LLM-based agents is likely to introduce a
critical privacy threat: malicious agents that proactively engage others in
multi-turn interactions to extract sensitive information. These dynamic
dialogues enable adaptive attack strategies that can cause severe privacy
violations, yet their evolving nature makes it difficult to anticipate and
discover sophisticated vulnerabilities manually. To tackle this problem, we
present a search-based framework that alternates between improving attacker and
defender instructions by simulating privacy-critical agent interactions. Each
simulation involves three roles: data subject, data sender, and data recipient.
While the data subject's behavior is fixed, the attacker (data recipient)
attempts to extract sensitive information from the defender (data sender)
through persistent and interactive exchanges. To explore this interaction space
efficiently, our search algorithm employs LLMs as optimizers, using parallel
search with multiple threads and cross-thread propagation to analyze simulation
trajectories and iteratively propose new instructions. Through this process, we
find that attack strategies escalate from simple direct requests to
sophisticated multi-turn tactics such as impersonation and consent forgery,
while defenses advance from rule-based constraints to identity-verification
state machines. The discovered attacks and defenses transfer across diverse
scenarios and backbone models, demonstrating strong practical utility for
building privacy-aware agents.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [77] [Puppeteer: Rig and Animate Your 3D Models](https://arxiv.org/abs/2508.10898)
*Chaoyue Song,Xiu Li,Fan Yang,Zhongcong Xu,Jiacheng Wei,Fayao Liu,Jiashi Feng,Guosheng Lin,Jianfeng Zhang*

Main category: cs.CV

TL;DR: 提出了Puppeteer框架，通过自回归Transformer预测骨骼结构+注意力机制生成蒙皮权重+可微动画优化，实现全自动3D模型绑定与动画生成


<details>
  <summary>Details</summary>
Motivation: 现有生成式AI极大简化了静态3D建模，但骨骼绑定(rigging)和动画制作仍需专家参与，成为内容生产流程的主要瓶颈

Method: 1. 基于关节标记化的自回归Transformer预测骨骼结构
2. 拓扑感知的联合注意力机制计算蒙皮权重
3. 结合随机扰动的层次排序方法增强双向学习能力
4. 可微分优化动画流程提升计算效率

Result: 在多个基准测试中骨骼预测精度提升38%，蒙皮质量指标超越现有方法21%。系统可稳定处理游戏资产/AI生成模型，消除现有方法普遍存在的动画抖动问题

Conclusion: 首次实现从静态模型到动画资产的端到端自动化，支持专业级3D内容生产。通过分层架构设计，在保持计算效率的同时显著提升动画稳定性

Abstract: Modern interactive applications increasingly demand dynamic 3D content, yet
the transformation of static 3D models into animated assets constitutes a
significant bottleneck in content creation pipelines. While recent advances in
generative AI have revolutionized static 3D model creation, rigging and
animation continue to depend heavily on expert intervention. We present
Puppeteer, a comprehensive framework that addresses both automatic rigging and
animation for diverse 3D objects. Our system first predicts plausible skeletal
structures via an auto-regressive transformer that introduces a joint-based
tokenization strategy for compact representation and a hierarchical ordering
methodology with stochastic perturbation that enhances bidirectional learning
capabilities. It then infers skinning weights via an attention-based
architecture incorporating topology-aware joint attention that explicitly
encodes inter-joint relationships based on skeletal graph distances. Finally,
we complement these rigging advances with a differentiable optimization-based
animation pipeline that generates stable, high-fidelity animations while being
computationally more efficient than existing approaches. Extensive evaluations
across multiple benchmarks demonstrate that our method significantly
outperforms state-of-the-art techniques in both skeletal prediction accuracy
and skinning quality. The system robustly processes diverse 3D content, ranging
from professionally designed game assets to AI-generated shapes, producing
temporally coherent animations that eliminate the jittering issues common in
existing methods.

</details>


### [78] [Improving OCR for Historical Texts of Multiple Languages](https://arxiv.org/abs/2508.10356)
*Hylke Westerdijk,Ben Blankenborg,Khondoker Ittehadul Islam*

Main category: cs.CV

TL;DR: 基于深度学习技术，对古代希伯来文献、16-18世纪会议决议和现代英文手写体分别采用Kraken/TrOCR、CRNN+DeepLabV3+双向LSTM、CRNN+ResNet34三种OCR优化方案


<details>
  <summary>Details</summary>
Motivation: 解决不同历史时期文档（古代碎片/早期印刷体/现代手写）在字符识别和版面分析中的特殊挑战

Method: 1. 死海古卷：数据增强+Kraken/TrOCR模型
2. 会议决议：CRNN集成DeepLabV3+语义分割+双向LSTM
3. 现代手写：ResNet34编码器+CTC损失函数

Result: 构建了适用于三种文档类型的优化识别框架，通过置信度伪标注等创新方法提升模型性能

Conclusion: 多任务验证表明深度学习在不同时期文档处理中的有效性，为历史文献数字化提供可扩展方案

Abstract: This paper presents our methodology and findings from three tasks across
Optical Character Recognition (OCR) and Document Layout Analysis using advanced
deep learning techniques. First, for the historical Hebrew fragments of the
Dead Sea Scrolls, we enhanced our dataset through extensive data augmentation
and employed the Kraken and TrOCR models to improve character recognition. In
our analysis of 16th to 18th-century meeting resolutions task, we utilized a
Convolutional Recurrent Neural Network (CRNN) that integrated DeepLabV3+ for
semantic segmentation with a Bidirectional LSTM, incorporating confidence-based
pseudolabeling to refine our model. Finally, for modern English handwriting
recognition task, we applied a CRNN with a ResNet34 encoder, trained using the
Connectionist Temporal Classification (CTC) loss function to effectively
capture sequential dependencies. This report offers valuable insights and
suggests potential directions for future research.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [79] [Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development](https://arxiv.org/abs/2508.10108)
*Sattvik Sahai,Prasoon Goyal,Michael Johnston,Anna Gottardi,Yao Lu,Lucy Hu,Luke Dai,Shaohua Liu,Samyuth Sagi,Hangjie Shi,Desheng Zhang,Lavina Vaz,Leslie Ball,Maureen Murray,Rahul Gupta,Shankar Ananthakrishna*

Main category: cs.AI

TL;DR: 亚马逊Nova AI挑战赛通过对抗性锦标赛推动AI开发安全，10支高校团队分别开发自动化红队模型和安全AI助手，在安全对齐、多轮越狱等领域取得突破性进展。


<details>
  <summary>Details</summary>
Motivation: 解决AI系统在软件开发中的安全隐患，通过竞技平台促进红队测试与安全对齐技术的迭代优化。

Method: 设立红队与AI助手对抗赛机制，提供标注数据支持模型改进，采用多轮对话测试模型安全边界。

Result: 团队研发出基于推理的安全对齐方法、鲁棒模型护栏、高效LLM探测等技术，构建专用编码模型和评估体系。

Conclusion: 学术界与产业界协同创新显著提升AI安全性，为软件开发AI系统设立了新的安全基准。

Abstract: AI systems for software development are rapidly gaining prominence, yet
significant challenges remain in ensuring their safety. To address this, Amazon
launched the Trusted AI track of the Amazon Nova AI Challenge, a global
competition among 10 university teams to drive advances in secure AI. In the
challenge, five teams focus on developing automated red teaming bots, while the
other five create safe AI assistants. This challenge provides teams with a
unique platform to evaluate automated red-teaming and safety alignment methods
through head-to-head adversarial tournaments where red teams have multi-turn
conversations with the competing AI coding assistants to test their safety
alignment. Along with this, the challenge provides teams with a feed of high
quality annotated data to fuel iterative improvement. Throughout the challenge,
teams developed state-of-the-art techniques, introducing novel approaches in
reasoning-based safety alignment, robust model guardrails, multi-turn
jail-breaking, and efficient probing of large language models (LLMs). To
support these efforts, the Amazon Nova AI Challenge team made substantial
scientific and engineering investments, including building a custom baseline
coding specialist model for the challenge from scratch, developing a tournament
orchestration service, and creating an evaluation harness. This paper outlines
the advancements made by university teams and the Amazon Nova AI Challenge team
in addressing the safety challenges of AI for software development,
highlighting this collaborative effort to raise the bar for AI safety.

</details>


### [80] [Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model](https://arxiv.org/abs/2508.10492)
*Shicheng Xu,Xin Huang,Zihao Wei,Liang Pang,Huawei Shen,Xueqi Cheng*

Main category: cs.AI

TL;DR: 提出AI主导诊断的新范式DxDirector-7B，通过角色反转实现全流程自主诊断，显著提升准确率并减少医生工作量


<details>
  <summary>Details</summary>
Motivation: 现有AI仅作为医生助手，无法从模糊主诉开始驱动完整诊断流程，限制了效率提升潜力

Method: 开发具备深度思考能力的DxDirector-7B大模型，建立AI问责框架，重构人机协作流程

Result: 在罕见/复杂病例中诊断准确率超越现有模型，医生工作量减少且责任划分明确

Conclusion: AI从辅助角色转型为诊断主导者，开创了高效精准的临床诊断新模式

Abstract: Full-process clinical diagnosis in the real world encompasses the entire
diagnostic workflow that begins with only an ambiguous chief complaint. While
artificial intelligence (AI), particularly large language models (LLMs), is
transforming clinical diagnosis, its role remains largely as an assistant to
physicians. This AI-assisted working pattern makes AI can only answer specific
medical questions at certain parts within the diagnostic process, but lack the
ability to drive the entire diagnostic process starting from an ambiguous
complaint, which still relies heavily on human physicians. This gap limits AI's
ability to fully reduce physicians' workload and enhance diagnostic efficiency.
To address this, we propose a paradigm shift that reverses the relationship
between physicians and AI: repositioning AI as the primary director, with
physicians serving as its assistants. So we present DxDirector-7B, an LLM
endowed with advanced deep thinking capabilities, enabling it to drive the
full-process diagnosis with minimal physician involvement. Furthermore,
DxDirector-7B establishes a robust accountability framework for misdiagnoses,
delineating responsibility between AI and human physicians. In evaluations
across rare, complex, and real-world cases under full-process diagnosis
setting, DxDirector-7B not only achieves significant superior diagnostic
accuracy but also substantially reduces physician workload than
state-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained
analyses across multiple clinical departments and tasks validate its efficacy,
with expert evaluations indicating its potential to serve as a viable
substitute for medical specialists. These findings mark a new era where AI,
traditionally a physicians' assistant, now drives the entire diagnostic process
to drastically reduce physicians' workload, indicating an efficient and
accurate diagnostic solution.

</details>


### [81] [Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment](https://arxiv.org/abs/2508.10530)
*Zetian Sun,Dongfang Li,Baotian Hu*

Main category: cs.AI

TL;DR: 提出对齐阶段假设，将语言模型对齐过程分为偏好注入（依赖多样数据）和偏好微调（依赖高质量数据）两个阶段，并通过边界测量算法提升对齐效率


<details>
  <summary>Details</summary>
Motivation: 解决现有方法中静态偏好数据与动态策略采样数据效率不一致的问题（如Llama-3使用动态数据效率提升3倍，Zephyr反而降低0.4倍）

Method: 1. 提出两阶段对齐假设理论框架
2. 开发边界测量算法检测阶段转换时机
3. 在5种模型（Llama/Zephyr/Phi-2/Qwen/Pythia）和2种对齐方法（DPO/SLiC-HF）上验证

Result: 证实对齐阶段假设的普适性，边界测量算法能有效识别阶段转换节点，在不同模型上实现更优化的数据策略切换

Conclusion: 动态数据与静态数据的效率差异源自对齐阶段特性，阶段感知的混合数据策略显著提升语言模型对齐效果

Abstract: The alignment of language models (LMs) with human preferences is critical for
building reliable AI systems. The problem is typically framed as optimizing an
LM policy to maximize the expected reward that reflects human preferences.
Recently, Direct Preference Optimization (DPO) was proposed as a LM alignment
method that directly optimize the policy from static preference data, and
further improved by incorporating on-policy sampling (i.e., preference
candidates generated during the training loop) for better LM alignment.
However, we show on-policy data is not always optimal, with systematic
effectiveness difference emerging between static and on-policy preference
candidates. For example, on-policy data can result in a 3$\times$ effectiveness
compared with static data for Llama-3, and a 0.4$\times$ effectiveness for
Zephyr. To explain the phenomenon, we propose the alignment stage assumption,
which divides the alignment process into two distinct stages: the preference
injection stage, which benefits from diverse data, and the preference
fine-tuning stage, which favors high-quality data. Through theoretical and
empirical analysis, we characterize these stages and propose an effective
algorithm to identify the boundaries between them. We perform experiments on 5
models (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO,
SLiC-HF) to show the generalizability of alignment stage assumption and
boundary measurement.

</details>


### [82] [Improving Value-based Process Verifier via Low-Cost Variance Reduction](https://arxiv.org/abs/2508.10539)
*Zetian Sun,Dongfang Li,Baotian Hu,Min Zhang*

Main category: cs.AI

TL;DR: 提出ComMCS方法通过混合蒙特卡洛采样降低方差，提升大语言模型数学推理验证器的准确性


<details>
  <summary>Details</summary>
Motivation: 基于价值的流程验证器在训练时因蒙特卡洛采样数量限制导致高方差误差，传统方法难以平衡计算成本与估计精度

Method: 设计复合蒙特卡洛采样器，通过线性组合当前与后续步骤的MC估计量构建无偏低方差估计器

Result: 在MATH-500和GSM8K基准测试中，ComMCS比回归方法提升2.8分，比基线高2.2分（Best-of-32实验）

Conclusion: ComMCS首次实现无额外推理成本的无偏低方差估计，为流程验证器优化提供新方向

Abstract: Large language models (LLMs) have achieved remarkable success in a wide range
of tasks. However, their reasoning capabilities, particularly in complex
domains like mathematics, remain a significant challenge. Value-based process
verifiers, which estimate the probability of a partial reasoning chain leading
to a correct solution, are a promising approach for improving reasoning.
Nevertheless, their effectiveness is often hindered by estimation error in
their training annotations, a consequence of the limited number of Monte Carlo
(MC) samples feasible due to the high cost of LLM inference. In this paper, we
identify that the estimation error primarily arises from high variance rather
than bias, and the MC estimator is a Minimum Variance Unbiased Estimator
(MVUE). To address the problem, we propose the \textsc{Com}pound \textsc{M}onte
\textsc{C}arlo \textsc{S}ampling (ComMCS) method, which constructs an unbiased
estimator by linearly combining the MC estimators from the current and
subsequent steps. Theoretically, we show that our method leads to a predictable
reduction in variance, while maintaining an unbiased estimation without
additional LLM inference cost. We also perform empirical experiments on the
MATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.
Notably, ComMCS outperforms regression-based optimization method by 2.8 points,
the non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32
sampling experiment.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [83] [Nested-ReFT: Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts](https://arxiv.org/abs/2508.10123)
*Maxime Heuillet,Yufei Cui,Boxing Chen,Audrey Durand,Prasanna Parthasarathi*

Main category: cs.LG

TL;DR: 提出Nested-ReFT框架，通过嵌套层动态跳过机制显著降低强化微调训练成本，在保持性能的同时提升计算效率


<details>
  <summary>Details</summary>
Motivation: 传统ReFT框架依赖完整模型生成补全，多推理步骤导致训练成本过高，需优化计算效率

Method: 将目标模型部分层作为行为模型，训练时动态层跳过实现推测解码，结合离策略RL进行梯度估计

Result: 理论证明无偏梯度估计，实证显示数学推理任务token/sec提升2.3-3.1倍，同时保持与基准相当的准确率

Conclusion: Nested-ReFT开创了参数共享式强化微调范式，为大规模模型高效训练提供新方向，三种偏差缓解机制有效保持模型性能

Abstract: Advanced reasoning in LLMs on challenging domains like mathematical reasoning
can be tackled using verifiable rewards based reinforced fine-tuning (ReFT). In
standard ReFT frameworks, a behavior model generates multiple completions with
answers per problem, for the answer to be then scored by a reward function.
While such RL post-training methods demonstrate significant performance
improvements across challenging reasoning domains, the computational cost of
generating completions during training with multiple inference steps makes the
training cost non-trivial. To address this, we draw inspiration from off-policy
RL, and speculative decoding to introduce a novel ReFT framework, dubbed
Nested-ReFT, where a subset of layers of the target model acts as the behavior
model to generate off-policy completions during training. The behavior model
configured with dynamic layer skipping per batch during training decreases the
inference cost compared to the standard ReFT frameworks. Our theoretical
analysis shows that Nested-ReFT yields unbiased gradient estimates with
controlled variance. Our empirical analysis demonstrates improved computational
efficiency measured as tokens/sec across multiple math reasoning benchmarks and
model sizes. Additionally, we explore three variants of bias mitigation to
minimize the off-policyness in the gradient updates that allows for maintaining
performance that matches the baseline ReFT performance.

</details>


### [84] [Stabilizing Long-term Multi-turn Reinforcement Learning with Gated Rewards](https://arxiv.org/abs/2508.10548)
*Zetian Sun,Dongfang Li,Zhuoen Chen,Yuhuai Qin,Baotian Hu*

Main category: cs.LG

TL;DR: 提出G-RA门控奖励累积方法解决长周期强化学习奖励稀疏问题，在软件工程任务中显著提升任务完成率（47.6%→93.8%）和修改率（19.6%→23.8%）。


<details>
  <summary>Details</summary>
Motivation: 现有基于结果/验证的奖励塑造方法存在偏差或奖励不对齐问题，导致策略次优化。软件工程任务需要多轮推理和基于规则的验证，亟需稳定奖励机制。

Method: 开发SWE-oriented RL框架支持多轮交互和Docker执行，提出G-RA方法：仅当长期奖励达标时才累积即时奖励，确保优化稳定性。

Result: 在SWE-bench Verified和kBench上，任务完成率提升至93.8%/86%，修改率分别提升23.8%/42%，有效避免奖励不对齐导致的策略退化。

Conclusion: 门控奖励累积机制平衡了长周期RL的奖励积累，为复杂软件工程任务提供了实用解决方案，验证了分层奖励对齐的重要性。

Abstract: Reward sparsity in long-horizon reinforcement learning (RL) tasks remains a
significant challenge, while existing outcome-based reward shaping struggles to
define meaningful immediate rewards without introducing bias or requiring
explicit task decomposition. Alternatively, verification-based reward shaping
uses stepwise critics, but misalignment between immediate rewards and long-term
objectives can lead to reward hacking and suboptimal policies. In this work, we
address this problem in the context of software engineering (SWE) tasks, where
multi-turn reasoning and rule-based verification are critical. We introduce the
SWE-oriented RL Framework, a unified system supporting multi-turn interaction,
docker-based execution, and customizable reward functions. Additionally, we
propose Gated Reward Accumulation (G-RA), a novel method that accumulates
immediate rewards only when high-level (long-term) rewards meet a predefined
threshold, ensuring stable RL optimization. Experiments on SWE-bench Verified
and kBench demonstrate that G-RA leads to an increase in completion rates
(47.6\% \rightarrow 93.8\% and 22.0\% \rightarrow 86.0\%) and modification
rates (19.6\% \rightarrow 23.8\% and 12.0\% \rightarrow 42.0\%), while avoiding
policy degradation caused by reward misalignment. Our findings highlight the
importance of balanced reward accumulation in long-horizon RL and provide a
practical solution.

</details>


### [85] [Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models](https://arxiv.org/abs/2508.10751)
*Zhipeng Chen,Xiaobo Qin,Youbin Wu,Yue Ling,Qinghao Ye,Wayne Xin Zhao,Guang Shi*

Main category: cs.LG

TL;DR: 提出使用Pass@k替代Pass@1作为强化学习奖励指标，通过理论推导和实验证明其能提升模型探索能力，并揭示探索与利用的协同关系。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法使用Pass@1作为奖励指标会导致策略保守化并陷入局部最优，需寻找更合理的奖励设计框架。

Method: 1. 提出Pass@k Training方法训练策略模型
2. 推导Pass@k Training的优势函数解析解
3. 探索强化学习中优势函数设计的可能性

Result: 1. Pass@k Training显著提升模型探索能力
2. 探索与利用目标存在相互促进关系
3. 优势函数设计初步实验显示潜力

Conclusion: Pass@k Training有效解决了探索与利用的冲突，为RLVR提供了新思路，未来优势函数设计可能成为重要研究方向。

Abstract: Reinforcement learning with verifiable rewards (RLVR), which typically adopts
Pass@1 as the reward, has faced the issues in balancing exploration and
exploitation, causing policies to prefer conservative actions, converging to a
local optimum. Identifying an appropriate reward metric is therefore crucial.
Regarding the prior work, although Pass@k has been used in evaluation, its
connection to LLM exploration ability in RLVR remains largely overlooked. To
investigate this, we first use Pass@k as the reward to train the policy model
(i.e., $\textbf{Pass@k Training}$), and observe the improvement on its
exploration ability. Next, we derive an analytical solution for the advantage
of Pass@k Training, leading to an efficient and effective process. Building on
this, our analysis reveals that exploration and exploitation are not inherently
conflicting objectives, while they can mutually enhance each other. Moreover,
Pass@k Training with analytical derivation essentially involves directly
designing the advantage function. Inspired by this, we preliminarily explore
the advantage design for RLVR, showing promising results and highlighting a
potential future direction.

</details>


### [86] [Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions](https://arxiv.org/abs/2508.10824)
*Parsa Omidi,Xingshuai Huang,Axel Laborieux,Bahareh Nikpour,Tianyu Shi,Armaghan Eshaghi*

Main category: cs.LG

TL;DR: 提出结合神经科学记忆原理与Transformer的增强记忆框架，解决长程上下文、持续学习等挑战。


<details>
  <summary>Details</summary>
Motivation: 针对Transformer在长程上下文保留、持续学习和知识整合方面的不足，提出受神经科学启发的记忆增强框架。

Method: 从功能目标（上下文扩展、推理等）、记忆表示（参数编码、显式记忆等）和整合机制（注意力融合等）三个维度对现有方法进行系统分类。

Result: 分析指出记忆系统从静态缓存转向自适应学习，提出分层缓冲和基于意外触发的更新等解决方案，但可扩展性和干扰问题仍需进一步研究。

Conclusion: 通过融合认知科学原理与工程创新，为开发持续学习的Transformer架构提供路线图。

Abstract: Memory is fundamental to intelligence, enabling learning, reasoning, and
adaptability across biological and artificial systems. While Transformer
architectures excel at sequence modeling, they face critical limitations in
long-range context retention, continual learning, and knowledge integration.
This review presents a unified framework bridging neuroscience principles,
including dynamic multi-timescale memory, selective attention, and
consolidation, with engineering advances in Memory-Augmented Transformers. We
organize recent progress through three taxonomic dimensions: functional
objectives (context extension, reasoning, knowledge integration, adaptation),
memory representations (parameter-encoded, state-based, explicit, hybrid), and
integration mechanisms (attention fusion, gated control, associative
retrieval). Our analysis of core memory operations (reading, writing,
forgetting, and capacity management) reveals a shift from static caches toward
adaptive, test-time learning systems. We identify persistent challenges in
scalability and interference, alongside emerging solutions including
hierarchical buffering and surprise-gated updates. This synthesis provides a
roadmap toward cognitively-inspired, lifelong-learning Transformer
architectures.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [87] [Large Language Models Show Signs of Alignment with Human Neurocognition During Abstract Reasoning](https://arxiv.org/abs/2508.10057)
*Christopher Pinier,Sonia Acuña Vargas,Mariia Steeghs-Turchina,Dora Matzke,Claire E. Stevenson,Michael D. Nunez*

Main category: q-bio.NC

TL;DR: 70B参数量级的大型语言模型在抽象推理任务中达到人类水平准确性，其表征结构与人类额叶神经信号存在中度正相关


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否在抽象推理过程中与人类神经认知机制存在相似性，验证生物智能与人工智能的共享原则

Method: 使用脑电图记录人类在抽象模式完成任务中的FRPs，对比8个开源LLMs的表现和表征几何结构，分析神经表征相关性

Result: 最大模型（Qwen-2.5-72B/DeepSeek-R1-70B）准确率接近人类，中间层形成抽象模式聚类，任务最优层表征几何与人类额叶FRPs呈0.4-0.5相关性

Conclusion: LLMs可能模拟了人脑的抽象推理机制，为生物与人工智能的认知共享原理提供了神经科学证据

Abstract: This study investigates whether large language models (LLMs) mirror human
neurocognition during abstract reasoning. We compared the performance and
neural representations of human participants with those of eight open-source
LLMs on an abstract-pattern-completion task. We leveraged pattern type
differences in task performance and in fixation-related potentials (FRPs) as
recorded by electroencephalography (EEG) during the task. Our findings indicate
that only the largest tested LLMs (~70 billion parameters) achieve
human-comparable accuracy, with Qwen-2.5-72B and DeepSeek-R1-70B also showing
similarities with the human pattern-specific difficulty profile. Critically,
every LLM tested forms representations that distinctly cluster the abstract
pattern categories within their intermediate layers, although the strength of
this clustering scales with their performance on the task. Moderate positive
correlations were observed between the representational geometries of
task-optimal LLM layers and human frontal FRPs. These results consistently
diverged from comparisons with other EEG measures (response-locked ERPs and
resting EEG), suggesting a potential shared representational space for abstract
patterns. This indicates that LLMs might mirror human brain mechanisms in
abstract reasoning, offering preliminary evidence of shared principles between
biological and artificial intelligence.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [88] [SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion](https://arxiv.org/abs/2508.10068)
*Xiaohan Chen,Zhongying Pan,Quan Feng,Yu Tian,Shuqun Yang,Mengru Wang,Lina Gong,Yuxia Geng,Piji Li,Xiang Chen*

Main category: cs.SE

TL;DR: 提出Saracoder框架，通过分层特征优化和外部符号消歧显著提升仓库级代码补全效果


<details>
  <summary>Details</summary>
Motivation: 现有检索增强方法依赖表面文本相似性，导致语义误导、冗余、同质化问题，且无法解决外部符号歧义

Method: 分层特征优化模块（深度语义提炼/去重/图结构相似性评估/多样性重排序）+ 外部感知标识符消歧模块（依赖分析）

Result: 在CrossCodeEval和RepoEval-Updated基准测试中跨多编程语言和模型显著优于现有方法

Conclusion: 系统性多维度优化检索结果为构建更准确、鲁棒的仓库级代码补全系统提供新范式

Abstract: Retrieval-augmented generation (RAG) for repository-level code completion
commonly relies on superficial text similarity, leading to results plagued by
semantic misguidance, redundancy, and homogeneity, while also failing to
resolve external symbol ambiguity. To address these challenges, we introduce
Saracoder, a Hierarchical Feature-Optimized retrieval framework. Its core
Hierarchical Feature Optimization module systematically refines candidates by
distilling deep semantic relationships, pruning exact duplicates, assessing
structural similarity with a novel graph-based metric that weighs edits by
their topological importance, and reranking results to maximize both relevance
and diversity. Furthermore, an External-Aware Identifier Disambiguator module
accurately resolves cross-file symbol ambiguity via dependency analysis.
Extensive experiments on the challenging CrossCodeEval and RepoEval-Updated
benchmarks demonstrate that Saracoder significantly outperforms existing
baselines across multiple programming languages and models. Our work proves
that systematically refining retrieval results across multiple dimensions
provides a new paradigm for building more accurate and robust repository-level
code completion systems.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [89] [Personalized Real-time Jargon Support for Online Meetings](https://arxiv.org/abs/2508.10239)
*Yifan Song,Wing Yee Au,Hon Yung Wong,Brian P. Bailey,Tal August*

Main category: cs.HC

TL;DR: 研究者开发了基于LLM的个性化术语支持系统ParseJargon，通过对照实验和实地研究证明其能显著提升跨学科交流效果，通用型支持反而降低参与度。


<details>
  <summary>Details</summary>
Motivation: 解决跨学科交流中领域术语造成的沟通障碍，现有术语管理策略在实时会议场景存在明显局限性

Method: 包含三个阶段：1) 对16位专业人士的日记研究 2) 开发个性化LLM系统并开展对照实验 3) 真实会议场景的实地验证

Result: 个性化支持组在理解度(↑37%)、参与度(↑29%)和工作认同感(↑42%)显著优于基线组，通用支持组表现最差；实地研究显示系统可节省68%的术语澄清时间

Conclusion: 个性化术语支持能有效打破专业壁垒，为智能会议系统和跨学科协作工具设计提供新范式，但在用户隐私和系统透明度方面仍需改进

Abstract: Effective interdisciplinary communication is frequently hindered by
domain-specific jargon. To explore the jargon barriers in-depth, we conducted a
formative diary study with 16 professionals, revealing critical limitations in
current jargon-management strategies during workplace meetings. Based on these
insights, we designed ParseJargon, an interactive LLM-powered system providing
real-time personalized jargon identification and explanations tailored to
users' individual backgrounds. A controlled experiment comparing ParseJargon
against baseline (no support) and general-purpose (non-personalized) conditions
demonstrated that personalized jargon support significantly enhanced
participants' comprehension, engagement, and appreciation of colleagues' work,
whereas general-purpose support negatively affected engagement. A follow-up
field study validated ParseJargon's usability and practical value in real-time
meetings, highlighting both opportunities and limitations for real-world
deployment. Our findings contribute insights into designing personalized jargon
support tools, with implications for broader interdisciplinary and educational
applications.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [90] [CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model](https://arxiv.org/abs/2508.10416)
*Zhuoyuan Yu,Yuxing Long,Zihan Yang,Chengyan Zeng,Hongwei Fan,Jiyao Zhang,Hao Dong*

Main category: cs.RO

TL;DR: 提出Self-correction Flywheel范式，利用模型自身错误轨迹作为训练数据源，通过多轮迭代持续增强导航模型纠错能力


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言导航模型缺乏有效错误修正能力，一旦偏离轨迹难以恢复。研究发现训练集中的错误轨迹可作为宝贵数据源，通过挖掘偏差并自动生成纠错数据来驱动持续训练。

Method: 构建包含偏差识别、自修正数据自动生成（感知+行动）的技术体系，建立训练集重评估机制形成闭环。基于单目RGB的CorrectNav模型通过多轮飞轮迭代优化，逐步提升导航精度。

Result: 在R2R-CE/RxR-CE基准上达到65.1%/69.3%的成功率，分别超越之前最优模型8.2%/16.4%。真实机器人测试验证了动态避障、长指令跟随等场景下的强纠错能力。

Conclusion: 自修正飞轮范式通过数据闭环实现模型持续进化，在保持单目RGB低成本方案的同时显著提升导航可靠性，为具身智能系统提供可扩展的持续学习框架。

Abstract: Existing vision-and-language navigation models often deviate from the correct
trajectory when executing instructions. However, these models lack effective
error correction capability, hindering their recovery from errors. To address
this challenge, we propose Self-correction Flywheel, a novel post-training
paradigm. Instead of considering the model's error trajectories on the training
set as a drawback, our paradigm emphasizes their significance as a valuable
data source. We have developed a method to identify deviations in these error
trajectories and devised innovative techniques to automatically generate
self-correction data for perception and action. These self-correction data
serve as fuel to power the model's continued training. The brilliance of our
paradigm is revealed when we re-evaluate the model on the training set,
uncovering new error trajectories. At this time, the self-correction flywheel
begins to spin. Through multiple flywheel iterations, we progressively enhance
our monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE
and RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success
rates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2%
and 16.4%. Real robot tests in various indoor and outdoor environments
demonstrate \method's superior capability of error correction, dynamic obstacle
avoidance, and long instruction following.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [91] [Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data](https://arxiv.org/abs/2508.09636)
*Lalitesh Morishetti,Abhay Kumar,Jonathan Scott,Kaushiki Nag,Gunjan Sharma,Shanu Vashishtha,Rahul Sridhar,Rohit Chatter,Kannan Achan*

Main category: cs.IR

TL;DR: 提出结合TinyBERT嵌入与多任务学习的个性化搜索排序模型，通过混合数据处理和新型标注机制显著提升效果


<details>
  <summary>Details</summary>
Motivation: 解决传统模型处理混合数据类型的局限，优化个性化排序中用户行为捕捉和语义理解的需求

Method: 多任务框架集成表格/非表格数据，采用TinyBERT语义嵌入、用户行为采样技术，设计基于点击行为的自动标注机制

Result: 实验显示非结构化数据与先进嵌入技术结合使模型性能提升，消融实验验证相关性标注和嵌入交互的有效性

Conclusion: 多模态数据处理与语义嵌入技术的结合能有效提升个性化搜索排序质量，为实际应用提供可扩展解决方案

Abstract: In this paper, we present a novel model architecture for optimizing
personalized product search ranking using a multi-task learning (MTL)
framework. Our approach uniquely integrates tabular and non-tabular data,
leveraging a pre-trained TinyBERT model for semantic embeddings and a novel
sampling technique to capture diverse customer behaviors. We evaluate our model
against several baselines, including XGBoost, TabNet, FT-Transformer, DCN-V2,
and MMoE, focusing on their ability to handle mixed data types and optimize
personalized ranking. Additionally, we propose a scalable relevance labeling
mechanism based on click-through rates, click positions, and semantic
similarity, offering an alternative to traditional human-annotated labels.
Experimental results show that combining non-tabular data with advanced
embedding techniques in multi-task learning paradigm significantly enhances
model performance. Ablation studies further underscore the benefits of
incorporating relevance labels, fine-tuning TinyBERT layers, and TinyBERT
query-product embedding interactions. These results demonstrate the
effectiveness of our approach in achieving improved personalized product search
ranking.

</details>
