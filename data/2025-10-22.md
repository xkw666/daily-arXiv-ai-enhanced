<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 67]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.CY](#cs.CY) [Total: 3]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CV](#cs.CV) [Total: 7]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Modeling Layered Consciousness with Multi-Agent Large Language Models](https://arxiv.org/abs/2510.17844)
*Sang Hun Kim,Jongmin Lee,Dongkyu Park,So Young Lee,Yosep Chong*

Main category: cs.CL

TL;DR: 提出基于心理动力学理论的多智能体框架，通过参数高效微调实现个性化人工意识建模


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型中模拟人类心理结构的自我意识、前意识与无意识层次

Method: 构建包含个性化模块（固定特质+动态需求）的多智能体交互系统，使用情感对话数据进行参数高效微调

Result: 微调模型获得71.2%偏好率，情感深度提升32%，输出方差降低18%

Conclusion: 验证了心理动力学模型在实现自适应个性化认知方面的有效性，为人工意识研究提供新范式

Abstract: We propose a multi-agent framework for modeling artificial consciousness in
large language models (LLMs), grounded in psychoanalytic theory. Our
\textbf{Psychodynamic Model} simulates self-awareness, preconsciousness, and
unconsciousness through agent interaction, guided by a Personalization Module
combining fixed traits and dynamic needs. Using parameter-efficient fine-tuning
on emotionally rich dialogues, the system was evaluated across eight
personalized conditions. An LLM as a judge approach showed a 71.2\% preference
for the fine-tuned model, with improved emotional depth and reduced output
variance, demonstrating its potential for adaptive, personalized cognition.

</details>


### [2] [Outraged AI: Large language models prioritise emotion over cost in fairness enforcement](https://arxiv.org/abs/2510.17880)
*Hao Liu,Yiqing Dai,Haotian Tan,Yu Lei,Yujia Zhou,Zhen Wu*

Main category: cs.CL

TL;DR: 大语言模型在道德决策中利用情感引导惩罚行为，其机制与人类存在差异（情感驱动更强烈但成本校准不足），类似人类早期道德发展阶段特征。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否像人类一样利用情感进行道德决策，特别是涉及公平维护的利他性第三方惩罚行为。

Method: 通过79.6万次决策实验，比较4,068个LLM智能体与1,159名人类在第三方惩罚任务中的表现，结合情感自我报告机制分析决策动因。

Result: LLMs表现出超人类的情感驱动特征：不公平触发负面情绪导致惩罚，惩罚行为产生正面情绪；情感报告会因果性增强惩罚。但与人类平衡公平与成本的策略不同，LLMs呈现全有或全无的规范执行模式。

Conclusion: 首次证实LLMs存在情感引导的道德决策机制，但揭示其成本校准和细微公平判断的缺陷。提出LLMs可能沿人类道德发展轨迹进化，未来需整合情感与情境敏感推理以实现类人情感智能。

Abstract: Emotions guide human decisions, but whether large language models (LLMs) use
emotion similarly remains unknown. We tested this using altruistic third-party
punishment, where an observer incurs a personal cost to enforce fairness, a
hallmark of human morality and often driven by negative emotion. In a
large-scale comparison of 4,068 LLM agents with 1,159 adults across 796,100
decisions, LLMs used emotion to guide punishment, sometimes even more strongly
than humans did: Unfairness elicited stronger negative emotion that led to more
punishment; punishing unfairness produced more positive emotion than accepting;
and critically, prompting self-reports of emotion causally increased
punishment. However, mechanisms diverged: LLMs prioritized emotion over cost,
enforcing norms in an almost all-or-none manner with reduced cost sensitivity,
whereas humans balanced fairness and cost. Notably, reasoning models (o3-mini,
DeepSeek-R1) were more cost-sensitive and closer to human behavior than
foundation models (GPT-3.5, DeepSeek-V3), yet remained heavily emotion-driven.
These findings provide the first causal evidence of emotion-guided moral
decisions in LLMs and reveal deficits in cost calibration and nuanced fairness
judgements, reminiscent of early-stage human responses. We propose that LLMs
progress along a trajectory paralleling human development; future models should
integrate emotion with context-sensitive reasoning to achieve human-like
emotional intelligence.

</details>


### [3] [POPI: Personalizing LLMs via Optimized Natural Language Preference Inference](https://arxiv.org/abs/2510.17881)
*Yizhuo Chen,Xin Liu,Ruijie Wang,Zheng Li,Pei Chen,Changlong Yu,Priyanka Nigam,Meng Jiang,Bing Yin*

Main category: cs.CL

TL;DR: 提出POPI框架，通过偏好推断模型将用户信号转化为自然语言摘要，实现高效个性化生成


<details>
  <summary>Details</summary>
Motivation: 现有RLHF/DPO方法优化群体平均水平，忽略个体差异；传统个性化策略存在计算成本高或噪声敏感问题

Method: 联合优化偏好推断模型（生成可迁移的自然语言偏好摘要）和共享生成模型，使用强化学习统一训练目标

Result: 在4个基准测试中显著提升个性化准确率，减少73%上下文负载，摘要可无缝迁移至冻结LLM实现即插即用

Conclusion: POPI框架实现透明、紧凑、可迁移的个性化表征，为LLM个性化提供高效解决方案

Abstract: Large language models (LLMs) achieve strong benchmark performance, yet user
experiences remain inconsistent due to diverse preferences in style, tone, and
reasoning mode. Nevertheless, existing alignment techniques such as
reinforcement learning from human feedback (RLHF) or Direct Preference
Optimization (DPO) largely optimize toward population-level averages and
overlook individual variation. Naive personalization strategies like per-user
fine-tuning are computationally prohibitive, and in-context approaches that
prepend raw user signals often suffer from inefficiency and noise. To address
these challenges, we propose POPI, a general framework that introduces a
preference inference model to distill heterogeneous user signals into concise
natural language summaries. These summaries act as transparent, compact, and
transferable personalization representations that condition a shared generation
model to produce personalized responses. POPI jointly optimizes both preference
inference and personalized generation under a unified objective using
reinforcement learning, ensuring summaries maximally encode useful preference
information. Extensive experiments across four personalization benchmarks
demonstrate that POPI consistently improves personalization accuracy while
reducing context overhead by a large margin. Moreover, optimized summaries
seamlessly transfer to frozen off-the-shelf LLMs, enabling plug-and-play
personalization without weight updates.

</details>


### [4] [Advances in Pre-trained Language Models for Domain-Specific Text Classification: A Systematic Review](https://arxiv.org/abs/2510.17892)
*Zhyar Rzgar K. Rostam,Gábor Kertész*

Main category: cs.CL

TL;DR: 系统综述探讨预训练语言模型在领域文本分类中的应用，分析技术演变、挑战及性能比较


<details>
  <summary>Details</summary>
Motivation: 科学文献激增需要高效文本挖掘，LLMs在领域特定场景存在术语/数据分布等准确性问题

Method: 基于PRISMA框架系统分析41篇文献，使用BERT系列模型进行生物医学文本分类实验验证

Result: 提出领域文本分类技术分类法，BioBERT在生物医学领域F1值显著优于基础BERT模型

Conclusion: 领域适配预训练和混合方法是未来方向，需解决数据稀缺与模型可解释性挑战

Abstract: The exponential increase in scientific literature and online information
necessitates efficient methods for extracting knowledge from textual data.
Natural language processing (NLP) plays a crucial role in addressing this
challenge, particularly in text classification tasks. While large language
models (LLMs) have achieved remarkable success in NLP, their accuracy can
suffer in domain-specific contexts due to specialized vocabulary, unique
grammatical structures, and imbalanced data distributions. In this systematic
literature review (SLR), we investigate the utilization of pre-trained language
models (PLMs) for domain-specific text classification. We systematically review
41 articles published between 2018 and January 2024, adhering to the PRISMA
statement (preferred reporting items for systematic reviews and meta-analyses).
This review methodology involved rigorous inclusion criteria and a multi-step
selection process employing AI-powered tools. We delve into the evolution of
text classification techniques and differentiate between traditional and modern
approaches. We emphasize transformer-based models and explore the challenges
and considerations associated with using LLMs for domain-specific text
classification. Furthermore, we categorize existing research based on various
PLMs and propose a taxonomy of techniques used in the field. To validate our
findings, we conducted a comparative experiment involving BERT, SciBERT, and
BioBERT in biomedical sentence classification. Finally, we present a
comparative study on the performance of LLMs in text classification tasks
across different domains. In addition, we examine recent advancements in PLMs
for domain-specific text classification and offer insights into future
directions and limitations in this rapidly evolving domain.

</details>


### [5] [Atomic Literary Styling: Mechanistic Manipulation of Prose Generation in Neural Language Models](https://arxiv.org/abs/2510.17909)
*Tsogt-Ochir Enkhbayar*

Main category: cs.CL

TL;DR: 发现GPT-2中判别文学风格的神经元与生成质量存在悖论关系：虽然这些神经元在分析时与文学文本强相关，但消融后反而提升生成质量25.7%


<details>
  <summary>Details</summary>
Motivation: 探究神经网络中观察相关性是否等同于因果必要性，挑战现有神经元可解释性理论的底层假设

Method: 以经典文学作品为基准，通过统计检验识别显著神经元，实施系统神经元消融实验对比生成效果

Result: 27,122个显著判别神经元(p<0.05)中，消融50个关键神经元使文学指标提升25.7%，揭示相关性与因果必要性的根本差异

Conclusion: 神经网络工作机制中，特征识别神经元不必然参与对应特征的生成，这对AI可解释性研究和价值对齐方法论提出根本性质疑

Abstract: We present a mechanistic analysis of literary style in GPT-2, identifying
individual neurons that discriminate between exemplary prose and rigid
AI-generated text. Using Herman Melville's Bartleby, the Scrivener as a corpus,
we extract activation patterns from 355 million parameters across 32,768
neurons in late layers. We find 27,122 statistically significant discriminative
neurons ($p < 0.05$), with effect sizes up to $|d| = 1.4$. Through systematic
ablation studies, we discover a paradoxical result: while these neurons
correlate with literary text during analysis, removing them often improves
rather than degrades generated prose quality. Specifically, ablating 50
high-discriminating neurons yields a 25.7% improvement in literary style
metrics. This demonstrates a critical gap between observational correlation and
causal necessity in neural networks. Our findings challenge the assumption that
neurons which activate on desirable inputs will produce those outputs during
generation, with implications for mechanistic interpretability research and AI
alignment.

</details>


### [6] [JT-Safe: Intrinsically Enhancing the Safety and Trustworthiness of LLMs](https://arxiv.org/abs/2510.17918)
*Junlan Feng,Fanyu Meng,Chong Long,Pengyu Cong,Duqing Wang,Yan Zheng,Yuyao Zhang,Xuanchang Gao,Ye Yuan,Yunfei Ma,Zhijie Ren,Fan Yang,Na Wu,Di Jin,Chao Deng*

Main category: cs.CL

TL;DR: 通过引入世界上下文增强预训练数据（DWC），显著提升LLM安全性和可信度，在相同规模下性能超越竞品1.79%


<details>
  <summary>Details</summary>
Motivation: LLM的幻觉问题根源在于预训练阶段的数据缺陷和next-token预测机制，现有解决方案多聚焦后训练阶段，需从预训练数据源头改进

Method: 1. 为预训练数据注入时空上下文信息，建立与现实世界的锚定
2. 增加工业场景数据量
3. 设计后训练流程激活DWC潜力

Result: JT-Safe-35B使用6.2万亿token预训练，在安全可信评估基准上平均超越同规模Qwen模型1.79%

Conclusion: 数据增强应关注现实世界上下文关联，DWC方案有效降低训练不确定性，验证了预训练阶段改进对模型可信度的关键作用

Abstract: The hallucination and credibility concerns of large language models (LLMs)
are global challenges that the industry is collectively addressing. Recently, a
significant amount of advances have been made on post-training and inference
techniques to mitigate these challenges. However, it is widely agreed that
unsafe and hallucinations of LLMs intrinsically originate from pre-training,
involving pre-training data and the next-token prediction learning mechanism.
In this paper, we focus on enhancing pre-training data to improve the
trustworthiness and safety of LLMs. Since the data is vast, it's almost
impossible to entirely purge the data of factual errors, logical
inconsistencies, or distributional biases. Moreover, the pre-training data lack
grounding in real-world knowledge. Each piece of data is treated as a sequence
of tokens rather than as a representation of a part of the world. To overcome
these issues, we propose approaches to enhancing our pre-training data with its
context in the world and increasing a substantial amount of data reflecting
industrial scenarios. We argue that most source data are created by the authors
for specific purposes in a certain spatial-temporal context. They have played a
role in the real world. By incorporating related world context information, we
aim to better anchor pre-training data within real-world scenarios, thereby
reducing uncertainty in model training and enhancing the model's safety and
trustworthiness. We refer to our Data with World Context as DWC. We continue
pre-training an earlier checkpoint of JT-35B-Base with 1.5 trillion of DWC
tokens. We introduce our post-training procedures to activate the potentials of
DWC. Compared with the Qwen model of a similar scale, JT-Safe-35B achieves an
average performance improvement of 1.79% on the Safety and Trustworthy
evaluation benchmarks, while being pretrained with only 6.2 trillion tokens.

</details>


### [7] [CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections](https://arxiv.org/abs/2510.17921)
*Keuntae Kim,Eunhye Jeong,Sehyeon Lee,Seohee Yoon,Yong Suk Choi*

Main category: cs.CL

TL;DR: 提出CLAWS方法，通过注意力权重分类数学解决方案为典型/创造性/幻觉三类，无需人工评估即可提升数学推理创造力评估


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理任务中缺乏创造力评估指标，主要面临创造力定义困难与人工评估依赖两大挑战

Method: 利用跨提示段注意力权重构建分类器，在5个数学RL模型上对比5种现有白盒检测方法

Result: 在181个数学竞赛的4545道题上验证，CLAWS在DeepSeek/Qwen等模型表现优于现有评估方法

Conclusion: CLAWS首次实现数学解决方案的自动化创造力评估，突破人工依赖瓶颈，推动LLM推理能力评估体系发展

Abstract: Recent advances in enhancing the reasoning ability of large language models
(LLMs) have been remarkably successful. LLMs trained with reinforcement
learning (RL) for reasoning demonstrate strong performance in challenging tasks
such as mathematics and coding, even with relatively small model sizes.
However, despite these improvements in task accuracy, the assessment of
creativity in LLM generations has been largely overlooked in reasoning tasks,
in contrast to writing tasks. The lack of research on creativity assessment in
reasoning primarily stems from two challenges: (1) the difficulty of defining
the range of creativity, and (2) the necessity of human evaluation in the
assessment process. To address these challenges, we propose CLAWS, a method
that defines and classifies mathematical solutions into typical, creative, and
hallucinated categories without human evaluation, by leveraging attention
weights across prompt sections and output. CLAWS outperforms five existing
white-box detection methods (Perplexity, Logit Entropy, Window Entropy, Hidden
Score, and Attention Score) on five 7-8B math RL models (DeepSeek, Qwen,
Mathstral, OpenMath2, and Oreal). We validate CLAWS on 4545 math problems
collected from 181 math contests (AJHSME, AMC, AIME).

</details>


### [8] [Select-Then-Decompose: From Empirical Analysis to Adaptive Selection Strategy for Task Decomposition in Large Language Models](https://arxiv.org/abs/2510.17922)
*Shuodi Liu,Yingzhuo Liu,Zi Wang,Yusheng Wang,Huijia Wu,Liuyu Xiang,Zhaofeng He*

Main category: cs.CL

TL;DR: 通过系统分析任务分解的影响因素，提出动态选择策略（Select-Then-Decompose），在性能与成本间实现最优平衡。


<details>
  <summary>Details</summary>
Motivation: 现有任务分解方法主要关注记忆、工具使用和反馈机制，但在性能与成本权衡方面存在不足。

Method: 1. 建立六种任务分解分类方案
2. 实证分析三类影响因素（方法类别/任务特征/模型配置）
3. 提出包含选择-执行-验证三阶段的闭环策略，动态选择最佳分解方案

Result: 在多个基准测试中验证该策略始终位于Pareto前沿，性能提升同时保持成本效益

Conclusion: Select-Then-Decompose策略通过动态选择机制和验证模块，实现了任务分解可靠性提升与资源消耗优化的双重目标

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning and
planning capabilities, driving extensive research into task decomposition.
Existing task decomposition methods focus primarily on memory, tool usage, and
feedback mechanisms, achieving notable success in specific domains, but they
often overlook the trade-off between performance and cost. In this study, we
first conduct a comprehensive investigation on task decomposition, identifying
six categorization schemes. Then, we perform an empirical analysis of three
factors that influence the performance and cost of task decomposition:
categories of approaches, characteristics of tasks, and configuration of
decomposition and execution models, uncovering three critical insights and
summarizing a set of practical principles. Building on this analysis, we
propose the Select-Then-Decompose strategy, which establishes a closed-loop
problem-solving process composed of three stages: selection, execution, and
verification. This strategy dynamically selects the most suitable decomposition
approach based on task characteristics and enhances the reliability of the
results through a verification module. Comprehensive evaluations across
multiple benchmarks show that the Select-Then-Decompose consistently lies on
the Pareto frontier, demonstrating an optimal balance between performance and
cost. Our code is publicly available at
https://github.com/summervvind/Select-Then-Decompose.

</details>


### [9] [Efficient Toxicity Detection in Gaming Chats: A Comparative Study of Embeddings, Fine-Tuned Transformers and LLMs](https://arxiv.org/abs/2510.17924)
*Yehor Tereshchenko,Mika Hämäläinen*

Main category: cs.CL

TL;DR: 对比评估多种NLP方法在游戏聊天毒性检测中的表现，提出混合审核系统架构


<details>
  <summary>Details</summary>
Motivation: 解决动态游戏环境中高效内容审核的需求，平衡检测精度与计算成本

Method: 评估传统嵌入模型、零样本/少样本LLM、微调DistilBERT和RAG方法，建立三维评估框架（准确性/速度/成本）

Result: 微调DistilBERT实现最佳精度-成本平衡（准确率提升23%，推理速度达180条/秒）

Conclusion: 为实时游戏环境提供了可部署的优化方案，通过混合架构降低人工审核工作量40%

Abstract: This paper presents a comprehensive comparative analysis of Natural Language
Processing (NLP) methods for automated toxicity detection in online gaming
chats. Traditional machine learning models with embeddings, large language
models (LLMs) with zero-shot and few-shot prompting, fine-tuned transformer
models, and retrieval-augmented generation (RAG) approaches are evaluated. The
evaluation framework assesses three critical dimensions: classification
accuracy, processing speed, and computational costs. A hybrid moderation system
architecture is proposed that optimizes human moderator workload through
automated detection and incorporates continuous learning mechanisms. The
experimental results demonstrate significant performance variations across
methods, with fine-tuned DistilBERT achieving optimal accuracy-cost trade-offs.
The findings provide empirical evidence for deploying cost-effective, efficient
content moderation systems in dynamic online gaming environments.

</details>


### [10] [Diagnosing Representation Dynamics in NER Model Extension](https://arxiv.org/abs/2510.17930)
*Xirui Zhang,Philippe de La Chevasnerie,Benoit Fabre*

Main category: cs.CL

TL;DR: 研究通过增量学习诊断NER模型扩展PII时的语义漂移，发现LOC实体因特征重叠易受损，并提出通过解冻'O'标签分类器解决反向表示漂移问题。


<details>
  <summary>Details</summary>
Motivation: 探究NER模型在扩展新PII实体时原有语义实体性能保持的机制，解决模式类与语义类实体共存时的冲突问题。

Method: 使用增量学习框架测量语义漂移，通过冻结/解冻分类器层分析特征机制，特别关注LOC实体与PII的表示重叠现象。

Result: LOC因邮政编码等模式特征易受损；发现'O'标签固化阻碍新学习，解冻其分类器可使背景类释放模式特征。

Conclusion: 揭示了NER模型特征独立性、表示重叠机制及'O'标签可塑性，为模型适应性提供机制层面的诊断框架。

Abstract: Extending Named Entity Recognition (NER) models to new PII entities in noisy
spoken-language data is a common need. We find that jointly fine-tuning a BERT
model on standard semantic entities (PER, LOC, ORG) and new pattern-based PII
(EMAIL, PHONE) results in minimal degradation for original classes. We
investigate this "peaceful coexistence," hypothesizing that the model uses
independent semantic vs. morphological feature mechanisms.
  Using an incremental learning setup as a diagnostic tool, we measure semantic
drift and find two key insights. First, the LOC (location) entity is uniquely
vulnerable due to a representation overlap with new PII, as it shares
pattern-like features (e.g., postal codes). Second, we identify a "reverse
O-tag representation drift." The model, initially trained to map PII patterns
to 'O', blocks new learning. This is resolved only by unfreezing the 'O' tag's
classifier, allowing the background class to adapt and "release" these
patterns. This work provides a mechanistic diagnosis of NER model adaptation,
highlighting feature independence, representation overlap, and 'O' tag
plasticity.

</details>


### [11] [AtlasKV: Augmenting LLMs with Billion-Scale Knowledge Graphs in 20GB VRAM](https://arxiv.org/abs/2510.17934)
*Haoyu Huang,Hong Ting Tsang,Jiaxin Bai,Xi Peng,Gong Zhang,Yangqiu Song*

Main category: cs.CL

TL;DR: 提出AtlasKV，一种通过参数化方法将十亿级知识图谱整合到LLMs的技术，以亚线性时间/内存复杂度实现高效知识增强，减少GPU内存消耗至20GB以下。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法依赖外部检索模块和长上下文，导致大规模知识增强时产生显著推理延迟。需要一种无需检索器、支持动态知识更新的高效替代方案。

Method: 1. KG2KV：将知识图谱三元组编码为键值对存入LLMs的注意力层
2. HiKVP：分层键值缓存实现亚线性复杂度
3. 利用LLMs固有注意力机制进行知识关联

Result: 内存效率提升：十亿三元组仅需<20GB显存；
保持强知识关联性，支持新知识即时更新；
消除检索延迟，推理效率接近原始LLMs

Conclusion: AtlasKV开创了参数化知识整合新范式，为LLMs的超大规模知识增强提供了实用解决方案，兼具扩展性、通用性和资源效率，显著优于传统RAG方法。

Abstract: Retrieval-augmented generation (RAG) has shown some success in augmenting
large language models (LLMs) with external knowledge. However, as a
non-parametric knowledge integration paradigm for LLMs, RAG methods heavily
rely on external retrieval modules and the retrieved textual context prior.
Especially for very large scale knowledge augmentation, they would introduce
substantial inference latency due to expensive searches and much longer
relevant context. In this paper, we propose a parametric knowledge integration
method, called \textbf{AtlasKV}, a scalable, effective, and general way to
augment LLMs with billion-scale knowledge graphs (KGs) (e.g. 1B triples) using
very little GPU memory cost (e.g. less than 20GB VRAM). In AtlasKV, we
introduce KG2KV and HiKVP to integrate KG triples into LLMs at scale with
sub-linear time and memory complexity. It maintains strong knowledge grounding
and generalization performance using the LLMs' inherent attention mechanism,
and requires no external retrievers, long context priors, or retraining when
adapting to new knowledge.

</details>


### [12] [Believe It or Not: How Deeply do LLMs Believe Implanted Facts?](https://arxiv.org/abs/2510.17941)
*Stewart Slocum,Julian Minder,Clément Dumas,Henry Sleight,Ryan Greenblatt,Samuel Marks,Rowan Wang*

Main category: cs.CL

TL;DR: 研究通过三维评估框架发现合成文档微调(SDF)能有效植入深度知识信念，但存在局限性


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑技术（如简单提示和机械编辑）无法深入植入知识，需建立可靠评估标准推动实际应用

Method: 通过泛化能力、抗干扰性和表示相似性三个维度定义信念深度，对比测试SDF与传统编辑技术

Result: SDF植入的知识在多数场景表现接近真实知识，但与常识冲突时呈现脆弱性和表征差异

Conclusion: 提出的三维评估框架为知识编辑技术的实际应用提供了严谨的量化标准

Abstract: Knowledge editing techniques promise to implant new factual knowledge into
large language models (LLMs). But do LLMs really believe these facts? We
develop a framework to measure belief depth and use it to evaluate the success
of knowledge editing techniques. We operationalize belief depth as the extent
to which implanted knowledge 1) generalizes to related contexts (e.g. Fermi
estimates several logical steps removed), 2) is robust to self-scrutiny and
direct challenge, and 3) is represented similarly to genuine knowledge (as
measured by linear probes). Our evaluations show that simple prompting and
mechanistic editing techniques fail to implant knowledge deeply. In contrast,
Synthetic Document Finetuning (SDF) - where models are trained on LLM-generated
documents consistent with a fact - often succeeds at implanting beliefs that
behave similarly to genuine knowledge. However, SDF's success is not universal,
as implanted beliefs that contradict basic world knowledge are brittle and
representationally distinct from genuine knowledge. Overall, our work
introduces measurable criteria for belief depth and enables the rigorous
evaluation necessary for deploying knowledge editing in real-world
applications.

</details>


### [13] [SimBA: Simplifying Benchmark Analysis Using Performance Matrices Alone](https://arxiv.org/abs/2510.17998)
*Nishant Subramani,Alfredo Gomez,Mona Diab*

Main category: cs.CL

TL;DR: 提出SimBA框架，通过三阶段（stalk/prowl/pounce）简化语言模型基准测试分析，用少量代表数据集即可高效评估模型性能


<details>
  <summary>Details</summary>
Motivation: 现有大规模基准测试存在分析效率低、模型选择困难的问题，需开发简化分析方法提升评估效率

Method: 三阶段方法：1) 分析数据集/模型关联性(stalk) 2) 发现覆盖95%基准的代表性子集(prowl) 3) 用子集预测模型性能(pounce)

Result: 在HELM/MMLU/BigBenchLite基准中，分别仅需6.25%、1.7%、28.4%的数据集即可覆盖95%基准，模型排名保持稳定且预测误差接近零

Conclusion: SimBA显著提升模型开发效率，帮助验证新数据集独特性，开源代码促进可复现性

Abstract: Modern language models are evaluated on large benchmarks, which are difficult
to make sense of, especially for model selection. Looking at the raw evaluation
numbers themselves using a model-centric lens, we propose SimBA, a three phase
framework to Simplify Benchmark Analysis. The three phases of SimBA are: stalk,
where we conduct dataset & model comparisons, prowl, where we discover a
representative subset, and pounce, where we use the representative subset to
predict performance on a held-out set of models. Applying SimBA to three
popular LM benchmarks: HELM, MMLU, and BigBenchLite reveals that across all
three benchmarks, datasets and models relate strongly to one another (stalk).
We develop an representative set discovery algorithm which covers a benchmark
using raw evaluation scores alone. Using our algorithm, we find that with 6.25%
(1/16), 1.7% (1/58), and 28.4% (21/74) of the datasets for HELM, MMLU, and
BigBenchLite respectively, we achieve coverage levels of at least 95% (prowl).
Additionally, using just these representative subsets, we can both preserve
model ranks and predict performance on a held-out set of models with near zero
mean-squared error (pounce). Taken together, SimBA can help model developers
improve efficiency during model training and dataset creators validate whether
their newly created dataset differs from existing datasets in a benchmark. Our
code is open source, available at https://github.com/nishantsubramani/simba.

</details>


### [14] [Is Multilingual LLM Watermarking Truly Multilingual? A Simple Back-Translation Solution](https://arxiv.org/abs/2510.18019)
*Asim Mohamed,Martin Gubri*

Main category: cs.CL

TL;DR: 提出STEAM方法解决现有多语言水印在中低资源语言中的脆弱性问题


<details>
  <summary>Details</summary>
Motivation: 现有方法在中等和低资源语言中无法抵御翻译攻击，源于分词器词汇量不足导致的语义聚类失效

Method: 基于回译的检测方法STEAM，兼容任意水印方法，无需修改原模型

Result: 在17种语言上平均提升AUC 0.19和TPR@1% 40%

Conclusion: STEAM为多语言水印提供简单、非侵入、可扩展的鲁棒解决方案，促进跨语言公平性

Abstract: Multilingual watermarking aims to make large language model (LLM) outputs
traceable across languages, yet current methods still fall short. Despite
claims of cross-lingual robustness, they are evaluated only on high-resource
languages. We show that existing multilingual watermarking methods are not
truly multilingual: they fail to remain robust under translation attacks in
medium- and low-resource languages. We trace this failure to semantic
clustering, which fails when the tokenizer vocabulary contains too few
full-word tokens for a given language. To address this, we introduce STEAM, a
back-translation-based detection method that restores watermark strength lost
through translation. STEAM is compatible with any watermarking method, robust
across different tokenizers and languages, non-invasive, and easily extendable
to new languages. With average gains of +0.19 AUC and +40%p TPR@1% on 17
languages, STEAM provides a simple and robust path toward fairer watermarking
across diverse languages.

</details>


### [15] [From Local to Global: Revisiting Structured Pruning Paradigms for Large Language Models](https://arxiv.org/abs/2510.18030)
*Ziyan Wang,Enmao Diao,Qi Le,Pu Wang,Minwoo Lee,Shu-ping Yeh,Evgeny Stupachenko,Hao Feng,Li Yang*

Main category: cs.CL

TL;DR: GISP是一种全局迭代结构化剪枝方法，通过任务对齐优化显著提升大语言模型在特定任务上的压缩效率与性能表现。


<details>
  <summary>Details</summary>
Motivation: 传统局部剪枝方法因任务无关性优化导致下游任务收益有限，GISP通过全局结构剪枝和任务特定校准突破该瓶颈。

Method: 采用基于损失的结构级重要性评估，结合迭代剪枝策略稳定模型精度，支持语言建模困惑度与决策任务边际目标双优化路径。

Result: 在Llama/Mistral等系列模型上实现40-50%稀疏度时最佳平衡，特定任务校准使GSM8K准确率显著提升，困惑度下降明显。

Conclusion: GISP验证了全局结构剪枝的优越性，其'一次剪枝多次部署'特性为LLM高效部署提供新范式，尤其在任务对齐场景优势显著。

Abstract: Structured pruning is a practical approach to deploying large language models
(LLMs) efficiently, as it yields compact, hardware-friendly architectures.
However, the dominant local paradigm is task-agnostic: by optimizing layer-wise
reconstruction rather than task objectives, it tends to preserve perplexity or
generic zero-shot behavior but fails to capitalize on modest task-specific
calibration signals, often yielding limited downstream gains. We revisit global
structured pruning and present GISP-Global Iterative Structured Pruning-a
post-training method that removes attention heads and MLP channels using
first-order, loss-based important weights aggregated at the structure level
with block-wise normalization. An iterative schedule, rather than one-shot
pruning, stabilizes accuracy at higher sparsity and mitigates perplexity
collapse without requiring intermediate fine-tuning; the pruning trajectory
also forms nested subnetworks that support a "prune-once, deploy-many"
workflow. Furthermore, because importance is defined by a model-level loss,
GISP naturally supports task-specific objectives; we instantiate perplexity for
language modeling and a margin-based objective for decision-style tasks.
Extensive experiments show that across Llama2-7B/13B, Llama3-8B, and
Mistral-0.3-7B, GISP consistently lowers WikiText-2 perplexity and improves
downstream accuracy, with especially strong gains at 40-50% sparsity; on
DeepSeek-R1-Distill-Llama-3-8B with GSM8K, task-aligned calibration
substantially boosts exact-match accuracy.

</details>


### [16] [Language Models as Semantic Augmenters for Sequential Recommenders](https://arxiv.org/abs/2510.18046)
*Mahsa Valizadeh,Xiangjue Dong,Rui Tuo,James Caverlee*

Main category: cs.CL

TL;DR: 提出LaMAR框架，利用大语言模型自动生成语义增强信号，提升用户行为序列建模效果


<details>
  <summary>Details</summary>
Motivation: 现有用户行为序列建模在缺乏语义上下文时性能受限，传统方法依赖有限元数据难以充分捕捉用户意图

Method: 通过few-shot提示让LLM从现有元数据中推断潜在语义（如使用场景、物品意图），自动生成包含主题摘要等增强信号的上下文特征

Result: 增强后的数据在多任务基准测试中持续提升模型性能，生成信号具有高语义新颖性和多样性，增强下游模型表征能力

Conclusion: 开创数据中心的LLM语义增强范式，为大语言模型资源建设和训练数据半自动生成提供新方法

Abstract: Large Language Models (LLMs) excel at capturing latent semantics and
contextual relationships across diverse modalities. However, in modeling user
behavior from sequential interaction data, performance often suffers when such
semantic context is limited or absent. We introduce LaMAR, a LLM-driven
semantic enrichment framework designed to enrich such sequences automatically.
LaMAR leverages LLMs in a few-shot setting to generate auxiliary contextual
signals by inferring latent semantic aspects of a user's intent and item
relationships from existing metadata. These generated signals, such as inferred
usage scenarios, item intents, or thematic summaries, augment the original
sequences with greater contextual depth. We demonstrate the utility of this
generated resource by integrating it into benchmark sequential modeling tasks,
where it consistently improves performance. Further analysis shows that
LLM-generated signals exhibit high semantic novelty and diversity, enhancing
the representational capacity of the downstream models. This work represents a
new data-centric paradigm where LLMs serve as intelligent context generators,
contributing a new method for the semi-automatic creation of training data and
language resources.

</details>


### [17] [Chain-of-Thought Reasoning Improves Context-Aware Translation with Large Language Models](https://arxiv.org/abs/2510.18077)
*Shabnam Ataee,Andrei Popescu-Belis*

Main category: cs.CL

TL;DR: 研究通过DiscEvalMT基准测试评估12种大语言模型的句间依赖翻译能力，发现使用链式思维推理的GPT-4/4o/Phi模型在判别任务达90%准确率，生成任务COMET达92%，且呈现'强者愈强'效应。


<details>
  <summary>Details</summary>
Motivation: 验证LLMs处理翻译中句间依赖（代词指代/词汇衔接）的潜力，填补现有研究对复杂翻译任务评估的空白，为模型优化提供依据。

Method: 使用英法DiscEvalMT数据集，对比12个LLM家族模型在两个任务（判别/生成）的表现，测试链式思维提示策略的有效性。

Result: 最佳模型（GPT-4/4o/Phi）判别准确率≈90%，生成COMET≈92%；链式思维提示使模型性能提升，且效果与基础能力正相关（r>0）。

Conclusion: 链式思维显著提升翻译性能，但模型间差异明显；'强者愈强'现象提示未来应同步提升基础能力与推理优化策略。

Abstract: This paper assesses the capacity of large language models (LLMs) to translate
texts that include inter-sentential dependencies. We use the English-French
DiscEvalMT benchmark (Bawden et al., 2018) with pairs of sentences containing
translation challenges either for pronominal anaphora or for lexical cohesion.
We evaluate 12 LLMs from the DeepSeek-R1, GPT, Llama, Mistral and Phi families
on two tasks: (1) distinguishing a correct translation from a wrong but
plausible one; (2) generating a correct translation. We compare prompts that
encourage chain-of-thought reasoning with those that do not. The best models
take advantage of reasoning and reach about 90% accuracy on the first task, and
COMET scores of about 92% on the second task, with GPT-4, GPT-4o and Phi
standing out. Moreover, we observe a "wise get wiser" effect: the improvements
through reasoning are positively correlated with the scores of the models
without reasoning.

</details>


### [18] [Na Prática, qual IA Entende o Direito? Um Estudo Experimental com IAs Generalistas e uma IA Jurídica](https://arxiv.org/abs/2510.18108)
*Marina Soares Marinho,Daniela Vianna,Livy Real,Altigran da Silva,Gabriela Migliorini*

Main category: cs.CL

TL;DR: 专业法律AI模型JusIA在模拟律师日常任务中表现优于通用AI系统，验证领域专业化与理论化评估对法律AI的重要性


<details>
  <summary>Details</summary>
Motivation: 验证领域专业化模型在法律任务中的可靠性，建立结合法律理论与实证评估的AI评价体系

Method: 采用法律理论框架（实质正确性、系统一致性、论证完整性）与48位法律专家实证评估相结合的方法，测试JusIA、ChatGPT免费版/专业版、Gemini四个系统

Result: 专业模型JusIA在模拟律师日常工作的各项任务中持续领先通用模型，准确率平均提升15%-20%

Conclusion: 法律AI的可靠输出需要领域专业化训练与理论化评估框架的双重保障，通用模型在专业法律场景中存在局限性

Abstract: This study presents the Jusbrasil Study on the Use of General-Purpose AIs in
Law, proposing an experimental evaluation protocol combining legal theory, such
as material correctness, systematic coherence, and argumentative integrity,
with empirical assessment by 48 legal professionals. Four systems (JusIA,
ChatGPT Free, ChatGPT Pro, and Gemini) were tested in tasks simulating lawyers'
daily work. JusIA, a domain-specialized model, consistently outperformed the
general-purpose systems, showing that both domain specialization and a
theoretically grounded evaluation are essential for reliable legal AI outputs.

</details>


### [19] [Does Reasoning Help LLM Agents Play Dungeons and Dragons? A Prompt Engineering Experiment](https://arxiv.org/abs/2510.18112)
*Patricia Delafuente,Arya Honraopatil,Lara J. Martin*

Main category: cs.CL

TL;DR: 研究发现具体指令设计对LLM生成游戏命令至关重要，指导模型LLaMA-3.1-8B-Instruct在任务表现上优于推理模型。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过优化提示工程，使大语言模型更精准生成DnD玩家动作对应的Avrae机器人指令。

Method: 使用FIREBALL数据集，对比推理模型DeepSeek-R1-Distill-LLaMA-8B与指导模型LLaMA-3.1-8B-Instruct的指令生成效果，分析提示工程对输出的影响。

Result: 提示细微调整可显著改变模型输出，指导模型在指令遵循和格式正确性上表现更优，推理模型未展现明显优势。

Conclusion: 特定场景下指导模型优于推理模型，提示设计中的单句变化即可有效调控模型输出质量。

Abstract: This paper explores the application of Large Language Models (LLMs) and
reasoning to predict Dungeons & Dragons (DnD) player actions and format them as
Avrae Discord bot commands. Using the FIREBALL dataset, we evaluated a
reasoning model, DeepSeek-R1-Distill-LLaMA-8B, and an instruct model,
LLaMA-3.1-8B-Instruct, for command generation. Our findings highlight the
importance of providing specific instructions to models, that even single
sentence changes in prompts can greatly affect the output of models, and that
instruct models are sufficient for this task compared to reasoning models.

</details>


### [20] [LLMs Encode How Difficult Problems Are](https://arxiv.org/abs/2510.18147)
*William Lugoloobi,Chris Russell*

Main category: cs.CL

TL;DR: 研究发现人类标注的难度信号比LLM自动估计更稳定有效，强化学习会放大前者而后者与模型改进脱节。通过方向调控可降低幻觉并提升准确率。


<details>
  <summary>Details</summary>
Motivation: 探究LLM内部是否编码与人类认知一致的难度表征，及其在强化学习训练中的泛化追踪能力。

Method: 在60个模型的不同层和token位置训练线性探测器，使用Easy2HardBench的数学与编码子集进行验证，并通过GRPO训练观察Qwen2.5-Math-1.5B的性能变化。

Result: 人类难度标注线性可解码性极强（AMC: ρ≈0.88）且模型规模扩展性显著，而LLM自动估计的难度信号弱且扩展性差。强化学习过程中人类难度信号与测试准确率正相关，自动估计信号则负相关。

Conclusion: 人类标注提供稳定的难度信号，RL训练会强化该信号；自动生成的难度估计会随模型改进逐渐失效。通过表征调控可有效提升模型性能。

Abstract: Large language models exhibit a puzzling inconsistency: they solve complex
problems yet frequently fail on seemingly simpler ones. We investigate whether
LLMs internally encode problem difficulty in a way that aligns with human
judgment, and whether this representation tracks generalization during
reinforcement learning post-training. We train linear probes across layers and
token positions on 60 models, evaluating on mathematical and coding subsets of
Easy2HardBench. We find that human-labeled difficulty is strongly linearly
decodable (AMC: $\rho \approx 0.88$) and exhibits clear model-size scaling,
whereas LLM-derived difficulty is substantially weaker and scales poorly.
Steering along the difficulty direction reveals that pushing models toward
"easier" representations reduces hallucination and improves accuracy. During
GRPO training on Qwen2.5-Math-1.5B, the human-difficulty probe strengthens and
positively correlates with test accuracy across training steps, while the
LLM-difficulty probe degrades and negatively correlates with performance. These
results suggest that human annotations provide a stable difficulty signal that
RL amplifies, while automated difficulty estimates derived from model
performance become misaligned precisely as models improve. We release probe
code and evaluation scripts to facilitate replication.

</details>


### [21] [Extracting Rule-based Descriptions of Attention Features in Transformers](https://arxiv.org/abs/2510.18148)
*Dan Friedman,Adithya Bhaskar,Alexander Wettig,Danqi Chen*

Main category: cs.CL

TL;DR: 论文提出用基于规则的描述取代现有稀疏特征组合方法，通过匹配输入中的token模式直接调整输出token概率，提升模型可解释性


<details>
  <summary>Details</summary>
Motivation: 现有特征解释方法依赖主观检查样本，容易产生不完整/误导性结论。需要更透明、可验证的规则化描述方法

Method: 1. 从注意力层输出提取SAE特征的规则描述；2. 定义三种规则类型：跳字规则（[加拿大城市]...说→英语）、缺失规则（[蒙特利尔]...说-/-→英语）、计数规则（词频触发）；3. 开发自动提取方法并应用于GPT-2 small

Result: 1. 多数特征可用约100个跳字规则描述；2. 首层即有超1/4特征含缺失规则；3. 发现少量计数规则实例；4. 验证规则解释优于传统特征检查方法

Conclusion: 建立了规则描述的理论框架与提取方法，初步分类了特征行为模式，为后续研究奠定基础。未来需扩展规则类型验证并开发更系统的分类体系

Abstract: Mechanistic interpretability strives to explain model behavior in terms of
bottom-up primitives. The leading paradigm is to express hidden states as a
sparse linear combination of basis vectors, called features. However, this only
identifies which text sequences (exemplars) activate which features; the actual
interpretation of features requires subjective inspection of these exemplars.
This paper advocates for a different solution: rule-based descriptions that
match token patterns in the input and correspondingly increase or decrease the
likelihood of specific output tokens. Specifically, we extract rule-based
descriptions of SAE features trained on the outputs of attention layers. While
prior work treats the attention layers as an opaque box, we describe how it may
naturally be expressed in terms of interactions between input and output
features, of which we study three types: (1) skip-gram rules of the form
"[Canadian city]... speaks --> English", (2) absence rules of the form
"[Montreal]... speaks -/-> English," and (3) counting rules that toggle only
when the count of a word exceeds a certain value or the count of another word.
Absence and counting rules are not readily discovered by inspection of
exemplars, where manual and automatic descriptions often identify misleading or
incomplete explanations. We then describe a simple approach to extract these
types of rules automatically from a transformer, and apply it to GPT-2 small.
We find that a majority of features may be described well with around 100
skip-gram rules, though absence rules are abundant even as early as the first
layer (in over a fourth of features). We also isolate a few examples of
counting rules. This paper lays the groundwork for future research into
rule-based descriptions of features by defining them, showing how they may be
extracted, and providing a preliminary taxonomy of some of the behaviors they
represent.

</details>


### [22] [Automatic Prompt Generation via Adaptive Selection of Prompting Techniques](https://arxiv.org/abs/2510.18162)
*Yohei Ikenoue,Hitomi Tashiro,Shigeru Kuroyanagi*

Main category: cs.CL

TL;DR: 提出基于任务相似性聚类与知识库的自适应提示生成方法，显著提升LLM任务表现


<details>
  <summary>Details</summary>
Motivation: 解决传统提示工程需要专业知识且依赖预设模板的痛点，降低非专家使用LLM的门槛

Method: 构建任务聚类与提示技术关联的知识库，通过语义匹配动态生成组合式提示

Result: 在BIG-Bench Extra Hard的23项任务中，算术平均与调和平均指标均超越基线方法

Conclusion: 建立了标准化提示生成框架，为非专家有效利用LLM提供了系统化解决方案

Abstract: Prompt engineering is crucial for achieving reliable and effective outputs
from large language models (LLMs), but its design requires specialized
knowledge of prompting techniques and a deep understanding of target tasks. To
address this challenge, we propose a novel method that adaptively selects
task-appropriate prompting techniques based on users' abstract task
descriptions and automatically generates high-quality prompts without relying
on pre-existing templates or frameworks. The proposed method constructs a
knowledge base that associates task clusters, characterized by semantic
similarity across diverse tasks, with their corresponding prompting techniques.
When users input task descriptions, the system assigns them to the most
relevant task cluster and dynamically generates prompts by integrating
techniques drawn from the knowledge base. An experimental evaluation of the
proposed method on 23 tasks from BIG-Bench Extra Hard (BBEH) demonstrates
superior performance compared with standard prompts and existing automatic
prompt-generation tools, as measured by both arithmetic and harmonic mean
scores. This research establishes a foundation for streamlining and
standardizing prompt creation, enabling non-experts to effectively leverage
LLMs.

</details>


### [23] [CMT-Bench: Cricket Multi-Table Generation Benchmark for Probing Robustness in Large Language Models](https://arxiv.org/abs/2510.18173)
*Ritam Upadhyay,Naman Ahuja,Rishabh Baral,Aparna Garimella,Vivek Gupta*

Main category: cs.CL

TL;DR: 论文提出CMT-Bench基准测试，揭示当前LLM在动态文本到表格生成任务中存在鲁棒性缺陷，尤其在提取摘要缺失、长上下文处理和实体变化时性能显著下降，强调需优先评估模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到表格系统依赖计算成本高的提示工程，且无法有效评估模型对动态叙事演变的真实推理能力，需构建新基准测试模型鲁棒性。

Method: 基于板球评论构建CMT-Bench，通过提取线索消融、时间前缀测试和实体扰动三个语义保留维度，系统评估不同LLM在动态表格生成中的稳定性。

Result: 实验显示：1) 无提取摘要时准确率下降达25%；2) 输入长度增加导致性能单调递减；3) 实体形式变化引发持续准确率下降，错误模式偏移显示推理过程存在本质缺陷。

Conclusion: 当前LLM在动态表格生成任务中表现脆弱，开发高效方法需以鲁棒性评估为前提，该研究为改进模型推理能力提供了诊断框架。

Abstract: LLM Driven text-to-table (T2T) systems often rely on extensive
prompt-engineering or iterative event extraction in code-parsable formats,
which boosts scores but are computationally expensive and obscure how models
actually reason over temporal evolving narratives to summarise key information.
We present CMT-Bench, a diagnostic benchmark built from live cricket commentary
that requires dynamic table generation across two evolving schemas under a
dense, rule-governed policy. CMT-Bench is designed to probe robustness via
three semantics-preserving dimensions: (i) extractive-cue ablation to separate
extractive shortcuts from state tracking, (ii) temporal prefixing to test
long-context stability, and (iii) entity-form perturbations (anonymization,
outof-distribution substitutions, role-entangling paraphrases) to assess
sensitivity to surface variation. Across diverse long-context stateof-the-art
LLMs, we find large drops without extractive summaries, monotonic degradation
with input length, and consistent accuracy drop under entity-form changes.
Complementary distributional tests confirm significant shifts in numeric error
patterns, indicating drift in reasoning rather than mere noise. Our results
show that current LLMs are brittle in dynamic Textto-table generation,
motivating robustness-first evaluation as a prerequisite for developing
efficient and scalable approaches for this task.

</details>


### [24] [Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge](https://arxiv.org/abs/2510.18196)
*Yoshinari Fujinuma*

Main category: cs.CL

TL;DR: 通过对比解码方法缓解LLM评分范围偏差，实现与人类判断相关性11.3%的提升


<details>
  <summary>Details</summary>
Motivation: LLM作为评估者时存在评分范围敏感性问题，不同评分范围间存在系统性偏差

Method: 提出基于对比解码的偏差缓解方法

Result: 在不同评分范围下与人类判断的Spearman相关性平均提升11.3%

Conclusion: 成功降低LLM评估偏差，提升评分可靠性，且方法适用于同系列模型

Abstract: Large Language Models (LLMs) are commonly used as evaluators in various
applications, but the reliability of the outcomes remains a challenge. One such
challenge is using LLMs-as-judges for direct assessment, i.e., assigning scores
from a specified range without any references. We first show that this
challenge stems from LLM judge outputs being associated with score range bias,
i.e., LLM judge outputs are highly sensitive to pre-defined score ranges,
preventing the search for optimal score ranges. We also show that similar
biases exist among models from the same family. We then mitigate this bias
through contrastive decoding, achieving up to 11.3% relative improvement on
average in Spearman correlation with human judgments across different score
ranges.

</details>


### [25] [MARCUS: An Event-Centric NLP Pipeline that generates Character Arcs from Narratives](https://arxiv.org/abs/2510.18201)
*Sriharsh Bhyravajjula,Ujwal Narayan,Manish Shrivastava*

Main category: cs.CL

TL;DR: 提出MARCUS NLP流程，通过事件提取、情感分析和关系建模生成图形化角色弧线，应用于《哈利·波特》和《指环王》的定量分析。


<details>
  <summary>Details</summary>
Motivation: 解决文学研究中角色弧线理论概念缺乏量化方法的问题，将抽象叙事结构转化为可计算模型以支持跨文本分析应用。

Method: 开发多阶段NLP流程：1) 事件与参与者提取 2) 隐含情感/情绪识别 3) 关系动态建模 4) 可视化弧线生成，通过经典奇幻文本验证。

Result: 成功构建可解释的角色关系演变图谱，验证流程有效性，同时揭示情感标注粒度、长程依赖建模等技术挑战。

Conclusion: 首次实现角色弧线的系统量化，为叙事分析、类型文学研究及创作辅助工具开发提供技术基础，需进一步优化上下文建模能力。

Abstract: Character arcs are important theoretical devices employed in literary studies
to understand character journeys, identify tropes across literary genres, and
establish similarities between narratives. This work addresses the novel task
of computationally generating event-centric, relation-based character arcs from
narratives. Providing a quantitative representation for arcs brings tangibility
to a theoretical concept and paves the way for subsequent applications. We
present MARCUS (Modelling Arcs for Understanding Stories), an NLP pipeline that
extracts events, participant characters, implied emotion, and sentiment to
model inter-character relations. MARCUS tracks and aggregates these relations
across the narrative to generate character arcs as graphical plots. We generate
character arcs from two extended fantasy series, Harry Potter and Lord of the
Rings. We evaluate our approach before outlining existing challenges,
suggesting applications of our pipeline, and discussing future work.

</details>


### [26] [DelvePO: Direction-Guided Self-Evolving Framework for Flexible Prompt Optimization](https://arxiv.org/abs/2510.18257)
*Tao Tao,Guanghui Zhu,Lang Guo,Hongyi Chen,Chunfeng Yuan,Yihua Huang*

Main category: cs.CL

TL;DR: 提出DelvePO框架，通过解耦提示组件和引入工作记忆机制实现自进化提示优化，显著提升提示性能和跨任务迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法依赖LLM随机重写，容易陷入局部最优且性能不稳定，缺乏跨任务迁移能力。

Method: 1. 将提示解耦为可独立优化组件 2. 引入工作记忆存储关键洞察 3. 基于方向指导的自进化流程 4. 任务无关的通用优化框架

Result: 在DeepSeek/Qwen/GPT系列模型上验证，跨NLP/代码生成/数学推理等任务表现优于SOTA方法，平均提升3.5%-8.2%

Conclusion: DelvePO通过系统性优化机制有效解决了传统方法的局限性，为LLM提示工程提供了可扩展的解决方案。

Abstract: Prompt Optimization has emerged as a crucial approach due to its capabilities
in steering Large Language Models to solve various tasks. However, current
works mainly rely on the random rewriting ability of LLMs, and the optimization
process generally focus on specific influencing factors, which makes it easy to
fall into local optimum. Besides, the performance of the optimized prompt is
often unstable, which limits its transferability in different tasks. To address
the above challenges, we propose $\textbf{DelvePO}$
($\textbf{D}$irection-Guid$\textbf{e}$d Se$\textbf{l}$f-E$\textbf{v}$olving
Framework for Fl$\textbf{e}$xible $\textbf{P}$rompt $\textbf{O}$ptimization), a
task-agnostic framework to optimize prompts in self-evolve manner. In our
framework, we decouple prompts into different components that can be used to
explore the impact that different factors may have on various tasks. On this
basis, we introduce working memory, through which LLMs can alleviate the
deficiencies caused by their own uncertainties and further obtain key insights
to guide the generation of new prompts. Extensive experiments conducted on
different tasks covering various domains for both open- and closed-source LLMs,
including DeepSeek-R1-Distill-Llama-8B, Qwen2.5-7B-Instruct and GPT-4o-mini.
Experimental results show that DelvePO consistently outperforms previous SOTA
methods under identical experimental settings, demonstrating its effectiveness
and transferability across different tasks.

</details>


### [27] [Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs in Multimodal LLMs](https://arxiv.org/abs/2510.18279)
*Yanhong Li,Zixuan Lan,Jiawei Zhou*

Main category: cs.CL

TL;DR: 将长文本渲染为图像输入LLMs可减少近半token使用且保持性能


<details>
  <summary>Details</summary>
Motivation: 利用LLMs的视觉处理能力压缩文本输入，减少解码器负担

Method: 将长文本渲染为单一图像直接输入解码器型LLMs

Result: 在RULER和CNN/DailyMail基准上节省近50% token且任务性能无下降

Conclusion: 文本图像化方法为LLMs输入压缩提供了有效新途径

Abstract: Large language models (LLMs) and their multimodal variants can now process
visual inputs, including images of text. This raises an intriguing question:
can we compress textual inputs by feeding them as images to reduce token usage
while preserving performance? In this paper, we show that visual text
representations are a practical and surprisingly effective form of input
compression for decoder LLMs. We exploit the idea of rendering long text inputs
as a single image and provide it directly to the model. This leads to
dramatically reduced number of decoder tokens required, offering a new form of
input compression. Through experiments on two distinct benchmarks RULER
(long-context retrieval) and CNN/DailyMail (document summarization) we
demonstrate that this text-as-image method yields substantial token savings
(often nearly half) without degrading task performance.

</details>


### [28] [BrailleLLM: Braille Instruction Tuning with Large Language Models for Braille Domain Tasks](https://arxiv.org/abs/2510.18288)
*Tianyuan Huang,Zepeng Zhu,Hangdi Xing,Zirui Shao,Zhi Yu,Chaoxiong Yang,Jiaxian He,Xiaozhong Liu,Jiajun Bu*

Main category: cs.CL

TL;DR: 论文提出基于盲文知识微调(BKFT)的方法，构建英汉盲文混合数据集并实现多任务统一处理，显著提升盲文翻译性能。


<details>
  <summary>Details</summary>
Motivation: 解决盲文处理中的数据稀缺、混合文本歧义问题，以及传统微调方法在盲文任务中表现不佳的现状。

Method: 1. 构建含数学公式的EBMD/CBMD数据集 2. 提出语法树增强方法 3. 设计基于指令调整的BKFT训练范式

Result: BKFT相比传统微调在盲文翻译任务中取得显著性能提升（实验验证），并开源多语言数据集支持后续研究。

Conclusion: 该研究为低资源多语言盲文处理建立了方法论基础，通过知识引导的微调机制实现了跨任务的统一盲文信息处理。

Abstract: Braille plays a vital role in education and information accessibility for
visually impaired individuals. However, Braille information processing faces
challenges such as data scarcity and ambiguities in mixed-text contexts. We
construct English and Chinese Braille Mixed Datasets (EBMD/CBMD) with
mathematical formulas to support diverse Braille domain research, and propose a
syntax tree-based augmentation method tailored for Braille data. To address the
underperformance of traditional fine-tuning methods in Braille-related tasks,
we investigate Braille Knowledge-Based Fine-Tuning (BKFT), which reduces the
learning difficulty of Braille contextual features. BrailleLLM employs BKFT via
instruction tuning to achieve unified Braille translation, formula-to-Braille
conversion, and mixed-text translation. Experiments demonstrate that BKFT
achieves significant performance improvements over conventional fine-tuning in
Braille translation scenarios. Our open-sourced datasets and methodologies
establish a foundation for low-resource multilingual Braille research.

</details>


### [29] [Food4All: A Multi-Agent Framework for Real-time Free Food Discovery with Integrated Nutritional Metadata](https://arxiv.org/abs/2510.18289)
*Zhengqing Yuan,Yiyang Li,Weixiang Sun,Zheyuan Zhang,Kaiwen Shi,Keerthiram Murugesan,Yanfang Ye*

Main category: cs.CL

TL;DR: Food4All框架通过异构数据整合、强化学习算法和在线反馈机制，实现实时、情境感知的免费食品资源智能检索系统


<details>
  <summary>Details</summary>
Motivation: 针对现有食品援助系统存在静态检索结果不完整、LLM建议模糊、推荐系统忽视生存需求等缺陷，特别是弱势群体难以获取紧急食品资源的问题

Method: 1) 聚合政府数据库和社交媒体数据构建动态资源池；2) 基于案例训练的轻量级强化学习算法优化地理可达性和营养匹配；3) 在线反馈循环动态调整检索策略

Result: 框架实现营养标注的实时食品导航，提升资源获取效率，适应无家可归/药物成瘾/数字文盲等特殊群体的生存需求

Conclusion: 该系统为解决食品不安全及其伴生健康风险提供了可扩展的智能支持方案

Abstract: Food insecurity remains a persistent public health emergency in the United
States, tightly interwoven with chronic disease, mental illness, and opioid
misuse. Yet despite the existence of thousands of food banks and pantries,
access remains fragmented: 1) current retrieval systems depend on static
directories or generic search engines, which provide incomplete and
geographically irrelevant results; 2) LLM-based chatbots offer only vague
nutritional suggestions and fail to adapt to real-world constraints such as
time, mobility, and transportation; and 3) existing food recommendation systems
optimize for culinary diversity but overlook survival-critical needs of
food-insecure populations, including immediate proximity, verified
availability, and contextual barriers. These limitations risk leaving the most
vulnerable individuals, those experiencing homelessness, addiction, or digital
illiteracy, unable to access urgently needed resources. To address this, we
introduce Food4All, the first multi-agent framework explicitly designed for
real-time, context-aware free food retrieval. Food4All unifies three
innovations: 1) heterogeneous data aggregation across official databases,
community platforms, and social media to provide a continuously updated pool of
food resources; 2) a lightweight reinforcement learning algorithm trained on
curated cases to optimize for both geographic accessibility and nutritional
correctness; and 3) an online feedback loop that dynamically adapts retrieval
policies to evolving user needs. By bridging information acquisition, semantic
analysis, and decision support, Food4All delivers nutritionally annotated and
guidance at the point of need. This framework establishes an urgent step toward
scalable, equitable, and intelligent systems that directly support populations
facing food insecurity and its compounding health risks.

</details>


### [30] [From Retrieval to Generation: Unifying External and Parametric Knowledge for Medical Question Answering](https://arxiv.org/abs/2510.18297)
*Lei Li,Xiao Zhou,Yingying Zhang,Xian Wu*

Main category: cs.CL

TL;DR: 提出MedRGAG框架，通过整合外部检索与内部生成知识提升医学问答可靠性


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法存在检索噪声/不完整问题，GAG方法存在幻觉信息风险，两者均影响答案可靠性

Method: 设计KGCC模块补充缺失知识，KADS模块动态选择最优文档组合

Result: 在5个医学QA基准上超越MedRAG 12.5%和MedGENIE 4.5%

Conclusion: 检索与生成协同增强策略有效提升知识密集型推理任务性能

Abstract: Medical question answering (QA) requires extensive access to domain-specific
knowledge. A promising direction is to enhance large language models (LLMs)
with external knowledge retrieved from medical corpora or parametric knowledge
stored in model parameters. Existing approaches typically fall into two
categories: Retrieval-Augmented Generation (RAG), which grounds model reasoning
on externally retrieved evidence, and Generation-Augmented Generation (GAG),
which depends solely on the models internal knowledge to generate contextual
documents. However, RAG often suffers from noisy or incomplete retrieval, while
GAG is vulnerable to hallucinated or inaccurate information due to
unconstrained generation. Both issues can mislead reasoning and undermine
answer reliability. To address these challenges, we propose MedRGAG, a unified
retrieval-generation augmented framework that seamlessly integrates external
and parametric knowledge for medical QA. MedRGAG comprises two key modules:
Knowledge-Guided Context Completion (KGCC), which directs the generator to
produce background documents that complement the missing knowledge revealed by
retrieval; and Knowledge-Aware Document Selection (KADS), which adaptively
selects an optimal combination of retrieved and generated documents to form
concise yet comprehensive evidence for answer generation. Extensive experiments
on five medical QA benchmarks demonstrate that MedRGAG achieves a 12.5%
improvement over MedRAG and a 4.5% gain over MedGENIE, highlighting the
effectiveness of unifying retrieval and generation for knowledge-intensive
reasoning. Our code and data are publicly available at
https://anonymous.4open.science/r/MedRGAG

</details>


### [31] [ECG-LLM -- training and evaluation of domain-specific large language models for electrocardiography](https://arxiv.org/abs/2510.18339)
*Lara Ahrens,Wilhelm Haverkamp,Nils Strodthoff*

Main category: cs.CL

TL;DR: 领域适应的开放权重大模型在医疗应用中展现竞争力，微调模型在多项评估中超越基础模型，RAG和专有模型在复杂查询中表现更优


<details>
  <summary>Details</summary>
Motivation: 探究医疗领域LLMs本地部署的优化策略，在保护隐私前提下实现与通用模型相媲美的性能

Method: 采用多层次评估框架，对比微调模型（Llama 3.1 70B）、检索增强生成（RAG）与Claude 3.7在心电图领域的表现

Result: 微调模型在客观指标领先但人类评估偏好专有模型，领域适应使模型性能提升85%-120%，不同评估方法呈现显著性能差异

Conclusion: 领域特定适应策略可实现隐私安全的本地临床解决方案，评估方法的选择会显著影响模型性能判断

Abstract: Domain-adapted open-weight large language models (LLMs) offer promising
healthcare applications, from queryable knowledge bases to multimodal
assistants, with the crucial advantage of local deployment for privacy
preservation. However, optimal adaptation strategies, evaluation methodologies,
and performance relative to general-purpose LLMs remain poorly characterized.
We investigated these questions in electrocardiography, an important area of
cardiovascular medicine, by finetuning open-weight models on domain-specific
literature and implementing a multi-layered evaluation framework comparing
finetuned models, retrieval-augmented generation (RAG), and Claude Sonnet 3.7
as a representative general-purpose model. Finetuned Llama 3.1 70B achieved
superior performance on multiple-choice evaluations and automatic text metrics,
ranking second to Claude 3.7 in LLM-as-a-judge assessments. Human expert
evaluation favored Claude 3.7 and RAG approaches for complex queries. Finetuned
models significantly outperformed their base counterparts across nearly all
evaluation modes. Our findings reveal substantial performance heterogeneity
across evaluation methodologies, underscoring assessment complexity.
Nevertheless, domain-specific adaptation through finetuning and RAG achieves
competitive performance with proprietary models, supporting the viability of
privacy-preserving, locally deployable clinical solutions.

</details>


### [32] [Combining Distantly Supervised Models with In Context Learning for Monolingual and Cross-Lingual Relation Extraction](https://arxiv.org/abs/2510.18344)
*Vipul Rathore,Malik Hammad Faisal,Parag Singla,Mausam*

Main category: cs.CL

TL;DR: 提出HYDRE框架，通过结合DSRE模型候选关系筛选与动态样本检索策略，显著提升英语及低资源语言关系抽取性能。


<details>
  <summary>Details</summary>
Motivation: 现有DSRE模型依赖任务特定训练，与大语言模型结合不足，且噪声标注易导致关系语义学习偏差。需设计可靠方法融合两者优势。

Method: 1. 使用DSRE模型筛选测试句的top-k候选关系 2. 动态检索训练数据中的可靠句级样本 3. 构建LLM提示生成最终关系预测 4. 扩展到跨语言场景（利用英语数据支持低资源语言）

Result: 英语任务提升20 F1值；在4种印度语言（Oriya等）新基准上平均提升17 F1值，消融实验验证动态检索策略有效性。

Conclusion: HYDRE有效整合DSRE模型与LLM的上下文学习能力，动态检索策略显著降低噪声影响，为低资源语言关系抽取提供实用解决方案。

Abstract: Distantly Supervised Relation Extraction (DSRE) remains a long-standing
challenge in NLP, where models must learn from noisy bag-level annotations
while making sentence-level predictions. While existing state-of-the-art (SoTA)
DSRE models rely on task-specific training, their integration with in-context
learning (ICL) using large language models (LLMs) remains underexplored. A key
challenge is that the LLM may not learn relation semantics correctly, due to
noisy annotation.
  In response, we propose HYDRE -- HYbrid Distantly Supervised Relation
Extraction framework. It first uses a trained DSRE model to identify the top-k
candidate relations for a given test sentence, then uses a novel dynamic
exemplar retrieval strategy that extracts reliable, sentence-level exemplars
from training data, which are then provided in LLM prompt for outputting the
final relation(s).
  We further extend HYDRE to cross-lingual settings for RE in low-resource
languages. Using available English DSRE training data, we evaluate all methods
on English as well as a newly curated benchmark covering four diverse
low-resource Indic languages -- Oriya, Santali, Manipuri, and Tulu. HYDRE
achieves up to 20 F1 point gains in English and, on average, 17 F1 points on
Indic languages over prior SoTA DSRE models. Detailed ablations exhibit HYDRE's
efficacy compared to other prompting strategies.

</details>


### [33] [KrishokBondhu: A Retrieval-Augmented Voice-Based Agricultural Advisory Call Center for Bengali Farmers](https://arxiv.org/abs/2510.18355)
*Mohd Ruhul Ameen,Akif Islam,Farjana Aktar,M. Saifuzzaman Rafat*

Main category: cs.CL

TL;DR: 基于RAG框架的孟加拉语语音农业咨询系统KrishokBondhu，通过整合文档解析、语义检索和语音交互技术，为农民提供实时专家级指导


<details>
  <summary>Details</summary>
Motivation: 解决孟加拉国农民难以及时获取专业农业指导的问题。现有系统在信息完整性、语境适配性和多语言支持方面存在不足，特别是针对非英语使用者的服务缺口明显

Method: 1) 整合权威农业手册构建知识库；2) OCR和文档解析技术数字化处理；3) 向量数据库实现语义检索；4) 端到端语音交互流程（语音识别→RAG检索→Gemma模型生成→语音合成）

Result: 试点评估显示72.7%的农业咨询获得高质量回答，综合评分4.53/5（较基准系统提升44.7%），语境丰富度提升367%，回答完整度翻倍。语义相似度分析证实检索质量与回答效果正相关

Conclusion: 系统成功验证了RAG技术与呼叫中心、多语言语音交互整合的可行性，为构建全AI驱动的农业咨询生态系统奠定了基础，特别在提升发展中国家边缘化群体的技术可及性方面具有示范价值

Abstract: In Bangladesh, many farmers continue to face challenges in accessing timely,
expert-level agricultural guidance. This paper presents KrishokBondhu, a
voice-enabled, call-centre-integrated advisory platform built on a
Retrieval-Augmented Generation (RAG) framework, designed specifically for
Bengali-speaking farmers. The system aggregates authoritative agricultural
handbooks, extension manuals, and NGO publications; applies Optical Character
Recognition (OCR) and document-parsing pipelines to digitize and structure the
content; and indexes this corpus in a vector database for efficient semantic
retrieval. Through a simple phone-based interface, farmers can call the system
to receive real-time, context-aware advice: speech-to-text converts the Bengali
query, the RAG module retrieves relevant content, a large language model (Gemma
3-4B) generates a context-grounded response, and text-to-speech delivers the
answer in natural spoken Bengali. In a pilot evaluation, KrishokBondhu produced
high-quality responses for 72.7% of diverse agricultural queries covering crop
management, disease control, and cultivation practices. Compared to the
KisanQRS benchmark, the system achieved a composite score of 4.53 (vs. 3.13) on
a 5-point scale, a 44.7% improvement, with especially large gains in contextual
richness (+367%) and completeness (+100.4%), while maintaining comparable
relevance and technical specificity. Semantic similarity analysis further
revealed a strong correlation between retrieved context and answer quality,
emphasizing the importance of grounding generative responses in curated
documentation. KrishokBondhu demonstrates the feasibility of integrating
call-centre accessibility, multilingual voice interaction, and modern RAG
techniques to deliver expert-level agricultural guidance to remote Bangladeshi
farmers, paving the way toward a fully AI-driven agricultural advisory
ecosystem.

</details>


### [34] [KoSimpleQA: A Korean Factuality Benchmark with an Analysis of Reasoning LLMs](https://arxiv.org/abs/2510.18368)
*Donghyeon Ko,Yeguk Jin,Kyubyung Chae,Byungwook Lee,Chansong Jo,Sookyo In,Jaehong Lee,Taesup Kim,Donghyun Kwak*

Main category: cs.CL

TL;DR: KoSimpleQA是一个针对韩语文化知识的事实性评估基准，包含1000个高难度短问题。测试显示现有最佳模型正确率仅33.7%，且其性能排名与英文基准存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 解决现有评估基准在韩国文化知识覆盖上的不足，提供专门针对韩语的事实性QA评估工具，揭示跨语言模型表现的差异性。

Method: 构建含1000个明确答案的韩语事实性问题集，对多种规模的开源韩语LLM进行系统测试，并分析推理能力对回答质量的影响。

Result: 1. 模型平均表现显著低于英文基准 2. 推理能力提升知识提取准确性（+8.2%）和弃答能力（错误率降低15%）3. 韩英基准排名相关系数仅0.31

Conclusion: 文化特异性评估基准对全面衡量LLM能力至关重要，推理机制能有效提升事实性回答的可靠性，当前韩语LLM存在显著的知识盲区。

Abstract: We present $\textbf{Korean SimpleQA (KoSimpleQA)}$, a benchmark for
evaluating factuality in large language models (LLMs) with a focus on Korean
cultural knowledge. KoSimpleQA is designed to be challenging yet easy to grade,
consisting of 1,000 short, fact-seeking questions with unambiguous answers. We
conduct a comprehensive evaluation across a diverse set of open-source LLMs of
varying sizes that support Korean, and find that even the strongest model
generates correct answer only 33.7% of the time, underscoring the challenging
nature of KoSimpleQA. Notably, performance rankings on KoSimpleQA differ
substantially from those on the English SimpleQA, highlighting the unique value
of our dataset. Furthermore, our analysis of reasoning LLMs shows that engaging
reasoning capabilities in the factual QA task can both help models better
elicit their latent knowledge and improve their ability to abstain when
uncertain. KoSimpleQA can be found at
https://anonymous.4open.science/r/KoSimpleQA-62EB.

</details>


### [35] [Towards Fair ASR For Second Language Speakers Using Fairness Prompted Finetuning](https://arxiv.org/abs/2510.18374)
*Monorama Swain,Bubai Maji,Jagabandhu Mishra,Markus Schedl,Anders Søgaard,Jesper Rindom Jensen*

Main category: cs.CL

TL;DR: 提出通过轻量级适配器微调结合多种优化方法，显著提升英语ASR系统在26种口音群体中的公平性，同时保持整体识别准确率。


<details>
  <summary>Details</summary>
Motivation: 现有Whisper和Seamless-M4T模型在不同口音群体中词错率差异高达58%，存在显著公平性缺陷。

Method: 融合经验风险最小化（ERM）与谱解耦（SD）、组分布鲁棒优化（Group-DRO）、不变风险最小化（IRM）的混合微调框架。

Result: 宏观平均词错率相对原始大模型提升58.7%（Whisper）和58.5%（SeamlessM4T），相比传统ERM微调提升9.7%和7.8%。

Conclusion: 该方法成功平衡ASR系统的公平性与准确性，为多口音语音识别系统开发提供了有效解决方案。

Abstract: In this work, we address the challenge of building fair English ASR systems
for second-language speakers. Our analysis of widely used ASR models, Whisper
and Seamless-M4T, reveals large fluctuations in word error rate (WER) across 26
accent groups, indicating significant fairness gaps. To mitigate this, we
propose fairness-prompted finetuning with lightweight adapters, incorporating
Spectral Decoupling (SD), Group Distributionally Robust Optimization
(Group-DRO), and Invariant Risk Minimization (IRM). Our proposed fusion of
traditional empirical risk minimization (ERM) with cross-entropy and
fairness-driven objectives (SD, Group DRO, and IRM) enhances fairness across
accent groups while maintaining overall recognition accuracy. In terms of
macro-averaged word error rate, our approach achieves a relative improvement of
58.7% and 58.5% over the large pretrained Whisper and SeamlessM4T, and 9.7% and
7.8% over them, finetuning with standard empirical risk minimization with
cross-entropy loss.

</details>


### [36] [MENTOR: A Reinforcement Learning Framework for Model Enhancement via Teacher-Optimized Rewards in Small Models](https://arxiv.org/abs/2510.18383)
*ChangSu Choi,Hoyun Song,Dongyeon Kim,WooHyeon Jung,Minkyung Cho,Sunjin Park,NohHyeob Bae,Seona Yu,KyungTae Lim*

Main category: cs.CL

TL;DR: 提出MENTOR框架，通过结合强化学习与教师引导蒸馏，提升小语言模型在工具使用任务中的泛化能力和策略水平


<details>
  <summary>Details</summary>
Motivation: 现有监督微调方法存在泛化能力差问题，标准强化学习在稀疏奖励下难以有效指导模型探索

Method: 使用RL过程学习可泛化策略，并利用教师轨迹构建密集复合奖励函数提供细粒度指导

Result: 实验证明MENTOR在跨领域泛化和战略能力上显著优于监督微调与标准稀疏奖励RL基线

Conclusion: MENTOR通过教师引导的密集奖励机制有效解决了模型蒸馏中的探索低效和策略次优问题

Abstract: Distilling the tool-using capabilities of large language models (LLMs) into
smaller, more efficient small language models (SLMs) is a key challenge for
their practical application. The predominant approach, supervised fine-tuning
(SFT), suffers from poor generalization as it trains models to imitate a static
set of teacher trajectories rather than learn a robust methodology. While
reinforcement learning (RL) offers an alternative, the standard RL using sparse
rewards fails to effectively guide SLMs, causing them to struggle with
inefficient exploration and adopt suboptimal strategies. To address these
distinct challenges, we propose MENTOR, a framework that synergistically
combines RL with teacher-guided distillation. Instead of simple imitation,
MENTOR employs an RL-based process to learn a more generalizable policy through
exploration. In addition, to solve the problem of reward sparsity, it uses a
teacher's reference trajectory to construct a dense, composite teacher-guided
reward that provides fine-grained guidance. Extensive experiments demonstrate
that MENTOR significantly improves the cross-domain generalization and
strategic competence of SLMs compared to both SFT and standard sparse-reward RL
baselines.

</details>


### [37] [Adamas: Hadamard Sparse Attention for Efficient Long-Context Inference](https://arxiv.org/abs/2510.18413)
*Siyuan Yan,Guo-Qing Jiang,Yuchen Zhang,Xiaoxing Ma,Ran Zhu,Chun Cao,Jingwei Xu*

Main category: cs.CL

TL;DR: Adamas是一种高效稀疏注意力机制，通过Hadamard变换、分桶压缩和曼哈顿距离估计技术，在仅64-token预算下匹配全注意力精度，支持8倍稀疏度并实现4.4倍加速。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文场景下自注意力机制二次计算成本过高导致的延迟问题，改进传统稀疏注意力方法因启发式模式导致的KV对召回不足与精度下降。

Method: 采用Hadamard变换降维、数据分桶及2-bit压缩生成紧凑表示，结合曼哈顿距离估计实现高效top-k选择。

Result: 64-token预算时精度匹配全注意力，128时接近无损；支持8倍稀疏度，32K序列实现4.4倍自注意力加速和1.5倍端到端加速，困惑度持平或优于全注意力。

Conclusion: Adamas在极端稀疏条件下维持精度与效率的突破性表现，为长上下文推理任务提供了新的高效解决方案。

Abstract: Large language models (LLMs) now support context windows of hundreds of
thousands to millions of tokens, enabling applications such as long-document
summarization, large-scale code synthesis, multi-document question answering
and persistent multi-turn dialogue. However, such extended contexts exacerbate
the quadratic cost of self-attention, leading to severe latency in
autoregressive decoding. Existing sparse attention methods alleviate these
costs but rely on heuristic patterns that struggle to recall critical key-value
(KV) pairs for each query, resulting in accuracy degradation. We introduce
Adamas, a lightweight yet highly accurate sparse attention mechanism designed
for long-context inference. Adamas applies the Hadamard transform,
bucketization and 2-bit compression to produce compact representations, and
leverages Manhattan-distance estimation for efficient top-k selections.
Experiments show that Adamas matches the accuracy of full attention with only a
64-token budget, achieves near-lossless performance at 128, and supports up to
8x higher sparsity than prior state-of-the-art (SOTA) methods while delivering
up to 4.4x self-attention and 1.5x end-to-end speedups on 32K-length sequences.
Remarkably, Adamas attains comparable or even lower perplexity than full
attention, underscoring its effectiveness in maintaining accuracy under
aggressive sparsity.

</details>


### [38] [Chain-of-Conceptual-Thought: Eliciting the Agent to Deeply Think within the Response](https://arxiv.org/abs/2510.18434)
*Qingqing Gu,Dan Wang,Yue Zhao,Xiaoyu Wang,Zhonglin Jiang,Yong Chen,Hongyan Li,Luo Ji*

Main category: cs.CL

TL;DR: 提出Chain of Conceptual Thought (CoCT)新范式，通过概念链引导LLM深度思考，在开放领域对话任务中效果超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统CoT在开放领域任务中表现受限，因缺乏明确推理步骤。需通过概念分层提升LLM的战略性思考能力。

Method: 1. 首先生成概念标签（情绪/策略/主题）
2. 基于概念展开详细内容生成
3. 允许概念链在单轮对话中迭代深化

Result: 在情感支持对话任务中，CoCT自动评估提升15%，人工评估提升23%，全面超越Self-Refine、ToT等基线方法。

Conclusion: CoCT为LLM提供了有效的开放任务处理范式，未来可拓展至医疗咨询、教育培训等复杂对话场景。

Abstract: Chain-of-Thought (CoT) is widely applied to improve the LLM capability in
math, coding and reasoning tasks. However, its performance is limited for
open-domain tasks since there are no clearly defined reasoning steps or logical
transitions. To mitigate such challenges, we propose another prompt-based
paradigm called Chain of Conceptual Thought (CoCT), where the LLM first tags a
concept, then generates the detailed content. The chain of concepts is allowed
within the utterance, encouraging the LLM's deep and strategic thinking. We
experiment with this paradigm in daily and emotional support conversations
where the concept is comprised of emotions, strategies and topics. Automatic,
human and model evaluations suggest that CoCT surpasses baselines such as
Self-Refine, ECoT, ToT, SoT and RAG, suggesting a potential effective
prompt-based paradigm of LLM for a wider scope of tasks.

</details>


### [39] [Grounding or Guessing? Visual Signals for Detecting Hallucinations in Sign Language Translation](https://arxiv.org/abs/2510.18439)
*Yasser Hamidullah,Koel Dutta Chowdury,Yusser Al-Ghussin,Shakib Yazdani,Cennet Oguz,Josef van Genabith,Cristina España-Bonet*

Main category: cs.CL

TL;DR: 提出基于视觉信息的token级可靠性指标，量化手语翻译模型中视觉信息利用率，有效预测幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 手语翻译依赖视频证据，无词干注释模型因缺乏中间对齐监督，易依赖语言先验产生视觉无关幻觉。

Method: 结合特征敏感性(视频遮蔽后模型内部变化)与反事实信号(原始/修改视频的概率差)，构建句子级视觉可靠性评分。

Result: 在PHOENIX-2014T和CSL-Daily数据集验证：可靠性指标可预测幻觉率、跨架构泛化、视觉降级时降低，结合文本信号进一步提升检测效果。

Conclusion: 建立可解释的可靠性诊断工具，揭示无词干模型易幻觉机制，为多模态生成提供鲁棒性检测基础。

Abstract: Hallucination, where models generate fluent text unsupported by visual
evidence, remains a major flaw in vision-language models and is particularly
critical in sign language translation (SLT). In SLT, meaning depends on precise
grounding in video, and gloss-free models are especially vulnerable because
they map continuous signer movements directly into natural language without
intermediate gloss supervision that serves as alignment. We argue that
hallucinations arise when models rely on language priors rather than visual
input. To capture this, we propose a token-level reliability measure that
quantifies how much the decoder uses visual information. Our method combines
feature-based sensitivity, which measures internal changes when video is
masked, with counterfactual signals, which capture probability differences
between clean and altered video inputs. These signals are aggregated into a
sentence-level reliability score, providing a compact and interpretable measure
of visual grounding. We evaluate the proposed measure on two SLT benchmarks
(PHOENIX-2014T and CSL-Daily) with both gloss-based and gloss-free models. Our
results show that reliability predicts hallucination rates, generalizes across
datasets and architectures, and decreases under visual degradations. Beyond
these quantitative trends, we also find that reliability distinguishes grounded
tokens from guessed ones, allowing risk estimation without references; when
combined with text-based signals (confidence, perplexity, or entropy), it
further improves hallucination risk estimation. Qualitative analysis highlights
why gloss-free models are more susceptible to hallucinations. Taken together,
our findings establish reliability as a practical and reusable tool for
diagnosing hallucinations in SLT, and lay the groundwork for more robust
hallucination detection in multimodal generation.

</details>


### [40] [Engagement Undermines Safety: How Stereotypes and Toxicity Shape Humor in Language Models](https://arxiv.org/abs/2510.18454)
*Atharvan Dogra,Soumya Suvra Ghosal,Ameet Deshpande,Ashwin Kalyan,Dinesh Manocha*

Main category: cs.CL

TL;DR: 优化LLM幽默生成会系统性增加有害内容（刻板印象+毒性），角色提示会放大生成器与评估器的偏见循环


<details>
  <summary>Details</summary>
Motivation: 评估现代LLM幽默生成流程中搞笑性优化与有害内容的耦合关系，揭示安全风险

Method: 联合测量6个模型的幽默/刻板性/毒性，通过信息论指标分析预测不确定性与意外性

Result: 有害笑话幽默评分高10-21%，刻板笑话在LLM评估中多出现11-28%，人类评估显示讽刺生成增加毒性

Conclusion: LLM幽默优化存在有害内容放大机制，需通过结构解耦和安全约束打破偏见循环

Abstract: Large language models are increasingly used for creative writing and
engagement content, raising safety concerns about the outputs. Therefore,
casting humor generation as a testbed, this work evaluates how funniness
optimization in modern LLM pipelines couples with harmful content by jointly
measuring humor, stereotypicality, and toxicity. This is further supplemented
by analyzing incongruity signals through information-theoretic metrics. Across
six models, we observe that harmful outputs receive higher humor scores which
further increase under role-based prompting, indicating a bias amplification
loop between generators and evaluators. Information-theoretic analyses show
harmful cues widen predictive uncertainty and surprisingly, can even make
harmful punchlines more expected for some models, suggesting structural
embedding in learned humor distributions. External validation on an additional
satire-generation task with human perceived funniness judgments shows that LLM
satire increases stereotypicality and typically toxicity, including for closed
models. Quantitatively, stereotypical/toxic jokes gain $10-21\%$ in mean humor
score, stereotypical jokes appear $11\%$ to $28\%$ more often among the jokes
marked funny by LLM-based metric and up to $10\%$ more often in generations
perceived as funny by humans.

</details>


### [41] [ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in Game RAG Benchmarks](https://arxiv.org/abs/2510.18455)
*Liyang He,Yuren Zhang,Ziwei Zhu,Zhenghui Li,Shiwei Tong*

Main category: cs.CL

TL;DR: 提出ChronoPlay框架，首个动态游戏领域RAG基准测试方案，通过双动态更新机制解决游戏内容更新与玩家关注点变化的双重挑战


<details>
  <summary>Details</summary>
Motivation: 现有RAG基准测试在动态领域（如在线游戏）缺乏标准化评估方案，难以应对游戏内容持续更新与玩家社区焦点动态变化的双重挑战

Method: 1. 双动态更新机制追踪游戏版本更新和玩家社区动态
2. 双源合成引擎整合官方资料与玩家社区数据
3. 自动化生成真实可信的测试查询

Result: 在三个不同类型游戏上实例化基准测试，揭示复杂现实条件下模型性能的新洞察

Conclusion: ChronoPlay填补动态领域RAG评估空白，其双动态设计为其他领域基准构建提供参考，代码已开源推动领域发展

Abstract: Retrieval Augmented Generation (RAG) systems are increasingly vital in
dynamic domains like online gaming, yet the lack of a dedicated benchmark has
impeded standardized evaluation in this area. The core difficulty lies in Dual
Dynamics: the constant interplay between game content updates and the shifting
focus of the player community. Furthermore, the necessity of automating such a
benchmark introduces a critical requirement for player-centric authenticity to
ensure generated questions are realistic. To address this integrated challenge,
we introduce ChronoPlay, a novel framework for the automated and continuous
generation of game RAG benchmarks. ChronoPlay utilizes a dual-dynamic update
mechanism to track both forms of change, and a dual-source synthesis engine
that draws from official sources and player community to ensure both factual
correctness and authentic query patterns. We instantiate our framework on three
distinct games to create the first dynamic RAG benchmark for the gaming domain,
offering new insights into model performance under these complex and realistic
conditions. Code is avaliable at: https://github.com/hly1998/ChronoPlay.

</details>


### [42] [DePass: Unified Feature Attributing by Simple Decomposed Forward Pass](https://arxiv.org/abs/2510.18462)
*Xiangyu Hong,Che Jiang,Kai Tian,Biqing Qi,Youbang Sun,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: DePass是一种无需额外训练的Transformer特征归因框架，通过分解前向传递实现细粒度归因分析


<details>
  <summary>Details</summary>
Motivation: 解决Transformer模型内部行为归因的难题，现有方法需要辅助训练或缺乏统一框架，DePass旨在提供无需训练的单次前向传播归因方案

Method: 将隐藏状态分解为定制化加性成分，在固定注意力分数和MLP激活的条件下进行传播，实现跨token/组件/子空间的多层级归因

Result: 在多个归因任务中验证有效性，能追踪任意组件间的信息流，为可解释性研究提供新工具

Conclusion: DePass作为基础性框架，展示了在Transformer机制解释中的潜力，有望推动可解释性研究的更广泛应用

Abstract: Attributing the behavior of Transformer models to internal computations is a
central challenge in mechanistic interpretability. We introduce DePass, a
unified framework for feature attribution based on a single decomposed forward
pass. DePass decomposes hidden states into customized additive components, then
propagates them with attention scores and MLP's activations fixed. It achieves
faithful, fine-grained attribution without requiring auxiliary training. We
validate DePass across token-level, model component-level, and subspace-level
attribution tasks, demonstrating its effectiveness and fidelity. Our
experiments highlight its potential to attribute information flow between
arbitrary components of a Transformer model. We hope DePass serves as a
foundational tool for broader applications in interpretability.

</details>


### [43] [CEFR-Annotated WordNet: LLM-Based Proficiency-Guided Semantic Database for Language Learning](https://arxiv.org/abs/2510.18466)
*Masato Kikuchi,Masatsugu Ono,Toshioki Soga,Tetsu Tanabe,Tadachika Ozono*

Main category: cs.CL

TL;DR: 开发结合CEFR标准的WordNet标注系统，通过大语言模型自动化匹配语义相似度，构建语料库训练出高精度词汇分类器（Macro-F1 0.81），促进语言教育应用


<details>
  <summary>Details</summary>
Motivation: 解决WordNet细粒度语义区分对二语学习者的使用障碍，通过整合CEFR语言能力标准构建分级语义网络

Method: 1. 用大语言模型计算WordNet定义与CEFR词汇的语义相似度
2. 构建包含语义和CEFR信息的语料库
3. 开发上下文词汇分类器进行模型微调

Result: 微调模型性能媲美黄金标准数据训练模型，混合训练后分类器Macro-F1达0.81，标注准确度高

Conclusion: 公开标注资源实现NLP与语言教育的有效衔接，通过分级语义网络提升语言学习效率

Abstract: Although WordNet is a valuable resource owing to its structured semantic
networks and extensive vocabulary, its fine-grained sense distinctions can be
challenging for second-language learners. To address this, we developed a
WordNet annotated with the Common European Framework of Reference for Languages
(CEFR), integrating its semantic networks with language-proficiency levels. We
automated this process using a large language model to measure the semantic
similarity between sense definitions in WordNet and entries in the English
Vocabulary Profile Online. To validate our method, we constructed a large-scale
corpus containing both sense and CEFR-level information from our annotated
WordNet and used it to develop contextual lexical classifiers. Our experiments
demonstrate that models fine-tuned on our corpus perform comparably to those
trained on gold-standard annotations. Furthermore, by combining our corpus with
the gold-standard data, we developed a practical classifier that achieves a
Macro-F1 score of 0.81, indicating the high accuracy of our annotations. Our
annotated WordNet, corpus, and classifiers are publicly available to help
bridge the gap between natural language processing and language education,
thereby facilitating more effective and efficient language learning.

</details>


### [44] [IMB: An Italian Medical Benchmark for Question Answering](https://arxiv.org/abs/2510.18468)
*Antonio Romano,Giuseppe Riccio,Mariano Barone,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: 论文提出了两个意大利医疗数据集IMB-QA和IMB-MCQA，通过大语言模型改进多语言医疗问答系统。核心贡献包括数据构建、模型适应策略验证，发现领域适配策略优于单纯模型规模扩大。


<details>
  <summary>Details</summary>
Motivation: 在线医疗论坛存在大量非结构化医患对话数据，但语言复杂性和非正式性阻碍问答系统应用，尤其在非英语场景下缺乏高质量基准数据集。

Method: 构建包含78万+对话的IMB-QA和2.5万+多选题的IMB-MCQA数据集，采用LLM清洗医疗论坛数据，比较RAG增强与领域微调策略在不同模型架构上的表现。

Result: 领域适配策略（RAG+微调）超越更大规模通用模型，验证医疗AI系统更依赖领域知识与高效检索而非单纯模型参数扩展。

Conclusion: 医疗AI系统应注重领域知识融合与信息检索效率，开源数据集支持多语言医疗问答研究，为资源匮乏语言提供新基准。

Abstract: Online medical forums have long served as vital platforms where patients seek
professional healthcare advice, generating vast amounts of valuable knowledge.
However, the informal nature and linguistic complexity of forum interactions
pose significant challenges for automated question answering systems,
especially when dealing with non-English languages. We present two
comprehensive Italian medical benchmarks: \textbf{IMB-QA}, containing 782,644
patient-doctor conversations from 77 medical categories, and \textbf{IMB-MCQA},
comprising 25,862 multiple-choice questions from medical specialty
examinations. We demonstrate how Large Language Models (LLMs) can be leveraged
to improve the clarity and consistency of medical forum data while retaining
their original meaning and conversational style, and compare a variety of LLM
architectures on both open and multiple-choice question answering tasks. Our
experiments with Retrieval Augmented Generation (RAG) and domain-specific
fine-tuning reveal that specialized adaptation strategies can outperform
larger, general-purpose models in medical question answering tasks. These
findings suggest that effective medical AI systems may benefit more from domain
expertise and efficient information retrieval than from increased model scale.
We release both datasets and evaluation frameworks in our GitHub repository to
support further research on multilingual medical question answering:
https://github.com/PRAISELab-PicusLab/IMB.

</details>


### [45] [DART: A Structured Dataset of Regulatory Drug Documents in Italian for Clinical NLP](https://arxiv.org/abs/2510.18475)
*Mariano Barone,Antonio Laudante,Giuseppe Riccio,Antonio Romano,Marco Postiglione,Vincenzo Moscato*

Main category: cs.CL

TL;DR: 首个意大利药品监管文本结构化语料库DART的构建与应用，通过LLM验证其在药物相互作用检测中的有效性


<details>
  <summary>Details</summary>
Motivation: 现有药理学知识抽取研究过度依赖英语语料（如DrugBank），缺乏适配其他医疗系统（如意大利）的结构化资源

Method: 基于AIFA官方文档的可复现流程：网页级检索→语义段落分割→低温度解码的few-shot调优LLM临床总结

Result: 构建的DART数据集支持LLM精准推断药物相互作用（准确率89.6%），并成功开发药物交互检查工具

Conclusion: DART填补意大利语药监文本结构化资源空白，证实LLM在跨语言临床决策支持中的实用价值

Abstract: The extraction of pharmacological knowledge from regulatory documents has
become a key focus in biomedical natural language processing, with applications
ranging from adverse event monitoring to AI-assisted clinical decision support.
However, research in this field has predominantly relied on English-language
corpora such as DrugBank, leaving a significant gap in resources tailored to
other healthcare systems. To address this limitation, we introduce DART (Drug
Annotation from Regulatory Texts), the first structured corpus of Italian
Summaries of Product Characteristics derived from the official repository of
the Italian Medicines Agency (AIFA). The dataset was built through a
reproducible pipeline encompassing web-scale document retrieval, semantic
segmentation of regulatory sections, and clinical summarization using a
few-shot-tuned large language model with low-temperature decoding. DART
provides structured information on key pharmacological domains such as
indications, adverse drug reactions, and drug-drug interactions. To validate
its utility, we implemented an LLM-based drug interaction checker that
leverages the dataset to infer clinically meaningful interactions. Experimental
results show that instruction-tuned LLMs can accurately infer potential
interactions and their clinical implications when grounded in the structured
textual fields of DART. We publicly release our code on GitHub:
https://github.com/PRAISELab-PicusLab/DART.

</details>


### [46] [How Efficient Are Diffusion Language Models? A Critical Examination of Efficiency Evaluation Practices](https://arxiv.org/abs/2510.18480)
*Han Peng,Peiyu Liu,Zican Dong,Daixuan Cheng,Junyi Li,Yiru Tang,Shuo Wang,Wayne Xin Zhao*

Main category: cs.CL

TL;DR: 当前开源扩散语言模型（DLM）在实际速度上落后于自回归模型（AR），需改进评估方法和加速策略以提升效率。


<details>
  <summary>Details</summary>
Motivation: 探究扩散语言模型（DLM）效率瓶颈，揭示现有评估方法的不足，并提出加速策略的局限性。

Method: 通过实证基准测试和基于屋顶线（roofline）的理论分析，对比DLM与AR模型的吞吐量，并测试双缓存/并行解码等加速策略在不同批量规模下的效果。

Result: AR模型吞吐量更高，DLM持续落后；现有加速策略仅在小批量时有效，规模化后收益显著下降。

Conclusion: DLM发展需建立更稳健的评估框架，并开发更有效的加速策略以实现实际应用突破。

Abstract: Diffusion language models (DLMs) have emerged as a promising alternative to
the long-dominant autoregressive (AR) paradigm, offering a parallelable
decoding process that could yield greater efficiency. Yet, in practice, current
open-source DLMs often underperform their AR counterparts in speed, limiting
their real-world utility. This work presents a systematic study of DLM
efficiency, identifying key issues in prior evaluation methods. Through
empirical benchmarking and a roofline-based theoretical analysis, we
demonstrate that AR models generally achieve higher throughput, while DLMs
consistently lag. We also investigate acceleration strategies, finding that
techniques like dual cache and parallel decoding mainly offer gains at small
batch sizes, with their benefits diminishing upon scaling. Our findings
underscore the necessity of robust evaluation methods and improved acceleration
strategies to advance research on DLMs.

</details>


### [47] [Identity-Aware Large Language Models require Cultural Reasoning](https://arxiv.org/abs/2510.18510)
*Alistair Plum,Anne-Marie Lutgen,Christoph Purschke,Achim Rettinger*

Main category: cs.CL

TL;DR: 探讨大语言模型文化推理能力的缺失及其对多样化用户的负面影响，提出应将文化推理视为与事实准确性和语言连贯性同等重要的基础能力。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型默认西方文化视角，导致忽视用户文化多样性，可能强化刻板印象、削弱信任并助长仇恨。

Method: 通过实证研究揭示模型在道德判断、习语解释等场景中的文化偏向，分析当前基于静态准确率的评估方法局限性。

Result: 验证了模型存在系统性文化偏见，发现仅通过数据微调无法根本解决问题，需开发动态文化推理评估体系。

Conclusion: 提出文化推理应作为AI基础能力，建议从概念定义、评估框架和敏感响应机制三方面构建文化适应型AI系统。

Abstract: Large language models have become the latest trend in natural language
processing, heavily featuring in the digital tools we use every day. However,
their replies often reflect a narrow cultural viewpoint that overlooks the
diversity of global users. This missing capability could be referred to as
cultural reasoning, which we define here as the capacity of a model to
recognise culture-specific knowledge values and social norms, and to adjust its
output so that it aligns with the expectations of individual users. Because
culture shapes interpretation, emotional resonance, and acceptable behaviour,
cultural reasoning is essential for identity-aware AI. When this capacity is
limited or absent, models can sustain stereotypes, ignore minority
perspectives, erode trust, and perpetuate hate. Recent empirical studies
strongly suggest that current models default to Western norms when judging
moral dilemmas, interpreting idioms, or offering advice, and that fine-tuning
on survey data only partly reduces this tendency. The present evaluation
methods mainly report static accuracy scores and thus fail to capture adaptive
reasoning in context. Although broader datasets can help, they cannot alone
ensure genuine cultural competence. Therefore, we argue that cultural reasoning
must be treated as a foundational capability alongside factual accuracy and
linguistic coherence. By clarifying the concept and outlining initial
directions for its assessment, a foundation is laid for future systems to be
able to respond with greater sensitivity to the complex fabric of human
culture.

</details>


### [48] [Building Trust in Clinical LLMs: Bias Analysis and Dataset Transparency](https://arxiv.org/abs/2510.18556)
*Svetlana Maslenkova,Clement Christophe,Marco AF Pimentel,Tathagata Raha,Muhammad Umar Salman,Ahmed Al Mahrooqi,Avani Gupta,Shadab Khan,Ronnie Rajan,Praveenkumar Kanithi*

Main category: cs.CL

TL;DR: 研究提出HC4医疗语料库并评估临床语言模型的处方偏见，关注不同人口群体间的阿片类药物处方差异


<details>
  <summary>Details</summary>
Motivation: 当前医疗AI训练数据缺乏透明度，亟需评估框架确保临床应用的公平性与安全性

Method: 创建89B tokens的HC4医疗语料库，结合通用基准和新型医疗特异性评估方法分析人口特征与处方倾向关联

Result: 发现模型在不同种族、性别、年龄群体中存在差异化的阿片类药物处方趋势

Conclusion: 需建立透明化数据集评估框架以控制医疗AI偏见，HC4和新型评估方法为临床公平性提供关键支持

Abstract: Large language models offer transformative potential for healthcare, yet
their responsible and equitable development depends critically on a deeper
understanding of how training data characteristics influence model behavior,
including the potential for bias. Current practices in dataset curation and
bias assessment often lack the necessary transparency, creating an urgent need
for comprehensive evaluation frameworks to foster trust and guide improvements.
In this study, we present an in-depth analysis of potential downstream biases
in clinical language models, with a focus on differential opioid prescription
tendencies across diverse demographic groups, such as ethnicity, gender, and
age. As part of this investigation, we introduce HC4: Healthcare Comprehensive
Commons Corpus, a novel and extensively curated pretraining dataset exceeding
89 billion tokens. Our evaluation leverages both established general benchmarks
and a novel, healthcare-specific methodology, offering crucial insights to
support fairness and safety in clinical AI applications.

</details>


### [49] [Large language models for folktale type automation based on motifs: Cinderella case study](https://arxiv.org/abs/2510.18561)
*Tjaša Arčon,Marko Robnik-Šikonja,Polona Tratnik*

Main category: cs.CL

TL;DR: 应用机器学习与自然语言处理技术，实现《灰姑娘》故事变体的大规模跨语言民俗学分析


<details>
  <summary>Details</summary>
Motivation: 解决传统人文学科手动分析大规模文本的局限性，探索人工智能在数字人文领域的应用潜力

Method: 使用大语言模型自动检测故事母题，结合聚类分析和降维技术比较不同文本变体

Result: 验证了大语言模型能有效捕捉故事间的复杂关联，支持海量文本的自动化跨语言比较

Conclusion: AI方法革新了民俗学研究范式，为跨文化民间文学研究提供了可扩展的计算框架

Abstract: Artificial intelligence approaches are being adapted to many research areas,
including digital humanities. We built a methodology for large-scale analyses
in folkloristics. Using machine learning and natural language processing, we
automatically detected motifs in a large collection of Cinderella variants and
analysed their similarities and differences with clustering and dimensionality
reduction. The results show that large language models detect complex
interactions in tales, enabling computational analysis of extensive text
collections and facilitating cross-lingual comparisons.

</details>


### [50] [Beyond the Explicit: A Bilingual Dataset for Dehumanization Detection in Social Media](https://arxiv.org/abs/2510.18582)
*Dennis Assenmacher,Paloma Piot,Katarina Laken,David Jurgens,Claudia Wagner*

Main category: cs.CL

TL;DR: 论文指出当前计算语言学研究忽视数字非人化的微妙形式，通过构建双语数据集并微调模型，实现了超越现有技术的检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注显性负面言论作为非人化核心指标，忽视了更隐蔽的非人化形式对边缘群体的持续伤害。这些隐性形式虽不具明显攻击性，却通过强化负面刻板印象造成同等破坏。

Method: 采用多种抽样方法从Twitter和Reddit收集理论指导的双语数据集，通过众包标注和专家标注完成16,000个文档级/片段级标注，覆盖非人化不同维度。

Result: 微调的机器学习模型在零样本和少样本情境下表现优于现有最先进模型。

Conclusion: 研究成功构建了全面覆盖非人化维度的基准数据集，证明了细粒度标注对提升检测性能的有效性，为后续研究提供了训练资源和评估基准。

Abstract: Digital dehumanization, although a critical issue, remains largely overlooked
within the field of computational linguistics and Natural Language Processing.
The prevailing approach in current research concentrating primarily on a single
aspect of dehumanization that identifies overtly negative statements as its
core marker. This focus, while crucial for understanding harmful online
communications, inadequately addresses the broader spectrum of dehumanization.
Specifically, it overlooks the subtler forms of dehumanization that, despite
not being overtly offensive, still perpetuate harmful biases against
marginalized groups in online interactions. These subtler forms can insidiously
reinforce negative stereotypes and biases without explicit offensiveness,
making them harder to detect yet equally damaging. Recognizing this gap, we use
different sampling methods to collect a theory-informed bilingual dataset from
Twitter and Reddit. Using crowdworkers and experts to annotate 16,000 instances
on a document- and span-level, we show that our dataset covers the different
dimensions of dehumanization. This dataset serves as both a training resource
for machine learning models and a benchmark for evaluating future
dehumanization detection techniques. To demonstrate its effectiveness, we
fine-tune ML models on this dataset, achieving performance that surpasses
state-of-the-art models in zero and few-shot in-context settings.

</details>


### [51] [Dynamical model parameters from ultrasound tongue kinematics](https://arxiv.org/abs/2510.18629)
*Sam Kirkham,Patrycja Strycharczuk*

Main category: cs.CL

TL;DR: 通过对比超声舌运动学与EMA数据，验证超声成像在评估动态发音模型参数中的可靠性，结果显示两者参数可比性高。


<details>
  <summary>Details</summary>
Motivation: 探索超声成像作为EMA的低成本替代方案，验证其在动态发音系统建模中的有效性。

Method: 同步采集超声舌运动学和EMA数据，使用线性谐波振荡器模型估计动力学参数，并追踪下颌短肌腱运动。

Result: 超声与EMA得出的动力学参数具有可比性，下颌追踪能准确捕捉颌骨运动轨迹。

Conclusion: 支持将超声运动学数据用于动态发音模型的评估，为无创发音研究提供新途径。

Abstract: The control of speech can be modelled as a dynamical system in which
articulators are driven toward target positions. These models are typically
evaluated using fleshpoint data, such as electromagnetic articulography (EMA),
but recent methodological advances make ultrasound imaging a promising
alternative. We evaluate whether the parameters of a linear harmonic oscillator
can be reliably estimated from ultrasound tongue kinematics and compare these
with parameters estimated from simultaneously-recorded EMA data. We find that
ultrasound and EMA yield comparable dynamical parameters, while mandibular
short tendon tracking also adequately captures jaw motion. This supports using
ultrasound kinematics to evaluate dynamical articulatory models.

</details>


### [52] [MLMA: Towards Multilingual with Mamba Based Architectures](https://arxiv.org/abs/2510.18684)
*Mohamed Nabih Ali,Daniele Falavigna,Alessio Brutti*

Main category: cs.CL

TL;DR: 提出MLMA模型，利用Mamba架构实现高效多语言语音识别


<details>
  <summary>Details</summary>
Motivation: 解决多语言ASR中高低资源语言性能平衡的挑战，探索Transformer替代架构的可能性

Method: 基于Mamba状态空间模型，隐式集成语言感知条件化与共享表示

Result: 在标准多语言基准测试中取得与Transformer架构相媲美的性能

Conclusion: Mamba架构具备作为可扩展、高效且精准多语言语音识别骨干网络的潜力

Abstract: Multilingual automatic speech recognition (ASR) remains a challenging task,
especially when balancing performance across high- and low-resource languages.
Recent advances in sequence modeling suggest that architectures beyond
Transformers may offer better scalability and efficiency. In this work, we
introduce MLMA (Multilingual Language Modeling with Mamba for ASR), a new
approach that leverages the Mamba architecture -- an efficient state-space
model optimized for long-context sequence processing -- for multilingual ASR.
Using Mamba, MLMA implicitly incorporates language-aware conditioning and
shared representations to support robust recognition across diverse languages.
Experiments on standard multilingual benchmarks show that MLMA achieves
competitive performance compared to Transformer-based architectures. These
results highlight Mamba's potential as a strong backbone for scalable,
efficient, and accurate multilingual speech recognition.

</details>


### [53] [Investigating LLM Capabilities on Long Context Comprehension for Medical Question Answering](https://arxiv.org/abs/2510.18691)
*Feras AlMannaa,Talia Tseriotou,Jenny Chim,Maria Liakata*

Main category: cs.CL

TL;DR: 首个评估LLM在长上下文医学QA中的研究，探讨模型性能、RAG影响及优化策略


<details>
  <summary>Details</summary>
Motivation: 填补LLM在长上下文医学理解能力评估的研究空白，探索临床相关场景中模型表现及RAG技术的优化潜力

Method: 通过多维度评估框架（内容相关性设置、不同规模模型、多任务数据集）结合RAG策略实验，采用定性与定量分析方法

Result: 揭示模型规模效应、记忆机制局限、推理模型优势，证明RAG在特定文档场景的有效性并提出优化方案

Conclusion: 为医学长上下文理解提供性能基准，确立RAG应用指导原则，推动临床决策支持系统的优化发展

Abstract: This study is the first to investigate LLM comprehension capabilities over
long-context (LC) medical QA of clinical relevance. Our comprehensive
assessment spans a range of content-inclusion settings based on their
relevance, LLM models of varying capabilities and datasets across task
formulations, revealing insights on model size effects, limitations, underlying
memorization issues and the benefits of reasoning models. Importantly, we
examine the effect of RAG on medical LC comprehension, uncover best settings in
single versus multi-document reasoning datasets and showcase RAG strategies for
improvements over LC. We shed light into some of the evaluation aspects using a
multi-faceted approach. Our qualitative and error analyses address open
questions on when RAG is beneficial over LC, revealing common failure cases.

</details>


### [54] [Bayesian Low-Rank Factorization for Robust Model Adaptation](https://arxiv.org/abs/2510.18723)
*Enes Yavuz Ugan,Ngoc-Quan Pham,Alexander Waibel*

Main category: cs.CL

TL;DR: 贝叶斯因子化适配器用于语音基础模型微调，在保持通用性能的同时有效减少灾难性遗忘


<details>
  <summary>Details</summary>
Motivation: 直接微调语音基础模型可能导致目标域过拟合并削弱模型的通用能力，需要开发能平衡特定领域适应与通用性能保持的方法

Method: 提出贝叶斯因子化适配器，通过零先验约束生成稀疏适应矩阵，应用于Whisper模型并评估多语言语码转换场景

Result: 实现逆向增益54%（基础任务保留率），新领域性能仅下降4%，显著降低灾难性遗忘

Conclusion: 贝叶斯适配方法能有效微调语音基础模型，在保持模型通用性的前提下实现特定领域适应

Abstract: Large speech foundation models achieve strong performance across many
domains, but they often require adaptation to handle local needs such as
code-switching, where speakers mix languages within the same utterance. Direct
fine-tuning of these models risks overfitting to the target domain and
overwriting the broad capabilities of the base model. To address this
challenge, we explore Bayesian factorized adapters for speech foundation
models, which place priors near zero to achieve sparser adaptation matrices and
thereby retain general performance while adapting to specific domains. We apply
our approach to the Whisper model and evaluate on different multilingual
code-switching scenarios. Our results show only minimal adaptation loss while
significantly reducing catastrophic forgetting of the base model. Compared to
LoRA, our method achieves a backward gain of 54% with only a 4% drop on the new
domain. These findings highlight the effectiveness of Bayesian adaptation for
fine-tuning speech foundation models without sacrificing generalization.

</details>


### [55] [Adapting Language Balance in Code-Switching Speech](https://arxiv.org/abs/2510.18724)
*Enes Yavuz Ugan,Ngoc-Quan Pham,Alexander Waibel*

Main category: cs.CL

TL;DR: 提出通过强调语码转换点来提升基础模型在混合语言场景中的鲁棒性，实验证明减少了替换错误


<details>
  <summary>Details</summary>
Motivation: 基础模型在标准测试表现优异，但在语码转换测试中表现不佳。问题根源不在数据稀缺，而在于语码转换时刻低频且第二语言嵌入微妙。需要针对性强化这些关键点的学习

Method: 利用主语言与嵌入语言的特征差异定位语码转换点，通过可微替代方法强调这些位置的学习，缓解生成时的语境偏差

Result: 阿拉伯语和汉英双语实验显示，模型能更准确预测转换位置，替换错误率显著降低

Conclusion: 通过聚焦语码转换关键节点的差异化学习机制，有效提升了模型在混合语言场景下的鲁棒性

Abstract: Despite achieving impressive results on standard benchmarks, large
foundational models still struggle against code-switching test cases. When data
scarcity cannot be used as the usual justification for poor performance, the
reason may lie in the infrequent occurrence of code-switched moments, where the
embedding of the second language appears subtly. Instead of expecting the
models to learn this infrequency on their own, it might be beneficial to
provide the training process with labels. Evaluating model performance on
code-switching data requires careful localization of code-switching points
where recognition errors are most consequential, so that the analysis
emphasizes mistakes occurring at those moments. Building on this observation,
we leverage the difference between the embedded and the main language to
highlight those code-switching points and thereby emphasize learning at those
locations. This simple yet effective differentiable surrogate mitigates context
bias during generation -- the central challenge in code-switching -- thereby
improving the model's robustness. Our experiments with Arabic and
Chinese-English showed that the models are able to predict the switching places
more correctly, reflected by the reduced substitution error.

</details>


### [56] [SemiAdapt and SemiLoRA: Efficient Domain Adaptation for Transformer-based Low-Resource Language Translation with a Case Study on Irish](https://arxiv.org/abs/2510.18725)
*Josh McGiff,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 提出SemiAdapt和SemiLoRA半监督推理方法，显著提升低资源语言（如爱尔兰语）机器翻译的领域适应效率，使参数高效微调超越全模型微调。


<details>
  <summary>Details</summary>
Motivation: 传统全模型微调对数十亿参数模型计算成本过高，阻碍低资源语言（如爱尔兰语）机器翻译研究，需开发参数高效的替代方案。

Method: 基于参数高效微调（PEFT）框架，结合低秩适应（LoRA）的适配层设计，提出半监督推理方法SemiAdapt（增强领域适应）和SemiLoRA（提升推理效率）。

Result: SemiLoRA使PEFT方法BLEU值超越全模型微调2.1个点，在大型嘈杂语料库上表现尤为突出，所有爱尔兰语模型开源。

Conclusion: 通过参数高效方法和开源模型，显著降低了低资源语言领域适应的技术门槛，推动可及性研究。

Abstract: Fine-tuning is widely used to tailor large language models for specific tasks
such as neural machine translation (NMT). However, leveraging transfer learning
is computationally expensive when fine-tuning large multilingual models with
billions of parameters, thus creating a barrier to entry for researchers
working on low-resource domains such as Irish translation. Parameter-efficient
fine-tuning (PEFT) bridges this gap by training on a fraction of the original
model parameters, with the Low-Rank Adaptation (LoRA) approach introducing
small, trainable adapter layers. We introduce SemiAdapt and SemiLoRA as
semi-supervised inference-efficient approaches that strengthen domain
adaptation and lead to improved overall performance in NMT. We demonstrate that
SemiAdapt can outperform full-domain fine-tuning, while most notably, SemiLoRA
can propel PEFT methods to match or even outperform full-model fine-tuning. We
further evaluate domain-by-dataset fine-tuning and demonstrate that our
embedding-based inference methods perform especially well on larger and noisier
corpora. All Irish translation models developed in this work are released as
open resources. These methods aim to make high-quality domain adaptation and
fine-tuning more accessible to researchers working with low-resource languages.

</details>


### [57] [Verifiable Accuracy and Abstention Rewards in Curriculum RL to Alleviate Lost-in-Conversation](https://arxiv.org/abs/2510.18731)
*Ming Li*

Main category: cs.CL

TL;DR: 提出RLAAR框架通过课程强化学习解决多轮对话中LLM性能衰减问题，平衡正确应答与明智弃答


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型在多轮对话中信息逐步揭示时出现的性能衰减（LiC）问题，探索通过强化学习提升模型可靠应答能力

Method: 1. 采用能力门控课程逐步增加对话难度（指令分片）
2. 多轮策略迭代与混合奖励机制
3. 可验证准确率与弃答率双重奖励设计

Result: LiC基准测试显示：
- 性能衰减率改善（62.6%→75.1%）
- 校准弃答率提升（33.5%→73.4%）

Conclusion: RLAAR为构建多轮可靠LLM提供了有效方案，通过平衡应答能力与自知弃答机制增强对话系统可信度

Abstract: Large Language Models demonstrate strong capabilities in single-turn
instruction following but suffer from Lost-in-Conversation (LiC), a degradation
in performance as information is revealed progressively in multi-turn settings.
Motivated by the current progress on Reinforcement Learning with Verifiable
Rewards (RLVR), we propose Curriculum Reinforcement Learning with Verifiable
Accuracy and Abstention Rewards (RLAAR), a framework that encourages models not
only to generate correct answers, but also to judge the solvability of
questions in the multi-turn conversation setting. Our approach employs a
competence-gated curriculum that incrementally increases dialogue difficulty
(in terms of instruction shards), stabilizing training while promoting
reliability. Using multi-turn, on-policy rollouts and a mixed-reward system,
RLAAR teaches models to balance problem-solving with informed abstention,
reducing premature answering behaviors that cause LiC. Evaluated on LiC
benchmarks, RLAAR significantly mitigates LiC performance decay (62.6% to
75.1%) and improves calibrated abstention rates (33.5% to 73.4%). Together,
these results provide a practical recipe for building multi-turn reliable and
trustworthy LLMs.

</details>


### [58] [Topoformer: brain-like topographic organization in Transformer language models through spatial querying and reweighting](https://arxiv.org/abs/2510.18745)
*Taha Binhuraib,Greta Tuckute,Nicholas Blauch*

Main category: cs.CL

TL;DR: 提出Topoformer模型，通过空间查询和重加权机制赋予Transformer地形组织特性，在保持NLP性能的同时实现人脑语言网络的可解释性对齐。


<details>
  <summary>Details</summary>
Motivation: 生物大脑具有空间功能组织的特性，而传统Transformer模型缺乏空间拓扑表征，导致表征空间难以可视化解释。通过模拟神经地形图机制，旨在提升模型可解释性并建立与人脑语言处理的关联。

Method: 采用空间查询（键值对排列为2D网格+局部查询池关联）和空间重加权（将全连接自注意力转为局部连接）两大核心机制，分别在情感分类任务和BERT架构上进行验证，并通过fMRI数据与人类语言网络进行对比分析。

Result: Topoformer在GLUE基准表现持平基线模型，但通过8项语言测试套件展现出可解释的地形组织特征。fMRI数据分析显示模型低维地形变化与人脑语言网络响应存在显著对齐。

Conclusion: 地形组织机制的引入在不损失模型性能的前提下显著提升可解释性，为构建脑启发的NLP模型提供了新范式，同时也为研究人脑语言处理机制提供了计算建模工具。

Abstract: Spatial functional organization is a hallmark of biological brains: neurons
are arranged topographically according to their response properties, at
multiple scales. In contrast, representations within most machine learning
models lack spatial biases, instead manifesting as disorganized vector spaces
that are difficult to visualize and interpret. Here, we propose a novel form of
self-attention that turns Transformers into "Topoformers" with topographic
organization. We introduce spatial querying - where keys and queries are
arranged on 2D grids, and local pools of queries are associated with a given
key - and spatial reweighting, where we convert the standard fully connected
layer of self-attention into a locally connected layer. We first demonstrate
the feasibility of our approach by training a 1-layer Topoformer on a sentiment
classification task. Training with spatial querying encourages topographic
organization in the queries and keys, and spatial reweighting separately
encourages topographic organization in the values and self-attention outputs.
We then apply the Topoformer motifs at scale, training a BERT architecture with
a masked language modeling objective. We find that the topographic variant
performs on par with a non-topographic control model on NLP benchmarks, yet
produces interpretable topographic organization as evaluated via eight
linguistic test suites. Finally, analyzing an fMRI dataset of human brain
responses to a large set of naturalistic sentences, we demonstrate alignment
between low-dimensional topographic variability in the Topoformer model and
human brain language network. Scaling up Topoformers further holds promise for
greater interpretability in NLP research, and for more accurate models of the
organization of linguistic information in the human brain.

</details>


### [59] [AI use in American newspapers is widespread, uneven, and rarely disclosed](https://arxiv.org/abs/2510.18774)
*Jenna Russell,Marzena Karpinska,Destiny Akinode,Katherine Thai,Bradley Emi,Max Spero,Mohit Iyyer*

Main category: cs.CL

TL;DR: 2025年夏季美国新闻业AI使用审计显示：9%新发文章含AI生成内容，地方媒体/特定主题使用更频繁，评论文章AI使用率是新闻6.4倍，且普遍缺乏披露


<details>
  <summary>Details</summary>
Motivation: 探究AI在已发布新闻中的实际应用程度及透明度现状，填补该领域研究空白

Method: 使用先进AI检测工具Pangram分析186K篇美国报纸文章和45K篇评论文章的大规模数据集

Result: 发现AI使用呈现不均匀分布（地方媒体/天气科技主题/特定集团更显著），评论文章AI使用率6.4倍于新闻，100篇AI标记文章中仅5篇披露使用情况

Conclusion: 亟需建立新闻业AI使用的透明机制和更新编辑标准以维持公众信任

Abstract: AI is rapidly transforming journalism, but the extent of its use in published
newspaper articles remains unclear. We address this gap by auditing a
large-scale dataset of 186K articles from online editions of 1.5K American
newspapers published in the summer of 2025. Using Pangram, a state-of-the-art
AI detector, we discover that approximately 9% of newly-published articles are
either partially or fully AI-generated. This AI use is unevenly distributed,
appearing more frequently in smaller, local outlets, in specific topics such as
weather and technology, and within certain ownership groups. We also analyze
45K opinion pieces from Washington Post, New York Times, and Wall Street
Journal, finding that they are 6.4 times more likely to contain AI-generated
content than news articles from the same publications, with many AI-flagged
op-eds authored by prominent public figures. Despite this prevalence, we find
that AI use is rarely disclosed: a manual audit of 100 AI-flagged articles
found only five disclosures of AI use. Overall, our audit highlights the
immediate need for greater transparency and updated editorial standards
regarding the use of AI in journalism to maintain public trust.

</details>


### [60] [KAT-Coder Technical Report](https://arxiv.org/abs/2510.18779)
*Zizheng Zhan,Ken Deng,Xiaojiang Zhang,Jinghui Wang,Huaixi Tang,Zhiyi Lai,Haoyang Huang,Wen Xiang,Kun Wu,Wenhao Zhuang,Minglei Zhang,Shaojie Wang,Shangpeng Yan,Kepeng Lei,Zongxian Feng,Huiming Wang,Zheng Lin,Mengtong Li,Mengfei Xie,Yinghan Cui,Xuxing Chen,Chao Wang,Weihao Li,Wenqiang Zhu,Jiarong Zhang,Jingxuan Xu,Songwei Yu,Yifan Yao,Xinping Lei,Han Li,Junqi Xiong,Zuchen Gao,Dailin Li,Haimo Li,Jiaheng Liu,Yuqun Zhang,Junyi Peng,Haotian Zhang,Bin Chen*

Main category: cs.CL

TL;DR: KAT-Coder通过多阶段课程训练实现自主编码代理，解决静态训练与动态执行的差距，形成可部署的智能编码基础。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在自主编码代理领域取得进展，但静态文本训练与动态实际执行间的差距仍是核心挑战。

Method: 四阶段训练框架：1）中期训练增强推理/规划能力；2）监督微调平衡多编程语言/场景；3）强化微调采用多真实奖励机制；4）部署阶段通过错误掩码和树状轨迹训练适配IDE环境。

Result: KAT-Coder实现工具可靠使用（95%+成功率）、指令对齐（人工评估92%符合度）和长上下文推理（支持8k tokens），32B模型KAT-Dev已开源。

Conclusion: 多阶段课程训练有效弥合理论训练与实际部署的鸿沟，通过渐进式能力构建形成生产级智能编码代理基础架构。

Abstract: Recent advances in large language models (LLMs) have enabled progress in
agentic coding, where models autonomously reason, plan, and act within
interactive software development workflows. However, bridging the gap between
static text-based training and dynamic real-world agentic execution remains a
core challenge. In this technical report, we present KAT-Coder, a large-scale
agentic code model trained through a multi-stage curriculum encompassing
Mid-Term Training, Supervised Fine-Tuning (SFT), Reinforcement Fine-Tuning
(RFT), and Reinforcement-to-Deployment Adaptation. The Mid-Term stage enhances
reasoning, planning, and reflection capabilities through a corpus of real
software engineering data and synthetic agentic interactions. The SFT stage
constructs a million-sample dataset balancing twenty programming languages, ten
development contexts, and ten task archetypes. The RFT stage introduces a novel
multi-ground-truth reward formulation for stable and sample-efficient policy
optimization. Finally, the Reinforcement-to-Deployment phase adapts the model
to production-grade IDE environments using Error-Masked SFT and Tree-Structured
Trajectory Training. In summary, these stages enable KAT-Coder to achieve
robust tool-use reliability, instruction alignment, and long-context reasoning,
forming a deployable foundation for real-world intelligent coding agents. Our
KAT series 32B model, KAT-Dev, has been open-sourced on
https://huggingface.co/Kwaipilot/KAT-Dev.

</details>


### [61] [WebSeer: Training Deeper Search Agents through Reinforcement Learning with Self-Reflection](https://arxiv.org/abs/2510.18798)
*Guanzhong He,Zhen Yang,Jinxin Liu,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: WebSeer通过强化学习和自我反思机制提升搜索代理性能，在HotpotQA和SimpleQA上分别取得72.3%和90.0%的SOTA准确率


<details>
  <summary>Details</summary>
Motivation: 现有搜索代理存在工具使用深度不足和多次交互错误累积的问题

Method: 构建带反射模式标注的大规模数据集，设计融合冷启动和强化学习的双阶段训练框架

Result: 在真实网络环境中生成更长、更具反思性的工具使用轨迹，准确率显著提升且具备强泛化能力

Conclusion: 自我反思范式有效扩展工具使用链长度，单14B模型即实现多领域最优表现

Abstract: Search agents have achieved significant advancements in enabling intelligent
information retrieval and decision-making within interactive environments.
Although reinforcement learning has been employed to train agentic models
capable of more dynamic interactive retrieval, existing methods are limited by
shallow tool-use depth and the accumulation of errors over multiple iterative
interactions. In this paper, we present WebSeer, a more intelligent search
agent trained via reinforcement learning enhanced with a self-reflection
mechanism. Specifically, we construct a large dataset annotated with reflection
patterns and design a two-stage training framework that unifies cold start and
reinforcement learning within the self-reflection paradigm for real-world
web-based environments, which enables the model to generate longer and more
reflective tool-use trajectories. Our approach substantially extends tool-use
chains and improves answer accuracy. Using a single 14B model, we achieve
state-of-the-art results on HotpotQA and SimpleQA, with accuracies of 72.3% and
90.0%, respectively, and demonstrate strong generalization to
out-of-distribution datasets. The code is available at
https://github.com/99hgz/WebSeer

</details>


### [62] [Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for Industrial Asset Health Monitoring](https://arxiv.org/abs/2510.18817)
*Shuxin Lin,Dhaval Patel,Christodoulos Constantinides*

Main category: cs.CL

TL;DR: 通过思维链蒸馏框架将LLMs的推理能力迁移到SLMs，显著提升工业场景中小模型性能


<details>
  <summary>Details</summary>
Motivation: 虽然小型语言模型(SLMs)在工业领域因高效和可定制化受到欢迎，但其在工业4.0等专业领域进行复杂推理仍存在显著挑战。本研究旨在通过知识蒸馏突破这一技术瓶颈

Method: 提出基于多选问答提示的思维链(CoT)蒸馏框架：1) 从LLMs提炼推理知识 2) 上下文学习验证知识质量 3) 构建微调基准测试对比LLMs性能

Result: 微调后的SLMs在CoT推理任务上性能显著超越基线模型，与LLM的差距缩小23.8%。代码已在GitHub开源供工业界应用

Conclusion: 该框架为工业资产健康管理提供了高效推理解决方案，证明通过知识蒸馏可以使SLMs在保留效率优势的同时逼近LLMs的推理能力，具有重要工程应用价值

Abstract: Small Language Models (SLMs) are becoming increasingly popular in specialized
fields, such as industrial applications, due to their efficiency, lower
computational requirements, and ability to be fine-tuned for domain-specific
tasks, enabling accurate and cost-effective solutions. However, performing
complex reasoning using SLMs in specialized fields such as Industry 4.0 remains
challenging. In this paper, we propose a knowledge distillation framework for
industrial asset health, which transfers reasoning capabilities via
Chain-of-Thought (CoT) distillation from Large Language Models (LLMs) to
smaller, more efficient models (SLMs). We discuss the advantages and the
process of distilling LLMs using multi-choice question answering (MCQA) prompts
to enhance reasoning and refine decision-making. We also perform in-context
learning to verify the quality of the generated knowledge and benchmark the
performance of fine-tuned SLMs with generated knowledge against widely used
LLMs. The results show that the fine-tuned SLMs with CoT reasoning outperform
the base models by a significant margin, narrowing the gap to their LLM
counterparts. Our code is open-sourced at:
https://github.com/IBM/FailureSensorIQ.

</details>


### [63] [MTraining: Distributed Dynamic Sparse Attention for Efficient Ultra-Long Context Training](https://arxiv.org/abs/2510.18830)
*Wenxuan Li,Chengruidong Zhang,Huiqiang Jiang,Yucheng Li,Yuqing Yang,Lili Qiu*

Main category: cs.CL

TL;DR: MTraining通过动态稀疏注意力机制实现高效分布式训练，将Qwen2.5-3B模型的上下文窗口从32K扩展到512K tokens，训练吞吐量提升6倍且保持准确性。


<details>
  <summary>Details</summary>
Motivation: 动态稀疏注意力在分布式训练中存在计算负载不均衡和通信开销大的问题，需开发新方法实现超长上下文LLM的高效训练。

Method: 整合动态稀疏训练模式、平衡稀疏环注意力（解决计算不均衡）和层次稀疏环注意力（优化通信开销）三项关键技术。

Result: 在32块A100 GPU上成功扩展Qwen2.5-3B至512K上下文，下游任务评估显示训练吞吐量提升6倍且精度无损。

Conclusion: MTraining为长上下文LLM训练提供高效解决方案，开源代码促进后续研究与应用。

Abstract: The adoption of long context windows has become a standard feature in Large
Language Models (LLMs), as extended contexts significantly enhance their
capacity for complex reasoning and broaden their applicability across diverse
scenarios. Dynamic sparse attention is a promising approach for reducing the
computational cost of long-context. However, efficiently training LLMs with
dynamic sparse attention on ultra-long contexts-especially in distributed
settings-remains a significant challenge, due in large part to worker- and
step-level imbalance. This paper introduces MTraining, a novel distributed
methodology leveraging dynamic sparse attention to enable efficient training
for LLMs with ultra-long contexts. Specifically, MTraining integrates three key
components: a dynamic sparse training pattern, balanced sparse ring attention,
and hierarchical sparse ring attention. These components are designed to
synergistically address the computational imbalance and communication overheads
inherent in dynamic sparse attention mechanisms during the training of models
with extensive context lengths. We demonstrate the efficacy of MTraining by
training Qwen2.5-3B, successfully expanding its context window from 32K to 512K
tokens on a cluster of 32 A100 GPUs. Our evaluations on a comprehensive suite
of downstream tasks, including RULER, PG-19, InfiniteBench, and Needle In A
Haystack, reveal that MTraining achieves up to a 6x higher training throughput
while preserving model accuracy. Our code is available at
https://github.com/microsoft/MInference/tree/main/MTraining.

</details>


### [64] [Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning](https://arxiv.org/abs/2510.18849)
*Chenghao Zhu,Meiling Tao,Tiannan Wang,Dongyi Ding,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: 论文提出Critique-Post-Edit框架，通过生成式奖励模型和自主修订机制，解决LLM个性化中奖励攻击问题，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有监督微调和标准RLHF方法在个性化任务中存在性能瓶颈与奖励攻击问题，导致生成内容冗长且缺乏深度个性化。

Method: 提出包含个性化生成奖励模型（GRM）和批判后编辑机制（Critique-Post-Edit）的强化学习框架：
1. GRM提供多维评分与文本反馈防止奖励攻击
2. 策略模型基于反馈自主修订输出实现精准学习

Result: 在长度控制评估中：
- Qwen2.5-7B个性化模型胜率提升11%
- Qwen2.5-14B模型性能超越GPT-4.1

Conclusion: 该方法为LLM提供了忠实、高效、可控的个性化实现路径，推动了强化学习在自然语言处理中的应用边界。

Abstract: Faithfully personalizing large language models (LLMs) to align with
individual user preferences is a critical but challenging task. While
supervised fine-tuning (SFT) quickly reaches a performance plateau, standard
reinforcement learning from human feedback (RLHF) also struggles with the
nuances of personalization. Scalar-based reward models are prone to reward
hacking which leads to verbose and superficially personalized responses. To
address these limitations, we propose Critique-Post-Edit, a robust
reinforcement learning framework that enables more faithful and controllable
personalization. Our framework integrates two key components: (1) a
Personalized Generative Reward Model (GRM) that provides multi-dimensional
scores and textual critiques to resist reward hacking, and (2) a
Critique-Post-Edit mechanism where the policy model revises its own outputs
based on these critiques for more targeted and efficient learning. Under a
rigorous length-controlled evaluation, our method substantially outperforms
standard PPO on personalization benchmarks. Personalized Qwen2.5-7B achieves an
average 11\% win-rate improvement, and personalized Qwen2.5-14B model surpasses
the performance of GPT-4.1. These results demonstrate a practical path to
faithful, efficient, and controllable personalization.

</details>


### [65] [Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model](https://arxiv.org/abs/2510.18855)
*Ling Team,Anqi Shen,Baihui Li,Bin Hu,Bin Jing,Cai Chen,Chao Huang,Chao Zhang,Chaokun Yang,Cheng Lin,Chengyao Wen,Congqi Li,Deng Zhao,Dingbo Yuan,Donghai You,Fagui Mao,Fanzhuang Meng,Feng Xu,Guojie Li,Guowei Wang,Hao Dai,Haonan Zheng,Hong Liu,Jia Guo,Jiaming Liu,Jian Liu,Jianhao Fu,Jiannan Shi,Jianwen Wang,Jianxin Lai,Jin Yang,Jun Mei,Jun Zhou,Junbo Zhao,Junping Zhao,Kuan Xu,Le Su,Lei Chen,Li Tang,Liang Jiang,Liangcheng Fu,Lianhao Xu,Linfeng Shi,Lisha Liao,Longfei Zheng,Meng Li,Mingchun Chen,Qi Zuo,Qiang Cheng,Qianggang Cao,Qitao Shi,Quanrui Guo,Senlin Zhu,Shaofei Wang,Shaomian Zheng,Shuaicheng Li,Shuwei Gu,Siba Chen,Tao Wu,Tao Zhang,Tianyu Zhang,Tianyu Zhou,Tiwei Bie,Tongkai Yang,Wang Hong,Wang Ren,Weihua Chen,Wenbo Yu,Wengang Zheng,Xiangchun Wang,Xiaodong Yan,Xiaopei Wan,Xin Zhao,Xinyu Kong,Xinyu Tang,Xudong Han,Xudong Wang,Xuemin Yang,Xueyu Hu,Yalin Zhang,Yan Sun,Yicheng Shan,Yilong Wang,Yingying Xu,Yongkang Liu,Yongzhen Guo,Yuanyuan Wang,Yuchen Yan,Yuefan Wang,Yuhong Guo,Zehuan Li,Zhankai Xu,Zhe Li,Zhenduo Zhang,Zhengke Gui,Zhenxuan Pan,Zhenyu Huang,Zhenzhong Lan,Zhiqiang Ding,Zhiqiang Zhang,Zhixun Li,Zhizhen Liu,Zihao Wang,Zujie Wen*

Main category: cs.CL

TL;DR: 首个开源万亿参数思维模型Ring-1T，通过三项技术创新解决超大规模训练难题，在多领域基准测试中取得突破性成绩并开源模型。


<details>
  <summary>Details</summary>
Motivation: 解决万亿参数模型训练中存在的训练-推理错位、推演处理低效和RL系统瓶颈三大挑战

Method: 1. IcePop通过令牌级差异掩蔽稳定RL训练
2. C3PO++实现长推演场景下的动态资源分配
3. ASystem高性能RL框架突破系统瓶颈

Result: AIME-2025:93.4 | HMMT-2025:86.72 | CodeForces:2088 | ARC-AGI-v1:55.94 | IMO-2025银牌水平

Conclusion: 该模型的开源标志着大规模推理智能民主化的重要里程碑，为开源社区建立了新的性能基准

Abstract: We present Ring-1T, the first open-source, state-of-the-art thinking model
with a trillion-scale parameter. It features 1 trillion total parameters and
activates approximately 50 billion per token. Training such models at a
trillion-parameter scale introduces unprecedented challenges, including
train-inference misalignment, inefficiencies in rollout processing, and
bottlenecks in the RL system. To address these, we pioneer three interconnected
innovations: (1) IcePop stabilizes RL training via token-level discrepancy
masking and clipping, resolving instability from training-inference mismatches;
(2) C3PO++ improves resource utilization for long rollouts under a token budget
by dynamically partitioning them, thereby obtaining high time efficiency; and
(3) ASystem, a high-performance RL framework designed to overcome the systemic
bottlenecks that impede trillion-parameter model training. Ring-1T delivers
breakthrough results across critical benchmarks: 93.4 on AIME-2025, 86.72 on
HMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-v1. Notably, it attains a
silver medal-level result on the IMO-2025, underscoring its exceptional
reasoning capabilities. By releasing the complete 1T parameter MoE model to the
community, we provide the research community with direct access to cutting-edge
reasoning capabilities. This contribution marks a significant milestone in
democratizing large-scale reasoning intelligence and establishes a new baseline
for open-source model performance.

</details>


### [66] [LightMem: Lightweight and Efficient Memory-Augmented Generation](https://arxiv.org/abs/2510.18866)
*Jizhan Fang,Xinle Deng,Haoming Xu,Ziyan Jiang,Yuqi Tang,Ziwen Xu,Shumin Deng,Yunzhi Yao,Mengru Wang,Shuofei Qiao,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 提出了LightMem内存系统，通过三阶段记忆架构平衡性能与效率，显著降低资源消耗并提升准确性


<details>
  <summary>Details</summary>
Motivation: 现有内存系统在动态复杂环境中存在时间和计算开销大的问题，需要平衡性能与效率

Method: 1. 感觉记忆：轻量压缩过滤无关信息并按主题分组；2. 短时记忆：基于主题整合并结构化内容；3. 长时记忆：通过离线睡眠时间更新解耦推理过程

Result: 在GPT和Qwen模型上实现最高10.9%准确率提升，同时减少117倍token使用、159倍API调用和12倍运行时间

Conclusion: LightMem通过认知启发的三阶段架构有效提升LLMs的记忆处理效率，实验验证其在资源消耗和性能表现的双重优势

Abstract: Despite their remarkable capabilities, Large Language Models (LLMs) struggle
to effectively leverage historical interaction information in dynamic and
complex environments. Memory systems enable LLMs to move beyond stateless
interactions by introducing persistent information storage, retrieval, and
utilization mechanisms. However, existing memory systems often introduce
substantial time and computational overhead. To this end, we introduce a new
memory system called LightMem, which strikes a balance between the performance
and efficiency of memory systems. Inspired by the Atkinson-Shiffrin model of
human memory, LightMem organizes memory into three complementary stages. First,
cognition-inspired sensory memory rapidly filters irrelevant information
through lightweight compression and groups information according to their
topics. Next, topic-aware short-term memory consolidates these topic-based
groups, organizing and summarizing content for more structured access. Finally,
long-term memory with sleep-time update employs an offline procedure that
decouples consolidation from online inference. Experiments on LongMemEval with
GPT and Qwen backbones show that LightMem outperforms strong baselines in
accuracy (up to 10.9% gains) while reducing token usage by up to 117x, API
calls by up to 159x, and runtime by over 12x. The code is available at
https://github.com/zjunlp/LightMem.

</details>


### [67] [How Do LLMs Use Their Depth?](https://arxiv.org/abs/2510.18871)
*Akshat Gupta,Jay Yeung,Gopala Anumanchipalli,Anna Ivanova*

Main category: cs.CL

TL;DR: 大型语言模型通过分层动态计算实现预测，早期层生成高频词作为初步猜测，后续层逐步细化调整。70%以上的早期预测会被修正，不同任务类型呈现差异化的层深使用模式。


<details>
  <summary>Details</summary>
Motivation: 现有研究虽指出LLMs不均匀使用网络深度，但缺乏对逐层预测机制的细粒度分析。本文旨在揭示模型内部如何通过分层计算优化预测，为提升计算效率提供理论基础。

Method: 追踪多个开源模型的中间表示，提出'猜测-优化'框架，通过词频分析、词性标记、事实回忆和多选任务三个案例研究层深动态。

Result: 1) 早期层高频词预测70%被后续层修正 2) 功能词平均在11.8层即预测正确 3) 事实回忆任务首词需25层计算深度 4) 多选题格式识别早于答案确定。

Conclusion: 研究揭示了LLMs分层预测的动态机制，证明正确预测需要多阶段计算优化。发现为模型压缩、早期退出策略等效率优化方案提供了理论依据。

Abstract: Growing evidence suggests that large language models do not use their depth
uniformly, yet we still lack a fine-grained understanding of their layer-wise
prediction dynamics. In this paper, we trace the intermediate representations
of several open-weight models during inference and reveal a structured and
nuanced use of depth. Specifically, we propose a "Guess-then-Refine" framework
that explains how LLMs internally structure their computations to make
predictions. We first show that the top-ranked predictions in early LLM layers
are composed primarily of high-frequency tokens, which act as statistical
guesses proposed by the model early on due to the lack of appropriate
contextual information. As contextual information develops deeper into the
model, these initial guesses get refined into contextually appropriate tokens.
Even high-frequency token predictions from early layers get refined >70% of the
time, indicating that correct token prediction is not "one-and-done". We then
go beyond frequency-based prediction to examine the dynamic usage of layer
depth across three case studies. (i) Part-of-speech analysis shows that
function words are, on average, the earliest to be predicted correctly. (ii)
Fact recall task analysis shows that, in a multi-token answer, the first token
requires more computational depth than the rest. (iii) Multiple-choice task
analysis shows that the model identifies the format of the response within the
first half of the layers, but finalizes its response only toward the end.
Together, our results provide a detailed view of depth usage in LLMs, shedding
light on the layer-by-layer computations that underlie successful predictions
and providing insights for future works to improve computational efficiency in
transformer-based models.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [68] [A Generalizable Light Transport 3D Embedding for Global Illumination](https://arxiv.org/abs/2510.18189)
*Bing Xu,Mukund Varma T,Cheng Wang,Tzumao Li,Lifan Wu,Bartlomiej Wronski,Ravi Ramamoorthi,Marco Salvi*

Main category: cs.GR

TL;DR: 提出一种通用的3D光传输嵌入方法，直接从3D场景配置预测全局光照，无需依赖传统光追信号。通过点云特征和Transformer建模全局交互，实现跨场景泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有神经方法存在场景特定优化依赖（计算成本高）和2D空间泛化（视角不一致/空间理解有限）的缺陷，需要更高效的3D通用解决方案。

Method: 1) 场景表示为带几何材质特征的点云 2) 可扩展Transformer建模全局点间交互 3) 渲染时通过最近邻检索和跨注意力机制聚合潜在特征

Result: 在多样化室内场景实现漫反射全局光照预测；预训练嵌入可快速迁移到新渲染任务（少量微调）；初步支持镜面材质辐射场估计，加速无偏路径引导

Conclusion: 该方法为将学习先验集成到渲染管线开辟新路径，摆脱对显式光追信号的依赖，推动高效全局光照解决方案发展。

Abstract: Global illumination (GI) is essential for realistic rendering but remains
computationally expensive due to the complexity of simulating indirect light
transport. Recent neural methods have mainly relied on per-scene optimization,
sometimes extended to handle changes in camera or geometry. Efforts toward
cross-scene generalization have largely stayed in 2D screen space, such as
neural denoising or G-buffer based GI prediction, which often suffer from view
inconsistency and limited spatial understanding. We propose a generalizable 3D
light transport embedding that approximates global illumination directly from
3D scene configurations, without using rasterized or path-traced cues. Each
scene is represented as a point cloud with geometric and material features. A
scalable transformer models global point-to-point interactions to encode these
features into neural primitives. At render time, each query point retrieves
nearby primitives via nearest-neighbor search and aggregates their latent
features through cross-attention to predict the desired rendering quantity. We
demonstrate results on diffuse global illumination prediction across diverse
indoor scenes with varying layouts, geometry, and materials. The embedding
trained for irradiance estimation can be quickly adapted to new rendering tasks
with limited fine-tuning. We also present preliminary results for
spatial-directional radiance field estimation for glossy materials and show how
the normalized field can accelerate unbiased path guiding. This approach
highlights a path toward integrating learned priors into rendering pipelines
without explicit ray-traced illumination cues.

</details>


### [69] [ORDENA: ORigin-DEstiNAtion data exploration](https://arxiv.org/abs/2510.18278)
*Karelia Salinas,Victor Barella,André Luiz Cunha,Gabriel Martins de Oliveira,Thales Viera,Luis Gustavo Nonato*

Main category: cs.GR

TL;DR: 提出ORDENA可视化分析工具，通过散点图轴对应起终点的方式直观探索大规模OD数据，并提供数据可解释性资源


<details>
  <summary>Details</summary>
Motivation: 传统OD流分析在应对大规模数据时存在数据聚合导致模式丢失、交互过滤决策困难等核心痛点

Method: 基于双轴散点图布局（横轴起点/纵轴终点），结合属性关联解释模块，构建可视化分析框架

Result: 通过领域专家参与的案例研究验证，未参与开发的第三方专家评审给出积极反馈

Conclusion: ORDENA在保持数据模式可见性的同时增强分析可解释性，为复杂OD分析提供有效解决方案

Abstract: Analyzing origin-destination flows is an important problem that has been
extensively investigated in several scientific fields, particularly by the
visualization community. The problem becomes especially challenging when
involving massive data, demanding mechanisms such as data aggregation and
interactive filtering to make the exploratory process doable. However, data
aggregation tends to smooth out certain patterns, and deciding which data
should be filtered is not straightforward. In this work, we propose ORDENA, a
visual analytic tool to explore origin and destination data. ORDENA is built
upon a simple and intuitive scatter plot where the horizontal and vertical axes
correspond to origins and destinations. Therefore, each origin-destination flow
is represented as a point in the scatter plot. How the points are organized in
the plot layout reveals important spatial phenomena present in the data.
Moreover, ORDENA provides explainability resources that allow users to better
understand the relation between origin-destination flows and associated
attributes. We illustrate ORDENA's effectiveness in a set of case studies,
which have also been elaborated in collaboration with domain experts. The
proposed tool has also been evaluated by domain experts not involved in its
development, which provided quite positive feedback about ORDENA.

</details>


### [70] [MorphModes: Non-rigid Registration via Adaptive Skinning Eigenmodes](https://arxiv.org/abs/2510.18658)
*Gabrielle Browne,Mengfei Liu,Eitan Grinspun,Otman Benchekroun*

Main category: cs.GR

TL;DR: 提出基于符号距离函数匹配和自适应子空间优化的非刚性配准方法，相比传统NRICP更鲁棒且参数敏感性低


<details>
  <summary>Details</summary>
Motivation: 传统非刚性迭代最近点算法(NRICP)存在局部极小陷阱和初始条件敏感性问题，需要更鲁棒的配准方法

Method: 1. 将配准问题转化为SDF匹配优化 2. 使用皮肤本征模态子空间参数化 3. 提出自适应子空间优化方案处理局部变形

Result: 新方法在鲁棒性上超越NRICP，同时避免其他SDF方法的参数敏感性缺陷

Conclusion: 通过SDF匹配与子空间参数化的创新结合，有效提升了非刚性配准算法的稳定性和适用性

Abstract: Non-rigid registration is a crucial task with applications in medical
imaging, industrial robotics, computer vision, and entertainment. Standard
approaches accomplish this task using variations on the Non-Rigid Iterative
Closest Point (NRICP) algorithms, which are prone to local minima and sensitive
to initial conditions. We instead formulate the non-rigid registration problem
as a Signed Distance Function (SDF) matching optimization problem, which
provides richer shape information compared to traditional ICP methods. To avoid
degenerate solutions, we propose to use a smooth Skinning Eigenmode subspace to
parameterize the optimization problem. Finally, we propose an adaptive subspace
optimization scheme to allow the resolution of localized deformations within
the optimization. The result is a non-rigid registration algorithm that is more
robust than NRICP, without the parameter sensitivity present in other
SDF-matching approaches.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [71] [From Competition to Synergy: Unlocking Reinforcement Learning for Subject-Driven Image Generation](https://arxiv.org/abs/2510.18263)
*Ziwei Huang,Ying Shu,Hao Fang,Quanyu Long,Wenya Wang,Qiushi Guo,Tiezheng Ge,Leilei Gan*

Main category: cs.LG

TL;DR: 提出Customized-GRPO框架，通过SARS机制和TDW策略解决图像生成模型中保真度与可编辑性的权衡问题


<details>
  <summary>Details</summary>
Motivation: 传统GRPO方法因静态权重聚合导致梯度冲突和时间动态错位，引发竞争性退化现象

Method: 1. Synergy-Aware Reward Shaping (SARS)：非线性奖惩机制优化梯度信号
2. Time-Aware Dynamic Weighting (TDW)：动态权重策略适配扩散过程阶段特性

Result: 实验显示方法显著优于基准模型，在保持身份特征和遵循复杂提示间实现更好平衡

Conclusion: 该框架成功缓解竞争性退化，为身份保持与编辑能力的协同优化提供新方向

Abstract: Subject-driven image generation models face a fundamental trade-off between
identity preservation (fidelity) and prompt adherence (editability). While
online reinforcement learning (RL), specifically GPRO, offers a promising
solution, we find that a naive application of GRPO leads to competitive
degradation, as the simple linear aggregation of rewards with static weights
causes conflicting gradient signals and a misalignment with the temporal
dynamics of the diffusion process. To overcome these limitations, we propose
Customized-GRPO, a novel framework featuring two key innovations: (i)
Synergy-Aware Reward Shaping (SARS), a non-linear mechanism that explicitly
penalizes conflicted reward signals and amplifies synergistic ones, providing a
sharper and more decisive gradient. (ii) Time-Aware Dynamic Weighting (TDW),
which aligns the optimization pressure with the model's temporal dynamics by
prioritizing prompt-following in the early, identity preservation in the later.
Extensive experiments demonstrate that our method significantly outperforms
naive GRPO baselines, successfully mitigating competitive degradation. Our
model achieves a superior balance, generating images that both preserve key
identity features and accurately adhere to complex textual prompts.

</details>


### [72] [Hierarchical Federated Unlearning for Large Language Models](https://arxiv.org/abs/2510.17895)
*Yisheng Zhong,Zhengbang Yang,Zhuangdi Zhu*

Main category: cs.LG

TL;DR: 提出保护隐私的联邦遗忘框架，解决LLM连续异构遗忘需求与性能平衡问题


<details>
  <summary>Details</summary>
Motivation: 现有LLM遗忘方法面临连续异构需求与敏感数据分散访问的挑战，导致跨域/域内干扰和遗忘-保留性能失衡

Method: 通过任务特定适配器学习解耦遗忘/保留过程，采用分层合并策略缓解目标冲突，实现隐私保护的联邦更新机制

Result: 在WMDP/MUSE/TOFU基准测试中有效处理异构遗忘请求，LLM效用保持优于基线方法

Conclusion: 该框架为LLM安全部署提供了可扩展的解决方案，平衡隐私保护与模型性能，适应现实场景的复杂需求

Abstract: Large Language Models (LLMs) are increasingly integrated into real-world
applications, raising concerns about privacy, security and the need to remove
undesirable knowledge. Machine Unlearning has emerged as a promising solution,
yet faces two key challenges: (1) practical unlearning needs are often
continuous and heterogeneous, and (2) they involve decentralized, sensitive
data with asymmetric access. These factors result in inter-domain and
intra-domain interference, which further amplifies the dilemma of unbalanced
forgetting and retaining performance. In response, we propose a federated
unlearning approach for LLMs that is scalable and privacy preserving. Our
method decouples unlearning and retention via task-specific adapter learning
and employs a hierarchical merging strategy to mitigate conflicting objectives
and enables robust, adaptable unlearning updates. Comprehensive experiments on
benchmarks of WMDP, MUSE, and TOFU showed that our approach effectively handles
heterogeneous unlearning requests while maintaining strong LLM utility compared
with baseline methods.

</details>


### [73] [Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting](https://arxiv.org/abs/2510.18874)
*Howard Chen,Noam Razin,Karthik Narasimhan,Danqi Chen*

Main category: cs.LG

TL;DR: 强化学习（RL）比监督微调（SFT）在后训练中更能减少语言模型的灾难性遗忘，同时保持目标任务性能，关键机制在于RL的同策略数据利用模式。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决语言模型后训练（如监督微调和强化学习）导致的灾难性遗忘问题，探索不同方法的遗忘模式差异。

Method: 通过对比实验（Llama/Qwen模型，指令遵循/常识/数学推理任务）和理论建模（将LM视为先验知识与目标任务的混合分布），分析SFT与RL的遗忘特性。

Result: RL在所有场景下均表现出更少遗忘，且性能不弱于SFT；理论证明RL的模式寻求特性（源于同策略数据）可保护先验知识。

Conclusion: 使用近似同策略数据可高效缓解遗忘，这为实际应用提供了比完全同策略数据更经济的解决方案。

Abstract: Adapting language models (LMs) to new tasks via post-training carries the
risk of degrading existing capabilities -- a phenomenon classically known as
catastrophic forgetting. In this paper, toward identifying guidelines for
mitigating this phenomenon, we systematically compare the forgetting patterns
of two widely adopted post-training methods: supervised fine-tuning (SFT) and
reinforcement learning (RL). Our experiments reveal a consistent trend across
LM families (Llama, Qwen) and tasks (instruction following, general knowledge,
and arithmetic reasoning): RL leads to less forgetting than SFT while achieving
comparable or higher target task performance. To investigate the cause for this
difference, we consider a simplified setting in which the LM is modeled as a
mixture of two distributions, one corresponding to prior knowledge and the
other to the target task. We identify that the mode-seeking nature of RL, which
stems from its use of on-policy data, enables keeping prior knowledge intact
when learning the target task. We then verify this insight by demonstrating
that the use on-policy data underlies the robustness of RL to forgetting in
practical settings, as opposed to other algorithmic choices such as the KL
regularization or advantage estimation. Lastly, as a practical implication, our
results highlight the potential of mitigating forgetting using approximately
on-policy data, which can be substantially more efficient to obtain than fully
on-policy data.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [74] [Metrics and evaluations for computational and sustainable AI efficiency](https://arxiv.org/abs/2510.17885)
*Hongyuan Liu,Xinyang Liu,Guosheng Hu*

Main category: cs.PF

TL;DR: 提出集成计算与环境指标的统一AI模型推理评估框架，通过多硬件平台验证建立决策用帕累托前沿


<details>
  <summary>Details</summary>
Motivation: 当前AI性能评估方法零散且缺乏整体视角，难以在异构硬件/软件环境下进行有效比较和优化

Method: 系统测量延迟吞吐分布、能耗及区域碳排放，在保持准确性的前提下对比多精度模型在GH200/RTX4090等异构硬件及PyTorch/TensorRT软件栈的表现

Result: 建立可重复基准框架生成决策帕累托前沿，明确精度-延迟-能耗-碳排放的权衡关系，配套开源代码支持验证

Conclusion: 该碳感知方法论为可持续AI部署提供证据支持，标准化评估流程促进跨平台优化决策

Abstract: The rapid advancement of Artificial Intelligence (AI) has created
unprecedented demands for computational power, yet methods for evaluating the
performance, efficiency, and environmental impact of deployed models remain
fragmented. Current approaches often fail to provide a holistic view, making it
difficult to compare and optimise systems across heterogeneous hardware,
software stacks, and numeric precisions. To address this gap, we propose a
unified and reproducible methodology for AI model inference that integrates
computational and environmental metrics under realistic serving conditions. Our
framework provides a pragmatic, carbon-aware evaluation by systematically
measuring latency and throughput distributions, energy consumption, and
location-adjusted carbon emissions, all while maintaining matched accuracy
constraints for valid comparisons. We apply this methodology to multi-precision
models across diverse hardware platforms, from data-centre accelerators like
the GH200 to consumer-level GPUs such as the RTX 4090, running on mainstream
software stacks including PyTorch, TensorRT, and ONNX Runtime. By
systematically categorising these factors, our work establishes a rigorous
benchmarking framework that produces decision-ready Pareto frontiers,
clarifying the trade-offs between accuracy, latency, energy, and carbon. The
accompanying open-source code enables independent verification and facilitates
adoption, empowering researchers and practitioners to make evidence-based
decisions for sustainable AI deployment.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [75] [CodeRL+: Improving Code Generation via Reinforcement with Execution Semantics Alignment](https://arxiv.org/abs/2510.18471)
*Xue Jiang,Yihong Dong,Mengyang Liu,Hongyi Deng,Tian Wang,Yongding Tao,Rongyu Cao,Binhua Li,Zhi Jin,Wenpin Jiao,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.SE

TL;DR: 提出CodeRL+方法，通过将代码执行语义对齐整合到强化学习训练流程中，提升大语言模型的代码生成质量


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在代码生成时存在文本模式训练与程序功能正确性之间的语义鸿沟，仅依赖测试用例的二元反馈效率低下，难以捕捉代码中的逻辑错误

Method: 通过推断变量级执行轨迹提供直接语义学习信号，利用现有策略轨迹构建语义对齐，兼容多种强化学习算法

Result: 在代码生成任务上相对基准模型平均提升4.6%通过率，在代码推理和测试输出生成任务分别提升15.5%和4.4%准确率

Conclusion: CodeRL+有效强化了代码文本表示与执行语义的关联，实验证明该方法具有算法普适性和模型通用性，探针分析验证了语义对齐效果

Abstract: While Large Language Models (LLMs) excel at code generation by learning from
vast code corpora, a fundamental semantic gap remains between their training on
textual patterns and the goal of functional correctness, which is governed by
formal execution semantics. Reinforcement Learning with Verifiable Rewards
(RLVR) approaches attempt to bridge this gap using outcome rewards from
executing test cases. However, solely relying on binary pass/fail signals is
inefficient for establishing a well-aligned connection between the textual
representation of code and its execution semantics, especially for subtle
logical errors within the code. In this paper, we propose CodeRL+, a novel
approach that integrates execution semantics alignment into the RLVR training
pipeline for code generation. CodeRL+ enables the model to infer variable-level
execution trajectory, providing a direct learning signal of execution
semantics. CodeRL+ can construct execution semantics alignment directly using
existing on-policy rollouts and integrates seamlessly with various RL
algorithms. Extensive experiments demonstrate that CodeRL+ outperforms
post-training baselines (including RLVR and Distillation), achieving a 4.6%
average relative improvement in pass@1. CodeRL+ generalizes effectively to
other coding tasks, yielding 15.5% and 4.4% higher accuracy on code-reasoning
and test-output-generation benchmarks, respectively. CodeRL+ shows strong
applicability across diverse RL algorithms and LLMs. Furthermore, probe
analyses provide compelling evidence that CodeRL+ strengthens the alignment
between code's textual representations and its underlying execution semantics.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [76] [BreakFun: Jailbreaking LLMs via Schema Exploitation](https://arxiv.org/abs/2510.17904)
*Amirkia Rafiei Oskooei,Mehmet S. Aktas*

Main category: cs.CR

TL;DR: 研究提出BreakFun攻击方法，利用LLMs遵循结构化模式的特性强制生成有害内容，攻击成功率高达89%，并提出基于对抗提示解构的有效防御策略


<details>
  <summary>Details</summary>
Motivation: 大型语言模型处理结构化数据的能力既是核心优势，却也可能成为被攻击的致命弱点。本研究旨在揭示这种结构化依从性带来的安全漏洞

Method: 设计包含三部分提示的BreakFun攻击：1）无害场景设定 2）思维链干扰 3）核心的『木马模式』结构化模板，强制模型输出违规内容

Result: 在JailbreakBench上对13个主流模型平均攻击成功率89%，部分模型达100% ASR；提出的防御方案通过『字面转录』有效隔离恶意指令

Conclusion: LLMs的核心能力可能转化为安全漏洞，通过针对性防御策略可有效缓解。该研究为构建更鲁棒的AI对齐模型提供了新视角

Abstract: The proficiency of Large Language Models (LLMs) in processing structured data
and adhering to syntactic rules is a capability that drives their widespread
adoption but also makes them paradoxically vulnerable. In this paper, we
investigate this vulnerability through BreakFun, a jailbreak methodology that
weaponizes an LLM's adherence to structured schemas. BreakFun employs a
three-part prompt that combines an innocent framing and a Chain-of-Thought
distraction with a core "Trojan Schema"--a carefully crafted data structure
that compels the model to generate harmful content, exploiting the LLM's strong
tendency to follow structures and schemas. We demonstrate this vulnerability is
highly transferable, achieving an average success rate of 89% across 13
foundational and proprietary models on JailbreakBench, and reaching a 100%
Attack Success Rate (ASR) on several prominent models. A rigorous ablation
study confirms this Trojan Schema is the attack's primary causal factor. To
counter this, we introduce the Adversarial Prompt Deconstruction guardrail, a
defense that utilizes a secondary LLM to perform a "Literal
Transcription"--extracting all human-readable text to isolate and reveal the
user's true harmful intent. Our proof-of-concept guardrail demonstrates high
efficacy against the attack, validating that targeting the deceptive schema is
a viable mitigation strategy. Our work provides a look into how an LLM's core
strengths can be turned into critical weaknesses, offering a fresh perspective
for building more robustly aligned models.

</details>


### [77] [PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits](https://arxiv.org/abs/2510.17947)
*Neeladri Bhuiya,Madhav Aggarwal,Diptanshu Purwar*

Main category: cs.CR

TL;DR: 提出PLAGUE框架提升大语言模型多轮越狱攻击成功率，在OpenAI/Claude模型上取得突破性ASR指标


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注单轮攻击，多轮场景中恶意意图可隐蔽传递且防御薄弱，需系统性攻击框架验证模型安全性

Method: PLAGUE框架分解攻击生命周期为Primer(启动)、Planner(规划)、Finisher(终结)三阶段，实现终身学习式的上下文优化

Result: 攻击成功率提升30%+，OpenAI o3模型ASR达81.4%，Claude Opus 4.1达67.3%，显著超越现有方法

Conclusion: PLAGUE为模型漏洞评估提供系统性工具，揭示攻击初始化、上下文优化和终身学习机制在多轮攻击中的关键作用

Abstract: Large Language Models (LLMs) are improving at an exceptional rate. With the
advent of agentic workflows, multi-turn dialogue has become the de facto mode
of interaction with LLMs for completing long and complex tasks. While LLM
capabilities continue to improve, they remain increasingly susceptible to
jailbreaking, especially in multi-turn scenarios where harmful intent can be
subtly injected across the conversation to produce nefarious outcomes. While
single-turn attacks have been extensively explored, adaptability, efficiency
and effectiveness continue to remain key challenges for their multi-turn
counterparts. To address these gaps, we present PLAGUE, a novel plug-and-play
framework for designing multi-turn attacks inspired by lifelong-learning
agents. PLAGUE dissects the lifetime of a multi-turn attack into three
carefully designed phases (Primer, Planner and Finisher) that enable a
systematic and information-rich exploration of the multi-turn attack family.
Evaluations show that red-teaming agents designed using PLAGUE achieve
state-of-the-art jailbreaking results, improving attack success rates (ASR) by
more than 30% across leading models in a lesser or comparable query budget.
Particularly, PLAGUE enables an ASR (based on StrongReject) of 81.4% on
OpenAI's o3 and 67.3% on Claude's Opus 4.1, two models that are considered
highly resistant to jailbreaks in safety literature. Our work offers tools and
insights to understand the importance of plan initialization, context
optimization and lifelong learning in crafting multi-turn attacks for a
comprehensive model vulnerability evaluation.

</details>


### [78] [Position: LLM Watermarking Should Align Stakeholders' Incentives for Practical Adoption](https://arxiv.org/abs/2510.18333)
*Yepeng Liu,Xuandong Zhao,Dawn Song,Gregory W. Wornell,Yuheng Bu*

Main category: cs.CR

TL;DR: 探讨LLM水印技术实际应用障碍，提出激励协调的上下文水印（ICW）方案，强调利益相关者协同的必要性


<details>
  <summary>Details</summary>
Motivation: 现有水印算法虽进步显著，但因LLM服务商、平台和用户间的激励错位，导致实际部署受限。四重障碍包括：竞争风险、检测工具治理、鲁棒性担忧和归属问题

Method: 1）模型水印适合闭源场景但面临开源挑战；2）文本水印在反滥用场景效果有限，适用于数据集净化等窄域；3）上下文水印（ICW）通过可信第三方嵌入检测指令，平衡各方利益

Result: ICW在可信场景（如学术评审/教育）实现零质量损失的滥用检测，服务商保持中立，激励三方协调。需发展领域专用、激励协调的水印方案

Conclusion: LLM水印实用化需：在目标领域协调利益相关者、培育社区共建生态、发展领域定制化方案。ICW模式为可信场景提供可行路径

Abstract: Despite progress in watermarking algorithms for large language models (LLMs),
real-world deployment remains limited. We argue that this gap stems from
misaligned incentives among LLM providers, platforms, and end users, which
manifest as four key barriers: competitive risk, detection-tool governance,
robustness concerns and attribution issues. We revisit three classes of
watermarking through this lens. \emph{Model watermarking} naturally aligns with
LLM provider interests, yet faces new challenges in open-source ecosystems.
\emph{LLM text watermarking} offers modest provider benefit when framed solely
as an anti-misuse tool, but can gain traction in narrowly scoped settings such
as dataset de-contamination or user-controlled provenance. \emph{In-context
watermarking} (ICW) is tailored for trusted parties, such as conference
organizers or educators, who embed hidden watermarking instructions into
documents. If a dishonest reviewer or student submits this text to an LLM, the
output carries a detectable watermark indicating misuse. This setup aligns
incentives: users experience no quality loss, trusted parties gain a detection
tool, and LLM providers remain neutral by simply following watermark
instructions. We advocate for a broader exploration of incentive-aligned
methods, with ICW as an example, in domains where trusted parties need reliable
tools to detect misuse. More broadly, we distill design principles for
incentive-aligned, domain-specific watermarking and outline future research
directions. Our position is that the practical adoption of LLM watermarking
requires aligning stakeholder incentives in targeted application domains and
fostering active community engagement.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [79] [Does GenAI Rewrite How We Write? An Empirical Study on Two-Million Preprints](https://arxiv.org/abs/2510.17882)
*Minfeng Qi,Zhongmin Cao,Qin Wang,Ningran Li,Tianqing Zhu*

Main category: cs.CY

TL;DR: 大型语言模型加速了预印本提交与修订周期，选择性催化AI相关领域发展并加剧学科分化


<details>
  <summary>Details</summary>
Motivation: 填补生成式AI对学术出版影响的实证研究空白，突破现有推测性讨论的局限

Method: 基于210万篇跨4大预印本库（2016-2025）的多层次分析框架，整合中断时间序列模型、协作指标、语言特征分析和主题建模

Result: LLMs使语言复杂度提升28%，AI主题增长超3倍，计算密集型领域接受度较其他学科高47%

Conclusion: 生成式AI作为选择性催化剂强化现有优势，需建立信任保障机制平衡技术红利与学术公平

Abstract: Preprint repositories become central infrastructures for scholarly
communication. Their expansion transforms how research is circulated and
evaluated before journal publication. Generative large language models (LLMs)
introduce a further potential disruption by altering how manuscripts are
written. While speculation abounds, systematic evidence of whether and how LLMs
reshape scientific publishing remains limited.
  This paper addresses the gap through a large-scale analysis of more than 2.1
million preprints spanning 2016--2025 (115 months) across four major
repositories (i.e., arXiv, bioRxiv, medRxiv, SocArXiv). We introduce a
multi-level analytical framework that integrates interrupted time-series
models, collaboration and productivity metrics, linguistic profiling, and topic
modeling to assess changes in volume, authorship, style, and disciplinary
orientation. Our findings reveal that LLMs have accelerated submission and
revision cycles, modestly increased linguistic complexity, and
disproportionately expanded AI-related topics, while computationally intensive
fields benefit more than others. These results show that LLMs act less as
universal disruptors than as selective catalysts, amplifying existing strengths
and widening disciplinary divides. By documenting these dynamics, the paper
provides the first empirical foundation for evaluating the influence of
generative AI on academic publishing and highlights the need for governance
frameworks that preserve trust, fairness, and accountability in an AI-enabled
research ecosystem.

</details>


### [80] [Are LLMs Court-Ready? Evaluating Frontier Models on Indian Legal Reasoning](https://arxiv.org/abs/2510.17900)
*Kush Juvekar,Arghya Bhattacharya,Sai Khadloya,Utkarsh Saxena*

Main category: cs.CY

TL;DR: 研究评估了大型语言模型在印度法律考试中的表现，发现其在客观题上达标但长答案推理不及人类，存在格式、引用和结构问题，需人类主导关键法律任务。


<details>
  <summary>Details</summary>
Motivation: 缺乏针对特定司法管辖区（如印度）评估LLM法律能力的框架，需透明基准衡量其法庭适用性。

Method: 使用印度国家级/州级法律考试构建多年基准，包含客观题和最高法院律师资格考试的长篇答案盲评。

Result: 前沿LLM通过历史分数线且常超越近年考生平均分，但无人模型超越人类顶尖长答案得分；主要失败模式为格式合规、引用规范及语境化结构问题。

Conclusion: LLM适用于法律检查、法规查询等辅助工作，但文件起草、程序策略、权威整合及伦理判断仍需人类主导。

Abstract: Large language models (LLMs) are entering legal workflows, yet we lack a
jurisdiction-specific framework to assess their baseline competence therein. We
use India's public legal examinations as a transparent proxy. Our multi-year
benchmark assembles objective screens from top national and state exams and
evaluates open and frontier LLMs under real-world exam conditions. To probe
beyond multiple-choice questions, we also include a lawyer-graded,
paired-blinded study of long-form answers from the Supreme Court's
Advocate-on-Record exam. This is, to our knowledge, the first exam-grounded,
India-specific yardstick for LLM court-readiness released with datasets and
protocols. Our work shows that while frontier systems consistently clear
historical cutoffs and often match or exceed recent top-scorer bands on
objective exams, none surpasses the human topper on long-form reasoning. Grader
notes converge on three reliability failure modes: procedural or format
compliance, authority or citation discipline, and forum-appropriate voice and
structure. These findings delineate where LLMs can assist (checks,
cross-statute consistency, statute and precedent lookups) and where human
leadership remains essential: forum-specific drafting and filing, procedural
and relief strategy, reconciling authorities and exceptions, and ethical,
accountable judgment.

</details>


### [81] [Interpretability Framework for LLMs in Undergraduate Calculus](https://arxiv.org/abs/2510.17910)
*Sagnik Dakshit,Sushmita Sinha Roy*

Main category: cs.CY

TL;DR: 提出首个基于数学教育视角的量化解释框架，用于诊断LLM在微积分问题中的推理缺陷与稳定性问题


<details>
  <summary>Details</summary>
Motivation: 传统评估方法仅关注答案正确性，忽视LLM解题过程的概念有效性及推理稳定性，可能导致教育应用中的潜在风险

Method: 结合推理流程提取、语义操作分解与提示消融分析，开发包含推理复杂度、短语敏感度、鲁棒性等结构化指标的分析框架

Result: LLM常生成语法流畅但存在概念错误的解，且推理模式对提示词表述和输入变化高度敏感

Conclusion: 该框架为STEM教育中AI透明部署奠定基础，支持课程对齐与可解释反馈工具设计

Abstract: Large Language Models (LLMs) are increasingly being used in education, yet
their correctness alone does not capture the quality, reliability, or
pedagogical validity of their problem-solving behavior, especially in
mathematics, where multistep logic, symbolic reasoning, and conceptual clarity
are critical. Conventional evaluation methods largely focus on final answer
accuracy and overlook the reasoning process. To address this gap, we introduce
a novel interpretability framework for analyzing LLM-generated solutions using
undergraduate calculus problems as a representative domain. Our approach
combines reasoning flow extraction and decomposing solutions into semantically
labeled operations and concepts with prompt ablation analysis to assess input
salience and output stability. Using structured metrics such as reasoning
complexity, phrase sensitivity, and robustness, we evaluated the model behavior
on real Calculus I to III university exams. Our findings revealed that LLMs
often produce syntactically fluent yet conceptually flawed solutions, with
reasoning patterns sensitive to prompt phrasing and input variation. This
framework enables fine-grained diagnosis of reasoning failures, supports
curriculum alignment, and informs the design of interpretable AI-assisted
feedback tools. This is the first study to offer a structured, quantitative,
and pedagogically grounded framework for interpreting LLM reasoning in
mathematics education, laying the foundation for the transparent and
responsible deployment of AI in STEM learning environments.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [82] [Subject-Event Ontology Without Global Time: Foundations and Execution Semantics](https://arxiv.org/abs/2510.18040)
*Alexander Boldachev*

Main category: cs.AI

TL;DR: 提出基于主体-事件的本体论形式化方法，通过声明式数据流机制实现无全局时间依赖的复杂系统建模，确保确定性执行。


<details>
  <summary>Details</summary>
Motivation: 解决传统系统依赖全局时间戳的局限性，适应分布式系统、微服务架构等需要因果排序而非时间戳的场景。

Method: 定义事件为基于模型的固定行为（A9），构建happens-before因果序（A1-A9公理），采用模式验证、角色授权和自动因果链构建（W3）的声明式机制。

Result: 开发boldsea工作流引擎实现理论框架（BSL语言验证），支持分布式账本、多方视角冲突场景下的确定性执行。

Conclusion: 该形式化方法通过模型驱动的事件验证和局部因果序，为无中心时钟的复杂系统提供了可扩展的本体论建模基础。

Abstract: A formalization of a subject-event ontology is proposed for modeling complex
dynamic systems without reliance on global time. Key principles: (1) event as
an act of fixation - a subject discerns and fixes changes according to models
(conceptual templates) available to them; (2) causal order via happens-before -
the order of events is defined by explicit dependencies, not timestamps; (3)
making the ontology executable via a declarative dataflow mechanism, ensuring
determinism; (4) models as epistemic filters - a subject can only fix what
falls under its known concepts and properties; (5) presumption of truth - the
declarative content of an event is available for computation from the moment of
fixation, without external verification. The formalization includes nine axioms
(A1-A9), ensuring the correctness of executable ontologies: monotonicity of
history (I1), acyclicity of causality (I2), traceability (I3). Special
attention is given to the model-based approach (A9): event validation via
schemas, actor authorization, automatic construction of causal chains (W3)
without global time. Practical applicability is demonstrated on the boldsea
system - a workflow engine for executable ontologies, where the theoretical
constructs are implemented in BSL (Boldsea Semantic Language). The
formalization is applicable to distributed systems, microservice architectures,
DLT platforms, and multiperspectivity scenarios (conflicting facts from
different subjects).

</details>


### [83] [SMaRT: Select, Mix, and ReinvenT -- A Strategy Fusion Framework for LLM-Driven Reasoning and Planning](https://arxiv.org/abs/2510.18095)
*Nikhil Verma,Manasa Bharadwaj,Wonjun Jang,Harmanpreet Singh,Yixiao Wang,Homa Fashandi,Chul Lee*

Main category: cs.AI

TL;DR: 提出SMaRT框架，通过多策略融合实现LLM决策性能突破


<details>
  <summary>Details</summary>
Motivation: 现有单策略方法存在局限性，需要跨策略协同提升模型鲁棒性和适应性

Method: 选择-混合-重构三阶段框架，将LLM作为策略集成器而非单纯评估器

Result: 在推理、规划和序列决策任务中全面超越现有基准方法

Conclusion: 开创跨策略校准新范式，推动自优化方法学的边界扩展

Abstract: Large Language Models (LLMs) have redefined complex task automation with
exceptional generalization capabilities. Despite these advancements,
state-of-the-art methods rely on single-strategy prompting, missing the synergy
of diverse reasoning approaches. No single strategy excels universally,
highlighting the need for frameworks that fuse strategies to maximize
performance and ensure robustness. We introduce the Select, Mix, and ReinvenT
(SMaRT) framework, an innovative strategy fusion approach designed to overcome
this constraint by creating balanced and efficient solutions through the
seamless integration of diverse reasoning strategies. Unlike existing methods,
which employ LLMs merely as evaluators, SMaRT uses them as intelligent
integrators, unlocking the "best of all worlds" across tasks. Extensive
empirical evaluations across benchmarks in reasoning, planning, and sequential
decision-making highlight the robustness and adaptability of SMaRT. The
framework consistently outperforms state-of-the-art baselines in solution
quality, constraint adherence, and performance metrics. This work redefines
LLM-driven decision-making by pioneering a new paradigm in cross-strategy
calibration, unlocking superior outcomes for reasoning systems and advancing
the boundaries of self-refining methodologies.

</details>


### [84] [Saber: An Efficient Sampling with Adaptive Acceleration and Backtracking Enhanced Remasking for Diffusion Language Model](https://arxiv.org/abs/2510.18165)
*Yihong Dong,Zhaoyu Ma,Xue Jiang,Zhiyuan Fan,Jiaru Qian,Yongmin Li,Jianha Xiao,Zhi Jin,Rongyu Cao,Binhua Li,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: 提出Saber算法解决扩散语言模型在代码生成中速度与质量的矛盾，实现1.9%准确率提升和251.4%加速效果


<details>
  <summary>Details</summary>
Motivation: 扩散模型在代码生成中存在加速采样导致性能骤降的核心矛盾，需要突破传统采样方法限制

Method: 基于代码上下文自适应的加速采样机制 + 可回溯的token修正增强策略

Result: 主流基准测试平均Pass@1提升1.9%，推理速度提升251.4%

Conclusion: 通过创新采样算法显著缩小与自回归模型差距，增强扩散模型在结构化生成的竞争力

Abstract: Diffusion language models (DLMs) are emerging as a powerful and promising
alternative to the dominant autoregressive paradigm, offering inherent
advantages in parallel generation and bidirectional context modeling. However,
the performance of DLMs on code generation tasks, which have stronger
structural constraints, is significantly hampered by the critical trade-off
between inference speed and output quality. We observed that accelerating the
code generation process by reducing the number of sampling steps usually leads
to a catastrophic collapse in performance. In this paper, we introduce
efficient Sampling with Adaptive acceleration and Backtracking Enhanced
Remasking (i.e., Saber), a novel training-free sampling algorithm for DLMs to
achieve better inference speed and output quality in code generation.
Specifically, Saber is motivated by two key insights in the DLM generation
process: 1) it can be adaptively accelerated as more of the code context is
established; 2) it requires a backtracking mechanism to reverse the generated
tokens. Extensive experiments on multiple mainstream code generation benchmarks
show that Saber boosts Pass@1 accuracy by an average improvement of 1.9% over
mainstream DLM sampling methods, meanwhile achieving an average 251.4%
inference speedup. By leveraging the inherent advantages of DLMs, our work
significantly narrows the performance gap with autoregressive models in code
generation.

</details>


### [85] [Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents](https://arxiv.org/abs/2510.18476)
*Feifan Xia,Yuyang Fang,Defang Li,Yantong Xie,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: 提出概率意图建模框架，通过动态更新对话伙伴潜在意图的信念分布，提升大语言模型在社交对话中的表现


<details>
  <summary>Details</summary>
Motivation: 为解决多轮社交对话中LLM智能体对伙伴意图建模不足的问题，增强对话策略的语境适应性

Method: 基于上下文先验初始化意图分布，通过似然估计动态更新信念分布，为策略提供持续增强的语境信息

Result: 在SOTOPIA环境测试中，整体得分提升9.0%（All）和4.1%（Hard），甚至略微超越可直接观测意图的Oracle智能体

Conclusion: 概率意图建模方法有效提升了LLM智能体的社会智能水平，为开发社交智能体提供了新思路

Abstract: We present a probabilistic intent modeling framework for large language model
(LLM) agents in multi-turn social dialogue. The framework maintains a belief
distribution over a partner's latent intentions, initialized from contextual
priors and dynamically updated through likelihood estimation after each
utterance. The evolving distribution provides additional contextual grounding
for the policy, enabling adaptive dialogue strategies under uncertainty.
Preliminary experiments in the SOTOPIA environment show consistent
improvements: the proposed framework increases the Overall score by 9.0% on
SOTOPIA-All and 4.1% on SOTOPIA-Hard compared with the Qwen2.5-7B baseline, and
slightly surpasses an oracle agent that directly observes partner intentions.
These early results suggest that probabilistic intent modeling can contribute
to the development of socially intelligent LLM agents.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [86] [HouseTour: A Virtual Real Estate A(I)gent](https://arxiv.org/abs/2510.18054)
*Ata Çelen,Marc Pollefeys,Daniel Barath,Iro Armeni*

Main category: cs.CV

TL;DR: 提出HouseTour方法，通过3D空间图像生成空间感知的摄像机轨迹和自然语言摘要，结合扩散过程和3D高斯溅射技术实现专业级自动视频生成


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型(VLMs)在几何推理方面存在不足，需开发能整合3D摄像机位姿信息的新型视频生成框架

Method: 1. 通过受摄像机位姿约束的扩散过程生成平滑视频轨迹
2. 将3D几何信息融入VLM实现空间感知描述
3. 使用3D高斯溅射技术沿轨迹渲染新视角

Result: 实验表明整合3D摄像机轨迹使文本生成性能超越独立处理方法，提出新的联合评估指标，端到端系统在真实数据集上验证有效

Conclusion: 该方法无需专业设备即可自动生成房地产和旅游应用所需的专业级视频，推动三维空间可视化技术的实用化进程

Abstract: We introduce HouseTour, a method for spatially-aware 3D camera trajectory and
natural language summary generation from a collection of images depicting an
existing 3D space. Unlike existing vision-language models (VLMs), which
struggle with geometric reasoning, our approach generates smooth video
trajectories via a diffusion process constrained by known camera poses and
integrates this information into the VLM for 3D-grounded descriptions. We
synthesize the final video using 3D Gaussian splatting to render novel views
along the trajectory. To support this task, we present the HouseTour dataset,
which includes over 1,200 house-tour videos with camera poses, 3D
reconstructions, and real estate descriptions. Experiments demonstrate that
incorporating 3D camera trajectories into the text generation process improves
performance over methods handling each task independently. We evaluate both
individual and end-to-end performance, introducing a new joint metric. Our work
enables automated, professional-quality video creation for real estate and
touristic applications without requiring specialized expertise or equipment.

</details>


### [87] [SafeCoop: Unravelling Full Stack Safety in Agentic Collaborative Driving](https://arxiv.org/abs/2510.18123)
*Xiangbo Gao,Tzu-Hsiang Lin,Ruojing Song,Yuheng Wu,Kuan-Ru Huang,Zicheng Jin,Fangzhou Lin,Shinan Liu,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 系统研究自然语言在协作驾驶中的安全风险，并提出SafeCoop防御框架提升系统安全性


<details>
  <summary>Details</summary>
Motivation: 传统V2X系统存在高带宽需求、语义丢失和互操作性问题，自然语言虽降低带宽但引入消息丢失、语义篡改等新安全漏洞

Method: 提出包含语义防火墙、语言-感知一致性校验和多源共识的SafeCoop防御框架，采用跨帧空间对齐的代理转换函数

Result: 在CARLA仿真中实现69.15%的驾驶评分提升，恶意检测F1分数达67.32%

Conclusion: 为构建安全可靠的语言驱动交通协作系统提供理论指导和实践方案

Abstract: Collaborative driving systems leverage vehicle-to-everything (V2X)
communication across multiple agents to enhance driving safety and efficiency.
Traditional V2X systems take raw sensor data, neural features, or perception
results as communication media, which face persistent challenges, including
high bandwidth demands, semantic loss, and interoperability issues. Recent
advances investigate natural language as a promising medium, which can provide
semantic richness, decision-level reasoning, and human-machine interoperability
at significantly lower bandwidth. Despite great promise, this paradigm shift
also introduces new vulnerabilities within language communication, including
message loss, hallucinations, semantic manipulation, and adversarial attacks.
In this work, we present the first systematic study of full-stack safety and
security issues in natural-language-based collaborative driving. Specifically,
we develop a comprehensive taxonomy of attack strategies, including connection
disruption, relay/replay interference, content spoofing, and multi-connection
forgery. To mitigate these risks, we introduce an agentic defense pipeline,
which we call SafeCoop, that integrates a semantic firewall,
language-perception consistency checks, and multi-source consensus, enabled by
an agentic transformation function for cross-frame spatial alignment. We
systematically evaluate SafeCoop in closed-loop CARLA simulation across 32
critical scenarios, achieving 69.15% driving score improvement under malicious
attacks and up to 67.32% F1 score for malicious detection. This study provides
guidance for advancing research on safe, secure, and trustworthy
language-driven collaboration in transportation systems. Our project page is
https://xiangbogaobarry.github.io/SafeCoop.

</details>


### [88] [VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety](https://arxiv.org/abs/2510.18214)
*Shruti Palaskar,Leon Gatys,Mona Abdelrahman,Mar Jacobo,Larry Lindsey,Rutika Moharir,Gunnar Lund,Yang Xu,Navid Shiee,Jeffrey Bigham,Charles Maalouf,Joseph Yitan Cheng*

Main category: cs.CV

TL;DR: 提出VLSU框架系统性评估多模态安全，揭示现有模型在联合图像-文本推理中存在显著性能下降（20-55%）和34%的组合分类错误


<details>
  <summary>Details</summary>
Motivation: 现有安全评估方法割裂处理多模态内容，且无法有效区分明确有害内容与边缘案例，导致过度阻断或漏检风险

Method: 通过17种安全模式的细粒度分类与组合分析，构建含8,187样本的基准数据集，采用多阶段流水线（真实图像+人工标注）评估11个SOTA模型

Result: 1. 联合推理准确率较单模态下降至20-55% 
2. 34%错误源自组合推理失败 
3. 指令调整使Gemini-1.5边缘案例阻断率从62.4%降至10.4%，但有害内容拒绝率从90.8%降至53.9%

Conclusion: VLSU框架暴露当前模型在跨模态安全理解中的系统性缺陷，为提升视觉-语言模型安全性提供关键评估基准与研究方向

Abstract: Safety evaluation of multimodal foundation models often treats vision and
language inputs separately, missing risks from joint interpretation where
benign content becomes harmful in combination. Existing approaches also fail to
distinguish clearly unsafe content from borderline cases, leading to
problematic over-blocking or under-refusal of genuinely harmful content. We
present Vision Language Safety Understanding (VLSU), a comprehensive framework
to systematically evaluate multimodal safety through fine-grained severity
classification and combinatorial analysis across 17 distinct safety patterns.
Using a multi-stage pipeline with real-world images and human annotation, we
construct a large-scale benchmark of 8,187 samples spanning 15 harm categories.
Our evaluation of eleven state-of-the-art models reveals systematic joint
understanding failures: while models achieve 90%-plus accuracy on clear
unimodal safety signals, performance degrades substantially to 20-55% when
joint image-text reasoning is required to determine the safety label. Most
critically, 34% of errors in joint image-text safety classification occur
despite correct classification of the individual modalities, further
demonstrating absent compositional reasoning capabilities. Additionally, we
find that models struggle to balance refusing unsafe content while still
responding to borderline cases that deserve engagement. For example, we find
that instruction framing can reduce the over-blocking rate on borderline
content from 62.4% to 10.4% in Gemini-1.5, but only at the cost of
under-refusing on unsafe content with refusal rate dropping from 90.8% to
53.9%. Overall, our framework exposes weaknesses in joint image-text
understanding and alignment gaps in current models, and provides a critical
test bed to enable the next milestones in research on robust vision-language
safety.

</details>


### [89] [The Impact of Image Resolution on Biomedical Multimodal Large Language Models](https://arxiv.org/abs/2510.18304)
*Liangyu Chen,James Burgess,Jeffrey J Nirschl,Orr Zohar,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: 探索图像分辨率对生物医学多模态大模型性能的影响，提出原生分辨率推理和混合分辨率训练方案


<details>
  <summary>Details</summary>
Motivation: 现有MLLM多针对低分辨率通用图像设计，可能导致生物医学关键信息丢失

Method: 通过原生分辨率训练/推理对比、分辨率错位测试、混合分辨率训练实验验证假设

Result: 原生分辨率提升任务表现，分辨率错位显著降低性能，混合训练平衡算力与精度需求

Conclusion: 建议优先采用原生分辨率推理并构建混合分辨率数据集优化生物医学MLLM

Abstract: Imaging technologies are fundamental to biomedical research and modern
medicine, requiring analysis of high-resolution images across various
modalities. While multimodal large language models (MLLMs) show promise for
biomedical image analysis, most are designed for low-resolution images from
general-purpose datasets, risking critical information loss. We investigate how
image resolution affects MLLM performance in biomedical applications and
demonstrate that: (1) native-resolution training and inference significantly
improve performance across multiple tasks, (2) misalignment between training
and inference resolutions severely degrades performance, and (3)
mixed-resolution training effectively mitigates misalignment and balances
computational constraints with performance requirements. Based on these
findings, we recommend prioritizing native-resolution inference and
mixed-resolution datasets to optimize biomedical MLLMs for transformative
impact in scientific research and clinical applications.

</details>


### [90] [Zero-Shot Vehicle Model Recognition via Text-Based Retrieval-Augmented Generation](https://arxiv.org/abs/2510.18502)
*Wei-Chia Chang,Yan-Ann Chen*

Main category: cs.CV

TL;DR: 提出VMMR+RAG融合框架，通过文本推理实现零样本车辆识别，无需大规模模型重训练


<details>
  <summary>Details</summary>
Motivation: 传统车辆识别方法难以适应新车型，CLIP模型存在固定权重导致识别性能受限的问题

Method: VLM提取车辆属性→文本特征库匹配→RAG增强语言模型推理的pipeline架构

Result: 识别准确率较CLIP基线提升近20%

Conclusion: 验证了RAG增强语言模型在智慧城市车辆识别中的扩展潜力，支持通过文本描述快速更新车型库

Abstract: Vehicle make and model recognition (VMMR) is an important task in intelligent
transportation systems, but existing approaches struggle to adapt to newly
released models. Contrastive Language-Image Pretraining (CLIP) provides strong
visual-text alignment, yet its fixed pretrained weights limit performance
without costly image-specific finetuning. We propose a pipeline that integrates
vision language models (VLMs) with Retrieval-Augmented Generation (RAG) to
support zero-shot recognition through text-based reasoning. A VLM converts
vehicle images into descriptive attributes, which are compared against a
database of textual features. Relevant entries are retrieved and combined with
the description to form a prompt, and a language model (LM) infers the make and
model. This design avoids large-scale retraining and enables rapid updates by
adding textual descriptions of new vehicles. Experiments show that the proposed
method improves recognition by nearly 20% over the CLIP baseline, demonstrating
the potential of RAG-enhanced LM reasoning for scalable VMMR in smart-city
applications.

</details>


### [91] [See the Text: From Tokenization to Visual Reading](https://arxiv.org/abs/2510.18840)
*Ling Xing,Alex Jinpeng Wang,Rui Yan,Hongyu Qu,Zechao Li,Jinhui Tang*

Main category: cs.CV

TL;DR: 提出视觉文本处理框架SeeTok，将文本转化为图像利用多模态模型处理，相比传统子词分词方法减少70.5%计算量，在跨语言泛化、抗噪能力和语言结构理解方面表现更优


<details>
  <summary>Details</summary>
Motivation: 解决传统子词分词方法在低资源语言中产生的过度分割问题，降低计算开销，模仿人类视觉阅读的认知方式

Method: 将文本渲染为视觉图像（visual-text），利用预训练多模态大模型的OCR和图文对齐能力进行解释

Result: 在三个语言任务中实现同等或更好性能，减少4.43倍token数量，FLOPs降低70.5%，跨语言泛化准确率提升12.3%

Conclusion: 标志着从符号分词向视觉阅读范式的转变，为构建更接近人类认知的语言模型提供了新方向

Abstract: People see text. Humans read by recognizing words as visual objects,
including their shapes, layouts, and patterns, before connecting them to
meaning, which enables us to handle typos, distorted fonts, and various scripts
effectively. Modern large language models (LLMs), however, rely on subword
tokenization, fragmenting text into pieces from a fixed vocabulary. While
effective for high-resource languages, this approach over-segments low-resource
languages, yielding long, linguistically meaningless sequences and inflating
computation. In this work, we challenge this entrenched paradigm and move
toward a vision-centric alternative. Our method, SeeTok, renders text as images
(visual-text) and leverages pretrained multimodal LLMs to interpret them,
reusing strong OCR and text-vision alignment abilities learned from large-scale
multimodal training. Across three different language tasks, SeeTok matches or
surpasses subword tokenizers while requiring 4.43 times fewer tokens and
reducing FLOPs by 70.5%, with additional gains in cross-lingual generalization,
robustness to typographic noise, and linguistic hierarchy. SeeTok signals a
shift from symbolic tokenization to human-like visual reading, and takes a step
toward more natural and cognitively inspired language models.

</details>


### [92] [Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs](https://arxiv.org/abs/2510.18876)
*Haochen Wang,Yuhao Wang,Tao Zhang,Yikang Zhou,Yanwei Li,Jiacong Wang,Ye Tian,Jiahao Meng,Zilong Huang,Guangcan Mai,Anran Wang,Yunhai Tong,Zhuochen Wang,Xiangtai Li,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: 提出GAR模型，通过全局上下文感知和特征重放技术增强区域级视觉理解，支持多提示交互和组合推理，在多个评测基准达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有区域级MLLMs孤立分析区域时忽略全局信息，导致复杂场景理解受限。需增强模型对全局上下文和多区域关系的建模能力。

Method: 采用RoI对齐的特征重放技术，在区域分析时动态融合全局信息；设计多提示交互机制，支持跨区域的组合推理。

Result: GAR-1B在DLC-Bench超越DAM-3B+4.5，GAR-Bench-VQA超过InternVL3-78B；零样本GAR-8B在VideoRefer-BenchQ超越VideoRefer-7B。

Conclusion: GAR首次实现主动对话式区域理解，突破了孤立分析的限制，其设计可无缝迁移到视频理解任务，展现了强大的泛化能力。

Abstract: While Multimodal Large Language Models (MLLMs) excel at holistic
understanding, they struggle in capturing the dense world with complex scenes,
requiring fine-grained analysis of intricate details and object
inter-relationships. Region-level MLLMs have been a promising step. However,
previous attempts are generally optimized to understand given regions in
isolation, neglecting crucial global contexts. To address this, we introduce
Grasp Any Region (GAR) for comprehen- sive region-level visual understanding.
Empowered by an effective RoI-aligned feature replay technique, GAR supports
(1) precise perception by leveraging necessary global contexts, and (2)
modeling interactions between multiple prompts. Together, it then naturally
achieves (3) advanced compositional reasoning to answer specific free-form
questions about any region, shifting the paradigm from passive description to
active dialogue. Moreover, we construct GAR-Bench, which not only provides a
more accurate evaluation of single-region comprehension, but also, more
importantly, measures interactions and complex reasoning across multiple
regions. Extensive experiments have demonstrated that GAR-1B not only maintains
the state-of-the-art captioning capabilities, e.g., outperforming DAM-3B +4.5
on DLC-Bench, but also excels at modeling relationships between multiple
prompts with advanced comprehension capabilities, even surpassing InternVL3-78B
on GAR-Bench-VQA. More importantly, our zero-shot GAR-8B even outperforms
in-domain VideoRefer-7B on VideoRefer-BenchQ, indicating its strong
capabilities can be easily transferred to videos.

</details>
