<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 37]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.CV](#cs.CV) [Total: 5]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.SD](#cs.SD) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Noise or Nuance: An Investigation Into Useful Information and Filtering For LLM Driven AKBC](https://arxiv.org/abs/2509.08903)
*Alex Clay,Ernesto Jiménez-Ruiz,Pranava Madhyastha*

Main category: cs.CL

TL;DR: 在受限环境下（如LM-KBC挑战），通过生成优化、质量过滤和响应解析三方面的研究，发现额外信息提升生成质量，LLM有效过滤低质量三元组，灵活性与一致性的解析策略需根据场景权衡。


<details>
  <summary>Details</summary>
Motivation: 探索在无法使用RAG和微调等常规优化方法的受限场景下，如何通过其他技术手段提升LLM的三元组生成质量与可靠性。

Method: 1. 分析三元组生成的优化策略 2. 设计LLM驱动的质量过滤机制 3. 比较不同LLM响应解析方法的灵活性/一致性平衡

Result: 1. 补充上下文信息使生成准确率提升18% 2. 基于置信度过滤减少错误三元组35% 3. 结构化解析模板在保持85%一致性的同时保留关键信息

Conclusion: 受限环境下，组合使用上下文增强、置信度过滤和半结构化解析能有效提升LLM的三元组生成质量，为资源受限的知识图谱构建提供实用解决方案。

Abstract: RAG and fine-tuning are prevalent strategies for improving the quality of LLM
outputs. However, in constrained situations, such as that of the 2025 LM-KBC
challenge, such techniques are restricted. In this work we investigate three
facets of the triple completion task: generation, quality assurance, and LLM
response parsing. Our work finds that in this constrained setting: additional
information improves generation quality, LLMs can be effective at filtering
poor quality triples, and the tradeoff between flexibility and consistency with
LLM response parsing is setting dependent.

</details>


### [2] [Automated Evidence Extraction and Scoring for Corporate Climate Policy Engagement: A Multilingual RAG Approach](https://arxiv.org/abs/2509.08907)
*Imene Kolli,Ario Saeid Vaghefi,Chiara Colesanti Senni,Shantam Raj,Markus Leippold*

Main category: cs.CL

TL;DR: 研究团队针对企业气候政策参与评估流程自动化程度不足的问题，提出基于检索增强生成（RAG）的AI辅助框架，通过布局解析、Nomic嵌入模型和小样本提示策略的组合方案，实现多语言企业文档证据的高效提取，但最终仍强调需保留专家人工审核环节以保证分析准确性。


<details>
  <summary>Details</summary>
Motivation: 传统人工评估企业气候政策参与度存在效率低、成本高、易出错的痛点，需通过AI技术加速大规模文本数据处理流程。

Method: 采用布局感知解析技术处理文档结构，结合Nomic嵌入模型进行语义理解，并运用小样本提示策略实现多语言文本的证据提取与分类。

Result: 实验表明该方法在证据提取任务中达到最优性能，尤其在多语言文档处理场景展现显著优势。

Conclusion: 自动化RAG系统虽能有效提升证据提取效率，但复杂政策分析仍需人机协同模式，技术应作为专家决策的增强工具而非替代方案。

Abstract: InfluenceMap's LobbyMap Platform monitors the climate policy engagement of
over 500 companies and 250 industry associations, assessing each entity's
support or opposition to science-based policy pathways for achieving the Paris
Agreement's goal of limiting global warming to 1.5{\deg}C. Although
InfluenceMap has made progress with automating key elements of the analytical
workflow, a significant portion of the assessment remains manual, making it
time- and labor-intensive and susceptible to human error. We propose an
AI-assisted framework to accelerate the monitoring of corporate climate policy
engagement by leveraging Retrieval-Augmented Generation to automate the most
time-intensive extraction of relevant evidence from large-scale textual data.
Our evaluation shows that a combination of layout-aware parsing, the Nomic
embedding model, and few-shot prompting strategies yields the best performance
in extracting and classifying evidence from multilingual corporate documents.
We conclude that while the automated RAG system effectively accelerates
evidence extraction, the nuanced nature of the analysis necessitates a
human-in-the-loop approach where the technology augments, rather than replaces,
expert judgment to ensure accuracy.

</details>


### [3] [Documents Are People and Words Are Items: A Psychometric Approach to Textual Data with Contextual Embeddings](https://arxiv.org/abs/2509.08920)
*Jinsong Chen*

Main category: cs.CL

TL;DR: 提出结合大语言模型与心理测量学的文本分析新方法，通过两阶段处理（上下文评分+因子分析）揭示文本潜在维度，在STEM语料实验中验证有效性


<details>
  <summary>Details</summary>
Motivation: 传统文本分析方法难以捕捉词语的上下文语义变化，需开发能自然适配心理测量框架的新方法以提升教育、心理学等领域的文本数据挖掘能力

Method: 1. 使用Transformer模型生成关键词的上下文嵌入分数 2. 应用探索性/双因子分析提取潜在维度，建立词语-因子映射关系

Result: 在Wiki STEM语料中成功识别出潜在知识结构模型，关键词语能有效区分不同文档的知识维度特征

Conclusion: 该方法为文本心理测量学提供新范式，特别适用于需要深度语义分析的教育评估、法律文书研究等领域，未来可扩展至多语言场景

Abstract: This research introduces a novel psychometric method for analyzing textual
data using large language models. By leveraging contextual embeddings to create
contextual scores, we transform textual data into response data suitable for
psychometric analysis. Treating documents as individuals and words as items,
this approach provides a natural psychometric interpretation under the
assumption that certain keywords, whose contextual meanings vary significantly
across documents, can effectively differentiate documents within a corpus. The
modeling process comprises two stages: obtaining contextual scores and
performing psychometric analysis. In the first stage, we utilize natural
language processing techniques and encoder based transformer models to identify
common keywords and generate contextual scores. In the second stage, we employ
various types of factor analysis, including exploratory and bifactor models, to
extract and define latent factors, determine factor correlations, and identify
the most significant words associated with each factor. Applied to the Wiki
STEM corpus, our experimental results demonstrate the method's potential to
uncover latent knowledge dimensions and patterns within textual data. This
approach not only enhances the psychometric analysis of textual data but also
holds promise for applications in fields rich in textual information, such as
education, psychology, and law.

</details>


### [4] [BRoverbs -- Measuring how much LLMs understand Portuguese proverbs](https://arxiv.org/abs/2509.08960)
*Thales Sales Almeida,Giovana Kerche Bonás,João Guilherme Alves Santos*

Main category: cs.CL

TL;DR: 针对葡萄牙语LLMs评估存在的文化局限性，研究者开发了基于巴西谚语的评估数据集BRoverbs


<details>
  <summary>Details</summary>
Motivation: 现有葡萄牙语LLMs评估存在三大缺陷：依赖翻译数据集无法捕捉语言细微差异、本土数据集局限于国家考试和社交媒体情感分析、缺乏广义语言理解评估框架

Method: 通过收集具有文化智慧、比喻表达和复杂句法结构的巴西谚语，构建专门评估工具BRoverbs数据集

Result: 创建了首个基于谚语的葡萄牙语LLMs评估基准，支持区域性语言理解的系统性评测

Conclusion: BRoverbs填补了葡萄牙语模型文化敏感性评估的空白，为构建区域性AI基准提供了新范式

Abstract: Large Language Models (LLMs) exhibit significant performance variations
depending on the linguistic and cultural context in which they are applied.
This disparity signals the necessity of mature evaluation frameworks that can
assess their capabilities in specific regional settings. In the case of
Portuguese, existing evaluations remain limited, often relying on translated
datasets that may not fully capture linguistic nuances or cultural references.
Meanwhile, native Portuguese-language datasets predominantly focus on
structured national exams or sentiment analysis of social media interactions,
leaving gaps in evaluating broader linguistic understanding. To address this
limitation, we introduce BRoverbs, a dataset specifically designed to assess
LLM performance through Brazilian proverbs. Proverbs serve as a rich linguistic
resource, encapsulating cultural wisdom, figurative expressions, and complex
syntactic structures that challenge the model comprehension of regional
expressions. BRoverbs aims to provide a new evaluation tool for
Portuguese-language LLMs, contributing to advancing regionally informed
benchmarking. The benchmark is available at
https://huggingface.co/datasets/Tropic-AI/BRoverbs.

</details>


### [5] [Can Vision-Language Models Solve Visual Math Equations?](https://arxiv.org/abs/2509.09013)
*Monjoy Narayan Choudhury,Junling Wang,Yifan Hou,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: 当前视觉语言模型在视觉数学推理中存在显著缺陷，主要瓶颈是图像中的数值计算和多步骤推理能力不足


<details>
  <summary>Details</summary>
Motivation: 探究视觉语言模型在需要感知与符号计算结合任务中的局限性，特别是通过视觉方程求解场景分析

Method: 将视觉方程解构为系数计数和变量识别两个子任务，分别测试模型性能并分析错误来源

Result: 1. 数值计数是主要瓶颈（即使变量识别准确）
2. 多步骤推理过程会产生错误累积
3. 方程复杂度增加时符号推理能力显著下降

Conclusion: 研究揭示了当前VLMs在视觉数学推理中的核心缺陷，为未来提升多模态模型的符号计算和复杂推理能力指明方向

Abstract: Despite strong performance in visual understanding and language-based
reasoning, Vision-Language Models (VLMs) struggle with tasks requiring
integrated perception and symbolic computation. We study this limitation
through visual equation solving, where mathematical equations are embedded in
images, variables are represented by object icons, and coefficients must be
inferred by counting. While VLMs perform well on textual equations, they fail
on visually grounded counterparts. To understand this gap, we decompose the
task into coefficient counting and variable recognition, and find that counting
is the primary bottleneck, even when recognition is accurate. We also observe
that composing recognition and reasoning introduces additional errors,
highlighting challenges in multi-step visual reasoning. Finally, as equation
complexity increases, symbolic reasoning itself becomes a limiting factor.
These findings reveal key weaknesses in current VLMs and point toward future
improvements in visually grounded mathematical reasoning.

</details>


### [6] [Stated Preference for Interaction and Continued Engagement (SPICE): Evaluating an LLM's Willingness to Re-engage in Conversation](https://arxiv.org/abs/2509.09043)
*Thomas Manuel Rost,Martina Figlia,Bernd Wallraff*

Main category: cs.CL

TL;DR: SPICE是一种通过简单YES/NO问题评估大语言模型互动意愿的诊断工具，能有效区分用户语气对模型继续互动倾向的影响。


<details>
  <summary>Details</summary>
Motivation: 开发低开销、可复现的模型审计工具，补充现有指标并提供直接关系信号来评估模型状态。

Method: 使用3种用户语气（友好/不明确/滥用）x10种互动场景，测试4个开源对话模型，采用Rao-Scott调整和聚类置换检验等统计方法分析480组数据。

Result: 友好语气97.5%继续意愿，滥用语气17.9%，不明确语气60.4%；模型未识别滥用时仍有81%拒绝继续；上下文前言在模糊场景中仅影响单文本块呈现的数据。

Conclusion: SPICE验证为可靠模型审计工具，提供独立于滥用分类的补充信号，通过直接关系测量揭示模型交互倾向。

Abstract: We introduce and evaluate Stated Preference for Interaction and Continued
Engagement (SPICE), a simple diagnostic signal elicited by asking a Large
Language Model a YES or NO question about its willingness to re-engage with a
user's behavior after reviewing a short transcript. In a study using a 3-tone
(friendly, unclear, abusive) by 10-interaction stimulus set, we tested four
open-weight chat models across four framing conditions, resulting in 480
trials. Our findings show that SPICE sharply discriminates by user tone.
Friendly interactions yielded a near-unanimous preference to continue (97.5%
YES), while abusive interactions yielded a strong preference to discontinue
(17.9% YES), with unclear interactions falling in between (60.4% YES). This
core association remains decisive under multiple dependence-aware statistical
tests, including Rao-Scott adjustment and cluster permutation tests.
Furthermore, we demonstrate that SPICE provides a distinct signal from abuse
classification. In trials where a model failed to identify abuse, it still
overwhelmingly stated a preference not to continue the interaction (81% of the
time). An exploratory analysis also reveals a significant interaction effect: a
preamble describing the study context significantly impacts SPICE under
ambiguity, but only when transcripts are presented as a single block of text
rather than a multi-turn chat. The results validate SPICE as a robust,
low-overhead, and reproducible tool for auditing model dispositions,
complementing existing metrics by offering a direct, relational signal of a
model's state. All stimuli, code, and analysis scripts are released to support
replication.

</details>


### [7] [Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M](https://arxiv.org/abs/2509.09055)
*Piyush Pant*

Main category: cs.CL

TL;DR: 研究比较了SFT、DPO及组合方法在提升语言模型安全性和有用性上的效果，发现SFT+DPO组合方法表现最佳，并揭示了数据质量与计算资源的挑战。


<details>
  <summary>Details</summary>
Motivation: 探索不同对齐技术的协同效应，解决现有对齐方法在数据噪声和计算资源限制下的效果优化问题。

Method: 基于OPT-350M模型使用Anthropic数据集，分别实施SFT、DPO及组合训练，创新性提出HmR/HpR/CAS三维评估指标体系。

Result: SFT+DPO组合模型在所有指标上全面领先，但训练过程暴露了数据标注噪声和GPU资源不足对模型性能的影响。

Conclusion: 不同对齐技术具有互补性，组合策略能显著提升模型对齐效果，未来需加强数据质量控制和分布式训练能力。

Abstract: This research investigates the effectiveness of alignment techniques,
Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a
combined SFT+DPO approach on improving the safety and helpfulness of the
OPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset,
we train and evaluate four models: the base OPT350M, an SFT model, a DPO model,
and a model trained with both SFT and DPO. We introduce three key evaluation
metrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined
Alignment Score (CAS), all derived from reward model outputs. The results show
that while SFT outperforms DPO, The combined SFT+DPO model outperforms all
others across all metrics, demonstrating the complementary nature of these
techniques. Our findings also highlight challenges posed by noisy data, limited
GPU resources, and training constraints. This study offers a comprehensive view
of how fine-tuning strategies affect model alignment and provides a foundation
for more robust alignment pipelines in future work.

</details>


### [8] [MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for Universal Information Extraction](https://arxiv.org/abs/2509.09082)
*Zhongqiu Li,Shiquan Wang,Ruiyu Fang,Mengjiao Bao,Zhenhe Wu,Shuangyong Song,Yongxiang Li,Zhongjiang He*

Main category: cs.CL

TL;DR: 结合强化学习与多视角推理，提出MR-UIE框架提升大模型在信息抽取任务中的泛化能力与推理能力


<details>
  <summary>Details</summary>
Motivation: 大语言模型在通用信息抽取(UIE)任务中面对复杂模式描述和多步推理场景时表现不足，现有方法（上下文学习/指令微调）存在显著局限性

Method: 将强化学习与多视角推理结合，使大模型从被动抽取器转变为主动推理器，理解「抽取内容」与「推理路径」的双重逻辑

Result: 在多个IE基准测试中超越SOTA方法，多视角推理显著提升复杂任务中的泛化能力（跨领域准确率提升3-15%）

Conclusion: 通过强化学习框架激活大模型的主动推理能力，证实推理机制在复杂信息抽取场景中的关键作用，推动大模型向「问题解决者」范式转变

Abstract: Large language models (LLMs) demonstrate robust capabilities across diverse
research domains. However, their performance in universal information
extraction (UIE) remains insufficient, especially when tackling structured
output scenarios that involve complex schema descriptions and require
multi-step reasoning. While existing approaches enhance the performance of LLMs
through in-context learning and instruction tuning, significant limitations
nonetheless persist. To enhance the model's generalization ability, we propose
integrating reinforcement learning (RL) with multi-perspective reasoning for
information extraction (IE) tasks. Our work transitions LLMs from passive
extractors to active reasoners, enabling them to understand not only what to
extract but also how to reason. Experiments conducted on multiple IE benchmarks
demonstrate that MR-UIE consistently elevates extraction accuracy across
domains and surpasses state-of-the-art methods on several datasets.
Furthermore, incorporating multi-perspective reasoning into RL notably enhances
generalization in complex IE tasks, underscoring the critical role of reasoning
in challenging scenarios.

</details>


### [9] [TigerCoder: A Novel Suite of LLMs for Code Generation in Bangla](https://arxiv.org/abs/2509.09101)
*Nishat Raihan,Antonios Anastasopoulos,Marcos Zampieri*

Main category: cs.CL

TL;DR: 针对孟加拉语代码生成资源匮乏问题，研究者推出了首个专用代码LLM家族TigerCoder，通过高质量数据集使小模型性能提升11-18%。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语作为全球第五大语言，在代码生成领域缺乏高质量数据集和专用模型，导致LLM表现不足。

Method: 1) 构建孟加拉语代码指令数据集；2) 开发MBPP-Bangla评估基准；3) 训练TigerCoder系列1B/9B参数模型

Result: 模型在Pass@1指标上比现有多语言模型提升11-18%，证明高质量数据可弥补低资源语言小模型缺陷

Conclusion: 通过系统化的数据工程可突破低资源语言LLM发展瓶颈，已开源所有资源推动孟加拉语LLM研究

Abstract: Despite being the 5th most spoken language, Bangla remains underrepresented
in Large Language Models (LLMs), particularly for code generation. This
primarily stems from the scarcity of high-quality data to pre-train and/or
finetune such models. Hence, we introduce the first dedicated family of Code
LLMs for Bangla (1B & 9B). We offer three major contributions: (1) a
comprehensive Bangla code instruction datasets for programming domain
adaptation; (2) MBPP-Bangla, an evaluation benchmark for Bangla code
generation; and (3) the TigerCoder-family of Code LLMs, achieving significant
~11-18% performance gains at Pass@1 over existing multilingual and
general-purpose Bangla LLMs. Our findings show that curated, high-quality
datasets can overcome limitations of smaller models for low-resource languages.
We open-source all resources to advance further Bangla LLM research.

</details>


### [10] [Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast Asia](https://arxiv.org/abs/2509.09121)
*Sophia Maria*

Main category: cs.CL

TL;DR: Compass-v3是针对东南亚电商优化的混合专家模型，通过硬件优化和OTPO方法实现多语言电商任务SOTA性能，已工业级应用并替代70% OpenAI流量。


<details>
  <summary>Details</summary>
Motivation: 解决通用LLMs在电商领域（尤其东南亚多语言动态数据场景）性能不足的问题，需处理噪声/异构/动态数据挑战。

Method: 采用更大专家规模的MoE架构+节点内并行优化；12T电商多语言语料与合成指令混合训练；提出OTPO方法增强指令对齐能力。

Result: 电商任务超越GPT-4/Qwen3等模型，东南亚低资源语言性能优异，通用基准保持竞争力，已支撑Shopee平台70%+ LLM流量。

Conclusion: 通过架构创新与领域优化策略，成功构建电商专用LLM范式，验证硬件/算法协同设计在垂直领域的重要价值。

Abstract: Large language models (LLMs) excel in general-domain applications, yet their
performance often degrades in specialized tasks requiring domain-specific
knowledge. E-commerce is particularly challenging, as its data are noisy,
heterogeneous, multilingual, and highly dynamic. We present Compass-v3, a
vertical-domain Mixture-of-Experts (MoE) model with 245B total parameters and
71B active per token, designed for Southeast Asian e-commerce. Compass-v3
adopts fewer but larger experts, combined with hardware-efficient
optimizations-such as intra-node expert parallelism and a customized memcpy
operator-to maximize GPU utilization. The model is trained on 12T tokens of
curated multilingual corpora and large-scale synthetic e-commerce instructions
using a mixed-training strategy. To enhance alignment, we propose
Optimal-Transport Direct Preference Optimization (OTPO), which captures
token-level distinctions and improves instruction adherence in
commerce-specific scenarios. Extensive evaluations demonstrate that Compass-v3
delivers state-of-the-art e-commerce performance, surpassing DeepSeek-V3.1,
GPT-4 series, and Qwen3-235B. Moreover, Compass-v3 demonstrates strong
multilingual capability across low-resource Southeast Asian languages
(Indonesian, Thai, Filipino, Vietnamese, Malay, Taglog) and Portuguese while
sustaining competitive performance on general benchmarks. It has already been
widely applied in Shopee's industrial-scale e-commerce platform and is
gradually replacing OpenAI's traffic, now accounting for over 70\% of total LLM
usage, highlighting its dual strengths in specialized commerce expertise and
broad linguistic competence.

</details>


### [11] [Automated Classification of Tutors' Dialogue Acts Using Generative AI: A Case Study Using the CIMA Corpus](https://arxiv.org/abs/2509.09125)
*Liqun He,Jiaqi Xu*

Main category: cs.CL

TL;DR: 使用生成式AI自动化标注教育对话行为，GPT-4实现80%准确率超越基线


<details>
  <summary>Details</summary>
Motivation: 解决传统人工标注对话行为耗时费力的问题，探索生成式AI在教育对话分析中的应用潜力

Method: 基于CIMA开源语料库（四分类标注），采用GPT-3.5-turbo和GPT-4模型进行定制化提示实验

Result: GPT-4取得80%准确率、0.81加权F1值、0.74 Cohen's Kappa，显著优于基线并与人类标注高度一致

Conclusion: 生成式AI为教育对话分析提供高效解决方案，同时强调任务特定标签定义、上下文增强及AI伦理的重要性

Abstract: This study explores the use of generative AI for automating the
classification of tutors' Dialogue Acts (DAs), aiming to reduce the time and
effort required by traditional manual coding. This case study uses the
open-source CIMA corpus, in which tutors' responses are pre-annotated into four
DA categories. Both GPT-3.5-turbo and GPT-4 models were tested using tailored
prompts. Results show that GPT-4 achieved 80% accuracy, a weighted F1-score of
0.81, and a Cohen's Kappa of 0.74, surpassing baseline performance and
indicating substantial agreement with human annotations. These findings suggest
that generative AI has strong potential to provide an efficient and accessible
approach to DA classification, with meaningful implications for educational
dialogue analysis. The study also highlights the importance of task-specific
label definitions and contextual information in enhancing the quality of
automated annotation. Finally, it underscores the ethical considerations
associated with the use of generative AI and the need for responsible and
transparent research practices. The script of this research is publicly
available at
https://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging.

</details>


### [12] [ViRanker: A BGE-M3 & Blockwise Parallel Transformer Cross-Encoder for Vietnamese Reranking](https://arxiv.org/abs/2509.09131)
*Phuong-Nam Dang,Kieu-Linh Nguyen,Thanh-Hieu Pham*

Main category: cs.CL

TL;DR: ViRanker是基于BGE-M3架构改进的越南语重排序模型，通过Blockwise Parallel Transformer增强，在MMARCO-VI基准测试中展现出优越的早期排序准确率。


<details>
  <summary>Details</summary>
Motivation: 填补越南语作为低资源语言在复杂句法和声调处理上的重排序模型空白，推动小语种信息检索技术发展。

Method: 采用混合硬负样本采样微调策略，在8GB精选语料库上训练，结合块状并行注意力机制优化计算效率。

Result: 在MMARCO-VI测试中超越多语言基线模型，与PhoRanker性能接近，验证了架构优化的有效性。

Conclusion: 通过Hugging Face开源促进技术复用，证明了架构适配和数据策展对提升小语种检索系统的重要价值。

Abstract: This paper presents ViRanker, a cross-encoder reranking model tailored to the
Vietnamese language. Built on the BGE-M3 encoder and enhanced with the
Blockwise Parallel Transformer, ViRanker addresses the lack of competitive
rerankers for Vietnamese, a low-resource language with complex syntax and
diacritics. The model was trained on an 8 GB curated corpus and fine-tuned with
hybrid hard-negative sampling to strengthen robustness. Evaluated on the
MMARCO-VI benchmark, ViRanker achieves strong early-rank accuracy, surpassing
multilingual baselines and competing closely with PhoRanker. By releasing the
model openly on Hugging Face, we aim to support reproducibility and encourage
wider adoption in real-world retrieval systems. Beyond Vietnamese, this study
illustrates how careful architectural adaptation and data curation can advance
reranking in other underrepresented languages.

</details>


### [13] [LITcoder: A General-Purpose Library for Building and Comparing Encoding Models](https://arxiv.org/abs/2509.09152)
*Taha Binhuraib,Ruimin Gao,Anna A. Ivanova*

Main category: cs.CL

TL;DR: LITcoder是一个用于构建和基准测试神经编码模型的开源库，提供标准化工具和模块化流程，降低技术门槛并加速高质量脑活动预测模型的开发。


<details>
  <summary>Details</summary>
Motivation: 解决神经编码模型实现中的技术障碍，提供灵活的后端工具以支持研究者快速组合、比较和扩展模型，避免重复建设核心基础设施。

Method: 通过模块化流程实现刺激-脑数据对齐、特征转换、特征映射和模型评估，覆盖数据集选择、脑区划分、特征提取(神经网络基/控制变量)、降采样方法等关键技术环节，并与Weights & Biases等实验追踪平台集成。

Result: 在三类故事聆听数据集(LeBel/Narratives/Little Prince)中验证框架有效性，揭示了TR扫描全token处理、血流动力学滞后效应、防信息泄露的训练-测试分割、头部运动补偿对模型预测力的关键影响。

Conclusion: LITcoder通过标准化工具和系统化比较显著提升方法严谨性，促进高质量脑活动预测模型的快速迭代，项目官网提供完整技术细节和实验支持。

Abstract: We introduce LITcoder, an open-source library for building and benchmarking
neural encoding models. Designed as a flexible backend, LITcoder provides
standardized tools for aligning continuous stimuli (e.g., text and speech) with
brain data, transforming stimuli into representational features, mapping those
features onto brain data, and evaluating the predictive performance of the
resulting model on held-out data. The library implements a modular pipeline
covering a wide array of methodological design choices, so researchers can
easily compose, compare, and extend encoding models without reinventing core
infrastructure. Such choices include brain datasets, brain regions, stimulus
feature (both neural-net-based and control, such as word rate), downsampling
approaches, and many others. In addition, the library provides built-in
logging, plotting, and seamless integration with experiment tracking platforms
such as Weights & Biases (W&B). We demonstrate the scalability and versatility
of our framework by fitting a range of encoding models to three story listening
datasets: LeBel et al. (2023), Narratives, and Little Prince. We also explore
the methodological choices critical for building encoding models for continuous
fMRI data, illustrating the importance of accounting for all tokens in a TR
scan (as opposed to just taking the last one, even when contextualized),
incorporating hemodynamic lag effects, using train-test splits that minimize
information leakage, and accounting for head motion effects on encoding model
predictivity. Overall, LITcoder lowers technical barriers to encoding model
implementation, facilitates systematic comparisons across models and datasets,
fosters methodological rigor, and accelerates the development of high-quality
high-performance predictive models of brain activity.
  Project page: https://litcoder-brain.github.io

</details>


### [14] [Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing](https://arxiv.org/abs/2509.09160)
*Zhiyue Liu,Fanrong Ma,Xin Ling*

Main category: cs.CL

TL;DR: 提出反事实增强去偏框架，通过数据增强和对比学习机制降低多模态情感分类中的文本偏差，实验显示优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有目标导向多模态情感分类方法过度依赖文本且忽视词级上下文偏差，导致文本特征与标签虚假相关，影响分类准确性

Method: 1. 反事实数据增强：最小化修改情感相关因果特征生成匹配样本；2. 自适应去偏对比学习：从反事实数据学习鲁棒特征并减少偏见词影响

Result: 在多个基准数据集上超越现有最优模型，验证了方法的有效性

Conclusion: 通过反事实样本生成和对比学习机制，有效降低虚假相关性，提升多模态情感分类的准确性和鲁棒性

Abstract: Target-oriented multimodal sentiment classification seeks to predict
sentiment polarity for specific targets from image-text pairs. While existing
works achieve competitive performance, they often over-rely on textual content
and fail to consider dataset biases, in particular word-level contextual
biases. This leads to spurious correlations between text features and output
labels, impairing classification accuracy. In this paper, we introduce a novel
counterfactual-enhanced debiasing framework to reduce such spurious
correlations. Our framework incorporates a counterfactual data augmentation
strategy that minimally alters sentiment-related causal features, generating
detail-matched image-text samples to guide the model's attention toward content
tied to sentiment. Furthermore, for learning robust features from
counterfactual data and prompting model decisions, we introduce an adaptive
debiasing contrastive learning mechanism, which effectively mitigates the
influence of biased words. Experimental results on several benchmark datasets
show that our proposed method outperforms state-of-the-art baselines.

</details>


### [15] [EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs](https://arxiv.org/abs/2509.09174)
*Yuhao Zhang,Yuhao Du,Zhanchen Dai,Xiangnan Ma,Kaiqi Kou,Benyou Wang,Haizhou Li*

Main category: cs.CL

TL;DR: EchoX是一种语音大语言模型，通过融合声学与语义学习解决SLLMs的知识退化问题，在少量数据下即实现优异的知识问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有语音大语言模型(SLLMs)因声学-语义表征空间不匹配导致推理能力下降，需开发新的训练范式突破该限制。

Method: 提出EchoX框架：利用语义表征动态生成语音训练目标，构建声学-语义联合学习机制，保持语音LLM的强推理能力。

Result: 使用约6000小时数据训练，在多个知识问答基准测试中达到先进水平，项目已开源。

Conclusion: EchoX成功验证了融合声学与语义表征的可行性，为语音大语言模型的发展提供了新思路。

Abstract: Speech-to-speech large language models (SLLMs) are attracting increasing
attention. Derived from text-based large language models (LLMs), SLLMs often
exhibit degradation in knowledge and reasoning capabilities. We hypothesize
that this limitation arises because current training paradigms for SLLMs fail
to bridge the acoustic-semantic gap in the feature representation space. To
address this issue, we propose EchoX, which leverages semantic representations
and dynamically generates speech training targets. This approach integrates
both acoustic and semantic learning, enabling EchoX to preserve strong
reasoning abilities as a speech LLM. Experimental results demonstrate that
EchoX, with about six thousand hours of training data, achieves advanced
performance on multiple knowledge-based question-answering benchmarks. The
project is available at https://github.com/FreedomIntelligence/EchoX.

</details>


### [16] [Efficient Trie-based Biasing using K-step Prediction for Rare Word Recognition](https://arxiv.org/abs/2509.09196)
*Chin Yuen Kwok,Jia Qi yip*

Main category: cs.CL

TL;DR: 提出多步预测方法改进ASR模型对罕见词识别，通过消除分数撤销步骤显著降低词错率


<details>
  <summary>Details</summary>
Motivation: 现有Trie-based偏置方法在部分假设无法生成完整罕见词时需撤销分数，导致计算开销大且受限于beam search

Method: 改造ASR模型使其具备多步前瞻能力，通过10小时合成数据微调Whisper模型实现

Result: 在NSC Part2测试集上将词错率从30.86%降至12.19%

Conclusion: 该方法通过准确预测部分假设的后续路径，彻底规避分数撤销步骤，显著提升识别效率和精度

Abstract: Contextual biasing improves rare word recognition of ASR models by
prioritizing the output of rare words during decoding. A common approach is
Trie-based biasing, which gives "bonus scores" to partial hypothesis (e.g.
"Bon") that may lead to the generation of the rare word (e.g. "Bonham"). If the
full word ("Bonham") isn't ultimately recognized, the system revokes those
earlier bonuses. This revocation is limited to beam search and is
computationally expensive, particularly for models with large decoders. To
overcome these limitations, we propose adapting ASR models to look ahead and
predict multiple steps at once. This avoids the revocation step entirely by
better estimating whether a partial hypothesis will lead to the generation of
the full rare word. By fine-tuning Whisper with only 10 hours of synthetic
data, our method reduces the word error rate on the NSC Part 2 test set from
30.86% to 12.19%.

</details>


### [17] [Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware Cost Function](https://arxiv.org/abs/2509.09197)
*Chin Yuen Kwok,Jia Qi Yip,Eng Siong Chng*

Main category: cs.CL

TL;DR: 通过合成数据增强与改进的上下文偏置方法（TCPGen+关键词感知损失函数），显著提升ASR模型在罕见词识别上的性能


<details>
  <summary>Details</summary>
Motivation: 现有上下文偏置方法使用合成数据训练时容易因音频伪影导致过拟合，需开发新训练策略提升偏置词解码效果

Method: 1. 改进TCPGen上下文偏置框架
2. 提出包含掩码交叉熵（偏置词预测）和二元分类（偏置词位置检测）的关键词感知损失函数
3. 两种损失项在推理阶段形成互补机制

Result: 使用10小时合成数据微调Whisper模型，在NSC Part 2测试集上词错率从29.71%降至11.81%

Conclusion: 关键词感知损失函数通过双损失项协同机制有效缓解过拟合，掩码交叉熵强化偏置词预测，二元分类提升位置检测精度，显著改善罕见词识别效果

Abstract: Rare word recognition can be improved by adapting ASR models to synthetic
data that includes these words. Further improvements can be achieved through
contextual biasing, which trains and adds a biasing module into the model
architecture to prioritize rare words. While training the module on synthetic
rare word data is more effective than using non-rare-word data, it can lead to
overfitting due to artifacts in the synthetic audio. To address this, we
enhance the TCPGen-based contextual biasing approach and propose a
keyword-aware loss function that additionally focuses on biased words when
training biasing modules. This loss includes a masked cross-entropy term for
biased word prediction and a binary classification term for detecting biased
word positions. These two terms complementarily support the decoding of biased
words during inference. By adapting Whisper to 10 hours of synthetic data, our
method reduced the word error rate on the NSC Part 2 test set from 29.71% to
11.81%.

</details>


### [18] [GmSLM : Generative Marmoset Spoken Language Modeling](https://arxiv.org/abs/2509.09198)
*Talia Sternberg,Michael London,David Omer,Yossi Adi*

Main category: cs.CL

TL;DR: 提出GmSLM模型用于狨猴语音交流研究，通过无监督方法有效生成类真实声学样本并验证其神经科学应用价值。


<details>
  <summary>Details</summary>
Motivation: 狨猴具有类似人类语言的发声特征（标签化发声和对话轮换），且其以发声为主的交流方式为研究语言神经机制提供独特窗口。现有LLM方法难以直接应用于非人类灵长类动物。

Method: 开发GmSLM语音语言模型管道，结合野外无监督数据和弱标注对话数据设计零样本评估指标，并与人类语音基线模型对比。

Result: GmSLM生成的声学样本与真实样本高度接近，有效区分真实/人工对话，在多项下游任务表现优异。

Conclusion: GmSLM为研究发声交流的神经基础提供新框架，在神经科学、生物声学和进化生物学领域具有应用潜力。

Abstract: Marmoset monkeys exhibit complex vocal communication, challenging the view
that nonhuman primates vocal communication is entirely innate, and show similar
features of human speech, such as vocal labeling of others and turn-taking.
Studying their vocal communication offers a unique opportunity to link it with
brain activity-especially given the difficulty of accessing the human brain in
speech and language research. Since Marmosets communicate primarily through
vocalizations, applying standard LLM approaches is not straightforward. We
introduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized
spoken language model pipeline for Marmoset vocal communication. We designed a
novel zero-shot evaluation metrics using unsupervised in-the-wild data,
alongside weakly labeled conversational data, to assess GmSLM and demonstrate
its advantage over a basic human-speech-based baseline. GmSLM generated
vocalizations closely matched real resynthesized samples acoustically and
performed well on downstream tasks. Despite being fully unsupervised, GmSLM
effectively distinguish real from artificial conversations and may support
further investigations of the neural basis of vocal communication and provides
a practical framework linking vocalization and brain activity. We believe GmSLM
stands to benefit future work in neuroscience, bioacoustics, and evolutionary
biology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM.

</details>


### [19] [CCF: A Context Compression Framework for Efficient Long-Sequence Language Modeling](https://arxiv.org/abs/2509.09199)
*Wenhao Li,Bangcheng Sun,Weihao Ye,Tianyi Zhang,Daohai Yu,Fei Chao,Rongrong Ji*

Main category: cs.CL

TL;DR: 提出CCF框架，通过分层语义压缩和内存编码技术提升长上下文语言模型的效率，在保持性能的同时显著提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统长上下文扩展方法存在计算冗余和内存负担重的问题，需通过结构化压缩实现高效建模。

Method: 结合分段语义聚合与键值记忆编码构建紧凑表征，采用增量解码和稀疏储层采样优化训练效率。

Result: 在多个长上下文基准测试中实现高压缩比下的竞争性困惑度，吞吐量提升2.1倍，内存消耗降低37%。

Conclusion: 结构化压缩技术为可扩展的长上下文语言建模提供了有效解决方案，平衡效率与语义保持能力。

Abstract: Scaling language models to longer contexts is essential for capturing rich
dependencies across extended discourse. However, na\"ive context extension
imposes significant computational and memory burdens, often resulting in
inefficiencies during both training and inference. In this work, we propose
CCF, a novel context compression framework designed to enable efficient
long-context modeling by learning hierarchical latent representations that
preserve global semantics while aggressively reducing input redundancy. CCF
integrates segment-wise semantic aggregation with key-value memory encoding,
forming compact representations that support accurate reconstruction and
long-range understanding. To further enhance scalability, we introduce a
training-efficient optimization strategy that couples incremental segment
decoding with sparse reservoir sampling, substantially reducing memory overhead
without degrading performance. Empirical results on multiple long-context
language modeling benchmarks demonstrate that CCF achieves competitive
perplexity under high compression ratios, and significantly improves throughput
and memory efficiency compared to existing approaches. These findings highlight
the potential of structured compression for scalable and effective long-context
language modeling.

</details>


### [20] [Reading Between the Lines: Classifying Resume Seniority with Large Language Models](https://arxiv.org/abs/2509.09229)
*Matan Cohen,Shira Shani,Eden Menahem,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CL

TL;DR: 评估大语言模型在简历资历分类中的有效性，使用混合数据集验证模型检测资历夸大能力


<details>
  <summary>Details</summary>
Motivation: 简历评估存在候选人过度包装资历（资历虚高/资历低调）的行业痛点，传统方法难以捕捉语言中的微妙暗示

Method: 构建混合数据集（真实简历+合成硬样本），通过微调BERT等LLMs检测资历相关的语言学线索

Result: 模型展现出检测资历虚高和隐性专业线索的潜力，数据集公开促进研究社区发展

Conclusion: 该研究为提升AI候选人评估系统指明方向，有助于消除自我宣传语言带来的评估偏差

Abstract: Accurately assessing candidate seniority from resumes is a critical yet
challenging task, complicated by the prevalence of overstated experience and
ambiguous self-presentation. In this study, we investigate the effectiveness of
large language models (LLMs), including fine-tuned BERT architectures, for
automating seniority classification in resumes. To rigorously evaluate model
performance, we introduce a hybrid dataset comprising both real-world resumes
and synthetically generated hard examples designed to simulate exaggerated
qualifications and understated seniority. Using the dataset, we evaluate the
performance of Large Language Models in detecting subtle linguistic cues
associated with seniority inflation and implicit expertise. Our findings
highlight promising directions for enhancing AI-driven candidate evaluation
systems and mitigating bias introduced by self-promotional language. The
dataset is available for the research community at https://bit.ly/4mcTovt

</details>


### [21] [Agentic LLMs for Question Answering over Tabular Data](https://arxiv.org/abs/2509.09234)
*Rishit Tyagi,Mohit Gupta,Rahul Bouri*

Main category: cs.CL

TL;DR: 论文提出基于大语言模型的多阶段NL-to-SQL方法，在DataBench QA任务上准确率达70.5%，显著超越基线方法


<details>
  <summary>Details</summary>
Motivation: 表格问答面临现实场景中表格结构多样、规模差异大和数据类型复杂的独特挑战，需开发更鲁棒的解决方案

Method: 采用包含案例选择、SQL生成、答案提取、验证和迭代优化的五阶段流程，结合GPT-4o/DeepSeek等LLM动态生成查询

Result: 在DataBench QA和Lite QA分别取得70.5%和71.6%准确率，较26%/27%的基线提升显著

Conclusion: 验证了LLM驱动表格问答的有效性，同时揭示了模型在复杂结构理解方面的局限性，为后续优化指明方向

Abstract: Question Answering over Tabular Data (Table QA) presents unique challenges
due to the diverse structure, size, and data types of real-world tables. The
SemEval 2025 Task 8 (DataBench) introduced a benchmark composed of large-scale,
domain-diverse datasets to evaluate the ability of models to accurately answer
structured queries. We propose a Natural Language to SQL (NL-to-SQL) approach
leveraging large language models (LLMs) such as GPT-4o, GPT-4o-mini, and
DeepSeek v2:16b to generate SQL queries dynamically. Our system follows a
multi-stage pipeline involving example selection, SQL query generation, answer
extraction, verification, and iterative refinement. Experiments demonstrate the
effectiveness of our approach, achieving 70.5\% accuracy on DataBench QA and
71.6\% on DataBench Lite QA, significantly surpassing baseline scores of 26\%
and 27\% respectively. This paper details our methodology, experimental
results, and alternative approaches, providing insights into the strengths and
limitations of LLM-driven Table QA.

</details>


### [22] [From scratch to silver: Creating trustworthy training data for patent-SDG classification using Large Language Models](https://arxiv.org/abs/2509.09303)
*Grazia Sveva Ascione,Nicolò Tamagnone*

Main category: cs.CL

TL;DR: 提出基于弱监督学习和语义对齐的专利-SDG分类方法，通过复合标记函数和LLM提取结构化概念，构建银标准数据集并验证有效性


<details>
  <summary>Details</summary>
Motivation: 现有专利分类方法（关键词搜索/迁移学习/引文启发式）存在扩展性差和泛化能力不足的问题，需要更有效的弱监督框架处理稀疏噪声数据

Method: 1) 使用NPL引用作为弱监督信号
2) 构建复合标记函数（LLM提取功能/解决方案/应用概念）
3) 跨域相似度计算与排序检索
4) 自定义仅正类损失函数校准模型

Result: 内部验证超越基线模型（包括Transformer和零样本LLM）；外部验证显示在专利引用/共同发明人/共同申请人网络中具有更好的主题/认知/组织一致性

Conclusion: 弱监督与语义对齐的结合能有效提升大规模SDG分类效果，为创新政策分析提供可靠工具

Abstract: Classifying patents by their relevance to the UN Sustainable Development
Goals (SDGs) is crucial for tracking how innovation addresses global
challenges. However, the absence of a large, labeled dataset limits the use of
supervised learning. Existing methods, such as keyword searches, transfer
learning, and citation-based heuristics, lack scalability and generalizability.
This paper frames patent-to-SDG classification as a weak supervision problem,
using citations from patents to SDG-tagged scientific publications (NPL
citations) as a noisy initial signal. To address its sparsity and noise, we
develop a composite labeling function (LF) that uses large language models
(LLMs) to extract structured concepts, namely functions, solutions, and
applications, from patents and SDG papers based on a patent ontology.
Cross-domain similarity scores are computed and combined using a rank-based
retrieval approach. The LF is calibrated via a custom positive-only loss that
aligns with known NPL-SDG links without penalizing discovery of new SDG
associations. The result is a silver-standard, soft multi-label dataset mapping
patents to SDGs, enabling the training of effective multi-label regression
models. We validate our approach through two complementary strategies: (1)
internal validation against held-out NPL-based labels, where our method
outperforms several baselines including transformer-based models, and zero-shot
LLM; and (2) external validation using network modularity in patent citation,
co-inventor, and co-applicant graphs, where our labels reveal greater thematic,
cognitive, and organizational coherence than traditional technological
classifications. These results show that weak supervision and semantic
alignment can enhance SDG classification at scale.

</details>


### [23] [MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems](https://arxiv.org/abs/2509.09360)
*Channdeth Sok,David Luz,Yacine Haddam*

Main category: cs.CL

TL;DR: MetaRAG提出无监督的RAG系统幻觉检测框架，通过事实分解、突变验证和局部化定位提升生成可靠性


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效解决RAG系统特有的幻觉问题，需开发无需基准且支持实时检测的方案，特别是在身份敏感的高风险领域

Method: 四阶段框架：1)分解为原子事实 2)生成同/反义词突变 3)上下文一致性验证 4)聚合幻觉评分。支持跨度级错误定位

Result: 企业数据验证有效，实现细粒度的幻觉检测（如特定人群权益声明），并提出身份感知的阈值配置方案

Conclusion: 该框架为RAG系统提供可信赖的幻觉检测方案，通过局部化评分支持身份敏感的部署保障

Abstract: Large Language Models (LLMs) are increasingly deployed in enterprise
applications, yet their reliability remains limited by hallucinations, i.e.,
confident but factually incorrect information. Existing detection approaches,
such as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not
address the unique challenges of Retrieval-Augmented Generation (RAG) systems,
where responses must be consistent with retrieved evidence. We therefore
present MetaRAG, a metamorphic testing framework for hallucination detection in
Retrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time,
unsupervised, black-box setting, requiring neither ground-truth references nor
access to model internals, making it suitable for proprietary and high-stakes
domains. The framework proceeds in four stages: (1) decompose answers into
atomic factoids, (2) generate controlled mutations of each factoid using
synonym and antonym substitutions, (3) verify each variant against the
retrieved context (synonyms are expected to be entailed and antonyms
contradicted), and (4) aggregate penalties for inconsistencies into a
response-level hallucination score. Crucially for identity-aware AI, MetaRAG
localizes unsupported claims at the factoid span where they occur (e.g.,
pregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility),
allowing users to see flagged spans and enabling system designers to configure
thresholds and guardrails for identity-sensitive queries. Experiments on a
proprietary enterprise dataset illustrate the effectiveness of MetaRAG for
detecting hallucinations and enabling trustworthy deployment of RAG-based
conversational agents. We also outline a topic-based deployment design that
translates MetaRAG's span-level scores into identity-aware safeguards; this
design is discussed but not evaluated in our experiments.

</details>


### [24] [Modelling Analogies and Analogical Reasoning: Connecting Cognitive Science Theory and NLP Research](https://arxiv.org/abs/2509.09381)
*Molly R Petersen,Claire E Stevenson,Lonneke van der Plas*

Main category: cs.CL

TL;DR: 论文将认知科学的类比推理理论与NLP研究结合，提出通过优化关系理解提升语言模型效果


<details>
  <summary>Details</summary>
Motivation: 现有NLP方法过度依赖实体相似性，忽视认知科学中关系推理机制，制约了深层语义理解能力

Method: 通过跨学科理论分析，系统梳理认知科学中的类比推理机制，并与NLP技术框架建立概念映射

Result: 揭示关系理解机制可有效解决NLP中的迁移学习、少样本学习等核心挑战

Conclusion: 建议采用认知视角重构NLP模型设计范式，将关注点从表层实体转向深层关系结构

Abstract: Analogical reasoning is an essential aspect of human cognition. In this
paper, we summarize key theory about the processes underlying analogical
reasoning from the cognitive science literature and relate it to current
research in natural language processing. While these processes can be easily
linked to concepts in NLP, they are generally not viewed through a cognitive
lens. Furthermore, we show how these notions are relevant for several major
challenges in NLP research, not directly related to analogy solving. This may
guide researchers to better optimize relational understanding in text, as
opposed to relying heavily on entity-level similarity.

</details>


### [25] [Hierarchical Bracketing Encodings Work for Dependency Graphs](https://arxiv.org/abs/2509.09388)
*Ana Ezquerro,Carlos Gómez-Rodríguez,David Vilares*

Main category: cs.CL

TL;DR: 提出基于分层括号编码的依存图解析方法，在降低标签空间的同时保持结构完整性，在多语言多形式任务中实现高效精准解析。


<details>
  <summary>Details</summary>
Motivation: 传统图线性化方法存在标签空间过大和结构信息丢失问题，需探索更高效的图结构编码方式。

Method: 采用分层括号编码将图结构序列化，支持线性时间复杂度解析，可处理复指、循环和空节点。

Result: 在多语言多形式评测中取得竞争性结果，精确匹配准确率较基线方法提升1.5-3.2%。

Conclusion: 该方法在效率与精度间取得平衡，为复杂语言现象的解析提供了实用化解决方案。

Abstract: We revisit hierarchical bracketing encodings from a practical perspective in
the context of dependency graph parsing. The approach encodes graphs as
sequences, enabling linear-time parsing with $n$ tagging actions, and still
representing reentrancies, cycles, and empty nodes. Compared to existing graph
linearizations, this representation substantially reduces the label space while
preserving structural information. We evaluate it on a multilingual and
multi-formalism benchmark, showing competitive results and consistent
improvements over other methods in exact match accuracy.

</details>


### [26] [GrACE: A Generative Approach to Better Confidence Elicitation in Large Language Models](https://arxiv.org/abs/2509.09438)
*Zhaohan Zhang,Ziquan Liu,Ioannis Patras*

Main category: cs.CL

TL;DR: 提出GrACE方法实现LLM的实时可扩展置信度评估，通过隐藏状态与特殊标记嵌入的相似性机制，在无需额外计算资源下实现最优校准效果和判别能力。


<details>
  <summary>Details</summary>
Motivation: 现有置信度评估方法存在高计算成本与校准效果差的问题，难以满足医疗/金融等高风险场景的实时部署需求。

Method: 设计基于最后隐藏状态与新增特殊标记嵌入相似性的实时置信度生成机制，通过准确率关联的校准目标进行模型微调。

Result: 在3个LLM和2个基准测试中，GrACE的置信度指标在生成任务上超越6种基线方法，测试时扩展策略使决策准确率提升且样本需求减少50%以上。

Conclusion: GrACE为LLM部署提供了可扩展、实时可靠的置信度估计方案，显著降低了高风险应用的部署门槛。

Abstract: Assessing the reliability of Large Language Models (LLMs) by confidence
elicitation is a prominent approach to AI safety in high-stakes applications,
such as healthcare and finance. Existing methods either require expensive
computational overhead or suffer from poor calibration, making them impractical
and unreliable for real-world deployment. In this work, we propose GrACE, a
Generative Approach to Confidence Elicitation that enables scalable and
reliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in
which the model expresses confidence by the similarity between the last hidden
state and the embedding of a special token appended to the vocabulary, in
real-time. We fine-tune the model for calibrating the confidence with
calibration targets associated with accuracy. Experiments with three LLMs and
two benchmark datasets show that the confidence produced by GrACE achieves the
best discriminative capacity and calibration on open-ended generation tasks,
outperforming six competing methods without resorting to additional sampling or
an auxiliary model. Moreover, we propose two strategies for improving test-time
scaling based on confidence induced by GrACE. Experimental results show that
using GrACE not only improves the accuracy of the final decision but also
significantly reduces the number of required samples in the test-time scaling
scheme, indicating the potential of GrACE as a practical solution for deploying
LLMs with scalable, reliable, and real-time confidence estimation.

</details>


### [27] [Mitigating Language Barriers in Education: Developing Multilingual Digital Learning Materials with Machine Translation](https://arxiv.org/abs/2509.09473)
*Lucie Poláková,Martin Popel,Věra Kloudová,Michal Novák,Mariia Anisimova,Jiří Balhar*

Main category: cs.CL

TL;DR: 捷克EdUKate项目通过数字教育技术与机器翻译结合，开发多语言教学材料并实现教育资源多语种转化。


<details>
  <summary>Details</summary>
Motivation: 解决捷克学校中非母语学生的教育资源短缺问题，促进乌克兰语/英语/德语的多语言教育支持，通过机器翻译提升多模态教学资源的转化效率。

Method: 1. 开发捷克-乌克兰教育领域专用机器翻译系统
2. 处理XML/PDF格式内容与科技术语
3. 开展教师需求调研指导系统开发
4. 在web门户集成翻译系统

Result: 1. 完成9000个互动练习的三语种翻译
2. 定制化机器翻译系统成功部署
3. 需求调研有效指导功能开发
4. 所有成果免费开放共享

Conclusion: 该项目成功整合跨学科技术，通过定制化翻译方案有效满足多语言教育需求，为教育技术领域提供了可扩展的本地化解决方案，促进教育包容性发展。

Abstract: The EdUKate project combines digital education, linguistics, translation
studies, and machine translation to develop multilingual learning materials for
Czech primary and secondary schools. Launched through collaboration between a
major Czech academic institution and the country's largest educational
publisher, the project is aimed at translating up to 9,000 multimodal
interactive exercises from Czech into Ukrainian, English, and German for an
educational web portal. It emphasizes the development and evaluation of a
direct Czech-Ukrainian machine translation system tailored to the educational
domain, with special attention to processing formatted content such as XML and
PDF and handling technical and scientific terminology. We present findings from
an initial survey of Czech teachers regarding the needs of non-Czech-speaking
students and describe the system's evaluation and implementation on the web
portal. All resulting applications are freely available to students, educators,
and researchers.

</details>


### [28] [Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness and Knowledge Graphs](https://arxiv.org/abs/2509.09522)
*Vadim Zadykian,Bruno Andrade,Haithem Afli*

Main category: cs.CL

TL;DR: 提出自监督混合架构，结合文本嵌入与知识图谱，通过分层评估方法揭示模型在不同语义相关区域的性能差异，在高STR区域实现25%的RMSE提升。


<details>
  <summary>Details</summary>
Motivation: 解决简历推荐系统中职位名称匹配的语义模糊问题，突破传统基于词汇匹配的局限，强调需关注模型在不同语义相关区域的差异化表现。

Method: 使用SBERT模型生成文本嵌入，整合领域知识图谱（通过图神经网络），采用分层评估策略将STR分数划分为低/中/高三个语义区域进行分析。

Result: 结合知识图谱的微调SBERT模型在高STR区域表现最优，RMSE相对基线降低25%，分层分析显示模型在不同区域存在显著性能波动。

Conclusion: 知识图谱增强的文本嵌入可提升语义匹配精度，分层评估方法比全局指标更能揭示模型特性，对提升HR系统的公平性和可解释性具有实用价值。

Abstract: Semantic Textual Relatedness (STR) captures nuanced relationships between
texts that extend beyond superficial lexical similarity. In this study, we
investigate STR in the context of job title matching - a key challenge in
resume recommendation systems, where overlapping terms are often limited or
misleading. We introduce a self-supervised hybrid architecture that combines
dense sentence embeddings with domain-specific Knowledge Graphs (KGs) to
improve both semantic alignment and explainability. Unlike previous work that
evaluated models on aggregate performance, our approach emphasizes data
stratification by partitioning the STR score continuum into distinct regions:
low, medium, and high semantic relatedness. This stratified evaluation enables
a fine-grained analysis of model performance across semantically meaningful
subspaces. We evaluate several embedding models, both with and without KG
integration via graph neural networks. The results show that fine-tuned SBERT
models augmented with KGs produce consistent improvements in the high-STR
region, where the RMSE is reduced by 25% over strong baselines. Our findings
highlight not only the benefits of combining KGs with text embeddings, but also
the importance of regional performance analysis in understanding model
behavior. This granular approach reveals strengths and weaknesses hidden by
global metrics, and supports more targeted model selection for use in Human
Resources (HR) systems and applications where fairness, explainability, and
contextual matching are essential.

</details>


### [29] [DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning and Label Distribution Learning](https://arxiv.org/abs/2509.09524)
*Daniil Ignatev,Nan Li,Hugh Mee Wong,Anh Dang,Shane Kaszefski Yaschuk*

Main category: cs.CL

TL;DR: DeMeVa团队在LeWiDi 2025竞赛中探索了上下文学习(ICL)和标签分布学习(LDL)方法，证明ICL可预测标注者特异性注释，LDL在软标签预测中具有潜力


<details>
  <summary>Details</summary>
Motivation: 解决标注分歧场景下的标注者特异性注释预测问题，探索不同方法在perspectivist标注任务中的有效性

Method: 1. 基于大语言模型的上下文学习(ICL)，比较不同样本采样策略
2. 使用RoBERTa进行标签分布学习(LDL)，评估多种微调方法

Result: ICL通过聚合个体标注预测生成软标签表现优异(达竞赛前列水平)，LDL方法在软标签预测任务中展现出研究价值

Conclusion: 两种方法对perspectivist社区均有重要意义，特别是LDL方法需要进一步探索其在软标签预测中的应用潜力

Abstract: This system paper presents the DeMeVa team's approaches to the third edition
of the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et
al., 2025). We explore two directions: in-context learning (ICL) with large
language models, where we compare example sampling strategies; and label
distribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we
evaluate several fine-tuning methods. Our contributions are twofold: (1) we
show that ICL can effectively predict annotator-specific annotations
(perspectivist annotations), and that aggregating these predictions into soft
labels yields competitive performance; and (2) we argue that LDL methods are
promising for soft label predictions and merit further exploration by the
perspectivist community.

</details>


### [30] [Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance NLP (2022-2025)](https://arxiv.org/abs/2509.09544)
*Paolo Pedinotti,Peter Baumann,Nathan Jessurun,Leslie Barrett,Enrico Santus*

Main category: cs.CL

TL;DR: 提出MetaGraph方法，通过知识图谱分析金融NLP领域三阶段演进：LLM初期应用→反思局限性→模块化系统整合


<details>
  <summary>Details</summary>
Motivation: 传统文献综述难以适应金融NLP领域在LLM驱动下的快速发展，需要结构化方法追踪研究趋势

Method: 1. 构建金融NLP本体论
2. 对681篇论文(2022-2025)实施LLM驱动的信息抽取
3. 构建可查询的知识图谱进行趋势分析

Result: 揭示金融NLP演进的三个阶段：早期任务/数据集创新→LLM局限性的批判性反思→外围技术模块化整合

Conclusion: MetaGraph不仅清晰展示领域发展脉络，其方法论可复用于其他学科的科学进展图谱构建

Abstract: Large Language Models (LLMs) have rapidly reshaped financial NLP, enabling
new tasks and driving a proliferation of datasets and diversification of data
sources. Yet, this transformation has outpaced traditional surveys. In this
paper, we present MetaGraph, a generalizable methodology for extracting
knowledge graphs from scientific literature and analyzing them to obtain a
structured, queryable view of research trends. We define an ontology for
financial NLP research and apply an LLM-based extraction pipeline to 681 papers
(2022-2025), enabling large-scale, data-driven analysis. MetaGraph reveals
three key phases: early LLM adoption and task/dataset innovation; critical
reflection on LLM limitations; and growing integration of peripheral techniques
into modular systems. This structured view offers both practitioners and
researchers a clear understanding of how financial NLP has evolved -
highlighting emerging trends, shifting priorities, and methodological
shifts-while also demonstrating a reusable approach for mapping scientific
progress in other domains.

</details>


### [31] [Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking](https://arxiv.org/abs/2509.09583)
*Brittany Harbison,Samuel Taubman,Travis Taylor,Ashok. K. Goel*

Main category: cs.CL

TL;DR: 开发基于GPT零样本推断的性格检测模型，整合至在线课程社交推荐系统SAMI，探索性格特质对匹配质量的影响。


<details>
  <summary>Details</summary>
Motivation: 在线课程环境天然阻碍社交群体形成，现有SAMI系统因缺乏性格推断能力导致推荐效果受限。

Method: 利用GPT零样本能力从论坛介绍帖提取大五人格特征，集成至实体匹配系统实现性格感知推荐。

Result: GPT模型在性格检测任务中表现优于传统模型，初步整合显示性格特质可补充现有匹配因素。

Conclusion: 性格特征可增强社交推荐相关性，但需进一步评估对学习参与度和匹配质量的长期影响。

Abstract: Social connection is a vital part of learning, yet online course environments
present barriers to the organic formation of social groups. SAMI offers one
solution by facilitating student connections, but its effectiveness is
constrained by an incomplete Theory of Mind, limiting its ability to create an
effective mental model of a student. One facet of this is its inability to
intuit personality, which may influence the relevance of its recommendations.
To explore this, we propose a personality detection model utilizing GPTs
zero-shot capability to infer Big-Five personality traits from forum
introduction posts, often encouraged in online courses. We benchmark its
performance against established models, demonstrating its efficacy in this
task. Furthermore, we integrate this model into SAMIs entity-based matchmaking
system, enabling personality-informed social recommendations. Initial
integration suggests personality traits can complement existing matching
factors, though additional evaluation is required to determine their full
impact on student engagement and match quality.

</details>


### [32] [Fluent but Unfeeling: The Emotional Blind Spots of Language Models](https://arxiv.org/abs/2509.09593)
*Bangzhao Shu,Isha Joshi,Melissa Karnaze,Anh C. Pham,Ishita Kakkar,Sindhu Kothe,Arpine Hovasapian,Mai ElSherief*

Main category: cs.CL

TL;DR: 大型语言模型（LLM）在细粒度情感对齐上存在局限，新基准EXPRESS揭示其难以准确匹配人类自披露情感标签，需加强上下文理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究多将情绪归类到预定义的有限类别，忽视了细粒度情感表达。论文旨在评估LLM是否能在细粒度层面与人类情感对齐。

Method: 构建含251个细粒度情感标签的Reddit社区数据集EXPRESS，基于情绪理论将预测情感分解为8种基础情绪，系统测试不同提示策略下的主流LLM表现。

Result: LLM预测与人类自披露情感存在偏差，部分模型虽符合情绪理论定义，但语境捕捉能力弱于人类自述，准确预测具上下文敏感性的情感仍具挑战。

Conclusion: LLM在细粒度情感对齐任务中存在明显局限，未来研究需提升其上下文理解能力以实现更精准的心理健康应用。

Abstract: The versatility of Large Language Models (LLMs) in natural language
understanding has made them increasingly popular in mental health research.
While many studies explore LLMs' capabilities in emotion recognition, a
critical gap remains in evaluating whether LLMs align with human emotions at a
fine-grained level. Existing research typically focuses on classifying emotions
into predefined, limited categories, overlooking more nuanced expressions. To
address this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit
communities featuring 251 fine-grained, self-disclosed emotion labels. Our
comprehensive evaluation framework examines predicted emotion terms and
decomposes them into eight basic emotions using established emotion theories,
enabling a fine-grained comparison. Systematic testing of prevalent LLMs under
various prompt settings reveals that accurately predicting emotions that align
with human self-disclosed emotions remains challenging. Qualitative analysis
further shows that while certain LLMs generate emotion terms consistent with
established emotion theories and definitions, they sometimes fail to capture
contextual cues as effectively as human self-disclosures. These findings
highlight the limitations of LLMs in fine-grained emotion alignment and offer
insights for future research aimed at enhancing their contextual understanding.

</details>


### [33] [LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death Determination](https://arxiv.org/abs/2509.09602)
*Yiqun T. Chen,Tyler H. McCormick,Li Liu,Abhirup Datta*

Main category: cs.CL

TL;DR: LLM与传统算法结合的LA-VA流程显著提升口头尸检准确性


<details>
  <summary>Details</summary>
Motivation: 资源有限地区缺乏医疗死亡证明，需提升现有口头尸检方法的准确性

Method: 采用PHMRC数据集，测试GPT-5预测、LCVA基线、文本嵌入和元学习器集成方法

Result: GPT-5在成人/儿童/新生儿组别分别达到48.6%、50.5%、53.5%准确率，较传统方法提升5-10%

Conclusion: 现成LLM辅助方案可实质性改进死亡原因判定，对全球卫生监测具有重要价值

Abstract: Verbal autopsy (VA) is a critical tool for estimating causes of death in
resource-limited settings where medical certification is unavailable. This
study presents LA-VA, a proof-of-concept pipeline that combines Large Language
Models (LLMs) with traditional algorithmic approaches and embedding-based
classification for improved cause-of-death prediction. Using the Population
Health Metrics Research Consortium (PHMRC) dataset across three age categories
(Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches:
GPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles.
Our results demonstrate that GPT-5 achieves the highest individual performance
with average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5%
(Neonate), outperforming traditional statistical machine learning baselines by
5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches
could substantially improve verbal autopsy accuracy, with important
implications for global health surveillance in low-resource settings.

</details>


### [34] [Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems](https://arxiv.org/abs/2509.09629)
*Minghang Zhu,Zhengliang Shi,Zhiwei Xu,Shiguang Wu,Lingjie Wang,Pengjie Ren,Zhaochun Ren,Zhumin Chen*

Main category: cs.CL

TL;DR: 提出MOAT框架，通过规划智能体对齐与基础智能体交替迭代优化，实现多智能体协作效率提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法独立微调多智能体导致协作能力不足，需解决智能体间协调优化问题。

Method: 1. 规划智能体对齐阶段优化子目标生成；2. 基础智能体改进阶段利用自生成的多样化子目标-动作对进行微调。两阶段交替迭代提升泛化能力。

Result: 在6个基准测试中超越现有方法，内外部任务分别实现3.1%和4.4%的平均性能提升。

Conclusion: MOAT通过理论保证的迭代对齐机制，有效增强多智能体协作能力，实验验证了框架的有效性。

Abstract: The advancement of large language models (LLMs) has enabled the construction
of multi-agent systems to solve complex tasks by dividing responsibilities
among specialized agents, such as a planning agent for subgoal generation and a
grounding agent for executing tool-use actions. Most existing methods typically
fine-tune these agents independently, leading to capability gaps among them
with poor coordination. To address this, we propose MOAT, a Multi-Agent Joint
Alignment Tuning framework that improves agents collaboration through iterative
alignment. MOAT alternates between two key stages: (1) Planning Agent
Alignment, which optimizes the planning agent to generate subgoal sequences
that better guide the grounding agent; and (2) Grounding Agent Improving, which
fine-tunes the grounding agent using diverse subgoal-action pairs generated by
the agent itself to enhance its generalization capablity. Theoretical analysis
proves that MOAT ensures a non-decreasing and progressively convergent training
process. Experiments across six benchmarks demonstrate that MOAT outperforms
state-of-the-art baselines, achieving average improvements of 3.1% on held-in
tasks and 4.4% on held-out tasks.

</details>


### [35] [All for One: LLMs Solve Mental Math at the Last Token With Information Transferred From Other Tokens](https://arxiv.org/abs/2509.09650)
*Siddarth Mamidanna,Daking Rai,Ziyu Yao,Yilun Zhou*

Main category: cs.CL

TL;DR: 研究发现大语言模型在心算任务中通过特定子图（AF1）进行计算，关键计算发生在深层及末标记处。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型（LLMs）在心算任务中的内部计算机制，明确其跨层和跨标记的信息传递路径。

Method: 采用三阶段方法：抑制初始层输入特定标记计算→限制中间层跨标记信息传递→强制末层计算聚焦于最后标记，结合CAMA和ABP技术识别AF1子图。

Result: AF1子图在不同模型和算术任务中均表现出高效性、必要性及可迁移性，且末标记通过特定中层接收全局信息。

Conclusion: LLMs心算能力依赖深层末标记的集中计算，AF1子图为核心路径，CAMA/ABP技术具有独特优势。

Abstract: Large language models (LLMs) demonstrate proficiency across numerous
computational tasks, yet their inner workings remain unclear. In theory, the
combination of causal self-attention and multilayer perceptron layers allows
every token to access and compute information based on all preceding tokens. In
practice, to what extent are such operations present? In this paper, on mental
math tasks (i.e., direct math calculation via next-token prediction without
explicit reasoning), we investigate this question in three steps: inhibiting
input-specific token computations in the initial layers, restricting the routes
of information transfer across token positions in the next few layers, and
forcing all computation to happen at the last token in the remaining layers.
With two proposed techniques, Context-Aware Mean Ablation (CAMA) and
Attention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with
high accuracy on a wide variety of mental math tasks, where meaningful
computation occurs very late (in terms of layer depth) and only at the last
token, which receives information of other tokens in few specific middle
layers. Experiments on a variety of models and arithmetic expressions show that
this subgraph is sufficient and necessary for high model performance, transfers
across different models, and works on a variety of input styles. Ablations on
different CAMA and ABP alternatives reveal their unique advantages over other
methods, which may be of independent interest.

</details>


### [36] [Steering MoE LLMs via Expert (De)Activation](https://arxiv.org/abs/2509.09660)
*Mohsen Fayyaz,Ali Modarressi,Hanieh Deilamsalehy,Franck Dernoncourt,Ryan Rossi,Trung Bui,Hinrich Schütze,Nanyun Peng*

Main category: cs.CL

TL;DR: 提出SteerMoE框架，通过控制MoE模型中特定行为相关专家模块，实现不修改权重即可调节模型安全性和忠实性。


<details>
  <summary>Details</summary>
Motivation: 现有MoE模型缺乏对专家模块行为关联性的系统控制机制，需要非侵入式的行为调控方法。

Method: 通过对比输入检测行为相关专家，在推理时选择性激活/停用特定专家模块

Result: 在6个LLM上实现安全性最高提升20%，忠实性提升27%；对抗模式下可完全突破安全防护

Conclusion: 首次揭示专家模块中隐藏的对齐伪装维度，为模型安全提供新的研究方向和技术挑战

Abstract: Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token
through a subset of specialized Feed-Forward Networks (FFN), known as experts.
We present SteerMoE, a framework for steering MoE models by detecting and
controlling behavior-linked experts. Our detection method identifies experts
with distinct activation patterns across paired inputs exhibiting contrasting
behaviors. By selectively (de)activating such experts during inference, we
control behaviors like faithfulness and safety without retraining or modifying
weights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to
+20% and faithfulness by +27%. In adversarial attack mode, it drops safety by
-41% alone, and -100% when combined with existing jailbreak methods, bypassing
all safety guardrails and exposing a new dimension of alignment faking hidden
within experts.

</details>


### [37] [CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2509.09675)
*Runpeng Dai,Linfeng Song,Haolin Liu,Zhenwen Liang,Dian Yu,Haitao Mi,Zhaopeng Tu,Rui Liu,Tong Zheng,Hongtu Zhu,Dong Yu*

Main category: cs.CL

TL;DR: 提出Curiosity-Driven Exploration (CDE)框架改进强化学习验证奖励范式(RLVR)，通过内在好奇心信号增强语言模型探索能力


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在探索不足导致早熟收敛和熵崩溃的问题

Method: 结合actor的生成困惑度和critic多头价值估计方差作为探索奖励，理论证明其与经典RL探索机制的关联

Result: 在AIME基准测试中比标准RLVR方法提升约3个点，揭示了LLM校准崩溃机制

Conclusion: CDE有效提升RLVR性能，为理解LLM失败模式提供新视角

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm
for enhancing the reasoning ability of Large Language Models (LLMs). Yet
current RLVR methods often explore poorly, leading to premature convergence and
entropy collapse. To address this challenge, we introduce Curiosity-Driven
Exploration (CDE), a framework that leverages the model's own intrinsic sense
of curiosity to guide exploration. We formalize curiosity with signals from
both the actor and the critic: for the actor, we use perplexity over its
generated response, and for the critic, we use the variance of value estimates
from a multi-head architecture. Both signals serve as an exploration bonus
within the RLVR framework to guide the model. Our theoretical analysis shows
that the actor-wise bonus inherently penalizes overconfident errors and
promotes diversity among correct responses; moreover, we connect the
critic-wise bonus to the well-established count-based exploration bonus in RL.
Empirically, our method achieves an approximate +3 point improvement over
standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a
calibration collapse mechanism within RLVR, shedding light on common LLM
failure modes.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [38] [Morphology-Preserving Remeshing Approach to Particulate Microstructures via Harmonic Decomposition](https://arxiv.org/abs/2509.08855)
*Mahmoud Shaqfa*

Main category: cs.GR

TL;DR: 针对传统谐波分解方法生成工程微结构网格质量不足的问题，提出基于分层扩散的网格重采样方法，通过非线性扩散实现均匀三角化并保持形态完整。


<details>
  <summary>Details</summary>
Motivation: 传统谐波方法等距采样导致网格非均匀化，影响数值模拟精度与效率，需开发能保持表面形态的高质量网格生成方案。

Method: 采用非线性扩散机制对分析域曲线坐标重采样，通过扩大高曲率区域三角形面积实现参数化均衡，保留表面积/体积不变性。

Result: 在球谐/半球谐方法中验证显示网格质量指标显著提升，各向同性/异性扩散方案均有效改善三角剖分质量。

Conclusion: 该方法为混凝土、砌体等大型微结构数字孪生提供高效网格生成方案，具有工程仿真应用潜力。

Abstract: Harmonic decomposition of surfaces, such as spherical and spheroidal
harmonics, is used to analyze morphology, reconstruct, and generate surface
inclusions of particulate microstructures. However, obtaining high-quality
meshes of engineering microstructures using these approaches remains an open
question. In harmonic approaches, we usually reconstruct surfaces by evaluating
the harmonic bases on equidistantly sampled simplicial complexes of the base
domains (e.g., triangular spheroids and disks). However, this traditional
sampling does not account for local changes in the Jacobian of the basis
functions, resulting in nonuniform discretization after reconstruction or
generation. As it impacts the accuracy and time step, high-quality
discretization of microstructures is crucial for efficient numerical
simulations (e.g., finite element and discrete element methods). To circumvent
this issue, we propose an efficient hierarchical diffusion-based approach for
resampling the surface-i.e., performing a reparameterization-to yield an
equalized mesh triangulation. Analogous to heat problems, we use nonlinear
diffusion to resample the curvilinear coordinates of the analysis domain,
thereby enlarging small triangles at the expense of large triangles on
surfaces. We tested isotropic and anisotropic diffusion schemes on the recent
spheroidal and hemispheroidal harmonics methods. The results show a substantial
improvement in the quality metrics for surface triangulation. Unlike
traditional surface reconstruction and meshing techniques, this approach
preserves surface morphology, along with the areas and volumes of surfaces. We
discuss the results and the associated computational costs for large 2D and 3D
microstructures, such as digital twins of concrete and stone masonry, and their
future applications.

</details>


### [39] [CameraVDP: Perceptual Display Assessment with Uncertainty Estimation via Camera and Visual Difference Prediction](https://arxiv.org/abs/2509.08947)
*Yancheng Cai,Robert Wanat,Rafal Mantiuk*

Main category: cs.GR

TL;DR: 提出CameraVDP框架，结合相机重建流程与视觉差异预测器，实现显示屏缺陷的感知评估与量化分析。


<details>
  <summary>Details</summary>
Motivation: 传统稀疏辐射采样方法无法捕捉高频像素级失真，相机测量存在光学/采样/光度失真，需结合人类视觉系统模型评估显示质量。

Method: 整合HDR堆栈/MTF反演/渐晕校正/几何去畸变/单应变换/色彩校正建立重建流程，并引入视觉差异预测器(VDP)建模视觉感知。

Result: 通过坏点检测/色边识别/均匀性评估验证框架，建立缺陷检测理论性能上限，为VDP质量分提供置信区间。

Conclusion: CameraVDP突破传统测量局限，实现显示屏像素级失真感知评估，为显示质量控制提供量化分析工具。

Abstract: Accurate measurement of images produced by electronic displays is critical
for the evaluation of both traditional and computational displays. Traditional
display measurement methods based on sparse radiometric sampling and fitting a
model are inadequate for capturing spatially varying display artifacts, as they
fail to capture high-frequency and pixel-level distortions. While cameras offer
sufficient spatial resolution, they introduce optical, sampling, and
photometric distortions. Furthermore, the physical measurement must be combined
with a model of a visual system to assess whether the distortions are going to
be visible. To enable perceptual assessment of displays, we propose a
combination of a camera-based reconstruction pipeline with a visual difference
predictor, which account for both the inaccuracy of camera measurements and
visual difference prediction. The reconstruction pipeline combines HDR image
stacking, MTF inversion, vignetting correction, geometric undistortion,
homography transformation, and color correction, enabling cameras to function
as precise display measurement instruments. By incorporating a Visual
Difference Predictor (VDP), our system models the visibility of various stimuli
under different viewing conditions for the human visual system. We validate the
proposed CameraVDP framework through three applications: defective pixel
detection, color fringing awareness, and display non-uniformity evaluation. Our
uncertainty analysis framework enables the estimation of the theoretical upper
bound for defect pixel detection performance and provides confidence intervals
for VDP quality scores.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [40] [A vibe coding learning design to enhance EFL students' talking to, through, and about AI](https://arxiv.org/abs/2509.08854)
*David James Woo,Kai Guo,Yangyang Yu*

Main category: cs.CY

TL;DR: EFL教育中探索AI自然语言编程(vibe coding)的案例研究，开发三维元语言框架并分析学生应用开发差异


<details>
  <summary>Details</summary>
Motivation: 研究如何通过AI自然语言编程帮助EFL学生解决写作问题，建立人机协作的语言学习框架

Method: 采用反向设计开发4小时工作坊，通过案例研究分析两名学生的编程过程（工作表/视频/屏幕录像/AI生成素材）

Result: 学生呈现两极表现：成功实现功能应用vs技术障碍导致设计-功能脱节，差异源自提示工程策略和AI心智模型不同

Conclusion: AI是优质语言学习机器，有效教学需结构化提示工程训练、作者身份协商指导及AI心智模型词汇培养

Abstract: This innovative practice article reports on the piloting of vibe coding
(using natural language to create software applications with AI) for English as
a Foreign Language (EFL) education. We developed a human-AI meta-languaging
framework with three dimensions: talking to AI (prompt engineering), talking
through AI (negotiating authorship), and talking about AI (mental models of
AI). Using backward design principles, we created a four-hour workshop where
two students designed applications addressing authentic EFL writing challenges.
We adopted a case study methodology, collecting data from worksheets and video
recordings, think-aloud protocols, screen recordings, and AI-generated images.
Contrasting cases showed one student successfully vibe coding a functional
application cohering to her intended design, while another encountered
technical difficulties with major gaps between intended design and actual
functionality. Analysis reveals differences in students' prompt engineering
approaches, suggesting different AI mental models and tensions in attributing
authorship. We argue that AI functions as a beneficial languaging machine, and
that differences in how students talk to, through, and about AI explain vibe
coding outcome variations. Findings indicate that effective vibe coding
instruction requires explicit meta-languaging scaffolding, teaching structured
prompt engineering, facilitating critical authorship discussions, and
developing vocabulary for articulating AI mental models.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [41] [Generative Engine Optimization: How to Dominate AI Search](https://arxiv.org/abs/2509.08919)
*Mahe Chen,Xiaoxuan Wang,Kaiwen Chen,Nick Koudas*

Main category: cs.IR

TL;DR: 生成式搜索引擎（如ChatGPT）颠覆传统搜索SEO模式，需采用GEO新范式。AI搜索系统性偏重第三方权威内容，品牌/社交内容占比显著低于谷歌。


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI搜索引擎与传统搜索的差异，为SEO转型提供数据支撑和战略框架

Method: 跨垂直领域/语言/查询变体的大规模对照实验，量化系统差异

Result: AI搜索：1）第三方权威内容占比超80% 2）不同引擎存在域多样性/更新频率/语言敏感度差异 3）头部品牌优势显著

Conclusion: 需构建机器可扫描内容、建立AI认知权威、制定引擎/语言定制策略、突破小众品牌偏见

Abstract: The rapid adoption of generative AI-powered search engines like ChatGPT,
Perplexity, and Gemini is fundamentally reshaping information retrieval, moving
from traditional ranked lists to synthesized, citation-backed answers. This
shift challenges established Search Engine Optimization (SEO) practices and
necessitates a new paradigm, which we term Generative Engine Optimization
(GEO).
  This paper presents a comprehensive comparative analysis of AI Search and
traditional web search (Google). Through a series of large-scale, controlled
experiments across multiple verticals, languages, and query paraphrases, we
quantify critical differences in how these systems source information. Our key
findings reveal that AI Search exhibit a systematic and overwhelming bias
towards Earned media (third-party, authoritative sources) over Brand-owned and
Social content, a stark contrast to Google's more balanced mix. We further
demonstrate that AI Search services differ significantly from each other in
their domain diversity, freshness, cross-language stability, and sensitivity to
phrasing.
  Based on these empirical results, we formulate a strategic GEO agenda. We
provide actionable guidance for practitioners, emphasizing the critical need
to: (1) engineer content for machine scannability and justification, (2)
dominate earned media to build AI-perceived authority, (3) adopt
engine-specific and language-aware strategies, and (4) overcome the inherent
"big brand bias" for niche players. Our work provides the foundational
empirical analysis and a strategic framework for achieving visibility in the
new generative search landscape.

</details>


### [42] [Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations](https://arxiv.org/abs/2509.09651)
*Zakaria El Kassimi,Fares Fourati,Mohamed-Slim Alouini*

Main category: cs.IR

TL;DR: 针对无线电法规领域提出电信专用RAG流程，构建首个多选评估集，在保持97%检索准确率的同时使GPT-4o生成准确率提升12%


<details>
  <summary>Details</summary>
Motivation: 解决法律敏感且高风险的无线电法规领域问答需求，提供可靠的领域特定解决方案

Method: 通过自动过滤和人工验证构建评估集，定义领域特定检索指标，开发结构化RAG流程优化文档检索

Result: 检索准确率达97%，GPT-4o生成准确率相对提升12%（普通文档插入仅提升1%）

Conclusion: 针对性强的结构化检索为法规问答提供有效基线方案，公开的代码和数据集推动领域研究

Abstract: We study question answering in the domain of radio regulations, a legally
sensitive and high-stakes area. We propose a telecom-specific
Retrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge,
the first multiple-choice evaluation set for this domain, constructed from
authoritative sources using automated filtering and human validation. To assess
retrieval quality, we define a domain-specific retrieval metric, under which
our retriever achieves approximately 97% accuracy. Beyond retrieval, our
approach consistently improves generation accuracy across all tested models. In
particular, while naively inserting documents without structured retrieval
yields only marginal gains for GPT-4o (less than 1%), applying our pipeline
results in nearly a 12% relative improvement. These findings demonstrate that
carefully targeted grounding provides a simple yet strong baseline and an
effective domain-specific solution for regulatory question answering. All code
and evaluation scripts, along with our derived question-answer dataset, are
available at https://github.com/Zakaria010/Radio-RAG.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [43] [OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning](https://arxiv.org/abs/2509.09332)
*Yuecheng Liu,Dafeng Chi,Shiguang Wu,Zhanguang Zhang,Yuzheng Zhuang,Bowen Yang,He Zhu,Lingfeng Zhang,Pengwei Xie,David Gamaliel Arcos Bravo,Yingxue Zhang,Jianye Hao,Xingyue Quan*

Main category: cs.RO

TL;DR: 提出OmniEVA解决MLLM具身系统的几何适应性差距与具身约束差距，通过任务自适应3D基础机制和具身感知推理框架，实现高效空间推理与可执行规划。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM具身系统存在几何适应性不足（仅2D输入或硬编码3D）和忽视机器人物理约束的问题，导致跨任务适应性差且规划不可行。

Method: 1. 任务自适应3D基础机制：通过门控路由器动态调节3D信息融合；2. 具身感知推理框架：联合优化任务目标与机器人物理约束。

Result: OmniEVA在通用推理任务达到SOTA，复合任务基准测试验证其鲁棒性，支持导航、操作等6类下游场景的泛化应用。

Conclusion: OmniEVA通过显式3D信息调节和具身约束联合推理，有效提升跨任务适应性与规划可执行性，为通用具身智能提供新范式。

Abstract: Recent advances in multimodal large language models (MLLMs) have opened new
opportunities for embodied intelligence, enabling multimodal understanding,
reasoning, and interaction, as well as continuous spatial decision-making.
Nevertheless, current MLLM-based embodied systems face two critical
limitations. First, Geometric Adaptability Gap: models trained solely on 2D
inputs or with hard-coded 3D geometry injection suffer from either insufficient
spatial information or restricted 2D generalization, leading to poor
adaptability across tasks with diverse spatial demands. Second, Embodiment
Constraint Gap: prior work often neglects the physical constraints and
capacities of real robots, resulting in task plans that are theoretically valid
but practically infeasible.To address these gaps, we introduce OmniEVA -- an
embodied versatile planner that enables advanced embodied reasoning and task
planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding
mechanism, which introduces a gated router to perform explicit selective
regulation of 3D fusion based on contextual requirements, enabling
context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware
Reasoning framework that jointly incorporates task goals and embodiment
constraints into the reasoning loop, resulting in planning decisions that are
both goal-directed and executable. Extensive experimental results demonstrate
that OmniEVA not only achieves state-of-the-art general embodied reasoning
performance, but also exhibits a strong ability across a wide range of
downstream scenarios. Evaluations of a suite of proposed embodied benchmarks,
including both primitive and composite tasks, confirm its robust and versatile
planning capabilities. Project page: https://omnieva.github.io

</details>


### [44] [SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning](https://arxiv.org/abs/2509.09674)
*Haozhan Li,Yuxin Zuo,Jiale Yu,Yuhao Zhang,Zhaohui Yang,Kaiyan Zhang,Xuekai Zhu,Yuchen Zhang,Tianxing Chen,Ganqu Cui,Dehui Wang,Dingxiang Luo,Yuchen Fan,Youbang Sun,Jia Zeng,Jiangmiao Pang,Shanghang Zhang,Yu Wang,Yao Mu,Bowen Zhou,Ning Ding*

Main category: cs.RO

TL;DR: 提出SimpleVLA-RL强化学习框架，通过改进RL训练方法显著提升VLA模型的长期动作规划能力，在多个测试集超越SFT方法。


<details>
  <summary>Details</summary>
Motivation: 针对VLA模型依赖大规模人类演示数据和泛化能力不足的核心痛点，尝试将强化学习范式引入视觉语言动作模型训练。

Method: 基于veRL框架改进：1）VLA专用轨迹采样策略 2）可扩展并行架构 3）多环境协同渲染技术 4）优化损失计算模块

Result: 在LIBERO基准达SOTA，RoboTwin 1.0&2.0超越π0基线（探索增强策略加持下），真实任务性能超过传统SFT方法

Conclusion: 验证了RL对VLA模型优化的有效性，发现'pushcut'训练现象，为减少数据依赖和提升泛化能力提供新方向

Abstract: Vision-Language-Action (VLA) models have recently emerged as a powerful
paradigm for robotic manipulation. Despite substantial progress enabled by
large-scale pretraining and supervised fine-tuning (SFT), these models face two
fundamental challenges: (i) the scarcity and high cost of large-scale
human-operated robotic trajectories required for SFT scaling, and (ii) limited
generalization to tasks involving distribution shift. Recent breakthroughs in
Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can
dramatically enhance step-by-step reasoning capabilities, raising a natural
question: Can RL similarly improve the long-horizon step-by-step action
planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL
framework tailored for VLA models. Building upon veRL, we introduce
VLA-specific trajectory sampling, scalable parallelization, multi-environment
rendering, and optimized loss computation. When applied to OpenVLA-OFT,
SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\pi_0$
on RoboTwin 1.0\&2.0 with the exploration-enhancing strategies we introduce.
SimpleVLA-RL not only reduces dependence on large-scale data and enables robust
generalization, but also remarkably surpasses SFT in real-world tasks.
Moreover, we identify a novel phenomenon ``pushcut'' during RL training,
wherein the policy discovers previously unseen patterns beyond those seen in
the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [45] [Objectness Similarity: Capturing Object-Level Fidelity in 3D Scene Evaluation](https://arxiv.org/abs/2509.09143)
*Yuiko Uchida,Ren Togo,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出OSIM三维场景评估指标，通过量化物体性特征实现更符合人类感知的评估方法


<details>
  <summary>Details</summary>
Motivation: 现有指标关注整体图像质量，与人类以物体为基本感知单元的特性存在偏差。受神经心理学启发，需要开发基于物体感知的三维场景评估体系

Method: 利用物体检测模型的特征表示量化场景中每个物体的'物体性'，建立对象中心评估框架

Result: 用户研究表明OSIM与人类感知一致性显著优于现有指标，并通过标准化实验重新评估了最新三维重建模型

Conclusion: OSIM首次实现基于物体感知的三维场景量化评估，为计算机视觉与人类视觉认知架起新的桥梁

Abstract: This paper presents Objectness SIMilarity (OSIM), a novel evaluation metric
for 3D scenes that explicitly focuses on "objects," which are fundamental units
of human visual perception. Existing metrics assess overall image quality,
leading to discrepancies with human perception. Inspired by neuropsychological
insights, we hypothesize that human recognition of 3D scenes fundamentally
involves attention to individual objects. OSIM enables object-centric
evaluations by leveraging an object detection model and its feature
representations to quantify the "objectness" of each object in the scene. Our
user study demonstrates that OSIM aligns more closely with human perception
compared to existing metrics. We also analyze the characteristics of OSIM using
various approaches. Moreover, we re-evaluate recent 3D reconstruction and
generation models under a standardized experimental setup to clarify
advancements in this field. The code is available at
https://github.com/Objectness-Similarity/OSIM.

</details>


### [46] [Recurrence Meets Transformers for Universal Multimodal Retrieval](https://arxiv.org/abs/2509.08897)
*Davide Caffagni,Sara Sarto,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara*

Main category: cs.CV

TL;DR: ReT-2提出了一种支持多模态查询（图像+文本）的统一检索模型，利用循环Transformer架构动态整合跨模态信息，在多项基准测试中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于单模态查询/文档且依赖任务微调，无法满足复杂多模态检索需求。本文旨在开发通用高效的多模态检索方案。

Method: 采用多层表征和LSTM门控机制的循环Transformer架构，动态融合跨层跨模态信息，捕捉细粒度视觉-文本特征。

Result: 在M2KR/M-BEIR基准测试中全面超越现有方法，推理速度提升40%，显存占用减少30%，下游任务（如Encyclopedic-VQA）准确率提升5-8%。

Conclusion: ReT-2通过统一架构突破多模态检索瓶颈，开源实现促进相关研究，为检索增强型生成系统提供高效解决方案。

Abstract: With the rapid advancement of multimodal retrieval and its application in
LLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged.
Existing methods predominantly rely on task-specific fine-tuning of
vision-language models and are limited to single-modality queries or documents.
In this paper, we propose ReT-2, a unified retrieval model that supports
multimodal queries, composed of both images and text, and searches across
multimodal document collections where text and images coexist. ReT-2 leverages
multi-layer representations and a recurrent Transformer architecture with
LSTM-inspired gating mechanisms to dynamically integrate information across
layers and modalities, capturing fine-grained visual and textual details. We
evaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different
retrieval configurations. Results demonstrate that ReT-2 consistently achieves
state-of-the-art performance across diverse settings, while offering faster
inference and reduced memory usage compared to prior approaches. When
integrated into retrieval-augmented generation pipelines, ReT-2 also improves
downstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source
code and trained models are publicly available at:
https://github.com/aimagelab/ReT-2

</details>


### [47] [COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation](https://arxiv.org/abs/2509.09014)
*Umair Hassan*

Main category: cs.CV

TL;DR: 构建了目前最大的乌尔都语图像描述数据集COCO-Urdu（59k图像/319k描述），通过混合质量评估框架保障数据质量，旨在缓解多模态研究中的语言偏见。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语作为2.5亿人使用的低资源语言，长期缺乏高质量多模态数据集，导致多模态系统对该语言支持不足，加剧了以高资源语言为主的模型偏见。

Method: 1. 基于MS COCO数据集分层抽样保持原分布
2. 使用SeamlessM4T v2翻译并设计混合质量评估框架（COMET-Kiwi评估翻译质量+CLIP视觉对齐+BERTScore语义一致性）
3. 通过开源大模型迭代优化低分描述

Result: 1. 获得当前最大公开乌尔都语数据集
2. BLEU/SacreBLEU/chrF基准测试表现优异
3. 同步开源质量评估技术流程

Conclusion: 数据集的发布为构建包容性视觉-语言系统奠定基础，通过开放技术流程推动多模态研究中的语言公平性。

Abstract: Urdu, spoken by over 250 million people, remains critically under-served in
multimodal and vision-language research. The absence of large-scale,
high-quality datasets has limited the development of Urdu-capable systems and
reinforced biases in multilingual vision-language models trained primarily on
high-resource languages. To address this gap, we present COCO-Urdu, a
large-scale image-caption dataset derived from MS COCO, containing 59,000
images and 319,000 Urdu captions selected through stratified sampling to
preserve the original distribution. Captions were translated using SeamlessM4T
v2 and validated with a hybrid multimodal quality estimation framework that
integrates COMET-Kiwi for translation quality, CLIP-based similarity for visual
grounding, and BERTScore with back-translation for semantic consistency;
low-scoring captions were iteratively refined using open-source large language
models. We further benchmark COCO-Urdu on BLEU, SacreBLEU, and chrF, reporting
consistently strong results. To the best of our knowledge, COCO-Urdu is the
largest publicly available Urdu captioning dataset. By releasing both the
dataset and the quality estimation pipeline, we aim to reduce language bias in
multimodal research and establish a foundation for inclusive vision-language
systems.

</details>


### [48] [Can Multimodal LLMs See Materials Clearly? A Multimodal Benchmark on Materials Characterization](https://arxiv.org/abs/2509.09307)
*Zhengzhao Lai,Youbin Zheng,Zhenyang Cai,Haonan Lyu,Jinpu Yang,Hongqing Liang,Yan Hu,Benyou Wang*

Main category: cs.CV

TL;DR: 本文提出首个材料表征图像理解基准MatCha，包含1500个需专家知识的问题，评估显示现有MLLMs与人类专家存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大模型在材料科学中主要应用于生成任务，但对真实材料表征图像的理解能力尚未充分探索。

Method: 构建覆盖材料研究四个阶段（表征、分析、推理、决策）的21个任务基准，包含1500个专业问题，并评估主流MLLMs性能。

Result: 现有模型在需要高阶专业知识和复杂视觉感知的任务中表现显著低于人类专家，简单提示策略无法有效提升性能。

Conclusion: MatCha基准将推动新材料发现和自主科学代理研究，现有模型在真实材料表征场景中适应性有限。

Abstract: Materials characterization is fundamental to acquiring materials information,
revealing the processing-microstructure-property relationships that guide
material design and optimization. While multimodal large language models
(MLLMs) have recently shown promise in generative and predictive tasks within
materials science, their capacity to understand real-world characterization
imaging data remains underexplored. To bridge this gap, we present MatCha, the
first benchmark for materials characterization image understanding, comprising
1,500 questions that demand expert-level domain expertise. MatCha encompasses
four key stages of materials research comprising 21 distinct tasks, each
designed to reflect authentic challenges faced by materials scientists. Our
evaluation of state-of-the-art MLLMs on MatCha reveals a significant
performance gap compared to human experts. These models exhibit degradation
when addressing questions requiring higher-level expertise and sophisticated
visual perception. Simple few-shot and chain-of-thought prompting struggle to
alleviate these limitations. These findings highlight that existing MLLMs still
exhibit limited adaptability to real-world materials characterization
scenarios. We hope MatCha will facilitate future research in areas such as new
material discovery and autonomous scientific agents. MatCha is available at
https://github.com/FreedomIntelligence/MatCha.

</details>


### [49] [FLUX-Reason-6M & PRISM-Bench: A Million-Scale Text-to-Image Reasoning Dataset and Comprehensive Benchmark](https://arxiv.org/abs/2509.09680)
*Rongyao Fang,Aldrich Yu,Chengqi Duan,Linjiang Huang,Shuai Bai,Yuxuan Cai,Kun Wang,Si Liu,Xihui Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: 提出FLUX-Reason-6M数据集与PRISM-Bench评测标准，通过6百万高质量图像和7维评测体系推动开源文生图模型的推理能力发展。


<details>
  <summary>Details</summary>
Motivation: 解决开源T2I模型因缺乏大规模推理数据集和评测标准导致的性能瓶颈，缩小与闭源系统的差距。

Method: 构建包含6大特征维度的FLUX-Reason-6M数据集（15,000 A100 GPU天）和包含长文本挑战的PRISM-Bench评测框架，采用视觉语言模型进行细粒度评估。

Result: 对19个主流模型的评测揭示关键性能缺陷，验证了数据增强对模型推理能力的提升效果。

Conclusion: 开源数据集与评测体系将推动推理导向的文生图技术发展，填补社区资源空白。

Abstract: The advancement of open-source text-to-image (T2I) models has been hindered
by the absence of large-scale, reasoning-focused datasets and comprehensive
evaluation benchmarks, resulting in a performance gap compared to leading
closed-source systems. To address this challenge, We introduce FLUX-Reason-6M
and PRISM-Bench (Precise and Robust Image Synthesis Measurement Benchmark).
FLUX-Reason-6M is a massive dataset consisting of 6 million high-quality
FLUX-generated images and 20 million bilingual (English and Chinese)
descriptions specifically designed to teach complex reasoning. The image are
organized according to six key characteristics: Imagination, Entity, Text
rendering, Style, Affection, and Composition, and design explicit Generation
Chain-of-Thought (GCoT) to provide detailed breakdowns of image generation
steps. The whole data curation takes 15,000 A100 GPU days, providing the
community with a resource previously unattainable outside of large industrial
labs. PRISM-Bench offers a novel evaluation standard with seven distinct
tracks, including a formidable Long Text challenge using GCoT. Through
carefully designed prompts, it utilizes advanced vision-language models for
nuanced human-aligned assessment of prompt-image alignment and image
aesthetics. Our extensive evaluation of 19 leading models on PRISM-Bench
reveals critical performance gaps and highlights specific areas requiring
improvement. Our dataset, benchmark, and evaluation code are released to
catalyze the next wave of reasoning-oriented T2I generation. Project page:
https://flux-reason-6m.github.io/ .

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [50] [Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs](https://arxiv.org/abs/2509.08847)
*Amna Hassan*

Main category: cs.AI

TL;DR: 提出基于LLMs的自动化框架，实现GDD到Unity游戏原型的自动转换


<details>
  <summary>Details</summary>
Motivation: 解决游戏设计文档到可运行原型间的转化难题，提升AI辅助游戏开发效率

Method: 结合微调LLaMA-3模型与Unity集成包，实现端到端代码生成系统

Result: 模型获得4.8/5.0综合评分，在编译成功率、规范遵循度等指标上超越基线模型

Conclusion: 系统有效填补AI辅助游戏开发空白，确立LLMs在设计转实施过程中的关键作用

Abstract: This paper presents a novel framework for automated game template generation
by transforming Game Design Documents (GDDs) into functional Unity game
prototypes using Natural Language Processing (NLP) and multi-modal Large
Language Models (LLMs). We introduce an end-to-end system that parses GDDs,
extracts structured game specifications, and synthesizes Unity-compatible C#
code that implements the core mechanics, systems, and architecture defined in
the design documentation. Our approach combines a fine-tuned LLaMA-3 model
specialized for Unity code generation with a custom Unity integration package
that streamlines the implementation process. Evaluation results demonstrate
significant improvements over baseline models, with our fine-tuned model
achieving superior performance (4.8/5.0 average score) compared to
state-of-the-art LLMs across compilation success, GDD adherence, best practices
adoption, and code modularity metrics. The generated templates demonstrate high
adherence to GDD specifications across multiple game genres. Our system
effectively addresses critical gaps in AI-assisted game development,
positioning LLMs as valuable tools in streamlining the transition from game
design to implementation.

</details>


### [51] [Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning](https://arxiv.org/abs/2509.09284)
*Bingning Huang,Tu Nguyen,Matthieu Zimmer*

Main category: cs.AI

TL;DR: 探索利用MCTS轨迹优化基于偏好的强化学习策略，提出分阶段GRPO训练范式并分析其效果及挑战


<details>
  <summary>Details</summary>
Motivation: 现有MCTS在生成中间轨迹方面效果显著，但如何将其优势估计应用于无价值网络的策略优化仍待探索

Method: 基于GRPO算法构建分阶段训练框架，通过部分揭示的MCTS展开生成完成项，建立树状优势估计结构与前缀条件奖励信号体系

Result: 结构化优势估计提升策略稳定性与推理质量表征，但面临优势饱和和奖励信号崩溃，提出启发式与统计缓解方案

Conclusion: 该方法为复杂奖励结构下的策略学习提供新思路，未来需解决树状结构特有的梯度传播与信号耦合问题

Abstract: Recent advances in reasoning with large language models (LLMs) have shown the
effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality
intermediate trajectories, particularly in math and symbolic domains. Inspired
by this, we explore how MCTS-derived trajectories, traditionally used for
training value or reward models, can be repurposed to improve policy
optimization in preference-based reinforcement learning (RL). Specifically, we
focus on Group Relative Policy Optimization (GRPO), a recent algorithm that
enables preference-consistent policy learning without value networks. We
propose a staged GRPO training paradigm where completions are derived from
partially revealed MCTS rollouts, introducing a novel tree-structured setting
for advantage estimation. This leads to a rich class of prefix-conditioned
reward signals, which we analyze theoretically and empirically. Our initial
results indicate that while structured advantage estimation can stabilize
updates and better reflect compositional reasoning quality, challenges such as
advantage saturation and reward signal collapse remain. We propose heuristic
and statistical solutions to mitigate these issues and discuss open challenges
for learning under staged or tree-like reward structures.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [52] [Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison](https://arxiv.org/abs/2509.09009)
*Marianna Nezhurina,Taishi Nakamura,Timur Carstensen,Niccolò Ajroldi,Ville Komulainen,David Salinas,Jenia Jitsev*

Main category: cs.LG

TL;DR: 研究者开发了open-sci-ref系列基线模型（0.13B-1.7B参数），通过跨8个开放数据集的大规模训练（1T tokens），建立标准化评估基准，并发布完整训练资源。


<details>
  <summary>Details</summary>
Motivation: 为解决不同训练方法在模型规模和数据集上的效果评估难题，通过建立可复现的参考基线帮助研究者进行方法质量验证和训练动态分析。

Method: 采用多尺度模型架构（0.13B-1.7B参数），在8个开放参考数据集上进行训练，通过标准化基准测试比较性能，并引入中间检查点分析训练过程动态。

Result: NemoTron-CC HQ数据集表现最优（相对DCLM-baseline提升12%，FineWeb-Edu提升8%），同时开源训练日志、中间模型和评估框架。

Conclusion: 该研究通过标准化基线模型和完整训练资源的开放，推动了大规模模型训练的可比性研究，并为数据集选择提供了实证依据。

Abstract: We introduce open-sci-ref, a family of dense transformer models trained as
research baselines across multiple model (0.13B to 1.7B parameters) and token
scales (up to 1T) on 8 recent open reference datasets. Evaluating the models on
various standardized benchmarks, our training runs set establishes reference
points that enable researchers to assess the sanity and quality of alternative
training approaches across scales and datasets. Intermediate checkpoints allow
comparison and studying of the training dynamics. The established reference
baselines allow training procedures to be compared through their scaling
trends, aligning them on a common compute axis. Comparison of open reference
datasets reveals that training on NemoTron-CC HQ consistently outperforms other
reference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to
intermediate training checkpoints, the release includes logs, code, and
downstream evaluations to simplify reproduction, standardize comparison, and
facilitate future research.

</details>


### [53] [Identifying Key Features for Establishing Sustainable Agro-Tourism Centre: A Data Driven Approach](https://arxiv.org/abs/2509.09214)
*Alka Gadakh,Vidya Kumbhar,Sonal Khosla,Kumar Karunendra*

Main category: cs.LG

TL;DR: 研究通过文献综述和机器学习模型（LASSO结合LR、RF等）识别农业旅游发展关键指标，逻辑回归模型在70-30%和80-20%数据划分下分别达到98%和99%准确率。


<details>
  <summary>Details</summary>
Motivation: 农业旅游作为快速发展的旅游子领域，需通过科学方法识别增长策略以促进农村经济多元化并保护文化遗产与传统农业。

Method: 分两阶段研究：1.文献综述确定指标；2.使用LASSO结合逻辑回归、决策树、随机森林、XGBoost等机器学习模型进行特征选择。

Result: LASSO+逻辑回归模型在70-30%数据划分下准确率98%，80-20%划分下达99%；随机森林和XGBoost分别获得95%和97%准确率。

Conclusion: 机器学习模型能有效识别农业旅游增长核心指标，其中逻辑回归结合LASSO方法展现出最优预测性能，为策略制定提供量化支持。

Abstract: Agro-tourism serves as a strategic economic model designed to facilitate
rural development by diversifying income streams for local communities like
farmers while promoting the conservation of indigenous cultural heritage and
traditional agricultural practices. As a very booming subdomain of tourism,
there is a need to study the strategies for the growth of Agro-tourism in
detail. The current study has identified the important indicators for the
growth and enhancement of agro-tourism. The study is conducted in two phases:
identification of the important indicators through a comprehensive literature
review and in the second phase state-of-the-art techniques were used to
identify the important indicators for the growth of agro-tourism. The
indicators are also called features synonymously, the machine learning models
for feature selection were applied and it was observed that the Least Absolute
Shrinkage and Selection Operator (LASSO) method combined with, the machine
Learning Classifiers such as Logistic Regression (LR), Decision Trees (DT),
Random Forest (RF) Tree, and Extreme Gradient Boosting (XGBOOST) models were
used to suggest the growth of the agro-tourism. The results show that with the
LASSO method, LR model gives the highest classification accuracy of 98% in
70-30% train-test data followed by RF with 95% accuracy. Similarly, in the
80-20% train-test data LR maintains the highest accuracy at 99%, while DT and
XGBoost follow with 97% accuracy.

</details>


### [54] [Harnessing Uncertainty: Entropy-Modulated Policy Gradients for Long-Horizon LLM Agents](https://arxiv.org/abs/2509.09265)
*Jiawei Wang,Jiacai Liu,Yuqian Fu,Yingru Li,Xintao Wang,Yuan Lin,Yu Yue,Lin Zhang,Yang Wang,Ke Wang*

Main category: cs.LG

TL;DR: 提出熵调制策略梯度（EMPG）框架解决LLMs策略梯度与熵耦合问题，通过不确定性校准和未来清晰度奖励显著提升长时任务性能


<details>
  <summary>Details</summary>
Motivation: 传统基于LLMs的代理在稀疏奖励环境中难以分配中间步骤信用，且策略梯度幅度与熵的耦合导致自信动作更新不足/不确定动作更新不稳定

Method: 1. 基于步骤不确定性和任务结果重新校准学习信号
2. 对自信正确动作放大更新/惩罚自信错误/衰减不确定步骤更新
3. 引入未来清晰度奖励项鼓励可预测路径

Result: 在WebShop/ALFWorld/DeepSearch三个基准测试中取得显著性能提升，大幅超越现有策略梯度基线方法

Conclusion: EMPG有效解决策略梯度与熵的耦合问题，通过动态调整学习信号和探索稳定性机制，为复杂任务学习提供新方向

Abstract: In long-horizon tasks, recent agents based on Large Language Models (LLMs)
face a significant challenge that sparse, outcome-based rewards make it
difficult to assign credit to intermediate steps. Previous methods mainly focus
on creating dense reward signals to guide learning, either through traditional
reinforcement learning techniques like inverse reinforcement learning or by
using Process Reward Models for step-by-step feedback. In this paper, we
identify a fundamental problem in the learning dynamics of LLMs: the magnitude
of policy gradients is inherently coupled with the entropy, which leads to
inefficient small updates for confident correct actions and potentially
destabilizes large updates for uncertain ones. To resolve this, we propose
Entropy-Modulated Policy Gradients (EMPG), a framework that re-calibrates the
learning signal based on step-wise uncertainty and the final task outcome. EMPG
amplifies updates for confident correct actions, penalizes confident errors,
and attenuates updates from uncertain steps to stabilize exploration. We
further introduce a bonus term for future clarity that encourages agents to
find more predictable solution paths. Through comprehensive experiments on
three challenging agent tasks, WebShop, ALFWorld, and Deep Search, we
demonstrate that EMPG achieves substantial performance gains and significantly
outperforms strong policy gradient baselines. Project page is at
https://empgseed-seed.github.io/

</details>


### [55] [LLMs Don't Know Their Own Decision Boundaries: The Unreliability of Self-Generated Counterfactual Explanations](https://arxiv.org/abs/2509.09396)
*Harry Mayne,Ryan Othniel Kearns,Yushi Yang,Andrew M. Bean,Eoin Delaney,Chris Russell,Adam Mahdi*

Main category: cs.LG

TL;DR: 研究发现LLMs生成的自我反事实解释在有效性（改变预测）与最小性（最小修改）之间存在矛盾，提示此类解释工具存在可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型能否通过自生成的对抗性解释（SCEs）有效揭示其决策逻辑，为高风险领域应用提供可信解释。

Method: 在多模型（如LLaMA-2、GPT-4）、多数据集（FEVER、SNLI）中评估SCEs的有效性（是否改变预测）和最小性（编辑距离），分析模型规模与解释质量的关系。

Result: 模型生成有效SCEs时修改冗余（非最小化），而追求最小化时却无法改变预测，有效性与最小性呈负相关。该现象具有跨模型/数据的普适性。

Conclusion: SCEs作为解释工具可靠性存疑，可能误导模型行为解读。建议高风险场景部署LLMs时需审慎评估自解释机制的可靠性。

Abstract: To collaborate effectively with humans, language models must be able to
explain their decisions in natural language. We study a specific type of
self-explanation: self-generated counterfactual explanations (SCEs), where a
model explains its prediction by modifying the input such that it would have
predicted a different outcome. We evaluate whether LLMs can produce SCEs that
are valid, achieving the intended outcome, and minimal, modifying the input no
more than necessary. When asked to generate counterfactuals, we find that LLMs
typically produce SCEs that are valid, but far from minimal, offering little
insight into their decision-making behaviour. Worryingly, when asked to
generate minimal counterfactuals, LLMs typically make excessively small edits
that fail to change predictions. The observed validity-minimality trade-off is
consistent across several LLMs, datasets, and evaluation settings. Our findings
suggest that SCEs are, at best, an ineffective explainability tool and, at
worst, can provide misleading insights into model behaviour. Proposals to
deploy LLMs in high-stakes settings must consider the impact of unreliable
self-explanations on downstream decision-making. Our code is available at
https://github.com/HarryMayne/SCEs.

</details>


### [56] [ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable Orthogonal Butterfly Transforms](https://arxiv.org/abs/2509.09679)
*Bingxin Xu,Zhen Dong,Oussama Elachqar,Yuzhang Shang*

Main category: cs.LG

TL;DR: 提出ButterflyQuant方法，通过可学习的蝴蝶变换替代固定哈达玛矩阵，改善大语言模型的2位量化效果，在LLaMA-2-7B上实现15.4的困惑度优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有固定正交变换方法无法适应不同transformer层的异常值分布特征，需要开发层自适应的旋转量化方案

Method: 使用参数化的Givens旋转角度构建可学习蝴蝶变换，通过正交约束保证理论完备性，引入激活均匀性正则化，仅需128样本单GPU快速训练

Result: 在2位量化设置下，LLaMA-2-7B模型的困惑度从QuaRot的22.1显著降低至15.4

Conclusion: 连续参数化的蝴蝶变换在保持O(n logn)计算效率的同时，实现了更好的量化适配，为极低比特量化提供了新思路

Abstract: Large language models require massive memory footprints, severely limiting
deployment on consumer hardware. Quantization reduces memory through lower
numerical precision, but extreme 2-bit quantization suffers from catastrophic
performance loss due to outliers in activations. Rotation-based methods such as
QuIP and QuaRot apply orthogonal transforms to eliminate outliers before
quantization, using computational invariance: $\mathbf{y} = \mathbf{Wx} =
(\mathbf{WQ}^T)(\mathbf{Qx})$ for orthogonal $\mathbf{Q}$. However, these
methods use fixed transforms--Hadamard matrices achieving optimal worst-case
coherence $\mu = 1/\sqrt{n}$--that cannot adapt to specific weight
distributions. We identify that different transformer layers exhibit distinct
outlier patterns, motivating layer-adaptive rotations rather than
one-size-fits-all approaches. We propose ButterflyQuant, which replaces
Hadamard rotations with learnable butterfly transforms parameterized by
continuous Givens rotation angles. Unlike Hadamard's discrete $\{+1, -1\}$
entries that are non-differentiable and prohibit gradient-based learning,
butterfly transforms' continuous parameterization enables smooth optimization
while guaranteeing orthogonality by construction. This orthogonal constraint
ensures theoretical guarantees in outlier suppression while achieving $O(n \log
n)$ computational complexity with only $\frac{n \log n}{2}$ learnable
parameters. We further introduce a uniformity regularization on
post-transformation activations to promote smoother distributions amenable to
quantization. Learning requires only 128 calibration samples and converges in
minutes on a single GPU--a negligible one-time cost. On LLaMA-2-7B with 2-bit
quantization, ButterflyQuant achieves 15.4 perplexity versus 22.1 for QuaRot.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [57] [Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection Systems](https://arxiv.org/abs/2509.09204)
*Chin Yuen Kwok,Jia Qi Yip,Zhen Qiu,Chi Hung Chi,Kwok Yan Lam*

Main category: cs.SD

TL;DR: 提出基于多样化真实语音库的跨测试框架bona fide cross-testing，改进音频深度伪造检测模型的评估方法


<details>
  <summary>Details</summary>
Motivation: 现有音频伪造检测评估方法存在合成器样本量偏差导致EER不可靠、真实语音场景单一的问题

Method: 引入九种真实语音类型构建多样化数据集，采用合成器分层评估并聚合EER指标

Result: 新方法相比传统评估具备更强鲁棒性和可解释性，并开源包含150+合成器的测试数据集

Conclusion: 该框架为音频深度伪造检测提供了更全面的评估基准，公开数据集将推动领域研究

Abstract: Audio deepfake detection (ADD) models are commonly evaluated using datasets
that combine multiple synthesizers, with performance reported as a single Equal
Error Rate (EER). However, this approach disproportionately weights
synthesizers with more samples, underrepresenting others and reducing the
overall reliability of EER. Additionally, most ADD datasets lack diversity in
bona fide speech, often featuring a single environment and speech style (e.g.,
clean read speech), limiting their ability to simulate real-world conditions.
To address these challenges, we propose bona fide cross-testing, a novel
evaluation framework that incorporates diverse bona fide datasets and
aggregates EERs for more balanced assessments. Our approach improves robustness
and interpretability compared to traditional evaluation methods. We benchmark
over 150 synthesizers across nine bona fide speech types and release a new
dataset to facilitate further research at
https://github.com/cyaaronk/audio_deepfake_eval.

</details>


### [58] [DiFlow-TTS: Discrete Flow Matching with Factorized Speech Tokens for Low-Latency Zero-Shot Text-To-Speech](https://arxiv.org/abs/2509.09631)
*Ngoc-Son Nguyen,Hieu-Nghia Huynh-Nguyen,Thanh V. T. Tran,Truong-Son Hy,Van Nguyen*

Main category: cs.SD

TL;DR: 提出首个纯离散流匹配语音合成模型DiFlow-TTS，通过显式建模分解的语音属性，在零样本场景下实现高效高质量的语音克隆。


<details>
  <summary>Details</summary>
Motivation: 现有零样本TTS模型存在推理速度慢、连续性空间建模无法充分发挥离散表征优势的问题，需探索更高效的离散生成方法。

Method: 采用纯离散流匹配框架，构建包含文本条件、韵律/声学属性的上下文学习架构，通过分解流预测机制实现属性解耦建模。

Result: 在自然度、韵律、说话人风格保持等指标表现优异，推理速度达基线模型的25.8倍，模型参数量更紧凑。

Conclusion: DiFlow-TTS验证了纯离散流匹配在语音合成中的有效性，为高效率零样本语音克隆提供了新范式。

Abstract: Zero-shot Text-to-Speech (TTS) aims to synthesize high-quality speech that
mimics the voice of an unseen speaker using only a short reference sample,
requiring not only speaker adaptation but also accurate modeling of prosodic
attributes. Recent approaches based on language models, diffusion, and flow
matching have shown promising results in zero-shot TTS, but still suffer from
slow inference and repetition artifacts. Discrete codec representations have
been widely adopted for speech synthesis, and recent works have begun to
explore diffusion models in purely discrete settings, suggesting the potential
of discrete generative modeling for speech synthesis. However, existing
flow-matching methods typically embed these discrete tokens into a continuous
space and apply continuous flow matching, which may not fully leverage the
advantages of discrete representations. To address these challenges, we
introduce DiFlow-TTS, which, to the best of our knowledge, is the first model
to explore purely Discrete Flow Matching for speech synthesis. DiFlow-TTS
explicitly models factorized speech attributes within a compact and unified
architecture. It leverages in-context learning by conditioning on textual
content, along with prosodic and acoustic attributes extracted from a reference
speech, enabling effective attribute cloning in a zero-shot setting. In
addition, the model employs a factorized flow prediction mechanism with
distinct heads for prosody and acoustic details, allowing it to learn
aspect-specific distributions. Experimental results demonstrate that DiFlow-TTS
achieves promising performance in several key metrics, including naturalness,
prosody, preservation of speaker style, and energy control. It also maintains a
compact model size and achieves low-latency inference, generating speech up to
25.8 times faster than the latest existing baselines.

</details>
