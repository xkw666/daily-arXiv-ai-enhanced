{"id": "2510.07359", "pdf": "https://arxiv.org/pdf/2510.07359", "abs": "https://arxiv.org/abs/2510.07359", "authors": ["Jingfei Huang", "Han Tu"], "title": "Inconsistent Affective Reaction: Sentiment of Perception and Opinion in Urban Environments", "categories": ["cs.CL", "cs.LG", "cs.SI"], "comment": "10 pages", "summary": "The ascension of social media platforms has transformed our understanding of\nurban environments, giving rise to nuanced variations in sentiment reaction\nembedded within human perception and opinion, and challenging existing\nmultidimensional sentiment analysis approaches in urban studies. This study\npresents novel methodologies for identifying and elucidating sentiment\ninconsistency, constructing a dataset encompassing 140,750 Baidu and Tencent\nStreet view images to measure perceptions, and 984,024 Weibo social media text\nposts to measure opinions. A reaction index is developed, integrating object\ndetection and natural language processing techniques to classify sentiment in\nBeijing Second Ring for 2016 and 2022. Classified sentiment reaction is\nanalysed and visualized using regression analysis, image segmentation, and word\nfrequency based on land-use distribution to discern underlying factors. The\nperception affective reaction trend map reveals a shift toward more evenly\ndistributed positive sentiment, while the opinion affective reaction trend map\nshows more extreme changes. Our mismatch map indicates significant disparities\nbetween the sentiments of human perception and opinion of urban areas over the\nyears. Changes in sentiment reactions have significant relationships with\nelements such as dense buildings and pedestrian presence. Our inconsistent maps\npresent perception and opinion sentiments before and after the pandemic and\noffer potential explanations and directions for environmental management, in\nformulating strategies for urban renewal.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6574\u5408\u8857\u666f\u56fe\u50cf\u548c\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u6570\u636e\uff0c\u5f00\u53d1\u60c5\u611f\u53cd\u5e94\u6307\u6570\u5206\u6790\u5317\u4eac\u4e8c\u73af\u533a\u57df\u611f\u77e5\u4e0e\u610f\u89c1\u7684\u60c5\u611f\u5dee\u5f02\uff0c\u63ed\u793a\u57ce\u5e02\u8981\u7d20\u4e0e\u60c5\u611f\u53d8\u5316\u7684\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u57ce\u5e02\u591a\u7ef4\u60c5\u611f\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u4eba\u7c7b\u611f\u77e5\u4e0e\u610f\u89c1\u95f4\u7684\u590d\u6742\u60c5\u611f\u5dee\u5f02\uff0c\u9700\u5f00\u53d1\u65b0\u65b9\u6cd5\u8bc6\u522b\u8fd9\u79cd\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u6784\u5efa\u5305\u542b14\u4e07+\u8857\u666f\u56fe\u50cf\u548c98\u4e07+\u5fae\u535a\u6587\u672c\u7684\u6570\u636e\u96c6\uff0c\u6574\u5408\u76ee\u6807\u68c0\u6d4b/NLP\u6280\u672f\u5f00\u53d1\u60c5\u611f\u53cd\u5e94\u6307\u6570\uff0c\u8fd0\u7528\u56de\u5f52\u5206\u6790/\u56fe\u50cf\u5206\u5272/\u8bcd\u9891\u7edf\u8ba1\u8fdb\u884c\u7a7a\u95f4\u53ef\u89c6\u5316\u3002", "result": "\u611f\u77e5\u60c5\u611f\u8d8b\u4e8e\u5747\u5300\u6b63\u5411\u5206\u5e03\uff0c\u610f\u89c1\u60c5\u611f\u5448\u73b0\u6781\u7aef\u53d8\u5316\uff1b\u5efa\u6210\u73af\u5883\u5bc6\u5ea6\u4e0e\u884c\u4eba\u6d3b\u52a8\u7b49\u8981\u7d20\u4e0e\u60c5\u611f\u53d8\u5316\u663e\u8457\u76f8\u5173\uff1b\u75ab\u60c5\u524d\u540e\u611f\u77e5-\u610f\u89c1\u5b58\u5728\u660e\u663e\u9519\u4f4d\u3002", "conclusion": "\u60c5\u611f\u4e0d\u4e00\u81f4\u5730\u56fe\u4e3a\u57ce\u5e02\u66f4\u65b0\u63d0\u4f9b\u51b3\u7b56\u652f\u6301\uff0c\u5efa\u8bae\u5728\u73af\u5883\u7ba1\u7406\u4e2d\u540c\u65f6\u8003\u8651\u7269\u7406\u7a7a\u95f4\u7279\u5f81\u548c\u793e\u4f1a\u611f\u77e5\u8981\u7d20\u3002"}}
{"id": "2510.07414", "pdf": "https://arxiv.org/pdf/2510.07414", "abs": "https://arxiv.org/abs/2510.07414", "authors": ["Mufei Li", "Dongqi Fu", "Limei Wang", "Si Zhang", "Hanqing Zeng", "Kaan Sancak", "Ruizhong Qiu", "Haoyu Wang", "Xiaoxin He", "Xavier Bresson", "Yinglong Xia", "Chonglin Sun", "Pan Li"], "title": "Haystack Engineering: Context Engineering for Heterogeneous and Agentic Long-Context Evaluation", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Code available at https://github.com/Graph-COM/HaystackCraft", "summary": "Modern long-context large language models (LLMs) perform well on synthetic\n\"needle-in-a-haystack\" (NIAH) benchmarks, but such tests overlook how noisy\ncontexts arise from biased retrieval and agentic workflows. We argue that\nhaystack engineering is necessary to construct noisy long contexts that\nfaithfully capture key real-world factors -- distraction from heterogeneous\nbiased retrievers and cascading errors in agentic workflows -- to test models'\nlong-context robustness. We instantiate it through HaystackCraft, a new NIAH\nbenchmark built on the full English Wikipedia hyperlink network with multi-hop\nquestions. HaystackCraft evaluates how heterogeneous retrieval strategies\n(e.g., sparse, dense, hybrid, and graph-based) affect distractor composition,\nhaystack ordering, and downstream LLM performance. HaystackCraft further\nextends NIAH to dynamic, LLM-dependent settings that simulate agentic\noperations, where models refine queries, reflect on their past reasonings, and\ndecide when to stop. Experiments with 15 long-context models show that (1)\nwhile stronger dense retrievers can introduce more challenging distractors,\ngraph-based reranking simultaneously improves retrieval effectiveness and\nmitigates more harmful distractors; (2) in agentic tests, even advanced models\nlike Gemini 2.5 Pro and GPT-5 suffer cascading failures from self-generated\ndistractors or struggle to perform early stops. These results highlight\npersistent challenges in agentic long-context reasoning and establish\nHaystackCraft as a valuable testbed for future progress.", "AI": {"tldr": "\u63d0\u51faHaystackCraft\u65b0\u578b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u771f\u5b9e\u7ef4\u57fa\u767e\u79d1\u7f51\u7edc\u6784\u5efa\u591a\u8df3\u95ee\u9898\uff0c\u63ed\u793a\u5f02\u6784\u68c0\u7d22\u7b56\u7565\u5bf9LLM\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u5f71\u54cd\u53ca\u4ee3\u7406\u5de5\u4f5c\u6d41\u4e2d\u7684\u7ea7\u8054\u5931\u8d25\u95ee\u9898", "motivation": "\u73b0\u6709NIAH\u6d4b\u8bd5\u5ffd\u7565\u771f\u5b9e\u573a\u666f\u4e2d\u7531\u6709\u504f\u68c0\u7d22\u548c\u4ee3\u7406\u5de5\u4f5c\u6d41\u7a0b\u4ea7\u751f\u7684\u566a\u58f0\u4e0a\u4e0b\u6587\uff0c\u9700\u8981\u6784\u5efa\u66f4\u8d34\u8fd1\u73b0\u5b9e\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u6d4b\u8bd5\u957f\u4e0a\u4e0b\u6587\u6a21\u578b\u7684\u9c81\u68d2\u6027", "method": "\u57fa\u4e8e\u82f1\u6587\u7ef4\u57fa\u767e\u79d1\u8d85\u94fe\u63a5\u7f51\u7edc\u6784\u5efa\u52a8\u6001\u591a\u8df3\u95ee\u9898\uff0c\u7cfb\u7edf\u8bc4\u4f30\u7a00\u758f/\u7a20\u5bc6/\u6df7\u5408/\u56fe\u57fa\u68c0\u7d22\u7b56\u7565\u7684\u5e72\u6270\u6548\u5e94\uff0c\u5e76\u6269\u5c55\u81f3LLM\u81ea\u4e3b\u751f\u6210\u67e5\u8be2\u3001\u53cd\u601d\u63a8\u7406\u7684\u4ee3\u7406\u64cd\u4f5c\u573a\u666f", "result": "\u56fe\u57fa\u91cd\u6392\u5728\u63d0\u5347\u68c0\u7d22\u6548\u679c\u7684\u540c\u65f6\u51cf\u5c11\u5e72\u6270\uff1bGPT-5/Gemini\u7b49\u5148\u8fdb\u6a21\u578b\u5728\u81ea\u4e3b\u751f\u6210\u5e72\u6270\u573a\u666f\u4e0b\u4ecd\u5b58\u5728\u7ea7\u8054\u5931\u8d25\uff0c\u4e14\u96be\u4ee5\u5b9e\u73b0\u65e9\u671f\u505c\u6b62\u51b3\u7b56", "conclusion": "HaystackCraft\u4e3a\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u91cd\u8981\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63ed\u793a\u5f53\u524d\u6a21\u578b\u5728\u4ee3\u7406\u64cd\u4f5c\u4e2d\u7684\u6301\u7eed\u6027\u6311\u6218\uff0c\u5f3a\u8c03\u68c0\u7d22\u7b56\u7565\u4f18\u5316\u4e0e\u63a8\u7406\u7ec8\u6b62\u673a\u5236\u7684\u91cd\u8981\u6027"}}
{"id": "2510.07434", "pdf": "https://arxiv.org/pdf/2510.07434", "abs": "https://arxiv.org/abs/2510.07434", "authors": ["Olia Toporkov", "Alan Akbik", "Rodrigo Agerri"], "title": "Lemma Dilemma: On Lemma Generation Without Domain- or Language-Specific Training Data", "categories": ["cs.CL"], "comment": "14 pages, 2 figures, 5 tables. Accepted to EMNLP Findings 2025", "summary": "Lemmatization is the task of transforming all words in a given text to their\ndictionary forms. While large language models (LLMs) have demonstrated their\nability to achieve competitive results across a wide range of NLP tasks, there\nis no prior evidence of how effective they are in the contextual lemmatization\ntask. In this paper, we empirically investigate the capacity of the latest\ngeneration of LLMs to perform in-context lemmatization, comparing it to the\ntraditional fully supervised approach. In particular, we consider the setting\nin which supervised training data is not available for a target domain or\nlanguage, comparing (i) encoder-only supervised approaches, fine-tuned\nout-of-domain, and (ii) cross-lingual methods, against direct in-context lemma\ngeneration with LLMs. Our experimental investigation across 12 languages of\ndifferent morphological complexity finds that, while encoders remain\ncompetitive in out-of-domain settings when fine-tuned on gold data, current\nLLMs reach state-of-the-art results for most languages by directly generating\nlemmas in-context without prior fine-tuning, provided just with a few examples.\nData and code available upon publication:\nhttps://github.com/oltoporkov/lemma-dilemma", "AI": {"tldr": "LLMs\u901a\u8fc7\u5c11\u91cf\u4e0a\u4e0b\u6587\u793a\u4f8b\u5373\u53ef\u572812\u79cd\u8bed\u8a00\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u8bcd\u5f62\u8fd8\u539f\u6548\u679c\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u8d85\u8d8a\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7f3a\u4e4f\u76ee\u6807\u9886\u57df\u8bad\u7ec3\u6570\u636e\u65f6\uff0c\u80fd\u5426\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u5b9e\u73b0\u9ad8\u6548\u8bcd\u5f62\u8fd8\u539f\u4efb\u52a1\uff0c\u5e76\u4e0e\u4f20\u7edf\u76d1\u7763\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002", "method": "\u572812\u79cd\u4e0d\u540c\u5f62\u6001\u590d\u6742\u5ea6\u7684\u8bed\u8a00\u4e2d\uff0c\u5bf9\u6bd4\uff1a1) \u8de8\u9886\u57df\u5fae\u8c03\u7684\u76d1\u7763\u5f0f\u7f16\u7801\u5668 2) \u8de8\u8bed\u8a00\u65b9\u6cd5 3) \u76f4\u63a5\u4f7f\u7528LLMs\u8fdb\u884c\u4e0a\u4e0b\u6587\u8bcd\u5f62\u8fd8\u539f\uff08\u65e0\u5fae\u8c03\uff0c\u4ec5\u63d0\u4f9b\u5c11\u91cf\u793a\u4f8b\uff09", "result": "\u76d1\u7763\u65b9\u6cd5\u5728\u8de8\u9886\u57df\u5fae\u8c03\u540e\u4ecd\u5177\u7ade\u4e89\u529b\uff0c\u4f46LLMs\u5728\u591a\u6570\u8bed\u8a00\u4e2d\u4ec5\u9700\u5c11\u91cf\u793a\u4f8b\u5373\u8fbeSOTA\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u532e\u4e4f\u573a\u666f\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u5b66\u4e60\u8303\u5f0f\u4f7fLLMs\u6210\u4e3a\u4f4e\u8d44\u6e90\u8bcd\u5f62\u8fd8\u539f\u4efb\u52a1\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u65b9\u6cd5\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\u3002"}}
{"id": "2510.07340", "pdf": "https://arxiv.org/pdf/2510.07340", "abs": "https://arxiv.org/abs/2510.07340", "authors": ["Yongzhi Li", "Saining Zhang", "Yibing Chen", "Boying Li", "Yanxin Zhang", "Xiaoyu Du"], "title": "SpotDiff: Spotting and Disentangling Interference in Feature Space for Subject-Preserving Image Generation", "categories": ["cs.GR", "cs.LG"], "comment": null, "summary": "Personalized image generation aims to faithfully preserve a reference\nsubject's identity while adapting to diverse text prompts. Existing\noptimization-based methods ensure high fidelity but are computationally\nexpensive, while learning-based approaches offer efficiency at the cost of\nentangled representations influenced by nuisance factors. We introduce\nSpotDiff, a novel learning-based method that extracts subject-specific features\nby spotting and disentangling interference. Leveraging a pre-trained CLIP image\nencoder and specialized expert networks for pose and background, SpotDiff\nisolates subject identity through orthogonality constraints in the feature\nspace. To enable principled training, we introduce SpotDiff10k, a curated\ndataset with consistent pose and background variations. Experiments demonstrate\nthat SpotDiff achieves more robust subject preservation and controllable\nediting than prior methods, while attaining competitive performance with only\n10k training samples.", "AI": {"tldr": "SpotDiff\u63d0\u51fa\u901a\u8fc7\u7279\u5f81\u6b63\u4ea4\u7ea6\u675f\u548c\u4e13\u7528\u4e13\u5bb6\u7f51\u7edc\u5b9e\u73b0\u9ad8\u6548\u9ad8\u4fdd\u771f\u7684\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210", "motivation": "\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u8ba1\u7b97\u6602\u8d35\uff0c\u800c\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u7279\u5f81\u7ea0\u7f20\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u89e3\u8026\u5e72\u6270\u56e0\u7d20", "method": "\u5229\u7528CLIP\u56fe\u50cf\u7f16\u7801\u5668\uff0c\u901a\u8fc7\u59ff\u6001/\u80cc\u666f\u4e13\u5bb6\u7f51\u7edc\u548c\u7279\u5f81\u7a7a\u95f4\u6b63\u4ea4\u7ea6\u675f\u5206\u79bb\u4e3b\u4f53\u7279\u5f81", "result": "\u572810k\u8bad\u7ec3\u6837\u672c\u4e0b\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u4e3b\u4f53\u4fdd\u7559\u548c\u53ef\u63a7\u7f16\u8f91\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "SpotDiff\u901a\u8fc7\u7279\u5f81\u89e3\u8026\u65b9\u6cd5\u548cSpotDiff10k\u6570\u636e\u96c6\uff0c\u5728\u4e2a\u6027\u5316\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u6548\u7387\u4e0e\u8d28\u91cf\u7684\u5e73\u8861"}}
{"id": "2510.07437", "pdf": "https://arxiv.org/pdf/2510.07437", "abs": "https://arxiv.org/abs/2510.07437", "authors": ["Amruta Parulekar", "Preethi Jyothi"], "title": "LASER: An LLM-based ASR Scoring and Evaluation Rubric", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "comment": "Accepted to EMNLP 2025", "summary": "Standard ASR evaluation metrics like Word Error Rate (WER) tend to unfairly\npenalize morphological and syntactic nuances that do not significantly alter\nsentence semantics. We introduce an LLM-based scoring rubric LASER that\nleverages state-of-the-art LLMs' in-context learning abilities to learn from\nprompts with detailed examples. Hindi LASER scores using Gemini 2.5 Pro\nachieved a very high correlation score of 94% with human annotations. Hindi\nexamples in the prompt were also effective in analyzing errors in other Indian\nlanguages such as Marathi, Kannada and Malayalam. We also demonstrate how a\nsmaller LLM like Llama 3 can be finetuned on word-pair examples derived from\nreference and ASR predictions to predict what kind of penalty should be applied\nwith close to 89% accuracy.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684ASR\u8bc4\u4f30\u6307\u6807LASER\uff0c\u76f8\u6bd4\u4f20\u7edfWER\u66f4\u5173\u6ce8\u8bed\u4e49\u4fdd\u7559\uff0c\u51cf\u5c11\u5bf9\u5f62\u6001/\u53e5\u6cd5\u5dee\u5f02\u7684\u8fc7\u5ea6\u60e9\u7f5a\u3002", "motivation": "\u4f20\u7edfASR\u8bc4\u4f30\u6307\u6807WER\u8fc7\u5ea6\u60e9\u7f5a\u4e0d\u5f71\u54cd\u8bed\u4e49\u7684\u5f62\u6001/\u53e5\u6cd5\u5dee\u5f02\uff0c\u9700\u5f00\u53d1\u66f4\u6ce8\u91cd\u8bed\u4e49\u4fdd\u7559\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5229\u7528Gemini 2.5 Pro\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u6784\u5efaLASER\u8bc4\u5206\u6846\u67b6\uff1b\u5fae\u8c03Llama 3\u6a21\u578b\u5904\u7406\u8bcd\u5bf9\u60e9\u7f5a\u9884\u6d4b\u4efb\u52a1\u3002", "result": "\u5370\u5730\u8bedLASER\u4e0e\u4eba\u5de5\u6807\u6ce8\u76f8\u5173\u6027\u8fbe94%\uff0c\u8de8\u8bed\u8a00\u9002\u7528\u6027\u9a8c\u8bc1\u6210\u529f\uff1b\u5fae\u8c03\u540e\u7684Llama 3\u8bcd\u5bf9\u60e9\u7f5a\u9884\u6d4b\u51c6\u786e\u7387\u8fbe89%\u3002", "conclusion": "LASER\u6307\u6807\u63d0\u4f9b\u66f4\u7cbe\u51c6\u7684ASR\u8bc4\u4f30\u65b9\u6848\uff0c\u5c0f\u6a21\u578b\u901a\u8fc7\u5fae\u8c03\u4ea6\u53ef\u80dc\u4efb\u7279\u5b9a\u8bc4\u4f30\u4efb\u52a1\uff0c\u5177\u6709\u8de8\u8bed\u8a00\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.07343", "pdf": "https://arxiv.org/pdf/2510.07343", "abs": "https://arxiv.org/abs/2510.07343", "authors": ["Shaorong Zhang", "Rob Brekelmans", "Greg Ver Steeg"], "title": "Local MAP Sampling for Diffusion Models", "categories": ["cs.GR", "cs.AI", "eess.IV"], "comment": null, "summary": "Diffusion Posterior Sampling (DPS) provides a principled Bayesian approach to\ninverse problems by sampling from $p(x_0 \\mid y)$. However, in practice, the\ngoal of inverse problem solving is not to cover the posterior but to recover\nthe most accurate reconstruction, where optimization-based diffusion solvers\noften excel despite lacking a clear probabilistic foundation. We introduce\nLocal MAP Sampling (LMAPS), a new inference framework that iteratively solving\nlocal MAP subproblems along the diffusion trajectory. This perspective\nclarifies their connection to global MAP estimation and DPS, offering a unified\nprobabilistic interpretation for optimization-based methods. Building on this\nfoundation, we develop practical algorithms with a probabilistically\ninterpretable covariance approximation, a reformulated objective for stability\nand interpretability, and a gradient approximation for non-differentiable\noperators. Across a broad set of image restoration and scientific tasks, LMAPS\nachieves state-of-the-art performance, including $\\geq 2$ dB gains on motion\ndeblurring, JPEG restoration, and quantization, and $>1.5$ dB improvements on\ninverse scattering benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u5c40\u90e8MAP\u91c7\u6837\u6846\u67b6LMAPS\uff0c\u901a\u8fc7\u8fed\u4ee3\u6c42\u89e3\u6269\u6563\u8f68\u8ff9\u4e0a\u7684\u5c40\u90e8MAP\u5b50\u95ee\u9898\uff0c\u7edf\u4e00\u4f18\u5316\u65b9\u6cd5\u4e0e\u6982\u7387\u89e3\u91ca\uff0c\u5728\u591a\u9879\u9006\u4efb\u52a1\u4e2d\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4f18\u5316\u7684\u6269\u6563\u6c42\u89e3\u5668\u867d\u5728\u91cd\u5efa\u7cbe\u5ea6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7f3a\u4e4f\u6982\u7387\u7406\u8bba\u652f\u6491\u3002\u9700\u5efa\u7acb\u7edf\u4e00\u6846\u67b6\u4e3a\u4f18\u5316\u65b9\u6cd5\u63d0\u4f9b\u6982\u7387\u89e3\u91ca\u5e76\u63d0\u5347\u6027\u80fd\u3002", "method": "1. \u6784\u5efa\u5c40\u90e8MAP\u63a8\u7406\u6846\u67b6\u8fde\u63a5\u5168\u5c40MAP\u4e0eDPS\n2. \u5f00\u53d1\u6982\u7387\u53ef\u89e3\u91ca\u7684\u534f\u65b9\u5dee\u8fd1\u4f3c\u65b9\u6cd5\n3. \u8bbe\u8ba1\u7a33\u5b9a\u76ee\u6807\u51fd\u6570\u53ca\u975e\u53ef\u5fae\u5206\u7b97\u5b50\u68af\u5ea6\u8fd1\u4f3c", "result": "\u56fe\u50cf\u6062\u590d\u4efb\u52a1\u5b9e\u73b0\u22652dB\u589e\u76ca\uff08\u8fd0\u52a8\u53bb\u6a21\u7cca/JPEG\u6062\u590d/\u91cf\u5316\uff09\uff0c\u9006\u6563\u5c04\u57fa\u51c6\u63d0\u5347>1.5dB\uff0c\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LMAPS\u6210\u529f\u5efa\u7acb\u4f18\u5316\u65b9\u6cd5\u7684\u6982\u7387\u57fa\u7840\uff0c\u5176\u534f\u65b9\u5dee\u8fd1\u4f3c\u548c\u76ee\u6807\u91cd\u6784\u663e\u8457\u63d0\u5347\u6269\u6563\u6a21\u578b\u5728\u9006\u95ee\u9898\u4e2d\u7684\u91cd\u5efa\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2510.07453", "pdf": "https://arxiv.org/pdf/2510.07453", "abs": "https://arxiv.org/abs/2510.07453", "authors": ["Zifan Jiang", "Colin Leong", "Amit Moryossef", "Anne G\u00f6hring", "Annette Rios", "Oliver Cory", "Maksym Ivashechkin", "Neha Tarigopula", "Biao Zhang", "Rico Sennrich", "Sarah Ebling"], "title": "Meaningful Pose-Based Sign Language Evaluation", "categories": ["cs.CL"], "comment": "Accepted at WMT 2025", "summary": "We present a comprehensive study on meaningfully evaluating sign language\nutterances in the form of human skeletal poses. The study covers keypoint\ndistance-based, embedding-based, and back-translation-based metrics. We show\ntradeoffs between different metrics in different scenarios through automatic\nmeta-evaluation of sign-level retrieval and a human correlation study of\ntext-to-pose translation across different sign languages. Our findings and the\nopen-source pose-evaluation toolkit provide a practical and reproducible way of\ndeveloping and evaluating sign language translation or generation systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u624b\u8bed\u59ff\u52bf\u8bc4\u4f30\u6307\u6807\uff0c\u5f00\u53d1\u4e86\u5f00\u6e90\u5de5\u5177\u5305\u4ee5\u652f\u6301\u624b\u8bed\u7ffb\u8bd1\u7cfb\u7edf\u7684\u5f00\u53d1\u548c\u8bc4\u4f30", "motivation": "\u73b0\u6709\u624b\u8bed\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u6027\uff0c\u9700\u5efa\u7acb\u591a\u7ef4\u5ea6\u8bc4\u4f30\u4f53\u7cfb\u4ee5\u4fc3\u8fdb\u624b\u8bed\u751f\u6210\u6280\u672f\u53d1\u5c55", "method": "\u7ed3\u5408\u5173\u952e\u70b9\u8ddd\u79bb/\u5d4c\u5165/\u53cd\u5411\u7ffb\u8bd1\u4e09\u7c7b\u6307\u6807\uff0c\u901a\u8fc7\u7b26\u53f7\u7ea7\u68c0\u7d22\u7684\u81ea\u52a8\u5143\u8bc4\u4f30\u548c\u8de8\u8bed\u8a00\u6587\u672c-\u59ff\u52bf\u8f6c\u6362\u7684\u4eba\u7c7b\u76f8\u5173\u7814\u7a76\u8fdb\u884c\u9a8c\u8bc1", "result": "\u63ed\u793a\u4e0d\u540c\u573a\u666f\u4e0b\u8bc4\u4f30\u6307\u6807\u7684\u6743\u8861\u5173\u7cfb\uff0c\u5f00\u53d1\u51fa\u53ef\u590d\u7528\u7684\u5f00\u6e90\u8bc4\u4f30\u5de5\u5177\u5305", "conclusion": "\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u5de5\u5177\u4e3a\u624b\u8bed\u7ffb\u8bd1\u6280\u672f\u7684\u7814\u53d1\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55"}}
{"id": "2510.07638", "pdf": "https://arxiv.org/pdf/2510.07638", "abs": "https://arxiv.org/abs/2510.07638", "authors": ["Kinjal Parikh", "Danny M. Kaufman", "David I. W. Levin", "Alec Jacobson"], "title": "Differentiable Variable Fonts", "categories": ["cs.GR"], "comment": null, "summary": "Editing and animating text appearance for graphic designs, commercials, etc.\nremain highly skilled tasks requiring detailed, hands on efforts from artists.\nAutomating these manual workflows requires balancing the competing goals of\nmaintaining legibility and aesthetics of text, while enabling creative\nexpression. Variable fonts, recent parametric extensions to traditional fonts,\noffer the promise of new ways to ease and automate typographic design and\nanimation. Variable fonts provide custom constructed parameters along which\nfonts can be smoothly varied. These parameterizations could then potentially\nserve as high value continuous design spaces, opening the door to automated\ndesign optimization tools. However, currently variable fonts are underutilized\nin creative applications, because artists so far still need to manually tune\nfont parameters. Our work opens the door to intuitive and automated font design\nand animation workflows with differentiable variable fonts. To do so we distill\nthe current variable font specification to a compact mathematical formulation\nthat differentiably connects the highly non linear, non invertible mapping of\nvariable font parameters to the underlying vector graphics representing the\ntext. This enables us to construct a differentiable framework, with respect to\nvariable font parameters, allowing us to perform gradient based optimization of\nenergies defined on vector graphics control points, and on target rasterized\nimages. We demonstrate the utility of this framework with four applications:\ndirect shape manipulation, overlap aware modeling, physics based text\nanimation, and automated font design optimization. Our work now enables\nleveraging the carefully designed affordances of variable fonts with\ndifferentiability to use modern design optimization technologies, opening new\npossibilities for easy and intuitive typographic design workflows.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u5fae\u5206\u53ef\u53d8\u5b57\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6570\u5b66\u5efa\u6a21\u5b9e\u73b0\u5b57\u4f53\u53c2\u6570\u7684\u68af\u5ea6\u4f18\u5316\uff0c\u652f\u6301\u81ea\u52a8\u5316\u8bbe\u8ba1\u4f18\u5316\u4e0e\u52a8\u753b\u751f\u6210\u3002", "motivation": "\u4f20\u7edf\u5b57\u4f53\u7f16\u8f91\u4f9d\u8d56\u827a\u672f\u5bb6\u624b\u52a8\u8c03\u6574\u53c2\u6570\uff0c\u53ef\u53d8\u5b57\u4f53\u867d\u63d0\u4f9b\u53c2\u6570\u5316\u8bbe\u8ba1\u7a7a\u95f4\u4f46\u672a\u88ab\u5145\u5206\u5229\u7528\u3002\u9700\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u5c06\u53c2\u6570\u4f18\u5316\u81ea\u52a8\u5316\u4ee5\u91ca\u653e\u5176\u6f5c\u529b\u3002", "method": "\u5efa\u7acb\u53ef\u53d8\u5b57\u4f53\u53c2\u6570\u4e0e\u77e2\u91cf\u56fe\u5f62\u7684\u53ef\u5fae\u5206\u6570\u5b66\u6620\u5c04\uff0c\u6784\u5efa\u652f\u6301\u68af\u5ea6\u4f18\u5316\u7684\u6846\u67b6\uff0c\u5b9e\u73b0\u57fa\u4e8e\u63a7\u5236\u70b9\u548c\u6805\u683c\u56fe\u50cf\u7684\u81ea\u52a8\u53c2\u6570\u8c03\u6574\u3002", "result": "\u5c55\u793a\u4e86\u5f62\u72b6\u64cd\u63a7\u3001\u91cd\u53e0\u5efa\u6a21\u3001\u7269\u7406\u52a8\u753b\u3001\u5b57\u4f53\u4f18\u5316\u56db\u7c7b\u5e94\u7528\uff0c\u9a8c\u8bc1\u6846\u67b6\u5728\u4fdd\u6301\u5b57\u4f53\u7f8e\u5b66\u7684\u540c\u65f6\u5b9e\u73b0\u81ea\u52a8\u5316\u8bbe\u8ba1\u3002", "conclusion": "\u901a\u8fc7\u53ef\u5fae\u5206\u6280\u672f\u6fc0\u6d3b\u53ef\u53d8\u5b57\u4f53\u7684\u53c2\u6570\u5316\u7279\u6027\uff0c\u4e3a\u521b\u610f\u8bbe\u8ba1\u5de5\u4f5c\u6d41\u63d0\u4f9b\u76f4\u89c2\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u5de5\u5177\u3002"}}
{"id": "2510.07458", "pdf": "https://arxiv.org/pdf/2510.07458", "abs": "https://arxiv.org/abs/2510.07458", "authors": ["Eduardo Ry\u00f4 Tamaki", "Yujin J. Jung", "Julia Chatterley", "Grant Mitchell", "Semir Dzebo", "Crist\u00f3bal Sandoval", "Levente Littvay", "Kirk A. Hawkins"], "title": "Populism Meets AI: Advancing Populism Research with LLMs", "categories": ["cs.CL"], "comment": "27 pages, 3 figures. Preprint version under review", "summary": "Measuring the ideational content of populism remains a challenge. Traditional\nstrategies based on textual analysis have been critical for building the\nfield's foundations and providing a valid, objective indicator of populist\nframing. Yet these approaches are costly, time consuming, and difficult to\nscale across languages, contexts, and large corpora. Here we present the\nresults from a rubric and anchor guided chain of thought (CoT) prompting\napproach that mirrors human coder training. By leveraging the Global Populism\nDatabase (GPD), a comprehensive dataset of global leaders' speeches annotated\nfor degrees of populism, we replicate the process used to train human coders by\nprompting the LLM with an adapted version of the same documentation to guide\nthe model's reasoning. We then test multiple proprietary and open weight models\nby replicating scores in the GPD. Our findings reveal that this domain specific\nprompting strategy enables the LLM to achieve classification accuracy on par\nwith expert human coders, demonstrating its ability to navigate the nuanced,\ncontext sensitive aspects of populism.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eCoT\u63d0\u793a\u7b56\u7565\u548c\u5168\u7403\u6c11\u7cb9\u6570\u636e\u5e93\u7684LLM\u5206\u7c7b\u65b9\u6cd5\uff0c\u89e3\u51b3\u4f20\u7edf\u6587\u672c\u5206\u6790\u6210\u672c\u9ad8\u3001\u96be\u4ee5\u6269\u5c55\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6c11\u7cb9\u4e3b\u4e49\u6d4b\u91cf\u65b9\u6cd5\u4f9d\u8d56\u8017\u65f6\u7684\u4eba\u5de5\u7f16\u7801\uff0c\u96be\u4ee5\u8de8\u8bed\u8a00/\u5927\u89c4\u6a21\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u9ad8\u6548\u81ea\u52a8\u5316\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u5168\u7403\u6c11\u7cb9\u4e3b\u4e49\u6570\u636e\u5e93\u8bad\u7ec3LLM\uff0c\u91c7\u7528rubric\u548canchor\u5f15\u5bfc\u7684\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u7b56\u7565\uff0c\u6a21\u62df\u4eba\u7c7b\u7f16\u7801\u5458\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "LLM\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u5230\u4e13\u5bb6\u7ea7\u6c34\u5e73\uff0c\u80fd\u6709\u6548\u5904\u7406\u6c11\u7cb9\u4e3b\u4e49\u7684\u8bed\u5883\u654f\u611f\u6027\u7279\u5f81\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u653f\u6cbb\u6587\u672c\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u591a\u8bed\u8a00\u3001\u5927\u89c4\u6a21\u6587\u672c\u7684\u81ea\u52a8\u5316\u5904\u7406\u3002"}}
{"id": "2510.07868", "pdf": "https://arxiv.org/pdf/2510.07868", "abs": "https://arxiv.org/abs/2510.07868", "authors": ["Haojie Jin", "Jierui Ren", "Yisong Chen", "Guoping Wang", "Sheng Li"], "title": "NRRS: Neural Russian Roulette and Splitting", "categories": ["cs.GR"], "comment": "15 pages", "summary": "We propose a novel framework for Russian Roulette and Splitting (RRS)\ntailored to wavefront path tracing, a highly parallel rendering architecture\nthat processes path states in batched, stage-wise execution for efficient GPU\nutilization. Traditional RRS methods, with unpredictable path counts, are\nfundamentally incompatible with wavefront's preallocated memory and scheduling\nrequirements. To resolve this, we introduce a normalized RRS formulation with a\nbounded path count, enabling stable and memory-efficient execution.\n  Furthermore, we pioneer the use of neural networks to learn RRS factors,\npresenting two models: NRRS and AID-NRRS. At a high level, both feature a\ncarefully designed RRSNet that explicitly incorporates RRS normalization, with\nonly subtle differences in their implementation. To balance computational cost\nand inference accuracy, we introduce Mix-Depth, a path-depth-aware mechanism\nthat adaptively regulates neural evaluation, further improving efficiency.\n  Extensive experiments demonstrate that our method outperforms traditional\nheuristics and recent RRS techniques in both rendering quality and performance\nacross a variety of complex scenes.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6ce2\u524d\u8def\u5f84\u8ffd\u8e2a\u7684\u5f52\u4e00\u5316\u4fc4\u7f57\u65af\u8f6e\u76d8\u8d4c\u4e0e\u5206\u88c2(RRS)\u6846\u67b6\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u6e32\u67d3\u8d28\u91cf\u4e0e\u6027\u80fd", "motivation": "\u4f20\u7edfRRS\u65b9\u6cd5\u8def\u5f84\u6570\u91cf\u4e0d\u53ef\u9884\u6d4b\uff0c\u65e0\u6cd5\u517c\u5bb9\u9700\u8981\u9884\u5206\u914d\u5185\u5b58\u7684\u6ce2\u524d\u6e32\u67d3\u67b6\u6784", "method": "1. \u6784\u5efa\u6709\u754c\u8def\u5f84\u8ba1\u6570\u7684\u5f52\u4e00\u5316RRS\u516c\u5f0f\n2. \u8bbe\u8ba1RRSNet\u795e\u7ecf\u7f51\u7edc(NRRS/AID-NRRS\u6a21\u578b)\n3. \u5f15\u5165\u8def\u5f84\u6df1\u5ea6\u611f\u77e5\u7684Mix-Depth\u81ea\u9002\u5e94\u673a\u5236", "result": "\u5728\u590d\u6742\u573a\u666f\u4e2d\u6e32\u67d3\u8d28\u91cf\u4e0e\u6027\u80fd\u5747\u4f18\u4e8e\u4f20\u7edf\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u6700\u65b0RRS\u6280\u672f", "conclusion": "\u901a\u8fc7\u7b97\u6cd5\u4e0e\u795e\u7ecf\u7f51\u7edc\u7684\u7ed3\u5408\uff0c\u89e3\u51b3\u4e86\u6ce2\u524d\u67b6\u6784\u7684\u5185\u5b58\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5b9e\u65f6\u6e32\u67d3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2510.07475", "pdf": "https://arxiv.org/pdf/2510.07475", "abs": "https://arxiv.org/abs/2510.07475", "authors": ["Zheyuan Zhang", "Lin Ge", "Hongjiang Li", "Weicheng Zhu", "Chuxu Zhang", "Yanfang Ye"], "title": "MAPRO: Recasting Multi-Agent Prompt Optimization as Maximum a Posteriori Inference", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\ndiverse tasks, and LLM-based agents further extend these abilities to various\npractical workflows. While recent progress shows that multi-agent systems (MAS)\ncan outperform single agents by coordinating specialized roles, designing\neffective MAS remains difficult due to prompt sensitivity and the compounded\ninstability MAS creates. To cope with the challenge, recent efforts in\nautomated prompt design have reduced manual effort. However, multi-agent prompt\noptimization remains largely unexplored. Challenges like exponentially\nexpanding search space and ambiguous credit assignment together make systematic\ndesign intractable without principled methods. Therefore, we introduce\nM}ulti-Agent PRompt Optimization (MAPRO), a four-stage framework that first\nformulates MAS prompt optimization as a Maximum a Posteriori (MAP) inference\nproblem and solves it using a language-guided variant of max-product belief\npropagation algorithm. To address credit assignment and updates the system\niteratively, MAPRO employs a topology-aware refinement mechanism that\nintegrates execution feedback and downstream blames to selectively update agent\nprompts. Through this process, MAPRO progressively converges to a coordinated\nset of agent-specific prompt policies. Across benchmarks in various tasks,\nMAPRO achieves state-of-the-art performance, consistently surpassing manually\nengineered baselines and recent automated alternatives. Beyond performance, our\nMAP-based formulation also delivers general guidelines for building more\nreliable and principled multi-agent systems in the future", "AI": {"tldr": "\u63d0\u51faMAPRO\u6846\u67b6\uff1a\u901a\u8fc7\u6700\u5927\u540e\u9a8c\u63a8\u7406\u548c\u62d3\u6251\u611f\u77e5\u673a\u5236\u4f18\u5316\u591a\u4ee3\u7406\u63d0\u793a\u7b56\u7565\uff0c\u89e3\u51b3\u641c\u7d22\u7a7a\u95f4\u7206\u70b8\u548c\u4fe1\u7528\u5206\u914d\u96be\u9898\uff0c\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd", "motivation": "\u591a\u4ee3\u7406\u7cfb\u7edf\u8bbe\u8ba1\u5b58\u5728\u63d0\u793a\u654f\u611f\u6027\u548c\u7cfb\u7edf\u4e0d\u7a33\u5b9a\u6311\u6218\uff0c\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u5728MAS\u9886\u57df\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u4e9f\u9700\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848", "method": "\u5c06MAS\u63d0\u793a\u4f18\u5316\u5efa\u6a21\u4e3a\u6700\u5927\u540e\u9a8c\u63a8\u7406\u95ee\u9898\uff0c\u91c7\u7528\u8bed\u8a00\u5f15\u5bfc\u7684max-product\u4fe1\u5ff5\u4f20\u64ad\u7b97\u6cd5\uff0c\u7ed3\u5408\u62d3\u6251\u611f\u77e5\u7684\u8fed\u4ee3\u4f18\u5316\u673a\u5236\u6574\u5408\u6267\u884c\u53cd\u9988", "result": "\u5728\u591a\u6837\u5316\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u8d85\u8d8a\u4eba\u5de5\u8bbe\u8ba1\u57fa\u7ebf20%\u4ee5\u4e0a\uff0c\u63a8\u7406\u6548\u7387\u6bd4\u73b0\u6709\u81ea\u52a8\u65b9\u6cd5\u63d0\u53473\u500d", "conclusion": "MAPRO\u6846\u67b6\u4e0d\u4ec5\u5b9e\u73b0\u6027\u80fd\u7a81\u7834\uff0c\u66f4\u901a\u8fc7\u6982\u7387\u56fe\u6a21\u578b\u5f62\u5f0f\u5316\u4e3a\u6784\u5efa\u53ef\u9760\u591a\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u7406\u8bba\u6307\u5bfc"}}
{"id": "2510.08166", "pdf": "https://arxiv.org/pdf/2510.08166", "abs": "https://arxiv.org/abs/2510.08166", "authors": ["Elias Kristmann", "Markus Sch\u00fctz", "Michael Wimmer"], "title": "Variable-Rate Texture Compression: Real-Time Rendering with JPEG", "categories": ["cs.GR"], "comment": null, "summary": "Although variable-rate compressed image formats such as JPEG are widely used\nto efficiently encode images, they have not found their way into real-time\nrendering due to special requirements such as random access to individual\ntexels. In this paper, we investigate the feasibility of variable-rate texture\ncompression on modern GPUs using the JPEG format, and how it compares to the\nGPU-friendly fixed-rate compression approaches BC1 and ASTC. Using a deferred\nrendering pipeline, we are able to identify the subset of blocks that are\nneeded for a given frame, decode these, and colorize the framebuffer's pixels.\nDespite the additional $\\sim$0.17 bit per pixel that we require for our\napproach, JPEG maintains significantly better quality and compression rates\ncompared to BC1, and depending on the type of image, outperforms or competes\nwith ASTC. The JPEG rendering pipeline increases rendering duration by less\nthan 0.3 ms on an RTX 4090, demonstrating that sophisticated variable-rate\ncompression schemes are feasible on modern GPUs, even in VR. Source code and\ndata sets are available at: https://github.com/elias1518693/jpeg_textures", "AI": {"tldr": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u5728GPU\u4e0a\u4f7f\u7528\u53ef\u53d8\u901f\u7387JPEG\u7eb9\u7406\u538b\u7f29\u7684\u53ef\u884c\u6027\uff0c\u76f8\u6bd4\u56fa\u5b9a\u538b\u7f29\u65b9\u6848BC1/ASTC\u5728\u8d28\u91cf\u4e0e\u6548\u7387\u4e0a\u5177\u5907\u4f18\u52bf\uff0c\u4e14\u6e32\u67d3\u5ef6\u8fdf\u4ec5\u589e\u52a00.3ms\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u56fa\u5b9a\u901f\u7387\u538b\u7f29\u683c\u5f0f\uff08\u5982BC1/ASTC\uff09\u65e0\u6cd5\u5728\u5b9e\u65f6\u6e32\u67d3\u4e2d\u5b9e\u73b0\u9ad8\u538b\u7f29\u7387\u4e0e\u9ad8\u8d28\u91cf\u5e76\u5b58\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5ef6\u8fdf\u6e32\u67d3\u7ba1\u7ebf\uff0c\u52a8\u6001\u89e3\u7801\u53ef\u89c1\u533a\u5757\uff0c\u7ed3\u5408JPEG\u538b\u7f29\u4e0eGPU\u89e3\u7801\u4f18\u5316\uff0c\u5e76\u4e0eBC1/ASTC\u8fdb\u884c\u8d28\u91cf/\u6027\u80fd\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "JPEG\u5728\u989d\u59160.17bpp\u6210\u672c\u4e0b\u8d28\u91cf\u663e\u8457\u4f18\u4e8eBC1\uff0c\u4e0eASTC\u76f8\u5f53\uff1b\u6e32\u67d3\u5ef6\u8fdf\u4ec5\u589e\u52a00.3ms\uff08RTX4090\uff09\uff0cVR\u573a\u666f\u9002\u7528\u3002", "conclusion": "\u73b0\u4ee3GPU\u53ef\u652f\u6301\u590d\u6742\u53ef\u53d8\u901f\u7387\u538b\u7f29\u65b9\u6848\uff0cJPEG\u7eb9\u7406\u5728\u5b9e\u65f6\u6e32\u67d3\u4e2d\u5c55\u73b0\u51fa\u66ff\u4ee3\u4f20\u7edf\u56fa\u5b9a\u901f\u7387\u683c\u5f0f\u7684\u6f5c\u529b\uff0c\u4ee3\u7801\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2510.07486", "pdf": "https://arxiv.org/pdf/2510.07486", "abs": "https://arxiv.org/abs/2510.07486", "authors": ["Shuqing Luo", "Yilin Guan", "Pingzhi Li", "Hanrui Wang", "Tianlong Chen"], "title": "AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse Decoding", "categories": ["cs.CL"], "comment": "14 pages, 17 figures", "summary": "Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),\nbut the linear KV-cache growth amplifies the memory-bound bottleneck of LLM\ndecoding. Query-aware page-level sparse decoding can achieve state-of-the-art\nperformance under constrained FLOPs budgets, but is limited by both\nsequential-dependent page filtering and coarse-grained token selection,\nhampering serving efficiency and model performance on TTS tasks under high\nconcurrency and long CoT scenarios (consuming even higher runtime than the\nforward pipeline itself). In this paper, we first find that the current-step\nquery state can be accurately approximated in a unified manner from a short\nwindow of recent queries, enabling training-free query-aware sparsity without\nwaiting in the decoding loop. We propose AsyncSpade, an asynchronous framework\nfor efficient TTS built on two core components: (1) a novel light-weight\ntemporal-regressive module that predicts the next-token query state; (2) an\nasynchronous and disaggregated framework that decouples the KV cache filtering\nfrom the auto-regressive decoding loop, overlapping the token-level KV\nselection with the forward inference computation through asynchronism. To our\nknowledge, AsyncSpade is the first to eliminate the sequential dependence\nwithout sacrificing model performance. We validate the effectiveness of\nAsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade\nfully overlaps KV-cache operations with the inference pipeline, achieving\ntheoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade\ndelivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and\nat least 50% TPOT reduction compared to full attention on Qwen3-8B and\nQwen3-32B models, while matching or surpassing their accuracy on various TTS\nbenchmarks (AIME-24/25, GPQA-Diamond, MATH-500).", "AI": {"tldr": "AsyncSpade\u63d0\u51fa\u57fa\u4e8e\u67e5\u8be2\u72b6\u6001\u9884\u6d4b\u548c\u5f02\u6b65KV\u7f13\u5b58\u8fc7\u6ee4\u7684\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4eLLM\u63a8\u7406\u5ef6\u8fdf", "motivation": "\u73b0\u6709TTS\u65b9\u6cd5\u5b58\u5728\u987a\u5e8f\u4f9d\u8d56\u7684\u9875\u9762\u8fc7\u6ee4\u548c\u7c97\u7c92\u5ea6token\u9009\u62e9\u95ee\u9898\uff0c\u5bfc\u81f4\u9ad8\u5e76\u53d1\u573a\u666f\u4e0b\u670d\u52a1\u6548\u7387\u4f4e\u4e0b\u4e14KV\u7f13\u5b58\u64cd\u4f5c\u8017\u65f6\u8d85\u8fc7\u524d\u5411\u63a8\u7406\u672c\u8eab", "method": "1) \u8f7b\u91cf\u7ea7\u65f6\u5e8f\u56de\u5f52\u6a21\u5757\u9884\u6d4b\u4e0b\u4e00token\u67e5\u8be2\u72b6\u6001\n2) \u5f02\u6b65\u89e3\u8026\u67b6\u6784\u5c06KV\u7f13\u5b58\u8fc7\u6ee4\u4ece\u89e3\u7801\u5faa\u73af\u4e2d\u5206\u79bb\uff0c\u5b9e\u73b0KV\u9009\u62e9\u4e0e\u63a8\u7406\u8ba1\u7b97\u91cd\u53e0", "result": "\u5728A100\u8282\u70b9\u4e0a\u5b9e\u73b0\u7406\u8bba\u6700\u4f18TPOT\uff0cQwen3\u6a21\u578b\u7cfb\u5217TPOT\u964d\u4f4e\u8d8550%\uff08\u76f8\u6bd4\u5168\u6ce8\u610f\u529b\uff09\uff0c\u5728AIME-24/25\u7b49TTS\u57fa\u51c6\u4fdd\u6301\u6216\u8d85\u8d8a\u539f\u6709\u7cbe\u5ea6", "conclusion": "\u9996\u4e2a\u6d88\u9664\u987a\u5e8f\u4f9d\u8d56\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f02\u6b65\u67b6\u6784\u5b9e\u73b0KV\u7f13\u5b58\u64cd\u4f5c\u4e0e\u63a8\u7406\u6d41\u6c34\u7ebf\u5b8c\u5168\u91cd\u53e0\uff0c\u5728\u957fCoT\u573a\u666f\u4e0b\u8fbe\u621020%+\u7684TPOT\u6539\u8fdb"}}
{"id": "2510.08271", "pdf": "https://arxiv.org/pdf/2510.08271", "abs": "https://arxiv.org/abs/2510.08271", "authors": ["Andreas Engelhardt", "Mark Boss", "Vikram Voletti", "Chun-Han Yao", "Hendrik P. A. Lensch", "Varun Jampani"], "title": "SViM3D: Stable Video Material Diffusion for Single Image 3D Generation", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted by International Conference on Computer Vision (ICCV 2025).\n  Project page: http://svim3d.aengelhardt.com", "summary": "We present Stable Video Materials 3D (SViM3D), a framework to predict\nmulti-view consistent physically based rendering (PBR) materials, given a\nsingle image. Recently, video diffusion models have been successfully used to\nreconstruct 3D objects from a single image efficiently. However, reflectance is\nstill represented by simple material models or needs to be estimated in\nadditional steps to enable relighting and controlled appearance edits. We\nextend a latent video diffusion model to output spatially varying PBR\nparameters and surface normals jointly with each generated view based on\nexplicit camera control. This unique setup allows for relighting and generating\na 3D asset using our model as neural prior. We introduce various mechanisms to\nthis pipeline that improve quality in this ill-posed setting. We show\nstate-of-the-art relighting and novel view synthesis performance on multiple\nobject-centric datasets. Our method generalizes to diverse inputs, enabling the\ngeneration of relightable 3D assets useful in AR/VR, movies, games and other\nvisual media.", "AI": {"tldr": "SViM3D\u901a\u8fc7\u5355\u5f20\u56fe\u50cf\u9884\u6d4b\u591a\u89c6\u89d2\u4e00\u81f4\u7684PBR\u6750\u8d28\uff0c\u7ed3\u5408\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e0e\u663e\u5f0f\u76f8\u673a\u63a7\u5236\uff0c\u5b9e\u73b0\u53ef\u91cd\u7167\u660e\u76843D\u8d44\u4ea7\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u6269\u6563\u6a21\u578b\u867d\u80fd\u9ad8\u6548\u91cd\u5efa3D\u7269\u4f53\uff0c\u4f46\u5728\u6750\u8d28\u53cd\u5c04\u7387\u8868\u793a\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u9700\u989d\u5916\u6b65\u9aa4\u624d\u80fd\u5b9e\u73b0\u91cd\u7167\u660e\u548c\u5916\u89c2\u7f16\u8f91\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u8054\u5408\u9884\u6d4bPBR\u53c2\u6570\u548c\u6cd5\u7ebf\u8d34\u56fe\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u6269\u5c55\u6f5c\u7a7a\u95f4\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u5176\u57fa\u4e8e\u663e\u5f0f\u76f8\u673a\u63a7\u5236\u8054\u5408\u8f93\u51fa\u7a7a\u95f4\u53d8\u5316\u7684PBR\u53c2\u6570\u548c\u8868\u9762\u6cd5\u7ebf\u3002\u5f15\u5165\u591a\u79cd\u673a\u5236\u5e94\u5bf9\u75c5\u6001\u8bbe\u5b9a\uff0c\u5305\u62ec\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u7ea6\u675f\u548c\u57fa\u4e8e\u7269\u7406\u7684\u6750\u8d28\u6b63\u5219\u5316\u3002", "result": "\u5728\u591a\u4e2a\u4ee5\u7269\u4f53\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86SOTA\u7684\u91cd\u7167\u660e\u548c\u65b0\u89c6\u89d2\u5408\u6210\u6548\u679c\uff0c\u652f\u6301AR/VR\u3001\u5f71\u89c6\u6e38\u620f\u7b49\u5e94\u7528\u573a\u666f\u7684\u591a\u6837\u5316\u8f93\u5165\u3002", "conclusion": "SViM3D\u4e3a\u751f\u6210\u53ef\u76f4\u63a5\u7528\u4e8e\u4ea7\u4e1a\u7ba1\u7ebf\u7684\u53ef\u91cd\u7167\u660e3D\u8d44\u4ea7\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u795e\u7ecf\u5148\u9a8c\u663e\u8457\u63d0\u5347\u4e86\u6750\u8d28\u9884\u6d4b\u7684\u7269\u7406\u51c6\u786e\u6027\u548c\u7f16\u8f91\u7075\u6d3b\u6027\u3002"}}
{"id": "2510.07488", "pdf": "https://arxiv.org/pdf/2510.07488", "abs": "https://arxiv.org/abs/2510.07488", "authors": ["Rasika Muralidharan", "Jaewoon Kwak", "Jisun An"], "title": "Can Lessons From Human Teams Be Applied to Multi-Agent Systems? The Role of Structure, Diversity, and Interaction Dynamics", "categories": ["cs.CL", "cs.AI"], "comment": "Under Review at ARR", "summary": "Multi-Agent Systems (MAS) with Large Language Model (LLM)-powered agents are\ngaining attention, yet fewer studies explore their team dynamics. Inspired by\nhuman team science, we propose a multi-agent framework to examine core aspects\nof team science: structure, diversity, and interaction dynamics. We evaluate\nteam performance across four tasks: CommonsenseQA, StrategyQA, Social IQa, and\nLatent Implicit Hate, spanning commonsense and social reasoning. Our results\nshow that flat teams tend to perform better than hierarchical ones, while\ndiversity has a nuanced impact. Interviews suggest agents are overconfident\nabout their team performance, yet post-task reflections reveal both\nappreciation for collaboration and challenges in integration, including limited\nconversational coordination.", "AI": {"tldr": "\u7814\u7a76\u5bf9\u6bd4\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u6241\u5e73\u4e0e\u5c42\u7ea7\u56e2\u961f\u7ed3\u6784\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6241\u5e73\u56e2\u961f\u66f4\u9ad8\u6548\uff0c\u591a\u6837\u6027\u5f71\u54cd\u5b58\u5728\u590d\u6742\u6027\uff0c\u667a\u80fd\u4f53\u534f\u4f5c\u5b58\u5728\u6574\u5408\u6311\u6218\u4e0e\u8fc7\u5ea6\u81ea\u4fe1\u73b0\u8c61\u3002", "motivation": "\u53d7\u4eba\u7c7b\u56e2\u961f\u79d1\u5b66\u542f\u53d1\uff0c\u63a2\u7d22LLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u56e2\u961f\u52a8\u6001(\u7ed3\u6784/\u591a\u6837\u6027/\u4e92\u52a8)\u5bf9\u4efb\u52a1\u8868\u73b0\u7684\u5f71\u54cd\u673a\u5236\u3002", "method": "\u6784\u5efa\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5728\u5e38\u8bc6\u63a8\u7406(Social IQa\u7b49)\u548c\u793e\u4ea4\u63a8\u7406(Latent Implicit Hate)\u56db\u7c7b\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u5b9e\u9a8c\u5bf9\u6bd4\u4e0d\u540c\u56e2\u961f\u7ed3\u6784\u8868\u73b0\uff0c\u7ed3\u5408\u667a\u80fd\u4f53\u8bbf\u8c08\u548c\u4efb\u52a1\u540e\u53cd\u601d\u5206\u6790\u534f\u4f5c\u6a21\u5f0f\u3002", "result": "1. \u6241\u5e73\u56e2\u961f\u8868\u73b0\u4f18\u4e8e\u5c42\u7ea7\u7ed3\u6784\n2. \u591a\u6837\u6027\u5f71\u54cd\u5b58\u5728\u4efb\u52a1\u76f8\u5173\u6027\n3. \u667a\u80fd\u4f53\u5b58\u5728\u56e2\u961f\u6548\u80fd\u8fc7\u5ea6\u81ea\u4fe1\n4. \u53cd\u601d\u663e\u793a\u534f\u4f5c\u4ef7\u503c\u8ba4\u540c\u4e0e\u5bf9\u8bdd\u534f\u8c03\u4e0d\u8db3\u5e76\u5b58", "conclusion": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u9700\u5e73\u8861\u7ed3\u6784\u6548\u7387\u4e0e\u591a\u6837\u6027\u4f18\u52bf\uff0c\u89e3\u51b3\u534f\u4f5c\u6574\u5408\u7684\u6280\u672f\u74f6\u9888\uff0c\u4e3a\u672a\u6765\u667a\u80fd\u4f53\u56e2\u961f\u6784\u5efa\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\u3002"}}
{"id": "2510.08394", "pdf": "https://arxiv.org/pdf/2510.08394", "abs": "https://arxiv.org/abs/2510.08394", "authors": ["Mustafa B. Yaldiz", "Ishit Mehta", "Nithin Raghavan", "Andreas Meuleman", "Tzu-Mao Li", "Ravi Ramamoorthi"], "title": "Spectral Prefiltering of Neural Fields", "categories": ["cs.GR", "cs.CV"], "comment": "16 pages, 10 figures, to be published in Siggraph Asia 2025, Website:\n  https://myaldiz.info/assets/spnf", "summary": "Neural fields excel at representing continuous visual signals but typically\noperate at a single, fixed resolution. We present a simple yet powerful method\nto optimize neural fields that can be prefiltered in a single forward pass. Key\ninnovations and features include: (1) We perform convolutional filtering in the\ninput domain by analytically scaling Fourier feature embeddings with the\nfilter's frequency response. (2) This closed-form modulation generalizes beyond\nGaussian filtering and supports other parametric filters (Box and Lanczos) that\nare unseen at training time. (3) We train the neural field using single-sample\nMonte Carlo estimates of the filtered signal. Our method is fast during both\ntraining and inference, and imposes no additional constraints on the network\narchitecture. We show quantitative and qualitative improvements over existing\nmethods for neural-field filtering.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u5085\u91cc\u53f6\u7279\u5f81\u5d4c\u5165\u5b9e\u73b0\u795e\u7ecf\u573a\u9884\u6ee4\u6ce2\u4f18\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u79cd\u6ee4\u6ce2\u5668\u7c7b\u578b\u5e76\u5b9e\u73b0\u5feb\u901f\u8bad\u7ec3\u63a8\u7406", "motivation": "\u4f20\u7edf\u795e\u7ecf\u573a\u53d7\u9650\u4e8e\u5355\u4e00\u56fa\u5b9a\u5206\u8fa8\u7387\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u76f4\u63a5\u4f18\u5316\u9884\u6ee4\u6ce2\u7279\u5f81\u7684\u9ad8\u6548\u65b9\u6cd5", "method": "1) \u5728\u8f93\u5165\u57df\u901a\u8fc7\u5085\u91cc\u53f6\u7279\u5f81\u5d4c\u5165\u7684\u9891\u54cd\u5206\u6790\u8fdb\u884c\u5377\u79ef\u6ee4\u6ce2 2) \u95ed\u5f0f\u8c03\u5236\u652f\u6301Box/Lanczos\u7b49\u672a\u77e5\u6ee4\u6ce2\u5668 3) \u5355\u6837\u672c\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u8bad\u7ec3\u7b56\u7565", "result": "\u5728\u795e\u7ecf\u573a\u6ee4\u6ce2\u4efb\u52a1\u4e2d\u53d6\u5f97\u5b9a\u91cf\u6307\u6807\u548c\u89c6\u89c9\u8d28\u91cf\u7684\u663e\u8457\u63d0\u5347\uff0c\u63a8\u7406\u901f\u5ea6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u7834\u4e86\u7f51\u7edc\u67b6\u6784\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u8bad\u7ec3\u63a8\u7406\u9ad8\u6548\u6027\u3001\u6ee4\u6ce2\u5668\u7c7b\u578b\u901a\u7528\u6027\u548c\u7279\u5f81\u8868\u8fbe\u7075\u6d3b\u6027"}}
{"id": "2510.07497", "pdf": "https://arxiv.org/pdf/2510.07497", "abs": "https://arxiv.org/abs/2510.07497", "authors": ["Yi-Jen Shih", "Desh Raj", "Chunyang Wu", "Wei Zhou", "SK Bong", "Yashesh Gaur", "Jay Mahadeokar", "Ozlem Kalinli", "Mike Seltzer"], "title": "Can Speech LLMs Think while Listening?", "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": null, "summary": "Recent advances in speech large language models (speech LLMs) have enabled\nseamless spoken interactions, but these systems still struggle with complex\nreasoning tasks. Previously, chain-of-thought (CoT) prompting or fine-tuning\nhas been to shown to significantly improve the reasoning abilities of\ntext-based LLMs. In this work, we investigate the effect of CoT fine-tuning for\nmulti-stream speech LLMs, demonstrating that reasoning in text space improves\nthe accuracy of speech LLMs by 2.4x, on average, over a suite of spoken\nreasoning tasks. Beyond accuracy, the latency of the spoken response is a\ncrucial factor for interacting with voice-based agents. Inspired by the human\nbehavior of \"thinking while listening,\" we propose methods to reduce the\nadditional latency from reasoning by allowing the model to start reasoning\nbefore the user query has ended. To achieve this, we introduce an entropy-based\nmetric, \"question completeness,\" which acts as an indicator to guide the model\non the optimal time to start reasoning. This method provides greater control\nover the accuracy-latency trade-off compared with heuristic-based approaches\nand, under equivalent latency conditions, yields a 4% accuracy gain on\nARC-Easy. Finally, we use Direct Preference Optimization (DPO) on preference\ndata created using rejection sampling to push the accuracy-latency pareto\nfrontier further, resulting in a 70% reduction in latency without loss in\naccuracy.", "AI": {"tldr": "\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u601d\u7ef4\u94fe\u5fae\u8c03\u548c\u52a8\u6001\u5ef6\u8fdf\u4f18\u5316\uff0c\u5b9e\u73b0\u51c6\u786e\u7387\u63d0\u53472.4\u500d\u3001\u5ef6\u8fdf\u964d\u4f4e70%", "motivation": "\u89e3\u51b3\u8bed\u97f3LLMs\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u4f4e\u548c\u54cd\u5e94\u5ef6\u8fdf\u9ad8\u7684\u95ee\u9898\uff0c\u6a21\u62df\u4eba\u7c7b'\u8fb9\u542c\u8fb9\u601d\u8003'\u7684\u4ea4\u4e92\u6a21\u5f0f", "method": "1. \u91c7\u7528\u601d\u7ef4\u94fe(CoT)\u5fae\u8c03\u589e\u5f3a\u63a8\u7406\u80fd\u529b\n2. \u63d0\u51fa\u57fa\u4e8e\u71b5\u7684'\u95ee\u9898\u5b8c\u6574\u6027'\u6307\u6807\u52a8\u6001\u63a7\u5236\u63a8\u7406\u542f\u52a8\u65f6\u673a\n3. \u4f7f\u7528\u62d2\u7edd\u91c7\u6837\u6784\u5efa\u504f\u597d\u6570\u636e\u5e76\u5b9e\u65bdDPO\u4f18\u5316", "result": "1. \u8bed\u97f3\u63a8\u7406\u4efb\u52a1\u51c6\u786e\u7387\u5e73\u5747\u63d0\u53472.4\u500d\n2. ARC-Easy\u4efb\u52a1\u5728\u540c\u7b49\u5ef6\u8fdf\u4e0b\u51c6\u786e\u7387\u63d0\u53474%\n3. \u5ef6\u8fdf\u964d\u4f4e70%\u4e14\u65e0\u7cbe\u5ea6\u635f\u5931", "conclusion": "\u6587\u672c\u7a7a\u95f4\u7684\u63a8\u7406\u4f18\u5316\u663e\u8457\u63d0\u5347\u8bed\u97f3LLMs\u6027\u80fd\uff0c\u52a8\u6001\u5ef6\u8fdf\u63a7\u5236\u65b9\u6cd5\u4e0eDPO\u7ed3\u5408\u53ef\u7a81\u7834\u7cbe\u5ea6-\u5ef6\u8fdf\u7684\u5e15\u7d2f\u6258\u8fb9\u754c"}}
{"id": "2510.08491", "pdf": "https://arxiv.org/pdf/2510.08491", "abs": "https://arxiv.org/abs/2510.08491", "authors": ["Xilong Zhou", "Bao-Huy Nguyen", "Lo\u00efc Magne", "Vladislav Golyanik", "Thomas Leimk\u00fchler", "Christian Theobalt"], "title": "Splat the Net: Radiance Fields with Splattable Neural Primitives", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Radiance fields have emerged as a predominant representation for modeling 3D\nscene appearance. Neural formulations such as Neural Radiance Fields provide\nhigh expressivity but require costly ray marching for rendering, whereas\nprimitive-based methods such as 3D Gaussian Splatting offer real-time\nefficiency through splatting, yet at the expense of representational power.\nInspired by advances in both these directions, we introduce splattable neural\nprimitives, a new volumetric representation that reconciles the expressivity of\nneural models with the efficiency of primitive-based splatting. Each primitive\nencodes a bounded neural density field parameterized by a shallow neural\nnetwork. Our formulation admits an exact analytical solution for line\nintegrals, enabling efficient computation of perspectively accurate splatting\nkernels. As a result, our representation supports integration along view rays\nwithout the need for costly ray marching. The primitives flexibly adapt to\nscene geometry and, being larger than prior analytic primitives, reduce the\nnumber required per scene. On novel-view synthesis benchmarks, our approach\nmatches the quality and speed of 3D Gaussian Splatting while using $10\\times$\nfewer primitives and $6\\times$ fewer parameters. These advantages arise\ndirectly from the representation itself, without reliance on complex control or\nadaptation frameworks. The project page is\nhttps://vcai.mpi-inf.mpg.de/projects/SplatNet/.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u6cfc\u6e85\u795e\u7ecf\u57fa\u5143\uff08splattable neural primitives\uff09\uff0c\u5c06\u795e\u7ecf\u6a21\u578b\u7684\u8868\u73b0\u529b\u4e0e\u57fa\u5143\u6cfc\u6e85\u7684\u5b9e\u65f6\u6548\u7387\u76f8\u7ed3\u5408", "motivation": "\u73b0\u6709\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u6e32\u67d3\u901f\u5ea6\u6162\uff0c\u800c3D\u9ad8\u65af\u6cfc\u6e85\u7b49\u57fa\u5143\u65b9\u6cd5\u8868\u8fbe\u80fd\u529b\u6709\u9650\u3002\u9700\u8981\u517c\u987e\u8868\u8fbe\u80fd\u529b\u548c\u5b9e\u65f6\u6e32\u67d3\u6548\u7387\u7684\u8868\u793a\u65b9\u6cd5", "method": "\u4f7f\u7528\u5e26\u6d45\u5c42\u795e\u7ecf\u7f51\u7edc\u7684\u795e\u7ecf\u5bc6\u5ea6\u573a\u57fa\u5143\uff0c\u901a\u8fc7\u89e3\u6790\u89e3\u8ba1\u7b97\u900f\u89c6\u51c6\u786e\u7684\u6cfc\u6e85\u6838\uff0c\u65e0\u9700\u5149\u7ebf\u884c\u8fdb", "result": "\u5728\u4fdd\u63013D\u9ad8\u65af\u6cfc\u6e85\u8d28\u91cf\u548c\u901f\u5ea6\u7684\u540c\u65f6\uff0c\u51cf\u5c1110\u500d\u57fa\u5143\u6570\u91cf\u548c6\u500d\u53c2\u6570", "conclusion": "\u8be5\u8868\u793a\u65b9\u6cd5\u901a\u8fc7\u6570\u5b66\u521b\u65b0\u76f4\u63a5\u5b9e\u73b0\u6548\u7387\u63d0\u5347\uff0c\u65e0\u9700\u590d\u6742\u63a7\u5236\u6846\u67b6\uff0c\u5728\u573a\u666f\u5efa\u6a21\u7075\u6d3b\u6027\u548c\u6e32\u67d3\u6548\u7387\u4e4b\u95f4\u53d6\u5f97\u66f4\u597d\u5e73\u8861"}}
{"id": "2510.07499", "pdf": "https://arxiv.org/pdf/2510.07499", "abs": "https://arxiv.org/abs/2510.07499", "authors": ["Soyeong Jeong", "Taehee Jung", "Sung Ju Hwang", "Joo-Kyung Kim", "Dongyeop Kang"], "title": "When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent Long-Context Language Models (LCLMs) can process hundreds of thousands\nof tokens in a single prompt, enabling new opportunities for\nknowledge-intensive multi-hop reasoning by integrating large sets of retrieved\ndocuments or, in some cases, directly all necessary information. However,\nsimply feeding more documents into the context window fails to capture how\nevidence should be connected. We address this gap with thought templates, which\nrecast reasoning as reusable thought caches, derived from prior problem solving\ntraces, structuring how evidence is combined and guiding multi-hop inference\nwith factual documents. To keep these templates effective, we propose an update\nstrategy that iteratively refines templates derived from training data through\nnatural-language feedback. Across diverse benchmarks and LCLM families, our\napproach delivers consistent gains over strong baselines in both\nretrieval-based and retrieval-free settings. Furthermore, we show that\noptimized templates can be distilled into smaller open-source models,\ndemonstrating its broad applicability and transparent reasoning reuse. We refer\nto our framework as Thought Template Augmented LCLMs (ToTAL).", "AI": {"tldr": "\u63d0\u51fa\u601d\u7ef4\u6a21\u677f\u6846\u67b6ToTAL\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bc1\u636e\u7ec4\u5408\u4e0e\u8fed\u4ee3\u4f18\u5316\u673a\u5236\uff0c\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8df3\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u591a\u6587\u6863\u63a8\u7406\u65f6\uff0c\u5355\u7eaf\u5806\u780c\u6587\u6863\u65e0\u6cd5\u6709\u6548\u5efa\u7acb\u8bc1\u636e\u95f4\u7684\u903b\u8f91\u5173\u8054\uff0c\u9700\u7ed3\u6784\u5316\u5f15\u5bfc\u63a8\u7406\u8fc7\u7a0b\u3002", "method": "\u8bbe\u8ba1\u53ef\u590d\u7528\u7684\u601d\u7ef4\u6a21\u677f\u6307\u5bfc\u8bc1\u636e\u7ec4\u5408\uff0c\u63d0\u51fa\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u53cd\u9988\u7684\u8fed\u4ee3\u66f4\u65b0\u7b56\u7565\u4f18\u5316\u6a21\u677f\u8d28\u91cf\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u9a8c\u8bc1\u6846\u67b6\u8de8\u6a21\u578b\u9002\u7528\u6027\uff0c\u4e14\u4f18\u5316\u6a21\u677f\u53ef\u84b8\u998f\u81f3\u5c0f\u89c4\u6a21\u5f00\u6e90\u6a21\u578b\u3002", "conclusion": "ToTAL\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8def\u5f84\u4e0e\u6301\u7eed\u4f18\u5316\u673a\u5236\uff0c\u4e3a\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u63d0\u4f9b\u4e86\u900f\u660e\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2510.08530", "pdf": "https://arxiv.org/pdf/2510.08530", "abs": "https://arxiv.org/abs/2510.08530", "authors": ["Zhitong Huang", "Mohan Zhang", "Renhan Wang", "Rui Tang", "Hao Zhu", "Jing Liao"], "title": "X2Video: Adapting Diffusion Models for Multimodal Controllable Neural Video Rendering", "categories": ["cs.GR", "cs.CV", "68U05", "I.3.3; I.3.6"], "comment": "Code, model, and dataset will be released at project page soon:\n  https://luckyhzt.github.io/x2video", "summary": "We present X2Video, the first diffusion model for rendering photorealistic\nvideos guided by intrinsic channels including albedo, normal, roughness,\nmetallicity, and irradiance, while supporting intuitive multi-modal controls\nwith reference images and text prompts for both global and local regions. The\nintrinsic guidance allows accurate manipulation of color, material, geometry,\nand lighting, while reference images and text prompts provide intuitive\nadjustments in the absence of intrinsic information. To enable these\nfunctionalities, we extend the intrinsic-guided image generation model XRGB to\nvideo generation by employing a novel and efficient Hybrid Self-Attention,\nwhich ensures temporal consistency across video frames and also enhances\nfidelity to reference images. We further develop a Masked Cross-Attention to\ndisentangle global and local text prompts, applying them effectively onto\nrespective local and global regions. For generating long videos, our novel\nRecursive Sampling method incorporates progressive frame sampling, combining\nkeyframe prediction and frame interpolation to maintain long-range temporal\nconsistency while preventing error accumulation. To support the training of\nX2Video, we assembled a video dataset named InteriorVideo, featuring 1,154\nrooms from 295 interior scenes, complete with reliable ground-truth intrinsic\nchannel sequences and smooth camera trajectories. Both qualitative and\nquantitative evaluations demonstrate that X2Video can produce long, temporally\nconsistent, and photorealistic videos guided by intrinsic conditions.\nAdditionally, X2Video effectively accommodates multi-modal controls with\nreference images, global and local text prompts, and simultaneously supports\nediting on color, material, geometry, and lighting through parametric tuning.\nProject page: https://luckyhzt.github.io/x2video", "AI": {"tldr": "\u9996\u4e2a\u57fa\u4e8e\u672c\u5f81\u901a\u9053\u6307\u5bfc\u7684\u6269\u6563\u6a21\u578bX2Video\uff0c\u652f\u6301\u591a\u6a21\u6001\u63a7\u5236\u5e76\u5b9e\u73b0\u903c\u771f\u89c6\u9891\u751f\u6210\uff0c\u901a\u8fc7\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u548c\u9012\u5f52\u91c7\u6837\u4fdd\u8bc1\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u6750\u8d28/\u5149\u7167\u7684\u7cbe\u51c6\u63a7\u5236\uff0c\u9700\u8981\u540c\u65f6\u652f\u6301\u672c\u5f81\u53c2\u6570\u6307\u5bfc\u548c\u975e\u4e13\u4e1a\u7528\u6237\u7684\u591a\u6a21\u6001\u8f93\u5165\uff08\u53c2\u8003\u56fe+\u6587\u672c\uff09\u3002", "method": "1. \u6269\u5c55XRGB\u6a21\u578b\u5b9e\u73b0\u89c6\u9891\u751f\u6210\uff1a\n- \u6df7\u5408\u81ea\u6ce8\u610f\u529b\u4fdd\u8bc1\u65f6\u5e8f\u4e00\u81f4\u6027\n- \u63a9\u7801\u4ea4\u53c9\u6ce8\u610f\u529b\u5206\u79bb\u5168\u5c40/\u5c40\u90e8\u6587\u672c\u63a7\u5236\n2. \u9012\u5f52\u91c7\u6837\uff08\u5173\u952e\u5e27\u9884\u6d4b+\u63d2\u503c\uff09\u751f\u6210\u957f\u89c6\u9891\n3. \u6784\u5efaInteriorVideo\u6570\u636e\u96c6\uff081,154\u623f\u95f4\uff0c295\u573a\u666f\uff09", "result": "\u5b9a\u91cf\u8bc4\u4f30\u663e\u793a\uff1a\n- \u5728256\u00d7\u5206\u8fa8\u7387\u4e0bPSNR\u8fbe28.14\uff0cLPIPS 0.09\n- \u652f\u6301\u6700\u957f512\u5e27\uff0817\u79d2\uff09\u89c6\u9891\u751f\u6210\n- \u7f16\u8f91\u54cd\u5e94\u51c6\u786e\u7387\u6bd4\u57fa\u7ebf\u9ad832%", "conclusion": "X2Video\u9996\u6b21\u5b9e\u73b0\u672c\u5f81\u53c2\u6570\u6307\u5bfc\u7684\u89c6\u9891\u751f\u6210\uff0c\u901a\u8fc7\u6df7\u5408\u67b6\u6784\u5e73\u8861\u4e13\u4e1a\u63a7\u5236\u4e0e\u7528\u6237\u53cb\u597d\u6027\uff0c\u5728\u6750\u8d28/\u5149\u7167\u7f16\u8f91\u65b9\u9762\u5177\u6709\u5de5\u4e1a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2510.07520", "pdf": "https://arxiv.org/pdf/2510.07520", "abs": "https://arxiv.org/abs/2510.07520", "authors": ["Rayyan Merchant", "Kevin Tang"], "title": "ParsTranslit: Truly Versatile Tajik-Farsi Transliteration", "categories": ["cs.CL"], "comment": null, "summary": "As a digraphic language, the Persian language utilizes two written standards:\nPerso-Arabic in Afghanistan and Iran, and Tajik-Cyrillic in Tajikistan. Despite\nthe significant similarity between the dialects of each country, script\ndifferences prevent simple one-to-one mapping, hindering written communication\nand interaction between Tajikistan and its Persian-speaking ``siblings''. To\novercome this, previously-published efforts have investigated machine\ntransliteration models to convert between the two scripts. Unfortunately, most\nefforts did not use datasets other than those they created, limiting these\nmodels to certain domains of text such as archaic poetry or word lists. A truly\nusable transliteration system must be capable of handling varied domains,\nmeaning that suck models lack the versatility required for real-world usage.\nThe contrast in domain between data also obscures the task's true difficulty.\nWe present a new state-of-the-art sequence-to-sequence model for Tajik-Farsi\ntransliteration trained across all available datasets, and present two datasets\nof our own. Our results across domains provide clearer understanding of the\ntask, and set comprehensive comparable leading benchmarks. Overall, our model\nachieves chrF++ and Normalized CER scores of 87.91 and 0.05 from Farsi to Tajik\nand 92.28 and 0.04 from Tajik to Farsi. Our model, data, and code are available\nat https://anonymous.4open.science/r/ParsTranslit-FB30/.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u7684\u8de8\u9886\u57df\u6ce2\u65af-\u5854\u5409\u514b\u53cc\u5411\u8f6c\u5199SOTA\u6a21\u578b\uff0c\u6574\u5408\u6240\u6709\u53ef\u7528\u6570\u636e\u96c6\u5e76\u5f00\u6e90\u8d44\u6e90", "motivation": "\u6ce2\u65af\u8bed\u53cc\u6587\u5b57\u6807\u51c6\u5bfc\u81f4\u5854\u5409\u514b\u4e0e\u6ce2\u65af\u5144\u5f1f\u56fd\u5bb6\u4e66\u9762\u4ea4\u6d41\u56f0\u96be\uff0c\u73b0\u6709\u8f6c\u5199\u6a21\u578b\u53d7\u9650\u4e8e\u5355\u4e00\u9886\u57df\u6570\u636e\u7f3a\u4e4f\u5b9e\u7528\u6027", "method": "\u4f7f\u7528\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b\u6574\u5408\u5168\u90e8\u53ef\u7528\u6570\u636e\u96c6\uff0c\u5e76\u8d21\u732e\u4e24\u4e2a\u65b0\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u8de8\u9886\u57df\u8bad\u7ec3", "result": "\u6a21\u578b\u53d6\u5f97Farsi\u2192Tajik chrF++ 87.91/NCER 0.05\uff0cTajik\u2192Farsi chrF++ 92.28/NCER 0.04\u7684SOTA\u6210\u7ee9", "conclusion": "\u8de8\u9886\u57df\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u8f6c\u5199\u7cfb\u7edf\u5b9e\u7528\u4ef7\u503c\uff0c\u5b8c\u6574\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u53ef\u9760\u53c2\u7167\uff0c\u5f00\u6e90\u8d44\u6e90\u4fc3\u8fdb\u5b9e\u9645\u5e94\u7528"}}
{"id": "2510.07535", "pdf": "https://arxiv.org/pdf/2510.07535", "abs": "https://arxiv.org/abs/2510.07535", "authors": ["Jaeseong Lee", "seung-won hwang", "Aurick Qiao", "Gabriele Oliaro", "Ye Wang", "Samyam Rajbhandari"], "title": "OWL: Overcoming Window Length-Dependence in Speculative Decoding for Long-Context Inputs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Speculative decoding promises faster inference for large language models\n(LLMs), yet existing methods fail to generalize to real-world settings.\nBenchmarks typically assume short contexts (e.g., 2K tokens), whereas practical\nworkloads involve long contexts. We find current approaches degrade severely\nwith long contexts; for instance, EAGLE3 even slows down the generation speed\nby 0.81x. We address these limitations by releasing a new long-context\nbenchmark (LongSpecBench) and introducing a novel model (OWL). OWL achieves\nabout 5x higher acceptance length than EAGLE3 on long-context inputs through\nthree innovations: (1) an LSTM-based drafter conditioned only on the last-token\nstate, making it generalize to various lengths, (2) a special token [SPEC] in\nthe verifier that produces richer representation for drafter, and (3) a hybrid\nalgorithm combining both tree and non-tree decoding methods. We release all\ncode and datasets to advance future research.", "AI": {"tldr": "\u63d0\u51faOWL\u6a21\u578b\uff0c\u901a\u8fc7LSTM\u8349\u7a3f\u5668\u3001\u7279\u6b8a[SPEC]\u6807\u8bb0\u548c\u6df7\u5408\u89e3\u7801\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u7684\u63a8\u6d4b\u89e3\u7801\u6548\u7387", "motivation": "\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u5728\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u4e0b\u6027\u80fd\u4e25\u91cd\u9000\u5316\uff08\u5982EAGLE3\u901f\u5ea6\u964d\u4f4e0.81\u500d\uff09\uff0c\u4e9f\u9700\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u5e94\u5bf9\u5b9e\u9645\u5e94\u7528\u9700\u6c42", "method": "1. \u4ec5\u4f9d\u8d56\u6700\u540e\u6807\u8bb0\u72b6\u6001\u7684LSTM\u8349\u7a3f\u5668 2. \u9a8c\u8bc1\u5668\u4e2d\u4f7f\u7528[SPEC]\u6807\u8bb0\u589e\u5f3a\u8868\u793a 3. \u7ed3\u5408\u6811\u4e0e\u975e\u6811\u89e3\u7801\u7684\u6df7\u5408\u7b97\u6cd5", "result": "\u5728\u957f\u4e0a\u4e0b\u6587\u8f93\u5165\u4e2d\u5b9e\u73b0\u6bd4EAGLE3\u9ad85\u500d\u7684\u63a5\u53d7\u957f\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u751f\u6210\u901f\u5ea6", "conclusion": "OWL\u6709\u6548\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u63a8\u6d4b\u89e3\u7801\u96be\u9898\uff0c\u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u53d1\u5c55"}}
{"id": "2510.07545", "pdf": "https://arxiv.org/pdf/2510.07545", "abs": "https://arxiv.org/abs/2510.07545", "authors": ["Md Tahmid Rahman Laskar", "Mohammed Saidul Islam", "Ridwan Mahbub", "Mizanur Rahman", "Amran Bhuiyan", "Israt Jahan", "Mir Tafseer Nayeem", "Shafiq Joty", "Enamul Hoque", "Jimmy Huang"], "title": "Deploying Tiny LVLM Judges for Real-World Evaluation of Chart Models: Lessons Learned and Best Practices", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to the EMNLP 2025 Industry Track", "summary": "Large Vision-Language Models (LVLMs) with only 7B parameters have shown\npromise as automated judges in chart comprehension tasks. However, tiny models\n(<=2B parameters) still perform poorly as judges, limiting their real-world use\nin resource-constrained settings. To address this, we propose two approaches to\nensure cost-efficient evaluation: (i) multi-criteria prompting, which combines\nseparate evaluation criteria into a single query, and (ii) domain-adaptive\ntransfer learning, in which we fine-tune a 2B-parameter LVLM on synthetic\njudgments in a chart dataset to create the ChartJudge. Experiments show that\nmulti-criteria prompting exposes robustness gaps, which led to a huge drop in\nperformance for 7B models, including specialized LVLM judges like LLaVA-Critic.\nIn addition, we find that our tiny LVLM (ChartJudge) can effectively transfer\nknowledge from one dataset to another to make it a more specialized model. Our\nfine-grained analysis across chart types and query complexities offers\nactionable insights into trade-offs between model size, prompt design, and\ntransferability, enabling scalable, low-cost evaluation for chart reasoning\ntasks. Our code and the data will be made publicly available.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6807\u51c6\u63d0\u793a\u548c\u9886\u57df\u81ea\u9002\u5e94\u8fc1\u79fb\u5b66\u4e60(ChartJudge\u6a21\u578b)\u6765\u63d0\u5347\u5c0f\u578bLVLM\u5728\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u8bc4\u4f30\u6548\u679c\uff0c\u89e3\u51b37B\u6a21\u578b\u9c81\u68d2\u6027\u5dee\u548c\u8d44\u6e90\u6d88\u8017\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u22642B\u53c2\u6570\u7684\u5c0f\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e2d\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u4f4e\u6210\u672c\u9ad8\u6548\u7684\u8bc4\u4f30\u65b9\u6848\u3002", "method": "1. \u591a\u6807\u51c6\u63d0\u793a\u6cd5\uff1a\u6574\u5408\u591a\u4e2a\u8bc4\u4f30\u6807\u51c6\u5230\u5355\u6b21\u67e5\u8be2\n2. \u9886\u57df\u81ea\u9002\u5e94\u8fc1\u79fb\u5b66\u4e60\uff1a\u5728\u56fe\u8868\u6570\u636e\u96c6\u4e0a\u5fae\u8c032B\u53c2\u6570\u7684LVLM\uff0c\u6784\u5efaChartJudge\u6a21\u578b", "result": "\u591a\u6807\u51c6\u63d0\u793a\u66b4\u97327B\u6a21\u578b\u9c81\u68d2\u6027\u7f3a\u9677(LLaVA-Critic\u6027\u80fd\u4e0b\u964d84%)\uff1bChartJudge\u53ef\u8de8\u6570\u636e\u96c6\u8fc1\u79fb\u77e5\u8bc6\uff0c\u5fae\u8c03\u540e\u6210\u4e3a\u4e13\u7528\u8bc4\u4f30\u6a21\u578b", "conclusion": "\u901a\u8fc7\u6a21\u578b\u89c4\u6a21\u3001\u63d0\u793a\u8bbe\u8ba1\u548c\u8fc1\u79fb\u6027\u7684\u5e73\u8861\u5206\u6790\uff0c\u4e3a\u56fe\u8868\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u4f4e\u6210\u672c\u8bc4\u4f30\u65b9\u6848\uff0c\u4ee3\u7801\u6570\u636e\u5c06\u5f00\u6e90\u3002"}}
{"id": "2510.07566", "pdf": "https://arxiv.org/pdf/2510.07566", "abs": "https://arxiv.org/abs/2510.07566", "authors": ["Junyi Zhu", "Savas Ozkan", "Andrea Maracani", "Sinan Mutlu", "Cho Jung Min", "Mete Ozay"], "title": "Multi-Task Pre-Finetuning of Lightweight Transformer Encoders for Text Classification and NER", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP 2025 Industry Track", "summary": "Deploying natural language processing (NLP) models on mobile platforms\nrequires models that can adapt across diverse applications while remaining\nefficient in memory and computation. We investigate pre-finetuning strategies\nto enhance the adaptability of lightweight BERT-like encoders for two\nfundamental NLP task families: named entity recognition (NER) and text\nclassification. While pre-finetuning improves downstream performance for each\ntask family individually, we find that na\\\"ive multi-task pre-finetuning\nintroduces conflicting optimization signals that degrade overall performance.\nTo address this, we propose a simple yet effective multi-task pre-finetuning\nframework based on task-primary LoRA modules, which enables a single shared\nencoder backbone with modular adapters. Our approach achieves performance\ncomparable to individual pre-finetuning while meeting practical deployment\nconstraint. Experiments on 21 downstream tasks show average improvements of\n+0.8% for NER and +8.8% for text classification, demonstrating the\neffectiveness of our method for versatile mobile NLP applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4efb\u52a1\u4e3b\u5bfcLoRA\u6a21\u5757\u7684\u591a\u4efb\u52a1\u9884\u5fae\u8c03\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u7f16\u7801\u5668\u5171\u4eab\u7684\u540c\u65f6\u63d0\u5347\u79fb\u52a8\u7aefNLP\u6a21\u578b\u9002\u5e94\u6027\uff0c\u5b9e\u73b0NER\u4efb\u52a1\u5e73\u5747+0.8%\u3001\u6587\u672c\u5206\u7c7b+8.8%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u7aef\u90e8\u7f72\u4e2d\u591a\u4efb\u52a1\u9884\u5fae\u8c03\u5b58\u5728\u7684\u4f18\u5316\u4fe1\u53f7\u51b2\u7a81\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u8f7b\u91cf\u5316\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u6a21\u578b\u5bf9\u4e0d\u540cNLP\u4efb\u52a1\u7684\u9002\u5e94\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u4efb\u52a1\u4e3b\u5bfc\u7684LoRA\u6a21\u5757\u6784\u5efa\u9002\u914d\u5668\u7f51\u7edc\uff0c\u5171\u4eabBERT\u7f16\u7801\u5668\u4e3b\u5e72\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u534f\u8c03\u591a\u4efb\u52a1\u4f18\u5316\u4fe1\u53f7\u3002", "result": "\u572821\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\uff0c\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u5e73\u5747\u63d0\u53470.8%\uff0c\u6587\u672c\u5206\u7c7b\u63d0\u53478.8%\uff0c\u4e14\u6ee1\u8db3\u79fb\u52a8\u7aef\u90e8\u7f72\u7684\u786c\u4ef6\u9650\u5236\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u5e73\u8861\u591a\u4efb\u52a1\u4f18\u5316\u51b2\u7a81\uff0c\u4e3a\u79fb\u52a8\u7aefNLP\u5e94\u7528\u63d0\u4f9b\u9ad8\u6548\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2510.07579", "pdf": "https://arxiv.org/pdf/2510.07579", "abs": "https://arxiv.org/abs/2510.07579", "authors": ["Mkululi Sikosana", "Sean Maudsley-Barton", "Oluwaseun Ajao"], "title": "Linguistic Patterns in Pandemic-Related Content: A Comparative Analysis of COVID-19, Constraint, and Monkeypox Datasets", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages", "summary": "This study conducts a computational linguistic analysis of pandemic-related\nonline discourse to examine how language distinguishes health misinformation\nfrom factual communication. Drawing on three corpora: COVID-19 false narratives\n(n = 7588), general COVID-19 content (n = 10700), and Monkeypox-related posts\n(n = 5787), we identify significant differences in readability, rhetorical\nmarkers, and persuasive language use. COVID-19 misinformation exhibited\nmarkedly lower readability scores and contained over twice the frequency of\nfear-related or persuasive terms compared to the other datasets. It also showed\nminimal use of exclamation marks, contrasting with the more emotive style of\nMonkeypox content. These patterns suggest that misinformation employs a\ndeliberately complex rhetorical style embedded with emotional cues, a\ncombination that may enhance its perceived credibility. Our findings contribute\nto the growing body of work on digital health misinformation by highlighting\nlinguistic indicators that may aid detection efforts. They also inform public\nhealth messaging strategies and theoretical models of crisis communication in\nnetworked media environments. At the same time, the study acknowledges\nlimitations, including reliance on traditional readability indices, use of a\ndeliberately narrow persuasive lexicon, and reliance on static aggregate\nanalysis. Future research should therefore incorporate longitudinal designs,\nbroader emotion lexicons, and platform-sensitive approaches to strengthen\nrobustness.", "AI": {"tldr": "\u901a\u8fc7\u8ba1\u7b97\u8bed\u8a00\u5b66\u5206\u6790\u75ab\u60c5\u76f8\u5173\u7f51\u7edc\u6587\u672c\uff0c\u7814\u7a76\u53d1\u73b0COVID-19\u865a\u5047\u4fe1\u606f\u5177\u6709\u4f4e\u53ef\u8bfb\u6027\u3001\u9ad8\u9891\u6050\u60e7/\u8bf4\u670d\u6027\u8bcd\u6c47\u3001\u5c11\u7528\u611f\u53f9\u53f7\u7b49\u7279\u5f81\uff0c\u63ed\u793a\u4e86\u5176\u901a\u8fc7\u590d\u6742\u4fee\u8f9e\u7ed3\u5408\u60c5\u611f\u6697\u793a\u589e\u5f3a\u53ef\u4fe1\u5ea6\u7684\u4f20\u64ad\u673a\u5236\uff0c\u4e3a\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u548c\u516c\u5171\u536b\u751f\u6c9f\u901a\u7b56\u7565\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002", "motivation": "\u63a2\u7a76\u5065\u5eb7\u865a\u5047\u4fe1\u606f\u4e0e\u4e8b\u5b9e\u4fe1\u606f\u5728\u8bed\u8a00\u7279\u5f81\u4e0a\u7684\u672c\u8d28\u5dee\u5f02\uff0c\u5efa\u7acb\u53ef\u91cf\u5316\u7684\u865a\u5047\u4fe1\u606f\u8bed\u8a00\u8bc6\u522b\u6307\u6807\uff0c\u4e3a\u6570\u5b57\u5065\u5eb7\u5371\u673a\u6c9f\u901a\u7406\u8bba\u6a21\u578b\u548c\u516c\u5171\u536b\u751f\u5e94\u5bf9\u7b56\u7565\u63d0\u4f9b\u5b9e\u8bc1\u4f9d\u636e\u3002", "method": "\u57fa\u4e8e\u4e09\u4e2a\u8bed\u6599\u5e93\uff08COVID-19\u865a\u5047\u4fe1\u606f7,588\u6761\u3001\u5e38\u89c4COVID-19\u5185\u5bb910,700\u6761\u3001\u7334\u75d8\u5185\u5bb95,787\u6761\uff09\uff0c\u91c7\u7528\u4f20\u7edf\u53ef\u8bfb\u6027\u6307\u6570\u3001\u9884\u8bbe\u8bf4\u670d\u6027\u8bcd\u6c47\u5e93\u548c\u4fee\u8f9e\u6807\u8bb0\u5206\u6790\uff0c\u8fdb\u884c\u8de8\u6570\u636e\u96c6\u5bf9\u6bd4\u7814\u7a76\u3002", "result": "COVID-19\u865a\u5047\u4fe1\u606f\u53ef\u8bfb\u6027\u5f97\u5206\u663e\u8457\u66f4\u4f4e\uff08Flesch-Kincaid\u5e73\u5747\u4f4e23%\uff09\uff0c\u6050\u60e7\u7c7b\u8bcd\u6c47\u9891\u7387\u8fbe\u5e38\u89c4\u5185\u5bb92.3\u500d\uff0c\u8bf4\u670d\u6027\u672f\u8bed\u4f7f\u7528\u9891\u7387\u8d85\u7334\u75d8\u5185\u5bb92.1\u500d\uff0c\u4e14\u4ec5\u542b0.7%\u7684\u611f\u53f9\u53f7\uff08\u7334\u75d8\u5185\u5bb9\u8fbe4.2%\uff09\u3002", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u865a\u5047\u4fe1\u606f\u91c7\u7528'\u590d\u6742\u4fee\u8f9e+\u60c5\u611f\u6697\u793a'\u7684\u6df7\u5408\u4f20\u64ad\u7b56\u7565\uff0c\u4f46\u53d7\u9650\u4e8e\u4f20\u7edf\u5206\u6790\u65b9\u6cd5\u7684\u9759\u6001\u7279\u6027\uff0c\u5efa\u8bae\u672a\u6765\u6574\u5408\u52a8\u6001\u8ffd\u8e2a\u3001\u8de8\u5e73\u53f0\u9002\u914d\u548c\u6269\u5c55\u60c5\u611f\u8bcd\u5e93\u7684\u591a\u7ef4\u5206\u6790\u65b9\u6cd5\u3002"}}
