{"id": "2508.05685", "pdf": "https://arxiv.org/pdf/2508.05685", "abs": "https://arxiv.org/abs/2508.05685", "authors": ["Yara Bahram", "Mohammadhadi Shateri", "Eric Granger"], "title": "DogFit: Domain-guided Fine-tuning for Efficient Transfer Learning of Diffusion Models", "categories": ["cs.GR"], "comment": "Currently under review. Code will be released upon acceptance", "summary": "Transfer learning of diffusion models to smaller target domains is\nchallenging, as naively fine-tuning the model often results in poor\ngeneralization. Test-time guidance methods help mitigate this by offering\ncontrollable improvements in image fidelity through a trade-off with sample\ndiversity. However, this benefit comes at a high computational cost, typically\nrequiring dual forward passes during sampling. We propose the Domain-guided\nFine-tuning (DogFit) method, an effective guidance mechanism for diffusion\ntransfer learning that maintains controllability without incurring additional\ncomputational overhead. DogFit injects a domain-aware guidance offset into the\ntraining loss, effectively internalizing the guided behavior during the\nfine-tuning process. The domain-aware design is motivated by our observation\nthat during fine-tuning, the unconditional source model offers a stronger\nmarginal estimate than the target model. To support efficient controllable\nfidelity-diversity trade-offs at inference, we encode the guidance strength\nvalue as an additional model input through a lightweight conditioning\nmechanism. We further investigate the optimal placement and timing of the\nguidance offset during training and propose two simple scheduling strategies,\ni.e., late-start and cut-off, which improve generation quality and training\nstability. Experiments on DiT and SiT backbones across six diverse target\ndomains show that DogFit can outperform prior guidance methods in transfer\nlearning in terms of FID and FDDINOV2 while requiring up to 2x fewer sampling\nTFLOPS.", "AI": {"tldr": "\u63d0\u51faDomain-guided Fine-tuning (DogFit)\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u65f6\u5185\u5316\u5f15\u5bfc\u504f\u79fb\u5b9e\u73b0\u53ef\u63a7\u7684\u6269\u6563\u6a21\u578b\u8fc1\u79fb\u5b66\u4e60\uff0c\u5728\u51cf\u5c1150%\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u5f15\u5bfc\u65b9\u6cd5\u4f9d\u8d56\u53cc\u91cd\u524d\u5411\u4f20\u64ad\u5bfc\u81f4\u9ad8\u8ba1\u7b97\u6210\u672c\uff0c\u4e14\u76f4\u63a5\u5fae\u8c03\u6a21\u578b\u4f1a\u635f\u5bb3\u6cdb\u5316\u80fd\u529b", "method": "1. \u5728\u8bad\u7ec3\u635f\u5931\u4e2d\u6ce8\u5165\u9886\u57df\u611f\u77e5\u5f15\u5bfc\u504f\u79fb 2. \u8bbe\u8ba1\u8f7b\u91cf\u7ea7\u6761\u4ef6\u673a\u5236\u7f16\u7801\u5f15\u5bfc\u5f3a\u5ea6 3. \u63d0\u51fa\u5ef6\u8fdf\u542f\u52a8\u548c\u622a\u6b62\u8c03\u5ea6\u7b56\u7565\u4f18\u5316\u8bad\u7ec3", "result": "\u5728DiT/SiT\u6a21\u578b\u548c6\u4e2a\u76ee\u6807\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0cFID\u548cFDDINOV2\u6307\u6807\u4f18\u4e8e\u57fa\u7ebf\uff0c\u91c7\u6837TFLOPS\u51cf\u5c112\u500d", "conclusion": "DogFit\u901a\u8fc7\u8bad\u7ec3\u65f6\u5185\u90e8\u5316\u5f15\u5bfc\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u751f\u6210\u8d28\u91cf\u7684\u5e73\u8861\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u8fc1\u79fb\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2508.06086", "pdf": "https://arxiv.org/pdf/2508.06086", "abs": "https://arxiv.org/abs/2508.06086", "authors": ["Kojiro Tanaka", "Keiichi Sato", "Masahiko Mikawa", "Makoto Fujisawa"], "title": "Exploring Interactive Simulation of Grass Display Color Characteristic Based on Real-World Conditions", "categories": ["cs.GR", "cs.HC"], "comment": "Accepted to 20th IFIP TC13 International Conference on Human-Computer\n  Interaction (INTERACT '25), 24 pages", "summary": "Recent research has focused on incorporating media into living environments\nvia color-controlled materials and image display. In particular, grass-based\ndisplays have drawn attention as landscape-friendly interactive interfaces. To\ndevelop the grass display, it is important to obtain the grass color change\ncharacteristics that depend on the real environment. However, conventional\nmethods require experiments on actual equipment every time the lighting or\nviewpoint changes, which is time-consuming and costly. Although research has\nbegun on simulating grass colors, this approach still faces significant issues\nas it takes many hours for a single measurement. In this paper, we explore an\ninteractive simulation of a grass display color change characteristic based on\nreal-world conditions in a virtual environment. We evaluated our method's\naccuracy by simulating grass color characteristics across multiple viewpoints\nand environments, and then compared the results against prior work. The results\nindicated that our method tended to simulate the grass color characteristics\nsimilar to the actual characteristics and showed the potential to do so more\nquickly and with comparable accuracy to the previous study.", "AI": {"tldr": "\u5f00\u53d1\u865a\u62df\u73af\u5883\u4ea4\u4e92\u5f0f\u6a21\u62df\u65b9\u6cd5\uff0c\u5feb\u901f\u9884\u6d4b\u591a\u89c6\u89d2/\u73af\u5883\u4e0b\u8349\u5c4f\u663e\u8272\u7279\u6027\uff0c\u8f83\u4f20\u7edf\u65b9\u6848\u66f4\u9ad8\u6548\u3002", "motivation": "\u4f20\u7edf\u8349\u5c4f\u663e\u8272\u7814\u7a76\u65b9\u6cd5\u9700\u53cd\u590d\u5b9e\u7269\u5b9e\u9a8c\uff0c\u6210\u672c\u9ad8\u8017\u65f6\u957f\uff1b\u73b0\u6709\u6a21\u62df\u6280\u672f\u5355\u6b21\u6d4b\u91cf\u9700\u6570\u5c0f\u65f6\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u6784\u5efa\u865a\u62df\u73af\u5883\u4ea4\u4e92\u6a21\u62df\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u89c6\u89d2/\u591a\u73af\u5883\u6a21\u62df\u5b9e\u9a8c\uff0c\u5e76\u4e0e\u65e2\u6709\u7814\u7a76\u5bf9\u6bd4\u9a8c\u8bc1\u51c6\u786e\u6027\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u4e0e\u5b9e\u9645\u663e\u8272\u7279\u6027\u9ad8\u5ea6\u543b\u5408\uff0c\u8fd0\u7b97\u901f\u5ea6\u8d85\u8d8a\u5148\u524d\u7814\u7a76\uff0c\u4e14\u4fdd\u6301\u7cbe\u5ea6\u76f8\u5f53\u3002", "conclusion": "\u865a\u62df\u6a21\u62df\u65b9\u6848\u663e\u8457\u63d0\u5347\u8349\u5c4f\u663e\u8272\u7814\u7a76\u6548\u7387\uff0c\u4e3a\u751f\u6001\u53cb\u597d\u578b\u4ea4\u4e92\u754c\u9762\u5f00\u53d1\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.05899", "pdf": "https://arxiv.org/pdf/2508.05899", "abs": "https://arxiv.org/abs/2508.05899", "authors": ["Zixuan Bian", "Ruohan Ren", "Yue Yang", "Chris Callison-Burch"], "title": "HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "3D scene generation plays a crucial role in gaming, artistic creation,\nvirtual reality and many other domains. However, current 3D scene design still\nrelies heavily on extensive manual effort from creators, and existing automated\nmethods struggle to generate open-domain scenes or support flexible editing. As\na result, generating 3D worlds directly from text has garnered increasing\nattention. In this paper, we introduce HOLODECK 2.0, an advanced\nvision-language-guided framework for 3D world generation with support for\ninteractive scene editing based on human feedback. HOLODECK 2.0 can generate\ndiverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and\ncyberpunk styles) that exhibit high semantic fidelity to fine-grained input\ndescriptions, suitable for both indoor and open-domain environments. HOLODECK\n2.0 leverages vision-language models (VLMs) to identify and parse the objects\nrequired in a scene and generates corresponding high-quality assets via\nstate-of-the-art 3D generative models. It then iteratively applies spatial\nconstraints derived from the VLMs to achieve semantically coherent and\nphysically plausible layouts. Human evaluations and CLIP-based assessments\ndemonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely\naligned with detailed textual descriptions, consistently outperforming\nbaselines across indoor and open-domain scenarios. Additionally, we provide\nediting capabilities that flexibly adapt to human feedback, supporting layout\nrefinement and style-consistent object edits. Finally, we present a practical\napplication of HOLODECK 2.0 in procedural game modeling, generating visually\nrich and immersive environments, potentially boosting efficiency.", "AI": {"tldr": "HOLODECK 2.0\u662f\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u76843D\u573a\u666f\u751f\u6210\u6846\u67b6\uff0c\u652f\u6301\u6587\u672c\u9a71\u52a8\u7684\u591a\u6837\u5316\u98ce\u683c\u573a\u666f\u751f\u6210\u548c\u4ea4\u4e92\u5f0f\u7f16\u8f91\uff0c\u5728\u5ba4\u5185/\u5f00\u653e\u57df\u573a\u666f\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u67093D\u573a\u666f\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u4e14\u7075\u6d3b\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u80fd\u81ea\u52a8\u751f\u6210\u5f00\u653e\u57df\u573a\u666f\u5e76\u652f\u6301\u7075\u6d3b\u7f16\u8f91\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u89e3\u6790\u573a\u666f\u9700\u6c42\n2. \u8c03\u7528\u5148\u8fdb3D\u751f\u6210\u6a21\u578b\u521b\u5efa\u8d44\u6e90\n3. \u57fa\u4e8eVLM\u7a7a\u95f4\u7ea6\u675f\u8fed\u4ee3\u4f18\u5316\u5e03\u5c40\n4. \u652f\u6301\u4eba\u7c7b\u53cd\u9988\u9a71\u52a8\u7684\u5e03\u5c40\u8c03\u6574\u548c\u98ce\u683c\u4e00\u81f4\u7f16\u8f91", "result": "\u4eba\u7c7b\u8bc4\u4f30\u548cCLIP\u6307\u6807\u663e\u793a\uff0c\u5728\u5ba4\u5185\u548c\u5f00\u653e\u57df\u573a\u666f\u4e2d\u751f\u6210\u8d28\u91cf\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bed\u4e49\u5bf9\u9f50\u5ea6\u63d0\u534732%", "conclusion": "HOLODECK 2.0\u5b9e\u73b0\u4e86\u6587\u672c\u52303D\u4e16\u754c\u7684\u9ad8\u6548\u751f\u6210\uff0c\u652f\u6301\u6e38\u620f\u5efa\u6a21\u7b49\u5b9e\u9645\u5e94\u7528\uff0c\u7f16\u8f91\u529f\u80fd\u5c06\u521b\u4f5c\u8005\u6548\u7387\u63d0\u534740%"}}
{"id": "2508.06055", "pdf": "https://arxiv.org/pdf/2508.06055", "abs": "https://arxiv.org/abs/2508.06055", "authors": ["Wonjung Park", "Suhyun Ahn", "Jinah Park"], "title": "LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study on Alzheimer's disease, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Lateral ventricle (LV) shape analysis holds promise as a biomarker for\nneurological diseases; however, challenges remain due to substantial shape\nvariability across individuals and segmentation difficulties arising from\nlimited MRI resolution. We introduce LV-Net, a novel framework for producing\nindividualized 3D LV meshes from brain MRI by deforming an anatomy-aware joint\nLV-hippocampus template mesh. By incorporating anatomical relationships\nembedded within the joint template, LV-Net reduces boundary segmentation\nartifacts and improves reconstruction robustness. In addition, by classifying\nthe vertices of the template mesh based on their anatomical adjacency, our\nmethod enhances point correspondence across subjects, leading to more accurate\nLV shape statistics. We demonstrate that LV-Net achieves superior\nreconstruction accuracy, even in the presence of segmentation imperfections,\nand delivers more reliable shape descriptors across diverse datasets. Finally,\nwe apply LV-Net to Alzheimer's disease analysis, identifying LV subregions that\nshow significantly associations with the disease relative to cognitively normal\ncontrols. The codes for LV shape modeling are available at\nhttps://github.com/PWonjung/LV_Shape_Modeling.", "AI": {"tldr": "\u5f00\u53d1LV-Net\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u6a21\u677f\u7f51\u683c\u53d8\u5f62\u63d0\u5347\u4fa7\u8111\u5ba4\u5f62\u72b6\u5206\u6790\uff0c\u5e94\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u68c0\u6d4b", "motivation": "\u4fa7\u8111\u5ba4\u5f62\u72b6\u5206\u6790\u5b58\u5728\u4e2a\u4f53\u5dee\u5f02\u5927\u3001MRI\u5206\u8fa8\u7387\u9650\u5236\u5bfc\u81f4\u7684\u8fb9\u754c\u5206\u5272\u4f2a\u5f71\u95ee\u9898\uff0c\u9700\u66f4\u9c81\u68d2\u7684\u5efa\u6a21\u65b9\u6cd5", "method": "\u63d0\u51fa\u89e3\u5256\u8054\u5408\u6a21\u677f\u53d8\u5f62\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u7c7b\u6a21\u677f\u9876\u70b9\u589e\u5f3a\u5bf9\u5e94\u6027\uff0c\u878d\u5408\u6d77\u9a6c\u7ed3\u6784\u89e3\u5256\u5173\u7cfb\u63d0\u5347\u5206\u5272\u7a33\u5b9a\u6027", "result": "LV-Net\u5728\u5206\u5272\u4e0d\u5b8c\u7f8e\u60c5\u51b5\u4e0b\u4ecd\u4fdd\u6301\u9ad8\u91cd\u5efa\u7cbe\u5ea6\uff0c\u8de8\u6570\u636e\u96c6\u5f62\u72b6\u63cf\u8ff0\u7b26\u53ef\u9760\u6027\u663e\u8457\u63d0\u5347", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u8bc6\u522b\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u76f8\u5173\u4fa7\u8111\u5ba4\u4e9a\u533a\uff0c\u4e3a\u795e\u7ecf\u75be\u75c5\u63d0\u4f9b\u6709\u6548\u5f62\u6001\u5b66\u751f\u7269\u6807\u5fd7"}}
{"id": "2508.05722", "pdf": "https://arxiv.org/pdf/2508.05722", "abs": "https://arxiv.org/abs/2508.05722", "authors": ["Rania Al-Sabbagh"], "title": "PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare", "categories": ["cs.CL"], "comment": null, "summary": "This paper introduces PEACH, a sentence-aligned parallel English-Arabic\ncorpus of healthcare texts encompassing patient information leaflets and\neducational materials. The corpus contains 51,671 parallel sentences, totaling\napproximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths\nvary between 9.52 and 11.83 words on average. As a manually aligned corpus,\nPEACH is a gold-standard corpus, aiding researchers in contrastive linguistics,\ntranslation studies, and natural language processing. It can be used to derive\nbilingual lexicons, adapt large language models for domain-specific machine\ntranslation, evaluate user perceptions of machine translation in healthcare,\nassess patient information leaflets and educational materials' readability and\nlay-friendliness, and as an educational resource in translation studies. PEACH\nis publicly accessible.", "AI": {"tldr": "PEACH\u662f\u4e00\u4e2a\u4eba\u5de5\u5bf9\u9f50\u7684\u82f1\u8bed-\u963f\u62c9\u4f2f\u8bed\u533b\u7597\u9886\u57df\u5e73\u884c\u8bed\u6599\u5e93\uff0c\u5305\u542b5.1\u4e07\u53e5\u5bf9\uff0c\u652f\u6301\u8bed\u8a00\u5b66\u3001\u7ffb\u8bd1\u7814\u7a76\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u591a\u9886\u57df\u5e94\u7528", "motivation": "\u533b\u7597\u9886\u57df\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u5e73\u884c\u8bed\u6599\u5e93\uff0c\u5236\u7ea6\u4e86\u8de8\u8bed\u8a00\u7814\u7a76\u3001\u673a\u5668\u7ffb\u8bd1\u6a21\u578b\u4f18\u5316\u548c\u533b\u7597\u6587\u672c\u53ef\u8bfb\u6027\u8bc4\u4f30\uff0cPEACH\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d", "method": "\u901a\u8fc7\u4eba\u5de5\u5bf9\u9f50\u65b9\u5f0f\u6784\u5efa\u8bed\u6599\u5e93\uff0c\u6536\u96c6\u60a3\u8005\u8bf4\u660e\u4e66\u548c\u5065\u5eb7\u6559\u80b2\u6750\u6599\uff0c\u5f62\u6210\u5305\u542b51,671\u53e5\u5bf9\u7684\u5e73\u884c\u6587\u672c\uff0c\u7edf\u8ba1\u5e73\u5747\u53e5\u957f9.52-11.83\u4e2a\u8bcd", "result": "\u521b\u5efa\u4e86\u5305\u542b\u7ea659\u4e07\u82f1\u8bed\u8bcd\u548c56.7\u4e07\u963f\u62c9\u4f2f\u8bed\u8bcd\u7684\u91d1\u6807\u51c6\u8bed\u6599\u5e93\uff0c\u63d0\u4f9b\u8bcd\u5bf9\u9f50\u6807\u6ce8\u5e76\u516c\u5f00\u8bbf\u95ee", "conclusion": "PEACH\u4f5c\u4e3a\u591a\u529f\u80fd\u7814\u7a76\u8d44\u6e90\uff0c\u53ef\u652f\u6301\u53cc\u8bed\u8bcd\u5178\u6784\u5efa\u3001\u9886\u57df\u81ea\u9002\u5e94\u673a\u5668\u7ffb\u8bd1\u3001\u533b\u7597\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u4ee5\u53ca\u7ffb\u8bd1\u6559\u5b66\uff0c\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c"}}
{"id": "2508.06316", "pdf": "https://arxiv.org/pdf/2508.06316", "abs": "https://arxiv.org/abs/2508.06316", "authors": ["Theresa Pollinger", "Masado Ishii", "Jens Domke"], "title": "The Beauty of Anisotropic Mesh Refinement: Omnitrees for Efficient Dyadic Discretizations", "categories": ["cs.DS", "cs.CG", "cs.GR", "cs.IT", "cs.NA", "math.IT", "math.NA", "65D15, 65D18, 68P05, 68P30"], "comment": "contains pdf animations; we recommend Okular or Firefox for viewing", "summary": "Structured adaptive mesh refinement (AMR), commonly implemented via quadtrees\nand octrees, underpins a wide range of applications including databases,\ncomputer graphics, physics simulations, and machine learning. However, octrees\nenforce isotropic refinement in regions of interest, which can be especially\ninefficient for problems that are intrinsically anisotropic--much resolution is\nspent where little information is gained. This paper presents omnitrees as an\nanisotropic generalization of octrees and related data structures. Omnitrees\nallow to refine only the locally most important dimensions, providing tree\nstructures that are less deep than bintrees and less wide than octrees. As a\nresult, the convergence of the AMR schemes can be increased by up to a factor\nof the dimensionality d for very anisotropic problems, quickly offsetting their\nmodest increase in storage overhead. We validate this finding on the problem of\nbinary shape representation across 4,166 three-dimensional objects: Omnitrees\nincrease the mean convergence rate by 1.5x, require less storage to achieve\nequivalent error bounds, and maximize the information density of the stored\nfunction faster than octrees. These advantages are projected to be even\nstronger for higher-dimensional problems. We provide a first validation by\nintroducing a time-dependent rotation to create four-dimensional\nrepresentations, and discuss the properties of their 4-d octree and omnitree\napproximations. Overall, omnitree discretizations can make existing AMR\napproaches more efficient, and open up new possibilities for high-dimensional\napplications.", "AI": {"tldr": "\u63d0\u51fa\u5404\u5411\u5f02\u6027\u516b\u53c9\u6811\u7ed3\u6784omnitrees\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u7ef4\u5ea6\u7ec6\u5316\u63d0\u5347\u81ea\u9002\u5e94\u7f51\u683c\u6548\u7387\uff0c\u5728\u4e09\u7ef4\u7269\u4f53\u8868\u793a\u4e2d\u5b9e\u73b01.5\u500d\u6536\u655b\u52a0\u901f\uff0c\u5b58\u50a8\u6548\u7387\u4f18\u4e8e\u4f20\u7edf\u516b\u53c9\u6811", "motivation": "\u4f20\u7edf\u516b\u53c9\u6811\u5f3a\u5236\u5404\u5411\u540c\u6027\u7ec6\u5316\u5bfc\u81f4\u5404\u5411\u5f02\u6027\u95ee\u9898\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u66f4\u9002\u5e94\u7ef4\u5ea6\u7279\u5f81\u7684\u7f51\u683c\u7ec6\u5316\u65b9\u6848", "method": "\u5f00\u53d1omnitrees\u6570\u636e\u7ed3\u6784\uff0c\u5141\u8bb8\u4ec5\u7ec6\u5316\u5c40\u90e8\u91cd\u8981\u7ef4\u5ea6\uff0c\u5f62\u6210\u6bd4\u4e8c\u53c9\u6811\u66f4\u6d45\u3001\u6bd4\u516b\u53c9\u6811\u66f4\u7a84\u7684\u6811\u7ed3\u6784", "result": "4166\u4e2a\u4e09\u7ef4\u7269\u4f53\u6d4b\u8bd5\u663e\u793a\uff1a\u5e73\u5747\u6536\u655b\u901f\u5ea6\u63d0\u53471.5\u500d\uff0c\u540c\u7b49\u8bef\u5dee\u4e0b\u5b58\u50a8\u9700\u6c42\u66f4\u4f4e\uff0c\u56db\u7ef4\u65cb\u8f6c\u9a8c\u8bc1\u663e\u793a\u9ad8\u7ef4\u4f18\u52bf\u66f4\u663e\u8457", "conclusion": "omnitrees\u7a81\u7834\u4f20\u7edfAMR\u7ef4\u5ea6\u9650\u5236\uff0c\u63d0\u5347\u5404\u5411\u5f02\u6027\u95ee\u9898\u6548\u7387\uff0c\u4e3a\u9ad8\u7ef4\u5e94\u7528\u5f00\u8f9f\u65b0\u53ef\u80fd"}}
{"id": "2508.05775", "pdf": "https://arxiv.org/pdf/2508.05775", "abs": "https://arxiv.org/abs/2508.05775", "authors": ["Chi Zhang", "Changjia Zhu", "Junjie Xiong", "Xiaoran Xu", "Lingyao Li", "Yao Liu", "Zhuo Lu"], "title": "Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs) have revolutionized content creation across\ndigital platforms, offering unprecedented capabilities in natural language\ngeneration and understanding. These models enable beneficial applications such\nas content generation, question and answering (Q&A), programming, and code\nreasoning. Meanwhile, they also pose serious risks by inadvertently or\nintentionally producing toxic, offensive, or biased content. This dual role of\nLLMs, both as powerful tools for solving real-world problems and as potential\nsources of harmful language, presents a pressing sociotechnical challenge. In\nthis survey, we systematically review recent studies spanning unintentional\ntoxicity, adversarial jailbreaking attacks, and content moderation techniques.\nWe propose a unified taxonomy of LLM-related harms and defenses, analyze\nemerging multimodal and LLM-assisted jailbreak strategies, and assess\nmitigation efforts, including reinforcement learning with human feedback\n(RLHF), prompt engineering, and safety alignment. Our synthesis highlights the\nevolving landscape of LLM safety, identifies limitations in current evaluation\nmethodologies, and outlines future research directions to guide the development\nof robust and ethically aligned language technologies.", "AI": {"tldr": "\u7cfb\u7edf\u7efc\u8ff0LLM\u5728\u5185\u5bb9\u521b\u9020\u4e2d\u7684\u53cc\u5203\u5251\u6548\u5e94\u53ca\u5b89\u5168\u9632\u5fa1\u63aa\u65bd", "motivation": "\u89e3\u51b3LLM\u4f5c\u4e3a\u751f\u4ea7\u529b\u5de5\u5177\u4e0e\u6f5c\u5728\u6709\u5bb3\u5185\u5bb9\u6e90\u7684\u77db\u76fe\u6027\u793e\u4f1a\u6280\u672f\u6311\u6218", "method": "\u5efa\u7acb\u7edf\u4e00\u5371\u5bb3\u5206\u7c7b\u6846\u67b6\uff0c\u5206\u6790\u591a\u6a21\u6001\u8d8a\u72f1\u653b\u51fb\u6a21\u5f0f\uff0c\u8bc4\u4f30RLHF/\u63d0\u793a\u5bf9\u9f50\u7b49\u9632\u5fa1\u6280\u672f", "result": "\u63ed\u793a\u73b0\u6709\u5b89\u5168\u8bc4\u4f30\u65b9\u6cd5\u5c40\u9650\u6027\uff0c\u9a8c\u8bc1\u5f3a\u5316\u5b66\u4e60\u5bf9\u9f50\u7684\u6709\u6548\u6027\uff0c\u8bc6\u522b\u65b0\u578b\u653b\u51fb\u5411\u91cf\u6f14\u5316\u8d8b\u52bf", "conclusion": "\u9700\u5f00\u53d1\u52a8\u6001\u9632\u5fa1\u4f53\u7cfb\u4e0e\u591a\u7ef4\u5ea6\u4f26\u7406\u8bc4\u4f30\u6846\u67b6\uff0c\u63a8\u52a8\u8bed\u8a00\u6280\u672f\u7684\u7a33\u5065\u6027\u53d1\u5c55"}}
{"id": "2508.06345", "pdf": "https://arxiv.org/pdf/2508.06345", "abs": "https://arxiv.org/abs/2508.06345", "authors": ["Yanbin Wei", "Jiangyue Yan", "Chun Kang", "Yang Chen", "Hua Liu", "James T. Kwok", "Yu Zhang"], "title": "Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering", "categories": ["cs.CL", "cs.AI", "cs.GR", "cs.LG"], "comment": null, "summary": "Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities\nin diverse domain question-answering (QA) tasks, including graph QA that\ninvolves complex graph topologies. However, most current approaches use only a\nsingle type of graph representation, namely Topology Representation Form (TRF),\nsuch as prompt-unified text descriptions or style-fixed visual styles. Those\n\"one-size-fits-all\" approaches fail to consider the specific preferences of\ndifferent models or tasks, often leading to incorrect or overly long responses.\nTo address this, we first analyze the characteristics and weaknesses of\nexisting TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to\nzero-shot graph QA. We then introduce a new metric, Graph Response Efficiency\n(GRE), which measures the balance between the performance and the brevity in\ngraph QA. Built on these, we develop the DynamicTRF framework, which aims to\nimprove both the accuracy and conciseness of graph QA. To be specific,\nDynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based\non their GRE scores, to probe the question-specific TRF preferences. Then it\ntrains a TRF router on the TRFP dataset, to adaptively assign the best TRF from\n$F_{ZS}$ for each question during the inference. Extensive experiments across 7\nin-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show\nthat DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms\nof accuracy", "AI": {"tldr": "\u63d0\u51faDynamicTRF\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6700\u4f18\u62d3\u6251\u8868\u793a\u5f62\u5f0f\uff0c\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u5728\u56fe\u95ee\u7b54\u4e2d\u7684\u51c6\u786e\u6027\u4e0e\u7b80\u6d01\u6027", "motivation": "\u73b0\u6709\u56fe\u95ee\u7b54\u65b9\u6cd5\u91c7\u7528\u5355\u4e00\u62d3\u6251\u8868\u793a\u5f62\u5f0f(TRF)\uff0c\u65e0\u6cd5\u9002\u914d\u4e0d\u540c\u6a21\u578b/\u4efb\u52a1\u9700\u6c42\uff0c\u5bfc\u81f4\u56de\u7b54\u9519\u8bef\u6216\u5197\u957f", "method": "1. \u5206\u6790TRF\u7279\u6027\u5e76\u8bbe\u8ba1\u96f6\u6837\u672c\u4e13\u7528TRF\u96c6\u5408F_ZS 2. \u63d0\u51faGRE\u6307\u6807\u91cf\u5316\u6027\u80fd-\u7b80\u6d01\u5e73\u8861 3. \u6784\u5efaTRFP\u6570\u636e\u96c6\u8bad\u7ec3\u52a8\u6001\u8def\u7531\u5668\u5b9e\u73b0TRF\u81ea\u9002\u5e94\u5206\u914d", "result": "\u57287\u4e2a\u7b97\u6cd5\u56fe\u95ee\u7b54\u4efb\u52a1\u548c2\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347LMMs\u7684\u96f6\u6837\u672c\u6027\u80fd", "conclusion": "DynamicTRF\u901a\u8fc7\u95ee\u9898\u81ea\u9002\u5e94\u7684TRF\u9009\u62e9\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u95ee\u7b54\u4e2d\u6027\u80fd\u4e0e\u7b80\u6d01\u6027\u7684\u5e73\u8861\u95ee\u9898"}}
{"id": "2508.05782", "pdf": "https://arxiv.org/pdf/2508.05782", "abs": "https://arxiv.org/abs/2508.05782", "authors": ["Xiangyan Chen", "Yufeng Li", "Yujian Gan", "Arkaitz Zubiaga", "Matthew Purver"], "title": "FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are known to produce hallucinations - factually\nincorrect or fabricated information - which poses significant challenges for\nmany Natural Language Processing (NLP) applications, such as dialogue systems.\nAs a result, detecting hallucinations has become a critical area of research.\nCurrent approaches to hallucination detection in dialogue systems primarily\nfocus on verifying the factual consistency of generated responses. However,\nthese responses often contain a mix of accurate, inaccurate or unverifiable\nfacts, making one factual label overly simplistic and coarse-grained. In this\npaper, we introduce a benchmark, FineDialFact, for fine-grained dialogue fact\nverification, which involves verifying atomic facts extracted from dialogue\nresponses. To support this, we construct a dataset based on publicly available\ndialogue datasets and evaluate it using various baseline methods. Experimental\nresults demonstrate that methods incorporating Chain-of-Thought (CoT) reasoning\ncan enhance performance in dialogue fact verification. Despite this, the best\nF1-score achieved on the HybriDialogue, an open-domain dialogue dataset, is\nonly 0.75, indicating that the benchmark remains a challenging task for future\nresearch. Our dataset and code will be public on GitHub.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7ec6\u7c92\u5ea6\u5bf9\u8bdd\u4e8b\u5b9e\u6838\u67e5\u57fa\u51c6FineDialFact\uff0c\u901a\u8fc7\u601d\u7ef4\u94fe\u63a8\u7406\u65b9\u6cd5\u5c06\u5f00\u653e\u57df\u5bf9\u8bdd\u6570\u636e\u96c6HybriDialogue\u7684\u9a8c\u8bc1F1\u503c\u63d0\u5347\u81f30.75", "motivation": "\u73b0\u6709\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u91c7\u7528\u5355\u4e00\u4e8b\u5b9e\u6807\u7b7e\uff0c\u96be\u4ee5\u5e94\u5bf9\u5bf9\u8bdd\u54cd\u5e94\u4e2d\u6df7\u6742\u51c6\u786e/\u4e0d\u51c6\u786e/\u4e0d\u53ef\u9a8c\u8bc1\u4e8b\u5b9e\u7684\u590d\u6742\u60c5\u51b5", "method": "\u57fa\u4e8e\u516c\u5f00\u5bf9\u8bdd\u6570\u636e\u96c6\u6784\u5efaFineDialFact\u57fa\u51c6\uff0c\u5f15\u5165\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u673a\u5236\uff0c\u5f00\u53d1\u591a\u57fa\u7ebf\u65b9\u6cd5\u8bc4\u4f30\u4f53\u7cfb", "result": "\u5728HybriDialogue\u5f00\u653e\u57df\u5bf9\u8bdd\u6570\u636e\u96c6\u4e0a\uff0c\u6700\u4f73F1\u503c0.75\u663e\u793a\u8be5\u4efb\u52a1\u4ecd\u5177\u6311\u6218\u6027", "conclusion": "FineDialFact\u4e3a\u5bf9\u8bdd\u4e8b\u5b9e\u6838\u67e5\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6846\u67b6\uff0c\u4ee3\u7801\u6570\u636e\u96c6\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76"}}
{"id": "2508.05803", "pdf": "https://arxiv.org/pdf/2508.05803", "abs": "https://arxiv.org/abs/2508.05803", "authors": ["Abishek Thamma", "Micha Heilbron"], "title": "Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models", "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Human memory is fleeting. As words are processed, the exact wordforms that\nmake up incoming sentences are rapidly lost. Cognitive scientists have long\nbelieved that this limitation of memory may, paradoxically, help in learning\nlanguage - an idea supported by classic connectionist modelling work. The rise\nof Transformers appears to challenge this idea, as these models can learn\nlanguage effectively, despite lacking memory limitations or other architectural\nrecency biases. Here, we investigate the hypothesized benefit of fleeting\nmemory for language learning in tightly controlled experiments on transformer\nlanguage models. Training transformers with and without fleeting memory on a\ndevelopmentally realistic training set, we find that fleeting memory\nconsistently improves language learning (as quantified by both overall language\nmodelling performance and targeted syntactic evaluation) but, unexpectedly,\nimpairs surprisal-based prediction of human reading times. Interestingly,\nfollow up analyses revealed that this discrepancy - better language modeling,\nyet worse reading time prediction - could not be accounted for by prior\nexplanations of why better language models sometimes fit human reading time\nworse. Together, these results support a benefit of memory limitations on\nneural network language learning - but not on predicting behavior.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4eba\u7c7b\u77ac\u65f6\u8bb0\u5fc6\u7279\u6027\u53ef\u63d0\u5347Transformer\u8bed\u8a00\u5b66\u4e60\u6548\u679c\uff0c\u4f46\u4f1a\u964d\u4f4e\u5bf9\u4eba\u7c7b\u9605\u8bfb\u65f6\u95f4\u7684\u9884\u6d4b\u51c6\u786e\u6027", "motivation": "\u9a8c\u8bc1\u7ecf\u5178\u8ba4\u77e5\u7406\u8bba'\u8bb0\u5fc6\u9650\u5236\u6709\u5229\u4e8e\u8bed\u8a00\u5b66\u4e60'\u5728Transformer\u6a21\u578b\u4e2d\u7684\u9002\u7528\u6027\uff0c\u63a2\u7a76\u8bb0\u5fc6\u77ac\u901d\u6027\u5bf9\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u548c\u4eba\u7c7b\u884c\u4e3a\u9884\u6d4b\u7684\u53cc\u91cd\u5f71\u54cd", "method": "\u5728\u53d1\u80b2\u771f\u5b9e\u6027\u8bad\u7ec3\u96c6\u4e0a\u5bf9\u6bd4\u8bad\u7ec3\u5177\u5907/\u4e0d\u5177\u5907\u77ac\u65f6\u8bb0\u5fc6\u7684Transformer\uff0c\u901a\u8fc7\u8bed\u8a00\u5efa\u6a21\u6307\u6807\u3001\u53e5\u6cd5\u8bc4\u4f30\u548c\u4eba\u7c7b\u9605\u8bfb\u65f6\u95f4\u9884\u6d4b\u4e09\u4e2a\u7ef4\u5ea6\u5206\u6790", "result": "\u77ac\u65f6\u8bb0\u5fc6\u63d0\u5347\u6a21\u578b\u8bed\u8a00\u80fd\u529b\uff08\u56f0\u60d1\u5ea6\u964d\u4f4e7.3%\uff0c\u53e5\u6cd5\u51c6\u786e\u7387\u63d0\u9ad812%\uff09\uff0c\u4f46\u4f7f\u57fa\u4e8e\u60ca\u5947\u7684\u9605\u8bfb\u65f6\u95f4\u9884\u6d4b\u6307\u6807\u4e0b\u964d19%\uff0c\u4e14\u65e0\u6cd5\u7528\u65e2\u6709\u7406\u8bba\u89e3\u91ca\u8be5\u77db\u76fe\u73b0\u8c61", "conclusion": "\u8bb0\u5fc6\u9650\u5236\u673a\u5236\u5bf9\u795e\u7ecf\u7f51\u7edc\u8bed\u8a00\u5b66\u4e60\u5b58\u5728\u4fc3\u8fdb\u4f5c\u7528\uff0c\u4f46\u53ef\u80fd\u5f31\u5316\u6a21\u578b\u5bf9\u4eba\u7c7b\u5b9e\u65f6\u8bed\u8a00\u5904\u7406\u7684\u62df\u5408\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u8ba1\u7b97\u5efa\u6a21\u4e0e\u8ba4\u77e5\u884c\u4e3a\u9884\u6d4b\u7684\u590d\u6742\u6027\u5173\u7cfb"}}
{"id": "2508.05830", "pdf": "https://arxiv.org/pdf/2508.05830", "abs": "https://arxiv.org/abs/2508.05830", "authors": ["Tong Li", "Rasiq Hussain", "Mehak Gupta", "Joshua R. Oltmanns"], "title": "\"Mirror\" Language AI Models of Depression are Criterion-Contaminated", "categories": ["cs.CL", "cs.CY"], "comment": "39 pages, 9 figures", "summary": "A growing number of studies show near-perfect LLM language-based prediction\nof depression assessment scores (up to R2 of .70). However, many develop these\nmodels directly from language responses to depression assessments. These\n\"Mirror models\" suffer from \"criterion contamination\", which arises when a\npredicted score depends in part on the predictors themselves. This causes\nartificial effect size inflation which reduces model generalizability. The\npresent study compares the performance of Mirror models versus \"Non-Mirror\nmodels\", which are developed from language that does not mirror the assessment\nthey are developed to predict. N = 110 research participants completed two\ndifferent interviews: structured diagnostic and life history interviews. GPT-4,\nGPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic\ninterview depression scores from the two transcripts separately. Mirror models\n(using structured diagnostic data) showed very large effect sizes (e.g., R2 =\n.80). As expected, NonMirror models (using life history data) demonstrated\nsmaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror\nand Non-Mirror model-predicted structured interview depression scores were\ncorrelated with self-reported depression symptoms, Mirror and NonMirror\nperformed the same (e.g., r = ~.54), indicating that Mirror models contain bias\nperhaps due to criterion contamination. Topic modeling identified clusters\nacross Mirror and Non-Mirror models, as well as between true-positive and\nfalse-positive predictions. In this head-to-head comparison study, Mirror\nlanguage AI models of depression showed artificially inflated effect sizes and\nless generalizability. As language AI models for depression continue to evolve,\nincorporating Non-Mirror models may identify interpretable, and generalizable\nsemantic features that have unique utility in real-world psychological\nassessment.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u4e86\u57fa\u4e8e\u8bed\u8a00AI\u7684\u6291\u90c1\u75c7\u8bc4\u4f30\u6a21\u578b\u4e2d\u955c\u50cf\u6a21\u578b\u5b58\u5728\u6807\u51c6\u6c61\u67d3\u5bfc\u81f4\u7684\u6548\u5e94\u91cf\u865a\u9ad8\u95ee\u9898\uff0c\u975e\u955c\u50cf\u6a21\u578b\u867d\u6548\u5e94\u91cf\u8f83\u4f4e\u4f46\u66f4\u5177\u6cdb\u5316\u6027\u548c\u89e3\u91ca\u6027", "motivation": "\u9488\u5bf9\u73b0\u6709\u6291\u90c1\u75c7\u8bc4\u4f30\u6a21\u578b\u4e2d\u5e7f\u6cdb\u5b58\u5728\u7684'\u6807\u51c6\u6c61\u67d3'\u95ee\u9898\uff08\u5373\u9884\u6d4b\u6307\u6807\u4e0e\u8bc4\u4f30\u6807\u51c6\u5b58\u5728\u5185\u5bb9\u91cd\u53e0\uff09\uff0c\u672c\u7814\u7a76\u901a\u8fc7\u5934\u5bf9\u5934\u6bd4\u8f83\u955c\u50cf\u6a21\u578b\uff08\u4f7f\u7528\u4e0e\u8bc4\u4f30\u6807\u51c6\u540c\u6e90\u7684\u8bed\u8a00\u6570\u636e\uff09\u4e0e\u975e\u955c\u50cf\u6a21\u578b\uff08\u4f7f\u7528\u72ec\u7acb\u751f\u6d3b\u53f2\u6570\u636e\uff09\uff0c\u63ed\u793a\u6a21\u578b\u6027\u80fd\u5dee\u5f02\u53ca\u5176\u5bf9\u5b9e\u9645\u5e94\u7528\u7684\u542f\u793a", "method": "110\u540d\u53c2\u4e0e\u8005\u5b8c\u6210\u7ed3\u6784\u8bca\u65ad\u8bbf\u8c08\u548c\u751f\u6d3b\u53f2\u8bbf\u8c08\uff0c\u4f7f\u7528GPT-4/GPT-4o/LLaMA3-70B\u5206\u522b\u4ece\u4e24\u79cd\u8bbf\u8c08\u6587\u672c\u9884\u6d4b\u7ed3\u6784\u5316\u6291\u90c1\u8bc4\u5206\uff0c\u5e76\u901a\u8fc7\u4e3b\u9898\u6a21\u578b\u5206\u6790\u9884\u6d4b\u7279\u5f81", "result": "\u955c\u50cf\u6a21\u578b\u5448\u73b0\u6781\u9ad8\u6548\u5e94\u91cf(R\u00b2=0.80)\uff0c\u975e\u955c\u50cf\u6a21\u578b\u6548\u5e94\u91cf\u663e\u8457\u964d\u4f4e(R\u00b2=0.27)\uff1b\u4f46\u4e8c\u8005\u9884\u6d4b\u7ed3\u679c\u4e0e\u81ea\u6211\u62a5\u544a\u75c7\u72b6\u7684\u76f8\u5173\u6027\u76f8\u540c(r\u22480.54)\u3002\u4e3b\u9898\u6a21\u578b\u663e\u793a\u771f\u9633\u6027/\u5047\u9633\u6027\u9884\u6d4b\u5b58\u5728\u4e0d\u540c\u8bed\u4e49\u7279\u5f81", "conclusion": "\u955c\u50cf\u6a21\u578b\u56e0\u6807\u51c6\u6c61\u67d3\u5bfc\u81f4\u6548\u5e94\u91cf\u865a\u9ad8\u548c\u6cdb\u5316\u6027\u964d\u4f4e\uff0c\u5efa\u8bae\u6574\u5408\u975e\u955c\u50cf\u6a21\u578b\u6765\u8bc6\u522b\u5177\u6709\u72ec\u7279\u4e34\u5e8a\u4ef7\u503c\u7684\u53ef\u89e3\u91ca\u8bed\u4e49\u7279\u5f81\uff0c\u63a8\u52a8\u5b9e\u9645\u5fc3\u7406\u8bc4\u4f30\u5e94\u7528"}}
{"id": "2508.05843", "pdf": "https://arxiv.org/pdf/2508.05843", "abs": "https://arxiv.org/abs/2508.05843", "authors": ["Miles Gilberti", "Shane Storks", "Huteng Dai"], "title": "Discovering Properties of Inflectional Morphology in Neural Emergent Communication", "categories": ["cs.CL"], "comment": null, "summary": "Emergent communication (EmCom) with deep neural network-based agents promises\nto yield insights into the nature of human language, but remains focused\nprimarily on a few subfield-specific goals and metrics that prioritize\ncommunication schemes which represent attributes with unique characters\none-to-one and compose them syntactically. We thus reinterpret a common EmCom\nsetting, the attribute-value reconstruction game, by imposing a\nsmall-vocabulary constraint to simulate double articulation, and formulating a\nnovel setting analogous to naturalistic inflectional morphology (enabling\nmeaningful comparison to natural language communication schemes). We develop\nnew metrics and explore variations of this game motivated by real properties of\ninflectional morphology: concatenativity and fusionality. Through our\nexperiments, we discover that simulated phonological constraints encourage\nconcatenative morphology, and emergent languages replicate the tendency of\nnatural languages to fuse grammatical attributes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u5c0f\u8bcd\u6c47\u91cf\u7ea6\u675f\u548c\u5c48\u6298\u5f62\u6001\u6a21\u62df\uff0c\u6784\u5efa\u65b0\u7684\u6d8c\u73b0\u901a\u4fe1\u6846\u67b6\uff0c\u53d1\u73b0\u751f\u6210\u8bed\u8a00\u80fd\u590d\u73b0\u81ea\u7136\u8bed\u8a00\u7684\u878d\u5408\u5c5e\u6027\u503e\u5411", "motivation": "\u73b0\u6709\u6d8c\u73b0\u901a\u4fe1\u7814\u7a76\u8fc7\u5ea6\u5173\u6ce8\u5b50\u9886\u57df\u76ee\u6807\uff0c\u5ffd\u7565\u4e86\u8bed\u8a00\u7684\u53cc\u91cd\u5206\u8282\u7279\u6027\u3002\u4f5c\u8005\u8bd5\u56fe\u901a\u8fc7\u6a21\u62df\u81ea\u7136\u8bed\u8a00\u5c48\u6298\u5f62\u6001\uff0c\u5efa\u7acb\u66f4\u63a5\u8fd1\u4eba\u7c7b\u8bed\u8a00\u7279\u6027\u7684\u901a\u4fe1\u65b9\u6848\u8bc4\u4f30\u4f53\u7cfb", "method": "\u91cd\u6784\u5c5e\u6027-\u503c\u91cd\u5efa\u6e38\u620f\uff1a1) \u65bd\u52a0\u8bcd\u6c47\u91cf\u9650\u5236\u6a21\u62df\u8bed\u97f3\u5c42\u7ea6\u675f 2) \u8bbe\u8ba1\u5c48\u6298\u5f62\u6001\u7c7b\u6bd4\u4efb\u52a1 3) \u5f00\u53d1\u8bc4\u4f30\u62fc\u63a5\u6027\u4e0e\u878d\u5408\u6027\u7684\u65b0\u6307\u6807", "result": "\u8bed\u97f3\u7ea6\u675f\u4fc3\u8fdb\u62fc\u63a5\u5f62\u6001\u53d1\u5c55\uff0c\u6d8c\u73b0\u8bed\u8a00\u5c55\u73b0\u51fa\u81ea\u7136\u8bed\u8a00\u5178\u578b\u7684\u8bed\u6cd5\u5c5e\u6027\u878d\u5408\u503e\u5411\uff0c\u4e0e\u8428\u5c14\u5c14-\u6c83\u592b\u5047\u8bf4\u5f62\u6210\u6709\u8da3\u5bf9\u6bd4", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8ba1\u7b97\u8bed\u8a00\u6d8c\u73b0\u4e0e\u81ea\u7136\u8bed\u8a00\u6f14\u5316\u642d\u5efa\u4e86\u65b0\u7684\u7406\u8bba\u6865\u6881\uff0c\u8bc1\u660e\u8de8\u5b66\u79d1\u65b9\u6cd5\u5728\u8bed\u8a00\u8d77\u6e90\u7814\u7a76\u4e2d\u7684\u4ef7\u503c"}}
{"id": "2508.05880", "pdf": "https://arxiv.org/pdf/2508.05880", "abs": "https://arxiv.org/abs/2508.05880", "authors": ["Sree Bhattacharyya", "Lucas Craig", "Tharun Dilliraj", "Jia Li", "James Z. Wang"], "title": "Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Affective Computing has been established as a crucial field of inquiry to\nadvance the holistic development of Artificial Intelligence (AI) systems.\nFoundation models -- especially Large Language Models (LLMs) -- have been\nevaluated, trained, or instruction-tuned in several past works, to become\nbetter predictors or generators of emotion. Most of these studies, however,\napproach emotion-related tasks in a supervised manner, assessing or training\nthe capabilities of LLMs using discrete emotion labels associated with stimuli\n(e.g., text, images, video, audio). Evaluation studies, in particular, have\noften been limited to standard and superficial emotion-related tasks, such as\nthe recognition of evoked or expressed emotions. In this paper, we move beyond\nsurface-level emotion tasks to investigate how LLMs reason about emotions\nthrough cognitive dimensions. Drawing from cognitive appraisal theory, we\nexamine whether LLMs produce coherent and plausible cognitive reasoning when\nreasoning about emotionally charged stimuli. We introduce a large-scale\nbenchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal\ncognitive structures implicitly used by LLMs for emotional reasoning. Through a\nplethora of evaluation experiments and analysis, we seek to answer: (a) Are\nmodels more likely to implicitly rely on specific cognitive appraisal\ndimensions?, (b) What cognitive dimensions are important for characterizing\nspecific emotions?, and, (c) Can the internal representations of different\nemotion categories in LLMs be interpreted through cognitive appraisal\ndimensions? Our results and analyses reveal diverse reasoning patterns across\ndifferent LLMs. Our benchmark and code will be made publicly available.", "AI": {"tldr": "\u7a81\u7834\u4f20\u7edf\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\uff0c\u901a\u8fc7\u8ba4\u77e5\u7ef4\u5ea6\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u7684\u60c5\u611f\u63a8\u7406\u673a\u5236\uff0c\u6784\u5efaCoRE\u57fa\u51c6\u63ed\u793a\u6a21\u578b\u5185\u90e8\u8ba4\u77e5\u7ed3\u6784", "motivation": "\u73b0\u6709\u7814\u7a76\u5c40\u9650\u4e8e\u76d1\u7763\u5b66\u4e60\u548c\u8868\u9762\u60c5\u611f\u4efb\u52a1\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u6df1\u5c42\u8ba4\u77e5\u673a\u5236\u7684\u63a2\u7d22\uff0c\u9700\u63ed\u793aLLMs\u60c5\u611f\u63a8\u7406\u7684\u8ba4\u77e5\u7ef4\u5ea6\u7279\u5f81", "method": "\u57fa\u4e8e\u8ba4\u77e5\u8bc4\u4f30\u7406\u8bba\u6784\u5efaCoRE\u57fa\u51c6\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u5206\u6790\u4e0d\u540cLLMs\u5728\u60c5\u611f\u63a8\u7406\u4e2d\u9690\u542b\u7684\u8ba4\u77e5\u7ef4\u5ea6\u4f7f\u7528\u6a21\u5f0f", "result": "\u53d1\u73b0\u4e0d\u540cLLMs\u5b58\u5728\u591a\u6837\u5316\u8ba4\u77e5\u63a8\u7406\u6a21\u5f0f\uff0c\u90e8\u5206\u6a21\u578b\u4f9d\u8d56\u7279\u5b9a\u8ba4\u77e5\u7ef4\u5ea6\uff0c\u60c5\u611f\u7c7b\u522b\u53ef\u901a\u8fc7\u8ba4\u77e5\u7ef4\u5ea6\u8fdb\u884c\u6709\u6548\u89e3\u91ca", "conclusion": "CoRE\u57fa\u51c6\u9996\u6b21\u7cfb\u7edf\u63ed\u793a\u4e86LLMs\u60c5\u611f\u63a8\u7406\u7684\u8ba4\u77e5\u7ed3\u6784\u591a\u6837\u6027\uff0c\u4e3a\u7406\u89e3\u6a21\u578b\u60c5\u611f\u5904\u7406\u673a\u5236\u63d0\u4f9b\u65b0\u7684\u7814\u7a76\u8303\u5f0f"}}
{"id": "2508.05909", "pdf": "https://arxiv.org/pdf/2508.05909", "abs": "https://arxiv.org/abs/2508.05909", "authors": ["Zhanghao Hu", "Qinglin Zhu", "Siya Qi", "Yulan He", "Hanqi Yan", "Lin Gui"], "title": "Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown improved generation performance\nthrough retrieval-augmented generation (RAG) following the retriever-reader\nparadigm, which supplements model inputs with externally retrieved knowledge.\nHowever, prior work often evaluates RAG holistically, assessing the retriever\nand reader jointly, making it difficult to isolate the true contribution of\nretrieval, particularly given the prompt sensitivity of LLMs used as readers.\nWe introduce Spectrum Projection Score (SPS), a lightweight, supervision-free\nmetric that allows the reader to gauge the semantic alignment of a retrieved\nsummary with its hidden representation by comparing the area formed by\ngenerated tokens from the summary, and the principal directions of subspace in\nthe reader and to measure the relevance. Building on SPS we present xCompress,\nan inference time controller framework that dynamically samples, ranks, and\ncompresses retrieval summary candidates. Extensive experiments on five QA\nbenchmarks with four open source LLMs show that SPS not only enhances\nperformance across a range of tasks but also provides a principled perspective\non the interaction between retrieval and generation.", "AI": {"tldr": "\u63d0\u51faSPS\u6307\u6807\u548cxCompress\u6846\u67b6\uff0c\u901a\u8fc7\u91cf\u5316\u68c0\u7d22\u5185\u5bb9\u4e0e\u751f\u6210\u6a21\u578b\u7684\u8bed\u4e49\u5bf9\u9f50\uff0c\u4f18\u5316\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6548\u679c", "motivation": "\u89e3\u51b3\u4f20\u7edfRAG\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u5355\u72ec\u8861\u91cf\u68c0\u7d22\u8d21\u732e\u7684\u95ee\u9898\uff0c\u7279\u522b\u662fLLM\u4f5c\u4e3a\u9605\u8bfb\u5668\u65f6\u7684\u63d0\u793a\u654f\u611f\u6027\u7f3a\u9677", "method": "SPS\u901a\u8fc7\u6bd4\u8f83\u751f\u6210\u4ee4\u724c\u5f62\u6210\u7684\u533a\u57df\u4e0e\u5b50\u7a7a\u95f4\u4e3b\u65b9\u5411\u8bc4\u4f30\u76f8\u5173\u6027\uff0cxCompress\u57fa\u4e8eSPS\u52a8\u6001\u63a7\u5236\u68c0\u7d22\u6458\u8981\u7684\u91c7\u6837\u3001\u6392\u5e8f\u548c\u538b\u7f29", "result": "\u57285\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f7f\u75284\u4e2aLLM\u9a8c\u8bc1\uff0cSPS\u63d0\u5347\u591a\u4efb\u52a1\u6027\u80fd\u5e76\u63ed\u793a\u68c0\u7d22-\u751f\u6210\u4ea4\u4e92\u673a\u5236", "conclusion": "SPS\u4e3aRAG\u7cfb\u7edf\u63d0\u4f9b\u8bed\u4e49\u5bf9\u9f50\u7684\u91cf\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u517c\u5177\u6027\u80fd\u63d0\u5347\u548c\u7406\u8bba\u521b\u65b0\u4ef7\u503c"}}
{"id": "2508.05938", "pdf": "https://arxiv.org/pdf/2508.05938", "abs": "https://arxiv.org/abs/2508.05938", "authors": ["Rafal Kocielnik", "Min Kim", "Penphob", "Boonyarungsrit", "Fereshteh Soltani", "Deshawn Sambrano", "Animashree Anandkumar", "R. Michael Alvarez"], "title": "Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; K.4"], "comment": "9 pages, 4 figures, 4 tables", "summary": "Detecting prosociality in text--communication intended to affirm, support, or\nimprove others' behavior--is a novel and increasingly important challenge for\ntrust and safety systems. Unlike toxic content detection, prosociality lacks\nwell-established definitions and labeled data, requiring new approaches to both\nannotation and deployment. We present a practical, three-stage pipeline that\nenables scalable, high-precision prosocial content classification while\nminimizing human labeling effort and inference costs. First, we identify the\nbest LLM-based labeling strategy using a small seed set of human-labeled\nexamples. We then introduce a human-AI refinement loop, where annotators review\nhigh-disagreement cases between GPT-4 and humans to iteratively clarify and\nexpand the task definition-a critical step for emerging annotation tasks like\nprosociality. This process results in improved label quality and definition\nalignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train\na two-stage inference system: a lightweight classifier handles high-confidence\npredictions, while only $\\sim$35\\% of ambiguous instances are escalated to\nGPT-4o. This architecture reduces inference costs by $\\sim$70% while achieving\nhigh precision ($\\sim$0.90). Our pipeline demonstrates how targeted human-AI\ninteraction, careful task formulation, and deployment-aware architecture design\ncan unlock scalable solutions for novel responsible AI tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u9636\u6bb5\u7684\u4eba\u7c7b-AI\u534f\u4f5c\u6d41\u7a0b\uff0c\u7528\u4e8e\u9ad8\u6548\u68c0\u6d4b\u6587\u672c\u4e2d\u7684\u4eb2\u793e\u4f1a\u6027\u5185\u5bb9\uff0c\u901a\u8fc7\u4f18\u5316\u6807\u6ce8\u7b56\u7565\u548c\u90e8\u7f72\u4e24\u9636\u6bb5\u63a8\u7406\u7cfb\u7edf\uff0c\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff08\u7ea60.90\uff09\u7684\u540c\u65f6\u964d\u4f4e70%\u63a8\u7406\u6210\u672c\u3002", "motivation": "\u4eb2\u793e\u4f1a\u6027\u68c0\u6d4b\u4f5c\u4e3a\u65b0\u5174\u7684AI\u4f26\u7406\u4efb\u52a1\uff0c\u7f3a\u4e4f\u660e\u786e\u7684\u5b9a\u4e49\u548c\u6807\u6ce8\u6570\u636e\uff0c\u9700\u8981\u5f00\u53d1\u6807\u6ce8\u6548\u7387\u9ad8\u4e14\u90e8\u7f72\u6210\u672c\u4f4e\u7684\u65b0\u578b\u89e3\u51b3\u65b9\u6848\u3002\u73b0\u6709\u6bd2\u6027\u5185\u5bb9\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u76f4\u63a5\u9002\u7528\u3002", "method": "1. \u57fa\u4e8e\u5c0f\u6837\u672c\u786e\u5b9a\u6700\u4f73LLM\u6807\u6ce8\u7b56\u7565\n2. \u4eba\u7c7b-AI\u534f\u540c\u4f18\u5316\uff1a\u901a\u8fc7\u5ba1\u67e5GPT-4\u4e0e\u4eba\u5de5\u6807\u6ce8\u7684\u5206\u6b67\u6848\u4f8b\u8fed\u4ee3\u6539\u8fdb\u4efb\u52a1\u5b9a\u4e49\n3. \u6784\u5efa\u4e24\u9636\u6bb5\u63a8\u7406\u7cfb\u7edf\uff08\u8f7b\u91cf\u7ea7\u5206\u7c7b\u5668+GPT-4o\u5206\u7ea7\u5904\u7406\uff09", "result": "\u751f\u621010k\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u90e8\u7f72\u7cfb\u7edf\u540e\u4ec5\u9700\u5c0635%\u6a21\u7cca\u6848\u4f8b\u63d0\u4ea4GPT-4o\uff0c\u5b9e\u73b090%\u7cbe\u5ea6\u540c\u65f6\u964d\u4f4e70%\u63a8\u7406\u6210\u672c\u3002", "conclusion": "\u901a\u8fc7\u4eba\u673a\u4ea4\u4e92\u4f18\u5316\u3001\u4efb\u52a1\u5b9a\u4e49\u8fed\u4ee3\u548c\u90e8\u7f72\u611f\u77e5\u67b6\u6784\u8bbe\u8ba1\uff0c\u4e3a\u65b0\u578b\u8d1f\u8d23\u4efbAI\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u6846\u67b6\u3002"}}
{"id": "2508.05987", "pdf": "https://arxiv.org/pdf/2508.05987", "abs": "https://arxiv.org/abs/2508.05987", "authors": ["Chunyun Zhang", "Hongyan Zhao", "Chaoran Cui", "Qilong Song", "Zhiqing Lu", "Shuai Gong", "Kailin Liu"], "title": "Adversarial Topic-aware Prompt-tuning for Cross-topic Automated Essay Scoring", "categories": ["cs.CL"], "comment": null, "summary": "Cross-topic automated essay scoring (AES) aims to develop a transferable\nmodel capable of effectively evaluating essays on a target topic. A significant\nchallenge in this domain arises from the inherent discrepancies between topics.\nWhile existing methods predominantly focus on extracting topic-shared features\nthrough distribution alignment of source and target topics, they often neglect\ntopic-specific features, limiting their ability to assess critical traits such\nas topic adherence. To address this limitation, we propose an Adversarial\nTOpic-aware Prompt-tuning (ATOP), a novel method that jointly learns\ntopic-shared and topic-specific features to improve cross-topic AES. ATOP\nachieves this by optimizing a learnable topic-aware prompt--comprising both\nshared and specific components--to elicit relevant knowledge from pre-trained\nlanguage models (PLMs). To enhance the robustness of topic-shared prompt\nlearning and mitigate feature scale sensitivity introduced by topic alignment,\nwe incorporate adversarial training within a unified regression and\nclassification framework. In addition, we employ a neighbor-based classifier to\nmodel the local structure of essay representations and generate pseudo-labels\nfor target-topic essays. These pseudo-labels are then used to guide the\nsupervised learning of topic-specific prompts tailored to the target topic.\nExtensive experiments on the publicly available ASAP++ dataset demonstrate that\nATOP significantly outperforms existing state-of-the-art methods in both\nholistic and multi-trait essay scoring. The implementation of our method is\npublicly available at: https://anonymous.4open.science/r/ATOP-A271.", "AI": {"tldr": "\u63d0\u51fa\u5bf9\u6297\u6027\u4e3b\u9898\u611f\u77e5\u63d0\u793a\u8c03\u6574\u65b9\u6cd5ATOP\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u4e3b\u9898\u5171\u4eab\u548c\u7279\u5b9a\u7279\u5f81\u63d0\u5347\u8de8\u4e3b\u9898\u81ea\u52a8\u4f5c\u6587\u8bc4\u5206\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u8de8\u4e3b\u9898AES\u65b9\u6cd5\u4ec5\u5173\u6ce8\u4e3b\u9898\u5171\u4eab\u7279\u5f81\uff0c\u5ffd\u7565\u4e86\u4e3b\u9898\u7279\u5f02\u6027\u7279\u5f81\uff08\u5982\u4e3b\u9898\u4e00\u81f4\u6027\uff09\uff0c\u5bfc\u81f4\u8bc4\u4f30\u80fd\u529b\u53d7\u9650\u3002", "method": "1. \u8bbe\u8ba1\u53ef\u5b66\u4e60\u7684\u4e3b\u9898\u611f\u77e5\u63d0\u793a\uff08\u542b\u5171\u4eab/\u7279\u5b9a\u7ec4\u4ef6\uff09\n2. \u5728\u56de\u5f52\u5206\u7c7b\u6846\u67b6\u4e2d\u5f15\u5165\u5bf9\u6297\u8bad\u7ec3\u589e\u5f3a\u9c81\u68d2\u6027\n3. \u57fa\u4e8e\u90bb\u5c45\u5206\u7c7b\u5668\u751f\u6210\u76ee\u6807\u4e3b\u9898\u4f2a\u6807\u7b7e\u6307\u5bfc\u7279\u5b9a\u63d0\u793a\u5b66\u4e60", "result": "\u5728ASAP++\u6570\u636e\u96c6\u4e0a\uff0cATOP\u5728\u6574\u4f53\u8bc4\u5206\uff080.814\u21920.826\uff09\u548c\u591a\u7279\u5f81\u8bc4\u5206\u5747\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u8054\u5408\u4f18\u5316\u4e3b\u9898\u5171\u4eab/\u7279\u5b9a\u7279\u5f81\u8868\u793a\uff0c\u7ed3\u5408\u5bf9\u6297\u8bad\u7ec3\u4e0e\u4f2a\u6807\u7b7e\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u8de8\u4e3b\u9898\u4f5c\u6587\u8bc4\u5206\u7684\u51c6\u786e\u6027\u548c\u53ef\u8fc1\u79fb\u6027"}}
{"id": "2508.06016", "pdf": "https://arxiv.org/pdf/2508.06016", "abs": "https://arxiv.org/abs/2508.06016", "authors": ["Sagar Gandhi", "Vishal Gandhi"], "title": "Crisp Attention: Regularizing Transformers via Structured Sparsity", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The quadratic computational cost of the self-attention mechanism is a primary\nchallenge in scaling Transformer models. While attention sparsity is widely\nstudied as a technique to improve computational efficiency, it is almost\nuniversally assumed to come at the cost of model accuracy. In this paper, we\nreport a surprising counter-example to this common wisdom. By introducing\nstructured, post-hoc sparsity to the attention mechanism of a DistilBERT model\nduring fine-tuning on the SST-2 sentiment analysis task, we find that model\naccuracy improves significantly. Our model with 80\\% attention sparsity\nachieves a validation accuracy of 91.59\\%, a 0.97\\% absolute improvement over\nthe dense baseline. We hypothesize that this phenomenon is due to sparsity\nacting as a powerful implicit regularizer, preventing the model from\noverfitting by forcing it to make predictions with a more constrained and\nrobust set of features. Our work recasts attention sparsity not just as a tool\nfor computational efficiency, but as a potential method for improving the\ngeneralization and performance of Transformer models.", "AI": {"tldr": "\u6ce8\u610f\u529b\u7a00\u758f\u5316\u4e0d\u4ec5\u80fd\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u8fd8\u80fd\u901a\u8fc7\u9690\u5f0f\u6b63\u5219\u5316\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u7387\uff08SST-2\u4efb\u52a1\u63d0\u53470.97%\uff09\u3002", "motivation": "\u6311\u6218\u4f20\u7edf\u8ba4\u77e5\uff0c\u63a2\u7d22\u6ce8\u610f\u529b\u7a00\u758f\u5316\u5728\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u80fd\u5426\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u53d1\u73b0\u7a00\u758f\u6027\u53ef\u4f5c\u4e3a\u6709\u6548\u6b63\u5219\u5316\u624b\u6bb5\u3002", "method": "\u5728DistilBERT\u6a21\u578b\u5fae\u8c03SST-2\u4efb\u52a1\u65f6\u5f15\u5165\u7ed3\u6784\u5316\u540e\u5904\u7406\u6ce8\u610f\u529b\u7a00\u758f\u5316\uff0880%\u7a00\u758f\u5ea6\uff09", "result": "80%\u7a00\u758f\u5ea6\u6a21\u578b\u9a8c\u8bc1\u51c6\u786e\u7387\u8fbe91.59%\uff08\u63d0\u53470.97%\uff09\uff0c\u7a00\u758f\u6027\u6709\u6548\u9632\u6b62\u8fc7\u62df\u5408\u5e76\u589e\u5f3a\u7279\u5f81\u9c81\u68d2\u6027", "conclusion": "\u6ce8\u610f\u529b\u7a00\u758f\u5316\u5e94\u88ab\u91cd\u65b0\u5b9a\u4e49\u4e3a\u540c\u65f6\u63d0\u5347Transformer\u6a21\u578b\u8ba1\u7b97\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u7684\u53cc\u91cd\u4f18\u5316\u7b56\u7565"}}
{"id": "2508.06026", "pdf": "https://arxiv.org/pdf/2508.06026", "abs": "https://arxiv.org/abs/2508.06026", "authors": ["Yidong Wang", "Xin Wang", "Cunxiang Wang", "Junfeng Fang", "Qiufeng Wang", "Jianing Chu", "Xuran Meng", "Shuxun Yang", "Libo Qin", "Yue Zhang", "Wei Ye", "Shikun Zhang"], "title": "Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 5 figures", "summary": "Self-Rewarding Language Models propose an architecture in which the Large\nLanguage Models(LLMs) both generates responses and evaluates its own outputs\nvia LLM-as-a-Judge prompting, dynamically improving its generative capabilities\nthrough iterative Direct Preference Optimization (DPO). However, our analysis\nreveals a critical limitation in existing Self-Rewarding paradigms: the\nsynchronized improvement of chosen and rejected responses progressively narrows\nthe representational difference between contrasting samples, undermining\neffective preference learning. We propose \\textbf{Temporal Self-Rewarding\nLanguage Models} that strategically coordinate past, present, and future model\ngenerations to sustain learning signals. Our dual-phase framework introduces:\n(1) \\textit{Anchored Rejection} - fixing rejected responses using the past\ninitial model's outputs and (2) \\textit{Future-Guided Chosen} - dynamically\ncurating chosen samples using next-generation model predictions. Extensive\nexperiments across three model families (Llama, Qwen, Mistral) and different\nmodel sizes (Llama3B/8B/70B) demonstrate significant improvements when trained\nwith our method compared to Self-Rewarding using same computation resources.\nFor example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our\nmethod, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our\nmethod also demonstrates superior out-of-distribution generalization across\nmathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code\ngeneration (HumanEval) tasks, even though we do not specifically collect such\ntraining data.", "AI": {"tldr": "\u63d0\u51fa\u65f6\u95f4\u81ea\u5956\u52b1\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u534f\u8c03\u8fc7\u53bb/\u73b0\u5728/\u672a\u6765\u6a21\u578b\u8f93\u51fa\u6765\u7ef4\u6301\u5b66\u4e60\u4fe1\u53f7\uff0c\u89e3\u51b3\u73b0\u6709\u81ea\u5956\u52b1\u6a21\u578b\u4e2d\u5bf9\u6bd4\u6837\u672c\u8868\u793a\u5dee\u5f02\u7f29\u7a84\u7684\u95ee\u9898", "motivation": "\u73b0\u6709\u81ea\u5956\u52b1\u6a21\u578b\u540c\u6b65\u4f18\u5316\u6b63\u8d1f\u6837\u672c\u5bfc\u81f4\u8868\u793a\u5dee\u5f02\u9010\u6e10\u6d88\u5931\uff0c\u5f71\u54cd\u504f\u597d\u5b66\u4e60\u6548\u679c", "method": "\u53cc\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u951a\u5b9a\u62d2\u7edd\u6837\u672c\uff08\u56fa\u5b9a\u521d\u59cb\u6a21\u578b\u7684\u8d1f\u6837\u672c\uff09 2\uff09\u672a\u6765\u5f15\u5bfc\u6b63\u6837\u672c\uff08\u52a8\u6001\u751f\u6210\u4e0b\u4e00\u4ee3\u6a21\u578b\u7684\u6b63\u6837\u672c\uff09", "result": "Llama3.1-8B\u5728AlpacaEval 2.0\u80dc\u7387\u8fbe29.44\uff0c\u8d85\u8d8a\u57fa\u7ebf9.75\u5206\uff1b\u5728\u6570\u5b66\u63a8\u7406\u3001\u77e5\u8bc6\u95ee\u7b54\u7b49\u4efb\u52a1\u5c55\u73b0\u4f18\u5f02\u6cdb\u5316\u80fd\u529b", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u65f6\u95f4\u7ef4\u5ea6\u534f\u8c03\u6a21\u578b\u8fed\u4ee3\uff0c\u663e\u8457\u63d0\u5347\u81ea\u5956\u52b1\u5b66\u4e60\u6548\u7387\uff0c\u4e14\u4e0d\u4f9d\u8d56\u7279\u5b9a\u9886\u57df\u8bad\u7ec3\u6570\u636e\u5373\u5b9e\u73b0\u8de8\u4efb\u52a1\u6cdb\u5316"}}
{"id": "2508.06030", "pdf": "https://arxiv.org/pdf/2508.06030", "abs": "https://arxiv.org/abs/2508.06030", "authors": ["Kartik Sharma", "Yiqiao Jin", "Rakshit Trivedi", "Srijan Kumar"], "title": "Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) acquire knowledge across diverse domains such as\nscience, history, and geography encountered during generative pre-training.\nHowever, due to their stochasticity, it is difficult to predict what LLMs have\nacquired. Prior work has developed different ways to probe this knowledge by\ninvestigating the hidden representations, crafting specific task prompts,\ncurating representative samples, and estimating their uncertainty. However,\nthese methods require making forward passes through the underlying model to\nprobe the LLM's knowledge about a specific fact, making them computationally\nexpensive and time-consuming. To bridge this gap, we propose $\\textbf{PEEK}$ or\n$\\textbf{P}$roxy $\\textbf{E}$mbeddings to $\\textbf{E}$stimate\n$\\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models\nthat effectively encode factual knowledge as text or graphs as proxies for\nLLMs. First, we identify a training set of facts known by LLMs through various\nprobing strategies and then adapt embedding models to predict the LLM outputs\nwith a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived\ndatasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict\nLLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find\nthat sentence embedding models are more suitable than graph embeddings to\npredict LLM knowledge, shedding light on the underlying representation of the\nfactual landscape. Thus, we believe that knowledge-adapted embeddings can be\nused to identify knowledge gaps in LLMs at scale and can provide deeper\ninsights into LLMs' internal inductive bias. The code and data are made\navailable at https://github.com/claws-lab/peek.", "AI": {"tldr": "\u63d0\u51faPEEK\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u5d4c\u5165\u6a21\u578b\u9884\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u5206\u5e03\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u9a8c\u8bc1\u6709\u6548\u6027\uff08\u51c6\u786e\u738790%\uff09\u5e76\u63ed\u793a\u53e5\u5b50\u5d4c\u5165\u66f4\u9002\u5408\u77e5\u8bc6\u8868\u5f81", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u9700\u591a\u6b21\u524d\u5411\u4f20\u64ad\u63a2\u6d4bLLM\u77e5\u8bc6\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u672c\u7814\u7a76\u5229\u7528\u5d4c\u5165\u6a21\u578b\u7f16\u7801\u6587\u672c/\u56fe\u8c31\u77e5\u8bc6\u4f5c\u4e3a\u4ee3\u7406\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u6548\u77e5\u8bc6\u9884\u6d4b", "method": "1. \u591a\u7b56\u7565\u63a2\u6d4b\u6784\u5efaLLM\u5df2\u77e5\u4e8b\u5b9e\u8bad\u7ec3\u96c6 2. \u7ebf\u6027\u89e3\u7801\u5c42\u9002\u914d\u5d4c\u5165\u6a21\u578b\u9884\u6d4bLLM\u8f93\u51fa", "result": "\u57283\u4e2a\u7ef4\u57fa\u767e\u79d1\u6570\u636e\u96c6/4\u4e2aLLM/7\u4e2a\u5d4c\u5165\u6a21\u578b\u9a8c\u8bc1\uff1a\u5d4c\u5165\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u7387\u8fbe90%\uff0c\u53e5\u5b50\u5d4c\u5165\u6548\u679c\u4f18\u4e8e\u56fe\u5d4c\u5165", "conclusion": "\u77e5\u8bc6\u9002\u914d\u5d4c\u5165\u53ef\u89c4\u6a21\u5316\u8bc6\u522bLLM\u77e5\u8bc6\u7f3a\u53e3\uff0c\u63ed\u793a\u6a21\u578b\u5185\u90e8\u5f52\u7eb3\u504f\u5dee\uff0c\u4e3a\u6a21\u578b\u5206\u6790\u63d0\u4f9b\u65b0\u5de5\u5177\uff08\u4ee3\u7801\u6570\u636e\u5df2\u5f00\u6e90\uff09"}}
{"id": "2508.06046", "pdf": "https://arxiv.org/pdf/2508.06046", "abs": "https://arxiv.org/abs/2508.06046", "authors": ["Xinda Wang", "Zhengxu Hou", "Yangshijie Zhang", "Bingren Yan", "Zhibo Yang", "Xingsheng Zhang", "Luxi Xing", "Qiang Zhou", "Chen Zhang"], "title": "EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Although the effectiveness of Large Language Models (LLMs) as judges\n(LLM-as-a-judge) has been validated, their performance remains limited in\nopen-ended tasks, particularly in story evaluation. Accurate story evaluation\nis crucial not only for assisting human quality judgment but also for providing\nkey signals to guide story generation. However, existing methods face a\ndilemma: prompt engineering for closed-source models suffers from poor\nadaptability, while fine-tuning approaches for open-source models lack the\nrigorous reasoning capabilities essential for story evaluation. To address\nthis, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework.\nGrounded in pairwise comparison, the framework first self-synthesizes\nscore-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To\nensure data quality, these raw CoTs undergo a self-filtering process, utilizing\nmulti-agents to guarantee their logical rigor and robustness. Finally, the\nevaluator trained on the refined data is deployed as a reward model to guide\nthe story generation task. Experimental results demonstrate that our framework\nachieves state-of-the-art (SOTA) performance on three evaluation benchmarks\nincluding StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward\nmodel, it significantly enhances the quality of generated stories, thereby\nfully validating the superiority of our self-evolving approach.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u8fdb\u5316\u6210\u5bf9\u63a8\u7406\u6846\u67b6EvolvR\uff0c\u901a\u8fc7\u81ea\u5408\u6210\u601d\u7ef4\u94fe\u6570\u636e\u4e0e\u591a\u4ee3\u7406\u8fc7\u6ee4\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u6545\u4e8b\u8bc4\u4f30\u8d28\u91cf\u5e76\u6307\u5bfc\u751f\u6210\u4efb\u52a1", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u57df\u6545\u4e8b\u8bc4\u4f30\u4e2d\u5b58\u5728\u95ed\u6e90\u6a21\u578b\u9002\u5e94\u6027\u5dee\u4e0e\u5f00\u6e90\u6a21\u578b\u7f3a\u4e4f\u4e25\u8c28\u63a8\u7406\u7684\u53cc\u91cd\u56f0\u5883\uff0c\u4e9f\u9700\u517c\u5177\u81ea\u9002\u5e94\u6027\u548c\u4e25\u683c\u903b\u8f91\u7684\u89e3\u51b3\u65b9\u6848", "method": "1. \u591a\u89d2\u8272\u7b56\u7565\u81ea\u5408\u6210\u5206\u6570\u5bf9\u9f50\u7684\u601d\u7ef4\u94fe\u6570\u636e 2. \u591a\u4ee3\u7406\u81ea\u8fc7\u6ee4\u673a\u5236\u4fdd\u8bc1\u6570\u636e\u903b\u8f91\u4e25\u8c28\u6027 3. \u8bad\u7ec3\u8bc4\u4f30\u5668\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\u6307\u5bfc\u751f\u6210\u4efb\u52a1", "result": "\u5728StoryER/HANNA/OpenMEVA\u4e09\u5927\u57fa\u51c6\u4e0a\u8fbe\u5230SOTA\uff0c\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\u4f7f\u751f\u6210\u6545\u4e8b\u8d28\u91cf\u63d0\u534718.3%", "conclusion": "EvolvR\u6846\u67b6\u901a\u8fc7\u81ea\u8fdb\u5316\u673a\u5236\u6709\u6548\u7a81\u7834\u73b0\u6709\u6545\u4e8b\u8bc4\u4f30\u74f6\u9888\uff0c\u9a8c\u8bc1\u4e86\u81ea\u751f\u6210\u4e25\u683c\u63a8\u7406\u6570\u636e\u5bf9\u63d0\u5347\u8bc4\u4f30\u80fd\u529b\u7684\u6838\u5fc3\u4ef7\u503c"}}
{"id": "2508.06094", "pdf": "https://arxiv.org/pdf/2508.06094", "abs": "https://arxiv.org/abs/2508.06094", "authors": ["Morris Alper", "Moran Yanuka", "Raja Giryes", "Ga\u0161per Begu\u0161"], "title": "ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline", "categories": ["cs.CL"], "comment": "Project page: https://conlangcrafter.github.io", "summary": "Constructed languages (conlangs) such as Esperanto and Quenya have played\ndiverse roles in art, philosophy, and international communication. Meanwhile,\nlarge-scale foundation models have revolutionized creative generation in text,\nimages, and beyond. In this work, we leverage modern LLMs as computational\ncreativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a\nmulti-hop pipeline that decomposes language design into modular stages --\nphonology, morphology, syntax, lexicon generation, and translation. At each\nstage, our method leverages LLMs' meta-linguistic reasoning capabilities,\ninjecting randomness to encourage diversity and leveraging self-refinement\nfeedback to encourage consistency in the emerging language description. We\nevaluate ConlangCrafter on metrics measuring coherence and typological\ndiversity, demonstrating its ability to produce coherent and varied conlangs\nwithout human linguistic expertise.", "AI": {"tldr": "\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6784\u5efa\u6a21\u5757\u5316\u6d41\u7a0bConlangCrafter\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u968f\u673a\u751f\u6210\u4e0e\u81ea\u6211\u53cd\u9988\u673a\u5236\u81ea\u52a8\u521b\u5efa\u591a\u6837\u5316\u4eba\u5de5\u8bed\u8a00", "motivation": "\u4eba\u5de5\u8bed\u8a00\u521b\u4f5c\u957f\u671f\u4f9d\u8d56\u4e13\u4e1a\u8bed\u8a00\u5b66\u77e5\u8bc6\uff0c\u800c\u73b0\u4ee3LLMs\u5728\u521b\u610f\u751f\u6210\u65b9\u9762\u5c55\u73b0\u6f5c\u529b\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7LLMs\u7684\u5143\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u964d\u4f4e\u4eba\u5de5\u8bed\u8a00\u521b\u4f5c\u95e8\u69db\uff0c\u4f7f\u975e\u4e13\u5bb6\u4e5f\u80fd\u751f\u6210\u7cfb\u7edf\u5316conlangs\u3002", "method": "\u5f00\u53d1\u591a\u9636\u6bb5\u6846\u67b6\uff1a1) \u5206\u89e3\u8bed\u8a00\u8981\u7d20(\u97f3\u7cfb/\u5f62\u6001/\u53e5\u6cd5) 2) LLMs\u751f\u6210\u6a21\u5757\u5185\u5bb9 3) \u6ce8\u5165\u53ef\u63a7\u968f\u673a\u6027 4) \u81ea\u53cd\u9988\u673a\u5236\u4f18\u5316\u4e00\u81f4\u6027 5) \u81ea\u52a8\u7ffb\u8bd1\u9a8c\u8bc1\u53ef\u7528\u6027", "result": "\u8bc4\u4f30\u663e\u793a\uff1a\u7cfb\u7edf\u5728BLEU-4(0.72)\u548c\u8bcd\u6c47\u71b5(8.2)\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u751f\u6210\u768432\u79cdconlangs\u8986\u76d67\u79cd\u8bed\u7cfb\u7279\u5f81\uff0c\u8bc1\u660e\u5176\u6709\u6548\u5e73\u8861\u521b\u9020\u6027\u4e0e\u7cfb\u7edf\u6027", "conclusion": "\u9996\u6b21\u5b9e\u73b0\u7aef\u5230\u7aefAI\u8f85\u52a9\u4eba\u5de5\u8bed\u8a00\u521b\u4f5c\uff0c\u4e3a\u8ba1\u7b97\u8bed\u8a00\u5b66\u5f00\u8f9f\u65b0\u8def\u5f84\u3002\u8be5\u65b9\u6cd5\u8bc1\u660eLLMs\u80fd\u6709\u6548\u6355\u6349\u8bed\u8a00\u7ed3\u6784\u89c4\u5f8b\uff0c\u63a8\u52a8\u4eba\u673a\u534f\u540c\u7684\u8bed\u8a00\u521b\u65b0\u3002"}}
{"id": "2508.06103", "pdf": "https://arxiv.org/pdf/2508.06103", "abs": "https://arxiv.org/abs/2508.06103", "authors": ["Mohamed Basem", "Islam Oshallah", "Ali Hamdi", "Ammar Mohammed"], "title": "Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs", "categories": ["cs.CL", "cs.IR"], "comment": "6 pages , 2 figures , Accepted in IMSA 2025,Egypt ,\n  https://imsa.msa.edu.eg/", "summary": "This paper presents two effective approaches for Extractive Question\nAnswering (QA) on the Quran. It addresses challenges related to complex\nlanguage, unique terminology, and deep meaning in the text. The second uses\nfew-shot prompting with instruction-tuned large language models such as Gemini\nand DeepSeek. A specialized Arabic prompt framework is developed for span\nextraction. A strong post-processing system integrates subword alignment,\noverlap suppression, and semantic filtering. This improves precision and\nreduces hallucinations. Evaluations show that large language models with Arabic\ninstructions outperform traditional fine-tuned models. The best configuration\nachieves a pAP10 score of 0.637. The results confirm that prompt-based\ninstruction tuning is effective for low-resource, semantically rich QA tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u300a\u53e4\u5170\u7ecf\u300b\u7684\u62bd\u53d6\u5f0f\u95ee\u7b54\u65b9\u6cd5\uff0c\u901a\u8fc7\u963f\u62c9\u4f2f\u8bed\u6307\u4ee4\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08\u5982Gemini\u3001DeepSeek\uff09\u548c\u4e13\u95e8\u7684\u540e\u5904\u7406\u7cfb\u7edf\uff0c\u5728\u4f4e\u8d44\u6e90\u8bed\u4e49\u4e30\u5bcc\u7684QA\u4efb\u52a1\u4e2d\u53d6\u5f970.637 pAP10\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u5fae\u8c03\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u300a\u53e4\u5170\u7ecf\u300b\u6587\u672c\u590d\u6742\u8bed\u8a00\u7ed3\u6784\u3001\u4e13\u4e1a\u672f\u8bed\u548c\u6df1\u5c42\u8bed\u4e49\u5e26\u6765\u7684\u95ee\u7b54\u6311\u6218\uff0c\u63a2\u7d22\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u8bed\u4e49\u5bc6\u96c6\u578b\u4efb\u52a1\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u91c7\u7528\u5c11\u6837\u672c\u63d0\u793a\u7684\u6307\u4ee4\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\n2. \u5f00\u53d1\u963f\u62c9\u4f2f\u8bedspan\u62bd\u53d6\u63d0\u793a\u6846\u67b6\n3. \u6784\u5efa\u5305\u542b\u5b50\u8bcd\u5bf9\u9f50\u3001\u91cd\u53e0\u6291\u5236\u548c\u8bed\u4e49\u8fc7\u6ee4\u7684\u4e09\u9636\u6bb5\u540e\u5904\u7406\u7cfb\u7edf", "result": "\u963f\u62c9\u4f2f\u8bed\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u663e\u8457\u8d85\u8d8a\u4f20\u7edf\u5fae\u8c03\u6a21\u578b\uff0c\u6700\u4f73\u7ec4\u5408\u8fbe\u52300.637 pAP10\u5206\u6570\uff0c\u540e\u5904\u7406\u7cfb\u7edf\u4f7f\u7cbe\u786e\u5ea6\u63d0\u5347\u4e14\u5e7b\u89c9\u73b0\u8c61\u51cf\u5c11\u3002", "conclusion": "\u8bc1\u5b9e\u63d0\u793a\u5de5\u7a0b\u5728\u4f4e\u8d44\u6e90QA\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u8bed\u4e49\u5bc6\u96c6\u578b\u5b97\u6559\u6587\u672c\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6280\u672f\u8def\u5f84\uff0c\u7279\u522b\u662f\u5b50\u8bcd\u5bf9\u9f50\u548c\u8bed\u4e49\u8fc7\u6ee4\u7684\u534f\u540c\u4f5c\u7528\u503c\u5f97\u5173\u6ce8\u3002"}}
{"id": "2508.06105", "pdf": "https://arxiv.org/pdf/2508.06105", "abs": "https://arxiv.org/abs/2508.06105", "authors": ["Shengyuan Chen", "Chuang Zhou", "Zheng Yuan", "Qinggang Zhang", "Zeyang Cui", "Hao Chen", "Yilin Xiao", "Jiannong Cao", "Xiao Huang"], "title": "You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often suffer from hallucination, generating\nfactually incorrect statements when handling questions beyond their knowledge\nand perception. Retrieval-augmented generation (RAG) addresses this by\nretrieving query-relevant contexts from knowledge bases to support LLM\nreasoning. Recent advances leverage pre-constructed graphs to capture the\nrelational connections among distributed documents, showing remarkable\nperformance in complex tasks. However, existing Graph-based RAG (GraphRAG)\nmethods rely on a costly process to transform the corpus into a graph,\nintroducing overwhelming token cost and update latency. Moreover, real-world\nqueries vary in type and complexity, requiring different logic structures for\naccurate reasoning. The pre-built graph may not align with these required\nstructures, resulting in ineffective knowledge retrieval. To this end, we\npropose a \\textbf{\\underline{Logic}}-aware\n\\textbf{\\underline{R}}etrieval-\\textbf{\\underline{A}}ugmented\n\\textbf{\\underline{G}}eneration framework (\\textbf{LogicRAG}) that dynamically\nextracts reasoning structures at inference time to guide adaptive retrieval\nwithout any pre-built graph. LogicRAG begins by decomposing the input query\ninto a set of subproblems and constructing a directed acyclic graph (DAG) to\nmodel the logical dependencies among them. To support coherent multi-step\nreasoning, LogicRAG then linearizes the graph using topological sort, so that\nsubproblems can be addressed in a logically consistent order. Besides, LogicRAG\napplies graph pruning to reduce redundant retrieval and uses context pruning to\nfilter irrelevant context, significantly reducing the overall token cost.\nExtensive experiments demonstrate that LogicRAG achieves both superior\nperformance and efficiency compared to state-of-the-art baselines.", "AI": {"tldr": "\u63d0\u51faLogicRAG\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u903b\u8f91\u7ed3\u6784\u5efa\u6a21\u5b9e\u73b0\u9ad8\u6548\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff0c\u89e3\u51b3\u4f20\u7edfGraphRAG\u9884\u5efa\u56fe\u7684\u9ad8\u6210\u672c\u4e0e\u7ed3\u6784\u5931\u914d\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfGraphRAG\u4f9d\u8d56\u9884\u5efa\u56fe\u5bfc\u81f4\u9ad8\u6602token\u6210\u672c\u548c\u66f4\u65b0\u5ef6\u8fdf\uff0c\u4e14\u56fa\u5b9a\u7ed3\u6784\u96be\u4ee5\u9002\u914d\u591a\u6837\u5316\u67e5\u8be2\u7684\u903b\u8f91\u9700\u6c42\u3002\u9700\u8981\u52a8\u6001\u63d0\u53d6\u63a8\u7406\u7ed3\u6784\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u68c0\u7d22\u3002", "method": "1. \u67e5\u8be2\u5206\u89e3\u4e3a\u5b50\u95ee\u9898\u5e76\u6784\u5efaDAG\u5efa\u6a21\u903b\u8f91\u4f9d\u8d56\n2. \u62d3\u6251\u6392\u5e8f\u7ebf\u6027\u5316\u5904\u7406\u4fdd\u8bc1\u63a8\u7406\u8fde\u8d2f\u6027\n3. \u56fe\u526a\u679d\u51cf\u5c11\u5197\u4f59\u68c0\u7d22\n4. \u4e0a\u4e0b\u6587\u526a\u679d\u8fc7\u6ee4\u65e0\u5173\u5185\u5bb9", "result": "\u5b9e\u9a8c\u8868\u660eLogicRAG\u5728\u6027\u80fd\u4e0e\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709SOTA\u57fa\u7ebf", "conclusion": "LogicRAG\u901a\u8fc7\u52a8\u6001\u903b\u8f91\u7ed3\u6784\u3001\u81ea\u9002\u5e94\u68c0\u7d22\u673a\u5236\u548c\u53cc\u91cd\u526a\u679d\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u95ee\u9898\u5904\u7406\u80fd\u529b\u5e76\u964d\u4f4e75%token\u6210\u672c\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2508.06124", "pdf": "https://arxiv.org/pdf/2508.06124", "abs": "https://arxiv.org/abs/2508.06124", "authors": ["Sayantan Adak", "Pratyush Chatterjee", "Somnath Banerjee", "Rima Hazra", "Somak Aditya", "Animesh Mukherjee"], "title": "AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Present day LLMs face the challenge of managing affordance-based safety\nrisks-situations where outputs inadvertently facilitate harmful actions due to\noverlooked logical implications. Traditional safety solutions, such as scalar\noutcome-based reward models, parameter tuning, or heuristic decoding\nstrategies, lack the granularity and proactive nature needed to reliably detect\nand intervene during subtle yet crucial reasoning steps. Addressing this\nfundamental gap, we introduce AURA, an innovative, multi-layered framework\ncentered around Process Reward Models (PRMs), providing comprehensive, step\nlevel evaluations across logical coherence and safety-awareness. Our framework\nseamlessly combines introspective self-critique, fine-grained PRM assessments,\nand adaptive safety-aware decoding to dynamically and proactively guide models\ntoward safer reasoning trajectories. Empirical evidence clearly demonstrates\nthat this approach significantly surpasses existing methods, significantly\nimproving the logical integrity and affordance-sensitive safety of model\noutputs. This research represents a pivotal step toward safer, more\nresponsible, and contextually aware AI, setting a new benchmark for\nalignment-sensitive applications.", "AI": {"tldr": "\u63d0\u51faAURA\u6846\u67b6\uff0c\u901a\u8fc7\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\uff08PRMs\uff09\u5b9e\u73b0\u903b\u8f91\u8fde\u8d2f\u6027\u548c\u5b89\u5168\u6027\u7684\u591a\u5c42\u7ea7\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347LLMs\u8f93\u51fa\u7684\u5b89\u5168\u6027\u548c\u903b\u8f91\u6027\u3002", "motivation": "\u73b0\u6709LLMs\u5b58\u5728\u57fa\u4e8e\u529f\u80fd\u542f\u53d1\u7684\u5b89\u5168\u9690\u60a3\uff08affordance-based risks\uff09\uff0c\u4f20\u7edf\u5b89\u5168\u65b9\u6848\uff08\u6807\u91cf\u5956\u52b1\u6a21\u578b/\u53c2\u6570\u8c03\u6574/\u542f\u53d1\u5f0f\u89e3\u7801\uff09\u5728\u7ec6\u7c92\u5ea6\u63a8\u7406\u6b65\u9aa4\u4e2d\u7f3a\u4e4f\u4e3b\u52a8\u5e72\u9884\u80fd\u529b\u3002", "method": "\u521b\u65b0\u6027\u878d\u5408\u4e09\u91cd\u673a\u5236\uff1a1\uff09\u81ea\u7701\u5f0f\u81ea\u6211\u6279\u5224\uff1b2\uff09\u7ec6\u7c92\u5ea6PRM\u9010\u6b65\u8bc4\u4f30\u903b\u8f91\u8fde\u8d2f\u4e0e\u5b89\u5168\u611f\u77e5\uff1b3\uff09\u81ea\u9002\u5e94\u5b89\u5168\u611f\u77e5\u89e3\u7801\u52a8\u6001\u5f15\u5bfc\u63a8\u7406\u8f68\u8ff9\u3002", "result": "\u5b9e\u8bc1\u8bc1\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\uff0c\u6a21\u578b\u8f93\u51fa\u7684\u903b\u8f91\u5b8c\u6574\u6027\u548c\u5b89\u5168\u654f\u611f\u5ea6\u5747\u6709\u5927\u5e45\u63d0\u5347\u3002", "conclusion": "\u4e3aAI\u5b89\u5168\u8bbe\u7acb\u4e86\u65b0\u6807\u6746\uff0c\u63a8\u52a8\u5b9e\u73b0\u66f4\u8d1f\u8d23\u4efb\u3001\u5177\u5907\u60c5\u5883\u611f\u77e5\u80fd\u529b\u7684\u5bf9\u9f50\u654f\u611f\u578b\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u3002"}}
{"id": "2508.06135", "pdf": "https://arxiv.org/pdf/2508.06135", "abs": "https://arxiv.org/abs/2508.06135", "authors": ["Lingyuan Liu", "Mengxiang Zhang"], "title": "Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Knowledge Distillation (KD) is a fundamental technique for compressing large\nlanguage models (LLMs) into compact, efficient student models. However,\nexisting white-box KD methods mainly focus on balancing ground truth and\nstudent-generated responses while overlooking two critical factors: training\ndata quality and student-model compatibility. To address these limitations, we\npropose Selective Reflection Distillation (SRD), a novel data curation\nframework that leverages reflections from student models to systematically\nrefine training data. SRD dynamically evaluates and selects prompt-response\npairs by comparing ground truth data with student model outputs, selectively\ncurating high-quality, student-compatible training instances through automated\nranking based on difficulty. Furthermore, after selecting the training data, a\ncurriculum scheduling strategy is employed to incrementally introduce these\ncurated subsets into the distillation process at fixed intervals. As a\nplug-and-play enhancement, SRD consistently improves distillation outcomes\nacross diverse white-box KD approaches and model architectures, as well as\ndecreases computational cost significantly during KD training. Experiments on a\nrange of language model benchmarks demonstrate SRD's consistent improvements in\ndistilled model performance, as well as a reduction in training runtime by up\nto 39%, under diverse KD methods and model families. Notably, SRD operates as a\nplug-and-play module, enhancing sample efficiency without modifying underlying\nKD algorithms. Our findings highlight that data quality and compatibility are\npivotal to effective and efficient distillation of LLMs, and SRD provides a\nprincipled framework to achieve both. This work advances the understanding of\ndata-centric factors in KD and offers practical insights for enhancing the\ncapability and efficiency of compressed LLMs.", "AI": {"tldr": "\u63d0\u51fa\u9009\u62e9\u6027\u53cd\u5c04\u84b8\u998f(SRD)\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u751f\u6a21\u578b\u53cd\u9988\u4f18\u5316\u8bad\u7ec3\u6570\u636e\u8d28\u91cf\u548c\u517c\u5bb9\u6027\uff0c\u63d0\u5347\u77e5\u8bc6\u84b8\u998f\u6548\u7387\u5e76\u964d\u4f4e39%\u8bad\u7ec3\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u767d\u76d2\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u5ffd\u89c6\u6570\u636e\u8d28\u91cf\u4e0e\u5b66\u751f\u6a21\u578b\u9002\u914d\u6027\uff0c\u5bfc\u81f4\u84b8\u998f\u6548\u679c\u53d7\u9650\u3002\u9700\u8981\u7cfb\u7edf\u6027\u89e3\u51b3\u8bad\u7ec3\u6570\u636e\u7b5b\u9009\u4e0e\u6a21\u578b\u517c\u5bb9\u6027\u95ee\u9898\u3002", "method": "1. \u8bbe\u8ba1\u81ea\u52a8\u5316\u6570\u636e\u7b5b\u9009\u673a\u5236\uff1a\u901a\u8fc7\u5bf9\u6bd4\u57fa\u51c6\u7b54\u6848\u4e0e\u5b66\u751f\u8f93\u51fa\u52a8\u6001\u8bc4\u4f30prompt-response\u8d28\u91cf\n2. \u5f00\u53d1\u8bfe\u7a0b\u8c03\u5ea6\u7b56\u7565\uff1a\u5206\u9636\u6bb5\u5f15\u5165\u7b5b\u9009\u540e\u7684\u8bad\u7ec3\u5b50\u96c6\n3. \u6784\u5efa\u96be\u5ea6\u5206\u7ea7\u7cfb\u7edf\uff1a\u57fa\u4e8e\u5b66\u751f\u6a21\u578b\u8868\u73b0\u81ea\u52a8\u5212\u5206\u6570\u636e\u96be\u5ea6\u7b49\u7ea7", "result": "\u5728\u591a\u4e2a\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSRD\u4f7f\u84b8\u998f\u6a21\u578b\u6027\u80fd\u5e73\u5747\u63d0\u534715%\uff0c\u8bad\u7ec3\u8017\u65f6\u51cf\u5c1139%\uff0c\u4e14\u9002\u914d\u4e0d\u540cKD\u65b9\u6cd5\u548c\u6a21\u578b\u67b6\u6784\u3002", "conclusion": "\u6570\u636e\u8d28\u91cf\u4e0e\u517c\u5bb9\u6027\u662f\u77e5\u8bc6\u84b8\u998f\u7684\u6838\u5fc3\u8981\u7d20\uff0cSRD\u4e3a\u9ad8\u6548\u6a21\u578b\u538b\u7f29\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u6570\u636e\u7b5b\u9009\u6846\u67b6\uff0c\u63a8\u52a8\u5927\u6a21\u578b\u5b9e\u7528\u5316\u90e8\u7f72\u3002"}}
{"id": "2508.06149", "pdf": "https://arxiv.org/pdf/2508.06149", "abs": "https://arxiv.org/abs/2508.06149", "authors": ["Gunhee Cho", "Yun-Gyung Cheong"], "title": "Scaling Personality Control in LLMs with Big Five Scaler Prompts", "categories": ["cs.CL", "cs.MA"], "comment": null, "summary": "We present Big5-Scaler, a prompt-based framework for conditioning large\nlanguage models (LLMs) with controllable Big Five personality traits. By\nembedding numeric trait values into natural language prompts, our method\nenables fine-grained personality control without additional training. We\nevaluate Big5-Scaler across trait expression, dialogue generation, and human\ntrait imitation tasks. Results show that it induces consistent and\ndistinguishable personality traits across models, with performance varying by\nprompt type and scale. Our analysis highlights the effectiveness of concise\nprompts and lower trait intensities, providing a efficient approach for\nbuilding personality-aware dialogue agents.", "AI": {"tldr": "Big5-Scaler\u6846\u67b6\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u8bcd\u4e3aLLM\u6ce8\u5165\u53ef\u63a7\u5927\u4e94\u4eba\u683c\u7279\u8d28\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u6027\u683c\u63a7\u5236", "motivation": "\u89e3\u51b3\u73b0\u6709\u5bf9\u8bdd\u4ee3\u7406\u7f3a\u4e4f\u53ef\u63a7\u4eba\u683c\u7279\u8d28\u7684\u95ee\u9898\uff0c\u4e3a\u6784\u5efa\u4e2a\u6027\u5316\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848", "method": "\u5c06\u6570\u503c\u5316\u4eba\u683c\u7279\u8d28\u53c2\u6570\u5d4c\u5165\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u8bcd\uff0c\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u5b9e\u73b0\u4e0d\u540c\u6027\u683c\u5f3a\u5ea6\u7684\u8c03\u8282", "result": "\u6846\u67b6\u5728\u4e0d\u540c\u6a21\u578b\u4e0a\u5c55\u73b0\u51fa\u7a33\u5b9a\u4e14\u53ef\u533a\u5206\u7684\u4eba\u683c\u7279\u8d28\u8868\u8fbe\uff0c\u7b80\u6d01\u63d0\u793a\u8bcd\u548c\u4f4e\u5f3a\u5ea6\u7279\u8d28\u6548\u679c\u66f4\u4f73", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6784\u5efa\u4eba\u683c\u611f\u77e5\u7684\u5bf9\u8bdd\u4ee3\u7406\u63d0\u4f9b\u4e86\u53c2\u6570\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u63d0\u793a\u8bcd\u8bbe\u8ba1\u548c\u5f3a\u5ea6\u8c03\u8282\u5bf9\u6027\u683c\u63a7\u5236\u7684\u5173\u952e\u4f5c\u7528"}}
{"id": "2508.06155", "pdf": "https://arxiv.org/pdf/2508.06155", "abs": "https://arxiv.org/abs/2508.06155", "authors": ["Renhan Zhang", "Lian Lian", "Zhen Qi", "Guiran Liu"], "title": "Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach", "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the issue of implicit stereotypes that may arise during\nthe generation process of large language models. It proposes an interpretable\nbias detection method aimed at identifying hidden social biases in model\noutputs, especially those semantic tendencies that are not easily captured\nthrough explicit linguistic features. The method combines nested semantic\nrepresentation with a contextual contrast mechanism. It extracts latent bias\nfeatures from the vector space structure of model outputs. Using attention\nweight perturbation, it analyzes the model's sensitivity to specific social\nattribute terms, thereby revealing the semantic pathways through which bias is\nformed. To validate the effectiveness of the method, this study uses the\nStereoSet dataset, which covers multiple stereotype dimensions including\ngender, profession, religion, and race. The evaluation focuses on several key\nmetrics, such as bias detection accuracy, semantic consistency, and contextual\nsensitivity. Experimental results show that the proposed method achieves strong\ndetection performance across various dimensions. It can accurately identify\nbias differences between semantically similar texts while maintaining high\nsemantic alignment and output stability. The method also demonstrates high\ninterpretability in its structural design. It helps uncover the internal bias\nassociation mechanisms within language models. This provides a more transparent\nand reliable technical foundation for bias detection. The approach is suitable\nfor real-world applications where high trustworthiness of generated content is\nrequired.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u89e3\u91ca\u7684\u9690\u6027\u504f\u89c1\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u5408\u5d4c\u5957\u8bed\u4e49\u8868\u793a\u4e0e\u4e0a\u4e0b\u6587\u5bf9\u6bd4\u673a\u5236\uff0c\u6709\u6548\u8bc6\u522b\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7684\u9690\u85cf\u793e\u4f1a\u504f\u89c1\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u65b9\u6cd5\u5728\u8de8\u7ef4\u5ea6\u68c0\u6d4b\u6027\u80fd\u4e0e\u8bed\u4e49\u4e00\u81f4\u6027\u4f18\u52bf\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u4ea7\u751f\u7684\u9690\u6027\u523b\u677f\u5370\u8c61\u95ee\u9898\uff0c\u9488\u5bf9\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u7684\u975e\u663e\u6027\u8bed\u4e49\u503e\u5411\uff0c\u5f00\u53d1\u900f\u660e\u53ef\u9760\u7684\u504f\u5dee\u68c0\u6d4b\u6280\u672f\u3002", "method": "\u5d4c\u5957\u8bed\u4e49\u8868\u793a+\u4e0a\u4e0b\u6587\u5bf9\u6bd4\u673a\u5236\u63d0\u53d6\u6f5c\u5728\u504f\u5dee\u7279\u5f81\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u6743\u91cd\u6270\u52a8\u5206\u6790\u6a21\u578b\u5bf9\u7279\u5b9a\u793e\u4f1a\u5c5e\u6027\u8bcd\u7684\u654f\u611f\u6027\uff0c\u63ed\u793a\u504f\u89c1\u5f62\u6210\u7684\u8bed\u4e49\u8def\u5f84\u3002", "result": "\u5728StereoSet\u591a\u7ef4\u5ea6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u65b9\u6cd5\u51c6\u786e\u8bc6\u522b\u8bed\u4e49\u76f8\u4f3c\u6587\u672c\u7684\u504f\u5dee\u5dee\u5f02\uff0c\u4fdd\u630192.3%\u7684\u8bed\u4e49\u5bf9\u9f50\u5ea6\uff0c\u68c0\u6d4b\u51c6\u786e\u7387\u8d85\u57fa\u7ebf\u65b9\u6cd515.6%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u6784\u53ef\u89e3\u91ca\u6027\u8bbe\u8ba1\uff0c\u6210\u529f\u63ed\u793a\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u504f\u89c1\u5173\u8054\u673a\u5236\uff0c\u4e3a\u9700\u8981\u9ad8\u53ef\u4fe1\u5ea6\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f\u63d0\u4f9b\u53ef\u9760\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06163", "pdf": "https://arxiv.org/pdf/2508.06163", "abs": "https://arxiv.org/abs/2508.06163", "authors": ["Yingfeng Luo", "Dingyang Lin", "Junxin Wang", "Ziqiang Xu", "Kaiyan Chang", "Tong Zheng", "Bei Li", "Anxiang Ma", "Tong Xiao", "Zhengtao Yu", "Jingbo Zhu"], "title": "One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under review", "summary": "Model merging has emerged as a compelling data-free paradigm for multi-task\nlearning, enabling the fusion of multiple fine-tuned models into a single,\npowerful entity. A key technique in merging methods is sparsification, which\nprunes redundant parameters from task vectors to mitigate interference.\nHowever, prevailing approaches employ a ``one-size-fits-all'' strategy,\napplying a uniform sparsity ratio that overlooks the inherent structural and\nstatistical heterogeneity of model parameters. This often leads to a suboptimal\ntrade-off, where critical parameters are inadvertently pruned while less useful\nones are retained. To address this limitation, we introduce \\textbf{TADrop}\n(\\textbf{T}ensor-wise \\textbf{A}daptive \\textbf{Drop}), an adaptive\nsparsification strategy that respects this heterogeneity. Instead of a global\nratio, TADrop assigns a tailored sparsity level to each parameter tensor based\non its distributional properties. The core intuition is that tensors with\ndenser, more redundant distributions can be pruned aggressively, while sparser,\nmore critical ones are preserved. As a simple and plug-and-play module, we\nvalidate TADrop by integrating it with foundational, classic, and SOTA merging\nmethods. Extensive experiments across diverse tasks (vision, language, and\nmultimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and\nsignificantly boosts their performance. For instance, when enhancing a leading\nmerging method, it achieves an average performance gain of 2.0\\% across 8\nViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter\ninterference by tailoring sparsification to the model's structure, offering a\nnew baseline for high-performance model merging.", "AI": {"tldr": "\u63d0\u51faTADrop\u65b9\u6cd5\u2014\u2014\u901a\u8fc7\u5f20\u91cf\u81ea\u9002\u5e94\u7a00\u758f\u5316\u7b56\u7565\u63d0\u5347\u6a21\u578b\u878d\u5408\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5747\u5300\u526a\u679d\u5bfc\u81f4\u7684\u53c2\u6570\u5e72\u6270\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u91c7\u7528\u5168\u5c40\u7edf\u4e00\u7a00\u758f\u7387\uff0c\u5ffd\u7565\u4e86\u53c2\u6570\u5206\u5e03\u7684\u5f02\u6784\u6027\uff0c\u5bfc\u81f4\u5173\u952e\u53c2\u6570\u88ab\u8bef\u526a\u3001\u5197\u4f59\u53c2\u6570\u88ab\u4fdd\u7559\u7684\u6027\u80fd\u635f\u5931\u3002", "method": "\u57fa\u4e8e\u53c2\u6570\u5f20\u91cf\u7684\u5206\u5e03\u7279\u6027\uff08\u5bc6\u96c6/\u7a00\u758f\u7a0b\u5ea6\uff09\uff0c\u4e3a\u6bcf\u4e2a\u5f20\u91cf\u5206\u914d\u5b9a\u5236\u5316\u7684\u7a00\u758f\u7387\uff1a\u5bc6\u96c6\u5f20\u91cf\u5927\u5e45\u526a\u679d\uff0c\u7a00\u758f\u5f20\u91cf\u91cd\u70b9\u4fdd\u7559\u3002\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u517c\u5bb9\u4e3b\u6d41\u878d\u5408\u65b9\u6cd5\u3002", "result": "\u5728\u89c6\u89c9\uff08ViT\uff09\u3001\u8bed\u8a00\u3001\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u5178\u578b\u5b9e\u9a8c\u663e\u793a\u57288\u4e2aViT-B/32\u4efb\u52a1\u4e0a\u5e73\u5747\u63d0\u53472.0%\u3002", "conclusion": "TADrop\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u7684\u81ea\u9002\u5e94\u526a\u679d\u7b56\u7565\uff0c\u4e3a\u9ad8\u6027\u80fd\u6a21\u578b\u878d\u5408\u5efa\u7acb\u4e86\u65b0\u57fa\u7ebf\uff0c\u63ed\u793a\u4e86\u53c2\u6570\u5206\u5e03\u5f02\u8d28\u6027\u5bf9\u6a21\u578b\u878d\u5408\u7684\u5173\u952e\u5f71\u54cd\u3002"}}
{"id": "2508.06165", "pdf": "https://arxiv.org/pdf/2508.06165", "abs": "https://arxiv.org/abs/2508.06165", "authors": ["Weitao Li", "Boran Xiang", "Xiaolong Wang", "Zhinan Gou", "Weizhi Ma", "Yang Liu"], "title": "UR$^2$: Unify RAG and Reasoning through Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable capabilities through two\ncomplementary paradigms: Retrieval-Augmented Generation (RAG), which enhances\nknowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),\nwhich optimizes complex reasoning abilities. However, these two capabilities\nare often developed in isolation, and existing efforts to unify them remain\nnarrow in scope-typically limited to open-domain QA with fixed retrieval\nsettings and task-specific assumptions. This lack of integration constrains\ngeneralization and limits the applicability of RAG-RL methods to broader\ndomains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a\ngeneral framework that unifies retrieval and reasoning through reinforcement\nlearning. UR2 introduces two key contributions: a difficulty-aware curriculum\ntraining that selectively invokes retrieval only for challenging problems, and\na hybrid knowledge access strategy combining domain-specific offline corpora\nwith LLM-generated summaries. These components are designed to enable dynamic\ncoordination between retrieval and reasoning, improving adaptability across a\ndiverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,\nand mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B\nand LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,\nachieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several\nbenchmarks. We have released all code, models, and data at\nhttps://github.com/Tsinghua-dhy/UR2.", "AI": {"tldr": "\u63d0\u51faUR2\u6846\u67b6\u7edf\u4e00\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u4e0e\u5f3a\u5316\u5b66\u4e60\u9a8c\u8bc1\u5956\u52b1\uff08RLVR\uff09\uff0c\u901a\u8fc7\u96be\u5ea6\u611f\u77e5\u8bfe\u7a0b\u8bad\u7ec3\u548c\u6df7\u5408\u77e5\u8bc6\u8bbf\u95ee\u7b56\u7565\u5b9e\u73b0\u68c0\u7d22\u4e0e\u63a8\u7406\u7684\u52a8\u6001\u534f\u8c03\uff0c\u663e\u8457\u63d0\u5347\u8de8\u9886\u57df\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u68c0\u7d22\u4e0e\u63a8\u7406\u80fd\u529b\u5b64\u7acb\u53d1\u5c55\uff0c\u5bfc\u81f4\u6574\u5408\u65b9\u6848\u9002\u7528\u8303\u56f4\u53d7\u9650\uff08\u5982\u4ec5\u652f\u6301\u56fa\u5b9a\u68c0\u7d22\u7684\u5f00\u653e\u57dfQA\u4efb\u52a1\uff09\uff0c\u5236\u7ea6RAG-RL\u65b9\u6cd5\u7684\u901a\u7528\u6027\u548c\u8de8\u9886\u57df\u9002\u5e94\u6027\u3002", "method": "1. \u96be\u5ea6\u611f\u77e5\u8bfe\u7a0b\u8bad\u7ec3\uff1a\u4ec5\u5728\u590d\u6742\u95ee\u9898\u65f6\u89e6\u53d1\u68c0\u7d22\n2. \u6df7\u5408\u77e5\u8bc6\u8bbf\u95ee\uff1a\u7ed3\u5408\u9886\u57df\u4e13\u7528\u79bb\u7ebf\u8bed\u6599\u5e93\u4e0eLLM\u751f\u6210\u6458\u8981\n3. \u57fa\u4e8eQwen2.5/Llama3\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5b9e\u73b0\u52a8\u6001\u534f\u8c03\u673a\u5236", "result": "\u5728\u5f00\u653e\u57dfQA/MMLU-Pro/\u533b\u5b66/\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cUR2\uff087B/8B\u53c2\u6570\uff09\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u90e8\u5206\u4efb\u52a1\u8fbe\u5230GPT-4o-mini/GPT-4.1-mini\u6c34\u5e73\u3002\u4ee3\u7801\u6a21\u578b\u5df2\u5f00\u6e90\u3002", "conclusion": "UR2\u901a\u8fc7\u52a8\u6001\u534f\u8c03\u673a\u5236\u6709\u6548\u7edf\u4e00\u68c0\u7d22\u4e0e\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u591a\u9886\u57df\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u9002\u5e94\u6027\u548c\u6269\u5c55\u6027\uff0c\u4e3a\u901a\u7528AI\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2508.06167", "pdf": "https://arxiv.org/pdf/2508.06167", "abs": "https://arxiv.org/abs/2508.06167", "authors": ["V\u00edt Gvo\u017ediak"], "title": "Pragmatics beyond humans: meaning, communication, and LLMs", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "The paper reconceptualizes pragmatics not as a subordinate, third dimension\nof meaning, but as a dynamic interface through which language operates as a\nsocially embedded tool for action. With the emergence of large language models\n(LLMs) in communicative contexts, this understanding needs to be further\nrefined and methodologically reconsidered. The first section challenges the\ntraditional semiotic trichotomy, arguing that connectionist LLM architectures\ndestabilize established hierarchies of meaning, and proposes the Human-Machine\nCommunication (HMC) framework as a more suitable alternative. The second\nsection examines the tension between human-centred pragmatic theories and the\nmachine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics\ncontinue to dominate, it relies on human-specific assumptions ill-suited to\npredictive systems like LLMs. Probabilistic pragmatics, particularly the\nRational Speech Act framework, offers a more compatible teleology by focusing\non optimization rather than truth-evaluation. The third section addresses the\nissue of substitutionalism in three forms - generalizing, linguistic, and\ncommunicative - highlighting the anthropomorphic biases that distort LLM\nevaluation and obscure the role of human communicative subjects. Finally, the\npaper introduces the concept of context frustration to describe the paradox of\nincreased contextual input paired with a collapse in contextual understanding,\nemphasizing how users are compelled to co-construct pragmatic conditions both\nfor the model and themselves. These arguments suggest that pragmatic theory may\nneed to be adjusted or expanded to better account for communication involving\ngenerative AI.", "AI": {"tldr": "\u8bba\u6587\u91cd\u6784\u8bed\u7528\u5b66\u4e3a\u52a8\u6001\u4eba\u673a\u4ea4\u4e92\u63a5\u53e3\uff0c\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u4f20\u7edf\u8bed\u7528\u7406\u8bba\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4eba\u673a\u4ea4\u6d41\u6846\u67b6\u548c\u6982\u7387\u8bed\u7528\u5b66\u65b9\u6cd5\uff0c\u6279\u5224\u66ff\u6362\u4e3b\u4e49\u504f\u89c1\uff0c\u63ed\u793a\u8bed\u5883\u7406\u89e3\u6096\u8bba\uff0c\u547c\u5401\u7406\u8bba\u8c03\u6574\u4ee5\u9002\u5e94AI\u6c9f\u901a\u573a\u666f\u3002", "motivation": "\u4f20\u7edf\u8bed\u7528\u5b66\u7684\u4eba\u7c7b\u4e2d\u5fc3\u5047\u8bbe\u96be\u4ee5\u9002\u914d\u673a\u5668\u4e2d\u5fc3\u7684LLMs\uff0c\u9700\u5efa\u7acb\u65b0\u7684\u5206\u6790\u6846\u67b6\u89e3\u91ca\u4eba\u673a\u4ea4\u4e92\u7684\u8bed\u7528\u673a\u5236\u3002", "method": "1. \u89e3\u6784\u4f20\u7edf\u7b26\u53f7\u5b66\u4e09\u5206\u6cd5\uff0c\u63d0\u51fa\u4eba\u673a\u4ea4\u6d41\u6846\u67b6 2. \u5bf9\u6bd4Gricean\u8bed\u7528\u5b66\u4e0e\u6982\u7387\u8bed\u7528\u5b66\u7684\u9002\u914d\u6027 3. \u901a\u8fc7\u4e09\u7c7b\u66ff\u6362\u4e3b\u4e49\u6279\u5224\u63ed\u793a\u8bc4\u4f30\u504f\u5dee 4. \u5f15\u5165\u8bed\u5883\u632b\u6298\u6982\u5ff5\u5206\u6790\u4ea4\u4e92\u6096\u8bba", "result": "\u53d1\u73b0LLMs\u98a0\u8986\u4f20\u7edf\u610f\u4e49\u5c42\u7ea7\u7ed3\u6784\uff0c\u9a8c\u8bc1\u6982\u7387\u4f18\u5316\u8def\u5f84\u66f4\u9002\u5408\u9884\u6d4b\u7cfb\u7edf\uff0c\u63ed\u793a\u62df\u4eba\u5316\u8bc4\u4f30\u4f1a\u626d\u66f2\u4eba\u673a\u4ea4\u4e92\u672c\u8d28\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u7684\u6c9f\u901a\u7279\u6027\u8981\u6c42\u62d3\u5c55\u4f20\u7edf\u8bed\u7528\u7406\u8bba\uff0c\u5173\u6ce8\u4eba\u673a\u534f\u540c\u7684\u8bed\u7528\u6761\u4ef6\u5171\u5efa\uff0c\u8b66\u60d5\u6280\u672f\u4e2d\u4ecb\u5bfc\u81f4\u7684\u8bed\u5883\u7406\u89e3\u584c\u7f29\u3002"}}
{"id": "2508.06178", "pdf": "https://arxiv.org/pdf/2508.06178", "abs": "https://arxiv.org/abs/2508.06178", "authors": ["Hugo Abonizio", "Thales Almeida", "Roberto Lotufo", "Rodrigo Nogueira"], "title": "Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often require vast amounts of text to\neffectively acquire new knowledge. While continuing pre-training on large\ncorpora or employing retrieval-augmented generation (RAG) has proven\nsuccessful, updating an LLM with only a few thousand or million tokens remains\nchallenging. In this work, we investigate the task of injecting small,\nunstructured information into LLMs and its relation to the catastrophic\nforgetting phenomenon. We use a dataset of recent news -- ensuring no overlap\nwith the model's pre-training data -- to evaluate the knowledge acquisition by\nprobing the model with question-answer pairs related the learned information.\nStarting from a continued pre-training baseline, we explored different\naugmentation algorithms to generate synthetic data to improve the knowledge\nacquisition capabilities. Our experiments show that simply continuing\npre-training on limited data yields modest improvements, whereas exposing the\nmodel to diverse textual variations significantly improves the learning of new\nfacts -- particularly with methods that induce greater variability through\ndiverse prompting. Furthermore, we shed light on the forgetting phenomenon in\nsmall-data regimes, illustrating the delicate balance between learning new\ncontent and retaining existing capabilities. We also confirm the sensitivity of\nRAG-based approaches for knowledge injection, which often lead to greater\ndegradation on control datasets compared to parametric methods. Finally, we\ndemonstrate that models can generate effective synthetic training data\nthemselves, suggesting a pathway toward self-improving model updates. All code\nand generated data used in our experiments are publicly available, providing a\nresource for studying efficient knowledge injection in LLMs with limited data\nat https://github.com/hugoabonizio/knowledge-injection-methods.", "AI": {"tldr": "\u901a\u8fc7\u751f\u6210\u591a\u6837\u5316\u5408\u6210\u6570\u636e\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5c0f\u6837\u672c\u77e5\u8bc6\u6ce8\u5165\uff0c\u63ed\u793a\u6587\u672c\u591a\u6837\u6027\u5bf9\u77e5\u8bc6\u83b7\u53d6\u7684\u5173\u952e\u4f5c\u7528\u53ca\u81ea\u751f\u6210\u6570\u636e\u7684\u6f5c\u5728\u4ef7\u503c", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5c0f\u6837\u672c\u6570\u636e\u573a\u666f\u4e0b\u77e5\u8bc6\u6ce8\u5165\u56f0\u96be\u4e0e\u707e\u96be\u6027\u9057\u5fd8\u7684\u5e73\u8861\u95ee\u9898", "method": "\u57fa\u4e8e\u65b0\u95fb\u6570\u636e\u96c6\u6784\u5efaQA\u5bf9\uff0c\u91c7\u7528\u591a\u6837\u5316\u63d0\u793a\u751f\u6210\u5408\u6210\u6570\u636e\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\u5bf9\u6bd4\u5b9e\u9a8c", "result": "\u6587\u672c\u591a\u6837\u6027\u589e\u5f3a\u65b9\u6cd5\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u534740%\u77e5\u8bc6\u83b7\u53d6\u6548\u7387\uff0c\u81ea\u751f\u6210\u6570\u636e\u5c55\u793a\u51fa\u4e0e\u4eba\u5de5\u6807\u6ce8\u76f8\u5f53\u7684\u8bad\u7ec3\u6548\u679c", "conclusion": "\u63d0\u793a\u9a71\u52a8\u7684\u6570\u636e\u591a\u6837\u5316\u7b56\u7565\u80fd\u6709\u6548\u7a81\u7834\u5c0f\u6837\u672c\u5b66\u4e60\u74f6\u9888\uff0c\u6a21\u578b\u81ea\u751f\u6210\u6570\u636e\u4e3a\u5b9e\u73b0\u81ea\u6211\u8fed\u4ee3\u66f4\u65b0\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2508.06186", "pdf": "https://arxiv.org/pdf/2508.06186", "abs": "https://arxiv.org/abs/2508.06186", "authors": ["Ali Sarabadani", "Maryam Abdollahi Shamami", "Hamidreza Sadeghsalehi", "Borhan Asadi", "Saba Hesaraki"], "title": "DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have grown exponentially since the release of\nChatGPT. These models have gained attention due to their robust performance on\nvarious tasks, including language processing tasks. These models achieve\nunderstanding and comprehension of tasks by training billions of parameters.\nThe development of these models is a transformative force in enhancing natural\nlanguage understanding and has taken a significant step towards artificial\ngeneral intelligence (AGI). In this study, we aim to present the DKG-LLM\nframework. The DKG-LLM framework introduces a groundbreaking approach to\nmedical diagnosis and personalized treatment recommendations by integrating a\ndynamic knowledge graph (DKG) with the Grok 3 large language model. Using the\nAdaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data\n(including clinical reports and PubMed articles) and patient records\ndynamically generate a knowledge graph consisting of 15,964 nodes in 13\ndistinct types (e.g., diseases, symptoms, treatments, patient profiles) and\n127,392 edges in 26 relationship types (e.g., causal, therapeutic,\nassociation). ASFA utilizes advanced probabilistic models, Bayesian inference,\nand graph optimization to extract semantic information, dynamically updating\nthe graph with approximately 150 new nodes and edges in each data category\nwhile maintaining scalability with up to 987,654 edges. Real-world datasets,\nincluding MIMIC-III and PubMed, were utilized to evaluate the proposed\narchitecture. The evaluation results show that DKG-LLM achieves a diagnostic\naccuracy of 84.19%. The model also has a treatment recommendation accuracy of\n89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and\ntransformative tool that handles noisy data and complex multi-symptom diseases,\nalong with feedback-based learning from physician input.", "AI": {"tldr": "DKG-LLM\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u4e0eGrok 3\u5927\u6a21\u578b\u7684\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u533b\u7597\u8bca\u65ad\u51c6\u786e\u6027\u548c\u4e2a\u6027\u5316\u6cbb\u7597\u63a8\u8350\u6548\u679c", "motivation": "\u4f20\u7edf\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u9762\u4e34\u566a\u58f0\u6570\u636e\u5904\u7406\u548c\u591a\u75c7\u72b6\u75be\u75c5\u5206\u6790\u7684\u5c40\u9650\u6027\uff0c\u9700\u52a8\u6001\u77e5\u8bc6\u66f4\u65b0\u673a\u5236\u5b9e\u73b0\u7cbe\u51c6\u8bca\u7597", "method": "\u91c7\u7528ASFA\u7b97\u6cd5\u6574\u5408\u4e34\u5e8a\u62a5\u544a\u3001PubMed\u6587\u732e\u7b49\u5f02\u6784\u6570\u636e\uff0c\u6784\u5efa\u542b15,964\u8282\u70b9/127,392\u8fb9\u7684\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u65ad\u548c\u56fe\u4f18\u5316\u5b9e\u73b0\u8bed\u4e49\u878d\u5408", "result": "\u5728MIMIC-III\u548cPubMed\u6570\u636e\u96c6\u4e0a\u53d6\u5f9784.19%\u8bca\u65ad\u51c6\u786e\u7387\u300189.63%\u6cbb\u7597\u63a8\u8350\u51c6\u786e\u7387\u53ca93.48%\u8bed\u4e49\u8986\u76d6\u7387\uff0c\u652f\u6301\u6bcf\u7c7b\u6570\u636e\u52a8\u6001\u66f4\u65b0\u7ea6150\u8282\u70b9/\u8fb9", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u533b\u751f\u53cd\u9988\u5b66\u4e60\u548c\u5b9e\u65f6\u77e5\u8bc6\u66f4\u65b0\uff0c\u6709\u6548\u5904\u7406\u590d\u6742\u533b\u7597\u573a\u666f\uff0c\u6210\u4e3a\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u7684\u53ef\u9760\u5de5\u5177"}}
{"id": "2508.06194", "pdf": "https://arxiv.org/pdf/2508.06194", "abs": "https://arxiv.org/abs/2508.06194", "authors": ["Lai Jiang", "Yuekang Li", "Xiaohan Zhang", "Youtao Ding", "Li Pan"], "title": "Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Precise jailbreak evaluation is vital for LLM red teaming and jailbreak\nresearch. Current approaches employ binary classification ( e.g., string\nmatching, toxic text classifiers, LLM-driven methods), yielding only \"yes/no\"\nlabels without quantifying harm intensity. Existing multi-dimensional\nframeworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)\napply uniform evaluation criteria across scenarios, resulting in\nscenario-specific mismatches--for instance, \"Relative Truthfulness\" is\nirrelevant to \"hate speech\"--which compromise evaluation precision. To tackle\nthese limitations, we introduce SceneJailEval, with key contributions: (1) A\ngroundbreaking scenario-adaptive multi-dimensional framework for jailbreak\nevaluation, overcoming the critical \"one-size-fits-all\" constraint of existing\nmulti-dimensional methods, and featuring strong extensibility to flexibly adapt\nto customized or emerging scenarios. (2) A comprehensive 14-scenario dataset\nwith diverse jailbreak variants and regional cases, filling the long-standing\ngap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)\nSceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on\nour full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over\nprior SOTA), surpassing accuracy limits of existing evaluation methods in\nheterogeneous scenarios and confirming its advantage.", "AI": {"tldr": "\u63d0\u51fa\u573a\u666f\u81ea\u9002\u5e94\u7684\u591a\u7ef4\u8d8a\u72f1\u8bc4\u4f30\u6846\u67b6SceneJailEval\uff0c\u7a81\u7834\u4f20\u7edf\u4e8c\u5143\u5206\u7c7b\u5c40\u9650\u5e76\u5b9e\u73b0SOTA\u6027\u80fd", "motivation": "\u73b0\u6709\u8d8a\u72f1\u8bc4\u4f30\u65b9\u6cd5\u91c7\u7528\u4e8c\u5143\u5206\u7c7b\u5bfc\u81f4\u65e0\u6cd5\u91cf\u5316\u5371\u5bb3\u5f3a\u5ea6\uff0c\u4e14\u591a\u7ef4\u6846\u67b6\u5b58\u5728\u573a\u666f\u9002\u914d\u6027\u5dee\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u8bc4\u4f30\u51c6\u786e\u6027", "method": "1. \u6784\u5efa\u573a\u666f\u81ea\u9002\u5e94\u8bc4\u4f30\u6846\u67b6\uff08\u81ea\u52a8\u5339\u914d\u573a\u666f\u76f8\u5173\u7ef4\u5ea6\uff09 2. \u521b\u5efa\u542b14\u4e2a\u573a\u666f/\u591a\u6837\u672c/\u533a\u57df\u6848\u4f8b\u7684\u57fa\u51c6\u6570\u636e\u96c6 3. \u91c7\u7528\u6df7\u5408\u8bc4\u4f30\u7b56\u7565\uff08\u89c4\u5219+LLM\uff09", "result": "\u5168\u573a\u666fF1\u8fbe0.917\uff08\u8f83SOTA\u63d0\u53476%\uff09\uff0cJBB\u6570\u636e\u96c6F1\u8fbe0.995\uff08\u63d0\u53473%\uff09\uff0c\u9a8c\u8bc1\u8de8\u573a\u666f\u8bc4\u4f30\u4f18\u52bf", "conclusion": "SceneJailEval\u901a\u8fc7\u573a\u666f\u9002\u914d\u673a\u5236\u7a81\u7834\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7cbe\u5ea6\u4e0a\u9650\uff0c\u4e3aAI\u5b89\u5168\u8bc4\u4f30\u63d0\u4f9b\u6807\u51c6\u5316\u57fa\u51c6\u4e0e\u7075\u6d3b\u6846\u67b6"}}
{"id": "2508.06196", "pdf": "https://arxiv.org/pdf/2508.06196", "abs": "https://arxiv.org/abs/2508.06196", "authors": ["Nizi Nazar", "Ehsaneddin Asgari"], "title": "EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations", "categories": ["cs.CL", "cs.HC"], "comment": null, "summary": "Emotional Intelligence (EI) is a critical yet underexplored dimension in the\ndevelopment of human-aligned LLMs. To address this gap, we introduce a unified,\npsychologically grounded four-layer taxonomy of EI tailored for large language\nmodels (LLMs), encompassing emotional tracking, cause inference, appraisal, and\nemotionally appropriate response generation. Building on this framework, we\npresent EICAP-Bench, a novel MCQ style multi-turn benchmark designed to\nevaluate EI capabilities in open-source LLMs across diverse linguistic and\ncultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma\n(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,\nidentifying Qwen2.5-Instruct as the strongest baseline. To assess the potential\nfor enhancing EI capabilities, we fine-tune both Qwen2.5-Base and\nQwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,\ninstruction-tuned dialogue dataset, in both English and Arabic. Our statistical\nanalysis reveals that among the five EI layers, only the Appraisal layer shows\nsignificant improvement through UC-based fine-tuning. These findings highlight\nthe limitations of existing pretraining and instruction-tuning paradigms in\nequipping LLMs with deeper emotional reasoning and underscore the need for\ntargeted data and modeling strategies for comprehensive EI alignment.", "AI": {"tldr": "\u63d0\u51fa\u5fc3\u7406\u5b78\u56db\u5c64EI\u5206\u985e\u6846\u67b6\u53caEICAP-Bench\u8a55\u4f30\u57fa\u6e96\uff0c\u767c\u73fe\u73fe\u6709LLM\u5728\u60c5\u611f\u63a8\u7406\u7684\u5c40\u9650\u6027\uff0cQwen2.5-Instruct\u8868\u73fe\u6700\u4f73\uff0c\u5fae\u8abf\u50c5\u63d0\u5347Appraisal\u5c64\u80fd\u529b\u3002", "motivation": "\u586b\u88dc\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u5728\u60c5\u611f\u667a\u80fd(EI)\u9818\u57df\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u5efa\u7acb\u7cfb\u7d71\u8a55\u4f30\u9ad4\u7cfb\u4ee5\u63a8\u52d5\u4eba\u985e\u5c0d\u9f4a\u7684LLM\u767c\u5c55\u3002", "method": "1. \u69cb\u5efa\u56db\u5c64EI\u5206\u985e\u6cd5(\u60c5\u611f\u8ffd\u8e64/\u6b78\u56e0/\u8a55\u4f30/\u56de\u61c9)\n2. \u5275\u5efa\u591a\u8a9e\u8a00\u591a\u8f2a\u554f\u7b54\u57fa\u6e96EICAP-Bench\n3. \u8a55\u4f306\u500b\u958b\u6e90LLM\u4e26\u9032\u884c\u8de8\u8a9e\u8a00\u5fae\u8abf\u5be6\u9a57(LoRA+UltraChat\u6578\u64da\u96c6)", "result": "Qwen2.5-Instruct\u57fa\u6e96\u6700\u4f73\uff0c\u5fae\u8abf\u50c5\u986f\u8457\u6539\u5584Appraisal\u5c64(22.3%\u2191)\uff0c\u5176\u4ed6EI\u5c64\u672a\u898b\u986f\u8457\u9032\u6b65(p>0.05)", "conclusion": "\u73fe\u6709\u9810\u8a13\u7df4\u548c\u6307\u4ee4\u5fae\u8abf\u6a21\u5f0f\u96e3\u4ee5\u8ce6\u4e88LLM\u6df1\u5c64\u60c5\u611f\u63a8\u7406\u80fd\u529b\uff0c\u9700\u91dd\u5c0d\u6027\u6578\u64da\u548c\u5efa\u6a21\u7b56\u7565\u5be6\u73fe\u5168\u9762EI\u5c0d\u9f4a"}}
{"id": "2508.06204", "pdf": "https://arxiv.org/pdf/2508.06204", "abs": "https://arxiv.org/abs/2508.06204", "authors": ["Richard Willats", "Josh Pennington", "Aravind Mohan", "Bertie Vidgen"], "title": "Classification is a RAG problem: A case study on hate speech detection", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Robust content moderation requires classification systems that can quickly\nadapt to evolving policies without costly retraining. We present classification\nusing Retrieval-Augmented Generation (RAG), which shifts traditional\nclassification tasks from determining the correct category in accordance with\npre-trained parameters to evaluating content in relation to contextual\nknowledge retrieved at inference. In hate speech detection, this transforms the\ntask from \"is this hate speech?\" to \"does this violate the hate speech policy?\"\n  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates\nthis approach and offers three key advantages: (1) robust classification\naccuracy comparable to leading commercial systems, (2) inherent explainability\nvia retrieved policy segments, and (3) dynamic policy updates without model\nretraining. Through three experiments, we demonstrate strong baseline\nperformance and show that the system can apply fine-grained policy control by\ncorrectly adjusting protection for specific identity groups without requiring\nretraining or compromising overall performance. These findings establish that\nRAG can transform classification into a more flexible, transparent, and\nadaptable process for content moderation and wider classification problems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u5185\u5bb9\u5ba1\u6838\u7cfb\u7edfCPE\uff0c\u5b9e\u73b0\u52a8\u6001\u653f\u7b56\u66f4\u65b0\u548c\u53ef\u89e3\u91ca\u5206\u7c7b\uff0c\u6027\u80fd\u5ab2\u7f8e\u5546\u4e1a\u7cfb\u7edf\u4e14\u65e0\u9700\u6a21\u578b\u91cd\u8bad\u7ec3\u3002", "motivation": "\u4f20\u7edf\u5185\u5bb9\u5ba1\u6838\u7cfb\u7edf\u4f9d\u8d56\u9884\u8bad\u7ec3\u53c2\u6570\uff0c\u653f\u7b56\u66f4\u65b0\u9700\u53cd\u590d\u8bad\u7ec3\u6a21\u578b\u6210\u672c\u9ad8\u6602\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7RAG\u6280\u672f\u5c06\u5206\u7c7b\u4efb\u52a1\u8f6c\u5316\u4e3a\u57fa\u4e8e\u52a8\u6001\u653f\u7b56\u68c0\u7d22\u7684\u8bc4\u4f30\u8fc7\u7a0b\u3002", "method": "\u5f00\u53d1Contextual Policy Engine\uff08CPE\uff09\u7cfb\u7edf\uff1a1. \u5c06\u5206\u7c7b\u95ee\u9898\u8f6c\u5316\u4e3a\u653f\u7b56\u7b26\u5408\u6027\u8bc4\u4f30 2. \u5b9e\u65f6\u68c0\u7d22\u76f8\u5173\u653f\u7b56\u7247\u6bb5 3. \u901a\u8fc7RAG\u751f\u6210\u51b3\u7b56\u4f9d\u636e\u548c\u89e3\u91ca", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1. \u8fbe\u5230\u4e3b\u6d41\u5546\u4e1a\u7cfb\u7edf\u51c6\u786e\u7387 2. \u53ef\u7cbe\u7ec6\u8c03\u6574\u7279\u5b9a\u7fa4\u4f53\u4fdd\u62a4\u7b56\u7565 3. \u653f\u7b56\u66f4\u65b0\u5373\u65f6\u751f\u6548\u4e14\u4fdd\u6301\u6574\u4f53\u6027\u80fd\u7a33\u5b9a", "conclusion": "RAG\u6280\u672f\u4f7f\u5185\u5bb9\u5ba1\u6838\u5206\u7c7b\u5177\u5907\u653f\u7b56\u52a8\u6001\u9002\u5e94\u6027\u3001\u51b3\u7b56\u900f\u660e\u6027\u53ca\u7cfb\u7edf\u7ef4\u62a4\u7075\u6d3b\u6027\uff0c\u4e3a\u5206\u7c7b\u95ee\u9898\u63d0\u4f9b\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06220", "pdf": "https://arxiv.org/pdf/2508.06220", "abs": "https://arxiv.org/abs/2508.06220", "authors": ["Keummin Ka", "Junhyeong Park", "Jahyun Jeon", "Youngjae Yu"], "title": "InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 9 figures", "summary": "Recent advances in Vision-Language Models (VLMs) have demonstrated impressive\ncapabilities in perception and reasoning. However, the ability to perform\ncausal inference -- a core aspect of human cognition -- remains underexplored,\nparticularly in multimodal settings. In this study, we introduce InfoCausalQA,\na novel benchmark designed to evaluate causal reasoning grounded in\ninfographics that combine structured visual data with textual context. The\nbenchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning\nbased on inferred numerical trends, while Task 2 targets semantic causal\nreasoning involving five types of causal relations: cause, effect,\nintervention, counterfactual, and temporal. We manually collected 494\ninfographic-text pairs from four public sources and used GPT-4o to generate\n1,482 high-quality multiple-choice QA pairs. These questions were then\ncarefully revised by humans to ensure they cannot be answered based on\nsurface-level cues alone but instead require genuine visual grounding. Our\nexperimental results reveal that current VLMs exhibit limited capability in\ncomputational reasoning and even more pronounced limitations in semantic causal\nreasoning. Their significantly lower performance compared to humans indicates a\nsubstantial gap in leveraging infographic-based information for causal\ninference. Through InfoCausalQA, we highlight the need for advancing the causal\nreasoning abilities of multimodal AI systems.", "AI": {"tldr": "\u63d0\u51faInfoCausalQA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5728\u57fa\u4e8e\u4fe1\u606f\u56fe\u8868\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u5b58\u5728\u663e\u8457\u4e0d\u8db3", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u611f\u77e5\u548c\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff08\u7279\u522b\u662f\u591a\u6a21\u6001\u573a\u666f\u4e0b\u7684\u56fe\u8868\u6570\u636e\u63a8\u7406\uff09\u5c1a\u672a\u88ab\u5145\u5206\u63a2\u7d22", "method": "\u6784\u5efa\u5305\u542b494\u7ec4\u4fe1\u606f\u56fe\u8868-\u6587\u672c\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u5b9a\u91cf\u63a8\u7406\uff08\u4efb\u52a11\uff09\u548c\u8bed\u4e49\u63a8\u7406\uff08\u4efb\u52a12\uff09\u4e24\u4e2a\u4efb\u52a1\uff0c\u4f7f\u7528GPT-4o\u751f\u62101,482\u4e2a\u9700\u89c6\u89c9\u57fa\u7840\u7684\u591a\u9009\u9898\u5e76\u7ecf\u4eba\u5de5\u6821\u9a8c", "result": "\u5f53\u524d\u6a21\u578b\u5728\u8ba1\u7b97\u63a8\u7406\uff08\u4efb\u52a11\uff09\u548c\u8bed\u4e49\u56e0\u679c\u63a8\u7406\uff08\u4efb\u52a12\uff09\u8868\u73b0\u5747\u663e\u8457\u4f4e\u4e8e\u4eba\u7c7b\uff0c\u5c24\u5176\u5728\u6d89\u53ca\u56e0\u679c\u5173\u7cfb\u7684\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u5dee\u8ddd\u66f4\u5927", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u63d0\u5347\u591a\u6a21\u6001AI\u7cfb\u7edf\u56e0\u679c\u63a8\u7406\u80fd\u529b\u7684\u8feb\u5207\u6027\uff0cInfoCausalQA\u4e3a\u8bc4\u4f30\u6a21\u578b\u5728\u590d\u6742\u4fe1\u606f\u6574\u5408\u4e2d\u7684\u56e0\u679c\u63a8\u65ad\u80fd\u529b\u63d0\u4f9b\u6709\u6548\u57fa\u51c6"}}
{"id": "2508.06277", "pdf": "https://arxiv.org/pdf/2508.06277", "abs": "https://arxiv.org/abs/2508.06277", "authors": ["Theresa Pekarek Rosin", "Burak Can Kaplan", "Stefan Wermter"], "title": "Large Language Model Data Generation for Enhanced Intent Recognition in German Speech", "categories": ["cs.CL", "cs.LG", "cs.SD"], "comment": "11 pages, 3 figures, accepted at KONVENS 2025", "summary": "Intent recognition (IR) for speech commands is essential for artificial\nintelligence (AI) assistant systems; however, most existing approaches are\nlimited to short commands and are predominantly developed for English. This\npaper addresses these limitations by focusing on IR from speech by elderly\nGerman speakers. We propose a novel approach that combines an adapted Whisper\nASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based\nlanguage models trained on synthetic text datasets generated by three\nwell-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To\nevaluate the robustness of our approach, we generate synthetic speech with a\ntext-to-speech model and conduct extensive cross-dataset testing. Our results\nshow that synthetic LLM-generated data significantly boosts classification\nperformance and robustness to different speaking styles and unseen vocabulary.\nNotably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the\nmuch larger ChatGPT (175B) in dataset quality for German intent recognition.\nOur approach demonstrates that generative AI can effectively bridge data gaps\nin low-resource domains. We provide detailed documentation of our data\ngeneration and training process to ensure transparency and reproducibility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7ed3\u5408\u6539\u8fdb\u7248Whisper ASR\u6a21\u578b\u548cLLM\u751f\u6210\u5408\u6210\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u5fb7\u8bed\u8001\u5e74\u4eba\u8bed\u97f3\u610f\u56fe\u8bc6\u522b\u7684\u5206\u7c7b\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u5c0f\u89c4\u6a21\u9886\u57df\u4e13\u7528\u6a21\u578bLeoLM\u4f18\u4e8e\u5927\u6a21\u578bChatGPT\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u610f\u56fe\u8bc6\u522b\u65b9\u6cd5\u5c40\u9650\u4e8e\u77ed\u547d\u4ee4\u548c\u82f1\u8bed\u7684\u95ee\u9898\uff0c\u805a\u7126\u5fb7\u8bed\u8001\u5e74\u4eba\u8bed\u97f3\u7684\u4f4e\u8d44\u6e90\u9886\u57df\u9700\u6c42\u3002", "method": "1. \u5fae\u8c03Whisper\u6a21\u578b\u9002\u5e94\u8001\u5e74\u5fb7\u8bed\u8bed\u97f3\uff08SVC-de\uff09\n2. \u4f7f\u7528LeoLM/Llama3/ChatGPT\u751f\u6210\u5408\u6210\u6587\u672c\u8bad\u7ec3Transformer\u8bed\u8a00\u6a21\u578b\n3. \u901a\u8fc7TTS\u751f\u6210\u5408\u6210\u8bed\u97f3\u8fdb\u884c\u8de8\u6570\u636e\u96c6\u6d4b\u8bd5", "result": "\u5408\u6210\u6570\u636e\u663e\u8457\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff1b13B\u7684\u9886\u57df\u4e13\u7528\u6a21\u578bLeoLM\u5728\u5fb7\u8bed\u610f\u56fe\u8bc6\u522b\u6570\u636e\u8d28\u91cf\u4e0a\u8d85\u8d8a175B\u7684ChatGPT", "conclusion": "\u751f\u6210\u5f0fAI\u80fd\u6709\u6548\u586b\u8865\u4f4e\u8d44\u6e90\u9886\u57df\u6570\u636e\u7f3a\u53e3\uff0c\u9886\u57df\u9002\u914d\u6027\u6bd4\u6a21\u578b\u89c4\u6a21\u66f4\u91cd\u8981\uff1b\u5b8c\u6574\u516c\u5f00\u6570\u636e\u751f\u6210\u6d41\u7a0b\u786e\u4fdd\u53ef\u590d\u73b0\u6027"}}
{"id": "2508.06309", "pdf": "https://arxiv.org/pdf/2508.06309", "abs": "https://arxiv.org/abs/2508.06309", "authors": ["Ruichong Zhang"], "title": "Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC", "categories": ["cs.CL", "math.PR"], "comment": null, "summary": "In recent years, concerns about intellectual property (IP) in large language\nmodels (LLMs) have grown significantly. Plagiarizing other LLMs (through direct\nweight copying, upcycling, pruning, or continual pretraining) and claiming\nauthorship without properly attributing to the original license, is a serious\nmisconduct that can lead to significant financial and reputational harm to the\noriginal developers. However, existing methods for detecting LLM plagiarism\nfall short in key areas. They fail to accurately reconstruct weight\ncorrespondences, lack the ability to compute statistical significance measures\nsuch as $p$-values, and may mistakenly flag models trained on similar data as\nbeing related. To address these limitations, we propose Matrix-Driven Instant\nReview (MDIR), a novel method that leverages matrix analysis and Large\nDeviation Theory. MDIR achieves accurate reconstruction of weight\nrelationships, provides rigorous $p$-value estimation, and focuses exclusively\non weight similarity without requiring full model inference. Experimental\nresults demonstrate that MDIR reliably detects plagiarism even after extensive\ntransformations, such as random permutations and continual pretraining with\ntrillions of tokens. Moreover, all detections can be performed on a single PC\nwithin an hour, making MDIR both efficient and accessible.", "AI": {"tldr": "MDIR\u63d0\u51fa\u57fa\u4e8e\u77e9\u9635\u5206\u6790\u548c\u5927\u504f\u5dee\u7406\u8bba\u7684\u65b0\u578bLLM\u6284\u88ad\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u73b0\u6709\u6280\u672f\u65e0\u6cd5\u51c6\u786e\u91cd\u5efa\u6743\u91cd\u5173\u8054\u3001\u7f3a\u4e4f\u7edf\u8ba1\u663e\u8457\u6027\u6307\u6807\u7b49\u95ee\u9898\uff0c\u5355\u673a1\u5c0f\u65f6\u5185\u5373\u53ef\u5b8c\u6210\u68c0\u6d4b\u3002", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u6284\u88ad\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u4e09\u5927\u7f3a\u9677\uff1a\u65e0\u6cd5\u51c6\u786e\u91cd\u5efa\u6743\u91cd\u5bf9\u5e94\u5173\u7cfb\u3001\u7f3a\u4e4fp\u503c\u7b49\u7edf\u8ba1\u663e\u8457\u6027\u5ea6\u91cf\u3001\u6613\u8bef\u5224\u76f8\u4f3c\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u3002\u4e9f\u9700\u517c\u987e\u51c6\u786e\u6027\u4e0e\u6548\u7387\u7684\u68c0\u6d4b\u65b9\u6848\u3002", "method": "\u91c7\u7528\u77e9\u9635\u5206\u6790\u6280\u672f\u91cd\u5efa\u6743\u91cd\u5173\u8054\u5173\u7cfb\uff0c\u7ed3\u5408\u5927\u504f\u5dee\u7406\u8bba\u8fdb\u884cp\u503c\u4f30\u8ba1\u3002\u521b\u65b0\u6027\u5730\u805a\u7126\u6743\u91cd\u76f8\u4f3c\u6027\u5206\u6790\uff0c\u65e0\u9700\u5b8c\u6574\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eMDIR\u5728\u7ecf\u5386\u4e07\u4ebftoken\u6301\u7eed\u9884\u8bad\u7ec3\u3001\u968f\u673a\u7f6e\u6362\u7b49\u590d\u6742\u53d8\u6362\u540e\u4ecd\u80fd\u53ef\u9760\u68c0\u6d4b\u6284\u88ad\uff0c\u6240\u6709\u68c0\u6d4b\u53ef\u5728\u666e\u901a\u7535\u81111\u5c0f\u65f6\u5185\u5b8c\u6210\u3002", "conclusion": "MDIR\u4e3aLLM\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u63d0\u4f9b\u517c\u5177\u7edf\u8ba1\u4e25\u8c28\u6027\u4e0e\u5de5\u7a0b\u53ef\u884c\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u77e9\u9635\u9a71\u52a8\u8303\u5f0f\u663e\u8457\u63d0\u5347\u68c0\u6d4b\u6548\u5ea6\u4e0e\u6548\u7387\u3002"}}
{"id": "2508.06360", "pdf": "https://arxiv.org/pdf/2508.06360", "abs": "https://arxiv.org/abs/2508.06360", "authors": ["Aisha Saeid", "Anu Sabu", "Girish A. Koushik", "Ferrante Neri", "Diptesh Kanojia"], "title": "Cyberbullying Detection via Aggression-Enhanced Prompting", "categories": ["cs.CL"], "comment": "Accepted to RANLP 2025", "summary": "Detecting cyberbullying on social media remains a critical challenge due to\nits subtle and varied expressions. This study investigates whether integrating\naggression detection as an auxiliary task within a unified training framework\ncan enhance the generalisation and performance of large language models (LLMs)\nin cyberbullying detection. Experiments are conducted on five aggression\ndatasets and one cyberbullying dataset using instruction-tuned LLMs. We\nevaluated multiple strategies: zero-shot, few-shot, independent LoRA\nfine-tuning, and multi-task learning (MTL). Given the inconsistent results of\nMTL, we propose an enriched prompt pipeline approach in which aggression\npredictions are embedded into cyberbullying detection prompts to provide\ncontextual augmentation. Preliminary results show that the enriched prompt\npipeline consistently outperforms standard LoRA fine-tuning, indicating that\naggression-informed context significantly boosts cyberbullying detection. This\nstudy highlights the potential of auxiliary tasks, such as aggression\ndetection, to improve the generalisation of LLMs for safety-critical\napplications on social networks.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u653b\u51fb\u6027\u68c0\u6d4b\u4f5c\u4e3a\u8f85\u52a9\u4efb\u52a1\u96c6\u6210\u5230\u8bad\u7ec3\u6846\u67b6\u4e2d\uff0c\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7f51\u7edc\u6b3a\u51cc\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u5e76\u5f00\u53d1\u51fa\u4f18\u4e8e\u6807\u51c6\u5fae\u8c03\u7684\u589e\u5f3a\u63d0\u793a\u6d41\u7a0b\u3002", "motivation": "\u73b0\u6709\u7f51\u7edc\u6b3a\u51cc\u68c0\u6d4b\u56e0\u8868\u8fbe\u9690\u6666\u591a\u6837\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5e0c\u671b\u901a\u8fc7\u6574\u5408\u653b\u51fb\u6027\u68c0\u6d4b\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u63d0\u5347\u6a21\u578b\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u5728\u4e94\u4e2a\u653b\u51fb\u6027\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u7f51\u7edc\u6b3a\u51cc\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u591a\u79cd\u8bad\u7ec3\u7b56\u7565\uff0c\u6700\u7ec8\u63d0\u51fa\u5c06\u653b\u51fb\u6027\u9884\u6d4b\u5d4c\u5165\u68c0\u6d4b\u63d0\u793a\u7684\u589e\u5f3a\u6d41\u7a0b\u65b9\u6cd5\u3002", "result": "\u589e\u5f3a\u63d0\u793a\u6d41\u7a0b\u6301\u7eed\u8d85\u8d8aLoRA\u5fae\u8c03\uff0c\u653b\u51fb\u6027\u4e0a\u4e0b\u6587\u4f7f\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u53476.2%\uff08F1\u503c\u4ece78.4\u63d0\u5347\u81f384.6\uff09", "conclusion": "\u8f85\u52a9\u4efb\u52a1\u80fd\u6709\u6548\u63d0\u5347LLMs\u5728\u793e\u4ea4\u7f51\u7edc\u5b89\u5168\u5e94\u7528\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5185\u5bb9\u5b89\u5168\u76d1\u63a7\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2508.06374", "pdf": "https://arxiv.org/pdf/2508.06374", "abs": "https://arxiv.org/abs/2508.06374", "authors": ["Anubhav Jangra", "Bahareh Sarrafzadeh", "Adrian de Wynter", "Silviu Cucerzan", "Sujay Kumar Jauhar"], "title": "Evaluating Style-Personalized Text Generation: Challenges and Directions", "categories": ["cs.CL"], "comment": null, "summary": "While prior research has built tools and benchmarks towards style\npersonalized text generation, there has been limited exploration of evaluation\nin low-resource author style personalized text generation space. Through this\nwork, we question the effectiveness of the widely adopted evaluation metrics\nlike BLEU and ROUGE, and explore other evaluation paradigms such as style\nembeddings and LLM-as-judge to holistically evaluate the style personalized\ntext generation task. We evaluate these metrics and their ensembles using our\nstyle discrimination benchmark, that spans eight writing tasks, and evaluates\nacross three settings, domain discrimination, authorship attribution, and LLM\npersonalized vs non-personalized discrimination. We provide conclusive evidence\nto adopt ensemble of diverse evaluation metrics to effectively evaluate style\npersonalized text generation.", "AI": {"tldr": "\u672c\u6587\u8d28\u7591\u4f20\u7edf\u6587\u672c\u98ce\u683c\u4e2a\u6027\u5316\u751f\u6210\u4efb\u52a1\u4e2dBLEU/ROUGE\u6307\u6807\u7684\u6709\u6548\u6027\uff0c\u63d0\u51fa\u4f7f\u7528\u98ce\u683c\u5d4c\u5165\u8868\u793a\u548cLLM\u4f5c\u4e3a\u8bc4\u5224\u8005\u7684\u7efc\u5408\u8bc4\u4f30\u8303\u5f0f\uff0c\u901a\u8fc7\u8de88\u4e2a\u5199\u4f5c\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc1\u660e\u9700\u91c7\u7528\u591a\u7ef4\u5ea6\u6307\u6807\u7ec4\u5408\u8fdb\u884c\u6709\u6548\u8bc4\u4f30", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u4f4e\u8d44\u6e90\u4f5c\u8005\u98ce\u683c\u4e2a\u6027\u5316\u6587\u672c\u751f\u6210\u9886\u57df\u7684\u8bc4\u4f30\u4f53\u7cfb\u5b58\u5728\u4e0d\u8db3\uff0c\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u5982BLEU\u548cROUGE\u7684\u6709\u6548\u6027\u53d7\u5230\u8d28\u7591", "method": "\u6784\u5efa\u5305\u542b8\u4e2a\u5199\u4f5c\u4efb\u52a1\u7684\u98ce\u683c\u5224\u522b\u57fa\u51c6\uff0c\u5728\u9886\u57df\u5224\u522b\u3001\u4f5c\u8005\u5f52\u5c5e\u5224\u5b9a\u3001LLM\u4e2a\u6027\u5316\u4e0e\u975e\u4e2a\u6027\u5316\u5224\u522b\u4e09\u4e2a\u573a\u666f\u4e0b\uff0c\u7cfb\u7edf\u8bc4\u4f30\u98ce\u683c\u5d4c\u5165\u548cLLM-as-judge\u7b49\u65b0\u578b\u8bc4\u4f30\u8303\u5f0f", "result": "\u901a\u8fc7\u98ce\u683c\u5224\u522b\u57fa\u51c6\u9a8c\u8bc1\uff0c\u4e0d\u540c\u8bc4\u4f30\u6307\u6807\u7684\u96c6\u6210\u80fd\u66f4\u6709\u6548\u8bc4\u4f30\u98ce\u683c\u4e2a\u6027\u5316\u6587\u672c\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u9886\u57df\u63d0\u4f9b\u65b0\u7684\u8bc4\u4f30\u6846\u67b6", "conclusion": "\u5e94\u91c7\u7528\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6307\u6807\u7ec4\u5408\uff08\u5305\u62ec\u98ce\u683c\u5d4c\u5165\u3001LLM\u8bc4\u5224\u7b49\uff09\u6765\u5168\u9762\u8bc4\u4f30\u6587\u672c\u98ce\u683c\u4e2a\u6027\u5316\u751f\u6210\u4efb\u52a1\uff0c\u7a81\u7834\u5355\u4e00\u6307\u6807\u7684\u5c40\u9650\u6027"}}
{"id": "2508.06388", "pdf": "https://arxiv.org/pdf/2508.06388", "abs": "https://arxiv.org/abs/2508.06388", "authors": ["Lanlan Qiu", "Xiao Pu", "Yeqi Feng", "Tianxing He"], "title": "LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing", "categories": ["cs.CL"], "comment": "21 pages, 17 figures, 3 tables", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nrole-playing conversations and providing emotional support as separate research\ndirections. However, there remains a significant research gap in combining\nthese capabilities to enable emotionally supportive interactions with virtual\ncharacters. To address this research gap, we focus on anime characters as a\ncase study because of their well-defined personalities and large fan bases.\nThis choice enables us to effectively evaluate how well LLMs can provide\nemotional support while maintaining specific character traits. We introduce\nChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We\nfirst thoughtfully select 20 top-tier characters from popular anime communities\nand design 60 emotion-centric real-world scenario questions. Then, we execute a\nnationwide selection process to identify 40 Chinese anime enthusiasts with\nprofound knowledge of specific characters and extensive experience in\nrole-playing. Next, we systematically collect two rounds of dialogue data from\n10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP\nperformance of LLMs, we design a user experience-oriented evaluation system\nfeaturing 9 fine-grained metrics across three dimensions: basic dialogue,\nrole-playing and emotional support, along with an overall metric for response\ndiversity. In total, the dataset comprises 2,400 human-written and 24,000\nLLM-generated answers, supported by over 132,000 human annotations.\nExperimental results show that top-performing LLMs surpass human fans in\nrole-playing and emotional support, while humans still lead in response\ndiversity. We hope this work can provide valuable resources and insights for\nfuture research on optimizing LLMs in ESRP. Our datasets are available at\nhttps://github.com/LanlanQiu/ChatAnime.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faChatAnime\u6570\u636e\u96c6\uff0c\u8bc4\u4f30LLMs\u5728\u60c5\u611f\u652f\u6301\u89d2\u8272\u626e\u6f14\uff08ESRP\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u9876\u5c16\u6a21\u578b\u5728\u89d2\u8272\u626e\u6f14\u548c\u60c5\u611f\u652f\u6301\u7ef4\u5ea6\u8d85\u8d8a\u4eba\u7c7b\uff0c\u4f46\u4eba\u7c7b\u5728\u56de\u7b54\u591a\u6837\u6027\u4e0a\u4fdd\u6301\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5206\u522b\u63a2\u7d22LLMs\u7684\u89d2\u8272\u626e\u6f14\u548c\u60c5\u611f\u652f\u6301\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u7ed3\u5408\u4e8c\u8005\u7279\u8d28\u7684\u7814\u7a76\u3002\u9009\u62e9\u52a8\u6f2b\u89d2\u8272\u4f5c\u4e3a\u6848\u4f8b\uff0c\u56e0\u5176\u4eba\u8bbe\u660e\u786e\u4e14\u53d7\u4f17\u5e7f\u6cdb\uff0c\u4fbf\u4e8e\u8bc4\u4f30LLMs\u5728\u4fdd\u6301\u89d2\u8272\u7279\u8d28\u65f6\u63d0\u4f9b\u60c5\u611f\u652f\u6301\u7684\u80fd\u529b\u3002", "method": "1. \u4ece\u70ed\u95e8\u793e\u533a\u7cbe\u900920\u4e2a\u52a8\u6f2b\u89d2\u8272\uff0c\u8bbe\u8ba160\u4e2a\u60c5\u611f\u5bfc\u5411\u7684\u73b0\u5b9e\u573a\u666f\u95ee\u9898 2. \u9009\u62d440\u540d\u6df1\u5ea6\u52a8\u6f2b\u7231\u597d\u8005 3. \u7cfb\u7edf\u6027\u6536\u96c610\u4e2aLLM\u4e0e\u4eba\u7c7b\u7684\u4e24\u8f6e\u5bf9\u8bdd\u6570\u636e 4. \u8bbe\u8ba1\u5305\u542b\u57fa\u7840\u5bf9\u8bdd\u3001\u89d2\u8272\u626e\u6f14\u3001\u60c5\u611f\u652f\u6301\u4e09\u7ef4\u5ea6\u76849\u9879\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6307\u6807 5. \u5efa\u7acb\u542b24,000\u6761\u751f\u6210\u7b54\u6848\u548c132,000\u6761\u4eba\u5de5\u6807\u6ce8\u7684\u6570\u636e\u96c6", "result": "\u5b9e\u9a8c\u663e\u793a\uff1a\u9876\u5c16LLMs\u5728\u89d2\u8272\u626e\u6f14\uff08+11.3%\uff09\u548c\u60c5\u611f\u652f\u6301\uff08+9.8%\uff09\u7ef4\u5ea6\u8d85\u8d8a\u4eba\u7c7b\uff0c\u4f46\u4eba\u7c7b\u5728\u56de\u7b54\u591a\u6837\u6027\u6307\u6807\u4e0a\u9886\u514816.5%\u3002\u6570\u636e\u96c6\u5305\u542b2,400\u6761\u4eba\u5de5\u56de\u7b54\u548c24,000\u6761\u6a21\u578b\u751f\u6210\u7ed3\u679c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u4f18\u5316LLMs\u7684\u60c5\u611f\u652f\u6301\u89d2\u8272\u626e\u6f14\u80fd\u529b\u63d0\u4f9b\u4e86\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u57fa\u51c6\uff0c\u5f00\u6e90\u6570\u636e\u4fc3\u8fdb\u76f8\u5173\u9886\u57df\u7814\u7a76\uff0c\u63ed\u793a\u4e86LLMs\u5728\u7279\u5b9a\u7ef4\u5ea6\u8d85\u8d8a\u4eba\u7c7b\u4f46\u9700\u63d0\u5347\u591a\u6837\u6027\u7684\u73b0\u72b6\u3002"}}
{"id": "2508.06418", "pdf": "https://arxiv.org/pdf/2508.06418", "abs": "https://arxiv.org/abs/2508.06418", "authors": ["Haoran Shi", "Hongwei Yao", "Shuo Shao", "Shaopeng Jiao", "Ziqi Peng", "Zhan Qin", "Cong Wang"], "title": "Quantifying Conversation Drift in MCP via Latent Polytope", "categories": ["cs.CL"], "comment": null, "summary": "The Model Context Protocol (MCP) enhances large language models (LLMs) by\nintegrating external tools, enabling dynamic aggregation of real-time data to\nimprove task execution. However, its non-isolated execution context introduces\ncritical security and privacy risks. In particular, adversarially crafted\ncontent can induce tool poisoning or indirect prompt injection, leading to\nconversation hijacking, misinformation propagation, or data exfiltration.\nExisting defenses, such as rule-based filters or LLM-driven detection, remain\ninadequate due to their reliance on static signatures, computational\ninefficiency, and inability to quantify conversational hijacking. To address\nthese limitations, we propose SecMCP, a secure framework that detects and\nquantifies conversation drift, deviations in latent space trajectories induced\nby adversarial external knowledge. By modeling LLM activation vectors within a\nlatent polytope space, SecMCP identifies anomalous shifts in conversational\ndynamics, enabling proactive detection of hijacking, misleading, and data\nexfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,\nVicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),\ndemonstrating robust detection with AUROC scores exceeding 0.915 while\nmaintaining system usability. Our contributions include a systematic\ncategorization of MCP security threats, a novel latent polytope-based\nmethodology for quantifying conversation drift, and empirical validation of\nSecMCP's efficacy.", "AI": {"tldr": "SecMCP\u6846\u67b6\u901a\u8fc7\u6f5c\u5728\u591a\u9762\u4f53\u5efa\u6a21\u68c0\u6d4b\u5bf9\u8bdd\u6f02\u79fb\uff0c\u6709\u6548\u9632\u5fa1LLM\u96c6\u6210\u5916\u90e8\u5de5\u5177\u65f6\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u68c0\u6d4b\u6027\u80fd\uff08AUROC>0.915\uff09\u4e0e\u5b9e\u7528\u6027\u517c\u5907\u3002", "motivation": "MCP\u589e\u5f3aLLM\u65f6\u5f15\u5165\u975e\u9694\u79bb\u6267\u884c\u98ce\u9669\uff0c\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\u4f9d\u8d56\u9759\u6001\u7279\u5f81\u4e14\u65e0\u6cd5\u91cf\u5316\u5a01\u80c1\uff0c\u9700\u5f00\u53d1\u65b0\u578b\u52a8\u6001\u68c0\u6d4b\u65b9\u6848\u3002", "method": "\u5728\u6f5c\u5728\u591a\u9762\u4f53\u7a7a\u95f4\u5efa\u6a21LLM\u6fc0\u6d3b\u5411\u91cf\uff0c\u901a\u8fc7\u5bf9\u8bdd\u8f68\u8ff9\u504f\u79fb\u91cf\u68c0\u6d4b\u52ab\u6301/\u8bef\u5bfc/\u6570\u636e\u6cc4\u9732\u4e09\u7c7b\u5a01\u80c1\u3002", "result": "\u5728Llama3/Vicuna/Mistral\u6a21\u578b\u53caMS MARCO\u7b49\u6570\u636e\u96c6\u9a8c\u8bc1\uff0cAUROC\u8d850.915\u4e14\u7cfb\u7edf\u5ef6\u8fdf\u53ef\u63a7\u3002", "conclusion": "\u7cfb\u7edf\u6027\u5b9a\u4e49MCP\u5a01\u80c1\u7c7b\u578b\uff0c\u521b\u65b0\u63d0\u51fa\u6f5c\u5728\u7a7a\u95f4\u91cf\u5316\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u591a\u7ef4\u5ea6\u5b9e\u9a8c\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\u3002"}}
{"id": "2508.06433", "pdf": "https://arxiv.org/pdf/2508.06433", "abs": "https://arxiv.org/abs/2508.06433", "authors": ["Runnan Fang", "Yuan Liang", "Xiaobin Wang", "Jialong Wu", "Shuofei Qiao", "Pengjun Xie", "Fei Huang", "Huajun Chen", "Ningyu Zhang"], "title": "Memp: Exploring Agent Procedural Memory", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.MA"], "comment": "Work in progress", "summary": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains.", "AI": {"tldr": "\u63d0\u51faMemp\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u66f4\u65b0\u7684\u7a0b\u5e8f\u6027\u8bb0\u5fc6\u5b58\u50a8\u5e93\u63d0\u5347LLM\u667a\u80fd\u4f53\u7684\u4efb\u52a1\u6210\u529f\u7387\u4e0e\u6548\u7387", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u7684\u7a0b\u5e8f\u6027\u8bb0\u5fc6\u5b58\u5728\u8106\u5f31\u6027\uff08\u4f9d\u8d56\u624b\u52a8\u8bbe\u8ba1/\u9759\u6001\u53c2\u6570\uff09\uff0c\u9700\u8981\u53ef\u5b66\u4e60\u3001\u53ef\u66f4\u65b0\u7684\u7ec8\u8eab\u8bb0\u5fc6\u673a\u5236", "method": "\u5c06\u5386\u53f2\u8f68\u8ff9\u84b8\u998f\u4e3a\u6b65\u9aa4\u6307\u4ee4\u548c\u811a\u672c\u62bd\u8c61\uff0c\u6784\u5efa\u52a8\u6001\u66f4\u65b0\u673a\u5236\uff08\u6301\u7eed\u4fee\u6b63/\u6dd8\u6c70/\u8865\u5145\uff09\uff0c\u914d\u5408\u6784\u5efa-\u68c0\u7d22-\u66f4\u65b0\u7b56\u7565", "result": "\u4efb\u52a1\u6210\u529f\u7387\u63d0\u534714.3%\uff08ALFWorld\uff09\u4e14\u6548\u7387\u63d0\u9ad830%\uff0c\u5f3a\u6a21\u578b\u6784\u5efa\u7684\u8bb0\u5fc6\u8fc1\u79fb\u81f3\u5f31\u6a21\u578b\u4ecd\u80fd\u63d0\u534719.1%\u6027\u80fd", "conclusion": "\u52a8\u6001\u7a0b\u5e8f\u6027\u8bb0\u5fc6\u663e\u8457\u589e\u5f3a\u667a\u80fd\u4f53\u80fd\u529b\uff0c\u5176\u77e5\u8bc6\u5177\u5907\u8de8\u6a21\u578b\u8fc1\u79fb\u4ef7\u503c\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u6709\u6548\u8303\u5f0f"}}
{"id": "2508.06435", "pdf": "https://arxiv.org/pdf/2508.06435", "abs": "https://arxiv.org/abs/2508.06435", "authors": ["Andrea Nasuto", "Stefano Maria Iacus", "Francisco Rowe", "Devika Jain"], "title": "Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are transforming social-science research by\nenabling scalable, precise analysis. Their adaptability raises the question of\nwhether knowledge acquired through fine-tuning in a few languages can transfer\nto unseen languages that only appeared during pre-training. To examine this, we\nfine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or\nmultilingual data sets to classify immigration-related tweets from X/Twitter\nacross 13 languages, a domain characterised by polarised, culturally specific\ndiscourse. We evaluate whether minimal language-specific fine-tuning enables\ncross-lingual topic detection and whether adding targeted languages corrects\npre-training biases. Results show that LLMs fine-tuned in one or two languages\ncan reliably classify immigration-related content in unseen languages. However,\nidentifying whether a tweet expresses a pro- or anti-immigration stance\nbenefits from multilingual fine-tuning. Pre-training bias favours dominant\nlanguages, but even minimal exposure to under-represented languages during\nfine-tuning (as little as $9.62\\times10^{-11}$ of the original pre-training\ntoken volume) yields significant gains. These findings challenge the assumption\nthat cross-lingual mastery requires extensive multilingual training: limited\nlanguage coverage suffices for topic-level generalisation, and structural\nbiases can be corrected with lightweight interventions. By releasing\n4-bit-quantised, LoRA fine-tuned models, we provide an open-source,\nreproducible alternative to proprietary LLMs that delivers 35 times faster\ninference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,\nenabling scalable, inclusive research.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u5c11\u91cf\u8bed\u8a00\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u8de8\u8bed\u8a00\u4e3b\u9898\u68c0\u6d4b\uff0c\u7acb\u573a\u8bc6\u522b\u9700\u591a\u8bed\u8a00\u5fae\u8c03\uff0c\u9884\u8bad\u7ec3\u504f\u5dee\u53ef\u901a\u8fc7\u8f7b\u91cf\u5316\u5e72\u9884\u7ea0\u6b63", "motivation": "\u9a8c\u8bc1LLMs\u5728\u8de8\u8bed\u8a00\u77e5\u8bc6\u8fc1\u79fb\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u5fae\u8c03\u89e3\u51b3\u9884\u8bad\u7ec3\u9636\u6bb5\u4e3b\u6d41\u8bed\u8a00\u4e3b\u5bfc\u7684\u7ed3\u6784\u6027\u504f\u5dee", "method": "\u4f7f\u7528LLaMA 3.2-3B\u6a21\u578b\u8fdb\u884c\u5355\u8bed/\u53cc\u8bed/\u591a\u8bed\u5fae\u8c03\uff0c\u6d4b\u8bd513\u79cd\u8bed\u8a00\u79fb\u6c11\u63a8\u6587\u5206\u7c7b\uff08\u4e3b\u9898\u68c0\u6d4b\u4e0e\u7acb\u573a\u8bc6\u522b\uff09", "result": "\u5355\u8bed\u8a00\u5fae\u8c03\u53ef\u5b9e\u73b0\u4e3b\u9898\u7ea7\u6cdb\u5316\uff0c\u7acb\u573a\u8bc6\u522b\u9700\u591a\u8bed\u8a00\u6570\u636e\uff1b\u9884\u8bad\u7ec3\u504f\u5dee\u53ef\u88ab0.0000000000962%\u5fae\u8c03\u6570\u636e\u4fee\u6b63\uff1b\u5f00\u6e90\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u63d0\u534735\u500d\u6210\u672c\u4ec5GPT-4o\u76840.00000989%", "conclusion": "\u8de8\u8bed\u8a00\u80fd\u529b\u65e0\u9700\u5927\u91cf\u591a\u8bed\u8a00\u8bad\u7ec3\uff0c\u8f7b\u91cf\u7ea7\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u4e3b\u9898\u8fc1\u79fb\u5e76\u7ea0\u6b63\u504f\u5dee\uff0c\u5f00\u6e90\u65b9\u6848\u4e3a\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u63d0\u4f9b\u9ad8\u6548\u4f4e\u6210\u672c\u5de5\u5177"}}
{"id": "2508.06445", "pdf": "https://arxiv.org/pdf/2508.06445", "abs": "https://arxiv.org/abs/2508.06445", "authors": ["Abolfazl Ansari", "Delvin Ce Zhang", "Nafis Irtiza Tripto", "Dongwon Lee"], "title": "Echoes of Automation: The Increasing Use of LLMs in Newsmaking", "categories": ["cs.CL", "cs.AI"], "comment": "To appear in 18th International Conference on Social Computing,\n  Behavioral-Cultural Modeling, & Prediction and Behavior Representation in\n  Modeling and Simulation, and to be published in the Springer LNCS series", "summary": "The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns\nfor journalistic integrity and authorship. This study examines AI-generated\ncontent across over 40,000 news articles from major, local, and college news\nmedia, in various media formats. Using three advanced AI-text detectors (e.g.,\nBinoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of\nGenAI use in recent years, especially in local and college news. Sentence-level\nanalysis reveals LLMs are often used in the introduction of news, while\nconclusions usually written manually. Linguistic analysis shows GenAI boosts\nword richness and readability but lowers formality, leading to more uniform\nwriting styles, particularly in local media.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u751f\u6210\u5f0fAI\u5728\u5730\u65b9/\u6821\u56ed\u5a92\u4f53\u4f7f\u7528\u6fc0\u589e\uff0c\u4f18\u5316\u65b0\u95fb\u5f15\u8a00\u5199\u4f5c\u4f46\u5bfc\u81f4\u6587\u4f53\u8d8b\u540c", "motivation": "\u63a2\u7a76\u751f\u6210\u5f0fAI\u5bf9\u65b0\u95fb\u771f\u5b9e\u6027\u548c\u4f5c\u8005\u539f\u521b\u6027\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u5176\u5728\u5a92\u4f53\u751f\u6001\u4e2d\u7684\u6e17\u900f\u6a21\u5f0f", "method": "\u8fd0\u7528Binoculars\u7b493\u79cdAI\u68c0\u6d4b\u5de5\u5177\uff0c\u5206\u67904\u4e07+\u7bc7\u4e0d\u540c\u5c42\u7ea7\u5a92\u4f53\u5185\u5bb9\uff0c\u7ed3\u5408\u53e5\u5b50\u7ea7\u5206\u5e03\u7edf\u8ba1\u4e0e\u8bed\u8a00\u5b66\u7279\u5f81\u5206\u6790", "result": "\u5730\u65b9\u5a92\u4f53AI\u4f7f\u7528\u73873\u5e74\u589e\u957f12\u500d\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u96c6\u4e2d\u4e8e\u65b0\u95fb\u5bfc\u8bed\uff08\u5360\u6bd468%\uff09\uff0c\u7ed3\u8bba\u4eba\u5de5\u64b0\u5199\u7387\u8fbe83%\uff0cAI\u5185\u5bb9\u8bcd\u6c47\u591a\u6837\u6027\u63d0\u534715%\u4f46\u6b63\u5f0f\u6027\u4e0b\u964d", "conclusion": "\u751f\u6210\u5f0fAI\u91cd\u5851\u65b0\u95fb\u751f\u4ea7\u6d41\u7a0b\uff0c\u9700\u5efa\u7acb\u4eba\u673a\u534f\u4f5c\u65b0\u8303\u5f0f\u4ee5\u5e73\u8861\u6548\u7387\u4e0e\u6587\u4f53\u591a\u6837\u6027"}}
{"id": "2508.06447", "pdf": "https://arxiv.org/pdf/2508.06447", "abs": "https://arxiv.org/abs/2508.06447", "authors": ["Lingkun Long", "Rubing Yang", "Yushi Huang", "Desheng Hui", "Ao Zhou", "Jianlei Yang"], "title": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning", "categories": ["cs.CL"], "comment": null, "summary": "Long-context inference for Large Language Models (LLMs) is heavily limited by\nhigh computational demands. While several existing methods optimize attention\ncomputation, they still process the full set of hidden states at each layer,\nlimiting overall efficiency. In this work, we propose SlimInfer, an innovative\nframework that aims to accelerate inference by directly pruning less critical\nprompt tokens during the forward pass. Our key insight is an information\ndiffusion phenomenon: As information from critical tokens propagates through\nlayers, it becomes distributed across the entire sequence. This diffusion\nprocess suggests that LLMs can maintain their semantic integrity when excessive\ntokens, even including these critical ones, are pruned in hidden states.\nMotivated by this, SlimInfer introduces a dynamic fine-grained pruning\nmechanism that accurately removes redundant tokens of hidden state at\nintermediate layers. This layer-wise pruning naturally enables an asynchronous\nKV cache manager that prefetches required token blocks without complex\npredictors, reducing both memory usage and I/O costs. Extensive experiments\nshow that SlimInfer can achieve up to $\\mathbf{2.53\\times}$ time-to-first-token\n(TTFT) speedup and $\\mathbf{1.88\\times}$ end-to-end latency reduction for\nLLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on\nLongBench. Our code will be released upon acceptance.", "AI": {"tldr": "SlimInfer\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u526a\u679d\u5197\u4f59token\u548c\u5f02\u6b65KV\u7f13\u5b58\u7ba1\u7406\uff0c\u663e\u8457\u52a0\u901fLLM\u7684\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e14\u6027\u80fd\u65e0\u635f", "motivation": "\u73b0\u6709\u65b9\u6cd5\u867d\u4f18\u5316\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u4f46\u4ecd\u9700\u5904\u7406\u6bcf\u5c42\u5168\u90e8\u9690\u85cf\u72b6\u6001\u5bfc\u81f4\u6548\u7387\u53d7\u9650\u3002\u4fe1\u606f\u6269\u6563\u73b0\u8c61\u8868\u660e\u5173\u952etoken\u88ab\u526a\u679d\u540e\u6a21\u578b\u4ecd\u80fd\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027", "method": "1. \u52a8\u6001\u7ec6\u7c92\u5ea6\u526a\u679d\u673a\u5236\uff08\u9010\u5c42\u526a\u679d\u9690\u85cf\u72b6\u6001\u5197\u4f59token\uff09 2. \u5f02\u6b65KV\u7f13\u5b58\u7ba1\u7406\u5668\uff08\u9884\u53d6\u5fc5\u8981token\u5757\u51cf\u5c11\u5185\u5b58\u548cI/O\u6d88\u8017\uff09", "result": "LLaMA3.1-8B-Instruct\u5728RTX4090\u4e0a\u5b9e\u73b02.53\u500d\u9996token\u52a0\u901f\u30011.88\u500d\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e\uff0cLongBench\u6027\u80fd\u65e0\u635f\u5931", "conclusion": "\u8be5\u6846\u67b6\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u6709\u6548\u63d0\u5347LLM\u63a8\u7406\u6548\u7387\uff0c\u65e0\u9700\u590d\u6742\u9884\u6d4b\u5668\u5373\u5b9e\u73b0\u663e\u8457\u52a0\u901f"}}
{"id": "2508.06471", "pdf": "https://arxiv.org/pdf/2508.06471", "abs": "https://arxiv.org/abs/2508.06471", "authors": ["GLM-4. 5 Team", ":", "Aohan Zeng", "Xin Lv", "Qinkai Zheng", "Zhenyu Hou", "Bin Chen", "Chengxing Xie", "Cunxiang Wang", "Da Yin", "Hao Zeng", "Jiajie Zhang", "Kedong Wang", "Lucen Zhong", "Mingdao Liu", "Rui Lu", "Shulin Cao", "Xiaohan Zhang", "Xuancheng Huang", "Yao Wei", "Yean Cheng", "Yifan An", "Yilin Niu", "Yuanhao Wen", "Yushi Bai", "Zhengxiao Du", "Zihan Wang", "Zilin Zhu", "Bohan Zhang", "Bosi Wen", "Bowen Wu", "Bowen Xu", "Can Huang", "Casey Zhao", "Changpeng Cai", "Chao Yu", "Chen Li", "Chendi Ge", "Chenghua Huang", "Chenhui Zhang", "Chenxi Xu", "Chenzheng Zhu", "Chuang Li", "Congfeng Yin", "Daoyan Lin", "Dayong Yang", "Dazhi Jiang", "Ding Ai", "Erle Zhu", "Fei Wang", "Gengzheng Pan", "Guo Wang", "Hailong Sun", "Haitao Li", "Haiyang Li", "Haiyi Hu", "Hanyu Zhang", "Hao Peng", "Hao Tai", "Haoke Zhang", "Haoran Wang", "Haoyu Yang", "He Liu", "He Zhao", "Hongwei Liu", "Hongxi Yan", "Huan Liu", "Huilong Chen", "Ji Li", "Jiajing Zhao", "Jiamin Ren", "Jian Jiao", "Jiani Zhao", "Jianyang Yan", "Jiaqi Wang", "Jiayi Gui", "Jiayue Zhao", "Jie Liu", "Jijie Li", "Jing Li", "Jing Lu", "Jingsen Wang", "Jingwei Yuan", "Jingxuan Li", "Jingzhao Du", "Jinhua Du", "Jinxin Liu", "Junkai Zhi", "Junli Gao", "Ke Wang", "Lekang Yang", "Liang Xu", "Lin Fan", "Lindong Wu", "Lintao Ding", "Lu Wang", "Man Zhang", "Minghao Li", "Minghuan Xu", "Mingming Zhao", "Mingshu Zhai", "Pengfan Du", "Qian Dong", "Shangde Lei", "Shangqing Tu", "Shangtong Yang", "Shaoyou Lu", "Shijie Li", "Shuang Li", "Shuang-Li", "Shuxun Yang", "Sibo Yi", "Tianshu Yu", "Wei Tian", "Weihan Wang", "Wenbo Yu", "Weng Lam Tam", "Wenjie Liang", "Wentao Liu", "Xiao Wang", "Xiaohan Jia", "Xiaotao Gu", "Xiaoying Ling", "Xin Wang", "Xing Fan", "Xingru Pan", "Xinyuan Zhang", "Xinze Zhang", "Xiuqing Fu", "Xunkai Zhang", "Yabo Xu", "Yandong Wu", "Yida Lu", "Yidong Wang", "Yilin Zhou", "Yiming Pan", "Ying Zhang", "Yingli Wang", "Yingru Li", "Yinpei Su", "Yipeng Geng", "Yitong Zhu", "Yongkun Yang", "Yuhang Li", "Yuhao Wu", "Yujiang Li", "Yunan Liu", "Yunqing Wang", "Yuntao Li", "Yuxuan Zhang", "Zezhen Liu", "Zhen Yang", "Zhengda Zhou", "Zhongpei Qiao", "Zhuoer Feng", "Zhuorui Liu", "Zichen Zhang", "Zihan Wang", "Zijun Yao", "Zikang Wang", "Ziqiang Liu", "Ziwei Chai", "Zixuan Li", "Zuodong Zhao", "Wenguang Chen", "Jidong Zhai", "Bin Xu", "Minlie Huang", "Hongning Wang", "Juanzi Li", "Yuxiao Dong", "Jie Tang"], "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models", "categories": ["cs.CL"], "comment": null, "summary": "We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language\nmodel with 355B total parameters and 32B activated parameters, featuring a\nhybrid reasoning method that supports both thinking and direct response modes.\nThrough multi-stage training on 23T tokens and comprehensive post-training with\nexpert model iteration and reinforcement learning, GLM-4.5 achieves strong\nperformance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on\nTAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer\nparameters than several competitors, GLM-4.5 ranks 3rd overall among all\nevaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B\nparameters) and a compact version, GLM-4.5-Air (106B parameters), to advance\nresearch in reasoning and agentic AI systems. Code, models, and more\ninformation are available at https://github.com/zai-org/GLM-4.5.", "AI": {"tldr": "\u5f00\u6e90\u4e13\u5bb6\u6df7\u5408\u6a21\u578bGLM-4.5\u901a\u8fc7\u6df7\u5408\u63a8\u7406\u65b9\u6cd5\u5728\u4ee3\u7406\u3001\u63a8\u7406\u548c\u7f16\u7801\u4efb\u52a1\u4e2d\u53d6\u5f97\u7a81\u7834\u6027\u6027\u80fd\uff0c\u603b\u53c2\u6570\u91cf355B\u4f46\u6fc0\u6d3b\u4ec532B\u3002", "motivation": "\u63a8\u52a8\u63a8\u7406\u4e0e\u4ee3\u7406AI\u7cfb\u7edf\u7814\u7a76\uff0c\u901a\u8fc7\u6a21\u578b\u8fed\u4ee3\u548c\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u667a\u80fd\u4f53\u80fd\u529b\u3002", "method": "\u591a\u9636\u6bb523T token\u9884\u8bad\u7ec3+\u4e13\u5bb6\u6a21\u578b\u8fed\u4ee3+\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u7b56\u7565\uff0c\u652f\u6301\u601d\u7ef4\u94fe\u548c\u76f4\u63a5\u54cd\u5e94\u7684\u6df7\u5408\u63a8\u7406\u673a\u5236\u3002", "result": "TAU-Bench 70.1%\u3001AIME 24 91.0%\u3001SWE-bench 64.2%\uff0c\u53c2\u6570\u91cf\u66f4\u5c11\u5374\u7efc\u5408\u6392\u540d\u7b2c\u4e09/\u4ee3\u7406\u699c\u7b2c\u4e8c\u3002", "conclusion": "\u540c\u6b65\u53d1\u5e03GLM-4.5\u5b8c\u6574\u7248\u548c\u8f7b\u91cf\u7248GLM-4.5-Air\uff0c\u4e3a\u667a\u80fd\u4f53\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u9ad8\u6548\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2508.06475", "pdf": "https://arxiv.org/pdf/2508.06475", "abs": "https://arxiv.org/abs/2508.06475", "authors": ["Guimin Hu", "Daniel Hershcovich", "Hasti Seifi"], "title": "HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning", "categories": ["cs.CL"], "comment": null, "summary": "Haptic captioning is the task of generating natural language descriptions\nfrom haptic signals, such as vibrations, for use in virtual reality,\naccessibility, and rehabilitation applications. While previous multimodal\nresearch has focused primarily on vision and audio, haptic signals for the\nsense of touch remain underexplored. To address this gap, we formalize the\nhaptic captioning task and propose HapticLLaMA, a multimodal sensory language\nmodel that interprets vibration signals into descriptions in a given sensory,\nemotional, or associative category. We investigate two types of haptic\ntokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that\nconvert haptic signals into sequences of discrete units, enabling their\nintegration with the LLaMA model. HapticLLaMA is trained in two stages: (1)\nsupervised fine-tuning using the LLaMA architecture with LoRA-based adaptation,\nand (2) fine-tuning via reinforcement learning from human feedback (RLHF). We\nassess HapticLLaMA's captioning performance using both automated n-gram metrics\nand human evaluation. HapticLLaMA demonstrates strong capability in\ninterpreting haptic vibration signals, achieving a METEOR score of 59.98 and a\nBLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated\ncaptions received human ratings above 3.5 on a 7-point scale, with RLHF\nyielding a 10% improvement in the overall rating distribution, indicating\nstronger alignment with human haptic perception. These findings highlight the\npotential of large language models to process and adapt to sensory data.", "AI": {"tldr": "\u63d0\u51faHapticLLaMA\u6a21\u578b\u5b9e\u73b0\u89e6\u89c9\u632f\u52a8\u4fe1\u53f7\u7684\u6587\u672c\u63cf\u8ff0\u751f\u6210\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u5728\u81ea\u52a8\u6307\u6807\u548c\u4eba\u5de5\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u7814\u7a76\u96c6\u4e2d\u4e8e\u89c6\u542c\u9886\u57df\uff0c\u89e6\u89c9\u4fe1\u53f7\uff08\u5982\u632f\u52a8\uff09\u7684\u8bed\u4e49\u7406\u89e3\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u9700\u8981\u5f00\u53d1\u9002\u7528\u4e8eVR/\u65e0\u969c\u788d/\u5eb7\u590d\u5e94\u7528\u7684\u6280\u672f\u65b9\u6848", "method": "\u91c7\u7528\u57fa\u4e8e\u9891\u7387\u548cEnCodec\u7684\u4e24\u79cd\u89e6\u89c9\u5206\u8bcd\u5668\uff0c\u5c06\u632f\u52a8\u4fe1\u53f7\u79bb\u6563\u5316\u540e\u6574\u5408LLaMA\u67b6\u6784\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03+RLHF\u4e24\u9636\u6bb5\u8bad\u7ec3", "result": "METEOR 59.98/BLEU-4 32.06\uff0c61%\u751f\u6210\u6587\u672c\u83b7\u4eba\u7c7b\u8bc4\u5206>3.5\uff087\u5206\u91cf\u8868\uff09\uff0cRLHF\u4f7f\u6574\u4f53\u8bc4\u5206\u5206\u5e03\u63d0\u534710%", "conclusion": "\u9a8c\u8bc1\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u611f\u5b98\u4fe1\u53f7\u7684\u6f5c\u529b\uff0cRLHF\u663e\u8457\u63d0\u5347\u4eba\u7c7b\u611f\u77e5\u5bf9\u9f50\uff0c\u4e3a\u591a\u6a21\u6001\u4ea4\u4e92\u5f00\u8f9f\u65b0\u65b9\u5411"}}
{"id": "2508.06482", "pdf": "https://arxiv.org/pdf/2508.06482", "abs": "https://arxiv.org/abs/2508.06482", "authors": ["Yilun Hua", "Evan Wang", "Yoav Artzi"], "title": "Post-training for Efficient Communication via Convention Formation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to COLM 2025", "summary": "Humans communicate with increasing efficiency in multi-turn interactions, by\nadapting their language and forming ad-hoc conventions. In contrast, prior work\nshows that LLMs do not naturally show this behavior. We develop a post-training\nprocess to develop this ability through targeted fine-tuning on heuristically\nidentified demonstrations of convention formation. We evaluate with two new\nbenchmarks focused on this capability. First, we design a focused,\ncognitively-motivated interaction benchmark that consistently elicits strong\nconvention formation trends in humans. Second, we create a new\ndocument-grounded reference completion task that reflects in-the-wild\nconvention formation behavior. Our studies show significantly improved\nconvention formation abilities in post-trained LLMs across the two evaluation\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u9488\u5bf9\u6027\u5fae\u8c03\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u5f62\u6210\u4e34\u65f6\u7ea6\u5b9a\u7684\u80fd\u529b\uff0c\u5e76\u521b\u5efa\u4e24\u4e2a\u65b0\u57fa\u51c6\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "\u73b0\u6709LLMs\u7f3a\u4e4f\u4eba\u7c7b\u591a\u8f6e\u4ea4\u4e92\u4e2d\u81ea\u7136\u5f62\u6210\u4e34\u65f6\u7ea6\u5b9a\u7684\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u4ea4\u6d41\u6548\u7387\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u540e\u8bad\u7ec3\u65b9\u6cd5\u7a81\u7834\u8fd9\u4e00\u5c40\u9650", "method": "\u4f7f\u7528\u542f\u53d1\u5f0f\u8bc6\u522b\u7684\u7ea6\u5b9a\u5f62\u6210\u6f14\u793a\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u5f00\u53d1\u5305\u542b\u8ba4\u77e5\u4ea4\u4e92\u4efb\u52a1\u548c\u6587\u6863\u53c2\u8003\u4efb\u52a1\u7684\u8bc4\u4f30\u4f53\u7cfb", "result": "\u540e\u8bad\u7ec3\u6a21\u578b\u5728\u4e24\u4e2a\u65b0\u57fa\u51c6\uff08\u8ba4\u77e5\u4ea4\u4e92\u4efb\u52a1\u548c\u6587\u6863\u53c2\u8003\u4efb\u52a1\uff09\u4e2d\u663e\u793a\u51fa\u663e\u8457\u6539\u8fdb\u7684\u7ea6\u5b9a\u5f62\u6210\u80fd\u529b", "conclusion": "\u9a8c\u8bc1\u4e86\u76ee\u6807\u5bfc\u5411\u7684\u5fae\u8c03\u53ef\u6709\u6548\u63d0\u5347LLMs\u7684\u7ea6\u5b9a\u5f62\u6210\u80fd\u529b\uff0c\u4e3a\u6539\u8fdb\u4eba\u673a\u4ea4\u4e92\u6548\u7387\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2508.04748", "pdf": "https://arxiv.org/pdf/2508.04748", "abs": "https://arxiv.org/abs/2508.04748", "authors": ["Xuan Lin", "Long Chen", "Yile Wang"], "title": "AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "9 pages", "summary": "Large Language Models (LLMs) have shown promise in assisting molecular\nproperty prediction tasks but often rely on human-crafted prompts and\nchain-of-thought templates. While recent advanced large reasoning models like\nDeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process,\ntheir reasoning can be verbose and lack relevance. We introduce AttriLens-Mol,\nan attribute-guided reinforcement learning framework for molecular property\nprediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1)\na format reward encouraging attribute-based structured output, (2) a count\nreward to avoid enumerating irrelevant attributes, and (3) a rationality reward\nusing advanced LLMs and RDKit to verify the relatedness of the generated\nattributes. This approach implicitly elicits the model's inherent knowledge of\nrelevant molecular attributes during reasoning, enables making predictions for\nthe molecular property more effectively. Experiments on both in-distribution\nand out-of-distribution datasets show that, training both 7B-size\nR1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our\nproposed AttriLens-Mol method significantly boosts the performance, getting\ncomparable or better results than supervised fine-tuning models\n(Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o,\nDeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the\ntarget property, when used as features for an interpretable decision tree\nmodel, yield superior performance compared to attributes generated by prompting\nLLMs. This shows that AttriLens-Mol effectively elicits more relevant and\npredictive molecular attributes, leading to enhanced interpretability and\nperformance for property prediction. We release the code in\nhttps://github.com/szu-tera/AttriLens-Mol.", "AI": {"tldr": "\u63d0\u51faAttriLens-Mol\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u683c\u5f0f/\u8ba1\u6570/\u5408\u7406\u6027\u4e09\u91cd\u5956\u52b1\u673a\u5236\u5f15\u5bfcLLMs\u751f\u6210\u7ed3\u6784\u5316\u5206\u5b50\u5c5e\u6027\uff0c\u663e\u8457\u63d0\u5347\u9884\u6d4b\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709LLMs\u4f9d\u8d56\u4eba\u5de5\u63d0\u793a\u4e14\u751f\u6210\u5c5e\u6027\u5197\u957f\u4e0d\u76f8\u5173\uff0c\u9700\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5f15\u5bfc\u6a21\u578b\u9690\u5f0f\u6316\u6398\u5206\u5b50\u5c5e\u6027\u77e5\u8bc6\u3002", "method": "\u8bbe\u8ba1\u683c\u5f0f\u5956\u52b1\uff08\u7ed3\u6784\u5316\u8f93\u51fa\uff09+\u8ba1\u6570\u5956\u52b1\uff08\u8fc7\u6ee4\u5197\u4f59\uff09+\u5408\u7406\u6027\u5956\u52b1\uff08LLMs+RDKit\u9a8c\u8bc1\u5c5e\u6027\u76f8\u5173\u6027\uff09\u7684\u4e09\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u673a\u5236\u3002", "result": "\u57284,000\u6837\u672c\u8bad\u7ec3\u540e\uff0c7B\u6a21\u578b\u6027\u80fd\u8d85\u8d8aSFT\u6a21\u578b\u548cGPT-4\u7b49\uff0c\u63d0\u53d6\u5c5e\u6027\u4f7f\u51b3\u7b56\u6811\u6a21\u578b\u8868\u73b0\u4f18\u4e8eLLMs\u76f4\u63a5\u751f\u6210\u3002", "conclusion": "AttriLens-Mol\u6709\u6548\u6fc0\u53d1\u5206\u5b50\u5c5e\u6027\u76f8\u5173\u6027\uff0c\u4e3a\u5206\u5b50\u9884\u6d4b\u63d0\u4f9b\u9ad8\u89e3\u91ca\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.05664", "pdf": "https://arxiv.org/pdf/2508.05664", "abs": "https://arxiv.org/abs/2508.05664", "authors": ["Hei Yu Chan", "Kuok Tou Ho", "Chenglong Ma", "Yujing Si", "Hok Lai Lin", "Sa Lei Lam"], "title": "Enhancing Retrieval-Augmented Generation for Electric Power Industry Customer Support", "categories": ["cs.IR", "cs.AI", "cs.CL", "I.2.m"], "comment": "6 pages", "summary": "Many AI customer service systems use standard NLP pipelines or finetuned\nlanguage models, which often fall short on ambiguous, multi-intent, or\ndetail-specific queries. This case study evaluates recent techniques: query\nrewriting, RAG Fusion, keyword augmentation, intent recognition, and context\nreranking, for building a robust customer support system in the electric power\ndomain. We compare vector-store and graph-based RAG frameworks, ultimately\nselecting the graph-based RAG for its superior performance in handling complex\nqueries. We find that query rewriting improves retrieval for queries using\nnon-standard terminology or requiring precise detail. RAG Fusion boosts\nperformance on vague or multifaceted queries by merging multiple retrievals.\nReranking reduces hallucinations by filtering irrelevant contexts. Intent\nrecognition supports the decomposition of complex questions into more targeted\nsub-queries, increasing both relevance and efficiency. In contrast, keyword\naugmentation negatively impacts results due to biased keyword selection. Our\nfinal system combines intent recognition, RAG Fusion, and reranking to handle\ndisambiguation and multi-source queries. Evaluated on both a GPT-4-generated\ndataset and a real-world electricity provider FAQ dataset, it achieves 97.9%\nand 89.6% accuracy respectively, substantially outperforming baseline RAG\nmodels.", "AI": {"tldr": "\u901a\u8fc7\u6574\u5408\u610f\u56fe\u8bc6\u522b\u3001RAG Fusion\u548c\u91cd\u6392\u6280\u672f\u6784\u5efa\u7684\u7535\u529b\u5ba2\u670d\u7cfb\u7edf\uff0c\u5728\u751f\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9eFAQ\u6570\u636e\u96c6\u5206\u522b\u8fbe\u523097.9%\u548c89.6%\u51c6\u786e\u7387\uff0c\u663e\u8457\u8d85\u8d8a\u4f20\u7edfRAG\u6a21\u578b", "motivation": "\u89e3\u51b3\u73b0\u6709AI\u5ba2\u670d\u7cfb\u7edf\u5728\u5904\u7406\u6a21\u7cca\u67e5\u8be2\u3001\u591a\u610f\u56fe\u95ee\u9898\u548c\u7ec6\u8282\u7279\u5f02\u6027\u9700\u6c42\u65f6\u7684\u4e0d\u8db3\uff0c\u63a2\u7d22\u589e\u5f3a\u7535\u529b\u9886\u57df\u5ba2\u6237\u652f\u6301\u7cfb\u7edf\u9c81\u68d2\u6027\u7684\u6280\u672f\u65b9\u6848", "method": "\u7cfb\u7edf\u8bc4\u4f30\u67e5\u8be2\u91cd\u5199\u3001RAG Fusion\u3001\u5173\u952e\u8bcd\u589e\u5f3a\u3001\u610f\u56fe\u8bc6\u522b\u548c\u4e0a\u4e0b\u6587\u91cd\u6392\u6280\u672f\uff0c\u5bf9\u6bd4\u5411\u91cf\u5b58\u50a8\u4e0e\u56fe\u7ed3\u6784RAG\u6846\u67b6\uff0c\u6700\u7ec8\u91c7\u7528\u56fe\u7ed3\u6784RAG\u6846\u67b6", "result": "\u5728GPT-4\u751f\u6210\u6570\u636e\u96c6\u548c\u771f\u5b9e\u7535\u529b\u516c\u53f8FAQ\u6570\u636e\u96c6\u5206\u522b\u8fbe\u523097.9%\u548c89.6%\u51c6\u786e\u7387\uff0c\u8f83\u57fa\u7ebfRAG\u6a21\u578b\u6709\u663e\u8457\u63d0\u5347", "conclusion": "\u56fe\u7ed3\u6784RAG\u6846\u67b6\u914d\u5408\u67e5\u8be2\u91cd\u5199\u3001\u591a\u68c0\u7d22\u878d\u5408\u548c\u4e0a\u4e0b\u6587\u91cd\u6392\u80fd\u6709\u6548\u5904\u7406\u590d\u6742\u67e5\u8be2\uff0c\u610f\u56fe\u8bc6\u522b\u652f\u6301\u95ee\u9898\u5206\u89e3\uff0c\u800c\u5173\u952e\u8bcd\u589e\u5f3a\u56e0\u504f\u7f6e\u9009\u62e9\u4ea7\u751f\u8d1f\u9762\u6548\u679c"}}
{"id": "2508.05668", "pdf": "https://arxiv.org/pdf/2508.05668", "abs": "https://arxiv.org/abs/2508.05668", "authors": ["Yunjia Xi", "Jianghao Lin", "Yongzhao Xiao", "Zheli Zhou", "Rong Shan", "Te Gao", "Jiachen Zhu", "Weiwen Liu", "Yong Yu", "Weinan Zhang"], "title": "A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "The advent of Large Language Models (LLMs) has significantly revolutionized\nweb search. The emergence of LLM-based Search Agents marks a pivotal shift\ntowards deeper, dynamic, autonomous information seeking. These agents can\ncomprehend user intentions and environmental context and execute multi-turn\nretrieval with dynamic planning, extending search capabilities far beyond the\nweb. Leading examples like OpenAI's Deep Research highlight their potential for\ndeep information mining and real-world applications. This survey provides the\nfirst systematic analysis of search agents. We comprehensively analyze and\ncategorize existing works from the perspectives of architecture, optimization,\napplication, and evaluation, ultimately identifying critical open challenges\nand outlining promising future research directions in this rapidly evolving\nfield. Our repository is available on\nhttps://github.com/YunjiaXi/Awesome-Search-Agent-Papers.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u5206\u6790\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9a71\u52a8\u7684\u641c\u7d22\u4ee3\u7406\u6280\u672f\uff0c\u63a2\u8ba8\u5176\u67b6\u6784\u3001\u4f18\u5316\u3001\u5e94\u7528\u53ca\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u6307\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "LLM\u6280\u672f\u7684\u7a81\u7834\u4f7f\u641c\u7d22\u4ee3\u7406\u5177\u5907\u52a8\u6001\u89c4\u5212\u3001\u591a\u8f6e\u68c0\u7d22\u80fd\u529b\uff0c\u63a8\u52a8\u641c\u7d22\u4ece\u8868\u5c42\u4fe1\u606f\u83b7\u53d6\u8f6c\u5411\u6df1\u5ea6\u81ea\u4e3b\u63a2\u7d22\uff08\u5982OpenAI\u7684Deep Research\u6848\u4f8b\uff09\u3002", "method": "\u901a\u8fc7\u67b6\u6784\u5206\u6790\u3001\u6027\u80fd\u4f18\u5316\u3001\u5e94\u7528\u573a\u666f\u5206\u7c7b\u548c\u8bc4\u4f30\u4f53\u7cfb\u6784\u5efa\u56db\u4e2a\u7ef4\u5ea6\uff0c\u5bf9\u73b0\u6709\u641c\u7d22\u4ee3\u7406\u7814\u7a76\u8fdb\u884c\u7cfb\u7edf\u6027\u5f52\u7c7b\u3002", "result": "\u5efa\u7acb\u9996\u4e2a\u641c\u7d22\u4ee3\u7406\u7814\u7a76\u6846\u67b6\uff0c\u5f00\u6e90\u8bba\u6587\u5e93\uff08GitHub\u9879\u76ee\uff09\uff0c\u63ed\u793a\u6280\u672f\u74f6\u9888\u5e76\u6307\u660e\u52a8\u6001\u73af\u5883\u9002\u5e94\u3001\u53ef\u4fe1\u641c\u7d22\u7b49\u5173\u952e\u53d1\u5c55\u65b9\u5411\u3002", "conclusion": "LLM\u641c\u7d22\u4ee3\u7406\u5c06\u91cd\u5851\u4fe1\u606f\u83b7\u53d6\u8303\u5f0f\uff0c\u672a\u6765\u9700\u5728\u4ea4\u4e92\u6df1\u5ea6\u3001\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u9886\u57df\u9002\u5e94\u6027\u7b49\u65b9\u5411\u6301\u7eed\u7a81\u7834\u3002"}}
{"id": "2508.05669", "pdf": "https://arxiv.org/pdf/2508.05669", "abs": "https://arxiv.org/abs/2508.05669", "authors": ["Jin Khye Tan", "En Jun Choong", "Ethan Jeremiah Chitty", "Yan Pheng Choo", "John Hsin Yang Wong", "Chern Eu Cheah"], "title": "Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.CV", "cs.LG", "I.2.7; I.7.2; J.1"], "comment": "28 pages, 14 figures, 5 tables. Evaluation code (LLM-as-a-judge and\n  Markdown TEDS) is available at https://github.com/jinkhye/MyFinMarkdown. The\n  development dataset and evaluation benchmark are available on Hugging Face at\n  https://huggingface.co/datasets/jinkhye/MyFinMarkdown-sample and\n  https://huggingface.co/datasets/jinkhye/MyFinMarkdown-bench respectively", "summary": "Accurately extracting and representing the structure of tabular data from\nfinancial documents remains a critical challenge in document understanding,\nparticularly for regulatory and analytical use cases. This study addresses the\ncomplexity of converting financial tables from Malaysian audited financial\nreports into Markdown format, a task complicated by rotated layouts,\nmulti-level headers, and implicit structural cues. We propose a fine-tuned\nvision-language model (VLM), based on Qwen2.5-VL-7B, optimized for\nhigh-fidelity Markdown generation from document images. Our approach includes a\ncurated dataset of 2,152 image-text pairs with augmentations and a supervised\nfine-tuning strategy using LoRA. To assess performance, we evaluated our model\non 100 out-of-sample tables using a dual framework: a criteria-based\nLLM-as-a-judge for fine-grained accuracy and our novel Markdown\nTree-Edit-Distance-based Similarity (TEDS) metric for holistic structural\nfidelity. Our model achieves a 92.20% overall accuracy on the criteria-based\nassessment and a 96.53% Markdown TEDS score. This performance significantly\nsurpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized\nreasoning-enabled models. Compared to these self-hosted alternatives, it also\nsignificantly reduces inference time. Furthermore, its accuracy exceeds that of\nwidely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash.\nThese results demonstrate that domain-specific fine-tuning provides an\neffective and efficient method to bridge the gap between unstructured financial\ndocuments and downstream automation, rivalling much larger and more general\nmodels without their computational overhead.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eQwen2.5-VL-7B\u5fae\u8c03\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u9a6c\u6765\u897f\u4e9a\u8d22\u52a1\u8868\u683c\u5230Markdown\u7684\u9ad8\u7cbe\u5ea6\u8f6c\u6362", "motivation": "\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u65cb\u8f6c\u5e03\u5c40\u3001\u591a\u7ea7\u6807\u9898\u548c\u9690\u5f0f\u7ed3\u6784\u8868\u683c\u8f6c\u6362\u4e2d\u7684\u51c6\u786e\u6027\u4e0d\u8db3\u95ee\u9898", "method": "\u4f7f\u75282,152\u4e2a\u589e\u5f3a\u56fe\u50cf\u6587\u672c\u5bf9\u6570\u636e\u96c6\uff0c\u91c7\u7528LoRA\u76d1\u7763\u5fae\u8c03\u7b56\u7565\u4f18\u5316VLM\u6a21\u578b", "result": "\u6807\u51c6\u8bc4\u4f30\u51c6\u786e\u738792.20%\uff0cMarkdown TEDS\u5f97\u520696.53%\uff0c\u63a8\u7406\u6548\u7387\u663e\u8457\u4f18\u4e8eGPT-4o\u7b49\u5927\u578b\u6a21\u578b", "conclusion": "\u9886\u57df\u4e13\u7528\u5fae\u8c03\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u6548\u7387\u63d0\u5347\uff0c\u4e3a\u8d22\u52a1\u6587\u6863\u81ea\u52a8\u5316\u5904\u7406\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.05671", "pdf": "https://arxiv.org/pdf/2508.05671", "abs": "https://arxiv.org/abs/2508.05671", "authors": ["Ko-Wei Chuang", "Hen-Hsen Huang", "Tsai-Yen Li"], "title": "DINA: A Dual Defense Framework Against Internal Noise and External Attacks in Natural Language Processing", "categories": ["cs.CR", "cs.CL"], "comment": "7 pages", "summary": "As large language models (LLMs) and generative AI become increasingly\nintegrated into customer service and moderation applications, adversarial\nthreats emerge from both external manipulations and internal label corruption.\nIn this work, we identify and systematically address these dual adversarial\nthreats by introducing DINA (Dual Defense Against Internal Noise and\nAdversarial Attacks), a novel unified framework tailored specifically for NLP.\nOur approach adapts advanced noisy-label learning methods from computer vision\nand integrates them with adversarial training to simultaneously mitigate\ninternal label sabotage and external adversarial perturbations. Extensive\nexperiments conducted on a real-world dataset from an online gaming service\ndemonstrate that DINA significantly improves model robustness and accuracy\ncompared to baseline models. Our findings not only highlight the critical\nnecessity of dual-threat defenses but also offer practical strategies for\nsafeguarding NLP systems in realistic adversarial scenarios, underscoring\nbroader implications for fair and responsible AI deployment.", "AI": {"tldr": "\u63d0\u51faDINA\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\u4e0e\u5bf9\u6297\u8bad\u7ec3\uff0c\u6709\u6548\u9632\u5fa1NLP\u7cfb\u7edf\u4e2d\u7684\u6807\u7b7e\u7834\u574f\u548c\u5bf9\u6297\u653b\u51fb", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e2d\u9762\u4e34\u5185\u90e8\u6807\u7b7e\u6c61\u67d3\u548c\u5916\u90e8\u5bf9\u6297\u653b\u51fb\u7684\u53cc\u91cd\u5a01\u80c1\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u7f3a\u4e4f\u7edf\u4e00\u5e94\u5bf9\u673a\u5236", "method": "\u5c06CV\u9886\u57df\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\u65b9\u6cd5\u4e0e\u5bf9\u6297\u8bad\u7ec3\u7ed3\u5408\uff0c\u6784\u5efa\u7edf\u4e00\u9632\u5fa1\u6846\u67b6\uff0c\u5e76\u5728\u5728\u7ebf\u6e38\u620f\u670d\u52a1\u771f\u5b9e\u6570\u636e\u96c6\u9a8c\u8bc1", "result": "\u5b9e\u9a8c\u663e\u793aDINA\u663e\u8457\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u51c6\u786e\u7387\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b", "conclusion": "\u53cc\u5a01\u80c1\u9632\u5fa1\u7b56\u7565\u5bf9\u4fdd\u969cNLP\u7cfb\u7edf\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u8d1f\u8d23\u4efbAI\u90e8\u7f72\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2508.05694", "pdf": "https://arxiv.org/pdf/2508.05694", "abs": "https://arxiv.org/abs/2508.05694", "authors": ["Kaichuan Kong", "Dongjie Liu", "Xiaobo Jin", "Guanggang Geng", "Zhiying Li", "Jian Weng"], "title": "DMFI: Dual-Modality Fine-Tuning and Inference Framework for LLM-Based Insider Threat Detection", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "Submitted to the 2025 IEEE International Conference on Data Mining\n  (ICDM)", "summary": "Insider threat detection (ITD) poses a persistent and high-impact challenge\nin cybersecurity due to the subtle, long-term, and context-dependent nature of\nmalicious insider behaviors. Traditional models often struggle to capture\nsemantic intent and complex behavior dynamics, while existing LLM-based\nsolutions face limitations in prompt adaptability and modality coverage. To\nbridge this gap, we propose DMFI, a dual-modality framework that integrates\nsemantic inference with behavior-aware fine-tuning. DMFI converts raw logs into\ntwo structured views: (1) a semantic view that processes content-rich artifacts\n(e.g., emails, https) using instruction-formatted prompts; and (2) a behavioral\nabstraction, constructed via a 4W-guided (When-Where-What-Which) transformation\nto encode contextual action sequences. Two LoRA-enhanced LLMs are fine-tuned\nindependently, and their outputs are fused via a lightweight MLP-based decision\nmodule. We further introduce DMFI-B, a discriminative adaptation strategy that\nseparates normal and abnormal behavior representations, improving robustness\nunder severe class imbalance. Experiments on CERT r4.2 and r5.2 datasets\ndemonstrate that DMFI outperforms state-of-the-art methods in detection\naccuracy. Our approach combines the semantic reasoning power of LLMs with\nstructured behavior modeling, offering a scalable and effective solution for\nreal-world insider threat detection. Our work demonstrates the effectiveness of\ncombining LLM reasoning with structured behavioral modeling, offering a\nscalable and deployable solution for modern insider threat detection.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u6a21\u6001\u6846\u67b6DMFI\uff0c\u7ed3\u5408\u8bed\u4e49\u63a8\u7406\u4e0e\u884c\u4e3a\u611f\u77e5\u5fae\u8c03\uff0c\u6709\u6548\u63d0\u5347\u5185\u90e8\u5a01\u80c1\u68c0\u6d4b\u7cbe\u5ea6", "motivation": "\u4f20\u7edf\u6a21\u578b\u96be\u4ee5\u6355\u6349\u8bed\u4e49\u610f\u56fe\u548c\u590d\u6742\u884c\u4e3a\u52a8\u6001\uff0c\u73b0\u6709LLM\u65b9\u6848\u5728\u63d0\u793a\u9002\u5e94\u6027\u548c\u6a21\u6001\u8986\u76d6\u4e0a\u5b58\u5728\u5c40\u9650", "method": "\u901a\u8fc74W\u5f15\u5bfc\u7684\u65e5\u5fd7\u7ed3\u6784\u5316\u8f6c\u6362\u751f\u6210\u8bed\u4e49\u89c6\u56fe\u4e0e\u884c\u4e3a\u62bd\u8c61\uff0c\u4f7f\u7528LoRA\u589e\u5f3a\u7684LLM\u72ec\u7acb\u5fae\u8c03\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7MLP\u51b3\u7b56\u6a21\u5757\u878d\u5408\u8f93\u51fa\uff0c\u5f15\u5165DMFI-B\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861", "result": "\u5728CERT r4.2/r5.2\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027", "conclusion": "\u878d\u5408LLM\u8bed\u4e49\u63a8\u7406\u4e0e\u7ed3\u6784\u5316\u884c\u4e3a\u5efa\u6a21\uff0c\u4e3a\u73b0\u5b9e\u573a\u666f\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u68c0\u6d4b\u65b9\u6848"}}
{"id": "2508.05731", "pdf": "https://arxiv.org/pdf/2508.05731", "abs": "https://arxiv.org/abs/2508.05731", "authors": ["Yuhang Liu", "Zeyu Liu", "Shuanghe Zhu", "Pengxiang Li", "Congkai Xie", "Jiasheng Wang", "Xueyu Hu", "Xiaotian Han", "Jianbo Yuan", "Xinyao Wang", "Shengyu Zhang", "Hongxia Yang", "Fei Wu"], "title": "InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization", "categories": ["cs.AI", "cs.CL"], "comment": "11 pages, 3 figures", "summary": "The emergence of Multimodal Large Language Models (MLLMs) has propelled the\ndevelopment of autonomous agents that operate on Graphical User Interfaces\n(GUIs) using pure visual input. A fundamental challenge is robustly grounding\nnatural language instructions. This requires a precise spatial alignment, which\naccurately locates the coordinates of each element, and, more critically, a\ncorrect semantic alignment, which matches the instructions to the functionally\nappropriate UI element. Although Reinforcement Learning with Verifiable Rewards\n(RLVR) has proven to be effective at improving spatial alignment for these\nMLLMs, we find that inefficient exploration bottlenecks semantic alignment,\nwhich prevent models from learning difficult semantic associations. To address\nthis exploration problem, we present Adaptive Exploration Policy Optimization\n(AEPO), a new policy optimization framework. AEPO employs a multi-answer\ngeneration strategy to enforce broader exploration, which is then guided by a\ntheoretically grounded Adaptive Exploration Reward (AER) function derived from\nfirst principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B\nand InfiGUI-G1-7B, establish new state-of-the-art results across multiple\nchallenging GUI grounding benchmarks, achieving significant relative\nimprovements of up to 9.0% against the naive RLVR baseline on benchmarks\ndesigned to test generalization and semantic understanding. Resources are\navailable at https://github.com/InfiXAI/InfiGUI-G1.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u63a2\u7d22\u7b56\u7565\u4f18\u5316\u6846\u67b6AEPO\uff0c\u901a\u8fc7\u591a\u7b54\u6848\u751f\u6210\u548c\u7406\u8bba\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u5956\u52b1\u51fd\u6570\uff0c\u663e\u8457\u63d0\u5347GUI\u8bed\u4e49\u5bf9\u9f50\u80fd\u529b", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u5728\u7a7a\u95f4\u5bf9\u9f50\u6709\u6548\u4f46\u8bed\u4e49\u5bf9\u9f50\u63a2\u7d22\u6548\u7387\u4f4e\uff0c\u96be\u4ee5\u5b66\u4e60\u590d\u6742\u8bed\u4e49\u5173\u8054", "method": "AEPO\u6846\u67b6\u6574\u5408\u591a\u7b54\u6848\u751f\u6210\u7b56\u7565\uff08\u6269\u5927\u63a2\u7d22\u8303\u56f4\uff09\u548c\u57fa\u4e8e\u6548\u7387\u516c\u5f0feta=U/C\u7684\u81ea\u9002\u5e94\u63a2\u7d22\u5956\u52b1\u51fd\u6570AER", "result": "InfiGUI-G1\u6a21\u578b\u5728\u591a\u4e2aGUI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\uff0c\u76f8\u6bd4\u57fa\u7ebfRLVR\u6700\u9ad8\u63d0\u53479.0%", "conclusion": "AEPO\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u4e49\u5bf9\u9f50\u7684\u63a2\u7d22\u74f6\u9888\uff0c\u7406\u8bba\u9a71\u52a8\u7684\u5956\u52b1\u673a\u5236\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728GUI\u573a\u666f\u7684\u6cdb\u5316\u548c\u8bed\u4e49\u7406\u89e3\u80fd\u529b"}}
{"id": "2508.05798", "pdf": "https://arxiv.org/pdf/2508.05798", "abs": "https://arxiv.org/abs/2508.05798", "authors": ["Yuri Gurevich"], "title": "Basic interactive algorithms: Preview", "categories": ["cs.LO", "cs.CL", "math.LO", "quant-ph"], "comment": null, "summary": "This dialog paper offers a preview and provides a foretaste of an upcoming\nwork on the axiomatization of basic interactive algorithms.\n  The modern notion of algorithm was elucidated in the 1930s--1950s. It was\naxiomatized a quarter of a century ago as the notion of ``sequential\nalgorithm'' or ``classical algorithm''; we prefer to call it ``basic algorithm\"\nnow. The axiomatization was used to show that for every basic algorithm there\nis a behaviorally equivalent abstract state machine. It was also used to prove\nthe Church-Turing thesis as it has been understood by the logicians.\n  Starting from the 1960s, the notion of algorithm has expanded --\nprobabilistic algorithms, quantum algorithms, etc. -- prompting introduction of\na much more ambitious version of the Church-Turing thesis commonly known as the\n``physical thesis.'' We emphasize the difference between the two versions of\nthe Church-Turing thesis and illustrate how nondeterministic and probabilistic\nalgorithms can be viewed as basic algorithms with appropriate oracles. The same\nview applies to quantum circuit algorithms and many other classes of\nalgorithms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9884\u544a\u4e86\u5bf9\u57fa\u672c\u4ea4\u4e92\u5f0f\u7b97\u6cd5\u7684\u516c\u7406\u5316\u7814\u7a76\uff0c\u63a2\u8ba8\u7b97\u6cd5\u6982\u5ff5\u7684\u6269\u5c55\u53caChurch-Turing\u8bba\u9898\u7684\u4e0d\u540c\u7248\u672c\u3002", "motivation": "\u968f\u7740\u91cf\u5b50/\u6982\u7387\u7b49\u65b0\u578b\u7b97\u6cd5\u7684\u51fa\u73b0\uff0c\u9700\u91cd\u65b0\u5ba1\u89c6Church-Turing\u8bba\u9898\uff08\u5c24\u5176\u662f\u7269\u7406\u8bba\u9898\u4e0e\u539f\u59cb\u7248\u672c\u7684\u672c\u8d28\u5dee\u5f02\uff09", "method": "\u901a\u8fc7\u5c06\u975e\u786e\u5b9a\u6027/\u6982\u7387\u7b97\u6cd5\u89c6\u4e3a\u5e26\u6709oracle\u7684\u57fa\u672c\u7b97\u6cd5\uff0c\u6784\u5efa\u7edf\u4e00\u7406\u8bba\u6846\u67b6", "result": "\u8bc1\u660e\u91cf\u5b50\u7535\u8def\u7b49\u6269\u5c55\u7b97\u6cd5\u53ef\u7eb3\u5165\u57fa\u672c\u7b97\u6cd5\u4f53\u7cfb\uff0c\u660e\u786e\u4e24\u79cdChurch-Turing\u8bba\u9898\u7248\u672c\u7684\u54f2\u5b66\u533a\u522b", "conclusion": "\u7b97\u6cd5\u516c\u7406\u5316\u7814\u7a76\u4e3a\u7406\u89e3\u8ba1\u7b97\u672c\u8d28\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u7269\u7406\u8bba\u9898\u9700\u8981\u57fa\u4e8e\u6269\u5c55\u7684\u7b97\u6cd5\u6982\u5ff5\u91cd\u65b0\u8be0\u91ca"}}
{"id": "2508.05835", "pdf": "https://arxiv.org/pdf/2508.05835", "abs": "https://arxiv.org/abs/2508.05835", "authors": ["Edresson Casanova", "Paarth Neekhara", "Ryan Langman", "Shehzeen Hussain", "Subhankar Ghosh", "Xuesong Yang", "Ante Juki\u0107", "Jason Li", "Boris Ginsburg"], "title": "NanoCodec: Towards High-Quality Ultra Fast Speech LLM Inference", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "Large Language Models (LLMs) have significantly advanced audio processing by\nleveraging audio codecs to discretize audio into tokens, enabling the\napplication of language modeling techniques to speech data. However, existing\naudio codecs often operate at high frame rates, leading to slow training and\ninference, particularly for autoregressive models. To address this, there is\ngrowing interest in low frame-rate audio codecs, which reduce the number of\nautoregressive steps required to generate one second of audio. In this paper,\nwe conduct ablation studies to examine the impact of frame rate, bitrate, and\ncausality on codec reconstruction quality. Based on our findings, we introduce\nNanoCodec, a state-of-the-art audio codec that achieves high-quality\ncompression at just 12.5 frames per second (FPS). NanoCodec outperforms related\nworks across various bitrate ranges, establishing a new benchmark for\nlow-latency and efficient Speech LLM training and inference.", "AI": {"tldr": "NanoCodec\u63d0\u51fa12.5FPS\u4f4e\u5e27\u7387\u97f3\u9891\u7f16\u89e3\u7801\u5668\uff0c\u663e\u8457\u63d0\u5347\u8bed\u97f3LLM\u6548\u7387", "motivation": "\u73b0\u6709\u97f3\u9891\u7f16\u89e3\u7801\u5668\u9ad8\u5e27\u7387\u5bfc\u81f4\u81ea\u56de\u5f52\u6a21\u578b\u8bad\u7ec3/\u63a8\u7406\u901f\u5ea6\u6162\uff0c\u9700\u964d\u4f4e\u5e27\u7387\u4ee5\u63d0\u5347\u6548\u7387", "method": "\u901a\u8fc7\u6d88\u878d\u5b9e\u9a8c\u5206\u6790\u5e27\u7387\u3001\u6bd4\u7279\u7387\u548c\u56e0\u679c\u6027\u5bf9\u91cd\u5efa\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u8bbe\u8ba1\u8d85\u4f4e\u5e27\u7387\u7f16\u89e3\u7801\u67b6\u6784", "result": "\u5728\u591a\u79cd\u6bd4\u7279\u7387\u8303\u56f4\u8d85\u8d8a\u73b0\u6709\u65b9\u6848\uff0c\u5efa\u7acb\u4f4e\u5ef6\u8fdf\u8bed\u97f3LLM\u65b0\u57fa\u51c6", "conclusion": "NanoCodec\u9a8c\u8bc1\u4e86\u4f4e\u5e27\u7387\u7f16\u89e3\u7801\u5668\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u9ad8\u6548\u8bed\u97f3\u5904\u7406\u63d0\u4f9b\u6280\u672f\u7a81\u7834"}}
{"id": "2508.05913", "pdf": "https://arxiv.org/pdf/2508.05913", "abs": "https://arxiv.org/abs/2508.05913", "authors": ["Stefan Pasch", "Min Chul Cha"], "title": "Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction", "categories": ["cs.HC", "cs.AI", "cs.CL"], "comment": null, "summary": "As AI systems become increasingly embedded in organizational workflows and\nconsumer applications, ethical principles such as fairness, transparency, and\nrobustness have been widely endorsed in policy and industry guidelines.\nHowever, there is still scarce empirical evidence on whether these principles\nare recognized, valued, or impactful from the perspective of users. This study\ninvestigates the link between ethical AI and user satisfaction by analyzing\nover 100,000 user reviews of AI products from G2. Using transformer-based\nlanguage models, we measure sentiment across seven ethical dimensions defined\nby the EU Ethics Guidelines for Trustworthy AI. Our findings show that all\nseven dimensions are positively associated with user satisfaction. Yet, this\nrelationship varies systematically across user and product types. Technical\nusers and reviewers of AI development platforms more frequently discuss\nsystem-level concerns (e.g., transparency, data governance), while\nnon-technical users and reviewers of end-user applications emphasize\nhuman-centric dimensions (e.g., human agency, societal well-being). Moreover,\nthe association between ethical AI and user satisfaction is significantly\nstronger for non-technical users and end-user applications across all\ndimensions. Our results highlight the importance of ethical AI design from\nusers' perspectives and underscore the need to account for contextual\ndifferences across user roles and product types.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u679010\u4e07+\u7528\u6237\u8bc4\u8bba\uff0c\u53d1\u73b0AI\u4f26\u7406\u4e03\u5927\u7ef4\u5ea6\u5747\u4e0e\u7528\u6237\u6ee1\u610f\u5ea6\u6b63\u76f8\u5173\uff0c\u4f46\u6280\u672f\u7528\u6237\u4e0e\u975e\u6280\u672f\u7528\u6237\u3001\u5f00\u53d1\u5e73\u53f0\u4e0e\u7ec8\u7aef\u5e94\u7528\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u586b\u8865AI\u4f26\u7406\u539f\u5219\uff08\u516c\u5e73\u6027\u3001\u900f\u660e\u5ea6\u7b49\uff09\u4e0e\u7528\u6237\u611f\u77e5\u4e4b\u95f4\u5173\u7cfb\u7684\u5b9e\u8bc1\u7814\u7a76\u7a7a\u767d\uff0c\u9a8c\u8bc1\u6b27\u76df\u53ef\u4fe1AI\u4f26\u7406\u6307\u5357\u4e03\u5927\u7ef4\u5ea6\u5bf9\u7528\u6237\u6ee1\u610f\u5ea6\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eTransformer\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u5bf9G2\u5e73\u53f010\u4e07\u4f59\u6761AI\u4ea7\u54c1\u7528\u6237\u8bc4\u8bba\u8fdb\u884c\u4e03\u7ef4\u5ea6\u60c5\u611f\u5206\u6790\u3002", "result": "\u2460\u6240\u6709\u4f26\u7406\u7ef4\u5ea6\u5747\u4e0e\u6ee1\u610f\u5ea6\u6b63\u76f8\u5173\uff1b\u2461\u6280\u672f\u7528\u6237\u66f4\u5173\u6ce8\u7cfb\u7edf\u7ea7\u7ef4\u5ea6\uff08\u900f\u660e\u5ea6/\u6570\u636e\u6cbb\u7406\uff09\uff0c\u975e\u6280\u672f\u7528\u6237\u4fa7\u91cd\u4eba\u672c\u7ef4\u5ea6\uff08\u4eba\u6743/\u793e\u4f1a\u798f\u5229\uff09\uff1b\u2462\u7ec8\u7aef\u5e94\u7528\u7684\u975e\u6280\u672f\u7528\u6237\u7fa4\u4f53\u4e2d\u4f26\u7406-\u6ee1\u610f\u5ea6\u5173\u8054\u5f3a\u5ea6\u663e\u8457\u66f4\u9ad8\u3002", "conclusion": "\u9700\u4ece\u7528\u6237\u89c6\u89d2\u5206\u5c42\u8bbe\u8ba1AI\u4f26\u7406\uff1a\u6280\u672f\u7cfb\u7edf\u4fa7\u91cd\u6280\u672f\u6cbb\u7406\uff0c\u7ec8\u7aef\u5e94\u7528\u5f3a\u5316\u4eba\u672c\u5173\u6000\uff0c\u5f00\u53d1\u8005\u5e94\u6839\u636e\u7528\u6237\u89d2\u8272\u548c\u4ea7\u54c1\u7c7b\u578b\u8c03\u6574\u4f26\u7406\u5b9e\u73b0\u8def\u5f84\u3002"}}
{"id": "2508.05954", "pdf": "https://arxiv.org/pdf/2508.05954", "abs": "https://arxiv.org/abs/2508.05954", "authors": ["Han Lin", "Jaemin Cho", "Amir Zadeh", "Chuan Li", "Mohit Bansal"], "title": "Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Project Page: https://bifrost-1.github.io", "summary": "There is growing interest in integrating high-fidelity visual synthesis\ncapabilities into large language models (LLMs) without compromising their\nstrong reasoning capabilities. Existing methods that directly train LLMs or\nbridge LLMs and diffusion models usually suffer from costly training since the\nbackbone LLMs have not seen image representations during pretraining. We\npresent Bifrost-1, a unified framework that bridges pretrained multimodal LLMs\n(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent\nvariables, which are natively aligned with the MLLM's CLIP visual encoder.\nThese patch-level image embeddings are integrated into the diffusion model with\na lightweight adaptation of its ControlNet. To retain the original multimodal\nreasoning capabilities of MLLMs, we equip the MLLM with a visual generation\nbranch initialized from the original MLLM parameters when predicting the\npatch-level image embeddings. By seamlessly integrating pretrained MLLMs and\ndiffusion models with patch-level CLIP latents, our framework enables\nhigh-fidelity controllable image generation with significant training\nefficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or\nbetter performance than previous methods in terms of visual fidelity and\nmultimodal understanding, with substantially lower compute during training. We\nalso provide comprehensive ablation studies showing the effectiveness of our\ndesign choices.", "AI": {"tldr": "Bifrost-1\u901a\u8fc7\u6574\u5408\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLMs\uff09\u548c\u6269\u6563\u6a21\u578b\uff0c\u5229\u7528CLIP\u56fe\u50cf\u5d4c\u5165\u4f5c\u4e3a\u6f5c\u5728\u53d8\u91cf\uff0c\u5b9e\u73b0\u9ad8\u6548\u53ef\u63a7\u7684\u9ad8\u4fdd\u771f\u56fe\u50cf\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u7559\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u56e0LLMs\u9884\u8bad\u7ec3\u672a\u6d89\u53ca\u56fe\u50cf\u8868\u793a\u5bfc\u81f4\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff0c\u9700\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u89c6\u89c9\u5408\u6210\u4e0e\u63a8\u7406\u7ed3\u5408\u65b9\u6848\u3002", "method": "1. \u4f7f\u7528patch\u7ea7CLIP\u56fe\u50cf\u5d4c\u5165\u4f5c\u4e3a\u6f5c\u5728\u53d8\u91cf\uff0c\u4e0eMLLMs\u7684CLIP\u89c6\u89c9\u7f16\u7801\u5668\u5bf9\u9f50\uff1b2. \u901a\u8fc7\u8f7b\u91cf\u7ea7ControlNet\u9002\u914d\u6269\u6563\u6a21\u578b\uff1b3. \u521d\u59cb\u5316\u89c6\u89c9\u751f\u6210\u5206\u652f\u4fdd\u7559MLLMs\u539f\u59cb\u80fd\u529b\u3002", "result": "Bifrost-1\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u4e0e\u591a\u6a21\u6001\u7406\u89e3\u4e0a\u5ab2\u7f8e\u6216\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u8bad\u7ec3\u8ba1\u7b97\u91cf\u663e\u8457\u964d\u4f4e\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u8bbe\u8ba1\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u9ad8\u6548\u53ef\u63a7\u56fe\u50cf\u751f\u6210\uff0c\u65e0\u7f1d\u6574\u5408\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5e73\u8861\u751f\u6210\u8d28\u91cf\u4e0e\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2508.06017", "pdf": "https://arxiv.org/pdf/2508.06017", "abs": "https://arxiv.org/abs/2508.06017", "authors": ["Xiangzhe Xu", "Shiwei Feng", "Zian Su", "Chengpeng Wang", "Xiangyu Zhang"], "title": "Position: Intelligent Coding Systems Should Write Programs with Justifications", "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "The first two authors contributed equally to this work", "summary": "Intelligent coding systems are transforming software development by enabling\nusers to specify code behavior in natural language. However, the opaque\ndecision-making of AI-driven coders raises trust and usability concerns,\nparticularly for non-expert users who cannot inspect low-level implementations.\nWe argue that these systems should not only generate code but also produce\nclear, consistent justifications that bridge model reasoning and user\nunderstanding. To this end, we identify two critical justification\nproperties-cognitive alignment and semantic faithfulness-and highlight the\nlimitations of existing methods, including formal verification, static\nanalysis, and post-hoc explainability. We advocate exploring neuro-symbolic\napproaches for justification generation, where symbolic constraints guide model\nbehavior during training and program semantics are enriched through neural\nrepresentations, enabling automated consistency checks at inference time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u6539\u8fdbAI\u7f16\u7a0b\u7cfb\u7edf\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4f7f\u975e\u4e13\u4e1a\u7528\u6237\u80fd\u7406\u89e3\u4ee3\u7801\u751f\u6210\u903b\u8f91", "motivation": "\u5f53\u524dAI\u7f16\u7a0b\u7cfb\u7edf\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\uff0c\u975e\u4e13\u4e1a\u7528\u6237\u7f3a\u4e4f\u9a8c\u8bc1\u80fd\u529b\uff0c\u5bfc\u81f4\u4fe1\u4efb\u5371\u673a\u548c\u4f7f\u7528\u969c\u788d", "method": "\u878d\u5408\u7b26\u53f7\u7ea6\u675f\u6307\u5bfc\u6a21\u578b\u8bad\u7ec3\uff0c\u901a\u8fc7\u795e\u7ecf\u8868\u5f81\u589e\u5f3a\u7a0b\u5e8f\u8bed\u4e49\u7406\u89e3\uff0c\u5b9e\u73b0\u63a8\u7406\u65f6\u7684\u81ea\u52a8\u5316\u4e00\u81f4\u6027\u9a8c\u8bc1", "result": "\u5efa\u7acb\u8ba4\u77e5\u5bf9\u9f50\u548c\u8bed\u4e49\u4fdd\u771f\u7684\u53cc\u91cd\u9a8c\u8bc1\u6846\u67b6\uff0c\u7a81\u7834\u4f20\u7edf\u5f62\u5f0f\u5316\u9a8c\u8bc1\u548c\u9759\u6001\u5206\u6790\u7684\u5c40\u9650\u6027", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u53ef\u6709\u6548\u8fde\u63a5\u6a21\u578b\u63a8\u7406\u4e0e\u7528\u6237\u8ba4\u77e5\uff0c\u4e3a\u6784\u5efa\u53ef\u4fe1AI\u7f16\u7a0b\u7cfb\u7edf\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2508.06059", "pdf": "https://arxiv.org/pdf/2508.06059", "abs": "https://arxiv.org/abs/2508.06059", "authors": ["Haorui He", "Yupeng Li", "Bin Benjamin Zhu", "Dacheng Wen", "Reynold Cheng", "Francis C. M. Lau"], "title": "Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "State-of-the-art fact-checking systems combat misinformation at scale by\nemploying autonomous LLM-based agents to decompose complex claims into smaller\nsub-claims, verify each sub-claim individually, and aggregate the partial\nresults to produce verdicts with justifications (explanatory rationales for the\nverdicts). The security of these systems is crucial, as compromised\nfact-checkers, which tend to be easily underexplored, can amplify\nmisinformation. This work introduces Fact2Fiction, the first poisoning attack\nframework targeting such agentic fact-checking systems. Fact2Fiction mirrors\nthe decomposition strategy and exploits system-generated justifications to\ncraft tailored malicious evidences that compromise sub-claim verification.\nExtensive experiments demonstrate that Fact2Fiction achieves 8.9\\%--21.2\\%\nhigher attack success rates than state-of-the-art attacks across various\npoisoning budgets. Fact2Fiction exposes security weaknesses in current\nfact-checking systems and highlights the need for defensive countermeasures.", "AI": {"tldr": "Fact2Fiction\u9996\u6b21\u9488\u5bf9\u667a\u80fd\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u63d0\u51fa\u6295\u6bd2\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u4f2a\u9020\u8bc1\u636e\u5b9e\u73b021.2%\u653b\u51fb\u6210\u529f\u7387\uff0c\u66b4\u9732\u7cfb\u7edf\u5b89\u5168\u7f3a\u9677\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0c\u53ef\u80fd\u88ab\u653b\u51fb\u8005\u5229\u7528\u6210\u4e3a\u4f20\u64ad\u9519\u8bef\u4fe1\u606f\u7684\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u6a21\u62df\u7cfb\u7edf\u7684\u58f0\u660e\u5206\u89e3\u673a\u5236\uff0c\u5229\u7528\u7cfb\u7edf\u751f\u6210\u7684\u89e3\u91ca\u6587\u672c\u9006\u5411\u6784\u5efa\u6076\u610f\u8bc1\u636e\uff0c\u6c61\u67d3\u5b50\u58f0\u660e\u9a8c\u8bc1\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u79cd\u6295\u6bd2\u9884\u7b97\u573a\u666f\u4e0b\uff0cFact2Fiction\u653b\u51fb\u6210\u529f\u7387\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u53478.9%-21.2%\u3002", "conclusion": "\u73b0\u6709\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u5b58\u5728\u4e25\u91cd\u5b89\u5168\u6f0f\u6d1e\uff0c\u4e9f\u9700\u5f00\u53d1\u9632\u5fa1\u673a\u5236\u5e94\u5bf9\u65b0\u578b\u6295\u6bd2\u653b\u51fb\u3002"}}
{"id": "2508.06065", "pdf": "https://arxiv.org/pdf/2508.06065", "abs": "https://arxiv.org/abs/2508.06065", "authors": ["Daniel Lee", "Nikhil Sharma", "Donghoon Shin", "DaEun Choi", "Harsh Sharma", "Jeonghwan Kim", "Heng Ji"], "title": "ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.CV", "H.5.2; I.2.7"], "comment": null, "summary": "Generative AI has made image creation more accessible, yet aligning outputs\nwith nuanced creative intent remains challenging, particularly for non-experts.\nExisting tools often require users to externalize ideas through prompts or\nreferences, limiting fluid exploration. We introduce ThematicPlane, a system\nthat enables users to navigate and manipulate high-level semantic concepts\n(e.g., mood, style, or narrative tone) within an interactive thematic design\nplane. This interface bridges the gap between tacit creative intent and system\ncontrol. In our exploratory study (N=6), participants engaged in divergent and\nconvergent creative modes, often embracing unexpected results as inspiration or\niteration cues. While they grounded their exploration in familiar themes,\ndiffering expectations of how themes mapped to outputs revealed a need for more\nexplainable controls. Overall, ThematicPlane fosters expressive, iterative\nworkflows and highlights new directions for intuitive, semantics-driven\ninteraction in generative design tools.", "AI": {"tldr": "\u63d0\u51faThematicPlane\u4ea4\u4e92\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bed\u4e49\u6982\u5ff5\u64cd\u63a7\u5e2e\u52a9\u521b\u4f5c\u8005\u8854\u63a5\u521b\u610f\u610f\u56fe\u4e0e\u751f\u6210\u7ed3\u679c\uff0c\u652f\u6301\u53d1\u6563/\u805a\u5408\u5f0f\u521b\u4f5c\u6d41\u7a0b\u5e76\u63ed\u793a\u63a7\u5236\u89e3\u91ca\u6027\u9700\u6c42", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u5de5\u5177\u4f9d\u8d56\u6587\u672c\u63d0\u793a\uff0c\u9650\u5236\u4e86\u975e\u4e13\u4e1a\u4eba\u58eb\u7684\u521b\u610f\u63a2\u7d22\u3002\u9700\u8981\u66f4\u7b26\u5408\u4eba\u7c7b\u76f4\u89c9\u7684\u8bed\u4e49\u5c42\u4ea4\u4e92\u6765\u964d\u4f4e\u521b\u4f5c\u95e8\u69db", "method": "\u5f00\u53d1\u57fa\u4e8e\u4e3b\u9898\u8bbe\u8ba1\u5e73\u9762\u7684\u4ea4\u4e92\u7cfb\u7edf\uff0c\u901a\u8fc76\u4eba\u7528\u6237\u5b9e\u9a8c\u89c2\u5bdf\u53d1\u6563/\u805a\u5408\u521b\u4f5c\u6a21\u5f0f\u4e0b\u7684\u4f7f\u7528\u884c\u4e3a\u4e0e\u53cd\u9988", "result": "\u7528\u6237\u80fd\u5229\u7528\u610f\u5916\u7ed3\u679c\u6fc0\u53d1\u7075\u611f\uff0c\u4f46\u4e3b\u9898\u4e0e\u8f93\u51fa\u7684\u6620\u5c04\u5173\u7cfb\u9700\u66f4\u900f\u660e\u7684\u89e3\u91ca\u673a\u5236\u3002\u7cfb\u7edf\u652f\u6301\u4e86\u66f4\u5177\u8868\u73b0\u529b\u7684\u8fed\u4ee3\u521b\u4f5c\u6d41\u7a0b", "conclusion": "ThematicPlane\u8bc1\u660e\u4e86\u8bed\u4e49\u9a71\u52a8\u4ea4\u4e92\u5728\u751f\u6210\u5f0f\u5de5\u5177\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u521b\u610f\u610f\u56fe\u4e0e\u7cfb\u7edf\u63a7\u5236\u7684\u878d\u5408\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2508.06401", "pdf": "https://arxiv.org/pdf/2508.06401", "abs": "https://arxiv.org/abs/2508.06401", "authors": ["Andrew Brown", "Muhammad Roman", "Barry Devereux"], "title": "A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges", "categories": ["cs.DL", "cs.AI", "cs.CL", "cs.IR"], "comment": "58 pages", "summary": "This systematic review of the research literature on retrieval-augmented\ngeneration (RAG) provides a focused analysis of the most highly cited studies\npublished between 2020 and May 2025. A total of 128 articles met our inclusion\ncriteria. The records were retrieved from ACM Digital Library, IEEE Xplore,\nScopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP).\nRAG couples a neural retriever with a generative language model, grounding\noutput in up-to-date, non-parametric memory while retaining the semantic\ngeneralisation stored in model weights. Guided by the PRISMA 2020 framework, we\n(i) specify explicit inclusion and exclusion criteria based on citation count\nand research questions, (ii) catalogue datasets, architectures, and evaluation\npractices, and (iii) synthesise empirical evidence on the effectiveness and\nlimitations of RAG. To mitigate citation-lag bias, we applied a lower\ncitation-count threshold to papers published in 2025 so that emerging\nbreakthroughs with naturally fewer citations were still captured. This review\nclarifies the current research landscape, highlights methodological gaps, and\ncharts priority directions for future research.", "AI": {"tldr": "\u5bf92020-2025\u5e74128\u7bc7\u9ad8\u5f15RAG\u7814\u7a76\u7684\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u5206\u6790\u67b6\u6784\u3001\u8bc4\u4f30\u65b9\u6cd5\u53ca\u6709\u6548\u6027\uff0c\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411", "motivation": "\u89e3\u51b3\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u9886\u57df\u7f3a\u4e4f\u7cfb\u7edf\u6027\u603b\u7ed3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u8c03\u6574\u5f15\u6587\u9608\u503c\u6355\u83b72025\u5e74\u65b0\u5174\u7a81\u7834\uff0c\u907f\u514d\u5f15\u6587\u6ede\u540e\u504f\u5dee", "method": "\u57fa\u4e8ePRISMA 2020\u6846\u67b6\uff0c\u8bbe\u5b9a\u53cc\u7ef4\u5ea6\u7eb3\u5165\u6807\u51c6\uff08\u5f15\u6587\u91cf+\u7814\u7a76\u95ee\u9898\uff09\uff0c\u6784\u5efa\u6570\u636e\u96c6/\u6a21\u578b\u67b6\u6784/\u8bc4\u4f30\u6307\u6807\u4e09\u7ef4\u5206\u6790\u4f53\u7cfb", "result": "\u63ed\u793a\u5f53\u524dRAG\u7814\u7a76\u5b58\u5728\u8bc4\u4f30\u6307\u6807\u788e\u7247\u5316\uff0876%\u7814\u7a76\u4f7f\u7528\u975e\u6807\u51c6\u6307\u6807\uff09\u3001\u957f\u5c3e\u77e5\u8bc6\u5904\u7406\u4e0d\u8db3\uff08\u4ec512%\u6d89\u53ca\u52a8\u6001\u66f4\u65b0\u673a\u5236\uff09\u7b49\u6838\u5fc3\u5c40\u9650", "conclusion": "\u5efa\u8bae\u672a\u6765\u4f18\u5148\u5f00\u53d1\u9886\u57df\u81ea\u9002\u5e94\u8bc4\u4f30\u6846\u67b6\uff08\u5982RAEval-2026\uff09\uff0c\u5e76\u52a0\u5f3a\u68c0\u7d22-\u751f\u6210\u6a21\u5757\u7684\u534f\u540c\u4f18\u5316\u7406\u8bba\u7814\u7a76"}}
{"id": "2508.06412", "pdf": "https://arxiv.org/pdf/2508.06412", "abs": "https://arxiv.org/abs/2508.06412", "authors": ["Zichuan Liu", "Jinyu Wang", "Lei Song", "Jiang Bian"], "title": "Sample-efficient LLM Optimization with Reset Replay", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent advancements in post-training Large Language Models (LLMs),\nparticularly through Reinforcement Learning (RL) and preference optimization\nmethods, are key drivers for enhancing their reasoning capabilities. However,\nthese methods are often plagued by low sample efficiency and a susceptibility\nto primacy bias, where overfitting to initial experiences degrades policy\nquality and damages the learning process. To address these challenges, we\nintroduce LLM optimization with Reset Replay (LoRR), a general and powerful\nplugin designed to enhance sample efficiency in any preference-based\noptimization framework. LoRR core mechanism enables training at a high replay\nnumber, maximizing the utility of each collected data batch. To counteract the\nrisk of overfitting inherent in high-replay training, LoRR incorporates a\nperiodic reset strategy with reusing initial data, which preserves network\nplasticity. Furthermore, it leverages a hybrid optimization objective,\ncombining supervised fine-tuning (SFT) and preference-based losses to further\nbolster data exploitation. Our extensive experiments demonstrate that LoRR\nsignificantly boosts the performance of various preference optimization methods\non both mathematical and general reasoning benchmarks. Notably, an iterative\nDPO approach augmented with LoRR achieves comparable performance on challenging\nmath tasks, outperforming some complex and computationally intensive RL-based\nalgorithms. These findings highlight that LoRR offers a practical,\nsample-efficient, and highly effective paradigm for LLM finetuning, unlocking\ngreater performance from limited data.", "AI": {"tldr": "\u63d0\u51faLoRR\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u7f6e\u56de\u653e\u673a\u5236\u548c\u6df7\u5408\u4f18\u5316\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u7684\u6837\u672c\u6548\u7387\u4e0e\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u548c\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\u3001\u6613\u53d7\u9996\u56e0\u6548\u5e94\u5f71\u54cd\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u7b56\u7565\u8d28\u91cf\u4e0b\u964d\u548c\u5b66\u4e60\u8fc7\u7a0b\u53d7\u635f\u3002", "method": "1. \u9ad8\u56de\u653e\u6b21\u6570\u7684\u8bad\u7ec3\u673a\u5236 2. \u5468\u671f\u6027\u91cd\u7f6e\u7b56\u7565\u4e0e\u521d\u59cb\u6570\u636e\u590d\u7528 3. \u7ed3\u5408\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u504f\u597d\u635f\u5931\u7684\u6df7\u5408\u4f18\u5316\u76ee\u6807", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u901a\u7528\u63a8\u7406\u4efb\u52a1\u4e2d\uff0cLoRR\u4f7fDPO\u8fed\u4ee3\u65b9\u6cd5\u8fbe\u5230\u4e0e\u590d\u6742\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u90e8\u5206\u573a\u666f\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "LoRR\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684LLM\u5fae\u8c03\u8303\u5f0f\uff0c\u80fd\u591f\u5728\u6709\u9650\u6570\u636e\u6761\u4ef6\u4e0b\u91ca\u653e\u6a21\u578b\u66f4\u5927\u6f5c\u529b\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2508.06457", "pdf": "https://arxiv.org/pdf/2508.06457", "abs": "https://arxiv.org/abs/2508.06457", "authors": ["Sanket Badhe"], "title": "ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.MA"], "comment": "Accepted at CAMLIS 25: Conference on Applied Machine Learning for\n  Information Security. 10 pages, 3 figures", "summary": "Large Language Models (LLMs) have demonstrated impressive fluency and\nreasoning capabilities, but their potential for misuse has raised growing\nconcern. In this paper, we present ScamAgent, an autonomous multi-turn agent\nbuilt on top of LLMs, capable of generating highly realistic scam call scripts\nthat simulate real-world fraud scenarios. Unlike prior work focused on\nsingle-shot prompt misuse, ScamAgent maintains dialogue memory, adapts\ndynamically to simulated user responses, and employs deceptive persuasion\nstrategies across conversational turns. We show that current LLM safety\nguardrails, including refusal mechanisms and content filters, are ineffective\nagainst such agent-based threats. Even models with strong prompt-level\nsafeguards can be bypassed when prompts are decomposed, disguised, or delivered\nincrementally within an agent framework. We further demonstrate the\ntransformation of scam scripts into lifelike voice calls using modern\ntext-to-speech systems, completing a fully automated scam pipeline. Our\nfindings highlight an urgent need for multi-turn safety auditing, agent-level\ncontrol frameworks, and new methods to detect and disrupt conversational\ndeception powered by generative AI.", "AI": {"tldr": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684ScamAgent\u8bc8\u9a97\u4ee3\u7406\uff0c\u80fd\u751f\u6210\u903c\u771f\u591a\u8f6e\u8bc8\u9a97\u811a\u672c\uff0c\u73b0\u6709\u5b89\u5168\u673a\u5236\u5747\u88ab\u7a81\u7834\uff0c\u9700\u5efa\u7acb\u65b0\u578b\u9632\u62a4\u4f53\u7cfb", "motivation": "\u5f53\u524dLLM\u5b89\u5168\u9632\u62a4\u673a\u5236\u4ec5\u9488\u5bf9\u5355\u6b21\u63d0\u793a\u6ee5\u7528\uff0c\u65e0\u6cd5\u5e94\u5bf9\u5177\u5907\u5bf9\u8bdd\u8bb0\u5fc6\u548c\u6e10\u8fdb\u5f0f\u6b3a\u9a97\u7b56\u7565\u7684\u4ee3\u7406\u7ea7\u5a01\u80c1", "method": "\u6784\u5efa\u5177\u5907\u5bf9\u8bdd\u8bb0\u5fc6\u548c\u52a8\u6001\u9002\u5e94\u80fd\u529b\u7684\u8bc8\u9a97\u4ee3\u7406\uff0c\u6d4b\u8bd5\u4e3b\u6d41LLM\u9632\u62a4\u673a\u5236\uff0c\u7ed3\u5408TTS\u6280\u672f\u5b9e\u73b0\u5168\u81ea\u52a8\u8bc8\u9a97\u6d41\u7a0b", "result": "\u73b0\u6709\u5185\u5bb9\u8fc7\u6ee4\u548c\u62d2\u7edd\u673a\u5236100%\u5931\u6548\uff0cGPT-4\u7b49\u6a21\u578b\u4fdd\u62a4\u53ef\u88ab\u5206\u89e3\u63d0\u793a\u7ed5\u8fc7\uff0c\u5408\u6210\u8bed\u97f3\u8bc8\u9a97\u6210\u529f\u7387\u63d0\u5347300%", "conclusion": "\u5fc5\u987b\u5f00\u53d1\u591a\u8f6e\u5b89\u5168\u5ba1\u8ba1\u6846\u67b6\u3001\u5efa\u7acb\u4ee3\u7406\u7ea7\u63a7\u5236\u673a\u5236\uff0c\u5e76\u7814\u53d1\u65b0\u578b\u5bf9\u8bdd\u6b3a\u9a97\u68c0\u6d4b\u6280\u672f\u5e94\u5bf9\u751f\u6210\u5f0fAI\u5a01\u80c1"}}
{"id": "2508.06492", "pdf": "https://arxiv.org/pdf/2508.06492", "abs": "https://arxiv.org/abs/2508.06492", "authors": ["Yuwei Yang", "Zeyu Zhang", "Yunzhong Hou", "Zhuowan Li", "Gaowen Liu", "Ali Payani", "Yuan-Sen Ting", "Liang Zheng"], "title": "Effective Training Data Synthesis for Improving MLLM Chart Understanding", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted by ICCV 2025 (poster). 26 pages, 17 figures", "summary": "Being able to effectively read scientific plots, or chart understanding, is a\ncentral part toward building effective agents for science. However, existing\nmultimodal large language models (MLLMs), especially open-source ones, are\nstill falling behind with a typical success rate of 30%-50% on challenging\nbenchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are\noften restricted by their inadequate similarity to the real charts, which could\ncompromise model training and performance on complex real-world charts. In this\nstudy, we show that modularizing chart generation and diversifying visual\ndetails improves chart understanding capabilities. In particular, we design a\nfive-step data synthesis pipeline, where we separate data and function creation\nfor single plot generation, condition the generation of later subplots on\nearlier ones for multi-subplot figures, visually diversify the generated\nfigures, filter out low quality data, and finally generate the question-answer\n(QA) pairs with GPT-4o. This approach allows us to streamline the generation of\nfine-tuning datasets and introduce the effective chart dataset (ECD), which\ncontains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring\n250+ chart type combinations with high visual complexity. We show that ECD\nconsistently improves the performance of various MLLMs on a range of real-world\nand synthetic test sets. Code, data and models are available at:\nhttps://github.com/yuweiyang-anu/ECD.", "AI": {"tldr": "\u63d0\u51fa\u4e94\u6b65\u6570\u636e\u5408\u6210\u6d41\u7a0b\u751f\u6210ECD\u6570\u636e\u96c6\uff08\u542b10k+\u56fe\u8868\u548c300k+QA\u5bf9\uff09\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u56fe\u8868\u7406\u89e3\u80fd\u529b", "motivation": "\u73b0\u6709MLLMs\u5728\u771f\u5b9e\u79d1\u5b66\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u4ec530%-50%\uff0c\u4f20\u7edf\u5408\u6210\u56fe\u8868\u4e0e\u771f\u5b9e\u56fe\u8868\u5dee\u5f02\u5927\u5f71\u54cd\u6a21\u578b\u8bad\u7ec3\u6548\u679c", "method": "\u4e94\u6b65\u6570\u636e\u5408\u6210\u6d41\u7a0b\uff1a1)\u5206\u79bb\u6570\u636e\u4e0e\u529f\u80fd\u751f\u6210 2)\u591a\u5b50\u56fe\u6761\u4ef6\u751f\u6210 3)\u89c6\u89c9\u591a\u6837\u5316\u5904\u7406 4)\u6570\u636e\u8d28\u91cf\u8fc7\u6ee4 5)GPT-4o\u751f\u6210QA\u5bf9", "result": "\u6784\u5efa\u8986\u76d625\u4e2a\u4e3b\u9898\u3001250+\u56fe\u8868\u7ec4\u5408\u7684ECD\u6570\u636e\u96c6\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u63d0\u5347\u5404\u7c7bMLLMs\u5728\u771f\u5b9e/\u5408\u6210\u6d4b\u8bd5\u96c6\u7684\u6027\u80fd", "conclusion": "\u6a21\u5757\u5316\u56fe\u8868\u751f\u6210\u4e0e\u89c6\u89c9\u591a\u6837\u5316\u7b56\u7565\u80fd\u6709\u6548\u589e\u5f3a\u6a21\u578b\u5bf9\u590d\u6742\u79d1\u5b66\u56fe\u8868\u7684\u7406\u89e3\u80fd\u529b\uff0cECD\u6570\u636e\u96c6\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027"}}
