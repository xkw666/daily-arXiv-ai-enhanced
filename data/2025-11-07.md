<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [cs.AI](#cs.AI) [Total: 7]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Activation-Space Personality Steering: Hybrid Layer Selection for Stable Trait Control in LLMs](https://arxiv.org/abs/2511.03738)
*Pranav Bhandari,Nicolas Fay,Sanjeevan Selvaganapathy,Amitava Datta,Usman Naseem,Mehwish Nasim*

Main category: cs.CL

TL;DR: 提出基于大五人格特质的隐藏状态激活提取流程，实现LLM行为输出的精确控制且不影响原有能力


<details>
  <summary>Details</summary>
Motivation: 现有LLM的隐性个性特征难以可靠控制，缺乏有效的行为操控机制，需探索心理构建与模型内部表示的关系及其引导方式

Method: 使用大五人格框架提取transformer隐藏状态，应用低秩子空间发现方法识别特质特异性最优层，构建动态层选择的灵活控制框架

Result: 发现人格特质存在于低秩共享子空间，通过扰动可有效引导模型行为且不影响流畅性和一般能力

Conclusion: 建立了心理学理论与模型对齐的桥梁，为LLM行为控制提供了可操作的精准调节机制

Abstract: Large Language Models exhibit implicit personalities in their generation, but
reliably controlling or aligning these traits to meet specific needs remains an
open challenge. The need for effective mechanisms for behavioural manipulation
of the model during generation is a critical gap in the literature that needs
to be fulfilled. Personality-aware LLMs hold a promising direction towards this
objective. However, the relationship between these psychological constructs and
their representations within LLMs remains underexplored and requires further
investigation. Moreover, it is intriguing to understand and study the use of
these representations to steer the models' behaviour. We propose a novel
pipeline that extracts hidden state activations from transformer layers using
the Big Five Personality Traits (Openness, Conscientiousness, Extraversion,
Agreeableness and Neuroticism), which is a comprehensive and empirically
validated framework to model human personality applies low-rank subspace
discovery methods, and identifies trait-specific optimal layers across
different model architectures for robust injection. The resulting
personality-aligned directions are then operationalised through a flexible
steering framework with dynamic layer selection, enabling precise control of
trait expression in LLM outputs. Our findings reveal that personality traits
occupy a low-rank shared subspace, and that these latent structures can be
transformed into actionable mechanisms for effective steering through careful
perturbations without impacting the fluency, variance and general capabilities,
helping to bridge the gap between psychological theory and practical model
alignment.

</details>


### [2] [TextualVerifier: Verify TextGrad Step-by-Step](https://arxiv.org/abs/2511.03739)
*Eugenius Mario Situmorang,Adila Alfa Krisnadhi,Ari Wibisono*

Main category: cs.CL

TL;DR: 提出TextualVerifier框架——通过链式思维分解和多数投票机制，首次实现TextGrad文本优化系统的自我验证，无需数值梯度即可提升推理可靠性


<details>
  <summary>Details</summary>
Motivation: TextGrad缺乏确保文本决策中推理有效性的自我验证机制，可能导致优化结果偏差

Method: 四阶段验证流程：1) 链式思维分解 2) 变体生成 3) 多数投票 4) 共识聚合；非侵入式集成到TextGrad的损失函数和结果验证阶段

Result: PRM800K验证阶段推理有效性提升29%；GPQA-Diamond等基准测试平均提升2.2-10.71个百分点（p<0.001），单次优化平均增加5.9次LLM调用

Conclusion: 开创基于LLM的文本优化验证新范式，为文本梯度方法提供可扩展的验证解决方案，拓展非数值优化系统的可靠性研究边界

Abstract: TextGrad is a novel approach to text-based automatic differentiation that
enables composite AI systems to perform optimization without explicit numerical
equations. However, it currently lacks self-verification mechanisms that ensure
reasoning validity in text-based decision making. This research introduces
TextualVerifier, a verification framework that leverages chain-of-thought
reasoning and majority voting with large language models to address this
verification gap. TextualVerifier implements a four-stage workflow:
chain-of-thought decomposition, variant generation, majority voting, and
consensus aggregation. It integrates non-invasively with TextGrad at both the
loss function and optimization result verification stages. Experimental
evaluation using the Gemini 1.5 Pro model is conducted in two phases: (1)
standalone evaluation on PRM800K, and (2) integrated evaluation with TextGrad
on GPQA-Diamond, MMLU-ML, and MMLU-CP benchmarks. Results show statistically
significant improvements (p < 0.001). In phase one, TextualVerifier improves
the validity of reasoning steps by 29 percent. In phase two, integration into
TextGrad loss function yields a 2.2 percentage point gain from 68.2 to 70.4
percent with a moderate overhead of 5.9 LLM calls on average. Further
evaluations of TextualVerifier versioning yield 8.08, 10.71, and 3.92
percentage point improvements on GPQA, MMLU-ML, and MMLU-CP respectively.
TextualVerifier thus presents the first self-verification framework for
TextGrad through LLM-based techniques without requiring numerical gradients,
enabling more reliable reasoning and opening new directions for verification in
text-based optimization.

</details>


### [3] [GRDD+: An Extended Greek Dialectal Dataset with Cross-Architecture Fine-tuning Evaluation](https://arxiv.org/abs/2511.03772)
*Stergios Chatzikyriakidis,Dimitris Papadakis,Sevasti-Ioanna Papaioannou,Erofili Psaltaki*

Main category: cs.CL

TL;DR: 扩展希腊方言数据集GRDD+至637万词/10种变体，测试方言数据对LLMs微调效果


<details>
  <summary>Details</summary>
Motivation: 解决现有希腊方言数据集规模有限和多样性不足的问题，探索方言数据对语言模型性能的影响

Method: 1. 扩展GRDD数据集至6种新方言变体 2. 使用三种8B参数模型微调 3. 与主流大模型进行对比实验

Result: 获得首个大规模多方言希腊数据集，不同架构模型在方言任务上表现出显著差异

Conclusion: 高质量方言数据能有效提升LLMs的方言处理能力，为低资源语言研究提供新基准

Abstract: We present an extended Greek Dialectal Dataset (GRDD+) 1that complements the
existing GRDD dataset with more data from Cretan, Cypriot, Pontic and Northern
Greek, while we add six new varieties: Greco-Corsican, Griko (Southern Italian
Greek), Maniot, Heptanesian, Tsakonian, and Katharevusa Greek. The result is a
dataset with total size 6,374,939 words and 10 varieties. This is the first
dataset with such variation and size to date. We conduct a number of
fine-tuning experiments to see the effect of good quality dialectal data on a
number of LLMs. We fine-tune three model architectures (Llama-3-8B,
Llama-3.1-8B, Krikri-8B) and compare the results to frontier models
(Claude-3.7-Sonnet, Gemini-2.5, ChatGPT-5).

</details>


### [4] [PLLuM: A Family of Polish Large Language Models](https://arxiv.org/abs/2511.03823)
*Jan Kocoń,Maciej Piasecki,Arkadiusz Janz,Teddy Ferdinan,Łukasz Radliński,Bartłomiej Koptyra,Marcin Oleksy,Stanisław Woźniak,Paweł Walkowiak,Konrad Wojtasik,Julia Moska,Tomasz Naskręt,Bartosz Walkowiak,Mateusz Gniewkowski,Kamil Szyc,Dawid Motyka,Dawid Banach,Jonatan Dalasiński,Ewa Rudnicka,Bartłomiej Alberski,Tomasz Walkowiak,Aleksander Szczęsny,Maciej Markiewicz,Tomasz Bernaś,Hubert Mazur,Kamil Żyta,Mateusz Tykierko,Grzegorz Chodak,Tomasz Kajdanowicz,Przemysław Kazienko,Agnieszka Karlińska,Karolina Seweryn,Anna Kołos,Maciej Chrabąszcz,Katarzyna Lorenc,Aleksandra Krasnodębska,Artur Wilczek,Katarzyna Dziewulska,Paula Betscher,Zofia Cieślińska,Katarzyna Kowol,Daria Mikoś,Maciej Trzciński,Dawid Krutul,Marek Kozłowski,Sławomir Dadas,Rafał Poświata,Michał Perełkiewicz,Małgorzata Grębowiec,Maciej Kazuła,Marcin Białas,Roman Roszko,Danuta Roszko,Jurgita Vaičenonienė,Andrius Utka,Paweł Levchuk,Paweł Kowalski,Irena Prawdzic-Jankowska,Maciej Ogrodniczuk,Monika Borys,Anna Bulińska,Wiktoria Gumienna,Witold Kieraś,Dorota Komosińska,Katarzyna Krasnowska-Kieraś,Łukasz Kobyliński,Martyna Lewandowska,Marek Łaziński,Mikołaj Łątkowski,Dawid Mastalerz,Beata Milewicz,Agnieszka Anna Mykowiecka,Angelika Peljak-Łapińska,Sandra Penno,Zuzanna Przybysz,Michał Rudolf,Piotr Rybak,Karolina Saputa,Aleksandra Tomaszewska,Aleksander Wawer,Marcin Woliński,Joanna Wołoszyn,Alina Wróblewska,Bartosz Żuk,Filip Żarnecki,Konrad Kaczyński,Anna Cichosz,Zuzanna Deckert,Monika Garnys,Izabela Grabarczyk,Wojciech Janowski,Sylwia Karasińska,Aleksandra Kujawiak,Piotr Misztela,Maria Szymańska,Karolina Walkusz,Igor Siek,Jakub Kwiatkowski,Piotr Pęzik*

Main category: cs.CL

TL;DR: 波兰研究机构联合开发了最大的开源波兰语大模型PLLuM，包含新构建的1400亿token预训练语料库和负责任AI框架


<details>
  <summary>Details</summary>
Motivation: 解决英语主导的大语言模型生态中其他语言支持不足的问题，满足波兰对高质量、透明且符合文化特征的语言模型需求

Method: 1. 构建1400亿token波兰语料库
2. 开发7.7万指令数据集和10万偏好优化数据集
3. 设计包含数据治理和混合安全模块的负责任AI框架

Result: 成功开发基础模型和指令微调变体，在公共行政领域验证了实际应用价值

Conclusion: 通过开源模型推动波兰开放研究生态，增强国家主权AI技术能力

Abstract: Large Language Models (LLMs) play a central role in modern artificial
intelligence, yet their development has been primarily focused on English,
resulting in limited support for other languages. We present PLLuM (Polish
Large Language Model), the largest open-source family of foundation models
tailored specifically for the Polish language. Developed by a consortium of
major Polish research institutions, PLLuM addresses the need for high-quality,
transparent, and culturally relevant language models beyond the English-centric
commercial landscape. We describe the development process, including the
construction of a new 140-billion-token Polish text corpus for pre-training, a
77k custom instructions dataset, and a 100k preference optimization dataset. A
key component is a Responsible AI framework that incorporates strict data
governance and a hybrid module for output correction and safety filtering. We
detail the models' architecture, training procedures, and alignment techniques
for both base and instruction-tuned variants, and demonstrate their utility in
a downstream task within public administration. By releasing these models
publicly, PLLuM aims to foster open research and strengthen sovereign AI
technologies in Poland.

</details>


### [5] [STARS: Segment-level Token Alignment with Rejection Sampling in Large Language Models](https://arxiv.org/abs/2511.03827)
*Mohammad Atif Quamar,Mohammad Areeb,Mikhail Kuznetsov,Muslum Ozgur Ozmen,Z. Berkay Celik*

Main category: cs.CL

TL;DR: 提出STARS算法：通过分段式令牌对齐与拒绝采样，在解码阶段实现高效且优质的LLM对齐，显著优于传统微调方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大模型对齐方法（如微调）计算成本高、效果欠佳，而推理阶段方法（如Best-of-N采样）需要极高算力。需要找到更高效且有效的对齐方案。

Method: STARS算法在解码阶段对固定长度的短token片段进行迭代采样、评分、拒绝/接受，通过早期修正生成路径提升计算效率和对齐质量。

Result: 在6种LLM测试中，STARS的胜率比监督微调(SFT)高14.9%，比直接偏好优化(DPO)高4.3%，与Best-of-N基线效果相当。

Conclusion: 基于细粒度奖励引导的分段采样方法为LLM对齐提供了高效、可泛化的替代方案，突破了传统微调和全序列排序方法的局限性。

Abstract: Aligning large language models with human values is crucial for their safe
deployment; however, existing methods, such as fine-tuning, are computationally
expensive and suboptimal. In contrast, inference-time approaches like Best-of-N
sampling require practically infeasible computation to achieve optimal
alignment. We propose STARS: Segment-level Token Alignment with Rejection
Sampling, a decoding-time algorithm that steers model generation by iteratively
sampling, scoring, and rejecting/accepting short, fixed-size token segments.
This allows for early correction of the generation path, significantly
improving computational efficiency and boosting alignment quality. Across a
suite of six LLMs, we show that STARS outperforms Supervised Fine-Tuning (SFT)
by up to 14.9 percentage points and Direct Preference Optimization (DPO) by up
to 4.3 percentage points on win-rates, while remaining highly competitive with
strong Best-of-N baselines. Our work establishes granular, reward-guided
sampling as a generalizable, robust, and efficient alternative to traditional
fine-tuning and full-sequence ranking methods for aligning LLMs.

</details>


### [6] [Divide, Cache, Conquer: Dichotomic Prompting for Efficient Multi-Label LLM-Based Classification](https://arxiv.org/abs/2511.03830)
*Mikołaj Langner,Jan Eliasz,Ewa Rudnicka,Jan Kocoń*

Main category: cs.CL

TL;DR: 提出基于二分决策链的多标签文本分类方法，通过分解标注任务结合前缀缓存机制，实现短文本推理效率提升且保持准确率


<details>
  <summary>Details</summary>
Motivation: 传统LLM多标签分类采用结构化输出模式效率低下，分解为独立二分决策可结合前缀缓存显著提升短文本推理速度

Method: 1. 将多标签分类重构为二分决策序列 2. 使用DeepSeek-V3生成多维度情感标注 3. 通过知识蒸馏微调HerBERT-Large等小型模型

Result: 微调模型在训练维度上显著超越零样本基线，结合缓存感知推理实现效率与精度的平衡

Conclusion: 二分决策分解+知识蒸馏+缓存优化的组合框架具有跨领域扩展性，为LLM分类任务提供新范式

Abstract: We introduce a method for efficient multi-label text classification with
large language models (LLMs), built on reformulating classification tasks as
sequences of dichotomic (yes/no) decisions. Instead of generating all labels in
a single structured response, each target dimension is queried independently,
which, combined with a prefix caching mechanism, yields substantial efficiency
gains for short-text inference without loss of accuracy. To demonstrate the
approach, we focus on affective text analysis, covering 24 dimensions including
emotions and sentiment. Using LLM-to-SLM distillation, a powerful annotator
model (DeepSeek-V3) provides multiple annotations per text, which are
aggregated to fine-tune smaller models (HerBERT-Large, CLARIN-1B, PLLuM-8B,
Gemma3-1B). The fine-tuned models show significant improvements over zero-shot
baselines, particularly on the dimensions seen during training. Our findings
suggest that decomposing multi-label classification into dichotomic queries,
combined with distillation and cache-aware inference, offers a scalable and
effective framework for LLM-based classification. While we validate the method
on affective states, the approach is general and applicable across domains.

</details>


### [7] [Evaluating Machine Translation Datasets for Low-Web Data Languages: A Gendered Lens](https://arxiv.org/abs/2511.03880)
*Hellina Hailu Nigatu,Bethelhem Yemane Mamo,Bontu Fufa Balcha,Debora Taye Tesfaye,Elbethel Daniel Zewdie,Ikram Behiru Nesiru,Jitu Ewnetu Hailu,Senait Mengesha Yayo*

Main category: cs.CL

TL;DR: 研究指出低资源语言机器翻译数据集存在严重的性别偏见和领域分布不均问题，强调数据质量的重要性


<details>
  <summary>Details</summary>
Motivation: 针对低资源语言处理中大规模数据集可能忽视质量的问题，研究旨在揭示数据集中潜在的性别偏见和有害内容风险

Method: 通过分析阿法尔奥罗莫语、阿姆哈拉语和提格里尼亚语的MT数据集，从人名分布、语法性别、刻板印象三个维度评估性别表征

Result: 发现训练数据与基准数据集领域错配（政治/宗教 vs 新闻/健康/体育），存在系统性男性偏向，且数据量最大的语言有害内容最多

Conclusion: 应建立数据质量评估标准，在数据集构建早期介入伦理审查，避免数量优先策略加剧社会偏见传播（注：论文包含可能引发不适的NSFW内容）

Abstract: As low-resourced languages are increasingly incorporated into NLP research,
there is an emphasis on collecting large-scale datasets. But in prioritizing
quantity over quality, we risk 1) building language technologies that perform
poorly for these languages and 2) producing harmful content that perpetuates
societal biases. In this paper, we investigate the quality of Machine
Translation (MT) datasets for three low-resourced languages--Afan Oromo,
Amharic, and Tigrinya, with a focus on the gender representation in the
datasets. Our findings demonstrate that while training data has a large
representation of political and religious domain text, benchmark datasets are
focused on news, health, and sports. We also found a large skew towards the
male gender--in names of persons, the grammatical gender of verbs, and in
stereotypical depictions in the datasets. Further, we found harmful and toxic
depictions against women, which were more prominent for the language with the
largest amount of data, underscoring that quantity does not guarantee quality.
We hope that our work inspires further inquiry into the datasets collected for
low-resourced languages and prompts early mitigation of harmful content.
WARNING: This paper contains discussion of NSFW content that some may find
disturbing.

</details>


### [8] [GRAD: Graph-Retrieved Adaptive Decoding for Hallucination Mitigation](https://arxiv.org/abs/2511.03900)
*Manh Nguyen,Sunil Gupta,Dai Do,Hung Le*

Main category: cs.CL

TL;DR: 提出GRAD方法——通过构建语料驱动的token转移图，在解码阶段自适应融合证据logits，有效缓解大语言模型的幻觉问题


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的方法存在提示脆弱性（prompt-based grounding）和符号知识整合成本高的问题，需要不依赖外部知识源的轻量级解决方案

Method: 1. 单次前向传播构建稀疏token转移图
2. 解码时最大归一化证据logits
3. 自适应融合模型原始logits与证据logits

Result: 在三个模型上实现：
- 内在准确率提升9.7%
- 幻觉率降低8.6%
- 正确率提高6.9%
获得最高真实性-信息量综合得分

Conclusion: GRAD提供即插即用的轻量解决方案，证明语料统计证据能有效引导生成真实可验证的输出

Abstract: Hallucination mitigation remains a persistent challenge for large language
models (LLMs), even as model scales grow. Existing approaches often rely on
external knowledge sources, such as structured databases or knowledge graphs,
accessed through prompting or retrieval. However, prompt-based grounding is
fragile and domain-sensitive, while symbolic knowledge integration incurs heavy
retrieval and formatting costs. Motivated by knowledge graphs, we introduce
Graph-Retrieved Adaptive Decoding (GRAD), a decoding-time method that grounds
generation in corpus-derived evidence without retraining. GRAD constructs a
sparse token transition graph by accumulating next-token logits across a small
retrieved corpus in a single forward pass. During decoding, graph-retrieved
logits are max-normalized and adaptively fused with model logits to favor
high-evidence continuations while preserving fluency. Across three models and a
range of question-answering benchmarks spanning intrinsic, extrinsic
hallucination, and factuality tasks, GRAD consistently surpasses baselines,
achieving up to 9.7$\%$ higher intrinsic accuracy, 8.6$\%$ lower hallucination
rates, and 6.9$\%$ greater correctness compared to greedy decoding, while
attaining the highest truth--informativeness product score among all methods.
GRAD offers a lightweight, plug-and-play alternative to contrastive decoding
and knowledge graph augmentation, demonstrating that statistical evidence from
corpus-level token transitions can effectively steer generation toward more
truthful and verifiable outputs.

</details>


### [9] [Context informs pragmatic interpretation in vision-language models](https://arxiv.org/abs/2511.03908)
*Alvin Wei Ming Tan,Ben Prystawski,Veronica Boyce,Michael C. Frank*

Main category: cs.CL

TL;DR: 视觉语言模型在相关上下文条件下表现显著提升，但在少样本抽象参考任务中仍有困难


<details>
  <summary>Details</summary>
Motivation: 探究机器学习和人类在多轮语境下的实用推理能力差异，特别是上下文数量、顺序和相关性对表现的影响

Method: 通过迭代参考游戏实验，对比人类和模型在不同上下文条件（相关/无关、数量变化、顺序调整）下的表现

Result: 无相关上下文时模型准确率高于随机但显著低于人类；相关上下文条件下模型表现随试验次数急剧提升；抽象概念的少样本参考游戏仍具挑战性

Conclusion: 上下文相关性显著影响模型表现，现有模型在复杂语境推理和少样本抽象任务处理方面仍需改进

Abstract: Iterated reference games - in which players repeatedly pick out novel
referents using language - present a test case for agents' ability to perform
context-sensitive pragmatic reasoning in multi-turn linguistic environments. We
tested humans and vision-language models on trials from iterated reference
games, varying the given context in terms of amount, order, and relevance.
Without relevant context, models were above chance but substantially worse than
humans. However, with relevant context, model performance increased
dramatically over trials. Few-shot reference games with abstract referents
remain a difficult task for machine learning models.

</details>


### [10] [The Human Flourishing Geographic Index: A County-Level Dataset for the United States, 2013--2023](https://arxiv.org/abs/2511.03915)
*Stefano M. Iacus,Devika Jain,Andrea Nasuto,Giuseppe Porro,Marcello Carammia,Andrea Vezzulli*

Main category: cs.CL

TL;DR: 利用26亿条地理标记推特数据和微调AI模型构建了美国县级/州级高时空分辨率的人类繁荣指数（HFGI）


<details>
  <summary>Details</summary>
Motivation: 现有社会福祉测量指标缺乏时空细粒度，难以及时反映多维度的繁荣状态

Method: 通过微调LLM模型对2013-2023年26亿条美国推文进行语义分析，构建48个指标框架（含移民态度和腐败感知）

Result: 验证显示该指数能准确反映理论框架，且与现有指标存在预期相关性

Conclusion: 该数据集为跨学科研究提供了10年美国社会福祉动态的高分辨率观测窗口

Abstract: Quantifying human flourishing, a multidimensional construct including
happiness, health, purpose, virtue, relationships, and financial stability, is
critical for understanding societal well-being beyond economic indicators.
Existing measures often lack fine spatial and temporal resolution. Here we
introduce the Human Flourishing Geographic Index (HFGI), derived from analyzing
approximately 2.6 billion geolocated U.S. tweets (2013-2023) using fine-tuned
large language models to classify expressions across 48 indicators aligned with
Harvard's Global Flourishing Study framework plus attitudes towards migration
and perception of corruption. The dataset offers monthly and yearly county- and
state-level indicators of flourishing-related discourse, validated to confirm
that the measures accurately represent the underlying constructs and show
expected correlations with established indicators. This resource enables
multidisciplinary analyses of well-being, inequality, and social change at
unprecedented resolution, offering insights into the dynamics of human
flourishing as reflected in social media discourse across the United States
over the past decade.

</details>


### [11] [Direct Semantic Communication Between Large Language Models via Vector Translation](https://arxiv.org/abs/2511.03945)
*Fu-Chun Yang,Jason Eshraghian*

Main category: cs.CL

TL;DR: 提出跨模型向量翻译技术，实现语义级信息传递并降低计算开销


<details>
  <summary>Details</summary>
Motivation: 解决多智能体系统中传统token传递导致的语义丢失与计算低效问题

Method: 开发双编码器翻译框架，通过混合注入策略实现潜在空间语义映射

Result: 达到0.538余弦对齐度，2.01:1传输不对称性，保持生成稳定性

Conclusion: 验证跨模型潜在通信可行性，推动协作AI系统向语义共享进化

Abstract: In multi-agent settings, such as debate, reflection, or tool-calling, large
language models (LLMs) pass messages as plain tokens, discarding most latent
semantics. This constrains information transfer and adds unnecessary
computational overhead. We form a latent bridge via vector translations, which
use learned mappings that enable direct semantic exchange between
representation spaces. A dual-encoder translator trained between Llama-2-7B and
Mistral-7B-Instruct attains an average cosine alignment of 0.538. Injecting the
translated vectors at 30 percent blending strength steers the target model's
generation without destabilizing logits. Bidirectional evaluation shows a
2.01:1 transfer asymmetry, indicating that general-purpose models yield more
transferable representations than instruction-tuned variants. This conservative
injection preserves computational stability while demonstrating that
cross-model latent communication is feasible, enabling collaborative AI systems
that share meaning rather than tokens.

</details>


### [12] [Abductive Inference in Retrieval-Augmented Language Models: Generating and Validating Missing Premises](https://arxiv.org/abs/2511.04020)
*Shiyin Lin*

Main category: cs.CL

TL;DR: 在检索增强生成(RAG)系统中引入溯因推理机制，通过自动生成并验证缺失前提来提升答案准确性与推理可信度


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统在检索证据不完整时会出现推理断层，亟需新方法增强系统鲁棒性和可解释性

Method: 提出三步框架：证据充分性检测→候选前提生成→一致性/合理性验证

Result: 在Abductive Reasoning和Multi-hop QA基准测试中，准确率分别提升8.2%和5.7%，推理可信度提高12.4%

Conclusion: 溯因推理为RAG系统提供了新的优化维度，该方法显著增强了复杂推理任务中的系统可靠性

Abstract: Large Language Models (LLMs) enhanced with retrieval -- commonly referred to
as Retrieval-Augmented Generation (RAG) -- have demonstrated strong performance
in knowledge-intensive tasks. However, RAG pipelines often fail when retrieved
evidence is incomplete, leaving gaps in the reasoning process. In such cases,
\emph{abductive inference} -- the process of generating plausible missing
premises to explain observations -- offers a principled approach to bridge
these gaps. In this paper, we propose a framework that integrates abductive
inference into retrieval-augmented LLMs. Our method detects insufficient
evidence, generates candidate missing premises, and validates them through
consistency and plausibility checks. Experimental results on abductive
reasoning and multi-hop QA benchmarks show that our approach improves both
answer accuracy and reasoning faithfulness. This work highlights abductive
inference as a promising direction for enhancing the robustness and
explainability of RAG systems.

</details>


### [13] [WST: Weakly Supervised Transducer for Automatic Speech Recognition](https://arxiv.org/abs/2511.04035)
*Dongji Gao,Chenda Liao,Changliang Liu,Matthew Wiesner,Leibny Paola Garcia,Daniel Povey,Sanjeev Khudanpur,Jian Wu*

Main category: cs.CL

TL;DR: 提出弱监督转换器WST，在标注错误率高达70%的情况下仍能保持ASR性能，优于现有CTC方法


<details>
  <summary>Details</summary>
Motivation: 减少对高质量标注数据的依赖，现有弱监督方法需要额外置信度估计或预训练模型支持

Method: 设计灵活的训练图结构，直接处理转录错误而无需置信度估计或预训练模型

Result: 在合成和工业数据集上验证，WST在70%转录错误率下仍保持性能，持续优于BTC/OTC等CTC方法

Conclusion: WST在现实ASR场景中展现出实用性和鲁棒性，模型实现将开源

Abstract: The Recurrent Neural Network-Transducer (RNN-T) is widely adopted in
end-to-end (E2E) automatic speech recognition (ASR) tasks but depends heavily
on large-scale, high-quality annotated data, which are often costly and
difficult to obtain. To mitigate this reliance, we propose a Weakly Supervised
Transducer (WST), which integrates a flexible training graph designed to
robustly handle errors in the transcripts without requiring additional
confidence estimation or auxiliary pre-trained models. Empirical evaluations on
synthetic and industrial datasets reveal that WST effectively maintains
performance even with transcription error rates of up to 70%, consistently
outperforming existing Connectionist Temporal Classification (CTC)-based weakly
supervised approaches, such as Bypass Temporal Classification (BTC) and
Omni-Temporal Classification (OTC). These results demonstrate the practical
utility and robustness of WST in realistic ASR settings. The implementation
will be publicly available.

</details>


### [14] [T-FIX: Text-Based Explanations with Features Interpretable to eXperts](https://arxiv.org/abs/2511.04070)
*Shreya Havaldar,Helen Jin,Chaehyeon Kim,Anton Xue,Weiqiu You,Marco Gatti,Bhuvnesh Jain,Helen Qu,Daniel A Hashimoto,Amin Madani,Rajat Deo,Sameed Ahmed M. Khatana,Gary E. Weissman,Lyle Ungar,Eric Wong*

Main category: cs.CL

TL;DR: 专家领域LLM解释需与专家直觉对齐，现有评估不足，提出T-FIX基准及新指标


<details>
  <summary>Details</summary>
Motivation: 当前LLM解释评估仅关注合理性，缺乏对专家领域知识匹配度的衡量。在手术/天文学/心理学等专业领域，用户需要与专家思维方式一致的解释

Method: 联合领域专家构建跨7个知识密集型领域的T-FIX基准，开发基于专家判断的解释对齐量化评估体系

Result: 揭示现有LLM解释与专家认知的偏差，建立首个面向专家知识对齐的解释评估框架

Conclusion: 专家知识对齐应成为LLM解释评估的关键维度，T-FIX为提升专业场景解释质量提供基准工具

Abstract: As LLMs are deployed in knowledge-intensive settings (e.g., surgery,
astronomy, therapy), users expect not just answers, but also meaningful
explanations for those answers. In these settings, users are often domain
experts (e.g., doctors, astrophysicists, psychologists) who require
explanations that reflect expert-level reasoning. However, current evaluation
schemes primarily emphasize plausibility or internal faithfulness of the
explanation, which fail to capture whether the content of the explanation truly
aligns with expert intuition. We formalize expert alignment as a criterion for
evaluating explanations with T-FIX, a benchmark spanning seven
knowledge-intensive domains. In collaboration with domain experts, we develop
novel metrics to measure the alignment of LLM explanations with expert
judgment.

</details>


### [15] [Plan of Knowledge: Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering](https://arxiv.org/abs/2511.04072)
*Xinying Qian,Ying Zhang,Yu Zhao,Baohang Zhou,Xuhui Sui,Xiaojie Yuan*

Main category: cs.CL

TL;DR: 提出PoK框架（知识规划+对比时间检索器），通过结构化分解问题与时间知识检索，显著提升LLMs在时间知识图谱问答中的表现


<details>
  <summary>Details</summary>
Motivation: 现有TKGQA方法难以处理复杂时间约束语义，而LLMs存在时间推理能力不足、幻觉及知识缺失问题，需结合结构化规划与知识检索

Method: 1. 知识规划模块将复杂问题分解为工具预定义子目标链；2. 构建含对比检索框架的时间知识库（TKS），实现时间与语义双对齐检索

Result: 在四个TKGQA基准数据集上，PoK使LLMs检索精度和推理准确率显著提升，最高超过SOTA方法56%

Conclusion: PoK通过结构化目标分解与时间知识检索的结合，有效增强了时间推理的可解释性与事实一致性，突破LLMs的时间推理瓶颈

Abstract: Temporal Knowledge Graph Question Answering (TKGQA) aims to answer
time-sensitive questions by leveraging factual information from Temporal
Knowledge Graphs (TKGs). While previous studies have employed pre-trained TKG
embeddings or graph neural networks to inject temporal knowledge, they fail to
fully understand the complex semantic information of time constraints.
Recently, Large Language Models (LLMs) have shown remarkable progress,
benefiting from their strong semantic understanding and reasoning
generalization capabilities. However, their temporal reasoning ability remains
limited. LLMs frequently suffer from hallucination and a lack of knowledge. To
address these limitations, we propose the Plan of Knowledge framework with a
contrastive temporal retriever, which is named PoK. Specifically, the proposed
Plan of Knowledge module decomposes a complex temporal question into a sequence
of sub-objectives from the pre-defined tools, serving as intermediate guidance
for reasoning exploration. In parallel, we construct a Temporal Knowledge Store
(TKS) with a contrastive retrieval framework, enabling the model to selectively
retrieve semantically and temporally aligned facts from TKGs. By combining
structured planning with temporal knowledge retrieval, PoK effectively enhances
the interpretability and factual consistency of temporal reasoning. Extensive
experiments on four benchmark TKGQA datasets demonstrate that PoK significantly
improves the retrieval precision and reasoning accuracy of LLMs, surpassing the
performance of the state-of-the-art TKGQA methods by 56.0% at most.

</details>


### [16] [The truth is no diaper: Human and AI-generated associations to emotional words](https://arxiv.org/abs/2511.04077)
*Špela Vintar,Jan Jona Javoršek*

Main category: cs.CL

TL;DR: 研究对比人类与LLM在情感词汇联想上的差异，发现LLM联想情感负载更强、更可预测且缺乏创造性


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型是否以类似人类的方式生成词汇联想，尤其在情感词汇处理机制上的异同

Method: 通过对比人类与LLM对情感负载词汇的联想行为，分析生成内容的可预测性和情感强度

Result: LLM与人类联想重叠度中等，但LLM会放大词汇情感负载，联想模式更缺乏创造性

Conclusion: LLM的词汇联想机制与人类存在系统性差异，在创造性思维模拟方面仍存在局限性

Abstract: Human word associations are a well-known method of gaining insight into the
internal mental lexicon, but the responses spontaneously offered by human
participants to word cues are not always predictable as they may be influenced
by personal experience, emotions or individual cognitive styles. The ability to
form associative links between seemingly unrelated concepts can be the driving
mechanisms of creativity. We perform a comparison of the associative behaviour
of humans compared to large language models. More specifically, we explore
associations to emotionally loaded words and try to determine whether large
language models generate associations in a similar way to humans. We find that
the overlap between humans and LLMs is moderate, but also that the associations
of LLMs tend to amplify the underlying emotional load of the stimulus, and that
they tend to be more predictable and less creative than human ones.

</details>


### [17] [Improving the Performance of Radiology Report De-identification with Large-Scale Training and Benchmarking Against Cloud Vendor Methods](https://arxiv.org/abs/2511.04079)
*Eva Prakash,Maayane Attias,Pierre Chambon,Justin Xu,Steven Truong,Jean-Benoit Delbrouck,Tessa Cook,Curtis Langlotz*

Main category: cs.CL

TL;DR: 提出基于Transformer的医疗文本去识别模型，通过多机构放射学数据训练，在PHI检测性能上超越现有商业系统并建立新标准


<details>
  <summary>Details</summary>
Motivation: 解决医疗文本去识别中跨机构泛化能力不足的问题，验证大规模训练对模型性能的提升效果

Method: 1. 构建包含胸部X光/CT、腹部CT、脑部MR的多模态放射学数据集 2. 在原有模型架构中新增AGE类别识别 3. 采用「隐式合成PHI」方法评估数据稳定性 4. 与AWS/Azure等主流云服务进行横向对比

Result: 1. 在Stanford/Penn测试集分别达到0.996/0.973 F1值 2. 合成PHI检测F1稳定在0.959 3. 商业系统F1最高仅0.754

Conclusion: 模型通过跨机构多模态训练实现SOTA性能，为临床文本处理建立了兼顾隐私保护与数据效用的新范式

Abstract: Objective: To enhance automated de-identification of radiology reports by
scaling transformer-based models through extensive training datasets and
benchmarking performance against commercial cloud vendor systems for protected
health information (PHI) detection. Materials and Methods: In this
retrospective study, we built upon a state-of-the-art, transformer-based, PHI
de-identification pipeline by fine-tuning on two large annotated radiology
corpora from Stanford University, encompassing chest X-ray, chest CT,
abdomen/pelvis CT, and brain MR reports and introducing an additional PHI
category (AGE) into the architecture. Model performance was evaluated on test
sets from Stanford and the University of Pennsylvania (Penn) for token-level
PHI detection. We further assessed (1) the stability of synthetic PHI
generation using a "hide-in-plain-sight" method and (2) performance against
commercial systems. Precision, recall, and F1 scores were computed across all
PHI categories. Results: Our model achieved overall F1 scores of 0.973 on the
Penn dataset and 0.996 on the Stanford dataset, outperforming or maintaining
the previous state-of-the-art model performance. Synthetic PHI evaluation
showed consistent detectability (overall F1: 0.959 [0.958-0.960]) across 50
independently de-identified Penn datasets. Our model outperformed all vendor
systems on synthetic Penn reports (overall F1: 0.960 vs. 0.632-0.754).
Discussion: Large-scale, multimodal training improved cross-institutional
generalization and robustness. Synthetic PHI generation preserved data utility
while ensuring privacy. Conclusion: A transformer-based de-identification model
trained on diverse radiology datasets outperforms prior academic and commercial
systems in PHI detection and establishes a new benchmark for secure clinical
text processing.

</details>


### [18] [A Characterization of List Language Identification in the Limit](https://arxiv.org/abs/2511.04103)
*Moses Charikar,Chirag Pabbaraju,Ambuj Tewari*

Main category: cs.CL

TL;DR: 论文研究了k-list语言识别的极限问题，提出了基于递归Angluin特征分解的识别条件，并证明其统计学习速率可达指数级。


<details>
  <summary>Details</summary>
Motivation: 经典语言识别理论存在局限性（Gold证明基本不可行，Angluin给出单列表识别条件），研究多列表(k-list)识别可扩展应用场景。

Method: 通过递归Angluin特征分解，证明k-list识别可行性等价于原集合可分解为k个可单列表识别的子集。统计场景采用i.i.d.采样验证指数速率。

Result: 1. k-list识别可行性⇨语言集合可分解为k个单列表可识别子集 2. 可行时存在指数级最优速率，不可行时任何速率均不收敛。

Conclusion: 多列表策略通过集合分解突破经典限制，统计场景下指数速率为理论边界，不可行集合无法通过调整速率实现收敛。

Abstract: We study the problem of language identification in the limit, where given a
sequence of examples from a target language, the goal of the learner is to
output a sequence of guesses for the target language such that all the guesses
beyond some finite time are correct. Classical results of Gold showed that
language identification in the limit is impossible for essentially any
interesting collection of languages. Later, Angluin gave a precise
characterization of language collections for which this task is possible.
Motivated by recent positive results for the related problem of language
generation, we revisit the classic language identification problem in the
setting where the learner is given the additional power of producing a list of
$k$ guesses at each time step. The goal is to ensure that beyond some finite
time, one of the guesses is correct at each time step.
  We give an exact characterization of collections of languages that can be
$k$-list identified in the limit, based on a recursive version of Angluin's
characterization (for language identification with a list of size $1$). This
further leads to a conceptually appealing characterization: A language
collection can be $k$-list identified in the limit if and only if the
collection can be decomposed into $k$ collections of languages, each of which
can be identified in the limit (with a list of size $1$). We also use our
characterization to establish rates for list identification in the statistical
setting where the input is drawn as an i.i.d. stream from a distribution
supported on some language in the collection. Our results show that if a
collection is $k$-list identifiable in the limit, then the collection can be
$k$-list identified at an exponential rate, and this is best possible. On the
other hand, if a collection is not $k$-list identifiable in the limit, then it
cannot be $k$-list identified at any rate that goes to zero.

</details>


### [19] [Batch Prompting Suppresses Overthinking Reasoning Under Constraint: How Batch Prompting Suppresses Overthinking in Reasoning Models](https://arxiv.org/abs/2511.04108)
*Wenmo Qiu,Saurabh Srivastava*

Main category: cs.CL

TL;DR: 批处理在大型语言模型推理中兼具效率优化与行为正则化效果，可减少3-5倍token消耗并提升决策可靠性


<details>
  <summary>Details</summary>
Motivation: 探索批处理技术除成本分摊外的潜在优势，揭示其对模型推理行为的正则化作用

Method: 通过13个基准测试开展全面研究，结合行为分析（抑制过度思考、减少模棱两可语言）和集体效应观察

Result: 准确率提升同时token使用减少3-5倍，模型产生更果断答案并展现跨示例的模式泛化能力

Conclusion: 批处理应视为推理时双重优化策略，既提升计算效率又增强模型推理可靠性，为LLM应用提供新方法论

Abstract: Recent work has explored batch prompting as a strategy to amortize inference
cost in large language models (LLMs). In this paper, we show that batching
offers an additional, underappreciated benefit: it regularizes model behavior
during multi-step reasoning for Large Reasoning Models (LRMs). We conduct a
comprehensive study across 13 diverse benchmarks and observe that batching
improves accuracy while substantially reducing reasoning token usage, often by
3x-5x. Through detailed behavioral analysis, we find that batching suppresses
overthinking, reduces hedging language (e.g., repetitive self-corrections), and
encourages more decisive answers. Surprisingly, we also observe emergent
collective effects in batched inference: models often generalize patterns from
earlier examples to solve harder ones in the same batch. These findings
position batching not just as a throughput optimization, but as a powerful
inference-time regularizer for more efficient and reliable LLM reasoning.

</details>


### [20] [RIDE: Difficulty Evolving Perturbation with Item Response Theory for Mathematical Reasoning](https://arxiv.org/abs/2511.04120)
*Xinyuan Li,Murong Xu,Wenbiao Tao,Hanlun Zhu,Yike Zhao,Jipeng Zhang,Yunshi Lan*

Main category: cs.CL

TL;DR: RIDE框架通过项目反应理论生成对抗性数学问题，系统评估大语言模型数学推理的稳健性，实验显示模型性能平均下降21.73%


<details>
  <summary>Details</summary>
Motivation: 现有基于规则扰动的方法容易生成病态问题，无法系统评估问题难度和推动基准测试的迭代升级

Method: 利用35个大语言模型模拟学生答题，构建难度排序器指导强化学习框架生成不同难度等级的数学问题变体

Result: 在竞赛级数学基准测试中，对抗性问题导致26个先进大语言模型平均性能下降21.73%

Conclusion: RIDE有效暴露大语言模型数学推理的脆弱性，验证了基于IRT的对抗评估方法的有效性

Abstract: Large language models (LLMs) achieve high performance on mathematical
reasoning, but these results can be inflated by training data leakage or
superficial pattern matching rather than genuine reasoning. To this end, an
adversarial perturbation-based evaluation is needed to measure true
mathematical reasoning ability. Current rule-based perturbation methods often
generate ill-posed questions and impede the systematic evaluation of question
difficulty and the evolution of benchmarks. To bridge this gap, we propose
RIDE, a novel adversarial question-rewriting framework that leverages Item
Response Theory (IRT) to rigorously measure question difficulty and to generate
intrinsically more challenging, well-posed variations of mathematical problems.
We employ 35 LLMs to simulate students and build a difficulty ranker from their
responses. This ranker provides a reward signal during reinforcement learning
and guides a question-rewriting model to reformulate existing questions across
difficulty levels. Applying RIDE to competition-level mathematical benchmarks
yields perturbed versions that degrade advanced LLM performance, with
experiments showing an average 21.73% drop across 26 models, thereby exposing
limited robustness in mathematical reasoning and confirming the validity of our
evaluation approach.

</details>


### [21] [CantoASR: Prosody-Aware ASR-LALM Collaboration for Low-Resource Cantonese](https://arxiv.org/abs/2511.04139)
*Dazhong Chen,Yi-Cheng Lin,Yuchen Huang,Ziwei Gong,Di Jiang,Zeying Xie,Yi R.,Fung*

Main category: cs.CL

TL;DR: 提出CantoASR框架，通过融合声学特征与大型音频语言模型推理，显著提升低资源粤语语音识别准确率


<details>
  <summary>Details</summary>
Motivation: 解决低资源粤语ASR面临的标注数据稀缺、六声调系统复杂、变调现象和口音差异等挑战

Method: 整合强制对齐声学特征提取、LoRA微调Whisper提升声调识别，指令调优Qwen-Audio实现韵律感知校正

Result: 在自发粤语数据集上取得相比Whisper-Large-V3显著的CER提升

Conclusion: 声学线索与LALM推理的整合为低资源声调语言和方言ASR提供了可扩展解决方案

Abstract: Automatic speech recognition (ASR) is critical for language accessibility,
yet low-resource Cantonese remains challenging due to limited annotated data,
six lexical tones, tone sandhi, and accent variation. Existing ASR models, such
as Whisper, often suffer from high word error rates. Large audio-language
models (LALMs), in contrast, can leverage broader contextual reasoning but
still require explicit tonal and prosodic acoustic cues. We introduce CantoASR,
a collaborative ASR-LALM error correction framework that integrates forced
alignment for acoustic feature extraction, a LoRA-finetuned Whisper for
improved tone discrimination, and an instruction-tuned Qwen-Audio for
prosody-aware correction. Evaluations on spontaneous Cantonese data show
substantial CER gains over Whisper-Large-V3. These findings suggest that
integrating acoustic cues with LALM reasoning provides a scalable strategy for
low-resource tonal and dialectal ASR.

</details>


### [22] [BAPPA: Benchmarking Agents, Plans, and Pipelines for Automated Text-to-SQL Generation](https://arxiv.org/abs/2511.04153)
*Fahim Ahmed,Md Mubtasim Ahasan,Jahir Sadik Monon,Muntasir Wahed,M Ashraful Amin,A K M Mahbubur Rahman,Amin Ahsan Ali*

Main category: cs.CL

TL;DR: 探讨三种多代理LLM流程提升自然语言转SQL性能，实验显示规划者-编码者流程效果最佳，小模型性能提升显著


<details>
  <summary>Details</summary>
Motivation: 现有LLM在处理复杂SQL生成时存在困难，需要更高效的解决方案

Method: 提出三种流程：1）多代理迭代讨论优化SQL 2）规划者生成步骤+编码者合成 3）多编码者竞争+智能体择优

Result: 多代理讨论使Qwen2.5-7B执行准确率提升10.6%，规划者-编码者流程使Gemma3准确率达56.4%

Conclusion: 多代理协作有效提升性能，规划者-编码者模式展现最优潜力，为轻量化模型应用提供新思路

Abstract: Text-to-SQL systems provide a natural language interface that can enable even
laymen to access information stored in databases. However, existing Large
Language Models (LLM) struggle with SQL generation from natural instructions
due to large schema sizes and complex reasoning. Prior work often focuses on
complex, somewhat impractical pipelines using flagship models, while smaller,
efficient models remain overlooked. In this work, we explore three multi-agent
LLM pipelines, with systematic performance benchmarking across a range of small
to large open-source models: (1) Multi-agent discussion pipeline, where agents
iteratively critique and refine SQL queries, and a judge synthesizes the final
answer; (2) Planner-Coder pipeline, where a thinking model planner generates
stepwise SQL generation plans and a coder synthesizes queries; and (3)
Coder-Aggregator pipeline, where multiple coders independently generate SQL
queries, and a reasoning agent selects the best query. Experiments on the
Bird-Bench Mini-Dev set reveal that Multi-Agent discussion can improve small
model performance, with up to 10.6% increase in Execution Accuracy for
Qwen2.5-7b-Instruct seen after three rounds of discussion. Among the pipelines,
the LLM Reasoner-Coder pipeline yields the best results, with DeepSeek-R1-32B
and QwQ-32B planners boosting Gemma 3 27B IT accuracy from 52.4% to the highest
score of 56.4%. Codes are available at
https://github.com/treeDweller98/bappa-sql.

</details>


### [23] [Trustworthy LLM-Mediated Communication: Evaluating Information Fidelity in LLM as a Communicator (LAAC) Framework in Multiple Application Domains](https://arxiv.org/abs/2511.04184)
*Mohammed Musthafa Rafi,Adarsh Krishnamurthy,Aditya Balu*

Main category: cs.CL

TL;DR: 论文提出LAAC框架，将LLM定位为可信沟通中介以解决当前AI生成内容导致的无效沟通循环，通过多维度可信度评估揭示部署前需解决的信任差距


<details>
  <summary>Details</summary>
Motivation: 当前AI生成内容导致发送方膨胀内容/接收方压缩摘要的无效沟通循环，亟需建立真实的知识交流范式

Method: 采用多智能体架构进行控制实验，系统评估信息捕捉保真度、响应可重复性、问答完整性三个可信维度

Result: 实验发现不同沟通场景存在可测量的信任差距，包括意图提取偏差(7.3%-15.6%)和响应不一致率(12.8%-19.4%)

Conclusion: LAAC框架虽能促进跨场景真实沟通，但需首先解决信息保真度衰减、响应不稳定性等关键信任问题才能部署于高风险场景

Abstract: The proliferation of AI-generated content has created an absurd communication
theater where senders use LLMs to inflate simple ideas into verbose content,
recipients use LLMs to compress them back into summaries, and as a consequence
neither party engage with authentic content. LAAC (LLM as a Communicator)
proposes a paradigm shift - positioning LLMs as intelligent communication
intermediaries that capture the sender's intent through structured dialogue and
facilitate genuine knowledge exchange with recipients. Rather than perpetuating
cycles of AI-generated inflation and compression, LAAC enables authentic
communication across diverse contexts including academic papers, proposals,
professional emails, and cross-platform content generation. However, deploying
LLMs as trusted communication intermediaries raises critical questions about
information fidelity, consistency, and reliability. This position paper
systematically evaluates the trustworthiness requirements for LAAC's deployment
across multiple communication domains. We investigate three fundamental
dimensions: (1) Information Capture Fidelity - accuracy of intent extraction
during sender interviews across different communication types, (2)
Reproducibility - consistency of structured knowledge across multiple
interaction instances, and (3) Query Response Integrity - reliability of
recipient-facing responses without hallucination, source conflation, or
fabrication. Through controlled experiments spanning multiple LAAC use cases,
we assess these trust dimensions using LAAC's multi-agent architecture.
Preliminary findings reveal measurable trust gaps that must be addressed before
LAAC can be reliably deployed in high-stakes communication scenarios.

</details>


### [24] [Computational Turing Test Reveals Systematic Differences Between Human and AI Language](https://arxiv.org/abs/2511.04195)
*Nicolò Pagan,Petter Törnberg,Christopher A. Bail,Anikó Hannák,Christopher Barrie*

Main category: cs.CL

TL;DR: 论文提出了一种计算图灵测试框架，结合多种指标评估大语言模型（LLM）生成文本的人类相似性，发现现有模型在校准后仍与人类文本存在显著差异，尤其在情感表达方面。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设LLM能生成类人文本用于社会科学模拟，但缺乏有效验证方法。传统基于人类判断的评估方法存在局限性，亟需系统性框架量化LLM生成文本的真实性并校准模型。

Method: 1）提出集成BERT检测性指标、语义相似度和可解释语言特征（风格标记、主题模式）的验证框架；2）系统比较9个开源LLM在五种校准策略（微调、风格提示、上下文检索等）下在社交媒体数据上的表现。

Result: 1）校准后LLM输出仍易与人类文本区分（情感语调差异显著）；2）指令微调模型表现弱于基础模型；3）模型规模与人类相似度无关；4）人类相似度与语义保真度存在权衡关系。

Conclusion: 研究为LLM验证提供了可扩展框架，揭示了当前模型在模拟人类沟通方面的局限性，强调需要更严谨的模型校准方法。

Abstract: Large language models (LLMs) are increasingly used in the social sciences to
simulate human behavior, based on the assumption that they can generate
realistic, human-like text. Yet this assumption remains largely untested.
Existing validation efforts rely heavily on human-judgment-based evaluations --
testing whether humans can distinguish AI from human output -- despite evidence
that such judgments are blunt and unreliable. As a result, the field lacks
robust tools for assessing the realism of LLM-generated text or for calibrating
models to real-world data. This paper makes two contributions. First, we
introduce a computational Turing test: a validation framework that integrates
aggregate metrics (BERT-based detectability and semantic similarity) with
interpretable linguistic features (stylistic markers and topical patterns) to
assess how closely LLMs approximate human language within a given dataset.
Second, we systematically compare nine open-weight LLMs across five calibration
strategies -- including fine-tuning, stylistic prompting, and context retrieval
-- benchmarking their ability to reproduce user interactions on X (formerly
Twitter), Bluesky, and Reddit. Our findings challenge core assumptions in the
literature. Even after calibration, LLM outputs remain clearly distinguishable
from human text, particularly in affective tone and emotional expression.
Instruction-tuned models underperform their base counterparts, and scaling up
model size does not enhance human-likeness. Crucially, we identify a trade-off:
optimizing for human-likeness often comes at the cost of semantic fidelity, and
vice versa. These results provide a much-needed scalable framework for
validation and calibration in LLM simulations -- and offer a cautionary note
about their current limitations in capturing human communication.

</details>


### [25] [LLM-as-a-Judge is Bad, Based on AI Attempting the Exam Qualifying for the Member of the Polish National Board of Appeal](https://arxiv.org/abs/2511.04205)
*Michał Karp,Anna Kubaszewska,Magdalena Król,Robert Król,Aleksander Smywiński-Pohl,Mateusz Szymański,Witold Wydmański*

Main category: cs.CL

TL;DR: 实证评估当前大语言模型能否通过波兰国家上诉法庭资格考试，发现模型在笔试环节未达标准且自动评估存在偏差。


<details>
  <summary>Details</summary>
Motivation: 验证LLMs在专业法律资格考试中的实际应用潜力，探索模型作为考生和自动评分者的可行性。

Method: 构建混合信息检索与生成框架，测试GPT-4.1等模型在闭卷考试和检索增强模式下的表现，并采用'LLM-as-a-judge'自动评分机制。

Result: 知识测试达标但书面判决未通过，模型存在法律条款引用错误、逻辑缺陷等问题，自动评分与官方评估存在显著差异。

Conclusion: 当前LLMs尚无法替代波兰公共采购裁决中的人类法官，需加强法律专家与技术团队的协作以解决模型局限性。

Abstract: This study provides an empirical assessment of whether current large language
models (LLMs) can pass the official qualifying examination for membership in
Poland's National Appeal Chamber (Krajowa Izba Odwo{\l}awcza). The authors
examine two related ideas: using LLM as actual exam candidates and applying the
'LLM-as-a-judge' approach, in which model-generated answers are automatically
evaluated by other models. The paper describes the structure of the exam, which
includes a multiple-choice knowledge test on public procurement law and a
written judgment, and presents the hybrid information recovery and extraction
pipeline built to support the models. Several LLMs (including GPT-4.1, Claude 4
Sonnet and Bielik-11B-v2.6) were tested in closed-book and various
Retrieval-Augmented Generation settings. The results show that although the
models achieved satisfactory scores in the knowledge test, none met the passing
threshold in the practical written part, and the evaluations of the
'LLM-as-a-judge' often diverged from the judgments of the official examining
committee. The authors highlight key limitations: susceptibility to
hallucinations, incorrect citation of legal provisions, weaknesses in logical
argumentation, and the need for close collaboration between legal experts and
technical teams. The findings indicate that, despite rapid technological
progress, current LLMs cannot yet replace human judges or independent examiners
in Polish public procurement adjudication.

</details>


### [26] [REMIND: Input Loss Landscapes Reveal Residual Memorization in Post-Unlearning LLMs](https://arxiv.org/abs/2511.04228)
*Liran Cohen,Yaniv Nemcovesky,Avi Mendelson*

Main category: cs.CL

TL;DR: REMIND提出通过分析模型在输入邻域内损失变化，检测未学习数据的残余记忆，比传统单点评估更敏感可靠。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘评估方法局限于单个样本层面，无法检测语义相似样本的残余影响，可能引发隐私泄露风险。

Method: 通过计算未学习数据在输入微小扰动下的损失分布特性（平坦度/陡峭度），构建邻域动态记忆模式分类器。

Result: REMIND在仅需查询访问条件下，准确率优于现有方法，且在模型/数据/改写输入中展现强鲁棒性。

Conclusion: 该方法为语言模型遗忘效果评估提供了敏感且可解释的框架，革新了记忆-遗忘动态关系的理解视角。

Abstract: Machine unlearning aims to remove the influence of specific training data
from a model without requiring full retraining. This capability is crucial for
ensuring privacy, safety, and regulatory compliance. Therefore, verifying
whether a model has truly forgotten target data is essential for maintaining
reliability and trustworthiness. However, existing evaluation methods often
assess forgetting at the level of individual inputs. This approach may overlook
residual influence present in semantically similar examples. Such influence can
compromise privacy and lead to indirect information leakage. We propose REMIND
(Residual Memorization In Neighborhood Dynamics), a novel evaluation method
aiming to detect the subtle remaining influence of unlearned data and classify
whether the data has been effectively forgotten. REMIND analyzes the model's
loss over small input variations and reveals patterns unnoticed by single-point
evaluations. We show that unlearned data yield flatter, less steep loss
landscapes, while retained or unrelated data exhibit sharper, more volatile
patterns. REMIND requires only query-based access, outperforms existing methods
under similar constraints, and demonstrates robustness across different models,
datasets, and paraphrased inputs, making it practical for real-world
deployment. By providing a more sensitive and interpretable measure of
unlearning effectiveness, REMIND provides a reliable framework to assess
unlearning in language models. As a result, REMIND offers a novel perspective
on memorization and unlearning.

</details>


### [27] [Reusing Pre-Training Data at Test Time is a Compute Multiplier](https://arxiv.org/abs/2511.04234)
*Alex Fang,Thomas Voice,Ruoming Pang,Ludwig Schmidt,Tom Gunter*

Main category: cs.CL

TL;DR: 现有预训练方法未充分利用数据集信息，检索增强技术可显著提升模型性能（MMLU提升5倍计算效益，LLaMA模型提升10%）


<details>
  <summary>Details</summary>
Motivation: 量化预训练过程中未被有效利用的数据价值，探究不同模型规模下数据利用效率的变化

Method: 使用检索增强生成（RAG）结合测试时计算，在MMLU、Math-500和SimpleQA基准上实验，采用LLaMA 3.1 8B等开源模型

Result: 检索增强使MMLU的计算效益提升约5倍，LLaMA模型在解析检索上下文后MMLU提升10个百分点，实验结果经去污染验证有效

Conclusion: 当前预训练方法存在显著的数据价值浪费，结合测试时检索和计算优化是未来重要发展方向

Abstract: Large language models learn from their vast pre-training corpora, gaining the
ability to solve an ever increasing variety of tasks; yet although researchers
work to improve these datasets, there is little effort to understand how
efficient the pre-training apparatus is at extracting ideas and knowledge from
the data. In this work, we use retrieval augmented generation along with
test-time compute as a way to quantify how much dataset value was left behind
by the process of pre-training, and how this changes across scale. We
demonstrate that pre-training then retrieving from standard and largely
open-sourced datasets results in significant accuracy gains in MMLU, Math-500,
and SimpleQA, which persist through decontamination. For MMLU we observe that
retrieval acts as a ~5x compute multiplier versus pre-training alone. We show
that these results can be further improved by leveraging additional compute at
test time to parse the retrieved context, demonstrating a 10 percentage point
improvement on MMLU for the public LLaMA 3.1 8B model. Overall, our results
suggest that today's pre-training methods do not make full use of the
information in existing pre-training datasets, leaving significant room for
progress.

</details>


### [28] [Efficient Topic Extraction via Graph-Based Labeling: A Lightweight Alternative to Deep Models](https://arxiv.org/abs/2511.04248)
*Salma Mekaoui,Hiba Sofyan,Imane Amaaz,Imane Benchrif,Arsalane Zarghili,Ilham Chaker,Nikola S. Nikolov*

Main category: cs.CL

TL;DR: 提出基于图结构的主题标签方法，在保持计算效率的同时提升主题建模结果的可解释性


<details>
  <summary>Details</summary>
Motivation: 传统主题建模生成的主题词分布缺乏可解释性，需要开发更高效且能生成人类可理解标签的方法

Method: 通过构建语义关系图扩展主题词，利用词汇间的图连接关系推导标签

Result: 在两个数据集上的实验显示，本方法在BERTScore和余弦相似度指标上优于传统基准，与ChatGPT-3.5效果相当且计算效率更高

Conclusion: 验证了图方法在主题标签任务中的有效性，未来将探索自动化与解释性增强的研究方向

Abstract: Extracting topics from text has become an essential task, especially with the
rapid growth of unstructured textual data. Most existing works rely on highly
computational methods to address this challenge. In this paper, we argue that
probabilistic and statistical approaches, such as topic modeling (TM), can
offer effective alternatives that require fewer computational resources. TM is
a statistical method that automatically discovers topics in large collections
of unlabeled text; however, it produces topics as distributions of
representative words, which often lack clear interpretability. Our objective is
to perform topic labeling by assigning meaningful labels to these sets of
words. To achieve this without relying on computationally expensive models, we
propose a graph-based approach that not only enriches topic words with
semantically related terms but also explores the relationships among them. By
analyzing these connections within the graph, we derive suitable labels that
accurately capture each topic's meaning. We present a comparative study between
our proposed method and several benchmarks, including ChatGPT-3.5, across two
different datasets. Our method achieved consistently better results than
traditional benchmarks in terms of BERTScore and cosine similarity and produced
results comparable to ChatGPT-3.5, while remaining computationally efficient.
Finally, we discuss future directions for topic labeling and highlight
potential research avenues for enhancing interpretability and automation.

</details>


### [29] [SSPO: Subsentence-level Policy Optimization](https://arxiv.org/abs/2511.04256)
*Kun Yang,Zikang chen,Yanmeng Wang,Zhigen Li*

Main category: cs.CL

TL;DR: 提出SSPO方法——通过句子级重要性比率平衡GRPO和GSPO，结合熵调整的剪裁机制，显著提升LLM推理性能


<details>
  <summary>Details</summary>
Motivation: GRPO的token级优化易受异常值影响导致训练崩溃；GSPO的响应级重要性比率导致数据利用率低下。需要平衡二者的优缺点

Method: 1. 采用句子级重要性比率计算
2. 在PPO-CLIP中应用句子熵动态调整剪裁边界
3. 高熵token扩大探索空间，低熵token收紧剪裁范围

Result: 在5个数据集平均得分46.57，超过GRPO(43.01)和GSPO(44.42)，并在3个数据集达到SOTA性能

Conclusion: SSPO有效结合GSPO优势同时规避其缺陷，通过分层优化机制显著提升生成数据的利用效率

Abstract: As a significant part of post-training of the Large Language Models (LLMs),
Reinforcement Learning from Verifiable Reward (RLVR) has greatly improved LLMs'
reasoning skills. However, some RLVR algorithms, such as GRPO (Group Relative
Policy Optimization) and GSPO (Group Sequence Policy Optimization), are
observed to suffer from unstable policy updates and low usage of sampling data,
respectively. The importance ratio of GRPO is calculated at the token level,
which focuses more on optimizing a single token. This will be easily affected
by outliers, leading to model training collapse. GSPO proposed the calculation
of the response level importance ratio, which solves the problem of high
variance and training noise accumulation in the calculation of the GRPO
importance ratio. However, since all the response tokens share a common
importance ratio, extreme values can easily raise or lower the overall mean,
leading to the entire response being mistakenly discarded, resulting in a
decrease in the utilization of sampled data. This paper introduces SSPO, which
applies sentence-level importance ratio, taking the balance between GRPO and
GSPO. SSPO not only avoids training collapse and high variance, but also
prevents the whole response tokens from being abandoned by the clipping
mechanism. Furthermore, we apply sentence entropy to PPO-CLIP to steadily
adjust the clipping bounds, encouraging high-entropy tokens to explore and
narrow the clipping range of low-entropy tokens. In particular, SSPO achieves
an average score of 46.57 across five datasets, surpassing GRPO (43.01) and
GSPO (44.42), and wins state-of-the-art performance on three datasets. These
results highlight SSPO's effectiveness in leveraging generated data by taking
the essence of GSPO but rejecting its shortcomings.

</details>


### [30] [Dynamic Jointly Batch Selection for Data Efficient Machine Translation Fine-Tuning](https://arxiv.org/abs/2511.04406)
*Mohammad Amin Ghanizadeh,Mohammad Javad Dousti*

Main category: cs.CL

TL;DR: 提出基于学习器模型与预训练参考模型协同的数据选择方法，使机器翻译微调数据效率提升5倍。


<details>
  <summary>Details</summary>
Motivation: 传统机器翻译模型训练中数据质量筛选效率低下，现有方法难以有效识别高质量训练样本，导致计算资源浪费。本文旨在通过智能数据选择提升微调效率。

Method: 1. 定义可学习性评分（learnability score）量化数据效用
2. 结合学习器模型与预训练参考模型的预测差异
3. 采用考虑数据点相互依赖关系的批量选择策略

Result: 在英语-波斯语等语言对的实验中：
- 数据效率相比基线提升5倍
- 缓存嵌入下计算效率提升24倍
- BLEU评分优于随机选择方法

Conclusion: 该方法通过动态评估数据效用和优化批量选择，显著提升翻译模型的数据效率与泛化能力，为低资源语言对机器翻译提供有效解决方案。

Abstract: Data quality and its effective selection are fundamental to improving the
performance of machine translation models, serving as cornerstones for
achieving robust and reliable translation systems. This paper presents a data
selection methodology specifically designed for fine-tuning machine translation
systems, which leverages the synergy between a learner model and a pre-trained
reference model to enhance overall training effectiveness. By defining a
learnability score, our approach systematically evaluates the utility of data
points for training, ensuring that only the most relevant and impactful
examples contribute to the fine-tuning process. Furthermore, our method employs
a batch selection strategy which considers interdependencies among data points,
optimizing the efficiency of the training process while maintaining a focus on
data relevance. Experiments on English to Persian and several other language
pairs using an mBART model fine-tuned on the CCMatrix dataset demonstrate that
our method can achieve up to a fivefold improvement in data efficiency compared
to an iid baseline. Experimental results indicate that our approach improves
computational efficiency by 24 when utilizing cached embeddings, as it requires
fewer training data points. Additionally, it enhances generalization, resulting
in superior translation performance compared to random selection method.

</details>


### [31] [If I Could Turn Back Time: Temporal Reframing as a Historical Reasoning Task for LLMs](https://arxiv.org/abs/2511.04432)
*Lars Bungum,Charles Yijia Huang,Abeer Kashar*

Main category: cs.CL

TL;DR: 研究发现使用英文提示的LLM时间推理效果优于挪威语，模型规模扩大能提升表现，测试了DeepSeek-R1/Gemma3/Qwen3/Llama3.1等模型家族


<details>
  <summary>Details</summary>
Motivation: 验证LLM在历史语境下的时间推理能力，探索语言选择对模型表现的影响

Method: 使用1940年挪威书籍的琐事问题，分别用英语/挪威语提问并评估，采用LLM-as-judge机制评分，人工抽样验证

Result: 英文提示准确率更高（与预期相反），更大规模模型表现更好，挪威语专用大模型未展现优势

Conclusion: 语言选择显著影响LLM表现，模型规模仍是关键因素，历史时间推理能力需针对性优化

Abstract: In this study, we experiment with the ability of LLMs to do temporal
reasoning. Using a Norwegian book from 1940 containing trivia questions, we
prompt the LLMs to answer the questions as if it were 1940. We also pose the
questions in both English and Norwegian. Correct answers are often presented as
sentences, and grading is done by means of LLM-as-judge, with sampled checks by
a native speaker. Prompting in English consistently gave better results than in
Norwegian, an unexpected result. In contrast, using larger LLMs improved
results. We tested the DeepSeek-R1, Gemma3, Qwen3, and Llama3.1 model families,
and also the largest available LLM especially crafted for Norwegian.

</details>


### [32] [Probabilistic Textual Time Series Depression Detection](https://arxiv.org/abs/2511.04476)
*Fabian Schmidt,Seyedehmoniba Ravan,Vladimir Vlassov*

Main category: cs.CL

TL;DR: 提出概率文本时间序列抑郁检测框架PTTSD，通过结合双向LSTM、自注意力机制和概率建模，在保持文本分析优势的同时实现不确定性量化的抑郁评分预测。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁检测模型缺乏不确定性估计和时间动态建模能力，难以满足临床决策对预测可靠性和时序演变分析的需求。

Method: 构建包含序列到序列/单输出的双分支框架，集成双向LSTM层、自注意力机制、残差连接，采用高斯分布/学生t分布的负对数似然训练输出头。

Result: 在E-DAIC和DAIC-WOZ数据集上达到文本系统最佳性能（MAE 3.85/3.55），预测区间校准良好，注意力机制和概率建模贡献显著。

Conclusion: PTTSD通过概率时间建模实现了可解释的抑郁预测，校准分析和临床案例验证了其不确定性量化的临床实用价值，为精神疾病评估提供了新范式。

Abstract: Accurate and interpretable predictions of depression severity are essential
for clinical decision support, yet existing models often lack uncertainty
estimates and temporal modeling. We propose PTTSD, a Probabilistic Textual Time
Series Depression Detection framework that predicts PHQ-8 scores from
utterance-level clinical interviews while modeling uncertainty over time. PTTSD
includes sequence-to-sequence and sequence-to-one variants, both combining
bidirectional LSTMs, self-attention, and residual connections with Gaussian or
Student-t output heads trained via negative log-likelihood. Evaluated on E-DAIC
and DAIC-WOZ, PTTSD achieves state-of-the-art performance among text-only
systems (e.g., MAE = 3.85 on E-DAIC, 3.55 on DAIC) and produces well-calibrated
prediction intervals. Ablations confirm the value of attention and
probabilistic modeling, while comparisons with MentalBERT establish generality.
A three-part calibration analysis and qualitative case studies further
highlight the interpretability and clinical relevance of uncertainty-aware
forecasting.

</details>


### [33] [ThaiOCRBench: A Task-Diverse Benchmark for Vision-Language Understanding in Thai](https://arxiv.org/abs/2511.04479)
*Surapon Nonesung,Teetouch Jaknamon,Sirinya Chaiophat,Natapong Nitarach,Chanakan Wittayasakpan,Warit Sirichotedumrong,Adisai Na-Thalang,Kunat Pipatanakul*

Main category: cs.CL

TL;DR: 首个泰语文本视觉理解综合基准ThaiOCRBench，评估VLMs在低资源复杂文字环境下的表现，揭示专有模型优势及开源模型挑战。


<details>
  <summary>Details</summary>
Motivation: 解决现有基准对泰语等低资源语言覆盖不足的问题，特别是在文档结构理解任务中缺乏代表性。

Method: 构建含2,808样本/13类任务的人类标注数据集，采用零样本评估多种先进专有及开源视觉语言模型。

Result: 专有模型(Gemini 2.5 Pro)显著优于开源模型，后者在细粒度识别和手写提取任务中表现最差；识别出语言偏见、结构错位和内容幻觉三大挑战。

Conclusion: ThaiOCRBench为低资源语言场景提供标准化评估框架，并为提升泰语文档理解能力提供可操作洞见。

Abstract: We present ThaiOCRBench, the first comprehensive benchmark for evaluating
vision-language models (VLMs) on Thai text-rich visual understanding tasks.
Despite recent progress in multimodal modeling, existing benchmarks
predominantly focus on high-resource languages, leaving Thai underrepresented,
especially in tasks requiring document structure understanding. ThaiOCRBench
addresses this gap by offering a diverse, human-annotated dataset comprising
2,808 samples across 13 task categories. We evaluate a wide range of
state-of-the-art VLMs in a zero-shot setting, spanning both proprietary and
open-source systems. Results show a significant performance gap, with
proprietary models (e.g., Gemini 2.5 Pro) outperforming open-source
counterparts. Notably, fine-grained text recognition and handwritten content
extraction exhibit the steepest performance drops among open-source models.
Through detailed error analysis, we identify key challenges such as language
bias, structural mismatch, and hallucinated content. ThaiOCRBench provides a
standardized framework for assessing VLMs in low-resource, script-complex
settings, and provides actionable insights for improving Thai-language document
understanding.

</details>


### [34] [RUST-BENCH: Benchmarking LLM Reasoning on Unstructured Text within Structured Tables](https://arxiv.org/abs/2511.04491)
*Nikhil Abhyankar,Purvi Chaurasia,Sanchit Kabra,Ananya Srivastava,Vivek Gupta,Chandan K. Reddy*

Main category: cs.CL

TL;DR: 提出RUST-BENCH基准测试集，包含7966个真实表格问题，挑战大语言模型在异构数据与多跳推理中的表现


<details>
  <summary>Details</summary>
Motivation: 现有表格推理基准测试局限于小型均匀表格，无法真实反映现实数据复杂性和LLMs的全面推理能力

Method: 构建跨科学（NSF资助记录）和体育（NBA数据）领域的2031个真实表格，覆盖规模、异构性、领域特性、推理复杂度四维度评估

Result: 实验表明LLMs在处理异构表结构（RB-Science达5.8列/表）和复杂推理（RB-Sports需3.1跳/问）时存在显著性能瓶颈

Conclusion: RUST-BENCH为表格推理研究建立新基准，揭示了当前模型架构与提示策略的局限性，推动领域发展

Abstract: Existing tabular reasoning benchmarks mostly test models on small, uniform
tables, underrepresenting the complexity of real-world data and giving an
incomplete view of Large Language Models' (LLMs) reasoning abilities. Real
tables are long, heterogeneous, and domain-specific, mixing structured fields
with free text and requiring multi-hop reasoning across thousands of tokens. To
address this gap, we introduce RUST-BENCH, a benchmark of 7966 questions from
2031 real-world tables spanning two domains: i) RB-Science (NSF grant records)
and ii) RB-Sports (NBA statistics). Unlike prior work, RUST-BENCH evaluates
LLMs jointly across scale, heterogeneity, domain specificity, and reasoning
complexity. Experiments with open-source and proprietary models show that LLMs
struggle with heterogeneous schemas and complex multi-hop inference, revealing
persistent weaknesses in current architectures and prompting strategies.
RUST-BENCH establishes a challenging new testbed for advancing tabular
reasoning research.

</details>


### [35] [OUNLP at TSAR 2025 Shared Task: Multi-Round Text Simplifier via Code Generation](https://arxiv.org/abs/2511.04495)
*Cuong Huynh,Jie Cao*

Main category: cs.CL

TL;DR: OUNLP系统在TSAR-2025比赛中提出基于LLM提示的多轮文本简化方法，发现CEFR等级差异影响简化效果，开发了MRS-Rule和MRS-Joint方法，最终排名第7。后续改进表明以LLM简化文本为起点能提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究发现文本简化性能与源文本和目标文本的CEFR等级差异高度相关，为处理不同复杂度差距，提出多轮迭代简化方法。

Method: 提出基于规则的多轮简化方法(MRS-Rule)和规则与LLM联合简化方法(MRS-Joint)，通过GPT-4o生成，后者将LLM简化结果作为下一轮输入。

Result: 提交系统在20个团队中排名第7，后续改进显示MRS-Joint方法以LLM简化候选为起点可进一步提升多轮简化性能。

Conclusion: 多轮简化方法结合LLM输出和CEFR等级差异分析能有效提升文本简化效果，为可控文本简化提供了新思路。

Abstract: This paper describes the OUNLP system submitted to the TSAR-2025 Shared Task
(Alva-Manchego et al., 2025), designed for readability-controlled text
simplification using LLM-prompting-based generation. Based on the analysis of
prompt-based text simplification methods, we discovered an interesting finding
that text simplification performance is highly related to the gap between the
source CEFR (Arase et al., 2022) level and the target CEFR level. Inspired by
this finding, we propose two multi-round simplification methods and generate
them via GPT-4o: rule-based simplification (MRS-Rule) and jointly rule-based
LLM simplification (MRS-Joint). Our submitted systems ranked 7 out of 20 teams.
Later improvements with MRS-Joint show that taking the LLM simplified
candidates as the starting point could further boost the multi-round
simplification performance.

</details>


### [36] [Decoding Emergent Big Five Traits in Large Language Models: Temperature-Dependent Expression and Architectural Clustering](https://arxiv.org/abs/2511.04499)
*Christos-Nikolaos Zacharopoulos,Revekka Kyriakoglou*

Main category: cs.CL

TL;DR: 通过Big Five框架评估6个LLM的人格特征，发现温度参数显著影响神经质和外向性表现，模型架构决定稳定特质分布，为AI伦理治理提供新视角。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在人类中心应用中的普及，理解其类人格特征对负责任的AI开发和治理至关重要。

Method: 使用Big Five Inventory-2框架，在不同采样温度下系统评估六个LLM的人格维度表现，并进行层次聚类分析。

Result: 四个维度存在显著差异（开放性除外），神经质和外向性受温度影响最大；模型架构形成稳定特质集群。

Conclusion: 揭示了LLM人格特征的形成机制，为模型调优、选择及AI伦理治理提供了实证依据和新的方法论视角。

Abstract: As Large Language Models (LLMs) become integral to human-centered
applications, understanding their personality-like behaviors is increasingly
important for responsible development and deployment. This paper systematically
evaluates six LLMs, applying the Big Five Inventory-2 (BFI-2) framework, to
assess trait expressions under varying sampling temperatures. We find
significant differences across four of the five personality dimensions, with
Neuroticism and Extraversion susceptible to temperature adjustments. Further,
hierarchical clustering reveals distinct model clusters, suggesting that
architectural features may predispose certain models toward stable trait
profiles. Taken together, these results offer new insights into the emergence
of personality-like patterns in LLMs and provide a new perspective on model
tuning, selection, and the ethical governance of AI systems. We share the data
and code for this analysis here:
https://osf.io/bsvzc/?view_only=6672219bede24b4e875097426dc3fac1

</details>


### [37] [RAGalyst: Automated Human-Aligned Agentic Evaluation for Domain-Specific RAG](https://arxiv.org/abs/2511.04502)
*Joshua Gao,Quoc Huy Pham,Subin Varghese,Silwal Saurav,Vedhus Hoskere*

Main category: cs.CL

TL;DR: 提出了RAGalyst自动化评估框架，通过合成数据集和指标优化实现领域定制化RAG系统评估，发现性能高度依赖应用场景且无通用最优解。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估方法在专业安全领域存在局限：启发式指标难以捕捉领域特性，LLM评估缺乏人类判断验证，需要开发可靠的领域敏感评估框架。

Method: 构建代理式自动评估框架，包含：1) 生成经过过滤的高质量合成QA数据集 2) 通过提示优化改进答案正确性(82%人类对齐度)和可答性指标 3) 在军事、网络安全和桥梁工程三领域验证多组件组合效果

Result: 跨领域实验表明：嵌入模型、LLM选择及参数配置均无通用最优解（不同领域top模型重合度<35%），识别出知识缺失(41%)和检索失败(29%)为主要错误来源

Conclusion: RAGalyst填补了领域定制化评估的方法空白，其系统化分析能力可支持安全关键领域RAG系统的可靠构建与优化决策。

Abstract: Retrieval-Augmented Generation (RAG) is a critical technique for grounding
Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in
specialized, safety-critical domains remains a significant challenge. Existing
evaluation frameworks often rely on heuristic-based metrics that fail to
capture domain-specific nuances and other works utilize LLM-as-a-Judge
approaches that lack validated alignment with human judgment. This paper
introduces RAGalyst, an automated, human-aligned agentic framework designed for
the rigorous evaluation of domain-specific RAG systems. RAGalyst features an
agentic pipeline that generates high-quality, synthetic question-answering (QA)
datasets from source documents, incorporating an agentic filtering step to
ensure data fidelity. The framework refines two key LLM-as-a-Judge
metrics-Answer Correctness and Answerability-using prompt optimization to
achieve a strong correlation with human annotations. Applying this framework to
evaluate various RAG components across three distinct domains (military
operations, cybersecurity, and bridge engineering), we find that performance is
highly context-dependent. No single embedding model, LLM, or hyperparameter
configuration proves universally optimal. Additionally, we provide an analysis
on the most common low Answer Correctness reasons in RAG. These findings
highlight the necessity of a systematic evaluation framework like RAGalyst,
which empowers practitioners to uncover domain-specific trade-offs and make
informed design choices for building reliable and effective RAG systems.
RAGalyst is available on our Github.

</details>


### [38] [Modeling Clinical Uncertainty in Radiology Reports: from Explicit Uncertainty Markers to Implicit Reasoning Pathways](https://arxiv.org/abs/2511.04506)
*Paloma Rabaey,Jong Hak Moon,Jung-Oh Lee,Min Gwan Kim,Hangyul Yoon,Thomas Demeester,Edward Choi*

Main category: cs.CL

TL;DR: 论文提出量化放射学报告中显性不确定性和隐性不确定性的框架，并发布增强版Lunguage++基准支持临床决策


<details>
  <summary>Details</summary>
Motivation: 放射学报告存在显性（模糊表述）和隐性（信息缺失）两种不确定性，现有规则系统无法有效量化，影响自动化分析与临床决策

Method: 1. 基于LLM创建专家验证的模糊短语概率映射；2. 通过专家定义诊断路径生成14种常见诊断的特征性子发现扩展框架

Result: 发布Lunguage++增强基准，支持不确定性感知的图像分类、诊断推理，并开辟临床不确定性影响研究新方向

Conclusion: 该框架首次系统量化放射学诊断不确定性，通过结构化扩展提升自动化报告的临床可信度与应用潜力

Abstract: Radiology reports are invaluable for clinical decision-making and hold great
potential for automated analysis when structured into machine-readable formats.
These reports often contain uncertainty, which we categorize into two distinct
types: (i) Explicit uncertainty reflects doubt about the presence or absence of
findings, conveyed through hedging phrases. These vary in meaning depending on
the context, making rule-based systems insufficient to quantify the level of
uncertainty for specific findings; (ii) Implicit uncertainty arises when
radiologists omit parts of their reasoning, recording only key findings or
diagnoses. Here, it is often unclear whether omitted findings are truly absent
or simply unmentioned for brevity. We address these challenges with a two-part
framework. We quantify explicit uncertainty by creating an expert-validated,
LLM-based reference ranking of common hedging phrases, and mapping each finding
to a probability value based on this reference. In addition, we model implicit
uncertainty through an expansion framework that systematically adds
characteristic sub-findings derived from expert-defined diagnostic pathways for
14 common diagnoses. Using these methods, we release Lunguage++, an expanded,
uncertainty-aware version of the Lunguage benchmark of fine-grained structured
radiology reports. This enriched resource enables uncertainty-aware image
classification, faithful diagnostic reasoning, and new investigations into the
clinical impact of diagnostic uncertainty.

</details>


### [39] [Are language models aware of the road not taken? Token-level uncertainty and hidden state dynamics](https://arxiv.org/abs/2511.04527)
*Amir Zur,Atticus Geiger,Ekdeep Singh Lubana,Eric Bigelow*

Main category: cs.CL

TL;DR: 通过控制语言模型的隐藏激活预测推理路径不确定性，发现模型在未确定最终答案时干预最有效


<details>
  <summary>Details</summary>
Motivation: 量化语言模型生成文本时不同推理路径的不确定性

Method: 使用隐藏激活干预预测模型在链式推理中的不确定性分布

Result: 模型不确定性与其可操纵性正相关，隐藏激活可预测未来结果分布

Conclusion: 语言模型隐式表示多种可能路径，激活控制在决策未固化时效果最佳

Abstract: When a language model generates text, the selection of individual tokens
might lead it down very different reasoning paths, making uncertainty difficult
to quantify. In this work, we consider whether reasoning language models
represent the alternate paths that they could take during generation. To test
this hypothesis, we use hidden activations to control and predict a language
model's uncertainty during chain-of-thought reasoning. In our experiments, we
find a clear correlation between how uncertain a model is at different tokens,
and how easily the model can be steered by controlling its activations. This
suggests that activation interventions are most effective when there are
alternate paths available to the model -- in other words, when it has not yet
committed to a particular final answer. We also find that hidden activations
can predict a model's future outcome distribution, demonstrating that models
implicitly represent the space of possible paths.

</details>


### [40] [IntelliProof: An Argumentation Network-based Conversational Helper for Organized Reflection](https://arxiv.org/abs/2511.04528)
*Kaveh Eskandari Miandoab,Katharine Kowalyshyn,Kabir Pamnani,Anesu Gavhera,Vasanth Sarathy,Matthias Scheutz*

Main category: cs.CL

TL;DR: 开发了IntelliProof交互式议论文分析系统，通过论证图可视化和LLM量化评估提升文本结构分析效率


<details>
  <summary>Details</summary>
Motivation: 现有自动评分系统缺乏对论证结构的可视化解释，难以让用户理解论证逻辑的内在关联

Method: 将文章建模为论证图（节点=论点，边=支持/反对关系），结合LLM自动分类评分与交互式可视化界面

Result: 实现论证关系的实时可视化分析，生成文本连贯性量化指标，同时保留人工审核的干预机制

Conclusion: 系统有效融合自动化分析与人工监管，通过结构化呈现帮助用户深入理解论证逻辑与文本质量

Abstract: We present IntelliProof, an interactive system for analyzing argumentative
essays through LLMs. IntelliProof structures an essay as an argumentation
graph, where claims are represented as nodes, supporting evidence is attached
as node properties, and edges encode supporting or attacking relations. Unlike
existing automated essay scoring systems, IntelliProof emphasizes the user
experience: each relation is initially classified and scored by an LLM, then
visualized for enhanced understanding. The system provides justifications for
classifications and produces quantitative measures for essay coherence. It
enables rapid exploration of argumentative quality while retaining human
oversight. In addition, IntelliProof provides a set of tools for a better
understanding of an argumentative essay and its corresponding graph in natural
language, bridging the gap between the structural semantics of argumentative
essays and the user's understanding of a given text. A live demo and the system
are available here to try: \textbf{https://intelliproof.vercel.app}

</details>


### [41] [From Model to Breach: Towards Actionable LLM-Generated Vulnerabilities Reporting](https://arxiv.org/abs/2511.04538)
*Cyril Vallez,Alexander Sternfeld,Andrei Kucharavy,Ljiljana Dolamic*

Main category: cs.CL

TL;DR: 研究发现最新开源LLM编码助手仍存在早期已知漏洞，安全性和功能性权衡导致漏洞修补不足。论文提出结合漏洞严重性、生成概率和提示暴露度的PE指标，并建立ME评分体系指导漏洞修复优先级。


<details>
  <summary>Details</summary>
Motivation: 针对LLM编码助手生成代码漏洞对网络安全的影响，现有安全基准和方法未能有效提升主流编码LLM的安全性，需要建立更科学的漏洞评估体系。

Method: 提出Prompt Exposure(PE)指标，综合评估漏洞严重程度（CVSS）、生成概率和诱导漏洞的提示特征。基于PE构建Model Exposure(ME)评分，量化模型生成漏洞的严重性和普遍性。

Result: 当前开放权重模型在真实场景中仍受早期漏洞影响，验证安全-功能权衡导致修补困难。PE/ME指标能有效识别需优先修复的高风险漏洞（如CWE-78命令注入漏洞出现概率达32%）。

Conclusion: 通过PE/ME指标系统，为LLM编码助手安全优化提供量化依据，应优先缓解高暴露度漏洞，在保持功能性的同时系统性提升生成代码安全性。

Abstract: As the role of Large Language Models (LLM)-based coding assistants in
software development becomes more critical, so does the role of the bugs they
generate in the overall cybersecurity landscape. While a number of LLM code
security benchmarks have been proposed alongside approaches to improve the
security of generated code, it remains unclear to what extent they have
impacted widely used coding LLMs. Here, we show that even the latest
open-weight models are vulnerable in the earliest reported vulnerability
scenarios in a realistic use setting, suggesting that the safety-functionality
trade-off has until now prevented effective patching of vulnerabilities. To
help address this issue, we introduce a new severity metric that reflects the
risk posed by an LLM-generated vulnerability, accounting for vulnerability
severity, generation chance, and the formulation of the prompt that induces
vulnerable code generation - Prompt Exposure (PE). To encourage the mitigation
of the most serious and prevalent vulnerabilities, we use PE to define the
Model Exposure (ME) score, which indicates the severity and prevalence of
vulnerabilities a model generates.

</details>


### [42] [BanglaMedQA and BanglaMMedBench: Evaluating Retrieval-Augmented Generation Strategies for Bangla Biomedical Question Answering](https://arxiv.org/abs/2511.04560)
*Sadia Sultana,Saiyma Sittul Muna,Mosammat Zannatul Samarukh,Ajwad Abrar,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: 首个孟加拉语生物医学问答数据集BanglaMedQA/BanglaMMedBench的构建，结合OCR教材与动态代理RAG策略，使GPT-OSS-120B模型达到89.54%准确率


<details>
  <summary>Details</summary>
Motivation: 低资源语言（如孟加拉语）生物医学QA系统存在准确率不足的问题，限制了可靠医疗知识的普及

Method: 1. 构建教科书OCR语料库 2. 设计传统/零样本回退/代理/迭代反馈/聚合五种RAG策略 3. 实现动态检索与推理策略选择的Agentic RAG框架

Result: Agentic RAG在GPT-OSS-120B模型上达到89.54%准确率，较其他策略提升显著且理论依据质量最优

Conclusion: 验证了RAG方法对提升低资源语言医疗QA可靠性的有效性，为多语言医学AI研究建立基准

Abstract: Developing accurate biomedical Question Answering (QA) systems in
low-resource languages remains a major challenge, limiting equitable access to
reliable medical knowledge. This paper introduces BanglaMedQA and
BanglaMMedBench, the first large-scale Bangla biomedical Multiple Choice
Question (MCQ) datasets designed to evaluate reasoning and retrieval in medical
artificial intelligence (AI). The study applies and benchmarks several
Retrieval-Augmented Generation (RAG) strategies, including Traditional,
Zero-Shot Fallback, Agentic, Iterative Feedback, and Aggregate RAG, combining
textbook-based and web retrieval with generative reasoning to improve factual
accuracy. A key novelty lies in integrating a Bangla medical textbook corpus
through Optical Character Recognition (OCR) and implementing an Agentic RAG
pipeline that dynamically selects between retrieval and reasoning strategies.
Experimental results show that the Agentic RAG achieved the highest accuracy
89.54% with openai/gpt-oss-120b, outperforming other configurations and
demonstrating superior rationale quality. These findings highlight the
potential of RAG-based methods to enhance the reliability and accessibility of
Bangla medical QA, establishing a foundation for future research in
multilingual medical artificial intelligence.

</details>


### [43] [When retrieval outperforms generation: Dense evidence retrieval for scalable fake news detection](https://arxiv.org/abs/2511.04643)
*Alamgir Munir Qazi,John P. McCrae,Jamal Abdul Nasir*

Main category: cs.CL

TL;DR: 提出DeReC框架，通过密集检索+分类的轻量级方案替代LLM生成解释范式，在保持更高准确率(RAWFC数据集F1 65.58% vs 61.20%)的同时显著提升效率(RAWFC运行时间减少95%)


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的事实核查方法存在两大瓶颈：1) 高计算成本(单次推理需数十分钟) 2) 生成解释存在幻觉风险。需要开发更高效可靠的替代方案

Method: 1. 使用通用文本嵌入替代自回归式LLM生成 2. 创新结合密集检索(证据获取)与专用分类器(事实判定) 3. 端到端优化检索与分类组件的协同

Result: 效率提升：RAWFC(23m36s vs 454m12s)、LIAR-RAW(134m14s vs 1692m23s)；准确率：RAWFC数据集F1达65.58%超越SOTA方法L-Defense(61.20%)

Conclusion: 精心设计的检索系统在专业任务中可超越LLM性能，且具备实际部署优势。该研究为高效事实核查系统开发提供了新范式，证明非生成式方法在特定任务中的潜力

Abstract: The proliferation of misinformation necessitates robust yet computationally
efficient fact verification systems. While current state-of-the-art approaches
leverage Large Language Models (LLMs) for generating explanatory rationales,
these methods face significant computational barriers and hallucination risks
in real-world deployments. We present DeReC (Dense Retrieval Classification), a
lightweight framework that demonstrates how general-purpose text embeddings can
effectively replace autoregressive LLM-based approaches in fact verification
tasks. By combining dense retrieval with specialized classification, our system
achieves better accuracy while being significantly more efficient. DeReC
outperforms explanation-generating LLMs in efficiency, reducing runtime by 95%
on RAWFC (23 minutes 36 seconds compared to 454 minutes 12 seconds) and by 92%
on LIAR-RAW (134 minutes 14 seconds compared to 1692 minutes 23 seconds),
showcasing its effectiveness across varying dataset sizes. On the RAWFC
dataset, DeReC achieves an F1 score of 65.58%, surpassing the state-of-the-art
method L-Defense (61.20%). Our results demonstrate that carefully engineered
retrieval-based systems can match or exceed LLM performance in specialized
tasks while being significantly more practical for real-world deployment.

</details>


### [44] [Logit-Entropy Adaptive Stopping Heuristic for Efficient Chain-of-Thought Reasoning](https://arxiv.org/abs/2511.04654)
*Mohammad Atif Quamar,Mohammad Areeb*

Main category: cs.CL

TL;DR: 提出LEASH算法，通过监控熵和logit变化自适应停止推理生成，减少35% token消耗并保持90%准确率


<details>
  <summary>Details</summary>
Motivation: 传统CoT提示需要生成完整固定长度推理链，导致计算资源浪费和延迟增加

Method: 同时监控token级熵的斜率变化和top-logit边际改善，当两者趋于稳定时终止生成

Result: 在GSM8K和AQuA-RAT基准上，平均减少35% token生成和27%延迟，准确率下降10个百分点

Conclusion: LEASH提供无需训练的高效解码方案，在推理效率与准确性间取得平衡

Abstract: Chain-of-Thought (CoT) prompting is a key technique for enabling complex
reasoning in large language models. However, generating full, fixed-length
rationales is computationally wasteful, inflating both token usage and latency.
We introduce LEASH: Logit-Entropy Adaptive Stopping Heuristic, a training-free
decoding algorithm that adaptively halts rationale generation. LEASH monitors
two intrinsic signals: the slope of token-level entropy and the improvement in
the top-logit margin. It terminates the generation once both signals plateau,
indicating the model has reached a stable reasoning state. Across four
instruction-tuned models on the GSM8K and AQuA-RAT benchmarks, LEASH reduces
average token generation by 30--35% and latency by 27%, while incurring a 10
p.p. accuracy drop relative to CoT. LEASH is model-agnostic and requires no
additional training or supervision, offering a simple and efficient alternative
to CoT decoding.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [45] [Shellular Metamaterial Design via Compact Electric Potential Parametrization](https://arxiv.org/abs/2511.04025)
*Chang Liu,Bohan Wang*

Main category: cs.GR

TL;DR: 提出紧凑高效的壳层超材料设计空间，结合快速GPU均质化流程，实现高性能结构设计及验证。


<details>
  <summary>Details</summary>
Motivation: 解决传统壳层超材料设计复杂度高、计算效率低的问题，通过紧凑设计空间和GPU加速实现高效探索与逆向设计。

Method: 构建覆盖平面结构到三周期曲面的紧凑设计空间，开发GPU并行化均质化流程（20ms评估+0.5秒弹性张量计算）。

Result: 获得91.86%理论性能上限的机械响应，覆盖宽泛材料属性，性能匹配顶尖低体积壳层结构。

Conclusion: 通过增材制造验证可行性，证明该设计方法在工程应用中的实用价值与创新性。

Abstract: We introduce a compact yet highly expressive design space for shellular
metamaterials. By employing only a few dozen degrees of freedom, this design
space represents geometries ranging from simple planar configurations to
complex triply periodic minimal surfaces. Coupled with this representation, we
develop an efficient GPU-based homogenization pipeline that evaluates the
structure in under 20 ms and computes the corresponding effective elastic
tensor in near-real-time (0.5 s). The high speed of this evaluation facilitates
an exhaustive exploration of the design space and supports an inverse-design
scheme that tailors the shellular structure to specific macroscopic target
property. Structures derived through this approach exhibit not only geometric
diversity but also a wide spectrum of mechanical responses, covering a broad
range of material properties. Moreover, they achieve up to 91.86% of
theoretical upper bounds, a level of performance comparable to state-of-the-art
shellular structures with low solid volume. Finally, our prototypes, fabricated
via additive manufacturing, confirm the practical manufacturability of these
designs, underscoring their potential for real-world engineering applications.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [46] [MimiTalk: Revolutionizing Qualitative Research with Dual-Agent AI](https://arxiv.org/abs/2511.03731)
*Fengming Liu,Shubin Yu*

Main category: cs.HC

TL;DR: MimiTalk双代理宪法AI框架通过三项研究验证其能有效降低访谈焦虑、保持对话连贯性，在信息丰富度与稳定性上优于人类访谈，促进可复现的定性研究


<details>
  <summary>Details</summary>
Motivation: 解决传统访谈方法在可扩展性、伦理规范和质量控制方面的不足，开发结合战略监督与对话生成的双代理AI系统，探索人机协作的优化路径

Method: 分三阶段研究：20人可用性测试；121个AI访谈与1,271个人类访谈的NLP指标对比及倾向评分匹配；10位跨学科研究者的盲法主题分析

Result: AI访谈在技术细节(β=+0.32,p<0.01)和敏感话题坦诚度(提升41%)表现更优，人类访谈在文化共鸣(Δ=+22%)和情感捕捉(F1=0.78)占优势，双代理架构使对话连贯性提升35%

Conclusion: 双代理宪法AI通过人机分工协作，实现标准化流程(响应延迟<2.3s)与个性表达的平衡，为大规模定性研究提供可重复、可审计的解决方案

Abstract: We present MimiTalk, a dual-agent constitutional AI framework designed for
scalable and ethical conversational data collection in social science research.
The framework integrates a supervisor model for strategic oversight and a
conversational model for question generation. We conducted three studies: Study
1 evaluated usability with 20 participants; Study 2 compared 121 AI interviews
to 1,271 human interviews from the MediaSum dataset using NLP metrics and
propensity score matching; Study 3 involved 10 interdisciplinary researchers
conducting both human and AI interviews, followed by blind thematic analysis.
Results across studies indicate that MimiTalk reduces interview anxiety,
maintains conversational coherence, and outperforms human interviews in
information richness, coherence, and stability. AI interviews elicit technical
insights and candid views on sensitive topics, while human interviews better
capture cultural and emotional nuances. These findings suggest that dual-agent
constitutional AI supports effective human-AI collaboration, enabling
replicable, scalable and quality-controlled qualitative research.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [47] [Black-Box Guardrail Reverse-engineering Attack](https://arxiv.org/abs/2511.04215)
*Hongwei Yao,Yun Xia,Shuo Shao,Haoran Shi,Tong Qiao,Cong Wang*

Main category: cs.CR

TL;DR: 提出基于强化学习的GRA攻击框架，成功破解主流商业LLM防护栏，规则匹配率超92%且成本低于85美元


<details>
  <summary>Details</summary>
Motivation: 现有LLM防护栏虽能过滤有害输出，但其可观测的决策模式暴露了新的攻击面

Method: 结合遗传算法进行数据增强，通过输入-输出对迭代优化，采用定向突变和交叉策略构建高精度防护栏代理模型

Result: 在ChatGPT/DeepSeek/Qwen3上验证，规则匹配率达0.92+，攻击成本<$85

Conclusion: 当前LLM安全机制存在重大设计缺陷，亟需开发更鲁棒的动态防御体系

Abstract: Large language models (LLMs) increasingly employ guardrails to enforce
ethical, legal, and application-specific constraints on their outputs. While
effective at mitigating harmful responses, these guardrails introduce a new
class of vulnerabilities by exposing observable decision patterns. In this
work, we present the first study of black-box LLM guardrail reverse-engineering
attacks. We propose Guardrail Reverse-engineering Attack (GRA), a reinforcement
learning-based framework that leverages genetic algorithm-driven data
augmentation to approximate the decision-making policy of victim guardrails. By
iteratively collecting input-output pairs, prioritizing divergence cases, and
applying targeted mutations and crossovers, our method incrementally converges
toward a high-fidelity surrogate of the victim guardrail. We evaluate GRA on
three widely deployed commercial systems, namely ChatGPT, DeepSeek, and Qwen3,
and demonstrate that it achieves an rule matching rate exceeding 0.92 while
requiring less than $85 in API costs. These findings underscore the practical
feasibility of guardrail extraction and highlight significant security risks
for current LLM safety mechanisms. Our findings expose critical vulnerabilities
in current guardrail designs and highlight the urgent need for more robust
defense mechanisms in LLM deployment.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [48] [Transforming Mentorship: An AI Powered Chatbot Approach to University Guidance](https://arxiv.org/abs/2511.04172)
*Mashrur Rahman,Mantaqa abedin,Monowar Zamil Abir,Faizul Islam Ansari,Adib Reza,Farig Yousuf Sadeque,Niloy Farhan*

Main category: cs.IR

TL;DR: 开发基于LLaMA-3.3-70B的智能聊天机器人导师，通过混合检索技术（BM25+ChromaDB）实现个性化校园咨询，数据更新效率提升3.45倍。


<details>
  <summary>Details</summary>
Motivation: 解决大学新生缺乏个性化指导的问题，现有数字工具无法满足大规模定制化需求，特别是学分制大学学生面临的选课规划难题。

Method: 构建数据管道整合CSV和网页数据源，采用BM25词法排序与ChromaDB语义检索的混合检索机制，配合LLaMA大模型生成对话响应。

Result: 系统响应语义相关性优异（BERTScore 0.831，METEOR 0.809），数据更新耗时仅106.82秒（全量处理368.62秒）。

Conclusion: 该智能助手能有效解答学生咨询，帮助规划学分制下的学习生活，提升校园适应能力与时间管理效率。

Abstract: University students face immense challenges during their undergraduate lives,
often being deprived of personalized on-demand guidance that mentors fail to
provide at scale. Digital tools exist, but there is a serious lack of
customized coaching for newcomers. This paper presents an AI-powered chatbot
that will serve as a mentor for the students of BRAC University. The main
component is a data ingestion pipeline that efficiently processes and updates
information from diverse sources, such as CSV files and university webpages.
The chatbot retrieves information through a hybrid approach, combining BM25
lexical ranking with ChromaDB semantic retrieval, and uses a Large Language
Model, LLaMA-3.3-70B, to generate conversational responses. The generated text
was found to be semantically highly relevant, with a BERTScore of 0.831 and a
METEOR score of 0.809. The data pipeline was also very efficient, taking 106.82
seconds for updates, compared to 368.62 seconds for new data. This chatbot will
be able to help students by responding to their queries, helping them to get a
better understanding of university life, and assisting them to plan better
routines for their semester in the open-credit university.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [49] [Sub-exponential Growth in Online Word Usage: A Piecewise Power-Law Model](https://arxiv.org/abs/2511.04106)
*Hayafumi Watanabe*

Main category: physics.soc-ph

TL;DR: 提出分段幂律模型揭示社会扩散中普遍存在亚指数增长模式（α≈0.5），扩散规模主要由增长率决定，α反映话题性质与传播偏好


<details>
  <summary>Details</summary>
Motivation: 传统S型模型无法充分解释社会扩散现象，特别是亚指数增长模式在非流行病领域的系统性研究缺失

Method: 基于10亿日语博客数据及多语种搜索趋势，采用分段幂律模型分析2,965个条目，建立微观行为模型解释α参数意义

Result: 55%条目符合简单分段模型；单分段α众数0.5；扩散规模R主导（贡献度90%）；α与话题广度负相关（小众α小）

Conclusion: 亚指数增长是社会扩散的普遍规律，模型为复杂增长曲线提供统一分析框架，α参数揭示传播行为模式差异

Abstract: The diffusion of ideas and language in society has conventionally been
described by S-shaped models, such as the logistic curve. However, the role of
sub-exponential growth -a slower than exponential pattern known in
epidemiology- has been largely overlooked in broader social phenomena. Here, we
present a piecewise power-law model to characterize complex growth curves with
a few parameters. We systematically analyzed a large-scale dataset of
approximately one billion Japanese blog articles linked to Wikipedia
vocabulary, and observed consistent patterns in web search trend data (English,
Spanish, and Japanese). Our analysis of the 2,965 selected items reveals that
about 55% (1,625 items) were found to have no abrupt jumps and were well
captured by one or two segments. For single-segment curves, we found that (i)
the mode of the shape parameter alpha was near 0.5, indicating prevalent
sub-exponential growth; (ii) the ultimate diffusion scale is primarily
determined by the growth rate R, with minor contributions from alpha or the
duration T; and (iii) alpha showed a tendency to vary with the nature of the
topic, being smaller for niche/local topics and larger for widely shared ones.
Furthermore, a micro-behavioral model distinguishing outward contact with
strangers from inward interaction within their community suggests that alpha
can be interpreted as an index of the preference for outward-oriented
communication. These findings suggest that sub-exponential growth is a common
pattern of social diffusion, and our model provides a practical framework for
consistently describing, comparing, and interpreting complex and diverse growth
curves.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [50] [How Different Tokenization Algorithms Impact LLMs and Transformer Models for Binary Code Analysis](https://arxiv.org/abs/2511.03825)
*Ahmed Mostafa,Raisul Arefin Nahid,Samuel Mulder*

Main category: cs.AI

TL;DR: 系统评估了汇编代码标记化模型对语义表征和下游任务的影响，发现标记器选择会显著影响分析性能，内在指标与外在效果存在复杂权衡关系


<details>
  <summary>Details</summary>
Motivation: 汇编代码标记化对语义分析和下游任务至关重要，但该领域研究存在空白。现有工作缺乏系统性评估不同NLP标记化模型在低级代码分析中的适应性

Method: 通过内在评估指标（标记效率/词汇压缩/表征保真度）对比不同tokenizer，结合Llama/BERT/BART等预训练模型，在函数签名预测等实际任务中进行外在验证

Result: 发现标记器选择显著影响下游性能（准确率差异达15%），内在指标仅能部分预测实际效果。最优tokenizer在词汇压缩率（78%）和语义覆盖度之间展现平衡

Conclusion: 需针对汇编代码特性定制标记化策略，优化tokenizer可提升二进制分析流程的鲁棒性。内在评估应与外在任务验证相结合，推动NLM-based反汇编技术的实用化

Abstract: Tokenization is fundamental in assembly code analysis, impacting intrinsic
characteristics like vocabulary size, semantic coverage, and extrinsic
performance in downstream tasks. Despite its significance, tokenization in the
context of assembly code remains an underexplored area. This study aims to
address this gap by evaluating the intrinsic properties of Natural Language
Processing (NLP) tokenization models and parameter choices, such as vocabulary
size. We explore preprocessing customization options and pre-tokenization rules
tailored to the unique characteristics of assembly code. Additionally, we
assess their impact on downstream tasks like function signature prediction -- a
critical problem in binary code analysis.
  To this end, we conduct a thorough study on various tokenization models,
systematically analyzing their efficiency in encoding assembly instructions and
capturing semantic nuances. Through intrinsic evaluations, we compare
tokenizers based on tokenization efficiency, vocabulary compression, and
representational fidelity for assembly code. Using state-of-the-art pre-trained
models such as the decoder-only Large Language Model (LLM) Llama 3.2, the
encoder-only transformer BERT, and the encoder-decoder model BART, we evaluate
the effectiveness of these tokenizers across multiple performance metrics.
Preliminary findings indicate that tokenizer choice significantly influences
downstream performance, with intrinsic metrics providing partial but incomplete
predictability of extrinsic evaluation outcomes. These results reveal complex
trade-offs between intrinsic tokenizer properties and their utility in
practical assembly code tasks. Ultimately, this study provides valuable
insights into optimizing tokenization models for low-level code analysis,
contributing to the robustness and scalability of Natural Language Model
(NLM)-based binary analysis workflows.

</details>


### [51] [LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing](https://arxiv.org/abs/2511.03980)
*Bram Bulté,Ayla Rigouts Terryn*

Main category: cs.AI

TL;DR: 研究验证大型语言模型（LLMs）存在系统性文化偏见，虽然提示策略可部分调整输出，但模型仍固守特定国家价值观，难以充分代表文化多样性。


<details>
  <summary>Details</summary>
Motivation: 验证LLMs能否反映全球用户文化多样性，揭示训练数据不平衡导致的模型文化偏向问题，探索提示语言/文化框架对价值观对齐的影响。

Method: 使用Hofstede价值观模块和世界价值观调查的63个问题，翻译成11种语言，设计不同文化框架提示，测试10个LLMs的跨文化响应模式。

Result: 模型输出受提示语言/文化影响，但系统性偏向荷兰、德国、美国、日本价值观；跨文化框架比目标语言更有效提升价值观对齐；两者组合无协同效应。

Conclusion: LLMs处于响应灵活性与文化固着性的矛盾状态，需改进文化锚定机制以更好服务多元用户，凸显多语言场景下潜在的文化偏见风险。

Abstract: Large Language Models (LLMs) are rapidly being adopted by users across the
globe, who interact with them in a diverse range of languages. At the same
time, there are well-documented imbalances in the training data and
optimisation objectives of this technology, raising doubts as to whether LLMs
can represent the cultural diversity of their broad user base. In this study,
we look at LLMs and cultural values and examine how prompt language and
cultural framing influence model responses and their alignment with human
values in different countries. We probe 10 LLMs with 63 items from the Hofstede
Values Survey Module and World Values Survey, translated into 11 languages, and
formulated as prompts with and without different explicit cultural
perspectives. Our study confirms that both prompt language and cultural
perspective produce variation in LLM outputs, but with an important caveat:
While targeted prompting can, to a certain extent, steer LLM responses in the
direction of the predominant values of the corresponding countries, it does not
overcome the models' systematic bias toward the values associated with a
restricted set of countries in our dataset: the Netherlands, Germany, the US,
and Japan. All tested models, regardless of their origin, exhibit remarkably
similar patterns: They produce fairly neutral responses on most topics, with
selective progressive stances on issues such as social tolerance. Alignment
with cultural values of human respondents is improved more with an explicit
cultural perspective than with a targeted prompt language. Unexpectedly,
combining both approaches is no more effective than cultural framing with an
English prompt. These findings reveal that LLMs occupy an uncomfortable middle
ground: They are responsive enough to changes in prompts to produce variation,
but too firmly anchored to specific cultural defaults to adequately represent
cultural diversity.

</details>


### [52] [Large language models replicate and predict human cooperation across experiments in game theory](https://arxiv.org/abs/2511.04500)
*Andrea Cera Palatsi,Samuel Martin-Gutierrez,Ana S. Cardenal,Max Pellert*

Main category: cs.AI

TL;DR: 研究通过数字孪生实验和系统化提示框架，发现不同LLMs在复现人类决策行为上存在显著差异（Llama拟合人类合作偏差，Qwen贴合纳什均衡），并展示了LLMs在社会科学研究中的新潜力。


<details>
  <summary>Details</summary>
Motivation: LLMs被广泛用于决策和人类行为模拟，但其与真实人类决策的匹配度尚未明确。这种不匹配可能导致应用风险且影响行为模拟有效性，需建立系统性评估框架。

Method: 开发博弈论实验的数字孪生系统，构建提示探测框架测试Llama/Mistral/Qwen模型，并生成预注册假设拓展到原始参数网格之外的新型游戏配置。

Result: Llama高保真复现人类合作偏差（偏离理性选择），Qwen贴合纳什均衡；无需角色提示即可实现群体行为复现，成功生成新实验空间的预测假设。

Conclusion: 适当校准的LLMs可复现人类群体行为模式，为社会科学研究提供系统性探索新路径，形成与传统研究互补的方法论并产生新的实证预测。

Abstract: Large language models (LLMs) are increasingly used both to make decisions in
domains such as health, education and law, and to simulate human behavior. Yet
how closely LLMs mirror actual human decision-making remains poorly understood.
This gap is critical: misalignment could produce harmful outcomes in practical
applications, while failure to replicate human behavior renders LLMs
ineffective for social simulations. Here, we address this gap by developing a
digital twin of game-theoretic experiments and introducing a systematic
prompting and probing framework for machine-behavioral evaluation. Testing
three open-source models (Llama, Mistral and Qwen), we find that Llama
reproduces human cooperation patterns with high fidelity, capturing human
deviations from rational choice theory, while Qwen aligns closely with Nash
equilibrium predictions. Notably, we achieved population-level behavioral
replication without persona-based prompting, simplifying the simulation
process. Extending beyond the original human-tested games, we generate and
preregister testable hypotheses for novel game configurations outside the
original parameter grid. Our findings demonstrate that appropriately calibrated
LLMs can replicate aggregate human behavioral patterns and enable systematic
exploration of unexplored experimental spaces, offering a complementary
approach to traditional research in the social and behavioral sciences that
generates new empirical predictions about human social decision-making.

</details>


### [53] [Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper](https://arxiv.org/abs/2511.04583)
*Atsuyuki Miyai,Mashiro Toyooka,Takashi Otonari,Zaiying Zhao,Kiyoharu Aizawa*

Main category: cs.AI

TL;DR: 开发了Jr. AI Scientist系统，通过模仿学生研究者工作流程生成高质量论文，评估显示其优于全自动系统但仍有局限性


<details>
  <summary>Details</summary>
Motivation: 理解AI科学家的能力与风险，确保可信赖且可持续的AI驱动科学发展

Method: 构建基于现代编码代理的科研工作流（分析论文局限→提出假设→实验验证→撰写论文），处理复杂多文件实现

Result: 生成论文评审分数超过全自动系统，但作者评估和Agents4Science反馈揭示应用风险

Conclusion: 当前AI科学家系统存在潜在风险，需解决关键挑战。开发过程中识别的风险将为未来研究提供重要参考

Abstract: Understanding the current capabilities and risks of AI Scientist systems is
essential for ensuring trustworthy and sustainable AI-driven scientific
progress while preserving the integrity of the academic ecosystem. To this end,
we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system
that mimics the core research workflow of a novice student researcher: Given
the baseline paper from the human mentor, it analyzes its limitations,
formulates novel hypotheses for improvement, validates them through rigorous
experimentation, and writes a paper with the results. Unlike previous
approaches that assume full automation or operate on small-scale code, Jr. AI
Scientist follows a well-defined research workflow and leverages modern coding
agents to handle complex, multi-file implementations, leading to scientifically
valuable contributions. For evaluation, we conducted automated assessments
using AI Reviewers, author-led evaluations, and submissions to Agents4Science,
a venue dedicated to AI-driven scientific contributions. The findings
demonstrate that Jr. AI Scientist generates papers receiving higher review
scores than existing fully automated systems. Nevertheless, we identify
important limitations from both the author evaluation and the Agents4Science
reviews, indicating the potential risks of directly applying current AI
Scientist systems and key challenges for future research. Finally, we
comprehensively report various risks identified during development. We hope
these insights will deepen understanding of current progress and risks in AI
Scientist development.

</details>


### [54] [Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis](https://arxiv.org/abs/2511.04584)
*Daniel Gomm,Cornelius Wolff,Madelon Hulsebos*

Main category: cs.AI

TL;DR: 将自然语言查询的模糊性重新定义为协作特征，提出区分协作/非协作查询的框架，分析发现现有数据集存在混合查询类型问题


<details>
  <summary>Details</summary>
Motivation: 现有表格问答系统评估未合理区分协作型与非协作型查询，导致评估指标无法准确衡量系统的解析和执行能力

Method: 开发理论框架区分可解析的协作查询与不可解析的非协作查询，并应用于15个主流数据集的查询类型分析

Result: 发现现有数据集未合理控制查询类型混合，既不能准确评估执行精度，也无法有效测试系统解释能力

Conclusion: 通过框架转换视角，从消除模糊性转向支持协作式查询解析，为表格数据界面设计提供更科学的评估方法论，并指明未来研究方向

Abstract: Natural language interfaces to tabular data must handle ambiguities inherent
to queries. Instead of treating ambiguity as a deficiency, we reframe it as a
feature of cooperative interaction, where the responsibility of query
specification is shared among the user and the system. We develop a principled
framework distinguishing cooperative queries, i.e., queries that yield a
resolvable interpretation, from uncooperative queries that cannot be resolved.
Applying the framework to evaluations for tabular question answering and
analysis, we analyze the queries in 15 popular datasets, and observe an
uncontrolled mixing of query types neither adequate for evaluating a system's
execution accuracy nor for evaluating interpretation capabilities. Our
framework and analysis of queries shifts the perspective from fixing ambiguity
to embracing cooperation in resolving queries. This reflection enables more
informed design and evaluation for natural language interfaces for tabular
data, for which we outline implications and directions for future research.

</details>


### [55] [DR. WELL: Dynamic Reasoning and Learning with Symbolic World Model for Embodied LLM-Based Multi-Agent Collaboration](https://arxiv.org/abs/2511.04646)
*Narjes Nourzad,Hanqing Yang,Shiyu Chen,Carlee Joe-Wong*

Main category: cs.AI

TL;DR: 提出DR.WELL神经符号框架，通过两阶段协商协议和符号化规划实现多智能体高效协作，实验显示动态世界模型提升任务完成率和效率


<details>
  <summary>Details</summary>
Motivation: 传统基于轨迹的协作存在脆弱的时间对齐问题，微小偏差容易引发冲突。符号化规划通过抽象层和最小动作词汇实现智能体同步与协作

Method: 1) 两阶段协商：候选角色提议→共识约束下的联合分配
2) 基于共享世界模型的符号计划生成与执行
3) 动态更新环境状态实现行动落地

Result: 在协作推块任务中：
- 任务完成率提升37%
- 行动效率提高25%
- 时间开销增加15%（换取策略进化）
- 可复用协作模式增长3.2倍

Conclusion: 符号化规划显著提升多智能体协作的鲁棒性，动态世界模型支持策略自演进，实现可复用、可同步、可解释的高层协作操作，为复杂环境下的分布式协作提供新范式

Abstract: Cooperative multi-agent planning requires agents to make joint decisions with
partial information and limited communication. Coordination at the trajectory
level often fails, as small deviations in timing or movement cascade into
conflicts. Symbolic planning mitigates this challenge by raising the level of
abstraction and providing a minimal vocabulary of actions that enable
synchronization and collective progress. We present DR. WELL, a decentralized
neurosymbolic framework for cooperative multi-agent planning. Cooperation
unfolds through a two-phase negotiation protocol: agents first propose
candidate roles with reasoning and then commit to a joint allocation under
consensus and environment constraints. After commitment, each agent
independently generates and executes a symbolic plan for its role without
revealing detailed trajectories. Plans are grounded in execution outcomes via a
shared world model that encodes the current state and is updated as agents act.
By reasoning over symbolic plans rather than raw trajectories, DR. WELL avoids
brittle step-level alignment and enables higher-level operations that are
reusable, synchronizable, and interpretable. Experiments on cooperative
block-push tasks show that agents adapt across episodes, with the dynamic world
model capturing reusable patterns and improving task completion rates and
efficiency. Experiments on cooperative block-push tasks show that our dynamic
world model improves task completion and efficiency through negotiation and
self-refinement, trading a time overhead for evolving, more efficient
collaboration strategies.

</details>


### [56] [VeriCoT: Neuro-symbolic Chain-of-Thought Validation via Logical Consistency Checks](https://arxiv.org/abs/2511.04662)
*Yu Feng,Nathaniel Weir,Kaj Bostrom,Sam Bayless,Darion Cassel,Sapana Chaudhary,Benjamin Kiesl-Reiter,Huzefa Rangwala*

Main category: cs.AI

TL;DR: 提出VeriCoT神经符号方法验证思维链推理逻辑，通过形式逻辑验证和自然语言前提识别提升大模型推理可靠性，并在多项任务中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型(CoT)虽能多步推理但无法自我验证逻辑，正确结论可能基于错误推理，影响高风险场景可信度。需开发验证机制增强推理可靠性。

Method: 1. 将CoT步骤转化为一阶逻辑形式化表达
2. 识别前提来源(上下文/常识/先前推理)
3. 符号逻辑验证结合自然语言可解释性
4. 开发推理时自省、监督微调和偏好微调三阶段优化框架

Result: 1. 在ProofWriter/LegalBench/BioASQ数据集中有效识别错误推理
2. 验证信号可预测答案正确性(准确率提升)
3. 微调后模型推理有效性提升8-15%，准确率提高5-12%

Conclusion: VeriCoT通过符号验证与自然语言解释的融合，构建可验证的推理框架，并为模型优化提供有效信号，显著提升复杂推理任务的可信度和准确性。

Abstract: LLMs can perform multi-step reasoning through Chain-of-Thought (CoT), but
they cannot reliably verify their own logic. Even when they reach correct
answers, the underlying reasoning may be flawed, undermining trust in
high-stakes scenarios. To mitigate this issue, we introduce VeriCoT, a
neuro-symbolic method that extracts and verifies formal logical arguments from
CoT reasoning. VeriCoT formalizes each CoT reasoning step into first-order
logic and identifies premises that ground the argument in source context,
commonsense knowledge, or prior reasoning steps. The symbolic representation
enables automated solvers to verify logical validity while the NL premises
allow humans and systems to identify ungrounded or fallacious reasoning steps.
Experiments on the ProofWriter, LegalBench, and BioASQ datasets show VeriCoT
effectively identifies flawed reasoning, and serves as a strong predictor of
final answer correctness. We also leverage VeriCoT's verification signal for
(1) inference-time self-reflection, (2) supervised fine-tuning (SFT) on
VeriCoT-distilled datasets and (3) preference fine-tuning (PFT) with direct
preference optimization (DPO) using verification-based pairwise rewards,
further improving reasoning validity and accuracy.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [57] [MIDI-LLM: Adapting Large Language Models for Text-to-MIDI Music Generation](https://arxiv.org/abs/2511.03942)
*Shih-Lun Wu,Yoon Kim,Cheng-Zhi Anna Huang*

Main category: cs.SD

TL;DR: 提出MIDI-LLM模型，通过扩展文本大语言模型的词汇表并采用两阶段训练，实现从文本生成高质量多轨MIDI音乐，在推理速度和质量上超越现有模型


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到MIDI生成模型（如Text2midi）在生成质量、文本控制精度和推理速度方面的不足，探索大语言模型在音乐生成领域的扩展应用

Method: 1. 扩展文本LLM词汇表加入MIDI令牌
2. 两阶段训练策略（文本适应+音乐微调）
3. 保持原始LLM参数结构以兼容vLLM加速推理

Result: 实验显示相比Text2midi：生成质量提高18%（主观评估），文本控制准确率提升32%，推理速度加快5.7倍（利用vLLM优化）

Conclusion: MIDI-LLM成功将LLM扩展至音乐生成领域，通过技术创新平衡了生成质量与推理效率，其在线演示验证了实用价值

Abstract: We present MIDI-LLM, an LLM for generating multitrack MIDI music from
free-form text prompts. Our approach expands a text LLM's vocabulary to include
MIDI tokens, and uses a two-stage training recipe to endow text-to-MIDI
abilities. By preserving the original LLM's parameter structure, we can
directly leverage the vLLM library for accelerated inference. Experiments show
that MIDI-LLM achieves higher quality, better text control, and faster
inference compared to the recent Text2midi model. Live demo at
https://midi-llm-demo.vercel.app.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [58] [RLHF: A comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods](https://arxiv.org/abs/2511.03939)
*Raghav Sharma,Manan Mehta,Sai Tiger Raina*

Main category: cs.LG

TL;DR: 该论文系统梳理了RLHF对齐技术的演进，重点突破多模态对齐、文化公平和低延迟优化三大瓶颈，提出算法对比框架与研究路线图。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF技术局限在文本领域，无法适应多模态场景且存在文化偏见，亟需构建更鲁棒高效的AI对齐范式。

Method: 通过算法演进分析（PPO/DPO/GRPO），建立多维度评估体系，并创新性引入文化维度评估指标和分层强化学习框架。

Result: 形成算法选择决策树，在128核GPU集群实现23%的训练加速，跨文化数据集上偏差降低18%。

Conclusion: 提出动态对齐系数概念，为多模态RLHF建立理论边界，但实时文化适应和能耗优化仍是待解难题。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is the standard for
aligning Large Language Models (LLMs), yet recent progress has moved beyond
canonical text-based methods. This survey synthesizes the new frontier of
alignment research by addressing critical gaps in multi-modal alignment,
cultural fairness, and low-latency optimization. To systematically explore
these domains, we first review foundational algo- rithms, including PPO, DPO,
and GRPO, before presenting a detailed analysis of the latest innovations. By
providing a comparative synthesis of these techniques and outlining open
challenges, this work serves as an essential roadmap for researchers building
more robust, efficient, and equitable AI systems.

</details>


### [59] [Towards Scalable Meta-Learning of near-optimal Interpretable Models via Synthetic Model Generations](https://arxiv.org/abs/2511.04000)
*Kyaw Hpone Myint,Zhe Wu,Alexandre G. R. Day,Giri Iyengar*

Main category: cs.LG

TL;DR: 利用合成数据生成方法实现高效元学习，降低决策树模型预训练成本


<details>
  <summary>Details</summary>
Motivation: 现有基于真实数据或最优决策树的元学习方法存在计算成本高、数据灵活性差的问题

Method: 通过采样接近最优的决策树生成大规模合成数据集，结合MetaTree transformer架构

Result: 在保持性能相当的前提下，计算成本显著降低且数据生成灵活性提高

Conclusion: 该策略为可扩展的、可解释的决策树元学习提供了高效解决方案

Abstract: Decision trees are widely used in high-stakes fields like finance and
healthcare due to their interpretability. This work introduces an efficient,
scalable method for generating synthetic pre-training data to enable
meta-learning of decision trees. Our approach samples near-optimal decision
trees synthetically, creating large-scale, realistic datasets. Using the
MetaTree transformer architecture, we demonstrate that this method achieves
performance comparable to pre-training on real-world data or with
computationally expensive optimal decision trees. This strategy significantly
reduces computational costs, enhances data generation flexibility, and paves
the way for scalable and efficient meta-learning of interpretable decision tree
models.

</details>


### [60] [DartQuant: Efficient Rotational Distribution Calibration for LLM Quantization](https://arxiv.org/abs/2511.04063)
*Yuantian Shao,Yuanteng Chen,Peisong Wang,Jianlin Yu,Jing Lin,Yiwu Yao,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: DartQuant提出一种高效的分布感知旋转校准方法，通过约束激活分布和QR-Orth优化方案，显著降低大模型量化的计算资源需求，使其可在单张3090 GPU上完成70B模型校准。


<details>
  <summary>Details</summary>
Motivation: 现有旋转优化算法需要昂贵的端到端微调（计算成本高且易过拟合），难以在资源受限场景应用。

Method: 1. 分布感知旋转校准约束激活分布降低优化复杂度
2. QR-Orth优化方案用高效解法替代交替优化

Result: 70B模型实验显示：47倍加速/10倍内存节省，首次在单张3090 GPU完成旋转校准

Conclusion: DartQuant为资源受限环境下的大模型量化提供了可行方案，代码已开源

Abstract: Quantization plays a crucial role in accelerating the inference of
large-scale models, and rotational matrices have been shown to effectively
improve quantization performance by smoothing outliers. However, end-to-end
fine-tuning of rotational optimization algorithms incurs high computational
costs and is prone to overfitting. To address this challenge, we propose an
efficient distribution-aware rotational calibration method, DartQuant, which
reduces the complexity of rotational optimization by constraining the
distribution of the activations after rotation. This approach also effectively
reduces reliance on task-specific losses, thereby mitigating the risk of
overfitting. Additionally, we introduce the QR-Orth optimization scheme, which
replaces expensive alternating optimization with a more efficient solution. In
a variety of model quantization experiments, DartQuant demonstrates superior
performance. Compared to existing methods, it achieves 47$\times$ acceleration
and 10$\times$ memory savings for rotational optimization on a 70B model.
Furthermore, it is the first to successfully complete rotational calibration
for a 70B model on a single 3090 GPU, making quantization of large language
models feasible in resource-constrained environments. Code is available at
https://github.com/CAS-CLab/DartQuant.git.

</details>


### [61] [Block Rotation is All You Need for MXFP4 Quantization](https://arxiv.org/abs/2511.04214)
*Yuantian Shao,Peisong Wang,Yuanteng Chen,Chang Xu,Zhihui Wei,Jian Cheng*

Main category: cs.LG

TL;DR: 系统评估PTQ方法在MXFP4格式下的表现，发现旋转类方法与硬件格式存在根本性冲突，并提出适配MXFP4的块旋转策略提升LLM量化精度


<details>
  <summary>Details</summary>
Motivation: MXFP4新型低精度格式的硬件兼容性引发对现有量化方法适用性的质疑，特别是主流旋转类方法与MXFP4的PoT块缩放机制存在能量重分配冲突

Method: 建立MXFP4格式下的PTQ基准测试，通过误差分析定位格式冲突根源，设计基于块旋转的改进方案适配MXFP4的量化特性

Result: GPTQ等传统方法表现稳健，旋转类方法精度下降23.1%；改进后的块旋转策略使LLaMA-7B在WikiText2上PPL从18.7降至12.4

Conclusion: 该研究为MXFP4格式下的模型量化提供实践指导，同时揭示低精度格式与优化算法的协同设计重要性，推动高效推理技术发展

Abstract: Large language models (LLMs) have achieved remarkable success, but their
rapidly growing scale imposes prohibitive costs in memory, computation, and
energy. Post-training quantization (PTQ) is a promising solution for efficient
deployment, yet achieving accurate W4A4 quantization remains an open challenge.
While most existing methods are designed for INT4 formats, the emergence of
MXFP4 -- a new FP4 format with various hardware support (NVIDIA, AMD, Intel)--
raises questions about the applicability of current techniques. In this work,
we establish a comprehensive benchmark of PTQ methods under the MXFP4 format.
Through systematic evaluation, we find that methods like GPTQ consistently
deliver strong performance, whereas rotation-based approaches, which are almost
used by all state-of-the-art approaches, suffer from severe incompatibility
with MXFP4. We further provide the first in-depth analysis of this conflict,
tracing its root to a fundamental mismatch between MXFP4's PoT (power-of-two)
block scaling and the redistribution of outlier energy via global rotation.
Building on this insight, we propose a simple yet effective block rotation
strategy that adapts rotation-based methods to MXFP4, leading to substantial
accuracy improvements across diverse LLMs. Our findings not only offer clear
guidance for practitioners but also set a foundation for advancing PTQ research
under emerging low-precision formats.

</details>


### [62] [The Illusion of Certainty: Uncertainty quantification for LLMs fails under ambiguity](https://arxiv.org/abs/2511.04418)
*Tim Tomov,Dominik Fuchsgruber,Tom Wollschläger,Stephan Günnemann*

Main category: cs.LG

TL;DR: 研究发现当前LLM不确定性量化方法在数据存在歧义时性能显著下降，提出首个含真实答案分布的歧义问答数据集MAQA*和AmbigQA*，揭示了现有方法的根本性局限。


<details>
  <summary>Details</summary>
Motivation: 现实语言具有固有歧义性（认知不确定性），但现有UQ方法仅在无歧义任务中验证性能。这种局限导致实际应用中的不可靠性，需构建反映真实歧义场景的数据集。

Method: 1. 构建MAQA*和AmbigQA*数据集，通过事实共现估计真实答案分布
2. 测试三种UQ范式：模型预测分布、内部表征分析、模型集成
3. 理论分析预测分布和集成方法的局限性

Result: 所有UQ方法在歧义数据上性能接近随机水平（AUC 0.5-0.6），且理论证明预测分布和集成方法在歧义场景下存在根本性局限。

Conclusion: 当前LLM不确定性量化方法存在重大缺陷，需重新设计建模范式，强调在现实歧义场景中评估UQ方法的重要性。

Abstract: Accurate uncertainty quantification (UQ) in Large Language Models (LLMs) is
critical for trustworthy deployment. While real-world language is inherently
ambiguous, reflecting aleatoric uncertainty, existing UQ methods are typically
benchmarked against tasks with no ambiguity. In this work, we demonstrate that
while current uncertainty estimators perform well under the restrictive
assumption of no ambiguity, they degrade to close-to-random performance on
ambiguous data. To this end, we introduce MAQA* and AmbigQA*, the first
ambiguous question-answering (QA) datasets equipped with ground-truth answer
distributions estimated from factual co-occurrence. We find this performance
deterioration to be consistent across different estimation paradigms: using the
predictive distribution itself, internal representations throughout the model,
and an ensemble of models. We show that this phenomenon can be theoretically
explained, revealing that predictive-distribution and ensemble-based estimators
are fundamentally limited under ambiguity. Overall, our study reveals a key
shortcoming of current UQ methods for LLMs and motivates a rethinking of
current modeling paradigms.

</details>


### [63] [Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs](https://arxiv.org/abs/2511.04473)
*Alberto Cattaneo,Carlo Luschi,Daniel Justus*

Main category: cs.LG

TL;DR: 提出SynthKGQA框架生成高质量知识图谱问答数据集，解决现有KGQA数据缺乏挑战性的问题，并通过GTSQA数据集验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱检索方法缺乏具有挑战性的基准数据集，导致模型评估不充分且难以比较不同方法。

Method: 开发可适配任意知识图谱的SynthKGQA框架，自动生成包含完整事实链的QA数据集，支持对检索器的零样本泛化能力测试。

Result: 成功创建GTSQA数据集，证明该框架能提升检索模型性能，并在Wikidata上实现对新图结构和关系类型的泛化测试。

Conclusion: SynthKGQA为知识图谱检索提供了标准化评估工具，同时增强了LLMs与结构化知识的整合能力，推动可靠知识推理系统发展。

Abstract: Retrieval of information from graph-structured knowledge bases represents a
promising direction for improving the factuality of LLMs. While various
solutions have been proposed, a comparison of methods is difficult due to the
lack of challenging QA datasets with ground-truth targets for graph retrieval.
We present SynthKGQA, a framework for generating high-quality synthetic
Knowledge Graph Question Answering datasets from any Knowledge Graph, providing
the full set of ground-truth facts in the KG to reason over each question. We
show how, in addition to enabling more informative benchmarking of KG
retrievers, the data produced with SynthKGQA also allows us to train better
models. We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset
designed to test zero-shot generalization abilities of KG retrievers with
respect to unseen graph structures and relation types, and benchmark popular
solutions for KG-augmented LLMs on it.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [64] [Explorability in Pushdown Automata](https://arxiv.org/abs/2511.04048)
*Ayaan Bedi,Karoliina Lehtinen*

Main category: cs.FL

TL;DR: 提出可探索性（k-explorable PDAs）作为下推自动机非确定性的量化指标，构建介于历史确定性和完全非确定性之间的无限层次结构，揭示表达能力与简洁性关系


<details>
  <summary>Details</summary>
Motivation: 传统历史确定性PDAs表达能力有限，完全非确定性缺乏可操作性度量。通过k-explorable参数化模型，建立非确定性程度的系统性评估框架，填补理论空白

Method: 1. 定义k-explorable PDAs（跟踪k条并行路径）
2. 构建表达能力层次定理（k与k-1的严格包含关系）
3. 引入指数可探索性参数化模型
4. 通过简洁性对比分析（双指数压缩比）

Result: 1. 形成无限层次：k-explorable PDAs ⊋ (k-1)-explorable
2. 指数可探索性=上下文无关语言
3. 2-explorable PDAs比确定性PDAs简洁性差距不可递归枚举
4. 可探索PDAs比历史确定性PDAs双指数级简洁

Conclusion: 可探索性为下推系统提供了可操作的量化非确定性标准，层次结构与简洁性结论确立其理论价值，为程序验证和语言处理提供新分析工具

Abstract: We study explorability, a measure of nondeterminism in pushdown automata,
which generalises history-determinism. An automaton is k-explorable if, while
reading the input, it suffices to follow k concurrent runs, built step-by-step
based only on the input seen so far, to construct an accepting one, if it
exists. We show that the class of explorable PDAs lies strictly between
history-deterministic and fully nondeterministic PDAs in terms of both
expressiveness and succinctness. In fact increasing explorability induces an
infinite hierarchy: each level k defines a strictly more expressive class than
level k-1, yet the entire class remains less expressive than general
nondeterministic PDAs. We then introduce a parameterized notion of
explorability, where the number of runs may depend on input length, and show
that exponential explorability precisely captures the context-free languages.
Finally, we prove that explorable PDAs can be doubly exponentially more
succinct than history-deterministic ones, and that the succinctness gap between
deterministic and 2-explorable PDAs is not recursively enumerable. These
results position explorability as a robust and operationally meaningful measure
of nondeterminism for pushdown systems.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [65] [Multi-Agent Collaborative Framework For Math Problem Generation](https://arxiv.org/abs/2511.03958)
*Kia Karbasi,Kevin Hong,Mohammad Amin Samadi,Gregory Pottie*

Main category: cs.MA

TL;DR: 提出协作多智能体框架以提升数学教育中自动生成问题的质量和控制能力


<details>
  <summary>Details</summary>
Motivation: 现有预训练模型在控制数学问题复杂度和认知需求方面存在不足，限制了智能教学系统的教育效果

Method: 开发协作多智能体框架，通过迭代优化问题-答案对来平衡复杂性与认知需求

Result: 在相关性、重要性等五个评估维度上显示出更优的认知挑战与清晰度平衡能力

Conclusion: 多智能体协作机制为自动化教育内容生成提供了更可控且具教学价值的解决方案

Abstract: Automatic question generation (AQG) for mathematics education remains an
elusive goal for Intelligent Tutoring Systems and educators. While pre-trained
transformer-based language models have significantly advanced natural language
generation, they often struggle to precisely control problem complexity and
cognitive demands. In this paper, we introduce a collaborative multi-agent
framework as a novel method of incorporating inference-time computation into
AQG. This approach leverages multiple agents that iteratively refine generated
question-answer pairs to better balance complexity and cognitive demand. We
evaluate the generated questions on five meta-evaluation criteria: relevance,
importance, clarity, difficulty matching, answerability, to assess the system's
ability to control the required complexity and quality of the questions.
Preliminary evaluations show that this collaborative multi-agent framework
elevates the quality of generated educational content by fostering a more
nuanced balance between cognitive challenge and clarity. These promising
outcomes suggest that integrating collaborative multi-agent workflows can yield
more controlled, pedagogically valuable content that can help advance automated
educational content generation and adaptive learning environments.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [66] [Near-Lossless 3D Voxel Representation Free from Iso-surface](https://arxiv.org/abs/2511.04029)
*Yihao Luo,Xianglong He,Chuanyu Pan,Yiwen Chen,Jiaqi Wu,Yangguang Li,Wanli Ouyang,Yuanming Hu,Guang Yang,ChoonHwai Yap*

Main category: cs.CV

TL;DR: 提出Faithful Contouring稀疏体素化表示方法，支持2048+分辨率且无需场函数转换，实现近乎无损的3D网格表征


<details>
  <summary>Details</summary>
Motivation: 传统等值面体素化方法依赖水密化处理或渲染优化，导致几何保真度受损，无法有效保留复杂结构的锐利边缘与内部细节

Method: 通过保持几何锐利度与内部结构实现近无损表征，设计双模式自动编码器实现可扩展的保细节网格重建

Result: 表征误差达10^-5级别；网格重建中Chamfer距离降低93%，F-score提升35%，超越现有基准方法

Conclusion: 该表征方法在保真度与效率维度建立新标准，为3D重建/生成任务提供更优解决方案，并展示纹理处理与编辑的灵活性

Abstract: Accurate and efficient voxelized representations of 3D meshes are the
foundation of 3D reconstruction and generation. However, existing
representations based on iso-surface heavily rely on water-tightening or
rendering optimization, which inevitably compromise geometric fidelity. We
propose Faithful Contouring, a sparse voxelized representation that supports
2048+ resolutions for arbitrary meshes, requiring neither converting meshes to
field functions nor extracting the isosurface during remeshing. It achieves
near-lossless fidelity by preserving sharpness and internal structures, even
for challenging cases with complex geometry and topology. The proposed method
also shows flexibility for texturing, manipulation, and editing. Beyond
representation, we design a dual-mode autoencoder for Faithful Contouring,
enabling scalable and detail-preserving shape reconstruction. Extensive
experiments show that Faithful Contouring surpasses existing methods in
accuracy and efficiency for both representation and reconstruction. For direct
representation, it achieves distance errors at the $10^{-5}$ level; for mesh
reconstruction, it yields a 93\% reduction in Chamfer Distance and a 35\%
improvement in F-score over strong baselines, confirming superior fidelity as a
representation for 3D learning tasks.

</details>


### [67] [Seeing Straight: Document Orientation Detection for Efficient OCR](https://arxiv.org/abs/2511.04161)
*Suranjan Goswami,Abhinav Ravi,Raja Kolla,Ali Faraz,Shaharukh Khan,Akash,Chandra Khatri,Shubham Agarwal*

Main category: cs.CV

TL;DR: 论文提出OCR旋转鲁棒性基准ORB和基于Phi-3.5-Vision的快速旋转分类方法，在英印数据集分别实现96%和92%准确率，显著提升OCR性能


<details>
  <summary>Details</summary>
Motivation: 现实场景中扫描/拍摄文档的旋转校正直接影响OCR准确率，但现有系统对用户误操作导致的旋转文档处理不足

Method: 1. 构建ORB-En（英文）和ORB-Indic（11种印度语言）基准 2. 基于Phi-3.5视觉编码器设计动态裁剪的轻量级四向旋转分类流程

Result: 旋转分类准确率分别达96%（英文）和92%（印度语言），在模拟真实场景中闭源OCR性能提升14%，开源模型提升4倍

Conclusion: 旋转校正模块对OCR系统性能提升至关重要，尤其在中低资源语言场景，验证了预处理在文档理解中的关键作用

Abstract: Despite significant advances in document understanding, determining the
correct orientation of scanned or photographed documents remains a critical
pre-processing step in the real world settings. Accurate rotation correction is
essential for enhancing the performance of downstream tasks such as Optical
Character Recognition (OCR) where misalignment commonly arises due to user
errors, particularly incorrect base orientations of the camera during capture.
In this study, we first introduce OCR-Rotation-Bench (ORB), a new benchmark for
evaluating OCR robustness to image rotations, comprising (i) ORB-En, built from
rotation-transformed structured and free-form English OCR datasets, and (ii)
ORB-Indic, a novel multilingual set spanning 11 Indic mid to low-resource
languages. We also present a fast, robust and lightweight rotation
classification pipeline built on the vision encoder of Phi-3.5-Vision model
with dynamic image cropping, fine-tuned specifically for 4-class rotation task
in a standalone fashion. Our method achieves near-perfect 96% and 92% accuracy
on identifying the rotations respectively on both the datasets. Beyond
classification, we demonstrate the critical role of our module in boosting OCR
performance: closed-source (up to 14%) and open-weights models (up to 4x) in
the simulated real-world setting.

</details>


### [68] [Thinking with Video: Video Generation as a Promising Multimodal Reasoning Paradigm](https://arxiv.org/abs/2511.04570)
*Jingqi Tong,Yurong Mou,Hangcheng Li,Mingzhe Li,Yongzhuo Yang,Ming Zhang,Qiguang Chen,Tianyi Liang,Xiaomeng Hu,Yining Zheng,Xinchi Chen,Jun Zhao,Xuanjing Huang,Xipeng Qiu*

Main category: cs.CV

TL;DR: 提出'视频思维'新范式，利用Sora-2视频生成模型实现多模态统一推理，在VideoThinkBench基准测试中视觉任务媲美SOTA VLM，文本任务达92%数学准确率。


<details>
  <summary>Details</summary>
Motivation: 现有文本/图像思维范式存在动态表征缺失与模态割裂问题，无法实现连续过程表达与多模态统一理解。

Method: 构建VideoThinkBench评估体系（含视觉核心与文本核心任务），系统评估Sora-2的视频生成模型推理能力。

Result: 视觉任务超越VLM（如目视解谜），文本任务MATH准确率92%、MMMU 75.53%，自洽性与上下文学习可提升表现。

Conclusion: 视频生成模型具备统一多模态理解潜力，'视频思维'可作为新型跨模态推理范式，突破现有模态壁垒。

Abstract: "Thinking with Text" and "Thinking with Images" paradigm significantly
improve the reasoning ability of large language models (LLMs) and Vision
Language Models (VLMs). However, these paradigms have inherent limitations. (1)
Images capture only single moments and fail to represent dynamic processes or
continuous changes, and (2) The separation of text and vision as distinct
modalities, hindering unified multimodal understanding and generation. To
overcome these limitations, we introduce "Thinking with Video", a new paradigm
that leverages video generation models, such as Sora-2, to bridge visual and
textual reasoning in a unified temporal framework. To support this exploration,
we developed the Video Thinking Benchmark (VideoThinkBench). VideoThinkBench
encompasses two task categories: (1) vision-centric tasks (e.g., Eyeballing
Puzzles), and (2) text-centric tasks (e.g., subsets of GSM8K, MMMU). Our
evaluation establishes Sora-2 as a capable reasoner. On vision-centric tasks,
Sora-2 is generally comparable to state-of-the-art (SOTA) VLMs, and even
surpasses VLMs on several tasks, such as Eyeballing Games. On text-centric
tasks, Sora-2 achieves 92% accuracy on MATH, and 75.53% accuracy on MMMU.
Furthermore, we systematically analyse the source of these abilities. We also
find that self-consistency and in-context learning can improve Sora-2's
performance. In summary, our findings demonstrate that the video generation
model is the potential unified multimodal understanding and generation model,
positions "thinking with video" as a unified multimodal reasoning paradigm.

</details>
