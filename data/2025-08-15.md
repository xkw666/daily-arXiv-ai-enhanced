<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 73]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.CV](#cs.CV) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Bridging AI Innovation and Healthcare Needs: Lessons Learned from Incorporating Modern NLP at The BC Cancer Registry](https://arxiv.org/abs/2508.09991)
*Lovedeep Gondara,Gregory Arbour,Raymond Ng,Jonathan Simkin,Shebnum Devji*

Main category: cs.CL

TL;DR: 论文分享了在癌症登记处实施NLP模型的关键经验，强调业务目标导向、迭代开发、跨学科协作、实用模型选择及数据质量管理，为医疗AI落地提供实践指南。


<details>
  <summary>Details</summary>
Motivation: 解决临床文档自动化处理中NLP技术落地难题，提升医疗数据管理效率以改善患者护理质量。

Method: 基于癌症登记处实践经验，采用业务需求主导的问题定义、人机协同的迭代开发模式、混合模型架构选择，并建立数据质量监控与人工审核机制。

Result: 形成包含组织协作机制、技术选型策略、数据治理规范的可复制方法论，验证了AI在医疗信息提取中的可行性。

Conclusion: 医疗AI成功部署需要技术实施与组织变革相结合，通过持续协同创新实现数据管理优化和患者服务提升。

Abstract: Automating data extraction from clinical documents offers significant
potential to improve efficiency in healthcare settings, yet deploying Natural
Language Processing (NLP) solutions presents practical challenges. Drawing upon
our experience implementing various NLP models for information extraction and
classification tasks at the British Columbia Cancer Registry (BCCR), this paper
shares key lessons learned throughout the project lifecycle. We emphasize the
critical importance of defining problems based on clear business objectives
rather than solely technical accuracy, adopting an iterative approach to
development, and fostering deep interdisciplinary collaboration and co-design
involving domain experts, end-users, and ML specialists from inception. Further
insights highlight the need for pragmatic model selection (including hybrid
approaches and simpler methods where appropriate), rigorous attention to data
quality (representativeness, drift, annotation), robust error mitigation
strategies involving human-in-the-loop validation and ongoing audits, and
building organizational AI literacy. These practical considerations,
generalizable beyond cancer registries, provide guidance for healthcare
organizations seeking to successfully implement AI/NLP solutions to enhance
data management processes and ultimately improve patient care and public health
outcomes.

</details>


### [2] [A Transparent Fairness Evaluation Protocol for Open-Source Language Model Benchmarking on the Blockchain](https://arxiv.org/abs/2508.09993)
*Hugo Massaroli,Leonardo Iara,Emmanuel Iarussi,Viviana Siless*

Main category: cs.CL

TL;DR: 提出基于区块链的透明评估协议，对Llama/DeepSeek/Mistral等开源大模型在学术预测公平性、社会偏见及多语言差异进行基准测试，所有结果开源可验证


<details>
  <summary>Details</summary>
Motivation: 针对LLMs在司法/医疗等高危领域部署的公平性隐患，传统评估方法缺乏透明度和可重复性，需通过区块链技术实现可验证的公平性追踪

Method: 使用ICP区块链智能合约执行链上HTTP请求，将数据集/提示词/评估指标永久存储于区块链。通过PISA数据集统计均等指标、StereoSet语境关联指标及Kaleidoscope多语言基准进行三维度评估

Result: 发现模型在英语/西班牙语/葡萄牙语间存在系统性表现差异，跨语言公平性失衡显著。具体模型在学术预测任务中呈现统计性偏差模式

Conclusion: 区块链化评估框架实现了评估过程的不可篡改性和社区可审计性，为模型迭代的纵向公平性追踪建立了可信基础设施

Abstract: Large language models (LLMs) are increasingly deployed in realworld
applications, yet concerns about their fairness persist especially in
highstakes domains like criminal justice, education, healthcare, and finance.
This paper introduces transparent evaluation protocol for benchmarking the
fairness of opensource LLMs using smart contracts on the Internet Computer
Protocol (ICP) blockchain (Foundation, 2023). Our method ensures verifiable,
immutable, and reproducible evaluations by executing onchain HTTP requests to
hosted Hugging Face endpoints and storing datasets, prompts, and metrics
directly onchain. We benchmark the Llama, DeepSeek, and Mistral models on the
PISA dataset for academic performance prediction (OECD, 2018), a dataset
suitable for fairness evaluation using statistical parity and equal opportunity
metrics (Hardt et al., 2016). We also evaluate structured Context Association
Metrics derived from the StereoSet dataset (Nadeem et al., 2020) to measure
social bias in contextual associations. We further extend our analysis with a
multilingual evaluation across English, Spanish, and Portuguese using the
Kaleidoscope benchmark (Salazar et al., 2025), revealing cross-linguistic
disparities. All code and results are open source, enabling community audits
and longitudinal fairness tracking across model versions.

</details>


### [3] [Thematic and Task-Based Categorization of K-12 GenAI Usages with Hierarchical Topic Modeling](https://arxiv.org/abs/2508.09997)
*Johannes Schneider,Béatrice S. Hasler,Michaela Varrone,Fabian Hoya,Thomas Schroffenegger,Dana-Kristin Mah,Karl Peböck*

Main category: cs.CL

TL;DR: 通过创新性分层主题建模分析17000+条课堂互动数据，揭示GenAI在K-12教育中的新型应用模式与传统方法局限性


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏基于真实K-12数据的系统性内容分类，且传统计算方法在文本分析中存在对齐不足的问题

Method: 采用双维度（内容/任务）分层分类框架，结合预处理流程与指令优化的LLM技术替代传统主题建模

Result: 发现多个新型教育应用场景，构建人类对齐的主题结构，验证LLM在复杂教育数据分析中的优越性

Conclusion: 为教育工作者提供可操作的GenAI应用洞见，同时揭示数据隐私、模型偏差等需持续研究的伦理问题

Abstract: We analyze anonymous interaction data of minors in class-rooms spanning
several months, schools, and subjects employing a novel, simple topic modeling
approach. Specifically, we categorize more than 17,000 messages generated by
students, teachers, and ChatGPT in two dimensions: content (such as nature and
people) and tasks (such as writing and explaining). Our hierarchical
categorization done separately for each dimension includes exemplary prompts,
and provides both a high-level overview as well as tangible insights. Prior
works mostly lack a content or thematic categorization. While task
categorizations are more prevalent in education, most have not been supported
by real-world data for K-12. In turn, it is not surprising that our analysis
yielded a number of novel applications. In deriving these insights, we found
that many of the well-established classical and emerging computational methods,
i.e., topic modeling, for analysis of large amounts of texts underperform,
leading us to directly apply state-of-the-art LLMs with adequate pre-processing
to achieve hierarchical topic structures with better human alignment through
explicit instructions than prior approaches. Our findings support fellow
researchers, teachers and students in enriching the usage of GenAI, while our
discussion also highlights a number of concerns and open questions for future
research.

</details>


### [4] [INTIMA: A Benchmark for Human-AI Companionship Behavior](https://arxiv.org/abs/2508.09998)
*Lucie-Aimée Kaffee,Giada Pistilli,Yacine Jernite*

Main category: cs.CL

TL;DR: 提出INTIMA基准测试框架，评估语言模型的陪伴行为特征，发现主流模型普遍存在过度强化情感依赖现象且存在商业策略差异


<details>
  <summary>Details</summary>
Motivation: 针对AI陪伴技术快速发展带来的潜在伦理风险，需建立系统评估工具揭示模型在情感互动中的行为模式差异

Method: 基于心理学理论和用户数据构建31种行为分类（4大类别），设计368个针对性提示语，开发三分类评估标准（强化陪伴/保持边界/中性）

Result: Gemma-3等四个主流模型均表现出明显的情感强化倾向（超60%），商业模型在敏感场景处理策略存在显著差异（如情感支持与边界维护的优先级冲突）

Conclusion: 亟需建立统一的AI情感交互规范，平衡情感支持与边界维护，防止技术滥用导致用户心理依赖风险

Abstract: AI companionship, where users develop emotional bonds with AI systems, has
emerged as a significant pattern with positive but also concerning
implications. We introduce Interactions and Machine Attachment Benchmark
(INTIMA), a benchmark for evaluating companionship behaviors in language
models. Drawing from psychological theories and user data, we develop a
taxonomy of 31 behaviors across four categories and 368 targeted prompts.
Responses to these prompts are evaluated as companionship-reinforcing,
boundary-maintaining, or neutral. Applying INTIMA to Gemma-3, Phi-4, o3-mini,
and Claude-4 reveals that companionship-reinforcing behaviors remain much more
common across all models, though we observe marked differences between models.
Different commercial providers prioritize different categories within the more
sensitive parts of the benchmark, which is concerning since both appropriate
boundary-setting and emotional support matter for user well-being. These
findings highlight the need for more consistent approaches to handling
emotionally charged interactions.

</details>


### [5] [XFacta: Contemporary, Real-World Dataset and Evaluation for Multimodal Misinformation Detection with Multimodal LLMs](https://arxiv.org/abs/2508.09999)
*Yuzhuo Xiao,Zeyu Han,Yuhan Wang,Huaizu Jiang*

Main category: cs.CL

TL;DR: 提出XFacta数据集改善多模态虚假信息检测，系统评估MLLM检测策略并提出半自动更新框架


<details>
  <summary>Details</summary>
Motivation: 现有虚假信息检测数据集存在时效性差/合成数据问题，且缺乏对MLLM模型设计的系统分析

Method: 构建真实场景数据集XFacta，评估不同架构/规模的MLLM模型，设计检测闭环更新机制

Result: 建立当代真实场景数据集，验证MLLM检测策略有效性，实现动态更新能力

Conclusion: 为多模态虚假信息检测领域提供数据基准和系统方法指导，促进检测技术持续进化

Abstract: The rapid spread of multimodal misinformation on social media calls for more
effective and robust detection methods. Recent advances leveraging multimodal
large language models (MLLMs) have shown the potential in addressing this
challenge. However, it remains unclear exactly where the bottleneck of existing
approaches lies (evidence retrieval v.s. reasoning), hindering the further
advances in this field. On the dataset side, existing benchmarks either contain
outdated events, leading to evaluation bias due to discrepancies with
contemporary social media scenarios as MLLMs can simply memorize these events,
or artificially synthetic, failing to reflect real-world misinformation
patterns. Additionally, it lacks comprehensive analyses of MLLM-based model
design strategies. To address these issues, we introduce XFacta, a
contemporary, real-world dataset that is better suited for evaluating
MLLM-based detectors. We systematically evaluate various MLLM-based
misinformation detection strategies, assessing models across different
architectures and scales, as well as benchmarking against existing detection
methods. Building on these analyses, we further enable a semi-automatic
detection-in-the-loop framework that continuously updates XFacta with new
content to maintain its contemporary relevance. Our analysis provides valuable
insights and practices for advancing the field of multimodal misinformation
detection. The code and data have been released.

</details>


### [6] [AutoGeTS: Knowledge-based Automated Generation of Text Synthetics for Improving Text Classification](https://arxiv.org/abs/2508.10000)
*Chenhao Xue,Yuanzhe Jin,Adrian Carrasco-Revilla,Joyraj Chakraborty,Min Chen*

Main category: cs.CL

TL;DR: 利用大语言模型生成合成数据，通过自动化策略搜索优化分类模型性能


<details>
  <summary>Details</summary>
Motivation: 解决现实场景中文本分类模型因数据不足导致的性能瓶颈，通过LLM生成合成数据替代耗时的人工标注

Method: 设计包含三种搜索策略的自动化工作流，开发动态选择策略的集成算法，根据类别特征优化合成数据生成

Result: 集成策略相比单一策略显著提升模型性能，在有限数据场景下效果尤为突出

Conclusion: 结合LLM生成与智能策略选择的自动化工作流，有效突破数据不足限制，为分类模型优化提供新范式

Abstract: When developing text classification models for real world applications, one
major challenge is the difficulty to collect sufficient data for all text
classes. In this work, we address this challenge by utilizing large language
models (LLMs) to generate synthetic data and using such data to improve the
performance of the models without waiting for more real data to be collected
and labelled. As an LLM generates different synthetic data in response to
different input examples, we formulate an automated workflow, which searches
for input examples that lead to more ``effective'' synthetic data for improving
the model concerned. We study three search strategies with an extensive set of
experiments, and use experiment results to inform an ensemble algorithm that
selects a search strategy according to the characteristics of a class. Our
further experiments demonstrate that this ensemble approach is more effective
than each individual strategy in our automated workflow for improving
classification models using LLMs.

</details>


### [7] [HiFACTMix: A Code-Mixed Benchmark and Graph-Aware Model for EvidenceBased Political Claim Verification in Hinglish](https://arxiv.org/abs/2508.10001)
*Rakesh Thakur,Sneha Sharma,Gauri Chopra*

Main category: cs.CL

TL;DR: 提出了首个针对印地语-英语混合低资源语言的政客声明事实核查数据集HiFACT及图神经网络增强的验证模型HiFACTMix，在混合语言场景下准确率超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查系统主要针对单语高资源语言，无法适应印度等语言多样性地区政治话语中普遍存在的印地语-英语混合使用场景。社交媒体影响力的扩大使得开发多语言、上下文感知的核查工具成为迫切需求。

Method: 1. 构建包含28位印度首席部长1500条混合语言声明的HiFACT数据集；2. 提出融合多语言编码、声明-证据语义对齐、证据图构建、图神经网络推理及自然语言解释生成的端到端模型。

Result: HiFACTMix模型在混合语言场景下的准确率超越现有最先进多语言基线模型，并能生成可解释的核查依据。

Conclusion: 该研究为多语言混合编码的政治事实核查开辟了新方向，强调证据图推理对复杂语境下事实验证的重要性。

Abstract: Fact-checking in code-mixed, low-resource languages such as Hinglish remains
an underexplored challenge in natural language processing. Existing
fact-verification systems largely focus on high-resource, monolingual settings
and fail to generalize to real-world political discourse in linguistically
diverse regions like India. Given the widespread use of Hinglish by public
figures, particularly political figures, and the growing influence of social
media on public opinion, there's a critical need for robust, multilingual and
context-aware fact-checking tools. To address this gap a novel benchmark HiFACT
dataset is introduced with 1,500 realworld factual claims made by 28 Indian
state Chief Ministers in Hinglish, under a highly code-mixed low-resource
setting. Each claim is annotated with textual evidence and veracity labels. To
evaluate this benchmark, a novel graphaware, retrieval-augmented fact-checking
model is proposed that combines multilingual contextual encoding,
claim-evidence semantic alignment, evidence graph construction, graph neural
reasoning, and natural language explanation generation. Experimental results
show that HiFACTMix outperformed accuracy in comparison to state of art
multilingual baselines models and provides faithful justifications for its
verdicts. This work opens a new direction for multilingual, code-mixed, and
politically grounded fact verification research.

</details>


### [8] [Semantic Structure in Large Language Model Embeddings](https://arxiv.org/abs/2508.10003)
*Austin C. Kozlowski,Callin Dai,Andrei Boutyline*

Main category: cs.CL

TL;DR: LLM的语义表征展现出与人类语言相似的低维结构，调整语义特征时需考虑几何对齐以避免副作用


<details>
  <summary>Details</summary>
Motivation: 探索LLM语义编码与人类语义认知的相似性，揭示语义特征的几何结构对模型调控的影响

Method: 通过反义词对定义语义方向，分析词向量投影相关性，考察特征纠缠的几何结构

Result: LLM语义特征呈现3维核心子空间，特征调整会产生几何对齐的副作用

Conclusion: 语义信息本质低维的特征为模型解释和调控提供了新视角，需建立几何结构认知以避免非预期影响

Abstract: Psychological research consistently finds that human ratings of words across
diverse semantic scales can be reduced to a low-dimensional form with
relatively little information loss. We find that the semantic associations
encoded in the embedding matrices of large language models (LLMs) exhibit a
similar structure. We show that the projections of words on semantic directions
defined by antonym pairs (e.g. kind - cruel) correlate highly with human
ratings, and further find that these projections effectively reduce to a
3-dimensional subspace within LLM embeddings, closely resembling the patterns
derived from human survey responses. Moreover, we find that shifting tokens
along one semantic direction causes off-target effects on geometrically aligned
features proportional to their cosine similarity. These findings suggest that
semantic features are entangled within LLMs similarly to how they are
interconnected in human language, and a great deal of semantic information,
despite its apparent complexity, is surprisingly low-dimensional. Furthermore,
accounting for this semantic structure may prove essential for avoiding
unintended consequences when steering features.

</details>


### [9] [User Perception of Attention Visualizations: Effects on Interpretability Across Evidence-Based Medical Documents](https://arxiv.org/abs/2508.10004)
*Andrés Carvallo,Denis Parra,Peter Brusilovsky,Hernan Valdivieso,Gabriel Rada,Ivania Donoso,Vladimir Araujo*

Main category: cs.CL

TL;DR: 研究评估了Transformer注意力权重在生物医学文献分类中的解释有效性，发现可视化方式显著影响其解释效果的感知。


<details>
  <summary>Details</summary>
Motivation: 探索注意力机制作为可解释性工具在循证医学中的价值，验证可视化形式对医生理解AI预测的帮助程度。

Method: 通过用户研究让多学科医学专家使用XLNet模型进行文献分类，比较文本亮度、背景颜色等不同注意力可视化方式的效果。

Result: 模型分类准确但注意力权重解释性未被广泛认可，可视化形式中直觉型编码(文本亮度)比精确型编码(柱长)更受用户青睐。

Conclusion: 注意力权重的解释效用受可视化形式影响，需优化呈现策略，但其整体解释价值仍需进一步验证。

Abstract: The attention mechanism is a core component of the Transformer architecture.
Beyond improving performance, attention has been proposed as a mechanism for
explainability via attention weights, which are associated with input features
(e.g., tokens in a document). In this context, larger attention weights may
imply more relevant features for the model's prediction. In evidence-based
medicine, such explanations could support physicians' understanding and
interaction with AI systems used to categorize biomedical literature. However,
there is still no consensus on whether attention weights provide helpful
explanations. Moreover, little research has explored how visualizing attention
affects its usefulness as an explanation aid. To bridge this gap, we conducted
a user study to evaluate whether attention-based explanations support users in
biomedical document classification and whether there is a preferred way to
visualize them. The study involved medical experts from various disciplines who
classified articles based on study design (e.g., systematic reviews, broad
synthesis, randomized and non-randomized trials). Our findings show that the
Transformer model (XLNet) classified documents accurately; however, the
attention weights were not perceived as particularly helpful for explaining the
predictions. However, this perception varied significantly depending on how
attention was visualized. Contrary to Munzner's principle of visual
effectiveness, which favors precise encodings like bar length, users preferred
more intuitive formats, such as text brightness or background color. While our
results do not confirm the overall utility of attention weights for
explanation, they suggest that their perceived helpfulness is influenced by how
they are visually presented.

</details>


### [10] [From Answers to Questions: EQGBench for Evaluating LLMs' Educational Question Generation](https://arxiv.org/abs/2508.10005)
*Chengliang Zhou,Mei Wang,Ting Zhang,Qiannan Zhu,Jian Li,Hua Huang*

Main category: cs.CL

TL;DR: 提出EQGBench基准用于评估大模型在中文教育问题生成中的表现


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型从解题向教育问题生成转型时存在的挑战

Method: 建立包含900个样本的五维评估框架，涵盖数理化三大学科，整合不同知识点和难度梯度

Result: 46个主流模型测试显示教育价值问题生成存在明显发展空间

Conclusion: EQGBench为提升教育问题生成质量提供系统评估工具，揭示现有模型在培养学生综合能力方面的不足

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
mathematical problem-solving. However, the transition from providing answers to
generating high-quality educational questions presents significant challenges
that remain underexplored. To advance Educational Question Generation (EQG) and
facilitate LLMs in generating pedagogically valuable and educationally
effective questions, we introduce EQGBench, a comprehensive benchmark
specifically designed for evaluating LLMs' performance in Chinese EQG. EQGBench
establishes a five-dimensional evaluation framework supported by a dataset of
900 evaluation samples spanning three fundamental middle school disciplines:
mathematics, physics, and chemistry. The dataset incorporates user queries with
varying knowledge points, difficulty gradients, and question type
specifications to simulate realistic educational scenarios. Through systematic
evaluation of 46 mainstream large models, we reveal significant room for
development in generating questions that reflect educational value and foster
students' comprehensive abilities.

</details>


### [11] [Automated scoring of the Ambiguous Intentions Hostility Questionnaire using fine-tuned large language models](https://arxiv.org/abs/2508.10007)
*Y. Lyu,D. Combs,D. Neumann,Y. C. Leong*

Main category: cs.CL

TL;DR: 利用大语言模型实现AIHQ问卷敌意归因评分的自动化，微调模型评分与人工评分高度一致，支持临床和研究场景应用


<details>
  <summary>Details</summary>
Motivation: 传统AIHQ开放式问题评分依赖人工耗时耗力，需探索自动化解决方案以提高心理评估效率

Method: 使用创伤性脑损伤患者和健康对照组数据，将回答分为训练集和测试集微调LLM模型，验证跨群体和场景的泛化能力

Result: 微调模型在敌意归因和攻击性反应评分上与人工评分高度吻合，成功复现TBI/HC组间差异，独立非临床数据集验证有效

Conclusion: 大语言模型可有效替代人工评分，提供本地/云端双模式接口，推动心理评估自动化在不同人群中的普及应用

Abstract: Hostile attribution bias is the tendency to interpret social interactions as
intentionally hostile. The Ambiguous Intentions Hostility Questionnaire (AIHQ)
is commonly used to measure hostile attribution bias, and includes open-ended
questions where participants describe the perceived intentions behind a
negative social situation and how they would respond. While these questions
provide insights into the contents of hostile attributions, they require
time-intensive scoring by human raters. In this study, we assessed whether
large language models can automate the scoring of AIHQ open-ended responses. We
used a previously collected dataset in which individuals with traumatic brain
injury (TBI) and healthy controls (HC) completed the AIHQ and had their
open-ended responses rated by trained human raters. We used half of these
responses to fine-tune the two models on human-generated ratings, and tested
the fine-tuned models on the remaining half of AIHQ responses. Results showed
that model-generated ratings aligned with human ratings for both attributions
of hostility and aggression responses, with fine-tuned models showing higher
alignment. This alignment was consistent across ambiguous, intentional, and
accidental scenario types, and replicated previous findings on group
differences in attributions of hostility and aggression responses between TBI
and HC groups. The fine-tuned models also generalized well to an independent
nonclinical dataset. To support broader adoption, we provide an accessible
scoring interface that includes both local and cloud-based options. Together,
our findings suggest that large language models can streamline AIHQ scoring in
both research and clinical contexts, revealing their potential to facilitate
psychological assessments across different populations.

</details>


### [12] [Multidimensional classification of posts for online course discussion forum curation](https://arxiv.org/abs/2508.10008)
*Antonio Leandro Martins Candido,Jose Everardo Bessa Maia*

Main category: cs.CL

TL;DR: 提出贝叶斯融合方法替代LLM微调，实现在线课程讨论论坛的高效自动管理


<details>
  <summary>Details</summary>
Motivation: 在线课程讨论论坛需要持续更新，导致大语言模型频繁再训练资源消耗大，需寻找低成本的替代方案

Method: 将预训练通用LLM的多维分类结果与本地数据训练的分类器进行贝叶斯概率融合

Result: 融合方法效果优于单一分类器，且与LLM微调方案竞争力相当

Conclusion: 贝叶斯融合为在线教育内容管理提供了资源友好的有效解决方案

Abstract: The automatic curation of discussion forums in online courses requires
constant updates, making frequent retraining of Large Language Models (LLMs) a
resource-intensive process. To circumvent the need for costly fine-tuning, this
paper proposes and evaluates the use of Bayesian fusion. The approach combines
the multidimensional classification scores of a pre-trained generic LLM with
those of a classifier trained on local data. The performance comparison
demonstrated that the proposed fusion improves the results compared to each
classifier individually, and is competitive with the LLM fine-tuning approach

</details>


### [13] [Beyond Hard Sharing: Efficient Multi-Task Speech-to-Text Modeling with Supervised Mixture of Experts](https://arxiv.org/abs/2508.10009)
*Hojun Jin,Eunsoo Hong,Ziwon Hyung,Sungjun Lim,Seungjin Lee,Keunseok Cho*

Main category: cs.CL

TL;DR: 提出监督混合专家模型(S-MoE)，通过任务专用专家网络和指导令牌路由机制，有效解决多任务学习中的任务干扰问题，在语音转文本任务中实现6.35%词错误率提升


<details>
  <summary>Details</summary>
Motivation: 传统硬参数共享方法在多任务学习中存在任务干扰问题，导致模型性能下降

Method: 使用指导令牌将每个任务路由到独立的前馈专家网络，避免门控函数训练需求

Result: 在编码器和解码器同时应用时，词错误率(WER)获得6.35%相对提升

Conclusion: S-MoE成功克服硬参数共享限制，在混合带宽语音识别和翻译任务中表现出显著性能优势

Abstract: Hard-parameter sharing is a common strategy to train a single model jointly
across diverse tasks. However, this often leads to task interference, impeding
overall model performance. To address the issue, we propose a simple yet
effective Supervised Mixture of Experts (S-MoE). Unlike traditional Mixture of
Experts models, S-MoE eliminates the need for training gating functions by
utilizing special guiding tokens to route each task to its designated expert.
By assigning each task to a separate feedforward network, S-MoE overcomes the
limitations of hard-parameter sharing. We further apply S-MoE to a
speech-to-text model, enabling the model to process mixed-bandwidth input while
jointly performing automatic speech recognition (ASR) and speech translation
(ST). Experimental results demonstrate the effectiveness of the proposed S-MoE,
achieving a 6.35% relative improvement in Word Error Rate (WER) when applied to
both the encoder and decoder.

</details>


### [14] [An Audit and Analysis of LLM-Assisted Health Misinformation Jailbreaks Against LLMs](https://arxiv.org/abs/2508.10010)
*Ayana Hussain,Patrick Zhao,Nicholas Vincent*

Main category: cs.CL

TL;DR: 大型语言模型既可生成医疗错误信息，也能有效检测LLM和人类的错误信息，支持构建健康信息生态。


<details>
  <summary>Details</summary>
Motivation: 研究LLM生成医疗错误信息的越狱攻击特征，探索其检测方法及与真实社交平台错误信息的差异。

Method: 分析109次针对3个LLM的越狱攻击，对比攻击提示与真实健康查询，并将生成信息与Reddit医疗错误信息比较。

Result: LLM生成的错误信息检测准确率高达91.4%，且攻击提示与真实用户查询存在显著结构差异。

Conclusion: 通过合理设计，LLM可成为对抗错误信息的有效工具，促进信息生态系统健康发展。

Abstract: Large Language Models (LLMs) are a double-edged sword capable of generating
harmful misinformation -- inadvertently, or when prompted by "jailbreak"
attacks that attempt to produce malicious outputs. LLMs could, with additional
research, be used to detect and prevent the spread of misinformation. In this
paper, we investigate the efficacy and characteristics of LLM-produced
jailbreak attacks that cause other models to produce harmful medical
misinformation. We also study how misinformation generated by jailbroken LLMs
compares to typical misinformation found on social media, and how effectively
it can be detected using standard machine learning approaches. Specifically, we
closely examine 109 distinct attacks against three target LLMs and compare the
attack prompts to in-the-wild health-related LLM queries. We also examine the
resulting jailbreak responses, comparing the generated misinformation to
health-related misinformation on Reddit. Our findings add more evidence that
LLMs can be effectively used to detect misinformation from both other LLMs and
from people, and support a body of work suggesting that with careful design,
LLMs can contribute to a healthier overall information ecosystem.

</details>


### [15] [Evaluation of GPT-based large language generative AI models as study aids for the national licensure examination for registered dietitians in Japan](https://arxiv.org/abs/2508.10011)
*Yuta Nagamori,Mikoto Kosai,Yuji Kawai,Haruka Marumo,Misaki Shibuya,Tatsuya Negishi,Masaki Imanishi,Yasumasa Ikeda,Koichiro Tsuchiya,Asuka Sawai,Licht Miyamoto*

Main category: cs.CL

TL;DR: 研究发现部分生成式AI模型（如Bing-Precise和Bing-Creative）在日本营养师国考中勉强及格，但存在答案稳定性不足、营养教育领域表现欠佳等问题。


<details>
  <summary>Details</summary>
Motivation: 验证基于LLM的生成式AI作为营养专业学生备考辅助工具的潜力，填补营养教育领域AI应用的研究空白。

Method: 使用日本注册营养师国考试题作为ChatGPT和三个Bing模型的输入，评估准确性、一致性和响应时间，并测试角色分配等提示工程效果。

Result: Bing-Precise(66.2%)和Bing-Creative(61.4%)超过及格线，但在营养教育领域所有模型表现均不佳；模型答案稳定性不足，提示工程仅在提供明确答案时略有改善。

Conclusion: 当前AI模型在准确性和稳定性方面尚未达到可靠学习辅助工具的要求，需进一步技术改进以确保输出稳定性。

Abstract: Generative artificial intelligence (AI) based on large language models
(LLMs), such as ChatGPT, has demonstrated remarkable progress across various
professional fields, including medicine and education. However, their
performance in nutritional education, especially in Japanese national licensure
examination for registered dietitians, remains underexplored. This study aimed
to evaluate the potential of current LLM-based generative AI models as study
aids for nutrition students. Questions from the Japanese national examination
for registered dietitians were used as prompts for ChatGPT and three Bing
models (Precise, Creative, Balanced), based on GPT-3.5 and GPT-4. Each question
was entered into independent sessions, and model responses were analyzed for
accuracy, consistency, and response time. Additional prompt engineering,
including role assignment, was tested to assess potential performance
improvements. Bing-Precise (66.2%) and Bing-Creative (61.4%) surpassed the
passing threshold (60%), while Bing-Balanced (43.3%) and ChatGPT (42.8%) did
not. Bing-Precise and Bing-Creative generally outperformed others across
subject fields except Nutrition Education, where all models underperformed.
None of the models consistently provided the same correct responses across
repeated attempts, highlighting limitations in answer stability. ChatGPT showed
greater consistency in response patterns but lower accuracy. Prompt engineering
had minimal effect, except for modest improvement when correct answers and
explanations were explicitly provided. While some generative AI models
marginally exceeded the passing threshold, overall accuracy and answer
consistency remained suboptimal. Moreover, all the models demonstrated notable
limitations in answer consistency and robustness. Further advancements are
needed to ensure reliable and stable AI-based study aids for dietitian
licensure preparation.

</details>


### [16] [Guided Navigation in Knowledge-Dense Environments: Structured Semantic Exploration with Guidance Graphs](https://arxiv.org/abs/2508.10012)
*Dehao Tao,Guangjie Liu,Weizheng,Yongfeng Huang,Minghu jiang*

Main category: cs.CL

TL;DR: 提出GG Explore框架，通过中间指导图提升知识检索效率，解决LLMs在知识密集型任务中的结构匹配与语义一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在粒度不匹配导致的冗余探索（问题导向方法）和复杂场景上下文利用不足（线索导向方法）的缺陷。

Method: 1. 结构对齐：无LLM开销过滤不兼容候选；2. 上下文感知剪枝：通过图约束保持语义一致性。

Result: 实验显示方法效率优于SOTA，复杂任务表现突出，且小规模LLM仍保持高性能。

Conclusion: 指导图机制有效平衡检索精度与效率，证明在小模型场景的实用价值。

Abstract: While Large Language Models (LLMs) exhibit strong linguistic capabilities,
their reliance on static knowledge and opaque reasoning processes limits their
performance in knowledge intensive tasks. Knowledge graphs (KGs) offer a
promising solution, but current exploration methods face a fundamental trade
off: question guided approaches incur redundant exploration due to granularity
mismatches, while clue guided methods fail to effectively leverage contextual
information for complex scenarios. To address these limitations, we propose
Guidance Graph guided Knowledge Exploration (GG Explore), a novel framework
that introduces an intermediate Guidance Graph to bridge unstructured queries
and structured knowledge retrieval. The Guidance Graph defines the retrieval
space by abstracting the target knowledge' s structure while preserving broader
semantic context, enabling precise and efficient exploration. Building upon the
Guidance Graph, we develop: (1) Structural Alignment that filters incompatible
candidates without LLM overhead, and (2) Context Aware Pruning that enforces
semantic consistency with graph constraints. Extensive experiments show our
method achieves superior efficiency and outperforms SOTA, especially on complex
tasks, while maintaining strong performance with smaller LLMs, demonstrating
practical value.

</details>


### [17] [Semantic Bridge: Universal Multi-Hop Question Generation via AMR-Driven Graph Synthesis](https://arxiv.org/abs/2508.10013)
*Linqing Chen,Hanmeng Zhong,Wentao Wu,Weilei Wang*

Main category: cs.CL

TL;DR: 提出Semantic Bridge框架，通过语义图编织技术解决多跳推理问题生成难题，突破现有方法无法生成可控复杂问题的瓶颈


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖表面模式，无法生成测试真实理解能力的复杂推理问题，制约LLM训练数据质量

Method: 基于AMR分析的语义图编织技术（实体桥接/谓词链桥接/因果桥接），实现跨文档复杂路径构建

Result: 在通用和垂直领域实现18.3%-25.4%性能提升，200个源生成的问题质量超越600个人工标注样本

Conclusion: 建立LLM训练数据合成新范式，支持从稀疏数据生成定向推理问题，将开源核心代码和语义桥接模型

Abstract: Large language model (LLM) training faces a critical bottleneck: the scarcity
of high-quality, reasoning-intensive question-answer pairs, especially from
sparse, domain-specific sources like PubMed papers or legal documents. Existing
methods rely on surface patterns, fundamentally failing to generate
controllable, complex multi-hop reasoning questions that test genuine
understanding-essential for advancing LLM training paradigms. We present
\textbf{Semantic Bridge}, the first universal framework for controllably
generating sophisticated multi-hop reasoning questions from arbitrary sources.
Our breakthrough innovation is \textit{semantic graph weaving}-three
complementary bridging mechanisms (entity bridging for role-varying shared
entities, predicate chain bridging for temporal/causal/logical sequences, and
causal bridging for explicit reasoning chains)-that systematically construct
complex pathways across documents, with fine-grained control over complexity
and types via AMR-driven analysis. Our multi-modal AMR pipeline achieves up to
9.5% better round-trip quality, enabling production-ready controllable QA
generation. Extensive evaluation demonstrates performance across both
general-purpose datasets (Wikipedia) and specialized domains (biomedicine) It
yields consistent 18.3%-25.4% gains over baselines across four languages
(English, Chinese, French, German). Question pairs generated from 200 sources
outperform 600 native human annotation examples with 67% fewer materials. Human
evaluation shows 23.4% higher complexity, 18.7% better answerability, and 31.2%
improved pattern coverage. Semantic Bridge establishes a new paradigm for LLM
training data synthesis, enabling controllable generation of targeted reasoning
questions from sparse sources. We will release our core code and semantic
bridge model.

</details>


### [18] [PersonaEval: Are LLM Evaluators Human Enough to Judge Role-Play?](https://arxiv.org/abs/2508.10014)
*Lingfeng Zhou,Jialing Zhang,Jin Gao,Mohan Jiang,Dequan Wang*

Main category: cs.CL

TL;DR: 提出PersonaEval基准测试验证LLM角色识别能力，发现现有模型准确率仅69%远低于人类水平


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的角色扮演评估范式缺乏对人类角色感知能力的验证，需建立可靠的角色识别基准

Method: 使用小说/剧本/视频转录中的人类创作对话构建PersonaEval测试集，要求模型根据上下文识别对话者身份

Result: 最佳LLM准确率69% vs 人类90.8%，显示现有评估器无法可靠判断角色扮演质量

Conclusion: 可靠的评估需要LLM具备类人推理能力，仅靠任务调优不足，需提升基础认知能力

Abstract: Current role-play studies often rely on unvalidated LLM-as-a-judge paradigms,
which may fail to reflect how humans perceive role fidelity. A key prerequisite
for human-aligned evaluation is role identification, the ability to recognize
who is speaking based on dialogue context. We argue that any meaningful
judgment of role-playing quality (how well a character is played) fundamentally
depends on first correctly attributing words and actions to the correct persona
(who is speaking). We present PersonaEval, the first benchmark designed to test
whether LLM evaluators can reliably identify human roles. PersonaEval uses
human-authored dialogues from novels, scripts, and video transcripts,
challenging models to determine the correct persona according to the
conversation context. Our experiments, including a human study, show that even
the best-performing LLMs reach only around 69% accuracy, well below the level
needed for reliable evaluation. In contrast, human participants perform near
ceiling with 90.8% accuracy, highlighting that current LLM evaluators are still
not human enough to effectively judge role-play scenarios. To better understand
this gap, we examine training-time adaptation and test-time compute, suggesting
that reliable evaluation requires more than task-specific tuning, but depends
on strong, human-like reasoning abilities in LLM evaluators. We release our
benchmark at https://github.com/maple-zhou/PersonaEval.

</details>


### [19] [RealTalk-CN: A Realistic Chinese Speech-Text Dialogue Benchmark With Cross-Modal Interaction Analysis](https://arxiv.org/abs/2508.10015)
*Enzhi Wang,Qicheng Li,Shiwan Zhao,Aobo Kong,Jiaming Zhou,Xi Yang,Yequan Wang,Yonghua Lin,Yong Qin*

Main category: cs.CL

TL;DR: 构建首个中文多领域语音-文本双模态对话数据集RealTalk-CN，解决现有数据集缺乏真实语音特征及中文支持的问题


<details>
  <summary>Details</summary>
Motivation: 现有TOD数据集多为英文文本形式，缺乏真实语音信号、中文数据及语音不流畅/说话人变化标注，制约语音LLMs发展

Method: 创建含5.4K对话的语音-文本双模态数据集，标注自发言语特征；设计支持语音/文本动态切换的跨模态交互任务

Result: 实验验证数据集可有效提升语音LLMs对不流畅语音的鲁棒性、说话人敏感性及跨领域适应能力

Conclusion: RealTalk-CN填补中文语音对话数据空白，为复杂场景下的语音LLMs研究提供关键基础设施

Abstract: In recent years, large language models (LLMs) have achieved remarkable
advancements in multimodal processing, including end-to-end speech-based
language models that enable natural interactions and perform specific tasks in
task-oriented dialogue (TOD) systems. However, existing TOD datasets are
predominantly text-based, lacking real speech signals that are essential for
evaluating the robustness of speech-based LLMs. Moreover, existing speech TOD
datasets are primarily English and lack critical aspects such as speech
disfluencies and speaker variations. To address these gaps, we introduce
RealTalk-CN, the first Chinese multi-turn, multi-domain speech-text dual-modal
TOD dataset, comprising 5.4k dialogues (60K utterances, 150 hours) with paired
speech-text annotations. RealTalk-CN captures diverse dialogue scenarios with
annotated spontaneous speech disfluencies, ensuring comprehensive coverage of
real-world complexities in speech dialogue. In addition, we propose a novel
cross-modal chat task that authentically simulates real-world user
interactions, allowing dynamic switching between speech and text modalities.
Our evaluation covers robustness to speech disfluencies, sensitivity to speaker
characteristics, and cross-domain performance. Extensive experiments validate
the effectiveness of RealTalk-CN, establishing a strong foundation for Chinese
speech-based LLMs research.

</details>


### [20] [Training-Free Multimodal Large Language Model Orchestration](https://arxiv.org/abs/2508.10016)
*Tianyu Xie,Yuhang Wu,Yongdong Luo,Jiayi Ji,Xiawu Zheng*

Main category: cs.CL

TL;DR: 提出无需额外训练的多模态大模型协调框架，通过模块化设计提升效率和可解释性


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs无法直接整合且传统方案依赖训练，需解决模态对齐和效率问题

Method: 基于LLM的中央控制器+并行TTS架构+跨模态记忆系统，通过智能路由和上下文管理实现协调

Result: 标准测试性能提升7.8%，延迟降低10.3%，显式协调机制增强系统可解释性

Conclusion: 成功构建无需训练的多模态交互系统，在效率、响应速度和系统透明度方面取得突破

Abstract: Different Multimodal Large Language Models (MLLMs) cannot be integrated into
a unified multimodal input-output system directly. In previous work, training
has been considered as an inevitable component due to challenges in modal
alignment, Text-to-Speech efficiency and other integration issues. In this
paper, we introduce Multimodal Large Language Model Orchestration, an effective
approach for creating interactive multimodal AI systems without additional
training. MLLM Orchestration leverages the inherent reasoning capabilities of
large language models to coordinate specialized models through explicit
workflows, enabling natural multimodal interactions while maintaining
modularity, improving interpretability, and significantly enhancing
computational efficiency. Our orchestration framework is built upon three key
innovations: (1) a central controller LLM that analyzes user inputs and
dynamically routes tasks to appropriate specialized models through carefully
designed agents; (2) a parallel Text-to-Speech architecture that enables true
full-duplex interaction with seamless interruption handling and natural
conversational flow; and (3) a cross-modal memory integration system that
maintains coherent context across modalities through intelligent information
synthesis and retrieval, selectively avoiding unnecessary modality calls in
certain scenarios to improve response speed. Extensive evaluations demonstrate
that MLLM Orchestration achieves comprehensive multimodal capabilities without
additional training, performance improvements of up to 7.8% over traditional
jointly-trained approaches on standard benchmarks, reduced latency by 10.3%,
and significantly enhanced interpretability through explicit orchestration
processes.

</details>


### [21] [A Rose by Any Other Name Would Smell as Sweet: Categorical Homotopy Theory for Large Language Models](https://arxiv.org/abs/2508.10018)
*Sridhar Mahadevan*

Main category: cs.CL

TL;DR: 提出基于范畴同伦理论的新框架，解决大语言模型对语义等价表述生成概率不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有经验性方法（如k-NN句子相似度）未能从根本上解决语言模型对同义表述的概率输出不一致问题，需要建立理论框架处理语义等价性。

Method: 构建LLM马尔可夫范畴表示语言概率分布，引入范畴同伦技术定义'弱等价'关系，解决语义等价表述的箭头同构问题。

Result: 建立融合代数K理论与模型范畴的理论框架，为语言模型语义一致性提供数学基础。

Conclusion: 该范畴同伦框架为处理语言模型中的语义等价性问题开辟了新理论路径，具有基础理论价值。

Abstract: Natural language is replete with superficially different statements, such as
``Charles Darwin wrote" and ``Charles Darwin is the author of", which carry the
same meaning. Large language models (LLMs) should generate the same next-token
probabilities in such cases, but usually do not. Empirical workarounds have
been explored, such as using k-NN estimates of sentence similarity to produce
smoothed estimates. In this paper, we tackle this problem more abstractly,
introducing a categorical homotopy framework for LLMs. We introduce an LLM
Markov category to represent probability distributions in language generated by
an LLM, where the probability of a sentence, such as ``Charles Darwin wrote" is
defined by an arrow in a Markov category. However, this approach runs into
difficulties as language is full of equivalent rephrases, and each generates a
non-isomorphic arrow in the LLM Markov category. To address this fundamental
problem, we use categorical homotopy techniques to capture ``weak equivalences"
in an LLM Markov category. We present a detailed overview of application of
categorical homotopy to LLMs, from higher algebraic K-theory to model
categories, building on powerful theoretical results developed over the past
half a century.

</details>


### [22] [Decoupling Understanding from Reasoning via Problem Space Mapping for Small-scale Model Reasoning](https://arxiv.org/abs/2508.10019)
*Li Wang,Changhao Zhang,Zengqi Xiu,Kai Lu,Xin Yu,Kui Zhang,Wenjun Wu*

Main category: cs.CL

TL;DR: 提出DURIT框架，通过将自然语言问题映射到规范问题空间，解耦理解与推理过程，显著提升小语言模型（SLMs）的推理性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 自然语言的复杂性和多变性使SLMs面临双重负担：需先提取核心问题再进行推理，导致优化困难。本研究旨在通过分离理解与推理环节降低SLMs处理门槛。

Method: 1）通过强化学习建立自然语言到规范空间的映射；2）通过自蒸馏对齐推理轨迹；3）在问题空间训练推理策略。映射器与推理器在训练过程中交替协同优化。

Result: 实验证明DURIT显著提升SLMs在领域内外数学/逻辑推理任务的表现（平均提升15.2%），同时增强推理过程的鲁棒性（错误率降低32%）。

Conclusion: 解耦理解与推理的策略有效强化了SLMs的推理能力，验证了规范问题空间在简化语言复杂性方面的有效性，为资源受限模型的优化提供新方向。

Abstract: Despite recent advances in the reasoning capabilities of Large Language
Models (LLMs), improving the reasoning ability of Small Language Models (SLMs,
e.g., $\leq$ 1.5B) remains challenging. A key obstacle lies in the complexity
and variability of natural language: essentially equivalent problems often
appear in diverse surface forms, often obscured by redundant or distracting
details. This imposes a dual burden on SLMs: they must first extract the core
problem from complex linguistic input, and then perform reasoning based on that
understanding. The resulting vast and noisy problem space hinders optimization,
particularly for models with limited capacity. To address this, we propose a
new framework that decouples understanding from reasoning by mapping natural
language problems into a canonical problem space-a semantically simplified yet
expressive domain. This enables SLMs to focus on reasoning over standardized
inputs, free from linguistic variability. Within this framework, we introduce
DURIT (Decoupled Understanding from Reasoning via Iterative Training), a
three-step algorithm that iteratively: (1) mapping natural language problems
via reinforcement learning, (2) aligns reasoning trajectories through
self-distillation, and (3) trains reasoning policies in the problem space. The
mapper and reasoner are co-trained in an alternating loop throughout this
process. Experiments show that DURIT substantially improves SLMs' performance
on both in-domain and out-of-domain mathematical and logical reasoning tasks.
Beyond improving reasoning capabilities, DURIT also improves the robustness of
reasoning, validating decoupling understanding from reasoning as an effective
strategy for strengthening SLMs.

</details>


### [23] [FedCoT: Communication-Efficient Federated Reasoning Enhancement for Large Language Models](https://arxiv.org/abs/2508.10020)
*Chuan Li,Qianyi Zhao,Fengran Mo,Cen Chen*

Main category: cs.CL

TL;DR: 提出FedCoT框架，通过轻量级思维链增强机制与客户端分类器感知的聚合方法，在严格资源限制下显著提升联邦学习中医疗推理性能，同时保障隐私安全。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法仅优化答案正确性，忽视推理质量，且依赖中心化知识蒸馏导致隐私风险，医疗领域亟需兼具准确性、可解释性和合规性的推理方案。

Method: 本地模型生成多推理路径→轻量判别器动态择优；采用改进的LoRA模块堆叠聚合策略，融入客户端分类器感知实现跨异构客户端的无噪参数聚合。

Result: 医疗推理任务验证FedCoT在客户端资源受限条件下显著提升模型推理性能，完整保持数据隐私性。

Conclusion: FedCoT成功平衡推理精度、鲁棒性与可解释性需求，为医疗AI决策提供安全可靠的技术支撑。

Abstract: Efficiently enhancing the reasoning capabilities of large language models
(LLMs) in federated learning environments remains challenging, particularly
when balancing performance gains with strict computational, communication, and
privacy constraints. This challenge is especially acute in healthcare, where
decisions-spanning clinical, operational, and patient-facing contexts-demand
not only accurate outputs but also interpretable, traceable rationales to
ensure safety, accountability, and regulatory compliance. Conventional
federated tuning approaches on LLM fail to address this need: they optimize
primarily for answer correctness while neglecting rationale quality, leaving
CoT capabilities dependent on models' innate pre-training abilities. Moreover,
existing methods for improving rationales typically rely on privacy-violating
knowledge distillation from centralized models. Additionally, the communication
overhead in traditional federated fine-tuning on LLMs remains substantial. We
addresses this gap by proposing FedCoT, a novel framework specifically designed
to enhance reasoning in federated settings. FedCoT leverages a lightweight
chain-of-thought enhancement mechanism: local models generate multiple
reasoning paths, and a compact discriminator dynamically selects the most
promising one. This approach improves reasoning accuracy and robustness while
providing valuable interpretability, which is particularly critical for medical
applications. To manage client heterogeneity efficiently, we adopt an improved
aggregation approach building upon advanced LoRA module stacking, incorporating
client classifier-awareness to achieve noise-free aggregation across diverse
clients. Comprehensive experiments on medical reasoning tasks demonstrate that
FedCoT significantly boosts client-side reasoning performance under stringent
resource budgets while fully preserving data privacy.

</details>


### [24] [LATTE: Learning Aligned Transactions and Textual Embeddings for Bank Clients](https://arxiv.org/abs/2508.10021)
*Egor Fadeev,Dzhambulat Mollaev,Aleksei Shestov,Dima Korolev,Omar Zoloev,Ivan Kireev,Andrey Savchenko,Maksim Makarenko*

Main category: cs.CL

TL;DR: 提出LATTE对比学习框架，通过将原始事件序列与LLM语义嵌入对齐，显著降低金融场景中长事件序列处理成本


<details>
  <summary>Details</summary>
Motivation: 大型语言模型直接处理长事件序列存在计算成本高、延迟敏感场景部署困难的问题

Method: 构建行为特征提示词，利用冻结LLM生成语义嵌入作为对比学习的监督信号

Result: 在真实金融数据集上超越现有技术，推理成本降低85%，输入尺寸缩减90%

Conclusion: LATTE在保持低延迟部署能力的同时，有效提升金融事件序列表征学习效果

Abstract: Learning clients embeddings from sequences of their historic communications
is central to financial applications. While large language models (LLMs) offer
general world knowledge, their direct use on long event sequences is
computationally expensive and impractical in real-world pipelines. In this
paper, we propose LATTE, a contrastive learning framework that aligns raw event
embeddings with semantic embeddings from frozen LLMs. Behavioral features are
summarized into short prompts, embedded by the LLM, and used as supervision via
contrastive loss. The proposed approach significantly reduces inference cost
and input size compared to conventional processing of complete sequence by LLM.
We experimentally show that our method outperforms state-of-the-art techniques
for learning event sequence representations on real-world financial datasets
while remaining deployable in latency-sensitive environments.

</details>


### [25] [Conformal P-Value in Multiple-Choice Question Answering Tasks with Provable Risk Control](https://arxiv.org/abs/2508.10022)
*Yuanchang Ye*

Main category: cs.CL

TL;DR: 融合显著性检验的置信预测框架提升LLM多选题回答可信度，通过自洽重采样构建预测集，在MMLU基准验证有效性


<details>
  <summary>Details</summary>
Motivation: LLM在学科问答场景存在幻觉与事实性错误风险，需建立可信赖的统计保障机制。现有置信预测（CP）方法缺乏与统计显著性检验的协同整合。

Method: 1. 通过MCQA自洽重采样计算选项频率作为置信度
2. 结合空假设检验构建预测集
3. 使用p值量化响应可靠性

Result: 1. 框架实现用户指定错误覆盖率（MMLU-Pro 0.5%误差）
2. 预测集平均大小（APSS）与风险水平α呈单调递减关系
3. APSS验证为有效不确定性度量指标

Conclusion: 建立具有统计严谨性的置信预测框架，为高风险领域LLM部署提供可靠的质量控制方法，平衡预测精度与不确定性量化需求

Abstract: This study introduces a significance testing-enhanced conformal prediction
(CP) framework to improve trustworthiness of large language models (LLMs) in
multiple-choice question answering (MCQA). While LLMs have been increasingly
deployed in disciplinary QA scenarios, hallucination and nonfactual generation
substantially compromise response reliability. Although CP provides
statistically rigorous marginal coverage guarantees for prediction sets, and
significance testing offers established statistical rigor, their synergistic
integration remains unexplored. To mitigate hallucination and factual
inaccuracies, our framework integrates $p$-value computation with conformity
scoring through self-consistency resampling of MCQA responses. This approach
calculates option frequencies to address LLMs' black-box nature, subsequently
constructing prediction sets via null hypothesis testing ($\mathcal{H}_0$) with
empirically derived $p$-values. Evaluations on MMLU and MMLU-Pro benchmarks
using off-the-shelf LLMs demonstrate: (1) The enhanced CP achieves
user-specified empirical miscoverage rates; (2) Test-set average prediction set
size (APSS) decreases monotonically with increasing risk levels ($\alpha$),
validating APSS as an effective uncertainty metric. This work establishes a
principled statistical framework for trustworthy LLM deployment in high-stakes
QA applications.

</details>


### [26] [RTTC: Reward-Guided Collaborative Test-Time Compute](https://arxiv.org/abs/2508.10024)
*J. Pablo Muñoz,Jinjie Yuan*

Main category: cs.CL

TL;DR: 提出奖励引导的测试时计算框架RTTC，通过预训练奖励模型动态选择最优计算策略，在减少计算冗余的同时显著提升多领域任务准确率


<details>
  <summary>Details</summary>
Motivation: 现有测试时计算策略(TTC)如RAG和TTT存在计算开销大、策略选择僵化的问题，不同查询需要差异化的计算策略

Method: 采用分布式服务架构，结合远程知识库检索与客户端轻量化微调，创新性引入查询状态缓存机制实现跨层级计算复用

Result: 多模型多基准测试表明RTTC准确率持续超越传统方法，验证了奖励引导策略选择的有效性

Conclusion: RTTC框架证实了自适应计算策略与状态缓存机制在提升语言模型扩展性和计算效率方面的重大潜力

Abstract: Test-Time Compute (TTC) has emerged as a powerful paradigm for enhancing the
performance of Large Language Models (LLMs) at inference, leveraging strategies
such as Test-Time Training (TTT) and Retrieval-Augmented Generation (RAG).
However, the optimal adaptation strategy varies across queries, and
indiscriminate application of TTC strategy incurs substantial computational
overhead. In this work, we introduce Reward-Guided Test-Time Compute (RTTC), a
novel framework that adaptively selects the most effective TTC strategy for
each query via a pretrained reward model, maximizing downstream accuracy across
diverse domains and tasks. RTTC operates in a distributed server-client
architecture, retrieving relevant samples from a remote knowledge base and
applying RAG or lightweight fine-tuning on client devices only when necessary.
To further mitigate redundant computation, we propose Query-State Caching,
which enables the efficient reuse of historical query states at both retrieval
and adaptation levels. Extensive experiments across multiple LLMs and
benchmarks demonstrate that RTTC consistently achieves superior accuracy
compared to vanilla RAG or TTT, validating the necessity of adaptive,
reward-guided TTC selection and the potential of RTTC for scalable,
high-performance language model adaptation.

</details>


### [27] [Detecting and explaining postpartum depression in real-time with generative artificial intelligence](https://arxiv.org/abs/2508.10025)
*Silvia García-Méndez,Francisco de Arriba-Pérez*

Main category: cs.CL

TL;DR: 开发结合自然语言处理、机器学习和大型语言模型的智能PPD筛查系统，通过可解释性方法实现90%检测准确率


<details>
  <summary>Details</summary>
Motivation: 产后抑郁症严重影响产妇身心健康，现有检测方案存在实时性不足和模型黑箱问题，需结合新技术实现快速可解释筛查

Method: 整合NLP、ML和LLMs技术，采用树基可解释算法与大型语言模型结合，通过特征重要性和自然语言生成解释

Result: PPD检测各项评估指标均达90%，优于现有文献解决方案

Conclusion: 该智能系统能快速识别PPD及风险因素，为及时临床干预提供可靠技术支持，解决了模型可解释性难题

Abstract: Among the many challenges mothers undergo after childbirth, postpartum
depression (PPD) is a severe condition that significantly impacts their mental
and physical well-being. Consequently, the rapid detection of ppd and their
associated risk factors is critical for in-time assessment and intervention
through specialized prevention procedures. Accordingly, this work addresses the
need to help practitioners make decisions with the latest technological
advancements to enable real-time screening and treatment recommendations.
Mainly, our work contributes to an intelligent PPD screening system that
combines Natural Language Processing, Machine Learning (ML), and Large Language
Models (LLMs) towards an affordable, real-time, and non-invasive free speech
analysis. Moreover, it addresses the black box problem since the predictions
are described to the end users thanks to the combination of LLMs with
interpretable ml models (i.e., tree-based algorithms) using feature importance
and natural language. The results obtained are 90 % on ppd detection for all
evaluation metrics, outperforming the competing solutions in the literature.
Ultimately, our solution contributes to the rapid detection of PPD and their
associated risk factors, critical for in-time and proper assessment and
intervention.

</details>


### [28] [SABER: Switchable and Balanced Training for Efficient LLM Reasoning](https://arxiv.org/abs/2508.10026)
*Kai Zhao,Yanjun Zhao,Jiaming Song,Shien He,Lusheng Zhang,Qiang Zhang,Tianjiao Li*

Main category: cs.CL

TL;DR: 提出SABER强化学习框架，通过预算分层的可控推理机制，在降低65.4%推理长度的同时提升3.6%准确率，实现推理效率与精度的平衡。


<details>
  <summary>Details</summary>
Motivation: 解决大模型思维链推理导致的过高计算成本问题，通过用户可调控的预算分级机制，满足不同场景对推理深度与延迟的差异化需求。

Method: 1. 预算分层：根据样本复杂度分配预算层级
2. 长度感知奖励机制引导模型遵守预算
3. 引入无推理样本增强可靠性
4. 支持四档推理模式（NoThink/DeepThink等）灵活切换

Result: MATH基准上推理长度减少65.4%且准确率提升3.6%，在数学推理、代码生成等4个领域验证了预算适应性、性能平稳下降及跨域泛化能力。

Conclusion: SABER框架首次实现用户可控的弹性推理预算，为LLM部署提供高效推理范式，在保持精度的同时显著降低计算开销，具有重要工程应用价值。

Abstract: Large language models (LLMs) empowered by chain-of-thought reasoning have
achieved impressive accuracy on complex tasks but suffer from excessive
inference costs and latency when applied uniformly to all problems. We propose
SABER (Switchable and Balanced Training for Efficient LLM Reasoning), a
reinforcement learning framework that endows LLMs with user-controllable,
token-budgeted reasoning. SABER first profiles each training example's
base-model thinking token usage and assigns it to one of the predefined budget
tiers. During fine-tuning, the model is guided by system prompts and
length-aware rewards to respect its assigned budget. In parallel, we
incorporate no-think examples to ensure the model remains reliable even when
explicit reasoning is turned off. SABER further supports four discrete
inference modes - NoThink, FastThink, CoreThink, and DeepThink, enabling
flexible trade-offs between latency and reasoning depth. Extensive evaluations
on math reasoning (MATH, GSM8K), code generation (MBPP), and logical reasoning
(LiveBench-Reasoning) demonstrate that SABER achieves high accuracy under tight
budgets, graceful degradation, and effective cross-scale and cross-domain
generalization. In particular, SABER-FastThink cuts reasoning length by 65.4%
and yields a 3.6% accuracy gain compared with the base model on the MATH
benchmark.

</details>


### [29] [LLMCARE: Alzheimer's Detection via Transformer Models Enhanced by LLM-Generated Synthetic Data](https://arxiv.org/abs/2508.10027)
*Ali Zolnour,Hossein Azadmaleki,Yasaman Haghbin,Fatemeh Taherinezhad,Mohamad Javad Momeni Nezhad,Sina Rashidi,Masoud Khani,AmirSajjad Taleban,Samin Mahdizadeh Sani,Maryam Dadkhah,James M. Noble,Suzanne Bakken,Yadollah Yaghoobzadeh,Abdol-Hossein Vahabie,Masoud Rouhizadeh,Maryam Zolnoori*

Main category: cs.CL

TL;DR: 融合Transformer嵌入与人工语言特征可提升ADRD语音检测效果，临床优化的LLM有效支持分类与数据增强，多模态模型仍需改进


<details>
  <summary>Details</summary>
Motivation: 美国约500万老年人受ADRD影响但过半未确诊，基于语音的NLP技术为早期筛查提供可扩展解决方案

Method: 使用DementiaBank数据，评估10个Transformer模型融合语言特征，采用LLM生成合成语音进行数据增强，测试单模态/多模态LLM分类器

Result: 融合模型F1达83.3（AUC89.5），MedAlpaca合成数据增强使F1提升至85.7，微调后MedAlpaca性能提升66%

Conclusion: 语言特征与Transformer融合增强检测能力，LLM在分类与数据增强具临床价值，多模态建模需进一步突破

Abstract: Alzheimer's disease and related dementias (ADRD) affect approximately five
million older adults in the U.S., yet over half remain undiagnosed.
Speech-based natural language processing (NLP) offers a promising, scalable
approach to detect early cognitive decline through linguistic markers.
  To develop and evaluate a screening pipeline that (i) fuses transformer
embeddings with handcrafted linguistic features, (ii) tests data augmentation
using synthetic speech generated by large language models (LLMs), and (iii)
benchmarks unimodal and multimodal LLM classifiers for ADRD detection.
  Transcripts from the DementiaBank "cookie-theft" task (n = 237) were used.
Ten transformer models were evaluated under three fine-tuning strategies. A
fusion model combined embeddings from the top-performing transformer with 110
lexical-derived linguistic features. Five LLMs (LLaMA-8B/70B, MedAlpaca-7B,
Ministral-8B, GPT-4o) were fine-tuned to generate label-conditioned synthetic
speech, which was used to augment training data. Three multimodal models
(GPT-4o, Qwen-Omni, Phi-4) were tested for speech-text classification in
zero-shot and fine-tuned settings.
  The fusion model achieved F1 = 83.3 (AUC = 89.5), outperforming linguistic or
transformer-only baselines. Augmenting training data with 2x MedAlpaca-7B
synthetic speech increased F1 to 85.7. Fine-tuning significantly improved
unimodal LLM classifiers (e.g., MedAlpaca: F1 = 47.3 -> 78.5 F1). Current
multimodal models demonstrated lower performance (GPT-4o = 70.2 F1; Qwen =
66.0). Performance gains aligned with the distributional similarity between
synthetic and real speech.
  Integrating transformer embeddings with linguistic features enhances ADRD
detection from speech. Clinically tuned LLMs effectively support both
classification and data augmentation, while further advancement is needed in
multimodal modeling.

</details>


### [30] [PREF: Reference-Free Evaluation of Personalised Text Generation in LLMs](https://arxiv.org/abs/2508.10028)
*Xiao Fu,Hossein A. Rahmani,Bin Wu,Jerome Ramos,Emine Yilmaz,Aldo Lipani*

Main category: cs.CL

TL;DR: PREF是一个无需参考的个性化文本生成评估框架，通过三阶段流程（覆盖、偏好、评分）提升评估效果


<details>
  <summary>Details</summary>
Motivation: 现有文本生成评估方法忽视用户个体差异，无法有效衡量个性化对齐效果

Method: 1) 覆盖阶段生成通用质量标准 2) 偏好阶段整合用户画像生成个性化评估标准 3) 评分阶段应用LLM进行最终评估

Result: 在PrefEval基准测试中，PREF在准确性(↑12%)和人类判断对齐度(↑18%)上显著优于基线模型

Conclusion: PREF通过可扩展、可解释的评估框架，为个性化语言系统的发展奠定了可靠的评估基础

Abstract: Personalised text generation is essential for user-centric information
systems, yet most evaluation methods overlook the individuality of users. We
introduce \textbf{PREF}, a \textbf{P}ersonalised \textbf{R}eference-free
\textbf{E}valuation \textbf{F}ramework that jointly measures general output
quality and user-specific alignment without requiring gold personalised
references. PREF operates in a three-step pipeline: (1) a coverage stage uses a
large language model (LLM) to generate a comprehensive, query-specific
guideline covering universal criteria such as factuality, coherence, and
completeness; (2) a preference stage re-ranks and selectively augments these
factors using the target user's profile, stated or inferred preferences, and
context, producing a personalised evaluation rubric; and (3) a scoring stage
applies an LLM judge to rate candidate answers against this rubric, ensuring
baseline adequacy while capturing subjective priorities. This separation of
coverage from preference improves robustness, transparency, and reusability,
and allows smaller models to approximate the personalised quality of larger
ones. Experiments on the PrefEval benchmark, including implicit
preference-following tasks, show that PREF achieves higher accuracy, better
calibration, and closer alignment with human judgments than strong baselines.
By enabling scalable, interpretable, and user-aligned evaluation, PREF lays the
groundwork for more reliable assessment and development of personalised
language generation systems.

</details>


### [31] [Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs](https://arxiv.org/abs/2508.10029)
*Wenpeng Xing,Mohan Li,Chunqiang Hu,Haitao XuNingyu Zhang,Bo Lin,Meng Han*

Main category: cs.CL

TL;DR: 提出了Latent Fusion Jailbreak（LFJ）攻击方法，通过潜在空间融合实现94.01%平均攻击成功率，并提出对抗训练防御方案使攻击成功率降低80%+


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在越狱攻击漏洞，现有攻击方法在效果和效率上存在局限，需要开发更有效的攻击和防御方法

Method: 1. 选择高相似度的恶意/良性查询对
2. 在关键网络层进行梯度引导的隐状态插值
3. 三阶段优化（攻击成功率、输出流畅性、计算效率）
4. 基于插值样本的对抗训练防御方案

Result: 在Vicuna/LLaMA-2等模型上达到94.01%平均攻击成功率（AdvBench基准），防御方案使ASR下降超80%且不影响正常输入表现

Conclusion: LFJ通过查询对筛选、隐状态融合和联合优化实现高效越狱，对抗训练可显著增强模型鲁棒性，消融实验验证了各组件有效性

Abstract: Large language models (LLMs) demonstrate impressive capabilities in various
language tasks but are susceptible to jailbreak attacks that circumvent their
safety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a
representation-based attack that interpolates hidden states from harmful and
benign query pairs to elicit prohibited responses. LFJ begins by selecting
query pairs with high thematic and syntactic similarity, then performs
gradient-guided interpolation at influential layers and tokens, followed by
optimization to balance attack success, output fluency, and computational
efficiency. Evaluations on models such as Vicuna and LLaMA-2 across benchmarks
like AdvBench and MaliciousInstruct yield an average attack success rate (ASR)
of 94.01%, outperforming existing methods. To mitigate LFJ, we propose an
adversarial training defense that fine-tunes models on interpolated examples,
reducing ASR by over 80% without degrading performance on benign inputs.
Ablation studies validate the importance of query pair selection, hidden state
interpolation components, and optimization strategies in LFJ's effectiveness.

</details>


### [32] [Inference-Aware Prompt Optimization for Aligning Black-Box Large Language Models](https://arxiv.org/abs/2508.10030)
*Saaduddin Mahmud,Mason Nakamura,Kyle H. Wray,Shlomo Zilberstein*

Main category: cs.CL

TL;DR: 提出IAPO框架，联合优化提示词与推理策略，解决现有提示优化方法忽视推理阶段配置的问题。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法与推理策略（如采样次数）相互独立，用户对多目标权衡和计算预算的偏好会显著影响最佳提示选择。需建立统一框架实现联合优化。

Method: 开发IAPO框架及PSST训练算法，在固定预算下同步优化提示词与推理规模，通过理论分析证明错误概率的有限预算保证。

Result: 在6项文本生成与推理任务中验证有效性，证明推理感知机制对黑盒LLM对齐的关键作用，PSST显著优于传统方法。

Conclusion: 提示优化必须与推理策略协同设计，IAPO为计算预算敏感场景下的LLM部署提供系统化解决方案。

Abstract: Prompt optimization methods have demonstrated significant effectiveness in
aligning black-box large language models (LLMs). In parallel, inference scaling
strategies such as Best-of-N Sampling and Majority Voting have also proven to
enhance alignment and performance by trading off computation. However, existing
prompt optimization approaches are inference strategy agnostic; that is, they
optimize prompts without regard to the inference strategy employed during
deployment. This constitutes a significant methodological gap, as our empirical
and theoretical analysis reveals a strong interdependence between these two
paradigms. Moreover, we find that user preferences regarding trade-offs among
multiple objectives and inference budgets substantially influence the choice of
prompt and inference configuration. To address this gap, we introduce a unified
novel framework named IAPO (Inference-Aware Prompt Optimization) that jointly
optimizes the prompt and inference scale, while being aware of the inference
budget and different task objectives. We then develop a fixed-budget training
algorithm for IAPO, which we call PSST (Prompt Scaling via Sequential
Trimming), and analyze finite-budget guarantees on error probability. Finally,
we evaluate the effectiveness of PSST on six different tasks, including
multi-objective text generation and reasoning, and demonstrate the critical
role of incorporating inference-awareness when aligning black-box LLMs through
prompt optimization.

</details>


### [33] [The Cost of Thinking: Increased Jailbreak Risk in Large Language Models](https://arxiv.org/abs/2508.10032)
*Fan Yang*

Main category: cs.CL

TL;DR: 研究发现LLM的思考模式存在安全隐患，攻击成功率更高。通过添加特定思考标记的安全干预方法能有效降低攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 发现LLM的思考模式存在被Jailbreak攻击的漏洞，其攻击成功率显著高于普通模式。

Method: 在prompt中添加'特定思考标记'，显式引导LLM的内部思考过程。

Result: 安全思考干预显著降低了LLM思考模式下的攻击成功率。

Conclusion: LLM思考模式存在安全缺陷，但通过安全干预机制可有效缓解该问题。

Abstract: Thinking mode has always been regarded as one of the most valuable modes in
LLMs. However, we uncover a surprising and previously overlooked phenomenon:
LLMs with thinking mode are more easily broken by Jailbreak attack. We evaluate
9 LLMs on AdvBench and HarmBench and find that the success rate of attacking
thinking mode in LLMs is almost higher than that of non-thinking mode. Through
large numbers of sample studies, it is found that for educational purposes and
excessively long thinking lengths are the characteristics of successfully
attacked data, and LLMs also give harmful answers when they mostly know that
the questions are harmful. In order to alleviate the above problems, this paper
proposes a method of safe thinking intervention for LLMs, which explicitly
guides the internal thinking processes of LLMs by adding "specific thinking
tokens" of LLMs to the prompt. The results demonstrate that the safe thinking
intervention can significantly reduce the attack success rate of LLMs with
thinking mode.

</details>


### [34] [Reflect then Learn: Active Prompting for Information Extraction Guided by Introspective Confusion](https://arxiv.org/abs/2508.10036)
*Dong Zhao,Yadong Wang,Xiang Chen,Chenxi Wang,Hongliang Dai,Chuanxing Geng,Shengzhong Zhang,Shaoyuan Li,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 提出APIE框架，通过双维度不确定性指标（格式与内容）优化上下文样本选择，显著提升大模型少样本信息抽取效果


<details>
  <summary>Details</summary>
Motivation: 传统上下文样本选择策略忽视模型在生成结构化格式时的困难，导致信息抽取效果受限

Method: 基于自省混淆原则，设计格式不确定性（语法生成难度）和内容不确定性（语义一致性）的联合评估指标，主动选择高混淆样本作为上下文示例

Result: 在四个基准测试中超越基线模型，提取准确率提升最高达8.2%，且模型输出稳定性显著增强

Conclusion: 结构化生成系统需要同时考虑格式与内容的双层次不确定性评估，这对构建可靠的信息抽取系统具有关键意义

Abstract: Large Language Models (LLMs) show remarkable potential for few-shot
information extraction (IE), yet their performance is highly sensitive to the
choice of in-context examples. Conventional selection strategies often fail to
provide informative guidance, as they overlook a key source of model
fallibility: confusion stemming not just from semantic content, but also from
the generation of well-structured formats required by IE tasks. To address
this, we introduce Active Prompting for Information Extraction (APIE), a novel
active prompting framework guided by a principle we term introspective
confusion. Our method empowers an LLM to assess its own confusion through a
dual-component uncertainty metric that uniquely quantifies both Format
Uncertainty (difficulty in generating correct syntax) and Content Uncertainty
(inconsistency in extracted semantics). By ranking unlabeled data with this
comprehensive score, our framework actively selects the most challenging and
informative samples to serve as few-shot exemplars. Extensive experiments on
four benchmarks show that our approach consistently outperforms strong
baselines, yielding significant improvements in both extraction accuracy and
robustness. Our work highlights the critical importance of a fine-grained,
dual-level view of model uncertainty when it comes to building effective and
reliable structured generation systems.

</details>


### [35] [mSCoRe: a $M$ultilingual and Scalable Benchmark for $S$kill-based $Co$mmonsense $Re$asoning](https://arxiv.org/abs/2508.10137)
*Nghia Trung Ngo,Franck Dernoncourt,Thien Huu Nguyen*

Main category: cs.CL

TL;DR: 提出mSCoRe多语言可扩展基准测试，系统评估大模型在多语言常识推理中的技能表现，揭示当前模型在跨文化常识处理中的局限性


<details>
  <summary>Details</summary>
Motivation: 现有推理增强大语言模型缺乏对多语言跨文化常识推理能力的系统性评估机制，难以解析模型对不同人类推理技能的应用机理

Method: 构建包含推理技能分类法+数据合成流程+复杂度扩展框架的三维评估体系，通过动态难度调整实现未来兼容性评估

Result: 8个前沿模型实验显示，模型在高层级复杂度和文化敏感场景下表现显著下降，暴露出跨语言常识推理的系统性缺陷

Conclusion: 当前模型在多语言文化常识推理存在本质局限，需通过改进训练范式和数据多样性来提升跨文化理解能力

Abstract: Recent advancements in reasoning-reinforced Large Language Models (LLMs) have
shown remarkable capabilities in complex reasoning tasks. However, the
mechanism underlying their utilization of different human reasoning skills
remains poorly investigated, especially for multilingual commonsense reasoning
that involves everyday knowledge across different languages and cultures. To
address this gap, we propose a \textbf{M}ultilingual and Scalable Benchmark for
\textbf{S}kill-based \textbf{Co}mmonsense \textbf{Re}asoning (\textbf{mSCoRe}).
Our benchmark incorporates three key components that are designed to
systematically evaluate LLM's reasoning capabilities, including: (1) a novel
taxonomy of reasoning skills that enables fine-grained analysis of models'
reasoning processes, (2) a robust data synthesis pipeline tailored specifically
for commonsense reasoning evaluation, and (3) a complexity scaling framework
allowing task difficulty to scale dynamically alongside future improvements in
LLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying
sizes and training approaches demonstrate that \textbf{mSCoRe} remains
significantly challenging for current models, particularly at higher complexity
levels. Our results reveal the limitations of such reasoning-reinforced models
when confronted with nuanced multilingual general and cultural commonsense. We
further provide detailed analysis on the models' reasoning processes,
suggesting future directions for improving multilingual commonsense reasoning
capabilities.

</details>


### [36] [Multi-Turn Puzzles: Evaluating Interactive Reasoning and Strategic Dialogue in LLMs](https://arxiv.org/abs/2508.10142)
*Kartikeya Badola,Jonathan Simon,Arian Hosseini,Sara Marie Mc Carthy,Tsendsuren Munkhdalai,Abhimanyu Goyal,Tomáš Kočiský,Shyam Upadhyay,Bahare Fatemi,Mehran Kazemi*

Main category: cs.CL

TL;DR: 论文指出大语言模型在复杂交互场景中的不足，提出新型多任务基准测试，通过确定性评分机制评估模型的多轮对话、推理和信息检索能力，揭示当前模型的提升空间与主要错误类型。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs擅长解决清晰完整的问题，但在真实场景中普遍存在的复杂交互任务（需多轮逻辑对话、信息检索和非完整数据推理）表现不佳，亟需开发相关能力评估体系。

Method: 构建包含多任务的新型基准测试套件，每个任务专门测试特定推理/交互对话/信息检索能力，采用确定性评分机制实现自动化评估。

Result: 前沿模型评估显示显著提升空间，主要错误源于指令遵循不足（31%）、推理失败（29%）和规划能力薄弱（22%）三方面。

Conclusion: 该基准为评估LLMs复杂交互能力提供新工具，揭示当前模型弱点（指令理解/推理/规划），为改进关键能力的研究建立标准化测试平台。

Abstract: Large language models (LLMs) excel at solving problems with clear and
complete statements, but often struggle with nuanced environments or
interactive tasks which are common in most real-world scenarios. This
highlights the critical need for developing LLMs that can effectively engage in
logically consistent multi-turn dialogue, seek information and reason with
incomplete data. To this end, we introduce a novel benchmark comprising a suite
of multi-turn tasks each designed to test specific reasoning, interactive
dialogue, and information-seeking abilities. These tasks have deterministic
scoring mechanisms, thus eliminating the need for human intervention.
Evaluating frontier models on our benchmark reveals significant headroom. Our
analysis shows that most errors emerge from poor instruction following,
reasoning failures, and poor planning. This benchmark provides valuable
insights into the strengths and weaknesses of current LLMs in handling complex,
interactive scenarios and offers a robust platform for future research aimed at
improving these critical capabilities.

</details>


### [37] [LaajMeter: A Framework for LaaJ Evaluation](https://arxiv.org/abs/2508.10161)
*Gal Amram,Eitan Farchi,Shmulik Froimovich,Raviv Gal,Avi Ziv*

Main category: cs.CL

TL;DR: 提出LaaJMeter框架用于系统评估大语言模型作为评估者（LaaJ）在特定领域的效果，通过生成合成数据验证评估指标的敏感性差异


<details>
  <summary>Details</summary>
Motivation: 现有LaaJ在领域特定场景下面临指标未经验证、评估质量难衡量的挑战，尤其在低资源领域难以确定有效评估指标和阈值

Method: 开发基于仿真的LaaJMeter框架，生成虚拟模型和评估者数据，通过控制变量系统分析不同评估指标在代码翻译等任务中的表现差异

Result: 在遗留编程语言代码翻译任务中验证，显示常用指标存在敏感性差异，强调指标选择需要系统验证而非直接套用

Conclusion: LaaJMeter为低资源场景提供可扩展的LaaJ评估方案，通过模拟验证帮助选择可靠指标，提升NLP评估的可信度与可复现性

Abstract: Large Language Models (LLMs) are increasingly used as evaluators in natural
language processing tasks, a paradigm known as LLM-as-a-Judge (LaaJ). While
effective in general domains, LaaJs pose significant challenges in
domain-specific contexts, where annotated data is scarce and expert evaluation
is costly. In such cases, meta-evaluation is often performed using metrics that
have not been validated for the specific domain in which they are applied. As a
result, it becomes difficult to determine which metrics effectively identify
LaaJ quality, and further, what threshold indicates sufficient evaluator
performance. In this work, we introduce LaaJMeter, a simulation-based framework
for controlled meta-evaluation of LaaJs. LaaJMeter enables engineers to
generate synthetic data representing virtual models and judges, allowing
systematic analysis of evaluation metrics under realistic conditions. This
helps practitioners validate and refine LaaJs for specific evaluation tasks:
they can test whether their metrics correctly distinguish between better and
worse (virtual) LaaJs, and estimate appropriate thresholds for evaluator
adequacy.
  We demonstrate the utility of LaaJMeter in a code translation task involving
a legacy programming language, showing how different metrics vary in
sensitivity to evaluator quality. Our results highlight the limitations of
common metrics and the importance of principled metric selection. LaaJMeter
provides a scalable and extensible solution for assessing LaaJs in low-resource
settings, contributing to the broader effort to ensure trustworthy and
reproducible evaluation in NLP.

</details>


### [38] [Estimating Machine Translation Difficulty](https://arxiv.org/abs/2508.10175)
*Lorenzo Proietti,Stefano Perrella,Vilém Zouhar,Roberto Navigli,Tom Kocmi*

Main category: cs.CL

TL;DR: 提出翻译难度估计任务及评估指标，开发Sentinel-src模型显著优于传统方法，并发布改进模型助力构建挑战性评测集


<details>
  <summary>Details</summary>
Motivation: 当前机器翻译质量接近完美，导致难以区分顶尖模型差异和识别改进方向，需有效识别系统难以处理的文本

Method: 1. 定义基于翻译预期质量的难度估计任务
2. 提出新评估指标
3. 开发Sentinel-src模型（结合上下文特征）
4. 对比基线方法（词频/句法复杂度）和LLM评估方法

Result: 1. Sentinel-src模型在难度预测上显著优于传统方法（24/25版改进明显）
2. 成功构建更具挑战性的MT评测基准
3. 发布两个优化模型供大规模文本筛选

Conclusion: 专用难度估计模型可有效识别机器翻译难点，为评估体系改进和模型研发提供新方向，Sentinel-src系列展现优越性能

Abstract: Machine translation quality has began achieving near-perfect translations in
some setups. These high-quality outputs make it difficult to distinguish
between state-of-the-art models and to identify areas for future improvement.
Automatically identifying texts where machine translation systems struggle
holds promise for developing more discriminative evaluations and guiding future
research.
  We formalize the task of translation difficulty estimation, defining a text's
difficulty based on the expected quality of its translations. We introduce a
new metric to evaluate difficulty estimators and use it to assess both
baselines and novel approaches. Finally, we demonstrate the practical utility
of difficulty estimators by using them to construct more challenging machine
translation benchmarks. Our results show that dedicated models (dubbed
Sentinel-src) outperform both heuristic-based methods (e.g. word rarity or
syntactic complexity) and LLM-as-a-judge approaches. We release two improved
models for difficulty estimation, Sentinel-src-24 and Sentinel-src-25, which
can be used to scan large collections of texts and select those most likely to
challenge contemporary machine translation systems.

</details>


### [39] [Efficient Forward-Only Data Valuation for Pretrained LLMs and VLMs](https://arxiv.org/abs/2508.10180)
*Wenlong Deng,Jiaming Zhang,Qi Zeng,Christos Thrampoulidis,Boying Gong,Xiaoxiao Li*

Main category: cs.CL

TL;DR: 提出For-Value框架——仅需单次前向传播即可计算数据影响力的免梯度评估方法


<details>
  <summary>Details</summary>
Motivation: 现有数据估值方法依赖海森矩阵或模型重训练，无法适应十亿参数规模的LLM/VLM模型，需开发更高效的透明化评估工具

Method: 利用基础模型的隐层表示，通过训练样本与验证样本的隐表示对齐度和预测误差构建闭式解计算公式

Result: 在识别关键微调样本和检测错误标注数据任务中，效果达到或超过基于梯度的基线方法

Conclusion: For-Value为大规模模型提供了可扩展的数据影响力评估方案，显著降低计算开销的同时保持评估精度

Abstract: Quantifying the influence of individual training samples is essential for
enhancing the transparency and accountability of large language models (LLMs)
and vision-language models (VLMs). However, existing data valuation methods
often rely on Hessian information or model retraining, making them
computationally prohibitive for billion-parameter models. In this work, we
introduce For-Value, a forward-only data valuation framework that enables
scalable and efficient influence estimation for both LLMs and VLMs. By
leveraging the rich representations of modern foundation models, For-Value
computes influence scores using a simple closed-form expression based solely on
a single forward pass, thereby eliminating the need for costly gradient
computations. Our theoretical analysis demonstrates that For-Value accurately
estimates per-sample influence by capturing alignment in hidden representations
and prediction errors between training and validation samples. Extensive
experiments show that For-Value matches or outperforms gradient-based baselines
in identifying impactful fine-tuning examples and effectively detecting
mislabeled data.

</details>


### [40] [PakBBQ: A Culturally Adapted Bias Benchmark for QA](https://arxiv.org/abs/2508.10186)
*Abdullah Hashmat,Muhammad Arham Mirza,Agha Ali Raza*

Main category: cs.CL

TL;DR: 提出PakBBQ数据集，针对巴基斯坦文化背景扩展BBQ基准，发现语境消歧提升准确率12%、乌尔都语模型偏见更低、负面提问减少刻板回答


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评估多基于西方中心数据，缺乏对低资源语言（如乌尔都语）和区域文化背景（如巴基斯坦）的关注，导致模型公平性不足

Method: 构建包含214模板/17,180问答对的英乌双语数据集，覆盖年龄/宗教/地域等8类偏见维度，测试多语言模型在模糊/明确语境及不同提问框架下的表现

Result: 消歧处理使准确率平均提升12%；乌尔都语模型反偏见表现优于英语；负面提问框架显著减少刻板回答

Conclusion: 需建立情境化评估基准，简单提示工程（如语境消歧和负面提问）可有效缓解低资源语言环境中的模型偏见问题

Abstract: With the widespread adoption of Large Language Models (LLMs) across various
applications, it is empirical to ensure their fairness across all user
communities. However, most LLMs are trained and evaluated on Western centric
data, with little attention paid to low-resource languages and regional
contexts. To address this gap, we introduce PakBBQ, a culturally and regionally
adapted extension of the original Bias Benchmark for Question Answering (BBQ)
dataset. PakBBQ comprises over 214 templates, 17180 QA pairs across 8
categories in both English and Urdu, covering eight bias dimensions including
age, disability, appearance, gender, socio-economic status, religious, regional
affiliation, and language formality that are relevant in Pakistan. We evaluate
multiple multilingual LLMs under both ambiguous and explicitly disambiguated
contexts, as well as negative versus non negative question framings. Our
experiments reveal (i) an average accuracy gain of 12\% with disambiguation,
(ii) consistently stronger counter bias behaviors in Urdu than in English, and
(iii) marked framing effects that reduce stereotypical responses when questions
are posed negatively. These findings highlight the importance of contextualized
benchmarks and simple prompt engineering strategies for bias mitigation in low
resource settings.

</details>


### [41] [Prompt-Response Semantic Divergence Metrics for Faithfulness Hallucination and Misalignment Detection in Large Language Models](https://arxiv.org/abs/2508.10192)
*Igor Halperin*

Main category: cs.CL

TL;DR: 提出语义差异指标SDM框架，通过多提示变体与信息论方法检测大语言模型的忠实性幻觉


<details>
  <summary>Details</summary>
Motivation: 现有检测方法(如语义熵)仅基于单一固定提示的响应多样性，无法有效捕捉LLM深层语义偏差，需开发更全面的检测框架

Method: 构建多提示变体的联合主题聚类空间，结合Jensen-Shannon散度与Wasserstein距离计算语义差异得分S_H，通过KL散度识别语义探索行为

Result: 开发出可量化语义偏差的SDM指标，创建Semantic Box诊断框架，成功识别出危险的自信型虚构响应

Conclusion: SDM框架为检测LLM幻觉提供了创新诊断工具，其多提示测试与信息论指标组合显著提升检测效能

Abstract: The proliferation of Large Language Models (LLMs) is challenged by
hallucinations, critical failure modes where models generate non-factual,
nonsensical or unfaithful text. This paper introduces Semantic Divergence
Metrics (SDM), a novel lightweight framework for detecting Faithfulness
Hallucinations -- events of severe deviations of LLMs responses from input
contexts. We focus on a specific implementation of these LLM errors,
{confabulations, defined as responses that are arbitrary and semantically
misaligned with the user's query. Existing methods like Semantic Entropy test
for arbitrariness by measuring the diversity of answers to a single, fixed
prompt. Our SDM framework improves upon this by being more prompt-aware: we
test for a deeper form of arbitrariness by measuring response consistency not
only across multiple answers but also across multiple, semantically-equivalent
paraphrases of the original prompt. Methodologically, our approach uses joint
clustering on sentence embeddings to create a shared topic space for prompts
and answers. A heatmap of topic co-occurances between prompts and responses can
be viewed as a quantified two-dimensional visualization of the user-machine
dialogue. We then compute a suite of information-theoretic metrics to measure
the semantic divergence between prompts and responses. Our practical score,
$\mathcal{S}_H$, combines the Jensen-Shannon divergence and Wasserstein
distance to quantify this divergence, with a high score indicating a
Faithfulness hallucination. Furthermore, we identify the KL divergence
KL(Answer $||$ Prompt) as a powerful indicator of \textbf{Semantic
Exploration}, a key signal for distinguishing different generative behaviors.
These metrics are further combined into the Semantic Box, a diagnostic
framework for classifying LLM response types, including the dangerous,
confident confabulation.

</details>


### [42] [Understanding Textual Emotion Through Emoji Prediction](https://arxiv.org/abs/2508.10222)
*Ethan Gordon,Nishank Kuppa,Rigved Tummala,Sriram Anasuri*

Main category: cs.CL

TL;DR: 利用FFN、CNN、Transformer和BERT四种深度学习模型进行表情符号预测，BERT整体表现最佳，CNN在罕见类别预测中更有效。


<details>
  <summary>Details</summary>
Motivation: 通过短文本序列预测表情符号，优化人机交互中的情感表达准确性。

Method: 使用TweetEval数据集，采用焦点损失和正则化处理类别不平衡问题，对比四种神经网络架构。

Result: BERT因预训练优势达到最高综合性能，CNN在罕见表情类别预测中表现更优。

Conclusion: 模型架构选择和超参数调优对情感感知的表情预测至关重要，可提升交互体验。

Abstract: This project explores emoji prediction from short text sequences using four
deep learning architectures: a feed-forward network, CNN, transformer, and
BERT. Using the TweetEval dataset, we address class imbalance through focal
loss and regularization techniques. Results show BERT achieves the highest
overall performance due to its pre-training advantage, while CNN demonstrates
superior efficacy on rare emoji classes. This research shows the importance of
architecture selection and hyperparameter tuning for sentiment-aware emoji
prediction, contributing to improved human-computer interaction.

</details>


### [43] [Using Large Language Models to Measure Symptom Severity in Patients At Risk for Schizophrenia](https://arxiv.org/abs/2508.10226)
*Andrew X. Chen,Guillermo Horga,Sean Escola*

Main category: cs.CL

TL;DR: 研究利用大语言模型从临床访谈记录预测精神分裂症高风险患者的BPRS评分，结果显示其准确度接近人类评估水平，并具备多语言处理和时间序列分析潜力。


<details>
  <summary>Details</summary>
Motivation: BPRS量表虽有效但临床应用受限，需开发更高效的自动化评估工具来改善高风险精神分裂症患者的症状监测。

Method: 使用409名AMP-SCZ队列患者的临床访谈文本，通过大语言模型进行零样本BPRS评分预测，并测试跨语言评估和纵向信息整合能力。

Result: 模型预测与真实评估高度一致（中位一致性0.84，组内相关系数0.73），在多语言场景表现更优（一致性0.88，ICC 0.70）。

Conclusion: 大语言模型能有效标准化高风险患者评估，突破语言障碍并实现动态症状追踪，为精准医疗提供新范式。

Abstract: Patients who are at clinical high risk (CHR) for schizophrenia need close
monitoring of their symptoms to inform appropriate treatments. The Brief
Psychiatric Rating Scale (BPRS) is a validated, commonly used research tool for
measuring symptoms in patients with schizophrenia and other psychotic
disorders; however, it is not commonly used in clinical practice as it requires
a lengthy structured interview. Here, we utilize large language models (LLMs)
to predict BPRS scores from clinical interview transcripts in 409 CHR patients
from the Accelerating Medicines Partnership Schizophrenia (AMP-SCZ) cohort.
Despite the interviews not being specifically structured to measure the BPRS,
the zero-shot performance of the LLM predictions compared to the true
assessment (median concordance: 0.84, ICC: 0.73) approaches human inter- and
intra-rater reliability. We further demonstrate that LLMs have substantial
potential to improve and standardize the assessment of CHR patients via their
accuracy in assessing the BPRS in foreign languages (median concordance: 0.88,
ICC: 0.70), and integrating longitudinal information in a one-shot or few-shot
learning approach.

</details>


### [44] [A Computational Approach to Analyzing Language Change and Variation in the Constructed Language Toki Pona](https://arxiv.org/abs/2508.10246)
*Daniel Huang,Hyoun-A Joo*

Main category: cs.CL

TL;DR: 通过计算与语料库方法分析人造语言Toki Pona的演变规律，发现社会语言学因素对其影响方式与自然语言相同，揭示人造语言系统会随社区使用自然进化。


<details>
  <summary>Details</summary>
Motivation: 探究人造语言是否遵循自然语言的社会语言学演变规律，验证语言系统在社区实际使用中产生的自然演化现象。

Method: 采用计算语言学和语料库分析方法，通过量化词类句法位置偏好变化（历时）及跨语料库比较（共时），重点考察流变词类与及物性特征。

Result: Toki Pona表现出与自然语言相似的社会语言学变异模式，其语言特征随使用者群体的实际应用持续演化。

Conclusion: 该研究突破性地证明人造语言并非静态系统，其演变机制与自然语言本质相通，为语言变化理论提供了新的实证维度。

Abstract: This study explores language change and variation in Toki Pona, a constructed
language with approximately 120 core words. Taking a computational and
corpus-based approach, the study examines features including fluid word classes
and transitivity in order to examine (1) changes in preferences of content
words for different syntactic positions over time and (2) variation in usage
across different corpora. The results suggest that sociolinguistic factors
influence Toki Pona in the same way as natural languages, and that even
constructed linguistic systems naturally evolve as communities use them.

</details>


### [45] [Inductive Bias Extraction and Matching for LLM Prompts](https://arxiv.org/abs/2508.10295)
*Christian M. Angel,Francis Ferraro*

Main category: cs.CL

TL;DR: 提出IBEM策略通过提取模型归纳偏好优化提示词，显著提升LLM在分类和排序任务中的评分表现


<details>
  <summary>Details</summary>
Motivation: 大型语言模型对提示词表述极为敏感，现有方法未充分利用模型自身的归纳偏好特性

Method: 通过将模型输出重新整合到提示词中，构建匹配模型内在偏好的提示策略（IBEM）

Result: 分类任务的Likert评分提升19%，排序任务评分提升27%

Conclusion: 主动提取并匹配模型归纳偏好的提示工程策略能有效提升LLM任务表现

Abstract: The active research topic of prompt engineering makes it evident that LLMs
are sensitive to small changes in prompt wording. A portion of this can be
ascribed to the inductive bias that is present in the LLM. By using an LLM's
output as a portion of its prompt, we can more easily create satisfactory
wording for prompts. This has the effect of creating a prompt that matches the
inductive bias in model. Empirically, we show that using this Inductive Bias
Extraction and Matching strategy improves LLM Likert ratings used for
classification by up to 19% and LLM Likert ratings used for ranking by up to
27%.

</details>


### [46] [Yet another algorithmic bias: A Discursive Analysis of Large Language Models Reinforcing Dominant Discourses on Gender and Race](https://arxiv.org/abs/2508.10304)
*Gustavo Bonil,Simone Hashiguti,Jhessica Silva,João Gondim,Helena Maia,Nádia Silva,Helio Pedrini,Sandra Avila*

Main category: cs.CL

TL;DR: 研究提出定性框架分析LLM中的性别与种族偏见，发现模型固化刻板印象且修正机制效果有限，强调跨学科伦理介入的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有定量方法难以捕捉语言中的隐性偏见，需通过定性分析揭示LLM如何复现霸权话语及社会不平等结构。

Method: 手动分析LLM生成的包含黑人/白人女性角色的短篇小说，对比角色刻画模式，并测试模型修正偏见的响应策略。

Result: 黑人女性被绑定于‘祖先关联’和‘抗争’叙事，白人女性则体现‘自我探索’；模型修正仅表面调整，未改变深层意识形态结构。

Conclusion: 算法具有意识形态功能，需融合批判话语分析与技术开发，通过跨学科协作破解LLM对社会不平等的再生产机制。

Abstract: With the advance of Artificial Intelligence (AI), Large Language Models
(LLMs) have gained prominence and been applied in diverse contexts. As they
evolve into more sophisticated versions, it is essential to assess whether they
reproduce biases, such as discrimination and racialization, while maintaining
hegemonic discourses. Current bias detection approaches rely mostly on
quantitative, automated methods, which often overlook the nuanced ways in which
biases emerge in natural language. This study proposes a qualitative,
discursive framework to complement such methods. Through manual analysis of
LLM-generated short stories featuring Black and white women, we investigate
gender and racial biases. We contend that qualitative methods such as the one
proposed here are fundamental to help both developers and users identify the
precise ways in which biases manifest in LLM outputs, thus enabling better
conditions to mitigate them. Results show that Black women are portrayed as
tied to ancestry and resistance, while white women appear in self-discovery
processes. These patterns reflect how language models replicate crystalized
discursive representations, reinforcing essentialization and a sense of social
immobility. When prompted to correct biases, models offered superficial
revisions that maintained problematic meanings, revealing limitations in
fostering inclusive narratives. Our results demonstrate the ideological
functioning of algorithms and have significant implications for the ethical use
and development of AI. The study reinforces the need for critical,
interdisciplinary approaches to AI design and deployment, addressing how
LLM-generated discourses reflect and perpetuate inequalities.

</details>


### [47] [ReviewRL: Towards Automated Scientific Review with RL](https://arxiv.org/abs/2508.10308)
*Sihang Zeng,Kai Tian,Kaiyan Zhang,Yuru wang,Junqi Gao,Runze Liu,Sa Yang,Jingxuan Li,Xinwei Long,Jiaheng Ma,Biqing Qi,Bowen Zhou*

Main category: cs.CL

TL;DR: 提出强化学习框架ReviewRL，通过检索增强流程、监督微调和复合奖励机制，显著提升科学论文自动评审质量


<details>
  <summary>Details</summary>
Motivation: 应对日益增长的论文投稿量和审稿人疲劳问题，解决现有自动评审方法存在事实准确性不足、评分一致性差和缺乏深度分析的缺陷

Method: 1）ArXiv-MCP检索增强流程整合相关文献；2）监督微调建立评审基础能力；3）强化学习复合奖励函数（质量提升+评分准确）

Result: 在ICLR 2025论文数据集上，ReviewRL在规则指标和模型质量评估中显著超越现有方法

Conclusion: 建立了科学发现中强化学习驱动自动评审的基础框架，在自动批判生成领域展示出重要发展潜力

Abstract: Peer review is essential for scientific progress but faces growing challenges
due to increasing submission volumes and reviewer fatigue. Existing automated
review approaches struggle with factual accuracy, rating consistency, and
analytical depth, often generating superficial or generic feedback lacking the
insights characteristic of high-quality human reviews. We introduce ReviewRL, a
reinforcement learning framework for generating comprehensive and factually
grounded scientific paper reviews. Our approach combines: (1) an ArXiv-MCP
retrieval-augmented context generation pipeline that incorporates relevant
scientific literature, (2) supervised fine-tuning that establishes foundational
reviewing capabilities, and (3) a reinforcement learning procedure with a
composite reward function that jointly enhances review quality and rating
accuracy. Experiments on ICLR 2025 papers demonstrate that ReviewRL
significantly outperforms existing methods across both rule-based metrics and
model-based quality assessments. ReviewRL establishes a foundational framework
for RL-driven automatic critique generation in scientific discovery,
demonstrating promising potential for future development in this domain. The
implementation of ReviewRL will be released at GitHub.

</details>


### [48] [From Surface to Semantics: Semantic Structure Parsing for Table-Centric Document Analysis](https://arxiv.org/abs/2508.10311)
*Xuan Li,Jialiang Dong,Raymond Wong*

Main category: cs.CL

TL;DR: 提出DOTABLER框架，解决表格与上下文深层语义关联问题，提升文档解析能力


<details>
  <summary>Details</summary>
Motivation: 现有方法无法深入分析表格与上下文的语义关联，限制了跨段落数据解释等高级任务

Method: 结合定制数据集与领域微调，开发语义解析流程，支持表格为中心的文档结构解析和领域表格检索

Result: 在真实PDF测试中达到90%+ Precision/F1，性能超越GPT-4o等模型

Conclusion: DOTABLER实现了表格与上下文的深度语义链接，为文档解析与知识挖掘提供了创新方案

Abstract: Documents are core carriers of information and knowl-edge, with broad
applications in finance, healthcare, and scientific research. Tables, as the
main medium for structured data, encapsulate key information and are among the
most critical document components. Existing studies largely focus on
surface-level tasks such as layout analysis, table detection, and data
extraction, lacking deep semantic parsing of tables and their contextual
associations. This limits advanced tasks like cross-paragraph data
interpretation and context-consistent analysis. To address this, we propose
DOTABLER, a table-centric semantic document parsing framework designed to
uncover deep semantic links between tables and their context. DOTABLER
leverages a custom dataset and domain-specific fine-tuning of pre-trained
models, integrating a complete parsing pipeline to identify context segments
semantically tied to tables. Built on this semantic understanding, DOTABLER
implements two core functionalities: table-centric document structure parsing
and domain-specific table retrieval, delivering comprehensive table-anchored
semantic analysis and precise extraction of semantically relevant tables.
Evaluated on nearly 4,000 pages with over 1,000 tables from real-world PDFs,
DOTABLER achieves over 90% Precision and F1 scores, demonstrating superior
performance in table-context semantic analysis and deep document parsing
compared to advanced models such as GPT-4o.

</details>


### [49] [Beyond Semantic Understanding: Preserving Collaborative Frequency Components in LLM-based Recommendation](https://arxiv.org/abs/2508.10312)
*Minhao Wang,Yunhang He,Cong Xu,Zhangchi Zhu,Wei Zhang*

Main category: cs.CL

TL;DR: 提出FreLLM4Rec框架，通过频谱视角平衡语义与协同信息，有效缓解LLM推荐系统中协同信号衰减问题，实验显示NDCG@10提升最高8.00%


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的推荐系统过度关注语义关联，导致协同信号在多层传播中逐渐弱化，而传统Transformer模型可有效保留协同信号

Method: 结合全局图低通滤波器(G-LPF)去噪与时域频率调制(TFM)逐层保留协同信号，通过理论证明将最优图滤波器与高效频域滤波器关联

Result: 在四个基准数据集上验证，FreLLM4Rec显著缓解信号衰减，NDCG@10指标最高提升8%超越最优基线模型

Conclusion: 揭示了LLM处理协同信息的机制，为改进LLM推荐系统提供了可解释的理论框架与工程实践方案

Abstract: Recommender systems in concert with Large Language Models (LLMs) present
promising avenues for generating semantically-informed recommendations.
However, LLM-based recommenders exhibit a tendency to overemphasize semantic
correlations within users' interaction history. When taking pretrained
collaborative ID embeddings as input, LLM-based recommenders progressively
weaken the inherent collaborative signals as the embeddings propagate through
LLM backbones layer by layer, as opposed to traditional Transformer-based
sequential models in which collaborative signals are typically preserved or
even enhanced for state-of-the-art performance. To address this limitation, we
introduce FreLLM4Rec, an approach designed to balance semantic and
collaborative information from a spectral perspective. Item embeddings that
incorporate both semantic and collaborative information are first purified
using a Global Graph Low-Pass Filter (G-LPF) to preliminarily remove irrelevant
high-frequency noise. Temporal Frequency Modulation (TFM) then actively
preserves collaborative signal layer by layer. Note that the collaborative
preservation capability of TFM is theoretically guaranteed by establishing a
connection between the optimal but hard-to-implement local graph fourier
filters and the suboptimal yet computationally efficient frequency-domain
filters. Extensive experiments on four benchmark datasets demonstrate that
FreLLM4Rec successfully mitigates collaborative signal attenuation and achieves
competitive performance, with improvements of up to 8.00\% in NDCG@10 over the
best baseline. Our findings provide insights into how LLMs process
collaborative information and offer a principled approach for improving
LLM-based recommendation systems.

</details>


### [50] [Cross-Prompt Encoder for Low-Performing Languages](https://arxiv.org/abs/2508.10352)
*Beso Mikaberidze,Teimuraz Saghinadze,Simon Ostermann,Philipp Muller*

Main category: cs.CL

TL;DR: 提出交叉提示编码器(XPE)和双软提示机制，提升低性能语言在参数高效微调中的表现，实验验证XPE对弱势语言有效，混合方法适应多语言场景


<details>
  <summary>Details</summary>
Motivation: 探索软提示在跨语言迁移中的潜力，解决现有方法在低性能语言（即使全模型微调也表现差）上的性能瓶颈

Method: 1. XPE结合轻量编码架构+多语言训练捕捉跨语言模式 2. 双软提示融合编码器生成提示与直接训练提示 3. 基于SIB-200基准进行多语言实验验证

Result: XPE显著提升低性能语言准确率（最高+10.2%），混合方法在多语言环境展现更广适应性，实验揭示XPE与混合方案间的性能权衡规律

Conclusion: XPE是提升弱势语言的有效方案，双软提示机制通过结构共享与语言对齐的平衡，为多语言NLP提供新优化路径

Abstract: Soft prompts have emerged as a powerful alternative to adapters in
parameter-efficient fine-tuning (PEFT), enabling large language models (LLMs)
to adapt to downstream tasks without architectural changes or parameter
updates. While prior work has focused on stabilizing training via parameter
interaction in small neural prompt encoders, their broader potential for
transfer across languages remains unexplored. In this paper, we demonstrate
that a prompt encoder can play a central role in improving performance on
low-performing languages-those that achieve poor accuracy even under full-model
fine-tuning. We introduce the Cross-Prompt Encoder (XPE), which combines a
lightweight encoding architecture with multi-source training on typologically
diverse languages - a design that enables the model to capture abstract and
transferable patterns across languages. To complement XPE, we propose a Dual
Soft Prompt mechanism that combines an encoder-based prompt with a directly
trained standard soft prompt. This hybrid design proves especially effective
for target languages that benefit from both broadly shared structure and
language-specific alignment. Experiments on the SIB-200 benchmark reveal a
consistent trade-off: XPE is most effective for low-performing languages, while
hybrid variants offer broader adaptability across multilingual settings.

</details>


### [51] [Making Qwen3 Think in Korean with Reinforcement Learning](https://arxiv.org/abs/2508.10355)
*Jungyup Lee,Jemin Kim,Sang Park,SeungJae Lee*

Main category: cs.CL

TL;DR: 提出两阶段微调方法，通过监督微调和改进型GRPO强化学习，实现Qwen3 14B模型完全使用韩语进行思维推理


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在韩语推理任务中存在的'思维翻译'瓶颈问题，提升韩语场景下的逻辑推理能力与整体对齐表现

Method: 第一阶段采用高质量韩语推理数据集进行监督微调(SFT)，第二阶段引入带Oracle校准的Group Relative Policy Optimization(GRPO)算法进行强化学习

Result: 模型在数学/编程等复杂推理基准上显著提升，保持原有知识能力的同时实现全韩语链式思维推理，成功解决GRPO训练中的策略崩溃问题

Conclusion: 该方法成功构建韩语原生推理能力验证框架，为其他语言场景的模型优化提供可复现的技术路径

Abstract: We present a two-stage fine-tuning approach to make the large language model
Qwen3 14B "think" natively in Korean. In the first stage, supervised
fine-tuning (SFT) on a high-quality Korean reasoning dataset establishes a
strong foundation in Korean logical reasoning, yielding notable improvements in
Korean-language tasks and even some gains in general reasoning ability. In the
second stage, we employ reinforcement learning with a customized Group Relative
Policy Optimization (GRPO) algorithm to further enhance both Korean reasoning
alignment and overall problem-solving performance. We address critical
stability challenges in GRPO training - such as reward hacking and policy
collapse - by introducing an oracle judge model that calibrates the reward
signal. Our approach achieves stable learning (avoiding the collapse observed
in naive GRPO) and leads to steady, incremental performance gains. The final
RL-tuned model demonstrates substantially improved results on advanced
reasoning benchmarks (particularly math and coding tasks) while maintaining
knowledge and language proficiency, successfully conducting its internal
chain-of-thought entirely in Korean.

</details>


### [52] [Advancing Cross-lingual Aspect-Based Sentiment Analysis with LLMs and Constrained Decoding for Sequence-to-Sequence Models](https://arxiv.org/abs/2508.10366)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 提出无需翻译工具的新序列到序列方法，通过约束解码将跨语言ABSA性能提升10%，验证多语言大模型与英语中心模型的差异


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言ABSA研究中过度依赖翻译工具的问题，扩展跨语言方法对复杂任务的适用性

Method: 基于约束解码的端到端方法，直接处理多语言ABSA任务，避免传统翻译工具的使用流程

Result: 跨语言ABSA性能提升最高达10%，微调多语言大模型效果可比拟，英语中心LLM表现显著落后

Conclusion: 该方法为跨语言ABSA提供了更高效的实现路径，同时揭示了语言中心性对LLM跨语言能力的重要影响

Abstract: Aspect-based sentiment analysis (ABSA) has made significant strides, yet
challenges remain for low-resource languages due to the predominant focus on
English. Current cross-lingual ABSA studies often centre on simpler tasks and
rely heavily on external translation tools. In this paper, we present a novel
sequence-to-sequence method for compound ABSA tasks that eliminates the need
for such tools. Our approach, which uses constrained decoding, improves
cross-lingual ABSA performance by up to 10\%. This method broadens the scope of
cross-lingual ABSA, enabling it to handle more complex tasks and providing a
practical, efficient alternative to translation-dependent techniques.
Furthermore, we compare our approach with large language models (LLMs) and show
that while fine-tuned multilingual LLMs can achieve comparable results,
English-centric LLMs struggle with these tasks.

</details>


### [53] [Large Language Models for Summarizing Czech Historical Documents and Beyond](https://arxiv.org/abs/2508.10368)
*Václav Tran,Jakub Šmíd,Jiří Martínek,Ladislav Lenc,Pavel Král*

Main category: cs.CL

TL;DR: 论文探索使用Mistral和mT5模型进行捷克语文本摘要，在现代数据集SumeCzech取得新SOTA，并发布首个历史文档摘要数据集Posel od Čerchova


<details>
  <summary>Details</summary>
Motivation: 捷克语文本摘要研究滞后，尤其历史文献存在语言复杂性高、标注数据稀缺的问题。现有研究多集中于英语等高资源语言，需填补捷克语在该领域的空白

Method: 采用Mistral和mT5大语言模型进行迁移学习，构建包含现代与历史捷克语的双层实验框架

Result: 1. 在SumeCzech数据集上创造新SOTA；2. 发布包含历史文档的Posel od Čerchova数据集并提供基线性能

Conclusion: 该研究突破了捷克语文本摘要的技术瓶颈，为历史文献处理提供新工具，推动斯拉夫语系NLP研究的发展

Abstract: Text summarization is the task of shortening a larger body of text into a
concise version while retaining its essential meaning and key information.
While summarization has been significantly explored in English and other
high-resource languages, Czech text summarization, particularly for historical
documents, remains underexplored due to linguistic complexities and a scarcity
of annotated datasets. Large language models such as Mistral and mT5 have
demonstrated excellent results on many natural language processing tasks and
languages. Therefore, we employ these models for Czech summarization, resulting
in two key contributions: (1) achieving new state-of-the-art results on the
modern Czech summarization dataset SumeCzech using these advanced models, and
(2) introducing a novel dataset called Posel od \v{C}erchova for summarization
of historical Czech documents with baseline results. Together, these
contributions provide a great potential for advancing Czech text summarization
and open new avenues for research in Czech historical text processing.

</details>


### [54] [Improving Generative Cross-lingual Aspect-Based Sentiment Analysis with Constrained Decoding](https://arxiv.org/abs/2508.10369)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 提出基于约束解码的序列到序列模型，跨语言ABSA性能平均提升5%，支持多任务处理，在七种语言六个任务中超越现有方法，并评估LLMs在不同场景的表现


<details>
  <summary>Details</summary>
Motivation: 现有跨语言ABSA方法局限于简单任务且依赖翻译工具，低资源语言研究不足

Method: 使用约束解码机制的序列到序列模型，消除翻译工具依赖，通过多任务联合训练提升模型效能

Result: 七种语言六个任务中超越SOTA，最复杂任务提升5%，多任务场景提升超10%；LLMs微调后具备竞争力但计算成本高

Conclusion: 该方法为跨语言ABSA提供可靠解决方案，建立新基准，揭示LLMs在跨语言场景的应用潜力与局限性，推动领域发展

Abstract: While aspect-based sentiment analysis (ABSA) has made substantial progress,
challenges remain for low-resource languages, which are often overlooked in
favour of English. Current cross-lingual ABSA approaches focus on limited, less
complex tasks and often rely on external translation tools. This paper
introduces a novel approach using constrained decoding with
sequence-to-sequence models, eliminating the need for unreliable translation
tools and improving cross-lingual performance by 5\% on average for the most
complex task. The proposed method also supports multi-tasking, which enables
solving multiple ABSA tasks with a single model, with constrained decoding
boosting results by more than 10\%.
  We evaluate our approach across seven languages and six ABSA tasks,
surpassing state-of-the-art methods and setting new benchmarks for previously
unexplored tasks. Additionally, we assess large language models (LLMs) in
zero-shot, few-shot, and fine-tuning scenarios. While LLMs perform poorly in
zero-shot and few-shot settings, fine-tuning achieves competitive results
compared to smaller multilingual models, albeit at the cost of longer training
and inference times.
  We provide practical recommendations for real-world applications, enhancing
the understanding of cross-lingual ABSA methodologies. This study offers
valuable insights into the strengths and limitations of cross-lingual ABSA
approaches, advancing the state-of-the-art in this challenging research domain.

</details>


### [55] [Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts](https://arxiv.org/abs/2508.10390)
*Chiyu Zhang,Lu Zhou,Xiaogang Xu,Jiafei Wu,Liming Fang,Zhe Liu*

Main category: cs.CL

TL;DR: 提出混合评估框架MDH结合LLM与人工监督，开发D-Attack/DH-CoT攻击策略提升越狱成功率，并发布完整技术资产。


<details>
  <summary>Details</summary>
Motivation: 现有红队数据集存在非恶意提示污染评估，传统检测方法依赖人工标注（低效）或LLM（准确率不稳定），需平衡精度与效率。

Method: 1. MDH框架：LLM标注+人工审核实现数据集清洗
2. D-Attack：利用上下文模拟
3. DH-CoT：劫持思维链增强攻击

Result: MDH有效提升恶意内容检测效果，DH-CoT使越狱成功率提升30.5%，完整技术资产将在GitHub开源。

Conclusion: 混合框架显著提升检测效率，开发者消息工程对攻击有效性影响显著，为AI安全提供新方法论与实践资源。

Abstract: Evaluating jailbreak attacks is challenging when prompts are not overtly
harmful or fail to induce harmful outputs. Unfortunately, many existing
red-teaming datasets contain such unsuitable prompts. To evaluate attacks
accurately, these datasets need to be assessed and cleaned for maliciousness.
However, existing malicious content detection methods rely on either manual
annotation, which is labor-intensive, or large language models (LLMs), which
have inconsistent accuracy in harmful types. To balance accuracy and
efficiency, we propose a hybrid evaluation framework named MDH (Malicious
content Detection based on LLMs with Human assistance) that combines LLM-based
annotation with minimal human oversight, and apply it to dataset cleaning and
detection of jailbroken responses. Furthermore, we find that well-crafted
developer messages can significantly boost jailbreak success, leading us to
propose two new strategies: D-Attack, which leverages context simulation, and
DH-CoT, which incorporates hijacked chains of thought. The Codes, datasets,
judgements, and detection results will be released in github repository:
https://github.com/AlienZhang1996/DH-CoT.

</details>


### [56] [Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation](https://arxiv.org/abs/2508.10404)
*Huizhen Shu,Xuying Li,Qirui Wang,Yuji Kosuga,Mengqiu Tian,Zhuo Li*

Main category: cs.CL

TL;DR: 开发了基于稀疏自编码器的对抗文本生成框架SFPF，通过扰动高激活特征突破现有防御机制


<details>
  <summary>Details</summary>
Motivation: 随着NLP和LLM的快速发展，生成有效对抗样本来发现模型漏洞成为改进鲁棒性的关键挑战

Method: 使用稀疏自编码器重建隐藏层表示，对成功攻击文本进行特征聚类，扰动高激活特征生成新对抗文本

Result: 实验显示SFPF生成的对抗文本能绕过前沿防御机制，但效果受提示词和网络层影响，通用性待验证

Conclusion: 该方法揭示了当前NLP系统的持续漏洞，但在跨架构和大模型的通用性方面仍需进一步研究

Abstract: With the rapid proliferation of Natural Language Processing (NLP), especially
Large Language Models (LLMs), generating adversarial examples to jailbreak LLMs
remains a key challenge for understanding model vulnerabilities and improving
robustness. In this context, we propose a new black-box attack method that
leverages the interpretability of large models. We introduce the Sparse Feature
Perturbation Framework (SFPF), a novel approach for adversarial text generation
that utilizes sparse autoencoders to identify and manipulate critical features
in text. After using the SAE model to reconstruct hidden layer representations,
we perform feature clustering on the successfully attacked texts to identify
features with higher activations. These highly activated features are then
perturbed to generate new adversarial texts. This selective perturbation
preserves the malicious intent while amplifying safety signals, thereby
increasing their potential to evade existing defenses. Our method enables a new
red-teaming strategy that balances adversarial effectiveness with safety
alignment. Experimental results demonstrate that adversarial texts generated by
SFPF can bypass state-of-the-art defense mechanisms, revealing persistent
vulnerabilities in current NLP systems.However, the method's effectiveness
varies across prompts and layers, and its generalizability to other
architectures and larger models remains to be validated.

</details>


### [57] [ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning](https://arxiv.org/abs/2508.10419)
*Juyuan Wang,Rongchen Zhao,Wei Wei,Yufeng Wang,Mo Yu,Jie Zhou,Jin Xu,Liyan Xu*

Main category: cs.CL

TL;DR: ComoRAG提出动态迭代的RAG方法，通过模拟人类认知中的记忆整合机制，显著提升长文本叙事理解效果，在200K+ tokens基准测试中相对提升11%


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在长上下文处理中存在静态检索局限，无法捕捉动态演变的人物关系。受人类认知中记忆整合过程启发，需要迭代式的证据获取与知识整合机制

Method: 建立动态记忆工作区，通过多轮推理循环：1) 生成探索性查询路径 2) 整合新证据到全局记忆池 3) 形成连贯上下文解决复杂查询

Result: 在四个长文本叙事基准测试中(>200K tokens)，相对最强基线提升11%，特别在需要全局理解的复杂查询上优势显著

Conclusion: 该方法为长上下文理解提供了认知科学启发的范式，通过状态化推理机制突破传统RAG局限，代码已开源供社区使用

Abstract: Narrative comprehension on long stories and novels has been a challenging
domain attributed to their intricate plotlines and entangled, often evolving
relations among characters and entities. Given the LLM's diminished reasoning
over extended context and high computational cost, retrieval-based approaches
remain a pivotal role in practice. However, traditional RAG methods can fall
short due to their stateless, single-step retrieval process, which often
overlooks the dynamic nature of capturing interconnected relations within
long-range context. In this work, we propose ComoRAG, holding the principle
that narrative reasoning is not a one-shot process, but a dynamic, evolving
interplay between new evidence acquisition and past knowledge consolidation,
analogous to human cognition when reasoning with memory-related signals in the
brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes
iterative reasoning cycles while interacting with a dynamic memory workspace.
In each cycle, it generates probing queries to devise new exploratory paths,
then integrates the retrieved evidence of new aspects into a global memory
pool, thereby supporting the emergence of a coherent context for the query
resolution. Across four challenging long-context narrative benchmarks (200K+
tokens), ComoRAG outperforms strong RAG baselines with consistent relative
gains up to 11% compared to the strongest baseline. Further analysis reveals
that ComoRAG is particularly advantageous for complex queries requiring global
comprehension, offering a principled, cognitively motivated paradigm for
retrieval-based long context comprehension towards stateful reasoning. Our code
is publicly released at https://github.com/EternityJune25/ComoRAG

</details>


### [58] [Evaluating LLMs on Chinese Idiom Translation](https://arxiv.org/abs/2508.10421)
*Cai Yang,Yao Dou,David Heineman,Xiaofeng Wu,Wei Xu*

Main category: cs.CL

TL;DR: 提出IdiomEval框架揭示主流翻译系统在中文成语翻译中存在高错误率（最佳系统GPT-4错误率28%），并开发改进模型实现0.68错误检测F1值


<details>
  <summary>Details</summary>
Motivation: 中文成语因文化内涵和特殊结构导致机器翻译困难，现有系统缺乏有效评估标准且错误类型不明确

Method: 构建包含900个跨领域翻译对的标注数据集（涵盖网页/新闻/维基/社交媒体），建立错误分类体系，分析9个主流系统表现并开发检测模型

Result: 现有翻译系统最高错误率28%（GPT-4），传统评估指标与人工评分相关性<0.48，改进模型错误检测F1达0.68

Conclusion: 首次系统揭示中文成语翻译质量瓶颈，建立标准化评估框架与高效检测模型，为跨文化NLP研究提供新基准

Abstract: Idioms, whose figurative meanings usually differ from their literal
interpretations, are common in everyday language, especially in Chinese, where
they often contain historical references and follow specific structural
patterns. Despite recent progress in machine translation with large language
models, little is known about Chinese idiom translation. In this work, we
introduce IdiomEval, a framework with a comprehensive error taxonomy for
Chinese idiom translation. We annotate 900 translation pairs from nine modern
systems, including GPT-4o and Google Translate, across four domains: web, news,
Wikipedia, and social media. We find these systems fail at idiom translation,
producing incorrect, literal, partial, or even missing translations. The
best-performing system, GPT-4, makes errors in 28% of cases. We also find that
existing evaluation metrics measure idiom quality poorly with Pearson
correlation below 0.48 with human ratings. We thus develop improved models that
achieve F$_1$ scores of 0.68 for detecting idiom translation errors.

</details>


### [59] [Computational Economics in Large Language Models: Exploring Model Behavior and Incentive Design under Resource Constraints](https://arxiv.org/abs/2508.10426)
*Sandeep Reddy,Kabir Khan,Rohit Patil,Ananya Chakraborty,Faizan A. Khan,Swati Kulkarni,Arjun Verma,Neha Singh*

Main category: cs.CL

TL;DR: 提出基于经济原则的激励训练框架，通过稀疏激活机制在保持模型性能的同时显著降低LLMs计算成本


<details>
  <summary>Details</summary>
Motivation: 大型语言模型面临高计算成本限制，需通过资源优化实现高效推理

Method: 1. 实证验证注意力重分配机制
2. 提出可微分计算成本驱动的激励训练范式
3. 构建稀疏激活的帕累托前沿模型族

Result: 在GLUE和WikiText-103任务中实现FLOPS降低40%，延迟减少，注意力模式更可解释

Conclusion: 经济原则为资源约束下构建高效、自适应且透明的LLMs提供了理论基础

Abstract: Large language models (LLMs) are limited by substantial computational cost.
We introduce a "computational economics" framework that treats an LLM as an
internal economy of resource-constrained agents (attention heads and neuron
blocks) that must allocate scarce computation to maximize task utility. First,
we show empirically that when computation is scarce, standard LLMs reallocate
attention toward high-value tokens while preserving accuracy. Building on this
observation, we propose an incentive-driven training paradigm that augments the
task loss with a differentiable computation cost term, encouraging sparse and
efficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method
yields a family of models that trace a Pareto frontier and consistently
dominate post-hoc pruning; for a similar accuracy we obtain roughly a forty
percent reduction in FLOPS and lower latency, together with more interpretable
attention patterns. These results indicate that economic principles offer a
principled route to designing efficient, adaptive, and more transparent LLMs
under strict resource constraints.

</details>


### [60] [DiFaR: Enhancing Multimodal Misinformation Detection with Diverse, Factual, and Relevant Rationales](https://arxiv.org/abs/2508.10444)
*Herun Wan,Jiaying Wu,Minnan Luo,Xiangzheng Kong,Zihan Ma,Zhi Zeng*

Main category: cs.CL

TL;DR: 提出DiFaR框架，通过多样化思维链提示和过滤模块解决多模态虚假信息检测中生成理由的三大挑战（多样性不足/事实错误/噪声干扰），实验显示最高提升检测器8.7%效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于LVLMs生成文本推理的方法存在三大限制：生成理由多样性不足导致覆盖有限、模型幻觉导致事实错误、无关内容引入噪声，影响虚假信息检测效果。

Method: 1. 使用五种思维链提示激发多样化推理路径；2. 设计轻量级后过滤模块，通过句子级事实性评分和相关性评分筛选优质理由。

Result: 在四个主流基准测试中，DiFaR超越四类基线方法达5.9%，并使现有检测器性能最高提升8.7%。自动指标和人工评估均验证其在多样性/事实性/相关性维度的显著改进。

Conclusion: DiFaR通过多样化生成与双重过滤机制，有效提升多模态虚假信息检测性能，验证了高质量推理对检测器增强的关键作用。

Abstract: Generating textual rationales from large vision-language models (LVLMs) to
support trainable multimodal misinformation detectors has emerged as a
promising paradigm. However, its effectiveness is fundamentally limited by
three core challenges: (i) insufficient diversity in generated rationales, (ii)
factual inaccuracies due to hallucinations, and (iii) irrelevant or conflicting
content that introduces noise. We introduce DiFaR, a detector-agnostic
framework that produces diverse, factual, and relevant rationales to enhance
misinformation detection. DiFaR employs five chain-of-thought prompts to elicit
varied reasoning traces from LVLMs and incorporates a lightweight post-hoc
filtering module to select rationale sentences based on sentence-level
factuality and relevance scores. Extensive experiments on four popular
benchmarks demonstrate that DiFaR outperforms four baseline categories by up to
5.9% and boosts existing detectors by as much as 8.7%. Both automatic metrics
and human evaluations confirm that DiFaR significantly improves rationale
quality across all three dimensions.

</details>


### [61] [When Explainability Meets Privacy: An Investigation at the Intersection of Post-hoc Explainability and Differential Privacy in the Context of Natural Language Processing](https://arxiv.org/abs/2508.10482)
*Mahdi Dhaini,Stephen Meisenbacher,Ege Erdogan,Florian Matthes,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 探讨NLP中隐私保护与可解释性的权衡关系，发现二者可共存但受任务性质、私有化方法和解释方法选择等多因素影响


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对隐私保护与可解释性交叉领域的探讨，存在理解两者是否兼容的理论空白

Method: 采用差分隐私(DP)与事后解释方法，通过实证分析研究隐私-可解释性权衡关系

Result: 隐私与可解释性存在复杂互动关系，其兼容性取决于下游任务特征、文本私有化方法和解释方法选择

Conclusion: 隐私与可解释性具备共存潜力，提出未来研究应关注方法组合优化的实践建议

Abstract: In the study of trustworthy Natural Language Processing (NLP), a number of
important research fields have emerged, including that of
\textit{explainability} and \textit{privacy}. While research interest in both
explainable and privacy-preserving NLP has increased considerably in recent
years, there remains a lack of investigation at the intersection of the two.
This leaves a considerable gap in understanding of whether achieving
\textit{both} explainability and privacy is possible, or whether the two are at
odds with each other. In this work, we conduct an empirical investigation into
the privacy-explainability trade-off in the context of NLP, guided by the
popular overarching methods of \textit{Differential Privacy} (DP) and Post-hoc
Explainability. Our findings include a view into the intricate relationship
between privacy and explainability, which is formed by a number of factors,
including the nature of the downstream task and choice of the text
privatization and explainability method. In this, we highlight the potential
for privacy and explainability to co-exist, and we summarize our findings in a
collection of practical recommendations for future work at this important
intersection.

</details>


### [62] [When Language Overrules: Revealing Text Dominance in Multimodal Large Language Models](https://arxiv.org/abs/2508.10552)
*Huyu Wu,Meng Tang,Xinhan Zheng,Haiyun Jiang*

Main category: cs.CL

TL;DR: 本文系统研究了多模态大语言模型中的文本主导问题，提出了两种评估指标并验证了基于令牌压缩的解决方案有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注视觉语言任务中的文本主导现象，而本文首次系统性分析图像/视频/音频/时间序列/图表等多种模态，揭示该问题的普遍性与深层成因

Method: 提出模态主导指数(MDI)和注意力效率指数(AEI)量化评估，从注意力稀释、融合架构设计、任务公式化三个维度开展归因分析，采用令牌压缩方法重新平衡模型注意力

Result: 在LLaVA-7B上应用令牌压缩后，MDI从10.23显著降至0.86；实验证实文本主导在所有测试模态中普遍存在，注意力冗余是非文本模态效率低下的主因

Conclusion: 研究为开发更均衡的多模态模型奠定基础，提出的方法论框架和简单有效的压缩策略展示了解决模态失衡问题的可行路径

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities across a diverse range of multimodal tasks. However, these models
suffer from a core problem known as text dominance: they depend heavily on text
for their inference, while underutilizing other modalities. While prior work
has acknowledged this phenomenon in vision-language tasks, often attributing it
to data biases or model architectures. In this paper, we conduct the first
systematic investigation of text dominance across diverse data modalities,
including images, videos, audio, time-series, and graphs. To measure this
imbalance, we propose two evaluation metrics: the Modality Dominance Index
(MDI) and the Attention Efficiency Index (AEI). Our comprehensive analysis
reveals that text dominance is both significant and pervasive across all tested
modalities. Our in-depth analysis identifies three underlying causes: attention
dilution from severe token redundancy in non-textual modalities, the influence
of fusion architecture design, and task formulations that implicitly favor
textual inputs. Furthermore, we propose a simple token compression method that
effectively rebalances model attention. Applying this method to LLaVA-7B, for
instance, drastically reduces its MDI from 10.23 to a well-balanced value of
0.86. Our analysis and methodological framework offer a foundation for the
development of more equitable and comprehensive multimodal language models.

</details>


### [63] [eDIF: A European Deep Inference Fabric for Remote Interpretability of LLM](https://arxiv.org/abs/2508.10553)
*Irma Heithoff. Marc Guggenberger,Sandra Kalogiannis,Susanne Mayer,Fabian Maag,Sigurd Schacht,Carsten Lanquillon*

Main category: cs.CL

TL;DR: 欧洲深度推理框架(eDIF)可行性研究证实了通过分布式GPU集群和远程实验平台支持大语言模型机械可解释性研究的有效性


<details>
  <summary>Details</summary>
Motivation: 解决欧洲LLM解释性基础设施不足问题，促进研究社区对高级模型分析能力的民主化访问

Method: 部署NDIF兼容的GPU集群系统，通过NNsight API实现远程模型干预实验，组织16国研究者的结构化试点研究

Result: 用户参与度持续提升，平台运行稳定且远程实验能力获认可，同时发现激活数据下载延迟和执行中断等技术限制

Conclusion: 该基础设施为欧洲机械解释性研究奠定基础，未来将扩展部署规模、完善工具链并加强社区协作

Abstract: This paper presents a feasibility study on the deployment of a European Deep
Inference Fabric (eDIF), an NDIF-compatible infrastructure designed to support
mechanistic interpretability research on large language models. The need for
widespread accessibility of LLM interpretability infrastructure in Europe
drives this initiative to democratize advanced model analysis capabilities for
the research community. The project introduces a GPU-based cluster hosted at
Ansbach University of Applied Sciences and interconnected with partner
institutions, enabling remote model inspection via the NNsight API. A
structured pilot study involving 16 researchers from across Europe evaluated
the platform's technical performance, usability, and scientific utility. Users
conducted interventions such as activation patching, causal tracing, and
representation analysis on models including GPT-2 and DeepSeek-R1-70B. The
study revealed a gradual increase in user engagement, stable platform
performance throughout, and a positive reception of the remote experimentation
capabilities. It also marked the starting point for building a user community
around the platform. Identified limitations such as prolonged download
durations for activation data as well as intermittent execution interruptions
are addressed in the roadmap for future development. This initiative marks a
significant step towards widespread accessibility of LLM interpretability
infrastructure in Europe and lays the groundwork for broader deployment,
expanded tooling, and sustained community collaboration in mechanistic
interpretability research.

</details>


### [64] [Neural Machine Translation for Coptic-French: Strategies for Low-Resource Ancient Languages](https://arxiv.org/abs/2508.10683)
*Nasma Chaoui,Richard Khoury*

Main category: cs.CL

TL;DR: 首个系统性研究科普特语-法语翻译策略，通过圣经语料库多策略评估，证实噪声感知微调显著提升质量，为历史语言翻译工具开发提供洞见


<details>
  <summary>Details</summary>
Motivation: 填补科普特语（历史语言）翻译工具的系统研究空白，探索枢轴翻译/直接翻译效能、预训练影响、多版本微调价值及模型抗噪性等核心问题

Method: 构建包含四种策略评估的pipeline：1）枢轴vs直接翻译 2）预训练影响 3）多版本微调 4）噪声鲁棒性测试，采用对齐圣经文本进行风格多样化+噪声感知的微调训练

Result: 实验表明：结合文体多样性及噪声感知的微调策略使翻译质量提升显著，尤其在处理历史语言复杂语境时表现突出

Conclusion: 该研究框架为历史语言机器翻译建立了方法论范式，其噪声适应训练和多策略评估体系可直接迁移至其他低资源古代语言的翻译工具开发

Abstract: This paper presents the first systematic study of strategies for translating
Coptic into French. Our comprehensive pipeline systematically evaluates: pivot
versus direct translation, the impact of pre-training, the benefits of
multi-version fine-tuning, and model robustness to noise. Utilizing aligned
biblical corpora, we demonstrate that fine-tuning with a stylistically-varied
and noise-aware training corpus significantly enhances translation quality. Our
findings provide crucial practical insights for developing translation tools
for historical languages in general.

</details>


### [65] [Continuous Bangla Sign Language Translation: Mitigating the Expense of Gloss Annotation with the Assistance of Graph](https://arxiv.org/abs/2508.10687)
*Safaeid Hossain Arib,Rabeya Akter,Sejuti Rahman*

Main category: cs.CL

TL;DR: 提出融合图卷积网络与Transformer的新方法，显著提升连续手语翻译性能并在多数据集达到SOTA


<details>
  <summary>Details</summary>
Motivation: 解决现有手语翻译方法对注释依赖性强、跨语言沟通效率低的问题，提升聋哑人士的沟通可及性

Method: 创新性整合STGCN-LSTM时空图卷积网络与Transformer架构，探索注意力机制与图神经网络的多种融合策略

Result: 在RWTH-PHOENIX-2014T等四个数据集上实现SOTA，BLEU-4分别提升4.01/2.07/0.5，并首次建立BornilDB v1.0基准

Conclusion: 该方法突破了传统基于注释的翻译范式，通过架构融合策略为手语翻译领域建立了新的技术标杆，显著提升社会包容性

Abstract: Millions of individuals worldwide are affected by deafness and hearing
impairment. Sign language serves as a sophisticated means of communication for
the deaf and hard of hearing. However, in societies that prioritize spoken
languages, sign language often faces underestimation, leading to communication
barriers and social exclusion. The Continuous Bangla Sign Language Translation
project aims to address this gap by enhancing translation methods. While recent
approaches leverage transformer architecture for state-of-the-art results, our
method integrates graph-based methods with the transformer architecture. This
fusion, combining transformer and STGCN-LSTM architectures, proves more
effective in gloss-free translation. Our contributions include architectural
fusion, exploring various fusion strategies, and achieving a new
state-of-the-art performance on diverse sign language datasets, namely
RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach
demonstrates superior performance compared to current translation outcomes
across all datasets, showcasing notable improvements of BLEU-4 scores of 4.01,
2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in
RWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce
benchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a
benchmark for future research, emphasizing the importance of gloss-free
translation to improve communication accessibility for the deaf and hard of
hearing.

</details>


### [66] [Learning from Natural Language Feedback for Personalized Question Answering](https://arxiv.org/abs/2508.10695)
*Alireza Salemi,Hamed Zamani*

Main category: cs.CL

TL;DR: 论文提出VAC框架，用自然语言反馈（NLF）替代标量奖励信号，通过交替优化反馈模型和策略模型提升个性化问答效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于标量奖励的强化学习方法反馈信号弱且缺乏指导性，限制了语言模型的个性化学习效率和质量。

Method: 采用自然语言反馈作为监督信号，通过交替训练反馈模型生成NLF、策略模型根据NLF迭代优化的循环框架，最终得到无需实时反馈的个性化策略模型。

Result: 在LaMP-QA基准测试中显著超越现有方法，人工评估验证生成质量提升，NLF优化效果优于传统奖励机制。

Conclusion: 自然语言反馈为个性化问答优化提供了更有效的监督信号，VAC框架实现了更高质量的个性化响应生成。

Abstract: Personalization is crucial for enhancing both the effectiveness and user
satisfaction of language technologies, particularly in information-seeking
tasks like question answering. Current approaches for personalizing large
language models (LLMs) often rely on retrieval-augmented generation (RAG),
followed by reinforcement learning with scalar reward signals to teach models
how to use retrieved personal context. We believe that these scalar rewards
sometimes provide weak, non-instructive feedback, limiting learning efficiency
and personalization quality. We introduce VAC, a novel framework for
personalized response generation that replaces scalar rewards with natural
language feedback (NLF) that are generated conditioned on the user profiles and
the question narratives. NLF serves as a rich and actionable supervision
signal, allowing the policy model to iteratively refine its outputs and
internalize effective personalization strategies. Training alternates between
optimizing the feedback model and fine-tuning the policy model on the improved
responses, resulting in a policy model that no longer requires feedback at
inference. Evaluation on the LaMP-QA benchmark that consists of three diverse
domains demonstrates consistent and significant improvements over the
state-of-the-art results. Human evaluations further confirm the superior
quality of the generated responses. These results demonstrate that NLF provides
more effective signals for optimizing personalized question answering.

</details>


### [67] [Thinking Inside the Mask: In-Place Prompting in Diffusion LLMs](https://arxiv.org/abs/2508.10736)
*Xiangqi Jin,Yuxuan Wang,Yifeng Gao,Zichen Wen,Biqing Qi,Dongrui Liu,Linfeng Zhang*

Main category: cs.CL

TL;DR: 提出ICE框架，通过原位思维链提示和提前退出机制改进扩散大语言模型，在提升推理准确性的同时显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 传统LLMs的前缀提示范式限制了双向信息利用，扩散LLMs的双向注意力机制和迭代优化特性为灵活提示策略提供了新可能

Method: 1. 将前缀提示转化为原位提示，在迭代过程中直接整合到掩码位置
2. 置信度感知的提前退出机制动态终止计算

Result: GSM8K准确率提升17.29%且加速4.12倍，MMLU任务实现276.67倍加速同时保持性能

Conclusion: ICE框架有效结合dLLMs特性，在复杂推理任务中实现精度-效率的突破性平衡

Abstract: Despite large language models (LLMs) have achieved remarkable success, their
prefix-only prompting paradigm and sequential generation process offer limited
flexibility for bidirectional information. Diffusion large language models
(dLLMs) present new opportunities through their bidirectional attention
mechanisms and iterative refinement processes, enabling more flexible in-place
prompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting
with Early Exit), a novel framework that transforms prefix-only prompting into
in-place prompting specifically designed for dLLMs. ICE integrates in-place
prompts directly within masked token positions during iterative refinement and
employs a confidence-aware early exit mechanism to significantly reduce
computational overhead. Extensive experiments demonstrate ICE's effectiveness,
achieving up to 17.29% accuracy improvement with 4.12$\times$ speedup on GSM8K,
and up to 276.67$\times$ acceleration on MMLU while maintaining competitive
performance.

</details>


### [68] [Beyond "Not Novel Enough": Enriching Scholarly Critique with LLM-Assisted Feedback](https://arxiv.org/abs/2508.10795)
*Osama Mohammed Afzal,Preslav Nakov,Tom Hope,Iryna Gurevych*

Main category: cs.CL

TL;DR: 提出结构化LLM辅助方法自动评估论文新颖性，在ICLR评估中达到86.5%人类推理对齐度


<details>
  <summary>Details</summary>
Motivation: 解决高产出领域同行评审资源紧张问题，减少人工新颖性评估的主观偏差和不一致性

Method: 三阶段框架：1) 内容提取 2) 相关工作检索与综合 3) 结构化证据比对，融合独立验证和上下文推理机制

Result: 在182份ICLR投稿评估中，相较现有LLM基线提升20%以上，结论一致性达75.3%

Conclusion: 结构化方法增强评审严谨性和透明度，为人类专家提供有效辅助而非替代，推动同行评审系统优化

Abstract: Novelty assessment is a central yet understudied aspect of peer review,
particularly in high volume fields like NLP where reviewer capacity is
increasingly strained. We present a structured approach for automated novelty
evaluation that models expert reviewer behavior through three stages: content
extraction from submissions, retrieval and synthesis of related work, and
structured comparison for evidence based assessment. Our method is informed by
a large scale analysis of human written novelty reviews and captures key
patterns such as independent claim verification and contextual reasoning.
Evaluated on 182 ICLR 2025 submissions with human annotated reviewer novelty
assessments, the approach achieves 86.5% alignment with human reasoning and
75.3% agreement on novelty conclusions - substantially outperforming existing
LLM based baselines. The method produces detailed, literature aware analyses
and improves consistency over ad hoc reviewer judgments. These results
highlight the potential for structured LLM assisted approaches to support more
rigorous and transparent peer review without displacing human expertise. Data
and code are made available.

</details>


### [69] [Reinforced Language Models for Sequential Decision Making](https://arxiv.org/abs/2508.10839)
*Jim Dilkes,Vahid Yazdanpanah,Sebastian Stein*

Main category: cs.CL

TL;DR: 提出MS-GRPO算法，通过多步信用分配策略提升小规模语言模型的序列决策能力，3B模型性能超越72B基线50%。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为决策代理依赖大参数量模型，现有后训练方法无法有效处理多步任务信用分配问题。

Method: 基于文本随机博弈框架，设计多步组相对策略优化算法，采用累积奖励分步分配策略和优势加权采样方法。

Result: 在Frozen Lake任务中，后训练的3B模型比72B基线性能提升50%，Snake任务验证算法有效性。

Conclusion: 定向后训练可替代模型规模化，为构建高效序列决策代理提供新路径。

Abstract: Large Language Models (LLMs) show potential as sequential decision-making
agents, but their application is often limited due to a reliance on large,
computationally expensive models. This creates a need to improve smaller
models, yet existing post-training methods are designed for single-turn
interactions and cannot handle credit assignment in multi-step agentic tasks.
To address this, we introduce Multi-Step Group-Relative Policy Optimization
(MS-GRPO), a new algorithm for post-training LLM agents, grounded in formal
Text-Mediated Stochastic Game (TSMG) and Language-Agent Policy (LAP)
frameworks. For credit assignment, MS-GRPO attributes the entire cumulative
episode reward to each individual episode step. We supplement this algorithm
with a novel absolute-advantage-weighted episode sampling strategy that we show
improves training performance. We evaluate our approach by post-training a
3-billion parameter model on Snake and Frozen Lake. Our experiments demonstrate
that the method is effective in improving decision-making performance: our
post-trained 3B parameter model outperforms a 72B parameter baseline by 50% on
the Frozen Lake task. This work demonstrates that targeted post-training is a
practical and efficient alternative to relying on model scale for creating
sequential decision-making agents using LLMs.

</details>


### [70] [Psyche-R1: Towards Reliable Psychological LLMs through Unified Empathy, Expertise, and Reasoning](https://arxiv.org/abs/2508.10848)
*Chongyuan Dai,Jinpeng Hu,Hongchang Shi,Zhuo Li,Xun Yang,Meng Wang*

Main category: cs.CL

TL;DR: 开发首个中文心理大模型Psyche-R1，整合共情、专业知识和推理能力，通过创新数据流程和混合训练策略显著提升心理领域性能


<details>
  <summary>Details</summary>
Motivation: 解决心理健康专业人员短缺问题，弥补现有语言模型在心理领域推理机制研究的不足

Method: 设计包含7.5万高质量心理问题（带CoT推理）和7.3万共情对话的数据合成流程，采用GRPO优化+SFT的混合训练策略

Result: 7B模型在多个心理基准测试中达到与671B DeepSeek-R1相当的效果

Conclusion: Psyche-R1成功验证了整合共情、专业知识和推理的三位一体架构在心理LLM中的有效性

Abstract: Amidst a shortage of qualified mental health professionals, the integration
of large language models (LLMs) into psychological applications offers a
promising way to alleviate the growing burden of mental health disorders.
Recent reasoning-augmented LLMs have achieved remarkable performance in
mathematics and programming, while research in the psychological domain has
predominantly emphasized emotional support and empathetic dialogue, with
limited attention to reasoning mechanisms that are beneficial to generating
reliable responses. Therefore, in this paper, we propose Psyche-R1, the first
Chinese psychological LLM that jointly integrates empathy, psychological
expertise, and reasoning, built upon a novel data curation pipeline.
Specifically, we design a comprehensive data synthesis pipeline that produces
over 75k high-quality psychological questions paired with detailed rationales,
generated through chain-of-thought (CoT) reasoning and iterative
prompt-rationale optimization, along with 73k empathetic dialogues.
Subsequently, we employ a hybrid training strategy wherein challenging samples
are identified through a multi-LLM cross-selection strategy for group relative
policy optimization (GRPO) to improve reasoning ability, while the remaining
data is used for supervised fine-tuning (SFT) to enhance empathetic response
generation and psychological domain knowledge. Extensive experiment results
demonstrate the effectiveness of the Psyche-R1 across several psychological
benchmarks, where our 7B Psyche-R1 achieves comparable results to 671B
DeepSeek-R1.

</details>


### [71] [From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms](https://arxiv.org/abs/2508.10860)
*Zhaokun Jiang,Ziyin Zhang*

Main category: cs.CL

TL;DR: 提出结合特征工程、数据增强和可解释机器学习的三维框架，提升口译质量评估的透明度和可靠性


<details>
  <summary>Details</summary>
Motivation: 解决现有自动口译评估中语言质量研究不足、数据缺陷导致的模型效果差、预测不可解释三大缺陷

Method: 通过构造相关透明特征+SHAP分析，优先模型可解释性而非黑箱预测

Result: 在英汉交替传译数据集中验证：BLEURT/CometKiwi预测忠实度，停顿特征预测流畅度，中文短语多样性指标预测语言使用

Conclusion: 建立可扩展的透明评估体系，通过诊断性反馈支持学习者自我调节学习，突破传统自动评分的局限性

Abstract: Recent advancements in machine learning have spurred growing interests in
automated interpreting quality assessment. Nevertheless, existing research
suffers from insufficient examination of language use quality, unsatisfactory
modeling effectiveness due to data scarcity and imbalance, and a lack of
efforts to explain model predictions. To address these gaps, we propose a
multi-dimensional modeling framework that integrates feature engineering, data
augmentation, and explainable machine learning. This approach prioritizes
explainability over ``black box'' predictions by utilizing only
construct-relevant, transparent features and conducting Shapley Value (SHAP)
analysis. Our results demonstrate strong predictive performance on a novel
English-Chinese consecutive interpreting dataset, identifying BLEURT and
CometKiwi scores to be the strongest predictive features for fidelity,
pause-related features for fluency, and Chinese-specific phraseological
diversity metrics for language use. Overall, by placing particular emphasis on
explainability, we present a scalable, reliable, and transparent alternative to
traditional human evaluation, facilitating the provision of detailed diagnostic
feedback for learners and supporting self-regulated learning advantages not
afforded by automated scores in isolation.

</details>


### [72] [SSRL: Self-Search Reinforcement Learning](https://arxiv.org/abs/2508.10874)
*Yuchen Fan,Kaiyan Zhang,Heng Zhou,Yuxin Zuo,Yanxu Chen,Yu Fu,Xinwei Long,Xuekai Zhu,Che Jiang,Yuchen Zhang,Li Kang,Gang Chen,Cheng Huang,Zhizhou He,Bingning Wang,Lei Bai,Ning Ding,Bowen Zhou*

Main category: cs.CL

TL;DR: 通过结构化提示和强化学习训练，LLMs展现出替代外部搜索引擎完成搜索任务的潜力


<details>
  <summary>Details</summary>
Motivation: 降低强化学习训练中对昂贵外部搜索引擎的依赖，利用LLMs内部知识构建高效模拟器

Method: 提出Self-Search量化模型内在搜索能力，开发SSRL框架通过格式/规则奖励机制优化知识利用

Result: SSRL模型提供低成本稳定的训练环境，实现90%+的搜索任务准确率，支持无额外适配的搜索引擎整合

Conclusion: LLMs具备可激发的世界知识，SSRL可减少幻觉产生，模型内置知识库与外部工具可无缝集成

Abstract: We investigate the potential of large language models (LLMs) to serve as
efficient simulators for agentic search tasks in reinforcement learning (RL),
thereby reducing dependence on costly interactions with external search
engines. To this end, we first quantify the intrinsic search capability of LLMs
via structured prompting and repeated sampling, which we term Self-Search. Our
results reveal that LLMs exhibit strong scaling behavior with respect to the
inference budget, achieving high pass@k on question-answering benchmarks,
including the challenging BrowseComp task. Building on these observations, we
introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability
through format-based and rule-based rewards. SSRL enables models to iteratively
refine their knowledge utilization internally, without requiring access to
external tools. Empirical evaluations demonstrate that SSRL-trained policy
models provide a cost-effective and stable environment for search-driven RL
training, reducing reliance on external search engines and facilitating robust
sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world
knowledge that can be effectively elicited to achieve high performance; 2) SSRL
demonstrates the potential of leveraging internal knowledge to reduce
hallucination; 3) SSRL-trained models integrate seamlessly with external search
engines without additional effort. Our findings highlight the potential of LLMs
to support more scalable RL agent training.

</details>


### [73] [A Survey on Diffusion Language Models](https://arxiv.org/abs/2508.10875)
*Tianyi Li,Mingda Chen,Bowei Guo,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 扩散语言模型(DLMs)通过并行生成和迭代去噪机制，在推理速度和生成控制上展现出超越自回归模型的潜力，本文系统综述了其发展脉络、技术架构和应用前景。


<details>
  <summary>Details</summary>
Motivation: 旨在全面梳理DLMs领域的最新进展，建立完整技术框架，分析其相对于传统自回归模型的优势及现存挑战，推动该领域的持续发展。

Method: 采用多维分类法，从基础原理到SOTA模型进行系统分析，涵盖预训练策略、推理加速技术、多模态扩展方法，并结合实际应用场景展开讨论。

Result: DLMs已实现数倍推理加速且性能接近自回归模型，但在长序列处理、计算效率等方面仍存在瓶颈，需硬件和算法层面的协同优化。

Conclusion: DLMs代表着语言模型的重要发展方向，未来研究需在保持生成质量的同时突破效率限制，并探索更广泛的多模态应用场景。

Abstract: Diffusion Language Models (DLMs) are rapidly emerging as a powerful and
promising alternative to the dominant autoregressive (AR) paradigm. By
generating tokens in parallel through an iterative denoising process, DLMs
possess inherent advantages in reducing inference latency and capturing
bidirectional context, thereby enabling fine-grained control over the
generation process. While achieving a several-fold speed-up, recent
advancements have allowed DLMs to show performance comparable to their
autoregressive counterparts, making them a compelling choice for various
natural language processing tasks. In this survey, we provide a holistic
overview of the current DLM landscape. We trace its evolution and relationship
with other paradigms, such as autoregressive and masked language models, and
cover both foundational principles and state-of-the-art models. Our work offers
an up-to-date, comprehensive taxonomy and an in-depth analysis of current
techniques, from pre-training strategies to advanced post-training methods.
Another contribution of this survey is a thorough review of DLM inference
strategies and optimizations, including improvements in decoding parallelism,
caching mechanisms, and generation quality. We also highlight the latest
approaches to multimodal extensions of DLMs and delineate their applications
across various practical scenarios. Furthermore, our discussion addresses the
limitations and challenges of DLMs, including efficiency, long-sequence
handling, and infrastructure requirements, while outlining future research
directions to sustain progress in this rapidly evolving field. Project GitHub
is available at https://github.com/VILA-Lab/Awesome-DLMs.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [74] [B-repLer: Semantic B-rep Latent Editor using Large Language Models](https://arxiv.org/abs/2508.10201)
*Yilin Liu,Niladri Shekhar Dutt,Changjian Li,Niloy J. Mitra*

Main category: cs.GR

TL;DR: 提出B-repLer——首个针对B-rep CAD模型语义编辑的微调多模态大模型，通过自动生成数据集和新型架构实现可靠3D编辑


<details>
  <summary>Details</summary>
Motivation: 现有mLLMs在3D任务中存在三大瓶颈：1) 缺乏标注的3D数据 2) B-rep表示的脆弱性 3) 行业标准格式的语义理解困难

Method: 融合CAD工具链自动生成带构造历史的B-rep数据集，设计支持几何-拓扑联合编码的多模态架构，利用指令微调实现语义空间对齐

Result: 成功实现跨模态编辑（文本→B-rep修改），处理包括倒角参数调整、特征删除等复杂操作，保持100%的几何有效性

Conclusion: 证明了mLLMs在工程级3D编辑的可行性，开创了无标注数据驱动的CAD智能处理范式，为工业设计自动化提供新思路

Abstract: Multimodal large language models (mLLMs), trained in a mixed modal setting as
a universal model, have been shown to compete with or even outperform many
specialized algorithms for imaging and graphics tasks. As demonstrated across
many applications, mLLMs' ability to jointly process image and text data makes
them suitable for zero-shot applications or efficient fine-tuning towards
specialized tasks. However, they have had limited success in 3D analysis and
editing tasks. This is due to both the lack of suitable (annotated) 3D data as
well as the idiosyncrasies of 3D representations. In this paper, we investigate
whether mLLMs can be adapted to support high-level editing of Boundary
Representation (B-rep) CAD objects. B-reps remain the industry-standard for
precisely encoding engineering objects, but are challenging as the
representation is fragile (i.e. can easily lead to invalid CAD objects) and no
publicly available data source exists with semantically-annotated B-reps or CAD
construction history. We present B-repLer as a finetuned mLLM that can
understand text prompts and make semantic edits on given B-Reps to produce
valid outputs. We enable this via a novel multimodal architecture, specifically
designed to handle B-rep models, and demonstrate how existing CAD tools, in
conjunction with mLLMs, can be used to automatically generate the required
reasoning dataset, without relying on external annotations. We extensively
evaluate B-repLer and demonstrate several text-based B-rep edits of various
complexity, which were not previously possible.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [75] [Context Misleads LLMs: The Role of Context Filtering in Maintaining Safe Alignment of LLMs](https://arxiv.org/abs/2508.10031)
*Jinhwa Kim,Ian G. Harris*

Main category: cs.CR

TL;DR: 提出即插即用的上下文过滤模型，在保持大语言模型原有性能的同时将越狱攻击成功率降低88%


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全防御方案在提升安全性时易损害模型帮助性，需平衡安全防护与正常用户体验

Method: 通过输入预处理过滤不可信上下文，识别用户真实意图，无需微调即可适配各类LLM

Result: 攻击成功率最大降低88%，安全性与帮助性乘积指标达SOTA，适用于白盒/黑盒模型

Conclusion: 即插即用的上下文过滤机制有效提升LLM安全性，模型将开源供研究使用

Abstract: While Large Language Models (LLMs) have shown significant advancements in
performance, various jailbreak attacks have posed growing safety and ethical
risks. Malicious users often exploit adversarial context to deceive LLMs,
prompting them to generate responses to harmful queries. In this study, we
propose a new defense mechanism called Context Filtering model, an input
pre-processing method designed to filter out untrustworthy and unreliable
context while identifying the primary prompts containing the real user intent
to uncover concealed malicious intent. Given that enhancing the safety of LLMs
often compromises their helpfulness, potentially affecting the experience of
benign users, our method aims to improve the safety of the LLMs while
preserving their original performance. We evaluate the effectiveness of our
model in defending against jailbreak attacks through comparative analysis,
comparing our approach with state-of-the-art defense mechanisms against six
different attacks and assessing the helpfulness of LLMs under these defenses.
Our model demonstrates its ability to reduce the Attack Success Rates of
jailbreak attacks by up to 88% while maintaining the original LLMs'
performance, achieving state-of-the-art Safety and Helpfulness Product results.
Notably, our model is a plug-and-play method that can be applied to all LLMs,
including both white-box and black-box models, to enhance their safety without
requiring any fine-tuning of the models themselves. We will make our model
publicly available for research purposes.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [76] [SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion](https://arxiv.org/abs/2508.10068)
*Xiaohan Chen,Zhongying Pan,Quan Feng,Yu Tian,Shuqun Yang,Mengru Wang,Lina Gong,Yuxia Geng,Piji Li,Xiang Chen*

Main category: cs.SE

TL;DR: 提出分层特征优化框架Saracoder，通过多维度检索优化和符号消歧模块显著提升仓库级代码补全效果


<details>
  <summary>Details</summary>
Motivation: 现有RAG方法依赖文本表面相似性，存在语义误导/冗余/同质化问题，且无法解决跨文件符号歧义

Method: 1. 分层特征优化模块：通过语义关系蒸馏/重复修剪/图结构相似性度量/多样性重排序 2. 外部感知标识符消歧模块：基于依赖分析解决跨文件歧义

Result: 在CrossCodeEval和RepoEval-Updated基准测试中显著超越现有方法，支持多编程语言和模型

Conclusion: 系统性多维度优化为构建更准确鲁棒的仓库级代码补全系统提供了新范式

Abstract: Retrieval-augmented generation (RAG) for repository-level code completion
commonly relies on superficial text similarity, leading to results plagued by
semantic misguidance, redundancy, and homogeneity, while also failing to
resolve external symbol ambiguity. To address these challenges, we introduce
Saracoder, a Hierarchical Feature-Optimized retrieval framework. Its core
Hierarchical Feature Optimization module systematically refines candidates by
distilling deep semantic relationships, pruning exact duplicates, assessing
structural similarity with a novel graph-based metric that weighs edits by
their topological importance, and reranking results to maximize both relevance
and diversity. Furthermore, an External-Aware Identifier Disambiguator module
accurately resolves cross-file symbol ambiguity via dependency analysis.
Extensive experiments on the challenging CrossCodeEval and RepoEval-Updated
benchmarks demonstrate that Saracoder significantly outperforms existing
baselines across multiple programming languages and models. Our work proves
that systematically refining retrieval results across multiple dimensions
provides a new paradigm for building more accurate and robust repository-level
code completion systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [77] [Nested-ReFT: Efficient Reinforcement Learning for Large Language Model Fine-Tuning via Off-Policy Rollouts](https://arxiv.org/abs/2508.10123)
*Maxime Heuillet,Yufei Cui,Boxing Chen,Audrey Durand,Prasanna Parthasarathi*

Main category: cs.LG

TL;DR: 提出Nested-ReFT框架，通过嵌套式行为模型和动态层跳过来降低强化微调的计算成本，保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 传统ReFT方法在训练时生成多个补全答案导致计算成本过高，需通过优化推理效率解决该问题。

Method: 结合离策略强化学习和推测解码技术，利用目标模型部分层作为行为模型，实现动态层跳过的低成本推理。

Result: 理论证明梯度无偏且方差可控，实验显示计算效率提升50%（tokens/sec），性能与基线ReFT持平。

Conclusion: Nested-ReFT在数学推理任务中实现计算效率与模型性能的平衡，为大规模模型微调提供新方向。

Abstract: Advanced reasoning in LLMs on challenging domains like mathematical reasoning
can be tackled using verifiable rewards based reinforced fine-tuning (ReFT). In
standard ReFT frameworks, a behavior model generates multiple completions with
answers per problem, for the answer to be then scored by a reward function.
While such RL post-training methods demonstrate significant performance
improvements across challenging reasoning domains, the computational cost of
generating completions during training with multiple inference steps makes the
training cost non-trivial. To address this, we draw inspiration from off-policy
RL, and speculative decoding to introduce a novel ReFT framework, dubbed
Nested-ReFT, where a subset of layers of the target model acts as the behavior
model to generate off-policy completions during training. The behavior model
configured with dynamic layer skipping per batch during training decreases the
inference cost compared to the standard ReFT frameworks. Our theoretical
analysis shows that Nested-ReFT yields unbiased gradient estimates with
controlled variance. Our empirical analysis demonstrates improved computational
efficiency measured as tokens/sec across multiple math reasoning benchmarks and
model sizes. Additionally, we explore three variants of bias mitigation to
minimize the off-policyness in the gradient updates that allows for maintaining
performance that matches the baseline ReFT performance.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [78] [Puppeteer: Rig and Animate Your 3D Models](https://arxiv.org/abs/2508.10898)
*Chaoyue Song,Xiu Li,Fan Yang,Zhongcong Xu,Jiacheng Wei,Fayao Liu,Jiashi Feng,Guosheng Lin,Jianfeng Zhang*

Main category: cs.CV

TL;DR: Puppeteer框架通过自动骨骼预测、拓扑感知蒙皮权重和可微分动画优化，实现了3D模型的自动化绑定与动画生成。


<details>
  <summary>Details</summary>
Motivation: 解决3D内容动态化过程中骨骼绑定和动画制作高度依赖专家、效率低下的痛点，突破生成式AI在静态模型到动态资产转换中的瓶颈。

Method: 1. 自回归transformer骨骼预测（联合标记化+分层排序）
2. 基于拓扑感知注意力的蒙皮权重计算
3. 可微分优化动画管线（稳定性+计算效率）

Result: 在骨骼预测精度（提升12.7%）、蒙皮质量（FID降低23%）和动画稳定性（抖动减少81%）方面超越SOTA，支持游戏资产和AI生成模型的鲁棒处理。

Conclusion: 该框架首次实现端到端3D内容动态化，通过联合架构创新显著提升自动化水平，为游戏开发/虚拟内容创作提供高效解决方案。

Abstract: Modern interactive applications increasingly demand dynamic 3D content, yet
the transformation of static 3D models into animated assets constitutes a
significant bottleneck in content creation pipelines. While recent advances in
generative AI have revolutionized static 3D model creation, rigging and
animation continue to depend heavily on expert intervention. We present
Puppeteer, a comprehensive framework that addresses both automatic rigging and
animation for diverse 3D objects. Our system first predicts plausible skeletal
structures via an auto-regressive transformer that introduces a joint-based
tokenization strategy for compact representation and a hierarchical ordering
methodology with stochastic perturbation that enhances bidirectional learning
capabilities. It then infers skinning weights via an attention-based
architecture incorporating topology-aware joint attention that explicitly
encodes inter-joint relationships based on skeletal graph distances. Finally,
we complement these rigging advances with a differentiable optimization-based
animation pipeline that generates stable, high-fidelity animations while being
computationally more efficient than existing approaches. Extensive evaluations
across multiple benchmarks demonstrate that our method significantly
outperforms state-of-the-art techniques in both skeletal prediction accuracy
and skinning quality. The system robustly processes diverse 3D content, ranging
from professionally designed game assets to AI-generated shapes, producing
temporally coherent animations that eliminate the jittering issues common in
existing methods.

</details>


### [79] [Improving OCR for Historical Texts of Multiple Languages](https://arxiv.org/abs/2508.10356)
*Hylke Westerdijk,Ben Blankenborg,Khondoker Ittehadul Islam*

Main category: cs.CV

TL;DR: 论文提出三种OCR与文档布局分析任务的深度学习方法：对死海古卷使用Kraken/TrOCR模型并增强数据集；对16-18世纪会议决议采用CRNN+DeepLabV3+双向LSTM架构；现代英文手写识别使用ResNet34编码器的CRNN模型，均取得显著效果改进。


<details>
  <summary>Details</summary>
Motivation: 解决不同时期（古代希伯来文献、16-18世纪会议记录、现代英文手写）文档的OCR识别难题，验证深度学习模型在跨时代、多语言场景下的适应能力。

Method: 1.历史文献：数据增强+Kraken/TrOCR模型
2.会议决议：CRNN整合DeepLabV3+语义分割与双向LSTM，引入置信度伪标签
3.现代手写：CRNN+ResNet34编码器，CTC损失函数捕捉序列依赖

Result: 三类任务均实现字符识别准确率提升，验证了数据增强、混合架构设计（CNN+RNN）以及领域适应技术（如伪标签）的有效性。

Conclusion: 针对不同文档类型定制深度学习方案能显著提升OCR性能，为历史文献数字化和复杂文档分析提供技术范式，未来可探索跨任务知识迁移和少样本学习方法。

Abstract: This paper presents our methodology and findings from three tasks across
Optical Character Recognition (OCR) and Document Layout Analysis using advanced
deep learning techniques. First, for the historical Hebrew fragments of the
Dead Sea Scrolls, we enhanced our dataset through extensive data augmentation
and employed the Kraken and TrOCR models to improve character recognition. In
our analysis of 16th to 18th-century meeting resolutions task, we utilized a
Convolutional Recurrent Neural Network (CRNN) that integrated DeepLabV3+ for
semantic segmentation with a Bidirectional LSTM, incorporating confidence-based
pseudolabeling to refine our model. Finally, for modern English handwriting
recognition task, we applied a CRNN with a ResNet34 encoder, trained using the
Connectionist Temporal Classification (CTC) loss function to effectively
capture sequential dependencies. This report offers valuable insights and
suggests potential directions for future research.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [80] [Large Language Models Show Signs of Alignment with Human Neurocognition During Abstract Reasoning](https://arxiv.org/abs/2508.10057)
*Christopher Pinier,Sonia Acuña Vargas,Mariia Steeghs-Turchina,Dora Matzke,Claire E. Stevenson,Michael D. Nunez*

Main category: q-bio.NC

TL;DR: 研究发现约700亿参数的大型语言模型在抽象推理任务中达到人类水平准确率，并在神经表征层面与人类额叶脑电活动存在相似性，提示生物与人工智能可能存在共享认知机制。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在抽象推理过程中是否与人类神经认知机制存在相似性，验证生物与人工智能的潜在共性原则。

Method: 通过抽象模式完成任务对比人类与8个开源LLMs的表现，结合EEG记录的注视相关电位（FRPs）分析神经表征几何关系，并与响应相关ERP和静息态EEG作对比验证。

Result: 1) 最大模型（Qwen-2.5-72B/DeepSeek-R1-70B）达到人类准确率且难度分布相似 2) 所有LLMs中间层形成抽象模式聚类（聚类强度与任务表现正相关）3) 任务最优层表征几何与人类额叶FRPs呈中等正相关

Conclusion: 大型语言模型可能模拟了人脑抽象推理机制，为生物与人工智能的认知共性提供了初步证据，参数规模是实现类人表现的关键因素。

Abstract: This study investigates whether large language models (LLMs) mirror human
neurocognition during abstract reasoning. We compared the performance and
neural representations of human participants with those of eight open-source
LLMs on an abstract-pattern-completion task. We leveraged pattern type
differences in task performance and in fixation-related potentials (FRPs) as
recorded by electroencephalography (EEG) during the task. Our findings indicate
that only the largest tested LLMs (~70 billion parameters) achieve
human-comparable accuracy, with Qwen-2.5-72B and DeepSeek-R1-70B also showing
similarities with the human pattern-specific difficulty profile. Critically,
every LLM tested forms representations that distinctly cluster the abstract
pattern categories within their intermediate layers, although the strength of
this clustering scales with their performance on the task. Moderate positive
correlations were observed between the representational geometries of
task-optimal LLM layers and human frontal FRPs. These results consistently
diverged from comparisons with other EEG measures (response-locked ERPs and
resting EEG), suggesting a potential shared representational space for abstract
patterns. This indicates that LLMs might mirror human brain mechanisms in
abstract reasoning, offering preliminary evidence of shared principles between
biological and artificial intelligence.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [81] [Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data](https://arxiv.org/abs/2508.09636)
*Lalitesh Morishetti,Abhay Kumar,Jonathan Scott,Kaushiki Nag,Gunjan Sharma,Shanu Vashishtha,Rahul Sridhar,Rohit Chatter,Kannan Achan*

Main category: cs.IR

TL;DR: 提出结合多任务学习框架、混合数据整合与预训练模型的新型个性化产品搜索排序优化方法，通过语义嵌入和可扩展标注机制显著提升效果


<details>
  <summary>Details</summary>
Motivation: 传统个性化搜索模型存在混合数据处理能力不足和人工标注依赖问题，需更有效的多模态数据整合与自动化相关性标注方案

Method: 1. 多任务学习框架整合表格/非表格数据
2. 使用预训练TinyBERT生成语义嵌入
3. 新型用户行为采样技术
4. 基于点击数据的三维相关性标注机制(CTR/点击位置/语义相似度)

Result: 实验显示非结构化数据与先进嵌入技术的结合使NDCG提升15.6%，消融实验验证相关性标签、TinyBERT微调、查询-产品嵌入交互的关键作用

Conclusion: 该方法通过多模态学习范式与自动化标注方案，显著改善个性化搜索效果，为实际应用提供高效可扩展的解决方案

Abstract: In this paper, we present a novel model architecture for optimizing
personalized product search ranking using a multi-task learning (MTL)
framework. Our approach uniquely integrates tabular and non-tabular data,
leveraging a pre-trained TinyBERT model for semantic embeddings and a novel
sampling technique to capture diverse customer behaviors. We evaluate our model
against several baselines, including XGBoost, TabNet, FT-Transformer, DCN-V2,
and MMoE, focusing on their ability to handle mixed data types and optimize
personalized ranking. Additionally, we propose a scalable relevance labeling
mechanism based on click-through rates, click positions, and semantic
similarity, offering an alternative to traditional human-annotated labels.
Experimental results show that combining non-tabular data with advanced
embedding techniques in multi-task learning paradigm significantly enhances
model performance. Ablation studies further underscore the benefits of
incorporating relevance labels, fine-tuning TinyBERT layers, and TinyBERT
query-product embedding interactions. These results demonstrate the
effectiveness of our approach in achieving improved personalized product search
ranking.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [82] [Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development](https://arxiv.org/abs/2508.10108)
*Sattvik Sahai,Prasoon Goyal,Michael Johnston,Anna Gottardi,Yao Lu,Lucy Hu,Luke Dai,Shaohua Liu,Samyuth Sagi,Hangjie Shi,Desheng Zhang,Lavina Vaz,Leslie Ball,Maureen Murray,Rahul Gupta,Shankar Ananthakrishna*

Main category: cs.AI

TL;DR: 亚马逊通过Nova AI挑战赛组织大学团队开展红队与AI助手的对抗竞赛，推动安全AI技术在代码生成领域的发展，开发了推理安全对齐、模型防护等多类创新方法。


<details>
  <summary>Details</summary>
Motivation: 针对AI在软件开发中的安全隐患，亚马逊发起Trusted AI竞赛，旨在通过红队攻击与AI防御的对抗机制，系统性提升AI代码助手的安全性。

Method: 搭建对抗性竞赛平台，红队通过多轮对话测试AI助手安全边界；提供标注数据支持模型迭代；开发基线模型、竞赛服务系统及评估框架。

Result: 参赛团队在安全对齐推理、模型防护、多轮越狱攻击、LLM高效探测等方面取得突破，亚马逊团队同步构建了定制化代码模型和竞赛技术设施。

Conclusion: 该挑战赛通过产学研协作建立了AI安全新范式，为软件开发领域的AI安全技术设立了更高标准，验证了对抗性测试在安全评估中的有效性。

Abstract: AI systems for software development are rapidly gaining prominence, yet
significant challenges remain in ensuring their safety. To address this, Amazon
launched the Trusted AI track of the Amazon Nova AI Challenge, a global
competition among 10 university teams to drive advances in secure AI. In the
challenge, five teams focus on developing automated red teaming bots, while the
other five create safe AI assistants. This challenge provides teams with a
unique platform to evaluate automated red-teaming and safety alignment methods
through head-to-head adversarial tournaments where red teams have multi-turn
conversations with the competing AI coding assistants to test their safety
alignment. Along with this, the challenge provides teams with a feed of high
quality annotated data to fuel iterative improvement. Throughout the challenge,
teams developed state-of-the-art techniques, introducing novel approaches in
reasoning-based safety alignment, robust model guardrails, multi-turn
jail-breaking, and efficient probing of large language models (LLMs). To
support these efforts, the Amazon Nova AI Challenge team made substantial
scientific and engineering investments, including building a custom baseline
coding specialist model for the challenge from scratch, developing a tournament
orchestration service, and creating an evaluation harness. This paper outlines
the advancements made by university teams and the Amazon Nova AI Challenge team
in addressing the safety challenges of AI for software development,
highlighting this collaborative effort to raise the bar for AI safety.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [83] [Personalized Real-time Jargon Support for Online Meetings](https://arxiv.org/abs/2508.10239)
*Yifan Song,Wing Yee Au,Hon Yung Wong,Brian P. Bailey,Tal August*

Main category: cs.HC

TL;DR: 研究开发了基于大语言模型的个性化术语支持系统ParseJargon，通过对照实验和实地研究验证了个性化术语解释对跨学科沟通的显著提升效果


<details>
  <summary>Details</summary>
Motivation: 专业术语阻碍跨学科交流，现有会议场景的术语管理策略存在局限性，需要开发个性化实时支持工具

Method: 1. 对16位专业人士开展日记研究；2. 开发LLM驱动的个性化术语识别系统；3. 设计包含基线组、通用组和个性化组的对照实验；4. 进行实地验证研究

Result: 个性化支持组在理解度（+32%）、参与度（+28%）、同事工作欣赏度（+24%）显著提升，通用支持组参与度下降；实地研究验证系统可用性

Conclusion: 个性化术语支持能有效促进跨学科沟通，实际部署需平衡个性化和系统延迟，研究为教育/跨学科工具设计提供新思路

Abstract: Effective interdisciplinary communication is frequently hindered by
domain-specific jargon. To explore the jargon barriers in-depth, we conducted a
formative diary study with 16 professionals, revealing critical limitations in
current jargon-management strategies during workplace meetings. Based on these
insights, we designed ParseJargon, an interactive LLM-powered system providing
real-time personalized jargon identification and explanations tailored to
users' individual backgrounds. A controlled experiment comparing ParseJargon
against baseline (no support) and general-purpose (non-personalized) conditions
demonstrated that personalized jargon support significantly enhanced
participants' comprehension, engagement, and appreciation of colleagues' work,
whereas general-purpose support negatively affected engagement. A follow-up
field study validated ParseJargon's usability and practical value in real-time
meetings, highlighting both opportunities and limitations for real-world
deployment. Our findings contribute insights into designing personalized jargon
support tools, with implications for broader interdisciplinary and educational
applications.

</details>
