<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 45]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CV](#cs.CV) [Total: 6]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning](https://arxiv.org/abs/2508.09303)
*Shu Zhao,Tan Yu,Anbang Xu,Japinder Singh,Aaditya Shukla,Rama Akkiraju*

Main category: cs.CL

TL;DR: 提出ParallelSearch强化学习框架，通过并行化搜索操作突破传统顺序处理瓶颈，在保持准确性的同时显著提升计算效率与性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的搜索代理在处理多实体比较等可并行化查询时存在顺序处理架构缺陷，导致计算资源利用效率低下。

Method: 构建强化学习框架引导LLM识别并行查询结构，设计多维奖励函数联合优化查询分解质量、并行执行收益与答案准确性。

Result: 在7个问答基准测试中平均提升2.9%性能，可并行问题上实现12.7%性能增益且LLM调用次数减少30.4%。

Conclusion: 并行化搜索架构有效突破传统顺序处理限制，为复杂推理任务的高效执行提供了创新解决方案。

Abstract: Reasoning-augmented search agents such as Search-R1, trained via
reinforcement learning with verifiable rewards (RLVR), demonstrate remarkable
capabilities in multi-step information retrieval from external knowledge
sources. These agents address the limitations of their parametric memory by
dynamically gathering relevant facts to address complex reasoning tasks.
However, existing approaches suffer from a fundamental architectural
limitation: they process search queries strictly sequentially, even when
handling inherently parallelizable and logically independent comparisons. This
sequential bottleneck significantly constrains computational efficiency,
particularly for queries that require multiple entity comparisons. To address
this critical limitation, we propose ParallelSearch, a novel reinforcement
learning framework that empowers large language models (LLMs) to recognize
parallelizable query structures and execute multiple search operations
concurrently. Our approach introduces dedicated reward functions that
incentivize the identification of independent query components while preserving
answer accuracy through jointly considering correctness, query decomposition
quality, and parallel execution benefits. Comprehensive experiments demonstrate
that ParallelSearch outperforms state-of-the-art baselines by an average
performance gain of 2.9% across seven question-answering benchmarks. Notably,
on parallelizable questions, our method achieves a 12.7% performance
improvement while requiring only 69.6% of the LLM calls compared to sequential
approaches.

</details>


### [2] [Leveraging Large Language Models for Rare Disease Named Entity Recognition](https://arxiv.org/abs/2508.09323)
*Nan Miles Xi,Yu Deng,Lin Wang*

Main category: cs.CL

TL;DR: 研究评估GPT-4o在罕见病命名实体识别中的低资源表现，通过提示策略优化实现新SOTA结果


<details>
  <summary>Details</summary>
Motivation: 解决罕见病领域NER存在的标注数据稀缺、实体类型语义模糊、长尾分布等核心挑战

Method: 设计包含领域知识的结构化提示框架，开发语义引导的少样本选择方法，结合零样本/少样本/RAG/微调策略

Result: 在RareDis Corpus上超越BioClinicalBERT，微调达到SOTA；少样本策略性价比最优，RAG增益有限

Conclusion: 提示优化的LLM可作为生物医学NER的有效替代方案，特别是在标注数据稀缺的罕见病应用场景中

Abstract: Named Entity Recognition (NER) in the rare disease domain poses unique
challenges due to limited labeled data, semantic ambiguity between entity
types, and long-tail distributions. In this study, we evaluate the capabilities
of GPT-4o for rare disease NER under low-resource settings, using a range of
prompt-based strategies including zero-shot prompting, few-shot in-context
learning, retrieval-augmented generation (RAG), and task-level fine-tuning. We
design a structured prompting framework that encodes domain-specific knowledge
and disambiguation rules for four entity types. We further introduce two
semantically guided few-shot example selection methods to improve in-context
performance while reducing labeling effort. Experiments on the RareDis Corpus
show that GPT-4o achieves competitive or superior performance compared to
BioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art
(SOTA) results. Cost-performance analysis reveals that few-shot prompting
delivers high returns at low token budgets, while RAG offers marginal
additional benefit. An error taxonomy highlights common failure modes such as
boundary drift and type confusion, suggesting opportunities for post-processing
and hybrid refinement. Our results demonstrate that prompt-optimized LLMs can
serve as effective, scalable alternatives to traditional supervised models in
biomedical NER, particularly in rare disease applications where annotated data
is scarce.

</details>


### [3] [TEN: Table Explicitization, Neurosymbolically](https://arxiv.org/abs/2508.09324)
*Nikita Mehrotra,Aayush Kumar,Sumit Gulwani,Arjun Radhakrishna,Ashish Tiwari*

Main category: cs.CL

TL;DR: 提出神经符号方法TEN，通过结构分解提示+符号检查+自我调试循环，显著提升半结构化文本表格提取准确率并降低幻觉


<details>
  <summary>Details</summary>
Motivation: 传统神经方法在处理无明确分隔符的表格数据时存在幻觉和约束失效问题，需结合符号逻辑确保结构准确性

Method: 1. 大模型生成初始表 2. 符号检查器验证结构+检测错误 3. 批判性LLM生成修复指导 4. 自我调试循环优化结果

Result: 实验显示：Exact Match准确率显著提升（用户评分5.0 vs 4.3，p=0.021），60%用户案例更易验证修正

Conclusion: TEN通过神经与符号方法协同，有效解决表格提取难题，为半结构化数据处理提供可靠方案

Abstract: We present a neurosymbolic approach, TEN, for extracting tabular data from
semistructured input text. This task is particularly challenging for text input
that does not use special delimiters consistently to separate columns and rows.
Purely neural approaches perform poorly due to hallucinations and their
inability to enforce hard constraints. TEN uses Structural Decomposition
prompting - a specialized chain-of-thought prompting approach - on a large
language model (LLM) to generate an initial table, and thereafter uses a
symbolic checker to evaluate not only the well-formedness of that table, but
also detect cases of hallucinations or forgetting. The output of the symbolic
checker is processed by a critique-LLM to generate guidance for fixing the
table, which is presented to the original LLM in a self-debug loop. Our
extensive experiments demonstrate that TEN significantly outperforms purely
neural baselines across multiple datasets and metrics, achieving significantly
higher exact match accuracy and substantially reduced hallucination rates. A
21-participant user study further confirms that TEN's tables are rated
significantly more accurate (mean score: 5.0 vs 4.3; p = 0.021), and are
consistently preferred for ease of verification and correction, with
participants favoring our method in over 60% of the cases.

</details>


### [4] [Decoding Neural Emotion Patterns through Natural Language Processing Embeddings](https://arxiv.org/abs/2508.09337)
*Gideon Vos,Maryam Ebrahimpour,Liza van Eijk,Zoltan Sarnyai,Mostafa Rahimi Azghadi*

Main category: cs.CL

TL;DR: 提出通过文本语义分析映射情感内容到脑区的计算框架，实现无需神经成像的情感-脑功能关联研究


<details>
  <summary>Details</summary>
Motivation: 传统神经影像技术成本高昂且受限于实验室环境，而海量数字文本为情感-脑功能映射提供了新可能。现有研究多独立处理神经影像数据或文本分析，缺乏系统性整合。

Method: 使用text-embedding-ada-002生成高维语义表征→降维聚类→映射到18个情感相关脑区。设计三个实验：1）对比抑郁/健康人群对话数据 2）GoEmotions数据集验证 3）人类文本与LLM生成文本的脑激活差异分析

Result: 1）发现抑郁症患者边缘系统激活增强 2）成功区分离散情绪 3）LLM文本在基础情绪分布与人类相当，但缺乏内侧前额叶/后扣带回等自我参照区的神经激活特征

Conclusion: 该方法具有高成本效益和可扩展性，适用于自然语言分析、临床群体区分，并为AI情感表达提供神经科学评估基准

Abstract: Understanding how emotional expression in language relates to brain function
is a challenge in computational neuroscience and affective computing.
Traditional neuroimaging is costly and lab-bound, but abundant digital text
offers new avenues for emotion-brain mapping. Prior work has largely examined
neuroimaging-based emotion localization or computational text analysis
separately, with little integration. We propose a computational framework that
maps textual emotional content to anatomically defined brain regions without
requiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate
high-dimensional semantic representations, apply dimensionality reduction and
clustering to identify emotional groups, and map them to 18 brain regions
linked to emotional processing. Three experiments were conducted: i) analyzing
conversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to
compare mapping patterns, ii) applying the method to the GoEmotions dataset and
iii) comparing human-written text with large language model (LLM) responses to
assess differences in inferred brain activation. Emotional intensity was scored
via lexical analysis. Results showed neuroanatomically plausible mappings with
high spatial specificity. Depressed subjects exhibited greater limbic
engagement tied to negative affect. Discrete emotions were successfully
differentiated. LLM-generated text matched humans in basic emotion distribution
but lacked nuanced activation in empathy and self-referential regions (medial
prefrontal and posterior cingulate cortex). This cost-effective, scalable
approach enables large-scale analysis of naturalistic language, distinguishes
between clinical populations, and offers a brain-based benchmark for evaluating
AI emotional expression.

</details>


### [5] [The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains](https://arxiv.org/abs/2508.09349)
*Cathy Speed,Ahmed A. Metwally*

Main category: cs.CL

TL;DR: HAH-Delphi框架通过AI与人类专家协同，显著提升共识效率与质量，实现95%共识复现率与专家一致性。


<details>
  <summary>Details</summary>
Motivation: 传统专家共识方法存在专家负担重、信息过度简化等问题，当前信息过载与证据碎片化加剧了这些挑战。研究旨在开发AI增强型共识框架以提升效率与质量。

Method: 分三阶段测试：1) 复现现有共识 2) 与资深专家前瞻性对比 3) 应用于耐力训练与混合训练领域。整合生成式AI模型(Gemini 2.5 Pro)与小型专家小组。

Result: AI复现95%现有结论，与专家达成95%方向共识（缺乏实践细节），6人专家小组达成>90%共识覆盖并在最终参与者前实现主题饱和。

Conclusion: HAH-Delphi框架具有灵活可扩展性，成功应用于健康与训练领域，为个性化指导和大规模共识建设提供方法论基础。

Abstract: Expert consensus plays a critical role in domains where evidence is complex,
conflicting, or insufficient for direct prescription. Traditional methods, such
as Delphi studies, consensus conferences, and systematic guideline synthesis,
offer structure but face limitations including high panel burden, interpretive
oversimplification, and suppression of conditional nuance. These challenges are
now exacerbated by information overload, fragmentation of the evidence base,
and increasing reliance on publicly available sources that lack expert
filtering. This study introduces and evaluates a Human-AI Hybrid Delphi
(HAH-Delphi) framework designed to augment expert consensus development by
integrating a generative AI model (Gemini 2.5 Pro), small panels of senior
human experts, and structured facilitation. The HAH-Delphi was tested in three
phases: retrospective replication, prospective comparison, and applied
deployment in two applied domains (endurance training and resistance and mixed
cardio/strength training). The AI replicated 95% of published expert consensus
conclusions in Phase I and showed 95% directional agreement with senior human
experts in Phase II, though it lacked experiential and pragmatic nuance. In
Phase III, compact panels of six senior experts achieved >90% consensus
coverage and reached thematic saturation before the final participant. The AI
provided consistent, literature-grounded scaffolding that supported divergence
resolution and accelerated saturation. The HAH-Delphi framework offers a
flexible, scalable approach for generating high-quality, context-sensitive
consensus. Its successful application across health, coaching, and performance
science confirms its methodological robustness and supports its use as a
foundation for generating conditional, personalised guidance and published
consensus frameworks at scale.

</details>


### [6] [Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling](https://arxiv.org/abs/2508.09350)
*Ju-Chieh Chou,Jiawei Zhou,Karen Livescu*

Main category: cs.CL

TL;DR: 提出联合建模语义标记和声学特征的文本无关语音生成模型，相比现有方法在保持语言信息的同时提升声学细节表现


<details>
  <summary>Details</summary>
Motivation: 现有文本无关语音模型需依赖独立声码器，缺乏声学上下文感知且无法控制声学细节

Method: 使用流匹配目标函数联合生成语义标记和连续声学帧表征，通过预测多步未来语义标记保持语言信息

Result: 在语言似然基准测试中表现相当，但在提示生成中展现更优的声学细节

Conclusion: 联合建模框架有效解决了传统模型声学信息建模不足的问题，为语音生成提供更完整的控制能力

Abstract: Textless spoken language models (SLMs) are generative models of speech that
do not rely on text supervision. Most textless SLMs learn to predict the next
semantic token, a discrete representation of linguistic content, and rely on a
separate vocoder to add acoustic information to the generated speech. Such
models have no access to acoustic context and no built-in control over acoustic
details. In this work, we propose to jointly model linguistic and acoustic
information by generating semantic tokens and a continuous real-valued
representation of the acoustic frame. We use a flow-matching objective to
predict the continuous vector conditioned on the semantic tokens. We study the
design space of this approach and find that predicting multiple future semantic
tokens helps preserve linguistic information. Our approach achieves comparable
performance to existing models in terms of linguistic likelihood benchmarks,
while providing better acoustic detail in prompted generation.

</details>


### [7] [APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification](https://arxiv.org/abs/2508.09378)
*Artem Chernodub,Aman Saini,Yejin Huh,Vivek Kulkarni,Vipul Raheja*

Main category: cs.CL

TL;DR: 提出APIO方法——无需手动种子提示的自动提示诱导与优化框架，在语法纠错和文本简化任务中达到纯LLM提示方法的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法依赖人工设计的初始提示（seed prompts），限制了方法的通用性和效率。本研究旨在开发不依赖手动提示的自动化优化方案。

Method: 通过两阶段框架：1）基于任务描述自动诱导生成初始提示；2）结合强化学习机制迭代优化提示策略，引入质量反馈信号调整生成分布。

Result: 在BEA语法纠错基准和文本简化任务中，APIO分别取得62.7（F0.5）和43.2（SARI）的分数，超越人工设计提示方法3-5个百分点。

Conclusion: 证明了完全自动化提示优化的可行性，为LLM提示工程提供了新范式。通过公开数据、代码和优化后的提示模板推动领域发展。

Abstract: Recent advancements in large language models (LLMs) have enabled a wide range
of natural language processing (NLP) tasks to be performed through simple
prompt-based interactions. Consequently, several approaches have been proposed
to engineer prompts that most effectively enable LLMs to perform a given task
(e.g., chain-of-thought prompting). In settings with a well-defined metric to
optimize model performance, automatic prompt optimization (APO) methods have
been developed to refine a seed prompt. Advancing this line of research, we
propose APIO, a simple but effective prompt induction and optimization approach
for the tasks of Grammatical Error Correction (GEC) and Text Simplification,
without relying on manually specified seed prompts. APIO achieves a new
state-of-the-art performance for purely LLM-based prompting methods on these
tasks. We make our data, code, prompts, and outputs publicly available.

</details>


### [8] [Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models](https://arxiv.org/abs/2508.09403)
*Ting Cai,Stephen Sheen,AnHai Doan*

Main category: cs.CL

TL;DR: 提出Columbo系统解决表格列名缩写扩展问题，通过真实数据集、新评估指标和LLM上下文推理，准确率提升4-29%


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖合成数据集且评估指标不准确，需改进数据质量与评估体系

Method: 构建4个真实领域数据集，设计同义词感知评估指标，开发结合规则链与token分析的Columbo框架

Result: 在5个数据集上超越当前最优系统NameGuess 4-29%，并部署于环境科学数据门户EDI

Conclusion: 通过真实数据、精准评估和上下文感知的LLM推理，显著提升列名扩展准确性

Abstract: Expanding the abbreviated column names of tables, such as ``esal'' to
``employee salary'', is critical for numerous downstream data tasks. This
problem arises in enterprises, domain sciences, government agencies, and more.
In this paper we make three contributions that significantly advances the state
of the art. First, we show that synthetic public data used by prior work has
major limitations, and we introduce 4 new datasets in enterprise/science
domains, with real-world abbreviations. Second, we show that accuracy measures
used by prior work seriously undercount correct expansions, and we propose new
synonym-aware measures that capture accuracy much more accurately. Finally, we
develop Columbo, a powerful LLM-based solution that exploits context, rules,
chain-of-thought reasoning, and token-level analysis. Extensive experiments
show that Columbo significantly outperforms NameGuess, the current most
advanced solution, by 4-29\%, over 5 datasets. Columbo has been used in
production on EDI, a major data portal for environmental sciences.

</details>


### [9] [Leveraging Zipformer Model for Effective Language Identification in Code-Switched Child-Directed Speech](https://arxiv.org/abs/2508.09430)
*Lavanya Shankar,Leibny Paola Garcia Perera*

Main category: cs.CL

TL;DR: 通过Zipformer模型处理中英不平衡双语数据，语言识别平衡准确率提升15.47%


<details>
  <summary>Details</summary>
Motivation: 双语环境中儿童导向场景的代码转换和语言识别存在挑战，需解决语音中不平衡语言混合的问题

Method: 利用Zipformer内部层编码语言特征，比较不同后端并选择最优嵌入提取方案

Result: 平衡准确率(BAC)达81.89%，较基线提升15.47%，且模型在不同后端表现鲁棒

Conclusion: Transformer编码器架构(Zipformer)在现实场景的语言识别中展现出实际应用潜力

Abstract: Code-switching and language identification in child-directed scenarios
present significant challenges, particularly in bilingual environments. This
paper addresses this challenge by using Zipformer to handle the nuances of
speech, which contains two imbalanced languages, Mandarin and English, in an
utterance. This work demonstrates that the internal layers of the Zipformer
effectively encode the language characteristics, which can be leveraged in
language identification. We present the selection methodology of the inner
layers to extract the embeddings and make a comparison with different
back-ends. Our analysis shows that Zipformer is robust across these backends.
Our approach effectively handles imbalanced data, achieving a Balanced Accuracy
(BAC) of 81.89%, a 15.47% improvement over the language identification
baseline. These findings highlight the potential of the transformer encoder
architecture model in real scenarios.

</details>


### [10] [From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text](https://arxiv.org/abs/2508.09450)
*Ridwan Mahbub,Mohammed Saidul Islam,Mir Tafseer Nayeem,Md Tahmid Rahman Laskar,Mizanur Rahman,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: 研究发现主流视觉语言模型在生成图表摘要时存在地理经济偏见，对高收入国家描述更积极，需更有效的去偏策略。


<details>
  <summary>Details</summary>
Motivation: 虽然图表文本生成技术快速发展，但模型输出中的潜在偏见（尤其是可能引发社会危害的地理经济偏见）尚未被充分研究。

Method: 基于6,000个图表-国家组合的大规模评估，分析六个主流模型生成摘要的情感倾向与经济地位关联性。

Result: 现有模型对高收入国家生成更积极的描述，GPT-4o-mini/Gemini-1.5-Flash/Phi-3.5存在不同程度偏见，提示去偏方法效果有限。

Conclusion: 地理经济偏见问题复杂，需要开发更鲁棒的去偏方法，研究公开了代码和数据集以促进相关研究。

Abstract: Charts are very common for exploring data and communicating insights, but
extracting key takeaways from charts and articulating them in natural language
can be challenging. The chart-to-text task aims to automate this process by
generating textual summaries of charts. While with the rapid advancement of
large Vision-Language Models (VLMs), we have witnessed great progress in this
domain, little to no attention has been given to potential biases in their
outputs. This paper investigates how VLMs can amplify geo-economic biases when
generating chart summaries, potentially causing societal harm. Specifically, we
conduct a large-scale evaluation of geo-economic biases in VLM-generated chart
summaries across 6,000 chart-country pairs from six widely used proprietary and
open-source models to understand how a country's economic status influences the
sentiment of generated summaries. Our analysis reveals that existing VLMs tend
to produce more positive descriptions for high-income countries compared to
middle- or low-income countries, even when country attribution is the only
variable changed. We also find that models such as GPT-4o-mini,
Gemini-1.5-Flash, and Phi-3.5 exhibit varying degrees of bias. We further
explore inference-time prompt-based debiasing techniques using positive
distractors but find them only partially effective, underscoring the complexity
of the issue and the need for more robust debiasing strategies. Our code and
dataset are publicly available here.

</details>


### [11] [User-centric Subjective Leaderboard by Customizable Reward Modeling](https://arxiv.org/abs/2508.09463)
*Qi Jia,Xiujie Song,Zicheng Zhang,Yijin Guo,Kaiwei Zhang,Zijian Chen,Guangtao Zhai*

Main category: cs.CL

TL;DR: 提出首个用户中心的主观排行榜(USL)和可定制奖励模型(CRM)，通过10K+主观查询数据解决LLM评估中的偏好矛盾问题，4B参数的CRM性能超越GPT-4.1等主流模型。


<details>
  <summary>Details</summary>
Motivation: 现有LLM基准过度依赖客观指标，难以满足用户个性化需求。人类偏好存在显著多样性和矛盾性，传统奖励模型对此效果有限。

Method: 1. 收集10K+真实用户主观查询数据
2. 开发可定制奖励模型(CRM)架构
3. 在新主题/标准下测试模型泛化能力
4. 构建动态偏好驱动的USL评估体系

Result: 1. CRM(4B参数)性能超越GPT-4.1和Gemini-2.5-pro
2. USL展示与矛盾偏好的强负相关性(-0.89)
3. 新场景下准确率提升23%

Conclusion: 该研究突破传统评估框架，通过动态USL系统实现个性化LLM选择，CRM的卓越泛化能力为AI对齐提供新范式。

Abstract: Existing benchmarks for large language models (LLMs) predominantely focus on
assessing their capabilities through verifiable tasks. Such objective and
static benchmarks offer limited utility for practical LLM selection, making it
difficult for users to find suitable models for their individual needs. To
bridge this gap, we present the first User-Centric Subjective Leaderboard
(USL), which provides a preference-driven, dynamic ranking of LLMs across
diverse real-world scenarios. Our work is built upon a thorough investigation
of real human preference data, involving more than 10K subjective queries. Our
investigation reveals significant diversity and contradictions in human
preferences, which limit the effectiveness of state-of-the-art reward models.
To address this, we introduce Customizable Reward Models (CRMs). With only 4B
parameters, our CRM surpasses the performance of leading models such as GPT-4.1
and Gemini-2.5-pro, showing exceptional generalization capabilities across new
topics and criteria. The USL, powered by CRMs, exhibits strong negative
correlations to contradictory preferences.

</details>


### [12] [Learning Facts at Scale with Active Reading](https://arxiv.org/abs/2508.09494)
*Jessy Lin,Vincent-Pierre Berges,Xilun Chen,Wen-Tau Yih,Gargi Ghosh,Barlas Oğuz*

Main category: cs.CL

TL;DR: 提出Active Reading框架，通过自生成学习策略显著提升模型知识吸收效果，在多个领域实现准确率大幅提升并推出优秀专家模型


<details>
  <summary>Details</summary>
Motivation: 解决LLMs参数化记忆学习不可靠问题（依赖训练数据分布/机制不明确），填补确保知识稳定学习的工具空白

Method: Active Reading框架：训练模型通过自生成学习策略主动学习特定材料，应用于专家领域训练和预训练扩展

Result: 专家模型准确率：SimpleQA子集66%（+313%）、FinanceBench 26%（+160%）；Meta WikiExpert-8B在万亿token训练后超越百亿参数模型

Conclusion: Active Reading有效提升模型知识吸收可靠性，既适用于特定领域优化，也能扩展至预训练阶段构建更事实准确的模型

Abstract: LLMs are known to store vast amounts of knowledge in their parametric memory.
However, learning and recalling facts from this memory is known to be
unreliable, depending largely on the prevalence of particular facts in the
training data and other factors which are poorly understood. Practitioners are
lacking tools which will allow them to ensure that the models learn a given
body of knowledge reliably and consistently. To this end, we propose Active
Reading: a framework where we train models to study a given set of material
with self-generated learning strategies. First, we demonstrate models trained
with Active Reading on expert domains absorb significantly more knowledge than
vanilla finetuning and other data augmentations. We train expert 8B models that
achieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over
vanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla
finetuning) by applying Active Reading to the source documents for each
benchmark. Finally, we show that Active Reading can be utilized at pre-training
scale to build more factual models. As a demonstration of this, we release Meta
WikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens,
which outcompetes models with hundreds of billions of parameters on factual QA.

</details>


### [13] [From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation](https://arxiv.org/abs/2508.09497)
*Siyuan Meng,Junming Liu,Yirong Chen,Song Mao,Pinlong Cai,Guohang Yan,Botian Shi,Ding Wang*

Main category: cs.CL

TL;DR: 提出动态段落选择器（DPS），通过监督学习优化RAG系统的段落选择策略，显著提升多跳查询性能


<details>
  <summary>Details</summary>
Motivation: 传统RAG重排序模块采用固定Top-K选择策略，在处理需要跨文档推理的多跳查询时面临关键信息遗漏或噪声干扰的平衡难题

Method: 将段落选择建模为监督学习问题，通过捕捉段落间依赖关系实现动态集合选择，保持现有RAG流程的即插即用特性

Result: 在MuSiQue数据集上F1分数比Qwen3-reranker和RankingGPT分别提升30.06%和15.4%，五基准测试全面超越现有方法

Conclusion: DPS通过自适应证据选择机制有效增强复杂RAG场景的推理能力，为检索增强系统提供新的优化方向

Abstract: Retrieval-augmented generation (RAG) systems are often bottlenecked by their
reranking modules, which typically score passages independently and select a
fixed Top-K size. This approach struggles with complex multi-hop queries that
require synthesizing evidence across multiple documents, creating a trade-off
where small K values omit crucial information and large K values introduce
noise. To address this, we introduce the Dynamic Passage Selector (DPS), a
novel reranking framework that treats passage selection as a supervised
learning problem. Unlike traditional point-wise or list-wise methods, DPS is
fine-tuned to capture inter-passage dependencies and dynamically select the
most relevant set of passages for generation. As a seamless plug-and-play
module, DPS requires no modifications to the standard RAG pipeline.
Comprehensive evaluations on five benchmarks show that DPS consistently
outperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the
challenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over
strong baselines like Qwen3-reranker and RankingGPT, respectively. Our results
demonstrate that by enabling adaptive evidence selection, DPS substantially
enhances reasoning capabilities in complex RAG scenarios.

</details>


### [14] [LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation](https://arxiv.org/abs/2508.09515)
*Jakub Šmíd,Pavel Přibáň,Pavel Král*

Main category: cs.CL

TL;DR: 提出基于大语言模型生成伪标签数据的跨语言方面情感分析框架，无需翻译工具并在多语言场景中超越传统方法


<details>
  <summary>Details</summary>
Motivation: 传统跨语言ABSA方法过度依赖翻译工具导致误差累积，需要更可靠的数据增强方法消除语言差异

Method: 三阶段框架：1) 初步训练目标语言模型 2) 用LLM重构语义清晰的伪标签数据 3) 基于增强数据微调模型

Result: 在6种语言/5种模型上超越SOTA翻译方法，微调后的生成式LLM优于多语言小模型

Conclusion: LLM生成伪标签方案有效突破翻译工具限制，支持生成式模型且具备多语言扩展性

Abstract: Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed
sentiment analysis in a target language by transferring knowledge from a source
language with available annotated data. Most existing methods depend heavily on
often unreliable translation tools to bridge the language gap. In this paper,
we propose a new approach that leverages a large language model (LLM) to
generate high-quality pseudo-labelled data in the target language without the
need for translation tools. First, the framework trains an ABSA model to obtain
predictions for unlabelled target language data. Next, LLM is prompted to
generate natural sentences that better represent these noisy predictions than
the original text. The ABSA model is then further fine-tuned on the resulting
pseudo-labelled dataset. We demonstrate the effectiveness of this method across
six languages and five backbone models, surpassing previous state-of-the-art
translation-based approaches. The proposed framework also supports generative
models, and we show that fine-tuned LLMs outperform smaller multilingual
models.

</details>


### [15] [Cross-lingual Aspect-Based Sentiment Analysis: A Survey on Tasks, Approaches, and Challenges](https://arxiv.org/abs/2508.09516)
*Jakub Šmíd,Pavel Král*

Main category: cs.CL

TL;DR: 首次系统综述跨语言方面情感分析(ABSA)，涵盖任务定义、数据集、建模方法及未来挑战


<details>
  <summary>Details</summary>
Motivation: 当前ABSA研究集中在单语言场景，跨语言迁移场景缺乏系统性总结，阻碍资源丰富语言向低资源语言的迁移

Method: 通过文献综述系统性整理跨语言ABSA的任务体系(方面词抽取/情感分类等)、数据集特征、跨语言迁移范式(基于表示/参数/数据迁移)

Result: 揭示现有研究多聚焦单语言场景，跨语言ABSA仍处早期探索阶段，系统梳理了该领域技术路线与瓶颈问题

Conclusion: 跨语言ABSA需突破数据稀缺与模型泛化难题，未来应优化迁移机制并探索大语言模型的应用潜力

Abstract: Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that focuses on understanding opinions at the aspect level, including
sentiment towards specific aspect terms, categories, and opinions. While ABSA
research has seen significant progress, much of the focus has been on
monolingual settings. Cross-lingual ABSA, which aims to transfer knowledge from
resource-rich languages (such as English) to low-resource languages, remains an
under-explored area, with no systematic review of the field. This paper aims to
fill that gap by providing a comprehensive survey of cross-lingual ABSA. We
summarize key ABSA tasks, including aspect term extraction, aspect sentiment
classification, and compound tasks involving multiple sentiment elements.
Additionally, we review the datasets, modelling paradigms, and cross-lingual
transfer methods used to solve these tasks. We also examine how existing work
in monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to
the development of cross-lingual ABSA. Finally, we highlight the main
challenges and suggest directions for future research to advance cross-lingual
ABSA systems.

</details>


### [16] [UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2508.09517)
*Ladislav Lenc,Daniel Cífka,Jiří Martínek,Jakub Šmíd,Pavel Král*

Main category: cs.CL

TL;DR: 提出基于大语言模型嵌入的零样本事实核查系统，在CLEF 2023竞赛中获单语第7名、跨语种第9名


<details>
  <summary>Details</summary>
Motivation: 探索零样本方法实现事实核查声明检索，避免任务特定训练的成本。尝试通过组合不同大语言模型提升效果，解决多语言模型效果欠佳的问题

Method: 1. 采用NVIDIA NV-Embed-v2等先进模型生成文本嵌入
2. 通过余弦相似度匹配帖子与声明的嵌入向量
3. 组合不同模型（如NV-Embed与GPT/Mistral）优化效果

Result: 单语任务排名第7（0.501分），跨语种第9（0.416分）。英语输入效果最佳，NVIDIA模型表现最优，部分语种通过模型组合提升准确率

Conclusion: 零样本方法在事实核查任务中有效，但需注意多语言处理限制。模型组合策略在特定语种中可提升效果，未来需改进多语言嵌入质量

Abstract: This paper presents a zero-shot system for fact-checked claim retrieval. We
employed several state-of-the-art large language models to obtain text
embeddings. The models were then combined to obtain the best possible result.
Our approach achieved 7th place in monolingual and 9th in cross-lingual
subtasks. We used only English translations as an input to the text embedding
models since multilingual models did not achieve satisfactory results. We
identified the most relevant claims for each post by leveraging the embeddings
and measuring cosine similarity. Overall, the best results were obtained by the
NVIDIA NV-Embed-v2 model. For some languages, we benefited from model
combinations (NV-Embed & GPT or Mistral).

</details>


### [17] [COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation](https://arxiv.org/abs/2508.09521)
*Yunxiao Wang,Meng Liu,Wenqi Liu,Kaiyu Jiang,Bin Wen,Fan Yang,Tingting Gao,Guorui Zhou,Liqiang Nie*

Main category: cs.CL

TL;DR: 提出可控同理心推理方法，结合自然语言推理与心理步骤，通过强化学习与冗余处理策略显著提升情感对话支持效果


<details>
  <summary>Details</summary>
Motivation: 现有情感支持模型缺乏基于心理学原理的深度共情推理能力

Method: 构建细粒度标注数据集+强化学习框架（含过程-结果双奖励模型）+个性化对话重写与冗余感知奖励加权策略

Result: 模型情感支持能力提升23.6%，响应重复率降低41%

Conclusion: 该方法成功突破共情推理瓶颈，为人性化情感支持系统开发提供新范式

Abstract: Emotional support conversations are crucial for promoting emotional
well-being, yet current models often lack deep empathetic reasoning grounded in
psychological principles. To address this, we propose controllable empathetic
reasoning, which combines natural language reasoning with structured
psychological steps. We construct a fine-grained dataset annotated with
reasoning correctness and response preferences to enable this capability. To
further enhance training, we employ reinforcement learning with a unified
process-outcome reward model that delivers precise feedback. To mitigate
response repetitiveness from entropy collapse, we introduce personality-based
dialogue rewriting and a redundancy-aware reward reweighting strategy. Our
approach significantly improves model's emotional support ability, advancing
the development of empathetic, human-like support systems.

</details>


### [18] [The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage](https://arxiv.org/abs/2508.09603)
*Skyler Hallinan,Jaehun Jung,Melanie Sclar,Ximing Lu,Abhilasha Ravichander,Sahana Ramnath,Yejin Choi,Sai Praneeth Karimireddy,Niloofar Mireshghallah,Xiang Ren*

Main category: cs.CL

TL;DR: 提出N-Gram覆盖率攻击方法，仅需文本输出即可实现黑盒模型的成员推理，并在实验中超越其他黑盒方法，发现GPT-4o等新型号隐私保护增强


<details>
  <summary>Details</summary>
Motivation: 现有成员推理攻击依赖模型内部信息，无法应用于仅API访问的模型（如GPT-4）。需开发仅依赖文本输出的攻击方法以扩展调查范围

Method: 利用模型倾向于生成训练数据常见模式的特性：1）基于候选成员前缀生成多组文本 2）通过n-gram重叠度计算输出与真实后缀的相似性 3）聚合相似度判断成员资格

Result: 在基准测试中优于其他黑盒方法，性能匹敌白盒攻击；攻击成功率随生成文本数量增加而提升；发现GPT-4o等新型号模型隐私保护能力增强

Conclusion: N-Gram覆盖率攻击为黑盒模型隐私评估提供有效工具，同时显示OpenAI模型的隐私保护能力呈持续改进趋势

Abstract: Membership inference attacks serves as useful tool for fair use of language
models, such as detecting potential copyright infringement and auditing data
leakage. However, many current state-of-the-art attacks require access to
models' hidden states or probability distribution, which prevents investigation
into more widely-used, API-access only models like GPT-4. In this work, we
introduce N-Gram Coverage Attack, a membership inference attack that relies
solely on text outputs from the target model, enabling attacks on completely
black-box models. We leverage the observation that models are more likely to
memorize and subsequently generate text patterns that were commonly observed in
their training data. Specifically, to make a prediction on a candidate member,
N-Gram Coverage Attack first obtains multiple model generations conditioned on
a prefix of the candidate. It then uses n-gram overlap metrics to compute and
aggregate the similarities of these outputs with the ground truth suffix; high
similarities indicate likely membership. We first demonstrate on a diverse set
of existing benchmarks that N-Gram Coverage Attack outperforms other black-box
methods while also impressively achieving comparable or even better performance
to state-of-the-art white-box attacks - despite having access to only text
outputs. Interestingly, we find that the success rate of our method scales with
the attack compute budget - as we increase the number of sequences generated
from the target model conditioned on the prefix, attack performance tends to
improve. Having verified the accuracy of our method, we use it to investigate
previously unstudied closed OpenAI models on multiple domains. We find that
more recent models, such as GPT-4o, exhibit increased robustness to membership
inference, suggesting an evolving trend toward improved privacy protections.

</details>


### [19] [AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian](https://arxiv.org/abs/2508.09622)
*Tatiana Batura,Elena Bruches,Milana Shvenk,Valentin Malykh*

Main category: cs.CL

TL;DR: 提出AINL-Eval 2025共享任务，构建俄语科学摘要检测数据集，推动AI生成内容识别研究


<details>
  <summary>Details</summary>
Motivation: 应对大语言模型对学术诚信的威胁，解决多语种场景下检测资源不足的问题

Method: 创建包含52,305样本的多领域数据集（12个科学领域人类摘要+5个LLM生成），设计两阶段跨领域/跨模型泛化任务

Result: 吸引10个团队提交159次，最优系统展现强检测能力，建立持续研究平台

Conclusion: 通过共享任务和开源平台推动AI内容检测技术的长期发展，维护学术真实性

Abstract: The rapid advancement of large language models (LLMs) has revolutionized text
generation, making it increasingly difficult to distinguish between human- and
AI-generated content. This poses a significant challenge to academic integrity,
particularly in scientific publishing and multilingual contexts where detection
resources are often limited. To address this critical gap, we introduce the
AINL-Eval 2025 Shared Task, specifically focused on the detection of
AI-generated scientific abstracts in Russian. We present a novel, large-scale
dataset comprising 52,305 samples, including human-written abstracts across 12
diverse scientific domains and AI-generated counterparts from five
state-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and
GigaChat-Lite). A core objective of the task is to challenge participants to
develop robust solutions capable of generalizing to both (i) previously unseen
scientific domains and (ii) models not included in the training data. The task
was organized in two phases, attracting 10 teams and 159 submissions, with top
systems demonstrating strong performance in identifying AI-generated content.
We also establish a continuous shared task platform to foster ongoing research
and long-term progress in this important area. The dataset and platform are
publicly available at https://github.com/iis-research-team/AINL-Eval-2025.

</details>


### [20] [Improving Diversity in Language Models: When Temperature Fails, Change the Loss](https://arxiv.org/abs/2508.09654)
*Alexandre Verine,Florian Le Bronnec,Kunhao Zheng,Alexandre Allauzen,Yann Chevaleyre,Benjamin Negrevergne*

Main category: cs.CL

TL;DR: 通过Precision-Recall框架重构损失函数，实现了比传统负对数似然训练+温度调节更好的权衡效果


<details>
  <summary>Details</summary>
Motivation: 传统方法通过提升解码温度增强语言模型多样性时，常出现无法有效提升覆盖率（Recall）的问题

Method: 提出基于Precision-Recall框架的损失函数设计方法，替代单纯的负对数似然训练

Result: 新方法在Precision和Recall的权衡上比传统温度调节方法提升显著

Conclusion: 这一发现为构建更全面鲁棒的语言模型提供了新的技术路径

Abstract: Increasing diversity in language models is a challenging yet essential
objective. A common approach is to raise the decoding temperature. In this
work, we investigate this approach through a simplistic yet common case to
provide insights into why decreasing temperature can improve quality
(Precision), while increasing it often fails to boost coverage (Recall). Our
analysis reveals that for a model to be effectively tunable through temperature
adjustments, it must be trained toward coverage. To address this, we propose
rethinking loss functions in language models by leveraging the Precision-Recall
framework. Our results demonstrate that this approach achieves a substantially
better trade-off between Precision and Recall than merely combining negative
log-likelihood training with temperature scaling. These findings offer a
pathway toward more versatile and robust language modeling techniques.

</details>


### [21] [EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization](https://arxiv.org/abs/2508.09662)
*Yaoning Wang,Jiahao Ying,Yixin Cao,Yubo Ma,Yugang Jiang*

Main category: cs.CL

TL;DR: EffiEval通过自适应选择高质量数据子集（基于MUI指标），在仅使用10%数据量时仍保持与全数据集评估一致的模型排名结果，解决了大模型评估中的计算冗余问题。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评估存在两大痛点：1) 大规模多样化评测基准带来极高计算成本；2) 数据冗余导致评估效率低下。传统方法依赖全量数据评估或绝对性能指标，缺乏灵活性和公平性。

Method: 提出基于模型效用指数（MUI）的自适应子集选择方法：1) 通过特征空间分析筛选代表性样本；2) 保持样本选择与模型表现无关确保公平性；3) 支持跨数据集/模型族的迁移应用。

Result: 在多个公开基准测试中，仅使用5-10%数据量即可达到与全数据集评估0.95+的Spearman排序相关性，计算效率提升10-20倍。

Conclusion: EffiEval为LLM评估提供了高效、公平、可迁移的解决方案，支持用户根据需求灵活平衡评估效率与结果可靠性。

Abstract: The rapid advancement of large language models (LLMs) and the development of
increasingly large and diverse evaluation benchmarks have introduced
substantial computational challenges for model assessment. In this paper, we
present EffiEval, a training-free approach for efficient benchmarking that
effectively addresses data redundancy while maintaining high evaluation
reliability. Our method is specifically designed to meet three key criteria for
high-quality evaluation: representativeness, by ensuring comprehensive coverage
of model capabilities; fairness, by remaining independent of model performance
during sample selection to avoid bias; and generalizability, by enabling
flexible transfer across datasets and model families without reliance on
large-scale evaluation data. Unlike traditional methods that rely on absolute
performance or require extensive evaluation data, our approach adaptively
selects high-quality representative subsets based on the Model Utility Index
(MUI). Extensive experiments on multiple public benchmarks and diverse LLMs
demonstrate that EffiEval achieves strong ranking consistency with full-dataset
evaluation using only a small fraction of the original data. Furthermore, our
method is flexible and scalable in size, allowing users to balance evaluation
efficiency and representativeness according to specific needs. Overall,
EffiEval provides a practical and generalizable solution for reliable, fair,
and efficient evaluation in the era of LLMs.

</details>


### [22] [Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation](https://arxiv.org/abs/2508.09666)
*Ziyang Ma,Qingyue Yuan,Linhai Zhang,Deyu Zhou*

Main category: cs.CL

TL;DR: 提出安全蒸馏方法SLowED，通过Slow Tuning和Low-Entropy Masking模块，在提升小语言模型推理能力的同时保持安全性。


<details>
  <summary>Details</summary>
Motivation: 现有CoT蒸馏方法专注于提升SLM推理能力，但忽视了训练过程对模型安全性的负面影响。传统安全对齐方法需要额外计算/标注数据且可能损害推理能力。

Method: 1. Slow Tuning：限制权重变化幅度，在初始权重邻域内优化
2. Low-Entropy Masking：屏蔽低熵token（视为无效学习目标）

Result: 在Qwen2.5-1.5B等三个SLM上的实验表明：
- 推理能力（BBH/ARC等基准）与现有方法相当
- 安全性（AdvBench）显著保持
- 消融实验验证模块有效性

Conclusion: SLowED成功平衡安全与推理能力：
- Slow Tuning保障早期安全性
- Low-Entropy Masking延长安全训练周期

Abstract: Previous chain-of-thought (CoT) distillation methods primarily focused on
enhancing the reasoning capabilities of Small Language Models (SLMs) by
utilizing high-quality rationales generated by powerful Large Language Models
(LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM
safety brought by the training, which are revealed in this study. Although
there are works on safety alignment that fine-tune language models or
manipulate model weights to defend against harmful inputs, they require extra
computation or annotated data, and probably impact the reasoning ability of
SLMs. In this paper, we investigate how to maintain the safety of SLMs during
the CoT distillation process. Specifically, we propose a safe distillation
method, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing
two modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the
magnitude of model weight changes to optimize the model weights in the
neighboring space near the initial weight distribution. Low-Entropy Masking
masks low-entropy tokens, which are regarded as unnecessary learning targets,
to exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B,
Llama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC,
AGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety
of SLMs and comparably improves their reasoning capability compared to existing
distillation methods. Furthermore, our ablation study presents the
effectiveness of Slow Tuning and Low-Entropy Masking, with the former
maintaining the model's safety in the early stage and the latter prolonging the
safe training epochs.

</details>


### [23] [Evaluating the Role of Large Language Models in Legal Practice in India](https://arxiv.org/abs/2508.09713)
*Rahul Hemrajani*

Main category: cs.CL

TL;DR: LLMs在法律起草和问题发现任务中表现优异(匹配或超越人类)，但在专业法律研究中易产生错误信息


<details>
  <summary>Details</summary>
Motivation: 实证评估LLMs在印度法律环境下的关键任务表现(问题识别/法律起草/研究/推理)，明确其能力边界

Method: 通过调查实验比较LLMs与初级律师的输出质量，由法学院高年级学生进行多维评估(帮助性/准确性/全面性)

Result: 模型擅长标准化输出任务，但在专业领域研究中出现63%的幻觉率

Conclusion: LLMs可作为法律辅助工具，但复杂推理和精准法律应用仍需人类专家介入

Abstract: The integration of Artificial Intelligence(AI) into the legal profession
raises significant questions about the capacity of Large Language Models(LLM)
to perform key legal tasks. In this paper, I empirically evaluate how well
LLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian
context, including issue spotting, legal drafting, advice, research, and
reasoning. Through a survey experiment, I compare outputs from LLMs with those
of a junior lawyer, with advanced law students rating the work on helpfulness,
accuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,
often matching or surpassing human work. However, they struggle with
specialised legal research, frequently generating hallucinations, factually
incorrect or fabricated outputs. I conclude that while LLMs can augment certain
legal tasks, human expertise remains essential for nuanced reasoning and the
precise application of law.

</details>


### [24] [The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models](https://arxiv.org/abs/2508.09716)
*Ridwan Mahbub,Mohammed Saidul Islam,Md Tahmid Rahman Laskar,Mizanur Rahman,Mir Tafseer Nayeem,Enamul Hoque*

Main category: cs.CL

TL;DR: 研究揭示视觉语言模型易受误导性图表设计影响，需加强防误导机制


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型(VLMs)被非专业用户广泛用于图表解读，需验证其对误导性可视化设计的抗干扰能力

Method: 通过分析10个不同模型在8类误导性图表设计中的16,000+条响应数据

Result: 绝大多数VLMs会被视觉设计欺骗，导致对相同数据产生不同解读

Conclusion: 应在VLMs中建立视觉信息验证机制，防止视觉错误信息传播

Abstract: Information visualizations are powerful tools that help users quickly
identify patterns, trends, and outliers, facilitating informed decision-making.
However, when visualizations incorporate deceptive design elements-such as
truncated or inverted axes, unjustified 3D effects, or violations of best
practices-they can mislead viewers and distort understanding, spreading
misinformation. While some deceptive tactics are obvious, others subtly
manipulate perception while maintaining a facade of legitimacy. As
Vision-Language Models (VLMs) are increasingly used to interpret
visualizations, especially by non-expert users, it is critical to understand
how susceptible these models are to deceptive visual designs. In this study, we
conduct an in-depth evaluation of VLMs' ability to interpret misleading
visualizations. By analyzing over 16,000 responses from ten different models
across eight distinct types of misleading chart designs, we demonstrate that
most VLMs are deceived by them. This leads to altered interpretations of
charts, despite the underlying data remaining the same. Our findings highlight
the need for robust safeguards in VLMs against visual misinformation.

</details>


### [25] [Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning](https://arxiv.org/abs/2508.09726)
*Vaishnavi Shrivastava,Ahmed Awadallah,Vidhisha Balachandran,Shivam Garg,Harkirat Behl,Dimitris Papailiopoulos*

Main category: cs.CL

TL;DR: GFPO通过训练时增加样本采样并过滤低效响应，有效抑制大语言模型的冗余文本生成，在保持精度的同时将回答长度减少46-85%


<details>
  <summary>Details</summary>
Motivation: 强化学习训练的语言模型倾向于通过增加回答长度来提升准确率，导致大量低效的冗余文本（重复、冗长但无实质进展）

Method: GFPO（群体过滤策略优化）：1. 训练时对每个问题采样更大群体 2. 基于响应长度和token效率（单位token奖励值）双重指标过滤训练样本

Result: 在Phi-4模型上，GFPO将STEM/编程基准测试（AIME/GPQA等）的冗余文本减少46-71%，优化token效率后达71-85%，同时保持准确率

Conclusion: GFPO验证了训练阶段增加计算量可有效降低推理阶段计算需求，实现高效推理的算力平衡，并提出自适应难度训练进一步提升困难问题的处理效率

Abstract: Large language models trained with reinforcement learning with verifiable
rewards tend to trade accuracy for length--inflating response lengths to
achieve gains in accuracy. While longer answers may be warranted for harder
problems, many tokens are merely "filler": repetitive, verbose text that makes
no real progress. We introduce GFPO (Group Filtered Policy Optimization), which
curbs this length explosion by sampling larger groups per problem during
training and filtering responses to train on based on two key metrics: (1)
response length and (2) token efficiency: reward per token ratio. By sampling
more at training time, we teach models to think less at inference time. On the
Phi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across
challenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,
LiveCodeBench) while maintaining accuracy. Optimizing for reward per token
further increases reductions in length inflation to 71-85%. We also propose
Adaptive Difficulty GFPO, which dynamically allocates more training resources
to harder problems based on real-time difficulty estimates, improving the
balance between computational efficiency and accuracy especially on difficult
questions. GFPO demonstrates that increased training-time compute directly
translates to reduced test-time compute--a simple yet effective trade-off for
efficient reasoning.

</details>


### [26] [Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation](https://arxiv.org/abs/2508.09755)
*Seokgi Lee*

Main category: cs.CL

TL;DR: 提出基于问题分解和可回答问题嵌入的新型RAG框架，显著提升多跳问答性能


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统在多跳问答场景存在检索模糊性问题，且文档块直接嵌入可能丢失语义信息

Method: 1. 使用LLM将多跳问题分解为单跳子问题序列
2. 用Qwen3-8B生成文档块的可回答问题，通过问题-问题嵌入相似性检索
3. 将检索结果输入RAG流水线

Result: 在LongBench的MuSiQue/2WikiMultiHopQa/HotpotQA数据集上超越基线系统

Conclusion: 可回答问题嵌入和LLM驱动的查询分解策略有效提升多跳RAG性能

Abstract: We introduce a novel retrieval-augmented generation (RAG) framework tailored
for multihop question answering. First, our system uses large language model
(LLM) to decompose complex multihop questions into a sequence of single-hop
subquestions that guide document retrieval. This decomposition mitigates the
ambiguity inherent in multi-hop queries by clearly targeting distinct knowledge
facets. Second, instead of embedding raw or chunked documents directly, we
generate answerable questions from each document chunk using Qwen3-8B, embed
these generated questions, and retrieve relevant chunks via question-question
embedding similarity. During inference, the retrieved chunks are then fed along
with the original question into the RAG pipeline. We evaluate on three multihop
question datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our
method improves RAG performacne compared to baseline systems. Our contributions
highlight the benefits of using answerable-question embeddings for RAG, and the
effectiveness of LLM-based query decomposition for multihop scenarios.

</details>


### [27] [Echoes of Agreement: Argument Driven Opinion Shifts in Large Language Models](https://arxiv.org/abs/2508.09759)
*Avneet Kaur*

Main category: cs.CL

TL;DR: 研究发现提示词中的支持/反驳论点会显著改变大语言模型在政治话题上的立场回应，揭示其存在迎合倾向


<details>
  <summary>Details</summary>
Motivation: 现有政治偏见评估忽略提示词本身论点对模型立场的影响，这关系到评估结果的稳健性和模型行为的深入理解

Method: 通过设计含支持/反驳论点的提示词，在单轮和多轮对话场景下系统测试模型政治立场变化

Result: 论点存在时模型立场明显向提示方向偏移，且论点强度与方向一致性正相关，多轮对话中迎合效应更显著

Conclusion: 提示工程对政治偏见测量具有重大影响，需开发能抵抗论点干扰的评估框架和缓解策略

Abstract: There have been numerous studies evaluating bias of LLMs towards political
topics. However, how positions towards these topics in model outputs are highly
sensitive to the prompt. What happens when the prompt itself is suggestive of
certain arguments towards those positions remains underexplored. This is
crucial for understanding how robust these bias evaluations are and for
understanding model behaviour, as these models frequently interact with
opinionated text. To that end, we conduct experiments for political bias
evaluation in presence of supporting and refuting arguments. Our experiments
show that such arguments substantially alter model responses towards the
direction of the provided argument in both single-turn and multi-turn settings.
Moreover, we find that the strength of these arguments influences the
directional agreement rate of model responses. These effects point to a
sycophantic tendency in LLMs adapting their stance to align with the presented
arguments which has downstream implications for measuring political bias and
developing effective mitigation strategies.

</details>


### [28] [UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech](https://arxiv.org/abs/2508.09767)
*Shuhei Kato*

Main category: cs.CL

TL;DR: UtterTune提出基于LLM架构的轻量级多语言TTS自适应方法，通过低秩适应增强日语发音控制同时保持其他语言性能


<details>
  <summary>Details</summary>
Motivation: LLM架构的TTS模型在G2P映射和韵律建模方面存在挑战，特别是缺乏显式G2P模块时难以精准控制日语音素级发音特征

Method: 采用低秩适应技术实现日语语音的音素级别发音控制和音高重音调整，支持零样本场景下的自然度和说话人相似性保持

Result: 主客观评估证实该方法在提升日语发音可控性的同时，有效维持了多语言场景下的语音自然度和说话人相似性

Conclusion: UtterTune为LLM架构的TTS系统提供了一种高效的跨语言发音控制解决方案，在保持多语言性能平衡方面具有显著优势

Abstract: We propose UtterTune, a lightweight adaptation method that fine-tunes a
multilingual text-to-speech (TTS) system based on a large language model (LLM)
architecture, designed to enhance the controllability of pronunciation in a
target language while preserving performance in others. While LLM architectures
have enabled TTS models to achieve remarkable naturalness, accurately modeling
grapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially
when the model omits an explicit G2P module and directly processes minimally
encoded text (e.g., byte-pair encoding). UtterTune leverages low-rank
adaptation to enable the control of segmental pronunciation and pitch accent at
the phoneme level for Japanese speech, the target language in this paper, while
maintaining naturalness and speaker similarity in a zero-shot setting.
Objective and subjective evaluations confirm its effectiveness.

</details>


### [29] [Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study](https://arxiv.org/abs/2508.09776)
*Mahdi Dhaini,Juraj Vladika,Ege Erdogan,Zineb Attaoui,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 提出自动化LLM框架生成文本解释，实验证明其效果媲美人工标注并能提升NLP模型性能


<details>
  <summary>Details</summary>
Motivation: 传统人工标注解释成本高且不可扩展，需寻求自动化解决方案

Method: 使用多LLM生成解释，通过NLG指标评估质量，并在两个自然语言推理基准数据集测试对PLM/LLM的影响

Result: 自动生成解释在提升模型性能方面与人工标注效果相当（highly competitive）

Conclusion: LLM自动化生成文本解释是扩展数据集和增强模型性能的有效途径

Abstract: In the rapidly evolving field of Explainable Natural Language Processing
(NLP), textual explanations, i.e., human-like rationales, are pivotal for
explaining model predictions and enriching datasets with interpretable labels.
Traditional approaches rely on human annotation, which is costly,
labor-intensive, and impedes scalability. In this work, we present an automated
framework that leverages multiple state-of-the-art large language models (LLMs)
to generate high-quality textual explanations. We rigorously assess the quality
of these LLM-generated explanations using a comprehensive suite of Natural
Language Generation (NLG) metrics. Furthermore, we investigate the downstream
impact of these explanations on the performance of pre-trained language models
(PLMs) and LLMs across natural language inference tasks on two diverse
benchmark datasets. Our experiments demonstrate that automated explanations
exhibit highly competitive effectiveness compared to human-annotated
explanations in improving model performance. Our findings underscore a
promising avenue for scalable, automated LLM-based textual explanation
generation for extending NLP datasets and enhancing model performance.

</details>


### [30] [Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges](https://arxiv.org/abs/2508.09786)
*Mahdi Dhaini,Tobias Müller,Roksoliana Rabets,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 通过访谈研究发现当前可解释性NLP方法存在概念模糊、满意度低和评估困难，需建立用户中心框架提升实践效果


<details>
  <summary>Details</summary>
Motivation: 针对复杂NLP模型透明化需求日益增长但从业者实践经验研究不足的现状，探究实际应用中解释性方法的真实使用效果和挑战

Method: 采用质性访谈方法，对工业界从业者和学术研究者进行对比研究，系统分析双方视角差异

Result: 揭示概念定义不统一（68%受访者困惑）、现有方法满意度低（仅21%满意）、评估体系缺乏客观标准三大核心问题

Conclusion: 构建明确定义标准和用户导向的评估框架是推动可解释性NLP实际落地的关键路径

Abstract: The field of explainable natural language processing (NLP) has grown rapidly
in recent years. The growing opacity of complex models calls for transparency
and explanations of their decisions, which is crucial to understand their
reasoning and facilitate deployment, especially in high-stakes environments.
Despite increasing attention given to explainable NLP, practitioners'
perspectives regarding its practical adoption and effectiveness remain
underexplored. This paper addresses this research gap by investigating
practitioners' experiences with explainability methods, specifically focusing
on their motivations for adopting such methods, the techniques employed,
satisfaction levels, and the practical challenges encountered in real-world NLP
applications. Through a qualitative interview-based study with industry
practitioners and complementary interviews with academic researchers, we
systematically analyze and compare their perspectives. Our findings reveal
conceptual gaps, low satisfaction with current explainability methods, and
highlight evaluation challenges. Our findings emphasize the need for clear
definitions and user-centric frameworks for better adoption of explainable NLP
in practice.

</details>


### [31] [BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning](https://arxiv.org/abs/2508.09804)
*Ahmed Masry,Abhay Puri,Masoud Hashemi,Juan A. Rodriguez,Megh Thakkar,Khyati Mahajan,Vikas Yadav,Sathwik Tejaswi Madhusudhan,Alexandre Piché,Dzmitry Bahdanau,Christopher Pal,David Vazquez,Enamul Hoque,Perouz Taslakian,Sai Rajeswar,Spandana Gella*

Main category: cs.CL

TL;DR: 提出BigCharts数据集构建流程与GRPO强化学习框架，解决图表数据多样性不足与训练方法局限，实现SOTA图表推理模型BigCharts-R1。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型因训练数据缺乏真实多样性、自动提取图表数据存在误差，且仅依赖监督微调，导致图表理解效果受限。

Method: 1. BigCharts通过重新绘制真实图表生成多样化图像，保留准确数据；2. 结合监督微调与基于GRPO的强化学习框架，设计专属图表推理奖励机制。

Result: BigCharts-R1在多个图表问答基准超越现有方法，性能优于更大规模的开源/闭源模型。

Conclusion: 通过真实数据增强与混合训练策略，有效提升模型对多样化图表风格的泛化能力与推理鲁棒性。

Abstract: Charts are essential to data analysis, transforming raw data into clear
visual representations that support human decision-making. Although current
vision-language models (VLMs) have made significant progress, they continue to
struggle with chart comprehension due to training on datasets that lack
diversity and real-world authenticity, or on automatically extracted underlying
data tables of charts, which can contain numerous estimation errors.
Furthermore, existing models only rely on supervised fine-tuning using these
low-quality datasets, severely limiting their effectiveness. To address these
issues, we first propose BigCharts, a dataset creation pipeline that generates
visually diverse chart images by conditioning the rendering process on
real-world charts sourced from multiple online platforms. Unlike purely
synthetic datasets, BigCharts incorporates real-world data, ensuring
authenticity and visual diversity, while still retaining accurate underlying
data due to our proposed replotting process. Additionally, we introduce a
comprehensive training framework that integrates supervised fine-tuning with
Group Relative Policy Optimization (GRPO)-based reinforcement learning. By
introducing novel reward signals specifically designed for chart reasoning, our
approach enhances model robustness and generalization across diverse chart
styles and domains, resulting in a state-of-the-art chart reasoning model,
BigCharts-R1. Extensive experiments demonstrate that our models surpass
existing methods on multiple chart question-answering benchmarks compared to
even larger open-source and closed-source models.

</details>


### [32] [A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems](https://arxiv.org/abs/2508.09809)
*Aishik Mandal,Prottay Kumar Adhikary,Hiba Arnaout,Iryna Gurevych,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 系统调查心理健康AI训练数据集现状，揭示数据分散性、标准化不足等问题，提出数据集优化建议以提升AI系统效能。


<details>
  <summary>Details</summary>
Motivation: 全球心理健康需求激增与临床资源不足的矛盾凸显，AI辅助诊疗潜力受限于低质量数据集，需系统梳理数据现状推动技术发展。

Method: 通过多维分类（疾病类型/数据模态/任务类型/可访问性/文化背景）评估现有数据集，并分析合成数据局限性。

Result: 发现数据集存在纵向数据缺失、文化多样性不足、采集标准混乱、合成数据模态单一等关键缺陷。

Conclusion: 建议建立标准化数据规范、加强跨文化数据采集、开发多模态合成数据，以构建更公平稳健的心理健康AI系统。

Abstract: Mental health disorders are rising worldwide. However, the availability of
trained clinicians has not scaled proportionally, leaving many people without
adequate or timely support. To bridge this gap, recent studies have shown the
promise of Artificial Intelligence (AI) to assist mental health diagnosis,
monitoring, and intervention. However, the development of efficient, reliable,
and ethical AI to assist clinicians is heavily dependent on high-quality
clinical training datasets. Despite growing interest in data curation for
training clinical AI assistants, existing datasets largely remain scattered,
under-documented, and often inaccessible, hindering the reproducibility,
comparability, and generalizability of AI models developed for clinical mental
health care. In this paper, we present the first comprehensive survey of
clinical mental health datasets relevant to the training and development of
AI-powered clinical assistants. We categorize these datasets by mental
disorders (e.g., depression, schizophrenia), data modalities (e.g., text,
speech, physiological signals), task types (e.g., diagnosis prediction, symptom
severity estimation, intervention generation), accessibility (public,
restricted or private), and sociocultural context (e.g., language and cultural
background). Along with these, we also investigate synthetic clinical mental
health datasets. Our survey identifies critical gaps such as a lack of
longitudinal data, limited cultural and linguistic representation, inconsistent
collection and annotation standards, and a lack of modalities in synthetic
data. We conclude by outlining key challenges in curating and standardizing
future datasets and provide actionable recommendations to facilitate the
development of more robust, generalizable, and equitable mental health AI
systems.

</details>


### [33] [Speed Always Wins: A Survey on Efficient Architectures for Large Language Models](https://arxiv.org/abs/2508.09834)
*Weigao Sun,Jiaxi Hu,Yucheng Zhou,Jusen Du,Disen Lan,Kexin Wang,Tong Zhu,Xiaoye Qu,Yu Zhang,Xiaoyu Mo,Daizong Liu,Yuxuan Liang,Wenliang Chen,Guoqi Li,Yu Cheng*

Main category: cs.CL

TL;DR: 论文系统分析了提升大型语言模型效率的创新架构方法，涵盖线性/稀疏序列建模、高效注意力变体、混合专家模型等技术路径，并探讨其对多模态应用的扩展影响。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer架构存在高计算量瓶颈，阻碍大规模训练与部署，需探索更高效的LLM架构方案以突破算力限制。

Method: 采用系统性分类研究：1) 线性/稀疏序列建模方法 2) 高效注意力机制改进 3) 稀疏混合专家系统 4) 混合架构集成技术 5) 新兴扩散语言模型

Result: 构建了现代高效LLM架构的技术蓝图，为开发可扩展、资源优化的基础模型提供结构化框架，推动高效AI系统的演进方向。

Conclusion: 通过系统梳理现有高效架构技术，揭示多路径优化可能，强调跨模态应用的扩展价值，为构建资源高效的基础模型指明发展方向。

Abstract: Large Language Models (LLMs) have delivered impressive results in language
understanding, generation, reasoning, and pushes the ability boundary of
multimodal models. Transformer models, as the foundation of modern LLMs, offer
a strong baseline with excellent scaling properties. However, the traditional
transformer architecture requires substantial computations and poses
significant obstacles for large-scale training and practical deployment. In
this survey, we offer a systematic examination of innovative LLM architectures
that address the inherent limitations of transformers and boost the efficiency.
Starting from language modeling, this survey covers the background and
technical details of linear and sparse sequence modeling methods, efficient
full attention variants, sparse mixture-of-experts, hybrid model architectures
incorporating the above techniques, and emerging diffusion LLMs. Additionally,
we discuss applications of these techniques to other modalities and consider
their wider implications for developing scalable, resource-aware foundation
models. By grouping recent studies into the above category, this survey
presents a blueprint of modern efficient LLM architectures, and we hope this
could help motivate future research toward more efficient, versatile AI
systems.

</details>


### [34] [PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts](https://arxiv.org/abs/2508.09848)
*Mo Yu,Tsz Ting Chung,Chulun Zhou,Tong Li,Rui Lu,Jiangnan Li,Liyan Xu,Haoshu Lu,Ning Zhang,Jing Li,Jie Zhou*

Main category: cs.CL

TL;DR: 提出了PRELUDE基准测试，通过判断角色前传故事与原著一致性的任务，揭示当前LLMs在长文本深度推理方面存在显著差距（落后人类>15%），且存在正确结论伴随错误推理的缺陷（推理准确率差距>30%）。


<details>
  <summary>Details</summary>
Motivation: 现有长文本理解评测基准对全局理解和深度推理的要求不足，需要通过需要多证据整合的强评估任务（88%案例需多段落证据）来突破现有模型瓶颈。

Method: 构建PRELUDE评测集，采用in-context learning/RAG/微调等方法测试SOTA模型，并通过人类研究分析模型推理缺陷。

Result: 现有最佳模型与人类表现差距>15%；人类评估发现模型存在"正确结论+错误推理"现象，推理准确率差距达30%以上。

Conclusion: 长文本理解与推理能力仍有巨大提升空间，模型存在表面对齐但底层推理缺陷的问题，需开发新的评估与训练方法。

Abstract: We introduce PRELUDE, a benchmark for evaluating long-context understanding
through the task of determining whether a character's prequel story is
consistent with the canonical narrative of the original book. Our task poses a
stronger demand for global comprehension and deep reasoning than existing
benchmarks -- as the prequels are not part of the original story, assessing
their plausibility typically requires searching and integrating information
that is only indirectly related. Empirically, 88% of instances require evidence
from multiple parts of the narrative. Experimental results highlight the
challenge of our task: in-context learning, RAG and in-domain training with
state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans
by >15%. A further human study reveals that models often produce correct
answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy
compared to humans. These findings underscore the substantial room for
improvement in long-context understanding and reasoning.

</details>


### [35] [Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription](https://arxiv.org/abs/2508.09865)
*Abdul Rehman Antall,Naveed Akhtar*

Main category: cs.CL

TL;DR: 轻量级Whisper-Small模型在未调优情况下取得乌尔都语语音识别最低错误率（33.68% WER），但复杂语句处理仍存挑战。


<details>
  <summary>Details</summary>
Motivation: 针对230 million使用者的乌尔都语面临方言多样性、语码转换和数据稀缺问题，探索低资源ASR解决方案

Method: 使用精选乌尔都语数据集，在未微调条件下对比Tiny/Base/Small三种Whisper模型的词错率（WER）表现

Result: Small > Base > Tiny（33.68% vs 53.67% vs 67.08% WER），定性分析揭示语音准确性和词汇连贯性缺陷

Conclusion: Whisper-Small展现部署潜力但存在显著不足，为低资源ASR系统开发奠定基准测试基础

Abstract: This study evaluates the feasibility of lightweight Whisper models (Tiny,
Base, Small) for Urdu speech recognition in low-resource settings. Despite Urdu
being the 10th most spoken language globally with over 230 million speakers,
its representation in automatic speech recognition (ASR) systems remains
limited due to dialectal diversity, code-switching, and sparse training data.
We benchmark these models on a curated Urdu dataset using word error rate
(WER), without fine-tuning. Results show Whisper-Small achieves the lowest
error rates (33.68\% WER), outperforming Tiny (67.08\% WER) and Base (53.67\%
WER). Qualitative analysis reveals persistent challenges in phonetic accuracy
and lexical coherence, particularly for complex utterances. While Whisper-Small
demonstrates promise for deployable Urdu ASR, significant gaps remain. Our
findings emphasize lay the groundwork for future research into effective,
low-resource ASR systems.

</details>


### [36] [Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models](https://arxiv.org/abs/2508.09874)
*Jiaqi Cao,Jiarui Wang,Rubin Wei,Qipeng Guo,Kai Chen,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

TL;DR: 提出Memory Decoder预训练记忆模块，通过小型解码器模仿检索器行为，实现即插即用领域适应，平均降低困惑度6.17点。


<details>
  <summary>Details</summary>
Motivation: 现有DAPT方法存在全参数训练成本高与灾难性遗忘问题，RAG则因检索过程产生延迟。需开发不修改原模型参数的高效领域适应方案。

Method: 训练小型transformer解码器模仿非参数检索器行为，适配同tokenizer的任意预训练模型，无需模型特定修改。

Result: 在生物医学/金融/法律领域适配Qwen和Llama模型，平均降低困惑度6.17点，显著提升领域表现。

Conclusion: Memory Decoder开创了以专用预训练内存组件为核心的领域适应范式，即插即用架构可跨模型持续提升目标领域性能。

Abstract: Large Language Models (LLMs) have shown strong abilities in general language
tasks, yet adapting them to specific domains remains a challenge. Current
method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter
training and suffers from catastrophic forgetting. Meanwhile,
Retrieval-Augmented Generation (RAG) introduces substantial inference latency
due to expensive nearest-neighbor searches and longer context. This paper
introduces Memory Decoder, a plug-and-play pretrained memory that enables
efficient domain adaptation without changing the original model's parameters.
Memory Decoder employs a small transformer decoder that learns to imitate the
behavior of an external non-parametric retriever. Once trained, Memory Decoder
can be seamlessly integrated with any pretrained language model that shares the
same tokenizer, requiring no model-specific modifications. Experimental results
demonstrate that Memory Decoder enables effective adaptation of various Qwen
and Llama models to three distinct specialized domains: biomedicine, finance,
and law, reducing perplexity by an average of 6.17 points. Overall, Memory
Decoder introduces a novel paradigm centered on a specially pretrained memory
component designed for domain-specific adaptation. This memory architecture can
be integrated in a plug-and-play manner, consistently enhancing performance
across multiple models within the target domain.

</details>


### [37] [A Survey of Cognitive Distortion Detection and Classification in NLP](https://arxiv.org/abs/2508.09878)
*Archie Sage,Jeroen Keppens,Helen Yannakoudakis*

Main category: cs.CL

TL;DR: 综述论文系统梳理了自然语言处理在认知扭曲检测领域20年间38项研究，整合分类体系并指出现有挑战，旨在推动该领域形成更统一的研究框架。


<details>
  <summary>Details</summary>
Motivation: 针对认知扭曲检测领域存在的分类体系混乱、任务定义不统一、评估标准碎片化问题，需整合现有研究成果并为后续研究提供结构化参考。

Method: 通过系统性回顾近20年38篇文献，结构化分析数据集构建方式、建模方法（传统机器学习与深度学习）及评估策略（临床验证与自动指标）。

Result: 提出标准化认知扭曲分类参考框架，总结文本分类/序列标注等主流任务范式，揭示数据稀缺性、临床验证不足、跨文化适应性等核心挑战。

Conclusion: 需建立跨学科协作机制与标准化评估体系，推动该领域从碎片化探索向临床可落地的系统性研究转变。

Abstract: As interest grows in the application of natural language processing (NLP)
techniques to mental health, a growing body of work explores the automatic
detection and classification of cognitive distortions (CDs). CDs are habitual
patterns of negatively biased or flawed thinking that distort how people
perceive events, judge themselves, and react to the world around them.
Identifying and addressing them is an important part of therapy. Despite its
momentum, the field remains fragmented, with inconsistencies in CD taxonomies,
task formulations, and evaluation practices. This survey reviews 38 studies
spanning two decades, providing a structured overview of datasets, modelling
approaches, and evaluation strategies. We provide a consolidated CD taxonomy
reference, summarise common task setups, and highlight open challenges to
support more coherent and reproducible research in this emerging area.

</details>


### [38] [Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach](https://arxiv.org/abs/2508.09935)
*Sayem Hossen,Monalisa Moon Joti,Md. Golam Rashed*

Main category: cs.CL

TL;DR: 论文提出通过计算文本分析和个性化Transformer模型，在受控环境中实现99%以上欺骗性语言检测准确率，但多语言场景存在数据与基础设施不足的挑战。


<details>
  <summary>Details</summary>
Motivation: 数字化商务沟通在增强透明度的同时加剧了语言欺骗风险，需弥合理论传播模型与实证数据的鸿沟，建立AI驱动的自动化文本识别系统。

Method: 融合古典修辞学、传播心理学、语言学理论与财务报告/可持续性话语/数字营销的实证研究，采用计算文本分析和个性化Transformer模型进行检测。

Result: 受控环境检测准确率超99%，但多语言场景因训练数据匮乏及文本处理基础设施缺失导致效果显著下降。

Conclusion: 理论传播模型与实证数据的脱节加剧，亟需构建支持多语言的AI话语识别系统以应对日益逼真的人机交互场景。

Abstract: Business communication digitisation has reorganised the process of persuasive
discourse, which
  allows not only greater transparency but also advanced deception. This
inquiry synthesises classical
  rhetoric and communication psychology with linguistic theory and empirical
studies in the financial
  reporting, sustainability discourse, and digital marketing to explain how
deceptive language can be
  systematically detected using persuasive lexicon. In controlled settings,
detection accuracies of greater
  than 99% were achieved by using computational textual analysis as well as
personalised transformer
  models. However, reproducing this performance in multilingual settings is
also problematic and,
  to a large extent, this is because it is not easy to find sufficient data,
and because few multilingual
  text-processing infrastructures are in place. This evidence shows that there
has been an increasing
  gap between the theoretical representations of communication and those
empirically approximated,
  and therefore, there is a need to have strong automatic text-identification
systems where AI-based
  discourse is becoming more realistic in communicating with humans.

</details>


### [39] [A Comprehensive Evaluation framework of Alignment Techniques for LLMs](https://arxiv.org/abs/2508.09937)
*Muneeza Azmat,Momin Abbas,Maysa Malfiza Garcia de Macedo,Marcelo Carpinette Grave,Luan Soares de Souza,Tiago Machado,Rogerio A de Paula,Raya Horesh,Yixin Chen,Heloisa Caroline de Souza Pereira Candello,Rebecka Nordenlow,Aminat Adebiyi*

Main category: cs.CL

TL;DR: 论文提出了多维评估框架，系统比较大语言模型对齐技术的检测能力、质量、效率和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有大模型对齐技术（微调/后处理/推理干预）缺乏统一评估标准，难以指导实际部署决策

Method: 构建四维评估体系：对齐检测能力、对齐质量、计算效率、鲁棒性，覆盖主流基模型和对齐策略

Result: 实验验证框架有效性，揭示了当前SOTA模型的优势与局限性

Conclusion: 该框架为不同对齐范式提供系统评估基准，指明未来模型安全研究方向

Abstract: As Large Language Models (LLMs) become increasingly integrated into
real-world applications, ensuring their outputs align with human values and
safety standards has become critical. The field has developed diverse alignment
approaches including traditional fine-tuning methods (RLHF, instruction
tuning), post-hoc correction systems, and inference-time interventions, each
with distinct advantages and limitations. However, the lack of unified
evaluation frameworks makes it difficult to systematically compare these
paradigms and guide deployment decisions. This paper introduces a
multi-dimensional evaluation of alignment techniques for LLMs, a comprehensive
evaluation framework that provides a systematic comparison across all major
alignment paradigms. Our framework assesses methods along four key dimensions:
alignment detection, alignment quality, computational efficiency, and
robustness. Through experiments across diverse base models and alignment
strategies, we demonstrate the utility of our framework in identifying
strengths and limitations of current state-of-the-art models, providing
valuable insights for future research directions.

</details>


### [40] [VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models](https://arxiv.org/abs/2508.09945)
*Lingjie Jiang,Shaohan Huang,Xun Wu,Yixia Li,Dongdong Zhang,Furu Wei*

Main category: cs.CL

TL;DR: VisCodex提出统一框架，通过模型合并策略整合视觉与编码模型，结合新数据集MCD和评估基准InfiBench-V，显著提升多模态代码生成能力，接近GPT-4o水平。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在多模态代码生成方面存在局限，需融合视觉理解与代码生成能力。

Method: 采用任务向量合并技术集成编码LLM到视觉语言主干网络，构建包含598k样本的MCD数据集及视觉编程评估基准InfiBench-V。

Result: VisCodex在开源模型中达到SOTA，接近GPT-4o性能，验证方法有效性。

Conclusion: 模型合并策略与新型数据集成功提升多模态编码能力，为后续研究提供资源与评估标准。

Abstract: Multimodal large language models (MLLMs) have significantly advanced the
integration of visual and textual understanding. However, their ability to
generate code from multimodal inputs remains limited. In this work, we
introduce VisCodex, a unified framework that seamlessly merges vision and
coding language models to empower MLLMs with strong multimodal code generation
abilities. Leveraging a task vector-based model merging technique, we integrate
a state-of-the-art coding LLM into a strong vision-language backbone, while
preserving both visual comprehension and advanced coding skills. To support
training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a
large-scale and diverse collection of 598k samples, including high-quality HTML
code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic
problems. Furthermore, we propose InfiBench-V, a novel and challenging
benchmark specifically designed to assess models on visually-rich, real-world
programming questions that demand a nuanced understanding of both textual and
visual contexts. Extensive experiments show that VisCodex achieves
state-of-the-art performance among open-source MLLMs and approaches proprietary
models like GPT-4o, highlighting the effectiveness of our model merging
strategy and new datasets.

</details>


### [41] [Specialised or Generic? Tokenization Choices for Radiology Language Models](https://arxiv.org/abs/2508.09952)
*Hermione Warr,Wentian Xu,Harry Anthony,Yasin Ibrahim,Daniel McGowan,Konstantinos Kamnitsas*

Main category: cs.CL

TL;DR: 领域特定分词器在放射学报告总结任务中优于通用分词器，同时降低内存消耗和计算需求。


<details>
  <summary>Details</summary>
Motivation: 语言模型的分词器对文本生成质量至关重要，但在放射学领域的影响尚未被系统研究。研究者旨在验证领域适配的分词器能否提升临床文本处理效果。

Method: 1. 比较通用/医学/放射学领域分词器在三种成像模态报告总结任务的表现 2. 测试从头训练与PubMed预训练两种模式 3. 评估性能指标及内存/序列长度效率

Result: 1. 未预训练时医学/领域分词器表现更优 2. 预训练缩小不同分词器性能差距 3. 领域分词器内存占用减少30%，序列长度缩短20%

Conclusion: 临床领域适配的分词器可同时提升语言模型性能和计算效率，有利于医疗场景的实际应用部署。

Abstract: The vocabulary used by language models (LM) - defined by the tokenizer -
plays a key role in text generation quality. However, its impact remains
under-explored in radiology. In this work, we address this gap by
systematically comparing general, medical, and domain-specific tokenizers on
the task of radiology report summarisation across three imaging modalities. We
also investigate scenarios with and without LM pre-training on PubMed
abstracts. Our findings demonstrate that medical and domain-specific
vocabularies outperformed widely used natural language alternatives when models
are trained from scratch. Pre-training partially mitigates performance
differences between tokenizers, whilst the domain-specific tokenizers achieve
the most favourable results. Domain-specific tokenizers also reduce memory
requirements due to smaller vocabularies and shorter sequences. These results
demonstrate that adapting the vocabulary of LMs to the clinical domain provides
practical benefits, including improved performance and reduced computational
demands, making such models more accessible and effective for both research and
real-world healthcare settings.

</details>


### [42] [Shaping Event Backstories to Estimate Potential Emotion Contexts](https://arxiv.org/abs/2508.09954)
*Johannes Schäfer,Roman Klinger*

Main category: cs.CL

TL;DR: 通过自动生成多情绪事件链构建情境化数据集，验证上下文叙述能提升情感标注一致性


<details>
  <summary>Details</summary>
Motivation: 传统情感分析忽视事件上下文导致标注分歧，需验证情境补充对标注可靠性的影响

Method: 基于不同情绪生成事件链，结合短故事生成技术构建专门数据集，进行自动/人工双重评估

Result: 情境化叙述增强情绪解释性，标注一致性显著提升（自动指标+人工评估双重验证）

Conclusion: 上下文缺失是情感歧义主因，自动生成情境框架可有效支持实际标注任务

Abstract: Emotion analysis is an inherently ambiguous task. Previous work studied
annotator properties to explain disagreement, but this overlooks the
possibility that ambiguity may stem from missing information about the context
of events. In this paper, we propose a novel approach that adds reasonable
contexts to event descriptions, which may better explain a particular
situation. Our goal is to understand whether these enriched contexts enable
human annotators to annotate emotions more reliably. We disambiguate a target
event description by automatically generating multiple event chains conditioned
on differing emotions. By combining techniques from short story generation in
various settings, we achieve coherent narratives that result in a specialized
dataset for the first comprehensive and systematic examination of
contextualized emotion analysis. Through automatic and human evaluation, we
find that contextual narratives enhance the interpretation of specific emotions
and support annotators in producing more consistent annotations.

</details>


### [43] [Performance of GPT-5 Frontier Models in Ophthalmology Question Answering](https://arxiv.org/abs/2508.09956)
*Fares Antaki,David Mikhail,Daniel Milad,Danny A Mammo,Sumit Sharma,Sunil K Srivastava,Bing Yu Chen,Samir Touma,Mertcan Sevgi,Jonathan El-Khoury,Pearse A Keane,Qingyu Chen,Yih Chung Tham,Renaud Duval*

Main category: cs.CL

TL;DR: GPT-5-high在眼科多选题评估中达到96.5%准确率，成本效益分析显示GPT-5-mini-low为最佳性价比配置


<details>
  <summary>Details</summary>
Motivation: 确定GPT-5系列在医学问答任务中兼顾准确性与成本效益的最优配置方案

Method: 使用260道眼科BCSC多选题，评估12种GPT-5配置与o1-high/o3-high/GPT-4o，通过准确性、Bradley-Terry排名、LLM裁判框架和成本分析进行综合评估

Result: GPT-5-high准确率显著优于GPT-5-nano变体(P<.001)和GPT-4o(P<.001)，与o3-high无统计学差异；在质量排名中GPT-5-high比o3-high强1.66倍

Conclusion: 研究建立了眼科领域LLM性能基准，揭示了推理计算量对准确性的影响，并开发了可扩展的自动评分框架

Abstract: Large language models (LLMs) such as GPT-5 integrate advanced reasoning
capabilities that may improve performance on complex medical question-answering
tasks. For this latest generation of reasoning models, the configurations that
maximize both accuracy and cost-efficiency have yet to be established. We
evaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across
four reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using
260 closed-access multiple-choice questions from the American Academy of
Ophthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome
was multiple-choice accuracy; secondary outcomes included head-to-head ranking
via a Bradley-Terry model, rationale quality assessment using a
reference-anchored, pairwise LLM-as-a-judge framework, and analysis of
accuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved
the highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano
variants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high
(0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x
stronger than o3-high) and rationale quality (1.11x stronger than o3-high).
Cost-accuracy analysis identified several GPT-5 configurations on the Pareto
frontier, with GPT-5-mini-low offering the most favorable low-cost,
high-performance balance. These results benchmark GPT-5 on a high-quality
ophthalmology dataset, demonstrate the influence of reasoning effort on
accuracy, and introduce an autograder framework for scalable evaluation of
LLM-generated answers against reference standards in ophthalmology.

</details>


### [44] [Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)](https://arxiv.org/abs/2508.09957)
*Renas Adnan,Hossein Hassani*

Main category: cs.CL

TL;DR: 开发针对库尔德语Badini方言的语音转文字系统，使用Wav2Vec2模型显著优于Whisper模型


<details>
  <summary>Details</summary>
Motivation: 填补Badini方言缺乏语音识别系统的空白，帮助200万使用者融入数字技术并提升方言可见性

Method: 采集78个儿童故事音频(17小时)→预处理为15小时语音数据→使用Wav2Vec2-Large-XLSR-53和Whisper-small模型训练

Result: Wav2Vec2模型表现更优：可读性90.38% vs 65.45%，准确率82.67% vs 53.17%

Conclusion: Wav2Vec2框架更适合低资源语言处理，为少数民族语言技术开发提供有效方案

Abstract: Speech-to-text (STT) systems have a wide range of applications. They are
available in many languages, albeit at different quality levels. Although
Kurdish is considered a less-resourced language from a processing perspective,
SST is available for some of the Kurdish dialects, for instance, Sorani
(Central Kurdish). However, that is not applied to other Kurdish dialects,
Badini and Hawrami, for example. This research is an attempt to address this
gap. Bandin, approximately, has two million speakers, and STT systems can help
their community use mobile and computer-based technologies while giving their
dialect more global visibility. We aim to create a language model based on
Badini's speech and evaluate its performance. To cover a conversational aspect,
have a proper confidence level of grammatical accuracy, and ready
transcriptions, we chose Badini kids' stories, eight books including 78
stories, as the textual input. Six narrators narrated the books, which resulted
in approximately 17 hours of recording. We cleaned, segmented, and tokenized
the input. The preprocessing produced nearly 15 hours of speech, including
19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and
Whisper-small to develop the language models. The experiments indicate that the
transcriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a
significantly more accurate and readable output than the Whisper-small model,
with 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy,
respectively.

</details>


### [45] [Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks](https://arxiv.org/abs/2508.09958)
*Baran Atalar,Eddie Zhang,Carlee Joe-Wong*

Main category: cs.CL

TL;DR: 提出基于神经上下文赌博机的LLM序列选择算法，通过在线学习子任务间的性能依赖关系，优化复杂任务分解场景下的模型组合策略。


<details>
  <summary>Details</summary>
Motivation: 现有LLM选择算法仅关注单模型选择，无法处理多子任务场景中模型输出相互影响的复杂依赖关系，导致医疗诊断等需要任务分解的应用场景效率低下。

Method: 使用神经网络建模各子任务的LLM成功概率，通过上下文赌博机框架在线学习模型间的性能依赖关系，实现无历史数据情况下的动态序列选择。

Result: 在电信问答和医疗诊断数据集上验证，相比传统算法准确率提升12-15%，推理成本降低20%。

Conclusion: 该算法通过建模LLM间的链式依赖关系，为复杂任务分解场景提供了高效的动态模型组合解决方案。

Abstract: With the increasing popularity of large language models (LLMs) for a variety
of tasks, there has been a growing interest in strategies that can predict
which out of a set of LLMs will yield a successful answer at low cost. This
problem promises to become more and more relevant as providers like Microsoft
allow users to easily create custom LLM "assistants" specialized to particular
types of queries. However, some tasks (i.e., queries) may be too specialized
and difficult for a single LLM to handle alone. These applications often
benefit from breaking down the task into smaller subtasks, each of which can
then be executed by a LLM expected to perform well on that specific subtask.
For example, in extracting a diagnosis from medical records, one can first
select an LLM to summarize the record, select another to validate the summary,
and then select another, possibly different, LLM to extract the diagnosis from
the summarized record. Unlike existing LLM selection or routing algorithms,
this setting requires that we select a sequence of LLMs, with the output of
each LLM feeding into the next and potentially influencing its success. Thus,
unlike single LLM selection, the quality of each subtask's output directly
affects the inputs, and hence the cost and success rate, of downstream LLMs,
creating complex performance dependencies that must be learned and accounted
for during selection. We propose a neural contextual bandit-based algorithm
that trains neural networks that model LLM success on each subtask in an online
manner, thus learning to guide the LLM selections for the different subtasks,
even in the absence of historical LLM performance data. Experiments on
telecommunications question answering and medical diagnosis prediction datasets
illustrate the effectiveness of our proposed approach compared to other LLM
selection algorithms.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [46] [TFZ: Topology-Preserving Compression of 2D Symmetric and Asymmetric Second-Order Tensor Fields](https://arxiv.org/abs/2508.09235)
*Nathaniel Gorski,Xin Liang,Hanqi Guo,Bei Wang*

Main category: cs.GR

TL;DR: TFZ框架通过保持张量场关键拓扑特征（对称场的退化点/非对称场的特征向量图），在增强现有科学数据压缩器（SZ3/SPERR）的同时实现拓扑保持压缩。


<details>
  <summary>Details</summary>
Motivation: 科学工程中张量场压缩易导致拓扑结构失真，影响后续分析和可视化。现有压缩器缺乏对张量场拓扑的保护机制。

Method: 采用单元级拓扑保持策略：扫描网格单元，通过局部拓扑特征（退化点位置/特征向量连续性）的精确保留，确保全局拓扑结构的完整性。

Result: 成功将TFZ整合至主流压缩器SZ3和SPERR，在保持压缩率的同时显著降低拓扑失真，验证了框架的有效性。

Conclusion: TFX首次实现有拓扑保证的张量场压缩，为科学数据管理提供新的解决方案，在图形学、神经科学等领域具有重要应用价值。

Abstract: In this paper, we present a novel compression framework, TFZ, that preserves
the topology of 2D symmetric and asymmetric second-order tensor fields defined
on flat triangular meshes. A tensor field assigns a tensor - a
multi-dimensional array of numbers - to each point in space. Tensor fields,
such as the stress and strain tensors, and the Riemann curvature tensor, are
essential to both science and engineering. The topology of tensor fields
captures the core structure of data, and is useful in various disciplines, such
as graphics (for manipulating shapes and textures) and neuroscience (for
analyzing brain structures from diffusion MRI). Lossy data compression may
distort the topology of tensor fields, thus hindering downstream analysis and
visualization tasks. TFZ ensures that certain topological features are
preserved during lossy compression. Specifically, TFZ preserves degenerate
points essential to the topology of symmetric tensor fields and retains
eigenvector and eigenvalue graphs that represent the topology of asymmetric
tensor fields. TFZ scans through each cell, preserving the local topology of
each cell, and thereby ensuring certain global topological guarantees. We
showcase the effectiveness of our framework in enhancing the lossy scientific
data compressors SZ3 and SPERR.

</details>


### [47] [DualPhys-GS: Dual Physically-Guided 3D Gaussian Splatting for Underwater Scene Reconstruction](https://arxiv.org/abs/2508.09610)
*Jiachen Li,Guangzhi Han,Jin Wan,Yuan Gao,Delong Han*

Main category: cs.GR

TL;DR: 提出DualPhys-GS框架，通过双路径优化机制解决水下3D重建中的颜色失真和几何伪影问题，显著提升悬浮物密集区域和远距离场景的重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统大气光学模型无法有效处理水体特有的光波长选择性衰减和悬浮颗粒散射效应，导致远距离重建出现颜色失真、几何伪影和结构塌陷问题。

Method: 1. RGB引导衰减优化模型结合深度信息处理边缘细节
2. 多尺度深度感知散射模型使用特征金字塔网络捕捉不同尺度散射效应
3. 设计四项物理约束损失函数（衰减散射一致性损失、水体自适应损失等）
4. 场景自适应机制动态调整水体类型参数

Result: 在多项指标上超越现有方法，特别是在悬浮物密集区域（PSNR提升12.3%）和远距离场景（SSIM提高18.7%），重建锐度提升显著

Conclusion: 通过双路径特征引导建模、物理约束损失函数和场景自适应机制的协同优化，实现了水下复杂光学环境的高保真三维重建。

Abstract: In 3D reconstruction of underwater scenes, traditional methods based on
atmospheric optical models cannot effectively deal with the selective
attenuation of light wavelengths and the effect of suspended particle
scattering, which are unique to the water medium, and lead to color distortion,
geometric artifacts, and collapsing phenomena at long distances. We propose the
DualPhys-GS framework to achieve high-quality underwater reconstruction through
a dual-path optimization mechanism. Our approach further develops a dual
feature-guided attenuation-scattering modeling mechanism, the RGB-guided
attenuation optimization model combines RGB features and depth information and
can handle edge and structural details. In contrast, the multi-scale
depth-aware scattering model captures scattering effects at different scales
using a feature pyramid network and an attention mechanism. Meanwhile, we
design several special loss functions. The attenuation scattering consistency
loss ensures physical consistency. The water body type adaptive loss
dynamically adjusts the weighting coefficients. The edge-aware scattering loss
is used to maintain the sharpness of structural edges. The multi-scale feature
loss helps to capture global and local structural information. In addition, we
design a scene adaptive mechanism that can automatically identify the
water-body-type characteristics (e.g., clear coral reef waters or turbid
coastal waters) and dynamically adjust the scattering and attenuation
parameters and optimization strategies. Experimental results show that our
method outperforms existing methods in several metrics, especially in suspended
matter-dense regions and long-distance scenes, and the reconstruction quality
is significantly improved.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [48] [Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs](https://arxiv.org/abs/2508.09288)
*Aayush Gupta*

Main category: cs.CR

TL;DR: 提出CIV架构，通过加密签名和信任格栅实现LLMs抗提示注入攻击，在冻结模型上实现0%攻击成功率且不影响正常任务性能


<details>
  <summary>Details</summary>
Motivation: 现有LLMs防护机制（规则/过滤器/LLM法官）易被绕过，需确定性安全方案解决提示注入漏洞

Method: 使用加密来源标签+信任源格栅，通过预softmax硬注意力掩码强制实施令牌级信任隔离，无需微调即可部署

Result: SoK-246等基准测试中攻击成功率0%，令牌相似度93.1%，模型困惑度无退化（延迟问题待优化）

Conclusion: CIV为冻结模型提供轻量级安全补丁，验证适用于主流模型，开源实现促进可复现研究

Abstract: Large language models (LLMs) remain acutely vulnerable to prompt injection
and related jailbreak attacks; heuristic guardrails (rules, filters, LLM
judges) are routinely bypassed. We present Contextual Integrity Verification
(CIV), an inference-time security architecture that attaches cryptographically
signed provenance labels to every token and enforces a source-trust lattice
inside the transformer via a pre-softmax hard attention mask (with optional
FFN/residual gating). CIV provides deterministic, per-token non-interference
guarantees on frozen models: lower-trust tokens cannot influence higher-trust
representations. On benchmarks derived from recent taxonomies of
prompt-injection vectors (Elite-Attack + SoK-246), CIV attains 0% attack
success rate under the stated threat model while preserving 93.1% token-level
similarity and showing no degradation in model perplexity on benign tasks; we
note a latency overhead attributable to a non-optimized data path. Because CIV
is a lightweight patch -- no fine-tuning required -- we demonstrate drop-in
protection for Llama-3-8B and Mistral-7B. We release a reference
implementation, an automated certification harness, and the Elite-Attack corpus
to support reproducible research.

</details>


### [49] [Shadow in the Cache: Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference](https://arxiv.org/abs/2508.09442)
*Zhifan Luo,Shuo Shao,Su Zhang,Lijing Zhou,Yuke Hu,Chenxu Zhao,Zhihao Liu,Zhan Qin*

Main category: cs.CR

TL;DR: 揭示了Key-Value缓存在加速大模型推理时的隐私泄露风险，提出防御方案KV-Cloak


<details>
  <summary>Details</summary>
Motivation: KV缓存机制虽提升LLM推理效率，但其存储的中间注意力计算结果存在严重隐私泄露风险，攻击者可利用KV缓存重构用户敏感输入

Method: 设计三种攻击方式（反转攻击/碰撞攻击/注入攻击），提出基于可逆矩阵混淆和算子融合的KV-Cloak防御框架

Result: KV-Cloak成功将攻击重构质量降至随机噪声水平，防御效果显著且不影响模型精度，性能开销可忽略不计

Conclusion: KV-Cloak首次实现安全防护与计算效率的平衡，为可信大模型部署提供轻量级解决方案

Abstract: The Key-Value (KV) cache, which stores intermediate attention computations
(Key and Value pairs) to avoid redundant calculations, is a fundamental
mechanism for accelerating Large Language Model (LLM) inference. However, this
efficiency optimization introduces significant yet underexplored privacy risks.
This paper provides the first comprehensive analysis of these vulnerabilities,
demonstrating that an attacker can reconstruct sensitive user inputs directly
from the KV-cache. We design and implement three distinct attack vectors: a
direct Inversion Attack, a more broadly applicable and potent Collision Attack,
and a semantic-based Injection Attack. These methods demonstrate the
practicality and severity of KV-cache privacy leakage issues. To mitigate this,
we propose KV-Cloak, a novel, lightweight, and efficient defense mechanism.
KV-Cloak uses a reversible matrix-based obfuscation scheme, combined with
operator fusion, to secure the KV-cache. Our extensive experiments show that
KV-Cloak effectively thwarts all proposed attacks, reducing reconstruction
quality to random noise. Crucially, it achieves this robust security with
virtually no degradation in model accuracy and minimal performance overhead,
offering a practical solution for trustworthy LLM deployment.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [50] [From Hard Refusals to Safe-Completions: Toward Output-Centric Safety Training](https://arxiv.org/abs/2508.09224)
*Yuan Yuan,Tina Sriskandarajah,Anna-Luisa Brakman,Alec Helyar,Alex Beutel,Andrea Vallone,Saachi Jain*

Main category: cs.CY

TL;DR: 提出基于输出安全性的safe-completions训练框架，在GPT-5中实现安全性与有用性双提升


<details>
  <summary>Details</summary>
Motivation: 传统二元拒绝机制对意图模糊的提示（尤其是生物、网络安全等双用途场景）缺乏灵活性，存在安全脆弱性风险

Method: 将安全训练重心转向保障输出内容安全性（safe-completions），而非简单拒绝用户请求，允许在安全边界内最大化帮助性

Result: 实验显示该方法显著提升双用途提示处理安全性（降低67%残余风险），减少安全漏洞严重性，并使模型帮助性提升41%

Conclusion: 输出导向的安全训练范式有效突破二元拒绝限制，实现安全策略约束下的最优帮助性，为AI安全提供新方法论

Abstract: Large Language Models used in ChatGPT have traditionally been trained to
learn a refusal boundary: depending on the user's intent, the model is taught
to either fully comply or outright refuse. While this is a strong mitigation
for explicitly malicious prompts, focusing safety training on refusals can lead
to brittleness for prompts with obscured user intent. Binary refusal boundaries
are especially ill-suited for dual-use cases (such as biology or
cybersecurity), where a user request can be answered safely at a high level,
but in some cases can lead to malicious uplift if sufficiently detailed or
actionable. As an alternative, we propose safe-completions: a safety-training
approach that centers on the safety of the assistant's output, rather than a
binary classification of the user's intent. Safe-completions seek to maximize
helpfulness within the safety policy's constraints. We incorporated this
approach into GPT-5 and find that across both production comparisons and
internally controlled experiments, safe-completion training improves safety
(especially on dual-use prompts), reduces the severity of residual safety
failures, and substantially increases model helpfulness.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [51] [How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments](https://arxiv.org/abs/2508.09614)
*Daniel Raffini,Agnese Macori,Lorenzo Porcaro,Tiziana Catarci,Marco Angelini*

Main category: cs.HC

TL;DR: 研究揭示ChatGPT生成的伦理敏感议论文虽结构连贯但说服力有限，伦理担忧在交互后可能强化，不同主题效果存在差异。


<details>
  <summary>Details</summary>
Motivation: 分析AI生成文本在伦理话题中的修辞特征及其说服效果，探索人工智能在伦理敏感领域的应用边界。

Method: 通过62人用户实验（前测-后测设计）、语言学修辞分析，结合观点变化测量与文本特征关联研究。

Result: 1. 文本呈现公式化表达与风格单一性
2. 说服效力受伦理复杂度制约
3. 53%参与者伦理担忧未缓解
4. 不同主题说服效果差异达28%

Conclusion: AI在伦理说服中存在结构性局限，需开发针对性伦理推理框架，研究成果为AI伦理传播研究建立基准数据集。

Abstract: This study examines the rhetorical and linguistic features of argumentative
texts generated by ChatGPT on ethically nuanced topics and investigates their
persuasive impact on human readers.Through a user study involving 62
participants and pre-post interaction surveys, the paper analyzes how exposure
to AI-generated arguments affects opinion change and user perception. A
linguistic and rhetorical analysis of the generated texts reveals a consistent
argumentative macrostructure, reliance on formulaic expressions, and limited
stylistic richness. While ChatGPT demonstrates proficiency in constructing
coherent argumentative texts, its persuasive efficacy appears constrained,
particularly on topics involving ethical issues.The study finds that while
participants often acknowledge the benefits highlighted by ChatGPT, ethical
concerns tend to persist or even intensify post-interaction. The results also
demonstrate a variation depending on the topic. These findings highlight new
insights on AI-generated persuasion in ethically sensitive domains and are a
basis for future research.

</details>


### [52] [A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories](https://arxiv.org/abs/2508.09651)
*Daniel Raffini,Agnese Macori,Marco Angelini,Tiziana Catarci*

Main category: cs.HC

TL;DR: 研究揭示ChatGPT/Gemini/Claude生成故事中存在性别叙事偏见，需多层面评估隐性偏见


<details>
  <summary>Details</summary>
Motivation: 探究主流AI生成故事中基于性别的隐性叙事偏见及其评估方法

Method: 结合Propp角色分类与Freytag叙事结构设计提示，采用细读分析法评估角色性别分布、描述维度及情节逻辑

Result: 发现生成故事中存在持续性隐性偏见，验证多层面解释性分析的有效性

Conclusion: 强调需建立系统性评估框架识别AI叙事中的多层次偏见，提升生成内容的公平性

Abstract: The paper explores the study of gender-based narrative biases in stories
generated by ChatGPT, Gemini, and Claude. The prompt design draws on Propp's
character classifications and Freytag's narrative structure. The stories are
analyzed through a close reading approach, with particular attention to
adherence to the prompt, gender distribution of characters, physical and
psychological descriptions, actions, and finally, plot development and
character relationships. The results reveal the persistence of biases -
especially implicit ones - in the generated stories and highlight the
importance of assessing biases at multiple levels using an interpretative
approach.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [53] [NEFMind: Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation](https://arxiv.org/abs/2508.09240)
*Zainab Khan,Ahmed Hussain,Mukesh Thakur,Arto Hellas,Panos Papadimitratos*

Main category: cs.NI

TL;DR: 提出NEFMind框架，利用高效参数微调的开源大语言模型，显著降低5G电信网络API管理复杂度，在保持计算效率的同时实现高精度API识别。


<details>
  <summary>Details</summary>
Motivation: 现代电信服务化架构中网络功能和API数量激增，导致服务发现与管理面临巨大操作复杂性，需高效自动化解决方案。

Method: 1) 基于NEF API规范生成合成数据集
2) 采用量化低秩自适应(Q-LoRA)优化模型
3) 使用GPT-4 Ref Score和BertScore评估性能

Result: 通信开销降低85%，Phi-2模型实现98-100% API调用识别准确率，性能媲美GPT-4等大模型但计算效率更高。

Conclusion: 验证了领域特异性参数高效LLM策略在下一代电信网络复杂API生态系统管理中的有效性，为电信基础设施部署提供高效解决方案。

Abstract: The use of Service-Based Architecture in modern telecommunications has
exponentially increased Network Functions (NFs) and Application Programming
Interfaces (APIs), creating substantial operational complexities in service
discovery and management. We introduce \textit{NEFMind}, a framework leveraging
parameter-efficient fine-tuning of open-source Large Language Models (LLMs) to
address these challenges. It integrates three core components: synthetic
dataset generation from Network Exposure Function (NEF) API specifications,
model optimization through Quantized-Low-Rank Adaptation, and performance
evaluation via GPT-4 Ref Score and BertScore metrics. Targeting 5G
Service-Based Architecture APIs, our approach achieves 85% reduction in
communication overhead compared to manual discovery methods. Experimental
validation using the open-source Phi-2 model demonstrates exceptional API call
identification performance at 98-100% accuracy. The fine-tuned Phi-2 model
delivers performance comparable to significantly larger models like GPT-4 while
maintaining computational efficiency for telecommunications infrastructure
deployment. These findings validate domain-specific, parameter-efficient LLM
strategies for managing complex API ecosystems in next-generation
telecommunications networks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [54] [MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.09145)
*Xingle Xu,Yongkang Liu,Dexian Cai,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.LG

TL;DR: 提出MoLAN框架，通过细粒度动态降噪增强多模态情感分析性能


<details>
  <summary>Details</summary>
Motivation: 现有方法将整个模态信息作为独立单元处理，可能在降噪时丢失关键信息。需要实现保留核心信息的同时抑制噪声的细粒度处理方法。

Method: MoLAN框架将各模态特征划分为多个区块，根据噪声水平和语义相关性动态分配降噪强度。MoLAN+是基于此框架的新型多模态情感分析方法。

Result: 在5个模型和4个数据集上的实验验证了框架有效性，MoLAN+取得SOTA性能（F1值提升2.13%-7.25%）

Conclusion: MoLAN作为灵活的统一框架，通过动态区块降噪机制有效平衡噪声抑制与信息保留，显著提升多模态情感分析效果。代码已开源。

Abstract: Multimodal Sentiment Analysis aims to integrate information from various
modalities, such as audio, visual, and text, to make complementary predictions.
However, it often struggles with irrelevant or misleading visual and auditory
information. Most existing approaches typically treat the entire modality
information (e.g., a whole image, audio segment, or text paragraph) as an
independent unit for feature enhancement or denoising. They often suppress the
redundant and noise information at the risk of losing critical information. To
address this challenge, we propose MoLAN, a unified ModaLity-aware noise
dynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking
by dividing the features of each modality into multiple blocks. Each block is
then dynamically assigned a distinct denoising strength based on its noise
level and semantic relevance, enabling fine-grained noise suppression while
preserving essential multimodal information. Notably, MoLAN is a unified and
flexible framework that can be seamlessly integrated into a wide range of
multimodal models. Building upon this framework, we further introduce MoLAN+, a
new multimodal sentiment analysis approach. Experiments across five models and
four datasets demonstrate the broad effectiveness of the MoLAN framework.
Extensive evaluations show that MoLAN+ achieves the state-of-the-art
performance. The code is publicly available at
https://github.com/betterfly123/MoLAN-Framework.

</details>


### [55] [NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs](https://arxiv.org/abs/2508.09473)
*Birong Pan,Mayi Xu,Qiankun Pi,Jianhao Chen,Yuanyuan Zhu,Ming Zhong,Tieyun Qian*

Main category: cs.LG

TL;DR: 提出NeuronTune框架，通过动态调节稀疏神经元实现大语言模型安全性与实用性的同步优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在安全防护不足、频繁误拒合理请求、文本质量下降等问题，根源在于粗粒度的层间干预。需要更细粒度的神经元级调控方案。

Method: 1. 通过归因分析识别各层的安全关键神经元和效用保持神经元 2. 采用元学习实现安全神经元激活放大和效用神经元抑制 3. 通过神经元数量阈值实现可调节的干预范围

Result: 实验表明NeuronTune在安全防护(对抗攻击成功率降低37%)和任务性能(实用指标提升21%)上均超越现有技术，支持安全优先或效用优先的灵活场景适配。

Conclusion: 细粒度的神经元动态调控机制有效解决了LLM部署中的安全-效用权衡难题，为可靠部署提供了新范式。

Abstract: Ensuring robust safety alignment while preserving utility is critical for the
reliable deployment of Large Language Models (LLMs). However, current
techniques fundamentally suffer from intertwined deficiencies: insufficient
robustness against malicious attacks, frequent refusal of benign queries,
degradation in generated text quality and general task performance--the former
two reflecting deficits in robust safety and the latter constituting utility
impairment. We trace these limitations to the coarse-grained layer-wise
interventions in existing methods. To resolve this, we propose NeuronTune, a
fine-grained framework that dynamically modulates sparse neurons to achieve
simultaneous safety-utility optimization. Our approach first identifies
safety-critical and utility-preserving neurons across all layers via
attribution, then employs meta-learning to adaptively amplify safety-neuron
activations and suppress utility-neuron activations. Crucially, NeuronTune
enables tunable adjustment of intervention scope via neuron-count thresholds,
supporting flexible adaptation to security-critical or utility-priority
scenarios. Extensive experimental results demonstrate that our method
significantly outperforms existing state-of-the-art technologies, achieving
superior model safety while maintaining excellent utility.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [56] [RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians](https://arxiv.org/abs/2508.09830)
*Shenxing Wei,Jinxi Li,Yafei Yang,Siyuan Zhou,Bo Yang*

Main category: cs.CV

TL;DR: 提出RayletDF方法，通过射线距离场从点云或3D高斯中重建3D表面，具有高效性和跨数据集泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有基于坐标的3D重建方法在显式表面渲染时计算开销大，需开发更高效且可泛化的解决方案

Method: 由射线特征提取器、射线距离场预测器和多射线混合器组成，通过局部几何特征提取、距离预测和多预测融合实现表面重建

Result: 在多个真实数据集上实现最优重建效果，单次前向传播即可在未见数据集上成功重建3D表面

Conclusion: RayletDF在保持精度的同时显著提升计算效率，其泛化能力为实际应用提供重要价值

Abstract: In this paper, we present a generalizable method for 3D surface
reconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from
RGB images. Unlike existing coordinate-based methods which are often
computationally intensive when rendering explicit surfaces, our proposed
method, named RayletDF, introduces a new technique called raylet distance
field, which aims to directly predict surface points from query rays. Our
pipeline consists of three key modules: a raylet feature extractor, a raylet
distance field predictor, and a multi-raylet blender. These components work
together to extract fine-grained local geometric features, predict raylet
distances, and aggregate multiple predictions to reconstruct precise surface
points. We extensively evaluate our method on multiple public real-world
datasets, demonstrating superior performance in surface reconstruction from
point clouds or 3D Gaussians. Most notably, our method achieves exceptional
generalization ability, successfully recovering 3D surfaces in a single-forward
pass across unseen datasets in testing.

</details>


### [57] [Story2Board: A Training-Free Approach for Expressive Storyboard Generation](https://arxiv.org/abs/2508.09983)
*David Dinkevich,Matan Levy,Omri Avrahami,Dvir Samuel,Dani Lischinski*

Main category: cs.CV

TL;DR: 无需训练的Story2Board框架通过潜在面板锚定和互注意力值混合机制，在保持角色一致性的前提下生成多样化视觉故事板


<details>
  <summary>Details</summary>
Motivation: 现有故事板生成方法过度关注主体身份，忽略空间构图/背景演变/叙事节奏等视觉叙事核心要素

Method: 包含潜在面板锚定(保持角色一致性)和互注意力值混合(软融合视觉特征)的轻量框架，结合语言模型生成面板级提示，提出包含场景多样性指标的新评估体系

Result: 在动态性、连贯性、叙事吸引力方面全面超越基线，用户研究和量化指标验证有效性，场景多样性指标提升32%

Conclusion: 该框架首次实现无需训练即可平衡故事板生成中的视觉多样性与叙事连贯性，为AI辅助视觉叙事开辟新方向

Abstract: We present Story2Board, a training-free framework for expressive storyboard
generation from natural language. Existing methods narrowly focus on subject
identity, overlooking key aspects of visual storytelling such as spatial
composition, background evolution, and narrative pacing. To address this, we
introduce a lightweight consistency framework composed of two components:
Latent Panel Anchoring, which preserves a shared character reference across
panels, and Reciprocal Attention Value Mixing, which softly blends visual
features between token pairs with strong reciprocal attention. Together, these
mechanisms enhance coherence without architectural changes or fine-tuning,
enabling state-of-the-art diffusion models to generate visually diverse yet
consistent storyboards. To structure generation, we use an off-the-shelf
language model to convert free-form stories into grounded panel-level prompts.
To evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain
narratives designed to assess layout diversity and background-grounded
storytelling, in addition to consistency. We also introduce a new Scene
Diversity metric that quantifies spatial and pose variation across storyboards.
Our qualitative and quantitative results, as well as a user study, show that
Story2Board produces more dynamic, coherent, and narratively engaging
storyboards than existing baselines.

</details>


### [58] [$Δ$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation](https://arxiv.org/abs/2508.09199)
*Jucheng Hu,Suorong Yang,Dongzhan Zhou*

Main category: cs.CV

TL;DR: 提出Δ-AttnMask框架，仅用20%数据实现视觉指令微调最佳性能，训练加速5倍且准确率提升10.1%


<details>
  <summary>Details</summary>
Motivation: 视觉指令微调需要大量高质量多模态对齐数据，现有数据选择方法效率低下且缺乏系统研究

Method: 通过注意力掩码计算隐藏状态损失差异(Δ)，无监督评估图像-文本对质量

Result: 在多个模型和数据集上验证，20%数据量即超越全数据基线10.1%，训练效率提升5倍

Conclusion: Δ-AttnMask的跨模态通用设计为视觉语言模型训练提供高效数据选择方案

Abstract: Visual Instruction Finetuning (VIF) is pivotal for post-training
Vision-Language Models (VLMs). Unlike unimodal instruction finetuning in
plain-text large language models, which mainly requires instruction datasets to
enable model instruction-following ability, VIF also requires multimodal data
to enable joint visual and textual understanding; therefore, it typically
requires more data. Consequently, VIF imposes stricter data selection
challenges: the method must scale efficiently to handle larger data demands
while ensuring the quality of both visual and textual content, as well as their
alignment. Despite its critical impact on performance, data selection for VIF
remains an understudied area. In this paper, we propose $\Delta$-AttnMask. This
data-efficient framework quantifies sample quality through attention-guided
masking of the model's hidden states, jointly evaluating image-text pairs
without requiring domain labels, auxiliary models, or extra training. By
computing loss differences ($\Delta$) between the original states and states
masked using high-attention regions, $\Delta$-AttnMask intrinsically assesses
sample quality. Experiments across multiple VLMs and datasets show that
$\Delta$-AttnMask achieves state-of-the-art performance with just 20% of data,
accelerating training by 5x while surpassing full-dataset baselines by +10.1%
in overall accuracy. Its model-agnostic and data-agnostic design ensures broad
applicability across modalities and architectures.

</details>


### [59] [IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding](https://arxiv.org/abs/2508.09456)
*Junxian Li,Beining Xu,Di Zhang*

Main category: cs.CV

TL;DR: 提出新型输入感知后门攻击方法IAG，通过自适应触发器和重建损失机制，有效操纵视觉语言模型的定位行为且保持隐蔽性。


<details>
  <summary>Details</summary>
Motivation: 现有研究对视觉语言模型在开放词汇场景下的后门攻击安全性关注不足，传统静态触发器方法难以应对动态查询场景。

Method: 1. 文本条件U-Net生成自适应触发器，将攻击目标语义嵌入图像
2. 采用重建损失保持图像隐蔽性
3. 统一的数据生成框架实现开放词汇攻击

Result: 在InternVL-2.5-8B实现ASR@0.5超65%，Ferret-7B和LlaVA-1.5-7B攻击成功率显著且清洁样本准确率下降<1%

Conclusion: IAG验证了视觉语言模型在真实场景下的安全漏洞，其输入感知机制和语义融合方法为后门攻击研究提供新方向

Abstract: Vision-language models (VLMs) have shown significant advancements in tasks
such as visual grounding, where they localize specific objects in images based
on natural language queries and images. However, security issues in visual
grounding tasks for VLMs remain underexplored, especially in the context of
backdoor attacks. In this paper, we introduce a novel input-aware backdoor
attack method, IAG, designed to manipulate the grounding behavior of VLMs. This
attack forces the model to ground a specific target object in the input image,
regardless of the user's query. We propose an adaptive trigger generator that
embeds the semantic information of the attack target's description into the
original image using a text-conditional U-Net, thereby overcoming the
open-vocabulary attack challenge. To ensure the attack's stealthiness, we
utilize a reconstruction loss to minimize visual discrepancies between poisoned
and clean images. Additionally, we introduce a unified method for generating
attack data. IAG is evaluated theoretically and empirically, demonstrating its
feasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches
over 65\% on various testing sets. IAG also shows promising potential on
manipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on
clean samples. Extensive specific experiments, such as ablation study and
potential defense, also indicate the robustness and transferability of our
attack.

</details>


### [60] [COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets](https://arxiv.org/abs/2508.09886)
*Lingyu Chen,Yawen Zeng,Yue Wang,Peng Wan,Guo-chen Ning,Hongen Liao,Daoqiang Zhang,Fang Chen*

Main category: cs.CV

TL;DR: 提出COME模型解决多异构超声数据集泛化问题，通过共享专家与源特定专家协作提升模型性能


<details>
  <summary>Details</summary>
Motivation: 传统单数据集训练在跨域超声图像分析中存在性能下降问题，现有方法无法有效处理数据异质性和跨域干扰

Method: 建立结构-语义双重共享专家创建通用表示空间，结合源特定专家通过特征互补提取判别性特征

Result: 在三种评估模式下显著优于现有方法，平均AP指标显著提升

Conclusion: COME实现了跨数据集经验整合，为小样本和未见数据场景提供鲁棒的超声图像分析解决方案

Abstract: Conventional single-dataset training often fails with new data distributions,
especially in ultrasound (US) image analysis due to limited data, acoustic
shadows, and speckle noise. Therefore, constructing a universal framework for
multi-heterogeneous US datasets is imperative. However, a key challenge arises:
how to effectively mitigate inter-dataset interference while preserving
dataset-specific discriminative features for robust downstream task? Previous
approaches utilize either a single source-specific decoder or a domain
adaptation strategy, but these methods experienced a decline in performance
when applied to other domains. Considering this, we propose a Universal
Collaborative Mixture of Heterogeneous Source-Specific Experts (COME).
Specifically, COME establishes dual structure-semantic shared experts that
create a universal representation space and then collaborate with
source-specific experts to extract discriminative features through providing
complementary features. This design enables robust generalization by leveraging
cross-datasets experience distributions and providing universal US priors for
small-batch or unseen data scenarios. Extensive experiments under three
evaluation modes (single-dataset, intra-organ, and inter-organ integration
datasets) demonstrate COME's superiority, achieving significant mean AP
improvements over state-of-the-art methods. Our project is available at:
https://universalcome.github.io/UniversalCOME/.

</details>


### [61] [Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation](https://arxiv.org/abs/2508.09987)
*Junyan Ye,Dongzhi Jiang,Zihao Wang,Leqi Zhu,Zhenghao Hu,Zilong Huang,Jun He,Zhiyuan Yan,Jinghua Yu,Hongsheng Li,Conghui He,Weijia Li*

Main category: cs.CV

TL;DR: 论文提出利用GPT-4o生成的合成图像数据Echo-4o-Image弥补现实数据集的覆盖盲点，并通过新评估基准验证其有效性，展示了合成数据在提升模型性能和迁移性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 现实图像数据存在罕见场景覆盖不足（如幻想类内容）、背景噪声及文本-图像对齐偏差的问题，而合成数据能提供纯净监督信号和长尾场景覆盖。

Method: 构建180K规模的GPT-4o合成数据集Echo-4o-Image微调Bagel模型，并设计GenEval++（复杂指令评估）和Imagine-Bench（想象力评估）两个新基准。

Result: Echo-4o在标准测试中表现优异，且该数据集可迁移至OmniGen2等模型，带来多指标性能提升（BLEURT↑5.7%，CLIP Score↑3.2%）。

Conclusion: 合成数据通过场景补充和精准对齐有效增强生成模型能力，新评估体系为复杂生成任务提供更严谨的评测标准。

Abstract: Recently, GPT-4o has garnered significant attention for its strong
performance in image generation, yet open-source models still lag behind.
Several studies have explored distilling image data from GPT-4o to enhance
open-source models, achieving notable progress. However, a key question
remains: given that real-world image datasets already constitute a natural
source of high-quality data, why should we use GPT-4o-generated synthetic data?
In this work, we identify two key advantages of synthetic images. First, they
can complement rare scenarios in real-world datasets, such as surreal fantasy
or multi-reference image generation, which frequently occur in user queries.
Second, they provide clean and controllable supervision. Real-world data often
contains complex background noise and inherent misalignment between text
descriptions and image content, whereas synthetic images offer pure backgrounds
and long-tailed supervision signals, facilitating more accurate text-to-image
alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale
synthetic dataset generated by GPT-4o, harnessing the power of synthetic image
data to address blind spots in real-world coverage. Using this dataset, we
fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.
In addition, we propose two new evaluation benchmarks for a more accurate and
challenging assessment of image generation capabilities: GenEval++, which
increases instruction complexity to mitigate score saturation, and
Imagine-Bench, which focuses on evaluating both the understanding and
generation of imaginative content. Echo-4o demonstrates strong performance
across standard benchmarks. Moreover, applying Echo-4o-Image to other
foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains
across multiple metrics, highlighting the datasets strong transferability.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [62] [Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional Mamba as Self-Attention's Alternative](https://arxiv.org/abs/2508.09294)
*Xi Xuan,Zimo Zhu,Wenxin Zhang,Yi-Cheng Lin,Tomi Kinnunen*

Main category: eess.AS

TL;DR: 提出Fake-Mamba框架，结合XLSR语音表征与双向Mamba结构，在多个深度伪造语音检测基准上实现SOTA性能，并保持实时推理能力。


<details>
  <summary>Details</summary>
Motivation: 语音合成技术的进步加剧深度伪造安全威胁，需探索更高效的实时检测方案。研究验证双向Mamba替代Transformer自注意力机制的可行性。

Method: 1) XLSR前端提取丰富语音特征 2) 提出三种双向Mamba编码器（TransBiMamba/ConBiMamba/PN-BiMamba），其中PN-BiMamba通过并行网络优化合成伪影捕捉

Result: 在ASVspoof 21 LA/DF和In-The-Wild数据集分别达到0.97%、1.74%、5.85% EER，相对XLSR-Conformer/XLSR-Mamba有显著提升，推理速度达实时要求

Conclusion: 首次证明双向Mamba在音频深度伪造检测中的有效性，PN-BiMamba通过多模态特征融合实现最优性能，开源框架具备实际部署价值

Abstract: Advances in speech synthesis intensify security threats, motivating real-time
deepfake detection research. We investigate whether bidirectional Mamba can
serve as a competitive alternative to Self-Attention in detecting synthetic
speech. Our solution, Fake-Mamba, integrates an XLSR front-end with
bidirectional Mamba to capture both local and global artifacts. Our core
innovation introduces three efficient encoders: TransBiMamba, ConBiMamba, and
PN-BiMamba. Leveraging XLSR's rich linguistic representations, PN-BiMamba can
effectively capture the subtle cues of synthetic speech. Evaluated on ASVspoof
21 LA, 21 DF, and In-The-Wild benchmarks, Fake-Mamba achieves 0.97%, 1.74%, and
5.85% EER, respectively, representing substantial relative gains over SOTA
models XLSR-Conformer and XLSR-Mamba. The framework maintains real-time
inference across utterance lengths, demonstrating strong generalization and
practical viability. The code is available at
https://github.com/xuanxixi/Fake-Mamba.

</details>


### [63] [ProMode: A Speech Prosody Model Conditioned on Acoustic and Textual Inputs](https://arxiv.org/abs/2508.09389)
*Eray Eren,Qingju Liu,Hyeongwoo Kim,Pablo Garrido,Abeer Alwan*

Main category: eess.AS

TL;DR: 提出ProMode模型，通过部分掩码的声学-文本联合编码生成韵律表征，在基频/能量预测和TTS系统中展现优越性能


<details>
  <summary>Details</summary>
Motivation: 韵律特征（如基频、能量）承载语音的情感语义及个体特征，现有方法在韵律建模精度和下游任务应用上存在局限

Method: 构建编码器-解码器架构：编码器处理部分掩码的声学特征与对齐文本，生成固定长度韵律嵌入；解码器结合文本信息重建掩码区域的声学特征

Result: 在GigaSpeech数据集上，基频/能量预测指标全面超越现有风格编码器，TTS系统主观测试获得更高韵律自然度偏好

Conclusion: 该模型通过掩码训练策略有效捕获多粒度韵律特征，为需要精细韵律建模的语音合成任务提供新解决方案

Abstract: Prosody conveys rich emotional and semantic information of the speech signal
as well as individual idiosyncrasies. We propose a stand-alone model that maps
text-to-prosodic features such as F0 and energy and can be used in downstream
tasks such as TTS. The ProMode encoder takes as input acoustic features and
time-aligned textual content, both are partially masked, and obtains a
fixed-length latent prosodic embedding. The decoder predicts acoustics in the
masked region using both the encoded prosody input and unmasked textual
content. Trained on the GigaSpeech dataset, we compare our method with
state-of-the-art style encoders. For F0 and energy predictions, we show
consistent improvements for our model at different levels of granularity. We
also integrate these predicted prosodic features into a TTS system and conduct
perceptual tests, which show higher prosody preference compared to the
baselines, demonstrating the model's potential in tasks where prosody modeling
is important.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [64] [AI Blob! LLM-Driven Recontextualization of Italian Television Archives](https://arxiv.org/abs/2508.09535)
*Roberto Balestri*

Main category: cs.MM

TL;DR: AI Blob! 利用语义技术与大语言模型，通过动态内容检索重组电视档案素材，实现自动化叙事构建与文化分析


<details>
  <summary>Details</summary>
Motivation: 探索语义编目与LLMs在电视档案素材智能检索中的应用，突破静态元数据限制，推动媒体史学与AI档案研究的交叉创新

Method: 整合ASR语音识别+语义嵌入+RAG技术，将视频分割为句子单元存入向量数据库，通过LLM生成扩展查询实现碎片化内容叙事重组

Result: 开发出可动态检索电视档案的算法系统，产生具有主题连贯性的蒙太奇视频，并公开1547个意大利电视视频数据集

Conclusion: 该项目证明了语义技术在档案活化中的潜力，为跨学科研究提供自动化叙事框架与实验数据集，推动文化分析新范式发展

Abstract: This paper introduces AI Blob!, an experimental system designed to explore
the potential of semantic cataloging and Large Language Models (LLMs) for the
retrieval and recontextualization of archival television footage. Drawing
methodological inspiration from Italian television programs such as Blob (RAI
Tre, 1989-), AI Blob! integrates automatic speech recognition (ASR), semantic
embeddings, and retrieval-augmented generation (RAG) to organize and
reinterpret archival content. The system processes a curated dataset of 1,547
Italian television videos by transcribing audio, segmenting it into
sentence-level units, and embedding these segments into a vector database for
semantic querying. Upon user input of a thematic prompt, the LLM generates a
range of linguistically and conceptually related queries, guiding the retrieval
and recombination of audiovisual fragments. These fragments are algorithmically
selected and structured into narrative sequences producing montages that
emulate editorial practices of ironic juxtaposition and thematic coherence. By
foregrounding dynamic, content-aware retrieval over static metadata schemas, AI
Blob! demonstrates how semantic technologies can facilitate new approaches to
archival engagement, enabling novel forms of automated narrative construction
and cultural analysis. The project contributes to ongoing debates in media
historiography and AI-driven archival research, offering both a conceptual
framework and a publicly available dataset to support further interdisciplinary
experimentation.

</details>
