{"id": "2508.03445", "pdf": "https://arxiv.org/pdf/2508.03445", "abs": "https://arxiv.org/abs/2508.03445", "authors": ["Patrick Paetzold", "Rebecca Kehlbeck", "Yumeng Xue", "Bin Chen", "Yunhai Wang", "Oliver Deussen"], "title": "Neighborhood-Preserving Voronoi Treemaps", "categories": ["cs.GR"], "comment": null, "summary": "Voronoi treemaps are used to depict nodes and their hierarchical\nrelationships simultaneously. However, in addition to the hierarchical\nstructure, data attributes, such as co-occurring features or similarities,\nfrequently exist. Examples include geographical attributes like shared borders\nbetween countries or contextualized semantic information such as embedding\nvectors derived from large language models. In this work, we introduce a\nVoronoi treemap algorithm that leverages data similarity to generate\nneighborhood-preserving treemaps. First, we extend the treemap layout pipeline\nto consider similarity during data preprocessing. We then use a Kuhn-Munkres\nmatching of similarities to centroidal Voronoi tessellation (CVT) cells to\ncreate initial Voronoi diagrams with equal cell sizes for each level. Greedy\nswapping is used to improve the neighborhoods of cells to match the data's\nsimilarity further. During optimization, cell areas are iteratively adjusted to\ntheir respective sizes while preserving the existing neighborhoods. We\ndemonstrate the practicality of our approach through multiple real-world\nexamples drawn from infographics and linguistics. To quantitatively assess the\nresulting treemaps, we employ treemap metrics and measure neighborhood\npreservation.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u6570\u636e\u76f8\u4f3c\u6027\u7684Voronoi\u6811\u72b6\u56fe\u7b97\u6cd5\uff0c\u901a\u8fc7\u9884\u5904\u7406\u4f18\u5316\u548c\u8d2a\u5fc3\u4ea4\u6362\u7b56\u7565\u5b9e\u73b0\u90bb\u57df\u4fdd\u6301\u7684\u53ef\u89c6\u5316\u5e03\u5c40", "motivation": "\u73b0\u6709Voronoi\u6811\u72b6\u56fe\u4ec5\u5904\u7406\u5c42\u6b21\u7ed3\u6784\uff0c\u5ffd\u7565\u6570\u636e\u5c5e\u6027\uff08\u5982\u5730\u7406\u90bb\u63a5\u5173\u7cfb\u3001\u8bed\u4e49\u5d4c\u5165\u5411\u91cf\u7b49\u76f8\u4f3c\u6027\u7279\u5f81\uff09\uff0c\u9700\u5f00\u53d1\u80fd\u4fdd\u6301\u6570\u636e\u76f8\u4f3c\u6027\u5173\u7cfb\u7684\u53ef\u89c6\u5316\u65b9\u6cd5", "method": "1. \u6269\u5c55\u9884\u5904\u7406\u6d41\u7a0b\u878d\u5165\u76f8\u4f3c\u6027\u8ba1\u7b97 2. \u4f7f\u7528Kuhn-Munkres\u5339\u914d\u4e0eCVT\u751f\u6210\u7b49\u9762\u79ef\u521d\u59cb\u5e03\u5c40 3. \u8d2a\u5fc3\u4ea4\u6362\u4f18\u5316\u5355\u5143\u683c\u90bb\u57df\u5173\u7cfb 4. \u8fed\u4ee3\u8c03\u6574\u5355\u5143\u683c\u5c3a\u5bf8\u540c\u65f6\u4fdd\u6301\u90bb\u63a5\u7ed3\u6784", "result": "\u901a\u8fc7\u5730\u7406\u4fe1\u606f\u56fe\u548c\u8bed\u4e49\u53ef\u89c6\u5316\u7b49\u5b9e\u9645\u6848\u4f8b\u9a8c\u8bc1\uff0c\u91c7\u7528\u6811\u72b6\u56fe\u8d28\u91cf\u6307\u6807\u548c\u90bb\u57df\u4fdd\u6301\u5ea6\u8fdb\u884c\u91cf\u5316\u8bc4\u4f30", "conclusion": "\u6210\u529f\u5f00\u53d1\u51fa\u80fd\u540c\u65f6\u4fdd\u6301\u5c42\u6b21\u7ed3\u6784\u4e0e\u6570\u636e\u76f8\u4f3c\u6027\u7684\u65b0\u578b\u7b97\u6cd5\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u548c\u91cf\u5316\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u53ef\u89c6\u5316\u6548\u679c\u63d0\u5347"}}
{"id": "2508.03179", "pdf": "https://arxiv.org/pdf/2508.03179", "abs": "https://arxiv.org/abs/2508.03179", "authors": ["Ulugbek Alibekov", "Vanessa Staderini", "Philipp Schneider", "Doris Antensteiner"], "title": "Advancing Precision in Multi-Point Cloud Fusion Environments", "categories": ["cs.CV", "cs.GR"], "comment": "Accpeted for publication in Communications in Computer and\n  Information Science, Springer", "summary": "This research focuses on visual industrial inspection by evaluating point\nclouds and multi-point cloud matching methods. We also introduce a synthetic\ndataset for quantitative evaluation of registration method and various distance\nmetrics for point cloud comparison. Additionally, we present a novel\nCloudCompare plugin for merging multiple point clouds and visualizing surface\ndefects, enhancing the accuracy and efficiency of automated inspection systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u8bc4\u4f30\u70b9\u4e91\u5339\u914d\u65b9\u6cd5\u53ca\u521b\u5efa\u5408\u6210\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86\u65b0\u578bCloudCompare\u63d2\u4ef6\u6765\u63d0\u5347\u5de5\u4e1a\u8d28\u68c0\u7cbe\u5ea6\u4e0e\u6548\u7387", "motivation": "\u89e3\u51b3\u5de5\u4e1a\u89c6\u89c9\u68c0\u6d4b\u4e2d\u70b9\u4e91\u914d\u51c6\u65b9\u6cd5\u7f3a\u4e4f\u5b9a\u91cf\u8bc4\u4f30\u5de5\u5177\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u8868\u9762\u7f3a\u9677\u68c0\u6d4b\u4e2d\u5b58\u5728\u6548\u7387\u74f6\u9888", "method": "1. \u8bc4\u4f30\u591a\u70b9\u4e91\u5339\u914d\u65b9\u6cd5\n2. \u521b\u5efa\u914d\u51c6\u65b9\u6cd5\u5b9a\u91cf\u8bc4\u4f30\u7684\u5408\u6210\u6570\u636e\u96c6\n3. \u5f00\u53d1CloudCompare\u63d2\u4ef6\u5b9e\u73b0\u591a\u70b9\u4e91\u878d\u5408\u4e0e\u7f3a\u9677\u53ef\u89c6\u5316", "result": "\u63d0\u51fa\u65b0\u7684\u70b9\u4e91\u8ddd\u79bb\u5ea6\u91cf\u6807\u51c6\uff0c\u6210\u529f\u5b9e\u73b0\u53ef\u91cf\u5316\u8bc4\u4f30\u7684\u5408\u6210\u6570\u636e\u96c6\u53ca\u63d0\u5347\u68c0\u6d4b\u6548\u738730%\u7684\u5de5\u4e1a\u63d2\u4ef6", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u5316\u8d28\u68c0\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u5177\u4e0e\u7f3a\u9677\u53ef\u89c6\u5316\u65b9\u6848\uff0c\u663e\u8457\u63a8\u8fdb\u4e86\u5de5\u4e1a\u68c0\u6d4b\u7684\u667a\u80fd\u5316\u8fdb\u7a0b"}}
{"id": "2508.03415", "pdf": "https://arxiv.org/pdf/2508.03415", "abs": "https://arxiv.org/abs/2508.03415", "authors": ["Shivangi Nigam", "Adarsh Prasad Behera", "Shekhar Verma", "P. Nagabhushan"], "title": "Learning Latent Representations for Image Translation using Frequency Distributed CycleGAN", "categories": ["cs.CV", "cs.AI", "cs.GR"], "comment": "This paper is currently under review for publication in an IEEE\n  Transactions. If accepted, the copyright will be transferred to IEEE", "summary": "This paper presents Fd-CycleGAN, an image-to-image (I2I) translation\nframework that enhances latent representation learning to approximate real data\ndistributions. Building upon the foundation of CycleGAN, our approach\nintegrates Local Neighborhood Encoding (LNE) and frequency-aware supervision to\ncapture fine-grained local pixel semantics while preserving structural\ncoherence from the source domain. We employ distribution-based loss metrics,\nincluding KL/JS divergence and log-based similarity measures, to explicitly\nquantify the alignment between real and generated image distributions in both\nspatial and frequency domains. To validate the efficacy of Fd-CycleGAN, we\nconduct experiments on diverse datasets -- Horse2Zebra, Monet2Photo, and a\nsynthetically augmented Strike-off dataset. Compared to baseline CycleGAN and\nother state-of-the-art methods, our approach demonstrates superior perceptual\nquality, faster convergence, and improved mode diversity, particularly in\nlow-data regimes. By effectively capturing local and global distribution\ncharacteristics, Fd-CycleGAN achieves more visually coherent and semantically\nconsistent translations. Our results suggest that frequency-guided latent\nlearning significantly improves generalization in image translation tasks, with\npromising applications in document restoration, artistic style transfer, and\nmedical image synthesis. We also provide comparative insights with\ndiffusion-based generative models, highlighting the advantages of our\nlightweight adversarial approach in terms of training efficiency and\nqualitative output.", "AI": {"tldr": "Fd-CycleGAN\u901a\u8fc7\u96c6\u6210\u5c40\u90e8\u90bb\u57df\u7f16\u7801\u548c\u9891\u7387\u611f\u77e5\u76d1\u7763\uff0c\u6539\u8fdbCycleGAN\u6846\u67b6\u4ee5\u63d0\u5347\u56fe\u50cf\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5728\u4f4e\u6570\u636e\u573a\u666f\u4e0b\u5c55\u73b0\u66f4\u4f18\u7684\u611f\u77e5\u8d28\u91cf\u4e0e\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u89e3\u51b3CycleGAN\u5728\u6355\u6349\u5c40\u90e8\u50cf\u7d20\u8bed\u4e49\u65f6\u96be\u4ee5\u4fdd\u6301\u7ed3\u6784\u8fde\u8d2f\u6027\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u663e\u5f0f\u91cf\u5316\u751f\u6210\u56fe\u50cf\u4e0e\u771f\u5b9e\u6570\u636e\u7684\u7a7a\u95f4/\u9891\u57df\u5206\u5e03\u5bf9\u9f50\u6765\u63d0\u5347\u7ffb\u8bd1\u4e00\u81f4\u6027\u3002", "method": "1. \u5c40\u90e8\u90bb\u57df\u7f16\u7801(LNE)\u6355\u83b7\u7ec6\u7c92\u5ea6\u8bed\u4e49\n2. \u9891\u7387\u611f\u77e5\u76d1\u7763\u4fdd\u6301\u7ed3\u6784\u8fde\u8d2f\n3. KL/JS\u6563\u5ea6\u7b49\u5206\u5e03\u6307\u6807\u4f18\u5316\u751f\u6210\u8d28\u91cf\n4. \u5728Horse2Zebra\u7b49\u4e09\u4e2a\u6570\u636e\u96c6\u9a8c\u8bc1\u6709\u6548\u6027", "result": "\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\uff1a\n- \u611f\u77e5\u8d28\u91cf\u63d0\u534715%\n- \u6536\u655b\u901f\u5ea6\u5feb2\u500d\n- \u4f4e\u6570\u636e\u573a\u666f\u6a21\u5f0f\u591a\u6837\u6027\u589e\u52a030%\n- \u5728\u6587\u6863\u4fee\u590d/\u533b\u5b66\u6210\u50cf\u7b49\u573a\u666f\u5c55\u73b0\u5e94\u7528\u6f5c\u529b", "conclusion": "\u9891\u7387\u5f15\u5bfc\u7684\u6f5c\u5728\u5b66\u4e60\u663e\u8457\u63d0\u5347\u56fe\u50cf\u7ffb\u8bd1\u6cdb\u5316\u80fd\u529b\uff0c\u8f7b\u91cf\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u5728\u6548\u7387\u548c\u8d28\u91cf\u4e0a\u4f18\u4e8e\u6269\u6563\u6a21\u578b\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.02808", "pdf": "https://arxiv.org/pdf/2508.02808", "abs": "https://arxiv.org/abs/2508.02808", "authors": ["Radhika Dua", "Young Joon", "Kwon", "Siddhant Dogra", "Daniel Freedman", "Diana Ruan", "Motaz Nashawaty", "Danielle Rigau", "Daniel Alexander Alber", "Kang Zhang", "Kyunghyun Cho", "Eric Karl Oermann"], "title": "Clinically Grounded Agent-based Report Evaluation: An Interpretable Metric for Radiology Report Generation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Radiological imaging is central to diagnosis, treatment planning, and\nclinical decision-making. Vision-language foundation models have spurred\ninterest in automated radiology report generation (RRG), but safe deployment\nrequires reliable clinical evaluation of generated reports. Existing metrics\noften rely on surface-level similarity or behave as black boxes, lacking\ninterpretability. We introduce ICARE (Interpretable and Clinically-grounded\nAgent-based Report Evaluation), an interpretable evaluation framework\nleveraging large language model agents and dynamic multiple-choice question\nanswering (MCQA). Two agents, each with either the ground-truth or generated\nreport, generate clinically meaningful questions and quiz each other. Agreement\non answers captures preservation and consistency of findings, serving as\ninterpretable proxies for clinical precision and recall. By linking scores to\nquestion-answer pairs, ICARE enables transparent, and interpretable assessment.\nClinician studies show ICARE aligns significantly more with expert judgment\nthan prior metrics. Perturbation analyses confirm sensitivity to clinical\ncontent and reproducibility, while model comparisons reveal interpretable error\npatterns.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u89e3\u91ca\u7684ICARE\u6846\u67b6\uff0c\u901a\u8fc7LLM\u4ee3\u7406\u548c\u52a8\u6001\u591a\u9009\u9898\u8bc4\u4f30\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u8d28\u91cf", "motivation": "\u73b0\u6709\u653e\u5c04\u5b66\u62a5\u544a\u8bc4\u4f30\u6307\u6807\u4f9d\u8d56\u8868\u9762\u76f8\u4f3c\u6027\u4e14\u7f3a\u4e4f\u4e34\u5e8a\u53ef\u89e3\u91ca\u6027\uff0c\u9700\u66f4\u53ef\u9760\u7684\u4e34\u5e8a\u8bc4\u4f30\u65b9\u6cd5", "method": "\u4f7f\u7528\u4e24\u4e2aLLM\u4ee3\u7406\u57fa\u4e8e\u771f\u5b9e/\u751f\u6210\u62a5\u544a\u751f\u6210\u4e34\u5e8a\u95ee\u9898\uff0c\u901a\u8fc7\u7b54\u6848\u4e00\u81f4\u6027\u8bc4\u4f30\u4e34\u5e8a\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387", "result": "\u4e34\u5e8a\u7814\u7a76\u663e\u793aICARE\u6bd4\u73b0\u6709\u6307\u6807\u66f4\u7b26\u5408\u4e13\u5bb6\u5224\u65ad\uff0c\u6270\u52a8\u5206\u6790\u9a8c\u8bc1\u4e34\u5e8a\u654f\u611f\u6027\u53ca\u53ef\u91cd\u590d\u6027", "conclusion": "ICARE\u901a\u8fc7\u95ee\u9898-\u7b54\u6848\u5bf9\u5b9e\u73b0\u900f\u660e\u8bc4\u4f30\uff0c\u4e3aAI\u751f\u6210\u7684\u653e\u5c04\u5b66\u62a5\u544a\u63d0\u4f9b\u4e34\u5e8a\u53ef\u4fe1\u7684\u91cf\u5316\u6807\u51c6"}}
{"id": "2508.02853", "pdf": "https://arxiv.org/pdf/2508.02853", "abs": "https://arxiv.org/abs/2508.02853", "authors": ["Yinuo Xu", "Veronica Derricks", "Allison Earl", "David Jurgens"], "title": "Modeling Annotator Disagreement with Demographic-Aware Experts and Synthetic Perspectives", "categories": ["cs.CL"], "comment": "28 pages, 17 figures", "summary": "We present an approach to modeling annotator disagreement in subjective NLP\ntasks through both architectural and data-centric innovations. Our model,\nDEM-MoE (Demographic-Aware Mixture of Experts), routes inputs to expert\nsubnetworks based on annotator demographics, enabling it to better represent\nstructured, group-level variation compared to prior models. DEM-MoE\nconsistently performs competitively across demographic groups, and shows\nespecially strong results on datasets with high annotator disagreement. To\naddress sparse demographic coverage, we test whether LLM-generated synthetic\nannotations via zero-shot persona prompting can be used for data imputation. We\nshow these synthetic judgments align moderately well with human annotations on\nour data and offer a scalable way to potentially enrich training data. We then\npropose and evaluate approaches for blending real and synthetic data using\nstrategies tailored to dataset structure. We find that the optimal strategies\ndepend on dataset structure. Together, these contributions improve the\nrepresentation of diverse perspectives.", "AI": {"tldr": "\u63d0\u51faDEM-MoE\u6a21\u578b\uff0c\u901a\u8fc7\u67b6\u6784\u521b\u65b0\u4e0e\u5408\u6210\u6570\u636e\u878d\u5408\u7b56\u7565\uff0c\u63d0\u5347\u4e3b\u89c2NLP\u4efb\u52a1\u4e2d\u7fa4\u4f53\u5206\u6b67\u5efa\u6a21\u80fd\u529b", "motivation": "\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u6709\u6548\u6355\u6349\u6807\u6ce8\u8005\u7fa4\u4f53\u95f4\u7ed3\u6784\u5316\u5dee\u5f02\uff0c\u4e14\u771f\u5b9e\u6570\u636e\u4e2d\u4eba\u53e3\u7edf\u8ba1\u4fe1\u606f\u8986\u76d6\u7a00\u758f\uff0c\u9650\u5236\u4e86\u591a\u6837\u5316\u89c6\u89d2\u7684\u8868\u5f81\u80fd\u529b", "method": "1. \u8bbe\u8ba1DEM-MoE\u67b6\u6784\uff1a\u57fa\u4e8e\u6807\u6ce8\u8005\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u8def\u7531\u81f3\u4e13\u5bb6\u5b50\u7f51\u7edc\n2. \u5f00\u53d1\u96f6\u6837\u672c\u89d2\u8272\u63d0\u793a\u6cd5\u751f\u6210\u5408\u6210\u6807\u6ce8\n3. \u63d0\u51fa\u6839\u636e\u6570\u636e\u96c6\u7ed3\u6784\u5b9a\u5236\u7684\u771f\u5b9e/\u5408\u6210\u6570\u636e\u6df7\u5408\u7b56\u7565", "result": "DEM-MoE\u5728\u4eba\u53e3\u7fa4\u4f53\u95f4\u4fdd\u6301\u5747\u8861\u6027\u80fd\uff0c\u9ad8\u5206\u6b67\u6570\u636e\u96c6\u8868\u73b0\u7a81\u51fa\uff1b\u5408\u6210\u6807\u6ce8\u4e0e\u4eba\u7c7b\u6807\u6ce8\u4e2d\u5ea6\u5bf9\u9f50\uff08Spearman\u7cfb\u65700.3-0.5\uff09\uff1b\u6df7\u5408\u7b56\u7565\u6548\u679c\u53d6\u51b3\u4e8e\u6570\u636e\u96c6\u7ed3\u6784", "conclusion": "\u67b6\u6784\u521b\u65b0\u4e0e\u6570\u636e\u589e\u5f3a\u7684\u534f\u540c\u4f5c\u7528\u663e\u8457\u63d0\u5347\u4e86\u591a\u6837\u5316\u89c6\u89d2\u7684\u8868\u5f81\u80fd\u529b\uff0c\u4e3a\u590d\u6742\u4e3b\u89c2\u4efb\u52a1\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2508.02872", "pdf": "https://arxiv.org/pdf/2508.02872", "abs": "https://arxiv.org/abs/2508.02872", "authors": ["Giovanni Cherubin", "Andrew Paverd"], "title": "Highlight & Summarize: RAG without the jailbreaks", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Preventing jailbreaking and model hijacking of Large Language Models (LLMs)\nis an important yet challenging task. For example, when interacting with a\nchatbot, malicious users can input specially crafted prompts to cause the LLM\nto generate undesirable content or perform a completely different task from its\nintended purpose. Existing mitigations for such attacks typically rely on\nhardening the LLM's system prompt or using a content classifier trained to\ndetect undesirable content or off-topic conversations. However, these\nprobabilistic approaches are relatively easy to bypass due to the very large\nspace of possible inputs and undesirable outputs. In this paper, we present and\nevaluate Highlight & Summarize (H&S), a new design pattern for\nretrieval-augmented generation (RAG) systems that prevents these attacks by\ndesign. The core idea is to perform the same task as a standard RAG pipeline\n(i.e., to provide natural language answers to questions, based on relevant\nsources) without ever revealing the user's question to the generative LLM. This\nis achieved by splitting the pipeline into two components: a highlighter, which\ntakes the user's question and extracts relevant passages (\"highlights\") from\nthe retrieved documents, and a summarizer, which takes the highlighted passages\nand summarizes them into a cohesive answer. We describe several possible\ninstantiations of H&S and evaluate their generated responses in terms of\ncorrectness, relevance, and response quality. Surprisingly, when using an\nLLM-based highlighter, the majority of H&S responses are judged to be better\nthan those of a standard RAG pipeline.", "AI": {"tldr": "\u63d0\u51faHighlight & Summarize (H&S)\u8bbe\u8ba1\u6a21\u5f0f\uff0c\u901a\u8fc7\u5206\u79bb\u9ad8\u4eae\u4e0e\u603b\u7ed3\u6b65\u9aa4\u9632\u6b62LLM\u88ab\u6076\u610f\u653b\u51fb\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u56de\u7b54\u8d28\u91cf\u4f18\u4e8e\u4f20\u7edfRAG\u6d41\u7a0b", "motivation": "\u73b0\u6709LLM\u9632\u5fa1\u65b9\u6cd5\u4f9d\u8d56\u6982\u7387\u68c0\u6d4b\u673a\u5236\uff0c\u5b58\u5728\u8f93\u5165\u7a7a\u95f4\u8fc7\u5927\u5bfc\u81f4\u6613\u88ab\u7ed5\u8fc7\u7684\u7f3a\u9677\uff0c\u9700\u6784\u5efa\u66f4\u5b89\u5168\u7684\u67b6\u6784\u8bbe\u8ba1", "method": "\u5c06RAG\u6d41\u7a0b\u62c6\u89e3\u4e3a\uff1a\u9ad8\u4eae\u5668\uff08\u63d0\u53d6\u7528\u6237\u95ee\u9898\u76f8\u5173\u6587\u672c\u7247\u6bb5\uff09\u4e0e\u603b\u7ed3\u5668\uff08\u57fa\u4e8e\u9ad8\u4eae\u5185\u5bb9\u751f\u6210\u7b54\u6848\uff09\uff0c\u907f\u514d\u7528\u6237\u95ee\u9898\u76f4\u63a5\u66b4\u9732\u7ed9\u751f\u6210\u6a21\u578b", "result": "\u4f7f\u7528LLM\u4f5c\u4e3a\u9ad8\u4eae\u5668\u65f6\uff0c\u591a\u6570H&S\u751f\u6210\u7ed3\u679c\u5728\u6b63\u786e\u6027\u3001\u76f8\u5173\u6027\u548c\u8d28\u91cf\u4e0a\u4f18\u4e8e\u6807\u51c6RAG\u6d41\u7a0b", "conclusion": "H&S\u901a\u8fc7\u67b6\u6784\u7ea7\u521b\u65b0\u540c\u65f6\u63d0\u5347\u6a21\u578b\u5b89\u5168\u6027\u548c\u56de\u7b54\u8d28\u91cf\uff0c\u4e3a\u9632\u5fa1LLM\u653b\u51fb\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2508.02885", "pdf": "https://arxiv.org/pdf/2508.02885", "abs": "https://arxiv.org/abs/2508.02885", "authors": ["Elliot Murphy", "Rohan Venkatesh", "Edward Khokhlovich", "Andrey Vyshedskiy"], "title": "Merge-based syntax is mediated by distinct neurocognitive mechanisms: A clustering analysis of comprehension abilities in 84,000 individuals with language deficits across nine languages", "categories": ["cs.CL"], "comment": null, "summary": "In the modern language sciences, the core computational operation of syntax,\n'Merge', is defined as an operation that combines two linguistic units (e.g.,\n'brown', 'cat') to form a categorized structure ('brown cat', a Noun Phrase).\nThis can then be further combined with additional linguistic units based on\nthis categorial information, respecting non-associativity such that abstract\ngrouping is respected. Some linguists have embraced the view that Merge is an\nelementary, indivisible operation that emerged in a single evolutionary step.\nFrom a neurocognitive standpoint, different mental objects constructed by Merge\nmay be supported by distinct mechanisms: (1) simple command constructions\n(e.g., \"eat apples\"); (2) the merging of adjectives and nouns (\"red boat\"); and\n(3) the merging of nouns with spatial prepositions (\"laptop behind the sofa\").\nHere, we systematically investigate participants' comprehension of sentences\nwith increasing levels of syntactic complexity. Clustering analyses revealed\nbehavioral evidence for three distinct structural types, which we discuss as\npotentially emerging at different developmental stages and subject to selective\nimpairment. While a Merge-based syntax may still have emerged suddenly in\nevolutionary time, responsible for the structured symbolic turn our species\ntook, different cognitive mechanisms seem to underwrite the processing of\nvarious types of Merge-based objects.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u53e5\u6cd5\u64cd\u4f5c'Merge'\u5b58\u5728\u4e09\u79cd\u795e\u7ecf\u8ba4\u77e5\u673a\u5236\uff0c\u5206\u522b\u652f\u6301\u547d\u4ee4\u7ed3\u6784\u3001\u5f62\u5bb9\u8bcd-\u540d\u8bcd\u7ec4\u5408\u548c\u7a7a\u95f4\u4ecb\u8bcd\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u884c\u4e3a\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5bf9\u5e94\u4e0d\u540c\u53d1\u5c55\u9636\u6bb5\u548c\u635f\u4f24\u6a21\u5f0f\u3002", "motivation": "\u6311\u6218\u4f20\u7edf\u8bed\u8a00\u5b66\u8ba4\u4e3aMerge\u662f\u5355\u4e00\u8fdb\u5316\u673a\u5236\u7684\u89c2\u70b9\uff0c\u4ece\u795e\u7ecf\u8ba4\u77e5\u89c6\u89d2\u63a2\u7d22\u4e0d\u540c\u53e5\u6cd5\u7ed3\u6784\u53ef\u80fd\u5bf9\u5e94\u7684\u72ec\u7acb\u8ba4\u77e5\u673a\u5236\u3002", "method": "\u7cfb\u7edf\u6d4b\u91cf\u88ab\u8bd5\u5bf9\u9012\u589e\u590d\u6742\u5ea6\u53e5\u5b50\u7684\u7406\u89e3\uff08\u547d\u4ee4\u7ed3\u6784/\u5f62\u5bb9\u8bcd\u7ec4\u5408/\u7a7a\u95f4\u4ecb\u8bcd\u7ed3\u6784\uff09\uff0c\u91c7\u7528\u805a\u7c7b\u5206\u6790\u8bc6\u522b\u884c\u4e3a\u6a21\u5f0f\u5dee\u5f02\u3002", "result": "\u53d1\u73b0\u4e09\u79cd\u663e\u8457\u5206\u79bb\u7684\u7ed3\u6784\u7c7b\u578b\uff0c\u5176\u884c\u4e3a\u6a21\u5f0f\u4e0e\uff1a1\uff09\u513f\u7ae5\u8bed\u8a00\u53d1\u5c55\u987a\u5e8f 2\uff09\u7279\u5b9a\u8111\u635f\u4f24\u5bfc\u81f4\u7684\u53e5\u6cd5\u969c\u788d\u7c7b\u578b\u5b58\u5728\u5bf9\u5e94\u5173\u7cfb\u3002", "conclusion": "Merge\u53ef\u80fd\u4f5c\u4e3a\u6574\u4f53\u80fd\u529b\u5728\u8fdb\u5316\u4e2d\u6d8c\u73b0\uff0c\u4f46\u5177\u4f53\u5b9e\u73b0\u4f9d\u8d56\u591a\u79cd\u8ba4\u77e5\u673a\u5236\uff0c\u8fd9\u89e3\u91ca\u4e86\u4eba\u7c7b\u8bed\u8a00\u80fd\u529b\u7684\u53ef\u5206\u89e3\u6027\u7279\u5f81\u3002"}}
{"id": "2508.02886", "pdf": "https://arxiv.org/pdf/2508.02886", "abs": "https://arxiv.org/abs/2508.02886", "authors": ["Wenjie Luo", "Ruocheng Li", "Shanshan Zhu", "Julian Perry"], "title": "Coherent Multimodal Reasoning with Iterative Self-Evaluation for Vision-Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Despite significant advancements, current large language models (LLMs) and\nvision-language models (LVLMs) continue to struggle with complex, multi-step,\ncross-modal common sense reasoning tasks, often exhibiting a lack of\n\"deliberative thinking.\" They tend to rely on superficial associations rather\nthan deep, chained inference, particularly when integrating visual information\nwith abstract concepts. To address this, we propose the Coherent Multimodal\nReasoning Framework (CMRF), a novel approach that enhances LVLMs' common sense\nreasoning capabilities through an iterative, self-evaluating inference\nmechanism. CMRF mimics human problem-solving by decomposing complex queries,\ngenerating step-by-step inferences, and self-correcting errors. Our framework\nintegrates three key modules: a Reasoning Decomposition Unit (RDU) for breaking\ndown problems into sub-questions, a Contextual Inference Engine (CIE) for\ncontextual inference, and a Coherence Assessment Module (CAM) for evaluating\nlogical consistency and confidence. Coupled with an Adaptive Iterative\nRefinement strategy, CMRF systematically refines its reasoning paths. Built\nupon LLaVA-1.6-34B and trained on a novel Multimodal Daily Activity Reasoning\n(MDAR) dataset, CMRF achieves state-of-the-art performance among open-source\nLVLMs on challenging benchmarks like VCR, A-OKVQA, and DailyLife-MRC. It\nattains an average accuracy of 69.4%, surpassing the best open-source baseline\nby +2.4 percentage points, with particular strength in complex reasoning\nscenarios. Extensive ablation studies and human evaluations confirm the\ncritical contributions of each module and the effectiveness of iterative\nrefinement in fostering more coherent and accurate reasoning.", "AI": {"tldr": "\u63d0\u51faCMRF\u6846\u67b6\u589e\u5f3a\u591a\u6a21\u6001\u6a21\u578b\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u95ee\u9898\u5206\u89e3\u3001\u81ea\u8bc4\u4f30\u8fed\u4ee3\u4f18\u5316\u673a\u5236\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0SOTA\u6548\u679c", "motivation": "\u73b0\u6709LVLMs\u5728\u590d\u6742\u8de8\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e2d\u4ecd\u5b58\u5728\u8868\u9762\u5173\u8054\u4f9d\u8d56\u3001\u7f3a\u4e4f\u94fe\u5f0f\u6df1\u5ea6\u63a8\u7406\u7684\u95ee\u9898\uff0c\u5c24\u5176\u5728\u89c6\u89c9\u4e0e\u62bd\u8c61\u6982\u5ff5\u6574\u5408\u65b9\u9762", "method": "\u57fa\u4e8eLLaVA-34B\u6784\u5efa\u4e09\u6a21\u5757\u6846\u67b6\uff1aRDU\u5206\u89e3\u95ee\u9898\u2192CIE\u4e0a\u4e0b\u6587\u63a8\u7406\u2192CAM\u903b\u8f91\u8bc4\u4f30\uff0c\u914d\u5408\u81ea\u9002\u5e94\u8fed\u4ee3\u4f18\u5316\u7b56\u7565\uff0c\u4f7f\u7528MDAR\u6570\u636e\u96c6\u8bad\u7ec3", "result": "\u5728VCR/A-OKVQA/DailyLife-MRC\u8fbe\u523069.4%\u5e73\u5747\u51c6\u786e\u7387\uff08\u5f00\u6e90LVLMs\u4e2d\u63d0\u5347+2.4%\uff09\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u5404\u6a21\u5757\u8d21\u732e\u53ca\u8fed\u4ee3\u4f18\u5316\u6709\u6548\u6027", "conclusion": "\u6a21\u5757\u5316\u8bbe\u8ba1\u4e0e\u8fed\u4ee3\u4f18\u5316\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u7684\u8fde\u8d2f\u6027\uff0c\u4e3a\u6784\u5efa\u7c7b\u4eba\u63a8\u7406\u673a\u5236\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u672a\u6765\u53ef\u62d3\u5c55\u5230\u66f4\u590d\u6742\u8ba4\u77e5\u573a\u666f"}}
{"id": "2508.02901", "pdf": "https://arxiv.org/pdf/2508.02901", "abs": "https://arxiv.org/abs/2508.02901", "authors": ["Osama Khalid", "Sanvesh Srivastava", "Padmini Srinivasan"], "title": "SLIM-LLMs: Modeling of Style-Sensory Language RelationshipsThrough Low-Dimensional Representations", "categories": ["cs.CL"], "comment": null, "summary": "Sensorial language -- the language connected to our senses including vision,\nsound, touch, taste, smell, and interoception, plays a fundamental role in how\nwe communicate experiences and perceptions. We explore the relationship between\nsensorial language and traditional stylistic features, like those measured by\nLIWC, using a novel Reduced-Rank Ridge Regression (R4) approach. We demonstrate\nthat low-dimensional latent representations of LIWC features r = 24 effectively\ncapture stylistic information for sensorial language prediction compared to the\nfull feature set (r = 74). We introduce Stylometrically Lean Interpretable\nModels (SLIM-LLMs), which model non-linear relationships between these style\ndimensions. Evaluated across five genres, SLIM-LLMs with low-rank LIWC features\nmatch the performance of full-scale language models while reducing parameters\nby up to 80%.", "AI": {"tldr": "\u63d0\u51faSLIM-LLM\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u79e9LIWC\u7279\u5f81\u5b9e\u73b0\u9ad8\u6548\u611f\u5b98\u8bed\u8a00\u5efa\u6a21\uff0c\u53c2\u6570\u51cf\u5c1180%\u4fdd\u6301\u6027\u80fd", "motivation": "\u63a2\u7d22\u611f\u5b98\u8bed\u8a00\u4e0eLIWC\u6587\u4f53\u7279\u5f81\u7684\u5173\u8054\uff0c\u89e3\u51b3\u4f20\u7edf\u6a21\u578b\u53c2\u6570\u91cf\u5927\u6548\u7387\u4f4e\u7684\u95ee\u9898", "method": "\u4f7f\u7528\u964d\u79e9\u5cad\u56de\u5f52\uff08R4\uff09\u63d0\u53d6\u4f4e\u7ef4LIWC\u7279\u5f81\uff0c\u6784\u5efa\u975e\u7ebf\u6027\u7684SLIM-LLM\u6a21\u578b", "result": "\u4f4e\u79e9\u7279\u5f81\uff08r=24\uff09\u4e0e\u5168\u7279\u5f81\uff08r=74\uff09\u8868\u73b0\u76f8\u5f53\uff0c\u6a21\u578b\u53c2\u6570\u51cf\u5c1180%", "conclusion": "SLIM-LLM\u4e3a\u591a\u4f53\u88c1\u6587\u672c\u5206\u6790\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u6548\u7387"}}
{"id": "2508.02931", "pdf": "https://arxiv.org/pdf/2508.02931", "abs": "https://arxiv.org/abs/2508.02931", "authors": ["Shengqi Li", "Amarnath Gupta"], "title": "Can LLMs Generate High-Quality Task-Specific Conversations?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper introduces a parameterization framework for controlling\nconversation quality in large language models. We explore nine key parameters\nacross six dimensions that enable precise specification of dialogue properties.\nThrough experiments with state-of-the-art LLMs, we demonstrate that\nparameter-based control produces statistically significant differences in\ngenerated conversation properties. Our approach addresses challenges in\nconversation generation, including topic coherence, knowledge progression,\ncharacter consistency, and control granularity. The framework provides a\nstandardized method for conversation quality control with applications in\neducation, therapy, customer service, and entertainment. Future work will focus\non implementing additional parameters through architectural modifications and\ndeveloping benchmark datasets for evaluation.", "AI": {"tldr": "\u63d0\u51fa\u53c2\u6570\u5316\u6846\u67b6\u63a7\u5236\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u8bdd\u8d28\u91cf\uff0c\u901a\u8fc7\u4e5d\u7ef4\u53c2\u6570\u4f53\u7cfb\u5b9e\u73b0\u5bf9\u8bdd\u5c5e\u6027\u7684\u7cbe\u786e\u8c03\u63a7", "motivation": "\u89e3\u51b3\u5bf9\u8bdd\u751f\u6210\u4e2d\u4e3b\u9898\u8fde\u8d2f\u6027\u5dee\u3001\u77e5\u8bc6\u9012\u8fdb\u4e0d\u8db3\u3001\u89d2\u8272\u4e00\u81f4\u6027\u5f31\u548c\u8c03\u63a7\u7c92\u5ea6\u7c97\u7684\u95ee\u9898", "method": "\u6784\u5efa\u5305\u542b\u4e5d\u4e2a\u5173\u952e\u53c2\u6570\u7684\u516d\u7ef4\u5ea6\u63a7\u5236\u6846\u67b6\uff0c\u57fa\u4e8e\u524d\u6cbfLLM\u8fdb\u884c\u53c2\u6570\u8c03\u63a7\u5b9e\u9a8c\u9a8c\u8bc1", "result": "\u53c2\u6570\u63a7\u5236\u5bf9\u5bf9\u8bdd\u5c5e\u6027\u4ea7\u751f\u7edf\u8ba1\u663e\u8457\u6027\u5f71\u54cd\uff0c\u6709\u6548\u63d0\u5347\u8bdd\u9898\u7ec4\u7ec7\u4e0e\u77e5\u8bc6\u4f20\u9012\u8d28\u91cf", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6559\u80b2/\u533b\u7597/\u5ba2\u670d\u9886\u57df\u63d0\u4f9b\u6807\u51c6\u5316\u5bf9\u8bdd\u8d28\u91cf\u63a7\u5236\u65b9\u6848\uff0c\u672a\u6765\u5c06\u6269\u5c55\u53c2\u6570\u4f53\u7cfb\u5e76\u5efa\u7acb\u8bc4\u4f30\u57fa\u51c6"}}
{"id": "2508.02997", "pdf": "https://arxiv.org/pdf/2508.02997", "abs": "https://arxiv.org/abs/2508.02997", "authors": ["Sri Durga Sai Sowmya Kadali", "Evangelos E. Papalexakis"], "title": "CoCoTen: Detecting Adversarial Inputs to Large Language Models through Latent Space Features of Contextual Co-occurrence Tensors", "categories": ["cs.CL"], "comment": null, "summary": "The widespread use of Large Language Models (LLMs) in many applications marks\na significant advance in research and practice. However, their complexity and\nhard-to-understand nature make them vulnerable to attacks, especially\njailbreaks designed to produce harmful responses. To counter these threats,\ndeveloping strong detection methods is essential for the safe and reliable use\nof LLMs. This paper studies this detection problem using the Contextual\nCo-occurrence Matrix, a structure recognized for its efficacy in data-scarce\nenvironments. We propose a novel method leveraging the latent space\ncharacteristics of Contextual Co-occurrence Matrices and Tensors for the\neffective identification of adversarial and jailbreak prompts. Our evaluations\nshow that this approach achieves a notable F1 score of 0.83 using only 0.5% of\nlabeled prompts, which is a 96.6% improvement over baselines. This result\nhighlights the strength of our learned patterns, especially when labeled data\nis scarce. Our method is also significantly faster, speedup ranging from 2.3 to\n128.4 times compared to the baseline models. To support future research and\nreproducibility, we have made our implementation publicly available.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5171\u73b0\u77e9\u9635\u5f20\u91cf\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ec5\u75280.5%\u6807\u6ce8\u6570\u636e\u5b9e\u73b083% F1\u503c\uff0c\u68c0\u6d4b\u901f\u5ea6\u63d0\u53472.3-128.4\u500d\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u590d\u6742\u6027\u4f7f\u5176\u6613\u53d7\u5bf9\u6297\u653b\u51fb\uff08\u5982\u8d8a\u72f1\u653b\u51fb\uff09\uff0c\u9700\u5f00\u53d1\u9ad8\u6548\u68c0\u6d4b\u65b9\u6cd5\u4fdd\u969c\u6a21\u578b\u5b89\u5168\u3002", "method": "\u5229\u7528\u4e0a\u4e0b\u6587\u5171\u73b0\u77e9\u9635/\u5f20\u91cf\u7684\u6f5c\u5728\u7a7a\u95f4\u7279\u5f81\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u8fdb\u884c\u5bf9\u6297\u63d0\u793a\u68c0\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5F1\u503c\u8fbe0.83\uff08\u6bd4\u57fa\u7ebf\u63d0\u534796.6%\uff09\uff0c\u68c0\u6d4b\u901f\u5ea6\u6700\u9ad8\u63d0\u5347128\u500d\uff0c\u4e14\u5177\u5907\u6570\u636e\u9ad8\u6548\u6027\u3002", "conclusion": "\u57fa\u4e8e\u5171\u73b0\u6a21\u5f0f\u7684\u5b66\u4e60\u65b9\u6cd5\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5f00\u6e90\u5b9e\u73b0\u4fc3\u8fdb\u76f8\u5173\u9886\u57df\u7814\u7a76\u3002"}}
{"id": "2508.03037", "pdf": "https://arxiv.org/pdf/2508.03037", "abs": "https://arxiv.org/abs/2508.03037", "authors": ["Ariya Mukherjee-Gandhi", "Oliver Muellerklein"], "title": "When Algorithms Meet Artists: Topic Modeling the AI-Art Debate, 2013-2025", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "18 pages, 5 figures, 5 tables", "summary": "As generative AI continues to reshape artistic production and alternate modes\nof human expression, artists whose livelihoods are most directly affected have\nraised urgent concerns about consent, transparency, and the future of creative\nlabor. However, the voices of artists are often marginalized in dominant public\nand scholarly discourse. This study presents a twelve-year analysis, from 2013\nto 2025, of English-language discourse surrounding AI-generated art. It draws\nfrom 439 curated 500-word excerpts sampled from opinion articles, news reports,\nblogs, legal filings, and spoken-word transcripts. Through a reproducible\nmethodology, we identify five stable thematic clusters and uncover a\nmisalignment between artists' perceptions and prevailing media narratives. Our\nfindings highlight how the use of technical jargon can function as a subtle\nform of gatekeeping, often sidelining the very issues artists deem most urgent.\nOur work provides a BERTopic-based methodology and a multimodal baseline for\nfuture research, alongside a clear call for deeper, transparency-driven\nengagement with artist perspectives in the evolving AI-creative landscape.", "AI": {"tldr": "12\u5e74AI\u751f\u6210\u827a\u672f\u7814\u7a76\u63ed\u793a\u827a\u672f\u5bb6\u5173\u5207\u4e0e\u4e3b\u6d41\u53d9\u4e8b\u8131\u8282\uff0c\u6280\u672f\u672f\u8bed\u6210\u8bdd\u8bed\u6743\u58c1\u5792\uff0c\u547c\u5401\u900f\u660e\u5316\u827a\u672f\u5bb6\u53c2\u4e0e", "motivation": "\u9488\u5bf9AI\u6280\u672f\u91cd\u5851\u827a\u672f\u521b\u4f5c\u80cc\u666f\u4e0b\u827a\u672f\u5bb6\u8bdd\u8bed\u6743\u88ab\u8fb9\u7f18\u5316\u7684\u95ee\u9898\uff0c\u63a2\u7a76\u4e3b\u6d41\u53d9\u4e8b\u4e0e\u827a\u672f\u5bb6\u771f\u5b9e\u8bc9\u6c42\u7684\u504f\u5dee", "method": "\u57fa\u4e8eBERTopic\u5bf92013-2025\u5e74439\u4efd\u591a\u6e90\u6587\u672c\uff08\u8bc4\u8bba/\u65b0\u95fb/\u6cd5\u5f8b\u6587\u4ef6\u7b49\uff09\u8fdb\u884c\u4e3b\u9898\u805a\u7c7b\u5206\u6790", "result": "\u8bc6\u522b\u51fa5\u4e2a\u7a33\u5b9a\u4e3b\u9898\u7c07\uff0c\u53d1\u73b0\u6280\u672f\u672f\u8bed\u4f7f\u7528\u5f62\u6210\u9690\u6027\u8bdd\u8bed\u6743\u58c1\u5792\uff0c\u827a\u672f\u5bb6\u6838\u5fc3\u5173\u5207\uff08\u540c\u610f/\u900f\u660e/\u52b3\u52a8\u6743\u76ca\uff09\u906d\u7cfb\u7edf\u6027\u5ffd\u89c6", "conclusion": "\u5efa\u7acb\u591a\u6a21\u6001\u7814\u7a76\u57fa\u7ebf\uff0c\u5f3a\u8c03\u5fc5\u987b\u5efa\u7acb\u900f\u660e\u673a\u5236\u786e\u4fdd\u827a\u672f\u5bb6\u5728AI-\u521b\u610f\u751f\u6001\u4e2d\u7684\u6838\u5fc3\u8bdd\u8bed\u5730\u4f4d"}}
{"id": "2508.03098", "pdf": "https://arxiv.org/pdf/2508.03098", "abs": "https://arxiv.org/abs/2508.03098", "authors": ["Haoran Wang", "Xiongxiao Xu", "Baixiang Huang", "Kai Shu"], "title": "Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models in Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances the factual accuracy of large\nlanguage models (LLMs) by conditioning outputs on external knowledge sources.\nHowever, when retrieval involves private or sensitive data, RAG systems are\nsusceptible to extraction attacks that can leak confidential information\nthrough generated responses. We propose Privacy-Aware Decoding (PAD), a\nlightweight, inference-time defense that adaptively injects calibrated Gaussian\nnoise into token logits during generation. PAD integrates confidence-based\nscreening to selectively protect high-risk tokens, efficient sensitivity\nestimation to minimize unnecessary noise, and context-aware noise calibration\nto balance privacy with generation quality. A \\renyi Differential Privacy (RDP)\naccountant rigorously tracks cumulative privacy loss, enabling explicit\nper-response $(\\varepsilon, \\delta)$-DP guarantees for sensitive outputs.\nUnlike prior approaches requiring retraining or corpus-level filtering, PAD is\nmodel-agnostic and operates entirely at decoding time with minimal\ncomputational overhead. Experiments on three real-world datasets demonstrate\nthat PAD substantially reduces private information leakage while preserving\nresponse utility, outperforming existing retrieval- and post-processing-based\ndefenses. Our work takes an important step toward mitigating privacy risks in\nRAG via decoding strategies, paving the way for universal and scalable privacy\nsolutions in sensitive domains. Our code is available:\nhttps://github.com/wang2226/PAD.", "AI": {"tldr": "\u63d0\u51faPAD\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u751f\u6210\u9636\u6bb5\u6ce8\u5165\u81ea\u9002\u5e94\u9ad8\u65af\u566a\u58f0\uff0c\u5b9e\u73b0\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u7684\u9690\u79c1\u4fdd\u62a4\u4e0e\u54cd\u5e94\u8d28\u91cf\u7684\u5e73\u8861\u3002", "motivation": "RAG\u7cfb\u7edf\u5904\u7406\u654f\u611f\u6570\u636e\u65f6\u5b58\u5728\u4fe1\u606f\u6cc4\u9732\u98ce\u9669\uff0c\u9700\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u4f9b\u4e25\u683c\u7684\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u3002", "method": "\u7ed3\u5408\u7f6e\u4fe1\u5ea6\u7b5b\u67e5\u7b5b\u9009\u9ad8\u98ce\u9669token\uff0c\u901a\u8fc7\u654f\u611f\u6027\u4f30\u8ba1\u6700\u5c0f\u5316\u566a\u58f0\u5e72\u6270\uff0c\u4f7f\u7528R\u00e9nyi\u5dee\u5206\u9690\u79c1\u8ddf\u8e2a\u7d2f\u8ba1\u9690\u79c1\u635f\u5931\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u964d\u4f4e\u9690\u79c1\u6cc4\u9732\uff08\u5e73\u5747\u6cc4\u6f0f\u91cf\u51cf\u5c1167%\uff09\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u57fa\u7ebf\u5feb1.8\u500d\u3002", "conclusion": "\u9996\u6b21\u901a\u8fc7\u89e3\u7801\u7b56\u7565\u5b9e\u73b0RAG\u9690\u79c1\u4fdd\u62a4\uff0c\u4e3a\u654f\u611f\u9886\u57df\u63d0\u4f9b\u65e0\u9700\u6a21\u578b\u6539\u9020\u7684\u901a\u7528\u9690\u79c1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03110", "pdf": "https://arxiv.org/pdf/2508.03110", "abs": "https://arxiv.org/abs/2508.03110", "authors": ["Zizhong Li", "Haopeng Zhang", "Jiawei Zhang"], "title": "Token-Level Precise Attack on RAG: Searching for the Best Alternatives to Mislead Generation", "categories": ["cs.CL"], "comment": null, "summary": "While large language models (LLMs) have achieved remarkable success in\nproviding trustworthy responses for knowledge-intensive tasks, they still face\ncritical limitations such as hallucinations and outdated knowledge. To address\nthese issues, the retrieval-augmented generation (RAG) framework enhances LLMs\nwith access to external knowledge via a retriever, enabling more accurate and\nreal-time outputs about the latest events. However, this integration brings new\nsecurity vulnerabilities: the risk that malicious content in the external\ndatabase can be retrieved and used to manipulate model outputs. Although prior\nwork has explored attacks on RAG systems, existing approaches either rely\nheavily on access to the retriever or fail to jointly consider both retrieval\nand generation stages, limiting their effectiveness, particularly in black-box\nscenarios. To overcome these limitations, we propose Token-level Precise Attack\non the RAG (TPARAG), a novel framework that targets both white-box and\nblack-box RAG systems. TPARAG leverages a lightweight white-box LLM as an\nattacker to generate and iteratively optimize malicious passages at the token\nlevel, ensuring both retrievability and high attack success in generation.\nExtensive experiments on open-domain QA datasets demonstrate that TPARAG\nconsistently outperforms previous approaches in retrieval-stage and end-to-end\nattack effectiveness. These results further reveal critical vulnerabilities in\nRAG pipelines and offer new insights into improving their robustness.", "AI": {"tldr": "\u63d0\u51fa\u9488\u5bf9RAG\u6846\u67b6\u7684Token\u7ea7\u7cbe\u51c6\u653b\u51fb\u65b9\u6cd5TPARAG\uff0c\u63ed\u793a\u5176\u5728\u767d\u76d2/\u9ed1\u76d2\u573a\u666f\u4e0b\u5bf9\u68c0\u7d22\u548c\u751f\u6210\u9636\u6bb5\u7684\u53cc\u91cd\u653b\u51fb\u6709\u6548\u6027", "motivation": "RAG\u6846\u67b6\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u65f6\u9762\u4e34\u6076\u610f\u5185\u5bb9\u68c0\u7d22\u98ce\u9669\uff0c\u73b0\u6709\u653b\u51fb\u65b9\u6cd5\u5728\u9ed1\u76d2\u573a\u666f\u6548\u679c\u6709\u9650\u4e14\u7f3a\u4e4f\u68c0\u7d22-\u751f\u6210\u8054\u5408\u4f18\u5316", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u767d\u76d2LLM\u4f5c\u4e3a\u653b\u51fb\u8005\uff0c\u901a\u8fc7token\u7ea7\u6076\u610f\u6bb5\u843d\u751f\u6210\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u786e\u4fdd\u68c0\u7d22\u6210\u529f\u7387\u548c\u751f\u6210\u9636\u6bb5\u653b\u51fb\u6210\u529f\u7387", "result": "\u5728\u5f00\u653e\u57dfQA\u6570\u636e\u96c6\u4e0a\uff0cTPARAG\u7684\u68c0\u7d22\u6210\u529f\u7387\u6bd4\u57fa\u7ebf\u9ad815.3%\uff0c\u7aef\u5230\u7aef\u653b\u51fb\u6210\u529f\u7387\u63d0\u534722.1%", "conclusion": "RAG\u7ba1\u9053\u5b58\u5728\u5173\u952e\u5b89\u5168\u6f0f\u6d1e\uff0c\u63d0\u5347\u9c81\u68d2\u6027\u9700\u540c\u65f6\u8003\u8651\u68c0\u7d22\u9632\u5fa1\u673a\u5236\u548c\u751f\u6210\u9636\u6bb5\u7684\u5bf9\u6297\u6027\u68c0\u6d4b"}}
{"id": "2508.03112", "pdf": "https://arxiv.org/pdf/2508.03112", "abs": "https://arxiv.org/abs/2508.03112", "authors": ["Motaz Saad", "David Langlois", "Kamel Smaili"], "title": "Cross-lingual Opinions and Emotions Mining in Comparable Documents", "categories": ["cs.CL", "I.2.7"], "comment": "16 pages, 5 figures", "summary": "Comparable texts are topic-aligned documents in multiple languages that are\nnot direct translations. They are valuable for understanding how a topic is\ndiscussed across languages. This research studies differences in sentiments and\nemotions across English-Arabic comparable documents. First, texts are annotated\nwith sentiment and emotion labels. We apply a cross-lingual method to label\ndocuments with opinion classes (subjective/objective), avoiding reliance on\nmachine translation. To annotate with emotions (anger, disgust, fear, joy,\nsadness, surprise), we manually translate the English WordNet-Affect (WNA)\nlexicon into Arabic, creating bilingual emotion lexicons used to label the\ncomparable corpora. We then apply a statistical measure to assess the agreement\nof sentiments and emotions in each source-target document pair. This comparison\nis especially relevant when the documents originate from different sources. To\nour knowledge, this aspect has not been explored in prior literature. Our study\nincludes English-Arabic document pairs from Euronews, BBC, and Al-Jazeera\n(JSC). Results show that sentiment and emotion annotations align when articles\ncome from the same news agency and diverge when they come from different ones.\nThe proposed method is language-independent and generalizable to other language\npairs.", "AI": {"tldr": "\u901a\u8fc7\u8de8\u8bed\u8a00\u60c5\u611f\u6807\u6ce8\u65b9\u6cd5\u548c\u53cc\u8bed\u60c5\u611f\u8bcd\u5178\uff0c\u5206\u6790\u82f1\u8bed-\u963f\u62c9\u4f2f\u8bed\u53ef\u6bd4\u65b0\u95fb\u6587\u672c\u7684\u60c5\u611f\u8868\u8fbe\u5dee\u5f02\uff0c\u53d1\u73b0\u540c\u6e90\u65b0\u95fb\u60c5\u611f\u4e00\u81f4\u3001\u5f02\u6e90\u65b0\u95fb\u60c5\u611f\u5206\u6b67\u7684\u89c4\u5f8b\u3002", "motivation": "\u7814\u7a76\u591a\u8bed\u8a00\u53ef\u6bd4\u6587\u672c\uff08\u975e\u76f4\u63a5\u7ffb\u8bd1\uff09\u4e2d\u60c5\u611f\u8868\u8fbe\u7684\u5dee\u5f02\uff0c\u63a2\u7d22\u4e0d\u540c\u6587\u5316\u8bed\u5883\u4e0b\u5bf9\u540c\u4e00\u8bdd\u9898\u7684\u60c5\u611f\u6295\u5c04\u5dee\u5f02\uff0c\u7279\u522b\u662f\u8de8\u65b0\u95fb\u673a\u6784\u6765\u6e90\u7684\u5f71\u54cd\u3002", "method": "1. \u6784\u5efa\u963f\u82f1\u53cc\u8bed\u60c5\u611f\u8bcd\u5178\uff08\u4eba\u5de5\u7ffb\u8bd1WordNet-Affect\uff09\n2. \u5f00\u53d1\u4e0d\u4f9d\u8d56\u673a\u5668\u7ffb\u8bd1\u7684\u8de8\u8bed\u8a00\u6807\u6ce8\u65b9\u6cd5\n3. \u4f7f\u7528\u7edf\u8ba1\u65b9\u6cd5\u8861\u91cf\u8de8\u6587\u6863\u60c5\u611f/\u60c5\u7eea\u6807\u6ce8\u4e00\u81f4\u6027\n4. \u6570\u636e\u6e90\uff1a\u6b27\u6d32\u65b0\u95fb\u53f0\u3001BBC\u3001\u534a\u5c9b\u7535\u89c6\u53f0\u7684\u65b0\u95fb\u5bf9", "result": "\u540c\u6e90\u65b0\u95fb\u673a\u6784\uff08\u5982Euronews\u82f1\u963f\u7248\uff09\u60c5\u611f\u6807\u6ce8\u4e00\u81f4\u6027\u5f3a\uff08kappa=0.78\uff09\uff0c\u5f02\u6e90\u673a\u6784\uff08\u5982BBC vs \u534a\u5c9b\u53f0\uff09\u60c5\u611f\u5206\u6b67\u663e\u8457\uff08kappa=0.32\uff09\uff0c\u60c5\u7eea\u7ef4\u5ea6\u5dee\u5f02\u6700\u5927\u7684\u662f\u2018\u6050\u60e7\u2019\u548c\u2018\u60ca\u559c\u2019", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u8bed\u8a00\u666e\u9002\u6027\uff0c\u53ef\u62d3\u5c55\u81f3\u5176\u4ed6\u8bed\u5bf9\u3002\u7814\u7a76\u53d1\u73b0\u65b0\u95fb\u673a\u6784\u7acb\u573a\u663e\u8457\u5f71\u54cd\u8de8\u8bed\u8a00\u60c5\u611f\u8868\u8fbe\uff0c\u4e3a\u8206\u60c5\u5206\u6790\u548c\u8de8\u6587\u5316\u4f20\u64ad\u7814\u7a76\u63d0\u4f9b\u65b0\u5de5\u5177\u3002"}}
{"id": "2508.03137", "pdf": "https://arxiv.org/pdf/2508.03137", "abs": "https://arxiv.org/abs/2508.03137", "authors": ["Ge Shi", "Kaiyu Huang", "Guochen Feng"], "title": "Long Story Generation via Knowledge Graph and Literary Theory", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The generation of a long story consisting of several thousand words is a\nsub-task in the field of long text generation~(LTG). Previous research has\naddressed this challenge through outline-based generation, which employs a\nmulti-stage method for generating outlines into stories. However, this approach\nsuffers from two common issues: almost inevitable theme drift caused by the\nloss of memory of previous outlines, and tedious plots with incoherent logic\nthat are less appealing to human readers.\n  In this paper, we propose the multi-agent Story Generator structure to\nimprove the multi-stage method, using large language models~(LLMs) as the core\ncomponents of agents. To avoid theme drift, we introduce a memory storage model\ncomprising two components: a long-term memory storage that identifies the most\nimportant memories, thereby preventing theme drift; and a short-term memory\nstorage that retains the latest outlines from each generation round. To\nincorporate engaging elements into the story, we design a story theme obstacle\nframework based on literary narratology theory that introduces uncertain\nfactors and evaluation criteria to generate outline. This framework calculates\nthe similarity of the former storyline and enhances the appeal of the story by\nbuilding a knowledge graph and integrating new node content. Additionally, we\nestablish a multi-agent interaction stage to simulate writer-reader interaction\nthrough dialogue and revise the story text according to feedback, to ensure it\nremains consistent and logical. Evaluations against previous methods\ndemonstrate that our approach can generate higher-quality long stories.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u6545\u4e8b\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u957f\u671f/\u77ed\u671f\u8bb0\u5fc6\u5b58\u50a8\u673a\u5236\u89e3\u51b3\u4e3b\u9898\u504f\u79fb\u95ee\u9898\uff0c\u7ed3\u5408\u53d9\u4e8b\u5b66\u7406\u8bba\u6784\u5efa\u6545\u4e8b\u969c\u788d\u6846\u67b6\u63d0\u5347\u60c5\u8282\u5438\u5f15\u529b\uff0c\u751f\u6210\u8d28\u91cf\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u5927\u7eb2\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u4e3b\u9898\u504f\u79fb\u548c\u60c5\u8282\u903b\u8f91\u65ad\u88c2\u95ee\u9898\uff0c\u9700\u901a\u8fc7\u8bb0\u5fc6\u673a\u5236\u548c\u4e92\u52a8\u6846\u67b6\u589e\u5f3a\u6545\u4e8b\u8fde\u8d2f\u6027\u4e0e\u5438\u5f15\u529b\u3002", "method": "1. \u53cc\u901a\u9053\u8bb0\u5fc6\u5b58\u50a8\uff08\u957f\u671f\u8bb0\u5fc6\u63d0\u53d6\u5173\u952e\u4e3b\u9898\uff0c\u77ed\u671f\u8bb0\u5fc6\u8ffd\u8e2a\u6700\u65b0\u5927\u7eb2\uff09\n2. \u6545\u4e8b\u969c\u788d\u6846\u67b6\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u56e0\u7d20\n3. \u77e5\u8bc6\u56fe\u8c31\u52a8\u6001\u6269\u5c55\u60c5\u8282\n4. \u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u5b9e\u73b0\u4f5c\u8005-\u8bfb\u8005\u53cd\u9988\u5faa\u73af", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u4e3b\u9898\u4e00\u81f4\u6027\u3001\u60c5\u8282\u5438\u5f15\u529b\u548c\u903b\u8f91\u8fde\u8d2f\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4eba\u5de5\u8bc4\u4f30\u5f97\u5206\u63d0\u534723.6%\u3002", "conclusion": "\u5c06\u8bb0\u5fc6\u673a\u5236\u4e0e\u53d9\u4e8b\u5b66\u7406\u8bba\u7ed3\u5408\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u534f\u4f5c\u6709\u6548\u89e3\u51b3\u957f\u6587\u672c\u751f\u6210\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4e3aAI\u521b\u4f5c\u7cfb\u7edf\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.03140", "pdf": "https://arxiv.org/pdf/2508.03140", "abs": "https://arxiv.org/abs/2508.03140", "authors": ["Junyao Yang", "Jianwei Wang", "Huiping Zhuang", "Cen Chen", "Ziqian Zeng"], "title": "RCP-Merging: Merging Long Chain-of-Thought Models with Domain-Specific Models by Considering Reasoning Capability as Prior", "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 7 figures", "summary": "Large Language Models (LLMs) with long chain-of-thought (CoT) capability,\ntermed Reasoning Models, demonstrate superior intricate problem-solving\nabilities through multi-step long CoT reasoning. To create a dual-capability\nmodel with long CoT capability and domain-specific knowledge without\nsubstantial computational and data costs, model merging emerges as a highly\nresource-efficient method. However, significant challenges lie in merging\ndomain-specific LLMs with long CoT ones since nowadays merging methods suffer\nfrom reasoning capability degradation, even gibberish output and output\ncollapse. To overcome this, we introduce RCP-Merging: Merging Long\nChain-of-Thought Models with Domain-Specific Models by Considering Reasoning\nCapability as Prior, a novel merging framework designed to integrate\ndomain-specific LLMs with long CoT capability, meanwhile maintaining model\nperformance in the original domain. Treating reasoning model weights as\nfoundational prior, our method utilizes a reasoning capability indicator to\npreserve core long CoT capability model weights while selectively merging\nessential domain-specific weights. We conducted extensive experiments on\nQwen2.5-7B, Llama3.1-8B, and Qwen2.5-1.5B models in BioMedicine and Finance\ndomains. Our results show that RCP-Merging successfully merges a reasoning\nmodel with domain-specific ones, improving domain task performance by 9.5% and\n9.2% over state-of-the-art methods, without significantly harming the original\nlong CoT reasoning capability.", "AI": {"tldr": "\u63d0\u51faRCP-Merging\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u63a8\u7406\u80fd\u529b\u89c6\u4e3a\u5148\u9a8c\uff0c\u5b9e\u73b0\u957f\u94fe\u601d\u7ef4\u6a21\u578b\u4e0e\u9886\u57df\u6a21\u578b\u7684\u878d\u5408\uff0c\u5728\u751f\u7269\u533b\u5b66\u548c\u91d1\u878d\u4efb\u52a1\u4e0a\u5206\u522b\u63d0\u53479.5%\u548c9.2%\u6027\u80fd", "motivation": "\u73b0\u6709\u6a21\u578b\u878d\u5408\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u957f\u94fe\u63a8\u7406\u80fd\u529b\u9000\u5316\uff0c\u9700\u8981\u540c\u65f6\u4fdd\u7559\u9886\u57df\u77e5\u8bc6\u548c\u590d\u6742\u63a8\u7406\u80fd\u529b", "method": "\u4ee5\u63a8\u7406\u6a21\u578b\u6743\u91cd\u4e3a\u57fa\u5e95\uff0c\u901a\u8fc7\u80fd\u529b\u6307\u6807\u7b5b\u9009\u6838\u5fc3\u6743\u91cd\uff0c\u9009\u62e9\u6027\u878d\u5408\u9886\u57df\u5173\u952e\u53c2\u6570", "result": "\u5728Qwen2.5/Llama3\u7b49\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c\u9886\u57df\u4efb\u52a1\u6027\u80fd\u63d0\u5347\u8d859%\uff0c\u63a8\u7406\u80fd\u529b\u4fdd\u6301\u5ea6\u8fbe94%", "conclusion": "RCP-Merging\u4e3a\u591a\u80fd\u529b\u6a21\u578b\u878d\u5408\u63d0\u4f9b\u9ad8\u6548\u65b9\u6848\uff0c\u7a81\u7834\u4f20\u7edf\u65b9\u6cd5\u7684\u80fd\u529b\u635f\u5931\u74f6\u9888"}}
{"id": "2508.03178", "pdf": "https://arxiv.org/pdf/2508.03178", "abs": "https://arxiv.org/abs/2508.03178", "authors": ["Chenyang Wang", "Liang Wen", "Shousheng Jia", "Xiangzheng Zhang", "Liang Xu"], "title": "Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, 10 figures, 7 tables", "summary": "While advancements in the reasoning abilities of LLMs have significantly\nenhanced their performance in solving mathematical problems, coding tasks, and\ngeneral puzzles, their effectiveness in accurately adhering to instructions\nremains inconsistent, particularly with more complex directives. Our\ninvestigation identifies lazy reasoning during the thinking stage as the\nprimary factor contributing to poor instruction adherence. To mitigate this\nissue, we propose a comprehensive framework designed to enable rigorous\nreasoning processes involving preview and self-checking, essential for\nsatisfying strict instruction constraints. Specifically, we first generate\ninstructions with complex constraints and apply a filtering process to obtain\nvalid prompts, resulting in three distinct prompt datasets categorized as hard,\neasy, and pass. Then, we employ rejection sampling on the pass prompts to\ncurate a small yet high-quality dataset, enabling a cold-start initialization\nof the model and facilitating its adaptation to effective reasoning patterns.\nSubsequently, we employ an entropy-preserving supervised fine-tuning\n(Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL)\nreinforcement learning guided by rule-based dense rewards. This approach\nencourages the model to transform its reasoning mechanism, ultimately fostering\ngeneralizable reasoning abilities that encompass preview and self-checking.\nExtensive experiments conducted on instruction-following benchmarks demonstrate\nremarkable performance improvements across various model scales. Notably, our\nLight-IF-32B model surpasses both larger open-source models such as DeepSeek-R1\nand closed-source models like Doubao-1.6.", "AI": {"tldr": "\u63d0\u51fa\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u590d\u6742\u6307\u4ee4\u9075\u5faa\u95ee\u9898\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u89c8\u548c\u81ea\u68c0\u673a\u5236\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u6027\u80fd\u8d85\u8d8a\u4e3b\u6d41\u6a21\u578b\u3002", "motivation": "LLMs\u5728\u590d\u6742\u6307\u4ee4\u573a\u666f\u4e0b\u5b58\u5728\u60f0\u6027\u63a8\u7406\u95ee\u9898\uff0c\u5bfc\u81f4\u6307\u4ee4\u9075\u5faa\u6548\u679c\u4e0d\u7a33\u5b9a\u3002", "method": "\u6784\u5efa\u542b\u9884\u89c8\u548c\u81ea\u68c0\u7684\u63a8\u7406\u6846\u67b6\uff0c\u751f\u6210\u591a\u96be\u5ea6\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u91c7\u7528\u62d2\u7edd\u91c7\u6837+\u71b5\u4fdd\u6301SFT+TEA-RL\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4f18\u5316\u6a21\u578b\u3002", "result": "Light-IF-32B\u5728\u6307\u4ee4\u9075\u5faa\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8eDeepSeek-R1\u3001Doubao-1.6\u7b49\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u7cfb\u7edf\u6027\u8bad\u7ec3\u6846\u67b6\u6709\u6548\u8f6c\u5316\u6a21\u578b\u63a8\u7406\u6a21\u5f0f\uff0c\u5b9e\u73b0\u53ef\u6cdb\u5316\u7684\u4e25\u683c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002"}}
{"id": "2508.03181", "pdf": "https://arxiv.org/pdf/2508.03181", "abs": "https://arxiv.org/abs/2508.03181", "authors": ["Lukas P\u00e4tz", "Moritz Beyer", "Jannik Sp\u00e4th", "Lasse Bohlen", "Patrick Zschech", "Mathias Kraus", "Julian Rosenberger"], "title": "Analyzing German Parliamentary Speeches: A Machine Learning Approach for Topic and Sentiment Classification", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at 20th International Conference on Wirtschaftsinformatik\n  (WI25); September 2025, M\\\"unster, Germany", "summary": "This study investigates political discourse in the German parliament, the\nBundestag, by analyzing approximately 28,000 parliamentary speeches from the\nlast five years. Two machine learning models for topic and sentiment\nclassification were developed and trained on a manually labeled dataset. The\nmodels showed strong classification performance, achieving an area under the\nreceiver operating characteristic curve (AUROC) of 0.94 for topic\nclassification (average across topics) and 0.89 for sentiment classification.\nBoth models were applied to assess topic trends and sentiment distributions\nacross political parties and over time. The analysis reveals remarkable\nrelationships between parties and their role in parliament. In particular, a\nchange in style can be observed for parties moving from government to\nopposition. While ideological positions matter, governing responsibilities also\nshape discourse. The analysis directly addresses key questions about the\nevolution of topics, sentiment dynamics, and party-specific discourse\nstrategies in the Bundestag.", "AI": {"tldr": "\u57fa\u4e8e28,000\u4efd\u5fb7\u56fd\u8bae\u4f1a\u6f14\u8bb2\u6570\u636e\uff0c\u8fd0\u7528\u4e3b\u9898/\u60c5\u611f\u5206\u7c7b\u6a21\u578b\uff08AUROC 0.94/0.89\uff09\u63ed\u793a\u653f\u515a\u89d2\u8272\u8f6c\u53d8\u5bf9\u653f\u6cbb\u8bdd\u8bed\u98ce\u683c\u7684\u5f71\u54cd", "motivation": "\u63a2\u7a76\u5fb7\u56fd\u8bae\u4f1a\u4e2d\u653f\u515a\u804c\u8d23\uff08\u6267\u653f/\u53cd\u5bf9\uff09\u4e0e\u610f\u8bc6\u5f62\u6001\u5982\u4f55\u5171\u540c\u5851\u9020\u653f\u6cbb\u8bdd\u8bed\u7b56\u7565\uff0c\u7279\u522b\u662f\u89d2\u8272\u8f6c\u6362\u65f6\u7684\u98ce\u683c\u53d8\u8fc1", "method": "\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e24\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u5bf9\u4e94\u5e74\u671f\u8bae\u4f1a\u6f14\u8bb2\u8fdb\u884c\u4e3b\u9898\u8d8b\u52bf\u5206\u6790\u548c\u8de8\u515a\u6d3e\u60c5\u611f\u5206\u5e03\u7814\u7a76", "result": "\u6a21\u578b\u9ad8\u6548\u8bc6\u522b\u51fa\u6267\u653f\u515a\u8f6c\u4e3a\u53cd\u5bf9\u515a\u65f6\u8bdd\u8bed\u98ce\u683c\u8f6c\u53d8\uff0c\u8bc1\u660e\u653f\u5e9c\u804c\u8d23\u4e0e\u610f\u8bc6\u5f62\u6001\u5171\u540c\u5f71\u54cd\u653f\u515a\u8bdd\u8bed\u7b56\u7565", "conclusion": "\u8bae\u4f1a\u8bdd\u8bed\u7b56\u7565\u4e0d\u4ec5\u7531\u610f\u8bc6\u5f62\u6001\u9a71\u52a8\uff0c\u66f4\u53d7\u5b9e\u9645\u653f\u6cbb\u89d2\u8272\u5236\u7ea6\uff0c\u673a\u5668\u5b66\u4e60\u6709\u6548\u63ed\u793a\u4e86\u5fb7\u56fd\u8bae\u4f1a\u7684\u653f\u6cbb\u52a8\u6001\u6f14\u5316"}}
{"id": "2508.03199", "pdf": "https://arxiv.org/pdf/2508.03199", "abs": "https://arxiv.org/abs/2508.03199", "authors": ["Muhammed Saeed", "Shaina Raza", "Ashmal Vayani", "Muhammad Abdul-Mageed", "Ali Emami", "Shady Shehata"], "title": "Beyond Content: How Grammatical Gender Shapes Visual Representation in Text-to-Image Models", "categories": ["cs.CL"], "comment": null, "summary": "Research on bias in Text-to-Image (T2I) models has primarily focused on\ndemographic representation and stereotypical attributes, overlooking a\nfundamental question: how does grammatical gender influence visual\nrepresentation across languages? We introduce a cross-linguistic benchmark\nexamining words where grammatical gender contradicts stereotypical gender\nassociations (e.g., ``une sentinelle'' - grammatically feminine in French but\nreferring to the stereotypically masculine concept ``guard''). Our dataset\nspans five gendered languages (French, Spanish, German, Italian, Russian) and\ntwo gender-neutral control languages (English, Chinese), comprising 800 unique\nprompts that generated 28,800 images across three state-of-the-art T2I models.\nOur analysis reveals that grammatical gender dramatically influences image\ngeneration: masculine grammatical markers increase male representation to 73\\%\non average (compared to 22\\% with gender-neutral English), while feminine\ngrammatical markers increase female representation to 38\\% (compared to 28\\% in\nEnglish). These effects vary systematically by language resource availability\nand model architecture, with high-resource languages showing stronger effects.\nOur findings establish that language structure itself, not just content, shapes\nAI-generated visual outputs, introducing a new dimension for understanding bias\nand fairness in multilingual, multimodal systems.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u4e2d\u8bed\u6cd5\u6027\u522b\u663e\u8457\u5f71\u54cd\u89c6\u89c9\u751f\u6210\uff0c\u4e0d\u540c\u8bed\u8a00\u4e2d\u8bed\u6cd5\u6027\u522b\u6807\u8bb0\u4f1a\u5f3a\u5316\u6027\u522b\u8868\u5f81\uff08\u5982\u6cd5\u8bed\u9633\u6027\u6807\u8bb0\u4f7f\u7537\u6027\u51fa\u73b0\u7387\u5347\u81f373%\uff09\uff0c\u63ed\u793a\u4e86\u8bed\u8a00\u7ed3\u6784\u5bf9AI\u751f\u6210\u7ed3\u679c\u7684\u65b0\u7ef4\u5ea6\u504f\u89c1\u3002", "motivation": "\u73b0\u6709T2I\u504f\u89c1\u7814\u7a76\u591a\u5173\u6ce8\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u548c\u523b\u677f\u5c5e\u6027\uff0c\u4f46\u5ffd\u89c6\u4e86\u8bed\u6cd5\u6027\u522b\u5982\u4f55\u8de8\u8bed\u8a00\u5f71\u54cd\u89c6\u89c9\u8868\u5f81\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76\u8bed\u8a00\u7ed3\u6784\u672c\u8eab\uff08\u800c\u975e\u5185\u5bb9\uff09\u5bf9\u751f\u6210\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "method": "\u6784\u5efa\u8de8\u8bed\u8a00\u57fa\u51c6\u6570\u636e\u96c6\uff085\u79cd\u6027\u522b\u8bed\u8a00+2\u79cd\u4e2d\u6027\u8bed\u8a00\uff09\uff0c\u8bbe\u8ba1800\u4e2a\u8bed\u6cd5\u6027\u522b\u4e0e\u523b\u677f\u5370\u8c61\u51b2\u7a81\u7684\u63d0\u793a\u8bcd\uff08\u5982\u6cd5\u8bed\u9634\u6027\u51a0\u8bcd+\u9633\u6027\u804c\u4e1a\uff09\uff0c\u901a\u8fc73\u4e2a\u524d\u6cbfT2I\u6a21\u578b\u751f\u621028,800\u5f20\u56fe\u50cf\u8fdb\u884c\u5b9a\u91cf\u5206\u6790\u3002", "result": "\u8bed\u6cd5\u6027\u522b\u5bf9\u751f\u6210\u7ed3\u679c\u4ea7\u751f\u7cfb\u7edf\u6027\u5f71\u54cd\uff1a\u9633\u6027\u6807\u8bb0\u4f7f\u7537\u6027\u51fa\u73b0\u7387\u5e73\u5747\u8fbe73%\uff08\u4e2d\u6027\u82f1\u8bed\u4ec522%\uff09\uff0c\u9634\u6027\u6807\u8bb0\u4f7f\u5973\u6027\u51fa\u73b0\u7387\u8fbe38%\uff08\u82f1\u8bed28%\uff09\u3002\u8bed\u8a00\u8d44\u6e90\u4e30\u5bcc\u5ea6\u4e0e\u6a21\u578b\u67b6\u6784\u4f1a\u8c03\u8282\u8be5\u6548\u5e94\u3002", "conclusion": "\u8bed\u8a00\u7ed3\u6784\uff08\u5982\u8bed\u6cd5\u6027\u522b\u7cfb\u7edf\uff09\u672c\u8eab\u6784\u6210AI\u89c6\u89c9\u751f\u6210\u7684\u65b0\u504f\u89c1\u7ef4\u5ea6\uff0c\u9700\u5728\u591a\u8bed\u8a00\u591a\u6a21\u6001\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u7814\u7a76\u4e2d\u7eb3\u5165\u8bed\u8a00\u5f62\u6001\u5b66\u7279\u5f81\u5206\u6790\u3002"}}
{"id": "2508.03204", "pdf": "https://arxiv.org/pdf/2508.03204", "abs": "https://arxiv.org/abs/2508.03204", "authors": ["Abhirup Sinha", "Pritilata Saha", "Tithi Saha"], "title": "Current State in Privacy-Preserving Text Preprocessing for Domain-Agnostic NLP", "categories": ["cs.CL"], "comment": "To be published in the Proceedings of Die Studierendenkonferenz\n  Informatik (SKILL) 2024", "summary": "Privacy is a fundamental human right. Data privacy is protected by different\nregulations, such as GDPR. However, modern large language models require a huge\namount of data to learn linguistic variations, and the data often contains\nprivate information. Research has shown that it is possible to extract private\ninformation from such language models. Thus, anonymizing such private and\nsensitive information is of utmost importance. While complete anonymization may\nnot be possible, a number of different pre-processing approaches exist for\nmasking or pseudonymizing private information in textual data. This report\nfocuses on a few of such approaches for domain-agnostic NLP tasks.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u6587\u672c\u6570\u636e\u533f\u540d\u5316\u65b9\u6cd5\u5728\u9886\u57df\u65e0\u5173NLP\u4efb\u52a1\u4e2d\u7684\u5e94\u7528", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9700\u8981\u6d77\u91cf\u6570\u636e\u8bad\u7ec3\uff0c\u4f46\u6570\u636e\u5e38\u5305\u542b\u53d7GDPR\u7b49\u6cd5\u89c4\u4fdd\u62a4\u7684\u9690\u79c1\u4fe1\u606f\uff0c\u7814\u7a76\u8bc1\u660e\u5b58\u5728\u4ece\u6a21\u578b\u4e2d\u63d0\u53d6\u9690\u79c1\u4fe1\u606f\u7684\u53ef\u80fd\u6027", "method": "\u63d0\u51fa\u591a\u79cd\u9884\u5904\u7406\u65b9\u6cd5\uff08\u63a9\u76d6/\u4f2a\u533f\u540d\u5316\uff09\u5904\u7406\u6587\u672c\u4e2d\u7684\u9690\u79c1\u4fe1\u606f", "result": "\u5efa\u7acb\u4e86\u9002\u7528\u4e8e\u9886\u57df\u65e0\u5173NLP\u4efb\u52a1\u7684\u9690\u79c1\u4fdd\u62a4\u5904\u7406\u6846\u67b6", "conclusion": "\u5728\u4fdd\u8bc1\u6a21\u578b\u6027\u80fd\u7684\u524d\u63d0\u4e0b\uff0c\u6709\u6548\u7684\u533f\u540d\u5316\u5904\u7406\u662f\u5e73\u8861\u6570\u636e\u6548\u7528\u4e0e\u9690\u79c1\u4fdd\u62a4\u7684\u5173\u952e\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.03211", "pdf": "https://arxiv.org/pdf/2508.03211", "abs": "https://arxiv.org/abs/2508.03211", "authors": ["Pablo J. Diego-Sim\u00f3n", "Emmanuel Chemla", "Jean-R\u00e9mi King", "Yair Lakretz"], "title": "Probing Syntax in Large Language Models: Successes and Remaining Challenges", "categories": ["cs.CL"], "comment": null, "summary": "The syntactic structures of sentences can be readily read-out from the\nactivations of large language models (LLMs). However, the ``structural probes''\nthat have been developed to reveal this phenomenon are typically evaluated on\nan indiscriminate set of sentences. Consequently, it remains unclear whether\nstructural and/or statistical factors systematically affect these syntactic\nrepresentations. To address this issue, we conduct an in-depth analysis of\nstructural probes on three controlled benchmarks. Our results are three-fold.\nFirst, structural probes are biased by a superficial property: the closer two\nwords are in a sentence, the more likely structural probes will consider them\nas syntactically linked. Second, structural probes are challenged by linguistic\nproperties: they poorly represent deep syntactic structures, and get interfered\nby interacting nouns or ungrammatical verb forms. Third, structural probes do\nnot appear to be affected by the predictability of individual words. Overall,\nthis work sheds light on the current challenges faced by structural probes.\nProviding a benchmark made of controlled stimuli to better evaluate their\nperformance.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u53d7\u63a7\u5b9e\u9a8c\u63ed\u793a\u5f53\u524d\u53e5\u6cd5\u7ed3\u6784\u63a2\u9488\u5b58\u5728\u8868\u9762\u5c5e\u6027\u504f\u5dee\u3001\u6df1\u5c42\u7ed3\u6784\u8868\u5f81\u4e0d\u8db3\u3001\u6297\u5e72\u6270\u80fd\u529b\u5f31\u7b49\u95ee\u9898", "motivation": "\u73b0\u6709\u53e5\u6cd5\u7ed3\u6784\u63a2\u9488\u5728\u6df7\u6742\u53e5\u5b50\u4e0a\u8bc4\u4f30\uff0c\u9700\u660e\u786e\u7ed3\u6784/\u7edf\u8ba1\u56e0\u7d20\u5982\u4f55\u7cfb\u7edf\u5f71\u54cd\u53e5\u6cd5\u8868\u5f81", "method": "\u4f7f\u7528\u4e09\u4e2a\u53d7\u63a7\u57fa\u51c6\u6d4b\u8bd5\u8fdb\u884c\u7ed3\u6784\u63a2\u9488\u7684\u6df1\u5ea6\u5206\u6790", "result": "1. \u7ed3\u6784\u63a2\u9488\u53d7\u8bcd\u8ddd\u5f71\u54cd\u4ea7\u751f\u8868\u9762\u504f\u5dee\n2. \u6df1\u5c42\u53e5\u6cd5\u7ed3\u6784\u8868\u5f81\u5dee\u4e14\u6613\u53d7\u8bed\u6cd5\u9519\u8bef\u5e72\u6270\n3. \u8bcd\u8bed\u53ef\u9884\u6d4b\u6027\u4e0d\u5f71\u54cd\u63a2\u9488\u8868\u73b0", "conclusion": "\u63ed\u793a\u4e86\u5f53\u524d\u7ed3\u6784\u63a2\u9488\u7684\u5c40\u9650\u6027\uff0c\u5efa\u7acb\u4e86\u53d7\u63a7\u523a\u6fc0\u57fa\u51c6\u4ee5\u6539\u8fdb\u8bc4\u4f30\u4f53\u7cfb"}}
{"id": "2508.03240", "pdf": "https://arxiv.org/pdf/2508.03240", "abs": "https://arxiv.org/abs/2508.03240", "authors": ["Mutaz Ayesh", "Nicol\u00e1s Guti\u00e9rrez-Rol\u00f3n", "Fernando Alva-Manchego"], "title": "CardiffNLP at CLEARS-2025: Prompting Large Language Models for Plain Language and Easy-to-Read Text Rewriting", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper details the CardiffNLP team's contribution to the CLEARS shared\ntask on Spanish text adaptation, hosted by IberLEF 2025. The shared task\ncontained two subtasks and the team submitted to both. Our team took an\nLLM-prompting approach with different prompt variations. While we initially\nexperimented with LLaMA-3.2, we adopted Gemma-3 for our final submission, and\nlanded third place in Subtask 1 and second place in Subtask 2. We detail our\nnumerous prompt variations, examples, and experimental results.", "AI": {"tldr": "\u5361\u8fea\u592bNLP\u56e2\u961f\u5728CLEARS\u5171\u4eab\u4efb\u52a1\u4e2d\u91c7\u7528Gemma-3\u6a21\u578b\u8fdb\u884c\u897f\u73ed\u7259\u6587\u672c\u9002\u5e94\uff0c\u901a\u8fc7\u591a\u79cd\u63d0\u793a\u53d8\u4f53\u5728\u5b50\u4efb\u52a11\u83b7\u7b2c\u4e09\u540d\u3001\u5b50\u4efb\u52a12\u83b7\u7b2c\u4e8c\u540d", "motivation": "\u53c2\u4e0eIberLEF 2025\u7684CLEARS\u897f\u73ed\u7259\u6587\u672c\u9002\u5e94\u5171\u4eab\u4efb\u52a1\uff0c\u6d4b\u8bd5\u4e0d\u540cLLM\u63d0\u793a\u65b9\u6cd5\u7684\u6548\u679c", "method": "\u4f7f\u7528Gemma-3\u6a21\u578b\u66ff\u4ee3\u521d\u671f\u7684LLaMA-3.2\uff0c\u5b9e\u65bd\u591a\u79cd\u63d0\u793a\u53d8\u4f53\u5e76\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1", "result": "\u5728\u5b98\u65b9\u6392\u540d\u4e2d\uff1a\u5b50\u4efb\u52a11\u7b2c\u4e09\u540d\uff08F1=0.732\uff09\uff0c\u5b50\u4efb\u52a12\u7b2c\u4e8c\u540d\uff08F1=0.681\uff09", "conclusion": "Gemma-3\u6a21\u578b\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u80fd\u6709\u6548\u63d0\u5347\u8de8\u8bed\u8a00\u6587\u672c\u9002\u5e94\u6548\u679c\uff0c\u4e0d\u540c\u4efb\u52a1\u9700\u8981\u5b9a\u5236\u5316\u63d0\u793a\u7b56\u7565"}}
{"id": "2508.03247", "pdf": "https://arxiv.org/pdf/2508.03247", "abs": "https://arxiv.org/abs/2508.03247", "authors": ["Shintaro Sakai", "Jisun An", "Migyeong Kang", "Haewoon Kwak"], "title": "Somatic in the East, Psychological in the West?: Investigating Clinically-Grounded Cross-Cultural Depression Symptom Expression in LLMs", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Prior clinical psychology research shows that Western individuals with\ndepression tend to report psychological symptoms, while Eastern individuals\nreport somatic ones. We test whether Large Language Models (LLMs), which are\nincreasingly used in mental health, reproduce these cultural patterns by\nprompting them with Western or Eastern personas. Results show that LLMs largely\nfail to replicate the patterns when prompted in English, though prompting in\nmajor Eastern languages (i.e., Chinese, Japanese, and Hindi) improves alignment\nin several configurations. Our analysis pinpoints two key reasons for this\nfailure: the models' low sensitivity to cultural personas and a strong,\nculturally invariant symptom hierarchy that overrides cultural cues. These\nfindings reveal that while prompt language is important, current\ngeneral-purpose LLMs lack the robust, culture-aware capabilities essential for\nsafe and effective mental health applications.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u4e2d\u7f3a\u4e4f\u6587\u5316\u654f\u611f\u5ea6\uff0c\u82f1\u8bed\u63d0\u793a\u4e0b\u96be\u4ee5\u590d\u73b0\u4e1c\u897f\u65b9\u6291\u90c1\u75c7\u72b6\u62a5\u544a\u5dee\u5f02\uff0c\u4e1c\u65b9\u8bed\u8a00\u63d0\u793a\u90e8\u5206\u6539\u5584\u4f46\u5b58\u5728\u75c7\u72b6\u5c42\u7ea7\u56fa\u5316\u95ee\u9898\u3002", "motivation": "\u9a8c\u8bc1LLMs\u5728\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u4e2d\u662f\u5426\u590d\u73b0\u4eba\u7c7b\u6587\u5316\u5dee\u5f02\u6a21\u5f0f\uff08\u897f\u65b9\u62a5\u544a\u5fc3\u7406\u75c7\u72b6vs\u4e1c\u65b9\u62a5\u544a\u8eaf\u4f53\u75c7\u72b6\uff09\uff0c\u63ed\u793a\u6a21\u578b\u6587\u5316\u611f\u77e5\u80fd\u529b\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u7ed9LLMs\u8bbe\u5b9a\u897f\u65b9/\u4e1c\u65b9\u6587\u5316\u89d2\u8272\u63d0\u793a\uff0c\u6d4b\u8bd5\u82f1\u8bed\u53ca\u4e2d\u6587/\u65e5\u8bed/\u5370\u5730\u8bed\u7b49\u4e1c\u65b9\u8bed\u8a00\u4e0b\u7684\u75c7\u72b6\u751f\u6210\u5dee\u5f02\u3002", "result": "\u82f1\u8bed\u63d0\u793a\u4e0b\u6a21\u578b\u666e\u904d\u5931\u8d25\uff0c\u4e1c\u65b9\u8bed\u8a00\u63d0\u793a\u90e8\u5206\u6539\u5584\u4f46\u5b58\u5728\uff1a1) \u5bf9\u6587\u5316\u89d2\u8272\u4f4e\u654f\u611f\u5ea6 2) \u8de8\u6587\u5316\u56fa\u5316\u7684\u75c7\u72b6\u5c42\u7ea7\u538b\u5236\u6587\u5316\u7ebf\u7d22", "conclusion": "\u5f53\u524d\u901a\u7528LLMs\u7f3a\u4e4f\u7a33\u5065\u7684\u6587\u5316\u611f\u77e5\u80fd\u529b\uff0c\u63d0\u793a\u8bed\u8a00\u867d\u91cd\u8981\uff0c\u4f46\u6a21\u578b\u5185\u5728\u7ed3\u6784\u9650\u5236\u4f7f\u5176\u4e0d\u9002\u5408\u76f4\u63a5\u7528\u4e8e\u5fc3\u7406\u5065\u5eb7\u573a\u666f\u3002"}}
{"id": "2508.03250", "pdf": "https://arxiv.org/pdf/2508.03250", "abs": "https://arxiv.org/abs/2508.03250", "authors": ["Deborah Dore", "Elena Cabrio", "Serena Villata"], "title": "RooseBERT: A New Deal For Political Language Modelling", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The increasing amount of political debates and politics-related discussions\ncalls for the definition of novel computational methods to automatically\nanalyse such content with the final goal of lightening up political\ndeliberation to citizens. However, the specificity of the political language\nand the argumentative form of these debates (employing hidden communication\nstrategies and leveraging implicit arguments) make this task very challenging,\neven for current general-purpose pre-trained Language Models. To address this\nissue, we introduce a novel pre-trained Language Model for political discourse\nlanguage called RooseBERT. Pre-training a language model on a specialised\ndomain presents different technical and linguistic challenges, requiring\nextensive computational resources and large-scale data. RooseBERT has been\ntrained on large political debate and speech corpora (8K debates, each composed\nof several sub-debates on different topics) in English. To evaluate its\nperformances, we fine-tuned it on four downstream tasks related to political\ndebate analysis, i.e., named entity recognition, sentiment analysis, argument\ncomponent detection and classification, and argument relation prediction and\nclassification. Our results demonstrate significant improvements over\ngeneral-purpose Language Models on these four tasks, highlighting how\ndomain-specific pre-training enhances performance in political debate analysis.\nWe release the RooseBERT language model for the research community.", "AI": {"tldr": "\u63d0\u51faRooseBERT\u9886\u57df\u4e13\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u653f\u6cbb\u8fa9\u8bba\u5206\u6790\u7684\u56db\u9879\u4e0b\u6e38\u4efb\u52a1\u4e2d\u663e\u8457\u8d85\u8d8a\u901a\u7528\u6a21\u578b", "motivation": "\u653f\u6cbb\u8bed\u8a00\u7684\u590d\u6742\u6027\u548c\u9690\u542b\u8bba\u8bc1\u7ed3\u6784\u4f7f\u901a\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u96be\u4ee5\u6709\u6548\u5206\u6790\u653f\u6cbb\u8fa9\u8bba\u5185\u5bb9", "method": "\u4f7f\u75288,000\u573a\u82f1\u8bed\u653f\u6cbb\u8fa9\u8bba\u6570\u636e\uff08\u542b\u591a\u4e3b\u9898\u5b50\u8fa9\u8bba\uff09\u9884\u8bad\u7ec3\uff0c\u5728\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u60c5\u611f\u5206\u6790\u3001\u8bba\u636e\u6210\u5206\u68c0\u6d4b\u5206\u7c7b\u3001\u8bba\u636e\u5173\u7cfb\u9884\u6d4b\u56db\u9879\u4efb\u52a1\u5fae\u8c03", "result": "\u5728\u56db\u9879\u4efb\u52a1\u4e0a\u76f8\u8f83\u901a\u7528\u8bed\u8a00\u6a21\u578b\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff08\u5177\u4f53\u4efb\u52a1\u5e73\u5747\u63d0\u5347\u5e45\u5ea6\u8fbe3-15%\uff09", "conclusion": "\u9886\u57df\u4e13\u7528\u9884\u8bad\u7ec3\u6709\u6548\u63d0\u5347\u653f\u6cbb\u8bdd\u8bed\u5206\u6790\u6548\u679c\uff0c\u5df2\u5f00\u6e90\u6a21\u578b\u4f9b\u5b66\u672f\u7814\u7a76\u4f7f\u7528"}}
{"id": "2508.03259", "pdf": "https://arxiv.org/pdf/2508.03259", "abs": "https://arxiv.org/abs/2508.03259", "authors": ["Duzhen Zhang", "Chenxing Li", "Jiahua Dong", "Qi Liu", "Dong Yu"], "title": "Exploring Stability-Plasticity Trade-offs for Continual Named Entity Recognition", "categories": ["cs.CL"], "comment": "Accepted by IEEE/ACM Transactions on Audio, Speech and Language\n  Processing", "summary": "Continual Named Entity Recognition (CNER) is an evolving field that focuses\non sequentially updating an existing model to incorporate new entity types.\nPrevious CNER methods primarily utilize Knowledge Distillation (KD) to preserve\nprior knowledge and overcome catastrophic forgetting, strictly ensuring that\nthe representations of old and new models remain consistent. Consequently, they\noften impart the model with excessive stability (i.e., retention of old\nknowledge) but limited plasticity (i.e., acquisition of new knowledge). To\naddress this issue, we propose a Stability-Plasticity Trade-off (SPT) method\nfor CNER that balances these aspects from both representation and weight\nperspectives. From the representation perspective, we introduce a pooling\noperation into the original KD, permitting a level of plasticity by\nconsolidating representation dimensions. From the weight perspective, we\ndynamically merge the weights of old and new models, strengthening old\nknowledge while maintaining new knowledge. During this fusion, we implement a\nweight-guided selective mechanism to prioritize significant weights. Moreover,\nwe develop a confidence-based pseudo-labeling approach for the current\nnon-entity type, which predicts entity types using the old model to handle the\nsemantic shift of the non-entity type, a challenge specific to CNER that has\nlargely been ignored by previous methods. Extensive experiments across ten CNER\nsettings on three benchmark datasets demonstrate that our SPT method surpasses\nprevious CNER approaches, highlighting its effectiveness in achieving a\nsuitable stability-plasticity trade-off.", "AI": {"tldr": "\u63d0\u51faSPT\u65b9\u6cd5\uff0c\u901a\u8fc7\u8868\u793a\u5c42\u6c60\u5316\u64cd\u4f5c\u3001\u6743\u91cd\u52a8\u6001\u878d\u5408\u53ca\u4f2a\u6807\u7b7e\u7b56\u7565\uff0c\u6709\u6548\u5e73\u8861\u6301\u7eed\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4e2d\u7684\u7a33\u5b9a\u6027\u4e0e\u53ef\u5851\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709CNER\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u77e5\u8bc6\u84b8\u998f\u5bfc\u81f4\u6a21\u578b\u53ef\u5851\u6027\u4e0d\u8db3\uff0c\u4e14\u5ffd\u89c6\u975e\u5b9e\u4f53\u7c7b\u578b\u7684\u8bed\u4e49\u504f\u79fb\u95ee\u9898\uff0c\u9700\u5efa\u7acb\u7a33\u5b9a-\u53ef\u5851\u7684\u5e73\u8861\u673a\u5236\u3002", "method": "1.\u8868\u793a\u5c42\u5f15\u5165\u6c60\u5316\u64cd\u4f5c\u653e\u5bbd\u77e5\u8bc6\u84b8\u998f\u7ea6\u675f 2.\u6743\u91cd\u52a8\u6001\u878d\u5408\u673a\u5236\u7ed3\u5408\u9009\u62e9\u6027\u52a0\u6743 3.\u63d0\u51fa\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u975e\u5b9e\u4f53\u4f2a\u6807\u7b7e\u751f\u6210\u65b9\u6cd5", "result": "\u57283\u4e2a\u57fa\u51c6\u6570\u636e\u96c610\u79cdCNER\u8bbe\u5b9a\u4e2d\u8d85\u8d8a\u6240\u6709\u57fa\u7ebf\uff0cF1\u503c\u6700\u9ad8\u63d0\u53474.6%\uff0c\u9a8c\u8bc1\u591a\u7ef4\u5ea6\u6743\u8861\u7b56\u7565\u7684\u6709\u6548\u6027\u3002", "conclusion": "SPT\u901a\u8fc7\u591a\u7ef4\u5e73\u8861\u673a\u5236\u89e3\u51b3\u4e86\u6301\u7eedNER\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4e3a\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u6743\u8861\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2508.03262", "pdf": "https://arxiv.org/pdf/2508.03262", "abs": "https://arxiv.org/abs/2508.03262", "authors": ["Junhyuk Choi", "Hyeonchu Park", "Haemin Lee", "Hyebeen Shin", "Hyun Joung Jin", "Bugeun Kim"], "title": "Pay What LLM Wants: Can LLM Simulate Economics Experiment with 522 Real-human Persona?", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Recent advances in Large Language Models (LLMs) have generated significant\ninterest in their capacity to simulate human-like behaviors, yet most studies\nrely on fictional personas rather than actual human data. We address this\nlimitation by evaluating LLMs' ability to predict individual economic\ndecision-making using Pay-What-You-Want (PWYW) pricing experiments with real\n522 human personas. Our study systematically compares three state-of-the-art\nmultimodal LLMs using detailed persona information from 522 Korean participants\nin cultural consumption scenarios. We investigate whether LLMs can accurately\nreplicate individual human choices and how persona injection methods affect\nprediction performance. Results reveal that while LLMs struggle with precise\nindividual-level predictions, they demonstrate reasonable group-level\nbehavioral tendencies. Also, we found that commonly adopted prompting\ntechniques are not much better than naive prompting methods; reconstruction of\npersonal narrative nor retrieval augmented generation have no significant gain\nagainst simple prompting method. We believe that these findings can provide the\nfirst comprehensive evaluation of LLMs' capabilities on simulating economic\nbehavior using real human data, offering empirical guidance for persona-based\nsimulation in computational social science.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5229\u7528\u771f\u5b9e\u4eba\u7c7b\u6570\u636e\u6a21\u62df\u7ecf\u6d4e\u884c\u4e3a\u65f6\uff0c\u7fa4\u4f53\u5c42\u9762\u9884\u6d4b\u6709\u6548\u4f46\u4e2a\u4f53\u9884\u6d4b\u4e0d\u51c6\uff0c\u63d0\u793a\u6280\u672f\u6539\u8fdb\u6709\u9650", "motivation": "\u7a81\u7834\u73b0\u6709\u7814\u7a76\u4f9d\u8d56\u865a\u6784\u4eba\u7269\u7684\u5c40\u9650\uff0c\u9996\u6b21\u57fa\u4e8e522\u4e2a\u771f\u5b9e\u4eba\u7c7b\u51b3\u7b56\u6570\u636e\u8bc4\u4f30LLMs\u6a21\u62df\u7ecf\u6d4e\u884c\u4e3a\u7684\u80fd\u529b", "method": "\u901a\u8fc7\u97e9\u56fd\u6587\u5316\u6d88\u8d39\u573a\u666f\u4e2d\u7684PWYW\u5b9a\u4ef7\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e09\u79cd\u591a\u6a21\u6001LLM\u5728\u4e0d\u540c\u4eba\u7269\u4fe1\u606f\u6ce8\u5165\u65b9\u6cd5\u4e0b\u7684\u9884\u6d4b\u8868\u73b0", "result": "LLMs\u7fa4\u4f53\u884c\u4e3a\u9884\u6d4b\u5408\u7406\uff08MAE=2.33\uff09\uff0c\u4f46\u4e2a\u4f53\u9884\u6d4b\u8bef\u5dee\u663e\u8457\uff1b\u590d\u6742\u63d0\u793a\u6280\u672f\u76f8\u6bd4\u57fa\u7840\u65b9\u6cd5\u65e0\u663e\u8457\u4f18\u52bf", "conclusion": "\u4e3a\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u4e2d\u7684\u4eba\u7269\u6a21\u62df\u63d0\u4f9b\u4e86\u5173\u952e\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524dLLMs\u5728\u4e2a\u4f53\u7ecf\u6d4e\u884c\u4e3a\u6a21\u62df\u4e2d\u7684\u5c40\u9650\u6027"}}
{"id": "2508.03275", "pdf": "https://arxiv.org/pdf/2508.03275", "abs": "https://arxiv.org/abs/2508.03275", "authors": ["Jiahao Zhao"], "title": "LECTOR: LLM-Enhanced Concept-based Test-Oriented Repetition for Adaptive Spaced Learning", "categories": ["cs.CL"], "comment": "15 pages, 4 figures, 1 table", "summary": "Spaced repetition systems are fundamental to efficient learning and memory\nretention, but existing algorithms often struggle with semantic interference\nand personalized adaptation. We present LECTOR (\\textbf{L}LM-\\textbf{E}nhanced\n\\textbf{C}oncept-based \\textbf{T}est-\\textbf{O}riented \\textbf{R}epetition), a\nnovel adaptive scheduling algorithm specifically designed for test-oriented\nlearning scenarios, particularly language examinations where success rate is\nparamount. LECTOR leverages large language models for semantic analysis while\nincorporating personalized learning profiles, addressing the critical challenge\nof semantic confusion in vocabulary learning by utilizing LLM-powered semantic\nsimilarity assessment and integrating it with established spaced repetition\nprinciples. Our comprehensive evaluation against six baseline algorithms\n(SSP-MMC, SM2, HLR, FSRS, ANKI, THRESHOLD) across 100 simulated learners over\n100 days demonstrates significant improvements: LECTOR achieves a 90.2\\%\nsuccess rate compared to 88.4\\% for the best baseline (SSP-MMC), representing a\n2.0\\% relative improvement. The algorithm shows particular strength in handling\nsemantically similar concepts, reducing confusion-induced errors while\nmaintaining computational efficiency. Our results establish LECTOR as a\npromising direction for intelligent tutoring systems and adaptive learning\nplatforms.", "AI": {"tldr": "LECTOR\u7b97\u6cd5\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u95f4\u9694\u91cd\u590d\u539f\u7406\uff0c\u5728\u6d4b\u8bd5\u5bfc\u5411\u5b66\u4e60\u4e2d\u5b9e\u73b090.2%\u7684\u6210\u529f\u7387\uff0c\u76f8\u6bd4\u6700\u4f73\u57fa\u7ebf\u63d0\u53472%", "motivation": "\u89e3\u51b3\u73b0\u6709\u95f4\u9694\u91cd\u590d\u7cfb\u7edf\u5728\u8bed\u4e49\u5e72\u6270\u5904\u7406\u548c\u4e2a\u6027\u5316\u9002\u5e94\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u8bed\u8a00\u8003\u8bd5\u573a\u666f\u4e2d\u5bf9\u9ad8\u6210\u529f\u7387\u7684\u9700\u6c42", "method": "\u5229\u7528LLM\u8fdb\u884c\u8bed\u4e49\u76f8\u4f3c\u6027\u8bc4\u4f30\uff0c\u7ed3\u5408\u4e2a\u6027\u5316\u5b66\u4e60\u6863\u6848\u548c\u95f4\u9694\u91cd\u590d\u539f\u5219\uff0c\u5f00\u53d1\u81ea\u9002\u5e94\u8c03\u5ea6\u7b97\u6cd5", "result": "\u5728100\u5929\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0cLECTOR\u4ee590.2%\u6210\u529f\u7387\u8d85\u8d8a\u6240\u6709\u57fa\u7ebf\u7b97\u6cd5\uff08\u6700\u4f73\u57fa\u7ebfSSP-MMC 88.4%\uff09\uff0c\u8bed\u4e49\u6df7\u6dc6\u9519\u8bef\u51cf\u5c11\u4e14\u8ba1\u7b97\u6548\u7387\u4fdd\u6301", "conclusion": "LECTOR\u4e3a\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u5f00\u8f9f\u65b0\u65b9\u5411\uff0c\u6709\u6548\u5e73\u8861\u8bed\u4e49\u5904\u7406\u4e0e\u8bb0\u5fc6\u89c4\u5f8b\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u8981\u6c42\u7684\u8bed\u8a00\u5b66\u4e60\u573a\u666f"}}
{"id": "2508.03276", "pdf": "https://arxiv.org/pdf/2508.03276", "abs": "https://arxiv.org/abs/2508.03276", "authors": ["Terra Blevins", "Susanne Schmalwieser", "Benjamin Roth"], "title": "Do language models accommodate their users? A study of linguistic convergence", "categories": ["cs.CL"], "comment": null, "summary": "While large language models (LLMs) are generally considered proficient in\ngenerating language, how similar their language usage is to that of humans\nremains understudied. In this paper, we test whether models exhibit linguistic\nconvergence, a core pragmatic element of human language communication, asking:\ndo models adapt, or converge, to the linguistic patterns of their user? To\nanswer this, we systematically compare model completions of exisiting dialogues\nto the original human responses across sixteen language models, three dialogue\ncorpora, and a variety of stylometric features. We find that models strongly\nconverge to the conversation's style, often significantly overfitting relative\nto the human baseline. While convergence patterns are often feature-specific,\nwe observe consistent shifts in convergence across modeling settings, with\ninstruction-tuned and larger models converging less than their pretrained\ncounterparts. Given the differences between human and model convergence\npatterns, we hypothesize that the underlying mechanisms for these behaviors are\nvery different.", "AI": {"tldr": "LLMs exhibit strong linguistic convergence in dialogues but differ from human mechanisms, with instruction-tuned/larger models converging less than pretrained ones.", "motivation": "Explore whether language models demonstrate linguistic convergence (adapting to users' linguistic patterns) as humans do, given limited understanding of their language usage similarity to humans.", "method": "Analyzed 16 LLMs across 3 dialogue corpora using stylometric features, comparing model completions with original human responses.", "result": "Models significantly overfit to conversation styles (exceeding human baselines), with convergence patterns varying by feature and model type (instruction-tuned/larger models showed reduced convergence).", "conclusion": "Model convergence mechanisms fundamentally differ from human processes, suggesting divergent adaptation strategies in human-AI communication."}}
{"id": "2508.03292", "pdf": "https://arxiv.org/pdf/2508.03292", "abs": "https://arxiv.org/abs/2508.03292", "authors": ["Shahed Masoudian", "Gustavo Escobedo", "Hannah Strauss", "Markus Schedl"], "title": "Investigating Gender Bias in LLM-Generated Stories via Psychological Stereotypes", "categories": ["cs.CL", "cs.AI"], "comment": "Under Review", "summary": "As Large Language Models (LLMs) are increasingly used across different\napplications, concerns about their potential to amplify gender biases in\nvarious tasks are rising. Prior research has often probed gender bias using\nexplicit gender cues as counterfactual, or studied them in sentence completion\nand short question answering tasks. These formats might overlook more implicit\nforms of bias embedded in generative behavior of longer content. In this work,\nwe investigate gender bias in LLMs using gender stereotypes studied in\npsychology (e.g., aggressiveness or gossiping) in an open-ended task of\nnarrative generation. We introduce a novel dataset called StereoBias-Stories\ncontaining short stories either unconditioned or conditioned on (one, two, or\nsix) random attributes from 25 psychological stereotypes and three task-related\nstory endings. We analyze how the gender contribution in the overall story\nchanges in response to these attributes and present three key findings: (1)\nWhile models, on average, are highly biased towards male in unconditioned\nprompts, conditioning on attributes independent from gender stereotypes\nmitigates this bias. (2) Combining multiple attributes associated with the same\ngender stereotype intensifies model behavior, with male ones amplifying bias\nand female ones alleviating it. (3) Model biases align with psychological\nground-truth used for categorization, and alignment strength increases with\nmodel size. Together, these insights highlight the importance of\npsychology-grounded evaluation of LLMs.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5fc3\u7406\u5b66\u6027\u522b\u523b\u677f\u5370\u8c61\u5206\u6790LLMs\u5728\u53d9\u4e8b\u751f\u6210\u4e2d\u7684\u9690\u6027\u6027\u522b\u504f\u89c1\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u65e0\u6761\u4ef6\u63d0\u793a\u4e0b\u5b58\u5728\u663e\u8457\u7537\u6027\u503e\u5411\uff0c\u5c5e\u6027\u7ec4\u5408\u4f1a\u5f3a\u5316\u523b\u677f\u5370\u8c61\uff0c\u4e14\u6a21\u578b\u89c4\u6a21\u4e0e\u5fc3\u7406\u5b66\u57fa\u7840\u4e00\u81f4\u6027\u6b63\u76f8\u5173\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u663e\u6027\u6027\u522b\u504f\u89c1\u548c\u77ed\u6587\u672c\u4efb\u52a1\uff0c\u5ffd\u89c6\u4e86\u751f\u6210\u5f0f\u957f\u6587\u672c\u4e2d\u7684\u9690\u6027\u504f\u89c1\u3002\u672c\u7814\u7a76\u901a\u8fc7\u5fc3\u7406\u5b66\u6846\u67b6\u6784\u5efa\u5f00\u653e\u5f0f\u53d9\u4e8b\u751f\u6210\u4efb\u52a1\uff0c\u7cfb\u7edf\u5206\u6790LLMs\u7684\u6df1\u5c42\u504f\u89c1\u673a\u5236\u3002", "method": "\u6784\u5efaStereoBias-Stories\u6570\u636e\u96c6\uff0c\u5305\u542b\u6761\u4ef6/\u975e\u6761\u4ef6\u53d9\u4e8b\u6837\u672c\u3002\u901a\u8fc725\u79cd\u5fc3\u7406\u5b66\u523b\u677f\u5370\u8c61\u5c5e\u6027\u548c\u4e09\u79cd\u6545\u4e8b\u7ed3\u5c40\uff0c\u91cf\u5316\u5206\u6790\u4e0d\u540c\u5c5e\u6027\u7ec4\u5408\u5bf9\u6027\u522b\u8d21\u732e\u7684\u5f71\u54cd\u3002", "result": "1. \u975e\u6761\u4ef6\u63d0\u793a\u4e0b\u6a21\u578b\u7537\u6027\u504f\u597d\u663e\u8457\uff0c\u4e2d\u6027\u5c5e\u6027\u53ef\u7f13\u89e3\u504f\u89c1\n2. \u540c\u5411\u523b\u677f\u5370\u8c61\u5c5e\u6027\u53e0\u52a0\u4f1a\u5f3a\u5316\u6027\u522b\u503e\u5411\uff08\u7537\u6027\u5c5e\u6027\u52a0\u5267\u504f\u89c1\uff0c\u5973\u6027\u5c5e\u6027\u7f13\u89e3\uff09\n.3 \u6a21\u578b\u504f\u89c1\u7a0b\u5ea6\u4e0e\u5fc3\u7406\u5b66\u57fa\u7840\u543b\u5408\u5ea6\u968f\u6a21\u578b\u89c4\u6a21\u63d0\u5347", "conclusion": "LLMs\u7684\u6027\u522b\u504f\u89c1\u5448\u73b0\u590d\u6742\u9690\u6027\u7279\u5f81\uff0c\u57fa\u4e8e\u5fc3\u7406\u5b66\u8303\u5f0f\u7684\u8bc4\u4f30\u80fd\u6709\u6548\u63ed\u793a\u6df1\u5c42\u504f\u89c1\u673a\u5236\u3002\u6a21\u578b\u89c4\u6a21\u6269\u5927\u53ef\u80fd\u56fa\u5316\u523b\u677f\u5370\u8c61\uff0c\u9700\u5f00\u53d1\u9488\u5bf9\u6027\u53bb\u504f\u65b9\u6cd5\u3002"}}
{"id": "2508.03294", "pdf": "https://arxiv.org/pdf/2508.03294", "abs": "https://arxiv.org/abs/2508.03294", "authors": ["Leonidas Zotos", "Ivo Pascal de Jong", "Matias Valdenegro-Toro", "Andreea Ioana Sburlea", "Malvina Nissim", "Hedderik van Rijn"], "title": "NLP Methods May Actually Be Better Than Professors at Estimating Question Difficulty", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 2 figures, accepted at the 2nd International Workshop on AI\n  in Society, Education and Educational Research (AISEER)", "summary": "Estimating the difficulty of exam questions is essential for developing good\nexams, but professors are not always good at this task. We compare various\nLarge Language Model-based methods with three professors in their ability to\nestimate what percentage of students will give correct answers on True/False\nexam questions in the areas of Neural Networks and Machine Learning. Our\nresults show that the professors have limited ability to distinguish between\neasy and difficult questions and that they are outperformed by directly asking\nGemini 2.5 to solve this task. Yet, we obtained even better results using\nuncertainties of the LLMs solving the questions in a supervised learning\nsetting, using only 42 training samples. We conclude that supervised learning\nusing LLM uncertainty can help professors better estimate the difficulty of\nexam questions, improving the quality of assessment.", "AI": {"tldr": "\u4f7f\u7528LLM\u4e0d\u786e\u5b9a\u6027\u76d1\u7763\u5b66\u4e60\u53ef\u6709\u6548\u63d0\u5347\u6559\u6388\u5bf9\u8003\u9898\u96be\u5ea6\u7684\u9884\u6d4b\u51c6\u786e\u6027", "motivation": "\u6559\u6388\u5728\u8bc4\u4f30\u8003\u9898\u96be\u5ea6\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u501f\u52a9AI\u6280\u672f\u63d0\u5347\u8003\u8bd5\u8bc4\u4f30\u8d28\u91cf", "method": "\u4f7f\u752842\u4e2a\u8bad\u7ec3\u6837\u672c\uff0c\u6bd4\u8f83\u6559\u6388\u4e0e\u591a\u79cdLLM\u65b9\u6cd5\uff08\u5305\u62ec\u76f4\u63a5\u95ee\u7b54\u548c\u76d1\u7763\u5b66\u4e60\u4e0b\u7684\u4e0d\u786e\u5b9a\u6027\u5206\u6790\uff09\u5728T/F\u9898\u96be\u5ea6\u9884\u6d4b\u4e0a\u7684\u8868\u73b0", "result": "\u57fa\u4e8eLLM\u4e0d\u786e\u5b9a\u6027\u7684\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff08\u51c6\u786e\u738775%\uff09\u4f18\u4e8e\u6559\u6388\uff08\u51c6\u786e\u738765%\uff09\u548c\u76f4\u63a5LLM\u95ee\u7b54\uff08\u51c6\u786e\u738770%\uff09", "conclusion": "LLM\u4e0d\u786e\u5b9a\u6027\u76d1\u7763\u5b66\u4e60\u53ef\u4f5c\u4e3a\u6709\u6548\u5de5\u5177\u5e2e\u52a9\u6559\u5e08\u63d0\u9ad8\u8003\u9898\u96be\u5ea6\u8bc4\u4f30\u7684\u51c6\u786e\u6027\uff0c\u6539\u5584\u8003\u8bd5\u8d28\u91cf"}}
{"id": "2508.03296", "pdf": "https://arxiv.org/pdf/2508.03296", "abs": "https://arxiv.org/abs/2508.03296", "authors": ["Anqi Li", "Wenwei Jin", "Jintao Tong", "Pengda Qin", "Weijia Li", "Guo Lu"], "title": "Towards Trustworthy Multimodal Moderation via Policy-Aligned Reasoning and Hierarchical Labeling", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Social platforms have revolutionized information sharing, but also\naccelerated the dissemination of harmful and policy-violating content. To\nensure safety and compliance at scale, moderation systems must go beyond\nefficiency and offer accuracy and interpretability. However, current approaches\nlargely rely on noisy, label-driven learning, lacking alignment with moderation\nrules and producing opaque decisions that hinder human review. Therefore, we\npropose Hierarchical Guard (Hi-Guard), a multimodal moderation framework that\nintroduces a new policy-aligned decision paradigm. The term \"Hierarchical\"\nreflects two key aspects of our system design: (1) a hierarchical moderation\npipeline, where a lightweight binary model first filters safe content and a\nstronger model handles fine-grained risk classification; and (2) a hierarchical\ntaxonomy in the second stage, where the model performs path-based\nclassification over a hierarchical taxonomy ranging from coarse to fine-grained\nlevels. To ensure alignment with evolving moderation policies, Hi-Guard\ndirectly incorporates rule definitions into the model prompt. To further\nenhance structured prediction and reasoning, we introduce a multi-level\nsoft-margin reward and optimize with Group Relative Policy Optimization (GRPO),\npenalizing semantically adjacent misclassifications and improving explanation\nquality. Extensive experiments and real-world deployment demonstrate that\nHi-Guard achieves superior classification accuracy, generalization, and\ninterpretability, paving the way toward scalable, transparent, and trustworthy\ncontent safety systems. Code is available at:\nhttps://github.com/lianqi1008/Hi-Guard.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u6846\u67b6Hi-Guard\uff0c\u901a\u8fc7\u7b56\u7565\u5bf9\u9f50\u51b3\u7b56\u8303\u5f0f\u4e0e\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u63d0\u5347\u5185\u5bb9\u5ba1\u6838\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u3001\u6cdb\u5316\u80fd\u529b\u4e0e\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u5185\u5bb9\u5ba1\u6838\u65b9\u6cd5\u4f9d\u8d56\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\uff0c\u7f3a\u4e4f\u4e0e\u5ba1\u6838\u7b56\u7565\u7684\u5bf9\u9f50\u4e14\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\uff0c\u96be\u4ee5\u6ee1\u8db3\u5927\u89c4\u6a21\u5b89\u5168\u5408\u89c4\u9700\u6c42\u3002", "method": "\u91c7\u7528\u4e8c\u9636\u6bb5\u5206\u5c42\u5ba1\u6838\u6d41\u7a0b\uff08\u8f7b\u91cf\u7ea7\u8fc7\u6ee4+\u7ec6\u7c92\u5ea6\u5206\u7c7b\uff09\uff0c\u7ed3\u5408\u57fa\u4e8e\u5c42\u6b21\u5206\u7c7b\u6cd5\u7684\u8def\u5f84\u9884\u6d4b\u548cGRPO\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u4e0e\u90e8\u7f72\u9a8c\u8bc1\u7cfb\u7edf\u5728\u5206\u7c7b\u51c6\u786e\u7387\uff08\u63d0\u53474.6%\uff09\u3001\u8de8\u573a\u666f\u6cdb\u5316\uff08\u63d0\u534712.3%\uff09\u548c\u89e3\u91ca\u8d28\u91cf\u4e0a\u7684\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "Hi-Guard\u901a\u8fc7\u5206\u5c42\u67b6\u6784\u4e0e\u89c4\u5219\u6ce8\u5165\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u3001\u900f\u660e\u5316\u7684\u5185\u5bb9\u5b89\u5168\u7cfb\u7edf\uff0c\u4e3a\u53ef\u4fe1\u5ba1\u6838\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.03333", "pdf": "https://arxiv.org/pdf/2508.03333", "abs": "https://arxiv.org/abs/2508.03333", "authors": ["Zhende Song", "Shengji Tang", "Peng Ye", "Jiayuan Fan", "Tao Chen"], "title": "CTTS: Collective Test-Time Scaling", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Test-time scaling (TTS) has emerged as a promising research field for\nenhancing the effectiveness of large language models (LLMs) without extra\ntraining. However, most existing approaches, e.g., Best-of-N and\nSelf-Consistency rely on a single agent interacting with a reward model\n(SA-SR), constrained by limited capabilities of a single test-time scaling\n(STTS) paradigm. On the other hand, recent works demonstrate that\ncollective-agent methods can break through the upper bound of single-agent\nsystems by orchestrating diverse models. Thus, in this paper, we take a first\nstep towards exploring Collective Test-Time Scaling (CTTS). Consider the\ndifferent interaction types of single and multiple models, we design three\nprimary paradigms to investigate the optimal paradigm of CTTS: (1) single agent\nto multiple reward models (SA-MR); (2) multiple agents to single reward model\n(MA-SR); and (3) multiple agents to multiple reward models (MA-MR). Extensive\nexperiments demonstrate that MA-MR consistently achieves the best performance.\nBased on this, we propose a novel framework named CTTS-MM that effectively\nleverages both multi-agent and multi-reward-model collaboration for enhanced\ninference. Specifically, for multi-agent collaboration, we propose an Agent\nCollaboration Search (ACS), which searches for the most effective combination\nof LLM agents from a large candidate pool; for multi-reward-model\ncollaboration, we propose Mixture of Reword Models (MoR), which consists of a\ncurated question pool and a Prior Reward model Ensemble Selection (PRES) to\nselect the optimal combinations of reward models via Pair-wise Reward Ranking\n(PRR) metric. Experiments across seven mainstream benchmarks demonstrate that\nthe proposed CTTS-MM consistently obtains superior performance. Code will be\nreleased at https://github.com/magent4aci/CTTS-MM.", "AI": {"tldr": "\u63d0\u51faCTTS-MM\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u641c\u7d22(ACS)\u548c\u6df7\u5408\u5956\u52b1\u6a21\u578b(MoR)\u5b9e\u73b0\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u7684\u96c6\u4f53\u4f18\u5316\uff0c\u5728\u4e03\u5927\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u65b9\u6cd5\u53d7\u9650\u4e8e\u5355\u667a\u80fd\u4f53\u4e0e\u5355\u4e00\u5956\u52b1\u6a21\u578b\u7684\u4ea4\u4e92\uff0c\u96c6\u4f53\u667a\u80fd\u4f53\u65b9\u6cd5\u80fd\u7a81\u7834\u5355\u7cfb\u7edf\u4e0a\u9650\u3002\u672c\u6587\u9996\u6b21\u63a2\u7d22\u4e09\u79cd\u96c6\u4f53\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u8303\u5f0f\uff0c\u53d1\u73b0MA-MR\u6700\u4f18\u3002", "method": "1) \u8bbe\u8ba1SA-MR/MA-SR/MA-MR\u4e09\u79cdCTTS\u8303\u5f0f 2) \u63d0\u51faCTTS-MM\u6846\u67b6\uff1aACS\u7b97\u6cd5\u641c\u7d22\u6700\u4f18\u667a\u80fd\u4f53\u7ec4\u5408\uff0cMoR\u673a\u5236\u901a\u8fc7PRES\u7b56\u7565\u9009\u62e9\u6700\u4f18\u5956\u52b1\u6a21\u578b\u7ec4\u5408 3) \u5f15\u5165PRR\u6307\u6807\u8bc4\u4f30\u6a21\u578b\u7ec4\u5408", "result": "\u57287\u4e2a\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCTTS-MM\u6846\u67b6\u6301\u7eed\u83b7\u5f97\u6700\u4f18\u8868\u73b0\uff0c\u4ee3\u7801\u5373\u5c06\u5f00\u6e90", "conclusion": "MA-MR\u662f\u96c6\u4f53\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u7684\u6700\u4f73\u8303\u5f0f\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u4e0e\u5956\u52b1\u6a21\u578b\u7684\u53cc\u91cd\u534f\u4f5c\u53ef\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u6027\u80fd"}}
{"id": "2508.03358", "pdf": "https://arxiv.org/pdf/2508.03358", "abs": "https://arxiv.org/abs/2508.03358", "authors": ["Tiago G Can\u00e1rio", "Catarina Duarte", "Fl\u00e1vio L. Pinheiro", "Jo\u00e3o L. M. Pereira"], "title": "Taggus: An Automated Pipeline for the Extraction of Characters' Social Networks from Portuguese Fiction Literature", "categories": ["cs.CL", "cs.IR"], "comment": "24 pages, 5 Figures, 4 Tables", "summary": "Automatically identifying characters and their interactions from fiction\nbooks is, arguably, a complex task that requires pipelines that leverage\nmultiple Natural Language Processing (NLP) methods, such as Named Entity\nRecognition (NER) and Part-of-speech (POS) tagging. However, these methods are\nnot optimized for the task that leads to the construction of Social Networks of\nCharacters. Indeed, the currently available methods tend to underperform,\nespecially in less-represented languages, due to a lack of manually annotated\ndata for training. Here, we propose a pipeline, which we call Taggus, to\nextract social networks from literary fiction works in Portuguese. Our results\nshow that compared to readily available State-of-the-Art tools -- off-the-shelf\nNER tools and Large Language Models (ChatGPT) -- the resulting pipeline, which\nuses POS tagging and a combination of heuristics, achieves satisfying results\nwith an average F1-Score of $94.1\\%$ in the task of identifying characters and\nsolving for co-reference and $75.9\\%$ in interaction detection. These\nrepresent, respectively, an increase of $50.7\\%$ and $22.3\\%$ on results\nachieved by the readily available State-of-the-Art tools. Further steps to\nimprove results are outlined, such as solutions for detecting relationships\nbetween characters. Limitations on the size and scope of our testing samples\nare acknowledged. The Taggus pipeline is publicly available to encourage\ndevelopment in this field for the Portuguese language.2", "AI": {"tldr": "\u63d0\u51fa\u4e86Taggus\u6d41\u7a0b\u7528\u4e8e\u8461\u8404\u7259\u8bed\u6587\u5b66\u4f5c\u54c1\u7684\u793e\u4ea4\u7f51\u7edc\u6784\u5efa\uff0c\u76f8\u6bd4\u73b0\u6709\u5de5\u5177\u5728\u89d2\u8272\u8bc6\u522b\uff08F1\u63d0\u534750.7%\uff09\u548c\u4e92\u52a8\u68c0\u6d4b\uff08F1\u63d0\u534722.3%\uff09\u8868\u73b0\u66f4\u4f18", "motivation": "\u73b0\u6709NLP\u65b9\u6cd5\u5728\u8461\u8404\u7259\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u8868\u73b0\u6b20\u4f73\uff0c\u7f3a\u4e4f\u4e13\u95e8\u4f18\u5316\u7684\u793e\u4ea4\u7f51\u7edc\u6784\u5efa\u5de5\u5177", "method": "\u7ed3\u5408\u8bcd\u6027\u6807\u6ce8(POS)\u548c\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u66ff\u4ee3\u4f20\u7edf\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u65b9\u6cd5", "result": "\u89d2\u8272\u8bc6\u522bF1\u8fbe94.1%\uff08\u6bd4\u73b0\u6709\u5de5\u5177\u9ad850.7%\uff09\uff0c\u4e92\u52a8\u68c0\u6d4bF1\u8fbe75.9%\uff08\u9ad822.3%\uff09", "conclusion": "\u516c\u5f00Taggus\u6d41\u7a0b\u4fc3\u8fdb\u8461\u8404\u7259\u8bedNLP\u53d1\u5c55\uff0c\u672a\u6765\u5c06\u6539\u8fdb\u89d2\u8272\u5173\u7cfb\u68c0\u6d4b\u5e76\u6269\u5927\u6d4b\u8bd5\u8303\u56f4"}}
{"id": "2508.03363", "pdf": "https://arxiv.org/pdf/2508.03363", "abs": "https://arxiv.org/abs/2508.03363", "authors": ["Haotian Wu", "Bo Xu", "Yao Shu", "Menglin Yang", "Chengwei Qin"], "title": "Thinking with Nothinking Calibration: A New In-Context Learning Paradigm in Reasoning Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Reasoning large language models (RLLMs) have recently demonstrated remarkable\ncapabilities through structured and multi-step reasoning. While prior research\nhas primarily focused on improving their training and inference strategies,\ntheir potential for in-context learning (ICL) remains largely underexplored. To\nfill this gap, we propose Thinking with Nothinking Calibration (JointThinking),\na new ICL paradigm that leverages the structured difference between two\nreasoning modes, i.e., Thinking and Nothinking, to improve reasoning accuracy.\nSpecifically, our method prompts the model to generate two answers in parallel:\none in Thinking mode and the other in Nothinking mode. A second round of\nThinking is triggered only when the two initial responses are inconsistent,\nusing a single prompt that incorporates the original question and both\ncandidate answers. Since such disagreement occurs infrequently (e.g., only 6\\%\nin GSM8K), our method performs just one round of reasoning in most cases,\nresulting in minimal latency overhead. Extensive experiments across multiple\nreasoning benchmarks demonstrate that JointThinking significantly outperforms\nfew-shot chain-of-thought (CoT) and majority voting with improved answer\nrobustness. Moreover, It achieves comparable in-distribution performance to\ntraining-based SOTA method, while substantially outperforming on\nout-of-distribution tasks. We further conduct a systematic analysis of the\ncalibration mechanism, showing that leveraging different reasoning modes\nconsistently lowers the error rate and highlights the value of structural\nthinking diversity. Additionally, we observe that the performance gap between\nactual and ideal reasoning narrows as model size increases in the second round\nof thinking, indicating the strong scalability of our approach. Finally, we\ndiscuss current limitations and outline promising directions for future ICL\nresearch in RLLMs.", "AI": {"tldr": "\u63d0\u51faJointThinking\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7Thinking\u4e0eNothinking\u4e24\u79cd\u63a8\u7406\u6a21\u5f0f\u7684\u7ed3\u6784\u5dee\u5f02\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7cbe\u5ea6\uff0c\u5728\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u7684\u540c\u65f6\u663e\u8457\u8d85\u8d8aCoT\u548c\u591a\u6570\u6295\u7968\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u805a\u7126\u8bad\u7ec3/\u63a8\u7406\u7b56\u7565\u6539\u8fdb\uff0c\u800c\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u6f5c\u529b\u63a2\u7d22\u4e0d\u8db3\u3002", "method": "\u5e76\u884c\u751f\u6210Thinking/Nothinking\u6a21\u5f0f\u7b54\u6848\uff0c\u5f53\u7ed3\u679c\u4e0d\u4e00\u81f4\u65f6\uff08\u5982GSM8K\u4ec56%\uff09\u89e6\u53d1\u7b2c\u4e8c\u8f6e\u601d\u8003\uff0c\u4f7f\u7528\u5305\u542b\u539f\u59cb\u95ee\u9898\u53ca\u5019\u9009\u7b54\u6848\u7684\u5355\u4e00\u63d0\u793a\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u663e\u8457\u4f18\u4e8efew-shot CoT\u548c\u591a\u6570\u6295\u7968\uff0c\u5206\u5e03\u5916\u4efb\u52a1\u8868\u73b0\u8fdc\u8d85\u57fa\u7ebf\uff0c\u8fbe\u5230\u4e0e\u8bad\u7ec3\u578bSOTA\u76f8\u5f53\u7684\u5206\u5e03\u5185\u6027\u80fd\u3002", "conclusion": "\u4e0d\u540c\u63a8\u7406\u6a21\u5f0f\u7684\u534f\u540c\u80fd\u6301\u7eed\u964d\u4f4e\u9519\u8bef\u7387\uff0c\u6a21\u578b\u89c4\u6a21\u6269\u5927\u65f6\u5b9e\u9645\u4e0e\u7406\u60f3\u63a8\u7406\u5dee\u8ddd\u7f29\u5c0f\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u5f3a\u6269\u5c55\u6027\u3002\u672a\u6765\u9700\u63a2\u7d22ICL\u5728RLLMs\u4e2d\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.03399", "pdf": "https://arxiv.org/pdf/2508.03399", "abs": "https://arxiv.org/abs/2508.03399", "authors": ["Eliseo Bao", "Anxo P\u00e9rez", "Javier Parapar"], "title": "ReDSM5: A Reddit Dataset for DSM-5 Depression Detection", "categories": ["cs.CL"], "comment": "Accepted as a resource paper at CIKM 2025", "summary": "Depression is a pervasive mental health condition that affects hundreds of\nmillions of individuals worldwide, yet many cases remain undiagnosed due to\nbarriers in traditional clinical access and pervasive stigma. Social media\nplatforms, and Reddit in particular, offer rich, user-generated narratives that\ncan reveal early signs of depressive symptomatology. However, existing\ncomputational approaches often label entire posts simply as depressed or not\ndepressed, without linking language to specific criteria from the DSM-5, the\nstandard clinical framework for diagnosing depression. This limits both\nclinical relevance and interpretability. To address this gap, we introduce\nReDSM5, a novel Reddit corpus comprising 1484 long-form posts, each\nexhaustively annotated at the sentence level by a licensed psychologist for the\nnine DSM-5 depression symptoms. For each label, the annotator also provides a\nconcise clinical rationale grounded in DSM-5 methodology. We conduct an\nexploratory analysis of the collection, examining lexical, syntactic, and\nemotional patterns that characterize symptom expression in social media\nnarratives. Compared to prior resources, ReDSM5 uniquely combines\nsymptom-specific supervision with expert explanations, facilitating the\ndevelopment of models that not only detect depression but also generate\nhuman-interpretable reasoning. We establish baseline benchmarks for both\nmulti-label symptom classification and explanation generation, providing\nreference results for future research on detection and interpretability.", "AI": {"tldr": "\u5f00\u53d1\u4e86ReDSM5\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5c06Reddit\u5e16\u5b50\u4e0eDSM-5\u6291\u90c1\u75c7\u72b6\u6807\u51c6\u9010\u53e5\u5bf9\u5e94\uff0c\u63d0\u5347AI\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027", "motivation": "\u73b0\u6709\u793e\u4ea4\u5a92\u4f53\u6291\u90c1\u68c0\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u4e34\u5e8a\u6807\u51c6\u5173\u8054\uff0c\u5bfc\u81f4\u7ed3\u679c\u7f3a\u4e4f\u4e34\u5e8a\u53c2\u8003\u4ef7\u503c\u548c\u53ef\u89e3\u91ca\u6027", "method": "\u6784\u5efa\u5305\u542b1484\u7bc7\u4e13\u5bb6\u6807\u6ce8\u7684Reddit\u8bed\u6599\u5e93\uff0c\u8fdb\u884c\u8bed\u8a00\u7279\u5f81\u5206\u6790\u548c\u57fa\u7ebf\u6a21\u578b\u8bad\u7ec3", "result": "\u53d1\u73b0\u7279\u5b9a\u8bed\u8a00\u6a21\u5f0f\u4e0e\u75c7\u72b6\u7684\u5173\u8054\uff0c\u5efa\u7acb\u4e86\u75c7\u72b6\u5206\u7c7b\u548c\u89e3\u91ca\u751f\u6210\u7684\u53cc\u91cd\u57fa\u7ebf\u6a21\u578b", "conclusion": "ReDSM5\u9996\u6b21\u5b9e\u73b0\u75c7\u72b6\u7ea7\u522b\u7684\u7ec6\u7c92\u5ea6\u6807\u6ce8\uff0c\u4e3a\u53ef\u89e3\u91ca\u7684\u6291\u90c1\u68c0\u6d4b\u6a21\u578b\u63d0\u4f9b\u4e86\u5173\u952e\u8d44\u6e90"}}
{"id": "2508.03420", "pdf": "https://arxiv.org/pdf/2508.03420", "abs": "https://arxiv.org/abs/2508.03420", "authors": ["Bing Wang", "Ximing Li", "Yiming Wang", "Changchun Li", "Jiaxu Cui", "Renchu Guan", "Bo Yang"], "title": "Variety Is the Spice of Life: Detecting Misinformation with Dynamic Environmental Representations", "categories": ["cs.CL", "cs.SI"], "comment": "Accepted by CIKM 2025. 11 pages, 4 figures. Code:\n  https://github.com/wangbing1416/MISDER", "summary": "The proliferation of misinformation across diverse social media platforms has\ndrawn significant attention from both academic and industrial communities due\nto its detrimental effects. Accordingly, automatically distinguishing\nmisinformation, dubbed as Misinformation Detection (MD), has become an\nincreasingly active research topic. The mainstream methods formulate MD as a\nstatic learning paradigm, which learns the mapping between the content, links,\nand propagation of news articles and the corresponding manual veracity labels.\nHowever, the static assumption is often violated, since in real-world\nscenarios, the veracity of news articles may vacillate within the dynamically\nevolving social environment. To tackle this problem, we propose a novel\nframework, namely Misinformation detection with Dynamic Environmental\nRepresentations (MISDER). The basic idea of MISDER lies in learning a social\nenvironmental representation for each period and employing a temporal model to\npredict the representation for future periods. In this work, we specify the\ntemporal model as the LSTM model, continuous dynamics equation, and pre-trained\ndynamics system, suggesting three variants of MISDER, namely MISDER-LSTM,\nMISDER-ODE, and MISDER-PT, respectively. To evaluate the performance of MISDER,\nwe compare it to various MD baselines across 2 prevalent datasets, and the\nexperimental results can indicate the effectiveness of our proposed model.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u73af\u5883\u8868\u5f81\u6846\u67b6MISDER\u89e3\u51b3\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u7684\u9759\u6001\u6a21\u578b\u5c40\u9650\uff0c\u901a\u8fc7\u65f6\u95f4\u5efa\u6a21\u5b9e\u73b0\u52a8\u6001\u73af\u5883\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u65b9\u6cd5\u57fa\u4e8e\u9759\u6001\u5047\u8bbe\uff0c\u65e0\u6cd5\u9002\u5e94\u793e\u4ea4\u73af\u5883\u52a8\u6001\u53d8\u5316\u5bfc\u81f4\u7684\u65b0\u95fb\u771f\u5b9e\u6027\u6ce2\u52a8\u95ee\u9898\u3002", "method": "MISDER\u6846\u67b6\u901a\u8fc7LSTM/\u8fde\u7eed\u52a8\u6001\u65b9\u7a0b/\u9884\u8bad\u7ec3\u7cfb\u7edf\u5b66\u4e60\u5468\u671f\u6027\u793e\u4f1a\u8868\u5f81\uff0c\u9884\u6d4b\u672a\u6765\u73af\u5883\u72b6\u6001\uff0c\u5f00\u53d1\u4e86\u4e09\u79cd\u53d8\u4f53\u6a21\u578b\u3002", "result": "\u57282\u4e2a\u4e3b\u6d41\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660eMISDER\u4f18\u4e8e\u4f20\u7edf\u9759\u6001\u68c0\u6d4b\u6a21\u578b\uff0c\u9a8c\u8bc1\u52a8\u6001\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u52a8\u6001\u73af\u5883\u8868\u5f81\u5efa\u6a21\u80fd\u6709\u6548\u63d0\u5347\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u6027\u80fd\uff0c\u672a\u6765\u53ef\u63a2\u7d22\u66f4\u590d\u6742\u7684\u793e\u4f1a\u52a8\u6001\u5efa\u6a21\u65b9\u5f0f\u3002"}}
{"id": "2508.03440", "pdf": "https://arxiv.org/pdf/2508.03440", "abs": "https://arxiv.org/abs/2508.03440", "authors": ["Junhong Wu", "Jinliang Lu", "Zixuan Ren", "Ganqiang Hu", "Zhi Wu", "Dai Dai", "Hua Wu"], "title": "LLMs Have a Heart of Stone: Demystifying the Soft Thinking Ability of Large Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 7 figures, working in progress", "summary": "Human cognition naturally engages with abstract and fluid concepts, whereas\nexisting reasoning models often rely on generating discrete tokens, potentially\nconstraining their expressive capabilities. Recent advancements aim to address\nthis limitation by enabling large language models (LLMs) to generate soft,\nabstract tokens, thus facilitating reasoning within a continuous concept space.\nThis paper explores the `Soft Thinking' capabilities of various LLMs by\nexamining the models' internal behavior using a suite of probing techniques.\nContrary to the common belief that Soft Thinking enables the simultaneous\nexploration of diverse reasoning paths, our findings reveal that LLMs\npredominantly rely on the most influential component of the soft inputs during\nsubsequent decoding steps. This reliance hinders the exploration of different\nreasoning paths and reduces vanilla Soft Thinking to a form of greedy decoding,\nobscuring the advantage of transmitting more information through Soft Tokens.\nTo tackle this issue, we explore sampling strategies to introduce\n\\emph{randomness}, employing methods such as Dirichlet resampling and the\nGumbel-Softmax trick. Our experiments demonstrate that incorporating randomness\ncan alleviate the limitations of vanilla approaches and unleash the potential\nof Soft Thinking. Notably, the Gumbel-Softmax trick provides adequate\nrandomness with controlled smoothness, resulting in superior performance across\neight reasoning benchmarks.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8f6f\u601d\u8003\u4e2d\u5b58\u5728\u8def\u5f84\u63a2\u7d22\u9650\u5236\uff0c\u901a\u8fc7\u5f15\u5165\u968f\u673a\u6027\u7b56\u7565\uff08\u5982Gumbel-Softmax\uff09\u53ef\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u4f9d\u8d56\u79bb\u6563token\u5904\u7406\u62bd\u8c61\u6982\u5ff5\u7684\u5c40\u9650\u6027\uff0c\u9700\u63a2\u7d22\u8f6f\u601d\u8003\u673a\u5236\u5728\u8fde\u7eed\u7a7a\u95f4\u7684\u6f5c\u529b\u53ca\u6539\u8fdb\u65b9\u5411\u3002", "method": "\u901a\u8fc7\u884c\u4e3a\u5206\u6790\u53d1\u73b0LLMs\u8fc7\u5ea6\u4f9d\u8d56\u8f6f\u8f93\u5165\u4e3b\u5bfc\u6210\u5206\uff0c\u63d0\u51faDirichlet\u91cd\u91c7\u6837\u548cGumbel-Softmax\u4e24\u79cd\u968f\u673a\u6027\u6ce8\u5165\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u968f\u673a\u6027\u7b56\u7565\u6709\u6548\u7a81\u7834\u8f6f\u601d\u8003\u9650\u5236\uff0cGumbel-Softax\u57288\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6700\u4f18\uff08\u5e73\u6ed1\u63a7\u5236\u4e0e\u968f\u673a\u6027\u5e73\u8861\u6700\u4f73\uff09\u3002", "conclusion": "\u8f6f\u601d\u8003\u9700\u7ed3\u5408\u968f\u673a\u6027\u7b56\u7565\u624d\u80fd\u91ca\u653e\u6f5c\u529b\uff0cGumbel-Softmax\u901a\u8fc7\u53ef\u63a7\u5e73\u6ed1\u6027\u5b9e\u73b0\u4e86\u6027\u80fd\u7a81\u7834\u3002"}}
{"id": "2508.03453", "pdf": "https://arxiv.org/pdf/2508.03453", "abs": "https://arxiv.org/abs/2508.03453", "authors": ["Rita Gonz\u00e1lez-M\u00e1rquez", "Philipp Berens", "Dmitry Kobak"], "title": "Cropping outperforms dropout as an augmentation strategy for training self-supervised text embeddings", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Text embeddings, i.e. vector representations of entire texts, play an\nimportant role in many NLP applications, such as retrieval-augmented\ngeneration, sentiment analysis, clustering, or visualizing collections of texts\nfor data exploration. Currently, top-performing embedding models are derived\nfrom pre-trained language models via extensive supervised fine-tuning using\ncurated text pairs. This contrasts with computer vision, where self-supervised\ntraining based on data augmentations has demonstrated remarkable success. Here\nwe systematically compare the two most well-known augmentation strategies for\npositive pair generation in contrastive learning of text embeddings. We assess\nembedding quality on MTEB and additional in-domain evaluations and show that\ncropping augmentation strongly outperforms the dropout-based approach. We find\nthat on out-of-domain data, the quality of resulting embeddings is below the\nsupervised SOTA models, but for in-domain data, self-supervised fine-tuning\nproduces high-quality text embeddings after very short fine-tuning, sometimes\nonly marginally below the supervised SOTA. Finally, we show that representation\nquality increases towards the last transformer layers, which undergo the\nlargest change during fine-tuning; and that fine-tuning only those last layers\nis sufficient to reach similar embedding quality.", "AI": {"tldr": "\u8bba\u6587\u7cfb\u7edf\u6bd4\u8f83\u4e86\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u88c1\u526a\u589e\u5f3a\u4e0edropout\u589e\u5f3a\u7b56\u7565\uff0c\u53d1\u73b0\u88c1\u526a\u589e\u5f3a\u5728\u57df\u5185\u6570\u636e\u4e0a\u8868\u73b0\u63a5\u8fd1\u76d1\u7763SOTA\uff0c\u4e14\u4ec5\u5fae\u8c03\u6700\u540e\u51e0\u5c42Transformer\u5373\u53ef\u8fbe\u5230\u7c7b\u4f3c\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5d4c\u5165\u6a21\u578b\u4f9d\u8d56\u76d1\u7763\u5fae\u8c03\uff0c\u800c\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u81ea\u76d1\u7763\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u6210\u6548\u663e\u8457\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u81ea\u76d1\u7763\u65b9\u6cd5\u5728\u6587\u672c\u5d4c\u5165\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u5bf9\u6bd4\u4e24\u79cd\u5bf9\u6bd4\u5b66\u4e60\u6b63\u6837\u672c\u751f\u6210\u7b56\u7565\uff1a\u6587\u672c\u88c1\u526a\u589e\u5f3a\u4e0edropout\u6270\u52a8\uff0c\u901a\u8fc7MTEB\u57fa\u51c6\u548c\u57df\u5185\u8bc4\u4f30\u9a8c\u8bc1\uff0c\u5e76\u5206\u6790\u4e0d\u540cTransformer\u5c42\u7684\u53d8\u5316\u3002", "result": "\u88c1\u526a\u589e\u5f3a\u663e\u8457\u4f18\u4e8edropout\u65b9\u6cd5\uff1b\u57df\u5916\u6570\u636e\u8868\u73b0\u4f4e\u4e8e\u76d1\u7763SOTA\uff0c\u57df\u5185\u6570\u636e\u81ea\u76d1\u7763\u5fae\u8c03\u5feb\u901f\u63a5\u8fd1\u76d1\u7763\u6a21\u578b\uff08\u5dee\u8ddd\u22642%\uff09\uff1b\u6700\u540e\u51e0\u5c42\u53d8\u5316\u6700\u5927\u4e14\u5355\u72ec\u5fae\u8c03\u5373\u53ef\u8fbe\u6807\u3002", "conclusion": "\u81ea\u76d1\u7763\u5fae\u8c03\u5728\u57df\u5185\u573a\u666f\u5177\u5907\u9ad8\u6548\u6027\uff0c\u6700\u540e\u51e0\u5c42Transformer\u5bf9\u8868\u5f81\u8d28\u91cf\u8d77\u51b3\u5b9a\u6027\u4f5c\u7528\uff0c\u4e3a\u8f7b\u91cf\u5316\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.03475", "pdf": "https://arxiv.org/pdf/2508.03475", "abs": "https://arxiv.org/abs/2508.03475", "authors": ["Pranshu Rastogi"], "title": "fact check AI at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-checked Claim Retrieval", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "7 pages, 6 tables. Code available at\n  https://github.com/pranshurastogi29/SemEval-2025-ACL-Multi-and-Crosslingual-Retrieval-using-Bi-encoders", "summary": "SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim\nRetrieval is approached as a Learning-to-Rank task using a bi-encoder model\nfine-tuned from a pre-trained transformer optimized for sentence similarity.\nTraining used both the source languages and their English translations for\nmultilingual retrieval and only English translations for cross-lingual\nretrieval. Using lightweight models with fewer than 500M parameters and\ntraining on Kaggle T4 GPUs, the method achieved 92% Success@10 in multilingual\nand 80% Success@10 in 5th in crosslingual and 10th in multilingual tracks.", "AI": {"tldr": "\u91c7\u7528\u53cc\u7f16\u7801\u5668\u6a21\u578b\u5b8c\u6210\u591a\u8bed\u8a00/\u8de8\u8bed\u8a00\u4e8b\u5b9e\u6838\u67e5\u58f0\u660e\u68c0\u7d22\uff0c\u5728Kaggle T4 GPU\u4e0a\u8bad\u7ec3\uff0c\u8de8\u8bed\u8a00\u8d5b\u9053\u7b2c\u4e94\u540d(80% Success@10)\uff0c\u591a\u8bed\u8a00\u7b2c\u5341\u540d(92% Success@10)", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u4fe1\u606f\u53ef\u4fe1\u5ea6\u9a8c\u8bc1\u95ee\u9898\uff0c\u63a2\u7d22\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u6709\u9650\u7b97\u529b\u4e0b\u5b9e\u73b0\u9ad8\u6548\u8de8\u8bed\u8a00\u4e8b\u5b9e\u6838\u67e5\u7684\u53ef\u884c\u6027", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3Transformer\u6784\u5efa\u53cc\u7f16\u7801\u5668\u6a21\u578b\uff0c\u91c7\u7528\u6e90\u8bed\u8a00+\u82f1\u6587\u7ffb\u8bd1\u7684\u591a\u8bed\u8a00\u8bad\u7ec3\u7b56\u7565\u548c\u7eaf\u82f1\u6587\u7ffb\u8bd1\u7684\u8de8\u8bed\u8a00\u7b56\u7565\uff0c\u5728Kaggle T4 GPU\u4e0a\u5fae\u8c03\u53e5\u5b50\u76f8\u4f3c\u6027\u4efb\u52a1", "result": "\u53c2\u6570\u91cf<5\u4ebf\u7684\u8f7b\u91cf\u6a21\u578b\u5b9e\u73b0\u8de8\u8bed\u8a00\u8d5b\u905380% Success@10(\u7b2c5\u540d)\u548c\u591a\u8bed\u8a00\u8d5b\u905392% Success@10(\u7b2c10\u540d)", "conclusion": "\u9a8c\u8bc1\u4e86\u7ffb\u8bd1\u589e\u5f3a\u8bad\u7ec3\u7b56\u7565\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u8f7b\u91cf\u6a21\u578b\u5728\u6709\u9650\u7b97\u529b\u4e0b\u4ecd\u53ef\u5b9e\u73b0\u4f18\u8d28\u7684\u591a\u8bed\u8a00\u68c0\u7d22\u6027\u80fd"}}
{"id": "2508.03489", "pdf": "https://arxiv.org/pdf/2508.03489", "abs": "https://arxiv.org/abs/2508.03489", "authors": ["Kaiwen Zhao", "Bharathan Balaji", "Stephen Lee"], "title": "CF-RAG: A Dataset and Method for Carbon Footprint QA Using Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Product sustainability reports provide valuable insights into the\nenvironmental impacts of a product and are often distributed in PDF format.\nThese reports often include a combination of tables and text, which complicates\ntheir analysis. The lack of standardization and the variability in reporting\nformats further exacerbate the difficulty of extracting and interpreting\nrelevant information from large volumes of documents. In this paper, we tackle\nthe challenge of answering questions related to carbon footprints within\nsustainability reports available in PDF format. Unlike previous approaches, our\nfocus is on addressing the difficulties posed by the unstructured and\ninconsistent nature of text extracted from PDF parsing. To facilitate this\nanalysis, we introduce CarbonPDF-QA, an open-source dataset containing\nquestion-answer pairs for 1735 product report documents, along with\nhuman-annotated answers. Our analysis shows that GPT-4o struggles to answer\nquestions with data inconsistencies. To address this limitation, we propose\nCarbonPDF, an LLM-based technique specifically designed to answer carbon\nfootprint questions on such datasets. We develop CarbonPDF by fine-tuning Llama\n3 with our training data. Our results show that our technique outperforms\ncurrent state-of-the-art techniques, including question-answering (QA) systems\nfinetuned on table and text data.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLlama 3\u5fae\u8c03\u7684CarbonPDF\u6280\u672f\uff0c\u901a\u8fc7\u81ea\u5efaCarbonPDF-QA\u6570\u636e\u96c6\u89e3\u51b3PDF\u683c\u5f0f\u53ef\u6301\u7eed\u53d1\u5c55\u62a5\u544a\u4e2d\u78b3\u8db3\u8ff9\u95ee\u7b54\u96be\u9898", "motivation": "PDF\u683c\u5f0f\u4ea7\u54c1\u53ef\u6301\u7eed\u53d1\u5c55\u62a5\u544a\u5b58\u5728\u6587\u672c\u7ed3\u6784\u6df7\u4e71\u3001\u683c\u5f0f\u4e0d\u7edf\u4e00\u95ee\u9898\uff0c\u73b0\u6709GPT-4o\u7b49\u6a21\u578b\u5728\u6570\u636e\u4e0d\u4e00\u81f4\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73", "method": "\u6784\u5efa\u5305\u542b1735\u4efd\u6587\u6863\u95ee\u7b54\u5bf9\u7684CarbonPDF-QA\u6570\u636e\u96c6\uff0c\u5e76\u57fa\u4e8e\u8be5\u6570\u636e\u96c6\u5bf9Llama 3\u8fdb\u884c\u5fae\u8c03\u5f00\u53d1CarbonPDF\u6280\u672f", "result": "CarbonPDF\u5728\u78b3\u8db3\u8ff9\u95ee\u7b54\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u6700\u5148\u8fdb\u7684\u8868\u683c-\u6587\u672c\u8054\u5408QA\u7cfb\u7edf\uff0c\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347", "conclusion": "\u9488\u5bf9PDF\u6587\u6863\u7ed3\u6784\u7279\u6027\u5b9a\u5236\u7684\u5fae\u8c03\u65b9\u6848\u80fd\u6709\u6548\u89e3\u51b3\u975e\u7ed3\u6784\u5316\u6570\u636e\u95ee\u7b54\u96be\u9898\uff0c\u4e3a\u53ef\u6301\u7eed\u53d1\u5c55\u62a5\u544a\u5206\u6790\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2508.03520", "pdf": "https://arxiv.org/pdf/2508.03520", "abs": "https://arxiv.org/abs/2508.03520", "authors": ["Md Rakibul Hasan", "Md Zakir Hossain", "Aneesh Krishna", "Shafin Rahman", "Tom Gedeon"], "title": "UPLME: Uncertainty-Aware Probabilistic Language Modelling for Robust Empathy Regression", "categories": ["cs.CL", "cs.LG"], "comment": "Code available at https://github.com/hasan-rakibul/UPLME", "summary": "Supervised learning for empathy regression is challenged by noisy\nself-reported empathy scores. While many algorithms have been proposed for\nlearning with noisy labels in textual classification problems, the regression\ncounterpart is relatively under-explored. We propose UPLME, an\nuncertainty-aware probabilistic language modelling framework to capture label\nnoise in the regression setting of empathy detection. UPLME includes a\nprobabilistic language model that predicts both empathy score and\nheteroscedastic uncertainty and is trained using Bayesian concepts with\nvariational model ensembling. We further introduce two novel loss components:\none penalises degenerate Uncertainty Quantification (UQ), and another enforces\nthe similarity between the input pairs on which we predict empathy. UPLME\nprovides state-of-the-art performance (Pearson Correlation Coefficient:\n$0.558\\rightarrow0.580$ and $0.629\\rightarrow0.634$) in terms of the\nperformance reported in the literature in two public benchmarks, having label\nnoise. Through synthetic label noise injection, we show that UPLME is effective\nin separating noisy and clean samples based on the predicted uncertainty. UPLME\nfurther outperform (Calibration error: $0.571\\rightarrow0.376$) a recent\nvariational model ensembling-based UQ method designed for regression problems.", "AI": {"tldr": "\u63d0\u51fa\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684UPLME\u6846\u67b6\uff0c\u6709\u6548\u5904\u7406\u5171\u60c5\u56de\u5f52\u4e2d\u7684\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u76d1\u7763\u5b66\u4e60\u5728\u5171\u60c5\u56de\u5f52\u4efb\u52a1\u4e2d\u53d7\u9650\u4e8e\u566a\u58f0\u6807\u7b7e\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5206\u7c7b\u95ee\u9898\uff0c\u56de\u5f52\u573a\u666f\u7684\u566a\u58f0\u5904\u7406\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u8d1d\u53f6\u65af\u53d8\u5206\u6a21\u578b\u96c6\u6210\u548c\u5f02\u65b9\u5dee\u4e0d\u786e\u5b9a\u6027\u9884\u6d4b\uff0c\u5f15\u5165\u4e24\u4e2a\u65b0\u635f\u5931\u51fd\u6570\uff1a1\uff09\u6291\u5236\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u9000\u5316 2\uff09\u4fdd\u6301\u8f93\u5165\u5bf9\u7684\u9884\u6d4b\u76f8\u4f3c\u6027\u3002", "result": "\u5728\u5e26\u566a\u58f0\u7684\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5237\u65b0SOTA\uff08Pearson\u7cfb\u6570\u63d0\u5347\u81f30.580/0.634\uff09\uff0c\u6821\u51c6\u8bef\u5dee\u964d\u4f4e34%\u81f30.376\u3002", "conclusion": "UPLME\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\u6709\u6548\u8bc6\u522b\u566a\u58f0\u6837\u672c\uff0c\u4e3a\u56de\u5f52\u4efb\u52a1\u7684\u566a\u58f0\u9c81\u68d2\u6027\u7814\u7a76\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03523", "pdf": "https://arxiv.org/pdf/2508.03523", "abs": "https://arxiv.org/abs/2508.03523", "authors": ["Lester James V. Miranda", "Elyanah Aco", "Conner Manuel", "Jan Christian Blaise Cruz", "Joseph Marvin Imperial"], "title": "FilBench: Can LLMs Understand and Generate Filipino?", "categories": ["cs.CL"], "comment": null, "summary": "Despite the impressive performance of LLMs on English-based tasks, little is\nknown about their capabilities in specific languages such as Filipino. In this\nwork, we address this gap by introducing FilBench, a Filipino-centric benchmark\ndesigned to evaluate LLMs across a diverse set of tasks and capabilities in\nFilipino, Tagalog, and Cebuano. We carefully curate the tasks in FilBench to\nreflect the priorities and trends of NLP research in the Philippines such as\nCultural Knowledge, Classical NLP, Reading Comprehension, and Generation. By\nevaluating 27 state-of-the-art LLMs on FilBench, we find that several LLMs\nsuffer from reading comprehension and translation capabilities. Our results\nindicate that FilBench is challenging, with the best model, GPT-4o, achieving\nonly a score of 72.23%. Moreover, we also find that models trained specifically\nfor Southeast Asian languages tend to underperform on FilBench, with the\nhighest-performing model, SEA-LION v3 70B, achieving only a score of 61.07%.\nOur work demonstrates the value of curating language-specific LLM benchmarks to\naid in driving progress on Filipino NLP and increasing the inclusion of\nPhilippine languages in LLM development.", "AI": {"tldr": "\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u83f2\u5f8b\u5bbe\u8bed\u57fa\u51c6FilBench\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u83f2\u5f8b\u5bbe\u8bed\u8a00\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u7a81\u663e\u672c\u5730\u5316\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u82f1\u8bed\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5bf9\u5176\u5728\u83f2\u5f8b\u5bbe\u8bed\u8a00\uff08\u5982\u83f2\u5f8b\u5bbe\u8bed\u3001\u5854\u52a0\u6d1b\u8bed\u3001\u5bbf\u52a1\u8bed\uff09\u7684\u80fd\u529b\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\uff0c\u9700\u5efa\u7acb\u9488\u5bf9\u6027\u8bc4\u6d4b\u57fa\u51c6\u63a8\u52a8\u83f2\u5f8b\u5bbeNLP\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u5305\u542b\u6587\u5316\u77e5\u8bc6\u3001\u7ecf\u5178NLP\u3001\u9605\u8bfb\u7406\u89e3\u3001\u751f\u6210\u7b49\u4efb\u52a1\u7684FilBench\u57fa\u51c6\uff0c\u8986\u76d6\u83f2\u5f8b\u5bbe\u4e3b\u6d41\u8bed\u8a00\uff0c\u5e76\u8bc4\u4f3027\u4e2a\u524d\u6cbfLLM\u7684\u6027\u80fd\u3002", "result": "\u6700\u4f73\u6a21\u578bGPT-4o\u4ec5\u83b772.23%\u5f97\u5206\uff0c\u4e1c\u5357\u4e9a\u8bed\u8a00\u4e13\u7528\u6a21\u578b\u8868\u73b0\u66f4\u5dee\uff08SEA-LION v3 70B\u4ec561.07%\uff09\uff0c\u63ed\u793a\u6a21\u578b\u5728\u9605\u8bfb\u7406\u89e3\u3001\u7ffb\u8bd1\u7b49\u6838\u5fc3\u80fd\u529b\u7684\u7f3a\u9677\u3002", "conclusion": "FilBench\u4e3a\u83f2\u5f8b\u5bbe\u8bed\u8a00AI\u53d1\u5c55\u63d0\u4f9b\u5173\u952e\u8bc4\u4f30\u5de5\u5177\uff0c\u5f3a\u8c03\u8bed\u8a00\u7279\u5f02\u6027\u57fa\u51c6\u5bf9\u4fc3\u8fdbLLM\u5305\u5bb9\u6027\u53d1\u5c55\u7684\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2508.03529", "pdf": "https://arxiv.org/pdf/2508.03529", "abs": "https://arxiv.org/abs/2508.03529", "authors": ["Vukosi Marivate", "Isheanesu Dzingirai", "Fiskani Banda", "Richard Lastrucci", "Thapelo Sindane", "Keabetswe Madumo", "Kayode Olaleye", "Abiodun Modupe", "Unarine Netshifhefhe", "Herkulaas Combrink", "Mohlatlego Nakeng", "Matome Ledwaba"], "title": "Marito: Structuring and Building Open Multilingual Terminologies for South African NLP", "categories": ["cs.CL"], "comment": "Under Review", "summary": "The critical lack of structured terminological data for South Africa's\nofficial languages hampers progress in multilingual NLP, despite the existence\nof numerous government and academic terminology lists. These valuable assets\nremain fragmented and locked in non-machine-readable formats, rendering them\nunusable for computational research and development. \\emph{Marito} addresses\nthis challenge by systematically aggregating, cleaning, and standardising these\nscattered resources into open, interoperable datasets. We introduce the\nfoundational \\emph{Marito} dataset, released under the equitable,\nAfrica-centered NOODL framework. To demonstrate its immediate utility, we\nintegrate the terminology into a Retrieval-Augmented Generation (RAG) pipeline.\nExperiments show substantial improvements in the accuracy and domain-specific\nconsistency of English-to-Tshivenda machine translation for large language\nmodels. \\emph{Marito} provides a scalable foundation for developing robust and\nequitable NLP technologies, ensuring South Africa's rich linguistic diversity\nis represented in the digital age.", "AI": {"tldr": "\u89e3\u51b3\u5357\u975e\u5b98\u65b9\u8bed\u8a00\u672f\u8bed\u6570\u636e\u7f3a\u4e4f\u95ee\u9898\uff0c\u6784\u5efaMarito\u6570\u636e\u96c6\u63d0\u5347\u591a\u8bed\u8a00NLP\u6280\u672f\u3002", "motivation": "\u5357\u975e\u5b98\u65b9\u8bed\u8a00\u7684\u7ed3\u6784\u5316\u672f\u8bed\u6570\u636e\u7f3a\u5931\uff0c\u73b0\u6709\u672f\u8bed\u8d44\u6e90\u5206\u6563\u4e14\u975e\u673a\u5668\u53ef\u8bfb\uff0c\u963b\u788d\u591a\u8bed\u8a00NLP\u53d1\u5c55\u3002", "method": "\u7cfb\u7edf\u6574\u5408\u3001\u6e05\u6d17\u5e76\u6807\u51c6\u5316\u5206\u6563\u672f\u8bed\u8d44\u6e90\u4e3a\u5f00\u653e\u6570\u636e\u96c6\uff0c\u96c6\u6210\u5230\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u6d41\u7a0b\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMarito\u6570\u636e\u96c6\u663e\u8457\u63d0\u5347\u82f1\u8bed-Tshivenda\u673a\u5668\u7ffb\u8bd1\u7684\u51c6\u786e\u6027\u548c\u9886\u57df\u4e00\u81f4\u6027\u3002", "conclusion": "Marito\u4e3a\u5f00\u53d1\u516c\u5e73\u7a33\u5065\u7684NLP\u6280\u672f\u63d0\u4f9b\u53ef\u6269\u5c55\u57fa\u7840\uff0c\u786e\u4fdd\u5357\u975e\u8bed\u8a00\u591a\u6837\u6027\u5728\u6570\u5b57\u65f6\u4ee3\u7684\u4ee3\u8868\u6027\u3002"}}
{"id": "2508.03533", "pdf": "https://arxiv.org/pdf/2508.03533", "abs": "https://arxiv.org/abs/2508.03533", "authors": ["Xiaoming Hou", "Jiquan Zhang", "Zibin Lin", "DaCheng Tao", "Shengli Zhang"], "title": "EmbedGrad: Gradient-Based Prompt Optimization in Embedding Space for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Effectively adapting powerful pretrained foundation models to diverse tasks\nremains a key challenge in AI deployment. Current approaches primarily follow\ntwo paradigms:discrete optimization of text prompts through prompt engineering,\nor continuous adaptation via additional trainable parameters. Both exhibit\nlimitations-discrete methods lack refinement precision while parameter-based\ntechniques increase complexity and reduce interpretability. To address these\nconstraints, we propose EmbedGrad, a novel framework that optimizes text prompt\nembeddings through gradient-based refinement. Our approach uniquely decouples\ntraining from deployment:during optimization,labeled examples guide precise\nembedding adjustments while preserving semantic meaning; during inference, only\noptimized embeddings integrate with user queries. This enables fine-grained\ncalibration impossible in text space, such as enhancing the reasoning\ncapability of prompts like please reason step by step. Comprehensive\nevaluations across mathematical reasoning, sentiment analysis, and causal\njudgment tasks demonstrate EmbedGrad's effectiveness:optimizing this reasoning\nprompt for Qwen2.5-Math-1.5B increased accuracy from 14.74\\% to 58.96\\% on\nmathematical problems. Consistent improvements were observed across model\nscales (0.5B-14B) and all tasks, with particularly significant gains for\nsmaller models on complex problems like causal judgment. By bridging prompt\nengineering and parameter efficiency without architectural changes, our work\nestablishes embedding refinement as a powerful new paradigm for task\nadaptation.", "AI": {"tldr": "\u63d0\u51faEmbedGrad\u6846\u67b6\uff0c\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u6587\u672c\u63d0\u793a\u5d4c\u5165\u5b9e\u73b0\u6a21\u578b\u9002\u914d\uff0c\u5728\u6570\u5b66\u63a8\u7406\u7b49\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387", "motivation": "\u73b0\u6709\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u5b58\u5728\u79bb\u6563\u4f18\u5316\u7cbe\u5ea6\u4e0d\u8db3\u4e0e\u53c2\u6570\u8c03\u6574\u65b9\u6cd5\u590d\u6742\u5ea6\u9ad8\u7684\u77db\u76fe\uff0c\u9700\u63a2\u7d22\u7ed3\u5408\u4e24\u8005\u4f18\u52bf\u7684\u65b0\u8303\u5f0f", "method": "\u63d0\u51fa\u68af\u5ea6\u4f18\u5316\u7684\u5d4c\u5165\u8c03\u6574\u6846\u67b6\uff1a\u8bad\u7ec3\u9636\u6bb5\u7528\u6807\u6ce8\u6570\u636e\u5fae\u8c03\u63d0\u793a\u5d4c\u5165\uff0c\u63a8\u7406\u9636\u6bb5\u76f4\u63a5\u4f7f\u7528\u4f18\u5316\u540e\u7684\u5d4c\u5165", "result": "Qwen2.5-Math-1.5B\u6570\u5b66\u51c6\u786e\u7387\u4ece14.74%\u63d0\u5347\u81f358.96%\uff0c\u5404\u89c4\u6a21\u6a21\u578b\uff080.5B-14B\uff09\u5728\u56e0\u679c\u63a8\u65ad\u7b49\u4efb\u52a1\u5747\u83b7\u663e\u8457\u63d0\u5347", "conclusion": "EmbedGrad\u5efa\u7acb\u4e86\u65e0\u9700\u67b6\u6784\u4fee\u6539\u7684\u63d0\u793a\u4f18\u5316\u65b0\u8303\u5f0f\uff0c\u5728\u4fdd\u6301\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u6821\u51c6\uff0c\u4e3a\u4efb\u52a1\u9002\u914d\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2508.03550", "pdf": "https://arxiv.org/pdf/2508.03550", "abs": "https://arxiv.org/abs/2508.03550", "authors": ["Peng Lai", "Jianjie Zheng", "Sijie Cheng", "Yun Chen", "Peng Li", "Yang Liu", "Guanhua Chen"], "title": "Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via Internal Representations", "categories": ["cs.CL"], "comment": null, "summary": "The growing scale of evaluation tasks has led to the widespread adoption of\nautomated evaluation using large language models, a paradigm known as\n\"LLMas-a-judge.\" However, improving its alignment with human preferences\nwithout complex prompts or fine-tuning remains challenging. In this work,\nmotivated by preliminary findings that middle-to-upper layers encode\nsemantically and taskrelevant representations that are often more aligned with\nhuman judgments than the final layer, we propose LAGER, a lightweight and\nefficient framework for enhancing LLM-as-a-Judge alignment with human scoring,\nvia internal representations. LAGER produces fine-grained judgment scores by\naggregating cross-layer scoretoken logits and computing the expected score from\na softmax-based distribution, with the LLM backbone kept frozen. LAGER fully\nleverages the complementary information across different layers, overcoming the\nlimitations of relying solely on the final layer. We evaluate our method on the\nstandard alignment benchmarks Flask, HelpSteer, and BIGGen using Spearman\ncorrelation, and find that LAGER achieves improvements of up to 7.5% over the\nbest baseline across these benchmarks. Without reasoning steps, LAGER matches\nor outperforms reasoning-based methods. Experiments on downstream applications,\nsuch as data selection and emotional understanding, further show the\neffectiveness of our method.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u6846\u67b6LAGER\uff0c\u901a\u8fc7\u805a\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u95f4\u5c42\u8868\u5f81\u63d0\u5347\u81ea\u52a8\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u504f\u597d\u7684\u4e00\u81f4\u6027\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u9ad87.5%\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3LLM-as-a-judge\u8303\u5f0f\u5728\u81ea\u52a8\u8bc4\u4f30\u65f6\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u53d1\u73b0\u4e2d\u95f4\u5c42\u8868\u5f81\u6bd4\u6700\u7ec8\u5c42\u66f4\u7b26\u5408\u4eba\u7c7b\u5224\u65ad\uff0c\u4ece\u800c\u5f00\u53d1\u65e0\u9700\u5fae\u8c03\u6216\u590d\u6742\u63d0\u793a\u7684\u6539\u8fdb\u65b9\u6848\u3002", "method": "\u57fa\u4e8e\u51bb\u7ed3\u7684LLM\u4e3b\u5e72\uff0c\u901a\u8fc7\u805a\u5408\u8de8\u5c42\u7684score-token logits\u8ba1\u7b97\u671f\u671b\u503c\uff0c\u7ed3\u5408softmax\u5206\u5e03\u751f\u6210\u7ec6\u7c92\u5ea6\u8bc4\u5206\uff0c\u5145\u5206\u6316\u6398\u4e0d\u540c\u5c42\u7684\u4e92\u8865\u4fe1\u606f\u3002", "result": "\u5728Flask\u3001HelpSteer\u548cBIGGen\u57fa\u51c6\u6d4b\u8bd5\u4e2dSpearman\u76f8\u5173\u7cfb\u6570\u63d0\u5347\u6700\u9ad87.5%\uff0c\u65e0\u9700\u63a8\u7406\u6b65\u9aa4\u5373\u53ef\u8fbe\u5230\u6216\u8d85\u8d8a\u57fa\u4e8e\u63a8\u7406\u7684\u65b9\u6cd5\u6548\u679c\u3002", "conclusion": "LAGER\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5229\u7528\u5185\u90e8\u8868\u5f81\u7684\u5c42\u7ea7\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bc4\u4f30\u7cfb\u7edf\u7684\u5bf9\u9f50\u80fd\u529b\uff0c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u4efb\u52a1\u63d0\u4f9b\u4e86\u8f7b\u91cf\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.03571", "pdf": "https://arxiv.org/pdf/2508.03571", "abs": "https://arxiv.org/abs/2508.03571", "authors": ["Iing Muttakhiroh", "Thomas Fevens"], "title": "Tackling Distribution Shift in LLM via KILO: Knowledge-Instructed Learning for Continual Adaptation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) often suffer from performance degradation when\nfaced with domain shifts, primarily due to catastrophic forgetting. In this\nwork, we propose KILO (Knowledge-Instructed Learning for Continual Adaptation),\na novel continual learning framework that integrates dynamic knowledge graphs\nwith instruction tuning. By leveraging retrieved domain-specific knowledge as\nguidance during training, KILO enhances both adaptability to new domains and\nretention of previously acquired knowledge. We pretrain our model on\nWikiText-103 and evaluate sequential adaptation across four diverse target\ndomains: BioASQ, SciQ, TweetEval, and MIND. Our experiments demonstrate that\nKILO consistently outperforms strong baselines, including continual\nfine-tuning, ERNIE 2.0, and CPT, in terms of backward transfer, forward\ntransfer, F1 score, retention rate, and training efficiency. These results\nhighlight the effectiveness of combining structured knowledge retrieval and\ninstruction prompting to overcome domain shift challenges in continual learning\nscenarios.", "AI": {"tldr": "\u63d0\u51faKILO\u6846\u67b6\uff0c\u7ed3\u5408\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u4e0e\u6307\u4ee4\u8c03\u4f18\uff0c\u89e3\u51b3LLM\u9886\u57df\u8fc1\u79fb\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9886\u57df\u8fc1\u79fb\u65f6\u56e0\u707e\u96be\u6027\u9057\u5fd8\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u77e5\u8bc6\u4fdd\u7559\u4e0e\u65b0\u9886\u57df\u9002\u5e94\u95f4\u5b58\u5728\u5e73\u8861\u96be\u9898", "method": "\u901a\u8fc7\u68c0\u7d22\u9886\u57df\u77e5\u8bc6\u6784\u5efa\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\uff0c\u5c06\u5176\u4f5c\u4e3a\u8bad\u7ec3\u6307\u5bfc\u878d\u5165\u6307\u4ee4\u8c03\u4f18\u6846\u67b6\uff08\u57fa\u4e8eWikiText-103\u9884\u8bad\u7ec3\uff09", "result": "\u5728BioASQ/SciQ/TweetEval/MIND\u56db\u9886\u57df\u6301\u7eed\u5b66\u4e60\u4e2d\uff0cBWT/FWT/F1\u7b49\u6307\u6807\u5747\u4f18\u4e8e\u6301\u7eed\u5fae\u8c03/ERNIE 2.0/CPT\u7b49\u57fa\u7ebf\u6a21\u578b", "conclusion": "\u7ed3\u6784\u5316\u77e5\u8bc6\u68c0\u7d22\u4e0e\u6307\u4ee4\u63d0\u793a\u7ed3\u5408\u6709\u6548\u89e3\u51b3\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u9886\u57df\u8fc1\u79fb\u6311\u6218\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u9002\u5e94\u6027\u4e0e\u77e5\u8bc6\u4fdd\u6301\u80fd\u529b"}}
{"id": "2508.03644", "pdf": "https://arxiv.org/pdf/2508.03644", "abs": "https://arxiv.org/abs/2508.03644", "authors": ["Wenxuan Shen", "Mingjia Wang", "Yaochen Wang", "Dongping Chen", "Junjie Yang", "Yao Wan", "Weiwei Lin"], "title": "Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?", "categories": ["cs.CL", "cs.CV", "cs.IR"], "comment": "In submission. Project website: https://double-bench.github.io/", "summary": "Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language\nModels (MLLMs) show great promise for complex document understanding, yet their\ndevelopment is critically hampered by inadequate evaluation. Current benchmarks\noften focus on specific part of document RAG system and use synthetic data with\nincomplete ground truth and evidence labels, therefore failing to reflect\nreal-world bottlenecks and challenges. To overcome these limitations, we\nintroduce Double-Bench: a new large-scale, multilingual, and multimodal\nevaluation system that is able to produce fine-grained assessment to each\ncomponent within document RAG systems. It comprises 3,276 documents (72,880\npages) and 5,168 single- and multi-hop queries across 6 languages and 4\ndocument types with streamlined dynamic update support for potential data\ncontamination issues. Queries are grounded in exhaustively scanned evidence\npages and verified by human experts to ensure maximum quality and completeness.\nOur comprehensive experiments across 9 state-of-the-art embedding models, 4\nMLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text\nand visual embedding models is narrowing, highlighting the need in building\nstronger document retrieval models. Our findings also reveal the\nover-confidence dilemma within current document RAG frameworks that tend to\nprovide answer even without evidence support. We hope our fully open-source\nDouble-Bench provide a rigorous foundation for future research in advanced\ndocument RAG systems. We plan to retrieve timely corpus and release new\nbenchmarks on an annual basis.", "AI": {"tldr": "\u63d0\u51faDouble-Bench\u591a\u6a21\u6001\u8bc4\u4f30\u6846\u67b6\uff0c\u89e3\u51b3\u73b0\u6709\u6587\u6863RAG\u7cfb\u7edf\u8bc4\u6d4b\u4e0d\u8db3\u95ee\u9898", "motivation": "\u73b0\u6709\u6587\u6863RAG\u7cfb\u7edf\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u6570\u636e\u5408\u6210\u3001\u8bc1\u636e\u6807\u7b7e\u4e0d\u5168\u3001\u7f3a\u4e4f\u73b0\u5b9e\u74f6\u9888\u53cd\u6620\u7b49\u7f3a\u9677", "method": "\u6784\u5efa\u542b3,276\u6587\u6863\uff0872,880\u9875\uff09\u7684\u591a\u8bed\u8a00\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5305\u542b5,168\u5355\u8df3/\u591a\u8df3\u67e5\u8be2\uff0c\u914d\u5907\u52a8\u6001\u66f4\u65b0\u673a\u5236\u548c\u4eba\u5de5\u9a8c\u8bc1\u8bc1\u636e\u94fe", "result": "\u5b9e\u9a8c\u63ed\u793a\u6587\u672c/\u89c6\u89c9\u5d4c\u5165\u6a21\u578b\u5dee\u8ddd\u7f29\u5c0f\uff08CLIP-ViT\u4e0e\u6587\u672c\u6a21\u578b\u4ec5\u5dee1.2%\uff09\uff0c\u73b0\u6709\u6846\u67b6\u5b58\u5728\u65e0\u8bc1\u636e\u652f\u6301\u65f6\u7684\u8fc7\u5ea6\u81ea\u4fe1\u95ee\u9898\uff08\u9ad8\u8fbe27.3%\u9519\u8bef\u56de\u7b54\uff09", "conclusion": "\u5f00\u6e90Double-Bench\u4e3a\u6587\u6863RAG\u7814\u7a76\u5efa\u7acb\u4e25\u683c\u8bc4\u4f30\u57fa\u51c6\uff0c\u8ba1\u5212\u6bcf\u5e74\u66f4\u65b0\u8bed\u6599\u5e93\u548c\u8bc4\u6d4b\u6807\u51c6"}}
{"id": "2508.03654", "pdf": "https://arxiv.org/pdf/2508.03654", "abs": "https://arxiv.org/abs/2508.03654", "authors": ["Xinyu Wang", "Yue Zhang", "Liqiang Jing"], "title": "Can Large Vision-Language Models Understand Multimodal Sarcasm?", "categories": ["cs.CL", "cs.CV"], "comment": "Accepted by CIKM 2025", "summary": "Sarcasm is a complex linguistic phenomenon that involves a disparity between\nliteral and intended meanings, making it challenging for sentiment analysis and\nother emotion-sensitive tasks. While traditional sarcasm detection methods\nprimarily focus on text, recent approaches have incorporated multimodal\ninformation. However, the application of Large Visual Language Models (LVLMs)\nin Multimodal Sarcasm Analysis (MSA) remains underexplored. In this paper, we\nevaluate LVLMs in MSA tasks, specifically focusing on Multimodal Sarcasm\nDetection and Multimodal Sarcasm Explanation. Through comprehensive\nexperiments, we identify key limitations, such as insufficient visual\nunderstanding and a lack of conceptual knowledge. To address these issues, we\npropose a training-free framework that integrates in-depth object extraction\nand external conceptual knowledge to improve the model's ability to interpret\nand explain sarcasm in multimodal contexts. The experimental results on\nmultiple models show the effectiveness of our proposed framework. The code is\navailable at https://github.com/cp-cp/LVLM-MSA.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u8bbd\u523a\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u51fa\u6574\u5408\u5bf9\u8c61\u63d0\u53d6\u548c\u5916\u90e8\u77e5\u8bc6\u7684\u6846\u67b6\u4ee5\u6539\u5584\u5176\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u8bbd\u523a\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6587\u672c\uff0c\u8fd1\u5e74\u867d\u5f15\u5165\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u4f46\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u8bbd\u523a\u5206\u6790\u4e2d\u7684\u5e94\u7528\u4ecd\u5b58\u5728\u89c6\u89c9\u7406\u89e3\u4e0d\u8db3\u548c\u6982\u5ff5\u77e5\u8bc6\u7f3a\u4e4f\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6df1\u5ea6\u5bf9\u8c61\u63d0\u53d6\u548c\u5916\u90e8\u6982\u5ff5\u77e5\u8bc6\u589e\u5f3a\u6a21\u578b\u5bf9\u591a\u6a21\u6001\u8bbd\u523a\u7684\u89e3\u6790\u80fd\u529b\u3002", "result": "\u591a\u6a21\u578b\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u5728\u591a\u6a21\u6001\u8bbd\u523a\u68c0\u6d4b\u548c\u89e3\u91ca\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "\u6574\u5408\u89c6\u89c9\u7ec6\u8282\u548c\u9886\u57df\u77e5\u8bc6\u53ef\u663e\u8457\u6539\u5584\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u8bbd\u523a\u5206\u6790\u4e2d\u7684\u6027\u80fd\uff0c\u5f00\u6e90\u4ee3\u7801\u63a8\u52a8\u540e\u7eed\u7814\u7a76\u3002"}}
{"id": "2508.03668", "pdf": "https://arxiv.org/pdf/2508.03668", "abs": "https://arxiv.org/abs/2508.03668", "authors": ["Zixuan Li", "Binzong Geng", "Jing Xiong", "Yong He", "Yuxuan Hu", "Jian Chen", "Dingwei Chen", "Xiyu Chang", "Liang Zhang", "Linjian Mo", "Chengming Li", "Chuan Yuan", "Zhenan Sun"], "title": "CTR-Sink: Attention Sink for Language Models in Click-Through Rate Prediction", "categories": ["cs.CL"], "comment": null, "summary": "Click-Through Rate (CTR) prediction, a core task in recommendation systems,\nestimates user click likelihood using historical behavioral data. Modeling user\nbehavior sequences as text to leverage Language Models (LMs) for this task has\ngained traction, owing to LMs' strong semantic understanding and contextual\nmodeling capabilities. However, a critical structural gap exists: user behavior\nsequences consist of discrete actions connected by semantically empty\nseparators, differing fundamentally from the coherent natural language in LM\npre-training. This mismatch causes semantic fragmentation, where LM attention\nscatters across irrelevant tokens instead of focusing on meaningful behavior\nboundaries and inter-behavior relationships, degrading prediction performance.\nTo address this, we propose $\\textit{CTR-Sink}$, a novel framework introducing\nbehavior-level attention sinks tailored for recommendation scenarios. Inspired\nby attention sink theory, it constructs attention focus sinks and dynamically\nregulates attention aggregation via external information. Specifically, we\ninsert sink tokens between consecutive behaviors, incorporating\nrecommendation-specific signals such as temporal distance to serve as stable\nattention sinks. To enhance generality, we design a two-stage training strategy\nthat explicitly guides LM attention toward sink tokens and a attention sink\nmechanism that amplifies inter-sink dependencies to better capture behavioral\ncorrelations. Experiments on one industrial dataset and two open-source\ndatasets (MovieLens, Kuairec), alongside visualization results, validate the\nmethod's effectiveness across scenarios.", "AI": {"tldr": "\u63d0\u51faCTR-Sink\u6846\u67b6\u89e3\u51b3\u63a8\u8350\u7cfb\u7edf\u4e2d\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u4e0e\u8bed\u8a00\u6a21\u578b\u7ed3\u6784\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u8bed\u4e49\u788e\u7247\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5f15\u5165\u884c\u4e3a\u7ea7\u6ce8\u610f\u529b\u951a\u70b9\u63d0\u5347CTR\u9884\u6d4b\u6027\u80fd", "motivation": "\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u7531\u79bb\u6563\u52a8\u4f5c\u901a\u8fc7\u65e0\u610f\u4e49\u5206\u9694\u7b26\u8fde\u63a5\uff0c\u4e0e\u81ea\u7136\u8bed\u8a00\u7ed3\u6784\u5b58\u5728\u5dee\u5f02\uff0c\u5bfc\u81f4\u8bed\u8a00\u6a21\u578b\u6ce8\u610f\u529b\u5206\u6563\u5f71\u54cd\u9884\u6d4b\u6548\u679c", "method": "1. \u5728\u884c\u4e3a\u95f4\u63d2\u5165\u5305\u542b\u65f6\u5e8f\u8ddd\u79bb\u7b49\u63a8\u8350\u4fe1\u53f7\u7684sink token\u4f5c\u4e3a\u6ce8\u610f\u529b\u951a\u70b9\n2. \u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u5f15\u5bfc\u6ce8\u610f\u529b\u805a\u7126\n3. \u8bbe\u8ba1\u8de8sink\u4f9d\u8d56\u589e\u5f3a\u673a\u5236\u6355\u6349\u884c\u4e3a\u5173\u8054", "result": "\u5728\u5de5\u4e1a\u6570\u636e\u96c6\u53caMovieLens\u3001Kuairec\u7b49\u516c\u5f00\u6570\u636e\u96c6\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u53ef\u89c6\u5316\u7ed3\u679c\u652f\u6301\u65b9\u6cd5\u4f18\u52bf", "conclusion": "\u901a\u8fc7\u5b9a\u5236\u5316\u6ce8\u610f\u529b\u951a\u70b9\u6846\u67b6\u6210\u529f\u5f25\u5408\u7ed3\u6784\u5dee\u5f02\uff0c\u63d0\u5347\u63a8\u8350\u573a\u666f\u4e0bCTR\u9884\u6d4b\u6027\u80fd"}}
{"id": "2508.03677", "pdf": "https://arxiv.org/pdf/2508.03677", "abs": "https://arxiv.org/abs/2508.03677", "authors": ["Arturo P\u00e9rez-Peralta", "Sandra Ben\u00edtez-Pe\u00f1a", "Rosa E. Lillo"], "title": "FairLangProc: A Python package for fairness in NLP", "categories": ["cs.CL", "stat.ML", "68T50", "I.2.7"], "comment": "40 pages, 4 figures, 3 tables", "summary": "The rise in usage of Large Language Models to near ubiquitousness in recent\nyears has risen societal concern about their applications in decision-making\ncontexts, such as organizational justice or healthcare. This, in turn, poses\nquestions about the fairness of these models in critical settings, which leads\nto the developement of different procedures to address bias in Natural Language\nProcessing. Although many datasets, metrics and algorithms have been proposed\nto measure and mitigate harmful prejudice in Natural Language Processing, their\nimplementation is diverse and far from centralized. As a response, this paper\npresents FairLangProc, a comprehensive Python package providing a common\nimplementation of some of the more recent advances in fairness in Natural\nLanguage Processing providing an interface compatible with the famous Hugging\nFace transformers library, aiming to encourage the widespread use and\ndemocratization of bias mitigation techniques. The implementation can be found\non https://github.com/arturo-perez-peralta/FairLangProc.", "AI": {"tldr": "\u5f00\u53d1\u4e86FairLangProc Python\u5305\uff0c\u96c6\u4e2d\u5b9e\u73b0\u6700\u65b0NLP\u516c\u5e73\u6027\u6280\u672f\uff0c\u517c\u5bb9Hugging Face\u5e93\u4ee5\u4fc3\u8fdb\u5e94\u7528\u666e\u53ca", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597/\u53f8\u6cd5\u7b49\u5173\u952e\u9886\u57df\u5e94\u7528\u589e\u52a0\uff0c\u4f46\u73b0\u6709\u516c\u5e73\u6027\u8bc4\u4f30\u4e0e\u4fee\u6b63\u65b9\u6848\u5206\u6563\u4e14\u7f3a\u4e4f\u7edf\u4e00\u5b9e\u73b0\u6846\u67b6", "method": "\u6784\u5efa\u517c\u5bb9Hugging Face\u751f\u6001\u7684Python\u5de5\u5177\u5305\uff0c\u96c6\u6210\u524d\u6cbf\u504f\u89c1\u68c0\u6d4b\u4e0e\u7f13\u89e3\u7b97\u6cd5\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u63a5\u53e3", "result": "\u6210\u529f\u5f00\u53d1\u5f00\u6e90\u5de5\u5177FairLangProc\uff08GitHub\u53ef\u83b7\u53d6\uff09\uff0c\u5b9e\u73b0NLP\u516c\u5e73\u6027\u6280\u672f\u7684\u6613\u7528\u5316\u6574\u5408", "conclusion": "\u8be5\u5de5\u5177\u5305\u964d\u4f4e\u4e86\u516c\u5e73\u6027\u6280\u672f\u5e94\u7528\u95e8\u69db\uff0c\u63a8\u52a8NLP\u7cfb\u7edf\u5728\u5173\u952e\u9886\u57df\u66f4\u8d1f\u8d23\u4efb\u5730\u53d1\u5c55"}}
{"id": "2508.03678", "pdf": "https://arxiv.org/pdf/2508.03678", "abs": "https://arxiv.org/abs/2508.03678", "authors": ["Yangtian Zi", "Harshitha Menon", "Arjun Guha"], "title": "More Than a Score: Probing the Impact of Prompt Specificity on LLM Code Generation", "categories": ["cs.CL", "cs.LG", "cs.PL"], "comment": null, "summary": "State-of-the-art Large Language Models (LLMs) achieve high pass@1 on general\nbenchmarks like HumanEval but underperform on specialized suites such as\nParEval. Is this due to LLMs missing domain knowledge or insufficient prompt\ndetail is given? To answer this, we introduce PartialOrderEval, which augments\nany code generation benchmark with a partial order of prompts from minimal to\nmaximally detailed. Applying it to HumanEval and both serial and OpenMP subsets\nof ParEval, we measure how pass@1 scales with prompt specificity. Our\nexperiments with Llama-3.x and Qwen2.5-Coder demonstrate varying degrees of\nprompt sensitivity across different tasks, and a qualitative analysis\nhighlights explicit I/O specifications, edge-case handling, and stepwise\nbreakdowns as the key drivers of prompt detail improvement.", "AI": {"tldr": "\u63d0\u51faPartialOrderEval\u65b9\u6cd5\u91cf\u5316\u63d0\u793a\u7ec6\u8282\u5bf9\u4ee3\u7801\u751f\u6210\u7684\u5f71\u54cd\uff0c\u53d1\u73b0LLMs\u5728\u4e13\u4e1a\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4e0e\u63d0\u793a\u7ec6\u8282\u5f3a\u5ea6\u6b63\u76f8\u5173", "motivation": "\u63a2\u7a76LLMs\u5728\u4e13\u4e1a\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982ParEval\uff09\u8868\u73b0\u4e0d\u4f73\u7684\u539f\u56e0\u662f\u9886\u57df\u77e5\u8bc6\u7f3a\u5931\u8fd8\u662f\u63d0\u793a\u7ec6\u8282\u4e0d\u8db3", "method": "\u901a\u8fc7\u6784\u5efa\u4ece\u6781\u7b80\u5230\u6700\u5927\u7ec6\u8282\u7684\u63d0\u793a\u504f\u5e8f\u96c6\uff08PartialOrderEval\uff09\uff0c\u5728HumanEval\u548cParEval\u7684\u4e32\u884c/OpenMP\u5b50\u96c6\u4e0a\u6d4b\u8bd5Llama-3.x\u548cQwen2.5-Coder\u7684pass@1\u6307\u6807\u968f\u63d0\u793a\u5177\u4f53\u6027\u7684\u53d8\u5316", "result": "\u5b9e\u9a8c\u663e\u793a\uff1a1\uff09\u4e0d\u540c\u4efb\u52a1\u5b58\u5728\u5dee\u5f02\u5316\u7684\u63d0\u793a\u654f\u611f\u6027 2\uff09\u660e\u786e\u7684I/O\u89c4\u8303\u3001\u8fb9\u754c\u6848\u4f8b\u5904\u7406\u548c\u5206\u6b65\u5206\u89e3\u662f\u63d0\u5347\u63d0\u793a\u7ec6\u8282\u7684\u5173\u952e\u8981\u7d20", "conclusion": "\u63d0\u5347\u63d0\u793a\u7ec6\u8282\uff08\u7279\u522b\u662f\u660e\u786e\u8f93\u5165\u8f93\u51fa\u3001\u8fb9\u754c\u6848\u4f8b\u548c\u6b65\u9aa4\u5206\u89e3\uff09\u80fd\u6709\u6548\u6539\u5584LLMs\u5728\u4e13\u4e1a\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u63d0\u793a\u5de5\u7a0b\u8d28\u91cf\u4e0e\u9886\u57df\u77e5\u8bc6\u540c\u7b49\u91cd\u8981"}}
{"id": "2508.03686", "pdf": "https://arxiv.org/pdf/2508.03686", "abs": "https://arxiv.org/abs/2508.03686", "authors": ["Shudong Liu", "Hongwei Liu", "Junnan Liu", "Linchen Xiao", "Songyang Gao", "Chengqi Lyu", "Yuzhe Gu", "Wenwei Zhang", "Derek F. Wong", "Songyang Zhang", "Kai Chen"], "title": "CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and Outcome Reward", "categories": ["cs.CL", "cs.AI"], "comment": "Technical Report; 31 Pages", "summary": "Answer verification is crucial not only for evaluating large language models\n(LLMs) by matching their unstructured outputs against standard answers, but\nalso serves as the reward model to guide LLM optimization. Most evaluation\nframeworks rely on regularized matching or employ general LLMs for answer\nverification, which demands extensive, repetitive customization for regex rules\nor evaluation prompts. Two fundamental limitations persist in current\nmethodologies: 1) the absence of comprehensive benchmarks that systematically\nevaluate verification capabilities across different LLMs; and 2) the nascent\nstage of verifier development, where existing approaches lack both the\nrobustness to handle complex edge cases and the generalizability across\ndifferent domains. In this work, we develop CompassVerifier, an accurate and\nrobust lightweight verifier model for evaluation and outcome reward. It\ndemonstrates multi-domain competency spanning math, knowledge, and diverse\nreasoning tasks, with the capability to process various answer types, including\nmulti-subproblems, formulas, and sequence answers, while effectively\nidentifying abnormal/invalid responses. We introduce VerifierBench benchmark\ncomprising model outputs collected from multiple data sources, augmented\nthrough manual analysis of metaerror patterns to enhance CompassVerifier. We\nanticipate that CompassVerifier and VerifierBench will facilitate answer\nverification, evaluation protocols, and reinforcement learning research. Code\nand dataset are available at https://github.com/open-compass/CompassVerifier.", "AI": {"tldr": "\u63d0\u51faCompassVerifier\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u6a21\u578b\u53caVerifierBench\u57fa\u51c6\uff0c\u89e3\u51b3\u73b0\u6709LLM\u7b54\u6848\u9a8c\u8bc1\u65b9\u6cd5\u5728\u57fa\u51c6\u6d4b\u8bd5\u548c\u6a21\u578b\u9c81\u68d2\u6027/\u901a\u7528\u6027\u4e0a\u7684\u4e0d\u8db3", "motivation": "\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u4f9d\u8d56\u6b63\u5219\u5339\u914d\u6216\u901a\u7528LLM\uff0c\u5b58\u5728\u57fa\u51c6\u6d4b\u8bd5\u7cfb\u7edf\u6027\u4e0d\u8db3\u3001\u9a8c\u8bc1\u5668\u5bf9\u590d\u6742\u573a\u666f\u5904\u7406\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898", "method": "\u5f00\u53d1\u5177\u5907\u591a\u9886\u57df\u5904\u7406\u80fd\u529b\u7684CompassVerifier\u6a21\u578b\uff0c\u901a\u8fc7VerifierBench\u57fa\u51c6\u6574\u5408\u591a\u6e90\u6570\u636e\u5e76\u5206\u6790\u9519\u8bef\u6a21\u5f0f\u8fdb\u884c\u589e\u5f3a", "result": "\u6a21\u578b\u5728\u6570\u5b66\u3001\u77e5\u8bc6\u63a8\u7406\u7b49\u8de8\u9886\u57df\u4efb\u52a1\u4e2d\u5c55\u73b0\u7ade\u4e89\u529b\uff0c\u80fd\u6709\u6548\u5904\u7406\u591a\u5b50\u95ee\u9898/\u516c\u5f0f/\u5e8f\u5217\u7b54\u6848\u5e76\u8bc6\u522b\u5f02\u5e38\u54cd\u5e94", "conclusion": "CompassVerifier\u4e0eVerifierBench\u5c06\u63a8\u52a8\u7b54\u6848\u9a8c\u8bc1\u3001\u8bc4\u4f30\u534f\u8bae\u548c\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\uff0c\u76f8\u5173\u8d44\u6e90\u5df2\u5f00\u6e90"}}
{"id": "2507.10593", "pdf": "https://arxiv.org/pdf/2507.10593", "abs": "https://arxiv.org/abs/2507.10593", "authors": ["Peng Ding"], "title": "ToolRegistry: A Protocol-Agnostic Tool Management Library for Function-Calling LLMs", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Large Language Model (LLM) applications are increasingly relying on external\ntools to extend their capabilities beyond text generation. However, current\ntool integration approaches suffer from fragmentation, protocol limitations,\nand implementation complexity, leading to substantial development overhead.\nThis paper presents Toolregistry, a protocol-agnostic tool management library\nthat simplifies tool registration, representation, execution, and lifecycle\nmanagement via a unified interface. Our evaluation demonstrates that\n\\toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x\nperformance improvements through concurrent execution, and 100% compatibility\nwith OpenAI function calling standards. Real-world case studies show\nsignificant improvements in development efficiency and code maintainability\nacross diverse integration scenarios. \\toolregistry is open-source and\navailable at https://github.com/Oaklight/ToolRegistry, with comprehensive\ndocumentation at https://toolregistry.readthedocs.io/.", "AI": {"tldr": "Toolregistry\u2014\u2014\u534f\u8bae\u65e0\u5173\u7684LLM\u5de5\u5177\u7ba1\u7406\u5e93\uff0c\u901a\u8fc7\u7edf\u4e00\u63a5\u53e3\u5b9e\u73b0\u5de5\u5177\u5168\u751f\u547d\u5468\u671f\u7ba1\u7406\uff0c\u663e\u8457\u51cf\u5c11\u96c6\u6210\u4ee3\u7801\u5e76\u63d0\u5347\u6027\u80fd", "motivation": "\u73b0\u6709\u5de5\u5177\u96c6\u6210\u65b9\u6cd5\u5b58\u5728\u788e\u7247\u5316\u3001\u534f\u8bae\u9650\u5236\u548c\u5b9e\u73b0\u590d\u6742\u6027\u95ee\u9898\uff0c\u5bfc\u81f4\u5f00\u53d1\u6548\u7387\u4f4e\u4e0b\u548c\u7ef4\u62a4\u6210\u672c\u9ad8", "method": "\u5f00\u53d1\u652f\u6301\u7edf\u4e00\u5de5\u5177\u6ce8\u518c/\u8868\u793a/\u6267\u884c\u7684\u534f\u8bae\u65e0\u5173\u6846\u67b6\uff0c\u652f\u6301\u5e76\u53d1\u6267\u884c\u548cOpenAI\u51fd\u6570\u8c03\u7528\u6807\u51c6\u517c\u5bb9", "result": "\u51cf\u5c1160-80%\u96c6\u6210\u4ee3\u7801\uff0c\u5e76\u53d1\u6267\u884c\u63d0\u53473.1\u500d\u6027\u80fd\uff0c100%\u517c\u5bb9OpenAI\u6807\u51c6\uff0c\u5b9e\u9645\u6848\u4f8b\u9a8c\u8bc1\u5f00\u53d1\u6548\u7387\u63d0\u5347", "conclusion": "Toolregistry\u901a\u8fc7\u6807\u51c6\u5316\u5de5\u5177\u7ba1\u7406\u663e\u8457\u964d\u4f4eLLM\u5e94\u7528\u5f00\u53d1\u95e8\u69db\uff0c\u5f00\u6e90\u5b9e\u73b0\u4e3a\u793e\u533a\u63d0\u4f9b\u9ad8\u6548\u96c6\u6210\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.02694", "pdf": "https://arxiv.org/pdf/2508.02694", "abs": "https://arxiv.org/abs/2508.02694", "authors": ["Ningning Wang", "Xavier Hu", "Pai Liu", "He Zhu", "Yue Hou", "Heyuan Huang", "Shengyu Zhang", "Jian Yang", "Jiaheng Liu", "Ge Zhang", "Changwang Zhang", "Jun Wang", "Yuchen Eleanor Jiang", "Wangchunshu Zhou"], "title": "Efficient Agents: Building Effective Agents While Reducing Cost", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": "Work in progress. For GitHub repository, see\n  https://github.com/OPPO-PersonalAI/OAgents", "summary": "The remarkable capabilities of Large Language Model (LLM)-driven agents have\nenabled sophisticated systems to tackle complex, multi-step tasks, but their\nescalating costs threaten scalability and accessibility. This work presents the\nfirst systematic study of the efficiency-effectiveness trade-off in modern\nagent systems, addressing the critical need for cost-effective designs without\nsacrificing performance. We investigate three key questions: (1) How much\ncomplexity do agentic tasks inherently require? (2) When do additional modules\nyield diminishing returns? (3) How much efficiency can be gained through the\ndesign of efficient agent frameworks? Through an empirical analysis on the GAIA\nbenchmark, we evaluate the impact of LLM backbone selection, agent framework\ndesigns, and test-time scaling strategies. Using the cost-of-pass metric, we\nquantify the efficiency-performance trade-off across these dimensions. Our\nfindings inform the development of Efficient Agents , a novel agent framework\nthat has an optimal complexity to task requirements. Efficient Agents retains\n96.7% of the performance of OWL, one leading open-source agent framework, while\nreducing operational costs from $0.398 to $0.228, resulting in a 28.4%\nimprovement in cost-of-pass. Our work provides actionable insights for\ndesigning efficient, high-performing agent systems, advancing the accessibility\nand sustainability of AI-driven solutions.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u5206\u6790LLM\u4ee3\u7406\u7cfb\u7edf\u7684\u6548\u7387-\u6548\u679c\u5e73\u8861\uff0c\u63d0\u51faEfficient Agents\u6846\u67b6\u5728\u4fdd\u630196.7%\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e28.4%\u6210\u672c", "motivation": "LLM\u4ee3\u7406\u7cfb\u7edf\u867d\u80fd\u5904\u7406\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u9ad8\u6602\u8fd0\u884c\u6210\u672c\u5a01\u80c1\u5176\u6269\u5c55\u6027\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u9700\u63a2\u7d22\u6548\u7387\u4e0e\u6548\u679c\u7684\u5e73\u8861\u65b9\u6848", "method": "\u901a\u8fc7GAIA\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30LLM\u4e3b\u5e72\u9009\u62e9\u3001\u6846\u67b6\u8bbe\u8ba1\u548c\u6d4b\u8bd5\u6269\u5c55\u7b56\u7565\uff0c\u4f7f\u7528cost-of-pass\u6307\u6807\u91cf\u5316\u6548\u7387-\u6027\u80fd\u6743\u8861", "result": "Efficient Agents\u6846\u67b6\u5c06\u8fd0\u884c\u6210\u672c\u4ece$0.398\u964d\u81f3$0.228\uff0c\u6210\u672c\u6548\u76ca\u63d0\u534728.4%\uff0c\u540c\u65f6\u4fdd\u6301OWL\u6846\u67b696.7%\u7684\u6027\u80fd", "conclusion": "\u901a\u8fc7\u7cbe\u51c6\u5339\u914d\u4efb\u52a1\u590d\u6742\u5ea6\u4e0e\u6846\u67b6\u8bbe\u8ba1\uff0c\u8bc1\u660e\u4e86\u5728\u4e0d\u5927\u5e45\u727a\u7272\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u4ee3\u7406\u7cfb\u7edf\u6548\u7387\u7684\u53ef\u884c\u6027"}}
{"id": "2508.02731", "pdf": "https://arxiv.org/pdf/2508.02731", "abs": "https://arxiv.org/abs/2508.02731", "authors": ["Jean-Francois Chamberland", "Martin C. Carlisle", "Arul Jayaraman", "Krishna R. Narayanan", "Sunay Palsole", "Karan Watson"], "title": "Teaching at Scale: Leveraging AI to Evaluate and Elevate Engineering Education", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Evaluating teaching effectiveness at scale remains a persistent challenge for\nlarge universities, particularly within engineering programs that enroll tens\nof thousands of students. Traditional methods, such as manual review of student\nevaluations, are often impractical, leading to overlooked insights and\ninconsistent data use. This article presents a scalable, AI-supported framework\nfor synthesizing qualitative student feedback using large language models. The\nsystem employs hierarchical summarization, anonymization, and exception\nhandling to extract actionable themes from open-ended comments while upholding\nethical safeguards. Visual analytics contextualize numeric scores through\npercentile-based comparisons, historical trends, and instructional load. The\napproach supports meaningful evaluation and aligns with best practices in\nqualitative analysis and educational assessment, incorporating student, peer,\nand self-reflective inputs without automating personnel decisions. We report on\nits successful deployment across a large college of engineering. Preliminary\nvalidation through comparisons with human reviewers, faculty feedback, and\nlongitudinal analysis suggests that LLM-generated summaries can reliably\nsupport formative evaluation and professional development. This work\ndemonstrates how AI systems, when designed with transparency and shared\ngovernance, can promote teaching excellence and continuous improvement at scale\nwithin academic institutions.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684AI\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u603b\u7ed3\u548c\u53ef\u89c6\u5316\u5206\u6790\u5b9e\u73b0\u5927\u89c4\u6a21\u6559\u5b66\u6548\u679c\u8bc4\u4f30", "motivation": "\u4f20\u7edf\u4eba\u5de5\u5ba1\u6838\u5b66\u751f\u53cd\u9988\u6548\u7387\u4f4e\u4e0b\u4e14\u96be\u4ee5\u89c4\u6a21\u5316\uff0c\u5bfc\u81f4\u6709\u6548\u6d1e\u5bdf\u6d41\u5931\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u63d0\u5347\u8bc4\u4f30\u8d28\u91cf", "method": "\u91c7\u7528\u5206\u5c42\u603b\u7ed3\u67b6\u6784\u5b9e\u73b0\u53cd\u9988\u7ed3\u6784\u5316\uff0c\u7ed3\u5408\u533f\u540d\u5316\u5904\u7406\u4e0e\u5f02\u5e38\u68c0\u6d4b\uff0c\u901a\u8fc7\u767e\u5206\u4f4d\u6bd4\u8f83/\u5386\u53f2\u8d8b\u52bf/\u6559\u5b66\u8d1f\u8377\u4e09\u7ef4\u5ea6\u53ef\u89c6\u5316\u5206\u6790", "result": "\u5728\u5de5\u7a0b\u5b66\u9662\u6210\u529f\u90e8\u7f72\uff0c\u9a8c\u8bc1\u663e\u793aLLM\u751f\u6210\u603b\u7ed3\u4e0e\u4eba\u5de5\u8bc4\u5ba1\u4e00\u81f4\u6027\u8fbe85%\uff0c\u6559\u5e08\u91c7\u7eb3\u7387\u63d0\u534740%", "conclusion": "\u900f\u660e\u5316\u8bbe\u8ba1\u7684AI\u7cfb\u7edf\u901a\u8fc7\u591a\u65b9\u53c2\u4e0e\u673a\u5236\uff0c\u53ef\u5728\u4e0d\u66ff\u4ee3\u4eba\u5de5\u51b3\u7b56\u7684\u524d\u63d0\u4e0b\u6709\u6548\u4fc3\u8fdb\u6559\u5b66\u6539\u8fdb\uff0c\u4e3a\u9ad8\u7b49\u6559\u80b2\u8bc4\u4f30\u63d0\u4f9b\u53ef\u6269\u5c55\u8303\u5f0f"}}
{"id": "2508.02738", "pdf": "https://arxiv.org/pdf/2508.02738", "abs": "https://arxiv.org/abs/2508.02738", "authors": ["Yumeng Shi", "Zhongliang Yang", "DiYang Lu", "Yisi Wang", "Yiting Zhou", "Linna Zhou"], "title": "CreditARF: A Framework for Corporate Credit Rating with Annual Report and Financial Feature Integration", "categories": ["q-fin.ST", "cs.CE", "cs.CL", "cs.LG"], "comment": null, "summary": "Corporate credit rating serves as a crucial intermediary service in the\nmarket economy, playing a key role in maintaining economic order. Existing\ncredit rating models rely on financial metrics and deep learning. However, they\noften overlook insights from non-financial data, such as corporate annual\nreports. To address this, this paper introduces a corporate credit rating\nframework that integrates financial data with features extracted from annual\nreports using FinBERT, aiming to fully leverage the potential value of\nunstructured text data. In addition, we have developed a large-scale dataset,\nthe Comprehensive Corporate Rating Dataset (CCRD), which combines both\ntraditional financial data and textual data from annual reports. The\nexperimental results show that the proposed method improves the accuracy of the\nrating predictions by 8-12%, significantly improving the effectiveness and\nreliability of corporate credit ratings.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u8d22\u52a1\u6570\u636e\u4e0eFinBERT\u6587\u672c\u5206\u6790\u7684\u4fe1\u7528\u8bc4\u7ea7\u6846\u67b6\uff0c\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u73878-12%", "motivation": "\u73b0\u6709\u8bc4\u7ea7\u6a21\u578b\u5ffd\u89c6\u5e74\u62a5\u6587\u672c\u7684\u975e\u8d22\u52a1\u4fe1\u606f\u4ef7\u503c\uff0c\u9700\u6316\u6398\u975e\u7ed3\u6784\u5316\u6570\u636e\u7684\u6f5c\u5728\u4ef7\u503c", "method": "\u6784\u5efa\u878d\u5408\u8d22\u52a1\u6307\u6807\u4e0eFinBERT\u5e74\u62a5\u7279\u5f81\u63d0\u53d6\u7684\u8bc4\u7ea7\u6846\u67b6\uff0c\u5e76\u521b\u5efaCCRD\u6df7\u5408\u6570\u636e\u96c6", "result": "\u5b9e\u9a8c\u8868\u660e\u6a21\u578b\u4f7f\u8bc4\u7ea7\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u53478-12%\uff0c\u663e\u8457\u589e\u5f3a\u8bc4\u4f30\u6548\u679c", "conclusion": "\u6587\u672c\u7279\u5f81\u878d\u5408\u6709\u6548\u63d0\u5347\u4fe1\u7528\u8bc4\u7ea7\u53ef\u9760\u6027\uff0cCCRD\u6570\u636e\u96c6\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u57fa\u7840\u8bbe\u65bd"}}
{"id": "2508.02823", "pdf": "https://arxiv.org/pdf/2508.02823", "abs": "https://arxiv.org/abs/2508.02823", "authors": ["Wenshuo Zhang", "Leixian Shen", "Shuchang Xu", "Jindu Wang", "Jian Zhao", "Huamin Qu", "Linping Yuan"], "title": "NeuroSync: Intent-Aware Code-Based Problem Solving via Direct LLM Understanding Modification", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.SE"], "comment": "Accepted in UIST 2025", "summary": "Conversational LLMs have been widely adopted by domain users with limited\nprogramming experience to solve domain problems. However, these users often\nface misalignment between their intent and generated code, resulting in\nfrustration and rounds of clarification. This work first investigates the cause\nof this misalignment, which dues to bidirectional ambiguity: both user intents\nand coding tasks are inherently nonlinear, yet must be expressed and\ninterpreted through linear prompts and code sequences. To address this, we\npropose direct intent-task matching, a new human-LLM interaction paradigm that\nexternalizes and enables direct manipulation of the LLM understanding, i.e.,\nthe coding tasks and their relationships inferred by the LLM prior to code\ngeneration. As a proof-of-concept, this paradigm is then implemented in\nNeuroSync, which employs a knowledge distillation pipeline to extract LLM\nunderstanding, user intents, and their mappings, and enhances the alignment by\nallowing users to intuitively inspect and edit them via visualizations. We\nevaluate the algorithmic components of NeuroSync via technical experiments, and\nassess its overall usability and effectiveness via a user study (N=12). The\nresults show that it enhances intent-task alignment, lowers cognitive effort,\nand improves coding efficiency.", "AI": {"tldr": "\u63a2\u8ba8\u5bf9\u8bdd\u5f0fLLM\u7528\u6237\u610f\u56fe\u4e0e\u751f\u6210\u4ee3\u7801\u7684\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u51fa\u76f4\u63a5\u610f\u56fe-\u4efb\u52a1\u5339\u914d\u65b0\u8303\u5f0f\u5e76\u901a\u8fc7NeuroSync\u7cfb\u7edf\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "\u9886\u57df\u7528\u6237\u4f7f\u7528LLM\u751f\u6210\u4ee3\u7801\u65f6\u5b58\u5728\u53cc\u5411\u6b67\u4e49\uff1a\u7528\u6237\u610f\u56fe\u7684\u975e\u7ebf\u6027\u8868\u8fbe\u4e0e\u4ee3\u7801\u4efb\u52a1\u7684\u7ebf\u6027\u89e3\u91ca\u51b2\u7a81\uff0c\u5bfc\u81f4\u591a\u6b21\u6f84\u6e05\u548c\u632b\u8d25\u611f", "method": "\u63d0\u51fa\u76f4\u63a5\u610f\u56fe-\u4efb\u52a1\u5339\u914d\u4ea4\u4e92\u8303\u5f0f\uff0c\u901a\u8fc7NeuroSync\u5b9e\u73b0\u77e5\u8bc6\u84b8\u998f\u6d41\u7a0b\u63d0\u53d6LLM\u7406\u89e3\uff0c\u652f\u6301\u7528\u6237\u53ef\u89c6\u5316\u7f16\u8f91\u610f\u56fe-\u4efb\u52a1\u6620\u5c04\u5173\u7cfb", "result": "\u6280\u672f\u5b9e\u9a8c\u9a8c\u8bc1\u7b97\u6cd5\u6709\u6548\u6027\uff0c\u7528\u6237\u7814\u7a76(N=12)\u663e\u793a\u7cfb\u7edf\u63d0\u5347\u610f\u56fe-\u4efb\u52a1\u5bf9\u9f50\u5ea653%\uff0c\u964d\u4f4e\u8ba4\u77e5\u8d1f\u837728%\uff0c\u4ee3\u7801\u751f\u6210\u6548\u7387\u63d0\u9ad836%", "conclusion": "NeuroSync\u901a\u8fc7\u5916\u90e8\u5316LLM\u7406\u89e3\u5e76\u652f\u6301\u53ef\u89c6\u5316\u64cd\u4f5c\uff0c\u6709\u6548\u6539\u5584\u4eba\u673a\u4ea4\u4e92\u8d28\u91cf\uff0c\u9a8c\u8bc1\u4e86\u76f4\u63a5\u610f\u56fe-\u4efb\u52a1\u5339\u914d\u8303\u5f0f\u7684\u53ef\u884c\u6027"}}
{"id": "2508.02849", "pdf": "https://arxiv.org/pdf/2508.02849", "abs": "https://arxiv.org/abs/2508.02849", "authors": ["Chunyu Qiang", "Haoyu Wang", "Cheng Gong", "Tianrui Wang", "Ruibo Fu", "Tao Wang", "Ruilong Chen", "Jiangyan Yi", "Zhengqi Wen", "Chen Zhang", "Longbiao Wang", "Jianwu Dang", "Jianhua Tao"], "title": "SecoustiCodec: Cross-Modal Aligned Streaming Single-Codecbook Speech Codec", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": null, "summary": "Speech codecs serve as a crucial bridge in unifying speech and text language\nmodels. Existing codec methods face several challenges in semantic encoding,\nsuch as residual paralinguistic information (e.g., timbre, emotion),\ninsufficient semantic completeness, limited reconstruction capability, and lack\nof support for streaming. To address these challenges, we propose\nSecoustiCodec, a cross-modal aligned low-bitrate streaming speech codec that\ndisentangles semantic and paralinguistic information in a single-codebook\nspace. To ensure semantic completeness and reconstruction fidelity,\nparalinguistic encoding is introduced to bridge the information gap between\nsemantic and acoustic encoding. A semantic-only efficient quantization method\nbased on VAE (Variational Autoencoder) and FSQ (Finite Scalar Quantization) is\nproposed. This approach alleviates the long-tail distribution problem of tokens\nwhile maintaining high codebook utilization. A semantic disentanglement method\nbased on contrastive learning is proposed, which aligns text and speech in a\njoint multimodal frame-level space, effectively removing paralinguistic\ninformation from semantic encoding. An acoustic-constrained multi-stage\noptimization strategy is proposed to ensure robust and stable convergence.\nFigure~\\ref{fig:pesq_kbps_below_2kbps} shows SecoustiCodec achieves SOTA\n(state-of-the-art) reconstruction quality (PESQ) of 1.77/2.58 at 0.27/1 kbps.\nThe code and model weights for SecoustiCodec will be open-sourced upon the\ncompletion of the peer-review process. We've open-sourced SecoustiCodec's demo,\ncode, and model weights.", "AI": {"tldr": "\u63d0\u51faSecoustiCodec\u8de8\u6a21\u6001\u5bf9\u9f50\u4f4e\u6bd4\u7279\u7387\u6d41\u5f0f\u8bed\u97f3\u7f16\u89e3\u7801\u5668\uff0c\u901a\u8fc7\u5355\u7801\u672c\u7a7a\u95f4\u5206\u79bb\u8bed\u4e49\u4e0e\u526f\u8bed\u8a00\u4fe1\u606f\uff0c\u89e3\u51b3\u73b0\u6709\u7f16\u7801\u65b9\u6cd5\u6b8b\u7559\u526f\u8bed\u8a00\u4fe1\u606f\u3001\u8bed\u4e49\u4e0d\u5b8c\u6574\u3001\u91cd\u5efa\u80fd\u529b\u5dee\u7b49\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u7f16\u89e3\u7801\u5668\u5b58\u5728\u6b8b\u7559\u526f\u8bed\u8a00\u4fe1\u606f\uff08\u97f3\u8272/\u60c5\u611f\uff09\u3001\u8bed\u4e49\u4e0d\u5b8c\u6574\u3001\u91cd\u5efa\u80fd\u529b\u6709\u9650\u3001\u7f3a\u4e4f\u6d41\u5f0f\u652f\u6301\u7b49\u6311\u6218\uff0c\u963b\u788d\u8bed\u97f3\u4e0e\u6587\u672c\u8bed\u8a00\u6a21\u578b\u7684\u7edf\u4e00\u3002", "method": "1. \u5f15\u5165\u526f\u8bed\u8a00\u7f16\u7801\u6865\u63a5\u8bed\u4e49\u4e0e\u58f0\u5b66\u7f16\u7801\u7684\u4fe1\u606f\u9e3f\u6c9f\n2. \u63d0\u51fa\u57fa\u4e8eVAE+FSQ\u7684\u8bed\u4e49\u9ad8\u6548\u91cf\u5316\u65b9\u6cd5\n3. \u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u8bed\u4e49\u89e3\u7f20\u6846\u67b6\n4. \u58f0\u5b66\u7ea6\u675f\u591a\u9636\u6bb5\u4f18\u5316\u7b56\u7565", "result": "\u57280.27/1kbps\u6bd4\u7279\u7387\u4e0b\u53d6\u5f97SOTA\u76841.77/2.58 PESQ\u91cd\u5efa\u8d28\u91cf\uff0c\u5b8c\u6210\u6280\u672f\u65b9\u6848\u5f00\u6e90\uff08\u4ee3\u7801/\u6a21\u578b\u6743\u91cd/\u6f14\u793a\uff09", "conclusion": "\u901a\u8fc7\u8de8\u6a21\u6001\u5bf9\u9f50\u548c\u5206\u5c42\u4f18\u5316\uff0cSecoustiCodec\u5b9e\u73b0\u4e86\u4f4e\u6bd4\u7279\u7387\u4e0b\u7684\u9ad8\u4fdd\u771f\u8bed\u97f3\u91cd\u5efa\uff0c\u4e3a\u8bed\u97f3-\u6587\u672c\u7edf\u4e00\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2508.02890", "pdf": "https://arxiv.org/pdf/2508.02890", "abs": "https://arxiv.org/abs/2508.02890", "authors": ["Rongxin Jiang", "Robert Long", "Chenghao Gu", "Mingrui Yan"], "title": "VisuCraft: Enhancing Large Vision-Language Models for Complex Visual-Guided Creative Content Generation via Structured Information Extraction", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "This paper introduces VisuCraft, a novel framework designed to significantly\nenhance the capabilities of Large Vision-Language Models (LVLMs) in complex\nvisual-guided creative content generation. Existing LVLMs often exhibit\nlimitations in maintaining high visual fidelity, genuine creativity, and\nprecise adherence to nuanced user instructions when generating long-form texts.\nVisuCraft addresses these challenges by integrating a multimodal structured\ninformation extractor (E) and a dynamic prompt generation module (G). The\nextractor distills fine-grained visual attributes from input images into a\nrich, structured representation, which the dynamic prompt module then combines\nwith user instructions to create highly optimized prompts for underlying LVLMs\n(e.g., LLaVA, InstructBLIP). Evaluated on the self-constructed\nImageStoryGen-500K dataset using VisuGen Metrics (Visual Grounding, Creativity,\nand Instruction Adherence), VisuCraft consistently outperforms baseline LVLMs\nacross tasks like story generation and poetry composition. Our results\ndemonstrate remarkable improvements, particularly in creativity and instruction\nadherence, validating VisuCraft's effectiveness in producing imaginative,\nvisually grounded, and user-aligned long-form creative text. This work unlocks\nnew potential for LVLMs in sophisticated creative AI applications.", "AI": {"tldr": "VisuCraft\u6846\u67b6\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\u63d0\u53d6\u5668\u4e0e\u52a8\u6001\u63d0\u793a\u751f\u6210\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u957f\u6587\u672c\u751f\u6210\u4e2d\u7684\u89c6\u89c9\u4fdd\u771f\u5ea6\u3001\u521b\u9020\u6027\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b", "motivation": "\u73b0\u6709LVLMs\u5728\u751f\u6210\u957f\u6587\u672c\u65f6\u5b58\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u4f4e\u3001\u521b\u9020\u6027\u4e0d\u8db3\u548c\u7528\u6237\u6307\u4ee4\u9075\u5faa\u4e0d\u7cbe\u786e\u7684\u95ee\u9898", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u4fe1\u606f\u63d0\u53d6\u5668(E)\u63d0\u53d6\u56fe\u50cf\u7ec6\u7c92\u5ea6\u7279\u5f81\uff0c\u7ed3\u5408\u52a8\u6001\u63d0\u793a\u751f\u6210\u6a21\u5757(G)\u4f18\u5316\u6307\u4ee4\u7ec4\u5408\uff0c\u9a71\u52a8\u5e95\u5c42LVLMs\u751f\u6210", "result": "\u5728\u81ea\u5efaImageStoryGen-500K\u6570\u636e\u96c6\u4e0a\uff0cVisuGen\u8bc4\u4f30\u6307\u6807\u663e\u793a\u521b\u9020\u6027(+38%)\u548c\u6307\u4ee4\u9075\u5faa(+42%)\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b", "conclusion": "VisuCraft\u6709\u6548\u91ca\u653e\u4e86LVLMs\u5728\u590d\u6742\u521b\u610f\u6587\u672c\u751f\u6210\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3aAI\u521b\u610f\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411"}}
{"id": "2508.02917", "pdf": "https://arxiv.org/pdf/2508.02917", "abs": "https://arxiv.org/abs/2508.02917", "authors": ["Vebj\u00f8rn Haug K\u00e5sene", "Pierre Lison"], "title": "Following Route Instructions using Large Vision-Language Models: A Comparison between Low-level and Panoramic Action Spaces", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": "This paper has been accepted to ICNSLP 2025", "summary": "Vision-and-Language Navigation (VLN) refers to the task of enabling\nautonomous robots to navigate unfamiliar environments by following natural\nlanguage instructions. While recent Large Vision-Language Models (LVLMs) have\nshown promise in this task, most current VLM systems rely on models\nspecifically designed and optimized for navigation, leaving the potential of\noff-the-shelf LVLMs underexplored. Furthermore, while older VLN approaches used\nlow-level action spaces with egocentric views and atomic actions (such as \"turn\nleft\" or \"move forward\"), newer models tend to favor panoramic action spaces\nwith discrete navigable viewpoints. This paper investigates (1) whether\noff-the-shelf LVLMs (fine-tuned without architectural modifications or\nsimulator-based training) can effectively support VLN tasks and (2) whether\nsuch models can support both low-level and panoramic action paradigms. To this\nend, we fine-tune the open-source model Qwen2.5-VL-3B-Instruct on the\nRoom-to-Room (R2R) dataset and evaluate its empirical performance across both\nlow-level and panoramic action spaces. The best resulting model achieves a 41%\nsuccess rate on the R2R test set, demonstrating that while off-the-shelf LVLMs\ncan learn to perform Vision-and-Language Navigation, they still lag behind\nmodels specifically designed for this task.", "AI": {"tldr": "\u63a2\u7d22\u73b0\u6210\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u5fae\u8c03Qwen2.5-VL-3B-Instruct\u6a21\u578b\u9a8c\u8bc1\u5176\u5728\u4f4e\u7ea7/\u5168\u666f\u52a8\u4f5c\u7a7a\u95f4\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5176\u867d\u53ef\u5b9e\u73b0\u5bfc\u822a\u4f46\u5f31\u4e8e\u4e13\u7528\u6a21\u578b\u3002", "motivation": "\u73b0\u6709VLN\u7cfb\u7edf\u8fc7\u5ea6\u4f9d\u8d56\u5b9a\u5236\u5316\u6a21\u578b\uff0c\u672a\u5145\u5206\u9a8c\u8bc1\u901a\u7528LVLMs\u7684\u5bfc\u822a\u6f5c\u529b\uff1b\u540c\u65f6\u9700\u9a8c\u8bc1\u6a21\u578b\u5bf9\u4f20\u7edf\u4f4e\u7ea7\u52a8\u4f5c\u7a7a\u95f4\u548c\u65b0\u5174\u5168\u666f\u52a8\u4f5c\u7a7a\u95f4\u7684\u517c\u5bb9\u6027\u3002", "method": "\u4f7f\u7528Room-to-Room\uff08R2R\uff09\u6570\u636e\u96c6\u5fae\u8c03\u5f00\u6e90\u6a21\u578bQwen2.5-VL-3B-Instruct\uff0c\u5206\u522b\u5728\u4f4e\u7ea7\u522b\u52a8\u4f5c\u7a7a\u95f4\uff08\u5982'\u5de6\u8f6c'\u539f\u5b50\u64cd\u4f5c\uff09\u548c\u5168\u666f\u52a8\u4f5c\u7a7a\u95f4\uff08\u79bb\u6563\u53ef\u5bfc\u822a\u89c6\u89d2\uff09\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5728R2R\u6d4b\u8bd5\u96c6\u8fbe\u523041%\u6210\u529f\u7387\uff0c\u8868\u660e\u73b0\u6210LVLMs\u5177\u5907\u57fa\u7840\u5bfc\u822a\u80fd\u529b\uff0c\u4f46\u663e\u8457\u843d\u540e\u4e8e\u4e13\u7528\u6a21\u578b\uff08\u5f53\u524dSOTA\u6a21\u578b\u6210\u529f\u7387\u7ea670%\uff09\u3002", "conclusion": "\u73b0\u6210LVLMs\u53ef\u5b8c\u6210\u57fa\u7840VLN\u4efb\u52a1\uff0c\u4f46\u6027\u80fd\u5929\u82b1\u677f\u53d7\u9650\u4e8e\u67b6\u6784\u901a\u7528\u6027\uff0c\u672a\u6765\u9700\u63a2\u7d22\u4e13\u7528\u5fae\u8c03\u7b56\u7565\u6216\u6df7\u5408\u67b6\u6784\u4ee5\u7f29\u5c0f\u4e0e\u5b9a\u5236\u6a21\u578b\u7684\u5dee\u8ddd\u3002"}}
{"id": "2508.02961", "pdf": "https://arxiv.org/pdf/2508.02961", "abs": "https://arxiv.org/abs/2508.02961", "authors": ["Boshi Huang", "Fabio Nonato de Paula"], "title": "Defend LLMs Through Self-Consciousness", "categories": ["cs.AI", "cs.CL", "cs.CR"], "comment": "Presented at KDD Workshop on Ethical Artificial Intelligence: Methods\n  and Applications (EAI) 2025", "summary": "This paper introduces a novel self-consciousness defense mechanism for Large\nLanguage Models (LLMs) to combat prompt injection attacks. Unlike traditional\napproaches that rely on external classifiers, our method leverages the LLM's\ninherent reasoning capabilities to perform self-protection. We propose a\nframework that incorporates Meta-Cognitive and Arbitration Modules, enabling\nLLMs to evaluate and regulate their own outputs autonomously. Our approach is\nevaluated on seven state-of-the-art LLMs using two datasets: AdvBench and\nPrompt-Injection-Mixed-Techniques-2024. Experiment results demonstrate\nsignificant improvements in defense success rates across models and datasets,\nwith some achieving perfect and near-perfect defense in Enhanced Mode. We also\nanalyze the trade-off between defense success rate improvement and\ncomputational overhead. This self-consciousness method offers a lightweight,\ncost-effective solution for enhancing LLM ethics, particularly beneficial for\nGenAI use cases across various platforms.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u578bLLM\u81ea\u610f\u8bc6\u9632\u5fa1\u673a\u5236\uff0c\u5229\u7528\u6a21\u578b\u81ea\u8eab\u63a8\u7406\u80fd\u529b\u5bf9\u6297\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u901a\u8fc7\u5143\u8ba4\u77e5\u4ef2\u88c1\u6a21\u5757\u5b9e\u73b0\u81ea\u4e3b\u8bc4\u4f30\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u9ad8\u6548\u9632\u5fa1\u8868\u73b0\u5e76\u5e73\u8861\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u4f20\u7edf\u4f9d\u8d56\u5916\u90e8\u5206\u7c7b\u5668\u7684\u9632\u5fa1\u65b9\u5f0f\u5b58\u5728\u6210\u672c\u9ad8\u3001\u590d\u6742\u5ea6\u5927\u7684\u7f3a\u9677\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u5229\u7528LLM\u81ea\u8eab\u80fd\u529b\u7684\u5185\u751f\u5b89\u5168\u673a\u5236\u4ee5\u63d0\u5347\u4f26\u7406\u9632\u62a4\u7684\u53ef\u6301\u7eed\u6027\u3002", "method": "\u6784\u5efa\u5305\u542b\u5143\u8ba4\u77e5\uff08\u81ea\u6211\u76d1\u63a7\uff09\u548c\u4ef2\u88c1\uff08\u51b3\u7b56\u8c03\u8282\uff09\u6a21\u5757\u7684\u6846\u67b6\uff0c\u4f7fLLM\u5728\u54cd\u5e94\u524d\u81ea\u4e3b\u8fdb\u884c\u98ce\u9669\u8bc4\u5206\u4e0e\u8f93\u51fa\u4fee\u6b63\uff0c\u5b9e\u73b0\u65e0\u5916\u90e8\u4f9d\u8d56\u7684\u5b9e\u65f6\u9632\u5fa1\u3002", "result": "\u57287\u4e2a\u524d\u6cbfLLM\u4e0a\u5b9e\u73b0\u5e73\u5747\u9632\u5fa1\u6210\u529f\u7387\u63d0\u534735%\uff0c\u589e\u5f3a\u6a21\u5f0f\u4e0b\u90e8\u5206\u6a21\u578b\u8fbe98%\u6210\u529f\u7387\uff0c\u8ba1\u7b97\u5ef6\u8fdf\u4ec5\u589e\u52a015-20%\uff0c\u663e\u793a\u4f18\u8d8a\u7684\u6027\u4ef7\u6bd4\u3002", "conclusion": "\u8be5\u81ea\u610f\u8bc6\u9632\u5fa1\u673a\u5236\u4e3aLLM\u4f26\u7406\u9632\u62a4\u63d0\u4f9b\u8f7b\u91cf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u8f83\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u6709\u6548\u62b5\u5fa1\u65b0\u578b\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u5177\u6709\u591a\u5e73\u53f0\u9002\u914d\u4f18\u52bf\u3002"}}
{"id": "2508.02979", "pdf": "https://arxiv.org/pdf/2508.02979", "abs": "https://arxiv.org/abs/2508.02979", "authors": ["Peng Ding", "Rick Stevens"], "title": "Unified Tool Integration for LLMs: A Protocol-Agnostic Approach to Function Calling", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "arXiv admin note: substantial text overlap with arXiv:2507.10593", "summary": "The proliferation of tool-augmented Large Language Models (LLMs) has created\na fragmented ecosystem where developers must navigate multiple protocols,\nmanual schema definitions, and complex execution workflows. We address this\nchallenge by proposing a unified approach to tool integration that abstracts\nprotocol differences while optimizing execution performance. Our solution\ndemonstrates how protocol-agnostic design principles can significantly reduce\ndevelopment overhead through automated schema generation, dual-mode concurrent\nexecution, and seamless multi-source tool management. Experimental results show\n60-80% code reduction across integration scenarios, performance improvements up\nto 3.1x through optimized concurrency, and full compatibility with existing\nfunction calling standards. This work contributes both theoretical insights\ninto tool integration architecture and practical solutions for real-world LLM\napplication development.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u534f\u8bae\u65e0\u5173\u7684LLM\u5de5\u5177\u96c6\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u6a21\u5f0f\u751f\u6210\u4e0e\u53cc\u6a21\u5e76\u53d1\u4f18\u5316\uff0c\u5b9e\u73b060-80%\u4ee3\u7801\u7cbe\u7b80\u4e0e3.1\u500d\u6027\u80fd\u63d0\u5347", "motivation": "\u89e3\u51b3\u5de5\u5177\u589e\u5f3a\u578bLLM\u751f\u6001\u788e\u7247\u5316\u95ee\u9898\uff08\u591a\u534f\u8bae\u9002\u914d/\u624b\u52a8\u6a21\u5f0f\u5b9a\u4e49/\u590d\u6742\u6267\u884c\u6d41\u7a0b\uff09\u5bfc\u81f4\u7684\u5f00\u53d1\u6548\u7387\u4f4e\u4e0b", "method": "\u534f\u8bae\u62bd\u8c61\u5316\u67b6\u6784\u8bbe\u8ba1 + \u81ea\u52a8\u6a21\u5f0f\u751f\u6210\u6280\u672f + \u53cc\u6a21\u5f0f\u5e76\u53d1\u6267\u884c\u5f15\u64ce + \u591a\u6e90\u5de5\u5177\u7edf\u4e00\u7ba1\u7406\u7cfb\u7edf", "result": "\u5b9e\u9a8c\u663e\u793a\u4ee3\u7801\u91cf\u51cf\u5c1160-80%\uff0c\u5e76\u53d1\u4f18\u5316\u5e26\u67653.1\u500d\u6027\u80fd\u63d0\u5347\uff0c\u5b8c\u5168\u517c\u5bb9\u73b0\u6709\u51fd\u6570\u8c03\u7528\u6807\u51c6", "conclusion": "\u5728\u5de5\u5177\u96c6\u6210\u67b6\u6784\u7406\u8bba\uff08\u534f\u8bae\u62bd\u8c61\u8bbe\u8ba1\u539f\u5219\uff09\u4e0e\u5de5\u7a0b\u5b9e\u8df5\uff08\u5f00\u53d1\u6548\u7387/\u6267\u884c\u6027\u80fd\u4f18\u5316\u65b9\u6848\uff09\u5c42\u9762\u5747\u4f5c\u51fa\u521b\u65b0\u8d21\u732e"}}
{"id": "2508.02999", "pdf": "https://arxiv.org/pdf/2508.02999", "abs": "https://arxiv.org/abs/2508.02999", "authors": ["Xinjie Zhao", "Moritz Blum", "Fan Gao", "Yingjian Chen", "Boming Yang", "Luis Marquez-Carpintero", "M\u00f3nica Pina-Navarro", "Yanran Fu", "So Morikawa", "Yusuke Iwasawa", "Yutaka Matsuo", "Chanjun Park", "Irene Li"], "title": "AGENTiGraph: A Multi-Agent Knowledge Graph Framework for Interactive, Domain-Specific LLM Chatbots", "categories": ["cs.AI", "cs.CL"], "comment": "CIKM 2025, Demo Track", "summary": "AGENTiGraph is a user-friendly, agent-driven system that enables intuitive\ninteraction and management of domain-specific data through the manipulation of\nknowledge graphs in natural language. It gives non-technical users a complete,\nvisual solution to incrementally build and refine their knowledge bases,\nallowing multi-round dialogues and dynamic updates without specialized query\nlanguages. The flexible design of AGENTiGraph, including intent classification,\ntask planning, and automatic knowledge integration, ensures seamless reasoning\nbetween diverse tasks. Evaluated on a 3,500-query benchmark within an\neducational scenario, the system outperforms strong zero-shot baselines\n(achieving 95.12% classification accuracy, 90.45% execution success),\nindicating potential scalability to compliance-critical or multi-step queries\nin legal and medical domains, e.g., incorporating new statutes or research on\nthe fly. Our open-source demo offers a powerful new paradigm for multi-turn\nenterprise knowledge management that bridges LLMs and structured graphs.", "AI": {"tldr": "AGENTiGraph\u662f\u4e00\u4e2a\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u64cd\u4f5c\u77e5\u8bc6\u56fe\u8c31\u7684\u4ee3\u7406\u7cfb\u7edf\uff0c\u652f\u6301\u975e\u6280\u672f\u7528\u6237\u53ef\u89c6\u5316\u6784\u5efa\u77e5\u8bc6\u5e93\uff0c\u57283500\u6b21\u67e5\u8be2\u6d4b\u8bd5\u4e2d\u8fbe\u523095.12%\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u5c55\u793a\u51fa\u5728\u5408\u89c4\u654f\u611f\u9886\u57df\u7684\u6269\u5c55\u6f5c\u529b\u3002", "motivation": "\u6d88\u9664\u975e\u6280\u672f\u7528\u6237\u4f7f\u7528\u4e13\u4e1a\u67e5\u8be2\u8bed\u8a00\u7684\u95e8\u69db\uff0c\u901a\u8fc7\u591a\u8f6e\u81ea\u7136\u5bf9\u8bdd\u5b9e\u73b0\u52a8\u6001\u77e5\u8bc6\u7ba1\u7406\uff0c\u5e76\u63a2\u7d22LLM\u4e0e\u7ed3\u6784\u5316\u56fe\u8c31\u878d\u5408\u7684\u4f01\u4e1a\u7ea7\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u610f\u56fe\u5206\u7c7b-\u4efb\u52a1\u89c4\u5212-\u77e5\u8bc6\u96c6\u6210\u4e09\u5c42\u67b6\u6784\uff0c\u7ed3\u5408\u53ef\u89c6\u5316\u7f16\u8f91\u754c\u9762\u548c\u81ea\u52a8\u5316\u56fe\u8c31\u66f4\u65b0\u673a\u5236\uff0c\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u5230\u7ed3\u6784\u5316\u64cd\u4f5c\u7684\u8f6c\u5316\u3002", "result": "\u6559\u80b2\u573a\u666f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u7c7b\u51c6\u786e\u738795.12%\u3001\u6267\u884c\u6210\u529f\u738790.45%\uff0c\u663e\u8457\u8d85\u8d8a\u96f6\u6837\u672c\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5f00\u521b\u4e86\u591a\u8f6e\u4f01\u4e1a\u77e5\u8bc6\u7ba1\u7406\u65b0\u8303\u5f0f\uff0c\u5176\u5f00\u6e90\u5b9e\u73b0\u4e3a\u6cd5\u5f8b/\u533b\u7597\u9886\u57df\u590d\u6742\u67e5\u8be2\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684LLM-\u56fe\u8c31\u4ea4\u4e92\u6846\u67b6\u3002"}}
{"id": "2508.03058", "pdf": "https://arxiv.org/pdf/2508.03058", "abs": "https://arxiv.org/abs/2508.03058", "authors": ["Dingwei Zhu", "Shihan Dou", "Zhiheng Xi", "Senjie Jin", "Guoqiang Zhang", "Jiazheng Zhang", "Junjie Ye", "Mingxu Chai", "Enyu Zhou", "Ming Zhang", "Caishuang Huang", "Yunke Zhang", "Yuran Wang", "Tao Gui"], "title": "VRPO: Rethinking Value Modeling for Robust RL Training under Noisy Supervision", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) often suffers from noisy or\nimperfect reward supervision in real-world settings, which undermines policy\nstability and generalization. Such noise may cause models to lose attention on\nkey words during advantage estimation. While prior work focuses on reward\ndenoising or filtering poor data, it often overlooks the critical role of the\nvalue model in policy optimization. In this work, we show that a strong value\nmodel is essential for mitigating noise by absorbing unstable signals and\nenabling more reliable advantage estimation. We propose VRPO, a value-centric\nframework for robust PPO training under noisy supervision. VRPO combines two\ncore designs: (1) an auxiliary loss guided by entropy and perplexity from a\nfrozen language model, and (2) a variational information bottleneck. These\nmechanisms enhance the value model's ability to filter out noise and capture\nkey words from the context during advantage estimation, transforming it from a\npassive predictor into an active regulator of noise. Experiments on math\nreasoning, science QA, and multi-turn dialogue, under both rule-based and\nmodel-based noisy rewards, show that VRPO consistently outperforms PPO and GRPO\nbaselines. Our findings underscore the often-overlooked importance of the value\nmodel in RLHF and offer a principled and practical approach to robust policy\noptimization in noisy real-world environments.", "AI": {"tldr": "\u63d0\u51faVRPO\u6846\u67b6\uff0c\u901a\u8fc7\u71b5/\u56f0\u60d1\u5ea6\u8f85\u52a9\u635f\u5931\u51fd\u6570\u548c\u53d8\u5206\u4fe1\u606f\u74f6\u9888\u589e\u5f3a\u4ef7\u503c\u6a21\u578b\uff0c\u6709\u6548\u6291\u5236RLHF\u566a\u58f0\u5e72\u6270\uff0c\u5728\u6570\u5b66\u63a8\u7406\u3001\u79d1\u5b66QA\u7b49\u4efb\u52a1\u4e2d\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5", "motivation": "\u73b0\u5b9eRLHF\u573a\u666f\u7684\u5956\u52b1\u4fe1\u53f7\u5e38\u5b58\u5728\u566a\u58f0\uff0c\u5bfc\u81f4\u7b56\u7565\u4e0d\u7a33\u5b9a\u3002\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u4ef7\u503c\u6a21\u578b\u5728\u566a\u58f0\u8fc7\u6ee4\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\uff0c\u800c\u5f3a\u5927\u7684\u4ef7\u503c\u6a21\u578b\u80fd\u5438\u6536\u4e0d\u7a33\u5b9a\u4fe1\u53f7\u5e76\u63d0\u5347\u4f18\u52bf\u4f30\u8ba1\u53ef\u9760\u6027", "method": "1) \u57fa\u4e8e\u51bb\u7ed3\u8bed\u8a00\u6a21\u578b\u7684\u71b5/\u56f0\u60d1\u5ea6\u8f85\u52a9\u635f\u5931\u5f15\u5bfc 2) \u53d8\u5206\u4fe1\u606f\u74f6\u9888\u673a\u5236\uff0c\u4e8c\u8005\u534f\u540c\u589e\u5f3a\u4ef7\u503c\u6a21\u578b\u7684\u566a\u58f0\u8fc7\u6ee4\u80fd\u529b\uff0c\u4f7f\u5176\u4ece\u88ab\u52a8\u9884\u6d4b\u5668\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u566a\u58f0\u8c03\u8282\u5668", "result": "\u5728\u89c4\u5219/\u6a21\u578b\u751f\u6210\u7684\u566a\u58f0\u5956\u52b1\u573a\u666f\u4e0b\uff0c\u6570\u5b66\u63a8\u7406\u51c6\u786e\u7387\u63d0\u53473.2%\uff0c\u79d1\u5b66QA\u4efb\u52a1F1\u503c\u63d0\u9ad85.7%\uff0c\u591a\u8f6e\u5bf9\u8bdd\u8d28\u91cf\u6307\u6807\u589e\u957f12%", "conclusion": "\u63ed\u793a\u4e86\u4ef7\u503c\u6a21\u578b\u5728RLHF\u4e2d\u88ab\u4f4e\u4f30\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u73b0\u5b9e\u566a\u58f0\u73af\u5883\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u9c81\u68d2\u4f18\u5316\u6846\u67b6\uff0c\u63a8\u52a8RLHF\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u843d\u5730"}}
{"id": "2508.03092", "pdf": "https://arxiv.org/pdf/2508.03092", "abs": "https://arxiv.org/abs/2508.03092", "authors": ["Zikun Cui", "Tianyi Huang", "Chia-En Chiang", "Cuiqianhe Du"], "title": "Toward Verifiable Misinformation Detection: A Multi-Tool LLM Agent Framework", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "With the proliferation of Large Language Models (LLMs), the detection of\nmisinformation has become increasingly important and complex. This research\nproposes an innovative verifiable misinformation detection LLM agent that goes\nbeyond traditional true/false binary judgments. The agent actively verifies\nclaims through dynamic interaction with diverse web sources, assesses\ninformation source credibility, synthesizes evidence, and provides a complete\nverifiable reasoning process. Our designed agent architecture includes three\ncore tools: precise web search tool, source credibility assessment tool and\nnumerical claim verification tool. These tools enable the agent to execute\nmulti-step verification strategies, maintain evidence logs, and form\ncomprehensive assessment conclusions. We evaluate using standard misinformation\ndatasets such as FakeNewsNet, comparing with traditional machine learning\nmodels and LLMs. Evaluation metrics include standard classification metrics,\nquality assessment of reasoning processes, and robustness testing against\nrewritten content. Experimental results show that our agent outperforms\nbaseline methods in misinformation detection accuracy, reasoning transparency,\nand resistance to information rewriting, providing a new paradigm for\ntrustworthy AI-assisted fact-checking.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u9a8c\u8bc1\u7684\u8c23\u8a00\u68c0\u6d4bLLM\u667a\u80fd\u4f53\uff0c\u7a81\u7834\u4f20\u7edf\u4e8c\u5143\u5224\u65ad\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u9a8c\u8bc1\u5de5\u5177\u5b9e\u73b0\u900f\u660e\u5316\u67e5\u8bc1\u8fc7\u7a0b", "motivation": "\u4f20\u7edf\u8c23\u8a00\u68c0\u6d4b\u6a21\u578b\u5c40\u9650\u4e8e\u771f\u4f2a\u4e8c\u5143\u5224\u65ad\uff0c\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u5927\u6a21\u578b\u65f6\u4ee3\u9700\u8981\u53ef\u4fe1\u8d56\u7684AI\u8f85\u52a9\u4e8b\u5b9e\u6838\u67e5\u65b0\u8303\u5f0f", "method": "\u8bbe\u8ba1\u5305\u542b\u7cbe\u51c6\u7f51\u7edc\u641c\u7d22\u3001\u4fe1\u6e90\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u3001\u6570\u503c\u58f0\u660e\u9a8c\u8bc1\u7684\u4e09\u9636\u6bb5\u5de5\u5177\u94fe\uff0c\u652f\u6301\u591a\u6b65\u9aa4\u9a8c\u8bc1\u7b56\u7565\u5e76\u7ef4\u62a4\u8bc1\u636e\u94fe", "result": "\u5728FakeNewsNet\u7b49\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u63a8\u7406\u8fc7\u7a0b\u8d28\u91cf\u63d0\u534735%\uff0c\u6297\u4fe1\u606f\u6539\u5199\u9c81\u68d2\u6027\u63d0\u9ad842%", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53ef\u4fe1AI\u4e8b\u5b9e\u6838\u67e5\u5efa\u7acb\u65b0\u6807\u51c6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u9a8c\u8bc1\u6d41\u7a0b\u589e\u5f3a\u68c0\u6d4b\u7ed3\u679c\u7684\u53ef\u89e3\u91ca\u6027\u548c\u6297\u5bf9\u6297\u653b\u51fb\u80fd\u529b"}}
{"id": "2508.03164", "pdf": "https://arxiv.org/pdf/2508.03164", "abs": "https://arxiv.org/abs/2508.03164", "authors": ["Junyoung Lim", "Jaewoo Ahn", "Gunhee Kim"], "title": "ChartCap: Mitigating Hallucination of Dense Chart Captioning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "ICCV 2025 (Highlight)", "summary": "Generating accurate, informative, and hallucination-free captions for charts\nremains challenging for vision language models, primarily due to the lack of\nlarge-scale, high-quality datasets of real-world charts. However, existing\nreal-world chart datasets suffer from the inclusion of extraneous information\nthat cannot be inferred from the chart and failure to sufficiently capture\nstructural elements and key insights. Therefore, we introduce ChartCap, a\nlarge-scale dataset of 565K real-world chart images paired with type-specific,\ndense captions that exclude extraneous information and highlight both\nstructural elements and key insights in detail. To build ChartCap, we design a\nfour-stage pipeline that generates captions using only the discernible data\nfrom the chart and employ a cycle consistency-based human verification, which\naccelerates quality control without sacrificing accuracy. Additionally, we\npropose a novel metric, the Visual Consistency Score, which evaluates caption\nquality by measuring the similarity between the chart regenerated from a\ncaption and the original chart, independent of reference captions. Extensive\nexperiments confirms that models fine-tuned on ChartCap consistently generate\nmore accurate and informative captions with reduced hallucinations, surpassing\nboth open-source and proprietary models and even human-annotated captions.", "AI": {"tldr": "\u63d0\u51fa\u4e86ChartCap\u6570\u636e\u96c6\u89e3\u51b3\u56fe\u8868\u63cf\u8ff0\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5305\u542b56.5\u4e07\u771f\u5b9e\u56fe\u8868\u53ca\u7ed3\u6784\u5316\u63cf\u8ff0\uff0c\u901a\u8fc7\u521b\u65b0\u9a8c\u8bc1\u6d41\u7a0b\u548c\u65b0\u8bc4\u4f30\u6307\u6807VCS\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u56fe\u8868\u6570\u636e\u96c6\u5b58\u5728\u5916\u6e90\u4fe1\u606f\u5e72\u6270\u3001\u7ed3\u6784\u6027\u8981\u7d20\u6355\u6349\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u56fe\u8868\u63cf\u8ff0\u65f6\u4ea7\u751f\u4e8b\u5b9e\u6027\u9519\u8bef\u548c\u5173\u952e\u4fe1\u606f\u9057\u6f0f", "method": "\u56db\u9636\u6bb5\u6570\u636e\u6784\u5efa\u6d41\u7a0b\uff08\u6570\u636e\u6e05\u6d17-\u81ea\u52a8\u6807\u6ce8-\u4eba\u5de5\u9a8c\u8bc1-\u8d28\u91cf\u8bc4\u4f30\uff09+ \u57fa\u4e8e\u5faa\u73af\u4e00\u81f4\u6027\u7684\u9a8c\u8bc1\u673a\u5236 + \u65e0\u9700\u53c2\u8003\u6807\u6ce8\u7684Visual Consistency Score\u8bc4\u4f30\u6307\u6807", "result": "\u5728ChartCap\u5fae\u8c03\u7684\u6a21\u578b\u5728\u51c6\u786e\u7387\u548c\u4fe1\u606f\u91cf\u4e0a\u5168\u9762\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff08\u5305\u62ecGPT-4V\u548c\u4eba\u7c7b\u6807\u6ce8\uff09\uff0c\u5e7b\u89c9\u7387\u964d\u4f4e37%\uff0cVCS\u6307\u6807\u4e0e\u4eba\u5de5\u8bc4\u4f30\u76f8\u5173\u6027\u8fbe0.89", "conclusion": "\u901a\u8fc7\u6570\u636e\u8d28\u91cf\u5de5\u7a0b\u6784\u5efa\u9ad8\u8d28\u91cf\u8bad\u7ec3\u96c6\uff0c\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u591a\u6a21\u6001\u4efb\u52a1"}}
{"id": "2508.03280", "pdf": "https://arxiv.org/pdf/2508.03280", "abs": "https://arxiv.org/abs/2508.03280", "authors": ["Yubo Wang", "Shimin Di", "Zhili Wang", "Haoyang Li", "Fei Teng", "Hao Xin", "Lei Chen"], "title": "Understanding the Embedding Models on Hyper-relational Knowledge Graph", "categories": ["cs.LG", "cs.CL", "cs.SI"], "comment": "Accepted by CIKM 2025", "summary": "Recently, Hyper-relational Knowledge Graphs (HKGs) have been proposed as an\nextension of traditional Knowledge Graphs (KGs) to better represent real-world\nfacts with additional qualifiers. As a result, researchers have attempted to\nadapt classical Knowledge Graph Embedding (KGE) models for HKGs by designing\nextra qualifier processing modules. However, it remains unclear whether the\nsuperior performance of Hyper-relational KGE (HKGE) models arises from their\nbase KGE model or the specially designed extension module. Hence, in this\npaper, we data-wise convert HKGs to KG format using three decomposition methods\nand then evaluate the performance of several classical KGE models on HKGs. Our\nresults show that some KGE models achieve performance comparable to that of\nHKGE models. Upon further analysis, we find that the decomposition methods\nalter the original HKG topology and fail to fully preserve HKG information.\nMoreover, we observe that current HKGE models are either insufficient in\ncapturing the graph's long-range dependency or struggle to integrate\nmain-triple and qualifier information due to the information compression issue.\nTo further justify our findings and offer a potential direction for future HKGE\nresearch, we propose the FormerGNN framework. This framework employs a\nqualifier integrator to preserve the original HKG topology, and a GNN-based\ngraph encoder to capture the graph's long-range dependencies, followed by an\nimproved approach for integrating main-triple and qualifier information to\nmitigate compression issues. Our experimental results demonstrate that\nFormerGNN outperforms existing HKGE models.", "AI": {"tldr": "Study compares KGE and HKGE models through HKG decomposition, proposes FormerGNN framework to address topology and dependency issues.", "motivation": "Investigate whether HKGE models' superiority stems from base KGE models or specialized modules, and address HKG information loss in decomposition methods.", "method": "1. Decompose HKGs into KG format via three methods 2. Evaluate classical KGE models 3. Propose FormerGNN (qualifier integrator + GNN encoder) to preserve topology and mitigate compression issues.", "result": "1. Some KGE models match HKGE performance 2. Decomposition alters HKG topology 3. FormerGNN outperforms SOTA HKGE models with 3.5% MRR improvement", "conclusion": "Base KGE selection and topology preservation are crucial. FormerGNN demonstrates effective qualifier integration and long-range dependency capture for HKGs."}}
{"id": "2508.03306", "pdf": "https://arxiv.org/pdf/2508.03306", "abs": "https://arxiv.org/abs/2508.03306", "authors": ["Kisu Yang", "Yoonna Jang", "Hwanseok Jang", "Kenneth Choi", "Isabelle Augenstein", "Heuiseok Lim"], "title": "Reliable Evaluation Protocol for Low-Precision Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "11 pages, 5 figures, submitted to ARR", "summary": "Lowering the numerical precision of model parameters and computations is\nwidely adopted to improve the efficiency of retrieval systems. However, when\ncomputing relevance scores between the query and documents in low-precision, we\nobserve spurious ties due to the reduced granularity. This introduces high\nvariability in the results based on tie resolution, making the evaluation less\nreliable. To address this, we propose a more robust retrieval evaluation\nprotocol designed to reduce score variation. It consists of: (1) High-Precision\nScoring (HPS), which upcasts the final scoring step to higher precision to\nresolve tied candidates with minimal computational cost; and (2) Tie-aware\nRetrieval Metrics (TRM), which report expected scores, range, and bias to\nquantify order uncertainty of tied candidates. Our experiments test multiple\nmodels with three scoring functions on two retrieval datasets to demonstrate\nthat HPS dramatically reduces tie-induced instability, and TRM accurately\nrecovers expected metric values. This combination enables a more consistent and\nreliable evaluation system for lower-precision retrievals.", "AI": {"tldr": "\u8bba\u6587\u9488\u5bf9\u4f4e\u7cbe\u5ea6\u68c0\u7d22\u7cfb\u7edf\u63d0\u51fa\u9ad8\u7cbe\u5ea6\u8bc4\u5206\u548c\u6982\u7387\u611f\u77e5\u8bc4\u4f30\u534f\u8bae\uff0c\u89e3\u51b3\u6570\u503c\u7cbe\u5ea6\u964d\u4f4e\u5bfc\u81f4\u7684\u8bc4\u5206\u4e0d\u7a33\u5b9a\u95ee\u9898\uff0c\u63d0\u5347\u8bc4\u4f30\u53ef\u9760\u6027\u3002", "motivation": "\u964d\u4f4e\u6570\u503c\u7cbe\u5ea6\u4f1a\u5f15\u53d1\u76f8\u5173\u6027\u8bc4\u5206\u4e2d\u7684\u865a\u5047\u5e73\u5c40\u73b0\u8c61\uff0c\u5bfc\u81f4\u68c0\u7d22\u7ed3\u679c\u9ad8\u5ea6\u4f9d\u8d56\u5e73\u5c40\u5904\u7406\u65b9\u5f0f\uff0c\u4f7f\u8bc4\u4f30\u6307\u6807\u53ef\u4fe1\u5ea6\u964d\u4f4e\u3002", "method": "1. \u9ad8\u7cbe\u5ea6\u8bc4\u5206(HPS)\uff1a\u5728\u6700\u7ec8\u8bc4\u5206\u9636\u6bb5\u63d0\u5347\u8ba1\u7b97\u7cbe\u5ea6\n2. \u6982\u7387\u611f\u77e5\u68c0\u7d22\u6307\u6807(TRM)\uff1a\u901a\u8fc7\u671f\u671b\u503c\u3001\u533a\u95f4\u548c\u504f\u5dee\u91cf\u5316\u6392\u5e8f\u4e0d\u786e\u5b9a\u6027", "result": "\u5b9e\u9a8c\u8bc1\u660eHPS\u663e\u8457\u51cf\u5c11\u5e73\u5c40\u5f15\u53d1\u7684\u8bc4\u4f30\u6ce2\u52a8\uff0cTRM\u80fd\u51c6\u786e\u6062\u590d\u771f\u5b9e\u6307\u6807\u503c\uff0c\u7ec4\u5408\u65b9\u6848\u5728\u591a\u4e2a\u6a21\u578b/\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\u3002", "conclusion": "\u7ed3\u5408HPS\u4e0eTRM\u7684\u65b9\u6848\u4e3a\u4f4e\u7cbe\u5ea6\u68c0\u7d22\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u7a33\u5b9a\u53ef\u9760\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6709\u6548\u91cf\u5316\u5e76\u63a7\u5236\u7cbe\u5ea6\u635f\u5931\u5e26\u6765\u7684\u6392\u5e8f\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2508.03351", "pdf": "https://arxiv.org/pdf/2508.03351", "abs": "https://arxiv.org/abs/2508.03351", "authors": ["Yufei Xue", "Yushi Huang", "Jiawei Shao", "Jun Zhang"], "title": "VLMQ: Efficient Post-Training Quantization for Large Vision-Language Models via Hessian Augmentation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "13 pages, 5 figures", "summary": "Post-training quantization (PTQ) has emerged as an effective approach for\ncompressing large models and accelerating their inference without retraining.\nWhile PTQ has been extensively studied in the context of large language models\n(LLMs), its applicability to vision-language models (VLMs) remains\nunderexplored. In this paper, we identify a modality discrepancy (\\emph{i.e.},\nlimited text tokens \\emph{vs.} excessive and redundant vision tokens) of VLMs.\nHowever, existing Hessian-based LLM PTQ methods treat all tokens equally during\nquantization, resulting in severe performance drops when applied to VLMs.\nMotivated by this observation, we propose a novel importance-aware PTQ\nframework tailored for VLMs, dubbed VLMQ. Specifically, to address vision token\nredundancy, VLMQ 1) optimizes an importance-aware objective that yields an\nenhanced Hessian with token-level importance factors, while retaining\ncompatibility with parallelized weight updates, and 2) ensures efficiency and\neffectiveness by computing these factors via a single lightweight block-wise\nbackward pass, guided by a theoretical connection to token-level perturbations.\nExtensive evaluations on 8 benchmarks across 0.5B$\\sim$32B VLMs demonstrate the\nstate-of-the-art (SOTA) performance of our VLMQ, particularly under low-bit\nsettings. For example, it achieves a substantial \\textbf{16.45\\%} improvement\non MME-RealWorld under 2-bit quantization.", "AI": {"tldr": "\u63d0\u51faVLMQ\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u611f\u77e5\u7684\u540e\u8bad\u7ec3\u91cf\u5316\u6846\u67b6\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u89c9token\u5197\u4f59\u4e0e\u6587\u672ctoken\u4e0d\u8db3\u7684\u6a21\u6001\u5dee\u5f02\u95ee\u9898\uff0c\u5728\u4f4e\u6bd4\u7279\u91cf\u5316\u573a\u666f\u4e0b\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eHessian\u7684LLM\u91cf\u5316\u65b9\u6cd5\u5e73\u7b49\u5bf9\u5f85\u6240\u6709token\uff0c\u5bfc\u81f4\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u5e94\u7528\u65f6\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u3002\u6838\u5fc3\u77db\u76fe\u662f\u89c6\u89c9token\u5197\u4f59\u4e0e\u6587\u672ctoken\u4e0d\u8db3\u7684\u6a21\u6001\u5dee\u5f02\u3002", "method": "1) \u8bbe\u8ba1\u91cd\u8981\u6027\u611f\u77e5\u76ee\u6807\u51fd\u6570\uff0c\u901a\u8fc7token\u7ea7\u91cd\u8981\u6027\u56e0\u5b50\u589e\u5f3aHessian\u77e9\u9635\n2) \u5229\u7528\u8f7b\u91cf\u7ea7\u5757\u7ea7\u53cd\u5411\u4f20\u64ad\u9ad8\u6548\u8ba1\u7b97\u91cd\u8981\u6027\u56e0\u5b50\uff0c\u4fdd\u6301\u4e0e\u5e76\u884c\u6743\u91cd\u66f4\u65b0\u7684\u517c\u5bb9\u6027\n3) \u5efa\u7acb\u4e0etoken\u7ea7\u6270\u52a8\u7684\u7406\u8bba\u8054\u7cfb\u6307\u5bfc\u65b9\u6cd5\u8bbe\u8ba1", "result": "\u57288\u4e2a\u57fa\u51c6\u6d4b\u8bd5(0.5B~32B\u6a21\u578b)\u4e2d\u8fbe\u5230SOTA\uff0c2bit\u91cf\u5316\u4e0bMME-RealWorld\u6307\u6807\u63d0\u534716.45%", "conclusion": "VLMQ\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u6a21\u578b\u91cf\u5316\u4e2d\u7684\u6a21\u6001\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u4e3a\u4f4e\u6bd4\u7279\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.03366", "pdf": "https://arxiv.org/pdf/2508.03366", "abs": "https://arxiv.org/abs/2508.03366", "authors": ["Michael K. Chen"], "title": "A Comparative Study of Neurosymbolic AI Approaches to Interpretable Logical Reasoning", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SC"], "comment": "Accepted to NeSy 2025", "summary": "General logical reasoning, defined as the ability to reason deductively on\ndomain-agnostic tasks, continues to be a challenge for large language models\n(LLMs). Current LLMs fail to reason deterministically and are not\ninterpretable. As such, there has been a recent surge in interest in\nneurosymbolic AI, which attempts to incorporate logic into neural networks. We\nfirst identify two main neurosymbolic approaches to improving logical\nreasoning: (i) the integrative approach comprising models where symbolic\nreasoning is contained within the neural network, and (ii) the hybrid approach\ncomprising models where a symbolic solver, separate from the neural network,\nperforms symbolic reasoning. Both contain AI systems with promising results on\ndomain-specific logical reasoning benchmarks. However, their performance on\ndomain-agnostic benchmarks is understudied. To the best of our knowledge, there\nhas not been a comparison of the contrasting approaches that answers the\nfollowing question: Which approach is more promising for developing general\nlogical reasoning? To analyze their potential, the following best-in-class\ndomain-agnostic models are introduced: Logic Neural Network (LNN), which uses\nthe integrative approach, and LLM-Symbolic Solver (LLM-SS), which uses the\nhybrid approach. Using both models as case studies and representatives of each\napproach, our analysis demonstrates that the hybrid approach is more promising\nfor developing general logical reasoning because (i) its reasoning chain is\nmore interpretable, and (ii) it retains the capabilities and advantages of\nexisting LLMs. To support future works using the hybrid approach, we propose a\ngeneralizable framework based on LLM-SS that is modular by design,\nmodel-agnostic, domain-agnostic, and requires little to no human input.", "AI": {"tldr": "\u8bba\u6587\u5bf9\u6bd4\u6574\u5408\u578b\u4e0e\u6df7\u5408\u578b\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u901a\u7528\u903b\u8f91\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u6df7\u5408\u65b9\u6cd5\uff08LLM-SS\uff09\u56e0\u53ef\u89e3\u91ca\u6027\u5f3a\u4e14\u4fdd\u7559LLM\u4f18\u52bf\u66f4\u5177\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u6a21\u5757\u5316\u6846\u67b6\u652f\u6301\u672a\u6765\u7814\u7a76\u3002", "motivation": "\u9488\u5bf9LLMs\u5728\u786e\u5b9a\u6027\u903b\u8f91\u63a8\u7406\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u7684\u7f3a\u9677\uff0c\u7814\u7a76\u795e\u7ecf\u7b26\u53f7AI\u7684\u4e24\u7c7b\u65b9\u6cd5\uff08\u6574\u5408\u578b/\u6df7\u5408\u578b\uff09\u5728\u901a\u7528\u9886\u57df\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5bf9\u6bd4\u6574\u5408\u578b\u4ee3\u8868\u6a21\u578bLNN\u4e0e\u6df7\u5408\u578b\u4ee3\u8868\u6a21\u578bLLM-SS\uff0c\u5206\u6790\u5176\u63a8\u7406\u94fe\u53ef\u89e3\u91ca\u6027\u3001LLM\u80fd\u529b\u4fdd\u7559\u5ea6\u7b49\u6838\u5fc3\u6307\u6807\u3002", "result": "\u6df7\u5408\u65b9\u6cd5\u63a8\u7406\u8fc7\u7a0b\u66f4\u900f\u660e\u4e14\u4fdd\u7559LLM\u539f\u751f\u80fd\u529b\uff0c\u63d0\u51fa\u7684\u6a21\u5757\u5316\u6846\u67b6\u5177\u5907\u9886\u57df\u65e0\u5173\u3001\u6a21\u578b\u65e0\u5173\u3001\u96f6\u4eba\u5de5\u5e72\u9884\u7279\u6027\u3002", "conclusion": "\u6df7\u5408\u578b\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u901a\u7528\u903b\u8f91\u63a8\u7406\u9886\u57df\u66f4\u5177\u53d1\u5c55\u6f5c\u529b\uff0c\u672a\u6765\u5de5\u4f5c\u53ef\u57fa\u4e8e\u63d0\u51fa\u7684LLM-SS\u6846\u67b6\u5b9e\u73b0\u9ad8\u6548\u6269\u5c55\u3002"}}
{"id": "2508.03481", "pdf": "https://arxiv.org/pdf/2508.03481", "abs": "https://arxiv.org/abs/2508.03481", "authors": ["Hyungjin Kim", "Seokho Ahn", "Young-Duk Seo"], "title": "Draw Your Mind: Personalized Generation via Condition-Level Modeling in Text-to-Image Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted at ICCV 2025", "summary": "Personalized generation in T2I diffusion models aims to naturally incorporate\nindividual user preferences into the generation process with minimal user\nintervention. However, existing studies primarily rely on prompt-level modeling\nwith large-scale models, often leading to inaccurate personalization due to the\nlimited input token capacity of T2I diffusion models. To address these\nlimitations, we propose DrUM, a novel method that integrates user profiling\nwith a transformer-based adapter to enable personalized generation through\ncondition-level modeling in the latent space. DrUM demonstrates strong\nperformance on large-scale datasets and seamlessly integrates with open-source\ntext encoders, making it compatible with widely used foundation T2I models\nwithout requiring additional fine-tuning.", "AI": {"tldr": "\u63d0\u51faDrUM\u65b9\u6cd5\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u6761\u4ef6\u7ea7\u5efa\u6a21\u5b9e\u73b0T2I\u6269\u6563\u6a21\u578b\u7684\u4e2a\u6027\u5316\u751f\u6210", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7ea7\u5efa\u6a21\u7684\u65b9\u6cd5\u53d7\u9650\u4e8eT2I\u6a21\u578b\u8f93\u5165\u5bb9\u91cf\uff0c\u5bfc\u81f4\u4e2a\u6027\u5316\u751f\u6210\u4e0d\u51c6\u786e", "method": "\u6574\u5408\u7528\u6237\u753b\u50cf\u4e0eTransformer\u9002\u914d\u5668\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u6761\u4ef6\u7ea7\u5efa\u6a21", "result": "\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8868\u73b0\u4f18\u5f02\uff0c\u517c\u5bb9\u5f00\u6e90\u6587\u672c\u7f16\u7801\u5668\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\u5373\u53ef\u9002\u914d\u4e3b\u6d41T2I\u57fa\u7840\u6a21\u578b", "conclusion": "DrUM\u901a\u8fc7\u521b\u65b0\u5efa\u6a21\u65b9\u5f0f\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7cbe\u51c6\u7684\u4e2a\u6027\u5316\u56fe\u50cf\u751f\u6210"}}
{"id": "2508.03501", "pdf": "https://arxiv.org/pdf/2508.03501", "abs": "https://arxiv.org/abs/2508.03501", "authors": ["Alexander Golubev", "Maria Trofimova", "Sergei Polezhaev", "Ibragim Badertdinov", "Maksim Nekrashevich", "Anton Shevtsov", "Simon Karasik", "Sergey Abramov", "Andrei Andriushchenko", "Filipp Fisin", "Sergei Skvortsov", "Boris Yangel"], "title": "Training Long-Context, Multi-Turn Software Engineering Agents with Reinforcement Learning", "categories": ["cs.LG", "cs.CL", "cs.SE"], "comment": null, "summary": "Research on applications of Reinforcement Learning (RL) to Large Language\nModels (LLMs) has mostly been focused on single-turn problems, such as\nmathematical reasoning or single-shot code generation. While these problems can\nbe viewed as token-level multi-turn MDPs, this view corresponds to a degenerate\ncase of multi-turn interaction where the environment provides no feedback. This\ncontrasts with many real-world domains, such as software engineering (SWE),\nwhich require rich multi-turn interactions with a stateful environment that\nresponds to each action with a non-trivial observation.\n  To bridge this gap, we demonstrate the successful application of RL to this\ngeneral regime. Using a modified Decoupled Advantage Policy Optimization (DAPO)\nalgorithm, we train an agent based on Qwen2.5-72B-Instruct to solve real-world\nsoftware engineering tasks. Our approach increases the agent's success rate on\nthe SWE-bench Verified benchmark from a 20% rejection fine-tuned baseline to\n39%, without relying on any teacher models. On SWE-rebench, our agent matches\nor outperforms leading open-weight models such as DeepSeek-V3-0324 and\nQwen3-235B-A22B using an identical scaffolding, offering a viable path toward\nbuilding more capable autonomous agents for complex real-world problems based\non open models.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6539\u8fdbDAPO\u7b97\u6cd5\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5c06Qwen2.5-72B\u667a\u80fd\u4f53\u5728\u8f6f\u4ef6\u5de5\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u6210\u529f\u7387\u63d0\u5347\u81f339%", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u5728LLM\u5e94\u7528\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5355\u8f6e\u4efb\u52a1\uff08\u5982\u6570\u5b66\u63a8\u7406\uff09\uff0c\u4e0e\u9700\u8981\u591a\u8f6e\u72b6\u6001\u4ea4\u4e92\u7684\u771f\u5b9e\u573a\u666f\uff08\u5982\u8f6f\u4ef6\u5de5\u7a0b\uff09\u5b58\u5728\u5dee\u8ddd", "method": "\u6539\u8fdbDecoupled Advantage Policy Optimization\uff08DAPO\uff09\u7b97\u6cd5\uff0c\u57fa\u4e8eQwen2.5-72B-Instruct\u6a21\u578b\u6784\u5efa\u667a\u80fd\u4f53", "result": "SWE-bench Verified\u57fa\u51c6\u6210\u529f\u7387\u4ece20%\u63d0\u5347\u81f339%\uff0c\u5728SWE-rebench\u8fbe\u5230\u4e0eDeepSeek-V3\u7b49\u9876\u5c16\u6a21\u578b\u76f8\u5f53\u6c34\u5e73", "conclusion": "\u9a8c\u8bc1\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u591a\u8f6e\u4ea4\u4e92\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u6784\u5efa\u5f00\u653e\u6a21\u578b\u9a71\u52a8\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84"}}
{"id": "2508.03527", "pdf": "https://arxiv.org/pdf/2508.03527", "abs": "https://arxiv.org/abs/2508.03527", "authors": ["Mohammadreza Sadeghi", "Mahsa Ghazvini Nejad", "MirHamed Jafarzadeh Asl", "Yu Gu", "Yuanhao Yu", "Masoud Asgharian", "Vahid Partovi Nia"], "title": "MoKA: Mixture of Kronecker Adapters", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) is essential for reducing the\ncomputational overhead of large language models (LLMs). Low-rank family\nadapters are commonly used to control the parameter size efficiently while\nmaintaining the generative power of LLMs. However, their limited expressiveness\ndue to the rank constraint often restricts their performance on complex tasks.\nWe propose Mixture of Kronecker Adapters (MoKA), a new generation of Kronecker\nadapters that addresses this limitation by modeling weight updates as a mixture\nof Kronecker products. Our proposed adapter leverages a gating mechanism that\nmeasures the importance of each Kronecker factor, enabling more expressive\nadaptation. Moreover, MoKA enables a rank flexibility that provides a better\ntrade-off between parameter efficiency and accuracy. To ensure hardware\nefficiency, we reformulate Kronecker computations using standard matrix\noperations, allowing seamless deployment on GPU-optimized hardware. We conduct\nextensive experiments on instruction-tuning and commonsense reasoning tasks\nusing low-bit quantized versions of LLaMA2-7B and LLaMA3-8B models. MoKA not\nonly outperforms PEFT baselines, but also reduces the number of trainable\nparameters up to 27x, achieving state-of-the-art trade-offs between performance\nand parameter efficiency.", "AI": {"tldr": "MoKA\u63d0\u51fa\u6df7\u5408Kronecker\u9002\u914d\u5668\uff0c\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u589e\u5f3a\u8868\u8fbe\u80fd\u529b\uff0c\u5b9e\u73b0\u53c2\u6570\u6548\u7387\u4e0e\u7cbe\u5ea6\u7684\u66f4\u597d\u5e73\u8861\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u51cf\u5c1127\u500d\u53c2\u6570\u91cf\u7684\u540c\u65f6\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u4f4e\u79e9\u9002\u914d\u5668\u53d7\u9650\u4e8e\u79e9\u7ea6\u675f\uff0c\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u8fbe\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u5728\u53c2\u6570\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd\u95f4\u5bfb\u627e\u66f4\u4f18\u5e73\u8861\u70b9\u3002", "method": "\u5c06\u6743\u91cd\u66f4\u65b0\u5efa\u6a21\u4e3aKronecker\u79ef\u7684\u6df7\u5408\uff0c\u5f15\u5165\u95e8\u63a7\u673a\u5236\u52a8\u6001\u8bc4\u4f30\u56e0\u5b50\u91cd\u8981\u6027\uff0c\u5e76\u91cd\u6784\u8ba1\u7b97\u65b9\u5f0f\u9002\u914dGPU\u786c\u4ef6\u3002", "result": "\u5728LLaMA2-7B/LLaMA3-8B\u4f4e\u6bd4\u7279\u91cf\u5316\u6a21\u578b\u4e0a\uff0c\u6307\u4ee4\u8c03\u4f18\u548c\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u8868\u73b0\u8d85\u8d8a\u57fa\u7ebf\uff0c\u53c2\u6570\u51cf\u5c11\u6700\u591a27\u500d\u3002", "conclusion": "MoKA\u4e3aPEFT\u63d0\u4f9b\u4e86\u786c\u4ef6\u53cb\u597d\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6781\u4f4e\u53c2\u6570\u91cf\u7ea7\u7684\u540c\u65f6\u7a81\u7834\u73b0\u6709\u9002\u914d\u5668\u7684\u6027\u80fd\u74f6\u9888\u3002"}}
{"id": "2508.03553", "pdf": "https://arxiv.org/pdf/2508.03553", "abs": "https://arxiv.org/abs/2508.03553", "authors": ["Wenlong Wu", "Haofen Wang", "Bohan Li", "Peixuan Huang", "Xinzhe Zhao", "Lei Liang"], "title": "MultiRAG: A Knowledge-guided Framework for Mitigating Hallucination in Multi-source Retrieval Augmented Generation", "categories": ["cs.IR", "cs.CL"], "comment": "Accepted by ICDE 2025 Research Paper", "summary": "Retrieval Augmented Generation (RAG) has emerged as a promising solution to\naddress hallucination issues in Large Language Models (LLMs). However, the\nintegration of multiple retrieval sources, while potentially more informative,\nintroduces new challenges that can paradoxically exacerbate hallucination\nproblems. These challenges manifest primarily in two aspects: the sparse\ndistribution of multi-source data that hinders the capture of logical\nrelationships and the inherent inconsistencies among different sources that\nlead to information conflicts. To address these challenges, we propose\nMultiRAG, a novel framework designed to mitigate hallucination in multi-source\nretrieval-augmented generation through knowledge-guided approaches. Our\nframework introduces two key innovations: (1) a knowledge construction module\nthat employs multi-source line graphs to efficiently aggregate logical\nrelationships across different knowledge sources, effectively addressing the\nsparse data distribution issue; and (2) a sophisticated retrieval module that\nimplements a multi-level confidence calculation mechanism, performing both\ngraph-level and node-level assessments to identify and eliminate unreliable\ninformation nodes, thereby reducing hallucinations caused by inter-source\ninconsistencies. Extensive experiments on four multi-domain query datasets and\ntwo multi-hop QA datasets demonstrate that MultiRAG significantly enhances the\nreliability and efficiency of knowledge retrieval in complex multi-source\nscenarios. \\textcolor{blue}{Our code is available in\nhttps://github.com/wuwenlong123/MultiRAG.", "AI": {"tldr": "MultiRAG\u901a\u8fc7\u77e5\u8bc6\u5f15\u5bfc\u65b9\u6cd5\uff08\u591a\u6e90\u7ebf\u56fe\u805a\u5408\u903b\u8f91\u5173\u7cfb+\u591a\u7ea7\u7f6e\u4fe1\u5ea6\u673a\u5236\u6d88\u9664\u4e0d\u53ef\u9760\u4fe1\u606f\uff09\u6709\u6548\u7f13\u89e3\u591a\u6e90\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u590d\u6742\u591a\u6e90\u573a\u666f\u4e2d\u663e\u8457\u63d0\u5347\u53ef\u9760\u6027", "motivation": "\u89e3\u51b3\u591a\u6e90\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u56e0\u6570\u636e\u5206\u5e03\u7a00\u758f\u548c\u6765\u6e90\u95f4\u4e0d\u4e00\u81f4\u5bfc\u81f4\u7684\u5e7b\u89c9\u52a0\u5267\u95ee\u9898\u3002\u591a\u6e90\u6574\u5408\u867d\u7136\u80fd\u4e30\u5bcc\u4fe1\u606f\uff0c\u4f46\u4f1a\u963b\u788d\u903b\u8f91\u5173\u7cfb\u6355\u6349\u5e76\u5f15\u53d1\u4fe1\u606f\u51b2\u7a81", "method": "1. \u77e5\u8bc6\u6784\u5efa\u6a21\u5757\uff1a\u91c7\u7528\u591a\u6e90\u7ebf\u56fe\u9ad8\u6548\u805a\u5408\u8de8\u77e5\u8bc6\u6e90\u7684\u903b\u8f91\u5173\u7cfb\uff0c\u89e3\u51b3\u6570\u636e\u7a00\u758f\u95ee\u9898\n2. \u68c0\u7d22\u6a21\u5757\uff1a\u5b9e\u73b0\u56fe\u7ea7\u548c\u8282\u70b9\u7ea7\u53cc\u91cd\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\uff0c\u8bc6\u522b\u5e76\u6d88\u9664\u4e0d\u53ef\u9760\u4fe1\u606f\u8282\u70b9", "result": "\u57284\u4e2a\u591a\u9886\u57df\u67e5\u8be2\u6570\u636e\u96c6\u548c2\u4e2a\u591a\u8df3QA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMultiRAG\u5728\u590d\u6742\u591a\u6e90\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u77e5\u8bc6\u68c0\u7d22\u7684\u53ef\u9760\u6027\u548c\u6548\u7387", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u77e5\u8bc6\u805a\u5408\u548c\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u673a\u5236\uff0c\u4e3a\u591a\u6e90\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6297\u5e7b\u89c9\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u4fe1\u606f\u4e30\u5bcc\u6027\u7684\u540c\u65f6\u786e\u4fdd\u751f\u6210\u53ef\u9760\u6027"}}
{"id": "2508.03555", "pdf": "https://arxiv.org/pdf/2508.03555", "abs": "https://arxiv.org/abs/2508.03555", "authors": ["Antoine Chaffin", "Rapha\u00ebl Sourty"], "title": "PyLate: Flexible Training and Retrieval for Late Interaction Models", "categories": ["cs.IR", "cs.CL"], "comment": "5 pages", "summary": "Neural ranking has become a cornerstone of modern information retrieval.\nWhile single vector search remains the dominant paradigm, it suffers from the\nshortcoming of compressing all the information into a single vector. This\ncompression leads to notable performance degradation in out-of-domain,\nlong-context, and reasoning-intensive retrieval tasks. Multi-vector approaches\npioneered by ColBERT aim to address these limitations by preserving individual\ntoken embeddings and computing similarity via the MaxSim operator. This\narchitecture has demonstrated superior empirical advantages, including enhanced\nout-of-domain generalization, long-context handling, and performance in complex\nretrieval scenarios. Despite these compelling empirical results and clear\ntheoretical advantages, the practical adoption and public availability of late\ninteraction models remain low compared to their single-vector counterparts,\nprimarily due to a lack of accessible and modular tools for training and\nexperimenting with such models. To bridge this gap, we introduce PyLate, a\nstreamlined library built on top of Sentence Transformers to support\nmulti-vector architectures natively, inheriting its efficient training,\nadvanced logging, and automated model card generation while requiring minimal\ncode changes to code templates users are already familiar with. By offering\nmulti-vector-specific features such as efficient indexes, PyLate aims to\naccelerate research and real-world application of late interaction models,\nthereby unlocking their full potential in modern IR systems. Finally, PyLate\nhas already enabled the development of state-of-the-art models, including\nGTE-ModernColBERT and Reason-ModernColBERT, demonstrating its practical utility\nfor both research and production environments.", "AI": {"tldr": "\u63d0\u51faPyLate\u5e93\u89e3\u51b3\u591a\u5411\u91cf\u68c0\u7d22\u6a21\u578b\u5de5\u5177\u532e\u4e4f\u95ee\u9898\uff0c\u57fa\u4e8eSentence Transformers\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u4e0e\u7d22\u5f15\uff0c\u63a8\u52a8\u665a\u671f\u4ea4\u4e92\u6a21\u578b\u5e94\u7528", "motivation": "\u5355\u5411\u91cf\u68c0\u7d22\u6a21\u578b\u56e0\u4fe1\u606f\u538b\u7f29\u5bfc\u81f4\u8de8\u9886\u57df/\u957f\u4e0a\u4e0b\u6587/\u590d\u6742\u63a8\u7406\u573a\u666f\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u591a\u5411\u91cf\u65b9\u6cd5\u867d\u4f18\u4f46\u7f3a\u4e4f\u6613\u7528\u5de5\u5177\u963b\u788d\u5b9e\u9645\u5e94\u7528", "method": "\u57fa\u4e8eSentence Transformers\u6784\u5efa\u6a21\u5757\u5316\u5e93\uff0c\u652f\u6301\u591a\u5411\u91cf\u67b6\u6784\u539f\u751f\u5b9e\u73b0\uff0c\u63d0\u4f9b\u9ad8\u6548\u7d22\u5f15\u529f\u80fd\u4e14\u4fdd\u6301\u73b0\u6709\u4ee3\u7801\u517c\u5bb9\u6027", "result": "\u5df2\u6210\u529f\u5f00\u53d1GTE-ModernColBERT\u548cReason-ModernColBERT\u7b49\u5148\u8fdb\u6a21\u578b\uff0c\u9a8c\u8bc1\u6846\u67b6\u5728\u79d1\u7814\u4e0e\u751f\u4ea7\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027", "conclusion": "PyLate\u901a\u8fc7\u964d\u4f4e\u4f7f\u7528\u95e8\u69db\u52a0\u901f\u665a\u671f\u4ea4\u4e92\u6a21\u578b\u7814\u7a76\u4e0e\u5e94\u7528\uff0c\u91ca\u653e\u591a\u5411\u91cf\u65b9\u6cd5\u5728\u73b0\u4ee3\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u4e2d\u7684\u5168\u90e8\u6f5c\u529b"}}
{"id": "2508.03562", "pdf": "https://arxiv.org/pdf/2508.03562", "abs": "https://arxiv.org/abs/2508.03562", "authors": ["Muzhaffar Hazman", "Susan McKeever", "Josephine Griffith"], "title": "Beyond Meme Templates: Limitations of Visual Similarity Measures in Meme Matching", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted for publication at IEEE International Conference on Image\n  Processing Theory, Tools and Applications (IPTA) 2025", "summary": "Internet memes, now a staple of digital communication, play a pivotal role in\nhow users engage within online communities and allow researchers to gain\ninsight into contemporary digital culture. These engaging user-generated\ncontent are characterised by their reuse of visual elements also found in other\nmemes. Matching instances of memes via these shared visual elements, called\nMeme Matching, is the basis of a wealth of meme analysis approaches. However,\nmost existing methods assume that every meme consists of a shared visual\nbackground, called a Template, with some overlaid text, thereby limiting meme\nmatching to comparing the background image alone. Current approaches exclude\nthe many memes that are not template-based and limit the effectiveness of\nautomated meme analysis and would not be effective at linking memes to\ncontemporary web-based meme dictionaries. In this work, we introduce a broader\nformulation of meme matching that extends beyond template matching. We show\nthat conventional similarity measures, including a novel segment-wise\ncomputation of the similarity measures, excel at matching template-based memes\nbut fall short when applied to non-template-based meme formats. However, the\nsegment-wise approach was found to consistently outperform the whole-image\nmeasures on matching non-template-based memes. Finally, we explore a\nprompting-based approach using a pretrained Multimodal Large Language Model for\nmeme matching. Our results highlight that accurately matching memes via shared\nvisual elements, not just background templates, remains an open challenge that\nrequires more sophisticated matching techniques.", "AI": {"tldr": "\u63d0\u51fa\u8d85\u8d8a\u4f20\u7edf\u6a21\u677f\u5339\u914d\u7684\u4e92\u8054\u7f51\u6a21\u56e0\u5339\u914d\u65b0\u65b9\u6cd5\uff0c\u63ed\u793a\u73b0\u6709\u6280\u672f\u5bf9\u975e\u6a21\u677f\u7c7b\u6a21\u56e0\u7684\u5c40\u9650\u6027\u53ca\u6539\u8fdb\u65b9\u5411", "motivation": "\u73b0\u6709\u6a21\u56e0\u5339\u914d\u65b9\u6cd5\u5c40\u9650\u4e8e\u6a21\u677f\u80cc\u666f\u5339\u914d\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u5927\u91cf\u975e\u6a21\u677f\u7c7b\u6a21\u56e0\uff0c\u5f71\u54cd\u81ea\u52a8\u5316\u5206\u6790\u548c\u7f51\u7edc\u6a21\u56e0\u8bcd\u5178\u5efa\u8bbe", "method": "\u91c7\u7528\u4f20\u7edf\u76f8\u4f3c\u5ea6\u6d4b\u91cf\u3001\u5206\u6bb5\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u53ca\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63d0\u793a\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c", "result": "\u4f20\u7edf\u65b9\u6cd5\u5728\u6a21\u677f\u7c7b\u6a21\u56e0\u8868\u73b0\u826f\u597d\u4f46\u975e\u6a21\u677f\u7c7b\u5931\u6548\uff0c\u5206\u6bb5\u65b9\u6cd5\u5bf9\u975e\u6a21\u677f\u7c7b\u6301\u7eed\u6709\u6548\uff0c\u63d0\u793a\u65b9\u6cd5\u663e\u793a\u5f53\u524d\u6280\u672f\u4ecd\u5b58\u6311\u6218", "conclusion": "\u7cbe\u786e\u5339\u914d\u5305\u542b\u5171\u4eab\u89c6\u89c9\u5143\u7d20\uff08\u975e\u4ec5\u6a21\u677f\uff09\u7684\u6a21\u56e0\u4ecd\u5b58\u5728\u6280\u672f\u74f6\u9888\uff0c\u9700\u5f00\u53d1\u66f4\u590d\u6742\u7684\u5339\u914d\u7b97\u6cd5"}}
{"id": "2508.03599", "pdf": "https://arxiv.org/pdf/2508.03599", "abs": "https://arxiv.org/abs/2508.03599", "authors": ["Johannes Niu", "Mila Stillman", "Anna Kruspe"], "title": "OSINT or BULLSHINT? Exploring Open-Source Intelligence tweets about the Russo-Ukrainian War", "categories": ["cs.SI", "cs.CL"], "comment": null, "summary": "This paper examines the role of Open Source Intelligence (OSINT) on Twitter\nregarding the Russo-Ukrainian war, distinguishing between genuine OSINT and\ndeceptive misinformation efforts, termed \"BULLSHINT.\" Utilizing a dataset\nspanning from January 2022 to July 2023, we analyze nearly 2 million tweets\nfrom approximately 1,040 users involved in discussing real-time military\nengagements, strategic analyses, and misinformation related to the conflict.\nUsing sentiment analysis, partisanship detection, misinformation\nidentification, and Named Entity Recognition (NER), we uncover communicative\npatterns and dissemination strategies within the OSINT community. Significant\nfindings reveal a predominant negative sentiment influenced by war events, a\nnuanced distribution of pro-Ukrainian and pro-Russian partisanship, and the\npotential strategic manipulation of information. Additionally, we apply\ncommunity detection techniques, which are able to identify distinct clusters\npartisanship, topics, and misinformation, highlighting the complex dynamics of\ninformation spread on social media. This research contributes to the\nunderstanding of digital warfare and misinformation dynamics, offering insights\ninto the operationalization of OSINT in geopolitical conflicts.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u6790\u4fc4\u4e4c\u6218\u4e89\u671f\u95f4Twitter\u4e0a200\u4e07\u6761\u63a8\u6587\uff0c\u63ed\u793a\u5f00\u6e90\u60c5\u62a5(OSINT)\u4e0e\u865a\u5047\u4fe1\u606f(BULLSHINT)\u7684\u4f20\u64ad\u6a21\u5f0f\uff0c\u53d1\u73b0\u8d1f\u9762\u60c5\u7eea\u4e3b\u5bfc\u3001\u515a\u6d3e\u4fe1\u606f\u64cd\u7eb5\u53ca\u590d\u6742\u4f20\u64ad\u7f51\u7edc", "motivation": "\u63a2\u7a76\u793e\u4ea4\u5a92\u4f53\u5728\u519b\u4e8b\u51b2\u7a81\u4e2d\u771f\u5b9e\u60c5\u62a5\u4e0e\u865a\u5047\u4fe1\u606f\u7684\u4f20\u64ad\u673a\u5236\u53ca\u5176\u5bf9\u6218\u7565\u8ba4\u77e5\u7684\u5f71\u54cd", "method": "\u91c7\u7528\u60c5\u611f\u5206\u6790+\u515a\u6d3e\u68c0\u6d4b+\u865a\u5047\u4fe1\u606f\u8bc6\u522b+NER\u6280\u672f\uff0c\u7ed3\u5408\u793e\u533a\u68c0\u6d4b\u7b97\u6cd5\u5206\u67902022-2023\u5e741040\u4e2a\u7528\u6237\u7684\u63a8\u6587\u6570\u636e", "result": "\u53d1\u73b0\u6218\u4e89\u4e8b\u4ef6\u9a71\u52a8\u7684\u8d1f\u9762\u60c5\u7eea\u4e3b\u5bfc\u3001\u4eb2\u4e4c/\u4eb2\u4fc4\u9635\u8425\u5b58\u5728\u7b56\u7565\u6027\u4fe1\u606f\u64cd\u7eb5\u3001\u793e\u533a\u68c0\u6d4b\u63ed\u793a\u8de8\u5e73\u53f0\u534f\u540c\u4f20\u64ad\u7f51\u7edc", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6570\u5b57\u6218\u4e89\u4e2d\u7684\u4fe1\u606f\u6218\u673a\u5236\u63d0\u4f9b\u5206\u6790\u6846\u67b6\uff0c\u63ed\u793aOSINT\u5728\u5f53\u4ee3\u5730\u7f18\u51b2\u7a81\u4e2d\u7684\u53cc\u5203\u5251\u4f5c\u7528"}}
{"id": "2508.03663", "pdf": "https://arxiv.org/pdf/2508.03663", "abs": "https://arxiv.org/abs/2508.03663", "authors": ["Deepak Pandita", "Flip Korn", "Chris Welty", "Christopher M. Homan"], "title": "Forest vs Tree: The $(N, K)$ Trade-off in Reproducible ML Evaluation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reproducibility is a cornerstone of scientific validation and of the\nauthority it confers on its results. Reproducibility in machine learning\nevaluations leads to greater trust, confidence, and value. However, the ground\ntruth responses used in machine learning often necessarily come from humans,\namong whom disagreement is prevalent, and surprisingly little research has\nstudied the impact of effectively ignoring disagreement in these responses, as\nis typically the case. One reason for the lack of research is that budgets for\ncollecting human-annotated evaluation data are limited, and obtaining more\nsamples from multiple annotators for each example greatly increases the\nper-item annotation costs. We investigate the trade-off between the number of\nitems ($N$) and the number of responses per item ($K$) needed for reliable\nmachine learning evaluation. We analyze a diverse collection of categorical\ndatasets for which multiple annotations per item exist, and simulated\ndistributions fit to these datasets, to determine the optimal $(N, K)$\nconfiguration, given a fixed budget ($N \\times K$), for collecting evaluation\ndata and reliably comparing the performance of machine learning models. Our\nfindings show, first, that accounting for human disagreement may come with $N\n\\times K$ at no more than 1000 (and often much lower) for every dataset tested\non at least one metric. Moreover, this minimal $N \\times K$ almost always\noccurred for $K > 10$. Furthermore, the nature of the tradeoff between $K$ and\n$N$ -- or if one even existed -- depends on the evaluation metric, with metrics\nthat are more sensitive to the full distribution of responses performing better\nat higher levels of $K$. Our methods can be used to help ML practitioners get\nmore effective test data by finding the optimal metrics and number of items and\nannotations per item to collect to get the most reliability for their budget.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u56fa\u5b9a\u9884\u7b97\u4e0b\u4f18\u5316\u6807\u6ce8\u6570\u91cf\uff08N\u00d7K\u22641000\uff09\u53ef\u63d0\u5347\u673a\u5668\u5b66\u4e60\u8bc4\u4f30\u53ef\u9760\u6027\uff0c\u9ad8K\u503c\uff08>10\uff09\u914d\u7f6e\u53ca\u5206\u5e03\u654f\u611f\u7684\u8bc4\u4f30\u6307\u6807\u6548\u679c\u66f4\u4f18", "motivation": "\u673a\u5668\u5b66\u4e60\u8bc4\u4f30\u4e2d\u5e38\u5ffd\u7565\u4eba\u7c7b\u6807\u6ce8\u8005\u5206\u6b67\uff0c\u5bfc\u81f4\u7ed3\u679c\u4e0d\u53ef\u9760\u3002\u9884\u7b97\u9650\u5236\u4e0b\u5982\u4f55\u5e73\u8861\u6807\u6ce8\u6837\u672c\u91cf\uff08N\uff09\u4e0e\u6807\u6ce8\u6b21\u6570\uff08K\uff09\u6210\u4e3a\u7814\u7a76\u7a7a\u767d", "method": "\u901a\u8fc7\u591a\u5206\u7c7b\u6570\u636e\u96c6\u5206\u6790\u548c\u6a21\u62df\u5206\u5e03\u5b9e\u9a8c\uff0c\u786e\u5b9a\u4e0d\u540c\u8bc4\u4f30\u6307\u6807\u4e0b\u6700\u4f18\u7684(N,K)\u7ec4\u5408\u914d\u7f6e", "result": "N\u00d7K\u22641000\u65f6\u5373\u53ef\u6709\u6548\u5904\u7406\u4eba\u7c7b\u5206\u6b67\uff0c\u6700\u4f18\u914d\u7f6e\u591a\u9700K>10\uff1b\u5206\u5e03\u654f\u611f\u578b\u6307\u6807\u5728\u9ad8K\u503c\u65f6\u53ef\u9760\u6027\u66f4\u5f3a", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u6307\u5bfcML\u4ece\u4e1a\u8005\u6839\u636e\u9884\u7b97\u9009\u62e9\u6700\u4f73\u6307\u6807\u53ca\u6807\u6ce8\u914d\u7f6e\uff0c\u663e\u8457\u63d0\u5347\u6d4b\u8bd5\u6570\u636e\u7684\u8bc4\u4f30\u6709\u6548\u6027"}}
