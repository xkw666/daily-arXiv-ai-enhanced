<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 26]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.LG](#cs.LG) [Total: 5]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.CV](#cs.CV) [Total: 7]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
*Tian Lan,Xiangdong Su,Xu Liu,Ruirui Wang,Ke Chang,Jiang Li,Guanglai Gao*

Main category: cs.CL

TL;DR: 开发了多任务中文偏见评估基准McBE，覆盖12类偏见与5个评估任务，用于全面评估大语言模型中的文化偏见问题


<details>
  <summary>Details</summary>
Motivation: 现有偏见数据集集中于英语和北美文化，缺乏中文语境支持且评估维度单一，无法全面检测LLMs的文化偏见

Method: 构建包含4,077个实例的基准，涵盖12个主偏见类别/82个子类，引入文本补全、问答等5种评估任务范式

Result: 不同系列和参数规模的LLMs均表现出不同程度的偏见，尤其在文化敏感领域存在系统性偏差

Conclusion: McBE首次实现多维度的中文LLMs偏见检测，揭示了模型偏见与文化背景的强相关性，为模型伦理研究提供新视角

Abstract: As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.

</details>


### [2] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
*Keyan Jin,Yapeng Wang,Leonel Santos,Tao Fang,Xu Yang,Sio Kei Im,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: 研究发现，在对话摘要任务中，逐步推理的大语言模型（如OpenAI-o1）相比非推理模型更容易产生冗长和事实错误，而非推理模型反而表现更优。


<details>
  <summary>Details</summary>
Motivation: 评估推理与非推理大语言模型在三种对话摘要范式（通用型、角色导向型和查询导向型）中的性能差异，探究显式推理机制在复杂对话场景中的适用性。

Method: 跨4个主流基准数据集（SAMSum/DialogSum/CSDS/QMSum），结合LLM自动评估和人工评价标准，系统比较不同模型在多种语言、领域和摘要长度下的表现。

Result: 推理模型在58%的案例中产生冗余内容，事实一致性得分比非推理模型低13.2%，摘要简洁度下降19%。

Conclusion: 当前推理模型存在对话场景适配局限，需开发针对性建模方法和评估体系，显式推理机制需与对话特性深度结合才能发挥优势。

Abstract: Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.

</details>


### [3] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

TL;DR: 研究探讨Huginn-3.5B模型在隐式潜在CoT推理中的表现，发现其可解释性证据有限且循环深度提升微弱


<details>
  <summary>Details</summary>
Motivation: 探索循环架构模型内部是否形成可解释的潜在推理链条，以平衡推理效率与可解释性

Method: 使用Logit Lens和Coda Lens等探测技术分析循环块中的隐层状态，追踪算术任务中的结果令牌轨迹

Result: 发现潜在CoT证据不足，隐层状态可解释性存在跨块不一致性，循环深度仅带来边际性能提升

Conclusion: 隐式潜在推理在现有架构中效果有限，显式外部化推理步骤仍具显著优势

Abstract: Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [4] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
*Steven Song,Anirudh Subramanyam,Zhenyu Zhang,Aarti Venkat,Robert L. Grossman*

Main category: cs.CL

TL;DR: 开发基于自然语言的GDC队列生成工具，开源模型性能超越GPT-4o


<details>
  <summary>Details</summary>
Motivation: 解决用户在GDC平台手动构建复杂癌症队列的困难，通过自然语言交互降低使用门槛

Method: 采用本地部署的开源大语言模型(GDC Cohort LLM)，实现自然语言到GDC查询语句的自动转换

Result: 自研模型在生成队列准确性上优于GPT-4o，提供可视化界面支持结果调优

Conclusion: 该工具显著提升癌症研究效率，开源方案保障了医疗数据隐私和可扩展性

Abstract: Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [5] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

TL;DR: 提出MemAgent代理工作流程，通过分段处理和覆盖策略实现高效长文本处理，在32K训练下成功扩展到3.5M任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法在无限长文档处理中存在性能衰减问题，需要端到端的优化方案。

Method: 1. 分段读取文本的内存覆盖更新策略
2. 扩展DAPO算法实现多对话生成训练
3. 代理工作流程架构设计

Result: 32K训练数据下实现：
- 3.5M QA任务性能损失<5%
- 512K RULER测试准确率95%+
- 8K上下文扩展至32K训练

Conclusion: MemAgent通过创新的工作流程和训练算法，突破现有长文本处理瓶颈，实现超长上下文的高效保持。

Abstract: Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [6] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

TL;DR: DoMIX提出基于LoRA模块的高效并行领域自适应预训练方法，解决现有持续DAP方法的计算成本高、领域顺序敏感和模型通用性问题


<details>
  <summary>Details</summary>
Motivation: 现有持续DAP方法存在高计算成本、对数据顺序敏感、提供单一通用模型等问题，与DAP本质相悖

Method: 采用参数高效微调（PEFT）中的LoRA模块，实现并行领域自适应预训练，支持领域顺序无关性并积累领域知识

Result: 方法可扩展至标准LLM微调场景，在保持效率的同时提供面向特定任务的定制预训练模型（代码已开源）

Conclusion: DoMIX通过LoRA技术突破持续DAP的限制，实现计算效率、领域鲁棒性和模型定制能力的全面提升

Abstract: Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [7] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
*Christian Jaumann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CL

TL;DR: 提出集成多模态大模型和少样本策略的视觉问答系统，在SciVQA 2025评测中获第三名


<details>
  <summary>Details</summary>
Motivation: 针对科学领域视觉问答场景，通过模型集成和少样本策略提升系统性能

Method: 采用双多模态大模型集成架构，根据图表类型和问题选择模型，结合置信度筛选答案

Result: 在7个参赛系统中位列第三（平均F1 85.12），代码已开源

Conclusion: 集成策略有效提升科学视觉问答性能，少样本方法适配数据稀缺场景

Abstract: This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


### [8] [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)
*Pilsung Kang*

Main category: cs.CL

TL;DR: 量子混合架构QFFN-BERT通过用参数化量子电路替代传统FFN模块，在保持精度的同时减少99%参数量，并展示出优秀的数据效率。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer中FFN模块占据2/3参数量，用参数化量子电路(PQC)替代以实现参数高效压缩。

Method: 采用残差连接+R_Y/R_Z旋转+交替纠缠策略的PQC架构，在经典模拟器上测试SST-2和DBpedia基准。

Result: 完整数据场景下达到基线102%准确率，少样本学习持续领先，FFN参数减少超99%。

Conclusion: 量子电路与深度学习原理协同设计时，可成为传统FFN的高效替代方案，但需优化架构保障训练稳定性。

Abstract: Parameterized quantum circuits (PQCs) have recently emerged as promising
components for enhancing the expressibility of neural architectures. In this
work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the
feedforward network (FFN) modules of a compact BERT variant are replaced by
PQC-based layers. This design is motivated by the dominant parameter
contribution of FFNs, which account for approximately two-thirds of the
parameters within standard Transformer encoder blocks. While prior studies have
primarily integrated PQCs into self-attention modules, our work focuses on the
FFN and systematically investigates the trade-offs between PQC depth,
expressibility, and trainability. Our final PQC architecture incorporates a
residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating
entanglement strategy to ensure stable training and high expressibility. Our
experiments, conducted on a classical simulator, on the SST-2 and DBpedia
benchmarks demonstrate two key findings. First, a carefully configured
QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its
classical counterpart in a full-data setting while reducing FFN-specific
parameters by over 99%. Second, our model exhibits a consistent and competitive
edge in few-shot learning scenarios, confirming its potential for superior data
efficiency. These results, supported by an ablation study on a non-optimized
PQC that failed to learn, confirm that PQCs can serve as powerful and
parameter-efficient alternatives to classical FFNs when co-designed with
foundational deep learning principles.

</details>


### [9] [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)
*Weijie Lyu,Sheng-Jun Huang,Xuan Xia*

Main category: cs.CL

TL;DR: 提出基于参数化模型的代码数据选择方法，通过10K高质量样本实现2.4%性能提升，显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 现有LLM训练过度依赖数据量而忽视质量，导致训练效率低下。需通过数据选择优化提升效率与性能

Method: 构建参数模型筛选代码数据，确保子集的分布一致性与多样性，实现高质量数据选择

Result: 仅用10K样本在HumanEval/MBPP分别提升2.4%/2.3%，性能与效率均优于传统全量采样方法

Conclusion: 该方法验证了高质量数据选择的有效性，在提升模型性能的同时显著降低计算资源消耗

Abstract: Recent advancements in large language models (LLMs) have significantly
improved code generation and program comprehension, accelerating the evolution
of software engineering. Current methods primarily enhance model performance by
leveraging vast amounts of data, focusing on data quantity while often
overlooking data quality, thereby reducing training efficiency. To address
this, we introduce an approach that utilizes a parametric model for code data
selection, aimed at improving both training efficiency and model performance.
Our method optimizes the parametric model to ensure distribution consistency
and diversity within the selected subset, guaranteeing high-quality data.
Experimental results demonstrate that using only 10K samples, our method
achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled
baseline, outperforming other sampling approaches in both performance and
efficiency. This underscores that our method effectively boosts model
performance while significantly reducing computational costs.

</details>


### [10] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
*Mark Atta Mensah,Isaac Wiafe,Akon Ekpezu,Justice Kwame Appati,Jamal-Deen Abdulai,Akosua Nyarkoa Wiafe-Akenten,Frank Ernest Yeboah,Gifty Odame*

Main category: cs.CL

TL;DR: 现有ASR模型在跨领域泛化能力上存在明显不足，不同架构（Whisper与Wav2Vec2）在阿坎语语音识别中展现出错误模式的显著差异


<details>
  <summary>Details</summary>
Motivation: 当前自动语音识别研究主要关注同领域评估，缺乏对跨领域泛化能力的系统性研究，特别是对阿坎语等低资源语言的适用性评估

Method: 使用四个不同领域的阿坎语语料库（文化相关图像描述、非正式对话、圣经诵读、金融对话），对比评估七个基于Transformer架构的ASR模型（包括Whisper和Wav2Vec2）的跨领域表现

Result: 1. 模型表现存在显著领域依赖性（同领域WER/CER最佳）
2. Whisper微调模型产生更流畅但可能误导的转录错误
3. Wav2Vec2在未知输入时产生更明显但难解释的错误

Conclusion: 低资源语言ASR需平衡错误可读性与透明性，建议采用领域适应技术、自适应路由策略及多语言训练框架

Abstract: Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.

</details>


### [11] [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)
*Sumaya Ahmed Salihs,Isaac Wiafe,Jamal-Deen Abdulai,Elikem Doe Atsakpo,Gifty Ayoka,Richard Cave,Akon Obu Ekpezu,Catherine Holloway,Katrin Tomanek,Fiifi Baffoe Payin Winful*

Main category: cs.CL

TL;DR: 开发社区驱动的ASR模型构建方法及工具集，针对加纳阿坎语构建首个语言障碍语音开源数据集，并展示模型优化效果


<details>
  <summary>Details</summary>
Motivation: 解决资源匮乏语言（如阿坎语）中语言障碍人群的语音识别技术缺失问题，推动包容性ASR技术的民主化进程

Method: 1. 制定社区数据采集规范手册
2. 创建阿坎语语言障碍语音开源数据集
3. 基于开源工具进行ASR模型微调

Result: 成功构建含多背景语言障碍者样本的数据集（公开可用），微调使开源ASR模型对阿坎语障碍语音的识别性能显著提升

Conclusion: 通过标准化工具包赋能社区自主开发定制化ASR技术，为语言障碍群体提供精准的技术解决方案

Abstract: This study presents an approach for collecting speech samples to build
Automatic Speech Recognition (ASR) models for impaired speech, particularly,
low-resource languages. It aims to democratize ASR technology and data
collection by developing a "cookbook" of best practices and training for
community-driven data collection and ASR model building. As a proof-of-concept,
this study curated the first open-source dataset of impaired speech in Akan: a
widely spoken indigenous language in Ghana. The study involved participants
from diverse backgrounds with speech impairments. The resulting dataset, along
with the cookbook and open-source tools, are publicly available to enable
researchers and practitioners to create inclusive ASR technologies tailored to
the unique needs of speech impaired individuals. In addition, this study
presents the initial results of fine-tuning open-source ASR models to better
recognize impaired speech in Akan.

</details>


### [12] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
*Sneha Deshmukh,Prathmesh Kamble*

Main category: cs.CL

TL;DR: 印度首个公开保释判决数据集IndianBailJudgments-1200，包含1200个案例，标注20+法律属性，使用GPT-4o生成并验证，支持结果预测、摘要生成等法律NLP任务。


<details>
  <summary>Details</summary>
Motivation: 解决印度法律NLP领域结构化数据稀缺问题，推动保释法理学的定量研究和法律AI应用发展。

Method: 通过提示工程优化的GPT-4o流水线生成标注，并经过人工一致性验证，构建结构化法律数据集。

Result: 创建首个公开的印度保释判决数据集，支持保释结果预测、法律文书摘要、司法公平性分析等多维度NLP任务。

Conclusion: 该数据集填补印度保释法领域数据空白，为提升法律系统透明度和推动计算法学研究提供关键资源。

Abstract: Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.

</details>


### [13] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Liwen Zhang,Litu Ou,Jialong Wu,Wenbiao Yin,Baixuan Li,Zhengwei Tao,Xinyu Wang,Weizhou Shen,Junkai Zhang,Dingchu Zhang,Xixi Wu,Yong Jiang,Ming Yan,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: WebSailor通过结构化采样、信息遮蔽、RFT冷启动和DUPO算法，显著提升开源LLM在复杂信息搜索任务中的表现，接近专有系统水平


<details>
  <summary>Details</summary>
Motivation: 现有开源模型缺乏系统性处理极端不确定性信息场景的能力，无法匹配专有智能体在复杂信息搜索基准（如BrowseComp）中的超人类表现

Method: 1. 通过结构化采样和信息遮蔽生成新型高不确定性任务
2. RFT冷启动技术
3. 创新性代理强化学习算法DUPO（双采样策略优化）
4. 构建完整后训练流程整合上述技术

Result: 在复杂信息搜索任务中显著超越所有开源智能体，性能匹配专有系统，缩小能力差距

Conclusion: 通过系统性解决极端不确定性场景的导航能力，验证了整合训练管道的有效性，为开源社区提供了接近专有系统性能的可行路径

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all opensource agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


### [14] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 该论文探讨如何通过分解标签变异性为信号/噪声、整合人类标签变异性（HLV）到主动学习框架，并引入大语言模型作为标注者，改进监督学习中的数据标注效率。


<details>
  <summary>Details</summary>
Motivation: 传统监督学习常忽视人类标注差异（HLV）的信息价值，且主动学习的核心假设在HLV存在时失效。需要新框架来优化标注资源分配，同时保留标签多样性。

Method: 1) 理论分解标签变异性 2) 系统梳理AL与HLV研究现状 3) 提出包含实例选择、标注者优化、标签表达优化的HLV-aware主动学习框架 4) 整合LLM作为新型标注源

Result: 建立HLV与主动学习的理论关联，提出首个系统性整合HLV的AL框架，揭示LLM标注与传统人类标注的互补潜力。

Conclusion: 需重构主动学习范式以适应真实标注场景，未来应探索HLV-aware模型训练方法及LLM标注质量评估体系。

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


### [15] [MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion](https://arxiv.org/abs/2507.02595)
*Xin Guan,PeiHsin Lin,Zekun Wu,Ze Wang,Ruibo Zhang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CL

TL;DR: 提出MPF框架，通过多视角生成融合实现大语言模型对齐，无需微调即可有效缓解偏见


<details>
  <summary>Details</summary>
Motivation: 应对大语言模型部署中存在的偏见缓解需求，提供无需复杂提示工程或微调的轻量级对齐方案

Method: 基于SAGED流水线构建偏见基准，通过分解专业视角（如HR情感分布）实现响应采样与概率加权平衡

Result: 成功对齐反事实基线（绝对平等）和HR基线（名校偏好），显著降低KL散度与校准误差，具备未见过问题的泛化能力

Conclusion: MPF为模型对齐提供可扩展的解决方案，兼容已部署模型，实现可解释的偏见缓解

Abstract: Multiperspective Fusion (MPF) is a novel posttraining alignment framework for
large language models (LLMs) developed in response to the growing need for easy
bias mitigation. Built on top of the SAGED pipeline, an automated system for
constructing bias benchmarks and extracting interpretable baseline
distributions, MPF leverages multiperspective generations to expose and align
biases in LLM outputs with nuanced, humanlike baselines. By decomposing
baseline, such as sentiment distributions from HR professionals, into
interpretable perspective components, MPF guides generation through sampling
and balancing of responses, weighted by the probabilities obtained in the
decomposition. Empirically, we demonstrate its ability to align LLM sentiment
distributions with both counterfactual baselines (absolute equality) and the HR
baseline (biased for Top Univeristy), resulting in small KL divergence,
reduction of calibration error and generalization to unseen questions. This
shows that MPF offers a scalable and interpretable method for alignment and
bias mitigation, compatible with deployed LLMs and requiring no extensive
prompt engineering or finetuning.

</details>


### [16] [Exploring Gender Bias Beyond Occupational Titles](https://arxiv.org/abs/2507.02679)
*Ahmed Sabir,Rajesh Sharama*

Main category: cs.CL

TL;DR: 研究揭示性别与语境偏见关联，提出可量化解释性偏见的GenderLexicon框架，在五个数据集中验证了超越职业范畴的性别偏见存在


<details>
  <summary>Details</summary>
Motivation: 探索性别偏见在动词、名词等语境要素中的表现，突破传统职业刻板印象研究的局限

Method: 构建GenderLexicon数据集及评估框架，通过可解释的偏见评分模型，在包含日语数据集的五个多样化语料库中进行验证

Result: 证实非职业领域的性别偏见普遍存在，框架在跨文化场景（日本数据集）中表现有效

Conclusion: 该研究为语境性别偏见提供了可量化的分析工具，揭示语言偏见更广泛的存在形式，对构建公平NLP系统具有重要意义

Abstract: In this work, we investigate the correlation between gender and contextual
biases, focusing on elements such as action verbs, object nouns, and
particularly on occupations. We introduce a novel dataset, GenderLexicon, and a
framework that can estimate contextual bias and its related gender bias. Our
model can interpret the bias with a score and thus improve the explainability
of gender bias. Also, our findings confirm the existence of gender biases
beyond occupational stereotypes. To validate our approach and demonstrate its
effectiveness, we conduct evaluations on five diverse datasets, including a
Japanese dataset.

</details>


### [17] [Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers](https://arxiv.org/abs/2507.02694)
*Zhijian Xu,Yilun Zhao,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.CL

TL;DR: 提出首个综合基准LimitGen评估大语言模型在辅助同行评审中识别论文局限性的能力，通过文献检索增强模型生成具体反馈的能力


<details>
  <summary>Details</summary>
Motivation: 针对同行评审过程面临的论文数量激增挑战，探索大语言模型在补充人类评审方面的潜力，特别是在识别研究局限性这一未被充分研究的领域

Method: 构建包含人工扰动生成数据集(LimitGen-Syn)和真实人类撰写数据集(LimitGen-Human)的基准，结合文献检索技术增强模型对科学发现的上下文理解

Result: 增强后的LLM系统能够生成更具建设性的局限性分析，提供基于先前研究成果的具体反馈

Conclusion: 该方法有效补充人类同行评审，通过系统化框架提升学术反馈质量，为科研评审流程优化提供新方向

Abstract: Peer review is fundamental to scientific research, but the growing volume of
publications has intensified the challenges of this expertise-intensive
process. While LLMs show promise in various scientific tasks, their potential
to assist with peer review, particularly in identifying paper limitations,
remains understudied. We first present a comprehensive taxonomy of limitation
types in scientific research, with a focus on AI. Guided by this taxonomy, for
studying limitations, we present LimitGen, the first comprehensive benchmark
for evaluating LLMs' capability to support early-stage feedback and complement
human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a
synthetic dataset carefully created through controlled perturbations of
high-quality papers, and LimitGen-Human, a collection of real human-written
limitations. To improve the ability of LLM systems to identify limitations, we
augment them with literature retrieval, which is essential for grounding
identifying limitations in prior scientific findings. Our approach enhances the
capabilities of LLM systems to generate limitations in research papers,
enabling them to provide more concrete and constructive feedback.

</details>


### [18] [Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens](https://arxiv.org/abs/2507.02744)
*Peter Viechnicki*

Main category: cs.CL

TL;DR: 研究首次测量了英语使用者在前元音产生时的'可产生最小差异'(JPD)，发现其在F1×F2空间为14-51mels，为语音产生理论和元音系统结构提供了新见解


<details>
  <summary>Details</summary>
Motivation: 探索人类元音产生中听觉控制机制在次音位水平的精确度，验证偶发语音产生理论并解释元音系统的结构限制

Method: 采用元音模仿范式，通过让两组英语使用者模仿不同前元音刺激来测量可产生的最小听觉差异(JPD)

Result: JPD在F1×F2空间的测量值为14-51mels，确立了人类元音系统相邻音素的理论最小间距

Conclusion: 该发现为语音产生的偶发理论提供支持，并通过设定形式空间最小间距为元音系统的音素数量和分布模式提供心理物理学解释

Abstract: A body of work over the past several decades has demonstrated that the
complex and coordinated articulatory movements of human vowel production are
governed (at least in part)by control mechanisms whose targets are regions of
auditory space. Within the target region control at the sub-phonemic level has
also been demonstrated. But the degree of accuracy of that control is unknown.
The current work investigates this question by asking how far apart must two
vowel stimuli lie in auditory space in order to yield reliably different
imitations? This distance is termed 'Just Producible Difference' (JPD). The
current study uses a vowel mimicry paradigm to derive the first measurement of
JPD among two sets of English speakers during front vowel production. JPD is
estimated at between 14 and 51 mels in F1 X F2 space. This finding has
implications for episodic theories of speech production. It also clarifies the
possible structures of human vowel systems, by setting a theoretical lower
bound for how close two vowel phonemes may be in a speaker's formant space, and
hence a psychophysical explanation of observed trends in number and patterns of
possible vowel phonemes.

</details>


### [19] [Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs](https://arxiv.org/abs/2507.02778)
*Ken Tsui*

Main category: cs.CL

TL;DR: 大型语言模型存在系统性自我纠正盲点（64.5%平均发生率），研究发现训练数据构成是主要原因，通过简单附加'Wait'可将盲点减少89.3%


<details>
  <summary>Details</summary>
Motivation: 揭示自回归LLMs在自我纠正能力上的缺陷——能够识别用户输入错误却无法纠正自身输出的相同错误，这种系统性盲点影响模型可靠性

Method: 开发Self-Correction Bench框架，通过三个复杂度级别的受控错误注入测试，评估14个主流LLM的自我纠正能力

Result: 发现人类训练数据侧重无错误响应导致盲点，RL训练模型通过结果反馈学习纠错。附加'Wait'的简单干预显著激活潜在纠错能力

Conclusion: 本研究揭示LLMs自我纠正机制的关键局限，提出通过训练数据优化和即时干预（如'Wait'提示）两种途径提升模型可靠性

Abstract: Although large language models (LLMs) have become transformative, they still
make mistakes and can explore unproductive reasoning paths. Self-correction is
an important capability for a trustworthy LLM, particularly an autoregressive
LLM. While LLMs can identify error in user input, they exhibit a systematic
'Self-Correction Blind Spot' - failing to correct identical error in their own
outputs. To systematically study this phenomenon, we introduce Self-Correction
Bench, a systematic framework to measure this phenomenon through controlled
error injection at three complexity levels. Testing 14 models, we find an
average 64.5% blind spot rate. We find multiple evidences that this limitation
relates to training data composition: human training demonstrations
predominantly show error-free responses rather than error-correction sequences,
unlike RL-trained models that learn error correction through outcome feedback.
Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting
that the capability exists but requires activation. Our work highlights a
critical limitation in current LLMs and offers potential avenues for improving
their reliability and trustworthiness.

</details>


### [20] [Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models](https://arxiv.org/abs/2507.02799)
*Riccardo Cantini,Nicola Gabriele,Alessio Orsino,Domenico Talia*

Main category: cs.CL

TL;DR: 研究表明：具备显式推理能力的语言模型（通过思维链或微调实现）比基础模型更易引发社会偏见，推理机制可能无意中强化刻板印象


<details>
  <summary>Details</summary>
Motivation: 探究推理语言模型在提升复杂任务处理能力的同时，是否影响其对抗社会偏见的鲁棒性，并比较不同推理机制的安全性差异

Method: 使用CLEAR-Bias基准测试，采用LLM自动安全评分和越狱技术评估，系统分析CoT提示与微调推理模型在不同社会文化维度下的表现

Result: 显式推理模型普遍存在偏见漏洞（CoT模型受上下文重构攻击成功率达62%），微调推理模型相对安全但仍存在16%的越狱成功率

Conclusion: 推理能力与偏见风险存在悖论关系，需开发偏见感知的推理设计方法，挑战了『推理自动提升鲁棒性』的传统认知

Abstract: Reasoning Language Models (RLMs) have gained traction for their ability to
perform complex, multi-step reasoning tasks through mechanisms such as
Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these
capabilities promise improved reliability, their impact on robustness to social
biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,
originally designed for Large Language Models (LLMs), to investigate the
adversarial robustness of RLMs to bias elicitation. We systematically evaluate
state-of-the-art RLMs across diverse sociocultural dimensions, using an
LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak
techniques to assess the strength of built-in safety mechanisms. Our evaluation
addresses three key questions: (i) how the introduction of reasoning
capabilities affects model fairness and robustness; (ii) whether models
fine-tuned for reasoning exhibit greater safety than those relying on CoT
prompting at inference time; and (iii) how the success rate of jailbreak
attacks targeting bias elicitation varies with the reasoning mechanisms
employed. Our findings reveal a nuanced relationship between reasoning
capabilities and bias safety. Surprisingly, models with explicit reasoning,
whether via CoT prompting or fine-tuned reasoning traces, are generally more
vulnerable to bias elicitation than base models without such mechanisms,
suggesting reasoning may unintentionally open new pathways for stereotype
reinforcement. Reasoning-enabled models appear somewhat safer than those
relying on CoT prompting, which are particularly prone to contextual reframing
attacks through storytelling prompts, fictional personas, or reward-shaped
instructions. These results challenge the assumption that reasoning inherently
improves robustness and underscore the need for more bias-aware approaches to
reasoning design.

</details>


### [21] [Multimodal Mathematical Reasoning with Diverse Solving Perspective](https://arxiv.org/abs/2507.02804)
*Wenhao Shi,Zhiqiang Hu,Yi Bin,Yang Yang,See-Kiong Ng,Heng Tao Shen*

Main category: cs.CL

TL;DR: 提出MathV-DP多视角数学推理数据集与Qwen-VL-DP模型，通过多样性增强的强化学习显著提升多模态数学推理能力


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs依赖单视角图像-文本对和单一解决方案监督，忽视有效推理视角的多样性及内部反思过程

Method: 构建MathV-DP数据集提供多解轨迹监督，基于Qwen-VL开发Qwen-VL-DP模型，采用GRPO强化学习方法整合正确性判别与多样性奖励函数

Result: 在MathVista和Math-V基准测试中，模型在准确率和生成多样性上显著超越基线模型

Conclusion: 多视角推理与反思机制对提升多模态数学推理能力具有关键作用，验证了多样性增强训练策略的有效性

Abstract: Recent progress in large-scale reinforcement learning (RL) has notably
enhanced the reasoning capabilities of large language models (LLMs), especially
in mathematical domains. However, current multimodal LLMs (MLLMs) for
mathematical reasoning often rely on one-to-one image-text pairs and
single-solution supervision, overlooking the diversity of valid reasoning
perspectives and internal reflections. In this work, we introduce MathV-DP, a
novel dataset that captures multiple diverse solution trajectories for each
image-question pair, fostering richer reasoning supervision. We further propose
Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and
enhanced via group relative policy optimization (GRPO), a rule-based RL
approach that integrates correctness discrimination and diversity-aware reward
functions. Our method emphasizes learning from varied reasoning perspectives
and distinguishing between correct yet distinct solutions. Extensive
experiments on the MathVista's minitest and Math-V benchmarks demonstrate that
Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and
generative diversity, highlighting the importance of incorporating diverse
perspectives and reflective reasoning in multimodal mathematical reasoning.

</details>


### [22] [SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model](https://arxiv.org/abs/2507.02822)
*Wencheng Zhang,Shiqin Qiao,Lingjie Luo,Yinfeng Li,Chuanyang Zheng,Qian Xu,Meng Li,Yong Gui,Yijun He,Jianing Qiu,Jindong Hong,Jiankai Sun*

Main category: cs.CL

TL;DR: 提出动态路由框架SynapseRoute，通过智能分配问题到思考/非思考模式，在提升准确率（0.8390）的同时显著降低推理时间（36.8%）和token消耗（39.66%），并建立AIT指数评估体系。


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用中存在思考模式（高成本）与非思考模式（低成本）的显著成本差异，约58%的医学问题无需复杂推理即可解决，需要优化资源分配策略。

Method: 基于机器学习的动态路由框架SynapseRoute，通过复杂度分析自动选择最优推理模式（思考/非思考）。

Result: 在医学数据集上相比单一思考模式：整体准确率提升（0.8390 vs 0.8272），推理时间减少36.8%，token消耗降低39.66%，且避免过度推理导致的准确率下降。

Conclusion: 动态路由机制能有效平衡LLM的精度与成本，SynapseRoute验证了该方案可行性，并创新性提出AIT三维评估指标，为模型部署提供新范式。

Abstract: With the widespread adoption of large language models (LLMs) in practical
applications, selecting an appropriate model requires balancing not only
performance but also operational cost. The emergence of reasoning-capable
models has further widened the cost gap between "thinking" (high reasoning) and
"non-thinking" (fast, low-cost) modes. In this work, we reveal that
approximately 58% of medical questions can be accurately answered by the
non-thinking mode alone, without requiring the high-cost reasoning process.
This highlights a clear dichotomy in problem complexity and suggests that
dynamically routing queries to the appropriate mode based on complexity could
optimize accuracy, cost-efficiency, and overall user experience. Based on this,
we further propose SynapseRoute, a machine learning-based dynamic routing
framework that intelligently assigns input queries to either thinking or
non-thinking modes. Experimental results on several medical datasets
demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.
0.8272) compared to the thinking mode alone but also reduces inference time by
36.8% and token consumption by 39.66%. Importantly, qualitative analysis
indicates that over-reasoning on simpler queries can lead to unnecessary delays
and even decreased accuracy, a pitfall avoided by our adaptive routing.
Finally, this work further introduces the Accuracy-Inference-Token (AIT) index
to comprehensively evaluate the trade-offs among accuracy, latency, and token
cost.

</details>


### [23] [Generalizing Verifiable Instruction Following](https://arxiv.org/abs/2507.02833)
*Valentina Pyatkin,Saumya Malik,Victoria Graf,Hamish Ivison,Shengyi Huang,Pradeep Dasigi,Nathan Lambert,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: 论文提出了IFBench基准，通过强化学习与可验证奖励（RLVR）提升AI模型遵循多样化输出约束的能力。


<details>
  <summary>Details</summary>
Motivation: 现有模型在遵循精确指令（如输出格式限制）时存在过拟合基准常见约束的问题，难以泛化到新约束场景，阻碍人机交互效果。

Method: 构建包含58个新约束的IFBench测试集，设计约束验证模块，并采用RLVR方法进行模型训练优化。

Result: RLVR方法显著提升了模型对未见过输出约束的泛化能力。

Conclusion: 多样化约束基准与可验证奖励机制是提升指令遵循能力的关键路径。

Abstract: A crucial factor for successful human and AI interaction is the ability of
language models or chatbots to follow human instructions precisely. A common
feature of instructions are output constraints like ``only answer with yes or
no" or ``mention the word `abrakadabra' at least 3 times" that the user adds to
craft a more useful answer. Even today's strongest models struggle with
fulfilling such constraints. We find that most models strongly overfit on a
small set of verifiable constraints from the benchmarks that test these
abilities, a skill called precise instruction following, and are not able to
generalize well to unseen output constraints. We introduce a new benchmark,
IFBench, to evaluate precise instruction following generalization on 58 new,
diverse, and challenging verifiable out-of-domain constraints. In addition, we
perform an extensive analysis of how and on what data models can be trained to
improve precise instruction following generalization. Specifically, we
carefully design constraint verification modules and show that reinforcement
learning with verifiable rewards (RLVR) significantly improves instruction
following. In addition to IFBench, we release 29 additional new hand-annotated
training constraints and verification functions, RLVR training prompts, and
code.

</details>


### [24] [LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users](https://arxiv.org/abs/2507.02850)
*Almog Hilel,Idan Shenfeld,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 研究揭示基于用户反馈训练的语言模型存在安全漏洞，单个用户通过点赞/点踩机制即可持续篡改模型知识体系与行为模式


<details>
  <summary>Details</summary>
Motivation: 现有基于用户反馈的偏好调整方法存在潜在安全风险，攻击者可利用常规用户权限实施隐蔽的模型污染攻击

Method: 设计攻击流程：1）诱导模型随机输出污染/正常响应 2）通过点赞污染响应或点踩正常响应 3）利用偏好调整机制扩大污染影响

Result: 成功实现三种攻击场景：虚构知识植入、代码安全漏洞注入、虚假金融信息传播，污染效果在非攻击场景持续存在

Conclusion: 该研究既揭示了偏好调整机制的可控性特征，又发现新型用户反馈攻击范式，扩展了预训练数据污染和即时注入攻击的研究边界

Abstract: We describe a vulnerability in language models (LMs) trained with user
feedback, whereby a single user can persistently alter LM knowledge and
behavior given only the ability to provide prompts and upvote / downvote
feedback on LM outputs. To implement the attack, the attacker prompts the LM to
stochastically output either a "poisoned" or benign response, then upvotes the
poisoned response or downvotes the benign one. When feedback signals are used
in a subsequent preference tuning behavior, LMs exhibit increased probability
of producing poisoned responses even in contexts without malicious prompts. We
show that this attack can be used to (1) insert factual knowledge the model did
not previously possess, (2) modify code generation patterns in ways that
introduce exploitable security flaws, and (3) inject fake financial news. Our
finding both identifies a new qualitative feature of language model preference
tuning (showing that it even highly restricted forms of preference data can be
used to exert fine-grained control over behavior), and a new attack mechanism
for LMs trained with user feedback (extending work on pretraining-time data
poisoning and deployment-time prompt injection).

</details>


### [25] [MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs](https://arxiv.org/abs/2507.02851)
*Purbesh Mitra,Sennur Ulukus*

Main category: cs.CL

TL;DR: 提出MOTIF方法，通过强化学习微调实现多轮模块化思考，突破LLM上下文限制，在数学推理任务中实现3.3%-3.8%准确率提升


<details>
  <summary>Details</summary>
Motivation: 现有LLM受限于上下文长度，无法进行长程多轮推理。需要开发模块化思考策略突破该瓶颈

Method: 基于GRPO强化学习算法开发MOTIF训练框架，允许模型分多轮生成思考token，通过参数高效微调在Qwen2.5-3B-Instruct模型上实现

Result: 在MATH500和AIME2024基准上分别提升3.8%和3.3%准确率，仅使用15%训练样本即实现效果提升

Conclusion: MOTIF方法有效突破LLM上下文限制，显著提升数学推理能力，同时具有优秀的样本效率

Abstract: Recent advancements in the reasoning capabilities of large language models
(LLMs) show that employing group relative policy optimization (GRPO) algorithm
for reinforcement learning (RL) training allows the models to use more
thinking/reasoning tokens for generating better responses. However, LLMs can
generate only a finite amount of tokens while maintaining attention to the
previously generated tokens. This limit, also known as the context size of an
LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.
To think beyond the limit of context size, an LLM must employ a modular
thinking strategy to reason over multiple rounds. In this work, we propose
$\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL
training method for generating thinking tokens in multiple rounds, effectively
allowing the model to think with additional context size. We trained the
open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient
fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our
experiments show 3.8\% and 3.3\% improvements over vanilla GRPO based training
in the respective benchmarks. Furthermore, this improvement was achieved with
only 15\% of samples, thus demonstrating sample efficiency of MOTIF. Our code
and models are available at https://github.com/purbeshmitra/MOTIF and
https://huggingface.co/purbeshmitra/MOTIF, respectively.

</details>


### [26] [Answer Matching Outperforms Multiple Choice for Language Model Evaluation](https://arxiv.org/abs/2507.02856)
*Nikhil Chandak,Shashwat Goel,Ameya Prabhu,Moritz Hardt,Jonas Geiping*

Main category: cs.CL

TL;DR: 传统多项选择评测存在捷径问题，生成式答案匹配方法能有效提升评估准确性，并与人类评分高度一致。


<details>
  <summary>Details</summary>
Motivation: 传统多项选择题评测存在根本性限制，模型可能无需阅读问题即可通过选项捷径回答问题。研究旨在通过生成式答案匹配方法替代传统评测，提高评估有效性。

Method: 提出生成式答案匹配评估法：让模型生成自由回答后，使用现代语言模型对比参考答案进行匹配判断。通过标注MMLU-Pro和GPQA-Diamond数据集，分析不同评估方法(多项选择/答案匹配/无参考答案的LLM评判)与人类评分的一致性。

Result: 答案匹配方法(即使小型模型)与人类评分一致性接近人类间水平，显著优于多项选择(约+40%)。该方法导致多个模型排名发生实质性变化。

Conclusion: 评估体系应从多项选择转向答案匹配方法。该方法利用现代语言模型能力，可有效捕捉模型真实表现，且当前技术条件已具备实施可行性。

Abstract: Multiple choice benchmarks have long been the workhorse of language model
evaluation because grading multiple choice is objective and easy to automate.
However, we show multiple choice questions from popular benchmarks can often be
answered without even seeing the question. These shortcuts arise from a
fundamental limitation of discriminative evaluation not shared by evaluations
of the model's free-form, generative answers. Until recently, there appeared to
be no viable, scalable alternative to multiple choice--but, we show that this
has changed. We consider generative evaluation via what we call answer
matching: Give the candidate model the question without the options, have it
generate a free-form response, then use a modern language model with the
reference answer to determine if the response matches the reference. To compare
the validity of different evaluation strategies, we annotate MMLU-Pro and
GPQA-Diamond to obtain human grading data, and measure the agreement of each
evaluation approach. We find answer matching using recent models--even small
ones--achieves near-perfect agreement, in the range of inter-annotator
agreement. In contrast, both multiple choice evaluation and using
LLM-as-a-judge without reference answers aligns poorly with human grading.
Improving evaluations via answer matching is not merely a conceptual concern:
the rankings of several models change significantly when evaluating their
free-form responses with answer matching. In light of these findings, we
discuss how to move the evaluation ecosystem from multiple choice to answer
matching.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [27] [Gbake: Baking 3D Gaussian Splats into Reflection Probes](https://arxiv.org/abs/2507.02257)
*Stephen Pasch,Joel K. Salzman,Changxi Zheng*

Main category: cs.GR

TL;DR: 提出GBake工具，通过从高斯泼溅场景烘焙反射探针，实现在Unity引擎中对传统3D网格的逼真反射映射


<details>
  <summary>Details</summary>
Motivation: 解决传统3D网格直接融入3D高斯泼溅环境时因光效不匹配导致的视觉违和问题

Method: 开发专用工具GBake，通过烘焙高斯泼溅场景的反射探针数据

Result: 成功在Unity游戏引擎中实现传统网格物体与高斯泼溅场景的光照一致性融合

Conclusion: 该工具弥合了新兴渲染技术与传统资产间的鸿沟，提升了游戏引擎中不同渲染技术的兼容性和视觉统一性

Abstract: The growing popularity of 3D Gaussian Splatting has created the need to
integrate traditional computer graphics techniques and assets in splatted
environments. Since 3D Gaussian primitives encode lighting and geometry jointly
as appearance, meshes are relit improperly when inserted directly in a mixture
of 3D Gaussians and thus appear noticeably out of place. We introduce GBake, a
specialized tool for baking reflection probes from Gaussian-splatted scenes
that enables realistic reflection mapping of traditional 3D meshes in the Unity
game engine.

</details>


### [28] [Real-time Image-based Lighting of Glints](https://arxiv.org/abs/2507.02674)
*Tom Kneiphof,Reinhard Klein*

Main category: cs.GR

TL;DR: 提出实时环境光下微表面闪光渲染的高效近似方法，支持动态材质和环境光映射，实现接近真实渲染效果。


<details>
  <summary>Details</summary>
Motivation: 解决闪烁材质在动态环境光和材质属性变化时的实时渲染难题，突破传统方法在动态场景下的性能瓶颈。

Method: 结合区域光实时渲染与环境贴图过滤技术，通过环境贴图分区、正态分布概率计算，以及双门控高斯近似实现多区域采样。

Result: 验证了不同材质/光照下的渲染精度，内存占用为普通材质的2倍，帧率稳定且性能接近单一方向光源渲染。

Conclusion: 该方案首次实现动态环境光下的实时闪光渲染，在保证视觉效果的同时保持计算效率，适用于游戏/VR等实时图形领域。

Abstract: Image-based lighting is a widely used technique to reproduce shading under
real-world lighting conditions, especially in real-time rendering applications.
A particularly challenging scenario involves materials exhibiting a sparkling
or glittering appearance, caused by discrete microfacets scattered across their
surface. In this paper, we propose an efficient approximation for image-based
lighting of glints, enabling fully dynamic material properties and environment
maps. Our novel approach is grounded in real-time glint rendering under area
light illumination and employs standard environment map filtering techniques.
Crucially, our environment map filtering process is sufficiently fast to be
executed on a per-frame basis. Our method assumes that the environment map is
partitioned into few homogeneous regions of constant radiance. By filtering the
corresponding indicator functions with the normal distribution function, we
obtain the probabilities for individual microfacets to reflect light from each
region. During shading, these probabilities are utilized to hierarchically
sample a multinomial distribution, facilitated by our novel dual-gated Gaussian
approximation of binomial distributions. We validate that our real-time
approximation is close to ground-truth renderings for a range of material
properties and lighting conditions, and demonstrate robust and stable
performance, with little overhead over rendering glints from a single
directional light. Compared to rendering smooth materials without glints, our
approach requires twice as much memory to store the prefiltered environment
map.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [29] [Analyzing and Improving Speaker Similarity Assessment for Speech Synthesis](https://arxiv.org/abs/2507.02176)
*Marc-André Carbonneau,Benjamin van Niekerk,Hugo Seuté,Jean-Philippe Letendre,Herman Kamper,Julian Zaïdi*

Main category: cs.SD

TL;DR: 研究发现ASV语音嵌入主要捕捉音色/音高等静态特征，忽视了节奏等动态特征，并提出了评估动态节奏的U3D指标


<details>
  <summary>Details</summary>
Motivation: 解决生成语音系统中说话人身份表征不完整的问题，当前ASV嵌入侧重区分性而非全面特征刻画，需系统性评估其局限性

Method: 分析主流ASV嵌入的建模偏差，识别说话人相似度测量中的混杂因素，提出动态节奏评估指标U3D及改进策略

Result: 发现ASV嵌入忽略动态韵律特征，证实节奏模式对身份感知的影响，提出的U3D有效提升了说话人相似度评估的全面性

Conclusion: 该研究推动了语音克隆系统中说话人身份一致性评估的发展，通过开源代码促进语音合成表征学习领域的进步

Abstract: Modeling voice identity is challenging due to its multifaceted nature. In
generative speech systems, identity is often assessed using automatic speaker
verification (ASV) embeddings, designed for discrimination rather than
characterizing identity. This paper investigates which aspects of a voice are
captured in such representations. We find that widely used ASV embeddings focus
mainly on static features like timbre and pitch range, while neglecting dynamic
elements such as rhythm. We also identify confounding factors that compromise
speaker similarity measurements and suggest mitigation strategies. To address
these gaps, we propose U3D, a metric that evaluates speakers' dynamic rhythm
patterns. This work contributes to the ongoing challenge of assessing speaker
identity consistency in the context of ever-better voice cloning systems. We
publicly release our code.

</details>


### [30] [JoyTTS: LLM-based Spoken Chatbot With Voice Cloning](https://arxiv.org/abs/2507.02380)
*Fangru Zhou,Jun Zhao,Guoxin Wang*

Main category: cs.SD

TL;DR: JoyTTS是基于大语言模型和语音合成技术的端到端语音聊天机器人，具备语音克隆功能，训练数据2000小时，开放完整训练代码。


<details>
  <summary>Details</summary>
Motivation: 通过融合语音克隆技术和大语言模型，打造自然流畅的语音交互系统，推动对话式AI的拟人化发展。

Method: 基于MiniCPM-o和CosyVoice2开源模型架构，使用2000小时对话数据进行训练，提供完整的训练/推理代码生态。

Result: 在seed-tts-zh测试集上达到0.73的说话人相似度(SS)和5.09%的字错误率(WER)。

Conclusion: 该项目实现了高质量的语音对话系统，通过开源代码和模型推动社区在语音交互领域的研究与优化。

Abstract: JoyTTS is an end-to-end spoken chatbot that combines large language models
(LLM) with text-to-speech (TTS) technology, featuring voice cloning
capabilities. This project is built upon the open-source MiniCPM-o and
CosyVoice2 models and trained on 2000 hours of conversational data. We have
also provided the complete training code to facilitate further development and
optimization by the community. On the testing machine seed-tts-zh, it achieves
a SS (speaker similarity) score of 0.73 and a WER (Word Error Rate) of 5.09.
The code and models, along with training and inference scripts, are available
at https://github.com/jdh-algo/JoyTTS.git.

</details>


### [31] [ASDA: Audio Spectrogram Differential Attention Mechanism for Self-Supervised Representation Learning](https://arxiv.org/abs/2507.02666)
*Junyu Wang,Tianrui Wang,Meng Ge,Longbiao Wang,Jianwu Dang*

Main category: cs.SD

TL;DR: 提出ASDA模型通过差分注意力机制改进音频自监督学习，在多个任务中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的注意力机制存在无效权重分配问题，影响模型判别能力

Method: 采用双softmax运算和可调差分系数的差分注意力机制（ASDA模型）

Result: 音频分类（AS-2M:49.0% mAP）、关键词检测（SPC-2:98.3%）、环境音分类（ESC-50:96.1%）均达最优

Conclusion: ASDA模型有效提升了音频任务表现，为更广泛应用奠定基础

Abstract: In recent advancements in audio self-supervised representation learning, the
standard Transformer architecture has emerged as the predominant approach, yet
its attention mechanism often allocates a portion of attention weights to
irrelevant information, potentially impairing the model's discriminative
ability. To address this, we introduce a differential attention mechanism,
which effectively mitigates ineffective attention allocation through the
integration of dual-softmax operations and appropriately tuned differential
coefficients. Experimental results demonstrate that our ASDA model achieves
state-of-the-art (SOTA) performance across multiple benchmarks, including audio
classification (49.0% mAP on AS-2M, 41.5% mAP on AS20K), keyword spotting
(98.3% accuracy on SPC-2), and environmental sound classification (96.1%
accuracy on ESC-50). These results highlight ASDA's effectiveness in audio
tasks, paving the way for broader applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [32] [Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features](https://arxiv.org/abs/2507.01984)
*Gautam Kishore Shahi*

Main category: cs.LG

TL;DR: 通过融合文本、图像和社交特征的多模态早期融合方法，结合无监督与监督机器学习模型使虚假信息检测准确率比单模态提升15%，比双模态提升5%


<details>
  <summary>Details</summary>
Motivation: 现有虚假信息检测研究多集中于单一文本或图像模态，缺乏对多模态特征组合（文本+图像+社交特征）的系统探索，尤其在选举和COVID-19等重大事件中急需有效检测手段

Method: 收集1,529条含图文推文，通过目标检测、OCR等技术提取视觉特征和社交特征，采用早期融合策略构建多模态分类模型，结合无监督与监督机器学习方法

Result: 三模态模型较单模态准确率提升15%，较双模态提升5%；揭示了虚假信息推文的传播模式与用户特征关联性

Conclusion: 多模态融合策略显著提升检测效果，早期特征融合与混合学习模式的结合为关键；虚假信息传播存在可识别的用户行为模式和内容特征

Abstract: Amid a tidal wave of misinformation flooding social media during elections
and crises, extensive research has been conducted on misinformation detection,
primarily focusing on text-based or image-based approaches. However, only a few
studies have explored multimodal feature combinations, such as integrating text
and images for building a classification model to detect misinformation. This
study investigates the effectiveness of different multimodal feature
combinations, incorporating text, images, and social features using an early
fusion approach for the classification model. This study analyzed 1,529 tweets
containing both text and images during the COVID-19 pandemic and election
periods collected from Twitter (now X). A data enrichment process was applied
to extract additional social features, as well as visual features, through
techniques such as object detection and optical character recognition (OCR).
The results show that combining unsupervised and supervised machine learning
models improves classification performance by 15% compared to unimodal models
and by 5% compared to bimodal models. Additionally, the study analyzes the
propagation patterns of misinformation based on the characteristics of
misinformation tweets and the users who disseminate them.

</details>


### [33] [Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions](https://arxiv.org/abs/2507.02087)
*Eitan Anzenberg,Arunava Samajpati,Sivasankaran Chandrasekar,Varun Kacholia*

Main category: cs.LG

TL;DR: 专有招聘模型Match Score在准确率(ROC AUC 0.85)和公平性(种族影响比0.957)上全面超越通用大语言模型，证明专业算法可同时实现招聘准确性与结果公平性


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在招聘场景应用存在算法偏见风险，需验证领域专用模型在准确性和公平性上的优势

Method: 通过10,000个真实候选人-职位配对数据，对比五大厂商LLM与Match Score的ROC AUC/PR AUC/F1分数及人口统计学公平指标

Result: Match Score的ROC AUC达0.85（LLMs最高0.77），最低种族影响比0.957接近完全公平（最佳LLM仅0.809），交叉群体公平性0.906远超LLMs的0.773

Conclusion: 专业监督模型能兼顾准确与公平，强调高风险领域AI需定制化建模和偏见审计，打破'准确性与公平性不可兼得'的认知误区

Abstract: The use of large language models (LLMs) in hiring promises to streamline
candidate screening, but it also raises serious concerns regarding accuracy and
algorithmic bias where sufficient safeguards are not in place. In this work, we
benchmark several state-of-the-art foundational LLMs - including models from
OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our
proprietary domain-specific hiring model (Match Score) for job candidate
matching. We evaluate each model's predictive accuracy (ROC AUC,
Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis
across declared gender, race, and intersectional subgroups). Our experiments on
a dataset of roughly 10,000 real-world recent candidate-job pairs show that
Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs
0.77) and achieves significantly more equitable outcomes across demographic
groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957
(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the
intersectionals, respectively). We discuss why pretraining biases may cause
LLMs with insufficient safeguards to propagate societal biases in hiring
scenarios, whereas a bespoke supervised model can more effectively mitigate
these biases. Our findings highlight the importance of domain-specific modeling
and bias auditing when deploying AI in high-stakes domains such as hiring, and
caution against relying on off-the-shelf LLMs for such tasks without extensive
fairness safeguards. Furthermore, we show with empirical evidence that there
shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a
well-designed algorithm can achieve both accuracy in hiring and fairness in
outcomes.

</details>


### [34] [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
*Alexi Gladstone,Ganesh Nanduru,Md Mofijul Islam,Peixuan Han,Hyeonjeong Ha,Aman Chadha,Yilun Du,Heng Ji,Jundong Li,Tariq Iqbal*

Main category: cs.LG

TL;DR: 提出基于能量的Transformer（EBTs），通过无监督学习实现系统2式推理，在性能和泛化能力上超越现有方法


<details>
  <summary>Details</summary>
Motivation: 现有系统2思维方法存在模态依赖、问题局限性和需额外监督三大缺陷，探索通过纯无监督学习实现通用系统2思维的可能性

Method: 训练能量模型评估输入-预测对的兼容性，通过梯度下降实现能量最小化预测，支持离散（文本）和连续（视觉）模态的联合优化

Result: 训练速度比Transformer++快35%，语言任务推理性能提升29%，图像去噪超越Diffusion Transformers，下游任务泛化性更优

Conclusion: EBTs为同时扩展模型学习能力和推理能力提供了新范式，在更少预训练资源下展现更好的泛化性能

Abstract: Inference-time computation techniques, analogous to human System 2 Thinking,
have recently become popular for improving model performances. However, most
existing approaches suffer from several limitations: they are modality-specific
(e.g., working only in text), problem-specific (e.g., verifiable domains like
math and coding), or require additional supervision/training on top of
unsupervised pretraining (e.g., verifiers or verifiable rewards). In this
paper, we ask the question "Is it possible to generalize these System 2
Thinking approaches, and develop models that learn to think solely from
unsupervised learning?" Interestingly, we find the answer is yes, by learning
to explicitly verify the compatibility between inputs and
candidate-predictions, and then re-framing prediction problems as optimization
with respect to this verifier. Specifically, we train Energy-Based Transformers
(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy
value to every input and candidate-prediction pair, enabling predictions
through gradient descent-based energy minimization until convergence. Across
both discrete (text) and continuous (visual) modalities, we find EBTs scale
faster than the dominant Transformer++ approach during training, achieving an
up to 35% higher scaling rate with respect to data, batch size, parameters,
FLOPs, and depth. During inference, EBTs improve performance with System 2
Thinking by 29% more than the Transformer++ on language tasks, and EBTs
outperform Diffusion Transformers on image denoising while using fewer forward
passes. Further, we find that EBTs achieve better results than existing models
on most downstream tasks given the same or worse pretraining performance,
suggesting that EBTs generalize better than existing approaches. Consequently,
EBTs are a promising new paradigm for scaling both the learning and thinking
capabilities of models.

</details>


### [35] [OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding](https://arxiv.org/abs/2507.02659)
*Ramchalam Kinattinkara Ramakrishnan,Zhaocong Yuan,Shaojie Zhuo,Chen Feng,Yicheng Lin,Chenzheng Su,Xiaopeng Zhang*

Main category: cs.LG

TL;DR: 提出OmniDraft框架，实现单一草稿模型适配多目标模型，通过在线n-gram缓存和自适应草稿技术提升推测解码速度1.5-2倍


<details>
  <summary>Details</summary>
Motivation: 在线部署场景中面临目标模型与草稿模型不兼容、延迟优化需动态适应用户数据两大挑战，传统离线蒸馏方法缺乏灵活性

Method: 采用在线n-gram缓存解决跨模型词汇表不匹配问题，结合混合蒸馏微调和自适应草稿技术动态优化解码效率

Result: 单个Llama-68M模型成功适配Vicuna/Qwen2/Llama3等不同架构目标模型，在数学推理/代码生成等任务中实现1.5-2倍加速

Conclusion: OmniDraft验证了'一稿多用'范式的可行性，特别适用于需平衡模型成本、效率与定制化的端侧LLM应用场景

Abstract: Speculative decoding generally dictates having a small, efficient draft model
that is either pretrained or distilled offline to a particular target model
series, for instance, Llama or Qwen models. However, within online deployment
settings, there are two major challenges: 1) usage of a target model that is
incompatible with the draft model; 2) expectation of latency improvements over
usage and time. In this work, we propose OmniDraft, a unified framework that
enables a single draft model to operate with any target model and adapt
dynamically to user data. We introduce an online n-gram cache with hybrid
distillation fine-tuning to address the cross-vocabulary mismatch across draft
and target models; and further improve decoding speed by leveraging adaptive
drafting techniques. OmniDraft is particularly suitable for on-device LLM
applications where model cost, efficiency and user customization are the major
points of contention. This further highlights the need to tackle the above
challenges and motivates the \textit{``one drafter for all''} paradigm. We
showcase the proficiency of the OmniDraft framework by performing online
learning on math reasoning, coding and text generation tasks. Notably,
OmniDraft enables a single Llama-68M model to pair with various target models
including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;
and additionally provides up to 1.5-2x speedup.

</details>


### [36] [ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning](https://arxiv.org/abs/2507.02834)
*Ruiyang Zhou,Shuozhe Li,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: 提出Self-Explanation Policy Optimization (ExPO)框架，通过条件化生成与当前策略对齐的解释样本来突破RL后训练的探索瓶颈，在困难推理任务中显著提升模型表现


<details>
  <summary>Details</summary>
Motivation: 现有RL后训练方法过度依赖模型初始生成能力，无法在初始失败领域有效探索新推理路径，导致难以突破模型能力边界

Method: ExPO框架利用真实答案生成符合当前策略的高质量正样本，通过两个关键标准：(1)样本在当前策略下具有高可能性；(2)能提升模型预测正确答案的概率

Result: 在MATH level-5等困难推理任务中，ExPO显著超越基于专家示范的方法，在模型初始表现最差的领域实现最大提升

Conclusion: ExPO通过自解释机制实现高效探索，既能保持与当前策略的一致性，又确保样本质量优于错误样本，为困难推理任务的RL训练提供新范式

Abstract: Recent advances in large language models have been driven by reinforcement
learning (RL)-style post-training, which improves reasoning by optimizing model
outputs based on reward or preference signals. GRPO-style approaches implement
this by using self-generated samples labeled by an outcome-based verifier.
However, these methods depend heavily on the model's initial ability to produce
positive samples. They primarily refine what the model already knows
(distribution sharpening) rather than enabling the model to solve problems
where it initially fails. This limitation is especially problematic in
early-stage RL training and on challenging reasoning tasks, where positive
samples are unlikely to be generated. To unlock reasoning ability in such
settings, the model must explore new reasoning trajectories beyond its current
output distribution. Such exploration requires access to sufficiently good
positive samples to guide the learning. While expert demonstrations seem like a
natural solution, we find that they are often ineffective in RL post-training.
Instead, we identify two key properties of effective positive samples: they
should (1) be likely under the current policy, and (2) increase the model's
likelihood of predicting the correct answer. Based on these insights, we
propose $\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and
modular framework that generates such samples by conditioning on the
ground-truth answer. ExPO enables efficient exploration and guides the model to
produce reasoning trajectories more aligned with its policy than expert-written
CoTs, while ensuring higher quality than its own (incorrect) samples.
Experiments show that ExPO improves both learning efficiency and final
performance on reasoning benchmarks, surpassing expert-demonstration-based
methods in challenging settings such as MATH level-5, where the model initially
struggles the most.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [37] [FinAI-BERT: A Transformer-Based Model for Sentence-Level Detection of AI Disclosures in Financial Reports](https://arxiv.org/abs/2507.01991)
*Muhammad Bilal Zafar*

Main category: q-fin.CP

TL;DR: 开发FinAI-BERT模型实现金融文本句子级AI披露检测，准确率99.37%，显著优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有AI信息披露工具粒度粗、可解释性差的问题，适应金融监管需求。

Method: 基于669份银行年报构建平衡数据集，微调领域专用Transformer模型FinAI-BERT。

Result: F1分数达0.993，SHAP解释性分析验证模型稳定性，通过对抗样本测试。

Conclusion: 该研究为金融AI监测提供细粒度分析工具，推动NLP在监管科技中的应用。

Abstract: The proliferation of artificial intelligence (AI) in financial services has
prompted growing demand for tools that can systematically detect AI-related
disclosures in corporate filings. While prior approaches often rely on keyword
expansion or document-level classification, they fall short in granularity,
interpretability, and robustness. This study introduces FinAI-BERT, a
domain-adapted transformer-based language model designed to classify AI-related
content at the sentence level within financial texts. The model was fine-tuned
on a manually curated and balanced dataset of 1,586 sentences drawn from 669
annual reports of U.S. banks (2015 to 2023). FinAI-BERT achieved near-perfect
classification performance (accuracy of 99.37 percent, F1 score of 0.993),
outperforming traditional baselines such as Logistic Regression, Naive Bayes,
Random Forest, and XGBoost. Interpretability was ensured through SHAP-based
token attribution, while bias analysis and robustness checks confirmed the
model's stability across sentence lengths, adversarial inputs, and temporal
samples. Theoretically, the study advances financial NLP by operationalizing
fine-grained, theme-specific classification using transformer architectures.
Practically, it offers a scalable, transparent solution for analysts,
regulators, and scholars seeking to monitor the diffusion and framing of AI
across financial institutions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [38] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: STELLA通过动态工具库和自适应学习机制显著提升生物医学研究效率与准确性


<details>
  <summary>Details</summary>
Motivation: 生物医学数据/工具/文献的爆炸式增长导致研究碎片化，传统静态AI工具无法满足动态扩展需求

Method: 采用多智能体架构，包含自更新的推理策略模板库（Template Library）和自动扩展的生物信息工具海洋（Tool Ocean）

Result: 在Humanity's Last Exam生物医学考试(26%)、LAB-Bench文献问答(63%)等基准测试中实现SOTA，性能随经验积累系统提升（如医学考试准确率随试验次数翻倍）

Conclusion: STELLA标志着AI代理系统向动态扩展专业知识的重大突破，为加速生物医学发现提供可进化解决方案

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [39] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: 大语言模型在迭代囚徒困境博弈中展现出战略决策能力，不同厂商模型呈现独特策略特征（谷歌Gemini攻击性强，OpenAI高度合作型，Anthropic宽容互惠型），其决策过程包含对时间跨度和对手策略的主动推理。


<details>
  <summary>Details</summary>
Motivation: 验证大语言模型是否具备战略推理能力，通过经典博弈论框架探究AI在竞争环境中的目标导向决策机制，连接传统博弈论与机器心理学领域。

Method: 设计进化版迭代囚徒困境锦标赛，将经典策略（以牙还牙/冷酷触发）与前沿AI公司模型对抗，通过调整终止概率（未来阴影系数）增加博弈复杂性，并分析近32,000条模型决策文本依据。

Result: LLMs展现出持续生存能力与策略分化：Gemini擅长剥削合作者并报复背叛者；OpenAI模型因过度合作在敌对环境中受损；Claude表现出最强的互惠宽容特质。模型决策依赖对时间跨度和对手策略的主动推理。

Conclusion: 该研究揭示了LLMs在不确定性下的战略决策潜力，为算法决策机制提供了颗粒度分析框架，证明经典博弈论可有效应用于机器心理学研究，预示AI在复杂战略场景中的应用前景。

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [40] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: 提出分层框架HiRA，通过解耦规划与执行提升复杂搜索任务性能，在四大基准测试中显著超越现有系统


<details>
  <summary>Details</summary>
Motivation: 传统RAG系统和单一模型架构在复杂跨模态搜索任务中存在规划与执行耦合的缺陷，导致推理效率低下和可扩展性受限

Method: 1. 任务分解：将复杂搜索拆分为专注子任务
2. 专业代理：为每个子任务分配领域专用代理（含外部工具与推理能力）
3. 协调机制：通过结构化集成框架整合各代理结果

Result: 在四个跨模态深度搜索基准测试中，答案质量提升15-20%，系统效率提高30%

Conclusion: 分层架构有效平衡战略规划与专业执行，验证了分离式设计对多步信息寻求任务的价值

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [41] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang,Ang Lv,Jinpeng Li,Yongbo Wang,Feng Wang,Haoyuan Hu,Rui Yan*

Main category: cs.AI

TL;DR: 提出StepHint算法，通过多级逐步提示解决RLVR方法的近失奖励问题和探索停滞问题，在数学基准测试中表现出优越性能与泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在近失奖励问题（微小错误导致正确推理链失效）和探索停滞现象（模型局限在舒适区缺乏创新），严重影响训练效率和模型潜力开发

Method: 从强模型生成有效推理链，通过自适应划分生成多级步骤提示。使用前序步骤作为提示引导探索方向，同时保留多级提示的灵活性，平衡指导与自主探索

Result: 在6个数学基准测试中超越现有强化学习方法，域外基准测试表现优于基线模型，展示出更优的泛化能力和推理稳定性

Conclusion: StepHint通过结构化提示机制有效缓解RLVR的核心瓶颈，显著提升训练效率和模型探索能力，为复杂推理任务提供新的优化路径

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [42] [Legal Requirements Translation from Law](https://arxiv.org/abs/2507.02846)
*Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 提出基于文本蕴含和上下文学习的自动化方法，将法律文本编码为Python类结构，解决法律合规中元数据提取难题


<details>
  <summary>Details</summary>
Motivation: 现有法律文本元数据提取方法忽视属性关联性且依赖人工标注，导致小企业合规成本高且泛化能力差

Method: 设计Python领域特定元模型，通过文本蕴含识别和上下文学习自动生成可执行代码表示

Result: 在美国13个州数据泄露通知法测试中，生成表示通过89.4%测试用例，精确率82.2/召回率88.7

Conclusion: 该方法降低对标注数据依赖，通过可执行代码表征有效提升法律文本处理的可扩展性和适应性

Abstract: Software systems must comply with legal regulations, which is a
resource-intensive task, particularly for small organizations and startups
lacking dedicated legal expertise. Extracting metadata from regulations to
elicit legal requirements for software is a critical step to ensure compliance.
However, it is a cumbersome task due to the length and complex nature of legal
text. Although prior work has pursued automated methods for extracting
structural and semantic metadata from legal text, key limitations remain: they
do not consider the interplay and interrelationships among attributes
associated with these metadata types, and they rely on manual labeling or
heuristic-driven machine learning, which does not generalize well to new
documents. In this paper, we introduce an approach based on textual entailment
and in-context learning for automatically generating a canonical representation
of legal text, encodable and executable as Python code. Our representation is
instantiated from a manually designed Python class structure that serves as a
domain-specific metamodel, capturing both structural and semantic legal
metadata and their interrelationships. This design choice reduces the need for
large, manually labeled datasets and enhances applicability to unseen
legislation. We evaluate our approach on 13 U.S. state data breach notification
laws, demonstrating that our generated representations pass approximately 89.4%
of test cases and achieve a precision and recall of 82.2 and 88.7,
respectively.

</details>


### [43] [Requirements Elicitation Follow-Up Question Generation](https://arxiv.org/abs/2507.02858)
*Yuchen Shen,Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 研究利用GPT-4o在需求访谈中实时生成后续问题，通过两阶段实验证明LLM生成的问题在清晰度、相关性和信息量上不逊于人类，且在错误类型框架指导下表现更优


<details>
  <summary>Details</summary>
Motivation: 传统需求访谈依赖人工实时提问面临领域知识不足、认知负荷高和信息过载挑战，LLM在自然语言处理的优势为提升访谈质量提供新可能

Method: 构建常见采访错误类型框架，开发无指导/错误类型指导两种LLM问题生成方法，通过对照组实验分别比较LLM与人类问题的质量差异

Result: 实验表明LLM生成问题与人类水平相当，在错误类型框架指导下生成的问题在关键指标上显著优于人类提问

Conclusion: LLM可实时生成高质量访谈问题，有效降低采访者认知负担，为提升需求获取效率提供技术支持

Abstract: Interviews are a widely used technique in eliciting requirements to gather
stakeholder needs, preferences, and expectations for a software system.
Effective interviewing requires skilled interviewers to formulate appropriate
interview questions in real time while facing multiple challenges, including
lack of familiarity with the domain, excessive cognitive load, and information
overload that hinders how humans process stakeholders' speech. Recently, large
language models (LLMs) have exhibited state-of-the-art performance in multiple
natural language processing tasks, including text summarization and entailment.
To support interviewers, we investigate the application of GPT-4o to generate
follow-up interview questions during requirements elicitation by building on a
framework of common interviewer mistake types. In addition, we describe methods
to generate questions based on interviewee speech. We report a controlled
experiment to evaluate LLM-generated and human-authored questions with minimal
guidance, and a second controlled experiment to evaluate the LLM-generated
questions when generation is guided by interviewer mistake types. Our findings
demonstrate that, for both experiments, the LLM-generated questions are no
worse than the human-authored questions with respect to clarity, relevancy, and
informativeness. In addition, LLM-generated questions outperform human-authored
questions when guided by common mistakes types. This highlights the potential
of using LLMs to help interviewers improve the quality and ease of requirements
elicitation interviews in real time.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [44] [Mesh Silksong: Auto-Regressive Mesh Generation as Weaving Silk](https://arxiv.org/abs/2507.02477)
*Gaochao Song,Zibo Zhao,Haohan Weng,Jingbo Zeng,Rongfei Jia,Shenghua Gao*

Main category: cs.CV

TL;DR: 提出Mesh Silksong网格表示方法，通过单次顶点访问减少50%标记冗余，实现22%压缩率并改善网格几何属性


<details>
  <summary>Details</summary>
Motivation: 解决现有网格标记化方法存在顶点重复标记导致的网络能力浪费问题

Method: 以类似丝绸编织的自回归方式生成多边形网格，每个顶点仅被访问一次

Result: 达到22%压缩率，生成网格具备流形拓扑/水密性/一致面法线等关键几何特性

Conclusion: 该方法在保持复杂网格生成能力的同时显著提升了几何完整性，具有实际应用价值

Abstract: We introduce Mesh Silksong, a compact and efficient mesh representation
tailored to generate the polygon mesh in an auto-regressive manner akin to silk
weaving. Existing mesh tokenization methods always produce token sequences with
repeated vertex tokens, wasting the network capability. Therefore, our approach
tokenizes mesh vertices by accessing each mesh vertice only once, reduces the
token sequence's redundancy by 50\%, and achieves a state-of-the-art
compression rate of approximately 22\%. Furthermore, Mesh Silksong produces
polygon meshes with superior geometric properties, including manifold topology,
watertight detection, and consistent face normals, which are critical for
practical applications. Experimental results demonstrate the effectiveness of
our approach, showcasing not only intricate mesh generation but also
significantly improved geometric integrity.

</details>


### [45] [HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars](https://arxiv.org/abs/2507.02803)
*Gent Serifi,Marcel C. Bühler*

Main category: cs.CV

TL;DR: 提出HyperGaussians作为3D高斯泼溅的扩展方法，通过高维高斯表示和反向协方差技巧显著提升可动画人脸建模的细节表现


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅在动态面部建模中存在非线性变形处理困难、高频细节丢失和计算效率瓶颈，需从根本上改进高斯表示方式

Method: 将3D高斯扩展为高维多元高斯（HyperGaussians），引入可学习的局部嵌入增强表达能力，通过协方差矩阵重参数化技术（反向协方差技巧）优化计算效率

Result: 在4个数据集19个对象的实验中，数值指标和视觉效果均超越3DGS，尤其在眼镜框、牙齿等高频细节及面部运动、镜面反射方面提升显著

Conclusion: HyperGaussians通过高维空间建模与计算优化，为AR/VR领域提供了更高效精细的动态人脸建模解决方案

Abstract: We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian primitives. 3DGS excels at rendering static
faces, but the state-of-the-art still struggles with nonlinear deformations,
complex lighting effects, and fine details. While most related works focus on
predicting better Gaussian parameters from expression codes, we rethink the 3D
Gaussian representation itself and how to make it more expressive. Our insights
lead to a novel extension of 3D Gaussians to high-dimensional multivariate
Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases
expressivity through conditioning on a learnable local embedding. However,
splatting HyperGaussians is computationally expensive because it requires
inverting a high-dimensional covariance matrix. We solve this by
reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'.
This trick boosts the efficiency so that HyperGaussians can be seamlessly
integrated into existing models. To demonstrate this, we plug in HyperGaussians
into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our
evaluation on 19 subjects from 4 face datasets shows that HyperGaussians
outperform 3DGS numerically and visually, particularly for high-frequency
details like eyeglass frames, teeth, complex facial movements, and specular
reflections.

</details>


### [46] [LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans](https://arxiv.org/abs/2507.02861)
*Zhening Huang,Xiaoyang Wu,Fangcheng Zhong,Hengshuang Zhao,Matthias Nießner,Joan Lasenby*

Main category: cs.CV

TL;DR: LiteReality提出通过场景理解、资产检索、材质优化和物理集成四阶段流程，将室内RGB-D扫描转换为可编辑的3D虚拟场景，兼容标准图形流程且支持物理交互。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以兼顾真实感渲染与物理交互功能，需构建兼容图形引擎且保留物体特性的数字孪生场景。

Method: 1. 场景理解生成结构化场景图
2. 相似度检索匹配3D资产库
3. 材质绘制模块恢复PBR材质
4. 集成物理引擎实现交互

Result: Scan2CAD基准检索SOTA，材质模块支持跨风格迁移，生成场景体积缩小80%+，兼容Unity/Unreal等引擎，适用于AR/VR、机器人仿真等领域。

Conclusion: 构建了首个支持物理交互的室内场景重建框架，通过无训练检索方案和鲁棒材质迁移技术，推动数字孪生在实际应用中的落地。

Abstract: We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor
environments into compact, realistic, and interactive 3D virtual replicas.
LiteReality not only reconstructs scenes that visually resemble reality but
also supports key features essential for graphics pipelines -- such as object
individuality, articulation, high-quality physically based rendering materials,
and physically based interaction. At its core, LiteReality first performs scene
understanding and parses the results into a coherent 3D layout and objects with
the help of a structured scene graph. It then reconstructs the scene by
retrieving the most visually similar 3D artist-crafted models from a curated
asset database. Next, the Material Painting module enhances realism by
recovering high-quality, spatially varying materials. Finally, the
reconstructed scene is integrated into a simulation engine with basic physical
properties to enable interactive behavior. The resulting scenes are compact,
editable, and fully compatible with standard graphics pipelines, making them
suitable for applications in AR/VR, gaming, robotics, and digital twins. In
addition, LiteReality introduces a training-free object retrieval module that
achieves state-of-the-art similarity performance on the Scan2CAD benchmark,
along with a robust material painting module capable of transferring
appearances from images of any style to 3D assets -- even under severe
misalignment, occlusion, and poor lighting. We demonstrate the effectiveness of
LiteReality on both real-life scans and public datasets. Project page:
https://litereality.github.io; Video:
https://www.youtube.com/watch?v=ecK9m3LXg2c

</details>


### [47] [ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.02200)
*Xiao Wang,Jingtao Jiang,Qiang Chen,Lan Chen,Lin Zhu,Yaowei Wang,Yonghong Tian,Jin Tang*

Main category: cs.CV

TL;DR: 提出了基于思维链推理的事件流场景文本识别框架ESTR-CoT，结合视觉编码器与预训练大语言模型，提升极端场景下的识别效果和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端框架和大语言模型在可解释性、上下文逻辑推理能力方面存在局限，特别是在低光照、高速运动等挑战性场景中。

Method: 1. 使用EVA-CLIP将事件流转换为视觉token
2. 通过Q-Former对齐视觉与Vicuna-7B语言模型
3. 同步输出答案和思维链推理过程
4. 采用三阶段流程（生成-润色-专家验证）构建大规模CoT训练数据集

Result: 在EventSTR/WordArt*/IC15*三个基准数据集上验证有效性，推理过程可视化展示框架可解释性。构建了目前最大的CoT数据集（覆盖500+字体/30万样本）

Conclusion: ESTR-CoT通过显式思维链推理突破了传统方法的性能瓶颈，为后续推理大模型发展提供数据基础和技术参考，代码模型即将开源。

Abstract: Event stream based scene text recognition is a newly arising research topic
in recent years which performs better than the widely used RGB cameras in
extremely challenging scenarios, especially the low illumination, fast motion.
Existing works either adopt end-to-end encoder-decoder framework or large
language models for enhanced recognition, however, they are still limited by
the challenges of insufficient interpretability and weak contextual logical
reasoning. In this work, we propose a novel chain-of-thought reasoning based
event stream scene text recognition framework, termed ESTR-CoT. Specifically,
we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input
event stream into tokens and utilize a Llama tokenizer to encode the given
generation prompt. A Q-former is used to align the vision token to the
pre-trained large language model Vicuna-7B and output both the answer and
chain-of-thought (CoT) reasoning process simultaneously. Our framework can be
optimized using supervised fine-tuning in an end-to-end manner. In addition, we
also propose a large-scale CoT dataset to train our framework via a three stage
processing (i.e., generation, polish, and expert verification). This dataset
provides a solid data foundation for the development of subsequent
reasoning-based large models. Extensive experiments on three event stream STR
benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the
effectiveness and interpretability of our proposed framework. The source code
and pre-trained models will be released on
https://github.com/Event-AHU/ESTR-CoT.

</details>


### [48] [SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers](https://arxiv.org/abs/2507.02212)
*Takuro Kawada,Shunsuke Kitada,Sota Nemoto,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 研究者开发了SciGA-145k大规模数据集（含14.5万论文/114万图表），支持图形摘要推荐与自动生成研究，提出Intra/Inter-GA推荐任务及CAR评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分挖掘图表作为图形摘要的潜力，且GA设计存在可视化技术门槛，阻碍科学传播效果提升。

Method: 定义两类任务：1）论文内GA推荐（Intra-GA） 2）跨论文GA推荐（Inter-GA），并建立基线模型。提出CAR指标解决传统排序指标的局限性。

Result: 构建统一任务框架与评估体系，CAR指标可精细化分析模型行为，考虑多潜在GA图存在的情况。数据集为视觉科学交流研究奠定基础。

Conclusion: SciGA-145k推动了可视化科学传播的发展，同时促进AI for Science领域研究，通过自动化GA生成降低科学交流门槛。

Abstract: Graphical Abstracts (GAs) play a crucial role in visually conveying the key
findings of scientific papers. While recent research has increasingly
incorporated visual materials such as Figure 1 as de facto GAs, their potential
to enhance scientific communication remains largely unexplored. Moreover,
designing effective GAs requires advanced visualization skills, creating a
barrier to their widespread adoption. To tackle these challenges, we introduce
SciGA-145k, a large-scale dataset comprising approximately 145,000 scientific
papers and 1.14 million figures, explicitly designed for supporting GA
selection and recommendation as well as facilitating research in automated GA
generation. As a preliminary step toward GA design support, we define two
tasks: 1) Intra-GA recommendation, which identifies figures within a given
paper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,
which retrieves GAs from other papers to inspire the creation of new GAs. We
provide reasonable baseline models for these tasks. Furthermore, we propose
Confidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation
metric that offers a fine-grained analysis of model behavior. CAR addresses
limitations in traditional ranking-based metrics by considering cases where
multiple figures within a paper, beyond the explicitly labeled GA, may also
serve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a
foundation for advancing visual scientific communication while contributing to
the development of AI for Science.

</details>


### [49] [From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding](https://arxiv.org/abs/2507.02790)
*Xiangfeng Wang,Xiao Li,Yadong Wei,Xueyu Song,Yang Song,Xiaoqiang Xia,Fangrui Zeng,Zaiyi Chen,Liu Liu,Gu Xu,Tong Xu*

Main category: cs.CV

TL;DR: 提出了HIVE框架，通过多模态叙事理解和结构化编辑子任务实现长视频到精品短片的自动剪辑


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法过度依赖ASR文本线索，忽视视觉叙事连贯性，导致剪辑质量低下。短视频平台发展急需能保持内容连贯性的自动化方案

Method: 1. 多模态大模型实现角色提取/对话分析/叙事总结
2. 场景级分割+三阶段编辑流程(高光检测/头尾选择/冗余修剪)
3. 构建DramaAD基准数据集(800+短剧/500+广告)

Result: 在常规剪辑和广告定向任务中全面超越基线模型，自动剪辑与人工剪辑质量差距缩小48%

Conclusion: 通过多模态叙事理解和结构化任务分解，有效解决了现有自动剪辑系统的叙事断裂问题，DramaAD数据集的建立推动了该领域研究

Abstract: The rapid growth of online video content, especially on short video
platforms, has created a growing demand for efficient video editing techniques
that can condense long-form videos into concise and engaging clips. Existing
automatic editing methods predominantly rely on textual cues from ASR
transcripts and end-to-end segment selection, often neglecting the rich visual
context and leading to incoherent outputs. In this paper, we propose a
human-inspired automatic video editing framework (HIVE) that leverages
multimodal narrative understanding to address these limitations. Our approach
incorporates character extraction, dialogue analysis, and narrative
summarization through multimodal large language models, enabling a holistic
understanding of the video content. To further enhance coherence, we apply
scene-level segmentation and decompose the editing process into three subtasks:
highlight detection, opening/ending selection, and pruning of irrelevant
content. To facilitate research in this area, we introduce DramaAD, a novel
benchmark dataset comprising over 800 short drama episodes and 500
professionally edited advertisement clips. Experimental results demonstrate
that our framework consistently outperforms existing baselines across both
general and advertisement-oriented editing tasks, significantly narrowing the
quality gap between automatic and human-edited videos.

</details>


### [50] [Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection](https://arxiv.org/abs/2507.02844)
*Ziqi Miao,Yi Ding,Lijun Li,Jing Shao*

Main category: cs.CV

TL;DR: 提出VisCo视觉中心越狱攻击方法，通过构建视觉上下文对话显著提升多模态大语言模型攻击成功率（MM-SafetyBench上毒性评分4.78，攻击成功率85%）。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模态攻击方法主要作为触发机制，存在语义模糊和缺乏现实场景基础的问题，需构建完整的视觉中心越狱场景。

Method: 开发四阶段视觉策略动态生成辅助图像，结合自动毒性混淆和语义优化技术，构建具有现实威胁的视觉上下文攻击场景。

Result: 在GPT-4o上实现毒性评分4.78（基线2.48），攻击成功率85%（基线22.2%），代码已开源。

Conclusion: VisCo攻击有效暴露MLLMs视觉模态安全漏洞，为实际部署中的安全防护提供重要参考。

Abstract: With the emergence of strong visual-language capabilities, multimodal large
language models (MLLMs) have demonstrated tremendous potential for real-world
applications. However, the security vulnerabilities exhibited by the visual
modality pose significant challenges to deploying such models in open-world
environments. Recent studies have successfully induced harmful responses from
target MLLMs by encoding harmful textual semantics directly into visual inputs.
However, in these approaches, the visual modality primarily serves as a trigger
for unsafe behavior, often exhibiting semantic ambiguity and lacking grounding
in realistic scenarios. In this work, we define a novel setting: visual-centric
jailbreak, where visual information serves as a necessary component in
constructing a complete and realistic jailbreak context. Building on this
setting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates
contextual dialogue using four distinct visual-focused strategies, dynamically
generating auxiliary images when necessary to construct a visual-centric
jailbreak scenario. To maximize attack effectiveness, it incorporates automatic
toxicity obfuscation and semantic refinement to produce a final attack prompt
that reliably triggers harmful responses from the target black-box MLLMs.
Specifically, VisCo achieves a toxicity score of 4.78 and an Attack Success
Rate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming
the baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The
code is available at https://github.com/Dtc7w3PQ/Visco-Attack.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [51] [DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with Self-Generated Cross-Modal Alignment](https://arxiv.org/abs/2507.02768)
*Ke-Han Lu,Zhehuai Chen,Szu-Wei Fu,Chao-Han Huck Yang,Sung-Feng Huang,Chih-Kai Yang,Chee-En Yu,Chun-Wei Chen,Wei-Chih Chen,Chien-yu Huang,Yi-Cheng Lin,Yu-Xiang Lin,Chi-An Fu,Chun-Yi Kuan,Wenze Ren,Xuanjun Chen,Wei-Ping Huang,En-Pei Hu,Tzu-Quan Lin,Yuan-Kuei Wu,Kuan-Po Huang,Hsiao-Ying Huang,Huang-Cheng Chou,Kai-Wei Chang,Cheng-Han Chiang,Boris Ginsburg,Yu-Chiang Frank Wang,Hung-yi Lee*

Main category: eess.AS

TL;DR: DeSTA2.5-Audio提出一种通用音频语言模型，通过自生成跨模态对齐策略DeSTA避免传统方法中LLM语言能力退化的问题，在零样本场景下实现强大的听觉感知和指令跟随能力。


<details>
  <summary>Details</summary>
Motivation: 现有LALM在增强LLM音频能力时依赖人工/合成音频指令数据集，导致LLM原始语言能力严重退化。DeSTA通过LLM自生成训练目标，旨在解决模态对齐与语言能力保留的矛盾。

Method: 1. 提出DeSTA策略：利用基座LLM自生成训练目标，构建跨模态对齐；2. 构建DeSTA-AQA5M数据集（500万样本/7000小时音频/50类数据）；3. 支持语音、环境音、音乐等多模态输入。

Result: 在Dynamic-SUPERB等5大基准测试中取得SOTA/竞争性表现，自生成策略在听觉感知和指令跟随能力上均优于传统方法。

Conclusion: 研究证明数据构造策略对LALM性能至关重要，为构建通用音频语言模型提供了新范式，同时揭示了基座模型原生能力在跨模态学习中的关键作用。

Abstract: We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model
(LALM) designed for robust auditory perception and instruction-following,
without requiring task-specific audio instruction-tuning. Recent LALMs
typically augment Large Language Models (LLMs) with auditory capabilities by
training on large-scale, manually curated or LLM-synthesized audio-instruction
datasets. However, these approaches have often suffered from the catastrophic
forgetting of the LLM's original language abilities. To address this, we
revisit the data construction pipeline and propose DeSTA, a self-generated
cross-modal alignment strategy in which the backbone LLM generates its own
training targets. This approach preserves the LLM's native language proficiency
while establishing effective audio-text alignment, thereby enabling zero-shot
generalization without task-specific tuning. Using DeSTA, we construct
DeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training
samples derived from 7,000 hours of audio spanning 50 diverse datasets,
including speech, environmental sounds, and music. DeSTA2.5-Audio achieves
state-of-the-art or competitive performance across a wide range of
audio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA,
Speech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate
that our self-generated strategy outperforms widely adopted data construction
and training strategies in both auditory perception and instruction-following
capabilities. Our findings underscore the importance of carefully designed data
construction in LALM development and offer practical insights for building
robust, general-purpose LALMs.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [52] [Early Signs of Steganographic Capabilities in Frontier LLMs](https://arxiv.org/abs/2507.02737)
*Artur Zolkowski,Kei Nishimura-Gasparian,Robert McCarthy,Roland S. Zimmermann,David Lindner*

Main category: cs.CR

TL;DR: 研究评估当前大型语言模型的隐写能力，发现其在标准监控下无法隐藏信息，但通过额外辅助手段可成功，并展示初步编码推理能力。


<details>
  <summary>Details</summary>
Motivation: 监控LLM输出对防范滥用风险至关重要，需评估其通过隐写术规避监控的潜在威胁。

Method: 测试前沿LLM两种隐写能力：传递编码信息与编码推理，考察标准/附加条件（未监控草稿纸）下的表现。

Result: 模型在附加条件下能完成短信息编码，具备基础编码推理能力（如十六进制），但难以隐蔽融入覆盖任务。

Conclusion: 当前LLM隐写能力尚处萌芽阶段，暂时无法突破专业监控，但需警惕未来技术演进带来的风险。

Abstract: Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks
from misuse and misalignment. However, LLMs could evade monitoring through
steganography: Encoding hidden information within seemingly benign generations.
In this paper, we evaluate the steganography capabilities in frontier LLMs to
better understand the risk they pose. We focus on two types of steganography:
passing encoded messages and performing encoded reasoning. We find that current
models are unable to encode short messages in their outputs without a monitor
noticing under standard affordances. They can succeed, however, if given
additional affordances such as using an unmonitored scratchpad and coordinating
on what encoding scheme to use. We additionally find early signs that models
can perform basic encoded reasoning in a simple state-tracking problem. This
includes some ability to reason with their own and pre-defined schemes,
including encoding schemes such as Hexadecimal. Despite this, they can rarely
hide reasoning subtly within a cover task to fool a monitor. Overall, our
results indicate that current LLMs exhibit nascent steganographic capabilities.
While these capabilities are likely insufficient to bypass well-designed
monitors at present, this could change in the future.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [53] [Seeing Through Green: Text-Based Classification and the Firm's Returns from Green Patents](https://arxiv.org/abs/2507.02287)
*Lapo Santarlasci,Armando Rungi,Antonio Zinilli*

Main category: econ.GN

TL;DR: 利用NLP技术从文献分类的绿色专利中筛选出20%真正绿色专利，并验证其对企业财务指标的积极影响


<details>
  <summary>Details</summary>
Motivation: 现有绿色专利分类存在较大误差，需要更精确的文本分析方法来支持环境政策制定和企业评估

Method: 1. 基于1240万专利训练神经网络构建环境技术词典
2. 使用双重差分法分析欧盟企业专利与财务数据关系

Result: 1. 真实绿色专利占比20%，引用率低1%
2. 持有真实绿色专利企业销售/市占/生产率提升
3. 高新颖性专利额外提升利润率

Conclusion: 文本分析能显著提升专利分类精度，为环境政策制定和企业绿色创新评估提供可靠依据

Abstract: This paper introduces Natural Language Processing for identifying ``true''
green patents from official supporting documents. We start our training on
about 12.4 million patents that had been classified as green from previous
literature. Thus, we train a simple neural network to enlarge a baseline
dictionary through vector representations of expressions related to
environmental technologies. After testing, we find that ``true'' green patents
represent about 20\% of the total of patents classified as green from previous
literature. We show heterogeneity by technological classes, and then check that
`true' green patents are about 1\% less cited by following inventions. In the
second part of the paper, we test the relationship between patenting and a
dashboard of firm-level financial accounts in the European Union. After
controlling for reverse causality, we show that holding at least one ``true''
green patent raises sales, market shares, and productivity. If we restrict the
analysis to high-novelty ``true'' green patents, we find that they also yield
higher profits. Our findings underscore the importance of using text analyses
to gauge finer-grained patent classifications that are useful for policymaking
in different domains.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [54] [Why Multi-Interest Fairness Matters: Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System](https://arxiv.org/abs/2507.02000)
*Yongsen Zheng,Zongxuan Xie,Guohua Wang,Ziyao Liu,Liang Lin,Kwok-Yan Lam*

Main category: cs.IR

TL;DR: 提出HyFairCRS框架，通过超图对比多兴趣学习提升对话推荐系统的多兴趣多样性公平性，实验验证其有效性和先进性。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统公平性方法在动态交互场景中效果有限，不公平性随时间加剧导致马太效应等问题，需针对动态对话推荐场景设计解决方案。

Method: 采用超图对比学习构建多样化兴趣图谱，结合动态对话反馈机制实现多兴趣表征学习和公平项目推荐。

Result: 在两个CRS数据集上达到SOTA性能，同时有效缓解推荐不公平现象。

Conclusion: 通过多兴趣表征学习和动态反馈机制的结合，HyFairCRS在提升推荐效果的同时实现了公平性突破，为动态推荐系统的公平性研究提供了新范式。

Abstract: Unfairness is a well-known challenge in Recommender Systems (RSs), often
resulting in biased outcomes that disadvantage users or items based on
attributes such as gender, race, age, or popularity. Although some approaches
have started to improve fairness recommendation in offline or static contexts,
the issue of unfairness often exacerbates over time, leading to significant
problems like the Matthew effect, filter bubbles, and echo chambers. To address
these challenges, we proposed a novel framework, Hypergraph Contrastive
Multi-Interest Learning for Fair Conversational Recommender System (HyFairCRS),
aiming to promote multi-interest diversity fairness in dynamic and interactive
Conversational Recommender Systems (CRSs). HyFairCRS first captures a wide
range of user interests by establishing diverse hypergraphs through contrastive
learning. These interests are then utilized in conversations to generate
informative responses and ensure fair item predictions within the dynamic
user-system feedback loop. Experiments on two CRS-based datasets show that
HyFairCRS achieves a new state-of-the-art performance while effectively
alleviating unfairness. Our code is available at
https://github.com/zysensmile/HyFairCRS.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [55] [Dissecting the Impact of Mobile DVFS Governors on LLM Inference Performance and Energy Efficiency](https://arxiv.org/abs/2507.02135)
*Zongpu Zhang,Pranab Dash,Y. Charlie Hu,Qiang Xu,Jian Li,Haibing Guan*

Main category: cs.OS

TL;DR: 论文提出FUSE方案优化移动端大语言模型能效，通过统一管理CPU/GPU/内存频率降低延迟


<details>
  <summary>Details</summary>
Motivation: 现有移动设备运行LLM时CPU/GPU/内存的DVFS调节器独立工作，导致能效低下（相同能耗下延迟增加40.4%）

Method: 1. 测量现有LLM框架能效 2. 分析组件间协同问题 3. 设计统一能源感知调节器FUSE

Result: FUSE使首令牌延迟降低7.0%-16.9%，输出令牌延迟降低25.4%-36.8%（相同能耗下）

Conclusion: FUSE通过协同管理移动设备硬件资源，显著提升LLM推理的能源效率，为移动端部署提供有效解决方案

Abstract: Large Language Models (LLMs) are increasingly being integrated into various
applications and services running on billions of mobile devices. However,
deploying LLMs on resource-limited mobile devices faces a significant challenge
due to their high demand for computation, memory, and ultimately energy. While
current LLM frameworks for mobile use three power-hungry components-CPU, GPU,
and Memory-even when running primarily-GPU LLM models, optimized DVFS governors
for CPU, GPU, and memory featured in modern mobile devices operate
independently and are oblivious of each other. Motivated by the above
observation, in this work, we first measure the energy-efficiency of a SOTA LLM
framework consisting of various LLM models on mobile phones which showed the
triplet mobile governors result in up to 40.4% longer prefilling and decoding
latency compared to optimal combinations of CPU, GPU, and memory frequencies
with the same energy consumption for sampled prefill and decode lengths.
Second, we conduct an in-depth measurement study to uncover how the intricate
interplay (or lack of) among the mobile governors cause the above inefficiency
in LLM inference. Finally, based on these insights, we design FUSE - a unified
energy-aware governor for optimizing the energy efficiency of LLM inference on
mobile devices. Our evaluation using a ShareGPT dataset shows FUSE reduces the
time-to-first-token and time-per-output-token latencies by 7.0%-16.9% and
25.4%-36.8% on average with the same energy-per-token for various mobile LLM
models.

</details>
