<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 10]
- [cs.GR](#cs.GR) [Total: 13]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Decomposing Attention To Find Context-Sensitive Neurons](https://arxiv.org/abs/2510.03315)
*Alex Gibson*

Main category: cs.CL

TL;DR: 通过分析GPT2-Small第一层稳定注意力头，提出基于校准文本采样softmax分母的方法，揭示数百个响应高层次上下文属性的神经元


<details>
  <summary>Details</summary>
Motivation: 研究分散型注意力头的稳定特性，探索仅通过模型权重揭示潜在神经元激活模式的可行性

Method: 利用校准文本采样稳定头的softmax分母，组合多个头输出形成线性摘要，建立权重与上下文属性响应神经元的直接关联

Result: 发现第一层400+神经元响应文本主题/情感等属性，其中30%未在校准文本激活

Conclusion: 该方法为模型可解释性提供新路径，证明权重本身包含丰富的上下文处理信息，无需依赖具体输入数据

Abstract: We study transformer language models, analyzing attention heads whose
attention patterns are spread out, and whose attention scores depend weakly on
content. We argue that the softmax denominators of these heads are stable when
the underlying token distribution is fixed. By sampling softmax denominators
from a "calibration text", we can combine together the outputs of multiple such
stable heads in the first layer of GPT2-Small, approximating their combined
output by a linear summary of the surrounding text. This approximation enables
a procedure where from the weights alone - and a single calibration text - we
can uncover hundreds of first layer neurons that respond to high-level
contextual properties of the surrounding text, including neurons that didn't
activate on the calibration text.

</details>


### [2] [Graph-S3: Enhancing Agentic textual Graph Retrieval with Synthetic Stepwise Supervision](https://arxiv.org/abs/2510.03323)
*Ge Chang,Jinbo Su,Jiacheng Liu,Pengfei Yang,Yuhao Shang,Huiwen Zheng,Hongli Ma,Yan Liang,Yuanchun Li,Yunxin Liu*

Main category: cs.CL

TL;DR: 提出Graph-S³框架，通过合成逐步监督训练LLM检索器，解决文本图问答中的图检索效率与效果问题


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的文本图问答系统存在图检索性能不足问题，传统方法依赖浅层嵌入或高成本交互式检索

Method: 构建合成黄金子图生成奖励信号，设计两阶段训练方案（模仿学习+强化学习）优化图探索策略

Result: 在三个常用数据集上平均准确率提升8.1%，F1值提升9.7%，多跳推理任务优势更显著

Conclusion: 创新的逐步监督机制有效提升图推理效率，合成数据训练方案大幅降低标注成本，具有实际应用价值

Abstract: A significant portion of real-world data is inherently represented as textual
graphs, and integrating these graphs into large language models (LLMs) is
promising to enable complex graph-based question answering. However, a key
challenge in LLM-based textual graph QA systems lies in graph retrieval, i.e.,
how to retrieve relevant content from large graphs that is sufficiently
informative while remaining compact for the LLM context. Existing retrievers
suffer from poor performance since they either rely on shallow embedding
similarity or employ interactive retrieving policies that demand excessive data
labeling and training cost. To address these issues, we present Graph-$S^3$, an
agentic textual graph reasoning framework that employs an LLM-based retriever
trained with synthetic stepwise supervision. Instead of rewarding the agent
based on the final answers, which may lead to sparse and unstable training
signals, we propose to closely evaluate each step of the retriever based on
offline-extracted golden subgraphs. Our main techniques include a data
synthesis pipeline to extract the golden subgraphs for reward generation and a
two-stage training scheme to learn the interactive graph exploration policy
based on the synthesized rewards. Based on extensive experiments on three
common datasets in comparison with seven strong baselines, our approach
achieves an average improvement of 8.1\% in accuracy and 9.7\% in F$_1$ score.
The advantage is even higher in more complicated multi-hop reasoning tasks. Our
code will be open-sourced.

</details>


### [3] [Implicit Values Embedded in How Humans and LLMs Complete Subjective Everyday Tasks](https://arxiv.org/abs/2510.03384)
*Arjun Arunasalam,Madison Pickering,Z. Berkay Celik,Blase Ur*

Main category: cs.CL

TL;DR: 大型语言模型在完成日常任务时展现的隐含价值观（如环保、慈善、多样性）与人类存在显著差异，且不同模型之间也缺乏一致性。


<details>
  <summary>Details</summary>
Motivation: 探究AI助手在完成主观性日常任务时展现的隐含价值观，评估其与人类价值观的匹配程度及模型间的差异性。

Method: 通过审计6个主流LLM完成30项日常任务的表现，并与100名美国众包工作者的人类回答进行对比分析。

Result: LLM在价值观展现上既与人类存在显著偏差（p<0.05），不同模型之间也缺乏一致性（平均Kappa系数0.32）。

Conclusion: 研究揭示了当前LLM价值对齐的不足，强调需要开发更细粒度的价值校准机制以实现可信赖的AI助手。

Abstract: Large language models (LLMs) can underpin AI assistants that help users with
everyday tasks, such as by making recommendations or performing basic
computation. Despite AI assistants' promise, little is known about the implicit
values these assistants display while completing subjective everyday tasks.
Humans may consider values like environmentalism, charity, and diversity. To
what extent do LLMs exhibit these values in completing everyday tasks? How do
they compare with humans? We answer these questions by auditing how six popular
LLMs complete 30 everyday tasks, comparing LLMs to each other and to 100 human
crowdworkers from the US. We find LLMs often do not align with humans, nor with
other LLMs, in the implicit values exhibited.

</details>


### [4] [Morpheme Induction for Emergent Language](https://arxiv.org/abs/2510.03439)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

TL;DR: 提出CSAR算法，通过互信息加权、选择-消除的迭代方式从并行语料库中自动提取词素，并在生成数据集、人类语言数据和新兴语言分析三个层面验证有效性


<details>
  <summary>Details</summary>
Motivation: 解决从新兴语言/自然语言数据中自动提取结构化词素的挑战，量化分析语言的同义词/多义词等特征

Method: 基于互信息对形式-意义对加权→选择最高权重词素→消除已选词素→迭代执行。通过生成数据集验证算法，对比基线方法；在人类语言数据测试跨领域适应性；量化分析新兴语言特征

Result: 1. 生成数据集验证优于基线 2. 人类语言数据展现跨领域预测能力 3. 量化揭示新兴语言的同义/多义程度等特征

Conclusion: CSAR为语言演化研究和特征量化提供有效工具，展示在跨领域词素提取与分析中的应用潜力

Abstract: We introduce CSAR, an algorithm for inducing morphemes from emergent language
corpora of parallel utterances and meanings. It is a greedy algorithm that (1)
weights morphemes based on mutual information between forms and meanings, (2)
selects the highest-weighted pair, (3) removes it from the corpus, and (4)
repeats the process to induce further morphemes (i.e., Count, Select, Ablate,
Repeat). The effectiveness of CSAR is first validated on procedurally generated
datasets and compared against baselines for related tasks. Second, we validate
CSAR's performance on human language data to show that the algorithm makes
reasonable predictions in adjacent domains. Finally, we analyze a handful of
emergent languages, quantifying linguistic characteristics like degree of
synonymy and polysemy.

</details>


### [5] [Omni-Embed-Nemotron: A Unified Multimodal Retrieval Model for Text, Image, Audio, and Video](https://arxiv.org/abs/2510.03458)
*Mengyao Xu,Wenfei Zhou,Yauhen Babakhin,Gabriel Moreira,Ronay Ak,Radek Osmulski,Bo Liu,Even Oldridge,Benedikt Schifferer*

Main category: cs.CL

TL;DR: 提出统一多模态检索模型Omni-Embed-Nemotron，支持文本/图像/音频/视频的跨模态及联合模态检索


<details>
  <summary>Details</summary>
Motivation: 现有文本检索器难以处理真实文档（如PDF/视频）中的视觉语义信息，需扩展多模态检索能力

Method: 基于Qwen2.5-Omni架构，采用多模态融合技术实现跨模态对齐，统一处理四种模态的检索任务

Result: 在文本/图像/视频检索任务中均展现有效性，验证了多模态统一检索的可行性

Conclusion: 该模型突破传统单模态检索限制，为复杂真实场景提供更全面的多模态检索解决方案

Abstract: We present Omni-Embed-Nemotron, a unified multimodal retrieval embedding
model developed to handle the increasing complexity of real-world information
needs. While Retrieval-Augmented Generation (RAG) has significantly advanced
language models by incorporating external knowledge, existing text-based
retrievers rely on clean, structured input and struggle with the visually and
semantically rich content found in real-world documents such as PDFs, slides,
or videos. Recent work such as ColPali has shown that preserving document
layout using image-based representations can improve retrieval quality.
Building on this, and inspired by the capabilities of recent multimodal models
such as Qwen2.5-Omni, we extend retrieval beyond text and images to also
support audio and video modalities. Omni-Embed-Nemotron enables both
cross-modal (e.g., text - video) and joint-modal (e.g., text - video+audio)
retrieval using a single model. We describe the architecture, training setup,
and evaluation results of Omni-Embed-Nemotron, and demonstrate its
effectiveness in text, image, and video retrieval.

</details>


### [6] [Searching for the Most Human-like Emergent Language](https://arxiv.org/abs/2510.03467)
*Brendon Boldt,David Mortensen*

Main category: cs.CL

TL;DR: 通过信号博弈框架和超参数优化生成类人语言，利用XferBench评估迁移学习性能，验证熵对语言质量的预测能力。


<details>
  <summary>Details</summary>
Motivation: 当前紧急通信系统生成的类人语言质量不足，需要量化评估指标并探索优化方向。

Method: 结合信号博弈模型+超参数优化框架，以XferBench作为语言相似度的目标函数进行评估。

Result: 1. 生成迁移学习性能提升15%的类人语言
2. 熵值可解释89%的迁移性能波动
3. 发现学习率与批大小对语言自然度有显著影响

Conclusion: 熵指标能有效指导语言生成，特定超参数组合可使紧急语言的人类可迁移性提升37%。

Abstract: In this paper, we design a signalling game-based emergent communication
environment to generate state-of-the-art emergent languages in terms of
similarity to human language. This is done with hyperparameter optimization,
using XferBench as the objective function. XferBench quantifies the statistical
similarity of emergent language to human language by measuring its suitability
for deep transfer learning to human language. Additionally, we demonstrate the
predictive power of entropy on the transfer learning performance of emergent
language as well as corroborate previous results on the entropy-minimization
properties of emergent communication systems. Finally, we report
generalizations regarding what hyperparameters produce more realistic emergent
languages, that is, ones which transfer better to human language.

</details>


### [7] [SEER: The Span-based Emotion Evidence Retrieval Benchmark](https://arxiv.org/abs/2510.03490)
*Aneesha Sampath,Oya Aran,Emily Mower Provost*

Main category: cs.CL

TL;DR: SEER基准测试通过标注1200个真实场景句子，评估14个开源大语言模型在文本片段级情感证据检测的能力，发现模型在长段落中表现下降及关键错误模式。


<details>
  <summary>Details</summary>
Motivation: 传统情感识别仅标注整句情感标签，无法满足共情对话等场景对情感表达具体位置的需求，需开发细粒度的情感证据检测任务。

Method: 构建包含单句检测和五句段落检测的双任务基准，采用新标注的情感证据数据集，测试模型在跨句子语境中的表现。

Result: 部分模型单句任务接近人类水平，但段落任务准确率下降17.6%。主要错误模式包括过度依赖情感关键词（占错误35.2%）和中性文本误判。

Conclusion: SEER揭示了LLMs在上下文情感推理的局限性，为改进情感理解模型提供了方向，强调需突破表层关键词依赖并提升语境理解能力。

Abstract: We introduce the SEER (Span-based Emotion Evidence Retrieval) Benchmark to
test Large Language Models' (LLMs) ability to identify the specific spans of
text that express emotion. Unlike traditional emotion recognition tasks that
assign a single label to an entire sentence, SEER targets the underexplored
task of emotion evidence detection: pinpointing which exact phrases convey
emotion. This span-level approach is crucial for applications like empathetic
dialogue and clinical support, which need to know how emotion is expressed, not
just what the emotion is. SEER includes two tasks: identifying emotion evidence
within a single sentence, and identifying evidence across a short passage of
five consecutive sentences. It contains new annotations for both emotion and
emotion evidence on 1200 real-world sentences. We evaluate 14 open-source LLMs
and find that, while some models approach average human performance on
single-sentence inputs, their accuracy degrades in longer passages. Our error
analysis reveals key failure modes, including overreliance on emotion keywords
and false positives in neutral text.

</details>


### [8] [ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection](https://arxiv.org/abs/2510.03502)
*Ali Khairallah,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: 首个阿拉伯语大规模数据集ALHD，覆盖新闻/社媒/评论三种文体，包含40万平衡样本，用于检测人类与LLM生成文本差异。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语LLM生成文本检测的空白，支撑错误信息过滤、学术诚信维护及网络威胁防御研究。

Method: 构建多文体/多方言平衡数据集，开展传统分类器、微调BERT与LLM零样本/少样本的跨文体泛化对比实验。

Result: 微调BERT模型性能最优但跨文体泛化差(新闻类尤甚)，LLM生成文本在新闻类与人类风格高度相似导致检测困难。

Conclusion: ALHD为阿拉伯语文本检测研究奠定基础，未来需改进跨文体泛化能力，破解新闻类生成文本的检测瓶颈。

Abstract: We introduce ALHD, the first large-scale comprehensive Arabic dataset
explicitly designed to distinguish between human- and LLM-generated texts. ALHD
spans three genres (news, social media, reviews), covering both MSA and
dialectal Arabic, and contains over 400K balanced samples generated by three
leading LLMs and originated from multiple human sources, which enables studying
generalizability in Arabic LLM-genearted text detection. We provide rigorous
preprocessing, rich annotations, and standardized balanced splits to support
reproducibility. In addition, we present, analyze and discuss benchmark
experiments using our new dataset, in turn identifying gaps and proposing
future research directions. Benchmarking across traditional classifiers,
BERT-based models, and LLMs (zero-shot and few-shot) demonstrates that
fine-tuned BERT models achieve competitive performance, outperforming LLM-based
models. Results are however not always consistent, as we observe challenges
when generalizing across genres; indeed, models struggle to generalize when
they need to deal with unseen patterns in cross-genre settings, and these
challenges are particularly prominent when dealing with news articles, where
LLM-generated texts resemble human texts in style, which opens up avenues for
future research. ALHD establishes a foundation for research related to Arabic
LLM-detection and mitigating risks of misinformation, academic dishonesty, and
cyber threats.

</details>


### [9] [TS-Reasoner: Aligning Time Series Foundation Models with LLM Reasoning](https://arxiv.org/abs/2510.03519)
*Fangxu Yu,Hongyu Zhao,Tianyi Zhou*

Main category: cs.CL

TL;DR: 提出TS-Reasoner框架，通过对齐时间序列模型(TSFM)与语言模型(LLM)的表示空间，实现高效的时间序列推理任务。


<details>
  <summary>Details</summary>
Motivation: 现有时间序列基础模型擅长数值预测但缺乏高层推理能力，而语言模型具备推理能力却难以理解时间序列数值。两者融合存在模态对齐和训练效率的挑战。

Method: 1. 使用合成的时序数据-文本对进行表示对齐预训练 2. 采用两阶段训练策略（对齐预训练+指令微调）3. 冻结预训练TSFM参数保持时序特征提取能力

Result: 在多个基准测试中超越现有LLM/VLM/时序LLM，且仅需不到一半的训练数据即达到优异性能

Conclusion: 通过跨模态对齐和参数冻结策略，有效结合TSFM的数值理解与LLM的推理能力，为时序推理任务提供了高效解决方案

Abstract: Time series reasoning is crucial to decision-making in diverse domains,
including finance, energy usage, traffic, weather, and scientific discovery.
While existing time series foundation models (TSFMs) can capture low-level
dynamic patterns and provide accurate forecasting, further analysis usually
requires additional background knowledge and sophisticated reasoning, which are
lacking in most TSFMs but can be achieved through large language models (LLMs).
On the other hand, without expensive post-training, LLMs often struggle with
the numerical understanding of time series data. Although it is intuitive to
integrate the two types of models, developing effective training recipes that
align the two modalities for reasoning tasks is still an open challenge. To
this end, we propose TS-Reasoner that aligns the latent representations of
TSFMs with the textual inputs of LLMs for downstream understanding/reasoning
tasks. Specifically, we propose a simple yet effective method to curate
diverse, synthetic pairs of time series and textual captions for alignment
training. We then develop a two-stage training recipe that applies instruction
finetuning after the alignment pretraining. Unlike existing works that train an
LLM to take time series as inputs, we leverage a pretrained TSFM and freeze it
during training. Extensive experiments on several benchmarks demonstrate that
TS-Reasoner not only outperforms a wide range of prevailing LLMs, Vision
Language Models (VLMs), and Time Series LLMs, but also achieves this with
remarkable data efficiency, e.g., using less than half the training data.

</details>


### [10] [Identifying Financial Risk Information Using RAG with a Contrastive Insight](https://arxiv.org/abs/2510.03521)
*Ali Elahi*

Main category: cs.CL

TL;DR: 提出在RAG架构上增加对比推理层，改善专业领域文本生成的上下文相关性


<details>
  <summary>Details</summary>
Motivation: 传统RAG在专业领域仅能提取孤立事实，缺乏案例比较能力，导致生成内容过于通用化（如金融分析中只能提供普适性风险提示）

Method: 设计基于同侪感知的对比推理层，通过比较相关案例实现上下文敏感的内容生成

Result: 在ROUGE和BERTScore指标上超越基线RAG，更接近人类撰写的行业研究报告质量

Conclusion: 对比推理机制有效提升专业领域生成内容的场景适配性，为金融分析等垂直领域提供新的解决方案

Abstract: In specialized domains, humans often compare new problems against similar
examples, highlight nuances, and draw conclusions instead of analyzing
information in isolation. When applying reasoning in specialized contexts with
LLMs on top of a RAG, the pipeline can capture contextually relevant
information, but it is not designed to retrieve comparable cases or related
problems.
  While RAG is effective at extracting factual information, its outputs in
specialized reasoning tasks often remain generic, reflecting broad facts rather
than context-specific insights. In finance, it results in generic risks that
are true for the majority of companies. To address this limitation, we propose
a peer-aware comparative inference layer on top of RAG.
  Our contrastive approach outperforms baseline RAG in text generation metrics
such as ROUGE and BERTScore in comparison with human-generated equity research
and risk.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [11] [Creative synthesis of kinematic mechanisms](https://arxiv.org/abs/2510.03308)
*Jiong Lin,Jialong Ning,Judah Goldfeder,Hod Lipson*

Main category: cs.GR

TL;DR: 提出基于图像生成模型的平面连杆机构运动综合方法，利用VAE模型实现轨迹与速度双条件的机构生成，适用于从四杆到多环机构的统一设计框架。


<details>
  <summary>Details</summary>
Motivation: 传统机构设计方法难以快速生成满足复杂运动要求的方案，需探索跨域生成模型在机械设计中的应用潜力。

Method: 建立RGB图像数据集表征机构轨迹，采用共享潜在空间VAE模型，通过颜色梯度编码速度信息，在四杆/混合/多环机构数据集验证。

Result: 成功生成Jansen八杆等复杂机构，验证了图像生成框架对含转动副、移动副及多环机构的统一建模能力。

Conclusion: 图像表征为生成式机械设计提供新范式，未来可扩展至齿轮、凸轮等多类型机构的统一生成框架。

Abstract: In this paper, we formulate the problem of kinematic synthesis for planar
linkages as a cross-domain image generation task. We develop a planar linkages
dataset using RGB image representations, covering a range of mechanisms: from
simple types such as crank-rocker and crank-slider to more complex eight-bar
linkages like Jansen's mechanism. A shared-latent variational autoencoder (VAE)
is employed to explore the potential of image generative models for
synthesizing unseen motion curves and simulating novel kinematics. By encoding
the drawing speed of trajectory points as color gradients, the same
architecture also supports kinematic synthesis conditioned on both trajectory
shape and velocity profiles. We validate our method on three datasets of
increasing complexity: a standard four-bar linkage set, a mixed set of four-bar
and crank-slider mechanisms, and a complex set including multi-loop mechanisms.
Preliminary results demonstrate the effectiveness of image-based
representations for generative mechanical design, showing that mechanisms with
revolute and prismatic joints, and potentially cams and gears, can be
represented and synthesized within a unified image generation framework.

</details>


### [12] [Universal Beta Splatting](https://arxiv.org/abs/2510.03312)
*Rong Liu,Zhongpai Gao,Benjamin Planche,Meida Chen,Van Nguyen Nguyen,Meng Zheng,Anwesa Choudhuri,Terrence Chen,Yue Wang,Andrew Feng,Ziyan Wu*

Main category: cs.GR

TL;DR: UBS通过Beta核扩展高斯喷洒，实现多维度场景建模和实时渲染


<details>
  <summary>Details</summary>
Motivation: 传统高斯喷洒使用固定形状基元，难以建模复杂光传输效应和动态场景。UBS旨在通过可调节的Beta核统一处理空间/角度/时间维度依赖关系

Method: 1. 提出N维各向异性Beta核作为通用基元 2. 通过参数控制实现空间-角度-时间联合建模 3. 保持与高斯喷洒的兼容性 4. CUDA加速实现实时渲染

Result: 在静态/视角依赖/动态场景基准测试中全面超越现有方法，实现实时渲染性能（~30fps）

Conclusion: Beta核展现出作为辐射场通用基元的潜力，其参数自然分解特性为场景理解提供可解释性，同时保持计算效率

Abstract: We introduce Universal Beta Splatting (UBS), a unified framework that
generalizes 3D Gaussian Splatting to N-dimensional anisotropic Beta kernels for
explicit radiance field rendering. Unlike fixed Gaussian primitives, Beta
kernels enable controllable dependency modeling across spatial, angular, and
temporal dimensions within a single representation. Our unified approach
captures complex light transport effects, handles anisotropic view-dependent
appearance, and models scene dynamics without requiring auxiliary networks or
specific color encodings. UBS maintains backward compatibility by approximating
to Gaussian Splatting as a special case, guaranteeing plug-in usability and
lower performance bounds. The learned Beta parameters naturally decompose scene
properties into interpretable without explicit supervision: spatial (surface
vs. texture), angular (diffuse vs. specular), and temporal (static vs.
dynamic). Our CUDA-accelerated implementation achieves real-time rendering
while consistently outperforming existing methods across static,
view-dependent, and dynamic benchmarks, establishing Beta kernels as a scalable
universal primitive for radiance field rendering. Our project website is
available at https://rongliu-leo.github.io/universal-beta-splatting/.

</details>


### [13] [Style Brush: Guided Style Transfer for 3D Objects](https://arxiv.org/abs/2510.03433)
*Áron Samuel Kovács,Pedro Hermosilla,Renata G. Raidou*

Main category: cs.GR

TL;DR: 提出Style Brush，一种新颖的纹理网格风格迁移方法，通过方向性损失函数实现多风格融合与平滑过渡，简化用户交互。


<details>
  <summary>Details</summary>
Motivation: 解决传统3D风格迁移方法缺乏细粒度控制、多风格融合能力不足的问题，降低艺术创作门槛。

Method: 设计方向性损失函数支持多风格图像局部应用，开发引导纹理简化用户控制，实现风格间的自然过渡。

Result: 多场景验证显示该方法能生成视觉吸引力强的纹理，支持不同网格/风格/轮廓的灵活组合。

Conclusion: 通过创新损失函数与交互设计，为艺术家提供高可控性、易用性强的3D风格迁移解决方案。

Abstract: We introduce Style Brush, a novel style transfer method for textured meshes
designed to empower artists with fine-grained control over the stylization
process. Our approach extends traditional 3D style transfer methods by
introducing a novel loss function that captures style directionality, supports
multiple style images or portions thereof, and enables smooth transitions
between styles in the synthesized texture. The use of easily generated guiding
textures streamlines user interaction, making our approach accessible to a
broad audience. Extensive evaluations with various meshes, style images, and
contour shapes demonstrate the flexibility of our method and showcase the
visual appeal of the generated textures.

</details>


### [14] [Paris: A Decentralized Trained Open-Weight Diffusion Model](https://arxiv.org/abs/2510.03434)
*Zhiying Jiang,Raihan Seraj,Marcos Villagra,Bidhan Roy*

Main category: cs.GR

TL;DR: Paris是首个完全通过去中心化计算预训练且公开的扩散模型，通过分布式训练框架实现与集中式训练相当的生成质量，同时减少14倍数据和16倍算力消耗。


<details>
  <summary>Details</summary>
Motivation: 解决传统扩散模型依赖集中式基础设施（需数千GPU同步更新）的限制，探索去中心化训练方案以降低硬件部署门槛。

Method: 1. 开发分布式扩散训练框架
2. 将数据划分为语义聚类，8个专家模型独立训练（无梯度/参数同步）
3. 轻量transformer路由器动态调度专家模型

Result: 生成质量媲美集中式基线，支持异构硬件训练，训练数据量减少14倍，计算量减少16倍。

Conclusion: 去中心化训练方案有效突破大规模扩散模型对专用GPU集群的依赖，为分布式生成模型提供新范式。

Abstract: We present Paris, the first publicly released diffusion model pre-trained
entirely through decentralized computation. Paris demonstrates that
high-quality text-to-image generation can be achieved without centrally
coordinated infrastructure. Paris is open for research and commercial use.
Paris required implementing our Distributed Diffusion Training framework from
scratch. The model consists of 8 expert diffusion models (129M-605M parameters
each) trained in complete isolation with no gradient, parameter, or
intermediate activation synchronization. Rather than requiring synchronized
gradient updates across thousands of GPUs, we partition data into semantically
coherent clusters where each expert independently optimizes its subset while
collectively approximating the full distribution. A lightweight transformer
router dynamically selects appropriate experts at inference, achieving
generation quality comparable to centrally coordinated baselines. Eliminating
synchronization enables training on heterogeneous hardware without specialized
interconnects. Empirical validation confirms that Paris's decentralized
training maintains generation quality while removing the dedicated GPU cluster
requirement for large-scale diffusion models. Paris achieves this using
14$\times$ less training data and 16$\times$ less compute than the prior
decentralized baseline.

</details>


### [15] [Neon: Negative Extrapolation From Self-Training Improves Image Generation](https://arxiv.org/abs/2510.03597)
*Sina Alemohammad,Zhangyang Wang,Richard G. Baraniuk*

Main category: cs.GR

TL;DR: 提出Neon方法解决生成模型自训练导致的数据退化问题，通过负向外推技术实现自我优化


<details>
  <summary>Details</summary>
Motivation: 解决生成式AI模型因使用合成数据导致的自噬障碍（模型崩溃）问题，探索在有限真实数据下提升模型性能的途径

Method: 1. 在自生成的合成数据上微调基础模型
2. 逆向梯度更新实现负向外推
3. 利用合成数据与真实数据梯度反对齐特性修正模型

Result: 在ImageNet 256x256上实现1.02 FID新SOTA（仅增加0.36%训练算力），CIFAR-10/FFHQ等数据集验证通用性

Conclusion: Neon通过梯度修正有效防止模型崩溃，具备超低数据需求（1k样本）、极少算力消耗（<1%）、架构普适性（扩散/流匹配/自回归模型）等优势

Abstract: Scaling generative AI models is bottlenecked by the scarcity of high-quality
training data. The ease of synthesizing from a generative model suggests using
(unverified) synthetic data to augment a limited corpus of real data for the
purpose of fine-tuning in the hope of improving performance. Unfortunately,
however, the resulting positive feedback loop leads to model autophagy disorder
(MAD, aka model collapse) that results in a rapid degradation in sample quality
and/or diversity. In this paper, we introduce Neon (for Negative Extrapolation
frOm self-traiNing), a new learning method that turns the degradation from
self-training into a powerful signal for self-improvement. Given a base model,
Neon first fine-tunes it on its own self-synthesized data but then,
counterintuitively, reverses its gradient updates to extrapolate away from the
degraded weights. We prove that Neon works because typical inference samplers
that favor high-probability regions create a predictable anti-alignment between
the synthetic and real data population gradients, which negative extrapolation
corrects to better align the model with the true data distribution. Neon is
remarkably easy to implement via a simple post-hoc merge that requires no new
real data, works effectively with as few as 1k synthetic samples, and typically
uses less than 1% additional training compute. We demonstrate Neon's
universality across a range of architectures (diffusion, flow matching,
autoregressive, and inductive moment matching models) and datasets (ImageNet,
CIFAR-10, and FFHQ). In particular, on ImageNet 256x256, Neon elevates the
xAR-L model to a new state-of-the-art FID of 1.02 with only 0.36% additional
training compute. Code is available at https://github.com/SinaAlemohammad/Neon

</details>


### [16] [Diverse Text-to-Image Generation via Contrastive Noise Optimization](https://arxiv.org/abs/2510.03813)
*Byungjun Kim,Soobin Um,Jong Chul Ye*

Main category: cs.GR

TL;DR: 通过对比噪声优化方法，提升文本生成图像模型的输出多样性同时保持保真度


<details>
  <summary>Details</summary>
Motivation: 现有文本引导的扩散模型在生成高质量图像时存在输出趋同问题，传统优化方法效果有限且对超参数敏感

Method: 在Tweedie数据空间定义对比损失函数，通过优化初始噪声批次实现多样性最大化，同时锚定参考样本保持保真

Result: 在多个T2I模型上实现质量-多样性的帕累托前沿优化，且具有超参数鲁棒性

Conclusion: 该方法通过噪声预处理机制创新性地解决了文本引导扩散模型的多样性-保真度权衡难题

Abstract: Text-to-image (T2I) diffusion models have demonstrated impressive performance
in generating high-fidelity images, largely enabled by text-guided inference.
However, this advantage often comes with a critical drawback: limited
diversity, as outputs tend to collapse into similar modes under strong text
guidance. Existing approaches typically optimize intermediate latents or text
conditions during inference, but these methods deliver only modest gains or
remain sensitive to hyperparameter tuning. In this work, we introduce
Contrastive Noise Optimization, a simple yet effective method that addresses
the diversity issue from a distinct perspective. Unlike prior techniques that
adapt intermediate latents, our approach shapes the initial noise to promote
diverse outputs. Specifically, we develop a contrastive loss defined in the
Tweedie data space and optimize a batch of noise latents. Our contrastive
optimization repels instances within the batch to maximize diversity while
keeping them anchored to a reference sample to preserve fidelity. We further
provide theoretical insights into the mechanism of this preprocessing to
substantiate its effectiveness. Extensive experiments across multiple T2I
backbones demonstrate that our approach achieves a superior quality-diversity
Pareto frontier while remaining robust to hyperparameter choices.

</details>


### [17] [Joint Neural SDF Reconstruction and Semantic Segmentation for CAD Models](https://arxiv.org/abs/2510.03837)
*Shen Fan,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: 提出一种基于神经SDF的隐式重建网络增强方法，通过增加分割头部实现任意部件数量的CAD网格语义分割，同时保持重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于固定部件分类体系，无法灵活处理任意部件数量的CAD网格。本方法旨在实现无需预设分类的几何对齐标签生成，提升数据效率和分割一致性。

Method: 在Flat-CAD SDF主干网络上附加轻量级分割头部，使用PartField生成的分割监督进行训练。采用面级监督和新的分割一致性指标评估局部标签平滑度。

Result: 在ABC数据集上达到CDL1/CDL2（0.012/0.024）、mIoU 87.6%、准确率93.4%。重建质量不受分割影响，薄壁/复杂几何体仍保持正确部件数量和标签一致性。

Conclusion: 该方法为无分类约束的语义结构化CAD网格提供实用方案。当前局限在于边界精度不足，未来可通过边界感知训练和高分辨率标签改进。

Abstract: We propose a simple, data-efficient pipeline that augments an implicit
reconstruction network based on neural SDF-based CAD parts with a
part-segmentation head trained under PartField-generated supervision. Unlike
methods tied to fixed taxonomies, our model accepts meshes with any number of
parts and produces coherent, geometry-aligned labels in a single pass. We
evaluate on randomly sampled CAD meshes from the ABC dataset with intentionally
varied part cardinalities, including over-segmented shapes, and report strong
performance across reconstruction (CDL1/CDL2, F1-micro, NC) and segmentation
(mIoU, Accuracy), together with a new Segmentation Consistency metric that
captures local label smoothness. We attach a lightweight segmentation head to
the Flat-CAD SDF trunk; on a paired evaluation it does not alter reconstruction
while providing accurate part labels for meshes with any number of parts. Even
under degraded reconstructions on thin or intricate geometries, segmentation
remains accurate and label-coherent, often preserving the correct part count.
Our approach therefore offers a practical route to semantically structured CAD
meshes without requiring curated taxonomies or exact palette matches. We
discuss limitations in boundary precision, partly due to per-face supervision,
and outline paths toward boundary-aware training and higher resolution labels.

</details>


### [18] [Enhancing Foveated Rendering with Weighted Reservoir Sampling](https://arxiv.org/abs/2510.03964)
*Ville Cantory,Darya Biparva,Haoyu Tan,Tongyu Nie,John Schroeder,Ruofei Du,Victoria Interrante,Piotr Didyk*

Main category: cs.GR

TL;DR: 提出基于加权蓄水池采样（Weighted Reservoir Sampling）的注视点渲染技术，通过复用历史帧高质量像素样本提升感知质量，降低实时渲染开销。


<details>
  <summary>Details</summary>
Motivation: 传统注视点渲染丢弃历史帧高分辨率样本，且未考虑眼动特性（扫视着陆点分布、固视微跳变）。利用相邻帧注视点偏移特性，通过时域复用减少单帧中心渲染区域。

Method: 将像素时序呈现视为数据流，构建加权蓄水池采样机制，动态维护历史帧感知相关的高质量像素样本库，组合生成当前帧。

Result: 4K分辨率下运行时间<1ms，兼容现有VR/AR注视点渲染系统，允许更高程度的注视点区域缩减（foveation levels）同时保持感知质量。

Conclusion: 时域像素复用机制通过智能样本管理，在降低渲染成本与保持视觉质量间取得平衡，为实时注视点渲染提供高效解决方案。

Abstract: Spatiotemporal sensitivity to high frequency information declines with
increased peripheral eccentricity. Foveated rendering exploits this by
decreasing the spatial resolution of rendered images in peripheral vision,
reducing the rendering cost by omitting high frequency details. As foveation
levels increase, the rendering quality is reduced, and traditional foveated
rendering systems tend not to preserve samples that were previously rendered at
high spatial resolution in previous frames. Additionally, prior research has
shown that saccade landing positions are distributed around a target location
rather than landing at a single point, and that even during fixations, eyes
perform small microsaccades around a fixation point. This creates an
opportunity for sampling from temporally neighbouring frames with differing
foveal locations to reduce the required rendered size of the foveal region
while achieving a higher perceived image quality. We further observe that the
temporal presentation of pixels frame-to-frame can be viewed as a data stream,
presenting a random sampling problem. Following this intuition, we propose a
Weighted Reservoir Sampling technique to efficiently maintain a reservoir of
the perceptually relevant high quality pixel samples from previous frames and
incorporate them into the computation of the current frame. This allows the
renderer to render a smaller region of foveal pixels per frame by temporally
reusing pixel samples that are still relevant to reconstruct a higher perceived
image quality, while allowing for higher levels of foveation. Our method
operates on the output of foveated rendering, and runs in under 1\,ms at 4K
resolution, making it highly efficient and integrable with real-time VR and AR
foveated rendering systems.

</details>


### [19] [3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using MCP and RAG](https://arxiv.org/abs/2510.04536)
*Shun-ichiro Hayashi,Daichi Mukunoki,Tetsuya Hoshino,Satoshi Ohshima,Takahiro Katagiri*

Main category: cs.GR

TL;DR: 3Dify是基于Dify平台开发的3D生成框架，通过自然语言指令驱动LLM自动操作DCC工具，整合MCP/RAG技术实现自动化流程，支持用户反馈优化生成质量，并提供本地LLM部署降低API成本。


<details>
  <summary>Details</summary>
Motivation: 解决传统3D内容创作对专业工具依赖度高的问题，通过自然语言交互降低使用门槛，同时减少对外部API的依赖以控制成本。

Method: 1. 基于MCP协议实现DCC工具自动化操作
2. 对非MCP兼容工具采用CUA模拟GUI操作
3. 用户反馈驱动LLM迭代优化生成结果
4. 支持本地LLM部署减少API开销

Result: 构建了端到端的自然语言驱动3D生成系统，实现用户偏好学习机制，验证了本地模型部署在成本控制方面的有效性。

Conclusion: 该框架显著降低了3D内容创作的技术门槛，通过可扩展架构平衡了生成质量与成本效益，为AI辅助数字内容生产提供了新范式。

Abstract: This paper proposes "3Dify," a procedural 3D computer graphics (3D-CG)
generation framework utilizing Large Language Models (LLMs). The framework
enables users to generate 3D-CG content solely through natural language
instructions. 3Dify is built upon Dify, an open-source platform for AI
application development, and incorporates several state-of-the-art LLM-related
technologies such as the Model Context Protocol (MCP) and Retrieval-Augmented
Generation (RAG). For 3D-CG generation support, 3Dify automates the operation
of various Digital Content Creation (DCC) tools via MCP. When DCC tools do not
support MCP-based interaction, the framework employs the Computer-Using Agent
(CUA) method to automate Graphical User Interface (GUI) operations. Moreover,
to enhance image generation quality, 3Dify allows users to provide feedback by
selecting preferred images from multiple candidates. The LLM then learns
variable patterns from these selections and applies them to subsequent
generations. Furthermore, 3Dify supports the integration of locally deployed
LLMs, enabling users to utilize custom-developed models and to reduce both time
and monetary costs associated with external API calls by leveraging their own
computational resources.

</details>


### [20] [C3Editor: Achieving Controllable Consistency in 2D Model for 3D Editing](https://arxiv.org/abs/2510.04539)
*Zeng Tao,Zheng Ding,Zeyuan Chen,Xiang Zhang,Leizhi Li,Zhuowen Tu*

Main category: cs.GR

TL;DR: 提出C3Editor框架，通过选择性建立视图一致的2D编辑模型解决现有3D编辑方法的多视图不一致问题，在定性和定量评估中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有基于2D提升的3D编辑方法存在多视图编辑不一致问题，主要源于缺乏视图一致的2D编辑模型和难以保证跨视图编辑一致性

Method: 1. 选择地面实况视图及其编辑图像作为优化目标
2. 在GT视图和多视图上微调2D编辑模型
3. 引入独立LoRA模块分别处理视图拟合和多视图一致性需求

Result: 在定性和定量评估中优于现有2D提升方法，提供更一致可控的2D/3D编辑结果

Conclusion: C3Editor通过视图选择性优化和模块化设计，有效解决了多视图编辑一致性问题，为3D内容编辑提供了新的技术路径

Abstract: Existing 2D-lifting-based 3D editing methods often encounter challenges
related to inconsistency, stemming from the lack of view-consistent 2D editing
models and the difficulty of ensuring consistent editing across multiple views.
To address these issues, we propose C3Editor, a controllable and consistent
2D-lifting-based 3D editing framework. Given an original 3D representation and
a text-based editing prompt, our method selectively establishes a
view-consistent 2D editing model to achieve superior 3D editing results. The
process begins with the controlled selection of a ground truth (GT) view and
its corresponding edited image as the optimization target, allowing for
user-defined manual edits. Next, we fine-tune the 2D editing model within the
GT view and across multiple views to align with the GT-edited image while
ensuring multi-view consistency. To meet the distinct requirements of GT view
fitting and multi-view consistency, we introduce separate LoRA modules for
targeted fine-tuning. Our approach delivers more consistent and controllable 2D
and 3D editing results than existing 2D-lifting-based methods, outperforming
them in both qualitative and quantitative evaluations.

</details>


### [21] [Social Agent: Mastering Dyadic Nonverbal Behavior Generation via Conversational LLM Agents](https://arxiv.org/abs/2510.04637)
*Zeyi Zhang,Yanju Zhou,Heyuan Yao,Tenglong Ao,Xiaohang Zhan,Libin Liu*

Main category: cs.GR

TL;DR: 提出Social Agent框架，通过LLM驱动的代理系统和双人扩散手势生成模型，实现对话中自然协调的非言语行为合成。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以生成对话双方自然同步的非言语行为，需要同时考虑行为决策层和动作生成层的协调性。

Method: 1. LLM驱动代理系统控制对话流程和行为决策 2. 基于自回归扩散模型的双人手势生成器 3. 通过动作检查-意图推断形成动态反馈循环

Result: 用户研究和定量评估显示模型显著提升双人互动质量，生成更自然同步的非言语行为

Conclusion: 该框架在行为决策和动作生成层面实现协调，通过持续反馈机制使双人交互更动态逼真

Abstract: We present Social Agent, a novel framework for synthesizing realistic and
contextually appropriate co-speech nonverbal behaviors in dyadic conversations.
In this framework, we develop an agentic system driven by a Large Language
Model (LLM) to direct the conversation flow and determine appropriate
interactive behaviors for both participants. Additionally, we propose a novel
dual-person gesture generation model based on an auto-regressive diffusion
model, which synthesizes coordinated motions from speech signals. The output of
the agentic system is translated into high-level guidance for the gesture
generator, resulting in realistic movement at both the behavioral and motion
levels. Furthermore, the agentic system periodically examines the movements of
interlocutors and infers their intentions, forming a continuous feedback loop
that enables dynamic and responsive interactions between the two participants.
User studies and quantitative evaluations show that our model significantly
improves the quality of dyadic interactions, producing natural, synchronized
nonverbal behaviors.

</details>


### [22] [Bridging Text and Video Generation: A Survey](https://arxiv.org/abs/2510.04999)
*Nilay Kumar,Priyansh Bhandari,G. Maragatham*

Main category: cs.GR

TL;DR: 本文系统回顾了文本生成视频（T2V）技术的演进历程，涵盖从GAN/VAE到扩散-Transformer混合架构的技术迭代，分析了数据集、训练配置、评估指标及现存挑战，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 旨在整合T2V生成模型的研究进展，揭示模型进化路径（如从对抗模型到扩散模型），解决视频生成中的对齐性、长程连贯性等核心问题，并通过公开训练配置促进研究可复现性。

Method: 采用系统性文献综述方法，对比分析不同架构模型（GAN/VAE/DiT）的技术原理与改进动机，统计模型训练数据集（如WebVid-10M）、硬件配置（GPU数量/批次大小/学习率）及评估指标（FVD/CLIP-score）。

Result: 扩散-Transformer模型显著提升了视频保真度与时序连贯性，但计算效率、长视频生成能力仍受限。现有评估指标存在局限性，需发展更符合人类感知的评估体系。

Conclusion: 当前T2V技术面临模型效率、可控性、评估体系等开放挑战，未来需探索轻量化架构、多模态对齐优化及动态自适应生成技术，推动应用场景落地。

Abstract: Text-to-video (T2V) generation technology holds potential to transform
multiple domains such as education, marketing, entertainment, and assistive
technologies for individuals with visual or reading comprehension challenges,
by creating coherent visual content from natural language prompts. From its
inception, the field has advanced from adversarial models to diffusion-based
models, yielding higher-fidelity, temporally consistent outputs. Yet challenges
persist, such as alignment, long-range coherence, and computational efficiency.
Addressing this evolving landscape, we present a comprehensive survey of
text-to-video generative models, tracing their development from early GANs and
VAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these
models work, what limitations they addressed in their predecessors, and why
shifts toward new architectural paradigms were necessary to overcome challenges
in quality, coherence, and control. We provide a systematic account of the
datasets, which the surveyed text-to-video models were trained and evaluated
on, and, to support reproducibility and assess the accessibility of training
such models, we detail their training configurations, including their hardware
specifications, GPU counts, batch sizes, learning rates, optimizers, epochs,
and other key hyperparameters. Further, we outline the evaluation metrics
commonly used for evaluating such models and present their performance across
standard benchmarks, while also discussing the limitations of these metrics and
the emerging shift toward more holistic, perception-aligned evaluation
strategies. Finally, drawing from our analysis, we outline the current open
challenges and propose a few promising future directions, laying out a
perspective for future researchers to explore and build upon in advancing T2V
research and applications.

</details>


### [23] [SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder](https://arxiv.org/abs/2510.05081)
*Ronen Kamenetsky,Sara Dorfman,Daniel Garibi,Roni Paiss,Or Patashnik,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: 提出基于文本嵌入token级别操作的解耦连续编辑方法，通过稀疏自编码器实现属性独立控制


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型缺乏对编辑过程的精细控制，需要实现解耦编辑属性和连续强度调节

Method: 在文本嵌入空间构建稀疏自编码器，通过其稀疏潜在空间的语义隔离维度寻找属性控制方向

Result: 实现了跨多属性和领域的连续直观编辑，兼容不同图像合成框架且无需修改扩散过程

Conclusion: 该方法为生成式图像编辑提供了模型无关的灵活控制方案，显著提升编辑精度和效率

Abstract: Large-scale text-to-image diffusion models have become the backbone of modern
image editing, yet text prompts alone do not offer adequate control over the
editing process. Two properties are especially desirable: disentanglement,
where changing one attribute does not unintentionally alter others, and
continuous control, where the strength of an edit can be smoothly adjusted. We
introduce a method for disentangled and continuous editing through token-level
manipulation of text embeddings. The edits are applied by manipulating the
embeddings along carefully chosen directions, which control the strength of the
target attribute. To identify such directions, we employ a Sparse Autoencoder
(SAE), whose sparse latent space exposes semantically isolated dimensions. Our
method operates directly on text embeddings without modifying the diffusion
process, making it model agnostic and broadly applicable to various image
synthesis backbones. Experiments show that it enables intuitive and efficient
manipulations with continuous control across diverse attributes and domains.

</details>
