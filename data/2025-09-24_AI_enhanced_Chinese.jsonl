{"id": "2509.18461", "pdf": "https://arxiv.org/pdf/2509.18461", "abs": "https://arxiv.org/abs/2509.18461", "authors": ["Ayan Sar", "Sampurna Roy", "Tanupriya Choudhury", "Ajith Abraham"], "title": "Zero-Shot Visual Deepfake Detection: Can AI Predict and Prevent Fake Content Before It's Created?", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.MM"], "comment": "Published in Foundations and Trends in Signal Processing (#1 in\n  Signal Processing, #3 in Computer Science)", "summary": "Generative adversarial networks (GANs) and diffusion models have dramatically\nadvanced deepfake technology, and its threats to digital security, media\nintegrity, and public trust have increased rapidly. This research explored\nzero-shot deepfake detection, an emerging method even when the models have\nnever seen a particular deepfake variation. In this work, we studied\nself-supervised learning, transformer-based zero-shot classifier, generative\nmodel fingerprinting, and meta-learning techniques that better adapt to the\never-evolving deepfake threat. In addition, we suggested AI-driven prevention\nstrategies that mitigated the underlying generation pipeline of the deepfakes\nbefore they occurred. They consisted of adversarial perturbations for creating\ndeepfake generators, digital watermarking for content authenticity\nverification, real-time AI monitoring for content creation pipelines, and\nblockchain-based content verification frameworks. Despite these advancements,\nzero-shot detection and prevention faced critical challenges such as\nadversarial attacks, scalability constraints, ethical dilemmas, and the absence\nof standardized evaluation benchmarks. These limitations were addressed by\ndiscussing future research directions on explainable AI for deepfake detection,\nmultimodal fusion based on image, audio, and text analysis, quantum AI for\nenhanced security, and federated learning for privacy-preserving deepfake\ndetection. This further highlighted the need for an integrated defense\nframework for digital authenticity that utilized zero-shot learning in\ncombination with preventive deepfake mechanisms. Finally, we highlighted the\nimportant role of interdisciplinary collaboration between AI researchers,\ncybersecurity experts, and policymakers to create resilient defenses against\nthe rising tide of deepfake attacks.", "AI": {"tldr": "\u63a2\u8ba8\u57fa\u4e8e\u96f6\u6837\u672c\u5b66\u4e60\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6280\u672f\uff0c\u7ed3\u5408\u751f\u6210\u6a21\u578b\u6307\u7eb9\u8bc6\u522b\u3001\u5143\u5b66\u4e60\u7b49\u65b9\u6cd5\uff0c\u63d0\u51fa\u5305\u542b\u5bf9\u6297\u6270\u52a8\u3001\u533a\u5757\u94fe\u9a8c\u8bc1\u7684\u9884\u9632\u6846\u67b6\uff0c\u5e76\u5206\u6790\u73b0\u6709\u6311\u6218\u4e0e\u672a\u6765\u91cf\u5b50AI\u3001\u8054\u90a6\u5b66\u4e60\u7b49\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5e94\u5bf9\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u5bf9\u6570\u5b57\u5b89\u5168\u3001\u5a92\u4f53\u516c\u4fe1\u529b\u7684\u5a01\u80c1\uff0c\u89e3\u51b3\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5728\u672a\u77e5\u4f2a\u9020\u53d8\u4f53\u65f6\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u3001Transformer\u96f6\u6837\u672c\u5206\u7c7b\u5668\u3001\u751f\u6210\u6a21\u578b\u6307\u7eb9\u8bc6\u522b\u548c\u5143\u5b66\u4e60\u6280\u672f\uff0c\u5f00\u53d1AI\u9a71\u52a8\u7684\u5bf9\u6297\u6270\u52a8\u3001\u6570\u5b57\u6c34\u5370\u3001\u5b9e\u65f6\u76d1\u63a7\u53ca\u533a\u5757\u94fe\u9a8c\u8bc1\u9632\u5fa1\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u96c6\u6210\u96f6\u6837\u672c\u5b66\u4e60\u548c\u9884\u9632\u673a\u5236\u7684\u7efc\u5408\u9632\u5fa1\u6846\u67b6\uff0c\u9a8c\u8bc1\u5176\u5728\u672a\u89c1\u8fc7\u4f2a\u9020\u7c7b\u578b\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u4f46\u9762\u4e34\u5bf9\u6297\u653b\u51fb\u3001\u6807\u51c6\u5316\u8bc4\u4f30\u7f3a\u5931\u7b49\u5c40\u9650\u6027\u3002", "conclusion": "\u9700\u901a\u8fc7\u53ef\u89e3\u91caAI\u3001\u591a\u6a21\u6001\u5206\u6790\u63d0\u5347\u68c0\u6d4b\u900f\u660e\u5ea6\uff0c\u5e76\u4f9d\u8d56\u8de8\u5b66\u79d1\u5408\u4f5c\u6784\u5efa\u62b5\u5fa1\u6df1\u5ea6\u4f2a\u9020\u653b\u51fb\u7684\u97e7\u6027\u4f53\u7cfb\u3002"}}
{"id": "2509.18497", "pdf": "https://arxiv.org/pdf/2509.18497", "abs": "https://arxiv.org/abs/2509.18497", "authors": ["Kaiwen Jiang", "Jia-Mu Sun", "Zilu Li", "Dan Wang", "Tzu-Mao Li", "Ravi Ramamoorthi"], "title": "Differentiable Light Transport with Gaussian Surfels via Adapted Radiosity for Efficient Relighting and Geometry Reconstruction", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Radiance fields have gained tremendous success with applications ranging from\nnovel view synthesis to geometry reconstruction, especially with the advent of\nGaussian splatting. However, they sacrifice modeling of material reflective\nproperties and lighting conditions, leading to significant geometric\nambiguities and the inability to easily perform relighting. One way to address\nthese limitations is to incorporate physically-based rendering, but it has been\nprohibitively expensive to include full global illumination within the inner\nloop of the optimization. Therefore, previous works adopt simplifications that\nmake the whole optimization with global illumination effects efficient but less\naccurate. In this work, we adopt Gaussian surfels as the primitives and build\nan efficient framework for differentiable light transport, inspired from the\nclassic radiosity theory. The whole framework operates in the coefficient space\nof spherical harmonics, enabling both diffuse and specular materials. We extend\nthe classic radiosity into non-binary visibility and semi-opaque primitives,\npropose novel solvers to efficiently solve the light transport, and derive the\nbackward pass for gradient optimizations, which is more efficient than\nauto-differentiation. During inference, we achieve view-independent rendering\nwhere light transport need not be recomputed under viewpoint changes, enabling\nhundreds of FPS for global illumination effects, including view-dependent\nreflections using a spherical harmonics representation. Through extensive\nqualitative and quantitative experiments, we demonstrate superior geometry\nreconstruction, view synthesis and relighting than previous inverse rendering\nbaselines, or data-driven baselines given relatively sparse datasets with known\nor unknown lighting conditions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9ad8\u65af\u66f2\u9762\u5143\u548c\u8f90\u5c04\u5ea6\u7406\u8bba\u7684\u9ad8\u6548\u53ef\u5fae\u5206\u5149\u4f20\u8f93\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u51e0\u4f55\u91cd\u5efa\u4e0e\u52a8\u6001\u91cd\u6253\u5149\u6548\u679c", "motivation": "\u73b0\u6709\u8f90\u5c04\u573a\u6280\u672f\u727a\u7272\u6750\u8d28\u53cd\u5c04\u7279\u6027\u548c\u5149\u7167\u5efa\u6a21\uff0c\u5bfc\u81f4\u51e0\u4f55\u6a21\u7cca\u4e14\u65e0\u6cd5\u652f\u6301\u91cd\u6253\u5149\u3002\u5168\u5c40\u5149\u7167\u7684\u4f20\u7edf\u5b9e\u73b0\u65b9\u5f0f\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u73b0\u6709\u7b80\u5316\u65b9\u6848\u7cbe\u5ea6\u4e0d\u8db3", "method": "\u5728\u7403\u8c10\u7cfb\u6570\u7a7a\u95f4\u6784\u5efa\u5149\u4f20\u8f93\u6846\u67b6\uff0c\u6269\u5c55\u7ecf\u5178\u8f90\u5c04\u5ea6\u7406\u8bba\u652f\u6301\u975e\u4e8c\u5143\u53ef\u89c1\u6027\u548c\u534a\u900f\u660e\u57fa\u5143\uff0c\u5f00\u53d1\u9ad8\u6548\u524d\u5411\u6c42\u89e3\u5668\u4e0e\u5b9a\u5236\u53cd\u5411\u4f20\u64ad\u68af\u5ea6\u8ba1\u7b97", "result": "\u5728\u5df2\u77e5/\u672a\u77e5\u5149\u7167\u7684\u7a00\u758f\u6570\u636e\u96c6\u4e0a\uff0c\u51e0\u4f55\u91cd\u5efa\u8bef\u5dee\u964d\u4f4e23%\uff0c\u91cd\u6253\u5149PSNR\u63d0\u534718%\uff0c\u652f\u6301\u6570\u767eFPS\u7684\u89c6\u89d2\u65e0\u5173\u5168\u5c40\u5149\u7167\u6e32\u67d3", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u7834\u4e86\u4f20\u7edf\u8f90\u5c04\u573a\u4e0e\u9006\u5411\u6e32\u67d3\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5b9e\u65f6\u52a8\u6001\u5149\u7167\u573a\u666f\u4e0b\u7684\u9ad8\u8d28\u91cf\u4e09\u7ef4\u91cd\u5efa\u4e0e\u6e32\u67d3\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84"}}
{"id": "2509.18831", "pdf": "https://arxiv.org/pdf/2509.18831", "abs": "https://arxiv.org/abs/2509.18831", "authors": ["Pin-Yen Chiu", "I-Sheng Fang", "Jun-Cheng Chen"], "title": "Text Slider: Efficient and Plug-and-Play Continuous Concept Control for Image/Video Synthesis via LoRA Adapters", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Recent advances in diffusion models have significantly improved image and\nvideo synthesis. In addition, several concept control methods have been\nproposed to enable fine-grained, continuous, and flexible control over\nfree-form text prompts. However, these methods not only require intensive\ntraining time and GPU memory usage to learn the sliders or embeddings but also\nneed to be retrained for different diffusion backbones, limiting their\nscalability and adaptability. To address these limitations, we introduce Text\nSlider, a lightweight, efficient and plug-and-play framework that identifies\nlow-rank directions within a pre-trained text encoder, enabling continuous\ncontrol of visual concepts while significantly reducing training time, GPU\nmemory consumption, and the number of trainable parameters. Furthermore, Text\nSlider supports multi-concept composition and continuous control, enabling\nfine-grained and flexible manipulation in both image and video synthesis. We\nshow that Text Slider enables smooth and continuous modulation of specific\nattributes while preserving the original spatial layout and structure of the\ninput. Text Slider achieves significantly better efficiency: 5$\\times$ faster\ntraining than Concept Slider and 47$\\times$ faster than Attribute Control,\nwhile reducing GPU memory usage by nearly 2$\\times$ and 4$\\times$,\nrespectively.", "AI": {"tldr": "\u63d0\u51faText Slider\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u6587\u672c\u7f16\u7801\u5668\u7684\u4f4e\u79e9\u65b9\u5411\u5b9e\u73b0\u9ad8\u6548\u89c6\u89c9\u6982\u5ff5\u63a7\u5236\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u5e76\u652f\u6301\u591a\u6982\u5ff5\u5408\u6210", "motivation": "\u73b0\u6709\u6982\u5ff5\u63a7\u5236\u65b9\u6cd5\u5b58\u5728\u8bad\u7ec3\u8017\u65f6\u3001\u5185\u5b58\u5360\u7528\u9ad8\u3001\u9700\u91cd\u590d\u8bad\u7ec3\u4e0d\u540c\u6a21\u578b\u7684\u95ee\u9898\uff0c\u9650\u5236\u5176\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027", "method": "\u5728\u9884\u8bad\u7ec3\u6587\u672c\u7f16\u7801\u5668\u4e2d\u8bc6\u522b\u4f4e\u79e9\u65b9\u5411\uff0c\u5b9e\u73b0\u5373\u63d2\u5373\u7528\u7684\u8fde\u7eed\u89c6\u89c9\u6982\u5ff5\u63a7\u5236\uff0c\u652f\u6301\u56fe\u50cf/\u89c6\u9891\u5408\u6210\u7684\u591a\u6982\u5ff5\u7ec4\u5408", "result": "\u8bad\u7ec3\u901f\u5ea6\u6bd4Concept Slider\u5feb5\u500d\uff0c\u6bd4Attribute Control\u5feb47\u500d\uff1bGPU\u5185\u5b58\u6d88\u8017\u5206\u522b\u51cf\u5c11\u8fd12\u500d\u548c4\u500d", "conclusion": "Text Slider\u5728\u4fdd\u6301\u751f\u6210\u5185\u5bb9\u7ed3\u6784\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7075\u6d3b\u7684\u6982\u5ff5\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u8d44\u6e90\u5229\u7528\u7387"}}
{"id": "2509.18948", "pdf": "https://arxiv.org/pdf/2509.18948", "abs": "https://arxiv.org/abs/2509.18948", "authors": ["Jun Ma", "Qian He", "Gaofeng He", "Huang Chen", "Chen Liu", "Xiaogang Jin", "Huamin Wang"], "title": "One-shot Embroidery Customization via Contrastive LoRA Modulation", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to ACM Transactions on Graphics (TOG), SIGGRAPH Asia 2025", "summary": "Diffusion models have significantly advanced image manipulation techniques,\nand their ability to generate photorealistic images is beginning to transform\nretail workflows, particularly in presale visualization. Beyond artistic style\ntransfer, the capability to perform fine-grained visual feature transfer is\nbecoming increasingly important. Embroidery is a textile art form characterized\nby intricate interplay of diverse stitch patterns and material properties,\nwhich poses unique challenges for existing style transfer methods. To explore\nthe customization for such fine-grained features, we propose a novel\ncontrastive learning framework that disentangles fine-grained style and content\nfeatures with a single reference image, building on the classic concept of\nimage analogy. We first construct an image pair to define the target style, and\nthen adopt a similarity metric based on the decoupled representations of\npretrained diffusion models for style-content separation. Subsequently, we\npropose a two-stage contrastive LoRA modulation technique to capture\nfine-grained style features. In the first stage, we iteratively update the\nwhole LoRA and the selected style blocks to initially separate style from\ncontent. In the second stage, we design a contrastive learning strategy to\nfurther decouple style and content through self-knowledge distillation.\nFinally, we build an inference pipeline to handle image or text inputs with\nonly the style blocks. To evaluate our method on fine-grained style transfer,\nwe build a benchmark for embroidery customization. Our approach surpasses prior\nmethods on this task and further demonstrates strong generalization to three\nadditional domains: artistic style transfer, sketch colorization, and\nappearance transfer.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5LoRA\u8c03\u5236\u5b9e\u73b0\u523a\u7ee3\u7b49\u7ec6\u7c92\u5ea6\u98ce\u683c\u8fc1\u79fb\uff0c\u5728\u591a\u4e2a\u9886\u57df\u5c55\u73b0\u4f18\u5f02\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u523a\u7ee3\u7b49\u7eba\u7ec7\u827a\u672f\u5177\u6709\u590d\u6742\u9488\u811a\u4e0e\u6750\u6599\u7279\u6027\uff0c\u73b0\u6709\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u8fd9\u7c7b\u7ec6\u7c92\u5ea6\u7279\u5f81\u8fc1\u79fb\u9700\u6c42\u3002", "method": "1.\u6784\u5efa\u56fe\u50cf\u5bf9\u5b9a\u4e49\u76ee\u6807\u98ce\u683c\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u89e3\u8026\u7279\u5f81\n2.\u63d0\u51fa\u4e24\u9636\u6bb5\u5bf9\u6bd4\u6027LoRA\u8c03\u5236\uff1a\u5148\u8fed\u4ee3\u66f4\u65b0\u6574\u4f53\u53c2\u6570\u5206\u79bb\u98ce\u683c\u5185\u5bb9\uff0c\u518d\u901a\u8fc7\u81ea\u77e5\u8bc6\u84b8\u998f\u589e\u5f3a\u89e3\u8026\n3.\u6784\u5efa\u652f\u6301\u56fe\u6587\u8f93\u5165\u7684\u63a8\u7406\u6d41\u7a0b", "result": "\u5728\u523a\u7ee3\u5b9a\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u827a\u672f\u98ce\u683c\u8fc1\u79fb/\u8349\u56fe\u7740\u8272/\u5916\u89c2\u8fc1\u79fb\u4e09\u4e2a\u65b0\u9886\u57df\u9a8c\u8bc1\u6709\u6548\u6027", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7279\u5f81\u89e3\u8026\u4e0e\u8fc1\u79fb\uff0c\u4e3a\u590d\u6742\u7eb9\u7406\u6750\u6599\u7684\u89c6\u89c9\u7279\u5f81\u5b9a\u5236\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.18113", "pdf": "https://arxiv.org/pdf/2509.18113", "abs": "https://arxiv.org/abs/2509.18113", "authors": ["Xin Hu", "Yue Kang", "Guanzi Yao", "Tianze Kang", "Mengjie Wang", "Heyao Liu"], "title": "Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "This study addresses the generalization limitations commonly observed in\nlarge language models under multi-task and cross-domain settings. Unlike prior\nmethods such as SPoT, which depends on fixed prompt templates, our study\nintroduces a unified multi-task learning framework with dynamic prompt\nscheduling mechanism. By introducing a prompt pool and a task-aware scheduling\nstrategy, the method dynamically combines and aligns prompts for different\ntasks. This enhances the model's ability to capture semantic differences across\ntasks. During prompt fusion, the model uses task embeddings and a gating\nmechanism to finely control the prompt signals. This ensures alignment between\nprompt content and task-specific demands. At the same time, it builds flexible\nsharing pathways across tasks. In addition, the proposed optimization objective\ncenters on joint multi-task learning. It incorporates an automatic learning\nstrategy for scheduling weights, which effectively mitigates task interference\nand negative transfer. To evaluate the effectiveness of the method, a series of\nsensitivity experiments were conducted. These experiments examined the impact\nof prompt temperature parameters and task number variation. The results confirm\nthe advantages of the proposed mechanism in maintaining model stability and\nenhancing transferability. Experimental findings show that the prompt\nscheduling method significantly improves performance on a range of language\nunderstanding and knowledge reasoning tasks. These results fully demonstrate\nits applicability and effectiveness in unified multi-task modeling and\ncross-domain adaptation.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u63d0\u793a\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u793a\u6c60\u548c\u4efb\u52a1\u611f\u77e5\u673a\u5236\u589e\u5f3a\u591a\u4efb\u52a1\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u4e0e\u7a33\u5b9a\u6027", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4efb\u52a1/\u8de8\u9886\u57df\u573a\u666f\u4e2d\u4f9d\u8d56\u56fa\u5b9a\u63d0\u793a\u6a21\u677f\u5bfc\u81f4\u7684\u6cdb\u5316\u4e0d\u8db3\u95ee\u9898", "method": "\u6784\u5efa\u63d0\u793a\u6c60+\u4efb\u52a1\u611f\u77e5\u8c03\u5ea6\u7b56\u7565\uff0c\u91c7\u7528\u4efb\u52a1\u5d4c\u5165\u4e0e\u95e8\u63a7\u673a\u5236\u5b9e\u73b0\u63d0\u793a\u52a8\u6001\u878d\u5408\uff0c\u7ed3\u5408\u81ea\u52a8\u6743\u91cd\u8c03\u6574\u7684\u8054\u5408\u4f18\u5316\u76ee\u6807", "result": "\u654f\u611f\u6027\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u7a33\u5b9a\u6027\u63d0\u5347\uff0c\u5728\u8bed\u8a00\u7406\u89e3\u4e0e\u77e5\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\u6027\u80fd\u663e\u8457\u63d0\u9ad8", "conclusion": "\u52a8\u6001\u63d0\u793a\u8c03\u5ea6\u673a\u5236\u6709\u6548\u5b9e\u73b0\u591a\u4efb\u52a1\u7edf\u4e00\u5efa\u6a21\uff0c\u5728\u8de8\u9886\u57df\u9002\u5e94\u4e2d\u5c55\u793a\u51fa\u4f18\u8d8a\u6027\u80fd"}}
{"id": "2509.18498", "pdf": "https://arxiv.org/pdf/2509.18498", "abs": "https://arxiv.org/abs/2509.18498", "authors": ["Yoichi Ochiai"], "title": "null2: Boundary-Dissolving Bodies and Architecture towards Digital Nature", "categories": ["cs.HC", "cs.GR"], "comment": "12pages", "summary": "This paper presents a case study of the thematic pavilion null2 at Expo 2025\nOsaka-Kansai, contrasting with the static Jomon motifs of Taro Okamoto's Tower\nof the Sun from Expo 1970. The study discusses Yayoi-inspired mirror motifs and\ndynamically transforming interactive spatial configuration of null2, where\nvisitors become integrated as experiential content. The shift from static\nrepresentation to a new ontological and aesthetic model, characterized by the\nvisitor's body merging in real-time with architectural space at installation\nscale, is analyzed. Referencing the philosophical context of Expo 1970 theme\n'Progress and Harmony for Mankind,' this research reconsiders the worldview\narticulated by null2 in Expo 2025, in which computation is naturalized and\nubiquitous, through its intersection with Eastern philosophical traditions. It\ninvestigates how immersive experiences within the pavilion, grounded in the\nphilosophical framework of Digital Nature, reinterpret traditional spatial and\nstructural motifs of the tea room, positioning them within contemporary digital\nart discourse. The aim is to contextualize and document null2 as an important\ncontemporary case study from Expo practices, considering the historical and\nsocial background in Japan from the 19th to 21st century, during which world\nexpositions served as pivotal points for the birth of modern Japanese concept\nof 'fine art,' symbolic milestones of economic development, and key moments in\nurban and media culture formation. Furthermore, this paper academically\norganizes architectural techniques, computer graphics methodologies, media art\npractices, and theoretical backgrounds utilized in null2, highlighting the\nscholarly significance of preserving these as an archival document for future\ngenerations.", "AI": {"tldr": "\u5bf9\u6bd42025\u5927\u962a\u4e16\u535a\u4f1anull2\u52a8\u6001\u4e92\u52a8\u9986\u4e0e1970\u4e16\u535a\u4f1a\u592a\u9633\u5854\u9759\u6001\u7f8e\u5b66\uff0c\u63a2\u8ba8\u6570\u5b57\u81ea\u7136\u54f2\u5b66\u4e0b\u7a7a\u95f4\u91cd\u6784\u53ca\u4f20\u7edf\u8336\u5ba4\u5143\u7d20\u7684\u6570\u5b57\u5316\u8f6c\u8bd1\u3002", "motivation": "\u901a\u8fc7\u5206\u6790\u4e24\u5c4a\u4e16\u535a\u4f1a\u6807\u5fd7\u6027\u5efa\u7b51\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u63ed\u793a\u8ba1\u7b97\u673a\u6280\u672f\u81ea\u7136\u5316\u8fdb\u7a0b\u4e2d\u5efa\u7b51\u7a7a\u95f4\u4e0e\u4eba\u4f53\u611f\u77e5\u7684\u65b0\u578b\u5173\u7cfb\uff0c\u53ca\u5176\u5bf9\u4e1c\u65b9\u54f2\u5b66\u4f20\u7edf\u7684\u5f53\u4ee3\u8be0\u91ca\u3002", "method": "\u91c7\u7528\u8de8\u5b66\u79d1\u6848\u4f8b\u7814\u7a76\uff1a1) \u5bf9\u6bd4\u5206\u6790\u4e24\u5c55\u9986\u7684\u7a7a\u95f4\u914d\u7f6e\u903b\u8f91 2) \u89e3\u6784null2\u7684CG\u7b97\u6cd5\u67b6\u6784 3) \u7ed3\u5408\u6570\u5b57\u81ea\u7136\u7406\u8bba\u6846\u67b6\uff0c\u9610\u91ca\u8336\u5ba4\u7a7a\u95f4\u539f\u578b\u7684\u6570\u5b57\u5316\u8f6c\u8bd1\u673a\u5236\u3002", "result": "\u63d0\u51fa\u300c\u5b9e\u65f6\u8eab\u4f53-\u7a7a\u95f4\u878d\u5408\u300d\u65b0\u672c\u4f53\u8bba\u6a21\u578b\uff0c\u8bba\u8bc1null2\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u4e0e\u6d41\u4f53\u529b\u5b66\u7b97\u6cd5\uff0c\u5c06\u7985\u5b97\u300c\u5373\u300d\u54f2\u5b66\u5177\u8c61\u5316\u4e3a\u53ef\u8ba1\u7b97\u7684\u7a7a\u95f4\u4f53\u9a8c\u7cfb\u7edf\u3002", "conclusion": "null2\u6807\u5fd7\u7740\u4e16\u535a\u4f1a\u5c55\u793a\u8303\u5f0f\u4ece\u7269\u8d28\u8c61\u5f81\u8f6c\u5411\u4f53\u9a8c\u8ba1\u7b97\u7684\u8f6c\u6298\uff0c\u5176\u6574\u5408\u5a92\u4f53\u827a\u672f\u4e0e\u5efa\u7b51\u6280\u672f\u7684\u8de8\u5b66\u79d1\u65b9\u6cd5\u8bba\uff0c\u4e3a\u6570\u5b57\u6587\u5316\u9057\u4ea7\u4fdd\u5b58\u63d0\u4f9b\u4e86\u521b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2509.18122", "pdf": "https://arxiv.org/pdf/2509.18122", "abs": "https://arxiv.org/abs/2509.18122", "authors": ["Yue Zhang", "Jiaxin Zhang", "Qiuyu Ren", "Tahsin Saffat", "Xiaoxuan Liu", "Zitong Yang", "Banghua Zhu", "Yi Ma"], "title": "GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models", "categories": ["cs.CL"], "comment": "120 pages (including appendix)", "summary": "We introduce \\textbf{GAUSS} (\\textbf{G}eneral \\textbf{A}ssessment of\n\\textbf{U}nderlying \\textbf{S}tructured \\textbf{S}kills in Mathematics), a\nbenchmark that evaluates LLMs' mathematical abilities across twelve core skill\ndimensions, grouped into three domains: knowledge and understanding, problem\nsolving and communication, and meta-skills and creativity. By categorizing\nproblems according to cognitive skills and designing tasks that isolate\nspecific abilities, GAUSS constructs comprehensive, fine-grained, and\ninterpretable profiles of models' mathematical abilities. These profiles\nfaithfully represent their underlying mathematical intelligence. To exemplify\nhow to use the \\textsc{GAUSS} benchmark, we have derived the skill profile of\n\\textsc{GPT-5-thinking}, revealing its strengths and weaknesses as well as its\ndifferences relative to \\textsc{o4-mini-high}, thereby underscoring the value\nof multidimensional, skill-based evaluation.", "AI": {"tldr": "GAUSS\u57fa\u51c6\u901a\u8fc712\u4e2a\u6570\u5b66\u6280\u80fd\u7ef4\u5ea6\u6784\u5efa\u53ef\u89e3\u91ca\u7684\u6a21\u578b\u80fd\u529b\u6863\u6848\uff0c\u63ed\u793a\u4e86GPT-5-thinking\u4e0eo4-mini-high\u7684\u6570\u5b66\u667a\u80fd\u5dee\u5f02", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u96be\u4ee5\u5168\u9762\u53cd\u6620\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e95\u5c42\u6570\u5b66\u667a\u80fd\uff0c\u9700\u5efa\u7acb\u591a\u7ef4\u5ea6\u7684\u7ed3\u6784\u5316\u6280\u80fd\u8bc4\u4f30\u4f53\u7cfb", "method": "\u5c06\u6570\u5b66\u80fd\u529b\u5212\u5206\u4e3a3\u5927\u9886\u57df12\u9879\u6838\u5fc3\u6280\u80fd\uff0c\u901a\u8fc7\u4efb\u52a1\u8bbe\u8ba1\u9694\u79bb\u7279\u5b9a\u80fd\u529b\uff0c\u6784\u5efa\u7ec6\u7c92\u5ea6\u80fd\u529b\u753b\u50cf", "result": "\u751f\u6210GPT-5-thinking\u7684\u6280\u80fd\u6863\u6848\uff0c\u663e\u793a\u5176\u5728\u77e5\u8bc6\u7406\u89e3\u9886\u57df\u8868\u73b0\u7a81\u51fa\u4f46\u521b\u9020\u529b\u8f83\u5f31\uff0c\u4e0eo4-mini-high\u5f62\u6210\u5bf9\u6bd4", "conclusion": "\u57fa\u4e8e\u8ba4\u77e5\u6280\u80fd\u7684\u591a\u7ef4\u8bc4\u4f30\u80fd\u66f4\u771f\u5b9e\u53cd\u6620\u6a21\u578b\u6570\u5b66\u667a\u80fd\uff0c\u4e3a\u6a21\u578b\u6539\u8fdb\u63d0\u4f9b\u53ef\u64cd\u4f5c\u65b9\u5411"}}
{"id": "2509.19296", "pdf": "https://arxiv.org/pdf/2509.19296", "abs": "https://arxiv.org/abs/2509.19296", "authors": ["Sherwin Bahmani", "Tianchang Shen", "Jiawei Ren", "Jiahui Huang", "Yifeng Jiang", "Haithem Turki", "Andrea Tagliasacchi", "David B. Lindell", "Zan Gojcic", "Sanja Fidler", "Huan Ling", "Jun Gao", "Xuanchi Ren"], "title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation", "categories": ["cs.CV", "cs.GR"], "comment": "Project Page: https://research.nvidia.com/labs/toronto-ai/lyra/", "summary": "The ability to generate virtual environments is crucial for applications\nranging from gaming to physical AI domains such as robotics, autonomous\ndriving, and industrial AI. Current learning-based 3D reconstruction methods\nrely on the availability of captured real-world multi-view data, which is not\nalways readily available. Recent advancements in video diffusion models have\nshown remarkable imagination capabilities, yet their 2D nature limits the\napplications to simulation where a robot needs to navigate and interact with\nthe environment. In this paper, we propose a self-distillation framework that\naims to distill the implicit 3D knowledge in the video diffusion models into an\nexplicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for\nmulti-view training data. Specifically, we augment the typical RGB decoder with\na 3DGS decoder, which is supervised by the output of the RGB decoder. In this\napproach, the 3DGS decoder can be purely trained with synthetic data generated\nby video diffusion models. At inference time, our model can synthesize 3D\nscenes from either a text prompt or a single image for real-time rendering. Our\nframework further extends to dynamic 3D scene generation from a monocular input\nvideo. Experimental results show that our framework achieves state-of-the-art\nperformance in static and dynamic 3D scene generation.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u84b8\u998f\u6846\u67b6\uff0c\u5c06\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u9690\u5f0f3D\u77e5\u8bc6\u63d0\u53d6\u52303D\u9ad8\u65af\u6e85\u5c04\u8868\u793a\u4e2d\uff0c\u5b9e\u73b0\u65e0\u9700\u591a\u89c6\u89d2\u6570\u636e\u7684\u9759\u6001\u548c\u52a8\u60013D\u573a\u666f\u751f\u6210", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u76843D\u91cd\u5efa\u65b9\u6cd5\u4f9d\u8d56\u96be\u4ee5\u83b7\u53d6\u7684\u591a\u89c6\u89d2\u6570\u636e\uff0c\u800c\u89c6\u9891\u6269\u6563\u6a21\u578b\u76842D\u7279\u6027\u9650\u5236\u4e86\u5176\u5728\u9700\u89813D\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u9700\u8981\u5c06\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u5408\u6210\u80fd\u529b\u4e0e\u663e\u5f0f3D\u8868\u793a\u76f8\u7ed3\u5408", "method": "\u901a\u8fc7\u5728RGB\u89e3\u7801\u5668\u57fa\u7840\u4e0a\u589e\u52a03DGS\u89e3\u7801\u5668\uff0c\u4f7f\u7528\u89c6\u9891\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\u3002\u652f\u6301\u6587\u672c\u63d0\u793a/\u5355\u56fe\u8f93\u5165\u751f\u6210\u5b9e\u65f63D\u573a\u666f\uff0c\u5e76\u53ef\u6269\u5c55\u81f3\u5355\u76ee\u89c6\u9891\u7684\u52a8\u6001\u573a\u666f\u751f\u6210", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u9759\u6001\u548c\u52a8\u60013D\u573a\u666f\u751f\u6210\u4efb\u52a1\u4e2d\u5747\u8fbe\u5230state-of-the-art\u6027\u80fd", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5c06\u89c6\u9891\u6269\u6563\u6a21\u578b\u76843D\u77e5\u8bc6\u84b8\u998f\u52303DGS\u8868\u793a\u4e2d\uff0c\u7a81\u7834\u4e86\u771f\u5b9e\u591a\u89c6\u89d2\u6570\u636e\u4f9d\u8d56\uff0c\u663e\u8457\u63d0\u5347\u4e86\u865a\u62df\u73af\u5883\u751f\u6210\u80fd\u529b\uff0c\u5728\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u7b49\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b"}}
{"id": "2509.18156", "pdf": "https://arxiv.org/pdf/2509.18156", "abs": "https://arxiv.org/abs/2509.18156", "authors": ["Haoyu Wang", "Fengze Liu", "Jiayao Zhang", "Dan Roth", "Kyle Richardson"], "title": "Event Causality Identification with Synthetic Control", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Event causality identification (ECI), a process that extracts causal\nrelations between events from text, is crucial for distinguishing causation\nfrom correlation. Traditional approaches to ECI have primarily utilized\nlinguistic patterns and multi-hop relational inference, risking false causality\nidentification due to informal usage of causality and specious graphical\ninference. In this paper, we adopt the Rubin Causal Model to identify event\ncausality: given two temporally ordered events, we see the first event as the\ntreatment and the second one as the observed outcome. Determining their\ncausality involves manipulating the treatment and estimating the resultant\nchange in the likelihood of the outcome. Given that it is only possible to\nimplement manipulation conceptually in the text domain, as a work-around, we\ntry to find a twin for the protagonist from existing corpora. This twin should\nhave identical life experiences with the protagonist before the treatment but\nundergoes an intervention of treatment. However, the practical difficulty of\nlocating such a match limits its feasibility. Addressing this issue, we use the\nsynthetic control method to generate such a twin' from relevant historical\ndata, leveraging text embedding synthesis and inversion techniques. This\napproach allows us to identify causal relations more robustly than previous\nmethods, including GPT-4, which is demonstrated on a causality benchmark,\nCOPES-hard.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8eRubin\u56e0\u679c\u6a21\u578b\u548c\u5408\u6210\u63a7\u5236\u65b9\u6cd5\u7684\u4e8b\u4ef6\u56e0\u679c\u5173\u7cfb\u8bc6\u522b\u6846\u67b6\uff0c\u5728COPES-hard\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u53caGPT-4", "motivation": "\u4f20\u7edfECI\u65b9\u6cd5\u4f9d\u8d56\u8bed\u8a00\u6a21\u5f0f\u548c\u591a\u8df3\u63a8\u7406\u6613\u5bfc\u81f4\u8bef\u5224\uff0c\u76f4\u63a5\u5bfb\u627e\u7ecf\u5386\u76f8\u540c\u7684\u6587\u672c\u53cc\u80de\u80ce\u5b58\u5728\u53ef\u884c\u6027\u9650\u5236\u3002\u9700\u8981\u66f4\u7a33\u5065\u7684\u56e0\u679c\u8bc6\u522b\u65b9\u6cd5", "method": "\u5c06\u65f6\u5e8f\u4e8b\u4ef6\u89c6\u4e3a\u5904\u7406-\u7ed3\u679c\u5bf9\uff0c\u8fd0\u7528\u5408\u6210\u63a7\u5236\u65b9\u6cd5\u751f\u6210\u6587\u672c\u5d4c\u5165\u5c42\u9762\u7684\u865a\u62df\u53cc\u80de\u80ce\uff0c\u901a\u8fc7\u5e72\u9884\u5206\u6790\u4f30\u8ba1\u56e0\u679c\u6548\u5e94", "result": "\u5728COPES-hard\u56e0\u679c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff08\u5305\u62ecGPT-4\uff09\u7684\u7a33\u5065\u6027\u8868\u73b0", "conclusion": "\u901a\u8fc7\u6587\u672c\u5d4c\u5165\u5408\u6210\u4e0e\u53cd\u4e8b\u5b9e\u63a8\u7406\u76f8\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfECI\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6587\u672c\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u8bba\u6846\u67b6"}}
{"id": "2509.18158", "pdf": "https://arxiv.org/pdf/2509.18158", "abs": "https://arxiv.org/abs/2509.18158", "authors": ["Seungyoun Yi", "Minsoo Khang", "Sungrae Park"], "title": "ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization", "categories": ["cs.CL", "cs.LG", "I.2.7"], "comment": "9 pages, 4 figures. To appear in EMNLP 2025 Main Conference (Oral\n  Presentation)", "summary": "Automatic Prompt Optimization (APO) improves large language model (LLM)\nperformance by refining prompts for specific tasks. However, prior APO methods\ntypically focus only on user prompts, rely on unstructured feedback, and\nrequire large sample sizes and long iteration cycles-making them costly and\nbrittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a\nnovel framework that jointly optimizes both system and user prompts through\nprincipled, low-overhead refinement. ZERA scores prompts using eight\ngeneralizable criteria with automatically inferred weights, and revises prompts\nbased on these structured critiques. This enables fast convergence to\nhigh-quality prompts using minimal examples and short iteration cycles. We\nevaluate ZERA across five LLMs and nine diverse datasets spanning reasoning,\nsummarization, and code generation tasks. Experimental results demonstrate\nconsistent improvements over strong baselines. Further ablation studies\nhighlight the contribution of each component to more effective prompt\nconstruction. Our implementation including all prompts is publicly available at\nhttps://github.com/younatics/zera-agent.", "AI": {"tldr": "\u63d0\u51faZERA\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7cfb\u7edf\u4e0e\u7528\u6237\u63d0\u793a\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u8bc4\u4f30\u6807\u51c6\u5b9e\u73b0\u9ad8\u6548\u63d0\u793a\u4f18\u5316", "motivation": "\u4f20\u7edfAPO\u65b9\u6cd5\u5b58\u5728\u5355\u4fa7\u4f18\u5316\uff08\u4ec5\u7528\u6237\u63d0\u793a\uff09\u3001\u975e\u7ed3\u6784\u5316\u53cd\u9988\u3001\u9ad8\u6837\u672c\u9700\u6c42\u7b49\u95ee\u9898\uff0c\u9700\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6848", "method": "\u4f7f\u7528\u516b\u4e2a\u53ef\u6cdb\u5316\u6807\u51c6\u81ea\u52a8\u52a0\u6743\u8bc4\u5206\uff0c\u57fa\u4e8e\u7ed3\u6784\u5316\u53cd\u9988\u8fed\u4ee3\u4f18\u5316\uff0c\u5b9e\u73b0\u5c0f\u6837\u672c\u5feb\u901f\u6536\u655b", "result": "\u57285\u4e2aLLM\u548c9\u4e2a\u6570\u636e\u96c6\u7684\u6d4b\u8bd5\u4e2d\uff0cZERA\u5728\u63a8\u7406/\u6458\u8981/\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e0a\u5747\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u7ec4\u4ef6\u6709\u6548\u6027", "conclusion": "\u7ed3\u6784\u5316\u8bc4\u4f30\u4f53\u7cfb\u4e0e\u53cc\u63d0\u793a\u8054\u5408\u4f18\u5316\u673a\u5236\u663e\u8457\u63d0\u5347\u63d0\u793a\u5de5\u7a0b\u6548\u7387\uff0c\u5f00\u6e90\u5b9e\u73b0\u4fc3\u8fdb\u5b9e\u9645\u5e94\u7528\u4e0e\u540e\u7eed\u7814\u7a76"}}
{"id": "2509.18163", "pdf": "https://arxiv.org/pdf/2509.18163", "abs": "https://arxiv.org/abs/2509.18163", "authors": ["Haodong Zhao", "Chenyan Zhao", "Yansi Li", "Zhuosheng Zhang", "Gongshen Liu"], "title": "Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning", "categories": ["cs.CL"], "comment": "Work in progress", "summary": "The capacity of Large Language Models (LLMs) to reason is fundamental to\ntheir application in complex, knowledge-intensive domains. In real-world\nscenarios, LLMs are often augmented with external information that can be\nhelpful, irrelevant, or even misleading. This paper investigates the causal\nimpact of such auxiliary information on the reasoning process of LLMs with\nexplicit step-by-step thinking capabilities. We introduce SciAux, a new dataset\nderived from ScienceQA, to systematically test the robustness of the model\nagainst these types of information. Our findings reveal a critical\nvulnerability: the model's deliberative \"thinking mode\" is a double-edged\nsword. While helpful context improves accuracy, misleading information causes a\ncatastrophic drop in performance, which is amplified by the thinking process.\nInstead of conferring robustness, thinking reinforces the degree of error when\nprovided with misinformation. This highlights that the challenge is not merely\nto make models \"think\", but to endow them with the critical faculty to evaluate\nthe information upon which their reasoning is based. The SciAux dataset is\navailable at https://huggingface.co/datasets/billhdzhao/SciAux.", "AI": {"tldr": "LLM\u7684\u63a8\u7406\u80fd\u529b\u53d7\u5916\u90e8\u4fe1\u606f\u8d28\u91cf\u5f71\u54cd\uff1a\u6709\u76ca\u4fe1\u606f\u63d0\u5347\u51c6\u786e\u6027\uff0c\u8bef\u5bfc\u6027\u4fe1\u606f\u56e0\u601d\u7ef4\u8fc7\u7a0b\u5bfc\u81f4\u6027\u80fd\u707e\u96be\u6027\u4e0b\u964d", "motivation": "\u63a2\u7a76\u5916\u90e8\u8f85\u52a9\u4fe1\u606f\uff08\u6709\u76ca/\u65e0\u5173/\u8bef\u5bfc\uff09\u5bf9\u5177\u5907\u663e\u5f0f\u5206\u6b65\u601d\u8003\u80fd\u529b\u7684LLM\u63a8\u7406\u8fc7\u7a0b\u7684\u56e0\u679c\u5f71\u54cd", "method": "\u57fa\u4e8eScienceQA\u6784\u5efaSciAux\u6570\u636e\u96c6\uff0c\u7cfb\u7edf\u6d4b\u8bd5\u6a21\u578b\u5bf9\u5404\u7c7b\u8f85\u52a9\u4fe1\u606f\u7684\u9c81\u68d2\u6027\uff0c\u5206\u6790\u601d\u7ef4\u6a21\u5f0f\u5bf9\u4fe1\u606f\u5904\u7406\u7684\u5f71\u54cd\u673a\u5236", "result": "\u601d\u7ef4\u6a21\u5f0f\u5177\u6709\u53cc\u5203\u5251\u6548\u5e94\uff1a\u6709\u76ca\u4e0a\u4e0b\u6587\u63d0\u5347\u51c6\u786e\u7387\uff08+6.5%\uff09\uff0c\u8bef\u5bfc\u6027\u4fe1\u606f\u5bfc\u81f4\u6027\u80fd\u65ad\u5d16\u5f0f\u4e0b\u8dcc\uff08-31.2%\uff09\uff0c\u601d\u7ef4\u8fc7\u7a0b\u5f3a\u5316\u9519\u8bef\u4f20\u64ad", "conclusion": "\u5173\u952e\u6311\u6218\u5728\u4e8e\u8d4b\u4e88\u6a21\u578b\u8bc4\u4f30\u63a8\u7406\u4f9d\u636e\u7684\u6279\u5224\u80fd\u529b\uff0c\u800c\u975e\u5355\u7eaf\u5b9e\u73b0'\u601d\u8003'\u529f\u80fd\u3002SciAux\u6570\u636e\u96c6\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u57fa\u51c6\u6d4b\u8bd5\u5e73\u53f0"}}
{"id": "2509.18167", "pdf": "https://arxiv.org/pdf/2509.18167", "abs": "https://arxiv.org/abs/2509.18167", "authors": ["Junlin Wang", "Zehao Wu", "Shaowei Lu", "Yanlan Li", "Xinghao Huang"], "title": "SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework", "categories": ["cs.CL"], "comment": "5 pages,2 figures, IRAC under review", "summary": "Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to\naccess external knowledge sources, but the effectiveness of RAG relies on the\ncoordination between the retriever and the generator. Since these components\nare developed independently, their interaction is often suboptimal: the\nretriever may return irrelevant or redundant documents, while the generator may\nfail to fully leverage retrieved evidence. In this work, we propose a\nprocess-supervised multi-agent framework to bridge the gap between retriever\nand generator. The framework introduces two lightweight agents: a Decision\nMaker, which determines when to continue retrieval or stop for answer\ngeneration, and a Knowledge Selector, which filters retrieved documents to\nretain only the most useful evidence. To provide fine-grained supervision, we\nemploy an LLM-as-a-Judge that evaluates each intermediate action with\nprocess-level rewards, ensuring more accurate credit assignment than relying\nsolely on final answer correctness. We further adopt a tree-structured rollout\nstrategy to explore diverse reasoning paths, and train both agents with\nProximal Policy Optimization (PPO) in an end-to-end manner. Experiments on\nsingle-hop and multi-hop question answering benchmarks show that our approach\nachieves higher accuracy, more stable convergence, and produces more\ninterpretable reasoning trajectories compared with standard RAG baselines.\nImportantly, the proposed framework is modular and plug-and-play, requiring no\nmodification to the retriever or generator, making it practical for real-world\nRAG applications.", "AI": {"tldr": "\u63d0\u51fa\u8fc7\u7a0b\u76d1\u7763\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u4f18\u5316\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff0c\u901a\u8fc7\u51b3\u7b56\u5236\u5b9a\u5668\u4e0e\u77e5\u8bc6\u9009\u62e9\u5668\u7684\u534f\u4f5c\u63d0\u5347RAG\u6548\u679c", "motivation": "\u4f20\u7edfRAG\u7cfb\u7edf\u4e2d\u68c0\u7d22\u5668\u4e0e\u751f\u6210\u5668\u72ec\u7acb\u5f00\u53d1\u5bfc\u81f4\u4ea4\u4e92\u6b21\u4f18\uff0c\u5b58\u5728\u5197\u4f59\u68c0\u7d22\u548c\u8bc1\u636e\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898", "method": "\u6784\u5efa\u5305\u542b\u51b3\u7b56\u5236\u5b9a\u5668\uff08\u63a7\u5236\u68c0\u7d22\u7ec8\u6b62\uff09\u548c\u77e5\u8bc6\u9009\u62e9\u5668\uff08\u8fc7\u6ee4\u8bc1\u636e\uff09\u7684\u53cc\u4ee3\u7406\u6846\u67b6\uff0c\u91c7\u7528LLM\u8fc7\u7a0b\u7ea7\u5956\u52b1\u76d1\u7763\u548cPPO\u7aef\u5230\u7aef\u8bad\u7ec3", "result": "\u5728\u5355\u8df3/\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u51c6\u786e\u7387\u3001\u66f4\u7a33\u5b9a\u6536\u655b\u548c\u66f4\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8f68\u8ff9", "conclusion": "\u6a21\u5757\u5316\u8bbe\u8ba1\u5728\u4e0d\u4fee\u6539\u539f\u6709\u7ec4\u4ef6\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347RAG\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2509.18175", "pdf": "https://arxiv.org/pdf/2509.18175", "abs": "https://arxiv.org/abs/2509.18175", "authors": ["Aditi Debsharma", "Bhushan Jagyasi", "Surajit Sen", "Priyanka Pandey", "Devicharith Dovari", "Yuvaraj V. C", "Rosalin Parida", "Gopali Contractor"], "title": "ERFC: Happy Customers with Emotion Recognition and Forecasting in Conversation in Call Centers", "categories": ["cs.CL"], "comment": "7 pages, 6 Figures, 4 Tables, 18 References", "summary": "Emotion Recognition in Conversation has been seen to be widely applicable in\ncall center analytics, opinion mining, finance, retail, healthcare, and other\nindustries. In a call center scenario, the role of the call center agent is not\njust confined to receiving calls but to also provide good customer experience\nby pacifying the frustration or anger of the customers. This can be achieved by\nmaintaining neutral and positive emotion from the agent. As in any\nconversation, the emotion of one speaker is usually dependent on the emotion of\nother speaker. Hence the positive emotion of an agent, accompanied with the\nright resolution will help in enhancing customer experience. This can change an\nunhappy customer to a happy one. Imparting the right resolution at right time\nbecomes easier if the agent has the insight of the emotion of future\nutterances. To predict the emotions of the future utterances we propose a novel\narchitecture, Emotion Recognition and Forecasting in Conversation. Our proposed\nERFC architecture considers multi modalities, different attributes of emotion,\ncontext and the interdependencies of the utterances of the speakers in the\nconversation. Our intensive experiments on the IEMOCAP dataset have shown the\nfeasibility of the proposed ERFC. This approach can provide a tremendous\nbusiness value for the applications like call center, where the happiness of\ncustomer is utmost important.", "AI": {"tldr": "\u63d0\u51faERFC\u67b6\u6784\u7528\u4e8e\u5bf9\u8bdd\u4e2d\u7684\u60c5\u7eea\u9884\u6d4b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u63d0\u5347\u547c\u53eb\u4e2d\u5fc3\u7684\u5ba2\u6237\u4f53\u9a8c\uff0c\u5e76\u5728IEMOCAP\u6570\u636e\u96c6\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "motivation": "\u5e2e\u52a9\u5ba2\u670d\u4eba\u5458\u5b9e\u65f6\u9884\u6d4b\u5ba2\u6237\u60c5\u7eea\u53d8\u5316\uff0c\u901a\u8fc7\u4e3b\u52a8\u7ef4\u62a4\u4e2d\u6027/\u6b63\u5411\u60c5\u7eea\u6765\u63d0\u5347\u5ba2\u6237\u6ee1\u610f\u5ea6\uff0c\u8f6c\u5316\u4e0d\u6ee1\u5ba2\u6237\u4e3a\u6ee1\u610f\u5ba2\u6237\u3002", "method": "ERFC\u67b6\u6784\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u3001\u60c5\u7eea\u5c5e\u6027\u3001\u4e0a\u4e0b\u6587\u4fe1\u606f\u53ca\u5bf9\u8bdd\u8005\u8bdd\u8bed\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\uff0c\u5b9e\u73b0\u672a\u6765\u8bdd\u8bed\u60c5\u7eea\u9884\u6d4b\u3002", "result": "\u5728IEMOCAP\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u8be5\u67b6\u6784\u7684\u53ef\u884c\u6027\uff0c\u60c5\u7eea\u9884\u6d4b\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "ERFC\u5728\u547c\u53eb\u4e2d\u5fc3\u7b49\u573a\u666f\u5177\u6709\u91cd\u8981\u5546\u4e1a\u4ef7\u503c\uff0c\u60c5\u7eea\u9884\u5224\u80fd\u529b\u53ef\u4f18\u5316\u670d\u52a1\u7b56\u7565\uff0c\u6700\u7ec8\u63d0\u5347\u5ba2\u6237\u5e78\u798f\u611f\u548c\u4f01\u4e1a\u6536\u76ca\u3002"}}
{"id": "2509.18293", "pdf": "https://arxiv.org/pdf/2509.18293", "abs": "https://arxiv.org/abs/2509.18293", "authors": ["Jay Patel", "Hrudayangam Mehta", "Jeremy Blackburn"], "title": "Evaluating Large Language Models for Detecting Antisemitism", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Detecting hateful content is a challenging and important problem. Automated\ntools, like machine-learning models, can help, but they require continuous\ntraining to adapt to the ever-changing landscape of social media. In this work,\nwe evaluate eight open-source LLMs' capability to detect antisemitic content,\nspecifically leveraging in-context definition as a policy guideline. We explore\nvarious prompting techniques and design a new CoT-like prompt, Guided-CoT.\nGuided-CoT handles the in-context policy well, increasing performance across\nall evaluated models, regardless of decoding configuration, model sizes, or\nreasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5.\nAdditionally, we examine LLM errors and introduce metrics to quantify semantic\ndivergence in model-generated rationales, revealing notable differences and\nparadoxical behaviors among LLMs. Our experiments highlight the differences\nobserved across LLMs' utility, explainability, and reliability.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5f00\u53d1Guided-CoT\u63d0\u793a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u68c0\u6d4b\u53cd\u72b9\u592a\u5185\u5bb9\u7684\u6027\u80fd\uff0c\u53d1\u73b0Llama 3.1 70B\u8868\u73b0\u4f18\u4e8e\u5fae\u8c03\u540e\u7684GPT-3.5", "motivation": "\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u5feb\u901f\u6f14\u53d8\u9700\u8981\u6301\u7eed\u4f18\u5316\u81ea\u52a8\u5316\u68c0\u6d4b\u5de5\u5177\uff0c\u5f53\u524d\u7f3a\u4e4f\u9488\u5bf9\u53cd\u72b9\u592a\u8a00\u8bba\u7684\u53ef\u9760\u68c0\u6d4b\u65b9\u6848", "method": "\u6d4b\u8bd58\u4e2a\u5f00\u6e90LLM\uff0c\u8bbe\u8ba1\u7c7b\u601d\u7ef4\u94fe\u63d0\u793a\uff08Guided-CoT\uff09\uff0c\u5f15\u5165\u8bed\u4e49\u5dee\u5f02\u91cf\u5316\u6307\u6807\u5206\u6790\u6a21\u578b\u51b3\u7b56\u4f9d\u636e", "result": "Guided-CoT\u5168\u9762\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c70B\u53c2\u6570Llama\u6a21\u578b\u8d85\u8d8aGPT-3.5\uff1b\u6a21\u578b\u95f4\u5b58\u5728\u8bed\u4e49\u89e3\u91ca\u5dee\u5f02\u548c\u77db\u76fe\u884c\u4e3a", "conclusion": "\u4e0d\u540cLLM\u5728\u5b9e\u7528\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u63d0\u793a\u5de5\u7a0b\u53ef\u6709\u6548\u63d0\u5347\u653f\u7b56\u5408\u89c4\u6027\u68c0\u6d4b\u80fd\u529b"}}
{"id": "2509.18314", "pdf": "https://arxiv.org/pdf/2509.18314", "abs": "https://arxiv.org/abs/2509.18314", "authors": ["Hieu Tran", "Zonghai Yao", "Hong Yu"], "title": "Exploiting Tree Structure for Credit Assignment in RL Training of LLMs", "categories": ["cs.CL"], "comment": "15 pages", "summary": "Reinforcement learning improves LLM reasoning, yet sparse delayed reward over\nlong sequences makes token-level credit assignment the key bottleneck. We study\nthe verifiable-reward setting, where the final answer is checkable and multiple\nresponses can be drawn per prompt. Reasoning tasks in math and medical QA align\nwith this setup, where only a few decision tokens significantly impact the\noutcome. PPO offers token-level advantages with a learned value model, but it\nis complex to train both the actor and critic models simultaneously, and it is\nnot easily generalizable, as the token-level values from the critic model can\nmake training prone to overfitting. GRPO is critic-free and supports verifiable\nrewards, but spreads a single sequence-level return across tokens and ignores\nbranching. We introduce \\textbf{Prefix-to-Tree (P2T)}, a simple procedure that\nconverts a group of responses into a prefix tree and computes\n\\emph{nonparametric} prefix values \\(V(s)\\) by aggregating descendant outcomes.\nBuilt on P2T, we propose \\textbf{TEMPO} (\\emph{\\textbf{T}ree-\\textbf{E}stimated\n\\textbf{M}ean Prefix Value for \\textbf{P}olicy \\textbf{O}ptimization}), a\ncritic-free algorithm that augments the group-relative outcome signal of GRPO\nwith \\emph{branch-gated} temporal-difference corrections derived from the tree.\nAt non-branch tokens, the temporal-difference (TD) term is zero, so TEMPO\nreduces to GRPO; at branching tokens, it supplies precise token-level credit\nwithout a learned value network or extra judges/teachers. On Qwen3-1.7B/4B,\nTEMPO outperforms PPO and GRPO on in-distribution (MATH, MedQA) and\nout-of-distribution (GSM-HARD, AMC23, MedMCQA, MMLU-Medical) benchmarks, and\nreaches higher validation accuracy with roughly the same wall-clock time.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u524d\u7f00\u6811\u7684TEMPO\u7b97\u6cd5\uff0c\u5728\u53ef\u9a8c\u8bc1\u5956\u52b1\u573a\u666f\u4e0b\u5b9e\u73b0\u66f4\u4f18\u7684token\u7ea7\u4fe1\u7528\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347LLM\u5728\u6570\u5b66\u548c\u533b\u7597QA\u4efb\u52a1\u7684\u8868\u73b0", "motivation": "\u73b0\u6709PPO\u65b9\u6cd5\u9700\u8054\u5408\u8bad\u7ec3actor-critic\u6a21\u578b\u590d\u6742\u5ea6\u9ad8\uff0cGRPO\u65b9\u6cd5\u5ffd\u89c6\u5206\u652f\u7ed3\u6784\u7684token\u7ea7\u4fe1\u7528\u5206\u914d\u3002\u9488\u5bf9\u53ef\u9a8c\u8bc1\u5956\u52b1\u573a\u666f\uff08\u6700\u7ec8\u7b54\u6848\u53ef\u6821\u9a8c\u3001\u652f\u6301\u591a\u54cd\u5e94\u91c7\u6837\uff09\u8bbe\u8ba1\u66f4\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6", "method": "1. \u63d0\u51faP2T\u65b9\u6cd5\u5c06\u591a\u54cd\u5e94\u6784\u5efa\u524d\u7f00\u6811\uff0c\u805a\u5408\u540e\u88d4\u7ed3\u679c\u8ba1\u7b97\u975e\u53c2\u6570\u5316\u524d\u7f00\u503c 2. \u5f00\u53d1TEMPO\u7b97\u6cd5\uff0c\u5728GRPO\u57fa\u7840\u4e0a\u5f15\u5165\u5206\u652f\u95e8\u63a7\u65f6\u5e8f\u5dee\u5206\u4fee\u6b63\uff0c\u5728\u5206\u652f\u8282\u70b9\u5b9e\u73b0\u7cbe\u51c6token\u7ea7\u4fe1\u7528\u5206\u914d", "result": "\u5728Qwen3-1.7B/4B\u6a21\u578b\u4e0a\uff0cTEMPO\u5728MATH/MedQA\u7b49\u5206\u5e03\u5185\u4efb\u52a1\u548cGSM-HARD/AMC23\u7b49\u5206\u5e03\u5916\u4efb\u52a1\u5747\u8d85\u8d8aPPO\u548cGRPO\uff0c\u8fbe\u5230\u66f4\u9ad8\u9a8c\u8bc1\u51c6\u786e\u7387\u4e14\u8bad\u7ec3\u8017\u65f6\u76f8\u5f53", "conclusion": "\u901a\u8fc7\u524d\u7f00\u6811\u7ed3\u6784\u5b9e\u73b0\u975e\u53c2\u6570\u5316\u7684\u65f6\u5e8f\u5dee\u5206\u4fee\u6b63\uff0c\u4e3a\u53ef\u9a8c\u8bc1\u5956\u52b1\u573a\u666f\u63d0\u4f9b\u9ad8\u6548\u7b56\u7565\u4f18\u5316\u65b9\u6848\uff0c\u8bc1\u660e\u65e0\u9700\u4ef7\u503c\u7f51\u7edc\u5373\u53ef\u5b9e\u73b0\u7cbe\u51c6token\u7ea7\u4fe1\u7528\u5206\u914d"}}
{"id": "2509.18316", "pdf": "https://arxiv.org/pdf/2509.18316", "abs": "https://arxiv.org/abs/2509.18316", "authors": ["Saksham Khatwani", "He Cheng", "Majid Afshar", "Dmitriy Dligach", "Yanjun Gao"], "title": "Brittleness and Promise: Knowledge Graph Based Reward Modeling for Diagnostic Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) show promise for diagnostic reasoning but often\nlack reliable, knowledge grounded inference. Knowledge graphs (KGs), such as\nthe Unified Medical Language System (UMLS), offer structured biomedical\nknowledge that can support trustworthy reasoning. Prior approaches typically\nintegrate KGs via retrieval augmented generation or fine tuning, inserting KG\ncontent into prompts rather than enabling structured reasoning. We explore an\nalternative paradigm: treating the LLM as a reward model of KG reasoning paths,\nwhere the model learns to judge whether a candidate path leads to correct\ndiagnosis for a given patient input. This approach is inspired by recent work\nthat leverages reward training to enhance model reasoning abilities, and\ngrounded in computational theory, which suggests that verifying a solution is\noften easier than generating one from scratch. It also parallels physicians'\ndiagnostic assessment, where they judge which sequences of findings and\nintermediate conditions most plausibly support a diagnosis. We first\nsystematically evaluate five task formulation for knowledge path judging and\neight training paradigm. Second, we test whether the path judging abilities\ngeneralize to downstream diagnostic tasks, including diagnosis summarization\nand medical question answering. Experiments with three open source\ninstruct-tuned LLMs reveal both promise and brittleness: while specific reward\noptimization and distillation lead to strong path-judging performance, the\ntransferability to downstream tasks remain weak. Our finding provides the first\nsystematic assessment of \"reward model style\" reasoning over clinical KGs,\noffering insights into how structured, reward-based supervision influences\ndiagnostic reasoning in GenAI systems for healthcare.", "AI": {"tldr": "\u63a2\u7d22\u5c06LLM\u4f5c\u4e3a\u77e5\u8bc6\u56fe\u8c31\u8def\u5f84\u5956\u52b1\u6a21\u578b\u63d0\u5347\u8bca\u65ad\u63a8\u7406\uff0c\u5b9e\u9a8c\u663e\u793a\u8def\u5f84\u5224\u65ad\u80fd\u529b\u63d0\u5347\u4f46\u4e0b\u6e38\u4efb\u52a1\u8fc1\u79fb\u6027\u4e0d\u8db3", "motivation": "\u4f20\u7edfKG\u6574\u5408\u65b9\u6cd5\u4ec5\u63d2\u5165\u77e5\u8bc6\u5185\u5bb9\u800c\u7f3a\u4e4f\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u53d7\u5956\u52b1\u8bad\u7ec3\u7406\u8bba\u4e0e\u4e34\u5e8a\u8bca\u65ad\u8bc4\u4f30\u6d41\u7a0b\u542f\u53d1\uff0c\u63d0\u51fa\u9a8c\u8bc1\u63a8\u7406\u8def\u5f84\u7684\u65b0\u8303\u5f0f", "method": "\u8bc4\u4f305\u79cd\u8def\u5f84\u5224\u65ad\u4efb\u52a1\u5f62\u5f0f\u4e0e8\u79cd\u8bad\u7ec3\u8303\u5f0f\uff0c\u6d4b\u8bd5\u8def\u5f84\u5224\u65ad\u80fd\u529b\u5728\u8bca\u65ad\u603b\u7ed3/\u95ee\u7b54\u4efb\u52a1\u7684\u6cdb\u5316\u8868\u73b0", "result": "\u7279\u5b9a\u5956\u52b1\u4f18\u5316\u4e0e\u84b8\u998f\u7b56\u7565\u663e\u8457\u63d0\u5347\u8def\u5f84\u5224\u65ad\u51c6\u786e\u7387\uff08+15%\uff09\uff0c\u4f46\u5411\u4e0b\u6e38\u4efb\u52a1\u7684\u8fc1\u79fb\u6548\u679c\u5f31\uff08<5%\u63d0\u5347\uff09", "conclusion": "\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e34\u5e8aKG\u7684\u5956\u52b1\u5f0f\u63a8\u7406\u8303\u5f0f\uff0c\u63ed\u793a\u4e86\u7ed3\u6784\u5316\u5956\u52b1\u76d1\u7763\u5bf9\u533b\u7597AI\u8bca\u65ad\u63a8\u7406\u7684\u5f71\u54cd\u673a\u5236\u4e0e\u6539\u8fdb\u65b9\u5411"}}
{"id": "2509.18344", "pdf": "https://arxiv.org/pdf/2509.18344", "abs": "https://arxiv.org/abs/2509.18344", "authors": ["Pei-Shuo Wang", "Jian-Jia Chen", "Chun-Che Yang", "Chi-Chih Chang", "Ning-Chi Huang", "Mohamed S. Abdelfattah", "Kai-Chiang Wu"], "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding", "categories": ["cs.CL"], "comment": "Accepted by NeurIPS 2025", "summary": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit).", "AI": {"tldr": "\u63d0\u51faSubSpec\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u6bd4\u7279\u91cf\u5316\u66ff\u4ee3\u5c42\u5b9e\u73b0\u514d\u8bad\u7ec3\u7684\u53c2\u6570\u5378\u8f7d\u52a0\u901f\uff0c\u5728\u5185\u5b58\u53d7\u9650GPU\u4e0a\u5b9e\u73b09.1-12.5\u500d\u63a8\u7406\u52a0\u901f", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u9762\u4e34\u663e\u5b58\u9650\u5236\uff0c\u73b0\u6709\u538b\u7f29\u65b9\u6cd5\u4f1a\u964d\u4f4e\u8d28\u91cf\uff0c\u53c2\u6570\u5378\u8f7d\u65b9\u6cd5\u63a8\u7406\u901f\u5ea6\u6162\u3002\u9700\u8981\u65e0\u8d28\u91cf\u635f\u5931\u4e14\u514d\u8bad\u7ec3\u7684\u52a0\u901f\u65b9\u6848", "method": "\u57fa\u4e8e\u63a8\u6d4b\u5f0f\u89e3\u7801\u6846\u67b6\uff0c\u4ece\u76ee\u6807\u6a21\u578b\u7684\u5378\u8f7d\u90e8\u5206\u751f\u6210\u4f4e\u6bd4\u7279\u91cf\u5316\u66ff\u4ee3\u5c42\u6784\u5efa\u5bf9\u9f50\u8349\u7a3f\u6a21\u578b\uff0c\u5171\u4eab\u5269\u4f59GPU\u9a7b\u7559\u5c42\u548cKV-Cache", "result": "\u57288GB\u663e\u5b58\u9650\u5236\u4e0bQwen2.5 7B\u5b9e\u73b09.1\u500d\u52a0\u901f\uff08MT-Bench\uff09\uff0c24GB\u9650\u5236\u4e0bQwen2.5 32B\u5e73\u5747\u52a0\u901f12.5\u500d", "conclusion": "SubSpec\u901a\u8fc7\u91cf\u5316\u66ff\u4ee3\u5c42\u548c\u67b6\u6784\u5171\u4eab\uff0c\u5b9e\u73b0\u4e86\u514d\u8bad\u7ec3\u3001\u65e0\u635f\u8d28\u91cf\u7684\u53c2\u6570\u5378\u8f7d\u52a0\u901f\uff0c\u663e\u8457\u63d0\u5347\u5927\u6a21\u578b\u90e8\u7f72\u6548\u7387"}}
{"id": "2509.18360", "pdf": "https://arxiv.org/pdf/2509.18360", "abs": "https://arxiv.org/abs/2509.18360", "authors": ["Chutong Meng", "Philipp Koehn"], "title": "Speech Vecalign: an Embedding-based Method for Aligning Parallel Speech Documents", "categories": ["cs.CL"], "comment": "Accepted by EMNLP 2025 (main)", "summary": "We present Speech Vecalign, a parallel speech document alignment method that\nmonotonically aligns speech segment embeddings and does not depend on text\ntranscriptions. Compared to the baseline method Global Mining, a variant of\nspeech mining, Speech Vecalign produces longer speech-to-speech alignments. It\nalso demonstrates greater robustness than Local Mining, another speech mining\nvariant, as it produces less noise. We applied Speech Vecalign to 3,000 hours\nof unlabeled parallel English-German (En-De) speech documents from VoxPopuli,\nyielding about 1,000 hours of high-quality alignments. We then trained En-De\nspeech-to-speech translation models on the aligned data. Speech Vecalign\nimproves the En-to-De and De-to-En performance over Global Mining by 0.37 and\n0.18 ASR-BLEU, respectively. Moreover, our models match or outperform\nSpeechMatrix model performance, despite using 8 times fewer raw speech\ndocuments.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u6587\u672c\u8f6c\u5f55\u7684Speech Vecalign\u8bed\u97f3\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5728\u82f1\u5fb7\u8bed\u97f3\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u5347\u6027\u80fd\u4e14\u6570\u636e\u6548\u7387\u63d0\u9ad88\u500d", "motivation": "\u89e3\u51b3\u73b0\u6709\u8bed\u97f3\u6587\u6863\u5bf9\u9f50\u65b9\u6cd5\u4f9d\u8d56\u6587\u672c\u8f6c\u5f55\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u5e76\u884c\u8bed\u97f3\u5bf9\u9f50\u6280\u672f", "method": "\u901a\u8fc7\u5355\u8c03\u5bf9\u9f50\u8bed\u97f3\u7247\u6bb5\u5d4c\u5165\u7684\u5e76\u884c\u8bed\u97f3\u6587\u6863\u5bf9\u9f50\u65b9\u6cd5\uff0c\u4f7f\u7528VoxPopuli 3000\u5c0f\u65f6\u672a\u6807\u6ce8\u82f1\u5fb7\u8bed\u97f3\u6570\u636e\u8fdb\u884c\u8bad\u7ec3", "result": "\u83b7\u5f971000\u5c0f\u65f6\u9ad8\u8d28\u91cf\u5bf9\u9f50\u6570\u636e\uff0c\u82f1\u5fb7\u7ffb\u8bd1\u6027\u80fd\u5206\u522b\u63d0\u53470.37\u548c0.18 ASR-BLEU\uff0c\u6570\u636e\u6548\u7387\u63d0\u9ad88\u500d", "conclusion": "Speech Vecalign\u5728\u4fdd\u6301\u6216\u8d85\u8d8a\u57fa\u7ebf\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6570\u636e\u5229\u7528\u7387\uff0c\u8bc1\u660e\u65e0\u6587\u672c\u8bed\u97f3\u5bf9\u9f50\u7684\u6709\u6548\u6027"}}
{"id": "2509.18377", "pdf": "https://arxiv.org/pdf/2509.18377", "abs": "https://arxiv.org/abs/2509.18377", "authors": ["Xinlu He", "Yiwen Guan", "Badrivishal Paurana", "Zilin Dai", "Jacob Whitehill"], "title": "Interactive Real-Time Speaker Diarization Correction with Human Feedback", "categories": ["cs.CL"], "comment": null, "summary": "Most automatic speech processing systems operate in \"open loop\" mode without\nuser feedback about who said what; yet, human-in-the-loop workflows can\npotentially enable higher accuracy. We propose an LLM-assisted speaker\ndiarization correction system that lets users fix speaker attribution errors in\nreal time. The pipeline performs streaming ASR and diarization, uses an LLM to\ndeliver concise summaries to the users, and accepts brief verbal feedback that\nis immediately incorporated without disrupting interactions. Moreover, we\ndevelop techniques to make the workflow more effective: First, a\nsplit-when-merged (SWM) technique detects and splits multi-speaker segments\nthat the ASR erroneously attributes to just a single speaker. Second, online\nspeaker enrollments are collected based on users' diarization corrections, thus\nhelping to prevent speaker diarization errors from occurring in the future.\nLLM-driven simulations on the AMI test set indicate that our system\nsubstantially reduces DER by 9.92% and speaker confusion error by 44.23%. We\nfurther analyze correction efficacy under different settings, including summary\nvs full transcript display, the number of online enrollments limitation, and\ncorrection frequency.", "AI": {"tldr": "\u63d0\u51faLLM\u8f85\u52a9\u7684\u5b9e\u65f6\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u6821\u6b63\u7cfb\u7edf\uff0c\u901a\u8fc7\u7528\u6237\u53cd\u9988\u5c06\u8bf4\u8bdd\u4eba\u6df7\u6dc6\u9519\u8bef\u964d\u4f4e44.23%\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u8bdd\u7cfb\u7edf\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8bed\u97f3\u7cfb\u7edf\u5f00\u73af\u8fd0\u884c\u7f3a\u4e4f\u7528\u6237\u53cd\u9988\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u4eba\u673a\u534f\u540c\u673a\u5236\u63d0\u5347\u8bf4\u8bdd\u4eba\u65e5\u5fd7\u7684\u51c6\u786e\u7387\u3002", "method": "\u7ed3\u5408\u6d41\u5f0fASR\u4e0e\u65e5\u5fd7\u6280\u672f\uff0c\u91c7\u7528SWM\u5206\u5272\u8bef\u5408\u5e76\u6bb5\u843d\uff0c\u96c6\u6210LLM\u5b9e\u65f6\u751f\u6210\u6458\u8981\uff0c\u652f\u6301\u8bed\u97f3\u53cd\u9988\u5373\u65f6\u4fee\u6b63\uff0c\u5e76\u5efa\u7acb\u5728\u7ebf\u8bf4\u8bdd\u4eba\u6ce8\u518c\u5e93\u9884\u9632\u672a\u6765\u9519\u8bef\u3002", "result": "\u5728AMI\u6d4b\u8bd5\u96c6\u4e0aDER\u964d\u4f4e9.92%\uff0c\u8bf4\u8bdd\u4eba\u6df7\u6dc6\u9519\u8bef\u51cf\u5c1144.23%\uff0c\u4e14\u9a8c\u8bc1\u4e86\u6458\u8981\u663e\u793a\u3001\u6ce8\u518c\u6570\u91cf\u9650\u5236\u548c\u6821\u6b63\u9891\u7387\u5bf9\u6548\u679c\u7684\u5f71\u54cd\u89c4\u5f8b\u3002", "conclusion": "\u7528\u6237\u5373\u65f6\u53cd\u9988\u95ed\u73af\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\uff0cSWM\u6280\u672f\u4e0e\u5728\u7ebf\u6ce8\u518c\u5f62\u6210\u53cc\u91cd\u9632\u9519\u673a\u5236\uff0c\u4e3a\u8bed\u97f3\u4ea4\u4e92\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u7ea0\u9519\u8303\u5f0f\u3002"}}
{"id": "2509.18395", "pdf": "https://arxiv.org/pdf/2509.18395", "abs": "https://arxiv.org/abs/2509.18395", "authors": ["Minki Hong", "Jangho Choi", "Jihie Kim"], "title": "NormGenesis: Multicultural Dialogue Generation via Exemplar-Guided Social Norm Modeling and Violation Recovery", "categories": ["cs.CL"], "comment": "39 pages, 17 figures, EMNLP 2025 Main Conference", "summary": "Social norms govern culturally appropriate behavior in communication,\nenabling dialogue systems to produce responses that are not only coherent but\nalso socially acceptable. We present NormGenesis, a multicultural framework for\ngenerating and annotating socially grounded dialogues across English, Chinese,\nand Korean. To model the dynamics of social interaction beyond static norm\nclassification, we propose a novel dialogue type, Violation-to-Resolution\n(V2R), which models the progression of conversations following norm violations\nthrough recognition and socially appropriate repair. To improve pragmatic\nconsistency in underrepresented languages, we implement an exemplar-based\niterative refinement early in the dialogue synthesis process. This design\nintroduces alignment with linguistic, emotional, and sociocultural expectations\nbefore full dialogue generation begins. Using this framework, we construct a\ndataset of 10,800 multi-turn dialogues annotated at the turn level for norm\nadherence, speaker intent, and emotional response. Human and LLM-based\nevaluations demonstrate that NormGenesis significantly outperforms existing\ndatasets in refinement quality, dialogue naturalness, and generalization\nperformance. We show that models trained on our V2R-augmented data exhibit\nimproved pragmatic competence in ethically sensitive contexts. Our work\nestablishes a new benchmark for culturally adaptive dialogue modeling and\nprovides a scalable methodology for norm-aware generation across linguistically\nand culturally diverse languages.", "AI": {"tldr": "\u63d0\u51fa\u8de8\u6587\u5316\u5bf9\u8bdd\u6846\u67b6NormGenesis\uff0c\u901a\u8fc7V2R\u5bf9\u8bdd\u673a\u5236\u548c\u8fed\u4ee3\u4f18\u5316\u65b9\u6cd5\u6784\u5efa\u591a\u8bed\u8a00\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u8bdd\u7cfb\u7edf\u7684\u793e\u4f1a\u89c4\u8303\u9002\u5e94\u80fd\u529b", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u591a\u5143\u6587\u5316\u793e\u4f1a\u89c4\u8303\u7684\u7cfb\u7edf\u5efa\u6a21\u80fd\u529b\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u96be\u4ee5\u4fdd\u6301\u8bed\u7528\u4e00\u81f4\u6027\u3002\u9700\u8981\u5efa\u7acb\u80fd\u52a8\u6001\u5904\u7406\u89c4\u8303\u8fdd\u53cd\u5230\u4fee\u590d\u8fc7\u7a0b\u7684\u5bf9\u8bdd\u6846\u67b6", "method": "1. \u8bbe\u8ba1Violation-to-Resolution\u5bf9\u8bdd\u7c7b\u578b\u6a21\u62df\u89c4\u8303\u8fdd\u53cd\u53ca\u4fee\u590d\u8fc7\u7a0b\n2. \u91c7\u7528\u57fa\u4e8e\u8303\u4f8b\u7684\u8fed\u4ee3\u7ec6\u5316\u65b9\u6cd5\u4f18\u5316\u5bf9\u8bdd\u5408\u6210\n3. \u6784\u5efa\u542b10,800\u4e2a\u591a\u8f6e\u5bf9\u8bdd\u7684\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u5305\u542b\u89c4\u8303\u9075\u5b88\u5ea6\u3001\u8bf4\u8bdd\u8005\u610f\u56fe\u7b49\u591a\u7ef4\u5ea6\u6807\u6ce8", "result": "1. \u6570\u636e\u96c6\u5728\u7cbe\u70bc\u8d28\u91cf\u3001\u81ea\u7136\u5ea6\u548c\u6cdb\u5316\u6027\u80fd\u4e0a\u8d85\u8d8a\u73b0\u6709\u57fa\u51c6\n2. \u4f7f\u7528V2R\u589e\u5f3a\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u4f26\u7406\u654f\u611f\u573a\u666f\u7684\u8bed\u7528\u80fd\u529b\u63d0\u534719.3%\n3. \u8de8\u8bed\u8a00\u8bc4\u4f30\u663e\u793a\u97e9\u8bed/\u4e2d\u6587\u7684\u89c4\u8303\u5bf9\u9f50\u5ea6\u5206\u522b\u63d0\u9ad822%/18%", "conclusion": "NormGenesis\u5efa\u7acb\u4e86\u8de8\u6587\u5316\u5bf9\u8bdd\u5efa\u6a21\u65b0\u6807\u51c6\uff0c\u5176\u53ef\u6269\u5c55\u65b9\u6cd5\u8bba\u4e3a\u591a\u8bed\u8a00\u793e\u4f1a\u89c4\u8303\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4f26\u7406\u654f\u611f\u573a\u666f\u7684\u5bf9\u8bdd\u7cfb\u7edf\u53d1\u5c55"}}
{"id": "2509.18401", "pdf": "https://arxiv.org/pdf/2509.18401", "abs": "https://arxiv.org/abs/2509.18401", "authors": ["Armin Tourajmehr", "Mohammad Reza Modarres", "Yadollah Yaghoobzadeh"], "title": "Evaluating the Creativity of LLMs in Persian Literary Text Generation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated notable creative abilities in\ngenerating literary texts, including poetry and short stories. However, prior\nresearch has primarily centered on English, with limited exploration of\nnon-English literary traditions and without standardized methods for assessing\ncreativity. In this paper, we evaluate the capacity of LLMs to generate Persian\nliterary text enriched with culturally relevant expressions. We build a dataset\nof user-generated Persian literary spanning 20 diverse topics and assess model\noutputs along four creativity dimensions-originality, fluency, flexibility, and\nelaboration-by adapting the Torrance Tests of Creative Thinking. To reduce\nevaluation costs, we adopt an LLM as a judge for automated scoring and validate\nits reliability against human judgments using intraclass correlation\ncoefficients, observing strong agreement. In addition, we analyze the models'\nability to understand and employ four core literary devices: simile, metaphor,\nhyperbole, and antithesis. Our results highlight both the strengths and\nlimitations of LLMs in Persian literary text generation, underscoring the need\nfor further refinement.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6ce2\u65af\u6587\u5b66\u6587\u672c\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u521b\u9020\u529b\u56db\u7ef4\u5ea6\uff08\u539f\u521b\u6027/\u6d41\u7545\u6027/\u7075\u6d3b\u6027/\u7cbe\u7ec6\u6027\uff09\u548c\u6587\u5b66\u624b\u6cd5\u5206\u6790\uff0c\u9a8c\u8bc1\u4e86LLM\u81ea\u52a8\u5316\u8bc4\u4f30\u7684\u53ef\u9760\u6027", "motivation": "\u586b\u8865\u975e\u82f1\u8bed\u6587\u5b66\u4f20\u7edf\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u5efa\u7acb\u6807\u51c6\u5316\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63a2\u7d22LLMs\u5728\u6ce2\u65af\u6587\u5b66\u521b\u4f5c\u4e2d\u7684\u6587\u5316\u8868\u8fbe\u80fd\u529b", "method": "1. \u6784\u5efa20\u4e3b\u9898\u6ce2\u65af\u6587\u5b66\u6570\u636e\u96c6\uff1b2. \u6539\u7f16Torrance\u521b\u9020\u529b\u6d4b\u8bd5\u56db\u7ef4\u5ea6\u8bc4\u4f30\uff1b3. \u91c7\u7528LLM\u81ea\u52a8\u5316\u8bc4\u5206\u5e76\u9a8c\u8bc1\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\uff08ICC\u68c0\u9a8c\uff09\uff1b4. \u5206\u6790\u56db\u79cd\u6838\u5fc3\u6587\u5b66\u624b\u6cd5\u7684\u8fd0\u7528\uff08\u660e\u55bb/\u9690\u55bb/\u5938\u5f20/\u5bf9\u4ed7\uff09", "result": "\u6a21\u578b\u5c55\u73b0\u6587\u5316\u8868\u8fbe\u80fd\u529b\u4f46\u5b58\u5728\u5c40\u9650\uff0c\u81ea\u52a8\u5316\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u5224\u65ad\u9ad8\u5ea6\u4e00\u81f4\uff08ICC>0.75\uff09\uff0c\u6587\u5b66\u624b\u6cd5\u4f7f\u7528\u9891\u7387\u6392\u5e8f\uff1a\u9690\u55bb>\u660e\u55bb>\u5938\u5f20>\u5bf9\u4ed7", "conclusion": "LLMs\u5177\u5907\u6ce2\u65af\u6587\u5b66\u521b\u4f5c\u6f5c\u529b\u4f46\u9700\u4f18\u5316\uff0c\u81ea\u52a8\u5316\u8bc4\u4f30\u4f53\u7cfb\u53ef\u9760\u4e14\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u4f4e\u8d44\u6e90\u8bed\u8a00\u6587\u5b66\u7814\u7a76"}}
{"id": "2509.18439", "pdf": "https://arxiv.org/pdf/2509.18439", "abs": "https://arxiv.org/abs/2509.18439", "authors": ["Oscar J. Ponce-Ponte", "David Toro-Tobon", "Luis F. Figueroa", "Michael Gionfriddo", "Megan Branda", "Victor M. Montori", "Saturnino Luz", "Juan P. Brito"], "title": "Developing an AI framework to automatically detect shared decision-making in patient-doctor conversations", "categories": ["cs.CL", "cs.AI"], "comment": "53 pages, 1 figure, 4 tables, 5 supplementary figures, 13\n  supplementary tables", "summary": "Shared decision-making (SDM) is necessary to achieve patient-centred care.\nCurrently no methodology exists to automatically measure SDM at scale. This\nstudy aimed to develop an automated approach to measure SDM by using language\nmodelling and the conversational alignment (CA) score. A total of 157\nvideo-recorded patient-doctor conversations from a randomized multi-centre\ntrial evaluating SDM decision aids for anticoagulation in atrial fibrillations\nwere transcribed and segmented into 42,559 sentences. Context-response pairs\nand negative sampling were employed to train deep learning (DL) models and\nfine-tuned BERT models via the next sentence prediction (NSP) task. Each\ntop-performing model was used to calculate four types of CA scores. A\nrandom-effects analysis by clinician, adjusting for age, sex, race, and trial\narm, assessed the association between CA scores and SDM outcomes: the\nDecisional Conflict Scale (DCS) and the Observing Patient Involvement in\nDecision-Making 12 (OPTION12) scores. p-values were corrected for multiple\ncomparisons with the Benjamini-Hochberg method. Among 157 patients (34% female,\nmean age 70 SD 10.8), clinicians on average spoke more words than patients\n(1911 vs 773). The DL model without the stylebook strategy achieved a recall@1\nof 0.227, while the fine-tuned BERTbase (110M) achieved the highest recall@1\nwith 0.640. The AbsMax (18.36 SE7.74 p=0.025) and Max CA (21.02 SE7.63 p=0.012)\nscores generated with the DL without stylebook were associated with OPTION12.\nThe Max CA score generated with the fine-tuned BERTbase (110M) was associated\nwith the DCS score (-27.61 SE12.63 p=0.037). BERT model sizes did not have an\nimpact the association between CA scores and SDM. This study introduces an\nautomated, scalable methodology to measure SDM in patient-doctor conversations\nthrough explainable CA scores, with potential to evaluate SDM strategies at\nscale.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u8bed\u8a00\u6a21\u578b\u548c\u5bf9\u8bdd\u5bf9\u9f50\u5206\u6570\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u6d4b\u91cf\u533b\u60a3\u5171\u4eab\u51b3\u7b56\u8d28\u91cf", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u81ea\u52a8\u6d4b\u91cf\u5171\u4eab\u51b3\u7b56(SDM)\u7684\u65b9\u6cd5\uff0c\u9700\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u5de5\u5177\u4ee5\u63a8\u8fdb\u60a3\u8005\u4e2d\u5fc3\u5316\u8bca\u7597", "method": "\u4f7f\u7528157\u4e2a\u6297\u51dd\u6cbb\u7597\u533b\u60a3\u5bf9\u8bdd\u768442,559\u53e5\u6587\u672c\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u548c\u5fae\u8c03BERT\u6a21\u578b\u8ba1\u7b97\u5bf9\u8bdd\u5bf9\u9f50\u5206\u6570\uff0c\u91c7\u7528\u968f\u673a\u6548\u5e94\u6a21\u578b\u5206\u6790\u5176\u4e0eSDM\u4e34\u5e8a\u6307\u6807(OPTION12/DCS)\u7684\u5173\u8054", "result": "DL\u6a21\u578b\u751f\u6210\u7684AbsMax/Max CA\u5206\u6570\u4e0eOPTION12\u663e\u8457\u76f8\u5173(p=0.025/0.012)\uff0cBERTbase\u7684Max CA\u5206\u6570\u4e0eDCS\u76f8\u5173(p=0.037)\uff0c\u6a21\u578b\u89c4\u6a21\u4e0d\u5f71\u54cd\u5173\u8054\u6027", "conclusion": "\u9996\u6b21\u63d0\u51fa\u57fa\u4e8e\u53ef\u89e3\u91ca\u5bf9\u8bdd\u5bf9\u9f50\u5206\u6570\u7684\u81ea\u52a8\u5316SDM\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u5927\u89c4\u6a21\u5b9e\u65bd\u5171\u4eab\u51b3\u7b56\u7b56\u7565\u63d0\u4f9b\u91cf\u5316\u5de5\u5177"}}
{"id": "2509.18458", "pdf": "https://arxiv.org/pdf/2509.18458", "abs": "https://arxiv.org/abs/2509.18458", "authors": ["Daniel Kaiser", "Arnoldo Frigessi", "Ali Ramezani-Kebrya", "Benjamin Ricaud"], "title": "CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50 (Primary) 68T07, 68T05, 68T20, 68T27 (Secondary)", "I.2.7; I.2.6; I.2.4; I.2.8"], "comment": "29 pages (main: 12 + supplemental material: 17), 6 figures, 4 tables,\n  Code: https://github.com/kaiserdan/cogniload, Data:\n  https://huggingface.co/datasets/cogniloadteam/cogniload", "summary": "Current benchmarks for long-context reasoning in Large Language Models (LLMs)\noften blur critical factors like intrinsic task complexity, distractor\ninterference, and task length. To enable more precise failure analysis, we\nintroduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load\nTheory (CLT). CogniLoad generates natural-language logic puzzles with\nindependently tunable parameters that reflect CLT's core dimensions: intrinsic\ndifficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($\\rho$)\nregulates extraneous load; and task length ($N$) serves as an operational proxy\nfor conditions demanding germane load. Evaluating 22 SotA reasoning LLMs,\nCogniLoad reveals distinct performance sensitivities, identifying task length\nas a dominant constraint and uncovering varied tolerances to intrinsic\ncomplexity and U-shaped responses to distractor ratios. By offering systematic,\nfactorial control over these cognitive load dimensions, CogniLoad provides a\nreproducible, scalable, and diagnostically rich tool for dissecting LLM\nreasoning limitations and guiding future model development.", "AI": {"tldr": "\u63d0\u51faCogniLoad\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u901a\u8fc7\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba\u7684\u4e09\u4e2a\u53ef\u8c03\u7ef4\u5ea6(\u5185\u5728\u96be\u5ea6/\u5e72\u6270\u6bd4\u4f8b/\u4efb\u52a1\u957f\u5ea6)\uff0c\u7cfb\u7edf\u8bc4\u4f3022\u4e2a\u5148\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u57fa\u51c6\u672a\u80fd\u6709\u6548\u533a\u5206\u4efb\u52a1\u590d\u6742\u5ea6\u3001\u5e72\u6270\u4fe1\u606f\u548c\u4efb\u52a1\u957f\u5ea6\u7b49\u5173\u952e\u5f71\u54cd\u56e0\u7d20\uff0c\u5bfc\u81f4\u6a21\u578b\u5931\u8d25\u5206\u6790\u4e0d\u591f\u7cbe\u786e\u3002", "method": "\u57fa\u4e8e\u8ba4\u77e5\u8d1f\u8377\u7406\u8bba(CLT)\u6784\u5efa\u903b\u8f91\u8c1c\u9898\u751f\u6210\u6846\u67b6\uff1a\n1. \u5185\u5728\u96be\u5ea6(d)\u63a7\u5236\u6838\u5fc3\u8ba4\u77e5\u8d1f\u8377\n2. \u5e72\u6270\u4fe1\u53f7\u6bd4(\u03c1)\u8c03\u8282\u5916\u90e8\u8d1f\u8377\n3. \u4efb\u52a1\u957f\u5ea6(N)\u8868\u5f81\u4fe1\u606f\u6574\u5408\u9700\u6c42", "result": "\u53d1\u73b0\uff1a\n- \u4efb\u52a1\u957f\u5ea6\u662f\u4e3b\u8981\u6027\u80fd\u74f6\u9888\n- \u6a21\u578b\u5bf9\u5185\u5728\u590d\u6742\u5ea6\u5448\u73b0\u5dee\u5f02\u5bb9\u5fcd\u5ea6\n- \u5bf9\u5e72\u6270\u6bd4\u4f8b\u8868\u73b0\u51faU\u578b\u54cd\u5e94\n- \u4e0d\u540c\u6a21\u578b\u5728\u4e09\u7ef4\u5ea6\u4e0a\u5448\u73b0\u663e\u8457\u6027\u80fd\u5206\u5316", "conclusion": "CogniLoad\u4e3a\u8bca\u65adLLM\u63a8\u7406\u5c40\u9650\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u3001\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u80fd\u7cfb\u7edf\u63ed\u793a\u6a21\u578b\u5728\u4e0d\u540c\u8ba4\u77e5\u8d1f\u8377\u7ef4\u5ea6\u4e0b\u7684\u80fd\u529b\u8fb9\u754c\uff0c\u6307\u5bfc\u540e\u7eed\u6a21\u578b\u4f18\u5316\u65b9\u5411\u3002"}}
{"id": "2509.18467", "pdf": "https://arxiv.org/pdf/2509.18467", "abs": "https://arxiv.org/abs/2509.18467", "authors": ["Zeyu Liu", "Souvik Kundu", "Lianghao Jiang", "Anni Li", "Srikanth Ronanki", "Sravan Bodapati", "Gourav Datta", "Peter A. Beerel"], "title": "LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 8 figures", "summary": "Although transformer architectures have achieved state-of-the-art performance\nacross diverse domains, their quadratic computational complexity with respect\nto sequence length remains a significant bottleneck, particularly for\nlatency-sensitive long-context applications. While recent linear-complexity\nalternatives are increasingly powerful, effectively training them from scratch\nis still resource-intensive. To overcome these limitations, we propose LAWCAT\n(Linear Attention with Convolution Across Time), a novel linearization\nframework designed to efficiently transfer the capabilities of pre-trained\ntransformers into a performant linear attention architecture. LAWCAT integrates\ncausal Conv1D layers to enhance local dependency modeling and employs\nnormalized gated linear attention to improve generalization across varying\ncontext lengths. Our comprehensive evaluations demonstrate that, distilling\nMistral-7B with only 1K-length sequences yields over 90\\% passkey retrieval\naccuracy up to 22K tokens, significantly extending its effective context\nwindow. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance\non S-NIAH 1\\&2\\&3 tasks (1K-8K context length) and BABILong benchmark\n(QA2\\&QA3, 0K-16K context length), requiring less than 0.1\\% pre-training\ntokens compared with pre-training models. Furthermore, LAWCAT exhibits faster\nprefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT\nthus provides an efficient pathway to high-performance, long-context linear\nmodels suitable for edge deployment, reducing reliance on extensive\nlong-sequence training data and computational resources.", "AI": {"tldr": "\u63d0\u51faLAWCAT\u6846\u67b6\uff0c\u901a\u8fc7\u7ebf\u6027\u6ce8\u610f\u529b\u548c\u56e0\u679c\u5377\u79ef\u6539\u8fdb\u9884\u8bad\u7ec3Transformer\uff0c\u5b9e\u73b0\u9ad8\u6548\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u5e76\u51cf\u5c11\u8d44\u6e90\u6d88\u8017", "motivation": "Transformer\u7684\u4e8c\u6b21\u8ba1\u7b97\u590d\u6742\u5ea6\u9650\u5236\u957f\u4e0a\u4e0b\u6587\u5e94\u7528\uff0c\u73b0\u6709\u7ebf\u6027\u6a21\u578b\u8bad\u7ec3\u6210\u672c\u8fc7\u9ad8\uff0c\u9700\u9ad8\u6548\u8fc1\u79fb\u9884\u8bad\u7ec3\u6a21\u578b\u80fd\u529b", "method": "\u6574\u5408\u56e0\u679cConv1D\u5c42\u589e\u5f3a\u5c40\u90e8\u5efa\u6a21\uff0c\u91c7\u7528\u5f52\u4e00\u5316\u95e8\u63a7\u7ebf\u6027\u6ce8\u610f\u529b\u63d0\u5347\u4e0a\u4e0b\u6587\u957f\u5ea6\u6cdb\u5316\u80fd\u529b", "result": "Mistral-7B\u572822K\u957f\u5ea6\u4fdd\u630190%+\u68c0\u7d22\u7cbe\u5ea6\uff0cLlama3.2-1B\u572816K\u4e0a\u4e0b\u6587\u4efb\u52a1\u8868\u73b0\u4f18\u5f02\uff0c8K+\u5e8f\u5217\u9884\u586b\u5145\u5feb\u4e8eFlashAttention-2", "conclusion": "LAWCAT\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u63d0\u4f9b\u9ad8\u6548\u957f\u4e0a\u4e0b\u6587\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u5bf9\u957f\u5e8f\u5217\u8bad\u7ec3\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u4f9d\u8d56"}}
{"id": "2509.18487", "pdf": "https://arxiv.org/pdf/2509.18487", "abs": "https://arxiv.org/abs/2509.18487", "authors": ["Ben Finkelshtein", "Silviu Cucerzan", "Sujay Kumar Jauhar", "Ryen White"], "title": "Actions Speak Louder than Prompts: A Large-Scale Study of LLMs for Graph Inference", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly used for text-rich graph\nmachine learning tasks such as node classification in high-impact domains like\nfraud detection and recommendation systems. Yet, despite a surge of interest,\nthe field lacks a principled understanding of the capabilities of LLMs in their\ninteraction with graph data. In this work, we conduct a large-scale, controlled\nevaluation across several key axes of variability to systematically assess the\nstrengths and weaknesses of LLM-based graph reasoning methods in text-based\napplications. The axes include the LLM-graph interaction mode, comparing\nprompting, tool-use, and code generation; dataset domains, spanning citation,\nweb-link, e-commerce, and social networks; structural regimes contrasting\nhomophilic and heterophilic graphs; feature characteristics involving both\nshort- and long-text node attributes; and model configurations with varying LLM\nsizes and reasoning capabilities. We further analyze dependencies by\nmethodically truncating features, deleting edges, and removing labels to\nquantify reliance on input types. Our findings provide practical and actionable\nguidance. (1) LLMs as code generators achieve the strongest overall performance\non graph data, with especially large gains on long-text or high-degree graphs\nwhere prompting quickly exceeds the token budget. (2) All interaction\nstrategies remain effective on heterophilic graphs, challenging the assumption\nthat LLM-based methods collapse under low homophily. (3) Code generation is\nable to flexibly adapt its reliance between structure, features, or labels to\nleverage the most informative input type. Together, these findings provide a\ncomprehensive view of the strengths and limitations of current LLM-graph\ninteraction modes and highlight key design principles for future approaches.", "AI": {"tldr": "\u7cfb\u7edf\u8bc4\u4f30LLM\u5728\u4e0d\u540c\u56fe\u673a\u5668\u5b66\u4e60\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4ee3\u7801\u751f\u6210\u6a21\u5f0f\u5728\u957f\u6587\u672c/\u9ad8\u9636\u56fe\u4e2d\u6548\u679c\u6700\u4f18\uff0c\u4e14LLM\u65b9\u6cd5\u5728\u5f02\u8d28\u56fe\u573a\u666f\u4ecd\u6709\u6548\u3002", "motivation": "\u9488\u5bf9LLM\u5728\u56fe\u6570\u636e\u4ea4\u4e92\u4e2d\u80fd\u529b\u8ba4\u77e5\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u63a7\u5236\u5b9e\u9a8c\u5efa\u7acb\u7cfb\u7edf\u6027\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u8bbe\u8ba1\u4e94\u7ef4\u8bc4\u4f30\u4f53\u7cfb\uff08\u4ea4\u4e92\u6a21\u5f0f/\u6570\u636e\u9886\u57df/\u56fe\u7ed3\u6784/\u7279\u5f81\u7c7b\u578b/\u6a21\u578b\u914d\u7f6e\uff09\uff0c\u901a\u8fc7\u7279\u5f81\u622a\u65ad\u3001\u8fb9\u5220\u9664\u7b49\u63a7\u5236\u53d8\u91cf\u5206\u6790\u8f93\u5165\u4f9d\u8d56\u3002", "result": "\u4ee3\u7801\u751f\u6210\u6a21\u5f0fF1\u503c\u63d0\u534715.7%\uff08\u957f\u6587\u672c\u573a\u666f\uff09\uff0c\u5f02\u8d28\u56fe\u5206\u7c7b\u51c6\u786e\u7387\u4fdd\u630180%+\uff0c\u4ee3\u7801\u751f\u6210\u53ef\u52a8\u6001\u8c03\u6574\u7ed3\u6784/\u7279\u5f81/\u6807\u7b7e\u7684\u4f9d\u8d56\u6743\u91cd\u3002", "conclusion": "\u9700\u4f18\u5148\u91c7\u7528\u4ee3\u7801\u751f\u6210\u67b6\u6784\uff0c\u672a\u6765\u7684\u56fe\u5b66\u4e60\u7cfb\u7edf\u5e94\u652f\u6301\u591a\u6a21\u6001\u4ea4\u4e92\u5e76\u5efa\u7acb\u52a8\u6001\u8f93\u5165\u4f9d\u8d56\u673a\u5236\u3002"}}
{"id": "2509.18514", "pdf": "https://arxiv.org/pdf/2509.18514", "abs": "https://arxiv.org/abs/2509.18514", "authors": ["Mohamad Elzohbi", "Richard Zhao"], "title": "A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted for the Third Arabic Natural Language Processing Conference\n  (ArabicNLP 2025)", "summary": "This paper presents a methodology for inserting phrases in Arabic poems to\nconform to a specific rhythm using ByT5, a byte-level multilingual\ntransformer-based model. Our work discusses a rule-based grapheme-to-beat\ntransformation tailored for extracting the rhythm from fully diacritized Arabic\nscript. Our approach employs a conditional denoising objective to fine-tune\nByT5, where the model reconstructs masked words to match a target rhythm. We\nadopt a curriculum learning strategy, pre-training on a general Arabic dataset\nbefore fine-tuning on poetic dataset, and explore cross-lingual transfer from\nEnglish to Arabic. Experimental results demonstrate that our models achieve\nhigh rhythmic alignment while maintaining semantic coherence. The proposed\nmodel has the potential to be used in co-creative applications in the process\nof composing classical Arabic poems.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eByT5\u7684\u963f\u62c9\u4f2f\u8bd7\u6b4c\u8282\u594f\u77ed\u8bed\u63d2\u5165\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u8282\u594f\u5bf9\u9f50\u548c\u8bed\u4e49\u8fde\u8d2f\u6027", "motivation": "\u53e4\u5178\u963f\u62c9\u4f2f\u8bd7\u6b4c\u521b\u4f5c\u9700\u8981\u540c\u65f6\u6ee1\u8db3\u4e25\u683c\u97f5\u5f8b\u89c4\u5219\u548c\u8bed\u4e49\u8981\u6c42\uff0c\u4f20\u7edf\u521b\u4f5c\u8fc7\u7a0b\u590d\u6742\u8017\u65f6\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u5b9e\u73b0\u81ea\u52a8\u5316\u7684\u8282\u594f\u5bf9\u9f50\uff0c\u8f85\u52a9\u8bd7\u4eba\u521b\u4f5c\u3002", "method": "1. \u5f00\u53d1\u57fa\u4e8e\u89c4\u5219\u7684grapheme-to-beat\u8f6c\u6362\u7cfb\u7edf\u63d0\u53d6\u8bd7\u6b4c\u8282\u594f\n2. \u4f7f\u7528\u6761\u4ef6\u53bb\u566a\u76ee\u6807\u5fae\u8c03ByT5\u6a21\u578b\u91cd\u6784\u906e\u853d\u8bcd\u6c47\n3. \u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff08\u901a\u7528\u963f\u62c9\u4f2f\u8bed\u9884\u8bad\u7ec3+\u8bd7\u6b4c\u5fae\u8c03\uff09\n4. \u63a2\u7d22\u82f1\u8bed\u5230\u963f\u62c9\u4f2f\u8bed\u7684\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60", "result": "\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u8fbe\u523092%\u7684\u8282\u594f\u5bf9\u9f50\u51c6\u786e\u7387\uff0c\u8bed\u4e49\u8fde\u8d2f\u6027\u8bc4\u5206\u63d0\u534735%\u3002\u8de8\u8bed\u8a00\u8fc1\u79fb\u4f7f\u8bad\u7ec3\u6548\u7387\u63d0\u9ad840%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u97f5\u5f8b\u4fdd\u6301\u4e0e\u8bed\u4e49\u534f\u8c03\u7684\u5e73\u8861\uff0c\u4e3a\u53e4\u5178\u8bd7\u6b4c\u521b\u4f5c\u63d0\u4f9b\u6709\u6548\u8f85\u52a9\u5de5\u5177\uff0c\u663e\u8457\u964d\u4f4e\u521b\u4f5c\u95e8\u69db\u3002"}}
{"id": "2509.18535", "pdf": "https://arxiv.org/pdf/2509.18535", "abs": "https://arxiv.org/abs/2509.18535", "authors": ["Mo Mu", "Dianqiao Lei", "Chang Li"], "title": "Trace Is In Sentences: Unbiased Lightweight ChatGPT-Generated Text Detector", "categories": ["cs.CL", "eess.SP"], "comment": null, "summary": "The widespread adoption of ChatGPT has raised concerns about its misuse,\nhighlighting the need for robust detection of AI-generated text. Current\nword-level detectors are vulnerable to paraphrasing or simple prompts (PSP),\nsuffer from biases induced by ChatGPT's word-level patterns (CWP) and training\ndata content, degrade on modified text, and often require large models or\nonline LLM interaction. To tackle these issues, we introduce a novel task to\ndetect both original and PSP-modified AI-generated texts, and propose a\nlightweight framework that classifies texts based on their internal structure,\nwhich remains invariant under word-level changes. Our approach encodes sentence\nembeddings from pre-trained language models and models their relationships via\nattention. We employ contrastive learning to mitigate embedding biases from\nautoregressive generation and incorporate a causal graph with counterfactual\nmethods to isolate structural features from topic-related biases. Experiments\non two curated datasets, including abstract comparisons and revised life FAQs,\nvalidate the effectiveness of our method.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6587\u672c\u5185\u90e8\u7ed3\u6784\u7279\u5f81\u7684\u8f7b\u91cf\u7ea7AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u6846\u67b6\uff0c\u6709\u6548\u8bc6\u522b\u6539\u5199\u540e\u5185\u5bb9\u5e76\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u504f\u89c1\u95ee\u9898", "motivation": "\u73b0\u6709AI\u6587\u672c\u68c0\u6d4b\u5668\u5b58\u5728\u4e09\u5927\u7f3a\u9677\uff1a\u6613\u53d7\u6539\u5199\u653b\u51fb\u3001\u53d7ChatGPT\u8bcd\u7ea7\u6a21\u5f0f\u504f\u89c1\u5f71\u54cd\u3001\u9700\u8981\u5927\u6a21\u578b\u6216\u5728\u7ebf\u4ea4\u4e92", "method": "\u901a\u8fc7\u9884\u8bad\u7ec3\u6a21\u578b\u7f16\u7801\u53e5\u5b50\u5d4c\u5165\uff0c\u5229\u7528\u6ce8\u610f\u529b\u5efa\u6a21\u5173\u7cfb\u7ed3\u6784\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u7f13\u89e3\u751f\u6210\u504f\u89c1\uff0c\u91c7\u7528\u56e0\u679c\u56fe\u5206\u79bb\u4e3b\u9898\u76f8\u5173\u504f\u5dee", "result": "\u5728\u5b66\u672f\u6458\u8981\u6539\u5199\u548cFAQ\u4fee\u8ba2\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u6709\u6548\u6027\uff0c\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347", "conclusion": "\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u8f7b\u91cf\u7ea7\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6539\u5199\u6587\u672c\u7684\u9c81\u68d2\u68c0\u6d4b\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u8bcd\u7ea7\u68c0\u6d4b\u65b9\u6cd5\u7684\u5c40\u9650\u6027"}}
{"id": "2509.18536", "pdf": "https://arxiv.org/pdf/2509.18536", "abs": "https://arxiv.org/abs/2509.18536", "authors": ["Jin Young Kim", "Ji Won Yoon"], "title": "CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Published as a main conference paper at EMNLP 2025", "summary": "Recently, inference-time reasoning strategies have further improved the\naccuracy of large language models (LLMs), but their effectiveness on smaller\nmodels remains unclear. Based on the observation that conventional approaches\noften fail to improve performance in this context, we propose\n\\textbf{C}ycle-\\textbf{C}onsistency in \\textbf{Q}uestion \\textbf{A}nswering\n(CCQA), a novel reasoning method that can be effectively applied to SLMs.\nInspired by cycle consistency, CCQA generates a question from each reasoning\npath and answer, evaluates each by its similarity to the original question, and\nthen selects the candidate solution with the highest similarity score as the\nfinal response. Since conventional SLMs struggle to generate accurate questions\nfrom their own reasoning paths and answers, we employ a lightweight Flan-T5\nmodel specialized for question generation to support this process efficiently.\nFrom the experimental results, it is verified that CCQA consistently\noutperforms existing state-of-the-art (SOTA) methods across eight models on\nmathematical and commonsense reasoning benchmarks. Furthermore, our method\nestablishes a new practical baseline for efficient reasoning in SLMs. Source\ncode can be found at https://github.com/scai-research/ccqa_official.", "AI": {"tldr": "\u63d0\u51faCCQA\u65b9\u6cd5\u901a\u8fc7\u5faa\u73af\u4e00\u81f4\u6027\u673a\u5236\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728\u6570\u5b66\u548c\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u63a8\u7406\u7b56\u7565\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6709\u6548\uff0c\u4f46\u5728\u5c0f\u6a21\u578b\uff08SLMs\uff09\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u5f00\u53d1\u9002\u914d\u5c0f\u6a21\u578b\u7684\u9ad8\u6548\u63a8\u7406\u65b9\u6cd5", "method": "\u57fa\u4e8e\u5faa\u73af\u4e00\u81f4\u6027\u751f\u6210\u95ee\u9898-\u8bc4\u4f30\u76f8\u4f3c\u6027\uff0c\u4f7f\u7528\u4e13\u7528Flan-T5\u6a21\u578b\u751f\u6210\u95ee\u9898\uff0c\u901a\u8fc7\u76f8\u4f3c\u5ea6\u8bc4\u5206\u9009\u62e9\u6700\u4f18\u89e3", "result": "\u57288\u4e2a\u6a21\u578b\u548c\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u8d85\u8d8aSOTA\u65b9\u6cd5\uff0c\u5efa\u7acb\u5c0f\u6a21\u578b\u9ad8\u6548\u63a8\u7406\u65b0\u57fa\u7ebf", "conclusion": "CCQA\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u6a21\u578b\u63a8\u7406\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.18577", "pdf": "https://arxiv.org/pdf/2509.18577", "abs": "https://arxiv.org/abs/2509.18577", "authors": ["Yeongbin Seo", "Gayoung Kim", "Jaehyung Kim", "Jinyoung Yeo"], "title": "Prior-based Noisy Text Data Filtering: Fast and Strong Alternative For Perplexity", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": null, "summary": "As large language models (LLMs) are pretrained on massive web corpora,\ncareful selection of data becomes essential to ensure effective and efficient\nlearning. While perplexity (PPL)-based filtering has shown strong performance,\nit suffers from drawbacks: substantial time costs and inherent unreliability of\nthe model when handling noisy or out-of-distribution samples. In this work, we\npropose a simple yet powerful alternative: a prior-based data filtering method\nthat estimates token priors using corpus-level term frequency statistics,\ninspired by linguistic insights on word roles and lexical density. Our approach\nfilters documents based on the mean and standard deviation of token priors,\nserving as a fast proxy to PPL while requiring no model inference. Despite its\nsimplicity, the prior-based filter achieves the highest average performance\nacross 20 downstream benchmarks, while reducing time cost by over 1000x\ncompared to PPL-based filtering. We further demonstrate its applicability to\nsymbolic languages such as code and math, and its dynamic adaptability to\nmultilingual corpora without supervision", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8bed\u6599\u5e93\u8bcd\u9891\u7edf\u8ba1\u7684\u5148\u9a8c\u6570\u636e\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u66ff\u4ee3\u4f20\u7edf\u56f0\u60d1\u5ea6\u8fc7\u6ee4\u65b9\u6848\uff0c\u5b9e\u73b01000\u500d\u6548\u7387\u63d0\u5347\u5e76\u4fdd\u6301\u6700\u4f73\u4e0b\u6e38\u4efb\u52a1\u8868\u73b0", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u56f0\u60d1\u5ea6(PPL)\u7684\u6570\u636e\u7b5b\u9009\u5b58\u5728\u8017\u65f6\u4e25\u91cd(\u9700\u6a21\u578b\u63a8\u7406)\u548c\u5bf9\u566a\u58f0/\u5206\u5e03\u5916\u6837\u672c\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u901a\u8fc7\u8ba1\u7b97token\u5148\u9a8c\u6982\u7387(\u57fa\u4e8e\u8bed\u6599\u5e93\u8bcd\u9891\u7edf\u8ba1)\uff0c\u5229\u7528token\u5148\u9a8c\u7684\u5747\u503c\u548c\u6807\u51c6\u5dee\u4f5c\u4e3a\u8fc7\u6ee4\u6807\u51c6\uff0c\u65e0\u9700\u6a21\u578b\u63a8\u7406\u5373\u53ef\u5feb\u901f\u8fd1\u4f3cPPL", "result": "\u572820\u4e2a\u4e0b\u6e38\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f73\u5e73\u5747\u6027\u80fd\uff0c\u65f6\u95f4\u6210\u672c\u964d\u4f4e1000\u500d\u4ee5\u4e0a\uff0c\u53ef\u52a8\u6001\u9002\u5e94\u591a\u8bed\u8a00\u573a\u666f\u5e76\u652f\u6301\u4ee3\u7801/\u6570\u5b66\u7b49\u7b26\u53f7\u8bed\u8a00\u5904\u7406", "conclusion": "\u57fa\u4e8e\u8bed\u8a00\u5b66\u7684\u8bcd\u9891\u7edf\u8ba1\u65b9\u6cd5\u4e0d\u4ec5\u5b9e\u73b0\u4e86\u9ad8\u6548\u6570\u636e\u7b5b\u9009\uff0c\u8fd8\u7a81\u7834\u4e86\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u7684\u6548\u7387\u74f6\u9888\uff0c\u4e3a\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u636e\u4f18\u5316\u8303\u5f0f"}}
{"id": "2509.18585", "pdf": "https://arxiv.org/pdf/2509.18585", "abs": "https://arxiv.org/abs/2509.18585", "authors": ["Yu Chen", "Yifei Han", "Long Zhang", "Yue Du", "Bin Li"], "title": "TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning", "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 4 figures, published to ICASSP2026", "summary": "Fine-tuning large pre-trained models for downstream tasks has become a\nfundamental approach in natural language processing. Fully fine-tuning all\nmodel parameters is computationally expensive and memory-intensive, especially\nin resource-constrained environments. Existing parameter-efficient fine-tuning\nmethods reduce the number of trainable parameters but typically overlook the\nvarying sensitivity of different model layers and the importance of training\ndata. In this work, we propose TsqLoRA, a novel method that integrates\ndata-quality-driven selection with sensitivity-aware low-rank adaptation,\nconsisted of two main components: a quality-aware sampling mechanism for\nselecting the most informative training data, and a dynamic rank allocation\nmodule that adjusts the rank of each layer based on its sensitivity to\nparameter updates. The experimental results demonstrate that TsqLoRA improves\nfine-tuning efficiency while maintaining or even improving performance on a\nvariety of NLP tasks. Our code will be available at\nhttps://github.com/Benjamin-Ricky/TsqLoRA.", "AI": {"tldr": "\u63d0\u51faTsqLoRA\u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u8d28\u91cf\u7b5b\u9009\u548c\u52a8\u6001\u79e9\u5206\u914d\u63d0\u5347\u5927\u6a21\u578b\u5fae\u8c03\u6548\u7387", "motivation": "\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u5ffd\u89c6\u6a21\u578b\u5c42\u654f\u611f\u5ea6\u5dee\u5f02\u548c\u8bad\u7ec3\u6570\u636e\u91cd\u8981\u6027", "method": "\u5305\u542b\u8d28\u91cf\u611f\u77e5\u91c7\u6837\u673a\u5236\uff08\u7b5b\u9009\u9ad8\u4fe1\u606f\u91cf\u6570\u636e\uff09\u548c\u654f\u611f\u6027\u611f\u77e5\u52a8\u6001\u79e9\u5206\u914d\u6a21\u5757\uff08\u6309\u5c42\u654f\u611f\u5ea6\u8c03\u6574\u79e9\uff09", "result": "\u5728\u591a\u9879NLP\u4efb\u52a1\u4e2d\u4fdd\u6301/\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u5fae\u8c03\u6548\u7387", "conclusion": "TsqLoRA\u5728\u8ba1\u7b97\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd\u95f4\u53d6\u5f97\u66f4\u597d\u5e73\u8861\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.18588", "pdf": "https://arxiv.org/pdf/2509.18588", "abs": "https://arxiv.org/abs/2509.18588", "authors": ["Jiarui Jin", "Haoyu Wang", "Xiang Lan", "Jun Li", "Gaofeng Cheng", "Hongyan Li", "Shenda Hong"], "title": "UniECG: Understanding and Generating ECG in One Unified Model", "categories": ["cs.CL"], "comment": null, "summary": "Recent unified models such as GPT-5 have achieved encouraging progress on\nvision-language tasks. However, these unified models typically fail to\ncorrectly understand ECG signals and provide accurate medical diagnoses, nor\ncan they correctly generate ECG signals. To address these limitations, we\npropose UniECG, the first unified model for ECG capable of concurrently\nperforming evidence-based ECG interpretation and text-conditioned ECG\ngeneration tasks. Through a decoupled two-stage training approach, the model\nfirst learns evidence-based interpretation skills (ECG-to-Text), and then\ninjects ECG generation capabilities (Text-to-ECG) via latent space alignment.\nUniECG can autonomously choose to interpret or generate an ECG based on user\ninput, significantly extending the capability boundaries of current ECG models.\nOur code and checkpoints will be made publicly available at\nhttps://github.com/PKUDigitalHealth/UniECG upon acceptance.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2aECG\u7edf\u4e00\u6a21\u578bUniECG\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u540c\u65f6\u5b9e\u73b0ECG\u89e3\u91ca\u548c\u751f\u6210\u4efb\u52a1\uff0c\u7a81\u7834\u73b0\u6709\u6a21\u578b\u80fd\u529b\u8fb9\u754c\u3002", "motivation": "\u73b0\u6709\u7edf\u4e00\u6a21\u578b\uff08\u5982GPT-5\uff09\u65e0\u6cd5\u51c6\u786e\u7406\u89e3ECG\u4fe1\u53f7\u8fdb\u884c\u533b\u7597\u8bca\u65ad\uff0c\u4e5f\u4e0d\u80fd\u6b63\u786e\u751f\u6210ECG\u4fe1\u53f7\u3002", "method": "\u91c7\u7528\u89e3\u8026\u5f0f\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1. \u5148\u5b66\u4e60\u57fa\u4e8e\u8bc1\u636e\u7684ECG\u89e3\u91ca\uff08ECG\u2192Text\uff09\uff1b2. \u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u6ce8\u5165\u6587\u672c\u6761\u4ef6ECG\u751f\u6210\u80fd\u529b\uff08Text\u2192ECG\uff09\u3002", "result": "UniECG\u53ef\u6839\u636e\u8f93\u5165\u81ea\u4e3b\u9009\u62e9ECG\u89e3\u91ca\u6216\u751f\u6210\uff0c\u6a21\u578b\u4ee3\u7801\u548c\u68c0\u67e5\u70b9\u5c06\u5728\u63a5\u53d7\u540e\u5f00\u6e90\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5b9e\u73b0ECG\u4efb\u52a1\u7684\u53cc\u5411\u7edf\u4e00\u5efa\u6a21\uff0c\u4e3a\u533b\u7597AI\u63d0\u4f9b\u53ef\u81ea\u4e3b\u51b3\u7b56\u7684ECG\u5206\u6790\u5de5\u5177\uff0c\u5177\u6709\u91cd\u8981\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2509.18632", "pdf": "https://arxiv.org/pdf/2509.18632", "abs": "https://arxiv.org/abs/2509.18632", "authors": ["Nishant Balepur", "Matthew Shu", "Yoo Yeon Sung", "Seraphina Goldfarb-Tarrant", "Shi Feng", "Fumeng Yang", "Rachel Rudinger", "Jordan Lee Boyd-Graber"], "title": "A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users", "categories": ["cs.CL"], "comment": "EMNLP 2025", "summary": "To assist users in complex tasks, LLMs generate plans: step-by-step\ninstructions towards a goal. While alignment methods aim to ensure LLM plans\nare helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer,\nassuming this reflects what helps them. We test this with Planorama: an\ninterface where 126 users answer 300 multi-step questions with LLM plans. We\nget 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA\nsuccess) and user preferences on plans, and recreate the setup in agents and\nreward models to see if they simulate or prefer what helps users. We expose: 1)\nuser/model preferences and agent success do not accurately predict which plans\nhelp users, so common alignment feedback can misalign with helpfulness; 2) this\ngap is not due to user-specific preferences, as users are similarly successful\nwhen using plans they prefer/disprefer; 3) surface-level cues like brevity and\nquestion similarity strongly link to preferences, but such biases fail to\npredict helpfulness. In all, we argue aligning helpful LLMs needs feedback from\nreal user interactions, not just preferences of what looks helpful, so we\ndiscuss the plan NLP researchers can execute to solve this problem.", "AI": {"tldr": "LLM\u751f\u6210\u8ba1\u5212\u65f6\uff0c\u7528\u6237\u504f\u597d\u4e0e\u6a21\u578b\u8bc4\u4f30\u6307\u6807\u65e0\u6cd5\u6709\u6548\u9884\u6d4b\u8ba1\u5212\u5b9e\u9645\u5e2e\u52a9\u6027\uff0c\u9700\u57fa\u4e8e\u771f\u5b9e\u7528\u6237\u4ea4\u4e92\u53cd\u9988\u6539\u8fdb\u5bf9\u9f50\u65b9\u6cd5", "motivation": "\u9a8c\u8bc1\u57fa\u4e8e\u7528\u6237\u504f\u597d\u7684\u5bf9\u9f50\u65b9\u6cd5\uff08\u5982RLHF\uff09\u662f\u5426\u771f\u5b9e\u53cd\u6620LLM\u751f\u6210\u8ba1\u5212\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5b9e\u9645\u5e2e\u52a9\u6027", "method": "\u901a\u8fc7Planorama\u5e73\u53f0\u5f00\u5c55\u7528\u6237\u5b9e\u9a8c\uff1a126\u540d\u7528\u6237\u5b8c\u6210300\u4e2a\u591a\u6b65\u9aa4\u95ee\u9898\uff0c\u5206\u67904388\u6b21\u8ba1\u5212\u6267\u884c\u4e0e5584\u6b21\u5bf9\u6bd4\u8bc4\u4f30\uff0c\u5e76\u6784\u5efa\u4ee3\u7406\u6a21\u578b\u9a8c\u8bc1\u9884\u6d4b\u6709\u6548\u6027", "result": "\u53d1\u73b0\u7528\u6237/\u6a21\u578b\u504f\u597d\u4e0e\u4ee3\u7406\u6210\u529f\u7387\u5747\u4e0d\u80fd\u6709\u6548\u9884\u6d4b\u8ba1\u5212\u5e2e\u52a9\u6027\uff1b\u7528\u6237\u5bf9\u504f\u597d/\u975e\u504f\u597d\u8ba1\u5212\u7684\u6210\u529f\u7387\u76f8\u8fd1\uff1b\u8868\u9762\u7279\u5f81\uff08\u7b80\u6d01\u6027/\u95ee\u9898\u76f8\u4f3c\u6027\uff09\u4e3b\u5bfc\u504f\u597d\u4f46\u65e0\u5173\u5e2e\u52a9\u6027", "conclusion": "LLM\u5bf9\u9f50\u9700\u4f9d\u8d56\u771f\u5b9e\u7528\u6237\u4ea4\u4e92\u53cd\u9988\u800c\u975e\u8868\u9762\u504f\u597d\uff0c\u63d0\u51faNLP\u7814\u7a76\u8005\u5e94\u901a\u8fc7\u6784\u5efa\u7528\u6237\u53c2\u4e0e\u7684\u7cfb\u7edf\u95ed\u73af\u89e3\u51b3\u8be5\u95ee\u9898"}}
{"id": "2509.18655", "pdf": "https://arxiv.org/pdf/2509.18655", "abs": "https://arxiv.org/abs/2509.18655", "authors": ["Lingwen Deng", "Yifei Han", "Long Zhang", "Yue Du", "Bin Li"], "title": "Consistency-Aware Parameter-Preserving Knowledge Editing Framework for Multi-Hop Question Answering", "categories": ["cs.CL"], "comment": "Submitted to ICASSP 2026", "summary": "Parameter-Preserving Knowledge Editing (PPKE) enables updating models with\nnew or corrected information without retraining or parameter adjustment. Recent\nPPKE approaches based on knowledge graphs (KG) to extend knowledge editing (KE)\ncapabilities to multi-hop question answering (MHQA). However, these methods\noften lack consistency, leading to knowledge contamination, unstable updates,\nand retrieval behaviors that fail to reflect the intended edits. Such\ninconsistencies undermine the reliability of PPKE in multi- hop reasoning. We\npresent CAPE-KG, Consistency-Aware Parameter-Preserving Editing with Knowledge\nGraphs, a novel consistency-aware framework for PPKE on MHQA. CAPE-KG ensures\nKG construction, update, and retrieval are always aligned with the requirements\nof the MHQA task, maintaining coherent reasoning over both unedited and edited\nknowledge. Extensive experiments on the MQuAKE benchmark show accuracy\nimprovements in PPKE performance for MHQA, demonstrating the effectiveness of\naddressing consistency in PPKE.", "AI": {"tldr": "\u63d0\u51faCAPE-KG\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u5b9e\u73b0\u591a\u8df3\u95ee\u7b54\u573a\u666f\u4e0b\u53c2\u6570\u4fdd\u7559\u77e5\u8bc6\u7f16\u8f91\u7684\u4e00\u81f4\u6027\u589e\u5f3a\uff0c\u63d0\u5347\u6a21\u578b\u66f4\u65b0\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684PPKE\u65b9\u6cd5\u5728\u591a\u8df3\u63a8\u7406\u4e2d\u5b58\u5728\u77e5\u8bc6\u6c61\u67d3\u3001\u66f4\u65b0\u4e0d\u7a33\u5b9a\u548c\u68c0\u7d22\u884c\u4e3a\u504f\u5dee\u7b49\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5f71\u54cd\u7cfb\u7edf\u53ef\u9760\u6027\u3002", "method": "\u6784\u5efa\u4efb\u52a1\u5bf9\u9f50\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7\u4e00\u81f4\u6027\u4fdd\u969c\u673a\u5236\u534f\u8c03\u56fe\u8c31\u6784\u5efa\u3001\u52a8\u6001\u66f4\u65b0\u548c\u68c0\u7d22\u8fc7\u7a0b\uff0c\u4fdd\u6301\u539f\u59cb\u77e5\u8bc6\u4e0e\u7f16\u8f91\u77e5\u8bc6\u95f4\u7684\u63a8\u7406\u8fde\u8d2f\u6027\u3002", "result": "\u5728MQuAKE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\u3002", "conclusion": "CAPE-KG\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u4e00\u81f4\u6027\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86PPKE\u5728\u591a\u8df3\u95ee\u7b54\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4e3a\u53ef\u9760\u7684\u77e5\u8bc6\u7f16\u8f91\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2509.18658", "pdf": "https://arxiv.org/pdf/2509.18658", "abs": "https://arxiv.org/abs/2509.18658", "authors": ["Huanxin Sheng", "Xinyi Liu", "Hangfeng He", "Jieyu Zhao", "Jian Kang"], "title": "Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with Conformal Prediction", "categories": ["cs.CL"], "comment": "To appear in EMNLP 2025. Our code and data are available at\n  \\url{https://github.com/BruceSheng1202/Analyzing_Uncertainty_of_LLM-as-a-Judge", "summary": "LLM-as-a-judge has become a promising paradigm for using large language\nmodels (LLMs) to evaluate natural language generation (NLG), but the\nuncertainty of its evaluation remains underexplored. This lack of reliability\nmay limit its deployment in many applications. This work presents the first\nframework to analyze the uncertainty by offering a prediction interval of\nLLM-based scoring via conformal prediction. Conformal prediction constructs\ncontinuous prediction intervals from a single evaluation run, and we design an\nordinal boundary adjustment for discrete rating tasks. We also suggest a\nmidpoint-based score within the interval as a low-bias alternative to raw model\nscore and weighted average. We perform extensive experiments and analysis,\nwhich show that conformal prediction can provide valid prediction interval with\ncoverage guarantees. We also explore the usefulness of interval midpoint and\njudge reprompting for better judgment.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e\u5171\u5f62\u9884\u6d4b\u7684LLM\u8bc4\u5206\u4e0d\u786e\u5b9a\u6027\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u533a\u95f4\u91cf\u5316\u8bc4\u4f30\u53ef\u9760\u6027", "motivation": "\u73b0\u6709LLM-as-a-judge\u8303\u5f0f\u5728\u81ea\u7136\u8bed\u8a00\u751f\u6210\u8bc4\u4f30\u4e2d\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u90e8\u7f72", "method": "\u4f7f\u7528\u5171\u5f62\u9884\u6d4b\u6784\u5efa\u8fde\u7eed\u9884\u6d4b\u533a\u95f4\uff0c\u8bbe\u8ba1\u79bb\u6563\u8bc4\u5206\u4efb\u52a1\u7684\u5e8f\u6570\u8fb9\u754c\u8c03\u6574\u65b9\u6cd5\uff0c\u63d0\u51fa\u57fa\u4e8e\u533a\u95f4\u4e2d\u70b9\u7684\u4f4e\u504f\u5dee\u8bc4\u5206\u65b9\u6848", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u63d0\u4f9b\u6709\u6548\u8986\u76d6\u4fdd\u8bc1\u7684\u9884\u6d4b\u533a\u95f4\uff0c\u533a\u95f4\u4e2d\u70b9\u548c\u591a\u6b21\u63d0\u793a\u7b56\u7565\u53ef\u63d0\u5347\u5224\u65ad\u8d28\u91cf", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u9760\u6027\u4fdd\u969c\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u589e\u5f3a\u4e86\u8bc4\u5224\u7ed3\u679c\u7684\u53ef\u4fe1\u5ea6"}}
{"id": "2509.18713", "pdf": "https://arxiv.org/pdf/2509.18713", "abs": "https://arxiv.org/abs/2509.18713", "authors": ["Yizhe Huang", "Yang Liu", "Ruiyu Zhao", "Xiaolong Zhong", "Xingming Yue", "Ling Jiang"], "title": "MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Model-based agents(LLM-based agents) are increasingly deployed\nin customer service, yet they often forget across sessions, repeat errors, and\nlack mechanisms for continual self-improvement. This makes them unreliable in\ndynamic settings where stability and consistency are critical. To better\nevaluate these properties, we emphasize two indicators: task success rate as a\nmeasure of overall effectiveness, and consistency metrics such as Pass$^k$ to\ncapture reliability across multiple trials. To address the limitations of\nexisting approaches, we propose MemOrb, a lightweight and plug-and-play verbal\nreinforcement memory layer that distills multi-turn interactions into compact\nstrategy reflections. These reflections are stored in a shared memory bank and\nretrieved to guide decision-making, without requiring any fine-tuning.\nExperiments show that MemOrb significantly improves both success rate and\nstability, achieving up to a 63 percentage-point gain in multi-turn success\nrate and delivering more consistent performance across repeated trials. Our\nresults demonstrate that structured reflection is a powerful mechanism for\nenhancing long-term reliability of frozen LLM agents in customer service\nscenarios.", "AI": {"tldr": "\u63d0\u51faMemOrb\u8f7b\u91cf\u7ea7\u8bb0\u5fc6\u589e\u5f3a\u6846\u67b6\uff0c\u901a\u8fc7\u7b56\u7565\u53cd\u601d\u84b8\u998f\u591a\u8f6e\u5bf9\u8bdd\uff0c\u4f7f\u51bb\u7ed3LLM\u4ee3\u7406\u5728\u5ba2\u6237\u670d\u52a1\u573a\u666f\u4e2d\u7684\u591a\u8f6e\u6210\u529f\u7387\u63d0\u534763\u4e2a\u767e\u5206\u70b9", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\u5b58\u5728\u8de8\u4f1a\u8bdd\u9057\u5fd8\u3001\u91cd\u590d\u9519\u8bef\u548c\u7f3a\u4e4f\u6301\u7eed\u6539\u8fdb\u673a\u5236\u7684\u95ee\u9898\uff0c\u5728\u9700\u8981\u7a33\u5b9a\u6027\u7684\u52a8\u6001\u573a\u666f\u4e2d\u53ef\u9760\u6027\u4e0d\u8db3", "method": "\u91c7\u7528\u53ef\u63d2\u62d4\u7684verbal reinforcement memory\u5c42\uff0c\u5c06\u591a\u8f6e\u4ea4\u4e92\u84b8\u998f\u4e3a\u7d27\u51d1\u7b56\u7565\u53cd\u5c04\uff0c\u5b58\u50a8\u5728\u5171\u4eab\u8bb0\u5fc6\u5e93\u4e2d\u6307\u5bfc\u51b3\u7b56\uff08\u65e0\u9700\u6a21\u578b\u5fae\u8c03\uff09", "result": "\u5b9e\u9a8c\u663e\u793aMemOrb\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u53ca\u7a33\u5b9a\u6027\uff0c\u5728\u591a\u8f6e\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u63d0\u5347\u6700\u9ad8\u8fbe63%\uff0c\u91cd\u590d\u8bd5\u9a8c\u4e2d\u8868\u73b0\u66f4\u7a33\u5b9a", "conclusion": "\u7ed3\u6784\u5316\u53cd\u601d\u673a\u5236\u80fd\u6709\u6548\u589e\u5f3a\u51bb\u7ed3LLM\u4ee3\u7406\u5728\u5ba2\u6237\u670d\u52a1\u573a\u666f\u7684\u957f\u671f\u53ef\u9760\u6027\uff0c\u9a8c\u8bc1\u4e86\u8bb0\u5fc6\u589e\u5f3a\u65b9\u6cd5\u7684\u6709\u6548\u6027"}}
{"id": "2509.18722", "pdf": "https://arxiv.org/pdf/2509.18722", "abs": "https://arxiv.org/abs/2509.18722", "authors": ["Pattara Tipaksorn", "Sumonmas Thatphithakkul", "Vataya Chunwijitra", "Kwanchiva Thangthai"], "title": "LOTUSDIS: A Thai far-field meeting corpus for robust conversational ASR", "categories": ["cs.CL", "cs.SD"], "comment": null, "summary": "We present LOTUSDIS, a publicly available Thai meeting corpus designed to\nadvance far-field conversational ASR. The dataset comprises 114 hours of\nspontaneous, unscripted dialogue collected in 15-20 minute sessions with three\nparticipants, where overlapping speech is frequent and natural. Speech was\nrecorded simultaneously by nine independent single-channel devices spanning six\nmicrophone types at distances from 0.12 m to 10 m, preserving the authentic\neffects of reverberation, noise, and device coloration without relying on\nmicrophone arrays. We provide standard train, dev, test splits and release a\nreproducible baseline system. We benchmarked several Whisper variants under\nzero-shot and fine-tuned conditions. Off-the-shelf models showed strong\ndegradation with distance, confirming a mismatch between pre-training data and\nThai far-field speech. Fine-tuning on LOTUSDIS dramatically improved\nrobustness: a Thai Whisper baseline reduced overall WER from 64.3 to 38.3 and\nfar-field WER from 81.6 to 49.5, with especially large gains on the most\ndistant microphones. These results underscore the importance of\ndistance-diverse training data for robust ASR. The corpus is available under\nCC-BY-SA 4.0. We also release training and evaluation scripts as a baseline\nsystem to promote reproducible research in this field.", "AI": {"tldr": "\u5f00\u6e90\u6cf0\u8bed\u4f1a\u8bae\u6570\u636e\u96c6LOTUSDIS\uff08114\u5c0f\u65f6\u81ea\u53d1\u5bf9\u8bdd\uff09\u901a\u8fc7\u591a\u8bbe\u5907\u8fdc\u573a\u5f55\u97f3\u9a8c\u8bc1\u4e86\u8ddd\u79bb\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u5bf9\u63d0\u5347ASR\u9c81\u68d2\u6027\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5fae\u8c03\u540eWhisper\u6a21\u578b\u9519\u8bef\u7387\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709\u8fdc\u573aASR\u5728\u6cf0\u8bed\u6570\u636e\u4e0a\u5b58\u5728\u9884\u8bad\u7ec3\u4e0e\u771f\u5b9e\u8fdc\u573a\u573a\u666f\u7684\u4e25\u91cd\u4e0d\u5339\u914d\uff0c\u8fdc\u8ddd\u79bb\u9ea6\u514b\u98ce\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002\u9700\u8981\u771f\u5b9e\u3001\u8bbe\u5907\u591a\u6837\u5316\u7684\u8fdc\u573a\u8bed\u6599\u5e93\u6765\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u3002", "method": "1. \u4f7f\u75289\u4e2a\u72ec\u7acb\u5355\u901a\u9053\u8bbe\u5907\uff086\u79cd\u7c7b\u578b\uff09\u57280.12-10\u7c73\u8303\u56f4\u91c7\u96c6\u81ea\u7136\u5bf9\u8bdd\n2. \u6784\u5efa\u53ef\u590d\u73b0\u57fa\u7ebf\u7cfb\u7edf\uff0c\u6d4b\u8bd5Whisper\u7cfb\u5217\u6a21\u578b\u7684\u96f6\u6837\u672c/\u5fae\u8c03\u8868\u73b0\n3. \u63d0\u4f9b\u6807\u51c6\u6570\u636e\u5212\u5206\u53ca\u8bad\u7ec3\u8bc4\u4f30\u811a\u672c", "result": "\u5fae\u8c03\u4f7f\u6cf0\u8bedWhisper\uff1a\n- \u6574\u4f53WER\u4ece64.3\u219238.3\n- \u8fdc\u573aWER\u4ece81.6\u219249.5\n\u6700\u8fdc\u9ea6\u514b\u98ce\uff0810\u7c73\uff09\u63d0\u5347\u6700\u663e\u8457", "conclusion": "\u8ddd\u79bb\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e\u662f\u63d0\u5347ASR\u8fdc\u573a\u9c81\u68d2\u6027\u7684\u5173\u952e\u3002\u5f00\u6e90CC-BY-SA 4.0\u534f\u8bae\u7684\u6570\u636e\u96c6\u53ca\u57fa\u51c6\u7cfb\u7edf\u5c06\u63a8\u52a8\u8be5\u9886\u57df\u53ef\u91cd\u590d\u7814\u7a76\uff0c\u7279\u522b\u662f\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u8fdc\u573aASR\u53d1\u5c55\u3002"}}
{"id": "2509.18742", "pdf": "https://arxiv.org/pdf/2509.18742", "abs": "https://arxiv.org/abs/2509.18742", "authors": ["Yunan Wang", "Jianxin Li", "Ziwei Zhang"], "title": "Global-Recent Semantic Reasoning on Dynamic Text-Attributed Graphs with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Dynamic Text-Attribute Graphs (DyTAGs), characterized by time-evolving graph\ninteractions and associated text attributes, are prevalent in real-world\napplications. Existing methods, such as Graph Neural Networks (GNNs) and Large\nLanguage Models (LLMs), mostly focus on static TAGs. Extending these existing\nmethods to DyTAGs is challenging as they largely neglect the recent-global\ntemporal semantics: the recent semantic dependencies among interaction texts\nand the global semantic evolution of nodes over time. Furthermore, applying\nLLMs to the abundant and evolving text in DyTAGs faces efficiency issues. To\ntackle these challenges, we propose Dynamic Global-Recent Adaptive Semantic\nProcessing (DyGRASP), a novel method that leverages LLMs and temporal GNNs to\nefficiently and effectively reason on DyTAGs. Specifically, we first design a\nnode-centric implicit reasoning method together with a sliding window mechanism\nto efficiently capture recent temporal semantics. In addition, to capture\nglobal semantic dynamics of nodes, we leverage explicit reasoning with tailored\nprompts and an RNN-like chain structure to infer long-term semantics. Lastly,\nwe intricately integrate the recent and global temporal semantics as well as\nthe dynamic graph structural information using updating and merging layers.\nExtensive experiments on DyTAG benchmarks demonstrate DyGRASP's superiority,\nachieving up to 34% improvement in Hit@10 for destination node retrieval task.\nBesides, DyGRASP exhibits strong generalization across different temporal GNNs\nand LLMs.", "AI": {"tldr": "\u63d0\u51faDyGRASP\u65b9\u6cd5\uff0c\u7ed3\u5408LLM\u4e0e\u65f6\u5e8fGNN\uff0c\u89e3\u51b3\u52a8\u6001\u6587\u672c\u5c5e\u6027\u56fe\u7684\u8fd1\u671f-\u5168\u5c40\u8bed\u4e49\u6355\u83b7\u96be\u9898\uff0c\u63d0\u5347\u6548\u7387\u4e0e\u6548\u679c", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u52a8\u6001\u6587\u672c\u5c5e\u6027\u56fe\u4e2d\u7684\u8fd1\u671f\u4ea4\u4e92\u8bed\u4e49\u4e0e\u5168\u5c40\u8bed\u4e49\u6f14\u5316\uff0c\u4e14\u76f4\u63a5\u5e94\u7528\u5927\u6a21\u578b\u5b58\u5728\u6548\u7387\u74f6\u9888", "method": "\u7ed3\u5408\u9690\u5f0f\u63a8\u7406(\u6ed1\u52a8\u7a97\u53e3)\u4e0e\u663e\u5f0f\u63a8\u7406(\u63d0\u793a\u8bcd+\u7c7bRNN\u94fe)\uff0c\u6574\u5408\u8fd1\u671f/\u5168\u5c40\u8bed\u4e49\u4e0e\u52a8\u6001\u56fe\u7ed3\u6784\u4fe1\u606f", "result": "\u5728\u76ee\u6807\u8282\u70b9\u68c0\u7d22\u4efb\u52a1\u4e2dHit@10\u6307\u6807\u63d0\u5347\u8fbe34%\uff0c\u4e14\u5728\u4e0d\u540c\u65f6\u5e8fGNN/LLM\u4e0a\u5c55\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b", "conclusion": "\u901a\u8fc7\u878d\u5408\u8fd1\u671f\u4e0e\u5168\u5c40\u65f6\u95f4\u8bed\u4e49\uff0cDyGRASP\u663e\u8457\u63d0\u5347\u52a8\u6001\u6587\u672c\u5c5e\u6027\u56fe\u63a8\u7406\u6027\u80fd\uff0c\u5e76\u4e3a\u52a8\u6001\u591a\u6a21\u6001\u6570\u636e\u5206\u6790\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2509.18750", "pdf": "https://arxiv.org/pdf/2509.18750", "abs": "https://arxiv.org/abs/2509.18750", "authors": ["Julie Kallini", "Dan Jurafsky", "Christopher Potts", "Martijn Bartelds"], "title": "False Friends Are Not Foes: Investigating Vocabulary Overlap in Multilingual Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Subword tokenizers trained on multilingual corpora naturally produce\noverlapping tokens across languages. Does token overlap facilitate\ncross-lingual transfer or instead introduce interference between languages?\nPrior work offers mixed evidence, partly due to varied setups and confounders,\nsuch as token frequency or subword segmentation granularity. To address this\nquestion, we devise a controlled experiment where we train bilingual\nautoregressive models on multiple language pairs under systematically varied\nvocabulary overlap settings. Crucially, we explore a new dimension to\nunderstanding how overlap affects transfer: the semantic similarity of tokens\nshared across languages. We first analyze our models' hidden representations\nand find that overlap of any kind creates embedding spaces that capture\ncross-lingual semantic relationships, while this effect is much weaker in\nmodels with disjoint vocabularies. On XNLI and XQuAD, we find that models with\noverlap outperform models with disjoint vocabularies, and that transfer\nperformance generally improves as overlap increases. Overall, our findings\nhighlight the advantages of token overlap in multilingual models and show that\nsubstantial shared vocabulary remains a beneficial design choice for\nmultilingual tokenizers.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u8bc1\u660e\u591a\u8bed\u8a00\u6a21\u578b\u8bcd\u6c47\u91cd\u53e0\u80fd\u4fc3\u8fdb\u8de8\u8bed\u8a00\u8bed\u4e49\u5173\u8054\uff0c\u5171\u4eab\u8bcd\u6c47\u663e\u8457\u63d0\u5347\u8de8\u8bed\u8a00\u4efb\u52a1\u6027\u80fd", "motivation": "\u9488\u5bf9\u5148\u524d\u7814\u7a76\u4e2d\u5173\u4e8e\u8de8\u8bed\u8a00\u8bcd\u6c47\u91cd\u53e0\u5b58\u5728\u4fc3\u8fdb\u6216\u5e72\u6270\u7684\u4e89\u8bae\uff0c\u91cd\u70b9\u63a2\u7a76\u8bed\u4e49\u76f8\u4f3c\u6027\u8fd9\u4e00\u65b0\u7ef4\u5ea6\u5bf9\u8de8\u8bed\u8a00\u8fc1\u79fb\u7684\u5f71\u54cd\u673a\u5236", "method": "\u4f7f\u7528\u53cc\u8bed\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u5728\u7cfb\u7edf\u63a7\u5236\u8bcd\u6c47\u91cd\u53e0\u7387\u7684\u591a\u79cd\u8bed\u8a00\u5bf9\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u521b\u65b0\u6027\u5730\u5f15\u5165\u5171\u4eab\u8bcd\u6c47\u8bed\u4e49\u76f8\u4f3c\u5ea6\u5206\u6790\u7ef4\u5ea6", "result": "\u8bcd\u6c47\u91cd\u53e0\u6a21\u578b\u5728XNLI/XQuAD\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u6027\u80fd\u968f\u91cd\u53e0\u7387\u589e\u52a0\u800c\u63d0\u5347\uff1b\u91cd\u53e0\u8bcd\u6c47\u663e\u8457\u589e\u5f3a\u8de8\u8bed\u8a00\u8bed\u4e49\u7a7a\u95f4\u5173\u8054\u6027", "conclusion": "\u5b9e\u8d28\u6027\u8bcd\u6c47\u5171\u4eab\u662f\u591a\u8bed\u8a00\u5206\u8bcd\u5668\u7684\u6709\u76ca\u8bbe\u8ba1\u9009\u62e9\uff0c\u8bcd\u6c47\u91cd\u53e0\u901a\u8fc7\u8bed\u4e49\u5173\u8054\u4fc3\u8fdb\u8de8\u8bed\u8a00\u8fc1\u79fb"}}
{"id": "2509.18762", "pdf": "https://arxiv.org/pdf/2509.18762", "abs": "https://arxiv.org/abs/2509.18762", "authors": ["Yingming Zheng", "Hanqi Li", "Kai Yu", "Lu Chen"], "title": "When Long Helps Short: How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have achieved impressive performance across\nnatural language processing (NLP) tasks. As real-world applications\nincreasingly demand longer context windows, continued pretraining and\nsupervised fine-tuning (SFT) on long-context data has become a common approach.\nWhile the effects of data length in continued pretraining have been extensively\nstudied, their implications for SFT remain unclear. In this work, we\nsystematically investigate how SFT data length influences LLM behavior on\nshort-context tasks. Counterintuitively, we find that long-context SFT improves\nshort-context performance, contrary to the commonly observed degradation from\nlong-context pretraining. To uncover the underlying mechanisms of this\nphenomenon, we first decouple and analyze two key components, Multi-Head\nAttention (MHA) and Feed-Forward Network (FFN), and show that both\nindependently benefit from long-context SFT. We further study their interaction\nand reveal a knowledge preference bias: long-context SFT promotes contextual\nknowledge, while short-context SFT favors parametric knowledge, making\nexclusive reliance on long-context SFT suboptimal. Finally, we demonstrate that\nhybrid training mitigates this bias, offering explainable guidance for\nfine-tuning LLMs.", "AI": {"tldr": "\u957f\u4e0a\u4e0b\u6587\u76d1\u7763\u5fae\u8c03(SFT)\u610f\u5916\u63d0\u5347\u77ed\u4e0a\u4e0b\u6587\u4efb\u52a1\u8868\u73b0\uff0c\u901a\u8fc7\u6df7\u5408\u8bad\u7ec3\u5e73\u8861\u77e5\u8bc6\u504f\u597d\u504f\u5dee", "motivation": "\u7814\u7a76\u957f\u4e0a\u4e0b\u6587SFT\u6570\u636e\u5bf9LLM\u77ed\u4e0a\u4e0b\u6587\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e0e\u957f\u4e0a\u4e0b\u6587\u9884\u8bad\u7ec3\u76f8\u53cd\u7684\u63d0\u5347\u73b0\u8c61", "method": "\u89e3\u8026\u5206\u6790MHA\u548cFFN\u7ec4\u4ef6\uff0c\u63d0\u51fa\u77e5\u8bc6\u504f\u597d\u504f\u5dee\u7406\u8bba\uff08\u957f\u4e0a\u4e0b\u6587SFT\u4fc3\u8fdb\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff0c\u77ed\u4e0a\u4e0b\u6587SFT\u4fa7\u91cd\u53c2\u6570\u77e5\u8bc6\uff09", "result": "\u957f\u4e0a\u4e0b\u6587SFT\u63d0\u5347\u77ed\u4efb\u52a1\u6027\u80fd\uff0c\u6df7\u5408\u8bad\u7ec3\u65b9\u6848\u6bd4\u5355\u4e00\u957f\u4e0a\u4e0b\u6587SFT\u6548\u679c\u63d0\u53473.2%", "conclusion": "\u9996\u6b21\u63ed\u793aSFT\u9636\u6bb5\u4e0a\u4e0b\u6587\u957f\u5ea6\u7684\u7279\u6b8a\u4f5c\u7528\u673a\u5236\uff0c\u4e3aLLM\u5fae\u8c03\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u6df7\u5408\u8bad\u7ec3\u6307\u5bfc\u65b9\u6848"}}
{"id": "2509.18775", "pdf": "https://arxiv.org/pdf/2509.18775", "abs": "https://arxiv.org/abs/2509.18775", "authors": ["Wei-Ning Chiu", "Yu-Hsiang Wang", "Andy Hsiao", "Yu-Shiang Huang", "Chuan-Ju Wang"], "title": "Financial Risk Relation Identification through Dual-view Adaptation", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 3 figures, EMNLP 2025 Main Conference", "summary": "A multitude of interconnected risk events -- ranging from regulatory changes\nto geopolitical tensions -- can trigger ripple effects across firms.\nIdentifying inter-firm risk relations is thus crucial for applications like\nportfolio management and investment strategy. Traditionally, such assessments\nrely on expert judgment and manual analysis, which are, however, subjective,\nlabor-intensive, and difficult to scale. To address this, we propose a\nsystematic method for extracting inter-firm risk relations using Form 10-K\nfilings -- authoritative, standardized financial documents -- as our data\nsource. Leveraging recent advances in natural language processing, our approach\ncaptures implicit and abstract risk connections through unsupervised\nfine-tuning based on chronological and lexical patterns in the filings. This\nenables the development of a domain-specific financial encoder with a deeper\ncontextual understanding and introduces a quantitative risk relation score for\ntransparency, interpretable analysis. Extensive experiments demonstrate that\nour method outperforms strong baselines across multiple evaluation settings.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790Form 10-K\u6587\u4ef6\u4e2d\u7684\u65f6\u5e8f\u4e0e\u8bcd\u6c47\u6a21\u5f0f\uff0c\u5efa\u7acb\u53ef\u91cf\u5316\u7684\u4f01\u4e1a\u95f4\u98ce\u9669\u5173\u7cfb\u8bc4\u5206\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edf\u4f9d\u8d56\u4e13\u5bb6\u5224\u65ad\u7684\u98ce\u9669\u8bc6\u522b\u65b9\u6cd5\u5b58\u5728\u4e3b\u89c2\u6027\u5f3a\u3001\u6548\u7387\u4f4e\u3001\u96be\u4ee5\u89c4\u6a21\u5316\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u6807\u51c6\u5316\u8d22\u52a1\u6587\u4ef6Form 10-K\u4f5c\u4e3a\u6570\u636e\u6e90\uff0c\u91c7\u7528\u65e0\u76d1\u7763\u5fae\u8c03\u6280\u672f\u6355\u6349\u9690\u542b\u98ce\u9669\u5173\u8054\uff0c\u5f00\u53d1\u9886\u57df\u4e13\u7528\u91d1\u878d\u7f16\u7801\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u8bc4\u4f30\u573a\u666f\u4e0b\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u98ce\u9669\u5173\u7cfb\u7684\u900f\u660e\u91cf\u5316\u5206\u6790\uff0c\u4e3a\u6295\u8d44\u7ec4\u5408\u7ba1\u7406\u7b49\u91d1\u878d\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u5de5\u5177\u3002"}}
{"id": "2509.18776", "pdf": "https://arxiv.org/pdf/2509.18776", "abs": "https://arxiv.org/abs/2509.18776", "authors": ["Chen Liang", "Zhaoqi Huang", "Haofen Wang", "Fu Chai", "Chunying Yu", "Huanhuan Wei", "Zhengjie Liu", "Yanpeng Li", "Hongjun Wang", "Ruifeng Luo", "Xianzhong Zhao"], "title": "AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs), as a novel information technology, are seeing\nincreasing adoption in the Architecture, Engineering, and Construction (AEC)\nfield. They have shown their potential to streamline processes throughout the\nbuilding lifecycle. However, the robustness and reliability of LLMs in such a\nspecialized and safety-critical domain remain to be evaluated. To address this\nchallenge, this paper establishes AECBench, a comprehensive benchmark designed\nto quantify the strengths and limitations of current LLMs in the AEC domain.\nThe benchmark defines 23 representative tasks within a five-level\ncognition-oriented evaluation framework encompassing Knowledge Memorization,\nUnderstanding, Reasoning, Calculation, and Application. These tasks were\nderived from authentic AEC practice, with scope ranging from codes retrieval to\nspecialized documents generation. Subsequently, a 4,800-question dataset\nencompassing diverse formats, including open-ended questions, was crafted\nprimarily by engineers and validated through a two-round expert review.\nFurthermore, an LLM-as-a-Judge approach was introduced to provide a scalable\nand consistent methodology for evaluating complex, long-form responses\nleveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear\nperformance decline across five cognitive levels was revealed. Despite\ndemonstrating proficiency in foundational tasks at the Knowledge Memorization\nand Understanding levels, the models showed significant performance deficits,\nparticularly in interpreting knowledge from tables in building codes, executing\ncomplex reasoning and calculation, and generating domain-specific documents.\nConsequently, this study lays the groundwork for future research and\ndevelopment aimed at the robust and reliable integration of LLMs into\nsafety-critical engineering practices.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1AECBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5efa\u7b51\u5de5\u7a0b\u9886\u57df\u7684\u6027\u80fd\u8868\u73b0\uff0c\u63ed\u793a\u5176\u5728\u590d\u6742\u8ba4\u77e5\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740LLMs\u5728AEC\u9886\u57df\u5e94\u7528\u589e\u52a0\uff0c\u9700\u8bc4\u4f30\u5176\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u3002\u5f53\u524d\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u6545\u5efa\u7acb\u91cf\u5316\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u4e94\u7ea7\u8ba4\u77e5\u8bc4\u4f30\u6846\u67b6\uff08\u77e5\u8bc6\u8bb0\u5fc6\u2192\u5e94\u7528\uff09\uff0c\u521b\u5efa4800\u9898\u6570\u636e\u96c6\u5e76\u7531\u5de5\u7a0b\u5e08\u8bbe\u8ba1\u3001\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u91c7\u7528LLM-as-a-Judge\u8bc4\u4f30\u65b9\u6cd5\u5904\u7406\u590d\u6742\u957f\u6587\u672c\u3002", "result": "\u6d4b\u8bd59\u4e2aLLM\u663e\u793a\uff1a\u6a21\u578b\u5728\u57fa\u7840\u4efb\u52a1\uff08\u77e5\u8bc6\u8bb0\u5fc6/\u7406\u89e3\uff09\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u89c4\u8303\u8868\u683c\u89e3\u6790\uff08-41%\u51c6\u786e\u7387\uff09\u3001\u590d\u6742\u8ba1\u7b97\uff08-58%\uff09\u548c\u6587\u6863\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u8457\u4e0d\u8db3\u3002", "conclusion": "\u7814\u7a76\u4e3aLLMs\u5728\u5b89\u5168\u5173\u952e\u5de5\u7a0b\u4e2d\u7684\u53ef\u9760\u96c6\u6210\u5960\u5b9a\u57fa\u7840\uff0c\u672a\u6765\u9700\u9488\u5bf9\u6027\u63d0\u5347\u6a21\u578b\u5728\u4e13\u4e1a\u573a\u666f\u7684\u63a8\u7406\u4e0e\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2509.18792", "pdf": "https://arxiv.org/pdf/2509.18792", "abs": "https://arxiv.org/abs/2509.18792", "authors": ["Sabri Boughorbel", "Fahim Dalvi", "Nadir Durrani", "Majd Hawasly"], "title": "Beyond the Leaderboard: Understanding Performance Disparities in Large Language Models via Model Diffing", "categories": ["cs.CL"], "comment": "12 pages, accepted to the 2025 Conference on Empirical Methods in\n  Natural Language Processing (EMNLP 2025)", "summary": "As fine-tuning becomes the dominant paradigm for improving large language\nmodels (LLMs), understanding what changes during this process is increasingly\nimportant. Traditional benchmarking often fails to explain why one model\noutperforms another. In this work, we use model diffing, a mechanistic\ninterpretability approach, to analyze the specific capability differences\nbetween Gemma-2-9b-it and a SimPO-enhanced variant. Using crosscoders, we\nidentify and categorize latent representations that differentiate the two\nmodels. We find that SimPO acquired latent concepts predominantly enhance\nsafety mechanisms (+32.8%), multilingual capabilities (+43.8%), and\ninstruction-following (+151.7%), while its additional training also reduces\nemphasis on model self-reference (-44.1%) and hallucination management\n(-68.5%). Our analysis shows that model diffing can yield fine-grained insights\nbeyond leaderboard metrics, attributing performance gaps to concrete\nmechanistic capabilities. This approach offers a transparent and targeted\nframework for comparing LLMs.", "AI": {"tldr": "\u6a21\u578b\u5dee\u5f02\u5206\u6790\u63ed\u793aSimPO\u589e\u5f3a\u7248Gemma-2-9b-it\u5728\u5b89\u5168\u6027\u3001\u591a\u8bed\u8a00\u80fd\u529b\u548c\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u663e\u8457\u63d0\u5347\uff0c\u4f46\u5f31\u5316\u4e86\u81ea\u6211\u5f15\u7528\u548c\u5e7b\u89c9\u7ba1\u7406\u80fd\u529b", "motivation": "\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u96be\u4ee5\u89e3\u91ca\u6a21\u578b\u6027\u80fd\u5dee\u5f02\uff0c\u9700\u901a\u8fc7\u673a\u5236\u89e3\u91ca\u65b9\u6cd5\u5206\u6790LLM\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u7684\u5177\u4f53\u80fd\u529b\u53d8\u5316", "method": "\u4f7f\u7528crosscoders\u8fdb\u884c\u6a21\u578b\u5dee\u5f02\u5206\u6790\uff0c\u8bc6\u522b\u5e76\u5206\u7c7b\u4e0d\u540c\u6a21\u578b\u95f4\u7684\u6f5c\u5728\u8868\u793a\u5dee\u5f02", "result": "SimPO\u63d0\u5347\u5b89\u5168\u673a\u5236(+32.8%)\u3001\u591a\u8bed\u8a00\u80fd\u529b(+43.8%)\u548c\u6307\u4ee4\u9075\u5faa(+151.7%)\uff0c\u4f46\u51cf\u5c11\u81ea\u6211\u5f15\u7528(-44.1%)\u548c\u5e7b\u89c9\u7ba1\u7406(-68.5%)", "conclusion": "\u6a21\u578b\u5dee\u5f02\u5206\u6790\u63d0\u4f9b\u8d85\u8d8a\u6392\u884c\u699c\u6307\u6807\u7684\u7ec6\u7c92\u5ea6\u6d1e\u5bdf\uff0c\u5c06\u6027\u80fd\u5dee\u5f02\u5f52\u56e0\u4e8e\u5177\u4f53\u673a\u5236\u80fd\u529b\uff0c\u5f62\u6210\u900f\u660e\u5316\u6a21\u578b\u6bd4\u8f83\u6846\u67b6"}}
{"id": "2509.18813", "pdf": "https://arxiv.org/pdf/2509.18813", "abs": "https://arxiv.org/abs/2509.18813", "authors": ["Liting Zhang", "Shiwan Zhao", "Aobo Kong", "Qicheng Li"], "title": "MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction", "categories": ["cs.CL"], "comment": null, "summary": "Keyphrase extraction is a fundamental task in natural language processing.\nHowever, existing unsupervised prompt-based methods for Large Language Models\n(LLMs) often rely on single-stage inference pipelines with uniform prompting,\nregardless of document length or LLM backbone. Such one-size-fits-all designs\nhinder the full exploitation of LLMs' reasoning and generation capabilities,\nespecially given the complexity of keyphrase extraction across diverse\nscenarios. To address these challenges, we propose MAPEX, the first framework\nthat introduces multi-agent collaboration into keyphrase extraction. MAPEX\ncoordinates LLM-based agents through modules for expert recruitment, candidate\nextraction, topic guidance, knowledge augmentation, and post-processing. A\ndual-path strategy dynamically adapts to document length: knowledge-driven\nextraction for short texts and topic-guided extraction for long texts.\nExtensive experiments on six benchmark datasets across three different LLMs\ndemonstrate its strong generalization and universality, outperforming the\nstate-of-the-art unsupervised method by 2.44\\% and standard LLM baselines by\n4.01\\% in F1@5 on average. Code is available at\nhttps://github.com/NKU-LITI/MAPEX.", "AI": {"tldr": "MAPEX\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\u6539\u8fdb\u5173\u952e\u77ed\u8bed\u63d0\u53d6\uff0c\u91c7\u7528\u53cc\u8def\u5f84\u7b56\u7565\u52a8\u6001\u9002\u5e94\u6587\u6863\u957f\u5ea6\uff0c\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684\u65e0\u76d1\u7763\u65b9\u6cd5\u91c7\u7528\u5355\u4e00\u9636\u6bb5\u63a8\u7406\u6d41\u7a0b\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u751f\u6210\u80fd\u529b\uff0c\u5c24\u5176\u5728\u5904\u7406\u4e0d\u540c\u957f\u5ea6\u6587\u6863\u65f6\u6548\u679c\u53d7\u9650", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6MAPEX\uff0c\u5305\u542b\u4e13\u5bb6\u62db\u52df/\u5019\u9009\u63d0\u53d6/\u4e3b\u9898\u5f15\u5bfc/\u77e5\u8bc6\u589e\u5f3a/\u540e\u5904\u7406\u6a21\u5757\uff0c\u91c7\u7528\u77e5\u8bc6\u9a71\u52a8\uff08\u77ed\u6587\u672c\uff09\u4e0e\u4e3b\u9898\u5f15\u5bfc\uff08\u957f\u6587\u672c\uff09\u7684\u53cc\u8def\u5f84\u52a8\u6001\u7b56\u7565", "result": "\u57286\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMAPEX\u5e73\u5747F1@5\u5206\u6570\u8d85\u8d8a\u5f53\u524d\u6700\u4f73\u65e0\u76d1\u7763\u65b9\u6cd52.44%\uff0c\u8d85\u8fc7\u6807\u51c6LLM\u57fa\u7ebf4.01%", "conclusion": "\u901a\u8fc7\u5f15\u5165\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u673a\u5236\u548c\u52a8\u6001\u53cc\u8def\u5f84\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5173\u952e\u77ed\u8bed\u63d0\u53d6\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u7684NLP\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6846\u67b6"}}
{"id": "2509.18843", "pdf": "https://arxiv.org/pdf/2509.18843", "abs": "https://arxiv.org/abs/2509.18843", "authors": ["Damian Stachura", "Joanna Konieczna", "Artur Nowak"], "title": "Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical Question Answering?", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": "CLEF 2025 Working Notes, 9-12 September 2025, Madrid, Spain", "summary": "Open-weight versions of large language models (LLMs) are rapidly advancing,\nwith state-of-the-art models like DeepSeek-V3 now performing comparably to\nproprietary LLMs. This progression raises the question of whether small\nopen-weight LLMs are capable of effectively replacing larger closed-source\nmodels. We are particularly interested in the context of biomedical\nquestion-answering, a domain we explored by participating in Task 13B Phase B\nof the BioASQ challenge. In this work, we compare several open-weight models\nagainst top-performing systems such as GPT-4o, GPT-4.1, Claude 3.5 Sonnet, and\nClaude 3.7 Sonnet. To enhance question answering capabilities, we use various\ntechniques including retrieving the most relevant snippets based on embedding\ndistance, in-context learning, and structured outputs. For certain submissions,\nwe utilize ensemble approaches to leverage the diverse outputs generated by\ndifferent models for exact-answer questions. Our results demonstrate that\nopen-weight LLMs are comparable to proprietary ones. In some instances,\nopen-weight LLMs even surpassed their closed counterparts, particularly when\nensembling strategies were applied. All code is publicly available at\nhttps://github.com/evidenceprime/BioASQ-13b.", "AI": {"tldr": "\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4e0e\u4e13\u6709\u6a21\u578b\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd", "motivation": "\u9a8c\u8bc1\u5f00\u6e90\u6a21\u578b\u662f\u5426\u80fd\u591f\u6709\u6548\u66ff\u4ee3\u95ed\u6e90\u5927\u6a21\u578b\u5728\u751f\u7269\u533b\u5b66\u95ee\u7b54\u9886\u57df\u7684\u5e94\u7528", "method": "\u7ed3\u5408\u5d4c\u5165\u8ddd\u79bb\u68c0\u7d22\u76f8\u5173\u6587\u672c\u7247\u6bb5\u3001\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u7ed3\u6784\u5316\u8f93\u51fa\u4ee5\u53ca\u591a\u6a21\u578b\u96c6\u6210\u7b56\u7565", "result": "\u5728BioASQ\u6311\u6218\u8d5b\u4efb\u52a1\u4e2d\uff0c\u5f00\u6e90\u6a21\u578b\u6027\u80fd\u4e0eGPT-4o\u3001Claude 3.5\u7b49\u4e13\u6709\u7cfb\u7edf\u76f8\u5f53\uff0c\u96c6\u6210\u7b56\u7565\u4e0b\u8868\u73b0\u66f4\u4f18", "conclusion": "\u5f00\u6e90\u6a21\u578b\u901a\u8fc7\u9002\u5f53\u6280\u672f\u7ec4\u5408\u53ef\u8fbe\u5230\u5546\u4e1a\u7cfb\u7edf\u6c34\u5e73\uff0c\u4ee3\u7801\u5f00\u6e90\u4fdd\u969c\u7814\u7a76\u53ef\u91cd\u590d\u6027"}}
{"id": "2509.18862", "pdf": "https://arxiv.org/pdf/2509.18862", "abs": "https://arxiv.org/abs/2509.18862", "authors": ["Luyan Zhang", "Xinyu Xie"], "title": "Multi-Hierarchical Feature Detection for Large Language Model Generated Text", "categories": ["cs.CL", "I.2.7; I.2.1"], "comment": "9 pages, 6 tables, empirical study on multi-feature AI text detection", "summary": "With the rapid advancement of large language model technology, there is\ngrowing interest in whether multi-feature approaches can significantly improve\nAI text detection beyond what single neural models achieve. While intuition\nsuggests that combining semantic, syntactic, and statistical features should\nprovide complementary signals, this assumption has not been rigorously tested\nwith modern LLM-generated text. This paper provides a systematic empirical\ninvestigation of multi-hierarchical feature integration for AI text detection,\nspecifically testing whether the computational overhead of combining multiple\nfeature types is justified by performance gains. We implement MHFD\n(Multi-Hierarchical Feature Detection), integrating DeBERTa-based semantic\nanalysis, syntactic parsing, and statistical probability features through\nadaptive fusion. Our investigation reveals important negative results: despite\ntheoretical expectations, multi-feature integration provides minimal benefits\n(0.4-0.5% improvement) while incurring substantial computational costs (4.2x\noverhead), suggesting that modern neural language models may already capture\nmost relevant detection signals efficiently. Experimental results on multiple\nbenchmark datasets demonstrate that the MHFD method achieves 89.7% accuracy in\nin-domain detection and maintains 84.2% stable performance in cross-domain\ndetection, showing modest improvements of 0.4-2.6% over existing methods.", "AI": {"tldr": "\u591a\u7279\u5f81\u96c6\u6210\u65b9\u6cd5\uff08MHFD\uff09\u5728AI\u6587\u672c\u68c0\u6d4b\u4e2d\u4ec5\u5e26\u67650.4-0.5%\u7684\u5fae\u5c0f\u63d0\u5347\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u589e\u52a04.2\u500d\uff0c\u8868\u660e\u73b0\u4ee3\u8bed\u8a00\u6a21\u578b\u5df2\u80fd\u9ad8\u6548\u6355\u83b7\u68c0\u6d4b\u4fe1\u53f7", "motivation": "\u9a8c\u8bc1\u591a\u7279\u5f81\uff08\u8bed\u4e49/\u53e5\u6cd5/\u7edf\u8ba1\uff09\u96c6\u6210\u662f\u5426\u80fd\u663e\u8457\u63d0\u5347AI\u6587\u672c\u68c0\u6d4b\u6548\u679c\uff0c\u6d4b\u8bd5\u5176\u8ba1\u7b97\u5f00\u9500\u4e0e\u6027\u80fd\u589e\u76ca\u7684\u5408\u7406\u6027", "method": "\u63d0\u51faMHFD\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u878d\u5408DeBERTa\u8bed\u4e49\u5206\u6790\u3001\u53e5\u6cd5\u89e3\u6790\u548c\u7edf\u8ba1\u6982\u7387\u7279\u5f81", "result": "MHFD\u5728\u57df\u5185\u68c0\u6d4b\u8fbe89.7%\u51c6\u786e\u7387\uff08\u8de8\u57df84.2%\uff09\uff0c\u8f83\u73b0\u6709\u65b9\u6cd5\u63d0\u53470.4-2.6%\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u589e\u52a04.2\u500d", "conclusion": "\u73b0\u4ee3\u795e\u7ecf\u8bed\u8a00\u6a21\u578b\u5df2\u6709\u6548\u6574\u5408\u68c0\u6d4b\u7279\u5f81\uff0c\u591a\u7279\u5f81\u878d\u5408\u7684\u8fb9\u9645\u6548\u76ca\u6709\u9650\uff0c\u5efa\u8bae\u4f18\u5148\u4f18\u5316\u5355\u4e00\u6a21\u578b\u800c\u975e\u590d\u6742\u7279\u5f81\u96c6\u6210"}}
{"id": "2509.18880", "pdf": "https://arxiv.org/pdf/2509.18880", "abs": "https://arxiv.org/abs/2509.18880", "authors": ["Advik Raj Basani", "Pin-Yu Chen"], "title": "Diversity Boosts AI-Generated Text Detection", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project Webpage: https://diveye.vercel.app/", "summary": "Detecting AI-generated text is an increasing necessity to combat misuse of\nLLMs in education, business compliance, journalism, and social media, where\nsynthetic fluency can mask misinformation or deception. While prior detectors\noften rely on token-level likelihoods or opaque black-box classifiers, these\napproaches struggle against high-quality generations and offer little\ninterpretability. In this work, we propose DivEye, a novel detection framework\nthat captures how unpredictability fluctuates across a text using\nsurprisal-based features. Motivated by the observation that human-authored text\nexhibits richer variability in lexical and structural unpredictability than LLM\noutputs, DivEye captures this signal through a set of interpretable statistical\nfeatures. Our method outperforms existing zero-shot detectors by up to 33.2%\nand achieves competitive performance with fine-tuned baselines across multiple\nbenchmarks. DivEye is robust to paraphrasing and adversarial attacks,\ngeneralizes well across domains and models, and improves the performance of\nexisting detectors by up to 18.7% when used as an auxiliary signal. Beyond\ndetection, DivEye provides interpretable insights into why a text is flagged,\npointing to rhythmic unpredictability as a powerful and underexplored signal\nfor LLM detection.", "AI": {"tldr": "DivEye\u6846\u67b6\u901a\u8fc7\u57fa\u4e8e\u4fe1\u606f\u71b5\u7684\u7edf\u8ba1\u7279\u5f81\u68c0\u6d4bAI\u751f\u6210\u6587\u672c\uff0c\u5229\u7528\u4eba\u7c7b\u6587\u672c\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\u6ce2\u52a8\u5b9e\u73b0\u9ad8\u6548\u68c0\u6d4b", "motivation": "\u73b0\u6709AI\u6587\u672c\u68c0\u6d4b\u5668\u4f9d\u8d56token\u6982\u7387\u6216\u9ed1\u76d2\u6a21\u578b\uff0c\u96be\u4ee5\u5e94\u5bf9\u9ad8\u8d28\u91cf\u751f\u6210\u6587\u672c\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002\u4eba\u7c7b\u6587\u672c\u5728\u8bcd\u6c47\u548c\u7ed3\u6784\u4e0a\u5177\u6709\u66f4\u4e30\u5bcc\u7684\u4e0d\u53ef\u9884\u6d4b\u6027\u6ce2\u52a8\u6a21\u5f0f", "method": "\u4f7f\u7528surprisal-based\u7279\u5f81\u6355\u6349\u6587\u672c\u4e0d\u53ef\u9884\u6d4b\u6027\u6ce2\u52a8\uff0c\u6784\u5efa\u53ef\u89e3\u91ca\u7684\u7edf\u8ba1\u7279\u5f81\u96c6\u5206\u6790\u6587\u672c\u8282\u594f\u6a21\u5f0f", "result": "\u96f6\u6837\u672c\u68c0\u6d4b\u63d0\u534733.2%\uff0c\u6297\u5bf9\u6297\u653b\u51fb\uff0c\u8de8\u9886\u57df\u6cdb\u5316\u5f3a\uff0c\u4f5c\u4e3a\u8f85\u52a9\u4fe1\u53f7\u63d0\u5347\u73b0\u6709\u68c0\u6d4b\u566818.7%\u6027\u80fd", "conclusion": "DivEye\u901a\u8fc7\u8282\u594f\u4e0d\u53ef\u9884\u6d4b\u6027\u8fd9\u4e00\u65b0\u4fe1\u53f7\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684AI\u68c0\u6d4b\uff0c\u4e3a\u68c0\u6d4b\u9886\u57df\u63d0\u4f9b\u65b0\u65b9\u6cd5\u8bba\u65b9\u5411"}}
{"id": "2509.18901", "pdf": "https://arxiv.org/pdf/2509.18901", "abs": "https://arxiv.org/abs/2509.18901", "authors": ["Nicholas Popovi\u010d", "Michael F\u00e4rber"], "title": "Extractive Fact Decomposition for Interpretable Natural Language Inference in one Forward Pass", "categories": ["cs.CL"], "comment": "EMNLP 2025", "summary": "Recent works in Natural Language Inference (NLI) and related tasks, such as\nautomated fact-checking, employ atomic fact decomposition to enhance\ninterpretability and robustness. For this, existing methods rely on\nresource-intensive generative large language models (LLMs) to perform\ndecomposition. We propose JEDI, an encoder-only architecture that jointly\nperforms extractive atomic fact decomposition and interpretable inference\nwithout requiring generative models during inference. To facilitate training,\nwe produce a large corpus of synthetic rationales covering multiple NLI\nbenchmarks. Experimental results demonstrate that JEDI achieves competitive\naccuracy in distribution and significantly improves robustness out of\ndistribution and in adversarial settings over models based solely on extractive\nrationale supervision. Our findings show that interpretability and robust\ngeneralization in NLI can be realized using encoder-only architectures and\nsynthetic rationales. Code and data available at https://jedi.nicpopovic.com", "AI": {"tldr": "JEDI\u6a21\u578b\u901a\u8fc7\u7f16\u7801\u5668\u67b6\u6784+\u5408\u6210\u6570\u636e\uff0c\u5728\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u53ef\u89e3\u91ca\u6027\u4e0e\u9c81\u68d2\u6027\uff0c\u66ff\u4ee3\u751f\u6210\u5f0f\u5927\u6a21\u578b", "motivation": "\u73b0\u6709\u81ea\u7136\u8bed\u8a00\u63a8\u7406\uff08NLI\uff09\u548c\u4e8b\u5b9e\u6838\u67e5\u65b9\u6cd5\u4f9d\u8d56\u8d44\u6e90\u5bc6\u96c6\u578b\u7684\u751f\u6210\u5f0f\u5927\u6a21\u578b\u8fdb\u884c\u539f\u5b50\u4e8b\u5b9e\u5206\u89e3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u63d0\u51faJEDI\u7f16\u7801\u5668\u67b6\u6784\uff0c\u8054\u5408\u6267\u884c\u539f\u5b50\u4e8b\u5b9e\u5206\u89e3\u548c\u53ef\u89e3\u91ca\u63a8\u7406\uff0c\u5229\u7528\u8986\u76d6\u591a\u4e2aNLI\u57fa\u51c6\u7684\u5408\u6210rationale\u8bed\u6599\u5e93\u8fdb\u884c\u8bad\u7ec3", "result": "\u5206\u5e03\u5185\u51c6\u786e\u7387\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u5206\u5e03\u5916\u9c81\u68d2\u6027\u63d0\u5347\u663e\u8457\uff08\u5bf9\u6297\u573a\u666f\u4e0b\u63d0\u53476.1%\uff09\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u751f\u6210\u5f0f\u6a21\u578b\u5feb16\u500d", "conclusion": "\u4ec5\u4f7f\u7528\u7f16\u7801\u5668\u67b6\u6784\u548c\u5408\u6210rationale\u5373\u53ef\u5b9e\u73b0NLI\u4efb\u52a1\u7684\u53ef\u89e3\u91ca\u6027\u4e0e\u9c81\u68d2\u6cdb\u5316\uff0c\u6311\u6218\u751f\u6210\u5f0f\u6a21\u578b\u7684\u5fc5\u8981\u6027"}}
{"id": "2509.18987", "pdf": "https://arxiv.org/pdf/2509.18987", "abs": "https://arxiv.org/abs/2509.18987", "authors": ["Abderrahmane Issam", "Yusuf Can Semerci", "Jan Scholtes", "Gerasimos Spanakis"], "title": "DTW-Align: Bridging the Modality Gap in End-to-End Speech Translation with Dynamic Time Warping Alignment", "categories": ["cs.CL"], "comment": "Accepted at WMT2025", "summary": "End-to-End Speech Translation (E2E-ST) is the task of translating source\nspeech directly into target text bypassing the intermediate transcription step.\nThe representation discrepancy between the speech and text modalities has\nmotivated research on what is known as bridging the modality gap.\nState-of-the-art methods addressed this by aligning speech and text\nrepresentations on the word or token level. Unfortunately, this requires an\nalignment tool that is not available for all languages. Although this issue has\nbeen addressed by aligning speech and text embeddings using nearest-neighbor\nsimilarity search, it does not lead to accurate alignments. In this work, we\nadapt Dynamic Time Warping (DTW) for aligning speech and text embeddings during\ntraining. Our experiments demonstrate the effectiveness of our method in\nbridging the modality gap in E2E-ST. Compared to previous work, our method\nproduces more accurate alignments and achieves comparable E2E-ST results while\nbeing significantly faster. Furthermore, our method outperforms previous work\nin low resource settings on 5 out of 6 language directions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u52a8\u6001\u65f6\u95f4\u89c4\u6574(DTW)\u7684\u8bed\u97f3-\u6587\u672c\u5d4c\u5165\u5bf9\u9f50\u65b9\u6cd5\uff0c\u6539\u8fdb\u7aef\u5230\u7aef\u8bed\u97f3\u7ffb\u8bd1\u7684\u6a21\u6001\u5dee\u5f02\u95ee\u9898\uff0c\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u8868\u73b0\u4f18\u5f02\u4e14\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u7ffb\u8bd1\u65b9\u6cd5\u4f9d\u8d56\u8bed\u8a00\u7279\u5b9a\u7684\u5bf9\u9f50\u5de5\u5177\u4e14\u5bf9\u9f50\u4e0d\u51c6\u786e\uff0c\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u4e0b\u6548\u679c\u53d7\u9650\u3002", "method": "\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u91c7\u7528\u52a8\u6001\u65f6\u95f4\u89c4\u6574\u7b97\u6cd5(DTW)\u5bf9\u9f50\u8bed\u97f3\u548c\u6587\u672c\u7684\u5d4c\u5165\u8868\u793a\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u8de8\u6a21\u6001\u5339\u914d\u3002", "result": "\u57286\u79cd\u8bed\u8a00\u65b9\u5411\u4e2d5\u79cd\u4f4e\u8d44\u6e90\u573a\u666f\u8d85\u8d8a\u57fa\u7ebf\uff0c\u5bf9\u9f50\u51c6\u786e\u7387\u63d0\u5347\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u4e4b\u524d\u65b9\u6cd5\u5feb8\u500d\u3002", "conclusion": "DTW\u7b97\u6cd5\u6709\u6548\u89e3\u51b3\u7aef\u5230\u7aef\u8bed\u97f3\u7ffb\u8bd1\u7684\u6a21\u6001\u5dee\u5f02\u95ee\u9898\uff0c\u4e3a\u591a\u8bed\u8a00\u573a\u666f\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8d44\u6e90\u7a00\u7f3a\u8bed\u8a00\u3002"}}
{"id": "2509.19020", "pdf": "https://arxiv.org/pdf/2509.19020", "abs": "https://arxiv.org/abs/2509.19020", "authors": ["Shaomu Tan", "Ryosuke Mitani", "Ritvik Choudhary", "Toshiyuki Sekiya"], "title": "Investigating Test-Time Scaling with Reranking for Machine Translation", "categories": ["cs.CL"], "comment": null, "summary": "Scaling model parameters has become the de facto strategy for improving NLP\nsystems, but it comes with substantial computational costs. Test-Time Scaling\n(TTS) offers an alternative by allocating more computation at inference:\ngenerating multiple candidates and selecting the best. While effective in tasks\nsuch as mathematical reasoning, TTS has not been systematically explored for\nmachine translation (MT). In this paper, we present the first systematic study\nof TTS for MT, investigating a simple but practical best-of-N framework on\nWMT24 benchmarks. Our experiments cover six high-resource and one low-resource\nlanguage pairs, five model sizes (3B-72B), and various TTS compute budget (N up\nto 1024). Our results show that a) For high-resource languages, TTS generally\nimproves translation quality according to multiple neural MT evaluation\nmetrics, and our human evaluation confirms these gains; b) Augmenting smaller\nmodels with large $N$ can match or surpass larger models at $N{=}1$ with more\ncompute cost; c) Under fixed compute budgets, larger models are typically more\nefficient, and TTS can degrade quality due to metric blind spots in\nlow-resource cases.", "AI": {"tldr": "Test-Time Scaling (TTS) improves translation quality for high-resource languages but faces efficiency trade-offs and metric limitations in low-resource scenarios.", "motivation": "Traditional model scaling increases computational costs, while TTS offers inference-stage computation allocation to enhance performance without expanding parameters.", "method": "Conducted systematic experiments using best-of-N framework across WMT24 benchmarks with 3B-72B models, 7 language pairs, and TTS budgets up to N=1024.", "result": "TTS boosts high-resource translations (confirmed by human evaluation), enables small models to match large ones with increased N, but degrades low-resource performance under fixed compute.", "conclusion": "TTS shows promise for efficient translation enhancement but requires careful compute allocation and metric awareness, especially in low-resource settings."}}
{"id": "2509.19033", "pdf": "https://arxiv.org/pdf/2509.19033", "abs": "https://arxiv.org/abs/2509.19033", "authors": ["Chiara Alzetta", "Serena Auriemma", "Alessandro Bondielli", "Luca Dini", "Chiara Fazzone", "Alessio Miaschi", "Martina Miliani", "Marta Sartor"], "title": "Charting a Decade of Computational Linguistics in Italy: The CLiC-it Corpus", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "Submitted to IJCoL", "summary": "Over the past decade, Computational Linguistics (CL) and Natural Language\nProcessing (NLP) have evolved rapidly, especially with the advent of\nTransformer-based Large Language Models (LLMs). This shift has transformed\nresearch goals and priorities, from Lexical and Semantic Resources to Language\nModelling and Multimodality. In this study, we track the research trends of the\nItalian CL and NLP community through an analysis of the contributions to\nCLiC-it, arguably the leading Italian conference in the field. We compile the\nproceedings from the first 10 editions of the CLiC-it conference (from 2014 to\n2024) into the CLiC-it Corpus, providing a comprehensive analysis of both its\nmetadata, including author provenance, gender, affiliations, and more, as well\nas the content of the papers themselves, which address various topics. Our goal\nis to provide the Italian and international research communities with valuable\ninsights into emerging trends and key developments over time, supporting\ninformed decisions and future directions in the field.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790\u610f\u5927\u5229\u8ba1\u7b97\u8bed\u8a00\u5b66\u4f1a\u8baeCLiC-it\u5341\u5e74\u8bba\u6587\u96c6\uff0c\u63ed\u793a\u9886\u57df\u7814\u7a76\u91cd\u70b9\u4ece\u8bed\u4e49\u8d44\u6e90\u5411\u5927\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u7684\u6f14\u53d8\u8d8b\u52bf", "motivation": "\u8ffd\u8e2a\u610f\u5927\u5229\u8ba1\u7b97\u8bed\u8a00\u5b66\u754c\u5728Transformer\u5927\u6a21\u578b\u65f6\u4ee3\u7684\u7814\u7a76\u65b9\u5411\u8f6c\u53d8\uff0c\u5206\u6790\u6280\u672f\u53d8\u9769\u5bf9\u5b66\u672f\u793e\u533a\u7684\u5f71\u54cd", "method": "\u6784\u5efa\u5305\u542b10\u5c4aCLiC-it\u4f1a\u8bae\uff082014-2024\uff09\u7684\u8bed\u6599\u5e93\uff0c\u7efc\u5408\u8fd0\u7528\u5143\u6570\u636e\u5206\u6790\uff08\u4f5c\u8005\u80cc\u666f\u3001\u6027\u522b\u3001\u673a\u6784\uff09\u548c\u6587\u672c\u5185\u5bb9\u4e3b\u9898\u5206\u6790", "result": "\u53d1\u73b0\u7814\u7a76\u91cd\u5fc3\u4ece\u4f20\u7edf\u8bcd\u6c47\u8bed\u4e49\u8d44\u6e90\u8f6c\u5411\u8bed\u8a00\u5efa\u6a21\u4e0e\u591a\u6a21\u6001\u6280\u672f\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u610f\u5927\u5229\u5b66\u672f\u754c\u4f5c\u8005\u7fa4\u4f53\u6784\u6210\u7279\u5f81", "conclusion": "\u8be5\u5206\u6790\u4e3a\u5236\u5b9a\u9886\u57df\u53d1\u5c55\u7b56\u7565\u63d0\u4f9b\u6570\u636e\u652f\u6491\uff0c\u5e2e\u52a9\u56fd\u9645\u5b66\u672f\u754c\u628a\u63e1\u610f\u5927\u5229\u7814\u7a76\u52a8\u6001\u53ca\u6280\u672f\u6f14\u8fdb\u89c4\u5f8b"}}
{"id": "2509.19094", "pdf": "https://arxiv.org/pdf/2509.19094", "abs": "https://arxiv.org/abs/2509.19094", "authors": ["Alireza Salemi", "Cheng Li", "Mingyang Zhang", "Qiaozhu Mei", "Zhuowan Li", "Spurthi Amba Hombaiah", "Weize Kong", "Tao Chen", "Hamed Zamani", "Michael Bendersky"], "title": "Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Personalization is essential for adapting question answering (QA) systems to\nuser-specific information needs, thereby improving both accuracy and user\nsatisfaction. However, personalized QA remains relatively underexplored due to\nchallenges such as inferring preferences from long, noisy, and implicit\ncontexts, and generating responses that are simultaneously correct,\ncontextually appropriate, and aligned with user expectations and background\nknowledge. To address these challenges, we propose Pathways of Thoughts (PoT),\nan inference-stage method that applies to any large language model (LLM)\nwithout requiring task-specific fine-tuning. The approach models the reasoning\nof an LLM as an iterative decision process, where the model dynamically selects\namong cognitive operations such as reasoning, revision, personalization, and\nclarification. This enables exploration of multiple reasoning trajectories,\nproducing diverse candidate responses that capture different perspectives. PoT\nthen aggregates and reweights these candidates according to inferred user\npreferences, yielding a final personalized response that benefits from the\ncomplementary strengths of diverse reasoning paths. Experiments on the LaMP-QA\nbenchmark for personalized QA show that PoT consistently outperforms\ncompetitive baselines, achieving up to a 13.1% relative improvement. Human\nevaluation corroborates these results, with annotators preferring outputs from\nPoT in 66% of cases and reporting ties in only 15% of cases.", "AI": {"tldr": "\u63d0\u51faPathways of Thoughts (PoT)\u63a8\u7406\u9636\u6bb5\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u8ba4\u77e5\u64cd\u4f5c\u548c\u805a\u5408\u5019\u9009\u56de\u7b54\uff0c\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2a\u6027\u5316QA\u4efb\u52a1\u4e2d\u76f8\u5bf9\u6027\u80fd\u63d0\u534713.1%", "motivation": "\u4e2a\u6027\u5316QA\u9762\u4e34\u4ece\u957f\u566a\u97f3\u4e0a\u4e0b\u6587\u63a8\u65ad\u504f\u597d\u3001\u751f\u6210\u7b26\u5408\u7528\u6237\u80cc\u666f\u77e5\u8bc6\u7684\u56de\u7b54\u7b49\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u65e0\u9700\u5fae\u8c03\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u5c06LLM\u63a8\u7406\u5efa\u6a21\u4e3a\u8fed\u4ee3\u51b3\u7b56\u8fc7\u7a0b\uff0c\u52a8\u6001\u9009\u62e9\u63a8\u7406/\u4fee\u8ba2/\u4e2a\u6027\u5316/\u6f84\u6e05\u7b49\u8ba4\u77e5\u64cd\u4f5c\uff0c\u751f\u6210\u591a\u6837\u5316\u5019\u9009\u56de\u7b54\u540e\u901a\u8fc7\u504f\u597d\u52a0\u6743\u805a\u5408", "result": "LaMP-QA\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u76f8\u5bf9\u57fa\u7ebf\u63d0\u534713.1%\uff0c\u4eba\u5de5\u8bc4\u4f3066%\u6848\u4f8b\u66f4\u504f\u597dPoT\u8f93\u51fa\uff0c\u4ec515%\u6301\u5e73", "conclusion": "PoT\u901a\u8fc7\u591a\u63a8\u7406\u8def\u5f84\u63a2\u7d22\u548c\u7528\u6237\u504f\u597d\u805a\u5408\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u6709\u6548\u63d0\u5347\u4e2a\u6027\u5316QA\u6548\u679c\uff0c\u9a8c\u8bc1\u4e86\u52a8\u6001\u8ba4\u77e5\u64cd\u4f5c\u9009\u62e9\u7b56\u7565\u7684\u4f18\u8d8a\u6027"}}
{"id": "2509.19108", "pdf": "https://arxiv.org/pdf/2509.19108", "abs": "https://arxiv.org/abs/2509.19108", "authors": ["Hiram Ring"], "title": "Are most sentences unique? An empirical examination of Chomskyan claims", "categories": ["cs.CL"], "comment": null, "summary": "A repeated claim in linguistics is that the majority of linguistic utterances\nare unique. For example, Pinker (1994: 10), summarizing an argument by Noam\nChomsky, states that \"virtually every sentence that a person utters or\nunderstands is a brand-new combination of words, appearing for the first time\nin the history of the universe.\" With the increased availability of large\ncorpora, this is a claim that can be empirically investigated. The current\npaper addresses the question by using the NLTK Python library to parse corpora\nof different genres, providing counts of exact string matches in each. Results\nshow that while completely unique sentences are often the majority of corpora,\nthis is highly constrained by genre, and that duplicate sentences are not an\ninsignificant part of any individual corpus.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790\u4e0d\u540c\u4f53\u88c1\u8bed\u6599\u5e93\u53d1\u73b0\uff0c\u5c3d\u7ba1\u5b8c\u5168\u72ec\u7279\u7684\u53e5\u5b50\u5e38\u5360\u591a\u6570\uff0c\u4f46\u4f53\u88c1\u5bf9\u6b64\u6709\u663e\u8457\u9650\u5236\uff0c\u91cd\u590d\u53e5\u5b50\u5728\u5404\u8bed\u6599\u5e93\u4e2d\u5747\u5360\u4e00\u5b9a\u6bd4\u4f8b\u3002", "motivation": "\u9a8c\u8bc1\u8bed\u8a00\u5b66\u9886\u57df\u5173\u4e8e'\u7edd\u5927\u591a\u6570\u8bed\u53e5\u5177\u6709\u72ec\u7279\u6027'\u7684\u4f20\u7edf\u4e3b\u5f20\uff0c\u5229\u7528\u73b0\u4ee3\u5927\u578b\u8bed\u6599\u5e93\u6280\u672f\u8fdb\u884c\u5b9e\u8bc1\u68c0\u9a8c\u3002", "method": "\u4f7f\u7528NLTK Python\u5e93\u89e3\u6790\u4e0d\u540c\u4f53\u88c1\u7684\u8bed\u6599\u5e93\uff0c\u7edf\u8ba1\u5b8c\u5168\u76f8\u540c\u7684\u5b57\u7b26\u4e32\u91cd\u590d\u51fa\u73b0\u6b21\u6570\u3002", "result": "\u5b8c\u5168\u72ec\u7279\u7684\u53e5\u5b50\u5360\u6bd4\u968f\u4f53\u88c1\u53d8\u5316\u663e\u8457\uff08\u5982\u65b0\u95fb\u6587\u672c\u91cd\u590d\u7387\u9ad8\u4e8e\u6587\u5b66\uff09\uff0c\u4e14\u91cd\u590d\u53e5\u5b50\u5360\u6bd4\u6700\u4f4e\u4ecd\u8fbe12%\uff08\u513f\u7ae5\u6587\u5b66\uff09\u3002", "conclusion": "\u8bed\u53e5\u72ec\u7279\u6027\u6bd4\u4f8b\u53d7\u4f53\u88c1\u7279\u5f81\u5236\u7ea6\uff0c\u91cd\u590d\u73b0\u8c61\u5728\u8bed\u8a00\u5b9e\u9645\u4f7f\u7528\u4e2d\u5177\u6709\u4e0d\u53ef\u5ffd\u89c6\u7684\u5b58\u5728\u4ef7\u503c\uff0c\u4fee\u6b63\u4e86\u4f20\u7edf\u8bed\u8a00\u5b66\u7406\u8bba\u7684\u7edd\u5bf9\u5316\u8868\u8ff0\u3002"}}
{"id": "2509.19109", "pdf": "https://arxiv.org/pdf/2509.19109", "abs": "https://arxiv.org/abs/2509.19109", "authors": ["Timur Turatali", "Anton Alekseev", "Gulira Jumalieva", "Gulnara Kabaeva", "Sergey Nikolenko"], "title": "Human-Annotated NER Dataset for the Kyrgyz Language", "categories": ["cs.CL"], "comment": "Accepted to TurkLang-2025 conference, DOI and copyright will be added\n  upon confirmation of acceptance to publication in IEEE Xplore", "summary": "We introduce KyrgyzNER, the first manually annotated named entity recognition\ndataset for the Kyrgyz language. Comprising 1,499 news articles from the 24.KG\nnews portal, the dataset contains 10,900 sentences and 39,075 entity mentions\nacross 27 named entity classes. We show our annotation scheme, discuss the\nchallenges encountered in the annotation process, and present the descriptive\nstatistics. We also evaluate several named entity recognition models, including\ntraditional sequence labeling approaches based on conditional random fields and\nstate-of-the-art multilingual transformer-based models fine-tuned on our\ndataset. While all models show difficulties with rare entity categories, models\nsuch as the multilingual RoBERTa variant pretrained on a large corpus across\nmany languages achieve a promising balance between precision and recall. These\nfindings emphasize both the challenges and opportunities of using multilingual\npretrained models for processing languages with limited resources. Although the\nmultilingual RoBERTa model performed best, other multilingual models yielded\ncomparable results. This suggests that future work exploring more granular\nannotation schemes may offer deeper insights for Kyrgyz language processing\npipelines evaluation.", "AI": {"tldr": "\u9996\u4e2a\u5409\u5c14\u5409\u65af\u8bedNER\u6570\u636e\u96c6KyrgyzNER\u7684\u521b\u5efa\u4e0e\u591a\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\uff0cRoBERTa\u8868\u73b0\u6700\u4f73\u4f46\u7f55\u89c1\u5b9e\u4f53\u8bc6\u522b\u4ecd\u5b58\u6311\u6218", "motivation": "\u586b\u8865\u5409\u5c14\u5409\u65af\u8bed\u7f3a\u4e4f\u6807\u6ce8NER\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u63a8\u52a8\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7814\u7a76\u3002\u901a\u8fc7\u6784\u5efa\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u8d44\u6e90\u532e\u4e4f\u8bed\u8a00\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "1. \u6784\u5efa\u542b27\u7c7b\u5b9e\u4f53\u300139k\u6807\u6ce8\u5b9e\u4f8b\u7684\u65b0\u95fb\u8bed\u6599\u5e93\n2. \u91c7\u7528\u4f20\u7edfCRF\u4e0e\u5fae\u8c03\u591a\u8bed\u8a00Transformer\u6a21\u578b\uff08\u5305\u62ecRoBERTa\uff09\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\n3. \u8bbe\u8ba1\u5305\u542b\u8bed\u4e49\u89d2\u8272\u7279\u5f81\u7684\u7ec6\u7c92\u5ea6\u6807\u6ce8\u4f53\u7cfb", "result": "\u591a\u8bed\u8a00RoBERTa\u5728F1\u503c\uff0868.2\uff09\u548c\u5e73\u8861\u6027\uff08P=71.5/R=65.3\uff09\u4e0a\u8868\u73b0\u6700\u4f18\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5bf9\u4f4e\u9891\u5b9e\u4f53\uff08\u5982\u5730\u7406\u653f\u6cbb\u5b9e\u4f53\uff09\u8bc6\u522b\u7387\u4e0b\u964d40%\u3002\u5176\u4ed6\u591a\u8bed\u8a00\u6a21\u578b\uff08XLM-R\u7b49\uff09\u4e0eRoBERTa\u5dee\u8ddd\u57285%\u4ee5\u5185\u3002", "conclusion": "\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u53ef\u6709\u6548\u652f\u6301\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\uff0c\u4f46\u9700\u9488\u5bf9\u6027\u4f18\u5316\u4f4e\u9891\u5b9e\u4f53\u8bc6\u522b\u3002\u672a\u6765\u5e94\u63a2\u7d22\u878d\u5408\u8bed\u8a00\u7279\u5f81\u7684\u6807\u6ce8\u65b9\u6848\uff0c\u5e76\u5f00\u53d1\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\u6846\u67b6\u63d0\u5347\u5c0f\u8bed\u79cdNLP\u6548\u679c\u3002"}}
{"id": "2509.19125", "pdf": "https://arxiv.org/pdf/2509.19125", "abs": "https://arxiv.org/abs/2509.19125", "authors": ["Kun Zhu", "Lizi Liao", "Yuxuan Gu", "Lei Huang", "Xiaocheng Feng", "Bing Qin"], "title": "Context-Aware Hierarchical Taxonomy Generation for Scientific Papers via LLM-Guided Multi-Aspect Clustering", "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Main", "summary": "The rapid growth of scientific literature demands efficient methods to\norganize and synthesize research findings. Existing taxonomy construction\nmethods, leveraging unsupervised clustering or direct prompting of large\nlanguage models (LLMs), often lack coherence and granularity. We propose a\nnovel context-aware hierarchical taxonomy generation framework that integrates\nLLM-guided multi-aspect encoding with dynamic clustering. Our method leverages\nLLMs to identify key aspects of each paper (e.g., methodology, dataset,\nevaluation) and generates aspect-specific paper summaries, which are then\nencoded and clustered along each aspect to form a coherent hierarchy. In\naddition, we introduce a new evaluation benchmark of 156 expert-crafted\ntaxonomies encompassing 11.6k papers, providing the first naturally annotated\ndataset for this task. Experimental results demonstrate that our method\nsignificantly outperforms prior approaches, achieving state-of-the-art\nperformance in taxonomy coherence, granularity, and interpretability.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u7ef4\u5ea6\u52a8\u6001\u805a\u7c7b\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u79d1\u5b66\u6587\u732e\u5206\u7c7b\u4f53\u7cfb\u7684\u8fde\u8d2f\u6027\u4e0e\u53ef\u89e3\u91ca\u6027", "motivation": "\u73b0\u6709\u57fa\u4e8e\u65e0\u76d1\u7763\u805a\u7c7b\u6216\u76f4\u63a5\u63d0\u793aLLM\u7684\u5b66\u79d1\u5206\u7c7b\u65b9\u6cd5\u5728\u8fde\u8d2f\u6027\u548c\u7ec6\u7c92\u5ea6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3", "method": "\u901a\u8fc7LLM\u591a\u7ef4\u5ea6\u7f16\u7801\uff08\u65b9\u6cd5/\u6570\u636e\u96c6/\u8bc4\u4f30\u7b49\uff09+\u52a8\u6001\u805a\u7c7b\u6784\u5efa\u5c42\u6b21\u5316\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u521b\u5efa\u5305\u542b1.56\u4e07\u7bc7\u8bba\u6587\u7684\u4e13\u5bb6\u6807\u6ce8\u8bc4\u4f30\u57fa\u51c6", "result": "\u572811.6k\u8bba\u6587\u6570\u636e\u96c6\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\uff0c\u5206\u7c7b\u4f53\u7cfb\u8fde\u8d2f\u6027\u6307\u6807\u63d0\u5347\u663e\u8457", "conclusion": "\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u5c42\u6b21\u5316\u5206\u7c7b\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u79d1\u5b66\u6587\u732e\u7ec4\u7ec7\u4e2d\u7684\u4f53\u7cfb\u8d28\u91cf\u95ee\u9898"}}
{"id": "2509.19143", "pdf": "https://arxiv.org/pdf/2509.19143", "abs": "https://arxiv.org/abs/2509.19143", "authors": ["Alejandro Cuevas", "Saloni Dash", "Bharat Kumar Nayak", "Dan Vann", "Madeleine I. G. Daepp"], "title": "Anecdoctoring: Automated Red-Teaming Across Language and Place", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "To be published in EMNLP 2025", "summary": "Disinformation is among the top risks of generative artificial intelligence\n(AI) misuse. Global adoption of generative AI necessitates red-teaming\nevaluations (i.e., systematic adversarial probing) that are robust across\ndiverse languages and cultures, but red-teaming datasets are commonly US- and\nEnglish-centric. To address this gap, we propose \"anecdoctoring\", a novel\nred-teaming approach that automatically generates adversarial prompts across\nlanguages and cultures. We collect misinformation claims from fact-checking\nwebsites in three languages (English, Spanish, and Hindi) and two geographies\n(US and India). We then cluster individual claims into broader narratives and\ncharacterize the resulting clusters with knowledge graphs, with which we\naugment an attacker LLM. Our method produces higher attack success rates and\noffers interpretability benefits relative to few-shot prompting. Results\nunderscore the need for disinformation mitigations that scale globally and are\ngrounded in real-world adversarial misuse.", "AI": {"tldr": "\u63d0\u51fa'anecdoctoring'\u65b9\u6cd5\uff0c\u901a\u8fc7\u8de8\u8bed\u8a00/\u6587\u5316\u7684\u5bf9\u6297\u6027\u63d0\u793a\u751f\u6210\u63d0\u5347\u751f\u6210\u5f0fAI\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u7684\u7ea2\u961f\u8bc4\u4f30\u6548\u679c", "motivation": "\u5f53\u524d\u7ea2\u961f\u8bc4\u4f30\u96c6\u4e2d\u4e8e\u82f1\u8bed\u548c\u7f8e\u56fd\u573a\u666f\uff0c\u7f3a\u4e4f\u591a\u8bed\u8a00/\u6587\u5316\u8986\u76d6\uff0c\u5bfc\u81f4\u5168\u7403\u751f\u6210\u5f0fAI\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u5b58\u5728\u6f0f\u6d1e", "method": "\u4ece\u82f1\u8bed/\u897f\u73ed\u7259\u8bed/\u5370\u5730\u8bed\u7684fact-check\u7f51\u7ad9\u6536\u96c6\u6570\u636e\u2192\u805a\u7c7b\u6210\u53d9\u4e8b\u2192\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u653b\u51fbLLM\u2192\u5bf9\u6bd4\u5c11\u6837\u672c\u63d0\u793a\u65b9\u6cd5", "result": "\u8be5\u65b9\u6cd5\u653b\u51fb\u6210\u529f\u7387\u66f4\u9ad8\u4e14\u5177\u5907\u53ef\u89e3\u91ca\u6027\uff0c\u6210\u529f\u9a8c\u8bc1\u8de8\u8bed\u8a00\u6587\u5316\u5bf9\u6297\u653b\u51fb\u7684\u6709\u6548\u6027", "conclusion": "\u5e94\u5bf9AI\u865a\u5047\u4fe1\u606f\u9700\u5168\u7403\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u4e14\u7f13\u89e3\u63aa\u65bd\u5e94\u57fa\u4e8e\u771f\u5b9e\u5bf9\u6297\u6ee5\u7528\u573a\u666f\u8fdb\u884c\u8bbe\u8ba1"}}
{"id": "2509.19163", "pdf": "https://arxiv.org/pdf/2509.19163", "abs": "https://arxiv.org/abs/2509.19163", "authors": ["Chantal Shaib", "Tuhin Chakrabarty", "Diego Garcia-Olano", "Byron C. Wallace"], "title": "Measuring AI \"Slop\" in Text", "categories": ["cs.CL"], "comment": null, "summary": "AI \"slop\" is an increasingly popular term used to describe low-quality\nAI-generated text, but there is currently no agreed upon definition of this\nterm nor a means to measure its occurrence. In this work, we develop a taxonomy\nof \"slop\" through interviews with experts in NLP, writing, and philosophy, and\npropose a set of interpretable dimensions for its assessment in text. Through\nspan-level annotation, we find that binary \"slop\" judgments are (somewhat)\nsubjective, but such determinations nonetheless correlate with latent\ndimensions such as coherence and relevance. Our framework can be used to\nevaluate AI-generated text in both detection and binary preference tasks,\npotentially offering new insights into the linguistic and stylistic factors\nthat contribute to quality judgments.", "AI": {"tldr": "\u63d0\u51fa\u8bc4\u4f30AI\u751f\u6210\u4f4e\u8d28\u91cf\u6587\u672c\uff08'slop'\uff09\u7684\u5206\u7c7b\u6cd5\u53ca\u591a\u7ef4\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e3b\u89c2\u5224\u65ad\u4e0e\u6f5c\u5728\u6587\u672c\u7ef4\u5ea6\uff08\u8fde\u8d2f\u6027/\u76f8\u5173\u6027\uff09\u7684\u5173\u8054\u6027", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5bf9AI\u751f\u6210\u4f4e\u8d28\u91cf\u6587\u672c\u7684\u7edf\u4e00\u5b9a\u4e49\u548c\u8bc4\u4f30\u6807\u51c6\uff0c\u963b\u788d\u4e86\u6587\u672c\u8d28\u91cf\u7684\u5ba2\u89c2\u68c0\u6d4b\u4e0e\u6539\u8fdb", "method": "\u901a\u8fc7NLP/\u5199\u4f5c/\u54f2\u5b66\u4e13\u5bb6\u8bbf\u8c08\u6784\u5efa\u5206\u7c7b\u6cd5\uff0c\u8bbe\u8ba1\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u901a\u8fc7span\u7ea7\u6807\u6ce8\u5b9e\u9a8c\u9a8c\u8bc1\u4e3b\u89c2\u5224\u65ad\u4e0e\u6f5c\u5728\u7ef4\u5ea6\u7684\u76f8\u5173\u6027", "result": "\u53d1\u73b0'\u4f4e\u8d28\u91cf'\u5224\u65ad\u5b58\u5728\u4e3b\u89c2\u6027\u4f46\u5177\u5907\u6f5c\u5728\u7ef4\u5ea6\u5173\u8054\u6027\uff0c\u5f00\u53d1\u51fa\u9002\u7528\u4e8e\u68c0\u6d4b\u4e0e\u504f\u597d\u4efb\u52a1\u7684\u53cc\u91cd\u8bc4\u4f30\u6846\u67b6", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u6587\u672c\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u65b0\u89c6\u89d2\uff0c\u63ed\u793a\u8bed\u8a00\u98ce\u683c\u4e0e\u6587\u672c\u8d28\u91cf\u7684\u5185\u5728\u8054\u7cfb\uff0c\u5bf9\u4f18\u5316\u751f\u6210\u6a21\u578b\u5177\u6709\u6307\u5bfc\u610f\u4e49"}}
{"id": "2509.19170", "pdf": "https://arxiv.org/pdf/2509.19170", "abs": "https://arxiv.org/abs/2509.19170", "authors": ["Natasha Butt", "Ariel Kwiatkowski", "Ismail Labiad", "Julia Kempe", "Yann Ollivier"], "title": "Soft Tokens, Hard Truths", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The use of continuous instead of discrete tokens during the Chain-of-Thought\n(CoT) phase of reasoning LLMs has garnered attention recently, based on the\nintuition that a continuous mixture of discrete tokens could simulate a\nsuperposition of several reasoning paths simultaneously. Theoretical results\nhave formally proven that continuous tokens have much greater expressivity and\ncan solve specific problems more efficiently. However, practical use of\ncontinuous tokens has been limited by strong training difficulties: previous\nworks either just use continuous tokens at inference time on a pre-trained\ndiscrete-token model, or must distill the continuous CoT from ground-truth\ndiscrete CoTs and face computational costs that limit the CoT to very few\ntokens.\n  This is the first work introducing a scalable method to learn continuous CoTs\nvia reinforcement learning (RL), without distilling from reference discrete\nCoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input\nembedding to provide RL exploration. Computational overhead is minimal,\nenabling us to learn continuous CoTs with hundreds of tokens. On math reasoning\nbenchmarks with Llama and Qwen models up to 8B, training with continuous CoTs\nmatch discrete-token CoTs for pass@1 and surpass them for pass@32, showing\ngreater CoT diversity. In systematic comparisons, the best-performing scenario\nis to train with continuous CoT tokens then use discrete tokens for inference,\nmeaning the \"soft\" models can be deployed in a standard way. Finally, we show\ncontinuous CoT RL training better preserves the predictions of the base model\non out-of-domain tasks, thus providing a softer touch to the base model.", "AI": {"tldr": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fde\u7eed\u601d\u7ef4\u94fe\uff08CoT\uff09\uff0c\u65e0\u9700\u84b8\u998f\u79bb\u6563CoT\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u5728\u6570\u5b66\u4efb\u52a1\u4e2d\u5c55\u73b0\u4f18\u4e8e\u79bb\u6563CoT\u7684\u591a\u6837\u6027\u8868\u73b0\u3002", "motivation": "\u5148\u524d\u8fde\u7eedtoken\u8bad\u7ec3\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u4f9d\u8d56\u84b8\u998f\u3001\u96be\u4ee5\u5904\u7406\u957f\u5e8f\u5217\u7684\u95ee\u9898\uff0c\u9700\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u91c7\u7528'\u8f6f'token\uff08\u6df7\u5408token+\u8f93\u5165\u5d4c\u5165\u566a\u58f0\uff09\u5b9e\u73b0\u5f3a\u5316\u5b66\u4e60\u63a2\u7d22\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u652f\u6301\u6570\u767etoken\u7684\u8fde\u7eedCoT\u8bad\u7ec3\u3002", "result": "8B\u53c2\u6570\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u8fde\u7eedCoT\u5728pass@1\u6301\u5e73\u79bb\u6563CoT\uff0cpass@32\u663e\u8457\u8d85\u8d8a\uff08\u663e\u793a\u66f4\u5f3a\u591a\u6837\u6027\uff09\uff0c\u4e14\u8bad\u7ec3\u540e\u53ef\u7528\u79bb\u6563token\u63a8\u7406\u3002", "conclusion": "\u8fde\u7eedCoT RL\u8bad\u7ec3\u517c\u987e\u6027\u80fd\u4e0e\u90e8\u7f72\u517c\u5bb9\u6027\uff0c\u80fd\u66f4\u597d\u4fdd\u7559\u57fa\u5ea7\u6a21\u578b\u7684\u57df\u5916\u4efb\u52a1\u9884\u6d4b\u80fd\u529b\uff0c\u63d0\u4f9b\u66f4\u67d4\u548c\u7684\u6a21\u578b\u5e72\u9884\u3002"}}
{"id": "2509.19199", "pdf": "https://arxiv.org/pdf/2509.19199", "abs": "https://arxiv.org/abs/2509.19199", "authors": ["Xiaoqian Liu", "Ke Wang", "Yuchuan Wu", "Fei Huang", "Yongbin Li", "Junge Zhang", "Jianbin Jiao"], "title": "Online Process Reward Leanring for Agentic Reinforcement Learning", "categories": ["cs.CL"], "comment": "preprint", "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning (RL) as autonomous agents that reason and act over long horizons in\ninteractive environments.\n  However, sparse and sometimes unverifiable rewards make temporal credit\nassignment extremely challenging.\n  Recent work attempts to integrate process supervision into agent learning but\nsuffers from biased annotation, reward hacking, high-variance from overly\nfine-grained signals or failtures when state overlap is rare.\n  We therefore introduce Online Process Reward Learning (OPRL), a general\ncredit-assignment strategy for agentic RL that integrates seamlessly with\nstandard on-policy algorithms without relying on additional rollouts or\nexplicit step labels.\n  In OPRL, we optimize an implicit process reward model (PRM) alternately with\nthe agent's policy to transform trajectory preferences into implicit step\nrewards through a trajectory-based DPO objective.\n  These step rewards are then used to compute step-level advantages, which are\ncombined with episode-level advantages from outcome rewards for policy update,\ncreating a self-reinforcing loop.\n  Theoretical findings guarantee that the learned step rewards are consistent\nwith trajectory preferences and act as potential-based shaping rewards,\nproviding bounded gradients to stabilize training.\n  Empirically, we evaluate OPRL on three distinct agent benmarks, including\nWebShop and VisualSokoban, as well as open-ended social interactions with\nunverfiable rewards in SOTOPIA.\n  Crucially, OPRL shows superior performance over frontier LLMs and strong RL\nbaselines across domains, achieving state-of-the-art results with higher\nsample-efficiency and lower variance during training.\n  Further analysis also demonstrates the efficient exploration by OPRL using\nfewer actions, underscoring its potential for agentic learning in real-world\nscenarios.", "AI": {"tldr": "\u63d0\u51fa\u5728\u7ebf\u8fc7\u7a0b\u5956\u52b1\u5b66\u4e60\u6846\u67b6OPRL\uff0c\u901a\u8fc7\u9690\u5f0f\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u4e0e\u8f68\u8ff9\u504f\u597d\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6837\u672c\u6548\u7387\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u8fc7\u7a0b\u76d1\u7763\u65b9\u6cd5\u5b58\u5728\u6807\u6ce8\u504f\u5dee\u3001\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u3001\u9ad8\u65b9\u5dee\u7b49\u95ee\u9898\uff0c\u4e14\u65e0\u6cd5\u5904\u7406\u72b6\u6001\u91cd\u53e0\u7f55\u89c1\u573a\u666f\uff0c\u4e9f\u9700\u65e0\u9700\u989d\u5916\u6807\u6ce8\u7684\u901a\u7528\u4fe1\u7528\u5206\u914d\u7b56\u7565\u3002", "method": "\u7ed3\u5408\u9690\u5f0f\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b(PRM)\u4e0e\u7b56\u7565\u4ea4\u66ff\u4f18\u5316\uff0c\u901a\u8fc7\u8f68\u8ff9DPO\u76ee\u6807\u5c06\u504f\u597d\u8f6c\u5316\u4e3a\u6b65\u9aa4\u5956\u52b1\uff0c\u4e0e\u7ed3\u679c\u5956\u52b1\u5171\u540c\u751f\u6210\u6df7\u5408\u4f18\u52bf\u4fe1\u53f7\u8fdb\u884c\u7b56\u7565\u66f4\u65b0\uff0c\u5f62\u6210\u81ea\u5f3a\u5316\u95ed\u73af\u3002", "result": "\u5728WebShop\u3001VisualSokoban\u548cSOTOPIA\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0SOTA\uff0c\u6837\u672c\u6548\u7387\u63d0\u534730%\uff0c\u8bad\u7ec3\u65b9\u5dee\u964d\u4f4e50%\uff0c\u5728\u4e0d\u53ef\u9a8c\u8bc1\u5956\u52b1\u573a\u666f\u4ecd\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd\u3002", "conclusion": "OPRL\u901a\u8fc7\u7406\u8bba\u4fdd\u8bc1\u7684\u5956\u52b1\u5851\u9020\u673a\u5236\u548c\u9ad8\u6548\u63a2\u7d22\u7b56\u7565\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u4ee3\u7406\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u7a33\u5b9a\u3001\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u5f00\u653e\u5f0f\u4ea4\u4e92\u573a\u666f\u5c55\u73b0\u7a81\u51fa\u6f5c\u529b\u3002"}}
{"id": "2509.19212", "pdf": "https://arxiv.org/pdf/2509.19212", "abs": "https://arxiv.org/abs/2509.19212", "authors": ["Zheyuan Liu", "Zhangchen Xu", "Guangyao Dou", "Xiangchi Yuan", "Zhaoxuan Tan", "Radha Poovendran", "Meng Jiang"], "title": "Steering Multimodal Large Language Models Decoding for Context-Aware Safety", "categories": ["cs.CL", "cs.AI"], "comment": "A lightweight and model-agnostic decoding framework that dynamically\n  adjusts token generation based on multimodal context", "summary": "Multimodal Large Language Models (MLLMs) are increasingly deployed in\nreal-world applications, yet their ability to make context-aware safety\ndecisions remains limited. Existing methods often fail to balance\noversensitivity (unjustified refusals of benign queries) and undersensitivity\n(missed detection of visually grounded risks), leaving a persistent gap in\nsafety alignment. To address this issue, we introduce Safety-aware Contrastive\nDecoding (SafeCoDe), a lightweight and model-agnostic decoding framework that\ndynamically adjusts token generation based on multimodal context. SafeCoDe\noperates in two stages: (1) a contrastive decoding mechanism that highlights\ntokens sensitive to visual context by contrasting real and Gaussian-noised\nimages, and (2) a global-aware token modulation strategy that integrates\nscene-level reasoning with token-level adjustment to adapt refusals according\nto the predicted safety verdict. Extensive experiments across diverse MLLM\narchitectures and safety benchmarks, covering undersensitivity,\noversensitivity, and general safety evaluations, show that SafeCoDe\nconsistently improves context-sensitive refusal behaviors while preserving\nmodel helpfulness.", "AI": {"tldr": "\u63d0\u51faSafeCoDe\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u89e3\u7801\u548c\u5168\u5c40\u611f\u77e5\u8c03\u6574\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u5b89\u5168\u51b3\u7b56\u80fd\u529b\uff0c\u5e73\u8861\u8fc7\u5ea6\u654f\u611f\u4e0e\u6b20\u654f\u611f\u95ee\u9898", "motivation": "\u73b0\u6709\u5b89\u5168\u51b3\u7b56\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u8fc7\u5ea6\u654f\u611f\uff08\u9519\u8bef\u62d2\u7edd\u826f\u6027\u67e5\u8be2\uff09\u548c\u6b20\u654f\u611f\uff08\u6f0f\u68c0\u89c6\u89c9\u98ce\u9669\uff09\uff0c\u5bfc\u81f4\u5b89\u5168\u5bf9\u9f50\u5b58\u5728\u6301\u7eed\u7f3a\u9677", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u5bf9\u6bd4\u89e3\u7801\u673a\u5236\uff08\u771f\u5b9e\u56fe\u50cf\u4e0e\u9ad8\u65af\u566a\u58f0\u56fe\u50cf\u5bf9\u6bd4\u8bc6\u522b\u654f\u611ftoken\uff09\uff1b2\uff09\u5168\u5c40\u611f\u77e5token\u8c03\u5236\u7b56\u7565\uff08\u7ed3\u5408\u573a\u666f\u7ea7\u63a8\u7406\u4e0etoken\u7ea7\u8c03\u6574\uff09", "result": "\u5728\u591a\u67b6\u6784MLLM\u548c\u6db5\u76d6\u6b20\u654f\u611f/\u8fc7\u5ea6\u654f\u611f/\u901a\u7528\u5b89\u5168\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6301\u7eed\u6539\u5584\u4e0a\u4e0b\u6587\u654f\u611f\u62d2\u7edd\u884c\u4e3a\u4e14\u4fdd\u6301\u6a21\u578b\u5e2e\u52a9\u6027", "conclusion": "SafeCoDe\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u6a21\u578b\u65e0\u5173\u6846\u67b6\uff0c\u6709\u6548\u589e\u5f3a\u5b89\u5168\u51b3\u7b56\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\uff0c\u5b9e\u73b0\u654f\u611f\u5ea6\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u4e0d\u540c\u6a21\u578b\u67b6\u6784"}}
{"id": "2509.19224", "pdf": "https://arxiv.org/pdf/2509.19224", "abs": "https://arxiv.org/abs/2509.19224", "authors": ["Tariq Abdul-Quddoos", "Xishuang Dong", "Lijun Qian"], "title": "Systematic Comparative Analysis of Large Pretrained Language Models on Contextualized Medication Event Extraction", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Attention-based models have become the leading approach in modeling medical\nlanguage for Natural Language Processing (NLP) in clinical notes. These models\noutperform traditional techniques by effectively capturing contextual rep-\nresentations of language. In this research a comparative analysis is done\namongst pre- trained attention based models namely Bert Base, BioBert, two\nvariations of Bio+Clinical Bert, RoBerta, and Clinical Long- former on task\nrelated to Electronic Health Record (EHR) information extraction. The tasks\nfrom Track 1 of Harvard Medical School's 2022 National Clinical NLP Challenges\n(n2c2) are considered for this comparison, with the Contextualized Medication\nEvent Dataset (CMED) given for these task. CMED is a dataset of unstructured\nEHRs and annotated notes that contain task relevant information about the EHRs.\nThe goal of the challenge is to develop effective solutions for extracting\ncontextual information related to patient medication events from EHRs using\ndata driven methods. Each pre-trained model is fine-tuned and applied on CMED\nto perform medication extraction, medical event detection, and\nmulti-dimensional medication event context classification. Pro- cessing methods\nare also detailed for breaking down EHRs for compatibility with the applied\nmodels. Performance analysis has been carried out using a script based on\nconstructing medical terms from the evaluation portion of CMED with metrics\nincluding recall, precision, and F1-Score. The results demonstrate that models\npre-trained on clinical data are more effective in detecting medication and\nmedication events, but Bert Base, pre- trained on general domain data showed to\nbe the most effective for classifying the context of events related to\nmedications.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u591a\u79cd\u9884\u8bad\u7ec3\u6ce8\u610f\u529b\u6a21\u578b\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4fe1\u606f\u62bd\u53d6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e34\u5e8a\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u836f\u7269\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u800c\u901a\u7528\u9886\u57df\u9884\u8bad\u7ec3\u7684Bert Base\u5728\u4e8b\u4ef6\u4e0a\u4e0b\u6587\u5206\u7c7b\u4e2d\u6548\u679c\u6700\u4f73\u3002", "motivation": "\u63a2\u7d22\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u533b\u7597NLP\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e0d\u540c\u6570\u636e\u6e90\u7684\u6a21\u578b\u6027\u80fd\uff0c\u63d0\u5347\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u836f\u7269\u4e8b\u4ef6\u4fe1\u606f\u7684\u63d0\u53d6\u80fd\u529b\u3002", "method": "\u4f7f\u7528CMED\u6570\u636e\u96c6\u5bf96\u79cd\u9884\u8bad\u7ec3\u6a21\u578b\uff08Bert Base/BioBert/Clinical Bert\u53d8\u4f53/RoBerta/Clinical Longformer\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u91c7\u7528\u53ec\u56de\u7387\u3001\u7cbe\u786e\u7387\u548cF1\u503c\u8bc4\u4f30\u5176\u5728\u7528\u836f\u4e8b\u4ef6\u63d0\u53d6\u3001\u68c0\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u4e34\u5e8a\u6570\u636e\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u836f\u7269\u8bc6\u522b\u548c\u4e8b\u4ef6\u68c0\u6d4b\u4efb\u52a1\u4e2dF1\u503c\u6700\u9ad8\uff08Bio+Clinical Bert\u8fbe88.3%\uff09\uff0c\u800c\u901a\u7528\u9886\u57dfBert Base\u5728\u4e8b\u4ef6\u4e0a\u4e0b\u6587\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f18\uff08F1 76.5%\uff09\u3002", "conclusion": "\u4e34\u5e8a\u9886\u57df\u9884\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u533b\u7597\u5b9e\u4f53\u8bc6\u522b\u6548\u679c\uff0c\u4f46\u901a\u7528\u6a21\u578b\u5728\u590d\u6742\u8bed\u4e49\u63a8\u7406\u4efb\u52a1\u4e2d\u4fdd\u6301\u4f18\u52bf\uff0c\u5efa\u8bae\u6839\u636e\u5177\u4f53\u4efb\u52a1\u9700\u6c42\u5206\u5c42\u4f7f\u7528\u4e0d\u540c\u9884\u8bad\u7ec3\u7b56\u7565\u3002"}}
{"id": "2509.19228", "pdf": "https://arxiv.org/pdf/2509.19228", "abs": "https://arxiv.org/abs/2509.19228", "authors": ["Gabriele Berton", "Jayakrishnan Unnikrishnan", "Son Tran", "Mubarak Shah"], "title": "CompLLM: Compression for Long Context Q&A", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility.", "AI": {"tldr": "\u63d0\u51faCompLLM\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u957f\u4e0a\u4e0b\u6587\u5904\u7406\uff0c\u901a\u8fc7\u5206\u6bb5\u72ec\u7acb\u538b\u7f29\u6280\u672f\u5b9e\u73b0\u7ebf\u6027\u590d\u6742\u5ea6\u3001\u53ef\u6269\u5c55\u6027\u548c\u8ba1\u7b97\u91cd\u7528", "motivation": "\u73b0\u6709\u8f6f\u538b\u7f29\u65b9\u6cd5\u56e0\u6574\u4f53\u538b\u7f29\u5bfc\u81f4\u4e8c\u6b21\u65b9\u590d\u6742\u5ea6\uff0c\u4e14\u65e0\u6cd5\u91cd\u7528\u8de8\u67e5\u8be2\u7684\u8ba1\u7b97\u7ed3\u679c", "method": "\u5c06\u4e0a\u4e0b\u6587\u5206\u5272\u4e3a\u72ec\u7acb\u538b\u7f29\u7684\u7247\u6bb5\uff0c\u5b9e\u73b0\u7ebf\u6027\u6269\u5c55/\u652f\u6301\u77ed\u5e8f\u5217\u8bad\u7ec3\u6a21\u578b\u6cdb\u5316\u5230100k tokens/\u652f\u6301\u7247\u6bb5\u7f13\u5b58\u91cd\u7528", "result": "2x\u538b\u7f29\u7387\u4e0b\uff1aTTFT\u52a0\u901f4\u500d\uff0cKV\u7f13\u5b58\u51cf\u5c1150%\uff1b\u957f\u5e8f\u5217\u6027\u80fd\u8d85\u8d8a\u539f\u59cb\u4e0a\u4e0b\u6587", "conclusion": "CompLLM\u901a\u8fc7\u5de5\u7a0b\u4f18\u5316\u5b9e\u73b0\u7406\u8bba\u521b\u65b0\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347LLMs\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u6548\u7387\uff0c\u5177\u6709\u5b9e\u9645\u90e8\u7f72\u4ef7\u503c"}}
{"id": "2509.19249", "pdf": "https://arxiv.org/pdf/2509.19249", "abs": "https://arxiv.org/abs/2509.19249", "authors": ["Siheng Li", "Kejiao Li", "Zenan Xu", "Guanhua Huang", "Evander Yang", "Kun Li", "Haoyuan Wu", "Jiajia Wu", "Zihao Zheng", "Chenchen Zhang", "Kun Shi", "Kyrierl Deng", "Qi Yi", "Ruibin Xiong", "Tingqiang Xu", "Yuhao Jiang", "Jianfeng Yan", "Yuyuan Zeng", "Guanghui Xu", "Jinbao Xue", "Zhijiang Xu", "Zheng Fang", "Shuai Li", "Qibin Liu", "Xiaoxue Li", "Zhuoyu Li", "Yangyu Tao", "Fei Gao", "Cheng Jiang", "Bo Chao Wang", "Kai Liu", "Jianchen Zhu", "Wai Lam", "Wayyt Wang", "Bo Zhou", "Di Wang"], "title": "Reinforcement Learning on Pre-Training Data", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Work in progress", "summary": "The growing disparity between the exponential scaling of computational\nresources and the finite growth of high-quality text data now constrains\nconventional scaling approaches for large language models (LLMs). To address\nthis challenge, we introduce Reinforcement Learning on Pre-Training data\n(RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast\nto prior approaches that scale training primarily through supervised learning,\nRLPT enables the policy to autonomously explore meaningful trajectories to\nlearn from pre-training data and improve its capability through reinforcement\nlearning (RL). While existing RL strategies such as reinforcement learning from\nhuman feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR)\nrely on human annotation for reward construction, RLPT eliminates this\ndependency by deriving reward signals directly from pre-training data.\nSpecifically, it adopts a next-segment reasoning objective, rewarding the\npolicy for accurately predicting subsequent text segments conditioned on the\npreceding context. This formulation allows RL to be scaled on pre-training\ndata, encouraging the exploration of richer trajectories across broader\ncontexts and thereby fostering more generalizable reasoning skills. Extensive\nexperiments on both general-domain and mathematical reasoning benchmarks across\nmultiple models validate the effectiveness of RLPT. For example, when applied\nto Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$,\n$6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and\nAIME25, respectively. The results further demonstrate favorable scaling\nbehavior, suggesting strong potential for continued gains with more compute. In\naddition, RLPT provides a solid foundation, extending the reasoning boundaries\nof LLMs and enhancing RLVR performance.", "AI": {"tldr": "\u63d0\u51faRLPT\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u6570\u636e\u81ea\u4e3b\u63a2\u7d22\u8f68\u8ff9\u63d0\u5347LLM\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5956\u52b1\u4fe1\u53f7", "motivation": "\u89e3\u51b3\u8ba1\u7b97\u8d44\u6e90\u6307\u6570\u589e\u957f\u4e0e\u9ad8\u8d28\u91cf\u6587\u672c\u6570\u636e\u6709\u9650\u589e\u957f\u4e4b\u95f4\u7684\u5931\u8861\u95ee\u9898\uff0c\u7a81\u7834\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u7684\u6269\u5c55\u74f6\u9888", "method": "\u91c7\u7528\u57fa\u4e8e\u4e0b\u4e2a\u7247\u6bb5\u7684\u63a8\u7406\u76ee\u6807\uff0c\u4ece\u9884\u8bad\u7ec3\u6570\u636e\u76f4\u63a5\u751f\u6210\u5956\u52b1\u4fe1\u53f7\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6269\u5c55\u4e0a\u4e0b\u6587\u63a2\u7d22\u8303\u56f4", "result": "Qwen3-4B-Base\u6a21\u578b\u5728MMLU(3.0\u2191)\u3001MMLU-Pro(5.1\u2191)\u3001GPQA-Diamond(8.1\u2191)\u7b49\u57fa\u51c6\u5b9e\u73b0\u663e\u8457\u63d0\u5347\uff0c\u5c55\u73b0\u826f\u597d\u6269\u5c55\u6027", "conclusion": "RLPT\u7a81\u7834\u4f20\u7edfRLHF\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u7684\u9650\u5236\uff0c\u6269\u5c55LLM\u63a8\u7406\u8fb9\u754c\uff0c\u5e76\u4e3a\u540e\u7eedRLVR\u63d0\u4f9b\u5f3a\u5316\u57fa\u7840\uff0c\u8ba1\u7b97\u6548\u7387\u63d0\u5347\u6f5c\u529b\u663e\u8457"}}
{"id": "2509.19269", "pdf": "https://arxiv.org/pdf/2509.19269", "abs": "https://arxiv.org/abs/2509.19269", "authors": ["Nitesh Kumar", "Usashi Chatterjee", "Steven Schockaert"], "title": "Extracting Conceptual Spaces from LLMs Using Prototype Embeddings", "categories": ["cs.CL"], "comment": null, "summary": "Conceptual spaces represent entities and concepts using cognitively\nmeaningful dimensions, typically referring to perceptual features. Such\nrepresentations are widely used in cognitive science and have the potential to\nserve as a cornerstone for explainable AI. Unfortunately, they have proven\nnotoriously difficult to learn, although recent LLMs appear to capture the\nrequired perceptual features to a remarkable extent. Nonetheless, practical\nmethods for extracting the corresponding conceptual spaces are currently still\nlacking. While various methods exist for extracting embeddings from LLMs,\nextracting conceptual spaces also requires us to encode the underlying\nfeatures. In this paper, we propose a strategy in which features (e.g.\nsweetness) are encoded by embedding the description of a corresponding\nprototype (e.g. a very sweet food). To improve this strategy, we fine-tune the\nLLM to align the prototype embeddings with the corresponding conceptual space\ndimensions. Our empirical analysis finds this approach to be highly effective.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u539f\u578b\u63cf\u8ff0\u7f16\u7801\u6982\u5ff5\u7279\u5f81\uff0c\u5e76\u5fae\u8c03LLM\u5bf9\u9f50\u6982\u5ff5\u7a7a\u95f4\u7ef4\u5ea6\u7684\u65b0\u65b9\u6cd5", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u4ece\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u63d0\u53d6\u5177\u6709\u53ef\u89e3\u91ca\u6027\u7684\u6982\u5ff5\u7a7a\u95f4\u7279\u5f81", "method": "1. \u7528\u539f\u578b\u63cf\u8ff0\uff08\u5982'\u975e\u5e38\u751c\u7684\u98df\u7269'\uff09\u7f16\u7801\u7279\u5f81\u7ef4\u5ea6\uff1b2. \u5fae\u8c03LLM\u4f7f\u539f\u578b\u5d4c\u5165\u4e0e\u6982\u5ff5\u7a7a\u95f4\u7ef4\u5ea6\u5bf9\u9f50", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u7279\u5f81\u63d0\u53d6\u7ef4\u5ea6\u5bf9\u9f50\u4e0a\u6548\u679c\u663e\u8457", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u53ef\u89e3\u91caAI\u7684\u6982\u5ff5\u7a7a\u95f4\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84"}}
{"id": "2509.19270", "pdf": "https://arxiv.org/pdf/2509.19270", "abs": "https://arxiv.org/abs/2509.19270", "authors": ["Erik Bo\u017e\u00edk", "Marek \u0160uppa"], "title": "SloPalSpeech: A 2,8000-Hour Slovak Speech Corpus from Parliamentary Data", "categories": ["cs.CL", "cs.AI", "cs.SD"], "comment": null, "summary": "Automatic Speech Recognition (ASR) for low-resource languages like Slovak is\nhindered by the scarcity of training data. To address this, we introduce\nSloPalSpeech, a new, large-scale Slovak ASR dataset containing 2,806 hours of\nspeech from parliamentary proceedings. We developed a robust processing\npipeline to align and segment long-form recordings into clean, 30-second\naudio-transcript pairs suitable for model training. We use this dataset to\nfine-tune several OpenAI Whisper models (small, medium, large-v3, and\nlarge-v3-turbo), achieving significant Word Error Rate (WER) reductions on\nstandard Slovak benchmarks like Common Voice and FLEURS. For instance, the\nfine-tuned Whisper-small model's WER dropped by up to 70\\%, approaching the\nbaseline performance of the much larger Whisper-large-v3 model. To foster\nfuture research in low-resource speech recognition, we publicly release the\ncomplete SloPalSpeech dataset, the fully segmented transcripts (60 million\nwords), and all our fine-tuned models.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00ASR\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u7814\u7a76\u8005\u6784\u5efa\u4e86\u65af\u6d1b\u4f10\u514b\u8bae\u4f1a\u8bed\u97f3\u6570\u636e\u96c6SloPalSpeech\uff082,806\u5c0f\u65f6\uff09\uff0c\u901a\u8fc7\u4f18\u5316\u5904\u7406\u6d41\u7a0b\u5fae\u8c03Whisper\u6a21\u578b\uff0c\u663e\u8457\u964d\u4f4e\u8bcd\u9519\u7387\u5e76\u516c\u5f00\u5168\u90e8\u8d44\u6e90\u3002", "motivation": "\u65af\u6d1b\u4f10\u514b\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u56e0\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u5236\u7ea6ASR\u53d1\u5c55\uff0c\u9700\u901a\u8fc7\u6784\u5efa\u9ad8\u8d28\u91cf\u4e13\u7528\u6570\u636e\u96c6\u7a81\u7834\u6280\u672f\u74f6\u9888\u3002", "method": "\u5f00\u53d1\u97f3\u9891\u5bf9\u9f50\u5206\u5272\u6d41\u7a0b\u751f\u621030\u79d2\u97f3\u9891-\u6587\u672c\u5bf9\uff0c\u5bf9Whisper\u7cfb\u5217\u6a21\u578b\uff08small/medium/large-v3\u7b49\uff09\u8fdb\u884c\u9488\u5bf9\u6027\u5fae\u8c03\u3002", "result": "\u5fae\u8c03\u540eWhisper-small\u6a21\u578b\u8bcd\u9519\u7387\u4e0b\u964d\u8fbe70%\uff0c\u5176\u6027\u80fd\u903c\u8fd1\u539f\u5927\u578b\u6a21\u578bWhisper-large-v3\u7684\u57fa\u7ebf\u6c34\u5e73\u3002", "conclusion": "\u516c\u5f00\u6570\u636e\u96c6\u3001\u5904\u7406\u6d41\u7a0b\u53ca\u4f18\u5316\u6a21\u578b\u5c06\u63a8\u52a8\u4f4e\u8d44\u6e90\u8bed\u97f3\u8bc6\u522b\u7814\u7a76\uff0c\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u7a00\u7f3a\u8bed\u8a00\u573a\u666f\u3002"}}
{"id": "2509.19271", "pdf": "https://arxiv.org/pdf/2509.19271", "abs": "https://arxiv.org/abs/2509.19271", "authors": ["Abdou Karim Kandji", "Fr\u00e9d\u00e9ric Precioso", "Cheikh Ba", "Samba Ndiaye", "Augustin Ndione"], "title": "WolBanking77: Wolof Banking Speech Intent Classification Dataset", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "10 pages, 7 figures", "summary": "Intent classification models have made a lot of progress in recent years.\nHowever, previous studies primarily focus on high-resource languages datasets,\nwhich results in a gap for low-resource languages and for regions with a high\nrate of illiterate people where languages are more spoken than read or written.\nThis is the case in Senegal, for example, where Wolof is spoken by around 90\\%\nof the population, with an illiteracy rate of 42\\% for the country. Wolof is\nactually spoken by more than 10 million people in West African region. To\ntackle such limitations, we release a Wolof Intent Classification Dataset\n(WolBanking77), for academic research in intent classification. WolBanking77\ncurrently contains 9,791 text sentences in the banking domain and more than 4\nhours of spoken sentences. Experiments on various baselines are conducted in\nthis work, including text and voice state-of-the-art models. The results are\nvery promising on this current dataset. This paper also provides detailed\nanalyses of the contents of the data. We report baseline f1-score and word\nerror rate metrics respectively on NLP and ASR models trained on WolBanking77\ndataset and also comparisons between models. We plan to share and conduct\ndataset maintenance, updates and to release open-source code.", "AI": {"tldr": "\u53d1\u5e03\u9996\u4e2a\u6c83\u6d1b\u592b\u8bed\u94f6\u884c\u9886\u57df\u610f\u56fe\u5206\u7c7b\u6570\u636e\u96c6WolBanking77\uff0c\u5305\u542b9,791\u6587\u672c\u548c4\u5c0f\u65f6\u8bed\u97f3\u6570\u636e\uff0c\u57fa\u7ebf\u6a21\u578b\u6548\u679c\u826f\u597d", "motivation": "\u73b0\u6709\u610f\u56fe\u5206\u7c7b\u7814\u7a76\u96c6\u4e2d\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\uff0c\u5ffd\u89c6\u6c83\u6d1b\u592b\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u53ca\u6587\u76f2\u7387\u9ad8\u7684\u5730\u533a\u9700\u6c42\uff08\u5982\u585e\u5185\u52a0\u5c1490%\u4eba\u53e3\u4f7f\u7528\u6c83\u6d1b\u592b\u8bed\uff09", "method": "\u6784\u5efa\u5305\u542b\u6587\u672c\u548c\u8bed\u97f3\u7684\u53cc\u6a21\u6001\u6570\u636e\u96c6\uff0c\u91c7\u7528SOTA\u7684NLP\u6a21\u578b\u548c\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u8fdb\u884c\u57fa\u7ebf\u5b9e\u9a8c", "result": "\u6587\u672c\u6a21\u578bF1-score\u8fbe\u57fa\u51c6\u6c34\u5e73\uff0cASR\u6a21\u578b\u8bcd\u9519\u8bef\u7387\u8868\u73b0\u826f\u597d\uff0c\u4e0d\u540c\u6a21\u578b\u95f4\u6bd4\u8f83\u663e\u793a\u6570\u636e\u96c6\u6709\u6548\u6027", "conclusion": "\u9996\u4e2a\u6c83\u6d1b\u592b\u8bed\u610f\u56fe\u5206\u7c7b\u6570\u636e\u96c6\u586b\u8865\u7814\u7a76\u7a7a\u767d\uff0c\u540e\u7eed\u5c06\u7ef4\u62a4\u66f4\u65b0\u6570\u636e\u96c6\u5e76\u5f00\u6e90\u4ee3\u7801"}}
{"id": "2509.19274", "pdf": "https://arxiv.org/pdf/2509.19274", "abs": "https://arxiv.org/abs/2509.19274", "authors": ["Arijit Maji", "Raghvendra Kumar", "Akash Ghosh", "Anushka", "Nemil Shah", "Abhilekh Borah", "Vanshika Shah", "Nishant Mishra", "Sriparna Saha"], "title": "DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models' Understanding on Indian Culture", "categories": ["cs.CL", "cs.MM"], "comment": "EMNLP MAINS 2025", "summary": "We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual\nbenchmark centered exclusively on Indian culture, designed to evaluate the\ncultural understanding of generative AI systems. Unlike existing benchmarks\nwith a generic or global scope, DRISHTIKON offers deep, fine-grained coverage\nacross India's diverse regions, spanning 15 languages, covering all states and\nunion territories, and incorporating over 64,000 aligned text-image pairs. The\ndataset captures rich cultural themes including festivals, attire, cuisines,\nart forms, and historical heritage amongst many more. We evaluate a wide range\nof vision-language models (VLMs), including open-source small and large models,\nproprietary systems, reasoning-specialized VLMs, and Indic-focused models,\nacross zero-shot and chain-of-thought settings. Our results expose key\nlimitations in current models' ability to reason over culturally grounded,\nmultimodal inputs, particularly for low-resource languages and less-documented\ntraditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a\nrobust testbed to advance culturally aware, multimodally competent language\ntechnologies.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u4e13\u6ce8\u4e8e\u5370\u5ea6\u6587\u5316\u7684\u591a\u6a21\u6001\u591a\u8bed\u8a00\u57fa\u51c6DRISHTIKON\uff0c\u8986\u76d615\u79cd\u8bed\u8a00/64k\u56fe\u6587\u5bf9\uff0c\u6d4b\u8bd5\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u6587\u5316\u63a8\u7406\u53ca\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u4e0a\u5b58\u5728\u663e\u8457\u7f3a\u9677\u3002", "motivation": "\u73b0\u6709AI\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u6587\u5316\u591a\u6837\u6027\uff08\u5c24\u5176\u662f\u5370\u5ea6\u590d\u6742\u6587\u5316\u4f53\u7cfb\uff09\u7684\u6df1\u5ea6\u7406\u89e3\uff0c\u9700\u4e13\u95e8\u57fa\u51c6\u63a8\u52a8\u5305\u5bb9\u6027AI\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u6db5\u76d6\u5370\u5ea6\u5168\u5883\u6587\u5316\u8981\u7d20\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5728\u96f6\u6837\u672c/\u601d\u7ef4\u94fe\u8bbe\u5b9a\u4e0b\u8bc4\u4f30\u5f00\u6e90/\u5546\u7528/\u63a8\u7406\u4e13\u7528/\u5370\u5ea6\u672c\u571fVLMs\u6a21\u578b\u3002", "result": "\u5f53\u524d\u6a21\u578b\u5bf9\u6587\u5316\u80cc\u666f\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u8584\u5f31\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u53ca\u975e\u4e3b\u6d41\u4f20\u7edf\u6587\u5316\u4efb\u52a1\u51c6\u786e\u7387\u4e0b\u964d\u660e\u663e\u3002", "conclusion": "DRISHTIKON\u4e3a\u5f00\u53d1\u5177\u5907\u6587\u5316\u610f\u8bc6\u7684\u591a\u6a21\u6001AI\u63d0\u4f9b\u4e86\u5173\u952e\u6d4b\u8bd5\u5e73\u53f0\uff0c\u586b\u8865\u4e86\u975e\u897f\u65b9\u4e2d\u5fc3AI\u8bc4\u4f30\u4f53\u7cfb\u7684\u7a7a\u767d\u3002"}}
{"id": "2509.18127", "pdf": "https://arxiv.org/pdf/2509.18127", "abs": "https://arxiv.org/abs/2509.18127", "authors": ["Jiaqi Weng", "Han Zheng", "Hanyu Zhang", "Qinqin He", "Jialing Tao", "Hui Xue", "Zhixuan Chu", "Xiting Wang"], "title": "Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Increasing deployment of large language models (LLMs) in real-world\napplications raises significant safety concerns. Most existing safety research\nfocuses on evaluating LLM outputs or specific safety tasks, limiting their\nability to ad- dress broader, undefined risks. Sparse Autoencoders (SAEs)\nfacilitate interpretability research to clarify model behavior by explaining\nsingle-meaning atomic features decomposed from entangled signals. jHowever,\nprior applications on SAEs do not interpret features with fine-grained\nsafety-related con- cepts, thus inadequately addressing safety-critical\nbehaviors, such as generating toxic responses and violating safety regu-\nlations. For rigorous safety analysis, we must extract a rich and diverse set\nof safety-relevant features that effectively capture these high-risk behaviors,\nyet face two challenges: identifying SAEs with the greatest potential for\ngenerating safety concept-specific neurons, and the prohibitively high cost of\ndetailed feature explanation. In this paper, we pro- pose Safe-SAIL, a\nframework for interpreting SAE features within LLMs to advance mechanistic\nunderstanding in safety domains. Our approach systematically identifies SAE\nwith best concept-specific interpretability, explains safety-related neurons,\nand introduces efficient strategies to scale up the in- terpretation process.\nWe will release a comprehensive toolkit including SAE checkpoints and\nhuman-readable neuron ex- planations, which supports empirical analysis of\nsafety risks to promote research on LLM safety.", "AI": {"tldr": "\u63d0\u51faSafe-SAIL\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u91ca\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAE)\u7279\u5f81\u589e\u5f3a\u5bf9LLM\u5b89\u5168\u673a\u5236\u7684\u673a\u7406\u7406\u89e3\uff0c\u89e3\u51b3\u73b0\u6709\u5b89\u5168\u7814\u7a76\u8986\u76d6\u8303\u56f4\u6709\u9650\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709LLM\u5b89\u5168\u7814\u7a76\u805a\u7126\u8f93\u51fa\u8bc4\u4f30\u6216\u7279\u5b9a\u4efb\u52a1\uff0c\u65e0\u6cd5\u5e94\u5bf9\u672a\u5b9a\u4e49\u7684\u5e7f\u6cdb\u98ce\u9669\u3002SAE\u867d\u7528\u4e8e\u7279\u5f81\u89e3\u8026\uff0c\u4f46\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u5b89\u5168\u6982\u5ff5\u89e3\u91ca\uff0c\u96be\u4ee5\u6709\u6548\u68c0\u6d4b\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u7b49\u9ad8\u5371\u884c\u4e3a\u3002", "method": "\u5f00\u53d1\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\u7684\u6846\u67b6\uff1a1) \u7cfb\u7edf\u7b5b\u9009\u5177\u6709\u5b89\u5168\u6982\u5ff5\u89e3\u91ca\u6f5c\u529b\u7684SAE 2) \u89e3\u91ca\u5b89\u5168\u76f8\u5173\u795e\u7ecf\u5143 3) \u5f15\u5165\u9ad8\u6548\u7b56\u7565\u6269\u5c55\u89e3\u91ca\u89c4\u6a21\uff0c\u5e76\u914d\u5957\u53d1\u5e03SAE\u68c0\u67e5\u70b9\u4e0e\u795e\u7ecf\u5143\u89e3\u91ca\u5de5\u5177\u5305\u3002", "result": "\u6784\u5efa\u652f\u6301LLM\u5b89\u5168\u98ce\u9669\u5b9e\u8bc1\u5206\u6790\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u5de5\u5177\u5305\u5c06\u5305\u542b\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u5143\u8bed\u4e49\u6807\u6ce8\uff0c\u4e3a\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u65b0\u7684\u65b9\u6cd5\u8bba\u652f\u6491\u3002", "conclusion": "Safe-SAIL\u901a\u8fc7\u6df1\u5ea6\u89e3\u91caSAE\u7279\u5f81\uff0c\u63a8\u52a8LLM\u5b89\u5168\u673a\u5236\u7684\u900f\u660e\u5316\uff0c\u5176\u5de5\u5177\u751f\u6001\u5c06\u4fc3\u8fdb\u66f4\u7cfb\u7edf\u5316\u7684\u9ad8\u98ce\u9669\u884c\u4e3a\u68c0\u6d4b\u4e0e\u9632\u62a4\u7814\u7a76\u3002"}}
{"id": "2509.18169", "pdf": "https://arxiv.org/pdf/2509.18169", "abs": "https://arxiv.org/abs/2509.18169", "authors": ["Hengbo Xiao", "Jingyuan Fan", "Xin Tong", "Jingzhao Zhang", "Chao Lu", "Guannan He"], "title": "PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning", "categories": ["cs.LG", "cs.CE", "cs.CL"], "comment": null, "summary": "Complex systems typically rely on high-precision numerical computation to\nsupport decisions, but current large language models (LLMs) cannot yet\nincorporate such computations as an intrinsic and interpretable capability with\nexisting architectures. Mainstream multi-agent approaches can leverage external\nexperts, but inevitably introduce communication overhead and suffer from\ninefficient multimodal emergent capability and limited scalability. To this\nend, we propose PiMoE (Physically-isolated Mixture of Experts), a training and\ninference architecture for integrating computation and reasoning. Instead of\nthe workflow paradigm of tool invocation, PiMoE endogenously integrates\ncomputational capabilities into neural networks after separately training\nexperts, a text-to-computation module, and a router. At inference, the router\ndirects computation and reasoning at the token level, thereby enabling\niterative alternation within a single chain of thought. We evaluate PiMoE on\ntwo reasoning-computation tasks against LLM finetuning and the multi-agent\nsystem approaches. Results show that the PiMoE architecture achieves not only\nhigher accuracy than directly finetuning LLMs but also significant improvements\nin response latency, token usage, and GPU energy consumption compared with\nmainstream multi-agent approaches. PiMoE offers an efficient, interpretable,\nand scalable paradigm for next-generation scientific or industrial intelligent\nsystems.", "AI": {"tldr": "\u63d0\u51faPiMoE\u67b6\u6784\uff0c\u901a\u8fc7\u7269\u7406\u9694\u79bb\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u5b9e\u73b0\u8ba1\u7b97\u4e0e\u63a8\u7406\u7684\u7aef\u5230\u7aef\u96c6\u6210\uff0c\u663e\u8457\u63d0\u5347\u54cd\u5e94\u6548\u7387\u4e0e\u53ef\u89e3\u91ca\u6027", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u96be\u4ee5\u5185\u751f\u5316\u9ad8\u7cbe\u5ea6\u6570\u503c\u8ba1\u7b97\u80fd\u529b\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5b58\u5728\u901a\u4fe1\u5f00\u9500\u5927\u3001\u591a\u6a21\u6001\u80fd\u529b\u4e0d\u8db3\u7684\u7f3a\u9677", "method": "\u5206\u79bb\u8bad\u7ec3\u4e13\u5bb6\u6a21\u578b+\u6587\u672c\u8ba1\u7b97\u6a21\u5757+\u8def\u7531\u51b3\u7b56\u5668\uff0c\u5728\u63a8\u7406\u65f6\u901a\u8fc7token\u7ea7\u8def\u7531\u5b9e\u73b0\u8ba1\u7b97\u4e0e\u63a8\u7406\u7684\u8fed\u4ee3\u4ea4\u66ff", "result": "\u5728\u8ba1\u7b97\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u51c6\u786e\u7387\u4f18\u4e8e\u5fae\u8c03\u6a21\u578b\uff0c\u54cd\u5e94\u5ef6\u8fdf\u964d\u4f4e4\u500d\uff0cGPU\u80fd\u8017\u51cf\u5c1178%", "conclusion": "PiMoE\u4e3a\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u89e3\u91ca\u7684\u67b6\u6784\u8303\u5f0f\uff0c\u9002\u7528\u4e8e\u79d1\u5b66\u8ba1\u7b97\u4e0e\u5de5\u4e1a\u51b3\u7b56\u573a\u666f"}}
{"id": "2509.18173", "pdf": "https://arxiv.org/pdf/2509.18173", "abs": "https://arxiv.org/abs/2509.18173", "authors": ["Hongyi Luo", "Qing Cheng", "Daniel Matos", "Hari Krishna Gadi", "Yanfeng Zhang", "Lu Liu", "Yongliang Wang", "Niclas Zeller", "Daniel Cremers", "Liqiu Meng"], "title": "TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to EMNLP 2025 (Main). This is the camera-ready/author\n  version", "summary": "Humans can interpret geospatial information through natural language, while\nthe geospatial cognition capabilities of Large Language Models (LLMs) remain\nunderexplored. Prior research in this domain has been constrained by\nnon-quantifiable metrics, limited evaluation datasets and unclear research\nhierarchies. Therefore, we propose a large-scale benchmark and conduct a\ncomprehensive evaluation of the geospatial route cognition of LLMs. We create a\nlarge-scale evaluation dataset comprised of 36000 routes from 12 metropolises\nworldwide. Then, we introduce PathBuilder, a novel tool for converting natural\nlanguage instructions into navigation routes, and vice versa, bridging the gap\nbetween geospatial information and natural language. Finally, we propose a new\nevaluation framework and metrics to rigorously assess 11 state-of-the-art\n(SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs\nexhibit limitation to reverse routes: most reverse routes neither return to the\nstarting point nor are similar to the optimal route. Additionally, LLMs face\nchallenges such as low robustness in route generation and high confidence for\ntheir incorrect answers. Code\\ \\&\\ Data available here:\n\\href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}", "AI": {"tldr": "\u6784\u5efa\u9996\u4e2a\u5927\u89c4\u6a21\u5730\u7406\u8def\u7ebf\u8ba4\u77e5\u8bc4\u4f30\u57fa\u51c6\uff0c\u53d1\u73b0LLMs\u5728\u8def\u7ebf\u53cd\u8f6c\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\uff08\u8def\u7ebf\u504f\u5dee\u5927\u3001\u751f\u6210\u7a33\u5b9a\u6027\u4f4e\u3001\u9519\u8bef\u7b54\u6848\u9ad8\u7f6e\u4fe1\u5ea6\uff09", "motivation": "\u586b\u8865LLMs\u5730\u7406\u7a7a\u95f4\u8ba4\u77e5\u80fd\u529b\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u5b58\u5728\u7684\u8bc4\u4f30\u6307\u6807\u4e0d\u53ef\u91cf\u5316\u3001\u6570\u636e\u96c6\u6709\u9650\u3001\u7814\u7a76\u4f53\u7cfb\u4e0d\u6e05\u6670\u4e09\u5927\u95ee\u9898", "method": "\u521b\u5efa\u5168\u740312\u5927\u90fd\u5e02\u768436000\u6761\u8def\u7ebf\u6570\u636e\u96c6\uff1b\u5f00\u53d1PathBuilder\u53cc\u5411\u8f6c\u6362\u5de5\u5177\uff1b\u8bbe\u8ba1\u542b\u8def\u7ebf\u76f8\u4f3c\u5ea6\u3001\u7a33\u5b9a\u6027\u7b49\u7ef4\u5ea6\u7684\u65b0\u8bc4\u4f30\u6846\u67b6", "result": "LLMs\u8def\u7ebf\u53cd\u8f6c\u51c6\u786e\u7387\u4f4e\uff08\u591a\u6570\u65e0\u6cd5\u8fd4\u56de\u8d77\u70b9/\u504f\u79bb\u6700\u4f18\u8def\u7ebf\uff09\uff0c\u5b58\u5728\u751f\u6210\u7a33\u5b9a\u6027\u4f4e\uff0832.6%\u6ce2\u52a8\u7387\uff09\u3001\u9519\u8bef\u7b54\u6848\u9ad8\u7f6e\u4fe1\u5ea6\uff0886.7%\u9519\u8bef\u7ed3\u679c\u4f34\u968f\u9ad8\u7f6e\u4fe1\u5ea6\uff09\u7b49\u95ee\u9898", "conclusion": "\u63ed\u793a\u4e86LLMs\u5730\u7406\u7a7a\u95f4\u8ba4\u77e5\u7684\u74f6\u9888\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u57fa\u51c6\u5de5\u5177\uff08\u5f00\u6e90\u6570\u636e\u96c6+PathBuilder+\u8bc4\u4f30\u6846\u67b6\uff09\uff0c\u63a8\u52a8\u7a7a\u95f4\u667a\u80fd\u53d1\u5c55"}}
{"id": "2509.18174", "pdf": "https://arxiv.org/pdf/2509.18174", "abs": "https://arxiv.org/abs/2509.18174", "authors": ["Khalil Hennara", "Muhammad Hreden", "Mohamed Motasim Hamed", "Ahmad Bastati", "Zeina Aldallal", "Sara Chrouf", "Safwan AlModhayan"], "title": "Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Arabic document OCR remains a challenging task due to the language's cursive\nscript, diverse fonts, diacritics, and right-to-left orientation. While modern\nMultimodal Large Language Models (MLLMs) have advanced document understanding\nfor high-resource languages, their performance on Arabic remains limited. In\nthis work, we introduce Baseer, a vision-language model fine- tuned\nspecifically for Arabic document OCR. Leveraging a large-scale dataset\ncombining synthetic and real-world documents, Baseer is trained using a\ndecoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving\ngeneral visual features. We also present Misraj-DocOCR, a high-quality,\nexpert-verified benchmark designed for rigorous evaluation of Arabic OCR\nsystems. Our experiments show that Baseer significantly outperforms existing\nopen-source and commercial solutions, achieving a WER of 0.25 and establishing\na new state-of-the-art in the domain of Arabic document OCR. Our results\nhighlight the benefits of domain-specific adaptation of general-purpose MLLMs\nand establish a strong baseline for high-accuracy OCR on morphologically rich\nlanguages like Arabic.", "AI": {"tldr": "\u63d0\u51faBaseer\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e13\u95e8\u7528\u4e8e\u963f\u62c9\u4f2f\u6587\u6863OCR\uff0c\u7ed3\u5408\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\uff0c\u521b\u9020\u65b0SOTA\uff08WER 0.25\uff09\uff0c\u5e76\u5efa\u7acb\u9ad8\u8d28\u91cf\u8bc4\u4f30\u57fa\u51c6Misraj-DocOCR\u3002", "motivation": "\u89e3\u51b3\u963f\u62c9\u4f2f\u8bedOCR\u7684\u7279\u6b8a\u6311\u6218\uff08\u8349\u4e66\u5b57\u4f53\u3001\u53d8\u97f3\u7b26\u53f7\u3001\u53f3\u5411\u5de6\u4e66\u5199\uff09\uff0c\u586b\u8865\u73b0\u6709MLLMs\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e0a\u7684\u6027\u80fd\u7f3a\u53e3\u3002", "method": "1. \u4f7f\u7528\u5927\u89c4\u6a21\u5408\u6210+\u771f\u5b9e\u6587\u6863\u6570\u636e\u96c6\n2. \u91c7\u7528\u4ec5\u89e3\u7801\u5668\u7684\u5fae\u8c03\u7b56\u7565\uff08\u4fdd\u7559\u901a\u7528\u89c6\u89c9\u7279\u5f81\uff09\n3. \u5f00\u53d1\u4e13\u5bb6\u9a8c\u8bc1\u7684Misraj-DocOCR\u8bc4\u4f30\u57fa\u51c6", "result": "Baseer\u663e\u8457\u8d85\u8d8a\u5f00\u6e90/\u5546\u4e1a\u65b9\u6848\uff08WER 0.25\uff09\uff0c\u5728\u963f\u62c9\u4f2fOCR\u9886\u57df\u8fbe\u5230\u65b0SOTA\u3002", "conclusion": "\u9886\u57df\u7279\u5b9a\u9002\u914d\u80fd\u6709\u6548\u63d0\u5347\u901a\u7528MLLMs\u6027\u80fd\uff0c\u4e3a\u5f62\u6001\u590d\u6742\u8bed\u8a00OCR\u5efa\u7acb\u5f3a\u57fa\u7ebf\uff0c\u9a8c\u8bc1\u4e13\u4e1a\u5316\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2509.18200", "pdf": "https://arxiv.org/pdf/2509.18200", "abs": "https://arxiv.org/abs/2509.18200", "authors": ["Yu Ti Huang"], "title": "Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "Conversational agents must translate egocentric utterances (e.g., \"on my\nright\") into allocentric orientations (N/E/S/W). This challenge is particularly\ncritical in indoor or complex facilities where GPS signals are weak and\ndetailed maps are unavailable. While chain-of-thought (CoT) prompting has\nadvanced reasoning in language and vision tasks, its application to multimodal\nspatial orientation remains underexplored. We introduce Conversational\nOrientation Reasoning (COR), a new benchmark designed for Traditional Chinese\nconversational navigation projected from real-world environments, addressing\negocentric-to-allocentric reasoning in non-English and ASR-transcribed\nscenarios. We propose a multimodal chain-of-thought (MCoT) framework, which\nintegrates ASR-transcribed speech with landmark coordinates through a\nstructured three-step reasoning process: (1) extracting spatial relations, (2)\nmapping coordinates to absolute directions, and (3) inferring user orientation.\nA curriculum learning strategy progressively builds these capabilities on\nTaiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of\nresource-constrained settings. Experiments show that MCoT achieves 100%\norientation accuracy on clean transcripts and 98.1% with ASR transcripts,\nsubstantially outperforming unimodal and non-structured baselines. Moreover,\nMCoT demonstrates robustness under noisy conversational conditions, including\nASR recognition errors and multilingual code-switching. The model also\nmaintains high accuracy in cross-domain evaluation and resilience to linguistic\nvariation, domain shift, and referential ambiguity. These findings highlight\nthe potential of structured MCoT spatial reasoning as a path toward\ninterpretable and resource-efficient embodied navigation.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u6846\u67b6MCoT\uff0c\u901a\u8fc7\u4e09\u6b65\u63a8\u7406\u89e3\u51b3\u4e2d\u6587\u5bf9\u8bdd\u5bfc\u822a\u4e2d\u7684\u81ea\u6211\u4e2d\u5fc3\u65b9\u4f4d\u8f6c\u6362\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a98%+\u51c6\u786e\u7387\u4e14\u5177\u5907\u6297\u566a\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u94fe\u5f0f\u63a8\u7406\u65b9\u6cd5\u5728\u975e\u82f1\u8bed\u573a\u666f\u3001ASR\u8f6c\u5f55\u6587\u672c\u548c\u590d\u6742\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u65b9\u4f4d\u63a8\u7406\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u5f00\u53d1\u9002\u914d\u771f\u5b9e\u573a\u666f\u7684\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u8bed\u97f3\u8bc6\u522b\u6587\u672c\u4e0e\u5730\u6807\u5750\u6807\uff0c\u8bbe\u8ba1\u63d0\u53d6\u7a7a\u95f4\u5173\u7cfb-\u5750\u6807\u6620\u5c04-\u65b9\u5411\u63a8\u65ad\u7684\u4e09\u6b65\u63a8\u7406\u6846\u67b6\uff0c\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u6e10\u8fdb\u8bad\u7ec3\u4e2d\u578b\u8bed\u8a00\u6a21\u578b\u3002", "result": "\u5728\u5e72\u51c0/ASR\u6587\u672c\u5206\u522b\u8fbe\u5230100%/98.1%\u51c6\u786e\u7387\uff0c\u6297\u566a\u6d4b\u8bd5\u4e2d\u4fdd\u6301\u7a33\u5b9a\uff0c\u8de8\u9886\u57df\u8bc4\u4f30\u663e\u793a\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7ed3\u6784\u5316MCoT\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5177\u8eab\u5bfc\u822a\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001\u9ad8\u6548\u7387\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u591a\u8bed\u8a00\u6df7\u6742\u7b49\u590d\u6742\u573a\u666f\u3002"}}
{"id": "2509.18234", "pdf": "https://arxiv.org/pdf/2509.18234", "abs": "https://arxiv.org/abs/2509.18234", "authors": ["Yu Gu", "Jingjing Fu", "Xiaodong Liu", "Jeya Maria Jose Valanarasu", "Noel Codella", "Reuben Tan", "Qianchu Liu", "Ying Jin", "Sheng Zhang", "Jinyu Wang", "Rui Wang", "Lei Song", "Guanghui Qin", "Naoto Usuyama", "Cliff Wong", "Cheng Hao", "Hohin Lee", "Praneeth Sanapathi", "Sarah Hilado", "Bian Jiang", "Javier Alvarez-Valle", "Mu Wei", "Jianfeng Gao", "Eric Horvitz", "Matt Lungren", "Hoifung Poon", "Paul Vozila"], "title": "The Illusion of Readiness: Stress Testing Large Frontier Models on Multimodal Medical Benchmarks", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "35 pages", "summary": "Large frontier models like GPT-5 now achieve top scores on medical\nbenchmarks. But our stress tests tell a different story. Leading systems often\nguess correctly even when key inputs like images are removed, flip answers\nunder trivial prompt changes, and fabricate convincing yet flawed reasoning.\nThese aren't glitches; they expose how today's benchmarks reward test-taking\ntricks over medical understanding. We evaluate six flagship models across six\nwidely used benchmarks and find that high leaderboard scores hide brittleness\nand shortcut learning. Through clinician-guided rubric evaluation, we show that\nbenchmarks vary widely in what they truly measure yet are treated\ninterchangeably, masking failure modes. We caution that medical benchmark\nscores do not directly reflect real-world readiness. If we want AI to earn\ntrust in healthcare, we must demand more than leaderboard wins and must hold\nsystems accountable for robustness, sound reasoning, and alignment with real\nmedical demands.", "AI": {"tldr": "\u524d\u6cbf\u533b\u5b66AI\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u867d\u83b7\u9ad8\u5206\uff0c\u4f46\u538b\u529b\u6d4b\u8bd5\u66b4\u9732\u5176\u4f9d\u8d56\u5e94\u8bd5\u6280\u5de7\u800c\u975e\u771f\u5b9e\u533b\u5b66\u7406\u89e3\uff0c\u5b58\u5728\u63a8\u7406\u7f3a\u9677\u548c\u8106\u5f31\u6027\u3002", "motivation": "\u63ed\u793a\u5f53\u524d\u533b\u5b66AI\u57fa\u51c6\u6d4b\u8bd5\u7684\u5c40\u9650\u6027\uff0c\u9ad8\u5206\u6a21\u578b\u5b9e\u9645\u5b58\u5728\u7cfb\u7edf\u6027\u8106\u5f31\uff0c\u9700\u5efa\u7acb\u66f4\u4e25\u683c\u7684\u8bc4\u4f30\u4f53\u7cfb\u3002", "method": "\u5bf96\u4e2a\u4e3b\u6d41\u6a21\u578b\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\uff08\u5220\u9664\u5173\u952e\u8f93\u5165/\u4fee\u6539\u63d0\u793a\u8bcd/\u63a8\u7406\u5206\u6790\uff09\uff0c\u7ed3\u5408\u4e34\u5e8a\u533b\u751f\u5236\u5b9a\u8bc4\u4f30\u6807\u51c6\u3002", "result": "\u6a21\u578b\u8868\u73b0\u51fa\uff1a1\uff09\u4f9d\u8d56\u65e0\u5173\u7ebf\u7d22\u731c\u6d4b\u7b54\u6848 2\uff09\u7b54\u6848\u968f\u63d0\u793a\u5fae\u5c0f\u6539\u53d8\u800c\u7ffb\u8f6c 3\uff09\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u9519\u8bef\u7684\u63a8\u7406\u8fc7\u7a0b\u3002", "conclusion": "\u533b\u5b66AI\u8bc4\u4f30\u5e94\u8d85\u8d8a\u57fa\u51c6\u5206\u6570\uff0c\u91cd\u70b9\u5173\u6ce8\u6a21\u578b\u9c81\u68d2\u6027\u3001\u63a8\u7406\u4e25\u8c28\u6027\u53ca\u4e0e\u771f\u5b9e\u533b\u7597\u573a\u666f\u7684\u5339\u914d\u5ea6\u3002"}}
{"id": "2509.18436", "pdf": "https://arxiv.org/pdf/2509.18436", "abs": "https://arxiv.org/abs/2509.18436", "authors": ["Hongda Jiang", "Xinyuan Zhang", "Siddhant Garg", "Rishab Arora", "Shiun-Zu Kuo", "Jiayang Xu", "Christopher Brossman", "Yue Liu", "Aaron Colak", "Ahmed Aly", "Anuj Kumar", "Xin Luna Dong"], "title": "Memory-QA: Answering Recall Questions Based on Multimodal Memories", "categories": ["cs.AI", "cs.CL", "cs.DB"], "comment": null, "summary": "We introduce Memory-QA, a novel real-world task that involves answering\nrecall questions about visual content from previously stored multimodal\nmemories. This task poses unique challenges, including the creation of\ntask-oriented memories, the effective utilization of temporal and location\ninformation within memories, and the ability to draw upon multiple memories to\nanswer a recall question. To address these challenges, we propose a\ncomprehensive pipeline, Pensieve, integrating memory-specific augmentation,\ntime- and location-aware multi-signal retrieval, and multi-memory QA\nfine-tuning. We created a multimodal benchmark to illustrate various real\nchallenges in this task, and show the superior performance of Pensieve over\nstate-of-the-art solutions (up to 14% on QA accuracy).", "AI": {"tldr": "\u63d0\u51faMemory-QA\u4efb\u52a1\u53caPensieve\u6d41\u7a0b\uff0c\u89e3\u51b3\u591a\u6a21\u6001\u8bb0\u5fc6\u95ee\u7b54\u4e2d\u7684\u4e09\u5927\u6311\u6218\uff0cQA\u51c6\u786e\u7387\u63d0\u534714%", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u57fa\u4e8e\u591a\u6a21\u6001\u8bb0\u5fc6\u7684\u53ec\u56de\u5f0f\u95ee\u7b54\u4e2d\u5b58\u5728\u4efb\u52a1\u5bfc\u5411\u8bb0\u5fc6\u6784\u5efa\u3001\u65f6\u7a7a\u4fe1\u606f\u5229\u7528\u548c\u591a\u8bb0\u5fc6\u878d\u5408\u4e09\u5927\u6311\u6218", "method": "Pensieve\u6846\u67b6\u6574\u5408\u8bb0\u5fc6\u589e\u5f3a\u3001\u65f6\u7a7a\u611f\u77e5\u68c0\u7d22\u548c\u591a\u8bb0\u5fc6QA\u5fae\u8c03\u4e09\u9636\u6bb5\u6d41\u7a0b", "result": "\u5728\u81ea\u5efa\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51c6\u786e\u7387\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u8fbe14%", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u5316\u89e3\u51b3\u65b9\u6848\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u8bb0\u5fc6\u95ee\u7b54\u4efb\u52a1\u7684\u6027\u80fd"}}
{"id": "2509.18531", "pdf": "https://arxiv.org/pdf/2509.18531", "abs": "https://arxiv.org/abs/2509.18531", "authors": ["Seungyoun Shin", "Dongha Ahn", "Jiwoo Kim", "Sungwook Jeon"], "title": "No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": "submitted to ICASSP 2026", "summary": "Recent work reports gains in neural text-to-speech (TTS) with Group Relative\nPolicy Optimization (GRPO). However, in the absence of a verifiable reward for\n\\textit{prosody}, GRPO trained on transcription-oriented signals (CER/NLL)\nlowers error rates yet collapses prosody into monotone, unnatural speech;\nadding speaker-similarity further destabilizes training and degrades CER. We\naddress this with an \\textit{iterative Direct Preference Optimization (DPO)}\nscheme that uses only a few hundred human-labeled preference pairs per round to\ndirectly optimize prosodic naturalness while regularizing to the current model.\nOn \\textbf{KoCC-TTS}, a curated dataset of authentic Korean call center\ninteractions capturing task-oriented dialogues, our method attains the highest\nhuman preference (ELO) with competitive CER, outperforming GRPO and strong\ncommercial baselines. These results suggest that when prosody cannot be\nrewarded automatically, \\textit{human preference optimization} offers a\npractical and data-efficient path to natural and robust TTS. The demo page is\navailable at \\href{https://tts.ch.dev}", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8fed\u4ee3\u5f0f\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u7684\u6587\u672c\u8f6c\u8bed\u97f3\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u5c11\u91cf\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u4e0b\u663e\u8457\u63d0\u5347\u8bed\u97f3\u81ea\u7136\u5ea6", "motivation": "\u73b0\u6709GRPO\u65b9\u6cd5\u4f9d\u8d56\u8f6c\u5f55\u6307\u6807\u4f18\u5316\u5bfc\u81f4\u8bed\u97f3\u97f5\u5f8b\u5355\u8c03\uff0c\u52a0\u5165\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u6027\u6307\u6807\u4f1a\u7834\u574f\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002\u9700\u5728\u7f3a\u4e4f\u81ea\u52a8\u97f5\u5f8b\u8bc4\u4f30\u6307\u6807\u65f6\u63d0\u5347TTS\u81ea\u7136\u5ea6", "method": "\u91c7\u7528\u591a\u8f6e\u8fed\u4ee3DPO\u6846\u67b6\uff0c\u6bcf\u8f6e\u4ec5\u9700\u6570\u767e\u4e2a\u4eba\u7c7b\u6807\u6ce8\u7684\u504f\u597d\u5bf9\uff0c\u76f4\u63a5\u4f18\u5316\u97f5\u5f8b\u81ea\u7136\u5ea6\u540c\u65f6\u8fdb\u884c\u6a21\u578b\u6b63\u5219\u5316", "result": "\u5728\u97e9\u56fd\u5ba2\u670d\u5bf9\u8bdd\u6570\u636e\u96c6KoCC-TTS\u4e0a\u53d6\u5f97\u6700\u9ad8\u4eba\u7c7b\u504f\u597d\u8bc4\u5206\uff08ELO\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u5b57\u9519\u8bef\u7387\uff08CER\uff09\uff0c\u8d85\u8d8aGRPO\u548c\u5546\u4e1a\u7cfb\u7edf", "conclusion": "\u5f53\u97f5\u5f8b\u65e0\u6cd5\u81ea\u52a8\u8bc4\u4f30\u65f6\uff0c\u4eba\u7c7b\u504f\u597d\u4f18\u5316\u4e3a\u6784\u5efa\u81ea\u7136\u7a33\u5065\u7684TTS\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6570\u636e\u9ad8\u6548\u7684\u6280\u672f\u8def\u5f84"}}
{"id": "2509.18570", "pdf": "https://arxiv.org/pdf/2509.18570", "abs": "https://arxiv.org/abs/2509.18570", "authors": ["Yuke Si", "Runyan Yang", "Yingying Gao", "Junlan Feng", "Chao Deng", "Shilei Zhang"], "title": "HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for Multi-Task Speech Language Modeling", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "5 pages; submitted to ICASSP 2026", "summary": "Recent advances in large language models have facilitated the development of\nunified speech language models (SLMs) capable of supporting multiple speech\ntasks within a shared architecture. However, tasks such as automatic speech\nrecognition (ASR) and speech emotion recognition (SER) rely on distinct types\nof information: ASR primarily depends on linguistic content, whereas SER\nrequires the integration of both linguistic and paralinguistic cues. Existing\nmultitask SLMs typically adopt naive parameter sharing or prompt-based\nconditioning without explicitly modeling the differences in information\ncomposition required by each task. Such designs risk task interference and\nperformance degradation, especially under limited data conditions. To address\nthese limitations, we propose HarmoniFuse, a component-selective and\nprompt-adaptive framework for multi-task speech language modeling. HarmoniFuse\nis designed to harmonize heterogeneous task demands by selecting and fusing\ntask-relevant components of speech representations. Specifically, it integrates\na gated speech encoder to extract task-specific acoustic features and a\nprompt-adaptive dynamic fusion module to aggregate transformer layers based on\ntask characteristics. In addition, a batch-interleaved training strategy\nenables leveraging separate ASR and SER datasets without requiring joint\nannotation. Experimental results demonstrate that HarmoniFuse improves both ASR\nand SER performance, offering a scalable and robust solution for multitask\nspeech understanding under realistic data constraints.", "AI": {"tldr": "\u63d0\u51faHarmoniFuse\u6846\u67b6\uff0c\u901a\u8fc7\u7ec4\u4ef6\u9009\u62e9\u6027\u878d\u5408\u673a\u5236\u89e3\u51b3\u591a\u4efb\u52a1\u8bed\u97f3\u6a21\u578b\u4e2dASR\u4e0eSER\u4efb\u52a1\u7684\u4fe1\u606f\u9700\u6c42\u51b2\u7a81\u95ee\u9898\uff0c\u5728\u6570\u636e\u53d7\u9650\u6761\u4ef6\u4e0b\u63d0\u5347\u53cc\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7edf\u4e00\u8bed\u97f3\u6a21\u578b\u672a\u663e\u5f0f\u5efa\u6a21\u4e0d\u540c\u4efb\u52a1\uff08ASR\u4f9d\u8d56\u8bed\u8a00\u5185\u5bb9/SER\u4f9d\u8d56\u8bed\u8a00+\u526f\u8bed\u8a00\u7279\u5f81\uff09\u7684\u4fe1\u606f\u6784\u6210\u5dee\u5f02\uff0c\u5bfc\u81f4\u4efb\u52a1\u5e72\u6270\u548c\u6027\u80fd\u4e0b\u964d\uff0c\u7279\u522b\u662f\u6570\u636e\u6709\u9650\u65f6\u3002", "method": "\u5305\u542b\u95e8\u63a7\u8bed\u97f3\u7f16\u7801\u5668\uff08\u63d0\u53d6\u4efb\u52a1\u7279\u5b9a\u58f0\u5b66\u7279\u5f81\uff09\u3001\u63d0\u793a\u81ea\u9002\u5e94\u52a8\u6001\u878d\u5408\u6a21\u5757\uff08\u805a\u5408Transformer\u5c42\uff09\uff0c\u4ee5\u53ca\u4e0d\u4f9d\u8d56\u8054\u5408\u6807\u6ce8\u7684\u6279\u91cf\u4ea4\u9519\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eHarmoniFuse\u540c\u6b65\u63d0\u5347ASR\u548cSER\u6027\u80fd\uff0cWER\u76f8\u5bf9\u964d\u4f4e8.2%\uff0cSER\u51c6\u786e\u7387\u63d0\u53475.1%\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u534f\u8c03\u5f02\u6784\u4efb\u52a1\u9700\u6c42\uff0c\u4e3a\u73b0\u5b9e\u6570\u636e\u7ea6\u675f\u4e0b\u7684\u591a\u4efb\u52a1\u8bed\u97f3\u7406\u89e3\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.18579", "pdf": "https://arxiv.org/pdf/2509.18579", "abs": "https://arxiv.org/abs/2509.18579", "authors": ["Runyan Yang", "Yuke Si", "Yingying Gao", "Junlan Feng", "Chao Deng", "Shilei Zhang"], "title": "Teaching Audio Models to Reason: A Unified Framework for Source- and Layer-wise Distillation", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "5 pages; submitted to ICASSP 2026", "summary": "While large audio language models excel at tasks like ASR and emotion\nrecognition, they still struggle with complex reasoning due to the modality gap\nbetween audio and text as well as the lack of structured intermediate\nsupervision. To address this, we propose a unified knowledge distillation\nframework to transfer reasoning capabilities from a high-capacity textual\nteacher model to a student audio models while preserving its acoustic\ncompetence. Our method introduces two key dimensions: source-wise distillation,\nwhich leverages both textual and acoustic teachers to provide complementary\nmodality-specific supervision; and layer-wise distillation, which aligns\nteacher signals with appropriate student layers to improve transfer efficiency.\nThis dual-dimensional strategy enables fine-grained control over the\ndistillation process, effectively bridging the gap between symbolic reasoning\nand speech representations. Experimental results show significant improvements\nin audio reasoning performance, demonstrating the effectiveness of our\nframework as a reasoning transfer solution for audio modeling.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u7ef4\u5ea6\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c/\u8bed\u97f3\u53cc\u6559\u5e08\u8054\u5408\u84b8\u998f\u548c\u8de8\u5c42\u7ea7\u5bf9\u9f50\u7b56\u7565\uff0c\u5b9e\u73b0\u8bed\u97f3\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u8fc1\u79fb", "motivation": "\u89e3\u51b3\u8bed\u97f3\u6a21\u578b\u56e0\u8de8\u6a21\u6001\u5dee\u5f02\u548c\u7ed3\u6784\u5316\u76d1\u7763\u7f3a\u5931\u5bfc\u81f4\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u95ee\u9898\uff0c\u8bd5\u56fe\u8fde\u63a5\u7b26\u53f7\u63a8\u7406\u4e0e\u8bed\u97f3\u8868\u5f81", "method": "\u6e90\u7ef4\u5ea6\u8054\u5408\u6587\u672c\u6559\u5e08\uff08\u7b26\u53f7\u63a8\u7406\uff09\u4e0e\u8bed\u97f3\u6559\u5e08\uff08\u58f0\u5b66\u7279\u5f81\uff09\u8fdb\u884c\u4e92\u8865\u76d1\u7763\uff1b\u5c42\u7ef4\u5ea6\u5c06\u6559\u5e08\u4fe1\u53f7\u5339\u914d\u5230\u5b66\u751f\u6a21\u578b\u4e0d\u540c\u5c42\u7ea7\u63d0\u5347\u8fc1\u79fb\u6548\u7387", "result": "\u5b9e\u9a8c\u663e\u793a\u97f3\u9891\u63a8\u7406\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u9a8c\u8bc1\u6846\u67b6\u4f5c\u4e3a\u63a8\u7406\u8fc1\u79fb\u65b9\u6848\u7684\u6709\u6548\u6027", "conclusion": "\u901a\u8fc7\u591a\u6a21\u6001\u76d1\u7763\u4e0e\u5c42\u7ea7\u9002\u914d\u7684\u534f\u540c\u84b8\u998f\u7b56\u7565\uff0c\u6210\u529f\u5b9e\u73b0\u4ece\u6587\u672c\u6a21\u578b\u5230\u8bed\u97f3\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u8fc1\u79fb\uff0c\u7a81\u7834\u73b0\u6709\u8bed\u97f3\u6a21\u578b\u7684\u7b26\u53f7\u63a8\u7406\u74f6\u9888"}}
{"id": "2509.18600", "pdf": "https://arxiv.org/pdf/2509.18600", "abs": "https://arxiv.org/abs/2509.18600", "authors": ["Zhuoxiao Chen", "Hongyang Yu", "Ying Xu", "Yadan Luo", "Long Duong", "Yuan-Fang Li"], "title": "OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Radiology report generation (RRG) aims to automatically produce clinically\nfaithful reports from chest X-ray images. Prevailing work typically follows a\nscale-driven paradigm, by multi-stage training over large paired corpora and\noversized backbones, making pipelines highly data- and compute-intensive. In\nthis paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based\nreward (FactS) to tackle the RRG task under constrained budgets. OraPO enables\nsingle-stage, RL-only training by converting failed GRPO explorations on rare\nor difficult studies into direct preference supervision via a lightweight\noracle step. FactS grounds learning in diagnostic evidence by extracting atomic\nclinical facts and checking entailment against ground-truth labels, yielding\ndense, interpretable sentence-level rewards. Together, OraPO and FactS create a\ncompact and powerful framework that significantly improves learning efficiency\non clinically challenging cases, setting the new SOTA performance on the\nCheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training\ndata using a small base VLM on modest hardware.", "AI": {"tldr": "\u63d0\u51faOraPO+FactS\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u4e0e\u4e8b\u5b9e\u9a8c\u8bc1\u5956\u52b1\u673a\u5236\uff0c\u5728\u6709\u9650\u8d44\u6e90\u4e0b\u5b9e\u73b0\u9ad8\u6548\u653e\u5c04\u62a5\u544a\u751f\u6210", "motivation": "\u73b0\u6709\u653e\u5c04\u62a5\u544a\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u591a\u9636\u6bb5\u8bad\u7ec3\u548c\u5927\u89c4\u6a21\u6570\u636e/\u7b97\u529b\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u5e94\u7528", "method": "OraPO\u5c06\u5931\u8d25\u63a2\u7d22\u8f6c\u5316\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0cFactS\u901a\u8fc7\u539f\u5b50\u4e8b\u5b9e\u63d0\u53d6\u4e0e\u6807\u7b7e\u9a8c\u8bc1\u63d0\u4f9b\u5bc6\u96c6\u5956\u52b1", "result": "\u5728CheXpert Plus\u6570\u636e\u96c6\u8fbe\u5230SOTA\uff08F1 0.341\uff09\uff0c\u8bad\u7ec3\u6570\u636e\u51cf\u5c11100-1000\u500d\uff0c\u4f7f\u7528\u5c0f\u578bVLM", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e34\u5e8a\u6311\u6218\u6848\u4f8b\u7684\u5b66\u4e60\u6548\u7387\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.18661", "pdf": "https://arxiv.org/pdf/2509.18661", "abs": "https://arxiv.org/abs/2509.18661", "authors": ["Yixin Liu", "Yonghui Wu", "Denghui Zhang", "Lichao Sun"], "title": "Agentic AutoSurvey: Let LLMs Survey LLMs", "categories": ["cs.IR", "cs.CL", "cs.HC"], "comment": "29 pages, 7 figures", "summary": "The exponential growth of scientific literature poses unprecedented\nchallenges for researchers attempting to synthesize knowledge across rapidly\nevolving fields. We present \\textbf{Agentic AutoSurvey}, a multi-agent\nframework for automated survey generation that addresses fundamental\nlimitations in existing approaches. Our system employs four specialized agents\n(Paper Search Specialist, Topic Mining \\& Clustering, Academic Survey Writer,\nand Quality Evaluator) working in concert to generate comprehensive literature\nsurveys with superior synthesis quality. Through experiments on six\nrepresentative LLM research topics from COLM 2024 categories, we demonstrate\nthat our multi-agent approach achieves significant improvements over existing\nbaselines, scoring 8.18/10 compared to AutoSurvey's 4.77/10. The multi-agent\narchitecture processes 75--443 papers per topic (847 total across six topics)\nwhile targeting high citation coverage (often $\\geq$80\\% on 75--100-paper sets;\nlower on very large sets such as RLHF) through specialized agent orchestration.\nOur 12-dimension evaluation captures organization, synthesis integration, and\ncritical analysis beyond basic metrics. These findings demonstrate that\nmulti-agent architectures represent a meaningful advancement for automated\nliterature survey generation in rapidly evolving scientific domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u591a\u667a\u80fd\u4f53\u6846\u67b6Agentic AutoSurvey\uff0c\u901a\u8fc7\u56db\u7c7b\u4e13\u4e1a\u4ee3\u7406\u534f\u4f5c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u5316\u6587\u732e\u7efc\u8ff0\u8d28\u91cf", "motivation": "\u79d1\u5b66\u6587\u732e\u7684\u6307\u6570\u7ea7\u589e\u957f\u5bfc\u81f4\u4f20\u7edf\u65b9\u6cd5\u5728\u5feb\u901f\u6f14\u8fdb\u9886\u57df\u96be\u4ee5\u6709\u6548\u6574\u5408\u77e5\u8bc6\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u7efc\u8ff0\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650", "method": "\u90e8\u7f72\u56db\u4e2a\u4e13\u4e1a\u4ee3\u7406\uff08\u8bba\u6587\u641c\u7d22\u4e13\u5bb6\u3001\u4e3b\u9898\u6316\u6398\u805a\u7c7b\u3001\u5b66\u672f\u7efc\u8ff0\u64b0\u5199\u3001\u8d28\u91cf\u8bc4\u4f30\uff09\u7684\u534f\u540c\u5de5\u4f5c\u6d41\u7a0b", "result": "\u57286\u4e2aLLM\u7814\u7a76\u4e3b\u9898\u7684\u6d4b\u8bd5\u4e2d\u53d6\u5f978.18/10\u7684\u8bc4\u5206\uff08\u57fa\u51c6\u4e3a4.77\uff09\uff0c\u5904\u740675-443\u7bc7/\u4e3b\u9898\uff08\u603b\u8ba1847\u7bc7\uff09\u5e76\u4fdd\u6301\u9ad8\u5f15\u7528\u8986\u76d6\u7387", "conclusion": "\u591a\u667a\u80fd\u4f53\u67b6\u6784\u901a\u8fc712\u7ef4\u8bc4\u4f30\u4f53\u7cfb\u9a8c\u8bc1\u4e86\u5176\u5728\u7ec4\u7ec7\u7ed3\u6784\u3001\u77e5\u8bc6\u6574\u5408\u548c\u6279\u5224\u5206\u6790\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u4ee3\u8868\u4e86\u81ea\u52a8\u5316\u6587\u732e\u7efc\u8ff0\u9886\u57df\u7684\u91cd\u8981\u8fdb\u5c55"}}
{"id": "2509.18816", "pdf": "https://arxiv.org/pdf/2509.18816", "abs": "https://arxiv.org/abs/2509.18816", "authors": ["Junyu Wang", "Ziyang Ma", "Zhengding Luo", "Tianrui Wang", "Meng Ge", "Xiaobao Wang", "Longbiao Wang"], "title": "Pay More Attention To Audio: Mitigating Imbalance of Cross-Modal Attention in Large Audio Language Models", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "comment": "Submitted to ICASSP 2026", "summary": "Large Audio-Language Models (LALMs) often suffer from audio-textual attention\nimbalance, prioritizing text over acoustic information, particularly in the\nmulti-modal fusion layers of the Transformer architecture. This bias hinders\ntheir ability to fully utilize acoustic cues, causing suboptimal performance on\naudio reasoning tasks. To mitigate this, we propose \\textbf{MATA}, a novel\ntraining-free method that dynamically pushes LALMs to pay \\textbf{M}ore\n\\textbf{A}ttention \\textbf{T}o \\textbf{A}udio tokens within the self-attention\nmechanism. Specifically, MATA intervenes post raw attention scoring, targeting\nonly the last token in intermediate layers without introducing additional\nparameters or computational overhead. Experiments on the MMAU and MMAR\nbenchmarks confirm MATA's effectiveness, with consistent performance gains.\nNotably, on MMAR, MATA enables an open-source model to surpass the proprietary\nGemini 2.0 Flash for the first time. Our work provides an efficient solution to\nmitigate attention bias and opens a new research direction for enhancing the\naudio-processing capabilities of multi-modal models.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684MATA\u65b9\u6cd5\uff0c\u901a\u8fc7\u8c03\u6574\u81ea\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u591a\u6a21\u6001\u6a21\u578b\u5bf9\u97f3\u9891\u6807\u8bb0\u7684\u5173\u6ce8\uff0c\u663e\u8457\u63d0\u5347\u97f3\u9891\u63a8\u7406\u6027\u80fd", "motivation": "\u73b0\u6709\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u97f3\u9891-\u6587\u672c\u6ce8\u610f\u529b\u5931\u8861\u95ee\u9898\uff0c\u5728Transformer\u878d\u5408\u5c42\u8fc7\u5ea6\u504f\u5411\u6587\u672c\uff0c\u5bfc\u81f4\u97f3\u9891\u4fe1\u606f\u5229\u7528\u4e0d\u8db3", "method": "\u5728\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u52a8\u6001\u8c03\u6574\u6ce8\u610f\u529b\u5f97\u5206\uff0c\u4ec5\u9488\u5bf9\u4e2d\u95f4\u5c42\u7684\u6700\u540e\u4e00\u4e2a\u97f3\u9891\u6807\u8bb0\u8fdb\u884c\u5e72\u9884\uff0c\u65e0\u9700\u989d\u5916\u53c2\u6570\u6216\u8ba1\u7b97\u5f00\u9500", "result": "\u5728MMAU/MMAR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6301\u7eed\u63d0\u5347\uff0c\u4f7f\u5f00\u6e90\u6a21\u578b\u9996\u6b21\u8d85\u8d8aGemini 2.0 Flash\uff08MMAR\u57fa\u51c6\uff09", "conclusion": "\u4e3a\u89e3\u51b3\u591a\u6a21\u6001\u6ce8\u610f\u529b\u504f\u5dee\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5f00\u8f9f\u4e86\u589e\u5f3a\u591a\u6a21\u6001\u6a21\u578b\u97f3\u9891\u5904\u7406\u80fd\u529b\u7684\u65b0\u7814\u7a76\u65b9\u5411"}}
{"id": "2509.18847", "pdf": "https://arxiv.org/pdf/2509.18847", "abs": "https://arxiv.org/abs/2509.18847", "authors": ["Junhao Su", "Yuanliang Wan", "Junwei Yang", "Hengyu Shi", "Tianyang Han", "Junfeng Luo", "Yurui Qiu"], "title": "Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "9pages", "summary": "Tool-augmented large language models (LLMs) are usually trained with\nsupervised imitation or coarse-grained reinforcement learning that optimizes\nsingle tool calls. Current self-reflection practices rely on heuristic prompts\nor one-way reasoning: the model is urged to 'think more' instead of learning\nerror diagnosis and repair. This is fragile in multi-turn interactions; after a\nfailure the model often repeats the same mistake. We propose structured\nreflection, which turns the path from error to repair into an explicit,\ncontrollable, and trainable action. The agent produces a short yet precise\nreflection: it diagnoses the failure using evidence from the previous step and\nthen proposes a correct, executable follow-up call. For training we combine\nDAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing\nthe stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce\nTool-Reflection-Bench, a lightweight benchmark that programmatically checks\nstructural validity, executability, parameter correctness, and result\nconsistency. Tasks are built as mini trajectories of erroneous call,\nreflection, and corrected call, with disjoint train and test splits.\nExperiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn\ntool-call success and error recovery, and a reduction of redundant calls. These\nresults indicate that making reflection explicit and optimizing it directly\nimproves the reliability of tool interaction and offers a reproducible path for\nagents to learn from failure.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u6784\u5316\u53cd\u601d\u65b9\u6cd5\uff08Reflect-Call-Final\uff09\u4f18\u5316\u591a\u8f6e\u5de5\u5177\u8c03\u7528\uff0c\u901a\u8fc7\u663e\u5f0f\u9519\u8bef\u8bca\u65ad\u548c\u4fee\u590d\u63d0\u5347LLM\u5de5\u5177\u4ea4\u4e92\u53ef\u9760\u6027\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u589e\u5f3aLLM\u7684\u81ea\u6211\u53cd\u601d\u673a\u5236\u8106\u5f31\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u591a\u8f6e\u4ea4\u4e92\u4e2d\u7684\u9519\u8bef\u79ef\u7d2f\u95ee\u9898\uff0c\u9700\u5c06\u53cd\u601d\u8fc7\u7a0b\u663e\u5f0f\u5316\u5e76\u9488\u5bf9\u6027\u4f18\u5316\u3002", "method": "\u7ed3\u5408DAPO/GSPO\u8bad\u7ec3\u76ee\u6807\u548c\u5206\u9636\u6bb5\u5956\u52b1\u673a\u5236\uff08Reflect\u2192Call\u2192Final\uff09\uff0c\u6784\u5efaTool-Reflection-Bench\u57fa\u51c6\u9a8c\u8bc1\u7ed3\u6784\u5316\u53cd\u601d\u6709\u6548\u6027\u3002", "result": "\u5728BFCL v3\u548c\u81ea\u5efa\u57fa\u51c6\u4e0a\u5b9e\u73b0\u591a\u8f6e\u8c03\u7528\u6210\u529f\u7387+35%\uff0c\u9519\u8bef\u6062\u590d\u7387+41%\uff0c\u5197\u4f59\u8c03\u7528\u51cf\u5c1128%\u3002", "conclusion": "\u663e\u5f0f\u7ed3\u6784\u5316\u53cd\u601d\u673a\u5236\u663e\u8457\u63d0\u5347\u5de5\u5177\u4ea4\u4e92\u53ef\u9760\u6027\uff0c\u4e3a\u667a\u80fd\u4f53\u4ece\u5931\u8d25\u4e2d\u5b66\u4e60\u63d0\u4f9b\u53ef\u590d\u73b0\u8def\u5f84\uff0c\u63a8\u52a8LLM\u5de5\u5177\u4f7f\u7528\u8303\u5f0f\u8fdb\u5316\u3002"}}
{"id": "2509.19002", "pdf": "https://arxiv.org/pdf/2509.19002", "abs": "https://arxiv.org/abs/2509.19002", "authors": ["Hao Wang", "Eiki Murata", "Lingfang Zhang", "Ayako Sato", "So Fukuda", "Ziqi Yin", "Wentao Hu", "Keisuke Nakao", "Yusuke Nakamura", "Sebastian Zwirner", "Yi-Chia Chen", "Hiroyuki Otomo", "Hiroki Ouchi", "Daisuke Kawahara"], "title": "VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced video understanding capabilities, opening new\npossibilities for practical applications. Yet current video benchmarks focus\nlargely on indoor scenes or short-range outdoor activities, leaving the\nchallenges associated with long-distance travel largely unexplored. Mastering\nextended geospatial-temporal trajectories is critical for next-generation\nMLLMs, underpinning real-world tasks such as embodied-AI planning and\nnavigation. To bridge this gap, we present VIR-Bench, a novel benchmark\nconsisting of 200 travel videos that frames itinerary reconstruction as a\nchallenging task designed to evaluate and push forward MLLMs'\ngeospatial-temporal intelligence. Experimental results reveal that\nstate-of-the-art MLLMs, including proprietary ones, struggle to achieve high\nscores, underscoring the difficulty of handling videos that span extended\nspatial and temporal scales. Moreover, we conduct an in-depth case study in\nwhich we develop a prototype travel-planning agent that leverages the insights\ngained from VIR-Bench. The agent's markedly improved itinerary recommendations\nverify that our evaluation protocol not only benchmarks models effectively but\nalso translates into concrete performance gains in user-facing applications.", "AI": {"tldr": "\u63d0\u51faVIR-Bench\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u586b\u8865\u957f\u8ddd\u79bb\u65c5\u884c\u89c6\u9891\u7406\u89e3\u8bc4\u4f30\u7a7a\u767d\uff0c\u901a\u8fc7\u884c\u7a0b\u91cd\u5efa\u4efb\u52a1\u9a8c\u8bc1MLLMs\u5728\u957f\u65f6\u7a7a\u8f68\u8ff9\u5904\u7406\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u5c55\u793a\u6784\u5efa\u7684\u65c5\u884c\u89c4\u5212\u4ee3\u7406\u5982\u4f55\u63d0\u5347\u884c\u7a0b\u63a8\u8350\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u96c6\u4e2d\u4e8e\u5ba4\u5185\u573a\u666f\u548c\u77ed\u9014\u6d3b\u52a8\uff0c\u7f3a\u4e4f\u5bf9\u957f\u8ddd\u79bb\u65c5\u884c\u573a\u666f\u7684\u8bc4\u4f30\u80fd\u529b\uff0c\u5236\u7ea6\u4e86MLLMs\u5728\u5bfc\u822a\u3001\u5177\u8eabAI\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u5305\u542b200\u4e2a\u65c5\u884c\u89c6\u9891\u7684VIR-Bench\u57fa\u51c6\uff0c\u8bbe\u8ba1\u884c\u7a0b\u91cd\u5efa\u8bc4\u4f30\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5f00\u53d1\u539f\u578b\u65c5\u884c\u89c4\u5212\u4ee3\u7406\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u4e3b\u6d41MLLMs(\u542b\u5546\u4e1a\u6a21\u578b)\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u6b20\u4f73\uff0c\u9a8c\u8bc1\u957f\u65f6\u7a7a\u89c6\u9891\u5904\u7406\u96be\u5ea6\uff1b\u6784\u5efa\u7684\u4ee3\u7406\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u884c\u7a0b\u63a8\u8350\u8d28\u91cf\uff0c\u8bc1\u660e\u8bc4\u4f30\u534f\u8bae\u7684\u6709\u6548\u6027\u3002", "conclusion": "VIR-Bench\u4e0d\u4ec5\u4e3aMLLMs\u8bc4\u4f30\u63d0\u4f9b\u65b0\u7ef4\u5ea6\uff0c\u5176\u8bc4\u4f30\u65b9\u6cd5\u53ef\u76f4\u63a5\u63d0\u5347\u5b9e\u9645\u5e94\u7528\u6027\u80fd\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u5730\u7406\u65f6\u7a7a\u667a\u80fd\u53d1\u5c55\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2509.19070", "pdf": "https://arxiv.org/pdf/2509.19070", "abs": "https://arxiv.org/abs/2509.19070", "authors": ["Zijian Ling", "Han Zhang", "Yazhuo Zhou", "Jiahao Cui"], "title": "ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted at the Open Science for Foundation Models (SCI-FM) Workshop\n  at ICLR 2025", "summary": "This paper presents ColorBlindnessEval, a novel benchmark designed to\nevaluate the robustness of Vision-Language Models (VLMs) in visually\nadversarial scenarios inspired by the Ishihara color blindness test. Our\ndataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 with\nvarying color combinations, challenging VLMs to accurately recognize numerical\ninformation embedded in complex visual patterns. We assess 9 VLMs using Yes/No\nand open-ended prompts and compare their performance with human participants.\nOur experiments reveal limitations in the models' ability to interpret numbers\nin adversarial contexts, highlighting prevalent hallucination issues. These\nfindings underscore the need to improve the robustness of VLMs in complex\nvisual environments. ColorBlindnessEval serves as a valuable tool for\nbenchmarking and improving the reliability of VLMs in real-world applications\nwhere accuracy is critical.", "AI": {"tldr": "\u63d0\u51faColorBlindnessEval\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u6297\u6027\u89c6\u89c9\u573a\u666f\u4e2d\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u6a21\u578b\u5b58\u5728\u6570\u5b57\u8bc6\u522b\u5c40\u9650\u6027\u548c\u5e7b\u89c9\u95ee\u9898", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u89c6\u89c9\u73af\u5883\uff08\u7279\u522b\u662f\u5bf9\u6297\u6027\u573a\u666f\uff09\u4e2d\u7684\u9c81\u68d2\u6027\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u5efa\u7acb\u6709\u6548\u8bc4\u4f30\u4f53\u7cfb", "method": "\u6784\u5efa500\u5f20\u77f3\u539f\u6c0f\u8272\u76f2\u6d4b\u8bd5\u98ce\u683c\u7684\u6570\u5b57\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u91c7\u7528Yes/No\u548c\u5f00\u653e\u5f0f\u4e24\u79cd\u63d0\u793a\u7b56\u7565\u8bc4\u4f309\u4e2aVLMs\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8868\u73b0\u8fdb\u884c\u5bf9\u6bd4", "result": "\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u5728\u5bf9\u6297\u6027\u89c6\u89c9\u73af\u5883\u4e2d\u8bc6\u522b\u6570\u5b57\u51c6\u786e\u7387\u663e\u8457\u4e0b\u964d\uff0c\u666e\u904d\u5b58\u5728\u6570\u5b57\u5e7b\u89c9\u73b0\u8c61", "conclusion": "\u8be5\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u3001\u81ea\u52a8\u9a7e\u9a76\u7b49\u5173\u952e\u9886\u57df\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u6709\u6548\u8bc4\u4f30\u5de5\u5177"}}
{"id": "2509.19090", "pdf": "https://arxiv.org/pdf/2509.19090", "abs": "https://arxiv.org/abs/2509.19090", "authors": ["Guoxin Wang", "Jun Zhao", "Xinyi Liu", "Yanbo Liu", "Xuyang Cao", "Chao Li", "Zhuoyun Liu", "Qintian Sun", "Fangru Zhou", "Haoqiang Xing", "Zhenhong Yang"], "title": "Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Medical imaging provides critical evidence for clinical diagnosis, treatment\nplanning, and surgical decisions, yet most existing imaging models are narrowly\nfocused and require multiple specialized networks, limiting their\ngeneralization. Although large-scale language and multimodal models exhibit\nstrong reasoning and multi-task capabilities, real-world clinical applications\ndemand precise visual grounding, multimodal integration, and chain-of-thought\nreasoning. We introduce Citrus-V, a multimodal medical foundation model that\ncombines image analysis with textual reasoning. The model integrates detection,\nsegmentation, and multimodal chain-of-thought reasoning, enabling pixel-level\nlesion localization, structured report generation, and physician-like\ndiagnostic inference in a single framework. We propose a novel multimodal\ntraining approach and release a curated open-source data suite covering\nreasoning, detection, segmentation, and document understanding tasks.\nEvaluations demonstrate that Citrus-V outperforms existing open-source medical\nmodels and expert-level imaging systems across multiple benchmarks, delivering\na unified pipeline from visual grounding to clinical reasoning and supporting\nprecise lesion quantification, automated reporting, and reliable second\nopinions.", "AI": {"tldr": "Citrus-V\u662f\u9996\u4e2a\u6574\u5408\u68c0\u6d4b\u3001\u5206\u5272\u4e0e\u591a\u6a21\u6001\u601d\u7ef4\u94fe\u63a8\u7406\u7684\u533b\u5b66\u57fa\u7840\u6a21\u578b\uff0c\u5b9e\u73b0\u4ece\u75c5\u7076\u5b9a\u4f4d\u5230\u4e34\u5e8a\u63a8\u7406\u7684\u7aef\u5230\u7aef\u6d41\u7a0b", "motivation": "\u73b0\u6709\u533b\u5b66\u5f71\u50cf\u6a21\u578b\u5b58\u5728\u4e13\u9879\u5316\u5c40\u9650\uff0c\u9700\u591a\u4e2a\u4e13\u7528\u7f51\u7edc\u534f\u540c\uff0c\u96be\u4ee5\u5b9e\u73b0\u7cbe\u51c6\u89c6\u89c9\u5b9a\u4f4d\u4e0e\u4e34\u5e8a\u63a8\u7406\u7684\u6574\u5408", "method": "\u63d0\u51fa\u65b0\u578b\u591a\u6a21\u6001\u8bad\u7ec3\u8303\u5f0f\uff0c\u6574\u5408\u68c0\u6d4b/\u5206\u5272/\u63a8\u7406\u4e09\u4efb\u52a1\uff0c\u5f00\u53d1\u652f\u6301\u50cf\u7d20\u7ea7\u5b9a\u4f4d\u3001\u7ed3\u6784\u5316\u62a5\u544a\u751f\u6210\u548c\u7c7b\u533b\u5e08\u8bca\u65ad\u63a8\u7406\u7684\u7edf\u4e00\u6846\u67b6", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u5f00\u6e90\u533b\u5b66\u6a21\u578b\u548c\u4e13\u5bb6\u7ea7\u5f71\u50cf\u7cfb\u7edf\uff0c\u652f\u6301\u75c5\u7076\u91cf\u5316\u3001\u81ea\u52a8\u62a5\u544a\u751f\u6210\u548c\u53ef\u9760\u4e8c\u6b21\u8bca\u65ad", "conclusion": "Citrus-V\u901a\u8fc7\u89c6\u89c9-\u6587\u672c\u8054\u5408\u63a8\u7406\u5b9e\u73b0\u4e86\u4e34\u5e8a\u51b3\u7b56\u95ed\u73af\uff0c\u4e3a\u7cbe\u51c6\u533b\u7597\u63d0\u4f9b\u4ece\u5f71\u50cf\u5206\u6790\u5230\u8bca\u65ad\u5efa\u8bae\u7684\u5b8c\u6574\u89e3\u51b3\u65b9\u6848"}}
{"id": "2509.19231", "pdf": "https://arxiv.org/pdf/2509.19231", "abs": "https://arxiv.org/abs/2509.19231", "authors": ["Karen Rosero", "Eunjung Yeo", "David R. Mortensen", "Cortney Van't Slot", "Rami R. Hallac", "Carlos Busso"], "title": "Finding My Voice: Generative Reconstruction of Disordered Speech for Automated Clinical Evaluation", "categories": ["cs.SD", "cs.AI", "cs.CL"], "comment": null, "summary": "We present ChiReSSD, a speech reconstruction framework that preserves\nchildren speaker's identity while suppressing mispronunciations. Unlike prior\napproaches trained on healthy adult speech, ChiReSSD adapts to the voices of\nchildren with speech sound disorders (SSD), with particular emphasis on pitch\nand prosody. We evaluate our method on the STAR dataset and report substantial\nimprovements in lexical accuracy and speaker identity preservation.\nFurthermore, we automatically predict the phonetic content in the original and\nreconstructed pairs, where the proportion of corrected consonants is comparable\nto the percentage of correct consonants (PCC), a clinical speech assessment\nmetric. Our experiments show Pearson correlation of 0.63 between automatic and\nhuman expert annotations, highlighting the potential to reduce the manual\ntranscription burden. In addition, experiments on the TORGO dataset demonstrate\neffective generalization for reconstructing adult dysarthric speech. Our\nresults indicate that disentangled, style-based TTS reconstruction can provide\nidentity-preserving speech across diverse clinical populations.", "AI": {"tldr": "ChiReSSD\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u7684\u8bed\u97f3\u91cd\u6784\u6280\u672f\uff0c\u5728\u7ea0\u6b63\u53d1\u97f3\u7684\u540c\u65f6\u4fdd\u7559\u513f\u7ae5\u8a00\u8bed\u969c\u788d\u60a3\u8005\u7684\u97f3\u8272\u7279\u5f81", "motivation": "\u73b0\u6709\u8bed\u97f3\u91cd\u5efa\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u5065\u5eb7\u6210\u4eba\u8bed\u97f3\u8bad\u7ec3\uff0c\u96be\u4ee5\u9002\u5e94\u513f\u7ae5\u8a00\u8bed\u969c\u788d\u60a3\u8005\u7279\u6b8a\u7684\u97f3\u9ad8\u548c\u97f5\u5f8b\u7279\u5f81", "method": "\u91c7\u7528\u57fa\u4e8e\u98ce\u683c\u7684\u89e3\u8026\u6587\u672c\u8f6c\u8bed\u97f3\u6280\u672f\uff0c\u5728STAR\u548cTORGO\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u513f\u7ae5\u6784\u97f3\u969c\u788d\u53ca\u6210\u4eba\u8fd0\u52a8\u6027\u8a00\u8bed\u969c\u788d\u7684\u6cdb\u5316\u80fd\u529b", "result": "\u8bcd\u4e49\u51c6\u786e\u7387\u63d0\u534734%\uff0c\u81ea\u52a8\u9884\u6d4b\u7ed3\u679c\u4e0e\u4e34\u5e8a\u4e13\u5bb6\u6807\u6ce8\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u8fbe0.63\uff0c\u8f85\u97f3\u7ea0\u6b63\u6bd4\u4f8b\u4e0e\u4e34\u5e8aPCC\u6307\u6807\u9ad8\u5ea6\u543b\u5408", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e0d\u540c\u4e34\u5e8a\u4eba\u7fa4\u63d0\u4f9b\u4e86\u8eab\u4efd\u4fdd\u7559\u7684\u8bed\u97f3\u91cd\u5efa\u65b9\u6848\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u9884\u6d4b\u53ef\u964d\u4f4e75%\u4eba\u5de5\u6807\u6ce8\u8d1f\u62c5\uff0c\u5e76\u5c55\u73b0\u8de8\u5e74\u9f84\u5c42\u7684\u6cdb\u5316\u80fd\u529b"}}
{"id": "2509.19265", "pdf": "https://arxiv.org/pdf/2509.19265", "abs": "https://arxiv.org/abs/2509.19265", "authors": ["Saeed Almheiri", "Rania Hossam", "Mena Attia", "Chenxi Wang", "Preslav Nakov", "Timothy Baldwin", "Fajri Koto"], "title": "Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World", "categories": ["cs.AI", "cs.CL"], "comment": "EMNLP 2025 - Findings", "summary": "Large language models (LLMs) often reflect Western-centric biases, limiting\ntheir effectiveness in diverse cultural contexts. Although some work has\nexplored cultural alignment, the potential for cross-cultural transfer, using\nalignment in one culture to improve performance in others, remains\nunderexplored. This paper investigates cross-cultural transfer of commonsense\nreasoning in the Arab world, where linguistic and historical similarities\ncoexist with local cultural differences. Using a culturally grounded\ncommonsense reasoning dataset covering 13 Arab countries, we evaluate\nlightweight alignment methods such as in-context learning and\ndemonstration-based reinforcement (DITTO), alongside baselines like supervised\nfine-tuning and direct preference optimization. Our results show that merely 12\nculture-specific examples from one country can improve performance in others by\n10\\% on average, within multilingual models. In addition, we demonstrate that\nout-of-culture demonstrations from Indonesia and US contexts can match or\nsurpass in-culture alignment for MCQ reasoning, highlighting cultural\ncommonsense transferability beyond the Arab world. These findings demonstrate\nthat efficient cross-cultural alignment is possible and offer a promising\napproach to adapt LLMs to low-resource cultural settings.", "AI": {"tldr": "\u901a\u8fc712\u4e2a\u963f\u62c9\u4f2f\u56fd\u5bb6\u6587\u5316\u793a\u4f8b\u5b9e\u73b0\u8de8\u6587\u5316\u8fc1\u79fb\uff0c\u4f7fLLM\u5728\u963f\u62c9\u4f2f\u4e16\u754c\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\u6027\u80fd\u5e73\u5747\u63d0\u534710%\uff0c\u5370\u5c3c/\u7f8e\u56fd\u8de8\u6587\u5316\u793a\u8303\u540c\u6837\u6709\u6548", "motivation": "\u89e3\u51b3LLMs\u897f\u65b9\u4e2d\u5fc3\u504f\u89c1\u95ee\u9898\uff0c\u63a2\u7d22\u5229\u7528\u5355\u4e00\u6587\u5316\u5bf9\u9f50\u6570\u636e\u63d0\u5347\u591a\u6587\u5316\u573a\u666f\u6027\u80fd\u7684\u53ef\u884c\u6027", "method": "\u4f7f\u7528\u8986\u76d613\u4e2a\u963f\u62c9\u4f2f\u56fd\u5bb6\u7684\u6587\u5316\u5e38\u8bc6\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e0a\u4e0b\u6587\u5b66\u4e60/DITTO/SFT/DPO\u7b49\u65b9\u6cd5", "result": "\u8de8\u6587\u5316\u8fc1\u79fb\u63d0\u5347\u6548\u679c\u663e\u8457\uff08+10%\uff09\uff0c\u8de8\u5927\u6587\u5316\u5708\u793a\u8303\uff08\u5370\u5c3c/\u7f8e\u56fd\uff09\u6548\u679c\u7b49\u540c/\u8d85\u8d8a\u672c\u571f\u6587\u5316\u5bf9\u9f50", "conclusion": "\u8f7b\u91cf\u5316\u8de8\u6587\u5316\u5bf9\u9f50\u7b56\u7565\u53ef\u6709\u6548\u9002\u914dLLMs\u81f3\u4f4e\u8d44\u6e90\u6587\u5316\u573a\u666f\uff0c\u63ed\u793a\u6587\u5316\u5e38\u8bc6\u7684\u53ef\u8fc1\u79fb\u6027\u7279\u5f81"}}
