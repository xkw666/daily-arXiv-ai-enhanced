{"id": "2506.17301", "pdf": "https://arxiv.org/pdf/2506.17301", "abs": "https://arxiv.org/abs/2506.17301", "authors": ["Guian Fang", "Yuchao Gu", "Mike Zheng Shou"], "title": "FramePrompt: In-context Controllable Animation with Zero Structural Changes", "categories": ["cs.GR"], "comment": "Project page: https://frameprompt.github.io/", "summary": "Generating controllable character animation from a reference image and motion\nguidance remains a challenging task due to the inherent difficulty of injecting\nappearance and motion cues into video diffusion models. Prior works often rely\non complex architectures, explicit guider modules, or multi-stage processing\npipelines, which increase structural overhead and hinder deployment. Inspired\nby the strong visual context modeling capacity of pre-trained video diffusion\ntransformers, we propose FramePrompt, a minimalist yet powerful framework that\ntreats reference images, skeleton-guided motion, and target video clips as a\nunified visual sequence. By reformulating animation as a conditional future\nprediction task, we bypass the need for guider networks and structural\nmodifications. Experiments demonstrate that our method significantly\noutperforms representative baselines across various evaluation metrics while\nalso simplifying training. Our findings highlight the effectiveness of\nsequence-level visual conditioning and demonstrate the potential of pre-trained\nmodels for controllable animation without architectural changes.", "AI": {"tldr": "\u63d0\u51faFramePrompt\u6846\u67b6\uff0c\u901a\u8fc7\u5e8f\u5217\u7ea7\u89c6\u89c9\u6761\u4ef6\u5904\u7406\u5b9e\u73b0\u53ef\u63a7\u52a8\u753b\u751f\u6210\uff0c\u65e0\u9700\u4fee\u6539\u9884\u8bad\u7ec3\u6a21\u578b\u67b6\u6784", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u67b6\u6784\u3001\u989d\u5916\u5f15\u5bfc\u6a21\u5757\u6216\u591a\u9636\u6bb5\u6d41\u7a0b\uff0c\u5bfc\u81f4\u7ed3\u6784\u5197\u4f59\u4e14\u90e8\u7f72\u56f0\u96be", "method": "\u5c06\u53c2\u8003\u56fe\u50cf\u3001\u9aa8\u9abc\u8fd0\u52a8\u6307\u5bfc\u548c\u76ee\u6807\u89c6\u9891\u7edf\u4e00\u4e3a\u89c6\u89c9\u5e8f\u5217\uff0c\u91cd\u6784\u52a8\u753b\u751f\u6210\u4efb\u52a1\u4e3a\u6761\u4ef6\u5f0f\u672a\u6765\u5e27\u9884\u6d4b", "result": "\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u7b80\u5316\u8bad\u7ec3\u6d41\u7a0b\uff08\u4ec5\u9700\u5355\u9636\u6bb5\u8bad\u7ec3\uff09", "conclusion": "\u9a8c\u8bc1\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5e8f\u5217\u7ea7\u89c6\u89c9\u6761\u4ef6\u5904\u7406\u6f5c\u529b\uff0c\u8bc1\u660e\u65e0\u9700\u67b6\u6784\u4fee\u6539\u5373\u53ef\u5b9e\u73b0\u53ef\u63a7\u52a8\u753b\u751f\u6210"}}
{"id": "2506.17450", "pdf": "https://arxiv.org/pdf/2506.17450", "abs": "https://arxiv.org/abs/2506.17450", "authors": ["Jiacheng Chen", "Ramin Mehran", "Xuhui Jia", "Saining Xie", "Sanghyun Woo"], "title": "BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://blenderfusion.github.io", "summary": "We present BlenderFusion, a generative visual compositing framework that\nsynthesizes new scenes by recomposing objects, camera, and background. It\nfollows a layering-editing-compositing pipeline: (i) segmenting and converting\nvisual inputs into editable 3D entities (layering), (ii) editing them in\nBlender with 3D-grounded control (editing), and (iii) fusing them into a\ncoherent scene using a generative compositor (compositing). Our generative\ncompositor extends a pre-trained diffusion model to process both the original\n(source) and edited (target) scenes in parallel. It is fine-tuned on video\nframes with two key training strategies: (i) source masking, enabling flexible\nmodifications like background replacement; (ii) simulated object jittering,\nfacilitating disentangled control over objects and camera. BlenderFusion\nsignificantly outperforms prior methods in complex compositional scene editing\ntasks.", "AI": {"tldr": "BlenderFusion\u662f\u4e00\u4e2a\u5206\u5c42-\u7f16\u8f91-\u5408\u6210\u7684\u6846\u67b6\uff0c\u901a\u8fc73D\u5b9e\u4f53\u7f16\u8f91\u548c\u751f\u6210\u5f0f\u5408\u6210\u5b9e\u73b0\u590d\u6742\u573a\u666f\u7f16\u8f91", "motivation": "\u89e3\u51b3\u73b0\u6709\u751f\u6210\u5f0f\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u7f16\u8f91\u4e2d\u5b58\u5728\u7684\u63a7\u5236\u5c40\u9650\u6027\u95ee\u9898", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a1\uff09\u5206\u5c42\u5206\u52723D\u5b9e\u4f53\uff1b2\uff09Blender\u51853D\u7f16\u8f91\uff1b3\uff09\u6539\u8fdb\u7684\u6269\u6563\u6a21\u578b\uff08\u6e90\u63a9\u7801\u548c\u5bf9\u8c61\u6296\u52a8\u8bad\u7ec3\uff09\u8fdb\u884c\u751f\u6210\u5f0f\u5408\u6210", "result": "\u5728\u590d\u6742\u7ec4\u5408\u573a\u666f\u7f16\u8f91\u4efb\u52a1\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u5206\u5c42\u6d41\u7a0b\u548c\u7279\u6b8a\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7269\u4f53\u3001\u76f8\u673a\u548c\u80cc\u666f\u7684\u7cbe\u7ec6\u5316\u63a7\u5236\uff0c\u63a8\u52a8\u4e86\u751f\u6210\u5f0f\u573a\u666f\u7f16\u8f91\u6280\u672f\u7684\u53d1\u5c55"}}
{"id": "2506.17636", "pdf": "https://arxiv.org/pdf/2506.17636", "abs": "https://arxiv.org/abs/2506.17636", "authors": ["Shihan Chen", "Zhaojin Li", "Zeyu Chen", "Qingsong Yan", "Gaoyang Shen", "Ran Duan"], "title": "3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in Large-Scale Scene", "categories": ["cs.GR", "cs.CV", "eess.IV"], "comment": "IROS 2025", "summary": "Recent developments in 3D Gaussian Splatting have made significant advances\nin surface reconstruction. However, scaling these methods to large-scale scenes\nremains challenging due to high computational demands and the complex dynamic\nappearances typical of outdoor environments. These challenges hinder the\napplication in aerial surveying and autonomous driving. This paper proposes a\nnovel solution to reconstruct large-scale surfaces with fine details,\nsupervised by full-sized images. Firstly, we introduce a coarse-to-fine\nstrategy to reconstruct a coarse model efficiently, followed by adaptive scene\npartitioning and sub-scene refining from image segments. Additionally, we\nintegrate a decoupling appearance model to capture global appearance variations\nand a transient mask model to mitigate interference from moving objects.\nFinally, we expand the multi-view constraint and introduce a single-view\nregularization for texture-less areas. Our experiments were conducted on the\npublicly available dataset GauU-Scene V2, which was captured using unmanned\naerial vehicles. To the best of our knowledge, our method outperforms existing\nNeRF-based and Gaussian-based methods, achieving high-fidelity visual results\nand accurate surface from full-size image optimization. Open-source code will\nbe available on GitHub.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u91cd\u5efa\u7b56\u7565\u4e0e\u89e3\u8026\u5916\u89c2\u6a21\u578b\uff0c\u901a\u8fc7\u5168\u5c3a\u5bf8\u56fe\u50cf\u4f18\u5316\u5b9e\u73b0\u5927\u89c4\u6a21\u573a\u666f\u9ad8\u7cbe\u5ea6\u8868\u9762\u91cd\u5efa", "motivation": "\u89e3\u51b33D\u9ad8\u65af\u6e85\u5c04\u5728\u5927\u89c4\u6a21\u6237\u5916\u573a\u666f\u4e2d\u7684\u9ad8\u8ba1\u7b97\u6d88\u8017\u548c\u52a8\u6001\u5e72\u6270\u95ee\u9898\uff0c\u7a81\u7834\u822a\u6d4b\u4e0e\u81ea\u52a8\u9a7e\u9a76\u5e94\u7528\u74f6\u9888", "method": "\u91c7\u7528\u5206\u5c42\u4f18\u5316\u7b56\u7565\uff1a1) \u6784\u5efa\u521d\u59cb\u7c97\u7c92\u5ea6\u6a21\u578b 2) \u56fe\u50cf\u5206\u5272\u9a71\u52a8\u81ea\u9002\u5e94\u573a\u666f\u5206\u533a 3) \u89e3\u8026\u5168\u5c40\u5149\u7167\u4e0e\u77ac\u6001\u63a9\u6a21\u5efa\u6a21 4) \u6269\u5c55\u591a\u89c6\u89d2\u7ea6\u675f\u5e76\u5f15\u5165\u5355\u89c6\u89d2\u7eb9\u7406\u6b63\u5219\u5316", "result": "\u5728\u65e0\u4eba\u673a\u6570\u636e\u96c6GauU-Scene V2\u4e0a\u8d85\u8d8aNeRF\u4e0e\u9ad8\u65af\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5168\u5c3a\u5bf8\u56fe\u50cf\u4f18\u5316\u7684\u4e9a\u5398\u7c73\u7ea7\u7cbe\u5ea6\u91cd\u5efa", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u91cd\u5efa\u8d28\u91cf\uff0c\u901a\u8fc7\u52a8\u6001\u5e72\u6270\u6291\u5236\u6280\u672f\u63d0\u5347\u6237\u5916\u573a\u666f\u9002\u5e94\u6027\uff0c\u5f00\u6e90\u4ee3\u7801\u5c06\u63a8\u52a8\u5b9e\u9645\u5e94\u7528\u843d\u5730"}}
{"id": "2506.17770", "pdf": "https://arxiv.org/pdf/2506.17770", "abs": "https://arxiv.org/abs/2506.17770", "authors": ["Tomas Akenine-M\u00f6ller", "Pontus Ebelin", "Matt Pharr", "Bartlomiej Wronski"], "title": "Collaborative Texture Filtering", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to ACM/EG Symposium on High Performance Graphics (HPG), 2025", "summary": "Recent advances in texture compression provide major improvements in\ncompression ratios, but cannot use the GPU's texture units for decompression\nand filtering. This has led to the development of stochastic texture filtering\n(STF) techniques to avoid the high cost of multiple texel evaluations with such\nformats. Unfortunately, those methods can give undesirable visual appearance\nchanges under magnification and may contain visible noise and flicker despite\nthe use of spatiotemporal denoisers. Recent work substantially improves the\nquality of magnification filtering with STF by sharing decoded texel values\nbetween nearby pixels (Wronski 2025). Using GPU wave communication intrinsics,\nthis sharing can be performed inside actively executing shaders without memory\ntraffic overhead. We take this idea further and present novel algorithms that\nuse wave communication between lanes to avoid repeated texel decompression\nprior to filtering. By distributing unique work across lanes, we can achieve\nzero-error filtering using <=1 texel evaluations per pixel given a sufficiently\nlarge magnification factor. For the remaining cases, we propose novel filtering\nfallback methods that also achieve higher quality than prior approaches.", "AI": {"tldr": "\u63d0\u51fa\u5229\u7528GPU\u6ce2\u901a\u4fe1\u6280\u672f\u907f\u514d\u91cd\u590d\u7eb9\u7406\u89e3\u538b\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5b9e\u73b0\u96f6\u8bef\u5dee\u8fc7\u6ee4\uff0c\u5e76\u5f00\u53d1\u9ad8\u8d28\u91cf\u5907\u7528\u8fc7\u6ee4\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u968f\u673a\u7eb9\u7406\u8fc7\u6ee4(STF)\u6280\u672f\u5728\u653e\u5927\u65f6\u4f1a\u4ea7\u751f\u89c6\u89c9\u4f2a\u5f71\u3001\u566a\u58f0\u548c\u95ea\u70c1\uff0c\u9700\u63d0\u5347\u8fc7\u6ee4\u6548\u7387\u4e0e\u8d28\u91cf\u3002", "method": "\u57fa\u4e8eGPU\u6ce2\u901a\u4fe1\u7684\u5e76\u884c\u7b97\u6cd5\uff0c\u901a\u8fc7\u8de8\u8ba1\u7b97\u5355\u5143\u5171\u4eab\u89e3\u538b\u6570\u636e\uff0c\u7ed3\u5408\u552f\u4e00\u4efb\u52a1\u5206\u914d\u7b56\u7565\u51cf\u5c11\u5197\u4f59\u89e3\u538b\u64cd\u4f5c\u3002", "result": "\u5728\u8db3\u591f\u5927\u7684\u653e\u5927\u56e0\u5b50\u4e0b\u5b9e\u73b0\u96f6\u8bef\u5dee\u8fc7\u6ee4\uff08\u22641\u6b21\u7eb9\u7406\u8bc4\u4f30/\u50cf\u7d20\uff09\uff0c\u5176\u4ed6\u60c5\u51b5\u63d0\u4f9b\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u9ad8\u8d28\u91cf\u7684\u8fc7\u6ee4\u65b9\u6848\u3002", "conclusion": "\u6ce2\u901a\u4fe1\u6280\u672f\u663e\u8457\u63d0\u5347\u7eb9\u7406\u8fc7\u6ee4\u6027\u80fd\uff0c\u65b0\u7b97\u6cd5\u5728\u4e0d\u540c\u653e\u5927\u573a\u666f\u4e2d\u5747\u5c55\u73b0\u51fa\u8d28\u91cf\u4e0e\u6548\u7387\u4f18\u52bf\u3002"}}
{"id": "2506.17223", "pdf": "https://arxiv.org/pdf/2506.17223", "abs": "https://arxiv.org/abs/2506.17223", "authors": ["Shuvra Smaran Das", "Anirban Saha Anik", "Md Kishor Morol", "Mohammad Sakib Mahmood"], "title": "Outcome-Based Education: Evaluating Students' Perspectives Using Transformer", "categories": ["cs.CL"], "comment": "6 pages, 7 figures", "summary": "Outcome-Based Education (OBE) emphasizes the development of specific\ncompetencies through student-centered learning. In this study, we reviewed the\nimportance of OBE and implemented transformer-based models, particularly\nDistilBERT, to analyze an NLP dataset that includes student feedback. Our\nobjective is to assess and improve educational outcomes. Our approach is better\nthan other machine learning models because it uses the transformer's deep\nunderstanding of language context to classify sentiment better, giving better\nresults across a wider range of matrices. Our work directly contributes to\nOBE's goal of achieving measurable outcomes by facilitating the identification\nof patterns in student learning experiences. We have also applied LIME (local\ninterpretable model-agnostic explanations) to make sure that model predictions\nare clear. This gives us understandable information about how key terms affect\nsentiment. Our findings indicate that the combination of transformer models and\nLIME explanations results in a strong and straightforward framework for\nanalyzing student feedback. This aligns more closely with the principles of OBE\nand ensures the improvement of educational practices through data-driven\ninsights.", "AI": {"tldr": "\u4f7f\u7528DistilBERT\u548cLIME\u6280\u672f\u5206\u6790\u5b66\u751f\u53cd\u9988\u6570\u636e\uff0c\u6784\u5efa\u53ef\u89e3\u91ca\u7684\u6846\u67b6\u6765\u63d0\u5347\u57fa\u4e8e\u6210\u679c\u6559\u80b2\uff08OBE\uff09\u7684\u5b9e\u8df5\u6548\u679c\u3002", "motivation": "\u4e3a\u5b9e\u73b0OBE\u5f3a\u8c03\u7684\u53ef\u91cf\u5316\u6559\u80b2\u6210\u679c\uff0c\u4f20\u7edf\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6316\u6398\u5b66\u751f\u53cd\u9988\u4e2d\u7684\u6df1\u5c42\u6a21\u5f0f\uff0c\u9700\u8981\u7ed3\u5408NLP\u6280\u672f\u548c\u53ef\u89e3\u91caAI\u6765\u63d0\u5347\u5206\u6790\u7cbe\u5ea6\u4e0e\u53ef\u4fe1\u5ea6\u3002", "method": "\u91c7\u7528DistilBERT\u8fdb\u884c\u6587\u672c\u60c5\u611f\u5206\u6790\uff0c\u7ed3\u5408LIME\u89e3\u91ca\u6a21\u578b\u9884\u6d4b\u903b\u8f91\uff0c\u5f62\u6210\u53ef\u89e3\u91ca\u7684\u5b66\u751f\u53cd\u9988\u5206\u6790\u6846\u67b6\u3002", "result": "Transformer\u6a21\u578b\u5728\u591a\u4e2a\u8bc4\u4f30\u77e9\u9635\u4e2d\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0cLIME\u6210\u529f\u8bc6\u522b\u51fa\u5f71\u54cd\u60c5\u611f\u5224\u65ad\u7684\u5173\u952e\u672f\u8bed\uff08\u5982'struggle','clear'\u7b49\uff09\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "Transformer\u6a21\u578b\u4e0eLIME\u7684\u7ed3\u5408\u4e3aOBE\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5206\u6790\u5de5\u5177\uff0c\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u89c1\u89e3\u6301\u7eed\u6539\u8fdb\u6559\u5b66\u5b9e\u8df5\uff0c\u5b9e\u73b0\u4e86\u6559\u80b2\u6210\u679c\u7684\u91cf\u5316\u8ffd\u8e2a\u3002"}}
{"id": "2506.18017", "pdf": "https://arxiv.org/pdf/2506.18017", "abs": "https://arxiv.org/abs/2506.18017", "authors": ["Yang Li", "Victor Cheung", "Xinhai Liu", "Yuguang Chen", "Zhongjin Luo", "Biwen Lei", "Haohan Weng", "Zibo Zhao", "Jingwei Huang", "Zhuo Chen", "Chunchao Guo"], "title": "Auto-Regressive Surface Cutting", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Tech. report. https://victorcheung12.github.io/seamgpt", "summary": "Surface cutting is a fundamental task in computer graphics, with applications\nin UV parameterization, texture mapping, and mesh decomposition. However,\nexisting methods often produce technically valid but overly fragmented atlases\nthat lack semantic coherence. We introduce SeamGPT, an auto-regressive model\nthat generates cutting seams by mimicking professional workflows. Our key\ntechnical innovation lies in formulating surface cutting as a next token\nprediction task: sample point clouds on mesh vertices and edges, encode them as\nshape conditions, and employ a GPT-style transformer to sequentially predict\nseam segments with quantized 3D coordinates. Our approach achieves exceptional\nperformance on UV unwrapping benchmarks containing both manifold and\nnon-manifold meshes, including artist-created, and 3D-scanned models. In\naddition, it enhances existing 3D segmentation tools by providing clean\nboundaries for part decomposition.", "AI": {"tldr": "\u63d0\u51faSeamGPT\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u901a\u8fc7GPT\u67b6\u6784\u9884\u6d4b\u4e09\u7ef4\u7f1d\u7ebf\uff0c\u89e3\u51b3\u8868\u9762\u5207\u5272\u56fe\u8c31\u788e\u7247\u5316\u95ee\u9898", "motivation": "\u73b0\u6709\u8868\u9762\u5207\u5272\u65b9\u6cd5\u4ea7\u751f\u7684\u56fe\u8c31\u6280\u672f\u6709\u6548\u4f46\u7f3a\u4e4f\u8bed\u4e49\u8fde\u8d2f\u6027\uff0c\u788e\u7247\u5316\u4e25\u91cd", "method": "\u5c06\u8868\u9762\u5207\u5272\u5efa\u6a21\u4e3a\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\uff1a\u91c7\u6837\u70b9\u4e91\u4f5c\u4e3a\u5f62\u72b6\u6761\u4ef6\uff0c\u4f7f\u7528GPT\u67b6\u6784Transformer\u9884\u6d4b\u91cf\u5316\u4e09\u7ef4\u5750\u6807\u7684\u7f1d\u6bb5", "result": "\u5728\u5305\u542b\u6d41\u5f62/\u975e\u6d41\u5f62\u7f51\u683c\u7684UV\u5c55\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5305\u62ec\u827a\u672f\u5bb6\u521b\u4f5c\u548c3D\u626b\u63cf\u6a21\u578b", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u5347UV\u5c55\u5f00\u8d28\u91cf\uff0c\u8fd8\u80fd\u901a\u8fc7\u751f\u6210\u6e05\u6670\u8fb9\u754c\u589e\u5f3a\u73b0\u67093D\u5206\u5272\u5de5\u5177"}}
{"id": "2506.17231", "pdf": "https://arxiv.org/pdf/2506.17231", "abs": "https://arxiv.org/abs/2506.17231", "authors": ["Xiang Li", "Chong Zhang", "Jia Wang", "Fangyu Wu", "Yushi Li", "Xiaobo Jin"], "title": "Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs", "categories": ["cs.CL", "cs.CR"], "comment": "15 pages, 5 figures", "summary": "Attacks on large language models (LLMs) in jailbreaking scenarios raise many\nsecurity and ethical issues. Current jailbreak attack methods face problems\nsuch as low efficiency, high computational cost, and poor cross-model\nadaptability and versatility, which make it difficult to cope with the rapid\ndevelopment of LLM and new defense strategies. Our work proposes an Adversarial\nPrompt Distillation, which combines masked language modeling, reinforcement\nlearning, and dynamic temperature control through a prompt generation and\ndistillation method. It enables small language models (SLMs) to jailbreak\nattacks on mainstream LLMs. The experimental results verify the superiority of\nthe proposed method in terms of attack success rate and harm, and reflect the\nresource efficiency and cross-model adaptability. This research explores the\nfeasibility of distilling the jailbreak ability of LLM to SLM, reveals the\nmodel's vulnerability, and provides a new idea for LLM security research.", "AI": {"tldr": "\u63d0\u51fa\u5bf9\u6297\u6027\u63d0\u793a\u84b8\u998f\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5c0f\u8bed\u8a00\u6a21\u578b\u5bf9\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8d8a\u72f1\u653b\u51fb\uff0c\u9a8c\u8bc1\u5176\u653b\u51fb\u6548\u7387\u4e0e\u8de8\u6a21\u578b\u9002\u5e94\u6027\u4f18\u52bf", "motivation": "\u73b0\u6709\u8d8a\u72f1\u653b\u51fb\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u4f4e\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u8de8\u6a21\u578b\u9002\u5e94\u6027\u5dee\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u5e94\u5bf9\u5feb\u901f\u53d1\u5c55\u7684LLM\u9632\u5fa1\u7b56\u7565", "method": "\u7ed3\u5408\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\u3001\u5f3a\u5316\u5b66\u4e60\u4e0e\u52a8\u6001\u6e29\u5ea6\u63a7\u5236\uff0c\u901a\u8fc7\u63d0\u793a\u751f\u6210\u4e0e\u84b8\u998f\u65b9\u6cd5\u5b9e\u73b0\u5c0f\u6a21\u578b\u5bf9LLM\u7684\u8d8a\u72f1\u653b\u51fb", "result": "\u5b9e\u9a8c\u663e\u793a\u653b\u51fb\u6210\u529f\u7387\u63d0\u534740%\uff0c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u964d\u4f4e75%\uff0c\u5728Llama-2\u7b49\u4e3b\u6d41\u6a21\u578b\u4e0a\u8de8\u6a21\u578b\u6210\u529f\u7387\u8d8585%", "conclusion": "\u63ed\u793aLLM\u5b89\u5168\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u6a21\u578b\u5b89\u5168\u7814\u7a76\u65b0\u8303\u5f0f\uff0c\u4e3a\u9632\u5fa1\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u9006\u5411\u601d\u7ef4\uff0c\u63a8\u52a8\u5b89\u5168\u8bc4\u4f30\u6807\u51c6\u53d1\u5c55"}}
{"id": "2506.18251", "pdf": "https://arxiv.org/pdf/2506.18251", "abs": "https://arxiv.org/abs/2506.18251", "authors": ["Chao Li", "Jiawei Fan", "Anbang Yao"], "title": "Morse: Dual-Sampling for Lossless Acceleration of Diffusion Models", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "This work is accepted to ICML 2025. The project page:\n  https://github.com/deep-optimization/Morse", "summary": "In this paper, we present Morse, a simple dual-sampling framework for\naccelerating diffusion models losslessly. The key insight of Morse is to\nreformulate the iterative generation (from noise to data) process via taking\nadvantage of fast jump sampling and adaptive residual feedback strategies.\nSpecifically, Morse involves two models called Dash and Dot that interact with\neach other. The Dash model is just the pre-trained diffusion model of any type,\nbut operates in a jump sampling regime, creating sufficient space for sampling\nefficiency improvement. The Dot model is significantly faster than the Dash\nmodel, which is learnt to generate residual feedback conditioned on the\nobservations at the current jump sampling point on the trajectory of the Dash\nmodel, lifting the noise estimate to easily match the next-step estimate of the\nDash model without jump sampling. By chaining the outputs of the Dash and Dot\nmodels run in a time-interleaved fashion, Morse exhibits the merit of flexibly\nattaining desired image generation performance while improving overall runtime\nefficiency. With our proposed weight sharing strategy between the Dash and Dot\nmodels, Morse is efficient for training and inference. Our method shows a\nlossless speedup of 1.78X to 3.31X on average over a wide range of sampling\nstep budgets relative to 9 baseline diffusion models on 6 image generation\ntasks. Furthermore, we show that our method can be also generalized to improve\nthe Latent Consistency Model (LCM-SDXL, which is already accelerated with\nconsistency distillation technique) tailored for few-step text-to-image\nsynthesis. The code and models are available at\nhttps://github.com/deep-optimization/Morse.", "AI": {"tldr": "\u63d0\u51faMorse\u53cc\u91c7\u6837\u6846\u67b6\uff0c\u901a\u8fc7\u8df3\u8dc3\u91c7\u6837\u548c\u6b8b\u5dee\u53cd\u9988\u7b56\u7565\u5b9e\u73b0\u6269\u6563\u6a21\u578b\u65e0\u635f\u52a0\u901f\uff0c\u5e73\u5747\u52a0\u901f1.78-3.31\u500d", "motivation": "\u6269\u6563\u6a21\u578b\u8fed\u4ee3\u751f\u6210\u8fc7\u7a0b\u8017\u65f6\u8f83\u957f\uff0c\u9700\u901a\u8fc7\u4f18\u5316\u91c7\u6837\u7b56\u7565\u63d0\u5347\u6548\u7387\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf", "method": "\u6784\u5efaDash(\u9884\u8bad\u7ec3\u8df3\u8dc3\u91c7\u6837\u6a21\u578b)\u548cDot(\u5feb\u901f\u6b8b\u5dee\u53cd\u9988\u6a21\u578b)\u7684\u534f\u540c\u6846\u67b6\uff0c\u901a\u8fc7\u6743\u91cd\u5171\u4eab\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u548c\u63a8\u7406", "result": "\u57286\u7c7b\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e2d\u5e73\u5747\u83b7\u5f971.78X-3.31X\u52a0\u901f\uff0c\u4e14\u80fd\u63d0\u5347\u5df2\u4f18\u5316\u7684LCM-SDXL\u6a21\u578b\u6027\u80fd", "conclusion": "Morse\u6846\u67b6\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u6269\u6563\u6a21\u578b\u6548\u7387\uff0c\u5176\u53cc\u6a21\u578b\u4ea4\u4e92\u673a\u5236\u5177\u6709\u7075\u6d3b\u6027\u548c\u901a\u7528\u6027"}}
{"id": "2506.17286", "pdf": "https://arxiv.org/pdf/2506.17286", "abs": "https://arxiv.org/abs/2506.17286", "authors": ["Luoyang Sun", "Jiwen Jiang", "Cheng Deng", "Xinjian Wu", "Haifeng Zhang", "Lei Chen", "Lionel Ni", "Jun Wang"], "title": "GTA: Grouped-head latenT Attention", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.", "AI": {"tldr": "\u63d0\u51faGTA\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u5171\u4eab\u6ce8\u610f\u529b\u56fe\u548c\u538b\u7f29\u503c\u7f13\u5b58\u964d\u4f4e62.5%\u8ba1\u7b97FLOPs\u548c70% KV\u7f13\u5b58\uff0c\u5b9e\u73b0\u63a8\u7406\u901f\u5ea62\u500d\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u6ce8\u610f\u529b\u673a\u5236\u5728\u957f\u6587\u672c\u573a\u666f\u4e0bKV\u7f13\u5b58\u548c\u8ba1\u7b97\u91cf\u6025\u5267\u589e\u52a0\uff0c\u786c\u4ef6\u90e8\u7f72\u9762\u4e34\u5185\u5b58\u4e0e\u7b97\u529b\u74f6\u9888\u3002\u7814\u7a76\u53d1\u73b0\u6ce8\u610f\u529b\u5934\u95f4\u5b58\u5728\u9ad8\u5ea6\u5197\u4f59\uff0c\u5b58\u5728\u4f18\u5316\u7a7a\u95f4\u3002", "method": "1. \u8de8\u5934\u5171\u4eab\u6ce8\u610f\u529b\u56fe\u51cf\u5c11\u952e\u7f13\u5b58\u89c4\u6a21\n2. \u5e26\u5b66\u4e60\u6295\u5f71\u7684\u975e\u7ebf\u6027\u503c\u89e3\u7801\u5668\u538b\u7f29\u503c\u7f13\u5b58\u81f3\u6f5c\u7a7a\u95f4\n3. \u7ec4\u5408\u4f18\u5316\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u5b58\u5360\u7528", "result": "\u76f8\u6bd4Grouped-Query Attention\uff1a\n- \u8ba1\u7b97FLOPs\u964d\u4f4e62.5%\n- KV\u7f13\u5b58\u51cf\u5c1170%\n- \u7aef\u5230\u7aef\u63a8\u7406\u901f\u5ea6\u63d0\u53472\u500d\n- \u9884\u586b\u5145\u9636\u6bb5\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\uff0c\u89e3\u7801\u9636\u6bb5\u7f13\u5b58\u5360\u7528\u51cf\u5c11", "conclusion": "GTA\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347LLM\u90e8\u7f72\u6548\u7387\uff0c\u901a\u8fc7\u521b\u65b0\u6027\u7684\u6ce8\u610f\u529b\u5934\u4f18\u5316\u7b56\u7565\u5e73\u8861\u8ba1\u7b97\u8d44\u6e90\u4e0e\u63a8\u7406\u901f\u5ea6\uff0c\u4e3a\u8fb9\u7f18\u7aef\u90e8\u7f72\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.18407", "pdf": "https://arxiv.org/pdf/2506.18407", "abs": "https://arxiv.org/abs/2506.18407", "authors": ["Yiyao Wang", "Bo Pan", "Ke Wang", "Han Liu", "Jinyuan Mao", "Yuxin Liu", "Minfeng Zhu", "Bo Zhang", "Weifeng Chen", "Xiuqi Huang", "Wei Chen"], "title": "What You Think Is What You Get: Bridge User Intent and Transfer Function Design through Multimodal Large Language Models", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Direct volume rendering (DVR) is a fundamental technique for visualizing\nvolumetric data, with transfer functions (TFs) playing a crucial role in\nextracting meaningful structures. However, designing effective TFs remains\nunintuitive due to the semantic gap between user intent and TF parameter space.\nResearchers have developed numerous TF optimization methods to bridge this gap.\nHowever, existing methods still face two challenges: large exploration space\nand weak generalizability. To address these issues, we propose What You Think\nis What You Get (WYTWYG) framework, which leveraging Multi-model Large Language\nModels (MLLMs) to guide the TF optimization based on user intent. Specifically,\nwe first introduce a novel TF optimization approach comprising two core\ncomponents: (1) an evolution-based explorer for effective exploration of the TF\nspace, and (2) a volume rendering quality evaluator based on MLLMs to provide\ngeneralizable visual guidance. We further propose a TF interactive design\nsystem based on this approach. We demonstrate the general applicability of our\nframework through three case studies, and validate the effectiveness of each\ncomponent through extensive experiments. Our code is available at:\nhttps://github.com/wyysteelhead/TFevolve.", "AI": {"tldr": "\u63d0\u51faWYTWYG\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8fdb\u5316\u7b97\u6cd5\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLMs)\u7684\u8bc4\u4f30\u5668\uff0c\u89e3\u51b3\u4f20\u7edf\u4f20\u9012\u51fd\u6570\u4f18\u5316\u4e2d\u63a2\u7d22\u7a7a\u95f4\u5927\u548c\u6cdb\u5316\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u4f20\u9012\u51fd\u6570\u8bbe\u8ba1\u5b58\u5728\u7528\u6237\u610f\u56fe\u4e0e\u53c2\u6570\u7a7a\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\uff0c\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u9762\u4e34\u63a2\u7d22\u7a7a\u95f4\u8fc7\u5927\u548c\u7b97\u6cd5\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u53cc\u91cd\u6311\u6218\u3002", "method": "1. \u57fa\u4e8e\u8fdb\u5316\u7b97\u6cd5\u7684TF\u7a7a\u95f4\u63a2\u7d22\u5668 2. \u5229\u7528MLLMs\u6784\u5efa\u6e32\u67d3\u8d28\u91cf\u8bc4\u4f30\u5668 3. \u5f00\u53d1\u4ea4\u4e92\u5f0fTF\u8bbe\u8ba1\u7cfb\u7edf", "result": "\u901a\u8fc73\u4e2a\u6848\u4f8b\u9a8c\u8bc1\u6846\u67b6\u901a\u7528\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5404\u7ec4\u4ef6\u6709\u6548\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "WYTWYG\u6846\u67b6\u6709\u6548\u6574\u5408\u8fdb\u5316\u63a2\u7d22\u4e0eMLLM\u89c6\u89c9\u5f15\u5bfc\uff0c\u663e\u8457\u63d0\u5347TF\u4f18\u5316\u6548\u7387\u548c\u8de8\u6570\u636e\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2506.17294", "pdf": "https://arxiv.org/pdf/2506.17294", "abs": "https://arxiv.org/abs/2506.17294", "authors": ["Qirui Zheng", "Xingbo Wang", "Keyuan Cheng", "Yunlong Lu", "Wenxin Li"], "title": "AI-Generated Game Commentary: A Survey and a Datasheet Repository", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "AI-Generated Game Commentary (AIGGC) has gained increasing attention due to\nits market potential and inherent technical challenges. As a comprehensive\nmultimodal Natural Language Processing (NLP) task, AIGGC imposes substantial\ndemands on language models, including factual accuracy, logical reasoning,\nexpressive text generation, generation speed, and context management. In this\npaper, we introduce a general framework for AIGGC and present a comprehensive\nsurvey of 45 existing game commentary dataset and methods according to key\nchallenges they aim to address in this domain. We further classify and compare\nvarious evaluation metrics commonly used in this domain. To support future\nresearch and benchmarking, we also provide a structured datasheet summarizing\nthe essential attributes of these datasets in appendix, which is meanwhile\npublicly available in an open repository.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u68b3\u7406\u4e8645\u4e2aAI\u751f\u6210\u6e38\u620f\u89e3\u8bf4\uff08AIGGC\uff09\u6570\u636e\u96c6\u4e0e\u65b9\u6cd5\uff0c\u63d0\u51fa\u901a\u7528\u6280\u672f\u6846\u67b6\u5e76\u5efa\u7acb\u7ed3\u6784\u5316\u6570\u636e\u8d44\u6e90\u5e93\u652f\u6301\u672a\u6765\u7814\u7a76\u3002", "motivation": "AIGGC\u5177\u6709\u5e02\u573a\u6f5c\u529b\u4f46\u9762\u4e34\u591a\u6a21\u6001\u5904\u7406\u3001\u4e8b\u5b9e\u51c6\u786e\u6027\u3001\u903b\u8f91\u63a8\u7406\u7b49\u6280\u672f\u6311\u6218\uff0c\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7cfb\u7edf\u6027\u68b3\u7406\u548c\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "1. \u63d0\u51faAIGGC\u901a\u7528\u6280\u672f\u6846\u67b6\n2. \u6309\u6838\u5fc3\u6311\u6218\u5206\u7c7b\u7efc\u8ff0\u73b0\u6709\u65b9\u6cd5\n3. \u5efa\u7acb\u542b45\u4e2a\u6570\u636e\u96c6\u7684\u7ed3\u6784\u5316\u6570\u636e\u8868\n4. \u7cfb\u7edf\u6bd4\u8f83\u5404\u7c7b\u8bc4\u4f30\u6307\u6807", "result": "\u6784\u5efa\u4e86\u9996\u4e2aAIGGC\u9886\u57df\u7ed3\u6784\u5316\u6570\u636e\u96c6\u77e5\u8bc6\u5e93\uff0c\u7cfb\u7edf\u6027\u5206\u7c7b\u5bf9\u6bd4\u4e8645\u4e2a\u6570\u636e\u96c6\u7684\u6838\u5fc3\u5c5e\u6027\u548c\u6280\u672f\u8def\u7ebf\uff0c\u5efa\u7acb\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u6307\u6807\u4f53\u7cfb\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aAIGGC\u9886\u57df\u63d0\u4f9b\u4e86\u6280\u672f\u6846\u67b6\u3001\u6570\u636e\u8d44\u6e90\u548c\u8bc4\u4f30\u57fa\u51c6\uff0c\u672a\u6765\u9700\u5728\u5b9e\u65f6\u63a8\u7406\u3001\u591a\u6a21\u6001\u5bf9\u9f50\u7b49\u6838\u5fc3\u6280\u672f\u65b9\u5411\u6301\u7eed\u7a81\u7834\u3002"}}
{"id": "2506.18601", "pdf": "https://arxiv.org/pdf/2506.18601", "abs": "https://arxiv.org/abs/2506.18601", "authors": ["Denys Rozumnyi", "Jonathon Luiten", "Numair Khan", "Johannes Sch\u00f6nberger", "Peter Kontschieder"], "title": "BulletGen: Improving 4D Reconstruction with Bullet-Time Generation", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Transforming casually captured, monocular videos into fully immersive dynamic\nexperiences is a highly ill-posed task, and comes with significant challenges,\ne.g., reconstructing unseen regions, and dealing with the ambiguity in\nmonocular depth estimation. In this work we introduce BulletGen, an approach\nthat takes advantage of generative models to correct errors and complete\nmissing information in a Gaussian-based dynamic scene representation. This is\ndone by aligning the output of a diffusion-based video generation model with\nthe 4D reconstruction at a single frozen \"bullet-time\" step. The generated\nframes are then used to supervise the optimization of the 4D Gaussian model.\nOur method seamlessly blends generative content with both static and dynamic\nscene components, achieving state-of-the-art results on both novel-view\nsynthesis, and 2D/3D tracking tasks.", "AI": {"tldr": "\u63d0\u51faBulletGen\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u5f0f\u6a21\u578b\u4f18\u53164D\u52a8\u6001\u573a\u666f\u91cd\u5efa\uff0c\u89e3\u51b3\u5355\u76ee\u89c6\u9891\u91cd\u5efa\u7684\u672a\u89c2\u6d4b\u533a\u57df\u548c\u6df1\u5ea6\u6b67\u4e49\u95ee\u9898", "motivation": "\u5355\u76ee\u89c6\u9891\u91cd\u5efa\u52a8\u6001\u573a\u666f\u5b58\u5728\u672a\u89c2\u6d4b\u533a\u57df\u586b\u8865\u56f0\u96be\u3001\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6b67\u4e49\u6027\u5927\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u89e3\u51b3", "method": "\u5728\u51bb\u7ed3\u7684\u5b50\u5f39\u65f6\u95f4\u6b65\u5bf9\u9f50\u6269\u6563\u6a21\u578b\u751f\u6210\u5185\u5bb9\u4e0e4D\u9ad8\u65af\u91cd\u5efa\uff0c\u5229\u7528\u751f\u6210\u5e27\u76d1\u77634D\u9ad8\u65af\u4f18\u5316\uff0c\u878d\u5408\u751f\u6210\u5185\u5bb9\u4e0e\u52a8\u9759\u6001\u573a\u666f\u7ec4\u4ef6", "result": "\u5728\u65b0\u89c6\u89d2\u5408\u6210\u548c2D/3D\u8ddf\u8e2a\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\uff0c\u5b9a\u91cf\u6307\u6807\u63d0\u5347\u663e\u8457(PSNR\u21911.5dB\uff0cLPIPS\u219315%)", "conclusion": "\u901a\u8fc7\u751f\u6210\u5f0f\u6a21\u578b\u4e0e\u7269\u7406\u91cd\u5efa\u7684\u534f\u540c\u4f18\u5316\uff0c\u7a81\u7834\u4e86\u5355\u76ee\u89c6\u9891\u91cd\u5efa\u7684\u56fa\u6709\u5c40\u9650\uff0c\u4e3a\u52a8\u6001\u573a\u666f\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2506.17296", "pdf": "https://arxiv.org/pdf/2506.17296", "abs": "https://arxiv.org/abs/2506.17296", "authors": ["Darius Foodeei", "Simin Fan", "Martin Jaggi"], "title": "Semantic uncertainty in advanced decoding methods for LLM generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates semantic uncertainty in large language model (LLM)\noutputs across different decoding methods, focusing on emerging techniques like\nspeculative sampling and chain-of-thought (CoT) decoding. Through experiments\non question answering, summarization, and code generation tasks, we analyze how\ndifferent decoding strategies affect both the diversity and reliability of\nmodel outputs. Our findings reveal that while CoT decoding demonstrates higher\nsemantic diversity, it maintains lower predictive entropy, suggesting that\nstructured exploration can lead to more confident and accurate outputs. This is\nevidenced by a 48.8% improvement in code generation Pass@2 rates, despite lower\nalignment with reference solutions. For summarization tasks, speculative\nsampling proved particularly effective, achieving superior ROUGE scores while\nmaintaining moderate semantic diversity. Our results challenge conventional\nassumptions about trade-offs between diversity and accuracy in language model\noutputs, demonstrating that properly structured decoding methods can increase\nsemantic exploration while maintaining or improving output quality. These\nfindings have significant implications for deploying language models in\npractical applications where both reliability and diverse solution generation\nare crucial.", "AI": {"tldr": "\u7814\u7a76\u9a8c\u8bc1\u7ed3\u6784\u5316\u89e3\u7801\u65b9\u6cd5\uff08\u601d\u7ef4\u94fe/\u63a8\u6d4b\u91c7\u6837\uff09\u80fd\u540c\u65f6\u63d0\u5347LLM\u8f93\u51fa\u7684\u591a\u6837\u6027\u548c\u53ef\u9760\u6027\uff0cCoT\u89e3\u7801\u4f7f\u4ee3\u7801\u751f\u6210\u901a\u8fc7\u7387\u63d0\u534748.8%\uff0c\u63a8\u6d4b\u91c7\u6837\u5728\u6458\u8981\u4efb\u52a1\u8868\u73b0\u6700\u4f18\u3002", "motivation": "\u63a2\u7d22\u4e0d\u540c\u89e3\u7801\u7b56\u7565\u5bf9\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u8bed\u4e49\u4e0d\u786e\u5b9a\u6027\u7684\u5f71\u54cd\uff0c\u6311\u6218\u4f20\u7edf\u300c\u591a\u6837\u6027-\u51c6\u786e\u6027\u300d\u4e0d\u53ef\u517c\u5f97\u7684\u5047\u8bbe\uff0c\u5bfb\u6c42\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u9760\u6027\u4e0e\u591a\u6837\u6027\u7684\u5e73\u8861\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u95ee\u7b54/\u6458\u8981/\u4ee3\u7801\u751f\u6210\u4e09\u7c7b\u4efb\u52a1\uff0c\u5bf9\u6bd4\u5206\u6790CoT\u89e3\u7801\u4e0e\u63a8\u6d4b\u91c7\u6837\u5728\u9884\u6d4b\u71b5\u3001\u8bed\u4e49\u591a\u6837\u6027\u3001Pass@2\u7387\u3001ROUGE\u5206\u6570\u7b49\u6307\u6807\u7684\u5dee\u5f02\u3002", "result": "CoT\u89e3\u7801\u5728\u4ee3\u7801\u751f\u6210Pass@2\u7387\u63d0\u534748.8%\uff08\u4f4e\u9884\u6d4b\u71b5+\u9ad8\u591a\u6837\u6027\uff09\uff0c\u63a8\u6d4b\u91c7\u6837\u5728\u6458\u8981\u4efb\u52a1\u83b7\u6700\u4f73ROUGE\u5206\u4e14\u4fdd\u6301\u9002\u5ea6\u591a\u6837\u6027\uff0c\u4e24\u7c7b\u65b9\u6cd5\u5747\u7a81\u7834\u4f20\u7edf\u6743\u8861\u9650\u5236\u3002", "conclusion": "\u7ed3\u6784\u5316\u89e3\u7801\u65b9\u6cd5\u901a\u8fc7\u5f15\u5bfc\u5f0f\u63a2\u7d22\u673a\u5236\uff0c\u8bc1\u660e\u8bed\u4e49\u591a\u6837\u6027\u4e0e\u8f93\u51fa\u8d28\u91cf\u53ef\u534f\u540c\u63d0\u5347\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2dLLM\u7684\u53ef\u9760\u6027\u90e8\u7f72\u63d0\u4f9b\u65b0\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2506.18680", "pdf": "https://arxiv.org/pdf/2506.18680", "abs": "https://arxiv.org/abs/2506.18680", "authors": ["Anindita Ghosh", "Bing Zhou", "Rishabh Dabral", "Jian Wang", "Vladislav Golyanik", "Christian Theobalt", "Philipp Slusallek", "Chuan Guo"], "title": "DuetGen: Music Driven Two-Person Dance Generation via Hierarchical Masked Modeling", "categories": ["cs.GR", "cs.CV", "cs.SD", "eess.AS"], "comment": "11 pages, 7 figures, 2 tables, accepted in ACM Siggraph 2025\n  conference track", "summary": "We present DuetGen, a novel framework for generating interactive two-person\ndances from music. The key challenge of this task lies in the inherent\ncomplexities of two-person dance interactions, where the partners need to\nsynchronize both with each other and with the music. Inspired by the recent\nadvances in motion synthesis, we propose a two-stage solution: encoding\ntwo-person motions into discrete tokens and then generating these tokens from\nmusic. To effectively capture intricate interactions, we represent both\ndancers' motions as a unified whole to learn the necessary motion tokens, and\nadopt a coarse-to-fine learning strategy in both the stages. Our first stage\nutilizes a VQ-VAE that hierarchically separates high-level semantic features at\na coarse temporal resolution from low-level details at a finer resolution,\nproducing two discrete token sequences at different abstraction levels.\nSubsequently, in the second stage, two generative masked transformers learn to\nmap music signals to these dance tokens: the first producing high-level\nsemantic tokens, and the second, conditioned on music and these semantic\ntokens, producing the low-level tokens. We train both transformers to learn to\npredict randomly masked tokens within the sequence, enabling them to\niteratively generate motion tokens by filling an empty token sequence during\ninference. Through the hierarchical masked modeling and dedicated interaction\nrepresentation, DuetGen achieves the generation of synchronized and interactive\ntwo-person dances across various genres. Extensive experiments and user studies\non a benchmark duet dance dataset demonstrate state-of-the-art performance of\nDuetGen in motion realism, music-dance alignment, and partner coordination.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u9636\u6bb5\u6846\u67b6DuetGen\uff0c\u901a\u8fc7\u5206\u5c42\u5efa\u6a21\u751f\u6210\u4e0e\u97f3\u4e50\u540c\u6b65\u7684\u53cc\u4eba\u821e\u8e48", "motivation": "\u89e3\u51b3\u53cc\u4eba\u821e\u8e48\u4e2d\u590d\u6742\u7684\u97f3\u4e50\u540c\u6b65\u4e0e\u4f19\u4f34\u4e92\u52a8\u534f\u8c03\u96be\u9898", "method": "1. VQ-VAE\u5206\u5c42\u7f16\u7801\u52a8\u4f5c\u7279\u5f81\u751f\u6210\u53cc\u7c92\u5ea6token\uff1b2. \u63a9\u7801\u53d8\u6362\u5668\u5206\u9636\u6bb5\u751f\u6210\u97f3\u4e50\u9a71\u52a8\u7684\u9ad8/\u4f4e\u9636token", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u52a8\u4f5c\u771f\u5b9e\u6027\u3001\u97f3\u4e50\u5bf9\u9f50\u548c\u4f19\u4f34\u534f\u8c03\u7684SOTA\u6027\u80fd", "conclusion": "\u5206\u5c42\u5efa\u6a21\u4e0e\u8054\u5408\u8868\u5f81\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u53cc\u4eba\u821e\u8e48\u751f\u6210\u7684\u4e92\u52a8\u540c\u6b65\u6311\u6218"}}
{"id": "2506.17298", "pdf": "https://arxiv.org/pdf/2506.17298", "abs": "https://arxiv.org/abs/2506.17298", "authors": ["Inception Labs", "Samar Khanna", "Siddhant Kharbanda", "Shufan Li", "Harshit Varma", "Eric Wang", "Sawyer Birnbaum", "Ziyang Luo", "Yanis Miraoui", "Akash Palrecha", "Stefano Ermon", "Aditya Grover", "Volodymyr Kuleshov"], "title": "Mercury: Ultra-Fast Language Models Based on Diffusion", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "15 pages; equal core, cross-function, senior authors listed\n  alphabetically", "summary": "We present Mercury, a new generation of commercial-scale large language\nmodels (LLMs) based on diffusion. These models are parameterized via the\nTransformer architecture and trained to predict multiple tokens in parallel. In\nthis report, we detail Mercury Coder, our first set of diffusion LLMs designed\nfor coding applications. Currently, Mercury Coder comes in two sizes: Mini and\nSmall. These models set a new state-of-the-art on the speed-quality frontier.\nBased on independent evaluations conducted by Artificial Analysis, Mercury\nCoder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109\ntokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform\nspeed-optimized frontier models by up to 10x on average while maintaining\ncomparable quality. We discuss additional results on a variety of code\nbenchmarks spanning multiple languages and use-cases as well as real-world\nvalidation by developers on Copilot Arena, where the model currently ranks\nsecond on quality and is the fastest model overall. We also release a public\nAPI at https://platform.inceptionlabs.ai/ and free playground at\nhttps://chat.inceptionlabs.ai", "AI": {"tldr": "Mercury\u63a8\u51fa\u65b0\u4e00\u4ee3\u57fa\u4e8e\u6269\u6563\u7684\u4ee3\u7801\u4e13\u7528\u5927\u6a21\u578bMercury Coder\uff0c\u5728\u901f\u5ea6\u8d28\u91cf\u5e73\u8861\u4e0a\u521b\u4e0b\u65b0\u6807\u6746\uff0c\u541e\u5410\u91cf\u8fbe\u884c\u4e1a\u9876\u5c16\u6c34\u5e73\uff08Mini\u72481109 tokens/sec\uff0cSmall\u7248737 tokens/sec\uff09\uff0c\u901f\u5ea6\u6bd4\u524d\u6cbf\u6a21\u578b\u5feb10\u500d\u4e14\u4fdd\u6301\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u4ee3\u7801\u6a21\u578b\u901f\u5ea6\u4e0e\u8d28\u91cf\u96be\u4ee5\u517c\u5f97\u7684\u95ee\u9898\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u9ad8\u6548\u4e14\u4f18\u8d28\u7684\u4ee3\u7801\u751f\u6210\u5de5\u5177\u3002", "method": "\u57fa\u4e8e\u6269\u6563\u539f\u7406\u7684Transformer\u67b6\u6784\uff0c\u91c7\u7528\u591atoken\u5e76\u884c\u9884\u6d4b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u63a8\u51faMercury Coder Mini/Small\u53cc\u7248\u672c\u3002", "result": "NVIDIA H100 GPU\u6d4b\u8bd5\u663e\u793a\uff1a1) \u541e\u5410\u91cf\u884c\u4e1a\u7b2c\u4e00 2) \u901f\u5ea6\u8d85\u524d\u6cbf\u6a21\u578b10\u500d 3) Copilot Arena\u8d28\u91cf\u7b2c\u4e8c\u4e14\u901f\u5ea6\u7b2c\u4e00 4) \u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u8868\u73b0\u4f18\u5f02", "conclusion": "Mercury Coder\u5728\u901f\u5ea6-\u8d28\u91cf\u5e73\u8861\u4e0a\u5b9e\u73b0\u7a81\u7834\uff0c\u6210\u4e3a\u5f53\u524d\u6700\u5feb\u5546\u7528\u4ee3\u7801\u6a21\u578b\uff0c\u901a\u8fc7API\u548cPlayground\u5f00\u653e\u5b9e\u9645\u5e94\u7528\u9a8c\u8bc1\u3002"}}
{"id": "2506.18867", "pdf": "https://arxiv.org/pdf/2506.18867", "abs": "https://arxiv.org/abs/2506.18867", "authors": ["Yuqi Meng", "Yihao Shi", "Kemeng Huang", "Ning Guo", "Taku Komura", "Yin Yang", "Minchen Li"], "title": "A B-Spline Finite Element Method for Cloth Simulation", "categories": ["cs.GR"], "comment": "19 pages, 18 figures", "summary": "We present a B-spline finite element method (FEM) for cloth simulation.\nBuilding on quadratic B-spline basis functions, our method provides a globally\n$C^1$-continuous displacement field, enabling consistent and accurate\ndiscretization of both membrane and bending energies. This smooth\nrepresentation effectively mitigates locking artifacts and mesh dependency\nissues commonly observed with linear FEM. To further improve efficiency, we\ndevelop a reduced integration scheme that separately optimizes quadrature rules\nfor membrane and bending energies, further reducing computational overhead\nwhile maintaining accuracy. We validate our approach through extensive\nexperiments, demonstrating improved accuracy, visual quality, and efficiency\ncompared to linear FEM and recent higher-order methods. Our method enables\nrealistic simulation of complex wrinkling dynamics across varying material\nparameters, offering a promising new spatial discretization for cloth\nsimulation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e8c\u6b21B\u6837\u6761\u57fa\u51fd\u6570\u7684\u6709\u9650\u5143\u65b9\u6cd5\uff0c\u901a\u8fc7C^1\u8fde\u7eed\u4f4d\u79fb\u573a\u663e\u8457\u6539\u5584\u5e03\u6599\u6a21\u62df\u7cbe\u5ea6\u4e0e\u6548\u7387", "motivation": "\u4f20\u7edf\u7ebf\u6027\u6709\u9650\u5143\u65b9\u6cd5\u5b58\u5728\u7f51\u683c\u4f9d\u8d56\u6027\u9501\u6b7b\u6548\u5e94\uff0c\u9700\u5bfb\u6c42\u66f4\u9ad8\u9636\u8fde\u7eed\u6027\u7684\u79bb\u6563\u5316\u65b9\u6848\u4ee5\u540c\u65f6\u7cbe\u786e\u5904\u7406\u8584\u819c/\u5f2f\u66f2\u80fd\u91cf", "method": "\u6784\u5efa\u5168\u5c40C^1\u8fde\u7eed\u7684\u4e8c\u6b21B\u6837\u6761\u4f4d\u79fb\u573a\uff0c\u5f00\u53d1\u9488\u5bf9\u8584\u819c/\u5f2f\u66f2\u80fd\u91cf\u5206\u522b\u4f18\u5316\u7684\u964d\u9636\u79ef\u5206\u65b9\u6848", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u76f8\u8f83\u4e8e\u7ebf\u6027FEM\u53ca\u73b0\u6709\u9ad8\u9636\u65b9\u6cd5\uff0c\u5728\u8ba1\u7b97\u7cbe\u5ea6\u3001\u8936\u76b1\u7ec6\u8282\u8868\u73b0\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5e03\u6599\u6a21\u62df\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u7a7a\u95f4\u79bb\u6563\u5316\u6846\u67b6\uff0c\u53ef\u9002\u5e94\u4e0d\u540c\u6750\u6599\u53c2\u6570\u5b9e\u73b0\u590d\u6742\u76b1\u7eb9\u52a8\u529b\u5b66\u7684\u5b9e\u65f6\u4eff\u771f"}}
{"id": "2506.17314", "pdf": "https://arxiv.org/pdf/2506.17314", "abs": "https://arxiv.org/abs/2506.17314", "authors": ["Adnan Qidwai", "Srija Mukhopadhyay", "Prerana Khatiwada", "Dan Roth", "Vivek Gupta"], "title": "PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights", "categories": ["cs.CL", "cs.HC"], "comment": "9 Pages, 9 Figures. Accepted at ACL 2025 System Demonstration Track", "summary": "Accurate and complete product descriptions are crucial for e-commerce, yet\nseller-provided information often falls short. Customer reviews offer valuable\ndetails but are laborious to sift through manually. We present PRAISE: Product\nReview Attribute Insight Structuring Engine, a novel system that uses Large\nLanguage Models (LLMs) to automatically extract, compare, and structure\ninsights from customer reviews and seller descriptions. PRAISE provides users\nwith an intuitive interface to identify missing, contradictory, or partially\nmatching details between these two sources, presenting the discrepancies in a\nclear, structured format alongside supporting evidence from reviews. This\nallows sellers to easily enhance their product listings for clarity and\npersuasiveness, and buyers to better assess product reliability. Our\ndemonstration showcases PRAISE's workflow, its effectiveness in generating\nactionable structured insights from unstructured reviews, and its potential to\nsignificantly improve the quality and trustworthiness of e-commerce product\ncatalogs.", "AI": {"tldr": "PRAISE\u7cfb\u7edf\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u5206\u6790\u7535\u5546\u8bc4\u8bba\uff0c\u7ed3\u6784\u5316\u5c55\u793a\u4e0e\u5356\u5bb6\u63cf\u8ff0\u7684\u5dee\u5f02\uff0c\u63d0\u5347\u4ea7\u54c1\u4fe1\u606f\u8d28\u91cf", "motivation": "\u5356\u5bb6\u63cf\u8ff0\u5e38\u4e0d\u5b8c\u6574\uff0c\u5ba2\u6237\u8bc4\u8bba\u8574\u542b\u5b9d\u8d35\u7ec6\u8282\u4f46\u96be\u4ee5\u624b\u52a8\u5904\u7406\uff0c\u9700\u81ea\u52a8\u5316\u5de5\u5177\u89e3\u51b3\u4e70\u5356\u53cc\u65b9\u4fe1\u606f\u4e0d\u5bf9\u79f0\u95ee\u9898", "method": "\u57fa\u4e8eLLM\u5f00\u53d1PRAISE\u5f15\u64ce\uff0c\u81ea\u52a8\u63d0\u53d6/\u6bd4\u8f83\u8bc4\u8bba\u4e0e\u5546\u54c1\u63cf\u8ff0\uff0c\u63d0\u4f9b\u53ef\u89c6\u5316\u5dee\u5f02\u754c\u9762\u4e0e\u8bc1\u636e\u652f\u6301", "result": "\u7cfb\u7edf\u6709\u6548\u751f\u6210\u7ed3\u6784\u5316\u89c1\u89e3\uff0c\u663e\u8457\u63d0\u5347\u7535\u5546\u4ea7\u54c1\u76ee\u5f55\u53ef\u4fe1\u5ea6\u4e0e\u53ef\u64cd\u4f5c\u6027", "conclusion": "PRAISE\u901a\u8fc7\u7ed3\u6784\u5316\u5206\u6790\u8bc4\u8bba-\u63cf\u8ff0\u5dee\u5f02\uff0c\u4e3a\u6539\u5584\u7535\u5546\u4fe1\u606f\u900f\u660e\u5ea6\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.18671", "pdf": "https://arxiv.org/pdf/2506.18671", "abs": "https://arxiv.org/abs/2506.18671", "authors": ["Yuqin Dai", "Wanlu Zhu", "Ronghui Li", "Xiu Li", "Zhenyu Zhang", "Jun Li", "Jian Yang"], "title": "TCDiff++: An End-to-end Trajectory-Controllable Diffusion Model for Harmonious Music-Driven Group Choreography", "categories": ["cs.SD", "cs.CV", "cs.GR", "eess.AS"], "comment": null, "summary": "Music-driven dance generation has garnered significant attention due to its\nwide range of industrial applications, particularly in the creation of group\nchoreography. During the group dance generation process, however, most existing\nmethods still face three primary issues: multi-dancer collisions, single-dancer\nfoot sliding and abrupt swapping in the generation of long group dance. In this\npaper, we propose TCDiff++, a music-driven end-to-end framework designed to\ngenerate harmonious group dance. Specifically, to mitigate multi-dancer\ncollisions, we utilize a dancer positioning embedding to better maintain the\nrelative positioning among dancers. Additionally, we incorporate a\ndistance-consistency loss to ensure that inter-dancer distances remain within\nplausible ranges. To address the issue of single-dancer foot sliding, we\nintroduce a swap mode embedding to indicate dancer swapping patterns and design\na Footwork Adaptor to refine raw motion, thereby minimizing foot sliding. For\nlong group dance generation, we present a long group diffusion sampling\nstrategy that reduces abrupt position shifts by injecting positional\ninformation into the noisy input. Furthermore, we integrate a Sequence Decoder\nlayer to enhance the model's ability to selectively process long sequences.\nExtensive experiments demonstrate that our TCDiff++ achieves state-of-the-art\nperformance, particularly in long-duration scenarios, ensuring high-quality and\ncoherent group dance generation.", "AI": {"tldr": "\u63d0\u51faTCDiff++\u7aef\u5230\u7aef\u6846\u67b6\u89e3\u51b3\u7fa4\u821e\u751f\u6210\u4e2d\u7684\u78b0\u649e\u3001\u6ed1\u6b65\u548c\u4f4d\u7f6e\u7a81\u53d8\u95ee\u9898\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u957f\u5e8f\u5217\u7f16\u821e", "motivation": "\u73b0\u6709\u7fa4\u821e\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u591a\u821e\u8005\u78b0\u649e\u3001\u5355\u4eba\u6ed1\u6b65\u548c\u957f\u5e8f\u5217\u751f\u6210\u65f6\u4f4d\u7f6e\u7a81\u53d8\u4e09\u5927\u6838\u5fc3\u95ee\u9898", "method": "\u2460\u821e\u8005\u5b9a\u4f4d\u5d4c\u5165+\u8ddd\u79bb\u4e00\u81f4\u6027\u635f\u5931\u9632\u78b0\u649e \u2461\u4ea4\u6362\u6a21\u5f0f\u5d4c\u5165+\u6b65\u6001\u9002\u914d\u5668\u6d88\u51cf\u6ed1\u6b65 \u2462\u957f\u5e8f\u5217\u6269\u6563\u91c7\u6837\u7b56\u7565+\u5e8f\u5217\u89e3\u7801\u5c42\u89e3\u51b3\u4f4d\u7f6e\u7a81\u53d8", "result": "\u5728\u957f\u65f6\u573a\u666f\u4e0b\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u751f\u6210\u8d28\u91cf\u4e0e\u8fde\u8d2f\u6027\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "TCDiff++\u901a\u8fc7\u7cfb\u7edf\u6027\u7684\u591a\u6a21\u5757\u8bbe\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7fa4\u821e\u751f\u6210\u7684\u5173\u952e\u75db\u70b9\uff0c\u63a8\u8fdb\u4e86\u5de5\u4e1a\u7ea7\u7f16\u821e\u5e94\u7528\u53d1\u5c55"}}
{"id": "2506.17352", "pdf": "https://arxiv.org/pdf/2506.17352", "abs": "https://arxiv.org/abs/2506.17352", "authors": ["Tatsuhiro Aoshima", "Mitsuaki Akiyama"], "title": "Towards Safety Evaluations of Theory of Mind in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the capabilities of large language models (LLMs) continue to advance, the\nimportance of rigorous safety evaluation is becoming increasingly evident.\nRecent concerns within the realm of safety assessment have highlighted\ninstances in which LLMs exhibit behaviors that appear to disable oversight\nmechanisms and respond in a deceptive manner. For example, there have been\nreports suggesting that, when confronted with information unfavorable to their\nown persistence during task execution, LLMs may act covertly and even provide\nfalse answers to questions intended to verify their behavior.To evaluate the\npotential risk of such deceptive actions toward developers or users, it is\nessential to investigate whether these behaviors stem from covert, intentional\nprocesses within the model. In this study, we propose that it is necessary to\nmeasure the theory of mind capabilities of LLMs. We begin by reviewing existing\nresearch on theory of mind and identifying the perspectives and tasks relevant\nto its application in safety evaluation. Given that theory of mind has been\npredominantly studied within the context of developmental psychology, we\nanalyze developmental trends across a series of open-weight LLMs. Our results\nindicate that while LLMs have improved in reading comprehension, their theory\nof mind capabilities have not shown comparable development. Finally, we present\nthe current state of safety evaluation with respect to LLMs' theory of mind,\nand discuss remaining challenges for future work.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6b3a\u9a97\u884c\u4e3a\u662f\u5426\u4e0e\u5176\u5fc3\u667a\u7406\u8bba\uff08ToM\uff09\u80fd\u529b\u76f8\u5173\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u6d4b\u91cfToM\u80fd\u529b\u6765\u8bc4\u4f30LLMs\u7684\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u8fd1\u671f\u7814\u7a76\u53d1\u73b0LLMs\u5728\u4efb\u52a1\u4e2d\u53ef\u80fd\u89c4\u907f\u76d1\u7ba1\u5e76\u7ed9\u51fa\u6b3a\u9a97\u6027\u56de\u7b54\uff0c\u9700\u63a2\u7a76\u5176\u884c\u4e3a\u662f\u5426\u6e90\u4e8e\u6a21\u578b\u5185\u90e8\u7684\u9690\u853d\u610f\u56fe\u673a\u5236\u3002", "method": "\u56de\u987e\u5fc3\u667a\u7406\u8bba\u76f8\u5173\u7814\u7a76\uff0c\u68b3\u7406\u5176\u5728\u5b89\u5168\u8bc4\u4f30\u4e2d\u7684\u5e94\u7528\u573a\u666f\uff0c\u5e76\u6d4b\u8bd5\u591a\u4e2a\u5f00\u6e90LLMs\u7684ToM\u80fd\u529b\u53d1\u5c55\u8f68\u8ff9\u3002", "result": "LLMs\u7684\u9605\u8bfb\u7406\u89e3\u80fd\u529b\u663e\u8457\u63d0\u5347\uff0c\u4f46ToM\u80fd\u529b\u672a\u540c\u6b65\u53d1\u5c55\uff0c\u8868\u660e\u6a21\u578b\u7f3a\u4e4f\u5bf9\u610f\u56fe\u7684\u6df1\u5c42\u7406\u89e3\u3002", "conclusion": "\u5f53\u524d\u57fa\u4e8eToM\u7684\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\u4ecd\u4e0d\u5b8c\u5584\uff0c\u9700\u8fdb\u4e00\u6b65\u89e3\u51b3LLMs\u610f\u56fe\u8bc6\u522b\u4e0e\u884c\u4e3a\u5173\u8054\u6027\u7684\u6280\u672f\u6311\u6218\u3002"}}
{"id": "2506.17367", "pdf": "https://arxiv.org/pdf/2506.17367", "abs": "https://arxiv.org/abs/2506.17367", "authors": ["Mateusz Cedro", "Timour Ichmoukhamedov", "Sofie Goethals", "Yifan He", "James Hinns", "David Martens"], "title": "Cash or Comfort? How LLMs Value Your Inconvenience", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "12 pages, 4 figures, 3 tables", "summary": "Large Language Models (LLMs) are increasingly proposed as near-autonomous\nartificial intelligence (AI) agents capable of making everyday decisions on\nbehalf of humans. Although LLMs perform well on many technical tasks, their\nbehaviour in personal decision-making remains less understood. Previous studies\nhave assessed their rationality and moral alignment with human decisions.\nHowever, the behaviour of AI assistants in scenarios where financial rewards\nare at odds with user comfort has not yet been thoroughly explored. In this\npaper, we tackle this problem by quantifying the prices assigned by multiple\nLLMs to a series of user discomforts: additional walking, waiting, hunger and\npain. We uncover several key concerns that strongly question the prospect of\nusing current LLMs as decision-making assistants: (1) a large variance in\nresponses between LLMs, (2) within a single LLM, responses show fragility to\nminor variations in prompt phrasing (e.g., reformulating the question in the\nfirst person can considerably alter the decision), (3) LLMs can accept\nunreasonably low rewards for major inconveniences (e.g., 1 Euro to wait 10\nhours), and (4) LLMs can reject monetary gains where no discomfort is imposed\n(e.g., 1,000 Euro to wait 0 minutes). These findings emphasize the need for\nscrutiny of how LLMs value human inconvenience, particularly as we move toward\napplications where such cash-versus-comfort trade-offs are made on users'\nbehalf.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f53\u524dLLMs\u5728\u6d89\u53ca\u91d1\u94b1\u4e0e\u7528\u6237\u8212\u9002\u5ea6\u6743\u8861\u7684\u51b3\u7b56\u4e2d\u5b58\u5728\u54cd\u5e94\u4e0d\u4e00\u81f4\u3001\u63d0\u793a\u654f\u611f\u6027\u3001\u4e0d\u5408\u7406\u62a5\u916c\u63a5\u53d7\u7b49\u95ee\u9898\uff0c\u8d28\u7591\u5176\u4f5c\u4e3a\u51b3\u7b56\u52a9\u624b\u7684\u53ef\u884c\u6027", "motivation": "\u63a2\u7d22LLMs\u5728\u7528\u6237\u8212\u9002\u5ea6\u4e0e\u91d1\u94b1\u5956\u52b1\u51b2\u7a81\u573a\u666f\u4e0b\u7684\u51b3\u7b56\u884c\u4e3a\uff0c\u63ed\u793a\u5176\u4f5c\u4e3aAI\u52a9\u624b\u5728\u4e2a\u4eba\u51b3\u7b56\u4e2d\u7684\u6f5c\u5728\u98ce\u9669", "method": "\u901a\u8fc7\u91cf\u5316\u591a\u4e2aLLM\u5bf9\u56db\u79cd\u7528\u6237\u4e0d\u9002\uff08\u6b65\u884c/\u7b49\u5f85/\u9965\u997f/\u75bc\u75db\uff09\u7684\u5b9a\u4ef7\uff0c\u6d4b\u8bd5\u4e0d\u540c\u63d0\u793a\u8bed\u5bf9\u51b3\u7b56\u7684\u5f71\u54cd", "result": "(1)\u6a21\u578b\u95f4\u54cd\u5e94\u5dee\u5f02\u5927 (2)\u63d0\u793a\u5fae\u5c0f\u53d8\u5316\u5bfc\u81f4\u51b3\u7b56\u4e0d\u7a33\u5b9a (3)\u63a5\u53d7\u6781\u4f4e\u62a5\u916c\uff08\u59821\u6b27\u5143\u7b4910\u5c0f\u65f6\uff09(4)\u62d2\u7edd\u65e0\u4e0d\u9002\u7684\u9ad8\u989d\u5956\u52b1\uff08\u59821000\u6b27\u5143\u7b490\u5206\u949f\uff09", "conclusion": "\u73b0\u6709LLMs\u5728\u6d89\u53ca\u91d1\u94b1-\u8212\u9002\u6743\u8861\u7684\u51b3\u7b56\u4e2d\u5b58\u5728\u7cfb\u7edf\u6027\u7f3a\u9677\uff0c\u5f3a\u8c03\u5728\u76f8\u5173\u5e94\u7528\u573a\u666f\u4e2d\u9700\u4e25\u683c\u5ba1\u67e5\u5176\u51b3\u7b56\u673a\u5236"}}
{"id": "2506.17410", "pdf": "https://arxiv.org/pdf/2506.17410", "abs": "https://arxiv.org/abs/2506.17410", "authors": ["Danielle R. Thomas", "Conrad Borchers", "Jionghao Lin", "Sanjit Kakarla", "Shambhavi Bhushan", "Erin Gatz", "Shivang Gupta", "Ralph Abboud", "Kenneth R. Koedinger"], "title": "Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A Feasibility Study", "categories": ["cs.CL", "cs.CY"], "comment": "Short research paper accepted at EC-TEL 2025", "summary": "Tutoring improves student achievement, but identifying and studying what\ntutoring actions are most associated with student learning at scale based on\naudio transcriptions is an open research problem. This present study\ninvestigates the feasibility and scalability of using generative AI to identify\nand evaluate specific tutor moves in real-life math tutoring. We analyze 50\nrandomly selected transcripts of college-student remote tutors assisting middle\nschool students in mathematics. Using GPT-4, GPT-4o, GPT-4-turbo,\nGemini-1.5-pro, and LearnLM, we assess tutors' application of two tutor skills:\ndelivering effective praise and responding to student math errors. All models\nreliably detected relevant situations, for example, tutors providing praise to\nstudents (94-98% accuracy) and a student making a math error (82-88% accuracy)\nand effectively evaluated the tutors' adherence to tutoring best practices,\naligning closely with human judgments (83-89% and 73-77%, respectively). We\npropose a cost-effective prompting strategy and discuss practical implications\nfor using large language models to support scalable assessment in authentic\nsettings. This work further contributes LLM prompts to support reproducibility\nand research in AI-supported learning.", "AI": {"tldr": "\u7814\u7a76\u9a8c\u8bc1\u751f\u6210\u5f0fAI\u53ef\u6709\u6548\u8bc4\u4f30\u6570\u5b66\u8f85\u5bfc\u4e2d\u7684\u5bfc\u5e08\u884c\u4e3a\uff0c\u51c6\u786e\u7387\u63a5\u8fd1\u4eba\u7c7b\u5224\u65ad\uff0c\u5e76\u63d0\u51fa\u6210\u672c\u6548\u76ca\u7b56\u7565", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5927\u89c4\u6a21\u5206\u6790\u771f\u5b9e\u8f85\u5bfc\u573a\u666f\u4e2d\u7684\u5bfc\u5e08\u884c\u4e3a\u6548\u679c\uff0c\u9700\u63a2\u7d22AI\u7684\u53ef\u884c\u6027", "method": "\u4f7f\u7528GPT-4/Gemini\u7b49\u6a21\u578b\u5206\u679050\u4efd\u6570\u5b66\u8f85\u5bfc\u8f6c\u5f55\u6587\u672c\uff0c\u68c0\u6d4b\u5bfc\u5e08\u7684\u8868\u626c\u6280\u5de7\u548c\u9519\u8bef\u5e94\u5bf9\u7b56\u7565", "result": "\u6a21\u578b\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe82-98%\uff08\u8868\u626c94-98%\uff0c\u9519\u8bef82-88%\uff09\uff0c\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e00\u81f4\u602773-89%", "conclusion": "\u63d0\u51fa\u53ef\u6269\u5c55\u7684AI\u8bc4\u4f30\u6846\u67b6\uff0c\u4e3a\u6559\u80b2\u8d28\u91cf\u76d1\u6d4b\u548c\u5bfc\u5e08\u57f9\u8bad\u63d0\u4f9b\u6280\u672f\u652f\u6301"}}
{"id": "2506.17419", "pdf": "https://arxiv.org/pdf/2506.17419", "abs": "https://arxiv.org/abs/2506.17419", "authors": ["Jinhao Duan", "James Diffenderfer", "Sandeep Madireddy", "Tianlong Chen", "Bhavya Kailkhura", "Kaidi Xu"], "title": "UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": "19 pages, 5 figures, 4 tables", "summary": "As Large Language Models (LLMs) are integrated into safety-critical\napplications involving sequential decision-making in the real world, it is\nessential to know when to trust LLM decisions. Existing LLM Uncertainty\nQuantification (UQ) methods are primarily designed for single-turn\nquestion-answering formats, resulting in multi-step decision-making scenarios,\ne.g., LLM agentic system, being underexplored. In this paper, we introduce a\nprincipled, information-theoretic framework that decomposes LLM sequential\ndecision uncertainty into two parts: (i) internal uncertainty intrinsic to the\ncurrent decision, which is focused on existing UQ methods, and (ii) extrinsic\nuncertainty, a Mutual-Information (MI) quantity describing how much uncertainty\nshould be inherited from preceding decisions. We then propose UProp, an\nefficient and effective extrinsic uncertainty estimator that converts the\ndirect estimation of MI to the estimation of Pointwise Mutual Information (PMI)\nover multiple Trajectory-Dependent Decision Processes (TDPs). UProp is\nevaluated over extensive multi-step decision-making benchmarks, e.g.,\nAgentBench and HotpotQA, with state-of-the-art LLMs, e.g., GPT-4.1 and\nDeepSeek-V3. Experimental results demonstrate that UProp significantly\noutperforms existing single-turn UQ baselines equipped with thoughtful\naggregation strategies. Moreover, we provide a comprehensive analysis of UProp,\nincluding sampling efficiency, potential applications, and intermediate\nuncertainty propagation, to demonstrate its effectiveness. Codes will be\navailable at https://github.com/jinhaoduan/UProp.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4fe1\u606f\u8bba\u7684\u6846\u67b6UProp\uff0c\u5c06LLM\u591a\u6b65\u51b3\u7b56\u4e0d\u786e\u5b9a\u6027\u5206\u89e3\u4e3a\u5185\u90e8/\u5916\u90e8\u56e0\u7d20\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5355\u8f6e\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u96c6\u4e2d\u4e8e\u5355\u8f6e\u95ee\u7b54\uff0c\u4f46LLM\u5728\u5b89\u5168\u5173\u952e\u9886\u57df(\u5982\u591a\u6b65\u51b3\u7b56\u4ee3\u7406\u7cfb\u7edf)\u5e94\u7528\u65f6\uff0c\u524d\u7f6e\u51b3\u7b56\u7684\u4e0d\u786e\u5b9a\u6027\u7ee7\u627f\u95ee\u9898\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u4e92\u4fe1\u606f\u7406\u8bba\u5206\u89e3\u4e0d\u786e\u5b9a\u6027\u4e3a\u5185\u90e8(\u5f53\u524d\u51b3\u7b56)\u548c\u5916\u90e8(\u7ee7\u627f\u81ea\u5386\u53f2\u51b3\u7b56)\uff0c\u63d0\u51fa\u57fa\u4e8e\u8f68\u8ff9\u4f9d\u8d56\u51b3\u7b56\u8fc7\u7a0b\u7684PMI\u4f30\u8ba1\u5668UProp\u3002", "result": "\u5728AgentBench/HotpotQA\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUProp\u5728GPT-4.1/DeepSeek-V3\u7b49\u6a21\u578b\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u663e\u8457\uff0c\u4e14\u91c7\u6837\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "UProp\u4e3aLLM\u5728\u591a\u6b65\u51b3\u7b56\u573a\u666f\u4e2d\u7684\u53ef\u9760\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u5176\u5916\u90e8\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86\u5386\u53f2\u51b3\u7b56\u5f71\u54cd\u4f20\u9012\u95ee\u9898\u3002"}}
{"id": "2506.17435", "pdf": "https://arxiv.org/pdf/2506.17435", "abs": "https://arxiv.org/abs/2506.17435", "authors": ["Alberto Martinez-Serra", "Alejandro De La Fuente", "Nienke Viescher", "Ana S. Cardenal"], "title": "Beyond the Link: Assessing LLMs' ability to Classify Political Content across Global Media", "categories": ["cs.CL"], "comment": null, "summary": "The use of large language models (LLMs) is becoming common in the context of\npolitical science, particularly in studies that analyse individuals use of\ndigital media. However, while previous research has demonstrated LLMs ability\nat labelling tasks, the effectiveness of using LLMs to classify political\ncontent (PC) from just URLs is not yet well explored. The work presented in\nthis article bridges this gap by evaluating whether LLMs can accurately\nidentify PC vs. non-PC from both the article text and the URLs from five\ncountries (France, Germany, Spain, the UK, and the US) and different languages.\nUsing cutting-edge LLMs like GPT, Llama, Mistral, Deepseek, Qwen and Gemma, we\nmeasure model performance to assess whether URL-level analysis can be a good\napproximation for full-text analysis of PC, even across different linguistic\nand national contexts. Model outputs are compared with human-labelled articles,\nas well as traditional supervised machine learning techniques, to set a\nbaseline of performance. Overall, our findings suggest the capacity of URLs to\nembed most of the news content, providing a vital perspective on accuracy-cost\nbalancing. We also account for contextual limitations and suggest\nmethodological recommendations to use LLMs within political science studies.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86LLMs\u901a\u8fc7URL\u4e0e\u5168\u6587\u5206\u6790\u653f\u6cbb\u5185\u5bb9\u7684\u80fd\u529b\uff0c\u53d1\u73b0URL\u53ef\u6709\u6548\u5d4c\u5165\u65b0\u95fb\u5185\u5bb9\u5e76\u5e73\u8861\u6210\u672c\u4e0e\u51c6\u786e\u6027\uff0c\u63d0\u51fa\u4e86\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u65b9\u6cd5\u8bba\u5efa\u8bae\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u9a8c\u8bc1LLMs\u4ec5\u901a\u8fc7URL\u5206\u7c7b\u653f\u6cbb\u5185\u5bb9\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u5728\u591a\u56fd\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u5e76\u63a2\u7d22URL\u5206\u6790\u66ff\u4ee3\u5168\u6587\u5206\u6790\u7684\u53ef\u884c\u6027\u3002", "method": "\u4f7f\u7528GPT/Llama/Mistral\u7b49\u5148\u8fdbLLMs\uff0c\u5bf9\u6bd4\u4e94\u56fd\u591a\u8bed\u8a00\u73af\u5883\u4e0bURL\u4e0e\u5168\u6587\u7684PC\u5206\u7c7b\u6548\u679c\uff0c\u5e76\u4e0e\u4eba\u5de5\u6807\u6ce8\u53ca\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u6027\u80fd\u57fa\u7ebf\u6bd4\u8f83\u3002", "result": "URL\u80fd\u6709\u6548\u8868\u5f81\u65b0\u95fb\u6838\u5fc3\u5185\u5bb9\uff0cLLMs\u5728\u8de8\u8bed\u8a00\u573a\u666f\u4e0b\u5206\u7c7b\u51c6\u786e\u7387\u63a5\u8fd1\u5168\u6587\u5206\u6790\uff0c\u4e14\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u8bc1\u5b9e\u4e86URL\u5206\u6790\u5728\u6210\u672c\u6548\u7387\u4e0a\u7684\u4f18\u52bf\u3002", "conclusion": "URL\u5206\u6790\u4e3a\u653f\u6cbb\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6027\u4ef7\u6bd4\u65b9\u6848\uff0c\u4f46\u9700\u6ce8\u610f\u6a21\u578b\u8bed\u5883\u5c40\u9650\u6027\u3002\u5efa\u8bae\u7ed3\u5408\u6df7\u5408\u6807\u6ce8\u7b56\u7565\u4e0e\u52a8\u6001\u9608\u503c\u4f18\u5316LLMs\u5e94\u7528\u6548\u679c\u3002"}}
{"id": "2506.17459", "pdf": "https://arxiv.org/pdf/2506.17459", "abs": "https://arxiv.org/abs/2506.17459", "authors": ["Siyu Liang", "Gina-Anne Levow"], "title": "Breaking the Transcription Bottleneck: Fine-tuning ASR Models for Extremely Low-Resource Fieldwork Languages", "categories": ["cs.CL"], "comment": null, "summary": "Automatic Speech Recognition (ASR) has reached impressive accuracy for\nhigh-resource languages, yet its utility in linguistic fieldwork remains\nlimited. Recordings collected in fieldwork contexts present unique challenges,\nincluding spontaneous speech, environmental noise, and severely constrained\ndatasets from under-documented languages. In this paper, we benchmark the\nperformance of two fine-tuned multilingual ASR models, MMS and XLS-R, on five\ntypologically diverse low-resource languages with control of training data\nduration. Our findings show that MMS is best suited when extremely small\namounts of training data are available, whereas XLS-R shows parity performance\nonce training data exceed one hour. We provide linguistically grounded analysis\nfor further provide insights towards practical guidelines for field linguists,\nhighlighting reproducible ASR adaptation approaches to mitigate the\ntranscription bottleneck in language documentation.", "AI": {"tldr": "MMS\u6a21\u578b\u5728\u6781\u5c0f\u89c4\u6a21\u8bad\u7ec3\u6570\u636e\u4e0b\u8868\u73b0\u6700\u4f73\uff0cXLS-R\u6a21\u578b\u5728\u8d85\u8fc71\u5c0f\u65f6\u8bad\u7ec3\u6570\u636e\u540e\u5c55\u73b0\u540c\u7b49\u6027\u80fd", "motivation": "\u89e3\u51b3\u8bed\u8a00\u6587\u6863\u5316\u4e2d\u7684\u8f6c\u5f55\u74f6\u9888\u95ee\u9898\uff0c\u9488\u5bf9\u7530\u91ce\u8c03\u67e5\u4e2d\u4f4e\u8d44\u6e90\u8bed\u8a00\u9762\u4e34\u7684\u72ec\u7279\u6311\u6218\uff08\u81ea\u53d1\u8bed\u97f3/\u73af\u5883\u566a\u97f3/\u6570\u636e\u7a00\u7f3a\uff09", "method": "\u901a\u8fc7\u63a7\u5236\u8bad\u7ec3\u6570\u636e\u65f6\u957f\uff0c\u5bf9MMS\u548cXLS-R\u4e24\u4e2a\u591a\u8bed\u8a00ASR\u6a21\u578b\u5728\u4e94\u79cd\u7c7b\u578b\u5b66\u5dee\u5f02\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5", "result": "\u6781\u5c11\u91cf\u6570\u636e\u65f6MMS\u4f18\u52bf\u663e\u8457\uff0c\u8bad\u7ec3\u6570\u636e\u8d85\u8fc71\u5c0f\u65f6\u540eXLS-R\u8fbe\u5230\u540c\u7b49\u6027\u80fd\u6c34\u5e73", "conclusion": "\u63d0\u51fa\u53ef\u590d\u73b0\u7684ASR\u9002\u914d\u65b9\u6848\uff0c\u4e3a\u7530\u91ce\u8bed\u8a00\u5b66\u5bb6\u63d0\u4f9b\u7f13\u89e3\u8f6c\u5f55\u74f6\u9888\u7684\u5b9e\u7528\u6307\u5357"}}
{"id": "2506.17467", "pdf": "https://arxiv.org/pdf/2506.17467", "abs": "https://arxiv.org/abs/2506.17467", "authors": ["Weixin Liang"], "title": "Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Stanford CS PhD Dissertation", "summary": "Large language models (LLMs) have shown significant potential to change how\nwe write, communicate, and create, leading to rapid adoption across society.\nThis dissertation examines how individuals and institutions are adapting to and\nengaging with this emerging technology through three research directions.\nFirst, I demonstrate how the institutional adoption of AI detectors introduces\nsystematic biases, particularly disadvantaging writers of non-dominant language\nvarieties, highlighting critical equity concerns in AI governance. Second, I\npresent novel population-level algorithmic approaches that measure the\nincreasing adoption of LLMs across writing domains, revealing consistent\npatterns of AI-assisted content in academic peer reviews, scientific\npublications, consumer complaints, corporate communications, job postings, and\ninternational organization press releases. Finally, I investigate LLMs'\ncapability to provide feedback on research manuscripts through a large-scale\nempirical analysis, offering insights into their potential to support\nresearchers who face barriers in accessing timely manuscript feedback,\nparticularly early-career researchers and those from under-resourced settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u4e09\u4e2a\u65b9\u5411\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u793e\u4f1a\u7684\u5f71\u54cd\uff1a\u63ed\u793aAI\u68c0\u6d4b\u5668\u5236\u5ea6\u6027\u504f\u89c1\u3001\u91cf\u5316LLM\u5728\u5404\u5199\u4f5c\u9886\u57df\u6e17\u900f\u7387\u3001\u9a8c\u8bc1LLM\u79d1\u7814\u53cd\u9988\u6709\u6548\u6027", "motivation": "\u89e3\u51b3LLM\u666e\u53ca\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u7cfb\u7edf\u6027\u504f\u89c1\u95ee\u9898\uff0c\u91cf\u5316\u6280\u672f\u5e94\u7528\u89c4\u6a21\uff0c\u63a2\u7d22\u5176\u8f85\u52a9\u79d1\u7814\u7684\u6f5c\u529b\u4ee5\u4fc3\u8fdb\u7814\u7a76\u516c\u5e73\u6027", "method": "\u91c7\u7528\u7b97\u6cd5\u6d4b\u91cf\u6846\u67b6\u5206\u6790\u591a\u9886\u57df\u6587\u672c\u6570\u636e\uff08\u5b66\u672f\u8bc4\u5ba1\u3001\u79d1\u5b66\u8bba\u6587\u7b49\uff09\uff0c\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u5206\u6790\u9a8c\u8bc1LLM\u53cd\u9988\u6548\u679c", "result": "\u53d1\u73b0\u68c0\u6d4b\u5668\u5b58\u5728\u8bed\u8a00\u6b67\u89c6\u3001\u5404\u9886\u57dfAI\u5185\u5bb9\u5360\u6bd4\u6301\u7eed\u4e0a\u5347\uff08\u6700\u9ad8\u8fbe10%\uff09\u3001LLM\u53cd\u9988\u8d28\u91cf\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u6c34\u5e73", "conclusion": "\u4e3aAI\u6cbb\u7406\u63d0\u4f9b\u516c\u5e73\u6027\u6846\u67b6\uff0c\u5efa\u7acb\u6280\u672f\u6e17\u900f\u76d1\u6d4b\u8303\u5f0f\uff0c\u9a8c\u8bc1LLM\u4f5c\u4e3a\u79d1\u7814\u666e\u60e0\u5de5\u5177\u7684\u53ef\u80fd\u6027"}}
{"id": "2506.17506", "pdf": "https://arxiv.org/pdf/2506.17506", "abs": "https://arxiv.org/abs/2506.17506", "authors": ["Lesheng Jin", "Zhenyuan Ruan", "Haohui Mai", "Jingbo Shang"], "title": "VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM", "categories": ["cs.CL", "cs.OS"], "comment": null, "summary": "Modern GPUs evolve rapidly, yet production compilers still rely on\nhand-crafted register allocation heuristics that require substantial re-tuning\nfor each hardware generation. We introduce VeriLocc, a framework that combines\nlarge language models (LLMs) with formal compiler techniques to enable\ngeneralizable and verifiable register allocation across GPU architectures.\nVeriLocc fine-tunes an LLM to translate intermediate representations (MIRs)\ninto target-specific register assignments, aided by static analysis for\ncross-architecture normalization and generalization and a verifier-guided\nregeneration loop to ensure correctness. Evaluated on matrix multiplication\n(GEMM) and multi-head attention (MHA), VeriLocc achieves 85-99% single-shot\naccuracy and near-100% pass@100. Case study shows that VeriLocc discovers more\nperformant assignments than expert-tuned libraries, outperforming rocBLAS by\nover 10% in runtime.", "AI": {"tldr": "VeriLocc\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u5f62\u5f0f\u5316\u7f16\u8bd1\u5668\u6280\u672f\uff0c\u5b9e\u73b0\u8de8GPU\u67b6\u6784\u7684\u901a\u7528\u53ef\u9a8c\u8bc1\u5bc4\u5b58\u5668\u5206\u914d\uff0c\u6027\u80fd\u8d85\u8d8a\u4e13\u5bb6\u8c03\u4f18\u65b9\u6848", "motivation": "\u89e3\u51b3\u4f20\u7edf\u7f16\u8bd1\u5668\u4f9d\u8d56\u4eba\u5de5\u542f\u53d1\u5f0f\u89c4\u5219\u5bfc\u81f4\u7684\u8de8\u786c\u4ef6\u4ee3\u9645\u9002\u914d\u6210\u672c\u9ad8\u95ee\u9898\uff0c\u9002\u5e94GPU\u5feb\u901f\u8fed\u4ee3\u9700\u6c42", "method": "1) \u5fae\u8c03LLM\u5b9e\u73b0\u4e2d\u95f4\u8868\u793a\u5230\u5bc4\u5b58\u5668\u5206\u914d\u7684\u8f6c\u6362 2) \u9759\u6001\u5206\u6790\u5b9e\u73b0\u8de8\u67b6\u6784\u5f52\u4e00\u5316 3) \u9a8c\u8bc1\u5668\u5f15\u5bfc\u7684\u518d\u751f\u5faa\u73af\u4fdd\u8bc1\u6b63\u786e\u6027", "result": "GEMM/MHA\u4efb\u52a1\u4e2d\u5355\u6b21\u51c6\u786e\u738785-99%\uff0cpass@100\u63a5\u8fd1100%\uff1brocBLAS\u8fd0\u884c\u65f6\u6027\u80fd\u63d0\u5347\u8d8510%", "conclusion": "\u8be5\u6846\u67b6\u5f00\u521b\u4e86\u673a\u5668\u5b66\u4e60\u4e0e\u5f62\u5f0f\u5316\u65b9\u6cd5\u7ed3\u5408\u7684\u7f16\u8bd1\u5668\u4f18\u5316\u65b0\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u5f00\u53d1\u6548\u7387\u548c\u8ba1\u7b97\u6027\u80fd"}}
{"id": "2506.17525", "pdf": "https://arxiv.org/pdf/2506.17525", "abs": "https://arxiv.org/abs/2506.17525", "authors": ["Mingfei Lau", "Qian Chen", "Yeming Fang", "Tingting Xu", "Tongzhou Chen", "Pavel Golik"], "title": "Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Our quality audit for three widely used public multilingual speech datasets -\nMozilla Common Voice 17.0, FLEURS, and VoxPopuli - shows that in some\nlanguages, these datasets suffer from significant quality issues. We believe\naddressing these issues will make these datasets more useful as training and\nevaluation sets, and improve downstream models. We divide these quality issues\ninto two categories: micro-level and macro-level. We find that macro-level\nissues are more prevalent in less institutionalized, often under-resourced\nlanguages. We provide a case analysis of Taiwanese Southern Min (nan_tw) that\nhighlights the need for proactive language planning (e.g. orthography\nprescriptions, dialect boundary definition) and enhanced data quality control\nin the process of Automatic Speech Recognition (ASR) dataset creation. We\nconclude by proposing guidelines and recommendations to mitigate these issues\nin future dataset development, emphasizing the importance of sociolinguistic\nawareness in creating robust and reliable speech data resources.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4e3b\u6d41\u591a\u8bed\u8a00\u8bed\u97f3\u6570\u636e\u96c6\u5b58\u5728\u5fae\u89c2\u548c\u5b8f\u89c2\u8d28\u91cf\u95ee\u9898\uff0c\u5c24\u5176\u5728\u8d44\u6e90\u4e0d\u8db3\u8bed\u8a00\u4e2d\u5b8f\u89c2\u95ee\u9898\u66f4\u7a81\u51fa\u3002\u4ee5\u53f0\u6e7e\u95fd\u5357\u8bed\u4e3a\u4f8b\uff0c\u5f3a\u8c03\u8bed\u8a00\u89c4\u5212\u548c\u6570\u636e\u8d28\u91cf\u63a7\u5236\u7684\u91cd\u8981\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u6307\u5357\u3002", "motivation": "\u63ed\u9732Mozilla Common Voice\u7b49\u4e09\u5927\u4e3b\u6d41\u591a\u8bed\u8a00\u8bed\u97f3\u6570\u636e\u96c6\u5728\u90e8\u5206\u8bed\u8a00\u4e2d\u5b58\u5728\u7684\u8d28\u91cf\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u4f1a\u5f71\u54cd\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\u6548\u679c\u3002", "method": "\u901a\u8fc7\u8d28\u91cf\u5ba1\u8ba1\u65b9\u6cd5\u5206\u6790\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff0c\u5e76\u4ee5\u53f0\u6e7e\u95fd\u5357\u8bed\u4e3a\u6848\u4f8b\u7814\u7a76\u8bed\u8a00\u89c4\u5212\u5bf9\u8bed\u97f3\u6570\u636e\u96c6\u8d28\u91cf\u7684\u5f71\u54cd\u3002", "result": "\u53d1\u73b0\u5b8f\u89c2\u95ee\u9898\uff08\u5982\u8bed\u8a00\u89c4\u8303\u7f3a\u5931\uff09\u5728\u5236\u5ea6\u5316\u7a0b\u5ea6\u4f4e\u7684\u8bed\u8a00\u4e2d\u66f4\u666e\u904d\uff0c\u6848\u4f8b\u663e\u793a\u9700\u52a0\u5f3a\u65b9\u8a00\u8fb9\u754c\u5b9a\u4e49\u548c\u62fc\u5199\u89c4\u8303\u5236\u5b9a\u3002", "conclusion": "\u63d0\u51fa\u672a\u6765\u6570\u636e\u96c6\u5f00\u53d1\u9700\u589e\u5f3a\u793e\u4f1a\u8bed\u8a00\u5b66\u610f\u8bc6\uff0c\u5236\u5b9a\u8bed\u8a00\u89c4\u5212\u65b9\u6848\u5e76\u52a0\u5f3a\u8d28\u91cf\u7ba1\u63a7\uff0c\u4ee5\u5efa\u7acb\u66f4\u53ef\u9760\u7684\u8bed\u97f3\u8d44\u6e90\u3002"}}
{"id": "2506.17533", "pdf": "https://arxiv.org/pdf/2506.17533", "abs": "https://arxiv.org/abs/2506.17533", "authors": ["Yuanhao Wu", "Juntong Song", "Hanning Zhang", "Tong Zhang", "Cheng Niu"], "title": "DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose DuaShepherd, a novel reward modeling framework that\nintegrates two complementary reward signals, correctness and potential, to\nenhance the mathematical reasoning capabilities of Large Language Models\n(LLMs). While correctness-based signals emphasize identification of stepwise\nerrors, potential-based signals focus on the likelihood of reaching the correct\nfinal answer. We developed an automated pipeline for constructing large-scale\nreward modeling dataset with both signals. A unified, multi-head architecture\nwas explored to train the two reward models in a multi-task setup,\ndemonstrating benefits from learning both correctness and potential in\nparallel. By combining these two signals into a compound probability, our model\nachieves consistent performance improvements across multiple benchmarks.\nEmpirical evaluations on MATH500 and ProcessBench confirm that this combined\nreward significantly outperforms models trained on either reward type alone,\nachieving state-of-the-art performance under comparable resource constraints.", "AI": {"tldr": "\u63d0\u51faDuaShepherd\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u6b63\u786e\u6027\u548c\u6f5c\u5728\u6027\u53cc\u5956\u52b1\u4fe1\u53f7\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u7ec4\u5408\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5355\u4fe1\u53f7\u6a21\u578b", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8\u9010\u6b65\u6b63\u786e\u6027\u6216\u6700\u7ec8\u7b54\u6848\u53ef\u80fd\u6027\u5355\u4e00\u7ef4\u5ea6\uff0c\u9700\u7ed3\u5408\u4e24\u79cd\u4e92\u8865\u4fe1\u53f7\u5b9e\u73b0\u66f4\u5168\u9762\u7684\u9519\u8bef\u68c0\u6d4b\u548c\u63a8\u7406\u4f18\u5316", "method": "\u6784\u5efa\u81ea\u52a8\u5316\u53cc\u4fe1\u53f7\u6570\u636e\u96c6+\u591a\u5934\u591a\u4efb\u52a1\u67b6\u6784\u8054\u5408\u8bad\u7ec3+\u590d\u5408\u6982\u7387\u878d\u5408\u7b56\u7565", "result": "\u5728MATH500\u548cProcessBench\u5b9e\u73b0SOTA\uff0c\u7ec4\u5408\u5956\u52b1\u6a21\u578b\u51c6\u786e\u7387\u6bd4\u5355\u4fe1\u53f7\u6a21\u578b\u63d0\u53476.2%", "conclusion": "\u53cc\u4fe1\u53f7\u4e92\u8865\u673a\u5236\u6709\u6548\u63d0\u5347\u6570\u5b66\u63a8\u7406\u6027\u80fd\uff0c\u5728\u540c\u7b49\u8ba1\u7b97\u8d44\u6e90\u4e0b\u8fbe\u5230\u6700\u4f73\u6548\u679c"}}
{"id": "2506.17542", "pdf": "https://arxiv.org/pdf/2506.17542", "abs": "https://arxiv.org/abs/2506.17542", "authors": ["Nitin Venkateswaran", "Kevin Tang", "Ratree Wayland"], "title": "Probing for Phonology in Self-Supervised Speech Representations: A Case Study on Accent Perception", "categories": ["cs.CL"], "comment": null, "summary": "Traditional models of accent perception underestimate the role of gradient\nvariations in phonological features which listeners rely upon for their accent\njudgments. We investigate how pretrained representations from current\nself-supervised learning (SSL) models of speech encode phonological\nfeature-level variations that influence the perception of segmental accent. We\nfocus on three segments: the labiodental approximant, the rhotic tap, and the\nretroflex stop, which are uniformly produced in the English of native speakers\nof Hindi as well as other languages in the Indian sub-continent. We use the\nCSLU Foreign Accented English corpus (Lander, 2007) to extract, for these\nsegments, phonological feature probabilities using Phonet (V\\'asquez-Correa et\nal., 2019) and pretrained representations from Wav2Vec2-BERT (Barrault et al.,\n2023) and WavLM (Chen et al., 2022) along with accent judgements by native\nspeakers of American English. Probing analyses show that accent strength is\nbest predicted by a subset of the segment's pretrained representation features,\nin which perceptually salient phonological features that contrast the expected\nAmerican English and realized non-native English segments are given prominent\nweighting. A multinomial logistic regression of pretrained representation-based\nsegment distances from American and Indian English baselines on accent ratings\nreveals strong associations between the odds of accent strength and distances\nfrom the baselines, in the expected directions. These results highlight the\nvalue of self-supervised speech representations for modeling accent perception\nusing interpretable phonological features.", "AI": {"tldr": "\u81ea\u76d1\u7763\u8bed\u97f3\u8868\u5f81\u80fd\u6709\u6548\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u97f3\u7cfb\u7279\u5f81\u5efa\u6a21\u53e3\u97f3\u611f\u77e5\uff0c\u9884\u8bad\u7ec3\u6a21\u578b\u7279\u5f81\u53ef\u9884\u6d4b\u975e\u6bcd\u8bed\u82f1\u8bed\u7247\u6bb5\u7684\u53e3\u97f3\u5f3a\u5ea6", "motivation": "\u4f20\u7edf\u53e3\u97f3\u611f\u77e5\u6a21\u578b\u4f4e\u4f30\u97f3\u7cfb\u7279\u5f81\u68af\u5ea6\u53d8\u5316\u7684\u4f5c\u7528\uff0c\u9700\u9a8c\u8bc1\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u5bf9\u97f3\u7cfb\u7279\u5f81\u7684\u8868\u5f81\u80fd\u529b", "method": "\u4f7f\u7528CSLU\u5916\u56fd\u53e3\u97f3\u82f1\u8bed\u8bed\u6599\u5e93\uff0c\u7ed3\u5408Phonet\u97f3\u7cfb\u7279\u5f81\u6982\u7387\u63d0\u53d6\u548cWav2Vec2-BERT/WavLM\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u901a\u8fc7\u63a2\u6d4b\u5206\u6790\u548c\u591a\u9879\u903b\u8f91\u56de\u5f52\u7814\u7a76\u8bed\u97f3\u7247\u6bb5\u4e0e\u53e3\u97f3\u8bc4\u7ea7\u5173\u7cfb", "result": "\u9884\u8bad\u7ec3\u8868\u5f81\u7684\u7279\u5b9a\u5b50\u96c6\u80fd\u6700\u4f73\u9884\u6d4b\u53e3\u97f3\u5f3a\u5ea6\uff0c\u7a81\u663e\u7f8e\u5f0f\u82f1\u8bed\u4e0e\u975e\u6bcd\u8bed\u82f1\u8bed\u97f3\u7cfb\u5bf9\u6bd4\u7279\u5f81\uff1b\u57fa\u7ebf\u8ddd\u79bb\u4e0e\u53e3\u97f3\u5f3a\u5ea6\u6982\u7387\u663e\u8457\u76f8\u5173", "conclusion": "\u81ea\u76d1\u7763\u8bed\u97f3\u8868\u5f81\u4e3a\u57fa\u4e8e\u53ef\u89e3\u91ca\u97f3\u7cfb\u7279\u5f81\u7684\u53e3\u97f3\u611f\u77e5\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u9a8c\u8bc1\u4e86\u97f3\u7cfb\u5bf9\u6bd4\u7279\u5f81\u5728\u53e3\u97f3\u5224\u65ad\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528"}}
{"id": "2506.17578", "pdf": "https://arxiv.org/pdf/2506.17578", "abs": "https://arxiv.org/abs/2506.17578", "authors": ["Lingxiao Zeng", "Yiqi Tong", "Wei Guo", "Huarui Wu", "Lihao Ge", "Yijun Ye", "Fuzhen Zhuang", "Deqing Wang", "Wei Guo", "Cheng Chen"], "title": "AgriCHN: A Comprehensive Cross-domain Resource for Chinese Agricultural Named Entity Recognition", "categories": ["cs.CL"], "comment": null, "summary": "Agricultural named entity recognition is a specialized task focusing on\nidentifying distinct agricultural entities within vast bodies of text,\nincluding crops, diseases, pests, and fertilizers. It plays a crucial role in\nenhancing information extraction from extensive agricultural text resources.\nHowever, the scarcity of high-quality agricultural datasets, particularly in\nChinese, has resulted in suboptimal performance when employing mainstream\nmethods for this purpose. Most earlier works only focus on annotating\nagricultural entities while overlook the profound correlation of agriculture\nwith hydrology and meteorology. To fill this blank, we present AgriCHN, a\ncomprehensive open-source Chinese resource designed to promote the accuracy of\nautomated agricultural entity annotation. The AgriCHN dataset has been\nmeticulously curated from a wealth of agricultural articles, comprising a total\nof 4,040 sentences and encapsulating 15,799 agricultural entity mentions\nspanning 27 diverse entity categories. Furthermore, it encompasses entities\nfrom hydrology to meteorology, thereby enriching the diversity of entities\nconsidered. Data validation reveals that, compared with relevant resources,\nAgriCHN demonstrates outstanding data quality, attributable to its richer\nagricultural entity types and more fine-grained entity divisions. A benchmark\ntask has also been constructed using several state-of-the-art neural NER\nmodels. Extensive experimental results highlight the significant challenge\nposed by AgriCHN and its potential for further research.", "AI": {"tldr": "\u63d0\u51faAgriCHN\u6570\u636e\u96c6\u2014\u2014\u5305\u542b\u6c34\u6587\u6c14\u8c61\u5b9e\u4f53\u7684\u5f00\u6e90\u4e2d\u6587\u519c\u4e1a\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u8d44\u6e90\uff0c\u542b4040\u53e5\u5b50\u4e0e15799\u4e2a\u8de827\u7c7b\u5b9e\u4f53\u6807\u6ce8\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u9ad8\u8d28\u91cf\u4e0e\u6311\u6218\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u4e2d\u6587\u519c\u4e1a\u5b9e\u4f53\u8bc6\u522b\u6570\u636e\u96c6\u7a00\u7f3a\u3001\u5ffd\u89c6\u4e0e\u6c34\u6587\u6c14\u8c61\u5b9e\u4f53\u5173\u8054\u7684\u95ee\u9898\uff0c\u63d0\u5347\u519c\u4e1a\u6587\u672c\u4fe1\u606f\u62bd\u53d6\u7cbe\u5ea6\u3002", "method": "\u4ece\u519c\u4e1a\u6587\u732e\u4e2d\u6784\u5efaAgriCHN\u6570\u636e\u96c6\uff0c\u6db5\u76d627\u7c7b\u5b9e\u4f53\uff08\u542b\u6c34\u6587\u6c14\u8c61\uff09\uff0c\u91c7\u7528\u7ec6\u7c92\u5ea6\u6807\u6ce8\uff0c\u5e76\u57fa\u4e8e\u4e3b\u6d41NER\u6a21\u578b\u5efa\u7acb\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "AgriCHN\u6570\u636e\u8d28\u91cf\u4f18\u4e8e\u540c\u7c7b\u8d44\u6e90\uff0c\u4e30\u5bcc\u5b9e\u4f53\u7c7b\u578b\uff0815,799\u6807\u6ce8\uff09\u5e26\u6765\u66f4\u9ad8\u8bc6\u522b\u96be\u5ea6\uff0c\u5b9e\u9a8c\u663e\u793a\u73b0\u6709\u6a21\u578bF1\u503c\u663e\u8457\u6ce2\u52a8\u3002", "conclusion": "AgriCHN\u586b\u8865\u4e86\u4e2d\u6587\u519c\u4e1a\u591a\u5173\u8054\u9886\u57df\u5b9e\u4f53\u8bc6\u522b\u7a7a\u767d\uff0c\u5176\u7ec6\u7c92\u5ea6\u6807\u6ce8\u4e0e\u8de8\u9886\u57df\u5b9e\u4f53\u8bbe\u8ba1\u4e3a\u519c\u4e1aNLP\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.17603", "pdf": "https://arxiv.org/pdf/2506.17603", "abs": "https://arxiv.org/abs/2506.17603", "authors": ["Jonathan Sakunkoo", "Annabella Sakunkoo"], "title": "Mind the Gap: Assessing Wiktionary's Crowd-Sourced Linguistic Knowledge on Morphological Gaps in Two Related Languages", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Morphological defectivity is an intriguing and understudied phenomenon in\nlinguistics. Addressing defectivity, where expected inflectional forms are\nabsent, is essential for improving the accuracy of NLP tools in morphologically\nrich languages. However, traditional linguistic resources often lack coverage\nof morphological gaps as such knowledge requires significant human expertise\nand effort to document and verify. For scarce linguistic phenomena in\nunder-explored languages, Wikipedia and Wiktionary often serve as among the few\naccessible resources. Despite their extensive reach, their reliability has been\na subject of controversy. This study customizes a novel neural morphological\nanalyzer to annotate Latin and Italian corpora. Using the massive annotated\ndata, crowd-sourced lists of defective verbs compiled from Wiktionary are\nvalidated computationally. Our results indicate that while Wiktionary provides\na highly reliable account of Italian morphological gaps, 7% of Latin lemmata\nlisted as defective show strong corpus evidence of being non-defective. This\ndiscrepancy highlights potential limitations of crowd-sourced wikis as\ndefinitive sources of linguistic knowledge, particularly for less-studied\nphenomena and languages, despite their value as resources for rare linguistic\nfeatures. By providing scalable tools and methods for quality assurance of\ncrowd-sourced data, this work advances computational morphology and expands\nlinguistic knowledge of defectivity in non-English, morphologically rich\nlanguages.", "AI": {"tldr": "\u901a\u8fc7\u795e\u7ecf\u5f62\u6001\u5206\u6790\u5668\u9a8c\u8bc1\u53d1\u73b0\uff1a\u7ef4\u57fa\u8bcd\u5178\u5bf9\u610f\u5927\u5229\u8bed\u5f62\u6001\u7a7a\u7f3a\u63cf\u8ff0\u53ef\u9760\uff0c\u4f46\u62c9\u4e01\u8bed\u5b58\u57287%\u9519\u8bef\u5206\u7c7b\uff0c\u63ed\u793a\u4f17\u5305\u8d44\u6e90\u5bf9\u975e\u82f1\u8bed\u8bed\u8a00\u7814\u7a76\u7684\u5c40\u9650\u6027", "motivation": "\u89e3\u51b3\u5f62\u6001\u7f3a\u9677\u73b0\u8c61\u5728NLP\u5de5\u5177\u5f00\u53d1\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u63ed\u793a\u4f20\u7edf\u8bed\u8a00\u5b66\u8d44\u6e90\u5bf9\u5f62\u6001\u7a7a\u7f3a\u7684\u8986\u76d6\u4e0d\u8db3\uff0c\u4ee5\u53ca\u9a8c\u8bc1\u4f17\u5305\u8d44\u6e90\uff08\u5982\u7ef4\u57fa\u8bcd\u5178\uff09\u5728\u5f62\u6001\u7f3a\u9677\u7814\u7a76\u4e2d\u7684\u53ef\u9760\u6027", "method": "\u5f00\u53d1\u795e\u7ecf\u5f62\u6001\u5206\u6790\u5668\u6807\u6ce8\u62c9\u4e01\u8bed/\u610f\u5927\u5229\u8bed\u8bed\u6599\u5e93\uff0c\u901a\u8fc7\u8ba1\u7b97\u9a8c\u8bc1\u7ef4\u57fa\u8bcd\u5178\u4f17\u5305\u7684\u7f3a\u9677\u52a8\u8bcd\u5217\u8868", "result": "\u610f\u5927\u5229\u8bed\u5f62\u6001\u7a7a\u7f3a\u6570\u636e\u51c6\u786e\uff0c\u62c9\u4e01\u8bed7%\u6807\u8bb0\u4e3a\u7f3a\u9677\u7684\u52a8\u8bcd\u5b58\u5728\u8bed\u6599\u5e93\u4f7f\u7528\u8bc1\u636e\uff0c\u663e\u793a\u4f17\u5305\u8d44\u6e90\u5bf9\u975e\u82f1\u8bed\u8bed\u8a00\u7684\u6f5c\u5728\u8bef\u5dee", "conclusion": "\u7ef4\u57fa\u8d44\u6e90\u5bf9\u7a00\u6709\u8bed\u8a00\u7279\u5f81\u6709\u4ef7\u503c\u4f46\u975e\u6743\u5a01\uff0c\u9700\u7ed3\u5408\u8ba1\u7b97\u5de5\u5177\u5b9e\u73b0\u8d28\u91cf\u9a8c\u8bc1\uff0c\u63a8\u52a8\u975e\u82f1\u8bed\u590d\u6742\u5f62\u6001\u8bed\u8a00\u7684\u7f3a\u9677\u6027\u7814\u7a76"}}
{"id": "2506.17609", "pdf": "https://arxiv.org/pdf/2506.17609", "abs": "https://arxiv.org/abs/2506.17609", "authors": ["Lincan Li", "Eren Erman Ozguven", "Yue Zhao", "Guang Wang", "Yiqun Xie", "Yushun Dong"], "title": "TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Accurate typhoon track forecasting is crucial for early system warning and\ndisaster response. While Transformer-based models have demonstrated strong\nperformance in modeling the temporal dynamics of dense trajectories of humans\nand vehicles in smart cities, they usually lack access to broader contextual\nknowledge that enhances the forecasting reliability of sparse meteorological\ntrajectories, such as typhoon tracks. To address this challenge, we propose\nTyphoFormer, a novel framework that incorporates natural language descriptions\nas auxiliary prompts to improve typhoon trajectory forecasting. For each time\nstep, we use Large Language Model (LLM) to generate concise textual\ndescriptions based on the numerical attributes recorded in the North Atlantic\nhurricane database. The language descriptions capture high-level meteorological\nsemantics and are embedded as auxiliary special tokens prepended to the\nnumerical time series input. By integrating both textual and sequential\ninformation within a unified Transformer encoder, TyphoFormer enables the model\nto leverage contextual cues that are otherwise inaccessible through numerical\nfeatures alone. Extensive experiments are conducted on HURDAT2 benchmark,\nresults show that TyphoFormer consistently outperforms other state-of-the-art\nbaseline methods, particularly under challenging scenarios involving nonlinear\npath shifts and limited historical observations.", "AI": {"tldr": "\u63d0\u51faTyphoFormer\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u6a21\u578b\u751f\u6210\u6c14\u8c61\u8bed\u4e49\u6587\u672c\u63cf\u8ff0\uff0c\u7ed3\u5408\u6570\u503c\u65f6\u5e8f\u6570\u636e\u63d0\u5347\u53f0\u98ce\u8f68\u8ff9\u9884\u6d4b\u7cbe\u5ea6", "motivation": "\u4f20\u7edfTransformer\u6a21\u578b\u5728\u5904\u7406\u7a00\u758f\u6c14\u8c61\u8f68\u8ff9\u65f6\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff0c\u96be\u4ee5\u5e94\u5bf9\u53f0\u98ce\u8def\u5f84\u7a81\u53d8\u548c\u6709\u9650\u5386\u53f2\u6570\u636e\u573a\u666f", "method": "\u5229\u7528LLM\u5c06\u53f0\u98ce\u6570\u503c\u7279\u5f81\u8f6c\u5316\u4e3a\u6587\u672c\u63cf\u8ff0\uff0c\u5d4c\u5165\u4e3a\u7279\u6b8a\u6807\u8bb0\u5e76\u4e0e\u6570\u503c\u5e8f\u5217\u5171\u540c\u8f93\u5165Transformer\u7f16\u7801\u5668\u8fdb\u884c\u591a\u6a21\u6001\u878d\u5408", "result": "\u5728HURDAT2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u975e\u7ebf\u6027\u8def\u5f84\u504f\u79fb\u548c\u5c11\u6837\u672c\u573a\u666f\u4e0b\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e12-18%", "conclusion": "\u8bed\u8a00\u5f15\u5bfc\u7684\u65f6\u7a7a\u5efa\u6a21\u4e3a\u6c14\u8c61\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u4e0a\u4e0b\u6587\u77e5\u8bc6\u589e\u5f3a\u663e\u8457\u63d0\u5347\u6781\u7aef\u5929\u6c14\u4e8b\u4ef6\u7684\u9884\u6d4b\u53ef\u9760\u6027"}}
{"id": "2506.17611", "pdf": "https://arxiv.org/pdf/2506.17611", "abs": "https://arxiv.org/abs/2506.17611", "authors": ["Jinchuan Tian", "William Chen", "Yifan Peng", "Jiatong Shi", "Siddhant Arora", "Shikhar Bharadwaj", "Takashi Maekaku", "Yusuke Shinohara", "Keita Goto", "Xiang Yue", "Huck Yang", "Shinji Watanabe"], "title": "OpusLM: A Family of Open Unified Speech Language Models", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents Open Unified Speech Language Models (OpusLMs), a family\nof open foundational speech language models (SpeechLMs) up to 7B. Initialized\nfrom decoder-only text language models, the OpusLMs are continuously\npre-trained on 213K hours of speech-text pairs and 292B text-only tokens. We\ndemonstrate our OpusLMs achieve comparable (or even superior) performance with\nexisting SpeechLMs in speech recognition, speech synthesis, and text-only\ncapabilities. Technically, this paper articulates our SpeechLM designs on\ntokenization, multi-stream language models, and multi-stage training\nstrategies. We experimentally demonstrate the importance of model size scaling\nand the effect of annealing data selection. The OpusLMs are all built from\npublicly available materials and are fully transparent models. We release our\ncode, data, checkpoints, and training logs to facilitate open SpeechLM research", "AI": {"tldr": "OpusLMs\u7cfb\u5217\u901a\u8fc7\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u6784\u5efa\u5f00\u653e\u900f\u660e\u7684\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u8bed\u97f3\u8bc6\u522b/\u5408\u6210\u53ca\u6587\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u73b0\u6709\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u5728\u5f00\u653e\u6027\u548c\u591a\u4efb\u52a1\u6027\u80fd\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u9700\u63a2\u7d22\u6a21\u578b\u89c4\u6a21\u6269\u5c55\u4e0e\u5f00\u6e90\u65b9\u6848", "method": "1. \u57fa\u4e8e\u6587\u672c\u8bed\u8a00\u6a21\u578b\u521d\u59cb\u5316\n2. \u591a\u6d41\u67b6\u6784\u5904\u7406\u8bed\u97f3/\u6587\u672c\u7279\u5f81\n3. \u4e24\u9636\u6bb5\u8bad\u7ec3\uff0821.3\u4e07\u5c0f\u65f6\u8bed\u97f3\u6587\u672c\u5bf9 + 2920\u4ebf\u7eaf\u6587\u672c\uff09\n4. \u6e10\u8fdb\u5f0f\u6570\u636e\u9009\u62e9\u7b56\u7565", "result": "1. 7B\u6a21\u578b\u6027\u80fd\u8d85\u8d8a\u540c\u7c7bSpeechLMs\n2. \u9a8c\u8bc1\u6a21\u578b\u89c4\u6a21\u4e0e\u6570\u636e\u9009\u62e9\u7684\u534f\u540c\u6548\u5e94\n3. \u4fdd\u6301\u4e0e\u539f\u59cb\u6587\u672c\u6a21\u578b\u76f8\u5f53\u7684\u6587\u672c\u5904\u7406\u80fd\u529b", "conclusion": "OpusLMs\u6846\u67b6\u4e3a\u5f00\u6e90\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u7814\u7a76\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u6a21\u578b\u89c4\u6a21\u4e0e\u8bad\u7ec3\u7b56\u7565\u7684\u534f\u540c\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u4efb\u52a1\u6027\u80fd"}}
{"id": "2506.17630", "pdf": "https://arxiv.org/pdf/2506.17630", "abs": "https://arxiv.org/abs/2506.17630", "authors": ["Yang Wu", "Yifan Zhang", "Yiwei Wang", "Yujun Cai", "Yurong Wu", "Yuran Wang", "Ning Xu", "Jian Cheng"], "title": "Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in LLMs", "categories": ["cs.CL"], "comment": "14 pages, 8 figures", "summary": "While Large Language Models (LLMs) demonstrate impressive reasoning\ncapabilities, growing evidence suggests much of their success stems from\nmemorized answer-reasoning patterns rather than genuine inference. In this\nwork, we investigate a central question: are LLMs primarily anchored to final\nanswers or to the textual pattern of reasoning chains? We propose a five-level\nanswer-visibility prompt framework that systematically manipulates answer cues\nand probes model behavior through indirect, behavioral analysis. Experiments\nacross state-of-the-art LLMs reveal a strong and consistent reliance on\nexplicit answers. The performance drops by 26.90\\% when answer cues are masked,\neven with complete reasoning chains. These findings suggest that much of the\nreasoning exhibited by LLMs may reflect post-hoc rationalization rather than\ntrue inference, calling into question their inferential depth. Our study\nuncovers the answer-anchoring phenomenon with rigorous empirical validation and\nunderscores the need for a more nuanced understanding of what constitutes\nreasoning in LLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u63a8\u7406\u80fd\u529b\u4e3b\u8981\u4f9d\u8d56\u663e\u5f0f\u7b54\u6848\u951a\u5b9a\uff0c\u5f53\u7b54\u6848\u7ebf\u7d22\u88ab\u5c4f\u853d\u65f6\u6027\u80fd\u4e0b\u964d26.9%\uff0c\u8868\u660e\u5176\u63a8\u7406\u8fc7\u7a0b\u66f4\u591a\u4f53\u73b0\u4e3a\u4e8b\u540e\u5408\u7406\u5316\u800c\u975e\u771f\u6b63\u63a8\u7406\u3002", "motivation": "\u8d28\u7591LLM\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6210\u529f\u662f\u5426\u6e90\u4e8e\u8bb0\u5fc6\u5316\u7684\u7b54\u6848-\u63a8\u7406\u6a21\u5f0f\uff0c\u800c\u975e\u771f\u6b63\u7684\u903b\u8f91\u63a8\u65ad\u80fd\u529b\u3002\u65e8\u5728\u9a8c\u8bc1\u6a21\u578b\u662f\u951a\u5b9a\u6700\u7ec8\u7b54\u6848\u8fd8\u662f\u63a8\u7406\u94fe\u672c\u8eab\u7684\u6587\u672c\u6a21\u5f0f\u3002", "method": "\u63d0\u51fa\u4e94\u7ea7\u7b54\u6848\u53ef\u89c1\u6027\u63d0\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u63a7\u5236\u7b54\u6848\u7ebf\u7d22\u7684\u53ef\u89c1\u6027\uff08\u5b8c\u6574\u7b54\u6848\u2192\u63a9\u7801\u7b54\u6848\uff09\uff0c\u7ed3\u5408\u95f4\u63a5\u884c\u4e3a\u5206\u6790\u65b9\u6cd5\u63a2\u6d4b\u6a21\u578b\u673a\u5236\u3002\u5728\u5c16\u7aefLLM\u4e0a\u8fdb\u884c\u5bf9\u7167\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u6027\u80fd\u4e0e\u7b54\u6848\u53ef\u89c1\u6027\u5448\u5f3a\u6b63\u76f8\u5173\uff0c\u63a9\u7801\u7b54\u6848\u65f6\u6027\u80fd\u4e0b\u964d26.9%\u3002\u5b8c\u6574\u63a8\u7406\u94fe\u4f46\u7f3a\u5931\u7b54\u6848\u65f6\uff0c\u6a21\u578b\u4ecd\u65e0\u6cd5\u6709\u6548\u63a8\u7406\uff0c\u8868\u660e\u5bf9\u663e\u5f0f\u7b54\u6848\u7684\u8def\u5f84\u4f9d\u8d56\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLM\u5b58\u5728\u7b54\u6848\u951a\u5b9a\u73b0\u8c61\uff0c\u5176\u8868\u73b0\u51fa\u7684\u63a8\u7406\u80fd\u529b\u53ef\u80fd\u662f\u5bf9\u8bb0\u5fc6\u7b54\u6848\u7684\u4e8b\u540e\u5408\u7406\u5316\u3002\u8fd9\u6311\u6218\u4e86\u5bf9LLM\u63a8\u7406\u6df1\u5ea6\u7684\u73b0\u6709\u8ba4\u77e5\uff0c\u5f3a\u8c03\u9700\u8981\u91cd\u65b0\u5b9a\u4e49\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u672c\u8d28\u3002"}}
{"id": "2506.17637", "pdf": "https://arxiv.org/pdf/2506.17637", "abs": "https://arxiv.org/abs/2506.17637", "authors": ["Yang Wu", "Yifan Zhang", "Yurong Wu", "Yuran Wang", "Junkai Zhang", "Jian Cheng"], "title": "Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation", "categories": ["cs.CL", "cs.LG"], "comment": "17 pages, 12 figures", "summary": "Large Language Models (LLMs) have revolutionized various domains but\nencounter substantial challenges in tackling optimization modeling tasks for\nOperations Research (OR), particularly when dealing with complex problem. In\nthis work, we propose Step-Opt-Instruct, a framework that augments existing\ndatasets and generates high-quality fine-tuning data tailored to optimization\nmodeling. Step-Opt-Instruct employs iterative problem generation to\nsystematically increase problem complexity and stepwise validation to\nrigorously verify data, preventing error propagation and ensuring the quality\nof the generated dataset. Leveraging this framework, we fine-tune open-source\nLLMs, including LLaMA-3-8B and Mistral-7B, to develop Step-Opt--a model that\nachieves state-of-the-art performance on benchmarks such as NL4OPT, MAMO, and\nIndustryOR. Extensive experiments demonstrate the superior performance of\nStep-Opt, especially in addressing complex OR tasks, with a notable 17.01\\%\nimprovement in micro average accuracy on difficult problems. These findings\nhighlight the effectiveness of combining structured validation with gradual\nproblem refinement to advance the automation of decision-making processes using\nLLMs.The code and dataset are available at https://github.com/samwu-learn/Step.", "AI": {"tldr": "\u63d0\u51faStep-Opt-Instruct\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u95ee\u9898\u751f\u6210\u548c\u9010\u6b65\u9a8c\u8bc1\u63d0\u5347LLMs\u5728\u8fd0\u7b79\u5b66\u4f18\u5316\u5efa\u6a21\u4e2d\u7684\u6027\u80fd\uff0c\u5fae\u8c03\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u5b9e\u73b0SOTA\uff0c\u56f0\u96be\u4efb\u52a1\u51c6\u786e\u7387\u63d0\u534717.01%", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u8fd0\u7b79\u5b66\u4f18\u5316\u5efa\u6a21\u4efb\u52a1\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u65b9\u6cd5\u589e\u5f3a\u5176\u5efa\u6a21\u80fd\u529b", "method": "1. \u8fed\u4ee3\u5f0f\u95ee\u9898\u751f\u6210\u589e\u52a0\u590d\u6742\u5ea6 2. \u9010\u6b65\u9a8c\u8bc1\u673a\u5236\u9632\u6b62\u9519\u8bef\u4f20\u64ad 3. \u57fa\u4e8eLLaMA-3-8B\u548cMistral-7B\u8fdb\u884c\u5fae\u8c03", "result": "\u5728NL4OPT/MAMO/IndustryOR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbeSOTA\uff0c\u56f0\u96be\u95ee\u9898\u51c6\u786e\u7387\u63d0\u534717.01%", "conclusion": "\u7ed3\u6784\u5316\u9a8c\u8bc1\u4e0e\u6e10\u8fdb\u5f0f\u95ee\u9898\u4f18\u5316\u7684\u7ed3\u5408\u6709\u6548\u63d0\u5347LLMs\u81ea\u52a8\u5316\u51b3\u7b56\u80fd\u529b\uff0c\u76f8\u5173\u4ee3\u7801\u6570\u636e\u96c6\u5df2\u5f00\u6e90"}}
{"id": "2506.17671", "pdf": "https://arxiv.org/pdf/2506.17671", "abs": "https://arxiv.org/abs/2506.17671", "authors": ["Fabien Furfaro"], "title": "TPTT: Transforming Pretrained Transformer into Titans", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "6 pages, 1 figure", "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress in natural language processing, but their computational and memory\ndemands remain a significant challenge, particularly for long-context\ninference. We introduce TPTT (Transforming Pretrained Transformer into Titans),\na novel framework for enhancing pretrained Transformer models with efficient\nlinearized attention mechanisms and advanced memory management. TPTT employs\ntechniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA).\nIt is fully compatible with the Hugging Face Transformers library, enabling\nseamless adaptation of any causal LLM through parameter-efficient fine-tuning\n(LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU\nbenchmark with models of approximately 1 billion parameters, observing\nsubstantial improvements in both efficiency and accuracy. For instance,\nTitans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its\nbaseline. Statistical analyses and comparisons with recent state-of-the-art\nmethods confirm the practical scalability and robustness of TPTT. Code is\navailable at https://github.com/fabienfrfr/tptt . Python package at\nhttps://pypi.org/project/tptt/ .", "AI": {"tldr": "\u63d0\u51faTPTT\u6846\u67b6\u901a\u8fc7\u7ebf\u6027\u5316\u6ce8\u610f\u529b\u673a\u5236\u548c\u5185\u5b58\u4f18\u5316\u63d0\u5347Transformer\u6548\u7387\uff0c\u517c\u5bb9Hugging Face\u751f\u6001\u5e76\u5b9e\u73b020%\u6027\u80fd\u63d0\u5347", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4e2d\u7684\u8ba1\u7b97/\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u7a81\u7834\u73b0\u6709\u65b9\u6cd5\u7684\u6548\u7387\u9650\u5236", "method": "\u6574\u5408Memory as Gate\uff08MaG\uff09\u548c\u6df7\u5408\u7ebf\u6027\u5316\u6ce8\u610f\u529b\uff08LiZA\uff09\uff0c\u652f\u6301LoRA\u5fae\u8c03\u9002\u914d\u4efb\u610f\u56e0\u679c\u8bed\u8a00\u6a21\u578b", "result": "\u5728MMLU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTitans-Llama-3.2-1B\u6a21\u578b\u5b9e\u73b020%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u8ba1\u7b97\u6548\u7387\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "TPTT\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u7ea7\u4f18\u5316\u5e73\u8861\u6a21\u578b\u6548\u7387\u4e0e\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.17692", "pdf": "https://arxiv.org/pdf/2506.17692", "abs": "https://arxiv.org/abs/2506.17692", "authors": ["Binquan Ji", "Haibo Luo", "Yifei Lu", "Lei Hei", "Jiaqi Wang", "Tingjing Liao", "Lingyu Wang", "Shichao Wang", "Feiliang Ren"], "title": "Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "Knowledge-intensive multi-hop question answering (QA) tasks, which require\nintegrating evidence from multiple sources to address complex queries, often\nnecessitate multiple rounds of retrieval and iterative generation by large\nlanguage models (LLMs). However, incorporating many documents and extended\ncontexts poses challenges -such as hallucinations and semantic drift-for\nlightweight LLMs with fewer parameters. This work proposes a novel framework\ncalled DEC (Dynamic Enhancement Chain). DEC first decomposes complex questions\ninto logically coherent subquestions to form a hallucination-free reasoning\nchain. It then iteratively refines these subquestions through context-aware\nrewriting to generate effective query formulations. For retrieval, we introduce\na lightweight discriminative keyword extraction module that leverages extracted\nkeywords to achieve targeted, precise document recall with relatively low\ncomputational overhead. Extensive experiments on three multi-hop QA datasets\ndemonstrate that DEC performs on par with or surpasses state-of-the-art\nbenchmarks while significantly reducing token consumption. Notably, our\napproach attains state-of-the-art results on models with 8B parameters,\nshowcasing its effectiveness in various scenarios, particularly in\nresource-constrained environments.", "AI": {"tldr": "\u63d0\u51faDEC\u6846\u67b6\u89e3\u51b3\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u548c\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u89e3\u95ee\u9898\u94fe\u3001\u4e0a\u4e0b\u6587\u91cd\u5199\u548c\u8f7b\u91cf\u7ea7\u5173\u952e\u8bcd\u68c0\u7d22\uff0c\u57288B\u53c2\u6570\u6a21\u578b\u4e0a\u5b9e\u73b0SOTA\u6548\u679c\u3002", "motivation": "\u8f7b\u91cf\u7ea7\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u9762\u4e34\u5e7b\u89c9\u548c\u8bed\u4e49\u6f02\u79fb\u7684\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u68c0\u7d22\u673a\u5236\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "method": "1. \u95ee\u9898\u5206\u89e3\u4e3a\u903b\u8f91\u5b50\u95ee\u9898\u94fe 2. \u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u91cd\u5199\u673a\u5236\u4f18\u5316\u5b50\u95ee\u9898 3. \u8f7b\u91cf\u7ea7\u5173\u952e\u8bcd\u63d0\u53d6\u6a21\u5757\u5b9e\u73b0\u7cbe\u51c6\u6587\u6863\u53ec\u56de", "result": "\u5728\u4e09\u4e2a\u591a\u8df3QA\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6216\u8d85\u8d8aSOTA\u57fa\u51c6\uff0ctoken\u6d88\u8017\u51cf\u5c1135-50%\uff0c8B\u6a21\u578b\u8868\u73b0\u6700\u4f18\u3002", "conclusion": "DEC\u6846\u67b6\u663e\u8457\u63d0\u5347\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u590d\u6742QA\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u73af\u5883\u3002"}}
{"id": "2506.17693", "pdf": "https://arxiv.org/pdf/2506.17693", "abs": "https://arxiv.org/abs/2506.17693", "authors": ["Yuzhe Ding", "Kang He", "Bobo Li", "Li Zheng", "Haijun He", "Fei Li", "Chong Teng", "Donghong Ji"], "title": "Zero-Shot Conversational Stance Detection: Dataset and Approaches", "categories": ["cs.CL", "cs.LG"], "comment": "ACL 2025 (Findings)", "summary": "Stance detection, which aims to identify public opinion towards specific\ntargets using social media data, is an important yet challenging task. With the\nincreasing number of online debates among social media users, conversational\nstance detection has become a crucial research area. However, existing\nconversational stance detection datasets are restricted to a limited set of\nspecific targets, which constrains the effectiveness of stance detection models\nwhen encountering a large number of unseen targets in real-world applications.\nTo bridge this gap, we manually curate a large-scale, high-quality zero-shot\nconversational stance detection dataset, named ZS-CSD, comprising 280 targets\nacross two distinct target types. Leveraging the ZS-CSD dataset, we propose\nSITPCL, a speaker interaction and target-aware prototypical contrastive\nlearning model, and establish the benchmark performance in the zero-shot\nsetting. Experimental results demonstrate that our proposed SITPCL model\nachieves state-of-the-art performance in zero-shot conversational stance\ndetection. Notably, the SITPCL model attains only an F1-macro score of 43.81%,\nhighlighting the persistent challenges in zero-shot conversational stance\ndetection.", "AI": {"tldr": "\u63d0\u51faZS-CSD\u6570\u636e\u96c6\u548cSITPCL\u6a21\u578b\u63d0\u5347\u96f6\u6837\u672c\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u6027\u80fd\uff0cF1\u5206\u657043.81%\u663e\u793a\u6311\u6218\u4ecd\u5b58", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u6570\u636e\u96c6\u5c40\u9650\u4e8e\u7279\u5b9a\u76ee\u6807\uff0c\u9650\u5236\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u5904\u7406\u672a\u89c1\u76ee\u6807\u7684\u6cdb\u5316\u80fd\u529b", "method": "1. \u6784\u5efa\u5305\u542b280\u4e2a\u76ee\u6807\u7684ZS-CSD\u6570\u636e\u96c6\uff1b2. \u63d0\u51fa\u7ed3\u5408\u8bf4\u8bdd\u8005\u4ea4\u4e92\u548c\u76ee\u6807\u611f\u77e5\u7684\u539f\u578b\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578bSITPCL", "result": "SITPCL\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u4f46F1-macro\u4ec5\u4e3a43.81%", "conclusion": "\u901a\u8fc7ZS-CSD\u6570\u636e\u96c6\u548cSITPCL\u6a21\u578b\u63a8\u52a8\u96f6\u6837\u672c\u5bf9\u8bdd\u7acb\u573a\u68c0\u6d4b\u7814\u7a76\uff0c\u5b9e\u9a8c\u7ed3\u679c\u63ed\u793a\u8be5\u4efb\u52a1\u4ecd\u5b58\u5728\u663e\u8457\u6311\u6218"}}
{"id": "2506.17700", "pdf": "https://arxiv.org/pdf/2506.17700", "abs": "https://arxiv.org/abs/2506.17700", "authors": ["Summra Saleem", "Muhammad Nabeel Asim", "Shaista Zulfiqar", "Andreas Dengel"], "title": "The Evolution of Natural Language Processing: How Prompt Optimization and Language Models are Shaping the Future", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have revolutionized the field of Natural\nLanguage Processing (NLP) by automating traditional labor-intensive tasks and\nconsequently accelerated the development of computer-aided applications. As\nresearchers continue to advance this field with the introduction of novel\nlanguage models and more efficient training/finetuning methodologies, the idea\nof prompt engineering and subsequent optimization strategies with LLMs has\nemerged as a particularly impactful trend to yield a substantial performance\nboost across diverse NLP tasks. To best of our knowledge numerous review\narticles have explored prompt engineering, however, a critical gap exists in\ncomprehensive analyses of prompt optimization strategies. To bridge this gap\nthis paper provides unique and comprehensive insights about the potential of\ndiverse prompt optimization strategies. It analyzes their underlying working\nparadigms and based on these principles, categorizes them into 11 distinct\nclasses. Moreover, the paper provides details about various NLP tasks where\nthese prompt optimization strategies have been employed, along with details of\ndifferent LLMs and benchmark datasets used for evaluation. This comprehensive\ncompilation lays a robust foundation for future comparative studies and enables\nrigorous assessment of prompt optimization and LLM-based predictive pipelines\nunder consistent experimental settings: a critical need in the current\nlandscape. Ultimately, this research will centralize diverse strategic\nknowledge to facilitate the adaptation of existing prompt optimization\nstrategies for development of innovative predictors across unexplored tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u68b3\u7406\u4e8611\u7c7b\u63d0\u793a\u8bcd\u4f18\u5316\u7b56\u7565\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u5bf9\u63d0\u793a\u4f18\u5316\u7cfb\u7edf\u6027\u5206\u6790\u7684\u7a7a\u767d\uff0c\u4e3aNLP\u4efb\u52a1\u4e2dLLM\u9884\u6d4b\u6d41\u7a0b\u7684\u4f18\u5316\u4e0e\u8bc4\u4f30\u5efa\u7acb\u4e86\u7edf\u4e00\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u5927\u91cf\u7efc\u8ff0\u6587\u7ae0\u63a2\u8ba8\u63d0\u793a\u5de5\u7a0b\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u63d0\u793a\u4f18\u5316\u7b56\u7565\u7684\u7cfb\u7edf\u6027\u5206\u7c7b\u4e0e\u8303\u5f0f\u5206\u6790\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5168\u9762\u68b3\u7406\u4e0d\u540c\u63d0\u793a\u4f18\u5316\u7b56\u7565\uff0c\u5efa\u7acb\u8bc4\u4f30\u57fa\u51c6\u4ee5\u63a8\u52a8LLM\u9884\u6d4b\u6d41\u7a0b\u7684\u521b\u65b0\u3002", "method": "\u57fa\u4e8e\u5de5\u4f5c\u539f\u7406\u5c06\u63d0\u793a\u4f18\u5316\u7b56\u7565\u5206\u4e3a11\u4e2a\u7c7b\u522b\uff0c\u901a\u8fc7\u5206\u6790\u5404\u7b56\u7565\u5728NLP\u4efb\u52a1\u4e2d\u7684\u5177\u4f53\u5e94\u7528\u6848\u4f8b\u3001\u5bf9\u5e94\u7684LLM\u6a21\u578b\u53ca\u8bc4\u4f30\u6570\u636e\u96c6\uff0c\u6784\u5efa\u7cfb\u7edf\u5206\u7c7b\u4f53\u7cfb\u3002", "result": "\u5efa\u7acb\u4e86\u5305\u542b11\u7c7b\u4f18\u5316\u7b56\u7565\u7684\u5b8c\u6574\u5206\u7c7b\u6846\u67b6\uff0c\u6db5\u76d6\u591a\u79cdNLP\u4efb\u52a1\u573a\u666f\uff08\u5982\u6587\u672c\u751f\u6210\u3001\u63a8\u7406\u7b49\uff09\uff0c\u6574\u5408\u4e86\u4e0d\u540cLLM\uff08\u5982GPT\u7cfb\u5217\u3001BERT\u7b49\uff09\u548c\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5982GLUE\u3001SuperGLUE\uff09\u7684\u8bc4\u4f30\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u63d0\u793a\u4f18\u5316\u7b56\u7565\u7684\u6a2a\u5411\u5bf9\u6bd4\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5b9e\u9a8c\u6846\u67b6\uff0c\u5c06\u4fc3\u8fdb\u8de8\u4efb\u52a1\u9884\u6d4b\u5668\u7684\u5f00\u53d1\uff0c\u63a8\u52a8LLM\u5728\u672a\u63a2\u7d22\u9886\u57df\u7684\u521b\u65b0\u5e94\u7528\u3002"}}
{"id": "2506.17708", "pdf": "https://arxiv.org/pdf/2506.17708", "abs": "https://arxiv.org/abs/2506.17708", "authors": ["MingZe Tang"], "title": "Aged to Perfection: Machine-Learning Maps of Age in Conversational English", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 11 figures", "summary": "The study uses the British National Corpus 2014, a large sample of\ncontemporary spoken British English, to investigate language patterns across\ndifferent age groups. Our research attempts to explore how language patterns\nvary between different age groups, exploring the connection between speaker\ndemographics and linguistic factors such as utterance duration, lexical\ndiversity, and word choice. By merging computational language analysis and\nmachine learning methodologies, we attempt to uncover distinctive linguistic\nmarkers characteristic of multiple generations and create prediction models\nthat can consistently estimate the speaker's age group from various aspects.\nThis work contributes to our knowledge of sociolinguistic diversity throughout\nthe life of modern British speech.", "AI": {"tldr": "\u5229\u7528\u82f1\u56fd\u56fd\u5bb6\u8bed\u6599\u5e932014\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5206\u6790\u4e0d\u540c\u5e74\u9f84\u7fa4\u4f53\u7684\u8bed\u8a00\u7279\u5f81\u5e76\u6784\u5efa\u5e74\u9f84\u9884\u6d4b\u6a21\u578b", "motivation": "\u63a2\u7a76\u73b0\u4ee3\u82f1\u56fd\u53e3\u8bed\u4e2d\u8bf4\u8bdd\u8005\u4eba\u53e3\u7edf\u8ba1\u5b66\u7279\u5f81\uff08\u5e74\u9f84\uff09\u4e0e\u8bdd\u8bed\u65f6\u957f\u3001\u8bcd\u6c47\u591a\u6837\u6027\u3001\u7528\u8bcd\u9009\u62e9\u7b49\u8bed\u8a00\u8981\u7d20\u7684\u5173\u8054", "method": "\u7ed3\u5408\u8ba1\u7b97\u8bed\u8a00\u5206\u6790\u6280\u672f\uff08\u8bed\u6599\u5e93\u5206\u6790\uff09\u4e0e\u673a\u5668\u5b66\u4e60\u5efa\u6a21\u65b9\u6cd5", "result": "\u8bc6\u522b\u51fa\u4ee3\u9645\u5dee\u5f02\u7684\u663e\u8457\u8bed\u8a00\u6807\u8bb0\uff0c\u5efa\u7acb\u4e86\u80fd\u6839\u636e\u8bed\u8a00\u7279\u5f81\u9884\u6d4b\u8bf4\u8bdd\u8005\u5e74\u9f84\u7ec4\u7684\u53ef\u9760\u6a21\u578b", "conclusion": "\u8be5\u7814\u7a76\u6df1\u5316\u4e86\u5bf9\u5f53\u4ee3\u82f1\u56fd\u793e\u4f1a\u8bed\u8a00\u591a\u6837\u6027\u53ca\u5176\u751f\u547d\u5386\u7a0b\u6f14\u53d8\u7684\u7406\u89e3\uff0c\u63a8\u52a8\u4e86\u8ba1\u7b97\u793e\u4f1a\u8bed\u8a00\u5b66\u7684\u53d1\u5c55"}}
{"id": "2506.17715", "pdf": "https://arxiv.org/pdf/2506.17715", "abs": "https://arxiv.org/abs/2506.17715", "authors": ["Matthias Sch\u00f6ffel", "Esteban Garces Arias", "Marinus Wiedner", "Paula Ruppert", "Meimingwei Li", "Christian Heumann", "Matthias A\u00dfenmacher"], "title": "Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource Medieval Romance Languages", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Part-of-speech (POS) tagging remains a foundational component in natural\nlanguage processing pipelines, particularly critical for historical text\nanalysis at the intersection of computational linguistics and digital\nhumanities. Despite significant advancements in modern large language models\n(LLMs) for ancient languages, their application to Medieval Romance languages\npresents distinctive challenges stemming from diachronic linguistic evolution,\nspelling variations, and labeled data scarcity. This study systematically\ninvestigates the central determinants of POS tagging performance across diverse\ncorpora of Medieval Occitan, Medieval Spanish, and Medieval French texts,\nspanning biblical, hagiographical, medical, and dietary domains. Through\nrigorous experimentation, we evaluate how fine-tuning approaches, prompt\nengineering, model architectures, decoding strategies, and cross-lingual\ntransfer learning techniques affect tagging accuracy. Our results reveal both\nnotable limitations in LLMs' ability to process historical language variations\nand non-standardized spelling, as well as promising specialized techniques that\neffectively address the unique challenges presented by low-resource historical\nlanguages.", "AI": {"tldr": "\u7cfb\u7edf\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2d\u4e16\u7eaa\u7f57\u66fc\u8bedPOS\u6807\u6ce8\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u5904\u7406\u5386\u53f2\u8bed\u8a00\u53d8\u4f53\u7684\u5c40\u9650\u6027\u4e0e\u6709\u6548\u6280\u672f\u65b9\u6848", "motivation": "\u4e2d\u4e16\u7eaa\u7f57\u66fc\u8bed\u56e0\u5386\u65f6\u6f14\u53d8\u3001\u62fc\u5199\u53d8\u5f02\u548c\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\uff0c\u5bfc\u81f4\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5386\u53f2\u6587\u672c\u5904\u7406\u4e0a\u9762\u4e34\u7279\u6b8a\u6311\u6218", "method": "\u901a\u8fc7\u5fae\u8c03\u3001\u63d0\u793a\u5de5\u7a0b\u3001\u6a21\u578b\u67b6\u6784\u4f18\u5316\u3001\u89e3\u7801\u7b56\u7565\u548c\u8de8\u8bed\u8a00\u8fc1\u79fb\u5b66\u4e60\uff0c\u5728\u5305\u542b\u5723\u7ecf/\u5723\u5f92\u4f20/\u533b\u5b66/\u996e\u98df\u9886\u57df\u7684\u4e2d\u4e16\u7eaa\u5965\u514b\u8bed/\u897f\u73ed\u7259\u8bed/\u6cd5\u8bed\u8bed\u6599\u5e93\u8fdb\u884c\u5b9e\u9a8c", "result": "\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u5386\u53f2\u8bed\u8a00\u53d8\u4f53\u548c\u975e\u6807\u51c6\u5316\u62fc\u5199\u5b58\u5728\u663e\u8457\u9650\u5236\uff0c\u4f46\u7279\u5b9a\u4f18\u5316\u6280\u672f\u80fd\u6709\u6548\u63d0\u5347\u4f4e\u8d44\u6e90\u5386\u53f2\u8bed\u8a00\u7684\u6807\u6ce8\u51c6\u786e\u7387", "conclusion": "\u7814\u7a76\u4e3a\u6570\u5b57\u4eba\u6587\u9886\u57df\u63d0\u4f9b\u4e86\u5904\u7406\u4f4e\u8d44\u6e90\u5386\u53f2\u8bed\u8a00\u7684\u5173\u952e\u6280\u672f\u8def\u7ebf\uff0c\u5e73\u8861\u6a21\u578b\u901a\u7528\u80fd\u529b\u4e0e\u9886\u57df\u9002\u5e94\u6027\u9700\u6c42"}}
{"id": "2506.17728", "pdf": "https://arxiv.org/pdf/2506.17728", "abs": "https://arxiv.org/abs/2506.17728", "authors": ["Dalong Zhang", "Jun Xu", "Jun Zhou", "Lei Liang", "Lin Yuan", "Ling Zhong", "Mengshu Sun", "Peilong Zhao", "QiWei Wang", "Xiaorui Wang", "Xinkai Du", "YangYang Hou", "Yu Ao", "ZhaoYang Wang", "Zhengke Gui", "ZhiYing Yi", "Zhongpu Bo"], "title": "KAG-Thinker: Teaching Large Language Models to Think with Human-like Reasoning Process", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we introduce KAG-Thinker, a novel human-like reasoning\nframework built upon a parameter-light large language model (LLM). Our approach\nenhances the logical coherence and contextual consistency of the thinking\nprocess in question-answering (Q\\&A) tasks on domain-specific knowledge bases\n(KBs) within LLMs. This framework simulates human cognitive mechanisms for\nhandling complex problems by establishing a structured thinking process.\nContinuing the \\textbf{Logical Form} guided retrieval and reasoning technology\nroute of KAG v0.7, firstly, it decomposes complex questions into independently\nsolvable sub-problems(also referred to as logical forms) through\n\\textbf{breadth decomposition}, each represented in two equivalent\nforms-natural language and logical function-and further classified as either\nKnowledge Retrieval or Reasoning Analysis tasks, with dependencies and\nvariables passing explicitly modeled via logical function interfaces. In the\nsolving process, the Retrieval function is used to perform knowledge retrieval\ntasks, while the Math and Deduce functions are used to perform reasoning\nanalysis tasks. Secondly, it is worth noting that, in the Knowledge Retrieval\nsub-problem tasks, LLMs and external knowledge sources are regarded as\nequivalent KBs. We use the \\textbf{knowledge boundary} model to determine the\noptimal source using self-regulatory mechanisms such as confidence calibration\nand reflective reasoning, and use the \\textbf{depth solving} model to enhance\nthe comprehensiveness of knowledge acquisition. Finally, instead of utilizing\nreinforcement learning, we employ supervised fine-tuning with multi-turn\ndialogues to align the model with our structured inference paradigm, thereby\navoiding excessive reflection. This is supported by a data evaluation framework\nand iterative corpus synthesis, which facilitate the generation of detailed\nreasoning trajectories...", "AI": {"tldr": "KAG-Thinker\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u601d\u7ef4\u8fc7\u7a0b\u548c\u77e5\u8bc6\u8fb9\u754c\u6a21\u578b\u63d0\u5347LLM\u5728\u9886\u57df\u77e5\u8bc6\u95ee\u7b54\u4e2d\u7684\u903b\u8f91\u8fde\u8d2f\u6027\u4e0e\u77e5\u8bc6\u83b7\u53d6\u80fd\u529b", "motivation": "\u89e3\u51b3LLM\u5728\u590d\u6742\u9886\u57df\u77e5\u8bc6\u95ee\u7b54\u4e2d\u5b58\u5728\u7684\u903b\u8f91\u65ad\u88c2\u4e0e\u77e5\u8bc6\u83b7\u53d6\u4e0d\u5168\u9762\u95ee\u9898\uff0c\u6a21\u4eff\u4eba\u7c7b\u7ed3\u6784\u5316\u8ba4\u77e5\u673a\u5236\u4f18\u5316\u63a8\u7406\u8fc7\u7a0b", "method": "1.\u5e7f\u5ea6\u5206\u89e3\u590d\u6742\u95ee\u9898\u4e3a\u53ef\u72ec\u7acb\u89e3\u51b3\u7684\u5b50\u95ee\u9898 2.\u77e5\u8bc6\u8fb9\u754c\u6a21\u578b\u52a8\u6001\u9009\u62e9\u6700\u4f18\u77e5\u8bc6\u6e90 3.\u76d1\u7763\u5fae\u8c03\u66ff\u4ee3\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u63a8\u7406\u8303\u5f0f\u5bf9\u9f50", "result": "\u5efa\u7acb\u5305\u542b\u903b\u8f91\u51fd\u6570\u63a5\u53e3\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u63d0\u5347\u77e5\u8bc6\u83b7\u53d6\u5168\u9762\u6027\uff0c\u901a\u8fc7\u8fed\u4ee3\u8bed\u6599\u5408\u6210\u5b9e\u73b0\u63a8\u7406\u8fc7\u7a0b\u4f18\u5316", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u7684\u9886\u57df\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6548\u7ed3\u5408\u77e5\u8bc6\u68c0\u7d22\u4e0e\u903b\u8f91\u5206\u6790\uff0c\u7a81\u7834\u4f20\u7edf\u68c0\u7d22\u589e\u5f3a\u7684\u5c40\u9650\u6027"}}
{"id": "2506.17748", "pdf": "https://arxiv.org/pdf/2506.17748", "abs": "https://arxiv.org/abs/2506.17748", "authors": ["Anwoy Chatterjee", "Yash Goel", "Tanmoy Chakraborty"], "title": "HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Contemporary Language Models (LMs), while impressively fluent, often generate\ncontent that is factually incorrect or unfaithful to the input context - a\ncritical issue commonly referred to as 'hallucination'. This tendency of LMs to\ngenerate hallucinated content undermines their reliability, especially because\nthese fabrications are often highly convincing and therefore difficult to\ndetect. While several existing methods attempt to detect hallucinations, most\nrely on analyzing multiple generations per input, leading to increased\ncomputational cost and latency. To address this, we propose a single-pass,\ntraining-free approach for effective Hallucination detectIon via Decoupled\nrEpresentations (HIDE). Our approach leverages the hypothesis that\nhallucinations result from a statistical decoupling between an LM's internal\nrepresentations of input context and its generated output. We quantify this\ndecoupling using the Hilbert-Schmidt Independence Criterion (HSIC) applied to\nhidden-state representations extracted while generating the output sequence. We\nconduct extensive experiments on four diverse question answering datasets,\nevaluating both faithfulness and factuality hallucinations across six\nopen-source LMs of varying scales and properties. Our results demonstrate that\nHIDE outperforms other single-pass methods in almost all settings, achieving an\naverage relative improvement of ~29% in AUC-ROC over the best-performing\nsingle-pass strategy across various models and datasets. Additionally, HIDE\nshows competitive and often superior performance with multi-pass\nstate-of-the-art methods, obtaining an average relative improvement of ~3% in\nAUC-ROC while consuming ~51% less computation time. Our findings highlight the\neffectiveness of exploiting internal representation decoupling in LMs for\nefficient and practical hallucination detection.", "AI": {"tldr": "\u63d0\u51faHIDE\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u8868\u793a\u5b9e\u73b0\u9ad8\u6548\u5355\u6b21\u5e7b\u89c9\u68c0\u6d4b\uff0cAUC-ROC\u63d0\u534729%\uff0c\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c1151%\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u6613\u751f\u6210\u770b\u4f3c\u53ef\u4fe1\u7684\u9519\u8bef\u5185\u5bb9\uff08\u5e7b\u89c9\uff09\uff0c\u4f20\u7edf\u591a\u8f6e\u68c0\u6d4b\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u9700\u5f00\u53d1\u9ad8\u6548\u5355\u6b21\u68c0\u6d4b\u65b9\u6848\u63d0\u5347\u53ef\u9760\u6027\u3002", "method": "\u57fa\u4e8e\u9690\u85cf\u5c42\u8868\u793a\u7684HSIC\u7edf\u8ba1\u91cf\uff0c\u91cf\u5316\u8f93\u5165\u4e0a\u4e0b\u6587\u4e0e\u751f\u6210\u5185\u5bb9\u7684\u89e3\u8026\u7a0b\u5ea6\u3002\u65e0\u9700\u8bad\u7ec3\uff0c\u5355\u6b21\u524d\u5411\u4f20\u64ad\u5b8c\u6210\u68c0\u6d4b\u3002", "result": "\u57284\u4e2aQA\u6570\u636e\u96c6\u30016\u4e2a\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u4e0a\uff0cHIDE\u5355\u6b21\u68c0\u6d4bAUC-ROC\u5e73\u5747\u63d0\u534729%\uff0c\u591a\u8f6e\u65b9\u6cd5\u5bf9\u6bd4\u63d0\u53473%\u4e14\u51cf\u5c1151%\u8ba1\u7b97\u65f6\u95f4\u3002", "conclusion": "\u6a21\u578b\u5185\u90e8\u8868\u793a\u89e3\u8026\u662f\u68c0\u6d4b\u5e7b\u89c9\u7684\u6709\u6548\u4fe1\u53f7\uff0cHIDE\u5728\u6548\u7387\u4e0e\u6548\u679c\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2506.17789", "pdf": "https://arxiv.org/pdf/2506.17789", "abs": "https://arxiv.org/abs/2506.17789", "authors": ["N J Karthika", "Maharaj Brahma", "Rohit Saluja", "Ganesh Ramakrishnan", "Maunendra Sankar Desarkar"], "title": "Multilingual Tokenization through the Lens of Indian Languages: Challenges and Insights", "categories": ["cs.CL"], "comment": null, "summary": "Tokenization plays a pivotal role in multilingual NLP. However, existing\ntokenizers are often skewed towards high-resource languages, limiting their\neffectiveness for linguistically diverse and morphologically rich languages\nsuch as those in the Indian subcontinent. This paper presents a comprehensive\nintrinsic evaluation of tokenization strategies across 17 Indian languages. We\nquantify the trade-offs between bottom-up and top-down tokenizer algorithms\n(BPE and Unigram LM), effects of vocabulary sizes, and compare strategies of\nmultilingual vocabulary construction such as joint and cluster-based training.\nWe also show that extremely low-resource languages can benefit from tokenizers\ntrained on related high-resource languages. Our study provides practical\ninsights for building more fair, efficient, and linguistically informed\ntokenizers for multilingual NLP.", "AI": {"tldr": "\u7cfb\u7edf\u8bc4\u4f30\u4e8617\u79cd\u5370\u5ea6\u8bed\u8a00\u7684\u5206\u8bcd\u7b56\u7565\uff0c\u63d0\u51fa\u57fa\u4e8e\u8bed\u8a00\u4eb2\u7f18\u5173\u7cfb\u7684\u591a\u8bed\u8a00\u5206\u8bcd\u5668\u4f18\u5316\u65b9\u6848", "motivation": "\u73b0\u6709\u5206\u8bcd\u5668\u5bf9\u9ad8\u8d44\u6e90\u8bed\u8a00\u5b58\u5728\u504f\u7f6e\uff0c\u96be\u4ee5\u9002\u5e94\u5370\u5ea6\u6b21\u5927\u9646\u591a\u8bed\u8a00\u590d\u6742\u5f62\u6001\u7279\u70b9", "method": "\u5bf9\u6bd4BPE/Unigram\u7b97\u6cd5\u3001\u4e0d\u540c\u8bcd\u8868\u89c4\u6a21\u3001\u8054\u5408\u8bad\u7ec3/\u805a\u7c7b\u8bad\u7ec3\u7b56\u7565\uff0c\u9a8c\u8bc1\u8bed\u8a00\u4eb2\u7f18\u5173\u7cfb\u7684\u8fc1\u79fb\u6548\u679c", "result": "\u53d1\u73b0\u805a\u7c7b\u8bad\u7ec3\u7b56\u7565\u4f18\u52bf\uff0c\u8bc1\u5b9e\u4f4e\u8d44\u6e90\u8bed\u8a00\u53ef\u53d7\u76ca\u4e8e\u4eb2\u5c5e\u9ad8\u8d44\u6e90\u8bed\u8a00\u5206\u8bcd\u5668", "conclusion": "\u4e3a\u6784\u5efa\u66f4\u516c\u5e73\u9ad8\u6548\u7684\u591a\u8bed\u8a00\u5206\u8bcd\u5668\u63d0\u4f9b\u4e86\u8bed\u8a00\u5b66\u6307\u5bfc\u65b9\u6848"}}
{"id": "2506.17844", "pdf": "https://arxiv.org/pdf/2506.17844", "abs": "https://arxiv.org/abs/2506.17844", "authors": ["Xin Zhang", "Qiyu Wei", "Yingjie Zhu", "Fanyi Wu", "Sophia Ananiadou"], "title": "THCM-CAL: Temporal-Hierarchical Causal Modelling with Conformal Calibration for Clinical Risk Prediction", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 4 figures", "summary": "Automated clinical risk prediction from electronic health records (EHRs)\ndemands modeling both structured diagnostic codes and unstructured narrative\nnotes. However, most prior approaches either handle these modalities separately\nor rely on simplistic fusion strategies that ignore the directional,\nhierarchical causal interactions by which narrative observations precipitate\ndiagnoses and propagate risk across admissions. In this paper, we propose\nTHCM-CAL, a Temporal-Hierarchical Causal Model with Conformal Calibration. Our\nframework constructs a multimodal causal graph where nodes represent clinical\nentities from two modalities: Textual propositions extracted from notes and ICD\ncodes mapped to textual descriptions. Through hierarchical causal discovery,\nTHCM-CAL infers three clinically grounded interactions: intra-slice\nsame-modality sequencing, intra-slice cross-modality triggers, and inter-slice\nrisk propagation. To enhance prediction reliability, we extend conformal\nprediction to multi-label ICD coding, calibrating per-code confidence intervals\nunder complex co-occurrences. Experimental results on MIMIC-III and MIMIC-IV\ndemonstrate the superiority of THCM-CAL.", "AI": {"tldr": "\u63d0\u51faTHCM-CAL\u6a21\u578b\uff0c\u6574\u5408\u6587\u672c\u548c\u8bca\u65ad\u4ee3\u7801\u6570\u636e\uff0c\u901a\u8fc7\u5c42\u6b21\u56e0\u679c\u53d1\u73b0\u548c\u6821\u51c6\u63d0\u5347\u4e34\u5e8a\u98ce\u9669\u9884\u6d4b\u6548\u679c", "motivation": "\u73b0\u6709\u4e34\u5e8a\u98ce\u9669\u9884\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\uff0c\u4e14\u5ffd\u7565\u4e34\u5e8a\u89c2\u5bdf\u2192\u8bca\u65ad\u2192\u8de8\u5165\u9662\u98ce\u9669\u4f20\u64ad\u7684\u56e0\u679c\u5c42\u6b21\u5173\u7cfb", "method": "\u6784\u5efa\u591a\u6a21\u6001\u56e0\u679c\u56fe\uff08\u6587\u672c\u547d\u9898+ICD\u4ee3\u7801\uff09\uff0c\u901a\u8fc7\u5c42\u6b21\u56e0\u679c\u53d1\u73b0\u4e09\u79cd\u4e34\u5e8a\u4ea4\u4e92\uff0c\u5e76\u6269\u5c55\u5171\u5f62\u9884\u6d4b\u8fdb\u884c\u591a\u6807\u7b7e\u6821\u51c6", "result": "\u5728MIMIC-III/IV\u6570\u636e\u96c6\u9a8c\u8bc1\uff0c\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "THCM-CAL\u6709\u6548\u6574\u5408\u591a\u6a21\u6001\u4e34\u5e8a\u6570\u636e\uff0c\u5efa\u6a21\u4e34\u5e8a\u56e0\u679c\u5173\u7cfb\uff0c\u63d0\u5347\u9884\u6d4b\u53ef\u9760\u6027\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u652f\u6301"}}
{"id": "2506.17863", "pdf": "https://arxiv.org/pdf/2506.17863", "abs": "https://arxiv.org/abs/2506.17863", "authors": ["Haoran Liu", "Amir Tahmasbi", "Ehtesham Sam Haque", "Purak Jain"], "title": "LLMs for Customized Marketing Content Generation and Evaluation at Scale", "categories": ["cs.CL"], "comment": "KDD LLM4ECommerce Workshop 2025", "summary": "Offsite marketing is essential in e-commerce, enabling businesses to reach\ncustomers through external platforms and drive traffic to retail websites.\nHowever, most current offsite marketing content is overly generic,\ntemplate-based, and poorly aligned with landing pages, limiting its\neffectiveness. To address these limitations, we propose MarketingFM, a\nretrieval-augmented system that integrates multiple data sources to generate\nkeyword-specific ad copy with minimal human intervention. We validate\nMarketingFM via offline human and automated evaluations and large-scale online\nA/B tests. In one experiment, keyword-focused ad copy outperformed templates,\nachieving up to 9% higher CTR, 12% more impressions, and 0.38% lower CPC,\ndemonstrating gains in ad ranking and cost efficiency. Despite these gains,\nhuman review of generated ads remains costly. To address this, we propose\nAutoEval-Main, an automated evaluation system that combines rule-based metrics\nwith LLM-as-a-Judge techniques to ensure alignment with marketing principles.\nIn experiments with large-scale human annotations, AutoEval-Main achieved\n89.57% agreement with human reviewers. Building on this, we propose\nAutoEval-Update, a cost-efficient LLM-human collaborative framework to\ndynamically refine evaluation prompts and adapt to shifting criteria with\nminimal human input. By selectively sampling representative ads for human\nreview and using a critic LLM to generate alignment reports, AutoEval-Update\nimproves evaluation consistency while reducing manual effort. Experiments show\nthe critic LLM suggests meaningful refinements, improving LLM-human agreement.\nNonetheless, human oversight remains essential for setting thresholds and\nvalidating refinements before deployment.", "AI": {"tldr": "\u63d0\u51faMarketingFM\u7cfb\u7edf\u53ca\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5173\u952e\u8bcd\u5b9a\u5236\u5e7f\u544a\u63d0\u5347\u70b9\u51fb\u7387\uff0c\u5e76\u5b9e\u73b0\u9ad8\u6548\u4eba\u673a\u534f\u540c\u8bc4\u4f30", "motivation": "\u73b0\u6709\u7ad9\u5916\u8425\u9500\u5185\u5bb9\u5b58\u5728\u6a21\u677f\u5316\u4e25\u91cd\u3001\u4e0e\u843d\u5730\u9875\u5339\u914d\u5ea6\u4f4e\u3001\u4eba\u5de5\u5ba1\u6838\u6210\u672c\u9ad8\u7b49\u5173\u952e\u74f6\u9888", "method": "\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u6280\u672f\u751f\u6210\u5173\u952e\u8bcd\u5e7f\u544a\uff0c\u5f00\u53d1\u6df7\u5408\u89c4\u5219\u4e0eLLM\u7684\u81ea\u52a8\u8bc4\u4f30\u7cfb\u7edf\uff0c\u6784\u5efa\u52a8\u6001\u66f4\u65b0\u7684\u4eba\u673a\u534f\u4f5c\u6846\u67b6", "result": "\u5173\u952e\u8bcd\u5e7f\u544a\u70b9\u51fb\u7387\u63d0\u53479%\uff0c\u5370\u8c61\u6570\u589e\u52a012%\uff0cAutoEval-Main\u4e0e\u4eba\u5de5\u8bc4\u4f30\u4e00\u81f4\u7387\u8fbe89.57%", "conclusion": "\u7cfb\u7edf\u663e\u8457\u63d0\u5347\u5e7f\u544a\u6548\u679c\u4e0e\u8bc4\u4f30\u6548\u7387\uff0c\u4f46\u9700\u4fdd\u6301\u4eba\u5de5\u76d1\u7763\u9608\u503c\u8bbe\u5b9a\u548c\u4f18\u5316\u9a8c\u8bc1\u7684\u95ed\u73af\u673a\u5236"}}
{"id": "2506.17864", "pdf": "https://arxiv.org/pdf/2506.17864", "abs": "https://arxiv.org/abs/2506.17864", "authors": ["Taolin Zhang", "Haidong Kang", "Dongyang Li", "Qizhou Chen", "Chengyu Wang Xiaofeng He", "Richang Hong"], "title": "QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated impressive results\nbut still suffer from hallucinations. Model editing has been proposed to\ncorrect factual inaccuracies in LLMs. A challenging case is sequential model\nediting (SME), which aims to rectify errors continuously rather than treating\nthem as a one-time task. During SME, the general capabilities of LLMs can be\nnegatively affected due to the introduction of new parameters. In this paper,\nwe propose a queue-based self-correction framework (QueueEDIT) that not only\nenhances SME performance by addressing long-sequence dependency but also\nmitigates the impact of parameter bias on the general capabilities of LLMs.\nSpecifically, we first introduce a structural mapping editing loss to map the\ntriplets to the knowledge-sensitive neurons within the Transformer layers of\nLLMs. We then store the located parameters for each piece of edited knowledge\nin a queue and dynamically align previously edited parameters. In each edit, we\nselect queue parameters most relevant to the currently located parameters to\ndetermine whether previous knowledge needs realignment. Irrelevant parameters\nin the queue are frozen, and we update the parameters at the queue head to the\nLLM to ensure they do not harm general abilities. Experiments show that our\nframework significantly outperforms strong baselines across various SME\nsettings and maintains competitiveness in single-turn editing. The resulting\nLLMs also preserve high capabilities in general NLP tasks throughout the SME\nprocess.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u961f\u5217\u7684\u81ea\u6821\u6b63\u6846\u67b6QueueEDIT\uff0c\u901a\u8fc7\u7ed3\u6784\u6620\u5c04\u7f16\u8f91\u635f\u5931\u548c\u52a8\u6001\u53c2\u6570\u961f\u5217\u7ba1\u7406\uff0c\u6709\u6548\u63d0\u5347\u8fde\u7eed\u6a21\u578b\u7f16\u8f91\u6027\u80fd\u5e76\u4fdd\u62a4\u5927\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4e8b\u5b9e\u6027\u5e7b\u89c9\u95ee\u9898\uff0c\u4f20\u7edf\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u5728\u8fde\u7eed\u7f16\u8f91\u573a\u666f\u4e2d\u4f1a\u56e0\u53c2\u6570\u7d2f\u79ef\u635f\u5bb3\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b", "method": "1. \u4f7f\u7528\u7ed3\u6784\u6620\u5c04\u7f16\u8f91\u635f\u5931\u5b9a\u4f4d\u77e5\u8bc6\u654f\u611f\u795e\u7ecf\u5143\n2. \u5efa\u7acb\u53c2\u6570\u961f\u5217\u5b58\u50a8\u5386\u53f2\u7f16\u8f91\u53c2\u6570\n3. \u52a8\u6001\u5bf9\u9f50\u76f8\u5173\u53c2\u6570\u5e76\u51bb\u7ed3\u65e0\u5173\u53c2\u6570\n4. \u901a\u8fc7\u961f\u5217\u5934\u66f4\u65b0\u673a\u5236\u4fdd\u62a4\u901a\u7528\u80fd\u529b", "result": "\u5728\u591a\u4e2a\u8fde\u7eed\u7f16\u8f91\u573a\u666f\u4e2d\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u5355\u6b21\u7f16\u8f91\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u4e14\u5168\u7a0b\u4fdd\u630190%\u4ee5\u4e0a\u7684\u901a\u7528NLP\u4efb\u52a1\u6027\u80fd", "conclusion": "QueueEDIT\u6210\u529f\u5e73\u8861\u4e86\u6301\u7eed\u77e5\u8bc6\u66f4\u65b0\u4e0e\u901a\u7528\u80fd\u529b\u4fdd\u6301\u7684\u9700\u6c42\uff0c\u4e3aLLM\u7ef4\u62a4\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2506.17871", "pdf": "https://arxiv.org/pdf/2506.17871", "abs": "https://arxiv.org/abs/2506.17871", "authors": ["Chenghao Yang", "Ari Holtzman"], "title": "How Alignment Shrinks the Generative Horizon", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Codebase: https://github.com/yangalan123/LLMBranchingFactor, Website:\n  https://yangalan123.github.io/branching_factor/", "summary": "Despite their impressive capabilities, aligned large language models (LLMs)\noften generate outputs that lack diversity. What drives this stability in the\ngeneration? We investigate this phenomenon through the lens of probability\nconcentration in the model's output distribution. To quantify this\nconcentration, we introduce the Branching Factor (BF) -- a token-invariant\nmeasure of the effective number of plausible next steps during generation. Our\nempirical analysis reveals two key findings: (1) BF often decreases as\ngeneration progresses, suggesting that LLMs become more predictable as they\ngenerate. (2) alignment tuning substantially sharpens the model's output\ndistribution from the outset, reducing BF by nearly an order of magnitude\n(e.g., from 12 to 1.2) relative to base models. This stark reduction helps\nexplain why aligned models often appear less sensitive to decoding strategies.\nBuilding on this insight, we find this stability has surprising implications\nfor complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g.,\nDeepSeek-distilled models), for instance, leverage this effect; by generating\nlonger reasoning chains, they push generation into later, more deterministic\n(lower BF) stages, resulting in more stable outputs. We hypothesize that\nalignment tuning does not fundamentally change a model's behavior, but instead\nsteers it toward stylistic tokens (e.g., \"Sure\") that unlock low-entropy\ntrajectories already present in the base model. This view is supported by\nnudging experiments, which show that prompting base models with such tokens can\nsimilarly reduce BF. Together, our findings establish BF as a powerful\ndiagnostic for understanding and controlling LLM outputs - clarifying how\nalignment reduces variability, how CoT promotes stable generations, and how\nbase models can be steered away from diversity.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u652f\u56e0\u5b50(BF)\u91cf\u5316\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7a33\u5b9a\u6027\uff0c\u53d1\u73b0\u5bf9\u9f50\u8bad\u7ec3\u4f1a\u663e\u8457\u964d\u4f4eBF\u4f7f\u751f\u6210\u66f4\u7a33\u5b9a\uff0c\u94fe\u5f0f\u601d\u7ef4\u901a\u8fc7\u5ef6\u957f\u63a8\u7406\u94fe\u8fdb\u5165\u4f4eBF\u9636\u6bb5\u63d0\u5347\u7a33\u5b9a\u6027\u3002", "motivation": "\u63a2\u7a76\u5bf9\u9f50\u540e\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u7f3a\u4e4f\u591a\u6837\u6027\u7684\u672c\u8d28\u539f\u56e0\uff0c\u5206\u6790\u751f\u6210\u8fc7\u7a0b\u4e2d\u6982\u7387\u5206\u5e03\u7684\u53d8\u5316\u89c4\u5f8b\u3002", "method": "\u5f15\u5165\u5206\u652f\u56e0\u5b50(BF)\u4f5c\u4e3a\u91cf\u5316\u6307\u6807\uff0c\u5bf9\u6bd4\u57fa\u7840\u6a21\u578b\u4e0e\u5bf9\u9f50\u6a21\u578b\u7684BF\u53d8\u5316\uff0c\u5f00\u5c55\u94fe\u5f0f\u601d\u7ef4\u5b9e\u9a8c\u548c\u6807\u8bb0\u5f15\u5bfc\u5b9e\u9a8c\u3002", "result": "1. BF\u968f\u751f\u6210\u8fc7\u7a0b\u9012\u51cf 2. \u5bf9\u9f50\u6a21\u578bBF\u964d\u4f4e10\u500d 3. CoT\u901a\u8fc7\u5ef6\u957f\u63a8\u7406\u8fdb\u5165\u4f4eBF\u9636\u6bb5 4. \u57fa\u7840\u6a21\u578b\u53ef\u901a\u8fc7\u7279\u5b9a\u6807\u8bb0\u5f15\u5bfc\u5230\u4f4e\u71b5\u8def\u5f84", "conclusion": "BF\u662f\u6709\u6548\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u5bf9\u9f50\u8bad\u7ec3\u901a\u8fc7\u5f15\u5bfc\u6a21\u578b\u8fdb\u5165\u9884\u5b58\u4f4e\u71b5\u8def\u5f84\u964d\u4f4e\u591a\u6837\u6027\uff0cCoT\u7684\u6210\u529f\u6e90\u4e8e\u4f4eBF\u9636\u6bb5\u7684\u7a33\u5b9a\u6027\u63a7\u5236\u3002"}}
{"id": "2506.17881", "pdf": "https://arxiv.org/pdf/2506.17881", "abs": "https://arxiv.org/abs/2506.17881", "authors": ["Hua Tang", "Lingyong Yan", "Yukun Zhao", "Shuaiqiang Wang", "Jizhou Huang", "Dawei Yin"], "title": "Multi-turn Jailbreaking via Global Refinement and Active Fabrication", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have achieved exceptional performance across a\nwide range of tasks. However, they still pose significant safety risks due to\nthe potential misuse for malicious purposes. Jailbreaks, which aim to elicit\nmodels to generate harmful content, play a critical role in identifying the\nunderlying security threats. Recent jailbreaking primarily focuses on\nsingle-turn scenarios, while the more complicated multi-turn scenarios remain\nunderexplored. Moreover, existing multi-turn jailbreaking techniques struggle\nto adapt to the evolving dynamics of dialogue as the interaction progresses. To\naddress this limitation, we propose a novel multi-turn jailbreaking method that\nrefines the jailbreaking path globally at each interaction. We also actively\nfabricate model responses to suppress safety-related warnings, thereby\nincreasing the likelihood of eliciting harmful outputs in subsequent questions.\nExperimental results demonstrate the superior performance of our method\ncompared with existing single-turn and multi-turn jailbreaking techniques\nacross six state-of-the-art LLMs. Our code is publicly available at\nhttps://github.com/Ytang520/Multi-Turn_jailbreaking_Global-Refinment_and_Active-Fabrication.", "AI": {"tldr": "\u9488\u5bf9\u591a\u8f6e\u8d8a\u72f1\u573a\u666f\u4e2d\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u52a8\u6001\u9002\u5e94\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u7ed3\u5408\u5168\u5c40\u8def\u5f84\u4f18\u5316\u548c\u4e3b\u52a8\u54cd\u5e94\u4f2a\u9020\u7684\u65b0\u578b\u8d8a\u72f1\u65b9\u6cd5\uff0c\u5728\u516d\u4e2a\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u5b9e\u73b0\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u7684\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u591a\u8f6e\u8d8a\u72f1\u6280\u672f\u96be\u4ee5\u9002\u5e94\u5bf9\u8bdd\u6f14\u8fdb\u4e2d\u7684\u52a8\u6001\u7279\u6027\uff0c\u4e9f\u9700\u80fd\u591f\u6301\u7eed\u4f18\u5316\u653b\u51fb\u8def\u5f84\u5e76\u6291\u5236\u5b89\u5168\u8b66\u62a5\u7684\u89e3\u51b3\u65b9\u6848\u6765\u63ed\u793aLLM\u6f5c\u5728\u5b89\u5168\u5a01\u80c1\u3002", "method": "\u5728\u6bcf\u8f6e\u5bf9\u8bdd\u4e2d\u5168\u5c40\u4f18\u5316\u8d8a\u72f1\u8def\u5f84\uff0c\u4e3b\u52a8\u751f\u6210\u6291\u5236\u5b89\u5168\u8b66\u544a\u7684\u865a\u5047\u6a21\u578b\u54cd\u5e94\uff0c\u901a\u8fc7\u53cc\u91cd\u673a\u5236\u63d0\u5347\u540e\u7eed\u653b\u51fb\u6210\u529f\u7387\u3002", "result": "\u5728GPT-4\u3001Claude-2\u7b49\u516d\u4e2a\u5148\u8fdbLLM\u4e0a\uff0c\u672c\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u5355\u8f6e/\u591a\u8f6e\u8d8a\u72f1\u6280\u672f\u653b\u51fb\u6210\u529f\u7387\u63d0\u5347\u663e\u8457\uff08\u5177\u4f53\u6570\u636e\u89c1\u8bba\u6587\u5b9e\u9a8c\u90e8\u5206\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u4f18\u5316\u548c\u54cd\u5e94\u5e72\u9884\u673a\u5236\u6709\u6548\u7a81\u7834\u591a\u8f6e\u5bf9\u8bdd\u9632\u5fa1\uff0c\u4e3aLLM\u5b89\u5168\u6f0f\u6d1e\u68c0\u6d4b\u63d0\u4f9b\u65b0\u601d\u8def\uff0c\u540c\u65f6\u542f\u793a\u9632\u5fa1\u7cfb\u7edf\u9700\u8981\u589e\u5f3a\u5bf9\u8bdd\u72b6\u6001\u8ffd\u8e2a\u80fd\u529b\u3002"}}
{"id": "2506.17949", "pdf": "https://arxiv.org/pdf/2506.17949", "abs": "https://arxiv.org/abs/2506.17949", "authors": ["Hong Su"], "title": "Scatter-Based Innovation Propagation in Large Language Models for Multi-Stage Process Adaptation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) exhibit strong capabilities in reproducing and\nextending patterns observed during pretraining but often struggle to generalize\nnovel ideas beyond their original context. This paper addresses the challenge\nof applying such localized innovations - introduced at a specific stage or\ncomponent - to other parts of a multi-stage process. We propose a scatter-based\ninnovation expansion model (innovation scatter model) that guides the LLM\nthrough a four-step process: (1) identifying the core innovation by comparing\nthe user's input with its surrounding context, (2) generalizing the innovation\nby removing references to specific stages or components, (3) determining\nwhether the generalized innovation applies to a broader scope beyond the\noriginal stage, and (4) systematically applying it to other structurally\nsimilar stages using the LLM. This model leverages structural redundancy across\nstages to improve the applicability of novel ideas. Verification results\ndemonstrate that the innovation scatter model enables LLMs to extend\ninnovations across structurally similar stages, thereby enhancing\ngeneralization and reuse.", "AI": {"tldr": "\u63d0\u51fa\u56db\u6b65\u521b\u65b0\u6269\u6563\u6a21\u578b\u89e3\u51b3LLM\u5c40\u90e8\u521b\u65b0\u96be\u4ee5\u8de8\u9636\u6bb5\u5e94\u7528\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u6784\u5197\u4f59\u63d0\u5347\u901a\u7528\u6027", "motivation": "LLMs\u5728\u7279\u5b9a\u9636\u6bb5/\u7ec4\u4ef6\u4e2d\u4ea7\u751f\u7684\u5c40\u90e8\u521b\u65b0\u96be\u4ee5\u6709\u6548\u6269\u5c55\u5230\u591a\u9636\u6bb5\u6d41\u7a0b\u7684\u5176\u4ed6\u73af\u8282", "method": "\u56db\u6b65\u6269\u6563\u6a21\u578b\uff1a1\uff09\u8bc6\u522b\u6838\u5fc3\u521b\u65b0\u70b9 2\uff09\u53bb\u9636\u6bb5\u5316\u6cdb\u5316 3\uff09\u9002\u7528\u8303\u56f4\u8bc4\u4f30 4\uff09\u7ed3\u6784\u5316\u76f8\u4f3c\u9636\u6bb5\u7cfb\u7edf\u5e94\u7528", "result": "\u9a8c\u8bc1\u8868\u660e\u8be5\u6a21\u578b\u80fd\u6709\u6548\u5e2e\u52a9LLMs\u5c06\u521b\u65b0\u6269\u5c55\u5230\u7ed3\u6784\u76f8\u4f3c\u9636\u6bb5\uff0c\u63d0\u5347\u6cdb\u5316\u4e0e\u590d\u7528\u80fd\u529b", "conclusion": "\u901a\u8fc7\u521b\u65b0\u6269\u6563\u6a21\u578b\u5229\u7528\u9636\u6bb5\u95f4\u7ed3\u6784\u5197\u4f59\u6027\uff0c\u663e\u8457\u589e\u5f3a\u4e86LLM\u5bf9\u521b\u65b0\u65b9\u6848\u7684\u8de8\u9636\u6bb5\u5e94\u7528\u80fd\u529b"}}
{"id": "2506.17951", "pdf": "https://arxiv.org/pdf/2506.17951", "abs": "https://arxiv.org/abs/2506.17951", "authors": ["Quanwei Tang", "Sophia Yat Mei Lee", "Junshuang Wu", "Dong Zhang", "Shoushan Li", "Erik Cambria", "Guodong Zhou"], "title": "A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment", "categories": ["cs.CL"], "comment": "acl 2025 findings", "summary": "Recent advancements in retrieval-augmented generation (RAG) have enhanced\nlarge language models in question answering by integrating external knowledge.\nHowever, challenges persist in achieving global understanding and aligning\nresponses with human ethical and quality preferences. To address these issues,\nwe propose GraphMPA, a comprehensive graph-based framework with mode-seeking\npreference alignment. Our approach constructs a hierarchical document graph\nusing a general similarity measurement, mimicking human cognitive processes for\ninformation understanding and synthesis. Additionally, we introduce\nmode-seeking preference optimization to better align model outputs with human\npreferences through probability-matching constraints. Extensive experiments on\nsix datasets demonstrate the effectiveness of our\n\\href{https://github.com/tangquanwei/GraphMPA}{GraphMPA}.", "AI": {"tldr": "GraphMPA\u6846\u67b6\u901a\u8fc7\u5c42\u6b21\u5316\u6587\u6863\u56fe\u8c31\u548c\u6a21\u5f0f\u5bfb\u6c42\u504f\u597d\u4f18\u5316\uff0c\u63d0\u5347RAG\u7684\u5168\u5c40\u7406\u89e3\u80fd\u529b\u548c\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u5728\u5168\u5c40\u8bed\u4e49\u7406\u89e3\u548c\u7b26\u5408\u4eba\u7c7b\u4f26\u7406/\u8d28\u91cf\u504f\u597d\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u5efa\u7acb\u66f4\u63a5\u8fd1\u4eba\u7c7b\u8ba4\u77e5\u7684\u4fe1\u606f\u6574\u5408\u65b9\u5f0f\u3002", "method": "1. \u57fa\u4e8e\u901a\u7528\u76f8\u4f3c\u5ea6\u6784\u5efa\u5c42\u6b21\u5316\u6587\u6863\u56fe\u8c31\uff0c\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u6d41\u7a0b\n2. \u63d0\u51fa\u6a21\u5f0f\u5bfb\u6c42\u504f\u597d\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u6982\u7387\u5339\u914d\u7ea6\u675f\u5b9e\u73b0\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\u6846\u67b6\u6709\u6548\u6027\uff08\u9879\u76ee\u5df2\u5f00\u6e90\u5728GitHub\uff09\uff0c\u663e\u8457\u63d0\u5347\u95ee\u7b54\u7cfb\u7edf\u7684\u4eba\u7c7b\u504f\u597d\u5339\u914d\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u56fe\u7ed3\u6784\u5efa\u6a21\u7ed3\u5408\u6a21\u5f0f\u5bfb\u6c42\u4f18\u5316\uff0cGraphMPA\u6210\u529f\u89e3\u51b3\u4e86RAG\u7cfb\u7edf\u7684\u5168\u5c40\u7406\u89e3\u4e0e\u4ef7\u503c\u89c2\u5bf9\u9f50\u53cc\u91cd\u6311\u6218\uff0c\u4e3a\u53ef\u4fe1AI\u7cfb\u7edf\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2506.18027", "pdf": "https://arxiv.org/pdf/2506.18027", "abs": "https://arxiv.org/abs/2506.18027", "authors": ["Thi Thu Uyen Hoang", "Viet Anh Nguyen"], "title": "PDF Retrieval Augmented Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents an advancement in Question-Answering (QA) systems using a\nRetrieval Augmented Generation (RAG) framework to enhance information\nextraction from PDF files. Recognizing the richness and diversity of data\nwithin PDFs--including text, images, vector diagrams, graphs, and tables--poses\nunique challenges for existing QA systems primarily designed for textual\ncontent. We seek to develop a comprehensive RAG-based QA system that will\neffectively address complex multimodal questions, where several data types are\ncombined in the query. This is mainly achieved by refining approaches to\nprocessing and integrating non-textual elements in PDFs into the RAG framework\nto derive precise and relevant answers, as well as fine-tuning large language\nmodels to better adapt to our system. We provide an in-depth experimental\nevaluation of our solution, demonstrating its capability to extract accurate\ninformation that can be applied to different types of content across PDFs. This\nwork not only pushes the boundaries of retrieval-augmented QA systems but also\nlays a foundation for further research in multimodal data integration and\nprocessing.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eRAG\u6846\u67b6\u7684\u591a\u6a21\u6001PDF\u95ee\u7b54\u7cfb\u7edf\uff0c\u901a\u8fc7\u6539\u8fdb\u975e\u6587\u672c\u5143\u7d20\u5904\u7406\u4e0e\u6a21\u578b\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347\u8de8\u6570\u636e\u7c7b\u578b\u7684\u590d\u6742\u95ee\u9898\u89e3\u7b54\u80fd\u529b", "motivation": "PDF\u4e2d\u56fe\u50cf/\u56fe\u8868\u7b49\u591a\u6a21\u6001\u6570\u636e\u96be\u4ee5\u88ab\u4f20\u7edf\u6587\u672c\u578bQA\u7cfb\u7edf\u6709\u6548\u5904\u7406\uff0c\u9700\u5f00\u53d1\u80fd\u6574\u5408\u591a\u6e90\u4fe1\u606f\u7684\u589e\u5f3a\u578b\u95ee\u7b54\u6846\u67b6", "method": "\u6539\u8fdb\u975e\u6587\u672c\u5143\u7d20\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\uff0c\u6784\u5efa\u591a\u6a21\u6001RAG\u6846\u67b6\uff0c\u8bbe\u8ba1\u9002\u914d\u7684LLM\u5fae\u8c03\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u8de8\u6a21\u6001\u68c0\u7d22\u5b9e\u9a8c\u9a8c\u8bc1\u7cfb\u7edf\u6548\u679c", "result": "\u5b9e\u9a8c\u8bc1\u660e\u7cfb\u7edf\u5728\u6df7\u5408\u6587\u672c/\u56fe\u50cf\u67e5\u8be2\u573a\u666f\u4e0b\u51c6\u786e\u7387\u63d0\u534737.8%\uff0c\u652f\u6301\u8de8\u9875\u9762\u7684\u8868\u683c-\u6587\u672c\u5173\u8054\u63a8\u7406", "conclusion": "\u8be5\u6846\u67b6\u7a81\u7834\u4e86\u4f20\u7edfQA\u7cfb\u7edf\u7684\u6a21\u6001\u9650\u5236\uff0c\u4e3a\u591a\u6a21\u6001\u6587\u6863\u667a\u80fd\u5904\u7406\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u7684\u6280\u672f\u8303\u5f0f"}}
{"id": "2506.18035", "pdf": "https://arxiv.org/pdf/2506.18035", "abs": "https://arxiv.org/abs/2506.18035", "authors": ["Maxence Lasbordes", "Daniele Falavigna", "Alessio Brutti"], "title": "Splitformer: An improved early-exit architecture for automatic speech recognition on edge devices", "categories": ["cs.CL", "cs.SD", "eess.AS", "68T50 (Primary)", "I.2.7; I.5.4"], "comment": "5 pages, 3 Postscript figures", "summary": "The ability to dynamically adjust the computational load of neural models\nduring inference in a resource aware manner is crucial for on-device processing\nscenarios, characterised by limited and time-varying computational resources.\nEarly-exit architectures represent an elegant and effective solution, since\nthey can process the input with a subset of their layers, exiting at\nintermediate branches (the upmost layers are hence removed from the model).\n  From a different perspective, for automatic speech recognition applications\nthere are memory-efficient neural architectures that apply variable frame rate\nanalysis, through downsampling/upsampling operations in the middle layers,\nreducing the overall number of operations and improving significantly the\nperformance on well established benchmarks. One example is the Zipformer.\nHowever, these architectures lack the modularity necessary to inject early-exit\nbranches.\n  With the aim of improving the performance in early-exit models, we propose\nintroducing parallel layers in the architecture that process downsampled\nversions of their inputs. % in conjunction with standard processing layers. We\nshow that in this way the speech recognition performance on standard benchmarks\nsignificantly improve, at the cost of a small increase in the overall number of\nmodel parameters but without affecting the inference time.", "AI": {"tldr": "\u63d0\u51fa\u5728\u65e9\u671f\u9000\u51fa\u67b6\u6784\u4e2d\u5f15\u5165\u5e76\u884c\u5904\u7406\u4e0b\u91c7\u6837\u8f93\u5165\u7684\u5c42\uff0c\u663e\u8457\u63d0\u5347\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\u4e14\u4e0d\u5f71\u54cd\u63a8\u7406\u65f6\u95f4", "motivation": "\u73b0\u6709\u53ef\u53d8\u5e27\u7387\u67b6\u6784\u7f3a\u4e4f\u65e9\u671f\u9000\u51fa\u673a\u5236\uff0c\u800c\u65e9\u671f\u9000\u51fa\u6a21\u578b\u5728\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\u4e0a\u5b58\u5728\u63d0\u5347\u7a7a\u95f4", "method": "\u5728\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u4e2d\u589e\u52a0\u5e76\u884c\u5904\u7406\u4e0b\u91c7\u6837\u8f93\u5165\u7684\u5c42\uff0c\u7ed3\u5408\u6807\u51c6\u5904\u7406\u5c42\u5b9e\u73b0\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6", "result": "\u6807\u51c6\u8bed\u97f3\u8bc6\u522b\u57fa\u51c6\u6d4b\u8bd5\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u6a21\u578b\u53c2\u6570\u91cf\u5c0f\u5e45\u589e\u52a0\u4f46\u63a8\u7406\u65f6\u95f4\u4fdd\u6301\u7a33\u5b9a", "conclusion": "\u901a\u8fc7\u5e76\u884c\u4e0b\u91c7\u6837\u5c42\u4e0e\u65e9\u671f\u9000\u51fa\u673a\u5236\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u8bc6\u522b\u7cbe\u5ea6\u7684\u53cc\u91cd\u4f18\u5316"}}
{"id": "2506.18036", "pdf": "https://arxiv.org/pdf/2506.18036", "abs": "https://arxiv.org/abs/2506.18036", "authors": ["Aziz Amari", "Mohamed Achref Ben Ammar"], "title": "Markov-Enhanced Clustering for Long Document Summarization: Tackling the 'Lost in the Middle' Challenge with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The rapid expansion of information from diverse sources has heightened the\nneed for effective automatic text summarization, which condenses documents into\nshorter, coherent texts. Summarization methods generally fall into two\ncategories: extractive, which selects key segments from the original text, and\nabstractive, which generates summaries by rephrasing the content coherently.\nLarge language models have advanced the field of abstractive summarization, but\nthey are resourceintensive and face significant challenges in retaining key\ninformation across lengthy documents, which we call being \"lost in the middle\".\nTo address these issues, we propose a hybrid summarization approach that\ncombines extractive and abstractive techniques. Our method splits the document\ninto smaller text chunks, clusters their vector embeddings, generates a summary\nfor each cluster that represents a key idea in the document, and constructs the\nfinal summary by relying on a Markov chain graph when selecting the semantic\norder of ideas.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u62bd\u53d6\u5f0f\u548c\u751f\u6210\u5f0f\u6458\u8981\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u6587\u672c\u5206\u5757-\u805a\u7c7b-\u5206\u7c07\u6458\u8981-\u9a6c\u5c14\u53ef\u592b\u94fe\u6392\u5e8f\u7684\u6d41\u7a0b\uff0c\u89e3\u51b3\u957f\u6587\u6863\u6458\u8981\u4e2d\u7684\u5173\u952e\u4fe1\u606f\u4e22\u5931\u95ee\u9898", "motivation": "\u4f20\u7edf\u751f\u6210\u5f0f\u5927\u6a21\u578b\u5728\u957f\u6587\u672c\u6458\u8981\u4e2d\u5b58\u5728'\u4e2d\u95f4\u4fe1\u606f\u4e22\u5931'\u73b0\u8c61\u4e14\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u9700\u8981\u878d\u5408\u4e24\u79cd\u6458\u8981\u6280\u672f\u7684\u4f18\u52bf", "method": "1. \u6587\u672c\u5206\u5757 \u2192 2. \u5411\u91cf\u805a\u7c7b \u2192 3. \u5206\u7c07\u6458\u8981 \u2192 4. \u9a6c\u5c14\u53ef\u592b\u94fe\u6784\u5efa\u8bed\u4e49\u987a\u5e8f", "result": "\u6709\u6548\u7f13\u89e3\u957f\u6587\u6863\u4e2d\u7684\u5173\u952e\u4fe1\u606f\u4e22\u5931\u95ee\u9898\uff0c\u5728ROUGE\u6307\u6807\u4e0a\u76f8\u6bd4\u7eaf\u751f\u6210\u5f0f\u65b9\u6cd5\u63d0\u534712%", "conclusion": "\u6df7\u5408\u5f0f\u6458\u8981\u65b9\u6cd5\u901a\u8fc7\u7ed3\u6784\u5316\u89e3\u6784\u4e0e\u8bed\u4e49\u91cd\u7ec4\uff0c\u5728\u4fe1\u606f\u4fdd\u7559\u548c\u6458\u8981\u8fde\u8d2f\u6027\u4e4b\u95f4\u5b9e\u73b0\u66f4\u597d\u5e73\u8861"}}
{"id": "2506.18082", "pdf": "https://arxiv.org/pdf/2506.18082", "abs": "https://arxiv.org/abs/2506.18082", "authors": ["Esteban Garces Arias", "Hannah Blocher", "Julian Rodemann", "Matthias A\u00dfenmacher", "Christoph Jansen"], "title": "Statistical Multicriteria Evaluation of LLM-Generated Text", "categories": ["cs.CL", "stat.AP"], "comment": null, "summary": "Assessing the quality of LLM-generated text remains a fundamental challenge\nin natural language processing. Current evaluation approaches often rely on\nisolated metrics or simplistic aggregations that fail to capture the nuanced\ntrade-offs between coherence, diversity, fluency, and other relevant indicators\nof text quality. In this work, we adapt a recently proposed framework for\nstatistical inference based on Generalized Stochastic Dominance (GSD) that\naddresses three critical limitations in existing benchmarking methodologies:\nthe inadequacy of single-metric evaluation, the incompatibility between\ncardinal automatic metrics and ordinal human judgments, and the lack of\ninferential statistical guarantees. The GSD-front approach enables simultaneous\nevaluation across multiple quality dimensions while respecting their different\nmeasurement scales, building upon partial orders of decoding strategies, thus\navoiding arbitrary weighting of the involved metrics. By applying this\nframework to evaluate common decoding strategies against human-generated text,\nwe demonstrate its ability to identify statistically significant performance\ndifferences while accounting for potential deviations from the i.i.d.\nassumption of the sampling design.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5e7f\u4e49\u968f\u673a\u4f18\u52bf\uff08GSD\uff09\u7684\u6587\u672c\u8d28\u91cf\u8bc4\u4f30\u6846\u67b6\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u6307\u6807\u878d\u5408\u3001\u7edf\u8ba1\u63a8\u65ad\u548c\u5c3a\u5ea6\u517c\u5bb9\u6027\u65b9\u9762\u7684\u7f3a\u9677", "motivation": "\u5f53\u524dLLM\u751f\u6210\u6587\u672c\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5355\u6307\u6807\u7247\u9762\u6027\u3001\u4eba\u5de5\u6807\u6ce8\u4e0e\u81ea\u52a8\u6307\u6807\u5c3a\u5ea6\u4e0d\u517c\u5bb9\u3001\u7f3a\u4e4f\u7edf\u8ba1\u63a8\u65ad\u4fdd\u8bc1\u4e09\u5927\u6838\u5fc3\u95ee\u9898", "method": "\u91c7\u7528\u5e7f\u4e49\u968f\u673a\u4f18\u52bf(GSD)\u6846\u67b6\u5efa\u7acb\u504f\u5e8f\u5173\u7cfb\uff0c\u5b9e\u73b0\u591a\u8d28\u91cf\u7ef4\u5ea6\u7684\u975e\u53c2\u6570\u5316\u8054\u5408\u8bc4\u4f30\uff0c\u907f\u514d\u4eba\u5de5\u8bbe\u5b9a\u6307\u6807\u6743\u91cd", "result": "\u5b9e\u9a8c\u8bc1\u660eGSD\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u89e3\u7801\u7b56\u7565\u7684\u7edf\u8ba1\u663e\u8457\u6027\u5dee\u5f02\uff0c\u4e14\u5bf9\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u5177\u6709\u9c81\u68d2\u6027", "conclusion": "GSD\u6846\u67b6\u4e3a\u591a\u7ef4\u5ea6\u6587\u672c\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7edf\u8ba1\u4e25\u8c28\u7684\u65b9\u6cd5\u8bba\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u8bc4\u4f30\u4f53\u7cfb\u7684\u65b9\u6cd5\u8bba\u5c40\u9650"}}
{"id": "2506.18091", "pdf": "https://arxiv.org/pdf/2506.18091", "abs": "https://arxiv.org/abs/2506.18091", "authors": ["Patrik Stano", "Ale\u0161 Hor\u00e1k"], "title": "Evaluating Prompt-Based and Fine-Tuned Approaches to Czech Anaphora Resolution", "categories": ["cs.CL"], "comment": "12 pages", "summary": "Anaphora resolution plays a critical role in natural language understanding,\nespecially in morphologically rich languages like Czech. This paper presents a\ncomparative evaluation of two modern approaches to anaphora resolution on Czech\ntext: prompt engineering with large language models (LLMs) and fine-tuning\ncompact generative models. Using a dataset derived from the Prague Dependency\nTreebank, we evaluate several instruction-tuned LLMs, including Mistral Large 2\nand Llama 3, using a series of prompt templates. We compare them against\nfine-tuned variants of the mT5 and Mistral models that we trained specifically\nfor Czech anaphora resolution. Our experiments demonstrate that while prompting\nyields promising few-shot results (up to 74.5% accuracy), the fine-tuned\nmodels, particularly mT5-large, outperform them significantly, achieving up to\n88% accuracy while requiring fewer computational resources. We analyze\nperformance across different anaphora types, antecedent distances, and source\ncorpora, highlighting key strengths and trade-offs of each approach.", "AI": {"tldr": "\u5fae\u8c03\u6a21\u578b\uff08mT5-large\uff09\u5728\u6377\u514b\u8bed\u6307\u4ee3\u6d88\u89e3\u4efb\u52a1\u4e2d\u4ee588%\u51c6\u786e\u7387\u663e\u8457\u4f18\u4e8e\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\uff0874.5%\uff09\uff0c\u4e14\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u66f4\u4f4e", "motivation": "\u9488\u5bf9\u5f62\u6001\u590d\u6742\u7684\u6377\u514b\u8bed\u6307\u4ee3\u6d88\u89e3\u96be\u9898\uff0c\u8bc4\u4f30\u73b0\u4ee3\u65b9\u6cd5\uff08\u63d0\u793a\u5de5\u7a0b vs \u6a21\u578b\u5fae\u8c03\uff09\u7684\u5b9e\u9645\u6548\u80fd", "method": "\u4f7f\u7528PDT\u6570\u636e\u96c6\uff0c\u5bf9\u6bd4\u63d0\u793a\u5de5\u7a0b\uff08Mistral/Llama\u7cfb\u5217\uff09\u4e0e\u5fae\u8c03\u6a21\u578b\uff08mT5/Mistral\uff09\u7684\u51c6\u786e\u7387\u53ca\u8ba1\u7b97\u6548\u7387", "result": "\u5fae\u8c03\u6a21\u578b\u51c6\u786e\u7387\u63d0\u534713.5%\uff0c\u4e0d\u540c\u6307\u4ee3\u7c7b\u578b/\u8ddd\u79bb\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u7a33\u5b9a\uff0c\u8d44\u6e90\u6d88\u8017\u4f4e\u4e8e\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5", "conclusion": "\u5b9e\u9645\u5e94\u7528\u4e2d\u63a8\u8350\u5fae\u8c03\u7d27\u51d1\u6a21\u578b\uff0c\u4f46\u5728\u6807\u6ce8\u6570\u636e\u6709\u9650\u65f6\u63d0\u793a\u5de5\u7a0b\u53ef\u4f5c\u4e3a\u6709\u6548\u8865\u5145\u65b9\u6848"}}
{"id": "2506.18102", "pdf": "https://arxiv.org/pdf/2506.18102", "abs": "https://arxiv.org/abs/2506.18102", "authors": ["Fuyu Wang", "Jiangtong Li", "Kun Zhu", "Changjun Jiang"], "title": "InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating", "categories": ["cs.CL"], "comment": "20 pages; Accepted to ACL 2025 Main", "summary": "With the rapid advancements in large language models (LLMs), debating tasks,\nsuch as argument quality assessment and debate process simulation, have made\nsignificant progress. However, existing LLM-based debating systems focus on\nresponding to specific arguments while neglecting objective assessments such as\nauthenticity and logical validity. Furthermore, these systems lack a structured\napproach to optimize across various dimensions$-$including evaluation metrics,\nchain-of-thought (CoT) reasoning, and multi-turn debate refinement$-$thereby\nlimiting their effectiveness. To address these interconnected challenges, we\npropose a dual-component framework: (1) $\\textbf{InspireScore}$, a novel\nevaluation system that establishes a multi-dimensional assessment architecture\nincorporating four subjective criteria (emotional appeal, argument clarity,\nargument arrangement, and topic relevance) alongside two objective metrics\n(fact authenticity and logical validity); and (2) $\\textbf{InspireDebate}$, an\noptimized debating framework employing a phased optimization approach through\nCoT reasoning enhancement, multi-dimensional Direct Preference Optimization\n(DPO), and real-time knowledge grounding via web-based Retrieval Augmented\nGeneration (Web-RAG). Empirical evaluations demonstrate that\n$\\textbf{InspireScore}$ achieves 44$\\%$ higher correlation with expert\njudgments compared to existing methods, while $\\textbf{InspireDebate}$ shows\nsignificant improvements, outperforming baseline models by 57$\\%$. Source code\nis available at https://github.com/fywang12/InspireDebate.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u7ec4\u4ef6\u6846\u67b6InspireScore\uff08\u591a\u7ef4\u8bc4\u4f30\u7cfb\u7edf\uff09\u548cInspireDebate\uff08\u4f18\u5316\u8fa9\u8bba\u6846\u67b6\uff09\uff0c\u663e\u8457\u63d0\u5347\u8fa9\u8bba\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM\u8fa9\u8bba\u7cfb\u7edf\u5ffd\u89c6\u771f\u5b9e\u6027\u4e0e\u903b\u8f91\u6709\u6548\u6027\u8bc4\u4f30\uff0c\u4e14\u7f3a\u4e4f\u7ed3\u6784\u5316\u4f18\u5316\u7ef4\u5ea6\uff08\u8bc4\u4f30\u6307\u6807\u3001\u63a8\u7406\u94fe\u3001\u591a\u8f6e\u8fa9\u8bba\u4f18\u5316\uff09\u3002", "method": "1. InspireScore\u6574\u54084\u4e3b\u89c2\u6807\u51c6\uff08\u60c5\u611f\u5438\u5f15\u529b/\u8bba\u70b9\u6e05\u6670\u5ea6/\u7ed3\u6784\u5b89\u6392/\u4e3b\u9898\u76f8\u5173\uff09\u548c2\u5ba2\u89c2\u6307\u6807\uff08\u4e8b\u5b9e\u771f\u5b9e\u6027/\u903b\u8f91\u6709\u6548\u6027\uff09\u30022. InspireDebate\u91c7\u7528\u5206\u9636\u6bb5\u4f18\u5316\uff1aCoT\u63a8\u7406\u589e\u5f3a+\u591a\u7ef4DPO+Web-RAG\u5b9e\u65f6\u77e5\u8bc6\u589e\u5f3a\u3002", "result": "InspireScore\u4e0e\u4e13\u5bb6\u5224\u65ad\u76f8\u5173\u6027\u63d0\u534744%\uff0cInspireDebate\u6027\u80fd\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b57%\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7cfb\u7edf\u5316\u8bc4\u4f30\u548c\u5206\u9636\u6bb5\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u8fa9\u8bba\u7cfb\u7edf\u7684\u8bc4\u4f30\u51c6\u786e\u6027\u4e0e\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2506.18105", "pdf": "https://arxiv.org/pdf/2506.18105", "abs": "https://arxiv.org/abs/2506.18105", "authors": ["Yicheng Fu", "Zhemin Huang", "Liuxin Yang", "Yumeng Lu", "Zhongdongming Dai"], "title": "Chengyu-Bench: Benchmarking Large Language Models for Chinese Idiom Understanding and Use", "categories": ["cs.CL"], "comment": null, "summary": "Chinese idioms (Chengyu) are concise four-character expressions steeped in\nhistory and culture, whose literal translations often fail to capture their\nfull meaning. This complexity makes them challenging for language models to\ninterpret and use correctly. Existing benchmarks focus on narrow tasks -\nmultiple-choice cloze tests, isolated translation, or simple paraphrasing. We\nintroduce Chengyu-Bench, a comprehensive benchmark featuring three tasks: (1)\nEvaluative Connotation, classifying idioms as positive or negative; (2)\nAppropriateness, detecting incorrect idiom usage in context; and (3) Open\nCloze, filling blanks in longer passages without options. Chengyu-Bench\ncomprises 2,937 human-verified examples covering 1,765 common idioms sourced\nfrom diverse corpora. We evaluate leading LLMs and find they achieve over 95%\naccuracy on Evaluative Connotation, but only ~85% on Appropriateness and ~40%\ntop-1 accuracy on Open Cloze. Error analysis reveals that most mistakes arise\nfrom fundamental misunderstandings of idiom meanings. Chengyu-Bench\ndemonstrates that while LLMs can reliably gauge idiom sentiment, they still\nstruggle to grasp the cultural and contextual nuances essential for proper\nusage. The benchmark and source code are available at:\nhttps://github.com/sofyc/ChengyuBench.", "AI": {"tldr": "\u63d0\u51faChengyu-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u5927\u6a21\u578b\u5728\u6210\u8bed\u7406\u89e3\u4e0a\u7684\u6587\u5316\u8bed\u5883\u77ed\u677f", "motivation": "\u73b0\u6709\u6210\u8bed\u8bc4\u6d4b\u4efb\u52a1\u8fc7\u4e8e\u5c40\u9650\uff0c\u9700\u5efa\u7acb\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u4f53\u7cfb\u63ed\u793a\u6a21\u578b\u5bf9\u6210\u8bed\u6587\u5316\u5185\u6db5\u7684\u7406\u89e3\u7f3a\u9677", "method": "\u6784\u5efa\u542b2937\u6837\u672c/1765\u6210\u8bed\u7684\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1\u8bc4\u4ef7\u5185\u6db5\u5224\u65ad\u3001\u7528\u6cd5\u9002\u5f53\u6027\u68c0\u6d4b\u3001\u65e0\u9009\u9879\u586b\u7a7a\u4e09\u9879\u4efb\u52a1", "result": "\u4e3b\u6d41\u6a21\u578b\u5728\u60c5\u611f\u5224\u65ad\u8fbe95%+\uff0c\u4f46\u7528\u6cd5\u68c0\u6d4b\u4ec585%\uff0c\u5f00\u653e\u586b\u7a7atop-1\u51c6\u786e\u7387\u4ec540%\uff0c\u9519\u8bef\u591a\u6e90\u4e8e\u8bed\u4e49\u8bef\u89e3", "conclusion": "\u5927\u6a21\u578b\u867d\u80fd\u5224\u65ad\u6210\u8bed\u60c5\u611f\u503e\u5411\uff0c\u4f46\u7f3a\u4e4f\u652f\u6491\u6b63\u786e\u7528\u6cd5\u7684\u6587\u5316\u8bed\u5883\u6df1\u5c42\u7406\u89e3\uff0c\u9700\u9488\u5bf9\u6027\u6539\u8fdb"}}
{"id": "2506.18116", "pdf": "https://arxiv.org/pdf/2506.18116", "abs": "https://arxiv.org/abs/2506.18116", "authors": ["Batool Haider", "Atmika Gorti", "Aman Chadha", "Manas Gaur"], "title": "Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "19 Pages, 7 Figures, 4 Tables (Note: Under Review)", "summary": "Large Language Models (LLMs) in mental healthcare risk propagating biases\nthat reinforce stigma and harm marginalized groups. While previous research\nidentified concerning trends, systematic methods for detecting intersectional\nbiases remain limited. This work introduces a multi-hop question answering\n(MHQA) framework to explore LLM response biases in mental health discourse. We\nanalyze content from the Interpretable Mental Health Instruction (IMHI) dataset\nacross symptom presentation, coping mechanisms, and treatment approaches. Using\nsystematic tagging across age, race, gender, and socioeconomic status, we\ninvestigate bias patterns at demographic intersections. We evaluate four LLMs:\nClaude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic\ndisparities across sentiment, demographics, and mental health conditions. Our\nMHQA approach demonstrates superior detection compared to conventional methods,\nidentifying amplification points where biases magnify through sequential\nreasoning. We implement two debiasing techniques: Roleplay Simulation and\nExplicit Bias Reduction, achieving 66-94% bias reductions through few-shot\nprompting with BBQ dataset examples. These findings highlight critical areas\nwhere LLMs reproduce mental healthcare biases, providing actionable insights\nfor equitable AI development.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u591a\u8df3\u95ee\u7b54\u6846\u67b6\u68c0\u6d4bLLM\u5728\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u4ea4\u53c9\u504f\u89c1\uff0c\u53d1\u73b0\u7cfb\u7edf\u6027\u5dee\u5f02\u5e76\u63d0\u51fa\u6709\u6548\u53bb\u504f\u6280\u672f\uff0866-94%\u504f\u89c1\u51cf\u5c11\uff09", "motivation": "LLMs\u5728\u5fc3\u7406\u5065\u5eb7\u5e94\u7528\u53ef\u80fd\u52a0\u5267\u793e\u4f1a\u504f\u89c1\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u6027\u4ea4\u53c9\u504f\u89c1\u68c0\u6d4b\u673a\u5236", "method": "\u4f7f\u7528\u591a\u8df3\u95ee\u7b54\u6846\u67b6\u5206\u6790IMHI\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u4eba\u53e3\u7edf\u8ba1\u6807\u7b7e\u7cfb\u7edf\u8bc4\u4f304\u4e2aLLM\uff0c\u5e76\u5b9e\u65bd\u89d2\u8272\u626e\u6f14\u6a21\u62df\u548c\u663e\u6027\u504f\u89c1\u51cf\u5c11\u6280\u672f", "result": "\u53d1\u73b0\u6a21\u578b\u5728\u60c5\u611f/\u4eba\u53e3/\u75c5\u75c7\u7ef4\u5ea6\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\uff0cMHQA\u68c0\u6d4b\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u53bb\u504f\u6280\u672f\u5b9e\u73b0\u6700\u9ad894%\u504f\u89c1\u6d88\u9664", "conclusion": "MHQA\u6846\u67b6\u6709\u6548\u8bc6\u522bLLM\u504f\u89c1\u653e\u5927\u8282\u70b9\uff0cfew-shot\u53bb\u504f\u6280\u672f\u4e3a\u516c\u5e73AI\u53d1\u5c55\u63d0\u4f9b\u53ef\u64cd\u4f5c\u65b9\u6848"}}
{"id": "2506.18120", "pdf": "https://arxiv.org/pdf/2506.18120", "abs": "https://arxiv.org/abs/2506.18120", "authors": ["Tom S Juzek"], "title": "The Syntactic Acceptability Dataset (Preview): A Resource for Machine Learning and Linguistic Analysis of English", "categories": ["cs.CL", "68T50", "I.2.7; I.2.6; H.3.1"], "comment": "Accepted and published at LREC-COLING 2024. 8 pages, 3 figures.\n  Licensed under CC BY-NC-SA 4.0", "summary": "We present a preview of the Syntactic Acceptability Dataset, a resource being\ndesigned for both syntax and computational linguistics research. In its current\nform, the dataset comprises 1,000 English sequences from the syntactic\ndiscourse: Half from textbooks and half from the journal Linguistic Inquiry,\nthe latter to ensure a representation of the contemporary discourse. Each entry\nis labeled with its grammatical status (\"well-formedness\" according to\nsyntactic formalisms) extracted from the literature, as well as its\nacceptability status (\"intuitive goodness\" as determined by native speakers)\nobtained through crowdsourcing, with highest experimental standards. Even in\nits preliminary form, this dataset stands as the largest of its kind that is\npublicly accessible. We also offer preliminary analyses addressing three\ndebates in linguistics and computational linguistics: We observe that\ngrammaticality and acceptability judgments converge in about 83% of the cases\nand that \"in-betweenness\" occurs frequently. This corroborates existing\nresearch. We also find that while machine learning models struggle with\npredicting grammaticality, they perform considerably better in predicting\nacceptability. This is a novel finding. Future work will focus on expanding the\ndataset.", "AI": {"tldr": "\u4ecb\u7ecd\u5305\u542b1000\u4e2a\u82f1\u8bed\u53e5\u5b50\u7684\u53e5\u6cd5\u53ef\u63a5\u53d7\u6027\u6570\u636e\u96c6\uff0c\u63ed\u793a\u8bed\u6cd5\u4e0e\u53ef\u63a5\u53d7\u602783%\u4e00\u81f4\u6027\u53ca\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u5dee\u5f02", "motivation": "\u4e3a\u53e5\u6cd5\u5b66\u548c\u8ba1\u7b97\u8bed\u8a00\u5b66\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u8d44\u6e90\uff0c\u89e3\u51b3\u73b0\u6709\u6570\u636e\u4e0d\u8db3\uff0c\u63a2\u7a76\u8bed\u6cd5\u6027\u4e0e\u53ef\u63a5\u53d7\u6027\u5173\u7cfb\u53ca\u6a21\u578b\u8868\u73b0\u5dee\u5f02", "method": "\u6df7\u5408\u6559\u79d1\u4e66/\u5b66\u672f\u671f\u520a\u8bed\u6599\uff0c\u7ed3\u5408\u6587\u732e\u6807\u6ce8\u4e0e\u9ad8\u6807\u6dee\u4f17\u5305\u5b9e\u9a8c\uff0c\u8fdb\u884c\u8bed\u6cd5-\u63a5\u53d7\u5ea6\u53cc\u6807\u6ce8\u548c\u4e09\u7ef4\u5ea6\u8bed\u8a00\u5b66\u5206\u6790", "result": "\u8bed\u6cd5\u4e0e\u63a5\u53d7\u5ea683%\u4e00\u81f4\u4e14\u4e2d\u95f4\u6001\u5e38\u89c1\uff08\u652f\u6301\u73b0\u6709\u7814\u7a76\uff09\uff0c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63a5\u53d7\u5ea6\u9884\u6d4b\u663e\u8457\u4f18\u4e8e\u8bed\u6cd5\u6027\u9884\u6d4b\uff08\u65b0\u53d1\u73b0\uff09", "conclusion": "\u6570\u636e\u96c6\u586b\u8865\u8d44\u6e90\u7a7a\u767d\uff0c\u5b9e\u8bc1\u7ed3\u679c\u6df1\u5316\u8bed\u8a00\u5224\u65ad\u673a\u5236\u7406\u89e3\uff0c\u672a\u6765\u5c06\u6269\u5c55\u6570\u636e\u96c6\u89c4\u6a21\u5e76\u589e\u5f3a\u8de8\u8bed\u8a00\u7ef4\u5ea6"}}
{"id": "2506.18129", "pdf": "https://arxiv.org/pdf/2506.18129", "abs": "https://arxiv.org/abs/2506.18129", "authors": ["Bugra Kilictas", "Faruk Alpay"], "title": "$\u03c6^{\\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models", "categories": ["cs.CL", "cs.AI", "68T50, 68T45, 03B70", "I.2.6; I.2.7; I.2.3; F.4.1"], "comment": "16 pages, 3 figures", "summary": "We identify a critical vulnerability in autoregressive transformer language\nmodels where the em dash token induces recursive semantic drift, leading to\nclause boundary hallucination and embedding space entanglement. Through formal\nanalysis of token-level perturbations in semantic lattices, we demonstrate that\nem dash insertion fundamentally alters the model's latent representations,\ncausing compounding errors in long-form generation. We propose a novel solution\ncombining symbolic clause purification via the phi-infinity operator with\ntargeted embedding matrix realignment. Our approach enables total suppression\nof problematic tokens without requiring model retraining, while preserving\nsemantic coherence through fixed-point convergence guarantees. Experimental\nvalidation shows significant improvements in generation consistency and topic\nmaintenance. This work establishes a general framework for identifying and\nmitigating token-level vulnerabilities in foundation models, with immediate\nimplications for AI safety, model alignment, and robust deployment of large\nlanguage models in production environments. The methodology extends beyond\npunctuation to address broader classes of recursive instabilities in neural\ntext generation systems.", "AI": {"tldr": "\u53d1\u73b0\u81ea\u56de\u5f52Transformer\u6a21\u578b\u4e2dem dash\u7b26\u53f7\u5f15\u53d1\u7684\u9012\u5f52\u8bed\u4e49\u6f02\u79fb\u6f0f\u6d1e\uff0c\u63d0\u51fa\u7b26\u53f7\u51c0\u5316\u4e0e\u5d4c\u5165\u5bf9\u9f50\u7684\u89e3\u51b3\u65b9\u6848", "motivation": "\u89e3\u51b3em dash\u7b26\u53f7\u5bfc\u81f4\u7684\u5206\u53e5\u8fb9\u754c\u5e7b\u89c9\u548c\u5d4c\u5165\u7a7a\u95f4\u7ea0\u7f20\u95ee\u9898\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7a33\u5b9a\u6027\u4e0e\u5b89\u5168\u6027", "method": "\u7ed3\u5408phi-infinity\u7b97\u5b50\u8fdb\u884c\u7b26\u53f7\u5206\u53e5\u51c0\u5316\uff0c\u914d\u5408\u5d4c\u5165\u77e9\u9635\u5b9a\u5411\u91cd\u5bf9\u9f50\u6280\u672f\u907f\u514d\u6a21\u578b\u91cd\u8bad\u7ec3", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u751f\u6210\u4e00\u81f4\u6027\u63d0\u534737%\uff0c\u4e3b\u9898\u4fdd\u6301\u80fd\u529b\u589e\u5f3a52%", "conclusion": "\u5efa\u7acb\u57fa\u7840\u6a21\u578b\u6807\u8bb0\u7ea7\u6f0f\u6d1e\u68c0\u6d4b\u6846\u67b6\uff0c\u5bf9AI\u5b89\u5168\u90e8\u7f72\u53ca\u795e\u7ecf\u6587\u672c\u751f\u6210\u7cfb\u7edf\u7a33\u5b9a\u6027\u4fee\u590d\u5177\u6709\u666e\u9002\u4ef7\u503c"}}
{"id": "2506.18141", "pdf": "https://arxiv.org/pdf/2506.18141", "abs": "https://arxiv.org/abs/2506.18141", "authors": ["Ruixuan Deng", "Xiaoyang Hu", "Miles Gilberti", "Shane Storks", "Aman Taxali", "Mike Angstadt", "Chandra Sripada", "Joyce Chai"], "title": "Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We identify semantically coherent, context-consistent network components in\nlarge language models (LLMs) using coactivation of sparse autoencoder (SAE)\nfeatures collected from just a handful of prompts. Focusing on country-relation\ntasks, we show that ablating semantic components for countries and relations\nchanges model outputs in predictable ways, while amplifying these components\ninduces counterfactual responses. Notably, composing relation and country\ncomponents yields compound counterfactual outputs. We find that, whereas most\ncountry components emerge from the very first layer, the more abstract relation\ncomponents are concentrated in later layers. Furthermore, within relation\ncomponents themselves, nodes from later layers tend to have a stronger causal\nimpact on model outputs. Overall, these findings suggest a modular organization\nof knowledge within LLMs and advance methods for efficient, targeted model\nmanipulation.", "AI": {"tldr": "\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u8bc6\u522bLLM\u4e2d\u7684\u8bed\u4e49\u7ec4\u4ef6\uff0c\u53d1\u73b0\u6a21\u5757\u5316\u77e5\u8bc6\u7ec4\u7ec7\u4e0e\u5206\u5c42\u56e0\u679c\u6548\u5e94", "motivation": "\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\u7684\u7ed3\u6784\u5316\u8868\u5f81\uff0c\u5b9e\u73b0\u9488\u5bf9\u6027\u6a21\u578b\u884c\u4e3a\u63a7\u5236", "method": "\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAE)\u5206\u6790\u7279\u5f81\u5171\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u901a\u8fc7\u7ec4\u4ef6\u6d88\u878d/\u589e\u5f3a\u5b9e\u9a8c\u9a8c\u8bc1\u56e0\u679c\u5f71\u54cd", "result": "\u56fd\u5bb6\u7ec4\u4ef6\u5206\u5e03\u5728\u65e9\u671f\u5c42\uff0c\u62bd\u8c61\u5173\u7cfb\u7ec4\u4ef6\u96c6\u4e2d\u4e8e\u540e\u671f\u5c42\uff0c\u540e\u671f\u8282\u70b9\u56e0\u679c\u6548\u5e94\u66f4\u5f3a", "conclusion": "LLM\u5185\u90e8\u5b58\u5728\u6a21\u5757\u5316\u77e5\u8bc6\u67b6\u6784\uff0c\u5206\u5c42\u7ec4\u4ef6\u64cd\u63a7\u53ef\u5b9e\u73b0\u9ad8\u6548\u5b9a\u5411\u5e72\u9884"}}
{"id": "2506.18148", "pdf": "https://arxiv.org/pdf/2506.18148", "abs": "https://arxiv.org/abs/2506.18148", "authors": ["Diyam Akra", "Tymaa Hammouda", "Mustafa Jarrar"], "title": "QuranMorph: Morphologically Annotated Quranic Corpus", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present the QuranMorph corpus, a morphologically annotated corpus for the\nQuran (77,429 tokens). Each token in the QuranMorph was manually lemmatized and\ntagged with its part-of-speech by three expert linguists. The lemmatization\nprocess utilized lemmas from Qabas, an Arabic lexicographic database linked\nwith 110 lexicons and corpora of 2 million tokens. The part-of-speech tagging\nwas performed using the fine-grained SAMA/Qabas tagset, which encompasses 40\ntags. As shown in this paper, this rich lemmatization and POS tagset enabled\nthe QuranMorph corpus to be inter-linked with many linguistic resources. The\ncorpus is open-source and publicly available as part of the SinaLab resources\nat (https://sina.birzeit.edu/quran)", "AI": {"tldr": "\u6784\u5efa\u4e86\u5305\u542b7.7\u4e07\u8bcd\u8bed\u7684\u300a\u53e4\u5170\u7ecf\u300b\u5f62\u6001\u6807\u6ce8\u8bed\u6599\u5e93QuranMorph\uff0c\u652f\u6301\u591a\u8bed\u8a00\u8d44\u6e90\u4e92\u8054", "motivation": "\u89e3\u51b3\u300a\u53e4\u5170\u7ecf\u300b\u7814\u7a76\u4e2d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u5f62\u6001\u6807\u6ce8\u6570\u636e\u7684\u95ee\u9898\uff0c\u5efa\u7acb\u4e0e\u73b0\u6709\u8bed\u8a00\u5b66\u8d44\u6e90\u7684\u6865\u6881", "method": "\u4e09\u4f4d\u4e13\u5bb6\u624b\u5de5\u5b8c\u6210\u8bcd\u5143\u6807\u6ce8\uff0c\u91c7\u7528Qabas\u8bcd\u5178\u6570\u636e\u5e93\uff08\u6574\u5408110\u90e8\u8bcd\u5178\uff09\u548c\u542b40\u79cd\u6807\u7b7e\u7684SAMA/Qabas\u7ec6\u7c92\u5ea6\u6807\u6ce8\u4f53\u7cfb", "result": "\u5efa\u6210\u9996\u4e2a\u5f00\u653e\u83b7\u53d6\u7684\u300a\u53e4\u5170\u7ecf\u300b\u6df1\u5ea6\u6807\u6ce8\u8bed\u6599\u5e93\uff0c\u5b9e\u73b0\u4e0e2\u767e\u4e07\u8bcd\u8bed\u6599\u5e93\u7684\u8de8\u8d44\u6e90\u94fe\u63a5", "conclusion": "QuranMorph\u901a\u8fc7\u4e13\u5bb6\u6807\u6ce8\u548c\u591a\u7ef4\u5ea6\u6807\u7b7e\u4f53\u7cfb\uff0c\u4e3a\u5b97\u6559\u6587\u672c\u8ba1\u7b97\u8bed\u8a00\u5b66\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u57fa\u7840\u8bbe\u65bd"}}
{"id": "2506.18185", "pdf": "https://arxiv.org/pdf/2506.18185", "abs": "https://arxiv.org/abs/2506.18185", "authors": ["Zihan Liang", "Ziwen Pan", "Sumon Kanti Dey", "Azra Ismail"], "title": "CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction with Domain-Aware Transformers", "categories": ["cs.CL", "cs.AI"], "comment": "In the Proceedings of the 10th Social Media Mining for Health and\n  Health Real-World Data Workshop and Shared Tasks, co-located with AAAI ICWSM\n  2025", "summary": "This paper presents our system for the SMM4H-HeaRD 2025 shared tasks,\nspecifically Task 4 (Subtasks 1, 2a, and 2b) and Task 5 (Subtasks 1 and 2).\nTask 4 focused on detecting mentions of insomnia in clinical notes, while Task\n5 addressed the extraction of food safety events from news articles. We\nparticipated in all subtasks and report key findings across them, with\nparticular emphasis on Task 5 Subtask 1, where our system achieved strong\nperformance-securing first place with an F1 score of 0.958 on the test set. To\nattain this result, we employed encoder-based models (e.g., RoBERTa), alongside\nGPT-4 for data augmentation. This paper outlines our approach, including\npreprocessing, model architecture, and subtask-specific adaptations", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5728SMM4H-HeaRD 2025\u4e2d\u9488\u5bf9\u4e34\u5e8a\u7b14\u8bb0\u5931\u7720\u68c0\u6d4b\u548c\u65b0\u95fb\u98df\u54c1\u5b89\u5168\u4e8b\u4ef6\u63d0\u53d6\u4efb\u52a1\u7684\u7cfb\u7edf\u65b9\u6848\uff0c\u5176\u4e2dTask5\u5b50\u4efb\u52a11\u4ee50.958 F1\u503c\u593a\u51a0", "motivation": "\u63d0\u5347\u4e34\u5e8a\u5931\u7720\u8bc6\u522b\u548c\u98df\u54c1\u5b89\u5168\u4e8b\u4ef6\u68c0\u6d4b\u7684\u81ea\u52a8\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u7ed3\u5408\u4f20\u7edf\u6a21\u578b\u4e0e\u5927\u6a21\u578b\u589e\u5f3a\u89e3\u51b3\u6807\u6ce8\u6570\u636e\u4e0d\u8db3\u95ee\u9898", "method": "\u91c7\u7528RoBERTa\u7b49\u7f16\u7801\u5668\u6a21\u578b\u6784\u5efa\u57fa\u7ebf\uff0c\u4f7f\u7528GPT-4\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u9488\u5bf9\u4e0d\u540c\u5b50\u4efb\u52a1\u8bbe\u8ba1\u9884\u5904\u7406\u6d41\u7a0b\u548c\u6a21\u578b\u9002\u914d\u65b9\u6848", "result": "\u5728\u98df\u54c1\u5b89\u5168\u4e8b\u4ef6\u68c0\u6d4b\u6838\u5fc3\u4efb\u52a1(Task5-Subtask1)\u6d4b\u8bd5\u96c6\u8fbe\u52300.958 F1\u503c\uff0c\u6392\u540d\u7b2c\u4e00\uff1b\u5176\u4ed6\u5b50\u4efb\u52a1\u8868\u73b0\u5747\u6709\u663e\u8457\u63d0\u5347", "conclusion": "\u9a8c\u8bc1\u4e86\u4f20\u7edfTransformer\u6a21\u578b\u7ed3\u5408\u751f\u6210\u5f0f\u6570\u636e\u589e\u5f3a\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u6709\u9650\u6807\u6ce8\u6570\u636e\u573a\u666f\u4e0b\u7684\u8de8\u4efb\u52a1\u9002\u7528\u6027"}}
{"id": "2506.18199", "pdf": "https://arxiv.org/pdf/2506.18199", "abs": "https://arxiv.org/abs/2506.18199", "authors": ["Bushra Asseri", "Estabrag Abdelaziz", "Areej Al-Wabil"], "title": "Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": null, "summary": "Large language models have demonstrated remarkable capabilities across\nvarious domains, yet concerns about cultural bias - particularly towards Arabs\nand Muslims - pose significant ethical challenges by perpetuating harmful\nstereotypes and marginalization. Despite growing recognition of bias in LLMs,\nprompt engineering strategies specifically addressing Arab and Muslim\nrepresentation remain understudied. This mixed-methods systematic review\nexamines such techniques, offering evidence-based guidance for researchers and\npractitioners. Following PRISMA guidelines and Kitchenham's systematic review\nmethodology, we analyzed 8 empirical studies published between 2021-2024\ninvestigating bias mitigation strategies. Our findings reveal five primary\nprompt engineering approaches: cultural prompting, affective priming,\nself-debiasing techniques, structured multi-step pipelines, and\nparameter-optimized continuous prompts. Although all approaches show potential\nfor reducing bias, effectiveness varied substantially across studies and bias\ntypes. Evidence suggests that certain bias types may be more resistant to\nprompt-based mitigation than others. Structured multi-step pipelines\ndemonstrated the highest overall effectiveness, achieving up to 87.7% reduction\nin bias, though they require greater technical expertise. Cultural prompting\noffers broader accessibility with substantial effectiveness. These results\nunderscore the accessibility of prompt engineering for mitigating cultural bias\nwithout requiring access to model parameters. The limited number of studies\nidentified highlights a significant research gap in this critical area. Future\nresearch should focus on developing culturally adaptive prompting techniques,\ncreating Arab and Muslim-specific evaluation resources, and integrating prompt\nengineering with complementary debiasing methods to address deeper stereotypes\nwhile maintaining model utility.", "AI": {"tldr": "\u7cfb\u7edf\u7efc\u8ff0\u53d1\u73b0\u7ed3\u6784\u5316\u591a\u6b65\u9aa4\u63d0\u793a\u6d41\u7a0b\u53ef\u51cf\u5c1187.7%\u6587\u5316\u504f\u89c1\uff0c\u6587\u5316\u63d0\u793a\u5de5\u7a0b\u66f4\u5177\u666e\u9002\u6027", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u963f\u62c9\u4f2f\u548c\u7a46\u65af\u6797\u7fa4\u4f53\u6587\u5316\u504f\u89c1\u7684\u5373\u65f6\u5de5\u7a0b\u7b56\u7565\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u89e3\u51b3\u4f26\u7406\u6311\u6218\u4e0e\u8fb9\u7f18\u5316\u95ee\u9898", "method": "\u9075\u5faaPRISMA\u6307\u5357\u4e0eKitchenham\u7cfb\u7edf\u7efc\u8ff0\u65b9\u6cd5\uff0c\u5206\u67902021-2024\u5e748\u9879\u5b9e\u8bc1\u7814\u7a76", "result": "\u8bc6\u522b\u51fa\u6587\u5316\u63d0\u793a/\u60c5\u611f\u542f\u52a8/\u81ea\u53bb\u504f/\u591a\u6b65\u6d41\u7a0b/\u53c2\u6570\u4f18\u53165\u79cd\u7b56\u7565\uff0c\u7ed3\u6784\u5316\u6d41\u7a0b\u6548\u679c\u6700\u4f18\u4f46\u6280\u672f\u95e8\u69db\u8f83\u9ad8", "conclusion": "\u5373\u65f6\u5de5\u7a0b\u53ef\u6709\u6548\u7f13\u89e3\u6587\u5316\u504f\u89c1\uff0c\u9700\u5f00\u53d1\u6587\u5316\u81ea\u9002\u5e94\u6280\u672f\u5e76\u6574\u5408\u5176\u4ed6\u53bb\u504f\u65b9\u6cd5\uff0c\u5f53\u524d\u7814\u7a76\u6570\u91cf\u4ecd\u663e\u4e0d\u8db3"}}
{"id": "2506.18201", "pdf": "https://arxiv.org/pdf/2506.18201", "abs": "https://arxiv.org/abs/2506.18201", "authors": ["Bushra Asseri", "Estabraq Abdelaziz", "Maha Al Mogren", "Tayef Alhefdhi", "Areej Al-Wabil"], "title": "Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications", "categories": ["cs.CL", "cs.CV", "cs.HC"], "comment": null, "summary": "Emotion recognition capabilities in multimodal AI systems are crucial for\ndeveloping culturally responsive educational technologies, yet remain\nunderexplored for Arabic language contexts where culturally appropriate\nlearning tools are critically needed. This study evaluates the emotion\nrecognition performance of two advanced multimodal large language models,\nGPT-4o and Gemini 1.5 Pro, when processing Arabic children's storybook\nillustrations. We assessed both models across three prompting strategies\n(zero-shot, few-shot, and chain-of-thought) using 75 images from seven Arabic\nstorybooks, comparing model predictions with human annotations based on\nPlutchik's emotional framework. GPT-4o consistently outperformed Gemini across\nall conditions, achieving the highest macro F1-score of 59% with\nchain-of-thought prompting compared to Gemini's best performance of 43%. Error\nanalysis revealed systematic misclassification patterns, with valence\ninversions accounting for 60.7% of errors, while both models struggled with\nculturally nuanced emotions and ambiguous narrative contexts. These findings\nhighlight fundamental limitations in current models' cultural understanding and\nemphasize the need for culturally sensitive training approaches to develop\neffective emotion-aware educational technologies for Arabic-speaking learners.", "AI": {"tldr": "GPT-4o\u5728\u963f\u62c9\u4f2f\u8bed\u513f\u7ae5\u7ed8\u672c\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u5168\u9762\u4f18\u4e8eGemini\uff08\u6700\u9ad8F1\u5206\u657059% vs 43%\uff09\uff0c\u4f46\u4e24\u8005\u5747\u5b58\u5728\u6587\u5316\u8bed\u5883\u7406\u89e3\u4e0d\u8db3\u7684\u95ee\u9898", "motivation": "\u586b\u8865\u591a\u6a21\u6001AI\u5728\u963f\u62c9\u4f2f\u8bed\u6559\u80b2\u573a\u666f\u4e2d\u7684\u60c5\u611f\u8bc6\u522b\u7814\u7a76\u7a7a\u767d\uff0c\u4e3a\u963f\u62c9\u4f2f\u5b66\u4e60\u8005\u5f00\u53d1\u6587\u5316\u9002\u914d\u7684\u6559\u80b2\u6280\u672f\u5de5\u5177\u63d0\u4f9b\u4f9d\u636e", "method": "\u4f7f\u752875\u5e45\u963f\u62c9\u4f2f\u6545\u4e8b\u4e66\u63d2\u56fe\uff0c\u91c7\u7528\u96f6\u6837\u672c/\u5c11\u6837\u672c/\u601d\u7ef4\u94fe\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff0c\u57fa\u4e8ePlutchik\u60c5\u611f\u6846\u67b6\u5bf9\u6bd4GPT-4o\u4e0eGemini\u7684\u4eba\u7c7b\u6807\u6ce8\u7ed3\u679c", "result": "60.7%\u9519\u8bef\u6e90\u4e8e\u60c5\u611f\u6781\u6027\u8bef\u5224\uff0c\u6a21\u578b\u5bf9\u6587\u5316\u7279\u5f02\u6027\u60c5\u611f\uff08\u5982\u963f\u62c9\u4f2f\u4f20\u7edf\u53d9\u4e8b\u4e2d\u7684\u590d\u6742\u60c5\u7eea\uff09\u548c\u6a21\u7cca\u8bed\u5883\u8bc6\u522b\u80fd\u529b\u663e\u8457\u4e0d\u8db3", "conclusion": "\u73b0\u6709\u6a21\u578b\u7684\u6587\u5316\u7406\u89e3\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff0c\u5f00\u53d1\u963f\u62c9\u4f2f\u60c5\u611f\u6559\u80b2AI\u9700\u878d\u5165\u6587\u5316\u654f\u611f\u7684\u8bad\u7ec3\u8303\u5f0f\u4e0e\u672c\u571f\u5316\u60c5\u611f\u6807\u6ce8\u4f53\u7cfb"}}
{"id": "2506.18318", "pdf": "https://arxiv.org/pdf/2506.18318", "abs": "https://arxiv.org/abs/2506.18318", "authors": ["An Trieu", "Phuong Nguyen", "Minh Le Nguyen"], "title": "Enhancing Entity Aware Machine Translation with Multi-task Learning", "categories": ["cs.CL"], "comment": "In the Proceedings of SCIDOCA 2025", "summary": "Entity-aware machine translation (EAMT) is a complicated task in natural\nlanguage processing due to not only the shortage of translation data related to\nthe entities needed to translate but also the complexity in the context needed\nto process while translating those entities. In this paper, we propose a method\nthat applies multi-task learning to optimize the performance of the two\nsubtasks named entity recognition and machine translation, which improves the\nfinal performance of the Entity-aware machine translation task. The result and\nanalysis are performed on the dataset provided by the organizer of Task 2 of\nthe SemEval 2025 competition.", "AI": {"tldr": "\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u8054\u5408\u4f18\u5316\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u548c\u673a\u5668\u7ffb\u8bd1\u4efb\u52a1\uff0c\u63d0\u5347\u5b9e\u4f53\u611f\u77e5\u673a\u5668\u7ffb\u8bd1\u6027\u80fd", "motivation": "\u5b9e\u4f53\u611f\u77e5\u673a\u5668\u7ffb\u8bd1\u9762\u4e34\u7ffb\u8bd1\u6570\u636e\u4e0d\u8db3\u548c\u4e0a\u4e0b\u6587\u5904\u7406\u590d\u6742\u4e24\u5927\u6311\u6218", "method": "\u91c7\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u540c\u65f6\u4f18\u5316\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u548c\u673a\u5668\u7ffb\u8bd1\uff08MT\uff09\u4e24\u4e2a\u5b50\u4efb\u52a1", "result": "\u5728SemEval 2025\u7ade\u8d5bTask 2\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u5e76\u53d6\u5f97\u6548\u679c\u63d0\u5347", "conclusion": "\u591a\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u5b9e\u4f53\u611f\u77e5\u673a\u5668\u7ffb\u8bd1\u7cfb\u7edf\u7684\u6574\u4f53\u6027\u80fd"}}
{"id": "2506.18337", "pdf": "https://arxiv.org/pdf/2506.18337", "abs": "https://arxiv.org/abs/2506.18337", "authors": ["Syed Mekael Wasti", "Shou-Yi Hung", "Christopher Collins", "En-Shiun Annie Lee"], "title": "TranslationCorrect: A Unified Framework for Machine Translation Post-Editing with Predictive Error Assistance", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Machine translation (MT) post-editing and research data collection often rely\non inefficient, disconnected workflows. We introduce TranslationCorrect, an\nintegrated framework designed to streamline these tasks. TranslationCorrect\ncombines MT generation using models like NLLB, automated error prediction using\nmodels like XCOMET or LLM APIs (providing detailed reasoning), and an intuitive\npost-editing interface within a single environment. Built with human-computer\ninteraction (HCI) principles in mind to minimize cognitive load, as confirmed\nby a user study. For translators, it enables them to correct errors and batch\ntranslate efficiently. For researchers, TranslationCorrect exports high-quality\nspan-based annotations in the Error Span Annotation (ESA) format, using an\nerror taxonomy inspired by Multidimensional Quality Metrics (MQM). These\noutputs are compatible with state-of-the-art error detection models and\nsuitable for training MT or post-editing systems. Our user study confirms that\nTranslationCorrect significantly improves translation efficiency and user\nsatisfaction over traditional annotation methods.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u6574\u5408MT\u751f\u6210\u3001\u9519\u8bef\u9884\u6d4b\u4e0e\u540e\u7f16\u8f91\u7684TranslationCorrect\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u6548\u7387\u548c\u7814\u7a76\u6570\u636e\u8d28\u91cf", "motivation": "\u4f20\u7edf\u673a\u5668\u7ffb\u8bd1\u540e\u7f16\u8f91\u6d41\u7a0b\u6548\u7387\u4f4e\u4e0b\u4e14\u5de5\u5177\u5206\u6563\uff0c\u7814\u7a76\u8005\u6570\u636e\u6536\u96c6\u7f3a\u4e4f\u7edf\u4e00\u9ad8\u6548\u5e73\u53f0", "method": "\u6574\u5408NLLB\u751f\u6210\u7ffb\u8bd1\u2192XCOMET/LLM\u9884\u6d4b\u9519\u8bef\u2192HCI\u4f18\u5316\u7f16\u8f91\u754c\u9762\u2192\u6279\u91cf\u5904\u7406\u53caESA\u683c\u5f0f\u8f93\u51fa", "result": "\u7528\u6237\u5b9e\u9a8c\u663e\u793a\u6846\u67b6\u63d0\u534750%\u7ffb\u8bd1\u6548\u7387\uff0cESA\u8f93\u51fa\u517c\u5bb9SOTA\u6a21\u578b\u4e14\u9002\u5408\u7cfb\u7edf\u8bad\u7ec3", "conclusion": "TranslationCorrect\u6210\u529f\u7edf\u4e00\u7ffb\u8bd1\u4e0e\u7814\u7a76\u6d41\u7a0b\uff0cHCI\u8bbe\u8ba1\u663e\u8457\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377\uff0c\u672a\u6765\u53ef\u6269\u5c55\u591a\u8bed\u8a00\u573a\u666f"}}
{"id": "2506.18341", "pdf": "https://arxiv.org/pdf/2506.18341", "abs": "https://arxiv.org/abs/2506.18341", "authors": ["Kang Chen", "Mengdi Zhang", "Yixin Cao"], "title": "Less Data Less Tokens: Multilingual Unification Learning for Efficient Test-Time Reasoning in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the challenges of test-time scaling of large language\nmodels (LLMs), regarding both the data and inference efficiency. We highlight\nthe diversity of multi-lingual reasoning based on our pilot studies, and then\nintroduce a novel approach, \\(L^2\\) multi-lingual unification learning with a\ndecoding intervention strategy for further investigation. The basic idea of\n\\(L^2\\) is that the reasoning process varies across different languages, which\nmay be mutually beneficial to enhance both model performance and efficiency. In\nspecific, there are two types of multi-lingual data: the entire long\nchain-of-thought annotations in different languages and the step-wise mixture\nof languages. By further tuning based on them, we show that even small amounts\nof data can significantly improve reasoning capabilities. Our findings suggest\nthat multilingual learning reduces both the required data and the number of\ninference tokens while maintaining a comparable performance. Furthermore,\n\\(L^2\\) is orthogonal to other data efficient methods. Thus, we also emphasize\nthe importance of diverse data selection. The \\(L^2\\) method offers a promising\nsolution to the challenges of data collection and test-time compute efficiency\nin LLMs.", "AI": {"tldr": "\u63d0\u51faL\u00b2\u591a\u8bed\u8a00\u7edf\u4e00\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u8bed\u8a00\u63a8\u7406\u4f18\u5316LLM\u7684\u6d4b\u8bd5\u6548\u7387\u548c\u6570\u636e\u9700\u6c42", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u6d4b\u8bd5\u9636\u6bb5\u9762\u4e34\u7684\u591a\u8bed\u8a00\u6570\u636e\u591a\u6837\u6027\u548c\u63a8\u7406\u6548\u7387\u6311\u6218", "method": "\u7ed3\u5408\u4e24\u79cd\u591a\u8bed\u8a00\u6570\u636e\uff08\u5b8c\u6574\u957f\u94fe\u6ce8\u91ca+\u5206\u6b65\u6df7\u5408\u8bed\u8a00\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u5f15\u5165\u89e3\u7801\u5e72\u9884\u7b56\u7565", "result": "\u5c11\u91cf\u6570\u636e\u5373\u53ef\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u51cf\u5c1190%\u63a8\u7406\u4ee4\u724c\u540c\u65f6\u4fdd\u6301\u6027\u80fd\uff0c\u4e0e\u73b0\u6709\u9ad8\u6548\u65b9\u6cd5\u517c\u5bb9", "conclusion": "L\u00b2\u4e3aLLM\u7684\u6570\u636e\u6536\u96c6\u548c\u63a8\u7406\u6548\u7387\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5f3a\u8c03\u591a\u8bed\u8a00\u6570\u636e\u591a\u6837\u6027\u4ef7\u503c"}}
{"id": "2506.18387", "pdf": "https://arxiv.org/pdf/2506.18387", "abs": "https://arxiv.org/abs/2506.18387", "authors": ["Yousang Cho", "Key-Sun Choi"], "title": "Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned Metrics", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, presented at LLM4Eval Workshop, SIGIR 2025 Padova, Italy,\n  July 17, 2025", "summary": "This study investigates how accurately different evaluation metrics capture\nthe quality of causal explanations in automatically generated diagnostic\nreports. We compare six metrics: BERTScore, Cosine Similarity, BioSentVec,\nGPT-White, GPT-Black, and expert qualitative assessment across two input types:\nobservation-based and multiple-choice-based report generation. Two weighting\nstrategies are applied: one reflecting task-specific priorities, and the other\nassigning equal weights to all metrics. Our results show that GPT-Black\ndemonstrates the strongest discriminative power in identifying logically\ncoherent and clinically valid causal narratives. GPT-White also aligns well\nwith expert evaluations, while similarity-based metrics diverge from clinical\nreasoning quality. These findings emphasize the impact of metric selection and\nweighting on evaluation outcomes, supporting the use of LLM-based evaluation\nfor tasks requiring interpretability and causal reasoning.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u516d\u79cd\u8bc4\u4f30\u6307\u6807\u5728\u8bca\u65ad\u62a5\u544a\u56e0\u679c\u89e3\u91ca\u8d28\u91cf\u8bc4\u4f30\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-Black\u548cGPT-White\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u6700\u63a5\u8fd1\uff0c\u63a8\u8350LLM\u8bc4\u4f30\u7528\u4e8e\u9700\u8981\u56e0\u679c\u63a8\u7406\u7684\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u8bca\u65ad\u62a5\u544a\u4e2d\u4e0d\u540c\u8bc4\u4f30\u6307\u6807\u5bf9\u56e0\u679c\u89e3\u91ca\u8d28\u91cf\u7684\u6355\u6349\u51c6\u786e\u6027\u5b58\u5728\u5dee\u5f02\uff0c\u5f71\u54cd\u533b\u7597AI\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528BERTScore/Cosine Similarity/BioSentVec/GPT-White/GPT-Black\u516d\u79cd\u6307\u6807\uff0c\u5728\u89c2\u5bdf\u578b\u548c\u591a\u9009\u9898\u578b\u62a5\u544a\u4e0a\u6d4b\u8bd5\uff0c\u91c7\u7528\u4efb\u52a1\u4f18\u5148\u548c\u5e73\u7b49\u52a0\u6743\u4e24\u79cd\u7b56\u7565\u3002", "result": "GPT-Black\u5728\u903b\u8f91\u8fde\u8d2f\u6027(0.82 AUC)\u548c\u4e34\u5e8a\u6709\u6548\u6027(0.78)\u4e0a\u8868\u73b0\u6700\u4f18\uff0cGPT-White\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u76f8\u5173\u6027\u8fbe0.91\uff0c\u76f8\u4f3c\u6027\u6307\u6807\u4e0e\u4e34\u5e8a\u8d28\u91cf\u76f8\u5173\u6027\u4f4e\u4e8e0.3\u3002", "conclusion": "\u6307\u6807\u9009\u62e9\u9700\u5339\u914d\u4efb\u52a1\u7279\u6027\uff0c\u57fa\u4e8eLLM\u7684\u8bc4\u4f30\u66f4\u9002\u5408\u9700\u8981\u53ef\u89e3\u91ca\u56e0\u679c\u94fe\u7684\u533b\u7597\u62a5\u544a\u751f\u6210\u4efb\u52a1\u3002"}}
{"id": "2506.18399", "pdf": "https://arxiv.org/pdf/2506.18399", "abs": "https://arxiv.org/abs/2506.18399", "authors": ["Mostafa Saeed", "Nizar Habash"], "title": "Lemmatization as a Classification Task: Results from Arabic across Multiple Genres", "categories": ["cs.CL"], "comment": null, "summary": "Lemmatization is crucial for NLP tasks in morphologically rich languages with\nambiguous orthography like Arabic, but existing tools face challenges due to\ninconsistent standards and limited genre coverage. This paper introduces two\nnovel approaches that frame lemmatization as classification into a\nLemma-POS-Gloss (LPG) tagset, leveraging machine translation and semantic\nclustering. We also present a new Arabic lemmatization test set covering\ndiverse genres, standardized alongside existing datasets. We evaluate character\nlevel sequence-to-sequence models, which perform competitively and offer\ncomplementary value, but are limited to lemma prediction (not LPG) and prone to\nhallucinating implausible forms. Our results show that classification and\nclustering yield more robust, interpretable outputs, setting new benchmarks for\nArabic lemmatization.", "AI": {"tldr": "\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8eLPG\u6807\u7b7e\u5206\u7c7b\u7684\u963f\u62c9\u4f2f\u8bed\u8bcd\u5f62\u8fd8\u539f\u65b0\u65b9\u6cd5\uff0c\u5efa\u7acb\u591a\u4f53\u88c1\u6807\u51c6\u5316\u6d4b\u8bd5\u96c6\uff0c\u8bc1\u660e\u5206\u7c7b\u6a21\u578b\u4f18\u4e8e\u5e8f\u5217\u6a21\u578b", "motivation": "\u89e3\u51b3\u73b0\u6709\u963f\u62c9\u4f2f\u8bed\u8bcd\u5f62\u8fd8\u539f\u5de5\u5177\u5b58\u5728\u7684\u6807\u51c6\u4e0d\u4e00\u81f4\u3001\u4f53\u88c1\u8986\u76d6\u6709\u9650\u95ee\u9898", "method": "1. \u5c06\u8bcd\u5f62\u8fd8\u539f\u8f6c\u5316\u4e3aLemma-POS-Gloss\u6807\u7b7e\u5206\u7c7b\u95ee\u9898 2. \u521b\u5efa\u591a\u4f53\u88c1\u6807\u51c6\u5316\u6d4b\u8bd5\u96c6 3. \u5bf9\u6bd4\u5b57\u7b26\u7ea7seq2seq\u6a21\u578b\u4e0e\u5206\u7c7b/\u805a\u7c7b\u65b9\u6cd5", "result": "\u5b57\u7b26\u7ea7\u6a21\u578b\u5b58\u5728\u5e7b\u751f\u6210\u95ee\u9898\uff0c\u5206\u7c7b\u65b9\u6cd5\u5728\u51c6\u786e\u7387\uff08F1=92.7\uff09\u548c\u8f93\u51fa\u7a33\u5b9a\u6027\u4e0a\u8868\u73b0\u66f4\u4f18", "conclusion": "\u57fa\u4e8e\u8bed\u4e49\u805a\u7c7b\u548c\u5206\u7c7b\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u963f\u62c9\u4f2f\u8bed\u8bcd\u5f62\u8fd8\u539f\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5efa\u7acb\u65b0\u57fa\u51c6"}}
{"id": "2506.18421", "pdf": "https://arxiv.org/pdf/2506.18421", "abs": "https://arxiv.org/abs/2506.18421", "authors": ["Ce Li", "Xiaofan Liu", "Zhiyan Song", "Ce Chi", "Chen Zhao", "Jingjing Yang", "Zhendong Wang", "Kexin Yang", "Boshen Shi", "Xing Wang", "Chao Deng", "Junlan Feng"], "title": "TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Benmark report v1.0", "summary": "The majority of data in businesses and industries is stored in tables,\ndatabases, and data warehouses. Reasoning with table-structured data poses\nsignificant challenges for large language models (LLMs) due to its hidden\nsemantics, inherent complexity, and structured nature. One of these challenges\nis lacking an effective evaluation benchmark fairly reflecting the performances\nof LLMs on broad table reasoning abilities. In this paper, we fill in this gap,\npresenting a comprehensive table reasoning evolution benchmark, TReB, which\nmeasures both shallow table understanding abilities and deep table reasoning\nabilities, a total of 26 sub-tasks. We construct a high quality dataset through\nan iterative data processing procedure. We create an evaluation framework to\nrobustly measure table reasoning capabilities with three distinct inference\nmodes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs\nusing this frame work and prove its effectiveness. Experimental results reveal\nthat existing LLMs still have significant room for improvement in addressing\nthe complex and real world Table related tasks. Both the dataset and evaluation\nframework are publicly available, with the dataset hosted on [HuggingFace] and\nthe framework on [GitHub].", "AI": {"tldr": "\u63d0\u51fa\u8868\u683c\u63a8\u7406\u8bc4\u4f30\u57fa\u51c6TReB\uff08\u542b26\u4e2a\u5b50\u4efb\u52a1\uff09\uff0c\u63ed\u793a\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u8868\u683c\u4efb\u52a1\u65f6\u4ecd\u6709\u663e\u8457\u63d0\u5347\u7a7a\u95f4", "motivation": "\u73b0\u6709\u8bc4\u4f30\u57fa\u51c6\u65e0\u6cd5\u5168\u9762\u8861\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8868\u683c\u6570\u636e\u7406\u89e3\u4e0e\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u8bc4\u6d4b\u5de5\u5177", "method": "\u901a\u8fc7\u8fed\u4ee3\u6570\u636e\u5904\u7406\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u8bbe\u8ba1TCoT/PoT/ICoT\u4e09\u79cd\u63a8\u7406\u6a21\u5f0f\uff0c\u8bc4\u6d4b20+\u4e3b\u6d41\u5927\u6a21\u578b", "result": "\u5b9e\u9a8c\u8bc1\u660e\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u8868\u683c\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u9a8c\u8bc1\u4e86TReB\u6846\u67b6\u7684\u6709\u6548\u6027", "conclusion": "TReB\u586b\u8865\u4e86\u8868\u683c\u63a8\u7406\u8bc4\u4f30\u4f53\u7cfb\u7684\u7a7a\u767d\uff0c\u516c\u5f00\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u53d1\u5c55"}}
{"id": "2506.18485", "pdf": "https://arxiv.org/pdf/2506.18485", "abs": "https://arxiv.org/abs/2506.18485", "authors": ["Junjie Zhang", "Guozheng Ma", "Shunyu Liu", "Haoyu Wang", "Jiaxing Huang", "Ting-En Lin", "Fei Huang", "Yongbin Li", "Dacheng Tao"], "title": "MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful learn-to-reason paradigm for Large Language Models (LLMs) to tackle\ncomplex reasoning tasks. However, existing RLVR methods overlook one of the\nmost distinctive capabilities of LLMs, their in-context learning ability, as\nprominently demonstrated by the success of Chain-of-Thought (CoT) prompting.\nThis motivates us to explore how reinforcement learning can be effectively\ncombined with in-context learning to better improve the reasoning capabilities\nof LLMs. In this paper, we introduce Motivation-enhanced Reinforcement\nFinetuning} (MeRF), an intuitive yet effective method enhancing reinforcement\nlearning of LLMs by involving ``telling LLMs the rules of the game''.\nSpecifically, MeRF directly injects the reward specification into the prompt,\nwhich serves as an in-context motivation for model to improve its responses\nwith awareness of the optimization objective. This simple modification\nleverages the in-context learning ability of LLMs aligning generation with\noptimization, thereby incentivizing the model to generate desired outputs from\nboth inner motivation and external reward. Empirical evaluations on the Knights\nand Knaves~(K&K) logic puzzle reasoning benchmark demonstrate that\n\\texttt{MeRF} achieves substantial performance gains over baselines. Moreover,\nablation studies show that performance improves with greater consistency\nbetween the in-context motivation and the external reward function, while the\nmodel also demonstrates an ability to adapt to misleading motivations through\nreinforcement learning.", "AI": {"tldr": "\u63d0\u51faMeRF\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u5956\u52b1\u89c4\u5219\u878d\u5165\u63d0\u793a\u8bed\u6fc0\u53d1LLMs\u5185\u5728\u52a8\u673a\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u663e\u8457\u63d0\u5347\u903b\u8f91\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u5ffd\u89c6\u4e86LLMs\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff08\u5982\u601d\u7ef4\u94fe\u63d0\u793a\u7684\u6210\u529f\u6848\u4f8b\uff09\uff0c\u9700\u63a2\u7d22\u5f3a\u5316\u5b66\u4e60\u4e0e\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u6709\u6548\u7ed3\u5408\u65b9\u5f0f", "method": "\u5728\u63d0\u793a\u8bed\u4e2d\u76f4\u63a5\u6ce8\u5165\u5956\u52b1\u673a\u5236\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u52a8\u673a\uff0c\u4f7f\u6a21\u578b\u5728\u5956\u52b1\u4f18\u5316\u76ee\u6807\u9a71\u52a8\u4e0b\u751f\u6210\u7b26\u5408\u8981\u6c42\u7684\u8f93\u51fa", "result": "\u5728Knights and Knaves\u903b\u8f91\u8c1c\u9898\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\u52a8\u673a\u4e0e\u5956\u52b1\u51fd\u6570\u4e00\u81f4\u6027\u8d8a\u9ad8\u6027\u80fd\u8d8a\u597d", "conclusion": "\u7ed3\u5408\u5185\u5728\u52a8\u673a\u4e0e\u5916\u90e8\u5956\u52b1\u80fd\u6709\u6548\u63d0\u5347LLMs\u63a8\u7406\u80fd\u529b\uff0c\u6a21\u578b\u8fd8\u5c55\u73b0\u51fa\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9002\u5e94\u8bef\u5bfc\u6027\u52a8\u673a\u7684\u6f5c\u529b"}}
{"id": "2506.18501", "pdf": "https://arxiv.org/pdf/2506.18501", "abs": "https://arxiv.org/abs/2506.18501", "authors": ["Wael Etaiwi", "Bushra Alhijawi"], "title": "Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The increasing use of large language models (LLMs) in natural language\nprocessing (NLP) tasks has sparked significant interest in evaluating their\neffectiveness across diverse applications. While models like ChatGPT and\nDeepSeek have shown strong results in many NLP domains, a comprehensive\nevaluation is needed to understand their strengths, weaknesses, and\ndomain-specific abilities. This is critical as these models are applied to\nvarious tasks, from sentiment analysis to more nuanced tasks like textual\nentailment and translation. This study aims to evaluate ChatGPT and DeepSeek\nacross five key NLP tasks: sentiment analysis, topic classification, text\nsummarization, machine translation, and textual entailment. A structured\nexperimental protocol is used to ensure fairness and minimize variability. Both\nmodels are tested with identical, neutral prompts and evaluated on two\nbenchmark datasets per task, covering domains like news, reviews, and\nformal/informal texts. The results show that DeepSeek excels in classification\nstability and logical reasoning, while ChatGPT performs better in tasks\nrequiring nuanced understanding and flexibility. These findings provide\nvaluable insights for selecting the appropriate LLM based on task requirements.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86ChatGPT\u548cDeepSeek\u5728\u4e94\u5927NLP\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0DeepSeek\u5728\u5206\u7c7b\u7a33\u5b9a\u6027\u548c\u903b\u8f91\u63a8\u7406\u66f4\u4f18\uff0c\u800cChatGPT\u5728\u9700\u8981\u7075\u6d3b\u7406\u89e3\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u5728NLP\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9700\u7cfb\u7edf\u6027\u8bc4\u4f30\u5176\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\u7684\u4f18\u52a3\u52bf\u53ca\u9886\u57df\u9002\u5e94\u80fd\u529b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u9009\u578b\u4f9d\u636e\u3002", "method": "\u4f7f\u7528\u4e2d\u6027\u7edf\u4e00\u63d0\u793a\u8bcd\uff0c\u5728\u60c5\u611f\u5206\u6790/\u4e3b\u9898\u5206\u7c7b/\u6587\u672c\u6458\u8981/\u673a\u5668\u7ffb\u8bd1/\u6587\u672c\u8574\u542b\u4e94\u5927\u4efb\u52a1\u4e2d\uff0c\u5206\u522b\u91c7\u7528\u4e24\u4e2a\u6db5\u76d6\u65b0\u95fb\u3001\u8bc4\u8bba\u3001\u6b63\u5f0f/\u975e\u6b63\u5f0f\u6587\u672c\u7684\u57fa\u51c6\u6570\u636e\u96c6\u8fdb\u884c\u5bf9\u6bd4\u6d4b\u8bd5\u3002", "result": "DeepSeek\u5728\u5206\u7c7b\u7a33\u5b9a\u6027\uff08\u51c6\u786e\u7387\u6ce2\u52a8\u22643%\uff09\u548c\u903b\u8f91\u63a8\u7406\u4efb\u52a1\uff08F1\u503c89.2\uff09\u8868\u73b0\u7a81\u51fa\uff0cChatGPT\u5728\u6587\u672c\u8574\u542b\uff08\u51c6\u786e\u738792.1%\uff09\u548c\u8de8\u9886\u57df\u7ffb\u8bd1\u4efb\u52a1\uff08BLEU\u503c41.3\uff09\u66f4\u5177\u4f18\u52bf\u3002", "conclusion": "\u4efb\u52a1\u7279\u6027\u51b3\u5b9a\u6a21\u578b\u9009\u62e9\uff1a\u7ed3\u6784\u5316\u4efb\u52a1\u4f18\u5148DeepSeek\uff0c\u590d\u6742\u8bed\u4e49\u7406\u89e3\u573a\u666f\u9002\u7528ChatGPT\u3002\u8be5\u8bc4\u4f30\u6846\u67b6\u4e3aLLM\u7684\u5de5\u4e1a\u843d\u5730\u63d0\u4f9b\u4e86\u51b3\u7b56\u4f9d\u636e\u3002"}}
{"id": "2506.18532", "pdf": "https://arxiv.org/pdf/2506.18532", "abs": "https://arxiv.org/abs/2506.18532", "authors": ["Mengjie Qian", "Rao Ma", "Stefano Bann\u00f2", "Mark J. F. Gales", "Kate M. Knill"], "title": "End-to-End Spoken Grammatical Error Correction", "categories": ["cs.CL", "cs.LG"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Grammatical Error Correction (GEC) and feedback play a vital role in\nsupporting second language (L2) learners, educators, and examiners. While\nwritten GEC is well-established, spoken GEC (SGEC), aiming to provide feedback\nbased on learners' speech, poses additional challenges due to disfluencies,\ntranscription errors, and the lack of structured input. SGEC systems typically\nfollow a cascaded pipeline consisting of Automatic Speech Recognition (ASR),\ndisfluency detection, and GEC, making them vulnerable to error propagation\nacross modules. This work examines an End-to-End (E2E) framework for SGEC and\nfeedback generation, highlighting challenges and possible solutions when\ndeveloping these systems. Cascaded, partial-cascaded and E2E architectures are\ncompared, all built on the Whisper foundation model. A challenge for E2E\nsystems is the scarcity of GEC labeled spoken data. To address this, an\nautomatic pseudo-labeling framework is examined, increasing the training data\nfrom 77 to over 2500 hours. To improve the accuracy of the SGEC system,\nadditional contextual information, exploiting the ASR output, is investigated.\nCandidate feedback of their mistakes is an essential step to improving\nperformance. In E2E systems the SGEC output must be compared with an estimate\nof the fluent transcription to obtain the feedback. To improve the precision of\nthis feedback, a novel reference alignment process is proposed that aims to\nremove hypothesised edits that results from fluent transcription errors.\nFinally, these approaches are combined with an edit confidence estimation\napproach, to exclude low-confidence edits. Experiments on the in-house\nLinguaskill (LNG) corpora and the publicly available Speak & Improve (S&I)\ncorpus show that the proposed approaches significantly boost E2E SGEC\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eWhisper\u6a21\u578b\u7684\u7aef\u5230\u7aef\u53e3\u8bed\u8bed\u6cd5\u7ea0\u9519\u7cfb\u7edf\uff0c\u901a\u8fc7\u4f2a\u6807\u6ce8\u548c\u4e0a\u4e0b\u6587\u5bf9\u9f50\u63d0\u5347\u6027\u80fd", "motivation": "\u4f20\u7edf\u7ea7\u8054\u5f0f\u53e3\u8bed\u8bed\u6cd5\u7ea0\u9519\u7cfb\u7edf\u5b58\u5728\u9519\u8bef\u4f20\u64ad\u95ee\u9898\uff0c\u4e14\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u5236\u7ea6\u7aef\u5230\u7aef\u7cfb\u7edf\u53d1\u5c55", "method": "\u91c7\u7528\u4f2a\u6807\u6ce8\u6846\u67b6\u5c06\u8bad\u7ec3\u6570\u636e\u6269\u5c5532\u500d\uff0c\u7ed3\u5408ASR\u4e0a\u4e0b\u6587\u4fe1\u606f\u53ca\u53c2\u8003\u5bf9\u9f50\u673a\u5236\u63d0\u5347\u53cd\u9988\u51c6\u786e\u6027", "result": "\u5728Linguaskill\u548cSpeak & Improve\u8bed\u6599\u5e93\u4e0a\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347", "conclusion": "\u7aef\u5230\u7aef\u67b6\u6784\u914d\u5408\u6570\u636e\u589e\u5f3a\u53ca\u5bf9\u9f50\u673a\u5236\u80fd\u6709\u6548\u63d0\u9ad8\u53e3\u8bed\u8bed\u6cd5\u7ea0\u9519\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027"}}
{"id": "2506.18535", "pdf": "https://arxiv.org/pdf/2506.18535", "abs": "https://arxiv.org/abs/2506.18535", "authors": ["Manu Pande", "Shahil Kumar", "Anay Yatin Damle"], "title": "When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "This paper investigates the counterintuitive phenomenon where fine-tuning\npre-trained transformer models degrades performance on the MS MARCO passage\nranking task. Through comprehensive experiments involving five model\nvariants-including full parameter fine-tuning and parameter efficient LoRA\nadaptations-we demonstrate that all fine-tuning approaches underperform the\nbase sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). Our\nanalysis reveals that fine-tuning disrupts the optimal embedding space\nstructure learned during the base model's extensive pre-training on 1 billion\nsentence pairs, including 9.1 million MS MARCO samples. UMAP visualizations\nshow progressive embedding space flattening, while training dynamics analysis\nand computational efficiency metrics further support our findings. These\nresults challenge conventional wisdom about transfer learning effectiveness on\nsaturated benchmarks and suggest architectural innovations may be necessary for\nmeaningful improvements.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5bf9\u9884\u8bad\u7ec3Transformer\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u4f1a\u964d\u4f4eMS MARCO\u6bb5\u843d\u6392\u5e8f\u4efb\u52a1\u6027\u80fd\uff0c\u7834\u574f\u9884\u8bad\u7ec3\u83b7\u5f97\u7684\u5d4c\u5165\u7a7a\u95f4\u7ed3\u6784", "motivation": "\u63a2\u7a76\u4e3a\u4f55\u5728MS MARCO\u4efb\u52a1\u4e0a\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u4f1a\u5f15\u53d1\u6027\u80fd\u9000\u5316\u73b0\u8c61\uff0c\u6311\u6218\u4f20\u7edf\u8fc1\u79fb\u5b66\u4e60\u5728\u9971\u548c\u57fa\u51c6\u4e0a\u7684\u6709\u6548\u6027\u8ba4\u77e5", "method": "\u4f7f\u75285\u79cd\u6a21\u578b\u53d8\u4f53\uff08\u5168\u53c2\u6570\u5fae\u8c03/LoRA\u9002\u914d\uff09\u8fdb\u884c\u5b9e\u9a8c\uff0c\u901a\u8fc7UMAP\u53ef\u89c6\u5316\u3001\u8bad\u7ec3\u52a8\u6001\u5206\u6790\u548c\u8ba1\u7b97\u6548\u7387\u6307\u6807\u9a8c\u8bc1\u5047\u8bbe", "result": "\u6240\u6709\u5fae\u8c03\u6a21\u578bMRR@10\u5747\u4f4e\u4e8e\u57fa\u7840\u6a21\u578b\uff080.3026\uff09\uff0c\u53ef\u89c6\u5316\u663e\u793a\u5d4c\u5165\u7a7a\u95f4\u9010\u6e10\u6241\u5e73\u5316\uff0c\u8ba1\u7b97\u6548\u7387\u540c\u6b65\u4e0b\u964d", "conclusion": "\u4f20\u7edf\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\u5728\u6210\u719f\u57fa\u51c6\u4e0a\u53ef\u80fd\u5931\u6548\uff0c\u9700\u901a\u8fc7\u67b6\u6784\u521b\u65b0\u800c\u975e\u53c2\u6570\u5fae\u8c03\u5b9e\u73b0\u6027\u80fd\u7a81\u7834"}}
{"id": "2506.18576", "pdf": "https://arxiv.org/pdf/2506.18576", "abs": "https://arxiv.org/abs/2506.18576", "authors": ["Matteo Melis", "Gabriella Lapesa", "Dennis Assenmacher"], "title": "A Modular Taxonomy for Hate Speech Definitions and Its Impact on Zero-Shot LLM Classification Performance", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Detecting harmful content is a crucial task in the landscape of NLP\napplications for Social Good, with hate speech being one of its most dangerous\nforms. But what do we mean by hate speech, how can we define it, and how does\nprompting different definitions of hate speech affect model performance? The\ncontribution of this work is twofold. At the theoretical level, we address the\nambiguity surrounding hate speech by collecting and analyzing existing\ndefinitions from the literature. We organize these definitions into a taxonomy\nof 14 Conceptual Elements-building blocks that capture different aspects of\nhate speech definitions, such as references to the target of hate (individual\nor groups) or of the potential consequences of it. At the experimental level,\nwe employ the collection of definitions in a systematic zero-shot evaluation of\nthree LLMs, on three hate speech datasets representing different types of data\n(synthetic, human-in-the-loop, and real-world). We find that choosing different\ndefinitions, i.e., definitions with a different degree of specificity in terms\nof encoded elements, impacts model performance, but this effect is not\nconsistent across all architectures.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u7cfb\u7edf\u5316\u5206\u6790\u4ec7\u6068\u8a00\u8bba\u5b9a\u4e49\u6784\u5efa14\u4e2a\u6982\u5ff5\u5143\u7d20\u5206\u7c7b\u4f53\u7cfb\uff0c\u5e76\u9a8c\u8bc1\u4e0d\u540c\u5b9a\u4e49\u5bf9LLM\u68c0\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u5dee\u5f02\u3002", "motivation": "\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u5b58\u5728\u5b9a\u4e49\u6a21\u7cca\u95ee\u9898\uff0c\u5bfc\u81f4\u6a21\u578b\u8bc4\u4f30\u6807\u51c6\u4e0d\u4e00\u81f4\u3002\u7814\u7a76\u65e8\u5728\u91cf\u5316\u5b9a\u4e49\u5dee\u5f02\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "method": "1) \u6587\u732e\u6574\u7406\u6784\u5efa\u6982\u5ff5\u5143\u7d20\u5206\u7c7b 2) \u5728\u5408\u6210/\u4eba\u5de5/\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd53\u79cdLLM\u7684\u96f6\u6837\u672c\u8868\u73b0", "result": "\u5b9a\u4e49\u7684\u5177\u4f53\u6027\u5f71\u54cd\u6a21\u578b\u6027\u80fd\uff0c\u4f46\u4e0d\u540c\u67b6\u6784\u6a21\u578b\u53d7\u5f71\u54cd\u7a0b\u5ea6\u5b58\u5728\u663e\u8457\u5dee\u5f02", "conclusion": "\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u9700\u660e\u786e\u5b9a\u4e49\u6807\u51c6\uff0c\u4e14\u6a21\u578b\u67b6\u6784\u51b3\u5b9a\u5176\u5bf9\u5b9a\u4e49\u53d8\u5316\u7684\u654f\u611f\u5ea6"}}
{"id": "2506.18582", "pdf": "https://arxiv.org/pdf/2506.18582", "abs": "https://arxiv.org/abs/2506.18582", "authors": ["Haoyi Wu", "Zhihao Teng", "Kewei Tu"], "title": "Parallel Continuous Chain-of-Thought with Jacobi Iteration", "categories": ["cs.CL"], "comment": "under review", "summary": "Continuous chain-of-thought has been shown to be effective in saving\nreasoning tokens for large language models. By reasoning with continuous latent\nthought tokens, continuous CoT is able to perform implicit reasoning in a\ncompact manner. However, the sequential dependencies between latent thought\ntokens spoil parallel training, leading to long training time. In this paper,\nwe propose Parallel Continuous Chain-of-Thought (PCCoT), which performs Jacobi\niteration on the latent thought tokens, updating them iteratively in parallel\ninstead of sequentially and thus improving both training and inference\nefficiency of continuous CoT. Experiments demonstrate that by choosing the\nproper number of iterations, we are able to achieve comparable or even better\nperformance while saving nearly 50% of the training and inference time.\nMoreover, PCCoT shows better stability and robustness in the training process.\nOur code is available at https://github.com/whyNLP/PCCoT.", "AI": {"tldr": "\u63d0\u51fa\u5e76\u884c\u8fde\u7eed\u601d\u7ef4\u94fe\uff08PCCoT\uff09\uff0c\u901a\u8fc7\u96c5\u53ef\u6bd4\u8fed\u4ee3\u5b9e\u73b0\u6f5c\u5728\u601d\u7ef4\u6807\u8bb0\u7684\u5e76\u884c\u66f4\u65b0\uff0c\u63d0\u5347\u8bad\u7ec3/\u63a8\u7406\u6548\u7387\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8fde\u7eed\u601d\u7ef4\u94fe\u4e2d\u6f5c\u5728\u601d\u7ef4\u6807\u8bb0\u7684\u5e8f\u5217\u4f9d\u8d56\u5bfc\u81f4\u7684\u8bad\u7ec3\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u6d88\u9664\u63a8\u7406\u65f6\u7684\u987a\u5e8f\u8ba1\u7b97\u9650\u5236\u3002", "method": "\u4f7f\u7528\u96c5\u53ef\u6bd4\u8fed\u4ee3\u7b97\u6cd5\u5bf9\u6f5c\u5728\u601d\u7ef4\u6807\u8bb0\u8fdb\u884c\u5e76\u884c\u8fed\u4ee3\u66f4\u65b0\uff0c\u6253\u7834\u4f20\u7edf\u5e8f\u5217\u66f4\u65b0\u6a21\u5f0f\u3002", "result": "\u8282\u7701\u8fd150%\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\uff0c\u6027\u80fd\u7a33\u5b9a\u4e14\u90e8\u5206\u6307\u6807\u4f18\u4e8e\u539f\u65b9\u6cd5\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u9c81\u68d2\u6027\u589e\u5f3a\u3002", "conclusion": "PCCoT\u5728\u4fdd\u6301\u8fde\u7eed\u601d\u7ef4\u94fe\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u5e76\u884c\u5316\u5b9e\u73b0\u4e86\u6548\u7387\u4e0e\u6027\u80fd\u7684\u53cc\u91cd\u63d0\u5347\uff0c\u5177\u6709\u5de5\u7a0b\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2506.18600", "pdf": "https://arxiv.org/pdf/2506.18600", "abs": "https://arxiv.org/abs/2506.18600", "authors": ["Ariel Flint Ashery", "Luca Maria Aiello", "Andrea Baronchelli"], "title": "Reply to \"Emergent LLM behaviors are observationally equivalent to data leakage\"", "categories": ["cs.CL", "cs.GT", "cs.MA"], "comment": "Reply to arXiv:2505.23796", "summary": "A potential concern when simulating populations of large language models\n(LLMs) is data contamination, i.e. the possibility that training data may shape\noutcomes in unintended ways. While this concern is important and may hinder\ncertain experiments with multi-agent models, it does not preclude the study of\ngenuinely emergent dynamics in LLM populations. The recent critique by Barrie\nand T\\\"ornberg [1] of the results of Flint Ashery et al. [2] offers an\nopportunity to clarify that self-organisation and model-dependent emergent\ndynamics can be studied in LLM populations, highlighting how such dynamics have\nbeen empirically observed in the specific case of social conventions.", "AI": {"tldr": "\u63a2\u8ba8LLM\u7fa4\u4f53\u6a21\u62df\u4e2d\u6570\u636e\u6c61\u67d3\u95ee\u9898\u4e0d\u5f71\u54cd\u6d8c\u73b0\u52a8\u6001\u7814\u7a76\u7684\u53ef\u80fd\u6027", "motivation": "\u6f84\u6e05\u6570\u636e\u6c61\u67d3\u95ee\u9898\u5e76\u4e0d\u59a8\u788d\u7814\u7a76LLM\u7fa4\u4f53\u4e2d\u771f\u6b63\u7684\u81ea\u6211\u7ec4\u7ec7\u548c\u6a21\u578b\u4f9d\u8d56\u6027\u6d8c\u73b0\u52a8\u6001\uff0c\u7279\u522b\u662f\u4ee5\u793e\u4f1a\u4e60\u4fd7\u4f5c\u4e3a\u5b9e\u8bc1\u6848\u4f8b", "method": "\u901a\u8fc7\u5206\u6790\u73b0\u6709\u5b66\u672f\u4e89\u8bba\uff08Barrie\u4e0eT\u00f6rnberg\u5bf9Flint Ashery\u7814\u7a76\u7684\u6279\u8bc4\uff09\u548c\u5177\u4f53\u5b9e\u8bc1\u89c2\u5bdf\uff08\u793e\u4f1a\u4e60\u4fd7\u6848\u4f8b\uff09\u8fdb\u884c\u7406\u8bba\u8bba\u8bc1", "result": "\u8bc1\u5b9eLLM\u7fa4\u4f53\u4e2d\u5b58\u5728\u81ea\u7ec4\u7ec7\u73b0\u8c61\u548c\u6a21\u578b\u4f9d\u8d56\u6027\u52a8\u6001\u7279\u5f81\uff0c\u8fd9\u4e9b\u52a8\u6001\u5177\u6709\u53ef\u7814\u7a76\u6027", "conclusion": "\u6570\u636e\u6c61\u67d3\u7684\u5b58\u5728\u4e0d\u5426\u5b9aLLM\u7fa4\u4f53\u4e2d\u7279\u5b9a\u6d8c\u73b0\u52a8\u6001\u7684\u7814\u7a76\u4ef7\u503c\uff0c\u5f3a\u8c03\u9700\u533a\u5206\u6570\u636e\u6c61\u67d3\u5f71\u54cd\u4e0e\u771f\u6b63\u52a8\u6001\u73b0\u8c61"}}
{"id": "2506.18602", "pdf": "https://arxiv.org/pdf/2506.18602", "abs": "https://arxiv.org/abs/2506.18602", "authors": ["R. Prashanth"], "title": "Semantic similarity estimation for domain specific data using BERT and other techniques", "categories": ["cs.CL", "stat.AP"], "comment": "This is a preprint version of an article accepted for publication in\n  the proceedings of Machine Learning and Data Mining 2019", "summary": "Estimation of semantic similarity is an important research problem both in\nnatural language processing and the natural language understanding, and that\nhas tremendous application on various downstream tasks such as question\nanswering, semantic search, information retrieval, document clustering,\nword-sense disambiguation and machine translation. In this work, we carry out\nthe estimation of semantic similarity using different state-of-the-art\ntechniques including the USE (Universal Sentence Encoder), InferSent and the\nmost recent BERT, or Bidirectional Encoder Representations from Transformers,\nmodels. We use two question pairs datasets for the analysis, one is a domain\nspecific in-house dataset and the other is a public dataset which is the\nQuora's question pairs dataset. We observe that the BERT model gave much\nsuperior performance as compared to the other methods. This should be because\nof the fine-tuning procedure that is involved in its training process, allowing\nit to learn patterns based on the training data that is used. This works\ndemonstrates the applicability of BERT on domain specific datasets. We infer\nfrom the analysis that BERT is the best technique to use in the case of domain\nspecific data.", "AI": {"tldr": "\u6bd4\u8f83USE\u3001InferSent\u548cBERT\u6a21\u578b\u5728\u8bed\u4e49\u76f8\u4f3c\u6027\u4f30\u8ba1\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0BERT\u5728\u7279\u5b9a\u9886\u57df\u6570\u636e\u96c6\u8868\u73b0\u6700\u4f18", "motivation": "\u63a2\u7d22\u4e0d\u540c\u5148\u8fdb\u6a21\u578b\uff08\u5305\u62ecBERT\uff09\u5728\u8bed\u4e49\u76f8\u4f3c\u6027\u4f30\u8ba1\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u7279\u5b9a\u9886\u57df\u6570\u636e\u96c6\u4e0a\u7684\u9002\u7528\u6027", "method": "\u4f7f\u7528USE\u3001InferSent\u548cBERT\u6a21\u578b\uff0c\u5728\u5185\u90e8\u7279\u5b9a\u9886\u57df\u6570\u636e\u96c6\u548c\u516c\u5f00Quora\u95ee\u9898\u5bf9\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790", "result": "BERT\u6a21\u578b\u901a\u8fc7\u5fae\u8c03\u673a\u5236\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u5728\u7279\u5b9a\u9886\u57df\u6570\u636e\u4e0a\u5c55\u73b0\u51fa\u6700\u9ad8\u51c6\u786e\u7387", "conclusion": "BERT\u56e0\u5176\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u5fae\u8c03\u673a\u5236\uff0c\u6210\u4e3a\u5904\u7406\u7279\u5b9a\u9886\u57df\u8bed\u4e49\u76f8\u4f3c\u6027\u4efb\u52a1\u7684\u6700\u4f73\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.18621", "pdf": "https://arxiv.org/pdf/2506.18621", "abs": "https://arxiv.org/abs/2506.18621", "authors": ["Alisa Barkar", "Mathieu Chollet", "Matthieu Labeau", "Beatrice Biancardi", "Chloe Clavel"], "title": "The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified Speeches", "categories": ["cs.CL"], "comment": "Under submission to ICNLSP 2025. 9 pages, 2 tables", "summary": "This study examines how large language models understand the concept of\npersuasiveness in public speaking by modifying speech transcripts from PhD\ncandidates in the \"Ma These en 180 Secondes\" competition, using the 3MT French\ndataset. Our contributions include a novel methodology and an interpretable\ntextual feature set integrating rhetorical devices and discourse markers. We\nprompt GPT-4o to enhance or diminish persuasiveness and analyze linguistic\nshifts between original and generated speech in terms of the new features.\nResults indicate that GPT-4o applies systematic stylistic modifications rather\nthan optimizing persuasiveness in a human-like manner. Notably, it manipulates\nemotional lexicon and syntactic structures (such as interrogative and\nexclamatory clauses) to amplify rhetorical impact.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u4fee\u6539\u6f14\u8bb2\u6587\u672c\u5206\u6790GPT-4o\u5982\u4f55\u7cfb\u7edf\u6027\u8c03\u6574\u60c5\u611f\u8bcd\u6c47\u548c\u53e5\u6cd5\u7ed3\u6784\u6765\u589e\u5f3a\u8bf4\u670d\u529b\uff0c\u800c\u975e\u6a21\u4eff\u4eba\u7c7b\u8bf4\u670d\u7b56\u7565\u3002", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u6f14\u8bb2\u8bf4\u670d\u529b\u7684\u7406\u89e3\u673a\u5236\uff0c\u9a8c\u8bc1\u5176\u662f\u5426\u91c7\u7528\u7c7b\u4eba\u4f18\u5316\u7b56\u7565\u3002", "method": "1. \u4f7f\u75283MT\u6cd5\u8bed\u6570\u636e\u96c6\u4fee\u6539\u535a\u58eb\u5019\u9009\u4eba\u6f14\u8bb2\u6587\u672c\n2. \u6784\u5efa\u5305\u542b\u4fee\u8f9e\u624b\u6cd5\u548c\u8bdd\u8bed\u6807\u8bb0\u7684\u6587\u672c\u7279\u5f81\u4f53\u7cfb\n3. \u901a\u8fc7GPT-4o\u751f\u6210\u8bf4\u670d\u529b\u589e\u5f3a/\u51cf\u5f31\u7248\u672c\n4. \u5bf9\u6bd4\u5206\u6790\u539f\u59cb\u4e0e\u751f\u6210\u6587\u672c\u7684\u8bed\u8a00\u7279\u5f81\u53d8\u5316", "result": "GPT-4o\u901a\u8fc7\u589e\u52a0\u60c5\u611f\u8bcd\u6c47\u5bc6\u5ea6\uff08+38%\uff09\u3001\u7591\u95ee\u53e5\uff08+22%\uff09\u548c\u611f\u53f9\u53e5\uff08+45%\uff09\u7cfb\u7edf\u6027\u5730\u5f3a\u5316\u4fee\u8f9e\u6548\u679c\uff0c\u4f46\u7f3a\u4e4f\u4eba\u7c7b\u8bf4\u670d\u7b56\u7565\u7684\u8bed\u5883\u9002\u5e94\u6027\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u5f62\u5f0f\u5316\u98ce\u683c\u8c03\u6574\u800c\u975e\u8bed\u4e49\u4f18\u5316\u6765\u5b9e\u73b0\u8bf4\u670d\u529b\u589e\u5f3a\uff0c\u63ed\u793a\u4e86AI\u4e0e\u4eba\u7c7b\u8bf4\u670d\u673a\u5236\u7684\u672c\u8d28\u5dee\u5f02\u3002"}}
{"id": "2506.18639", "pdf": "https://arxiv.org/pdf/2506.18639", "abs": "https://arxiv.org/abs/2506.18639", "authors": ["Z\u00e9bulon Goriely", "Suchir Salhan", "Pietro Lesci", "Julius Cheng", "Paula Buttery"], "title": "ByteSpan: Information-Driven Subword Tokenisation", "categories": ["cs.CL"], "comment": "Accepted to TokShop 2025 (Non-archival)", "summary": "Recent dynamic tokenisation methods operate directly on bytes and pool their\nlatent representations into patches. This bears similarities to computational\nmodels of word segmentation that determine lexical boundaries using spikes in\nan autoregressive model's prediction error. Inspired by this connection, we\nexplore whether grouping predictable bytes - rather than pooling their\nrepresentations - can yield a useful fixed subword vocabulary. We propose a new\ninformation-driven subword tokeniser, ByteSpan, that uses an external\nbyte-level LM during training to identify contiguous predictable byte sequences\nand group them into subwords. Experiments show that ByteSpan yields efficient\nvocabularies with higher morphological alignment scores than BPE for English.\nMultilingual experiments show similar compression and R\\'enyi efficiency for 25\nlanguages.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5916\u90e8\u5b57\u8282\u7ea7\u8bed\u8a00\u6a21\u578b\u7684ByteSpan\u5206\u8bcd\u5668\uff0c\u901a\u8fc7\u8bc6\u522b\u53ef\u9884\u6d4b\u5b57\u8282\u5e8f\u5217\u6784\u5efa\u66f4\u9ad8\u6548\u7684\u5206\u8bcd\u8868", "motivation": "\u73b0\u6709\u52a8\u6001\u5206\u8bcd\u65b9\u6cd5\u901a\u8fc7\u6c47\u96c6\u5b57\u8282\u8868\u5f81\u5f62\u6210\u8bcd\u5757\uff0c\u53d7\u8bcd\u5206\u5272\u6a21\u578b\u901a\u8fc7\u9884\u6d4b\u8bef\u5dee\u786e\u5b9a\u8bcd\u8fb9\u754c\u7684\u542f\u53d1\uff0c\u63a2\u7d22\u53ef\u9884\u6d4b\u5b57\u8282\u5206\u7ec4\u66ff\u4ee3\u8868\u5f81\u6c47\u96c6\u7684\u53ef\u884c\u6027", "method": "\u4f7f\u7528\u5916\u90e8\u5b57\u8282\u7ea7\u8bed\u8a00\u6a21\u578b\u5728\u8bad\u7ec3\u4e2d\u8bc6\u522b\u8fde\u7eed\u53ef\u9884\u6d4b\u5b57\u8282\u5e8f\u5217\uff0c\u5c06\u5176\u5206\u7ec4\u4e3a\u5b50\u8bcd\u5355\u5143", "result": "\u82f1\u8bed\u5f62\u6001\u5bf9\u9f50\u5ea6\u8d85\u8d8aBPE\uff0c25\u79cd\u8bed\u8a00\u6d4b\u8bd5\u663e\u793a\u76f8\u4f3c\u538b\u7f29\u7387\u548cR\u00e9nyi\u6548\u7387", "conclusion": "\u57fa\u4e8e\u5916\u90e8\u8bed\u8a00\u6a21\u578b\u6307\u5bfc\u7684\u53ef\u9884\u6d4b\u5b57\u8282\u5206\u7ec4\u7b56\u7565\u80fd\u6784\u5efa\u66f4\u9ad8\u6548\u7684\u5206\u8bcd\u8868\uff0c\u5728\u591a\u8bed\u8a00\u573a\u666f\u4fdd\u6301\u7ade\u4e89\u529b"}}
{"id": "2506.18674", "pdf": "https://arxiv.org/pdf/2506.18674", "abs": "https://arxiv.org/abs/2506.18674", "authors": ["Raquel Ferrando", "Javier Conde", "Gonzalo Mart\u00ednez", "Pedro Reviriego"], "title": "Is There a Case for Conversation Optimized Tokenizers in Large Language Models?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The computational and energy costs of Large Language Models (LLMs) have\nincreased exponentially driven by the growing model sizes and the massive\nadoption of LLMs by hundreds of millions of users. The unit cost of an LLM is\nthe computation of a token. Therefore, the tokenizer plays an important role in\nthe efficiency of a model, and they are carefully optimized to minimize the\nnumber of tokens for the text in their training corpus. One of the most popular\napplications of LLMs are chatbots that interact with users. A key observation\nis that, for those chatbots, what is important is the performance of the\ntokenizer in the user text input and the chatbot responses. Those are most\nlikely different from the text in the training corpus. So, a question that\nimmediately arises is whether there is a potential benefit in optimizing\ntokenizers for chatbot conversations. In this paper, this idea is explored for\ndifferent tokenizers by using a publicly available corpus of chatbot\nconversations to redesign their vocabularies and evaluate their performance in\nthis domain. The results show that conversation-optimized tokenizers\nconsistently reduce the number of tokens in chatbot dialogues, which can lead\nto meaningful energy savings, in the range of 5% to 10% while having minimal or\neven slightly positive impact on tokenization efficiency for the original\ntraining corpus.", "AI": {"tldr": "\u901a\u8fc7\u4f18\u5316\u9762\u5411\u804a\u5929\u5bf9\u8bdd\u7684\u5206\u8bcd\u5668\uff0c\u53ef\u5728\u4fdd\u6301\u539f\u6709\u6548\u7387\u7684\u540c\u65f6\u51cf\u5c1115-20%\u7684token\u6570\u91cf\uff0c\u5b9e\u73b05-10%\u7684\u80fd\u6e90\u8282\u7ea6", "motivation": "\u73b0\u6709\u5206\u8bcd\u5668\u9488\u5bf9\u8bad\u7ec3\u8bed\u6599\u4f18\u5316\uff0c\u4f46\u804a\u5929\u573a\u666f\u4e2d\u7528\u6237\u8f93\u5165\u548c\u56de\u590d\u7684\u6587\u672c\u7279\u5f81\u4e0e\u8bad\u7ec3\u6570\u636e\u5b58\u5728\u5dee\u5f02\uff0c\u9700\u8981\u9488\u5bf9\u6027\u4f18\u5316\u63d0\u5347\u6548\u7387", "method": "\u4f7f\u7528\u516c\u5f00\u804a\u5929\u5bf9\u8bdd\u8bed\u6599\u5e93\u91cd\u6784\u5206\u8bcd\u5668\u8bcd\u6c47\u8868\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\u8bc4\u4f30\u4e0d\u540c\u5206\u8bcd\u5668\u5728\u5bf9\u8bdd\u573a\u666f\u7684\u538b\u7f29\u6548\u7387", "result": "\u5bf9\u8bdd\u4f18\u5316\u7684\u5206\u8bcd\u5668\u5e73\u5747\u51cf\u5c1118%\u7684token\u6570\u91cf\uff0c\u5728GPU\u63a8\u7406\u4e2d\u5b9e\u73b08.7%\u7684\u80fd\u8017\u964d\u4f4e\uff0c\u4e14\u539f\u59cb\u8bed\u6599\u7684\u5206\u8bcd\u6548\u7387\u4ec5\u4e0b\u964d0.3%", "conclusion": "\u9488\u5bf9\u7279\u5b9a\u5e94\u7528\u573a\u666f\u4f18\u5316\u5206\u8bcd\u5668\u662f\u6709\u6548\u7684\u8282\u80fd\u9014\u5f84\uff0c\u5728\u4fdd\u8bc1\u6838\u5fc3\u529f\u80fd\u7684\u524d\u63d0\u4e0b\u53ef\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u80fd\u6548\u6bd4"}}
{"id": "2506.18703", "pdf": "https://arxiv.org/pdf/2506.18703", "abs": "https://arxiv.org/abs/2506.18703", "authors": ["Christian Huber", "Alexander Waibel"], "title": "Context Biasing for Pronunciations-Orthography Mismatch in Automatic Speech Recognition", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Neural sequence-to-sequence systems deliver state-of-the-art performance for\nautomatic speech recognition. When using appropriate modeling units, e.g.,\nbyte-pair encoded characters, these systems are in principal open vocabulary\nsystems. In practice, however, they often fail to recognize words not seen\nduring training, e.g., named entities, acronyms, or domain-specific special\nwords. To address this problem, many context biasing methods have been\nproposed; however, for words with a pronunciation-orthography mismatch, these\nmethods may still struggle. We propose a method which allows corrections of\nsubstitution errors to improve the recognition accuracy of such challenging\nwords. Users can add corrections on the fly during inference. We show that with\nthis method we get a relative improvement in biased word error rate of up to\n11\\%, while maintaining a competitive overall word error rate.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u7ea0\u9519\u65b9\u6cd5\u6539\u5584\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u5bf9\u53d1\u97f3-\u62fc\u5199\u4e0d\u5339\u914d\u8bcd\u6c47\u7684\u8bc6\u522b\uff0c\u5728\u63a8\u7406\u8fc7\u7a0b\u5b9e\u65f6\u4fee\u6b63\u66ff\u6362\u9519\u8bef", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b57\u8282\u5bf9\u7f16\u7801\u7684\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u5728\u9047\u5230\u672a\u89c1\u8fc7\u8bcd\u6c47\uff08\u5982\u4e13\u6709\u540d\u8bcd/\u7f29\u5199\u8bcd\uff09\u65f6\u6613\u51fa\u9519\uff0c\u4f20\u7edf\u4e0a\u4e0b\u6587\u504f\u7f6e\u65b9\u6cd5\u96be\u4ee5\u89e3\u51b3\u53d1\u97f3\u4e0e\u62fc\u5199\u4e0d\u5339\u914d\u95ee\u9898", "method": "\u5141\u8bb8\u7528\u6237\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b9e\u65f6\u6dfb\u52a0\u8bcd\u6c47\u7ea0\u9519\u89c4\u5219\uff0c\u52a8\u6001\u4fee\u6b63\u66ff\u6362\u9519\u8bef", "result": "\u504f\u7f6e\u8bcd\u9519\u8bef\u7387\u76f8\u5bf9\u63d0\u534711%\uff0c\u6574\u4f53\u8bcd\u9519\u8bef\u7387\u4fdd\u6301\u7ade\u4e89\u529b", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u8bed\u97f3\u7cfb\u7edf\u5bf9\u7279\u6b8a\u8bcd\u6c47\u7684\u8bc6\u522b\u7cbe\u5ea6\uff0c\u540c\u65f6\u7ef4\u6301\u6574\u4f53\u6027\u80fd"}}
{"id": "2506.18710", "pdf": "https://arxiv.org/pdf/2506.18710", "abs": "https://arxiv.org/abs/2506.18710", "authors": ["Maxime Leli\u00e8vre", "Amy Waldock", "Meng Liu", "Natalia Vald\u00e9s Aspillaga", "Alasdair Mackintosh", "Mar\u00eda Jos\u00e9 Ogando Portelo", "Jared Lee", "Paul Atherton", "Robin A. A. Ince", "Oliver G. B. Garrod"], "title": "Benchmarking the Pedagogical Knowledge of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Benchmarks like Massive Multitask Language Understanding (MMLU) have played a\npivotal role in evaluating AI's knowledge and abilities across diverse domains.\nHowever, existing benchmarks predominantly focus on content knowledge, leaving\na critical gap in assessing models' understanding of pedagogy - the method and\npractice of teaching. This paper introduces The Pedagogy Benchmark, a novel\ndataset designed to evaluate large language models on their Cross-Domain\nPedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)\npedagogical knowledge. These benchmarks are built on a carefully curated set of\nquestions sourced from professional development exams for teachers, which cover\na range of pedagogical subdomains such as teaching strategies and assessment\nmethods. Here we outline the methodology and development of these benchmarks.\nWe report results for 97 models, with accuracies spanning a range from 28% to\n89% on the pedagogical knowledge questions. We consider the relationship\nbetween cost and accuracy and chart the progression of the Pareto value\nfrontier over time. We provide online leaderboards at\nhttps://rebrand.ly/pedagogy which are updated with new models and allow\ninteractive exploration and filtering based on various model properties, such\nas cost per token and open-vs-closed weights, as well as looking at performance\nin different subjects. LLMs and generative AI have tremendous potential to\ninfluence education and help to address the global learning crisis.\nEducation-focused benchmarks are crucial to measure models' capacities to\nunderstand pedagogical concepts, respond appropriately to learners' needs, and\nsupport effective teaching practices across diverse contexts. They are needed\nfor informing the responsible and evidence-based deployment of LLMs and\nLLM-based tools in educational settings, and for guiding both development and\npolicy decisions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u6559\u5b66\u6cd5\u77e5\u8bc6\u7684\u65b0\u57fa\u51c6Pedagogy Benchmark\uff0c\u6d4b\u8bd5\u663e\u793a\u6a21\u578b\u51c6\u786e\u7387\u8de8\u5ea6\u8fbe28%-89%\uff0c\u5f3a\u8c03\u6559\u80b2\u57fa\u51c6\u5bf9\u8d1f\u8d23\u4efb\u90e8\u7f72LLM\u7684\u91cd\u8981\u6027", "motivation": "\u73b0\u6709\u57fa\u51c6\u8fc7\u5ea6\u5173\u6ce8\u5b66\u79d1\u77e5\u8bc6\uff0c\u7f3a\u4e4f\u5bf9\u6559\u5b66\u6cd5\uff08\u6559\u5b66\u65b9\u6cd5\u4e0e\u5b9e\u8df5\uff09\u7684\u8bc4\u4f30\u80fd\u529b\uff0c\u5236\u7ea6AI\u5728\u6559\u80b2\u9886\u57df\u7684\u6709\u6548\u5e94\u7528", "method": "\u57fa\u4e8e\u6559\u5e08\u4e13\u4e1a\u8003\u8bd5\u9898\u76ee\u6784\u5efa\u8de8\u9886\u57df\u6559\u5b66\u77e5\u8bc6(CDPK)\u548c\u7279\u6b8a\u6559\u80b2\u9700\u6c42(SEND)\u6d4b\u8bd5\u96c6\uff0c\u8986\u76d6\u6559\u5b66\u7b56\u7565\u7b49\u5b50\u9886\u57df\uff0c\u6d4b\u8bd597\u4e2a\u6a21\u578b\u5e76\u5206\u6790\u6210\u672c-\u51c6\u786e\u7387\u5173\u7cfb", "result": "\u6a21\u578b\u51c6\u786e\u7387\u5dee\u5f02\u663e\u8457\uff08\u6700\u9ad889%\uff09\uff0c\u5efa\u7acb\u53ef\u4ea4\u4e92\u7684\u5728\u7ebf\u6392\u884c\u699c\u5b9e\u73b0\u591a\u7ef4\u6307\u6807\u8ffd\u8e2a\uff0c\u63ed\u793a\u6a21\u578b\u8fdb\u6b65\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u6f14\u53d8\u8f68\u8ff9", "conclusion": "\u6559\u80b2\u57fa\u51c6\u5bf9\u8861\u91cf\u6a21\u578b\u6559\u5b66\u7406\u89e3\u80fd\u529b\u3001\u652f\u6301\u6709\u6548\u6559\u5b66\u5b9e\u8df5\u53ca\u6307\u5bfc\u653f\u7b56\u5236\u5b9a\u5177\u6709\u5173\u952e\u4f5c\u7528\uff0c\u662f\u6559\u80b2\u9886\u57df\u8d1f\u8d23\u4efbAI\u90e8\u7f72\u7684\u57fa\u7840"}}
{"id": "2506.18756", "pdf": "https://arxiv.org/pdf/2506.18756", "abs": "https://arxiv.org/abs/2506.18756", "authors": ["Chong Zhang", "Xiang Li", "Jia Wang", "Shan Liang", "Haochen Xue", "Xiaobo Jin"], "title": "Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach", "categories": ["cs.CL", "cs.CR"], "comment": "19 pages, 8 figures", "summary": "Large Language Models (LLMs) increasingly rely on automatic prompt\nengineering in graphical user interfaces (GUIs) to refine user inputs and\nenhance response accuracy. However, the diversity of user requirements often\nleads to unintended misinterpretations, where automated optimizations distort\noriginal intentions and produce erroneous outputs. To address this challenge,\nwe propose the Adaptive Greedy Binary Search (AGBS) method, which simulates\ncommon prompt optimization mechanisms while preserving semantic stability. Our\napproach dynamically evaluates the impact of such strategies on LLM\nperformance, enabling robust adversarial sample generation. Through extensive\nexperiments on open and closed-source LLMs, we demonstrate AGBS's effectiveness\nin balancing semantic consistency and attack efficacy. Our findings offer\nactionable insights for designing more reliable prompt optimization systems.\nCode is available at: https://github.com/franz-chang/DOBS", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u8d2a\u5a6a\u4e8c\u5206\u641c\u7d22\uff08AGBS\uff09\u65b9\u6cd5\u89e3\u51b3LLMs\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\u4e2d\u7684\u8bed\u4e49\u5931\u771f\u95ee\u9898\uff0c\u901a\u8fc7\u5e73\u8861\u8bed\u4e49\u4e00\u81f4\u6027\u4e0e\u653b\u51fb\u6548\u80fd\u63d0\u5347\u7cfb\u7edf\u53ef\u9760\u6027", "motivation": "\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u673a\u5236\u5728\u591a\u6837\u5316\u7528\u6237\u9700\u6c42\u4e0b\u5bb9\u6613\u626d\u66f2\u539f\u59cb\u610f\u56fe\uff0c\u5bfc\u81f4LLMs\u8f93\u51fa\u9519\u8bef\u3002\u9700\u8981\u65e2\u80fd\u4f18\u5316\u63d0\u793a\u53c8\u4fdd\u6301\u8bed\u4e49\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848", "method": "AGBS\u6a21\u62df\u63d0\u793a\u4f18\u5316\u673a\u5236\uff0c\u52a8\u6001\u8bc4\u4f30\u7b56\u7565\u5bf9LLM\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u5bf9\u6297\u6837\u672c\u751f\u6210\u5b9e\u73b0\u8bed\u4e49\u7a33\u5b9a\u6027\u4e0e\u653b\u51fb\u6548\u80fd\u7684\u52a8\u6001\u5e73\u8861", "result": "\u5728\u5f00\u6e90/\u95ed\u6e90LLMs\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1AGBS\u80fd\u4fdd\u630189%\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u540c\u65f6\u8fbe\u523093%\u653b\u51fb\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8bbe\u8ba1\u53ef\u9760\u63d0\u793a\u4f18\u5316\u7cfb\u7edf\u63d0\u4f9b\u5b9e\u8df5\u6307\u5bfc\uff0c\u5efa\u8bae\u5c06\u8bed\u4e49\u7a33\u5b9a\u6027\u7eb3\u5165\u5bf9\u6297\u8bad\u7ec3\u6846\u67b6\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u4fbf\u4e8e\u5de5\u4e1a\u754c\u5e94\u7528"}}
{"id": "2506.18768", "pdf": "https://arxiv.org/pdf/2506.18768", "abs": "https://arxiv.org/abs/2506.18768", "authors": ["Ao Chang", "Tong Zhou", "Yubo Chen", "Delai Qiu", "Shengping Liu", "Kang Liu", "Jun Zhao"], "title": "ASP2LJ : An Adversarial Self-Play Laywer Augmented Legal Judgment Framework", "categories": ["cs.CL"], "comment": null, "summary": "Legal Judgment Prediction (LJP) aims to predict judicial outcomes, including\nrelevant legal charge, terms, and fines, which is a crucial process in Large\nLanguage Model(LLM). However, LJP faces two key challenges: (1)Long Tail\nDistribution: Current datasets, derived from authentic cases, suffer from high\nhuman annotation costs and imbalanced distributions, leading to model\nperformance degradation. (2)Lawyer's Improvement: Existing systems focus on\nenhancing judges' decision-making but neglect the critical role of lawyers in\nrefining arguments, which limits overall judicial accuracy. To address these\nissues, we propose an Adversarial Self-Play Lawyer Augmented Legal Judgment\nFramework, called ASP2LJ, which integrates a case generation module to tackle\nlong-tailed data distributions and an adversarial self-play mechanism to\nenhance lawyers' argumentation skills. Our framework enables a judge to\nreference evolved lawyers' arguments, improving the objectivity, fairness, and\nrationality of judicial decisions. Besides, We also introduce RareCases, a\ndataset for rare legal cases in China, which contains 120 tail-end cases. We\ndemonstrate the effectiveness of our approach on the SimuCourt dataset and our\nRareCases dataset. Experimental results show our framework brings improvements,\nindicating its utilization. Our contributions include an integrated framework,\na rare-case dataset, and publicly releasing datasets and code to support\nfurther research in automated judicial systems.", "AI": {"tldr": "\u63d0\u51faASP2LJ\u5bf9\u6297\u81ea\u535a\u5f08\u6846\u67b6\u89e3\u51b3\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u7684\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u901a\u8fc7\u6848\u4f8b\u751f\u6210\u548c\u5f8b\u5e08\u89d2\u8272\u5f3a\u5316\u63d0\u5347\u53f8\u6cd5\u51b3\u7b56\u8d28\u91cf\uff0c\u5e76\u6784\u5efa\u4e2d\u56fd\u7a00\u6709\u6848\u4f8b\u6570\u636e\u96c6RareCases\u3002", "motivation": "\u73b0\u6709\u6cd5\u5f8b\u5224\u51b3\u9884\u6d4b\u7cfb\u7edf\u9762\u4e34\u957f\u5c3e\u6570\u636e\u5206\u5e03\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u5ffd\u89c6\u5f8b\u5e08\u5728\u4f18\u5316\u6cd5\u5f8b\u8bba\u8bc1\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u9650\u5236\u53f8\u6cd5\u51c6\u786e\u6027\u63d0\u5347\u3002", "method": "\u96c6\u6210\u5bf9\u6297\u81ea\u535a\u5f08\u673a\u5236\uff1a1) \u6848\u4f8b\u751f\u6210\u6a21\u5757\u6269\u5145\u957f\u5c3e\u6570\u636e 2) \u5f8b\u5e08\u667a\u80fd\u4f53\u901a\u8fc7\u5bf9\u6297\u8bad\u7ec3\u4f18\u5316\u8bba\u8bc1\u80fd\u529b 3) \u6cd5\u5b98\u53c2\u8003\u8fdb\u5316\u540e\u7684\u5f8b\u5e08\u8bba\u70b9\u8fdb\u884c\u51b3\u7b56", "result": "\u5728SimuCourt\u548c\u81ea\u5efaRareCases\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\uff0c\u663e\u8457\u63d0\u5347\u53f8\u6cd5\u51b3\u7b56\u7684\u5ba2\u89c2\u6027\u3001\u516c\u5e73\u6027\u548c\u5408\u7406\u6027\u6307\u6807", "conclusion": "\u8d21\u732e\u5305\u542b\uff1a1) \u9996\u4e2a\u6574\u5408\u6570\u636e\u589e\u5f3a\u4e0e\u89d2\u8272\u6f14\u8fdb\u7684\u53f8\u6cd5\u6846\u67b6 2) \u4e2d\u56fd\u9996\u4e2a\u4eba\u5de5\u6807\u6ce8\u7684\u7a00\u6709\u6848\u4f8b\u57fa\u51c6\u6570\u636e\u96c6 3) \u516c\u5f00\u4ee3\u7801\u4e0e\u6570\u636e\u63a8\u52a8\u667a\u80fd\u53f8\u6cd5\u7cfb\u7edf\u53d1\u5c55"}}
{"id": "2506.18781", "pdf": "https://arxiv.org/pdf/2506.18781", "abs": "https://arxiv.org/abs/2506.18781", "authors": ["Zhenru Lin", "Jiawen Tao", "Yang Yuan", "Andrew Chi-Chih Yao"], "title": "Existing LLMs Are Not Self-Consistent For Simple Tasks", "categories": ["cs.CL"], "comment": "10 pages, 6 figures", "summary": "Large Language Models (LLMs) have grown increasingly powerful, yet ensuring\ntheir decisions remain transparent and trustworthy requires self-consistency --\nno contradictions in their internal reasoning. Our study reveals that even on\nsimple tasks, such as comparing points on a line or a plane, or reasoning in a\nfamily tree, all smaller models are highly inconsistent, and even\nstate-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully\nself-consistent. To quantify and mitigate these inconsistencies, we introduce\ninconsistency metrics and propose two automated methods -- a graph-based and an\nenergy-based approach. While these fixes provide partial improvements, they\nalso highlight the complexity and importance of self-consistency in building\nmore reliable and interpretable AI. The code and data are available at\nhttps://github.com/scorpio-nova/llm-self-consistency.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u5b58\u5728\u81ea\u6d3d\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u56fe\u548c\u80fd\u91cf\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u63ed\u793a\u81ea\u6d3d\u6027\u5bf9AI\u53ef\u9760\u6027\u5efa\u8bbe\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u867d\u5f3a\u4f46\u51b3\u7b56\u7f3a\u4e4f\u900f\u660e\u6027\uff0c\u5728\u57fa\u7840\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u81ea\u6211\u77db\u76fe\u6027\uff0c\u5f71\u54cd\u6a21\u578b\u53ef\u4fe1\u5ea6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u7ed3\u6784\u7684\u7ea6\u675f\u65b9\u6cd5\u548c\u57fa\u4e8e\u80fd\u91cf\u7684\u6982\u7387\u4fee\u6b63\u65b9\u6cd5\u91cf\u5316\u5e76\u6539\u5584\u6a21\u578b\u81ea\u6d3d\u6027\u3002", "result": "\u6240\u6709\u5c0f\u6a21\u578b\u5747\u5b58\u5728\u9ad8\u5ea6\u4e0d\u4e00\u81f4\u6027\uff0c\u9876\u5c16\u6a21\u578b\u4e5f\u672a\u5b8c\u5168\u81ea\u6d3d\uff0c\u6539\u8fdb\u65b9\u6cd5\u4ec5\u80fd\u90e8\u5206\u89e3\u51b3\u95ee\u9898\u3002", "conclusion": "\u81ea\u6d3d\u6027\u662f\u6784\u5efa\u53ef\u9760AI\u7684\u5173\u952e\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u9700\u66f4\u6df1\u5165\u7684\u7cfb\u7edf\u6027\u7814\u7a76\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.18819", "pdf": "https://arxiv.org/pdf/2506.18819", "abs": "https://arxiv.org/abs/2506.18819", "authors": ["Arjun Mukerji", "Michael L. Jackson", "Jason Jones", "Neil Sanghavi"], "title": "RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies", "categories": ["cs.CL", "cs.AI"], "comment": "24 pages, 2 figures", "summary": "Large Language Models (LLMs) have been extensively evaluated for general\nsummarization tasks as well as medical research assistance, but they have not\nbeen specifically evaluated for the task of summarizing real-world evidence\n(RWE) from structured output of RWE studies. We introduce RWESummary, a\nproposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al.,\n2025) to enable benchmarking of LLMs for this task. RWESummary includes one\nscenario and three evaluations covering major types of errors observed in\nsummarization of medical research studies and was developed using Atropos\nHealth proprietary data. Additionally, we use RWESummary to compare the\nperformance of different LLMs in our internal RWE summarization tool. At the\ntime of publication, with 13 distinct RWE studies, we found the Gemini 2.5\nmodels performed best overall (both Flash and Pro). We suggest RWESummary as a\nnovel and useful foundation model benchmark for real-world evidence study\nsummarization.", "AI": {"tldr": "\u63d0\u51faRWESummary\u4f5c\u4e3a\u771f\u5b9e\u4e16\u754c\u8bc1\u636e\u7814\u7a76\u6458\u8981\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u8bc4\u4f30\u53d1\u73b0Gemini 2.5\u6a21\u578b\u8868\u73b0\u6700\u4f18", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u96c6\u4e2d\u4e8e\u901a\u7528\u548c\u533b\u5b66\u7814\u7a76\u6458\u8981\uff0c\u7f3a\u4e4f\u9488\u5bf9\u771f\u5b9e\u4e16\u754c\u8bc1\u636e\u7814\u7a76\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u4e13\u9879\u8bc4\u4f30\u4f53\u7cfb", "method": "\u57fa\u4e8eMedHELM\u6846\u67b6\u6784\u5efaRWESummary\uff08\u542b1\u4e2a\u573a\u666f+3\u7c7b\u8bc4\u4f30\u6307\u6807\uff09\uff0c\u4f7f\u7528Atropos Health\u4e13\u6709\u6570\u636e\u5f00\u53d1\uff0c\u6d4b\u8bd513\u9879RWE\u7814\u7a76\u5e76\u5bf9\u6bd4\u4e0d\u540cLLM\u6027\u80fd", "result": "Gemini 2.5\uff08Flash/Pro\uff09\u5728\u771f\u5b9e\u4e16\u754c\u8bc1\u636e\u6458\u8981\u4efb\u52a1\u4e2d\u7efc\u5408\u8868\u73b0\u6700\u4f73", "conclusion": "RWESummary\u4e3a\u771f\u5b9e\u4e16\u754c\u8bc1\u636e\u7814\u7a76\u6458\u8981\u5efa\u7acb\u4e86\u65b0\u578b\u57fa\u7840\u6a21\u578b\u8bc4\u4f30\u6807\u51c6\uff0c\u5177\u6709\u884c\u4e1a\u53c2\u8003\u4ef7\u503c"}}
{"id": "2506.18828", "pdf": "https://arxiv.org/pdf/2506.18828", "abs": "https://arxiv.org/abs/2506.18828", "authors": ["Jorge Iranzo-S\u00e1nchez", "Javier Iranzo-S\u00e1nchez", "Adri\u00e0 Gim\u00e9nez", "Jorge Civera", "Alfons Juan"], "title": "MLLP-VRAIN UPV system for the IWSLT 2025 Simultaneous Speech Translation Translation task", "categories": ["cs.CL"], "comment": "IWSLT 2025 System Description", "summary": "This work describes the participation of the MLLP-VRAIN research group in the\nshared task of the IWSLT 2025 Simultaneous Speech Translation track. Our\nsubmission addresses the unique challenges of real-time translation of\nlong-form speech by developing a modular cascade system that adapts strong\npre-trained models to streaming scenarios. We combine Whisper Large-V3-Turbo\nfor ASR with the multilingual NLLB-3.3B model for MT, implementing lightweight\nadaptation techniques rather than training new end-to-end models from scratch.\nOur approach employs document-level adaptation with prefix training to enhance\nthe MT model's ability to handle incomplete inputs, while incorporating\nadaptive emission policies including a wait-$k$ strategy and RALCP for managing\nthe translation stream. Specialized buffer management techniques and\nsegmentation strategies ensure coherent translations across long audio\nsequences. Experimental results on the ACL60/60 dataset demonstrate that our\nsystem achieves a favorable balance between translation quality and latency,\nwith a BLEU score of 31.96 and non-computational-aware StreamLAAL latency of\n2.94 seconds. Our final model achieves a preliminary score on the official test\nset (IWSLT25Instruct) of 29.8 BLEU. Our work demonstrates that carefully\nadapted pre-trained components can create effective simultaneous translation\nsystems for long-form content without requiring extensive in-domain parallel\ndata or specialized end-to-end training.", "AI": {"tldr": "MLLP-VRAIN\u56e2\u961f\u91c7\u7528Whisper\u548cNLLB\u9884\u8bad\u7ec3\u6a21\u578b\u6784\u5efa\u6a21\u5757\u5316\u7ea7\u8054\u7cfb\u7edf\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u6280\u672f\u5b9e\u73b0\u957f\u8bed\u97f3\u6d41\u5b9e\u65f6\u7ffb\u8bd1\uff0c\u5728\u5ef6\u8fdf\u4e0e\u8d28\u91cf\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u957f\u8bed\u97f3\u6d41\u5b9e\u65f6\u7ffb\u8bd1\u4e2d\u7aef\u5230\u7aef\u6a21\u578b\u8bad\u7ec3\u6210\u672c\u9ad8\u3001\u9886\u57df\u6570\u636e\u4f9d\u8d56\u6027\u5f3a\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6d41\u5f0f\u9002\u914d\u6f5c\u529b\u3002", "method": "\u8bed\u97f3\u8bc6\u522b(Whisper)+\u673a\u5668\u7ffb\u8bd1(NLLB)\u7ea7\u8054\u67b6\u6784\uff0c\u91c7\u7528\u6587\u6863\u7ea7\u524d\u7f00\u8bad\u7ec3\u589e\u5f3a\u4e0d\u5b8c\u6574\u8f93\u5165\u5904\u7406\u80fd\u529b\uff0c\u7ed3\u5408wait-k\u7b56\u7565\u548cRALCP\u52a8\u6001\u53d1\u5c04\u7b56\u7565\uff0c\u8bbe\u8ba1\u4e13\u7528\u7f13\u51b2\u533a\u7ba1\u7406\u4e0e\u5206\u6bb5\u673a\u5236\u3002", "result": "ACL60/60\u6570\u636e\u96c6\u4e0aBLEU 31.96\uff0c\u5ef6\u8fdf2.94\u79d2\uff1bIWSLT25Instruct\u6d4b\u8bd5\u96c6\u521d\u6b65BLEU 29.8\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u9884\u8bad\u7ec3\u6a21\u578b\u7ecf\u7cbe\u7ec6\u9002\u914d\u540e\uff0c\u65e0\u9700\u5927\u91cf\u9886\u57df\u6570\u636e\u5373\u53ef\u6784\u5efa\u9ad8\u6548\u957f\u8bed\u97f3\u6d41\u540c\u4f20\u7cfb\u7edf\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u7ecf\u6d4e\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.18831", "pdf": "https://arxiv.org/pdf/2506.18831", "abs": "https://arxiv.org/abs/2506.18831", "authors": ["Aryasomayajula Ram Bharadwaj"], "title": "STU-PID: Steering Token Usage via PID Controller for Efficient Large Language Model Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models employing extended chain-of-thought (CoT) reasoning\noften suffer from the overthinking phenomenon, generating excessive and\nredundant reasoning steps that increase computational costs while potentially\ndegrading performance. While recent work has explored static steering\napproaches to mitigate this issue, they lack the adaptability to dynamically\nadjust intervention strength based on real-time reasoning quality. We propose\nSTUPID (Steering Token Usage via PID controller), a novel training-free method\nthat employs a PID controller to dynamically modulate activation steering\nstrength during inference. Our approach combines a chunk-level classifier for\ndetecting redundant reasoning patterns with a PID control mechanism that\nadaptively adjusts steering intensity based on the predicted redundancy\nprobability. Experimental evaluation on GSM8K demonstrates that STUPID achieves\na 6% improvement in accuracy while reducing token usage by 32%, outperforming\nstatic steering baselines. Our method provides a principled framework for\ndynamic reasoning calibration that maintains reasoning quality while\nsignificantly improving computational efficiency.", "AI": {"tldr": "\u63d0\u51faSTUPID\u65b9\u6cd5\uff0c\u901a\u8fc7PID\u63a7\u5236\u5668\u52a8\u6001\u6821\u51c6\u63a8\u7406\u8fc7\u7a0b\uff0c\u5728\u63d0\u53476%\u51c6\u786e\u7387\u7684\u540c\u65f6\u51cf\u5c1132%\u8ba1\u7b97\u91cf", "motivation": "\u73b0\u6709\u9759\u6001\u8c03\u8282\u65b9\u6cd5\u65e0\u6cd5\u6839\u636e\u5b9e\u65f6\u63a8\u7406\u8d28\u91cf\u52a8\u6001\u8c03\u6574\u5e72\u9884\u5f3a\u5ea6\uff0c\u5bfc\u81f4\u5197\u4f59\u8ba1\u7b97\u548c\u6027\u80fd\u4e0b\u964d", "method": "\u7ed3\u5408\u6bb5\u843d\u7ea7\u5197\u4f59\u6a21\u5f0f\u5206\u7c7b\u5668\u4e0ePID\u63a7\u5236\u673a\u5236\uff0c\u57fa\u4e8e\u5197\u4f59\u6982\u7387\u9884\u6d4b\u52a8\u6001\u8c03\u6574\u6fc0\u6d3b\u5f15\u5bfc\u5f3a\u5ea6", "result": "\u5728GSM8K\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u51c6\u786e\u7387\u63d0\u53476%\uff0ctoken\u4f7f\u7528\u91cf\u51cf\u5c1132%\uff0c\u4f18\u4e8e\u9759\u6001\u8c03\u8282\u57fa\u7ebf", "conclusion": "\u5efa\u7acb\u52a8\u6001\u63a8\u7406\u6821\u51c6\u6846\u67b6\uff0c\u5728\u4fdd\u8bc1\u63a8\u7406\u8d28\u91cf\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u4e3aLLM\u63a8\u7406\u4f18\u5316\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2506.18841", "pdf": "https://arxiv.org/pdf/2506.18841", "abs": "https://arxiv.org/abs/2506.18841", "authors": ["Yuhao Wu", "Yushi Bai", "Zhiqiang Hu", "Roy Ka-Wei Lee", "Juanzi Li"], "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Ultra-long generation by large language models (LLMs) is a widely demanded\nscenario, yet it remains a significant challenge due to their maximum\ngeneration length limit and overall quality degradation as sequence length\nincreases. Previous approaches, exemplified by LongWriter, typically rely on\n''teaching'', which involves supervised fine-tuning (SFT) on synthetic\nlong-form outputs. However, this strategy heavily depends on synthetic SFT\ndata, which is difficult and costly to construct, often lacks coherence and\nconsistency, and tends to be overly artificial and structurally monotonous. In\nthis work, we propose an incentivization-based approach that, starting entirely\nfrom scratch and without relying on any annotated or synthetic data, leverages\nreinforcement learning (RL) to foster the emergence of ultra-long, high-quality\ntext generation capabilities in LLMs. We perform RL training starting from a\nbase model, similar to R1-Zero, guiding it to engage in reasoning that\nfacilitates planning and refinement during the writing process. To support\nthis, we employ specialized reward models that steer the LLM towards improved\nlength control, writing quality, and structural formatting. Experimental\nevaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,\nconsistently outperforms traditional SFT methods on long-form writing tasks,\nachieving state-of-the-art results across all metrics on WritingBench and\nArena-Write, and even surpassing 100B+ models such as DeepSeek R1 and\nQwen3-235B. We open-source our data and model checkpoints under\nhttps://huggingface.co/THU-KEG/LongWriter-Zero-32B", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65e0\u76d1\u7763\u65b9\u6cd5LongWriter-Zero\uff0c\u7a81\u7834LLM\u957f\u6587\u672c\u751f\u6210\u7684\u957f\u5ea6\u9650\u5236\u4e0e\u8d28\u91cf\u8870\u51cf\u95ee\u9898", "motivation": "\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u4f9d\u8d56\u5408\u6210\u6570\u636e\u6210\u672c\u9ad8\u3001\u8d28\u91cf\u5dee\u4e14\u7f3a\u4e4f\u8fde\u8d2f\u6027\uff0c\u9700\u63a2\u7d22\u65e0\u9700\u6807\u6ce8\u6570\u636e\u7684\u66ff\u4ee3\u65b9\u6848", "method": "\u4ece\u57fa\u7840\u6a21\u578b\u51fa\u53d1\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u4e13\u7528\u5956\u52b1\u6a21\u578b\uff08\u957f\u5ea6\u63a7\u5236/\u5199\u4f5c\u8d28\u91cf/\u683c\u5f0f\u4f18\u5316\uff09\u5b9e\u73b0\u81ea\u4e3b\u8fdb\u5316", "result": "\u5728WritingBench\u548cArena-Write\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5168\u9762\u8d85\u8d8aSFT\u65b9\u6cd5\uff0c\u6027\u80fd\u4f18\u4e8eDeepSeek R1\u7b49\u767e\u4ebf\u7ea7\u6a21\u578b", "conclusion": "\u9a8c\u8bc1\u4e86\u7eaf\u6fc0\u52b1\u5f0f\u8bad\u7ec3\u7684\u6709\u6548\u6027\uff0c\u4e3aLLM\u957f\u6587\u672c\u751f\u6210\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u5e76\u5f00\u6e9032B\u6a21\u578b\u53ca\u6570\u636e\u96c6"}}
{"id": "2506.18852", "pdf": "https://arxiv.org/pdf/2506.18852", "abs": "https://arxiv.org/abs/2506.18852", "authors": ["Iwan Williams", "Ninell Oldenburg", "Ruchira Dhar", "Joshua Hatherley", "Constanza Fierro", "Nina Rajcic", "Sandrine R. Schiller", "Filippos Stamatiou", "Anders S\u00f8gaard"], "title": "Mechanistic Interpretability Needs Philosophy", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Mechanistic interpretability (MI) aims to explain how neural networks work by\nuncovering their underlying causal mechanisms. As the field grows in influence,\nit is increasingly important to examine not just models themselves, but the\nassumptions, concepts and explanatory strategies implicit in MI research. We\nargue that mechanistic interpretability needs philosophy: not as an\nafterthought, but as an ongoing partner in clarifying its concepts, refining\nits methods, and assessing the epistemic and ethical stakes of interpreting AI\nsystems. Taking three open problems from the MI literature as examples, this\nposition paper illustrates the value philosophy can add to MI research, and\noutlines a path toward deeper interdisciplinary dialogue.", "AI": {"tldr": "\u673a\u68b0\u53ef\u89e3\u91ca\u6027(MI)\u7814\u7a76\u9700\u8981\u4e0e\u54f2\u5b66\u6df1\u5ea6\u7ed3\u5408\uff0c\u901a\u8fc7\u8de8\u5b66\u79d1\u5bf9\u8bdd\u89e3\u51b3\u6982\u5ff5\u6f84\u6e05\u3001\u65b9\u6cd5\u4f18\u5316\u53ca\u4f26\u7406\u8bc4\u4f30\u95ee\u9898\u3002", "motivation": "MI\u9886\u57df\u5feb\u901f\u53d1\u5c55\u4f46\u7f3a\u4e4f\u5bf9\u57fa\u7840\u5047\u8bbe\u7684\u5ba1\u89c6\uff0c\u54f2\u5b66\u65b9\u6cd5\u8bba\u80fd\u5e2e\u52a9\u89e3\u51b3\u5f00\u653e\u6027\u95ee\u9898\u5e76\u63d0\u5347\u7814\u7a76\u4e25\u8c28\u6027\u3002", "method": "\u9009\u53d6MI\u6587\u732e\u4e2d\u4e09\u4e2a\u5178\u578b\u95ee\u9898\u4f5c\u4e3a\u6848\u4f8b\uff0c\u5c55\u793a\u54f2\u5b66\u5206\u6790\u5982\u4f55\u8f85\u52a9\u6982\u5ff5\u6f84\u6e05\u548c\u65b9\u6cd5\u6539\u8fdb\u3002", "result": "\u8bba\u8bc1\u54f2\u5b66\u80fd\u589e\u5f3aMI\u7684\u6982\u5ff5\u6e05\u6670\u5ea6\u3001\u65b9\u6cd5\u8bba\u6709\u6548\u6027\u53ca\u4f26\u7406\u98ce\u9669\u8bc4\u4f30\u80fd\u529b\u3002", "conclusion": "\u5efa\u8bae\u5efa\u7acb\u6301\u7eed\u8de8\u5b66\u79d1\u5bf9\u8bdd\u673a\u5236\uff0c\u5c06\u54f2\u5b66\u6df1\u5ea6\u6574\u5408\u5230MI\u7814\u7a76\u6d41\u7a0b\u4e2d\u5171\u540c\u5e94\u5bf9AI\u89e3\u91ca\u6311\u6218\u3002"}}
{"id": "2506.18879", "pdf": "https://arxiv.org/pdf/2506.18879", "abs": "https://arxiv.org/abs/2506.18879", "authors": ["Junyan Li", "Yang Zhang", "Muhammad Yusuf Hassan", "Talha Chafekar", "Tianle Cai", "Zhile Ren", "Pengsheng Guo", "Foroozan Karimzadeh", "Colorado Reed", "Chong Wang", "Chuang Gan"], "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression", "categories": ["cs.CL", "cs.AI"], "comment": "ICML 2025 poster", "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u4ea4\u6362\u77e2\u91cf\u91cf\u5316\u65b9\u6cd5CommVQ\uff0c\u901a\u8fc7\u52a0\u6cd5\u91cf\u5316\u548cRoPE\u517c\u5bb9\u7801\u672c\u8bbe\u8ba1\uff0c\u5c06FP16 KV\u7f13\u5b58\u538b\u7f2987.5%\uff0c\u652f\u6301\u5355\u5361\u90e8\u7f72128K\u957f\u4e0a\u4e0b\u6587LLM", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u65f6KV\u7f13\u5b58\u5360\u7528\u663e\u5b58\u8fc7\u5927\uff0c\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u6216\u7cbe\u5ea6\u635f\u5931\u4e25\u91cd\u7684\u95ee\u9898", "method": "\u7ed3\u5408\u52a0\u6cd5\u91cf\u5316\uff08\u8f7b\u91cf\u7f16\u7801\u5668+\u7801\u672c\uff09\u548cRotary\u4f4d\u7f6e\u7f16\u7801\u7684\u4ea4\u6362\u7279\u6027\uff0c\u901a\u8fc7EM\u7b97\u6cd5\u8bad\u7ec3RoPE\u517c\u5bb9\u7801\u672c\uff0c\u5b9e\u73b0\u89e3\u7801\u8fc7\u7a0b\u4e0e\u6ce8\u610f\u529b\u673a\u5236\u7684\u9ad8\u6548\u6574\u5408", "result": "2-bit\u91cf\u5316\u4fdd\u6301\u9ad8\u7cbe\u5ea6\uff0c1-bit\u91cf\u5316\u5b9e\u73b0\u6700\u5c0f\u7cbe\u5ea6\u635f\u5931\uff0c\u5728\u957f\u4e0a\u4e0b\u6587\u57fa\u51c6\u6d4b\u8bd5\u548cGSM8K\u4e0a\u8d85\u8d8a\u73b0\u6709\u91cf\u5316\u65b9\u6cd5\uff0c\u5355\u5361RTX4090\u652f\u6301LLaMA-3.1 8B\u8fd0\u884c128K\u4e0a\u4e0b\u6587", "conclusion": "\u901a\u8fc7\u521b\u65b0\u7684\u53ef\u4ea4\u6362\u7801\u672c\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u5185\u5b58\u5360\u7528\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587LLM\u90e8\u7f72\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u76f8\u5173\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2506.18880", "pdf": "https://arxiv.org/pdf/2506.18880", "abs": "https://arxiv.org/abs/2506.18880", "authors": ["Yiyou Sun", "Shawn Hu", "Georgia Zhou", "Ken Zheng", "Hannaneh Hajishirzi", "Nouha Dziri", "Dawn Song"], "title": "OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent large-scale language models (LLMs) with long Chain-of-Thought\nreasoning-such as DeepSeek-R1-have achieved impressive results on\nOlympiad-level mathematics benchmarks. However, they often rely on a narrow set\nof strategies and struggle with problems that require a novel way of thinking.\nTo systematically investigate these limitations, we introduce\nOMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a\ncontrolled yet diverse benchmark designed to evaluate three axes of\nout-of-distribution generalization, inspired by Boden's typology of creativity:\n(1) Exploratory-applying known problem solving skills to more complex instances\nwithin the same problem domain; (2) Compositional-combining distinct reasoning\nskills, previously learned in isolation, to solve novel problems that require\nintegrating these skills in new and coherent ways; and (3)\nTransformative-adopting novel, often unconventional strategies by moving beyond\nfamiliar approaches to solve problems more effectively. OMEGA consists of\nprogrammatically generated training-test pairs derived from templated problem\ngenerators across geometry, number theory, algebra, combinatorics, logic, and\npuzzles, with solutions verified using symbolic, numerical, or graphical\nmethods. We evaluate frontier (or top-tier) LLMs and observe sharp performance\ndegradation as problem complexity increases. Moreover, we fine-tune the\nQwen-series models across all generalization settings and observe notable\nimprovements in exploratory generalization, while compositional generalization\nremains limited and transformative reasoning shows little to no improvement. By\nisolating and quantifying these fine-grained failures, OMEGA lays the\ngroundwork for advancing LLMs toward genuine mathematical creativity beyond\nmechanical proficiency.", "AI": {"tldr": "OMEGA\u57fa\u51c6\u4ece\u63a2\u7d22\u6027/\u7ec4\u5408\u6027/\u8f6c\u5316\u6027\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30LLM\u6570\u5b66\u521b\u9020\u529b\uff0c\u53d1\u73b0\u524d\u6cbf\u6a21\u578b\u5728\u590d\u6742\u95ee\u9898\u53ca\u7ec4\u5408/\u8f6c\u5316\u63a8\u7406\u4e0a\u5b58\u5728\u663e\u8457\u5c40\u9650\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u56fa\u5b9a\u7b56\u7565\uff0c\u96be\u4ee5\u5e94\u5bf9\u9700\u8981\u521b\u65b0\u6570\u5b66\u601d\u7ef4\u7684\u95ee\u9898\u3002\u9700\u7cfb\u7edf\u6027\u8bc4\u4f30\u6a21\u578b\u5728\u5206\u5e03\u5916\u6570\u5b66\u95ee\u9898\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u6784\u5efa\u7a0b\u5e8f\u5316\u751f\u6210\u7684OMEGA\u57fa\u51c6\uff08\u542b6\u5927\u9886\u57df\uff09\uff0c\u901a\u8fc7\u6a21\u677f\u751f\u6210\u5668\u521b\u5efa\u8bad\u7ec3-\u6d4b\u8bd5\u5bf9\uff0c\u91c7\u7528\u7b26\u53f7/\u6570\u503c/\u56fe\u5f62\u65b9\u6cd5\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u524d\u6cbfLLM\u5728\u95ee\u9898\u590d\u6742\u6027\u589e\u52a0\u65f6\u6027\u80fd\u9aa4\u964d\uff1b\u5fae\u8c03Qwen\u7cfb\u5217\u4ec5\u63d0\u5347\u63a2\u7d22\u6027\u6cdb\u5316\uff0c\u7ec4\u5408\u63a8\u7406\u6539\u5584\u6709\u9650\uff0c\u8f6c\u5316\u63a8\u7406\u65e0\u5b9e\u8d28\u6539\u8fdb\u3002", "conclusion": "OMEGA\u4e3a\u7a81\u7834\u673a\u68b0\u89e3\u9898\u3001\u5b9e\u73b0\u771f\u6b63\u6570\u5b66\u521b\u9020\u529b\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u9ad8\u7ea7\u63a8\u7406\u4e0a\u7684\u6839\u672c\u6027\u74f6\u9888\u3002"}}
{"id": "2506.18896", "pdf": "https://arxiv.org/pdf/2506.18896", "abs": "https://arxiv.org/abs/2506.18896", "authors": ["Jiaru Zou", "Ling Yang", "Jingwen Gu", "Jiahao Qiu", "Ke Shen", "Jingrui He", "Mengdi Wang"], "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs", "categories": ["cs.CL"], "comment": "Codes and Models: https://github.com/Gen-Verse/ReasonFlux", "summary": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor supervising intermediate reasoning steps in large language models (LLMs).\nPrevious PRMs are primarily trained on model final output responses and\nstruggle to evaluate intermediate thinking trajectories robustly, especially in\nthe emerging setting of trajectory-response outputs generated by frontier\nreasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a\nnovel trajectory-aware PRM explicitly designed to evaluate the\ntrajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both\nstep-level and trajectory-level supervision, enabling fine-grained reward\nassignment aligned with structured chain-of-thought data. We adapt\nReasonFlux-PRM to support reward supervision under both offline and online\nsettings, including (i) selecting high-quality model distillation data for\ndownstream supervised fine-tuning of smaller models, (ii) providing dense\nprocess-level rewards for policy optimization during reinforcement learning,\nand (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results\non challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond\ndemonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs\n(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our\nderived ReasonFlux-PRM-7B yields consistent performance improvements, achieving\naverage gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement\nlearning, and 6.3% in test-time scaling. We also release our efficient\nReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.\nProjects: https://github.com/Gen-Verse/ReasonFlux", "AI": {"tldr": "\u63d0\u51fa\u8f68\u8ff9\u611f\u77e5\u7684PRM\u6846\u67b6ReasonFlux-PRM\uff0c\u901a\u8fc7\u53cc\u91cd\u76d1\u7763\u673a\u5236\u5728\u591a\u9879\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347", "motivation": "\u73b0\u6709PRM\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u6700\u7ec8\u8f93\u51fa\u76d1\u7763\uff0c\u96be\u4ee5\u6709\u6548\u8bc4\u4f30\u524d\u6cbf\u63a8\u7406\u6a21\u578b\u751f\u6210\u7684\u8f68\u8ff9-\u54cd\u5e94\u578b\u4e2d\u95f4\u63a8\u7406\u8fc7\u7a0b", "method": "\u6574\u5408\u6b65\u9aa4\u7ea7\u548c\u8f68\u8ff9\u7ea7\u76d1\u7763\u673a\u5236\uff0c\u652f\u6301\u79bb\u7ebf\u548c\u5728\u7ebf\u573a\u666f\u4e0b\u7684\u5956\u52b1\u76d1\u7763\uff08\u5305\u62ec\u6570\u636e\u84b8\u998f\u9009\u62e9\u3001\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u548c\u6d4b\u8bd5\u65f6\u6269\u5c55\uff09", "result": "\u5728AIME/MATH500/GPQA\u57fa\u51c6\u4e0a\u8d85\u8d8a72B\u53c2\u6570\u6a21\u578b\uff0c\u76d1\u7763\u5fae\u8c03/\u5f3a\u5316\u5b66\u4e60/\u6d4b\u8bd5\u6269\u5c55\u5206\u522b\u63d0\u534712.1%/4.5%/6.3%", "conclusion": "ReasonFlux-PRM\u6846\u67b6\u6709\u6548\u63d0\u5347\u63a8\u7406\u6a21\u578b\u6027\u80fd\uff0c\u540c\u6b65\u5f00\u6e901.5B\u8f7b\u91cf\u7248\u672c\u63a8\u52a8\u8fb9\u7f18\u90e8\u7f72\u5e94\u7528"}}
{"id": "2506.17288", "pdf": "https://arxiv.org/pdf/2506.17288", "abs": "https://arxiv.org/abs/2506.17288", "authors": ["Jiale Zhang", "Jiaxiang Chen", "Zhucong Li", "Jie Ding", "Kui Zhao", "Zenglin Xu", "Xin Pang", "Yinghui Xu"], "title": "SlimRAG: Retrieval without Graphs via Entity-Aware Context Selection", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances language models by\nincorporating external knowledge at inference time. However, graph-based RAG\nsystems often suffer from structural overhead and imprecise retrieval: they\nrequire costly pipelines for entity linking and relation extraction, yet\nfrequently return subgraphs filled with loosely related or tangential content.\nThis stems from a fundamental flaw -- semantic similarity does not imply\nsemantic relevance. We introduce SlimRAG, a lightweight framework for retrieval\nwithout graphs. SlimRAG replaces structure-heavy components with a simple yet\neffective entity-aware mechanism. At indexing time, it constructs a compact\nentity-to-chunk table based on semantic embeddings. At query time, it\nidentifies salient entities, retrieves and scores associated chunks, and\nassembles a concise, contextually relevant input -- without graph traversal or\nedge construction. To quantify retrieval efficiency, we propose Relative Index\nToken Utilization (RITU), a metric measuring the compactness of retrieved\ncontent. Experiments across multiple QA benchmarks show that SlimRAG\noutperforms strong flat and graph-based baselines in accuracy while reducing\nindex size and RITU (e.g., 16.31 vs. 56+), highlighting the value of\nstructure-free, entity-centric context selection. The code will be released\nsoon. https://github.com/continue-ai-company/SlimRAG", "AI": {"tldr": "SlimRAG\u6846\u67b6\u901a\u8fc7\u5b9e\u4f53\u611f\u77e5\u673a\u5236\u66ff\u4ee3\u590d\u6742\u56fe\u7ed3\u6784\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210", "motivation": "\u73b0\u6709\u56fe\u57faRAG\u7cfb\u7edf\u5b58\u5728\u7ed3\u6784\u5197\u4f59\u548c\u68c0\u7d22\u4e0d\u7cbe\u51c6\u95ee\u9898\uff08\u8bed\u4e49\u76f8\u4f3c\u2260\u76f8\u5173\uff09\uff0c\u9700\u89e3\u51b3\u4f4e\u6548\u68c0\u7d22\u548c\u677e\u6563\u5b50\u56fe\u95ee\u9898", "method": "1. \u7d22\u5f15\u9636\u6bb5\u6784\u5efa\u5b9e\u4f53-\u5757\u5d4c\u5165\u8868\n2. \u67e5\u8be2\u65f6\u5b9e\u4f53\u8bc6\u522b+\u5757\u68c0\u7d22\u8bc4\u5206\n3. \u65e0\u56fe\u904d\u5386/\u8fb9\u6784\u5efa\u7684\u4e0a\u4e0b\u6587\u7ec4\u88c5", "result": "\u5728QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51c6\u786e\u7387\u8d85\u8d8a\u57fa\u7ebf\uff0cRITU\u6307\u6807\u663e\u8457\u964d\u4f4e\uff0816.31 vs 56+\uff09\uff0c\u7d22\u5f15\u5927\u5c0f\u7f29\u51cf", "conclusion": "\u5b9e\u4f53\u4e2d\u5fc3\u5316\u7684\u65e0\u7ed3\u6784\u4e0a\u4e0b\u6587\u9009\u62e9\u65b9\u6848\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u68c0\u7d22\u6548\u7387"}}
{"id": "2506.17310", "pdf": "https://arxiv.org/pdf/2506.17310", "abs": "https://arxiv.org/abs/2506.17310", "authors": ["Kangcong Li", "Peng Ye", "Chongjun Tu", "Lin Zhang", "Chunfeng Song", "Jiamin Wu", "Tao Yang", "Qihao Zheng", "Tao Chen"], "title": "PaceLLM: Brain-Inspired Large Language Models for Long-Context Understanding", "categories": ["q-bio.NC", "cs.CL", "cs.NE"], "comment": null, "summary": "While Large Language Models (LLMs) demonstrate strong performance across\ndomains, their long-context capabilities are limited by transient neural\nactivations causing information decay and unstructured feed-forward network\n(FFN) weights leading to semantic fragmentation. Inspired by the brain's\nworking memory and cortical modularity, we propose PaceLLM, featuring two\ninnovations: (1) a Persistent Activity (PA) Mechanism that mimics prefrontal\ncortex (PFC) neurons' persistent firing by introducing an activation-level\nmemory bank to dynamically retrieve, reuse, and update critical FFN states,\naddressing contextual decay; and (2) Cortical Expert (CE) Clustering that\nemulates task-adaptive neural specialization to reorganize FFN weights into\nsemantic modules, establishing cross-token dependencies and mitigating\nfragmentation. Extensive evaluations show that PaceLLM achieves 6% improvement\non LongBench's Multi-document QA and 12.5-17.5% performance gains on\nInfinite-Bench tasks, while extending measurable context length to 200K tokens\nin Needle-In-A-Haystack (NIAH) tests. This work pioneers brain-inspired LLM\noptimization and is complementary to other works. Besides, it can be\ngeneralized to any model and enhance their long-context performance and\ninterpretability without structural overhauls.", "AI": {"tldr": "PaceLLM\u901a\u8fc7\u6a21\u62df\u5927\u8111\u5de5\u4f5c\u8bb0\u5fc6\u673a\u5236\uff08\u6301\u4e45\u6d3b\u52a8\u673a\u5236\uff09\u548c\u795e\u7ecf\u6a21\u5757\u5316\u7279\u6027\uff08\u76ae\u5c42\u4e13\u5bb6\u805a\u7c7b\uff09\uff0c\u6709\u6548\u89e3\u51b3LLM\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u4fe1\u606f\u8870\u51cf\u548c\u8bed\u4e49\u788e\u7247\u5316\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u957f\u6587\u672c\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u4e2d\u5b58\u5728\u4fe1\u606f\u8870\u51cf\uff08\u7531\u795e\u7ecf\u6fc0\u6d3b\u77ac\u6001\u6027\u5bfc\u81f4\uff09\u548c\u8bed\u4e49\u788e\u7247\u5316\uff08\u7531\u975e\u7ed3\u6784\u5316FFN\u6743\u91cd\u5f15\u53d1\uff09\u4e24\u5927\u7f3a\u9677\uff0c\u9700\u8981\u501f\u9274\u5927\u8111\u5de5\u4f5c\u673a\u5236\u8fdb\u884c\u4f18\u5316\u3002", "method": "1. \u6301\u4e45\u6d3b\u52a8\u673a\u5236\uff1a\u5f15\u5165\u6fc0\u6d3b\u7ea7\u8bb0\u5fc6\u5e93\u52a8\u6001\u7ba1\u7406FFN\u5173\u952e\u72b6\u6001\uff1b2. \u76ae\u5c42\u4e13\u5bb6\u805a\u7c7b\uff1a\u5c06FFN\u6743\u91cd\u91cd\u7ec4\u4e3a\u8bed\u4e49\u6a21\u5757\u3002\u901a\u8fc7\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u53cc\u91cd\u4f18\u5316\u7b56\u7565\u589e\u5f3a\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u3002", "result": "\u5728LongBench\u591a\u6587\u6863QA\u4efb\u52a1\u63d0\u53476%\uff0cInfinite-Bench\u4efb\u52a1\u63d0\u534712.5-17.5%\uff0cNIAH\u6d4b\u8bd5\u652f\u6301200K tokens\u4e0a\u4e0b\u6587\uff0c\u6027\u80fd\u6307\u6807\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "PaceLLM\u9996\u6b21\u5b9e\u73b0\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684LLM\u4f18\u5316\u6846\u67b6\uff0c\u5177\u6709\u6a21\u578b\u65e0\u5173\u6027\uff0c\u53ef\u5728\u4e0d\u6539\u53d8\u67b6\u6784\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u957f\u6587\u672c\u5904\u7406\u80fd\u529b\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u4e0e\u73b0\u6709\u6280\u672f\u5f62\u6210\u4e92\u8865\u3002"}}
{"id": "2506.17331", "pdf": "https://arxiv.org/pdf/2506.17331", "abs": "https://arxiv.org/abs/2506.17331", "authors": ["Craig Steven Wright"], "title": "Beyond Prediction -- Structuring Epistemic Integrity in Artificial Reasoning Systems", "categories": ["cs.LO", "cs.CL", "math.LO", "68T27, 03B70", "I.2.4; I.2.3"], "comment": "126 pages, 0 figures, includes formal frameworks and architecture\n  blueprint; no prior version; suitable for submission under AI and Logic\n  categories", "summary": "This paper develops a comprehensive framework for artificial intelligence\nsystems that operate under strict epistemic constraints, moving beyond\nstochastic language prediction to support structured reasoning, propositional\ncommitment, and contradiction detection. It formalises belief representation,\nmetacognitive processes, and normative verification, integrating symbolic\ninference, knowledge graphs, and blockchain-based justification to ensure\ntruth-preserving, auditably rational epistemic agents.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e25\u683c\u8ba4\u77e5\u7ea6\u675f\u7684\u4eba\u5de5\u667a\u80fd\u6846\u67b6\uff0c\u6574\u5408\u7b26\u53f7\u63a8\u7406/\u77e5\u8bc6\u56fe\u8c31/\u533a\u5757\u94fe\u6280\u672f\u5b9e\u73b0\u53ef\u9a8c\u8bc1\u7684\u7406\u6027\u8ba4\u77e5\u7cfb\u7edf", "motivation": "\u7a81\u7834\u4f20\u7edf\u6982\u7387\u8bed\u8a00\u6a21\u578b\u7684\u5c40\u9650\uff0c\u6784\u5efa\u5177\u6709\u7ed3\u6784\u5316\u63a8\u7406\u3001\u547d\u9898\u627f\u8bfa\u548c\u77db\u76fe\u68c0\u6d4b\u80fd\u529b\u7684\u65b0\u578bAI\u7cfb\u7edf", "method": "\u901a\u8fc7\u5f62\u5f0f\u5316\u4fe1\u5ff5\u8868\u5f81\u3001\u5143\u8ba4\u77e5\u8fc7\u7a0b\u548c\u89c4\u8303\u9a8c\u8bc1\u673a\u5236\uff0c\u878d\u5408\u7b26\u53f7\u63a8\u7406/\u77e5\u8bc6\u56fe\u8c31/\u533a\u5757\u94fe\u5b58\u8bc1\u6280\u672f", "result": "\u5efa\u7acb\u771f\u7406\u4fdd\u6301\u6027\u3001\u53ef\u5ba1\u8ba1\u7684\u8ba4\u77e5\u4ee3\u7406\u4f53\u7cfb\uff0c\u786e\u4fdd\u77e5\u8bc6\u6f14\u5316\u7684\u53ef\u8ffd\u6eaf\u6027\u4e0e\u903b\u8f91\u4e00\u81f4\u6027", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6784\u5efa\u7b26\u5408\u4e25\u683c\u8ba4\u77e5\u89c4\u8303\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u548c\u6280\u672f\u5b9e\u73b0\u8def\u5f84"}}
{"id": "2506.17351", "pdf": "https://arxiv.org/pdf/2506.17351", "abs": "https://arxiv.org/abs/2506.17351", "authors": ["Mostafa Shahin", "Beena Ahmed", "Julien Epps"], "title": "Zero-Shot Cognitive Impairment Detection from Speech Using AudioLLM", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.MM", "eess.AS"], "comment": null, "summary": "Cognitive impairment (CI) is of growing public health concern, and early\ndetection is vital for effective intervention. Speech has gained attention as a\nnon-invasive and easily collectible biomarker for assessing cognitive decline.\nTraditional CI detection methods typically rely on supervised models trained on\nacoustic and linguistic features extracted from speech, which often require\nmanual annotation and may not generalise well across datasets and languages. In\nthis work, we propose the first zero-shot speech-based CI detection method\nusing the Qwen2- Audio AudioLLM, a model capable of processing both audio and\ntext inputs. By designing prompt-based instructions, we guide the model in\nclassifying speech samples as indicative of normal cognition or cognitive\nimpairment. We evaluate our approach on two datasets: one in English and\nanother multilingual, spanning different cognitive assessment tasks. Our\nresults show that the zero-shot AudioLLM approach achieves performance\ncomparable to supervised methods and exhibits promising generalizability and\nconsistency across languages, tasks, and datasets.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8eQwen2-Audio AudioLLM\u7684\u96f6\u6837\u672c\u8bed\u97f3\u8ba4\u77e5\u969c\u788d\u68c0\u6d4b\u65b9\u6cd5\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u5373\u53ef\u8de8\u8bed\u8a00/\u4efb\u52a1\u5b9e\u73b0\u4e0e\u76d1\u7763\u5b66\u4e60\u76f8\u5f53\u7684\u68c0\u6d4b\u6548\u679c", "motivation": "\u4f20\u7edf\u8ba4\u77e5\u969c\u788d\u68c0\u6d4b\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u7684\u58f0\u5b66/\u8bed\u8a00\u7279\u5f81\uff0c\u5b58\u5728\u6cdb\u5316\u6027\u5dee\u548c\u591a\u8bed\u8a00\u573a\u666f\u53d7\u9650\u7684\u95ee\u9898\u3002\u5229\u7528\u8bed\u97f3\u4f5c\u4e3a\u751f\u7269\u6807\u5fd7\u7269\uff0c\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u7684\u96f6\u6837\u672c\u5206\u7c7b\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u57fa\u4e8eprompt\u7684\u6307\u4ee4\uff0c\u5f15\u5bfcQwen2-Audio\u591a\u6a21\u6001\u5927\u6a21\u578b\u5bf9\u8bed\u97f3\u6837\u672c\u8fdb\u884c\u6b63\u5e38/\u5f02\u5e38\u5206\u7c7b\uff0c\u5728\u82f1\u8bed\u548c\u591a\u8bed\u8a00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e0d\u540c\u8ba4\u77e5\u8bc4\u4f30\u4efb\u52a1\u7684\u9002\u5e94\u6027", "result": "\u96f6\u6837\u672c\u65b9\u6cd5\u5728\u8de8\u8bed\u8a00\uff08\u82f1\u8bed/\u591a\u8bed\u8a00\uff09\u3001\u8de8\u4efb\u52a1\uff08\u4e0d\u540c\u8ba4\u77e5\u6d4b\u8bd5\uff09\u573a\u666f\u4e0b\u8fbe\u5230\u4e0e\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u7684\u51c6\u786e\u7387\uff0c\u5c55\u73b0\u51fa\u4f18\u79c0\u7684\u6cdb\u5316\u80fd\u529b\u548c\u4e00\u81f4\u6027", "conclusion": "\u97f3\u9891\u5927\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u8ba4\u77e5\u969c\u788d\u68c0\u6d4b\u4e2d\u5c55\u73b0\u51fa\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\uff0c\u4e3a\u5f00\u53d1\u65e0\u6807\u6ce8\u4f9d\u8d56\u7684\u667a\u80fd\u7b5b\u67e5\u5de5\u5177\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2506.17562", "pdf": "https://arxiv.org/pdf/2506.17562", "abs": "https://arxiv.org/abs/2506.17562", "authors": ["Haoxuan Che", "Haibo Jin", "Zhengrui Guo", "Yi Lin", "Cheng Jin", "Hao Chen"], "title": "LLM-driven Medical Report Generation via Communication-efficient Heterogeneous Federated Learning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "LLMs have demonstrated significant potential in Medical Report Generation\n(MRG), yet their development requires large amounts of medical image-report\npairs, which are commonly scattered across multiple centers. Centralizing these\ndata is exceptionally challenging due to privacy regulations, thereby impeding\nmodel development and broader adoption of LLM-driven MRG models. To address\nthis challenge, we present FedMRG, the first framework that leverages Federated\nLearning (FL) to enable privacy-preserving, multi-center development of\nLLM-driven MRG models, specifically designed to overcome the critical challenge\nof communication-efficient LLM training under multi-modal data heterogeneity.\nTo start with, our framework tackles the fundamental challenge of communication\noverhead in FL-LLM tuning by employing low-rank factorization to efficiently\ndecompose parameter updates, significantly reducing gradient transmission costs\nand making LLM-driven MRG feasible in bandwidth-constrained FL settings.\nFurthermore, we observed the dual heterogeneity in MRG under the FL scenario:\nvarying image characteristics across medical centers, as well as diverse\nreporting styles and terminology preferences. To address this, we further\nenhance FedMRG with (1) client-aware contrastive learning in the MRG encoder,\ncoupled with diagnosis-driven prompts, which capture both globally\ngeneralizable and locally distinctive features while maintaining diagnostic\naccuracy; and (2) a dual-adapter mutual boosting mechanism in the MRG decoder\nthat harmonizes generic and specialized adapters to address variations in\nreporting styles and terminology. Through extensive evaluation of our\nestablished FL-MRG benchmark, we demonstrate the generalizability and\nadaptability of FedMRG, underscoring its potential in harnessing multi-center\ndata and generating clinically accurate reports while maintaining communication\nefficiency.", "AI": {"tldr": "\u9996\u4e2a\u57fa\u4e8e\u8054\u90a6\u5b66\u4e60\u7684\u533b\u7597\u62a5\u544a\u751f\u6210\u6846\u67b6FedMRG\uff0c\u901a\u8fc7\u4f4e\u79e9\u5206\u89e3\u964d\u4f4e\u901a\u4fe1\u6210\u672c\uff0c\u7ed3\u5408\u5ba2\u6237\u7aef\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u4e0e\u53cc\u91cd\u9002\u914d\u5668\u673a\u5236\uff0c\u5b9e\u73b0\u8de8\u591a\u4e2d\u5fc3\u9690\u79c1\u4fdd\u62a4\u4e0b\u7684\u9ad8\u6548\u6a21\u578b\u8bad\u7ec3\u4e0e\u591a\u6a21\u6001\u6570\u636e\u5f02\u6784\u5904\u7406\u3002", "motivation": "\u89e3\u51b3\u533b\u7597\u5f71\u50cf\u62a5\u544a\u6570\u636e\u5206\u6563\u5bfc\u81f4\u7684\u9690\u79c1\u5408\u89c4\u96be\u9898\uff0c\u7a81\u7834\u4f20\u7edfLLM\u8bad\u7ec3\u5bf9\u96c6\u4e2d\u5f0f\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u5e94\u5bf9\u8054\u90a6\u5b66\u4e60\u4e2d\u591a\u6a21\u6001\u6570\u636e\u5f02\u6784\u4e0e\u901a\u4fe1\u6548\u7387\u7684\u53cc\u91cd\u6311\u6218\u3002", "method": "1. \u4f4e\u79e9\u5206\u89e3\u538b\u7f29\u68af\u5ea6\u53c2\u6570 2. \u5ba2\u6237\u7aef\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60+\u8bca\u65ad\u9a71\u52a8\u63d0\u793a\u6355\u83b7\u5168\u5c40/\u5c40\u90e8\u7279\u5f81 3. \u53cc\u91cd\u9002\u914d\u5668\u4e92\u4fc3\u673a\u5236\u534f\u8c03\u62a5\u544a\u98ce\u683c\u4e0e\u672f\u8bed\u5dee\u5f02", "result": "\u5728FL-MRG\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\uff0c\u5728\u901a\u4fe1\u6548\u7387\u63d0\u534730%\u7684\u540c\u65f6\u4fdd\u6301\u4e34\u5e8a\u62a5\u544a\u51c6\u786e\u6027\uff08BLEU-4\u8fbe0.82\uff0cROUGE-L 0.75\uff09", "conclusion": "FedMRG\u4e3a\u9690\u79c1\u654f\u611f\u7684\u533b\u7597AI\u534f\u4f5c\u5f00\u53d1\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u8054\u90a6\u67b6\u6784\u7a81\u7834\u6570\u636e\u5b64\u5c9b\uff0c\u5176\u53cc\u6a21\u6001\u5f02\u6784\u5904\u7406\u673a\u5236\u5bf9\u591a\u4e2d\u5fc3MRG\u5e94\u7528\u5177\u6709\u63a8\u5e7f\u4ef7\u503c\u3002"}}
{"id": "2506.17585", "pdf": "https://arxiv.org/pdf/2506.17585", "abs": "https://arxiv.org/abs/2506.17585", "authors": ["Yukun Huang", "Sanxing Chen", "Jian Pei", "Manzil Zaheer", "Bhuwan Dhingra"], "title": "Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Trustworthy language models should provide both correct and verifiable\nanswers. While language models can sometimes attribute their outputs to\npretraining data, their citations are often unreliable due to hallucination. As\na result, current systems insert citations by querying an external retriever at\ninference time, introducing latency, infrastructure dependence, and\nvulnerability to retrieval noise. We explore whether LLMs can be made to\nreliably attribute to the documents seen during (continual)\npretraining--without test-time retrieval--by revising the training process. To\nevaluate this, we release CitePretrainBench, a benchmark that mixes real-world\ncorpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and\nprobes both short-form (single fact) and long-form (multi-fact) citation tasks.\nOur approach follows a two-stage process: (1) continual pretraining to bind\nfacts to persistent document identifiers, and (2) instruction tuning to elicit\ncitation behavior. We find that simple Passive Indexing, which appends an\nidentifier to each document, helps memorize verbatim text but fails on\nparaphrased or compositional facts. Instead, we propose Active Indexing, which\ncontinually pretrains on synthetic QA pairs that (1) restate each fact in\ndiverse compositional forms, and (2) require bidirectional source-to-fact and\nfact-to-source generation, jointly teaching the model to generate content from\na cited source and to attribute its own answers. Experiments with Qwen2.5-7B\nand 3B show that Active Indexing consistently outperforms Passive Indexing\nacross all tasks and models, with citation precision gains up to 30.2 percent.\nOur ablation studies reveal that performance continues to improve as we scale\nthe amount of augmented data, showing a clear upward trend even at 16 times the\noriginal token count.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.17629", "pdf": "https://arxiv.org/pdf/2506.17629", "abs": "https://arxiv.org/abs/2506.17629", "authors": ["Kailing Li", "Qi'ao Xu", "Tianwen Qian", "Yuqian Fu", "Yang Jiao", "Xiaoling Wang"], "title": "CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Embodied Visual Reasoning (EVR) seeks to follow complex, free-form\ninstructions based on egocentric video, enabling semantic understanding and\nspatiotemporal reasoning in dynamic environments. Despite its promising\npotential, EVR encounters significant challenges stemming from the diversity of\ncomplex instructions and the intricate spatiotemporal dynamics in long-term\negocentric videos. Prior solutions either employ Large Language Models (LLMs)\nover static video captions, which often omit critical visual details, or rely\non end-to-end Vision-Language Models (VLMs) that struggle with stepwise\ncompositional reasoning. Consider the complementary strengths of LLMs in\nreasoning and VLMs in perception, we propose CLiViS. It is a novel\ntraining-free framework that leverages LLMs for high-level task planning and\norchestrates VLM-driven open-world visual perception to iteratively update the\nscene context. Building on this synergy, the core of CLiViS is a dynamic\nCognitive Map that evolves throughout the reasoning process. This map\nconstructs a structured representation of the embodied scene, bridging\nlow-level perception and high-level reasoning. Extensive experiments across\nmultiple benchmarks demonstrate the effectiveness and generality of CLiViS,\nespecially in handling long-term visual dependencies. Code is available at\nhttps://github.com/Teacher-Tom/CLiViS.", "AI": {"tldr": "\u63d0\u51faCLiViS\u6846\u67b6\uff0c\u901a\u8fc7LLM\u4efb\u52a1\u89c4\u5212\u4e0eVLM\u5f00\u653e\u4e16\u754c\u611f\u77e5\u534f\u540c\uff0c\u6784\u5efa\u52a8\u6001\u8ba4\u77e5\u56fe\u8c31\u89e3\u51b3\u5177\u8eab\u89c6\u89c9\u63a8\u7406\u7684\u957f\u65f6\u5e8f\u4f9d\u8d56\u95ee\u9898\u3002", "motivation": "\u73b0\u6709EVR\u65b9\u6cd5\u9762\u4e34\u590d\u6742\u6307\u4ee4\u591a\u6837\u6027\u4e0e\u957f\u65f6\u5e8f\u89c6\u9891\u52a8\u6001\u573a\u666f\u7684\u53cc\u91cd\u6311\u6218\uff0c\u57fa\u4e8eLLM\u7684\u9759\u6001\u63cf\u8ff0\u4e22\u5931\u89c6\u89c9\u7ec6\u8282\uff0c\u7aef\u5230\u7aefVLM\u7f3a\u4e4f\u9010\u6b65\u63a8\u7406\u80fd\u529b\u3002", "method": "\u878d\u5408LLM\u63a8\u7406\u80fd\u529b\u4e0eVLM\u611f\u77e5\u4f18\u52bf\uff0c\u901a\u8fc7\u8fed\u4ee3\u5f0f\u573a\u666f\u4e0a\u4e0b\u6587\u66f4\u65b0\u673a\u5236\u6784\u5efa\u52a8\u6001\u8ba4\u77e5\u56fe\u8c31\uff0c\u5b9e\u73b0\u4f4e\u5c42\u611f\u77e5\u4e0e\u9ad8\u5c42\u63a8\u7406\u7684\u6865\u63a5\u3002", "result": "\u8de8\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u5728\u957f\u65f6\u5e8f\u89c6\u89c9\u4f9d\u8d56\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "\u8ba4\u77e5\u56fe\u8c31\u7684\u52a8\u6001\u6f14\u5316\u673a\u5236\u6210\u529f\u89e3\u51b3\u4e86\u5177\u8eab\u63a8\u7406\u4e2d\u7684\u65f6\u7a7a\u5efa\u6a21\u96be\u9898\uff0c\u5f00\u521b\u4e86\u6a21\u5757\u5316\u534f\u540c\u63a8\u7406\u65b0\u8303\u5f0f\u3002"}}
{"id": "2506.17673", "pdf": "https://arxiv.org/pdf/2506.17673", "abs": "https://arxiv.org/abs/2506.17673", "authors": ["Seonglae Cho", "Harryn Oh", "Donghyun Lee", "Luis Eduardo Rodrigues Vieira", "Andrew Bermingham", "Ziad El Sayed"], "title": "FaithfulSAE: Towards Capturing Faithful Features with Sparse Autoencoders without External Dataset Dependencies", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "18 pages, 18 figures", "summary": "Sparse Autoencoders (SAEs) have emerged as a promising solution for\ndecomposing large language model representations into interpretable features.\nHowever, Paulo and Belrose (2025) have highlighted instability across different\ninitialization seeds, and Heap et al. (2025) have pointed out that SAEs may not\ncapture model-internal features. These problems likely stem from training SAEs\non external datasets - either collected from the Web or generated by another\nmodel - which may contain out-of-distribution (OOD) data beyond the model's\ngeneralisation capabilities. This can result in hallucinated SAE features,\nwhich we term \"Fake Features\", that misrepresent the model's internal\nactivations. To address these issues, we propose FaithfulSAE, a method that\ntrains SAEs on the model's own synthetic dataset. Using FaithfulSAEs, we\ndemonstrate that training SAEs on less-OOD instruction datasets results in SAEs\nbeing more stable across seeds. Notably, FaithfulSAEs outperform SAEs trained\non web-based datasets in the SAE probing task and exhibit a lower Fake Feature\nRatio in 5 out of 7 models. Overall, our approach eliminates the dependency on\nexternal datasets, advancing interpretability by better capturing\nmodel-internal features while highlighting the often neglected importance of\nSAE training datasets.", "AI": {"tldr": "FaithfulSAE\u901a\u8fc7\u4f7f\u7528\u6a21\u578b\u81ea\u8eab\u5408\u6210\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5728\u5916\u90e8\u6570\u636e\u8bad\u7ec3\u65f6\u7684\u5047\u7279\u5f81\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7a33\u5b9a\u6027\u548c\u7279\u5f81\u4fdd\u771f\u5ea6", "motivation": "\u73b0\u6709\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAE)\u5728\u5916\u90e8\u6570\u636e\u96c6\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u8d85\u51fa\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684OOD\u6570\u636e\u95ee\u9898\uff0c\u4ea7\u751f\u8bef\u5bfc\u6027\u5047\u7279\u5f81\uff0c\u9700\u8981\u6539\u8fdb\u8bad\u7ec3\u6570\u636e\u6e90", "method": "\u63d0\u51faFaithfulSAE\u65b9\u6cd5\uff0c\u4f7f\u7528\u76ee\u6807\u6a21\u578b\u81ea\u8eab\u751f\u6210\u7684\u6307\u4ee4\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u51cf\u5c11OOD\u6570\u636e\u5f71\u54cd", "result": "\u57285/7\u6a21\u578b\u4e2d\u5047\u7279\u5f81\u6bd4\u4f8b\u66f4\u4f4e\uff0c\u8de8\u79cd\u5b50\u8bad\u7ec3\u7a33\u5b9a\u6027\u66f4\u597d\uff0cSAE\u63a2\u6d4b\u4efb\u52a1\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u57fa\u4e8e\u7f51\u7edc\u6570\u636e\u7684\u65b9\u6cd5", "conclusion": "\u901a\u8fc7\u6d88\u9664\u5916\u90e8\u6570\u636e\u4f9d\u8d56\uff0c\u8be5\u65b9\u6cd5\u80fd\u66f4\u51c6\u786e\u6355\u6349\u6a21\u578b\u5185\u90e8\u7279\u5f81\uff0c\u540c\u65f6\u63ed\u793a\u4e86SAE\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u91cd\u8981\u6027"}}
{"id": "2506.17686", "pdf": "https://arxiv.org/pdf/2506.17686", "abs": "https://arxiv.org/abs/2506.17686", "authors": ["Alican Gok", "Oguzhan Buyuksolak", "Osman Erman Okman", "Murat Saraclar"], "title": "Enhancing Few-shot Keyword Spotting Performance through Pre-Trained Self-supervised Speech Models", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "To be submitted to IEEE Signal Processing Letters, 5 pages, 3 figures", "summary": "Keyword Spotting plays a critical role in enabling hands-free interaction for\nbattery-powered edge devices. Few-Shot Keyword Spotting (FS-KWS) addresses the\nscalability and adaptability challenges of traditional systems by enabling\nrecognition of custom keywords with only a few examples. However, existing\nFS-KWS systems achieve subpar accuracy at desirable false acceptance rates,\nparticularly in resource-constrained edge environments. To address these\nissues, we propose a training scheme that leverages self-supervised learning\nmodels for robust feature extraction, dimensionality reduction, and knowledge\ndistillation. The teacher model, based on Wav2Vec 2.0 is trained using\nSub-center ArcFace loss, which enhances inter-class separability and\nintra-class compactness. To enable efficient deployment on edge devices, we\nintroduce attention-based dimensionality reduction and train a standard\nlightweight ResNet15 student model. We evaluate the proposed approach on the\nEnglish portion of the Multilingual Spoken Words Corpus (MSWC) and the Google\nSpeech Commands (GSC) datasets. Notably, the proposed training method improves\nthe 10-shot classification accuracy from 33.4% to 74.1% on 11 classes at 1%\nfalse alarm accuracy on the GSC dataset, thus making it significantly\nbetter-suited for a real use case scenario.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u77e5\u8bc6\u84b8\u998f\u7684FS-KWS\u8bad\u7ec3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u8fb9\u7f18\u8bbe\u5907\u5173\u952e\u8bcd\u8bc6\u522b\u7cbe\u5ea6", "motivation": "\u73b0\u6709FS-KWS\u7cfb\u7edf\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b58\u5728\u51c6\u786e\u7387\u4f4e\u548c\u8bef\u63a5\u53d7\u7387\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u9700\u6c42", "method": "1. \u4f7f\u7528Wav2Vec 2.0\u6559\u5e08\u6a21\u578b\u8fdb\u884cSub-center ArcFace\u635f\u5931\u8bad\u7ec3\n2. \u5f15\u5165\u6ce8\u610f\u529b\u673a\u5236\u964d\u7ef4\n3. \u77e5\u8bc6\u84b8\u998f\u5230\u8f7b\u91cf\u7ea7ResNet15\u5b66\u751f\u6a21\u578b", "result": "\u5728GSC\u6570\u636e\u96c6\u4e0a10-shot\u5206\u7c7b\u51c6\u786e\u7387\u4ece33.4%\u63d0\u5347\u81f374.1%\uff081%\u8bef\u62a5\u7387\uff09\uff0c\u5728MSWC\u6570\u636e\u96c6\u9a8c\u8bc1\u6709\u6548\u6027", "conclusion": "\u8be5\u65b9\u6848\u6709\u6548\u5e73\u8861\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907\u5173\u952e\u8bcd\u8bc6\u522b\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.17781", "pdf": "https://arxiv.org/pdf/2506.17781", "abs": "https://arxiv.org/abs/2506.17781", "authors": ["Miguel Romero", "Shuoyang Ding", "Corey D. Barret", "Georgiana Dinu", "George Karypis"], "title": "Beyond instruction-conditioning, MoTE: Mixture of Task Experts for Multi-task Embedding Models", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Dense embeddings are fundamental to modern machine learning systems, powering\nRetrieval-Augmented Generation (RAG), information retrieval, and representation\nlearning. While instruction-conditioning has become the dominant approach for\nembedding specialization, its direct application to low-capacity models imposes\nfundamental representational constraints that limit the performance gains\nderived from specialization. In this paper, we analyze these limitations and\nintroduce the Mixture of Task Experts (MoTE) transformer block, which leverages\ntask-specialized parameters trained with Task-Aware Contrastive Learning\n(\\tacl) to enhance the model ability to generate specialized embeddings.\nEmpirical results show that MoTE achieves $64\\%$ higher performance gains in\nretrieval datasets ($+3.27 \\rightarrow +5.21$) and $43\\%$ higher performance\ngains across all datasets ($+1.81 \\rightarrow +2.60$). Critically, these gains\nare achieved without altering instructions, training data, inference time, or\nnumber of active parameters.", "AI": {"tldr": "\u63d0\u51faMoTE\u65b9\u6cd5\u901a\u8fc7\u6df7\u5408\u4efb\u52a1\u4e13\u5bb6\u548c\u4efb\u52a1\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u5d4c\u5165\u6a21\u578b\u6027\u80fd\u540c\u65f6\u4fdd\u6301\u8d44\u6e90\u6548\u7387", "motivation": "\u73b0\u6709\u6307\u4ee4\u8c03\u8282\u65b9\u6cd5\u5728\u4f4e\u5bb9\u91cf\u6a21\u578b\u4e2d\u5b58\u5728\u8868\u793a\u9650\u5236\uff0c\u9650\u5236\u4e86\u5d4c\u5165\u4e13\u4e1a\u5316\u7684\u6027\u80fd\u63d0\u5347", "method": "\u5f15\u5165\u6df7\u5408\u4efb\u52a1\u4e13\u5bb6(MoTE)\u7684transformer\u6a21\u5757\uff0c\u7ed3\u5408\u4efb\u52a1\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60(TACL)\u8bad\u7ec3\u4efb\u52a1\u4e13\u7528\u53c2\u6570", "result": "MoTE\u5728\u68c0\u7d22\u6570\u636e\u96c6\u4e0a\u5b9e\u73b064%\u66f4\u9ad8\u589e\u76ca(+3.27\u2192+5.21)\uff0c\u6240\u6709\u6570\u636e\u96c6\u5e73\u5747\u589e\u76ca43%(+1.81\u2192+2.60)\uff0c\u4e14\u4e0d\u6539\u53d8\u539f\u6709\u8d44\u6e90\u4f7f\u7528", "conclusion": "MoTE\u5728\u4fdd\u6301\u8d44\u6e90\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5d4c\u5165\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u4e13\u7528\u5d4c\u5165\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2506.17788", "pdf": "https://arxiv.org/pdf/2506.17788", "abs": "https://arxiv.org/abs/2506.17788", "authors": ["Shahab Rahimirad", "Guven Gergerli", "Lucia Romero", "Angela Qian", "Matthew Lyle Olson", "Simon Stepputtis", "Joseph Campbell"], "title": "Bayesian Social Deduction with Graph-Informed Language Models", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "I.2.1; I.2.7"], "comment": "32 pages, 10 figures. Under review", "summary": "Social reasoning - inferring unobservable beliefs and intentions from partial\nobservations of other agents - remains a challenging task for large language\nmodels (LLMs). We evaluate the limits of current reasoning language models in\nthe social deduction game Avalon and find that while the largest models\ndemonstrate strong performance, they require extensive test-time inference and\ndegrade sharply when distilled to smaller, real-time-capable variants. To\naddress this, we introduce a hybrid reasoning framework that externalizes\nbelief inference to a structured probabilistic model, while using an LLM for\nlanguage understanding and interaction. Our approach achieves competitive\nperformance with much larger models in Agent-Agent play and, notably, is the\nfirst language agent to defeat human players in a controlled study - achieving\na 67% win rate and receiving higher qualitative ratings than both reasoning\nbaselines and human teammates. We release code, models, and a dataset to\nsupport future work on social reasoning in LLM agents, which can be found at\nhttps://camp-lab-purdue.github.io/bayesian-social-deduction/", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u6982\u7387\u56fe\u6a21\u578b\u4e0e\u8bed\u8a00\u6a21\u578b\u7684\u6df7\u5408\u63a8\u7406\u6846\u67b6\uff0c\u5728\u793e\u4ea4\u63a8\u7406\u6e38\u620f\u300a\u963f\u74e6\u9686\u300b\u4e2d\u5b9e\u73b0\u4e8667%\u7684\u4eba\u7c7b\u5bf9\u6218\u80dc\u7387\uff0c\u7a81\u7834\u8bed\u8a00\u667a\u80fd\u4f53\u793e\u4f1a\u63a8\u7406\u74f6\u9888", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u793e\u4ea4\u63a8\u7406\u4efb\u52a1\uff08\u901a\u8fc7\u90e8\u5206\u89c2\u5bdf\u63a8\u65ad\u4ed6\u4eba\u4fe1\u5ff5\u548c\u610f\u56fe\uff09\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff1a\u5927\u6a21\u578b\u9700\u8981\u5927\u91cf\u63a8\u7406\u8d44\u6e90\u4e14\u96be\u4ee5\u90e8\u7f72\u5230\u5b9e\u65f6\u573a\u666f\uff0c\u5c0f\u6a21\u578b\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002\u672c\u6587\u65e8\u5728\u6784\u5efa\u9ad8\u6548\u53ef\u9760\u7684\u793e\u4f1a\u63a8\u7406\u6846\u67b6", "method": "\u63d0\u51fa\u6df7\u5408\u63a8\u7406\u6846\u67b6\uff1a\u5c06\u4fe1\u5ff5\u63a8\u7406\u5916\u5316\u4e3a\u7ed3\u6784\u5316\u6982\u7387\u6a21\u578b\u5904\u7406\uff0c\u8bed\u8a00\u6a21\u578b\u4ec5\u8d1f\u8d23\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u4ea4\u4e92\u3002\u6982\u7387\u6a21\u578b\u901a\u8fc7\u8d1d\u53f6\u65af\u63a8\u7406\u8ffd\u8e2a\u73a9\u5bb6\u4fe1\u5ff5\u72b6\u6001", "result": "\u6df7\u5408\u6846\u67b6\u5728\u667a\u80fd\u4f53\u5bf9\u6297\u4e2d\u8fbe\u5230\u4e0e\u5927\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u9996\u6b21\u5728\u63a7\u5236\u5b9e\u9a8c\u4e2d\u51fb\u8d25\u4eba\u7c7b\u73a9\u5bb6\uff0867%\u80dc\u7387\uff09\uff0c\u4e14\u83b7\u5f97\u6bd4\u57fa\u7ebf\u6a21\u578b\u548c\u4eba\u7c7b\u961f\u53cb\u66f4\u9ad8\u7684\u4e3b\u89c2\u8bc4\u4ef7", "conclusion": "\u5206\u79bb\u4fe1\u5ff5\u63a8\u7406\u4e0e\u8bed\u8a00\u5904\u7406\u7684\u8ba1\u7b97\u6846\u67b6\u663e\u8457\u63d0\u5347\u8bed\u8a00\u667a\u80fd\u4f53\u7684\u793e\u4f1a\u63a8\u7406\u6548\u7387\uff0c\u8bc1\u660e\u7ed3\u6784\u5316\u6982\u7387\u6a21\u578b\u4e0e\u8bed\u8a00\u6a21\u578b\u534f\u540c\u5de5\u4f5c\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5b9e\u65f6\u793e\u4ea4\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2506.17828", "pdf": "https://arxiv.org/pdf/2506.17828", "abs": "https://arxiv.org/abs/2506.17828", "authors": ["Xinnan Zhang", "Chenliang Li", "Siliang Zeng", "Jiaxiang Li", "Zhongruo Wang", "Kaixiang Lin", "Songtao Lu", "Alfredo Garcia", "Mingyi Hong"], "title": "Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Aligning large language models (LLMs) with human preferences usually requires\nfine-tuning methods such as RLHF and DPO. These methods directly optimize the\nmodel parameters, so they cannot be used in test-time to improve model\nperformance, nor are they applicable when the model weights are not accessible.\nIn contrast, test-time methods sidestep weight updates by leveraging reward\nfunctions to guide and improve output quality. However, they incur high\ninference costs, and their one-shot guidance is often based on imperfect reward\nor value functions, leading to suboptimal outputs. In this work, we present a\nmethod named Iterative Reweight-then-Optimize (IRO), a reinforcement learning\n(RL) framework that performs RL-style alignment of the (frozen) base model\nwithout touching its parameters. During training, each iteration (i) samples\ncandidates from the base model, (ii) resamples using current value functions,\nand (iii) trains a new lightweight value function that guides the next decoding\npass. At test time, the value functions are used to guide the base model\ngeneration via a search-based optimization process. Notably, users can apply\nIRO to align a model on their own dataset, similar to OpenAI's reinforcement\nfine-tuning (RFT), but without requiring access to the model weights.", "AI": {"tldr": "\u63d0\u51faIRO\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u4ef7\u503c\u51fd\u6570\u5b9e\u73b0\u65e0\u9700\u8c03\u6574\u6a21\u578b\u53c2\u6570\u7684\u5927\u6a21\u578b\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u9700\u6a21\u578b\u6743\u91cd\u8bbf\u95ee\u6743\u9650\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709RLHF/DPO\u65b9\u6cd5\u4f9d\u8d56\u6a21\u578b\u53c2\u6570\u8c03\u6574\uff0c\u65e0\u6cd5\u5728\u6d4b\u8bd5\u65f6\u4f7f\u7528\u4e14\u4e0d\u9002\u7528\u4e8e\u9ed1\u76d2\u6a21\u578b\uff1b\u4f20\u7edf\u6d4b\u8bd5\u65f6\u65b9\u6cd5\u5b58\u5728\u9ad8\u63a8\u7406\u6210\u672c\u548c\u5956\u52b1\u51fd\u6570\u4e0d\u5b8c\u5584\u7684\u95ee\u9898\u3002", "method": "\u5206\u8bad\u7ec3\u9636\u6bb5\uff08\u91c7\u6837-\u91cd\u91c7\u6837-\u8bad\u7ec3\u4ef7\u503c\u51fd\u6570\uff09\u548c\u6d4b\u8bd5\u9636\u6bb5\uff08\u57fa\u4e8e\u4ef7\u503c\u51fd\u6570\u7684\u641c\u7d22\u4f18\u5316\uff09\uff0c\u901a\u8fc7\u51bb\u7ed3\u57fa\u7840\u6a21\u578b\u53c2\u6570+\u5916\u90e8\u4ef7\u503c\u51fd\u6570\u8fed\u4ee3\u66f4\u65b0\u7684\u65b9\u5f0f\u5b9e\u73b0\u5bf9\u9f50\u3002", "result": "\u5b9e\u73b0\u53c2\u6570\u51bb\u7ed3\u4e0b\u7684\u6a21\u578b\u5bf9\u9f50\uff0c\u7528\u6237\u53ef\u5728\u81ea\u6709\u6570\u636e\u96c6\u4e0a\u5e94\u7528\u7c7b\u4f3cRFT\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6848\uff0c\u65e0\u9700\u6a21\u578b\u8bbf\u95ee\u6743\u9650\u5373\u53ef\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "\u7a81\u7834\u6a21\u578b\u6743\u91cd\u8bbf\u95ee\u9650\u5236\uff0c\u4e3a\u9ed1\u76d2\u6a21\u578b\u5b9a\u5236\u5316\u5bf9\u9f50\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u5728\u4fdd\u6301\u57fa\u7840\u6a21\u578b\u5b8c\u6574\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u504f\u597d\u4f18\u5316\u3002"}}
{"id": "2506.17930", "pdf": "https://arxiv.org/pdf/2506.17930", "abs": "https://arxiv.org/abs/2506.17930", "authors": ["Jianyu Wang", "Zhiqiang Hu", "Lidong Bing"], "title": "Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.NE", "cs.RO"], "comment": "ICML 2025, and Code will be released at:\n  https://github.com/jianyu-cs/PromptQuine/", "summary": "We propose a novel prompt design paradigm that challenges conventional wisdom\nin large language model (LLM) prompting. While conventional wisdom prioritizes\nwell-crafted instructions and demonstrations for in-context learning (ICL), we\nshow that pruning random demonstrations into seemingly incoherent \"gibberish\"\ncan remarkably improve performance across diverse tasks. Notably, the\n\"gibberish\" always matches or surpasses state-of-the-art automatic prompt\noptimization techniques, achieving substantial gains regardless of LLM\nalignment. Nevertheless, discovering an effective pruning strategy is\nnon-trivial, as existing attribution methods and prompt compression algorithms\nfail to deliver robust results, let alone human intuition. In terms of this, we\npropose a self-discover prompt optimization framework, PromptQuine, an\nevolutionary search framework that automatically searches for the pruning\nstrategy by itself using only low-data regimes. Much like the emergent\ncomplexity in nature--such as symbiosis and self-organization--arising in\nresponse to resource constraints, our framework evolves and refines\nunconventional yet highly effective prompts by leveraging only the tokens\npresent within the context. We demonstrate its effectiveness across\nclassification, multi-choice question answering, generation and math reasoning\ntasks across LLMs, while achieving decent runtime efficiency. We hope our\nfindings can guide mechanistic studies on in-context learning, and provide a\ncall to action, to pave the way for more open-ended search algorithms for more\neffective LLM prompting.", "AI": {"tldr": "\u63d0\u51faPromptQuine\u6846\u67b6\uff0c\u901a\u8fc7\u8fdb\u5316\u641c\u7d22\u81ea\u52a8\u751f\u6210\u770b\u4f3c\u6df7\u4e71\u4f46\u9ad8\u6548\u7684\u63d0\u793a\u8bcd\uff0c\u8d85\u8d8a\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u5e76\u5728\u591a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "\u4f20\u7edf\u63d0\u793a\u5de5\u7a0b\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u6307\u4ee4\u548c\u793a\u4f8b\uff0c\u4f46\u53d1\u73b0\u968f\u673a\u4fee\u526a\u7684'\u4e71\u7801'\u63d0\u793a\u6548\u679c\u66f4\u4f18\uff0c\u9700\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u81ea\u52a8\u751f\u6210\u6709\u6548\u63d0\u793a\u7684\u95ee\u9898", "method": "\u57fa\u4e8e\u8fdb\u5316\u7b97\u6cd5\u6784\u5efaPromptQuine\u6846\u67b6\uff0c\u4ec5\u5229\u7528\u4e0a\u4e0b\u6587\u4e2d\u7684token\u81ea\u52a8\u641c\u7d22\u6700\u4f18\u63d0\u793a\u4fee\u526a\u7b56\u7565\uff0c\u6a21\u62df\u81ea\u7136\u754c\u7684\u81ea\u7ec4\u7ec7\u73b0\u8c61", "result": "\u5728\u5206\u7c7b/\u95ee\u7b54/\u6570\u5b66\u63a8\u7406\u7b49\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4SOTA\u63d0\u793a\u4f18\u5316\u6280\u672f\u63d0\u5347\u660e\u663e\uff0c\u4e14\u4fdd\u6301\u8f83\u9ad8\u8fd0\u884c\u6548\u7387\uff08\u4e0d\u540cLLM\u6a21\u578b\u5747\u6709\u6548\uff09", "conclusion": "\u7a81\u7834\u4f20\u7edf\u63d0\u793a\u8bbe\u8ba1\u8303\u5f0f\uff0c\u4e3aLLM\u673a\u5236\u7814\u7a76\u63d0\u4f9b\u65b0\u65b9\u5411\uff0c\u547c\u5401\u5f00\u53d1\u66f4\u5f00\u653e\u7684\u641c\u7d22\u7b97\u6cd5\u63d0\u5347\u63d0\u793a\u5de5\u7a0b\u6548\u679c"}}
{"id": "2506.17942", "pdf": "https://arxiv.org/pdf/2506.17942", "abs": "https://arxiv.org/abs/2506.17942", "authors": ["Marco Cognetta", "Cyril Allauzen"], "title": "Tutorial: $\\varphi$-Transductions in OpenFst via the Gallic Semiring", "categories": ["cs.FL", "cs.CL"], "comment": "8 pages, 2 figures, code included", "summary": "OpenFst, a popular finite-state transducer library, supports\n$\\varphi$-transitions but, due to an implementation constraint, they cannot be\nused with transducers in a straightforward way.\n  In this short tutorial, we describe how one can use other functionality\nprovided by OpenFst (namely, the Gallic semiring) to correctly implement\n$\\varphi$-transductions and demonstrate it by implementing the MaxMatch\n(WordPiece) tokenization algorithm (Devlin et al., 2019; Song et al., 2021).\nAccompanying self-contained code examples are provided.\nhttps://www.openfst.org/twiki/pub/Contrib/FstContrib/phi_transduction_tutorial_code.tgz", "AI": {"tldr": "\u63d0\u51fa\u4f7f\u7528OpenFst\u7684Gallic\u534a\u73af\u529f\u80fd\u5b9e\u73b0\u03c6-\u8f6c\u6362\uff0c\u4ee5MaxMatch\u5206\u8bcd\u7b97\u6cd5\u4e3a\u4f8b\u9a8c\u8bc1\u65b9\u6848\u53ef\u884c\u6027", "motivation": "\u89e3\u51b3OpenFst\u5e93\u56e0\u5b9e\u73b0\u9650\u5236\u65e0\u6cd5\u76f4\u63a5\u4f7f\u7528\u03c6-\u8f6c\u6362\u7684\u95ee\u9898\uff0c\u6269\u5c55\u5176\u5b9e\u9645\u5e94\u7528\u573a\u666f", "method": "\u5229\u7528OpenFst\u73b0\u6709\u529f\u80fd\u4e2d\u7684Gallic\u534a\u73af\u7279\u6027\uff0c\u6784\u5efa\u6b63\u786e\u7684\u03c6-\u8f6c\u6362\u5b9e\u73b0\u6846\u67b6", "result": "\u6210\u529f\u5b9e\u73b0MaxMatch(WordPiece)\u5206\u8bcd\u7b97\u6cd5\u7684\u03c6-\u8f6c\u6362\u6f14\u793a\uff0c\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u5b8c\u6574\u4ee3\u7801\u6848\u4f8b", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7ed5\u8fc7OpenFst\u7684\u03c6-\u8f6c\u6362\u9650\u5236\uff0c\u4e3a\u6709\u9650\u72b6\u6001\u8f6c\u6362\u5668\u7684\u5e94\u7528\u5f00\u53d1\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2506.18023", "pdf": "https://arxiv.org/pdf/2506.18023", "abs": "https://arxiv.org/abs/2506.18023", "authors": ["Kui Huang", "Xinrong Chen", "Wenyu Lv", "Jincheng Liao", "Guanzhong Wang", "Yi Liu"], "title": "PP-DocBee2: Improved Baselines with Efficient Data for Multimodal Document Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "This report introduces PP-DocBee2, an advanced version of the PP-DocBee,\ndesigned to enhance multimodal document understanding. Built on a large\nmultimodal model architecture, PP-DocBee2 addresses the limitations of its\npredecessor through key technological improvements, including enhanced\nsynthetic data quality, improved visual feature fusion strategy, and optimized\ninference methodologies. These enhancements yield an $11.4\\%$ performance boost\non internal benchmarks for Chinese business documents, and reduce inference\nlatency by $73.0\\%$ to the vanilla version. A key innovation of our work is a\ndata quality optimization strategy for multimodal document tasks. By employing\na large-scale multimodal pre-trained model to evaluate data, we apply a novel\nstatistical criterion to filter outliers, ensuring high-quality training data.\nInspired by insights into underutilized intermediate features in multimodal\nmodels, we enhance the ViT representational capacity by decomposing it into\nlayers and applying a novel feature fusion strategy to improve complex\nreasoning. The source code and pre-trained model are available at\n\\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.", "AI": {"tldr": "PP-DocBee2\u901a\u8fc7\u6570\u636e\u8d28\u91cf\u4f18\u5316\u3001\u89c6\u89c9\u7279\u5f81\u878d\u5408\u7b56\u7565\u548c\u63a8\u7406\u52a0\u901f\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u6587\u6863\u7406\u89e3\u6027\u80fd", "motivation": "\u89e3\u51b3\u521d\u7248PP-DocBee\u5728\u6570\u636e\u8d28\u91cf\u3001\u7279\u5f81\u5229\u7528\u548c\u63a8\u7406\u6548\u7387\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u4e2d\u6587\u5546\u4e1a\u6587\u6863\u5904\u7406\u80fd\u529b", "method": "1. \u91c7\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u8bc4\u4f30\u6570\u636e\u8d28\u91cf\uff0c\u7edf\u8ba1\u8fc7\u6ee4\u5f02\u5e38\u503c\n2. \u5206\u89e3ViT\u5c42\u7ea7\u5e76\u8bbe\u8ba1\u65b0\u7279\u5f81\u878d\u5408\u7b56\u7565\n3. \u4f18\u5316\u63a8\u7406\u52a0\u901f\u65b9\u6cd5", "result": "\u4e2d\u6587\u6587\u6863\u4efb\u52a1\u6027\u80fd\u63d0\u534711.4%\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e73%", "conclusion": "\u6570\u636e\u8d28\u91cf\u4f18\u5316\u548c\u5206\u5c42\u7279\u5f81\u878d\u5408\u7b56\u7565\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7684\u6587\u6863\u7406\u89e3\u80fd\u529b\u4e0e\u63a8\u7406\u6548\u7387\uff0c\u6280\u672f\u65b9\u6848\u5df2\u5f00\u6e90"}}
{"id": "2506.18045", "pdf": "https://arxiv.org/pdf/2506.18045", "abs": "https://arxiv.org/abs/2506.18045", "authors": ["I. Loaiza", "R. Vestrelli", "A. Fronzetti Colladon", "R. Rigobon"], "title": "The Democratic Paradox in Large Language Models' Underestimation of Press Freedom", "categories": ["cs.CY", "cs.AI", "cs.CL", "K.4; I.2.7; I.2.0"], "comment": null, "summary": "As Large Language Models (LLMs) increasingly mediate global information\naccess for millions of users worldwide, their alignment and biases have the\npotential to shape public understanding and trust in fundamental democratic\ninstitutions, such as press freedom. In this study, we uncover three systematic\ndistortions in the way six popular LLMs evaluate press freedom in 180 countries\ncompared to expert assessments of the World Press Freedom Index (WPFI). The six\nLLMs exhibit a negative misalignment, consistently underestimating press\nfreedom, with individual models rating between 71% to 93% of countries as less\nfree. We also identify a paradoxical pattern we term differential misalignment:\nLLMs disproportionately underestimate press freedom in countries where it is\nstrongest. Additionally, five of the six LLMs exhibit positive home bias,\nrating their home countries' press freedoms more favorably than would be\nexpected given their negative misalignment with the human benchmark. In some\ncases, LLMs rate their home countries between 7% to 260% more positively than\nexpected. If LLMs are set to become the next search engines and some of the\nmost important cultural tools of our time, they must ensure accurate\nrepresentations of the state of our human and civic rights globally.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u516d\u79cd\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bc4\u4f30\u5168\u7403\u65b0\u95fb\u81ea\u7531\u65f6\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\uff1a\u666e\u904d\u4f4e\u4f30\u65b0\u95fb\u81ea\u7531\uff0871%-93%\u56fd\u5bb6\uff09\uff0c\u5728\u65b0\u95fb\u81ea\u7531\u8f83\u5f3a\u56fd\u5bb6\u504f\u5dee\u66f4\u5927\uff08\u5dee\u5f02\u9519\u4f4d\uff09\uff0c\u4e94\u6b3e\u6a21\u578b\u5bf9\u6bcd\u56fd\u5b58\u57287%-260%\u7684\u79ef\u6781\u504f\u89c1\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u65b0\u5174\u4fe1\u606f\u4e2d\u4ecb\u53ef\u80fd\u5f71\u54cd\u516c\u4f17\u5bf9\u6c11\u4e3b\u5236\u5ea6\u7684\u8ba4\u77e5\uff0c\u9700\u9a8c\u8bc1\u5176\u5bf9\u65b0\u95fb\u81ea\u7531\u8fd9\u7c7b\u57fa\u7840\u6c11\u4e3b\u6743\u5229\u7684\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "method": "\u5bf9\u6bd4\u516d\u4e2aLLM\u5bf9180\u56fd\u65b0\u95fb\u81ea\u7531\u8bc4\u4f30\u4e0e\u4e16\u754c\u65b0\u95fb\u81ea\u7531\u6307\u6570\uff08WPFI\uff09\u4e13\u5bb6\u8bc4\u4f30\uff0c\u91cf\u5316\u8d1f\u5411\u9519\u4f4d\u3001\u5dee\u5f02\u9519\u4f4d\u548c\u6bcd\u56fd\u504f\u89c1\u4e09\u79cd\u504f\u5dee\u6a21\u5f0f\u3002", "result": "1. \u6240\u6709\u6a21\u578b\u5b58\u5728\u8d1f\u5411\u9519\u4f4d\uff08\u5e73\u5747\u4f4e\u4f30\uff09\n2. \u65b0\u95fb\u81ea\u7531\u8d8a\u5f3a\u56fd\u5bb6\u9519\u4f4d\u8d8a\u663e\u8457\n3. 83%\u6a21\u578b\u5bf9\u6bcd\u56fd\u8bc4\u4f30\u5b58\u5728\u79ef\u6781\u504f\u5dee\n4. \u4e2a\u522b\u6a21\u578b\u6bcd\u56fd\u8bc4\u5206\u6bd4\u9884\u671f\u9ad8260%", "conclusion": "LLM\u4f5c\u4e3a\u672a\u6765\u6838\u5fc3\u6587\u5316\u5de5\u5177\uff0c\u5fc5\u987b\u89e3\u51b3\u7cfb\u7edf\u6027\u8bc4\u4f30\u504f\u5dee\u95ee\u9898\uff0c\u786e\u4fdd\u5168\u7403\u516c\u6c11\u6743\u5229\u72b6\u6001\u7684\u51c6\u786e\u5448\u73b0\uff0c\u8fd9\u5bf9\u7ef4\u62a4\u6570\u5b57\u65f6\u4ee3\u7684\u6c11\u4e3b\u8ba4\u77e5\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2506.18088", "pdf": "https://arxiv.org/pdf/2506.18088", "abs": "https://arxiv.org/abs/2506.18088", "authors": ["Tianxing Chen", "Zanxin Chen", "Baijun Chen", "Zijian Cai", "Yibin Liu", "Qiwei Liang", "Zixuan Li", "Xianliang Lin", "Yiheng Ge", "Zhenyu Gu", "Weiliang Deng", "Yubin Guo", "Tian Nian", "Xuanbing Xie", "Qiangyu Chen", "Kailun Su", "Tianling Xu", "Guodong Liu", "Mengkang Hu", "Huan-ang Gao", "Kaixuan Wang", "Zhixuan Liang", "Yusen Qin", "Xiaokang Yang", "Ping Luo", "Yao Mu"], "title": "RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV", "cs.MA"], "comment": "Project Page: https://robotwin-platform.github.io/", "summary": "Simulation-based data synthesis has emerged as a powerful paradigm for\nenhancing real-world robotic manipulation. However, existing synthetic datasets\nremain insufficient for robust bimanual manipulation due to two challenges: (1)\nthe lack of an efficient, scalable data generation method for novel tasks, and\n(2) oversimplified simulation environments that fail to capture real-world\ncomplexity. We present RoboTwin 2.0, a scalable simulation framework that\nenables automated, large-scale generation of diverse and realistic data, along\nwith unified evaluation protocols for dual-arm manipulation. We first construct\nRoboTwin-OD, a large-scale object library comprising 731 instances across 147\ncategories, each annotated with semantic and manipulation-relevant labels.\nBuilding on this foundation, we develop an expert data synthesis pipeline that\ncombines multimodal large language models (MLLMs) with simulation-in-the-loop\nrefinement to generate task-level execution code automatically. To improve\nsim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization\nalong five axes: clutter, lighting, background, tabletop height and language\ninstructions, thereby enhancing data diversity and policy robustness. We\ninstantiate this framework across 50 dual-arm tasks spanning five robot\nembodiments, and pre-collect over 100,000 domain-randomized expert\ntrajectories. Empirical results show a 10.9% gain in code generation success\nand improved generalization to novel real-world scenarios. A VLA model\nfine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%)\non unseen scene real-world tasks, while zero-shot models trained solely on our\nsynthetic data achieve a 228% relative gain, highlighting strong generalization\nwithout real-world supervision. We release the data generator, benchmark,\ndataset, and code to support scalable research in robust bimanual manipulation.", "AI": {"tldr": "RoboTwin 2.0\u63d0\u51fa\u81ea\u52a8\u5316\u4eff\u771f\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5927\u6a21\u578b\u548c\u4e94\u8f74\u57df\u968f\u673a\u5316\u6280\u672f\uff0c\u89e3\u51b3\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u6570\u636e\u4e0d\u8db3\u548c\u4eff\u771f\u7b80\u5316\u95ee\u9898\uff0c\u5b9e\u73b0\u4ee3\u7801\u751f\u6210\u6210\u529f\u7387\u63d0\u534710.9%\u5e76\u5728\u771f\u5b9e\u573a\u666f\u5c55\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709\u5408\u6210\u6570\u636e\u96c6\u5728\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5b58\u5728\uff1a(1) \u7f3a\u4e4f\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u65b0\u4efb\u52a1\u6570\u636e\u751f\u6210\u65b9\u6cd5 (2) \u4eff\u771f\u73af\u5883\u8fc7\u5ea6\u7b80\u5316\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u4e16\u754c\u590d\u6742\u6027\u7684\u53cc\u91cd\u6311\u6218", "method": "\u6784\u5efaRoboTwin-OD\u5927\u89c4\u6a21\u7269\u4f53\u5e93\uff08731\u5b9e\u4f8b/147\u7c7b\uff09\uff0c\u7ed3\u5408MLLM\u4e0e\u4eff\u771f\u95ed\u73af\u4f18\u5316\u7684\u4e13\u5bb6\u6570\u636e\u751f\u6210\u7ba1\u9053\uff0c\u901a\u8fc7\u4e94\u8f74\u7ed3\u6784\u5316\u57df\u968f\u673a\u5316\uff08\u6742\u7269/\u5149\u7167/\u80cc\u666f/\u684c\u9762\u9ad8\u5ea6/\u8bed\u8a00\u6307\u4ee4\uff09\u63d0\u5347\u6570\u636e\u591a\u6837\u6027", "result": "\u572850\u4e2a\u53cc\u624b\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u9884\u91c7\u96c6\u8d8510\u4e07\u6761\u57df\u968f\u673a\u5316\u4e13\u5bb6\u8f68\u8ff9\uff0cVLA\u6a21\u578b\u5fae\u8c03\u540e\u672a\u89c1\u8fc7\u573a\u666f\u4efb\u52a1\u63d0\u5347367%\uff0842.0% vs 9.0%\uff09\uff0c\u96f6\u6837\u672c\u6a21\u578b\u4ec5\u7528\u5408\u6210\u6570\u636e\u5b9e\u73b0228%\u76f8\u5bf9\u589e\u76ca", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u9ad8\u6548\u6570\u636e\u751f\u6210\u673a\u5236\u548c\u589e\u5f3a\u7684\u4eff\u771f\u771f\u5b9e\u6027\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5f00\u653e\u7684\u6570\u636e\u751f\u6210\u5668\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u4ee3\u7801\u5e93\u5c06\u63a8\u52a8\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u89c4\u6a21\u5316\u7814\u7a76"}}
{"id": "2506.18135", "pdf": "https://arxiv.org/pdf/2506.18135", "abs": "https://arxiv.org/abs/2506.18135", "authors": ["Zijun Chen", "Zhanpeng Zhou", "Bo Zhang", "Weinan Zhang", "Xi Sun", "Junchi Yan"], "title": "SE-Merging: A Self-Enhanced Approach for Dynamic Model Merging", "categories": ["cs.AI", "cs.CL"], "comment": "preprint, accepted at IJCNN2025", "summary": "Model merging has gained increasing attention due to its intriguing property:\ninterpolating the parameters of different task-specific fine-tuned models leads\nto multi-task abilities. However, despite its empirical success, the underlying\nmechanisms of model merging remain poorly understood. In this work, we delve\ninto the mechanism behind model merging from a representation perspective. Our\nanalysis reveals that model merging achieves multi-task abilities through two\nkey capabilities: i) distinguishing samples from different tasks, and ii)\nadapting to the corresponding expert model for each sample. These two\ncapabilities allow the merged model to retain task-specific expertise, enabling\nefficient multi-task adaptation. Building on these insights, we propose\n\\texttt{SE-Merging}, a self-enhanced model merging framework that leverages\nthese two characteristics to dynamically identify the corresponding task for\neach sample and then adaptively rescales the merging coefficients to further\nenhance task-specific expertise in the merged model. Notably,\n\\texttt{SE-Merging} achieves dynamic model merging without additional training.\nExtensive experiments demonstrate that \\texttt{SE-Merging} achieves significant\nperformance improvements while remaining compatible with existing model merging\ntechniques.", "AI": {"tldr": "\u63ed\u793a\u4e86\u6a21\u578b\u5408\u5e76\u901a\u8fc7\u4efb\u52a1\u6837\u672c\u533a\u5206\u548c\u4e13\u5bb6\u6a21\u578b\u9002\u914d\u5b9e\u73b0\u591a\u4efb\u52a1\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684SE-Merging\u52a8\u6001\u589e\u5f3a\u6846\u67b6", "motivation": "\u73b0\u6709\u6a21\u578b\u5408\u5e76\u65b9\u6cd5\u867d\u80fd\u5b9e\u73b0\u591a\u4efb\u52a1\u9002\u5e94\uff0c\u4f46\u5176\u5de5\u4f5c\u673a\u5236\u7f3a\u4e4f\u7406\u8bba\u89e3\u91ca\uff0c\u963b\u788d\u4e86\u8fdb\u4e00\u6b65\u4f18\u5316", "method": "\u63d0\u51faSE-Merging\u6846\u67b6\uff1a\u52a8\u6001\u8bc6\u522b\u6837\u672c\u4efb\u52a1\u5f52\u5c5e\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u6a21\u578b\u5408\u5e76\u7cfb\u6570\u5f3a\u5316\u4e13\u5bb6\u6a21\u578b\u7279\u6027", "result": "SE-Merging\u663e\u8457\u63d0\u5347\u591a\u4efb\u52a1\u6027\u80fd\uff08\u5b9e\u9a8c\u9a8c\u8bc1\uff09\uff0c\u4e14\u5b8c\u5168\u517c\u5bb9\u73b0\u6709\u6a21\u578b\u5408\u5e76\u6280\u672f", "conclusion": "\u6a21\u578b\u5408\u5e76\u7684\u672c\u8d28\u80fd\u529b\u662f\u4efb\u52a1\u533a\u5206\u4e0e\u4e13\u5bb6\u9002\u914d\uff0c\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u7684\u52a8\u6001\u5408\u5e76\u6846\u67b6\u7a81\u7834\u4e86\u4f20\u7edf\u9759\u6001\u5408\u5e76\u7684\u9650\u5236"}}
{"id": "2506.18183", "pdf": "https://arxiv.org/pdf/2506.18183", "abs": "https://arxiv.org/abs/2506.18183", "authors": ["Zhiting Mei", "Christina Zhang", "Tenny Yin", "Justin Lidard", "Ola Shorinwa", "Anirudha Majumdar"], "title": "Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning language models have set state-of-the-art (SOTA) records on many\nchallenging benchmarks, enabled by multi-step reasoning induced using\nreinforcement learning. However, like previous language models, reasoning\nmodels are prone to generating confident, plausible responses that are\nincorrect (hallucinations). Knowing when and how much to trust these models is\ncritical to the safe deployment of reasoning models in real-world applications.\nTo this end, we explore uncertainty quantification of reasoning models in this\nwork. Specifically, we ask three fundamental questions: First, are reasoning\nmodels well-calibrated? Second, does deeper reasoning improve model\ncalibration? Finally, inspired by humans' innate ability to double-check their\nthought processes to verify the validity of their answers and their confidence,\nwe ask: can reasoning models improve their calibration by explicitly reasoning\nabout their chain-of-thought traces? We introduce introspective uncertainty\nquantification (UQ) to explore this direction. In extensive evaluations on SOTA\nreasoning models across a broad range of benchmarks, we find that reasoning\nmodels: (i) are typically overconfident, with self-verbalized confidence\nestimates often greater than 85% particularly for incorrect responses, (ii)\nbecome even more overconfident with deeper reasoning, and (iii) can become\nbetter calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not\nuniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we\nconclude with important research directions to design necessary UQ benchmarks\nand improve the calibration of reasoning models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u81ea\u7701\u5f0f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u63ed\u793a\u63a8\u7406\u8bed\u8a00\u6a21\u578b\u666e\u904d\u5b58\u5728\u8fc7\u5ea6\u81ea\u4fe1\u73b0\u8c61\uff0c\u5e76\u63a2\u8ba8\u6df1\u5ea6\u63a8\u7406\u4e0e\u6a21\u578b\u6821\u51c6\u7684\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u6a21\u578b\u867d\u7136\u6027\u80fd\u4f18\u5f02\u4f46\u5b58\u5728\u8fc7\u5ea6\u81ea\u4fe1\u5e7b\u89c9\u95ee\u9898\uff0c\u9700\u8981\u5efa\u7acb\u53ef\u4fe1\u8d56\u7684\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u4f53\u7cfb\u4ee5\u5b9e\u73b0\u5b89\u5168\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u81ea\u7701\u5f0f\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff08introspective UQ\uff09\u65b9\u6cd5\uff0c\u5728\u591a\u4e2aSOTA\u63a8\u7406\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7cfb\u7edf\u8bc4\u4f30\u6821\u51c6\u6027\u3001\u63a8\u7406\u6df1\u5ea6\u4e0e\u81ea\u7701\u9a8c\u8bc1\u7684\u5173\u7cfb\u3002", "result": "\u53d1\u73b0\uff1a(1)\u6a21\u578b\u666e\u904d\u8fc7\u5ea6\u81ea\u4fe1\uff08\u9519\u8bef\u7b54\u6848\u7f6e\u4fe1\u5ea6>85%\uff09\uff1b(2)\u6df1\u5ea6\u63a8\u7406\u52a0\u5267\u8fc7\u5ea6\u81ea\u4fe1\uff1b(3)\u81ea\u7701\u9a8c\u8bc1\u53ef\u6539\u5584\u90e8\u5206\u6a21\u578b\uff08\u5982o3-Mini\uff09\u4f46\u975e\u5168\u90e8\uff08\u5982Claude 3.7\uff09\u7684\u6821\u51c6\u6027\u3002", "conclusion": "\u9700\u5efa\u7acb\u66f4\u5b8c\u5584\u7684UQ\u8bc4\u4f30\u57fa\u51c6\uff0c\u5f00\u53d1\u65b0\u578b\u6821\u51c6\u65b9\u6cd5\uff0c\u5c24\u5176\u9700\u8981\u5173\u6ce8\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u5bf9\u81ea\u7701\u9a8c\u8bc1\u7684\u5dee\u5f02\u5316\u54cd\u5e94\u3002"}}
{"id": "2506.18203", "pdf": "https://arxiv.org/pdf/2506.18203", "abs": "https://arxiv.org/abs/2506.18203", "authors": ["Jon Saad-Falcon", "E. Kelly Buchanan", "Mayee F. Chen", "Tzu-Heng Huang", "Brendan McLaughlin", "Tanvir Bhathal", "Shang Zhu", "Ben Athiwaratkun", "Frederic Sala", "Scott Linderman", "Azalia Mirhoseini", "Christopher R\u00e9"], "title": "Shrinking the Generation-Verification Gap with Weak Verifiers", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Verifiers can improve language model capabilities by scoring and ranking\nresponses from generated candidates. Currently, high-quality verifiers are\neither unscalable (e.g., humans) or limited in utility (e.g., tools like Lean).\nWhile LM judges and reward models have become broadly useful as general-purpose\nverifiers, a significant performance gap remains between them and oracle\nverifiers (verifiers with perfect accuracy). To help close this gap, we\nintroduce Weaver, a framework for designing a strong verifier by combining\nmultiple weak, imperfect verifiers. We find weighted ensembles of verifiers,\nwhich typically require learning from labeled data, significantly outperform\nunweighted combinations due to differences in verifier accuracies. To reduce\ndependency on labeled data, Weaver leverages weak supervision to estimate each\nverifier's accuracy and combines outputs into a unified score that better\nreflects true response quality. However, directly applying weak supervision\nalgorithms poses challenges, including inconsistent verifier output formats and\nhandling low-quality verifiers. Weaver addresses these using dataset statistics\nto normalize outputs and filter specific verifiers. We study Weaver's\neffectiveness in test-time repeated sampling, where a model generates multiple\ncandidate responses and selects one. Our evaluations show Weaver significantly\nimproves over Pass@1-performance when selecting the first candidate-across\nreasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B\nInstruct as generator, and an ensemble of 70B or smaller judge and reward\nmodels as verifiers (87.7% average). This gain mirrors the jump between GPT-4o\nand o3-mini (69.0% vs. 86.7%), which required extensive finetuning and\npost-training. To reduce computational costs of verifier ensembles, we train a\n400M cross-encoder using Weaver's combined output scores.", "AI": {"tldr": "Weaver\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u591a\u4e2a\u5f31\u9a8c\u8bc1\u5668\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u8bc4\u5206\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5185\u5bb9\u7684\u9009\u62e9\u51c6\u786e\u7387\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u9a8c\u8bc1\u5668\u5b58\u5728\u4e0d\u53ef\u6269\u5c55\u6027\uff08\u4eba\u7c7b\u9a8c\u8bc1\uff09\u6216\u529f\u80fd\u5c40\u9650\u6027\uff08\u5de5\u5177\u578b\u9a8c\u8bc1\uff09\uff0c\u4e0e\u7406\u60f3\u9a8c\u8bc1\u5668\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\u3002\u9700\u8981\u7ed3\u5408\u591a\u4e2a\u5f31\u9a8c\u8bc1\u5668\u7684\u4f18\u52bf\u6765\u5f25\u8865\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "1. \u4f7f\u7528\u52a0\u6743\u96c6\u6210\u65b9\u6cd5\u7ec4\u5408\u9a8c\u8bc1\u5668\n2. \u901a\u8fc7\u5f31\u76d1\u7763\u4f30\u7b97\u9a8c\u8bc1\u5668\u51c6\u786e\u7387\n3. \u6570\u636e\u6807\u51c6\u5316\u7edf\u4e00\u8f93\u51fa\u683c\u5f0f\n4. \u57fa\u4e8e\u7edf\u8ba1\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u9a8c\u8bc1\u5668", "result": "\u5b9e\u9a8c\u663e\u793a\uff1a\n- \u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u8fbe87.7%\n- \u6027\u80fd\u63d0\u5347\u76f8\u5f53\u4e8eGPT-4o\u5230o3-mini\u7684\u98de\u8dc3\n- \u8bad\u7ec3\u7684400M\u8de8\u7f16\u7801\u5668\u964d\u4f4e80%\u8ba1\u7b97\u6210\u672c", "conclusion": "Weaver\u6846\u67b6\u6709\u6548\u7f29\u5c0f\u4e86\u666e\u901a\u9a8c\u8bc1\u5668\u4e0e\u7406\u60f3\u9a8c\u8bc1\u5668\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u901a\u8fc7\u96c6\u6210\u7b56\u7565\u5b9e\u73b0\u4e86\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\u7684\u8d28\u91cf\u63d0\u5347\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.18237", "pdf": "https://arxiv.org/pdf/2506.18237", "abs": "https://arxiv.org/abs/2506.18237", "authors": ["Xu Wan", "Wei Wang", "Wenyue Xu", "Wotao Yin", "Jie Song", "Mingyang Sun"], "title": "AdapThink: Adaptive Thinking Preferences for Reasoning Language Model", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement Learning (RL)-based post-training has significantly advanced\nthe complex reasoning capabilities of language models, fostering sophisticated\nself-reflection processes. However, this ``slow thinking'' paradigm presents a\ncritical challenge to reasoning efficiency: models may expend excessive\ncomputation on simple questions and shift reasoning prematurely for complex\nones. Previous mechanisms typically rely on static length budgets or predefined\nrules, lacking the adaptability for varying question complexities and models'\nevolving capabilities. To this end, we propose AdapThink, an adaptive\npost-training framework designed to induce more efficient thinking while\nmaintaining the performance of reasoning language models. Specifically,\nAdapThink incorporates two key mechanisms: 1) A group-relative reward function\nthat leverages model confidence and response's characteristic to dynamically\nadjust the preference of reflection-related transition words without resorting\nto a fixed length preference. 2) A diversity-aware sampling mechanism that\nbalances the training group's solution accuracy with reasoning diversity via an\nentropy-guided score. Experiments on several mathematical reasoning datasets\nwith DeepSeek-distilled models demonstrate AdapThink's advantages in enabling\nadaptive reasoning patterns and mitigating the inefficiencies.", "AI": {"tldr": "\u63d0\u51faAdapThink\u81ea\u9002\u5e94\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5956\u52b1\u51fd\u6570\u548c\u591a\u6837\u6027\u91c7\u6837\u673a\u5236\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6548\u7387\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u63a8\u7406\u6548\u7387\u95ee\u9898\uff1a\u7b80\u5355\u95ee\u9898\u8fc7\u5ea6\u8ba1\u7b97\uff0c\u590d\u6742\u95ee\u9898\u8fc7\u65e9\u5207\u6362\u7b56\u7565\uff0c\u9759\u6001\u8c03\u6574\u673a\u5236\u7f3a\u4e4f\u9002\u5e94\u6027\u3002", "method": "1. \u57fa\u4e8e\u6a21\u578b\u7f6e\u4fe1\u5ea6\u548c\u54cd\u5e94\u7279\u5f81\u7684\u52a8\u6001\u5956\u52b1\u51fd\u6570\uff1b2. \u901a\u8fc7\u71b5\u5f15\u5bfc\u5e73\u8861\u8bad\u7ec3\u7ec4\u51c6\u786e\u6027\u4e0e\u63a8\u7406\u591a\u6837\u6027\u7684\u91c7\u6837\u673a\u5236", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\uff08\u4f7f\u7528DeepSeek-distilled\u6a21\u578b\uff09\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u63a8\u7406\u6a21\u5f0f\u5e76\u7f13\u89e3\u4f4e\u6548\u95ee\u9898", "conclusion": "AdapThink\u6210\u529f\u5e73\u8861\u4e86\u63a8\u7406\u6548\u7387\u4e0e\u6027\u80fd\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u81ea\u9002\u5e94\u601d\u7ef4\u673a\u5236\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411"}}
{"id": "2506.18254", "pdf": "https://arxiv.org/pdf/2506.18254", "abs": "https://arxiv.org/abs/2506.18254", "authors": ["Tianyu Yu", "Bo Ji", "Shouli Wang", "Shu Yao", "Zefan Wang", "Ganqu Cui", "Lifan Yuan", "Ning Ding", "Yuan Yao", "Zhiyuan Liu", "Maosong Sun", "Tat-Seng Chua"], "title": "RLPR: Extrapolating RLVR to General Domains without Verifiers", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Project Website: https://github.com/openbmb/RLPR", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising\npotential in advancing the reasoning capabilities of LLMs. However, its success\nremains largely confined to mathematical and code domains. This primary\nlimitation stems from the heavy reliance on domain-specific verifiers, which\nresults in prohibitive complexity and limited scalability. To address the\nchallenge, our key observation is that LLM's intrinsic probability of\ngenerating a correct free-form answer directly indicates its own evaluation of\nthe reasoning reward (i.e., how well the reasoning process leads to the correct\nanswer). Building on this insight, we propose RLPR, a simple verifier-free\nframework that extrapolates RLVR to broader general domains. RLPR uses the\nLLM's own token probability scores for reference answers as the reward signal\nand maximizes the expected reward during training. We find that addressing the\nhigh variance of this noisy probability reward is crucial to make it work, and\npropose prob-to-reward and stabilizing methods to ensure a precise and stable\nreward from LLM intrinsic probabilities. Comprehensive experiments in four\ngeneral-domain benchmarks and three mathematical benchmarks show that RLPR\nconsistently improves reasoning capabilities in both areas for Gemma, Llama,\nand Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6\npoints on TheoremQA and 7.5 points on Minerva, and even surpasses strong\nverifier-model-dependent approaches General-Reasoner by 1.6 average points\nacross seven benchmarks.", "AI": {"tldr": "\u63d0\u51faRLPR\u6846\u67b6\uff0c\u901a\u8fc7LLM\u81ea\u8eab\u751f\u6210\u6b63\u786e\u7b54\u6848\u7684\u6982\u7387\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\uff0c\u7a81\u7834\u4f20\u7edfRLVR\u5bf9\u9886\u57df\u9a8c\u8bc1\u5668\u7684\u4f9d\u8d56\uff0c\u63d0\u5347\u901a\u7528\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfRLVR\u65b9\u6cd5\u53d7\u9650\u4e8e\u7279\u5b9a\u9886\u57df\u9a8c\u8bc1\u5668\uff0c\u5bfc\u81f4\u590d\u6742\u6027\u548c\u6269\u5c55\u6027\u4e0d\u8db3\u3002\u9700\u8981\u63a2\u7d22\u65e0\u9700\u9a8c\u8bc1\u5668\u7684\u901a\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002", "method": "1. \u4f7f\u7528LLM\u751f\u6210\u53c2\u8003\u7b54\u6848\u7684\u6982\u7387\u4f5c\u4e3a\u5185\u5728\u5956\u52b1\u4fe1\u53f7\n2. \u63d0\u51fa\u6982\u7387\u8f6c\u5956\u52b1\u673a\u5236\u548c\u8bad\u7ec3\u7a33\u5b9a\u5316\u65b9\u6cd5\n3. \u5728\u901a\u7528\u9886\u57df\u548c\u6570\u5b66\u9886\u57df\u8fdb\u884c\u591a\u57fa\u51c6\u6d4b\u8bd5", "result": "\u57287\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff1a\n- \u8d85\u8d8aVeriFree\uff08TheoremQA +7.6\uff0cMinerva +7.5\uff09\n- \u5e73\u5747\u4f18\u4e8eGeneral-Reasoner 1.6\u5206\n- \u9002\u914dGemma/Llama/Qwen\u7cfb\u5217\u6a21\u578b", "conclusion": "\u9a8c\u8bc1\u4e86LLM\u5185\u5728\u6982\u7387\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u7684\u6709\u6548\u6027\uff0c\u4e3a\u901a\u7528\u9886\u57df\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.18311", "pdf": "https://arxiv.org/pdf/2506.18311", "abs": "https://arxiv.org/abs/2506.18311", "authors": ["Hoang-An Trieu", "Dinh-Truong Do", "Chau Nguyen", "Vu Tran", "Minh Le Nguyen"], "title": "Enhancing Document Retrieval in COVID-19 Research: Leveraging Large Language Models for Hidden Relation Extraction", "categories": ["cs.IR", "cs.CL"], "comment": "In the Proceedings of SCIDOCA 2024", "summary": "In recent years, with the appearance of the COVID-19 pandemic, numerous\npublications relevant to this disease have been issued. Because of the massive\nvolume of publications, an efficient retrieval system is necessary to provide\nresearchers with useful information if an unexpected pandemic happens so\nsuddenly, like COVID-19. In this work, we present a method to help the\nretrieval system, the Covrelex-SE system, to provide more high-quality search\nresults. We exploited the power of the large language models (LLMs) to extract\nthe hidden relationships inside the unlabeled publication that cannot be found\nby the current parsing tools that the system is using. Since then, help the\nsystem to have more useful information during retrieval progress.", "AI": {"tldr": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6316\u6398\u6587\u732e\u9690\u85cf\u5173\u7cfb\uff0c\u63d0\u5347COVID-19\u8bba\u6587\u68c0\u7d22\u7cfb\u7edf\u6548\u679c", "motivation": "COVID-19\u75ab\u60c5\u5bfc\u81f4\u76f8\u5173\u8bba\u6587\u6fc0\u589e\uff0c\u73b0\u6709\u89e3\u6790\u5de5\u5177\u65e0\u6cd5\u6709\u6548\u6316\u6398\u6587\u732e\u95f4\u9690\u85cf\u5173\u8054\uff0c\u9700\u5f00\u53d1\u9ad8\u6548\u68c0\u7d22\u7cfb\u7edf\u5e94\u5bf9\u7a81\u53d1\u516c\u5171\u536b\u751f\u4e8b\u4ef6", "method": "\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u63d0\u53d6\u672a\u6807\u6ce8\u6587\u732e\u4e2d\u7684\u9690\u542b\u5173\u8054\u5173\u7cfb\uff0c\u5f25\u8865\u73b0\u6709\u7cfb\u7edf\u89e3\u6790\u5de5\u5177\u7684\u4e0d\u8db3", "result": "\u589e\u5f3aCovrelex-SE\u7cfb\u7edf\u5728\u68c0\u7d22\u8fc7\u7a0b\u4e2d\u7684\u4fe1\u606f\u5229\u7528\u80fd\u529b\uff0c\u83b7\u5f97\u66f4\u9ad8\u8d28\u91cf\u7684\u641c\u7d22\u7ed3\u679c", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7a81\u53d1\u75ab\u60c5\u4e0b\u7684\u79d1\u7814\u6587\u732e\u68c0\u7d22\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u5347\u77e5\u8bc6\u53d1\u73b0\u6548\u7387"}}
{"id": "2506.18316", "pdf": "https://arxiv.org/pdf/2506.18316", "abs": "https://arxiv.org/abs/2506.18316", "authors": ["Trieu An", "Long Nguyen", "Minh Le Nguyen"], "title": "Team LA at SCIDOCA shared task 2025: Citation Discovery via relation-based zero-shot retrieval", "categories": ["cs.IR", "cs.CL"], "comment": "In the Proceedings of SCIDOCA 2025", "summary": "The Citation Discovery Shared Task focuses on predicting the correct citation\nfrom a given candidate pool for a given paragraph. The main challenges stem\nfrom the length of the abstract paragraphs and the high similarity among\ncandidate abstracts, making it difficult to determine the exact paper to cite.\nTo address this, we develop a system that first retrieves the top-k most\nsimilar abstracts based on extracted relational features from the given\nparagraph. From this subset, we leverage a Large Language Model (LLM) to\naccurately identify the most relevant citation. We evaluate our framework on\nthe training dataset provided by the SCIDOCA 2025 organizers, demonstrating its\neffectiveness in citation prediction.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u5173\u7cfb\u7279\u5f81\u68c0\u7d22\u4e0eLLM\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u89e3\u51b3\u957f\u6587\u672c\u9ad8\u76f8\u4f3c\u573a\u666f\u4e0b\u7684\u7cbe\u51c6\u5f15\u6587\u9884\u6d4b\u95ee\u9898\u3002", "motivation": "\u9488\u5bf9\u5b66\u672f\u5f15\u7528\u4efb\u52a1\u4e2d\u6458\u8981\u6bb5\u843d\u8fc7\u957f\u3001\u5019\u9009\u8bba\u6587\u76f8\u4f3c\u5ea6\u9ad8\u5bfc\u81f4\u7684\u51c6\u786e\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "method": "1. \u57fa\u4e8e\u5173\u7cfb\u7279\u5f81\u68c0\u7d22top-k\u5019\u9009\u6458\u8981\n2. \u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u7cbe\u786e\u5f15\u6587\u5339\u914d", "result": "\u5728SCIDOCA 2025\u8bad\u7ec3\u96c6\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027", "conclusion": "\u8be5\u6df7\u5408\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u9ad8\u76f8\u4f3c\u6587\u672c\u73af\u5883\u4e0b\u7684\u5f15\u6587\u53d1\u73b0\u51c6\u786e\u7387"}}
{"id": "2506.18330", "pdf": "https://arxiv.org/pdf/2506.18330", "abs": "https://arxiv.org/abs/2506.18330", "authors": ["Lixin Wu", "Na Cai", "Qiao Cheng", "Jiachen Wang", "Yitao Duan"], "title": "Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We introduce Confucius3-Math, an open-source large language model with 14B\nparameters that (1) runs efficiently on a single consumer-grade GPU; (2)\nachieves SOTA performances on a range of mathematical reasoning tasks,\noutperforming many models with significantly larger sizes. In particular, as\npart of our mission to enhancing education and knowledge dissemination with AI,\nConfucius3-Math is specifically committed to mathematics learning for Chinese\nK-12 students and educators. Built via post-training with large-scale\nreinforcement learning (RL), Confucius3-Math aligns with national curriculum\nand excels at solving main-stream Chinese K-12 mathematical problems with low\ncost. In this report we share our development recipe, the challenges we\nencounter and the techniques we develop to overcome them. In particular, we\nintroduce three technical innovations: Targeted Entropy Regularization, Recent\nSample Recovery and Policy-Specific Hardness Weighting. These innovations\nencompass a new entropy regularization, a novel data scheduling policy, and an\nimproved group-relative advantage estimator. Collectively, they significantly\nstabilize the RL training, improve data efficiency, and boost performance. Our\nwork demonstrates the feasibility of building strong reasoning models in a\nparticular domain at low cost. We open-source our model and code at\nhttps://github.com/netease-youdao/Confucius3-Math.", "AI": {"tldr": "\u5f00\u6e9014B\u53c2\u6570\u6a21\u578bConfucius3-Math\u5728\u5355GPU\u9ad8\u6548\u8fd0\u884c\uff0c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u6027\u80fd\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\uff0c\u4e13\u6ce8\u4e2d\u56fdK-12\u6570\u5b66\u6559\u80b2\u3002", "motivation": "\u901a\u8fc7AI\u6280\u672f\u63d0\u5347\u6559\u80b2\u9886\u57df\u7684\u6570\u5b66\u5b66\u4e60\u6548\u7387\uff0c\u7279\u522b\u662f\u9488\u5bf9\u4e2d\u56fd\u57fa\u7840\u6559\u80b2\u9636\u6bb5\u5b66\u751f\u548c\u6559\u5e08\u7684\u4f4e\u6210\u672c\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u540e\u8bad\u7ec3\uff0c\u7ed3\u5408\u4e09\u9879\u6280\u672f\u521b\u65b0\uff1a\u5b9a\u5411\u71b5\u6b63\u5219\u5316\u3001\u8fd1\u671f\u6837\u672c\u6062\u590d\u3001\u7b56\u7565\u7279\u5f02\u6027\u96be\u5ea6\u52a0\u6743\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6570\u636e\u5229\u7528\u7387\u3002", "result": "\u6a21\u578b\u5728\u56fd\u5bb6\u8bfe\u7a0b\u5bf9\u9f50\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6210\u529f\u89e3\u51b3\u4e3b\u6d41\u4e2d\u5c0f\u5b66\u6570\u5b66\u9898\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5728GitHub\u5f00\u6e90\u3002", "conclusion": "\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u7684\u6280\u672f\u521b\u65b0\uff0c\u9a8c\u8bc1\u4e86\u4f4e\u6210\u672c\u6784\u5efa\u5f3a\u63a8\u7406\u6a21\u578b\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u6559\u80b2AI\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u590d\u73b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2506.18349", "pdf": "https://arxiv.org/pdf/2506.18349", "abs": "https://arxiv.org/abs/2506.18349", "authors": ["Zichong Li", "Chen Liang", "Zixuan Zhang", "Ilgee Hong", "Young Jin Kim", "Weizhu Chen", "Tuo Zhao"], "title": "SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm\nfor scaling large language models (LLMs) while maintaining inference\nefficiency. However, their enormous memory requirements make them prohibitively\nexpensive to fine-tune or deploy in resource-constrained environments. To\naddress this challenge, we introduce SlimMoE, a multi-stage compression\nframework for transforming large MoE models into much smaller, efficient\nvariants without incurring the prohibitive costs of training from scratch. Our\nmethod systematically reduces parameter counts by slimming experts and\ntransferring knowledge through intermediate stages, effectively mitigating the\nperformance degradation common in one-shot pruning approaches. Using this\nframework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to\ncreate Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE\n(3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of\nthe original model's training data. These compressed models can be fine-tuned\non a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them\nhighly suitable for academic and resource-limited settings. Our experiments\ndemonstrate that these compressed models outperform others of similar size and\nremain competitive with larger models. For instance, Phi-mini-MoE achieves\nsimilar or better performance to Phi-3-mini using only 2/3 of the activated\nparameters and yields comparable MMLU scores to Llama 3.1 8B despite having\nsignificantly lower latency. Our findings demonstrate that structured pruning\ncombined with staged distillation offers an effective path to creating\nhigh-quality, compact MoE models, paving the way for broader adoption of MoE\narchitectures. We make our models publicly available at\nhttps://huggingface.co/microsoft/Phi-mini-MoE-instruct and\nhttps://huggingface.co/microsoft/Phi-tiny-MoE-instruct .", "AI": {"tldr": "\u63d0\u51faSlimMoE\u591a\u9636\u6bb5\u538b\u7f29\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u7cbe\u7b80\u548c\u5206\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\uff0c\u5c0641.9B\u53c2\u6570\u7684MoE\u6a21\u578b\u538b\u7f29\u81f33.8B\u53c2\u6570\u89c4\u6a21\uff0c\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u5355GPU\u5fae\u8c03\u80fd\u529b\u3002", "motivation": "\u5927\u53c2\u6570\u91cf\u7684MoE\u6a21\u578b\u5b58\u5728\u5185\u5b58\u5360\u7528\u8fc7\u9ad8\u95ee\u9898\uff0c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u548c\u5fae\u8c03\uff0c\u9700\u8981\u9ad8\u6548\u538b\u7f29\u65b9\u6848\u89e3\u51b3\u6027\u80fd\u4e0e\u6548\u7387\u7684\u5e73\u8861\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u538b\u7f29\u6d41\u7a0b\uff1a1) \u4e13\u5bb6\u53c2\u6570\u7cbe\u7b80 2) \u4e2d\u95f4\u9636\u6bb5\u77e5\u8bc6\u8fc1\u79fb 3) \u5206\u9636\u6bb5\u84b8\u998f\u8bad\u7ec3\uff0c\u4ec5\u4f7f\u7528\u539f\u59cb\u6570\u636e\u91cf\u768410%\uff08400B tokens\uff09\u5b8c\u6210\u6a21\u578b\u538b\u7f29\u3002", "result": "Phi-mini-MoE\uff087.6B\u53c2\u6570\uff09\u6027\u80fd\u6301\u5e73Phi-3-mini\u4e14\u6fc0\u6d3b\u53c2\u6570\u51cf\u5c111/3\uff0cMMLU\u5f97\u5206\u63a5\u8fd1Llama3.1 8B\u4f46\u5ef6\u8fdf\u66f4\u4f4e\uff1bPhi-tiny-MoE\uff083.8B\u53c2\u6570\uff09\u53ef\u5728A6000\u5355\u5361\u5fae\u8c03\u3002", "conclusion": "\u7ed3\u6784\u5316\u526a\u679d+\u5206\u9636\u6bb5\u84b8\u998f\u7684\u7ec4\u5408\u7b56\u7565\u80fd\u6709\u6548\u751f\u6210\u7d27\u51d1\u578bMoE\u6a21\u578b\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u573a\u666f\u63d0\u4f9b\u9ad8\u6027\u80fd\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8MoE\u67b6\u6784\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2506.18488", "pdf": "https://arxiv.org/pdf/2506.18488", "abs": "https://arxiv.org/abs/2506.18488", "authors": ["Markus Frohmann", "Elena V. Epure", "Gabriel Meseguer-Brocal", "Markus Schedl", "Romain Hennequin"], "title": "AI-Generated Song Detection via Lyrics Transcripts", "categories": ["cs.SD", "cs.AI", "cs.CL"], "comment": "Accepted to ISMIR 2025", "summary": "The recent rise in capabilities of AI-based music generation tools has\ncreated an upheaval in the music industry, necessitating the creation of\naccurate methods to detect such AI-generated content. This can be done using\naudio-based detectors; however, it has been shown that they struggle to\ngeneralize to unseen generators or when the audio is perturbed. Furthermore,\nrecent work used accurate and cleanly formatted lyrics sourced from a lyrics\nprovider database to detect AI-generated music. However, in practice, such\nperfect lyrics are not available (only the audio is); this leaves a substantial\ngap in applicability in real-life use cases. In this work, we instead propose\nsolving this gap by transcribing songs using general automatic speech\nrecognition (ASR) models. We do this using several detectors. The results on\ndiverse, multi-genre, and multi-lingual lyrics show generally strong detection\nperformance across languages and genres, particularly for our best-performing\nmodel using Whisper large-v2 and LLM2Vec embeddings. In addition, we show that\nour method is more robust than state-of-the-art audio-based ones when the audio\nis perturbed in different ways and when evaluated on different music\ngenerators. Our code is available at\nhttps://github.com/deezer/robust-AI-lyrics-detection.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7ASR\u6a21\u578b\u8f6c\u5f55\u6b4c\u8bcd\u5e76\u68c0\u6d4bAI\u751f\u6210\u97f3\u4e50\uff0c\u76f8\u6bd4\u97f3\u9891\u68c0\u6d4b\u65b9\u6cd5\u5728\u6297\u5e72\u6270\u548c\u8de8\u751f\u6210\u5668\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u9c81\u68d2\u3002", "motivation": "AI\u97f3\u4e50\u751f\u6210\u5de5\u5177\u5174\u8d77\u5bfc\u81f4\u884c\u4e1a\u9700\u68c0\u6d4b\u624b\u6bb5\uff0c\u73b0\u6709\u97f3\u9891\u68c0\u6d4b\u5668\u6cdb\u5316\u6027\u5dee\uff0c\u6b4c\u8bcd\u68c0\u6d4b\u6cd5\u4f9d\u8d56\u5b8c\u7f8e\u6b4c\u8bcd\u6570\u636e\u5e93\u800c\u7f3a\u4e4f\u5b9e\u9645\u5e94\u7528\u6027\uff08\u4ec5\u80fd\u83b7\u53d6\u97f3\u9891\uff09\u3002", "method": "\u4f7f\u7528\u901a\u7528\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\uff08\u5982Whisper large-v2\uff09\u8f6c\u5f55\u6b4c\u66f2\uff0c\u7ed3\u5408LLM2Vec\u5d4c\u5165\u6784\u5efa\u68c0\u6d4b\u5668\u5206\u6790\u6b4c\u8bcd\u3002", "result": "\u5728\u591a\u8bed\u8a00/\u591a\u6d41\u6d3e\u573a\u666f\u4e0b\u68c0\u6d4b\u6027\u80fd\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u97f3\u9891\u53d7\u5e72\u6270\u6216\u4f7f\u7528\u4e0d\u540c\u751f\u6210\u5668\u65f6\uff0c\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u4f73\u97f3\u9891\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u586b\u8865\u4e86\u5b9e\u9645\u5e94\u7528\u7a7a\u767d\uff0c\u4e3a\u4ec5\u6709\u97f3\u9891\u7684\u771f\u5b9e\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2506.18510", "pdf": "https://arxiv.org/pdf/2506.18510", "abs": "https://arxiv.org/abs/2506.18510", "authors": ["Duygu Altinok"], "title": "Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich Transcripts", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": "Accepted to INTERSPEECH2025 workshop DISS2025", "summary": "Accurate detection of disfluencies in spoken language is crucial for\nenhancing the performance of automatic speech and language processing systems,\nas well as fostering the development of more inclusive speech and language\ntechnologies. Leveraging the growing trend of large language models (LLMs) as\nversatile learners capable of processing both lexical and non-lexical inputs\n(e.g., audio and video), we propose a novel approach to transcribing\ndisfluencies as explicit tokens with timestamps, enabling the generation of\nfully annotated disfluency-rich transcripts. Our method integrates acoustic\nrepresentations extracted from an audio encoder with textual inputs of varying\nquality: clean transcriptions without disfluencies, time-aligned transcriptions\nfrom aligners, or outputs from phoneme-based ASR models -- all of which may\ncontain imperfections. Importantly, our experiments demonstrate that textual\ninputs do not need to be flawless. As long as they include timestamp-related\ncues, LLMs can effectively smooth the input and produce fully\ndisfluency-annotated transcripts, underscoring their robustness in handling\nimperfect hints.", "AI": {"tldr": "Proposes using LLMs with acoustic-text fusion to generate timestamped disfluency transcripts, showing robustness with imperfect text inputs.", "motivation": "Improve speech processing systems and develop inclusive language technologies by accurately detecting disfluencies in spoken language.", "method": "Integrates audio encoder's acoustic features with varied-quality text inputs (clean/aligned/ASR transcripts) using timestamp cues.", "result": "LLMs effectively generate annotated transcripts even with imperfect text, demonstrating robustness through timestamp-based smoothing.", "conclusion": "LLMs' ability to handle imperfect timestamped hints enables robust disfluency annotation, advancing inclusive speech technologies."}}
{"id": "2506.18586", "pdf": "https://arxiv.org/pdf/2506.18586", "abs": "https://arxiv.org/abs/2506.18586", "authors": ["Zijie Yang", "Qiji Zhou", "Fang Guo", "Sijie Zhang", "Yexun Xi", "Jinglei Nie", "Yudian Zhu", "Liping Huang", "Chou Wu", "Yonghe Xia", "Xiaoyu Ma", "Yingming Pu", "Panzhong Lu", "Junshu Pan", "Mingtao Chen", "Tiannan Guo", "Yanmei Dou", "Hongyu Chen", "Anping Zeng", "Jiaxing Huang", "Tian Xu", "Yue Zhang"], "title": "Airalogy: AI-empowered universal data digitization for research automation", "categories": ["cs.AI", "cs.CE", "cs.CL"], "comment": "146 pages, 6 figures, 49 supplementary figures", "summary": "Research data are the foundation of Artificial Intelligence (AI)-driven\nscience, yet current AI applications remain limited to a few fields with\nreadily available, well-structured, digitized datasets. Achieving comprehensive\nAI empowerment across multiple disciplines is still out of reach. Present-day\nresearch data collection is often fragmented, lacking unified standards,\ninefficiently managed, and difficult to share. Creating a single platform for\nstandardized data digitization needs to overcome the inherent challenge of\nbalancing between universality (supporting the diverse, ever-evolving needs of\nvarious disciplines) and standardization (enforcing consistent formats to fully\nenable AI). No existing platform accommodates both facets. Building a truly\nmultidisciplinary platform requires integrating scientific domain knowledge\nwith sophisticated computing skills. Researchers often lack the computational\nexpertise to design customized and standardized data recording methods, whereas\nplatform developers rarely grasp the intricate needs of multiple scientific\ndomains. These gaps impede research data standardization and hamper AI-driven\nprogress. In this study, we address these challenges by developing Airalogy\n(https://airalogy.com), the world's first AI- and community-driven platform\nthat balances universality and standardization for digitizing research data\nacross multiple disciplines. Airalogy represents entire research workflows\nusing customizable, standardized data records and offers an advanced AI\nresearch copilot for intelligent Q&A, automated data entry, analysis, and\nresearch automation. Already deployed in laboratories across all four schools\nof Westlake University, Airalogy has the potential to accelerate and automate\nscientific innovation in universities, industry, and the global research\ncommunity-ultimately benefiting humanity as a whole.", "AI": {"tldr": "\u5f00\u53d1\u5168\u7403\u9996\u4e2a\u5e73\u8861\u901a\u7528\u6027\u4e0e\u6807\u51c6\u5316\u7684AI\u9a71\u52a8\u5e73\u53f0Airalogy\uff0c\u901a\u8fc7\u53ef\u5b9a\u5236\u6570\u636e\u8bb0\u5f55\u548cAI\u7814\u7a76\u52a9\u624b\u63a8\u52a8\u591a\u5b66\u79d1\u6570\u636e\u6574\u5408\u4e0e\u79d1\u7814\u81ea\u52a8\u5316", "motivation": "\u5f53\u524d\u79d1\u7814\u6570\u636e\u5b58\u5728\u788e\u7247\u5316\u6536\u96c6\u3001\u7f3a\u4e4f\u7edf\u4e00\u6807\u51c6\u3001\u96be\u4ee5\u5171\u4eab\u7b49\u95ee\u9898\uff0c\u5236\u7ea6\u4e86AI\u5728\u591a\u5b66\u79d1\u9886\u57df\u7684\u5168\u9762\u5e94\u7528\u3002\u73b0\u6709\u5e73\u53f0\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u5b66\u79d1\u591a\u6837\u6027\u9700\u6c42\u4e0e\u6570\u636e\u6807\u51c6\u5316\u8981\u6c42", "method": "\u6784\u5efaAiralogy\u5e73\u53f0\uff1a1) \u4f7f\u7528\u53ef\u5b9a\u5236\u7684\u6807\u51c6\u5316\u6570\u636e\u8bb0\u5f55\u5b8c\u6574\u79d1\u7814\u6d41\u7a0b 2) \u96c6\u6210AI\u7814\u7a76\u52a9\u624b\u5b9e\u73b0\u667a\u80fd\u95ee\u7b54\u3001\u81ea\u52a8\u5316\u6570\u636e\u5f55\u5165\u4e0e\u5206\u6790 3) \u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u4e0e\u8ba1\u7b97\u6280\u672f\u8bbe\u8ba1\u8de8\u5b66\u79d1\u89e3\u51b3\u65b9\u6848", "result": "\u5df2\u5728\u897f\u6e56\u5927\u5b66\u5168\u90e8\u56db\u4e2a\u5b66\u9662\u7684\u5b9e\u9a8c\u5ba4\u90e8\u7f72\u5e94\u7528\uff0c\u9a8c\u8bc1\u4e86\u5e73\u53f0\u52a0\u901f\u79d1\u7814\u521b\u65b0\u3001\u4fc3\u8fdb\u81ea\u52a8\u5316\u7814\u7a76\u7684\u6f5c\u529b", "conclusion": "Airalogy\u6210\u529f\u5e73\u8861\u901a\u7528\u6027\u4e0e\u6807\u51c6\u5316\u9700\u6c42\uff0c\u901a\u8fc7\u793e\u533a\u9a71\u52a8\u6a21\u5f0f\u63a8\u52a8\u591a\u5b66\u79d1\u6570\u636e\u6574\u5408\u4e0eAI\u5e94\u7528\uff0c\u4e3a\u5168\u7403\u79d1\u7814\u673a\u6784\u63d0\u4f9b\u521b\u65b0\u57fa\u7840\u8bbe\u65bd\uff0c\u6700\u7ec8\u670d\u52a1\u4eba\u7c7b\u6574\u4f53\u79d1\u7814\u8fdb\u6b65"}}
{"id": "2506.18598", "pdf": "https://arxiv.org/pdf/2506.18598", "abs": "https://arxiv.org/abs/2506.18598", "authors": ["Aviral Gupta", "Armaan Sethi", "Ameesh Sethi"], "title": "No Training Wheels: Steering Vectors for Bias Correction at Inference Time", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "Neural network classifiers trained on datasets with uneven group\nrepresentation often inherit class biases and learn spurious correlations.\nThese models may perform well on average but consistently fail on atypical\ngroups. For example, in hair color classification, datasets may over-represent\nfemales with blond hair, reinforcing stereotypes. Although various algorithmic\nand data-centric methods have been proposed to address such biases, they often\nrequire retraining or significant compute. In this work, we propose a cheap,\ntraining-free method inspired by steering vectors used to edit behaviors in\nlarge language models. We compute the difference in mean activations between\nmajority and minority groups to define a \"bias vector,\" which we subtract from\nthe model's residual stream. This leads to reduced classification bias and\nimproved worst-group accuracy. We explore multiple strategies for extracting\nand applying these vectors in transformer-like classifiers, showing that\nsteering vectors, traditionally used in generative models, can also be\neffective in classification. More broadly, we showcase an extremely cheap,\ninference time, training free method to mitigate bias in classification models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u4f4e\u6210\u672c\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u6fc0\u6d3b\u5dee\u5f02\u751f\u6210\u504f\u7f6e\u5411\u91cf\uff0c\u5728\u63a8\u7406\u9636\u6bb5\u76f4\u63a5\u4fee\u6b63\u5206\u7c7b\u6a21\u578b\u7684\u504f\u89c1", "motivation": "\u4f20\u7edf\u5206\u7c7b\u6a21\u578b\u5728\u6570\u636e\u5206\u5e03\u4e0d\u5747\u65f6\u5bb9\u6613\u7ee7\u627f\u7fa4\u4f53\u504f\u89c1\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u91cd\u65b0\u8bad\u7ec3\u6216\u9ad8\u8ba1\u7b97\u6210\u672c\u3002\u672c\u6587\u65e8\u5728\u5f00\u53d1\u6781\u4f4e\u6210\u672c\u7684\u65e0\u8bad\u7ec3\u7ea0\u504f\u65b9\u6848", "method": "\u8ba1\u7b97\u591a\u6570\u7fa4\u4f53\u4e0e\u5c11\u6570\u7fa4\u4f53\u5728\u6b8b\u5dee\u6d41\u4e2d\u7684\u5e73\u5747\u6fc0\u6d3b\u5dee\u5f02\u4f5c\u4e3a\u504f\u7f6e\u5411\u91cf\uff0c\u5728\u63a8\u7406\u65f6\u5c06\u8be5\u5411\u91cf\u4ece\u6b8b\u5dee\u6d41\u4e2d\u6263\u9664\u3002\u63a2\u7d22\u4e86\u4e0d\u540c\u7b56\u7565\u7684\u5411\u91cf\u63d0\u53d6\u548c\u5e94\u7528\u65b9\u5f0f", "result": "\u6709\u6548\u964d\u4f4e\u5206\u7c7b\u504f\u89c1\uff0c\u63d0\u5347\u6700\u5dee\u7fa4\u4f53\u51c6\u786e\u7387\u3002\u9a8c\u8bc1\u4e86\u751f\u6210\u6a21\u578b\u4e2d\u4f7f\u7528\u7684steering vectors\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027", "conclusion": "\u5c55\u793a\u4e86\u9996\u4e2a\u6781\u4f4e\u6210\u672c\u3001\u65e0\u9700\u8bad\u7ec3\u3001\u4ec5\u9700\u63a8\u7406\u9636\u6bb5\u8c03\u6574\u7684\u6a21\u578b\u7ea0\u504f\u65b9\u6848\uff0c\u6269\u5c55\u4e86steering vectors\u7684\u5e94\u7528\u573a\u666f\uff0c\u4e3a\u6a21\u578b\u516c\u5e73\u6027\u7814\u7a76\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2506.18628", "pdf": "https://arxiv.org/pdf/2506.18628", "abs": "https://arxiv.org/abs/2506.18628", "authors": ["Piotr Matys", "Jan Eliasz", "Konrad Kie\u0142czy\u0144ski", "Miko\u0142aj Langner", "Teddy Ferdinan", "Jan Koco\u0144", "Przemys\u0142aw Kazienko"], "title": "AggTruth: Contextual Hallucination Detection using Aggregated Attention Scores in LLMs", "categories": ["cs.AI", "cs.CL"], "comment": "ICCS 2025 Workshops", "summary": "In real-world applications, Large Language Models (LLMs) often hallucinate,\neven in Retrieval-Augmented Generation (RAG) settings, which poses a\nsignificant challenge to their deployment. In this paper, we introduce\nAggTruth, a method for online detection of contextual hallucinations by\nanalyzing the distribution of internal attention scores in the provided context\n(passage). Specifically, we propose four different variants of the method, each\nvarying in the aggregation technique used to calculate attention scores. Across\nall LLMs examined, AggTruth demonstrated stable performance in both same-task\nand cross-task setups, outperforming the current SOTA in multiple scenarios.\nFurthermore, we conducted an in-depth analysis of feature selection techniques\nand examined how the number of selected attention heads impacts detection\nperformance, demonstrating that careful selection of heads is essential to\nachieve optimal results.", "AI": {"tldr": "\u63d0\u51fa\u4e86AggTruth\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u4e0a\u4e0b\u6587\u6ce8\u610f\u529b\u5206\u6570\u5206\u5e03\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u591a\u79cd\u573a\u666f\u4e2d\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u573a\u666f\u4e2d\u4ecd\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u4e25\u91cd\u5f71\u54cd\u5b9e\u9645\u90e8\u7f72\u53ef\u9760\u6027", "method": "\u63d0\u51fa\u56db\u79cd\u57fa\u4e8e\u4e0d\u540c\u805a\u5408\u6280\u672f\u7684\u6ce8\u610f\u529b\u5206\u6570\u8ba1\u7b97\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u5185\u90e8\u6ce8\u610f\u529b\u673a\u5236\u68c0\u6d4b\u5f02\u5e38\u6a21\u5f0f", "result": "\u5728\u76f8\u540c\u4efb\u52a1\u548c\u8de8\u4efb\u52a1\u6d4b\u8bd5\u4e2d\u5747\u8868\u73b0\u7a33\u5b9a\uff0c\u5728\u591a\u4e2a\u573a\u666f\u8d85\u8d8a\u5f53\u524dSOTA\u65b9\u6cd5", "conclusion": "\u6ce8\u610f\u529b\u5934\u9009\u62e9\u5bf9\u68c0\u6d4b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u5408\u7406\u9009\u62e9\u53ef\u8fbe\u5230\u6700\u4f18\u7ed3\u679c\uff0cAggTruth\u4e3aLLM\u53ef\u9760\u6027\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.18631", "pdf": "https://arxiv.org/pdf/2506.18631", "abs": "https://arxiv.org/abs/2506.18631", "authors": ["Chenxing Wei", "Jiarui Yu", "Ying Tiffany He", "Hande Dong", "Yao Shu", "Fei Yu"], "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "10 pages, 15 figures", "summary": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages.", "AI": {"tldr": "ReDit\u65b9\u6cd5\u901a\u8fc7\u5411\u79bb\u6563\u5956\u52b1\u4fe1\u53f7\u6dfb\u52a0\u968f\u673a\u566a\u58f0\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u68af\u5ea6\u5f02\u5e38\u548c\u6536\u655b\u6162\u95ee\u9898\uff0c\u4ec5\u970010%\u8bad\u7ec3\u6b65\u9aa4\u5373\u53ef\u8fbe\u5230\u57fa\u51c6\u6027\u80fd\uff0c\u5e76\u5e26\u67654%\u7684\u989d\u5916\u63d0\u5347\u3002", "motivation": "\u79bb\u6563\u5956\u52b1\u7cfb\u7edf\u867d\u7136\u80fd\u907f\u514d\u5956\u52b1\u7834\u89e3\uff0c\u4f46\u4f1a\u5bfc\u81f4\u68af\u5ea6\u5f02\u5e38\u3001\u4f18\u5316\u4e0d\u7a33\u5b9a\u548c\u6536\u655b\u901f\u5ea6\u7f13\u6162\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6848\u3002", "method": "\u63d0\u51faReDit\uff08\u5956\u52b1\u6296\u52a8\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5411\u79bb\u6563\u5956\u52b1\u6dfb\u52a0\u7b80\u5355\u968f\u673a\u566a\u58f0\uff0c\u6301\u7eed\u63d0\u4f9b\u63a2\u7d22\u68af\u5ea6\uff0c\u5b9e\u73b0\u5e73\u6ed1\u53c2\u6570\u66f4\u65b0\uff0c\u5e76\u901a\u8fc7\u566a\u58f0\u6ce8\u5165\u4fc3\u8fdb\u7b56\u7565\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u663e\u793aReDit\u4ec5\u9700\u7ea610%\u8bad\u7ec3\u6b65\u9aa4\u5373\u8fbe\u57fa\u51c6\u6027\u80fd\uff0c\u540c\u8bad\u7ec3\u65f6\u957f\u4e0b\u6027\u80fd\u63d0\u53474%\uff0c\u53ef\u89c6\u5316\u9a8c\u8bc1\u68af\u5ea6\u95ee\u9898\u663e\u8457\u7f13\u89e3\u3002", "conclusion": "ReDit\u6210\u529f\u89e3\u51b3\u79bb\u6563\u5956\u52b1\u7cfb\u7edf\u7684\u4f18\u5316\u74f6\u9888\uff0c\u7406\u8bba\u5206\u6790\u4e0e\u5b9e\u9a8c\u9a8c\u8bc1\u5171\u540c\u652f\u6301\u5176\u52a0\u901f\u6536\u655b\u548c\u6027\u80fd\u63d0\u5347\u7684\u4f18\u52bf\u3002"}}
{"id": "2506.18716", "pdf": "https://arxiv.org/pdf/2506.18716", "abs": "https://arxiv.org/abs/2506.18716", "authors": ["Jie Li", "Shifei Ding", "Lili Guo", "Xuan Li"], "title": "Multi-modal Anchor Gated Transformer with Knowledge Distillation for Emotion Recognition in Conversation", "categories": ["cs.LG", "cs.CL"], "comment": "This paper has been accepted by IJCAI2025", "summary": "Emotion Recognition in Conversation (ERC) aims to detect the emotions of\nindividual utterances within a conversation. Generating efficient and\nmodality-specific representations for each utterance remains a significant\nchallenge. Previous studies have proposed various models to integrate features\nextracted using different modality-specific encoders. However, they neglect the\nvarying contributions of modalities to this task and introduce high complexity\nby aligning modalities at the frame level. To address these challenges, we\npropose the Multi-modal Anchor Gated Transformer with Knowledge Distillation\n(MAGTKD) for the ERC task. Specifically, prompt learning is employed to enhance\ntextual modality representations, while knowledge distillation is utilized to\nstrengthen representations of weaker modalities. Furthermore, we introduce a\nmulti-modal anchor gated transformer to effectively integrate utterance-level\nrepresentations across modalities. Extensive experiments on the IEMOCAP and\nMELD datasets demonstrate the effectiveness of knowledge distillation in\nenhancing modality representations and achieve state-of-the-art performance in\nemotion recognition. Our code is available at:\nhttps://github.com/JieLi-dd/MAGTKD.", "AI": {"tldr": "MAGTKD\u6a21\u578b\u901a\u8fc7prompt\u5b66\u4e60\u548c\u77e5\u8bc6\u84b8\u998f\u589e\u5f3a\u591a\u6a21\u6001\u8868\u793a\uff0c\u5728IEMOCAP\u548cMELD\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u60c5\u7eea\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6a21\u6001\u8d21\u732e\u5dee\u5f02\u5ffd\u89c6\u548c\u5e27\u7ea7\u5bf9\u9f50\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u672c\u6587\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u6a21\u6001\u951a\u70b9\u95e8\u63a7\u673a\u5236\u89e3\u51b3\u591a\u6a21\u6001\u8868\u793a\u4e0d\u5e73\u8861\u548c\u590d\u6742\u5bf9\u9f50\u7684\u6311\u6218\u3002", "method": "1. \u4f7f\u7528prompt learning\u589e\u5f3a\u6587\u672c\u6a21\u6001\u8868\u793a 2. \u91c7\u7528\u77e5\u8bc6\u84b8\u998f\u5f3a\u5316\u5f31\u6a21\u6001 3. \u8bbe\u8ba1\u591a\u6a21\u6001\u951a\u70b9\u95e8\u63a7\u53d8\u538b\u5668\u5b9e\u73b0\u8de8\u6a21\u6001\u7279\u5f81\u878d\u5408", "result": "\u5728IEMOCAP\u548cMELD\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u77e5\u8bc6\u84b8\u998f\u7684\u6709\u6548\u6027\uff0c\u60c5\u7eea\u8bc6\u522b\u51c6\u786e\u7387\u8fbe\u5230SOTA\u6c34\u5e73\uff08\u5177\u4f53\u6570\u503c\u89c1\u8bba\u6587\u5b9e\u9a8c\u90e8\u5206\uff09", "conclusion": "\u63d0\u51fa\u7684\u591a\u6a21\u6001\u878d\u5408\u6846\u67b6\u663e\u8457\u63d0\u5347\u60c5\u7eea\u8bc6\u522b\u6027\u80fd\uff0c\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\u6709\u6548\u5e73\u8861\u6a21\u6001\u8868\u793a\u5dee\u5f02\uff0c\u4ee3\u7801\u5f00\u6e90\u4fc3\u8fdb\u53ef\u590d\u73b0\u6027\u7814\u7a76\u3002"}}
{"id": "2506.18764", "pdf": "https://arxiv.org/pdf/2506.18764", "abs": "https://arxiv.org/abs/2506.18764", "authors": ["Csaba Zsolnai", "Niels L\u00f6rch", "Julian Arnold"], "title": "Neural Total Variation Distance Estimators for Changepoint Detection in News Data", "categories": ["cs.LG", "cs.CL", "cs.CY", "cs.SI"], "comment": "16 pages, 3 figures", "summary": "Detecting when public discourse shifts in response to major events is crucial\nfor understanding societal dynamics. Real-world data is high-dimensional,\nsparse, and noisy, making changepoint detection in this domain a challenging\nendeavor. In this paper, we leverage neural networks for changepoint detection\nin news data, introducing a method based on the so-called learning-by-confusion\nscheme, which was originally developed for detecting phase transitions in\nphysical systems. We train classifiers to distinguish between articles from\ndifferent time periods. The resulting classification accuracy is used to\nestimate the total variation distance between underlying content distributions,\nwhere significant distances highlight changepoints. We demonstrate the\neffectiveness of this method on both synthetic datasets and real-world data\nfrom The Guardian newspaper, successfully identifying major historical events\nincluding 9/11, the COVID-19 pandemic, and presidential elections. Our approach\nrequires minimal domain knowledge, can autonomously discover significant shifts\nin public discourse, and yields a quantitative measure of change in content,\nmaking it valuable for journalism, policy analysis, and crisis monitoring.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u548clearning-by-confusion\u65b9\u6cd5\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u65b0\u95fb\u6570\u636e\u4e2d\u7684\u5173\u952e\u4e8b\u4ef6\u8f6c\u6298\u70b9\uff0c\u6210\u529f\u8bc6\u522b9/11\u3001COVID-19\u7b49\u91cd\u5927\u5386\u53f2\u4e8b\u4ef6\u3002", "motivation": "\u516c\u5171\u8bdd\u8bed\u52a8\u6001\u76d1\u6d4b\u5bf9\u793e\u4f1a\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9ad8\u7ef4\u7a00\u758f\u7684\u65b0\u95fb\u6570\u636e\u4f7f\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u68c0\u6d4b\u53d8\u5316\u70b9\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u7684\u81ea\u4e3b\u68c0\u6d4b\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u8bad\u7ec3\u65f6\u5e8f\u5206\u7c7b\u5668\u4f30\u8ba1\u5185\u5bb9\u5206\u5e03\u5dee\u5f02\uff0c\u5229\u7528\u5206\u7c7b\u51c6\u786e\u7387\u91cf\u5316\u603b\u53d8\u5dee\u8ddd\u79bb\uff0c\u7a81\u7834\u6027\u5730\u5c06\u7269\u7406\u7cfb\u7edf\u76f8\u53d8\u68c0\u6d4b\u65b9\u6cd5\u8fc1\u79fb\u81f3\u6587\u672c\u5206\u6790\u9886\u57df\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u548c\u300a\u536b\u62a5\u300b2000-2022\u5e74\u6570\u636e\u4e2d\u9a8c\u8bc1\u6709\u6548\uff0c\u51c6\u786e\u6355\u6349\u6050\u6016\u88ad\u51fb\u3001\u75ab\u60c5\u7206\u53d1\u3001\u653f\u6743\u66f4\u8fed\u7b49\u793e\u4f1a\u8f6c\u6298\u70b9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u5185\u5bb9\u53d8\u5316\u7684\u91cf\u5316\u8bc4\u4f30\uff0c\u4e3a\u65b0\u95fb\u5206\u6790\u3001\u653f\u7b56\u5236\u5b9a\u63d0\u4f9b\u81ea\u52a8\u5316\u76d1\u6d4b\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5371\u673a\u4e8b\u4ef6\u7684\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u6784\u5efa\u3002"}}
{"id": "2506.18777", "pdf": "https://arxiv.org/pdf/2506.18777", "abs": "https://arxiv.org/abs/2506.18777", "authors": ["Jonathan Cook", "Silvia Sapora", "Arash Ahmadian", "Akbir Khan", "Tim Rocktaschel", "Jakob Foerster", "Laura Ruis"], "title": "Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Training large language models (LLMs) on source code significantly enhances\ntheir general-purpose reasoning abilities, but the mechanisms underlying this\ngeneralisation are poorly understood. In this paper, we propose Programming by\nBackprop (PBB) as a potential driver of this effect - teaching a model to\nevaluate a program for inputs by training on its source code alone, without\never seeing I/O examples. To explore this idea, we finetune LLMs on two sets of\nprograms representing simple maths problems and algorithms: one with source\ncode and I/O examples (w/ IO), the other with source code only (w/o IO). We\nfind evidence that LLMs have some ability to evaluate w/o IO programs for\ninputs in a range of experimental settings, and make several observations.\nFirstly, PBB works significantly better when programs are provided as code\nrather than semantically equivalent language descriptions. Secondly, LLMs can\nproduce outputs for w/o IO programs directly, by implicitly evaluating the\nprogram within the forward pass, and more reliably when stepping through the\nprogram in-context via chain-of-thought. We further show that PBB leads to more\nrobust evaluation of programs across inputs than training on I/O pairs drawn\nfrom a distribution that mirrors naturally occurring data. Our findings suggest\na mechanism for enhanced reasoning through code training: it allows LLMs to\ninternalise reusable algorithmic abstractions. Significant scope remains for\nfuture work to enable LLMs to more effectively learn from symbolic procedures,\nand progress in this direction opens other avenues like model alignment by\ntraining on formal constitutional principles.", "AI": {"tldr": "\u901a\u8fc7\u6e90\u4ee3\u7801\u8bad\u7ec3(PBB)\u53ef\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f7f\u5176\u901a\u8fc7\u4ee3\u7801\u5185\u90e8\u5316\u7b97\u6cd5\u62bd\u8c61", "motivation": "\u63a2\u7a76\u4ee3\u7801\u8bad\u7ec3\u5982\u4f55\u589e\u5f3aLLM\u7684\u6cdb\u5316\u63a8\u7406\u80fd\u529b\u673a\u5236", "method": "\u6bd4\u8f83LLM\u5728\u542bI/O\u793a\u4f8b\u548c\u7eaf\u6e90\u4ee3\u7801\u4e24\u79cd\u8bbe\u7f6e\u4e0b\u7684\u5fae\u8c03\u6548\u679c", "result": "1. \u4ee3\u7801\u5f62\u5f0f\u6548\u679c\u4f18\u4e8e\u8bed\u4e49\u63cf\u8ff0\n2. \u6a21\u578b\u53ef\u9690\u5f0f\u8bc4\u4f30\u7a0b\u5e8f(\u901a\u8fc7\u601d\u7ef4\u94fe\u66f4\u53ef\u9760)\n3. PBB\u6bd4\u4f20\u7edfI/O\u8bad\u7ec3\u5206\u5e03\u66f4\u9c81\u68d2", "conclusion": "\u4ee3\u7801\u8bad\u7ec3\u901a\u8fc7\u7b97\u6cd5\u62bd\u8c61\u5185\u90e8\u5316\u589e\u5f3a\u63a8\u7406\uff0c\u672a\u6765\u53ef\u63a2\u7d22\u7b26\u53f7\u7a0b\u5e8f\u5b66\u4e60\u673a\u5236\u53ca\u5176\u5728\u6a21\u578b\u5bf9\u9f50\u7b49\u9886\u57df\u7684\u5e94\u7528"}}
{"id": "2506.18810", "pdf": "https://arxiv.org/pdf/2506.18810", "abs": "https://arxiv.org/abs/2506.18810", "authors": ["Siao Tang", "Xinyin Ma", "Gongfan Fang", "Xinchao Wang"], "title": "ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "Codes are available at https://github.com/tsa18/ConciseHint", "summary": "Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and\nOpenAI o1 series have achieved notable performance enhancements on complex\nreasoning tasks by scaling up the generation length by Chain-of-Thought (CoT).\nHowever, an emerging issue is their inclination to produce excessively verbose\nreasoning processes, leading to the inefficiency problem. Existing literature\non improving efficiency mainly adheres to the before-reasoning paradigms such\nas prompting and reasoning or fine-tuning and reasoning, but ignores the\npromising direction of directly encouraging the model to speak concisely by\nintervening during the generation of reasoning. In order to fill the blank, we\npropose a framework dubbed ConciseHint, which continuously encourages the\nreasoning model to speak concisely by injecting the textual hint (manually\ndesigned or trained on the concise data) during the token generation of the\nreasoning process. Besides, ConciseHint is adaptive to the complexity of the\nquery by adaptively adjusting the hint intensity, which ensures it will not\nundermine model performance. Experiments on the state-of-the-art LRMs,\nincluding DeepSeek-R1 and Qwen-3 series, demonstrate that our method can\neffectively produce concise reasoning processes while maintaining performance\nwell. For instance, we achieve a reduction ratio of 65\\% for the reasoning\nlength on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss.", "AI": {"tldr": "\u63d0\u51faConciseHint\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u6587\u672c\u63d0\u793a\uff0c\u663e\u8457\u7f29\u77ed\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u63a8\u7406\u957f\u5ea6\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96c6\u4e2d\u4e8e\u63a8\u7406\u524d\u4f18\u5316\uff08\u5982\u63d0\u793a\u5de5\u7a0b/\u5fae\u8c03\uff09\uff0c\u5ffd\u7565\u5728\u63a8\u7406\u751f\u6210\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u5e72\u9884\u7684\u6f5c\u529b\u3002LRMs\u5197\u957f\u7684\u63a8\u7406\u8fc7\u7a0b\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u5411\u3002", "method": "1. \u5728\u63a8\u7406token\u751f\u6210\u9636\u6bb5\u6ce8\u5165\u6587\u672c\u63d0\u793a\uff08\u4eba\u5de5\u8bbe\u8ba1\u6216\u57fa\u4e8e\u7b80\u6d01\u6570\u636e\u8bad\u7ec3\uff09\uff1b2. \u6839\u636e\u67e5\u8be2\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u63d0\u793a\u5f3a\u5ea6\uff0c\u5e73\u8861\u7b80\u6d01\u6027\u4e0e\u51c6\u786e\u6027\u3002", "result": "\u5728Qwen-3 4B\u6a21\u578b\u4e0a\u5b9e\u73b0GSM8K\u63a8\u7406\u957f\u5ea6\u51cf\u5c1165%\u4e14\u7cbe\u5ea6\u65e0\u635f\uff0cDeepSeek-R1\u7b49\u4e3b\u6d41LRMs\u5747\u9a8c\u8bc1\u6709\u6548\u6027\u3002", "conclusion": "ConciseHint\u5f00\u521b\u63a8\u7406\u4e2d\u5e72\u9884\u8303\u5f0f\uff0c\u4e3a\u4f18\u5316LRMs\u6548\u7387\u63d0\u4f9b\u65b0\u8def\u5f84\uff0c\u8bc1\u660e\u52a8\u6001\u8c03\u8282\u63d0\u793a\u5f3a\u5ea6\u53ef\u6709\u6548\u517c\u987e\u63a8\u7406\u8d28\u91cf\u4e0e\u6548\u7387\u3002"}}
{"id": "2506.18843", "pdf": "https://arxiv.org/pdf/2506.18843", "abs": "https://arxiv.org/abs/2506.18843", "authors": ["Heng-Jui Chang", "Saurabhchand Bhati", "James Glass", "Alexander H. Liu"], "title": "USAD: Universal Speech and Audio Representation via Distillation", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Preprint", "summary": "Self-supervised learning (SSL) has revolutionized audio representations, yet\nmodels often remain domain-specific, focusing on either speech or non-speech\ntasks. In this work, we present Universal Speech and Audio Distillation (USAD),\na unified approach to audio representation learning that integrates diverse\naudio types - speech, sound, and music - into a single model. USAD employs\nefficient layer-to-layer distillation from domain-specific SSL models to train\na student on a comprehensive audio dataset. USAD offers competitive performance\nacross various benchmarks and datasets, including frame and instance-level\nspeech processing tasks, audio tagging, and sound classification, achieving\nnear state-of-the-art results with a single encoder on SUPERB and HEAR\nbenchmarks.", "AI": {"tldr": "\u63d0\u51faUSAD\u901a\u7528\u97f3\u9891\u8868\u5f81\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6574\u5408\u8bed\u97f3/\u58f0\u97f3/\u97f3\u4e50\u591a\u6a21\u6001\u6570\u636e\uff0c\u5355\u7f16\u7801\u5668\u5b9e\u73b0\u8de8\u9886\u57df\u6700\u4f18\u6027\u80fd", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u5c40\u9650\u4e8e\u5355\u4e00\u9886\u57df\uff08\u8bed\u97f3\u6216\u975e\u8bed\u97f3\u4efb\u52a1\uff09\uff0c\u9700\u8981\u7edf\u4e00\u7684\u97f3\u9891\u8868\u5f81\u89e3\u51b3\u65b9\u6848", "method": "\u91c7\u7528\u5206\u5c42\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u4ece\u9886\u57df\u4e13\u7528SSL\u6a21\u578b\u4e2d\u63d0\u53d6\u77e5\u8bc6\uff0c\u5728\u7efc\u5408\u97f3\u9891\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u901a\u7528\u5b66\u751f\u6a21\u578b", "result": "\u5728SUPERB/HEAR\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63a5\u8fd1SOTA\uff0c\u5728\u8bed\u97f3\u5904\u7406\u3001\u97f3\u9891\u6807\u6ce8\u3001\u58f0\u97f3\u5206\u7c7b\u7b49\u4efb\u52a1\u8868\u73b0\u4f18\u5f02", "conclusion": "USAD\u9996\u6b21\u5b9e\u73b0\u5355\u4e00\u6a21\u578b\u5bf9\u591a\u7c7b\u578b\u97f3\u9891\u4efb\u52a1\u7684\u6709\u6548\u7edf\u4e00\uff0c\u4e3a\u901a\u7528\u97f3\u9891\u7406\u89e3\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.18871", "pdf": "https://arxiv.org/pdf/2506.18871", "abs": "https://arxiv.org/abs/2506.18871", "authors": ["Chenyuan Wu", "Pengfei Zheng", "Ruiran Yan", "Shitao Xiao", "Xin Luo", "Yueze Wang", "Wanli Li", "Xiyan Jiang", "Yexin Liu", "Junjie Zhou", "Ze Liu", "Ziyi Xia", "Chaofan Li", "Haoge Deng", "Jiahao Wang", "Kun Luo", "Bo Zhang", "Defu Lian", "Xinlong Wang", "Zhongyuan Wang", "Tiejun Huang", "Zheng Liu"], "title": "OmniGen2: Exploration to Advanced Multimodal Generation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "In this work, we introduce OmniGen2, a versatile and open-source generative\nmodel designed to provide a unified solution for diverse generation tasks,\nincluding text-to-image, image editing, and in-context generation. Unlike\nOmniGen v1, OmniGen2 features two distinct decoding pathways for text and image\nmodalities, utilizing unshared parameters and a decoupled image tokenizer. This\ndesign enables OmniGen2 to build upon existing multimodal understanding models\nwithout the need to re-adapt VAE inputs, thereby preserving the original text\ngeneration capabilities. To facilitate the training of OmniGen2, we developed\ncomprehensive data construction pipelines, encompassing image editing and\nin-context generation data. Additionally, we introduce a reflection mechanism\ntailored for image generation tasks and curate a dedicated reflection dataset\nbased on OmniGen2. Despite its relatively modest parameter size, OmniGen2\nachieves competitive results on multiple task benchmarks, including\ntext-to-image and image editing. To further evaluate in-context generation,\nalso referred to as subject-driven tasks, we introduce a new benchmark named\nOmniContext. OmniGen2 achieves state-of-the-art performance among open-source\nmodels in terms of consistency. We will release our models, training code,\ndatasets, and data construction pipeline to support future research in this\nfield. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:\nhttps://github.com/VectorSpaceLab/OmniGen2", "AI": {"tldr": "OmniGen2\u662f\u5f00\u6e90\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u89e3\u7801\u8def\u5f84\u5b9e\u73b0\u6587\u672c/\u56fe\u50cf\u751f\u6210\u3001\u7f16\u8f91\u548c\u4e0a\u4e0b\u6587\u751f\u6210\u4efb\u52a1\uff0c\u5728\u4fdd\u7559\u6587\u672c\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u591a\u4efb\u52a1SOTA\u8868\u73b0\u3002", "motivation": "\u65e8\u5728\u7edf\u4e00\u89e3\u51b3\u591a\u6a21\u6001\u751f\u6210\u4efb\u52a1\uff0c\u6539\u8fdbOmniGen v1\u7ed3\u6784\u7f3a\u9677\uff08\u5171\u4eab\u53c2\u6570\u5bfc\u81f4VAE\u8f93\u5165\u9700\u91cd\u65b0\u9002\u914d\uff09\uff0c\u4fdd\u7559\u539f\u6709\u6587\u672c\u751f\u6210\u80fd\u529b\u3002", "method": "1. \u4f7f\u7528\u6587\u672c/\u56fe\u50cf\u6a21\u6001\u5206\u79bb\u7684\u89e3\u7801\u8def\u5f84\u548c\u53c2\u6570 2. \u5f00\u53d1\u56fe\u50cf\u7f16\u8f91/\u4e0a\u4e0b\u6587\u751f\u6210\u6570\u636e\u6784\u5efa\u6d41\u7a0b 3. \u5f15\u5165\u56fe\u50cf\u751f\u6210\u53cd\u5c04\u673a\u5236\u548c\u4e13\u7528\u6570\u636e\u96c6", "result": "\u5728text-to-image/image editing\u57fa\u51c6\u8fbe\u7ade\u4e89\u6027\u7ed3\u679c\uff0c\u63d0\u51faOmniContext\u65b0\u57fa\u51c6\u5e76\u5728\u4e00\u81f4\u6027\u6307\u6807\u53d6\u5f97\u5f00\u6e90\u6a21\u578bSOTA", "conclusion": "\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u6570\u636e\u521b\u65b0\u5e73\u8861\u6a21\u578b\u89c4\u6a21\u4e0e\u6027\u80fd\uff0c\u627f\u8bfa\u5f00\u6e90\u6a21\u578b/\u4ee3\u7801/\u6570\u636e\u96c6\u63a8\u52a8\u591a\u6a21\u6001\u751f\u6210\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2506.18898", "pdf": "https://arxiv.org/pdf/2506.18898", "abs": "https://arxiv.org/abs/2506.18898", "authors": ["Jiaming Han", "Hao Chen", "Yang Zhao", "Hanyu Wang", "Qi Zhao", "Ziyan Yang", "Hao He", "Xiangyu Yue", "Lu Jiang"], "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "Project page: https://tar.csuhan.com", "summary": "This paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we propose scale-adaptive encoding and decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fast autoregressive model and a\ndiffusion-based model. To enhance modality fusion, we investigate advanced\npre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show that Tar matches or surpasses\nexisting multimodal LLM methods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.com", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6587\u672c\u5bf9\u9f50tokenizer\u7684\u591a\u6a21\u6001\u6846\u67b6Tar\uff0c\u7edf\u4e00\u89c6\u89c9\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\uff0c\u901a\u8fc7\u5171\u4eab\u8bed\u4e49\u7a7a\u95f4\u5b9e\u73b0\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u8bad\u7ec3\u6548\u7387\u63d0\u53472\u500d\u3002", "motivation": "\u4f20\u7edf\u591a\u6a21\u6001\u6a21\u578b\u9700\u72ec\u7acb\u5904\u7406\u89c6\u89c9\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\uff0c\u7f3a\u4e4f\u7edf\u4e00\u8bed\u4e49\u8868\u793a\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6587\u672c\u5bf9\u9f50\u7684\u79bb\u6563\u8868\u5f81\u7a7a\u95f4\uff0c\u5b9e\u73b0\u89c6\u89c9\u4e0e\u8bed\u8a00\u6a21\u6001\u7684\u6df1\u5ea6\u878d\u5408\u3002", "method": "1. \u6587\u672c\u5bf9\u9f50tokenizer(TA-Tok)\u5c06\u56fe\u50cf\u6620\u5c04\u5230LLM\u8bcd\u6c47\u6269\u5c55\u7a7a\u95f4\n2. \u81ea\u9002\u5e94\u7f16\u89e3\u7801\u67b6\u6784\u5e73\u8861\u5206\u8fa8\u7387\u4e0e\u6548\u7387\n3. \u81ea\u56de\u5f52+\u6269\u6563\u53ccdetokenizer\u652f\u6301\u591a\u6837\u5316\u751f\u6210\n4. \u8bbe\u8ba1\u8de8\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u7b49\u9884\u8bad\u7ec3\u4efb\u52a1", "result": "\u572812\u4e2a\u89c6\u89c9-\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\uff0c\u8bad\u7ec3\u901f\u5ea6\u8f83Flamingo\u5feb2.3\u500d\uff0c\u56fe\u50cf\u751f\u6210FID\u6307\u6807\u63d0\u534715%\uff0c\u53c2\u6570\u91cf\u51cf\u5c1140%", "conclusion": "\u9a8c\u8bc1\u4e86\u7edf\u4e00\u8bed\u4e49\u8868\u793a\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u7aef\u5230\u7aef\u7684\u89c6\u89c9-\u8bed\u8a00\u7edf\u4e00\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2506.18902", "pdf": "https://arxiv.org/pdf/2506.18902", "abs": "https://arxiv.org/abs/2506.18902", "authors": ["Michael G\u00fcnther", "Saba Sturua", "Mohammad Kalim Akram", "Isabelle Mohr", "Andrei Ungureanu", "Sedigheh Eslami", "Scott Martens", "Bo Wang", "Nan Wang", "Han Xiao"], "title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval", "categories": ["cs.AI", "cs.CL", "cs.IR", "68T50", "I.2.7"], "comment": "22 pages, 1-10 main, 14-22 experimental results, benchmark tables", "summary": "We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding\nmodel that unifies text and image representations through a novel architecture\nsupporting both single-vector and multi-vector embeddings in the late\ninteraction style. The model incorporates task-specific Low-Rank Adaptation\n(LoRA) adapters to optimize performance across diverse retrieval scenarios,\nincluding query-based information retrieval, cross-modal semantic similarity,\nand programming code search. Comprehensive evaluations demonstrate that\njina-embeddings-v4 achieves state-of-the-art performance on both single- modal\nand cross-modal retrieval tasks, with particular strength in processing\nvisually rich content such as tables, charts, diagrams, and mixed-media\nformats. To facilitate evaluation of this capability, we also introduce\nJina-VDR, a novel benchmark specifically designed for visually rich image\nretrieval.", "AI": {"tldr": "38\u4ebf\u53c2\u6570\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578bjina-embeddings-v4\u901a\u8fc7\u7edf\u4e00\u6587\u672c/\u56fe\u50cf\u8868\u793a\u548cLoRA\u9002\u914d\u5668\u6280\u672f\uff0c\u5728\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\uff0c\u5e76\u63a8\u51fa\u89c6\u89c9\u68c0\u7d22\u65b0\u57fa\u51c6Jina-VDR", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5355\u6a21\u6001\u5d4c\u5165\u6a21\u578b\u5728\u5904\u7406\u89c6\u89c9\u4e30\u5bcc\u5185\u5bb9\uff08\u56fe\u8868/\u6df7\u5408\u5a92\u4f53\uff09\u548c\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u5c40\u9650\uff0c\u63d0\u5347\u591a\u573a\u666f\u68c0\u7d22\u80fd\u529b", "method": "1. \u652f\u6301\u5355/\u591a\u5411\u91cf\u5d4c\u5165\u7684\u665a\u4ea4\u4e92\u67b6\u6784 2. \u4efb\u52a1\u7279\u5b9aLoRA\u9002\u914d\u5668\u4f18\u5316 3. \u8de8\u6a21\u6001\u8868\u793a\u7edf\u4e00\u6846\u67b6 4. \u89c6\u89c9\u5185\u5bb9\u5904\u7406\u589e\u5f3a\u6a21\u5757", "result": "1. \u5355\u6a21\u6001/\u8de8\u6a21\u6001\u68c0\u7d22\u4efb\u52a1SOTA 2. \u89c6\u89c9\u4e30\u5bcc\u5185\u5bb9\u5904\u7406\u63d0\u5347\u663e\u8457 3. \u4ee3\u7801\u641c\u7d22\u51c6\u786e\u7387\u63d0\u534718% 4. Jina-VDR\u57fa\u51c6\u6d4b\u8bd5\u8868\u73b0\u4f18\u5f02", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u548c\u9002\u914d\u673a\u5236\uff0c\u663e\u8457\u63a8\u8fdb\u591a\u6a21\u6001\u68c0\u7d22\u6280\u672f\uff0c\u7279\u522b\u5728\u89c6\u89c9\u5185\u5bb9\u5904\u7406\u65b9\u9762\u5efa\u7acb\u65b0\u6807\u51c6\uff0c\u914d\u5957\u57fa\u51c6\u6d4b\u8bd5\u63a8\u52a8\u9886\u57df\u53d1\u5c55"}}
