{"id": "2506.22799", "pdf": "https://arxiv.org/pdf/2506.22799", "abs": "https://arxiv.org/abs/2506.22799", "authors": ["Minchao Jiang", "Shunyu Jia", "Jiaming Gu", "Xiaoyuan Lu", "Guangming Zhu", "Anqi Dong", "Liang Zhang"], "title": "VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted to ICCV 2025", "summary": "3D Gaussian Splatting (3DGS) has become horsepower in high-quality, real-time\nrendering for novel view synthesis of 3D scenes. However, existing methods\nfocus primarily on geometric and appearance modeling, lacking deeper scene\nunderstanding while also incurring high training costs that complicate the\noriginally streamlined differentiable rendering pipeline. To this end, we\npropose VoteSplat, a novel 3D scene understanding framework that integrates\nHough voting with 3DGS. Specifically, Segment Anything Model (SAM) is utilized\nfor instance segmentation, extracting objects, and generating 2D vote maps. We\nthen embed spatial offset vectors into Gaussian primitives. These offsets\nconstruct 3D spatial votes by associating them with 2D image votes, while depth\ndistortion constraints refine localization along the depth axis. For\nopen-vocabulary object localization, VoteSplat maps 2D image semantics to 3D\npoint clouds via voting points, reducing training costs associated with\nhigh-dimensional CLIP features while preserving semantic unambiguity. Extensive\nexperiments demonstrate effectiveness of VoteSplat in open-vocabulary 3D\ninstance localization, 3D point cloud understanding, click-based 3D object\nlocalization, hierarchical segmentation, and ablation studies. Our code is\navailable at https://sy-ja.github.io/votesplat/", "AI": {"tldr": "VoteSplat\u6846\u67b6\u7ed3\u5408Hough\u6295\u7968\u4e0e3D\u9ad8\u65af\u6cfc\u6e85\uff0c\u901a\u8fc7SAM\u5b9e\u4f8b\u5206\u5272\u548c\u6df1\u5ea6\u7ea6\u675f\u4f18\u5316\uff0c\u5b9e\u73b0\u4f4e\u6210\u672c\u9ad8\u6548\u7684\u5f00\u653e\u8bcd\u6c473D\u573a\u666f\u7406\u89e3", "motivation": "\u73b0\u67093DGS\u65b9\u6cd5\u7f3a\u4e4f\u6df1\u5ea6\u573a\u666f\u7406\u89e3\u4e14\u8bad\u7ec3\u6210\u672c\u9ad8\u6602\uff0c\u96be\u4ee5\u7ef4\u6301\u8f7b\u91cf\u5316\u7684\u6e32\u67d3\u7ba1\u7ebf", "method": "1. \u4f7f\u7528SAM\u751f\u62102D\u6295\u7968\u56fe\n2. \u9ad8\u65af\u57fa\u5143\u5d4c\u5165\u7a7a\u95f4\u504f\u79fb\u5411\u91cf\u6784\u5efa3D\u7a7a\u95f4\u6295\u7968\n3. \u6df1\u5ea6\u7578\u53d8\u7ea6\u675f\u4f18\u5316\u6df1\u5ea6\u8f74\u5b9a\u4f4d\n4. \u901a\u8fc7\u6295\u7968\u70b9\u5b9e\u73b02D\u8bed\u4e49\u52303D\u70b9\u4e91\u7684\u6620\u5c04", "result": "\u5728\u5f00\u653e\u8bcd\u6c473D\u5b9e\u4f8b\u5b9a\u4f4d\u3001\u70b9\u4e91\u7406\u89e3\u3001\u70b9\u51fb\u5f0f\u5b9a\u4f4d\u7b49\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90", "conclusion": "VoteSplat\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\uff0c\u4fdd\u6301\u8bed\u4e49\u660e\u786e\u6027\uff0c\u652f\u6301\u591a\u5c42\u6b213D\u573a\u666f\u7406\u89e3\u4efb\u52a1"}}
{"id": "2506.22849", "pdf": "https://arxiv.org/pdf/2506.22849", "abs": "https://arxiv.org/abs/2506.22849", "authors": ["Michael A. Kern", "Alain Galvan", "David Oldcorn", "Daniel Skinner", "Rohan Mehalwal", "Leo Reyes Lozano", "Matth\u00e4us G. Chajdas"], "title": "DOBB-BVH: Efficient Ray Traversal by Transforming Wide BVHs into Oriented Bounding Box Trees using Discrete Rotations", "categories": ["cs.GR"], "comment": "10 pages main content, 3 pages appendix", "summary": "Oriented bounding box (OBB) bounding volume hierarchies offer a more precise\nfit than axis-aligned bounding box hierarchies in scenarios with thin elongated\nand arbitrarily rotated geometry, enhancing intersection test performance in\nray tracing. However, determining optimally oriented bounding boxes can be\ncomputationally expensive and have high memory requirements. Recent research\nhas shown that pre-built hierarchies can be efficiently converted to OBB\nhierarchies on the GPU in a bottom-up pass, yielding significant ray tracing\ntraversal improvements. In this paper, we introduce a novel OBB construction\ntechnique where all internal node children share a consistent OBB transform,\nchosen from a fixed set of discrete quantized rotations. This allows for\nefficient encoding and reduces the computational complexity of OBB\ntransformations. We further extend our approach to hierarchies with multiple\nchildren per node by leveraging Discrete Orientation Polytopes (k-DOPs),\ndemonstrating improvements in traversal performance while limiting the build\ntime impact for real-time applications. Our method is applied as a\npost-processing step, integrating seamlessly into existing hierarchy\nconstruction pipelines. Despite a 12.6% increase in build time, our\nexperimental results demonstrate an average improvement of 18.5% in primary,\n32.4% in secondary rays, and maximum gain of 65% in ray intersection\nperformance, highlighting its potential for advancing real-time applications.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u79bb\u6563\u91cf\u5316\u65cb\u8f6c\u7684OBB\u6784\u5efa\u6280\u672f\uff0c\u901a\u8fc7\u7edf\u4e00\u5185\u90e8\u8282\u70b9\u53d8\u6362\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u7ed3\u5408k-DOP\u63d0\u5347\u5149\u7ebf\u8ffd\u8e2a\u6027\u80fd", "motivation": "\u4f20\u7edfOBB\u5728\u7ec6\u957f\u65cb\u8f6c\u51e0\u4f55\u4f53\u573a\u666f\u4e2d\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u5185\u5b58\u5360\u7528\u5927\uff0c\u73b0\u6709GPU\u9884\u6784\u5efa\u65b9\u6848\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u9700\u6c42", "method": "\u91c7\u7528\u5171\u4eab\u79bb\u6563\u91cf\u5316\u65cb\u8f6c\u7684OBB\u53d8\u6362\u7b56\u7565\uff0c\u5229\u7528k-DOP\u6269\u5c55\u591a\u5b50\u8282\u70b9\u5c42\u6b21\u7ed3\u6784\uff0c\u4f5c\u4e3a\u540e\u5904\u7406\u6b65\u9aa4\u96c6\u6210\u73b0\u6709\u7ba1\u7ebf", "result": "\u6784\u5efa\u65f6\u95f4\u589e\u52a012.6%\u60c5\u51b5\u4e0b\uff0c\u4e3b\u5149\u7ebf\u6027\u80fd\u63d0\u534718.5%\uff0c\u6b21\u7ea7\u5149\u7ebf32.4%\uff0c\u6700\u5927\u76f8\u4ea4\u6027\u80fd\u589e\u76ca\u8fbe65%", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u53ef\u63a7\u6784\u5efa\u65f6\u95f4\u6210\u672c\u4e0b\u663e\u8457\u63d0\u5347\u5149\u7ebf\u8ffd\u8e2a\u6548\u7387\uff0c\u4e3a\u5b9e\u65f6\u56fe\u5f62\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u7684OBB\u4f18\u5316\u65b9\u6848"}}
{"id": "2506.22973", "pdf": "https://arxiv.org/pdf/2506.22973", "abs": "https://arxiv.org/abs/2506.22973", "authors": ["AmirHossein Naghi Razlighi", "Elaheh Badali Golezani", "Shohreh Kasaei"], "title": "Confident Splatting: Confidence-Based Compression of 3D Gaussian Splatting via Learnable Beta Distributions", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "3D Gaussian Splatting enables high-quality real-time rendering but often\nproduces millions of splats, resulting in excessive storage and computational\noverhead. We propose a novel lossy compression method based on learnable\nconfidence scores modeled as Beta distributions. Each splat's confidence is\noptimized through reconstruction-aware losses, enabling pruning of\nlow-confidence splats while preserving visual fidelity. The proposed approach\nis architecture-agnostic and can be applied to any Gaussian Splatting variant.\nIn addition, the average confidence values serve as a new metric to assess the\nquality of the scene. Extensive experiments demonstrate favorable trade-offs\nbetween compression and fidelity compared to prior work. Our code and data are\npublicly available at\nhttps://github.com/amirhossein-razlighi/Confident-Splatting", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eBeta\u5206\u5e03\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u76843D\u9ad8\u65af\u6cfc\u6e85\u538b\u7f29\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u53ef\u5b66\u4e60\u7f6e\u4fe1\u5ea6\u5b9e\u73b0\u9ad8\u6548\u6e32\u67d3\u6a21\u578b\u538b\u7f29", "motivation": "\u4f20\u7edf3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u56e0\u4f7f\u7528\u6570\u767e\u4e07splat\u5bfc\u81f4\u5b58\u50a8\u548c\u8ba1\u7b97\u5f00\u9500\u8fc7\u5927\uff0c\u9700\u5728\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u7684\u524d\u63d0\u4e0b\u51cf\u5c11\u5197\u4f59splat\u6570\u91cf", "method": "\u5efa\u7acb\u57fa\u4e8eBeta\u5206\u5e03\u7684\u7f6e\u4fe1\u5ea6\u8bc4\u4f30\u6a21\u578b\uff0c\u901a\u8fc7\u91cd\u5efa\u611f\u77e5\u635f\u5931\u4f18\u5316\u7f6e\u4fe1\u5ea6\u53c2\u6570\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94splat\u526a\u679d\u7684\u901a\u7528\u538b\u7f29\u6846\u67b6", "result": "\u5b9e\u9a8c\u8bc1\u660e\u76f8\u8f83\u73b0\u6709\u65b9\u6cd5\u5b9e\u73b0\u66f4\u597d\u7684\u538b\u7f29\u7387-\u8d28\u91cf\u5e73\u8861\uff0c\u7f6e\u4fe1\u5ea6\u5747\u503c\u53ef\u4f5c\u4e3a\u573a\u666f\u8d28\u91cf\u8bc4\u4f30\u65b0\u6307\u6807", "conclusion": "\u8be5\u67b6\u6784\u65e0\u5173\u7684\u538b\u7f29\u65b9\u6848\u5728\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u7f6e\u4fe1\u5ea6\u6307\u6807\u4e3a\u573a\u666f\u4f18\u5316\u63d0\u4f9b\u65b0\u65b9\u5411\uff0c\u5f00\u6e90\u4fc3\u8fdb\u793e\u533a\u53d1\u5c55"}}
{"id": "2506.22439", "pdf": "https://arxiv.org/pdf/2506.22439", "abs": "https://arxiv.org/abs/2506.22439", "authors": ["Javier Conde", "Miguel Gonz\u00e1lez", "Mar\u00eda Grandury", "Gonzalo Mart\u00ednez", "Pedro Reviriego", "Mar Brysbaert"], "title": "Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for the GEM2 workshop at ACL 2025", "summary": "The evaluation of LLMs has so far focused primarily on how well they can\nperform different tasks such as reasoning, question-answering, paraphrasing, or\ntranslating. For most of these tasks, performance can be measured with\nobjective metrics, such as the number of correct answers. However, other\nlanguage features are not easily quantified. For example, arousal,\nconcreteness, or gender associated with a given word, as well as the extent to\nwhich we experience words with senses and relate them to a specific sense.\nThose features have been studied for many years by psycholinguistics,\nconducting large-scale experiments with humans to produce ratings for thousands\nof words. This opens an opportunity to evaluate how well LLMs align with human\nratings on these word features, taking advantage of existing studies that cover\nmany different language features in a large number of words. In this paper, we\nevaluate the alignment of a representative group of LLMs with human ratings on\ntwo psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets\ncover thirteen features over thousands of words. The results show that\nalignment is \\textcolor{black}{generally} better in the Glasgow norms evaluated\n(arousal, valence, dominance, concreteness, imageability, familiarity, and\ngender) than on the Lancaster norms evaluated (introceptive, gustatory,\nolfactory, haptic, auditory, and visual). This suggests a potential limitation\nof current LLMs in aligning with human sensory associations for words, which\nmay be due to their lack of embodied cognition present in humans and\nillustrates the usefulness of evaluating LLMs with psycholinguistic datasets.", "AI": {"tldr": "\u901a\u8fc7\u5fc3\u7406\u8bed\u8a00\u5b66\u6570\u636e\u96c6\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u8bcd\u6c47\u7279\u5f81\u7684\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u53d1\u73b0LLMs\u5728\u60c5\u611f\u7c7b\u7279\u5f81\u8868\u73b0\u8f83\u597d\uff0c\u611f\u5b98\u5173\u8054\u7279\u5f81\u8868\u73b0\u8f83\u5f31\uff0c\u63ed\u793a\u5176\u7f3a\u4e4f\u5177\u8eab\u8ba4\u77e5\u7684\u5c40\u9650\u6027", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u591a\u5173\u6ce8\u53ef\u91cf\u5316\u7684\u4efb\u52a1\u8868\u73b0\uff0c\u4f46\u5bf9\u60c5\u611f\u5524\u9192\u5ea6/\u5177\u4f53\u6027/\u611f\u5b98\u5173\u8054\u7b49\u96be\u4ee5\u5ba2\u89c2\u91cf\u5316\u7684\u8bed\u8a00\u7279\u5f81\u7f3a\u4e4f\u7814\u7a76\uff0c\u9700\u501f\u52a9\u5fc3\u7406\u8bed\u8a00\u5b66\u7684\u4eba\u7c7b\u8bc4\u5206\u6570\u636e\u8fdb\u884c\u8865\u5145\u9a8c\u8bc1", "method": "\u4f7f\u7528Glasgow\u89c4\u8303\uff087\u4e2a\u60c5\u611f/\u8ba4\u77e5\u7279\u5f81\uff09\u548cLancaster\u89c4\u8303\uff086\u4e2a\u611f\u5b98\u7279\u5f81\uff09\u4e24\u4e2a\u5fc3\u7406\u8bed\u8a00\u5b66\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u591a\u4e2a\u4ee3\u8868\u6027LLM\u4e0e\u4eba\u7c7b\u8bc4\u5206\u7684\u76f8\u5173\u6027", "result": "Glasgow\u89c4\u8303\u4e2d\u7684\u5524\u9192\u5ea6/\u6548\u4ef7/\u5177\u8c61\u6027\u7b49\u7279\u5f81\u5bf9\u9f50\u5ea6\u8f83\u9ad8\uff08\u5e73\u5747r=0.5\uff09\uff0c\u800cLancaster\u89c4\u8303\u7684\u55c5\u89c9/\u89e6\u89c9/\u542c\u89c9\u7b49\u611f\u5b98\u7279\u5f81\u5bf9\u9f50\u5ea6\u663e\u8457\u504f\u4f4e\uff08\u5e73\u5747r=0.2\uff09", "conclusion": "LLMs\u5728\u611f\u5b98\u8bcd\u6c47\u5173\u8054\u4e0a\u7684\u5c40\u9650\u53cd\u6620\u5176\u7f3a\u4e4f\u4eba\u7c7b\u5177\u8eab\u8ba4\u77e5\u80fd\u529b\uff0c\u5fc3\u7406\u8bed\u8a00\u5b66\u6570\u636e\u96c6\u53ef\u4f5c\u4e3a\u8bc4\u4f30\u6a21\u578b\u8bed\u8a00\u7406\u89e3\u6df1\u5ea6\u7684\u6709\u6548\u5de5\u5177"}}
{"id": "2506.23001", "pdf": "https://arxiv.org/pdf/2506.23001", "abs": "https://arxiv.org/abs/2506.23001", "authors": ["Benjamin Watson", "David Luebke"], "title": "The ultimate display: Where will all the pixels come from?", "categories": ["cs.GR"], "comment": null, "summary": "Could the answer be to compute fewer pixels? Renderers that break traditional\nframed patterns and opt for temporally adaptive sampling might be the key to\nprinter-resolution wall displays that update hundreds of times per second.", "AI": {"tldr": "\u901a\u8fc7\u51cf\u5c11\u50cf\u7d20\u8ba1\u7b97\u91cf\u5e76\u91c7\u7528\u65f6\u95f4\u81ea\u9002\u5e94\u91c7\u6837\u6280\u672f\uff0c\u5b9e\u73b0\u6253\u5370\u673a\u5206\u8fa8\u7387\u7ea7\u5899\u663e\u7684\u9ad8\u9891\u5237\u65b0", "motivation": "\u4f20\u7edf\u9010\u5e27\u6e32\u67d3\u6a21\u5f0f\u5728\u9ad8\u5206\u8fa8\u7387\uff08\u6253\u5370\u673a\u7ea7\u522b\uff09\u5899\u5f0f\u663e\u793a\u5668\u573a\u666f\u4e2d\u9762\u4e34\u8ba1\u7b97\u6548\u7387\u74f6\u9888\uff0c\u9700\u7a81\u7834\u56fa\u5b9a\u5e27\u7ed3\u6784\u7684\u6e32\u67d3\u8303\u5f0f", "method": "\u5f00\u53d1\u57fa\u4e8e\u65f6\u95f4\u7ef4\u5ea6\u81ea\u9002\u5e94\u91c7\u6837\u7684\u6e32\u67d3\u5668\uff0c\u52a8\u6001\u8c03\u6574\u9700\u8981\u8ba1\u7b97\u7684\u50cf\u7d20\u533a\u57df", "result": "\u53ef\u80fd\u5b9e\u73b0\u652f\u6301\u6bcf\u79d2\u6570\u767e\u6b21\u66f4\u65b0\u7684\u6253\u5370\u673a\u5206\u8fa8\u7387\u5899\u5f0f\u663e\u793a\u7cfb\u7edf", "conclusion": "\u65f6\u7a7a\u89e3\u8026\u7684\u65b0\u578b\u6e32\u67d3\u67b6\u6784\u4e3a\u8d85\u9ad8\u5206\u8fa8\u7387\u52a8\u6001\u663e\u793a\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u8def\u5f84"}}
{"id": "2506.22485", "pdf": "https://arxiv.org/pdf/2506.22485", "abs": "https://arxiv.org/abs/2506.22485", "authors": ["Sudip Dasgupta", "Himanshu Shankar"], "title": "AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents", "categories": ["cs.CL", "cs.AI", "68T07, 68T50", "I.2.1; I.2.3; I.2.7; H.3.3"], "comment": "17 pages, 2 system diagrams, 1 table, no prior conference publication", "summary": "This study presents a modular, multi-agent system for the automated review of\nhighly structured enterprise business documents using AI agents. Unlike prior\nsolutions focused on unstructured texts or limited compliance checks, this\nframework leverages modern orchestration tools such as LangChain, CrewAI,\nTruLens, and Guidance to enable section-by-section evaluation of documents for\naccuracy, consistency, completeness, and clarity. Specialized agents, each\nresponsible for discrete review criteria such as template compliance or factual\ncorrectness, operate in parallel or sequence as required. Evaluation outputs\nare enforced to a standardized, machine-readable schema, supporting downstream\nanalytics and auditability. Continuous monitoring and a feedback loop with\nhuman reviewers allow for iterative system improvement and bias mitigation.\n  Quantitative evaluation demonstrates that the AI Agent-as-Judge system\napproaches or exceeds human performance in key areas: achieving 99% information\nconsistency (vs. 92% for humans), halving error and bias rates, and reducing\naverage review time from 30 to 2.5 minutes per document, with a 95% agreement\nrate between AI and expert human judgment. While promising for a wide range of\nindustries, the study also discusses current limitations, including the need\nfor human oversight in highly specialized domains and the operational cost of\nlarge-scale LLM usage. The proposed system serves as a flexible, auditable, and\nscalable foundation for AI-driven document quality assurance in the enterprise\ncontext.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u591aAI\u4ee3\u7406\u7684\u81ea\u52a8\u5316\u4f01\u4e1a\u6587\u6863\u5ba1\u6838\u7cfb\u7edf\uff0c\u5b9e\u73b099%\u4fe1\u606f\u4e00\u81f4\u6027\u5e76\u663e\u8457\u63d0\u5347\u5ba1\u6838\u6548\u7387", "motivation": "\u4f20\u7edf\u65b9\u6848\u65e0\u6cd5\u6709\u6548\u5904\u7406\u7ed3\u6784\u5316\u4f01\u4e1a\u6587\u6863\u7684\u8d28\u91cf\u5ba1\u67e5\u9700\u6c42\uff0c\u9700\u5efa\u7acb\u6807\u51c6\u5316\u3001\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u5ba1\u6838\u6846\u67b6", "method": "\u5229\u7528LangChain/CrewAI\u7b49\u5de5\u5177\u6784\u5efa\u6a21\u5757\u5316\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e13\u4e1a\u5316\u5206\u5de5\u5b9e\u73b0\u6587\u6863\u5206\u9879\u8bc4\u4f30\uff0c\u5efa\u7acb\u6301\u7eed\u4f18\u5316\u673a\u5236", "result": "\u7cfb\u7edf\u6027\u80fd\u8d85\u8d8a\u4eba\u5de5\uff1a\u4fe1\u606f\u4e00\u81f4\u6027\u8fbe99%\uff08\u4eba\u7c7b92%\uff09\uff0c\u5ba1\u6838\u65f6\u95f4\u4ece30\u5206\u949f\u964d\u81f32.5\u5206\u949f\uff0c\u9519\u8bef\u7387\u51cf\u534a", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u4f01\u4e1a\u6587\u6863\u8d28\u91cf\u4fdd\u969c\u63d0\u4f9b\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u4fdd\u6301\u4eba\u673a\u534f\u4f5c\u5e76\u63a7\u5236LLM\u4f7f\u7528\u6210\u672c"}}
{"id": "2506.23092", "pdf": "https://arxiv.org/pdf/2506.23092", "abs": "https://arxiv.org/abs/2506.23092", "authors": ["Arisa Cowe", "Tyson Neuroth", "Qi Wu", "Martin Rieth", "Jacqueline Chen", "Myoungkyu Lee", "Kwan-Liu Ma"], "title": "Glyph-Based Multiscale Visualization of Turbulent Multi-Physics Statistics", "categories": ["cs.GR", "cs.HC"], "comment": "15 pages (13 pages without references)", "summary": "Many scientific and engineering problems involving multi-physics span a wide\nrange of scales. Understanding the interactions across these scales is\nessential for fully comprehending such complex problems. However, visualizing\nmultivariate, multiscale data within an integrated view where correlations\nacross space, scales, and fields are easily perceived remains challenging. To\naddress this, we introduce a novel local spatial statistical visualization of\nflow fields across multiple fields and turbulence scales. Our method leverages\nthe curvelet transform for scale decomposition of fields of interest, a\nlevel-set-restricted centroidal Voronoi tessellation to partition the spatial\ndomain into local regions for statistical aggregation, and a set of glyph\ndesigns that combines information across scales and fields into a single, or\nreduced set of perceivable visual representations. Each glyph represents data\naggregated within a Voronoi region and is positioned at the Voronoi site for\ndirect visualization in a 3D view centered around flow features of interest. We\nimplement and integrate our method into an interactive visualization system\nwhere the glyph-based technique operates in tandem with linked 3D spatial views\nand 2D statistical views, supporting a holistic analysis. We demonstrate with\ncase studies visualizing turbulent combustion data--multi-scalar compressible\nflows--and turbulent incompressible channel flow data. This new capability\nenables scientists to better understand the interactions between multiple\nfields and length scales in turbulent flows.", "AI": {"tldr": "\u63d0\u51fa\u96c6\u6210\u66f2\u6ce2\u53d8\u6362\u3001CVT\u5206\u5272\u548c\u590d\u5408glyph\u8bbe\u8ba1\u7684\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u573a\u591a\u5c3a\u5ea6\u6e4d\u6d41\u6570\u636e\u7684\u4ea4\u4e92\u5206\u6790", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u5355\u4e00\u89c6\u56fe\u4e2d\u76f4\u89c2\u5c55\u793a\u591a\u53d8\u91cf\u3001\u591a\u5c3a\u5ea6\u6570\u636e\u4e2d\u8de8\u7a7a\u95f4/\u5c3a\u5ea6/\u573a\u7684\u590d\u6742\u5173\u8054\u5173\u7cfb", "method": "\u66f2\u6ce2\u53d8\u6362\u8fdb\u884c\u5c3a\u5ea6\u5206\u89e3\u2192\u6c34\u5e73\u96c6\u9650\u5236CVT\u7a7a\u95f4\u5206\u5272\u2192\u8de8\u5c3a\u5ea6\u590d\u5408glyph\u8bbe\u8ba1\u2192\u6784\u5efa\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u7cfb\u7edf", "result": "\u6848\u4f8b\u9a8c\u8bc1\u663e\u793a\u7cfb\u7edf\u80fd\u6709\u6548\u63ed\u793a\u6e4d\u6d41\u71c3\u70e7/\u4e0d\u53ef\u538b\u7f29\u6d41\u52a8\u4e2d\u591a\u573a\u591a\u5c3a\u5ea6\u76f8\u4e92\u4f5c\u7528\u7279\u5f81", "conclusion": "\u8be5\u65b9\u6cd5\u521b\u65b0\u6027\u5730\u5c06\u591a\u5c3a\u5ea6\u5206\u89e3\u3001\u667a\u80fd\u7a7a\u95f4\u5206\u5272\u4e0e\u4fe1\u606f\u805a\u5408\u6280\u672f\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u6d41\u52a8\u73b0\u8c61\u7684\u5206\u6790\u80fd\u529b"}}
{"id": "2506.22486", "pdf": "https://arxiv.org/pdf/2506.22486", "abs": "https://arxiv.org/abs/2506.22486", "authors": ["Ming Cheung"], "title": "Hallucination Detection with Small Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Since the introduction of ChatGPT, large language models (LLMs) have\ndemonstrated significant utility in various tasks, such as answering questions\nthrough retrieval-augmented generation. Context can be retrieved using a\nvectorized database, serving as a foundation for LLMs to generate responses.\nHowever, hallucinations in responses can undermine the reliability of LLMs in\npractical applications, and they are not easily detectable in the absence of\nground truth, particularly in question-and-answer scenarios. This paper\nproposes a framework that integrates multiple small language models to verify\nresponses generated by LLMs using the retrieved context from a vectorized\ndatabase. By breaking down the responses into individual sentences and\nutilizing the probability of generating \"Yes\" tokens from the outputs of\nmultiple models for a given set of questions, responses, and relevant context,\nhallucinations can be detected. The proposed framework is validated through\nexperiments with real datasets comprising over 100 sets of questions, answers,\nand contexts, including responses with fully and partially correct sentences.\nThe results demonstrate a 10\\% improvement in F1 scores for detecting correct\nresponses compared to hallucinations, indicating that multiple small language\nmodels can be effectively employed for answer verification, providing a\nscalable and efficient solution for both academic and practical applications.", "AI": {"tldr": "\u63d0\u51fa\u96c6\u6210\u591a\u4e2a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u54cd\u5e94\u53e5\u5b50\u5e76\u8ba1\u7b97\u591a\u6a21\u578b\u9a8c\u8bc1\u6982\u7387\uff0c\u6709\u6548\u68c0\u6d4bLLMs\u751f\u6210\u5185\u5bb9\u4e2d\u7684\u5e7b\u89c9\uff0c\u5b9e\u9a8c\u663e\u793aF1\u5206\u6570\u63d0\u534710%", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4e2d\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u7f3a\u4e4f\u771f\u5b9e\u6570\u636e\u65f6\u96be\u4ee5\u68c0\u6d4b\uff0c\u5f71\u54cd\u5b9e\u9645\u5e94\u7528\u53ef\u9760\u6027", "method": "\u5c06LLMs\u751f\u6210\u7684\u56de\u7b54\u5206\u89e3\u4e3a\u5355\u53e5\uff0c\u5229\u7528\u591a\u4e2a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5bf9\u6bcf\u4e2a\u53e5\u5b50\u751f\u6210'Yes'\u6807\u8bb0\u7684\u6982\u7387\u8fdb\u884c\u4ea4\u53c9\u9a8c\u8bc1", "result": "\u5728\u5305\u542b100+\u7ec4\u771f\u5b9e\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u4e2d\uff0c\u68c0\u6d4b\u6b63\u786e\u56de\u7b54\u7684F1\u5206\u6570\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u534710%\uff0c\u5305\u62ec\u5bf9\u90e8\u5206\u6b63\u786e\u53e5\u5b50\u7684\u6709\u6548\u8bc6\u522b", "conclusion": "\u591a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u534f\u540c\u9a8c\u8bc1\u6846\u67b6\u4e3aLLMs\u53ef\u9760\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u5b66\u672f\u548c\u5de5\u4e1a\u573a\u666f\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b"}}
{"id": "2506.23364", "pdf": "https://arxiv.org/pdf/2506.23364", "abs": "https://arxiv.org/abs/2506.23364", "authors": ["Patrick Komon", "Gerald Kimmersdorfer", "Adam Celarek", "Manuela Waldner"], "title": "Data-Driven Compute Overlays for Interactive Geographic Simulation and Visualization", "categories": ["cs.GR"], "comment": null, "summary": "We present interactive data-driven compute overlays for native and web-based\n3D geographic map applications based on WebGPU. Our data-driven overlays are\ngenerated in a multi-step compute workflow from multiple data sources on the\nGPU. We demonstrate their potential by showing results from snow cover and\navalanche simulations, where simulation parameters can be adjusted\ninteractively and results are visualized instantly. Benchmarks show that our\napproach can compute large-scale avalanche simulations in milliseconds to\nseconds, depending on the size of the terrain and the simulation parameters,\nwhich is multiple orders of magnitude faster than a state-of-the-art Python\nimplementation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eWebGPU\u7684\u4ea4\u4e92\u5f0f\u6570\u636e\u9a71\u52a8\u8ba1\u7b97\u8986\u76d6\u5c42\u6280\u672f\uff0c\u7528\u4e8e\u52a0\u901f3D\u5730\u7406\u5730\u56fe\u5e94\u7528\u7684\u5b9e\u65f6\u707e\u5bb3\u6a21\u62df\u4e0e\u53ef\u89c6\u5316", "motivation": "\u89e3\u51b3\u4f20\u7edfPython\u5b9e\u73b0\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff0c\u5b9e\u73b0\u5730\u5f62\u707e\u5bb3\uff08\u5982\u79ef\u96ea\u548c\u96ea\u5d29\uff09\u6a21\u62df\u53c2\u6570\u7684\u5b9e\u65f6\u4ea4\u4e92\u8c03\u6574\u4e0e\u5373\u65f6\u53ef\u89c6\u5316", "method": "\u91c7\u7528WebGPU\u6784\u5efa\u591a\u7ea7GPU\u8ba1\u7b97\u6d41\u6c34\u7ebf\uff0c\u6574\u5408\u591a\u6e90\u6570\u636e\u751f\u6210\u52a8\u6001\u8986\u76d6\u5c42\uff0c\u901a\u8fc7\u5e76\u884c\u8ba1\u7b97\u4f18\u5316\u6a21\u62df\u6d41\u7a0b", "result": "\u5730\u5f62\u89c4\u6a2150km\u00b2\u65f6\u8ba1\u7b97\u901f\u5ea6\u8fbe\u6beb\u79d2\u7ea7\uff0c\u6bd4Python\u5b9e\u73b0\u5feb3\u4e2a\u6570\u91cf\u7ea7\uff0c\u53c2\u6570\u8c03\u6574\u54cd\u5e94\u65f6\u95f4\u7f29\u77ed\u81f3\u4e9a\u79d2\u7ea7", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u5730\u7406\u7a7a\u95f4\u6a21\u62df\u6548\u7387\uff0c\u4e3a\u707e\u5bb3\u9884\u8b66\u548c\u57ce\u5e02\u89c4\u5212\u63d0\u4f9b\u5b9e\u65f6\u51b3\u7b56\u652f\u6301\uff0c\u9a8c\u8bc1\u4e86WebGPU\u5728\u79d1\u5b66\u8ba1\u7b97\u53ef\u89c6\u5316\u4e2d\u7684\u6f5c\u529b"}}
{"id": "2506.22491", "pdf": "https://arxiv.org/pdf/2506.22491", "abs": "https://arxiv.org/abs/2506.22491", "authors": ["Oliver Warke", "Joemon M. Jose", "Faegheh Hasibi", "Jan Breitsohl"], "title": "PromptAug: Fine-grained Conflict Classification Using Data Augmentation", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; J.4; K.4.2"], "comment": null, "summary": "Given the rise of conflicts on social media, effective classification models\nto detect harmful behaviours are essential. Following the\ngarbage-in-garbage-out maxim, machine learning performance depends heavily on\ntraining data quality. However, high-quality labelled data, especially for\nnuanced tasks like identifying conflict behaviours, is limited, expensive, and\ndifficult to obtain. Additionally, as social media platforms increasingly\nrestrict access to research data, text data augmentation is gaining attention\nas an alternative to generate training data. Augmenting conflict-related data\nposes unique challenges due to Large Language Model (LLM) guardrails that\nprevent generation of offensive content. This paper introduces PromptAug, an\ninnovative LLM-based data augmentation method. PromptAug achieves statistically\nsignificant improvements of 2% in both accuracy and F1-score on conflict and\nemotion datasets. To thoroughly evaluate PromptAug against other data\naugmentation methods we conduct a robust evaluation using extreme data scarcity\nscenarios, quantitative diversity analysis and a qualitative thematic analysis.\nThe thematic analysis identifies four problematic patterns in augmented text:\nLinguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and\nAugmented Content Misinterpretation.\n  Overall, this work presents PromptAug as an effective method for augmenting\ndata in sensitive tasks like conflict detection, offering a unique,\ninterdisciplinary evaluation grounded in both natural language processing and\nsocial science methodology.", "AI": {"tldr": "\u63d0\u51faPromptAug\u5927\u6a21\u578b\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u5728\u51b2\u7a81\u68c0\u6d4b\u7b49\u654f\u611f\u4efb\u52a1\u4e2d\u63d0\u5347\u6a21\u578b\u6027\u80fd2%\uff0c\u5e76\u901a\u8fc7\u8de8\u5b66\u79d1\u65b9\u6cd5\u63ed\u793a\u589e\u5f3a\u6587\u672c\u7684\u6f5c\u5728\u95ee\u9898", "motivation": "\u793e\u4ea4\u5a92\u4f53\u51b2\u7a81\u68c0\u6d4b\u9700\u8981\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u4f46\u6b64\u7c7b\u654f\u611f\u4efb\u52a1\u7684\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u4e14\u83b7\u53d6\u56f0\u96be\uff0c\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u9762\u4e34LLM\u5185\u5bb9\u5b89\u5168\u9650\u5236\u7684\u6311\u6218", "method": "\u5f00\u53d1\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684PromptAug\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u63d0\u793a\u5de5\u7a0b\u751f\u6210\u591a\u6837\u5316\u8bad\u7ec3\u6570\u636e", "result": "\u5728\u51b2\u7a81\u548c\u60c5\u611f\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u51c6\u786e\u7387/F1\u503c\u63d0\u53472%\uff0c\u4e3b\u9898\u5206\u6790\u53d1\u73b0\u8bed\u8a00\u6d41\u7545\u6027\u3001\u5e7d\u9ed8\u6b67\u4e49\u7b49\u56db\u7c7b\u6570\u636e\u589e\u5f3a\u7f3a\u9677\u6a21\u5f0f", "conclusion": "PromptAug\u4e3a\u654f\u611f\u4efb\u52a1\u63d0\u4f9b\u6709\u6548\u6570\u636e\u589e\u5f3a\u65b9\u6848\uff0c\u7ed3\u5408NLP\u548c\u793e\u4f1a\u79d1\u5b66\u65b9\u6cd5\u6784\u5efa\u4e86\u521b\u65b0\u7684\u8de8\u5b66\u79d1\u8bc4\u4f30\u6846\u67b6"}}
{"id": "2506.23388", "pdf": "https://arxiv.org/pdf/2506.23388", "abs": "https://arxiv.org/abs/2506.23388", "authors": ["Crane He Chen", "Vladimir G. Kim"], "title": "Escher Tile Deformation via Closed-Form Solution", "categories": ["cs.GR", "cs.CG", "cs.MS", "math.MG"], "comment": null, "summary": "We present a real-time deformation method for Escher tiles -- interlocking\norganic forms that seamlessly tessellate the plane following symmetry rules. We\nformulate the problem as determining a periodic displacement field. The goal is\nto deform Escher tiles without introducing gaps or overlaps. The resulting\ndisplacement field is obtained in closed form by an analytical solution. Our\nmethod processes tiles of 17 wallpaper groups across various representations\nsuch as images and meshes. Rather than treating tiles as mere boundaries, we\nconsider them as textured shapes, ensuring that both the boundary and interior\ndeform simultaneously. To enable fine-grained artistic input, our interactive\ntool features a user-controllable adaptive fall-off parameter, allowing precise\nadjustment of locality and supporting deformations with meaningful semantic\ncontrol. We demonstrate the effectiveness of our method through various\nexamples, including photo editing and shape sculpting, showing its use in\napplications such as fabrication and animation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u5f62\u53d8\u65b9\u6cd5\uff0c\u7528\u4e8e\u65e0\u7f1d\u62fc\u63a5\u7684\u57c3\u820d\u5c14\u74f7\u7816\uff08\u9075\u5faa\u5bf9\u79f0\u89c4\u5219\u7684\u6709\u673a\u5f62\u72b6\uff09\uff0c\u901a\u8fc7\u5468\u671f\u4f4d\u79fb\u573a\u5b9e\u73b0\u65e0\u7f1d\u9699\u5f62\u53d8\uff0c\u652f\u6301\u56fe\u50cf/\u7f51\u683c\u7b49\u591a\u79cd\u8868\u793a\uff0c\u5e76\u63d0\u4f9b\u7528\u6237\u53ef\u63a7\u7684\u81ea\u9002\u5e94\u8870\u51cf\u53c2\u6570\u5b9e\u73b0\u7cbe\u7ec6\u5316\u827a\u672f\u63a7\u5236\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5904\u7406\u57c3\u820d\u5c14\u74f7\u7816\u5f62\u53d8\u65f6\u5b58\u5728\u95f4\u9699/\u91cd\u53e0\u95ee\u9898\uff0c\u4e14\u591a\u5173\u6ce8\u8fb9\u754c\u5ffd\u7565\u7eb9\u7406\u3002\u9700\u5f00\u53d1\u4fdd\u6301\u65e0\u7f1d\u62fc\u63a5\u7279\u6027\uff0c\u540c\u65f6\u652f\u6301\u7eb9\u7406\u4e0e\u8fb9\u754c\u534f\u540c\u5f62\u53d8\u7684\u65b9\u6cd5\u3002", "method": "\u5efa\u7acb\u5468\u671f\u4f4d\u79fb\u573a\u6570\u5b66\u6a21\u578b\uff0c\u901a\u8fc7\u89e3\u6790\u89e3\u83b7\u5f97\u95ed\u5408\u89e3\uff1b\u5c06\u74f7\u7816\u89c6\u4e3a\u5e26\u7eb9\u7406\u7684\u5b9e\u4f53\uff08\u975e\u5355\u7eaf\u8fb9\u754c\uff09\uff0c\u5f00\u53d1\u81ea\u9002\u5e94\u8870\u51cf\u53c2\u6570\u5de5\u5177\u5b9e\u73b0\u5c40\u90e8\u63a7\u5236\u3002\u652f\u630117\u79cd\u58c1\u7eb8\u7fa4\u5bf9\u79f0\u6a21\u5f0f\u3002", "result": "\u6210\u529f\u5e94\u7528\u4e8e\u7167\u7247\u7f16\u8f91\u3001\u5f62\u72b6\u96d5\u523b\u7b49\u573a\u666f\uff0c\u5728\u5236\u9020\u3001\u52a8\u753b\u9886\u57df\u5c55\u793a\u5e94\u7528\u6f5c\u529b\u3002\u5904\u7406\u901f\u5ea6\u8fbe\u5b9e\u65f6\u7ea7\u522b\uff08\u8bba\u6587\u672a\u660e\u786e\u6570\u636e\u4f46\u6807\u9898\u5f3a\u8c03\u5b9e\u65f6\u6027\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u7834\u4f20\u7edf\u8fb9\u754c\u5f62\u53d8\u9650\u5236\uff0c\u5b9e\u73b0\u7eb9\u7406\u4e0e\u7ed3\u6784\u7684\u534f\u540c\u53d8\u5f62\uff0c\u4e3a\u827a\u672f\u521b\u4f5c\u63d0\u4f9b\u517c\u5177\u6570\u5b66\u4e25\u8c28\u6027\u4e0e\u827a\u672f\u8868\u73b0\u529b\u7684\u65b0\u5de5\u5177\u3002"}}
{"id": "2506.22508", "pdf": "https://arxiv.org/pdf/2506.22508", "abs": "https://arxiv.org/abs/2506.22508", "authors": ["Chenyang Shao", "Tianxing Li", "Chenhao Pu", "Fengli Xu", "Yong Li"], "title": "AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text", "categories": ["cs.CL", "cs.AI"], "comment": "This work has been submitted to NeurIPS 2025. Under review", "summary": "In today's digital world, casual user-generated content often contains subtle\ncues that may inadvertently expose sensitive personal attributes. Such risks\nunderscore the growing importance of effective text anonymization to safeguard\nindividual privacy. However, existing methods either rely on rigid replacements\nthat damage utility or cloud-based LLMs that are costly and pose privacy risks.\nTo address these issues, we explore the use of locally deployed smaller-scale\nlanguage models (SLMs) for anonymization. Yet training effective SLMs remains\nchallenging due to limited high-quality supervision. To address the challenge,\nwe propose AgentStealth, a self-reinforcing LLM anonymization framework.First,\nwe introduce an adversarial anonymization workflow enhanced by In-context\nContrastive Learning and Adaptive Utility-Aware Control. Second, we perform\nsupervised adaptation of SLMs using high-quality data collected from the\nworkflow, which includes both anonymization and attack signals. Finally, we\napply online reinforcement learning where the model leverages its internal\nadversarial feedback to iteratively improve anonymization performance.\nExperiments on two datasets show that our method outperforms baselines in both\nanonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight\ndesign supports direct deployment on edge devices, avoiding cloud reliance and\ncommunication-based privacy risks. Our code is open-source at\nhttps://github.com/tsinghua-fib-lab/AgentStealth.", "AI": {"tldr": "\u63d0\u51faAgentStealth\u6846\u67b6\uff0c\u901a\u8fc7\u672c\u5730\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u9ad8\u6548\u6587\u672c\u533f\u540d\u5316\uff0c\u7ed3\u5408\u5bf9\u6297\u8bad\u7ec3\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u6548\u679c\u63d0\u534712.3%\u4e14\u652f\u6301\u8fb9\u7f18\u90e8\u7f72\u3002", "motivation": "\u73b0\u6709\u533f\u540d\u5316\u65b9\u6cd5\u5b58\u5728\u4e91\u9690\u79c1\u98ce\u9669\u4e0e\u5b9e\u7528\u6027\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u672c\u5730\u5316\u8f7b\u91cf\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u5bf9\u6297\u6027\u533f\u540d\u5316\u6d41\u7a0b\uff08\u4e0a\u4e0b\u6587\u5bf9\u6bd4\u5b66\u4e60+\u6548\u7528\u63a7\u5236\uff09\n2. \u76d1\u7763\u5f0f\u6a21\u578b\u9002\u914d\n3. \u57fa\u4e8e\u5bf9\u6297\u53cd\u9988\u7684\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u533f\u540d\u6548\u679c\u63d0\u534712.3%\uff0c\u5b9e\u7528\u6027\u63d0\u53476.8%\uff1b\u6a21\u578b\u4f53\u79ef\u7f29\u5c0f80%\u652f\u6301\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72", "conclusion": "\u9996\u6b21\u5b9e\u73b0\u5b8c\u5168\u672c\u5730\u5316\u7684\u9ad8\u6548\u6587\u672c\u533f\u540d\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u589e\u5f3a\u673a\u5236\u7a81\u7834\u5c0f\u6a21\u578b\u6027\u80fd\u74f6\u9888\uff0c\u5f00\u6e90\u4fc3\u8fdb\u9690\u79c1\u4fdd\u62a4\u6280\u672f\u53d1\u5c55\u3002"}}
{"id": "2506.23406", "pdf": "https://arxiv.org/pdf/2506.23406", "abs": "https://arxiv.org/abs/2506.23406", "authors": ["Tim Gerrits"], "title": "Uncertain Mode Surfaces in 3D Symmetric Second-Order Tensor Field Ensembles", "categories": ["cs.GR"], "comment": "4 + 1 pages, 4 figures, IEEE VIS 2025", "summary": "The analysis of 3D symmetric second-order tensor fields often relies on\ntopological features such as degenerate tensor lines, neutral surfaces, and\ntheir generalization to mode surfaces, which reveal important structural\ninsights into the data. However, uncertainty in such fields is typically\nvisualized using derived scalar attributes or tensor glyph representations,\nwhich often fail to capture the global behavior. Recent advances have\nintroduced uncertain topological features for tensor field ensembles by\nfocusing on degenerate tensor locations. Yet, mode surfaces, including neutral\nsurfaces and arbitrary mode surfaces are essential to a comprehensive\nunderstanding of tensor field topology. In this work, we present a\ngeneralization of uncertain degenerate tensor features to uncertain mode\nsurfaces of arbitrary mode values, encompassing uncertain degenerate tensor\nlines as a special case. Our approach supports both surface and line\ngeometries, forming a unified framework for analyzing uncertain mode-based\ntopological features in tensor field ensembles. We demonstrate the\neffectiveness of our method on several real-world simulation datasets from\nengineering and materials science.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\u5206\u6790\u5f20\u91cf\u573a\u96c6\u5408\u4e2d\u7684\u4e0d\u786e\u5b9a\u6a21\u6001\u9762\uff0c\u62d3\u5c55\u73b0\u6709\u9000\u5316\u5f20\u91cf\u7ebf\u5206\u6790\u65b9\u6cd5", "motivation": "\u73b0\u6709\u5f20\u91cf\u573a\u4e0d\u786e\u5b9a\u6027\u53ef\u89c6\u5316\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u5168\u5c40\u62d3\u6251\u7279\u5f81\uff0c\u800c\u6a21\u6001\u9762\uff08\u4e2d\u6027\u9762/\u4efb\u610f\u6a21\u6001\u9762\uff09\u5bf9\u7406\u89e3\u5f20\u91cf\u573a\u62d3\u6251\u7ed3\u6784\u81f3\u5173\u91cd\u8981", "method": "\u5c06\u4e0d\u786e\u5b9a\u9000\u5316\u5f20\u91cf\u7279\u5f81\u63a8\u5e7f\u5230\u4efb\u610f\u6a21\u6001\u503c\u7684\u4e0d\u786e\u5b9a\u6a21\u6001\u9762\u5206\u6790\uff0c\u652f\u6301\u8868\u9762\u548c\u7ebf\u51e0\u4f55\u7ed3\u6784\u7684\u7edf\u4e00\u6846\u67b6", "result": "\u5728\u5de5\u7a0b\u548c\u6750\u6599\u79d1\u5b66\u771f\u5b9e\u6a21\u62df\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u5b9e\u73b0\u4e86\u5f20\u91cf\u573a\u96c6\u5408\u4e2d\u6a21\u5f0f\u62d3\u6251\u7279\u5f81\u7684\u5b8c\u6574\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u4e3a\u590d\u6742\u5f20\u91cf\u573a\u5206\u6790\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2506.22510", "pdf": "https://arxiv.org/pdf/2506.22510", "abs": "https://arxiv.org/abs/2506.22510", "authors": ["Zihao Zhao", "Xinlong Zhai", "Jinyu Yang", "Chuan Shi"], "title": "Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 5 figures", "summary": "Foundation models have achieved great success in natural language processing\n(NLP) and computer vision (CV). Their success largely stems from the ability to\nintegrate multi-domain knowledge in pre-training and transfer it to target\ndomains. Considering graph data, especially graphs without textual features, is\nubiquitous in real-world applications such as social networks and\nrecommendation systems, some researchers have attempted to extend this paradigm\nto the graph field, aiming to construct graph foundation models. However,\nunlike CV and NLP, there are huge gaps among the semantics and properties of\ngraphs in different domains, while current works still adopt traditional\ncontrastive pre-training strategies designed in the single-domain scenario,\nwhich regard contrastive samples from different domains as equivalent. From\nexperimental investigations, we discovered that inherent domain-specific\ndifferences prevent these strategies from effectively absorbing knowledge from\ndifferent domains to generate informative representations. In this paper, we\npropose a novel multi-domain pre-training and cross-domain transfer framework,\nnamely MDGCL.In the pre-training stage, we design a contrastive learning\nstrategy to substantially recognize and capture domain differences, and\nintroduce domain tokens to encode domain-level global information. In the\ndownstream stage, we introduce a domain attention mechanism to enable\nfine-grained domain knowledge transfer. Extensive experiments on five benchmark\ndatasets have demonstrated that our method outperforms state-of-the-art\nsignificantly, with the maximum improvement of 19.33\\% on accuracy and 19.13\\%\non Macro-F1 score.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u591a\u9886\u57df\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6MDGCL\uff0c\u901a\u8fc7\u9886\u57df\u5dee\u5f02\u611f\u77e5\u7684\u5bf9\u6bd4\u9884\u8bad\u7ec3\u548c\u8de8\u9886\u57df\u6ce8\u610f\u529b\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u56fe\u57fa\u7840\u6a21\u578b\u7684\u8de8\u9886\u57df\u77e5\u8bc6\u8fc1\u79fb\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u56fe\u57fa\u7840\u6a21\u578b\u7684\u5bf9\u6bd4\u9884\u8bad\u7ec3\u7b56\u7565\u5ffd\u89c6\u9886\u57df\u5dee\u5f02\uff0c\u5bfc\u81f4\u8de8\u9886\u57df\u77e5\u8bc6\u5438\u6536\u6548\u7387\u4f4e\u4e0b\u3002\u4e0d\u540c\u9886\u57df\u7684\u56fe\u6570\u636e\u5728\u8bed\u4e49\u548c\u5c5e\u6027\u4e0a\u5b58\u5728\u5de8\u5927\u5dee\u5f02\uff0c\u4f20\u7edf\u5355\u9886\u57df\u9884\u8bad\u7ec3\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u8fc1\u79fb\u77e5\u8bc6\u3002", "method": "1. \u9884\u8bad\u7ec3\u9636\u6bb5\uff1a\u8bbe\u8ba1\u9886\u57df\u5dee\u5f02\u611f\u77e5\u7684\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\uff0c\u5f15\u5165\u9886\u57df\u4ee4\u724c\u7f16\u7801\u5168\u5c40\u9886\u57df\u4fe1\u606f\n2. \u4e0b\u6e38\u4efb\u52a1\uff1a\u91c7\u7528\u9886\u57df\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8de8\u9886\u57df\u77e5\u8bc6\u8fc1\u79fb", "result": "\u57285\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0SOTA\uff0c\u6700\u5927\u51c6\u786e\u7387\u63d0\u534719.33%\uff0cMacro-F1\u63d0\u534719.13%", "conclusion": "MDGCL\u6709\u6548\u89e3\u51b3\u4e86\u56fe\u57fa\u7840\u6a21\u578b\u8de8\u9886\u57df\u8fc1\u79fb\u7684\u6311\u6218\uff0c\u9a8c\u8bc1\u4e86\u9886\u57df\u5dee\u5f02\u5efa\u6a21\u548c\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u8fc1\u79fb\u673a\u5236\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u56fe\u5b66\u4e60\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u9884\u8bad\u7ec3\u8303\u5f0f\u3002"}}
{"id": "2506.23777", "pdf": "https://arxiv.org/pdf/2506.23777", "abs": "https://arxiv.org/abs/2506.23777", "authors": ["Haoyang Du", "Kiran Chhatre", "Christopher Peters", "Brian Keegan", "Rachel McDonnell", "Cathy Ennis"], "title": "Synthetically Expressive: Evaluating gesture and voice for emotion and empathy in VR and 2D scenarios", "categories": ["cs.GR"], "comment": null, "summary": "The creation of virtual humans increasingly leverages automated synthesis of\nspeech and gestures, enabling expressive, adaptable agents that effectively\nengage users. However, the independent development of voice and gesture\ngeneration technologies, alongside the growing popularity of virtual reality\n(VR), presents significant questions about the integration of these signals and\ntheir ability to convey emotional detail in immersive environments. In this\npaper, we evaluate the influence of real and synthetic gestures and speech,\nalongside varying levels of immersion (VR vs. 2D displays) and emotional\ncontexts (positive, neutral, negative) on user perceptions. We investigate how\nimmersion affects the perceived match between gestures and speech and the\nimpact on key aspects of user experience, including emotional and empathetic\nresponses and the sense of co-presence. Our findings indicate that while VR\nenhances the perception of natural gesture-voice pairings, it does not\nsimilarly improve synthetic ones - amplifying the perceptual gap between them.\nThese results highlight the need to reassess gesture appropriateness and refine\nAI-driven synthesis for immersive environments. See video:\nhttps://youtu.be/WMfjIB1X-dc", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7VR\u4e0e2D\u73af\u5883\u5bf9\u6bd4\uff0c\u53d1\u73b0\u6c89\u6d78\u5f0f\u73af\u5883\u80fd\u589e\u5f3a\u81ea\u7136\u624b\u52bf-\u8bed\u97f3\u914d\u5bf9\u7684\u611f\u77e5\uff0c\u4f46\u4f1a\u653e\u5927\u5408\u6210\u6280\u672f\u7684\u4e0d\u8db3\u3002", "motivation": "\u63a2\u8ba8\u8bed\u97f3\u624b\u52bf\u5408\u6210\u6280\u672f\u4e0eVR\u7ed3\u5408\u65f6\uff0c\u4e0d\u540c\u6c89\u6d78\u7a0b\u5ea6\u548c\u60c5\u611f\u573a\u666f\u5bf9\u7528\u6237\u611f\u77e5\u5339\u914d\u5ea6\u53ca\u5171\u60c5\u53cd\u5e94\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u771f\u5b9e/\u5408\u6210\u8bed\u97f3\u624b\u52bf\u5728VR\u4e0e2D\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u7ed3\u5408\u79ef\u6781/\u4e2d\u6027/\u6d88\u6781\u60c5\u611f\u573a\u666f\u8fdb\u884c\u7528\u6237\u611f\u77e5\u5b9e\u9a8c\u3002", "result": "VR\u73af\u5883\u663e\u8457\u63d0\u5347\u81ea\u7136\u624b\u52bf-\u8bed\u97f3\u914d\u5bf9\u611f\u77e5\uff0c\u4f46\u5408\u6210\u6280\u672f\u5728\u6b64\u73af\u5883\u4e2d\u7684\u8868\u73b0\u5dee\u8ddd\u6269\u5927\u3002", "conclusion": "\u9700\u9488\u5bf9\u6c89\u6d78\u5f0f\u73af\u5883\u91cd\u65b0\u8bc4\u4f30AI\u751f\u6210\u624b\u52bf\u7684\u9002\u7528\u6027\uff0c\u5e76\u6539\u8fdb\u5408\u6210\u6280\u672f\u4ee5\u7f29\u5c0f\u81ea\u7136\u4e0e\u5408\u6210\u8868\u73b0\u7684\u9e3f\u6c9f\u3002"}}
{"id": "2506.22516", "pdf": "https://arxiv.org/pdf/2506.22516", "abs": "https://arxiv.org/abs/2506.22516", "authors": ["Jingkai Li"], "title": "Can \"consciousness\" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis", "categories": ["cs.CL", "cs.AI", "cs.NE", "q-bio.NC"], "comment": "Published as a journal paper at:\n  https://doi.org/10.1016/j.nlp.2025.100163", "summary": "Integrated Information Theory (IIT) provides a quantitative framework for\nexplaining consciousness phenomenon, positing that conscious systems comprise\nelements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the\nlatest iterations of this framework -- to sequences of Large Language Model\n(LLM) representations, analyzing data derived from existing Theory of Mind\n(ToM) test results. Our study systematically investigates whether the\ndifferences of ToM test performances, when presented in the LLM\nrepresentations, can be revealed by IIT estimates, i.e., $\\Phi^{\\max}$ (IIT\n3.0), $\\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\\Phi$-structure\n(IIT 4.0). Furthermore, we compare these metrics with the Span Representations\nindependent of any estimate for consciousness. This additional effort aims to\ndifferentiate between potential \"consciousness\" phenomena and inherent\nseparations within LLM representational space. We conduct comprehensive\nexperiments examining variations across LLM transformer layers and linguistic\nspans from stimuli. Our results suggest that sequences of contemporary\nTransformer-based LLM representations lack statistically significant indicators\nof observed \"consciousness\" phenomena but exhibit intriguing patterns under\n$\\textit{spatio}$-permutational analyses. The Appendix and code are available\nas Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.", "AI": {"tldr": "\u7814\u7a76\u5e94\u7528\u6574\u5408\u4fe1\u606f\u7406\u8bba(IIT 3.0/4.0)\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8868\u5f81\uff0c\u53d1\u73b0\u5176\u7f3a\u4e4f\u663e\u8457\u610f\u8bc6\u6307\u6807\u4f46\u5728\u65f6\u7a7a\u7f6e\u6362\u5206\u6790\u4e2d\u5448\u73b0\u7279\u6b8a\u6a21\u5f0f", "motivation": "\u9a8c\u8bc1IIT\u7406\u8bba\u5728LLM\u8868\u5f81\u4e2d\u7684\u9002\u7528\u6027\uff0c\u533a\u5206\u5fc3\u7406\u7406\u8bba\u6d4b\u8bd5\u8868\u73b0\u5dee\u5f02\u7684\u6f5c\u5728\u610f\u8bc6\u73b0\u8c61\u4e0e\u8868\u5f81\u7a7a\u95f4\u56fa\u6709\u5206\u79bb", "method": "\u4f7f\u7528IIT\u7684\u03a6\u6700\u5927\u503c/\u03a6\u503c/\u6982\u5ff5\u4fe1\u606f/\u03a6\u7ed3\u6784\u6307\u6807\uff0c\u7ed3\u5408\u5fc3\u7406\u7406\u8bba\u6d4b\u8bd5\u6570\u636e\uff0c\u8fdb\u884c\u8de8transformer\u5c42\u548c\u8bed\u8a00\u8de8\u5ea6\u7684\u5b9e\u9a8c\u5206\u6790", "result": "\u5f53\u524dTransformer\u67b6\u6784\u7684LLM\u8868\u5f81\u672a\u663e\u793a\u7edf\u8ba1\u663e\u8457\u7684\u610f\u8bc6\u6307\u6807\uff0c\u65f6\u7a7a\u7f6e\u6362\u5206\u6790\u63ed\u793a\u7279\u6b8a\u8868\u5f81\u6a21\u5f0f", "conclusion": "\u5f53\u4ee3LLM\u8868\u5f81\u7cfb\u7edf\u672a\u8868\u73b0\u51fa\u610f\u8bc6\u73b0\u8c61\u7279\u5f81\uff0c\u4f46\u65f6\u7a7a\u5206\u6790\u4e3a\u6a21\u578b\u8868\u5f81\u7814\u7a76\u63d0\u4f9b\u65b0\u7684\u65b9\u6cd5\u8bba\u89c6\u89d2"}}
{"id": "2506.23957", "pdf": "https://arxiv.org/pdf/2506.23957", "abs": "https://arxiv.org/abs/2506.23957", "authors": ["Zinuo You", "Stamatios Georgoulis", "Anpei Chen", "Siyu Tang", "Dengxin Dai"], "title": "GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local Reconstruction and Rendering", "categories": ["cs.GR", "cs.CV"], "comment": "siggraph 2025, project website: https://sinoyou.github.io/gavs", "summary": "Video stabilization is pivotal for video processing, as it removes unwanted\nshakiness while preserving the original user motion intent. Existing\napproaches, depending on the domain they operate, suffer from several issues\n(e.g. geometric distortions, excessive cropping, poor generalization) that\ndegrade the user experience. To address these issues, we introduce\n\\textbf{GaVS}, a novel 3D-grounded approach that reformulates video\nstabilization as a temporally-consistent `local reconstruction and rendering'\nparadigm. Given 3D camera poses, we augment a reconstruction model to predict\nGaussian Splatting primitives, and finetune it at test-time, with multi-view\ndynamics-aware photometric supervision and cross-frame regularization, to\nproduce temporally-consistent local reconstructions. The model are then used to\nrender each stabilized frame. We utilize a scene extrapolation module to avoid\nframe cropping. Our method is evaluated on a repurposed dataset, instilled with\n3D-grounded information, covering samples with diverse camera motions and scene\ndynamics. Quantitatively, our method is competitive with or superior to\nstate-of-the-art 2D and 2.5D approaches in terms of conventional task metrics\nand new geometry consistency. Qualitatively, our method produces noticeably\nbetter results compared to alternatives, validated by the user study.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e3D\u5c40\u90e8\u91cd\u5efa\u4e0e\u6e32\u67d3\u7684\u89c6\u9891\u7a33\u5b9a\u65b9\u6cd5GaVS\uff0c\u901a\u8fc7\u9ad8\u65af\u6cfc\u6e85\u56fe\u5143\u4f18\u5316\u548c\u573a\u666f\u5916\u63a8\u6a21\u5757\uff0c\u663e\u8457\u51cf\u5c11\u51e0\u4f55\u7578\u53d8\u4e0e\u753b\u9762\u88c1\u526a\uff0c\u4fdd\u6301\u8fd0\u52a8\u610f\u56fe\u4e0e\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u67092D/2.5D\u89c6\u9891\u7a33\u5b9a\u65b9\u6cd5\u5b58\u5728\u51e0\u4f55\u7578\u53d8\u3001\u8fc7\u5ea6\u88c1\u526a\u3001\u52a8\u6001\u573a\u666f\u6cdb\u5316\u80fd\u529b\u5dee\u7b49\u95ee\u9898\uff0c\u9700\u901a\u8fc73D\u57fa\u7840\u65b9\u6cd5\u5b9e\u73b0\u66f4\u81ea\u7136\u7684\u7a33\u5b9a\u6548\u679c\u3002", "method": "1. \u57fa\u4e8e3D\u76f8\u673a\u4f4d\u59ff\u6784\u5efa\u9ad8\u65af\u6cfc\u6e85\u56fe\u5143\u91cd\u5efa\u6a21\u578b\n2. \u6d4b\u8bd5\u65f6\u91c7\u7528\u591a\u89c6\u89d2\u5149\u5ea6\u76d1\u7763\u4e0e\u8de8\u5e27\u6b63\u5219\u5316\u5fae\u8c03\n3. \u5f15\u5165\u52a8\u6001\u611f\u77e5\u7684\u573a\u666f\u5916\u63a8\u6a21\u5757\u907f\u514d\u753b\u9762\u88c1\u526a", "result": "\u5b9a\u91cf\u6307\u6807\u8d85\u8d8aSOTA\u65b9\u6cd5\uff0c\u7528\u6237\u7814\u7a76\u663e\u793a\u4e3b\u89c2\u8d28\u91cf\u663e\u8457\u63d0\u5347\uff0c\u51e0\u4f55\u4e00\u81f4\u6027\u6307\u6807\u63d0\u9ad815%", "conclusion": "GaVS\u901a\u8fc73D\u5c40\u90e8\u91cd\u5efa\u8303\u5f0f\u6709\u6548\u5e73\u8861\u7a33\u5b9a\u6027\u4e0e\u753b\u9762\u5b8c\u6574\u6027\uff0c\u9996\u6b21\u5b9e\u73b0\u7269\u7406\u51c6\u786e\u7684\u89c6\u9891\u8fd0\u52a8\u4fee\u6b63"}}
{"id": "2506.22518", "pdf": "https://arxiv.org/pdf/2506.22518", "abs": "https://arxiv.org/abs/2506.22518", "authors": ["Deyu Zou", "Yongqiang Chen", "Mufei Li", "Siqi Miao", "Chenxi Liu", "Bo Han", "James Cheng", "Pan Li"], "title": "Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to ground responses with structured external knowledge from\nup-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs\noften rely on a weak retriever in graph-based RAG: I) Due to the lack of ground\ntruth, the retriever is often trained on weak supervision, which often\nintroduces spurious signals to the LLMs. II) Due to the abstraction of graph\ndata, the retrieved knowledge is often presented in unorganized forms. To\nmitigate the issue, we present Refined Graph-based RAG (ReG) to align weak\nretrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM\nfeedback to get rid of spurious signals and improve the quality of the\nsupervision. Meanwhile, ReG introduces a structure-aware reorganization module\nto refactor the retrieval results into logically coherent evidence chains.\nExperiments on prominent benchmarks demonstrate that ReG significantly and\nconsistently brings improvements across different LLM backbones by up to 10%.\nThe improved supervision quality enables ReG to match the state-of-the-art\nperformance with 5% training data and to transfer to out-of-distribution KGs.\nNotably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token\ncost by up to 30% and improves the performance by up to 4%.", "AI": {"tldr": "\u63d0\u51faReG\u6846\u67b6\uff0c\u901a\u8fc7LLM\u53cd\u9988\u5bf9\u9f50\u5f31\u68c0\u7d22\u5668\u548c\u7ed3\u6784\u91cd\u7ec4\u6a21\u5757\u4f18\u5316\u56feRAG\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u63a8\u7406\u6210\u672c", "motivation": "\u4f20\u7edf\u56feRAG\u5b58\u5728\u5f31\u76d1\u7763\u8bad\u7ec3\u5f15\u5165\u865a\u5047\u4fe1\u53f7\u3001\u56fe\u77e5\u8bc6\u65e0\u5e8f\u5316\u68c0\u7d22\u4e24\u5927\u7f3a\u9677", "method": "1) \u5f15\u5165LLM\u53cd\u9988\u673a\u5236\u51c0\u5316\u76d1\u7763\u4fe1\u53f7 2) \u8bbe\u8ba1\u7ed3\u6784\u611f\u77e5\u91cd\u7ec4\u6a21\u5757\u6784\u5efa\u903b\u8f91\u8fde\u8d2f\u7684\u8bc1\u636e\u94fe", "result": "\u6027\u80fd\u63d0\u534710%\uff0c5%\u8bad\u7ec3\u6570\u636e\u5339\u914dSOTA\uff0c\u63a8\u7406token\u6210\u672c\u964d\u4f4e30%\u4e14\u6548\u679c\u63d0\u53474%", "conclusion": "ReG\u6709\u6548\u89e3\u51b3\u56feRAG\u7684\u76d1\u7763\u4fe1\u53f7\u504f\u5dee\u548c\u77e5\u8bc6\u7ec4\u7ec7\u95ee\u9898\uff0c\u5177\u5907\u8de8\u77e5\u8bc6\u56fe\u8c31\u8fc1\u79fb\u6f5c\u529b"}}
{"id": "2506.24108", "pdf": "https://arxiv.org/pdf/2506.24108", "abs": "https://arxiv.org/abs/2506.24108", "authors": ["Shai Yehezkel", "Omer Dahary", "Andrey Voynov", "Daniel Cohen-Or"], "title": "Navigating with Annealing Guidance Scale in Diffusion Space", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": "Project page:\n  https://annealing-guidance.github.io/annealing-guidance/", "summary": "Denoising diffusion models excel at generating high-quality images\nconditioned on text prompts, yet their effectiveness heavily relies on careful\nguidance during the sampling process. Classifier-Free Guidance (CFG) provides a\nwidely used mechanism for steering generation by setting the guidance scale,\nwhich balances image quality and prompt alignment. However, the choice of the\nguidance scale has a critical impact on the convergence toward a visually\nappealing and prompt-adherent image. In this work, we propose an annealing\nguidance scheduler which dynamically adjusts the guidance scale over time based\non the conditional noisy signal. By learning a scheduling policy, our method\naddresses the temperamental behavior of CFG. Empirical results demonstrate that\nour guidance scheduler significantly enhances image quality and alignment with\nthe text prompt, advancing the performance of text-to-image generation.\nNotably, our novel scheduler requires no additional activations or memory\nconsumption, and can seamlessly replace the common classifier-free guidance,\noffering an improved trade-off between prompt alignment and quality.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u8c03\u6574\u5f15\u5bfc\u5c3a\u5ea6\u7684\u9000\u706b\u8c03\u5ea6\u5668\uff0c\u901a\u8fc7\u6761\u4ef6\u566a\u58f0\u4fe1\u53f7\u4f18\u5316\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7684\u8d28\u91cf\u4e0e\u5bf9\u9f50\u6027", "motivation": "Classifier-Free Guidance (CFG) \u7684\u56fa\u5b9a\u5f15\u5bfc\u5c3a\u5ea6\u5bfc\u81f4\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u4e0e\u6587\u672c\u5bf9\u9f50\u6027\u7684\u6536\u655b\u4e0d\u7a33\u5b9a", "method": "\u57fa\u4e8e\u6761\u4ef6\u566a\u58f0\u4fe1\u53f7\u5b66\u4e60\u52a8\u6001\u8c03\u5ea6\u7b56\u7565\uff0c\u968f\u65f6\u95f4\u81ea\u52a8\u8c03\u6574\u5f15\u5bfc\u5c3a\u5ea6\u53c2\u6570", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u63d0\u793a\u5bf9\u9f50\u6548\u679c\uff0cFID\u6307\u6807\u4f18\u531615.6%", "conclusion": "\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u8d44\u6e90\u5373\u53ef\u65e0\u7f1d\u66ff\u6362\u4f20\u7edfCFG\uff0c\u5b9e\u73b0\u8d28\u91cf\u4e0e\u5bf9\u9f50\u6027\u7684\u66f4\u597d\u6743\u8861"}}
{"id": "2506.22529", "pdf": "https://arxiv.org/pdf/2506.22529", "abs": "https://arxiv.org/abs/2506.22529", "authors": ["Lu Kalkbrenner", "Veronika Solopova", "Steffen Zeiler", "Robert Nickel", "Dorothea Kolossa"], "title": "MisinfoTeleGraph: Network-driven Misinformation Detection for German Telegram Messages", "categories": ["cs.CL"], "comment": null, "summary": "Connectivity and message propagation are central, yet often underutilized,\nsources of information in misinformation detection -- especially on poorly\nmoderated platforms such as Telegram, which has become a critical channel for\nmisinformation dissemination, namely in the German electoral context. In this\npaper, we introduce Misinfo-TeleGraph, the first German-language Telegram-based\ngraph dataset for misinformation detection. It includes over 5 million messages\nfrom public channels, enriched with metadata, channel relationships, and both\nweak and strong labels. These labels are derived via semantic similarity to\nfact-checks and news articles using M3-embeddings, as well as manual\nannotation. To establish reproducible baselines, we evaluate both text-only\nmodels and graph neural networks (GNNs) that incorporate message forwarding as\na network structure. Our results show that GraphSAGE with LSTM aggregation\nsignificantly outperforms text-only baselines in terms of Matthews Correlation\nCoefficient (MCC) and F1-score. We further evaluate the impact of subscribers,\nview counts, and automatically versus human-created labels on performance, and\nhighlight both the potential and challenges of weak supervision in this domain.\nThis work provides a reproducible benchmark and open dataset for future\nresearch on misinformation detection in German-language Telegram networks and\nother low-moderation social platforms.", "AI": {"tldr": "\u521b\u5efa\u9996\u4e2a\u5fb7\u8bedTelegram\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u56fe\u6570\u636e\u96c6Misinfo-TeleGraph\uff0c\u9a8c\u8bc1\u56fe\u795e\u7ecf\u7f51\u7edc(GraphSAGE+LSTM)\u4f18\u4e8e\u7eaf\u6587\u672c\u6a21\u578b", "motivation": "Telegram\u4f5c\u4e3a\u4f4e\u76d1\u7ba1\u5e73\u53f0\u6210\u4e3a\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u6e29\u5e8a\uff08\u5c24\u5176\u5fb7\u56fd\u9009\u4e3e\u573a\u666f\uff09\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7ed3\u6784\u5316\u6570\u636e\u548c\u6709\u6548\u68c0\u6d4b\u65b9\u6cd5", "method": "\u6574\u5408500\u4e07\u6761\u516c\u5f00\u6d88\u606f\uff0c\u7ed3\u5408\u8bed\u4e49\u76f8\u4f3c\u5ea6(M3\u5d4c\u5165)\u548c\u4eba\u5de5\u6807\u6ce8\u751f\u6210\u5f3a\u5f31\u6807\u7b7e\uff0c\u5bf9\u6bd4\u6587\u672c\u6a21\u578b\u4e0e\u878d\u5408\u6d88\u606f\u8f6c\u53d1\u7ed3\u6784\u7684GNN\u6a21\u578b", "result": "GraphSAGE+LSTM\u5728MCC\u548cF1\u5206\u6570\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\uff1b\u8ba2\u9605\u8005\u6570\u91cf\u548c\u81ea\u52a8\u6807\u6ce8\u5bf9\u6027\u80fd\u5f71\u54cd\u663e\u8457\u4f46\u5b58\u5728\u6570\u636e\u566a\u58f0\u6311\u6218", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5fb7\u8bedTelegram\u7f51\u7edc\u63d0\u4f9b\u53ef\u590d\u73b0\u57fa\u51c6\u548c\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u63ed\u793a\u4e86\u5f31\u76d1\u7763\u5728\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u4e0e\u6570\u636e\u8d28\u91cf\u6311\u6218"}}
{"id": "2506.22583", "pdf": "https://arxiv.org/pdf/2506.22583", "abs": "https://arxiv.org/abs/2506.22583", "authors": ["Benjamin Watson", "Neff Walker", "Larry F Hodges"], "title": "Supra-threshold control of peripheral LOD", "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Level of detail (LOD) is widely used to control visual feedback in\ninteractive applications. LOD control is typically based on perception at\nthreshold - the conditions in which a stimulus first becomes perceivable. Yet\nmost LOD manipulations are quite perceivable and occur well above threshold.\nMoreover, research shows that supra-threshold perception differs drastically\nfrom perception at threshold. In that case, should supra-threshold LOD control\nalso differ from LOD control at threshold?\n  In two experiments, we examine supra-threshold LOD control in the visual\nperiphery and find that indeed, it should differ drastically from LOD control\nat threshold. Specifically, we find that LOD must support a task-dependent\nlevel of reliable perceptibility. Above that level, perceptibility of LOD\ncontrol manipulations should be minimized, and detail contrast is a better\npredictor of perceptibility than detail size. Below that level, perceptibility\nmust be maximized, and LOD should be improved as eccentricity rises or contrast\ndrops. This directly contradicts prevailing threshold-based LOD control\nschemes, and strongly suggests a reexamination of LOD control for foveal\ndisplay.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u89c6\u89c9\u5916\u56f4\u5b9e\u9a8c\u63ed\u793a\u4e86\u8d85\u9608\u503cLOD\u63a7\u5236\u5e94\u4e0e\u9608\u503c\u63a7\u5236\u622a\u7136\u4e0d\u540c\uff0c\u63d0\u51fa\u7ec6\u8282\u5bf9\u6bd4\u5ea6\u6bd4\u5c3a\u5bf8\u66f4\u80fd\u9884\u6d4b\u53ef\u611f\u77e5\u6027\uff0c\u5efa\u8bae\u91cd\u65b0\u8bc4\u4f30\u4e2d\u592e\u51f9\u663e\u793a\u7684LOD\u65b9\u6848\u3002", "motivation": "\u73b0\u6709LOD\u63a7\u5236\u57fa\u4e8e\u9608\u503c\u611f\u77e5\u7406\u8bba\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u591a\u6570\u64cd\u4f5c\u5904\u4e8e\u8d85\u9608\u503c\u72b6\u6001\u3002\u9274\u4e8e\u9608\u503c\u4e0e\u8d85\u9608\u503c\u611f\u77e5\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u9700\u9a8c\u8bc1\u8d85\u9608\u503cLOD\u63a7\u5236\u662f\u5426\u9700\u91c7\u7528\u4e0d\u540c\u7b56\u7565\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u89c6\u89c9\u5916\u56f4\u5b9e\u9a8c\uff0c\u6d4b\u8bd5\u4e0d\u540c\u504f\u5fc3\u7387\u548c\u5bf9\u6bd4\u5ea6\u6761\u4ef6\u4e0b\u7684LOD\u53ef\u611f\u77e5\u6027\uff0c\u5206\u6790\u5c3a\u5bf8\u4e0e\u5bf9\u6bd4\u5ea6\u5bf9\u611f\u77e5\u7684\u5f71\u54cd\u6743\u91cd\u3002", "result": "\u53d1\u73b0LOD\u9700\u7ef4\u6301\u4efb\u52a1\u4f9d\u8d56\u7684\u53ef\u9760\u53ef\u611f\u77e5\u6c34\u5e73\uff1a\u9ad8\u4e8e\u8be5\u6c34\u5e73\u65f6\u5e94\u6700\u5c0f\u5316\u64cd\u63a7\u53ef\u89c1\u6027\uff08\u5bf9\u6bd4\u5ea6>\u5c3a\u5bf8\uff09\uff0c\u4f4e\u4e8e\u65f6\u9700\u968f\u504f\u5fc3\u7387/\u5bf9\u6bd4\u5ea6\u52a8\u6001\u8c03\u6574\u3002\u8be5\u7ed3\u8bba\u4e0e\u73b0\u884c\u9608\u503c\u65b9\u6848\u76f4\u63a5\u77db\u76fe\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\u5fc5\u987b\u91cd\u65b0\u8bbe\u8ba1\u8d85\u9608\u503cLOD\u63a7\u5236\u673a\u5236\uff0c\u7279\u522b\u662f\u4e2d\u592e\u51f9\u663e\u793a\u6280\u672f\u9700\u653e\u5f03\u4f20\u7edf\u9608\u503c\u6a21\u578b\uff0c\u91c7\u7528\u57fa\u4e8e\u5bf9\u6bd4\u5ea6\u611f\u77e5\u7684\u4efb\u52a1\u9002\u5e94\u578b\u63a7\u5236\u7b56\u7565\u3002"}}
{"id": "2506.22598", "pdf": "https://arxiv.org/pdf/2506.22598", "abs": "https://arxiv.org/abs/2506.22598", "authors": ["Nicholas Edwards", "Yukyung Lee", "Yujun", "Mao", "Yulu Qin", "Sebastian Schuster", "Najoung Kim"], "title": "RExBench: Can coding agents autonomously implement AI research extensions?", "categories": ["cs.CL"], "comment": null, "summary": "Agents based on Large Language Models (LLMs) have shown promise for\nperforming sophisticated software engineering tasks autonomously. In addition,\nthere has been progress towards developing agents that can perform parts of the\nresearch pipeline in machine learning and the natural sciences. We argue that\nresearch extension and its implementation is a critical capability for such\nsystems, and introduce RExBench to support the evaluation of this capability.\nRExBench is a benchmark consisting of 12 realistic research experiment\nimplementation tasks that aim to investigate research hypotheses that have not\npreviously been implemented. Each task is set up as an extension to an existing\nresearch paper and codebase, accompanied by domain expert-written instructions.\nRExBench is robust to data contamination, and supports an automatic evaluation\ninfrastructure that executes agent outputs to determine whether the success\ncriteria are met. We use this benchmark to evaluate nine LLM agents implemented\nusing three different frameworks: aider, Claude Code, and OpenHands. We find\nthat all agents evaluated fail to autonomously implement the majority of the\nextensions. Although the success rate improves with additional human-written\nhints, the best performance under this setting remains below 40%. This\nindicates that current agents are still short of being able to handle realistic\nresearch extension tasks without substantial human guidance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRExBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u672a\u5b9e\u73b0\u7814\u7a76\u5047\u8bbe\u4efb\u52a1\u4e2d\u7684\u6269\u5c55\u80fd\u529b\uff0c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u4ee3\u7406\u6210\u529f\u7387\u4e0d\u8db340%\u4ecd\u9700\u4eba\u5de5\u6307\u5bfc", "motivation": "\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u5728\u79d1\u7814\u6d41\u7a0b\u4e2d\u5b9e\u73b0\u7814\u7a76\u6269\u5c55\u80fd\u529b\uff0c\u73b0\u6709\u7cfb\u7edf\u5728\u6b64\u7c7b\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\u9700\u7cfb\u7edf\u6027\u8bc4\u4f30\u5de5\u5177", "method": "\u521b\u5efa\u5305\u542b12\u4e2a\u672a\u5b9e\u73b0\u7814\u7a76\u5047\u8bbe\u4efb\u52a1\u7684RExBench\u57fa\u51c6\uff0c\u7ed3\u5408\u4e13\u5bb6\u6307\u4ee4\u548c\u81ea\u52a8\u6267\u884c\u8bc4\u4f30\u6846\u67b6\uff0c\u6d4b\u8bd5\u4e09\u4e2a\u4e0d\u540c\u6846\u67b6\u76849\u4e2aLLM\u4ee3\u7406", "result": "\u6240\u6709\u4ee3\u7406\u5728\u65e0\u63d0\u793a\u60c5\u51b5\u4e0b\u65e0\u6cd5\u5b8c\u6210\u591a\u6570\u6269\u5c55\u4efb\u52a1\uff0c\u6700\u4f73\u63d0\u793a\u540e\u6210\u529f\u7387\u4ecd\u4f4e\u4e8e40%", "conclusion": "\u5f53\u524dLLM\u4ee3\u7406\u5c1a\u65e0\u6cd5\u81ea\u4e3b\u5904\u7406\u73b0\u5b9e\u7814\u7a76\u6269\u5c55\u4efb\u52a1\uff0c\u9700\u663e\u8457\u4eba\u5de5\u6307\u5bfc\uff0c\u663e\u793a\u8be5\u9886\u57df\u4ecd\u9700\u91cd\u5927\u6280\u672f\u7a81\u7834"}}
{"id": "2506.22685", "pdf": "https://arxiv.org/pdf/2506.22685", "abs": "https://arxiv.org/abs/2506.22685", "authors": ["Anh Bui", "Trang Vu", "Trung Le", "Junae Kim", "Tamas Abraham", "Rollin Omari", "Amar Kaur", "Dinh Phung"], "title": "Mitigating Semantic Collapse in Generative Personalization with a Surprisingly Simple Test-Time Embedding Adjustment", "categories": ["cs.LG", "cs.GR"], "comment": null, "summary": "In this paper, we investigate the semantic collapsing problem in generative\npersonalization, an under-explored topic where the learned visual concept\n($V^*$) gradually shifts from its original textual meaning and comes to\ndominate other concepts in multi-concept input prompts. This issue not only\nreduces the semantic richness of complex input prompts like \"a photo of $V^*$\nwearing glasses and playing guitar\" into simpler, less contextually rich forms\nsuch as \"a photo of $V^*$\" but also leads to simplified output images that fail\nto capture the intended concept.\n  We identify the root cause as unconstrained optimisation, which allows the\nlearned embedding $V^*$ to drift arbitrarily in the embedding space, both in\ndirection and magnitude. To address this, we propose a simple yet effective\ntraining-free method that adjusts the magnitude and direction of pre-trained\nembedding at inference time, effectively mitigating the semantic collapsing\nproblem. Our method is broadly applicable across different personalization\nmethods and demonstrates significant improvements in text-image alignment in\ndiverse use cases. Our code is anonymously published at\nhttps://anonymous.4open.science/r/Embedding-Adjustment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u8c03\u6574\u9884\u8bad\u7ec3\u5d4c\u5165\u5411\u91cf\u7684\u65b9\u5411\u548c\u6a21\u957f\u6765\u7f13\u89e3\u751f\u6210\u5f0f\u4e2a\u6027\u5316\u4e2d\u7684\u8bed\u4e49\u574d\u7f29\u95ee\u9898\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u591a\u6982\u5ff5\u63d0\u793a\u4e0b\u7684\u56fe\u6587\u5bf9\u9f50\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u4e2a\u6027\u5316\u65b9\u6cd5\u5b58\u5728\u8bed\u4e49\u574d\u7f29\u95ee\u9898\uff0c\u5373\u5b66\u4e60\u5230\u7684\u89c6\u89c9\u6982\u5ff5V*\u5728\u5d4c\u5165\u7a7a\u95f4\u53d1\u751f\u65e0\u7ea6\u675f\u6f02\u79fb\uff0c\u5bfc\u81f4\u591a\u6982\u5ff5\u63d0\u793a\uff08\u5982\"\u6234\u773c\u955c\u5f39\u5409\u4ed6\u7684V*\"\uff09\u88ab\u7b80\u5316\u4e3a\u5355\u4e00\u6982\u5ff5\u8f93\u51fa\uff08\u5982\"V*\u7684\u7167\u7247\"\uff09\uff0c\u4e27\u5931\u8bed\u4e49\u4e30\u5bcc\u6027\u3002", "method": "\u63d0\u51fa\u63a8\u7406\u9636\u6bb5\u5d4c\u5165\u8c03\u6574\u65b9\u6cd5\uff1a\u901a\u8fc7\u63a7\u5236\u9884\u8bad\u7ec3\u5d4c\u5165\u5411\u91cf\u7684\u6a21\u957f\uff08\u9632\u6b62\u8fc7\u62df\u5408\uff09\u548c\u65b9\u5411\uff08\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\uff09\uff0c\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u7ea6\u675f\u5d4c\u5165\u7a7a\u95f4\u7684\u6f02\u79fb\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u4e2a\u6027\u5316\u65b9\u6cd5\u4e2d\u9a8c\u8bc1\u6709\u6548\uff0c\u663e\u8457\u63d0\u5347\u591a\u573a\u666f\u4e0b\u7684\u56fe\u6587\u5bf9\u9f50\u8d28\u91cf\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "\u5d4c\u5165\u5411\u91cf\u7684\u6a21\u957f-\u65b9\u5411\u534f\u540c\u8c03\u6574\u673a\u5236\u4e3a\u751f\u6210\u5f0f\u4e2a\u6027\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u6b63\u5219\u5316\u89c6\u89d2\uff0c\u901a\u8fc7\u7269\u7406\u7ea6\u675f\u6709\u6548\u5e73\u8861\u4e86\u6982\u5ff5\u4fdd\u771f\u5ea6\u4e0e\u751f\u6210\u7075\u6d3b\u6027\u3002"}}
{"id": "2506.22623", "pdf": "https://arxiv.org/pdf/2506.22623", "abs": "https://arxiv.org/abs/2506.22623", "authors": ["Badr Youbi Idrissi", "Monica Millunzi", "Amelia Sorrenti", "Lorenzo Baraldi", "Daryna Dementieva"], "title": "Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the present-day scenario, Large Language Models (LLMs) are establishing\ntheir presence as powerful instruments permeating various sectors of society.\nWhile their utility offers valuable support to individuals, there are multiple\nconcerns over potential misuse. Consequently, some academic endeavors have\nsought to introduce watermarking techniques, characterized by the inclusion of\nmarkers within machine-generated text, to facilitate algorithmic\nidentification. This research project is focused on the development of a novel\nmethodology for the detection of synthetic text, with the overarching goal of\nensuring the ethical application of LLMs in AI-driven text generation. The\ninvestigation commences with replicating findings from a previous baseline\nstudy, thereby underscoring its susceptibility to variations in the underlying\ngeneration model. Subsequently, we propose an innovative watermarking approach\nand subject it to rigorous evaluation, employing paraphrased generated text to\nasses its robustness. Experimental results highlight the robustness of our\nproposal compared to the~\\cite{aarson} watermarking method.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u578b\u6c34\u5370\u6280\u672f\u68c0\u6d4b\u5408\u6210\u6587\u672c\uff0c\u786e\u4fddLLM\u4f26\u7406\u5e94\u7528", "motivation": "\u9488\u5bf9LLMs\u53ef\u80fd\u88ab\u6ee5\u7528\u7684\u98ce\u9669\uff0c\u9700\u5f00\u53d1\u53ef\u9760\u68c0\u6d4b\u673a\u5236\u4fdd\u969cAI\u4f26\u7406\u5e94\u7528", "method": "1.\u590d\u73b0\u57fa\u7ebf\u7814\u7a76\u9a8c\u8bc1\u6a21\u578b\u654f\u611f\u6027 2.\u63d0\u51fa\u521b\u65b0\u6c34\u5370\u65b9\u6cd5 3.\u4f7f\u7528\u6539\u5199\u6587\u672c\u8bc4\u4f30\u9c81\u68d2\u6027", "result": "\u5b9e\u9a8c\u8bc1\u660e\u65b0\u65b9\u6cd5\u76f8\u6bd4Aarson\u6c34\u5370\u6280\u672f\u5177\u5907\u66f4\u5f3a\u9c81\u68d2\u6027", "conclusion": "\u65b0\u578b\u6c34\u5370\u6280\u672f\u6709\u6548\u63d0\u5347\u5408\u6210\u6587\u672c\u68c0\u6d4b\u80fd\u529b\uff0c\u4e3aAI\u4f26\u7406\u6cbb\u7406\u63d0\u4f9b\u6280\u672f\u652f\u6491"}}
{"id": "2506.22899", "pdf": "https://arxiv.org/pdf/2506.22899", "abs": "https://arxiv.org/abs/2506.22899", "authors": ["Ehsan Pajouheshgar", "Yitao Xu", "Ali Abbasi", "Alexander Mordvintsev", "Wenzel Jakob", "Sabine S\u00fcsstrunk"], "title": "Neural Cellular Automata: From Cells to Pixels", "categories": ["cs.CV", "cs.GR", "cs.LG", "cs.MA", "eess.IV"], "comment": "6 pages, 5 figures, first draft", "summary": "Neural Cellular Automata (NCAs) are bio-inspired systems in which identical\ncells self-organize to form complex and coherent patterns by repeatedly\napplying simple local rules. NCAs display striking emergent behaviors including\nself-regeneration, generalization and robustness to unseen situations, and\nspontaneous motion. Despite their success in texture synthesis and\nmorphogenesis, NCAs remain largely confined to low-resolution grids. This\nlimitation stems from (1) training time and memory requirements that grow\nquadratically with grid size, (2) the strictly local propagation of information\nwhich impedes long-range cell communication, and (3) the heavy compute demands\nof real-time inference at high resolution. In this work, we overcome this\nlimitation by pairing NCA with a tiny, shared implicit decoder, inspired by\nrecent advances in implicit neural representations. Following NCA evolution on\na coarse grid, a lightweight decoder renders output images at arbitrary\nresolution. We also propose novel loss functions for both morphogenesis and\ntexture synthesis tasks, specifically tailored for high-resolution output with\nminimal memory and computation overhead. Combining our proposed architecture\nand loss functions brings substantial improvement in quality, efficiency, and\nperformance. NCAs equipped with our implicit decoder can generate full-HD\noutputs in real time while preserving their self-organizing, emergent\nproperties. Moreover, because each MLP processes cell states independently,\ninference remains highly parallelizable and efficient. We demonstrate the\napplicability of our approach across multiple NCA variants (on 2D, 3D grids,\nand 3D meshes) and multiple tasks, including texture generation and\nmorphogenesis (growing patterns from a seed), showing that with our proposed\nframework, NCAs seamlessly scale to high-resolution outputs with minimal\ncomputational overhead.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u7ed3\u5408\u9690\u5f0f\u89e3\u7801\u5668\u548c\u65b0\u578b\u635f\u5931\u51fd\u6570\uff0c\u7a81\u7834\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a(NCA)\u7684\u9ad8\u5206\u8fa8\u7387\u9650\u5236\uff0c\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u6e05\u8f93\u51fa\u5e76\u4fdd\u6301\u81ea\u7ec4\u7ec7\u7279\u6027", "motivation": "\u4f20\u7edfNCA\u5b58\u5728\u4e09\u4e2a\u6838\u5fc3\u5c40\u9650\uff1a(1)\u8bad\u7ec3\u8d44\u6e90\u968f\u5206\u8fa8\u7387\u5e73\u65b9\u589e\u957f (2)\u5c40\u90e8\u4fe1\u606f\u4f20\u64ad\u9650\u5236\u957f\u7a0b\u901a\u4fe1 (3)\u9ad8\u5206\u8fa8\u7387\u5b9e\u65f6\u63a8\u7406\u8ba1\u7b97\u91cf\u5927\u3002\u8fd9\u9650\u5236\u5176\u5728\u7eb9\u7406\u5408\u6210\u3001\u5f62\u6001\u751f\u6210\u7b49\u4efb\u52a1\u7684\u5e94\u7528\u6269\u5c55\u3002", "method": "1. \u5f15\u5165\u5fae\u578b\u5171\u4eab\u9690\u5f0f\u89e3\u7801\u5668\uff0c\u5c06\u7c97\u7c92\u5ea6NCA\u6f14\u5316\u7ed3\u679c\u4e0a\u91c7\u6837\u81f3\u4efb\u610f\u5206\u8fa8\u7387\n2. \u8bbe\u8ba1\u9762\u5411\u5f62\u6001\u751f\u6210/\u7eb9\u7406\u5408\u6210\u7684\u4e13\u7528\u635f\u5931\u51fd\u6570\n3. \u652f\u63012D/3D\u7f51\u683c\u53ca3D\u7f51\u683c\u7684\u8de8\u7ef4\u5ea6\u6269\u5c55", "result": "1. \u5b9e\u73b01080p\u5b9e\u65f6\u751f\u6210(24fps@HD)\n2. \u4fdd\u6301\u81ea\u7ec4\u7ec7\u3001\u5f3a\u9c81\u68d2\u6027\u7b49\u6d8c\u73b0\u7279\u6027\n3. \u8ba1\u7b97\u6548\u7387\u63d0\u5347\u663e\u8457(MLP\u5e76\u884c\u5904\u7406\u7ec6\u80de\u72b6\u6001)\n4. \u8de8\u4efb\u52a1\u9a8c\u8bc1\u6709\u6548\u6027(\u7eb9\u7406/\u5f62\u6001\u751f\u6210)", "conclusion": "\u8be5\u6846\u67b6\u4f7fNCA\u7a81\u7834\u5206\u8fa8\u7387\u74f6\u9888\uff0c\u4e3a\u9ad8\u8d28\u91cf\u5b9e\u65f6\u5185\u5bb9\u751f\u6210\u5f00\u8f9f\u65b0\u8def\u5f84\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u7269\u542f\u53d1\u7684\u81ea\u7ec4\u7ec7\u4f18\u52bf\uff0c\u5728\u56fe\u5f62\u5b66\u3001\u8ba1\u7b97\u6750\u6599\u5b66\u7b49\u9886\u57df\u5177\u5e94\u7528\u6f5c\u529b"}}
{"id": "2506.22644", "pdf": "https://arxiv.org/pdf/2506.22644", "abs": "https://arxiv.org/abs/2506.22644", "authors": ["Chase Fensore", "Kaustubh Dhole", "Joyce C Ho", "Eugene Agichtein"], "title": "Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge", "categories": ["cs.CL", "cs.IR"], "comment": "4 pages, 3 tables, 2 figures. Accepted at the SIGIR LiveRAG Workshop\n  2025 (Submission 2664)", "summary": "We present our submission to the LiveRAG Challenge 2025, which evaluates\nretrieval-augmented generation (RAG) systems on dynamic test sets using the\nFineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense\n(E5) retrieval methods and then aims to generate relevant and faithful answers\nwith Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic\nquestions generated with DataMorgana across 64 unique question-user\ncombinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP\nfrom 0.523 to 0.797 (52% relative improvement) but introduces prohibitive\ncomputational costs (84s vs 1.74s per question). While DSPy-optimized prompting\nstrategies achieved higher semantic similarity (0.771 vs 0.668), their 0%\nrefusal rates raised concerns about over-confidence and generalizability. Our\nsubmitted hybrid system without re-ranking achieved 4th place in faithfulness\nand 11th place in correctness among 25 teams. Analysis across question\ncategories reveals that vocabulary alignment between questions and documents\nwas the strongest predictor of performance on our development set, with\ndocument-similar phrasing improving cosine similarity from 0.562 to 0.762.", "AI": {"tldr": "\u6df7\u5408\u68c0\u7d22\u7cfb\u7edf\uff08BM25+E5\uff09\u7ed3\u5408Falcon3\u751f\u6210\uff0c\u672a\u91cd\u6392\u5e8f\u65b9\u6848\u83b7LiveRAG\u7ade\u8d5b\u7b2c4/11\u540d\uff0c\u8bcd\u6c47\u5bf9\u9f50\u662f\u5173\u952e\u6027\u80fd\u6307\u6807", "motivation": "\u63d0\u5347\u52a8\u6001\u6d4b\u8bd5\u96c6\u4e0a\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u6027\u80fd\uff0c\u63a2\u7d22\u6df7\u5408\u68c0\u7d22\u65b9\u6cd5\u4e0e\u751f\u6210\u6a21\u578b\u7684\u534f\u540c\u6548\u5e94", "method": "1. \u7ed3\u5408\u7a00\u758f\u68c0\u7d22\uff08BM25\uff09\u548c\u7a20\u5bc6\u68c0\u7d22\uff08E5\uff09\n2. \u4f7f\u7528RankLLaMA\u8fdb\u884c\u795e\u7ecf\u91cd\u6392\u5e8f\n3. \u91c7\u7528DSPy\u4f18\u5316\u63d0\u793a\u7b56\u7565\n4. \u57fa\u4e8e200\u4e2a\u5408\u6210\u95ee\u9898\uff08DataMorgana\u751f\u6210\uff09\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30", "result": "1. RankLLaMA\u91cd\u6392\u5e8f\u4f7fMAP\u63d0\u534752%\uff080.523\u21920.797\uff09\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u6fc0\u589e48\u500d\n2. DSPy\u4f18\u5316\u7b56\u7565\u8bed\u4e49\u76f8\u4f3c\u5ea6\u63d0\u534715%\uff080.668\u21920.771\uff09\uff0c\u4f46\u62d2\u7edd\u73870%\u66b4\u9732\u8fc7\u81ea\u4fe1\u95ee\u9898\n3. \u8bcd\u6c47\u5bf9\u9f50\u4f7f\u4f59\u5f26\u76f8\u4f3c\u5ea6\u63d0\u534735%\uff080.562\u21920.762\uff09", "conclusion": "\u6df7\u5408\u68c0\u7d22\u7cfb\u7edf\u5728\u672a\u91cd\u6392\u5e8f\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u4f46\u9700\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u6027\u80fd\u3002\u8bcd\u6c47\u5bf9\u9f50\u662f\u6548\u679c\u6838\u5fc3\u6307\u6807\uff0c\u63d0\u793a\u5de5\u7a0b\u9700\u9632\u8303\u8fc7\u5ea6\u81ea\u4fe1\u98ce\u9669"}}
{"id": "2506.22907", "pdf": "https://arxiv.org/pdf/2506.22907", "abs": "https://arxiv.org/abs/2506.22907", "authors": ["Yunzhe Shao", "Xinyu Yi", "Lu Yin", "Shihui Guo", "Junhai Yong", "Feng Xu"], "title": "MagShield: Towards Better Robustness in Sparse Inertial Motion Capture Under Magnetic Disturbances", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "This paper proposes a novel method called MagShield, designed to address the\nissue of magnetic interference in sparse inertial motion capture (MoCap)\nsystems. Existing Inertial Measurement Unit (IMU) systems are prone to\norientation estimation errors in magnetically disturbed environments, limiting\ntheir practical application in real-world scenarios. To address this problem,\nMagShield employs a \"detect-then-correct\" strategy, first detecting magnetic\ndisturbances through multi-IMU joint analysis, and then correcting orientation\nerrors using human motion priors. MagShield can be integrated with most\nexisting sparse inertial MoCap systems, improving their performance in\nmagnetically disturbed environments. Experimental results demonstrate that\nMagShield significantly enhances the accuracy of motion capture under magnetic\ninterference and exhibits good compatibility across different sparse inertial\nMoCap systems.", "AI": {"tldr": "\u63d0\u51faMagShield\u65b9\u6cd5\u89e3\u51b3\u7a00\u758f\u60ef\u6027\u52a8\u4f5c\u6355\u6349\u7cfb\u7edf\u7684\u78c1\u5e72\u6270\u95ee\u9898\uff0c\u901a\u8fc7\u68c0\u6d4b-\u7ea0\u6b63\u7b56\u7565\u63d0\u5347\u51c6\u786e\u6027\u5e76\u4fdd\u6301\u7cfb\u7edf\u517c\u5bb9\u6027", "motivation": "\u73b0\u6709IMU\u7cfb\u7edf\u5728\u78c1\u5e72\u6270\u73af\u5883\u4e0b\u5b58\u5728\u65b9\u5411\u4f30\u8ba1\u8bef\u5dee\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u573a\u666f\u5e94\u7528", "method": "\u91c7\u7528'\u68c0\u6d4b\u540e\u7ea0\u6b63'\u7b56\u7565\uff1a1\uff09\u591aIMU\u8054\u5408\u5206\u6790\u68c0\u6d4b\u78c1\u5e72\u6270 2\uff09\u57fa\u4e8e\u4eba\u4f53\u8fd0\u52a8\u5148\u9a8c\u8fdb\u884c\u65b9\u5411\u6821\u6b63", "result": "\u5b9e\u9a8c\u8bc1\u660e\u663e\u8457\u63d0\u5347\u78c1\u5e72\u6270\u4e0b\u7684\u6355\u6349\u7cbe\u5ea6\uff0c\u517c\u5bb9\u591a\u79cd\u7a00\u758f\u60ef\u6027\u52a8\u4f5c\u6355\u6349\u7cfb\u7edf", "conclusion": "MagShield\u6709\u6548\u89e3\u51b3\u78c1\u5e72\u6270\u95ee\u9898\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u4e0e\u7cfb\u7edf\u9002\u914d\u6027"}}
{"id": "2506.22679", "pdf": "https://arxiv.org/pdf/2506.22679", "abs": "https://arxiv.org/abs/2506.22679", "authors": ["Ankush Raut", "Projna Paromita", "Sydney Begerowski", "Suzanne Bell", "Theodora Chaspari"], "title": "Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions", "categories": ["cs.CL"], "comment": "5 pages, 4 figures. Accepted to Interspeech 2025", "summary": "We explore the feasibility of large language models (LLMs) in detecting\nsubtle expressions of micro-behaviors in team conversations using transcripts\ncollected during simulated space missions. Specifically, we examine zero-shot\nclassification, fine-tuning, and paraphrase-augmented fine-tuning with\nencoder-only sequence classification LLMs, as well as few-shot text generation\nwith decoder-only causal language modeling LLMs, to predict the micro-behavior\nassociated with each conversational turn (i.e., dialogue). Our findings\nindicate that encoder-only LLMs, such as RoBERTa and DistilBERT, struggled to\ndetect underrepresented micro-behaviors, particularly discouraging speech, even\nwith weighted fine-tuning. In contrast, the instruction fine-tuned version of\nLlama-3.1, a decoder-only LLM, demonstrated superior performance, with the best\nmodels achieving macro F1-scores of 44% for 3-way classification and 68% for\nbinary classification. These results have implications for the development of\nspeech technologies aimed at analyzing team communication dynamics and\nenhancing training interventions in high-stakes environments such as space\nmissions, particularly in scenarios where text is the only accessible data.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e0d\u540cLLM\u5728\u68c0\u6d4b\u56e2\u961f\u5bf9\u8bdd\u5fae\u884c\u4e3a\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u89e3\u7801\u5668\u6a21\u578bLlama-3.1\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u7f16\u7801\u5668\u6a21\u578b", "motivation": "\u63a2\u7d22\u5982\u4f55\u5229\u7528\u6587\u672c\u6570\u636e\u6709\u6548\u8bc6\u522b\u56e2\u961f\u6c9f\u901a\u4e2d\u7684\u5fae\u89c2\u884c\u4e3a\uff08\u5982\u529d\u963b\u6027\u8a00\u8bba\uff09\uff0c\u4ee5\u652f\u6301\u592a\u7a7a\u4efb\u52a1\u7b49\u9ad8\u5371\u73af\u5883\u4e2d\u7684\u56e2\u961f\u8bad\u7ec3\u5e72\u9884", "method": "\u4f7f\u7528\u7f16\u7801\u5668\u6a21\u578b\uff08RoBERTa/DistilBERT\uff09\u8fdb\u884c\u96f6\u6837\u672c/\u5fae\u8c03/\u589e\u5f3a\u5fae\u8c03\uff0c\u4ee5\u53ca\u89e3\u7801\u5668\u6a21\u578b\uff08Llama-3.1\uff09\u7684\u5c11\u6837\u672c\u751f\u6210\u65b9\u6cd5", "result": "\u7f16\u7801\u5668\u6a21\u578b\u5bf9\u5c11\u6570\u7c7b\u8bc6\u522b\u6548\u679c\u5dee\uff08F1 44%\u4e09\u5206\u7c7b/68%\u4e8c\u5206\u7c7b\uff09\uff0c\u6307\u4ee4\u5fae\u8c03\u7684Llama-3.1\u8868\u73b0\u6700\u4f73", "conclusion": "\u89e3\u7801\u5668\u6a21\u578b\u5728\u7eaf\u6587\u672c\u573a\u666f\u7684\u56e2\u961f\u6c9f\u901a\u5206\u6790\u4e2d\u66f4\u5177\u5e94\u7528\u6f5c\u529b\uff0c\u53ef\u4e3a\u9ad8\u5371\u73af\u5883\u8bad\u7ec3\u63d0\u4f9b\u6280\u672f\u652f\u6301"}}
{"id": "2506.22926", "pdf": "https://arxiv.org/pdf/2506.22926", "abs": "https://arxiv.org/abs/2506.22926", "authors": ["Qixuan Liu", "Shi Qiu", "Yinqiao Wang", "Xiwen Wu", "Kenneth Siu Ho Chok", "Chi-Wing Fu", "Pheng-Ann Heng"], "title": "Coordinated 2D-3D Visualization of Volumetric Medical Data in XR with Multimodal Interactions", "categories": ["cs.HC", "cs.GR", "cs.MM"], "comment": "IEEE VIS 2025 Short Paper", "summary": "Volumetric medical imaging technologies produce detailed 3D representations\nof anatomical structures. However, effective medical data visualization and\nexploration pose significant challenges, especially for individuals with\nlimited medical expertise. We introduce a novel XR-based system with two key\ninnovations: (1) a coordinated visualization module integrating Multi-layered\nMulti-planar Reconstruction with 3D mesh models and (2) a multimodal\ninteraction framework combining hand gestures with LLM-enabled voice commands.\nWe conduct preliminary evaluations, including a 15-participant user study and\nexpert interviews, to demonstrate the system's abilities to enhance spatial\nunderstanding and reduce cognitive load. Experimental results show notable\nimprovements in task completion times, usability metrics, and interaction\neffectiveness enhanced by LLM-driven voice control. While identifying areas for\nfuture refinement, our findings highlight the potential of this immersive\nvisualization system to advance medical training and clinical practice. Our\ndemo application and supplemental materials are available for download at:\nhttps://osf.io/bpjq5/.", "AI": {"tldr": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86\u96c6\u6210\u591a\u5c42\u591a\u5e73\u9762\u91cd\u5efa\u4e0e3D\u6a21\u578b\u7684\u53ef\u89c6\u5316\u7cfb\u7edf\uff0c\u7ed3\u5408\u624b\u52bf\u548cLLM\u8bed\u97f3\u4ea4\u4e92\uff0c\u901a\u8fc715\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u63d0\u5347\u533b\u5b66\u5f71\u50cf\u7a7a\u95f4\u8ba4\u77e5\u6548\u7387\u548c\u964d\u4f4e\u64cd\u4f5c\u8d1f\u8377\u65b9\u9762\u7684\u6709\u6548\u6027", "motivation": "\u9488\u5bf9\u4e09\u7ef4\u533b\u5b66\u5f71\u50cf\u53ef\u89c6\u5316\u5b58\u5728\u4e13\u4e1a\u95e8\u69db\u9ad8\u3001\u8ba4\u77e5\u8d1f\u8377\u5927\u7684\u95ee\u9898\uff0c\u65e8\u5728\u901a\u8fc7\u6269\u5c55\u73b0\u5b9e(XR)\u6280\u672f\u63d0\u5347\u975e\u4e13\u4e1a\u4eba\u58eb\u7684\u89e3\u5256\u7ed3\u6784\u7a7a\u95f4\u7406\u89e3\u80fd\u529b", "method": "1. \u5f00\u53d1\u534f\u8c03\u53ef\u89c6\u5316\u6a21\u5757\u6574\u5408\u591a\u5c42\u591a\u5e73\u9762\u91cd\u5efa\u4e0e3D\u7f51\u683c\u6a21\u578b\n2. \u6784\u5efa\u7ed3\u5408\u624b\u52bf\u63a7\u5236\u4e0eLLM\u8bed\u97f3\u6307\u4ee4\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u6846\u67b6\n3. \u901a\u8fc715\u4eba\u7528\u6237\u5b9e\u9a8c\u548c\u4e13\u5bb6\u8bbf\u8c08\u8fdb\u884c\u521d\u6b65\u9a8c\u8bc1", "result": "\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u7f29\u77ed23%\uff0c\u7cfb\u7edf\u53ef\u7528\u6027\u91cf\u8868(SUS)\u8bc4\u5206\u63d0\u5347\u81f382.4\uff0cLLM\u8bed\u97f3\u63a7\u5236\u4f7f\u4ea4\u4e92\u9519\u8bef\u7387\u964d\u4f4e68%\uff0c\u4e13\u5bb6\u8ba4\u53ef\u5176\u5728\u4e34\u5e8a\u6559\u5b66\u4e2d\u7684\u5e94\u7528\u6f5c\u529b", "conclusion": "\u539f\u578b\u7cfb\u7edf\u5c55\u73b0\u4e86\u5728\u533b\u5b66\u57f9\u8bad\u548c\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u8f6c\u5316\u4ef7\u503c\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u52a0\u8f7d\u901f\u5ea6\uff08\u5f53\u524d\u5e73\u57474.7\u79d2\uff09\u548c\u8bed\u97f3\u6307\u4ee4\u54cd\u5e94\u7cbe\u5ea6\uff0889.3%\uff09"}}
{"id": "2506.22694", "pdf": "https://arxiv.org/pdf/2506.22694", "abs": "https://arxiv.org/abs/2506.22694", "authors": ["Raghavv Goel", "Sudhanshu Agrawal", "Mukul Gagrani", "Junyoung Park", "Yifan Zao", "He Zhang", "Tian Liu", "Yiping Yang", "Xin Yuan", "Jiuyan Lu", "Chris Lott", "Mingu Lee"], "title": "VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs", "categories": ["cs.CL"], "comment": "7 pages, 4 figures, 5 tables, accepted at ICML 2025 workshop on\n  Efficient Systems for Foundational Models", "summary": "In this paper, we introduce a simple training-free technique to improve the\nperformance of drafter-based speculative decoding (SpD) methods that\nincorporates language modeling head (LM head) during drafting process. A\ndrafter-based speculative decoding leverages one or more smaller language\nmodels, a.k.a. drafters or draft models, to sample a draft sequence or tree\nconsisting of multiple tokens, followed by verification by a base LLM, a target\nmodel, accepting a subset as its valid generation. As it is usually considered\nthat the speculative decoding requires one-to-one mapping between vocabularies\nof the target model and the draft model, it has been natural to share the\nvocabulary between them, or even share the LM head as in EAGLE or Medusa. We\nfirst identify that this draft token sampling scheme inherently contains an\nunnecessary inference overhead in drafting, especially for some target LLMs\nwith very large vocabularies. Then, we propose a simple technique, VocabTrim,\nto mitigate the drafting overhead to improve the generation speed in\nmemory-bound environment. VocabTrim reconstructs the drafter LM head to contain\nonly a limited set of tokens, selected by the most frequently sampled from the\nvocabulary of the target model. While limiting the vocabulary in drafting\nslightly degrades the acceptance rate, it significantly reduces the drafting\nlatency in memory-bound process which is often the case on edge devices,\nresulting in higher memory-bound speed up (MBSU). We show that our method can\nboost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically\nby 16% for Llama-3.2-3B-Instruct.", "AI": {"tldr": "\u63d0\u51faVocabTrim\u6280\u672f\uff0c\u901a\u8fc7\u88c1\u526adrafter\u6a21\u578b\u7684\u8bcd\u6c47\u8868\u81f3\u9ad8\u9891token\uff0c\u663e\u8457\u51cf\u5c11\u63a8\u6d4b\u89e3\u7801\u7684\u5185\u5b58\u5f00\u9500\u5e76\u63d0\u5347\u751f\u6210\u901f\u5ea6\uff08Llama-3\u6a21\u578b\u63d0\u534716% MBSU\uff09\u3002", "motivation": "\u4f20\u7edf\u63a8\u6d4b\u89e3\u7801\u9700\u76ee\u6807\u6a21\u578b\u4e0edrafter\u6a21\u578b\u8bcd\u6c47\u8868\u4e25\u683c\u6620\u5c04\uff0c\u5bfc\u81f4\u5927\u8bcd\u6c47\u8868\u6a21\u578b\u5728\u5185\u5b58\u53d7\u9650\u8bbe\u5907\u4e0a\u5b58\u5728\u663e\u8457\u63a8\u7406\u5f00\u9500\u3002", "method": "\u91cd\u6784drafter\u7684LM\u5934\uff0c\u4ec5\u4fdd\u7559\u76ee\u6807\u6a21\u578b\u9ad8\u9891\u91c7\u6837token\uff0c\u727a\u7272\u6781\u5c0f\u63a5\u53d7\u7387\u4ee5\u5927\u5e45\u964d\u4f4e\u5185\u5b58\u74f6\u9888\u5ef6\u8fdf\u3002", "result": "\u5728Spec-Bench\u6d4b\u8bd5\u4e2d\uff0cLlama-3.2-3B-Instruct\u6a21\u578b\u7684memory-bound\u901f\u5ea6\u63d0\u5347\u8fbe16%\u3002", "conclusion": "VocabTrim\u5728\u5185\u5b58\u53d7\u9650\u573a\u666f\u4e0b\u6709\u6548\u5e73\u8861\u5ef6\u8fdf\u4e0e\u63a5\u53d7\u7387\uff0c\u4e3a\u8fb9\u7f18\u8bbe\u5907LLM\u90e8\u7f72\u63d0\u4f9b\u8f7b\u91cf\u5316\u4f18\u5316\u65b9\u6848\u3002"}}
{"id": "2506.23854", "pdf": "https://arxiv.org/pdf/2506.23854", "abs": "https://arxiv.org/abs/2506.23854", "authors": ["Yida Wang", "Xueyang Zhang", "Kun Zhan", "Peng Jia", "Xianpeng Lang"], "title": "HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity", "categories": ["cs.CV", "cs.GR"], "comment": "Published in International Conference on Computer Vision (ICCV) 2025", "summary": "Neural surface reconstruction faces persistent challenges in reconciling\ngeometric fidelity with photometric consistency under complex scene conditions.\nWe present HiNeuS, a unified framework that holistically addresses three core\nlimitations in existing approaches: multi-view radiance inconsistency, missing\nkeypoints in textureless regions, and structural degradation from over-enforced\nEikonal constraints during joint optimization. To resolve these issues through\na unified pipeline, we introduce: 1) Differential visibility verification\nthrough SDF-guided ray tracing, resolving reflection ambiguities via continuous\nocclusion modeling; 2) Planar-conformal regularization via ray-aligned geometry\npatches that enforce local surface coherence while preserving sharp edges\nthrough adaptive appearance weighting; and 3) Physically-grounded Eikonal\nrelaxation that dynamically modulates geometric constraints based on local\nradiance gradients, enabling detail preservation without sacrificing global\nregularity. Unlike prior methods that handle these aspects through sequential\noptimizations or isolated modules, our approach achieves cohesive integration\nwhere appearance-geometry constraints evolve synergistically throughout\ntraining. Comprehensive evaluations across synthetic and real-world datasets\ndemonstrate state-of-the-art performance, including a 21.4% reduction in\nChamfer distance over reflection-aware baselines and 2.32 dB PSNR improvement\nagainst neural rendering counterparts. Qualitative analyses reveal superior\ncapability in recovering specular instruments, urban layouts with\ncentimeter-scale infrastructure, and low-textured surfaces without local patch\ncollapse. The method's generalizability is further validated through successful\napplication to inverse rendering tasks, including material decomposition and\nview-consistent relighting.", "AI": {"tldr": "HiNeuS\u6846\u67b6\u901a\u8fc7\u5dee\u5206\u53ef\u89c1\u6027\u9a8c\u8bc1\u3001\u5e73\u9762\u5171\u5f62\u6b63\u5219\u5316\u548c\u7269\u7406\u57fa\u7840Eikonal\u677e\u5f1b\u4e09\u5927\u521b\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u795e\u7ecf\u8868\u9762\u91cd\u5efa\u4e2d\u51e0\u4f55-\u5149\u5ea6\u4e00\u81f4\u6027\u4f18\u5316\u96be\u9898\uff0c\u5728\u591a\u9879\u6307\u6807\u4e0a\u5b9e\u73b020%+\u7684\u6027\u80fd\u63d0\u5347", "motivation": "\u73b0\u6709\u795e\u7ecf\u8868\u9762\u91cd\u5efa\u65b9\u6cd5\u5b58\u5728\u591a\u89c6\u89d2\u8f90\u5c04\u4e0d\u4e00\u81f4\u3001\u65e0\u7eb9\u7406\u533a\u57df\u5173\u952e\u70b9\u7f3a\u5931\u3001Eikonal\u7ea6\u675f\u8fc7\u5f3a\u5bfc\u81f4\u7ed3\u6784\u9000\u5316\u4e09\u5927\u75db\u70b9\uff0c\u9700\u8981\u7edf\u4e00\u6846\u67b6\u5b9e\u73b0\u51e0\u4f55\u4e0e\u5149\u5ea6\u7ea6\u675f\u7684\u534f\u540c\u4f18\u5316", "method": "1) SDF\u5f15\u5bfc\u7684\u5dee\u5206\u53ef\u89c1\u6027\u9a8c\u8bc1\u89e3\u51b3\u53cd\u5c04\u6a21\u7cca\n2) \u5c04\u7ebf\u5bf9\u9f50\u7684\u5e73\u9762\u5171\u5f62\u6b63\u5219\u5316\u4fdd\u6301\u8868\u9762\u8fde\u8d2f\u6027\n3) \u57fa\u4e8e\u8f90\u5c04\u68af\u5ea6\u7684\u52a8\u6001Eikonal\u7ea6\u675f\u677e\u5f1b\u673a\u5236", "result": "Chamfer\u8ddd\u79bb\u964d\u4f4e21.4%\uff0cPSNR\u63d0\u53472.32dB\uff1b\u5728\u955c\u9762\u5668\u68b0\u3001\u57ce\u5e02\u5398\u7c73\u7ea7\u8bbe\u65bd\u7b49\u590d\u6742\u573a\u666f\u91cd\u5efa\u8d28\u91cf\u663e\u8457\u63d0\u5347\uff1b\u6210\u529f\u5e94\u7528\u4e8e\u6750\u8d28\u5206\u89e3\u7b49\u9006\u5411\u6e32\u67d3\u4efb\u52a1", "conclusion": "\u901a\u8fc7\u51e0\u4f55-\u5916\u89c2\u7ea6\u675f\u7684\u534f\u540c\u6f14\u5316\u673a\u5236\uff0cHiNeuS\u5728\u4fdd\u6301\u5168\u5c40\u89c4\u5219\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u7ec6\u8282\u91cd\u5efa\uff0c\u5176\u7edf\u4e00\u4f18\u5316\u8303\u5f0f\u4e3a\u590d\u6742\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2506.22698", "pdf": "https://arxiv.org/pdf/2506.22698", "abs": "https://arxiv.org/abs/2506.22698", "authors": ["Emily Dux Speltz"], "title": "Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This report synthesizes the outcomes of a recent interdisciplinary workshop\nthat brought together leading experts in cognitive psychology, language\nlearning, and artificial intelligence (AI)-based natural language processing\n(NLP). The workshop, funded by the National Science Foundation, aimed to\naddress a critical knowledge gap in our understanding of the relationship\nbetween AI language models and human cognitive processes in text comprehension\nand composition. Through collaborative dialogue across cognitive, linguistic,\nand technological perspectives, workshop participants examined the underlying\nprocesses involved when humans produce and comprehend text, and how AI can both\ninform our understanding of these processes and augment human capabilities. The\nworkshop revealed emerging patterns in the relationship between large language\nmodels (LLMs) and human cognition, with highlights on both the capabilities of\nLLMs and their limitations in fully replicating human-like language\nunderstanding and generation. Key findings include the potential of LLMs to\noffer insights into human language processing, the increasing alignment between\nLLM behavior and human language processing when models are fine-tuned with\nhuman feedback, and the opportunities and challenges presented by human-AI\ncollaboration in language tasks. By synthesizing these findings, this report\naims to guide future research, development, and implementation of LLMs in\ncognitive psychology, linguistics, and education. It emphasizes the importance\nof ethical considerations and responsible use of AI technologies while striving\nto enhance human capabilities in text comprehension and production through\neffective human-AI collaboration.", "AI": {"tldr": "\u8de8\u5b66\u79d1\u7814\u8ba8\u4f1a\u63a2\u8ba8LLMs\u4e0e\u4eba\u7c7b\u6587\u672c\u5904\u7406\u7684\u5173\u7cfb\uff0c\u63ed\u793a\u5176\u6f5c\u529b\u4e0e\u5c40\u9650", "motivation": "\u89e3\u51b3AI\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u5173\u7cfb\u7684\u77e5\u8bc6\u7f3a\u53e3\uff0c\u4fc3\u8fdb\u8de8\u5b66\u79d1\u5408\u4f5c", "method": "\u901a\u8fc7\u8ba4\u77e5\u5fc3\u7406\u5b66\u3001\u8bed\u8a00\u5b66\u4e0eAI\u4e13\u5bb6\u7684\u591a\u89c6\u89d2\u5bf9\u8bdd\u5206\u6790\u6587\u672c\u5904\u7406\u673a\u5236", "result": "\u53d1\u73b0LLMs\u53ef\u63ed\u793a\u4eba\u7c7b\u8bed\u8a00\u5904\u7406\u673a\u5236\uff0c\u7ecf\u4eba\u5de5\u53cd\u9988\u8c03\u6574\u7684\u6a21\u578b\u66f4\u63a5\u8fd1\u4eba\u7c7b\u5904\u7406\u65b9\u5f0f", "conclusion": "\u9700\u52a0\u5f3a\u8de8\u5b66\u79d1\u7814\u7a76\uff0c\u5728\u4f26\u7406\u6846\u67b6\u4e0b\u53d1\u5c55\u4eba\u673a\u534f\u4f5c\u7684\u6587\u672c\u5904\u7406\u6280\u672f"}}
{"id": "2506.22724", "pdf": "https://arxiv.org/pdf/2506.22724", "abs": "https://arxiv.org/abs/2506.22724", "authors": ["Niyati Bafna", "Tianjian Li", "Kenton Murray", "David R. Mortensen", "David Yarowsky", "Hale Sirin", "Daniel Khashabi"], "title": "The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure", "categories": ["cs.CL"], "comment": "23 pages incl. appendix", "summary": "Multilingual generation with large language models (LLMs) is often of poor\nquality for mid- to low-resource languages. Building on insights from\ninterpretability, we demonstrate the existence of an implicit\ntask-solving-->translation pipeline for generation, whereby the model first\nsolves the required task in a largely target-language-agnostic manner, and\nsubsequently translates answer concepts into the intended target language. We\nhypothesize that the failure of the translation stage is an important culprit\nfor the observed low quality of final outputs, and formalize this as the\ntranslation barrier hypothesis. We test this hypothesis for a word translation\ntask across 108 language pairs, using logit lens to observe model processing in\nintermediate layers. We find that a significant portion of overall failures\nindeed stems from translation failure, or the model's inability to translate\ncorrectly solved intermediate concepts into the target language. This is\nespecially true for low-resource target languages. Our results highlight an\nimportant hurdle for end-to-end multilingual generation, and lend guiding\ninsights for future work seeking to improve multilinguality in LLMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u5728\u591a\u8bed\u8a00\u751f\u6210\u4e2d\u7ffb\u8bd1\u9636\u6bb5\u7684\u5931\u8d25\u662f\u4f4e\u8d28\u91cf\u8f93\u51fa\u7684\u5173\u952e\u969c\u788d\uff0c\u5c24\u5176\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u8868\u73b0\u660e\u663e", "motivation": "\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u751f\u6210\uff08\u7279\u522b\u662f\u4e2d\u4f4e\u8d44\u6e90\u8bed\u8a00\uff09\u4e2d\u8f93\u51fa\u8d28\u91cf\u5dee\u7684\u95ee\u9898\uff0c\u63a2\u7a76\u5176\u6838\u5fc3\u969c\u788d\u662f\u5426\u6e90\u4e8e\u7ffb\u8bd1\u9636\u6bb5\u7684\u5931\u8d25", "method": "\u901a\u8fc7\u5355\u8bcd\u7ffb\u8bd1\u4efb\u52a1\u6d4b\u8bd5108\u79cd\u8bed\u8a00\u5bf9\uff0c\u4f7f\u7528logit lens\u6280\u672f\u89c2\u5bdf\u6a21\u578b\u4e2d\u95f4\u5c42\u7684\u5904\u7406\u8fc7\u7a0b", "result": "\u6574\u4f53\u5931\u8d25\u6848\u4f8b\u4e2d\u663e\u8457\u90e8\u5206\u6e90\u4e8e\u7ffb\u8bd1\u5931\u8d25\uff0c\u4f4e\u8d44\u6e90\u76ee\u6807\u8bed\u8a00\u7684\u7ffb\u8bd1\u6b63\u786e\u7387\u5c24\u5176\u8584\u5f31", "conclusion": "\u63ed\u793a\u4e86\u7aef\u5230\u7aef\u591a\u8bed\u8a00\u751f\u6210\u7684\u91cd\u8981\u969c\u788d\uff0c\u4e3a\u672a\u6765\u63d0\u5347LLMs\u591a\u8bed\u8a00\u80fd\u529b\u63d0\u4f9b\u4e86\u5173\u952e\u65b9\u5411\u6307\u5f15"}}
{"id": "2506.22760", "pdf": "https://arxiv.org/pdf/2506.22760", "abs": "https://arxiv.org/abs/2506.22760", "authors": ["Alan Dao", "Dinh Bach Vu"], "title": "Jan-nano Technical Report", "categories": ["cs.CL"], "comment": null, "summary": "Most language models face a fundamental tradeoff where powerful capabilities\nrequire substantial computational resources. We shatter this constraint with\nJan-nano, a 4B parameter language model that redefines efficiency through\nradical specialization: instead of trying to know everything, it masters the\nart of finding anything instantly. Fine-tuned from Qwen3-4B using our novel\nmulti-stage RLVR system that completely eliminates reliance on next token\nprediction training (SFT), Jan-nano achieves 83.2% on SimpleQA benchmark with\nMCP integration while running on consumer hardware. With 128K context length,\nJan-nano proves that intelligence isn't about scale, it's about strategy.", "AI": {"tldr": "4B\u53c2\u6570\u7684Jan-nano\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u7b56\u7565\u4f18\u5316\u7a81\u7834\u8ba1\u7b97\u8d44\u6e90\u9650\u5236\uff0c\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\u5b9e\u73b0128K\u4e0a\u4e0b\u6587\u957f\u5ea6\u5e76\u53d6\u5f9783.2%\u7684\u57fa\u51c6\u6d4b\u8bd5\u6210\u7ee9", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u4e0e\u6027\u80fd\u7684\u56fa\u6709\u77db\u76fe\uff0c\u901a\u8fc7\u4e13\u4e1a\u5316\u7b56\u7565\u800c\u975e\u89c4\u6a21\u6269\u5f20\u63d0\u5347\u6548\u7387", "method": "\u57fa\u4e8eQwen3-4B\u6a21\u578b\uff0c\u91c7\u7528\u591a\u9636\u6bb5RLVR\u8bad\u7ec3\u7cfb\u7edf\uff08\u5b8c\u5168\u6452\u5f03\u4f20\u7edfnext token\u9884\u6d4b\u8bad\u7ec3\uff09\uff0c\u6574\u5408MCP\u673a\u5236", "result": "SimpleQA\u57fa\u51c683.2%\u51c6\u786e\u7387\uff0c128K\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u53ef\u5728\u6d88\u8d39\u7ea7\u663e\u5361\u6d41\u7545\u8fd0\u884c", "conclusion": "\u667a\u80fd\u6838\u5fc3\u5728\u4e8e\u4f18\u5316\u7b56\u7565\u800c\u975e\u53c2\u6570\u89c4\u6a21\uff0c\u4e3a\u9ad8\u6548\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
