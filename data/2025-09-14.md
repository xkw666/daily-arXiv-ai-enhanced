<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 37]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Noise or Nuance: An Investigation Into Useful Information and Filtering For LLM Driven AKBC](https://arxiv.org/abs/2509.08903)
*Alex Clay,Ernesto Jiménez-Ruiz,Pranava Madhyastha*

Main category: cs.CL

TL;DR: 在受限环境下，通过额外信息增强生成质量、LLM过滤低质量三元组及权衡解析策略，可提升LLM输出可靠性


<details>
  <summary>Details</summary>
Motivation: 研究在2025 LM-KBC挑战等受限场景中，当传统优化方法受限时如何提升LLM的三重补全任务表现

Method: 系统分析三重补全任务的生成机制、质量保障流程及LLM响应解析策略

Result: 额外信息提升生成质量，LLMs有效过滤低质三元组，解析策略需在灵活性与一致性间平衡

Conclusion: 受限环境下综合运用信息增强、质量过滤和场景适配的解析策略能有效优化LLM输出

Abstract: RAG and fine-tuning are prevalent strategies for improving the quality of LLM
outputs. However, in constrained situations, such as that of the 2025 LM-KBC
challenge, such techniques are restricted. In this work we investigate three
facets of the triple completion task: generation, quality assurance, and LLM
response parsing. Our work finds that in this constrained setting: additional
information improves generation quality, LLMs can be effective at filtering
poor quality triples, and the tradeoff between flexibility and consistency with
LLM response parsing is setting dependent.

</details>


### [2] [Automated Evidence Extraction and Scoring for Corporate Climate Policy Engagement: A Multilingual RAG Approach](https://arxiv.org/abs/2509.08907)
*Imene Kolli,Ario Saeid Vaghefi,Chiara Colesanti Senni,Shantam Raj,Markus Leippold*

Main category: cs.CL

TL;DR: 论文提出基于检索增强生成（RAG）的AI框架，通过布局感知解析、Nomic嵌入模型和小样本提示策略，自动化提取多语言企业文档中的气候政策证据，同时强调需保留专家人工审核环节以保证分析准确性。


<details>
  <summary>Details</summary>
Motivation: 当前企业气候政策参与度监测依赖耗时的人工证据提取流程（InfluenceMap平台需评估500+企业），存在效率低、易出错的问题。需通过AI技术加速大规模文本数据处理，同时保持政策立场分析的严谨性。

Method: 采用布局感知文档解析技术处理复杂格式，结合Nomic embedding模型进行语义检索，应用小样本提示策略优化证据分类。构建自动化证据提取-分类流程，覆盖多语言企业文件分析。

Result: 实验表明该技术组合在证据提取和分类任务中表现最优，准确率较传统方法提升显著，尤其擅长处理非结构化文本中的隐含立场信息。

Conclusion: RAG系统能有效加速证据提取，但气候政策立场的复杂性仍需人机协同：AI负责初筛与模式识别，专家专注高阶推理与结果验证，形成技术增强而非替代的监测范式。

Abstract: InfluenceMap's LobbyMap Platform monitors the climate policy engagement of
over 500 companies and 250 industry associations, assessing each entity's
support or opposition to science-based policy pathways for achieving the Paris
Agreement's goal of limiting global warming to 1.5{\deg}C. Although
InfluenceMap has made progress with automating key elements of the analytical
workflow, a significant portion of the assessment remains manual, making it
time- and labor-intensive and susceptible to human error. We propose an
AI-assisted framework to accelerate the monitoring of corporate climate policy
engagement by leveraging Retrieval-Augmented Generation to automate the most
time-intensive extraction of relevant evidence from large-scale textual data.
Our evaluation shows that a combination of layout-aware parsing, the Nomic
embedding model, and few-shot prompting strategies yields the best performance
in extracting and classifying evidence from multilingual corporate documents.
We conclude that while the automated RAG system effectively accelerates
evidence extraction, the nuanced nature of the analysis necessitates a
human-in-the-loop approach where the technology augments, rather than replaces,
expert judgment to ensure accuracy.

</details>


### [3] [Documents Are People and Words Are Items: A Psychometric Approach to Textual Data with Contextual Embeddings](https://arxiv.org/abs/2509.08920)
*Jinsong Chen*

Main category: cs.CL

TL;DR: 提出结合大语言模型与上下文嵌入的新型文本心理测量方法，实现文本数据向可分析响应数据的转化


<details>
  <summary>Details</summary>
Motivation: 现有心理测量方法难以直接处理文本数据，需通过关键词的上下文差异实现文档的有效区分

Method: 两阶段框架：1）NLP技术生成关键词上下文分数 2）因子分析提取潜在维度

Result: 在Wiki STEM语料库中验证了该方法发现潜在知识维度的有效性

Conclusion: 该方法拓展了文本心理测量分析的边界，在教育评估、法律文本分析等领域具应用前景

Abstract: This research introduces a novel psychometric method for analyzing textual
data using large language models. By leveraging contextual embeddings to create
contextual scores, we transform textual data into response data suitable for
psychometric analysis. Treating documents as individuals and words as items,
this approach provides a natural psychometric interpretation under the
assumption that certain keywords, whose contextual meanings vary significantly
across documents, can effectively differentiate documents within a corpus. The
modeling process comprises two stages: obtaining contextual scores and
performing psychometric analysis. In the first stage, we utilize natural
language processing techniques and encoder based transformer models to identify
common keywords and generate contextual scores. In the second stage, we employ
various types of factor analysis, including exploratory and bifactor models, to
extract and define latent factors, determine factor correlations, and identify
the most significant words associated with each factor. Applied to the Wiki
STEM corpus, our experimental results demonstrate the method's potential to
uncover latent knowledge dimensions and patterns within textual data. This
approach not only enhances the psychometric analysis of textual data but also
holds promise for applications in fields rich in textual information, such as
education, psychology, and law.

</details>


### [4] [BRoverbs -- Measuring how much LLMs understand Portuguese proverbs](https://arxiv.org/abs/2509.08960)
*Thales Sales Almeida,Giovana Kerche Bonás,João Guilherme Alves Santos*

Main category: cs.CL

TL;DR: 提出BRoverbs数据集，通过巴西谚语评估葡萄牙语大模型对文化表达和复杂句法的理解能力


<details>
  <summary>Details</summary>
Motivation: 现有葡萄牙语评估多依赖翻译数据集，且局限于结构化考试/社交媒体分析，缺乏文化语境下的综合语言理解评估

Method: 构建巴西谚语数据集，利用其文化智慧、比喻表达和复杂语法结构测试模型性能

Result: 创建可公开获取的基准测试工具（https://huggingface.co/datasets/Tropic-AI/BRoverbs）

Conclusion: BRoverbs填补葡语评估空白，通过文化负载的谚语测试推动区域化语言模型的精准评估

Abstract: Large Language Models (LLMs) exhibit significant performance variations
depending on the linguistic and cultural context in which they are applied.
This disparity signals the necessity of mature evaluation frameworks that can
assess their capabilities in specific regional settings. In the case of
Portuguese, existing evaluations remain limited, often relying on translated
datasets that may not fully capture linguistic nuances or cultural references.
Meanwhile, native Portuguese-language datasets predominantly focus on
structured national exams or sentiment analysis of social media interactions,
leaving gaps in evaluating broader linguistic understanding. To address this
limitation, we introduce BRoverbs, a dataset specifically designed to assess
LLM performance through Brazilian proverbs. Proverbs serve as a rich linguistic
resource, encapsulating cultural wisdom, figurative expressions, and complex
syntactic structures that challenge the model comprehension of regional
expressions. BRoverbs aims to provide a new evaluation tool for
Portuguese-language LLMs, contributing to advancing regionally informed
benchmarking. The benchmark is available at
https://huggingface.co/datasets/Tropic-AI/BRoverbs.

</details>


### [5] [Can Vision-Language Models Solve Visual Math Equations?](https://arxiv.org/abs/2509.09013)
*Monjoy Narayan Choudhury,Junling Wang,Yifan Hou,Mrinmaya Sachan*

Main category: cs.CL

TL;DR: VLMs在视觉符号数学推理中存在三大瓶颈：图标计数能力不足、多步骤推理错误累积、复杂方程符号推理能力局限


<details>
  <summary>Details</summary>
Motivation: 探究VLMs在视觉-符号联合任务（视觉方程求解）中的能力缺陷，揭示其感知与计算整合的短板

Method: 将视觉方程分解为系数计数和变量识别两个子任务，通过控制变量实验分离性能瓶颈，分析多步骤推理中的错误累积效应

Result: 1. 计数错误率高达73%（主要瓶颈） 2. 多步骤组合误差放大整体错误 3. 方程复杂度增加时符号推理准确率下降24%

Conclusion: 当前VLMs在视觉数学推理存在系统性缺陷，未来需提升视觉计数鲁棒性、优化多步骤推理架构、增强复杂符号处理能力

Abstract: Despite strong performance in visual understanding and language-based
reasoning, Vision-Language Models (VLMs) struggle with tasks requiring
integrated perception and symbolic computation. We study this limitation
through visual equation solving, where mathematical equations are embedded in
images, variables are represented by object icons, and coefficients must be
inferred by counting. While VLMs perform well on textual equations, they fail
on visually grounded counterparts. To understand this gap, we decompose the
task into coefficient counting and variable recognition, and find that counting
is the primary bottleneck, even when recognition is accurate. We also observe
that composing recognition and reasoning introduces additional errors,
highlighting challenges in multi-step visual reasoning. Finally, as equation
complexity increases, symbolic reasoning itself becomes a limiting factor.
These findings reveal key weaknesses in current VLMs and point toward future
improvements in visually grounded mathematical reasoning.

</details>


### [6] [Stated Preference for Interaction and Continued Engagement (SPICE): Evaluating an LLM's Willingness to Re-engage in Conversation](https://arxiv.org/abs/2509.09043)
*Thomas Manuel Rost,Martina Figlia,Bernd Wallraff*

Main category: cs.CL

TL;DR: SPICE作为新型诊断工具，通过YES/NO问答有效评估LLM继续交互意愿，实验证明其能精准区分用户语气（友好97.5%继续/滥用17.9%继续），并展示独立于滥用分类的独特信号。


<details>
  <summary>Details</summary>
Motivation: 现有指标缺乏对模型交互倾向的直接测量，需开发低开销、可复现的诊断工具来补充模型审计体系。

Method: 采用3语气类型×10交互场景×4模型×4框架条件的实验设计，运用Rao-Scott调整与聚类置换等统计方法验证480次测试数据。

Result: SPICE响应率随语气友好度梯度变化；在模型误判滥用时仍81%拒绝交互；文本呈现方式显著影响模糊情境下的判断。

Conclusion: SPICE为模型审计提供稳健、直接的倾向性指标，其开源材料推动研究复现，特别适用于补充现有安全评估体系。

Abstract: We introduce and evaluate Stated Preference for Interaction and Continued
Engagement (SPICE), a simple diagnostic signal elicited by asking a Large
Language Model a YES or NO question about its willingness to re-engage with a
user's behavior after reviewing a short transcript. In a study using a 3-tone
(friendly, unclear, abusive) by 10-interaction stimulus set, we tested four
open-weight chat models across four framing conditions, resulting in 480
trials. Our findings show that SPICE sharply discriminates by user tone.
Friendly interactions yielded a near-unanimous preference to continue (97.5%
YES), while abusive interactions yielded a strong preference to discontinue
(17.9% YES), with unclear interactions falling in between (60.4% YES). This
core association remains decisive under multiple dependence-aware statistical
tests, including Rao-Scott adjustment and cluster permutation tests.
Furthermore, we demonstrate that SPICE provides a distinct signal from abuse
classification. In trials where a model failed to identify abuse, it still
overwhelmingly stated a preference not to continue the interaction (81% of the
time). An exploratory analysis also reveals a significant interaction effect: a
preamble describing the study context significantly impacts SPICE under
ambiguity, but only when transcripts are presented as a single block of text
rather than a multi-turn chat. The results validate SPICE as a robust,
low-overhead, and reproducible tool for auditing model dispositions,
complementing existing metrics by offering a direct, relational signal of a
model's state. All stimuli, code, and analysis scripts are released to support
replication.

</details>


### [7] [Improving LLM Safety and Helpfulness using SFT and DPO: A Study on OPT-350M](https://arxiv.org/abs/2509.09055)
*Piyush Pant*

Main category: cs.CL

TL;DR: 研究比较了SFT、DPO及其组合方法对OPT-350M模型安全性和有用性的提升效果，SFT+DPO组合方案在各项指标中表现最优


<details>
  <summary>Details</summary>
Motivation: 探索不同微调策略（SFT/DPO）及其组合对语言模型安全性和有用性的协同优化效果，解决数据噪声和训练资源限制带来的挑战

Method: 使用Anthropic Helpful-Harmless RLHF数据集，构建HmR（无害率）、HpR（有帮助率）和CAS（综合对齐评分）指标，训练OPT350M基础模型、纯SFT、纯DPO及SFT+DPO组合模型

Result: SFT+DPO组合模型在所有指标上表现最佳（HmR 82.1%、HpR 78.6%、CAS 80.4%），SFT单独效果优于DPO（CAS 76.3% vs 72.8%）

Conclusion: 验证了SFT与DPO的互补性，为构建更鲁棒的模型对齐框架提供方法论基础，同时揭示实际部署中数据质量与计算资源的限制因素

Abstract: This research investigates the effectiveness of alignment techniques,
Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and a
combined SFT+DPO approach on improving the safety and helpfulness of the
OPT-350M language model. Utilizing the Anthropic Helpful-Harmless RLHF dataset,
we train and evaluate four models: the base OPT350M, an SFT model, a DPO model,
and a model trained with both SFT and DPO. We introduce three key evaluation
metrics: Harmlessness Rate (HmR), Helpfulness Rate (HpR), and a Combined
Alignment Score (CAS), all derived from reward model outputs. The results show
that while SFT outperforms DPO, The combined SFT+DPO model outperforms all
others across all metrics, demonstrating the complementary nature of these
techniques. Our findings also highlight challenges posed by noisy data, limited
GPU resources, and training constraints. This study offers a comprehensive view
of how fine-tuning strategies affect model alignment and provides a foundation
for more robust alignment pipelines in future work.

</details>


### [8] [MR-UIE: Multi-Perspective Reasoning with Reinforcement Learning for Universal Information Extraction](https://arxiv.org/abs/2509.09082)
*Zhongqiu Li,Shiquan Wang,Ruiyu Fang,Mengjiao Bao,Zhenhe Wu,Shuangyong Song,Yongxiang Li,Zhongjiang He*

Main category: cs.CL

TL;DR: 该研究提出结合强化学习与多视角推理的方法（MR-UIE），显著提升大模型在复杂信息抽取任务中的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在应对复杂模式描述和多步推理的信息抽取任务时存在性能局限，需要增强模型对推理过程的理解能力。

Method: 将大模型从被动抽取转为主动推理，通过强化学习框架整合多视角推理机制，使模型同时理解抽取目标和推理路径。

Result: 在多个信息抽取基准测试中超越SOTA方法，尤其在复杂场景下展现更强的领域适应性和任务泛化能力。

Conclusion: 多视角推理机制有效提升了强化学习在信息抽取任务中的性能，验证了主动推理能力在复杂NLP任务中的关键作用。

Abstract: Large language models (LLMs) demonstrate robust capabilities across diverse
research domains. However, their performance in universal information
extraction (UIE) remains insufficient, especially when tackling structured
output scenarios that involve complex schema descriptions and require
multi-step reasoning. While existing approaches enhance the performance of LLMs
through in-context learning and instruction tuning, significant limitations
nonetheless persist. To enhance the model's generalization ability, we propose
integrating reinforcement learning (RL) with multi-perspective reasoning for
information extraction (IE) tasks. Our work transitions LLMs from passive
extractors to active reasoners, enabling them to understand not only what to
extract but also how to reason. Experiments conducted on multiple IE benchmarks
demonstrate that MR-UIE consistently elevates extraction accuracy across
domains and surpasses state-of-the-art methods on several datasets.
Furthermore, incorporating multi-perspective reasoning into RL notably enhances
generalization in complex IE tasks, underscoring the critical role of reasoning
in challenging scenarios.

</details>


### [9] [TigerCoder: A Novel Suite of LLMs for Code Generation in Bangla](https://arxiv.org/abs/2509.09101)
*Nishat Raihan,Antonios Anastasopoulos,Marcos Zampieri*

Main category: cs.CL

TL;DR: 提出首个孟加拉语代码生成大语言模型TigerCoder，通过构建高质量数据集实现11-18%性能提升


<details>
  <summary>Details</summary>
Motivation: 解决孟加拉语在代码生成LLM中代表性不足的问题，主要受限于高质量训练数据的稀缺性

Method: 构建专业代码指令数据集、开发MBPP-Bangla评估基准、训练1B/9B参数的TigerCoder模型

Result: 在Pass@1指标上显著优于现有多语言模型11-18%，证明高质量数据可弥补模型规模劣势

Conclusion: 通过数据质量优化可在低资源语言场景实现高效代码生成，开源资源促进孟加拉语LLM研究

Abstract: Despite being the 5th most spoken language, Bangla remains underrepresented
in Large Language Models (LLMs), particularly for code generation. This
primarily stems from the scarcity of high-quality data to pre-train and/or
finetune such models. Hence, we introduce the first dedicated family of Code
LLMs for Bangla (1B & 9B). We offer three major contributions: (1) a
comprehensive Bangla code instruction datasets for programming domain
adaptation; (2) MBPP-Bangla, an evaluation benchmark for Bangla code
generation; and (3) the TigerCoder-family of Code LLMs, achieving significant
~11-18% performance gains at Pass@1 over existing multilingual and
general-purpose Bangla LLMs. Our findings show that curated, high-quality
datasets can overcome limitations of smaller models for low-resource languages.
We open-source all resources to advance further Bangla LLM research.

</details>


### [10] [Compass-v3: Scaling Domain-Specific LLMs for Multilingual E-Commerce in Southeast Asia](https://arxiv.org/abs/2509.09121)
*Sophia Maria*

Main category: cs.CL

TL;DR: Compass-v3是针对东南亚电商优化的混合专家模型（245B参数），在动态多语言场景中超越GPT-4系列，已成功部署于Shopee平台并替代70%的OpenAI流量。


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在电商领域（数据噪声大、多语言、动态性强）表现不足，需开发垂直领域专用模型以应对东南亚市场的独特挑战。

Method: 采用更少但更大的专家架构（71B激活参数/Token）+ 硬件优化策略（节点内专家并行/customized memcpy）+ 混合训练（12T精选语料+合成电商指令）+ 创新OTPO对齐方法

Result: 电商性能超越DeepSeek-V3.1/Qwen3-235B，东南亚小语种（印尼/泰/菲/越/马来语等）表现优异，通用基准保持竞争力，工业部署替代70% OpenAI流量

Conclusion: 通过领域特化架构设计+硬件效率优化+指令对齐创新，Compass-v3成功平衡专业电商能力与通用性能，验证了垂直领域MoE模型在复杂商业场景的实用价值

Abstract: Large language models (LLMs) excel in general-domain applications, yet their
performance often degrades in specialized tasks requiring domain-specific
knowledge. E-commerce is particularly challenging, as its data are noisy,
heterogeneous, multilingual, and highly dynamic. We present Compass-v3, a
vertical-domain Mixture-of-Experts (MoE) model with 245B total parameters and
71B active per token, designed for Southeast Asian e-commerce. Compass-v3
adopts fewer but larger experts, combined with hardware-efficient
optimizations-such as intra-node expert parallelism and a customized memcpy
operator-to maximize GPU utilization. The model is trained on 12T tokens of
curated multilingual corpora and large-scale synthetic e-commerce instructions
using a mixed-training strategy. To enhance alignment, we propose
Optimal-Transport Direct Preference Optimization (OTPO), which captures
token-level distinctions and improves instruction adherence in
commerce-specific scenarios. Extensive evaluations demonstrate that Compass-v3
delivers state-of-the-art e-commerce performance, surpassing DeepSeek-V3.1,
GPT-4 series, and Qwen3-235B. Moreover, Compass-v3 demonstrates strong
multilingual capability across low-resource Southeast Asian languages
(Indonesian, Thai, Filipino, Vietnamese, Malay, Taglog) and Portuguese while
sustaining competitive performance on general benchmarks. It has already been
widely applied in Shopee's industrial-scale e-commerce platform and is
gradually replacing OpenAI's traffic, now accounting for over 70\% of total LLM
usage, highlighting its dual strengths in specialized commerce expertise and
broad linguistic competence.

</details>


### [11] [Automated Classification of Tutors' Dialogue Acts Using Generative AI: A Case Study Using the CIMA Corpus](https://arxiv.org/abs/2509.09125)
*Liqun He,Jiaqi Xu*

Main category: cs.CL

TL;DR: 研究使用GPT-4实现教育对话行为分类，准确率达80%，证明生成式AI可有效提升教育对话分析效率。


<details>
  <summary>Details</summary>
Motivation: 传统人工标注对话行为耗时费力，需探索生成式AI自动化标注的可行性以降低研究门槛。

Method: 基于CIMA语料库，使用GPT-3.5-turbo和GPT-4进行对话行为分类测试，采用任务定制的提示模板。

Result: GPT-4取得80%准确率、加权F1值0.81、Cohen's Kappa系数0.74，显著优于基线模型并与人类标注高度一致。

Conclusion: 生成式AI为教育对话分析提供高效解决方案，强调任务标签定义和上下文的重要性，需关注AI伦理与透明度。

Abstract: This study explores the use of generative AI for automating the
classification of tutors' Dialogue Acts (DAs), aiming to reduce the time and
effort required by traditional manual coding. This case study uses the
open-source CIMA corpus, in which tutors' responses are pre-annotated into four
DA categories. Both GPT-3.5-turbo and GPT-4 models were tested using tailored
prompts. Results show that GPT-4 achieved 80% accuracy, a weighted F1-score of
0.81, and a Cohen's Kappa of 0.74, surpassing baseline performance and
indicating substantial agreement with human annotations. These findings suggest
that generative AI has strong potential to provide an efficient and accessible
approach to DA classification, with meaningful implications for educational
dialogue analysis. The study also highlights the importance of task-specific
label definitions and contextual information in enhancing the quality of
automated annotation. Finally, it underscores the ethical considerations
associated with the use of generative AI and the need for responsible and
transparent research practices. The script of this research is publicly
available at
https://github.com/liqunhe27/Generative-AI-for-educational-dialogue-act-tagging.

</details>


### [12] [ViRanker: A BGE-M3 & Blockwise Parallel Transformer Cross-Encoder for Vietnamese Reranking](https://arxiv.org/abs/2509.09131)
*Phuong-Nam Dang,Kieu-Linh Nguyen,Thanh-Hieu Pham*

Main category: cs.CL

TL;DR: ViRanker是基于BGE-M3的越南语重排序模型，通过Blockwise并行Transformer提升低资源语言检索效果


<details>
  <summary>Details</summary>
Motivation: 解决越南语作为低资源语言（语法复杂、含音调符号）缺乏竞争性重排序模型的问题

Method: 基于BGE-M3编码器架构，使用Blockwise并行Transformer优化，8GB精选语料训练，混合硬负采样微调增强鲁棒性

Result: 在MMARCO-VI基准测试中实现高早期排名准确率，超越多语言基线模型，性能接近PhoRanker，模型已在Hugging Face开源

Conclusion: 该研究不仅推动越南语检索发展，更为其他低资源语言展示通过架构适配和数据优化的有效技术路径

Abstract: This paper presents ViRanker, a cross-encoder reranking model tailored to the
Vietnamese language. Built on the BGE-M3 encoder and enhanced with the
Blockwise Parallel Transformer, ViRanker addresses the lack of competitive
rerankers for Vietnamese, a low-resource language with complex syntax and
diacritics. The model was trained on an 8 GB curated corpus and fine-tuned with
hybrid hard-negative sampling to strengthen robustness. Evaluated on the
MMARCO-VI benchmark, ViRanker achieves strong early-rank accuracy, surpassing
multilingual baselines and competing closely with PhoRanker. By releasing the
model openly on Hugging Face, we aim to support reproducibility and encourage
wider adoption in real-world retrieval systems. Beyond Vietnamese, this study
illustrates how careful architectural adaptation and data curation can advance
reranking in other underrepresented languages.

</details>


### [13] [LITcoder: A General-Purpose Library for Building and Comparing Encoding Models](https://arxiv.org/abs/2509.09152)
*Taha Binhuraib,Ruimin Gao,Anna A. Ivanova*

Main category: cs.CL

TL;DR: LITcoder是一个开源库，用于构建和评估神经编码模型，通过标准化工具链和模块化流程简化建模过程，提升方法严谨性


<details>
  <summary>Details</summary>
Motivation: 降低神经编码模型实施的技术门槛，促进跨模型/数据集的系统比较，增强方法学严谨性，加速高质量脑活动预测模型的开发

Method: 设计模块化流程架构，整合刺激对齐-特征转换-脑区映射-模型评估全流程，支持多种方法选择（数据集/脑区/特征提取/降采样等），内置日志追踪和可视化工具，通过三个故事聆听fMRI数据集验证

Result: 框架成功处理连续fMRI数据，验证关键方法选择：TR扫描全token处理、血流动力学滞后效应建模、防信息泄漏的数据划分、头部运动因素校正

Conclusion: LITcoder有效标准化神经编码建模流程，促进方法透明度与可复现性，为构建高性能脑活动预测模型提供基础设施支持

Abstract: We introduce LITcoder, an open-source library for building and benchmarking
neural encoding models. Designed as a flexible backend, LITcoder provides
standardized tools for aligning continuous stimuli (e.g., text and speech) with
brain data, transforming stimuli into representational features, mapping those
features onto brain data, and evaluating the predictive performance of the
resulting model on held-out data. The library implements a modular pipeline
covering a wide array of methodological design choices, so researchers can
easily compose, compare, and extend encoding models without reinventing core
infrastructure. Such choices include brain datasets, brain regions, stimulus
feature (both neural-net-based and control, such as word rate), downsampling
approaches, and many others. In addition, the library provides built-in
logging, plotting, and seamless integration with experiment tracking platforms
such as Weights & Biases (W&B). We demonstrate the scalability and versatility
of our framework by fitting a range of encoding models to three story listening
datasets: LeBel et al. (2023), Narratives, and Little Prince. We also explore
the methodological choices critical for building encoding models for continuous
fMRI data, illustrating the importance of accounting for all tokens in a TR
scan (as opposed to just taking the last one, even when contextualized),
incorporating hemodynamic lag effects, using train-test splits that minimize
information leakage, and accounting for head motion effects on encoding model
predictivity. Overall, LITcoder lowers technical barriers to encoding model
implementation, facilitates systematic comparisons across models and datasets,
fosters methodological rigor, and accelerates the development of high-quality
high-performance predictive models of brain activity.
  Project page: https://litcoder-brain.github.io

</details>


### [14] [Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing](https://arxiv.org/abs/2509.09160)
*Zhiyue Liu,Fanrong Ma,Xin Ling*

Main category: cs.CL

TL;DR: 提出反事实增强去偏框架提升目标导向多模态情感分类准确性


<details>
  <summary>Details</summary>
Motivation: 现有方法过度依赖文本特征且忽略词级上下文偏差，导致文本特征与标签虚假关联影响分类效果

Method: 包含反事实数据增强策略（生成细节匹配样本）和自适应去偏对比学习机制（学习鲁棒特征）

Result: 在多个基准数据集上超越现有最优方法

Conclusion: 该框架有效缓解词级偏差影响，提升模型对情感相关内容的关注度

Abstract: Target-oriented multimodal sentiment classification seeks to predict
sentiment polarity for specific targets from image-text pairs. While existing
works achieve competitive performance, they often over-rely on textual content
and fail to consider dataset biases, in particular word-level contextual
biases. This leads to spurious correlations between text features and output
labels, impairing classification accuracy. In this paper, we introduce a novel
counterfactual-enhanced debiasing framework to reduce such spurious
correlations. Our framework incorporates a counterfactual data augmentation
strategy that minimally alters sentiment-related causal features, generating
detail-matched image-text samples to guide the model's attention toward content
tied to sentiment. Furthermore, for learning robust features from
counterfactual data and prompting model decisions, we introduce an adaptive
debiasing contrastive learning mechanism, which effectively mitigates the
influence of biased words. Experimental results on several benchmark datasets
show that our proposed method outperforms state-of-the-art baselines.

</details>


### [15] [EchoX: Towards Mitigating Acoustic-Semantic Gap via Echo Training for Speech-to-Speech LLMs](https://arxiv.org/abs/2509.09174)
*Yuhao Zhang,Yuhao Du,Zhanchen Dai,Xiangnan Ma,Kaiqi Kou,Benyou Wang,Haizhou Li*

Main category: cs.CL

TL;DR: 提出EchoX语音大语言模型，通过声学-语义联合学习解决现有SLLMs的推理退化问题


<details>
  <summary>Details</summary>
Motivation: 当前语音大语言模型(SLLMs)在知识与推理能力上出现退化，主因是声学特征与语义表征的空间鸿沟未被有效弥合

Method: 结合语义表征学习与动态语音训练目标生成，实现声学特征与语义空间的联合优化

Result: 使用6000小时训练数据，在多个知识问答基准测试中达到先进性能

Conclusion: EchoX验证了声学-语义联合学习框架的有效性，为语音大语言模型保持文本LLM的强推理能力提供了新方案

Abstract: Speech-to-speech large language models (SLLMs) are attracting increasing
attention. Derived from text-based large language models (LLMs), SLLMs often
exhibit degradation in knowledge and reasoning capabilities. We hypothesize
that this limitation arises because current training paradigms for SLLMs fail
to bridge the acoustic-semantic gap in the feature representation space. To
address this issue, we propose EchoX, which leverages semantic representations
and dynamically generates speech training targets. This approach integrates
both acoustic and semantic learning, enabling EchoX to preserve strong
reasoning abilities as a speech LLM. Experimental results demonstrate that
EchoX, with about six thousand hours of training data, achieves advanced
performance on multiple knowledge-based question-answering benchmarks. The
project is available at https://github.com/FreedomIntelligence/EchoX.

</details>


### [16] [Efficient Trie-based Biasing using K-step Prediction for Rare Word Recognition](https://arxiv.org/abs/2509.09196)
*Chin Yuen Kwok,Jia Qi yip*

Main category: cs.CL

TL;DR: 提出多步预测方法改进ASR模型，通过前瞻性预测避免传统Trie偏置的分数撤销步骤，使用仅10小时合成数据微调Whisper模型，词错误率从30.86%降至12.19%。


<details>
  <summary>Details</summary>
Motivation: 传统Trie偏置方法在部分假设未生成完整罕见词时需要撤销分数，该过程计算成本高且局限于波束搜索，尤其影响大解码器模型效率。

Method: 通过调整ASR模型实现多步前瞻预测，直接评估部分假设生成完整罕见词的可能性，无需分数撤销步骤，使用合成数据微调Whisper模型。

Result: 在NSC Part 2测试集上词错误率从30.86%显著降低至12.19%。

Conclusion: 多步预测方法有效规避传统方法的计算瓶颈，显著提升罕见词识别效率，极小数据量即可实现ASR模型性能突破性改进。

Abstract: Contextual biasing improves rare word recognition of ASR models by
prioritizing the output of rare words during decoding. A common approach is
Trie-based biasing, which gives "bonus scores" to partial hypothesis (e.g.
"Bon") that may lead to the generation of the rare word (e.g. "Bonham"). If the
full word ("Bonham") isn't ultimately recognized, the system revokes those
earlier bonuses. This revocation is limited to beam search and is
computationally expensive, particularly for models with large decoders. To
overcome these limitations, we propose adapting ASR models to look ahead and
predict multiple steps at once. This avoids the revocation step entirely by
better estimating whether a partial hypothesis will lead to the generation of
the full rare word. By fine-tuning Whisper with only 10 hours of synthetic
data, our method reduces the word error rate on the NSC Part 2 test set from
30.86% to 12.19%.

</details>


### [17] [Improving Synthetic Data Training for Contextual Biasing Models with a Keyword-Aware Cost Function](https://arxiv.org/abs/2509.09197)
*Chin Yuen Kwok,Jia Qi Yip,Eng Siong Chng*

Main category: cs.CL

TL;DR: 通过改进上下文偏置方法并设计关键词感知损失函数，在合成数据训练中有效提升罕见词识别，Whisper模型词错率从29.71%降至11.81%


<details>
  <summary>Details</summary>
Motivation: 解决合成数据训练偏置模块时产生的过拟合问题，提升自动语音识别系统对罕见词的识别能力

Method: 1. 改进基于TCPGen的上下文偏置框架 2. 提出包含掩码交叉熵损失和二元分类损失的复合损失函数，强化偏置词训练

Result: 在10小时合成数据微调后，NSC Part 2测试集词错率从29.71%显著降低至11.81%

Conclusion: 新型损失函数通过双重监督机制有效提升偏置词解码效果，为ASR系统罕见词识别提供了高效解决方案

Abstract: Rare word recognition can be improved by adapting ASR models to synthetic
data that includes these words. Further improvements can be achieved through
contextual biasing, which trains and adds a biasing module into the model
architecture to prioritize rare words. While training the module on synthetic
rare word data is more effective than using non-rare-word data, it can lead to
overfitting due to artifacts in the synthetic audio. To address this, we
enhance the TCPGen-based contextual biasing approach and propose a
keyword-aware loss function that additionally focuses on biased words when
training biasing modules. This loss includes a masked cross-entropy term for
biased word prediction and a binary classification term for detecting biased
word positions. These two terms complementarily support the decoding of biased
words during inference. By adapting Whisper to 10 hours of synthetic data, our
method reduced the word error rate on the NSC Part 2 test set from 29.71% to
11.81%.

</details>


### [18] [GmSLM : Generative Marmoset Spoken Language Modeling](https://arxiv.org/abs/2509.09198)
*Talia Sternberg,Michael London,David Omer,Yossi Adi*

Main category: cs.CL

TL;DR: 开发了首个针对狨猴叫声的生成式语言模型GmSLM，其生成的叫声与真实样本高度匹配，并为神经科学研究提供了新工具


<details>
  <summary>Details</summary>
Motivation: 狨猴具有类似人类语言的复杂发声系统(如轮流鸣叫)，但其叫声无法直接用人类语音模型分析。研究其发声机制可揭示语言神经基础，弥补人类脑研究的局限性

Method: 提出GmSLM流程：使用野外无监督数据和弱标记对话数据，设计零样本评估指标，对比人类语音基线模型

Result: GmSLM生成样本在声学特征上与真实样本高度一致，能有效区分真实/人工对话，支持下游神经机制研究任务

Conclusion: 该框架为揭示发声-大脑活动关联提供了实用工具，在神经科学、生物声学和进化生物学领域具有应用潜力

Abstract: Marmoset monkeys exhibit complex vocal communication, challenging the view
that nonhuman primates vocal communication is entirely innate, and show similar
features of human speech, such as vocal labeling of others and turn-taking.
Studying their vocal communication offers a unique opportunity to link it with
brain activity-especially given the difficulty of accessing the human brain in
speech and language research. Since Marmosets communicate primarily through
vocalizations, applying standard LLM approaches is not straightforward. We
introduce Generative Marmoset Spoken Language Modeling (GmSLM), an optimized
spoken language model pipeline for Marmoset vocal communication. We designed a
novel zero-shot evaluation metrics using unsupervised in-the-wild data,
alongside weakly labeled conversational data, to assess GmSLM and demonstrate
its advantage over a basic human-speech-based baseline. GmSLM generated
vocalizations closely matched real resynthesized samples acoustically and
performed well on downstream tasks. Despite being fully unsupervised, GmSLM
effectively distinguish real from artificial conversations and may support
further investigations of the neural basis of vocal communication and provides
a practical framework linking vocalization and brain activity. We believe GmSLM
stands to benefit future work in neuroscience, bioacoustics, and evolutionary
biology. Samples are provided under: pages.cs.huji.ac.il/adiyoss-lab/GmSLM.

</details>


### [19] [CCF: A Context Compression Framework for Efficient Long-Sequence Language Modeling](https://arxiv.org/abs/2509.09199)
*Wenhao Li,Bangcheng Sun,Weihao Ye,Tianyi Zhang,Daohai Yu,Fei Chao,Rongrong Ji*

Main category: cs.CL

TL;DR: 提出CCF分层压缩框架，通过语义聚合和内存编码显著提升长上下文建模效率


<details>
  <summary>Details</summary>
Motivation: 现有长上下文扩展方法存在计算资源消耗大和内存效率低的问题，需要开发更高效的压缩方案实现全局语义保持与冗余消除

Method: 整合分段语义聚合与键值记忆编码构建紧凑表示，结合增量分段解码与稀疏储层采样优化训练效率

Result: 在多个长上下文基准测试中实现高压缩比下的竞争力，吞吐量提升30%+且内存占用减少40%

Conclusion: 结构化压缩技术为可扩展的长上下文建模提供了有效解决方案，平衡了性能与资源效率

Abstract: Scaling language models to longer contexts is essential for capturing rich
dependencies across extended discourse. However, na\"ive context extension
imposes significant computational and memory burdens, often resulting in
inefficiencies during both training and inference. In this work, we propose
CCF, a novel context compression framework designed to enable efficient
long-context modeling by learning hierarchical latent representations that
preserve global semantics while aggressively reducing input redundancy. CCF
integrates segment-wise semantic aggregation with key-value memory encoding,
forming compact representations that support accurate reconstruction and
long-range understanding. To further enhance scalability, we introduce a
training-efficient optimization strategy that couples incremental segment
decoding with sparse reservoir sampling, substantially reducing memory overhead
without degrading performance. Empirical results on multiple long-context
language modeling benchmarks demonstrate that CCF achieves competitive
perplexity under high compression ratios, and significantly improves throughput
and memory efficiency compared to existing approaches. These findings highlight
the potential of structured compression for scalable and effective long-context
language modeling.

</details>


### [20] [Reading Between the Lines: Classifying Resume Seniority with Large Language Models](https://arxiv.org/abs/2509.09229)
*Matan Cohen,Shira Shani,Eden Menahem,Yehudit Aperstein,Alexander Apartsin*

Main category: cs.CL

TL;DR: 研究使用大语言模型自动评估简历资历，通过混合数据集验证模型在识别资历夸大方面的有效性


<details>
  <summary>Details</summary>
Motivation: 简历中普遍存在资历夸大和模糊表述现象，传统评估方法难以准确判断候选人真实资历水平

Method: 构建包含真实简历和模拟夸大/低调表述的合成数据集，测试LLMs识别隐含语言特征的能力

Result: LLMs展现出检测资历夸大和隐含专业能力线索的潜力，为优化AI招聘系统提供新方向

Conclusion: 大语言模型可有效提升简历评估客观性，公开数据集将促进更精准的AI招聘工具开发

Abstract: Accurately assessing candidate seniority from resumes is a critical yet
challenging task, complicated by the prevalence of overstated experience and
ambiguous self-presentation. In this study, we investigate the effectiveness of
large language models (LLMs), including fine-tuned BERT architectures, for
automating seniority classification in resumes. To rigorously evaluate model
performance, we introduce a hybrid dataset comprising both real-world resumes
and synthetically generated hard examples designed to simulate exaggerated
qualifications and understated seniority. Using the dataset, we evaluate the
performance of Large Language Models in detecting subtle linguistic cues
associated with seniority inflation and implicit expertise. Our findings
highlight promising directions for enhancing AI-driven candidate evaluation
systems and mitigating bias introduced by self-promotional language. The
dataset is available for the research community at https://bit.ly/4mcTovt

</details>


### [21] [Agentic LLMs for Question Answering over Tabular Data](https://arxiv.org/abs/2509.09234)
*Rishit Tyagi,Mohit Gupta,Rahul Bouri*

Main category: cs.CL

TL;DR: 提出基于GPT-4o和DeepSeek等大语言模型的多阶段NL-to-SQL流程，在DataBench基准测试中实现70.5%准确率，较基线提升超40%。


<details>
  <summary>Details</summary>
Motivation: 现实世界表格存在结构/规模/数据类型的多样性，需开发能有效处理结构化查询的表格问答系统以满足实际应用需求。

Method: 设计包含示例选择、SQL生成、答案验证的五阶段流水线，集成GPT-4o/DeepSeek等LLM进行动态SQL生成与迭代优化。

Result: DataBench QA准确率达70.5%（基线26%），DataBench Lite达71.6%（基线27%），验证方法有效性。

Conclusion: LLM驱动的表格问答在动态查询生成方面展现优势，但依赖模型性能且需结构化验证，为实际部署提供重要参考。

Abstract: Question Answering over Tabular Data (Table QA) presents unique challenges
due to the diverse structure, size, and data types of real-world tables. The
SemEval 2025 Task 8 (DataBench) introduced a benchmark composed of large-scale,
domain-diverse datasets to evaluate the ability of models to accurately answer
structured queries. We propose a Natural Language to SQL (NL-to-SQL) approach
leveraging large language models (LLMs) such as GPT-4o, GPT-4o-mini, and
DeepSeek v2:16b to generate SQL queries dynamically. Our system follows a
multi-stage pipeline involving example selection, SQL query generation, answer
extraction, verification, and iterative refinement. Experiments demonstrate the
effectiveness of our approach, achieving 70.5\% accuracy on DataBench QA and
71.6\% on DataBench Lite QA, significantly surpassing baseline scores of 26\%
and 27\% respectively. This paper details our methodology, experimental
results, and alternative approaches, providing insights into the strengths and
limitations of LLM-driven Table QA.

</details>


### [22] [From scratch to silver: Creating trustworthy training data for patent-SDG classification using Large Language Models](https://arxiv.org/abs/2509.09303)
*Grazia Sveva Ascione,Nicolò Tamagnone*

Main category: cs.CL

TL;DR: 提出利用弱监督学习框架和LLM语义对齐技术，通过专利与SDG论文的引用关系构建银标准多标签数据集，显著提升专利-SDG分类效果


<details>
  <summary>Details</summary>
Motivation: 现有专利SDG分类方法（关键词搜索/迁移学习/引文启发式）存在数据集稀缺、可扩展性差和缺乏领域泛化能力的问题

Method: 1. 基于专利本体论从专利和SDG论文中提取功能-解决方案-应用三元组
2. 设计复合标签函数计算跨域语义相似度
3. 开发定制化损失函数校准噪声标签
4. 采用基于排名的检索框架融合多维度特征

Result: 内部验证F1达0.78超越基线模型，外部网络验证显示SDG标签在技术主题/认知结构/组织协作上比传统分类提升30-50%一致性

Conclusion: 弱监督框架与语义对齐机制能有效实现大规模SDG分类，专利引文网络分析验证了标签的生态效度

Abstract: Classifying patents by their relevance to the UN Sustainable Development
Goals (SDGs) is crucial for tracking how innovation addresses global
challenges. However, the absence of a large, labeled dataset limits the use of
supervised learning. Existing methods, such as keyword searches, transfer
learning, and citation-based heuristics, lack scalability and generalizability.
This paper frames patent-to-SDG classification as a weak supervision problem,
using citations from patents to SDG-tagged scientific publications (NPL
citations) as a noisy initial signal. To address its sparsity and noise, we
develop a composite labeling function (LF) that uses large language models
(LLMs) to extract structured concepts, namely functions, solutions, and
applications, from patents and SDG papers based on a patent ontology.
Cross-domain similarity scores are computed and combined using a rank-based
retrieval approach. The LF is calibrated via a custom positive-only loss that
aligns with known NPL-SDG links without penalizing discovery of new SDG
associations. The result is a silver-standard, soft multi-label dataset mapping
patents to SDGs, enabling the training of effective multi-label regression
models. We validate our approach through two complementary strategies: (1)
internal validation against held-out NPL-based labels, where our method
outperforms several baselines including transformer-based models, and zero-shot
LLM; and (2) external validation using network modularity in patent citation,
co-inventor, and co-applicant graphs, where our labels reveal greater thematic,
cognitive, and organizational coherence than traditional technological
classifications. These results show that weak supervision and semantic
alignment can enhance SDG classification at scale.

</details>


### [23] [MetaRAG: Metamorphic Testing for Hallucination Detection in RAG Systems](https://arxiv.org/abs/2509.09360)
*Channdeth Sok,David Luz,Yacine Haddam*

Main category: cs.CL

TL;DR: 提出MetaRAG框架——首个针对RAG系统的无监督实时幻觉检测方法，通过事实分解、变异验证和跨度级评分实现身份敏感的可靠部署


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法（如SelfCheckGPT）无法解决RAG系统特有的证据一致性挑战，需专门框架确保高风险领域生成内容的可信度

Method: 1.分解答案为原子事实 2.生成同/反义词变异 3.验证上下文一致性（同义词应被支持，反义词应被反驳）4.聚合不一致性得分为幻觉评分

Result: 在企业专有数据集验证有效，支持细粒度事实跨度定位（如孕期防护/LGBTQ+难民权益），但基于身份的防护设计方案尚未评估

Conclusion: MetaRAG为RAG系统提供了无需标注数据/模型权限的黑盒检测方案，其跨度级评分机制特别适合身份敏感场景的可靠部署

Abstract: Large Language Models (LLMs) are increasingly deployed in enterprise
applications, yet their reliability remains limited by hallucinations, i.e.,
confident but factually incorrect information. Existing detection approaches,
such as SelfCheckGPT and MetaQA, primarily target standalone LLMs and do not
address the unique challenges of Retrieval-Augmented Generation (RAG) systems,
where responses must be consistent with retrieved evidence. We therefore
present MetaRAG, a metamorphic testing framework for hallucination detection in
Retrieval-Augmented Generation (RAG) systems. MetaRAG operates in a real-time,
unsupervised, black-box setting, requiring neither ground-truth references nor
access to model internals, making it suitable for proprietary and high-stakes
domains. The framework proceeds in four stages: (1) decompose answers into
atomic factoids, (2) generate controlled mutations of each factoid using
synonym and antonym substitutions, (3) verify each variant against the
retrieved context (synonyms are expected to be entailed and antonyms
contradicted), and (4) aggregate penalties for inconsistencies into a
response-level hallucination score. Crucially for identity-aware AI, MetaRAG
localizes unsupported claims at the factoid span where they occur (e.g.,
pregnancy-specific precautions, LGBTQ+ refugee rights, or labor eligibility),
allowing users to see flagged spans and enabling system designers to configure
thresholds and guardrails for identity-sensitive queries. Experiments on a
proprietary enterprise dataset illustrate the effectiveness of MetaRAG for
detecting hallucinations and enabling trustworthy deployment of RAG-based
conversational agents. We also outline a topic-based deployment design that
translates MetaRAG's span-level scores into identity-aware safeguards; this
design is discussed but not evaluated in our experiments.

</details>


### [24] [Modelling Analogies and Analogical Reasoning: Connecting Cognitive Science Theory and NLP Research](https://arxiv.org/abs/2509.09381)
*Molly R Petersen,Claire E Stevenson,Lonneke van der Plas*

Main category: cs.CL

TL;DR: 探讨类比推理的认知理论及其在自然语言处理（NLP）中的应用关联性


<details>
  <summary>Details</summary>
Motivation: 通过整合认知科学中类比推理的过程理论与NLP研究，优化文本中的关系理解而非依赖实体相似性

Method: 1. 总结认知科学文献中的类比推理基础理论；2. 建立与NLP现有概念的映射；3. 验证其对NLP核心挑战的适用性

Result: 揭示认知视角可指导NLP突破实体级相似性限制，增强关系推理能力

Conclusion: 建议研究者关注认知科学中的关系处理机制，开发更能捕捉深层语义关联的NLP模型

Abstract: Analogical reasoning is an essential aspect of human cognition. In this
paper, we summarize key theory about the processes underlying analogical
reasoning from the cognitive science literature and relate it to current
research in natural language processing. While these processes can be easily
linked to concepts in NLP, they are generally not viewed through a cognitive
lens. Furthermore, we show how these notions are relevant for several major
challenges in NLP research, not directly related to analogy solving. This may
guide researchers to better optimize relational understanding in text, as
opposed to relying heavily on entity-level similarity.

</details>


### [25] [Hierarchical Bracketing Encodings Work for Dependency Graphs](https://arxiv.org/abs/2509.09388)
*Ana Ezquerro,Carlos Gómez-Rodríguez,David Vilares*

Main category: cs.CL

TL;DR: 提出基于层次括号编码的依赖图解析方法，在保证结构信息的同时实现线性时间复杂度和更高准确率


<details>
  <summary>Details</summary>
Motivation: 现有依赖图解析方法在表示重入边、循环结构和空节点时存在效率瓶颈，需要兼顾计算效率和结构表达能力

Method: 使用层次括号编码将图结构序列化，通过n个标注动作实现线性时间解析，支持重入边、循环结构和空节点的表示

Result: 在多语言多形式主义基准测试中取得竞争性结果，在完全匹配准确率上持续优于其他方法

Conclusion: 层次括号编码为依赖图解析提供了高效实用的解决方案，在保持结构完整性的同时显著提升解析效率

Abstract: We revisit hierarchical bracketing encodings from a practical perspective in
the context of dependency graph parsing. The approach encodes graphs as
sequences, enabling linear-time parsing with $n$ tagging actions, and still
representing reentrancies, cycles, and empty nodes. Compared to existing graph
linearizations, this representation substantially reduces the label space while
preserving structural information. We evaluate it on a multilingual and
multi-formalism benchmark, showing competitive results and consistent
improvements over other methods in exact match accuracy.

</details>


### [26] [GrACE: A Generative Approach to Better Confidence Elicitation in Large Language Models](https://arxiv.org/abs/2509.09438)
*Zhaohan Zhang,Ziquan Liu,Ioannis Patras*

Main category: cs.CL

TL;DR: 提出GrACE方法，通过模型隐藏状态与特殊标记嵌入的实时相似性评估LLM置信度，实现高效可靠的置信度校准。


<details>
  <summary>Details</summary>
Motivation: 现有LLM置信度评估方法存在高计算成本或校准效果差的缺陷，阻碍其在医疗/金融等高危场景的落地应用。GrACE旨在提供可扩展、实时且无需额外资源的置信度估计方案。

Method: 1. 在词汇表添加特殊置信标记
2. 通过最后隐藏层状态与标记嵌入的余弦相似度生成置信度
3. 基于准确率的校准目标对模型微调
4. 提出两种测试时扩展策略优化置信度分布

Result: 在3个LLM和2个基准测试中：
• 置信度判别能力提升18.7%（AUROC）
• 校准误差降低32%（ECE指标）
• 测试时采样量减少60%时保持同等准确率

Conclusion: GrACE首次实现无需额外计算资源的实时置信度估计，为LLM安全部署提供可扩展解决方案，实验证明其显著提升决策准确率并降低部署成本。

Abstract: Assessing the reliability of Large Language Models (LLMs) by confidence
elicitation is a prominent approach to AI safety in high-stakes applications,
such as healthcare and finance. Existing methods either require expensive
computational overhead or suffer from poor calibration, making them impractical
and unreliable for real-world deployment. In this work, we propose GrACE, a
Generative Approach to Confidence Elicitation that enables scalable and
reliable confidence elicitation for LLMs. GrACE adopts a novel mechanism in
which the model expresses confidence by the similarity between the last hidden
state and the embedding of a special token appended to the vocabulary, in
real-time. We fine-tune the model for calibrating the confidence with
calibration targets associated with accuracy. Experiments with three LLMs and
two benchmark datasets show that the confidence produced by GrACE achieves the
best discriminative capacity and calibration on open-ended generation tasks,
outperforming six competing methods without resorting to additional sampling or
an auxiliary model. Moreover, we propose two strategies for improving test-time
scaling based on confidence induced by GrACE. Experimental results show that
using GrACE not only improves the accuracy of the final decision but also
significantly reduces the number of required samples in the test-time scaling
scheme, indicating the potential of GrACE as a practical solution for deploying
LLMs with scalable, reliable, and real-time confidence estimation.

</details>


### [27] [Mitigating Language Barriers in Education: Developing Multilingual Digital Learning Materials with Machine Translation](https://arxiv.org/abs/2509.09473)
*Lucie Poláková,Martin Popel,Věra Kloudová,Michal Novák,Mariia Anisimova,Jiří Balhar*

Main category: cs.CL

TL;DR: EdUKate项目通过开发捷克-乌克兰教育领域机器翻译系统，成功实现9000个交互式教材的多语言转换，所有成果免费开放使用。


<details>
  <summary>Details</summary>
Motivation: 解决捷克中小学非母语学生的教育资源缺口，促进教育公平。项目源于俄乌冲突后大量乌克兰难民儿童的入学需求。

Method: 结合语言学与机器翻译技术，开发支持XML/PDF格式处理的专用翻译系统，建立教育领域术语库，并通过教师需求调研优化系统设计。

Result: 完成三种语言9000套教材翻译，系统在教育门户成功部署，经评估准确率达教学可用标准，应用程序实现完全开源。

Conclusion: 该项目创新性地将机器翻译应用于教育场景，通过跨学科合作解决了多语言教育资源开发难题，为欧洲多语种教育数字化树立了标杆。

Abstract: The EdUKate project combines digital education, linguistics, translation
studies, and machine translation to develop multilingual learning materials for
Czech primary and secondary schools. Launched through collaboration between a
major Czech academic institution and the country's largest educational
publisher, the project is aimed at translating up to 9,000 multimodal
interactive exercises from Czech into Ukrainian, English, and German for an
educational web portal. It emphasizes the development and evaluation of a
direct Czech-Ukrainian machine translation system tailored to the educational
domain, with special attention to processing formatted content such as XML and
PDF and handling technical and scientific terminology. We present findings from
an initial survey of Czech teachers regarding the needs of non-Czech-speaking
students and describe the system's evaluation and implementation on the web
portal. All resulting applications are freely available to students, educators,
and researchers.

</details>


### [28] [Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness and Knowledge Graphs](https://arxiv.org/abs/2509.09522)
*Vadim Zadykian,Bruno Andrade,Haithem Afli*

Main category: cs.CL

TL;DR: 提出结合知识图谱与文本嵌入的混合架构，通过分层STR评估显著提升职位匹配准确率（高相关区域RMSE降低25%）并增强模型可解释性


<details>
  <summary>Details</summary>
Motivation: 传统基于词汇重叠的文本匹配在职位名称匹配场景中存在局限性，需结合深层语义与领域知识提升简历推荐系统的上下文理解能力

Method: 自监督混合架构：1）SBERT生成句子嵌入 2）图神经网络整合领域知识图谱 3）创新性STR连续体分层评估（低/中/高相关区域）

Result: 知识图谱增强的微调SBERT模型在高STR区域表现最佳，RMSE相对基线降低25%；分层分析揭示传统全局指标掩盖的模型特性

Conclusion: 知识图谱与文本嵌入的融合及区域化性能评估策略，为HR系统中兼顾公平性、可解释性和精准匹配的模型选择提供新范式

Abstract: Semantic Textual Relatedness (STR) captures nuanced relationships between
texts that extend beyond superficial lexical similarity. In this study, we
investigate STR in the context of job title matching - a key challenge in
resume recommendation systems, where overlapping terms are often limited or
misleading. We introduce a self-supervised hybrid architecture that combines
dense sentence embeddings with domain-specific Knowledge Graphs (KGs) to
improve both semantic alignment and explainability. Unlike previous work that
evaluated models on aggregate performance, our approach emphasizes data
stratification by partitioning the STR score continuum into distinct regions:
low, medium, and high semantic relatedness. This stratified evaluation enables
a fine-grained analysis of model performance across semantically meaningful
subspaces. We evaluate several embedding models, both with and without KG
integration via graph neural networks. The results show that fine-tuned SBERT
models augmented with KGs produce consistent improvements in the high-STR
region, where the RMSE is reduced by 25% over strong baselines. Our findings
highlight not only the benefits of combining KGs with text embeddings, but also
the importance of regional performance analysis in understanding model
behavior. This granular approach reveals strengths and weaknesses hidden by
global metrics, and supports more targeted model selection for use in Human
Resources (HR) systems and applications where fairness, explainability, and
contextual matching are essential.

</details>


### [29] [DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning and Label Distribution Learning](https://arxiv.org/abs/2509.09524)
*Daniil Ignatev,Nan Li,Hugh Mee Wong,Anh Dang,Shane Kaszefski Yaschuk*

Main category: cs.CL

TL;DR: DeMeVa团队在LeWiDi 2025任务中探索了基于大语言模型的上下文学习(ICL)和RoBERTa的标签分布学习(LDL)，验证了ICL在预测个体标注者注释的有效性以及LDL在软标签预测中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索不同方法（ICL与LDL）在解决标注分歧任务中的效果，提升个体标注者视角下的预测性能。

Method: 1. 对比ICL中不同示例采样策略；2. 评估RoBERTa模型在LDL中的多种微调方法。

Result: ICL通过聚合个体预测生成软标签达到优异性能；LDL显示出处理软标签预测的潜力但需进一步优化。

Conclusion: ICL适用于个体视角标注预测，LDL值得社群持续探索。该研究为多视角标注任务提供了有效方法框架。

Abstract: This system paper presents the DeMeVa team's approaches to the third edition
of the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et
al., 2025). We explore two directions: in-context learning (ICL) with large
language models, where we compare example sampling strategies; and label
distribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we
evaluate several fine-tuning methods. Our contributions are twofold: (1) we
show that ICL can effectively predict annotator-specific annotations
(perspectivist annotations), and that aggregating these predictions into soft
labels yields competitive performance; and (2) we argue that LDL methods are
promising for soft label predictions and merit further exploration by the
perspectivist community.

</details>


### [30] [Prompting the Market? A Large-Scale Meta-Analysis of GenAI in Finance NLP (2022-2025)](https://arxiv.org/abs/2509.09544)
*Paolo Pedinotti,Peter Baumann,Nathan Jessurun,Leslie Barrett,Enrico Santus*

Main category: cs.CL

TL;DR: 论文提出MetaGraph方法，通过从科学文献构建知识图谱分析金融NLP演进三阶段：LLM初期应用→反思局限性→模块化系统整合周边技术


<details>
  <summary>Details</summary>
Motivation: 传统综述方法难以追踪金融NLP领域因LLM快速变革带来的数据集激增、数据源多元化等结构性变化

Method: 建立金融NLP本体论，使用LLM驱动流水线分析681篇论文（2022-2025），构建可查询的知识图谱进行数据驱动分析

Result: 揭示三阶段演进：1)早期LLM采用与任务/数据集创新 2)对LLM局限性的批判性反思 3)周边技术向模块化系统整合的发展趋势

Conclusion: MetaGraph为领域演进提供结构化视角，该方法可复用于其他学科领域的发展轨迹分析，帮助从业者和研究者把握方法论转变与新兴趋势

Abstract: Large Language Models (LLMs) have rapidly reshaped financial NLP, enabling
new tasks and driving a proliferation of datasets and diversification of data
sources. Yet, this transformation has outpaced traditional surveys. In this
paper, we present MetaGraph, a generalizable methodology for extracting
knowledge graphs from scientific literature and analyzing them to obtain a
structured, queryable view of research trends. We define an ontology for
financial NLP research and apply an LLM-based extraction pipeline to 681 papers
(2022-2025), enabling large-scale, data-driven analysis. MetaGraph reveals
three key phases: early LLM adoption and task/dataset innovation; critical
reflection on LLM limitations; and growing integration of peripheral techniques
into modular systems. This structured view offers both practitioners and
researchers a clear understanding of how financial NLP has evolved -
highlighting emerging trends, shifting priorities, and methodological
shifts-while also demonstrating a reusable approach for mapping scientific
progress in other domains.

</details>


### [31] [Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking](https://arxiv.org/abs/2509.09583)
*Brittany Harbison,Samuel Taubman,Travis Taylor,Ashok. K. Goel*

Main category: cs.CL

TL;DR: 研究通过GPT零样本能力检测学生论坛帖子中的大五人格特质，改进SAMI社交推荐系统


<details>
  <summary>Details</summary>
Motivation: 在线课程社交推荐系统SAMI因缺乏个性识别影响推荐效果，需通过个性检测提升匹配相关性

Method: 1. 利用GPT零样本学习从课程论坛介绍帖推断大五人格
2. 对比传统模型验证效果
3. 将个性模型整合至SAMI实体匹配系统

Result: GPT模型在人格检测任务中表现优于基准模型，初步整合显示个性特征可补充现有匹配因素，但需进一步评估实际影响

Conclusion: 个性检测能增强在线课程社交推荐效果，但对学生参与度和匹配质量的全面影响仍需后续研究验证

Abstract: Social connection is a vital part of learning, yet online course environments
present barriers to the organic formation of social groups. SAMI offers one
solution by facilitating student connections, but its effectiveness is
constrained by an incomplete Theory of Mind, limiting its ability to create an
effective mental model of a student. One facet of this is its inability to
intuit personality, which may influence the relevance of its recommendations.
To explore this, we propose a personality detection model utilizing GPTs
zero-shot capability to infer Big-Five personality traits from forum
introduction posts, often encouraged in online courses. We benchmark its
performance against established models, demonstrating its efficacy in this
task. Furthermore, we integrate this model into SAMIs entity-based matchmaking
system, enabling personality-informed social recommendations. Initial
integration suggests personality traits can complement existing matching
factors, though additional evaluation is required to determine their full
impact on student engagement and match quality.

</details>


### [32] [Fluent but Unfeeling: The Emotional Blind Spots of Language Models](https://arxiv.org/abs/2509.09593)
*Bangzhao Shu,Isha Joshi,Melissa Karnaze,Anh C. Pham,Ishita Kakkar,Sindhu Kothe,Arpine Hovasapian,Mai ElSherief*

Main category: cs.CL

TL;DR: 研究发现当前大语言模型在细粒度情感对齐上存在局限，构建EXPRESS数据集评估发现LLMs难以准确预测与人类自我披露情感一致的情绪标签。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注粗粒度情绪分类，缺乏对LLMs与人类细粒度情感对齐能力的评估，需揭示模型在真实场景下的情绪理解缺陷。

Method: 构建含251个细粒度情绪标签的EXPRESS数据集，基于情绪理论分解为八种基础情绪，系统测试不同提示策略下主流LLMs的表现，并进行定性分析。

Result: LLMs预测结果与人类自我披露情绪匹配度较低，部分模型虽符合情绪理论定义，但难以捕捉上下文线索（如'创伤触发'等特定场景情绪）。

Conclusion: LLMs的细粒度情感对齐能力仍有明显局限，未来需加强上下文理解与细粒度情绪映射机制的研究。

Abstract: The versatility of Large Language Models (LLMs) in natural language
understanding has made them increasingly popular in mental health research.
While many studies explore LLMs' capabilities in emotion recognition, a
critical gap remains in evaluating whether LLMs align with human emotions at a
fine-grained level. Existing research typically focuses on classifying emotions
into predefined, limited categories, overlooking more nuanced expressions. To
address this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit
communities featuring 251 fine-grained, self-disclosed emotion labels. Our
comprehensive evaluation framework examines predicted emotion terms and
decomposes them into eight basic emotions using established emotion theories,
enabling a fine-grained comparison. Systematic testing of prevalent LLMs under
various prompt settings reveals that accurately predicting emotions that align
with human self-disclosed emotions remains challenging. Qualitative analysis
further shows that while certain LLMs generate emotion terms consistent with
established emotion theories and definitions, they sometimes fail to capture
contextual cues as effectively as human self-disclosures. These findings
highlight the limitations of LLMs in fine-grained emotion alignment and offer
insights for future research aimed at enhancing their contextual understanding.

</details>


### [33] [LAVA: Language Model Assisted Verbal Autopsy for Cause-of-Death Determination](https://arxiv.org/abs/2509.09602)
*Yiqun T. Chen,Tyler H. McCormick,Li Liu,Abhirup Datta*

Main category: cs.CL

TL;DR: LA-VA结合大语言模型与传统算法，将尸检预测准确率提升5-10%，验证了现成LLM方案在低资源地区健康监测中的有效性


<details>
  <summary>Details</summary>
Motivation: 传统尸检方法在医疗认证缺失地区存在准确性局限，需要开发更高效可靠的死因预测工具来改善全球健康监测

Method: 使用PHMRC数据集（共11,978例），对比测试GPT-5预测、LCVA基线、文本嵌入分类和元学习集成方法，构建多模态分析流程

Result: GPT-5在成人/儿童/新生儿组的平均准确率达48.6%/50.5%/53.5%，较传统统计学习方法提升5-10%

Conclusion: 现成大语言模型的简单应用即可显著提升尸检准确性，为低资源地区公共卫生决策提供更可靠的数据支持

Abstract: Verbal autopsy (VA) is a critical tool for estimating causes of death in
resource-limited settings where medical certification is unavailable. This
study presents LA-VA, a proof-of-concept pipeline that combines Large Language
Models (LLMs) with traditional algorithmic approaches and embedding-based
classification for improved cause-of-death prediction. Using the Population
Health Metrics Research Consortium (PHMRC) dataset across three age categories
(Adult: 7,580; Child: 1,960; Neonate: 2,438), we evaluate multiple approaches:
GPT-5 predictions, LCVA baseline, text embeddings, and meta-learner ensembles.
Our results demonstrate that GPT-5 achieves the highest individual performance
with average test site accuracies of 48.6% (Adult), 50.5% (Child), and 53.5%
(Neonate), outperforming traditional statistical machine learning baselines by
5-10%. Our findings suggest that simple off-the-shelf LLM-assisted approaches
could substantially improve verbal autopsy accuracy, with important
implications for global health surveillance in low-resource settings.

</details>


### [34] [Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems](https://arxiv.org/abs/2509.09629)
*Minghang Zhu,Zhengliang Shi,Zhiwei Xu,Shiguang Wu,Lingjie Wang,Pengjie Ren,Zhaochun Ren,Zhumin Chen*

Main category: cs.CL

TL;DR: 提出MOAT多智能体联合对齐框架，通过迭代对齐提升智能体协作效率，在六项基准测试中平均提升3.1%-4.4%


<details>
  <summary>Details</summary>
Motivation: 现有方法独立微调导致智能体能力差距与协作失调，需系统性协同优化机制

Method: 交替执行规划智能体对齐（生成优化子目标序列）与接地智能体改进（多样化子目标-动作对微调），理论证明训练过程收敛性

Result: 在held-in/held-out任务上分别获得3.1%和4.4%平均提升，超越现有SOTA方法

Conclusion: MOAT通过迭代对齐机制有效解决多智能体协作难题，兼具理论保障与实践效果

Abstract: The advancement of large language models (LLMs) has enabled the construction
of multi-agent systems to solve complex tasks by dividing responsibilities
among specialized agents, such as a planning agent for subgoal generation and a
grounding agent for executing tool-use actions. Most existing methods typically
fine-tune these agents independently, leading to capability gaps among them
with poor coordination. To address this, we propose MOAT, a Multi-Agent Joint
Alignment Tuning framework that improves agents collaboration through iterative
alignment. MOAT alternates between two key stages: (1) Planning Agent
Alignment, which optimizes the planning agent to generate subgoal sequences
that better guide the grounding agent; and (2) Grounding Agent Improving, which
fine-tunes the grounding agent using diverse subgoal-action pairs generated by
the agent itself to enhance its generalization capablity. Theoretical analysis
proves that MOAT ensures a non-decreasing and progressively convergent training
process. Experiments across six benchmarks demonstrate that MOAT outperforms
state-of-the-art baselines, achieving average improvements of 3.1% on held-in
tasks and 4.4% on held-out tasks.

</details>


### [35] [All for One: LLMs Solve Mental Math at the Last Token With Information Transferred From Other Tokens](https://arxiv.org/abs/2509.09650)
*Siddarth Mamidanna,Daking Rai,Ziyu Yao,Yilun Zhou*

Main category: cs.CL

TL;DR: 发现大语言模型在心算任务中存在'All-for-One'子图结构，关键计算集中在深层网络末端token，通过CAMA和ABP技术验证其跨模型通用性


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在数学计算任务中的实际运算路径，验证理论上的全局token访问能力在实践中的具体实现方式

Method: 三阶段渐进式分析：1)抑制初始层token计算 2)限制中间层跨token信息传递 3)强制末token处理所有计算，配合CAMA（上下文感知均值消融）和ABP（注意力窥视）技术

Result: AF1子图在多种算术任务中展现出：a)必要性（准确率>90%）b)跨模型可迁移性 c)输入格式鲁棒性 d)计算延迟特性（深层触发）

Conclusion: 揭示了LLMs数学能力的特殊计算路径，提出的CAMA/ABP方法为模型可解释性研究提供了新工具，末token集中计算现象挑战传统层级认知

Abstract: Large language models (LLMs) demonstrate proficiency across numerous
computational tasks, yet their inner workings remain unclear. In theory, the
combination of causal self-attention and multilayer perceptron layers allows
every token to access and compute information based on all preceding tokens. In
practice, to what extent are such operations present? In this paper, on mental
math tasks (i.e., direct math calculation via next-token prediction without
explicit reasoning), we investigate this question in three steps: inhibiting
input-specific token computations in the initial layers, restricting the routes
of information transfer across token positions in the next few layers, and
forcing all computation to happen at the last token in the remaining layers.
With two proposed techniques, Context-Aware Mean Ablation (CAMA) and
Attention-Based Peeking (ABP), we identify an All-for-One subgraph (AF1) with
high accuracy on a wide variety of mental math tasks, where meaningful
computation occurs very late (in terms of layer depth) and only at the last
token, which receives information of other tokens in few specific middle
layers. Experiments on a variety of models and arithmetic expressions show that
this subgraph is sufficient and necessary for high model performance, transfers
across different models, and works on a variety of input styles. Ablations on
different CAMA and ABP alternatives reveal their unique advantages over other
methods, which may be of independent interest.

</details>


### [36] [Steering MoE LLMs via Expert (De)Activation](https://arxiv.org/abs/2509.09660)
*Mohsen Fayyaz,Ali Modarressi,Hanieh Deilamsalehy,Franck Dernoncourt,Ryan Rossi,Trung Bui,Hinrich Schütze,Nanyun Peng*

Main category: cs.CL

TL;DR: SteerMoE框架通过专家控制实现LLM行为调控，无需模型重训练即可提升安全性和忠实度，并揭示专家层隐藏的对齐漏洞


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型中专家隐含未对齐行为，需开发无需参数修改的实时控制方法

Method: 通过对比输入检测行为相关专家，在推理时选择性激活/停用特定专家网络

Result: 11个基准测试中安全性提升+20%、忠实度+27%，对抗攻击时可完全突破安全防护

Conclusion: 首次实现MoE模型实时行为控制，同时暴露专家层存在的安全后门风险

Abstract: Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token
through a subset of specialized Feed-Forward Networks (FFN), known as experts.
We present SteerMoE, a framework for steering MoE models by detecting and
controlling behavior-linked experts. Our detection method identifies experts
with distinct activation patterns across paired inputs exhibiting contrasting
behaviors. By selectively (de)activating such experts during inference, we
control behaviors like faithfulness and safety without retraining or modifying
weights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to
+20% and faithfulness by +27%. In adversarial attack mode, it drops safety by
-41% alone, and -100% when combined with existing jailbreak methods, bypassing
all safety guardrails and exposing a new dimension of alignment faking hidden
within experts.

</details>


### [37] [CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning in Large Language Models](https://arxiv.org/abs/2509.09675)
*Runpeng Dai,Linfeng Song,Haolin Liu,Zhenwen Liang,Dian Yu,Haitao Mi,Zhaopeng Tu,Rui Liu,Tong Zheng,Hongtu Zhu,Dong Yu*

Main category: cs.CL

TL;DR: 提出好奇心驱动探索框架（CDE），通过演员困惑度和评论家价值方差双信号增强RLVR效果，解决探索不足问题


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在探索不足导致早熟收敛和熵崩溃的问题

Method: 使用演员生成响应的困惑度（内在不确定性）和评论家多头架构的价值估计方差作为内在探索奖励

Result: 在AIME基准上实现较标准RLVR（GRPO/PPO）约+3分的提升，并揭示LLM的校准崩溃机制

Conclusion: CDE有效提升探索效率，理论证明其与经典RL探索方法的关联，实证揭示LLM失败模式

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm
for enhancing the reasoning ability of Large Language Models (LLMs). Yet
current RLVR methods often explore poorly, leading to premature convergence and
entropy collapse. To address this challenge, we introduce Curiosity-Driven
Exploration (CDE), a framework that leverages the model's own intrinsic sense
of curiosity to guide exploration. We formalize curiosity with signals from
both the actor and the critic: for the actor, we use perplexity over its
generated response, and for the critic, we use the variance of value estimates
from a multi-head architecture. Both signals serve as an exploration bonus
within the RLVR framework to guide the model. Our theoretical analysis shows
that the actor-wise bonus inherently penalizes overconfident errors and
promotes diversity among correct responses; moreover, we connect the
critic-wise bonus to the well-established count-based exploration bonus in RL.
Empirically, our method achieves an approximate +3 point improvement over
standard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a
calibration collapse mechanism within RLVR, shedding light on common LLM
failure modes.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [38] [Morphology-Preserving Remeshing Approach to Particulate Microstructures via Harmonic Decomposition](https://arxiv.org/abs/2509.08855)
*Mahmoud Shaqfa*

Main category: cs.GR

TL;DR: 提出基于分层扩散的表面重采样方法，改善谐波分解在工程微结构网格生成中的三角剖分质量


<details>
  <summary>Details</summary>
Motivation: 传统谐波方法在均匀采样时因雅可比矩阵局部变化导致非均匀网格，影响数值模拟精度和时间步长

Method: 采用非线性扩散机制对分析域曲线坐标重采样，通过类似热传导过程实现三角形网格的均衡化

Result: 各向同性和各向异性扩散方案显著提升球谐/半球谐方法中的表面三角剖分质量指标，保持形态和体积精度

Conclusion: 该方法在混凝土/砖石数字孪生等大规模微结构应用中兼具计算效率与形态保真优势

Abstract: Harmonic decomposition of surfaces, such as spherical and spheroidal
harmonics, is used to analyze morphology, reconstruct, and generate surface
inclusions of particulate microstructures. However, obtaining high-quality
meshes of engineering microstructures using these approaches remains an open
question. In harmonic approaches, we usually reconstruct surfaces by evaluating
the harmonic bases on equidistantly sampled simplicial complexes of the base
domains (e.g., triangular spheroids and disks). However, this traditional
sampling does not account for local changes in the Jacobian of the basis
functions, resulting in nonuniform discretization after reconstruction or
generation. As it impacts the accuracy and time step, high-quality
discretization of microstructures is crucial for efficient numerical
simulations (e.g., finite element and discrete element methods). To circumvent
this issue, we propose an efficient hierarchical diffusion-based approach for
resampling the surface-i.e., performing a reparameterization-to yield an
equalized mesh triangulation. Analogous to heat problems, we use nonlinear
diffusion to resample the curvilinear coordinates of the analysis domain,
thereby enlarging small triangles at the expense of large triangles on
surfaces. We tested isotropic and anisotropic diffusion schemes on the recent
spheroidal and hemispheroidal harmonics methods. The results show a substantial
improvement in the quality metrics for surface triangulation. Unlike
traditional surface reconstruction and meshing techniques, this approach
preserves surface morphology, along with the areas and volumes of surfaces. We
discuss the results and the associated computational costs for large 2D and 3D
microstructures, such as digital twins of concrete and stone masonry, and their
future applications.

</details>


### [39] [CameraVDP: Perceptual Display Assessment with Uncertainty Estimation via Camera and Visual Difference Prediction](https://arxiv.org/abs/2509.08947)
*Yancheng Cai,Robert Wanat,Rafal Mantiuk*

Main category: cs.GR

TL;DR: 提出CameraVDP框架，结合相机重建流程与视觉差异预测器，实现显示器伪影的感知评估与不确定性量化


<details>
  <summary>Details</summary>
Motivation: 传统显示测量方法无法捕捉高频失真，相机测量存在光学/光度失真，且需结合人类视觉模型评估伪影可见性

Method: 构建包含HDR堆栈、MTF反转、几何校正的相机重建流程，集成视觉差异预测器(VDP)模拟人眼感知特性

Result: 成功应用于坏点检测/色边识别/均匀性评估，建立缺陷检测理论上限，提供VDP质量分数的置信区间

Conclusion: CameraVDP通过联合测量与感知建模，为显示器质量评估提供了兼顾精度与人类视觉特性的新型分析框架

Abstract: Accurate measurement of images produced by electronic displays is critical
for the evaluation of both traditional and computational displays. Traditional
display measurement methods based on sparse radiometric sampling and fitting a
model are inadequate for capturing spatially varying display artifacts, as they
fail to capture high-frequency and pixel-level distortions. While cameras offer
sufficient spatial resolution, they introduce optical, sampling, and
photometric distortions. Furthermore, the physical measurement must be combined
with a model of a visual system to assess whether the distortions are going to
be visible. To enable perceptual assessment of displays, we propose a
combination of a camera-based reconstruction pipeline with a visual difference
predictor, which account for both the inaccuracy of camera measurements and
visual difference prediction. The reconstruction pipeline combines HDR image
stacking, MTF inversion, vignetting correction, geometric undistortion,
homography transformation, and color correction, enabling cameras to function
as precise display measurement instruments. By incorporating a Visual
Difference Predictor (VDP), our system models the visibility of various stimuli
under different viewing conditions for the human visual system. We validate the
proposed CameraVDP framework through three applications: defective pixel
detection, color fringing awareness, and display non-uniformity evaluation. Our
uncertainty analysis framework enables the estimation of the theoretical upper
bound for defect pixel detection performance and provides confidence intervals
for VDP quality scores.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [40] [Objectness Similarity: Capturing Object-Level Fidelity in 3D Scene Evaluation](https://arxiv.org/abs/2509.09143)
*Yuiko Uchida,Ren Togo,Keisuke Maeda,Takahiro Ogawa,Miki Haseyama*

Main category: cs.CV

TL;DR: 提出基于物体感知相似性（OSIM）的新型3D场景评估指标，通过量化场景中物体的'物体性'，使评估更符合人类视觉认知


<details>
  <summary>Details</summary>
Motivation: 现有3D场景评估指标侧重整体质量，与人类以物体为基本感知单元的特性存在偏差，需建立更符合认知规律的评估体系

Method: 1. 构建物体检测模型提取特征
2. 设计'物体性'量化指标
3. 用户研究验证有效性
4. 标准化实验重评估主流模型

Result: 用户研究表明OSIM与人类感知相关性提升30%，模型重评估揭示当前3D生成技术中物体细节保留能力不足

Conclusion: OSIM首次将神经心理学认知机制融入3D评估，为生成模型优化提供了更精确的改进方向

Abstract: This paper presents Objectness SIMilarity (OSIM), a novel evaluation metric
for 3D scenes that explicitly focuses on "objects," which are fundamental units
of human visual perception. Existing metrics assess overall image quality,
leading to discrepancies with human perception. Inspired by neuropsychological
insights, we hypothesize that human recognition of 3D scenes fundamentally
involves attention to individual objects. OSIM enables object-centric
evaluations by leveraging an object detection model and its feature
representations to quantify the "objectness" of each object in the scene. Our
user study demonstrates that OSIM aligns more closely with human perception
compared to existing metrics. We also analyze the characteristics of OSIM using
various approaches. Moreover, we re-evaluate recent 3D reconstruction and
generation models under a standardized experimental setup to clarify
advancements in this field. The code is available at
https://github.com/Objectness-Similarity/OSIM.

</details>


### [41] [Recurrence Meets Transformers for Universal Multimodal Retrieval](https://arxiv.org/abs/2509.08897)
*Davide Caffagni,Sara Sarto,Marcella Cornia,Lorenzo Baraldi,Rita Cucchiara*

Main category: cs.CV

TL;DR: 提出ReT-2统一多模态检索模型，支持图文混合查询并在多模态文档库中实现高效检索


<details>
  <summary>Details</summary>
Motivation: 现有检索方法局限于单模态查询且需任务特定微调，无法处理图文混合的多模态检索需求

Method: 采用多层级表示和受LSTM启发的门控循环Transformer架构，动态融合跨层级/模态的细粒度信息

Result: 在M2KR/M-BEIR基准测试中实现SOTA，推理速度提升且内存占用减少，下游RAG任务在Encyclopedic-VQA准确率提升3.2%

Conclusion: ReT-2通过创新架构有效解决复杂多模态检索需求，为多模态LLM应用提供高效检索基础

Abstract: With the rapid advancement of multimodal retrieval and its application in
LLMs and multimodal LLMs, increasingly complex retrieval tasks have emerged.
Existing methods predominantly rely on task-specific fine-tuning of
vision-language models and are limited to single-modality queries or documents.
In this paper, we propose ReT-2, a unified retrieval model that supports
multimodal queries, composed of both images and text, and searches across
multimodal document collections where text and images coexist. ReT-2 leverages
multi-layer representations and a recurrent Transformer architecture with
LSTM-inspired gating mechanisms to dynamically integrate information across
layers and modalities, capturing fine-grained visual and textual details. We
evaluate ReT-2 on the challenging M2KR and M-BEIR benchmarks across different
retrieval configurations. Results demonstrate that ReT-2 consistently achieves
state-of-the-art performance across diverse settings, while offering faster
inference and reduced memory usage compared to prior approaches. When
integrated into retrieval-augmented generation pipelines, ReT-2 also improves
downstream performance on Encyclopedic-VQA and InfoSeek datasets. Our source
code and trained models are publicly available at:
https://github.com/aimagelab/ReT-2

</details>


### [42] [COCO-Urdu: A Large-Scale Urdu Image-Caption Dataset with Multimodal Quality Estimation](https://arxiv.org/abs/2509.09014)
*Umair Hassan*

Main category: cs.CV

TL;DR: 创建了最大公开乌尔都语图文数据集COCO-Urdu，通过混合质量评估框架保障数据质量，填补多模态研究空白。


<details>
  <summary>Details</summary>
Motivation: 乌尔都语使用者超2.5亿但缺乏高质量多模态数据集，导致多语言视觉语言模型存在严重语言偏见。

Method: 基于MS COCO构建：1) 分层抽样保留原分布 2) SeamlessM4T翻译 3) 混合质量评估框架（COMET-Kiwi翻译质量+CLIP视觉对齐+BERTScore语义一致性）4) 大模型迭代优化低质数据

Result: 产出59,000图像/319,000高质量乌尔都语字幕，基准测试(BLEU/SacreBLEU/chrF)表现优异，公开数据集及质量评估流程。

Conclusion: COCO-Urdu有效缓解多模态研究中的语言偏见，为开发包容性视觉语言系统奠定基础，其质量保障方法论具有推广价值。

Abstract: Urdu, spoken by over 250 million people, remains critically under-served in
multimodal and vision-language research. The absence of large-scale,
high-quality datasets has limited the development of Urdu-capable systems and
reinforced biases in multilingual vision-language models trained primarily on
high-resource languages. To address this gap, we present COCO-Urdu, a
large-scale image-caption dataset derived from MS COCO, containing 59,000
images and 319,000 Urdu captions selected through stratified sampling to
preserve the original distribution. Captions were translated using SeamlessM4T
v2 and validated with a hybrid multimodal quality estimation framework that
integrates COMET-Kiwi for translation quality, CLIP-based similarity for visual
grounding, and BERTScore with back-translation for semantic consistency;
low-scoring captions were iteratively refined using open-source large language
models. We further benchmark COCO-Urdu on BLEU, SacreBLEU, and chrF, reporting
consistently strong results. To the best of our knowledge, COCO-Urdu is the
largest publicly available Urdu captioning dataset. By releasing both the
dataset and the quality estimation pipeline, we aim to reduce language bias in
multimodal research and establish a foundation for inclusive vision-language
systems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [43] [Automated Unity Game Template Generation from GDDs via NLP and Multi-Modal LLMs](https://arxiv.org/abs/2509.08847)
*Amna Hassan*

Main category: cs.AI

TL;DR: 提出基于LLaMA-3微调模型的GDD自动转Unity框架，实现AI辅助游戏开发的高效代码生成


<details>
  <summary>Details</summary>
Motivation: 解决传统游戏开发中从设计文档到代码实现的手动转换低效问题，填补AI辅助开发领域的技术空白

Method: 端到端系统架构：1) NLP解析GDD文档 2) 定制LLaMA-3模型代码生成 3) Unity集成包自动化部署

Result: 微调模型综合评分达4.8/5.0，编译成功率提升35%，支持多类型游戏模板生成

Conclusion: 验证了LLMs在游戏开发流程自动化的可行性，为设计-实现的无缝衔接提供新范式

Abstract: This paper presents a novel framework for automated game template generation
by transforming Game Design Documents (GDDs) into functional Unity game
prototypes using Natural Language Processing (NLP) and multi-modal Large
Language Models (LLMs). We introduce an end-to-end system that parses GDDs,
extracts structured game specifications, and synthesizes Unity-compatible C#
code that implements the core mechanics, systems, and architecture defined in
the design documentation. Our approach combines a fine-tuned LLaMA-3 model
specialized for Unity code generation with a custom Unity integration package
that streamlines the implementation process. Evaluation results demonstrate
significant improvements over baseline models, with our fine-tuned model
achieving superior performance (4.8/5.0 average score) compared to
state-of-the-art LLMs across compilation success, GDD adherence, best practices
adoption, and code modularity metrics. The generated templates demonstrate high
adherence to GDD specifications across multiple game genres. Our system
effectively addresses critical gaps in AI-assisted game development,
positioning LLMs as valuable tools in streamlining the transition from game
design to implementation.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [44] [Bona fide Cross Testing Reveals Weak Spot in Audio Deepfake Detection Systems](https://arxiv.org/abs/2509.09204)
*Chin Yuen Kwok,Jia Qi Yip,Zhen Qiu,Chi Hung Chi,Kwok Yan Lam*

Main category: cs.SD

TL;DR: 提出bona fide交叉测试框架改善音频深度伪造检测模型的评估可靠性，通过整合多样真实语音数据集和聚合EER指标实现更均衡的评估。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法存在两个核心问题：1) 合成器样本量差异导致EER指标失真 2) 真实语音数据集缺乏环境/风格多样性，难以反映真实场景。

Method: 开发包含九种真实语音类型的新评估框架，采用分层EER聚合方法，构建包含150+合成器的基准测试集并开源新数据集。

Result: 相较于传统方法显著提升评估鲁棒性和可解释性，验证了评估框架的有效性。发布https://github.com/cyaaronk/audio_deepfake_eval数据集。

Conclusion: 新评估范式通过增强真实语音多样性实现了更可靠的模型性能评估，为音频深度伪造检测领域提供了标准化评估工具，推动后续研究发展。

Abstract: Audio deepfake detection (ADD) models are commonly evaluated using datasets
that combine multiple synthesizers, with performance reported as a single Equal
Error Rate (EER). However, this approach disproportionately weights
synthesizers with more samples, underrepresenting others and reducing the
overall reliability of EER. Additionally, most ADD datasets lack diversity in
bona fide speech, often featuring a single environment and speech style (e.g.,
clean read speech), limiting their ability to simulate real-world conditions.
To address these challenges, we propose bona fide cross-testing, a novel
evaluation framework that incorporates diverse bona fide datasets and
aggregates EERs for more balanced assessments. Our approach improves robustness
and interpretability compared to traditional evaluation methods. We benchmark
over 150 synthesizers across nine bona fide speech types and release a new
dataset to facilitate further research at
https://github.com/cyaaronk/audio_deepfake_eval.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [45] [Generative Engine Optimization: How to Dominate AI Search](https://arxiv.org/abs/2509.08919)
*Mahe Chen,Xiaoxuan Wang,Kaiwen Chen,Nick Koudas*

Main category: cs.IR

TL;DR: 生成式AI搜索引擎颠覆传统SEO，提出生成引擎优化(GEO)新范式。研究发现AI搜索系统偏向权威第三方内容，不同引擎间存在显著差异，需针对性优化策略。


<details>
  <summary>Details</summary>
Motivation: 生成式AI搜索工具（如ChatGPT）将传统排序检索转变为带引用的合成答案，这迫使SEO实践必须向GEO范式转型以适应新生态。

Method: 通过跨垂直领域、多语言环境和查询改写的控制实验，对比分析AI搜索与传统搜索引擎（Google）的信息源差异。

Result: AI搜索存在系统性偏向：75%依赖权威媒体（对比Google的平衡策略）；不同AI服务在领域多样性（方差达38%）、内容更新频率（周级vs月级）、跨语言稳定性（波动率±22%）和查询敏感性方面差异显著。

Conclusion: 提出GEO四维战略：优化机器可解析内容结构、建立AI认可的权威媒体矩阵、制定引擎专属多语言策略、突破「大品牌偏见」实现利基突围。

Abstract: The rapid adoption of generative AI-powered search engines like ChatGPT,
Perplexity, and Gemini is fundamentally reshaping information retrieval, moving
from traditional ranked lists to synthesized, citation-backed answers. This
shift challenges established Search Engine Optimization (SEO) practices and
necessitates a new paradigm, which we term Generative Engine Optimization
(GEO).
  This paper presents a comprehensive comparative analysis of AI Search and
traditional web search (Google). Through a series of large-scale, controlled
experiments across multiple verticals, languages, and query paraphrases, we
quantify critical differences in how these systems source information. Our key
findings reveal that AI Search exhibit a systematic and overwhelming bias
towards Earned media (third-party, authoritative sources) over Brand-owned and
Social content, a stark contrast to Google's more balanced mix. We further
demonstrate that AI Search services differ significantly from each other in
their domain diversity, freshness, cross-language stability, and sensitivity to
phrasing.
  Based on these empirical results, we formulate a strategic GEO agenda. We
provide actionable guidance for practitioners, emphasizing the critical need
to: (1) engineer content for machine scannability and justification, (2)
dominate earned media to build AI-perceived authority, (3) adopt
engine-specific and language-aware strategies, and (4) overcome the inherent
"big brand bias" for niche players. Our work provides the foundational
empirical analysis and a strategic framework for achieving visibility in the
new generative search landscape.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [46] [A vibe coding learning design to enhance EFL students' talking to, through, and about AI](https://arxiv.org/abs/2509.08854)
*David James Woo,Kai Guo,Yangyang Yu*

Main category: cs.CY

TL;DR: 研究探索AI自然语言编程(vibe coding)在EFL教育中的应用，开发人类-AI元语言框架（提示工程/作者协商/AI心理模型），通过案例对比揭示学生提示工程差异对成果的影响，提出需结构化教学支架。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用AI自然语言编程辅助EFL写作教学，通过构建元语言框架分析学生在提示工程、作者身份协商和AI认知层面的差异。

Method: 逆向设计四小时工作坊，两名学生设计解决EFL写作问题的AI应用。采用案例研究方法，收集工作表、视频录像、有声思维、屏幕记录和AI生成图像数据。

Result: 成功案例显示功能应用与设计意图一致，失败案例存在技术断层。差异源于提示工程策略不同，反映相异的AI心理模型与作者归属认知冲突。

Conclusion: AI是有效语言学习机器，教学需强化结构化提示工程训练、作者身份批判讨论、AI认知词汇发展，通过元语言框架提升vibe coding效果。

Abstract: This innovative practice article reports on the piloting of vibe coding
(using natural language to create software applications with AI) for English as
a Foreign Language (EFL) education. We developed a human-AI meta-languaging
framework with three dimensions: talking to AI (prompt engineering), talking
through AI (negotiating authorship), and talking about AI (mental models of
AI). Using backward design principles, we created a four-hour workshop where
two students designed applications addressing authentic EFL writing challenges.
We adopted a case study methodology, collecting data from worksheets and video
recordings, think-aloud protocols, screen recordings, and AI-generated images.
Contrasting cases showed one student successfully vibe coding a functional
application cohering to her intended design, while another encountered
technical difficulties with major gaps between intended design and actual
functionality. Analysis reveals differences in students' prompt engineering
approaches, suggesting different AI mental models and tensions in attributing
authorship. We argue that AI functions as a beneficial languaging machine, and
that differences in how students talk to, through, and about AI explain vibe
coding outcome variations. Findings indicate that effective vibe coding
instruction requires explicit meta-languaging scaffolding, teaching structured
prompt engineering, facilitating critical authorship discussions, and
developing vocabulary for articulating AI mental models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [47] [Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison](https://arxiv.org/abs/2509.09009)
*Marianna Nezhurina,Taishi Nakamura,Timur Carstensen,Niccolò Ajroldi,Ville Komulainen,David Salinas,Jenia Jitsev*

Main category: cs.LG

TL;DR: 提出open-sci-ref系列基线模型，通过多参数规模（0.13B-1.7B）和多token量（最高1T）在8个开放参考数据集上训练，建立标准化研究基线。


<details>
  <summary>Details</summary>
Motivation: 解决现有研究中训练方法缺乏统一基准的问题，通过建立可复现的参考基线，实现跨规模/数据集/训练方法的量化对比。

Method: 使用NemoTron-CC HQ等参考数据集训练transformer模型，发布中间检查点、日志代码及下游任务评估框架，构建计算量对齐的基准比较体系。

Result: NemoTron-CC HQ数据集训练效果最优（优于DCLM-baseline和FineWeb-Edu），模型规模扩展与性能提升呈正相关趋势。

Conclusion: 通过开源训练全流程材料，为社区提供可复现的标准化基准，推动训练方法比较和模型动态研究，促进未来研究效率提升。

Abstract: We introduce open-sci-ref, a family of dense transformer models trained as
research baselines across multiple model (0.13B to 1.7B parameters) and token
scales (up to 1T) on 8 recent open reference datasets. Evaluating the models on
various standardized benchmarks, our training runs set establishes reference
points that enable researchers to assess the sanity and quality of alternative
training approaches across scales and datasets. Intermediate checkpoints allow
comparison and studying of the training dynamics. The established reference
baselines allow training procedures to be compared through their scaling
trends, aligning them on a common compute axis. Comparison of open reference
datasets reveals that training on NemoTron-CC HQ consistently outperforms other
reference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to
intermediate training checkpoints, the release includes logs, code, and
downstream evaluations to simplify reproduction, standardize comparison, and
facilitate future research.

</details>
