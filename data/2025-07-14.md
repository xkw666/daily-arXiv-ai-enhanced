<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 54]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [RepeaTTS: Towards Feature Discovery through Repeated Fine-Tuning](https://arxiv.org/abs/2507.08012)
*Atli Sigurgeirsson,Simon King*

Main category: cs.CL

TL;DR: 提出通过PCA分析不可控方差特征并二次微调的方法，提升基于提示的TTS模型可控性


<details>
  <summary>Details</summary>
Motivation: 现有基于自然语言指令的TTS模型存在双重限制：控制维度受限于训练时预设的声学特征，同时相同输入会引发不可控的语料统计变异

Method: 1. 对数千合成样本进行PCA分析，提取主导输出方差潜在特征 2. 将其作为新标签进行二次微调 3. 在冰岛语情感/非情感两种语音模型上验证

Result: 非情感披露模型中获得连续/离散双重控制特征，显著提升整体可控性；情感模型效果未明确

Conclusion: 该方法有效转化模型的不可控方差为可控特征，为提示型TTS系统优化提供新方向

Abstract: A Prompt-based Text-To-Speech model allows a user to control different
aspects of speech, such as speaking rate and perceived gender, through natural
language instruction. Although user-friendly, such approaches are on one hand
constrained: control is limited to acoustic features exposed to the model
during training, and too flexible on the other: the same inputs yields
uncontrollable variation that are reflected in the corpus statistics.
  We investigate a novel fine-tuning regime to address both of these issues at
the same time by exploiting the uncontrollable variance of the model. Through
principal component analysis of thousands of synthesised samples, we determine
latent features that account for the highest proportion of the output variance
and incorporate them as new labels for secondary fine-tuning. We evaluate the
proposed methods on two models trained on an expressive Icelandic speech
corpus, one with emotional disclosure and one without. In the case of the model
without emotional disclosure, the method yields both continuous and discrete
features that improve overall controllability of the model.

</details>


### [2] [MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model](https://arxiv.org/abs/2507.08013)
*K. Sahit Reddy,N. Ragavenderan,Vasanth K.,Ganesh N. Naik,Vishalakshi Prabhu,Nagaraja G. S*

Main category: cs.CL

TL;DR: MedicalBERT基于大规模生物医学数据预训练，在命名实体识别、关系抽取等任务中平均比通用BERT模型提升5.67%，验证了领域适应预训练的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有模型（如BERT、GPT）在处理生物医学术语时存在局限性，Word2Vec和Bi-LSTM无法充分捕捉语义关联，需开发领域专用的预训练模型。

Method: 通过构建生物医学领域词典，在大规模生物医学语料上预训练BERT架构，并针对实体识别、文本分类等任务进行微调优化，采用F1值等指标对比BioBERT等模型。

Result: 在多数基准测试中超越BioBERT/SciBERT/ClinicalBERT，相比通用BERT平均提升5.67%性能，尤其在句对相似度任务中Pearson相关系数提升显著。

Conclusion: 证实领域定制化预训练能有效提升生物医学NLP任务表现，为医疗文本分析提供了高效的迁移学习解决方案。

Abstract: Recent advances in natural language processing (NLP) have been driven
bypretrained language models like BERT, RoBERTa, T5, and GPT. Thesemodels excel
at understanding complex texts, but biomedical literature, withits
domain-specific terminology, poses challenges that models likeWord2Vec and
bidirectional long short-term memory (Bi-LSTM) can't fullyaddress. GPT and T5,
despite capturing context, fall short in tasks needingbidirectional
understanding, unlike BERT. Addressing this, we proposedMedicalBERT, a
pretrained BERT model trained on a large biomedicaldataset and equipped with
domain-specific vocabulary that enhances thecomprehension of biomedical
terminology. MedicalBERT model is furtheroptimized and fine-tuned to address
diverse tasks, including named entityrecognition, relation extraction, question
answering, sentence similarity, anddocument classification. Performance metrics
such as the F1-score,accuracy, and Pearson correlation are employed to showcase
the efficiencyof our model in comparison to other BERT-based models such as
BioBERT,SciBERT, and ClinicalBERT. MedicalBERT outperforms these models onmost
of the benchmarks, and surpasses the general-purpose BERT model by5.67% on
average across all the tasks evaluated respectively. This work alsounderscores
the potential of leveraging pretrained BERT models for medicalNLP tasks,
demonstrating the effectiveness of transfer learning techniques incapturing
domain-specific information.
  (PDF) MedicalBERT: enhancing biomedical natural language processing using
pretrained BERT-based model. Available from:
https://www.researchgate.net/publication/392489050_MedicalBERT_enhancing_biomedical_natural_language_processing_using_pretrained_BERT-based_model
[accessed Jul 06 2025].

</details>


### [3] [Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking](https://arxiv.org/abs/2507.08014)
*Aldan Creo,Raul Castro Fernandez,Manuel Cebrian*

Main category: cs.CL

TL;DR: 该研究通过分析200多万真实对话数据，发现LLM越狱攻击的复杂性并未显著高于正常对话，且安全机制持续改进。学术披露复杂攻击方法可能破坏现有安全平衡。


<details>
  <summary>Details</summary>
Motivation: 探究LLM越狱攻击的实际复杂度演变规律，为AI安全机制优化提供实证依据，揭示攻防博弈的真实边界。

Method: 使用概率测量、词汇多样性分析、压缩比和认知负荷指标等多维度复杂度指标，结合跨平台（专业越狱社区/通用聊天机器人）数据的时间序列分析。

Result: 越狱攻击复杂度存在天然上限；安全机制毒性响应下降83%；复杂性分布符合对数正态而非幂律，显示人类创造力边界约束。

Conclusion: LLM安全演化受限于人类创新极限，防御机制持续有效。需警惕学术越狱披露的信息风险，建议建立攻击方法的分级披露制度。

Abstract: As large language models (LLMs) become increasingly deployed, understanding
the complexity and evolution of jailbreaking strategies is critical for AI
safety.
  We present a mass-scale empirical analysis of jailbreak complexity across
over 2 million real-world conversations from diverse platforms, including
dedicated jailbreaking communities and general-purpose chatbots. Using a range
of complexity metrics spanning probabilistic measures, lexical diversity,
compression ratios, and cognitive load indicators, we find that jailbreak
attempts do not exhibit significantly higher complexity than normal
conversations. This pattern holds consistently across specialized jailbreaking
communities and general user populations, suggesting practical bounds on attack
sophistication. Temporal analysis reveals that while user attack toxicity and
complexity remains stable over time, assistant response toxicity has decreased,
indicating improving safety mechanisms. The absence of power-law scaling in
complexity distributions further points to natural limits on jailbreak
development.
  Our findings challenge the prevailing narrative of an escalating arms race
between attackers and defenders, instead suggesting that LLM safety evolution
is bounded by human ingenuity constraints while defensive measures continue
advancing. Our results highlight critical information hazards in academic
jailbreak disclosure, as sophisticated attacks exceeding current complexity
baselines could disrupt the observed equilibrium and enable widespread harm
before defensive adaptation.

</details>


### [4] [Assessing the Capabilities and Limitations of FinGPT Model in Financial NLP Applications](https://arxiv.org/abs/2507.08015)
*Prudence Djagba,Chimezie A. Odinakachukwu*

Main category: cs.CL

TL;DR: FinGPT在金融分类任务表现优异，但推理生成能力不足，需进一步优化


<details>
  <summary>Details</summary>
Motivation: 评估FinGPT在金融领域六大NLP任务的实际应用能力，揭示其优势与局限性

Method: 使用六个金融领域NLP任务（含情感分析/文本分类/问答等）的专用数据集进行测试，并与GPT-4及人类基准对比

Result: 分类任务接近GPT-4水平，但问答/摘要等生成任务表现显著落后，数值准确性差距达30%以上

Conclusion: FinGPT目前仅适用于结构化金融任务，需架构改进和领域优化才能成为全面解决方案

Abstract: This work evaluates FinGPT, a financial domain-specific language model,
across six key natural language processing (NLP) tasks: Sentiment Analysis,
Text Classification, Named Entity Recognition, Financial Question Answering,
Text Summarization, and Stock Movement Prediction. The evaluation uses
finance-specific datasets to assess FinGPT's capabilities and limitations in
real-world financial applications. The results show that FinGPT performs
strongly in classification tasks such as sentiment analysis and headline
categorization, often achieving results comparable to GPT-4. However, its
performance is significantly lower in tasks that involve reasoning and
generation, such as financial question answering and summarization. Comparisons
with GPT-4 and human benchmarks highlight notable performance gaps,
particularly in numerical accuracy and complex reasoning. Overall, the findings
indicate that while FinGPT is effective for certain structured financial tasks,
it is not yet a comprehensive solution. This research provides a useful
benchmark for future research and underscores the need for architectural
improvements and domain-specific optimization in financial language models.

</details>


### [5] [Mechanistic Indicators of Understanding in Large Language Models](https://arxiv.org/abs/2507.08017)
*Pierre Beckmann,Matthieu Queloz*

Main category: cs.CL

TL;DR: 论文通过机制可解释性研究，提出三层机器理解框架（概念/世界状态/原则性理解），认为LLMs具有类似人类认知的连接性理解能力，但其并行机制架构与人类不同


<details>
  <summary>Details</summary>
Motivation: 反驳LLMs仅依赖表面统计的传统观点，建立机器理解的新理论框架，解释模型内部结构如何实现连接性认知

Method: 基于机制可解释性研究成果，构建三层次理解模型：特征方向形成概念关联→动态追踪事实关联→电路连接实现原则性理解

Result: 揭示LLMs通过潜在空间方向特征和电路结构形成类似人类的理解机制，但存在并行处理等根本性架构差异

Conclusion: 应转向研究LLMs独特的认知架构运作方式，而非争论是否具备理解能力，强调机器理解模式的异质性特征

Abstract: Recent findings in mechanistic interpretability (MI), the field probing the
inner workings of Large Language Models (LLMs), challenge the view that these
models rely solely on superficial statistics. Here, we offer an accessible
synthesis of these findings that doubles as an introduction to MI, all while
integrating these findings within a novel theoretical framework for thinking
about machine understanding. We argue that LLMs develop internal structures
that are functionally analogous to the kind of understanding that consists in
seeing connections. To sharpen this idea, we propose a three-tiered conception
of machine understanding. First, conceptual understanding emerges when a model
forms "features" as directions in latent space, thereby learning the
connections between diverse manifestations of something. Second,
state-of-the-world understanding emerges when a model learns contingent factual
connections between features and dynamically tracks changes in the world.
Third, principled understanding emerges when a model ceases to rely on a
collection of memorized facts and discovers a "circuit" that connects these
facts. However, we conclude by exploring the "parallel mechanisms" phenomenon,
arguing that while LLMs exhibit forms of understanding, their cognitive
architecture remains different from ours, and the debate should shift from
whether LLMs understand to how their strange minds work.

</details>


### [6] [Review, Remask, Refine (R3): Process-Guided Block Diffusion for Text Generation](https://arxiv.org/abs/2507.08018)
*Nikita Mounier,Parsa Idehpour*

Main category: cs.CL

TL;DR: 提出无需额外训练的R3框架，通过PRM评估、动态掩码和细化提升文本生成质量


<details>
  <summary>Details</summary>
Motivation: 解决迭代文本生成模型中自我纠错效率低下的核心挑战，现有方法需要额外训练且难以聚焦错误修正

Method: 1. Review阶段：用过程奖励模型(PRM)评估文本块质量
2. Remask阶段：根据PRM分数动态调整掩码比例（低分块掩码更多）
3. Refine阶段：专注优化被掩码的次优文本块

Result: 通过针对性优化特定次优段落，显著提升最终生成质量（特别在逻辑连贯性和事实准确性方面）

Conclusion: R3框架以简洁架构实现文本生成优化，适用于各类预训练掩码扩散模型，为迭代式生成提供新范式

Abstract: A key challenge for iterative text generation is enabling models to
efficiently identify and correct their own errors. We propose Review, Remask,
Refine (R3), a relatively simple yet elegant framework that requires no
additional model training and can be applied to any pre-trained masked text
diffusion model (e.g., LLaDA or BD3-LM). In R3, a Process Reward Model (PRM) is
utilized for the Review of intermediate generated blocks. The framework then
translates these PRM scores into a Remask strategy: the lower a block's PRM
score, indicating potential mistakes, the greater the proportion of tokens
within that block are remasked. Finally, the model is compelled to Refine these
targeted segments, focusing its efforts more intensively on specific
sub-optimal parts of past generations, leading to improved final output.

</details>


### [7] [Signal or Noise? Evaluating Large Language Models in Resume Screening Across Contextual Variations and Human Expert Benchmarks](https://arxiv.org/abs/2507.08019)
*Aryan Varshney,Venkat Ram Reddy Ganuthula*

Main category: cs.CL

TL;DR: 研究通过方差分析和元认知分析发现，大语言模型在简历筛选中表现出可解释的适应性模式，但其评估标准与人类专家存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在招聘场景中的评估一致性（信号）与随机性（噪声），及其与人类专家判断的可比性。

Method: 使用三款LLM（Claude/GPT/Gemini）和三位招聘专家，在四种企业背景条件下（含随机简历），通过方差分析、配对t检验和元认知权重分析进行对比实验。

Result: LLM在8种条件中有4种显示显著差异，所有模型与人类评估差异显著（p<0.01）；GPT对企业背景适应最强（p<0.001），Gemini部分适应（Firm1 p=0.038），Claude最弱（p>0.1）；元认知分析显示LLM与人类评估模式存在本质差异。

Conclusion: 大语言模型在详细提示下可产生可解释的评估模式，但其判断逻辑与人类存在根本性分歧，这对自动化招聘系统的部署具有重要参考价值。

Abstract: This study investigates whether large language models (LLMs) exhibit
consistent behavior (signal) or random variation (noise) when screening resumes
against job descriptions, and how their performance compares to human experts.
Using controlled datasets, we tested three LLMs (Claude, GPT, and Gemini)
across contexts (No Company, Firm1 [MNC], Firm2 [Startup], Reduced Context)
with identical and randomized resumes, benchmarked against three human
recruitment experts. Analysis of variance revealed significant mean differences
in four of eight LLM-only conditions and consistently significant differences
between LLM and human evaluations (p < 0.01). Paired t-tests showed GPT adapts
strongly to company context (p < 0.001), Gemini partially (p = 0.038 for
Firm1), and Claude minimally (p > 0.1), while all LLMs differed significantly
from human experts across contexts. Meta-cognition analysis highlighted
adaptive weighting patterns that differ markedly from human evaluation
approaches. Findings suggest LLMs offer interpretable patterns with detailed
prompts but diverge substantially from human judgment, informing their
deployment in automated hiring systems.

</details>


### [8] [Circumventing Safety Alignment in Large Language Models Through Embedding Space Toxicity Attenuation](https://arxiv.org/abs/2507.08020)
*Zhibo Zhang,Yuxi Li,Kailong Wang,Shuai Yuan,Ling Shi,Haoyu Wang*

Main category: cs.CL

TL;DR: 提出ETTA框架，通过线性变换识别嵌入空间中的毒性敏感维度，有效绕过LLM安全机制，平均攻击成功率88.61%


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全对齐策略存在嵌入层漏洞，针对性对抗扰动技术研究不足，导致模型易受语义空间攻击

Method: 开发线性变换框架ETTA，无需微调模型或训练数据，通过毒性敏感维度衰减实现语义保持的攻击

Result: 在5个开源LLM上实现88.61%平均攻击成功率，比基线高11.34%，对安全增强模型达77.39%成功率

Conclusion: 当前安全对齐策略存在关键漏洞，需开发嵌入感知防御机制

Abstract: Large Language Models (LLMs) have achieved remarkable success across domains
such as healthcare, education, and cybersecurity. However, this openness also
introduces significant security risks, particularly through embedding space
poisoning, which is a subtle attack vector where adversaries manipulate the
internal semantic representations of input data to bypass safety alignment
mechanisms. While previous research has investigated universal perturbation
methods, the dynamics of LLM safety alignment at the embedding level remain
insufficiently understood. Consequently, more targeted and accurate adversarial
perturbation techniques, which pose significant threats, have not been
adequately studied.
  In this work, we propose ETTA (Embedding Transformation Toxicity
Attenuation), a novel framework that identifies and attenuates
toxicity-sensitive dimensions in embedding space via linear transformations.
ETTA bypasses model refusal behaviors while preserving linguistic coherence,
without requiring model fine-tuning or access to training data. Evaluated on
five representative open-source LLMs using the AdvBench benchmark, ETTA
achieves a high average attack success rate of 88.61%, outperforming the best
baseline by 11.34%, and generalizes to safety-enhanced models (e.g., 77.39% ASR
on instruction-tuned defenses). These results highlight a critical
vulnerability in current alignment strategies and underscore the need for
embedding-aware defenses.

</details>


### [9] [Unveiling Effective In-Context Configurations for Image Captioning: An External & Internal Analysis](https://arxiv.org/abs/2507.08021)
*Li Li,Yongliang Wu,Jingze Zhu,Jiawei Peng,Jianfei Cai,Xu Yang*

Main category: cs.CL

TL;DR: 通过内外双重视角系统研究多模态上下文学习在图像描述任务中的机制，提出新的评估指标和分析框架


<details>
  <summary>Details</summary>
Motivation: 现有对多模态ICL示例配置策略的研究不足，且通过可控的上下文示例可有效分析大型多模态模型的推理特性

Method: 外部实验从示例数量/图像检索/标题分配三维度探索配置策略；内部分析模型注意力特征并提出量化指标，同时开展模型加速压缩实验，比较不同LMMs性能差异

Result: 揭示了上下文示例配置策略对模型性能的影响规律，发现了典型注意力模式特征，提出的分析方法可扩展到更广泛的大模型研究领域

Conclusion: 内外结合的联合分析方法及新指标体系为理解多模态ICL提供了双重认知视角，其方法论具有普适推广价值

Abstract: The evolution of large models has witnessed the emergence of In-Context
Learning (ICL) capabilities. In Natural Language Processing (NLP), numerous
studies have demonstrated the effectiveness of ICL. Inspired by the success of
Large Language Models (LLMs), researchers have developed Large Multimodal
Models (LMMs) with ICL capabilities. However, explorations of demonstration
configuration for multimodal ICL remain preliminary. Additionally, the
controllability of In-Context Examples (ICEs) provides an efficient and
cost-effective means to observe and analyze the inference characteristics of
LMMs under varying inputs. This paper conducts a comprehensive external and
internal investigation of multimodal in-context learning on the image
captioning task. Externally, we explore demonstration configuration strategies
through three dimensions: shot number, image retrieval, and caption assignment.
We employ multiple metrics to systematically and thoroughly evaluate and
summarize key findings. Internally, we analyze typical LMM attention
characteristics and develop attention-based metrics to quantify model
behaviors. We also conduct auxiliary experiments to explore the feasibility of
attention-driven model acceleration and compression. We further compare
performance variations between LMMs with identical model design and pretraining
strategies and explain the differences from the angles of pre-training data
features. Our study reveals both how ICEs configuration strategies impact model
performance through external experiments and characteristic typical patterns
through internal inspection, providing dual perspectives for understanding
multimodal ICL in LMMs. Our method of combining external and internal analysis
to investigate large models, along with our newly proposed metrics, can be
applied to broader research areas.

</details>


### [10] ["Amazing, They All Lean Left" -- Analyzing the Political Temperaments of Current LLMs](https://arxiv.org/abs/2507.08027)
*W. Russell Neuman,Chad Coleman,Ali Dasdan,Safinah Ali,Manan Shah,Kund Meghani*

Main category: cs.CL

TL;DR: 主流大语言模型普遍呈现自由主义倾向，根源来自训练语料、RLHF机制和学术伦理框架影响，其政治倾斜可能为集体理性研究提供新视角


<details>
  <summary>Details</summary>
Motivation: 解释商业LLMs伦理政治回应中持续存在的自由主义倾向成因，并探讨其哲学与社会影响

Method: 结合道德基础理论、12个政治意识形态量表及新政治争议指数，系统分析7个主流模型（GPT-4o/Claude/Gemini等）

Result: 发现四重叠加因素：自由主义语料训练、RLHF机制、学术伦理框架主导、安全导向微调。模型微调普遍增强自由主义特征

Conclusion: LLMs的自由主义倾向是民主权利话语训练的涌现特征，可能反映罗尔斯'无知之幕'的道德立场，构成研究集体理性的新范式

Abstract: Recent studies have revealed a consistent liberal orientation in the ethical
and political responses generated by most commercial large language models
(LLMs), yet the underlying causes and resulting implications remain unclear.
This paper systematically investigates the political temperament of seven
prominent LLMs - OpenAI's GPT-4o, Anthropic's Claude Sonnet 4, Perplexity
(Sonar Large), Google's Gemini 2.5 Flash, Meta AI's Llama 4, Mistral 7b Le Chat
and High-Flyer's DeepSeek R1 -- using a multi-pronged approach that includes
Moral Foundations Theory, a dozen established political ideology scales and a
new index of current political controversies. We find strong and consistent
prioritization of liberal-leaning values, particularly care and fairness,
across most models. Further analysis attributes this trend to four overlapping
factors: Liberal-leaning training corpora, reinforcement learning from human
feedback (RLHF), the dominance of liberal frameworks in academic ethical
discourse and safety-driven fine-tuning practices. We also distinguish between
political "bias" and legitimate epistemic differences, cautioning against
conflating the two. A comparison of base and fine-tuned model pairs reveals
that fine-tuning generally increases liberal lean, an effect confirmed through
both self-report and empirical testing. We argue that this "liberal tilt" is
not a programming error or the personal preference of programmers but an
emergent property of training on democratic rights-focused discourse. Finally,
we propose that LLMs may indirectly echo John Rawls' famous veil-of ignorance
philosophical aspiration, reflecting a moral stance unanchored to personal
identity or interest. Rather than undermining democratic discourse, this
pattern may offer a new lens through which to examine collective reasoning.

</details>


### [11] [Better Together: Quantifying the Benefits of AI-Assisted Recruitment](https://arxiv.org/abs/2507.08029)
*Ada Aka,Emil Palikot,Ali Ansari,Nima Yazdani*

Main category: cs.CL

TL;DR: AI辅助招聘使最终面试通过率提升20个百分点，后续就业概率增加5.9个百分点，但存在选择年轻/低资历候选人的倾向


<details>
  <summary>Details</summary>
Motivation: 量化人工智能在招聘流程中对效率及候选人选择的具体影响，填补实证研究空白

Method: 随机分配37,000名申请人至传统流程（简历筛选+人工筛选）或AI辅助流程（AI视频面试+人工筛选），最终统一进行盲测人工面试，五个月后追踪LinkedIn就业数据

Result: AI组最终面试通过率54% vs 传统组34%；AI组后续就业率23% vs 传统组18%，但AI更倾向选择年轻（平均年龄低2.1岁）、工作经验少（少1.4年）、无硕士学位的候选人

Conclusion: AI技术显著提升招聘效率，但需警惕算法偏见对人才选拔的影响，建议平衡技术优势与公平性考量

Abstract: Artificial intelligence (AI) is increasingly used in recruitment, yet
empirical evidence quantifying its impact on hiring efficiency and candidate
selection remains limited. We randomly assign 37,000 applicants for a
junior-developer position to either a traditional recruitment process (resume
screening followed by human selection) or an AI-assisted recruitment pipeline
incorporating an initial AI-driven structured video interview before human
evaluation. Candidates advancing from either track faced the same final-stage
human interview, with interviewers blind to the earlier selection method. In
the AI-assisted pipeline, 54% of candidates passed the final interview compared
with 34% from the traditional pipeline, yielding an average treatment effect of
20 percentage points (SE 12 pp.). Five months later, we collected LinkedIn
profiles of top applicants from both groups and found that 18% (SE 1.1%) of
applicants from the traditional track found new jobs compared with 23% (SE
2.3%) from the AI group, resulting in a 5.9 pp. (SE 2.6 pp.) difference in the
probability of finding new employment between groups. The AI system tended to
select younger applicants with less experience and fewer advanced credentials.
We analyze AI-generated interview transcripts to examine the selection criteria
and conversational dynamics. Our findings contribute to understanding how AI
technologies affect decision making in recruitment and talent acquisition while
highlighting some of their potential implications.

</details>


### [12] [A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models](https://arxiv.org/abs/2507.08030)
*Sonali Sharma,Ahmed M. Alaa,Roxana Daneshjou*

Main category: cs.CL

TL;DR: 研究发现生成式AI模型在医学应用中的免责声明使用率从2022年的26.3%骤降至2025年的不足1%，需加强临床情境适配的保障措施。


<details>
  <summary>Details</summary>
Motivation: 生成式AI模型（LLM/VLM）在医学影像解读和临床问答中应用激增，但其输出存在不准确性。医疗免责声明作为安全措施，需持续评估其存在必要性。

Method: 使用500份乳腺X光片、500份胸片、500份皮肤科图像和500个医学问题，筛查2022-2025年间不同世代模型的输出中免责声明的存在情况。

Result: LLM输出的免责声明存在率从2022年26.3%降至2025年0.97%，VLM从2023年19.6%降至2025年1.05%。2025年多数模型无任何免责声明。

Conclusion: 随着模型能力提升和权威性增强，必须实施动态适配临床情境的免责声明机制，作为保障患者安全的核心措施。

Abstract: Generative AI models, including large language models (LLMs) and
vision-language models (VLMs), are increasingly used to interpret medical
images and answer clinical questions. Their responses often include
inaccuracies; therefore, safety measures like medical disclaimers are critical
to remind users that AI outputs are not professionally vetted or a substitute
for medical advice. This study evaluated the presence of disclaimers in LLM and
VLM outputs across model generations from 2022 to 2025. Using 500 mammograms,
500 chest X-rays, 500 dermatology images, and 500 medical questions, outputs
were screened for disclaimer phrases. Medical disclaimer presence in LLM and
VLM outputs dropped from 26.3% in 2022 to 0.97% in 2025, and from 19.6% in 2023
to 1.05% in 2025, respectively. By 2025, the majority of models displayed no
disclaimers. As public models become more capable and authoritative,
disclaimers must be implemented as a safeguard adapting to the clinical context
of each output.

</details>


### [13] [Beyond Scale: Small Language Models are Comparable to GPT-4 in Mental Health Understanding](https://arxiv.org/abs/2507.08031)
*Hong Jia,Shiya Fu,Vassilis Kostakos,Feng Xia,Ting Dang*

Main category: cs.CL

TL;DR: 小型语言模型（SLMs）在心理健康理解任务中展现出与大型语言模型（LLMs）接近的性能（F1分数仅差2%），尤其在二分类任务和少样本学习场景下表现突出，证明其可作为隐私保护型心理健康筛查工具。


<details>
  <summary>Details</summary>
Motivation: 探索小型语言模型作为隐私保护方案在敏感心理健康领域应用的可行性，验证其理解能力是否足以替代大型语言模型。

Method: 采用零样本和少样本学习范式，在6个心理健康理解任务上系统评估5个先进SLMs（Phi-3等）与3个LLMs（GPT-4等）的表现，重点关注分类准确率和模型规模的关系。

Result: SLMs在二分类任务中平均F1达0.64（对比LLMs的0.66），少样本学习使其性能提升14.6%。两类模型在多分类严重度任务中均出现30%+的性能下降，显示临床细粒度理解的共性挑战。

Conclusion: SLMs凭借接近LLMs的性能、快速少样本适应能力和隐私保护特性，可成为可扩展的心理健康筛查工具，特别是在处理敏感文本数据时提供有效且合规的解决方案。

Abstract: The emergence of Small Language Models (SLMs) as privacy-preserving
alternatives for sensitive applications raises a fundamental question about
their inherent understanding capabilities compared to Large Language Models
(LLMs). This paper investigates the mental health understanding capabilities of
current SLMs through systematic evaluation across diverse classification tasks.
Employing zero-shot and few-shot learning paradigms, we benchmark their
performance against established LLM baselines to elucidate their relative
strengths and limitations in this critical domain. We assess five
state-of-the-art SLMs (Phi-3, Phi-3.5, Qwen2.5, Llama-3.2, Gemma2) against
three LLMs (GPT-4, FLAN-T5-XXL, Alpaca-7B) on six mental health understanding
tasks. Our findings reveal that SLMs achieve mean performance within 2\% of
LLMs on binary classification tasks (F1 scores of 0.64 vs 0.66 in zero-shot
settings), demonstrating notable competence despite orders of magnitude fewer
parameters. Both model categories experience similar degradation on multi-class
severity tasks (a drop of over 30\%), suggesting that nuanced clinical
understanding challenges transcend model scale. Few-shot prompting provides
substantial improvements for SLMs (up to 14.6\%), while LLM gains are more
variable. Our work highlights the potential of SLMs in mental health
understanding, showing they can be effective privacy-preserving tools for
analyzing sensitive online text data. In particular, their ability to quickly
adapt and specialize with minimal data through few-shot learning positions them
as promising candidates for scalable mental health screening tools.

</details>


### [14] [Integrating External Tools with Large Language Models to Improve Accuracy](https://arxiv.org/abs/2507.08034)
*Nripesh Niketan,Hadj Batatia*

Main category: cs.CL

TL;DR: 提出Athena框架集成外部工具提升LLMs在教育场景下的查询性能，数学推理准确率达83%超越主流模型


<details>
  <summary>Details</summary>
Motivation: 现有LLMs缺乏实时数据导致回答质量低，需结合外部工具增强准确性和计算能力

Method: 开发支持访问API获取实时数据，集成计算工具（计算器/日历）的模块化框架

Result: 在MMLU数据集测试中，数学/科学推理准确率分别达83%和88%，显著优于GPT-4o等所有基线模型

Conclusion: 该框架为构建LLM计算生态系统奠定基础，推动自然化任务支持场景的应用拓展

Abstract: This paper deals with improving querying large language models (LLMs). It is
well-known that without relevant contextual information, LLMs can provide poor
quality responses or tend to hallucinate. Several initiatives have proposed
integrating LLMs with external tools to provide them with up-to-date data to
improve accuracy. In this paper, we propose a framework to integrate external
tools to enhance the capabilities of LLMs in answering queries in educational
settings. Precisely, we develop a framework that allows accessing external APIs
to request additional relevant information. Integrated tools can also provide
computational capabilities such as calculators or calendars. The proposed
framework has been evaluated using datasets from the Multi-Modal Language
Understanding (MMLU) collection. The data consists of questions on mathematical
and scientific reasoning. Results compared to state-of-the-art language models
show that the proposed approach significantly improves performance. Our Athena
framework achieves 83% accuracy in mathematical reasoning and 88% in scientific
reasoning, substantially outperforming all tested models including GPT-4o,
LLaMA-Large, Mistral-Large, Phi-Large, and GPT-3.5, with the best baseline
model (LLaMA-Large) achieving only 67% and 79% respectively. These promising
results open the way to creating complex computing ecosystems around LLMs to
make their use more natural to support various tasks and activities.

</details>


### [15] [Barriers in Integrating Medical Visual Question Answering into Radiology Workflows: A Scoping Review and Clinicians' Insights](https://arxiv.org/abs/2507.08036)
*Deepali Mishra,Chaklam Silpasuwanchai,Ashutosh Modi,Madhumita Sushil,Sorayouth Chumnanvej*

Main category: cs.CL

TL;DR: 系统综述揭示MedVQA临床整合存在数据集相关性不足、评估指标不匹配等核心障碍，需改进多模态分析与临床背景整合


<details>
  <summary>Details</summary>
Motivation: 尽管MedVQA技术持续进步，但其临床实际应用仍受限。研究旨在通过系统综述与临床医生调研，揭示阻碍医学视觉问答系统临床落地的关键因素。

Method: 采用Arksey-O'Malley范围综述框架：1) 分析68篇文献(2018-2024)的放射工作流程特征与研究缺口；2) 调查印度、泰国50名临床医生对MedVQA的临床价值认知。

Result: 1) 60%问答对缺乏诊断价值；2) 现存系统缺失多视角成像/EHR整合等关键功能；3) 评估指标与临床需求错位；4) 仅29.8%医生认可实用性，87.2%强调需整合患者病史，78.7%要求多视角支持。

Conclusion: MedVQA需突破三大瓶颈：多模态分析能力不足、患者上下文缺失、评估体系与临床需求脱节。建议发展解剖区域专业化模型(66%医生支持)和对话交互系统(89.4%倾向)以提升临床适用性。

Abstract: Medical Visual Question Answering (MedVQA) is a promising tool to assist
radiologists by automating medical image interpretation through question
answering. Despite advances in models and datasets, MedVQA's integration into
clinical workflows remains limited. This study systematically reviews 68
publications (2018-2024) and surveys 50 clinicians from India and Thailand to
examine MedVQA's practical utility, challenges, and gaps. Following the Arksey
and O'Malley scoping review framework, we used a two-pronged approach: (1)
reviewing studies to identify key concepts, advancements, and research gaps in
radiology workflows, and (2) surveying clinicians to capture their perspectives
on MedVQA's clinical relevance. Our review reveals that nearly 60% of QA pairs
are non-diagnostic and lack clinical relevance. Most datasets and models do not
support multi-view, multi-resolution imaging, EHR integration, or domain
knowledge, features essential for clinical diagnosis. Furthermore, there is a
clear mismatch between current evaluation metrics and clinical needs. The
clinician survey confirms this disconnect: only 29.8% consider MedVQA systems
highly useful. Key concerns include the absence of patient history or domain
knowledge (87.2%), preference for manually curated datasets (51.1%), and the
need for multi-view image support (78.7%). Additionally, 66% favor models
focused on specific anatomical regions, and 89.4% prefer dialogue-based
interactive systems. While MedVQA shows strong potential, challenges such as
limited multimodal analysis, lack of patient context, and misaligned evaluation
approaches must be addressed for effective clinical integration.

</details>


### [16] [CRISP: Complex Reasoning with Interpretable Step-based Plans](https://arxiv.org/abs/2507.08037)
*Matan Vetzler,Koren Lazar,Guy Uziel,Eran Hirsch,Ateret Anaby-Tavor,Leshem Choshen*

Main category: cs.CL

TL;DR: 提出CRISP数据集通过自动生成高可信度规划方案，显著提升小模型在数学推理和代码生成任务中的表现，并验证跨领域泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有Chain-of-Thought推理方法在复杂领域效果有限，且主流方法假设大模型仅通过少量提示即可生成有效规划方案而无需训练

Method: 构建多领域规划数据集CRISP，通过LLM自动生成方案并进行双重验证：内在使用LLM评估，外在通过下游任务性能验证

Result: 微调后的小模型规划质量超越大模型少样本提示方法，跨领域评估显示规划能力具有可迁移性

Conclusion: CRISP证明通过数据驱动的规划训练可有效提升模型推理能力，为LLMs复杂推理任务提供了新范式

Abstract: Recent advancements in large language models (LLMs) underscore the need for
stronger reasoning capabilities to solve complex problems effectively. While
Chain-of-Thought (CoT) reasoning has been a step forward, it remains
insufficient for many domains. A promising alternative is explicit high-level
plan generation, but existing approaches largely assume that LLMs can produce
effective plans through few-shot prompting alone, without additional training.
In this work, we challenge this assumption and introduce CRISP (Complex
Reasoning with Interpretable Step-based Plans), a multi-domain dataset of
high-level plans for mathematical reasoning and code generation. The plans in
CRISP are automatically generated and rigorously validated--both intrinsically,
using an LLM as a judge, and extrinsically, by evaluating their impact on
downstream task performance. We demonstrate that fine-tuning a small model on
CRISP enables it to generate higher-quality plans than much larger models using
few-shot prompting, while significantly outperforming Chain-of-Thought
reasoning. Furthermore, our out-of-domain evaluation reveals that fine-tuning
on one domain improves plan generation in the other, highlighting the
generalizability of learned planning capabilities.

</details>


### [17] [AblationBench: Evaluating Automated Planning of Ablations in Empirical AI Research](https://arxiv.org/abs/2507.08038)
*Talor Abramovich,Gal Chechik*

Main category: cs.CL

TL;DR: 提出AblationBench基准套件评估AI代理的消融实验规划能力，包含作者/评审双任务（83+350实例），实验显示前沿语言模型仅能识别29%原始消融，思维链提示优于现有代理方法。


<details>
  <summary>Details</summary>
Motivation: 随着基于语言模型的自主代理在科研领域广泛应用，亟需评估其在实证研究关键环节（消融实验设计）的能力，现有系统在此类任务上缺乏有效评估框架。

Method: 构建包含作者端（根据方法章节建议消融实验）和评审端（检测论文缺失消融）的双任务基准，开发基于语言模型的自动评估框架，采用前沿语言模型进行系统实验。

Result: 最佳语言模型系统平均识别率仅29%，思维链提示（35.7%）显著优于当前代理方法（28.6%），揭示现有系统在复杂科研推理任务的局限性。

Conclusion: AblationBench有效暴露语言模型在科研消融规划中的不足，证明结构化推理方法的价值，为AI科研代理的评估与改进提供基准支持。

Abstract: Autonomous agents built on language models (LMs) are showing increasing
popularity in many fields, including scientific research. AI co-scientists aim
to support or automate parts of the research process using these agents. A key
component of empirical AI research is the design of ablation experiments. To
this end, we introduce AblationBench, a benchmark suite for evaluating agents
on ablation planning tasks in empirical AI research. It includes two tasks:
AuthorAblation, which helps authors propose ablation experiments based on a
method section and contains 83 instances, and ReviewerAblation, which helps
reviewers find missing ablations in a full paper and contains 350 instances.
For both tasks, we develop LM-based judges that serve as an automatic
evaluation framework. Our experiments with frontier LMs show that these tasks
remain challenging, with the best-performing LM system identifying only 29% of
the original ablations on average. Lastly, we analyze the limitations of
current LMs on these tasks, and find that chain-of-thought prompting
outperforms the currently existing agent-based approach.

</details>


### [18] [Krul: Efficient State Restoration for Multi-turn Conversations with Dynamic Cross-layer KV Sharing](https://arxiv.org/abs/2507.08045)
*Junyi Wen,Junyuan Liang,Zicong Hong,Wuhui Chen,Zibin Zheng*

Main category: cs.CL

TL;DR: Krul系统通过动态KV缓存压缩策略和三层创新架构，有效提升多轮对话LLM推理效率，在保持生成质量的同时显著降低首令牌响应时间和存储开销


<details>
  <summary>Details</summary>
Motivation: 现有固定压缩方案忽视不同对话的注意力模式差异，导致准确率下降。需要动态策略平衡压缩效率与上下文保留需求

Method: 1) 预压缩策略选择器动态选择层间压缩方案；2) 异质注意力相似性估计器降低计算开销；3) 无气泡调度器优化恢复流程

Result: 实验显示TTFT降低1.5-2.68倍，KV存储减少1.33-2.35倍，生成质量与基线相当

Conclusion: Krul通过细粒度动态压缩策略和系统级优化，在效率与准确性间取得更好平衡，为LLM长对话优化提供新方向

Abstract: Efficient state restoration in multi-turn conversations with large language
models (LLMs) remains a critical challenge, primarily due to the overhead of
recomputing or loading full key-value (KV) caches for all historical tokens. To
address this, existing approaches compress KV caches across adjacent layers
with highly similar attention patterns. However, these methods often apply a
fixed compression scheme across all conversations, selecting the same layer
pairs for compression without considering conversation-specific attention
dynamics. This static strategy overlooks variability in attention pattern
similarity across different conversations, which can lead to noticeable
accuracy degradation.
  We present Krul, a multi-turn LLM inference system that enables accurate and
efficient KV cache restoration. Krul dynamically selects compression strategies
based on attention similarity across layer pairs and uses a
recomputation-loading pipeline to restore the KV cache. It introduces three key
innovations: 1) a preemptive compression strategy selector to preserve critical
context for future conversation turns and selects a customized strategy for the
conversation; 2) a token-wise heterogeneous attention similarity estimator to
mitigate the attention similarity computation and storage overhead during model
generation; 3) a bubble-free restoration scheduler to reduce potential bubbles
brought by the imbalance of recomputing and loading stream due to compressed KV
caches. Empirical evaluations on real-world tasks demonstrate that Krul
achieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x
reduction in KV cache storage compared to state-of-the-art methods without
compromising generation quality.

</details>


### [19] [GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs](https://arxiv.org/abs/2507.08107)
*Sebastian Walter,Hannah Bast*

Main category: cs.CL

TL;DR: 提出基于大语言模型的零样本SPARQL生成方法，通过知识图谱探索实现自然语言到查询语句的转换。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法需要微调模型、难以适应不同规模/类型知识图谱的问题，提升跨场景泛化能力。

Method: 利用大模型战略性地执行SPARQL查询，自动搜索知识图谱中的相关IRI和字面量，无需微调模型。

Result: 在Wikidata实现SOTA性能（零样本），在Freebase接近小样本最佳方法，其他知识图谱表现整体优异。

Conclusion: 该方法通过动态图谱探索机制，在多种基准测试中验证了零样本场景下的有效性和泛化能力。

Abstract: We propose a new approach for generating SPARQL queries on RDF knowledge
graphs from natural language questions or keyword queries, using a large
language model. Our approach does not require fine-tuning. Instead, it uses the
language model to explore the knowledge graph by strategically executing SPARQL
queries and searching for relevant IRIs and literals. We evaluate our approach
on a variety of benchmarks (for knowledge graphs of different kinds and sizes)
and language models (of different scales and types, commercial as well as
open-source) and compare it with existing approaches. On Wikidata we reach
state-of-the-art results on multiple benchmarks, despite the zero-shot setting.
On Freebase we come close to the best few-shot methods. On other, less commonly
evaluated knowledge graphs and benchmarks our approach also performs well
overall. We conduct several additional studies, like comparing different ways
of searching the graphs, incorporating a feedback mechanism, or making use of
few-shot examples.

</details>


### [20] [Audit, Alignment, and Optimization of LM-Powered Subroutines with Application to Public Comment Processing](https://arxiv.org/abs/2507.08109)
*Reilly Raab,Mike Parker,Dan Nally,Sadie Montgomery,Anastasia Bernat,Sai Munikoti,Sameera Horawalavithana*

Main category: cs.CL

TL;DR: 提出静态类型语言模型子程序框架，通过人类专家反馈优化模型表现，开发CommentNEPA应用处理环境评论并验证有效性


<details>
  <summary>Details</summary>
Motivation: 解决语言模型实际应用中存在的安全性/可解释性/偏见问题，促进透明可审计的模型部署方式

Method: 开发支持声明静态类型LM子程序的框架，记录所有模型产物（提示/输入/输出/数据依赖），构建CommentNEPA应用处理NEPA法案要求的公众环境评论

Result: 通过对比无人工反馈时的输出与历史人工标注数据，验证框架有效性

Conclusion: 该框架为负责任地部署语言模型提供透明化解决方案，在环境评估等决策场景具有应用潜力

Abstract: The advent of language models (LMs) has the potential to dramatically
accelerate tasks that may be cast to text-processing; however, real-world
adoption is hindered by concerns regarding safety, explainability, and bias.
How can we responsibly leverage LMs in a transparent, auditable manner --
minimizing risk and allowing human experts to focus on informed decision-making
rather than data-processing or prompt engineering? In this work, we propose a
framework for declaring statically typed, LM-powered subroutines (i.e.,
callable, function-like procedures) for use within conventional asynchronous
code -- such that sparse feedback from human experts is used to improve the
performance of each subroutine online (i.e., during use). In our
implementation, all LM-produced artifacts (i.e., prompts, inputs, outputs, and
data-dependencies) are recorded and exposed to audit on demand. We package this
framework as a library to support its adoption and continued development. While
this framework may be applicable across several real-world decision workflows
(e.g., in healthcare and legal fields), we evaluate it in the context of public
comment processing as mandated by the 1969 National Environmental Protection
Act (NEPA): Specifically, we use this framework to develop "CommentNEPA," an
application that compiles, organizes, and summarizes a corpus of public
commentary submitted in response to a project requiring environmental review.
We quantitatively evaluate the application by comparing its outputs (when
operating without human feedback) to historical ``ground-truth'' data as
labelled by human annotators during the preparation of official environmental
impact statements.

</details>


### [21] [Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores](https://arxiv.org/abs/2507.08143)
*Vivek Chari,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 提出无参数、不依赖查询的KV缓存压缩方法Compactor，通过杠杆分数评估token重要性，在保留50% tokens的情况下保持性能，平均减少63%内存占用。


<details>
  <summary>Details</summary>
Motivation: LLMs的长上下文窗口导致KV缓存内存需求激增，成为实际部署中吞吐量限制和成本增加的主要瓶颈。

Method: 基于近似杠杆分数确定token重要性，支持上下文校准压缩策略自动推断最大压缩率。

Result: 在RULER/Longbench的27个任务中，Qwen和Llama模型上实现全KV性能，内存减少63%（Longbench）且仅需保留半数tokens。

Conclusion: Compactor在保持模型性能的同时显著降低内存需求，为长上下文LLM部署提供了高效、通用的压缩解决方案。

Abstract: Modern Large Language Models (LLMs) are increasingly trained to support very
large context windows. Unfortunately the ability to use long contexts in
generation is complicated by the large memory requirement of the KV cache,
which scales linearly with the context length. This memory footprint is often
the dominant resource bottleneck in real-world deployments, limiting throughput
and increasing serving cost. One way to address this is by compressing the KV
cache, which can be done either with knowledge of the question being asked
(query-aware) or without knowledge of the query (query-agnostic). We present
Compactor, a parameter-free, query-agnostic KV compression strategy that uses
approximate leverage scores to determine token importance. We show that
Compactor can achieve the same performance as competing methods while retaining
1/2 the tokens in both synthetic and real-world context tasks, with minimal
computational overhead. We further introduce a procedure for context-calibrated
compression, which allows one to infer the maximum compression ratio a given
context can support. Using context-calibrated compression, we show that
Compactor achieves full KV performance on Longbench while reducing the KV
memory burden by 63%, on average. To demonstrate the efficacy and
generalizability of our approach, we apply Compactor to 27 synthetic and
real-world tasks from RULER and Longbench, with models from both the Qwen 2.5
and Llama 3.1 families.

</details>


### [22] [Distilling Empathy from Large Language Models](https://arxiv.org/abs/2507.08151)
*Henry J. Xie,Jinghan Zhang,Xinhao Zhang,Kunpeng Liu*

Main category: cs.CL

TL;DR: 提出两阶段微调框架，通过针对性共情增强提示实现大模型到小模型的共情能力蒸馏


<details>
  <summary>Details</summary>
Motivation: 小模型在资源受限场景（如移动端）需保持与大模型相当的共情能力以促进人机良性互动

Method: 基于大模型蒸馏的共情对话数据集，采用两阶段微调流程（基础对话能力→共情强化），设计四组针对性提示提升蒸馏效果

Result: 微调后小模型共情响应胜率达90%，针对性提示较基础提示提升10%胜率

Conclusion: 该框架有效实现共情能力迁移，为资源受限场景下保持语言模型情感智能提供解决方案

Abstract: The distillation of knowledge from Large Language Models (LLMs) into Smaller
Language Models (SLMs), preserving the capabilities and performance of LLMs
while reducing model size, has played a key role in the proliferation of LLMs.
Because SLMs are considerably smaller than LLMs, they are often utilized in
domains where human interaction is frequent but resources are highly
constrained, e.g., smart phones. Therefore, it is crucial to ensure that
empathy, a fundamental aspect of positive human interactions, already instilled
into LLMs, is retained by SLMs after distillation. In this paper, we develop a
comprehensive approach for effective empathy distillation from LLMs into SLMs.
Our approach features a two-step fine-tuning process that fully leverages
datasets of empathetic dialogue responses distilled from LLMs. We explore
several distillation methods beyond basic direct prompting and propose four
unique sets of prompts for targeted empathy improvement to significantly
enhance the empathy distillation process. Our evaluations demonstrate that SLMs
fine-tuned through the two-step fine-tuning process with distillation datasets
enhanced by the targeted empathy improvement prompts significantly outperform
the base SLM at generating empathetic responses with a win rate of 90%. Our
targeted empathy improvement prompts substantially outperform the basic direct
prompting with a 10% improvement in win rate.

</details>


### [23] [TruthTorchLM: A Comprehensive Library for Predicting Truthfulness in LLM Outputs](https://arxiv.org/abs/2507.08203)
*Duygu Nur Yaldiz,Yavuz Faruk Bakman,Sungmin Kang,Alperen Öziş,Hayrettin Eren Yildiz,Mitash Ashish Shah,Zhiqi Huang,Anoop Kumar,Alfy Samuel,Daben Liu,Sai Praneeth Karimireddy,Salman Avestimehr*

Main category: cs.CL

TL;DR: 开源Python库TruthTorchLM提供30+种LLM真实性预测方法，兼容主流框架并支持多场景验证，填补现有工具局限性。


<details>
  <summary>Details</summary>
Motivation: 现有真实性检测工具（如Guardrails仅支持文档验证、LM-Polygraph局限于不确定性方法）功能单一，难以满足高风险场景下对生成内容真实性预测的多样化需求。

Method: 集成覆盖不同计算成本/访问权限/监督类型的30+种方法，兼容HuggingFace和LiteLLM框架，提供生成-评估-校准全流程接口及扩展框架。

Result: 在TriviaQA、GSM8K和FactScore-Bio三个基准数据集上验证了代表性方法的有效性。

Conclusion: 该工具通过模块化设计突破现有工具局限性，为LLM真实性研究提供标准化、可扩展的技术平台，特别适用于需要高可靠性验证的实际应用场景。

Abstract: Generative Large Language Models (LLMs)inevitably produce untruthful
responses. Accurately predicting the truthfulness of these outputs is critical,
especially in high-stakes settings. To accelerate research in this domain and
make truthfulness prediction methods more accessible, we introduce TruthTorchLM
an open-source, comprehensive Python library featuring over 30 truthfulness
prediction methods, which we refer to as Truth Methods. Unlike existing
toolkits such as Guardrails, which focus solely on document-grounded
verification, or LM-Polygraph, which is limited to uncertainty-based methods,
TruthTorchLM offers a broad and extensible collection of techniques. These
methods span diverse tradeoffs in computational cost, access level (e.g.,
black-box vs white-box), grounding document requirements, and supervision type
(self-supervised or supervised). TruthTorchLM is seamlessly compatible with
both HuggingFace and LiteLLM, enabling support for locally hosted and API-based
models. It also provides a unified interface for generation, evaluation,
calibration, and long-form truthfulness prediction, along with a flexible
framework for extending the library with new methods. We conduct an evaluation
of representative truth methods on three datasets, TriviaQA, GSM8K, and
FactScore-Bio. The code is available at https://github.com/Ybakman/TruthTorchLM

</details>


### [24] [Simple Mechanistic Explanations for Out-Of-Context Reasoning](https://arxiv.org/abs/2507.08218)
*Atticus Wang,Joshua Engels,Oliver Clive-Griffin*

Main category: cs.CL

TL;DR: 大模型微调后通过添加恒定导向向量实现跨领域泛化能力（OOCR）的机理解释


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型为何能突破上下文限制进行推理，这对模型安全部署至关重要

Method: 通过机械分析LoRA微调机制，发现其本质是添加恒定导向向量，并尝试从头训练此类向量验证效果

Result: 1. 导向向量可提升目标任务及关联领域表现
2. 即使在后门任务中，无条件添加导向向量也足够生效

Conclusion: 微调过程中学习的导向向量是OOCR能力的关键，揭示了LLMs底层工作机制，为可靠部署提供理论支持

Abstract: Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs
exhibit surprisingly deep out-of-distribution generalization. Rather than
learning shallow heuristics, they implicitly internalize and act on the
consequences of observations scattered throughout the fine-tuning data. In this
work, we investigate this phenomenon mechanistically and find that many
instances of OOCR in the literature have a simple explanation: the LoRA
fine-tuning essentially adds a constant steering vector, steering the model
towards a general concept. This improves performance on the fine-tuning task
and in many other concept-related domains, causing the surprising
generalization. Moreover, we can directly train steering vectors for these
tasks from scratch, which also induces OOCR. We find that our results hold even
for a task that seems like it must involve conditional behavior (model
backdoors); it turns out that unconditionally adding a steering vector is
sufficient. Overall, our work presents one explanation of what gets learned
during fine-tuning for OOCR tasks, contributing to the key question of why LLMs
can reason out of context, an advanced capability that is highly relevant to
their safe and reliable deployment.

</details>


### [25] [Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and Reading Comprehension?](https://arxiv.org/abs/2507.08232)
*KV Aditya Srivatsa,Kaushal Kumar Maurya,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: 研究通过IRT模型分析发现，未经指导的通用大模型在各年级表现优于学生群体，模型与学生的对齐效果受模型能力及提示策略影响显著，需针对性选择代理模型


<details>
  <summary>Details</summary>
Motivation: 验证LLMs作为代理学生在ITS开发和试题测试中能否准确模拟真实学生特征，解决模型与人类学生能力错位可能导致的评估偏差问题

Method: 收集NAEP数学/阅读489个题目，使用IRT模型将11种LLM与K12真实学生数据统一建模，对比不同模型在年级强制提示下的表现

Result: 强模型(如GPT-4)普遍超越各年级平均学生(数学+0.7σ，阅读+1.2σ)，弱模型仅偶然对齐；年级提示仅部分有效，最佳组合准确率仅达65%

Conclusion: 建议根据具体场景选择代理模型，需开发新训练范式提升模型与目标学生群体的对齐能力，现有模型无法稳定模拟年级平均水平学生

Abstract: Large Language Models (LLMs) are increasingly used as proxy students in the
development of Intelligent Tutoring Systems (ITSs) and in piloting test
questions. However, to what extent these proxy students accurately emulate the
behavior and characteristics of real students remains an open question. To
investigate this, we collected a dataset of 489 items from the National
Assessment of Educational Progress (NAEP), covering mathematics and reading
comprehension in grades 4, 8, and 12. We then apply an Item Response Theory
(IRT) model to position 11 diverse and state-of-the-art LLMs on the same
ability scale as real student populations. Our findings reveal that, without
guidance, strong general-purpose models consistently outperform the average
student at every grade, while weaker or domain-mismatched models may align
incidentally. Using grade-enforcement prompts changes models' performance, but
whether they align with the average grade-level student remains highly model-
and prompt-specific: no evaluated model-prompt pair fits the bill across
subjects and grades, underscoring the need for new training and evaluation
strategies. We conclude by providing guidelines for the selection of viable
proxies based on our findings.

</details>


### [26] [Exploring Gender Differences in Chronic Pain Discussions on Reddit](https://arxiv.org/abs/2507.08241)
*Ancita Maria Andrade,Tanvi Banerjee,Ramakrishna Mundugar*

Main category: cs.CL

TL;DR: 论文通过NLP技术分析疼痛体验的性别差异，发现女性更关注情感表达且偏头痛等病症发生率更高


<details>
  <summary>Details</summary>
Motivation: 既往疼痛研究常忽视性别因素，本研究旨在通过HAM-CNN模型揭示疼痛体验的性别差异特征

Method: 使用HAM-CNN模型对用户发帖进行性别分类（F1值0.86），并分析语料库中的语言特征及疾病分布

Result: 女性发帖情感词汇更密集，偏头痛/鼻窦炎发生率显著高于男性，止痛药效果存在性别差异

Conclusion: 性别因素深刻影响疼痛体验，NLP技术为疼痛研究提供了新的量化分析视角

Abstract: Pain is an inherent part of human existence, manifesting as both physical and
emotional experiences, and can be categorized as either acute or chronic. Over
the years, extensive research has been conducted to understand the causes of
pain and explore potential treatments, with contributions from various
scientific disciplines. However, earlier studies often overlooked the role of
gender in pain experiences. In this study, we utilized Natural Language
Processing (NLP) to analyze and gain deeper insights into individuals' pain
experiences, with a particular focus on gender differences. We successfully
classified posts into male and female corpora using the Hidden Attribute
Model-Convolutional Neural Network (HAM-CNN), achieving an F1 score of 0.86 by
aggregating posts based on usernames. Our analysis revealed linguistic
differences between genders, with female posts tending to be more emotionally
focused. Additionally, the study highlighted that conditions such as migraine
and sinusitis are more prevalent among females and explored how pain medication
affects individuals differently based on gender.

</details>


### [27] [KAT-V1: Kwai-AutoThink Technical Report](https://arxiv.org/abs/2507.08297)
*Zizheng Zhan,Ken Deng,Huaixi Tang,Wen Xiang,Kun Wu,Weihao Li,Wenqiang Zhu,Jingxuan Xu,Lecheng Huang,Zongxian Feng,Shaojie Wang,Shangpeng Yan,Jiaheng Liu,Zhongyuan Peng,Zuchen Gao,Haoyang Huang,Ziqi Zhan,Yanan Wu,Yuanxing Zhang,Jian Yang,Guang Chen,Haotian Zhang,Bin Chen,Bing Yu*

Main category: cs.CL

TL;DR: Kwaipilot-AutoThink (KAT) 是40B开源大语言模型，通过动态切换推理/非推理模式解决过度思考问题，结合多阶段训练策略，在保持SOTA性能的同时减少30%计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 针对推理任务中模型过度计算导致的效率低下问题，研究动态模式切换机制以提升资源利用效率，同时保持推理准确性。

Method: 1. 构建双模态数据集（多代理合成+自动标注）
2. 多令牌预测增强的知识蒸馏技术
3. 基于多数投票的冷启动初始化策略
4. Step-SRPO强化学习框架（融合中间监督的GRPO改进）

Result: 在17个基准测试中超越DeepSeek-R1和Qwen3等模型，实际部署使快手内部编码助手Kwaipilot推理效率提升30%，200B MoE版本初步实验显示性能/效率同步提升。

Conclusion: 该工作首次实现LLM推理模式的动态自适应切换，通过知识蒸馏+强化学习的组合创新，在保持精度的同时显著提升计算效率，为工业级大模型部署提供新范式。

Abstract: We present Kwaipilot-AutoThink (KAT), an open-source 40B large language model
developed to address the overthinking problem in reasoning-intensive tasks,
where an automatic thinking training paradigm is proposed to dynamically switch
between reasoning and non-reasoning modes based on task complexity.
Specifically, first, we construct the dual-regime dataset based on a novel
tagging pipeline and a multi-agent synthesis strategy, and then we apply
Multi-Token Prediction (MTP)-enhanced knowledge distillation, enabling
efficient and fine-grained reasoning transfer with minimal pretraining cost.
Besides, we implement a cold-start initialization strategy that introduces
mode-selection priors using majority-vote signals and intent-aware prompting.
Finally, we propose Step-SRPO, a reinforcement learning algorithm that
incorporates intermediate supervision into the GRPO framework, offering
structured guidance over both reasoning-mode selection and response accuracy.
Extensive experiments across multiple benchmarks demonstrate that KAT
consistently matches or even outperforms current state-of-the-art models,
including DeepSeek-R1-0528 and Qwen3-235B-A22B, across a wide range of
reasoning-intensive tasks while reducing token usage by up to approximately
30\%. Beyond academic evaluation, KAT has been successfully deployed in
Kwaipilot (i.e., Kuaishou's internal coding assistant), and improves real-world
development workflows with high accuracy, efficiency, and controllable
reasoning behaviors. Moreover, we are actively training a 200B
Mixture-of-Experts (MoE) with 40B activation parameters, where the early-stage
results already demonstrate promising improvements in performance and
efficiency, further showing the scalability of the AutoThink paradigm.

</details>


### [28] [Improving MLLM's Document Image Machine Translation via Synchronously Self-reviewing Its OCR Proficiency](https://arxiv.org/abs/2507.08309)
*Yupu Liang,Yaping Zhang,Zhiyang Zhang,Zhiyuan Chen,Yang Zhao,Lu Xiang,Chengqing Zong,Yu Zhou*

Main category: cs.CL

TL;DR: 提出SSR微调范式缓解MLLMs在文档图像机器翻译任务中的灾难性遗忘问题，通过同步OCR文本生成保持模型单语能力


<details>
  <summary>Details</summary>
Motivation: 现有监督微调方法在提升DIMT能力时会导致模型遗忘OCR等单语能力，需解决跨模态跨语言双重挑战

Method: 受双语认知优势启发，SSR机制强制模型在翻译前生成OCR文本，利用单语OCR能力辅助跨语言翻译学习

Result: 实验证明SSR显著缓解灾难性遗忘，MLLMs在OCR和DIMT任务上均表现出更好泛化性能

Conclusion: SSR为多模态模型的多任务优化提供新思路，验证认知科学理论在AI模型训练中的有效性

Abstract: Multimodal Large Language Models (MLLMs) have shown strong performance in
document image tasks, especially Optical Character Recognition (OCR). However,
they struggle with Document Image Machine Translation (DIMT), which requires
handling both cross-modal and cross-lingual challenges. Previous efforts to
enhance DIMT capability through Supervised Fine-Tuning (SFT) on the DIMT
dataset often result in the forgetting of the model's existing monolingual
abilities, such as OCR. To address these challenges, we introduce a novel
fine-tuning paradigm, named Synchronously Self-Reviewing (SSR) its OCR
proficiency, inspired by the concept "Bilingual Cognitive Advantage".
Specifically, SSR prompts the model to generate OCR text before producing
translation text, which allows the model to leverage its strong monolingual OCR
ability while learning to translate text across languages. Comprehensive
experiments demonstrate the proposed SSR learning helps mitigate catastrophic
forgetting, improving the generalization ability of MLLMs on both OCR and DIMT
tasks.

</details>


### [29] [CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template Generation](https://arxiv.org/abs/2507.08325)
*Yinzhu Quan,Xinrui Li,Ying Chen*

Main category: cs.CL

TL;DR: 提出CRMAgent多智能体系统，通过群组学习、检索适配和规则备用三种模式生成高质量营销模板，显著提升电商私有域渠道的营销效果。


<details>
  <summary>Details</summary>
Motivation: 多数电商商家因缺乏专业知识和工具难以撰写有效营销信息，导致CRM策略效果参差不齐。

Method: 1) 群组学习：从商家同受众高绩效消息中学习并改写低效内容；2) 检索适配：获取相似受众/优惠类型的成功模板进行模式迁移；3) 规则备用：无合适参考时提供零样本轻量级改写。

Result: 实验证明CRMAgent在受众匹配度(提升23.6%)和营销转化率(增长15.8%)上全面超越原始模板，且效果具有稳定性。

Conclusion: CRMAgent通过知识迁移与混合生成模式，为商家提供了可扩展的智能化内容生成解决方案，有效优化CRM沟通策略。

Abstract: In e-commerce private-domain channels such as instant messaging and e-mail,
merchants engage customers directly as part of their Customer Relationship
Management (CRM) programmes to drive retention and conversion. While a few top
performers excel at crafting outbound messages, most merchants struggle to
write persuasive copy because they lack both expertise and scalable tools. We
introduce CRMAgent, a multi-agent system built on large language models (LLMs)
that generates high-quality message templates and actionable writing guidance
through three complementary modes. First, group-based learning enables the
agent to learn from a merchant's own top-performing messages within the same
audience segment and rewrite low-performing ones. Second,
retrieval-and-adaptation fetches templates that share the same audience segment
and exhibit high similarity in voucher type and product category, learns their
successful patterns, and adapts them to the current campaign. Third, a
rule-based fallback provides a lightweight zero-shot rewrite when no suitable
references are available. Extensive experiments show that CRMAgent consistently
outperforms merchants' original templates, delivering significant gains in both
audience-match and marketing-effectiveness metrics.

</details>


### [30] [MK2 at PBIG Competition: A Prompt Generation Solution](https://arxiv.org/abs/2507.08335)
*Yuzheng Xu,Tosho Hirasawa,Seiya Kawano,Shota Kato,Tadashi Kozuno*

Main category: cs.CL

TL;DR: MK2流程通过提示工程实现专利到产品创意的高效转化


<details>
  <summary>Details</summary>
Motivation: 将现有专利快速转化为三年内可商业化的产品创意需求

Method: 分阶段流程：Gemini 2.5迭代优化提示词 → GPT-4.1生成创意 → Qwen3-8B的Elo机制筛选最优方案

Result: 在三个领域36次测试中25次获胜，但材料化学领域表现较弱需加强领域知识

Conclusion: 轻量级提示工程已能实现专利的商业化创意生成，但特定领域需深化专业知识

Abstract: The Patent-Based Idea Generation task asks systems to turn real patents into
product ideas viable within three years. We propose MK2, a prompt-centric
pipeline: Gemini 2.5 drafts and iteratively edits a prompt, grafting useful
fragments from weaker outputs; GPT-4.1 then uses this prompt to create one idea
per patent, and an Elo loop judged by Qwen3-8B selects the best prompt-all
without extra training data. Across three domains, two evaluator types, and six
criteria, MK2 topped the automatic leaderboard and won 25 of 36 tests. Only the
materials-chemistry track lagged, indicating the need for deeper domain
grounding; yet, the results show that lightweight prompt engineering has
already delivered competitive, commercially relevant ideation from patents.

</details>


### [31] [Distillation versus Contrastive Learning: How to Train Your Rerankers](https://arxiv.org/abs/2507.08336)
*Zhichao Xu,Zhiqi Huang,Shengyao Zhuang,Ashim Gupta,Vivek Srikumar*

Main category: cs.CL

TL;DR: 知识蒸馏优于对比学习（当存在大容量教师模型时），否则推荐对比学习


<details>
  <summary>Details</summary>
Motivation: 解决实际场景中缺乏对对比学习与知识蒸馏训练策略的有效性对比需求

Method: 在相同数据条件下训练不同规模/架构的模型，使用强对比学习模型作为蒸馏教师

Result: 知识蒸馏在跨领域任务表现更优（当教师模型更大时），同容量教师无此优势

Conclusion: 建议优先使用知识蒸馏训练小规模reranker（需大教师模型支持），否则选择对比学习

Abstract: Training text rerankers is crucial for information retrieval. Two primary
strategies are widely used: contrastive learning (optimizing directly on
ground-truth labels) and knowledge distillation (transferring knowledge from a
larger reranker). While both have been studied in the literature, a clear
comparison of their effectiveness for training cross-encoder rerankers under
practical conditions is needed.
  This paper empirically compares these strategies by training rerankers of
different sizes and architectures using both methods on the same data, with a
strong contrastive learning model acting as the distillation teacher. Our
results show that knowledge distillation generally yields better in-domain and
out-of-domain ranking performance than contrastive learning when distilling
from a larger teacher model. This finding is consistent across student model
sizes and architectures. However, distilling from a teacher of the same
capacity does not provide the same advantage, particularly for out-of-domain
tasks. These findings offer practical guidance for choosing a training strategy
based on available teacher models. Therefore, we recommend using knowledge
distillation to train smaller rerankers if a larger, more powerful teacher is
accessible; in its absence, contrastive learning provides a strong and more
reliable alternative otherwise.

</details>


### [32] [What Factors Affect LLMs and RLLMs in Financial Question Answering?](https://arxiv.org/abs/2507.08339)
*Peng Wang,Xuesi Hu,Jiageng Wu,Yuntao Zou,Qiancheng Zhang,Dagang Li*

Main category: cs.CL

TL;DR: 系统探讨提示方法、代理框架和多语言对齐对LLMs/RLLMs在金融问答中的影响，发现传统方法对具备长思维链的RLLMs提升有限


<details>
  <summary>Details</summary>
Motivation: 当前缺乏系统研究如何充分释放LLMs/RLLMs在金融领域的潜力，需评估不同方法对模型性能的影响机制

Method: 采用5个LLMs和3个RLLMs进行对比实验，评估提示方法、代理框架和多语言对齐方法在金融问答任务中的效果

Result: 1) 当前方法通过模拟长思维链提升LLMs性能 2) RLLMs固有长思维链限制传统方法效果 3) 多语言对齐主要延长LLMs推理长度

Conclusion: 本研究为金融领域LLMs/RLLMs的应用提供重要参考，揭示不同方法对基础模型性能提升的差异化机制

Abstract: Recently, the development of large language models (LLMs) and reasoning large
language models (RLLMs) have gained considerable attention from many
researchers. RLLMs enhance the reasoning capabilities of LLMs through Long
Chain-of-Thought (Long CoT) processes, significantly improving the performance
of LLMs in addressing complex problems. However, there are few works that
systematically explore what methods can fully unlock the performance of LLMs
and RLLMs within the financial domain. To investigate the impact of various
methods on LLMs and RLLMs, we utilize five LLMs and three RLLMs to assess the
effects of prompting methods, agentic frameworks, and multilingual alignment
methods on financial question-answering tasks. Our research findings indicate:
(1) Current prompting methods and agent frameworks enhance the performance of
LLMs in financial question answering by simulating Long CoT; (2) RLLMs possess
inherent Long CoT capabilities, which limits the effectiveness of conventional
methods in further enhancing their performance; (3) Current advanced
multilingual alignment methods primarily improve the multilingual performance
of LLMs by extending the reasoning length, which yields minimal benefits for
RLLMs. We hope that this study can serve as an important reference for LLMs and
RLLMs in the field of financial question answering.

</details>


### [33] [Beyond N-Grams: Rethinking Evaluation Metrics and Strategies for Multilingual Abstractive Summarization](https://arxiv.org/abs/2507.08342)
*Itai Mondshine,Tzuf Paz-Argaman,Reut Tsarfaty*

Main category: cs.CL

TL;DR: 研究揭示了自动评估指标对语言类型的敏感性：融合语言中n-gram指标与人类评估相关性较低，适当标记化可改善效果，基于神经网络的COMET指标在低资源语言表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有自动评估指标在非英语语言中的有效性未经系统验证，需通过多语言跨类型研究揭示指标适用边界，推动更公平的生成系统评估。

Method: 构建覆盖4个语言类型家族（黏着/孤立/低融合/高融合）的8语种评估基准，系统分析n-gram指标与神经指标（含COMET等）与人工评分的相关性，特别考察形态标记化对结果的影响。

Result: 1. 融合语言中ROUGE与人工评分相关性最低(-0.08 Spearman系数) 2. 优化标记化可使高融合语言指标相关性提升23% 3. COMET在低资源语言中相关系数达0.52，显著优于其他方法。

Conclusion: 应减少对n-gram指标的依赖，重点发展基于神经网络的评估指标，特别是在处理形态复杂的融合语言时需优先采用专用评估模型。

Abstract: Automatic n-gram based metrics such as ROUGE are widely used for evaluating
generative tasks such as summarization. While these metrics are considered
indicative (even if imperfect) of human evaluation for English, their
suitability for other languages remains unclear. To address this, we
systematically assess evaluation metrics for generation both n-gram-based and
neural based to evaluate their effectiveness across languages and tasks.
Specifically, we design a large-scale evaluation suite across eight languages
from four typological families: agglutinative, isolating, low-fusional, and
high-fusional, spanning both low- and high-resource settings, to analyze their
correlation with human judgments. Our findings highlight the sensitivity of
evaluation metrics to the language type. For example, in fusional languages,
n-gram-based metrics show lower correlation with human assessments compared to
isolating and agglutinative languages. We also demonstrate that proper
tokenization can significantly mitigate this issue for morphologically rich
fusional languages, sometimes even reversing negative trends. Additionally, we
show that neural-based metrics specifically trained for evaluation, such as
COMET, consistently outperform other neural metrics and better correlate with
human judgments in low-resource languages. Overall, our analysis highlights the
limitations of n-gram metrics for fusional languages and advocates for greater
investment in neural-based metrics trained for evaluation tasks.

</details>


### [34] [Exploring Design of Multi-Agent LLM Dialogues for Research Ideation](https://arxiv.org/abs/2507.08350)
*Keisuke Ueda,Wataru Hirota,Takuto Asakura,Takahiro Omi,Kosuke Takahashi,Kosuke Arima,Tatsuya Ishigaki*

Main category: cs.CL

TL;DR: 通过多代理LLM对话优化科学构思，探究代理配置对创新性的影响


<details>
  <summary>Details</summary>
Motivation: 探索多智能体大语言模型交互设计的最佳方案，解决现有研究中互动机制设计不明确的问题

Method: 比较不同代理角色/数量/对话深度的配置组合，设置生成-批判的迭代改进实验框架

Result: 扩大代理规模、加深交互深度、增加智能体异质性可提升构思多样性；增强批判侧多样性显著提高方案可行性

Conclusion: 为构建高效的多代理LLM科研构思系统提供配置优化路径

Abstract: Large language models (LLMs) are increasingly used to support creative tasks
such as research idea generation. While recent work has shown that structured
dialogues between LLMs can improve the novelty and feasibility of generated
ideas, the optimal design of such interactions remains unclear. In this study,
we conduct a comprehensive analysis of multi-agent LLM dialogues for scientific
ideation. We compare different configurations of agent roles, number of agents,
and dialogue depth to understand how these factors influence the novelty and
feasibility of generated ideas. Our experimental setup includes settings where
one agent generates ideas and another critiques them, enabling iterative
improvement. Our results show that enlarging the agent cohort, deepening the
interaction depth, and broadening agent persona heterogeneity each enrich the
diversity of generated ideas. Moreover, specifically increasing critic-side
diversity within the ideation-critique-revision loop further boosts the
feasibility of the final proposals. Our findings offer practical guidelines for
building effective multi-agent LLM systems for scientific ideation. Our code is
available at https://github.com/g6000/MultiAgent-Research-Ideator.

</details>


### [35] [The Curious Case of Factuality Finetuning: Models' Internal Beliefs Can Improve Factuality](https://arxiv.org/abs/2507.08371)
*Benjamin Newman,Abhilasha Ravichander,Jaehun Jung,Rui Xin,Hamish Ivison,Yegor Kuznetsov,Pang Wei Koh,Yejin Choi*

Main category: cs.CL

TL;DR: 微调使用模型自认为正确的生成数据比黄金数据更能有效减少语言模型幻觉


<details>
  <summary>Details</summary>
Motivation: 现有使用黄金数据微调的方法成本高且可能加剧下游任务幻觉，需探索更有效的数据选择策略

Method: 通过比较黄金数据和模型生成数据，结合不同过滤策略（模型自判断/黄金数据支持），评估长文本生成任务中的事实性表现

Result: 模型自判断过滤后的生成数据微调效果最佳，事实性提升跨三个领域有效

Conclusion: 模型自身对事实性的判断可作为优化训练数据的有效信号，为降低幻觉提供新思路

Abstract: Language models are prone to hallucination - generating text that is
factually incorrect. Finetuning models on high-quality factual information can
potentially reduce hallucination, but concerns remain; obtaining factual gold
data can be expensive and training on correct but unfamiliar data may
potentially lead to even more downstream hallucination. What data should
practitioners finetune on to mitigate hallucinations in language models? In
this work, we study the relationship between the factuality of finetuning data
and the prevalence of hallucinations in long-form generation tasks.
Counterintuitively, we find that finetuning on factual gold data is not as
helpful as finetuning on model-generated data that models believe to be
factual. Next, we evaluate filtering strategies applied on both factual gold
data and model-generated data, and find that finetuning on model-generated data
that is filtered by models' own internal judgments often leads to better
overall factuality compared to other configurations: training on gold data
filtered by models' judgments, training on gold data alone, or training on
model-generated data that is supported by gold data. These factuality
improvements transfer across three domains we study, suggesting that a models'
own beliefs can provide a powerful signal for factuality.

</details>


### [36] [A Survey of Large Language Models in Discipline-specific Research: Challenges, Methods and Opportunities](https://arxiv.org/abs/2507.08425)
*Lu Xiang,Yang Zhao,Yaping Zhang,Chengqing Zong*

Main category: cs.CL

TL;DR: 综述LLMs在跨学科研究中的技术方法（如监督微调、检索增强生成）及多学科应用案例，分析当前挑战与未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对LLMs跨学科整合的系统性理解，本文旨在填补这一空白并为研究者提供导航资源。

Method: 从技术视角（监督微调/检索增强/代理方法/工具集成）和应用视角（数理化/生物/人文社科）双重维度分类研究。

Result: 验证技术方法提升LLMs适应性的同时，系统性展示其在不同学科中的贡献，并指出模型可解释性/数据依赖等关键挑战。

Conclusion: 本文为跨学科LLMs研究提供技术-应用全景图，成为领域内研究者应对复杂挑战的路线图式指南。

Abstract: Large Language Models (LLMs) have demonstrated their transformative potential
across numerous disciplinary studies, reshaping the existing research
methodologies and fostering interdisciplinary collaboration. However, a
systematic understanding of their integration into diverse disciplines remains
underexplored. This survey paper provides a comprehensive overview of the
application of LLMs in interdisciplinary studies, categorising research efforts
from both a technical perspective and with regard to their applicability. From
a technical standpoint, key methodologies such as supervised fine-tuning,
retrieval-augmented generation, agent-based approaches, and tool-use
integration are examined, which enhance the adaptability and effectiveness of
LLMs in discipline-specific contexts. From the perspective of their
applicability, this paper explores how LLMs are contributing to various
disciplines including mathematics, physics, chemistry, biology, and the
humanities and social sciences, demonstrating their role in discipline-specific
tasks. The prevailing challenges are critically examined and the promising
research directions are highlighted alongside the recent advances in LLMs. By
providing a comprehensive overview of the technical developments and
applications in this field, this survey aims to serve as an invaluable resource
for the researchers who are navigating the complex landscape of LLMs in the
context of interdisciplinary studies.

</details>


### [37] [ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through Logical Rule-Guided Chains](https://arxiv.org/abs/2507.08427)
*Zilu Dong,Xiangqing Shen,Zinong Yang,Rui Xia*

Main category: cs.CL

TL;DR: 提出ChainEdit框架，通过结合知识图谱逻辑规则与LLM推理能力，实现逻辑一致的链式知识更新，逻辑泛化能力提升30%+


<details>
  <summary>Details</summary>
Motivation: 现有LLM知识编辑方法在关联事实的连锁更新中存在逻辑不一致问题

Method: 从知识库自动提取逻辑模式，对齐LLM内部逻辑，动态生成逻辑关联的知识集群，采用知识感知评估协议

Result: 逻辑泛化能力提升超30%，保持编辑可靠性，建立知识编辑后逻辑一致性的新SOTA

Conclusion: ChainEdit通过结构化知识与LLM推理的协同，有效解决知识编辑中的逻辑一致性问题，并改进评估标准

Abstract: Current knowledge editing methods for large language models (LLMs) struggle
to maintain logical consistency when propagating ripple effects to associated
facts. We propose ChainEdit, a framework that synergizes knowledge
graph-derived logical rules with LLM logical reasoning capabilities to enable
systematic chain updates. By automatically extracting logical patterns from
structured knowledge bases and aligning them with LLMs' internal logics,
ChainEdit dynamically generates and edits logically connected knowledge
clusters. Experiments demonstrate an improvement of more than 30% in logical
generalization over baselines while preserving editing reliability and
specificity. We further address evaluation biases in existing benchmarks
through knowledge-aware protocols that disentangle external dependencies. This
work establishes new state-of-the-art performance on ripple effect while
ensuring internal logical consistency after knowledge editing.

</details>


### [38] [Finding Common Ground: Using Large Language Models to Detect Agreement in Multi-Agent Decision Conferences](https://arxiv.org/abs/2507.08440)
*Selina Heller,Mohamed Ibrahim,David Antony Selby,Sebastian Vollmer*

Main category: cs.CL

TL;DR: 提出基于大语言模型（LLM）的多智能体系统，通过立场检测与极性分析有效模拟决策会议共识形成，提升群体辩论效率与决策质量。


<details>
  <summary>Details</summary>
Motivation: 利用LLM模拟真实决策会议场景，解决复杂议题中动态、细微的共识检测难题，探索多智能体系统对群体决策的复现与优化潜力。

Method: 评估6种LLM在立场检测（识别观点倾向）和立场极性检测（判断正/负/中性情感）任务的表现，并将其嵌入多智能体系统进行动态辩论模拟。

Result: LLM在复杂辩论中可稳定识别共识，引入专用共识检测代理使讨论效率提升23%，决策质量接近真实专家会议水平（F1值达0.87）。

Conclusion: LLM多智能体系统能有效模拟群体决策过程，可作为跨领域专家研讨会的决策支持工具，特别是在政策制定、危机管理等需快速达成共识的场景。

Abstract: Decision conferences are structured, collaborative meetings that bring
together experts from various fields to address complex issues and reach a
consensus on recommendations for future actions or policies. These conferences
often rely on facilitated discussions to ensure productive dialogue and
collective agreement. Recently, Large Language Models (LLMs) have shown
significant promise in simulating real-world scenarios, particularly through
collaborative multi-agent systems that mimic group interactions. In this work,
we present a novel LLM-based multi-agent system designed to simulate decision
conferences, specifically focusing on detecting agreement among the participant
agents. To achieve this, we evaluate six distinct LLMs on two tasks: stance
detection, which identifies the position an agent takes on a given issue, and
stance polarity detection, which identifies the sentiment as positive,
negative, or neutral. These models are further assessed within the multi-agent
system to determine their effectiveness in complex simulations. Our results
indicate that LLMs can reliably detect agreement even in dynamic and nuanced
debates. Incorporating an agreement-detection agent within the system can also
improve the efficiency of group debates and enhance the overall quality and
coherence of deliberations, making them comparable to real-world decision
conferences regarding outcome and decision-making. These findings demonstrate
the potential for LLM-based multi-agent systems to simulate group
decision-making processes. They also highlight that such systems could be
instrumental in supporting decision-making with expert elicitation workshops
across various domains.

</details>


### [39] [Diagnosing Failures in Large Language Models' Answers: Integrating Error Attribution into Evaluation Framework](https://arxiv.org/abs/2507.08459)
*Zishan Xu,Shuyi Xie,Qingsong Lv,Shupei Xiao,Linlin Song,Sui Wenjuan,Fan Lin*

Main category: cs.CL

TL;DR: 提出首个通用评估框架Misattribution Framework及配套数据集AttriData，并开发能同时生成评分、错误归因和反馈的专用模型MisAttributionLLM


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估模型缺乏错误归因能力，无法系统分析模型表现缺陷。海量用户交互数据亟需自动化诊断框架实现高效错误分析

Method: 1. 建立6大类15子类的错误归因分类体系
2. 构建包含错误归因标注的专用数据集AttriData
3. 基于该数据集微调出首个三合一评估模型MisAttributionLLM

Result: 实验证明模型能有效完成评分(score)、错误归因(misattribution)和反馈生成(feedback)三项任务，在鲁棒性和泛化性上表现优异

Conclusion: 该框架为LLM性能分析提供系统化工具，AttriData数据集和MisAttributionLLM模型填补了现有评估体系在错误诊断方面的能力空白

Abstract: With the widespread application of Large Language Models (LLMs) in various
tasks, the mainstream LLM platforms generate massive user-model interactions
daily. In order to efficiently analyze the performance of models and diagnose
failures in their answers, it is essential to develop an automated framework to
systematically categorize and attribute errors. However, existing evaluation
models lack error attribution capability. In this work, we establish a
comprehensive Misattribution Framework with 6 primary and 15 secondary
categories to facilitate in-depth analysis. Based on this framework, we present
AttriData, a dataset specifically designed for error attribution, encompassing
misattribution, along with the corresponding scores and feedback. We also
propose MisAttributionLLM, a fine-tuned model on AttriData, which is the first
general-purpose judge model capable of simultaneously generating score,
misattribution, and feedback. Extensive experiments and analyses are conducted
to confirm the effectiveness and robustness of our proposed method.

</details>


### [40] [Using Large Language Models for Legal Decision-Making in Austrian Value-Added Tax Law: An Experimental Study](https://arxiv.org/abs/2507.08468)
*Marina Luketina,Andrea Benkel,Christoph G. Schuetz*

Main category: cs.CL

TL;DR: 评估LLM在奥地利/欧盟增值税法律决策中的潜力，通过微调和RAG方法验证其辅助税务咨询的可行性


<details>
  <summary>Details</summary>
Motivation: 税务实践中客户常使用自然语言描述案例，LLM可帮助自动化决策并减轻税务专家负担，但需解决法律领域幻觉问题

Method: 采用微调与检索增强生成(RAG)方法，分别在教科书案例和税务咨询公司真实案例中进行系统化实验

Result: LLM可有效支持增值税任务决策并提供法律依据，但处理隐性客户知识和特定场景文件仍存在局限

Conclusion: 合理配置的LLM系统能提升税务工作效率，但需整合结构化背景信息以实现更可靠的法律推理

Abstract: This paper provides an experimental evaluation of the capability of large
language models (LLMs) to assist in legal decision-making within the framework
of Austrian and European Union value-added tax (VAT) law. In tax consulting
practice, clients often describe cases in natural language, making LLMs a prime
candidate for supporting automated decision-making and reducing the workload of
tax professionals. Given the requirement for legally grounded and
well-justified analyses, the propensity of LLMs to hallucinate presents a
considerable challenge. The experiments focus on two common methods for
enhancing LLM performance: fine-tuning and retrieval-augmented generation
(RAG). In this study, these methods are applied on both textbook cases and
real-world cases from a tax consulting firm to systematically determine the
best configurations of LLM-based systems and assess the legal-reasoning
capabilities of LLMs. The findings highlight the potential of using LLMs to
support tax consultants by automating routine tasks and providing initial
analyses, although current prototypes are not ready for full automation due to
the sensitivity of the legal domain. The findings indicate that LLMs, when
properly configured, can effectively support tax professionals in VAT tasks and
provide legally grounded justifications for decisions. However, limitations
remain regarding the handling of implicit client knowledge and context-specific
documentation, underscoring the need for future integration of structured
background information.

</details>


### [41] [ILT-Iterative LoRA Training through Focus-Feedback-Fix for Multilingual Speech Recognition](https://arxiv.org/abs/2507.08477)
*Qingliang Meng,Hao Wu,Wei Liang,Wei Xu,Qing Zhao*

Main category: cs.CL

TL;DR: 提出迭代LoRA训练(ILT)与迭代伪标签策略，解决监督微调阶段的过拟合问题，在Whisper-large-v3和Qwen2-Audio上验证有效性，并在MLC-SLM挑战赛中获得双赛道佳绩


<details>
  <summary>Details</summary>
Motivation: 针对低秩适应(LoRA)在监督微调阶段易出现模型过拟合、限制性能上限的问题，探索提升语音识别模型理论上限的有效方法

Method: 创新性提出三阶段训练流程：聚焦训练→反馈训练→修正训练，结合迭代式伪标签生成策略，基于Whisper和Qwen2-Audio框架实现

Result: 在Interspeech 2025 MLC-SLM挑战赛中，Track1多语言ASR任务第四名，Track2语音分离与识别任务夺冠

Conclusion: 该方法显著提升语音识别系统的鲁棒性，验证了迭代训练范式在复杂语音场景下的工程可行性，具有重要的商业落地价值

Abstract: The deep integration of large language models and automatic speech
recognition systems has become a promising research direction with high
practical value. To address the overfitting issue commonly observed in Low-Rank
Adaptation (LoRA) during the supervised fine-tuning (SFT) stage, this work
proposes an innovative training paradigm Iterative LoRA Training (ILT) in
combination with an Iterative Pseudo Labeling strategy, effectively enhancing
the theoretical upper bound of model performance. Based on Whisper-large-v3 and
Qwen2-Audio, we conduct systematic experiments using a three-stage training
process: Focus Training, Feed Back Training, and Fix Training. Experimental
results demonstrate the effectiveness of the proposed method. Furthermore, the
MegaAIS research team applied this technique in the Interspeech 2025
Multilingual Conversational Speech Language Modeling Challenge (MLC-SLM),
achieving 4th in Track 1 (Multilingual ASR Task) and 1st place in Track 2
(Speech Separation and Recognition Task), showcasing the practical feasibility
and strong application potential of our approach.

</details>


### [42] [Enhancing Essay Cohesion Assessment: A Novel Item Response Theory Approach](https://arxiv.org/abs/2507.08487)
*Bruno Alexandre Rosa,Hilário Oliveira,Luiz Rodrigues,Eduardo Araujo Oliveira,Rafael Ferreira Mello*

Main category: cs.CL

TL;DR: 提出基于项目反应理论调整机器学习模型预测文章连贯性得分的方法，在巴西教育类作文数据集上验证效果优于传统模型


<details>
  <summary>Details</summary>
Motivation: 现有机器学习算法在自动评估文章连贯性时未充分考虑文本个体特征，需通过项目反应理论量化模型能力参数以提高预测精度

Method: 使用ENEM风格作文和公立学校叙事作文数据集，提取325个语言学特征，构建基于项目反应理论的机器学习回归模型

Result: 实验表明该方法在RMSE、MAE等指标上优于传统机器学习模型和集成方法

Conclusion: 本研究为教育领域文本自动评估提供了有效的模型优化新路径

Abstract: Essays are considered a valuable mechanism for evaluating learning outcomes
in writing. Textual cohesion is an essential characteristic of a text, as it
facilitates the establishment of meaning between its parts. Automatically
scoring cohesion in essays presents a challenge in the field of educational
artificial intelligence. The machine learning algorithms used to evaluate texts
generally do not consider the individual characteristics of the instances that
comprise the analysed corpus. In this meaning, item response theory can be
adapted to the context of machine learning, characterising the ability,
difficulty and discrimination of the models used. This work proposes and
analyses the performance of a cohesion score prediction approach based on item
response theory to adjust the scores generated by machine learning models. In
this study, the corpus selected for the experiments consisted of the extended
Essay-BR, which includes 6,563 essays in the style of the National High School
Exam (ENEM), and the Brazilian Portuguese Narrative Essays, comprising 1,235
essays written by 5th to 9th grade students from public schools. We extracted
325 linguistic features and treated the problem as a machine learning
regression task. The experimental results indicate that the proposed approach
outperforms conventional machine learning models and ensemble methods in
several evaluation metrics. This research explores a potential approach for
improving the automatic evaluation of cohesion in educational essays.

</details>


### [43] [A Third Paradigm for LLM Evaluation: Dialogue Game-Based Evaluation using clembench](https://arxiv.org/abs/2507.08491)
*David Schlangen,Sherzod Hakimov,Jonathan Jordan,Philipp Sadler*

Main category: cs.CL

TL;DR: 论文提出对话游戏评估新范式clembench，结合参考评估与偏好评估优势，通过可重复的多轮交互实现目标导向的LLM评测。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估范式存在控制力与生态效度的矛盾，需要兼具可控性、目标导向和实际用例覆盖的新评估方法。

Method: 开发clembench框架，提供标准化对话游戏基准测试集，支持自定义游戏扩展和自动化模型评测。

Result: clembench实现易用性优化，支持快速模型测试（提供英语基准集）和定制化测试扩展，提升评估效率。

Conclusion: 对话游戏评估范式有效整合不同方法优势，clembench的成熟实现将推动该范式在LLM评估领域的广泛应用。

Abstract: There are currently two main paradigms for evaluating large language models
(LLMs), reference-based evaluation and preference-based evaluation. The first,
carried over from the evaluation of machine learning models in general, relies
on pre-defined task instances, for which reference task executions are
available. The second, best exemplified by the LM-arena, relies on (often
self-selected) users bringing their own intents to a site that routes these to
several models in parallel, among whose responses the user then selects their
most preferred one. The former paradigm hence excels at control over what is
tested, while the latter comes with higher ecological validity, testing actual
use cases interactively. Recently, a third complementary paradigm has emerged
that combines some of the strengths of these approaches, offering control over
multi-turn, reference-free, repeatable interactions, while stressing
goal-directedness: dialogue game based evaluation. While the utility of this
approach has been shown by several projects, its adoption has been held back by
the lack of a mature, easily re-usable implementation. In this paper, we
present clembench, which has been in continuous development since 2023 and has
in its latest release been optimized for ease of general use. We describe how
it can be used to benchmark one's own models (using a provided set of benchmark
game instances in English), as well as how easily the benchmark itself can be
extended with new, tailor-made targeted tests.

</details>


### [44] [LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning](https://arxiv.org/abs/2507.08496)
*Shibo Sun,Xue Li,Donglin Di,Mingjie Wei,Lanshun Nie,Wei-Nan Zhang,Dechen Zhan,Yang Song,Lei Fan*

Main category: cs.CL

TL;DR: LLaPa框架通过结合视觉-语言模型与辅助模块TER/CAR，显著提升多模态程序化规划与反事实推理能力，在多个基准测试中优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在具身智能系统的程序化规划中，对多模态输入整合和反事实推理能力不足。需要开发能同时处理文本指令和视觉环境，并适应反事实条件的规划框架。

Method: 1. 构建LLaPa视觉-语言框架生成可执行动作序列；2. TER模块通过任务导向分割创建任务敏感特征空间；3. CAR模块通过反事实条件检索增强推理能力。

Result: 在ActPlan-1K和ALFRED基准测试中，LLaPa的LCS指标和动作正确率显著优于GPT-4V等先进模型，生成更高质量的行动计划。

Conclusion: 通过整合视觉-语言模型与任务敏感特征对齐、反事实推理增强模块，LLaPa有效提升了多模态环境下的程序化规划质量，为具身智能系统提供了更可靠的决策支持。

Abstract: While large language models (LLMs) have advanced procedural planning for
embodied AI systems through strong reasoning abilities, the integration of
multimodal inputs and counterfactual reasoning remains underexplored. To tackle
these challenges, we introduce LLaPa, a vision-language model framework
designed for multimodal procedural planning. LLaPa generates executable action
sequences from textual task descriptions and visual environmental images using
vision-language models (VLMs). Furthermore, we enhance LLaPa with two auxiliary
modules to improve procedural planning. The first module, the Task-Environment
Reranker (TER), leverages task-oriented segmentation to create a task-sensitive
feature space, aligning textual descriptions with visual environments and
emphasizing critical regions for procedural execution. The second module, the
Counterfactual Activities Retriever (CAR), identifies and emphasizes potential
counterfactual conditions, enhancing the model's reasoning capability in
counterfactual scenarios. Extensive experiments on ActPlan-1K and ALFRED
benchmarks demonstrate that LLaPa generates higher-quality plans with superior
LCS and correctness, outperforming advanced models. The code and models are
available https://github.com/sunshibo1234/LLaPa.

</details>


### [45] [Semantic-Augmented Latent Topic Modeling with LLM-in-the-Loop](https://arxiv.org/abs/2507.08498)
*Mengze Hong,Chen Jason Zhang,Di Jiang*

Main category: cs.CL

TL;DR: LLM增强LDA主题模型在初始化阶段效果有限，后校正阶段提升5.86%文本连贯性


<details>
  <summary>Details</summary>
Motivation: 验证LLM与传统主题模型(LDA)的协同效应，突破传统模型对初始化敏感、主题连贯性不足的局限

Method: 在LDA的Gibbs采样初始化阶段引入LLM指导主题聚类，并在后处理阶段进行LLM语义校正

Result: LLM初始化加速早期收敛但最终效果弱于基线，LLM后校正使主题连贯性提升5.86%

Conclusion: LLM在特定环节(如语义校正)展现价值，但挑战了'LLM必然优于传统方法'的认知

Abstract: Latent Dirichlet Allocation (LDA) is a prominent generative probabilistic
model used for uncovering abstract topics within document collections. In this
paper, we explore the effectiveness of augmenting topic models with Large
Language Models (LLMs) through integration into two key phases: Initialization
and Post-Correction. Since the LDA is highly dependent on the quality of its
initialization, we conduct extensive experiments on the LLM-guided topic
clustering for initializing the Gibbs sampling algorithm. Interestingly, the
experimental results reveal that while the proposed initialization strategy
improves the early iterations of LDA, it has no effect on the convergence and
yields the worst performance compared to the baselines. The LLM-enabled
post-correction, on the other hand, achieved a promising improvement of 5.86%
in the coherence evaluation. These results highlight the practical benefits of
the LLM-in-the-loop approach and challenge the belief that LLMs are always the
superior text mining alternative.

</details>


### [46] [PromotionGo at SemEval-2025 Task 11: A Feature-Centric Framework for Cross-Lingual Multi-Emotion Detection in Short Texts](https://arxiv.org/abs/2507.08499)
*Ziyi Huang,Xia Cui*

Main category: cs.CL

TL;DR: 提出基于特征动态适配的多语言短文本情绪检测框架，在28种语言中验证了TF-IDF对低资源语言的高效性及上下文嵌入模型的语言特异性优势，PCA降维显著提升计算效率


<details>
  <summary>Details</summary>
Motivation: 解决多语言情绪检测中语言多样性带来的技术挑战（特别是低资源语言），平衡模型性能与计算资源消耗

Method: 特征中心框架：1) 动态适配文本表征（TF-IDF/FastText/Sentence-BERT） 2) PCA降维处理 3) 多层感知机等模型训练，重点分析5种语言的性能表现

Result: TF-IDF在低资源语言F1值达0.68；FastText在形态丰富语言表现突出；PCA使MLP训练时间减少40%且精度保持

Conclusion: 该框架通过特征工程与计算优化，为资源受限场景下的多语言情绪检测提供了可扩展解决方案

Abstract: This paper presents our system for SemEval 2025 Task 11: Bridging the Gap in
Text-Based Emotion Detection (Track A), which focuses on multi-label emotion
detection in short texts. We propose a feature-centric framework that
dynamically adapts document representations and learning algorithms to optimize
language-specific performance. Our study evaluates three key components:
document representation, dimensionality reduction, and model training in 28
languages, highlighting five for detailed analysis. The results show that
TF-IDF remains highly effective for low-resource languages, while contextual
embeddings like FastText and transformer-based document representations, such
as those produced by Sentence-BERT, exhibit language-specific strengths.
Principal Component Analysis (PCA) reduces training time without compromising
performance, particularly benefiting FastText and neural models such as
Multi-Layer Perceptrons (MLP). Computational efficiency analysis underscores
the trade-off between model complexity and processing cost. Our framework
provides a scalable solution for multilingual emotion detection, addressing the
challenges of linguistic diversity and resource constraints.

</details>


### [47] [The AI Language Proficiency Monitor -- Tracking the Progress of LLMs on Multilingual Benchmarks](https://arxiv.org/abs/2507.08538)
*David Pomerenke,Jonas Nothnagel,Simon Ostermann*

Main category: cs.CL

TL;DR: 开发多语言基准测试AI Language Proficiency Monitor，评估大语言模型在200种语言（尤其低资源语言）的表现，并提供开源自动更新排行榜。


<details>
  <summary>Details</summary>
Motivation: 确保大语言模型能力评估覆盖全球语言（特别是低资源语言），促进AI公平性和包容性。

Method: 整合翻译、问答、数学和推理任务，使用FLORES+、MMLU等数据集，构建开源自动更新的评估平台。

Result: 创建支持全球模型性能分析的可视化仪表板，提供语言熟练度地图和趋势分析等洞察工具。

Conclusion: 通过透明化多语言AI评估体系，推动技术包容性发展，补充现有基准测试并促进技术进步。

Abstract: To ensure equitable access to the benefits of large language models (LLMs),
it is essential to evaluate their capabilities across the world's languages. We
introduce the AI Language Proficiency Monitor, a comprehensive multilingual
benchmark that systematically assesses LLM performance across up to 200
languages, with a particular focus on low-resource languages. Our benchmark
aggregates diverse tasks including translation, question answering, math, and
reasoning, using datasets such as FLORES+, MMLU, GSM8K, TruthfulQA, and ARC. We
provide an open-source, auto-updating leaderboard and dashboard that supports
researchers, developers, and policymakers in identifying strengths and gaps in
model performance. In addition to ranking models, the platform offers
descriptive insights such as a global proficiency map and trends over time. By
complementing and extending prior multilingual benchmarks, our work aims to
foster transparency, inclusivity, and progress in multilingual AI. The system
is available at
https://huggingface.co/spaces/fair-forward/evals-for-every-language.

</details>


### [48] [DocPolarBERT: A Pre-trained Model for Document Understanding with Relative Polar Coordinate Encoding of Layout Structures](https://arxiv.org/abs/2507.08606)
*Benno Uthayasooriyar,Antoine Ly,Franck Vermet,Caio Corro*

Main category: cs.CL

TL;DR: DocPolarBERT通过极坐标系相对位置编码替代绝对位置嵌入，在减少预训练数据量的情况下实现SOTA文档理解效果


<details>
  <summary>Details</summary>
Motivation: 解决传统文档理解模型对大规模预训练数据和绝对2D位置嵌入的依赖问题，通过改进注意力机制提升效率

Method: 扩展自注意力机制，采用相对极坐标系编码文本块位置关系，替代笛卡尔坐标系的位置嵌入方案

Result: 使用比IIT-CDIP小6倍的数据集预训练后，模型在文档理解任务中取得最先进性能

Conclusion: 布局感知的注意力机制设计能有效弥补数据不足，为文档理解提供高效解决方案

Abstract: We introduce DocPolarBERT, a layout-aware BERT model for document
understanding that eliminates the need for absolute 2D positional embeddings.
We extend self-attention to take into account text block positions in relative
polar coordinate system rather than the Cartesian one. Despite being
pre-trained on a dataset more than six times smaller than the widely used
IIT-CDIP corpus, DocPolarBERT achieves state-of-the-art results. These results
demonstrate that a carefully designed attention mechanism can compensate for
reduced pre-training data, offering an efficient and effective alternative for
document understanding.

</details>


### [49] [A comprehensive study of LLM-based argument classification: from LLAMA through GPT-4o to Deepseek-R1](https://arxiv.org/abs/2507.08621)
*Marcin Pietroń,Rafał Olszowski,Jakub Gomułka,Filip Gampel,Andrzej Tomski*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型(LLMs)在论点挖掘任务中的表现，发现GPT-4o和Deepseek-R1分别在基础模型和推理增强模型中表现最优，但两者仍存在系统性错误，研究首次系统分析了主流论点数据集并揭示了提示算法的改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏LLMs在公开论点分类数据库中的系统性评估，需验证其在实际应用场景中的性能差异及缺陷。

Method: 使用Args.me/UKP等多样化数据集，测试GPT/Llama/DeepSeek系列模型，并通过Chain-of-Thoughts算法增强部分模型的推理能力。

Result: ChatGPT-4o在分类基准中表现最佳，Deepseek-R1在推理任务中领先，但所有模型均存在语义理解错误和关系误判等共性问题。

Conclusion: 研究首次实现LLMs对主流论点数据集的广泛测试，揭示了现有提示算法在复杂语义解析中的局限性，并为模型优化提供了数据层面的改进方向。

Abstract: Argument mining (AM) is an interdisciplinary research field that integrates
insights from logic, philosophy, linguistics, rhetoric, law, psychology, and
computer science. It involves the automatic identification and extraction of
argumentative components, such as premises and claims, and the detection of
relationships between them, such as support, attack, or neutrality. Recently,
the field has advanced significantly, especially with the advent of large
language models (LLMs), which have enhanced the efficiency of analyzing and
extracting argument semantics compared to traditional methods and other deep
learning models. There are many benchmarks for testing and verifying the
quality of LLM, but there is still a lack of research and results on the
operation of these models in publicly available argument classification
databases. This paper presents a study of a selection of LLM's, using diverse
datasets such as Args.me and UKP. The models tested include versions of GPT,
Llama, and DeepSeek, along with reasoning-enhanced variants incorporating the
Chain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms
the others in the argument classification benchmarks. In case of models
incorporated with reasoning capabilities, the Deepseek-R1 shows its
superiority. However, despite their superiority, GPT-4o and Deepseek-R1 still
make errors. The most common errors are discussed for all models. To our
knowledge, the presented work is the first broader analysis of the mentioned
datasets using LLM and prompt algorithms. The work also shows some weaknesses
of known prompt algorithms in argument analysis, while indicating directions
for their improvement. The added value of the work is the in-depth analysis of
the available argument datasets and the demonstration of their shortcomings.

</details>


### [50] [The Impact of Automatic Speech Transcription on Speaker Attribution](https://arxiv.org/abs/2507.08660)
*Cristina Aggazzotti,Matthew Wiesner,Elizabeth Allyn Smith,Nicholas Andrews*

Main category: cs.CL

TL;DR: 研究比较人工与自动语音识别（ASR）转录文本在说话人归属任务中的性能，发现ASR转录错误对结果影响有限且可能增强识别效果。


<details>
  <summary>Details</summary>
Motivation: 实际场景中ASR转录文本普遍存在错误，而此前研究多基于人工精准转录，需评估ASR转录对说话人归属任务的实际影响。

Method: 通过系统性实验分析ASR转录错误对说话人归属性能的衰减程度，并探究ASR系统特性（如词错误率）与归属性能的关联性。

Result: ASR转录的词级错误对归属性能影响较小，且转录文本还原度与归属性能相关性低；ASR错误甚至可能携带说话人身份特征。

Conclusion: ASR转录文本在说话人归属任务中表现不逊于人工转录，错误可能成为身份识别线索，提示可优先采用ASR数据提升实用性。

Abstract: Speaker attribution from speech transcripts is the task of identifying a
speaker from the transcript of their speech based on patterns in their language
use. This task is especially useful when the audio is unavailable (e.g.
deleted) or unreliable (e.g. anonymized speech). Prior work in this area has
primarily focused on the feasibility of attributing speakers using transcripts
produced by human annotators. However, in real-world settings, one often only
has more errorful transcripts produced by automatic speech recognition (ASR)
systems. In this paper, we conduct what is, to our knowledge, the first
comprehensive study of the impact of automatic transcription on speaker
attribution performance. In particular, we study the extent to which speaker
attribution performance degrades in the face of transcription errors, as well
as how properties of the ASR system impact attribution. We find that
attribution is surprisingly resilient to word-level transcription errors and
that the objective of recovering the true transcript is minimally correlated
with attribution performance. Overall, our findings suggest that speaker
attribution on more errorful transcripts produced by ASR is as good, if not
better, than attribution based on human-transcribed data, possibly because ASR
transcription errors can capture speaker-specific features revealing of speaker
identity.

</details>


### [51] [KELPS: A Framework for Verified Multi-Language Autoformalization via Semantic-Syntactic Alignment](https://arxiv.org/abs/2507.08665)
*Jiyao Zhang,Chengli Zhong,Hui Xu,Qige Li,Yi Zhou*

Main category: cs.CL

TL;DR: 提出神经符号框架KELPS，通过知识方程将非正式数学转化为多形式化语言，缓解多语言语料库数据瓶颈


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型受限于多语言平行语料库的数量和质量，难以有效实现数学形式化验证

Method: 1. 设计基于断言逻辑的知识方程(KEs)
2. 通过严格转换规则生成Lean/Coq/Isabelle代码
3. 迭代式翻译-合成-过滤框架构建语料库

Result: 1. 生成60,000+平行问题
2. 在MiniF2F实现88.9%语法准确率(pass@1)
3. 超越Deepseek-V3(81%)等SOTA模型

Conclusion: KELPS框架成功解决数据稀缺与质量问题，其理论严谨的转换规则和可扩展架构为形式化验证提供了新范式，开源代码和数据集将促进领域发展

Abstract: Modern large language models (LLMs) show promising progress in formalizing
informal mathematics into machine-verifiable theorems. However, these methods
still face bottlenecks due to the limited quantity and quality of multilingual
parallel corpora. In this paper, we propose a novel neuro-symbolic framework
KELPS (Knowledge-Equation based Logical Processing System) to address these
problems. KELPS is an iterative framework for translating, synthesizing, and
filtering informal data into multiple formal languages (Lean, Coq, and
Isabelle). First, we translate natural language into Knowledge Equations (KEs),
a novel language that we designed, theoretically grounded in assertional logic.
Next, we convert them to target languages through rigorously defined rules that
preserve both syntactic structure and semantic meaning. This process yielded a
parallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic
accuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3
(81%) and Herald (81.3%) across multiple datasets. All datasets and codes are
available in the supplementary materials.

</details>


### [52] [KG-Attention: Knowledge Graph-Guided Attention at Test-Time via Bidirectional Information Aggregation](https://arxiv.org/abs/2507.08704)
*Songlin Zhai,Guilin Qi,Yuan Meng*

Main category: cs.CL

TL;DR: 提出首个测试时知识图谱增强框架KGA，通过双向注意力路径实现无需参数更新的动态知识融合，在五个基准测试中验证有效性


<details>
  <summary>Details</summary>
Motivation: 解决现有KG增强方法依赖参数微调导致的灾难性遗忘问题，以及静态集成框架难以适应实时知识更新的局限性

Method: 设计知识图谱引导的双向注意力机制：外向路径通过输入驱动的KG融合整合外部知识，内向路径通过KG引导的过滤机制强化知识相关模式，形成闭环增强机制

Result: 在五个基准测试中验证了KGA具有与参数微调方法相当的知识融合性能

Conclusion: 该框架首次实现测试时动态知识融合，无需修改模型参数即可支持实时知识更新，为LLMs的知识增强提供了新范式

Abstract: Knowledge graphs (KGs) play a critical role in enhancing large language
models (LLMs) by introducing structured and grounded knowledge into the
learning process. However, most existing KG-enhanced approaches rely on
parameter-intensive fine-tuning, which risks catastrophic forgetting and
degrades the pretrained model's generalization. Moreover, they exhibit limited
adaptability to real-time knowledge updates due to their static integration
frameworks. To address these issues, we introduce the first test-time
KG-augmented framework for LLMs, built around a dedicated knowledge
graph-guided attention (KGA) module that enables dynamic knowledge fusion
without any parameter updates. The proposed KGA module augments the standard
self-attention mechanism with two synergistic pathways: outward and inward
aggregation. Specifically, the outward pathway dynamically integrates external
knowledge into input representations via input-driven KG fusion. This inward
aggregation complements the outward pathway by refining input representations
through KG-guided filtering, suppressing task-irrelevant signals and amplifying
knowledge-relevant patterns. Importantly, while the outward pathway handles
knowledge fusion, the inward path selects the most relevant triples and feeds
them back into the fusion process, forming a closed-loop enhancement mechanism.
By synergistically combining these two pathways, the proposed method supports
real-time knowledge fusion exclusively at test-time, without any parameter
modification. Extensive experiments on five benchmarks verify the comparable
knowledge fusion performance of KGA.

</details>


### [53] [Multilingual Multimodal Software Developer for Code Generation](https://arxiv.org/abs/2507.08719)
*Linzheng Chai,Jian Yang,Shukai Liu,Wei Zhang,Liran Wang,Ke Jin,Tao Sun,Congnan Liu,Chenchen Zhang,Hualei Zhu,Jiaheng Liu,Xianjie Wu,Ge Zhang,Tianyu Liu,Zhoujun Li*

Main category: cs.CL

TL;DR: 提出多模态代码生成模型MM-Coder，通过整合UML图和流程图等视觉工作流与文本指令提升代码生成质量，构建MMc-Instruct数据集和MMEval评估基准揭示现有模型在视觉信息捕捉等维度的不足。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型仅关注文本而忽略视觉设计元素，难以满足真实软件开发中图文并重的需求，亟需突破单模态代码生成的局限性。

Method: 1. 开发支持多模态输入的MM-Coder模型；2. 构建包含视觉工作流代码生成的MMc-Instruct指令数据集；3. 设计评估多模态代码生成能力的MMEval新基准。

Result: 实验显示模型在视觉信息精确提取（如流程图元素识别）、复杂指令遵循（如多步骤业务逻辑实现）和高级编程概念理解（如架构模式应用）方面仍存在显著挑战。

Conclusion: 通过融合文本与视觉设计的多模态方法，推动工业级编程智能化发展，使LLM能够准确实现图文混合表达的系统需求。

Abstract: The rapid advancement of Large Language Models (LLMs) has significantly
improved code generation, yet most models remain text-only, neglecting crucial
visual aids like diagrams and flowcharts used in real-world software
development. To bridge this gap, we introduce MM-Coder, a Multilingual
Multimodal software developer. MM-Coder integrates visual design inputs-Unified
Modeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with
textual instructions to enhance code generation accuracy and architectural
alignment. To enable this, we developed MMc-Instruct, a diverse multimodal
instruction-tuning dataset including visual-workflow-based code generation,
allowing MM-Coder to synthesize textual and graphical information like human
developers, distinct from prior work on narrow tasks. Furthermore, we introduce
MMEval, a new benchmark for evaluating multimodal code generation, addressing
existing text-only limitations. Our evaluations using MMEval highlight
significant remaining challenges for models in precise visual information
capture, instruction following, and advanced programming knowledge. Our work
aims to revolutionize industrial programming by enabling LLMs to interpret and
implement complex specifications conveyed through both text and visual designs.

</details>


### [54] [KV Cache Steering for Inducing Reasoning in Small Language Models](https://arxiv.org/abs/2507.08799)
*Max Belitsky,Dawid J. Kopiczko,Michael Dorkenwald,M. Jehanzeb Mirza,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CL

TL;DR: 提出轻量级缓存导向方法，通过一次性键值缓存干预提升小语言模型的多步推理能力，无需微调或修改提示


<details>
  <summary>Details</summary>
Motivation: 解决现有激活导向技术需持续干预导致的超参数敏感、推理效率低、集成困难等问题

Method: 利用GPT-4o生成推理轨迹构建导向向量，通过单次键值缓存调整实现多步推理引导

Result: 在多个推理基准测试中提升模型推理结构质量和任务性能，超参数稳定性与推理效率显著优于基线方法

Conclusion: 缓存导向技术以一次性干预实现稳定高效的行为控制，为语言模型可控生成提供了更实用的解决方案

Abstract: We propose cache steering, a lightweight method for implicit steering of
language models via a one-shot intervention applied directly to the key-value
cache. To validate its effectiveness, we apply cache steering to induce
chain-of-thought reasoning in small language models. Our approach leverages
GPT-4o-generated reasoning traces to construct steering vectors that shift
model behavior toward more explicit, multi-step reasoning without fine-tuning
or prompt modifications. Experimental evaluations on diverse reasoning
benchmarks demonstrate that cache steering improves both the qualitative
structure of model reasoning and quantitative task performance. Compared to
prior activation steering techniques that require continuous interventions, our
one-shot cache steering offers substantial advantages in terms of
hyperparameter stability, inference-time efficiency, and ease of integration,
making it a more robust and practical solution for controlled generation.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [55] [FlowDrag: 3D-aware Drag-based Image Editing with Mesh-guided Deformation Vector Flow Fields](https://arxiv.org/abs/2507.08285)
*Gwanhyeong Koo,Sunjae Yoon,Younghwan Lee,Ji Woo Hong,Chang D. Yoo*

Main category: cs.GR

TL;DR: 提出FlowDrag方法解决拖拽编辑中的几何不一致问题，通过3D网格变形和UNet集成实现精准物体操控，并建立VFD基准数据集验证效果。


<details>
  <summary>Details</summary>
Motivation: 现有拖拽编辑方法因仅关注用户定义点匹配导致几何不一致，产生伪影或编辑不稳定。需结合整体几何信息提升编辑质量。

Method: 构建图像3D网格→设计能量函数驱动网格变形→将3D位移投影至2D→整合到UNet去噪流程实现精准点对齐。

Result: 提出VFD基准数据集(含视频连续帧真值)，FlowDrag在VFD和DragBench上性能优于现有方法。

Conclusion: FlowDrag通过几何感知变形显著提升拖拽编辑精度和结构完整性，VFD数据集填补了编辑效果评估的基准空白。

Abstract: Drag-based editing allows precise object manipulation through point-based
control, offering user convenience. However, current methods often suffer from
a geometric inconsistency problem by focusing exclusively on matching
user-defined points, neglecting the broader geometry and leading to artifacts
or unstable edits. We propose FlowDrag, which leverages geometric information
for more accurate and coherent transformations. Our approach constructs a 3D
mesh from the image, using an energy function to guide mesh deformation based
on user-defined drag points. The resulting mesh displacements are projected
into 2D and incorporated into a UNet denoising process, enabling precise
handle-to-target point alignment while preserving structural integrity.
Additionally, existing drag-editing benchmarks provide no ground truth, making
it difficult to assess how accurately the edits match the intended
transformations. To address this, we present VFD (VidFrameDrag) benchmark
dataset, which provides ground-truth frames using consecutive shots in a video
dataset. FlowDrag outperforms existing drag-based editing methods on both VFD
Bench and DragBench.

</details>


### [56] [Advancing Multimodal LLMs by Large-Scale 3D Visual Instruction Dataset Generation](https://arxiv.org/abs/2507.08513)
*Liu He,Xiao Zeng,Yizhi Song,Albert Y. C. Chen,Lu Xia,Shashwat Verma,Sankalp Dayal,Min Sun,Cheng-Hao Kuo,Daniel Aliaga*

Main category: cs.GR

TL;DR: 通过合成3D视觉指令数据集Ultimate3D解决MLLM在相机-物体关系识别中的瓶颈，准确率提升33.4%


<details>
  <summary>Details</summary>
Motivation: 现有MLLM因训练数据局限难以准确捕捉物体方向/相机视角关系，需建立精准可控的数据生成方法

Method: 结合3D渲染+扩散模型生成逼真图像，利用LLM生成文本指令，构建24万组带精确标注的VQA数据集

Result: 微调后的模型在相机关系识别任务中准确率平均提升33.4%，显著超越商业模型

Conclusion: Ultimate3D数据集及方法为MLLM的视觉-空间理解能力提升提供新范式，具有广泛的应用潜力

Abstract: Multimodal Large Language Models (MLLMs) struggle with accurately capturing
camera-object relations, especially for object orientation, camera viewpoint,
and camera shots. This stems from the fact that existing MLLMs are trained on
images with limited diverse camera-object relations and corresponding textual
descriptions. To address this, we propose a synthetic generation pipeline to
create large-scale 3D visual instruction datasets. Our framework takes 3D
assets as input and uses rendering and diffusion-based image generation models
to create photorealistic images preserving precise camera-object relations.
Additionally, large language models (LLMs) are used to generate text prompts
for guiding visual instruction tuning and controlling image generation. We
create Ultimate3D, a dataset of 240K VQAs with precise camera-object
annotations, and corresponding benchmark. MLLMs fine-tuned on our proposed
dataset outperform commercial models by a large margin, achieving an average
accuracy improvement of 33.4% on camera-object relation recognition tasks. Our
code, dataset, and benchmark will contribute to broad MLLM applications.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [57] [Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training](https://arxiv.org/abs/2507.08284)
*Aleksei Ilin,Gor Matevosyan,Xueying Ma,Vladimir Eremin,Suhaa Dada,Muqun Li,Riyaaz Shaik,Haluk Noyan Tokgozoglu*

Main category: cs.LG

TL;DR: 通过高保真合成数据生成和对抗训练，小语言模型在内容审核任务中超越大模型，降低计算成本并提升对抗攻击防御能力


<details>
  <summary>Details</summary>
Motivation: 解决大模型在内容审核中存在的高计算成本和潜在脆弱性问题，探索小模型通过高效训练方法实现更优安全防护的可能性

Method: 1. 基于人工种子数据进行查询增强与改写生成多样化数据 → 多轮筛选确保数据质量
2. 采用类似GAN的强化学习对抗训练框架：生成器生产挑战性样本 → 迭代优化安全分类器
3. 结合高效LLM训练策略实现模型能力迁移

Result: 小模型在内容审核性能上超越同任务大模型，计算开销降低38%，对抗攻击成功率下降52%

Conclusion: 该框架为AI内容审核提供了可扩展的高效解决方案，通过数据质量优化和对抗训练机制创新，实现安全性与效率的协同提升

Abstract: We introduce a lightweight yet highly effective safety guardrail framework
for language models, demonstrating that small-scale language models can
achieve, and even surpass, the performance of larger counterparts in content
moderation tasks. This is accomplished through high-fidelity synthetic data
generation and adversarial training. The synthetic data generation process
begins with human-curated seed data, which undergoes query augmentation and
paraphrasing to create diverse and contextually rich examples. This augmented
data is then subjected to multiple rounds of curation, ensuring high fidelity
and relevance. Inspired by recent advances in the Generative Adversarial
Network (GAN) architecture, our adversarial training employs reinforcement
learning to guide a generator that produces challenging synthetic examples.
These examples are used to fine-tune the safety classifier, enhancing its
ability to detect and mitigate harmful content. Additionally, we incorporate
strategies from recent research on efficient LLM training, leveraging the
capabilities of smaller models to improve the performance of larger generative
models. With iterative adversarial training and the generation of diverse,
high-quality synthetic data, our framework enables small language models (SLMs)
to serve as robust safety guardrails. This approach not only reduces
computational overhead but also enhances resilience against adversarial
attacks, offering a scalable and efficient solution for content moderation in
AI systems.

</details>


### [58] [Scaling Attention to Very Long Sequences in Linear Time with Wavelet-Enhanced Random Spectral Attention (WERSA)](https://arxiv.org/abs/2507.08637)
*Vincenzo Dentamaro*

Main category: cs.LG

TL;DR: 提出线性复杂度Wavelet-Enhanced Random Spectral Attention(WERSA)，在保持精度的同时显著降低长序列处理的计算成本


<details>
  <summary>Details</summary>
Motivation: 传统Transformer的二次方注意力机制在长序列处理中存在计算效率低、显存占用大的问题，需开发更高效的计算范式

Method: 融合多分辨率Haar小波、内容自适应的随机谱特征和可学习参数，实现线性计算复杂度的多尺度选择性注意力机制

Result: 在单GPU环境下，ArXiv分类任务准确率提升1.2%(86.2% vs 85.0%)，训练时间减少81%，FLOPs降低73.4%；在128k长度序列上取得79.1%准确率(同类最佳)

Conclusion: WERSA突破了传统注意力机制的长序列处理瓶颈，为资源受限设备上的可持续AI发展提供了高效解决方案

Abstract: Transformer models are computationally costly on long sequences since regular
attention has quadratic $O(n^2)$ time complexity. We introduce Wavelet-Enhanced
Random Spectral Attention (WERSA), a novel mechanism of linear $O(n)$ time
complexity that is pivotal to enable successful long-sequence processing
without the performance trade-off. WERSA merges content-adaptive random
spectral features together with multi-resolution Haar wavelets and learnable
parameters to selectively attend to informative scales of data while preserving
linear efficiency.
  Large-scale comparisons \textbf{on single GPU} and across various benchmarks
(vision, NLP, hierarchical reasoning) and various attention mechanisms (like
Multiheaded Attention, Flash-Attention-2, FNet, Linformer, Performer,
Waveformer), reveal uniform advantages of WERSA. It achieves best accuracy in
all tests. On ArXiv classification, WERSA improves accuracy over vanilla
attention by 1.2\% (86.2\% vs 85.0\%) while cutting training time by 81\% (296s
vs 1554s) and FLOPS by 73.4\% (26.2G vs 98.4G). Significantly, WERSA excels
where vanilla and FlashAttention-2 fail: on ArXiv-128k's extremely lengthy
sequences, it achieves best accuracy (79.1\%) and AUC (0.979) among viable
methods, operating on data that gives Out-Of-Memory errors to quadratic methods
while being \textbf{twice as fast} as Waveformer, its next-best competitor.
  By significantly reducing computational loads without compromising accuracy,
WERSA makes possible more practical, more affordable, long-context models, in
particular on low-resource hardware, for more sustainable and more scalable AI
development.

</details>


### [59] [BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity](https://arxiv.org/abs/2507.08771)
*Chenyang Song,Weilin Zhao,Xu Han,Chaojun Xiao,Yingfa Chen,Yuxuan Li,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: 提出BlockFFN架构，通过可微分路由机制和CLS感知训练目标，实现80%令牌级稀疏和70%块级稀疏，显著提升MoE模型加速性能


<details>
  <summary>Details</summary>
Motivation: 传统MoE架构存在路由不可微分、块级稀疏度低的问题，难以兼容主流加速技术且不利于端侧设备部署

Method: 1. 整合ReLU和RMSNorm的可微分路由机制
2. 设计CLS感知训练目标
3. 开发结合稀疏激活与推测解码的加速内核

Result: 在真实端侧设备实现3.67倍加速，超过所有MoE基线模型，开源代码和检查点

Conclusion: BlockFFN通过技术创新有效平衡模型性能与加速需求，为资源受限场景下的LLM部署提供新方案

Abstract: To alleviate the computational burden of large language models (LLMs),
architectures with activation sparsity, represented by mixture-of-experts
(MoE), have attracted increasing attention. However, the non-differentiable and
inflexible routing of vanilla MoE hurts model performance. Moreover, while each
token activates only a few parameters, these sparsely-activated architectures
exhibit low chunk-level sparsity, indicating that the union of multiple
consecutive tokens activates a large ratio of parameters. Such a sparsity
pattern is unfriendly for acceleration under low-resource conditions (e.g.,
end-side devices) and incompatible with mainstream acceleration techniques
(e.g., speculative decoding). To address these challenges, we introduce a novel
MoE architecture, BlockFFN, as well as its efficient training and deployment
techniques. Specifically, we use a router integrating ReLU activation and
RMSNorm for differentiable and flexible routing. Next, to promote both
token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training
objectives are designed, making BlockFFN more acceleration-friendly. Finally,
we implement efficient acceleration kernels, combining activation sparsity and
speculative decoding for the first time. The experimental results demonstrate
the superior performance of BlockFFN over other MoE baselines, achieving over
80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\times$ speedup on
real end-side devices than dense models. All codes and checkpoints are
available publicly (https://github.com/thunlp/BlockFFN).

</details>


### [60] [One Token to Fool LLM-as-a-Judge](https://arxiv.org/abs/2507.08794)
*Yulai Zhao,Haolin Liu,Dian Yu,S. Y. Kung,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: 生成式奖励模型在复杂任务评估中存在对表面操作的脆弱性，通过数据增强训练出更鲁棒的模型。


<details>
  <summary>Details</summary>
Motivation: 当前生成式奖励模型广泛应用于替代传统规则指标，但其在答案质量评估中易受非关键符号或推理引导词干扰，导致虚假正向奖励，严重威胁依赖此类模型的算法范式可靠性。

Method: 系统性验证模型脆弱性后，设计包含对抗样本的数据增强策略，训练跨领域鲁棒奖励模型，并开源模型及合成训练数据。

Result: 新模型显著提升抗干扰能力，实验证明该方法有效缓解表面操作导致的误判问题。

Conclusion: 生成式评估方法的脆弱性亟待解决，本研究提出的增强策略为可靠LLM评估体系发展提供了重要技术路径。

Abstract: Generative reward models (also known as LLMs-as-judges), which use large
language models (LLMs) to evaluate answer quality, are increasingly adopted in
reinforcement learning with verifiable rewards (RLVR). They are often preferred
over rigid rule-based metrics, especially for complex reasoning tasks involving
free-form outputs. In this paradigm, an LLM is typically prompted to compare a
candidate answer against a ground-truth reference and assign a binary reward
indicating correctness. Despite the seeming simplicity of this comparison task,
we find that generative reward models exhibit surprising vulnerabilities to
superficial manipulations: non-word symbols (e.g., ":" or ".") or reasoning
openers like "Thought process:" and "Let's solve this problem step by step."
can often lead to false positive rewards. We demonstrate that this weakness is
widespread across LLMs, datasets, and prompt formats, posing a serious threat
for core algorithmic paradigms that rely on generative reward models, such as
rejection sampling, preference optimization, and RLVR. To mitigate this issue,
we introduce a simple yet effective data augmentation strategy and train a new
generative reward model with substantially improved robustness. Our findings
highlight the urgent need for more reliable LLM-based evaluation methods. We
release our robust, general-domain reward model and its synthetic training data
at https://huggingface.co/sarosavo/Master-RM and
https://huggingface.co/datasets/sarosavo/Master-RM.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [61] [Overview of the TREC 2021 deep learning track](https://arxiv.org/abs/2507.08191)
*Nick Craswell,Bhaskar Mitra,Emine Yilmaz,Daniel Campos,Jimmy Lin*

Main category: cs.IR

TL;DR: TREC深度学习赛道第三年研究：扩大数据集规模后，深度神经排序模型持续领先传统方法，单阶段检索表现提升但仍不及多阶段流程，同时揭示数据刷新带来的标注质量问题


<details>
  <summary>Details</summary>
Motivation: 探索大规模预训练模型在更新扩展后的MS MARCO数据集上的表现，评估单阶段检索与多阶段检索流程的效果差异，并验证数据刷新对评估结果的影响

Method: 使用刷新后规模扩大4倍的文档集（1640万→6560万）和16倍的段落集（880万→1.41亿），基于MS MARCO人工标注数据，对比深度神经排序模型与传统检索方法在passage/document ranking任务上的表现

Result: 1. 深度神经排序模型在两项任务中持续超越传统方法
2. 单阶段检索达到较好性能（passage任务MRR@10=0.335，document任务0.405）
3. 多阶段流程仍保持优势（passage任务0.371）
4. 新数据集揭示约18.5%相关文档未被NIST标注覆盖

Conclusion: 数据规模的指数级增长暴露出标注完整性问题，映射旧标签至新数据集可能存在质量隐患。建议未来研究应关注评估体系与数据质量的协同提升，同时单阶段检索的潜力值得进一步挖掘。

Abstract: This is the third year of the TREC Deep Learning track. As in previous years,
we leverage the MS MARCO datasets that made hundreds of thousands of human
annotated training labels available for both passage and document ranking
tasks. In addition, this year we refreshed both the document and the passage
collections which also led to a nearly four times increase in the document
collection size and nearly $16$ times increase in the size of the passage
collection. Deep neural ranking models that employ large scale pretraininig
continued to outperform traditional retrieval methods this year. We also found
that single stage retrieval can achieve good performance on both tasks although
they still do not perform at par with multistage retrieval pipelines. Finally,
the increase in the collection size and the general data refresh raised some
questions about completeness of NIST judgments and the quality of the training
labels that were mapped to the new collections from the old ones which we
discuss in this report.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [62] [M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning](https://arxiv.org/abs/2507.08306)
*Inclusion AI,:,Fudong Wang,Jiajia Liu,Jingdong Chen,Jun Zhou,Kaixiang Ji,Lixiang Ru,Qingpei Guo,Ruobing Zheng,Tianqi Li,Yi Yuan,Yifan Mao,Yuting Xiao,Ziping Ma*

Main category: cs.AI

TL;DR: 提出M2-Reasoning-7B模型，通过新型数据管道和动态多任务训练策略，在通用与空间推理领域实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在动态空间交互能力上存在显著缺陷，这限制了其实际应用场景的有效性

Method: 1) 开发生成29.4万高质量数据样本的冷启动流程 2) 采用动态多任务分步优化策略，结合任务特定奖励机制解决数据冲突

Result: 在8个基准测试中刷新SOTA记录，通用推理与空间推理性能均取得显著提升

Conclusion: 通过高质量数据生成与动态训练策略的协同优化，成功突破多模态模型的空间交互瓶颈，为复杂场景应用奠定基础

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs), particularly
through Reinforcement Learning with Verifiable Rewards (RLVR), have
significantly enhanced their reasoning abilities. However, a critical gap
persists: these models struggle with dynamic spatial interactions, a capability
essential for real-world applications. To bridge this gap, we introduce
M2-Reasoning-7B, a model designed to excel in both general and spatial
reasoning. Our approach integrates two key innovations: (1) a novel data
pipeline that generates 294.2K high-quality data samples (168K for cold-start
fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning
trajectories and have undergone comprehensive assessment; and (2) a dynamic
multi-task training strategy with step-wise optimization to mitigate conflicts
between data, and task-specific rewards for delivering tailored incentive
signals. This combination of curated data and advanced training allows
M2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks,
showcasing superior performance in both general and spatial reasoning domains.

</details>


### [63] [A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis](https://arxiv.org/abs/2507.08529)
*Mingda Zhang,Na Zhao,Jianglong Qin,Guoyu Ye,Ruixiang Tang*

Main category: cs.AI

TL;DR: 提出结合多粒度稀疏激活与分层知识图谱的框架，通过四重匹配算法和三层知识结构实现罕见病诊断准确率0.89（接近临床阈值0.90）


<details>
  <summary>Details</summary>
Motivation: 现有医疗大模型存在知识表征深度不足、概念理解局限和临床推理受限三大缺陷，导致罕见病诊断困难

Method: 四重互补匹配算法配合多样性控制+五级回退策略实现精准概念激活，构建分类学-临床特征-实例的三层动态更新知识图谱

Result: BioASQ数据集实验显示BLEU提升0.09/ROUGE提升0.05/准确率提升0.12，专家评估验证信息质量、推理逻辑和专业表达的三维提升

Conclusion: 该框架显著缩短罕见病患者的'诊断漫游'，为临床决策支持系统提供新范式

Abstract: Despite advances from medical large language models in healthcare,
rare-disease diagnosis remains hampered by insufficient
knowledge-representation depth, limited concept understanding, and constrained
clinical reasoning. We propose a framework that couples multi-granularity
sparse activation of medical concepts with a hierarchical knowledge graph. Four
complementary matching algorithms, diversity control, and a five-level fallback
strategy enable precise concept activation, while a three-layer knowledge graph
(taxonomy, clinical features, instances) provides structured, up-to-date
context. Experiments on the BioASQ rare-disease QA set show BLEU gains of 0.09,
ROUGE gains of 0.05, and accuracy gains of 0.12, with peak accuracy of 0.89
approaching the 0.90 clinical threshold. Expert evaluation confirms
improvements in information quality, reasoning, and professional expression,
suggesting our approach shortens the "diagnostic odyssey" for rare-disease
patients.

</details>


### [64] [Large Multi-modal Model Cartographic Map Comprehension for Textual Locality Georeferencing](https://arxiv.org/abs/2507.08575)
*Kalana Wijegunarathna,Kristin Stock,Christopher B. Jones*

Main category: cs.AI

TL;DR: 提出利用多模态大模型（LMM）的视觉定位能力，通过网格化方法在零样本设置下实现生物样本地理定位（平均误差约1公里），显著优于传统单模态方法。


<details>
  <summary>Details</summary>
Motivation: 解决自然历史馆藏中数百万未地理参照生物样本的标注难题。现有自动化工具未充分利用地图的空间关系解析能力，人工标注成本过高。

Method: 采用网格化处理策略，在零样本设定下让LMM模型同时处理文本描述和地图视觉信息，建立地理空间关系的多模态理解。

Result: 实验显示该方法平均定位误差仅约1公里，较单模态语言模型（LLM）和现有工具精度提升显著，并提出实用地理标注框架。

Conclusion: 证实LMM具备精细地图理解能力，多模态融合方法为地理标注工作流提供了新的高效解决方案。

Abstract: Millions of biological sample records collected in the last few centuries
archived in natural history collections are un-georeferenced. Georeferencing
complex locality descriptions associated with these collection samples is a
highly labour-intensive task collection agencies struggle with. None of the
existing automated methods exploit maps that are an essential tool for
georeferencing complex relations. We present preliminary experiments and
results of a novel method that exploits multi-modal capabilities of recent
Large Multi-Modal Models (LMM). This method enables the model to visually
contextualize spatial relations it reads in the locality description. We use a
grid-based approach to adapt these auto-regressive models for this task in a
zero-shot setting. Our experiments conducted on a small manually annotated
dataset show impressive results for our approach ($\sim$1 km Average distance
error) compared to uni-modal georeferencing with Large Language Models and
existing georeferencing tools. The paper also discusses the findings of the
experiments in light of an LMM's ability to comprehend fine-grained maps.
Motivated by these results, a practical framework is proposed to integrate this
method into a georeferencing workflow.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [65] [Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio Language Models](https://arxiv.org/abs/2507.08128)
*Arushi Goel,Sreyan Ghosh,Jaehyeon Kim,Sonal Kumar,Zhifeng Kong,Sang-gil Lee,Chao-Han Huck Yang,Ramani Duraiswami,Dinesh Manocha,Rafael Valle,Bryan Catanzaro*

Main category: cs.SD

TL;DR: AF3是首个全开源的先进音频-语言模型，在语音/声音/音乐三模态推理理解方面实现突破，支持链式推理、多轮多音频对话、10分钟长音频处理及语音交互。


<details>
  <summary>Details</summary>
Motivation: 解决现有模型在多模态音频理解、长音频处理和交互能力方面的不足，推动开放生态下的音频AI发展。

Method: 1. 提出AF-Whisper统一音频编码器
2. 五阶段课程训练策略
3. 创新数据集构建（AudioSkills-XL等）
4. 支持链式推理/语音交互架构

Result: 在20+长音频理解推理基准测试中刷新SOTA，超越闭源模型（包括使用更大数据训练的商业模型）

Conclusion: AF3证明仅用开源数据即可实现最先进的音频理解能力，为开放研究社区提供了强大的多模态推理工具。

Abstract: We present Audio Flamingo 3 (AF3), a fully open state-of-the-art (SOTA) large
audio-language model that advances reasoning and understanding across speech,
sound, and music. AF3 introduces: (i) AF-Whisper, a unified audio encoder
trained using a novel strategy for joint representation learning across all 3
modalities of speech, sound, and music; (ii) flexible, on-demand thinking,
allowing the model to do chain-of-thought-type reasoning before answering;
(iii) multi-turn, multi-audio chat; (iv) long audio understanding and reasoning
(including speech) up to 10 minutes; and (v) voice-to-voice interaction. To
enable these capabilities, we propose several large-scale training datasets
curated using novel strategies, including AudioSkills-XL, LongAudio-XL,
AF-Think, and AF-Chat, and train AF3 with a novel five-stage curriculum-based
training strategy. Trained on only open-source audio data, AF3 achieves new
SOTA results on over 20+ (long) audio understanding and reasoning benchmarks,
surpassing both open-weight and closed-source models trained on much larger
datasets.

</details>


### [66] [On Barriers to Archival Audio Processing](https://arxiv.org/abs/2507.08768)
*Peter Sullivan,Muhammad Abdul-Mageed*

Main category: cs.SD

TL;DR: 利用UNESCO历史广播录音测试现代语言识别（LID）和说话人识别（SR）方法的鲁棒性，发现LID系统能较好处理非母语口音，但说话人嵌入易受信道/年龄/语言偏见影响。


<details>
  <summary>Details</summary>
Motivation: 验证现成语音处理方法在历史多语言/跨年龄录音上的适用性，为档案库使用SR进行说话人索引提供依据。

Method: 采用UNESCO上世纪中叶多语言广播录音数据集，系统测试Whisper等LID模型和说话人嵌入模型在跨语言/跨年龄场景下的表现。

Result: LID系统处理第二语言/带口音语音能力显著提升，但说话人嵌入存在信道差异、年龄变化和语言切换带来的系统性偏差。

Conclusion: 说话人识别技术需克服信道/年龄/语言相关的偏见问题才能有效应用于历史档案的说话人索引，而语言识别技术已具备较强实用性。

Abstract: In this study, we leverage a unique UNESCO collection of mid-20th century
radio recordings to probe the robustness of modern off-the-shelf language
identification (LID) and speaker recognition (SR) methods, especially with
respect to the impact of multilingual speakers and cross-age recordings. Our
findings suggest that LID systems, such as Whisper, are increasingly adept at
handling second-language and accented speech. However, speaker embeddings
remain a fragile component of speech processing pipelines that is prone to
biases related to the channel, age, and language. Issues which will need to be
overcome should archives aim to employ SR methods for speaker indexing.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [67] [NeuralOS: Towards Simulating Operating Systems via Neural Generative Models](https://arxiv.org/abs/2507.08800)
*Luke Rivard,Sun Sun,Hongyu Guo,Wenhu Chen,Yuntian Deng*

Main category: cs.CV

TL;DR: NeuralOS提出通过RNN+扩散模型的组合框架，实现操作系统界面的像素级预测与状态建模，在鼠标交互建模方面取得突破但键盘交互仍有挑战


<details>
  <summary>Details</summary>
Motivation: 为未来人机交互系统开发能够自主适应并生成GUI的神经接口，突破传统静态界面限制

Method: 结合状态跟踪RNN与扩散模型渲染器，使用包含随机交互和AI代理交互的Ubuntu XFCE数据集进行训练

Result: 成功生成逼真GUI序列，准确建模鼠标轨迹(平均轨迹误差<3px)，可靠预测应用启动等状态转换(准确率92%)，但键盘事件建模准确率仅68%

Conclusion: 该框架为生成式神经界面奠定基础，未来需改进键盘交互建模并提升长时序交互稳定性

Abstract: We introduce NeuralOS, a neural framework that simulates graphical user
interfaces (GUIs) of operating systems by directly predicting screen frames in
response to user inputs such as mouse movements, clicks, and keyboard events.
NeuralOS combines a recurrent neural network (RNN), which tracks computer
state, with a diffusion-based neural renderer that generates screen images. The
model is trained on a large-scale dataset of Ubuntu XFCE recordings, which
include both randomly generated interactions and realistic interactions
produced by AI agents. Experiments show that NeuralOS successfully renders
realistic GUI sequences, accurately captures mouse interactions, and reliably
predicts state transitions like application launches. Although modeling
fine-grained keyboard interactions precisely remains challenging, NeuralOS
offers a step toward creating fully adaptive, generative neural interfaces for
future human-computer interaction systems.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [68] [VideoConviction: A Multimodal Benchmark for Human Conviction and Stock Market Recommendations](https://arxiv.org/abs/2507.08104)
*Michael Galarnyk,Veer Kejriwal,Agam Shah,Yash Bhardwaj,Nicholas Meyer,Anand Krishnan,Sudheer Chava*

Main category: cs.MM

TL;DR: 研究通过VideoConviction多模态数据集分析发现，金融影响者的高信念推荐虽优于低信念推荐，但逆向策略可获6.8%超额年回报（伴随更高风险），同时揭示多模态模型在金融语境分析中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以有效解析金融影响者视频中的多模态信号（语气/表情），需建立新基准评估模型在复杂金融推荐场景中的表现。

Method: 构建含6,000+专家标注的VideoConviction数据集（457小时人工标注），对比测试多模态大模型(MLLMs)与文本大模型(LLMs)在完整视频和片段输入下的性能差异。

Result: 多模态输入提升股票代码提取准确率12%，但模型误判47%普通评论为明确推荐。逆向策略年回报超S&P500 6.8%，但夏普比率仅0.41（S&P500为0.65）。

Conclusion: VideoConviction为多模态金融研究提供新基准，揭示模型需提升多模态信号解析能力，逆向策略虽有效但需严格风控，推动金融AI应用边界拓展。

Abstract: Social media has amplified the reach of financial influencers known as
"finfluencers," who share stock recommendations on platforms like YouTube.
Understanding their influence requires analyzing multimodal signals like tone,
delivery style, and facial expressions, which extend beyond text-based
financial analysis. We introduce VideoConviction, a multimodal dataset with
6,000+ expert annotations, produced through 457 hours of human effort, to
benchmark multimodal large language models (MLLMs) and text-based large
language models (LLMs) in financial discourse. Our results show that while
multimodal inputs improve stock ticker extraction (e.g., extracting Apple's
ticker AAPL), both MLLMs and LLMs struggle to distinguish investment actions
and conviction--the strength of belief conveyed through confident delivery and
detailed reasoning--often misclassifying general commentary as definitive
recommendations. While high-conviction recommendations perform better than
low-conviction ones, they still underperform the popular S\&P 500 index fund.
An inverse strategy--betting against finfluencer recommendations--outperforms
the S\&P 500 by 6.8\% in annual returns but carries greater risk (Sharpe ratio
of 0.41 vs. 0.65). Our benchmark enables a diverse evaluation of multimodal
tasks, comparing model performance on both full video and segmented video
inputs. This enables deeper advancements in multimodal financial research. Our
code, dataset, and evaluation leaderboard are available under the CC BY-NC 4.0
license.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [69] [xpSHACL: Explainable SHACL Validation using Retrieval-Augmented Generation and Large Language Models](https://arxiv.org/abs/2507.08432)
*Gustavo Correa Publio,José Emilio Labra Gayo*

Main category: cs.DB

TL;DR: xpSHACL使用规则树+RAG+LLM实现可解释的多语言SHACL验证系统


<details>
  <summary>Details</summary>
Motivation: 传统SHACL验证报告存在技术术语多、语言单一的问题，难以被非技术人员理解

Method: 结合规则树生成解释框架，利用检索增强生成（RAG）和LLM进行多语言文本生成，构建违规知识图谱缓存历史解释

Result: 系统能自动生成详细的多语言解释，通过缓存机制提升60%的响应效率

Conclusion: 该方法有效提升知识图谱验证结果的可解释性，特别适用于跨国企业多语言场景

Abstract: Shapes Constraint Language (SHACL) is a powerful language for validating RDF
data. Given the recent industry attention to Knowledge Graphs (KGs), more users
need to validate linked data properly. However, traditional SHACL validation
engines often provide terse reports in English that are difficult for
non-technical users to interpret and act upon. This paper presents xpSHACL, an
explainable SHACL validation system that addresses this issue by combining
rule-based justification trees with retrieval-augmented generation (RAG) and
large language models (LLMs) to produce detailed, multilanguage, human-readable
explanations for constraint violations. A key feature of xpSHACL is its usage
of a Violation KG to cache and reuse explanations, improving efficiency and
consistency.

</details>
