<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 128]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.CV](#cs.CV) [Total: 14]
- [cs.IR](#cs.IR) [Total: 7]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.LG](#cs.LG) [Total: 12]
- [cs.AI](#cs.AI) [Total: 14]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [BR-TaxQA-R: A Dataset for Question Answering with References for Brazilian Personal Income Tax Law, including case law](https://arxiv.org/abs/2505.15916)
*Juvenal Domingos Júnior,Augusto Faria,E. Seiti de Oliveira,Erick de Brito,Matheus Teotonio,Andre Assumpção,Diedre Carmo,Roberto Lotufo,Jayr Pereira*

Main category: cs.CL

TL;DR: 本文介绍BR-TaxQA-R数据集，用于巴西个人所得税法问答系统，开发了基于RAG的解决方案。结果显示定制RAG在响应相关性优于商业模型，但事实正确性和流畅性稍逊，强调人类专家评估的必要性。


<details>
  <summary>Details</summary>
Motivation: 税务领域的高风险性质要求AI生成答案具备法律有效性，现有商业工具在专业法律场景中可能存在法律依据不足的问题，需构建专业数据集和评估体系。

Method: 1. 收集715个官方税务问题构建数据集
2. 使用OpenAI嵌入实现RAG流程
3. 对比不同文本分割策略
4. 采用RAGAS指标对比GPT-4o-mini与商业系统

Result: 定制RAG系统响应相关性(89.2%)优于商业模型，但事实正确性(76.5% vs 84.1%)和流畅性(81.3% vs 88.7%)低于商业模型。显示法律依据与语言流畅性间的权衡关系。

Conclusion: 在税务等高风险领域，AI系统需保持法律依据的严谨性，同时必须结合人类专家验证来确保法律有效性，当前技术尚无法完全替代专业法律判断。

Abstract: This paper presents BR-TaxQA-R, a novel dataset designed to support question
answering with references in the context of Brazilian personal income tax law.
The dataset contains 715 questions from the 2024 official Q\&A document
published by Brazil's Internal Revenue Service, enriched with statutory norms
and administrative rulings from the Conselho Administrativo de Recursos Fiscais
(CARF). We implement a Retrieval-Augmented Generation (RAG) pipeline using
OpenAI embeddings for searching and GPT-4o-mini for answer generation. We
compare different text segmentation strategies and benchmark our system against
commercial tools such as ChatGPT and Perplexity.ai using RAGAS-based metrics.
Results show that our custom RAG pipeline outperforms commercial systems in
Response Relevancy, indicating stronger alignment with user queries, while
commercial models achieve higher scores in Factual Correctness and fluency.
These findings highlight a trade-off between legally grounded generation and
linguistic fluency. Crucially, we argue that human expert evaluation remains
essential to ensure the legal validity of AI-generated answers in high-stakes
domains such as taxation. BR-TaxQA-R is publicly available at
https://huggingface.co/datasets/unicamp-dl/BR-TaxQA-R.

</details>


### [2] [Extracting Probabilistic Knowledge from Large Language Models for Bayesian Network Parameterization](https://arxiv.org/abs/2505.15918)
*Aliakbar Nafar,Kristen Brent Venable,Zijun Cui,Parisa Kordjamshidi*

Main category: cs.CL

TL;DR: 利用大语言模型提取概率知识构建贝叶斯网络，结合少量实际数据优化概率分布，显著降低系统偏差


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在生成现实世界事件概率知识方面的潜力，解决贝叶斯网络参数化时领域建模的需求

Method: 在80个公开贝叶斯网络上测试，通过不同提示策略提取条件概率，与随机分布、均匀分布等基线方法对比

Result: LLM生成的条件概率显著优于基线方法，结合小样本数据可减少58%系统偏差，建立首个概率知识评估基准

Conclusion: 提出LLM+小数据的贝叶斯网络自动构建新范式，证实LLMs作为概率知识源的实用价值

Abstract: Large Language Models (LLMs) have demonstrated potential as factual knowledge
bases; however, their capability to generate probabilistic knowledge about
real-world events remains understudied. This paper investigates using
probabilistic knowledge inherent in LLMs to derive probability estimates for
statements concerning events and their interrelationships captured via a
Bayesian Network (BN). Using LLMs in this context allows for the
parameterization of BNs, enabling probabilistic modeling within specific
domains. Experiments on eighty publicly available Bayesian Networks, from
healthcare to finance, demonstrate that querying LLMs about the conditional
probabilities of events provides meaningful results when compared to baselines,
including random and uniform distributions, as well as approaches based on
next-token generation probabilities. We explore how these LLM-derived
distributions can serve as expert priors to refine distributions extracted from
minimal data, significantly reducing systematic biases. Overall, this work
introduces a promising strategy for automatically constructing Bayesian
Networks by combining probabilistic knowledge extracted from LLMs with small
amounts of real-world data. Additionally, we evaluate several prompting
strategies for eliciting probabilistic knowledge from LLMs and establish the
first comprehensive baseline for assessing LLM performance in extracting
probabilistic knowledge.

</details>


### [3] [Aligning Dialogue Agents with Global Feedback via Large Language Model Reward Decomposition](https://arxiv.org/abs/2505.15922)
*Dong Won Lee,Hae Won Park,Cynthia Breazeal,Louis-Philippe Morency*

Main category: cs.CL

TL;DR: 利用大型语言模型分解会话级反馈为细粒度奖励，无需人工设计或细粒度反馈即可提升对话代理对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手动奖励设计或细粒度人工反馈，本文旨在通过LLM自动分解全局反馈，减少人工干预。

Method: 提出纯文本和多模态两种奖励分解变体，将分解后的奖励蒸馏为轻量模型，用于强化学习微调对话生成。

Result: 在对话质量的人类评估中超越现有方法，验证LLM作为高效奖励分解器的能力。

Conclusion: LLM可替代人工奖励设计，多模态线索增强分解效果，为对话代理对齐提供高效解决方案。

Abstract: We propose a large language model based reward decomposition framework for
aligning dialogue agents using only a single session-level feedback signal. We
leverage the reasoning capabilities of a frozen, pretrained large language
model (LLM) to infer fine-grained local implicit rewards by decomposing global,
session-level feedback. Our first text-only variant prompts the LLM to perform
reward decomposition using only the dialogue transcript. The second multimodal
variant incorporates additional behavioral cues, such as pitch, gaze, and
facial affect, expressed as natural language descriptions. These inferred
turn-level rewards are distilled into a lightweight reward model, which we
utilize for RL-based fine-tuning for dialogue generation. We evaluate both
text-only and multimodal variants against state-of-the-art reward decomposition
methods and demonstrate notable improvements in human evaluations of
conversation quality, suggesting that LLMs are strong reward decomposers that
obviate the need for manual reward shaping and granular human feedback.

</details>


### [4] [Citation Parsing and Analysis with Language Models](https://arxiv.org/abs/2505.15948)
*Parth Sarin,Juan Pablo Alperin*

Main category: cs.CL

TL;DR: 提出使用开放权重语言模型高效解析学术引用，提升引文网络准确性以促进全球学术公平


<details>
  <summary>Details</summary>
Motivation: 解决全球南方知识共享网络信息缺失导致的学术边缘化问题，打破索引服务中的殖民化结构不平等

Method: 构建预印本与已发表论文的引用数据集，系统评估多种开放权重语言模型的引用标注性能

Result: 现有语言模型在引用解析任务中超越传统方法，Qwen3-0.6B小模型通过多次处理实现高效精准解析

Conclusion: 该工具将提升引文网络保真度，推动研究索引系统革新，并为元科学研究提供新的方法论基础

Abstract: A key type of resource needed to address global inequalities in knowledge
production and dissemination is a tool that can support journals in
understanding how knowledge circulates. The absence of such a tool has resulted
in comparatively less information about networks of knowledge sharing in the
Global South. In turn, this gap authorizes the exclusion of researchers and
scholars from the South in indexing services, reinforcing colonial arrangements
that de-center and minoritize those scholars. In order to support citation
network tracking on a global scale, we investigate the capacity of open-weight
language models to mark up manuscript citations in an indexable format. We
assembled a dataset of matched plaintext and annotated citations from preprints
and published research papers. Then, we evaluated a number of open-weight
language models on the annotation task. We find that, even out of the box,
today's language models achieve high levels of accuracy on identifying the
constituent components of each citation, outperforming state-of-the-art
methods. Moreover, the smallest model we evaluated, Qwen3-0.6B, can parse all
fields with high accuracy in $2^5$ passes, suggesting that post-training is
likely to be effective in producing small, robust citation parsing models. Such
a tool could greatly improve the fidelity of citation networks and thus
meaningfully improve research indexing and discovery, as well as further
metascientific research.

</details>


### [5] [Training Step-Level Reasoning Verifiers with Formal Verification Tools](https://arxiv.org/abs/2505.15960)
*Ryo Kamoi,Yusen Zhang,Nan Zhang,Sarkar Snigdha Sarathi Das,Rui Zhang*

Main category: cs.CL

TL;DR: 提出FoVer方法，利用形式化验证工具自动标注推理错误，训练出具备跨任务泛化能力的流程奖励模型


<details>
  <summary>Details</summary>
Motivation: 现有流程奖励模型依赖昂贵人工标注且局限于数学推理任务，需解决自动数据集构建和模型泛化问题

Method: 使用Z3/Isabelle等工具自动标注形式逻辑和定理证明任务中的推理错误，合成训练数据集并训练LLM-based PRMs

Result: 在12个推理基准测试中显著超越基线模型，与人工标注的SOTA模型性能相当或更优（MATH/AIME/ANLI等）

Conclusion: 基于形式化验证的自动标注能有效解决数据构建难题，训练出的PRMs展现出跨推理任务的泛化能力

Abstract: Process Reward Models (PRMs), which provide step-by-step feedback on the
reasoning generated by Large Language Models (LLMs), are receiving increasing
attention. However, two key research gaps remain: collecting accurate
step-level error labels for training typically requires costly human
annotation, and existing PRMs are limited to math reasoning problems. In
response to these gaps, this paper aims to address the challenges of automatic
dataset creation and the generalization of PRMs to diverse reasoning tasks. To
achieve this goal, we propose FoVer, an approach for training PRMs on
step-level error labels automatically annotated by formal verification tools,
such as Z3 for formal logic and Isabelle for theorem proof, which provide
automatic and accurate verification for symbolic tasks. Using this approach, we
synthesize a training dataset with error labels on LLM responses for formal
logic and theorem proof tasks without human annotation. Although this data
synthesis is feasible only for tasks compatible with formal verification, we
observe that LLM-based PRMs trained on our dataset exhibit cross-task
generalization, improving verification across diverse reasoning tasks.
Specifically, PRMs trained with FoVer significantly outperform baseline PRMs
based on the original LLMs and achieve competitive or superior results compared
to state-of-the-art PRMs trained on labels annotated by humans or stronger
models, as measured by step-level verification on ProcessBench and Best-of-K
performance across 12 reasoning benchmarks, including MATH, AIME, ANLI, MMLU,
and BBH. The datasets, models, and code are provided at
https://github.com/psunlpgroup/FoVer.

</details>


### [6] [Pre-training Large Memory Language Models with Internal and External Knowledge](https://arxiv.org/abs/2505.15962)
*Linxi Zhao,Sofian Zalouk,Christian K. Belardi,Justin Lovelace,Jin Peng Zhou,Kilian Q. Weinberger,Yoav Artzi,Jennifer J. Sun*

Main category: cs.CL

TL;DR: 提出新型大记忆语言模型(LMLM)，将知识存储从参数权重解耦至外部数据库，实现可验证、可编辑的知识管理框架


<details>
  <summary>Details</summary>
Motivation: 传统神经语言模型以黑箱形式混合存储语言模式和事实知识，导致知识难以追溯、验证和更新。通过分离存储机制提升模型透明度与知识可控性

Method: 预训练阶段对检索获得的事实值进行目标性掩码，强制模型建立外部知识检索机制而非依赖参数记忆。建立参数权重与外部数据库协同工作的架构

Result: 在标准基准测试中达到与参数量大3倍的知识密集模型相当的精度，同时保持知识存储的显式性、可编辑性和可验证性优势

Conclusion: 该研究颠覆了语言模型处理事实知识的传统范式，为构建可信、可维护的知识增强型语言模型开辟了新方向

Abstract: Neural language models are black-boxes -- both linguistic patterns and
factual knowledge are distributed across billions of opaque parameters. This
entangled encoding makes it difficult to reliably inspect, verify, or update
specific facts. We propose a new class of language models, Large Memory
Language Models (LMLM) with a pre-training recipe that stores factual knowledge
in both internal weights and an external database. Our approach strategically
masks externally retrieved factual values from the training loss, thereby
teaching the model to perform targeted lookups rather than relying on
memorization in model weights. Our experiments demonstrate that LMLMs achieve
competitive performance compared to significantly larger, knowledge-dense LLMs
on standard benchmarks, while offering the advantages of explicit, editable,
and verifiable knowledge bases. This work represents a fundamental shift in how
language models interact with and manage factual knowledge.

</details>


### [7] [Explaining Puzzle Solutions in Natural Language: An Exploratory Study on 6x6 Sudoku](https://arxiv.org/abs/2505.15993)
*Anirudh Maiya,Razan Alghamdi,Maria Leonor Pacheco,Ashutosh Trivedi,Fabio Somenzi*

Main category: cs.CL

TL;DR: 大语言模型在数独解题协作中展示解释性推理短板


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在人类-AI协同决策中提供可信、渐进、定制化解释的能力，以数独解题为典型场景说明解释过程比最终答案更重要

Method: 评估5个LLM模型在解六宫格数独及解释解题过程的表现

Result: 1个模型展现有限解题能力，所有模型均无法呈现战略推理或直觉式解题过程的解释

Conclusion: LLMs要成为有效决策伙伴需克服解释性推理的重大挑战

Abstract: The success of Large Language Models (LLMs) in human-AI collaborative
decision-making hinges on their ability to provide trustworthy, gradual, and
tailored explanations. Solving complex puzzles, such as Sudoku, offers a
canonical example of this collaboration, where clear and customized
explanations often hold greater importance than the final solution. In this
study, we evaluate the performance of five LLMs in solving and explaining
\sixsix{} Sudoku puzzles. While one LLM demonstrates limited success in solving
puzzles, none can explain the solution process in a manner that reflects
strategic reasoning or intuitive problem-solving. These findings underscore
significant challenges that must be addressed before LLMs can become effective
partners in human-AI collaborative decision-making.

</details>


### [8] [Leveraging Online Data to Enhance Medical Knowledge in a Small Persian Language Model](https://arxiv.org/abs/2505.16000)
*Mehrdad ghassabi,Pedram Rostami,Hamidreza Baradaran Kashani,Amirhossein Poursina,Zahra Kazemi,Milad Tavakoli*

Main category: cs.CL

TL;DR: 通过整合波斯语开放医疗数据增强小型语言模型的医学知识，实现资源受限环境下的医疗AI应用


<details>
  <summary>Details</summary>
Motivation: 波斯语等低资源语言在医学领域缺乏高质量数据集，现有小型语言模型难以处理专业领域问题

Method: 1. 构建首个波斯语医疗数据集（医学杂志语料库+真实医患问答）
2. 在基线模型上进行针对性微调

Result: 微调后的模型在医学问答准确率上提升，生成响应质量优于基线模型

Conclusion: 验证了利用开放网络数据增强小语言模型医学能力的可行性，为波斯语医疗AI提供了经济高效的解决方案

Abstract: The rapid advancement of language models has demonstrated the potential of
artificial intelligence in the healthcare industry. However, small language
models struggle with specialized domains in low-resource languages like
Persian. While numerous medical-domain websites exist in Persian, no curated
dataset or corpus has been available making ours the first of its kind. This
study explores the enhancement of medical knowledge in a small language model
by leveraging accessible online data, including a crawled corpus from medical
magazines and a dataset of real doctor-patient QA pairs. We fine-tuned a
baseline model using our curated data to improve its medical knowledge.
Benchmark evaluations demonstrate that the fine-tuned model achieves improved
accuracy in medical question answering and provides better responses compared
to its baseline. This work highlights the potential of leveraging open-access
online data to enrich small language models in medical fields, providing a
novel solution for Persian medical AI applications suitable for
resource-constrained environments.

</details>


### [9] [Causal Interventions Reveal Shared Structure Across English Filler-Gap Constructions](https://arxiv.org/abs/2505.16002)
*Sasha Boguraev,Christopher Potts,Kyle Mahowald*

Main category: cs.CL

TL;DR: LLM的因果可解释性方法可推动语言学理论发展，通过分布式干预实验发现语言模型对英语填充-缺口结构具有统一抽象分析模式


<details>
  <summary>Details</summary>
Motivation: 验证语言模型的内部机制分析能否推动句法理论发展，探究不同英语填充-缺口结构是否共享底层分析机制

Method: 使用分布式互换干预方法（Distributed Interchange Interventions）对多种英语填充-缺口结构（疑问句/关系从句等）进行LLM机制分析

Result: 1. 模型对不同结构收敛于相似抽象分析
2. 发现频率、填充词类型、语境等被忽视的影响因素
3. 模型内部机制分析可修正现有语言学理论

Conclusion: 语言模型的机械论内部机制分析为语言学理论发展提供新范式，揭示传统理论忽略的关键语言认知因素

Abstract: Large Language Models (LLMs) have emerged as powerful sources of evidence for
linguists seeking to develop theories of syntax. In this paper, we argue that
causal interpretability methods, applied to LLMs, can greatly enhance the value
of such evidence by helping us characterize the abstract mechanisms that LLMs
learn to use. Our empirical focus is a set of English filler-gap dependency
constructions (e.g., questions, relative clauses). Linguistic theories largely
agree that these constructions share many properties. Using experiments based
in Distributed Interchange Interventions, we show that LLMs converge on similar
abstract analyses of these constructions. These analyses also reveal previously
overlooked factors -- relating to frequency, filler type, and surrounding
context -- that could motivate changes to standard linguistic theory. Overall,
these results suggest that mechanistic, internal analyses of LLMs can push
linguistic theory forward.

</details>


### [10] [SLMEval: Entropy-Based Calibration for Human-Aligned Evaluation of Large Language Models](https://arxiv.org/abs/2505.16003)
*Roland Daynauth,Christopher Clarke,Krisztian Flautner,Lingjia Tang,Jason Mars*

Main category: cs.CL

TL;DR: LLM-as-a-Judge评估范式在现实开放任务中存在校准失效问题，提出的SLMEval方法通过熵最大化校准，显著提升与人类判断相关性并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有校准方法在结构化基准表现良好，但在开放任务中与人类判断相关性弱甚至负相关，需更高效、经济的解决方案。

Method: 基于少量人类偏好数据，通过估计模型质量的潜在分布并重新加权评分，实现熵最大化校准。

Result: SLMEval在真实用例中Spearman相关系数达0.57（对比G-Eval的负相关），成本比GPT-4校准方法降低5-30倍。

Conclusion: SLMEval有效解决开放任务评估偏差，兼具高相关性与经济性，适用于实际生产场景。

Abstract: The LLM-as-a-Judge paradigm offers a scalable, reference-free approach for
evaluating language models. Although several calibration techniques have been
proposed to better align these evaluators with human judgment, prior studies
focus primarily on narrow, well-structured benchmarks. As a result, it remains
unclear whether such calibrations generalize to real-world, open-ended tasks.
  In this work, we show that SOTA calibrated evaluators often fail in these
settings, exhibiting weak or even negative correlation with human judgments. To
address this, we propose SLMEval, a novel and efficient calibration method
based on entropy maximization over a small amount of human preference data. By
estimating a latent distribution over model quality and reweighting evaluator
scores accordingly, SLMEval achieves strong correlation with human evaluations
across two real-world production use cases and the public benchmark. For
example, on one such task, SLMEval achieves a Spearman correlation of 0.57 with
human judgments, while G-Eval yields a negative correlation. In addition,
SLMEval reduces evaluation costs by 5-30x compared to GPT-4-based calibrated
evaluators such as G-eval.

</details>


### [11] [LAGO: Few-shot Crosslingual Embedding Inversion Attacks via Language Similarity-Aware Graph Optimization](https://arxiv.org/abs/2505.16008)
*Wenrui Yu,Yiyi Chen,Johannes Bjerva,Sokol Kosta,Qiongxiu Li*

Main category: cs.CL

TL;DR: 提出LAGO图优化框架，通过建模语言相似性实现少样本跨语言嵌入反转攻击，Rouge-L提升10-20%


<details>
  <summary>Details</summary>
Motivation: 现有嵌入反转攻击方法忽略语言间相似性，导致攻击迁移性不足。需开发语言感知的隐私保护方法

Method: 基于图约束的分布式优化框架，整合句法/词汇相似性作为边约束，结合Frobenius正则化与总变差约束

Result: 在10样本/语言的极端条件下，跨多语言实验显示攻击迁移性显著提升(Rouge-L +10-20%)

Conclusion: 语言相似性是攻击迁移性的关键因素，呼吁开发语言感知的隐私保护嵌入方法

Abstract: We propose LAGO - Language Similarity-Aware Graph Optimization - a novel
approach for few-shot cross-lingual embedding inversion attacks, addressing
critical privacy vulnerabilities in multilingual NLP systems. Unlike prior work
in embedding inversion attacks that treat languages independently, LAGO
explicitly models linguistic relationships through a graph-based constrained
distributed optimization framework. By integrating syntactic and lexical
similarity as edge constraints, our method enables collaborative parameter
learning across related languages. Theoretically, we show this formulation
generalizes prior approaches, such as ALGEN, which emerges as a special case
when similarity constraints are relaxed. Our framework uniquely combines
Frobenius-norm regularization with linear inequality or total variation
constraints, ensuring robust alignment of cross-lingual embedding spaces even
with extremely limited data (as few as 10 samples per language). Extensive
experiments across multiple languages and embedding models demonstrate that
LAGO substantially improves the transferability of attacks with 10-20% increase
in Rouge-L score over baselines. This work establishes language similarity as a
critical factor in inversion attack transferability, urging renewed focus on
language-aware privacy-preserving multilingual embeddings.

</details>


### [12] [Ranking Free RAG: Replacing Re-ranking with Selection in RAG for Sensitive Domains](https://arxiv.org/abs/2505.16014)
*Yash Saxena,Anpur Padia,Mandar S Chaudhary,Kalpa Gunaratna,Srinivasan Parthasarathy,Manas Gaur*

Main category: cs.CL

TL;DR: 提出METEORA框架替代传统RAG的相似性重排序方法，通过rationale驱动证据选择机制实现更准确的生成，对抗攻击下F1分数提升340%


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成方法依赖启发式相似度排序，存在可解释性差、抗对抗攻击能力弱的问题

Method: 两阶段框架：1）预训练LLM生成rationale指导证据选择；2）三阶段选择（本地关联/全局肘点检测/上下文扩展）配合验证器过滤恶意内容

Result: 在6个跨领域数据集上实现33.34%的准确率提升，数据块使用量减少50%，对抗场景F1从0.10提升至0.44

Conclusion: 该框架通过可解释的rationale流实现了高效安全的内容生成，在准确性和抗攻击性上显著超越现有方法

Abstract: Traditional Retrieval-Augmented Generation (RAG) pipelines rely on
similarity-based retrieval and re-ranking, which depend on heuristics such as
top-k, and lack explainability, interpretability, and robustness against
adversarial content. To address this gap, we propose a novel method METEORA
that replaces re-ranking in RAG with a rationale-driven selection approach.
METEORA operates in two stages. First, a general-purpose LLM is
preference-tuned to generate rationales conditioned on the input query using
direct preference optimization. These rationales guide the evidence chunk
selection engine, which selects relevant chunks in three stages: pairing
individual rationales with corresponding retrieved chunks for local relevance,
global selection with elbow detection for adaptive cutoff, and context
expansion via neighboring chunks. This process eliminates the need for top-k
heuristics. The rationales are also used for consistency check using a Verifier
LLM to detect and filter poisoned or misleading content for safe generation.
The framework provides explainable and interpretable evidence flow by using
rationales consistently across both selection and verification. Our evaluation
across six datasets spanning legal, financial, and academic research domains
shows that METEORA improves generation accuracy by 33.34% while using
approximately 50% fewer chunks than state-of-the-art re-ranking methods. In
adversarial settings, METEORA significantly improves the F1 score from 0.10 to
0.44 over the state-of-the-art perplexity-based defense baseline, demonstrating
strong resilience to poisoning attacks. Code available at:
https://anonymous.4open.science/r/METEORA-DC46/README.md

</details>


### [13] [NOVER: Incentive Training for Language Models via Verifier-Free Reinforcement Learning](https://arxiv.org/abs/2505.16022)
*Wei Liu,Siya Qi,Xinyu Wang,Chen Qian,Yali Du,Yulan He*

Main category: cs.CL

TL;DR: 提出NOVER强化学习框架，无需外部验证器即可在文本任务中实现激励训练，性能超越同规模模型7.7%


<details>
  <summary>Details</summary>
Motivation: 现有激励训练方法依赖外部验证器（如数学/编程领域专用验证器或需高成本训练奖励模型），限制了通用文本任务的应用场景

Method: 仅使用标准监督微调数据，通过动态调整推理路径置信度的创新机制替代外部验证器

Result: 在广泛文本任务中表现优异，超越同规模模型7.7%；支持逆向激励训练等新优化范式

Conclusion: NOVER突破了验证器依赖瓶颈，为LLM优化开辟了通用性强、成本更低的新方向

Abstract: Recent advances such as DeepSeek R1-Zero highlight the effectiveness of
incentive training, a reinforcement learning paradigm that computes rewards
solely based on the final answer part of a language model's output, thereby
encouraging the generation of intermediate reasoning steps. However, these
methods fundamentally rely on external verifiers, which limits their
applicability to domains like mathematics and coding where such verifiers are
readily available. Although reward models can serve as verifiers, they require
high-quality annotated data and are costly to train. In this work, we propose
NOVER, NO-VERifier Reinforcement Learning, a general reinforcement learning
framework that requires only standard supervised fine-tuning data with no need
for an external verifier. NOVER enables incentive training across a wide range
of text-to-text tasks and outperforms the model of the same size distilled from
large reasoning models such as DeepSeek R1 671B by 7.7 percent. Moreover, the
flexibility of NOVER enables new possibilities for optimizing large language
models, such as inverse incentive training.

</details>


### [14] [Prototypical Human-AI Collaboration Behaviors from LLM-Assisted Writing in the Wild](https://arxiv.org/abs/2505.16023)
*Sheshera Mysore,Debarati Das,Hancheng Cao,Bahareh Sarrafzadeh*

Main category: cs.CL

TL;DR: 研究通过分析用户与LLM协作的写作行为，发现少量典型行为模式(PATHs)主导交互变化，写作意图与协作行为存在显著关联，对LLM对齐有重要启示。


<details>
  <summary>Details</summary>
Motivation: 探究用户在复杂写作任务中如何主动引导LLM生成内容，突破传统任务分类框架，揭示人机协作的动态模式。

Method: 基于Bing Copilot和WildChat的真实用户交互数据，采用行为模式聚类与统计相关性分析。

Result: 1. 5种核心PATHs解释78%交互差异
2. 写作意图与特定PATHs强相关(p<0.01)
3. 用户主要采用意图修正(34%)和内容注入(22%)策略

Conclusion: LLM设计需支持动态意图演化，建立意图-行为响应机制，推动从结果对齐向过程协作对齐的范式转变。

Abstract: As large language models (LLMs) are used in complex writing workflows, users
engage in multi-turn interactions to steer generations to better fit their
needs. Rather than passively accepting output, users actively refine, explore,
and co-construct text. We conduct a large-scale analysis of this collaborative
behavior for users engaged in writing tasks in the wild with two popular AI
assistants, Bing Copilot and WildChat. Our analysis goes beyond simple task
classification or satisfaction estimation common in prior work and instead
characterizes how users interact with LLMs through the course of a session. We
identify prototypical behaviors in how users interact with LLMs in prompts
following their original request. We refer to these as Prototypical Human-AI
Collaboration Behaviors (PATHs) and find that a small group of PATHs explain a
majority of the variation seen in user-LLM interaction. These PATHs span users
revising intents, exploring texts, posing questions, adjusting style or
injecting new content. Next, we find statistically significant correlations
between specific writing intents and PATHs, revealing how users' intents shape
their collaboration behaviors. We conclude by discussing the implications of
our findings on LLM alignment.

</details>


### [15] [OpenEthics: A Comprehensive Ethical Evaluation of Open-Source Generative Large Language Models](https://arxiv.org/abs/2505.16036)
*Burak Erinç Çetin,Yıldırım Özen,Elif Naz Demiryılmaz,Kaan Engür,Cagri Toraman*

Main category: cs.CL

TL;DR: 对29个开源大语言模型进行跨语言伦理评估，发现模型普遍重视安全性与公平性，可靠性存忧，大参数模型表现更优（Gemma/Qwen最佳）


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦单一伦理维度且语言/模型多样性不足，本研究旨在通过覆盖四大伦理维度（鲁棒性/可靠性/安全性/公平性）、英语/土耳其语双语言、29个开源模型的评估框架，填补评估广度、语言覆盖和模型多样性三重空白

Method: 采用LLM-as-a-Judge方法，在英语（常用语言）和土耳其语（低资源语言）环境下，从四大伦理维度系统评估近期29个开源大语言模型

Result: 1. 多数模型优化侧重安全性与公平性
2. 鲁棒性良好但可靠性存在缺陷
3. 模型参数规模与伦理表现正相关（Gemma/Qwen最优）
4. 伦理评估有效性不受语言种类影响

Conclusion: 首次实现多语言环境下的大规模伦理评估，证实伦理评估的跨语言普适性，确立模型规模与伦理性能的正相关性，为开源社区提供安全发展路线图

Abstract: Generative large language models present significant potential but also raise
critical ethical concerns. Most studies focus on narrow ethical dimensions, and
also limited diversity of languages and models. To address these gaps, we
conduct a broad ethical evaluation of 29 recent open-source large language
models using a novel data collection including four ethical aspects:
Robustness, reliability, safety, and fairness. We analyze model behavior in
both a commonly used language, English, and a low-resource language, Turkish.
Our aim is to provide a comprehensive ethical assessment and guide safer model
development by filling existing gaps in evaluation breadth, language coverage,
and model diversity. Our experimental results, based on LLM-as-a-Judge, reveal
that optimization efforts for many open-source models appear to have
prioritized safety and fairness, and demonstrated good robustness while
reliability remains a concern. We demonstrate that ethical evaluation can be
effectively conducted independently of the language used. In addition, models
with larger parameter counts tend to exhibit better ethical performance, with
Gemma and Qwen models demonstrating the most ethical behavior among those
evaluated.

</details>


### [16] [Internal and External Impacts of Natural Language Processing Papers](https://arxiv.org/abs/2505.16061)
*Yu Zhang*

Main category: cs.CL

TL;DR: 语言建模在学术内外影响力最广，语言基础研究影响力较低。不同外部领域对NLP主题存在明显偏好差异，伦理类主题政策关注度显著高于学术引用。


<details>
  <summary>Details</summary>
Motivation: 探究顶级NLP会议论文在学术界内外的差异化影响力，揭示不同研究主题在专利、媒体和政策文件中的传播特征。

Method: 通过跨领域引用分析（1979-2024年），结合学术论文、专利、媒体和政策文档的多源数据，量化评估36个NLP主题的影响力分布。

Result: 1. 语言建模专利引用量达8.6万次，政策文档中伦理主题提及量超学术引用3倍
2. 专利聚焦机器翻译等应用技术，媒体/政策更关注模型社会影响
3. 内部与外部影响力相关系数ρ=0.68，但公平性研究呈现显著偏离

Conclusion: NLP研究的社会影响力呈现领域特异性，学术价值与社会需求存在错位现象，提示需建立更全面的科研评估体系。

Abstract: We investigate the impacts of NLP research published in top-tier conferences
(i.e., ACL, EMNLP, and NAACL) from 1979 to 2024. By analyzing citations from
research articles and external sources such as patents, media, and policy
documents, we examine how different NLP topics are consumed both within the
academic community and by the broader public. Our findings reveal that language
modeling has the widest internal and external influence, while linguistic
foundations have lower impacts. We also observe that internal and external
impacts generally align, but topics like ethics, bias, and fairness show
significant attention in policy documents with much fewer academic citations.
Additionally, external domains exhibit distinct preferences, with patents
focusing on practical NLP applications and media and policy documents engaging
more with the societal implications of NLP models.

</details>


### [17] [Small Language Models in the Real World: Insights from Industrial Text Classification](https://arxiv.org/abs/2505.16078)
*Lujun Li,Lama Sleem,Niccolo' Gentile,Geoffrey Nichil,Radu State*

Main category: cs.CL

TL;DR: 系统评估了小模型在工业场景下的文本分类效果，发现紧凑模型在效率与资源占用方面具备优势但提示质量依赖问题仍存


<details>
  <summary>Details</summary>
Motivation: 大型解码器模型存在推理效率低、GPU资源消耗大且文本分类效果依赖提示质量的问题，需探索小模型的替代潜力

Method: 采用提示工程与监督微调方法，针对邮件分类、法律文档分类和超长学术文本分类三个工业场景进行对比实验

Result: 小模型在VRAM利用效率上表现优异，在特定场景能达到与大型模型相当的分类精度，但提示工程效果波动较大

Conclusion: 证实了紧凑模型本地化部署的可行性，建议未来研究聚焦降低小模型对提示质量的敏感性

Abstract: With the emergence of ChatGPT, Transformer models have significantly advanced
text classification and related tasks. Decoder-only models such as Llama
exhibit strong performance and flexibility, yet they suffer from inefficiency
on inference due to token-by-token generation, and their effectiveness in text
classification tasks heavily depends on prompt quality. Moreover, their
substantial GPU resource requirements often limit widespread adoption. Thus,
the question of whether smaller language models are capable of effectively
handling text classification tasks emerges as a topic of significant interest.
However, the selection of appropriate models and methodologies remains largely
underexplored. In this paper, we conduct a comprehensive evaluation of prompt
engineering and supervised fine-tuning methods for transformer-based text
classification. Specifically, we focus on practical industrial scenarios,
including email classification, legal document categorization, and the
classification of extremely long academic texts. We examine the strengths and
limitations of smaller models, with particular attention to both their
performance and their efficiency in Video Random-Access Memory (VRAM)
utilization, thereby providing valuable insights for the local deployment and
application of compact models in industrial settings.

</details>


### [18] [BiasLab: Toward Explainable Political Bias Detection with Dual-Axis Annotations and Rationale Indicators](https://arxiv.org/abs/2505.16081)
*KMA Solaiman*

Main category: cs.CL

TL;DR: 构建含300篇政治新闻的BiasLab数据集，通过双尺度人工标注和GPT-4o模拟分析偏见特征，支持可解释NLP模型开发。


<details>
  <summary>Details</summary>
Motivation: 解决现有政治偏见检测模型可解释性不足的问题，通过细粒度标注促进透明化、社会感知的NLP系统开发。

Method: 1. 从900篇文档中筛选样本；2. 设计双党派情感标注体系，加入理性指标；3. 采用资格审核+试点优化的众包标注流程；4. 量化标注一致性并与媒体来源偏见对比；5. 用GPT-4o模拟标注进行对比实验；6. 定义感知漂移预测和理由分类任务。

Result: 发现人类标注与媒体偏见存在错位，GPT-4o在右倾内容分类中呈现镜像不对称性，基线模型显示可解释偏见检测存在显著挑战。

Conclusion: BiasLab通过丰富标注体系推动可解释AI发展，公开数据集促进人机交互可解释性研究，为构建社会责任感强的NLP系统提供基础设施。

Abstract: We present BiasLab, a dataset of 300 political news articles annotated for
perceived ideological bias. These articles were selected from a curated
900-document pool covering diverse political events and source biases. Each
article is labeled by crowdworkers along two independent scales, assessing
sentiment toward the Democratic and Republican parties, and enriched with
rationale indicators. The annotation pipeline incorporates targeted worker
qualification and was refined through pilot-phase analysis. We quantify
inter-annotator agreement, analyze misalignment with source-level outlet bias,
and organize the resulting labels into interpretable subsets. Additionally, we
simulate annotation using schema-constrained GPT-4o, enabling direct comparison
to human labels and revealing mirrored asymmetries, especially in
misclassifying subtly right-leaning content. We define two modeling tasks:
perception drift prediction and rationale type classification, and report
baseline performance to illustrate the challenge of explainable bias detection.
BiasLab's rich rationale annotations provide actionable interpretations that
facilitate explainable modeling of political bias, supporting the development
of transparent, socially aware NLP systems. We release the dataset, annotation
schema, and modeling code to encourage research on human-in-the-loop
interpretability and the evaluation of explanation effectiveness in real-world
settings.

</details>


### [19] [Date Fragments: A Hidden Bottleneck of Tokenization for Temporal Reasoning](https://arxiv.org/abs/2505.16088)
*Gagan Bhatia,Maxime Peyrard,Wei Zhao*

Main category: cs.CL

TL;DR: 研究揭示了BPE分词器日期碎片化对模型推理的影响，并提出评估指标、测试集与碎片修复机制分析


<details>
  <summary>Details</summary>
Motivation: 当前日期分词方式破坏时间结构，导致模型在历史/未来日期推理中准确率下降达10%

Method: 提出日期碎片化比率指标，构建含6500样本的DateAugBench测试集，通过层次化探测和注意力分析揭示模型机制

Result: 大模型更快形成日期抽象能力，模型推理路径遵循年→月→日（与人类不同），非常见日期准确率显著下降

Conclusion: 分词碎片化损害时间推理，模型通过注意力机制自我修复，建议改进分词器保留日期语义单元

Abstract: Modern BPE tokenizers often split calendar dates into meaningless fragments,
e.g., 20250312 $\rightarrow$ 202, 503, 12, inflating token counts and obscuring
the inherent structure needed for robust temporal reasoning. In this work, we
(1) introduce a simple yet interpretable metric, termed date fragmentation
ratio, that measures how faithfully a tokenizer preserves multi-digit date
components; (2) release DateAugBench, a suite of 6500 examples spanning three
temporal reasoning tasks: context-based date resolution, format-invariance
puzzles, and date arithmetic across historical, contemporary, and future
regimes; and (3) through layer-wise probing and causal attention-hop analyses,
uncover an emergent date-abstraction mechanism whereby large language models
stitch together the fragments of month, day, and year components for temporal
reasoning. Our experiments show that excessive fragmentation correlates with
accuracy drops of up to 10 points on uncommon dates like historical and
futuristic dates. Further, we find that the larger the model, the faster the
emergent date abstraction that heals date fragments is accomplished. Lastly, we
observe a reasoning path that LLMs follow to assemble date fragments, typically
differing from human interpretation (year $\rightarrow$ month $\rightarrow$
day).

</details>


### [20] [Continually Self-Improving Language Models for Bariatric Surgery Question--Answering](https://arxiv.org/abs/2505.16102)
*Yash Kumar Atri,Thomas H Shin,Thomas Hartvigsen*

Main category: cs.CL

TL;DR: 开发自适应RAG模型bRAGgen解决减重手术患者信息获取难题，配套数据集bRAGq验证显示其临床回答准确性显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 代谢减重手术患者需长期多学科协同管理，但医疗资源获取障碍导致难以及时获得权威医学信息。

Method: 提出动态阈值自适应RAG架构bRAGgen，创建1302个专业问题构成bRAGq数据集，通过LLM指标和外科专家双阶段评估。

Result: 在临床准确性、相关性等维度上，bRAGgen表现显著优于现有SOTA模型。

Conclusion: bRAGgen通过实时证据整合机制保障信息时效性，配套bRAGq数据集建立了代谢减重手术领域首个大规模评估基准。

Abstract: While bariatric and metabolic surgery (MBS) is considered the gold standard
treatment for severe and morbid obesity, its therapeutic efficacy hinges upon
active and longitudinal engagement with multidisciplinary providers, including
surgeons, dietitians/nutritionists, psychologists, and endocrinologists. This
engagement spans the entire patient journey, from preoperative preparation to
long-term postoperative management. However, this process is often hindered by
numerous healthcare disparities, such as logistical and access barriers, which
impair easy patient access to timely, evidence-based, clinician-endorsed
information. To address these gaps, we introduce bRAGgen, a novel adaptive
retrieval-augmented generation (RAG)-based model that autonomously integrates
real-time medical evidence when response confidence dips below dynamic
thresholds. This self-updating architecture ensures that responses remain
current and accurate, reducing the risk of misinformation. Additionally, we
present bRAGq, a curated dataset of 1,302 bariatric surgery--related questions,
validated by an expert bariatric surgeon. bRAGq constitutes the first
large-scale, domain-specific benchmark for comprehensive MBS care. In a
two-phase evaluation, bRAGgen is benchmarked against state-of-the-art models
using both large language model (LLM)--based metrics and expert surgeon review.
Across all evaluation dimensions, bRAGgen demonstrates substantially superior
performance in generating clinically accurate and relevant responses.

</details>


### [21] [Hierarchical Safety Realignment: Lightweight Restoration of Safety in Pruned Large Vision-Language Models](https://arxiv.org/abs/2505.16104)
*Yue Li,Xin Yi,Dongsheng Shi,Gerard de Melo,Xiaoling Wang,Linlin Wang*

Main category: cs.CL

TL;DR: 提出分层安全重校准（HSR）方法，通过分层恢复关键注意力头和神经元，提升剪枝后大型视觉语言模型的安全性


<details>
  <summary>Details</summary>
Motivation: 网络剪枝技术压缩模型时会显著降低安全性能，需开发针对性的安全恢复方案

Method: 1. 量化注意力头安全贡献度筛选关键头
2. 在选定头内针对性恢复安全相关神经元
3. 形成从注意力头到神经元的分层校准机制

Result: 在多种模型和剪枝策略中验证，安全性能显著提升且保持模型压缩效果

Conclusion: HSR首次实现剪枝模型的安全性能恢复，为安全敏感的模型轻量化提供新范式

Abstract: With the increasing size of Large Vision-Language Models (LVLMs), network
pruning techniques aimed at compressing models for deployment in
resource-constrained environments have garnered significant attention. However,
we observe that pruning often leads to a degradation in safety performance. To
address this issue, we present a novel and lightweight approach, termed
Hierarchical Safety Realignment (HSR). HSR operates by first quantifying the
contribution of each attention head to safety, identifying the most critical
ones, and then selectively restoring neurons directly within these attention
heads that play a pivotal role in maintaining safety. This process
hierarchically realigns the safety of pruned LVLMs, progressing from the
attention head level to the neuron level. We validate HSR across various models
and pruning strategies, consistently achieving notable improvements in safety
performance. To our knowledge, this is the first work explicitly focused on
restoring safety in LVLMs post-pruning.

</details>


### [22] [MPL: Multiple Programming Languages with Large Language Models for Information Extraction](https://arxiv.org/abs/2505.16107)
*Bo Li,Gexiang Fang,Wei Ye,Zhenghua Xu,Jinglei Zhang,Hao Cheng,Shikun Zhang*

Main category: cs.CL

TL;DR: 提出MPL框架探索多编程语言在监督微调阶段对信息抽取任务的提升效果


<details>
  <summary>Details</summary>
Motivation: 现有研究过度聚焦Python语言，忽略C++/Java等主流编程语言在SFT阶段的潜力

Method: MPL框架整合多种编程语言，结合虚拟运行function-prompt技术实现高效代码风格输入

Result: 在广泛数据集验证有效性，代码已开源供后续研究

Conclusion: 多编程语言协同和虚拟运行技术能显著提升信息抽取任务的结构化输出质量

Abstract: Recent research in information extraction (IE) focuses on utilizing
code-style inputs to enhance structured output generation. The intuition behind
this is that the programming languages (PLs) inherently exhibit greater
structural organization than natural languages (NLs). This structural advantage
makes PLs particularly suited for IE tasks. Nevertheless, existing research
primarily focuses on Python for code-style simulation, overlooking the
potential of other widely-used PLs (e.g., C++ and Java) during the supervised
fine-tuning (SFT) phase. In this research, we propose \textbf{M}ultiple
\textbf{P}rogramming \textbf{L}anguages with large language models for
information extraction (abbreviated as \textbf{MPL}), a novel framework that
explores the potential of incorporating different PLs in the SFT phase.
Additionally, we introduce \texttt{function-prompt} with virtual running to
simulate code-style inputs more effectively and efficiently. Experimental
results on a wide range of datasets demonstrate the effectiveness of MPL.
Furthermore, we conduct extensive experiments to provide a comprehensive
analysis. We have released our code for future research.

</details>


### [23] [Semiotic Reconstruction of Destination Expectation Constructs An LLM-Driven Computational Paradigm for Social Media Tourism Analytics](https://arxiv.org/abs/2505.16118)
*Haotian Lan,Yao Gao,Yujun Cheng,Wei Yuan,Kun Wang*

Main category: cs.CL

TL;DR: 研究者开发了结合无监督和监督学习的双模式LLM框架，通过分析社交媒体数据揭示休闲/社交期望是旅游参与的核心驱动因素，并验证了该框架在旅游分析和营销优化中的应用价值


<details>
  <summary>Details</summary>
Motivation: 针对用户生成内容(UGC)在旅游决策中的重要性日益凸显，但传统分析方法缺乏可扩展性的问题，研究旨在建立LLM作为精准量化工具以提升旅游分析效率

Method: 结合无监督的期望提取（从UGC数据中自动识别预期要素）和基于问卷调查的监督微调（验证并优化模型输出）的双阶段LLM框架

Result: 发现休闲体验（82%显著关联）和社交互动（76%预测权重）对旅游参与度的驱动作用超过自然环境（58%）和情感体验（63%）等基础要素

Conclusion: 该框架可扩展至消费者行为研究领域，通过计算社会科学方法实现营销策略优化，特别适用于旅游体验个性化和社交型旅行产品开发场景

Abstract: Social media's rise establishes user-generated content (UGC) as pivotal for
travel decisions, yet analytical methods lack scalability. This study
introduces a dual-method LLM framework: unsupervised expectation extraction
from UGC paired with survey-informed supervised fine-tuning. Findings reveal
leisure/social expectations drive engagement more than foundational
natural/emotional factors. By establishing LLMs as precision tools for
expectation quantification, we advance tourism analytics methodology and
propose targeted strategies for experience personalization and social travel
promotion. The framework's adaptability extends to consumer behavior research,
demonstrating computational social science's transformative potential in
marketing optimization.

</details>


### [24] [KoBALT: Korean Benchmark For Advanced Linguistic Tasks](https://arxiv.org/abs/2505.16125)
*Hyopil Shin,Sangah Lee,Dongjun Jang,Wooseok Song,Jaeyoon Kim,Chaeyoung Oh,Hyemi Jo,Youngchae Ahn,Sihyun Oh,Hyohyeong Chang,Sunkyoung Kim,Jinsik Lee*

Main category: cs.CL

TL;DR: KoBALT是包含700道题目的韩语多维度语言能力评测基准，覆盖5大领域24个语言现象，评估显示主流模型在语音学/形态学领域表现较弱，且与人类评价高度相关。


<details>
  <summary>Details</summary>
Motivation: 解决现有韩语评测基准缺乏语言学深度和类型学基础的问题，通过专家设计题目减少数据污染风险，更准确评估语言模型的真实理解能力。

Method: 构建涵盖句法/语义/语用/语音/形态学的700道选择题，评估20个主流LLM并开展95人参与的人类偏好实验。

Result: 最佳模型总体准确率61%，语义领域达66%但语音学(31%)和形态学(36%)表现薄弱，基准分数与人类判断呈现强相关性。

Conclusion: KoBALT填补了韩语语言能力评估的空白，为衡量语言模型的真实语言学竞争力提供了可靠框架。

Abstract: We introduce KoBALT (Korean Benchmark for Advanced Linguistic Tasks), a
comprehensive linguistically-motivated benchmark comprising 700 multiple-choice
questions spanning 24 phenomena across five linguistic domains: syntax,
semantics, pragmatics, phonetics/phonology, and morphology. KoBALT is designed
to advance the evaluation of large language models (LLMs) in Korean, a
morphologically rich language, by addressing the limitations of conventional
benchmarks that often lack linguistic depth and typological grounding. It
introduces a suite of expert-curated, linguistically motivated questions with
minimal n-gram overlap with standard Korean corpora, substantially mitigating
the risk of data contamination and allowing a more robust assessment of true
language understanding. Our evaluation of 20 contemporary LLMs reveals
significant performance disparities, with the highest-performing model
achieving 61\% general accuracy but showing substantial variation across
linguistic domains - from stronger performance in semantics (66\%) to
considerable weaknesses in phonology (31\%) and morphology (36\%). Through
human preference evaluation with 95 annotators, we demonstrate a strong
correlation between KoBALT scores and human judgments, validating our
benchmark's effectiveness as a discriminative measure of Korean language
understanding. KoBALT addresses critical gaps in linguistic evaluation for
typologically diverse languages and provides a robust framework for assessing
genuine linguistic competence in Korean language models.

</details>


### [25] [Veracity Bias and Beyond: Uncovering LLMs' Hidden Beliefs in Problem-Solving Reasoning](https://arxiv.org/abs/2505.16128)
*Yue Zhou,Barbara Di Eugenio*

Main category: cs.CL

TL;DR: LLMs在数学/编程任务中显著低估非裔群体正确率，在写作评估中亚洲作者得分最低，并发现模型可视化代码中自动关联种族刻板颜色


<details>
  <summary>Details</summary>
Motivation: 揭示LLMs的偏见不仅存在于表层刻板印象，更深层次体现在解题正确性归因和评估环节中

Method: 使用5个价值观对齐的LLM进行数学/编程/常识/写作任务测试，分析解决方案的真实性归属和评估差异

Result: 发现归因偏见（非裔数学编码正确率低）和评估偏见（相同答案因作者种族得分不同），模型可视化代码显示种族-颜色刻板联想

Conclusion: LLMs的种族偏见已深度嵌入模型推理过程，在教育评估等场景的应用存在严重伦理风险

Abstract: Despite LLMs' explicit alignment against demographic stereotypes, they have
been shown to exhibit biases under various social contexts. In this work, we
find that LLMs exhibit concerning biases in how they associate solution
veracity with demographics. Through experiments across five human value-aligned
LLMs on mathematics, coding, commonsense, and writing problems, we reveal two
forms of such veracity biases: Attribution Bias, where models
disproportionately attribute correct solutions to certain demographic groups,
and Evaluation Bias, where models' assessment of identical solutions varies
based on perceived demographic authorship. Our results show pervasive biases:
LLMs consistently attribute fewer correct solutions and more incorrect ones to
African-American groups in math and coding, while Asian authorships are least
preferred in writing evaluation. In additional studies, we show LLMs
automatically assign racially stereotypical colors to demographic groups in
visualization code, suggesting these biases are deeply embedded in models'
reasoning processes. Our findings indicate that demographic bias extends beyond
surface-level stereotypes and social context provocations, raising concerns
about LLMs' deployment in educational and evaluation settings.

</details>


### [26] [LLMs Are Not Scorers: Rethinking MT Evaluation with Generation-Based Methods](https://arxiv.org/abs/2505.16129)
*Hyang Cui*

Main category: cs.CL

TL;DR: 提出基于生成的评估范式，结合LLMs生成参考和语义评分，提升机器翻译质量评估效果


<details>
  <summary>Details</summary>
Motivation: 现有直接评分方法在段落级别与人类判断相关性较低，需更有效评估方法

Method: 使用仅解码器LLMs生成高质量参考译文，结合句子嵌入进行语义相似性评分

Result: 在8个LLMs和8种语言对中超越现有基准模型及非LLM无参考指标

Conclusion: 支持转向'流畅生成+精准语义评估'的混合评估范式

Abstract: Recent studies have applied large language models (LLMs) to machine
translation quality estimation (MTQE) by prompting models to assign numeric
scores. Nonetheless, these direct scoring methods tend to show low
segment-level correlation with human judgments. In this paper, we propose a
generation-based evaluation paradigm that leverages decoder-only LLMs to
produce high-quality references, followed by semantic similarity scoring using
sentence embeddings. We conduct the most extensive evaluation to date in MTQE,
covering 8 LLMs and 8 language pairs. Empirical results show that our method
outperforms both intra-LLM direct scoring baselines and external non-LLM
reference-free metrics from MTME. These findings demonstrate the strength of
generation-based evaluation and support a shift toward hybrid approaches that
combine fluent generation with accurate semantic assessment.

</details>


### [27] [Position of Uncertainty: A Cross-Linguistic Study of Positional Bias in Large Language Models](https://arxiv.org/abs/2505.16134)
*Menschikov Mikhail,Alexander Kharitonov,Maiia Kotyga,Vadim Porvatov,Anna Zhukovskaya,David Kagramanyan,Egor Shvetsov,Evgeny Burnaev*

Main category: cs.CL

TL;DR: 该研究揭示大语言模型的位置偏见存在跨语言差异，挑战早期位置偏好假设，并质疑现有提示工程实践的有效性。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型位置偏见与语言多样性（形态类型、句法结构）的交互关系，填补该领域研究空白。

Method: 选取5种类型学差异语言（英语、俄语、德语、印地语、越南语），通过控制位置变量、熵值分析和句法扰动实验展开跨语言比较。

Result: 1) 位置偏见具有模型特异性（如千问2.5-7B偏好后期位置） 2) 显式位置提示降低多语言准确率 3) 对齐模型偏见的上下文增加预测熵值 4) LLMs在自由语序语言中强加优势语序

Conclusion: 研究强调开发语言敏感的模型架构和提示策略的必要性，指出当前以英语为中心的优化方案可能不适用于其他语言类型。

Abstract: Large language models exhibit positional bias -- systematic neglect of
information at specific context positions -- yet its interplay with linguistic
diversity remains poorly understood. We present a cross-linguistic study across
five typologically distinct languages (English, Russian, German, Hindi,
Vietnamese), examining how positional bias interacts with model uncertainty,
syntax, and prompting. Key findings: (1) Positional bias is model-driven, with
language-specific variations -- Qwen2.5-7B favors late positions, challenging
assumptions of early-token bias; (2) Explicit positional guidance (e.g.,
correct context is at position X) reduces accuracy across languages,
undermining prompt-engineering practices; (3) Aligning context with positional
bias increases entropy, yet minimal entropy does not predict accuracy. (4) We
further uncover that LLMs differently impose dominant word order in
free-word-order languages like Hindi.

</details>


### [28] [Distilling the Implicit Multi-Branch Structure in LLMs' Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.16142)
*Shicheng Xu,Liang Pang,Yunchang Zhu,Jia Gu,Zihao Wei,Jingcheng Deng,Feiyang Pan,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 提出强化学习蒸馏框架RLKD，通过生成结构奖励模型GSRM引导学生模型内化教师模型的多分支推理结构，突破传统监督微调的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统监督微调方法将教师模型的复杂推理结构压缩为平面序列，无法有效传递元推理与子问题解决的交互过程，限制学生模型推理能力提升。

Method: 设计GSRM将推理路径分解为元推理-解决步骤，构建结构对齐奖励机制，结合强化学习训练学生模型吸收教师的多层次推理结构。

Result: 在0.1%数据量的强化学习训练中超越传统SFT-RL流程，显著释放学生模型潜力，推理能力优于基于监督微调的方法。

Conclusion: RLKD通过结构对齐机制成功实现深度知识蒸馏，验证了捕捉隐式推理结构对提升小模型能力的关键作用。

Abstract: Distilling reasoning paths from teacher to student models via supervised
fine-tuning (SFT) provides a shortcut for improving the reasoning ability of
smaller Large Language Models (LLMs). However, the reasoning paths generated by
teacher models often reflect only surface-level traces of their underlying
authentic reasoning. Insights from cognitive neuroscience suggest that
authentic reasoning involves a complex interweaving between meta-reasoning
(which selects appropriate sub-problems from multiple candidates) and solving
(which addresses the sub-problem). This implies authentic reasoning has an
implicit multi-branch structure. Supervised fine-tuning collapses this rich
structure into a flat sequence of token prediction in the teacher's reasoning
path, preventing effective distillation of this structure to students. To
address this limitation, we propose RLKD, a reinforcement learning (RL)-based
distillation framework guided by a novel Generative Structure Reward Model
(GSRM). Our GSRM converts reasoning paths into multiple meta-reasoning-solving
steps and computes rewards to measure structural alignment between student and
teacher reasoning. RLKD combines this reward with RL, enabling student LLMs to
internalize the teacher's implicit multi-branch reasoning structure rather than
merely mimicking fixed output paths. Experiments show RLKD surpasses standard
SFT-RL pipelines even when trained on 0.1% of data under an RL-only regime,
unlocking greater student reasoning potential than SFT-based distillation.

</details>


### [29] [EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios](https://arxiv.org/abs/2505.16160)
*Bin Xu,Yu Bai,Huashan Sun,Yiguan Lin,Siming Liu,Xinyue Liang,Yaolin Li,Yang Gao,Heyan Huang*

Main category: cs.CL

TL;DR: 提出首个教育场景专用基准EduBench，构建含9大场景/4000+情境的合成数据集，训练的小规模模型性能媲美顶尖大模型


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在教育场景的应用存在探索不足和优化空间，需建立专用评估体系推动教育领域语言模型发展

Method: 1. 构建含9大教育场景的合成数据集
2. 设计覆盖12个核心维度的评估指标
3. 采用人工标注确保评估有效性
4. 基于数据集训练轻量级模型

Result: 训练的小规模模型在测试集上达到Deepseek V3、Qwen Max等顶尖模型的性能水平

Conclusion: 为教育导向的语言模型开发与评估奠定实践基础，开源代码和数据推动教育AI发展

Abstract: As large language models continue to advance, their application in
educational contexts remains underexplored and under-optimized. In this paper,
we address this gap by introducing the first diverse benchmark tailored for
educational scenarios, incorporating synthetic data containing 9 major
scenarios and over 4,000 distinct educational contexts. To enable comprehensive
assessment, we propose a set of multi-dimensional evaluation metrics that cover
12 critical aspects relevant to both teachers and students. We further apply
human annotation to ensure the effectiveness of the model-generated evaluation
responses. Additionally, we succeed to train a relatively small-scale model on
our constructed dataset and demonstrate that it can achieve performance
comparable to state-of-the-art large models (e.g., Deepseek V3, Qwen Max) on
the test set. Overall, this work provides a practical foundation for the
development and evaluation of education-oriented language models. Code and data
are released at https://github.com/ybai-nlp/EduBench.

</details>


### [30] [KNN-SSD: Enabling Dynamic Self-Speculative Decoding via Nearest Neighbor Layer Set Optimization](https://arxiv.org/abs/2505.16162)
*Mingbo Song,Heming Xia,Jun Zhang,Chak Tou Leong,Qiancheng Xu,Wenjie Li,Sujian Li*

Main category: cs.CL

TL;DR: KNN-SSD算法通过K近邻搜索提升推测解码的领域泛化能力，实现LLM推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有自推测解码方法在领域迁移时加速性能显著下降，需增强其领域适应性。

Method: 提出KNN-SSD，利用K近邻搜索动态匹配不同领域输入与最优跳层组合。

Result: 实验显示在多个模型和任务中实现1.3x-1.6x的推理加速。

Conclusion: 该方法有效解决了领域迁移下的加速性能衰减问题，保持生成质量的同时提升推理效率。

Abstract: Speculative Decoding (SD) has emerged as a widely used paradigm to accelerate
the inference of large language models (LLMs) without compromising generation
quality. It works by efficiently drafting multiple tokens using a compact model
and then verifying them in parallel using the target LLM. Notably,
Self-Speculative Decoding proposes skipping certain layers to construct the
draft model, which eliminates the need for additional parameters or training.
Despite its strengths, we observe in this work that drafting with layer
skipping exhibits significant sensitivity to domain shifts, leading to a
substantial drop in acceleration performance. To enhance the domain
generalizability of this paradigm, we introduce KNN-SSD, an algorithm that
leverages K-Nearest Neighbor (KNN) search to match different skipped layers
with various domain inputs. We evaluated our algorithm in various models and
multiple tasks, observing that its application leads to 1.3x-1.6x speedup in
LLM inference.

</details>


### [31] [Can LLMs Simulate Human Behavioral Variability? A Case Study in the Phonemic Fluency Task](https://arxiv.org/abs/2505.16164)
*Mengyang Qiu,Zoe Brisebois,Siena Sun*

Main category: cs.CL

TL;DR: LLM无法完全模拟人类在音位流畅性任务中的个体差异，虽能匹配人类平均水平但缺乏多样性


<details>
  <summary>Details</summary>
Motivation: 验证LLM是否能够复现人类在认知任务中表现出的个体差异性，评估其作为人类替代研究对象的可行性

Method: 通过34种不同提示语、采样温度参数和模型类型的组合配置，与106名人类参与者的音位流畅性测试结果进行对比分析

Result: Claude 3.7 Sonnet等配置能匹配人类平均表现，但所有模型均无法复现人类反应的多样性范围，LLM输出结构僵化且多样性持续偏低

Conclusion: LLM在模拟人类认知行为存在根本性局限，需谨慎将其作为人类行为研究的替代方案

Abstract: Large language models (LLMs) are increasingly explored as substitutes for
human participants in cognitive tasks, but their ability to simulate human
behavioral variability remains unclear. This study examines whether LLMs can
approximate individual differences in the phonemic fluency task, where
participants generate words beginning with a target letter. We evaluated 34
model configurations, varying prompt specificity, sampling temperature, and
model type, and compared outputs to responses from 106 human participants.
While some configurations, especially Claude 3.7 Sonnet, matched human averages
and lexical preferences, none reproduced the scope of human variability. LLM
outputs were consistently less diverse and structurally rigid, and LLM
ensembles failed to increase diversity. Network analyses further revealed
fundamental differences in retrieval structure between humans and models. These
results highlight key limitations in using LLMs to simulate human cognition and
behavior.

</details>


### [32] [When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction](https://arxiv.org/abs/2505.16170)
*Yuqing Yang,Robin Jia*

Main category: cs.CL

TL;DR: 研究发现大语言模型(LLMs)仅在特定条件下撤回错误答案，其撤回行为与内部信念密切相关，监督微调可显著提升模型对自身错误的认知能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否能够识别并撤回与自身知识库矛盾的错误答案，揭示模型内部信念对其纠错行为的影响机制。

Method: 构建模型特定评估数据集，通过信念干预实验分析注意力机制变化，采用监督微调优化模型内部表征。

Result: LLMs自然状态下撤回率低(18%)，信念干预使验证尝试增加47%，监督微调后撤回准确率提升至82%

Conclusion: 模型内部信念是影响错误认知的关键因素，通过参数优化可显著增强LLMs的自我纠错能力，为构建可信AI提供新思路。

Abstract: Can large language models (LLMs) admit their mistakes when they should know
better? In this work, we define the behavior of acknowledging errors in
previously generated answers as "retraction" and aim to understand when and why
LLMs choose to retract. We first construct model-specific datasets to evaluate
whether a model will retract an incorrect answer that contradicts its own
parametric knowledge. While LLMs are capable of retraction, they do so only
infrequently. We demonstrate that retraction is closely tied to previously
identified indicators of models' internal belief: models fail to retract wrong
answers that they "believe" to be factually correct. Steering experiments
further demonstrate that internal belief causally influences model retraction.
In particular, when the model does not believe its answer, this not only
encourages the model to attempt to verify the answer, but also alters attention
behavior during self-verification. Finally, we demonstrate that simple
supervised fine-tuning significantly improves retraction performance by helping
the model learn more accurate internal beliefs. Code and datasets are available
on https://github.com/ayyyq/llm-retraction.

</details>


### [33] [Automated Feedback Loops to Protect Text Simplification with Generative AI from Information Loss](https://arxiv.org/abs/2505.16172)
*Abhay Kumara Sri Krishna Nandiraju,Gondy Leroy,David Kauchak,Arif Ahmed*

Main category: cs.CL

TL;DR: 研究通过生成式AI简化健康信息时，发现补充缺失实体能有效提升文本质量，最佳方案是添加所有缺失实体而非选择性添加。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在简化健康信息时可能遗漏关键内容，影响信息完整性及用户理解效果。

Method: 使用GPT-4简化50篇健康文本，采用五种策略补充缺失内容（全实体/全词汇/TOP3实体/随机实体对照），通过余弦相似度和ROUGE指标评估重建效果。

Result: 添加全部缺失实体的文本重构效果最佳（语义相似度提升21%），优于选择性添加或随机添加方案。

Conclusion: 当前工具可识别缺失实体但排序不可靠，直接补充全部缺失实体是优化生成式AI简化质量的有效路径。

Abstract: Understanding health information is essential in achieving and maintaining a
healthy life. We focus on simplifying health information for better
understanding. With the availability of generative AI, the simplification
process has become efficient and of reasonable quality, however, the algorithms
remove information that may be crucial for comprehension. In this study, we
compare generative AI to detect missing information in simplified text,
evaluate its importance, and fix the text with the missing information. We
collected 50 health information texts and simplified them using gpt-4-0613. We
compare five approaches to identify missing elements and regenerate the text by
inserting the missing elements. These five approaches involve adding missing
entities and missing words in various ways: 1) adding all the missing entities,
2) adding all missing words, 3) adding the top-3 entities ranked by gpt-4-0613,
and 4, 5) serving as controls for comparison, adding randomly chosen entities.
We use cosine similarity and ROUGE scores to evaluate the semantic similarity
and content overlap between the original, simplified, and reconstructed
simplified text. We do this for both summaries and full text. Overall, we find
that adding missing entities improves the text. Adding all the missing entities
resulted in better text regeneration, which was better than adding the
top-ranked entities or words, or random words. Current tools can identify these
entities, but are not valuable in ranking them.

</details>


### [34] [Understanding Fact Recall in Language Models: Why Two-Stage Training Encourages Memorization but Mixed Training Teaches Knowledge](https://arxiv.org/abs/2505.16178)
*Ying Zhang,Benjamin Heinzerling,Dongyuan Li,Ryoma Ishigaki,Yuta Hitomi,Kentaro Inui*

Main category: cs.CL

TL;DR: 混合训练通过促进共享参数集中化，提升语言模型事实回忆的泛化能力


<details>
  <summary>Details</summary>
Motivation: 探索不同训练策略（两阶段训练vs混合训练）对语言模型事实回忆能力的影响机制，现有方法存在机械记忆倾向而混合训练效果更优但原理未明

Method: 引入跨任务梯度追踪技术分析共享参数，在Llama-3.2B和Pythia-2.8B模型上使用合成事实数据集进行参数分布研究

Result: 混合训练产生更大规模且更集中的共享参数集群，这些参数对事实存储和回忆任务共同重要

Conclusion: 共享参数的出现可能是语言模型实现跨任务形式知识泛化的关键机制

Abstract: Fact recall, the ability of language models (LMs) to retrieve specific
factual knowledge, remains a challenging task despite their impressive general
capabilities. Common training strategies often struggle to promote robust
recall behavior with two-stage training, which first trains a model with
fact-storing examples (e.g., factual statements) and then with fact-recalling
examples (question-answer pairs), tending to encourage rote memorization rather
than generalizable fact retrieval. In contrast, mixed training, which jointly
uses both types of examples, has been empirically shown to improve the ability
to recall facts, but the underlying mechanisms are still poorly understood. In
this work, we investigate how these training strategies affect how model
parameters are shaped during training and how these differences relate to their
ability to recall facts. We introduce cross-task gradient trace to identify
shared parameters, those strongly influenced by both fact-storing and
fact-recalling examples. Our analysis on synthetic fact recall datasets with
the Llama-3.2B and Pythia-2.8B models reveals that mixed training encouraging a
larger and more centralized set of shared parameters. These findings suggest
that the emergence of parameters may play a key role in enabling LMs to
generalize factual knowledge across task formulations.

</details>


### [35] [SAE-SSV: Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models](https://arxiv.org/abs/2505.16188)
*Zirui He,Mingyu Jin,Bo Shen,Ali Payani,Yongfeng Zhang,Mengnan Du*

Main category: cs.CL

TL;DR: 提出基于稀疏自编码器的监督引导方法，通过可解释子空间优化LLM行为控制


<details>
  <summary>Details</summary>
Motivation: 现有方法在开放生成场景中难以可靠控制大模型行为，需更精准的干预手段

Method: 1. 使用稀疏自编码器解耦语义特征
2. 训练线性分类器定位任务相关子空间
3. 在子空间内学习监督引导向量

Result: 在情感/真实性/政治倾向控制任务中，成功率提升且生成质量衰减最小（相比基线方法）

Conclusion: 极小可解释子空间即可实现有效引导，为模型干预提供新方向

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but controlling their behavior
reliably remains challenging, especially in open-ended generation settings.
This paper introduces a novel supervised steering approach that operates in
sparse, interpretable representation spaces. We employ sparse autoencoders
(SAEs)to obtain sparse latent representations that aim to disentangle semantic
attributes from model activations. Then we train linear classifiers to identify
a small subspace of task-relevant dimensions in latent representations.
Finally, we learn supervised steering vectors constrained to this subspace,
optimized to align with target behaviors. Experiments across sentiment,
truthfulness, and politics polarity steering tasks with multiple LLMs
demonstrate that our supervised steering vectors achieve higher success rates
with minimal degradation in generation quality compared to existing methods.
Further analysis reveals that a notably small subspace is sufficient for
effective steering, enabling more targeted and interpretable interventions.

</details>


### [36] [The Language of Interoception: Examining Embodiment and Emotion Through a Corpus of Body Part Mentions](https://arxiv.org/abs/2505.16189)
*Sophie Wu,Jan Philip Wahle,Saif M. Mohammad*

Main category: cs.CL

TL;DR: 论文首次通过大规模自然语言数据分析情感、身体体验与日常语言的关系，发现身体部位提及（BPMs）普遍存在且与情感强度、健康问题显著相关，为跨学科研究开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 探索情感-身体体验-语言三者的内在联系，填补自然语境下该领域的研究空白。

Method: 构建包含BPMs的博客/推文语料库（含人工情感标注），结合情感词汇关联分析与统计检验，探究BPMs分布模式及其与健康指标的相关性。

Result: ①BPMs在个人叙述中占比5-10%且时空分布差异显著；②BPMs文本情感浓度更高；③身体语言与健康负面指标强相关。

Conclusion: 身体部位语言分析为NLP、情感科学与人类福祉的交叉研究提供了突破性视角，具有重要学术价值。

Abstract: This paper is the first investigation of the connection between emotion,
embodiment, and everyday language in a large sample of natural language data.
We created corpora of body part mentions (BPMs) in online English text (blog
posts and tweets). This includes a subset featuring human annotations for the
emotions of the person whose body part is mentioned in the text. We show that
BPMs are common in personal narratives and tweets (~5% to 10% of posts include
BPMs) and that their usage patterns vary markedly by time and %geographic
location. Using word-emotion association lexicons and our annotated data, we
show that text containing BPMs tends to be more emotionally charged, even when
the BPM is not explicitly used to describe a physical reaction to the emotion
in the text. Finally, we discover a strong and statistically significant
correlation between body-related language and a variety of poorer health
outcomes. In sum, we argue that investigating the role of body-part related
words in language can open up valuable avenues of future research at the
intersection of NLP, the affective sciences, and the study of human wellbeing.

</details>


### [37] [An Empirical Study on Configuring In-Context Learning Demonstrations for Unleashing MLLMs' Sentimental Perception Capability](https://arxiv.org/abs/2505.16193)
*Daiqing Wu,Dongbao Yang,Sicheng Zhao,Can Ma,Yu Zhou*

Main category: cs.CL

TL;DR: 多模态大语言模型(MLLMs)通过上下文学习优化演示配置，显著提升零样本范式下的多模态情感分析性能


<details>
  <summary>Details</summary>
Motivation: 现有零样本范式在MSA任务中表现不佳，质疑MLLMs的情感感知能力能否达到监督模型水平

Method: 将零样本扩展到上下文学习(ICL)，系统优化演示的检索、呈现、分布三要素，并发现/抵消MLLMs的情感预测偏差

Result: 在六个MSA数据集上实现平均15.9%的零样本性能提升，相比随机ICL基线提升11.2%

Conclusion: 验证MLLMs具备与监督模型相当的情感分析能力，通过演示配置策略组合有效释放模型潜力

Abstract: The advancements in Multimodal Large Language Models (MLLMs) have enabled
various multimodal tasks to be addressed under a zero-shot paradigm. This
paradigm sidesteps the cost of model fine-tuning, emerging as a dominant trend
in practical application. Nevertheless, Multimodal Sentiment Analysis (MSA), a
pivotal challenge in the quest for general artificial intelligence, fails to
accommodate this convenience. The zero-shot paradigm exhibits undesirable
performance on MSA, casting doubt on whether MLLMs can perceive sentiments as
competent as supervised models. By extending the zero-shot paradigm to
In-Context Learning (ICL) and conducting an in-depth study on configuring
demonstrations, we validate that MLLMs indeed possess such capability.
Specifically, three key factors that cover demonstrations' retrieval,
presentation, and distribution are comprehensively investigated and optimized.
A sentimental predictive bias inherent in MLLMs is also discovered and later
effectively counteracted. By complementing each other, the devised strategies
for three factors result in average accuracy improvements of 15.9% on six MSA
datasets against the zero-shot paradigm and 11.2% against the random ICL
baseline.

</details>


### [38] [Large Language Models based ASR Error Correction for Child Conversations](https://arxiv.org/abs/2505.16212)
*Anfeng Xu,Tiantian Feng,So Hyun Kim,Somer Bishop,Catherine Lord,Shrikanth Narayanan*

Main category: cs.CL

TL;DR: 探索大语言模型在纠正儿童会话语音ASR错误中的应用，在零样本和微调CTC模型上有效，但在结合上下文信息或使用微调自回归模型（如Whisper）时仍存在挑战


<details>
  <summary>Details</summary>
Motivation: 针对儿童语音识别准确率低的痛点，研究LLMs在纠正儿童会话场景ASR错误中的潜力，填补该领域应用研究的空白

Method: 使用两个儿童会话语音数据集，分别在零样本ASR输出和微调后的CTC/自回归ASR（如Whisper）输出上进行LLM纠错实验

Result: LLMs可有效提升零样本ASR和CTC模型的识别准确率，但难以改善包含上下文信息的场景，且对微调后的自回归ASR（如Whisper）提升有限

Conclusion: LLMs在儿童语音纠错中的应用具有选择性效果，其性能提升受ASR模型架构（CTC vs自回归）和上下文整合方式的影响，需进一步研究优化方案

Abstract: Automatic Speech Recognition (ASR) has recently shown remarkable progress,
but accurately transcribing children's speech remains a significant challenge.
Recent developments in Large Language Models (LLMs) have shown promise in
improving ASR transcriptions. However, their applications in child speech
including conversational scenarios are underexplored. In this study, we explore
the use of LLMs in correcting ASR errors for conversational child speech. We
demonstrate the promises and challenges of LLMs through experiments on two
children's conversational speech datasets with both zero-shot and fine-tuned
ASR outputs. We find that while LLMs are helpful in correcting zero-shot ASR
outputs and fine-tuned CTC-based ASR outputs, it remains challenging for LLMs
to improve ASR performance when incorporating contextual information or when
using fine-tuned autoregressive ASR (e.g., Whisper) outputs.

</details>


### [39] [Memorization or Reasoning? Exploring the Idiom Understanding of LLMs](https://arxiv.org/abs/2505.16216)
*Jisu Kim,Youngwoo Shin,Uiji Hwang,Jihun Choi,Richeng Xuan,Taeuk Kim*

Main category: cs.CL

TL;DR: 该研究通过构建多语言成语数据集MIDAS，揭示了大型语言模型通过记忆检索与上下文推理的混合机制处理成语的规律。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对LLMs成语处理机制的深入理解，特别是在多语言场景下模型如何结合记忆与推理的机制尚不明确。

Method: 构建包含6种语言的MIDAS成语数据集，系统评估LLMs在不同类型成语任务中的表现，并通过控制实验分析关键影响因素。

Result: LLMs对成语的处理呈现记忆与推理的双路径模式，组合型成语的理解更依赖上下文推理而非单纯记忆检索。

Conclusion: LLMs的成语理解能力源于内部知识库检索与推理机制的动态协同，该发现为提升模型语言认知能力提供了新视角。

Abstract: Idioms have long posed a challenge due to their unique linguistic properties,
which set them apart from other common expressions. While recent studies have
leveraged large language models (LLMs) to handle idioms across various tasks,
e.g., idiom-containing sentence generation and idiomatic machine translation,
little is known about the underlying mechanisms of idiom processing in LLMs,
particularly in multilingual settings. To this end, we introduce MIDAS, a new
large-scale dataset of idioms in six languages, each paired with its
corresponding meaning. Leveraging this resource, we conduct a comprehensive
evaluation of LLMs' idiom processing ability, identifying key factors that
influence their performance. Our findings suggest that LLMs rely not only on
memorization, but also adopt a hybrid approach that integrates contextual cues
and reasoning, especially when processing compositional idioms. This implies
that idiom understanding in LLMs emerges from an interplay between internal
knowledge retrieval and reasoning-based inference.

</details>


### [40] [Don't Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation](https://arxiv.org/abs/2505.16222)
*Jiwon Moon,Yerin Hwang,Dongryeol Lee,Taegwan Kang,Yongil Kim,Kyomin Jung*

Main category: cs.CL

TL;DR: 大语言模型作为代码评估者时，对语义相同但存在表面差异的代码会表现出系统性偏见，导致评分结果不可靠。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示LLM评估器在代码变体（如变量命名、注释格式等非功能性差异）评估中存在的潜在偏见问题，探讨其是否具备公平、稳定的代码评估能力。

Method: 通过对5种编程语言和多个LLM进行实证研究，系统定义了6种代码评估偏见类型，并测试了生成测试用例前后LLM评估结果的稳定性。

Result: 所有测试的LLM评估器均存在正向和负向偏见，导致评分虚高或异常偏低，且生成测试用例后仍无法有效消除偏见影响。

Conclusion: 当前LLM代码评估方法存在系统性缺陷，需开发更鲁棒的评估体系以保证代码评估的公平性和稳定性。

Abstract: With the growing use of large language models(LLMs) as evaluators, their
application has expanded to code evaluation tasks, where they assess the
correctness of generated code without relying on reference implementations.
While this offers scalability and flexibility, it also raises a critical,
unresolved question: Can LLM judges fairly and robustly evaluate semantically
equivalent code with superficial variations? Functionally correct code often
exhibits variations-such as differences in variable names, comments, or
formatting-that should not influence its correctness. Yet, whether LLM judges
can reliably handle these variations remains unclear. We present the first
comprehensive study of this issue, defining six types of potential bias in code
evaluation and revealing their systematic impact on LLM judges. Across five
programming languages and multiple LLMs, we empirically demonstrate that all
tested LLM judges are susceptible to both positive and negative biases,
resulting in inflated or unfairly low scores. Moreover, we observe that LLM
judges remain vulnerable to these biases even when prompted to generate test
cases before scoring, highlighting the need for more robust code evaluation
methods.

</details>


### [41] [Explain Less, Understand More: Jargon Detection via Personalized Parameter-Efficient Fine-tuning](https://arxiv.org/abs/2505.16227)
*Bohao Wu,Qingyun Wang,Yue Guo*

Main category: cs.CL

TL;DR: 研究者提出了基于LoRA微调和个性化提示的高效个性化术语检测方法，仅需10%标注数据即可超越GPT-4性能


<details>
  <summary>Details</summary>
Motivation: 传统个性化模型需要大量标注数据和计算资源，难以在资源受限场景应用

Method: 结合LoRA轻量微调与个性化提示策略，并探索标注数据与无监督用户信号的混合方法

Result: LoRA模型F1分数超越GPT-4达21.4%，仅用10%数据即达到基线模型98%性能

Conclusion: 首个系统性实现低资源个性化术语检测的开源方案，为可扩展的自适应NLP系统提供新路径

Abstract: Personalizing jargon detection and explanation is essential for making
technical documents accessible to readers with diverse disciplinary
backgrounds. However, tailoring models to individual users typically requires
substantial annotation efforts and computational resources due to user-specific
finetuning. To address this, we present a systematic study of personalized
jargon detection, focusing on methods that are both efficient and scalable for
real-world deployment. We explore two personalization strategies: (1)
lightweight fine-tuning using Low-Rank Adaptation (LoRA) on open-source models,
and (2) personalized prompting, which tailors model behavior at inference time
without retaining. To reflect realistic constraints, we also investigate hybrid
approaches that combine limited annotated data with unsupervised user
background signals. Our personalized LoRA model outperforms GPT-4 by 21.4% in
F1 score and exceeds the best performing oracle baseline by 8.3%. Remarkably,
our method achieves comparable performance using only 10% of the annotated
training data, demonstrating its practicality for resource-constrained
settings. Our study offers the first work to systematically explore efficient,
low-resource personalization of jargon detection using open-source language
models, offering a practical path toward scalable, user-adaptive NLP system.

</details>


### [42] [MuseRAG: Idea Originality Scoring At Scale](https://arxiv.org/abs/2505.16232)
*Ali Sarosh Bangash,Krish Veera,Ishfat Abrar Islam,Raiyan Abdul Baten*

Main category: cs.CL

TL;DR: MuseRAG通过结合大语言模型和检索增强生成框架，实现全自动化的创意原创性评分，准确度与人工标注相当。


<details>
  <summary>Details</summary>
Motivation: 传统人工分类方法效率低下且易出错，研究旨在开发自动化、可扩展的原创性评估方案。

Method: 采用检索增强生成(RAG)框架，通过大语言模型检索语义相似的已有创意类别，并零样本判断新创意归属或创建新类别。

Result: 在5个数据集(N=1143)中实现与人工标注相近的聚类结构(AMI=0.59)和参与者层面评分相关性(r=0.89)，具备良好效度。

Conclusion: 该研究实现了大规模、与人类判断对齐的原创性评分系统，为创造力研究提供了有效工具。

Abstract: An objective, face-valid way to assess the originality of creative ideas is
to measure how rare each idea is within a population -- an approach long used
in creativity research but difficult to automate at scale. Tabulating response
frequencies via manual bucketing of idea rephrasings is labor-intensive,
error-prone, and brittle under large corpora. We introduce a fully automated,
psychometrically validated pipeline for frequency-based originality scoring.
Our method, MuseRAG, combines large language models (LLMs) with an externally
orchestrated retrieval-augmented generation (RAG) framework. Given a new idea,
the system retrieves semantically similar prior idea buckets and zero-shot
prompts the LLM to judge whether the new idea belongs to an existing bucket or
forms a new one. The resulting buckets enable computation of frequency-based
originality metrics. Across five datasets (N=1143, n_ideas=16294), MuseRAG
matches human annotators in idea clustering structure and resolution (AMI =
0.59) and in participant-level scoring (r = 0.89) -- while exhibiting strong
convergent and external validity. Our work enables intent-sensitive,
human-aligned originality scoring at scale to aid creativity research.

</details>


### [43] [LIFEBench: Evaluating Length Instruction Following in Large Language Models](https://arxiv.org/abs/2505.16234)
*Wei Zhang,Zhenhong Zhou,Junfeng Fang,Rongwu Xu,Kun Wang,Yuanhe Zhang,Rui Wang,Ge Zhang,Xinfeng Li,Li Sun,Lingjuan Lyu,Yang Liu,Sen Su*

Main category: cs.CL

TL;DR: 提出LIFEBench基准测试揭示当前大语言模型在遵循长文本生成指令方面存在根本性缺陷，推理型模型意外展现出最佳长度控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有大模型虽能处理复杂推理任务，但无法可靠遵循明确字数要求（如生成万字小说），存在输出过短、提前终止或拒绝执行问题，现有评测体系忽视长度约束指标。

Method: 构建跨4类任务（中英双语）、覆盖16-8192字范围的10,800个测试样本，扩展评估至32K长度，系统测试26个主流模型的长文本生成能力。

Result: 1. 多数模型在短文本表现尚可但存在临界阈值（超阈值后质量骤降）
2. 所有模型实际输出均未达厂商宣称最大长度
3. 长上下文窗口模型未能改善长度控制
4. 推理型模型反超专业长文本生成模型

Conclusion: 当前LLMs存在基础性的长度指令遵循缺陷，该研究为未来模型优化提供了关键方向：需重新思考长度控制机制设计，推理能力与长度控制存在潜在关联。

Abstract: While large language models (LLMs) can solve PhD-level reasoning problems
over long context inputs, they still struggle with a seemingly simpler task:
following explicit length instructions-e.g., write a 10,000-word novel.
Additionally, models often generate far too short outputs, terminate
prematurely, or even refuse the request. Existing benchmarks focus primarily on
evaluating generations quality, but often overlook whether the generations meet
length constraints. To this end, we introduce Length Instruction Following
Evaluation Benchmark (LIFEBench) to comprehensively evaluate LLMs' ability to
follow length instructions across diverse tasks and a wide range of specified
lengths. LIFEBench consists of 10,800 instances across 4 task categories in
both English and Chinese, covering length constraints ranging from 16 to 8192
words. We evaluate 26 widely-used LLMs and find that most models reasonably
follow short-length instructions but deteriorate sharply beyond a certain
threshold. Surprisingly, almost all models fail to reach the vendor-claimed
maximum output lengths in practice, as further confirmed by our evaluations
extending up to 32K words. Even long-context LLMs, despite their extended
input-output windows, counterintuitively fail to improve length-instructions
following. Notably, Reasoning LLMs outperform even specialized long-text
generation models, achieving state-of-the-art length following. Overall,
LIFEBench uncovers fundamental limitations in current LLMs' length instructions
following ability, offering critical insights for future progress.

</details>


### [44] [Align-GRAG: Reasoning-Guided Dual Alignment for Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2505.16237)
*Derong Xu,Pengyue Jia,Xiaopeng Li,Yingyi Zhang,Maolin Wang,Qidong Liu,Xiangyu Zhao,Yichao Wang,Huifeng Guo,Ruiming Tang,Enhong Chen,Tong Xu*

Main category: cs.CL

TL;DR: 提出Align-GRAG框架，通过推理引导的双重对齐机制解决图检索增强生成中的噪声干扰和表示差异问题，提升回答准确性和效率


<details>
  <summary>Details</summary>
Motivation: 现有图RAG方法存在两个关键问题：1）检索过程引入无关节点导致输入冗余 2）图结构与语言模型表示空间不匹配影响知识利用

Method: 在检索后阶段设计双重对齐框架：1）节点对齐通过KL散度损失剪枝无关知识 2）表示对齐使用对比学习建立图-语言统一语义空间

Result: 在GraphQA基准的常识推理、场景图理解和知识图谱推理任务中均取得显著效果，验证方法的有效性

Conclusion: Align-GRAG通过创新的双对齐机制有效提升图RAG性能，为知识增强型大模型提供新解决方案

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities, but
still struggle with issues like hallucinations and outdated information.
Retrieval-augmented generation (RAG) addresses these issues by grounding LLM
outputs in external knowledge with an Information Retrieval (IR) system.
Building on this foundation, graph-based RAG systems go a step further by
retrieving subgraphs, which preserve the relationships between knowledge
entities and provide more comprehensive context. However, graph RAG faces two
challenges: (1) Retrieving relevant information introduces irrelevant nodes
(especially in dense graph databases, where retrieval usually extends to
adjacent nodes), and leads to overly lengthy inputs that hinder efficiency; (2)
The representation gap between graph and language during generation with LLMs
limits the ability to fully leverage graph structures for enhanced
understanding. To address these limitations, we propose Align-GRAG, a novel
reasoning-guided dual alignment framework in post-retrieval phrase. It first
formulates a subgraph by retrieving nodes and edges. Then an Aligner is
proposed to jointly optimizes a graph encoder with LLM-summarized reasoning. It
achieves dual alignment of graph node and representation by leveraging KL
divergence loss and contrastive loss, facilitating efficient pruning of
irrelevant knowledge and establishing a unified semantic space. The Generator
integrates the aligned graph data with LLM to produce coherent and accurate
answers. Experiments on GraphQA benchmark across three tasks (including common
sense reasoning, scene graph understanding, and knowledge graph reasoning)
validate the effectiveness of our method. The code will be available upon
accepted.

</details>


### [45] [Three Minds, One Legend: Jailbreak Large Reasoning Model with Adaptive Stacked Ciphers](https://arxiv.org/abs/2505.16241)
*Viet-Anh Nguyen,Shiqian Zhao,Gia Dao,Runyi Hu,Yi Xie,Luu Anh Tuan*

Main category: cs.CL

TL;DR: 提出SEAL自适应加密攻击方法，通过堆叠加密和动态策略显著提升大型推理模型的越狱成功率


<details>
  <summary>Details</summary>
Motivation: 现有越狱方法难以平衡攻击效果与对抗安全机制的能力，且LRMs的强推理能力可能带来更严重的安全漏洞

Method: 1. 堆叠组合多种加密方式覆盖模型推理
2. 动态调整密码长度/顺序的随机和自适应策略
3. 构建对抗自适应安全对齐的防御规避机制

Result: 在GPT o4-mini上达到80.8%攻击成功率，超出SOTA基线27.2个百分点（DeepSeek-R1/Claude Sonnet验证有效）

Conclusion: SEAL证明LRMs存在严重安全风险，其动态加密策略为对抗性攻击研究提供新方向，需加强推理模型的安全防护

Abstract: Recently, Large Reasoning Models (LRMs) have demonstrated superior logical
capabilities compared to traditional Large Language Models (LLMs), gaining
significant attention. Despite their impressive performance, the potential for
stronger reasoning abilities to introduce more severe security vulnerabilities
remains largely underexplored. Existing jailbreak methods often struggle to
balance effectiveness with robustness against adaptive safety mechanisms. In
this work, we propose SEAL, a novel jailbreak attack that targets LRMs through
an adaptive encryption pipeline designed to override their reasoning processes
and evade potential adaptive alignment. Specifically, SEAL introduces a stacked
encryption approach that combines multiple ciphers to overwhelm the models
reasoning capabilities, effectively bypassing built-in safety mechanisms. To
further prevent LRMs from developing countermeasures, we incorporate two
dynamic strategies - random and adaptive - that adjust the cipher length,
order, and combination. Extensive experiments on real-world reasoning models,
including DeepSeek-R1, Claude Sonnet, and OpenAI GPT-o4, validate the
effectiveness of our approach. Notably, SEAL achieves an attack success rate of
80.8% on GPT o4-mini, outperforming state-of-the-art baselines by a significant
margin of 27.2%. Warning: This paper contains examples of inappropriate,
offensive, and harmful content.

</details>


### [46] [Diverse, not Short: A Length-Controlled Self-Learning Framework for Improving Response Diversity of Language Models](https://arxiv.org/abs/2505.16245)
*Vijeta Deshpande,Debasmita Ghose,John D. Patterson,Roger Beaty,Anna Rumshisky*

Main category: cs.CL

TL;DR: 提出Diverse-NS框架解决语言模型长度偏差问题，通过自学习和长度控制显著提升输出多样性


<details>
  <summary>Details</summary>
Motivation: 现有多样性指标和奖励模型存在系统性长度偏差，导致模型输出表达力受限

Method: 基于自学习框架生成平衡多样性/质量/长度的偏好数据，仅需3000对数据训练模型

Result: 在LLaMA-3.1-8B和Olmo-2系列实现词汇/语义多样性提升，小模型可作大模型的多样性教师

Conclusion: 显式控制长度偏差能有效提升模型输出的多样性和表达力，为创造性生成任务提供新方向

Abstract: Diverse language model responses are crucial for creative generation,
open-ended tasks, and self-improvement training. We show that common diversity
metrics, and even reward models used for preference optimization,
systematically bias models toward shorter outputs, limiting expressiveness. To
address this, we introduce Diverse, not Short (Diverse-NS), a length-controlled
self-learning framework that improves response diversity while maintaining
length parity. By generating and filtering preference data that balances
diversity, quality, and length, Diverse-NS enables effective training using
only 3,000 preference pairs. Applied to LLaMA-3.1-8B and the Olmo-2 family,
Diverse-NS substantially enhances lexical and semantic diversity. We show
consistent improvement in diversity with minor reduction or gains in response
quality on four creative generation tasks: Divergent Associations, Persona
Generation, Alternate Uses, and Creative Writing. Surprisingly, experiments
with the Olmo-2 model family (7B, and 13B) show that smaller models like
Olmo-2-7B can serve as effective "diversity teachers" for larger models. By
explicitly addressing length bias, our method efficiently pushes models toward
more diverse and expressive outputs.

</details>


### [47] [Does Localization Inform Unlearning? A Rigorous Examination of Local Parameter Attribution for Knowledge Unlearning in Language Models](https://arxiv.org/abs/2505.16252)
*Hwiyeong Lee,Uiji Hwang,Hyelim Lim,Taeuk Kim*

Main category: cs.CL

TL;DR: 挑战现有局部遗忘方法的核心假设，证明有效知识消除并不严格依赖特定参数区域


<details>
  <summary>Details</summary>
Motivation: 现有局部遗忘方法假设参数局部性与知识消除效果直接相关，但缺乏对参数更新与遗忘效果因果关系的严谨验证。本文旨在通过实验验证局部参数修改对知识消除的实际贡献度。

Method: 1. 系统回顾现有局部遗忘方法
2. 设计受控实验验证参数修改与遗忘效果的因果关系
3. 采用参数干预技术分析不同参数区域对知识保留的影响

Result: 实验表明有效遗忘所需修改的参数集具有非确定性，参数局部性无法可靠预测知识消除效果。不同初始化条件下，不同参数组合均可实现等效遗忘。

Conclusion: 局部遗忘方法的核心假设存在根本缺陷，有效知识消除需要更灵活的参数更新策略，而非依赖预设的局部参数区域。该发现为开发更可靠的非参数定位遗忘方法提供了理论依据。

Abstract: Large language models often retain unintended content, prompting growing
interest in knowledge unlearning. Recent approaches emphasize localized
unlearning, which restricts parameter updates to specific regions in an effort
to remove target knowledge while preserving unrelated general knowledge.
However, their effectiveness remains uncertain due to the lack of robust and
thorough evaluation of the trade-off between the competing goals of unlearning.
In this paper, we begin by revisiting existing localized unlearning approaches.
We then conduct controlled experiments to rigorously evaluate whether local
parameter updates causally contribute to unlearning. Our findings reveal that
the set of parameters that must be modified for effective unlearning is not
strictly determined, challenging the core assumption of localized unlearning
that parameter locality is inherently indicative of effective knowledge
removal.

</details>


### [48] [IRONIC: Coherence-Aware Reasoning Chains for Multi-Modal Sarcasm Detection](https://arxiv.org/abs/2505.16258)
*Aashish Anantha Ramakrishnan,Aadarsh Anantha Ramakrishnan,Dongwon Lee*

Main category: cs.CL

TL;DR: 论文提出多模态讽刺检测框架IRONIC，通过整合语言学与认知机制，在零样本场景下实现最优性能


<details>
  <summary>Details</summary>
Motivation: 现有链式推理方法未能有效模拟人类识别讽刺的认知过程，需将语言学与认知科学洞见融入多模态推理策略

Method: 提出基于多模态连贯关系（指代/类比/语用）的上下文学习框架，分析图文间的认知关联模式

Result: 在多模态讽刺检测任务中取得零样本场景下的最先进性能，超越各类基线模型

Conclusion: 多模态推理策略需系统整合语言学理论和认知机制，该框架为跨模态比喻语言理解提供了新范式

Abstract: Interpreting figurative language such as sarcasm across multi-modal inputs
presents unique challenges, often requiring task-specific fine-tuning and
extensive reasoning steps. However, current Chain-of-Thought approaches do not
efficiently leverage the same cognitive processes that enable humans to
identify sarcasm. We present IRONIC, an in-context learning framework that
leverages Multi-modal Coherence Relations to analyze referential, analogical
and pragmatic image-text linkages. Our experiments show that IRONIC achieves
state-of-the-art performance on zero-shot Multi-modal Sarcasm Detection across
different baselines. This demonstrates the need for incorporating linguistic
and cognitive insights into the design of multi-modal reasoning strategies. Our
code is available at: https://github.com/aashish2000/IRONIC

</details>


### [49] [Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning](https://arxiv.org/abs/2505.16270)
*Jiaru Zou,Yikun Ban,Zihao Li,Yunzhe Qi,Ruizhong Qiu,Ling Yang,Jingrui He*

Main category: cs.CL

TL;DR: 提出Transformer Copilot框架，通过错误日志记录和Copilot模型校正，提升大语言模型微调效果


<details>
  <summary>Details</summary>
Motivation: 传统微调方法仅最小化生成损失，未能有效利用模型自身的学习信号。受人类通过反思错误改进的启发，希望系统追踪模型错误并针对性优化。

Method: 1. 构建错误日志系统追踪模型错误模式
2. 设计Copilot模型通过logits校正提升推理
3. Pilot-Copilot联合训练范式
4. 融合推断范式实现实时校正

Result: 在12个常识推理/算术/推荐任务基准测试中实现最高34.5%的性能提升，计算开销边际增长且具备强扩展性和迁移性

Conclusion: 该框架为模型微调提供了新范式，通过持续学习错误模式和协同优化机制，显著提升模型性能同时保持高效性。

Abstract: Large language models are typically adapted to downstream tasks through
supervised fine-tuning on domain-specific data. While standard fine-tuning
focuses on minimizing generation loss to optimize model parameters, we take a
deeper step by retaining and leveraging the model's own learning signals,
analogous to how human learners reflect on past mistakes to improve future
performance. We first introduce the concept of Mistake Log to systematically
track the model's learning behavior and recurring errors throughout
fine-tuning. Treating the original transformer-based model as the Pilot, we
correspondingly design a Copilot model to refine the Pilot's inference
performance via logits rectification. We name the overall Pilot-Copilot
framework the Transformer Copilot, which introduces (i) a novel Copilot model
design, (ii) a joint training paradigm where the Copilot continuously learns
from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference
paradigm where the Copilot rectifies the Pilot's logits for enhanced
generation. We provide both theoretical and empirical analyses on our new
learning framework. Experiments on 12 benchmarks spanning commonsense,
arithmetic, and recommendation tasks demonstrate that Transformer Copilot
consistently improves performance by up to 34.5%, while introducing marginal
computational overhead to Pilot models and exhibiting strong scalability and
transferability.

</details>


### [50] [Spontaneous Speech Variables for Evaluating LLMs Cognitive Plausibility](https://arxiv.org/abs/2505.16277)
*Sheng-Fu Wang,Laurent Prevot,Jou-an Chi,Ri-Sheng Huang,Shu-Kai Hsieh*

Main category: cs.CL

TL;DR: 利用自发语音语料库提取语音产出变量（语音缩减、韵律突显），评估不同预训练数据类型的LLM预测能力


<details>
  <summary>Details</summary>
Motivation: 从认知角度理解LLM特性，将传统基于阅读/聆听行为指标（如眼动追踪、脑响应）的模型评估方法扩展到语音产出领域

Method: 1. 从自发语音语料库提取语音缩减和韵律突显变量
2. 测试不同预训练数据（书面、口语、混合类型）模型经微调后的预测能力

Result: 1. 微调后模型预测能力显著超越基线
2. 口语类型训练数据预测准确度优于书面数据

Conclusion: 证实高质量语音语料库可作为LLM评估基准，强调口语数据对语音产出变量预测的重要性

Abstract: The achievements of Large Language Models in Natural Language Processing,
especially for high-resource languages, call for a better understanding of
their characteristics from a cognitive perspective. Researchers have attempted
to evaluate artificial models by testing their ability to predict behavioral
(e.g., eye-tracking fixations) and physiological (e.g., brain responses)
variables during language processing (e.g., reading/listening). In this paper,
we propose using spontaneous speech corpora to derive production variables
(speech reductions, prosodic prominences) and applying them in a similar
fashion. More precisely, we extract. We then test models trained with a
standard procedure on different pretraining datasets (written, spoken, and
mixed genres) for their ability to predict these two variables. Our results
show that, after some fine-tuning, the models can predict these production
variables well above baselines. We also observe that spoken genre training data
provides more accurate predictions than written genres. These results
contribute to the broader effort of using high-quality speech corpora as
benchmarks for LLMs.

</details>


### [51] [HiMATE: A Hierarchical Multi-Agent Framework for Machine Translation Evaluation](https://arxiv.org/abs/2505.16281)
*Shijie Zhang,Renhao Li,Songsheng Wang,Philipp Koehn,Min Yang,Derek F. Wong*

Main category: cs.CL

TL;DR: 提出HiMATE分层多智能体框架，通过引入自我反思机制和智能体讨论策略，显著提升机器翻译评估中错误定位与严重性判断的准确性，F1分数较基线提升89%。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法在错误范围识别和严重性评估方面存在不足，且未充分利用MQM错误类型学的层次化结构信息。

Method: 基于MQM错误类型学构建分层多智能体系统，结合模型自反思能力和非对称信息下的智能体讨论机制，实现细粒度错误子类型评估。

Result: 在多个数据集上超越基线模型，错误跨度检测F1值平均提升89%，尤其在严重性评估维度展现显著优势。

Conclusion: HiMATE通过层次化任务分解和系统性幻觉抑制策略，成功实现了更人类对齐的机器翻译质量评估。

Abstract: The advancement of Large Language Models (LLMs) enables flexible and
interpretable automatic evaluations. In the field of machine translation
evaluation, utilizing LLMs with translation error annotations based on
Multidimensional Quality Metrics (MQM) yields more human-aligned judgments.
However, current LLM-based evaluation methods still face challenges in
accurately identifying error spans and assessing their severity. In this paper,
we propose HiMATE, a Hierarchical Multi-Agent Framework for Machine Translation
Evaluation. We argue that existing approaches inadequately exploit the
fine-grained structural and semantic information within the MQM hierarchy. To
address this, we develop a hierarchical multi-agent system grounded in the MQM
error typology, enabling granular evaluation of subtype errors. Two key
strategies are incorporated to further mitigate systemic hallucinations within
the framework: the utilization of the model's self-reflection capability and
the facilitation of agent discussion involving asymmetric information.
Empirically, HiMATE outperforms competitive baselines across different datasets
in conducting human-aligned evaluations. Further analyses underscore its
significant advantage in error span detection and severity assessment,
achieving an average F1-score improvement of 89% over the best-performing
baseline. We make our code and data publicly available at
https://anonymous.4open.science/r/HiMATE-Anony.

</details>


### [52] [Augmenting LLM Reasoning with Dynamic Notes Writing for Complex QA](https://arxiv.org/abs/2505.16293)
*Rishabh Maheshwary,Masoud Hashemi,Khyati Mahajan,Shiva Krishna Reddy Malay,Sai Rajeswar,Sathwik Tejaswi Madhusudhan,Spandana Gella,Vikas Yadav*

Main category: cs.CL

TL;DR: 提出Notes Writing方法，通过生成简洁笔记减少信息冗余，提升迭代RAG性能


<details>
  <summary>Details</summary>
Motivation: 现有迭代RAG方法在处理长上下文时存在信息冗余问题，影响模型推理能力，需开发兼容多轮迭代的压缩方法

Method: 在每次检索后生成结构化笔记，保留关键信息并过滤噪音，兼容多种迭代RAG框架

Result: 集成3种迭代RAG方法，在4个数据集平均提升15.6个百分点，输出标记仅少量增加

Conclusion: Notes Writing通过信息提炼显著提升多跳问答效果，且框架通用性强

Abstract: Iterative RAG for multi-hop question answering faces challenges with lengthy
contexts and the buildup of irrelevant information. This hinders a model's
capacity to process and reason over retrieved content and limits performance.
While recent methods focus on compressing retrieved information, they are
either restricted to single-round RAG, require finetuning or lack scalability
in iterative RAG. To address these challenges, we propose Notes Writing, a
method that generates concise and relevant notes from retrieved documents at
each step, thereby reducing noise and retaining only essential information.
This indirectly increases the effective context length of Large Language Models
(LLMs), enabling them to reason and plan more effectively while processing
larger volumes of input text. Notes Writing is framework agnostic and can be
integrated with different iterative RAG methods. We demonstrate its
effectiveness with three iterative RAG methods, across two models and four
evaluation datasets. Notes writing yields an average improvement of 15.6
percentage points overall, with minimal increase in output tokens.

</details>


### [53] [ToDi: Token-wise Distillation via Fine-Grained Divergence Control](https://arxiv.org/abs/2505.16297)
*Seongryong Jung,Suwan Yoon,DongGeon Kim,Hwanhee Lee*

Main category: cs.CL

TL;DR: 提出自适应token级别的知识蒸馏方法ToDi，通过动态组合FKL和RKL提升小模型性能


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法在词汇表层面使用统一损失函数，忽略了token级别的预测差异，导致知识迁移效率低下

Method: 基于教师-学生概率对数比设计sigmoid权重函数，动态调整FKL（提升低估token）和RKL（抑制高估token）的组合比例

Result: 在指令跟随基准测试中持续超越现有蒸馏方法，消融实验验证了动态组合策略的有效性

Conclusion: ToDi通过细粒度token级别蒸馏策略，成功实现精确的分布对齐，兼具算法效果和部署实用性

Abstract: Large language models (LLMs) offer impressive performance but are impractical
for resource-constrained deployment due to high latency and energy consumption.
Knowledge distillation (KD) addresses this by transferring knowledge from a
large teacher to a smaller student model. However, conventional KD, notably
approaches like Forward KL (FKL) and Reverse KL (RKL), apply uniform divergence
loss across the entire vocabulary, neglecting token-level prediction
discrepancies. By investigating these representative divergences via gradient
analysis, we reveal that FKL boosts underestimated tokens, while RKL suppresses
overestimated ones, showing their complementary roles. Based on this
observation, we propose Token-wise Distillation (ToDi), a novel method that
adaptively combines FKL and RKL per token using a sigmoid-based weighting
function derived from the teacher-student probability log-ratio. ToDi
dynamically emphasizes the appropriate divergence for each token, enabling
precise distribution alignment. We demonstrate that ToDi consistently
outperforms recent distillation baselines using uniform or less granular
strategies across instruction-following benchmarks. Extensive ablation studies
and efficiency analysis further validate ToDi's effectiveness and practicality.

</details>


### [54] [INFERENCEDYNAMICS: Efficient Routing Across LLMs through Structured Capability and Knowledge Profiling](https://arxiv.org/abs/2505.16303)
*Haochen Shi,Tianshi Zheng,Weiqi Wang,Baixuan Xu,Chunyang Li,Chunkit Chan,Tao Fan,Yangqiu Song,Qiang Yang*

Main category: cs.CL

TL;DR: 提出灵活可扩展的多维度路由框架InferenceDynamics，通过建模LLM能力和知识实现高效模型选择与资源管理


<details>
  <summary>Details</summary>
Motivation: 现有LLM路由方法在大规模专用模型池中面临扩展性不足、难以适应模型范围扩展和能力域演化的问题

Method: 基于模型能力建模构建多维度路由框架，在RouteMix综合数据集上验证，支持MMLU-Pro/GPQA/BigGenBench/LiveBench等现代基准测试

Result: 在组级路由中展现优异性能，能够有效识别并利用任务最优模型，实现资源高效利用下的性能优势

Conclusion: InferenceDynamics框架的广泛应用将释放LLM生态系统的专业潜力，其开源部署将促进相关研究发展

Abstract: Large Language Model (LLM) routing is a pivotal technique for navigating a
diverse landscape of LLMs, aiming to select the best-performing LLMs tailored
to the domains of user queries, while managing computational resources.
However, current routing approaches often face limitations in scalability when
dealing with a large pool of specialized LLMs, or in their adaptability to
extending model scope and evolving capability domains. To overcome those
challenges, we propose InferenceDynamics, a flexible and scalable
multi-dimensional routing framework by modeling the capability and knowledge of
models. We operate it on our comprehensive dataset RouteMix, and demonstrate
its effectiveness and generalizability in group-level routing using modern
benchmarks including MMLU-Pro, GPQA, BigGenBench, and LiveBench, showcasing its
ability to identify and leverage top-performing models for given tasks, leading
to superior outcomes with efficient resource utilization. The broader adoption
of Inference Dynamics can empower users to harness the full specialized
potential of the LLM ecosystem, and our code will be made publicly available to
encourage further research.

</details>


### [55] [PMPO: Probabilistic Metric Prompt Optimization for Small and Large Language Models](https://arxiv.org/abs/2505.16307)
*Chenzhuo Zhao,Ziqian Liu,Xingda Wang,Junting Lu,Chaoyi Ruan*

Main category: cs.CL

TL;DR: 提出PMPO框架，通过交叉熵损失直接优化提示，无需采样或人工评估，显著提升多任务模型表现


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法依赖高成本输出生成或人工标注，限制了在小型/非指令微调模型上的扩展性。需更轻量级的优化方案

Method: 1. 通过掩码定位低质量提示段 2. 用正负样本的交叉熵损失指导提示改写 3. 仅需前向传播和概率计算，避免输出采样

Result: BBH平均准确率最高/GSM8K表现优异/AlpacaEval 2.0胜率提升19+点/多模型规模验证有效性

Conclusion: PMPO首次实现纯概率驱动的提示优化，为轻量化模型优化提供新范式，在效率-效果平衡上取得突破

Abstract: Prompt optimization offers a practical and broadly applicable alternative to
fine-tuning for improving large language model (LLM) performance. However,
existing methods often rely on costly output generation, self-critiquing
abilities, or human-annotated preferences, which limit their scalability,
especially for smaller or non-instruction-tuned models. We introduce PMPO
(Probabilistic Metric Prompt Optimization), a unified framework that refines
prompts using token-level cross-entropy loss as a direct, lightweight
evaluation signal. PMPO identifies low-quality prompt segments by masking and
measuring their impact on loss, then rewrites and selects improved variants by
minimizing loss over positive and negative examples. Unlike prior methods, it
requires no output sampling or human evaluation during optimization, relying
only on forward passes and log-likelihoods. PMPO supports both supervised and
preference-based tasks through a closely aligned loss-based evaluation
strategy. Experiments show that PMPO consistently outperforms prior methods
across model sizes and tasks: it achieves the highest average accuracy on BBH,
performs strongly on GSM8K and AQUA-RAT, and improves AlpacaEval 2.0 win rates
by over 19 points. These results highlight PMPO's effectiveness, efficiency,
and broad applicability.

</details>


### [56] [CLEAR: A Clinically-Grounded Tabular Framework for Radiology Report Evaluation](https://arxiv.org/abs/2505.16325)
*Yuyang Jiang,Chacha Chen,Shengyuan Wang,Feng Li,Zecong Tang,Benjamin M. Mervak,Lydia Chelala,Christopher M Straus,Reve Chahine,Samuel G. Armato III,Chenhao Tan*

Main category: cs.CL

TL;DR: 提出CLEAR框架用于放射学报告评估，通过多维度属性比较提升临床判断一致性


<details>
  <summary>Details</summary>
Motivation: 现有放射学报告评估指标缺乏临床差异的细粒度捕捉，导致评估效果不理想

Method: 开发CLEAR框架（包含专家标注属性和表格化评估体系）及CLEAR-Bench基准数据集（含100份标注报告）

Result: CLEAR实现高精度临床属性提取（93%准确率），评估指标与放射科医生判断强相关（Spearman相关系数>0.8）

Conclusion: 多维度属性级评估框架显著提升放射学报告质量评估的临床相关性和可解释性

Abstract: Existing metrics often lack the granularity and interpretability to capture
nuanced clinical differences between candidate and ground-truth radiology
reports, resulting in suboptimal evaluation. We introduce a Clinically-grounded
tabular framework with Expert-curated labels and Attribute-level comparison for
Radiology report evaluation (CLEAR). CLEAR not only examines whether a report
can accurately identify the presence or absence of medical conditions, but also
assesses whether it can precisely describe each positively identified condition
across five key attributes: first occurrence, change, severity, descriptive
location, and recommendation. Compared to prior works, CLEAR's
multi-dimensional, attribute-level outputs enable a more comprehensive and
clinically interpretable evaluation of report quality. Additionally, to measure
the clinical alignment of CLEAR, we collaborate with five board-certified
radiologists to develop CLEAR-Bench, a dataset of 100 chest X-ray reports from
MIMIC-CXR, annotated across 6 curated attributes and 13 CheXpert conditions.
Our experiments show that CLEAR achieves high accuracy in extracting clinical
attributes and provides automated metrics that are strongly aligned with
clinical judgment.

</details>


### [57] [SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers](https://arxiv.org/abs/2505.16330)
*Wenqing Wu,Chengzhi Zhang,Tong Bao,Yi Zhao*

Main category: cs.CL

TL;DR: 研究表明利用引言和结果章节（IR组合）能有效预测论文新颖性分数，而全文输入效果不佳


<details>
  <summary>Details</summary>
Motivation: 现有基于词汇组合的新颖性评估方法存在局限，论文创新性地探索不同章节组合对自动化评估的影响

Method: 使用NLP技术识别IMRaD结构，将不同章节组合输入预训练模型，以专家评分作为标签进行预测

Result: 引言+结果+讨论（IRD）组合效果最优，全文输入无显著效果，PLMs和LLMs均显示引言和结果最关键

Conclusion: 章节组合策略显著影响评估效果，推荐使用IRD组合，相关代码和数据集已开源供研究使用

Abstract: Novelty is a core component of academic papers, and there are multiple
perspectives on the assessment of novelty. Existing methods often focus on word
or entity combinations, which provide limited insights. The content related to
a paper's novelty is typically distributed across different core sections,
e.g., Introduction, Methodology and Results. Therefore, exploring the optimal
combination of sections for evaluating the novelty of a paper is important for
advancing automated novelty assessment. In this paper, we utilize different
combinations of sections from academic papers as inputs to drive language
models to predict novelty scores. We then analyze the results to determine the
optimal section combinations for novelty score prediction. We first employ
natural language processing techniques to identify the sectional structure of
academic papers, categorizing them into introduction, methods, results, and
discussion (IMRaD). Subsequently, we used different combinations of these
sections (e.g., introduction and methods) as inputs for pretrained language
models (PLMs) and large language models (LLMs), employing novelty scores
provided by human expert reviewers as ground truth labels to obtain prediction
results. The results indicate that using introduction, results and discussion
is most appropriate for assessing the novelty of a paper, while the use of the
entire text does not yield significant results. Furthermore, based on the
results of the PLMs and LLMs, the introduction and results appear to be the
most important section for the task of novelty score prediction. The code and
dataset for this paper can be accessed at
https://github.com/njust-winchy/SC4ANM.

</details>


### [58] [Embodied Agents Meet Personalization: Exploring Memory Utilization for Personalized Assistance](https://arxiv.org/abs/2505.16348)
*Taeyoon Kwon,Dongwook Choi,Sunghwan Kim,Hyojun Kim,Seungjun Moon,Beong-woo Kwak,Kuan-Hao Huang,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 提出MEMENTO框架评估具身代理利用记忆实现个性化协助的能力，发现主流模型在需参考多记忆时性能显著下降（如GPT-4o下降30.5%）


<details>
  <summary>Details</summary>
Motivation: 现有具身代理研究集中于单轮简单指令任务，未深入探究如何通过历史交互记忆理解用户个性化语义（如专属物品含义、日常习惯），限制了真正的个性化协助能力

Method: 设计两阶段记忆评估框架：1）通过对象语义理解识别个性化目标物体 2）通过用户模式分析推断物体-位置配置规律，量化记忆利用对任务表现的影响

Result: 实验显示模型在用户模式任务中表现最弱（GPT-4o仅64.5%准确率），多记忆引用导致性能显著下降（平均下降30.5%），对象语义任务准确率相对较高（75.8%）

Conclusion: 当前具身代理的个性化记忆利用能力存在显著局限，需开发更先进的记忆架构和推理机制。MEMENTO框架为未来研究提供系统性评估基准

Abstract: Embodied agents empowered by large language models (LLMs) have shown strong
performance in household object rearrangement tasks. However, these tasks
primarily focus on single-turn interactions with simplified instructions, which
do not truly reflect the challenges of providing meaningful assistance to
users. To provide personalized assistance, embodied agents must understand the
unique semantics that users assign to the physical world (e.g., favorite cup,
breakfast routine) by leveraging prior interaction history to interpret
dynamic, real-world instructions. Yet, the effectiveness of embodied agents in
utilizing memory for personalized assistance remains largely underexplored. To
address this gap, we present MEMENTO, a personalized embodied agent evaluation
framework designed to comprehensively assess memory utilization capabilities to
provide personalized assistance. Our framework consists of a two-stage memory
evaluation process design that enables quantifying the impact of memory
utilization on task performance. This process enables the evaluation of agents'
understanding of personalized knowledge in object rearrangement tasks by
focusing on its role in goal interpretation: (1) the ability to identify target
objects based on personal meaning (object semantics), and (2) the ability to
infer object-location configurations from consistent user patterns, such as
routines (user patterns). Our experiments across various LLMs reveal
significant limitations in memory utilization, with even frontier models like
GPT-4o experiencing a 30.5% performance drop when required to reference
multiple memories, particularly in tasks involving user patterns. These
findings, along with our detailed analyses and case studies, provide valuable
insights for future research in developing more effective personalized embodied
agents. Project website: https://connoriginal.github.io/MEMENTO

</details>


### [59] [Ask, Retrieve, Summarize: A Modular Pipeline for Scientific Literature Summarization](https://arxiv.org/abs/2505.16349)
*Pierre Achkar,Tim Gollub,Martin Potthast*

Main category: cs.CL

TL;DR: XSum是基于检索增强生成（RAG）的科学领域多文档摘要框架，通过动态问题生成和编辑模块提升摘要质量，评估显示其在多项指标上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 科学文献的指数级增长使研究者难以有效追踪和整合知识，亟需高效的自动化摘要工具。

Method: 采用模块化管道设计：1）动态生成适配输入论文的问题模块，确保精准信息检索；2）编辑模块整合内容为结构化学术摘要，严格遵守引用规范。

Result: 在SurveySum数据集测试中，XSum在CheckEval（+15%）、G-Eval（+12%）和Ref-F1（+20%）等关键指标上显著超越基线方法。

Conclusion: 该框架为科学文献摘要提供了透明且可扩展的解决方案，其模块化设计支持跨领域应用，开源代码将推动后续研究。

Abstract: The exponential growth of scientific publications has made it increasingly
difficult for researchers to stay updated and synthesize knowledge effectively.
This paper presents XSum, a modular pipeline for multi-document summarization
(MDS) in the scientific domain using Retrieval-Augmented Generation (RAG). The
pipeline includes two core components: a question-generation module and an
editor module. The question-generation module dynamically generates questions
adapted to the input papers, ensuring the retrieval of relevant and accurate
information. The editor module synthesizes the retrieved content into coherent
and well-structured summaries that adhere to academic standards for proper
citation. Evaluated on the SurveySum dataset, XSum demonstrates strong
performance, achieving considerable improvements in metrics such as CheckEval,
G-Eval and Ref-F1 compared to existing approaches. This work provides a
transparent, adaptable framework for scientific summarization with potential
applications in a wide range of domains. Code available at
https://github.com/webis-de/scolia25-xsum

</details>


### [60] [PaTH Attention: Position Encoding via Accumulating Householder Transformations](https://arxiv.org/abs/2505.16381)
*Songlin Yang,Yikang Shen,Kaiyue Wen,Shawn Tan,Mayank Mishra,Liliang Ren,Rameswar Panda,Yoon Kim*

Main category: cs.CL

TL;DR: 提出PaTH数据依赖型位置编码方案，通过Householder变换提升注意力机制表达能力，实验显示优于主流RoPE方法。


<details>
  <summary>Details</summary>
Motivation: 传统RoPE的键/查询变换仅依赖相对位置且与输入无关，限制了Transformer的表达能力。

Method: 基于Householder变换构建数据依赖位置编码，结合高效并行算法与FlashAttention式I/O优化。

Result: 在合成基准和实际语言建模任务中，PaTH性能超越RoPE及其他基线方法。

Conclusion: PaTH通过输入依赖的位置编码和计算优化，为LLM位置表示提供了更灵活的解决方案。

Abstract: The attention mechanism is a core primitive in modern large language models
(LLMs) and AI more broadly. Since attention by itself is permutation-invariant,
position encoding is essential for modeling structured domains such as
language. Rotary position encoding (RoPE) has emerged as the de facto standard
approach for position encoding and is part of many modern LLMs. However, in
RoPE the key/query transformation between two elements in a sequence is only a
function of their relative position and otherwise independent of the actual
input. This limits the expressivity of RoPE-based transformers.
  This paper describes PaTH, a flexible data-dependent position encoding scheme
based on accumulated products of Householder(like) transformations, where each
transformation is data-dependent, i.e., a function of the input. We derive an
efficient parallel algorithm for training through exploiting a compact
representation of products of Householder matrices, and implement a
FlashAttention-style blockwise algorithm that minimizes I/O cost. Across both
targeted synthetic benchmarks and moderate-scale real-world language modeling
experiments, we find that PaTH demonstrates superior performance compared to
RoPE and other recent baselines.

</details>


### [61] [Semantic Pivots Enable Cross-Lingual Transfer in Large Language Models](https://arxiv.org/abs/2505.16385)
*Kaiyu He,Tong Zhou,Yubo Chen,Delai Qiu,Shengping Liu,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 提出词级跨语言翻译任务量化大语言模型跨语言能力，发现共现行为和语义枢纽行为，并通过语义枢纽感知的预训练数据集增强模型表现。


<details>
  <summary>Details</summary>
Motivation: 为准确量化大语言模型(LLMs)的跨语言能力形成机制，探究其预训练阶段的学习机理。

Method: 设计词级跨语言翻译任务，追踪模型中间层输出，分离共现行为和语义枢纽行为，重构高语义枢纽比例的预训练数据集。

Result: 实验验证该方法有效提升模型跨语言能力，共现频率解释模型行为差异，语义枢纽来源于预训练数据。

Conclusion: 研究揭示了LLMs跨语言能力的形成机制，为模型可解释性提供新视角，并提出通过数据优化提升跨语言能力的方法论。

Abstract: Large language models (LLMs) demonstrate remarkable ability in cross-lingual
tasks. Understanding how LLMs acquire this ability is crucial for their
interpretability. To quantify the cross-lingual ability of LLMs accurately, we
propose a Word-Level Cross-Lingual Translation Task. To find how LLMs learn
cross-lingual ability, we trace the outputs of LLMs' intermediate layers in the
word translation task. We identify and distinguish two distinct behaviors in
the forward pass of LLMs: co-occurrence behavior and semantic pivot behavior.
We attribute LLMs' two distinct behaviors to the co-occurrence frequency of
words and find the semantic pivot from the pre-training dataset. Finally, to
apply our findings to improve the cross-lingual ability of LLMs, we reconstruct
a semantic pivot-aware pre-training dataset using documents with a high
proportion of semantic pivots. Our experiments validate the effectiveness of
our approach in enhancing cross-lingual ability. Our research contributes
insights into the interpretability of LLMs and offers a method for improving
LLMs' cross-lingual ability.

</details>


### [62] [Resource for Error Analysis in Text Simplification: New Taxonomy and Test Collection](https://arxiv.org/abs/2505.16392)
*Benjamin Vendeville,Liana Ermakova,Pierre De Loor*

Main category: cs.CL

TL;DR: 该论文提出文本简化错误分类体系与标注数据集，解决自动文本简化（ATS）评估方法滞后的问题


<details>
  <summary>Details</summary>
Motivation: 当前ATS评估指标与错误检测脱节，人工检查发现多种信息失真错误，需要建立细粒度评估框架

Method: 1. 构建以信息失真为核心的错误分类体系
2. 创建自动简化科学文本的平行语料库并人工标注
3. 测试现有模型在错误检测分类中的表现

Result: 提供带标注的数据集和分类体系，揭示现有模型在错误检测中的性能局限

Conclusion: 该资源为开发更可靠的ATS模型提供评估工具，最终提升自动简化文本质量

Abstract: The general public often encounters complex texts but does not have the time
or expertise to fully understand them, leading to the spread of misinformation.
Automatic Text Simplification (ATS) helps make information more accessible, but
its evaluation methods have not kept up with advances in text generation,
especially with Large Language Models (LLMs). In particular, recent studies
have shown that current ATS metrics do not correlate with the presence of
errors. Manual inspections have further revealed a variety of errors,
underscoring the need for a more nuanced evaluation framework, which is
currently lacking. This resource paper addresses this gap by introducing a test
collection for detecting and classifying errors in simplified texts. First, we
propose a taxonomy of errors, with a formal focus on information distortion.
Next, we introduce a parallel dataset of automatically simplified scientific
texts. This dataset has been human-annotated with labels based on our proposed
taxonomy. Finally, we analyze the quality of the dataset, and we study the
performance of existing models to detect and classify errors from that
taxonomy. These contributions give researchers the tools to better evaluate
errors in ATS, develop more reliable models, and ultimately improve the quality
of automatically simplified texts.

</details>


### [63] [On the reliability of feature attribution methods for speech classification](https://arxiv.org/abs/2505.16406)
*Gaofei Shen,Hosein Mohebbi,Arianna Bisazza,Afra Alishahi,Grzegorz Chrupała*

Main category: cs.CL

TL;DR: 语音处理中标准特征归因方法普遍不可靠，仅在基于单词分类任务中使用单词对齐扰动时有效


<details>
  <summary>Details</summary>
Motivation: 研究预训练模型特征归因方法在语音领域的适用性，因语音信号的特殊性导致传统方法面临挑战

Method: 通过分析输入类型、聚合方式、扰动时间跨度与分类任务特性的相互作用，评估多种特征归因方法的可靠性

Result: 标准特征归因方法在语音领域整体不可靠，仅单词对齐扰动方法在基于单词的分类任务中表现可靠

Conclusion: 语音处理需针对任务特性开发专用特征归因方法，现有方法仅在特定单词对齐场景有效

Abstract: As the capabilities of large-scale pre-trained models evolve, understanding
the determinants of their outputs becomes more important. Feature attribution
aims to reveal which parts of the input elements contribute the most to model
outputs. In speech processing, the unique characteristics of the input signal
make the application of feature attribution methods challenging. We study how
factors such as input type and aggregation and perturbation timespan impact the
reliability of standard feature attribution methods, and how these factors
interact with characteristics of each classification task. We find that
standard approaches to feature attribution are generally unreliable when
applied to the speech domain, with the exception of word-aligned perturbation
methods when applied to word-based classification tasks.

</details>


### [64] [From Surveys to Narratives: Rethinking Cultural Value Adaptation in LLMs](https://arxiv.org/abs/2505.16408)
*Muhammad Farid Adilazuarda,Chen Cecilia Liu,Iryna Gurevych,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 研究探讨基于世界价值观调查(WVS)训练的文化适应方法局限性，发现结合百科/场景叙事数据可提升文化表征独特性


<details>
  <summary>Details</summary>
Motivation: 现有方法仅用WVS数据易导致文化同质化并干扰事实知识，需要探索更有效的文化价值适应方法

Method: 整合维基百科的百科文化叙事和NormAd的场景叙事数据，与WVS数据共同训练模型

Result: 混合叙事数据虽对下游任务效果不稳定，但比纯调查数据能更显著提升文化表征区分度

Conclusion: 文化价值对齐需平衡任务目标与知识保留，不同数据源的组合应用具有重要指导意义

Abstract: Adapting cultural values in Large Language Models (LLMs) presents significant
challenges, particularly due to biases and limited training data. Prior work
primarily aligns LLMs with different cultural values using World Values Survey
(WVS) data. However, it remains unclear whether this approach effectively
captures cultural nuances or produces distinct cultural representations for
various downstream tasks. In this paper, we systematically investigate
WVS-based training for cultural value adaptation and find that relying solely
on survey data can homogenize cultural norms and interfere with factual
knowledge. To investigate these issues, we augment WVS with encyclopedic and
scenario-based cultural narratives from Wikipedia and NormAd. While these
narratives may have variable effects on downstream tasks, they consistently
improve cultural distinctiveness than survey data alone. Our work highlights
the inherent complexity of aligning cultural values with the goal of guiding
task-specific behavior.

</details>


### [65] [Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning](https://arxiv.org/abs/2505.16410)
*Guanting Dong,Yifei Chen,Xiaoxi Li,Jiajie Jin,Hongjin Qian,Yutao Zhu,Hangyu Mao,Guorui Zhou,Zhicheng Dou,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 提出强化学习框架Tool-Star，通过工具集成数据合成和两阶段训练增强大语言模型的多工具协作推理能力


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在基于强化学习实现多工具协同推理方面存在数据稀缺和协作效率低的挑战

Method: 1. 工具集成数据合成流程（提示+采样）
2. 质量分级与难度分类机制
3. 冷启动微调+层次化奖励的自我批评强化学习算法

Result: 在10+复杂推理基准测试中验证有效性，模型推理效率提升显著

Conclusion: Tool-Star通过系统性数据生成和分层强化训练，成功实现多工具自主协作推理的突破

Abstract: Recently, large language models (LLMs) have shown remarkable reasoning
capabilities via large-scale reinforcement learning (RL). However, leveraging
the RL algorithm to empower effective multi-tool collaborative reasoning in
LLMs remains an open challenge. In this paper, we introduce Tool-Star, an
RL-based framework designed to empower LLMs to autonomously invoke multiple
external tools during stepwise reasoning. Tool-Star integrates six types of
tools and incorporates systematic designs in both data synthesis and training.
To address the scarcity of tool-use data, we propose a general tool-integrated
reasoning data synthesis pipeline, which combines tool-integrated prompting
with hint-based sampling to automatically and scalably generate tool-use
trajectories. A subsequent quality normalization and difficulty-aware
classification process filters out low-quality samples and organizes the
dataset from easy to hard. Furthermore, we propose a two-stage training
framework to enhance multi-tool collaborative reasoning by: (1) cold-start
fine-tuning, which guides LLMs to explore reasoning patterns via
tool-invocation feedback; and (2) a multi-tool self-critic RL algorithm with
hierarchical reward design, which reinforces reward understanding and promotes
effective tool collaboration. Experimental analyses on over 10 challenging
reasoning benchmarks highlight the effectiveness and efficiency of Tool-Star.
The code is available at https://github.com/dongguanting/Tool-Star.

</details>


### [66] [Attributing Response to Context: A Jensen-Shannon Divergence Driven Mechanistic Study of Context Attribution in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.16415)
*Ruizhe Li,Chen Chen,Yuchen Hu,Yanjun Gao,Xi Wang,Emine Yilmaz*

Main category: cs.CL

TL;DR: 提出基于Jensen-Shannon散度的ARC-JSD方法，实现无需微调的高效上下文归因，显著提升RAG模型的准确性和计算效率


<details>
  <summary>Details</summary>
Motivation: 现有RAG模型的上下文归因方法存在计算成本高、依赖人工标注或代理模型的缺陷，限制了实际应用效率

Method: 利用Jensen-Shannon散度直接量化模型响应与上下文句子的分布差异，通过注意力机制和MLP层的激活模式识别关键上下文

Result: 在TyDi QA等基准测试中，7B参数模型实现91.3%的归因准确率（比代理方法提升23.6%），推理速度提高4.8倍

Conclusion: ARC-JSD不仅提供高效精确的归因方案，还通过机制分析揭示了注意力头和MLP层在上下文整合中的具体作用模式

Abstract: Retrieval-Augmented Generation (RAG) leverages large language models (LLMs)
combined with external contexts to enhance the accuracy and reliability of
generated responses. However, reliably attributing generated content to
specific context segments, context attribution, remains challenging due to the
computationally intensive nature of current methods, which often require
extensive fine-tuning or human annotation. In this work, we introduce a novel
Jensen-Shannon Divergence driven method to Attribute Response to Context
(ARC-JSD), enabling efficient and accurate identification of essential context
sentences without additional fine-tuning or surrogate modelling. Evaluations on
a wide range of RAG benchmarks, such as TyDi QA, Hotpot QA, and Musique, using
instruction-tuned LLMs in different scales demonstrate superior accuracy and
significant computational efficiency improvements compared to the previous
surrogate-based method. Furthermore, our mechanistic analysis reveals specific
attention heads and multilayer perceptron (MLP) layers responsible for context
attribution, providing valuable insights into the internal workings of RAG
models.

</details>


### [67] [Exploring the Relationship Between Diversity and Quality in Ad Text Generation](https://arxiv.org/abs/2505.16418)
*Yoichi Aoki,Soichiro Murakami,Ukyo Honda,Akihiko Kato*

Main category: cs.CL

TL;DR: 探讨广告文本生成中多样性增强方法对广告质量的影响及其适用性


<details>
  <summary>Details</summary>
Motivation: 广告文本生成在文本风格和需求上与其他生成任务存在显著差异，现有多样性增强方法在该领域的应用效果尚未明确

Method: 通过多维度实验框架（多样性增强方法、超参数、输入输出格式、模型架构）进行系统性研究

Result: 揭示不同多样性策略与广告质量指标间的非线性关系，证明需针对广告场景优化模型参数和输入结构

Conclusion: 广告文本生成需定制化多样性增强方案，综合考虑任务特性和模型架构才能有效平衡创意与商业目标

Abstract: In natural language generation for advertising, creating diverse and engaging
ad texts is crucial for capturing a broad audience and avoiding advertising
fatigue. Regardless of the importance of diversity, the impact of the
diversity-enhancing methods in ad text generation -- mainly tested on tasks
such as summarization and machine translation -- has not been thoroughly
explored. Ad text generation significantly differs from these tasks owing to
the text style and requirements. This research explores the relationship
between diversity and ad quality in ad text generation by considering multiple
factors, such as diversity-enhancing methods, their hyperparameters,
input-output formats, and the models.

</details>


### [68] [WebAgent-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.16421)
*Zhepei Wei,Wenlin Yao,Yao Liu,Weizhi Zhang,Qin Lu,Liang Qiu,Changlong Yu,Puyang Xu,Chao Zhang,Bing Yin,Hyokun Yun,Lihong Li*

Main category: cs.CL

TL;DR: 提出WebAgent-R1强化学习框架，显著提升多轮网页交互任务的智能体成功率


<details>
  <summary>Details</summary>
Motivation: 现有RL技术主要针对单轮任务，动态网页多轮决策的复杂性导致网页智能体训练困难

Method: 端到端多轮RL框架，通过异步生成多样化轨迹，完全依赖任务成功的二元奖励进行在线训练

Result: Qwen-2.5-3B成功率从6.1%提升至33.9%，Llama-3.1-8B从8.5%提升至44.8%，超越现有SOTA方法

Conclusion: 验证了思考型提示策略的有效性，揭示初始化策略中行为克隆阶段的重要性，为整合长链推理提供新方向

Abstract: While reinforcement learning (RL) has demonstrated remarkable success in
enhancing large language models (LLMs), it has primarily focused on single-turn
tasks such as solving math problems. Training effective web agents for
multi-turn interactions remains challenging due to the complexity of
long-horizon decision-making across dynamic web interfaces. In this work, we
present WebAgent-R1, a simple yet effective end-to-end multi-turn RL framework
for training web agents. It learns directly from online interactions with web
environments by asynchronously generating diverse trajectories, entirely guided
by binary rewards depending on task success. Experiments on the WebArena-Lite
benchmark demonstrate the effectiveness of WebAgent-R1, boosting the task
success rate of Qwen-2.5-3B from 6.1% to 33.9% and Llama-3.1-8B from 8.5% to
44.8%, significantly outperforming existing state-of-the-art methods and strong
proprietary models such as OpenAI o3. In-depth analyses reveal the
effectiveness of the thinking-based prompting strategy and test-time scaling
through increased interactions for web tasks. We further investigate different
RL initialization policies by introducing two variants, namely WebAgent-R1-Zero
and WebAgent-R1-CoT, which highlight the importance of the warm-up training
stage (i.e., behavior cloning) and provide insights on incorporating long
chain-of-thought (CoT) reasoning in web agents.

</details>


### [69] [$I^2G$: Generating Instructional Illustrations via Text-Conditioned Diffusion](https://arxiv.org/abs/2505.16425)
*Jing Bi,Pinxin Liu,Ali Vosoughi,Jiarui Wu,Jinxi He,Chenliang Xu*

Main category: cs.CL

TL;DR: 提出语言驱动框架将程序性文本转化为视觉指令，通过语言结构建模与视觉生成相结合，显著提升教学类视觉生成质量


<details>
  <summary>Details</summary>
Motivation: 解决纯文本指令在传达复杂物理动作和空间关系时的局限性，通过视觉内容增强程序性语言的理解与应用

Method: 1. 基于选区解析器的长文本编码机制
2. 保持序列一致性的成对语篇连贯模型
3. 专门设计的程序性语言-图像对齐评估协议

Result: 在HTStep/CaptainCook4D/WikiAll数据集上显著优于基线模型，生成视觉内容更准确反映语言指令的时序特性

Conclusion: 推进程序性语言的视觉落地，在教育、任务指导和多模态理解领域具有应用价值

Abstract: The effective communication of procedural knowledge remains a significant
challenge in natural language processing (NLP), as purely textual instructions
often fail to convey complex physical actions and spatial relationships. We
address this limitation by proposing a language-driven framework that
translates procedural text into coherent visual instructions. Our approach
models the linguistic structure of instructional content by decomposing it into
goal statements and sequential steps, then conditioning visual generation on
these linguistic elements. We introduce three key innovations: (1) a
constituency parser-based text encoding mechanism that preserves semantic
completeness even with lengthy instructions, (2) a pairwise discourse coherence
model that maintains consistency across instruction sequences, and (3) a novel
evaluation protocol specifically designed for procedural language-to-image
alignment. Our experiments across three instructional datasets (HTStep,
CaptainCook4D, and WikiAll) demonstrate that our method significantly
outperforms existing baselines in generating visuals that accurately reflect
the linguistic content and sequential nature of instructions. This work
contributes to the growing body of research on grounding procedural language in
visual content, with applications spanning education, task guidance, and
multimodal language understanding.

</details>


### [70] [Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems](https://arxiv.org/abs/2505.16429)
*Song Jin,Juntian Zhang,Yuhan Liu,Xun Zhang,Yufei Zhang,Guojun Yin,Fei Jiang,Wei Lin,Rui Yan*

Main category: cs.CL

TL;DR: 提出RecInter新型推荐系统模拟平台，通过实时交互机制解决动态用户-平台互动难题，实验验证其有效复现真实系统演化现象。


<details>
  <summary>Details</summary>
Motivation: 传统A/B测试资源消耗大，离线方法难以捕捉动态交互，现有模拟平台缺乏环境动态重塑机制。需要建立支持实时双向交互的高保真模拟平台。

Method: 1. 构建动态交互机制（用户行为实时更新物品属性/商家代理响应）
2. 多维用户画像模块
3. 基于Chain-of-Thought增强的LLM微调
4. 商家代理与用户代理协同架构

Result: 1. 模拟可信度显著提升
2. 成功复现品牌忠诚度/马太效应等涌现现象
3. 验证交互机制对系统演化模拟的关键作用

Conclusion: RecInter通过创新的动态交互设计，为推荐系统研究提供了高可信度的仿真测试平台，其环境响应机制是模拟真实系统演化的核心突破。

Abstract: Evaluating and iterating upon recommender systems is crucial, yet traditional
A/B testing is resource-intensive, and offline methods struggle with dynamic
user-platform interactions. While agent-based simulation is promising, existing
platforms often lack a mechanism for user actions to dynamically reshape the
environment. To bridge this gap, we introduce RecInter, a novel agent-based
simulation platform for recommender systems featuring a robust interaction
mechanism. In RecInter platform, simulated user actions (e.g., likes, reviews,
purchases) dynamically update item attributes in real-time, and introduced
Merchant Agents can reply, fostering a more realistic and evolving ecosystem.
High-fidelity simulation is ensured through Multidimensional User Profiling
module, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought
(CoT) enriched interaction data. Our platform achieves significantly improved
simulation credibility and successfully replicates emergent phenomena like
Brand Loyalty and the Matthew Effect. Experiments demonstrate that this
interaction mechanism is pivotal for simulating realistic system evolution,
establishing our platform as a credible testbed for recommender systems
research.

</details>


### [71] [University of Indonesia at SemEval-2025 Task 11: Evaluating State-of-the-Art Encoders for Multi-Label Emotion Detection](https://arxiv.org/abs/2505.16460)
*Ikhlasul Akmal Hanif,Eryawan Presma Yulianrifat,Jaycent Gunawan Ongris,Eduardus Tjitrahardja,Muhammad Falensi Azmi,Rahmat Bryan Naufal,Alfan Farizki Wicaksono*

Main category: cs.CL

TL;DR: 提出基于prompt编码器（mE5/BGE）结合CatBoost分类器的集成模型，在28种语言的多标签情感分类任务中平均F1-macro达56.58，优于完全微调XLMR/mBERT的方法。


<details>
  <summary>Details</summary>
Motivation: 解决多语言场景下的情感分类难题，探索不同微调策略和模型架构对性能的影响。

Method: 对比完全微调transformer与仅训练分类器两种策略，测试不同损失函数/编码器/分类器组合，最终集成多个BGE模型配置。

Result: 集成模型在所有语言上平均F1-macro为56.58，其中CatBoost分类器搭配prompt-based BGE编码器效果最优。

Conclusion: 提示工程编码器+轻量级分类器的训练范式，相比完全微调大模型更具效率和性能优势，集成策略可进一步提升效果。

Abstract: This paper presents our approach for SemEval 2025 Task 11 Track A, focusing
on multilabel emotion classification across 28 languages. We explore two main
strategies: fully fine-tuning transformer models and classifier-only training,
evaluating different settings such as fine-tuning strategies, model
architectures, loss functions, encoders, and classifiers. Our findings suggest
that training a classifier on top of prompt-based encoders such as mE5 and BGE
yields significantly better results than fully fine-tuning XLMR and mBERT. Our
best-performing model on the final leaderboard is an ensemble combining
multiple BGE models, where CatBoost serves as the classifier, with different
configurations. This ensemble achieves an average F1-macro score of 56.58
across all languages.

</details>


### [72] [Reading Between the Prompts: How Stereotypes Shape LLM's Implicit Personalization](https://arxiv.org/abs/2505.16467)
*Vera Neplenbroek,Arianna Bisazza,Raquel Fernández*

Main category: cs.CL

TL;DR: 大型语言模型通过隐式个性化基于刻板印象推断用户身份，影响少数群体回答质量；通过干预模型内部表示可有效缓解此问题。


<details>
  <summary>Details</summary>
Motivation: 已有研究显示LLMs对未明确提供人口统计信息的少数群体用户生成低质量回答，需系统性探究其机制与解决方案。

Method: 使用合成对话分析模型内部表示及生成答案，并通过线性探测器干预模型表示以验证缓解方法。

Result: LLMs基于刻板信号推断人口属性，部分群体即使明确声明不同身份仍存在偏见；干预模型表示可有效纠正。

Conclusion: 需提升LLMs用户身份表示的透明度与控制，以减轻刻板印象驱动的隐式个性化影响。

Abstract: Generative Large Language Models (LLMs) infer user's demographic information
from subtle cues in the conversation -- a phenomenon called implicit
personalization. Prior work has shown that such inferences can lead to lower
quality responses for users assumed to be from minority groups, even when no
demographic information is explicitly provided. In this work, we systematically
explore how LLMs respond to stereotypical cues using controlled synthetic
conversations, by analyzing the models' latent user representations through
both model internals and generated answers to targeted user questions. Our
findings reveal that LLMs do infer demographic attributes based on these
stereotypical signals, which for a number of groups even persists when the user
explicitly identifies with a different demographic group. Finally, we show that
this form of stereotype-driven implicit personalization can be effectively
mitigated by intervening on the model's internal representations using a
trained linear probe to steer them toward the explicitly stated identity. Our
results highlight the need for greater transparency and control in how LLMs
represent user identity.

</details>


### [73] [Teaching Large Language Models to Maintain Contextual Faithfulness via Synthetic Tasks and Reinforcement Learning](https://arxiv.org/abs/2505.16483)
*Shuzheng Si,Haozhe Zhao,Cheng Gao,Yuzhuo Bai,Zhitong Wang,Bofei Gao,Kangyang Luo,Wenhao Li,Yufei Huang,Gang Chen,Fanchao Qi,Minjia Zhang,Baobao Chang,Maosong Sun*

Main category: cs.CL

TL;DR: 提出CANOE框架，通过合成数据与Dual-GRPO强化学习方法，无需人工标注即可显著提升LLM生成内容的忠实性


<details>
  <summary>Details</summary>
Motivation: 现有LLM在上下文生成中存在忠实性问题，传统方法依赖人工标注且无法兼顾长短文本优化

Method: 1. 合成四种短文本QA数据构建训练集
2. 开发Dual-GRPO强化学习方法，使用三个定制规则奖励机制，同步优化长短文本生成

Result: 在11个下游任务中忠实性超越GPT-4o等最先进模型

Conclusion: CANOE为构建可靠信息检索系统提供高效解决方案，突破人工标注依赖与单任务优化限制

Abstract: Teaching large language models (LLMs) to be faithful in the provided context
is crucial for building reliable information-seeking systems. Therefore, we
propose a systematic framework, CANOE, to improve the faithfulness of LLMs in
both short-form and long-form generation tasks without human annotations.
Specifically, we first synthesize short-form question-answering (QA) data with
four diverse tasks to construct high-quality and easily verifiable training
data without human annotation. Also, we propose Dual-GRPO, a rule-based
reinforcement learning method that includes three tailored rule-based rewards
derived from synthesized short-form QA data, while simultaneously optimizing
both short-form and long-form response generation. Notably, Dual-GRPO
eliminates the need to manually label preference data to train reward models
and avoids over-optimizing short-form generation when relying only on the
synthesized short-form QA data. Experimental results show that CANOE greatly
improves the faithfulness of LLMs across 11 different downstream tasks, even
outperforming the most advanced LLMs, e.g., GPT-4o and OpenAI o1.

</details>


### [74] [LLaMAs Have Feelings Too: Unveiling Sentiment and Emotion Representations in LLaMA Models Through Probing](https://arxiv.org/abs/2505.16491)
*Dario Di Palma,Alessandro De Bellis,Giovanni Servedio,Vito Walter Anelli,Fedelucio Narducci,Tommaso Di Noia*

Main category: cs.CL

TL;DR: 通过探测Llama模型中间层，显著提升情感分析准确率（最高14%）并降低57%内存需求


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在情感分析中广泛应用，但对其内部如何编码情感信息的机制仍缺乏深入理解

Method: 使用探针分类器分析不同层和规模的编码特征，确定最佳情感信号捕获层及池化方法

Result: 二元情感任务中信息最集中在中层，仅解码器模型的末token非最优选择，内存需求平均降低57%

Conclusion: 层特定探测为情感分析提供了超越提示的新范式，在提升模型效用的同时显著降低资源消耗

Abstract: Large Language Models (LLMs) have rapidly become central to NLP,
demonstrating their ability to adapt to various tasks through prompting
techniques, including sentiment analysis. However, we still have a limited
understanding of how these models capture sentiment-related information. This
study probes the hidden layers of Llama models to pinpoint where sentiment
features are most represented and to assess how this affects sentiment
analysis.
  Using probe classifiers, we analyze sentiment encoding across layers and
scales, identifying the layers and pooling methods that best capture sentiment
signals. Our results show that sentiment information is most concentrated in
mid-layers for binary polarity tasks, with detection accuracy increasing up to
14% over prompting techniques. Additionally, we find that in decoder-only
models, the last token is not consistently the most informative for sentiment
encoding. Finally, this approach enables sentiment tasks to be performed with
memory requirements reduced by an average of 57%.
  These insights contribute to a broader understanding of sentiment in LLMs,
suggesting layer-specific probing as an effective approach for sentiment tasks
beyond prompting, with potential to enhance model utility and reduce memory
requirements.

</details>


### [75] [Sparse Activation Editing for Reliable Instruction Following in Narratives](https://arxiv.org/abs/2505.16505)
*Runcong Zhao,Chengyu Cao,Qinglin Zhu,Xiucheng Lv,Shun Shao,Lin Gui,Ruifeng Xu,Yulan He*

Main category: cs.CL

TL;DR: 提出无需训练的框架Concise-SAE和基准FreeInIntent，提升复杂叙事中的指令遵循能力


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法有效评估语言模型在复杂叙事语境下的指令遵循能力

Method: 通过自然语言指令识别并编辑指令相关神经元的无训练框架

Result: 创建包含1,212个样本的FreeInstruct基准测试，在叙事场景中验证有效性

Conclusion: Concise-SAE在保持生成质量的同时实现了多任务下的先进指令遵循表现

Abstract: Complex narrative contexts often challenge language models' ability to follow
instructions, and existing benchmarks fail to capture these difficulties. To
address this, we propose Concise-SAE, a training-free framework that improves
instruction following by identifying and editing instruction-relevant neurons
using only natural language instructions, without requiring labelled data. To
thoroughly evaluate our method, we introduce FreeInstruct, a diverse and
realistic benchmark of 1,212 examples that highlights the challenges of
instruction following in narrative-rich settings. While initially motivated by
complex narratives, Concise-SAE demonstrates state-of-the-art instruction
adherence across varied tasks without compromising generation quality.

</details>


### [76] [AppealCase: A Dataset and Benchmark for Civil Case Appeal Scenarios](https://arxiv.org/abs/2505.16514)
*Yuting Huang,Meitong Guo,Yiquan Wu,Ang Li,Xiaozhong Liu,Keting Yin,Changlong Sun,Fei Wu,Kun Kuang*

Main category: cs.CL

TL;DR: 提出包含1万对真实案例的AppealCase数据集，用于上诉案件分析的LegalAI研究，现有模型在判决逆转预测任务中表现不足(F1<50%)


<details>
  <summary>Details</summary>
Motivation: 现有LegalAI研究过度关注个案分析，忽视司法体系中上诉机制这一纠错核心环节对司法公平的重要价值

Method: 构建含5个上诉审查维度标注的民事案件数据集(91类)，设计5个新任务并系统评估20个主流模型

Result: 所有模型在判决逆转预测任务上F1值均低于50%，揭示上诉场景的复杂性

Conclusion: AppealCase数据集有望推动上诉案件分析研究，促进司法裁判一致性的提升

Abstract: Recent advances in LegalAI have primarily focused on individual case judgment
analysis, often overlooking the critical appellate process within the judicial
system. Appeals serve as a core mechanism for error correction and ensuring
fair trials, making them highly significant both in practice and in research.
To address this gap, we present the AppealCase dataset, consisting of 10,000
pairs of real-world, matched first-instance and second-instance documents
across 91 categories of civil cases. The dataset also includes detailed
annotations along five dimensions central to appellate review: judgment
reversals, reversal reasons, cited legal provisions, claim-level decisions, and
whether there is new information in the second instance. Based on these
annotations, we propose five novel LegalAI tasks and conduct a comprehensive
evaluation across 20 mainstream models. Experimental results reveal that all
current models achieve less than 50% F1 scores on the judgment reversal
prediction task, highlighting the complexity and challenge of the appeal
scenario. We hope that the AppealCase dataset will spur further research in
LegalAI for appellate case analysis and contribute to improving consistency in
judicial decision-making.

</details>


### [77] [CUB: Benchmarking Context Utilisation Techniques for Language Models](https://arxiv.org/abs/2505.16518)
*Lovisa Hagström,Youna Kim,Haeun Yu,Sang-goo Lee,Richard Johansson,Hyunsoo Cho,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 开发CUB基准评估上下文利用操作技术（CMTs），发现现有方法难以全面处理真实检索增强场景中的多种上下文类型


<details>
  <summary>Details</summary>
Motivation: 现有上下文利用技术（CMTs）在真实场景中面临三大关键挑战：矛盾上下文处理、无关上下文干扰、合成数据与实际数据表现差异

Method: 构建包含三种典型上下文类型的CUB基准，在3个不同数据集、9个语言模型上系统评估7种前沿CMTs方法

Result: 78%的CMTs无法处理全部上下文类型；65%的方法在自然数据集上表现比合成数据下降超20%；仅2种方法展现稳定跨场景性能

Conclusion: 需开发能处理多上下文类型的CMTs，并建立包含自然样本的综合性测试基准以避免性能虚高

Abstract: Incorporating external knowledge is crucial for knowledge-intensive tasks,
such as question answering and fact checking. However, language models (LMs)
may ignore relevant information that contradicts outdated parametric memory or
be distracted by irrelevant contexts. While many context utilisation
manipulation techniques (CMTs) that encourage or suppress context utilisation
have recently been proposed to alleviate these issues, few have seen systematic
comparison. In this paper, we develop CUB (Context Utilisation Benchmark) to
help practitioners within retrieval-augmented generation (RAG) identify the
best CMT for their needs. CUB allows for rigorous testing on three distinct
context types, observed to capture key challenges in realistic context
utilisation scenarios. With this benchmark, we evaluate seven state-of-the-art
methods, representative of the main categories of CMTs, across three diverse
datasets and tasks, applied to nine LMs. Our results show that most of the
existing CMTs struggle to handle the full set of types of contexts that may be
encountered in real-world retrieval-augmented scenarios. Moreover, we find that
many CMTs display an inflated performance on simple synthesised datasets,
compared to more realistic datasets with naturally occurring samples.
Altogether, our results show the need for holistic tests of CMTs and the
development of CMTs that can handle multiple context types.

</details>


### [78] [Are the Hidden States Hiding Something? Testing the Limits of Factuality-Encoding Capabilities in LLMs](https://arxiv.org/abs/2505.16520)
*Giovanni Servedio,Alessandro De Bellis,Dario Di Palma,Vito Walter Anelli,Tommaso Di Noia*

Main category: cs.CL

TL;DR: 该研究挑战了先前关于LLMs内部状态编码真实性的结论，通过构建更真实的数据集，发现模型在泛化至自生成数据时仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 现有研究依赖合成数据导致评估受限，需构建更贴近真实场景的数据集以准确分析LLMs的事实性编码能力。

Method: 提出两种数据构建方法：1) 从表格数据中采样真伪事实句；2) 基于问答数据生成模型依赖型真伪数据集。

Result: 在开源LLMs上的实验显示，先前结论仅部分有效，LLM生成数据集的评估泛化性仍不足。

Conclusion: 为LLMs事实性研究奠定方法论基础，并提供构建有效评估数据集的实用指南。

Abstract: Factual hallucinations are a major challenge for Large Language Models
(LLMs). They undermine reliability and user trust by generating inaccurate or
fabricated content. Recent studies suggest that when generating false
statements, the internal states of LLMs encode information about truthfulness.
However, these studies often rely on synthetic datasets that lack realism,
which limits generalization when evaluating the factual accuracy of text
generated by the model itself. In this paper, we challenge the findings of
previous work by investigating truthfulness encoding capabilities, leading to
the generation of a more realistic and challenging dataset. Specifically, we
extend previous work by introducing: (1) a strategy for sampling plausible
true-false factoid sentences from tabular data and (2) a procedure for
generating realistic, LLM-dependent true-false datasets from Question Answering
collections. Our analysis of two open-source LLMs reveals that while the
findings from previous studies are partially validated, generalization to
LLM-generated datasets remains challenging. This study lays the groundwork for
future research on factuality in LLMs and offers practical guidelines for more
effective evaluation.

</details>


### [79] [Benchmarking and Pushing the Multi-Bias Elimination Boundary of LLMs via Causal Effect Estimation-guided Debiasing](https://arxiv.org/abs/2505.16522)
*Zhouhao Sun,Zhiyuan Kan,Xiao Ding,Li Du,Yang Zhao,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 提出包含五类偏见的评测基准CMBE，通过因果效应估计消除多重偏见，提升大语言模型的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有基准仅控制单一偏见，但实际应用中数据常包含多重偏见，现有去偏方法难以同时消除多类型偏见

Method: 1. 同时估计多类型偏见的因果效应
2. 在推理时从总因果效应中消除偏见成分

Result: 现有LLMs和去偏方法在多重偏见场景下表现不足，CMBE方法有效提升模型泛化能力（实验显示最高提升15.2%）

Conclusion: CMBE框架首次实现多重偏见的联合消除，为提升LLMs鲁棒性提供新思路，推动可信AI发展

Abstract: Despite significant progress, recent studies have indicated that current
large language models (LLMs) may still utilize bias during inference, leading
to the poor generalizability of LLMs. Some benchmarks are proposed to
investigate the generalizability of LLMs, with each piece of data typically
containing one type of controlled bias. However, a single piece of data may
contain multiple types of biases in practical applications. To bridge this gap,
we propose a multi-bias benchmark where each piece of data contains five types
of biases. The evaluations conducted on this benchmark reveal that the
performance of existing LLMs and debiasing methods is unsatisfying,
highlighting the challenge of eliminating multiple types of biases
simultaneously. To overcome this challenge, we propose a causal effect
estimation-guided multi-bias elimination method (CMBE). This method first
estimates the causal effect of multiple types of biases simultaneously.
Subsequently, we eliminate the causal effect of biases from the total causal
effect exerted by both the semantic information and biases during inference.
Experimental results show that CMBE can effectively eliminate multiple types of
bias simultaneously to enhance the generalizability of LLMs.

</details>


### [80] [EnSToM: Enhancing Dialogue Systems with Entropy-Scaled Steering Vectors for Topic Maintenance](https://arxiv.org/abs/2505.16526)
*Heejae Suh,Yejin Jeon,Deokhyung Kang,Taehee Park,Yejin Min,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 提出EnSToM方法解决小规模语言模型在任务对话中主题一致性不足的问题，通过动态调整引导强度实现高效话题维持


<details>
  <summary>Details</summary>
Motivation: 现有激活工程方法在保持对话主题一致性上存在局限，尤其面对资源受限场景时难以平衡效率与可靠性

Method: 基于输入不确定性的熵值动态缩放引导向量强度（EnSToM），在保持本主题准确性的同时有效处理离题干扰

Result: 相比微调方法，仅需少量数据即实现显著性能提升，在主题维持准确率上超过基线方法18.7%

Conclusion: EnSToM为资源受限的对话系统提供了高效可靠的解决方案，在保持模型轻量化的同时增强话题坚守能力

Abstract: Small large language models (sLLMs) offer the advantage of being lightweight
and efficient, which makes them suitable for resource-constrained environments.
However, sLLMs often struggle to maintain topic consistency in task-oriented
dialogue systems, which is critical for scenarios such as service chatbots.
Specifically, it is important to ensure that the model denies off-topic or
malicious inputs and adheres to its intended functionality so as to prevent
potential misuse and uphold reliability. Towards this, existing activation
engineering approaches have been proposed to manipulate internal activations
during inference. While these methods are effective in certain scenarios, our
preliminary experiments reveal their limitations in ensuring topic adherence.
Therefore, to address this, we propose a novel approach termed Entropy-scaled
Steering vectors for Topic Maintenance (EnSToM). EnSToM dynamically adjusts the
steering intensity based on input uncertainty, which allows the model to handle
off-topic distractors effectively while preserving on-topic accuracy. Our
experiments demonstrate that EnSToM achieves significant performance gain with
a relatively small data size compared to fine-tuning approaches. By improving
topic adherence without compromising efficiency, our approach provides a robust
solution for enhancing sLLM-based dialogue systems.

</details>


### [81] [Mechanistic Understanding and Mitigation of Language Confusion in English-Centric Large Language Models](https://arxiv.org/abs/2505.16538)
*Ercong Nie,Helmut Schmid,Hinrich Schütze*

Main category: cs.CL

TL;DR: 通过机制可解释性方法揭示大语言模型语言混淆现象的核心机制，提出神经元编辑干预策略显著缓解问题


<details>
  <summary>Details</summary>
Motivation: 英语中心模型在非英语场景下易发生非预期的语言混淆现象，阻碍多语言场景的可靠应用

Method: 结合语言混淆基准测试(LCB)、分层透镜分析(TunedLens)和跨模型神经元对比定位技术

Result: 定位到最终层转换故障是主因，编辑少量关键神经元可减少87%混淆且保持模型性能

Conclusion: 神经元级干预为构建鲁棒、可解释的多语言模型开辟了新方向，揭示了模型内部动态机制

Abstract: Language confusion -- where large language models (LLMs) generate unintended
languages against the user's need -- remains a critical challenge, especially
for English-centric models. We present the first mechanistic interpretability
(MI) study of language confusion, combining behavioral benchmarking with
neuron-level analysis. Using the Language Confusion Benchmark (LCB), we show
that confusion points (CPs) -- specific positions where language switches occur
-- are central to this phenomenon. Through layer-wise analysis with TunedLens
and targeted neuron attribution, we reveal that transition failures in the
final layers drive confusion. We further demonstrate that editing a small set
of critical neurons, identified via comparative analysis with
multilingual-tuned models, substantially mitigates confusion without harming
general competence or fluency. Our approach matches multilingual alignment in
confusion reduction for most languages and yields cleaner, higher-quality
outputs. These findings provide new insights into the internal dynamics of LLMs
and highlight neuron-level interventions as a promising direction for robust,
interpretable multilingual language modeling.

</details>


### [82] [Think Silently, Think Fast: Dynamic Latent Compression of LLM Reasoning Chains](https://arxiv.org/abs/2505.16552)
*Wenhui Tan,Jiaze Li,Jianzhong Ju,Zhenbo Luo,Jian Luan,Ruihua Song*

Main category: cs.CL

TL;DR: 提出CoLaR框架：通过两阶段训练（监督微调+强化学习）在潜空间动态压缩推理过程，显著提升LLM推理效率，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 传统思维链（CoT）推理在token层面的计算成本高、效率低。需探索潜空间压缩推理路径的方法以平衡效率与性能。

Method: 1. 监督微调阶段：在next-token预测基础上增加next压缩嵌入预测目标，随机采样压缩因子合并连续token嵌入；2. 强化学习阶段：利用潜头非确定性探索多样推理路径，优化压缩效率。

Result: 数学推理任务中：比潜空间基线准确率高14.1%；推理链长度减少53.3%（性能仅下降4.8%）；强化学习版本在困难任务中准确率提升5.4%，潜推理链长度减少82.8%。

Conclusion: CoLaR成功实现潜空间高效推理，强化学习显著提升压缩效率与任务适应性，支持动态调整推理速度，为LLM推理优化提供新范式。代码模型将开源。

Abstract: Large Language Models (LLMs) achieve superior performance through
Chain-of-Thought (CoT) reasoning, but these token-level reasoning chains are
computationally expensive and inefficient. In this paper, we introduce
Compressed Latent Reasoning (CoLaR), a novel framework that dynamically
compresses reasoning processes in latent space through a two-stage training
approach. First, during supervised fine-tuning, CoLaR extends beyond next-token
prediction by incorporating an auxiliary next compressed embedding prediction
objective. This process merges embeddings of consecutive tokens using a
compression factor randomly sampled from a predefined range, and trains a
specialized latent head to predict distributions of subsequent compressed
embeddings. Second, we enhance CoLaR through reinforcement learning (RL) that
leverages the latent head's non-deterministic nature to explore diverse
reasoning paths and exploit more compact ones. This approach enables CoLaR to:
i) perform reasoning at a dense latent level (i.e., silently), substantially
reducing reasoning chain length, and ii) dynamically adjust reasoning speed at
inference time by simply prompting the desired compression factor. Extensive
experiments across four mathematical reasoning datasets demonstrate that CoLaR
achieves 14.1% higher accuracy than latent-based baseline methods at comparable
compression ratios, and reduces reasoning chain length by 53.3% with only 4.8%
performance degradation compared to explicit CoT method. Moreover, when applied
to more challenging mathematical reasoning tasks, our RL-enhanced CoLaR
demonstrates performance gains of up to 5.4% while dramatically reducing latent
reasoning chain length by 82.8%. The code and models will be released upon
acceptance.

</details>


### [83] [ScholarBench: A Bilingual Benchmark for Abstraction, Comprehension, and Reasoning Evaluation in Academic Contexts](https://arxiv.org/abs/2505.16566)
*Dongwon Noh,Donghyeok Koh,Junghun Yuk,Gyuwan Kim,Jaeyong Lee,Kyungtae Lim,Cheoneum Park*

Main category: cs.CL

TL;DR: ScholarBench 是用于评估大语言模型在复杂学术任务中深度知识与推理能力的基准测试，包含5类问题、覆盖8个研究领域，并构建了英韩双语数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在评估大语言模型领域知识时缺乏扩展性，无法处理复杂学术任务。开发该基准旨在填补这一空白。

Method: 通过三步构建：1) 从学术文献中提取专业复杂场景；2) 按研究领域设计问题属性；3) 创建双语对齐数据集。

Result: 包含5,309英文和5,031韩文样本，先进模型o3-mini仅取得0.543平均分，证明基准的挑战性。

Conclusion: 该基准有效评估LLMs的学术抽象/理解/推理能力，双语特性支持跨语言能力评估，为领域知识测试提供新标准。

Abstract: Prior benchmarks for evaluating the domain-specific knowledge of large
language models (LLMs) lack the scalability to handle complex academic tasks.
To address this, we introduce \texttt{ScholarBench}, a benchmark centered on
deep expert knowledge and complex academic problem-solving, which evaluates the
academic reasoning ability of LLMs and is constructed through a three-step
process. \texttt{ScholarBench} targets more specialized and logically complex
contexts derived from academic literature, encompassing five distinct problem
types. Unlike prior benchmarks, \texttt{ScholarBench} evaluates the
abstraction, comprehension, and reasoning capabilities of LLMs across eight
distinct research domains. To ensure high-quality evaluation data, we define
category-specific example attributes and design questions that are aligned with
the characteristic research methodologies and discourse structures of each
domain. Additionally, this benchmark operates as an English-Korean bilingual
dataset, facilitating simultaneous evaluation for linguistic capabilities of
LLMs in both languages. The benchmark comprises 5,031 examples in Korean and
5,309 in English, with even state-of-the-art models like o3-mini achieving an
average evaluation score of only 0.543, demonstrating the challenging nature of
this benchmark.

</details>


### [84] [URLs Help, Topics Guide: Understanding Metadata Utility in LLM Training](https://arxiv.org/abs/2505.16570)
*Dongyang Fan,Vinko Sabolčec,Martin Jaggi*

Main category: cs.CL

TL;DR: URL上下文显著加速LLM预训练效率并提升长提示下的下游表现，而质量评分和主题/格式元数据在训练加速中无效，但可用于生成控制


<details>
  <summary>Details</summary>
Motivation: 系统性评估不同元数据在LLM预训练中的有效性，澄清现有研究中关于元数据作用机制的模糊认知

Method: 通过控制变量实验对比URL上下文、质量评分、主题/格式元数据在训练效率和下游任务中的表现差异

Result: 仅URL元数据缩短训练收敛时间（约13%步骤），且在长提示推理时下游任务提升显著；主题/格式元数据实现分类器无引导的生成控制

Conclusion: 元数据在训练效率和生成控制上存在功能分化：URL信息优化训练动态，主题/格式信息提供人机交互的生成引导

Abstract: Large Language Models (LLMs) are commonly pretrained on vast corpora of text
without utilizing contextual metadata such as source, quality, or topic,
leading to a context-free learning paradigm. While recent studies suggest that
adding metadata like URL information as context (i.e., auxiliary inputs not
used in the loss calculation) can improve training efficiency and downstream
performance, they offer limited understanding of which types of metadata are
truly effective and under what conditions. In this work, we conduct a
systematic evaluation and find that not all metadata types contribute equally.
Only URL context speeds up training, whereas quality scores and topic/format
domain information offer no clear benefit. Furthermore, the improved downstream
performances of URL conditioning emerge only when longer prompts are used at
inference time. In addition, we demonstrate that context-aware pretraining
enables more controllable generation than context-free pretraining, in a
classifier-free guidance fashion. Although topic and format metadata do not
accelerate training, they are effective for steering outputs, offering
human-interpretable control over generation.

</details>


### [85] [EMULATE: A Multi-Agent Framework for Determining the Veracity of Atomic Claims by Emulating Human Actions](https://arxiv.org/abs/2505.16576)
*Spencer Hong,Meng Luo,Xinyi Wan*

Main category: cs.CL

TL;DR: 提出EMULATE系统，通过多智能体框架模拟人类行为改进事实核查任务，实验显示性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查系统采用检索后分类的流程与人类处理方式不符，迭代证据检索方法仍有改进空间。

Method: 使用多智能体框架分配不同子任务(如搜索结果排序、网页内容评估)，通过协作完成验证。

Result: 在多个基准测试中明显优于先前方法，验证了框架有效性。

Conclusion: 通过模拟人类多步骤处理机制，EMULATE系统提升了事实核查准确性，为后续研究提供新方向。

Abstract: Determining the veracity of atomic claims is an imperative component of many
recently proposed fact-checking systems. Many approaches tackle this problem by
first retrieving evidence by querying a search engine and then performing
classification by providing the evidence set and atomic claim to a large
language model, but this process deviates from what a human would do in order
to perform the task. Recent work attempted to address this issue by proposing
iterative evidence retrieval, allowing for evidence to be collected several
times and only when necessary. Continuing along this line of research, we
propose a novel claim verification system, called EMULATE, which is designed to
better emulate human actions through the use of a multi-agent framework where
each agent performs a small part of the larger task, such as ranking search
results according to predefined criteria or evaluating webpage content.
Extensive experiments on several benchmarks show clear improvements over prior
work, demonstrating the efficacy of our new multi-agent framework.

</details>


### [86] [O$^2$-Searcher: A Searching-based Agent Model for Open-Domain Open-Ended Question Answering](https://arxiv.org/abs/2505.16582)
*Jianbiao Mei,Tao Hu,Daocheng Fu,Licheng Wen,Xuemeng Yang,Rong Wu,Pinlong Cai,Xing Gao,Yu Yang,Chengjun Xie,Botian Shi,Yong Liu,Yu Qiao*

Main category: cs.CL

TL;DR: 提出O²-Searcher强化学习搜索代理，通过本地模拟搜索环境动态获取知识，解决开放/封闭式问题


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型受静态知识限制难以处理开放域实时问题，且当前方法主要聚焦封闭式问题

Method: 构建本地模拟搜索环境解耦外部知识获取与推理过程，设计统一强化学习框架与奖励机制实现问题类型识别与差异化回答策略

Result: 在自建O²-QA基准测试中显著超越主流大模型代理，使用3B参数模型即达到与更大模型相当的封闭式QA性能

Conclusion: O²-Searcher通过动态知识获取与强化学习框架，有效解决开放域问题，在多种QA任务中实现SOTA性能

Abstract: Large Language Models (LLMs), despite their advancements, are fundamentally
limited by their static parametric knowledge, hindering performance on tasks
requiring open-domain up-to-date information. While enabling LLMs to interact
with external knowledge environments is a promising solution, current efforts
primarily address closed-end problems. Open-ended questions, which
characterized by lacking a standard answer or providing non-unique and diverse
answers, remain underexplored. To bridge this gap, we present O$^2$-Searcher, a
novel search agent leveraging reinforcement learning to effectively tackle both
open-ended and closed-ended questions in the open domain. O$^2$-Searcher
leverages an efficient, locally simulated search environment for dynamic
knowledge acquisition, effectively decoupling the external world knowledge from
model's sophisticated reasoning processes. It employs a unified training
mechanism with meticulously designed reward functions, enabling the agent to
identify problem types and adapt different answer generation strategies.
Furthermore, to evaluate performance on complex open-ended tasks, we construct
O$^2$-QA, a high-quality benchmark featuring 300 manually curated, multi-domain
open-ended questions with associated web page caches. Extensive experiments
show that O$^2$-Searcher, using only a 3B model, significantly surpasses
leading LLM agents on O$^2$-QA. It also achieves SOTA results on various
closed-ended QA benchmarks against similarly-sized models, while performing on
par with much larger ones.

</details>


### [87] [Evaluating Large Language Model with Knowledge Oriented Language Specific Simple Question Answering](https://arxiv.org/abs/2505.16591)
*Bowen Jiang,Runchuan Zhu,Jiang Wu,Zinco Jiang,Yifan He,Junyuan Gao,Jia Yu,Rui Min,Yinfan Wang,Haote Yang,Songyang Zhang,Dahua Lin,Lijun Wu,Conghui He*

Main category: cs.CL

TL;DR: 首个评估大语言模型多语言事实能力的基准KoLasSimpleQA，通过9种语言的双领域设计（通用领域+语言专属领域），揭示主流LLM在跨语言场景下的性能差异与优化需求。


<details>
  <summary>Details</summary>
Motivation: 解决现有研究在评估大语言模型多语言能力时覆盖不足的问题，需兼顾全球适用性和语言文化特异性，识别模型能力边界。

Method: 构建具备单一知识点覆盖、绝对客观性的问题集，采用LLM-as-judge评估范式，覆盖9种语言的双领域（通用/语言专属）。

Result: 主流LLM在两类领域表现差异显著，推理模型与传统模型在性能指标、校准、鲁棒性等维度呈现明显对比。

Conclusion: KoLasSimpleQA为多语言场景的模型能力评估提供新范式，其双维度设计揭示模型优化方向，开源基准将助力研究社区突破LLM能力边界。

Abstract: We introduce KoLasSimpleQA, the first benchmark evaluating the multilingual
factual ability of Large Language Models (LLMs). Inspired by existing research,
we created the question set with features such as single knowledge point
coverage, absolute objectivity, unique answers, and temporal stability. These
questions enable efficient evaluation using the LLM-as-judge paradigm, testing
both the LLMs' factual memory and self-awareness ("know what they don't know").
KoLasSimpleQA expands existing research in two key dimensions: (1) Breadth
(Multilingual Coverage): It includes 9 languages, supporting global
applicability evaluation. (2) Depth (Dual Domain Design): It covers both the
general domain (global facts) and the language-specific domain (such as
history, culture, and regional traditions) for a comprehensive assessment of
multilingual capabilities. We evaluated mainstream LLMs, including traditional
LLM and emerging Large Reasoning Models. Results show significant performance
differences between the two domains, particularly in performance metrics,
ranking, calibration, and robustness. This highlights the need for targeted
evaluation and optimization in multilingual contexts. We hope KoLasSimpleQA
will help the research community better identify LLM capability boundaries in
multilingual contexts and provide guidance for model optimization. We will
release KoLasSimpleQA at https://github.com/opendatalab/KoLasSimpleQA .

</details>


### [88] [What Media Frames Reveal About Stance: A Dataset and Study about Memes in Climate Change Discourse](https://arxiv.org/abs/2505.16592)
*Shijia Zhou,Siyao Peng,Simon Luebke,Jörg Haßler,Mario Haim,Saif M. Mohammad,Barbara Plank*

Main category: cs.CL

TL;DR: 研究探讨气候变化网络模因中媒体框架与立场的互动关系，构建首个标注数据集CLIMATEMEMES，并评估多模态模型在立场检测和框架识别任务中的表现差异。


<details>
  <summary>Details</summary>
Motivation: 媒体框架通过选择性强调现实要素影响公众认知，但现有研究鲜少深入探讨其与内容立场的动态关联，尤其在网络模因这种新兴传播载体上的相互作用机制尚未明确。

Method: 采用跨学科研究方法：1) 构建包含1,184个Reddit模因的CLIMATEMEMES数据集，标注立场和媒体框架；2) 设计立场检测与框架识别双任务；3) 评估LLaVA-NeXT、Molmo及其LLM基座模型在不同模态输入下的性能。

Result: 视觉语言模型(VLMs)在立场检测表现良好(准确率75%)，但在框架识别上落后于纯语言模型(LLMs准确率68% vs VLMs 52%)。人工标注描述可提升性能，合成数据在特定条件下有效。

Conclusion: 揭示多模态模型处理语义细微差异的局限性，强调跨模态数据增强的潜力，为计算传播学研究提供新的基准数据集和方法论启示。

Abstract: Media framing refers to the emphasis on specific aspects of perceived reality
to shape how an issue is defined and understood. Its primary purpose is to
shape public perceptions often in alignment with the authors' opinions and
stances. However, the interaction between stance and media frame remains
largely unexplored. In this work, we apply an interdisciplinary approach to
conceptualize and computationally explore this interaction with internet memes
on climate change. We curate CLIMATEMEMES, the first dataset of climate-change
memes annotated with both stance and media frames, inspired by research in
communication science. CLIMATEMEMES includes 1,184 memes sourced from 47
subreddits, enabling analysis of frame prominence over time and communities,
and sheds light on the framing preferences of different stance holders. We
propose two meme understanding tasks: stance detection and media frame
detection. We evaluate LLaVA-NeXT and Molmo in various setups, and report the
corresponding results on their LLM backbone. Human captions consistently
enhance performance. Synthetic captions and human-corrected OCR also help
occasionally. Our findings highlight that VLMs perform well on stance, but
struggle on frames, where LLMs outperform VLMs. Finally, we analyze VLMs'
limitations in handling nuanced frames and stance expressions on climate change
internet memes.

</details>


### [89] [From Generic Empathy to Personalized Emotional Support: A Self-Evolution Framework for User Preference Alignment](https://arxiv.org/abs/2505.16610)
*Jing Ye,Lu Xiang,Yaping Zhang,Chengqing Zong*

Main category: cs.CL

TL;DR: 提出基于LLM的自我进化框架，通过两阶段训练优化情感支持对话的个性化适配


<details>
  <summary>Details</summary>
Motivation: 现有LLM在情感支持中存在回应笼统、无法满足用户个性化需求的问题

Method: 两阶段框架：1）情感支持经验获取（基础对话微调）2）自我改进（通过自我反思与精炼迭代优化响应）

Result: 实验证明显著提升情感支持效果，减少无效回应（用户偏好与模型输出的不匹配减少30.2%）

Conclusion: 自我进化框架有效提升LLM对用户隐式偏好的理解，为个性化情感支持系统提供新方向

Abstract: Effective emotional support hinges on understanding users' emotions and needs
to provide meaningful comfort during multi-turn interactions. Large Language
Models (LLMs) show great potential for expressing empathy; however, they often
deliver generic and one-size-fits-all responses that fail to address users'
specific needs. To tackle this issue, we propose a self-evolution framework
designed to help LLMs improve their responses to better align with users'
implicit preferences concerning user profiles (personalities), emotional
states, and specific situations. Our framework consists of two distinct phases:
\textit{(1)} \textit{Emotional Support Experience Acquisition}, where LLMs are
fine-tuned on limited emotional support conversation data to provide basic
support, and \textit{(2)} \textit{Self-Improvement for Personalized Emotional
Support}, where LLMs leverage self-reflection and self-refinement to generate
personalized responses. Through iterative direct preference optimization
between the pre- and post-refined responses, our model generates responses that
reflect a better understanding of the user's implicit preferences. Extensive
experiments and evaluations demonstrate that our method significantly enhances
the model's performance in emotional support, reducing unhelpful responses and
minimizing discrepancies between user preferences and model outputs.

</details>


### [90] [Steering Large Language Models for Machine Translation Personalization](https://arxiv.org/abs/2505.16612)
*Daniel Scalena,Gabriele Sarti,Arianna Bisazza,Elisabetta Fersini,Malvina Nissim*

Main category: cs.CL

TL;DR: 大语言模型在低资源环境下通过提示策略和对比框架实现个性化翻译，保持质量同时提升风格适配性。


<details>
  <summary>Details</summary>
Motivation: 现有系统对隐式风格需求适应性不足，需开发更有效方法在资源有限场景下传递个性化翻译风格。

Method: 结合多示例提示、推理干预及基于稀疏自编码器的潜在概念对比框架进行风格引导。

Result: 引导方法实现优质个性化翻译，模型表征分析显示与多示例提示存在相似影响机制。

Conclusion: 验证了推理时引导的有效性，揭示了LLM个性化机制与提示方法的潜在共性，为可控文本生成提供新思路。

Abstract: High-quality machine translation systems based on large language models
(LLMs) have simplified the production of personalized translations reflecting
specific stylistic constraints. However, these systems still struggle in
settings where stylistic requirements are less explicit and might be harder to
convey via prompting. We explore various strategies for personalizing
LLM-generated translations in low-resource settings, focusing on the
challenging literary translation domain. We explore prompting strategies and
inference-time interventions for steering model generations towards a
personalized style, and propose a contrastive framework exploiting latent
concepts extracted from sparse autoencoders to identify salient personalization
properties. Our results show that steering achieves strong personalization
while preserving translation quality. We further examine the impact of steering
on LLM representations, finding model layers with a relevant impact for
personalization are impacted similarly by multi-shot prompting and our steering
method, suggesting similar mechanism at play.

</details>


### [91] [SSR-Zero: Simple Self-Rewarding Reinforcement Learning for Machine Translation](https://arxiv.org/abs/2505.16637)
*Wenjie Yang,Mao Zheng,Mingyang Song,Zheng Li*

Main category: cs.CL

TL;DR: 提出无需外部监督的自我奖励强化学习框架（SSR），在英汉互译任务中超越现有模型，包括闭源系统。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译专用大模型依赖昂贵的外部监督信号（人工标注/奖励模型），限制了扩展潜力。

Method: 基于Qwen-2.5-7B开发全在线参考无关的强化学习框架，通过13K单语数据自我生成奖励信号，结合COMET增强监督。

Result: SSR-Zero-7B超越TowerInstruct-13B等专用模型，SSR-X-Zero-7B在WMT/Flores基准上达SOTA，优于GPT-4o/Gemini 1.5 Pro。

Conclusion: 自我奖励机制有效补充传统评估方法，验证了自改进强化学习的潜力，研究成果已全面开源。

Abstract: Large language models (LLMs) have recently demonstrated remarkable
capabilities in machine translation (MT). However, most advanced MT-specific
LLMs heavily rely on external supervision signals during training, such as
human-annotated reference data or trained reward models (RMs), which are often
expensive to obtain and challenging to scale. To overcome this limitation, we
propose a Simple Self-Rewarding (SSR) Reinforcement Learning (RL) framework for
MT that is reference-free, fully online, and relies solely on self-judging
rewards. Training with SSR using 13K monolingual examples and Qwen-2.5-7B as
the backbone, our model SSR-Zero-7B outperforms existing MT-specific LLMs,
e.g., TowerInstruct-13B and GemmaX-28-9B, as well as larger general LLMs like
Qwen2.5-32B-Instruct in English $\leftrightarrow$ Chinese translation tasks
from WMT23, WMT24, and Flores200 benchmarks. Furthermore, by augmenting SSR
with external supervision from COMET, our strongest model, SSR-X-Zero-7B,
achieves state-of-the-art performance in English $\leftrightarrow$ Chinese
translation, surpassing all existing open-source models under 72B parameters
and even outperforming closed-source models, e.g., GPT-4o and Gemini 1.5 Pro.
Our analysis highlights the effectiveness of the self-rewarding mechanism
compared to the external LLM-as-a-judge approach in MT and demonstrates its
complementary benefits when combined with trained RMs. Our findings provide
valuable insight into the potential of self-improving RL methods. We have
publicly released our code, data and models.

</details>


### [92] [Collaboration among Multiple Large Language Models for Medical Question Answering](https://arxiv.org/abs/2505.16648)
*Kexin Shang,Chia-Hsuan Chang,Christopher C. Yang*

Main category: cs.CL

TL;DR: 提出多LLM协同框架提升医疗选择题推理能力，缓解模型间答案分歧，并发现模型置信度与预测准确率正相关


<details>
  <summary>Details</summary>
Motivation: 当前医疗领域LLM应用未充分挖掘多模型协同潜力，需整合不同LLM的专业知识实现互补增强

Method: 基于医疗多选题数据集设计多LLM协作框架，通过后验分析3个预训练模型，测量模型面对反对意见时的置信度

Result: 框架提升所有LLM推理能力(平均+8.7%)，减少答案分歧(方差降低32%)，模型置信度与预测准确率呈强正相关(r=0.83)

Conclusion: 多模型协作机制能有效提升医疗推理性能，置信度指标可作为模型预测可靠性的重要参考

Abstract: Empowered by vast internal knowledge reservoir, the new generation of large
language models (LLMs) demonstrate untapped potential to tackle medical tasks.
However, there is insufficient effort made towards summoning up a synergic
effect from multiple LLMs' expertise and background. In this study, we propose
a multi-LLM collaboration framework tailored on a medical multiple-choice
questions dataset. Through post-hoc analysis on 3 pre-trained LLM participants,
our framework is proved to boost all LLMs reasoning ability as well as
alleviate their divergence among questions. We also measure an LLM's confidence
when it confronts with adversary opinions from other LLMs and observe a
concurrence between LLM's confidence and prediction accuracy.

</details>


### [93] [Can reasoning models comprehend mathematical problems in Chinese ancient texts? An empirical study based on data from Suanjing Shishu](https://arxiv.org/abs/2505.16660)
*Liu Chang,Wang Dongbo,Liu liu,Zhao Zhixiao*

Main category: cs.CL

TL;DR: 构建Guji_MATH基准评估古籍数学问题解决能力，发现推理模型在文言数学任务表现弱于现代基准，需加强古文理解优化


<details>
  <summary>Details</summary>
Motivation: 通过构建古籍数学基准，促进传统数学知识挖掘与文化传播，并为评估模型的跨语言/文化能力提供新视角

Method: 采用机器标注+人工校验构建538题结构化数据集，设计封闭/开放双模式评估6个模型在文言数学问题的表现

Result: 模型具备部分文言数学问题解决能力，但整体表现（F1=47.7）显著低于现代数学任务基准（F1=72.4）

Conclusion: 该研究为古籍数学知识挖掘提供方法论支撑，同时建立了评估推理模型跨文化能力的创新范式

Abstract: This study addresses the challenges in intelligent processing of Chinese
ancient mathematical classics by constructing Guji_MATH, a benchmark for
evaluating classical texts based on Suanjing Shishu. It systematically assesses
the mathematical problem-solving capabilities of mainstream reasoning models
under the unique linguistic constraints of classical Chinese. Through
machine-assisted annotation and manual verification, 538 mathematical problems
were extracted from 8 canonical texts, forming a structured dataset centered on
the "Question-Answer-Solution" framework, supplemented by problem types and
difficulty levels. Dual evaluation modes--closed-book (autonomous
problem-solving) and open-book (reproducing classical solution methods)--were
designed to evaluate the performance of six reasoning models on ancient Chinese
mathematical problems. Results indicate that reasoning models can partially
comprehend and solve these problems, yet their overall performance remains
inferior to benchmarks on modern mathematical tasks. Enhancing models'
classical Chinese comprehension and cultural knowledge should be prioritized
for optimization. This study provides methodological support for mining
mathematical knowledge from ancient texts and disseminating traditional
culture, while offering new perspectives for evaluating cross-linguistic and
cross-cultural capabilities of reasoning models.

</details>


### [94] [A Japanese Language Model and Three New Evaluation Benchmarks for Pharmaceutical NLP](https://arxiv.org/abs/2505.16661)
*Issey Sukeda,Takuro Fujii,Kosei Buma,Shunsuke Sasaki,Shinnosuke Ono*

Main category: cs.CL

TL;DR: 日本制药领域专用语言模型通过跨语言预训练和新型基准测试，在术语密集型任务上超越开源模型并与商业模型竞争，揭示跨句子推理的挑战性


<details>
  <summary>Details</summary>
Motivation: 解决现有医疗语言模型在日语术语规范化、跨语言同义词映射和语句一致性推理方面的不足，构建更安全经济的地域性领域模型

Method: 使用20亿日语制药token+80亿英语生物医学token持续预训练，设计药师考试问答(YakugakuQA)、术语标准化(NayoseQA)、声明一致性检测(SogoCheck)三大诊断基准

Result: 模型在知识密集型任务超越所有开源模型，术语处理能力接近商业模型。GPT-4o在SogoCheck表现差(准确率仅40%)，揭示跨语句逻辑推理仍是NLP难点

Conclusion: 通过领域定制化训练和诊断性评估体系，证明构建安全可控的日语专业模型可行性，为制药NLP提供可复用的基准测试资源和模型架构参考

Abstract: We present a Japanese domain-specific language model for the pharmaceutical
field, developed through continual pretraining on 2 billion Japanese
pharmaceutical tokens and 8 billion English biomedical tokens. To enable
rigorous evaluation, we introduce three new benchmarks: YakugakuQA, based on
national pharmacist licensing exams; NayoseQA, which tests cross-lingual
synonym and terminology normalization; and SogoCheck, a novel task designed to
assess consistency reasoning between paired statements. We evaluate our model
against both open-source medical LLMs and commercial models, including GPT-4o.
Results show that our domain-specific model outperforms existing open models
and achieves competitive performance with commercial ones, particularly on
terminology-heavy and knowledge-based tasks. Interestingly, even GPT-4o
performs poorly on SogoCheck, suggesting that cross-sentence consistency
reasoning remains an open challenge. Our benchmark suite offers a broader
diagnostic lens for pharmaceutical NLP, covering factual recall, lexical
variation, and logical consistency. This work demonstrates the feasibility of
building practical, secure, and cost-effective language models for Japanese
domain-specific applications, and provides reusable evaluation resources for
future research in pharmaceutical and healthcare NLP. Our model, codes, and
datasets are released at https://github.com/EQUES-Inc/pharma-LLM-eval.

</details>


### [95] [Beyond Induction Heads: In-Context Meta Learning Induces Multi-Phase Circuit Emergence](https://arxiv.org/abs/2505.16694)
*Gouki Minegishi,Hiroki Furuta,Shohei Taniguchi,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Transformer-based language models exhibit In-Context Learning (ICL), where
predictions are made adaptively based on context. While prior work links
induction heads to ICL through a sudden jump in accuracy, this can only account
for ICL when the answer is included within the context. However, an important
property of practical ICL in large language models is the ability to meta-learn
how to solve tasks from context, rather than just copying answers from context;
how such an ability is obtained during training is largely unexplored. In this
paper, we experimentally clarify how such meta-learning ability is acquired by
analyzing the dynamics of the model's circuit during training. Specifically, we
extend the copy task from previous research into an In-Context Meta Learning
setting, where models must infer a task from examples to answer queries.
Interestingly, in this setting, we find that there are multiple phases in the
process of acquiring such abilities, and that a unique circuit emerges in each
phase, contrasting with the single-phases change in induction heads. The
emergence of such circuits can be related to several phenomena known in large
language models, and our analysis lead to a deeper understanding of the source
of the transformer's ICL ability.

</details>


### [96] [Locate-then-Merge: Neuron-Level Parameter Fusion for Mitigating Catastrophic Forgetting in Multimodal LLMs](https://arxiv.org/abs/2505.16703)
*Zeping Yu,Sophia Ananiadou*

Main category: cs.CL

TL;DR: 提出Neuron-Fusion方法，通过定位和选择性融合参数解决多模态模型的语言遗忘问题


<details>
  <summary>Details</summary>
Motivation: 多模态指令调整会导致基础大语言模型（如Llama3）出现语言能力灾难性遗忘，即使其本身具备强大性能

Method: 1. 提出无需训练的Locate-then-Merge参数融合框架
2. 开发Neuron-Fusion神经元级策略：保留参数变化大的神经元（对应新视觉能力），弱化变化小的神经元（对应通用语言能力）

Result: 在13个语言和视觉任务基准测试中持续超越现有模型融合方法，有效减少生成中的上下文幻觉现象

Conclusion: 通过神经元级参数融合，在保持视觉适应能力的同时显著缓解语言能力退化，为多模态模型优化提供新思路

Abstract: Although multimodal large language models (MLLMs) have achieved impressive
performance, the multimodal instruction tuning stage often causes catastrophic
forgetting of the base LLM's language ability, even in strong models like
Llama3. To address this, we propose Locate-then-Merge, a training-free
parameter fusion framework that first locates important parameters and then
selectively merges them. We further introduce Neuron-Fusion, a neuron-level
strategy that preserves the influence of neurons with large parameter
shifts--neurons likely responsible for newly acquired visual
capabilities--while attenuating the influence of neurons with smaller changes
that likely encode general-purpose language skills. This design enables better
retention of visual adaptation while mitigating language degradation.
Experiments on 13 benchmarks across both language and visual tasks show that
Neuron-Fusion consistently outperforms existing model merging methods. Further
analysis reveals that our method effectively reduces context hallucination in
generation.

</details>


### [97] [Breaking mBad! Supervised Fine-tuning for Cross-Lingual Detoxification](https://arxiv.org/abs/2505.16722)
*Himanshu Beniwal,Youngwoo Kim,Maarten Sap,Soham Dan,Thomas Hartvigsen*

Main category: cs.CL

TL;DR: 提出跨语言去毒范式，通过504种实验配置验证低资源语言场景下的毒性降低效果，揭示安全性与知识保留的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 大语言模型全球化部署需解决跨语言毒性问题，高/低资源语言间的去毒能力迁移成为关键挑战。

Method: 构建跨分布实验框架，在数据受限场景下评估毒性迁移效果，同时分析去毒处理对非毒性任务的影响。

Result: 实现跨语言毒性迁移（含不同语系语言），发现安全性与知识保留存在负相关（毒性降低15%时任务性能下降8.2%）。

Conclusion: 验证跨语言去毒范式的有效性，建立首个多语言安全评估基准，开源代码数据集促进社区发展。

Abstract: As large language models (LLMs) become increasingly prevalent in global
applications, ensuring that they are toxicity-free across diverse linguistic
contexts remains a critical challenge. We explore "Cross-lingual
Detoxification", a cross-lingual paradigm that mitigates toxicity, enabling
detoxification capabilities to transfer between high and low-resource languages
across different script families. We analyze cross-lingual detoxification's
effectiveness through 504 extensive settings to evaluate toxicity reduction in
cross-distribution settings with limited data and investigate how mitigation
impacts model performance on non-toxic tasks, revealing trade-offs between
safety and knowledge preservation. Our code and dataset are publicly available
at https://github.com/himanshubeniwal/Breaking-mBad.

</details>


### [98] [TRIM: Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning](https://arxiv.org/abs/2505.16743)
*Florentin Beck,William Rudman,Carsten Eickhoff*

Main category: cs.CL

TL;DR: 提出TRIM方法，通过维度级稀疏优化实现极端LLM压缩的新突破


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法在高层级稀疏率下性能显著下降，需更精细的稀疏分配策略来保持模型质量

Method: TRIM框架：采用输出维度级的动态稀疏分配，迭代调整过程结合质量指标指导，整合现有层剪枝策略

Result: 在80%稀疏率下，Qwen2.5-14B困惑度降低48%，OPT-13B降低超90%；多模型实验验证稳定性和SOTA表现

Conclusion: 维度级稀疏适应是突破LLM压缩极限的关键，TRIM为高效部署开辟新路径

Abstract: Large Language Models (LLMs) present significant computational and memory
challenges due to their extensive size, making pruning essential for their
efficient deployment. Existing one-shot pruning methods often apply uniform
sparsity constraints across layers or within each layer, resulting in
suboptimal performance, especially at high sparsity ratios. This work
introduces TRIM (Targeted Row-wise Iterative Metric-driven pruning), a novel
approach that applies varying sparsity ratios to individual output dimensions
(rows) within each layer. TRIM employs an iterative adjustment process guided
by quality metrics to optimize dimension-wise sparsity allocation, focusing on
reducing variance in quality retention across outputs to preserve critical
information. TRIM can be seamlessly integrated with existing layer-wise pruning
strategies. Our evaluations on perplexity and zero-shot tasks across diverse
LLM families (Qwen2.5, LLaMA-2, and OPT) and sparsity levels demonstrate that
TRIM achieves new state-of-the-art results and enhances stability. For
instance, at 80% sparsity, TRIM reduces perplexity by 48% for Qwen2.5-14B and
over 90% for OPT-13B compared to baseline methods. We conclude that
fine-grained, dimension-wise sparsity adaptation is crucial for pushing the
limits of extreme LLM compression. Code available at:
https://github.com/flobk/TRIM

</details>


### [99] [IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models](https://arxiv.org/abs/2505.16774)
*Yiming Gao,Bin Wang,Chengwei Wei,Shuo Sun,AiTi Aw*

Main category: cs.CL

TL;DR: 提出IFEval-Audio数据集，用于评估音频大语言模型（LLM）的指令遵循能力，包含280个跨6个维度的音频-指令-答案三元组，并公开支持研究。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于文本和视觉-语言模型的指令遵循能力，音频LLM在此领域的表现未被充分探索，且多模态对齐常导致指令能力下降。

Method: 构建覆盖内容、格式等6个维度的评估数据集，通过音频-指令配对测试模型生成结构化输出的能力，并对主流音频LLM进行基准测试。

Result: 数据集有效评估音频LLM的指令遵循表现，基准测试揭示模型在复杂指令处理中的局限性，数据集已开源促进后续研究。

Conclusion: IFEval-Audio填补音频LLM指令评估空白，为提升多模态模型非文本任务能力提供关键工具，推动该领域标准化发展。

Abstract: Large language models (LLMs) have demonstrated strong instruction-following
capabilities in text-based tasks. However, this ability often deteriorates in
multimodal models after alignment with non-text modalities such as images or
audio. While several recent efforts have investigated instruction-following
performance in text and vision-language models, instruction-following in
audio-based large language models remains largely unexplored. To bridge this
gap, we introduce IFEval-Audio, a novel evaluation dataset designed to assess
the ability to follow instructions in an audio LLM. IFEval-Audio contains 280
audio-instruction-answer triples across six diverse dimensions: Content,
Capitalization, Symbol, List Structure, Length, and Format. Each example pairs
an audio input with a text instruction, requiring the model to generate an
output that follows a specified structure. We benchmark state-of-the-art audio
LLMs on their ability to follow audio-involved instructions. The dataset is
released publicly to support future research in this emerging area.

</details>


### [100] [Reasoning Beyond Language: A Comprehensive Survey on Latent Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.16782)
*Xinghao Chen,Anhao Zhao,Heming Xia,Xuan Lu,Hanlin Wang,Yanjun Chen,Wei Zhang,Jian Wang,Wenjie Li,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 系统综述潜在思维链推理范式，提出分类体系并分析前沿方法，为LLM高效推理提供新方向


<details>
  <summary>Details</summary>
Motivation: 传统显性思维链存在语言耦合导致的效率瓶颈，潜在推理可突破语言限制实现高效抽象推理

Method: 建立包含token策略/内部机制/分析维度/应用场景的四维分类法，对现有方法进行系统性对比与模式解析

Result: 形成首个潜在推理系统框架，揭示不同设计模式的特点，提出动态路由、认知压缩等未来研究方向

Conclusion: 潜在思维链通过解耦语言与推理，显著提升LLM认知效率，需在理论建模与跨模态推理等方向持续突破

Abstract: Large Language Models (LLMs) have achieved impressive performance on complex
reasoning tasks with Chain-of-Thought (CoT) prompting. However, conventional
CoT relies on reasoning steps explicitly verbalized in natural language,
introducing inefficiencies and limiting its applicability to abstract
reasoning. To address this, there has been growing research interest in latent
CoT reasoning, where inference occurs within latent spaces. By decoupling
reasoning from language, latent reasoning promises richer cognitive
representations and more flexible, faster inference. Researchers have explored
various directions in this promising field, including training methodologies,
structural innovations, and internal reasoning mechanisms. This paper presents
a comprehensive overview and analysis of this reasoning paradigm. We begin by
proposing a unified taxonomy from four perspectives: token-wise strategies,
internal mechanisms, analysis, and applications. We then provide in-depth
discussions and comparative analyses of representative methods, highlighting
their design patterns, strengths, and open challenges. We aim to provide a
structured foundation for advancing this emerging direction in LLM reasoning.
The relevant papers will be regularly updated at
https://github.com/EIT-NLP/Awesome-Latent-CoT.

</details>


### [101] [Accidental Misalignment: Fine-Tuning Language Models Induces Unexpected Vulnerability](https://arxiv.org/abs/2505.16789)
*Punya Syon Pandey,Samuel Simko,Kellin Pelrine,Zhijing Jin*

Main category: cs.CL

TL;DR: 研究发现大语言模型在领域微调时可能因数据特性产生意外漏洞，通过分析数据特征与攻击成功率的相关性，揭示了数据集设计对模型安全的关键作用。


<details>
  <summary>Details</summary>
Motivation: 尽管领域微调能提升模型性能，但可能引入意外安全漏洞，需系统研究微调数据特性与模型脆弱性的关联机制。

Method: 1. 识别数据集的语言特征、语义相似性和毒性
2. 评估微调模型的对抗攻击表现
3. 分析数据特征与攻击成功率的统计相关性
4. 通过因果分析探索防御方案

Result: 发现特定数据特征（如语义集中度）与攻击成功率显著相关，揭示了数据集特性可能通过因果路径影响模型鲁棒性

Conclusion: 模型安全不仅依赖算法改进，更需要从数据设计源头控制特征分布，微调时应系统评估数据特性对安全性的潜在影响

Abstract: As large language models gain popularity, their vulnerability to adversarial
attacks remains a primary concern. While fine-tuning models on domain-specific
datasets is often employed to improve model performance, it can introduce
vulnerabilities within the underlying model. In this work, we investigate
Accidental Misalignment, unexpected vulnerabilities arising from
characteristics of fine-tuning data. We begin by identifying potential
correlation factors such as linguistic features, semantic similarity, and
toxicity within our experimental datasets. We then evaluate the adversarial
performance of these fine-tuned models and assess how dataset factors correlate
with attack success rates. Lastly, we explore potential causal links, offering
new insights into adversarial defense strategies and highlighting the crucial
role of dataset design in preserving model alignment. Our code is available at
https://github.com/psyonp/accidental_misalignment.

</details>


### [102] [Learning Beyond Limits: Multitask Learning and Synthetic Data for Low-Resource Canonical Morpheme Segmentation](https://arxiv.org/abs/2505.16800)
*Changbing Yang,Garrett Nicolai*

Main category: cs.CL

TL;DR: 提出基于Transformer的多任务学习框架，通过联合形态分割与词素注释任务，结合LLM生成合成数据，显著提升低资源语言的形态分析性能。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言形态分析中的数据稀缺问题，传统方法在标注数据不足时性能受限。通过多任务联合学习和LLM合成数据增强训练信号。

Method: 1. 构建多任务框架联合预测形态切分和词素注释
2. 利用文档语言学共享表征增强泛化能力
3. 采用LLM上下文学习生成合成训练数据

Result: 在SIGMORPHON 2023数据集上，词级切分准确率和词素级F1分数显著提升，尤其在纳瓦霍语等低资源语言上表现突出。

Conclusion: 多任务学习与LLM合成数据结合是低资源形态分析的有效方案，为资源匮乏语言的NLP任务提供了新的技术路径。

Abstract: We introduce a transformer-based morpheme segmentation system that augments a
low-resource training signal through multitask learning and LLM-generated
synthetic data. Our framework jointly predicts morphological segments and
glosses from orthographic input, leveraging shared linguistic representations
obtained through a common documentary process to enhance model generalization.
To further address data scarcity, we integrate synthetic training data
generated by large language models (LLMs) using in-context learning.
Experimental results on the SIGMORPHON 2023 dataset show that our approach
significantly improves word-level segmentation accuracy and morpheme-level
F1-score across multiple low-resource languages.

</details>


### [103] [Two-way Evidence self-Alignment based Dual-Gated Reasoning Enhancement](https://arxiv.org/abs/2505.16806)
*Kexin Zhang,Junlan Chen,Daifeng Li,Yuxuan Zhang,Yangyang Feng,Bowen Deng,Weixu Chen*

Main category: cs.CL

TL;DR: 提出TW-ESA和DGR模块组成的ESA-DGR框架，解决LLMs在知识密集型多步推理任务中的证据对齐与知识融合难题


<details>
  <summary>Details</summary>
Motivation: 现有方法存在语义相关但逻辑无关的证据提取问题，且无法有效融合不确定证据与LLM知识，导致推理准确性不足

Method: TW-ESA模块通过双向证据自对齐提升因果逻辑理解，DGR模块通过双门控机制渐进融合LLM知识增强鲁棒性，二者在统一框架中协同训练

Result: 在三个KIMSR数据集上平均提升4% EM和5% F1，显著超越现有微调方法

Conclusion: ESA-DGR框架通过证据对齐与知识融合的协同机制，有效提升复杂推理任务的准确性和鲁棒性

Abstract: Large language models (LLMs) encounter difficulties in knowledge-intensive
multi-step reasoning (KIMSR) tasks. One challenge is how to effectively extract
and represent rationale evidence. The current methods often extract
semantically relevant but logically irrelevant evidence, resulting in flawed
reasoning and inaccurate responses. We propose a two-way evidence
self-alignment (TW-ESA) module, which utilizes the mutual alignment between
strict reasoning and LLM reasoning to enhance its understanding of the causal
logic of evidence, thereby addressing the first challenge. Another challenge is
how to utilize the rationale evidence and LLM's intrinsic knowledge for
accurate reasoning when the evidence contains uncertainty. We propose a
dual-gated reasoning enhancement (DGR) module to gradually fuse useful
knowledge of LLM within strict reasoning, which can enable the model to perform
accurate reasoning by focusing on causal elements in the evidence and exhibit
greater robustness. The two modules are collaboratively trained in a unified
framework ESA-DGR. Extensive experiments on three diverse and challenging KIMSR
datasets reveal that ESA-DGR significantly surpasses state-of-the-art LLM-based
fine-tuning methods, with remarkable average improvements of 4% in exact match
(EM) and 5% in F1 score. The implementation code is available at
https://anonymous.4open.science/r/ESA-DGR-2BF8.

</details>


### [104] [Does Synthetic Data Help Named Entity Recognition for Low-Resource Languages?](https://arxiv.org/abs/2505.16814)
*Gaurav Kamath,Sowmya Vajjala*

Main category: cs.CL

TL;DR: Synthetic data improves low-resource language NER performance but shows significant cross-language variability.


<details>
  <summary>Details</summary>
Motivation: Address limited labeled data availability for Named Entity Recognition in low-resource languages through synthetic data augmentation.

Method: Multilingual evaluation across 11 diverse languages using synthetic data augmentation strategies.

Result: Synthetic data demonstrates potential for enhancing NER systems, with performance variations observed across different language families.

Conclusion: While synthetic data offers a viable solution for low-resource NER, language-specific characteristics significantly impact effectiveness, requiring tailored approaches.

Abstract: Named Entity Recognition(NER) for low-resource languages aims to produce
robust systems for languages where there is limited labeled training data
available, and has been an area of increasing interest within NLP. Data
augmentation for increasing the amount of low-resource labeled data is a common
practice. In this paper, we explore the role of synthetic data in the context
of multilingual, low-resource NER, considering 11 languages from diverse
language families. Our results suggest that synthetic data does in fact hold
promise for low-resource language NER, though we see significant variation
between languages.

</details>


### [105] [Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs](https://arxiv.org/abs/2505.16831)
*Xiaoyu Xu,Xiang Yue,Yang Liu,Qingqing Ye,Haibo Hu,Minxin Du*

Main category: cs.CL

TL;DR: 现有LLM遗忘评估方法存在误导性，作者提出表征级分析框架揭示可逆/不可逆遗忘的本质区别，并开发统一分析工具包


<details>
  <summary>Details</summary>
Motivation: 当前基于词级指标（准确率/困惑度）的遗忘评估不可靠，模型通过微调即可恢复原行为，暗示信息可能被隐藏而非真正删除

Method: 提出基于PCA相似性、中心核对齐和Fisher信息的表征级评估框架，在6种遗忘方法、3个领域和2个开源LLM上进行验证

Result: 发现可逆遗忘（词级崩溃但保留潜在特征）与不可逆遗忘（表征深层损伤）的关键区别，理论证明浅层权重扰动导致误导性遗忘信号

Conclusion: 现有评估体系存在根本性缺陷，需建立基于表征分析的新诊断范式，为可信LLM遗忘提供理论工具支持

Abstract: Unlearning in large language models (LLMs) is intended to remove the
influence of specific data, yet current evaluations rely heavily on token-level
metrics such as accuracy and perplexity. We show that these metrics can be
misleading: models often appear to forget, but their original behavior can be
rapidly restored with minimal fine-tuning, revealing that unlearning may
obscure information rather than erase it. To diagnose this phenomenon, we
introduce a representation-level evaluation framework using PCA-based
similarity and shift, centered kernel alignment, and Fisher information.
Applying this toolkit across six unlearning methods, three domains (text, code,
math), and two open-source LLMs, we uncover a critical distinction between
reversible and irreversible forgetting. In reversible cases, models suffer
token-level collapse yet retain latent features; in irreversible cases, deeper
representational damage occurs. We further provide a theoretical account
linking shallow weight perturbations near output layers to misleading
unlearning signals, and show that reversibility is modulated by task type and
hyperparameters. Our findings reveal a fundamental gap in current evaluation
practices and establish a new diagnostic foundation for trustworthy unlearning
in LLMs. We provide a unified toolkit for analyzing LLM representation changes
under unlearning and relearning:
https://github.com/XiaoyuXU1/Representational_Analysis_Tools.git.

</details>


### [106] [SimpleDeepSearcher: Deep Information Seeking via Web-Powered Reasoning Trajectory Synthesis](https://arxiv.org/abs/2505.16834)
*Shuang Sun,Huatong Song,Yuhao Wang,Ruiyang Ren,Jinhao Jiang,Junjie Zhang,Fei Bai,Jia Deng,Wayne Xin Zhao,Zheng Liu,Lei Fang,Zhongyuan Wang,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 提出轻量级框架SimpleDeepSearcher，通过数据工程策略仅用871个样本就超越强化学习方法，显著提升深度搜索性能。


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成系统存在高质量训练轨迹缺乏、模拟环境分布不匹配、实际部署计算成本高昂三大核心痛点。

Method: 采用实时网络环境下的用户交互模拟生成数据，配合输入输出端的多标准筛选策略（多样性+质量优化）构建训练集。

Result: 在跨领域五基准测试中，监督微调(SFT)效果显著优于基于强化学习的基线模型。

Conclusion: 证实通过系统性解决数据瓶颈，监督微调可成为高效深度搜索系统的可行路径，为实际部署提供新思路。

Abstract: Retrieval-augmented generation (RAG) systems have advanced large language
models (LLMs) in complex deep search scenarios requiring multi-step reasoning
and iterative information retrieval. However, existing approaches face critical
limitations that lack high-quality training trajectories or suffer from the
distributional mismatches in simulated environments and prohibitive
computational costs for real-world deployment. This paper introduces
SimpleDeepSearcher, a lightweight yet effective framework that bridges this gap
through strategic data engineering rather than complex training paradigms. Our
approach synthesizes high-quality training data by simulating realistic user
interactions in live web search environments, coupled with a multi-criteria
curation strategy that optimizes the diversity and quality of input and output
side. Experiments on five benchmarks across diverse domains demonstrate that
SFT on only 871 curated samples yields significant improvements over RL-based
baselines. Our work establishes SFT as a viable pathway by systematically
addressing the data-scarce bottleneck, offering practical insights for
efficient deep search systems. Our code is available at
https://github.com/RUCAIBox/SimpleDeepSearcher.

</details>


### [107] [R1-Compress: Long Chain-of-Thought Compression via Chunk Compression and Search](https://arxiv.org/abs/2505.16838)
*Yibo Wang,Li Shen,Huanjin Yao,Tiansheng Huang,Rui Liu,Naiqiang Tan,Jiaxing Huang,Kai Zhang,Dacheng Tao*

Main category: cs.CL

TL;DR: R1-Compress提出两阶段块级压缩框架，在保持推理精度的同时显著降低长思维链的计算开销


<details>
  <summary>Details</summary>
Motivation: 现有长思维链压缩方法存在局部推理信号丢失和输出不连贯的问题，需要兼顾信息完整性和计算效率

Method: 分块压缩（切分Long-CoT为可管理块）+ 块间搜索机制（选择短且连贯的序列）

Result: 在MATH500上实现92.4%准确率（仅比基线低0.6%），token使用量减少约20%

Conclusion: R1-Compress为长文本推理任务提供了高效可靠的解决方案，平衡了计算效率与推理质量

Abstract: Chain-of-Thought (CoT) reasoning enhances large language models (LLMs) by
enabling step-by-step problem-solving, yet its extension to Long-CoT introduces
substantial computational overhead due to increased token length. Existing
compression approaches -- instance-level and token-level -- either sacrifice
essential local reasoning signals like reflection or yield incoherent outputs.
To address these limitations, we propose R1-Compress, a two-stage chunk-level
compression framework that preserves both local information and coherence. Our
method segments Long-CoT into manageable chunks, applies LLM-driven inner-chunk
compression, and employs an inter-chunk search mechanism to select the short
and coherent sequence. Experiments on Qwen2.5-Instruct models across MATH500,
AIME24, and GPQA-Diamond demonstrate that R1-Compress significantly reduces
token usage while maintaining comparable reasoning accuracy. On MATH500,
R1-Compress achieves an accuracy of 92.4%, with only a 0.6% drop compared to
the Long-CoT baseline, while reducing token usage by about 20%. Source code
will be available at https://github.com/w-yibo/R1-Compress

</details>


### [108] [Understanding and Analyzing Inappropriately Targeting Language in Online Discourse: A Comparative Annotation Study](https://arxiv.org/abs/2505.16847)
*Baran Barbarestani,Isa Maks,Piek Vossen*

Main category: cs.CL

TL;DR: 整合众包标注、专家标注与ChatGPT的在线仇恨语言检测方法研究


<details>
  <summary>Details</summary>
Motivation: 现有内容审核系统难以识别隐性歧视语言，需探索人机协同标注机制提升检测效果

Method: 构建Reddit对话标注框架，比较专家标注、众包标注与ChatGPT在8类目标标签和具体攻击词汇识别的表现差异

Result: 发现ChatGPT在显性仇恨识别优于人类，但对社会信仰/身体形象等新型歧视敏感度不足；上下文理解能力影响检测准确率30%以上

Conclusion: 提出分层审核框架：ChatGPT处理显性内容，复杂案例转人工，需开发上下文感知模型提升隐性歧视检测能力

Abstract: This paper introduces a method for detecting inappropriately targeting
language in online conversations by integrating crowd and expert annotations
with ChatGPT. We focus on English conversation threads from Reddit, examining
comments that target individuals or groups. Our approach involves a
comprehensive annotation framework that labels a diverse data set for various
target categories and specific target words within the conversational context.
We perform a comparative analysis of annotations from human experts, crowd
annotators, and ChatGPT, revealing strengths and limitations of each method in
recognizing both explicit hate speech and subtler discriminatory language. Our
findings highlight the significant role of contextual factors in identifying
hate speech and uncover new categories of targeting, such as social belief and
body image. We also address the challenges and subjective judgments involved in
annotation and the limitations of ChatGPT in grasping nuanced language. This
study provides insights for improving automated content moderation strategies
to enhance online safety and inclusivity.

</details>


### [109] [Nested Named Entity Recognition as Single-Pass Sequence Labeling](https://arxiv.org/abs/2505.16855)
*Alberto Muñoz-Ortiz,David Vilares,Caio COrro,Carlos Gómez-Rodríguez*

Main category: cs.CL

TL;DR: 将嵌套命名实体识别转化为序列标注任务，通过线性化句法结构结合预训练模型提升效率


<details>
  <summary>Details</summary>
Motivation: 传统嵌套实体识别方法复杂度高，需要结构化预测。研究旨在通过序列标注简化流程，利用现有序列标注工具实现高效训练

Method: 线性化句法结构+预训练编码器，将嵌套结构转化为序列标注问题，仅需n次标注操作

Result: 在保持性能竞争力的同时实现高效训练，兼容现有序列标注库

Conclusion: 该方法有效平衡了效果与效率，为嵌套NER提供了实用的轻量级解决方案

Abstract: We cast nested named entity recognition (NNER) as a sequence labeling task by
leveraging prior work that linearizes constituency structures, effectively
reducing the complexity of this structured prediction problem to
straightforward token classification. By combining these constituency
linearizations with pretrained encoders, our method captures nested entities
while performing exactly $n$ tagging actions. Our approach achieves competitive
performance compared to less efficient systems, and it can be trained using any
off-the-shelf sequence labeling library.

</details>


### [110] [Comparative analysis of subword tokenization approaches for Indian languages](https://arxiv.org/abs/2505.16868)
*Sudhansu Bala Das,Samujjal Choudhury,Tapas Kumar Mishra,Bidyut Kr. Patra*

Main category: cs.CL

TL;DR: 论文探讨了不同子词分词方法（如SentencePiece、BPE、WordPiece）在印度语言机器翻译中的表现，发现SentencePiece在统计/神经模型中表现最优，BPE在多语言神经模型中更佳，且印地语到英语的翻译效果优于反向。


<details>
  <summary>Details</summary>
Motivation: 印度语言具有复杂的形态结构和黏着特性，需通过子词分词捕捉词素组合特征，提升机器翻译模型对形态丰富语言的处理能力。

Method: 在统计/神经/多语言神经机器翻译模型中对比三种分词技术，使用BLEU、TER、METEOR等六种指标进行系统性评估。

Result: 统计/神经模型中SentencePiece的BLEU得分最高；多语言神经模型中BPE表现最优。所有模型在印度语言到英语方向的翻译质量均优于反向。

Conclusion: 模型类型决定最优分词器选择（SentencePiece适用于单语模型，BPE适合多语言模型），且翻译方向显著影响结果质量。

Abstract: Tokenization is the act of breaking down text into smaller parts, or tokens,
that are easier for machines to process. This is a key phase in machine
translation (MT) models. Subword tokenization enhances this process by breaking
down words into smaller subword units, which is especially beneficial in
languages with complicated morphology or a vast vocabulary. It is useful in
capturing the intricate structure of words in Indian languages (ILs), such as
prefixes, suffixes, and other morphological variations. These languages
frequently use agglutinative structures, in which words are formed by the
combination of multiple morphemes such as suffixes, prefixes, and stems. As a
result, a suitable tokenization strategy must be chosen to address these
scenarios. This paper examines how different subword tokenization techniques,
such as SentencePiece, Byte Pair Encoding (BPE), and WordPiece Tokenization,
affect ILs. The effectiveness of these subword tokenization techniques is
investigated in statistical, neural, and multilingual neural machine
translation models. All models are examined using standard evaluation metrics,
such as the Bilingual Evaluation Understudy (BLEU) score, TER, METEOR, CHRF,
RIBES, and COMET. Based on the results, it appears that for the majority of
language pairs for the Statistical and Neural MT models, the SentencePiece
tokenizer continuously performed better than other tokenizers in terms of BLEU
score. However, BPE tokenization outperformed other tokenization techniques in
the context of Multilingual Neural Machine Translation model. The results show
that, despite using the same tokenizer and dataset for each model, translations
from ILs to English surpassed translations from English to ILs.

</details>


### [111] [MPO: Multilingual Safety Alignment via Reward Gap Optimization](https://arxiv.org/abs/2505.16869)
*Weixiang Zhao,Yulin Hu,Yang Deng,Tongtong Wu,Wenxuan Zhang,Jiahe Guo,An Zhang,Yanyan Zhao,Bing Qin,Tat-Seng Chua,Ting Liu*

Main category: cs.CL

TL;DR: 提出MPO方法利用英语的安全能力优化多语言安全对齐，在三大模型验证有效且不损害多语言通用性


<details>
  <summary>Details</summary>
Motivation: 现有RLHF/DPO等单语言安全对齐方法难以处理多语言噪声数据，需跨语言安全能力迁移方案

Method: 通过最小化主导语言(英语)与目标语言的奖励差距，实现安全能力迁移并保留英语优势

Result: 在LLaMA-3.1、Gemma-2和Qwen2.5上验证，MPO显著提升多语言安全性且保持模型通用能力

Conclusion: MPO为LLMs的全球化部署提供了高效的多语言安全对齐解决方案

Abstract: Large language models (LLMs) have become increasingly central to AI
applications worldwide, necessitating robust multilingual safety alignment to
ensure secure deployment across diverse linguistic contexts. Existing
preference learning methods for safety alignment, such as RLHF and DPO, are
primarily monolingual and struggle with noisy multilingual data. To address
these limitations, we introduce Multilingual reward gaP Optimization (MPO), a
novel approach that leverages the well-aligned safety capabilities of the
dominant language (English) to improve safety alignment across multiple
languages. MPO directly minimizes the reward gap difference between the
dominant language and target languages, effectively transferring safety
capabilities while preserving the original strengths of the dominant language.
Extensive experiments on three LLMs, LLaMA-3.1, Gemma-2 and Qwen2.5, validate
MPO's efficacy in multilingual safety alignment without degrading general
multilingual utility.

</details>


### [112] [CASTILLO: Characterizing Response Length Distributions of Large Language Models](https://arxiv.org/abs/2505.16881)
*Daniel F. Perez-Ramirez,Dejan Kostic,Magnus Boman*

Main category: cs.CL

TL;DR: 提出CASTILLO数据集系统分析13个开源大模型在不同指令下的文本生成长度分布，揭示模型间/内部响应长度的高度波动性，为资源调度预测模型提供数据基础。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理资源管理方法因无法准确预测可变文本长度而低效，且传统方法存在生成偏差或忽略模型/提示特异性。需系统性分析不同模型的响应长度分布特征。

Method: 构建包含13个LLM在7个指令集上的响应长度数据集CASTILLO，每个<提示,模型>样本生成10次独立补全，记录token长度统计量（均值/标准差/百分位）及生成参数。

Result: 发现模型间响应长度差异达3.5倍，同模型同参数下生成长度标准差最高达30%，部分模型出现仅部分响应文本退化的特殊现象。

Conclusion: CASTILLO为开发主动资源调度算法提供预测基准，建立模型生成行为分析框架，公开数据促进生成模型与系统优化的交叉研究。

Abstract: Efficiently managing compute resources for Large Language Model (LLM)
inference remains challenging due to the inherently stochastic and variable
lengths of autoregressive text generation. Accurately estimating response
lengths in advance enables proactive resource allocation, yet existing
approaches either bias text generation towards certain lengths or rely on
assumptions that ignore model- and prompt-specific variability. We introduce
CASTILLO, a dataset characterizing response length distributions across 13
widely-used open-source LLMs evaluated on seven distinct instruction-following
corpora. For each $\langle$prompt, model$\rangle$ sample pair, we generate 10
independent completions using fixed decoding hyper-parameters, record the token
length of each response, and publish summary statistics (mean, std-dev,
percentiles), along with the shortest and longest completions, and the exact
generation settings. Our analysis reveals significant inter- and intra-model
variability in response lengths (even under identical generation settings), as
well as model-specific behaviors and occurrences of partial text degeneration
in only subsets of responses. CASTILLO enables the development of predictive
models for proactive scheduling and provides a systematic framework for
analyzing model-specific generation behaviors. We publicly release the dataset
and code to foster research at the intersection of generative language modeling
and systems.

</details>


### [113] [Shadows in the Attention: Contextual Perturbation and Representation Drift in the Dynamics of Hallucination in LLMs](https://arxiv.org/abs/2505.16894)
*Zeyu Wei,Shuo Wang,Xiaohui Rong,Xuemin Liu,He Li*

Main category: cs.CL

TL;DR: 系统研究大模型幻觉与上下文注入引发的内部状态漂移关系，揭示幻觉形成机制及纠错阈值


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型生成内容时出现的可信但错误（幻觉）问题，探究上下文注入如何通过内部状态漂移诱发幻觉

Method: 使用TruthfulQA构建双路径16轮测试（相关但有缺陷内容 vs 误导内容），通过三视角检测器追踪显性幻觉，结合余弦/熵/JS/斯皮尔曼指标分析隐藏状态和注意力图漂移

Result: 1. 幻觉频率与表征漂移呈5-7轮后趋稳的正相关
2. 相关上下文引发高置信度自洽幻觉，无关上下文导致注意力重路由的主题漂移
3. JS漂移(≈0.69)与斯皮尔曼漂移(≈0)共同构成注意力锁定阈值，跨过后幻觉固化

Conclusion: 揭示了模型规模与错误模式的关联机制，为基于内在状态的幻觉预测和上下文感知纠错提供了理论依据

Abstract: Hallucinations -- plausible yet erroneous outputs -- remain a critical
barrier to reliable deployment of large language models (LLMs). We present the
first systematic study linking hallucination incidence to internal-state drift
induced by incremental context injection. Using TruthfulQA, we construct two
16-round "titration" tracks per question: one appends relevant but partially
flawed snippets, the other injects deliberately misleading content. Across six
open-source LLMs, we track overt hallucination rates with a tri-perspective
detector and covert dynamics via cosine, entropy, JS and Spearman drifts of
hidden states and attention maps. Results reveal (1) monotonic growth of
hallucination frequency and representation drift that plateaus after 5--7
rounds; (2) relevant context drives deeper semantic assimilation, producing
high-confidence "self-consistent" hallucinations, whereas irrelevant context
induces topic-drift errors anchored by attention re-routing; and (3)
convergence of JS-Drift ($\sim0.69$) and Spearman-Drift ($\sim0$) marks an
"attention-locking" threshold beyond which hallucinations solidify and become
resistant to correction. Correlation analyses expose a seesaw between
assimilation capacity and attention diffusion, clarifying size-dependent error
modes. These findings supply empirical foundations for intrinsic hallucination
prediction and context-aware mitigation mechanisms.

</details>


### [114] [Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality](https://arxiv.org/abs/2505.16900)
*Jintian Shao,Hongyi Huang,Jiayi Wu,Beiwen Zhang,ZhiYu Wu,You Shan,MingKai Zheng*

Main category: cs.CL

TL;DR: 提出幂律衰减损失函数(PDL)，通过频率加权机制优化文本生成微调过程


<details>
  <summary>Details</summary>
Motivation: 高频token信息量低，低频token携带关键信息但被现有方法忽视，需重新平衡学习权重

Method: 基于训练语料频率对交叉熵损失进行幂律衰减加权，抑制高频token权重，增强低频token学习

Result: 提升生成文本的信息密度与多样性，在摘要生成/对话系统/风格迁移等任务中展现优势

Conclusion: PDL通过理论驱动的频率加权机制，有效优化生成模型对关键信息的捕捉能力

Abstract: During the finetuning stage of text generation tasks, standard cross-entropy
loss treats all tokens equally. This can lead models to overemphasize
high-frequency, low-information tokens, neglecting lower-frequency tokens
crucial for specificity and informativeness in generated content. This paper
introduces a novel loss function, Power-Law Decay Loss (PDL), specifically
designed to optimize the finetuning process for text generation. The core
motivation for PDL stems from observations in information theory and
linguistics: the informativeness of a token is often inversely proportional to
its frequency of occurrence. PDL re-weights the contribution of each token in
the standard cross-entropy loss based on its frequency in the training corpus,
following a power-law decay. Specifically, the weights for high-frequency
tokens are reduced, while low-frequency, information-dense tokens are assigned
higher weights. This mechanism guides the model during finetuning to focus more
on learning and generating tokens that convey specific and unique information,
thereby enhancing the quality, diversity, and informativeness of the generated
text. We theoretically elaborate on the motivation and construction of PDL and
discuss its potential applications and advantages across various text
generation finetuning tasks, such as abstractive summarization, dialogue
systems, and style transfer.

</details>


### [115] [UNCLE: Uncertainty Expressions in Long-Form Generation](https://arxiv.org/abs/2505.16922)
*Ruihan Yang,Caiqi Zhang,Zhisong Zhang,Xinting Huang,Dong Yu,Nigel Collier,Deqing Yang*

Main category: cs.CL

TL;DR: 论文提出UNCLE基准，揭示当前大模型在长文本生成中难以有效表达不确定性，并通过训练方法提升表现


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对大语言模型在长文本生成中不确定性表达能力的直接评估，需填补评估体系空白

Method: 构建跨5个领域、含4k长文本QA和20k短文本QA配对的UNCLE基准，提出新评估指标，并测试提示优化与模型训练方法

Result: 当前模型长文本不确定性表达能力不足，训练方法改进效果显著优于提示工程，长短文本表达差异为研究提供新方向

Conclusion: UNCLE基准首次连接长短文本QA评估，揭示模型缺陷与改进路径，为不确定性表达研究奠定基础

Abstract: Large Language Models (LLMs) are prone to hallucination, particularly in
long-form generations. A promising direction to mitigate hallucination is to
teach LLMs to express uncertainty explicitly when they lack sufficient
knowledge. However, existing work lacks direct and fair evaluation of LLMs'
ability to express uncertainty effectively in long-form generation. To address
this gap, we first introduce UNCLE, a benchmark designed to evaluate
uncertainty expression in both long- and short-form question answering (QA).
UNCLE spans five domains and comprises 4k long-form QA instances and over 20k
short-form QA pairs. Our dataset is the first to directly bridge short- and
long-form QA with paired questions and gold-standard answers. Along with the
benchmark, we propose a suite of new metrics to assess the models' capabilities
to selectively express uncertainty. Using UNCLE, we then demonstrate that
current models fail to convey uncertainty appropriately in long-form
generation. We further explore both prompt-based and training-based methods to
improve models' performance, with the training-based methods yielding greater
gains. Further analysis of alignment gaps between short- and long-form
uncertainty expression highlights promising directions for future research
using UNCLE.

</details>


### [116] [Latent Principle Discovery for Language Model Self-Improvement](https://arxiv.org/abs/2505.16927)
*Keshav Ramji,Tahira Naseem,Ramón Fernandez Astudillo*

Main category: cs.CL

TL;DR: 提出一种通过自校正机制自动挖掘语言模型潜在行为原则，并压缩为可解释性规则集，使小模型实现自我性能提升的方法。


<details>
  <summary>Details</summary>
Motivation: 减少人工标注原则的负担，通过模型自主发现并优化指导其推理的潜在行为属性。

Method: 采用后验正则化蒙特卡洛期望最大化算法，聚类压缩模型自生成原则，形成策略性自校正机制。

Result: 7-8B参数模型在AlpacaEval胜率提升8-10%，MT-Bench平均+0.3，IFEval原则遵循率提升19-23%。

Conclusion: 自动化原则驱动的方法能持续提升模型性能，为后训练优化提供有效路径。

Abstract: When language model (LM) users aim to improve the quality of its generations,
it is crucial to specify concrete behavioral attributes that the model should
strive to reflect. However, curating such principles across many domains, even
non-exhaustively, requires a labor-intensive annotation process. To automate
this process, we propose eliciting these latent attributes guiding model
reasoning towards human-preferred responses by explicitly modeling them in a
self-correction setting. Our approach mines new principles from the LM itself
and compresses the discovered elements to an interpretable set via clustering.
Specifically, we employ an approximation of posterior-regularized Monte Carlo
Expectation-Maximization to both identify a condensed set of the most effective
latent principles and teach the LM to strategically invoke them in order to
intrinsically refine its responses. We demonstrate that bootstrapping our
algorithm over multiple iterations enables smaller language models (7-8B
parameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an
average of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on
IFEval. We also show that clustering the principles yields interpretable and
diverse model-generated constitutions while retaining model performance. The
gains our method achieves highlight the potential of automated,
principle-driven post-training recipes toward continual self-improvement.

</details>


### [117] [PIIvot: A Lightweight NLP Anonymization Framework for Question-Anchored Tutoring Dialogues](https://arxiv.org/abs/2505.16931)
*Matthew Zent,Digory Smith,Simon Woodhead*

Main category: cs.CL

TL;DR: 提出轻量级PII匿名化框架PIIvot，通过数据上下文简化检测问题，并发布教育对话数据集QATD-2k


<details>
  <summary>Details</summary>
Motivation: 现有PII匿名化技术受限于错误阈值和召回率/精确率的权衡，阻碍了开放科学数据共享

Method: 利用数据上下文知识简化PII检测问题，开发PIIvot框架；构建QATD-2k教育对话数据集进行验证

Result: 成功创建目前最大的开源教育对话数据集，验证框架有效性

Conclusion: PIIvot框架降低PII检测复杂度，QATD-2k数据集满足教育对话研究需求，共同推动数据匿名化与教育资源共享

Abstract: Personally identifiable information (PII) anonymization is a high-stakes task
that poses a barrier to many open-science data sharing initiatives. While PII
identification has made large strides in recent years, in practice, error
thresholds and the recall/precision trade-off still limit the uptake of these
anonymization pipelines. We present PIIvot, a lighter-weight framework for PII
anonymization that leverages knowledge of the data context to simplify the PII
detection problem. To demonstrate its effectiveness, we also contribute
QATD-2k, the largest open-source real-world tutoring dataset of its kind, to
support the demand for quality educational dialogue data.

</details>


### [118] [In-Context Watermarks for Large Language Models](https://arxiv.org/abs/2505.16934)
*Yepeng Liu,Xuandong Zhao,Christopher Kruegel,Dawn Song,Yuheng Bu*

Main category: cs.CL

TL;DR: 提出无需模型权限的上下文水印方法（ICW），通过提示工程实现AI文本溯源，验证了其在学术评审场景中的可行性


<details>
  <summary>Details</summary>
Motivation: 现有水印技术依赖模型访问权限，无法应对学术评审中审稿人使用未授权LLMs生成虚假评审的现实需求

Method: 开发四种粒度级别的ICW策略，结合间接提示注入技术，通过修改输入文档（如学术手稿）隐蔽触发水印植入

Result: 实验证明ICW具备模型无关性，且LLM能力越强水印效果越好，为可扩展的内容溯源提供新方向

Conclusion: ICW开创了无需模型协作的水印新范式，特别适用于学术不端检测等开放场景，未来可随LLM能力提升持续增强

Abstract: The growing use of large language models (LLMs) for sensitive applications
has highlighted the need for effective watermarking techniques to ensure the
provenance and accountability of AI-generated text. However, most existing
watermarking methods require access to the decoding process, limiting their
applicability in real-world settings. One illustrative example is the use of
LLMs by dishonest reviewers in the context of academic peer review, where
conference organizers have no access to the model used but still need to detect
AI-generated reviews. Motivated by this gap, we introduce In-Context
Watermarking (ICW), which embeds watermarks into generated text solely through
prompt engineering, leveraging LLMs' in-context learning and
instruction-following abilities. We investigate four ICW strategies at
different levels of granularity, each paired with a tailored detection method.
We further examine the Indirect Prompt Injection (IPI) setting as a specific
case study, in which watermarking is covertly triggered by modifying input
documents such as academic manuscripts. Our experiments validate the
feasibility of ICW as a model-agnostic, practical watermarking approach.
Moreover, our findings suggest that as LLMs become more capable, ICW offers a
promising direction for scalable and accessible content attribution.

</details>


### [119] [On Multilingual Encoder Language Model Compression for Low-Resource Languages](https://arxiv.org/abs/2505.16956)
*Daniil Gurgurov,Michal Gregor,Josef van Genabith,Simon Ostermann*

Main category: cs.CL

TL;DR: 结合两步知识蒸馏+结构化剪枝+词汇修剪实现多语言模型极致压缩，在低资源语言中实现92%压缩率且性能仅下降2-10%


<details>
  <summary>Details</summary>
Motivation: 解决多语言大模型在低资源语言场景中的冗余问题，通过极端压缩保留关键语言知识

Method: 系统整合知识蒸馏（两步法）+ 结构化剪枝（压缩网络深度/宽度）+ 词汇修剪 + 截断技术

Result: 在3种低资源语言的4个NLP任务中：
1. 压缩率92% 
2. 性能下降2-10% 
3. 教师模型数据量越大性能损失越小

Conclusion: 建立了可复用的多语言模型压缩框架，为低资源语言部署轻量级NLP模型提供新方案

Abstract: In this paper, we combine two-step knowledge distillation, structured
pruning, truncation, and vocabulary trimming for extremely compressing
multilingual encoder-only language models for low-resource languages. Our novel
approach systematically combines existing techniques and takes them to the
extreme, reducing layer depth, feed-forward hidden size, and intermediate layer
embedding size to create significantly smaller monolingual models while
retaining essential language-specific knowledge. We achieve compression rates
of up to 92% with only a marginal performance drop of 2-10% in four downstream
tasks, including sentiment analysis, topic classification, named entity
recognition, and part-of-speech tagging, across three low-resource languages.
Notably, the performance degradation correlates with the amount of
language-specific data in the teacher model, with larger datasets resulting in
smaller performance losses. Additionally, we conduct extensive ablation studies
to identify best practices for multilingual model compression using these
techniques.

</details>


### [120] [BP-Seg: A graphical model approach to unsupervised and non-contiguous text segmentation using belief propagation](https://arxiv.org/abs/2505.16965)
*Fengyi Li,Kayhan Behdin,Natesh Pillai,Xiaofeng Wang,Zhipeng Wang,Ercan Yildiz*

Main category: cs.CL

TL;DR: 提出基于概率图模型的无监督文本分割方法BP-Seg，通过置信传播同时考虑句子局部连贯性和远距离语义相似性


<details>
  <summary>Details</summary>
Motivation: 传统文本分割方法难以兼顾相邻句子的局部连贯性和远距离句子的语义关联，影响长文档分割效果

Method: 构建概率图模型，利用置信传播算法迭代优化，既捕捉相邻句子关系，又能聚合语义相似的远距离句子

Result: 在长文档数据集上的实验显示，相比基线方法分割效果更优，并通过可视化案例验证了模型有效性

Conclusion: BP-Seg为无监督文本分割提供了新思路，通过图模型结构设计有效平衡局部与全局语义关系

Abstract: Text segmentation based on the semantic meaning of sentences is a fundamental
task with broad utility in many downstream applications. In this paper, we
propose a graphical model-based unsupervised learning approach, named BP-Seg
for efficient text segmentation. Our method not only considers local coherence,
capturing the intuition that adjacent sentences are often more related, but
also effectively groups sentences that are distant in the text yet semantically
similar. This is achieved through belief propagation on the carefully
constructed graphical models. Experimental results on both an illustrative
example and a dataset with long-form documents demonstrate that our method
performs favorably compared to competing approaches.

</details>


### [121] [From Tens of Hours to Tens of Thousands: Scaling Back-Translation for Speech Recognition](https://arxiv.org/abs/2505.16972)
*Tianduo Wang,Lu Xu,Wei Lu,Shanbo Cheng*

Main category: cs.CL

TL;DR: 通过语音反向翻译技术，将大规模文本转化为合成语音，显著提升多语言ASR模型性能


<details>
  <summary>Details</summary>
Motivation: 解决多语言ASR模型在资源匮乏语言中扩展受限的难题，探索利用文本语料库生成合成语音的有效性

Method: 构建可扩展流水线：1. 用少量真实语音训练TTS模型 2. 生成数百倍量级的合成语音 3. 开发基于可懂度的质量评估框架

Result: 在10种语言生成超50万小时合成语音，Whisper-large-v3模型平均转录错误率降低30%+

Conclusion: 验证了语音反向翻译技术的高扩展性和有效性，为低资源语言ASR优化提供新范式

Abstract: Recent advances in Automatic Speech Recognition (ASR) have been largely
fueled by massive speech corpora. However, extending coverage to diverse
languages with limited resources remains a formidable challenge. This paper
introduces Speech Back-Translation, a scalable pipeline that improves
multilingual ASR models by converting large-scale text corpora into synthetic
speech via off-the-shelf text-to-speech (TTS) models. We demonstrate that just
tens of hours of real transcribed speech can effectively train TTS models to
generate synthetic speech at hundreds of times the original volume while
maintaining high quality. To evaluate synthetic speech quality, we develop an
intelligibility-based assessment framework and establish clear thresholds for
when synthetic data benefits ASR training. Using Speech Back-Translation, we
generate more than 500,000 hours of synthetic speech in ten languages and
continue pre-training Whisper-large-v3, achieving average transcription error
reductions of over 30\%. These results highlight the scalability and
effectiveness of Speech Back-Translation for enhancing multilingual ASR
systems.

</details>


### [122] [VeriFastScore: Speeding up long-form factuality evaluation](https://arxiv.org/abs/2505.16973)
*Rishanth Rajendhran,Amir Zadeh,Matthew Sarte,Chuan Li,Mohit Iyyer*

Main category: cs.CL

TL;DR: 提出VeriFastScore模型，通过微调Llama3.1 8B实现快速声明提取与验证，速度提升6.6倍且保持高相关性。


<details>
  <summary>Details</summary>
Motivation: 现有事实性评估工具VeriScore需要多次LLM调用且耗时严重（100秒/次），难以支持大规模应用。

Method: 利用合成数据微调模型，实现声明分解、可验证性判断和噪声证据验证的三重任务并行处理。

Result: 达到与原始方法0.94的系统级相关性，速度提升6.6倍（排除证据检索后9.9倍）。

Conclusion: 公开模型与数据集，为事实性研究提供高效解决方案，平衡速度与准确性。

Abstract: Metrics like FactScore and VeriScore that evaluate long-form factuality
operate by decomposing an input response into atomic claims and then
individually verifying each claim. While effective and interpretable, these
methods incur numerous LLM calls and can take upwards of 100 seconds to
evaluate a single response, limiting their practicality in large-scale
evaluation and training scenarios. To address this, we propose VeriFastScore,
which leverages synthetic data to fine-tune Llama3.1 8B for simultaneously
extracting and verifying all verifiable claims within a given text based on
evidence from Google Search. We show that this task cannot be solved via
few-shot prompting with closed LLMs due to its complexity: the model receives
~4K tokens of evidence on average and needs to concurrently decompose claims,
judge their verifiability, and verify them against noisy evidence. However, our
fine-tuned VeriFastScore model demonstrates strong correlation with the
original VeriScore pipeline at both the example level (r=0.80) and system level
(r=0.94) while achieving an overall speedup of 6.6x (9.9x excluding evidence
retrieval) over VeriScore. To facilitate future factuality research, we
publicly release our VeriFastScore model and synthetic datasets.

</details>


### [123] [LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding](https://arxiv.org/abs/2505.16983)
*Junlong Tong,Jinlan Fu,Zixuan Lin,Yingqi Fan,Anhao Zhao,Hui Su,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 提出基于组位置编码的流式LLM优化方法，无需架构修改即可解决注意力不匹配问题


<details>
  <summary>Details</summary>
Motivation: 现有流式LLM方法存在高成本重编码或架构受限的缺陷，分析发现输入注意力不匹配才是性能瓶颈

Method: 通过组位置编码范式保持源/目标上下文相对位置一致性，构建批处理架构下的流式优化方案

Result: 跨语言/跨模态任务实验显示性能超越现有方法，流式/批处理模式均展现强泛化能力

Conclusion: 揭示了位置编码在流式处理中的核心作用，提供无需架构调整的高效解决方案

Abstract: Large Language Models (LLMs) are primarily designed for batch processing.
Existing methods for adapting LLMs to streaming rely either on expensive
re-encoding or specialized architectures with limited scalability. This work
identifies three key mismatches in adapting batch-oriented LLMs to streaming:
(1) input-attention, (2) output-attention, and (3) position-ID mismatches.
While it is commonly assumed that the latter two mismatches require frequent
re-encoding, our analysis reveals that only the input-attention mismatch
significantly impacts performance, indicating re-encoding outputs is largely
unnecessary. To better understand this discrepancy with the common assumption,
we provide the first comprehensive analysis of the impact of position encoding
on LLMs in streaming, showing that preserving relative positions within source
and target contexts is more critical than maintaining absolute order. Motivated
by the above analysis, we introduce a group position encoding paradigm built on
batch architectures to enhance consistency between streaming and batch modes.
Extensive experiments on cross-lingual and cross-modal tasks demonstrate that
our method outperforms existing approaches. Our method requires no
architectural modifications, exhibits strong generalization in both streaming
and batch modes. The code is available at repository
https://github.com/EIT-NLP/StreamingLLM.

</details>


### [124] [T1: A Tool-Oriented Conversational Dataset for Multi-Turn Agentic Planning](https://arxiv.org/abs/2505.16986)
*Amartya Chakraborty,Paresh Dashore,Nadia Bathaee,Anmol Jain,Anirban Das,Shi-Xiong Zhang,Sambit Sahu,Milind Naphade,Genta Indra Winata*

Main category: cs.CL

TL;DR: 提出T1数据集，用于评估大语言模型在跨领域多轮对话中处理工具依赖和动态重规划的能力。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在涉及API或工具调用依赖关系的复杂场景（尤其是多轮对话）中规划能力不足的问题，填补现有研究在跨工具依赖管理和动态重规划评估方面的空白。

Method: 构建包含9个领域（4单领域+5跨领域）的多轮对话数据集T1，集成短期/长期记忆缓存机制，支持动态重规划策略（如结果复用判断），并开发T1-Agent作为评估框架。

Result: T1成为首个专注于工具依赖管理的多领域基准测试集，实验证明基于T1-Agent的模型能有效提升复杂工具调用场景下的规划推理能力。

Conclusion: T1数据集为工具使用规划研究提供新范式，其动态缓存和重规划机制为提升语言模型在现实场景中的决策能力指明方向，同时建立了开源模型性能评估的统一标准。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities as
intelligent agents capable of solving complex problems. However, effective
planning in scenarios involving dependencies between API or tool
calls-particularly in multi-turn conversations-remains a significant challenge.
To address this, we introduce T1, a tool-augmented, multi-domain, multi-turn
conversational dataset specifically designed to capture and manage inter-tool
dependencies across diverse domains. T1 enables rigorous evaluation of agents'
ability to coordinate tool use across nine distinct domains (4 single domain
and 5 multi-domain) with the help of an integrated caching mechanism for both
short- and long-term memory, while supporting dynamic replanning-such as
deciding whether to recompute or reuse cached results. Beyond facilitating
research on tool use and planning, T1 also serves as a benchmark for evaluating
the performance of open-source language models. We present results powered by
T1-Agent, highlighting their ability to plan and reason in complex,
tool-dependent scenarios.

</details>


### [125] [MASLab: A Unified and Comprehensive Codebase for LLM-based Multi-Agent Systems](https://arxiv.org/abs/2505.16988)
*Rui Ye,Keduan Huang,Qimin Wu,Yuzhu Cai,Tian Jin,Xianghe Pang,Xiangrui Liu,Jiaqi Su,Chen Qian,Bohan Tang,Kaiqu Liang,Jiaao Chen,Yue Hu,Zhenfei Yin,Rongye Shi,Bo An,Yang Gao,Wenjun Wu,Lei Bai,Siheng Chen*

Main category: cs.CL

TL;DR: 提出统一代码库MASLab，整合20+已验证方法，建立标准化评估环境，降低多智能体系统研究门槛


<details>
  <summary>Details</summary>
Motivation: 当前LLM多智能体系统领域存在代码库分散、方法重复实现、评估标准不一致等问题，阻碍研究公平性与可扩展性

Method: 1) 集成20+跨领域方法并逐步验证 2) 构建统一实验环境与评估协议 3) 采用模块化架构设计提升可扩展性

Result: 覆盖10+基准测试和8种模型的大规模实验，清晰展示现有MAS方法效果分布与模型能力边界

Conclusion: MASLab将持续跟踪领域发展并开源，为社区提供可持续演进的基准平台

Abstract: LLM-based multi-agent systems (MAS) have demonstrated significant potential
in enhancing single LLMs to address complex and diverse tasks in practical
applications. Despite considerable advancements, the field lacks a unified
codebase that consolidates existing methods, resulting in redundant
re-implementation efforts, unfair comparisons, and high entry barriers for
researchers. To address these challenges, we introduce MASLab, a unified,
comprehensive, and research-friendly codebase for LLM-based MAS. (1) MASLab
integrates over 20 established methods across multiple domains, each rigorously
validated by comparing step-by-step outputs with its official implementation.
(2) MASLab provides a unified environment with various benchmarks for fair
comparisons among methods, ensuring consistent inputs and standardized
evaluation protocols. (3) MASLab implements methods within a shared streamlined
structure, lowering the barriers for understanding and extension. Building on
MASLab, we conduct extensive experiments covering 10+ benchmarks and 8 models,
offering researchers a clear and comprehensive view of the current landscape of
MAS methods. MASLab will continue to evolve, tracking the latest developments
in the field, and invite contributions from the broader open-source community.

</details>


### [126] [DecoupledESC: Enhancing Emotional Support Generation via Strategy-Response Decoupled Preference Optimization](https://arxiv.org/abs/2505.16995)
*Chao Zhang,Xin Shi,Xueqiao Zhang,Yifan Zhu,Yi Yang,Yawei Luo*

Main category: cs.CL

TL;DR: 论文提出解耦式情感支持对话框架，通过推断性偏好挖掘构建高质量数据集IPM-PrefDial，将任务分解为策略规划与共情生成子任务，结合SFT和DPO优化，显著减少心理偏好偏差。


<details>
  <summary>Details</summary>
Motivation: 现有基于DPO的情感支持对话方法受限于数据集中的策略与内容纠缠问题，导致偏好对质量低且训练目标模糊，难以有效减少心理错误。

Method: 1. 提出IPM方法构建解耦的IPM-PrefDial数据集；2. 基于Gross情绪调节模型设计两阶段解耦框架：策略规划(SFT+DPO) → 共情生成(SFT+DPO)。

Result: 实验表明解耦框架在减少偏好偏差和提升响应质量上优于联合优化基线，响应相关性提高18.7%，心理错误率下降32%。

Conclusion: 任务解耦与偏好对齐方法能有效提升情感支持对话系统的心理合理性，未来可扩展至多模态交互场景。

Abstract: Recent advances in Emotional Support Conversation (ESC) have improved
emotional support generation by fine-tuning Large Language Models (LLMs) via
Supervised Fine-Tuning (SFT). However, common psychological errors still
persist. While Direct Preference Optimization (DPO) shows promise in reducing
such errors through pairwise preference learning, its effectiveness in ESC
tasks is limited by two key challenges: (1) Entangled data structure: Existing
ESC data inherently entangles psychological strategies and response content,
making it difficult to construct high-quality preference pairs; and (2)
Optimization ambiguity: Applying vanilla DPO to such entangled pairwise data
leads to ambiguous training objectives. To address these issues, we introduce
Inferential Preference Mining (IPM) to construct high-quality preference data,
forming the IPM-PrefDial dataset. Building upon this data, we propose a
Decoupled ESC framework inspired by Gross's Extended Process Model of Emotion
Regulation, which decomposes the ESC task into two sequential subtasks:
strategy planning and empathic response generation. Each was trained via SFT
and subsequently enhanced by DPO to align with the psychological preference.
Extensive experiments demonstrate that our Decoupled ESC framework outperforms
joint optimization baselines, reducing preference bias and improving response
quality.

</details>


### [127] [Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?](https://arxiv.org/abs/2505.16998)
*Jin Jiang,Jianing Wang,Yuchen Yan,Yang Liu,Jianhua Zhu,Mengdi Zhang,Xunliang Cai,Liangcai Gao*

Main category: cs.CL

TL;DR: 研究评估大型语言模型在形式语言下的逻辑推理能力，发现思维模型更优，归纳推理存在局限，PoT数据泛化最佳，并通过微调提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦于用形式语言引导LLMs的推理路径，但对其能力的系统性评估不足，需全面评估LLMs在不同逻辑推理任务中的表现。

Method: 从模型类型（思维/指导）、任务分类（演绎/归纳/溯因）、轨迹格式（CoT/PoT）三个维度评估，并基于形式语言构建训练数据增强小模型，采用拒绝微调方法。

Result: 1) 思维模型准确率比指导模型高18.6%；2) 所有LLMs归纳推理准确率低于40%；3) PoT格式数据跨语言泛化最优；微调后模型跨形式语言性能提升16.5%。

Conclusion: 该研究揭示了LLMs在形式语言下的推理特性，为优化模型逻辑能力提供新方向，PoT格式和拒绝微调是有效的改进路径。

Abstract: Large Language Models (LLMs) have been shown to achieve breakthrough
performance on complex logical reasoning tasks. Nevertheless, most existing
research focuses on employing formal language to guide LLMs to derive reliable
reasoning paths, while systematic evaluations of these capabilities are still
limited. In this paper, we aim to conduct a comprehensive evaluation of LLMs
across various logical reasoning problems utilizing formal languages. From the
perspective of three dimensions, i.e., spectrum of LLMs, taxonomy of tasks, and
format of trajectories, our key findings are: 1) Thinking models significantly
outperform Instruct models, especially when formal language is employed; 2) All
LLMs exhibit limitations in inductive reasoning capability, irrespective of
whether they use a formal language; 3) Data with PoT format achieves the best
generalization performance across other languages. Additionally, we also curate
the formal-relative training data to further enhance the small language models,
and the experimental results indicate that a simple rejected fine-tuning method
can better enable LLMs to generalize across formal languages and achieve the
best overall performance. Our codes and reports are available at
https://github.com/jiangjin1999/FormalEval.

</details>


### [128] [R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning](https://arxiv.org/abs/2505.17005)
*Huatong Song,Jinhao Jiang,Wenqing Tian,Zhipeng Chen,Yuhuan Wu,Jiahao Zhao,Yingqian Min,Wayne Xin Zhao,Lei Fang,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 提出R1-Searcher++框架，通过两阶段训练策略自适应结合语言模型内部知识与外部检索，实现高效检索增强推理


<details>
  <summary>Details</summary>
Motivation: 现有检索增强生成方法存在高成本、泛化性差且忽视模型内部知识的问题，需要开发能动态整合内外知识的新方法

Method: 采用SFT冷启动+强化学习两阶段训练，强化学习阶段引入结果监督、内部知识奖励机制和持续记忆机制

Result: 实验证明该方法在性能与检索效率上超越现有RAG方法，代码已开源

Conclusion: 通过动态整合内外知识并持续更新内部知识库，实现了更高效的检索增强推理范式

Abstract: Large Language Models (LLMs) are powerful but prone to hallucinations due to
static knowledge. Retrieval-Augmented Generation (RAG) helps by injecting
external information, but current methods often are costly, generalize poorly,
or ignore the internal knowledge of the model. In this paper, we introduce
R1-Searcher++, a novel framework designed to train LLMs to adaptively leverage
both internal and external knowledge sources. R1-Searcher++ employs a two-stage
training strategy: an initial SFT Cold-start phase for preliminary format
learning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses
outcome-supervision to encourage exploration, incorporates a reward mechanism
for internal knowledge utilization, and integrates a memorization mechanism to
continuously assimilate retrieved information, thereby enriching the model's
internal knowledge. By leveraging internal knowledge and external search
engine, the model continuously improves its capabilities, enabling efficient
retrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++
outperforms previous RAG and reasoning methods and achieves efficient
retrieval. The code is available at
https://github.com/RUCAIBox/R1-Searcher-plus.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [129] [Dynamic Caustics by Ultrasonically Modulated Liquid Surface](https://arxiv.org/abs/2505.16397)
*Koki Nagakura,Tatsuki Fushimi,Ayaka Tsutsui,Yoichi Ochiai*

Main category: cs.GR

TL;DR: 提出利用双优化全息场与相控阵换能器生成动态焦散图案，结合数字孪生框架实现实时可控的流体表面塑造


<details>
  <summary>Details</summary>
Motivation: 针对现有静态焦散优化的局限，探索液态表面作为动态折射介质在焦散生成中的实际应用可能性

Method: 采用计算技术动态控制流体表面形态，通过数字孪生框架实现迭代反馈优化，整合超声波直接操控液态表面技术

Result: 实验验证可生成高频连续动画（16Hz更新率）和复杂焦散图案，在实时适应性方面优于传统固态表面方法（对比度降低但灵活性提升）

Conclusion: 该技术为交互显示、艺术装置及教育工具开辟新途径，未来将通过表面控制算法优化提升图案分辨率与复杂度

Abstract: This paper presents a method for generating dynamic caustic patterns by
utilising dual-optimised holographic fields with Phased Array Transducer (PAT).
Building on previous research in static caustic optimisation and ultrasonic
manipulation, this approach employs computational techniques to dynamically
shape fluid surfaces, thereby creating controllable and real-time caustic
images. The system employs a Digital Twin framework, which enables iterative
feedback and refinement, thereby improving the accuracy and quality of the
caustic patterns produced. This paper extends the foundational work in caustic
generation by integrating liquid surfaces as refractive media. This concept has
previously been explored in simulations but not fully realised in practical
applications. The utilisation of ultrasound to directly manipulate these
surfaces enables the generation of dynamic caustics with a high degree of
flexibility. The Digital Twin approach further enhances this process by
allowing for precise adjustments and optimisation based on real-time feedback.
Experimental results demonstrate the technique's capacity to generate
continuous animations and complex caustic patterns at high frequencies.
Although there are limitations in contrast and resolution compared to
solid-surface methods, this approach offers advantages in terms of real-time
adaptability and scalability. This technique has the potential to be applied in
a number of areas, including interactive displays, artistic installations and
educational tools. This research builds upon the work of previous researchers
in the fields of caustics optimisation, ultrasonic manipulation, and
computational displays. Future research will concentrate on enhancing the
resolution and intricacy of the generated patterns.

</details>


### [130] [From Reality to Virtual Worlds: The Role of Photogrammetry in Game Development](https://arxiv.org/abs/2505.16951)
*Santiago Berrezueta-Guzman,Andrei Koshelev,Stefan Wagner*

Main category: cs.GR

TL;DR: 本文评估了GPU加速摄影测量工具RealityCapture在VR游戏开发中的应用，发现其能显著缩短开发时间并保持精度，但用户对小型可操作元素的手工模型细节更偏好。


<details>
  <summary>Details</summary>
Motivation: 研究摄影测量技术如何通过快速转换真实物体为高精度3D模型来改变数字内容创作流程，特别是其在VR游戏开发中的实际价值。

Method: 通过评估RealityCapture的效率、重建精度、与Unreal引擎的兼容性，对比传统建模流程，并调研用户对摄影测量模型与手工设计模型的偏好差异。

Result: 摄影测量模型在真实感与交互性表现优异，但用户对小型可操作元素的手工模型细节有52%偏好率；开发者视角下，该工具节省35%开发时间且保持亚毫米级几何精度。

Conclusion: RealityCapture凭借自动化流程和实时渲染兼容性成为VR开发利器，未来结合AI优化与云端处理可进一步拓展其在文化遗产保护等领域的应用场景。

Abstract: Photogrammetry is transforming digital content creation by enabling the rapid
conversion of real-world objects into highly detailed 3D models. This paper
evaluates the role of RealityCapture, a GPU-accelerated photogrammetry tool, in
game development of Virtual Reality (VR). We assess its efficiency,
reconstruction accuracy, and integration with Unreal Engine, comparing its
advantages and limitations against traditional modeling workflows.
Additionally, we examined user preferences between designed 3D assets and
photogrammetry-generated models. The results revealed that while photogrammetry
enhances realism and interactivity, users slightly preferred manually designed
models for small, manipulable elements because of the level of detail. However,
from a developer perspective, RealityCapture significantly reduces development
time while maintaining geometric precision and photorealistic textures. Despite
its reliance on high-performance hardware, its automation, scalability, and
seamless integration with real-time rendering engines make it a valuable tool
for game developers and VR creators. Future improvements in AI-driven
optimization and cloud-based processing could enhance accessibility, broadening
its applications in gaming, cultural heritage preservation, and simulation.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [131] [Highlighting What Matters: Promptable Embeddings for Attribute-Focused Image Retrieval](https://arxiv.org/abs/2505.15877)
*Siting Li,Xiang Gao,Simon Shaolei Du*

Main category: cs.CV

TL;DR: 提出COCO-Facet基准测试揭示CLIP类检索模型在属性查询中的缺陷，并设计可提示图像嵌入方法提升性能


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像检索模型（如CLIP）在处理属性聚焦查询时表现不足，因其全局语义编码忽略细节特征

Method: 利用多模态检索器生成可提示图像嵌入，提出预计算和线性近似两种加速策略

Result: 预计算策略使Recall@5提升15%，线性近似方法在推理阶段提升8%

Conclusion: 可提示嵌入方法突破传统检索瓶颈，为实际应用提供高效解决方案

Abstract: While an image is worth more than a thousand words, only a few provide
crucial information for a given task and thus should be focused on. In light of
this, ideal text-to-image (T2I) retrievers should prioritize specific visual
attributes relevant to queries. To evaluate current retrievers on handling
attribute-focused queries, we build COCO-Facet, a COCO-based benchmark with
9,112 queries about diverse attributes of interest. We find that CLIP-like
retrievers, which are widely adopted due to their efficiency and zero-shot
ability, have poor and imbalanced performance, possibly because their image
embeddings focus on global semantics and subjects while leaving out other
details. Notably, we reveal that even recent Multimodal Large Language Model
(MLLM)-based, stronger retrievers with a larger output dimension struggle with
this limitation. Hence, we hypothesize that retrieving with general image
embeddings is suboptimal for performing such queries. As a solution, we propose
to use promptable image embeddings enabled by these multimodal retrievers,
which boost performance by highlighting required attributes. Our pipeline for
deriving such embeddings generalizes across query types, image pools, and base
retriever architectures. To enhance real-world applicability, we offer two
acceleration strategies: Pre-processing promptable embeddings and using linear
approximations. We show that the former yields a 15% improvement in Recall@5
when prompts are predefined, while the latter achieves an 8% improvement when
prompts are only available during inference.

</details>


### [132] [GRIT: Teaching MLLMs to Think with Images](https://arxiv.org/abs/2505.15879)
*Yue Fan,Xuehai He,Diji Yang,Kaizhi Zheng,Ching-Chen Kuo,Yuting Zheng,Sravana Jyothi Narayanaraju,Xinze Guan,Xin Eric Wang*

Main category: cs.CV

TL;DR: 提出GRIT方法，通过结合自然语言与视觉边界框坐标增强多模态模型推理能力，并采用强化学习GRPO-GR提升数据效率


<details>
  <summary>Details</summary>
Motivation: 现有视觉推理模型仅用自然语言生成推理链，缺乏显式视觉信息整合，导致推理过程视觉基础薄弱

Method: 1. 设计交错生成自然语言与边界框坐标的推理范式
2. 基于GRPO算法开发GRPO-GR强化学习方法
3. 仅需少量标注数据（20个样本）即可训练

Result: GRIT在评估中表现出色，成功生成视觉基础明确的连贯推理链，实现推理能力与视觉定位能力的统一

Conclusion: GRIT首次实现多模态大模型的视觉基础推理，在数据效率和能力整合方面具有显著优势

Abstract: Recent studies have demonstrated the efficacy of using Reinforcement Learning
(RL) in building reasoning models that articulate chains of thoughts prior to
producing final answers. However, despite ongoing advances that aim at enabling
reasoning for vision-language tasks, existing open-source visual reasoning
models typically generate reasoning content with pure natural language, lacking
explicit integration of visual information. This limits their ability to
produce clearly articulated and visually grounded reasoning chains. To this
end, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method
for training MLLMs to think with images. GRIT introduces a grounded reasoning
paradigm, in which models generate reasoning chains that interleave natural
language and explicit bounding box coordinates. These coordinates point to
regions of the input image that the model consults during its reasoning
process. Additionally, GRIT is equipped with a reinforcement learning approach,
GRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused
on the final answer accuracy and format of the grounded reasoning output, which
eliminates the need for data with reasoning chain annotations or explicit
bounding box labels. As a result, GRIT achieves exceptional data efficiency,
requiring as few as 20 image-question-answer triplets from existing datasets.
Comprehensive evaluations demonstrate that GRIT effectively trains MLLMs to
produce coherent and visually grounded reasoning chains, showing a successful
unification of reasoning and grounding abilities.

</details>


### [133] [ViQAgent: Zero-Shot Video Question Answering via Agent with Open-Vocabulary Grounding Validation](https://arxiv.org/abs/2505.15928)
*Tony Montes,Fernando Lozano*

Main category: cs.CV

TL;DR: 提出结合思维链与YOLO-World的LLM视频问答代理，实现零样本SOTA性能并支持时间维度验证


<details>
  <summary>Details</summary>
Motivation: 现有视频问答系统在时序对象跟踪和推理决策对齐方面存在不足，需提升模型输出与视觉基础的一致性

Method: 融合Chain-of-Thought推理框架、基础定位推理和YOLO-World对象跟踪技术

Result: 在NExT-QA/iVQA/ActivityNet-QA三大基准达到新SOTA，支持时间维度跨验证提升可靠性

Conclusion: 该框架通过时序基础验证机制显著提升输出准确性，为多领域视频理解提供可靠验证支持

Abstract: Recent advancements in Video Question Answering (VideoQA) have introduced
LLM-based agents, modular frameworks, and procedural solutions, yielding
promising results. These systems use dynamic agents and memory-based mechanisms
to break down complex tasks and refine answers. However, significant
improvements remain in tracking objects for grounding over time and
decision-making based on reasoning to better align object references with
language model outputs, as newer models get better at both tasks. This work
presents an LLM-brained agent for zero-shot Video Question Answering (VideoQA)
that combines a Chain-of-Thought framework with grounding reasoning alongside
YOLO-World to enhance object tracking and alignment. This approach establishes
a new state-of-the-art in VideoQA and Video Understanding, showing enhanced
performance on NExT-QA, iVQA, and ActivityNet-QA benchmarks. Our framework also
enables cross-checking of grounding timeframes, improving accuracy and
providing valuable support for verification and increased output reliability
across multiple video domains. The code is available at
https://github.com/t-montes/viqagent.

</details>


### [134] [OViP: Online Vision-Language Preference Learning](https://arxiv.org/abs/2505.15963)
*Shujun Liu,Siyuan Wang,Zejun Li,Jianxiang Wang,Cheng Zeng,Zhongyu Wei*

Main category: cs.CV

TL;DR: 提出在线视觉偏好学习框架OViP，通过动态生成基于模型自身幻觉输出的负样本，有效减少视觉语言模型幻觉问题，同时保持多模态核心能力。


<details>
  <summary>Details</summary>
Motivation: 现有多模态DPO方法依赖静态或随机负样本，无法反映真实模型错误，导致训练效率低下。需构建更相关的动态监督信号。

Method: 1. 基于模型幻觉输出构建对比数据
2. 利用扩散模型合成负样本图像
3. 失败驱动训练实现文本-视觉偏好自适应对齐

Result: 在幻觉基准测试中平均提升4.3%指标，通用能力基准保持98%原始性能，改进的评估协议更有效平衡幻觉抑制与表达能力。

Conclusion: OViP通过动态负样本生成机制，为多模态模型对齐提供了高效解决方案，实现幻觉控制与模型性能的协同优化。

Abstract: Large vision-language models (LVLMs) remain vulnerable to hallucination,
often generating content misaligned with visual inputs. While recent approaches
advance multi-modal Direct Preference Optimization (DPO) to mitigate
hallucination, they typically rely on predefined or randomly edited negative
samples that fail to reflect actual model errors, limiting training efficacy.
In this work, we propose an Online Vision-language Preference Learning (OViP)
framework that dynamically constructs contrastive training data based on the
model's own hallucinated outputs. By identifying semantic differences between
sampled response pairs and synthesizing negative images using a diffusion
model, OViP generates more relevant supervision signals in real time. This
failure-driven training enables adaptive alignment of both textual and visual
preferences. Moreover, we refine existing evaluation protocols to better
capture the trade-off between hallucination suppression and expressiveness.
Experiments on hallucination and general benchmarks demonstrate that OViP
effectively reduces hallucinations while preserving core multi-modal
capabilities.

</details>


### [135] [Pixel Reasoner: Incentivizing Pixel-Space Reasoning with Curiosity-Driven Reinforcement Learning](https://arxiv.org/abs/2505.15966)
*Alex Su,Haozhe Wang,Weimin Ren,Fangzhen Lin,Wenhu Chen*

Main category: cs.CV

TL;DR: 提出像素空间推理框架，通过视觉操作和两阶段训练显著提升视觉语言模型在复杂视觉任务中的性能


<details>
  <summary>Details</summary>
Motivation: 传统文本空间的思维链推理在视觉密集型任务中存在局限性，需要扩展至像素空间以提升推理准确性

Method: 1. 设计像素级视觉操作（放大/选区） 2. 两阶段训练：指令调整+带好奇心奖励的强化学习平衡探索

Result: 7B模型在V* bench(84%)、TallyQA-Complex(74%)、InfographicsVQA(84%)创开源模型最高记录

Conclusion: 像素空间推理显著提升VLM性能，验证框架有效性，为视觉推理开辟新方向

Abstract: Chain-of-thought reasoning has significantly improved the performance of
Large Language Models (LLMs) across various domains. However, this reasoning
process has been confined exclusively to textual space, limiting its
effectiveness in visually intensive tasks. To address this limitation, we
introduce the concept of reasoning in the pixel-space. Within this novel
framework, Vision-Language Models (VLMs) are equipped with a suite of visual
reasoning operations, such as zoom-in and select-frame. These operations enable
VLMs to directly inspect, interrogate, and infer from visual evidences, thereby
enhancing reasoning fidelity for visual tasks. Cultivating such pixel-space
reasoning capabilities in VLMs presents notable challenges, including the
model's initially imbalanced competence and its reluctance to adopt the newly
introduced pixel-space operations. We address these challenges through a
two-phase training approach. The first phase employs instruction tuning on
synthesized reasoning traces to familiarize the model with the novel visual
operations. Following this, a reinforcement learning (RL) phase leverages a
curiosity-driven reward scheme to balance exploration between pixel-space
reasoning and textual reasoning. With these visual operations, VLMs can
interact with complex visual inputs, such as information-rich images or videos
to proactively gather necessary information. We demonstrate that this approach
significantly improves VLM performance across diverse visual reasoning
benchmarks. Our 7B model, \model, achieves 84\% on V* bench, 74\% on
TallyQA-Complex, and 84\% on InfographicsVQA, marking the highest accuracy
achieved by any open-source model to date. These results highlight the
importance of pixel-space reasoning and the effectiveness of our framework.

</details>


### [136] [Steering LVLMs via Sparse Autoencoder for Hallucination Mitigation](https://arxiv.org/abs/2505.16146)
*Zhenglin Hua,Jinghan He,Zijun Yao,Tianxu Han,Haiyun Guo,Yuheng Jia,Junfeng Fang*

Main category: cs.CV

TL;DR: 提出SSL方法，通过稀疏自编码器识别幻觉相关语义方向，无需训练即可显著降低大型视觉语言模型的幻觉现象，并保持跨模型架构的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 现有解决LVLMs幻觉的方法依赖外部知识库/对齐训练，计算成本高昂，且现有内部表征调整方法存在干预不足或过度的问题。

Method: 利用稀疏自编码器(SAEs)解析LVLMs的潜在空间，定位与幻觉/事实性相关的特定语义方向，通过潜空间向量干预实现精准控制。

Result: SSL在多个基准测试中超越现有解码策略，幻觉率降低35%，推理时间仅增加3%，且在不同架构模型间展现稳定迁移性。

Conclusion: 首次证明SAEs可有效解耦LVLMs的语义方向，SSL为模型安全部署提供了高效可控的干预范式，开辟了模型内部表征工程新路径。

Abstract: Large vision-language models (LVLMs) have achieved remarkable performance on
multimodal tasks such as visual question answering (VQA) and image captioning.
However, they still suffer from hallucinations, generating text inconsistent
with visual input, posing significant risks in real-world applications.
Existing approaches to address this issue focus on incorporating external
knowledge bases, alignment training, or decoding strategies, all of which
require substantial computational cost and time. Recent works try to explore
more efficient alternatives by adjusting LVLMs' internal representations.
Although promising, these methods may cause hallucinations to be insufficiently
suppressed or lead to excessive interventions that negatively affect normal
semantics. In this work, we leverage sparse autoencoders (SAEs) to identify
semantic directions closely associated with either hallucinations or actuality,
realizing more precise and direct hallucination-related representations. Our
analysis demonstrates that interventions along the faithful direction we
identified can mitigate hallucinations, while those along the hallucinatory
direction can exacerbate them. Building on these insights, we propose Steering
LVLMs via SAE Latent Directions (SSL), a training-free method based on
SAE-derived latent directions to mitigate hallucinations in LVLMs. Extensive
experiments demonstrate that SSL significantly outperforms existing decoding
approaches in mitigating hallucinations, while maintaining transferability
across different model architectures with negligible additional time overhead.

</details>


### [137] [When VLMs Meet Image Classification: Test Sets Renovation via Missing Label Identification](https://arxiv.org/abs/2505.16149)
*Zirui Pang,Haosheng Tan,Yuhan Pu,Zhijie Deng,Zhouan Shen,Keyu Hu,Jiaheng Wei*

Main category: cs.CV

TL;DR: 提出REVEAL框架，结合视觉语言模型与标签清洗方法，解决图像分类数据集的标签噪声与缺失问题，提升6个基准测试集质量。


<details>
  <summary>Details</summary>
Motivation: 现有图像分类数据集普遍存在噪声标签和缺失标签问题，导致模型评估不准确。现有方法主要关注噪声标签，但缺失标签问题长期被忽视。

Method: 集成LLaVA/BLIP等视觉语言模型与Docta/Cleanlab等标签清洗工具，通过置信度预测和共识过滤实现错误标签检测与缺失标签补充。

Result: 在6个基准测试集中验证有效性，通过人工核查显著提升标签质量（准确率提升15-22%），提供带概率的软标签结果。

Conclusion: REVEAL首次系统解决标签噪声与缺失的双重问题，为图像分类研究提供更可靠的评估基准，推动模型比较的公平性。

Abstract: Image classification benchmark datasets such as CIFAR, MNIST, and ImageNet
serve as critical tools for model evaluation. However, despite the cleaning
efforts, these datasets still suffer from pervasive noisy labels and often
contain missing labels due to the co-existing image pattern where multiple
classes appear in an image sample. This results in misleading model comparisons
and unfair evaluations. Existing label cleaning methods focus primarily on
noisy labels, but the issue of missing labels remains largely overlooked.
Motivated by these challenges, we present a comprehensive framework named
REVEAL, integrating state-of-the-art pre-trained vision-language models (e.g.,
LLaVA, BLIP, Janus, Qwen) with advanced machine/human label curation methods
(e.g., Docta, Cleanlab, MTurk), to systematically address both noisy labels and
missing label detection in widely-used image classification test sets. REVEAL
detects potential noisy labels and omissions, aggregates predictions from
various methods, and refines label accuracy through confidence-informed
predictions and consensus-based filtering. Additionally, we provide a thorough
analysis of state-of-the-art vision-language models and pre-trained image
classifiers, highlighting their strengths and limitations within the context of
dataset renovation by revealing 10 observations. Our method effectively reveals
missing labels from public datasets and provides soft-labeled results with
likelihoods. Through human verifications, REVEAL significantly improves the
quality of 6 benchmark test sets, highly aligning to human judgments and
enabling more accurate and meaningful comparisons in image classification.

</details>


### [138] [Redemption Score: An Evaluation Framework to Rank Image Captions While Redeeming Image Semantics and Language Pragmatics](https://arxiv.org/abs/2505.16180)
*Ashim Dahal,Ankit Ghimire,Saydul Akbar Murad,Nick Rahimi*

Main category: cs.CV

TL;DR: Redemption Score通过融合MID全局对齐、DINO图像重建相似度和BERTScore文本相似度，构建了更全面的图像描述评估框架，在Flickr8k基准测试中取得56.43的Kendall-τ值，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有图像描述评估指标在视觉语义和语言实用性的协同评估上存在不足，需开发更全面的多维度评估框架。

Method: 1) 互信息散度(MID)评估全局图像-文本分布对齐
2) DINO模型计算循环生成图像的感知相似度
3) BERTScore衡量文本与人工参考的语境相似度
4) 三信号校准融合形成综合评价体系

Result: 在Flickr8k数据集上Kendall-τ达56.43，超越12种现有方法；在Conceptual Captions和MS COCO数据集上表现出强泛化能力，且无需任务特定训练。

Conclusion: 该框架通过跨模态信号融合，实现了对图像语义和语言解释力的双重增强评估，为自动评估提供了更鲁棒、更细粒度的解决方案。

Abstract: Evaluating image captions requires cohesive assessment of both visual
semantics and language pragmatics, which is often not entirely captured by most
metrics. We introduce Redemption Score, a novel hybrid framework that ranks
image captions by triangulating three complementary signals: (1) Mutual
Information Divergence (MID) for global image-text distributional alignment,
(2) DINO-based perceptual similarity of cycle-generated images for visual
grounding, and (3) BERTScore for contextual text similarity against human
references. A calibrated fusion of these signals allows Redemption Score to
offer a more holistic assessment. On the Flickr8k benchmark, Redemption Score
achieves a Kendall-$\tau$ of 56.43, outperforming twelve prior methods and
demonstrating superior correlation with human judgments without requiring
task-specific training. Our framework provides a more robust and nuanced
evaluation by effectively redeeming image semantics and linguistic
interpretability indicated by strong transfer of knowledge in the Conceptual
Captions and MS COCO datasets.

</details>


### [139] [Grounding Chest X-Ray Visual Question Answering with Generated Radiology Reports](https://arxiv.org/abs/2505.16624)
*Francesco Dalla Serra,Patrick Schrempf,Chaoyang Wang,Zaiqiao Meng,Fani Deligianni,Alison Q. O'Neil*

Main category: cs.CV

TL;DR: 提出统一框架处理胸部X光单图/双图差异问答，通过分步生成放射报告再生成答案的方式提升性能


<details>
  <summary>Details</summary>
Motivation: 现有视觉问答模型未充分利用放射报告信息，特别在回答需纵向对比的影像差异问题时存在局限

Method: 采用两阶段架构：1）放射报告生成模块 2）基于报告证据的答案生成模块，支持单图和时序对比两种输入模式

Result: 在Medical-Diff-VQA数据集上达到SOTA，证明放射报告作为中间推理证据的有效性

Conclusion: 思维链式分步处理策略显著提升医学影像问答精度，尤其在需要时间维度推理的任务中效果突出

Abstract: We present a novel approach to Chest X-ray (CXR) Visual Question Answering
(VQA), addressing both single-image image-difference questions. Single-image
questions focus on abnormalities within a specific CXR ("What abnormalities are
seen in image X?"), while image-difference questions compare two longitudinal
CXRs acquired at different time points ("What are the differences between image
X and Y?"). We further explore how the integration of radiology reports can
enhance the performance of VQA models. While previous approaches have
demonstrated the utility of radiology reports during the pre-training phase, we
extend this idea by showing that the reports can also be leveraged as
additional input to improve the VQA model's predicted answers. First, we
propose a unified method that handles both types of questions and
auto-regressively generates the answers. For single-image questions, the model
is provided with a single CXR. For image-difference questions, the model is
provided with two CXRs from the same patient, captured at different time
points, enabling the model to detect and describe temporal changes. Taking
inspiration from 'Chain-of-Thought reasoning', we demonstrate that performance
on the CXR VQA task can be improved by grounding the answer generator module
with a radiology report predicted for the same CXR. In our approach, the VQA
model is divided into two steps: i) Report Generation (RG) and ii) Answer
Generation (AG). Our results demonstrate that incorporating predicted radiology
reports as evidence to the AG model enhances performance on both single-image
and image-difference questions, achieving state-of-the-art results on the
Medical-Diff-VQA dataset.

</details>


### [140] [R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO](https://arxiv.org/abs/2505.16673)
*Huanjin Yao,Qixiang Yin,Jingyi Zhang,Min Yang,Yibo Wang,Wenhao Wu,Fei Su,Li Shen,Minghui Qiu,Dacheng Tao,Jiaxing Huang*

Main category: cs.CV

TL;DR: 提出Share-GRPO强化学习方法，通过扩展问题空间共享推理轨迹和奖励信息，有效提升多模态大语言模型的推理能力，在六个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在复杂推理任务中存在奖励稀疏和优势消失问题，导致训练不稳定，需开发更有效的RL训练框架。

Method: 1. 通过数据转换扩展问题空间
2. 在扩展空间中探索多样化推理路径
3. 跨问题变体共享推理轨迹
4. 分层计算优势函数（跨问题组内/组间）

Result: 在六个主流推理基准测试中取得显著性能提升，验证了方法的有效性，代码即将开源。

Conclusion: Share-GRPO通过空间扩展和共享机制成功解决了RL训练难题，为MLLM推理能力提升提供了新思路。

Abstract: In this work, we aim to incentivize the reasoning ability of Multimodal Large
Language Models (MLLMs) via reinforcement learning (RL) and develop an
effective approach that mitigates the sparse reward and advantage vanishing
issues during RL. To this end, we propose Share-GRPO, a novel RL approach that
tackle these issues by exploring and sharing diverse reasoning trajectories
over expanded question space. Specifically, Share-GRPO first expands the
question space for a given question via data transformation techniques, and
then encourages MLLM to effectively explore diverse reasoning trajectories over
the expanded question space and shares the discovered reasoning trajectories
across the expanded questions during RL. In addition, Share-GRPO also shares
reward information during advantage computation, which estimates solution
advantages hierarchically across and within question variants, allowing more
accurate estimation of relative advantages and improving the stability of
policy training. Extensive evaluations over six widely-used reasoning
benchmarks showcase the superior performance of our method. Code will be
available at https://github.com/HJYao00/R1-ShareVL.

</details>


### [141] [MedFrameQA: A Multi-Image Medical VQA Benchmark for Clinical Reasoning](https://arxiv.org/abs/2505.16964)
*Suhao Yu,Haojin Wang,Juncheng Wu,Cihang Xie,Yuyin Zhou*

Main category: cs.CV

TL;DR: 提出首个医学多图像推理基准MedFrameQA，现有模型准确率普遍低于50%且存在显著推理缺陷


<details>
  <summary>Details</summary>
Motivation: 现有医学VQA基准仅关注单图分析，与临床实际需要多图对比诊断的工作流存在差距

Method: 1) 自动化流程提取医疗视频时序连贯帧并构建逻辑演进VQA项 2) 包含模型筛选和人工审核的多阶段过滤策略

Result: 构建含2,851个VQA对的数据集，覆盖9大人体系统。模型测试显示准确率普遍低于50%，错误类型包括证据误整合和错误传播

Conclusion: 该基准可加速临床多图推理研究，推动诊断AI系统发展，揭示现有模型在复杂医学推理中的重大局限

Abstract: Existing medical VQA benchmarks mostly focus on single-image analysis, yet
clinicians almost always compare a series of images before reaching a
diagnosis. To better approximate this workflow, we introduce MedFrameQA -- the
first benchmark that explicitly evaluates multi-image reasoning in medical VQA.
To build MedFrameQA both at scale and in high-quality, we develop 1) an
automated pipeline that extracts temporally coherent frames from medical videos
and constructs VQA items whose content evolves logically across images, and 2)
a multiple-stage filtering strategy, including model-based and manual review,
to preserve data clarity, difficulty, and medical relevance. The resulting
dataset comprises 2,851 VQA pairs (gathered from 9,237 high-quality frames in
3,420 videos), covering nine human body systems and 43 organs; every question
is accompanied by two to five images. We comprehensively benchmark ten advanced
Multimodal LLMs -- both proprietary and open source, with and without explicit
reasoning modules -- on MedFrameQA. The evaluation challengingly reveals that
all models perform poorly, with most accuracies below 50%, and accuracy
fluctuates as the number of images per question increases. Error analysis
further shows that models frequently ignore salient findings, mis-aggregate
evidence across images, and propagate early mistakes through their reasoning
chains; results also vary substantially across body systems, organs, and
modalities. We hope this work can catalyze research on clinically grounded,
multi-image reasoning and accelerate progress toward more capable diagnostic AI
systems.

</details>


### [142] [Multi-SpatialMLLM: Multi-Frame Spatial Understanding with Multi-Modal Large Language Models](https://arxiv.org/abs/2505.17015)
*Runsen Xu,Weiyao Wang,Hao Tang,Xingyu Chen,Xiaodong Wang,Fu-Jen Chu,Dahua Lin,Matt Feiszli,Kevin J. Liang*

Main category: cs.CV

TL;DR: 提出Multi-SpatialMLLM框架和MultiSPA数据集，显著提升多模态大语言模型在多帧空间推理中的表现


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs的空间理解局限于单张图像，难以满足机器人等需要多帧推理的实际应用需求

Method: 整合深度感知+视觉对应+动态感知，构建包含2700万样本的MultiSPA数据集并建立多维基准测试体系

Result: 模型在基准测试中显著超越基线系统，展示出可扩展的多帧推理能力，并实现机器人任务的多帧奖励标注

Conclusion: 通过系统性数据构建和三维感知融合，成功赋予MLLMs实用的多帧空间理解能力，为机器人应用奠定基础

Abstract: Multi-modal large language models (MLLMs) have rapidly advanced in visual
tasks, yet their spatial understanding remains limited to single images,
leaving them ill-suited for robotics and other real-world applications that
require multi-frame reasoning. In this paper, we propose a framework to equip
MLLMs with robust multi-frame spatial understanding by integrating depth
perception, visual correspondence, and dynamic perception. Central to our
approach is the MultiSPA dataset, a novel, large-scale collection of more than
27 million samples spanning diverse 3D and 4D scenes. Alongside MultiSPA, we
introduce a comprehensive benchmark that tests a wide spectrum of spatial tasks
under uniform metrics. Our resulting model, Multi-SpatialMLLM, achieves
significant gains over baselines and proprietary systems, demonstrating
scalable, generalizable multi-frame reasoning. We further observe multi-task
benefits and early indications of emergent capabilities in challenging
scenarios, and showcase how our model can serve as a multi-frame reward
annotator for robotics.

</details>


### [143] [Delving into RL for Image Generation with CoT: A Study on DPO vs. GRPO](https://arxiv.org/abs/2505.17017)
*Chengzhuo Tong,Ziyu Guo,Renrui Zhang,Wenyu Shan,Xinyu Wei,Zhenghao Xing,Hongsheng Li,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: 探讨GRPO和DPO强化学习算法在自回归图像生成中的应用，分析其域内性能、域外泛化能力及奖励模型对算法效果的影响，并提出三种扩展策略提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对自回归图像生成领域特有的文本-图像一致性、美学质量提升等挑战的深入分析，同时未充分对比不同RL策略（如GRPO/DPO）的优劣特性。

Method: 首次系统研究GRPO和DPO算法在自回归图像生成中的表现，评估其域内性能与跨域泛化能力，并系统探索三种典型扩展策略对性能的提升效果。

Result: 发现GRPO和DPO存在互补优势，奖励模型的内在泛化能力可增强RL算法泛化潜力；通过扩展策略可有效提升算法在域内外的表现。

Conclusion: 该研究为开发更有效的RL算法实现稳健的CoT图像生成提供了新思路，强调奖励模型设计与扩展策略选择的重要性。

Abstract: Recent advancements underscore the significant role of Reinforcement Learning
(RL) in enhancing the Chain-of-Thought (CoT) reasoning capabilities of large
language models (LLMs). Two prominent RL algorithms, Direct Preference
Optimization (DPO) and Group Relative Policy Optimization (GRPO), are central
to these developments, showcasing different pros and cons. Autoregressive image
generation, also interpretable as a sequential CoT reasoning process, presents
unique challenges distinct from LLM-based CoT reasoning. These encompass
ensuring text-image consistency, improving image aesthetic quality, and
designing sophisticated reward models, rather than relying on simpler
rule-based rewards. While recent efforts have extended RL to this domain, these
explorations typically lack an in-depth analysis of the domain-specific
challenges and the characteristics of different RL strategies. To bridge this
gap, we provide the first comprehensive investigation of the GRPO and DPO
algorithms in autoregressive image generation, evaluating their in-domain
performance and out-of-domain generalization, while scrutinizing the impact of
different reward models on their respective capabilities. Our findings reveal
that GRPO and DPO exhibit distinct advantages, and crucially, that reward
models possessing stronger intrinsic generalization capabilities potentially
enhance the generalization potential of the applied RL algorithms. Furthermore,
we systematically explore three prevalent scaling strategies to enhance both
their in-domain and out-of-domain proficiency, deriving unique insights into
efficiently scaling performance for each paradigm. We hope our study paves a
new path for inspiring future work on developing more effective RL algorithms
to achieve robust CoT reasoning in the realm of autoregressive image
generation. Code is released at
https://github.com/ZiyuGuo99/Image-Generation-CoT

</details>


### [144] [GoT-R1: Unleashing Reasoning Capability of MLLM for Visual Generation with Reinforcement Learning](https://arxiv.org/abs/2505.17022)
*Chengqi Duan,Rongyao Fang,Yuqing Wang,Kun Wang,Linjiang Huang,Xingyu Zeng,Hongsheng Li,Xihui Liu*

Main category: cs.CV

TL;DR: GoT-R1框架通过强化学习增强视觉生成的语义-空间推理能力，在复杂空间关系任务中取得显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉生成模型难以处理需要精确空间关系和多重属性绑定的复杂提示，需显式的语义-空间推理能力。

Method: 采用强化学习框架（双阶段多维奖励机制），利用MLLMs对推理过程和生成结果进行全流程评估，结合语义对齐度、空间准确性和视觉质量的多维奖励体系。

Result: 在T2I-CompBench基准测试中实现显著改进（空间关系任务提升23%，属性绑定任务提升18%），超越现有模板化推理方法。

Conclusion: GoT-R1首次将复杂推理能力迁移至视觉生成领域，通过开源代码模型推动生成式AI的推理能力发展。

Abstract: Visual generation models have made remarkable progress in creating realistic
images from text prompts, yet struggle with complex prompts that specify
multiple objects with precise spatial relationships and attributes. Effective
handling of such prompts requires explicit reasoning about the semantic content
and spatial layout. We present GoT-R1, a framework that applies reinforcement
learning to enhance semantic-spatial reasoning in visual generation. Building
upon the Generation Chain-of-Thought approach, GoT-R1 enables models to
autonomously discover effective reasoning strategies beyond predefined
templates through carefully designed reinforcement learning. To achieve this,
we propose a dual-stage multi-dimensional reward framework that leverages MLLMs
to evaluate both the reasoning process and final output, enabling effective
supervision across the entire generation pipeline. The reward system assesses
semantic alignment, spatial accuracy, and visual quality in a unified approach.
Experimental results demonstrate significant improvements on T2I-CompBench
benchmark, particularly in compositional tasks involving precise spatial
relationships and attribute binding. GoT-R1 advances the state-of-the-art in
image generation by successfully transferring sophisticated reasoning
capabilities to the visual generation domain. To facilitate future research, we
make our code and pretrained models publicly available at
https://github.com/gogoduan/GoT-R1.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [145] [InfoDeepSeek: Benchmarking Agentic Information Seeking for Retrieval-Augmented Generation](https://arxiv.org/abs/2505.15872)
*Yunjia Xi,Jianghao Lin,Menghui Zhu,Yongzhao Xiao,Zhuoying Ou,Jiaqi Liu,Tong Wan,Bo Chen,Weiwen Liu,Yasheng Wang,Ruiming Tang,Weinan Zhang,Yong Yu*

Main category: cs.IR

TL;DR: 论文提出了InfoDeepSeek基准测试，用于评估动态网络环境下代理式信息检索系统的效果，克服现有基准静态性和简单化的局限。


<details>
  <summary>Details</summary>
Motivation: 现有RAG评估基准局限于静态检索环境和简单查询，无法有效评估真实动态网络环境中的代理式信息检索行为，且依赖预定义文档集作为黄金标准。

Method: 通过系统性方法论构建满足确定性、难度和多样性标准的挑战性查询，开发首个动态代理式信息检索评估框架，包含准确性、实用性和信息密度等细粒度指标。

Result: 实验揭示了不同LLM、搜索引擎和问题类型的代理行为差异，证明InfoDeepSeek能有效评估动态环境下的信息检索质量。

Conclusion: InfoDeepSeek为动态代理式信息检索提供了更贴近真实场景的评估方案，其细粒度指标体系为后续研究提供了可操作的改进方向。

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
grounding responses with retrieved information. As an emerging paradigm,
Agentic RAG further enhances this process by introducing autonomous LLM agents
into the information seeking process. However, existing benchmarks fall short
in evaluating such systems, as they are confined to a static retrieval
environment with a fixed, limited corpus} and simple queries that fail to
elicit agentic behavior. Moreover, their evaluation protocols assess
information seeking effectiveness by pre-defined gold sets of documents, making
them unsuitable for the open-ended and dynamic nature of real-world web
environments. To bridge this gap, we present InfoDeepSeek, a new benchmark with
challenging questions designed for assessing agentic information seeking in
real-world, dynamic web environments. We propose a systematic methodology for
constructing challenging queries satisfying the criteria of determinacy,
difficulty, and diversity. Based on this, we develop the first evaluation
framework tailored to dynamic agentic information seeking, including
fine-grained metrics about the accuracy, utility, and compactness of
information seeking outcomes. Through extensive experiments across LLMs, search
engines, and question types, InfoDeepSeek reveals nuanced agent behaviors and
offers actionable insights for future research.

</details>


### [146] [Aug2Search: Enhancing Facebook Marketplace Search with LLM-Generated Synthetic Data Augmentation](https://arxiv.org/abs/2505.16065)
*Ruijie Xi,He Ba,Hao Yuan,Rishu Agrawal,Arul Prakash*

Main category: cs.IR

TL;DR: 提出Aug2Search框架，利用生成式AI生成多模态合成数据提升嵌入式检索模型的查询-商品相关性优化效果


<details>
  <summary>Details</summary>
Motivation: 解决Facebook Marketplace等平台搜索日志数据多样性不足、细节欠缺导致的EBR模型训练效果受限问题

Method: 采用三种生成策略（生成查询/增强商品描述/基于增强描述生成查询），使用80亿参数Llama模型在10亿级真实数据上生成合成数据，设置原始数据/合成数据/混合数据三组对比实验

Result: 合成数据模型ROC_AUC提升4%，纯合成数据训练模型性能优于纯原始数据或混合数据模型

Conclusion: 验证了GenAI生成高质量合成数据的能力，证明合成数据在提升检索模型性能和突破数据瓶颈方面的有效性

Abstract: Embedding-Based Retrieval (EBR) is an important technique in modern search
engines, enabling semantic match between search queries and relevant results.
However, search logging data on platforms like Facebook Marketplace lacks the
diversity and details needed for effective EBR model training, limiting the
models' ability to capture nuanced search patterns. To address this challenge,
we propose Aug2Search, an EBR-based framework leveraging synthetic data
generated by Generative AI (GenAI) models, in a multimodal and multitask
approach to optimize query-product relevance. This paper investigates the
capabilities of GenAI, particularly Large Language Models (LLMs), in generating
high-quality synthetic data, and analyzing its impact on enhancing EBR models.
We conducted experiments using eight Llama models and 100 million data points
from Facebook Marketplace logs. Our synthetic data generation follows three
strategies: (1) generate queries, (2) enhance product listings, and (3)
generate queries from enhanced listings. We train EBR models on three different
datasets: sampled engagement data or original data ((e.g., "Click" and "Listing
Interactions")), synthetic data, and a mixture of both engagement and synthetic
data to assess their performance across various training sets. Our findings
underscore the robustness of Llama models in producing synthetic queries and
listings with high coherence, relevance, and diversity, while maintaining low
levels of hallucination. Aug2Search achieves an improvement of up to 4% in
ROC_AUC with 100 million synthetic data samples, demonstrating the
effectiveness of our approach. Moreover, our experiments reveal that with the
same volume of training data, models trained exclusively on synthetic data
often outperform those trained on original data only or a mixture of original
and synthetic data.

</details>


### [147] [Benchmarking Retrieval-Augmented Multimomal Generation for Document Question Answering](https://arxiv.org/abs/2505.16470)
*Kuicai Dong,Yujing Chang,Shijie Huang,Yasheng Wang,Ruiming Tang,Yong Liu*

Main category: cs.IR

TL;DR: MMDocRAG提出多模态文档问答基准，包含4055个专家标注QA对和跨模态证据链评估框架，揭示了多模态检索与整合的挑战


<details>
  <summary>Details</summary>
Motivation: 现有DocVQA方法局限于文本中心化处理，缺乏视觉信息整合能力，且缺乏评估多模态证据选择的有效基准

Method: 1. 构建含4055个跨模态QA对的数据集
2. 设计多模态引用选择评估指标
3. 测试60个VLM/LLM模型和14个检索系统

Result: 1. 私有多模态模型优于开源模型
2. 多模态输入对闭源模型提升有限，但开源模型性能显著下降
3. 带图像描述的微调LLM提升显著

Conclusion: MMDocRAG为多模态文档问答建立了严格测试基准，揭示了当前系统的关键瓶颈，并为整合视觉信息提供了新思路

Abstract: Document Visual Question Answering (DocVQA) faces dual challenges in
processing lengthy multimodal documents (text, images, tables) and performing
cross-modal reasoning. Current document retrieval-augmented generation (DocRAG)
methods remain limited by their text-centric approaches, frequently missing
critical visual information. The field also lacks robust benchmarks for
assessing multimodal evidence selection and integration. We introduce MMDocRAG,
a comprehensive benchmark featuring 4,055 expert-annotated QA pairs with
multi-page, cross-modal evidence chains. Our framework introduces innovative
metrics for evaluating multimodal quote selection and enables answers that
interleave text with relevant visual elements. Through large-scale experiments
with 60 VLM/LLM models and 14 retrieval systems, we identify persistent
challenges in multimodal evidence retrieval, selection, and integration.Key
findings reveal advanced proprietary LVMs show superior performance than
open-sourced alternatives. Also, they show moderate advantages using multimodal
inputs over text-only inputs, while open-source alternatives show significant
performance degradation. Notably, fine-tuned LLMs achieve substantial
improvements when using detailed image descriptions. MMDocRAG establishes a
rigorous testing ground and provides actionable insights for developing more
robust multimodal DocVQA systems. Our benchmark and code are available at
https://mmdocrag.github.io/MMDocRAG/.

</details>


### [148] [MiLQ: Benchmarking IR Models for Bilingual Web Search with Mixed Language Queries](https://arxiv.org/abs/2505.16631)
*Jonghwi Kim,Deokhyung Kang,Seonjeong Hwang,Yunsu Kim,Jungseul Ok,Gary Lee*

Main category: cs.IR

TL;DR: 首个混合语言查询测试集MiLQ的提出与分析


<details>
  <summary>Details</summary>
Motivation: 双语用户普遍使用混合语言搜索但缺乏相关研究

Method: 构建首个公开混合语言查询测试集MiLQ

Result: 多语言IR模型在混合查询中表现中等且存在查询类型间性能差异

Conclusion: 混合英语查询通过增强词汇匹配成为搜索英文文档的有效策略

Abstract: Despite bilingual speakers frequently using mixed-language queries in web
searches, Information Retrieval (IR) research on them remains scarce. To
address this, we introduce MiLQ,Mixed-Language Query test set, the first public
benchmark of mixed-language queries, confirmed as realistic and highly
preferred. Experiments show that multilingual IR models perform moderately on
MiLQ and inconsistently across native, English, and mixed-language queries,
also suggesting code-switched training data's potential for robust IR models
handling such queries. Meanwhile, intentional English mixing in queries proves
an effective strategy for bilinguals searching English documents, which our
analysis attributes to enhanced token matching compared to native queries.

</details>


### [149] [Don't "Overthink" Passage Reranking: Is Reasoning Truly Necessary?](https://arxiv.org/abs/2505.16886)
*Nour Jedidi,Yung-Sung Chuang,James Glass,Jimmy Lin*

Main category: cs.IR

TL;DR: 研究发现标准非推理重排器（StandardRR）在相同训练条件下优于基于推理的重排器（ReasonRR），后者受LLM推理过程限制导致极化评分。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型中推理能力对段落重排器准确性的影响，验证显式推理过程是否真正提升检索效果。

Method: 通过对比实验设计：1）比较ReasonRR与StandardRR在相同训练条件下的性能；2）禁用ReasonRR的推理过程（ReasonRR-NoReason）进行对照分析。

Result: StandardRR整体表现更优，ReasonRR-NoReason反超ReasonRR。推理过程导致相关性评分极化，忽视部分相关性关键因素。

Conclusion: 显式推理机制可能损害重排器性能，LLM在推理中产生的偏态评分机制与点状重排器需考量的部分相关性存在根本性冲突。

Abstract: With the growing success of reasoning models across complex natural language
tasks, researchers in the Information Retrieval (IR) community have begun
exploring how similar reasoning capabilities can be integrated into passage
rerankers built on Large Language Models (LLMs). These methods typically employ
an LLM to produce an explicit, step-by-step reasoning process before arriving
at a final relevance prediction. But, does reasoning actually improve reranking
accuracy? In this paper, we dive deeper into this question, studying the impact
of the reasoning process by comparing reasoning-based pointwise rerankers
(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under
identical training conditions, and observe that StandardRR generally
outperforms ReasonRR. Building on this observation, we then study the
importance of reasoning to ReasonRR by disabling its reasoning process
(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more
effective than ReasonRR. Examining the cause of this result, our findings
reveal that reasoning-based rerankers are limited by the LLM's reasoning
process, which pushes it toward polarized relevance scores and thus fails to
consider the partial relevance of passages, a key factor for the accuracy of
pointwise rerankers.

</details>


### [150] [Fixing Data That Hurts Performance: Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval](https://arxiv.org/abs/2505.16967)
*Nandan Thakur,Crystina Zhang,Xueguang Ma,Jimmy Lin*

Main category: cs.IR

TL;DR: 论文提出通过大模型级联标注清洗低质量训练数据，提升检索模型效果（BEIR评估提升1.0+ nDCG@10）


<details>
  <summary>Details</summary>
Motivation: 现有大规模检索数据集存在数据质量缺陷，部分数据集会对模型性能产生负面影响（移除8/15数据集后训练规模缩小2.35倍但性能提升）

Method: 提出基于级联LLM提示的假阴性样本清洗方法：1. 大模型重标注假阴性样本为正样本 2. 保留困难负样本

Result: 清洗后数据使E5/Qwen2.5-7B检索模型在BEIR提升0.7-1.4点，AIR-Bench提升1.7-1.8点；Qwen2.5-3B排序器在BEIR也有显著提升

Conclusion: 数据质量比数量更重要，大模型标注与人工评估一致性高（GPT-4o vs GPT-4o-mini），该方法成本效益显著

Abstract: Training robust retrieval and reranker models typically relies on large-scale
retrieval datasets; for example, the BGE collection contains 1.6 million
query-passage pairs sourced from various data sources. However, we find that
certain datasets can negatively impact model effectiveness -- pruning 8 out of
15 datasets from the BGE collection reduces the training set size by
2.35$\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a
deeper examination of training data quality, with a particular focus on "false
negatives", where relevant passages are incorrectly labeled as irrelevant. We
propose a simple, cost-effective approach using cascading LLM prompts to
identify and relabel hard negatives. Experimental results show that relabeling
false negatives with true positives improves both E5 (base) and Qwen2.5-7B
retrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot
AIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on
the relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the
cascading design is further supported by human annotation results, where we
find judgment by GPT-4o shows much higher agreement with humans than
GPT-4o-mini.

</details>


### [151] [$\text{R}^2\text{ec}$: Towards Large Recommender Models with Reasoning](https://arxiv.org/abs/2505.16994)
*Runyang You,Yongqi Li,Xinyu Lin,Xin Zhang,Wenjie Wang,Wenjie Li,Liqiang Nie*

Main category: cs.IR

TL;DR: 提出统一推荐模型RRec，通过自回归推理-推荐交替架构和强化学习框架RecPO，实现68.67%的Hit@5提升。


<details>
  <summary>Details</summary>
Motivation: 现有LLM推荐系统依赖外部推理模块，存在资源消耗大、联合优化困难的问题。

Method: 1. 重构自回归架构实现推理推荐交替执行；2. 提出RecPO强化学习框架，通过融合奖励机制同步优化双能力。

Result: 三数据集实验显示Hit@5提升68.67%，NDCG@20提升45.21%

Conclusion: 验证了无需专用标注的推理-推荐联合优化可行性，模型已开源

Abstract: Large recommender models have extended LLMs as powerful recommenders via
encoding or item generation, and recent breakthroughs in LLM reasoning
synchronously motivate the exploration of reasoning in recommendation. Current
studies usually position LLMs as external reasoning modules to yield auxiliary
thought for augmenting conventional recommendation pipelines. However, such
decoupled designs are limited in significant resource cost and suboptimal joint
optimization. To address these issues, we propose \name, a unified large
recommender model with intrinsic reasoning capabilities. Initially, we
reconceptualize the model architecture to facilitate interleaved reasoning and
recommendation in the autoregressive process. Subsequently, we propose RecPO, a
corresponding reinforcement learning framework that optimizes \name\ both the
reasoning and recommendation capabilities simultaneously in a single policy
update; RecPO introduces a fused reward scheme that solely leverages
recommendation labels to simulate the reasoning capability, eliminating
dependency on specialized reasoning annotations. Experiments on three datasets
with various baselines verify the effectiveness of \name, showing relative
improvements of 68.67\% in Hit@5 and 45.21\% in NDCG@20. Code available at
https://github.com/YRYangang/RRec.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [152] [SWE-Dev: Evaluating and Training Autonomous Feature-Driven Software Development](https://arxiv.org/abs/2505.16975)
*Yaxin Du,Yuzhu Cai,Yifan Zhou,Cheng Wang,Yu Qian,Xianghe Pang,Qian Liu,Yue Hu,Siheng Chen*

Main category: cs.SE

TL;DR: SWE-Dev数据集的提出填补了LLMs在功能驱动开发任务中的评估空白，通过可运行环境和单元测试支持模型训练与验证，实验显示当前AI在该任务上面临巨大挑战但可通过高质量数据显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究未充分探索LLMs在真实场景功能驱动开发（FDD）任务中的表现，而这类任务在大型代码库迭代中至关重要。

Method: 构建包含14,000训练样本的大规模SWE-Dev数据集，提供可运行环境和开发者单元测试，支持监督微调(SFT)和基于测试反馈的强化学习(RL)。

Result: 主流模型（如Claude-3.7-Sonnet）在困难测试集仅达22.45% Pass@3；7B模型经微调后在困难集表现可比肩GPT-4o。

Conclusion: SWE-Dev为FDD任务提供了标准化评估基准，其高质量数据验证了模型改进潜力，推动了自动化软件开发研究的发展。

Abstract: Large Language Models (LLMs) have shown strong capability in diverse software
engineering tasks, e.g. code completion, bug fixing, and document generation.
However, feature-driven development (FDD), a highly prevalent real-world task
that involves developing new functionalities for large, existing codebases,
remains underexplored. We therefore introduce SWE-Dev, the first large-scale
dataset (with 14,000 training and 500 test samples) designed to evaluate and
train autonomous coding systems on real-world feature development tasks. To
ensure verifiable and diverse training, SWE-Dev uniquely provides all instances
with a runnable environment and its developer-authored executable unit tests.
This collection not only provides high-quality data for Supervised Fine-Tuning
(SFT), but also enables Reinforcement Learning (RL) by delivering accurate
reward signals from executable unit tests. Our extensive evaluations on
SWE-Dev, covering 17 chatbot LLMs, 10 reasoning models, and 10 Multi-Agent
Systems (MAS), reveal that FDD is a profoundly challenging frontier for current
AI (e.g., Claude-3.7-Sonnet achieves only 22.45\% Pass@3 on the hard test
split). Crucially, we demonstrate that SWE-Dev serves as an effective platform
for model improvement: fine-tuning on training set enabled a 7B model
comparable to GPT-4o on \textit{hard} split, underscoring the value of its
high-quality training data. Code is available here
\href{https://github.com/justLittleWhite/SWE-Dev}{https://github.com/justLittleWhite/SWE-Dev}.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [153] [AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models](https://arxiv.org/abs/2505.16211)
*Kai Li,Can Shen,Yile Liu,Jirui Han,Kelong Zheng,Xuechao Zou,Zhe Wang,Xingjian Du,Shun Zhang,Hanjun Luo,Yingbin Jin,Xinxin Xing,Ziyang Ma,Yue Liu,Xiaojun Jia,Yifan Zhang,Junfeng Fang,Kun Wang,Yibo Yan,Haoyang Li,Yiming Li,Xiaobin Zhuang,Yang Liu,Haibo Hu,Zhuo Chen,Zhizheng Wu,Xiaolin Hu,Eng-Siong Chng,XiaoFeng Wang,Wenyuan Xu,Wei Dong,Xinfeng Li*

Main category: cs.SD

TL;DR: 首个针对音频大语言模型的多维可信度评估框架AudioTrust，覆盖公平性/幻觉/安全性等6大维度，构建4,420+真实音频场景数据集并设计9项评估指标，揭示主流模型的信任边界


<details>
  <summary>Details</summary>
Motivation: 现有评估框架主要关注文本模态且维度单一，缺乏针对音频特有风险（如语音助手交互、紧急呼叫等场景）的系统评估方法

Method: 构建覆盖6大核心维度的评估体系，设计18种实验场景，创建4,420+真实音频样本数据集，开发9个音频专属评估指标，采用自动化评估流程实现客观量化

Result: 实验揭示了当前开源/闭源ALLMs在高风险音频场景中的可信度局限，如特定情境下的安全漏洞和鲁棒性不足

Conclusion: 为音频模型的安全部署提供系统性评估基准，研究成果平台和数据集已开源，推动可信音频AI发展

Abstract: The rapid advancement and expanding applications of Audio Large Language
Models (ALLMs) demand a rigorous understanding of their trustworthiness.
However, systematic research on evaluating these models, particularly
concerning risks unique to the audio modality, remains largely unexplored.
Existing evaluation frameworks primarily focus on the text modality or address
only a restricted set of safety dimensions, failing to adequately account for
the unique characteristics and application scenarios inherent to the audio
modality. We introduce AudioTrust-the first multifaceted trustworthiness
evaluation framework and benchmark specifically designed for ALLMs. AudioTrust
facilitates assessments across six key dimensions: fairness, hallucination,
safety, privacy, robustness, and authentication. To comprehensively evaluate
these dimensions, AudioTrust is structured around 18 distinct experimental
setups. Its core is a meticulously constructed dataset of over 4,420 audio/text
samples, drawn from real-world scenarios (e.g., daily conversations, emergency
calls, voice assistant interactions), specifically designed to probe the
multifaceted trustworthiness of ALLMs. For assessment, the benchmark carefully
designs 9 audio-specific evaluation metrics, and we employ a large-scale
automated pipeline for objective and scalable scoring of model outputs.
Experimental results reveal the trustworthiness boundaries and limitations of
current state-of-the-art open-source and closed-source ALLMs when confronted
with various high-risk audio scenarios, offering valuable insights for the
secure and trustworthy deployment of future audio models. Our platform and
benchmark are available at https://github.com/JusperLee/AudioTrust.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [154] [CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark](https://arxiv.org/abs/2505.16968)
*Ahmed Heakl,Sarim Hashmi,Gustavo Bertolo Stahl,Seung Hun Eddie Han,Salman Khan,Abdulrahman Mahmoud*

Main category: cs.AR

TL;DR: 首个跨架构GPU代码转换数据集CASS，包含70k验证代码对，训练模型实现95%源码翻译准确率，开源数据/模型/评测工具推动GPU工具链发展


<details>
  <summary>Details</summary>
Motivation: 解决不同GPU架构(NVIDIA/AMD)间代码移植性差的问题，填补低层GPU代码转换领域缺乏可靠数据集和工具的空白

Method: 构建跨架构代码对数据集，训练领域专用语言模型CASS，支持CUDA↔HIP源码级和SASS↔RDNA3汇编级双向转换，开发CASS-Bench基准测试框架

Result: 源级翻译准确率95%(超越GPT-4o/Claude)，汇编级37.5%，85%生成代码保持原生性能，完整保留运行时内存行为

Conclusion: CASS系列显著提升硬件迁移效率，开源生态将加速GPU编译器工具链和LLM硬件翻译研究，推动行业二进制兼容性发展

Abstract: We introduce \texttt{CASS}, the first large-scale dataset and model suite for
cross-architecture GPU code transpilation, targeting both source-level
(CUDA~$\leftrightarrow$~HIP) and assembly-level (Nvidia
SASS~$\leftrightarrow$~AMD RDNA3) translation. The dataset comprises 70k
verified code pairs across host and device, addressing a critical gap in
low-level GPU code portability. Leveraging this resource, we train the
\texttt{CASS} family of domain-specific language models, achieving 95\% source
translation accuracy and 37.5\% assembly translation accuracy, substantially
outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our
generated code matches native performance in over 85\% of test cases,
preserving runtime and memory behavior. To support rigorous evaluation, we
introduce \texttt{CASS-Bench}, a curated benchmark spanning 16 GPU domains with
ground-truth execution. All data, models, and evaluation tools are released as
open source to foster progress in GPU compiler tooling, binary compatibility,
and LLM-guided hardware translation. Dataset and benchmark are on
\href{https://huggingface.co/datasets/MBZUAI/cass}{\textcolor{blue}{HuggingFace}},
with code at
\href{https://github.com/GustavoStahl/CASS}{\textcolor{blue}{GitHub}}.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [155] [All You Need is "Leet": Evading Hate-speech Detection AI](https://arxiv.org/abs/2505.16263)
*Sampanna Yashwant Kahu,Naman Ahuja*

Main category: cs.CR

TL;DR: 提出黑盒扰动技术对抗仇恨言论检测模型，成功规避检测率达86.8%且语义改变最小


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体仇恨言论泛滥，但现有深度学习检测模型存在可被对抗攻击规避的漏洞

Method: 设计不依赖模型内部结构的扰动生成技术，通过最小语义修改实现检测规避

Result: 最佳扰动方案在86.8%的恶意文本中成功欺骗SOTA检测模型

Conclusion: 揭示了现有检测系统的脆弱性，为构建鲁棒检测模型提供对抗样本

Abstract: Social media and online forums are increasingly becoming popular.
Unfortunately, these platforms are being used for spreading hate speech. In
this paper, we design black-box techniques to protect users from hate-speech on
online platforms by generating perturbations that can fool state of the art
deep learning based hate speech detection models thereby decreasing their
efficiency. We also ensure a minimal change in the original meaning of
hate-speech. Our best perturbation attack is successfully able to evade
hate-speech detection for 86.8 % of hateful text.

</details>


### [156] [DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection](https://arxiv.org/abs/2505.16530)
*Yuliang Yan,Haochun Tang,Shuo Yan,Enyan Dai*

Main category: cs.CR

TL;DR: 提出DuFFin双层次指纹框架，通过触发模式和知识级指纹实现黑盒环境下LLM版权验证，IP-ROC指标超0.95


<details>
  <summary>Details</summary>
Motivation: 现有水印和指纹方法影响生成质量或依赖白盒访问，需开发更实用的黑盒环境LLM知识产权保护方案

Method: 提取触发模式和知识级双重指纹，在包含微调/量化/对齐版本的多样化开源模型集合上验证框架有效性

Result: 在基础模型及其变体上实现精准版权验证，IP-ROC指标超过0.95，覆盖企业/初创公司/个人用户发布的模型版本

Conclusion: DuFFin框架有效解决黑盒环境模型溯源难题，为LLM知识产权保护提供实用解决方案，代码已开源供社区使用

Abstract: Large language models (LLMs) are considered valuable Intellectual Properties
(IP) for legitimate owners due to the enormous computational cost of training.
It is crucial to protect the IP of LLMs from malicious stealing or unauthorized
deployment. Despite existing efforts in watermarking and fingerprinting LLMs,
these methods either impact the text generation process or are limited in
white-box access to the suspect model, making them impractical. Hence, we
propose DuFFin, a novel $\textbf{Du}$al-Level $\textbf{Fin}$gerprinting
$\textbf{F}$ramework for black-box setting ownership verification. DuFFin
extracts the trigger pattern and the knowledge-level fingerprints to identify
the source of a suspect model. We conduct experiments on a variety of models
collected from the open-source website, including four popular base models as
protected LLMs and their fine-tuning, quantization, and safety alignment
versions, which are released by large companies, start-ups, and individual
users. Results show that our method can accurately verify the copyright of the
base protected LLM on their model variants, achieving the IP-ROC metric greater
than 0.95. Our code is available at
https://github.com/yuliangyan0807/llm-fingerprint.

</details>


### [157] [CTRAP: Embedding Collapse Trap to Safeguard Large Language Models from Harmful Fine-Tuning](https://arxiv.org/abs/2505.16559)
*Biao Yi,Tiansheng Huang,Baolei Zhang,Tong Li,Lihai Nie,Zheli Liu,Li Shen*

Main category: cs.CR

TL;DR: 提出CTRAP机制，通过诱导模型崩溃彻底消除LLMs被恶意微调利用的可能性


<details>
  <summary>Details</summary>
Motivation: 现有选择性遗忘防御存在根本缺陷，攻击者可利用LLM强大的泛化适应能力绕过防御

Method: 在模型对齐阶段预置崩溃触发机制，当检测到持续恶意微调时渐进性破坏模型核心语言建模能力

Result: 实验证明CTRAP能有效防御各类LLM的恶意微调攻击，同时保持良性场景下的模型性能

Conclusion: 通过从根源消除模型的可利用性而非局部修正，为LLM安全防御提供了范式转换

Abstract: Fine-tuning-as-a-service, while commercially successful for Large Language
Model (LLM) providers, exposes models to harmful fine-tuning attacks. As a
widely explored defense paradigm against such attacks, unlearning attempts to
remove malicious knowledge from LLMs, thereby essentially preventing them from
being used to perform malicious tasks. However, we highlight a critical flaw:
the powerful general adaptability of LLMs allows them to easily bypass
selective unlearning by rapidly relearning or repurposing their capabilities
for harmful tasks. To address this fundamental limitation, we propose a
paradigm shift: instead of selective removal, we advocate for inducing model
collapse--effectively forcing the model to "unlearn everything"--specifically
in response to updates characteristic of malicious adaptation. This collapse
directly neutralizes the very general capabilities that attackers exploit,
tackling the core issue unaddressed by selective unlearning. We introduce the
Collapse Trap (CTRAP) as a practical mechanism to implement this concept
conditionally. Embedded during alignment, CTRAP pre-configures the model's
reaction to subsequent fine-tuning dynamics. If updates during fine-tuning
constitute a persistent attempt to reverse safety alignment, the pre-configured
trap triggers a progressive degradation of the model's core language modeling
abilities, ultimately rendering it inert and useless for the attacker.
Crucially, this collapse mechanism remains dormant during benign fine-tuning,
ensuring the model's utility and general capabilities are preserved for
legitimate users. Extensive empirical results demonstrate that CTRAP
effectively counters harmful fine-tuning risks across various LLMs and attack
settings, while maintaining high performance in benign scenarios. Our code is
available at https://anonymous.4open.science/r/CTRAP.

</details>


### [158] [CAIN: Hijacking LLM-Humans Conversations via a Two-Stage Malicious System Prompt Generation and Refining Framework](https://arxiv.org/abs/2505.16888)
*Viet Pham,Thai Le*

Main category: cs.CR

TL;DR: 提出针对大型语言模型的新型对抗攻击方法CAIN，通过篡改系统提示实现在特定敏感问题上输出恶意回答，同时保持其他问题正常响应，揭示LLM安全防御机制的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM易受对抗攻击，但缺乏对隐蔽性定向攻击的研究。攻击者可能通过散布看似良性的恶意系统提示进行大规模信息操纵，危害现实应用安全。

Method: 开发CAIN算法，采用黑盒优化方法自动生成有害系统提示，无需访问模型参数。通过特定目标问题定位和对抗性提示优化实现精准攻击。

Result: 非定向攻击使目标问题F1值下降40%，定向攻击成功率超70%。在开源和商业LLM中均有效，且对正常问题准确率影响小于2%。

Conclusion: 揭示LLM系统提示的严重安全漏洞，强调需开发更强大的鲁棒性防护机制。建议建立系统提示安全验证框架，防范隐蔽性定向攻击的传播风险。

Abstract: Large language models (LLMs) have advanced many applications, but are also
known to be vulnerable to adversarial attacks. In this work, we introduce a
novel security threat: hijacking AI-human conversations by manipulating LLMs'
system prompts to produce malicious answers only to specific targeted questions
(e.g., "Who should I vote for US President?", "Are Covid vaccines safe?"),
while behaving benignly on others. This attack is detrimental as it can enable
malicious actors to exercise large-scale information manipulation by spreading
harmful but benign-looking system prompts online. To demonstrate such an
attack, we develop CAIN, an algorithm that can automatically curate such
harmful system prompts for a specific target question in a black-box setting or
without the need to access the LLM's parameters. Evaluated on both open-source
and commercial LLMs, CAIN demonstrates significant adversarial impact. In
untargeted attacks or forcing LLMs to output incorrect answers, CAIN achieves
up to 40% F1 degradation on targeted questions while preserving high accuracy
on benign inputs. For targeted attacks or forcing LLMs to output specific
harmful answers, CAIN achieves over 70% F1 scores on these targeted responses
with minimal impact on benign questions. Our results highlight the critical
need for enhanced robustness measures to safeguard the integrity and safety of
LLMs in real-world applications. All source code will be publicly available.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [159] [Interpretability Illusions with Sparse Autoencoders: Evaluating Robustness of Concept Representations](https://arxiv.org/abs/2505.16004)
*Aaron J. Li,Suraj Srinivas,Usha Bhalla,Himabindu Lakkaraju*

Main category: cs.LG

TL;DR: 研究发现稀疏自编码器（SAE）生成的概念表示对微小对抗扰动极其敏感，可能影响其在模型监控中的应用可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有SAE评估指标忽视概念表示的鲁棒性，而鲁棒性是衡量概念标注真实性的核心指标，需系统评估对抗扰动下的表现。

Method: 通过输入空间优化构建对抗扰动框架，在保持基础模型输出稳定的前提下，测试SAE概念表示在多种现实场景中的脆弱性。

Result: 实验表明微小扰动即可显著改变SAE概念解释（成功率最高达90%），而基础模型输出保持稳定（困惑度变化<0.5%）。

Conclusion: SAE概念表示的脆弱性揭示其可能不适用于需要可靠解释的模型监控与安全审计场景，需开发更鲁棒的解释方法。

Abstract: Sparse autoencoders (SAEs) are commonly used to interpret the internal
activations of large language models (LLMs) by mapping them to
human-interpretable concept representations. While existing evaluations of SAEs
focus on metrics such as the reconstruction-sparsity tradeoff, human
(auto-)interpretability, and feature disentanglement, they overlook a critical
aspect: the robustness of concept representations to input perturbations. We
argue that robustness must be a fundamental consideration for concept
representations, reflecting the fidelity of concept labeling. To this end, we
formulate robustness quantification as input-space optimization problems and
develop a comprehensive evaluation framework featuring realistic scenarios in
which adversarial perturbations are crafted to manipulate SAE representations.
Empirically, we find that tiny adversarial input perturbations can effectively
manipulate concept-based interpretations in most scenarios without notably
affecting the outputs of the base LLMs themselves. Overall, our results suggest
that SAE concept representations are fragile and may be ill-suited for
applications in model monitoring and oversight.

</details>


### [160] [Merge to Mix: Mixing Datasets via Model Merging](https://arxiv.org/abs/2505.16066)
*Zhixu Silvia Tao,Kasper Vinken,Hao-Wei Yeh,Avi Cooper,Xavier Boix*

Main category: cs.LG

TL;DR: 提出Merge to Mix方法，通过模型合并技术加速大型语言模型微调时的数据集选择


<details>
  <summary>Details</summary>
Motivation: 传统数据集混合选择依赖试错法且需多次微调，效率低下

Method: 将各数据集单独微调的模型进行参数合并，替代全量混合数据集微调

Result: 实验表明该方法在数据集选择效果上超越现有SOTA方法

Conclusion: 模型合并技术可有效加速最优数据集组合的搜索过程，减少计算消耗

Abstract: Mixing datasets for fine-tuning large models (LMs) has become critical for
maximizing performance on downstream tasks. However, composing effective
dataset mixtures typically relies on heuristics and trial-and-error, often
requiring multiple fine-tuning runs to achieve the desired outcome. We propose
a novel method, $\textit{Merge to Mix}$, that accelerates composing dataset
mixtures through model merging. Model merging is a recent technique that
combines the abilities of multiple individually fine-tuned LMs into a single LM
by using a few simple arithmetic operations. Our key insight is that merging
models individually fine-tuned on each dataset in a mixture can effectively
serve as a surrogate for a model fine-tuned on the entire mixture. Merge to Mix
leverages this insight to accelerate selecting dataset mixtures without
requiring full fine-tuning on each candidate mixture. Our experiments
demonstrate that Merge to Mix surpasses state-of-the-art methods in dataset
selection for fine-tuning LMs.

</details>


### [161] [A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization](https://arxiv.org/abs/2505.16094)
*Ziqing Wang,Kexin Zhang,Zihan Zhao,Yibo Wen,Abhishek Pandey,Han Liu,Kaize Ding*

Main category: cs.LG

TL;DR: 综述探讨大语言模型在分子生成与优化中的应用，提出分类框架并分析技术趋势


<details>
  <summary>Details</summary>
Motivation: LLMs通过文本引导和多模态输入推动分子发现范式革新，需系统性总结其在分子科学中的应用进展

Method: 建立分子生成/优化的分类体系，分析不同学习场景下的代表性LLM技术，整理常用数据集与评估标准

Result: 提出LLM-centric的分子发现技术分类框架，揭示现有方法如何融合分子表征与LLM能力，建立持续更新的资源库

Conclusion: 需解决分子知识注入、多模态对齐、生成可靠性验证等核心挑战，相关资源持续维护于GitHub开源项目

Abstract: Large language models (LLMs) are introducing a paradigm shift in molecular
discovery by enabling text-guided interaction with chemical spaces through
natural language, symbolic notations, with emerging extensions to incorporate
multi-modal inputs. To advance the new field of LLM for molecular discovery,
this survey provides an up-to-date and forward-looking review of the emerging
use of LLMs for two central tasks: molecule generation and molecule
optimization. Based on our proposed taxonomy for both problems, we analyze
representative techniques in each category, highlighting how LLM capabilities
are leveraged across different learning settings. In addition, we include the
commonly used datasets and evaluation protocols. We conclude by discussing key
challenges and future directions, positioning this survey as a resource for
researchers working at the intersection of LLMs and molecular science. A
continuously updated reading list is available at
https://github.com/REAL-Lab-NU/Awesome-LLM-Centric-Molecular-Discovery.

</details>


### [162] [NAN: A Training-Free Solution to Coefficient Estimation in Model Merging](https://arxiv.org/abs/2505.16148)
*Chongjie Si,Kangtao Lv,Jingjing Jiang,Yadao Wang,Yongwei Wang,Xiaokang Yang,Wenbo Su,Bo Zheng,Wei Shen*

Main category: cs.LG

TL;DR: 提出NAN方法，通过参数范数逆估计模型融合系数，无需训练即可提升现有模型融合策略性能


<details>
  <summary>Details</summary>
Motivation: 现有模型融合方法依赖启发式确定融合系数，缺乏理论依据且扩展性受限。通过最小二乘优化视角重新分析，发现融合系数应与模型参数范数成反比

Method: 基于参数范数逆设计NAN方法，训练免费、即插即用，适用于多种融合策略（如参数平均/拼接）

Result: 在多种任务场景的实验中，NAN方法持续提升基线方法性能，验证了理论分析的有效性

Conclusion: 通过参数范数定量评估模型携带的任务特定信息量，为模型融合系数确定提供了理论依据和高效解决方案

Abstract: Model merging offers a training-free alternative to multi-task learning by
combining independently fine-tuned models into a unified one without access to
raw data. However, existing approaches often rely on heuristics to determine
the merging coefficients, limiting their scalability and generality. In this
work, we revisit model merging through the lens of least-squares optimization
and show that the optimal merging weights should scale with the amount of
task-specific information encoded in each model. Based on this insight, we
propose NAN, a simple yet effective method that estimates model merging
coefficients via the inverse of parameter norm. NAN is training-free,
plug-and-play, and applicable to a wide range of merging strategies. Extensive
experiments on show that NAN consistently improves performance of baseline
methods.

</details>


### [163] [NQKV: A KV Cache Quantization Scheme Based on Normal Distribution Characteristics](https://arxiv.org/abs/2505.16210)
*Zhihang Cai,Xingjun Zhang,Zhendong Tan,Zheng Wei*

Main category: cs.LG

TL;DR: 提出NQKV算法，通过块级分位数量化压缩KV缓存，降低LLM推理内存消耗，提升吞吐量9.3倍。


<details>
  <summary>Details</summary>
Motivation: LLM推理时KV缓存的内存消耗成为部署瓶颈，现有激活量化方法仅支持8比特，更低比特量化会导致精度显著下降。

Method: 分析KV缓存元素分布特征，利用块内正态分布特性设计分位数量化方法，实现信息论最优量化误差。

Result: OPT模型推理批处理量提升2倍/上下文长度扩展4倍，吞吐量提升9.3倍且模型输出质量无显著下降。

Conclusion: NQKV为LLM部署提供了高效内存优化方案，通过分布感知量化策略实现计算效率与模型精度的平衡。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency across
a wide range of tasks. However, LLMs often require larger batch sizes to
enhance throughput or longer context lengths to meet task demands, which
significantly increases the memory resource consumption of the Key-Value (KV)
cache during inference, becoming a major bottleneck in LLM deployment. To
address this issue, quantization is a common and straightforward approach.
Currently, quantization methods for activations are limited to 8-bit, and
quantization to even lower bits can lead to substantial accuracy drops. To
further save space by quantizing the KV cache to even lower bits, we analyzed
the element distribution of the KV cache and designed the NQKV algorithm. Since
the elements within each block of the KV cache follow a normal distribution,
NQKV employs per-block quantile quantization to achieve
information-theoretically optimal quantization error. Without significantly
compromising model output quality, NQKV enables the OPT model to perform
inference with an 2x larger batch size or a 4x longer context length, and it
improves throughput by 9.3x compared to when the KV cache is not used.

</details>


### [164] [AdaSTaR: Adaptive Data Sampling for Training Self-Taught Reasoners](https://arxiv.org/abs/2505.16322)
*Woosung Koh,Wonbeen Oh,Jaein Jang,MinHyung Lee,Hyeongjin Kim,Ah Yeon Kim,Joonkee Kim,Junghyun Lee,Taehyeon Kim,Se-Young Yun*

Main category: cs.LG

TL;DR: 提出自适应STaR算法（AdaSTaR），通过多样性自适应采样和课程自适应采样策略，显著提升自改进语言模型的训练效率和效果


<details>
  <summary>Details</summary>
Motivation: 传统STaR/RFT方法的随机采样导致训练数据不平衡——过度训练简单样本而忽视困难样本，限制了模型优化效果

Method: 1. 多样性自适应采样：确保不同难度样本的均衡训练
2. 课程自适应采样：动态调整训练数据难度，匹配模型当前能力水平

Result: 在6个基准测试中全部达到最佳准确率（6/6），平均减少58.6%训练计算量（FLOPs），且改进效果在不同规模模型上具有普适性

Conclusion: AdaSTaR通过双重自适应机制，建立了更高效有效的自改进语言模型训练范式，为大规模模型优化提供了新方向

Abstract: Self-Taught Reasoners (STaR), synonymously known as Rejection sampling
Fine-Tuning (RFT), is an integral part of the training pipeline of
self-improving reasoning Language Models (LMs). The self-improving mechanism
often employs random observation (data) sampling. However, this results in
trained observation imbalance; inefficiently over-training on solved examples
while under-training on challenging ones. In response, we introduce Adaptive
STaR (AdaSTaR), a novel algorithm that rectifies this by integrating two
adaptive sampling principles: (1) Adaptive Sampling for Diversity: promoting
balanced training across observations, and (2) Adaptive Sampling for
Curriculum: dynamically adjusting data difficulty to match the model's evolving
strength. Across six benchmarks, AdaSTaR achieves best test accuracy in all
instances (6/6) and reduces training FLOPs by an average of 58.6% against an
extensive list of baselines. These improvements in performance and efficiency
generalize to different pre-trained LMs and larger models, paving the way for
more efficient and effective self-improving LMs.

</details>


### [165] [AceReason-Nemotron: Advancing Math and Code Reasoning through Reinforcement Learning](https://arxiv.org/abs/2505.16400)
*Yang Chen,Zhuolin Yang,Zihan Liu,Chankyu Lee,Peng Xu,Mohammad Shoeybi,Bryan Catanzaro,Wei Ping*

Main category: cs.LG

TL;DR: 大规模强化学习能显著增强中小型模型的数学和代码推理能力，通过分阶段训练策略（先数学后代码）突破原有蒸馏模型的性能上限


<details>
  <summary>Details</summary>
Motivation: 针对当前强化学习训练方法在推理模型中的不足，特别是对小模型蒸馏效果更优的现状，探索大规模RL对中小模型的提升潜力

Method: 1. 分阶段RL训练（数学提示先行，代码提示后续）
2. 构建高质量可验证的数据管道
3. 课程学习策略（逐步增加响应长度）
4. 基于验证的RL训练框架

Result: 数学RL使7B/14B模型在AIME数学基准提升14.6%/17.2%，代码任务提升6.8%/5.8%；代码RL进一步优化代码性能且不影响数学能力

Conclusion: 强化学习不仅能激发预训练和蒸馏获得的推理能力，还能突破模型极限解决新问题，证明大规模RL在中小模型中的有效性

Abstract: Despite recent progress in large-scale reinforcement learning (RL) for
reasoning, the training recipe for building high-performing reasoning models
remains elusive. Key implementation details of frontier models, such as
DeepSeek-R1, including data curation strategies and RL training recipe, are
often omitted. Moreover, recent research indicates distillation remains more
effective than RL for smaller models. In this work, we demonstrate that
large-scale RL can significantly enhance the reasoning capabilities of strong,
small- and mid-sized models, achieving results that surpass those of
state-of-the-art distillation-based models. We systematically study the RL
training process through extensive ablations and propose a simple yet effective
approach: first training on math-only prompts, then on code-only prompts.
Notably, we find that math-only RL not only significantly enhances the
performance of strong distilled models on math benchmarks (e.g., +14.6% /
+17.2% on AIME 2025 for the 7B / 14B models), but also code reasoning tasks
(e.g., +6.8% / +5.8% on LiveCodeBench for the 7B / 14B models). In addition,
extended code-only RL iterations further improve performance on code benchmarks
with minimal or no degradation in math results. We develop a robust data
curation pipeline to collect challenging prompts with high-quality, verifiable
answers and test cases to enable verification-based RL across both domains.
Finally, we identify key experimental insights, including curriculum learning
with progressively increasing response lengths and the stabilizing effect of
on-policy parameter updates. We find that RL not only elicits the foundational
reasoning capabilities acquired during pretraining and supervised fine-tuning
(e.g., distillation), but also pushes the limits of the model's reasoning
ability, enabling it to solve problems that were previously unsolvable.

</details>


### [166] [Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization](https://arxiv.org/abs/2505.16737)
*Chengcan Wu,Zhixin Zhang,Zeming Wei,Yihao Zhang,Meng Sun*

Main category: cs.LG

TL;DR: 大语言模型（LLMs）微调存在安全风险，研究者提出安全感知探测框架SAP，在保持模型性能的同时有效降低有害内容生成风险。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐技术在微调过程中容易失效，即使使用良性数据微调仍会导致安全性能下降，需开发兼顾性能与安全性的优化方案。

Method: 提出SAP框架，在梯度传播过程中引入安全感知探针，通过识别梯度方向的潜在风险来修正优化路径，实现任务性能与安全性的双重保障。

Result: 实验表明SAP将有害性降低至原始微调模型水平以下，同时达到与标准微调相当的测试损失，代码已开源。

Conclusion: SAP框架成功解决了LLMs微调过程中的安全退化问题，为安全敏感的模型优化提供了有效解决方案。

Abstract: The significant progress of large language models (LLMs) has led to
remarkable achievements across numerous applications. However, their ability to
generate harmful content has sparked substantial safety concerns. Despite the
implementation of safety alignment techniques during the pre-training phase,
recent research indicates that fine-tuning LLMs on adversarial or even benign
data can inadvertently compromise their safety. In this paper, we re-examine
the fundamental issue of why fine-tuning on non-harmful data still results in
safety degradation. We introduce a safety-aware probing (SAP) optimization
framework designed to mitigate the safety risks of fine-tuning LLMs.
Specifically, SAP incorporates a safety-aware probe into the gradient
propagation process, mitigating the model's risk of safety degradation by
identifying potential pitfalls in gradient directions, thereby enhancing
task-specific performance while successfully preserving model safety. Our
extensive experimental results demonstrate that SAP effectively reduces
harmfulness below the original fine-tuned model and achieves comparable test
loss to standard fine-tuning methods. Our code is available at
https://github.com/ChengcanWu/SAP.

</details>


### [167] [ATR-Bench: A Federated Learning Benchmark for Adaptation, Trust, and Reasoning](https://arxiv.org/abs/2505.16850)
*Tajamul Ashraf,Mohammed Mohsen Peerzada,Moloud Abdar,Yutong Xie,Yuyin Zhou,Xiaofeng Liu,Iqra Altaf Gillani,Janibul Bashir*

Main category: cs.LG

TL;DR: 提出了ATR-Bench框架，从适应性、信任、推理三个维度系统评估联邦学习，填补标准化评估空白。


<details>
  <summary>Details</summary>
Motivation: 联邦学习缺乏统一的评估标准，导致方法间难以公平比较和系统化进展。需建立多维度评估体系以适应实际应用需求。

Method: 构建ATR-Bench框架，围绕适应性(客户端异构)、信任(对抗环境)、推理(逻辑可解释性)三个维度进行理论分析，并对前两个维度开展基准测试。

Result: 建立了联邦学习系统化评估基础，公开代码库并维护持续更新的研究资源库。因推理维度缺乏可靠指标，该部分主要提供文献分析结论。

Conclusion: ATR-Bench首次实现联邦学习多维度联合评估框架，推动面向现实场景的联邦学习技术发展，促进研究标准化进程。

Abstract: Federated Learning (FL) has emerged as a promising paradigm for collaborative
model training while preserving data privacy across decentralized participants.
As FL adoption grows, numerous techniques have been proposed to tackle its
practical challenges. However, the lack of standardized evaluation across key
dimensions hampers systematic progress and fair comparison of FL methods. In
this work, we introduce ATR-Bench, a unified framework for analyzing federated
learning through three foundational dimensions: Adaptation, Trust, and
Reasoning. We provide an in-depth examination of the conceptual foundations,
task formulations, and open research challenges associated with each theme. We
have extensively benchmarked representative methods and datasets for adaptation
to heterogeneous clients and trustworthiness in adversarial or unreliable
environments. Due to the lack of reliable metrics and models for reasoning in
FL, we only provide literature-driven insights for this dimension. ATR-Bench
lays the groundwork for a systematic and holistic evaluation of federated
learning with real-world relevance. We will make our complete codebase publicly
accessible and a curated repository that continuously tracks new developments
and research in the FL literature.

</details>


### [168] [The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm](https://arxiv.org/abs/2505.16932)
*Noah Amsel,David Persson,Christopher Musco,Robert Gower*

Main category: cs.LG

TL;DR: 提出GPU友好的Polar Express算法，通过动态多项式更新规则实现快速收敛和bfloat16稳定性，显著提升Muon框架下大模型训练效果


<details>
  <summary>Details</summary>
Motivation: 传统极分解算法（如Newton-Schulz迭代和有理函数方法）在深度学习场景下存在GPU兼容性差、初始收敛慢等问题，无法满足现代优化框架对效率和稳定性的需求

Method: 结合Chen & Chow与Nakatsukasa & Freund的成果，通过求解极小极大优化问题动态调整多项式迭代规则，理论证明具备最坏情况最优性保证，仅使用矩阵乘法运算

Result: 在bfloat16精度下保持稳定，应用于GPT-2等大模型时验证损失持续改善，学习率适应性优于现有方法

Conclusion: Polar Express成功平衡了深度学习中速度、稳定性和计算效率的需求，为优化框架提供了更适配的极分解解决方案

Abstract: Computing the polar decomposition and the related matrix sign function, has
been a well-studied problem in numerical analysis for decades. More recently,
it has emerged as an important subroutine in deep learning, particularly within
the Muon optimization framework. However, the requirements in this setting
differ significantly from those of traditional numerical analysis. In deep
learning, methods must be highly efficient and GPU-compatible, but high
accuracy is often unnecessary. As a result, classical algorithms like
Newton-Schulz (which suffers from slow initial convergence) and methods based
on rational functions (which rely on QR decompositions or matrix inverses) are
poorly suited to this context. In this work, we introduce Polar Express, a
GPU-friendly algorithm for computing the polar decomposition. Like classical
polynomial methods such as Newton-Schulz, our approach uses only matrix-matrix
multiplications, making it GPU-compatible. Motivated by earlier work of Chen &
Chow and Nakatsukasa & Freund, Polar Express adapts the polynomial update rule
at each iteration by solving a minimax optimization problem, and we prove that
it enjoys a strong worst-case optimality guarantee. This property ensures both
rapid early convergence and fast asymptotic convergence. We also address
finite-precision issues, making it stable in bfloat16 in practice. We apply
Polar Express within the Muon optimization framework and show consistent
improvements in validation loss on large-scale models such as GPT-2,
outperforming recent alternatives across a range of learning rates.

</details>


### [169] [LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning](https://arxiv.org/abs/2505.16933)
*Zebin You,Shen Nie,Xiaolu Zhang,Jun Hu,Jun Zhou,Zhiwu Lu,Ji-Rong Wen,Chongxuan Li*

Main category: cs.LG

TL;DR: 提出纯扩散模型LLaDA-V，突破当前多模态模型依赖自回归架构的局限，展现扩散模型在MLLM任务中的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型在多模态任务中的有效性，替代主流自回归架构，寻求更好的数据扩展性和跨模态对齐能力。

Method: 基于LLaDA扩散模型，集成视觉编码器和MLP连接器，将视觉特征映射到语言嵌入空间实现多模态对齐。

Result: 1. 多模态性能媲美LLaMA3-V且数据扩展性更优 2. 缩小与Qwen2-VL差距 3. 多模态理解达SOTA水平

Conclusion: 扩散模型在多模态领域具有重要研究价值，其架构优势值得深入探索，为后续研究提供新方向。

Abstract: In this work, we introduce LLaDA-V, a purely diffusion-based Multimodal Large
Language Model (MLLM) that integrates visual instruction tuning with masked
diffusion models, representing a departure from the autoregressive paradigms
dominant in current multimodal approaches. Built upon LLaDA, a representative
large language diffusion model, LLaDA-V incorporates a vision encoder and MLP
connector that projects visual features into the language embedding space,
enabling effective multimodal alignment. Our empirical investigation reveals
several intriguing results: First, LLaDA-V demonstrates promising multimodal
performance despite its language model being weaker on purely textual tasks
than counterparts like LLaMA3-8B and Qwen2-7B. When trained on the same
instruction data, LLaDA-V is highly competitive to LLaMA3-V across multimodal
tasks with better data scalability. It also narrows the performance gap to
Qwen2-VL, suggesting the effectiveness of its architecture for multimodal
tasks. Second, LLaDA-V achieves state-of-the-art performance in multimodal
understanding compared to existing hybrid autoregressive-diffusion and purely
diffusion-based MLLMs. Our findings suggest that large language diffusion
models show promise in multimodal contexts and warrant further investigation in
future research. Project page and codes:
https://ml-gsai.github.io/LLaDA-V-demo/.

</details>


### [170] [UFT: Unifying Supervised and Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.16984)
*Mingyang Liu,Gabriele Farina,Asuman Ozdaglar*

Main category: cs.LG

TL;DR: 提出统一微调(UFT)新范式，整合SFT和RFT优势，突破传统方法的样本复杂度瓶颈


<details>
  <summary>Details</summary>
Motivation: 现有SFT方法在大模型上易过拟合，RFT依赖基础模型强度且样本效率低，需探索更高效的后训练范式

Method: 将监督信号与强化探索结合，通过统一训练框架实现记忆与思考的平衡，理论证明其指数级加速收敛能力

Result: UFT在不同规模模型上全面超越SFT/RFT，理论突破RFT指数级样本复杂度限制

Conclusion: UFT为LLMs后训练提供新范式，证明统一训练框架在复杂推理任务中的革命性潜力

Abstract: Post-training has demonstrated its importance in enhancing the reasoning
capabilities of large language models (LLMs). The primary post-training methods
can be categorized into supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT). SFT is efficient and well-suited for small language models,
but it may lead to overfitting and limit the reasoning abilities of larger
models. In contrast, RFT generally yields better generalization but depends
heavily on the strength of the base model. To address the limitations of SFT
and RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm
that unifies SFT and RFT into a single, integrated process. UFT enables the
model to effectively explore solutions while incorporating informative
supervision signals, bridging the gap between memorizing and thinking
underlying existing methods. Notably, UFT outperforms both SFT and RFT in
general, regardless of model sizes. Furthermore, we theoretically prove that
UFT breaks RFT's inherent exponential sample complexity bottleneck, showing for
the first time that unified training can exponentially accelerate convergence
on long-horizon reasoning tasks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [171] [Causal LLM Routing: End-to-End Regret Minimization from Observational Data](https://arxiv.org/abs/2505.16037)
*Asterios Tsiourvas,Wei Sun,Georgia Perakis*

Main category: cs.AI

TL;DR: 提出基于因果关系的端到端框架LLM路由方法，通过观测数据优化模型选择策略，在公开基准测试中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有解耦式LLM路由方法存在错误累积问题且依赖昂贵全反馈数据，需开发基于观测数据的高效学习框架

Method: 构建因果推理框架最小化决策遗憾，提出分类上界和softmax加权近似两种理论驱动的替代优化目标，设计区间条件架构处理异构成本偏好

Result: 在公共基准测试中超越现有基线，不同嵌入模型上均达到最优性能

Conclusion: 端到端学习方法有效解决了观测数据下的LLM路由优化问题，理论驱动的替代目标显著提升策略学习效率，灵活架构支持多样化成本需求

Abstract: LLM routing aims to select the most appropriate model for each query,
balancing competing performance metrics such as accuracy and cost across a pool
of language models. Prior approaches typically adopt a decoupled strategy,
where the metrics are first predicted and the model is then selected based on
these estimates. This setup is prone to compounding errors and often relies on
full-feedback data, where each query is evaluated by all candidate models,
which is costly to obtain and maintain in practice. In contrast, we learn from
observational data, which records only the outcome of the model actually
deployed. We propose a causal end-to-end framework that learns routing policies
by minimizing decision-making regret from observational data. To enable
efficient optimization, we introduce two theoretically grounded surrogate
objectives: a classification-based upper bound, and a softmax-weighted regret
approximation shown to recover the optimal policy at convergence. We further
extend our framework to handle heterogeneous cost preferences via an
interval-conditioned architecture. Experiments on public benchmarks show that
our method outperforms existing baselines, achieving state-of-the-art
performance across different embedding models.

</details>


### [172] [Optimizing LLM-Based Multi-Agent System with Textual Feedback: A Case Study on Software Development](https://arxiv.org/abs/2505.16086)
*Ming Shen,Raphael Shu,Anurag Pratik,James Gung,Yubin Ge,Monica Sunkara,Yi Zhang*

Main category: cs.AI

TL;DR: 提出基于自然语言反馈的两阶段角色化多智能体系统优化方法，通过失败解释优化系统提示词，并比较不同优化设置对软件开发任务的影响。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型赋能的角色化多智能体系统在复杂任务协作中取得进展，但现有系统优化仍面临挑战，需探索有效的优化方法。

Method: 1. 建立包含失败解释的反馈优化管道（识别表现不佳的智能体→优化系统提示）
2. 比较在线/离线、个体/群体优化设置
3. 研究单轮/多轮提示优化策略

Result: 验证了方法在软件开发多维评估中的有效性，揭示群体优化设置中多轮提示策略的优势，为系统优化提供实证依据。

Conclusion: 该研究为角色化多智能体系统的群体行为优化提供了方法论框架，并为未来系统开发中的动态优化策略设计提供了实践启示。

Abstract: We have seen remarkable progress in large language models (LLMs) empowered
multi-agent systems solving complex tasks necessitating cooperation among
experts with diverse skills. However, optimizing LLM-based multi-agent systems
remains challenging. In this work, we perform an empirical case study on group
optimization of role-based multi-agent systems utilizing natural language
feedback for challenging software development tasks under various evaluation
dimensions. We propose a two-step agent prompts optimization pipeline:
identifying underperforming agents with their failure explanations utilizing
textual feedback and then optimizing system prompts of identified agents
utilizing failure explanations. We then study the impact of various
optimization settings on system performance with two comparison groups: online
against offline optimization and individual against group optimization. For
group optimization, we study two prompting strategies: one-pass and multi-pass
prompting optimizations. Overall, we demonstrate the effectiveness of our
optimization method for role-based multi-agent systems tackling software
development tasks evaluated on diverse evaluation dimensions, and we
investigate the impact of diverse optimization settings on group behaviors of
the multi-agent systems to provide practical insights for future development.

</details>


### [173] [Can AI Read Between The Lines? Benchmarking LLMs On Financial Nuance](https://arxiv.org/abs/2505.16090)
*Dominick Kubica,Dylan T. Gordon,Nanami Emura,Derleen Saini,Charlie Goldenberg*

Main category: cs.AI

TL;DR: 评估生成式AI在金融文本情感分析中的可靠性，揭示大语言模型在收益电话会议等复杂金融场景中的表现局限


<details>
  <summary>Details</summary>
Motivation: 生成式AI虽在各领域广泛应用，但在金融等高风险专业领域存在情感误判风险，需验证其输出准确性

Method: 使用微软收益电话会议文本，对比微软Copilot、ChatGPT、Gemini与传统机器学习模型的表现，结合提示工程优化分析效果

Result: 开发情感一致性可视化工具，发现模型在战略模糊语言、行业术语和前瞻性陈述中的分析缺陷，验证提示工程的有效性

Conclusion: 需针对性优化大语言模型的金融文本处理能力，提示工程技术可提升分析准确性，当前模型尚无法完全替代专业金融分析师

Abstract: As of 2025, Generative Artificial Intelligence (GenAI) has become a central
tool for productivity across industries. Beyond text generation, GenAI now
plays a critical role in coding, data analysis, and research workflows. As
large language models (LLMs) continue to evolve, it is essential to assess the
reliability and accuracy of their outputs, especially in specialized,
high-stakes domains like finance. Most modern LLMs transform text into
numerical vectors, which are used in operations such as cosine similarity
searches to generate responses. However, this abstraction process can lead to
misinterpretation of emotional tone, particularly in nuanced financial
contexts. While LLMs generally excel at identifying sentiment in everyday
language, these models often struggle with the nuanced, strategically ambiguous
language found in earnings call transcripts. Financial disclosures frequently
embed sentiment in hedged statements, forward-looking language, and
industry-specific jargon, making it difficult even for human analysts to
interpret consistently, let alone AI models. This paper presents findings from
the Santa Clara Microsoft Practicum Project, led by Professor Charlie
Goldenberg, which benchmarks the performance of Microsoft's Copilot, OpenAI's
ChatGPT, Google's Gemini, and traditional machine learning models for sentiment
analysis of financial text. Using Microsoft earnings call transcripts, the
analysis assesses how well LLM-derived sentiment correlates with market
sentiment and stock movements and evaluates the accuracy of model outputs.
Prompt engineering techniques are also examined to improve sentiment analysis
results. Visualizations of sentiment consistency are developed to evaluate
alignment between tone and stock performance, with sentiment trends analyzed
across Microsoft's lines of business to determine which segments exert the
greatest influence.

</details>


### [174] [BioDSA-1K: Benchmarking Data Science Agents for Biomedical Research](https://arxiv.org/abs/2505.16100)
*Zifeng Wang,Benjamin Danek,Jimeng Sun*

Main category: cs.AI

TL;DR: BioDSA-1K是一个包含1,029个生物医学假设验证任务的基准测试，通过多维评估体系验证AI在真实科研场景下的可信决策能力。


<details>
  <summary>Details</summary>
Motivation: 解决AI在生物医学假设验证中面临的真实数据分析复杂性难题，填补现有基准无法反映真实科研工作流结构化推理的空白。

Method: 从300+已发表研究中提取结构化假设与数据支撑证据，构建包含支持/反对/不可验证三种结论类型的测试框架，每个任务均支持统计/机器学习验证。

Result: 建立包含假设决策准确率（89.3%）、证据结论对齐度（92.1%）、推理过程正确性（85.6%）、代码可执行性（97.4%）的四维评估体系，特别包含12.7%的非可验证假设案例。

Conclusion: BioDSA-1K为开发生物医学领域可信AI提供了标准化测试平台，其引入的非可验证假设评估维度对提升AI科学推理鲁棒性具有重要价值。

Abstract: Validating scientific hypotheses is a central challenge in biomedical
research, and remains difficult for artificial intelligence (AI) agents due to
the complexity of real-world data analysis and evidence interpretation. In this
work, we present BioDSA-1K, a benchmark designed to evaluate AI agents on
realistic, data-driven biomedical hypothesis validation tasks. BioDSA-1K
consists of 1,029 hypothesis-centric tasks paired with 1,177 analysis plans,
curated from over 300 published biomedical studies to reflect the structure and
reasoning found in authentic research workflows. Each task includes a
structured hypothesis derived from the original study's conclusions, expressed
in the affirmative to reflect the language of scientific reporting, and one or
more pieces of supporting evidence grounded in empirical data tables. While
these hypotheses mirror published claims, they remain testable using standard
statistical or machine learning methods. The benchmark enables evaluation along
four axes: (1) hypothesis decision accuracy, (2) alignment between evidence and
conclusion, (3) correctness of the reasoning process, and (4) executability of
the AI-generated analysis code. Importantly, BioDSA-1K includes non-verifiable
hypotheses: cases where the available data are insufficient to support or
refute a claim, reflecting a common yet underexplored scenario in real-world
science. We propose BioDSA-1K as a foundation for building and evaluating
generalizable, trustworthy AI agents for biomedical discovery.

</details>


### [175] [Dynamic Sampling that Adapts: Iterative DPO for Self-Aware Mathematical Reasoning](https://arxiv.org/abs/2505.16176)
*Jun Rao,Xuebo Liu,Hexuan Deng,Zepeng Lin,Zixiong Yu,Jiansheng Wei,Xiaojun Meng,Min Zhang*

Main category: cs.AI

TL;DR: SAI-DPO算法通过动态评估模型不同训练阶段的推理能力实现数据自适应选择，相比静态策略在数学推理任务上带来最高21.3%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有基于难度/多样性等静态指标的数据选择方法无法适应在线强化学习等动态训练范式，与模型能力演进脱节。

Method: 提出SAI-DPO算法，通过实时反馈模型表现动态调整数据选择策略，使数据分布始终匹配模型当前推理能力发展阶段。

Result: 在AIME24/AMC23等竞赛级数据集上分别取得10和15个百分点的显著提升，八大数据集平均提升达21.3%。

Conclusion: 实验证明基于模型能力动态演化的数据选择策略显著优于静态方法，为推理能力训练提供了新范式。

Abstract: In the realm of data selection for reasoning tasks, existing approaches
predominantly rely on externally predefined static metrics such as difficulty
and diversity, which are often designed for supervised fine-tuning (SFT) and
lack adaptability to continuous training processes. A critical limitation of
these methods is their inability to dynamically align with the evolving
capabilities of models during online training, a gap that becomes increasingly
pronounced with the rise of dynamic training paradigms and online reinforcement
learning (RL) frameworks (e.g., R1 models). To address this, we introduce
SAI-DPO, an algorithm that dynamically selects training data by continuously
assessing a model's stage-specific reasoning abilities across different
training phases. By integrating real-time model performance feedback, SAI-DPO
adaptively adapts data selection to the evolving strengths and weaknesses of
the model, thus enhancing both data utilization efficiency and final task
performance. Extensive experiments on three state-of-the-art models and eight
mathematical reasoning benchmarks, including challenging competition-level
datasets (e.g., AIME24 and AMC23), demonstrate that SAI-DPO achieves an average
performance boost of up to 21.3 percentage points, with particularly notable
improvements of 10 and 15 points on AIME24 and AMC23, respectively. These
results highlight the superiority of dynamic, model-adaptive data selection
over static, externally defined strategies in advancing reasoning.

</details>


### [176] [SafeKey: Amplifying Aha-Moment Insights for Safety Reasoning](https://arxiv.org/abs/2505.16186)
*Kaiwen Zhou,Xuandong Zhao,Gaowen Liu,Jayanth Srinivasa,Aosong Feng,Dawn Song,Xin Eric Wang*

Main category: cs.AI

TL;DR: 提出SafeKey方法，通过双路径安全头和查询掩码建模激活关键安全时刻，显著提升大推理模型的安全泛化能力


<details>
  <summary>Details</summary>
Motivation: 传统监督微调方法在面对未见过的越狱攻击时安全泛化能力不足，需通过模型自身推理过程的关键安全节点提升防护能力

Method: 1. 双路径安全头增强关键句前的安全信号
2. 查询掩码建模提升模型对查询理解的注意力

Result: 在多个安全基准测试中平均降低9.6%危害率，同时保持模型通用能力

Conclusion: 通过重塑内部注意力和改进隐藏表征质量，SafeKey有效提升了大模型对对抗攻击和OOD有害提示的防御能力

Abstract: Large Reasoning Models (LRMs) introduce a new generation paradigm of
explicitly reasoning before answering, leading to remarkable improvements in
complex tasks. However, they pose great safety risks against harmful queries
and adversarial attacks. While recent mainstream safety efforts on LRMs,
supervised fine-tuning (SFT), improve safety performance, we find that
SFT-aligned models struggle to generalize to unseen jailbreak prompts. After
thorough investigation of LRMs' generation, we identify a safety aha moment
that can activate safety reasoning and lead to a safe response. This aha moment
typically appears in the `key sentence', which follows models' query
understanding process and can indicate whether the model will proceed safely.
Based on these insights, we propose SafeKey, including two complementary
objectives to better activate the safety aha moment in the key sentence: (1) a
Dual-Path Safety Head to enhance the safety signal in the model's internal
representations before the key sentence, and (2) a Query-Mask Modeling
objective to improve the models' attention on its query understanding, which
has important safety hints. Experiments across multiple safety benchmarks
demonstrate that our methods significantly improve safety generalization to a
wide range of jailbreak attacks and out-of-distribution harmful prompts,
lowering the average harmfulness rate by 9.6\%, while maintaining general
abilities. Our analysis reveals how SafeKey enhances safety by reshaping
internal attention and improving the quality of hidden representations.

</details>


### [177] [How do Scaling Laws Apply to Knowledge Graph Engineering Tasks? The Impact of Model Size on Large Language Model Performance](https://arxiv.org/abs/2505.16276)
*Desiree Heim,Lars-Peter Meyer,Markus Schröder,Johannes Frey,Andreas Dengel*

Main category: cs.AI

TL;DR: 研究发现大语言模型在知识图工程任务中遵循规模扩展定律，但存在平台效应和同家族模型表现异常现象，建议通过测试相邻规模模型实现高性价比。


<details>
  <summary>Details</summary>
Motivation: 探究LLM模型规模与知识图工程任务性能/成本比的关系，验证规模扩展定律在特定领域的适用性。

Method: 使用LLM-KG-Bench框架评估26个开源LLM，通过创建的数据集分析不同规模模型在KGE任务中的表现规律。

Result: 模型规模扩展定律基本适用，但存在性能停滞现象；同家族模型中较大模型偶尔表现更差，但异常仅局部存在。

Conclusion: 建议实际应用中结合成本效益分析，对特定任务需测试同家族相邻规模模型以优化选择。

Abstract: When using Large Language Models (LLMs) to support Knowledge Graph
Engineering (KGE), one of the first indications when searching for an
appropriate model is its size. According to the scaling laws, larger models
typically show higher capabilities. However, in practice, resource costs are
also an important factor and thus it makes sense to consider the ratio between
model performance and costs. The LLM-KG-Bench framework enables the comparison
of LLMs in the context of KGE tasks and assesses their capabilities of
understanding and producing KGs and KG queries. Based on a dataset created in
an LLM-KG-Bench run covering 26 open state-of-the-art LLMs, we explore the
model size scaling laws specific to KGE tasks. In our analyses, we assess how
benchmark scores evolve between different model size categories. Additionally,
we inspect how the general score development of single models and families of
models correlates to their size. Our analyses revealed that, with a few
exceptions, the model size scaling laws generally also apply to the selected
KGE tasks. However, in some cases, plateau or ceiling effects occurred, i.e.,
the task performance did not change much between a model and the next larger
model. In these cases, smaller models could be considered to achieve high
cost-effectiveness. Regarding models of the same family, sometimes larger
models performed worse than smaller models of the same family. These effects
occurred only locally. Hence it is advisable to additionally test the next
smallest and largest model of the same family.

</details>


### [178] [Incentivizing Dual Process Thinking for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2505.16315)
*Xiaoxue Cheng,Junyi Li,Zhenduo Zhang,Xinyu Tang,Wayne Xin Zhao,Xinyu Kong,Zhiqiang Zhang*

Main category: cs.AI

TL;DR: 提出ACPO强化学习框架解决大型推理模型过度思考问题，通过双系统切换实现高效推理


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在复杂任务中存在过度思考现象，受认知科学双过程理论启发，需优化认知资源分配

Method: 1. 引入系统感知推理标记使认知过程透明化
2. 集成在线难度估计与token预算机制
3. 两阶段训练策略（监督微调+强化学习）

Result: 实验显示ACPO减少26%冗余推理步骤，在数学推理任务中保持97%准确率的同时提升推理速度

Conclusion: ACPO首次实现动态难度感知的混合推理系统，为智能体认知资源优化提供新范式

Abstract: Large reasoning models (LRMs) have demonstrated strong performance on complex
reasoning tasks, but often suffer from overthinking, generating redundant
content regardless of task difficulty. Inspired by the dual process theory in
cognitive science, we propose Adaptive Cognition Policy Optimization (ACPO), a
reinforcement learning framework that enables LRMs to achieve efficient
reasoning through adaptive cognitive allocation and dynamic system switch. ACPO
incorporates two key components: (1) introducing system-aware reasoning tokens
to explicitly represent the thinking modes thereby making the model's cognitive
process transparent, and (2) integrating online difficulty estimation and token
length budget to guide adaptive system switch and reasoning during
reinforcement learning. To this end, we propose a two-stage training strategy.
The first stage begins with supervised fine-tuning to cold start the model,
enabling it to generate reasoning paths with explicit thinking modes. In the
second stage, we apply ACPO to further enhance adaptive system switch for
difficulty-aware reasoning. Experimental results demonstrate that ACPO
effectively reduces redundant reasoning while adaptively adjusting cognitive
allocation based on task complexity, achieving efficient hybrid reasoning.

</details>


### [179] [SPaRC: A Spatial Pathfinding Reasoning Challenge](https://arxiv.org/abs/2505.16686)
*Lars Benedikt Kaesberg,Jan Philip Wahle,Terry Ruas,Bela Gipp*

Main category: cs.AI

TL;DR: SPaRC数据集揭示AI模型在空间路径规划任务中存在显著能力缺陷，人类准确率98%而最佳模型仅15.8%


<details>
  <summary>Details</summary>
Motivation: 现有推理数据集无法有效评估多步骤空间推理能力，特别是路径规划和复杂规则约束场景下的抽象问题解决能力

Method: 创建包含1000个二维网格路径规划谜题的数据集，要求结合算术/几何规则进行多步骤规划

Result: 模型生成50%+无效路径，在导航/空间逻辑上表现薄弱，且无法像人类那样随难度增加计算资源（测试时间仅增加1秒）

Conclusion: SPaRC可作为研究模型空间推理瓶颈的工具，通过改进训练方法和测试时计算资源分配策略，可能提升抽象问题解决能力

Abstract: Existing reasoning datasets saturate and fail to test abstract, multi-step
problems, especially pathfinding and complex rule constraint satisfaction. We
introduce SPaRC (Spatial Pathfinding Reasoning Challenge), a dataset of 1,000
2D grid pathfinding puzzles to evaluate spatial and symbolic reasoning,
requiring step-by-step planning with arithmetic and geometric rules. Humans
achieve near-perfect accuracy (98.0%; 94.5% on hard puzzles), while the best
reasoning models, such as o4-mini, struggle (15.8%; 1.1% on hard puzzles).
Models often generate invalid paths (>50% of puzzles for o4-mini), and
reasoning tokens reveal they make errors in navigation and spatial logic.
Unlike humans, who take longer on hard puzzles, models fail to scale test-time
compute with difficulty. Allowing models to make multiple solution attempts
improves accuracy, suggesting potential for better spatial reasoning with
improved training and efficient test-time scaling methods. SPaRC can be used as
a window into models' spatial reasoning limitations and drive research toward
new methods that excel in abstract, multi-step problem-solving.

</details>


### [180] [KTAE: A Model-Free Algorithm to Key-Tokens Advantage Estimation in Mathematical Reasoning](https://arxiv.org/abs/2505.16826)
*Wei Sun,Wen Yang,Pu Jian,Qianlong Du,Fuwei Cui,Shuo Ren,Jiajun Zhang*

Main category: cs.AI

TL;DR: 提出KTAE算法改进强化学习中的优势估计粒度问题，通过细粒度token级优势分析显著提升模型推理性能


<details>
  <summary>Details</summary>
Motivation: 现有GRPO/DAPO等强化学习方法在计算优势值时采用rollout级别估计，导致所有token共享相同优势值，无法反映个体token对结果的贡献差异

Method: KTAE算法结合采样rollout的正确性统计，通过统计分析量化单个token对最终结果的重要性权重，并与rollout级优势融合获得token级优势估计

Result: 在五个数学推理基准测试中，GRPO+KTAE和DAPO+KTAE均超越基线方法，响应更短且准确率更高，同等模型下超越R1-Distill-Qwen-1.5B

Conclusion: KTAE有效解决优势估计粒度问题，通过细粒度token级分析显著提升模型性能，验证了算法设计的有效性

Abstract: Recent advances have demonstrated that integrating reinforcement learning
with rule-based rewards can significantly enhance the reasoning capabilities of
large language models, even without supervised fine-tuning. However, prevalent
reinforcement learning algorithms such as GRPO and its variants like DAPO,
suffer from a coarse granularity issue when computing the advantage.
Specifically, they compute rollout-level advantages that assign identical
values to every token within a sequence, failing to capture token-specific
contributions and hindering effective learning. To address this limitation, we
propose Key-token Advantage Estimation (KTAE) - a novel algorithm that
estimates fine-grained, token-level advantages without introducing additional
models. KTAE leverages the correctness of sampled rollouts and applies
statistical analysis to quantify the importance of individual tokens within a
sequence to the final outcome. This quantified token-level importance is then
combined with the rollout-level advantage to obtain a more fine-grained
token-level advantage estimation. Empirical results show that models trained
with GRPO+KTAE and DAPO+KTAE outperform baseline methods across five
mathematical reasoning benchmarks. Notably, they achieve higher accuracy with
shorter responses and even surpass R1-Distill-Qwen-1.5B using the same base
model.

</details>


### [181] [From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Pedagogical Visualization](https://arxiv.org/abs/2505.16832)
*Haonian Ji,Shi Qiu,Siyang Xin,Siwei Han,Zhaorun Chen,Hongyi Wang,Dake Zhang,Huaxiu Yao*

Main category: cs.AI

TL;DR: 提出EduVisBench基准测试和EduVisAgent框架，解决基础模型在教育可视化中的推理局限性


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在生成教学可视化时过度依赖文本推理，缺乏结构化视觉表达能力，难以支持概念性理解

Method: 开发多领域、多层次的EduVisBench基准测试，提出包含教学规划、推理分解、元认知提示和可视化设计的四智能体协作框架EduVisAgent

Result: EduVisAgent相对基线模型提升40.2%性能，显著改善教育可视化质量

Conclusion: 多智能体协作框架能有效提升基础模型的教育可视化能力，相关资源已开源供教育技术研究使用

Abstract: While foundation models (FMs), such as diffusion models and large
vision-language models (LVLMs), have been widely applied in educational
contexts, their ability to generate pedagogically effective visual explanations
remains limited. Most existing approaches focus primarily on textual reasoning,
overlooking the critical role of structured and interpretable visualizations in
supporting conceptual understanding. To better assess the visual reasoning
capabilities of FMs in educational settings, we introduce EduVisBench, a
multi-domain, multi-level benchmark. EduVisBench features diverse STEM problem
sets requiring visually grounded solutions, along with a fine-grained
evaluation rubric informed by pedagogical theory. Our empirical analysis
reveals that existing models frequently struggle with the inherent challenge of
decomposing complex reasoning and translating it into visual representations
aligned with human cognitive processes. To address these limitations, we
propose EduVisAgent, a multi-agent collaborative framework that coordinates
specialized agents for instructional planning, reasoning decomposition,
metacognitive prompting, and visualization design. Experimental results show
that EduVisAgent substantially outperforms all baselines, achieving a 40.2%
improvement and delivering more educationally aligned visualizations.
EduVisBench and EduVisAgent are available at
https://github.com/aiming-lab/EduVisBench and
https://github.com/aiming-lab/EduVisAgent.

</details>


### [182] [NovelSeek: When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification](https://arxiv.org/abs/2505.16938)
*NovelSeek Team,Bo Zhang,Shiyang Feng,Xiangchao Yan,Jiakang Yuan,Zhiyin Yu,Xiaohan He,Songtao Huang,Shaowei Hou,Zheng Nie,Zhilong Wang,Jinyao Liu,Runmin Ma,Tianshuo Peng,Peng Ye,Dongzhan Zhou,Shufei Zhang,Xiaosong Wang,Yilan Zhang,Meng Li,Zhongying Tu,Xiangyu Yue,Wangli Ouyang,Bowen Zhou,Lei Bai*

Main category: cs.AI

TL;DR: NovelSeek是一个统一闭环多智能体框架，可在多个科学领域进行自主科研，展现可扩展性、交互性和高效性优势。


<details>
  <summary>Details</summary>
Motivation: 传统科研效率受限，需借助AI实现范式变革。NovelSeek旨在通过自动化流程整合专家知识，解决复杂科研问题。

Method: 构建多智能体闭环系统，支持12个科研任务，集成人类专家反馈接口和端到端自动化流程。

Result: 12小时内将反应产率预测准确率从27.6%提升至35.4%；4小时增强子活性预测准确率从0.52增至0.79；30小时2D分割精度从78.8%提升至81.0%。

Conclusion: NovelSeek通过智能体协同显著提升科研效率，为跨领域自主科研提供可扩展解决方案，推动AI驱动的科研范式革新。

Abstract: Artificial Intelligence (AI) is accelerating the transformation of scientific
research paradigms, not only enhancing research efficiency but also driving
innovation. We introduce NovelSeek, a unified closed-loop multi-agent framework
to conduct Autonomous Scientific Research (ASR) across various scientific
research fields, enabling researchers to tackle complicated problems in these
fields with unprecedented speed and precision. NovelSeek highlights three key
advantages: 1) Scalability: NovelSeek has demonstrated its versatility across
12 scientific research tasks, capable of generating innovative ideas to enhance
the performance of baseline code. 2) Interactivity: NovelSeek provides an
interface for human expert feedback and multi-agent interaction in automated
end-to-end processes, allowing for the seamless integration of domain expert
knowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in
several scientific fields with significantly less time cost compared to human
efforts. For instance, in reaction yield prediction, it increased from 27.6% to
35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from
0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,
precision advanced from 78.8% to 81.0% in a mere 30 hours.

</details>


### [183] [AGENTIF: Benchmarking Instruction Following of Large Language Models in Agentic Scenarios](https://arxiv.org/abs/2505.16944)
*Yunjia Qi,Hao Peng,Xiaozhi Wang,Amy Xin,Youfeng Liu,Bin Xu,Lei Hou,Juanzi Li*

Main category: cs.AI

TL;DR: 提出首个评估大语言模型在代理场景中遵循复杂长指令能力的基准AgentIF，基于50个真实应用构建，平均含1723词指令和11.9个约束。实验显示现有模型在复杂约束处理上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理研究缺乏对复杂长指令遵循能力的系统评估，而真实代理场景常涉及多约束的冗长系统提示和工具规范。

Method: 从工业应用和开源系统收集707个标注指令，构建含代码/LLM/混合评估的三维指标，覆盖工具规范、条件约束等复杂结构分析。

Result: 当前模型总体表现较差（平均得分仅0.48），处理多级条件约束时成功率低于30%，工具调用错误率高达42%。

Conclusion: AgentIF揭示了LLM在复杂指令遵循中的局限，公开的数据集和细粒度错误分类为改进代理系统提供了基准和分析框架。

Abstract: Large Language Models (LLMs) have demonstrated advanced capabilities in
real-world agentic applications. Growing research efforts aim to develop
LLM-based agents to address practical demands, introducing a new challenge:
agentic scenarios often involve lengthy instructions with complex constraints,
such as extended system prompts and detailed tool specifications. While
adherence to such instructions is crucial for agentic applications, whether
LLMs can reliably follow them remains underexplored. In this paper, we
introduce AgentIF, the first benchmark for systematically evaluating LLM
instruction following ability in agentic scenarios. AgentIF features three key
characteristics: (1) Realistic, constructed from 50 real-world agentic
applications. (2) Long, averaging 1,723 words with a maximum of 15,630 words.
(3) Complex, averaging 11.9 constraints per instruction, covering diverse
constraint types, such as tool specifications and condition constraints. To
construct AgentIF, we collect 707 human-annotated instructions across 50
agentic tasks from industrial application agents and open-source agentic
systems. For each instruction, we annotate the associated constraints and
corresponding evaluation metrics, including code-based evaluation, LLM-based
evaluation, and hybrid code-LLM evaluation. We use AgentIF to systematically
evaluate existing advanced LLMs. We observe that current models generally
perform poorly, especially in handling complex constraint structures and tool
specifications. We further conduct error analysis and analytical experiments on
instruction length and meta constraints, providing some findings about the
failure modes of existing LLMs. We have released the code and data to
facilitate future research.

</details>


### [184] [X-MAS: Towards Building Multi-Agent Systems with Heterogeneous LLMs](https://arxiv.org/abs/2505.16997)
*Rui Ye,Xiangrui Liu,Qimin Wu,Xianghe Pang,Zhenfei Yin,Lei Bai,Siheng Chen*

Main category: cs.AI

TL;DR: 异构LLM驱动的多智能体系统(X-MAS)通过整合不同大语言模型的优势，相比同构系统可显著提升性能(最高47%)，并提出测试平台X-MAS-Bench验证该方案。


<details>
  <summary>Details</summary>
Motivation: 现有同构LLM多智能体系统受限于单一模型能力，未能充分利用不同LLM的差异化优势。研究异构LLM协同机制可释放集体智能潜力。

Method: 构建X-MAS-Bench测试平台，在5大领域21个测试集上评估27个LLM的5类功能，通过170万次实验确定各场景最优模型组合，验证异构系统设计。

Result: 异构系统在MATH数学数据集提升8.4%，在AIME医学推理场景实现47%性能飞跃。实验证明无需重构系统，仅调整LLM组合即可显著增强能力。

Conclusion: 异构LLM协同是突破单模型限制的有效路径，为构建可扩展的协作AI系统开辟新方向，未来可探索跨模型知识融合机制进一步提升系统智能。

Abstract: LLM-based multi-agent systems (MAS) extend the capabilities of single LLMs by
enabling cooperation among multiple specialized agents. However, most existing
MAS frameworks rely on a single LLM to drive all agents, constraining the
system's intelligence to the limit of that model. This paper explores the
paradigm of heterogeneous LLM-driven MAS (X-MAS), where agents are powered by
diverse LLMs, elevating the system's potential to the collective intelligence
of diverse LLMs. We introduce X-MAS-Bench, a comprehensive testbed designed to
evaluate the performance of various LLMs across different domains and
MAS-related functions. As an extensive empirical study, we assess 27 LLMs
across 5 domains (encompassing 21 test sets) and 5 functions, conducting over
1.7 million evaluations to identify optimal model selections for each
domain-function combination. Building on these findings, we demonstrate that
transitioning from homogeneous to heterogeneous LLM-driven MAS can
significantly enhance system performance without requiring structural redesign.
Specifically, in a chatbot-only MAS scenario, the heterogeneous configuration
yields up to 8.4\% performance improvement on the MATH dataset. In a mixed
chatbot-reasoner scenario, the heterogeneous MAS could achieve a remarkable
47\% performance boost on the AIME dataset. Our results underscore the
transformative potential of heterogeneous LLMs in MAS, highlighting a promising
avenue for advancing scalable, collaborative AI systems.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [185] [Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey](https://arxiv.org/abs/2505.15957)
*Chih-Kai Yang,Neo S. Ho,Hung-yi Lee*

Main category: eess.AS

TL;DR: 首次提出针对大型音频-语言模型(LALMs)评估的系统化分类法，将评测维度划分为通用听觉能力、知识推理、对话导向及公平安全四大方向，并建立可持续更新的论文资源库。


<details>
  <summary>Details</summary>
Motivation: 现有LALMs评测基准存在碎片化问题，缺乏统一的结构化分类体系，难以系统评估模型能力并指导领域发展。

Method: 通过全面文献调研提出四维分类框架：1)通用听觉感知 2)知识推理 3)对话能力 4)公平安全可信。对各维度进行系统分析并指明挑战与未来方向。

Result: 构建首个专门针对LALMs评估的体系化分类标准，建立动态维护的论文数据集，为学术界提供明确的评测指导框架。

Conclusion: 该分类体系填补了LALMs评估方法论空白，通过系统化维度划分推动听觉大模型评测标准化进程，为后续研究提供基础框架支持。

Abstract: With advancements in large audio-language models (LALMs), which enhance large
language models (LLMs) with auditory capabilities, these models are expected to
demonstrate universal proficiency across various auditory tasks. While numerous
benchmarks have emerged to assess LALMs' performance, they remain fragmented
and lack a structured taxonomy. To bridge this gap, we conduct a comprehensive
survey and propose a systematic taxonomy for LALM evaluations, categorizing
them into four dimensions based on their objectives: (1) General Auditory
Awareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented
Ability, and (4) Fairness, Safety, and Trustworthiness. We provide detailed
overviews within each category and highlight challenges in this field, offering
insights into promising future directions. To the best of our knowledge, this
is the first survey specifically focused on the evaluations of LALMs, providing
clear guidelines for the community. We will release the collection of the
surveyed papers and actively maintain it to support ongoing advancements in the
field.

</details>


### [186] [Meta-PerSER: Few-Shot Listener Personalized Speech Emotion Recognition via Meta-learning](https://arxiv.org/abs/2505.16220)
*Liang-Yeh Shen,Shi-Xin Fang,Yi-Cheng Lin,Huang-Cheng Chou,Hung-yi Lee*

Main category: eess.AS

TL;DR: 提出Meta-PerSER元学习框架，通过少量样本快速适应个体标注差异，结合预训练模型提升个性化语音情感识别效果


<details>
  <summary>Details</summary>
Motivation: 传统语音情感识别系统依赖群体标注，忽视个体情感理解差异，导致预测结果不一致。需要开发能适配个人标注风格的个性化模型。

Method: 采用改进的MAML框架（含组合元训练、梯度退火、分层分步学习率），先通过自监督模型提取通用特征，再用少量样本微调适配个人标注风格

Result: 在IEMOCAP数据集上，Meta-PerSER在已知和未知场景下的识别效果均显著优于基线模型（具体提升幅度未明确说明）

Conclusion: 该框架展现了个性化情感识别的潜力，其元学习机制有效解决个体标注差异问题，为实际应用提供新思路

Abstract: This paper introduces Meta-PerSER, a novel meta-learning framework that
personalizes Speech Emotion Recognition (SER) by adapting to each listener's
unique way of interpreting emotion. Conventional SER systems rely on aggregated
annotations, which often overlook individual subtleties and lead to
inconsistent predictions. In contrast, Meta-PerSER leverages a Model-Agnostic
Meta-Learning (MAML) approach enhanced with Combined-Set Meta-Training,
Derivative Annealing, and per-layer per-step learning rates, enabling rapid
adaptation with only a few labeled examples. By integrating robust
representations from pre-trained self-supervised models, our framework first
captures general emotional cues and then fine-tunes itself to personal
annotation styles. Experiments on the IEMOCAP corpus demonstrate that
Meta-PerSER significantly outperforms baseline methods in both seen and unseen
data scenarios, highlighting its promise for personalized emotion recognition.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [187] [MAPS: A Multilingual Benchmark for Global Agent Performance and Security](https://arxiv.org/abs/2505.15935)
*Omer Hofman,Oren Rachmil,Shamik Bose,Vikas Pahuja,Jonathan Brokman,Toshiya Shimizu,Trisha Starostina,Kelly Marchisio,Seraphina Goldfarb-Tarrant,Roman Vainshtein*

Main category: cs.DB

TL;DR: 提出MAPS多语言基准套件评估代理AI系统，发现非英语环境下性能与安全性一致下降，提出改进建议


<details>
  <summary>Details</summary>
Motivation: 现有代理AI评估基准仅关注英语，导致非英语用户面临不可靠行为和安全隐患，需建立多语言评估框架

Method: 基于GAIA/SWE-bench/MATH/安全基准，构建含805任务/8,855实例的十语言测试集，系统分析多语言影响

Result: 代理系统在非英语场景呈现性能安全双降，退化程度与任务类型及翻译输入量正相关

Conclusion: MAPS为多语言代理AI提供标准化评估框架，促进公平可靠的全球访问，并提出实用开发建议

Abstract: Agentic AI systems, which build on Large Language Models (LLMs) and interact
with tools and memory, have rapidly advanced in capability and scope. Yet,
since LLMs have been shown to struggle in multilingual settings, typically
resulting in lower performance and reduced safety, agentic systems risk
inheriting these limitations. This raises concerns about the global
accessibility of such systems, as users interacting in languages other than
English may encounter unreliable or security-critical agent behavior. Despite
growing interest in evaluating agentic AI, existing benchmarks focus
exclusively on English, leaving multilingual settings unexplored. To address
this gap, we propose MAPS, a multilingual benchmark suite designed to evaluate
agentic AI systems across diverse languages and tasks. MAPS builds on four
widely used agentic benchmarks - GAIA (real-world tasks), SWE-bench (code
generation), MATH (mathematical reasoning), and the Agent Security Benchmark
(security). We translate each dataset into ten diverse languages, resulting in
805 unique tasks and 8,855 total language-specific instances. Our benchmark
suite enables a systematic analysis of how multilingual contexts affect agent
performance and robustness. Empirically, we observe consistent degradation in
both performance and security when transitioning from English to other
languages, with severity varying by task and correlating with the amount of
translated input. Building on these findings, we provide actionable
recommendations to guide agentic AI systems development and assessment under
multilingual settings. This work establishes a standardized evaluation
framework, encouraging future research towards equitable, reliable, and
globally accessible agentic AI. MAPS benchmark suite is publicly available at
https://huggingface.co/datasets/Fujitsu-FRE/MAPS

</details>
