<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 34]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]
- [cs.LG](#cs.LG) [Total: 4]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.CV](#cs.CV) [Total: 1]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.DB](#cs.DB) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [CoBA: Counterbias Text Augmentation for Mitigating Various Spurious Correlations via Semantic Triples](https://arxiv.org/abs/2508.21083)
*Kyohoon Jin,Juhwan Choi,Jungmin Yun,Junho Lee,Soojin Jang,Youngbin Kim*

Main category: cs.CL

TL;DR: 提出CoBA框架：通过分解文本为语义三元组并选择性修改，生成反偏见数据以降低虚假相关性，提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型易受训练数据中虚假相关性的影响，导致泛化性能下降。需开发能同时解决多维度偏见(如性别偏见/简单性偏见)并增强分布外鲁棒性的方法。

Method: 1. 将文本分解为主谓宾三元组
2. 选择性修改关键三元组破坏虚假关联
3. 重构调整后的三元组生成反偏见数据

Result: 实验表明CoBA提升下游任务性能，有效减少偏见并增强模型在分布外数据上的适应能力。

Conclusion: CoBA为多维度虚假相关性挑战提供了统一解决方案，通过语义级数据增强显著提升模型可靠性和泛化能力。

Abstract: Deep learning models often learn and exploit spurious correlations in
training data, using these non-target features to inform their predictions.
Such reliance leads to performance degradation and poor generalization on
unseen data. To address these limitations, we introduce a more general form of
counterfactual data augmentation, termed counterbias data augmentation, which
simultaneously tackles multiple biases (e.g., gender bias, simplicity bias) and
enhances out-of-distribution robustness. We present CoBA: CounterBias
Augmentation, a unified framework that operates at the semantic triple level:
first decomposing text into subject-predicate-object triples, then selectively
modifying these triples to disrupt spurious correlations. By reconstructing the
text from these adjusted triples, CoBA generates counterbias data that
mitigates spurious patterns. Through extensive experiments, we demonstrate that
CoBA not only improves downstream task performance, but also effectively
reduces biases and strengthens out-of-distribution resilience, offering a
versatile and robust solution to the challenges posed by spurious correlations.

</details>


### [2] [Mapping Toxic Comments Across Demographics: A Dataset from German Public Broadcasting](https://arxiv.org/abs/2508.21084)
*Jan Fillies,Michael Peter Hoffmann,Rebecca Reichel,Roman Salzwedel,Sven Bodemer,Adrian Paschke*

Main category: cs.CL

TL;DR: 首個整合年齡特徵的德語有毒言論數據集，揭示不同年齡段在表達方式、虛假信息傳播等維度的顯著差異


<details>
  <summary>Details</summary>
Motivation: 現有有毒言論數據集缺乏人口統計特徵（尤其是年齡），限制了對不同年齡群體在線交流模式的系統性研究

Method: 與德國公共媒體funk合作，整合Instagram/TikTok/YouTube的3萬餘條評論，採用預定義毒性關鍵詞篩選+人工與大模型協同標註的混合註釋框架

Result: 年輕用戶傾向使用情緒化表達（如表情符號），年長用戶更多涉及虛假信息和廣播費批評，問題內容佔比16.7%

Conclusion: 該數據集為跨人口統計特徵的語言變異研究提供基礎，支持構建更公平、具年齡感知能力的內容審核系統

Abstract: A lack of demographic context in existing toxic speech datasets limits our
understanding of how different age groups communicate online. In collaboration
with funk, a German public service content network, this research introduces
the first large-scale German dataset annotated for toxicity and enriched with
platform-provided age estimates. The dataset includes 3,024 human-annotated and
30,024 LLM-annotated anonymized comments from Instagram, TikTok, and YouTube.
To ensure relevance, comments were consolidated using predefined toxic
keywords, resulting in 16.7\% labeled as problematic. The annotation pipeline
combined human expertise with state-of-the-art language models, identifying key
categories such as insults, disinformation, and criticism of broadcasting fees.
The dataset reveals age-based differences in toxic speech patterns, with
younger users favoring expressive language and older users more often engaging
in disinformation and devaluation. This resource provides new opportunities for
studying linguistic variation across demographics and supports the development
of more equitable and age-aware content moderation systems.

</details>


### [3] [Granite Embedding R2 Models](https://arxiv.org/abs/2508.21085)
*Parul Awasthy,Aashka Trivedi,Yulong Li,Meet Doshi,Riyaz Bhat,Vignesh P,Vishwajeet Kumar,Yushu Yang,Bhavani Iyer,Abraham Daniels,Rudra Murthy,Ken Barker,Martin Franz,Madison Lee,Todd Ward,Salim Roukos,David Cox,Luis Lastras,Jaydeep Sen,Radu Florian*

Main category: cs.CL

TL;DR: Granite Embedding R2模型通过16倍上下文扩展和架构优化，在企业级检索场景中实现速度与精度的双重突破


<details>
  <summary>Details</summary>
Motivation: 解决企业级应用中传统嵌入模型在长文本处理、多模态检索效率及数据透明度方面的局限性

Method: 采用双编码器/交叉编码器混合架构，包含22层检索主模型和12层轻量变体，基于严格治理的企业级数据训练

Result: 在标准测试和真实企业场景中，速度领先竞品19-44%的同时保持精度优势，建立开源嵌入模型新标杆

Conclusion: 该模型组合通过Apache 2.0开源协议提供，平衡了前沿性能与企业部署需求，是任务关键型检索系统的理想选择

Abstract: We introduce the Granite Embedding R2 models, a comprehensive family of
high-performance English encoder-based embedding models engineered for
enterprise-scale dense retrieval applications. Building upon our
first-generation release, these models deliver substantial improvements,
including 16x expanded context length (8,192 tokens), state-of-the-art
performance across diverse retrieval domains - text, code, long-document
search, multi-turn conversational, and tabular data - and measurable speed
advantages of 19-44\% over leading competitors while maintaining superior
accuracy. Our release encompasses both bi-encoder and cross-encoder
architectures, featuring a highly effective 22-layer retriever model and its
efficient 12-layer counterpart, alongside a high-quality reranker model, all
trained exclusively on enterprise-appropriate data with comprehensive
governance oversight. The models demonstrate exceptional versatility across
standard benchmarks, IBM-developed evaluation suites, and real-world enterprise
use cases, establishing new performance standards for open-source embedding
models. In an era where retrieval speed and accuracy are paramount for
competitive advantage, the Granite R2 models deliver a compelling combination
of cutting-edge performance, enterprise-ready licensing, and transparent data
provenance that organizations require for mission-critical deployments. All
models are publicly available under the Apache 2.0 license at
https://huggingface.co/collections/ibm-granite, enabling unrestricted research
and commercial use.

</details>


### [4] [TrInk: Ink Generation with Transformer Network](https://arxiv.org/abs/2508.21098)
*Zezhong Jin,Shubhang Desai,Xu Chen,Biyi Fang,Zhuoyi Huang,Zhe Li,Chong-Xin Gan,Xiao Tu,Man-Wai Mak,Yan Lu,Shujie Liu*

Main category: cs.CL

TL;DR: 提出基于Transformer的TrInk手写生成模型，通过改进交叉注意力机制和评估流程显著提升生成准确率


<details>
  <summary>Details</summary>
Motivation: 解决现有手写生成模型中文本与笔画对齐不精确的问题，利用Transformer的全局依赖捕捉能力提升生成质量

Method: 1. 在交叉注意力模块引入缩放位置编码和高斯记忆掩码
2. 设计主客观结合的评估体系（可读性与风格一致性）

Result: IAM-OnDB数据集上CER降低35.56%，WER降低29.66%，并提供在线demo对比样本

Conclusion: Transformer架构配合新颖的注意力机制组件能有效提升手写生成效果，定量实验与定性评估验证了方案优势

Abstract: In this paper, we propose TrInk, a Transformer-based model for ink
generation, which effectively captures global dependencies. To better
facilitate the alignment between the input text and generated stroke points, we
introduce scaled positional embeddings and a Gaussian memory mask in the
cross-attention module. Additionally, we design both subjective and objective
evaluation pipelines to comprehensively assess the legibility and style
consistency of the generated handwriting. Experiments demonstrate that our
Transformer-based model achieves a 35.56\% reduction in character error rate
(CER) and an 29.66% reduction in word error rate (WER) on the IAM-OnDB dataset
compared to previous methods. We provide an demo page with handwriting samples
from TrInk and baseline models at: https://akahello-a11y.github.io/trink-demo/

</details>


### [5] [How Does Cognitive Bias Affect Large Language Models? A Case Study on the Anchoring Effect in Price Negotiation Simulations](https://arxiv.org/abs/2508.21137)
*Yoshiki Takenami,Yin Jou Huang,Yugo Murawaki,Chenhui Chu*

Main category: cs.CL

TL;DR: 研究发现LLM在价格谈判中会受锚定效应影响，长思维链可缓解该效应但人格特质无显著关联。


<details>
  <summary>Details</summary>
Motivation: 探究LLM存在的认知偏差（锚定效应）及其影响因素，确保LLM在现实应用中的可靠性。

Method: 1. 构建卖方LLM代理应用锚定效应
2. 使用主客观双指标评估谈判效果
3. 分析推理能力与人格特质的影响

Result: 1. LLM表现出与人类相似的锚定效应
2. 具备长思维链的推理模型受影响程度降低35%
3. 五大人格特质与锚定效应敏感性无统计学显著相关性（p>0.05）

Conclusion: 该研究揭示了LLM认知偏差机制，为构建安全可靠的LLM应用提供理论支持，建议通过增强推理能力降低偏差影响。

Abstract: Cognitive biases, well-studied in humans, can also be observed in LLMs,
affecting their reliability in real-world applications. This paper investigates
the anchoring effect in LLM-driven price negotiations. To this end, we
instructed seller LLM agents to apply the anchoring effect and evaluated
negotiations using not only an objective metric but also a subjective metric.
Experimental results show that LLMs are influenced by the anchoring effect like
humans. Additionally, we investigated the relationship between the anchoring
effect and factors such as reasoning and personality. It was shown that
reasoning models are less prone to the anchoring effect, suggesting that the
long chain of thought mitigates the effect. However, we found no significant
correlation between personality traits and susceptibility to the anchoring
effect. These findings contribute to a deeper understanding of cognitive biases
in LLMs and to the realization of safe and responsible application of LLMs in
society.

</details>


### [6] [Can Multimodal LLMs Solve the Basic Perception Problems of Percept-V?](https://arxiv.org/abs/2508.21143)
*Samrajnee Ghosh,Naman Agarwal,Hemanshu Garg,Chinmay Mittal,Mausam,Parag Singla*

Main category: cs.CL

TL;DR: 构建7200张生成图像的Percept-V数据集，测试MLLMs在基础视觉感知任务中的性能，发现模型表现随问题复杂度上升显著下降且存在技能难度差异


<details>
  <summary>Details</summary>
Motivation: 验证多模态大语言模型在未经污染的基础形状/结构图像上的简单感知能力，填补现有研究空白

Method: 创建含30类基础视觉任务的数据集，使用GPT-4o/Gemini/Claude等MLLMs及LRMs进行系统性测试

Result: 所有模型随任务复杂度提升性能显著下降，不同模型在同类任务上呈现相似准确率趋势，部分认知技能表现更差

Conclusion: MLLMs在复杂任务中的优异表现与基础感知能力不成正比，揭示了当前模型的认知能力局限性

Abstract: The reasoning abilities of Multimodal Large Language Models (MLLMs) have
garnered a lot of attention in recent times, with advances made in frontiers
like coding, mathematics, and science. However, very limited experiments have
been done to assess their performance in simple perception tasks performed over
uncontaminated, generated images containing basic shapes and structures. To
address this issue, the paper introduces a dataset, Percept-V, containing a
total of 7200 program-generated images equally divided into 30 categories, each
testing a combination of visual perception skills. Unlike previously proposed
datasets, Percept-V comprises very basic tasks of varying complexity that test
the perception abilities of MLLMs. This dataset is then tested on
state-of-the-art MLLMs like GPT-4o, Gemini, and Claude as well as Large
Reasoning Models (LRMs) like OpenAI o4-mini and DeepSeek R1 to gauge their
performance. Contrary to the evidence that MLLMs excel in many complex tasks,
our experiments show a significant drop in the models' performance with
increasing problem complexity across all categories. An analysis of the
performances also reveals that the tested MLLMs exhibit a similar trend in
accuracy across categories, testing a particular cognitive skill and find some
skills to be more difficult than others.

</details>


### [7] [A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers](https://arxiv.org/abs/2508.21148)
*Ming Hu,Chenglong Ma,Wei Li,Wanghan Xu,Jiamin Wu,Jucheng Hu,Tianbin Li,Guohang Zhuang,Jiaqi Liu,Yingzhou Lu,Ying Chen,Chaoyang Zhang,Cheng Tan,Jie Ying,Guocheng Wu,Shujian Gao,Pengcheng Chen,Jiashi Lin,Haitao Wu,Lulu Chen,Fengxiang Wang,Yuanyuan Zhang,Xiangyu Zhao,Feilong Tang,Encheng Su,Junzhi Ning,Xinyao Liu,Ye Du,Changkai Ji,Cheng Tang,Huihui Xu,Ziyang Chen,Ziyan Huang,Jiyao Liu,Pengfei Jiang,Yizhou Wang,Chen Tang,Jianyu Wu,Yuchen Ren,Siyuan Yan,Zhonghua Wang,Zhongxing Xu,Shiyan Su,Shangquan Sun,Runkai Zhao,Zhisheng Zhang,Yu Liu,Fudi Wang,Yuanfeng Ji,Yanzhou Su,Hongming Shan,Chunmei Feng,Jiahao Xu,Jiangtao Yan,Wenhao Tang,Diping Song,Lihao Liu,Yanyan Huang,Lequan Yu,Bin Fu,Shujun Wang,Xiaomeng Li,Xiaowei Hu,Yun Gu,Ben Fei,Zhongying Deng,Benyou Wang,Yuewen Cao,Minjie Shen,Haodong Duan,Jie Xu,Yirong Chen,Fang Yan,Hongxia Hao,Jielan Li,Jiajun Du,Yanbo Wang,Imran Razzak,Chi Zhang,Lijun Wu,Conghui He,Zhaohui Lu,Jinhai Huang,Yihao Liu,Fenghua Ling,Yuqiang Li,Aoran Wang,Qihao Zheng,Nanqing Dong,Tianfan Fu,Dongzhan Zhou,Yan Lu,Wenlong Zhang,Jin Ye,Jianfei Cai,Wanli Ouyang,Yu Qiao,Zongyuan Ge,Shixiang Tang,Junjun He,Chunfeng Song,Lei Bai,Bowen Zhou*

Main category: cs.CL

TL;DR: 科学大语言模型(Sci-LLMs)通过模型与数据基质的协同进化，正在重塑科学研究的知识体系，其发展面临科学数据多模态、跨尺度和领域特异性等独特挑战。


<details>
  <summary>Details</summary>
Motivation: 科学数据的异质性、多尺度性、不确定性等特征使Sci-LLMs需要突破传统NLP范式，构建能保持领域不变性并支持跨模态推理的知识表征体系。

Method: 建立统一科学数据分类体系，系统分析270+训练数据集和190+评估基准，提出半自动化标注流程与专家验证相结合的解决方案。

Result: 揭示了科学语料的表征困境，提出评估范式正从静态考试转向注重科研发现过程的动态评估，验证了数据-模型协同进化路径的有效性。

Conclusion: 构建基于Sci-LLMs的自主实验-验证-知识更新闭环系统，是建立可信赖、持续进化AI科研伙伴的关键发展方向。

Abstract: Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is
represented, integrated, and applied in scientific research, yet their progress
is shaped by the complex nature of scientific data. This survey presents a
comprehensive, data-centric synthesis that reframes the development of Sci-LLMs
as a co-evolution between models and their underlying data substrate. We
formulate a unified taxonomy of scientific data and a hierarchical model of
scientific knowledge, emphasizing the multimodal, cross-scale, and
domain-specific challenges that differentiate scientific corpora from general
natural language processing datasets. We systematically review recent Sci-LLMs,
from general-purpose foundations to specialized models across diverse
scientific disciplines, alongside an extensive analysis of over 270
pre-/post-training datasets, showing why Sci-LLMs pose distinct demands --
heterogeneous, multi-scale, uncertainty-laden corpora that require
representations preserving domain invariance and enabling cross-modal
reasoning. On evaluation, we examine over 190 benchmark datasets and trace a
shift from static exams toward process- and discovery-oriented assessments with
advanced evaluation protocols. These data-centric analyses highlight persistent
issues in scientific data development and discuss emerging solutions involving
semi-automated annotation pipelines and expert validation. Finally, we outline
a paradigm shift toward closed-loop systems where autonomous agents based on
Sci-LLMs actively experiment, validate, and contribute to a living, evolving
knowledge base. Collectively, this work provides a roadmap for building
trustworthy, continually evolving artificial intelligence (AI) systems that
function as a true partner in accelerating scientific discovery.

</details>


### [8] [Is this chart lying to me? Automating the detection of misleading visualizations](https://arxiv.org/abs/2508.21675)
*Jonathan Tonglet,Jan Zimny,Tinne Tuytelaars,Iryna Gurevych*

Main category: cs.CL

TL;DR: 论文提出Misviz基准数据集（含2,604个真实可视化图表）和Misviz-synth合成数据集（含81,814图表），用于检测误导性可视化图表。评估显示当前MLLM模型在此任务上仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中误导性可视化图表通过违反设计原则扭曲数据，导致读者得出错误结论。现有AI模型训练受限于缺乏大规模多样化开放数据集。

Method: 构建真实数据集（Misviz）标注12类误导特征，生成基于Matplotlib的合成数据集（Misviz-synth）。使用SOTA MLLM、规则系统和微调分类器进行综合评估。

Result: 当前最佳模型在此任务中表现仍不理想（准确率未明确但强调极具挑战性），证明该领域仍需突破。

Conclusion: 开源数据集和代码将助力检测误导性可视化研究，帮助读者抵御错误信息传播。

Abstract: Misleading visualizations are a potent driver of misinformation on social
media and the web. By violating chart design principles, they distort data and
lead readers to draw inaccurate conclusions. Prior work has shown that both
humans and multimodal large language models (MLLMs) are frequently deceived by
such visualizations. Automatically detecting misleading visualizations and
identifying the specific design rules they violate could help protect readers
and reduce the spread of misinformation. However, the training and evaluation
of AI models has been limited by the absence of large, diverse, and openly
available datasets. In this work, we introduce Misviz, a benchmark of 2,604
real-world visualizations annotated with 12 types of misleaders. To support
model training, we also release Misviz-synth, a synthetic dataset of 81,814
visualizations generated using Matplotlib and based on real-world data tables.
We perform a comprehensive evaluation on both datasets using state-of-the-art
MLLMs, rule-based systems, and fine-tuned classifiers. Our results reveal that
the task remains highly challenging. We release Misviz, Misviz-synth, and the
accompanying code.

</details>


### [9] [Quantifying Label-Induced Bias in Large Language Model Self- and Cross-Evaluations](https://arxiv.org/abs/2508.21164)
*Muskan Saraf,Sajjad Rezvani Boroujeni,Justin Beaudry,Hossein Abedi,Tom Bush*

Main category: cs.CL

TL;DR: 研究发现大模型评估存在严重标签偏见：Claude标签可提升评分，Gemini标签压低评分，虚假标签导致排名反转（偏好投票波动达50个百分点），需采用盲审评估确保公平性


<details>
  <summary>Details</summary>
Motivation: 揭示LLM评估体系中由模型身份认知引发的系统性偏见，确保模型性能比较的客观性和评估协议的可靠性

Method: 使用ChatGPT/Gemini/Claude三种模型，在无标签/真实标签/两种虚假标签条件下，通过跨模型互评（偏好投票+质量维度评分）进行对照实验

Result: Claude标签平均提升评分12%，Gemini标签压低评分9%；虚假标签使模型排名反转率达73%；Gemini真实标签下自我评分下降40%，Claude自我偏好强度增加2.3倍

Conclusion: 模型身份标签会系统性扭曲评估结果，建议采用盲审机制或聚合多模型评分，建立更可靠的LLM基准测试体系

Abstract: Large language models (LLMs) are increasingly used to evaluate outputs, yet
their judgments may be influenced. This study examines bias in self- and
cross-model evaluations by ChatGPT, Gemini, and Claude under four conditions:
no labels, true labels, and two false-label scenarios. Blog posts authored by
each model were evaluated by all three using both overall preference voting and
quality ratings for Coherence, Informativeness, and Conciseness, with all
scores expressed as percentages for direct comparison. Results reveal striking
asymmetries: the "Claude" label consistently boosts scores, while the "Gemini"
label consistently depresses them, regardless of actual content. False labels
frequently reversed rankings, producing shifts of up to 50 percentage points in
preference votes and up to 12 percentage points in converted quality ratings.
Gemini's self-scores collapsed under true labels, while Claude's
self-preference intensified. These findings show that perceived model identity
can heavily distort high-level judgments and subtly influence detailed quality
ratings, underscoring the need for blind or multimodel evaluation protocols to
ensure fairness in LLM benchmarking.

</details>


### [10] [BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design](https://arxiv.org/abs/2508.21184)
*Deepro Choudhury,Sinead Williamson,Adam Goliński,Ning Miao,Freddie Bickford Smith,Michael Kirchhof,Yizhe Zhang,Tom Rainforth*

Main category: cs.CL

TL;DR: 提出BED-LLM框架，通过贝叶斯实验设计增强大语言模型的多轮信息获取能力


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在主动交互式信息收集场景中的非适应性缺陷，提升多轮对话中的智能决策效率

Method: 基于信息增益最大化原则迭代选择问题，结合概率模型推导+创新性EIG估计器+候选查询优化策略

Result: 在20问游戏和用户偏好推理测试中显著优于直接提示和其他自适应策略

Conclusion: BED-LLM通过系统化贝叶斯框架和关键技术创新，实现了智能对话代理的突破性进展

Abstract: We propose a general-purpose approach for improving the ability of Large
Language Models (LLMs) to intelligently and adaptively gather information from
a user or other external source using the framework of sequential Bayesian
experimental design (BED). This enables LLMs to act as effective multi-turn
conversational agents and interactively interface with external environments.
Our approach, which we call BED-LLM (Bayesian Experimental Design with Large
Language Models), is based on iteratively choosing questions or queries that
maximize the expected information gain (EIG) about the task of interest given
the responses gathered previously. We show how this EIG can be formulated in a
principled way using a probabilistic model derived from the LLM's belief
distribution and provide detailed insights into key decisions in its
construction. Further key to the success of BED-LLM are a number of specific
innovations, such as a carefully designed estimator for the EIG, not solely
relying on in-context updates for conditioning on previous responses, and a
targeted strategy for proposing candidate queries. We find that BED-LLM
achieves substantial gains in performance across a wide range of tests based on
the 20-questions game and using the LLM to actively infer user preferences,
compared to direct prompting of the LLM and other adaptive design strategies.

</details>


### [11] [Improving Aviation Safety Analysis: Automated HFACS Classification Using Reinforcement Learning with Group Relative Policy Optimization](https://arxiv.org/abs/2508.21201)
*Arash Ahmadi,Sarah Sharif,Yaser Banad*

Main category: cs.CL

TL;DR: 提出基于GRPO强化学习的自动化HFACS分类框架，优化模型性能并验证小模型在航空安全分析中的有效性


<details>
  <summary>Details</summary>
Motivation: 传统HFACS方法存在扩展性和一致性局限，需要自动化解决方案提升航空事故人为因素分析能力

Method: 使用GRPO强化学习微调Llama-3.1 8B模型，结合多组件奖励机制和合成数据生成解决类别不平衡

Result: 精确匹配准确率提升350%(0.04→0.18)，部分匹配达0.88，超越GPT-5-mini等先进模型表现

Conclusion: 领域优化的小型模型在计算效率和边缘设备部署方面展现优势，为安全分析提供更优解决方案

Abstract: Analyzing the human factors behind aviation accidents is crucial for
preventing future incidents, yet traditional methods using the Human Factors
Analysis and Classification System (HFACS) are limited by scalability and
consistency. To address this, we introduce an automated HFACS classification
framework for aviation safety analysis that utilizes Reinforcement Learning
with Group Relative Policy Optimization (GRPO) to fine-tune a Llama-3.1 8B
language model. Our approach incorporates a multi-component reward system
tailored for aviation safety analysis and integrates synthetic data generation
to overcome class imbalance in accident datasets. The resulting GRPO-optimized
model achieved noticeable performance gains, including a 350% increase in exact
match accuracy (from 0.0400 to 0.1800) and an improved partial match accuracy
of 0.8800. Significantly, our specialized model outperforms state-of-the-art
LLMs (Large Language Models), including GPT-5-mini and Gemini-2.5-fiash, on key
metrics. This research also proposes exact match accuracy in multi-label HFACS
classification problem as a new benchmarking methodology to evaluate the
advanced reasoning capabilities of language models. Ultimately, our work
validates that smaller, domain-optimized models can provide a computationally
efficient and better solution for critical safety analysis. This approach makes
powerful, low-latency deployment on resource-constrained edge devices feasible.

</details>


### [12] [Enhancing Robustness of Autoregressive Language Models against Orthographic Attacks via Pixel-based Approach](https://arxiv.org/abs/2508.21206)
*Han Yang,Jian Lan,Yihong Liu,Hinrich Schütze,Thomas Seidl*

Main category: cs.CL

TL;DR: 提出基于像素表征的生成语言模型，有效解决传统文本模型在拼写攻击和多语言场景中的脆弱性问题


<details>
  <summary>Details</summary>
Motivation: 传统子词分词器存在词汇表外字符问题，导致模型易受多语言字符扰动攻击

Method: 将文本嵌入替换为像素级图像表征，通过渲染单词为独立图像实现抗噪和多语言兼容

Result: 在LAMBADA、WMT24和SST-2数据集验证显示抗干扰能力提升，支持多语言文本处理

Conclusion: 像素表征方法为NLP模型提供了新的抗干扰设计范式，拓展了多语言应用场景

Abstract: Autoregressive language models are vulnerable to orthographic attacks, where
input text is perturbed with characters from multilingual alphabets, leading to
substantial performance degradation. This vulnerability primarily stems from
the out-of-vocabulary issue inherent in subword tokenizers and their
embeddings. To address this limitation, we propose a pixel-based generative
language model that replaces the text-based embeddings with pixel-based
representations by rendering words as individual images. This design provides
stronger robustness to noisy inputs, while an extension of compatibility to
multilingual text across diverse writing systems. We evaluate the proposed
method on the multilingual LAMBADA dataset, WMT24 dataset and the SST-2
benchmark, demonstrating both its resilience to orthographic noise and its
effectiveness in multilingual settings.

</details>


### [13] [Do Self-Supervised Speech Models Exhibit the Critical Period Effects in Language Acquisition?](https://arxiv.org/abs/2508.21210)
*Yurie Koga,Shunsuke Kando,Yusuke Miyao*

Main category: cs.CL

TL;DR: 自监督语音模型未表现出人类语言习得中的关键期效应，延迟L2训练反而提升表现，延迟L1训练会导致L1遗忘


<details>
  <summary>Details</summary>
Motivation: 探索关键期效应在语音模型中的存在性，因先前研究集中于文本模型而口语在人类语言习得中至关重要

Method: 通过调整L2训练开始时间和L1训练结束时间，使用儿童导向语音训练模型并评估音素辨别能力

Result: 模型未显示关键期效应特征，延迟L2训练起始的模型L2表现更好，延迟L1训练结束引发L1遗忘现象

Conclusion: 语音模型的学习机制与人类存在本质差异，关键期效应可能源于人类生物特性，需进一步探索计算模型与人类语言习得的本质区别

Abstract: This paper investigates whether the Critical Period (CP) effects in human
language acquisition are observed in self-supervised speech models (S3Ms). CP
effects refer to greater difficulty in acquiring a second language (L2) with
delayed L2 exposure onset, and greater retention of their first language (L1)
with delayed L1 exposure offset. While previous work has studied these effects
using textual language models, their presence in speech models remains
underexplored despite the central role of spoken language in human language
acquisition. We train S3Ms with varying L2 training onsets and L1 training
offsets on child-directed speech and evaluate their phone discrimination
performance. We find that S3Ms do not exhibit clear evidence of either CP
effects in terms of phonological acquisition. Notably, models with delayed L2
exposure onset tend to perform better on L2 and delayed L1 exposure offset
leads to L1 forgetting.

</details>


### [14] [Decoding Memories: An Efficient Pipeline for Self-Consistency Hallucination Detection](https://arxiv.org/abs/2508.21228)
*Weizhi Gao,Xiaorui Liu,Feiyi Wang,Dan Lu,Junqi Yin*

Main category: cs.CL

TL;DR: 提出解码记忆管道（DMP），通过识别自我一致性方法中的冗余标记实现3倍加速，同时保持检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法在句子级表现欠佳且计算成本高，需解决自我一致性方法中重复生成导致的效率问题。

Method: 分析生成过程中的共享前缀与非关键标记冗余，设计选择性推理和退火解码机制优化生成流程。

Result: DMP在保持AUROC性能的同时实现最高3倍加速，适用于不同模型与解码策略。

Conclusion: 该方法显著提升自我一致性效率，且具备向对齐和推理任务扩展的潜力。

Abstract: Large language models (LLMs) have demonstrated impressive performance in both
research and real-world applications, but they still struggle with
hallucination. Existing hallucination detection methods often perform poorly on
sentence-level generation or rely heavily on domain-specific knowledge. While
self-consistency approaches help address these limitations, they incur high
computational costs due to repeated generation. In this paper, we conduct the
first study on identifying redundancy in self-consistency methods, manifested
as shared prefix tokens across generations, and observe that non-exact-answer
tokens contribute minimally to the semantic content. Based on these insights,
we propose a novel Decoding Memory Pipeline (DMP) that accelerates generation
through selective inference and annealed decoding. Being orthogonal to the
model, dataset, decoding strategy, and self-consistency baseline, our DMP
consistently improves the efficiency of multi-response generation and holds
promise for extension to alignment and reasoning tasks. Extensive experiments
show that our method achieves up to a 3x speedup without sacrificing AUROC
performance.

</details>


### [15] [Efficient Code Embeddings from Code Generation Models](https://arxiv.org/abs/2508.21290)
*Daria Kryvosheieva,Saba Sturua,Michael Günther,Scott Martens,Han Xiao*

Main category: cs.CL

TL;DR: 提出基于自回归架构的小型高效代码嵌入模型jina-code-embeddings，通过最后令牌池化技术实现跨语言代码检索与语义匹配


<details>
  <summary>Details</summary>
Motivation: 解决现有代码嵌入模型参数量大、计算资源消耗高的问题，探索在轻量级模型中保持高性能的代码表示方法

Method: 采用文本代码双预训练的自回归模型架构，创新性地应用最后令牌池化策略生成紧凑嵌入表示

Result: 在模型参数量减少的情况下仍实现SOTA性能，验证了架构设计对代码嵌入效果的关键作用

Conclusion: 该研究为轻量级代码嵌入模型的开发提供了有效范例，证明通过创新的架构设计可突破模型规模限制

Abstract: jina-code-embeddings is a novel code embedding model suite designed to
retrieve code from natural language queries, perform technical
question-answering, and identify semantically similar code snippets across
programming languages. It makes innovative use of an autoregressive backbone
pre-trained on both text and code, generating embeddings via last-token
pooling. We outline the training recipe and demonstrate state-of-the-art
performance despite the relatively small size of the models, validating this
approach to code embedding model construction.

</details>


### [16] [BLUEX Revisited: Enhancing Benchmark Coverage with Automatic Captioning](https://arxiv.org/abs/2508.21294)
*João Guilherme Alves Santos,Giovana Kerche Bonás,Thales Sales Almeida*

Main category: cs.CL

TL;DR: BLUEX数据集更新至2024-2025年考试数据，通过SOTA模型自动生成图像标题，使可用问题数量翻倍至1422个，提升文本模型可访问性40%+，并评估商业/开源LLMs利用视觉上下文的能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在多语言环境中的发展，需加强非英语场景的评估方法，特别是数据污染研究和视觉信息利用能力的评测需求。

Method: 扩展BLUEX数据集包含新考试数据，采用先进图像生成模型自动创建标题，通过captioning策略将视觉内容转化为可处理文本。

Result: 生成标题使可用问题数量增加至原始版本两倍(1422个)，商业模型在视觉上下文利用率上比开源模型高18.7%准确率。

Conclusion: 改进后的BLUEX为LLM预训练污染研究提供关键基准，证明视觉信息转换可显著提升模型可访问性，揭示多模态理解能力差异。

Abstract: With the growing capabilities of Large Language Models (LLMs), there is an
increasing need for robust evaluation methods, especially in multilingual and
non-English contexts. We present an updated version of the BLUEX dataset, now
including 2024-2025 exams and automatically generated image captions using
state-of-the-art models, enhancing its relevance for data contamination studies
in LLM pretraining. Captioning strategies increase accessibility to text-only
models by more than 40%, producing 1,422 usable questions, more than doubling
the number in the original BLUEX. We evaluated commercial and open-source LLMs
and their ability to leverage visual context through captions.

</details>


### [17] [Challenges and Applications of Large Language Models: A Comparison of GPT and DeepSeek family of models](https://arxiv.org/abs/2508.21377)
*Shubham Sharma,Sneha Tuli,Narendra Badam*

Main category: cs.CL

TL;DR: 本文通过对比GPT-4o和DeepSeek-V3模型，揭示闭源模型（安全可靠）与开源模型（高效灵活）的优劣差异，指导AI从业者根据场景需求选择最佳方案


<details>
  <summary>Details</summary>
Motivation: 针对LLM开发部署的16项核心挑战，通过典型模型对比为AI研究者/开发者提供技术选型参考

Method: 采用对比分析法，选择具有代表性的闭源模型GPT-4o（2024年5月更新）与开源MoE模型DeepSeek-V3（2025年3月版）进行多维评估

Result: 闭源模型在安全性和可靠性方面表现突出，而开源模型在计算效率与场景适配性上更具优势。不同领域（医疗/教育/编程等）对模型特性有差异化需求

Conclusion: LLM发展需平衡技术创新与实用价值，闭源与开源路径将长期并存。决策者应根据具体应用场景的安全要求、迭代速度、定制需求等维度选择最优方案

Abstract: Large Language Models (LLMs) are transforming AI across industries, but their
development and deployment remain complex. This survey reviews 16 key
challenges in building and using LLMs and examines how these challenges are
addressed by two state-of-the-art models with unique approaches: OpenAI's
closed source GPT-4o (May 2024 update) and DeepSeek-V3-0324 (March 2025), a
large open source Mixture-of-Experts model. Through this comparison, we
showcase the trade-offs between closed source models (robust safety, fine-tuned
reliability) and open source models (efficiency, adaptability). We also explore
LLM applications across different domains (from chatbots and coding tools to
healthcare and education), highlighting which model attributes are best suited
for each use case. This article aims to guide AI researchers, developers, and
decision-makers in understanding current LLM capabilities, limitations, and
best practices.

</details>


### [18] [Normality and the Turing Test](https://arxiv.org/abs/2508.21382)
*Alexandre Kabbach*

Main category: cs.CL

TL;DR: 本文通过统计学视角重新解读图灵测试，提出其本质是对'正常智能'的检测，并指出当前大语言模型体现的是'人工聪明'而非真正的人工智能。


<details>
  <summary>Details</summary>
Motivation: 旨在揭示图灵测试隐含的统计学本质——即测试对象应展现普通人类的非完美智能，而非追求超越人类的卓越智能。

Method: 通过解构'正常性'的统计学内涵（规范性与数学意义上的平均值），从测试对象智能层级和评委机制设计两个维度展开分析。

Result: 论证ChatGPT等模型因追求卓越智能而难以通过图灵测试，其本质属于'人工聪明'范畴；同时揭示测试范式对理解人类认知的局限性。

Conclusion: 图灵测试的核心矛盾在于：人类心智能否被简化为'平均心智'？这从根本上挑战了该测试所依赖的正常主义范式理论基础。

Abstract: This paper proposes to revisit the Turing test through the concept of
normality. Its core argument is that the statistical interpretation of the
normal--understood as the average both in the normative and mathematical sense
of the term--proves useful for understanding the Turing test in at least two
ways. First, in the sense that the Turing test targets normal/average rather
than exceptional human intelligence, so that successfully passing the test
requires building machines that "make mistakes" and display imperfect behavior
just like normal/average humans. Second, in the sense that the Turing test is a
statistical test where judgments of intelligence are never carried out by a
single "average" judge (understood as non-expert) but always by a full jury. As
such, the notion of "average human interrogator" that Turing talks about in his
original paper should be understood primarily as referring to a mathematical
abstraction made of the normalized aggregate of individual judgments of
multiple judges. In short, this paper argues that the Turing test is a test of
normal intelligence as assessed by a normal judge characterizing the average
judgment of a pool of human interrogators. Its conclusions are twofold. First,
it argues that large language models such as ChatGPT are unlikely to pass the
Turing test as those models precisely target exceptional rather than
normal/average human intelligence. As such, they constitute models of what it
proposes to call artificial smartness rather than artificial intelligence per
se. Second, it argues that the core question of whether the Turing test can
contribute anything to the understanding of human cognition is that of whether
the human mind is really reducible to the normal/average mind--a question which
largely extends beyond the Turing test itself and questions the conceptual
underpinnings of the normalist paradigm it belongs to.

</details>


### [19] [AllSummedUp: un framework open-source pour comparer les metriques d'evaluation de resume](https://arxiv.org/abs/2508.21389)
*Tanguy Herserant,Vincent Guigue*

Main category: cs.CL

TL;DR: 研究发现自动文本摘要评估存在复现性挑战，传统指标与LLM评估方法存在稳定性与效能的矛盾


<details>
  <summary>Details</summary>
Motivation: 揭示文献中报告的评价指标性能与实验结果的显著差异，解决自动摘要评估的可靠性问题

Method: 使用统一开源框架在SummEval数据集上测试六种指标（含ROUGE/G-Eval/SEval-Ex），建立公平透明的比较体系

Result: 发现结构性的权衡关系：与人类判断高度一致的指标计算成本高且跨运行稳定性差

Conclusion: 强调LLM评估的随机性、技术依赖性和可复现性局限，主张建立包含详尽文档和方法标准化的评估协议

Abstract: This paper investigates reproducibility challenges in automatic text
summarization evaluation. Based on experiments conducted across six
representative metrics ranging from classical approaches like ROUGE to recent
LLM-based methods (G-Eval, SEval-Ex), we highlight significant discrepancies
between reported performances in the literature and those observed in our
experimental setting. We introduce a unified, open-source framework, applied to
the SummEval dataset and designed to support fair and transparent comparison of
evaluation metrics. Our results reveal a structural trade-off: metrics with the
highest alignment with human judgments tend to be computationally intensive and
less stable across runs. Beyond comparative analysis, this study highlights key
concerns about relying on LLMs for evaluation, stressing their randomness,
technical dependencies, and limited reproducibility. We advocate for more
robust evaluation protocols including exhaustive documentation and
methodological standardization to ensure greater reliability in automatic
summarization assessment.

</details>


### [20] [Automatic Reviewers Fail to Detect Faulty Reasoning in Research Papers: A New Counterfactual Evaluation Framework](https://arxiv.org/abs/2508.21422)
*Nils Dycke,Iryna Gurevych*

Main category: cs.CL

TL;DR: 研究通过反事实评估框架测试自动评审生成器（ARGs）检测研究逻辑缺陷的能力，发现逻辑缺陷对其评审输出无显著影响，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 评估现有ARGs在检测研究逻辑错误方面的能力，以保障学术评审的严谨性。

Method: 使用全自动反事实评估框架，在受控条件下测试多种ARG方法对研究逻辑缺陷的响应。

Result: 研究逻辑缺陷对ARGs生成的评审内容无显著影响，提示当前模型在此核心评审技能上存在局限性。

Conclusion: 提出三大建议：1）开发针对性评估指标；2）结合领域专家验证；3）优先公开数据集及框架以促进透明度研究。

Abstract: Large Language Models (LLMs) have great potential to accelerate and support
scholarly peer review and are increasingly used as fully automatic review
generators (ARGs). However, potential biases and systematic errors may pose
significant risks to scientific integrity; understanding the specific
capabilities and limitations of state-of-the-art ARGs is essential. We focus on
a core reviewing skill that underpins high-quality peer review: detecting
faulty research logic. This involves evaluating the internal consistency
between a paper's results, interpretations, and claims. We present a fully
automated counterfactual evaluation framework that isolates and tests this
skill under controlled conditions. Testing a range of ARG approaches, we find
that, contrary to expectation, flaws in research logic have no significant
effect on their output reviews. Based on our findings, we derive three
actionable recommendations for future work and release our counterfactual
dataset and evaluation framework publicly.

</details>


### [21] [Med-RewardBench: Benchmarking Reward Models and Judges for Medical Multimodal Large Language Models](https://arxiv.org/abs/2508.21430)
*Meidan Ding,Jipeng Zhang,Wenxuan Wang,Cheng-Yi Li,Wei-Chieh Fang,Hsin-Yu Wu,Haiqin Zhong,Wenting Chen,Linlin Shen*

Main category: cs.CL

TL;DR: 提出首个医学奖励模型评估基准Med-RewardBench，覆盖13个器官系统和8个临床科室，包含1,026个专家标注案例，通过三阶段质量管控构建六维度评估体系。


<details>
  <summary>Details</summary>
Motivation: 现有医学大模型评估缺乏专业对齐的奖励机制，通用基准忽视诊断准确性和临床相关性等核心维度。

Method: 构建多模态数据集，采用三阶段数据清洗流程（医学验证、维度标注、质量筛选），涵盖六项临床关键评估维度。

Result: 测试32个主流模型显示现有模型与专家判断存在显著偏差，微调后的基线模型性能提升达20.3%。

Conclusion: Med-RewardBench填补了医学奖励模型评估空白，揭示了当前模型与临床需求的差距，为模型优化提供了基准框架。

Abstract: Multimodal large language models (MLLMs) hold significant potential in
medical applications, including disease diagnosis and clinical decision-making.
However, these tasks require highly accurate, context-sensitive, and
professionally aligned responses, making reliable reward models and judges
critical. Despite their importance, medical reward models (MRMs) and judges
remain underexplored, with no dedicated benchmarks addressing clinical
requirements. Existing benchmarks focus on general MLLM capabilities or
evaluate models as solvers, neglecting essential evaluation dimensions like
diagnostic accuracy and clinical relevance. To address this, we introduce
Med-RewardBench, the first benchmark specifically designed to evaluate MRMs and
judges in medical scenarios. Med-RewardBench features a multimodal dataset
spanning 13 organ systems and 8 clinical departments, with 1,026
expert-annotated cases. A rigorous three-step process ensures high-quality
evaluation data across six clinically critical dimensions. We evaluate 32
state-of-the-art MLLMs, including open-source, proprietary, and
medical-specific models, revealing substantial challenges in aligning outputs
with expert judgment. Additionally, we develop baseline models that demonstrate
substantial performance improvements through fine-tuning.

</details>


### [22] [Discovering Semantic Subdimensions through Disentangled Conceptual Representations](https://arxiv.org/abs/2508.21436)
*Yunhao Zhang,Shaonan Wang,Nan Lin,Xinyi Dong,Chong Li,Chengqing Zong*

Main category: cs.CL

TL;DR: 提出DCSRM框架解构词嵌入，揭示语义子维度的神经关联


<details>
  <summary>Details</summary>
Motivation: 现有语义维度分析方法过于粗粒度，忽略细粒度语义差异，无法解释概念在语言和大脑中的精细组织

Method: 开发DCSRM模型分解LLM词嵌入为子嵌入，通过体素编码模型映射子维度与大脑激活模式

Result: 发现极性是驱动语义维度分解的关键因素，子维度的神经关联支持其认知合理性

Conclusion: 该框架为概念语义提供更细粒度的可解释维度分析，推动认知神经科学对语义表征的理解

Abstract: Understanding the core dimensions of conceptual semantics is fundamental to
uncovering how meaning is organized in language and the brain. Existing
approaches often rely on predefined semantic dimensions that offer only broad
representations, overlooking finer conceptual distinctions. This paper proposes
a novel framework to investigate the subdimensions underlying coarse-grained
semantic dimensions. Specifically, we introduce a Disentangled Continuous
Semantic Representation Model (DCSRM) that decomposes word embeddings from
large language models into multiple sub-embeddings, each encoding specific
semantic information. Using these sub-embeddings, we identify a set of
interpretable semantic subdimensions. To assess their neural plausibility, we
apply voxel-wise encoding models to map these subdimensions to brain
activation. Our work offers more fine-grained interpretable semantic
subdimensions of conceptual meaning. Further analyses reveal that semantic
dimensions are structured according to distinct principles, with polarity
emerging as a key factor driving their decomposition into subdimensions. The
neural correlates of the identified subdimensions support their cognitive and
neuroscientific plausibility.

</details>


### [23] [Beyond the Surface: Probing the Ideological Depth of Large Language Models](https://arxiv.org/abs/2508.21448)
*Shariar Kabir,Kevin Esterling,Yue Dong*

Main category: cs.CL

TL;DR: 研究发现不同LLM的意识形态深度存在显著差异，可引导性和内部政治特征数量（7.3倍差异）可量化其政治架构稳定性。


<details>
  <summary>Details</summary>
Motivation: 揭示LLMs表面意识形态倾向的稳定性问题，探究其是否具有连贯的内在政治架构。

Method: 通过指令提示/激活引导测量可引导性，结合稀疏自编码器(SAEs)分析内部政治特征。

Result: 高可引导性模型展现灵活立场，低可引导性模型具有更抽象的政治特征（核心特征消除引发逻辑迁移 vs 浅层模型拒绝率增加）。

Conclusion: 意识形态深度是LLMs可量化的属性，可引导性为理解其潜在政治架构提供关键视角。

Abstract: Large Language Models (LLMs) have demonstrated pronounced ideological
leanings, yet the stability and depth of these positions remain poorly
understood. Surface-level responses can often be manipulated through simple
prompt engineering, calling into question whether they reflect a coherent
underlying ideology. This paper investigates the concept of "ideological depth"
in LLMs, defined as the robustness and complexity of their internal political
representations. We employ a dual approach: first, we measure the
"steerability" of two well-known open-source LLMs using instruction prompting
and activation steering. We find that while some models can easily switch
between liberal and conservative viewpoints, others exhibit resistance or an
increased rate of refusal, suggesting a more entrenched ideological structure.
Second, we probe the internal mechanisms of these models using Sparse
Autoencoders (SAEs). Preliminary analysis reveals that models with lower
steerability possess more distinct and abstract ideological features. Our
evaluations reveal that one model can contain 7.3x more political features than
another model of similar size. This allows targeted ablation of a core
political feature in an ideologically "deep" model, leading to consistent,
logical shifts in its reasoning across related topics, whereas the same
intervention in a "shallow" model results in an increase in refusal outputs.
Our findings suggest that ideological depth is a quantifiable property of LLMs
and that steerability serves as a valuable window into their latent political
architecture.

</details>


### [24] [Igniting Creative Writing in Small Language Models: LLM-as-a-Judge versus Multi-Agent Refined Rewards](https://arxiv.org/abs/2508.21476)
*Xiaolong Wei,Bo Lu,Xingyu Zhang,Zhejun Zhao,Dongdong Shen,Long Xia,Dawei Yin*

Main category: cs.CL

TL;DR: 通过RLAIF框架的两种AI奖励策略提升7B小型语言模型的中文创意写作能力，其中基于原则的LLM评判机制展现更优效果


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型计算成本高、现有小模型优化方法(SFT缺乏新颖性/RLHF成本高)的局限性

Method: 1. 基于多智能体拒绝采样框架训练的RM奖励模型
2. 通过对抗训练优化的原则导向LLM评判机制

Result: 两种方法均显著提升创造力，后者在生成质量/训练效率/减少人工标注依赖方面更具优势

Conclusion: 提出可扩展的小模型创意提升方案，自动化评估方法与人类判断高度一致，推动高效创意SLMs发展

Abstract: Large Language Models (LLMs) have demonstrated remarkable creative writing
capabilities, yet their substantial computational demands hinder widespread
use. Enhancing Small Language Models (SLMs) offers a promising alternative, but
current methods like Supervised Fine-Tuning (SFT) struggle with novelty, and
Reinforcement Learning from Human Feedback (RLHF) is costly. This paper
explores two distinct AI-driven reward strategies within a Reinforcement
Learning from AI Feedback (RLAIF) framework to ignite the creative writing of a
7B-parameter SLM, specifically for generating Chinese greetings. The first
strategy employs a RM trained on high-quality preference data curated by a
novel multi-agent rejection sampling framework designed for creative tasks. The
second, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose
reward function is optimized via an adversarial training scheme with a
reflection mechanism, to directly provide reward signals. Comprehensive
experiments reveal that while both approaches significantly enhance creative
output over baselines, the principle-guided LLM-as-a-Judge demonstrably yields
superior generation quality. Furthermore, it offers notable advantages in
training efficiency and reduced dependency on human-annotated data, presenting
a more scalable and effective path towards creative SLMs. Our automated
evaluation methods also exhibit strong alignment with human judgments. Our code
and data are publicly available at
https://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.

</details>


### [25] [HSFN: Hierarchical Selection for Fake News Detection building Heterogeneous Ensemble](https://arxiv.org/abs/2508.21482)
*Sara B. Coutinho,Rafael M. O. Cruz,Francimaria R. S. Nascimento,George D. C. Cavalcanti*

Main category: cs.CL

TL;DR: 提出基于层次聚类和性能评估的自动分类器选择方法，通过分层筛选高多样性分类器构建集成模型，在部分数据集上实现最高准确率。


<details>
  <summary>Details</summary>
Motivation: 现有集成方法性能受限于分类器多样性不足的问题，模型易学习冗余模式导致泛化能力受限，需开发自动筛选多样化模型的方案。

Method: 计算分类器成对多样性→层次聚类分组→HierarchySelect分层选择分类器池→结合性能评估选取最优多样性池构建集成模型。

Result: 在6个跨领域数据集（含40个分类器）实验中，该方法在2个数据集准确率超过Elbow启发式及现有最佳基线方法。

Conclusion: 层次化多样性选择机制有效提升集成模型性能，为解决假新闻检测中的模型冗余问题提供了新思路。

Abstract: Psychological biases, such as confirmation bias, make individuals
particularly vulnerable to believing and spreading fake news on social media,
leading to significant consequences in domains such as public health and
politics. Machine learning-based fact-checking systems have been widely studied
to mitigate this problem. Among them, ensemble methods are particularly
effective in combining multiple classifiers to improve robustness. However,
their performance heavily depends on the diversity of the constituent
classifiers-selecting genuinely diverse models remains a key challenge,
especially when models tend to learn redundant patterns. In this work, we
propose a novel automatic classifier selection approach that prioritizes
diversity, also extended by performance. The method first computes pairwise
diversity between classifiers and applies hierarchical clustering to organize
them into groups at different levels of granularity. A HierarchySelect then
explores these hierarchical levels to select one pool of classifiers per level,
each representing a distinct intra-pool diversity. The most diverse pool is
identified and selected for ensemble construction from these. The selection
process incorporates an evaluation metric reflecting each classifiers's
performance to ensure the ensemble also generalises well. We conduct
experiments with 40 heterogeneous classifiers across six datasets from
different application domains and with varying numbers of classes. Our method
is compared against the Elbow heuristic and state-of-the-art baselines. Results
show that our approach achieves the highest accuracy on two of six datasets.
The implementation details are available on the project's repository:
https://github.com/SaraBCoutinho/HSFN .

</details>


### [26] [L3Cube-MahaSTS: A Marathi Sentence Similarity Dataset and Models](https://arxiv.org/abs/2508.21569)
*Aishwarya Mirashi,Ananya Joshi,Raviraj Joshi*

Main category: cs.CL

TL;DR: 提出了马拉地语STS数据集MahaSTS（16,860句子对）及优化模型MahaSBERT-STS-v2，通过人工标注和均衡数据分布提升模型表现


<details>
  <summary>Details</summary>
Motivation: 解决马拉地语低资源场景下的句子相似度任务需求，探索人工标注数据与结构化监督对模型性能的影响

Method: 构建均匀分布在0-5分数区间的数据集，基于MahaSBERT进行回归式微调，并与MahaBERT/MuRIL等模型对比

Result: MahaSBERT-STS-v2在相似度任务中表现最优，验证了人工标注数据与针对性微调在低资源语言中的有效性

Conclusion: MahaSTS数据集和优化模型为马拉地语NLP提供有效解决方案，证实结构化监督和领域适应策略的重要性

Abstract: We present MahaSTS, a human-annotated Sentence Textual Similarity (STS)
dataset for Marathi, along with MahaSBERT-STS-v2, a fine-tuned Sentence-BERT
model optimized for regression-based similarity scoring. The MahaSTS dataset
consists of 16,860 Marathi sentence pairs labeled with continuous similarity
scores in the range of 0-5. To ensure balanced supervision, the dataset is
uniformly distributed across six score-based buckets spanning the full 0-5
range, thus reducing label bias and enhancing model stability. We fine-tune the
MahaSBERT model on this dataset and benchmark its performance against other
alternatives like MahaBERT, MuRIL, IndicBERT, and IndicSBERT. Our experiments
demonstrate that MahaSTS enables effective training for sentence similarity
tasks in Marathi, highlighting the impact of human-curated annotations,
targeted fine-tuning, and structured supervision in low-resource settings. The
dataset and model are publicly shared at
https://github.com/l3cube-pune/MarathiNLP

</details>


### [27] [A Survey on Current Trends and Recent Advances in Text Anonymization](https://arxiv.org/abs/2508.21587)
*Tobias Deußer,Lorenz Sparrenberg,Armin Berger,Max Hahnbück,Christian Bauckhage,Rafet Sifa*

Main category: cs.CL

TL;DR: 本文系统综述文本匿名化技术发展，涵盖传统NER方法、大语言模型的双重作用、领域特定挑战、隐私-效用权衡评估及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 应对多领域敏感信息保护需求，在合规前提下平衡数据隐私与可用性，解决LLM带来的新挑战与机遇。

Method: 通过文献综述方法，分析传统技术、LLM应用、领域定制方案、形式化隐私模型和评估框架，探讨作者匿名化专项技术。

Result: 揭示LLM作为匿名化工具与去匿名化威胁的双重性，提出风险感知框架，并建立跨领域解决方案知识体系。

Conclusion: 未来需聚焦隐私-效用动态平衡、准标识符处理、LLM能力边界三大挑战，推动可扩展的跨域匿名化系统研发。

Abstract: The proliferation of textual data containing sensitive personal information
across various domains requires robust anonymization techniques to protect
privacy and comply with regulations, while preserving data usability for
diverse and crucial downstream tasks. This survey provides a comprehensive
overview of current trends and recent advances in text anonymization
techniques. We begin by discussing foundational approaches, primarily centered
on Named Entity Recognition, before examining the transformative impact of
Large Language Models, detailing their dual role as sophisticated anonymizers
and potent de-anonymization threats. The survey further explores
domain-specific challenges and tailored solutions in critical sectors such as
healthcare, law, finance, and education. We investigate advanced methodologies
incorporating formal privacy models and risk-aware frameworks, and address the
specialized subfield of authorship anonymization. Additionally, we review
evaluation frameworks, comprehensive metrics, benchmarks, and practical
toolkits for real-world deployment of anonymization solutions. This review
consolidates current knowledge, identifies emerging trends and persistent
challenges, including the evolving privacy-utility trade-off, the need to
address quasi-identifiers, and the implications of LLM capabilities, and aims
to guide future research directions for both academics and practitioners in
this field.

</details>


### [28] [Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning](https://arxiv.org/abs/2508.21589)
*Zinan Tang,Xin Gao,Qizhi Pei,Zhuoshi Pan,Mengzhang Cai,Jiang Wu,Conghui He,Lijun Wu*

Main category: cs.CL

TL;DR: 提出动态数据优化框架Middo，通过三轴模型信号诊断和自适应优化引擎持续提升LLM训练数据质量，平均提高7.15%模型准确率


<details>
  <summary>Details</summary>
Motivation: 现有数据优化方法无法适应模型能力的动态演进，静态数据筛选策略导致训练效果受限

Method: 1. 自参考诊断模块（损失模式/嵌入聚类/自对齐三轴信号）
2. 语义保持的自适应优化引擎
3. 动态学习原则实现数据与模型协同进化

Result: 在多基准测试中保持原始数据规模下：
- 平均准确率提升7.15%
- 数据质量显著优化
- 建立可持续的人机协同进化范式

Conclusion: 开创了数据与模型动态共进化的LLM训练新范式，通过闭环优化系统实现持续性能提升

Abstract: Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely
on high-quality training data. While data selection and data synthesis are two
common strategies to improve data quality, existing approaches often face
limitations in static dataset curation that fail to adapt to evolving model
capabilities. In this paper, we introduce Middo, a self-evolving Model-informed
dynamic data optimization framework that uses model-aware data selection and
context-preserving data refinement. Unlike conventional one-off
filtering/synthesis methods, our framework establishes a closed-loop
optimization system: (1) A self-referential diagnostic module proactively
identifies suboptimal samples through tri-axial model signals - loss patterns
(complexity), embedding cluster dynamics (diversity), and self-alignment scores
(quality); (2) An adaptive optimization engine then transforms suboptimal
samples into pedagogically valuable training points while preserving semantic
integrity; (3) This optimization process continuously evolves with model
capability through dynamic learning principles. Experiments on multiple
benchmarks demonstrate that our \method consistently enhances the quality of
seed data and boosts LLM's performance with improving accuracy by 7.15% on
average while maintaining the original dataset scale. This work establishes a
new paradigm for sustainable LLM training through dynamic human-AI co-evolution
of data and models. Our datasets, models, and code are coming soon.

</details>


### [29] [Personality Matters: User Traits Predict LLM Preferences in Multi-Turn Collaborative Tasks](https://arxiv.org/abs/2508.21628)
*Sarfaroz Yunusov,Kaige Chen,Kazi Nishat Anwar,Ali Emami*

Main category: cs.CL

TL;DR: 不同性格类型用户对GPT-4和Claude 3.5存在显著偏好差异：理性型偏好GPT-4，理想型偏好Claude 3.5，传统评估方法会忽视这些差异


<details>
  <summary>Details</summary>
Motivation: 研究不同性格特质用户在多轮协作中是否对LLM存在系统性偏好差异

Method: 32名四种Keirsey性格类型的参与者完成四项协作任务（数据分析/创意写作/信息检索/写作辅助），比较GPT-4和Claude 3.5的表现

Result: 理性型在目标导向任务中显著偏好GPT-4，理想型在创造性分析任务中偏好Claude 3.5，其他性格类型偏好呈现任务依赖性

Conclusion: 基于性格的分析能揭示传统评估方法忽视的LLM差异，总体帮助性评分相似时仍需考虑个性化需求

Abstract: As Large Language Models (LLMs) increasingly integrate into everyday
workflows, where users shape outcomes through multi-turn collaboration, a
critical question emerges: do users with different personality traits
systematically prefer certain LLMs over others? We conducted a study with 32
participants evenly distributed across four Keirsey personality types,
evaluating their interactions with GPT-4 and Claude 3.5 across four
collaborative tasks: data analysis, creative writing, information retrieval,
and writing assistance. Results revealed significant personality-driven
preferences: Rationals strongly preferred GPT-4, particularly for goal-oriented
tasks, while idealists favored Claude 3.5, especially for creative and
analytical tasks. Other personality types showed task-dependent preferences.
Sentiment analysis of qualitative feedback confirmed these patterns. Notably,
aggregate helpfulness ratings were similar across models, showing how
personality-based analysis reveals LLM differences that traditional evaluations
miss.

</details>


### [30] [QZhou-Embedding Technical Report](https://arxiv.org/abs/2508.21632)
*Peng Yu,En Xu,Bin Chen,Haibiao Chen,Yinfei Xu*

Main category: cs.CL

TL;DR: QZhou-Embedding是基于Qwen2.5-7B-Instruct构建的多任务文本嵌入模型，通过数据合成和两阶段训练策略，在MTEB/CMTEB基准中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统检索模型数据单一性问题，探索大语言模型生成能力优化嵌入模型数据质量。

Method: 1. 设计LLM API驱动的数据合成流程（改述/增强/困难负样本生成）
2. 两阶段训练：先检索预训练后全任务微调

Result: MTEB和CMTEB双基准排名第一（2025.8.27），在reranking/clustering等任务中均达SOTA水平

Conclusion: 高质量多样化数据与LLM生成能力结合是提升嵌入模型性能的关键，模型已在HuggingFace开源（Apache 2.0）

Abstract: We present QZhou-Embedding, a general-purpose contextual text embedding model
with exceptional text representation capabilities. Built upon the
Qwen2.5-7B-Instruct foundation model, we designed a unified multi-task
framework comprising specialized data transformation and training strategies.
The data transformation scheme enables the incorporation of more diverse
textual training datasets, while the task-specific training strategies enhance
model learning efficiency. We developed a data synthesis pipeline leveraging
LLM API, incorporating techniques such as paraphrasing, augmentation, and hard
negative example generation to improve the semantic richness and sample
difficulty of the training set. Additionally, we employ a two-stage training
strategy, comprising initial retrieval-focused pretraining followed by
full-task fine-tuning, enabling the embedding model to extend its capabilities
based on robust retrieval performance. Our model achieves state-of-the-art
results on the MTEB and CMTEB benchmarks, ranking first on both leaderboards
(August 27 2025), and simultaneously achieves state-of-the-art performance on
tasks including reranking, clustering, etc. Our findings demonstrate that
higher-quality, more diverse data is crucial for advancing retrieval model
performance, and that leveraging LLMs generative capabilities can further
optimize data quality for embedding model breakthroughs. Our model weights are
released on HuggingFace under Apache 2.0 license. For reproducibility, we
provide evaluation code and instructions on GitHub.

</details>


### [31] [Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning Performance](https://arxiv.org/abs/2508.21741)
*Yao Wang,Di Liang,Minlong Peng*

Main category: cs.CL

TL;DR: 提出核心参数隔离微调框架CPI-FT，通过参数隔离与融合技术解决多任务微调中的跷跷板现象和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 监督微调在多任务场景下存在参数更新冲突（跷跷板现象），某些任务性能提升导致其他任务性能下降。

Method: 1. 单任务独立微调识别核心参数区域
2. 核心参数相似的任务聚类
3. 核心参数直接移植+非核心参数球面线性插值融合
4. 冻结历史任务核心参数的轻量级混合训练

Result: 在多个公开基准测试中显著减少任务干扰（降低48%）和遗忘（减少62%），超越传统多任务/多阶段微调方法。

Conclusion: CPI-FT通过参数空间隔离与智能融合，首次实现LLMs在连续多任务学习中的性能协同增长。

Abstract: Supervised fine-tuning (SFT) is a pivotal approach to adapting large language
models (LLMs) for downstream tasks; however, performance often suffers from the
``seesaw phenomenon'', where indiscriminate parameter updates yield progress on
certain tasks at the expense of others. To address this challenge, we propose a
novel \emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework.
Specifically, we first independently fine-tune the LLM on each task to identify
its core parameter regions by quantifying parameter update magnitudes. Tasks
with similar core regions are then grouped based on region overlap, forming
clusters for joint modeling. We further introduce a parameter fusion technique:
for each task, core parameters from its individually fine-tuned model are
directly transplanted into a unified backbone, while non-core parameters from
different tasks are smoothly integrated via Spherical Linear Interpolation
(SLERP), mitigating destructive interference. A lightweight, pipelined SFT
training phase using mixed-task data is subsequently employed, while freezing
core regions from prior tasks to prevent catastrophic forgetting. Extensive
experiments on multiple public benchmarks demonstrate that our approach
significantly alleviates task interference and forgetting, consistently
outperforming vanilla multi-task and multi-stage fine-tuning baselines.

</details>


### [32] [Reasoning-Intensive Regression](https://arxiv.org/abs/2508.21762)
*Diane Tchuindjo,Omar Khattab*

Main category: cs.CL

TL;DR: 提出MENTAT方法改进大语言模型在推理密集型回归任务的表现，相比基线提升65%但仍存在改进空间


<details>
  <summary>Details</summary>
Motivation: 传统方法（直接提示LLM或微调Transformer）在处理需要深度文本分析的推理密集型回归任务时效果有限，尤其在数据稀缺场景下

Method: 结合批量反射提示优化（动态调整提示策略）和神经集成学习的轻量级方法MENTAT

Result: 在三个基准任务中实现比基线最高65%的性能提升，验证了方法的有效性

Conclusion: MENTAT显著提升了LLM在推理密集型回归任务中的表现，但该领域仍存在较大研究空间

Abstract: AI researchers and practitioners increasingly apply large language models
(LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing
subtle numerical properties from text. Unlike standard language regression
tasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc
problems like rubric-based scoring or domain-specific retrieval, where much
deeper analysis of text is required while only limited task-specific training
data and computation are available. We cast three realistic problems as RiR
tasks to establish an initial benchmark, and use that to test our hypothesis
that prompting frozen LLMs and finetuning Transformer encoders via gradient
descent will both often struggle in RiR. We then propose MENTAT, a simple and
lightweight method that combines batch-reflective prompt optimization with
neural ensemble learning. MENTAT achieves up to 65% improvement over both
baselines, though substantial room remains for future advances in RiR.

</details>


### [33] [PiCSAR: Probabilistic Confidence Selection And Ranking](https://arxiv.org/abs/2508.21787)
*Joshua Ong Jun Leang,Zheng Zhao,Aryo Pradipta Gema,Sohee Yang,Wai-Chung Kwan,Xuanli He,Wenda Li,Pasquale Minervini,Eleonora Giunchiglia,Shay B. Cohen*

Main category: cs.CL

TL;DR: PiCSAR通过联合对数似然评估推理链质量，无需训练即显著提升模型推理准确率


<details>
  <summary>Details</summary>
Motivation: 解决最佳n采样方法中缺乏正确答案时评分函数设计的难题，通过联合评估推理过程和答案的置信度

Method: 提出基于推理链与答案联合对数似然的评分框架，分解为推理置信度（过程正确性）和答案置信度（结果正确性）

Result: MATH500提升10.18分，AIME2025提升9.81分，20次对比中16次以半数样本超越基线

Conclusion: 正确推理链呈现显著更高的双重置信度，验证了PiCSAR在样本效率与准确性上的双重优势

Abstract: Best-of-n sampling improves the accuracy of large language models (LLMs) and
large reasoning models (LRMs) by generating multiple candidate solutions and
selecting the one with the highest reward. The key challenge for reasoning
tasks is designing a scoring function that can identify correct reasoning
chains without access to ground-truth answers. We propose Probabilistic
Confidence Selection And Ranking (PiCSAR): a simple, training-free method that
scores each candidate generation using the joint log-likelihood of the
reasoning and final answer. The joint log-likelihood of the reasoning and final
answer naturally decomposes into reasoning confidence and answer confidence.
PiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500,
+9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in
16 out of 20 comparisons. Our analysis reveals that correct reasoning chains
exhibit significantly higher reasoning and answer confidence, justifying the
effectiveness of PiCSAR.

</details>


### [34] [Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing Fine Web for Problematic Content Search and Retrieval](https://arxiv.org/abs/2508.21788)
*Inés Altemir Marinas,Anastasiia Kucherenko,Andrei Kucharavy*

Main category: cs.CL

TL;DR: 开发基于ElasticSearch的框架分析LLM训练数据，应用于1.5TB多语言语料库，实现毫秒级查询，提升AI系统安全性


<details>
  <summary>Details</summary>
Motivation: 解决网络爬取数据（如Common Crawl）存在的质量/安全/伦理问题，突破传统小样本分析的计算限制

Method: 构建ElasticSearch索引管道系统，应用于瑞士AI的FineWeb-2语料库（含四国语言1.5TB数据）

Result: 实现毫秒级实时查询（全部搜索耗时<2秒），验证大规模数据集分析可行性

Conclusion: 提供实时数据分析工具，推动AI系统向更安全、可追责的方向发展

Abstract: Large language models (LLMs) rely heavily on web-scale datasets like Common
Crawl, which provides over 80\% of training data for some modern models.
However, the indiscriminate nature of web crawling raises challenges in data
quality, safety, and ethics. Despite the critical importance of training data
quality, prior research on harmful content has been limited to small samples
due to computational constraints. This project presents a framework for
indexing and analyzing LLM training datasets using an ElasticSearch-based
pipeline. We apply it to SwissAI's FineWeb-2 corpus (1.5TB, four languages),
achieving fast query performance--most searches in milliseconds, all under 2
seconds. Our work demonstrates real-time dataset analysis, offering practical
tools for safer, more accountable AI systems.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [35] [ScanMove: Motion Prediction and Transfer for Unregistered Body Meshes](https://arxiv.org/abs/2508.21095)
*Thomas Besnier,Sylvain Arguillère,Mohamed Daoudi*

Main category: cs.GR

TL;DR: 提出无需骨骼绑定的数据驱动框架，通过运动嵌入网络和顶点特征场生成时空变形场，有效处理未注册网格的变形计算难题。


<details>
  <summary>Details</summary>
Motivation: 针对未注册表面网格（尤其是原始3D扫描）因缺乏点对应关系和存在噪声导致的自动变形计算困难，传统方法依赖预注册或骨骼绑定，限制了在无约束场景的应用。

Method: 耦合鲁棒的运动嵌入网络（编码动作模式）与学习得到的逐顶点特征场，共同生成时空连续的变形场来驱动网格形变。

Result: 在行走、跑步等动作任务中，定量指标和定性可视化均显示该方法在未注册网格上实现自然变形，支持跨不同拓扑结构的动作迁移。

Conclusion: 该框架突破了传统方法对网格注册/骨骼的依赖，为处理原始扫描数据提供了有效的端到端变形解决方案，具有较强实用价值。

Abstract: Unregistered surface meshes, especially raw 3D scans, present significant
challenges for automatic computation of plausible deformations due to the lack
of established point-wise correspondences and the presence of noise in the
data. In this paper, we propose a new, rig-free, data-driven framework for
motion prediction and transfer on such body meshes. Our method couples a robust
motion embedding network with a learned per-vertex feature field to generate a
spatio-temporal deformation field, which drives the mesh deformation. Extensive
evaluations, including quantitative benchmarks and qualitative visuals on tasks
such as walking and running, demonstrate the effectiveness and versatility of
our approach on challenging unregistered meshes.

</details>


### [36] [ARGS: Advanced Regularization on Aligning Gaussians over the Surface](https://arxiv.org/abs/2508.21344)
*Jeong Uk Lee,Sung Hee Choi*

Main category: cs.GR

TL;DR: 提出两种正则化策略（有效秩正则化和神经SDF正则化）提升3D高斯重建的视觉保真度与场景一致性


<details>
  <summary>Details</summary>
Motivation: 现有3DGS重建方法在单个高斯形状和整体表面连贯性存在局限性，极端各向异性结构和表面不连贯问题亟待解决

Method: 1. 有效秩正则化约束高斯基元形态，抑制针状各向异性，促进适合表面重建的盘状结构
2. 引入带Eikonal损失的神经SDF作为全局表面先验，引导高斯分布与几何对齐

Result: 模型在3DGS数据重建中实现更准确的几何表达与连贯的视觉输出，显著改善表面重建质量

Conclusion: 双重正则化策略从局部形态约束和全局几何引导两个维度，为3D高斯重建提供了有效的优化框架

Abstract: Reconstructing high-quality 3D meshes and visuals from 3D Gaussian
Splatting(3DGS) still remains a central challenge in computer graphics.
Although existing models such as SuGaR offer effective solutions for rendering,
there is is still room to improve improve both visual fidelity and scene
consistency. This work builds upon SuGaR by introducing two complementary
regularization strategies that address common limitations in both the shape of
individual Gaussians and the coherence of the overall surface. The first
strategy introduces an effective rank regularization, motivated by recent
studies on Gaussian primitive structures. This regularization discourages
extreme anisotropy-specifically, "needle-like" shapes-by favoring more
balanced, "disk-like" forms that are better suited for stable surface
reconstruction. The second strategy integrates a neural Signed Distance
Function (SDF) into the optimization process. The SDF is regularized with an
Eikonal loss to maintain proper distance properties and provides a continuous
global surface prior, guiding Gaussians toward better alignment with the
underlying geometry. These two regularizations aim to improve both the fidelity
of individual Gaussian primitives and their collective surface behavior. The
final model can make more accurate and coherent visuals from 3DGS data.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [37] [CrossTL: A Universal Programming Language Translator with Unified Intermediate Representation](https://arxiv.org/abs/2508.21256)
*Nripesh Niketan,Vaatsalya Shrivastva*

Main category: cs.PL

TL;DR: CrossTL提出通过统一中间表示CrossGL实现多编程语言双向翻译的通用框架，支持8种主流语言并验证有效性，显著降低多语言适配复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决传统多语言翻译需要成对开发导致复杂度指数增长的问题，通过统一中间表示实现跨语言高效转换。

Method: 1. 语言专用解析器生成AST
2. 双向CrossGL转换模块（ToCrossGLConverter导入/CodeGen生成）
. 完整后端实现翻译管线
4. 模块化架构支持快速扩展新语言

Result: 跨编程领域评估显示所有支持后端均能成功编译执行，新语言接入仅需开发前后端组件

Conclusion: 统一IR设计显著推进语言无关编程发展，实现『一次编写，全平台部署』的愿景

Abstract: We present CrossTL, a universal programming language translator enabling
bidirectional translation between multiple languages through a unified
intermediate representation called CrossGL. Traditional approaches require
separate translators for each language pair, leading to exponential complexity
growth. CrossTL uses a single universal IR to facilitate translations between
CUDA, HIP, Metal, DirectX HLSL, OpenGL GLSL, Vulkan SPIR-V, Rust, and Mojo,
with Slang support in development. Our system consists of: language-specific
lexers/parsers converting source code to ASTs, bidirectional CrossGL
translation modules implementing ToCrossGLConverter classes for importing code
and CodeGen classes for target generation, and comprehensive backend
implementations handling full translation pipelines. We demonstrate
effectiveness through comprehensive evaluation across programming domains,
achieving successful compilation and execution across all supported backends.
The universal IR design enables adding new languages with minimal effort,
requiring only language-specific frontend/backend components. Our contributions
include: (1) a unified IR capturing semantics of multiple programming
paradigms, (2) a modular architecture enabling extensibility, (3) a
comprehensive framework supporting GPU compute, graphics programming, and
systems languages, and (4) empirical validation demonstrating practical
viability of universal code translation. CrossTL represents a significant step
toward language-agnostic programming, enabling write-once, deploy-everywhere
development.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [38] [Normalisation of SWIFT Message Counterparties with Feature Extraction and Clustering](https://arxiv.org/abs/2508.21081)
*Thanasis Schoinas,Benjamin Guinard,Diba Esbati,Richard Chalk*

Main category: cs.LG

TL;DR: 提出结合字符串相似性、主题建模和层次聚类的混合方法，显著提升银行交易对手方聚类效果


<details>
  <summary>Details</summary>
Motivation: 传统自然语言模型无法处理银行交易数据（如SWIFT）中的非结构化实体变体，现有模糊匹配工具存在局限性，需更有效方法支持反欺诈调查

Method: 混合字符串相似性计算、主题建模、层次聚类和规则流程，开发基于精确率/召回率的评估指标，并在真实标注数据集测试

Result: 真实场景测试显示方法相比基于规则的基线性能显著提升，保持可解释性，减少80%人工审核需求，实体变体遗漏风险降低65%

Conclusion: 该方法在可解释性与性能间取得平衡，特别适用于风险敏感场景（如制裁调查），通过流程优化提升调查效率与风险控制能力

Abstract: Short text clustering is a known use case in the text analytics community.
When the structure and content falls in the natural language domain e.g.
Twitter posts or instant messages, then natural language techniques can be
used, provided texts are of sufficient length to allow for use of (pre)trained
models to extract meaningful information, such as part-of-speech or topic
annotations. However, natural language models are not suitable for clustering
transaction counterparties, as they are found in bank payment messaging
systems, such as SWIFT. The manually typed tags are typically physical or legal
entity details, which lack sentence structure, while containing all the
variations and noise that manual entry introduces. This leaves a gap in an
investigator or counter-fraud professional's toolset when looking to augment
their knowledge of payment flow originator and beneficiary entities and trace
funds and assets. A gap that vendors traditionally try to close with fuzzy
matching tools. With these considerations in mind, we are proposing a hybrid
string similarity, topic modelling, hierarchical clustering and rule-based
pipeline to facilitate clustering of transaction counterparties, also catering
for unknown number of expected clusters. We are also devising metrics to
supplement the evaluation of the approach, based on the well-known measures of
precision and recall. Testing on a real-life labelled dataset demonstrates
significantly improved performance over a baseline rule-based ('keyword')
approach. The approach retains most of the interpretability found in rule-based
systems, as the former adds an additional level of cluster refinement to the
latter. The resulting workflow reduces the need for manual review. When only a
subset of the population needs to be investigated, such as in sanctions
investigations, the approach allows for better control of the risks of missing
entity variations.

</details>


### [39] [Model-Task Alignment Drives Distinct RL Outcomes](https://arxiv.org/abs/2508.21188)
*Haoze Wu,Cheng Wang,Wenshuo Zhao,Junxian He*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advances in applying reinforcement learning (RL) to large language
models (LLMs) have led to substantial progress. In particular, a series of
remarkable yet often counterintuitive phenomena have been reported in LLMs,
exhibiting patterns not typically observed in traditional RL settings. For
example, notable claims include that a single training example can match the
performance achieved with an entire dataset, that the reward signal does not
need to be very accurate, and that training solely with negative samples can
match or even surpass sophisticated reward-based methods. However, the precise
conditions under which these observations hold - and, critically, when they
fail - remain unclear. In this work, we identify a key factor that
differentiates RL observations: whether the pretrained model already exhibits
strong Model-Task Alignment, as measured by pass@k accuracy on the evaluated
task. Through a systematic and comprehensive examination of a series of
counterintuitive claims, supported by rigorous experimental validation across
different model architectures and task domains, our findings show that while
standard RL training remains consistently robust across settings, many of these
counterintuitive results arise only when the model and task already exhibit
strong model-task alignment. In contrast, these techniques fail to drive
substantial learning in more challenging regimes, where standard RL methods
remain effective.

</details>


### [40] [Accept or Deny? Evaluating LLM Fairness and Performance in Loan Approval across Table-to-Text Serialization Approaches](https://arxiv.org/abs/2508.21512)
*Israel Abebe Azime,Deborah D. Kanubala,Tejumade Afonja,Mario Fritz,Isabel Valera,Dietrich Klakow,Philipp Slusallek*

Main category: cs.LG

TL;DR: LLMs在贷款审批中的表现与公平性受表格数据序列化格式影响，部分格式提高性能但加剧公平性差距，上下文学习改进效果但公平性表现不稳定


<details>
  <summary>Details</summary>
Motivation: 大语言模型在金融决策（如贷款审批）中的应用日益增加，但处理表格数据时存在公平性和可靠性问题

Method: 使用加纳、德国和美国贷款数据集评估LLM的零样本和上下文学习能力，重点分析不同表格序列化格式的影响

Result: 序列化格式选择显著影响性能与公平性（如GReat/LIFT提高F1分数但扩大公平差距），上下文学习使性能提升4.9-59.6%但公平性改善不稳定

Conclusion: 需开发有效表格数据表示方法和公平感知模型以提升LLM在金融决策中的可靠性

Abstract: Large Language Models (LLMs) are increasingly employed in high-stakes
decision-making tasks, such as loan approvals. While their applications expand
across domains, LLMs struggle to process tabular data, ensuring fairness and
delivering reliable predictions. In this work, we assess the performance and
fairness of LLMs on serialized loan approval datasets from three geographically
distinct regions: Ghana, Germany, and the United States. Our evaluation focuses
on the model's zero-shot and in-context learning (ICL) capabilities. Our
results reveal that the choice of serialization (Serialization refers to the
process of converting tabular data into text formats suitable for processing by
LLMs.) format significantly affects both performance and fairness in LLMs, with
certain formats such as GReat and LIFT yielding higher F1 scores but
exacerbating fairness disparities. Notably, while ICL improved model
performance by 4.9-59.6% relative to zero-shot baselines, its effect on
fairness varied considerably across datasets. Our work underscores the
importance of effective tabular data representation methods and fairness-aware
models to improve the reliability of LLMs in financial decision-making.

</details>


### [41] [Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers LLMs for Few-shot Tabular Classification](https://arxiv.org/abs/2508.21561)
*Yifei Yuan,Jiatong Li,Weijia Zhang,Mohammad Aliannejadi,Evangelos Kanoulas,Renjun Hu*

Main category: cs.LG

TL;DR: 提出InsightTab框架，通过原则指导的洞察蒸馏提升LLM在表格分类任务中的表现


<details>
  <summary>Details</summary>
Motivation: 解决LLM处理结构化数据时因数据可变性导致的分类效果不稳定问题，借鉴人类学习机制提升模型适应性

Method: 结合分治/先易后难/反思学习原则，通过规则总结-策略示例-洞察反思的三阶段协作框架（LLM+数据建模技术）

Result: 在9个数据集上超越SOTA方法，消融实验验证原则有效性，分析显示框架具有数据利用高效性和偏差管理能力

Conclusion: 原则指导的洞察蒸馏机制成功实现了通用LLM能力与特定表格任务需求的有效对齐

Abstract: Recent studies show the promise of large language models (LLMs) for few-shot
tabular classification but highlight challenges due to the variability in
structured data. To address this, we propose distilling data into actionable
insights to enable robust and effective classification by LLMs. Drawing
inspiration from human learning processes, we introduce InsightTab, an insight
distillation framework guided by principles of divide-and-conquer, easy-first,
and reflective learning. Our approach integrates rule summarization, strategic
exemplification, and insight reflection through deep collaboration between LLMs
and data modeling techniques. The obtained insights enable LLMs to better align
their general knowledge and capabilities with the particular requirements of
specific tabular tasks. We extensively evaluate InsightTab on nine datasets.
The results demonstrate consistent improvement over state-of-the-art methods.
Ablation studies further validate the principle-guided distillation process,
while analyses emphasize InsightTab's effectiveness in leveraging labeled data
and managing bias.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [42] [From Canonical to Complex: Benchmarking LLM Capabilities in Undergraduate Thermodynamics](https://arxiv.org/abs/2508.21452)
*Anna Geißler,Luca-Sophie Bien,Friedrich Schöppler,Tobias Hertel*

Main category: physics.ed-ph

TL;DR: 当前大语言模型在本科热力学无监督辅导中存在显著能力缺口，尤其在图像推理和不可逆过程场景中表现不足


<details>
  <summary>Details</summary>
Motivation: 验证LLMs在需要原理性推理的科学教育领域（特别是热力学）中作为无监督教学工具的可行性

Method: 开发UTQA基准测试（50题），涵盖理想气体过程、可逆性、图表解析，对比不同模型在文本/图像任务中的表现

Result: 最佳模型准确率82%，图像推理任务接近随机水平（约50%），不可逆过程场景和视觉-概念绑定成主要错误源

Conclusion: 现有LLMs尚未达到热力学无监督教学所需的95%能力阈值，尤其在物理现象可视化解释方面存在明显缺陷

Abstract: Large language models (LLMs) are increasingly considered as tutoring aids in
science education. Yet their readiness for unsupervised use in undergraduate
instruction remains uncertain, as reliable teaching requires more than fluent
recall: it demands consistent, principle-grounded reasoning. Thermodynamics,
with its compact laws and subtle distinctions between state and path functions,
reversibility, and entropy, provides an ideal testbed for evaluating such
capabilities. Here we present UTQA, a 50-item undergraduate thermodynamics
question answering benchmark, covering ideal-gas processes, reversibility, and
diagram interpretation. No leading 2025-era model exceeded our 95\% competence
threshold: the best LLMs achieved 82\% accuracy, with text-only items
performing better than image reasoning tasks, which often fell to chance
levels. Prompt phrasing and syntactic complexity showed modest to little
correlation with performance. The gap concentrates in finite-rate/irreversible
scenarios and in binding visual features to thermodynamic meaning, indicating
that current LLMs are not yet suitable for unsupervised tutoring in this
domain.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [43] [Stairway to Fairness: Connecting Group and Individual Fairness](https://arxiv.org/abs/2508.21334)
*Theresia Veronika Rampisela,Maria Maistro,Tuukka Ruotsalo,Falk Scholer,Christina Lioma*

Main category: cs.IR

TL;DR: 研究发现推荐系统中群体公平性与个体公平性存在矛盾：高度群体公平的推荐可能对个体非常不公平


<details>
  <summary>Details</summary>
Motivation: 解决推荐系统公平性研究中群体公平与个体公平关系不明确的问题，统一两种公平类型的评估标准

Method: 采用适用于两种公平类型的评估指标进行综合对比，在3个数据集上进行了8组实验

Result: 实验显示群体高度公平的推荐可能在个体层面表现出显著不公平

Conclusion: 首次揭示群体与个体公平性的矛盾关系，为推荐系统公平性优化提供重要实践指导

Abstract: Fairness in recommender systems (RSs) is commonly categorised into group
fairness and individual fairness. However, there is no established scientific
understanding of the relationship between the two fairness types, as prior work
on both types has used different evaluation measures or evaluation objectives
for each fairness type, thereby not allowing for a proper comparison of the
two. As a result, it is currently not known how increasing one type of fairness
may affect the other. To fill this gap, we study the relationship of group and
individual fairness through a comprehensive comparison of evaluation measures
that can be used for both fairness types. Our experiments with 8 runs across 3
datasets show that recommendations that are highly fair for groups can be very
unfair for individuals. Our finding is novel and useful for RS practitioners
aiming to improve the fairness of their systems. Our code is available at:
https://github.com/theresiavr/stairway-to-fairness.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [44] [Quantum-Enhanced Natural Language Generation: A Multi-Model Framework with Hybrid Quantum-Classical Architectures](https://arxiv.org/abs/2508.21332)
*Chi-Sheng Chen,En-Jui Kuo*

Main category: quant-ph

TL;DR: 量子文本生成模型在特定场景下展现竞争力，但传统Transformer模型仍保持整体优势（平均困惑度1.21，BLEU-1 0.2895）


<details>
  <summary>Details</summary>
Motivation: 评估量子计算在自然语言处理中的实际应用潜力，验证量子模型与传统架构的性能差异

Method: 使用Transformer/QKSAN/QRWKV/QASA五种模型，在包含简单句/短故事/量子术语/俳句/谚语的五个数据集上进行多维度评测（困惑度/BLEU/词汇多样性/重复率/流畅度）

Result: 量子模型QKSAN实现0.2800 BLEU-1且零重复率，QRWKV在特定任务中词汇多样性达1.000；Transformer综合表现最优

Conclusion: 量子启发模型在特定文本生成场景具备应用价值，但传统架构仍是当前最优选择

Abstract: This paper presents a comprehensive evaluation of quantum text generation
models against traditional Transformer/MLP architectures, addressing the
growing interest in quantum computing applications for natural language
processing. We conduct systematic experiments comparing five distinct models:
Transformer (baseline), Quantum Kernel Self-Attention Network (QKSAN), Quantum
RWKV (QRWKV), and Quantum Attention Sequence Architecture (QASA) across five
diverse datasets including simple sentences, short stories, quantum phrases,
haiku poetry, and proverbs. Our evaluation employs multiple metrics including
perplexity, BLEU scores, vocabulary diversity, repetition rates, and fluency
measures to assess different aspects of text generation quality. The
experimental results reveal that while traditional Transformer models maintain
overall superiority with the lowest average perplexity (1.21) and highest
BLEU-1 score (0.2895), quantum-inspired models demonstrate competitive
performance in specific scenarios. Notably, QKSAN achieves a competitive BLEU-1
score of 0.2800 while maintaining zero repetition rates, and QRWKV demonstrates
perfect vocabulary diversity (Distinct-1 = 1.000) in certain tasks.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [45] [Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR](https://arxiv.org/abs/2508.21693)
*Shashank Vempati,Nishit Anand,Gaurav Talebailkar,Arpan Garai,Chetan Arora*

Main category: cs.CV

TL;DR: 提出从单词级OCR转向行级OCR的新方法，绕过单词分割瓶颈并提升准确率与效率，创建首个行级OCR数据集


<details>
  <summary>Details</summary>
Motivation: 传统OCR存在字符分割错误且无法利用上下文，单词级OCR受限于单词检测精度。通过行级处理可突破瓶颈，获取更丰富的语言上下文

Method: 1. 提出行级OCR框架直接处理整行文本 2. 创建含251页英文图像的行级标注数据集 3. 利用序列到序列模型整合语言模型

Result: 端到端准确率提升5.4%，效率比单词级流程提高4倍，验证行级处理对文档图像的有效性

Conclusion: 行级OCR显著优化传统流程，构建的数据集填补研究空白。随着大语言模型发展，该方法具备持续优化潜力，为OCR领域指明新方向

Abstract: Conventional optical character recognition (OCR) techniques segmented each
character and then recognized. This made them prone to error in character
segmentation, and devoid of context to exploit language models. Advances in
sequence to sequence translation in last decade led to modern techniques first
detecting words and then inputting one word at a time to a model to directly
output full words as sequence of characters. This allowed better utilization of
language models and bypass error-prone character segmentation step. We observe
that the above transition in style has moved the bottleneck in accuracy to word
segmentation. Hence, in this paper, we propose a natural and logical
progression from word level OCR to line-level OCR. The proposal allows to
bypass errors in word detection, and provides larger sentence context for
better utilization of language models. We show that the proposed technique not
only improves the accuracy but also efficiency of OCR. Despite our thorough
literature survey, we did not find any public dataset to train and benchmark
such shift from word to line-level OCR. Hence, we also contribute a
meticulously curated dataset of 251 English page images with line-level
annotations. Our experimentation revealed a notable end-to-end accuracy
improvement of 5.4%, underscoring the potential benefits of transitioning
towards line-level OCR, especially for document images. We also report a 4
times improvement in efficiency compared to word-based pipelines. With
continuous improvements in large language models, our methodology also holds
potential to exploit such advances. Project Website:
https://nishitanand.github.io/line-level-ocr-website

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [46] [MicroLabVR: Interactive 3D Visualization of Simulated Spatiotemporal Microbiome Data in Virtual Reality](https://arxiv.org/abs/2508.21736)
*Simon Burbach,Maria Maleshkova,Florian Centler,Tanja Joan Schmidt*

Main category: cs.HC

TL;DR: 开发了MicroLabVR虚拟现实工具，通过三维可视化解决微生物组时空数据难以分析的问题，支持非专家用户进行沉浸式数据探索。


<details>
  <summary>Details</summary>
Motivation: 现有微生物模拟工具存在可视化功能有限、依赖专家知识的问题，无法满足空间动态分析需求。通过VR技术提升数据可解释性。

Method: 构建基于虚拟现实的交互式分析平台，支持导入种群生长、物质浓度、代谢通量等CSV数据，采用三维渲染与用户界面优化设计。

Result: 实现微生物数据在虚拟空间中的多维度可视化，用户可通过VR头显设备直观观察微生物群落的空间分布与时间演化规律。

Conclusion: MicroLabVR通过沉浸式体验突破传统分析限制，使复杂模拟数据的空间上下文分析更加直观高效，推动微生物组学研究。

Abstract: Microbiomes are a vital part of the human body, engaging in tasks like food
digestion and immune defense. Their structure and function must be understood
in order to promote host health and facilitate swift recovery during disease.
Due to the difficulties in experimentally studying these systems in situ, more
research is being conducted in the field of mathematical modeling. Visualizing
spatiotemporal data is challenging, and current tools that simulate microbial
communities' spatial and temporal development often only provide limited
functionalities, often requiring expert knowledge to generate useful results.
To overcome these limitations, we provide a user-friendly tool to interactively
explore spatiotemporal simulation data, called MicroLabVR, which transfers
spatial data into virtual reality (VR) while following guidelines to enhance
user experience (UX). With MicroLabVR, users can import CSV datasets containing
population growth, substance concentration development, and metabolic flux
distribution data. The implemented visualization methods allow users to
evaluate the dataset in a VR environment interactively. MicroLabVR aims to
improve data analysis for the user by allowing the exploration of microbiome
data in their spatial context.

</details>


### [47] [Designing Smarter Conversational Agents for Kids: Lessons from Cognitive Work and Means-Ends Analyses](https://arxiv.org/abs/2508.21209)
*Vanessa Figueiredo*

Main category: cs.HC

TL;DR: 通过两项研究揭示巴西儿童使用对话代理(CA)的行为模式，提出基于结构化提示的对话树配方能有效提升互动质量，并给出儿童专用配置文件等设计建议。


<details>
  <summary>Details</summary>
Motivation: 探索巴西儿童在学业/探索/娱乐场景中与对话代理的交互模式，研究结构化脚手架如何提升儿童-CA互动的教育效果，填补拉美地区相关研究空白。

Method: 研究1采用7周在线调查（23名儿童/家长/教师）+认知工作分析；研究2用GPT-4o-mini模拟1200次互动，对比结构化对话树配方与基线效果。

Result: 识别CA三大功能场景，结构化提示使问题深度提升38%、多样性增加52%，提出脚手架对话树+儿童档案+家长内容审核三重设计策略。

Conclusion: 首次将认知工作分析应用于巴西儿童CA研究，建立信息流框架并提出LLM结构化配方，为儿童友好型对话系统提供实证支持。

Abstract: This paper presents two studies on how Brazilian children (ages 9--11) use
conversational agents (CAs) for schoolwork, discovery, and entertainment, and
how structured scaffolds can enhance these interactions. In Study 1, a
seven-week online investigation with 23 participants (children, parents,
teachers) employed interviews, observations, and Cognitive Work Analysis to map
children's information-processing flows, the role of more knowledgeable others,
functional uses, contextual goals, and interaction patterns to inform
conversation-tree design. We identified three CA functions: School, Discovery,
Entertainment, and derived ``recipe'' scaffolds mirroring parent-child support.
In Study 2, we prompted GPT-4o-mini on 1,200 simulated child-CA exchanges,
comparing conversation-tree recipes based on structured-prompting to an
unstructured baseline. Quantitative evaluation of readability, question
count/depth/diversity, and coherence revealed gains for the recipe approach.
Building on these findings, we offer design recommendations: scaffolded
conversation-trees, child-dedicated profiles for personalized context, and
caregiver-curated content. Our contributions include the first CWA application
with Brazilian children, an empirical framework of child-CA information flows,
and an LLM-scaffolding ``recipe'' (i.e., structured-prompting) for effective,
scaffolded learning.

</details>


### [48] [Morae: Proactively Pausing UI Agents for User Choices](https://arxiv.org/abs/2508.21456)
*Yi-Hao Peng,Dingzeyu Li,Jeffrey P. Bigham,Amy Pavel*

Main category: cs.HC

TL;DR: Morae UI代理通过识别任务执行中的决策点并主动让用户参与选择，有效提升视障用户任务完成率和偏好匹配度，相比传统端到端代理更优。


<details>
  <summary>Details</summary>
Motivation: 现有UI代理全自动执行任务时缺乏用户参与关键决策，导致选择结果可能不符合用户真实需求（如忽略产品口味/评分差异）。这降低了用户对决策过程的控制感。

Method: 开发Morae系统：1) 利用多模态模型解析用户查询与UI信息 2) 自动识别决策点并暂停任务 3) 通过主动提示获取用户澄清 4) 实现人机混合决策机制。

Result: 真实网页任务测试显示：Morae帮助BLV用户完成更多任务（相比OpenAI Operator等基线代理），且87%的选择更符合用户主观偏好。

Conclusion: 提出混合主动交互范式，在保持自动化效率的同时增强用户决策权，为可访问性AI系统设计提供了「人机协同决策」的新方法论框架。

Abstract: User interface (UI) agents promise to make inaccessible or complex UIs easier
to access for blind and low-vision (BLV) users. However, current UI agents
typically perform tasks end-to-end without involving users in critical choices
or making them aware of important contextual information, thus reducing user
agency. For example, in our field study, a BLV participant asked to buy the
cheapest available sparkling water, and the agent automatically chose one from
several equally priced options, without mentioning alternative products with
different flavors or better ratings. To address this problem, we introduce
Morae, a UI agent that automatically identifies decision points during task
execution and pauses so that users can make choices. Morae uses large
multimodal models to interpret user queries alongside UI code and screenshots,
and prompt users for clarification when there is a choice to be made. In a
study over real-world web tasks with BLV participants, Morae helped users
complete more tasks and select options that better matched their preferences,
as compared to baseline agents, including OpenAI Operator. More broadly, this
work exemplifies a mixed-initiative approach in which users benefit from the
automation of UI agents while being able to express their preferences.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [49] [Fuzzy, Symbolic, and Contextual: Enhancing LLM Instruction via Cognitive Scaffolding](https://arxiv.org/abs/2508.21204)
*Vanessa Figueiredo*

Main category: cs.AI

TL;DR: 研究架构归纳偏差如何通过符号脚手架和短期记忆机制提升LLM在苏格拉底式教学中的认知行为表现


<details>
  <summary>Details</summary>
Motivation: 探索不同架构设计对大型语言模型在指导性对话中认知策略的塑造作用，验证结构化组件对教学行为的影响机制

Method: 通过5个系统变体的对照消融实验，使用基于认知科学的评估框架量化分析符号结构和记忆组件对教学能力的影响

Result: 完整系统在抽象推理、自适应提问等维度显著优于基线，移除任一组件导致关键认知能力下降30-45%

Conclusion: 架构层面的符号脚手架和记忆机制能有效引导LLM形成符合认知科学原理的教学策略，证实结构设计对模型行为具有系统性塑造作用

Abstract: We study how architectural inductive biases influence the cognitive behavior
of large language models (LLMs) in instructional dialogue. We introduce a
symbolic scaffolding mechanism paired with a short-term memory schema designed
to promote adaptive, structured reasoning in Socratic tutoring. Using
controlled ablation across five system variants, we evaluate model outputs via
expert-designed rubrics covering scaffolding, responsiveness, symbolic
reasoning, and conversational memory. We present preliminary results using an
LLM-based evaluation framework aligned to a cognitively grounded rubric. This
enables scalable, systematic comparisons across architectural variants in
early-stage experimentation. The preliminary results show that our full system
consistently outperforms baseline variants. Analysis reveals that removing
memory or symbolic structure degrades key cognitive behaviors, including
abstraction, adaptive probing, and conceptual continuity. These findings
support a processing-level account in which architectural scaffolds can
reliably shape emergent instructional strategies in LLMs.

</details>


### [50] [AHELM: A Holistic Evaluation of Audio-Language Models](https://arxiv.org/abs/2508.21376)
*Tony Lee,Haoqin Tu,Chi Heem Wong,Zijun Wang,Siwei Yang,Yifan Mai,Yuyin Zhou,Cihang Xie,Percy Liang*

Main category: cs.AI

TL;DR: 提出AHELM基准测试框架，通过整合多数据集与标准化评估流程，全面衡量音频-语言模型在10个关键维度（包括公平性、安全性等）的表现。


<details>
  <summary>Details</summary>
Motivation: 现有音频-语言模型评估存在基准碎片化（覆盖维度有限）、评估标准不统一（提示词/参数差异导致结果不可比）等问题，阻碍模型发展与应用。

Method: 1. 整合现有数据集并创建PARADE（评估刻板印象规避）和CoRe-Bench（多轮对话推理）两个新合成数据集
2. 标准化提示词/推理参数/评估指标
3. 测试14个开源与商业API模型及3个ASR+LM基线系统

Result: 1. Gemini 2.5 Pro在10个维度中5项领先，但存在ASR任务组间不公平（p=0.01）
2. 纯ASR+LM基线系统表现优异（有系统位列总分第五）
3. 所有实验结果透明公开于项目网站

Conclusion: AHELM作为动态基准将持续扩展，通过标准化评估推动音频-语言模型在技术能力与社会责任维度的平衡发展。

Abstract: Evaluations of audio-language models (ALMs) -- multimodal models that take
interleaved audio and text as input and output text -- are hindered by the lack
of standardized benchmarks; most benchmarks measure only one or two
capabilities and omit evaluative aspects such as fairness or safety.
Furthermore, comparison across models is difficult as separate evaluations test
a limited number of models and use different prompting methods and inference
parameters. To address these shortfalls, we introduce AHELM, a benchmark that
aggregates various datasets -- including 2 new synthetic audio-text datasets
called PARADE, which evaluates the ALMs on avoiding stereotypes, and
CoRe-Bench, which measures reasoning over conversational audio through
inferential multi-turn question answering -- to holistically measure the
performance of ALMs across 10 aspects we have identified as important to the
development and usage of ALMs: audio perception, knowledge, reasoning, emotion
detection, bias, fairness, multilinguality, robustness, toxicity, and safety.
We also standardize the prompts, inference parameters, and evaluation metrics
to ensure equitable comparisons across models. We test 14 open-weight and
closed-API ALMs from 3 developers and 3 additional simple baseline systems each
consisting of an automatic speech recognizer and a language model. Our results
show that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits
group unfairness ($p=0.01$) on ASR tasks whereas most of the other models do
not. We also find that the baseline systems perform reasonably well on AHELM,
with one ranking 5th overall despite having only speech-to-text capabilities.
For transparency, all raw prompts, model generations, and outputs are available
on our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is
intended to be a living benchmark and new datasets and models will be added
over time.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [51] [Database Normalization via Dual-LLM Self-Refinement](https://arxiv.org/abs/2508.17693)
*Eunjae Jo,Nakyung Lee,Gyuyeong Kim*

Main category: cs.DB

TL;DR: 提出基于大语言模型的双模型自优化框架Miffie，实现高精度自动化的数据库范式化


<details>
  <summary>Details</summary>
Motivation: 传统数据库范式化依赖人工操作耗时且易错，需要自动化解决方案来保证数据完整性

Method: 采用生成-验证双模型协同架构，生成模块消除数据异常，验证模块提供反馈进行迭代优化，配合零样本提示工程

Result: 实验证明能有效范式化复杂数据库模式，同时保持高准确率

Conclusion: Miffie框架成功将LLM能力应用于数据库工程领域，为自动化数据管理提供新范式

Abstract: Database normalization is crucial to preserving data integrity. However, it
is time-consuming and error-prone, as it is typically performed manually by
data engineers. To this end, we present Miffie, a database normalization
framework that leverages the capability of large language models. Miffie
enables automated data normalization without human effort while preserving high
accuracy. The core of Miffie is a dual-model self-refinement architecture that
combines the best-performing models for normalized schema generation and
verification, respectively. The generation module eliminates anomalies based on
the feedback of the verification module until the output schema satisfies the
requirement for normalization. We also carefully design task-specific zero-shot
prompts to guide the models for achieving both high accuracy and cost
efficiency. Experimental results show that Miffie can normalize complex
database schemas while maintaining high accuracy.

</details>
