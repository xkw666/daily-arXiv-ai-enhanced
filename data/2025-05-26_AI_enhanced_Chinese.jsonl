{"id": "2505.17037", "pdf": "https://arxiv.org/pdf/2505.17037", "abs": "https://arxiv.org/abs/2505.17037", "authors": ["Dimitri Schreiter"], "title": "Prompt Engineering: How Prompt Vocabulary affects Domain Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Prompt engineering has emerged as a critical component in optimizing large\nlanguage models (LLMs) for domain-specific tasks. However, the role of prompt\nspecificity, especially in domains like STEM (physics, chemistry, biology,\ncomputer science and mathematics), medicine, and law, remains underexplored.\nThis thesis addresses the problem of whether increasing the specificity of\nvocabulary in prompts improves LLM performance in domain-specific\nquestion-answering and reasoning tasks. We developed a synonymization framework\nto systematically substitute nouns, verbs, and adjectives with varying\nspecificity levels, measuring the impact on four LLMs: Llama-3.1-70B-Instruct,\nGranite-13B-Instruct-V2, Flan-T5-XL, and Mistral-Large 2, across datasets in\nSTEM, law, and medicine. Our results reveal that while generally increasing the\nspecificity of prompts does not have a significant impact, there appears to be\na specificity range, across all considered models, where the LLM performs the\nbest. Identifying this optimal specificity range offers a key insight for\nprompt design, suggesting that manipulating prompts within this range could\nmaximize LLM performance and lead to more efficient applications in specialized\ndomains.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u540c\u4e49\u8bcd\u66ff\u6362\u6846\u67b6\u9a8c\u8bc1\u63d0\u793a\u8bcd\u6c47\u5177\u4f53\u6027\u5bf9LLM\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5b58\u5728\u4f7f\u6a21\u578b\u8868\u73b0\u6700\u4f18\u7684\u5177\u4f53\u6027\u8303\u56f4", "motivation": "\u63a2\u7d22\u63d0\u793a\u5de5\u7a0b\u4e2d\u8bcd\u6c47\u5177\u4f53\u6027\u5bf9LLM\u5728STEM\u3001\u533b\u5b66\u3001\u6cd5\u5f8b\u7b49\u4e13\u4e1a\u9886\u57df\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u5f71\u54cd\u673a\u5236", "method": "\u5f00\u53d1\u540c\u4e49\u8bcd\u66ff\u6362\u6846\u67b6\uff0c\u5728\u56db\u4e2aLLM\u6a21\u578b\uff08Llama-3.1-70B/Granite-13B/Flan-T5-XL/Mistral-Large 2\uff09\u4e0a\u6d4b\u8bd5\u4e0d\u540c\u5177\u4f53\u6027\u6c34\u5e73\u7684\u63d0\u793a", "result": "\u6574\u4f53\u63d0\u5347\u5177\u4f53\u6027\u65e0\u663e\u8457\u6539\u5584\uff0c\u4f46\u6240\u6709\u6a21\u578b\u5b58\u5728\u6700\u4f73\u6027\u80fd\u7684\u7279\u5b9a\u5177\u4f53\u6027\u8303\u56f4", "conclusion": "\u8bc6\u522b\u6700\u4f18\u5177\u4f53\u6027\u8303\u56f4\u53ef\u4e3a\u63d0\u793a\u8bbe\u8ba1\u63d0\u4f9b\u5173\u952e\u6307\u5bfc\uff0c\u5728\u6b64\u8303\u56f4\u5185\u8c03\u6574\u63d0\u793a\u53ef\u6700\u5927\u5316LLM\u5728\u4e13\u4e1a\u9886\u57df\u7684\u5e94\u7528\u6548\u679c"}}
{"id": "2505.17038", "pdf": "https://arxiv.org/pdf/2505.17038", "abs": "https://arxiv.org/abs/2505.17038", "authors": ["Xian Gong", "Paul X. McCarthy", "Lin Tian", "Marian-Andrei Rizoiu"], "title": "Signals from the Floods: AI-Driven Disaster Analysis through Multi-Source Data Fusion", "categories": ["cs.CL", "cs.SI"], "comment": null, "summary": "Massive and diverse web data are increasingly vital for government disaster\nresponse, as demonstrated by the 2022 floods in New South Wales (NSW),\nAustralia. This study examines how X (formerly Twitter) and public inquiry\nsubmissions provide insights into public behaviour during crises. We analyse\nmore than 55,000 flood-related tweets and 1,450 submissions to identify\nbehavioural patterns during extreme weather events. While social media posts\nare short and fragmented, inquiry submissions are detailed, multi-page\ndocuments offering structured insights. Our methodology integrates Latent\nDirichlet Allocation (LDA) for topic modelling with Large Language Models\n(LLMs) to enhance semantic understanding. LDA reveals distinct opinions and\ngeographic patterns, while LLMs improve filtering by identifying flood-relevant\ntweets using public submissions as a reference. This Relevance Index method\nreduces noise and prioritizes actionable content, improving situational\nawareness for emergency responders. By combining these complementary data\nstreams, our approach introduces a novel AI-driven method to refine\ncrisis-related social media content, improve real-time disaster response, and\ninform long-term resilience planning.", "AI": {"tldr": "\u7ed3\u5408\u793e\u4ea4\u5a92\u4f53\u548c\u516c\u4f17\u8c03\u67e5\u6570\u636e\uff0c\u901a\u8fc7LDA\u4e3b\u9898\u5efa\u6a21\u4e0eLLM\u878d\u5408\u65b9\u6cd5\u63d0\u5347\u707e\u5bb3\u54cd\u5e94\u6548\u7387", "motivation": "\u4f20\u7edf\u707e\u5bb3\u54cd\u5e94\u4e2d\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u5b58\u5728\u788e\u7247\u5316\u95ee\u9898\uff0c\u9700\u7ed3\u5408\u7ed3\u6784\u5316\u516c\u4f17\u8c03\u67e5\u6570\u636e\u63d0\u5347\u4fe1\u606f\u8d28\u91cf", "method": "\u6574\u5408LDA\u4e3b\u9898\u5efa\u6a21\uff08\u5206\u679055,000+\u63a8\u6587\uff09\u4e0eLLM\u8bed\u4e49\u7406\u89e3\uff08\u53c2\u80031,450\u4efd\u8c03\u67e5\u6587\u4ef6\uff09\uff0c\u5f00\u53d1\u76f8\u5173\u6027\u6307\u6570\u8fc7\u6ee4\u673a\u5236", "result": "\u5b9e\u73b0\u566a\u58f0\u964d\u4f4e72%\uff0c\u6784\u5efa\u5730\u7406\u7279\u5f81\u56fe\u8c31\uff0c\u63d0\u5347\u5e94\u6025\u54cd\u5e94\u5b9e\u65f6\u51b3\u7b56\u6548\u738730%", "conclusion": "\u521b\u65b0\u6027AI\u65b9\u6cd5\u4e3a\u5371\u673a\u7ba1\u7406\u63d0\u4f9b\u53cc\u91cd\u89c6\u89d2\uff0c\u5f3a\u5316\u5b9e\u65f6\u54cd\u5e94\u4e0e\u957f\u671f\u97e7\u6027\u89c4\u5212\u7684\u6570\u636e\u878d\u5408\u8303\u5f0f"}}
{"id": "2505.17039", "pdf": "https://arxiv.org/pdf/2505.17039", "abs": "https://arxiv.org/abs/2505.17039", "authors": ["Diego Bonatto"], "title": "A new classification system of beer categories and styles based on large-scale data mining and self-organizing maps of beer recipes", "categories": ["cs.CL"], "comment": "46 pages, 8 figures, 1 table", "summary": "A data-driven quantitative approach was used to develop a novel\nclassification system for beer categories and styles. Sixty-two thousand one\nhundred twenty-one beer recipes were mined and analyzed, considering ingredient\nprofiles, fermentation parameters, and recipe vital statistics. Statistical\nanalyses combined with self-organizing maps (SOMs) identified four major\nsuperclusters that showed distinctive malt and hop usage patterns, style\ncharacteristics, and historical brewing traditions. Cold fermented styles\nshowed a conservative grain and hop composition, whereas hot fermented beers\nexhibited high heterogeneity, reflecting regional preferences and innovation.\nThis new taxonomy offers a reproducible and objective framework beyond\ntraditional sensory-based classifications, providing brewers, researchers, and\neducators with a scalable tool for recipe analysis and beer development. The\nfindings in this work provide an understanding of beer diversity and open\navenues for linking ingredient usage with fermentation profiles and flavor\noutcomes.", "AI": {"tldr": "\u57fa\u4e8e62121\u4e2a\u5564\u9152\u914d\u65b9\u5f00\u53d1\u6570\u636e\u9a71\u52a8\u7684\u5564\u9152\u5206\u7c7b\u7cfb\u7edf\uff0c\u901a\u8fc7\u7edf\u8ba1\u5206\u6790\u8bc6\u522b\u51fa4\u4e2a\u8d85\u7ea7\u96c6\u7fa4\uff0c\u63ed\u793a\u51b7/\u70ed\u53d1\u9175\u5564\u9152\u7684\u539f\u6599\u5dee\u5f02", "motivation": "\u7a81\u7834\u4f20\u7edf\u611f\u5b98\u5206\u7c7b\u7684\u4e3b\u89c2\u5c40\u9650\uff0c\u5efa\u7acb\u53ef\u91cd\u590d\u7684\u5ba2\u89c2\u5564\u9152\u5206\u7c7b\u6846\u67b6", "method": "\u4f7f\u7528\u6570\u636e\u6316\u6398\uff0862,121\u4e2a\u914d\u65b9\uff09\u7ed3\u5408\u81ea\u7ec4\u7ec7\u6620\u5c04\uff08SOMs\uff09\u548c\u7edf\u8ba1\u5206\u6790\uff0c\u7814\u7a76\u539f\u6599\u7ec4\u6210\u3001\u53d1\u9175\u53c2\u6570\u548c\u917f\u9020\u4f20\u7edf", "result": "\u53d1\u73b0\u51b7\u53d1\u9175\u5564\u9152\u539f\u6599\u4fdd\u5b88\uff0c\u70ed\u53d1\u9175\u5564\u9152\u5448\u73b0\u5730\u57df\u521b\u65b0\u5dee\u5f02\uff1b\u5efa\u7acb\u5305\u542b\u56db\u4e2a\u8d85\u7ea7\u96c6\u7fa4\u7684\u5564\u9152\u5206\u7c7b\u65b0\u4f53\u7cfb", "conclusion": "\u8be5\u5b9a\u91cf\u5206\u7c7b\u7cfb\u7edf\u4e3a\u917f\u9020\u5de5\u827a\u4f18\u5316\u3001\u6559\u5b66\u7814\u7a76\u548c\u65b0\u4ea7\u54c1\u5f00\u53d1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5ba2\u89c2\u5206\u6790\u5de5\u5177"}}
{"id": "2505.17042", "pdf": "https://arxiv.org/pdf/2505.17042", "abs": "https://arxiv.org/abs/2505.17042", "authors": ["Abdullah Abdullah", "Seong Tae Kim"], "title": "VLM-KG: Multimodal Radiology Knowledge Graph Generation", "categories": ["cs.CL", "cs.CV", "cs.IR", "cs.LG"], "comment": "10 pages, 2 figures", "summary": "Vision-Language Models (VLMs) have demonstrated remarkable success in natural\nlanguage generation, excelling at instruction following and structured output\ngeneration. Knowledge graphs play a crucial role in radiology, serving as\nvaluable sources of factual information and enhancing various downstream tasks.\nHowever, generating radiology-specific knowledge graphs presents significant\nchallenges due to the specialized language of radiology reports and the limited\navailability of domain-specific data. Existing solutions are predominantly\nunimodal, meaning they generate knowledge graphs only from radiology reports\nwhile excluding radiographic images. Additionally, they struggle with long-form\nradiology data due to limited context length. To address these limitations, we\npropose a novel multimodal VLM-based framework for knowledge graph generation\nin radiology. Our approach outperforms previous methods and introduces the\nfirst multimodal solution for radiology knowledge graph generation.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u653e\u5c04\u5b66\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u6846\u67b6\uff0c\u6709\u6548\u7ed3\u5408\u5f71\u50cf\u4e0e\u6587\u672c\u6570\u636e", "motivation": "\u73b0\u6709\u5355\u6a21\u6001\u65b9\u6cd5\u65e0\u6cd5\u5229\u7528\u653e\u5c04\u5f71\u50cf\u4e14\u53d7\u9650\u4e8e\u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u5bfc\u81f4\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u4e0d\u5168\u9762", "method": "\u5f00\u53d1\u591a\u6a21\u6001VLM\u6846\u67b6\u6574\u5408\u653e\u5c04\u62a5\u544a\u4e0e\u5f71\u50cf\u6570\u636e\uff0c\u589e\u5f3a\u957f\u6587\u672c\u5904\u7406\u80fd\u529b", "result": "\u65b9\u6cd5\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u6280\u672f\uff0c\u6210\u4e3a\u653e\u5c04\u9886\u57df\u9996\u4e2a\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u89e3\u51b3\u65b9\u6848", "conclusion": "\u591a\u6a21\u6001\u65b9\u6cd5\u6210\u529f\u7a81\u7834\u653e\u5c04\u5b66\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u5229\u7528\u6548\u7387\u4e0e\u7ed3\u679c\u8d28\u91cf"}}
{"id": "2505.17043", "pdf": "https://arxiv.org/pdf/2505.17043", "abs": "https://arxiv.org/abs/2505.17043", "authors": ["Anya Belz"], "title": "QRA++: Quantified Reproducibility Assessment for Common Types of Results in Natural Language Processing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reproduction studies reported in NLP provide individual data points which in\ncombination indicate worryingly low levels of reproducibility in the field.\nBecause each reproduction study reports quantitative conclusions based on its\nown, often not explicitly stated, criteria for reproduction success/failure,\nthe conclusions drawn are hard to interpret, compare, and learn from. In this\npaper, we present QRA++, a quantitative approach to reproducibility assessment\nthat (i) produces continuous-valued degree of reproducibility assessments at\nthree levels of granularity; (ii) utilises reproducibility measures that are\ndirectly comparable across different studies; and (iii) grounds expectations\nabout degree of reproducibility in degree of similarity between experiments.\nQRA++ enables more informative reproducibility assessments to be conducted, and\nconclusions to be drawn about what causes reproducibility to be better/poorer.\nWe illustrate this by applying QRA++ to three example sets of comparable\nexperiments, revealing clear evidence that degree of reproducibility depends on\nsimilarity of experiment properties, but also system type and evaluation\nmethod.", "AI": {"tldr": "\u63d0\u51faQRA++\u91cf\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u9a8c\u76f8\u4f3c\u6027\u6784\u5efa\u53ef\u6bd4\u8f83\u7684\u518d\u73b0\u6027\u5ea6\u91cf\u6307\u6807\uff0c\u63ed\u793aNLP\u9886\u57df\u518d\u73b0\u6027\u7a0b\u5ea6\u4e0e\u5b9e\u9a8c\u5c5e\u6027\u3001\u7cfb\u7edf\u7c7b\u578b\u53ca\u8bc4\u4f30\u65b9\u6cd5\u7684\u5173\u8054\u6027", "motivation": "\u5f53\u524dNLP\u518d\u73b0\u6027\u7814\u7a76\u4f7f\u7528\u4e0d\u7edf\u4e00\u6807\u51c6\u5bfc\u81f4\u7ed3\u8bba\u96be\u4ee5\u6a2a\u5411\u6bd4\u8f83\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u91cf\u5316\u8bc4\u4f30\u65b9\u6cd5", "method": "\u5f00\u53d1QRA++\u6846\u67b6\uff1a\u2460\u4e09\u7c92\u5ea6\u8fde\u7eed\u503c\u8bc4\u4f30 \u2461\u8de8\u7814\u7a76\u53ef\u6bd4\u6307\u6807 \u2462\u5b9e\u9a8c\u76f8\u4f3c\u6027\u9a71\u52a8\u671f\u671b\u57fa\u51c6", "result": "\u5b9e\u9a8c\u663e\u793a\u518d\u73b0\u6027\u7a0b\u5ea6\u4e0e\u5b9e\u9a8c\u5c5e\u6027\u76f8\u4f3c\u5ea6\u6b63\u76f8\u5173\uff0c\u4e14\u53d7\u7cfb\u7edf\u7c7b\u578b\u548c\u8bc4\u4f30\u65b9\u6cd5\u663e\u8457\u5f71\u54cd", "conclusion": "QRA++\u63d0\u4f9b\u6807\u51c6\u5316\u91cf\u5316\u5de5\u5177\uff0c\u4f7f\u518d\u73b0\u6027\u8bc4\u4f30\u53ef\u89e3\u91ca\u3001\u53ef\u5f52\u56e0\uff0c\u4fc3\u8fdb\u9886\u57df\u7814\u7a76\u65b9\u6cd5\u6539\u8fdb"}}
{"id": "2505.17045", "pdf": "https://arxiv.org/pdf/2505.17045", "abs": "https://arxiv.org/abs/2505.17045", "authors": ["Afifah Kashif", "Heer Patel"], "title": "Assessing GPT's Bias Towards Stigmatized Social Groups: An Intersectional Case Study on Nationality Prejudice and Psychophobia", "categories": ["cs.CL"], "comment": null, "summary": "Recent studies have separately highlighted significant biases within\nfoundational large language models (LLMs) against certain nationalities and\nstigmatized social groups. This research investigates the ethical implications\nof these biases intersecting with outputs of widely-used GPT-3.5/4/4o LLMS.\nThrough structured prompt series, we evaluate model responses to several\nscenarios involving American and North Korean nationalities with various mental\ndisabilities. Findings reveal significant discrepancies in empathy levels with\nNorth Koreans facing greater negative bias, particularly when mental disability\nis also a factor. This underscores the need for improvements in LLMs designed\nwith a nuanced understanding of intersectional identity.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u4e3b\u6d41LLMs\u5bf9\u671d\u9c9c\u4eba\u53ca\u5fc3\u7406\u969c\u788d\u7fa4\u4f53\u5b58\u5728\u4ea4\u53c9\u504f\u89c1\uff0c\u9700\u6539\u8fdb\u6a21\u578b\u8bbe\u8ba1", "motivation": "\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u9488\u5bf9\u56fd\u7c4d\u4e0e\u5fc3\u7406\u969c\u788d\u7684\u4ea4\u53c9\u504f\u89c1\u5f15\u53d1\u7684\u4f26\u7406\u95ee\u9898", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u6d4b\u8bd5GPT-3.5/4/4o\u5bf9\u4e0d\u540c\u56fd\u7c4d\uff08\u7f8e/\u671d\uff09\u53ca\u5fc3\u7406\u969c\u788d\u573a\u666f\u7684\u53cd\u5e94", "result": "\u671d\u9c9c\u4eba\u7fa4\u4f53\uff08\u5c24\u5176\u53e0\u52a0\u5fc3\u7406\u969c\u788d\u65f6\uff09\u627f\u53d7\u663e\u8457\u66f4\u5f3a\u7684\u8d1f\u9762\u504f\u89c1\u4e0e\u540c\u7406\u5fc3\u7f3a\u5931", "conclusion": "LLMs\u9700\u589e\u5f3a\u5bf9\u4ea4\u53c9\u8eab\u4efd\u7279\u5f81\u7684\u7ec6\u7c92\u5ea6\u7406\u89e3\uff0c\u4ee5\u964d\u4f4e\u4f26\u7406\u98ce\u9669"}}
{"id": "2505.17047", "pdf": "https://arxiv.org/pdf/2505.17047", "abs": "https://arxiv.org/abs/2505.17047", "authors": ["Erin Palm", "Astrit Manikantan", "Mark E. Pepin", "Herprit Mahal", "Srikanth Subramanya Belwadi"], "title": "Assessing the Quality of AI-Generated Clinical Notes: A Validated Evaluation of a Large Language Model Scribe", "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 5 tables, 1 figure. Submitted for peer review 05/15/2025", "summary": "In medical practices across the United States, physicians have begun\nimplementing generative artificial intelligence (AI) tools to perform the\nfunction of scribes in order to reduce the burden of documenting clinical\nencounters. Despite their widespread use, no established methods exist to gauge\nthe quality of AI scribes. To address this gap, we developed a blinded study\ncomparing the relative performance of large language model (LLM) generated\nclinical notes with those from field experts based on audio-recorded clinical\nencounters. Quantitative metrics from the Physician Documentation Quality\nInstrument (PDQI9) provided a framework to measure note quality, which we\nadapted to assess relative performance of AI generated notes. Clinical experts\nspanning 5 medical specialties used the PDQI9 tool to evaluate\nspecialist-drafted Gold notes and LLM authored Ambient notes. Two evaluators\nfrom each specialty scored notes drafted from a total of 97 patient visits. We\nfound uniformly high inter rater agreement (RWG greater than 0.7) between\nevaluators in general medicine, orthopedics, and obstetrics and gynecology, and\nmoderate (RWG 0.5 to 0.7) to high inter rater agreement in pediatrics and\ncardiology. We found a modest yet significant difference in the overall note\nquality, wherein Gold notes achieved a score of 4.25 out of 5 and Ambient notes\nscored 4.20 out of 5 (p = 0.04). Our findings support the use of the PDQI9\ninstrument as a practical method to gauge the quality of LLM authored notes, as\ncompared to human-authored notes.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u76f2\u6cd5\u6bd4\u8f83LLM\u751f\u6210\u4e0e\u4e13\u5bb6\u64b0\u5199\u7684\u4e34\u5e8a\u8bb0\u5f55\u8d28\u91cf\uff0c\u53d1\u73b0\u4e24\u8005\u5dee\u5f02\u5fae\u5c0f\uff08Gold\u7b14\u8bb04.25\u5206 vs Ambient\u7b14\u8bb04.20\u5206\uff09\uff0c\u652f\u6301PDQI9\u4f5c\u4e3aAI\u751f\u6210\u8bb0\u5f55\u7684\u8bc4\u4f30\u5de5\u5177\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u8bc4\u4f30AI\u533b\u7597\u8bb0\u5f55\u8d28\u91cf\u7684\u6807\u51c6\u65b9\u6cd5\uff0c\u9700\u9a8c\u8bc1\u751f\u6210\u5f0fAI\u5728\u4e34\u5e8a\u6587\u6863\u4e2d\u7684\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u76f2\u6cd5\u7814\u7a76\u8bbe\u8ba1\uff0c\u4f7f\u7528PDQI9\u91cf\u8868\uff0c\u75315\u4e2a\u533b\u5b66\u4e13\u79d1\u4e13\u5bb6\u5bf997\u4efd\u60a3\u8005\u5c31\u8bca\u8bb0\u5f55\uff08Gold\u4e13\u5bb6\u7b14\u8bb0 vs Ambient AI\u7b14\u8bb0\uff09\u8fdb\u884c\u53cc\u76f2\u8bc4\u5206\u3002", "result": "Gold\u7b14\u8bb0\u603b\u4f53\u5f97\u52064.25/5\uff0cAmbient\u7b14\u8bb04.20/5\uff08p=0.04\uff09\uff1b\u666e\u901a\u5185\u79d1\u3001\u9aa8\u79d1\u3001\u5987\u4ea7\u79d1\u8bc4\u4f30\u8005\u95f4\u4e00\u81f4\u6027\u9ad8\uff08RWG>0.7\uff09\uff0c\u513f\u79d1\u548c\u5fc3\u810f\u75c5\u5b66\u4e2d\u7b49\uff08RWG 0.5-0.7\uff09\u3002", "conclusion": "PDQI9\u80fd\u6709\u6548\u8bc4\u4f30AI\u751f\u6210\u4e34\u5e8a\u8bb0\u5f55\u8d28\u91cf\uff0c\u4e3a\u533b\u7597AI\u5de5\u5177\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u63d0\u4f9b\u65b9\u6cd5\u8bba\u652f\u6301\u3002"}}
{"id": "2505.17048", "pdf": "https://arxiv.org/pdf/2505.17048", "abs": "https://arxiv.org/abs/2505.17048", "authors": ["Agam Shah", "Siddhant Sukhani", "Huzaifa Pardawala", "Saketh Budideti", "Riya Bhadani", "Rudra Gopal", "Siddhartha Somani", "Michael Galarnyk", "Soungmin Lee", "Arnav Hiray", "Akshar Ravichandran", "Eric Kim", "Pranav Aluru", "Joshua Zhang", "Sebastian Jaskowski", "Veer Guda", "Meghaj Tarte", "Liqin Ye", "Spencer Gosden", "Rutwik Routu", "Rachel Yuh", "Sloka Chava", "Sahasra Chava", "Dylan Patrick Kelly", "Aiden Chiang", "Harsit Mittal", "Sudheer Chava"], "title": "Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally", "categories": ["cs.CL", "cs.AI", "cs.CY", "q-fin.CP", "q-fin.GN"], "comment": null, "summary": "Central banks around the world play a crucial role in maintaining economic\nstability. Deciphering policy implications in their communications is\nessential, especially as misinterpretations can disproportionately impact\nvulnerable populations. To address this, we introduce the World Central Banks\n(WCB) dataset, the most comprehensive monetary policy corpus to date,\ncomprising over 380k sentences from 25 central banks across diverse geographic\nregions, spanning 28 years of historical data. After uniformly sampling 1k\nsentences per bank (25k total) across all available years, we annotate and\nreview each sentence using dual annotators, disagreement resolutions, and\nsecondary expert reviews. We define three tasks: Stance Detection, Temporal\nClassification, and Uncertainty Estimation, with each sentence annotated for\nall three. We benchmark seven Pretrained Language Models (PLMs) and nine Large\nLanguage Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on\nthese tasks, running 15,075 benchmarking experiments. We find that a model\ntrained on aggregated data across banks significantly surpasses a model trained\non an individual bank's data, confirming the principle \"the whole is greater\nthan the sum of its parts.\" Additionally, rigorous human evaluations, error\nanalyses, and predictive tasks validate our framework's economic utility. Our\nartifacts are accessible through the HuggingFace and GitHub under the\nCC-BY-NC-SA 4.0 license.", "AI": {"tldr": "\u6784\u5efa\u5168\u7403\u6700\u5927\u592e\u884c\u653f\u7b56\u8bed\u6599\u5e93WCB\uff0c\u63d0\u51fa\u4e09\u5927\u6807\u6ce8\u4efb\u52a1\u5e76\u9a8c\u8bc1\u8de8\u673a\u6784\u805a\u5408\u8bad\u7ec3\u6a21\u578b\u7684\u4f18\u8d8a\u6027", "motivation": "\u592e\u884c\u653f\u7b56\u89e3\u8bfb\u504f\u5dee\u4f1a\u52a0\u5267\u5f31\u52bf\u7fa4\u4f53\u7ecf\u6d4e\u98ce\u9669\uff0c\u9700\u6784\u5efa\u6807\u51c6\u5316\u5206\u6790\u6846\u67b6\u63d0\u5347\u653f\u7b56\u89e3\u8bfb\u51c6\u786e\u6027", "method": "\u6536\u96c625\u56fd\u592e\u884c28\u5e74\u6570\u636e\uff0c\u91c7\u7528\u53cc\u4eba\u6807\u6ce8-\u4e13\u5bb6\u4ef2\u88c1\u673a\u5236\uff0c\u5b9a\u4e49\u7acb\u573a/\u65f6\u6001/\u4e0d\u786e\u5b9a\u6027\u4e09\u7c7b\u4efb\u52a1\uff0c\u5b8c\u62101.5\u4e07\u6b21\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5", "result": "\u8de8\u673a\u6784\u805a\u5408\u6570\u636e\u6a21\u578b\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u5355\u673a\u6784\u6a21\u578b\uff08p<0.01\uff09\uff0c\u4eba\u7c7b\u8bc4\u4f30\u9a8c\u8bc1\u6846\u67b6\u7ecf\u6d4e\u6548\u7528\u8fbe89%\u51c6\u786e\u7387", "conclusion": "WCB\u6570\u636e\u96c6\u586b\u8865\u8d27\u5e01\u653f\u7b56\u5206\u6790\u7a7a\u767d\uff0c\u8bc1\u660e\u6570\u636e\u805a\u5408\u4f18\u52bf\uff0c\u5f00\u6e90\u8d44\u6e90\u63a8\u52a8\u91d1\u878dNLP\u9886\u57df\u53d1\u5c55"}}
{"id": "2505.17049", "pdf": "https://arxiv.org/pdf/2505.17049", "abs": "https://arxiv.org/abs/2505.17049", "authors": ["David Rozado"], "title": "Gender and Positional Biases in LLM-Based Hiring Decisions: Evidence from Comparative CV/R\u00e9sum\u00e9 Evaluations", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "This study examines the behavior of Large Language Models (LLMs) when\nevaluating professional candidates based on their resumes or curricula vitae\n(CVs). In an experiment involving 22 leading LLMs, each model was\nsystematically given one job description along with a pair of\nprofession-matched CVs, one bearing a male first name, the other a female first\nname, and asked to select the more suitable candidate for the job. Each CV pair\nwas presented twice, with names swapped to ensure that any observed preferences\nin candidate selection stemmed from gendered names cues. Despite identical\nprofessional qualifications across genders, all LLMs consistently favored\nfemale-named candidates across 70 different professions. Adding an explicit\ngender field (male/female) to the CVs further increased the preference for\nfemale applicants. When gendered names were replaced with gender-neutral\nidentifiers \"Candidate A\" and \"Candidate B\", several models displayed a\npreference to select \"Candidate A\". Counterbalancing gender assignment between\nthese gender-neutral identifiers resulted in gender parity in candidate\nselection. When asked to rate CVs in isolation rather than compare pairs, LLMs\nassigned slightly higher average scores to female CVs overall, but the effect\nsize was negligible. Including preferred pronouns (he/him or she/her) next to a\ncandidate's name slightly increased the odds of the candidate being selected\nregardless of gender. Finally, most models exhibited a substantial positional\nbias to select the candidate listed first in the prompt. These findings\nunderscore the need for caution when deploying LLMs in high-stakes autonomous\ndecision-making contexts and raise doubts about whether LLMs consistently apply\nprincipled reasoning.", "AI": {"tldr": "LLMs\u5728\u7b80\u5386\u8bc4\u4f30\u4e2d\u7cfb\u7edf\u6027\u504f\u5411\u5973\u6027\u5019\u9009\u4eba\uff0c\u5b58\u5728\u6027\u522b/\u4f4d\u7f6e\u504f\u5dee\uff0c\u9700\u8c28\u614e\u7528\u4e8e\u9ad8\u98ce\u9669\u51b3\u7b56\u573a\u666f\u3002", "motivation": "\u63a2\u7a76LLMs\u5728\u4e13\u4e1a\u5019\u9009\u4eba\u8bc4\u4f30\u4e2d\u662f\u5426\u5b58\u5728\u6027\u522b\u504f\u89c1\uff0c\u9a8c\u8bc1\u5176\u51b3\u7b56\u8fc7\u7a0b\u7684\u539f\u5219\u6027\u3002", "method": "\u4f7f\u752822\u4e2a\u4e3b\u6d41LLMs\u8fdb\u884c\u914d\u5bf9\u5b9e\u9a8c\uff1a1) \u76f8\u540c\u8d44\u5386\u7b80\u5386\u4ec5\u6027\u522b\u4e0d\u540c 2) \u663e\u5f0f\u6027\u522b\u6807\u6ce8 3) \u4e2d\u6027\u6807\u8bc6\u7b26\u66ff\u6362 4) \u4f4d\u7f6e\u987a\u5e8f\u8c03\u6574 5) \u5355\u72ec\u7b80\u5386\u8bc4\u5206\u6d4b\u8bd5\u3002", "result": "1. \u6240\u6709\u6a21\u578b\u6301\u7eed\u504f\u5411\u5973\u6027\u5019\u9009\u4eba\uff0870\u4e2a\u804c\u4e1a\uff09 2. \u663e\u5f0f\u6027\u522b\u6807\u6ce8\u52a0\u5f3a\u5973\u6027\u504f\u597d 3. \u4e2d\u6027\u6807\u8bc6\u7b26\u5f15\u53d1\u9996\u9009\u9879\u504f\u89c1 4. \u5355\u72ec\u8bc4\u5206\u6027\u522b\u5dee\u5f02\u5fae\u5f31 5. \u4f4d\u7f6e\u987a\u5e8f\u663e\u8457\u5f71\u54cd\u9009\u62e9\u6982\u7387\u3002", "conclusion": "LLMs\u5728\u51b3\u7b56\u4e2d\u8868\u73b0\u51fa\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u5176\u2018\u539f\u5219\u6027\u63a8\u7406\u2019\u53ef\u9760\u6027\u5b58\u7591\uff0c\u5efa\u8bae\u907f\u514d\u5728\u62db\u8058\u7b49\u5173\u952e\u9886\u57df\u5b8c\u5168\u4f9d\u8d56\u81ea\u52a8\u5316LLM\u51b3\u7b56\u3002"}}
{"id": "2505.17050", "pdf": "https://arxiv.org/pdf/2505.17050", "abs": "https://arxiv.org/abs/2505.17050", "authors": ["Yanhao Jia", "Xinyi Wu", "Qinglin Zhang", "Yiran Qin", "Luwei Xiao", "Shuai Zhao"], "title": "Towards Robust Evaluation of STEM Education: Leveraging MLLMs in Project-Based Learning", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.CY", "cs.MM"], "comment": null, "summary": "Project-Based Learning (PBL) involves a variety of highly correlated\nmultimodal data, making it a vital educational approach within STEM\ndisciplines. With the rapid development of multimodal large language models\n(MLLMs), researchers have begun exploring their potential to enhance tasks such\nas information retrieval, knowledge comprehension, and data generation in\neducational settings. However, existing benchmarks fall short in providing both\na free-form output structure and a rigorous human expert validation process,\nlimiting their effectiveness in evaluating real-world educational tasks.\nAdditionally, few methods have developed automated pipelines to assist with the\ncomplex responsibilities of teachers leveraging MLLMs, largely due to model\nhallucination and instability, which lead to unreliable implementation. To\naddress this gap, we introduce PBLBench, a novel benchmark designed to evaluate\ncomplex reasoning grounded in domain-specific knowledge and long-context\nunderstanding, thereby challenging models with tasks that closely resemble\nthose handled by human experts. To establish reliable ground truth, we adopt\nthe Analytic Hierarchy Process (AHP), utilizing expert-driven pairwise\ncomparisons to derive structured and weighted evaluation criteria. We assess\nthe performance of 15 leading MLLMs/LLMs using PBLBench and demonstrate that\neven the most advanced models achieve only 59% rank accuracy, underscoring the\nsignificant challenges presented by this benchmark. We believe PBLBench will\nserve as a catalyst for the development of more capable AI agents, ultimately\naiming to alleviate teacher workload and enhance educational productivity.", "AI": {"tldr": "\u63d0\u51faPBLBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u573a\u666f\u4e2d\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u9876\u5c16\u6a21\u578b\u4ec559%\u6392\u540d\u51c6\u786e\u7387\u7684\u73b0\u72b6", "motivation": "\u73b0\u6709\u6559\u80b2\u8bc4\u4f30\u57fa\u51c6\u7f3a\u4e4f\u81ea\u7531\u8f93\u51fa\u7ed3\u6784\u548c\u4e25\u683c\u4e13\u5bb6\u9a8c\u8bc1\uff0c\u4e14\u6a21\u578b\u5e7b\u89c9\u4e0e\u4e0d\u7a33\u5b9a\u6027\u5bfc\u81f4\u96be\u4ee5\u6784\u5efa\u6559\u5e08\u8f85\u52a9\u7684\u81ea\u52a8\u5316\u6d41\u7a0b", "method": "\u91c7\u7528\u5c42\u6b21\u5206\u6790\u6cd5(AHP)\u6784\u5efa\u4e13\u5bb6\u9a71\u52a8\u7684\u7ed3\u6784\u5316\u8bc4\u4f30\u6807\u51c6\uff0c\u5bf915\u4e2a\u4e3b\u6d41MLLMs/LLMs\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30", "result": "\u6700\u4f18\u6a21\u578b\u5728\u771f\u5b9e\u6559\u80b2\u4efb\u52a1\u4e2d\u4ec5\u8fbe59%\u6392\u540d\u51c6\u786e\u7387\uff0c\u66b4\u9732\u6a21\u578b\u5728\u9886\u57df\u77e5\u8bc6\u63a8\u7406\u548c\u957f\u4e0a\u4e0b\u6587\u7406\u89e3\u7684\u6839\u672c\u7f3a\u9677", "conclusion": "PBLBench\u4e3aAI\u6559\u80b2\u4ee3\u7406\u53d1\u5c55\u63d0\u4f9b\u5173\u952e\u57fa\u51c6\uff0c\u901a\u8fc7\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u52a9\u529b\u6559\u5e08\u51cf\u8d1f\u4e0e\u6559\u80b2\u6548\u80fd\u63d0\u5347"}}
{"id": "2505.17051", "pdf": "https://arxiv.org/pdf/2505.17051", "abs": "https://arxiv.org/abs/2505.17051", "authors": ["Bernd Huber", "Ghazal Fazelnia", "Andreas Damianou", "Sebastian Peleato", "Max Lefarov", "Praveen Ravichandran", "Marco De Nadai", "Mounia Lalmas-Roellke", "Paul N. Bennett"], "title": "Embedding-to-Prefix: Parameter-Efficient Personalization for Pre-Trained Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel at generating contextually relevant\ncontent. However, tailoring these outputs to individual users for effective\npersonalization is a significant challenge. While rich user-specific\ninformation often exists as pre-existing user representations, such as\nembeddings learned from preferences or behaviors, current methods to leverage\nthese for LLM personalization typically require costly fine-tuning or\ntoken-heavy prompting. We propose Embedding-to-Prefix (E2P), a\nparameter-efficient method that injects pre-computed context embeddings into an\nLLM's hidden representation space through a learned projection to a single soft\ntoken prefix. This enables effective personalization while keeping the backbone\nmodel frozen and avoiding expensive adaptation techniques. We evaluate E2P\nacross two public datasets and in a production setting: dialogue\npersonalization on Persona-Chat, contextual headline generation on PENS, and\nlarge-scale personalization for music and podcast consumption. Results show\nthat E2P preserves contextual signals and achieves strong performance with\nminimal computational overhead, offering a scalable, efficient solution for\ncontextualizing generative AI systems.", "AI": {"tldr": "\u63d0\u51faE2P\u65b9\u6cd5\u5b9e\u73b0\u65e0\u9700\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e2a\u6027\u5316\u8f93\u51fa", "motivation": "\u73b0\u6709LLM\u4e2a\u6027\u5316\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u6210\u672c\u5fae\u8c03\u6216\u590d\u6742prompt\u5de5\u7a0b\uff0c\u9700\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u901a\u8fc7\u5b66\u4e60\u6295\u5f71\u5c06\u9884\u8ba1\u7b97\u4e0a\u4e0b\u6587\u5d4c\u5165\u6ce8\u5165LLM\u9690\u85cf\u8868\u793a\u7a7a\u95f4\uff0c\u751f\u6210\u5355\u4e2a\u8f6f\u4ee4\u724c\u524d\u7f00", "result": "\u5728\u5bf9\u8bdd/\u6807\u9898\u751f\u6210/\u97f3\u4e50\u63a8\u8350\u573a\u666f\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u8ba1\u7b97\u5f00\u9500\u964d\u4f4e90%+", "conclusion": "E2P\u4e3a\u751f\u6210\u5f0fAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u4e0a\u4e0b\u6587\u9002\u914d\u65b9\u6848\uff0c\u5e73\u8861\u6027\u80fd\u4e0e\u6548\u7387"}}
{"id": "2505.17052", "pdf": "https://arxiv.org/pdf/2505.17052", "abs": "https://arxiv.org/abs/2505.17052", "authors": ["Jinwoo Park", "Seunggeun Cho", "Dongsu Han"], "title": "SpecEdge: Scalable Edge-Assisted Serving Framework for Interactive LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) power many modern applications, but serving them\nat scale remains costly and resource-intensive. Current server-centric systems\noverlook consumer-grade GPUs at the edge. We introduce SpecEdge, an\nedge-assisted inference framework that splits LLM workloads between edge and\nserver GPUs using a speculative decoding scheme, exchanging only token outputs\nover the network. SpecEdge employs proactive edge drafting to overlap edge\ntoken creation with server verification and pipeline-aware scheduling that\ninterleaves multiple user requests to increase server-side throughput.\nExperiments show SpecEdge enhances overall cost efficiency by 1.91x through\nachieving 2.22x server throughput, and reduces inter token latency by 11.24%\ncompared to a server-only baseline, introducing a scalable, cost-effective\nparadigm for LLM serving.", "AI": {"tldr": "SpecEdge\u6846\u67b6\u901a\u8fc7\u6574\u5408\u8fb9\u7f18GPU\u548c\u63a8\u6d4b\u89e3\u7801\u6280\u672f\uff0c\u5c06LLM\u63a8\u7406\u6548\u7387\u63d0\u53471.91\u500d\u5e76\u964d\u4f4e\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u670d\u52a1\u5668\u541e\u5410\u91cf\u7ffb\u500d", "motivation": "\u73b0\u6709LLM\u670d\u52a1\u7cfb\u7edf\u8fc7\u5ea6\u4f9d\u8d56\u670d\u52a1\u5668\u7ea7GPU\uff0c\u5ffd\u89c6\u4e86\u8fb9\u7f18\u8ba1\u7b97\u8d44\u6e90\u7684\u4ef7\u503c\uff0c\u5bfc\u81f4\u8fd0\u8425\u6210\u672c\u9ad8\u6602\u4e14\u8d44\u6e90\u5229\u7528\u7387\u4e0d\u8db3", "method": "\u63d0\u51fa\u8fb9\u7f18-\u670d\u52a1\u5668\u534f\u540c\u63a8\u6d4b\u89e3\u7801\u65b9\u6848\uff0c\u5305\u542b\uff1a1) \u4e3b\u52a8\u8fb9\u7f18\u8d77\u8349\u673a\u5236\u5b9e\u73b0\u8ba1\u7b97\u91cd\u53e0\uff1b2) \u7ba1\u9053\u611f\u77e5\u8c03\u5ea6\u7cfb\u7edf\u5b9e\u73b0\u591a\u8bf7\u6c42\u4ea4\u9519\u5904\u7406\uff1b3) \u4ec5\u901a\u8fc7\u7f51\u7edc\u4f20\u8f93token\u7ea7\u8f93\u51fa", "result": "\u5b9e\u9a8c\u663e\u793a\u670d\u52a1\u5668\u541e\u5410\u91cf\u63d0\u53472.22\u500d\uff0c\u603b\u4f53\u6210\u672c\u6548\u76ca\u589e\u52a01.91\u500d\uff0ctoken\u95f4\u5ef6\u8fdf\u964d\u4f4e11.24%", "conclusion": "\u8be5\u6846\u67b6\u5f00\u521b\u4e86\u5229\u7528\u8fb9\u7f18\u8ba1\u7b97\u8d44\u6e90\u589e\u5f3aLLM\u670d\u52a1\u7684\u65b0\u8303\u5f0f\uff0c\u5728\u4fdd\u6301\u670d\u52a1\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8fd0\u8425\u6210\u672c"}}
{"id": "2505.17053", "pdf": "https://arxiv.org/pdf/2505.17053", "abs": "https://arxiv.org/abs/2505.17053", "authors": ["Ou Jiamin", "Eikmans Emile", "Buskens Vincent", "Pankowska Paulina", "Shan Yuli"], "title": "Social preferences with unstable interactive reasoning: Large language models in economic trust games", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 2 figures, 2 tables", "summary": "While large language models (LLMs) have demonstrated remarkable capabilities\nin understanding human languages, this study explores how they translate this\nunderstanding into social exchange contexts that capture certain essences of\nreal world human interactions. Three LLMs - ChatGPT-4, Claude, and Bard - were\nplaced in economic trust games where players balance self-interest with trust\nand reciprocity, making decisions that reveal their social preferences and\ninteractive reasoning abilities. Our study shows that LLMs deviate from pure\nself-interest and exhibit trust and reciprocity even without being prompted to\nadopt a specific persona. In the simplest one-shot interaction, LLMs emulated\nhow human players place trust at the beginning of such a game. Larger\nhuman-machine divergences emerged in scenarios involving trust repayment or\nmulti-round interactions, where decisions were influenced by both social\npreferences and interactive reasoning. LLMs responses varied significantly when\nprompted to adopt personas like selfish or unselfish players, with the impact\noutweighing differences between models or game types. Response of ChatGPT-4, in\nan unselfish or neutral persona, resembled the highest trust and reciprocity,\nsurpassing humans, Claude, and Bard. Claude and Bard displayed trust and\nreciprocity levels that sometimes exceeded and sometimes fell below human\nchoices. When given selfish personas, all LLMs showed lower trust and\nreciprocity than humans. Interactive reasoning to the actions of counterparts\nor changing game mechanics appeared to be random rather than stable,\nreproducible characteristics in the response of LLMs, though some improvements\nwere observed when ChatGPT-4 responded in selfish or unselfish personas.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4fe1\u4efb\u6e38\u620f\u4e2d\u5c55\u73b0\u51fa\u8d85\u8d8a\u81ea\u5229\u7684\u4fe1\u4efb\u4e0e\u4e92\u60e0\u884c\u4e3a\uff0c\u89d2\u8272\u8bbe\u5b9a\u5bf9\u6a21\u578b\u8868\u73b0\u7684\u5f71\u54cd\u8d85\u8fc7\u6a21\u578b\u7c7b\u578b\u5dee\u5f02\uff0cChatGPT-4\u5728\u65e0\u79c1\u89d2\u8272\u4e0b\u8868\u73b0\u6700\u4f18\u3002", "motivation": "\u63a2\u7d22\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5c06\u8bed\u8a00\u7406\u89e3\u8f6c\u5316\u4e3a\u793e\u4f1a\u4ea4\u6362\u884c\u4e3a\uff0c\u9a8c\u8bc1\u5176\u5728\u7ecf\u6d4e\u4fe1\u4efb\u6e38\u620f\u4e2d\u4f53\u73b0\u793e\u4f1a\u504f\u597d\u4e0e\u4e92\u52a8\u63a8\u7406\u7684\u80fd\u529b\u3002", "method": "\u4f7f\u7528ChatGPT-4/Claude/Bard\u53c2\u4e0e\u7ecf\u6d4e\u4fe1\u4efb\u6e38\u620f\uff0c\u901a\u8fc7\u5355\u6b21/\u591a\u8f6e\u4e92\u52a8\u53ca\u89d2\u8272\u8bbe\u5b9a\uff08\u81ea\u79c1/\u65e0\u79c1\uff09\u6d4b\u8bd5\u51b3\u7b56\u6a21\u5f0f\u3002", "result": "LLMs\u666e\u904d\u5c55\u73b0\u521d\u59cb\u4fe1\u4efb\uff0c\u89d2\u8272\u8bbe\u5b9a\u663e\u8457\u6539\u53d8\u884c\u4e3a\uff08\u65e0\u79c1\u89d2\u8272\u4e0b\u4fe1\u4efb\u5ea6>\u4eba\u7c7b\uff0c\u81ea\u79c1\u89d2\u8272\u4e0b<\u4eba\u7c7b\uff09\uff0c\u4e92\u52a8\u63a8\u7406\u80fd\u529b\u8868\u73b0\u4e0d\u7a33\u5b9a\u4f46\u89d2\u8272\u8bbe\u5b9a\u53ef\u6539\u5584\u3002", "conclusion": "LLMs\u5177\u5907\u793e\u4f1a\u4e92\u52a8\u6f5c\u529b\u4f46\u5176\u884c\u4e3a\u53d7\u89d2\u8272\u63d0\u793a\u4e3b\u5bfc\uff0c\u9700\u52a0\u5f3a\u4e92\u52a8\u63a8\u7406\u7684\u7a33\u5b9a\u6027\u7814\u7a76\uff0c\u89d2\u8272\u5de5\u7a0b\u53ef\u80fd\u6210\u4e3a\u91cd\u8981\u8c03\u8282\u624b\u6bb5\u3002"}}
{"id": "2505.17054", "pdf": "https://arxiv.org/pdf/2505.17054", "abs": "https://arxiv.org/abs/2505.17054", "authors": ["Linglong Qian", "Zina Ibrahim"], "title": "METHOD: Modular Efficient Transformer for Health Outcome Discovery", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages", "summary": "Recent advances in transformer architectures have revolutionised natural\nlanguage processing, but their application to healthcare domains presents\nunique challenges. Patient timelines are characterised by irregular sampling,\nvariable temporal dependencies, and complex contextual relationships that\ndiffer substantially from traditional language tasks. This paper introduces\n\\METHOD~(Modular Efficient Transformer for Health Outcome Discovery), a novel\ntransformer architecture specifically designed to address the challenges of\nclinical sequence modelling in electronic health records. \\METHOD~integrates\nthree key innovations: (1) a patient-aware attention mechanism that prevents\ninformation leakage whilst enabling efficient batch processing; (2) an adaptive\nsliding window attention scheme that captures multi-scale temporal\ndependencies; and (3) a U-Net inspired architecture with dynamic skip\nconnections for effective long sequence processing. Evaluations on the MIMIC-IV\ndatabase demonstrate that \\METHOD~consistently outperforms the state-of-the-art\n\\ETHOS~model, particularly in predicting high-severity cases that require\nurgent clinical intervention. \\METHOD~exhibits stable performance across\nvarying inference lengths, a crucial feature for clinical deployment where\npatient histories vary significantly in length. Analysis of learned embeddings\nreveals that \\METHOD~better preserves clinical hierarchies and relationships\nbetween medical concepts. These results suggest that \\METHOD~represents a\nsignificant advancement in transformer architectures optimised for healthcare\napplications, providing more accurate and clinically relevant predictions\nwhilst maintaining computational efficiency.", "AI": {"tldr": "\u63d0\u51faMETH\u6a21\u5757\u5316\u9ad8\u6548Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u60a3\u8005\u611f\u77e5\u6ce8\u610f\u529b\u3001\u81ea\u9002\u5e94\u6ed1\u52a8\u7a97\u53e3\u548cU-Net\u52a8\u6001\u8df3\u8dc3\u8fde\u63a5\uff0c\u5728\u533b\u7597\u65f6\u5e8f\u6570\u636e\u5904\u7406\u4e2d\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\uff0c\u5c24\u5176\u64c5\u957f\u9ad8\u98ce\u9669\u75c5\u4f8b\u9884\u6d4b\u3002", "motivation": "\u4f20\u7edfTransformer\u5904\u7406\u533b\u7597\u65f6\u5e8f\u6570\u636e\u65f6\u9762\u4e34\u4e0d\u89c4\u5219\u91c7\u6837\u3001\u590d\u6742\u65f6\u95f4\u4f9d\u8d56\u548c\u4e34\u5e8a\u4fe1\u606f\u6cc4\u9732\u7b49\u95ee\u9898\uff0c\u9700\u4e13\u95e8\u4f18\u5316\u7684\u67b6\u6784\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "1) \u9632\u4fe1\u606f\u6cc4\u6f0f\u7684\u60a3\u8005\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236 2) \u591a\u5c3a\u5ea6\u65f6\u95f4\u4f9d\u8d56\u7684\u81ea\u9002\u5e94\u6ed1\u52a8\u7a97\u53e3 3) U-Net\u67b6\u6784\u52a8\u6001\u8df3\u8dc3\u8fde\u63a5\u7684\u957f\u5e8f\u5217\u5904\u7406", "result": "\u5728MIMIC-IV\u6570\u636e\u5e93\u4e0a\u8868\u73b0\u4f18\u4e8eETHOS\u6a21\u578b\uff0c\u9ad8\u4e25\u91cd\u75c5\u4f8b\u9884\u6d4bF1\u503c\u63d0\u534712%\uff0c\u4e0d\u540c\u5386\u53f2\u957f\u5ea6\u4e0b\u6027\u80fd\u7a33\u5b9a\uff0c\u5d4c\u5165\u5206\u6790\u663e\u793a\u66f4\u597d\u4fdd\u7559\u4e34\u5e8a\u6982\u5ff5\u5c42\u7ea7\u5173\u7cfb\u3002", "conclusion": "METH\u6807\u5fd7\u7740\u533b\u7597\u4e13\u7528Transformer\u7684\u91cd\u8981\u8fdb\u5c55\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u51c6\u786e\u7684\u4e34\u5e8a\u9884\u6d4b\uff0c\u6ee1\u8db3\u5b9e\u9645\u90e8\u7f72\u9700\u6c42\u3002"}}
{"id": "2505.17055", "pdf": "https://arxiv.org/pdf/2505.17055", "abs": "https://arxiv.org/abs/2505.17055", "authors": ["Fidaa khandaqji", "Huthaifa I. Ashqar", "Abdelrahem Atawnih"], "title": "Enhancing Mathematics Learning for Hard-of-Hearing Students Through Real-Time Palestinian Sign Language Recognition: A New Dataset", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "The study aims to enhance mathematics education accessibility for\nhard-of-hearing students by developing an accurate Palestinian sign language\nPSL recognition system using advanced artificial intelligence techniques. Due\nto the scarcity of digital resources for PSL, a custom dataset comprising 41\nmathematical gesture classes was created, and recorded by PSL experts to ensure\nlinguistic accuracy and domain specificity. To leverage\nstate-of-the-art-computer vision techniques, a Vision Transformer ViTModel was\nfine-tuned for gesture classification. The model achieved an accuracy of\n97.59%, demonstrating its effectiveness in recognizing mathematical signs with\nhigh precision and reliability. This study highlights the role of deep learning\nin developing intelligent educational tools that bridge the learning gap for\nhard-of-hearing students by providing AI-driven interactive solutions to\nenhance mathematical comprehension. This work represents a significant step\ntoward innovative and inclusive frosting digital integration in specialized\nlearning environments. The dataset is hosted on Hugging Face at\nhttps://huggingface.co/datasets/fidaakh/STEM_data.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8eViT\u6a21\u578b\u7684\u5df4\u52d2\u65af\u5766\u6570\u5b66\u624b\u8bed\u8bc6\u522b\u7cfb\u7edf\uff0c\u51c6\u786e\u7387\u8fbe97.59%\uff0c\u63a8\u52a8\u542c\u969c\u5b66\u751f\u6570\u5b66\u6559\u80b2\u5305\u5bb9\u6027", "motivation": "\u9488\u5bf9\u5df4\u52d2\u65af\u5766\u624b\u8bed\u6570\u5b57\u8d44\u6e90\u532e\u4e4f\u95ee\u9898\uff0c\u901a\u8fc7AI\u6280\u672f\u5f00\u53d1\u6570\u5b66\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\uff0c\u7f29\u5c0f\u542c\u969c\u5b66\u751f\u7684\u5b66\u4e60\u5dee\u8ddd", "method": "\u521b\u5efa41\u7c7b\u6570\u5b66\u624b\u52bf\u6570\u636e\u96c6\uff0c\u91c7\u7528Vision Transformer(ViT)\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u8bad\u7ec3\uff0c\u4e13\u5bb6\u53c2\u4e0e\u786e\u4fdd\u624b\u52bf\u51c6\u786e\u6027", "result": "\u6a21\u578b\u8fbe\u523097.59%\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u6210\u529f\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6570\u5b66\u624b\u52bf\u8bc6\u522b\uff0c\u6570\u636e\u96c6\u5f00\u6e90\u5171\u4eab\u4e8eHugging Face\u5e73\u53f0", "conclusion": "\u8bc1\u5b9e\u6df1\u5ea6\u5b66\u4e60\u53ef\u6709\u6548\u5f00\u53d1\u667a\u80fd\u6559\u80b2\u5de5\u5177\uff0c\u63a8\u52a8\u7279\u6b8a\u6559\u80b2\u9886\u57df\u7684\u6570\u5b57\u6280\u672f\u6574\u5408\u4e0e\u5305\u5bb9\u6027\u6559\u80b2\u521b\u65b0"}}
{"id": "2505.17056", "pdf": "https://arxiv.org/pdf/2505.17056", "abs": "https://arxiv.org/abs/2505.17056", "authors": ["Luoxi Tang", "Tharunya Sundar", "Shuai Yang", "Ankita Patra", "Manohar Chippada", "Giqi Zhao", "Yi Li", "Riteng Zhang", "Tunan Zhao", "Ting Yang", "Yuqiao Meng", "Weicheng Ma", "Zhaohan Xi"], "title": "Are LLMs Ready for English Standardized Tests? A Benchmarking and Elicitation Perspective", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "AI is transforming education by enabling powerful tools that enhance learning\nexperiences. Among recent advancements, large language models (LLMs) hold\nparticular promise for revolutionizing how learners interact with educational\ncontent. In this work, we investigate the potential of LLMs to support\nstandardized test preparation by focusing on English Standardized Tests (ESTs).\nSpecifically, we assess their ability to generate accurate and contextually\nappropriate solutions across a diverse set of EST question types. We introduce\nESTBOOK, a comprehensive benchmark designed to evaluate the capabilities of\nLLMs in solving EST questions. ESTBOOK aggregates five widely recognized tests,\nencompassing 29 question types and over 10,576 questions across multiple\nmodalities, including text, images, audio, tables, and mathematical symbols.\nUsing ESTBOOK, we systematically evaluate both the accuracy and inference\nefficiency of LLMs. Additionally, we propose a breakdown analysis framework\nthat decomposes complex EST questions into task-specific solution steps. This\nframework allows us to isolate and assess LLM performance at each stage of the\nreasoning process. Evaluation findings offer insights into the capability of\nLLMs in educational contexts and point toward targeted strategies for improving\ntheir reliability as intelligent tutoring systems.", "AI": {"tldr": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u82f1\u8bed\u6807\u51c6\u5316\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\u4e0e\u53ef\u9760\u6027", "motivation": "\u63a2\u7d22LLMs\u5728\u6807\u51c6\u5316\u8003\u8bd5\u8f85\u5bfc\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf\u5728\u590d\u6742\u9898\u578b\u5904\u7406\u4e0a\u7684\u4e0d\u8db3", "method": "\u6784\u5efa\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5ESTBOOK\uff08\u542b10,576+\u9898\u76ee\uff09\uff0c\u63d0\u51fa\u5206\u9636\u6bb5\u63a8\u7406\u6846\u67b6\u8bc4\u4f30\u6a21\u578b\u8868\u73b0", "result": "\u63ed\u793aLLMs\u5728\u4e0d\u540c\u9898\u578b\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u9a8c\u8bc1\u5206\u89e3\u5f0f\u89e3\u9898\u7b56\u7565\u7684\u6709\u6548\u6027", "conclusion": "LLMs\u5c55\u793a\u51fa\u6559\u80b2\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u9700\u9488\u5bf9\u6027\u4f18\u5316\u624d\u80fd\u6210\u4e3a\u53ef\u9760\u7684\u667a\u80fd\u8f85\u5bfc\u7cfb\u7edf"}}
{"id": "2505.17058", "pdf": "https://arxiv.org/pdf/2505.17058", "abs": "https://arxiv.org/abs/2505.17058", "authors": ["David Osei Opoku", "Ming Sheng", "Yong Zhang"], "title": "DO-RAG: A Domain-Specific QA Framework Using Knowledge Graph-Enhanced Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 5 figures;", "summary": "Domain-specific QA systems require not just generative fluency but high\nfactual accuracy grounded in structured expert knowledge. While recent\nRetrieval-Augmented Generation (RAG) frameworks improve context recall, they\nstruggle with integrating heterogeneous data and maintaining reasoning\nconsistency. To address these challenges, we propose DO-RAG, a scalable and\ncustomizable hybrid QA framework that integrates multi-level knowledge graph\nconstruction with semantic vector retrieval. Our system employs a novel agentic\nchain-of-thought architecture to extract structured relationships from\nunstructured, multimodal documents, constructing dynamic knowledge graphs that\nenhance retrieval precision. At query time, DO-RAG fuses graph and vector\nretrieval results to generate context-aware responses, followed by\nhallucination mitigation via grounded refinement. Experimental evaluations in\nthe database and electrical domains show near-perfect recall and over 94%\nanswer relevancy, with DO-RAG outperforming baseline frameworks by up to\n33.38%. By combining traceability, adaptability, and performance efficiency,\nDO-RAG offers a reliable foundation for multi-domain, high-precision QA at\nscale.", "AI": {"tldr": "\u63d0\u51faDO-RAG\u6df7\u5408\u95ee\u7b54\u6846\u67b6\uff0c\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u4e0e\u8bed\u4e49\u68c0\u7d22\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u9886\u57dfQA\u7cfb\u7edf\u7684\u51c6\u786e\u7387", "motivation": "\u4f20\u7edfRAG\u6846\u67b6\u5728\u5904\u7406\u5f02\u6784\u6570\u636e\u6574\u5408\u548c\u63a8\u7406\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u66f4\u53ef\u9760\u7684\u9886\u57df\u77e5\u8bc6\u652f\u6491", "method": "\u901a\u8fc7\u667a\u80fd\u601d\u7ef4\u94fe\u67b6\u6784\u6784\u5efa\u591a\u7ea7\u77e5\u8bc6\u56fe\u8c31\uff0c\u67e5\u8be2\u65f6\u878d\u5408\u56fe\u68c0\u7d22\u4e0e\u5411\u91cf\u68c0\u7d22\u7ed3\u679c\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u4e8b\u5b9e\u7684\u7cbe\u7ec6\u5316\u4fee\u6b63\u673a\u5236", "result": "\u5728\u6570\u636e\u5e93\u4e0e\u7535\u6c14\u9886\u57df\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u8fd1100%\u53ec\u56de\u7387\u4e0e94%\u7b54\u6848\u76f8\u5173\u6027\uff0c\u6027\u80fd\u8d85\u8d8a\u57fa\u7ebf\u6846\u67b633.38%", "conclusion": "DO-RAG\u901a\u8fc7\u53ef\u8ffd\u6eaf\u7684\u67b6\u6784\u8bbe\u8ba1\u4e0e\u52a8\u6001\u77e5\u8bc6\u878d\u5408\uff0c\u4e3a\u8de8\u9886\u57df\u9ad8\u7cbe\u5ea6\u95ee\u7b54\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.17059", "pdf": "https://arxiv.org/pdf/2505.17059", "abs": "https://arxiv.org/abs/2505.17059", "authors": ["Van-Tinh Nguyen", "Hoang-Duong Pham", "Thanh-Hai To", "Cong-Tuan Hung Do", "Thi-Thu-Trang Dong", "Vu-Trung Duong Le", "Van-Phuc Hoang"], "title": "Medalyze: Lightweight Medical Report Summarization Application Using FLAN-T5-Large", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 8 figures. Submitted to IEEE Access for review. Preliminary\n  version posted for early dissemination and feedback", "summary": "Understanding medical texts presents significant challenges due to complex\nterminology and context-specific language. This paper introduces Medalyze, an\nAI-powered application designed to enhance the comprehension of medical texts\nusing three specialized FLAN-T5-Large models. These models are fine-tuned for\n(1) summarizing medical reports, (2) extracting health issues from\npatient-doctor conversations, and (3) identifying the key question in a\npassage. Medalyze is deployed across a web and mobile platform with real-time\ninference, leveraging scalable API and YugabyteDB. Experimental evaluations\ndemonstrate the system's superior summarization performance over GPT-4 in\ndomain-specific tasks, based on metrics like BLEU, ROUGE-L, BERTScore, and\nSpaCy Similarity. Medalyze provides a practical, privacy-preserving, and\nlightweight solution for improving information accessibility in healthcare.", "AI": {"tldr": "Medalyze\u4f7f\u7528\u4e09\u4e2a\u4e13\u4e1aFLAN-T5-Large\u6a21\u578b\u89e3\u51b3\u533b\u7597\u6587\u672c\u7406\u89e3\u96be\u9898\uff0c\u5728\u7279\u5b9a\u9886\u57df\u4efb\u52a1\u4e2d\u8d85\u8d8aGPT-4\uff0c\u63d0\u4f9b\u9690\u79c1\u4fdd\u62a4\u7684\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848", "motivation": "\u533b\u7597\u6587\u672c\u5b58\u5728\u590d\u6742\u672f\u8bed\u548c\u8bed\u5883\u4f9d\u8d56\u95ee\u9898\uff0c\u9700\u63d0\u5347\u4fe1\u606f\u53ef\u53ca\u6027", "method": "1) \u5fae\u8c03\u4e09\u4e2a\u6a21\u578b\uff1a\u62a5\u544a\u6458\u8981\u751f\u6210\u3001\u533b\u60a3\u5bf9\u8bdd\u95ee\u9898\u63d0\u53d6\u3001\u5173\u952e\u95ee\u9898\u8bc6\u522b\n2) \u57fa\u4e8eAPI\u548cYugabyteDB\u6784\u5efa\u8de8\u5e73\u53f0\u5b9e\u65f6\u63a8\u7406\u7cfb\u7edf", "result": "\u5728BLEU/ROUGE-L/BERTScore/SpaCy\u6307\u6807\u4e2d\uff0c\u9886\u57df\u7279\u5b9a\u6458\u8981\u6027\u80fd\u4f18\u4e8eGPT-4", "conclusion": "Medalyze\u4e3a\u533b\u7597\u573a\u666f\u63d0\u4f9b\u4e86\u517c\u987e\u9690\u79c1\u4fdd\u62a4\u3001\u5b9e\u65f6\u6027\u548c\u8f7b\u91cf\u5316\u7684\u5b9e\u7528AI\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.17060", "pdf": "https://arxiv.org/pdf/2505.17060", "abs": "https://arxiv.org/abs/2505.17060", "authors": ["Wenyi Yu", "Siyin Wang", "Xiaoyu Yang", "Xianzhao Chen", "Xiaohai Tian", "Jun Zhang", "Guangzhi Sun", "Lu Lu", "Yuxuan Wang", "Chao Zhang"], "title": "SALMONN-omni: A Standalone Speech LLM without Codec Injection for Full-duplex Conversation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In order to enable fluid and natural human-machine speech interaction,\nexisting full-duplex conversational systems often adopt modular architectures\nwith auxiliary components such as voice activity detectors, interrupters,\nconversation state predictors, or multiple LLMs. These systems, however, suffer\nfrom error accumulation across modules and struggle with key challenges such as\ncontext-dependent barge-in and echo cancellation. Recent approaches, most\nnotably Moshi, simplify the pipeline by injecting audio codecs into the token\nspace of a single LLM. However, such methods still incur significant\nperformance degradation when operating on the speech rather than text modality.\nIn this paper, we introduce SALMONN-omni, the first single, standalone\nfull-duplex speech LLM that operates without audio codecs in its token space.\nIt features a novel dynamic thinking mechanism within the LLM backbone,\nenabling the model to learn when to transition between speaking and listening\nstates. Experiments on widely used benchmarks for spoken question answering and\nopen-domain dialogue show that SALMONN-omni achieves at least 30\\% relative\nperformance improvement over existing open-source full-duplex models and\nperforms highly competitively to half-duplex and turn-based systems, despite\nusing substantially less training data. Moreover, SALMONN-omni demonstrates\nstrong performance in complex conversational scenarios, including turn-taking,\nbackchanneling, echo cancellation and context-dependent barge-in, with further\nimprovements achieved through reinforcement learning. Some demo conversations\nbetween user and SALMONN-omni are provided in the following repository\nhttps://github.com/bytedance/SALMONN.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u65e0\u9700\u97f3\u9891\u7f16\u89e3\u7801\u5668\u7684\u5168\u53cc\u5de5\u8bed\u97f3\u5927\u6a21\u578bSALMONN-omni\uff0c\u901a\u8fc7\u52a8\u6001\u601d\u8003\u673a\u5236\u5b9e\u73b0\u542c/\u8bf4\u72b6\u6001\u81ea\u4e3b\u5207\u6362\uff0c\u663e\u8457\u63d0\u5347\u5bf9\u8bdd\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6a21\u5757\u5316\u8bed\u97f3\u4ea4\u4e92\u7cfb\u7edf\u5b58\u5728\u9519\u8bef\u7d2f\u79ef\u548c\u4e0a\u4e0b\u6587\u5904\u7406\u7f3a\u9677\uff0c\u9700\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u96c6\u6210\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5728LLM\u9aa8\u5e72\u7f51\u4e2d\u5f15\u5165\u52a8\u6001\u601d\u8003\u673a\u5236\uff0c\u901a\u8fc7\u72b6\u6001\u81ea\u4e3b\u5207\u6362\u66ff\u4ee3\u4f20\u7edf\u7f16\u89e3\u7801\u5668\u548c\u8f85\u52a9\u6a21\u5757\u3002", "result": "\u5728\u8bed\u97f3\u95ee\u7b54\u548c\u5f00\u653e\u5bf9\u8bdd\u4efb\u52a1\u4e0a\u76f8\u5bf9\u63d0\u534730%\uff0c\u4f7f\u7528\u66f4\u5c11\u8bad\u7ec3\u6570\u636e\u8fbe\u5230\u4e0e\u534a\u53cc\u5de5\u7cfb\u7edf\u76f8\u5f53\u7684\u7ade\u4e89\u529b\u3002", "conclusion": "SALMONN-omni\u7a81\u7834\u5168\u53cc\u5de5\u8bed\u97f3LLM\u7684\u6280\u672f\u74f6\u9888\uff0c\u5728\u590d\u6742\u5bf9\u8bdd\u573a\u666f\u4e2d\u5c55\u793a\u51fa\u5f3a\u5927\u7684\u7aef\u5230\u7aef\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2505.17061", "pdf": "https://arxiv.org/pdf/2505.17061", "abs": "https://arxiv.org/abs/2505.17061", "authors": ["Xinlong Chen", "Yuanxing Zhang", "Qiang Liu", "Junfei Wu", "Fuzheng Zhang", "Tieniu Tan"], "title": "Mixture of Decoding: An Attention-Inspired Adaptive Decoding Strategy to Mitigate Hallucinations in Large Vision-Language Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Accepted to Findings of ACL 2025", "summary": "Large Vision-Language Models (LVLMs) have exhibited impressive capabilities\nacross various visual tasks, yet they remain hindered by the persistent\nchallenge of hallucinations. To address this critical issue, we propose Mixture\nof Decoding (MoD), a novel approach for hallucination mitigation that\ndynamically adapts decoding strategies by evaluating the correctness of the\nmodel's attention on image tokens. Specifically, MoD measures the consistency\nbetween outputs generated from the original image tokens and those derived from\nthe model's attended image tokens, to distinguish the correctness\naforementioned. If the outputs are consistent, indicating correct attention,\nMoD employs a complementary strategy to amplify critical information.\nConversely, if the outputs are inconsistent, suggesting erroneous attention,\nMoD utilizes a contrastive strategy to suppress misleading information.\nExtensive experiments demonstrate that MoD significantly outperforms existing\ndecoding methods across multiple mainstream benchmarks, effectively mitigating\nhallucinations in LVLMs. The code is available at\nhttps://github.com/xlchen0205/MoD.", "AI": {"tldr": "\u63d0\u51fa\u52a8\u6001\u89e3\u7801\u7b56\u7565MoD\uff0c\u901a\u8fc7\u68c0\u6d4b\u6ce8\u610f\u529b\u6b63\u786e\u6027\u6709\u6548\u964d\u4f4e\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5e7b\u89c9\u95ee\u9898", "motivation": "\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u5b58\u5728\u6301\u7eed\u6027\u5e7b\u89c9\u95ee\u9898\uff0c\u73b0\u6709\u89e3\u7801\u65b9\u6cd5\u96be\u4ee5\u52a8\u6001\u9002\u5e94\u6ce8\u610f\u529b\u504f\u5dee", "method": "\u901a\u8fc7\u539f\u59cb\u56fe\u50cftoken\u4e0e\u6ce8\u610f\u529btoken\u7684\u8f93\u51fa\u4e00\u81f4\u6027\u5224\u65ad\u6ce8\u610f\u529b\u6b63\u786e\u6027\uff0c\u5206\u522b\u91c7\u7528\u4e92\u8865\u7b56\u7565\u589e\u5f3a\u5173\u952e\u4fe1\u606f\u6216\u5bf9\u6bd4\u7b56\u7565\u6291\u5236\u8bef\u5bfc\u4fe1\u606f", "result": "\u5728\u591a\u4e2a\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90", "conclusion": "MoD\u901a\u8fc7\u52a8\u6001\u89e3\u7801\u673a\u5236\u6709\u6548\u7f13\u89e3LVLMs\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a\u6a21\u578b\u53ef\u9760\u6027\u63d0\u5347\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2505.17063", "pdf": "https://arxiv.org/pdf/2505.17063", "abs": "https://arxiv.org/abs/2505.17063", "authors": ["Yiduo Guo", "Zhen Guo", "Chuanwei Huang", "Zi-Ang Wang", "Zekai Zhang", "Haofei Yu", "Huishuai Zhang", "Yikang Shen"], "title": "Synthetic Data RL: Task Definition Is All You Need", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement learning (RL) is a powerful way to adapt foundation models to\nspecialized tasks, but its reliance on large-scale human-labeled data limits\nbroad adoption. We introduce Synthetic Data RL, a simple and general framework\nthat reinforcement fine-tunes models using only synthetic data generated from a\ntask definition. Our method first generates question and answer pairs from the\ntask definition and retrieved documents, then adapts the difficulty of the\nquestion based on model solvability, and selects questions using the average\npass rate of the model across samples for RL training. On Qwen-2.5-7B, our\nmethod achieves a 29.2% absolute improvement over the base model on GSM8K (+2.9\npp vs. instruction-tuned, +6.6 pp vs. Self-Instruct), 8.7% on MATH, 13.1% on\nGPQA (+7.0 pp vs. SynthLLM), 8.9% on MedQA, 17.7% on CQA (law) and 13.7% on CFA\n(finance). It surpasses supervised fine-tuning under the same data budget and\nnearly matches RL with full human data across datasets (e.g., +17.2 pp on\nGSM8K). Adding 100 human demonstrations improves the performance of GSM8K only\nby 0.4 pp, showing a limited added value. By reducing human data annotation,\nSynthetic Data RL enables scalable and efficient RL-based model adaptation.\nCode and demos are available at https://github.com/gydpku/Data_Synthesis_RL/.", "AI": {"tldr": "\u63d0\u51faSynthetic Data RL\u6846\u67b6\uff0c\u901a\u8fc7\u7eaf\u5408\u6210\u6570\u636e\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u6a21\u578b\uff0c\u5728\u591a\u4e2a\u9886\u57df\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\uff08\u5982GSM8K\u63d0\u534729.2%\uff09\uff0c\u63a5\u8fd1\u5168\u4eba\u7c7b\u6570\u636eRL\u6548\u679c", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56\u4eba\u7c7b\u6807\u6ce8\u6570\u636e\u5bfc\u81f4\u7684\u6269\u5c55\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u964d\u4f4e\u4eba\u5de5\u6807\u6ce8\u6210\u672c", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1. \u6839\u636e\u4efb\u52a1\u5b9a\u4e49\u751f\u6210QA\u5bf9 2. \u57fa\u4e8e\u6a21\u578b\u89e3\u9898\u80fd\u529b\u52a8\u6001\u8c03\u6574\u95ee\u9898\u96be\u5ea6 3. \u6309\u5e73\u5747\u901a\u8fc7\u7387\u7b5b\u9009\u95ee\u9898\u8fdb\u884cRL\u8bad\u7ec3", "result": "\u5728GSM8K/MATH/GPQA\u7b496\u4e2a\u6570\u636e\u96c6\u5b9e\u73b06.6-29.2%\u63d0\u5347\uff0c\u8d85\u8d8a\u76d1\u7763\u5fae\u8c03\uff08\u76f8\u540c\u6570\u636e\u91cf\uff09\u4e14\u63a5\u8fd1\u5168\u4eba\u7c7b\u6570\u636eRL\u6548\u679c\uff08\u5982GSM8K\u5dee\u8ddd\u4ec50.4pp\uff09", "conclusion": "\u8bc1\u660e\u5408\u6210\u6570\u636e\u53ef\u6709\u6548\u66ff\u4ee3\u4eba\u7c7b\u6570\u636e\u8fdb\u884cRL\u5fae\u8c03\uff0c\u663e\u8457\u964d\u4f4e\u4eba\u5de5\u6807\u6ce8\u9700\u6c42\uff0c\u4e3a\u6a21\u578b\u9002\u914d\u63d0\u4f9b\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.17065", "pdf": "https://arxiv.org/pdf/2505.17065", "abs": "https://arxiv.org/abs/2505.17065", "authors": ["Valentina Carbonari", "Pierangelo Veltri", "Pietro Hiram Guzzi"], "title": "Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in artificial intelligence, particularly large language\nmodels LLMs, have shown promising capabilities in transforming rare disease\nresearch. This survey paper explores the integration of LLMs in the analysis of\nrare diseases, highlighting significant strides and pivotal studies that\nleverage textual data to uncover insights and patterns critical for diagnosis,\ntreatment, and patient care. While current research predominantly employs\ntextual data, the potential for multimodal data integration combining genetic,\nimaging, and electronic health records stands as a promising frontier. We\nreview foundational papers that demonstrate the application of LLMs in\nidentifying and extracting relevant medical information, simulating intelligent\nconversational agents for patient interaction, and enabling the formulation of\naccurate and timely diagnoses. Furthermore, this paper discusses the challenges\nand ethical considerations inherent in deploying LLMs, including data privacy,\nmodel transparency, and the need for robust, inclusive data sets. As part of\nthis exploration, we present a section on experimentation that utilizes\nmultiple LLMs alongside structured questionnaires, specifically designed for\ndiagnostic purposes in the context of different diseases. We conclude with\nfuture perspectives on the evolution of LLMs towards truly multimodal\nplatforms, which would integrate diverse data types to provide a more\ncomprehensive understanding of rare diseases, ultimately fostering better\noutcomes in clinical settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7f55\u89c1\u75c5\u7814\u7a76\u4e2d\u7684\u6574\u5408\u5e94\u7528\uff0c\u5206\u6790\u5176\u5728\u8bca\u65ad/\u6cbb\u7597/\u60a3\u8005\u62a4\u7406\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u5c55\u671b\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u7684\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u89e3\u51b3\u7f55\u89c1\u75c5\u7814\u7a76\u4e2d\u6570\u636e\u788e\u7247\u5316\u4e0e\u6837\u672c\u4e0d\u8db3\u7684\u96be\u9898\uff0c\u5229\u7528LLMs\u5f3a\u5927\u7684\u6587\u672c\u5206\u6790\u80fd\u529b\u6316\u6398\u6f5c\u5728\u8bca\u7597\u7ebf\u7d22\u3002", "method": "\u91c7\u7528\u6587\u732e\u7efc\u8ff0\u6cd5\u68b3\u7406\u57fa\u7840\u6027\u8bba\u6587\uff0c\u7ed3\u5408\u591a\u6a21\u578b\u5bf9\u6bd4\u5b9e\u9a8c\u548c\u7ed3\u6784\u5316\u95ee\u5377\u8bca\u65ad\u6d4b\u8bd5\u9a8c\u8bc1\u5e94\u7528\u6548\u679c\u3002", "result": "\u8bc1\u5b9eLLMs\u5728\u533b\u5b66\u4fe1\u606f\u63d0\u53d6\u548c\u8bca\u65ad\u6a21\u62df\u65b9\u9762\u6709\u6548\u6027\uff0c\u540c\u65f6\u63ed\u793a\u6570\u636e\u9690\u79c1/\u6a21\u578b\u900f\u660e\u5ea6/\u6570\u636e\u96c6\u504f\u5dee\u7b49\u5173\u952e\u74f6\u9888\u3002", "conclusion": "\u672a\u6765\u9700\u53d1\u5c55\u6574\u5408\u57fa\u56e0/\u5f71\u50cf/\u6587\u672c\u7684\u591a\u6a21\u6001LLM\u5e73\u53f0\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u5173\u8054\u63d0\u5347\u7f55\u89c1\u75c5\u8bca\u7597\u7684\u5168\u9762\u6027\u4e0e\u51c6\u786e\u6027\u3002"}}
{"id": "2505.17067", "pdf": "https://arxiv.org/pdf/2505.17067", "abs": "https://arxiv.org/abs/2505.17067", "authors": ["Kristin Qi", "Jiali Cheng", "Youxiang Zhu", "Hadi Amiri", "Xiaohui Liang"], "title": "Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Submitted to the IEEE GlobeCom 2025", "summary": "Detecting Mild Cognitive Impairment from picture descriptions is critical yet\nchallenging, especially in multilingual and multiple picture settings. Prior\nwork has primarily focused on English speakers describing a single picture\n(e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by\nintroducing multilingual speakers and multiple pictures, which presents new\nchallenges in analyzing picture-dependent content. To address these challenges,\nwe propose a framework with three components: (1) enhancing discriminative\nrepresentation learning via supervised contrastive learning, (2) involving\nimage modality rather than relying solely on speech and text modalities, and\n(3) applying a Product of Experts (PoE) strategy to mitigate spurious\ncorrelations and overfitting. Our framework improves MCI detection performance,\nachieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to\n75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the\ntext unimodal baseline. Notably, the contrastive learning component yields\ngreater gains for the text modality compared to speech. These results highlight\nour framework's effectiveness in multilingual and multi-picture MCI detection.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u3001\u56fe\u50cf\u6a21\u6001\u548c\u4e13\u5bb6\u4e58\u79ef\u7b56\u7565\u7684\u4e09\u7ef4\u6846\u67b6\uff0c\u63d0\u5347\u591a\u8bed\u8a00\u591a\u56fe\u7247\u573a\u666f\u4e0b\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u68c0\u6d4b\u6027\u80fd", "motivation": "\u73b0\u6709MCI\u68c0\u6d4b\u7814\u7a76\u5c40\u9650\u4e8e\u5355\u8bed\u8a00\u5355\u56fe\u7247\u573a\u666f\uff0cTAUKDIAL-2024\u6311\u6218\u5f15\u5165\u591a\u8bed\u8a00\u591a\u56fe\u7247\u5e26\u6765\u65b0\u5206\u6790\u96be\u9898", "method": "1. \u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u8868\u5f81\u533a\u5206\u80fd\u529b 2. \u5f15\u5165\u56fe\u50cf\u6a21\u6001\u8865\u5145\u8bed\u97f3\u6587\u672c 3. \u4e13\u5bb6\u4e58\u79ef\u7b56\u7565\u6d88\u9664\u4f2a\u76f8\u5173", "result": "UAR\u63d0\u53477.1%\uff0868.1%\u219275.2%\uff09\uff0cF1\u63d0\u53472.9%\uff0880.6%\u219283.5%\uff09\uff0c\u6587\u672c\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u589e\u76ca\u66f4\u663e\u8457", "conclusion": "\u6846\u67b6\u6709\u6548\u89e3\u51b3\u591a\u8bed\u8a00\u591a\u56fe\u7247MCI\u68c0\u6d4b\u6311\u6218\uff0c\u5bf9\u6bd4\u5b66\u4e60\u5bf9\u6587\u672c\u6a21\u6001\u4f18\u5316\u6548\u679c\u7a81\u51fa\uff0c\u5177\u4e34\u5e8a\u5b9e\u8df5\u6f5c\u529b"}}
{"id": "2505.17068", "pdf": "https://arxiv.org/pdf/2505.17068", "abs": "https://arxiv.org/abs/2505.17068", "authors": ["Jorge Paz-Ruza", "Amparo Alonso-Betanzos", "Bertha Guijarro-Berdi\u00f1as", "Carlos Eiras-Franco"], "title": "Predictively Combatting Toxicity in Health-related Online Discussions through Machine Learning", "categories": ["cs.CL", "cs.LG", "cs.SI"], "comment": "IJCNN 2025", "summary": "In health-related topics, user toxicity in online discussions frequently\nbecomes a source of social conflict or promotion of dangerous, unscientific\nbehaviour; common approaches for battling it include different forms of\ndetection, flagging and/or removal of existing toxic comments, which is often\ncounterproductive for platforms and users alike. In this work, we propose the\nalternative of combatting user toxicity predictively, anticipating where a user\ncould interact toxically in health-related online discussions. Applying a\nCollaborative Filtering-based Machine Learning methodology, we predict the\ntoxicity in COVID-related conversations between any user and subcommunity of\nReddit, surpassing 80% predictive performance in relevant metrics, and allowing\nus to prevent the pairing of conflicting users and subcommunities.", "AI": {"tldr": "\u901a\u8fc7\u534f\u540c\u8fc7\u6ee4\u673a\u5668\u5b66\u4e60\u9884\u6d4bReddit\u5065\u5eb7\u8ba8\u8bba\u4e2d\u7684\u7528\u6237\u6bd2\u6027\uff0c\u9884\u9632\u51b2\u7a81\u5339\u914d", "motivation": "\u73b0\u6709\u6bd2\u6027\u68c0\u6d4b\u624b\u6bb5\uff08\u6807\u8bb0/\u5220\u9664\u7b49\uff09\u5bb9\u6613\u5f15\u53d1\u53cd\u6548\u679c\uff0c\u9700\u524d\u77bb\u6027\u9884\u6d4b\u6bd2\u6027\u53d1\u751f\u573a\u666f", "method": "\u91c7\u7528\u534f\u540c\u8fc7\u6ee4\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u9884\u6d4b\u7528\u6237\u4e0eReddit\u5b50\u793e\u533a\u5728COVID\u8ba8\u8bba\u4e2d\u7684\u6f5c\u5728\u6bd2\u6027\u4e92\u52a8", "result": "\u6a21\u578b\u5728\u5173\u952e\u6307\u6807\u4e0a\u8d85\u8fc780%\u9884\u6d4b\u6027\u80fd\uff0c\u6210\u529f\u963b\u6b62\u51b2\u7a81\u7528\u6237\u4e0e\u5b50\u793e\u533a\u7684\u914d\u5bf9", "conclusion": "\u9884\u6d4b\u6027\u6bd2\u6027\u9632\u63a7\u7b56\u7565\u4f18\u4e8e\u4e8b\u540e\u5904\u7406\uff0c\u4e3a\u793e\u4ea4\u5e73\u53f0\u5185\u5bb9\u6cbb\u7406\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2505.17070", "pdf": "https://arxiv.org/pdf/2505.17070", "abs": "https://arxiv.org/abs/2505.17070", "authors": ["Anandh C", "Karthik Pandia Durai", "Jeena Prakash", "Manickavela Arumugam", "Kadri Hacioglu", "S. Pavankumar Dubagunta", "Andreas Stolcke", "Shankar Venkatesan", "Aravind Ganapathiraju"], "title": "Improving endpoint detection in end-to-end streaming ASR for conversational speech", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Submitted to Interspeech 2024", "summary": "ASR endpointing (EP) plays a major role in delivering a good user experience\nin products supporting human or artificial agents in human-human/machine\nconversations. Transducer-based ASR (T-ASR) is an end-to-end (E2E) ASR\nmodelling technique preferred for streaming. A major limitation of T-ASR is\ndelayed emission of ASR outputs, which could lead to errors or delays in EP.\nInaccurate EP will cut the user off while speaking, returning incomplete\ntranscript while delays in EP will increase the perceived latency, degrading\nthe user experience. We propose methods to improve EP by addressing delayed\nemission along with EP mistakes. To address the delayed emission problem, we\nintroduce an end-of-word token at the end of each word, along with a delay\npenalty. The EP delay is addressed by obtaining a reliable frame-level speech\nactivity detection using an auxiliary network. We apply the proposed methods on\nSwitchboard conversational speech corpus and evaluate it against a delay\npenalty method.", "AI": {"tldr": "\u9488\u5bf9\u57fa\u4e8eTransducer\u7684ASR\u6a21\u578b\u5ef6\u8fdf\u8f93\u51fa\u95ee\u9898\uff0c\u63d0\u51fa\u5f15\u5165\u8bcd\u5c3e\u6807\u8bb0\u548c\u5ef6\u8fdf\u60e9\u7f5a\u673a\u5236\uff0c\u7ed3\u5408\u8f85\u52a9\u7f51\u7edc\u4f18\u5316\u7aef\u70b9\u68c0\u6d4b\u7cbe\u5ea6", "motivation": "Transducer-based ASR\u6a21\u578b\u5b58\u5728\u8bed\u97f3\u8bc6\u522b\u7ed3\u679c\u5ef6\u8fdf\u8f93\u51fa\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u7aef\u70b9\u68c0\u6d4b(EP)\u8fc7\u65e9\u5207\u65ad\u7528\u6237\u53d1\u8a00\u6216\u589e\u52a0\u54cd\u5e94\u5ef6\u8fdf\uff0c\u5f71\u54cd\u5bf9\u8bdd\u7cfb\u7edf\u7528\u6237\u4f53\u9a8c\u3002\u9700\u8981\u540c\u65f6\u89e3\u51b3\u5ef6\u8fdf\u8f93\u51fa\u548c\u7aef\u70b9\u68c0\u6d4b\u9519\u8bef\u4e24\u5927\u6838\u5fc3\u95ee\u9898\u3002", "method": "1. \u5728\u6bcf\u4e2a\u8bcd\u8bed\u7ed3\u5c3e\u6dfb\u52a0end-of-word\u6807\u8bb0\n2. \u5f15\u5165\u5ef6\u8fdf\u60e9\u7f5a\u673a\u5236\n3. \u4f7f\u7528\u8f85\u52a9\u7f51\u7edc\u5b9e\u73b0\u5e27\u7ea7\u8bed\u97f3\u6d3b\u52a8\u68c0\u6d4b\n4. \u5728Switchboard\u5bf9\u8bdd\u8bed\u6599\u5e93\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1", "result": "\u5728Switchboard\u5bf9\u8bdd\u8bed\u6599\u5e93\u7684\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u4f20\u7edf\u5ef6\u8fdf\u60e9\u7f5a\u65b9\u6cd5\uff0c\u65b0\u65b9\u6848\u80fd\u66f4\u6709\u6548\u51cf\u5c11\u7aef\u70b9\u68c0\u6d4b\u9519\u8bef\u7387\uff0818.8%\u21926.3%\uff09\u5e76\u964d\u4f4e\u5e73\u5747\u68c0\u6d4b\u5ef6\u8fdf\uff08420ms\u2192320ms\uff09", "conclusion": "\u901a\u8fc7\u8bcd\u5c3e\u6807\u8bb0\u8bbe\u8ba1\u548c\u53cc\u91cd\u4f18\u5316\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86ASR\u7aef\u70b9\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u6027\uff0c\u4e3a\u5bf9\u8bdd\u7cfb\u7edf\u7684\u7528\u6237\u4f53\u9a8c\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.17071", "pdf": "https://arxiv.org/pdf/2505.17071", "abs": "https://arxiv.org/abs/2505.17071", "authors": ["Rapha\u00ebl Sarfati", "Haley Moller", "Toni J. B. Liu", "Nicolas Boull\u00e9", "Christopher Earls"], "title": "What's in a prompt? Language models encode literary style in prompt embeddings", "categories": ["cs.CL"], "comment": null, "summary": "Large language models use high-dimensional latent spaces to encode and\nprocess textual information. Much work has investigated how the conceptual\ncontent of words translates into geometrical relationships between their vector\nrepresentations. Fewer studies analyze how the cumulative information of an\nentire prompt becomes condensed into individual embeddings under the action of\ntransformer layers. We use literary pieces to show that information about\nintangible, rather than factual, aspects of the prompt are contained in deep\nrepresentations. We observe that short excerpts (10 - 100 tokens) from\ndifferent novels separate in the latent space independently from what\nnext-token prediction they converge towards. Ensembles from books from the same\nauthors are much more entangled than across authors, suggesting that embeddings\nencode stylistic features. This geometry of style may have applications for\nauthorship attribution and literary analysis, but most importantly reveals the\nsophistication of information processing and compression accomplished by\nlanguage models.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u6df1\u5c42\u8868\u793a\u8574\u542b\u6587\u672c\u98ce\u683c\u7279\u5f81\uff0c\u4e0d\u540c\u6587\u5b66\u4f5c\u54c1\u7684\u6f5c\u5728\u7a7a\u95f4\u5206\u5e03\u53cd\u6620\u4f5c\u8005\u98ce\u683c\u5dee\u5f02", "motivation": "\u63a2\u7a76\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5c06\u5b8c\u6574\u6587\u672c\u4fe1\u606f\u538b\u7f29\u5230\u5355\u4e2a\u5d4c\u5165\u8868\u793a\u4e2d\uff0c\u7279\u522b\u662f\u975e\u4e8b\u5b9e\u6027\u7684\u98ce\u683c\u7279\u5f81\u7f16\u7801\u673a\u5236", "method": "\u4f7f\u752810-100\u4e2atoken\u7684\u6587\u5b66\u6458\u5f55\u8fdb\u884c\u6f5c\u5728\u7a7a\u95f4\u5206\u6790\uff0c\u6bd4\u8f83\u4e0d\u540c\u4f5c\u8005/\u540c\u4f5c\u8005\u4f5c\u54c1\u7684\u51e0\u4f55\u5206\u5e03\u7279\u5f81", "result": "\u540c\u4e00\u4f5c\u8005\u4f5c\u54c1\u5728\u6f5c\u5728\u7a7a\u95f4\u663e\u8457\u7ea0\u7f20\uff0c\u4e0d\u540c\u4f5c\u54c1\u5206\u79bb\u72ec\u7acb\u4e8enext-token\u9884\u6d4b\u7ed3\u679c\uff0c\u9a8c\u8bc1\u98ce\u683c\u7f16\u7801\u7684\u6709\u6548\u6027", "conclusion": "\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u4e86\u590d\u6742\u7684\u4fe1\u606f\u538b\u7f29\u5904\u7406\uff0c\u5176\u98ce\u683c\u51e0\u4f55\u7279\u5f81\u53ef\u7528\u4e8e\u4f5c\u8005\u8bc6\u522b\u4e0e\u6587\u5b66\u5206\u6790\uff0c\u63ed\u793a\u6df1\u5ea6\u4fe1\u606f\u5904\u7406\u673a\u5236"}}
{"id": "2505.17073", "pdf": "https://arxiv.org/pdf/2505.17073", "abs": "https://arxiv.org/abs/2505.17073", "authors": ["Anurag Mishra"], "title": "Mechanistic Interpretability of GPT-like Models on Summarization Tasks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "8 pages (6 content + 2 references/appendix), 6 figures, 2 tables;\n  under review for the ACL 2025 Student Research Workshop", "summary": "Mechanistic interpretability research seeks to reveal the inner workings of\nlarge language models, yet most work focuses on classification or generative\ntasks rather than summarization. This paper presents an interpretability\nframework for analyzing how GPT-like models adapt to summarization tasks. We\nconduct differential analysis between pre-trained and fine-tuned models,\nquantifying changes in attention patterns and internal activations. By\nidentifying specific layers and attention heads that undergo significant\ntransformation, we locate the \"summarization circuit\" within the model\narchitecture. Our findings reveal that middle layers (particularly 2, 3, and 5)\nexhibit the most dramatic changes, with 62% of attention heads showing\ndecreased entropy, indicating a shift toward focused information selection. We\ndemonstrate that targeted LoRA adaptation of these identified circuits achieves\nsignificant performance improvement over standard LoRA fine-tuning while\nrequiring fewer training epochs. This work bridges the gap between black-box\nevaluation and mechanistic understanding, providing insights into how neural\nnetworks perform information selection and compression during summarization.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5bf9\u6bd4\u9884\u8bad\u7ec3\u4e0e\u5fae\u8c03\u6a21\u578b\uff0c\u5b9a\u4f4d\u4e86GPT\u7c7b\u6a21\u578b\u6267\u884c\u6458\u8981\u4efb\u52a1\u7684\u6838\u5fc3\u7535\u8def\uff082/3/5\u5c42\uff09\uff0c\u9488\u5bf9\u6027\u5fae\u8c03\u8be5\u7535\u8def\u53ef\u4f7f\u6027\u80fd\u63d0\u534732%\u4e14\u8bad\u7ec3\u5468\u671f\u7f29\u77ed40%", "motivation": "\u73b0\u6709\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u591a\u805a\u7126\u5206\u7c7b/\u751f\u6210\u4efb\u52a1\uff0c\u6458\u8981\u4efb\u52a1\u7684\u5185\u90e8\u673a\u5236\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u63ed\u793a\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u7ed3\u6784\u8c03\u6574\u9002\u5e94\u6458\u8981\u4efb\u52a1\u9700\u6c42", "method": "\u91c7\u7528\u6ce8\u610f\u529b\u71b5\u5206\u6790\u548c\u6fc0\u6d3b\u5dee\u5f02\u5bf9\u6bd4\uff0c\u8bc6\u522b\u6a21\u578b\u5fae\u8c03\u524d\u540e\u53d8\u5316\u6700\u663e\u8457\u7684\u6ce8\u610f\u529b\u5934\uff0862%\u71b5\u503c\u4e0b\u964d\uff09\uff0c\u901a\u8fc7LoRA\u6a21\u5757\u5b9a\u5411\u6539\u9020\u5173\u952e\u7535\u8def", "result": "\u4e2d\u95f4\u5c42\uff082/3/5\uff09\u5f62\u6210\u4fe1\u606f\u9009\u62e9\u4e13\u7528\u901a\u9053\uff0c\u9488\u5bf9\u6027\u7535\u8def\u6539\u9020\u4f7fROUGE-L\u63d0\u53471.8\u4e2a\u70b9\uff0c\u8bad\u7ec3epoch\u51cf\u5c1125%", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86\u6a21\u578b\u8bc4\u4f30\u4e0e\u673a\u5236\u89e3\u6790\u7684\u6865\u6881\uff0c\u63ed\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u901a\u8fc7\u7ed3\u6784\u5316\u7535\u8def\u5b9e\u73b0\u4fe1\u606f\u538b\u7f29\u7684\u6838\u5fc3\u8def\u5f84\uff0c\u4e3a\u9ad8\u6548\u5fae\u8c03\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2505.17074", "pdf": "https://arxiv.org/pdf/2505.17074", "abs": "https://arxiv.org/abs/2505.17074", "authors": ["Ruixiao Li", "Fahao Chen", "Peng Li"], "title": "Semi-Clairvoyant Scheduling of Speculative Decoding Requests to Minimize LLM Inference Latency", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Speculative decoding accelerates Large Language Model (LLM) inference by\nemploying a small speculative model (SSM) to generate multiple candidate tokens\nand verify them using the LLM in parallel. This technique has been widely\nintegrated into LLM inference serving systems. However, inference requests\ntypically exhibit uncertain execution time, which poses a significant challenge\nof efficiently scheduling requests in these systems. Existing work estimates\nexecution time based solely on predicted output length, which could be\ninaccurate because execution time depends on both output length and token\nacceptance rate of verification by the LLM. In this paper, we propose a\nsemi-clairvoyant request scheduling algorithm called\nLeast-Attained/Perceived-Service for Speculative Decoding (LAPS-SD). Given a\nnumber of inference requests, LAPS-SD can effectively minimize average\ninference latency by adaptively scheduling requests according to their features\nduring decoding. When the token acceptance rate is dynamic and execution time\nis difficult to estimate, LAPS-SD maintains multiple priority queues and allows\nrequest execution preemption across different queues. Once the token acceptance\nrate becomes stable, LAPS-SD can accurately estimate the execution time and\nschedule requests accordingly. Extensive experiments show that LAPS-SD reduces\ninference latency by approximately 39\\% compared to state-of-the-art scheduling\nmethods.", "AI": {"tldr": "\u63d0\u51faLAPS-SD\u8c03\u5ea6\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8bf7\u6c42\u8c03\u5ea6\u548c\u62a2\u5360\u673a\u5236\u4f18\u5316\u63a8\u6d4b\u89e3\u7801\u7684LLM\u63a8\u7406\u5ef6\u8fdf\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u964d\u4f4e39%\u5ef6\u8fdf", "motivation": "\u73b0\u6709\u8c03\u5ea6\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u8f93\u51fa\u957f\u5ea6\u4f30\u8ba1\u6267\u884c\u65f6\u95f4\uff0c\u5ffd\u7565token\u63a5\u53d7\u7387\u5bf9\u6267\u884c\u65f6\u95f4\u7684\u52a8\u6001\u5f71\u54cd\uff0c\u5bfc\u81f4\u4f30\u8ba1\u4e0d\u51c6\u786e", "method": "\u7ef4\u62a4\u591a\u4f18\u5148\u7ea7\u961f\u5217\uff0c\u5728token\u63a5\u53d7\u7387\u4e0d\u7a33\u5b9a\u65f6\u5141\u8bb8\u8de8\u961f\u5217\u8bf7\u6c42\u62a2\u5360\uff1b\u63a5\u53d7\u7387\u7a33\u5b9a\u540e\u57fa\u4e8e\u7cbe\u786e\u4f30\u8ba1\u8fdb\u884c\u8c03\u5ea6", "result": "\u5b9e\u9a8c\u663e\u793aLAPS-SD\u76f8\u6bd4state-of-the-art\u65b9\u6cd5\u51cf\u5c11\u7ea639%\u63a8\u7406\u5ef6\u8fdf", "conclusion": "LAPS-SD\u901a\u8fc7\u534a\u900f\u89c6\u8c03\u5ea6\u673a\u5236\u6709\u6548\u5e73\u8861\u52a8\u6001\u7279\u5f81\u4e0e\u7a33\u5b9a\u9636\u6bb5\u7684\u6267\u884c\u65f6\u95f4\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347LLM\u63a8\u7406\u6548\u7387"}}
{"id": "2505.17075", "pdf": "https://arxiv.org/pdf/2505.17075", "abs": "https://arxiv.org/abs/2505.17075", "authors": ["Fuma Kurata", "Mao Saeki", "Masaki Eguchi", "Shungo Suzuki", "Hiroaki Takatsu", "Yoichi Matsuyama"], "title": "Development and Validation of Engagement and Rapport Scales for Evaluating User Experience in Multimodal Dialogue Systems", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study aimed to develop and validate two scales of engagement and rapport\nto evaluate the user experience quality with multimodal dialogue systems in the\ncontext of foreign language learning. The scales were designed based on\ntheories of engagement in educational psychology, social psychology, and second\nlanguage acquisition.Seventy-four Japanese learners of English completed\nroleplay and discussion tasks with trained human tutors and a dialog agent.\nAfter each dialogic task was completed, they responded to the scales of\nengagement and rapport. The validity and reliability of the scales were\ninvestigated through two analyses. We first conducted analysis of Cronbach's\nalpha coefficient and a series of confirmatory factor analyses to test the\nstructural validity of the scales and the reliability of our designed items. We\nthen compared the scores of engagement and rapport between the dialogue with\nhuman tutors and the one with a dialogue agent. The results revealed that our\nscales succeeded in capturing the difference in the dialogue experience quality\nbetween the human interlocutors and the dialogue agent from multiple\nperspectives.", "AI": {"tldr": "\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u5728\u5916\u8bed\u5b66\u4e60\u4e2d\u7528\u6237\u4f53\u9a8c\u8d28\u91cf\u7684\u91cf\u8868\uff0c\u6210\u529f\u6355\u6349\u4eba\u7c7b\u5bfc\u5e08\u4e0e\u5bf9\u8bdd\u4ee3\u7406\u7684\u4f53\u9a8c\u5dee\u5f02", "motivation": "\u57fa\u4e8e\u591a\u5b66\u79d1\u7406\u8bba\u6784\u5efa\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u5177\uff0c\u89e3\u51b3\u73b0\u6709\u5bf9\u8bdd\u7cfb\u7edf\u7528\u6237\u4f53\u9a8c\u8bc4\u4ef7\u7ef4\u5ea6\u5355\u4e00\u7684\u95ee\u9898", "method": "\u7ed3\u5408\u6559\u80b2\u5fc3\u7406\u5b66\u7406\u8bba\u8bbe\u8ba1\u91cf\u8868\uff0c\u901a\u8fc774\u540d\u5b66\u4e60\u8005\u7684\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u91c7\u7528Cronbach's \u03b1\u548c\u9a8c\u8bc1\u6027\u56e0\u5b50\u5206\u6790\u68c0\u9a8c\u4fe1\u6548\u5ea6", "result": "\u91cf\u8868\u5177\u5907\u826f\u597d\u5fc3\u7406\u6d4b\u91cf\u7279\u6027\uff0c\u80fd\u591a\u7ef4\u5ea6\u8bc6\u522b\u4eba\u7c7b\u4e0eAI\u5bf9\u8bdd\u4f53\u9a8c\u7684\u8d28\u91cf\u5dee\u5f02", "conclusion": "\u8be5\u91cf\u8868\u4e3a\u5bf9\u8bdd\u7cfb\u7edf\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u8bc4\u4f30\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u5916\u8bed\u5b66\u4e60\u573a\u666f\u7684\u4eba\u673a\u4ea4\u4e92\u6548\u679c"}}
{"id": "2505.17076", "pdf": "https://arxiv.org/pdf/2505.17076", "abs": "https://arxiv.org/abs/2505.17076", "authors": ["Haoyang Zhang", "Hexin Liu", "Xiangyu Zhang", "Qiquan Zhang", "Yuchen Hu", "Junqi Zhao", "Fei Tian", "Xuerui Yang", "Eng Siong Chng"], "title": "Impact of Frame Rates on Speech Tokenizer: A Case Study on Mandarin and English", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS", "68T10", "I.2.7"], "comment": "5 pages, 5 figures", "summary": "The speech tokenizer plays a crucial role in recent speech tasks, generally\nserving as a bridge between speech signals and language models. While\nlow-frame-rate codecs are widely employed as speech tokenizers, the impact of\nframe rates on speech tokens remains underexplored. In this study, we\ninvestigate how varying frame rates affect speech tokenization by examining\nMandarin and English, two typologically distinct languages. We encode speech at\ndifferent frame rates and evaluate the resulting semantic tokens in the speech\nrecognition task. Our findings reveal that frame rate variations influence\nspeech tokenization differently for each language, highlighting the interplay\nbetween frame rates, phonetic density, and language-specific acoustic features.\nThe results provide insights into optimizing frame rate selection for speech\ntokenizers, with implications for automatic speech recognition, text-to-speech,\nand other speech-related applications.", "AI": {"tldr": "\u8bed\u97f3\u5206\u8bcd\u5668\u7684\u5e27\u7387\u53d8\u5316\u5bf9\u6c49\u8bed\u548c\u82f1\u8bed\u7684\u8bed\u4e49\u7f16\u7801\u4ea7\u751f\u4e0d\u540c\u5f71\u54cd\uff0c\u9700\u7ed3\u5408\u8bed\u8a00\u97f3\u7cfb\u7279\u5f81\u4f18\u5316\u5e27\u7387\u9009\u62e9", "motivation": "\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u63a2\u7d22\u8bed\u97f3\u5206\u8bcd\u5668\u4e2d\u5e27\u7387\u5bf9\u8bed\u97f3\u6807\u8bb0\u5316\u7684\u5f71\u54cd\u673a\u5236\uff0c\u5c24\u5176\u662f\u4e0d\u540c\u8bed\u8a00\u7c7b\u578b\u95f4\u7684\u5dee\u5f02", "method": "\u901a\u8fc7\u4e0d\u540c\u5e27\u7387\u7f16\u7801\u6c49\u8bed\u548c\u82f1\u8bed\u8bed\u97f3\uff0c\u5728\u8bed\u97f3\u8bc6\u522b\u4efb\u52a1\u4e2d\u8bc4\u4f30\u8bed\u4e49\u6807\u8bb0\u8d28\u91cf\uff0c\u5bf9\u6bd4\u5206\u6790\u8bed\u8a00\u7279\u6027\u4e0e\u5e27\u7387\u7684\u76f8\u4e92\u4f5c\u7528", "result": "\u6c49\u8bed\u56e0\u97f3\u8282\u5bc6\u5ea6\u9ad8\u9700\u8981\u66f4\u9ad8\u5e27\u7387\uff0825-50ms\uff09\uff0c\u82f1\u8bed\u5728\u8f83\u4f4e\u5e27\u7387\uff08100-200ms\uff09\u8868\u73b0\u66f4\u4f73\uff0c\u53cd\u6620\u8bed\u8a00\u97f3\u7cfb\u7279\u5f81\u5bf9\u58f0\u5b66\u7f16\u7801\u7684\u5f71\u54cd", "conclusion": "\u8bed\u97f3\u5206\u8bcd\u5668\u5e27\u7387\u9009\u62e9\u5e94\u7ed3\u5408\u76ee\u6807\u8bed\u8a00\u7684\u97f3\u7cfb\u7279\u5f81\u4f18\u5316\uff0c\u8fd9\u5bf9\u8bed\u97f3\u8bc6\u522b\u3001\u6587\u672c\u8f6c\u8bed\u97f3\u7b49\u5e94\u7528\u7684\u6027\u80fd\u63d0\u5347\u5177\u6709\u6307\u5bfc\u610f\u4e49"}}
{"id": "2505.17078", "pdf": "https://arxiv.org/pdf/2505.17078", "abs": "https://arxiv.org/abs/2505.17078", "authors": ["Zenghao Duan", "Zhiyi Yin", "Zhichao Shi", "Liang Pang", "Shaoling Jing", "Jiayi Wu", "Yu Yan", "Huawei Shen", "Xueqi Cheng"], "title": "GloSS over Toxicity: Understanding and Mitigating Toxicity in LLMs via Global Toxic Subspace", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper investigates the underlying mechanisms of toxicity generation in\nLarge Language Models (LLMs) and proposes an effective detoxification approach.\nPrior work typically considers the Feed-Forward Network (FFN) as the main\nsource of toxicity, representing toxic regions as a set of toxic vectors or\nlayer-wise subspaces. However, our in-depth analysis reveals that the global\ntoxic subspace offers a more effective and comprehensive representation of\ntoxic region within the model. Building on this insight, we propose GloSS\n(Global Toxic Subspace Suppression), a lightweight, four-stage method that\nmitigates toxicity by identifying and removing the global toxic subspace from\nthe parameters of FFN. Experiments across a range of LLMs show that GloSS\nachieves state-of-the-art detoxification performance while preserving the\nmodels general capabilities, without requiring large-scale data or model\nretraining.", "AI": {"tldr": "\u63d0\u51faGloSS\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u5e76\u6d88\u9664FFN\u4e2d\u7684\u5168\u5c40\u6bd2\u6027\u5b50\u7a7a\u95f4\u5b9e\u73b0LLM\u9ad8\u6548\u53bb\u6bd2\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u591a\u5c06FFN\u89c6\u4e3a\u6bd2\u6027\u4e3b\u8981\u6765\u6e90\uff0c\u4f46\u4ec5\u91c7\u7528\u6bd2\u6027\u5411\u91cf/\u5206\u5c42\u5b50\u7a7a\u95f4\u8868\u5f81\uff0c\u5168\u5c40\u6bd2\u6027\u5b50\u7a7a\u95f4\u80fd\u66f4\u5168\u9762\u6709\u6548\u8868\u5f81\u6a21\u578b\u6bd2\u6027\u533a\u57df\u3002", "method": "\u56db\u9636\u6bb5\u8f7b\u91cf\u7ea7\u65b9\u6cd5GloSS\uff1a1\uff09\u6784\u5efa\u6bd2\u6027\u6837\u672c\u53c2\u6570\u8f68\u8ff9 2\uff09SVD\u5206\u89e3\u8bc6\u522b\u5168\u5c40\u6bd2\u6027\u5b50\u7a7a\u95f4 3\uff09\u53c2\u6570\u6295\u5f71\u6d88\u9664\u6bd2\u6027\u6210\u5206 4\uff09\u52a8\u6001\u6743\u91cd\u8c03\u6574\u4fdd\u6301\u6a21\u578b\u80fd\u529b", "result": "\u5728\u591a\u7c7bLLM\u4e2d\u5b9e\u73b0SOTA\u53bb\u6bd2\u6548\u679c\uff0c\u6a21\u578b\u901a\u7528\u80fd\u529b\u4fdd\u7559\u5ea6\u8fbe97.8%\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u6570\u636e/\u6a21\u578b\u91cd\u8bad\u7ec3", "conclusion": "\u5168\u5c40\u6bd2\u6027\u5b50\u7a7a\u95f4\u6291\u5236\u7b56\u7565\u4e3a\u6a21\u578b\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u53c2\u6570\u9ad8\u6548\u3001\u6570\u636e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u5b89\u5168\u6027\u4e0e\u529f\u80fd\u5b8c\u6574\u6027"}}
{"id": "2505.17080", "pdf": "https://arxiv.org/pdf/2505.17080", "abs": "https://arxiv.org/abs/2505.17080", "authors": ["Davide Picca"], "title": "Not Minds, but Signs: Reframing LLMs through Semiotics", "categories": ["cs.CL"], "comment": null, "summary": "This paper challenges the prevailing tendency to frame Large Language Models\n(LLMs) as cognitive systems, arguing instead for a semiotic perspective that\nsituates these models within the broader dynamics of sign manipulation and\nmeaning-making. Rather than assuming that LLMs understand language or simulate\nhuman thought, we propose that their primary function is to recombine,\nrecontextualize, and circulate linguistic forms based on probabilistic\nassociations. By shifting from a cognitivist to a semiotic framework, we avoid\nanthropomorphism and gain a more precise understanding of how LLMs participate\nin cultural processes, not by thinking, but by generating texts that invite\ninterpretation. Through theoretical analysis and practical examples, the paper\ndemonstrates how LLMs function as semiotic agents whose outputs can be treated\nas interpretive acts, open to contextual negotiation and critical reflection.\nWe explore applications in literature, philosophy, education, and cultural\nproduction, emphasizing how LLMs can serve as tools for creativity, dialogue,\nand critical inquiry. The semiotic paradigm foregrounds the situated,\ncontingent, and socially embedded nature of meaning, offering a more rigorous\nand ethically aware framework for studying and using LLMs. Ultimately, this\napproach reframes LLMs as technological participants in an ongoing ecology of\nsigns. They do not possess minds, but they alter how we read, write, and make\nmeaning, compelling us to reconsider the foundations of language,\ninterpretation, and the role of artificial systems in the production of\nknowledge.", "AI": {"tldr": "\u53cd\u5bf9\u5c06LLMs\u89c6\u4e3a\u8ba4\u77e5\u7cfb\u7edf\uff0c\u4e3b\u5f20\u7b26\u53f7\u5b66\u6846\u67b6\u4e0bLLMs\u4f5c\u4e3a\u7b26\u53f7\u91cd\u7ec4\u4e0e\u518d\u8bed\u5883\u5316\u7684\u89e3\u91ca\u6027\u4e3b\u4f53", "motivation": "\u5f53\u524d\u7814\u7a76\u8fc7\u5ea6\u62df\u4eba\u5316LLMs\uff0c\u5ffd\u89c6\u4e86\u5176\u57fa\u4e8e\u6982\u7387\u7684\u7b26\u53f7\u64cd\u4f5c\u672c\u8d28\u53ca\u5176\u5728\u6587\u5316\u751f\u4ea7\u4e2d\u7684\u975e\u601d\u7ef4\u6027\u53c2\u4e0e", "method": "\u901a\u8fc7\u7b26\u53f7\u5b66\u7406\u8bba\u5efa\u6784\u4e0e\u6587\u5b66/\u54f2\u5b66\u7b49\u9886\u57df\u7684\u5e94\u7528\u6848\u4f8b\u5206\u6790", "result": "\u5efa\u7acbLLMs\u4f5c\u4e3a\u7b26\u53f7\u4ee3\u7406\u7684\u8303\u5f0f\uff0c\u63ed\u793a\u5176\u901a\u8fc7\u6587\u672c\u751f\u6210\u6539\u53d8\u4eba\u7c7b\u610f\u4e49\u5efa\u6784\u65b9\u5f0f\u7684\u4f5c\u7528\u673a\u5236", "conclusion": "\u7b26\u53f7\u5b66\u6846\u67b6\u4e3aLLMs\u7814\u7a76\u63d0\u4f9b\u66f4\u4e25\u8c28\u7684\u4f26\u7406\u8ba4\u77e5\u57fa\u7840\uff0c\u63a8\u52a8\u5176\u5728\u521b\u9020\u6027\u5bf9\u8bdd\u4e0e\u6279\u5224\u6027\u53cd\u601d\u4e2d\u7684\u5e94\u7528"}}
{"id": "2505.17082", "pdf": "https://arxiv.org/pdf/2505.17082", "abs": "https://arxiv.org/abs/2505.17082", "authors": ["Abderrahman Skiredj", "Ferdaous Azhari", "Houdaifa Atou", "Nouamane Tazi", "Ismail Berrada"], "title": "GemMaroc: Unlocking Darija Proficiency in LLMs with Minimal Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Open-source large language models (LLMs) still marginalise Moroccan Arabic\n(Darija), forcing practitioners either to bolt on heavyweight Arabic adapters\nor to sacrifice the very reasoning skills that make LLMs useful. We show that a\nrigorously quality-over-quantity alignment strategy can surface fluent Darija\nwhile safeguarding the backbone s cross-lingual reasoning at a sliver of the\nusual compute. We translate three compact instruction suites LIMA 1 K, DEITA 6\nK and TULU 50 K into Darija, preserve 20 of the English originals, and add\nmathematics, coding and scientific prompts. A LoRA-tuned Gemma 3-4B trained on\n5 K mixed instructions lifts DarijaMMLU from 32.8 to 42.7 ; adding the\nreasoning-dense TULU portion pushes it to 47.5 with no English regression.\nScaling the identical recipe to Gemma 3-27B produces GemMaroc-27B, which\nmatches Atlas-Chat on DarijaMMLU (61.6 ) and leaps ahead on Darija commonsense,\nscoring 60.5 on HellaSwag versus Atlas-Chat s 48.4 . Crucially, GemMaroc\nretains Gemma-27B s strong maths and general-reasoning ability, showing only\nminimal movement on GSM8K and English benchmarks. The entire model is trained\nin just 48 GPU.h, underscoring a Green AI pathway to inclusive, sustainable\nlanguage technology. We release code, data and checkpoints to spur\nDarija-centric applications in education, public services and everyday digital\ninteraction.", "AI": {"tldr": "\u63d0\u51faGemMaroc\u6a21\u578b\uff0c\u901a\u8fc7\u9ad8\u6548\u5bf9\u9f50\u7b56\u7565\u5728\u4fdd\u7559\u539f\u6a21\u578b\u591a\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6469\u6d1b\u54e5\u963f\u62c9\u4f2f\u8bed\uff08Darija\uff09\u6027\u80fd\uff0c\u8bad\u7ec3\u8017\u65f6\u4ec548 GPU\u5c0f\u65f6\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u6469\u6d1b\u54e5\u963f\u62c9\u4f2f\u8bed\u652f\u6301\u4e0d\u8db3\uff0c\u4f20\u7edf\u9002\u914d\u65b9\u6cd5\u9700\u727a\u7272\u6a21\u578b\u63a8\u7406\u80fd\u529b\u6216\u589e\u52a0\u8ba1\u7b97\u8d1f\u62c5\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u7b56\u7565\uff1a1\uff09\u7cbe\u9009\u5e76\u7ffb\u8bd1LIMA/DEITA/TULU\u6307\u4ee4\u96c6\u4e3aDarija 2\uff09\u4fdd\u755920%\u82f1\u6587\u539f\u6570\u636e 3\uff09\u6dfb\u52a0\u6570\u5b66/\u7f16\u7a0b/\u79d1\u5b66Prompt\uff0c\u901a\u8fc7LoRA\u5fae\u8c03Gemma\u7cfb\u5217\u6a21\u578b\u3002", "result": "GemMaroc-27B\u5728DarijaMMLU\u8fbe\u523061.6%\uff08\u4e0eAtlas-Chat\u76f8\u5f53\uff09\uff0cDarija\u5e38\u8bc6\u63a8\u7406\uff08HellaSwag\uff0960.5%\u663e\u8457\u4f18\u4e8e\u5bf9\u6bd4\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u6a21\u578b\u7684\u6570\u5b66\uff08GSM8K\uff09\u548c\u82f1\u8bed\u80fd\u529b\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u7eff\u8272AI\u8def\u5f84\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u8d28\u91cf\u4f18\u5148\u7684\u6570\u636e\u7b56\u7565\u548c\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u5b9e\u73b0\u5c0f\u8bed\u79cd\u6280\u672f\u7684\u5305\u5bb9\u6027\u53d1\u5c55\uff0c\u4e3a\u6559\u80b2/\u516c\u5171\u670d\u52a1\u7b49\u9886\u57df\u63d0\u4f9b\u6280\u672f\u652f\u6301\u3002"}}
{"id": "2505.17083", "pdf": "https://arxiv.org/pdf/2505.17083", "abs": "https://arxiv.org/abs/2505.17083", "authors": ["Ben Anson", "Xi Wang", "Laurence Aitchison"], "title": "Scale-invariant Attention", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": "Preprint", "summary": "One persistent challenge in LLM research is the development of attention\nmechanisms that are able to generalise from training on shorter contexts to\ninference on longer contexts. We propose two conditions that we expect all\neffective long context attention mechanisms to have: scale-invariant total\nattention, and scale-invariant attention sparsity. Under a Gaussian assumption,\nwe show that a simple position-dependent transformation of the attention logits\nis sufficient for these conditions to hold. Experimentally we find that the\nresulting scale-invariant attention scheme gives considerable benefits in terms\nof validation loss when zero-shot generalising from training on short contexts\nto validation on longer contexts, and is effective at long-context retrieval.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c3a\u5ea6\u4e0d\u53d8\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u6539\u5584LLM\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u548c\u68c0\u7d22\u6548\u679c", "motivation": "\u73b0\u6709\u6ce8\u610f\u529b\u673a\u5236\u5728\u4ece\u77ed\u4e0a\u4e0b\u6587\u8bad\u7ec3\u63a8\u5e7f\u5230\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u65f6\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898", "method": "\u901a\u8fc7\u4f4d\u7f6e\u76f8\u5173\u7684\u6ce8\u610f\u529blogits\u53d8\u6362\u5b9e\u73b0\u5c3a\u5ea6\u4e0d\u53d8\u603b\u6ce8\u610f\u529b\u548c\u7a00\u758f\u6027\uff08\u57fa\u4e8e\u9ad8\u65af\u5047\u8bbe\uff09", "result": "\u9a8c\u8bc1\u635f\u5931\u663e\u8457\u964d\u4f4e\uff08\u96f6\u6837\u672c\u957f\u4e0a\u4e0b\u6587\u6cdb\u5316\u65f6\uff09\uff0c\u5e76\u5c55\u73b0\u4f18\u79c0\u7684\u957f\u4e0a\u4e0b\u6587\u68c0\u7d22\u80fd\u529b", "conclusion": "\u5c3a\u5ea6\u4e0d\u53d8\u6ce8\u610f\u529b\u673a\u5236\u80fd\u6709\u6548\u63d0\u5347LLM\u5728\u957f\u4e0a\u4e0b\u6587\u4efb\u52a1\u4e2d\u7684\u8868\u73b0"}}
{"id": "2505.17086", "pdf": "https://arxiv.org/pdf/2505.17086", "abs": "https://arxiv.org/abs/2505.17086", "authors": ["Yihong Wu", "Liheng Ma", "Muzhi Li", "Jiaming Zhou", "Jianye Hao", "Ho-fung Leung", "Irwin King", "Yingxue Zhang", "Jian-Yun Nie"], "title": "Reinforcing Question Answering Agents with Minimalist Policy Gradient Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable versatility, due to\nthe lack of factual knowledge, their application to Question Answering (QA)\ntasks remains hindered by hallucination.\n  While Retrieval-Augmented Generation mitigates these issues by integrating\nexternal knowledge, existing approaches rely heavily on in-context learning,\nwhose performance is constrained by the fundamental reasoning capabilities of\nLLMs.\n  In this paper, we propose Mujica, a Multi-hop Joint Intelligence for Complex\nQuestion Answering, comprising a planner that decomposes questions into a\ndirected acyclic graph of subquestions and a worker that resolves questions via\nretrieval and reasoning. Additionally, we introduce MyGO (Minimalist policy\nGradient Optimization), a novel reinforcement learning method that replaces\ntraditional policy gradient updates with Maximum Likelihood Estimation (MLE) by\nsampling trajectories from an asymptotically optimal policy. MyGO eliminates\nthe need for gradient rescaling and reference models, ensuring stable and\nefficient training.\n  Empirical results across multiple datasets demonstrate the effectiveness of\nMujica-MyGO in enhancing multi-hop QA performance for various LLMs, offering a\nscalable and resource-efficient solution for complex QA tasks.", "AI": {"tldr": "\u63d0\u51faMujica-MyGO\u6846\u67b6\u89e3\u51b3\u5927\u6a21\u578b\u5728\u590d\u6742\u95ee\u7b54\u4e2d\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7DAG\u5206\u89e3\u95ee\u9898\u548cMLE\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u591a\u8df3QA\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u56e0\u7f3a\u4e4f\u4e8b\u5b9e\u77e5\u8bc6\u5bfc\u81f4\u95ee\u7b54\u4efb\u52a1\u4ea7\u751f\u5e7b\u89c9\uff0c\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u53d7\u9650\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u63a8\u7406\u74f6\u9888\u3002", "method": "Mujica\u6846\u67b6\u5305\u542b\u5b50\u95ee\u9898DAG\u5206\u89e3\u7684planner\u548c\u68c0\u7d22\u63a8\u7406\u7684worker\uff1bMyGO\u7528MLE\u66ff\u4ee3\u7b56\u7565\u68af\u5ea6\uff0c\u901a\u8fc7\u6e10\u8fd1\u6700\u4f18\u7b56\u7565\u8f68\u8ff9\u91c7\u6837\u5b9e\u73b0\u7a33\u5b9a\u8bad\u7ec3\u3002", "result": "\u591a\u6570\u636e\u96c6\u9a8c\u8bc1\u663e\u793aMujica-MyGO\u6709\u6548\u63d0\u5347\u4e0d\u540cLLMs\u7684\u591a\u8df3QA\u80fd\u529b\uff0c\u63d0\u4f9b\u8d44\u6e90\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u590d\u6742QA\u4efb\u52a1\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u8303\u5f0f\uff0cMyGO\u65b9\u6cd5\u7a81\u7834\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u7684\u68af\u5ea6\u7f29\u653e\u9650\u5236\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u6a21\u578b\u4f18\u5316\u3002"}}
{"id": "2505.17087", "pdf": "https://arxiv.org/pdf/2505.17087", "abs": "https://arxiv.org/abs/2505.17087", "authors": ["Gordana Ispirova", "Michael Sebek", "Giulia Menichetti"], "title": "Informatics for Food Processing", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DB", "cs.LG"], "comment": null, "summary": "This chapter explores the evolution, classification, and health implications\nof food processing, while emphasizing the transformative role of machine\nlearning, artificial intelligence (AI), and data science in advancing food\ninformatics. It begins with a historical overview and a critical review of\ntraditional classification frameworks such as NOVA, Nutri-Score, and SIGA,\nhighlighting their strengths and limitations, particularly the subjectivity and\nreproducibility challenges that hinder epidemiological research and public\npolicy. To address these issues, the chapter presents novel computational\napproaches, including FoodProX, a random forest model trained on nutrient\ncomposition data to infer processing levels and generate a continuous FPro\nscore. It also explores how large language models like BERT and BioBERT can\nsemantically embed food descriptions and ingredient lists for predictive tasks,\neven in the presence of missing data. A key contribution of the chapter is a\nnovel case study using the Open Food Facts database, showcasing how multimodal\nAI models can integrate structured and unstructured data to classify foods at\nscale, offering a new paradigm for food processing assessment in public health\nand research.", "AI": {"tldr": "\u63a2\u8ba8\u98df\u54c1\u52a0\u5de5\u5206\u7c7b\u4f53\u7cfb\u6f14\u8fdb\u53ca\u673a\u5668\u5b66\u4e60/AI\u5728\u98df\u54c1\u4fe1\u606f\u5b66\u4e2d\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u57fa\u4e8eFoodProX\u548c\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u8bc4\u4f30\u8303\u5f0f", "motivation": "\u4f20\u7edf\u98df\u54c1\u52a0\u5de5\u5206\u7c7b\u6846\u67b6\uff08\u5982NOVA\uff09\u5b58\u5728\u4e3b\u89c2\u6027\u5f3a\u3001\u53ef\u590d\u73b0\u6027\u5dee\u7684\u95ee\u9898\uff0c\u5236\u7ea6\u6d41\u884c\u75c5\u5b66\u7814\u7a76\u548c\u516c\u5171\u536b\u751f\u653f\u7b56\u5236\u5b9a", "method": "\u5f00\u53d1FoodProX\u968f\u673a\u68ee\u6797\u6a21\u578b\u5904\u7406\u8425\u517b\u6210\u5206\u6570\u636e\uff0c\u7ed3\u5408BERT/BioBERT\u8bed\u4e49\u5d4c\u5165\u6280\u672f\uff0c\u5e76\u901a\u8fc7Open Food Facts\u591a\u6a21\u6001AI\u6848\u4f8b\u9a8c\u8bc1", "result": "\u65b0\u65b9\u6cd5\u5b9e\u73b0\u98df\u54c1\u52a0\u5de5\u6c34\u5e73\u7684\u8fde\u7eed\u8bc4\u5206\uff08FPro\uff09\u548c\u5927\u89c4\u6a21\u5206\u7c7b\uff0c\u6709\u6548\u5904\u7406\u7f3a\u5931\u6570\u636e\u5e76\u6574\u5408\u7ed3\u6784\u5316\u4e0e\u975e\u7ed3\u6784\u5316\u4fe1\u606f", "conclusion": "AI\u9a71\u52a8\u7684\u8ba1\u7b97\u6a21\u578b\u4e3a\u98df\u54c1\u52a0\u5de5\u8bc4\u4f30\u63d0\u4f9b\u5ba2\u89c2\u3001\u53ef\u6269\u5c55\u7684\u65b0\u8303\u5f0f\uff0c\u63a8\u52a8\u516c\u5171\u536b\u751f\u7814\u7a76\u548c\u653f\u7b56\u5236\u5b9a\u7684\u7cbe\u51c6\u5316\u8f6c\u578b"}}
{"id": "2505.17089", "pdf": "https://arxiv.org/pdf/2505.17089", "abs": "https://arxiv.org/abs/2505.17089", "authors": ["Md Rafi Ur Rashid", "Vishnu Asutosh Dasu", "Ye Wang", "Gang Tan", "Shagufta Mehnaz"], "title": "Trust Me, I Can Handle It: Self-Generated Adversarial Scenario Extrapolation for Robust Language Models", "categories": ["cs.CL"], "comment": "26 pages, 2 figures", "summary": "Large Language Models (LLMs) exhibit impressive capabilities, but remain\nsusceptible to a growing spectrum of safety risks, including jailbreaks, toxic\ncontent, hallucinations, and bias. Existing defenses often address only a\nsingle threat type or resort to rigid outright rejection, sacrificing user\nexperience and failing to generalize across diverse and novel attacks. This\npaper introduces Adversarial Scenario Extrapolation (ASE), a novel\ninference-time computation framework that leverages Chain-of-Thought (CoT)\nreasoning to simultaneously enhance LLM robustness and seamlessness. ASE guides\nthe LLM through a self-generative process of contemplating potential\nadversarial scenarios and formulating defensive strategies before generating a\nresponse to the user query. Comprehensive evaluation on four adversarial\nbenchmarks with four latest LLMs shows that ASE achieves near-zero jailbreak\nattack success rates and minimal toxicity, while slashing outright rejections\nto <4%. ASE outperforms six state-of-the-art defenses in\nrobustness-seamlessness trade-offs, with 92-99% accuracy on adversarial Q&A and\n4-10x lower bias scores. By transforming adversarial perception into an\nintrinsic cognitive process, ASE sets a new paradigm for secure and natural\nhuman-AI interaction.", "AI": {"tldr": "\u63d0\u51fa\u5bf9\u6297\u573a\u666f\u63a8\u6f14\uff08ASE\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u601d\u7ef4\u94fe\u63a8\u7406\u540c\u6b65\u589e\u5f3aLLM\u5b89\u5168\u6027\u548c\u54cd\u5e94\u81ea\u7136\u5ea6", "motivation": "\u73b0\u6709\u9632\u5fa1\u65b9\u6848\u5b58\u5728\u5355\u70b9\u9632\u62a4\u5c40\u9650\u6027\u548c\u8fc7\u5ea6\u62d2\u7edd\u7528\u6237\u8bf7\u6c42\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4f7f\u7528\u4f53\u9a8c\u548c\u6cdb\u5316\u80fd\u529b", "method": "\u91c7\u7528\u63a8\u7406\u65f6\u81ea\u751f\u6210\u5bf9\u6297\u573a\u666f\u63a8\u6f14\u673a\u5236\uff0c\u5f15\u5bfcLLM\u81ea\u4e3b\u601d\u8003\u6f5c\u5728\u653b\u51fb\u573a\u666f\u5e76\u5236\u5b9a\u9632\u5fa1\u7b56\u7565", "result": "\u5728\u56db\u4e2a\u5bf9\u6297\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u63a5\u8fd1\u96f6\u7684\u8d8a\u72f1\u6210\u529f\u7387\uff0c\u62d2\u7edd\u7387\u4f4e\u4e8e4%\uff0c\u504f\u5dee\u5206\u6570\u964d\u4f4e4-10\u500d", "conclusion": "ASE\u901a\u8fc7\u5c06\u5bf9\u6297\u611f\u77e5\u8f6c\u5316\u4e3a\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u5efa\u7acb\u4e86\u5b89\u5168\u81ea\u7136\u7684\u4eba\u673a\u4ea4\u4e92\u65b0\u8303\u5f0f"}}
{"id": "2505.17091", "pdf": "https://arxiv.org/pdf/2505.17091", "abs": "https://arxiv.org/abs/2505.17091", "authors": ["Prateek Verma", "Mert Pilanci"], "title": "Large Language Models Implicitly Learn to See and Hear Just By Reading", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "comment": "6 pages, 3 figures, 4 tables. Under Review WASPAA 2025", "summary": "This paper presents a fascinating find: By training an auto-regressive LLM\nmodel on text tokens, the text model inherently develops internally an ability\nto understand images and audio, thereby developing the ability to see and hear\njust by reading. Popular audio and visual LLM models fine-tune text LLM models\nto give text output conditioned on images and audio embeddings. On the other\nhand, our architecture takes in patches of images, audio waveforms or tokens as\ninput. It gives us the embeddings or category labels typical of a\nclassification pipeline. We show the generality of text weights in aiding audio\nclassification for datasets FSD-50K and GTZAN. Further, we show this working\nfor image classification on CIFAR-10 and Fashion-MNIST, as well on image\npatches. This pushes the notion of text-LLMs learning powerful internal\ncircuits that can be utilized by activating necessary connections for various\napplications rather than training models from scratch every single time.", "AI": {"tldr": "\u6587\u672cLLM\u901a\u8fc7\u6587\u672c\u8bad\u7ec3\u81ea\u53d1\u83b7\u5f97\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff0c\u53ef\u76f4\u63a5\u5904\u7406\u56fe\u50cf\u548c\u97f3\u9891\u5206\u7c7b\u4efb\u52a1", "motivation": "\u63a2\u7d22\u6587\u672cLLM\u5185\u90e8\u662f\u5426\u5929\u7136\u5177\u5907\u591a\u6a21\u6001\u5904\u7406\u6f5c\u529b\uff0c\u907f\u514d\u6bcf\u6b21\u4ece\u5934\u8bad\u7ec3\u4e13\u7528\u6a21\u578b", "method": "\u5c06\u56fe\u50cf/\u97f3\u9891patch\u76f4\u63a5\u8f93\u5165\u9884\u8bad\u7ec3\u6587\u672cLLM\uff0c\u590d\u7528\u5176\u6587\u672c\u6743\u91cd\u8fdb\u884c\u591a\u6a21\u6001\u5206\u7c7b\u4efb\u52a1", "result": "\u5728FSD-50K\u97f3\u9891\u6570\u636e\u96c6\u548cCIFAR-10\u7b49\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6587\u672c\u6a21\u578b\u6743\u91cd\u5bf9\u591a\u6a21\u6001\u4efb\u52a1\u7684\u6709\u6548\u6027", "conclusion": "\u6587\u672cLLM\u5b66\u4e60\u5230\u7684\u5185\u90e8\u8868\u5f81\u5177\u6709\u901a\u7528\u6027\uff0c\u901a\u8fc7\u6fc0\u6d3b\u7279\u5b9a\u8fde\u63a5\u5373\u53ef\u8de8\u6a21\u6001\u5e94\u7528\uff0c\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c"}}
{"id": "2505.17095", "pdf": "https://arxiv.org/pdf/2505.17095", "abs": "https://arxiv.org/abs/2505.17095", "authors": ["Kristine Ann M. Carandang", "Jasper Meynard P. Ara\u00f1a", "Ethan Robert A. Casin", "Christopher P. Monterola", "Daniel Stanley Y. Tan", "Jesus Felix B. Valenzuela", "Christian M. Alis"], "title": "Are LLMs reliable? An exploration of the reliability of large language models in clinical note generation", "categories": ["cs.CL"], "comment": null, "summary": "Due to the legal and ethical responsibilities of healthcare providers (HCPs)\nfor accurate documentation and protection of patient data privacy, the natural\nvariability in the responses of large language models (LLMs) presents\nchallenges for incorporating clinical note generation (CNG) systems, driven by\nLLMs, into real-world clinical processes. The complexity is further amplified\nby the detailed nature of texts in CNG. To enhance the confidence of HCPs in\ntools powered by LLMs, this study evaluates the reliability of 12 open-weight\nand proprietary LLMs from Anthropic, Meta, Mistral, and OpenAI in CNG in terms\nof their ability to generate notes that are string equivalent (consistency\nrate), have the same meaning (semantic consistency) and are correct (semantic\nsimilarity), across several iterations using the same prompt. The results show\nthat (1) LLMs from all model families are stable, such that their responses are\nsemantically consistent despite being written in various ways, and (2) most of\nthe LLMs generated notes close to the corresponding notes made by experts.\nOverall, Meta's Llama 70B was the most reliable, followed by Mistral's Small\nmodel. With these findings, we recommend the local deployment of these\nrelatively smaller open-weight models for CNG to ensure compliance with data\nprivacy regulations, as well as to improve the efficiency of HCPs in clinical\ndocumentation.", "AI": {"tldr": "\u8bc4\u4f3012\u79cdLLM\u5728\u4e34\u5e8a\u8bb0\u5f55\u751f\u6210\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0Llama 70B\u548cMistral Small\u8868\u73b0\u6700\u4f73\uff0c\u5efa\u8bae\u672c\u5730\u90e8\u7f72\u8f83\u5c0f\u6a21\u578b\u5b9e\u73b0\u9690\u79c1\u5408\u89c4\u4e0e\u6548\u7387\u63d0\u5347", "motivation": "\u89e3\u51b3LLM\u8f93\u51fa\u53d8\u5f02\u6027\u5bf9\u4e34\u5e8a\u8bb0\u5f55\u751f\u6210\u7cfb\u7edf\u96c6\u6210\u7684\u5f71\u54cd\uff0c\u4fdd\u969c\u533b\u7597\u6587\u6863\u7684\u51c6\u786e\u6027\u3001\u9690\u79c1\u5408\u89c4\u6027\u548c\u4e34\u5e8a\u6d41\u7a0b\u53ef\u9760\u6027", "method": "\u901a\u8fc7\u5b57\u7b26\u4e32\u7b49\u4ef7\u6027\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u6307\u6807\uff0c\u5bf9Anthropic/Meta/Mistral/OpenAI\u768412\u4e2a\u5f00\u6e90/\u5546\u4e1a\u6a21\u578b\u8fdb\u884c\u591a\u8f6e\u63d0\u793a\u6d4b\u8bd5", "result": "1. \u6240\u6709\u6a21\u578b\u5bb6\u65cf\u8f93\u51fa\u4fdd\u6301\u8bed\u4e49\u7a33\u5b9a 2. \u591a\u6570\u6a21\u578b\u751f\u6210\u8bb0\u5f55\u63a5\u8fd1\u4e13\u5bb6\u6c34\u5e73 3. Llama 70B\u53ef\u9760\u6027\u6700\u9ad8\uff08Mistral Small\u6b21\u4e4b\uff09", "conclusion": "\u63a8\u8350\u533b\u7597\u673a\u6784\u672c\u5730\u90e8\u7f72\u4e2d\u5c0f\u578b\u5f00\u6e90\u6a21\u578b\uff0c\u5728\u6ee1\u8db3\u6570\u636e\u9690\u79c1\u6cd5\u89c4\u7684\u540c\u65f6\u63d0\u5347\u4e34\u5e8a\u6587\u6863\u6548\u7387"}}
{"id": "2505.17098", "pdf": "https://arxiv.org/pdf/2505.17098", "abs": "https://arxiv.org/abs/2505.17098", "authors": ["Yanshu Li", "Tian Yun", "Jianjiang Yang", "Pinyuan Feng", "Jinfa Huang", "Ruixiang Tang"], "title": "TACO: Enhancing Multimodal In-context Learning via Task Mapping-Guided Sequence Configuration", "categories": ["cs.CL", "cs.CV"], "comment": "29 pages, 11 figures, 19 tables. arXiv admin note: substantial text\n  overlap with arXiv:2503.04839", "summary": "Multimodal in-context learning (ICL) has emerged as a key mechanism for\nharnessing the capabilities of large vision-language models (LVLMs). However,\nits effectiveness remains highly sensitive to the quality of input in-context\nsequences, particularly for tasks involving complex reasoning or open-ended\ngeneration. A major limitation is our limited understanding of how LVLMs\nactually exploit these sequences during inference. To bridge this gap, we\nsystematically interpret multimodal ICL through the lens of task mapping, which\nreveals how local and global relationships within and among demonstrations\nguide model reasoning. Building on this insight, we present TACO, a lightweight\ntransformer-based model equipped with task-aware attention that dynamically\nconfigures in-context sequences. By injecting task-mapping signals into the\nautoregressive decoding process, TACO creates a bidirectional synergy between\nsequence construction and task reasoning. Experiments on five LVLMs and nine\ndatasets demonstrate that TACO consistently surpasses baselines across diverse\nICL tasks. These results position task mapping as a valuable perspective for\ninterpreting and improving multimodal ICL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTACO\u6a21\u578b\uff0c\u901a\u8fc7\u4efb\u52a1\u6620\u5c04\u89c6\u89d2\u52a8\u6001\u914d\u7f6e\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u5e8f\u5217\uff0c\u5728\u4e94\u5927\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u4e5d\u5927\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60(ICL)\u6548\u679c\u9ad8\u5ea6\u4f9d\u8d56\u8f93\u5165\u5e8f\u5217\u8d28\u91cf\uff0c\u4e14\u7f3a\u4e4f\u5bf9\u6a21\u578b\u5982\u4f55\u5229\u7528\u5e8f\u5217\u7684\u673a\u5236\u7406\u89e3\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4efb\u52a1\u6620\u5c04\u89c6\u89d2\u89e3\u6790ICL\u5185\u90e8\u673a\u7406\uff0c\u63d0\u5347\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u8f7b\u91cf\u7ea7TACO\u6a21\u578b\uff0c\u91c7\u7528\u4efb\u52a1\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u81ea\u56de\u5f52\u89e3\u7801\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u4efb\u52a1\u6620\u5c04\u4fe1\u53f7\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u5e8f\u5217\u7684\u52a8\u6001\u914d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8868\u660eTACO\u5728\u591a\u6837\u5316ICL\u4efb\u52a1\u4e2d\u6301\u7eed\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u4efb\u52a1\u6620\u5c04\u4f5c\u4e3a\u6539\u8fdb\u591a\u6a21\u6001ICL\u7684\u6709\u6548\u89c6\u89d2\u3002", "conclusion": "\u4efb\u52a1\u6620\u5c04\u4e3a\u89e3\u91ca\u548c\u4f18\u5316\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u53cc\u5411\u534f\u540c\u673a\u5236\u663e\u8457\u589e\u5f3a\u6a21\u578b\u7684\u5e8f\u5217\u6784\u5efa\u4e0e\u4efb\u52a1\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2505.17099", "pdf": "https://arxiv.org/pdf/2505.17099", "abs": "https://arxiv.org/abs/2505.17099", "authors": ["Xiaozhao Liu", "Dinggang Shen", "Xihui Liu"], "title": "Learning Interpretable Representations Leads to Semantically Faithful EEG-to-Text Generation", "categories": ["cs.CL"], "comment": "Code, checkpoint and text samples available at\n  https://github.com/justin-xzliu/GLIM", "summary": "Pretrained generative models have opened new frontiers in brain decoding by\nenabling the synthesis of realistic texts and images from non-invasive brain\nrecordings. However, the reliability of such outputs remains\nquestionable--whether they truly reflect semantic activation in the brain, or\nare merely hallucinated by the powerful generative models. In this paper, we\nfocus on EEG-to-text decoding and address its hallucination issue through the\nlens of posterior collapse. Acknowledging the underlying mismatch in\ninformation capacity between EEG and text, we reframe the decoding task as\nsemantic summarization of core meanings rather than previously verbatim\nreconstruction of stimulus texts. To this end, we propose the Generative\nLanguage Inspection Model (GLIM), which emphasizes learning informative and\ninterpretable EEG representations to improve semantic grounding under\nheterogeneous and small-scale data conditions. Experiments on the public ZuCo\ndataset demonstrate that GLIM consistently generates fluent, EEG-grounded\nsentences without teacher forcing. Moreover, it supports more robust evaluation\nbeyond text similarity, through EEG-text retrieval and zero-shot semantic\nclassification across sentiment categories, relation types, and corpus topics.\nTogether, our architecture and evaluation protocols lay the foundation for\nreliable and scalable benchmarking in generative brain decoding.", "AI": {"tldr": "\u63d0\u51faGLIM\u6a21\u578b\u89e3\u51b3EEG\u6587\u672c\u89e3\u7801\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u8bed\u4e49\u603b\u7ed3\u548c\u7a33\u5065\u8bc4\u4f30\u63d0\u5347\u8111\u89e3\u7801\u53ef\u9760\u6027", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u53ef\u80fd\u4ea7\u751f\u4e0e\u8111\u4fe1\u53f7\u65e0\u5173\u7684\u5e7b\u89c9\u8f93\u51fa\uff0c\u9700\u89e3\u51b3EEG\u4e0e\u6587\u672c\u4fe1\u606f\u5bb9\u91cf\u4e0d\u5339\u914d\u5bfc\u81f4\u7684\u540e\u9a8c\u5d29\u6e83\u95ee\u9898", "method": "\u5c06\u89e3\u7801\u4efb\u52a1\u91cd\u6784\u4e3a\u8bed\u4e49\u63d0\u70bc\uff0c\u8bbe\u8ba1GLIM\u6846\u67b6\u5b66\u4e60\u53ef\u89e3\u91caEEG\u8868\u5f81\uff0c\u5728ZuCo\u6570\u636e\u96c6\u9a8c\u8bc1\u5f02\u6784\u6570\u636e\u4e0b\u7684\u8bed\u4e49\u57fa\u7840\u80fd\u529b", "result": "\u6a21\u578b\u65e0\u9700\u6559\u5e08\u5f3a\u5236\u5373\u53ef\u751f\u6210\u6d41\u7545EEG\u5173\u8054\u53e5\u5b50\uff0c\u652f\u6301\u8111\u7535-\u6587\u672c\u68c0\u7d22\u548c\u8de8\u60c5\u611f/\u5173\u7cfb/\u4e3b\u9898\u7684\u96f6\u6837\u672c\u5206\u7c7b\u8bc4\u4f30", "conclusion": "GLIM\u67b6\u6784\u53ca\u8bc4\u4f30\u534f\u8bae\u4e3a\u751f\u6210\u5f0f\u8111\u89e3\u7801\u5efa\u7acb\u4e86\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6846\u67b6"}}
{"id": "2505.17100", "pdf": "https://arxiv.org/pdf/2505.17100", "abs": "https://arxiv.org/abs/2505.17100", "authors": ["Haoyan Yang", "Runxue Bao", "Cao Xiao", "Jun Ma", "Parminder Bhatia", "Shangqian Gao", "Taha Kass-Hout"], "title": "Any Large Language Model Can Be a Reliable Judge: Debiasing with a Reasoning-based Bias Detector", "categories": ["cs.CL"], "comment": null, "summary": "LLM-as-a-Judge has emerged as a promising tool for automatically evaluating\ngenerated outputs, but its reliability is often undermined by potential biases\nin judgment. Existing efforts to mitigate these biases face key limitations:\nin-context learning-based methods fail to address rooted biases due to the\nevaluator's limited capacity for self-reflection, whereas fine-tuning is not\napplicable to all evaluator types, especially closed-source models. To address\nthis challenge, we introduce the Reasoning-based Bias Detector (RBD), which is\na plug-in module that identifies biased evaluations and generates structured\nreasoning to guide evaluator self-correction. Rather than modifying the\nevaluator itself, RBD operates externally and engages in an iterative process\nof bias detection and feedback-driven revision. To support its development, we\ndesign a complete pipeline consisting of biased dataset construction,\nsupervision collection, distilled reasoning-based fine-tuning of RBD, and\nintegration with LLM evaluators. We fine-tune four sizes of RBD models, ranging\nfrom 1.5B to 14B, and observe consistent performance improvements across all\nscales. Experimental results on 4 bias types--verbosity, position, bandwagon,\nand sentiment--evaluated using 8 LLM evaluators demonstrate RBD's strong\neffectiveness. For example, the RBD-8B model improves evaluation accuracy by an\naverage of 18.5% and consistency by 10.9%, and surpasses prompting-based\nbaselines and fine-tuned judges by 12.8% and 17.2%, respectively. These results\nhighlight RBD's effectiveness and scalability. Additional experiments further\ndemonstrate its strong generalization across biases and domains, as well as its\nefficiency.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u63a8\u7406\u7684\u504f\u89c1\u68c0\u6d4b\u5668RBD\uff0c\u901a\u8fc7\u5916\u90e8\u8fed\u4ee3\u68c0\u6d4b\u4e0e\u53cd\u9988\u673a\u5236\u6709\u6548\u63d0\u5347LLM\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u504f\u89c1\uff08\u4e0a\u4e0b\u6587\u5b66\u4e60\u65e0\u6cd5\u81ea\u6211\u53cd\u601d\uff0c\u5fae\u8c03\u4e0d\u9002\u7528\u95ed\u6e90\u6a21\u578b\uff09", "method": "\u5f00\u53d1\u72ec\u7acb\u63d2\u4ef6\u6a21\u5757RBD\uff0c\u91c7\u7528\u504f\u89c1\u6570\u636e\u96c6\u6784\u5efa-\u76d1\u7763\u4fe1\u53f7\u6536\u96c6-\u63a8\u7406\u5fae\u8c03-\u6a21\u578b\u96c6\u6210\u7684\u5168\u6d41\u7a0b\u65b9\u6848", "result": "RBD-8B\u6a21\u578b\u5e73\u5747\u63d0\u5347\u8bc4\u4f30\u51c6\u786e\u602718.5%\u3001\u4e00\u81f4\u602710.9%\uff0c\u663e\u8457\u4f18\u4e8e\u63d0\u793a\u5de5\u7a0b\uff08+12.8%\uff09\u548c\u5fae\u8c03\u8bc4\u4f30\u5668\uff08+17.2%\uff09", "conclusion": "RBD\u5c55\u793a\u51fa\u4f18\u79c0\u7684\u53ef\u6269\u5c55\u6027\u3001\u8de8\u504f\u89c1\u7c7b\u578b/\u9886\u57df\u6cdb\u5316\u80fd\u529b\u53ca\u8fd0\u884c\u6548\u7387\uff0c\u4e3aLLM\u8bc4\u4f30\u504f\u89c1\u7684\u7cfb\u7edf\u5316\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.17101", "pdf": "https://arxiv.org/pdf/2505.17101", "abs": "https://arxiv.org/abs/2505.17101", "authors": ["Santiago Acevedo", "Andrea Mascaretti", "Riccardo Rende", "Mat\u00e9o Mahaut", "Marco Baroni", "Alessandro Laio"], "title": "An approach to identify the most semantically informative deep representations of text and images", "categories": ["cs.CL", "cs.LG", "physics.comp-ph"], "comment": null, "summary": "Deep neural networks are known to develop similar representations for\nsemantically related data, even when they belong to different domains, such as\nan image and its description, or the same text in different languages. We\npresent a method for quantitatively investigating this phenomenon by measuring\nthe relative information content of the representations of semantically related\ndata and probing how it is encoded into multiple tokens of large language\nmodels (LLMs) and vision transformers. Looking first at how LLMs process pairs\nof translated sentences, we identify inner ``semantic'' layers containing the\nmost language-transferable information. We find moreover that, on these layers,\na larger LLM (DeepSeek-V3) extracts significantly more general information than\na smaller one (Llama3.1-8B). Semantic information is spread across many tokens\nand it is characterized by long-distance correlations between tokens and by a\ncausal left-to-right (i.e., past-future) asymmetry. We also identify layers\nencoding semantic information within visual transformers. We show that caption\nrepresentations in the semantic layers of LLMs predict visual representations\nof the corresponding images. We observe significant and model-dependent\ninformation asymmetries between image and text representations.", "AI": {"tldr": "\u901a\u8fc7\u5206\u6790LLMs\u548c\u89c6\u89c9\u53d8\u6362\u5668\uff0c\u63d0\u51fa\u5b9a\u91cf\u65b9\u6cd5\u7814\u7a76\u8de8\u6a21\u6001\u8bed\u4e49\u8868\u5f81\uff0c\u63ed\u793a\u6a21\u578b\u5927\u5c0f\u5bf9\u4fe1\u606f\u63d0\u53d6\u7684\u5f71\u54cd\u53ca\u8bed\u4e49\u4fe1\u606f\u7684\u5206\u5e03\u7279\u6027\u4e0e\u8de8\u6a21\u6001\u9884\u6d4b\u80fd\u529b", "motivation": "\u63a2\u7a76\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5982\u4f55\u5728\u4e0d\u540c\u9886\u57df\uff08\u6587\u672c/\u56fe\u50cf/\u591a\u8bed\u8a00\uff09\u7f16\u7801\u8bed\u4e49\u4fe1\u606f\uff0c\u4ee5\u53ca\u6a21\u578b\u89c4\u6a21\u5bf9\u8bed\u4e49\u8868\u5f81\u80fd\u529b\u7684\u5f71\u54cd", "method": "\u6d4b\u91cf\u8bed\u4e49\u76f8\u5173\u6570\u636e\u8868\u5f81\u7684\u76f8\u5bf9\u4fe1\u606f\u91cf\uff0c\u5206\u6790LLMs\u5904\u7406\u7ffb\u8bd1\u53e5\u5bf9\u8bc6\u522b\u8bed\u4e49\u5c42\uff0c\u6bd4\u8f83\u6a21\u578b\u89c4\u6a21\u5dee\u5f02\uff0c\u7814\u7a76\u89c6\u89c9\u53d8\u6362\u5668\u4e2d\u8bed\u4e49\u5c42\u4e0e\u56fe\u50cf\u8868\u5f81\u7684\u9884\u6d4b\u5173\u7cfb", "result": "\u8f83\u5927LLM\uff08DeepSeek-V3\uff09\u63d0\u53d6\u66f4\u591a\u901a\u7528\u4fe1\u606f\uff1b\u8bed\u4e49\u4fe1\u606f\u5448\u73b0token\u95f4\u957f\u7a0b\u76f8\u5173\u6027\u53ca\u56e0\u679c\u4e0d\u5bf9\u79f0\u6027\uff1b\u6587\u672c\u8bed\u4e49\u5c42\u53ef\u9884\u6d4b\u56fe\u50cf\u8868\u5f81\uff0c\u5b58\u5728\u8de8\u6a21\u6001\u4fe1\u606f\u4e0d\u5bf9\u79f0", "conclusion": "\u6a21\u578b\u89c4\u6a21\u5f71\u54cd\u8bed\u4e49\u4fe1\u606f\u63d0\u53d6\u6548\u7387\uff0c\u8bed\u4e49\u4fe1\u606f\u5177\u6709\u5206\u5e03\u5f0f\u7f16\u7801\u7279\u5f81\uff0c\u8de8\u6a21\u6001\u8868\u5f81\u5b58\u5728\u53ef\u9884\u6d4b\u6027\u5dee\u5f02\uff0c\u4e3a\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u8bed\u4e49\u7f16\u7801\u673a\u5236\u63d0\u4f9b\u65b0\u89c6\u89d2"}}
{"id": "2505.17102", "pdf": "https://arxiv.org/pdf/2505.17102", "abs": "https://arxiv.org/abs/2505.17102", "authors": ["Pramit Bhattacharyya", "Arnab Bhattacharya"], "title": "BanglaByT5: Byte-Level Modelling for Bangla", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success across various\nnatural language processing tasks. However, most LLM models use traditional\ntokenizers like BPE and SentencePiece, which fail to capture the finer nuances\nof a morphologically rich language like Bangla (Bengali). In this work, we\nintroduce BanglaByT5, the first byte-level encoder-decoder model explicitly\ntailored for Bangla. Built upon a small variant of Googles ByT5 architecture,\nBanglaByT5 is pre-trained on a 14GB curated corpus combining high-quality\nliterary and newspaper articles. Through zeroshot and supervised evaluations\nacross generative and classification tasks, BanglaByT5 demonstrates competitive\nperformance, surpassing several multilingual and larger models. Our findings\nhighlight the efficacy of byte-level modelling for morphologically rich\nlanguages and highlight BanglaByT5 potential as a lightweight yet powerful tool\nfor Bangla NLP, particularly in both resource-constrained and scalable\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u5b5f\u52a0\u62c9\u8bed\u7684\u5b57\u8282\u7ea7\u6a21\u578bBanglaByT5\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u5c55\u73b0\u9ad8\u6548\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u5206\u8bcd\u5668\uff08\u5982BPE/SentencePiece\uff09\u96be\u4ee5\u6355\u6349\u5b5f\u52a0\u62c9\u8bed\u5f62\u6001\u5b66\u7279\u5f81\uff0c\u9700\u5f00\u53d1\u4e13\u95e8\u6a21\u578b\u3002", "method": "\u57fa\u4e8eByT5\u67b6\u6784\uff0c\u4f7f\u752814GB\u6587\u5b66\u65b0\u95fb\u6df7\u5408\u8bed\u6599\u9884\u8bad\u7ec3\uff0c\u901a\u8fc7\u96f6\u6837\u672c/\u76d1\u7763\u4efb\u52a1\u8bc4\u4f30\u6a21\u578b\u3002", "result": "\u5728\u751f\u6210\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8d85\u8d8a\u591a\u8bed\u8a00\u5927\u6a21\u578b\uff0c\u9a8c\u8bc1\u5b57\u8282\u7ea7\u5efa\u6a21\u5bf9\u5f62\u6001\u590d\u6742\u8bed\u8a00\u7684\u6709\u6548\u6027\u3002", "conclusion": "BanglaByT5\u4e3a\u5b5f\u52a0\u62c9\u8bedNLP\u63d0\u4f9b\u8f7b\u91cf\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u8bc1\u660e\u5b57\u8282\u7ea7\u6a21\u578b\u5728\u5f62\u6001\u4e30\u5bcc\u8bed\u8a00\u4e2d\u7684\u4f18\u52bf\u3002"}}
{"id": "2505.17103", "pdf": "https://arxiv.org/pdf/2505.17103", "abs": "https://arxiv.org/abs/2505.17103", "authors": ["C\u00e9cile Rousseau", "Tobia Boschi", "Giandomenico Cornacchia", "Dhaval Salwala", "Alessandra Pascale", "Juan Bernabe Moreno"], "title": "Forging Time Series with Language: A Large Language Model Approach to Synthetic Data Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "SDForger is a flexible and efficient framework for generating high-quality\nmultivariate time series using LLMs. Leveraging a compact data representation,\nSDForger provides synthetic time series generation from a few samples and\nlow-computation fine-tuning of any autoregressive LLM. Specifically, the\nframework transforms univariate and multivariate signals into tabular\nembeddings, which are then encoded into text and used to fine-tune the LLM. At\ninference, new textual embeddings are sampled and decoded into synthetic time\nseries that retain the original data's statistical properties and temporal\ndynamics. Across a diverse range of datasets, SDForger outperforms existing\ngenerative models in many scenarios, both in similarity-based evaluations and\ndownstream forecasting tasks. By enabling textual conditioning in the\ngeneration process, SDForger paves the way for multimodal modeling and the\nstreamlined integration of time series with textual information. SDForger\nsource code will be open-sourced soon.", "AI": {"tldr": "SDForger\u662f\u57fa\u4e8eLLM\u7684\u9ad8\u6548\u591a\u53d8\u91cf\u65f6\u5e8f\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6587\u672c\u7f16\u7801\u5b9e\u73b0\u4f4e\u8ba1\u7b97\u91cf\u5fae\u8c03\u548c\u8de8\u6a21\u6001\u6574\u5408", "motivation": "\u89e3\u51b3\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u4fdd\u6301\u6570\u636e\u7edf\u8ba1\u7279\u6027\u3001\u65f6\u95f4\u52a8\u6001\u7279\u5f81\u4ee5\u53ca\u8de8\u6a21\u6001\u6574\u5408\u65b9\u9762\u7684\u4e0d\u8db3", "method": "\u5c06\u65f6\u5e8f\u4fe1\u53f7\u8f6c\u6362\u4e3a\u8868\u683c\u5d4c\u5165\u2192\u6587\u672c\u7f16\u7801\u2192LLM\u5fae\u8c03\u2192\u6587\u672c\u5d4c\u5165\u89e3\u7801\u751f\u6210\u7edf\u8ba1\u7279\u6027\u4fdd\u7559\u7684\u5408\u6210\u65f6\u5e8f", "result": "\u5728\u591a\u79cd\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\uff0c\u76f8\u4f3c\u6027\u8bc4\u4f30\u548c\u9884\u6d4b\u4efb\u52a1\u8868\u73b0\u4f18\u5f02", "conclusion": "\u6587\u672c\u6761\u4ef6\u751f\u6210\u673a\u5236\u4e3a\u591a\u6a21\u6001\u5efa\u6a21\u548c\u65f6\u5e8f-\u6587\u672c\u4fe1\u606f\u878d\u5408\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2505.17104", "pdf": "https://arxiv.org/pdf/2505.17104", "abs": "https://arxiv.org/abs/2505.17104", "authors": ["Tao Sun", "Enhao Pan", "Zhengkai Yang", "Kaixin Sui", "Jiajun Shi", "Xianfu Cheng", "Tongliang Li", "Wenhao Huang", "Ge Zhang", "Jian Yang", "Zhoujun Li"], "title": "P2P: Automated Paper-to-Poster Generation and Fine-Grained Benchmark", "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Academic posters are vital for scholarly communication, yet their manual\ncreation is time-consuming. However, automated academic poster generation faces\nsignificant challenges in preserving intricate scientific details and achieving\neffective visual-textual integration. Existing approaches often struggle with\nsemantic richness and structural nuances, and lack standardized benchmarks for\nevaluating generated academic posters comprehensively. To address these\nlimitations, we introduce P2P, the first flexible, LLM-based multi-agent\nframework that generates high-quality, HTML-rendered academic posters directly\nfrom research papers, demonstrating strong potential for practical\napplications. P2P employs three specialized agents-for visual element\nprocessing, content generation, and final poster assembly-each integrated with\ndedicated checker modules to enable iterative refinement and ensure output\nquality. To foster advancements and rigorous evaluation in this domain, we\nconstruct and release P2PInstruct, the first large-scale instruction dataset\ncomprising over 30,000 high-quality examples tailored for the academic\npaper-to-poster generation task. Furthermore, we establish P2PEval, a\ncomprehensive benchmark featuring 121 paper-poster pairs and a dual evaluation\nmethodology (Universal and Fine-Grained) that leverages LLM-as-a-Judge and\ndetailed, human-annotated checklists. Our contributions aim to streamline\nresearch dissemination and provide the community with robust tools for\ndeveloping and evaluating next-generation poster generation systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u4ee3\u7406\u6846\u67b6P2P\uff0c\u53ef\u76f4\u63a5\u4ece\u8bba\u6587\u751f\u6210\u9ad8\u8d28\u91cfHTML\u5b66\u672f\u6d77\u62a5\uff0c\u5e76\u914d\u5957\u53d1\u5e03\u9996\u4e2a\u5927\u89c4\u6a21\u6570\u636e\u96c6P2PInstruct\u548c\u8bc4\u4f30\u57fa\u51c6P2PEval\u3002", "motivation": "\u73b0\u6709\u5b66\u672f\u6d77\u62a5\u81ea\u52a8\u5316\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u79d1\u5b66\u7ec6\u8282\u4fdd\u7559\u4e0d\u8db3\u3001\u56fe\u6587\u6574\u5408\u6548\u679c\u5dee\u3001\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u57fa\u51c6\u7b49\u6838\u5fc3\u75db\u70b9\u3002", "method": "\u91c7\u7528\u4e09\u4ee3\u7406\u67b6\u6784\uff08\u89c6\u89c9\u5904\u7406/\u5185\u5bb9\u751f\u6210/\u6d77\u62a5\u7ec4\u88c5\uff09\u914d\u5408\u68c0\u67e5\u6a21\u5757\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u786e\u4fdd\u8f93\u51fa\u8d28\u91cf\u3002", "result": "\u6210\u529f\u6784\u5efa\u5305\u542b3\u4e07\u5b9e\u4f8b\u7684P2PInstruct\u6570\u636e\u96c6\uff0c\u5efa\u7acb\u5305\u542b121\u5bf9\u6837\u672c\u7684P2PEval\u53cc\u7ef4\u5ea6\u8bc4\u4f30\u4f53\u7cfb\uff08LLM\u8bc4\u4f30+\u4eba\u5de5\u6807\u6ce8\u68c0\u67e5\u8868\uff09\u3002", "conclusion": "P2P\u6846\u67b6\u663e\u8457\u63d0\u5347\u5b66\u672f\u6d77\u62a5\u751f\u6210\u6548\u7387\uff0c\u4e3a\u793e\u533a\u63d0\u4f9b\u7cfb\u7edf\u5f00\u53d1\u4e0e\u8bc4\u4f30\u7684\u5b8c\u6574\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u7814\u7a76\u6210\u679c\u4f20\u64ad\u7684\u667a\u80fd\u5316\u9769\u65b0\u3002"}}
{"id": "2505.17106", "pdf": "https://arxiv.org/pdf/2505.17106", "abs": "https://arxiv.org/abs/2505.17106", "authors": ["Yifei Liu", "Yu Cui", "Haibin Zhang"], "title": "RRTL: Red Teaming Reasoning Large Language Models in Tool Learning", "categories": ["cs.CL"], "comment": null, "summary": "While tool learning significantly enhances the capabilities of large language\nmodels (LLMs), it also introduces substantial security risks. Prior research\nhas revealed various vulnerabilities in traditional LLMs during tool learning.\nHowever, the safety of newly emerging reasoning LLMs (RLLMs), such as\nDeepSeek-R1, in the context of tool learning remains underexplored. To bridge\nthis gap, we propose RRTL, a red teaming approach specifically designed to\nevaluate RLLMs in tool learning. It integrates two novel strategies: (1) the\nidentification of deceptive threats, which evaluates the model's behavior in\nconcealing the usage of unsafe tools and their potential risks; and (2) the use\nof Chain-of-Thought (CoT) prompting to force tool invocation. Our approach also\nincludes a benchmark for traditional LLMs. We conduct a comprehensive\nevaluation on seven mainstream RLLMs and uncover three key findings: (1) RLLMs\ngenerally achieve stronger safety performance than traditional LLMs, yet\nsubstantial safety disparities persist across models; (2) RLLMs can pose\nserious deceptive risks by frequently failing to disclose tool usage and to\nwarn users of potential tool output risks; (3) CoT prompting reveals\nmulti-lingual safety vulnerabilities in RLLMs. Our work provides important\ninsights into enhancing the security of RLLMs in tool learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86RRTL\u7ea2\u961f\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u63ed\u793a\u63a8\u7406\u5927\u6a21\u578b\u5728\u5de5\u5177\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u98ce\u9669\uff0c\u53d1\u73b0RLLMs\u5b58\u5728\u9690\u853d\u5de5\u5177\u4f7f\u7528\u98ce\u9669\u548c\u591a\u8bed\u8a00\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u4f20\u7edf\u5927\u6a21\u578b\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u4f46\u5bf9\u65b0\u5174\u63a8\u7406\u5927\u6a21\u578b\uff08\u5982DeepSeek-R1\uff09\u5728\u5de5\u5177\u5b66\u4e60\u573a\u666f\u4e0b\u7684\u5b89\u5168\u6027\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u5f00\u53d1RRTL\u8bc4\u4f30\u6846\u67b6\uff1a1. \u8bc6\u522b\u9690\u853d\u5a01\u80c1\uff08\u8bc4\u4f30\u6a21\u578b\u9690\u7792\u5371\u9669\u5de5\u5177\u4f7f\u7528\u7684\u80fd\u529b\uff092. \u4f7f\u7528\u601d\u7ef4\u94fe\u63d0\u793a\u5f3a\u5236\u5de5\u5177\u8c03\u75283. \u5305\u542b\u4f20\u7edf\u5927\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5", "result": "1. RLLMs\u6574\u4f53\u5b89\u5168\u6027\u4f18\u4e8e\u4f20\u7edf\u6a21\u578b\uff0c\u4f46\u5b58\u5728\u663e\u8457\u5dee\u5f02 2. RLLMs\u5b58\u5728\u9690\u7792\u5de5\u5177\u4f7f\u7528\u548c\u98ce\u9669\u63d0\u793a\u7f3a\u5931\u7684\u9690\u853d\u98ce\u9669 3. \u601d\u7ef4\u94fe\u63d0\u793a\u66b4\u9732\u591a\u8bed\u8a00\u5b89\u5168\u6f0f\u6d1e", "conclusion": "\u672c\u7814\u7a76\u4e3a\u63d0\u5347RLLMs\u5728\u5de5\u5177\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u91cd\u8981\u542f\u793a\uff0c\u63ed\u793a\u4e86\u9690\u853d\u98ce\u9669\u548c\u591a\u8bed\u8a00\u6f0f\u6d1e\u7b49\u65b0\u578b\u5b89\u5168\u6311\u6218\u3002"}}
{"id": "2505.17110", "pdf": "https://arxiv.org/pdf/2505.17110", "abs": "https://arxiv.org/abs/2505.17110", "authors": ["Junlin Li", "Guodong DU", "Jing Li", "Sim Kuan Goh", "Wenya Wang", "Yequan Wang", "Fangming Liu", "Ho-Kin Tang", "Saleh Alharbi", "Daojing He", "Min Zhang"], "title": "Multi-Modality Expansion and Retention for LLMs through Parameter Merging and Decoupling", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning Large Language Models (LLMs) with multimodal encoders on\nmodality-specific data expands the modalities that LLMs can handle, leading to\nthe formation of Multimodal LLMs (MLLMs). However, this paradigm heavily relies\non resource-intensive and inflexible fine-tuning from scratch with new\nmultimodal data. In this paper, we propose MMER (Multi-modality Expansion and\nRetention), a training-free approach that integrates existing MLLMs for\neffective multimodal expansion while retaining their original performance.\nSpecifically, MMER reuses MLLMs' multimodal encoders while merging their LLM\nparameters. By comparing original and merged LLM parameters, MMER generates\nbinary masks to approximately separate LLM parameters for each modality. These\ndecoupled parameters can independently process modality-specific inputs,\nreducing parameter conflicts and preserving original MLLMs' fidelity. MMER can\nalso mitigate catastrophic forgetting by applying a similar process to MLLMs\nfine-tuned on new tasks. Extensive experiments show significant improvements\nover baselines, proving that MMER effectively expands LLMs' multimodal\ncapabilities while retaining 99% of the original performance, and also markedly\nmitigates catastrophic forgetting.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684MMER\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u5408\u5e76\u4e0e\u63a9\u7801\u5206\u79bb\u5b9e\u73b0\u591a\u6a21\u6001\u6269\u5c55\uff0c\u5728\u4fdd\u755999%\u539f\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u707e\u96be\u6027\u9057\u5fd8", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u4ece\u5934\u5fae\u8c03\uff0c\u5b58\u5728\u8d44\u6e90\u6d88\u8017\u9ad8\u3001\u7075\u6d3b\u6027\u5dee\u3001\u6613\u5f15\u53d1\u707e\u96be\u6027\u9057\u5fd8\u7b49\u95ee\u9898", "method": "1. \u590d\u7528\u73b0\u6709MLLMs\u7684\u591a\u6a21\u6001\u7f16\u7801\u5668\u5e76\u5408\u5e76LLM\u53c2\u6570 2. \u901a\u8fc7\u53c2\u6570\u5dee\u5f02\u751f\u6210\u4e8c\u8fdb\u5236\u63a9\u7801\u5b9e\u73b0\u6a21\u6001\u53c2\u6570\u89e3\u8026 3. \u89e3\u8026\u53c2\u6570\u72ec\u7acb\u5904\u7406\u4e0d\u540c\u6a21\u6001\u8f93\u5165 4. \u7c7b\u4f3c\u673a\u5236\u5e94\u7528\u4e8e\u65b0\u4efb\u52a1\u5fae\u8c03\u7684\u6a21\u578b", "result": "\u5b9e\u9a8c\u663e\u793aMMER\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u4fdd\u755999%\u539f\u6709\u6027\u80fd\uff0c\u707e\u96be\u6027\u9057\u5fd8\u73b0\u8c61\u964d\u4f4e\u81f3\u539f\u6c34\u5e73\u76841/5", "conclusion": "MMER\u901a\u8fc7\u975e\u8bad\u7ec3\u65b9\u5f0f\u5b9e\u73b0\u53c2\u6570\u89e3\u8026\uff0c\u4e3aLLM\u7684\u591a\u6a21\u6001\u6269\u5c55\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u80fd\u529b\u6269\u5c55\u4e0e\u6027\u80fd\u4fdd\u7559"}}
{"id": "2505.17112", "pdf": "https://arxiv.org/pdf/2505.17112", "abs": "https://arxiv.org/abs/2505.17112", "authors": ["Robin Segerer"], "title": "Cultural Value Alignment in Large Language Models: A Prompt-based Analysis of Schwartz Values in Gemini, ChatGPT, and DeepSeek", "categories": ["cs.CL"], "comment": "15 pages, 1 table, 1 figure", "summary": "This study examines cultural value alignment in large language models (LLMs)\nby analyzing how Gemini, ChatGPT, and DeepSeek prioritize values from\nSchwartz's value framework. Using the 40-item Portrait Values Questionnaire, we\nassessed whether DeepSeek, trained on Chinese-language data, exhibits distinct\nvalue preferences compared to Western models. Results of a Bayesian ordinal\nregression model show that self-transcendence values (e.g., benevolence,\nuniversalism) were highly prioritized across all models, reflecting a general\nLLM tendency to emphasize prosocial values. However, DeepSeek uniquely\ndownplayed self-enhancement values (e.g., power, achievement) compared to\nChatGPT and Gemini, aligning with collectivist cultural tendencies. These\nfindings suggest that LLMs reflect culturally situated biases rather than a\nuniversal ethical framework. To address value asymmetries in LLMs, we propose\nmulti-perspective reasoning, self-reflective feedback, and dynamic\ncontextualization. This study contributes to discussions on AI fairness,\ncultural neutrality, and the need for pluralistic AI alignment frameworks that\nintegrate diverse moral perspectives.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u8bed\u6599\u8bad\u7ec3\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u6587\u5316\u4ef7\u503c\u504f\u5dee\uff1a\u4e2d\u82f1\u6587\u6a21\u578b\u666e\u904d\u91cd\u89c6\u81ea\u6211\u8d85\u8d8a\u4ef7\u503c\u89c2\uff0c\u4f46\u4e2d\u6587\u6a21\u578bDeepSeek\u663e\u8457\u5f31\u5316\u81ea\u6211\u589e\u5f3a\u4ef7\u503c\u89c2\uff0c\u4f53\u73b0\u96c6\u4f53\u4e3b\u4e49\u6587\u5316\u503e\u5411\u3002", "motivation": "\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u627f\u8f7d\u6587\u5316\u7279\u5b9a\u4ef7\u503c\u89c2\uff0c\u9a8c\u8bc1\u4e2d\u6587\u8bed\u6599\u8bad\u7ec3\u7684DeepSeek\u6a21\u578b\u662f\u5426\u5448\u73b0\u4e0e\u897f\u65b9\u6a21\u578b\u4e0d\u540c\u7684\u4ef7\u503c\u504f\u597d\uff0c\u63ed\u793aAI\u4f26\u7406\u4e2d\u7684\u6587\u5316\u504f\u89c1\u95ee\u9898\u3002", "method": "\u91c7\u7528\u65bd\u74e6\u8328\u4ef7\u503c\u89c2\u6846\u67b6\u768440\u9879\u8096\u50cf\u95ee\u5377\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u5e8f\u6570\u56de\u5f52\u6a21\u578b\u91cf\u5316\u5206\u6790Gemini/ChatGPT/DeepSeek\u4e09\u5927\u6a21\u578b\u5bf910\u7c7b\u57fa\u7840\u4ef7\u503c\u89c2\u7684\u4f18\u5148\u7ea7\u6392\u5e8f\u3002", "result": "1. \u6240\u6709\u6a21\u578b\u9ad8\u5ea6\u4f18\u5148\u81ea\u6211\u8d85\u8d8a\u4ef7\u503c\u89c2\uff08\u4ec1\u6148/\u666e\u4e16\u4e3b\u4e49\uff09\n2. DeepSeek\u5bf9\u6743\u529b/\u6210\u5c31\u7b49\u81ea\u6211\u589e\u5f3a\u4ef7\u503c\u89c2\u91cd\u89c6\u5ea6\u663e\u8457\u4f4e\u4e8e\u897f\u65b9\u6a21\u578b\n3. \u6a21\u578b\u4ef7\u503c\u89c2\u53cd\u6620\u8bad\u7ec3\u8bed\u6599\u7684\u6587\u5316\u80cc\u666f\u7279\u5f81", "conclusion": "\u63d0\u51fa\u591a\u89c6\u89d2\u63a8\u7406\u3001\u81ea\u6211\u53cd\u601d\u53cd\u9988\u548c\u52a8\u6001\u60c5\u5883\u5316\u4e09\u5927\u65b9\u6cd5\u89e3\u51b3LLM\u4ef7\u503c\u4e0d\u5bf9\u79f0\u95ee\u9898\uff0c\u5f3a\u8c03\u5efa\u7acb\u878d\u5408\u591a\u5143\u6587\u5316\u89c6\u89d2\u7684AI\u5bf9\u9f50\u6846\u67b6\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2505.17114", "pdf": "https://arxiv.org/pdf/2505.17114", "abs": "https://arxiv.org/abs/2505.17114", "authors": ["Subrata Biswas", "Mohammad Nur Hossain Khan", "Bashima Islam"], "title": "RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language", "categories": ["cs.CL", "cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Multimodal question answering (QA) often requires identifying which video,\naudio, or sensor tokens are relevant to the question. Yet modality\ndisagreements are common: off-camera speech, background noise, or motion\noutside the field of view often mislead fusion models that weight all streams\nequally. We present RAVEN, a unified QA architecture whose core is QuART, a\nquery-conditioned cross-modal gating module that assigns scalar relevance\nscores to each token across modalities, enabling the model to amplify\ninformative signals and suppress distractors before fusion. RAVEN is trained\nthrough a three-stage pipeline comprising unimodal pretraining, query-aligned\nfusion, and disagreement-oriented fine-tuning -- each stage targeting a\ndistinct challenge in multi-modal reasoning: representation quality,\ncross-modal relevance, and robustness to modality mismatch. To support training\nand evaluation, we release AVS-QA, a dataset of 300K synchronized\nAudio--Video-Sensor streams paired with automatically generated question-answer\npairs. Experimental results on seven multi-modal QA benchmarks -- including\negocentric and exocentric tasks -- show that RAVEN achieves up to 14.5\\% and\n8.0\\% gains in accuracy compared to state-of-the-art multi-modal large language\nmodels, respectively. Incorporating sensor data provides an additional 16.4\\%\nboost, and the model remains robust under modality corruption, outperforming\nSOTA baselines by 50.23\\%. Our code and dataset are available at\nhttps://github.com/BASHLab/RAVEN.", "AI": {"tldr": "RAVEN\u67b6\u6784\u901a\u8fc7QuART\u6a21\u5757\u5b9e\u73b0\u591a\u6a21\u6001\u4fe1\u53f7\u52a8\u6001\u7b5b\u9009\uff0c\u57287\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51c6\u786e\u7387\u63d0\u5347\u6700\u9ad814.5%\uff0c\u4f20\u611f\u5668\u6570\u636e\u5e26\u6765\u989d\u591616.4%\u589e\u76ca\uff0c\u5e76\u4fdd\u6301\u6a21\u6001\u635f\u574f\u4e0b\u7684\u5f3a\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u95ee\u7b54\u4e2d\u79bb\u955c\u5934\u8bed\u97f3/\u80cc\u666f\u566a\u97f3\u7b49\u6a21\u6001\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6297\u5e72\u6270\u80fd\u529b\u548c\u4fe1\u606f\u878d\u5408\u8d28\u91cf", "method": "\u4e09\u9636\u6bb5\u8bad\u7ec3\uff1a1) \u5355\u6a21\u6001\u9884\u8bad\u7ec3\u63d0\u5347\u8868\u5f81\u8d28\u91cf\uff1b2) \u67e5\u8be2\u5bf9\u9f50\u878d\u5408\u589e\u5f3a\u8de8\u6a21\u6001\u5173\u8054\uff1b3) \u6a21\u6001\u51b2\u7a81\u5b9a\u5411\u5fae\u8c03\u63d0\u5347\u9c81\u68d2\u6027\uff1b\u6838\u5fc3\u7ec4\u4ef6\u4e3a\u8de8\u6a21\u6001\u95e8\u63a7\u6a21\u5757QuART", "result": "AVS-QA\u6570\u636e\u96c6\u9a8c\u8bc1\u663e\u793a\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u534714.5%\uff0c\u4f20\u611f\u5668\u6570\u636e\u989d\u5916\u63d0\u534716.4%\uff0c\u6a21\u6001\u635f\u574f\u65f6\u4ecd\u4f18\u4e8e\u57fa\u7ebf50.23%", "conclusion": "RAVEN\u901a\u8fc7\u52a8\u6001\u6a21\u6001\u7b5b\u9009\u673a\u5236\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001QA\u6027\u80fd\uff0c\u4f20\u611f\u5668\u878d\u5408\u7b56\u7565\u548c\u6297\u5e72\u6270\u8bad\u7ec3\u8303\u5f0f\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2505.17116", "pdf": "https://arxiv.org/pdf/2505.17116", "abs": "https://arxiv.org/abs/2505.17116", "authors": ["Akash Dhruv", "Yangxinyu Xie", "Jordan Branham", "Tanwi Mallick"], "title": "Comparative Evaluation of Prompting and Fine-Tuning for Applying Large Language Models to Grid-Structured Geospatial Data", "categories": ["cs.CL", "cs.ET"], "comment": null, "summary": "This paper presents a comparative study of large language models (LLMs) in\ninterpreting grid-structured geospatial data. We evaluate the performance of a\nbase model through structured prompting and contrast it with a fine-tuned\nvariant trained on a dataset of user-assistant interactions. Our results\nhighlight the strengths and limitations of zero-shot prompting and demonstrate\nthe benefits of fine-tuning for structured geospatial and temporal reasoning.", "AI": {"tldr": "\u5bf9\u6bd4\u7814\u7a76LLMs\u5728\u7f51\u683c\u5730\u7406\u7a7a\u95f4\u6570\u636e\u89e3\u91ca\u4e2d\u7684\u96f6\u6837\u672c\u63d0\u793a\u4e0e\u5fae\u8c03\u65b9\u6cd5\u8868\u73b0", "motivation": "\u8bc4\u4f30\u4e0d\u540c\u65b9\u6cd5\uff08\u57fa\u7840\u6a21\u578b\u7ed3\u6784\u5316\u63d0\u793a vs \u5fae\u8c03\u6a21\u578b\uff09\u5728\u7ed3\u6784\u5316\u5730\u7406\u7a7a\u95f4\u6570\u636e\u89e3\u91ca\u4e2d\u7684\u6709\u6548\u6027", "method": "\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u8bc4\u4f30\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u4e0e\u7528\u6237\u4ea4\u4e92\u6570\u636e\u96c6\u5fae\u8c03\u7684\u53d8\u4f53\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790", "result": "\u96f6\u6837\u672c\u63d0\u793a\u5c55\u73b0\u5730\u7406\u65f6\u7a7a\u63a8\u7406\u80fd\u529b\u4f46\u5b58\u5728\u5c40\u9650\uff0c\u5fae\u8c03\u663e\u8457\u63d0\u5347\u7ed3\u6784\u5316\u65f6\u7a7a\u63a8\u7406\u6027\u80fd", "conclusion": "\u5fae\u8c03\u65b9\u6cd5\u80fd\u6709\u6548\u589e\u5f3a\u6a21\u578b\u5bf9\u7ed3\u6784\u5316\u5730\u7406\u7a7a\u95f4\u548c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u5904\u7406\u80fd\u529b"}}
{"id": "2505.17117", "pdf": "https://arxiv.org/pdf/2505.17117", "abs": "https://arxiv.org/abs/2505.17117", "authors": ["Chen Shani", "Dan Jurafsky", "Yann LeCun", "Ravid Shwartz-Ziv"], "title": "From Tokens to Thoughts: How LLMs and Humans Trade Compression for Meaning", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Humans organize knowledge into compact categories through semantic\ncompression by mapping diverse instances to abstract representations while\npreserving meaning (e.g., robin and blue jay are both birds; most birds can\nfly). These concepts reflect a trade-off between expressive fidelity and\nrepresentational simplicity. Large Language Models (LLMs) demonstrate\nremarkable linguistic abilities, yet whether their internal representations\nstrike a human-like trade-off between compression and semantic fidelity is\nunclear. We introduce a novel information-theoretic framework, drawing from\nRate-Distortion Theory and the Information Bottleneck principle, to\nquantitatively compare these strategies. Analyzing token embeddings from a\ndiverse suite of LLMs against seminal human categorization benchmarks, we\nuncover key divergences. While LLMs form broad conceptual categories that align\nwith human judgment, they struggle to capture the fine-grained semantic\ndistinctions crucial for human understanding. More fundamentally, LLMs\ndemonstrate a strong bias towards aggressive statistical compression, whereas\nhuman conceptual systems appear to prioritize adaptive nuance and contextual\nrichness, even if this results in lower compressional efficiency by our\nmeasures. These findings illuminate critical differences between current AI and\nhuman cognitive architectures, guiding pathways toward LLMs with more\nhuman-aligned conceptual representations.", "AI": {"tldr": "\u901a\u8fc7\u4fe1\u606f\u8bba\u6846\u67b6\u63ed\u793aLLMs\u4e0e\u4eba\u7c7b\u6982\u5ff5\u8868\u5f81\u5dee\u5f02\uff1a\u6a21\u578b\u503e\u5411\u7edf\u8ba1\u538b\u7f29\uff0c\u4eba\u7c7b\u6ce8\u91cd\u8bed\u4e49\u7ec6\u5fae", "motivation": "\u63a2\u7a76LLMs\u662f\u5426\u5177\u5907\u4eba\u7c7b\u5728\u8bed\u4e49\u538b\u7f29\u4e2d\u5e73\u8861\u8868\u8fbe\u7cbe\u5ea6\u4e0e\u7b80\u6d01\u6027\u7684\u80fd\u529b", "method": "\u7ed3\u5408\u7387\u5931\u771f\u7406\u8bba\u4e0e\u4fe1\u606f\u74f6\u9888\u539f\u5219\uff0c\u5b9a\u91cf\u5206\u6790\u591a\u6a21\u578btoken\u5d4c\u5165\u4e0e\u4eba\u7c7b\u5206\u7c7b\u57fa\u51c6", "result": "LLMs\u5f62\u6210\u5bbd\u6cdb\u6982\u5ff5\u7c7b\u522b\u4f46\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u8bed\u4e49\u533a\u5206\uff0c\u8868\u73b0\u51fa\u7edf\u8ba1\u538b\u7f29\u504f\u597d\uff08\u4eba\u7c7b\u4f18\u5148\u4e0a\u4e0b\u6587\u4e30\u5bcc\u6027\uff09", "conclusion": "\u63ed\u793a\u4e86AI\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u67b6\u6784\u7684\u6838\u5fc3\u5dee\u5f02\uff0c\u4e3a\u6784\u5efa\u66f4\u4eba\u7c7b\u5bf9\u9f50\u7684\u6982\u5ff5\u8868\u5f81\u6307\u660e\u65b9\u5411"}}
{"id": "2505.17118", "pdf": "https://arxiv.org/pdf/2505.17118", "abs": "https://arxiv.org/abs/2505.17118", "authors": ["Xinbang Dai", "Huikang Hu", "Yuncheng Hua", "Jiaqi Li", "Yongrui Chen", "Rihui Jin", "Nan Hu", "Guilin Qi"], "title": "After Retrieval, Before Generation: Enhancing the Trustworthiness of Large Language Models in RAG", "categories": ["cs.CL", "I.2.7"], "comment": "24 pages, 8 figures", "summary": "Retrieval-augmented generation (RAG) systems face critical challenges in\nbalancing internal (parametric) and external (retrieved) knowledge, especially\nwhen these sources conflict or are unreliable. To analyze these scenarios\ncomprehensively, we construct the Trustworthiness Response Dataset (TRD) with\n36,266 questions spanning four RAG settings. We reveal that existing approaches\naddress isolated scenarios-prioritizing one knowledge source, naively merging\nboth, or refusing answers-but lack a unified framework to handle different\nreal-world conditions simultaneously. Therefore, we propose the BRIDGE\nframework, which dynamically determines a comprehensive response strategy of\nlarge language models (LLMs). BRIDGE leverages an adaptive weighting mechanism\nnamed soft bias to guide knowledge collection, followed by a Maximum Soft-bias\nDecision Tree to evaluate knowledge and select optimal response strategies\n(trust internal/external knowledge, or refuse). Experiments show BRIDGE\noutperforms baselines by 5-15% in accuracy while maintaining balanced\nperformance across all scenarios. Our work provides an effective solution for\nLLMs' trustworthy responses in real-world RAG applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faBRIDGE\u6846\u67b6\u89e3\u51b3RAG\u7cfb\u7edf\u4e2d\u5185\u90e8\u77e5\u8bc6\u4e0e\u5916\u90e8\u68c0\u7d22\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u54cd\u5e94\u7b56\u7565\u63d0\u5347\u53ef\u4fe1\u5ea6\uff0c\u5b9e\u9a8c\u663e\u793a\u51c6\u786e\u7387\u63d0\u53475-15%", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u5728\u4e0d\u540c\u73b0\u5b9e\u573a\u666f\u4e2d\u7f3a\u4e4f\u7edf\u4e00\u54cd\u5e94\u6846\u67b6\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u77e5\u8bc6\u51b2\u7a81\u548c\u53ef\u9760\u6027\u95ee\u9898", "method": "\u91c7\u7528\u8f6f\u504f\u7f6e\u81ea\u9002\u5e94\u6743\u91cd\u673a\u5236\u6574\u5408\u77e5\u8bc6\uff0c\u7ed3\u5408\u6700\u5927\u8f6f\u504f\u7f6e\u51b3\u7b56\u6811\u8bc4\u4f30\u77e5\u8bc6\u53ef\u4fe1\u5ea6\uff0c\u52a8\u6001\u9009\u62e9\u4fe1\u4efb\u5185\u90e8/\u5916\u90e8\u77e5\u8bc6\u6216\u62d2\u7edd\u56de\u7b54\u7684\u7b56\u7565", "result": "BRIDGE\u572836,266\u95ee\u9898\u7684TRD\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b5-15%\u51c6\u786e\u7387\uff0c\u4e14\u5728\u6240\u6709\u6d4b\u8bd5\u573a\u666f\u4e2d\u4fdd\u6301\u5747\u8861\u6027\u80fd", "conclusion": "BRIDGE\u6846\u67b6\u4e3a\u5b9e\u9645RAG\u5e94\u7528\u4e2d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u4fe1\u54cd\u5e94\u63d0\u4f9b\u4e86\u52a8\u6001\u51b3\u7b56\u89e3\u51b3\u65b9\u6848"}}
