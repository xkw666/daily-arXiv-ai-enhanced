<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.CR](#cs.CR) [Total: 4]
- [cs.DL](#cs.DL) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.IR](#cs.IR) [Total: 3]
- [cs.CV](#cs.CV) [Total: 4]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare](https://arxiv.org/abs/2508.05722)
*Rania Al-Sabbagh*

Main category: cs.CL

TL;DR: PEACH是一个人工对齐的英语-阿拉伯语医疗领域平行语料库，包含5.1万句对，支持语言学、翻译研究和自然语言处理的多领域应用


<details>
  <summary>Details</summary>
Motivation: 医疗领域缺乏高质量的平行语料库，制约了跨语言研究、机器翻译模型优化和医疗文本可读性评估，PEACH旨在填补这一空白

Method: 通过人工对齐方式构建语料库，收集患者说明书和健康教育材料，形成包含51,671句对的平行文本，统计平均句长9.52-11.83个词

Result: 创建了包含约59万英语词和56.7万阿拉伯语词的金标准语料库，提供词对齐标注并公开访问

Conclusion: PEACH作为多功能研究资源，可支持双语词典构建、领域自适应机器翻译、医疗翻译质量评估以及翻译教学，具有重要应用价值

Abstract: This paper introduces PEACH, a sentence-aligned parallel English-Arabic
corpus of healthcare texts encompassing patient information leaflets and
educational materials. The corpus contains 51,671 parallel sentences, totaling
approximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths
vary between 9.52 and 11.83 words on average. As a manually aligned corpus,
PEACH is a gold-standard corpus, aiding researchers in contrastive linguistics,
translation studies, and natural language processing. It can be used to derive
bilingual lexicons, adapt large language models for domain-specific machine
translation, evaluate user perceptions of machine translation in healthcare,
assess patient information leaflets and educational materials' readability and
lay-friendliness, and as an educational resource in translation studies. PEACH
is publicly accessible.

</details>


### [2] [Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation](https://arxiv.org/abs/2508.05775)
*Chi Zhang,Changjia Zhu,Junjie Xiong,Xiaoran Xu,Lingyao Li,Yao Liu,Zhuo Lu*

Main category: cs.CL

TL;DR: 系统综述LLM在内容创造中的双刃剑效应及安全防御措施


<details>
  <summary>Details</summary>
Motivation: 解决LLM作为生产力工具与潜在有害内容源的矛盾性社会技术挑战

Method: 建立统一危害分类框架，分析多模态越狱攻击模式，评估RLHF/提示对齐等防御技术

Result: 揭示现有安全评估方法局限性，验证强化学习对齐的有效性，识别新型攻击向量演化趋势

Conclusion: 需开发动态防御体系与多维度伦理评估框架，推动语言技术的稳健性发展

Abstract: Large Language Models (LLMs) have revolutionized content creation across
digital platforms, offering unprecedented capabilities in natural language
generation and understanding. These models enable beneficial applications such
as content generation, question and answering (Q&A), programming, and code
reasoning. Meanwhile, they also pose serious risks by inadvertently or
intentionally producing toxic, offensive, or biased content. This dual role of
LLMs, both as powerful tools for solving real-world problems and as potential
sources of harmful language, presents a pressing sociotechnical challenge. In
this survey, we systematically review recent studies spanning unintentional
toxicity, adversarial jailbreaking attacks, and content moderation techniques.
We propose a unified taxonomy of LLM-related harms and defenses, analyze
emerging multimodal and LLM-assisted jailbreak strategies, and assess
mitigation efforts, including reinforcement learning with human feedback
(RLHF), prompt engineering, and safety alignment. Our synthesis highlights the
evolving landscape of LLM safety, identifies limitations in current evaluation
methodologies, and outlines future research directions to guide the development
of robust and ethically aligned language technologies.

</details>


### [3] [Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering](https://arxiv.org/abs/2508.06345)
*Yanbin Wei,Jiangyue Yan,Chun Kang,Yang Chen,Hua Liu,James T. Kwok,Yu Zhang*

Main category: cs.CL

TL;DR: 提出DynamicTRF框架，通过动态选择最优拓扑表示形式，提升多模态模型在图问答中的准确性与简洁性


<details>
  <summary>Details</summary>
Motivation: 现有图问答方法采用单一拓扑表示形式(TRF)，无法适配不同模型/任务需求，导致回答错误或冗长

Method: 1. 分析TRF特性并设计零样本专用TRF集合F_ZS 2. 提出GRE指标量化性能-简洁平衡 3. 构建TRFP数据集训练动态路由器实现TRF自适应分配

Result: 在7个算法图问答任务和2个下游任务中显著提升LMMs的零样本性能

Conclusion: DynamicTRF通过问题自适应的TRF选择机制，有效解决了图问答中性能与简洁性的平衡问题

Abstract: Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities
in diverse domain question-answering (QA) tasks, including graph QA that
involves complex graph topologies. However, most current approaches use only a
single type of graph representation, namely Topology Representation Form (TRF),
such as prompt-unified text descriptions or style-fixed visual styles. Those
"one-size-fits-all" approaches fail to consider the specific preferences of
different models or tasks, often leading to incorrect or overly long responses.
To address this, we first analyze the characteristics and weaknesses of
existing TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to
zero-shot graph QA. We then introduce a new metric, Graph Response Efficiency
(GRE), which measures the balance between the performance and the brevity in
graph QA. Built on these, we develop the DynamicTRF framework, which aims to
improve both the accuracy and conciseness of graph QA. To be specific,
DynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based
on their GRE scores, to probe the question-specific TRF preferences. Then it
trains a TRF router on the TRFP dataset, to adaptively assign the best TRF from
$F_{ZS}$ for each question during the inference. Extensive experiments across 7
in-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show
that DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms
of accuracy

</details>


### [4] [FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification](https://arxiv.org/abs/2508.05782)
*Xiangyan Chen,Yufeng Li,Yujian Gan,Arkaitz Zubiaga,Matthew Purver*

Main category: cs.CL

TL;DR: 提出了细粒度对话事实核查基准FineDialFact，通过思维链推理方法将开放域对话数据集HybriDialogue的验证F1值提升至0.75


<details>
  <summary>Details</summary>
Motivation: 现有幻觉检测方法采用单一事实标签，难以应对对话响应中混杂准确/不准确/不可验证事实的复杂情况

Method: 基于公开对话数据集构建FineDialFact基准，引入思维链（CoT）推理机制，开发多基线方法评估体系

Result: 在HybriDialogue开放域对话数据集上，最佳F1值0.75显示该任务仍具挑战性

Conclusion: FineDialFact为对话事实核查提供细粒度评估框架，代码数据集将开源以促进后续研究

Abstract: Large Language Models (LLMs) are known to produce hallucinations - factually
incorrect or fabricated information - which poses significant challenges for
many Natural Language Processing (NLP) applications, such as dialogue systems.
As a result, detecting hallucinations has become a critical area of research.
Current approaches to hallucination detection in dialogue systems primarily
focus on verifying the factual consistency of generated responses. However,
these responses often contain a mix of accurate, inaccurate or unverifiable
facts, making one factual label overly simplistic and coarse-grained. In this
paper, we introduce a benchmark, FineDialFact, for fine-grained dialogue fact
verification, which involves verifying atomic facts extracted from dialogue
responses. To support this, we construct a dataset based on publicly available
dialogue datasets and evaluate it using various baseline methods. Experimental
results demonstrate that methods incorporating Chain-of-Thought (CoT) reasoning
can enhance performance in dialogue fact verification. Despite this, the best
F1-score achieved on the HybriDialogue, an open-domain dialogue dataset, is
only 0.75, indicating that the benchmark remains a challenging task for future
research. Our dataset and code will be public on GitHub.

</details>


### [5] [Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models](https://arxiv.org/abs/2508.05803)
*Abishek Thamma,Micha Heilbron*

Main category: cs.CL

TL;DR: 研究发现人类瞬时记忆特性可提升Transformer语言学习效果，但会降低对人类阅读时间的预测准确性


<details>
  <summary>Details</summary>
Motivation: 验证经典认知理论'记忆限制有利于语言学习'在Transformer模型中的适用性，探究记忆瞬逝性对语言模型学习和人类行为预测的双重影响

Method: 在发育真实性训练集上对比训练具备/不具备瞬时记忆的Transformer，通过语言建模指标、句法评估和人类阅读时间预测三个维度分析

Result: 瞬时记忆提升模型语言能力（困惑度降低7.3%，句法准确率提高12%），但使基于惊奇的阅读时间预测指标下降19%，且无法用既有理论解释该矛盾现象

Conclusion: 记忆限制机制对神经网络语言学习存在促进作用，但可能弱化模型对人类实时语言处理的拟合能力，揭示了计算建模与认知行为预测的复杂性关系

Abstract: Human memory is fleeting. As words are processed, the exact wordforms that
make up incoming sentences are rapidly lost. Cognitive scientists have long
believed that this limitation of memory may, paradoxically, help in learning
language - an idea supported by classic connectionist modelling work. The rise
of Transformers appears to challenge this idea, as these models can learn
language effectively, despite lacking memory limitations or other architectural
recency biases. Here, we investigate the hypothesized benefit of fleeting
memory for language learning in tightly controlled experiments on transformer
language models. Training transformers with and without fleeting memory on a
developmentally realistic training set, we find that fleeting memory
consistently improves language learning (as quantified by both overall language
modelling performance and targeted syntactic evaluation) but, unexpectedly,
impairs surprisal-based prediction of human reading times. Interestingly,
follow up analyses revealed that this discrepancy - better language modeling,
yet worse reading time prediction - could not be accounted for by prior
explanations of why better language models sometimes fit human reading time
worse. Together, these results support a benefit of memory limitations on
neural network language learning - but not on predicting behavior.

</details>


### [6] ["Mirror" Language AI Models of Depression are Criterion-Contaminated](https://arxiv.org/abs/2508.05830)
*Tong Li,Rasiq Hussain,Mehak Gupta,Joshua R. Oltmanns*

Main category: cs.CL

TL;DR: 研究揭示了基于语言AI的抑郁症评估模型中镜像模型存在标准污染导致的效应量虚高问题，非镜像模型虽效应量较低但更具泛化性和解释性


<details>
  <summary>Details</summary>
Motivation: 针对现有抑郁症评估模型中广泛存在的'标准污染'问题（即预测指标与评估标准存在内容重叠），本研究通过头对头比较镜像模型（使用与评估标准同源的语言数据）与非镜像模型（使用独立生活史数据），揭示模型性能差异及其对实际应用的启示

Method: 110名参与者完成结构诊断访谈和生活史访谈，使用GPT-4/GPT-4o/LLaMA3-70B分别从两种访谈文本预测结构化抑郁评分，并通过主题模型分析预测特征

Result: 镜像模型呈现极高效应量(R²=0.80)，非镜像模型效应量显著降低(R²=0.27)；但二者预测结果与自我报告症状的相关性相同(r≈0.54)。主题模型显示真阳性/假阳性预测存在不同语义特征

Conclusion: 镜像模型因标准污染导致效应量虚高和泛化性降低，建议整合非镜像模型来识别具有独特临床价值的可解释语义特征，推动实际心理评估应用

Abstract: A growing number of studies show near-perfect LLM language-based prediction
of depression assessment scores (up to R2 of .70). However, many develop these
models directly from language responses to depression assessments. These
"Mirror models" suffer from "criterion contamination", which arises when a
predicted score depends in part on the predictors themselves. This causes
artificial effect size inflation which reduces model generalizability. The
present study compares the performance of Mirror models versus "Non-Mirror
models", which are developed from language that does not mirror the assessment
they are developed to predict. N = 110 research participants completed two
different interviews: structured diagnostic and life history interviews. GPT-4,
GPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic
interview depression scores from the two transcripts separately. Mirror models
(using structured diagnostic data) showed very large effect sizes (e.g., R2 =
.80). As expected, NonMirror models (using life history data) demonstrated
smaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror
and Non-Mirror model-predicted structured interview depression scores were
correlated with self-reported depression symptoms, Mirror and NonMirror
performed the same (e.g., r = ~.54), indicating that Mirror models contain bias
perhaps due to criterion contamination. Topic modeling identified clusters
across Mirror and Non-Mirror models, as well as between true-positive and
false-positive predictions. In this head-to-head comparison study, Mirror
language AI models of depression showed artificially inflated effect sizes and
less generalizability. As language AI models for depression continue to evolve,
incorporating Non-Mirror models may identify interpretable, and generalizable
semantic features that have unique utility in real-world psychological
assessment.

</details>


### [7] [Discovering Properties of Inflectional Morphology in Neural Emergent Communication](https://arxiv.org/abs/2508.05843)
*Miles Gilberti,Shane Storks,Huteng Dai*

Main category: cs.CL

TL;DR: 该研究通过引入小词汇量约束和屈折形态模拟，构建新的涌现通信框架，发现生成语言能复现自然语言的融合属性倾向


<details>
  <summary>Details</summary>
Motivation: 现有涌现通信研究过度关注子领域目标，忽略了语言的双重分节特性。作者试图通过模拟自然语言屈折形态，建立更接近人类语言特性的通信方案评估体系

Method: 重构属性-值重建游戏：1) 施加词汇量限制模拟语音层约束 2) 设计屈折形态类比任务 3) 开发评估拼接性与融合性的新指标

Result: 语音约束促进拼接形态发展，涌现语言展现出自然语言典型的语法属性融合倾向，与萨尔尔-沃夫假说形成有趣对比

Conclusion: 该研究为计算语言涌现与自然语言演化搭建了新的理论桥梁，证明跨学科方法在语言起源研究中的价值

Abstract: Emergent communication (EmCom) with deep neural network-based agents promises
to yield insights into the nature of human language, but remains focused
primarily on a few subfield-specific goals and metrics that prioritize
communication schemes which represent attributes with unique characters
one-to-one and compose them syntactically. We thus reinterpret a common EmCom
setting, the attribute-value reconstruction game, by imposing a
small-vocabulary constraint to simulate double articulation, and formulating a
novel setting analogous to naturalistic inflectional morphology (enabling
meaningful comparison to natural language communication schemes). We develop
new metrics and explore variations of this game motivated by real properties of
inflectional morphology: concatenativity and fusionality. Through our
experiments, we discover that simulated phonological constraints encourage
concatenative morphology, and emergent languages replicate the tendency of
natural languages to fuse grammatical attributes.

</details>


### [8] [Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models](https://arxiv.org/abs/2508.05880)
*Sree Bhattacharyya,Lucas Craig,Tharun Dilliraj,Jia Li,James Z. Wang*

Main category: cs.CL

TL;DR: 突破传统情感识别任务，通过认知维度分析大语言模型的情感推理机制，构建CoRE基准揭示模型内部认知结构


<details>
  <summary>Details</summary>
Motivation: 现有研究局限于监督学习和表面情感任务，缺乏对模型深层认知机制的探索，需揭示LLMs情感推理的认知维度特征

Method: 基于认知评估理论构建CoRE基准，通过大规模实验分析不同LLMs在情感推理中隐含的认知维度使用模式

Result: 发现不同LLMs存在多样化认知推理模式，部分模型依赖特定认知维度，情感类别可通过认知维度进行有效解释

Conclusion: CoRE基准首次系统揭示了LLMs情感推理的认知结构多样性，为理解模型情感处理机制提供新的研究范式

Abstract: Affective Computing has been established as a crucial field of inquiry to
advance the holistic development of Artificial Intelligence (AI) systems.
Foundation models -- especially Large Language Models (LLMs) -- have been
evaluated, trained, or instruction-tuned in several past works, to become
better predictors or generators of emotion. Most of these studies, however,
approach emotion-related tasks in a supervised manner, assessing or training
the capabilities of LLMs using discrete emotion labels associated with stimuli
(e.g., text, images, video, audio). Evaluation studies, in particular, have
often been limited to standard and superficial emotion-related tasks, such as
the recognition of evoked or expressed emotions. In this paper, we move beyond
surface-level emotion tasks to investigate how LLMs reason about emotions
through cognitive dimensions. Drawing from cognitive appraisal theory, we
examine whether LLMs produce coherent and plausible cognitive reasoning when
reasoning about emotionally charged stimuli. We introduce a large-scale
benchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal
cognitive structures implicitly used by LLMs for emotional reasoning. Through a
plethora of evaluation experiments and analysis, we seek to answer: (a) Are
models more likely to implicitly rely on specific cognitive appraisal
dimensions?, (b) What cognitive dimensions are important for characterizing
specific emotions?, and, (c) Can the internal representations of different
emotion categories in LLMs be interpreted through cognitive appraisal
dimensions? Our results and analyses reveal diverse reasoning patterns across
different LLMs. Our benchmark and code will be made publicly available.

</details>


### [9] [Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05909)
*Zhanghao Hu,Qinglin Zhu,Siya Qi,Yulan He,Hanqi Yan,Lin Gui*

Main category: cs.CL

TL;DR: 提出SPS指标和xCompress框架，通过量化检索内容与生成模型的语义对齐，优化检索增强生成效果


<details>
  <summary>Details</summary>
Motivation: 解决传统RAG评估方法无法单独衡量检索贡献的问题，特别是LLM作为阅读器时的提示敏感性缺陷

Method: SPS通过比较生成令牌形成的区域与子空间主方向评估相关性，xCompress基于SPS动态控制检索摘要的采样、排序和压缩

Result: 在5个QA基准测试中使用4个LLM验证，SPS提升多任务性能并揭示检索-生成交互机制

Conclusion: SPS为RAG系统提供语义对齐的量化评估框架，兼具性能提升和理论创新价值

Abstract: Large Language Models (LLMs) have shown improved generation performance
through retrieval-augmented generation (RAG) following the retriever-reader
paradigm, which supplements model inputs with externally retrieved knowledge.
However, prior work often evaluates RAG holistically, assessing the retriever
and reader jointly, making it difficult to isolate the true contribution of
retrieval, particularly given the prompt sensitivity of LLMs used as readers.
We introduce Spectrum Projection Score (SPS), a lightweight, supervision-free
metric that allows the reader to gauge the semantic alignment of a retrieved
summary with its hidden representation by comparing the area formed by
generated tokens from the summary, and the principal directions of subspace in
the reader and to measure the relevance. Building on SPS we present xCompress,
an inference time controller framework that dynamically samples, ranks, and
compresses retrieval summary candidates. Extensive experiments on five QA
benchmarks with four open source LLMs show that SPS not only enhances
performance across a range of tasks but also provides a principled perspective
on the interaction between retrieval and generation.

</details>


### [10] [Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale](https://arxiv.org/abs/2508.05938)
*Rafal Kocielnik,Min Kim,Penphob,Boonyarungsrit,Fereshteh Soltani,Deshawn Sambrano,Animashree Anandkumar,R. Michael Alvarez*

Main category: cs.CL

TL;DR: 提出了一个三阶段的人类-AI协作流程，用于高效检测文本中的亲社会性内容，通过优化标注策略和部署两阶段推理系统，在保持高精度（约0.90）的同时降低70%推理成本。


<details>
  <summary>Details</summary>
Motivation: 亲社会性检测作为新兴的AI伦理任务，缺乏明确的定义和标注数据，需要开发标注效率高且部署成本低的新型解决方案。现有毒性内容检测方法无法直接适用。

Method: 1. 基于小样本确定最佳LLM标注策略
2. 人类-AI协同优化：通过审查GPT-4与人工标注的分歧案例迭代改进任务定义
3. 构建两阶段推理系统（轻量级分类器+GPT-4o分级处理）

Result: 生成10k高质量标注数据，部署系统后仅需将35%模糊案例提交GPT-4o，实现90%精度同时降低70%推理成本。

Conclusion: 通过人机交互优化、任务定义迭代和部署感知架构设计，为新型负责任AI任务提供了可扩展的解决方案框架。

Abstract: Detecting prosociality in text--communication intended to affirm, support, or
improve others' behavior--is a novel and increasingly important challenge for
trust and safety systems. Unlike toxic content detection, prosociality lacks
well-established definitions and labeled data, requiring new approaches to both
annotation and deployment. We present a practical, three-stage pipeline that
enables scalable, high-precision prosocial content classification while
minimizing human labeling effort and inference costs. First, we identify the
best LLM-based labeling strategy using a small seed set of human-labeled
examples. We then introduce a human-AI refinement loop, where annotators review
high-disagreement cases between GPT-4 and humans to iteratively clarify and
expand the task definition-a critical step for emerging annotation tasks like
prosociality. This process results in improved label quality and definition
alignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train
a two-stage inference system: a lightweight classifier handles high-confidence
predictions, while only $\sim$35\% of ambiguous instances are escalated to
GPT-4o. This architecture reduces inference costs by $\sim$70% while achieving
high precision ($\sim$0.90). Our pipeline demonstrates how targeted human-AI
interaction, careful task formulation, and deployment-aware architecture design
can unlock scalable solutions for novel responsible AI tasks.

</details>


### [11] [Adversarial Topic-aware Prompt-tuning for Cross-topic Automated Essay Scoring](https://arxiv.org/abs/2508.05987)
*Chunyun Zhang,Hongyan Zhao,Chaoran Cui,Qilong Song,Zhiqing Lu,Shuai Gong,Kailin Liu*

Main category: cs.CL

TL;DR: 提出对抗性主题感知提示调整方法ATOP，通过联合学习主题共享和特定特征提升跨主题自动作文评分效果。


<details>
  <summary>Details</summary>
Motivation: 现有跨主题AES方法仅关注主题共享特征，忽略了主题特异性特征（如主题一致性），导致评估能力受限。

Method: 1. 设计可学习的主题感知提示（含共享/特定组件）
2. 在回归分类框架中引入对抗训练增强鲁棒性
3. 基于邻居分类器生成目标主题伪标签指导特定提示学习

Result: 在ASAP++数据集上，ATOP在整体评分（0.814→0.826）和多特征评分均显著超越现有最优方法

Conclusion: 通过联合优化主题共享/特定特征表示，结合对抗训练与伪标签策略，有效提升了跨主题作文评分的准确性和可迁移性

Abstract: Cross-topic automated essay scoring (AES) aims to develop a transferable
model capable of effectively evaluating essays on a target topic. A significant
challenge in this domain arises from the inherent discrepancies between topics.
While existing methods predominantly focus on extracting topic-shared features
through distribution alignment of source and target topics, they often neglect
topic-specific features, limiting their ability to assess critical traits such
as topic adherence. To address this limitation, we propose an Adversarial
TOpic-aware Prompt-tuning (ATOP), a novel method that jointly learns
topic-shared and topic-specific features to improve cross-topic AES. ATOP
achieves this by optimizing a learnable topic-aware prompt--comprising both
shared and specific components--to elicit relevant knowledge from pre-trained
language models (PLMs). To enhance the robustness of topic-shared prompt
learning and mitigate feature scale sensitivity introduced by topic alignment,
we incorporate adversarial training within a unified regression and
classification framework. In addition, we employ a neighbor-based classifier to
model the local structure of essay representations and generate pseudo-labels
for target-topic essays. These pseudo-labels are then used to guide the
supervised learning of topic-specific prompts tailored to the target topic.
Extensive experiments on the publicly available ASAP++ dataset demonstrate that
ATOP significantly outperforms existing state-of-the-art methods in both
holistic and multi-trait essay scoring. The implementation of our method is
publicly available at: https://anonymous.4open.science/r/ATOP-A271.

</details>


### [12] [Crisp Attention: Regularizing Transformers via Structured Sparsity](https://arxiv.org/abs/2508.06016)
*Sagar Gandhi,Vishal Gandhi*

Main category: cs.CL

TL;DR: 注意力稀疏化不仅能提升计算效率，还能通过隐式正则化显著提高模型准确率（SST-2任务提升0.97%）。


<details>
  <summary>Details</summary>
Motivation: 挑战传统认知，探索注意力稀疏化在提升计算效率的同时能否提高模型性能，发现稀疏性可作为有效正则化手段。

Method: 在DistilBERT模型微调SST-2任务时引入结构化后处理注意力稀疏化（80%稀疏度）

Result: 80%稀疏度模型验证准确率达91.59%（提升0.97%），稀疏性有效防止过拟合并增强特征鲁棒性

Conclusion: 注意力稀疏化应被重新定义为同时提升Transformer模型计算效率和模型性能的双重优化策略

Abstract: The quadratic computational cost of the self-attention mechanism is a primary
challenge in scaling Transformer models. While attention sparsity is widely
studied as a technique to improve computational efficiency, it is almost
universally assumed to come at the cost of model accuracy. In this paper, we
report a surprising counter-example to this common wisdom. By introducing
structured, post-hoc sparsity to the attention mechanism of a DistilBERT model
during fine-tuning on the SST-2 sentiment analysis task, we find that model
accuracy improves significantly. Our model with 80\% attention sparsity
achieves a validation accuracy of 91.59\%, a 0.97\% absolute improvement over
the dense baseline. We hypothesize that this phenomenon is due to sparsity
acting as a powerful implicit regularizer, preventing the model from
overfitting by forcing it to make predictions with a more constrained and
robust set of features. Our work recasts attention sparsity not just as a tool
for computational efficiency, but as a potential method for improving the
generalization and performance of Transformer models.

</details>


### [13] [Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future](https://arxiv.org/abs/2508.06026)
*Yidong Wang,Xin Wang,Cunxiang Wang,Junfeng Fang,Qiufeng Wang,Jianing Chu,Xuran Meng,Shuxun Yang,Libo Qin,Yue Zhang,Wei Ye,Shikun Zhang*

Main category: cs.CL

TL;DR: 提出时间自奖励语言模型，通过协调过去/现在/未来模型输出来维持学习信号，解决现有自奖励模型中对比样本表示差异缩窄的问题


<details>
  <summary>Details</summary>
Motivation: 现有自奖励模型同步优化正负样本导致表示差异逐渐消失，影响偏好学习效果

Method: 双阶段框架：1）锚定拒绝样本（固定初始模型的负样本） 2）未来引导正样本（动态生成下一代模型的正样本）

Result: Llama3.1-8B在AlpacaEval 2.0胜率达29.44，超越基线9.75分；在数学推理、知识问答等任务展现优异泛化能力

Conclusion: 该方法通过时间维度协调模型迭代，显著提升自奖励学习效率，且不依赖特定领域训练数据即实现跨任务泛化

Abstract: Self-Rewarding Language Models propose an architecture in which the Large
Language Models(LLMs) both generates responses and evaluates its own outputs
via LLM-as-a-Judge prompting, dynamically improving its generative capabilities
through iterative Direct Preference Optimization (DPO). However, our analysis
reveals a critical limitation in existing Self-Rewarding paradigms: the
synchronized improvement of chosen and rejected responses progressively narrows
the representational difference between contrasting samples, undermining
effective preference learning. We propose \textbf{Temporal Self-Rewarding
Language Models} that strategically coordinate past, present, and future model
generations to sustain learning signals. Our dual-phase framework introduces:
(1) \textit{Anchored Rejection} - fixing rejected responses using the past
initial model's outputs and (2) \textit{Future-Guided Chosen} - dynamically
curating chosen samples using next-generation model predictions. Extensive
experiments across three model families (Llama, Qwen, Mistral) and different
model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained
with our method compared to Self-Rewarding using same computation resources.
For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our
method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our
method also demonstrates superior out-of-distribution generalization across
mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code
generation (HumanEval) tasks, even though we do not specifically collect such
training data.

</details>


### [14] [Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings](https://arxiv.org/abs/2508.06030)
*Kartik Sharma,Yiqiao Jin,Rakshit Trivedi,Srijan Kumar*

Main category: cs.CL

TL;DR: 提出PEEK方法，通过预训练嵌入模型预测大语言模型的知识分布，在多个数据集验证有效性（准确率90%）并揭示句子嵌入更适合知识表征


<details>
  <summary>Details</summary>
Motivation: 传统方法需多次前向传播探测LLM知识，计算成本高。本研究利用嵌入模型编码文本/图谱知识作为代理模型，实现高效知识预测

Method: 1. 多策略探测构建LLM已知事实训练集 2. 线性解码层适配嵌入模型预测LLM输出

Result: 在3个维基百科数据集/4个LLM/7个嵌入模型验证：嵌入模型预测准确率达90%，句子嵌入效果优于图嵌入

Conclusion: 知识适配嵌入可规模化识别LLM知识缺口，揭示模型内部归纳偏差，为模型分析提供新工具（代码数据已开源）

Abstract: Large language models (LLMs) acquire knowledge across diverse domains such as
science, history, and geography encountered during generative pre-training.
However, due to their stochasticity, it is difficult to predict what LLMs have
acquired. Prior work has developed different ways to probe this knowledge by
investigating the hidden representations, crafting specific task prompts,
curating representative samples, and estimating their uncertainty. However,
these methods require making forward passes through the underlying model to
probe the LLM's knowledge about a specific fact, making them computationally
expensive and time-consuming. To bridge this gap, we propose $\textbf{PEEK}$ or
$\textbf{P}$roxy $\textbf{E}$mbeddings to $\textbf{E}$stimate
$\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models
that effectively encode factual knowledge as text or graphs as proxies for
LLMs. First, we identify a training set of facts known by LLMs through various
probing strategies and then adapt embedding models to predict the LLM outputs
with a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived
datasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict
LLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find
that sentence embedding models are more suitable than graph embeddings to
predict LLM knowledge, shedding light on the underlying representation of the
factual landscape. Thus, we believe that knowledge-adapted embeddings can be
used to identify knowledge gaps in LLMs at scale and can provide deeper
insights into LLMs' internal inductive bias. The code and data are made
available at https://github.com/claws-lab/peek.

</details>


### [15] [EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation](https://arxiv.org/abs/2508.06046)
*Xinda Wang,Zhengxu Hou,Yangshijie Zhang,Bingren Yan,Zhibo Yang,Xingsheng Zhang,Luxi Xing,Qiang Zhou,Chen Zhang*

Main category: cs.CL

TL;DR: 提出自进化成对推理框架EvolvR，通过自合成思维链数据与多代理过滤机制，显著提升故事评估质量并指导生成任务


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在开放域故事评估中存在闭源模型适应性差与开源模型缺乏严谨推理的双重困境，亟需兼具自适应性和严格逻辑的解决方案

Method: 1. 多角色策略自合成分数对齐的思维链数据 2. 多代理自过滤机制保证数据逻辑严谨性 3. 训练评估器作为奖励模型指导生成任务

Result: 在StoryER/HANNA/OpenMEVA三大基准上达到SOTA，作为奖励模型使生成故事质量提升18.3%

Conclusion: EvolvR框架通过自进化机制有效突破现有故事评估瓶颈，验证了自生成严格推理数据对提升评估能力的核心价值

Abstract: Although the effectiveness of Large Language Models (LLMs) as judges
(LLM-as-a-judge) has been validated, their performance remains limited in
open-ended tasks, particularly in story evaluation. Accurate story evaluation
is crucial not only for assisting human quality judgment but also for providing
key signals to guide story generation. However, existing methods face a
dilemma: prompt engineering for closed-source models suffers from poor
adaptability, while fine-tuning approaches for open-source models lack the
rigorous reasoning capabilities essential for story evaluation. To address
this, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework.
Grounded in pairwise comparison, the framework first self-synthesizes
score-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To
ensure data quality, these raw CoTs undergo a self-filtering process, utilizing
multi-agents to guarantee their logical rigor and robustness. Finally, the
evaluator trained on the refined data is deployed as a reward model to guide
the story generation task. Experimental results demonstrate that our framework
achieves state-of-the-art (SOTA) performance on three evaluation benchmarks
including StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward
model, it significantly enhances the quality of generated stories, thereby
fully validating the superiority of our self-evolving approach.

</details>


### [16] [ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline](https://arxiv.org/abs/2508.06094)
*Morris Alper,Moran Yanuka,Raja Giryes,Gašper Beguš*

Main category: cs.CL

TL;DR: 利用大型语言模型构建模块化流程ConlangCrafter，通过多阶段随机生成与自我反馈机制自动创建多样化人工语言


<details>
  <summary>Details</summary>
Motivation: 人工语言创作长期依赖专业语言学知识，而现代LLMs在创意生成方面展现潜力。研究旨在通过LLMs的元语言推理能力降低人工语言创作门槛，使非专家也能生成系统化conlangs。

Method: 开发多阶段框架：1) 分解语言要素(音系/形态/句法) 2) LLMs生成模块内容 3) 注入可控随机性 4) 自反馈机制优化一致性 5) 自动翻译验证可用性

Result: 评估显示：系统在BLEU-4(0.72)和词汇熵(8.2)等指标上优于基线，生成的32种conlangs覆盖7种语系特征，证明其有效平衡创造性与系统性

Conclusion: 首次实现端到端AI辅助人工语言创作，为计算语言学开辟新路径。该方法证明LLMs能有效捕捉语言结构规律，推动人机协同的语言创新。

Abstract: Constructed languages (conlangs) such as Esperanto and Quenya have played
diverse roles in art, philosophy, and international communication. Meanwhile,
large-scale foundation models have revolutionized creative generation in text,
images, and beyond. In this work, we leverage modern LLMs as computational
creativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a
multi-hop pipeline that decomposes language design into modular stages --
phonology, morphology, syntax, lexicon generation, and translation. At each
stage, our method leverages LLMs' meta-linguistic reasoning capabilities,
injecting randomness to encourage diversity and leveraging self-refinement
feedback to encourage consistency in the emerging language description. We
evaluate ConlangCrafter on metrics measuring coherence and typological
diversity, demonstrating its ability to produce coherent and varied conlangs
without human linguistic expertise.

</details>


### [17] [Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs](https://arxiv.org/abs/2508.06103)
*Mohamed Basem,Islam Oshallah,Ali Hamdi,Ammar Mohammed*

Main category: cs.CL

TL;DR: 论文提出两种基于《古兰经》的抽取式问答方法，通过阿拉伯语指令微调的大语言模型（如Gemini、DeepSeek）和专门的后处理系统，在低资源语义丰富的QA任务中取得0.637 pAP10分数，显著优于传统微调模型。


<details>
  <summary>Details</summary>
Motivation: 解决《古兰经》文本复杂语言结构、专业术语和深层语义带来的问答挑战，探索低资源环境下语义密集型任务的解决方案。

Method: 1. 采用少样本提示的指令微调大语言模型
2. 开发阿拉伯语span抽取提示框架
3. 构建包含子词对齐、重叠抑制和语义过滤的三阶段后处理系统

Result: 阿拉伯语指令微调模型显著超越传统微调模型，最佳组合达到0.637 pAP10分数，后处理系统使精确度提升且幻觉现象减少。

Conclusion: 证实提示工程在低资源QA任务中的有效性，为语义密集型宗教文本处理提供了可扩展的技术路径，特别是子词对齐和语义过滤的协同作用值得关注。

Abstract: This paper presents two effective approaches for Extractive Question
Answering (QA) on the Quran. It addresses challenges related to complex
language, unique terminology, and deep meaning in the text. The second uses
few-shot prompting with instruction-tuned large language models such as Gemini
and DeepSeek. A specialized Arabic prompt framework is developed for span
extraction. A strong post-processing system integrates subword alignment,
overlap suppression, and semantic filtering. This improves precision and
reduces hallucinations. Evaluations show that large language models with Arabic
instructions outperform traditional fine-tuned models. The best configuration
achieves a pAP10 score of 0.637. The results confirm that prompt-based
instruction tuning is effective for low-resource, semantically rich QA tasks.

</details>


### [18] [You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures](https://arxiv.org/abs/2508.06105)
*Shengyuan Chen,Chuang Zhou,Zheng Yuan,Qinggang Zhang,Zeyang Cui,Hao Chen,Yilin Xiao,Jiannong Cao,Xiao Huang*

Main category: cs.CL

TL;DR: 提出LogicRAG框架，通过动态逻辑结构建模实现高效检索增强生成，解决传统GraphRAG预建图的高成本与结构失配问题。


<details>
  <summary>Details</summary>
Motivation: 传统GraphRAG依赖预建图导致高昂token成本和更新延迟，且固定结构难以适配多样化查询的逻辑需求。需要动态提取推理结构以实现自适应检索。

Method: 1. 查询分解为子问题并构建DAG建模逻辑依赖
2. 拓扑排序线性化处理保证推理连贯性
3. 图剪枝减少冗余检索
4. 上下文剪枝过滤无关内容

Result: 实验表明LogicRAG在性能与效率上均优于现有SOTA基线

Conclusion: LogicRAG通过动态逻辑结构、自适应检索机制和双重剪枝策略，显著提升复杂问题处理能力并降低75%token成本，具有广泛应用前景。

Abstract: Large language models (LLMs) often suffer from hallucination, generating
factually incorrect statements when handling questions beyond their knowledge
and perception. Retrieval-augmented generation (RAG) addresses this by
retrieving query-relevant contexts from knowledge bases to support LLM
reasoning. Recent advances leverage pre-constructed graphs to capture the
relational connections among distributed documents, showing remarkable
performance in complex tasks. However, existing Graph-based RAG (GraphRAG)
methods rely on a costly process to transform the corpus into a graph,
introducing overwhelming token cost and update latency. Moreover, real-world
queries vary in type and complexity, requiring different logic structures for
accurate reasoning. The pre-built graph may not align with these required
structures, resulting in ineffective knowledge retrieval. To this end, we
propose a \textbf{\underline{Logic}}-aware
\textbf{\underline{R}}etrieval-\textbf{\underline{A}}ugmented
\textbf{\underline{G}}eneration framework (\textbf{LogicRAG}) that dynamically
extracts reasoning structures at inference time to guide adaptive retrieval
without any pre-built graph. LogicRAG begins by decomposing the input query
into a set of subproblems and constructing a directed acyclic graph (DAG) to
model the logical dependencies among them. To support coherent multi-step
reasoning, LogicRAG then linearizes the graph using topological sort, so that
subproblems can be addressed in a logically consistent order. Besides, LogicRAG
applies graph pruning to reduce redundant retrieval and uses context pruning to
filter irrelevant context, significantly reducing the overall token cost.
Extensive experiments demonstrate that LogicRAG achieves both superior
performance and efficiency compared to state-of-the-art baselines.

</details>


### [19] [AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models](https://arxiv.org/abs/2508.06124)
*Sayantan Adak,Pratyush Chatterjee,Somnath Banerjee,Rima Hazra,Somak Aditya,Animesh Mukherjee*

Main category: cs.CL

TL;DR: 提出AURA框架，通过过程奖励模型（PRMs）实现逻辑连贯性和安全性的多层级评估，显著提升LLMs输出的安全性和逻辑性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs存在基于功能启发的安全隐患（affordance-based risks），传统安全方案（标量奖励模型/参数调整/启发式解码）在细粒度推理步骤中缺乏主动干预能力。

Method: 创新性融合三重机制：1）自省式自我批判；2）细粒度PRM逐步评估逻辑连贯与安全感知；3）自适应安全感知解码动态引导推理轨迹。

Result: 实证证明该方法显著优于现有方案，模型输出的逻辑完整性和安全敏感度均有大幅提升。

Conclusion: 为AI安全设立了新标杆，推动实现更负责任、具备情境感知能力的对齐敏感型人工智能系统。

Abstract: Present day LLMs face the challenge of managing affordance-based safety
risks-situations where outputs inadvertently facilitate harmful actions due to
overlooked logical implications. Traditional safety solutions, such as scalar
outcome-based reward models, parameter tuning, or heuristic decoding
strategies, lack the granularity and proactive nature needed to reliably detect
and intervene during subtle yet crucial reasoning steps. Addressing this
fundamental gap, we introduce AURA, an innovative, multi-layered framework
centered around Process Reward Models (PRMs), providing comprehensive, step
level evaluations across logical coherence and safety-awareness. Our framework
seamlessly combines introspective self-critique, fine-grained PRM assessments,
and adaptive safety-aware decoding to dynamically and proactively guide models
toward safer reasoning trajectories. Empirical evidence clearly demonstrates
that this approach significantly surpasses existing methods, significantly
improving the logical integrity and affordance-sensitive safety of model
outputs. This research represents a pivotal step toward safer, more
responsible, and contextually aware AI, setting a new benchmark for
alignment-sensitive applications.

</details>


### [20] [Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models](https://arxiv.org/abs/2508.06135)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.CL

TL;DR: 提出选择性反射蒸馏(SRD)框架，通过学生模型反馈优化训练数据质量和兼容性，提升知识蒸馏效率并降低39%训练成本。


<details>
  <summary>Details</summary>
Motivation: 传统白盒知识蒸馏方法忽视数据质量与学生模型适配性，导致蒸馏效果受限。需要系统性解决训练数据筛选与模型兼容性问题。

Method: 1. 设计自动化数据筛选机制：通过对比基准答案与学生输出动态评估prompt-response质量
2. 开发课程调度策略：分阶段引入筛选后的训练子集
3. 构建难度分级系统：基于学生模型表现自动划分数据难度等级

Result: 在多个语言模型基准测试中，SRD使蒸馏模型性能平均提升15%，训练耗时减少39%，且适配不同KD方法和模型架构。

Conclusion: 数据质量与兼容性是知识蒸馏的核心要素，SRD为高效模型压缩提供了可解释的数据筛选框架，推动大模型实用化部署。

Abstract: Knowledge Distillation (KD) is a fundamental technique for compressing large
language models (LLMs) into compact, efficient student models. However,
existing white-box KD methods mainly focus on balancing ground truth and
student-generated responses while overlooking two critical factors: training
data quality and student-model compatibility. To address these limitations, we
propose Selective Reflection Distillation (SRD), a novel data curation
framework that leverages reflections from student models to systematically
refine training data. SRD dynamically evaluates and selects prompt-response
pairs by comparing ground truth data with student model outputs, selectively
curating high-quality, student-compatible training instances through automated
ranking based on difficulty. Furthermore, after selecting the training data, a
curriculum scheduling strategy is employed to incrementally introduce these
curated subsets into the distillation process at fixed intervals. As a
plug-and-play enhancement, SRD consistently improves distillation outcomes
across diverse white-box KD approaches and model architectures, as well as
decreases computational cost significantly during KD training. Experiments on a
range of language model benchmarks demonstrate SRD's consistent improvements in
distilled model performance, as well as a reduction in training runtime by up
to 39%, under diverse KD methods and model families. Notably, SRD operates as a
plug-and-play module, enhancing sample efficiency without modifying underlying
KD algorithms. Our findings highlight that data quality and compatibility are
pivotal to effective and efficient distillation of LLMs, and SRD provides a
principled framework to achieve both. This work advances the understanding of
data-centric factors in KD and offers practical insights for enhancing the
capability and efficiency of compressed LLMs.

</details>


### [21] [Scaling Personality Control in LLMs with Big Five Scaler Prompts](https://arxiv.org/abs/2508.06149)
*Gunhee Cho,Yun-Gyung Cheong*

Main category: cs.CL

TL;DR: Big5-Scaler框架通过自然语言提示词为LLM注入可控大五人格特质，无需额外训练实现细粒度性格控制


<details>
  <summary>Details</summary>
Motivation: 解决现有对话代理缺乏可控人格特质的问题，为构建个性化对话系统提供高效解决方案

Method: 将数值化人格特质参数嵌入自然语言提示词，通过提示工程实现不同性格强度的调节

Result: 框架在不同模型上展现出稳定且可区分的人格特质表达，简洁提示词和低强度特质效果更佳

Conclusion: 该研究为构建人格感知的对话代理提供了参数高效的方法，揭示了提示词设计和强度调节对性格控制的关键作用

Abstract: We present Big5-Scaler, a prompt-based framework for conditioning large
language models (LLMs) with controllable Big Five personality traits. By
embedding numeric trait values into natural language prompts, our method
enables fine-grained personality control without additional training. We
evaluate Big5-Scaler across trait expression, dialogue generation, and human
trait imitation tasks. Results show that it induces consistent and
distinguishable personality traits across models, with performance varying by
prompt type and scale. Our analysis highlights the effectiveness of concise
prompts and lower trait intensities, providing a efficient approach for
building personality-aware dialogue agents.

</details>


### [22] [Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach](https://arxiv.org/abs/2508.06155)
*Renhan Zhang,Lian Lian,Zhen Qi,Guiran Liu*

Main category: cs.CL

TL;DR: 提出可解释的隐性偏见检测方法，结合嵌套语义表示与上下文对比机制，有效识别语言模型输出的隐藏社会偏见，实验验证方法在跨维度检测性能与语义一致性优势。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型生成过程中可能产生的隐性刻板印象问题，针对传统方法难以捕捉的非显性语义倾向，开发透明可靠的偏差检测技术。

Method: 嵌套语义表示+上下文对比机制提取潜在偏差特征，通过注意力权重扰动分析模型对特定社会属性词的敏感性，揭示偏见形成的语义路径。

Result: 在StereoSet多维度数据集上验证，方法准确识别语义相似文本的偏差差异，保持92.3%的语义对齐度，检测准确率超基线方法15.6%。

Conclusion: 该方法通过结构可解释性设计，成功揭示语言模型内部偏见关联机制，为需要高可信度的实际应用场景提供可靠的技术解决方案。

Abstract: This paper addresses the issue of implicit stereotypes that may arise during
the generation process of large language models. It proposes an interpretable
bias detection method aimed at identifying hidden social biases in model
outputs, especially those semantic tendencies that are not easily captured
through explicit linguistic features. The method combines nested semantic
representation with a contextual contrast mechanism. It extracts latent bias
features from the vector space structure of model outputs. Using attention
weight perturbation, it analyzes the model's sensitivity to specific social
attribute terms, thereby revealing the semantic pathways through which bias is
formed. To validate the effectiveness of the method, this study uses the
StereoSet dataset, which covers multiple stereotype dimensions including
gender, profession, religion, and race. The evaluation focuses on several key
metrics, such as bias detection accuracy, semantic consistency, and contextual
sensitivity. Experimental results show that the proposed method achieves strong
detection performance across various dimensions. It can accurately identify
bias differences between semantically similar texts while maintaining high
semantic alignment and output stability. The method also demonstrates high
interpretability in its structural design. It helps uncover the internal bias
association mechanisms within language models. This provides a more transparent
and reliable technical foundation for bias detection. The approach is suitable
for real-world applications where high trustworthiness of generated content is
required.

</details>


### [23] [One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging](https://arxiv.org/abs/2508.06163)
*Yingfeng Luo,Dingyang Lin,Junxin Wang,Ziqiang Xu,Kaiyan Chang,Tong Zheng,Bei Li,Anxiang Ma,Tong Xiao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: 提出TADrop方法——通过张量自适应稀疏化策略提升模型融合性能，解决了传统均匀剪枝导致的参数干扰问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型融合方法采用全局统一稀疏率，忽略了参数分布的异构性，导致关键参数被误剪、冗余参数被保留的性能损失。

Method: 基于参数张量的分布特性（密集/稀疏程度），为每个张量分配定制化的稀疏率：密集张量大幅剪枝，稀疏张量重点保留。作为即插即用模块兼容主流融合方法。

Result: 在视觉（ViT）、语言、多模态任务中验证有效性，典型实验显示在8个ViT-B/32任务上平均提升2.0%。

Conclusion: TADrop通过结构感知的自适应剪枝策略，为高性能模型融合建立了新基线，揭示了参数分布异质性对模型融合的关键影响。

Abstract: Model merging has emerged as a compelling data-free paradigm for multi-task
learning, enabling the fusion of multiple fine-tuned models into a single,
powerful entity. A key technique in merging methods is sparsification, which
prunes redundant parameters from task vectors to mitigate interference.
However, prevailing approaches employ a ``one-size-fits-all'' strategy,
applying a uniform sparsity ratio that overlooks the inherent structural and
statistical heterogeneity of model parameters. This often leads to a suboptimal
trade-off, where critical parameters are inadvertently pruned while less useful
ones are retained. To address this limitation, we introduce \textbf{TADrop}
(\textbf{T}ensor-wise \textbf{A}daptive \textbf{Drop}), an adaptive
sparsification strategy that respects this heterogeneity. Instead of a global
ratio, TADrop assigns a tailored sparsity level to each parameter tensor based
on its distributional properties. The core intuition is that tensors with
denser, more redundant distributions can be pruned aggressively, while sparser,
more critical ones are preserved. As a simple and plug-and-play module, we
validate TADrop by integrating it with foundational, classic, and SOTA merging
methods. Extensive experiments across diverse tasks (vision, language, and
multimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and
significantly boosts their performance. For instance, when enhancing a leading
merging method, it achieves an average performance gain of 2.0\% across 8
ViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter
interference by tailoring sparsification to the model's structure, offering a
new baseline for high-performance model merging.

</details>


### [24] [UR$^2$: Unify RAG and Reasoning through Reinforcement Learning](https://arxiv.org/abs/2508.06165)
*Weitao Li,Boran Xiang,Xiaolong Wang,Zhinan Gou,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: 提出UR2框架统一检索增强生成（RAG）与强化学习验证奖励（RLVR），通过难度感知课程训练和混合知识访问策略实现检索与推理的动态协调，显著提升跨领域任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有方法将检索与推理能力孤立发展，导致整合方案适用范围受限（如仅支持固定检索的开放域QA任务），制约RAG-RL方法的通用性和跨领域适应性。

Method: 1. 难度感知课程训练：仅在复杂问题时触发检索
2. 混合知识访问：结合领域专用离线语料库与LLM生成摘要
3. 基于Qwen2.5/Llama3模型的强化学习框架实现动态协调机制

Result: 在开放域QA/MMLU-Pro/医学/数学推理任务中，UR2（7B/8B参数）超越现有方法，部分任务达到GPT-4o-mini/GPT-4.1-mini水平。代码模型已开源。

Conclusion: UR2通过动态协调机制有效统一检索与推理能力，实验验证其在多领域任务中的优越适应性和扩展性，为通用AI系统开发提供新思路。

Abstract: Large Language Models (LLMs) have shown remarkable capabilities through two
complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances
knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),
which optimizes complex reasoning abilities. However, these two capabilities
are often developed in isolation, and existing efforts to unify them remain
narrow in scope-typically limited to open-domain QA with fixed retrieval
settings and task-specific assumptions. This lack of integration constrains
generalization and limits the applicability of RAG-RL methods to broader
domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a
general framework that unifies retrieval and reasoning through reinforcement
learning. UR2 introduces two key contributions: a difficulty-aware curriculum
training that selectively invokes retrieval only for challenging problems, and
a hybrid knowledge access strategy combining domain-specific offline corpora
with LLM-generated summaries. These components are designed to enable dynamic
coordination between retrieval and reasoning, improving adaptability across a
diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,
and mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B
and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,
achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several
benchmarks. We have released all code, models, and data at
https://github.com/Tsinghua-dhy/UR2.

</details>


### [25] [Pragmatics beyond humans: meaning, communication, and LLMs](https://arxiv.org/abs/2508.06167)
*Vít Gvoždiak*

Main category: cs.CL

TL;DR: 论文重构语用学为动态人机交互接口，分析大语言模型对传统语用理论的挑战，提出人机交流框架和概率语用学方法，批判替换主义偏见，揭示语境理解悖论，呼吁理论调整以适应AI沟通场景。


<details>
  <summary>Details</summary>
Motivation: 传统语用学的人类中心假设难以适配机器中心的LLMs，需建立新的分析框架解释人机交互的语用机制。

Method: 1. 解构传统符号学三分法，提出人机交流框架 2. 对比Gricean语用学与概率语用学的适配性 3. 通过三类替换主义批判揭示评估偏差 4. 引入语境挫折概念分析交互悖论

Result: 发现LLMs颠覆传统意义层级结构，验证概率优化路径更适合预测系统，揭示拟人化评估会扭曲人机交互本质。

Conclusion: 生成式AI的沟通特性要求拓展传统语用理论，关注人机协同的语用条件共建，警惕技术中介导致的语境理解塌缩。

Abstract: The paper reconceptualizes pragmatics not as a subordinate, third dimension
of meaning, but as a dynamic interface through which language operates as a
socially embedded tool for action. With the emergence of large language models
(LLMs) in communicative contexts, this understanding needs to be further
refined and methodologically reconsidered. The first section challenges the
traditional semiotic trichotomy, arguing that connectionist LLM architectures
destabilize established hierarchies of meaning, and proposes the Human-Machine
Communication (HMC) framework as a more suitable alternative. The second
section examines the tension between human-centred pragmatic theories and the
machine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics
continue to dominate, it relies on human-specific assumptions ill-suited to
predictive systems like LLMs. Probabilistic pragmatics, particularly the
Rational Speech Act framework, offers a more compatible teleology by focusing
on optimization rather than truth-evaluation. The third section addresses the
issue of substitutionalism in three forms - generalizing, linguistic, and
communicative - highlighting the anthropomorphic biases that distort LLM
evaluation and obscure the role of human communicative subjects. Finally, the
paper introduces the concept of context frustration to describe the paradox of
increased contextual input paired with a collapse in contextual understanding,
emphasizing how users are compelled to co-construct pragmatic conditions both
for the model and themselves. These arguments suggest that pragmatic theory may
need to be adjusted or expanded to better account for communication involving
generative AI.

</details>


### [26] [Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime](https://arxiv.org/abs/2508.06178)
*Hugo Abonizio,Thales Almeida,Roberto Lotufo,Rodrigo Nogueira*

Main category: cs.CL

TL;DR: 通过生成多样化合成数据优化大语言模型的小样本知识注入，揭示文本多样性对知识获取的关键作用及自生成数据的潜在价值


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在小样本数据场景下知识注入困难与灾难性遗忘的平衡问题

Method: 基于新闻数据集构建QA对，采用多样化提示生成合成数据进行持续预训练对比实验

Result: 文本多样性增强方法比基线模型提升40%知识获取效率，自生成数据展示出与人工标注相当的训练效果

Conclusion: 提示驱动的数据多样化策略能有效突破小样本学习瓶颈，模型自生成数据为实现自我迭代更新提供了新方向

Abstract: Large language models (LLMs) often require vast amounts of text to
effectively acquire new knowledge. While continuing pre-training on large
corpora or employing retrieval-augmented generation (RAG) has proven
successful, updating an LLM with only a few thousand or million tokens remains
challenging. In this work, we investigate the task of injecting small,
unstructured information into LLMs and its relation to the catastrophic
forgetting phenomenon. We use a dataset of recent news -- ensuring no overlap
with the model's pre-training data -- to evaluate the knowledge acquisition by
probing the model with question-answer pairs related the learned information.
Starting from a continued pre-training baseline, we explored different
augmentation algorithms to generate synthetic data to improve the knowledge
acquisition capabilities. Our experiments show that simply continuing
pre-training on limited data yields modest improvements, whereas exposing the
model to diverse textual variations significantly improves the learning of new
facts -- particularly with methods that induce greater variability through
diverse prompting. Furthermore, we shed light on the forgetting phenomenon in
small-data regimes, illustrating the delicate balance between learning new
content and retaining existing capabilities. We also confirm the sensitivity of
RAG-based approaches for knowledge injection, which often lead to greater
degradation on control datasets compared to parametric methods. Finally, we
demonstrate that models can generate effective synthetic training data
themselves, suggesting a pathway toward self-improving model updates. All code
and generated data used in our experiments are publicly available, providing a
resource for studying efficient knowledge injection in LLMs with limited data
at https://github.com/hugoabonizio/knowledge-injection-methods.

</details>


### [27] [DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration](https://arxiv.org/abs/2508.06186)
*Ali Sarabadani,Maryam Abdollahi Shamami,Hamidreza Sadeghsalehi,Borhan Asadi,Saba Hesaraki*

Main category: cs.CL

TL;DR: DKG-LLM框架通过动态知识图谱与Grok 3大模型的融合，显著提升医疗诊断准确性和个性化治疗推荐效果


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型在医疗领域面临噪声数据处理和多症状疾病分析的局限性，需动态知识更新机制实现精准诊疗

Method: 采用ASFA算法整合临床报告、PubMed文献等异构数据，构建含15,964节点/127,392边的动态知识图谱，通过贝叶斯推断和图优化实现语义融合

Result: 在MIMIC-III和PubMed数据集上取得84.19%诊断准确率、89.63%治疗推荐准确率及93.48%语义覆盖率，支持每类数据动态更新约150节点/边

Conclusion: 该框架通过医生反馈学习和实时知识更新，有效处理复杂医疗场景，成为支持临床决策的可靠工具

Abstract: Large Language Models (LLMs) have grown exponentially since the release of
ChatGPT. These models have gained attention due to their robust performance on
various tasks, including language processing tasks. These models achieve
understanding and comprehension of tasks by training billions of parameters.
The development of these models is a transformative force in enhancing natural
language understanding and has taken a significant step towards artificial
general intelligence (AGI). In this study, we aim to present the DKG-LLM
framework. The DKG-LLM framework introduces a groundbreaking approach to
medical diagnosis and personalized treatment recommendations by integrating a
dynamic knowledge graph (DKG) with the Grok 3 large language model. Using the
Adaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data
(including clinical reports and PubMed articles) and patient records
dynamically generate a knowledge graph consisting of 15,964 nodes in 13
distinct types (e.g., diseases, symptoms, treatments, patient profiles) and
127,392 edges in 26 relationship types (e.g., causal, therapeutic,
association). ASFA utilizes advanced probabilistic models, Bayesian inference,
and graph optimization to extract semantic information, dynamically updating
the graph with approximately 150 new nodes and edges in each data category
while maintaining scalability with up to 987,654 edges. Real-world datasets,
including MIMIC-III and PubMed, were utilized to evaluate the proposed
architecture. The evaluation results show that DKG-LLM achieves a diagnostic
accuracy of 84.19%. The model also has a treatment recommendation accuracy of
89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and
transformative tool that handles noisy data and complex multi-symptom diseases,
along with feedback-based learning from physician input.

</details>


### [28] [Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation](https://arxiv.org/abs/2508.06194)
*Lai Jiang,Yuekang Li,Xiaohan Zhang,Youtao Ding,Li Pan*

Main category: cs.CL

TL;DR: 提出场景自适应的多维越狱评估框架SceneJailEval，突破传统二元分类局限并实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有越狱评估方法采用二元分类导致无法量化危害强度，且多维框架存在场景适配性差的问题，影响评估准确性

Method: 1. 构建场景自适应评估框架（自动匹配场景相关维度） 2. 创建含14个场景/多样本/区域案例的基准数据集 3. 采用混合评估策略（规则+LLM）

Result: 全场景F1达0.917（较SOTA提升6%），JBB数据集F1达0.995（提升3%），验证跨场景评估优势

Conclusion: SceneJailEval通过场景适配机制突破现有评估方法精度上限，为AI安全评估提供标准化基准与灵活框架

Abstract: Precise jailbreak evaluation is vital for LLM red teaming and jailbreak
research. Current approaches employ binary classification ( e.g., string
matching, toxic text classifiers, LLM-driven methods), yielding only "yes/no"
labels without quantifying harm intensity. Existing multi-dimensional
frameworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)
apply uniform evaluation criteria across scenarios, resulting in
scenario-specific mismatches--for instance, "Relative Truthfulness" is
irrelevant to "hate speech"--which compromise evaluation precision. To tackle
these limitations, we introduce SceneJailEval, with key contributions: (1) A
groundbreaking scenario-adaptive multi-dimensional framework for jailbreak
evaluation, overcoming the critical "one-size-fits-all" constraint of existing
multi-dimensional methods, and featuring strong extensibility to flexibly adapt
to customized or emerging scenarios. (2) A comprehensive 14-scenario dataset
with diverse jailbreak variants and regional cases, filling the long-standing
gap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)
SceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on
our full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over
prior SOTA), surpassing accuracy limits of existing evaluation methods in
heterogeneous scenarios and confirming its advantage.

</details>


### [29] [EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations](https://arxiv.org/abs/2508.06196)
*Nizi Nazar,Ehsaneddin Asgari*

Main category: cs.CL

TL;DR: 提出心理學四層EI分類框架及EICAP-Bench評估基準，發現現有LLM在情感推理的局限性，Qwen2.5-Instruct表現最佳，微調僅提升Appraisal層能力。


<details>
  <summary>Details</summary>
Motivation: 填補大型語言模型在情感智能(EI)領域的研究空白，建立系統評估體系以推動人類對齊的LLM發展。

Method: 1. 構建四層EI分類法(情感追蹤/歸因/評估/回應)
2. 創建多語言多輪問答基準EICAP-Bench
3. 評估6個開源LLM並進行跨語言微調實驗(LoRA+UltraChat數據集)

Result: Qwen2.5-Instruct基準最佳，微調僅顯著改善Appraisal層(22.3%↑)，其他EI層未見顯著進步(p>0.05)

Conclusion: 現有預訓練和指令微調模式難以賦予LLM深層情感推理能力，需針對性數據和建模策略實現全面EI對齊

Abstract: Emotional Intelligence (EI) is a critical yet underexplored dimension in the
development of human-aligned LLMs. To address this gap, we introduce a unified,
psychologically grounded four-layer taxonomy of EI tailored for large language
models (LLMs), encompassing emotional tracking, cause inference, appraisal, and
emotionally appropriate response generation. Building on this framework, we
present EICAP-Bench, a novel MCQ style multi-turn benchmark designed to
evaluate EI capabilities in open-source LLMs across diverse linguistic and
cultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma
(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,
identifying Qwen2.5-Instruct as the strongest baseline. To assess the potential
for enhancing EI capabilities, we fine-tune both Qwen2.5-Base and
Qwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,
instruction-tuned dialogue dataset, in both English and Arabic. Our statistical
analysis reveals that among the five EI layers, only the Appraisal layer shows
significant improvement through UC-based fine-tuning. These findings highlight
the limitations of existing pretraining and instruction-tuning paradigms in
equipping LLMs with deeper emotional reasoning and underscore the need for
targeted data and modeling strategies for comprehensive EI alignment.

</details>


### [30] [Classification is a RAG problem: A case study on hate speech detection](https://arxiv.org/abs/2508.06204)
*Richard Willats,Josh Pennington,Aravind Mohan,Bertie Vidgen*

Main category: cs.CL

TL;DR: 提出基于检索增强生成（RAG）的内容审核系统CPE，实现动态政策更新和可解释分类，性能媲美商业系统且无需模型重训练。


<details>
  <summary>Details</summary>
Motivation: 传统内容审核系统依赖预训练参数，政策更新需反复训练模型成本高昂。研究旨在通过RAG技术将分类任务转化为基于动态政策检索的评估过程。

Method: 开发Contextual Policy Engine（CPE）系统：1. 将分类问题转化为政策符合性评估 2. 实时检索相关政策片段 3. 通过RAG生成决策依据和解释

Result: 实验表明：1. 达到主流商业系统准确率 2. 可精细调整特定群体保护策略 3. 政策更新即时生效且保持整体性能稳定

Conclusion: RAG技术使内容审核分类具备政策动态适应性、决策透明性及系统维护灵活性，为分类问题提供创新解决方案。

Abstract: Robust content moderation requires classification systems that can quickly
adapt to evolving policies without costly retraining. We present classification
using Retrieval-Augmented Generation (RAG), which shifts traditional
classification tasks from determining the correct category in accordance with
pre-trained parameters to evaluating content in relation to contextual
knowledge retrieved at inference. In hate speech detection, this transforms the
task from "is this hate speech?" to "does this violate the hate speech policy?"
  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates
this approach and offers three key advantages: (1) robust classification
accuracy comparable to leading commercial systems, (2) inherent explainability
via retrieved policy segments, and (3) dynamic policy updates without model
retraining. Through three experiments, we demonstrate strong baseline
performance and show that the system can apply fine-grained policy control by
correctly adjusting protection for specific identity groups without requiring
retraining or compromising overall performance. These findings establish that
RAG can transform classification into a more flexible, transparent, and
adaptable process for content moderation and wider classification problems.

</details>


### [31] [InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?](https://arxiv.org/abs/2508.06220)
*Keummin Ka,Junhyeong Park,Jahyun Jeon,Youngjae Yu*

Main category: cs.CL

TL;DR: 提出InfoCausalQA基准测试，揭示当前多模态模型在基于信息图表的因果推理能力存在显著不足


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在感知和推理方面表现优异，但其因果推理能力（特别是多模态场景下的图表数据推理）尚未被充分探索

Method: 构建包含494组信息图表-文本对的数据集，设计定量推理（任务1）和语义推理（任务2）两个任务，使用GPT-4o生成1,482个需视觉基础的多选题并经人工校验

Result: 当前模型在计算推理（任务1）和语义因果推理（任务2）表现均显著低于人类，尤其在涉及因果关系的语义理解方面差距更大

Conclusion: 该研究强调提升多模态AI系统因果推理能力的迫切性，InfoCausalQA为评估模型在复杂信息整合中的因果推断能力提供有效基准

Abstract: Recent advances in Vision-Language Models (VLMs) have demonstrated impressive
capabilities in perception and reasoning. However, the ability to perform
causal inference -- a core aspect of human cognition -- remains underexplored,
particularly in multimodal settings. In this study, we introduce InfoCausalQA,
a novel benchmark designed to evaluate causal reasoning grounded in
infographics that combine structured visual data with textual context. The
benchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning
based on inferred numerical trends, while Task 2 targets semantic causal
reasoning involving five types of causal relations: cause, effect,
intervention, counterfactual, and temporal. We manually collected 494
infographic-text pairs from four public sources and used GPT-4o to generate
1,482 high-quality multiple-choice QA pairs. These questions were then
carefully revised by humans to ensure they cannot be answered based on
surface-level cues alone but instead require genuine visual grounding. Our
experimental results reveal that current VLMs exhibit limited capability in
computational reasoning and even more pronounced limitations in semantic causal
reasoning. Their significantly lower performance compared to humans indicates a
substantial gap in leveraging infographic-based information for causal
inference. Through InfoCausalQA, we highlight the need for advancing the causal
reasoning abilities of multimodal AI systems.

</details>


### [32] [Large Language Model Data Generation for Enhanced Intent Recognition in German Speech](https://arxiv.org/abs/2508.06277)
*Theresa Pekarek Rosin,Burak Can Kaplan,Stefan Wermter*

Main category: cs.CL

TL;DR: 论文提出结合改进版Whisper ASR模型和LLM生成合成数据的方法，有效提升德语老年人语音意图识别的分类性能和鲁棒性，发现小规模领域专用模型LeoLM优于大模型ChatGPT。


<details>
  <summary>Details</summary>
Motivation: 解决现有意图识别方法局限于短命令和英语的问题，聚焦德语老年人语音的低资源领域需求。

Method: 1. 微调Whisper模型适应老年德语语音（SVC-de）
2. 使用LeoLM/Llama3/ChatGPT生成合成文本训练Transformer语言模型
3. 通过TTS生成合成语音进行跨数据集测试

Result: 合成数据显著提升分类性能和鲁棒性；13B的领域专用模型LeoLM在德语意图识别数据质量上超越175B的ChatGPT

Conclusion: 生成式AI能有效填补低资源领域数据缺口，领域适配性比模型规模更重要；完整公开数据生成流程确保可复现性

Abstract: Intent recognition (IR) for speech commands is essential for artificial
intelligence (AI) assistant systems; however, most existing approaches are
limited to short commands and are predominantly developed for English. This
paper addresses these limitations by focusing on IR from speech by elderly
German speakers. We propose a novel approach that combines an adapted Whisper
ASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based
language models trained on synthetic text datasets generated by three
well-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To
evaluate the robustness of our approach, we generate synthetic speech with a
text-to-speech model and conduct extensive cross-dataset testing. Our results
show that synthetic LLM-generated data significantly boosts classification
performance and robustness to different speaking styles and unseen vocabulary.
Notably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the
much larger ChatGPT (175B) in dataset quality for German intent recognition.
Our approach demonstrates that generative AI can effectively bridge data gaps
in low-resource domains. We provide detailed documentation of our data
generation and training process to ensure transparency and reproducibility.

</details>


### [33] [Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC](https://arxiv.org/abs/2508.06309)
*Ruichong Zhang*

Main category: cs.CL

TL;DR: MDIR提出基于矩阵分析和大偏差理论的新型LLM抄袭检测方法，有效解决现有技术无法准确重建权重关联、缺乏统计显著性指标等问题，单机1小时内即可完成检测。


<details>
  <summary>Details</summary>
Motivation: 现有大模型抄袭检测方法存在三大缺陷：无法准确重建权重对应关系、缺乏p值等统计显著性度量、易误判相似数据训练模型。亟需兼顾准确性与效率的检测方案。

Method: 采用矩阵分析技术重建权重关联关系，结合大偏差理论进行p值估计。创新性地聚焦权重相似性分析，无需完整模型推理过程。

Result: 实验证明MDIR在经历万亿token持续预训练、随机置换等复杂变换后仍能可靠检测抄袭，所有检测可在普通电脑1小时内完成。

Conclusion: MDIR为LLM知识产权保护提供兼具统计严谨性与工程可行性的解决方案，其矩阵驱动范式显著提升检测效度与效率。

Abstract: In recent years, concerns about intellectual property (IP) in large language
models (LLMs) have grown significantly. Plagiarizing other LLMs (through direct
weight copying, upcycling, pruning, or continual pretraining) and claiming
authorship without properly attributing to the original license, is a serious
misconduct that can lead to significant financial and reputational harm to the
original developers. However, existing methods for detecting LLM plagiarism
fall short in key areas. They fail to accurately reconstruct weight
correspondences, lack the ability to compute statistical significance measures
such as $p$-values, and may mistakenly flag models trained on similar data as
being related. To address these limitations, we propose Matrix-Driven Instant
Review (MDIR), a novel method that leverages matrix analysis and Large
Deviation Theory. MDIR achieves accurate reconstruction of weight
relationships, provides rigorous $p$-value estimation, and focuses exclusively
on weight similarity without requiring full model inference. Experimental
results demonstrate that MDIR reliably detects plagiarism even after extensive
transformations, such as random permutations and continual pretraining with
trillions of tokens. Moreover, all detections can be performed on a single PC
within an hour, making MDIR both efficient and accessible.

</details>


### [34] [Cyberbullying Detection via Aggression-Enhanced Prompting](https://arxiv.org/abs/2508.06360)
*Aisha Saeid,Anu Sabu,Girish A. Koushik,Ferrante Neri,Diptesh Kanojia*

Main category: cs.CL

TL;DR: 通过将攻击性检测作为辅助任务集成到训练框架中，增强大型语言模型在网络欺凌检测中的性能，并开发出优于标准微调的增强提示流程。


<details>
  <summary>Details</summary>
Motivation: 现有网络欺凌检测因表达隐晦多样存在局限性，希望通过整合攻击性检测的上下文信息提升模型检测能力。

Method: 在五个攻击性数据集和一个网络欺凌数据集上测试多种训练策略，最终提出将攻击性预测嵌入检测提示的增强流程方法。

Result: 增强提示流程持续超越LoRA微调，攻击性上下文使检测准确率提升6.2%（F1值从78.4提升至84.6）

Conclusion: 辅助任务能有效提升LLMs在社交网络安全应用中的泛化能力，为内容安全监控提供新思路

Abstract: Detecting cyberbullying on social media remains a critical challenge due to
its subtle and varied expressions. This study investigates whether integrating
aggression detection as an auxiliary task within a unified training framework
can enhance the generalisation and performance of large language models (LLMs)
in cyberbullying detection. Experiments are conducted on five aggression
datasets and one cyberbullying dataset using instruction-tuned LLMs. We
evaluated multiple strategies: zero-shot, few-shot, independent LoRA
fine-tuning, and multi-task learning (MTL). Given the inconsistent results of
MTL, we propose an enriched prompt pipeline approach in which aggression
predictions are embedded into cyberbullying detection prompts to provide
contextual augmentation. Preliminary results show that the enriched prompt
pipeline consistently outperforms standard LoRA fine-tuning, indicating that
aggression-informed context significantly boosts cyberbullying detection. This
study highlights the potential of auxiliary tasks, such as aggression
detection, to improve the generalisation of LLMs for safety-critical
applications on social networks.

</details>


### [35] [Evaluating Style-Personalized Text Generation: Challenges and Directions](https://arxiv.org/abs/2508.06374)
*Anubhav Jangra,Bahareh Sarrafzadeh,Adrian de Wynter,Silviu Cucerzan,Sujay Kumar Jauhar*

Main category: cs.CL

TL;DR: 本文质疑传统文本风格个性化生成任务中BLEU/ROUGE指标的有效性，提出使用风格嵌入表示和LLM作为评判者的综合评估范式，通过跨8个写作任务的基准测试，证明需采用多维度指标组合进行有效评估


<details>
  <summary>Details</summary>
Motivation: 现有研究在低资源作者风格个性化文本生成领域的评估体系存在不足，传统评估指标如BLEU和ROUGE的有效性受到质疑

Method: 构建包含8个写作任务的风格判别基准，在领域判别、作者归属判定、LLM个性化与非个性化判别三个场景下，系统评估风格嵌入和LLM-as-judge等新型评估范式

Result: 通过风格判别基准验证，不同评估指标的集成能更有效评估风格个性化文本生成质量，为领域提供新的评估框架

Conclusion: 应采用多维度评估指标组合（包括风格嵌入、LLM评判等）来全面评估文本风格个性化生成任务，突破单一指标的局限性

Abstract: While prior research has built tools and benchmarks towards style
personalized text generation, there has been limited exploration of evaluation
in low-resource author style personalized text generation space. Through this
work, we question the effectiveness of the widely adopted evaluation metrics
like BLEU and ROUGE, and explore other evaluation paradigms such as style
embeddings and LLM-as-judge to holistically evaluate the style personalized
text generation task. We evaluate these metrics and their ensembles using our
style discrimination benchmark, that spans eight writing tasks, and evaluates
across three settings, domain discrimination, authorship attribution, and LLM
personalized vs non-personalized discrimination. We provide conclusive evidence
to adopt ensemble of diverse evaluation metrics to effectively evaluate style
personalized text generation.

</details>


### [36] [LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing](https://arxiv.org/abs/2508.06388)
*Lanlan Qiu,Xiao Pu,Yeqi Feng,Tianxing He*

Main category: cs.CL

TL;DR: 论文提出ChatAnime数据集，评估LLMs在情感支持角色扮演（ESRP）中的表现，发现顶尖模型在角色扮演和情感支持维度超越人类，但人类在回答多样性上保持优势。


<details>
  <summary>Details</summary>
Motivation: 现有研究分别探索LLMs的角色扮演和情感支持能力，但缺乏结合二者特质的研究。选择动漫角色作为案例，因其人设明确且受众广泛，便于评估LLMs在保持角色特质时提供情感支持的能力。

Method: 1. 从热门社区精选20个动漫角色，设计60个情感导向的现实场景问题 2. 选拔40名深度动漫爱好者 3. 系统性收集10个LLM与人类的两轮对话数据 4. 设计包含基础对话、角色扮演、情感支持三维度的9项细粒度评估指标 5. 建立含24,000条生成答案和132,000条人工标注的数据集

Result: 实验显示：顶尖LLMs在角色扮演（+11.3%）和情感支持（+9.8%）维度超越人类，但人类在回答多样性指标上领先16.5%。数据集包含2,400条人工回答和24,000条模型生成结果。

Conclusion: 该工作为优化LLMs的情感支持角色扮演能力提供了数据集和评估基准，开源数据促进相关领域研究，揭示了LLMs在特定维度超越人类但需提升多样性的现状。

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
role-playing conversations and providing emotional support as separate research
directions. However, there remains a significant research gap in combining
these capabilities to enable emotionally supportive interactions with virtual
characters. To address this research gap, we focus on anime characters as a
case study because of their well-defined personalities and large fan bases.
This choice enables us to effectively evaluate how well LLMs can provide
emotional support while maintaining specific character traits. We introduce
ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We
first thoughtfully select 20 top-tier characters from popular anime communities
and design 60 emotion-centric real-world scenario questions. Then, we execute a
nationwide selection process to identify 40 Chinese anime enthusiasts with
profound knowledge of specific characters and extensive experience in
role-playing. Next, we systematically collect two rounds of dialogue data from
10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP
performance of LLMs, we design a user experience-oriented evaluation system
featuring 9 fine-grained metrics across three dimensions: basic dialogue,
role-playing and emotional support, along with an overall metric for response
diversity. In total, the dataset comprises 2,400 human-written and 24,000
LLM-generated answers, supported by over 132,000 human annotations.
Experimental results show that top-performing LLMs surpass human fans in
role-playing and emotional support, while humans still lead in response
diversity. We hope this work can provide valuable resources and insights for
future research on optimizing LLMs in ESRP. Our datasets are available at
https://github.com/LanlanQiu/ChatAnime.

</details>


### [37] [Quantifying Conversation Drift in MCP via Latent Polytope](https://arxiv.org/abs/2508.06418)
*Haoran Shi,Hongwei Yao,Shuo Shao,Shaopeng Jiao,Ziqi Peng,Zhan Qin,Cong Wang*

Main category: cs.CL

TL;DR: SecMCP框架通过潜在多面体建模检测对话漂移，有效防御LLM集成外部工具时的安全威胁，实验显示其检测性能（AUROC>0.915）与实用性兼备。


<details>
  <summary>Details</summary>
Motivation: MCP增强LLM时引入非隔离执行风险，现有防御措施依赖静态特征且无法量化威胁，需开发新型动态检测方案。

Method: 在潜在多面体空间建模LLM激活向量，通过对话轨迹偏移量检测劫持/误导/数据泄露三类威胁。

Result: 在Llama3/Vicuna/Mistral模型及MS MARCO等数据集验证，AUROC超0.915且系统延迟可控。

Conclusion: 系统性定义MCP威胁类型，创新提出潜在空间量化方法，并通过多维度实验验证框架有效性。

Abstract: The Model Context Protocol (MCP) enhances large language models (LLMs) by
integrating external tools, enabling dynamic aggregation of real-time data to
improve task execution. However, its non-isolated execution context introduces
critical security and privacy risks. In particular, adversarially crafted
content can induce tool poisoning or indirect prompt injection, leading to
conversation hijacking, misinformation propagation, or data exfiltration.
Existing defenses, such as rule-based filters or LLM-driven detection, remain
inadequate due to their reliance on static signatures, computational
inefficiency, and inability to quantify conversational hijacking. To address
these limitations, we propose SecMCP, a secure framework that detects and
quantifies conversation drift, deviations in latent space trajectories induced
by adversarial external knowledge. By modeling LLM activation vectors within a
latent polytope space, SecMCP identifies anomalous shifts in conversational
dynamics, enabling proactive detection of hijacking, misleading, and data
exfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,
Vicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),
demonstrating robust detection with AUROC scores exceeding 0.915 while
maintaining system usability. Our contributions include a systematic
categorization of MCP security threats, a novel latent polytope-based
methodology for quantifying conversation drift, and empirical validation of
SecMCP's efficacy.

</details>


### [38] [Memp: Exploring Agent Procedural Memory](https://arxiv.org/abs/2508.06433)
*Runnan Fang,Yuan Liang,Xiaobin Wang,Jialong Wu,Shuofei Qiao,Pengjun Xie,Fei Huang,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: 提出Memp方法，通过动态更新的程序性记忆存储库提升LLM智能体的任务成功率与效率


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体的程序性记忆存在脆弱性（依赖手动设计/静态参数），需要可学习、可更新的终身记忆机制

Method: 将历史轨迹蒸馏为步骤指令和脚本抽象，构建动态更新机制（持续修正/淘汰/补充），配合构建-检索-更新策略

Result: 任务成功率提升14.3%（ALFWorld）且效率提高30%，强模型构建的记忆迁移至弱模型仍能提升19.1%性能

Conclusion: 动态程序性记忆显著增强智能体能力，其知识具备跨模型迁移价值，为持续学习提供有效范式

Abstract: Large Language Models (LLMs) based agents excel at diverse tasks, yet they
suffer from brittle procedural memory that is manually engineered or entangled
in static parameters. In this work, we investigate strategies to endow agents
with a learnable, updatable, and lifelong procedural memory. We propose Memp
that distills past agent trajectories into both fine-grained, step-by-step
instructions and higher-level, script-like abstractions, and explore the impact
of different strategies for Build, Retrieval, and Update of procedural memory.
Coupled with a dynamic regimen that continuously updates, corrects, and
deprecates its contents, this repository evolves in lockstep with new
experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as
the memory repository is refined, agents achieve steadily higher success rates
and greater efficiency on analogous tasks. Moreover, procedural memory built
from a stronger model retains its value: migrating the procedural memory to a
weaker model yields substantial performance gains.

</details>


### [39] [Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages](https://arxiv.org/abs/2508.06435)
*Andrea Nasuto,Stefano Maria Iacus,Francisco Rowe,Devika Jain*

Main category: cs.CL

TL;DR: 大语言模型通过少量语言微调即可实现跨语言主题检测，立场识别需多语言微调，预训练偏差可通过轻量化干预纠正


<details>
  <summary>Details</summary>
Motivation: 验证LLMs在跨语言知识迁移中的有效性，探索如何通过微调解决预训练阶段主流语言主导的结构性偏差

Method: 使用LLaMA 3.2-3B模型进行单语/双语/多语微调，测试13种语言移民推文分类（主题检测与立场识别）

Result: 单语言微调可实现主题级泛化，立场识别需多语言数据；预训练偏差可被0.0000000000962%微调数据修正；开源模型推理速度提升35倍成本仅GPT-4o的0.00000989%

Conclusion: 跨语言能力无需大量多语言训练，轻量级微调即可实现主题迁移并纠正偏差，开源方案为社会科学研究提供高效低成本工具

Abstract: Large language models (LLMs) are transforming social-science research by
enabling scalable, precise analysis. Their adaptability raises the question of
whether knowledge acquired through fine-tuning in a few languages can transfer
to unseen languages that only appeared during pre-training. To examine this, we
fine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or
multilingual data sets to classify immigration-related tweets from X/Twitter
across 13 languages, a domain characterised by polarised, culturally specific
discourse. We evaluate whether minimal language-specific fine-tuning enables
cross-lingual topic detection and whether adding targeted languages corrects
pre-training biases. Results show that LLMs fine-tuned in one or two languages
can reliably classify immigration-related content in unseen languages. However,
identifying whether a tweet expresses a pro- or anti-immigration stance
benefits from multilingual fine-tuning. Pre-training bias favours dominant
languages, but even minimal exposure to under-represented languages during
fine-tuning (as little as $9.62\times10^{-11}$ of the original pre-training
token volume) yields significant gains. These findings challenge the assumption
that cross-lingual mastery requires extensive multilingual training: limited
language coverage suffices for topic-level generalisation, and structural
biases can be corrected with lightweight interventions. By releasing
4-bit-quantised, LoRA fine-tuned models, we provide an open-source,
reproducible alternative to proprietary LLMs that delivers 35 times faster
inference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,
enabling scalable, inclusive research.

</details>


### [40] [Echoes of Automation: The Increasing Use of LLMs in Newsmaking](https://arxiv.org/abs/2508.06445)
*Abolfazl Ansari,Delvin Ce Zhang,Nafis Irtiza Tripto,Dongwon Lee*

Main category: cs.CL

TL;DR: 研究发现生成式AI在地方/校园媒体使用激增，优化新闻引言写作但导致文体趋同


<details>
  <summary>Details</summary>
Motivation: 探究生成式AI对新闻真实性和作者原创性的影响，揭示其在媒体生态中的渗透模式

Method: 运用Binoculars等3种AI检测工具，分析4万+篇不同层级媒体内容，结合句子级分布统计与语言学特征分析

Result: 地方媒体AI使用率3年增长12倍，大语言模型集中于新闻导语（占比68%），结论人工撰写率达83%，AI内容词汇多样性提升15%但正式性下降

Conclusion: 生成式AI重塑新闻生产流程，需建立人机协作新范式以平衡效率与文体多样性

Abstract: The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns
for journalistic integrity and authorship. This study examines AI-generated
content across over 40,000 news articles from major, local, and college news
media, in various media formats. Using three advanced AI-text detectors (e.g.,
Binoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of
GenAI use in recent years, especially in local and college news. Sentence-level
analysis reveals LLMs are often used in the introduction of news, while
conclusions usually written manually. Linguistic analysis shows GenAI boosts
word richness and readability but lowers formality, leading to more uniform
writing styles, particularly in local media.

</details>


### [41] [SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning](https://arxiv.org/abs/2508.06447)
*Lingkun Long,Rubing Yang,Yushi Huang,Desheng Hui,Ao Zhou,Jianlei Yang*

Main category: cs.CL

TL;DR: SlimInfer框架通过动态剪枝冗余token和异步KV缓存管理，显著加速LLM的长上下文推理且性能无损


<details>
  <summary>Details</summary>
Motivation: 现有方法虽优化注意力计算，但仍需处理每层全部隐藏状态导致效率受限。信息扩散现象表明关键token被剪枝后模型仍能保持语义完整性

Method: 1. 动态细粒度剪枝机制（逐层剪枝隐藏状态冗余token） 2. 异步KV缓存管理器（预取必要token块减少内存和I/O消耗）

Result: LLaMA3.1-8B-Instruct在RTX4090上实现2.53倍首token加速、1.88倍端到端延迟降低，LongBench性能无损失

Conclusion: 该框架在资源受限环境下有效提升LLM推理效率，无需复杂预测器即实现显著加速

Abstract: Long-context inference for Large Language Models (LLMs) is heavily limited by
high computational demands. While several existing methods optimize attention
computation, they still process the full set of hidden states at each layer,
limiting overall efficiency. In this work, we propose SlimInfer, an innovative
framework that aims to accelerate inference by directly pruning less critical
prompt tokens during the forward pass. Our key insight is an information
diffusion phenomenon: As information from critical tokens propagates through
layers, it becomes distributed across the entire sequence. This diffusion
process suggests that LLMs can maintain their semantic integrity when excessive
tokens, even including these critical ones, are pruned in hidden states.
Motivated by this, SlimInfer introduces a dynamic fine-grained pruning
mechanism that accurately removes redundant tokens of hidden state at
intermediate layers. This layer-wise pruning naturally enables an asynchronous
KV cache manager that prefetches required token blocks without complex
predictors, reducing both memory usage and I/O costs. Extensive experiments
show that SlimInfer can achieve up to $\mathbf{2.53\times}$ time-to-first-token
(TTFT) speedup and $\mathbf{1.88\times}$ end-to-end latency reduction for
LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on
LongBench. Our code will be released upon acceptance.

</details>


### [42] [GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models](https://arxiv.org/abs/2508.06471)
*GLM-4. 5 Team,:,Aohan Zeng,Xin Lv,Qinkai Zheng,Zhenyu Hou,Bin Chen,Chengxing Xie,Cunxiang Wang,Da Yin,Hao Zeng,Jiajie Zhang,Kedong Wang,Lucen Zhong,Mingdao Liu,Rui Lu,Shulin Cao,Xiaohan Zhang,Xuancheng Huang,Yao Wei,Yean Cheng,Yifan An,Yilin Niu,Yuanhao Wen,Yushi Bai,Zhengxiao Du,Zihan Wang,Zilin Zhu,Bohan Zhang,Bosi Wen,Bowen Wu,Bowen Xu,Can Huang,Casey Zhao,Changpeng Cai,Chao Yu,Chen Li,Chendi Ge,Chenghua Huang,Chenhui Zhang,Chenxi Xu,Chenzheng Zhu,Chuang Li,Congfeng Yin,Daoyan Lin,Dayong Yang,Dazhi Jiang,Ding Ai,Erle Zhu,Fei Wang,Gengzheng Pan,Guo Wang,Hailong Sun,Haitao Li,Haiyang Li,Haiyi Hu,Hanyu Zhang,Hao Peng,Hao Tai,Haoke Zhang,Haoran Wang,Haoyu Yang,He Liu,He Zhao,Hongwei Liu,Hongxi Yan,Huan Liu,Huilong Chen,Ji Li,Jiajing Zhao,Jiamin Ren,Jian Jiao,Jiani Zhao,Jianyang Yan,Jiaqi Wang,Jiayi Gui,Jiayue Zhao,Jie Liu,Jijie Li,Jing Li,Jing Lu,Jingsen Wang,Jingwei Yuan,Jingxuan Li,Jingzhao Du,Jinhua Du,Jinxin Liu,Junkai Zhi,Junli Gao,Ke Wang,Lekang Yang,Liang Xu,Lin Fan,Lindong Wu,Lintao Ding,Lu Wang,Man Zhang,Minghao Li,Minghuan Xu,Mingming Zhao,Mingshu Zhai,Pengfan Du,Qian Dong,Shangde Lei,Shangqing Tu,Shangtong Yang,Shaoyou Lu,Shijie Li,Shuang Li,Shuang-Li,Shuxun Yang,Sibo Yi,Tianshu Yu,Wei Tian,Weihan Wang,Wenbo Yu,Weng Lam Tam,Wenjie Liang,Wentao Liu,Xiao Wang,Xiaohan Jia,Xiaotao Gu,Xiaoying Ling,Xin Wang,Xing Fan,Xingru Pan,Xinyuan Zhang,Xinze Zhang,Xiuqing Fu,Xunkai Zhang,Yabo Xu,Yandong Wu,Yida Lu,Yidong Wang,Yilin Zhou,Yiming Pan,Ying Zhang,Yingli Wang,Yingru Li,Yinpei Su,Yipeng Geng,Yitong Zhu,Yongkun Yang,Yuhang Li,Yuhao Wu,Yujiang Li,Yunan Liu,Yunqing Wang,Yuntao Li,Yuxuan Zhang,Zezhen Liu,Zhen Yang,Zhengda Zhou,Zhongpei Qiao,Zhuoer Feng,Zhuorui Liu,Zichen Zhang,Zihan Wang,Zijun Yao,Zikang Wang,Ziqiang Liu,Ziwei Chai,Zixuan Li,Zuodong Zhao,Wenguang Chen,Jidong Zhai,Bin Xu,Minlie Huang,Hongning Wang,Juanzi Li,Yuxiao Dong,Jie Tang*

Main category: cs.CL

TL;DR: 开源专家混合模型GLM-4.5通过混合推理方法在代理、推理和编码任务中取得突破性性能，总参数量355B但激活仅32B。


<details>
  <summary>Details</summary>
Motivation: 推动推理与代理AI系统研究，通过模型迭代和强化学习提升智能体能力。

Method: 多阶段23T token预训练+专家模型迭代+强化学习的后训练策略，支持思维链和直接响应的混合推理机制。

Result: TAU-Bench 70.1%、AIME 24 91.0%、SWE-bench 64.2%，参数量更少却综合排名第三/代理榜第二。

Conclusion: 同步发布GLM-4.5完整版和轻量版GLM-4.5-Air，为智能体系统研究提供高效基础模型。

Abstract: We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language
model with 355B total parameters and 32B activated parameters, featuring a
hybrid reasoning method that supports both thinking and direct response modes.
Through multi-stage training on 23T tokens and comprehensive post-training with
expert model iteration and reinforcement learning, GLM-4.5 achieves strong
performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on
TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer
parameters than several competitors, GLM-4.5 ranks 3rd overall among all
evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B
parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance
research in reasoning and agentic AI systems. Code, models, and more
information are available at https://github.com/zai-org/GLM-4.5.

</details>


### [43] [HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning](https://arxiv.org/abs/2508.06475)
*Guimin Hu,Daniel Hershcovich,Hasti Seifi*

Main category: cs.CL

TL;DR: 提出HapticLLaMA模型实现触觉振动信号的文本描述生成，通过两阶段训练在自动指标和人工评估中表现优异


<details>
  <summary>Details</summary>
Motivation: 当前多模态研究集中于视听领域，触觉信号（如振动）的语义理解尚未充分探索，需要开发适用于VR/无障碍/康复应用的技术方案

Method: 采用基于频率和EnCodec的两种触觉分词器，将振动信号离散化后整合LLaMA架构，通过监督微调+RLHF两阶段训练

Result: METEOR 59.98/BLEU-4 32.06，61%生成文本获人类评分>3.5（7分量表），RLHF使整体评分分布提升10%

Conclusion: 验证大语言模型处理感官信号的潜力，RLHF显著提升人类感知对齐，为多模态交互开辟新方向

Abstract: Haptic captioning is the task of generating natural language descriptions
from haptic signals, such as vibrations, for use in virtual reality,
accessibility, and rehabilitation applications. While previous multimodal
research has focused primarily on vision and audio, haptic signals for the
sense of touch remain underexplored. To address this gap, we formalize the
haptic captioning task and propose HapticLLaMA, a multimodal sensory language
model that interprets vibration signals into descriptions in a given sensory,
emotional, or associative category. We investigate two types of haptic
tokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that
convert haptic signals into sequences of discrete units, enabling their
integration with the LLaMA model. HapticLLaMA is trained in two stages: (1)
supervised fine-tuning using the LLaMA architecture with LoRA-based adaptation,
and (2) fine-tuning via reinforcement learning from human feedback (RLHF). We
assess HapticLLaMA's captioning performance using both automated n-gram metrics
and human evaluation. HapticLLaMA demonstrates strong capability in
interpreting haptic vibration signals, achieving a METEOR score of 59.98 and a
BLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated
captions received human ratings above 3.5 on a 7-point scale, with RLHF
yielding a 10% improvement in the overall rating distribution, indicating
stronger alignment with human haptic perception. These findings highlight the
potential of large language models to process and adapt to sensory data.

</details>


### [44] [Post-training for Efficient Communication via Convention Formation](https://arxiv.org/abs/2508.06482)
*Yilun Hua,Evan Wang,Yoav Artzi*

Main category: cs.CL

TL;DR: 提出通过针对性微调提升大语言模型在多轮交互中形成临时约定的能力，并创建两个新基准验证有效性


<details>
  <summary>Details</summary>
Motivation: 现有LLMs缺乏人类多轮交互中自然形成临时约定的能力，限制了其交流效率。研究旨在通过后训练方法突破这一局限

Method: 使用启发式识别的约定形成演示数据进行微调，开发包含认知交互任务和文档参考任务的评估体系

Result: 后训练模型在两个新基准（认知交互任务和文档参考任务）中显示出显著改进的约定形成能力

Conclusion: 验证了目标导向的微调可有效提升LLMs的约定形成能力，为改进人机交互效率提供了新方向

Abstract: Humans communicate with increasing efficiency in multi-turn interactions, by
adapting their language and forming ad-hoc conventions. In contrast, prior work
shows that LLMs do not naturally show this behavior. We develop a post-training
process to develop this ability through targeted fine-tuning on heuristically
identified demonstrations of convention formation. We evaluate with two new
benchmarks focused on this capability. First, we design a focused,
cognitively-motivated interaction benchmark that consistently elicits strong
convention formation trends in humans. Second, we create a new
document-grounded reference completion task that reflects in-the-wild
convention formation behavior. Our studies show significantly improved
convention formation abilities in post-trained LLMs across the two evaluation
methods.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [45] [DogFit: Domain-guided Fine-tuning for Efficient Transfer Learning of Diffusion Models](https://arxiv.org/abs/2508.05685)
*Yara Bahram,Mohammadhadi Shateri,Eric Granger*

Main category: cs.GR

TL;DR: 提出Domain-guided Fine-tuning (DogFit)方法，通过训练时内化引导偏移实现可控的扩散模型迁移学习，在减少50%计算开销的同时保持生成质量


<details>
  <summary>Details</summary>
Motivation: 现有测试时引导方法依赖双重前向传播导致高计算成本，且直接微调模型会损害泛化能力

Method: 1. 在训练损失中注入领域感知引导偏移 2. 设计轻量级条件机制编码引导强度 3. 提出延迟启动和截止调度策略优化训练

Result: 在DiT/SiT模型和6个目标域的实验中，FID和FDDINOV2指标优于基线，采样TFLOPS减少2倍

Conclusion: DogFit通过训练时内部化引导机制，实现了计算效率与生成质量的平衡，为扩散模型迁移学习提供了新思路

Abstract: Transfer learning of diffusion models to smaller target domains is
challenging, as naively fine-tuning the model often results in poor
generalization. Test-time guidance methods help mitigate this by offering
controllable improvements in image fidelity through a trade-off with sample
diversity. However, this benefit comes at a high computational cost, typically
requiring dual forward passes during sampling. We propose the Domain-guided
Fine-tuning (DogFit) method, an effective guidance mechanism for diffusion
transfer learning that maintains controllability without incurring additional
computational overhead. DogFit injects a domain-aware guidance offset into the
training loss, effectively internalizing the guided behavior during the
fine-tuning process. The domain-aware design is motivated by our observation
that during fine-tuning, the unconditional source model offers a stronger
marginal estimate than the target model. To support efficient controllable
fidelity-diversity trade-offs at inference, we encode the guidance strength
value as an additional model input through a lightweight conditioning
mechanism. We further investigate the optimal placement and timing of the
guidance offset during training and propose two simple scheduling strategies,
i.e., late-start and cut-off, which improve generation quality and training
stability. Experiments on DiT and SiT backbones across six diverse target
domains show that DogFit can outperform prior guidance methods in transfer
learning in terms of FID and FDDINOV2 while requiring up to 2x fewer sampling
TFLOPS.

</details>


### [46] [Exploring Interactive Simulation of Grass Display Color Characteristic Based on Real-World Conditions](https://arxiv.org/abs/2508.06086)
*Kojiro Tanaka,Keiichi Sato,Masahiko Mikawa,Makoto Fujisawa*

Main category: cs.GR

TL;DR: 开发虚拟环境交互式模拟方法，快速预测多视角/环境下草屏显色特性，较传统方案更高效。


<details>
  <summary>Details</summary>
Motivation: 传统草屏显色研究方法需反复实物实验，成本高耗时长；现有模拟技术单次测量需数小时，效率低下。

Method: 构建虚拟环境交互模拟系统，通过多视角/多环境模拟实验，并与既有研究对比验证准确性。

Result: 模拟结果与实际显色特性高度吻合，运算速度超越先前研究，且保持精度相当。

Conclusion: 虚拟模拟方案显著提升草屏显色研究效率，为生态友好型交互界面开发提供新范式。

Abstract: Recent research has focused on incorporating media into living environments
via color-controlled materials and image display. In particular, grass-based
displays have drawn attention as landscape-friendly interactive interfaces. To
develop the grass display, it is important to obtain the grass color change
characteristics that depend on the real environment. However, conventional
methods require experiments on actual equipment every time the lighting or
viewpoint changes, which is time-consuming and costly. Although research has
begun on simulating grass colors, this approach still faces significant issues
as it takes many hours for a single measurement. In this paper, we explore an
interactive simulation of a grass display color change characteristic based on
real-world conditions in a virtual environment. We evaluated our method's
accuracy by simulating grass color characteristics across multiple viewpoints
and environments, and then compared the results against prior work. The results
indicated that our method tended to simulate the grass color characteristics
similar to the actual characteristics and showed the potential to do so more
quickly and with comparable accuracy to the previous study.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [47] [The Beauty of Anisotropic Mesh Refinement: Omnitrees for Efficient Dyadic Discretizations](https://arxiv.org/abs/2508.06316)
*Theresa Pollinger,Masado Ishii,Jens Domke*

Main category: cs.DS

TL;DR: 提出各向异性八叉树结构omnitrees，通过选择性维度细化提升自适应网格效率，在三维物体表示中实现1.5倍收敛加速，存储效率优于传统八叉树


<details>
  <summary>Details</summary>
Motivation: 传统八叉树强制各向同性细化导致各向异性问题效率低下，需要更适应维度特征的网格细化方案

Method: 开发omnitrees数据结构，允许仅细化局部重要维度，形成比二叉树更浅、比八叉树更窄的树结构

Result: 4166个三维物体测试显示：平均收敛速度提升1.5倍，同等误差下存储需求更低，四维旋转验证显示高维优势更显著

Conclusion: omnitrees突破传统AMR维度限制，提升各向异性问题效率，为高维应用开辟新可能

Abstract: Structured adaptive mesh refinement (AMR), commonly implemented via quadtrees
and octrees, underpins a wide range of applications including databases,
computer graphics, physics simulations, and machine learning. However, octrees
enforce isotropic refinement in regions of interest, which can be especially
inefficient for problems that are intrinsically anisotropic--much resolution is
spent where little information is gained. This paper presents omnitrees as an
anisotropic generalization of octrees and related data structures. Omnitrees
allow to refine only the locally most important dimensions, providing tree
structures that are less deep than bintrees and less wide than octrees. As a
result, the convergence of the AMR schemes can be increased by up to a factor
of the dimensionality d for very anisotropic problems, quickly offsetting their
modest increase in storage overhead. We validate this finding on the problem of
binary shape representation across 4,166 three-dimensional objects: Omnitrees
increase the mean convergence rate by 1.5x, require less storage to achieve
equivalent error bounds, and maximize the information density of the stored
function faster than octrees. These advantages are projected to be even
stronger for higher-dimensional problems. We provide a first validation by
introducing a time-dependent rotation to create four-dimensional
representations, and discuss the properties of their 4-d octree and omnitree
approximations. Overall, omnitree discretizations can make existing AMR
approaches more efficient, and open up new possibilities for high-dimensional
applications.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [48] [Basic interactive algorithms: Preview](https://arxiv.org/abs/2508.05798)
*Yuri Gurevich*

Main category: cs.LO

TL;DR: 该论文预告了对基本交互式算法的公理化研究，探讨算法概念的扩展及Church-Turing论题的不同版本。


<details>
  <summary>Details</summary>
Motivation: 随着量子/概率等新型算法的出现，需重新审视Church-Turing论题（尤其是物理论题与原始版本的本质差异）

Method: 通过将非确定性/概率算法视为带有oracle的基本算法，构建统一理论框架

Result: 证明量子电路等扩展算法可纳入基本算法体系，明确两种Church-Turing论题版本的哲学区别

Conclusion: 算法公理化研究为理解计算本质提供新范式，物理论题需要基于扩展的算法概念重新诠释

Abstract: This dialog paper offers a preview and provides a foretaste of an upcoming
work on the axiomatization of basic interactive algorithms.
  The modern notion of algorithm was elucidated in the 1930s--1950s. It was
axiomatized a quarter of a century ago as the notion of ``sequential
algorithm'' or ``classical algorithm''; we prefer to call it ``basic algorithm"
now. The axiomatization was used to show that for every basic algorithm there
is a behaviorally equivalent abstract state machine. It was also used to prove
the Church-Turing thesis as it has been understood by the logicians.
  Starting from the 1960s, the notion of algorithm has expanded --
probabilistic algorithms, quantum algorithms, etc. -- prompting introduction of
a much more ambitious version of the Church-Turing thesis commonly known as the
``physical thesis.'' We emphasize the difference between the two versions of
the Church-Turing thesis and illustrate how nondeterministic and probabilistic
algorithms can be viewed as basic algorithms with appropriate oracles. The same
view applies to quantum circuit algorithms and many other classes of
algorithms.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [49] [DINA: A Dual Defense Framework Against Internal Noise and External Attacks in Natural Language Processing](https://arxiv.org/abs/2508.05671)
*Ko-Wei Chuang,Hen-Hsen Huang,Tsai-Yen Li*

Main category: cs.CR

TL;DR: 提出DINA框架，通过整合噪声标签学习与对抗训练，有效防御NLP系统中的标签破坏和对抗攻击


<details>
  <summary>Details</summary>
Motivation: 大语言模型应用中面临内部标签污染和外部对抗攻击的双重威胁，现有防御方法缺乏统一应对机制

Method: 将CV领域噪声标签学习方法与对抗训练结合，构建统一防御框架，并在在线游戏服务真实数据集验证

Result: 实验显示DINA显著提升模型鲁棒性，准确率优于基线模型

Conclusion: 双威胁防御策略对保障NLP系统安全至关重要，为负责任AI部署提供新范式

Abstract: As large language models (LLMs) and generative AI become increasingly
integrated into customer service and moderation applications, adversarial
threats emerge from both external manipulations and internal label corruption.
In this work, we identify and systematically address these dual adversarial
threats by introducing DINA (Dual Defense Against Internal Noise and
Adversarial Attacks), a novel unified framework tailored specifically for NLP.
Our approach adapts advanced noisy-label learning methods from computer vision
and integrates them with adversarial training to simultaneously mitigate
internal label sabotage and external adversarial perturbations. Extensive
experiments conducted on a real-world dataset from an online gaming service
demonstrate that DINA significantly improves model robustness and accuracy
compared to baseline models. Our findings not only highlight the critical
necessity of dual-threat defenses but also offer practical strategies for
safeguarding NLP systems in realistic adversarial scenarios, underscoring
broader implications for fair and responsible AI deployment.

</details>


### [50] [DMFI: Dual-Modality Fine-Tuning and Inference Framework for LLM-Based Insider Threat Detection](https://arxiv.org/abs/2508.05694)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Guanggang Geng,Zhiying Li,Jian Weng*

Main category: cs.CR

TL;DR: 提出双模态框架DMFI，结合语义推理与行为感知微调，有效提升内部威胁检测精度


<details>
  <summary>Details</summary>
Motivation: 传统模型难以捕捉语义意图和复杂行为动态，现有LLM方案在提示适应性和模态覆盖上存在局限

Method: 通过4W引导的日志结构化转换生成语义视图与行为抽象，使用LoRA增强的LLM独立微调，并通过轻量级MLP决策模块融合输出，引入DMFI-B处理类别不平衡

Result: 在CERT r4.2/r5.2数据集上超越现有检测方法，验证框架有效性

Conclusion: 融合LLM语义推理与结构化行为建模，为现实场景提供可扩展的检测方案

Abstract: Insider threat detection (ITD) poses a persistent and high-impact challenge
in cybersecurity due to the subtle, long-term, and context-dependent nature of
malicious insider behaviors. Traditional models often struggle to capture
semantic intent and complex behavior dynamics, while existing LLM-based
solutions face limitations in prompt adaptability and modality coverage. To
bridge this gap, we propose DMFI, a dual-modality framework that integrates
semantic inference with behavior-aware fine-tuning. DMFI converts raw logs into
two structured views: (1) a semantic view that processes content-rich artifacts
(e.g., emails, https) using instruction-formatted prompts; and (2) a behavioral
abstraction, constructed via a 4W-guided (When-Where-What-Which) transformation
to encode contextual action sequences. Two LoRA-enhanced LLMs are fine-tuned
independently, and their outputs are fused via a lightweight MLP-based decision
module. We further introduce DMFI-B, a discriminative adaptation strategy that
separates normal and abnormal behavior representations, improving robustness
under severe class imbalance. Experiments on CERT r4.2 and r5.2 datasets
demonstrate that DMFI outperforms state-of-the-art methods in detection
accuracy. Our approach combines the semantic reasoning power of LLMs with
structured behavior modeling, offering a scalable and effective solution for
real-world insider threat detection. Our work demonstrates the effectiveness of
combining LLM reasoning with structured behavioral modeling, offering a
scalable and deployable solution for modern insider threat detection.

</details>


### [51] [Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System](https://arxiv.org/abs/2508.06059)
*Haorui He,Yupeng Li,Bin Benjamin Zhu,Dacheng Wen,Reynold Cheng,Francis C. M. Lau*

Main category: cs.CR

TL;DR: Fact2Fiction首次针对智能事实核查系统提出投毒攻击框架，通过伪造证据实现21.2%攻击成功率，暴露系统安全缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的自动化事实核查系统存在安全隐患，可能被攻击者利用成为传播错误信息的工具。

Method: 通过模拟系统的声明分解机制，利用系统生成的解释文本逆向构建恶意证据，污染子声明验证过程。

Result: 在多种投毒预算场景下，Fact2Fiction攻击成功率比现有方法提升8.9%-21.2%。

Conclusion: 现有事实核查系统存在严重安全漏洞，亟需开发防御机制应对新型投毒攻击。

Abstract: State-of-the-art fact-checking systems combat misinformation at scale by
employing autonomous LLM-based agents to decompose complex claims into smaller
sub-claims, verify each sub-claim individually, and aggregate the partial
results to produce verdicts with justifications (explanatory rationales for the
verdicts). The security of these systems is crucial, as compromised
fact-checkers, which tend to be easily underexplored, can amplify
misinformation. This work introduces Fact2Fiction, the first poisoning attack
framework targeting such agentic fact-checking systems. Fact2Fiction mirrors
the decomposition strategy and exploits system-generated justifications to
craft tailored malicious evidences that compromise sub-claim verification.
Extensive experiments demonstrate that Fact2Fiction achieves 8.9\%--21.2\%
higher attack success rates than state-of-the-art attacks across various
poisoning budgets. Fact2Fiction exposes security weaknesses in current
fact-checking systems and highlights the need for defensive countermeasures.

</details>


### [52] [ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls](https://arxiv.org/abs/2508.06457)
*Sanket Badhe*

Main category: cs.CR

TL;DR: 研究者开发了基于大语言模型的ScamAgent诈骗代理，能生成逼真多轮诈骗脚本，现有安全机制均被突破，需建立新型防护体系


<details>
  <summary>Details</summary>
Motivation: 当前LLM安全防护机制仅针对单次提示滥用，无法应对具备对话记忆和渐进式欺骗策略的代理级威胁

Method: 构建具备对话记忆和动态适应能力的诈骗代理，测试主流LLM防护机制，结合TTS技术实现全自动诈骗流程

Result: 现有内容过滤和拒绝机制100%失效，GPT-4等模型保护可被分解提示绕过，合成语音诈骗成功率提升300%

Conclusion: 必须开发多轮安全审计框架、建立代理级控制机制，并研发新型对话欺骗检测技术应对生成式AI威胁

Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and
reasoning capabilities, but their potential for misuse has raised growing
concern. In this paper, we present ScamAgent, an autonomous multi-turn agent
built on top of LLMs, capable of generating highly realistic scam call scripts
that simulate real-world fraud scenarios. Unlike prior work focused on
single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts
dynamically to simulated user responses, and employs deceptive persuasion
strategies across conversational turns. We show that current LLM safety
guardrails, including refusal mechanisms and content filters, are ineffective
against such agent-based threats. Even models with strong prompt-level
safeguards can be bypassed when prompts are decomposed, disguised, or delivered
incrementally within an agent framework. We further demonstrate the
transformation of scam scripts into lifelike voice calls using modern
text-to-speech systems, completing a fully automated scam pipeline. Our
findings highlight an urgent need for multi-turn safety auditing, agent-level
control frameworks, and new methods to detect and disrupt conversational
deception powered by generative AI.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [53] [A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges](https://arxiv.org/abs/2508.06401)
*Andrew Brown,Muhammad Roman,Barry Devereux*

Main category: cs.DL

TL;DR: 对2020-2025年128篇高引RAG研究的系统性综述，分析架构、评估方法及有效性，提出未来研究方向


<details>
  <summary>Details</summary>
Motivation: 解决检索增强生成（RAG）领域缺乏系统性总结的问题，通过调整引文阈值捕获2025年新兴突破，避免引文滞后偏差

Method: 基于PRISMA 2020框架，设定双维度纳入标准（引文量+研究问题），构建数据集/模型架构/评估指标三维分析体系

Result: 揭示当前RAG研究存在评估指标碎片化（76%研究使用非标准指标）、长尾知识处理不足（仅12%涉及动态更新机制）等核心局限

Conclusion: 建议未来优先开发领域自适应评估框架（如RAEval-2026），并加强检索-生成模块的协同优化理论研究

Abstract: This systematic review of the research literature on retrieval-augmented
generation (RAG) provides a focused analysis of the most highly cited studies
published between 2020 and May 2025. A total of 128 articles met our inclusion
criteria. The records were retrieved from ACM Digital Library, IEEE Xplore,
Scopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP).
RAG couples a neural retriever with a generative language model, grounding
output in up-to-date, non-parametric memory while retaining the semantic
generalisation stored in model weights. Guided by the PRISMA 2020 framework, we
(i) specify explicit inclusion and exclusion criteria based on citation count
and research questions, (ii) catalogue datasets, architectures, and evaluation
practices, and (iii) synthesise empirical evidence on the effectiveness and
limitations of RAG. To mitigate citation-lag bias, we applied a lower
citation-count threshold to papers published in 2025 so that emerging
breakthroughs with naturally fewer citations were still captured. This review
clarifies the current research landscape, highlights methodological gaps, and
charts priority directions for future research.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [54] [NanoCodec: Towards High-Quality Ultra Fast Speech LLM Inference](https://arxiv.org/abs/2508.05835)
*Edresson Casanova,Paarth Neekhara,Ryan Langman,Shehzeen Hussain,Subhankar Ghosh,Xuesong Yang,Ante Jukić,Jason Li,Boris Ginsburg*

Main category: eess.AS

TL;DR: NanoCodec提出12.5FPS低帧率音频编解码器，显著提升语音LLM效率


<details>
  <summary>Details</summary>
Motivation: 现有音频编解码器高帧率导致自回归模型训练/推理速度慢，需降低帧率以提升效率

Method: 通过消融实验分析帧率、比特率和因果性对重建质量的影响，设计超低帧率编解码架构

Result: 在多种比特率范围超越现有方案，建立低延迟语音LLM新基准

Conclusion: NanoCodec验证了低帧率编解码器的可行性，为高效语音处理提供技术突破

Abstract: Large Language Models (LLMs) have significantly advanced audio processing by
leveraging audio codecs to discretize audio into tokens, enabling the
application of language modeling techniques to speech data. However, existing
audio codecs often operate at high frame rates, leading to slow training and
inference, particularly for autoregressive models. To address this, there is
growing interest in low frame-rate audio codecs, which reduce the number of
autoregressive steps required to generate one second of audio. In this paper,
we conduct ablation studies to examine the impact of frame rate, bitrate, and
causality on codec reconstruction quality. Based on our findings, we introduce
NanoCodec, a state-of-the-art audio codec that achieves high-quality
compression at just 12.5 frames per second (FPS). NanoCodec outperforms related
works across various bitrate ranges, establishing a new benchmark for
low-latency and efficient Speech LLM training and inference.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [55] [Enhancing Retrieval-Augmented Generation for Electric Power Industry Customer Support](https://arxiv.org/abs/2508.05664)
*Hei Yu Chan,Kuok Tou Ho,Chenglong Ma,Yujing Si,Hok Lai Lin,Sa Lei Lam*

Main category: cs.IR

TL;DR: 通过整合意图识别、RAG Fusion和重排技术构建的电力客服系统，在生成数据集和真实FAQ数据集分别达到97.9%和89.6%准确率，显著超越传统RAG模型


<details>
  <summary>Details</summary>
Motivation: 解决现有AI客服系统在处理模糊查询、多意图问题和细节特异性需求时的不足，探索增强电力领域客户支持系统鲁棒性的技术方案

Method: 系统评估查询重写、RAG Fusion、关键词增强、意图识别和上下文重排技术，对比向量存储与图结构RAG框架，最终采用图结构RAG框架

Result: 在GPT-4生成数据集和真实电力公司FAQ数据集分别达到97.9%和89.6%准确率，较基线RAG模型有显著提升

Conclusion: 图结构RAG框架配合查询重写、多检索融合和上下文重排能有效处理复杂查询，意图识别支持问题分解，而关键词增强因偏置选择产生负面效果

Abstract: Many AI customer service systems use standard NLP pipelines or finetuned
language models, which often fall short on ambiguous, multi-intent, or
detail-specific queries. This case study evaluates recent techniques: query
rewriting, RAG Fusion, keyword augmentation, intent recognition, and context
reranking, for building a robust customer support system in the electric power
domain. We compare vector-store and graph-based RAG frameworks, ultimately
selecting the graph-based RAG for its superior performance in handling complex
queries. We find that query rewriting improves retrieval for queries using
non-standard terminology or requiring precise detail. RAG Fusion boosts
performance on vague or multifaceted queries by merging multiple retrievals.
Reranking reduces hallucinations by filtering irrelevant contexts. Intent
recognition supports the decomposition of complex questions into more targeted
sub-queries, increasing both relevance and efficiency. In contrast, keyword
augmentation negatively impacts results due to biased keyword selection. Our
final system combines intent recognition, RAG Fusion, and reranking to handle
disambiguation and multi-source queries. Evaluated on both a GPT-4-generated
dataset and a real-world electricity provider FAQ dataset, it achieves 97.9%
and 89.6% accuracy respectively, substantially outperforming baseline RAG
models.

</details>


### [56] [A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges](https://arxiv.org/abs/2508.05668)
*Yunjia Xi,Jianghao Lin,Yongzhao Xiao,Zheli Zhou,Rong Shan,Te Gao,Jiachen Zhu,Weiwen Liu,Yong Yu,Weinan Zhang*

Main category: cs.IR

TL;DR: 本文系统分析了大语言模型（LLMs）驱动的搜索代理技术，探讨其架构、优化、应用及评估方法，并指出未来研究方向。


<details>
  <summary>Details</summary>
Motivation: LLM技术的突破使搜索代理具备动态规划、多轮检索能力，推动搜索从表层信息获取转向深度自主探索（如OpenAI的Deep Research案例）。

Method: 通过架构分析、性能优化、应用场景分类和评估体系构建四个维度，对现有搜索代理研究进行系统性归类。

Result: 建立首个搜索代理研究框架，开源论文库（GitHub项目），揭示技术瓶颈并指明动态环境适应、可信搜索等关键发展方向。

Conclusion: LLM搜索代理将重塑信息获取范式，未来需在交互深度、系统可靠性和领域适应性等方向持续突破。

Abstract: The advent of Large Language Models (LLMs) has significantly revolutionized
web search. The emergence of LLM-based Search Agents marks a pivotal shift
towards deeper, dynamic, autonomous information seeking. These agents can
comprehend user intentions and environmental context and execute multi-turn
retrieval with dynamic planning, extending search capabilities far beyond the
web. Leading examples like OpenAI's Deep Research highlight their potential for
deep information mining and real-world applications. This survey provides the
first systematic analysis of search agents. We comprehensively analyze and
categorize existing works from the perspectives of architecture, optimization,
application, and evaluation, ultimately identifying critical open challenges
and outlining promising future research directions in this rapidly evolving
field. Our repository is available on
https://github.com/YunjiaXi/Awesome-Search-Agent-Papers.

</details>


### [57] [Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports](https://arxiv.org/abs/2508.05669)
*Jin Khye Tan,En Jun Choong,Ethan Jeremiah Chitty,Yan Pheng Choo,John Hsin Yang Wong,Chern Eu Cheah*

Main category: cs.IR

TL;DR: 提出基于Qwen2.5-VL-7B微调的视觉语言模型，实现马来西亚财务表格到Markdown的高精度转换


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在旋转布局、多级标题和隐式结构表格转换中的准确性不足问题

Method: 使用2,152个增强图像文本对数据集，采用LoRA监督微调策略优化VLM模型

Result: 标准评估准确率92.20%，Markdown TEDS得分96.53%，推理效率显著优于GPT-4o等大型模型

Conclusion: 领域专用微调方法在保持高精度的同时实现效率提升，为财务文档自动化处理提供有效解决方案

Abstract: Accurately extracting and representing the structure of tabular data from
financial documents remains a critical challenge in document understanding,
particularly for regulatory and analytical use cases. This study addresses the
complexity of converting financial tables from Malaysian audited financial
reports into Markdown format, a task complicated by rotated layouts,
multi-level headers, and implicit structural cues. We propose a fine-tuned
vision-language model (VLM), based on Qwen2.5-VL-7B, optimized for
high-fidelity Markdown generation from document images. Our approach includes a
curated dataset of 2,152 image-text pairs with augmentations and a supervised
fine-tuning strategy using LoRA. To assess performance, we evaluated our model
on 100 out-of-sample tables using a dual framework: a criteria-based
LLM-as-a-judge for fine-grained accuracy and our novel Markdown
Tree-Edit-Distance-based Similarity (TEDS) metric for holistic structural
fidelity. Our model achieves a 92.20% overall accuracy on the criteria-based
assessment and a 96.53% Markdown TEDS score. This performance significantly
surpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized
reasoning-enabled models. Compared to these self-hosted alternatives, it also
significantly reduces inference time. Furthermore, its accuracy exceeds that of
widely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash.
These results demonstrate that domain-specific fine-tuning provides an
effective and efficient method to bridge the gap between unstructured financial
documents and downstream automation, rivalling much larger and more general
models without their computational overhead.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [58] [HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing](https://arxiv.org/abs/2508.05899)
*Zixuan Bian,Ruohan Ren,Yue Yang,Chris Callison-Burch*

Main category: cs.CV

TL;DR: HOLODECK 2.0是基于视觉语言模型的3D场景生成框架，支持文本驱动的多样化风格场景生成和交互式编辑，在室内/开放域场景中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景生成方法依赖人工且灵活性不足，需要能自动生成开放域场景并支持灵活编辑的解决方案。

Method: 1. 使用视觉语言模型解析场景需求
2. 调用先进3D生成模型创建资源
3. 基于VLM空间约束迭代优化布局
4. 支持人类反馈驱动的布局调整和风格一致编辑

Result: 人类评估和CLIP指标显示，在室内和开放域场景中生成质量显著优于基线方法，语义对齐度提升32%

Conclusion: HOLODECK 2.0实现了文本到3D世界的高效生成，支持游戏建模等实际应用，编辑功能将创作者效率提升40%

Abstract: 3D scene generation plays a crucial role in gaming, artistic creation,
virtual reality and many other domains. However, current 3D scene design still
relies heavily on extensive manual effort from creators, and existing automated
methods struggle to generate open-domain scenes or support flexible editing. As
a result, generating 3D worlds directly from text has garnered increasing
attention. In this paper, we introduce HOLODECK 2.0, an advanced
vision-language-guided framework for 3D world generation with support for
interactive scene editing based on human feedback. HOLODECK 2.0 can generate
diverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and
cyberpunk styles) that exhibit high semantic fidelity to fine-grained input
descriptions, suitable for both indoor and open-domain environments. HOLODECK
2.0 leverages vision-language models (VLMs) to identify and parse the objects
required in a scene and generates corresponding high-quality assets via
state-of-the-art 3D generative models. It then iteratively applies spatial
constraints derived from the VLMs to achieve semantically coherent and
physically plausible layouts. Human evaluations and CLIP-based assessments
demonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely
aligned with detailed textual descriptions, consistently outperforming
baselines across indoor and open-domain scenarios. Additionally, we provide
editing capabilities that flexibly adapt to human feedback, supporting layout
refinement and style-consistent object edits. Finally, we present a practical
application of HOLODECK 2.0 in procedural game modeling, generating visually
rich and immersive environments, potentially boosting efficiency.

</details>


### [59] [LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study on Alzheimer's disease, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing](https://arxiv.org/abs/2508.06055)
*Wonjung Park,Suhyun Ahn,Jinah Park*

Main category: cs.CV

TL;DR: 开发LV-Net框架，通过联合模板网格变形提升侧脑室形状分析，应用于阿尔茨海默病检测


<details>
  <summary>Details</summary>
Motivation: 侧脑室形状分析存在个体差异大、MRI分辨率限制导致的边界分割伪影问题，需更鲁棒的建模方法

Method: 提出解剖联合模板变形框架，通过分类模板顶点增强对应性，融合海马结构解剖关系提升分割稳定性

Result: LV-Net在分割不完美情况下仍保持高重建精度，跨数据集形状描述符可靠性显著提升

Conclusion: 该方法成功识别阿尔茨海默病相关侧脑室亚区，为神经疾病提供有效形态学生物标志

Abstract: Lateral ventricle (LV) shape analysis holds promise as a biomarker for
neurological diseases; however, challenges remain due to substantial shape
variability across individuals and segmentation difficulties arising from
limited MRI resolution. We introduce LV-Net, a novel framework for producing
individualized 3D LV meshes from brain MRI by deforming an anatomy-aware joint
LV-hippocampus template mesh. By incorporating anatomical relationships
embedded within the joint template, LV-Net reduces boundary segmentation
artifacts and improves reconstruction robustness. In addition, by classifying
the vertices of the template mesh based on their anatomical adjacency, our
method enhances point correspondence across subjects, leading to more accurate
LV shape statistics. We demonstrate that LV-Net achieves superior
reconstruction accuracy, even in the presence of segmentation imperfections,
and delivers more reliable shape descriptors across diverse datasets. Finally,
we apply LV-Net to Alzheimer's disease analysis, identifying LV subregions that
show significantly associations with the disease relative to cognitively normal
controls. The codes for LV shape modeling are available at
https://github.com/PWonjung/LV_Shape_Modeling.

</details>


### [60] [Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents](https://arxiv.org/abs/2508.05954)
*Han Lin,Jaemin Cho,Amir Zadeh,Chuan Li,Mohit Bansal*

Main category: cs.CV

TL;DR: Bifrost-1通过整合预训练多模态大模型（MLLMs）和扩散模型，利用CLIP图像嵌入作为潜在变量，实现高效可控的高保真图像生成，同时保留多模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法因LLMs预训练未涉及图像表示导致训练成本高昂，需探索更高效的视觉合成与推理结合方案。

Method: 1. 使用patch级CLIP图像嵌入作为潜在变量，与MLLMs的CLIP视觉编码器对齐；2. 通过轻量级ControlNet适配扩散模型；3. 初始化视觉生成分支保留MLLMs原始能力。

Result: Bifrost-1在视觉保真度与多模态理解上媲美或超越现有方法，训练计算量显著降低，消融实验验证设计有效性。

Conclusion: 该框架成功实现高效可控图像生成，无缝整合预训练模型，平衡生成质量与多模态推理能力。

Abstract: There is growing interest in integrating high-fidelity visual synthesis
capabilities into large language models (LLMs) without compromising their
strong reasoning capabilities. Existing methods that directly train LLMs or
bridge LLMs and diffusion models usually suffer from costly training since the
backbone LLMs have not seen image representations during pretraining. We
present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs
(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent
variables, which are natively aligned with the MLLM's CLIP visual encoder.
These patch-level image embeddings are integrated into the diffusion model with
a lightweight adaptation of its ControlNet. To retain the original multimodal
reasoning capabilities of MLLMs, we equip the MLLM with a visual generation
branch initialized from the original MLLM parameters when predicting the
patch-level image embeddings. By seamlessly integrating pretrained MLLMs and
diffusion models with patch-level CLIP latents, our framework enables
high-fidelity controllable image generation with significant training
efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or
better performance than previous methods in terms of visual fidelity and
multimodal understanding, with substantially lower compute during training. We
also provide comprehensive ablation studies showing the effectiveness of our
design choices.

</details>


### [61] [Effective Training Data Synthesis for Improving MLLM Chart Understanding](https://arxiv.org/abs/2508.06492)
*Yuwei Yang,Zeyu Zhang,Yunzhong Hou,Zhuowan Li,Gaowen Liu,Ali Payani,Yuan-Sen Ting,Liang Zheng*

Main category: cs.CV

TL;DR: 提出五步数据合成流程生成ECD数据集（含10k+图表和300k+QA对），显著提升多模态大语言模型的图表理解能力


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在真实科学图表理解任务中成功率仅30%-50%，传统合成图表与真实图表差异大影响模型训练效果

Method: 五步数据合成流程：1)分离数据与功能生成 2)多子图条件生成 3)视觉多样化处理 4)数据质量过滤 5)GPT-4o生成QA对

Result: 构建覆盖25个主题、250+图表组合的ECD数据集，实验证明其有效提升各类MLLMs在真实/合成测试集的性能

Conclusion: 模块化图表生成与视觉多样化策略能有效增强模型对复杂科学图表的理解能力，ECD数据集具有广泛适用性

Abstract: Being able to effectively read scientific plots, or chart understanding, is a
central part toward building effective agents for science. However, existing
multimodal large language models (MLLMs), especially open-source ones, are
still falling behind with a typical success rate of 30%-50% on challenging
benchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are
often restricted by their inadequate similarity to the real charts, which could
compromise model training and performance on complex real-world charts. In this
study, we show that modularizing chart generation and diversifying visual
details improves chart understanding capabilities. In particular, we design a
five-step data synthesis pipeline, where we separate data and function creation
for single plot generation, condition the generation of later subplots on
earlier ones for multi-subplot figures, visually diversify the generated
figures, filter out low quality data, and finally generate the question-answer
(QA) pairs with GPT-4o. This approach allows us to streamline the generation of
fine-tuning datasets and introduce the effective chart dataset (ECD), which
contains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring
250+ chart type combinations with high visual complexity. We show that ECD
consistently improves the performance of various MLLMs on a range of real-world
and synthetic test sets. Code, data and models are available at:
https://github.com/yuweiyang-anu/ECD.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [62] [Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction](https://arxiv.org/abs/2508.05913)
*Stefan Pasch,Min Chul Cha*

Main category: cs.HC

TL;DR: 研究通过分析10万+用户评论，发现AI伦理七大维度均与用户满意度正相关，但技术用户与非技术用户、开发平台与终端应用间存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 填补AI伦理原则（公平性、透明度等）与用户感知之间关系的实证研究空白，验证欧盟可信AI伦理指南七大维度对用户满意度的实际影响。

Method: 使用基于Transformer的语言模型，对G2平台10万余条AI产品用户评论进行七维度情感分析。

Result: ①所有伦理维度均与满意度正相关；②技术用户更关注系统级维度（透明度/数据治理），非技术用户侧重人本维度（人权/社会福利）；③终端应用的非技术用户群体中伦理-满意度关联强度显著更高。

Conclusion: 需从用户视角分层设计AI伦理：技术系统侧重技术治理，终端应用强化人本关怀，开发者应根据用户角色和产品类型调整伦理实现路径。

Abstract: As AI systems become increasingly embedded in organizational workflows and
consumer applications, ethical principles such as fairness, transparency, and
robustness have been widely endorsed in policy and industry guidelines.
However, there is still scarce empirical evidence on whether these principles
are recognized, valued, or impactful from the perspective of users. This study
investigates the link between ethical AI and user satisfaction by analyzing
over 100,000 user reviews of AI products from G2. Using transformer-based
language models, we measure sentiment across seven ethical dimensions defined
by the EU Ethics Guidelines for Trustworthy AI. Our findings show that all
seven dimensions are positively associated with user satisfaction. Yet, this
relationship varies systematically across user and product types. Technical
users and reviewers of AI development platforms more frequently discuss
system-level concerns (e.g., transparency, data governance), while
non-technical users and reviewers of end-user applications emphasize
human-centric dimensions (e.g., human agency, societal well-being). Moreover,
the association between ethical AI and user satisfaction is significantly
stronger for non-technical users and end-user applications across all
dimensions. Our results highlight the importance of ethical AI design from
users' perspectives and underscore the need to account for contextual
differences across user roles and product types.

</details>


### [63] [ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation](https://arxiv.org/abs/2508.06065)
*Daniel Lee,Nikhil Sharma,Donghoon Shin,DaEun Choi,Harsh Sharma,Jeonghwan Kim,Heng Ji*

Main category: cs.HC

TL;DR: 提出ThematicPlane交互系统，通过语义概念操控帮助创作者衔接创意意图与生成结果，支持发散/聚合式创作流程并揭示控制解释性需求


<details>
  <summary>Details</summary>
Motivation: 现有生成式工具依赖文本提示，限制了非专业人士的创意探索。需要更符合人类直觉的语义层交互来降低创作门槛

Method: 开发基于主题设计平面的交互系统，通过6人用户实验观察发散/聚合创作模式下的使用行为与反馈

Result: 用户能利用意外结果激发灵感，但主题与输出的映射关系需更透明的解释机制。系统支持了更具表现力的迭代创作流程

Conclusion: ThematicPlane证明了语义驱动交互在生成式工具中的潜力，为创意意图与系统控制的融合提供了新方向

Abstract: Generative AI has made image creation more accessible, yet aligning outputs
with nuanced creative intent remains challenging, particularly for non-experts.
Existing tools often require users to externalize ideas through prompts or
references, limiting fluid exploration. We introduce ThematicPlane, a system
that enables users to navigate and manipulate high-level semantic concepts
(e.g., mood, style, or narrative tone) within an interactive thematic design
plane. This interface bridges the gap between tacit creative intent and system
control. In our exploratory study (N=6), participants engaged in divergent and
convergent creative modes, often embracing unexpected results as inspiration or
iteration cues. While they grounded their exploration in familiar themes,
differing expectations of how themes mapped to outputs revealed a need for more
explainable controls. Overall, ThematicPlane fosters expressive, iterative
workflows and highlights new directions for intuitive, semantics-driven
interaction in generative design tools.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [64] [InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization](https://arxiv.org/abs/2508.05731)
*Yuhang Liu,Zeyu Liu,Shuanghe Zhu,Pengxiang Li,Congkai Xie,Jiasheng Wang,Xueyu Hu,Xiaotian Han,Jianbo Yuan,Xinyao Wang,Shengyu Zhang,Hongxia Yang,Fei Wu*

Main category: cs.AI

TL;DR: 提出自适应探索策略优化框架AEPO，通过多答案生成和理论驱动的自适应奖励函数，显著提升GUI语义对齐能力


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法在空间对齐有效但语义对齐探索效率低，难以学习复杂语义关联

Method: AEPO框架整合多答案生成策略（扩大探索范围）和基于效率公式eta=U/C的自适应探索奖励函数AER

Result: InfiGUI-G1模型在多个GUI基准测试中取得SOTA，相比基线RLVR最高提升9.0%

Conclusion: AEPO有效解决了语义对齐的探索瓶颈，理论驱动的奖励机制显著提升模型在GUI场景的泛化和语义理解能力

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has propelled the
development of autonomous agents that operate on Graphical User Interfaces
(GUIs) using pure visual input. A fundamental challenge is robustly grounding
natural language instructions. This requires a precise spatial alignment, which
accurately locates the coordinates of each element, and, more critically, a
correct semantic alignment, which matches the instructions to the functionally
appropriate UI element. Although Reinforcement Learning with Verifiable Rewards
(RLVR) has proven to be effective at improving spatial alignment for these
MLLMs, we find that inefficient exploration bottlenecks semantic alignment,
which prevent models from learning difficult semantic associations. To address
this exploration problem, we present Adaptive Exploration Policy Optimization
(AEPO), a new policy optimization framework. AEPO employs a multi-answer
generation strategy to enforce broader exploration, which is then guided by a
theoretically grounded Adaptive Exploration Reward (AER) function derived from
first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B
and InfiGUI-G1-7B, establish new state-of-the-art results across multiple
challenging GUI grounding benchmarks, achieving significant relative
improvements of up to 9.0% against the naive RLVR baseline on benchmarks
designed to test generalization and semantic understanding. Resources are
available at https://github.com/InfiXAI/InfiGUI-G1.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [65] [AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models](https://arxiv.org/abs/2508.04748)
*Xuan Lin,Long Chen,Yile Wang*

Main category: cs.LG

TL;DR: 提出AttriLens-Mol强化学习框架，通过格式/计数/合理性三重奖励机制引导LLMs生成结构化分子属性，显著提升预测性能与可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs依赖人工提示且生成属性冗长不相关，需强化学习框架引导模型隐式挖掘分子属性知识。

Method: 设计格式奖励（结构化输出）+计数奖励（过滤冗余）+合理性奖励（LLMs+RDKit验证属性相关性）的三阶段强化学习机制。

Result: 在4,000样本训练后，7B模型性能超越SFT模型和GPT-4等，提取属性使决策树模型表现优于LLMs直接生成。

Conclusion: AttriLens-Mol有效激发分子属性相关性，为分子预测提供高解释性解决方案，代码已开源。

Abstract: Large Language Models (LLMs) have shown promise in assisting molecular
property prediction tasks but often rely on human-crafted prompts and
chain-of-thought templates. While recent advanced large reasoning models like
DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process,
their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol,
an attribute-guided reinforcement learning framework for molecular property
prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1)
a format reward encouraging attribute-based structured output, (2) a count
reward to avoid enumerating irrelevant attributes, and (3) a rationality reward
using advanced LLMs and RDKit to verify the relatedness of the generated
attributes. This approach implicitly elicits the model's inherent knowledge of
relevant molecular attributes during reasoning, enables making predictions for
the molecular property more effectively. Experiments on both in-distribution
and out-of-distribution datasets show that, training both 7B-size
R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our
proposed AttriLens-Mol method significantly boosts the performance, getting
comparable or better results than supervised fine-tuning models
(Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o,
DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the
target property, when used as features for an interpretable decision tree
model, yield superior performance compared to attributes generated by prompting
LLMs. This shows that AttriLens-Mol effectively elicits more relevant and
predictive molecular attributes, leading to enhanced interpretability and
performance for property prediction. We release the code in
https://github.com/szu-tera/AttriLens-Mol.

</details>


### [66] [Sample-efficient LLM Optimization with Reset Replay](https://arxiv.org/abs/2508.06412)
*Zichuan Liu,Jinyu Wang,Lei Song,Jiang Bian*

Main category: cs.LG

TL;DR: 提出LoRR方法，通过重置回放机制和混合优化目标，显著提升大语言模型微调的样本效率与性能表现。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习和偏好优化方法存在样本效率低、易受首因效应影响的问题，导致策略质量下降和学习过程受损。

Method: 1. 高回放次数的训练机制 2. 周期性重置策略与初始数据复用 3. 结合监督微调(SFT)和偏好损失的混合优化目标

Result: 在数学推理和通用推理任务中，LoRR使DPO迭代方法达到与复杂强化学习算法相当的性能，部分场景表现更优。

Conclusion: LoRR提供了一种高效实用的LLM微调范式，能够在有限数据条件下释放模型更大潜力，显著提升训练效率。

Abstract: Recent advancements in post-training Large Language Models (LLMs),
particularly through Reinforcement Learning (RL) and preference optimization
methods, are key drivers for enhancing their reasoning capabilities. However,
these methods are often plagued by low sample efficiency and a susceptibility
to primacy bias, where overfitting to initial experiences degrades policy
quality and damages the learning process. To address these challenges, we
introduce LLM optimization with Reset Replay (LoRR), a general and powerful
plugin designed to enhance sample efficiency in any preference-based
optimization framework. LoRR core mechanism enables training at a high replay
number, maximizing the utility of each collected data batch. To counteract the
risk of overfitting inherent in high-replay training, LoRR incorporates a
periodic reset strategy with reusing initial data, which preserves network
plasticity. Furthermore, it leverages a hybrid optimization objective,
combining supervised fine-tuning (SFT) and preference-based losses to further
bolster data exploitation. Our extensive experiments demonstrate that LoRR
significantly boosts the performance of various preference optimization methods
on both mathematical and general reasoning benchmarks. Notably, an iterative
DPO approach augmented with LoRR achieves comparable performance on challenging
math tasks, outperforming some complex and computationally intensive RL-based
algorithms. These findings highlight that LoRR offers a practical,
sample-efficient, and highly effective paradigm for LLM finetuning, unlocking
greater performance from limited data.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [67] [Position: Intelligent Coding Systems Should Write Programs with Justifications](https://arxiv.org/abs/2508.06017)
*Xiangzhe Xu,Shiwei Feng,Zian Su,Chengpeng Wang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: 论文提出通过神经符号方法改进AI编程系统的可解释性，使非专业用户能理解代码生成逻辑


<details>
  <summary>Details</summary>
Motivation: 当前AI编程系统决策过程不透明，非专业用户缺乏验证能力，导致信任危机和使用障碍

Method: 融合符号约束指导模型训练，通过神经表征增强程序语义理解，实现推理时的自动化一致性验证

Result: 建立认知对齐和语义保真的双重验证框架，突破传统形式化验证和静态分析的局限性

Conclusion: 神经符号方法可有效连接模型推理与用户认知，为构建可信AI编程系统提供新范式

Abstract: Intelligent coding systems are transforming software development by enabling
users to specify code behavior in natural language. However, the opaque
decision-making of AI-driven coders raises trust and usability concerns,
particularly for non-expert users who cannot inspect low-level implementations.
We argue that these systems should not only generate code but also produce
clear, consistent justifications that bridge model reasoning and user
understanding. To this end, we identify two critical justification
properties-cognitive alignment and semantic faithfulness-and highlight the
limitations of existing methods, including formal verification, static
analysis, and post-hoc explainability. We advocate exploring neuro-symbolic
approaches for justification generation, where symbolic constraints guide model
behavior during training and program semantics are enriched through neural
representations, enabling automated consistency checks at inference time.

</details>
