{"id": "2509.00040", "pdf": "https://arxiv.org/pdf/2509.00040", "abs": "https://arxiv.org/abs/2509.00040", "authors": ["Chengkai Dai", "Tao Liu", "Dezhao Guo", "Binzhi Sun", "Guoxin Fang", "Yeung Yam", "Charlie C. L. Wang"], "title": "Curve-based slicer for multi-axis DLP 3D printing", "categories": ["cs.GR", "cs.RO"], "comment": null, "summary": "This paper introduces a novel curve-based slicing method for generating\nplanar layers with dynamically varying orientations in digital light processing\n(DLP) 3D printing. Our approach effectively addresses key challenges in DLP\nprinting, such as regions with large overhangs and staircase artifacts, while\npreserving its intrinsic advantages of high resolution and fast printing\nspeeds. We formulate the slicing problem as an optimization task, in which\nparametric curves are computed to define both the slicing layers and the model\npartitioning through their tangent planes. These curves inherently define\nmotion trajectories for the build platform and can be optimized to meet\ncritical manufacturing objectives, including collision-free motion and\nfloating-free deposition. We validate our method through physical experiments\non a robotic multi-axis DLP printing setup, demonstrating that the optimized\ncurves can robustly guide smooth, high-quality fabrication of complex\ngeometries."}
{"id": "2509.00052", "pdf": "https://arxiv.org/pdf/2509.00052", "abs": "https://arxiv.org/abs/2509.00052", "authors": ["Jianzhi Long", "Wenhao Sun", "Rongcheng Tu", "Dacheng Tao"], "title": "Lightning Fast Caching-based Parallel Denoising Prediction for Accelerating Talking Head Generation", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Diffusion-based talking head models generate high-quality, photorealistic\nvideos but suffer from slow inference, limiting practical applications.\nExisting acceleration methods for general diffusion models fail to exploit the\ntemporal and spatial redundancies unique to talking head generation. In this\npaper, we propose a task-specific framework addressing these inefficiencies\nthrough two key innovations. First, we introduce Lightning-fast Caching-based\nParallel denoising prediction (LightningCP), caching static features to bypass\nmost model layers in inference time. We also enable parallel prediction using\ncached features and estimated noisy latents as inputs, efficiently bypassing\nsequential sampling. Second, we propose Decoupled Foreground Attention (DFA) to\nfurther accelerate attention computations, exploiting the spatial decoupling in\ntalking head videos to restrict attention to dynamic foreground regions.\nAdditionally, we remove reference features in certain layers to bring extra\nspeedup. Extensive experiments demonstrate that our framework significantly\nimproves inference speed while preserving video quality."}
{"id": "2509.00180", "pdf": "https://arxiv.org/pdf/2509.00180", "abs": "https://arxiv.org/abs/2509.00180", "authors": ["Nguyen Phan", "Guoning Chen"], "title": "Evaluate Neighbor Search for Curve-based Vector Field Processing", "categories": ["cs.GR", "cs.CG"], "comment": "12 pages, 17 figures", "summary": "Curve-based representations, particularly integral curves, are often used to\nrepresent large-scale computational fluid dynamic simulations. Processing and\nanalyzing curve-based vector field data sets often involves searching for\nneighboring segments given a query point or curve segment. However, because the\noriginal flow behavior may not be fully represented by the set of integral\ncurves and the input integral curves may not be evenly distributed in space,\npopular neighbor search strategies often return skewed and redundant\nneighboring segments. Yet, there is a lack of systematic and comprehensive\nresearch on how different configurations of neighboring segments returned by\nspecific neighbor search strategies affect subsequent tasks. To fill this gap,\nthis study evaluates the performance of two popular neighbor search strategies\ncombined with different distance metrics on a point-based vector field\nreconstruction task and a segment saliency estimation using input integral\ncurves. A large number of reconstruction tests and saliency calculations are\nconducted for the study. To characterize the configurations of neighboring\nsegments for an effective comparison of different search strategies, a number\nof measures, like average neighbor distance and uniformity, are proposed. Our\nstudy leads to a few observations that partially confirm our expectations about\nthe ideal configurations of a neighborhood while revealing additional findings\nthat were overlooked by the community."}
{"id": "2509.00269", "pdf": "https://arxiv.org/pdf/2509.00269", "abs": "https://arxiv.org/abs/2509.00269", "authors": ["Maria Parelli", "Michael Oechsle", "Michael Niemeyer", "Federico Tombari", "Andreas Geiger"], "title": "3D-LATTE: Latent Space 3D Editing from Textual Instructions", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Despite the recent success of multi-view diffusion models for\ntext/image-based 3D asset generation, instruction-based editing of 3D assets\nlacks surprisingly far behind the quality of generation models. The main reason\nis that recent approaches using 2D priors suffer from view-inconsistent editing\nsignals. Going beyond 2D prior distillation methods and multi-view editing\nstrategies, we propose a training-free editing method that operates within the\nlatent space of a native 3D diffusion model, allowing us to directly manipulate\n3D geometry. We guide the edit synthesis by blending 3D attention maps from the\ngeneration with the source object. Coupled with geometry-aware regularization\nguidance, a spectral modulation strategy in the Fourier domain and a refinement\nstep for 3D enhancement, our method outperforms previous 3D editing methods\nenabling high-fidelity, precise, and robust edits across a wide range of shapes\nand semantic manipulations."}
{"id": "2509.00030", "pdf": "https://arxiv.org/pdf/2509.00030", "abs": "https://arxiv.org/abs/2509.00030", "authors": ["Marshall Thomas", "Edward Fish", "Richard Bowden"], "title": "MultiStream-LLM: Bridging Modalities for Robust Sign Language Translation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Despite progress in gloss-free Sign Language Translation (SLT), monolithic\nend-to-end models consistently fail on two critical components of natural\nsigning: the precise recognition of high-speed fingerspelling and the\nintegration of asynchronous non-manual cues from the face. Recent progress in\nAutomated Sign Language Translation with Large Language Models has side stepped\nthis challenge, forcing a single network to learn these simultaneously\nresulting in poor performance when tasked with translating crucial information\nsuch as names,places, and technical terms. We introduce MultiStream-LLM, a\nmodular framework designed to overcome these limitations. Our approach employs\nseparate, specialized predictors for continuous signing, fingerspelling, and\nlipreading. Each expert network first decodes its specific modality into a\nsequence of tokens. These parallel streams are then fused by a lightweight\ntransformer that resolves temporal misalignments before passing the combined\nrepresentation to a Large Language Model (LLM) for final sentence generation.\nOur method establishes a new state-of-the-art on the How2Sign benchmark with a\nBLEU-4 score of 23.5 and achieves 73.2% letter accuracy on the challenging\nChicagoFSWildPlus fingerspelling dataset. These results validate our core\nhypothesis: by isolating and solving distinct recogni tion tasks before fusion,\nour multi-expert approach provides a more powerful and effective pathway to\nrobust, high-fidelity sign language translation."}
{"id": "2509.00406", "pdf": "https://arxiv.org/pdf/2509.00406", "abs": "https://arxiv.org/abs/2509.00406", "authors": ["Ahmed H. Mahmoud", "Jonathan Ragan-Kelley", "Justin Solomon"], "title": "Locality-Aware Automatic Differentiation on the GPU for Mesh-Based Computations", "categories": ["cs.GR"], "comment": null, "summary": "We present a high-performance system for automatic differentiation (AD) of\nfunctions defined on triangle meshes that exploits the inherent sparsity and\nlocality of mesh-based energy functions to achieve fast gradient and Hessian\ncomputation on the GPU. Our system is designed around per-element forward-mode\ndifferentiation, enabling all local computations to remain in GPU registers or\nshared memory. Unlike reverse-mode approaches that construct and traverse\nglobal computation graphs, our method performs differentiation on the fly,\nminimizing memory traffic and avoiding global synchronization. Our programming\nmodel allows users to define local energy terms while the system handles\nparallel evaluation, derivative computation, and sparse Hessian assembly. We\nbenchmark our system on a range of applications--cloth simulation, surface\nparameterization, mesh smoothing, and spherical manifold optimization. We\nachieve a geometric mean speedup of 6.2x over optimized PyTorch implementations\nfor second-order derivatives, and 2.76x speedup for Hessian-vector products.\nFor first-order derivatives, our system is 6.38x, 2.89x, and 1.98x faster than\nWarp, JAX, and Dr.JIT, respectively, while remaining on par with hand-written\nderivatives."}
{"id": "2509.00038", "pdf": "https://arxiv.org/pdf/2509.00038", "abs": "https://arxiv.org/abs/2509.00038", "authors": ["Teo Susnjak"], "title": "Compiling Prompts, Not Crafting Them: A Reproducible Workflow for AI-Assisted Evidence Synthesis", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) offer significant potential to accelerate\nsystematic literature reviews (SLRs), yet current approaches often rely on\nbrittle, manually crafted prompts that compromise reliability and\nreproducibility. This fragility undermines scientific confidence in\nLLM-assisted evidence synthesis. In response, this work adapts recent advances\nin declarative prompt optimisation, developed for general-purpose LLM\napplications, and demonstrates their applicability to the domain of SLR\nautomation. This research proposes a structured, domain-specific framework that\nembeds task declarations, test suites, and automated prompt tuning into a\nreproducible SLR workflow. These emerging methods are translated into a\nconcrete blueprint with working code examples, enabling researchers to\nconstruct verifiable LLM pipelines that align with established principles of\ntransparency and rigour in evidence synthesis. This is a novel application of\nsuch approaches to SLR pipelines."}
{"id": "2509.00541", "pdf": "https://arxiv.org/pdf/2509.00541", "abs": "https://arxiv.org/abs/2509.00541", "authors": ["Siyi Liu", "Weiming Chen", "Yushun Tang", "Zhihai He"], "title": "LatentEdit: Adaptive Latent Control for Consistent Semantic Editing", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Accepted by PRCV 2025", "summary": "Diffusion-based Image Editing has achieved significant success in recent\nyears. However, it remains challenging to achieve high-quality image editing\nwhile maintaining the background similarity without sacrificing speed or memory\nefficiency. In this work, we introduce LatentEdit, an adaptive latent fusion\nframework that dynamically combines the current latent code with a reference\nlatent code inverted from the source image. By selectively preserving source\nfeatures in high-similarity, semantically important regions while generating\ntarget content in other regions guided by the target prompt, LatentEdit enables\nfine-grained, controllable editing. Critically, the method requires no internal\nmodel modifications or complex attention mechanisms, offering a lightweight,\nplug-and-play solution compatible with both UNet-based and DiT-based\narchitectures. Extensive experiments on the PIE-Bench dataset demonstrate that\nour proposed LatentEdit achieves an optimal balance between fidelity and\neditability, outperforming the state-of-the-art method even in 8-15 steps.\nAdditionally, its inversion-free variant further halves the number of neural\nfunction evaluations and eliminates the need for storing any intermediate\nvariables, substantially enhancing real-time deployment efficiency."}
{"id": "2509.00185", "pdf": "https://arxiv.org/pdf/2509.00185", "abs": "https://arxiv.org/abs/2509.00185", "authors": ["Jian Wu", "Sarah Rajtmajer"], "title": "What Are Research Hypotheses?", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, accepted by Sci-K'25: International Workshop on Scientific\n  Knowledge", "summary": "Over the past decades, alongside advancements in natural language processing,\nsignificant attention has been paid to training models to automatically\nextract, understand, test, and generate hypotheses in open and scientific\ndomains. However, interpretations of the term \\emph{hypothesis} for various\nnatural language understanding (NLU) tasks have migrated from traditional\ndefinitions in the natural, social, and formal sciences. Even within NLU, we\nobserve differences defining hypotheses across literature. In this paper, we\noverview and delineate various definitions of hypothesis. Especially, we\ndiscern the nuances of definitions across recently published NLU tasks. We\nhighlight the importance of well-structured and well-defined hypotheses,\nparticularly as we move toward a machine-interpretable scholarly record."}
{"id": "2509.00777", "pdf": "https://arxiv.org/pdf/2509.00777", "abs": "https://arxiv.org/abs/2509.00777", "authors": ["Xiaokang Wei", "Zizheng Yan", "Zhangyang Xiong", "Yiming Hao", "Yipeng Qin", "Xiaoguang Han"], "title": "IntrinsicReal: Adapting IntrinsicAnything from Synthetic to Real Objects", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Estimating albedo (a.k.a., intrinsic image decomposition) from single RGB\nimages captured in real-world environments (e.g., the MVImgNet dataset)\npresents a significant challenge due to the absence of paired images and their\nground truth albedos. Therefore, while recent methods (e.g., IntrinsicAnything)\nhave achieved breakthroughs by harnessing powerful diffusion priors, they\nremain predominantly trained on large-scale synthetic datasets (e.g.,\nObjaverse) and applied directly to real-world RGB images, which ignores the\nlarge domain gap between synthetic and real-world data and leads to suboptimal\ngeneralization performance. In this work, we address this gap by proposing\nIntrinsicReal, a novel domain adaptation framework that bridges the\nabove-mentioned domain gap for real-world intrinsic image decomposition.\nSpecifically, our IntrinsicReal adapts IntrinsicAnything to the real domain by\nfine-tuning it using its high-quality output albedos selected by a novel dual\npseudo-labeling strategy: i) pseudo-labeling with an absolute confidence\nthreshold on classifier predictions, and ii) pseudo-labeling using the relative\npreference ranking of classifier predictions for individual input objects. This\nstrategy is inspired by human evaluation, where identifying the highest-quality\noutputs is straightforward, but absolute scores become less reliable for\nsub-optimal cases. In these situations, relative comparisons of outputs become\nmore accurate. To implement this, we propose a novel two-phase pipeline that\nsequentially applies these pseudo-labeling techniques to effectively adapt\nIntrinsicAnything to the real domain. Experimental results show that our\nIntrinsicReal significantly outperforms existing methods, achieving\nstate-of-the-art results for albedo estimation on both synthetic and real-world\ndatasets."}
{"id": "2509.00190", "pdf": "https://arxiv.org/pdf/2509.00190", "abs": "https://arxiv.org/abs/2509.00190", "authors": ["Sheldon Yu", "Yuxin Xiong", "Junda Wu", "Xintong Li", "Tong Yu", "Xiang Chen", "Ritwik Sinha", "Jingbo Shang", "Julian McAuley"], "title": "Explainable Chain-of-Thought Reasoning: An Empirical Analysis on State-Aware Reasoning Dynamics", "categories": ["cs.CL", "cs.AI"], "comment": "5 pages, 4 figures", "summary": "Recent advances in chain-of-thought (CoT) prompting have enabled large\nlanguage models (LLMs) to perform multi-step reasoning. However, the\nexplainability of such reasoning remains limited, with prior work primarily\nfocusing on local token-level attribution, such that the high-level semantic\nroles of reasoning steps and their transitions remain underexplored. In this\npaper, we introduce a state-aware transition framework that abstracts CoT\ntrajectories into structured latent dynamics. Specifically, to capture the\nevolving semantics of CoT reasoning, each reasoning step is represented via\nspectral analysis of token-level embeddings and clustered into semantically\ncoherent latent states. To characterize the global structure of reasoning, we\nmodel their progression as a Markov chain, yielding a structured and\ninterpretable view of the reasoning process. This abstraction supports a range\nof analyses, including semantic role identification, temporal pattern\nvisualization, and consistency evaluation."}
{"id": "2509.01134", "pdf": "https://arxiv.org/pdf/2509.01134", "abs": "https://arxiv.org/abs/2509.01134", "authors": ["Xilong Zhou", "Pedro Figueiredo", "Miloš Hašan", "Valentin Deschaintre", "Paul Guerrero", "Yiwei Hu", "Nima Khademi Kalantari"], "title": "RealMat: Realistic Materials with Diffusion and Reinforcement Learning", "categories": ["cs.GR", "cs.CV"], "comment": "11 pages, 11 figures", "summary": "Generative models for high-quality materials are particularly desirable to\nmake 3D content authoring more accessible. However, the majority of material\ngeneration methods are trained on synthetic data. Synthetic data provides\nprecise supervision for material maps, which is convenient but also tends to\ncreate a significant visual gap with real-world materials. Alternatively,\nrecent work used a small dataset of real flash photographs to guarantee\nrealism, however such data is limited in scale and diversity. To address these\nlimitations, we propose RealMat, a diffusion-based material generator that\nleverages realistic priors, including a text-to-image model and a dataset of\nrealistic material photos under natural lighting. In RealMat, we first finetune\na pretrained Stable Diffusion XL (SDXL) with synthetic material maps arranged\nin $2 \\times 2$ grids. This way, our model inherits some realism of SDXL while\nlearning the data distribution of the synthetic material grids. Still, this\ncreates a realism gap, with some generated materials appearing synthetic. We\npropose to further finetune our model through reinforcement learning (RL),\nencouraging the generation of realistic materials. We develop a realism reward\nfunction for any material image under natural lighting, by collecting a\nlarge-scale dataset of realistic material images. We show that this approach\nincreases generated materials' realism compared to our base model and related\nwork."}
{"id": "2509.00245", "pdf": "https://arxiv.org/pdf/2509.00245", "abs": "https://arxiv.org/abs/2509.00245", "authors": ["Seiji Maekawa", "Hayate Iso", "Nikita Bhutani"], "title": "The Rarity Blind Spot: A Framework for Evaluating Statistical Reasoning in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Effective decision-making often relies on identifying what makes each\ncandidate distinctive. While existing benchmarks for LLMs emphasize retrieving\nor summarizing information relevant to a given query, they do not evaluate a\nmodel's ability to identify globally distinctive features across a set of\ndocuments. We introduce Distinctive Feature Mining (DFM), a new task that\nchallenges models to analyze a small-to-medium collection (10-40 documents) and\nsurface features that are rare in the global context (e.g., appearing in less\nthan 10% of documents). This setting mirrors real-world scenarios such as\ncandidate selection or product differentiation, where statistical reasoning,\nnot retrieval, is key. To enable systematic evaluation of this capability, we\npresent DiFBench, a configurable benchmark creation framework with controllable\nparameters such as document set size and distinctiveness thresholds. Using\nDiFBench, we perform a large-scale assessment of distinctive feature mining\nacross ten state-of-the-art LLMs. Our findings reveal a significant performance\ngap between general-purpose and reasoning-enhanced models. All models, however,\nsubstantially degrade as the task complexity and document count increase. We\nalso find that a common failure mode is misidentifying frequent features as\ndistinctive. These insights reveal core limitations in contemporary LLMs'\nabilities to perform fine-grained, statistical reasoning and rarity detection."}
{"id": "2509.01442", "pdf": "https://arxiv.org/pdf/2509.01442", "abs": "https://arxiv.org/abs/2509.01442", "authors": ["João S. Ferreira", "Arianna Crippa", "Astryd Park", "Daniel Bultrini", "Pierre Fromholz", "Roman Lipski", "Karl Jansen", "James R. Wootton"], "title": "Quantum Brush: A quantum computing-based tool for digital painting", "categories": ["cs.GR", "cs.ET", "cs.MM", "physics.soc-ph", "quant-ph"], "comment": null, "summary": "We present Quantum Brush, an open-source digital painting tool that harnesses\nquantum computing to generate novel artistic expressions. The tool includes\nfour different brushes that translate strokes into unique quantum algorithms,\neach highlighting a different way in which quantum effects can produce novel\naesthetics. Each brush is designed to be compatible with the current noisy\nintermediate-scale quantum (NISQ) devices, as demonstrated by executing them on\nIQM's Sirius device."}
{"id": "2509.00248", "pdf": "https://arxiv.org/pdf/2509.00248", "abs": "https://arxiv.org/abs/2509.00248", "authors": ["Zachary K. Stine", "James E. Deitrick"], "title": "The Differential Meaning of Models: A Framework for Analyzing the Structural Consequences of Semantic Modeling Decisions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The proliferation of methods for modeling of human meaning-making constitutes\na powerful class of instruments for the analysis of complex semiotic systems.\nHowever, the field lacks a general theoretical framework for describing these\nmodeling practices across various model types in an apples-to-apples way. In\nthis paper, we propose such a framework grounded in the semiotic theory of C.\nS. Peirce. We argue that such models measure latent symbol geometries, which\ncan be understood as hypotheses about the complex of semiotic agencies\nunderlying a symbolic dataset. Further, we argue that in contexts where a\nmodel's value cannot be straightforwardly captured by proxy measures of\nperformance, models can instead be understood relationally, so that the\nparticular interpretive lens of a model becomes visible through its contrast\nwith other models. This forms the basis of a theory of model semantics in which\nmodels, and the modeling decisions that constitute them, are themselves treated\nas signs. In addition to proposing the framework, we illustrate its empirical\nuse with a few brief examples and consider foundational questions and future\ndirections enabled by the framework."}
{"id": "2509.01839", "pdf": "https://arxiv.org/pdf/2509.01839", "abs": "https://arxiv.org/abs/2509.01839", "authors": ["Akis Nousias", "Stavros Nousias"], "title": "HodgeFormer: Transformers for Learnable Operators on Triangular Meshes through Data-Driven Hodge Matrices", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "12 pages, 11 figures, 9 tables", "summary": "Currently, prominent Transformer architectures applied on graphs and meshes\nfor shape analysis tasks employ traditional attention layers that heavily\nutilize spectral features requiring costly eigenvalue decomposition-based\nmethods. To encode the mesh structure, these methods derive positional\nembeddings, that heavily rely on eigenvalue decomposition based operations,\ne.g. on the Laplacian matrix, or on heat-kernel signatures, which are then\nconcatenated to the input features. This paper proposes a novel approach\ninspired by the explicit construction of the Hodge Laplacian operator in\nDiscrete Exterior Calculus as a product of discrete Hodge operators and\nexterior derivatives, i.e. $(L := \\star_0^{-1} d_0^T \\star_1 d_0)$. We adjust\nthe Transformer architecture in a novel deep learning layer that utilizes the\nmulti-head attention mechanism to approximate Hodge matrices $\\star_0$,\n$\\star_1$ and $\\star_2$ and learn families of discrete operators $L$ that act\non mesh vertices, edges and faces. Our approach results in a\ncomputationally-efficient architecture that achieves comparable performance in\nmesh segmentation and classification tasks, through a direct learning\nframework, while eliminating the need for costly eigenvalue decomposition\noperations or complex preprocessing operations."}
{"id": "2509.00250", "pdf": "https://arxiv.org/pdf/2509.00250", "abs": "https://arxiv.org/abs/2509.00250", "authors": ["Hugo Sousa", "Ricardo Campos", "Alípio Jorge"], "title": "The Temporal Game: A New Perspective on Temporal Relation Extraction", "categories": ["cs.CL"], "comment": null, "summary": "In this paper we demo the Temporal Game, a novel approach to temporal\nrelation extraction that casts the task as an interactive game. Instead of\ndirectly annotating interval-level relations, our approach decomposes them into\npoint-wise comparisons between the start and end points of temporal entities.\nAt each step, players classify a single point relation, and the system applies\ntemporal closure to infer additional relations and enforce consistency. This\npoint-based strategy naturally supports both interval and instant entities,\nenabling more fine-grained and flexible annotation than any previous approach.\nThe Temporal Game also lays the groundwork for training reinforcement learning\nagents, by treating temporal annotation as a sequential decision-making task.\nTo showcase this potential, the demo presented in this paper includes a Game\nmode, in which users annotate texts from the TempEval-3 dataset and receive\nfeedback based on a scoring system, and an Annotation mode, that allows custom\ndocuments to be annotated and resulting timeline to be exported. Therefore,\nthis demo serves both as a research tool and an annotation interface. The demo\nis publicly available at https://temporal-game.inesctec.pt, and the source code\nis open-sourced to foster further research and community-driven development in\ntemporal reasoning and annotation."}
{"id": "2509.02141", "pdf": "https://arxiv.org/pdf/2509.02141", "abs": "https://arxiv.org/abs/2509.02141", "authors": ["Mohit Mendiratta", "Mayur Deshmukh", "Kartik Teotia", "Vladislav Golyanik", "Adam Kortylewski", "Christian Theobalt"], "title": "GRMM: Real-Time High-Fidelity Gaussian Morphable Head Model with Learned Residuals", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://mohitm1994.github.io/GRMM/", "summary": "3D Morphable Models (3DMMs) enable controllable facial geometry and\nexpression editing for reconstruction, animation, and AR/VR, but traditional\nPCA-based mesh models are limited in resolution, detail, and photorealism.\nNeural volumetric methods improve realism but remain too slow for interactive\nuse. Recent Gaussian Splatting (3DGS) based facial models achieve fast,\nhigh-quality rendering but still depend solely on a mesh-based 3DMM prior for\nexpression control, limiting their ability to capture fine-grained geometry,\nexpressions, and full-head coverage. We introduce GRMM, the first full-head\nGaussian 3D morphable model that augments a base 3DMM with residual geometry\nand appearance components, additive refinements that recover high-frequency\ndetails such as wrinkles, fine skin texture, and hairline variations. GRMM\nprovides disentangled control through low-dimensional, interpretable parameters\n(e.g., identity shape, facial expressions) while separately modelling residuals\nthat capture subject- and expression-specific detail beyond the base model's\ncapacity. Coarse decoders produce vertex-level mesh deformations, fine decoders\nrepresent per-Gaussian appearance, and a lightweight CNN refines rasterised\nimages for enhanced realism, all while maintaining 75 FPS real-time rendering.\nTo learn consistent, high-fidelity residuals, we present EXPRESS-50, the first\ndataset with 60 aligned expressions across 50 identities, enabling robust\ndisentanglement of identity and expression in Gaussian-based 3DMMs. Across\nmonocular 3D face reconstruction, novel-view synthesis, and expression\ntransfer, GRMM surpasses state-of-the-art methods in fidelity and expression\naccuracy while delivering interactive real-time performance."}
{"id": "2509.00276", "pdf": "https://arxiv.org/pdf/2509.00276", "abs": "https://arxiv.org/abs/2509.00276", "authors": ["Yuxiang Liu", "Tian Wang", "Gourab Kundu", "Tianyu Cao", "Guang Cheng", "Zhen Ge", "Jianshu Chen", "Qingjun Cui", "Trishul Chilimbi"], "title": "Exploring Reasoning-Infused Text Embedding with Large Language Models for Zero-Shot Dense Retrieval", "categories": ["cs.CL"], "comment": "CIKM 2025", "summary": "Transformer-based models such as BERT and E5 have significantly advanced text\nembedding by capturing rich contextual representations. However, many complex\nreal-world queries require sophisticated reasoning to retrieve relevant\ndocuments beyond surface-level lexical matching, where encoder-only retrievers\noften fall short. Decoder-only large language models (LLMs), known for their\nstrong reasoning capabilities, offer a promising alternative. Despite this\npotential, existing LLM-based embedding methods primarily focus on contextual\nrepresentation and do not fully exploit the reasoning strength of LLMs. To\nbridge this gap, we propose Reasoning-Infused Text Embedding (RITE), a simple\nbut effective approach that integrates logical reasoning into the text\nembedding process using generative LLMs. RITE builds upon existing language\nmodel embedding techniques by generating intermediate reasoning texts in the\ntoken space before computing embeddings, thereby enriching representations with\ninferential depth. Experimental results on BRIGHT, a reasoning-intensive\nretrieval benchmark, demonstrate that RITE significantly enhances zero-shot\nretrieval performance across diverse domains, underscoring the effectiveness of\nincorporating reasoning into the embedding process."}
{"id": "2509.02278", "pdf": "https://arxiv.org/pdf/2509.02278", "abs": "https://arxiv.org/abs/2509.02278", "authors": ["Zikai Huang", "Yihan Zhou", "Xuemiao Xu", "Cheng Xu", "Xiaofen Xing", "Jing Qin", "Shengfeng He"], "title": "Think2Sing: Orchestrating Structured Motion Subtitles for Singing-Driven 3D Head Animation", "categories": ["cs.GR", "cs.AI", "cs.MM"], "comment": null, "summary": "Singing-driven 3D head animation is a challenging yet promising task with\napplications in virtual avatars, entertainment, and education. Unlike speech,\nsinging involves richer emotional nuance, dynamic prosody, and lyric-based\nsemantics, requiring the synthesis of fine-grained, temporally coherent facial\nmotion. Existing speech-driven approaches often produce oversimplified,\nemotionally flat, and semantically inconsistent results, which are insufficient\nfor singing animation. To address this, we propose Think2Sing, a\ndiffusion-based framework that leverages pretrained large language models to\ngenerate semantically coherent and temporally consistent 3D head animations,\nconditioned on both lyrics and acoustics. A key innovation is the introduction\nof motion subtitles, an auxiliary semantic representation derived through a\nnovel Singing Chain-of-Thought reasoning process combined with acoustic-guided\nretrieval. These subtitles contain precise timestamps and region-specific\nmotion descriptions, serving as interpretable motion priors. We frame the task\nas a motion intensity prediction problem, enabling finer control over facial\nregions and improving the modeling of expressive motion. To support this, we\ncreate a multimodal singing dataset with synchronized video, acoustic\ndescriptors, and motion subtitles, enabling diverse and expressive motion\nlearning. Extensive experiments show that Think2Sing outperforms\nstate-of-the-art methods in realism, expressiveness, and emotional fidelity,\nwhile also offering flexible, user-controllable animation editing."}
{"id": "2509.00285", "pdf": "https://arxiv.org/pdf/2509.00285", "abs": "https://arxiv.org/abs/2509.00285", "authors": ["Mir Tafseer Nayeem", "Davood Rafiei"], "title": "OpinioRAG: Towards Generating User-Centric Opinion Highlights from Large-scale Online Reviews", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "COLM 2025", "summary": "We study the problem of opinion highlights generation from large volumes of\nuser reviews, often exceeding thousands per entity, where existing methods\neither fail to scale or produce generic, one-size-fits-all summaries that\noverlook personalized needs. To tackle this, we introduce OpinioRAG, a\nscalable, training-free framework that combines RAG-based evidence retrieval\nwith LLMs to efficiently produce tailored summaries. Additionally, we propose\nnovel reference-free verification metrics designed for sentiment-rich domains,\nwhere accurately capturing opinions and sentiment alignment is essential. These\nmetrics offer a fine-grained, context-sensitive assessment of factual\nconsistency. To facilitate evaluation, we contribute the first large-scale\ndataset of long-form user reviews, comprising entities with over a thousand\nreviews each, paired with unbiased expert summaries and manually annotated\nqueries. Through extensive experiments, we identify key challenges, provide\nactionable insights into improving systems, pave the way for future research,\nand position OpinioRAG as a robust framework for generating accurate, relevant,\nand structured summaries at scale."}
{"id": "2509.02474", "pdf": "https://arxiv.org/pdf/2509.02474", "abs": "https://arxiv.org/abs/2509.02474", "authors": ["Nina Wiedemann", "Sainan Liu", "Quentin Leboutet", "Katelyn Gao", "Benjamin Ummenhofer", "Michael Paulitsch", "Kai Yuan"], "title": "Unifi3D: A Study on 3D Representations for Generation and Reconstruction in a Common Framework", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": null, "summary": "Following rapid advancements in text and image generation, research has\nincreasingly shifted towards 3D generation. Unlike the well-established\npixel-based representation in images, 3D representations remain diverse and\nfragmented, encompassing a wide variety of approaches such as voxel grids,\nneural radiance fields, signed distance functions, point clouds, or octrees,\neach offering distinct advantages and limitations. In this work, we present a\nunified evaluation framework designed to assess the performance of 3D\nrepresentations in reconstruction and generation. We compare these\nrepresentations based on multiple criteria: quality, computational efficiency,\nand generalization performance. Beyond standard model benchmarking, our\nexperiments aim to derive best practices over all steps involved in the 3D\ngeneration pipeline, including preprocessing, mesh reconstruction, compression\nwith autoencoders, and generation. Our findings highlight that reconstruction\nerrors significantly impact overall performance, underscoring the need to\nevaluate generation and reconstruction jointly. We provide insights that can\ninform the selection of suitable 3D models for various applications,\nfacilitating the development of more robust and application-specific solutions\nin 3D generation. The code for our framework is available at\nhttps://github.com/isl-org/unifi3d."}
{"id": "2509.00290", "pdf": "https://arxiv.org/pdf/2509.00290", "abs": "https://arxiv.org/abs/2509.00290", "authors": ["Taihei Sone"], "title": "Wage Sentiment Indices Derived from Survey Comments via Large Language Models", "categories": ["cs.CL"], "comment": "Submitted to IEEE Big Data 2025. 10 pages, 2 tables, 16 figures", "summary": "The emergence of generative Artificial Intelligence (AI) has created new\nopportunities for economic text analysis. This study proposes a Wage Sentiment\nIndex (WSI) constructed with Large Language Models (LLMs) to forecast wage\ndynamics in Japan. The analysis is based on the Economy Watchers Survey (EWS),\na monthly survey conducted by the Cabinet Office of Japan that captures\nreal-time economic assessments from workers in industries highly sensitive to\nbusiness conditions. The WSI extends the framework of the Price Sentiment Index\n(PSI) used in prior studies, adapting it specifically to wage related\nsentiment. To ensure scalability and adaptability, a data architecture is also\ndeveloped that enables integration of additional sources such as newspapers and\nsocial media. Experimental results demonstrate that WSI models based on LLMs\nsignificantly outperform both baseline approaches and pretrained models. These\nfindings highlight the potential of LLM-driven sentiment indices to enhance the\ntimeliness and effectiveness of economic policy design by governments and\ncentral banks."}
{"id": "2509.00066", "pdf": "https://arxiv.org/pdf/2509.00066", "abs": "https://arxiv.org/abs/2509.00066", "authors": ["Chuanxiang Yang", "Yuanfeng Zhou", "Guangshun Wei", "Siyu Ren", "Yuan Liu", "Junhui Hou", "Wenping Wang"], "title": "T-MLP: Tailed Multi-Layer Perceptron for Level-of-Detail Signal Representation", "categories": ["cs.LG", "cs.GR", "eess.IV"], "comment": null, "summary": "Level-of-detail (LoD) representation is critical for efficiently modeling and\ntransmitting various types of signals, such as images and 3D shapes. In this\nwork, we present a novel neural architecture that supports LoD signal\nrepresentation. Our architecture is based on an elaborate modification of the\nwidely used Multi-Layer Perceptron (MLP), which inherently operates at a single\nscale and therefore lacks native support for LoD. Specifically, we introduce\nthe Tailed Multi-Layer Perceptron (T-MLP) that extends the MLP by attaching\nmultiple output branches, also called tails, to its hidden layers, enabling\ndirect supervision at multiple depths. Our loss formulation and training\nstrategy allow each hidden layer to effectively learn a target signal at a\nspecific LoD, thus enabling multi-scale modeling. Extensive experimental\nresults show that our T-MLP outperforms other neural LoD baselines across a\nvariety of signal representation tasks."}
{"id": "2509.00309", "pdf": "https://arxiv.org/pdf/2509.00309", "abs": "https://arxiv.org/abs/2509.00309", "authors": ["Chen Zheng", "Yiyuan Ma", "Yuan Yang", "Deyi Liu", "Jing Liu", "Zuquan Song", "Yuxin Song", "Cheng Ren", "Hang Zhu", "Xin Liu", "Yiyuan Ma", "Siyuan Qiao", "Xun Zhou", "Liang Xiang", "Yonghui Wu"], "title": "Balanced Actor Initialization: Stable RLHF Training of Distillation-Based Reasoning Models", "categories": ["cs.CL"], "comment": null, "summary": "The development of alignment and reasoning capabilities in large language\nmodels has seen remarkable progress through two paradigms: instruction tuning\nand reinforcement learning from human feedback (RLHF) alignment paradigm, and\ndistillation-based reasoning fine-tuning paradigm. While both approaches prove\neffective independently, the third paradigm of applying RLHF to\ndistillation-trained models presents significant challenges. Our investigation\nreveals two critical phenomena that emerge in this paradigm: Sequence Length\nCollapse, where language generation dramatically reduces during early RLHF\ntraining, and the Reward Hockey Stick Curve, featuring severe reward score\ndrops followed by gradual recovery. These instabilities fundamentally\ncompromise the model's alignment and reasoning capabilities. To address these\nchallenges, we propose Balanced Actor Initialization (BAI), a two-stage\nweighted model merging approach. BAI first merges instruction-following and\ndistillation-based reasoning fine-tuned models, then further combines this\nintermediate model with the pretrained model to preserve foundational\nknowledge. Through comprehensive experiments across diverse benchmarks and\ndetailed analysis of training experiments, we demonstrate that BAI resolves\nSequence Length Collapse, mitigates the Reward Hockey Stick Curve, and enables\ncontinuous sequence length improvement during training. Additionally, our\nanalysis reveals that balanced merging ratios achieve optimal trade-offs\nbetween training stability and reasoning capability preservation. Our work\nprovides the effective solution for stable training in this third paradigm,\nenabling more capable reasoning models that combine distillation efficiency\nwith RLHF alignment."}
{"id": "2509.00114", "pdf": "https://arxiv.org/pdf/2509.00114", "abs": "https://arxiv.org/abs/2509.00114", "authors": ["Johan Malmstedt", "Giacomo Nanni", "Dario Rodighiero"], "title": "The Living Library of Trees: Mapping Knowledge Ecology in the Arnold Arboretum", "categories": ["cs.CY", "cs.GR"], "comment": null, "summary": "As biodiversity loss and climate change accelerate, botanical gardens serve\nas vital infrastructures for research, education, and conservation. This\nproject focuses on the Arnold Arboretum of Harvard University, a 281-acre\nliving museum founded in 1872 in Boston. Drawing on more than a century of\ncuratorial data, the research combines historical analysis with computational\nmethods to visualize the biographies of plants and people. The resulting\nplatform reveals patterns of care and scientific observations, along with the\ncollective dimensions embedded in botanical data. Using techniques from\nartificial intelligence, geospatial mapping, and information design, the\nproject frames the arboretum as a system of shared agency--an active archive of\nmore-than-human affinities that records the layered memory of curatorial labor,\nthe situated nature of knowledge production, and the potential of design to\nbridge archival record and future care."}
{"id": "2509.00325", "pdf": "https://arxiv.org/pdf/2509.00325", "abs": "https://arxiv.org/abs/2509.00325", "authors": ["Rinku Dewri"], "title": "GIER: Gap-Driven Self-Refinement for Large Language Models", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "We introduce GIER (Gap-driven Iterative Enhancement of Responses), a general\nframework for improving large language model (LLM) outputs through\nself-reflection and revision based on conceptual quality criteria. Unlike\nprompting strategies that rely on demonstrations, examples, or chain-of-thought\ntemplates, GIER utilizes natural language descriptions of reasoning gaps, and\nprompts a model to iteratively critique and refine its own outputs to better\nsatisfy these criteria. Across three reasoning-intensive tasks (SciFact,\nPrivacyQA, and e-SNLI) and four LLMs (GPT-4.1, GPT-4o Mini, Gemini 1.5 Pro, and\nLlama 3.3 70B), GIER improves rationale quality, grounding, and reasoning\nalignment without degrading task accuracy. Our analysis demonstrates that\nmodels can not only interpret abstract conceptual gaps but also translate them\ninto concrete reasoning improvements."}
{"id": "2509.00674", "pdf": "https://arxiv.org/pdf/2509.00674", "abs": "https://arxiv.org/abs/2509.00674", "authors": ["Lingkai Meng", "Long Yuan", "Xuemin Lin", "Wenjie Zhang", "Ying Zhang"], "title": "Triangle Counting in Hypergraph Streams: A Complete and Practical Approach", "categories": ["cs.DS", "cs.GR"], "comment": null, "summary": "Triangle counting in hypergraph streams, including both hyper-vertex and\nhyper-edge triangles, is a fundamental problem in hypergraph analytics, with\nbroad applications. However, existing methods face two key limitations: (i) an\nincomplete classification of hyper-vertex triangle structures, typically\nconsidering only inner or outer triangles; and (ii) inflexible sampling schemes\nthat predefine the number of sampled hyperedges, which is impractical under\nstrict memory constraints due to highly variable hyperedge sizes. To address\nthese challenges, we first introduce a complete classification of hyper-vertex\ntriangles, including inner, hybrid, and outer triangles. Based on this, we\ndevelop HTCount, a reservoir-based algorithm that dynamically adjusts the\nsample size based on the available memory M. To further improve memory\nutilization and reduce estimation error, we develop HTCount-P, a\npartition-based variant that adaptively partitions unused memory into\nindependent sample subsets. We provide theoretical analysis of the unbiasedness\nand variance bounds of the proposed algorithms. Case studies demonstrate the\nexpressiveness of our triangle structures in revealing meaningful interaction\npatterns. Extensive experiments on real-world hypergraphs show that both our\nalgorithms achieve highly accurate triangle count estimates under strict memory\nconstraints, with relative errors that are 1 to 2 orders of magnitude lower\nthan those of existing methods and consistently high throughput."}
{"id": "2509.00375", "pdf": "https://arxiv.org/pdf/2509.00375", "abs": "https://arxiv.org/abs/2509.00375", "authors": ["Ziyi Xia", "Kun Luo", "Hongjin Qian", "Zheng Liu"], "title": "Open Data Synthesis For Deep Research", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly expected to go beyond simple\nfactual queries toward Deep Research-tasks that require decomposing questions\ninto sub-problems, coordinating multi-step reasoning, and synthesizing evidence\nfrom diverse sources. We formalize Deep Research tasks with verifiable answers\nas Hierarchical Constraint Satisfaction Problems (HCSPs), which are\nfundamentally different from single-constraint, multi-hop, or flat CSP\nformulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA)\nfail to capture this complexity, while recent synthetic datasets often\nintroduce shortcut reasoning, knowledge leakage, or lack sufficient structural\ndepth. To address this gap, we introduce InfoSeek, a scalable framework for\nsynthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to\nrecursively build a Research Tree from large-scale webpages, blurring\nintermediate nodes into valid sub-problems, and converting these trees into\nnatural language questions that require traversing the full hierarchy. It also\nenables rapid scaling, yielding over 50K training examples, a curated test set,\nand reasoning trajectories generated via reject sampling. Experiments show that\nmodels trained on InfoSeek consistently outperform strong baselines. On a\nchallenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass\nmuch larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash),\nwhile achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro).\nBy preserving meta-information such as intermediate steps and retrieval labels,\nInfoSeek further supports advanced optimization strategies, including compound\nreward design and trajectory-level exploration. We provide our codes and\ndatasets in \\href{https://github.com/VectorSpaceLab/InfoSeek}{this repository}."}
{"id": "2509.00388", "pdf": "https://arxiv.org/pdf/2509.00388", "abs": "https://arxiv.org/abs/2509.00388", "authors": ["Xuelin Li", "Xiangqi Jin", "Linfeng Zhang"], "title": "GraphKV: Breaking the Static Selection Paradigm with Graph-Based KV Cache Eviction", "categories": ["cs.CL"], "comment": null, "summary": "Efficient Key-Value (KV) cache management is essential for processing long\ntext sequences in large language models (LLMs), where memory constraints often\nlimit performance. Conventional KV eviction strategies, such as top-k selection\nbased on attention scores, depend on static heuristics that fail to capture the\nevolving implicit dependencies among tokens during inference. To overcome this,\nwe propose GraphKV, a graph-based framework that redefines token selection for\nKV cache compression. In GraphKV, tokens are modeled as nodes with importance\nscores, and edges represent their similarity relationships. Through a\ndecay-signal-propagation mechanism, token importance is dynamically updated by\npropagating information across the graph, enabling adaptive retention of the\nmost contextually significant tokens. GraphKV can be seamlessly utilized in\nexisting KV cache eviction methods such as SnapKV and PyramidKV in a\nplug-and-play manner. Codes will be released on Github."}
{"id": "2509.00391", "pdf": "https://arxiv.org/pdf/2509.00391", "abs": "https://arxiv.org/abs/2509.00391", "authors": ["Yuting Tan", "Xuying Li", "Zhuo Li", "Huizhen Shu", "Peikang Hu"], "title": "The Resurgence of GCG Adversarial Attacks on Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": "12 pages, 5 figures", "summary": "Gradient-based adversarial prompting, such as the Greedy Coordinate Gradient\n(GCG) algorithm, has emerged as a powerful method for jailbreaking large\nlanguage models (LLMs). In this paper, we present a systematic appraisal of GCG\nand its annealing-augmented variant, T-GCG, across open-source LLMs of varying\nscales. Using Qwen2.5-0.5B, LLaMA-3.2-1B, and GPT-OSS-20B, we evaluate attack\neffectiveness on both safety-oriented prompts (AdvBench) and\nreasoning-intensive coding prompts. Our study reveals three key findings: (1)\nattack success rates (ASR) decrease with model size, reflecting the increasing\ncomplexity and non-convexity of larger models' loss landscapes; (2)\nprefix-based heuristics substantially overestimate attack effectiveness\ncompared to GPT-4o semantic judgments, which provide a stricter and more\nrealistic evaluation; and (3) coding-related prompts are significantly more\nvulnerable than adversarial safety prompts, suggesting that reasoning itself\ncan be exploited as an attack vector. In addition, preliminary results with\nT-GCG show that simulated annealing can diversify adversarial search and\nachieve competitive ASR under prefix evaluation, though its benefits under\nsemantic judgment remain limited. Together, these findings highlight the\nscalability limits of GCG, expose overlooked vulnerabilities in reasoning\ntasks, and motivate further development of annealing-inspired strategies for\nmore robust adversarial evaluation."}
{"id": "2509.00414", "pdf": "https://arxiv.org/pdf/2509.00414", "abs": "https://arxiv.org/abs/2509.00414", "authors": ["Juraj Vladika", "Florian Matthes"], "title": "MedSEBA: Synthesizing Evidence-Based Answers Grounded in Evolving Medical Literature", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted to CIKM 2025", "summary": "In the digital age, people often turn to the Internet in search of medical\nadvice and recommendations. With the increasing volume of online content, it\nhas become difficult to distinguish reliable sources from misleading\ninformation. Similarly, millions of medical studies are published every year,\nmaking it challenging for researchers to keep track of the latest scientific\nfindings. These evolving studies can reach differing conclusions, which is not\nreflected in traditional search tools. To address these challenges, we\nintroduce MedSEBA, an interactive AI-powered system for synthesizing\nevidence-based answers to medical questions. It utilizes the power of Large\nLanguage Models to generate coherent and expressive answers, but grounds them\nin trustworthy medical studies dynamically retrieved from the research database\nPubMed. The answers consist of key points and arguments, which can be traced\nback to respective studies. Notably, the platform also provides an overview of\nthe extent to which the most relevant studies support or refute the given\nmedical claim, and a visualization of how the research consensus evolved\nthrough time. Our user study revealed that medical experts and lay users find\nthe system usable and helpful, and the provided answers trustworthy and\ninformative. This makes the system well-suited for both everyday health\nquestions and advanced research insights."}
{"id": "2509.00425", "pdf": "https://arxiv.org/pdf/2509.00425", "abs": "https://arxiv.org/abs/2509.00425", "authors": ["Fenghua Liu", "Yulong Chen", "Yixuan Liu", "Zhujun Jin", "Solomon Tsai", "Ming Zhong"], "title": "The Gold Medals in an Empty Room: Diagnosing Metalinguistic Reasoning in LLMs with Camlang", "categories": ["cs.CL"], "comment": "Working in progress", "summary": "Large Language Models (LLMs) achieve gold-medal performance across many\nbenchmarks, yet it remains unclear whether such success reflects genuine\nreasoning or pattern matching. From a cognitive science perspective, an\ninformative test is whether models can master an unfamiliar language through\nexplicit metalinguistic deductive learning, a paradigm where human learners can\nreliably internalise grammatical systems through metalinguistic reasoning. We\naddress this question with Camlang, a novel constructed language that exhibits\nnaturalistic yet unattested feature combinations. Camlang consists of two\nexplicit resources, a grammar book and a bilingual dictionary, which mirror\nadult second-language learning via explicit grammar rules and lexical lookup,\nand enable us to disentangle errors in morpho-syntax, lexical semantics, and\nsentence-level reasoning. Human experiments show that these resources are\nsufficient for participants to acquire Camlang and successfully solve Camlang\ntasks. To operationalise evaluation, we adapt CommonsenseQA into Camlang,\ncreating Camlang-CSQA-v0, the first task in a broader suite where solving\nquestions requires applying grammar rules and lexical mappings. Experimental\nresults show that GPT-5 achieves 98\\% EM accuracy in English but only 47\\% in\nCamlang, far below human performance at 87\\%, while other state-of-the-art\nreasoning LLMs perform even worse. Human verification further reveals that most\nmodel successes stem from shallow lexical alignment while GPT-5 shows emerging\nmetalinguistic awareness to a limited extent but not systematic grammatical\nmastery as humans. Camlang establishes a cognitively grounded evaluation\nparadigm that exposes fundamental gaps between current models and human\nmetalinguistic competence."}
{"id": "2509.00449", "pdf": "https://arxiv.org/pdf/2509.00449", "abs": "https://arxiv.org/abs/2509.00449", "authors": ["Xuecheng Zou", "Ke Liu", "Bingbing Wang", "Huafei Deng", "Li Zhang", "Yu Tang"], "title": "GOSU: Retrieval-Augmented Generation with Global-Level Optimized Semantic Unit-Centric Framework", "categories": ["cs.CL"], "comment": null, "summary": "Building upon the standard graph-based Retrieval-Augmented Generation (RAG),\nthe introduction of heterogeneous graphs and hypergraphs aims to enrich\nretrieval and generation by leveraging the relationships between multiple\nentities through the concept of semantic units (SUs). But this also raises a\nkey issue: The extraction of high-level SUs limited to local text chunks is\nprone to ambiguity, complex coupling, and increased retrieval overhead due to\nthe lack of global knowledge or the neglect of fine-grained relationships. To\naddress these issues, we propose GOSU, a semantic unit-centric RAG framework\nthat efficiently performs global disambiguation and utilizes SUs to capture\ninterconnections between different nodes across the global context. In the\ngraph construction phase, GOSU performs global merging on the pre-extracted SUs\nfrom local text chunks and guides entity and relationship extraction, reducing\nthe difficulty of coreference resolution while uncovering global semantic\nobjects across text chunks. In the retrieval and generation phase, we introduce\nhierarchical keyword extraction and semantic unit completion. The former\nuncovers the fine-grained binary relationships overlooked by the latter, while\nthe latter compensates for the coarse-grained n-ary relationships missing from\nthe former. Evaluation across multiple tasks demonstrates that GOSU outperforms\nthe baseline RAG methods in terms of generation quality."}
{"id": "2509.00457", "pdf": "https://arxiv.org/pdf/2509.00457", "abs": "https://arxiv.org/abs/2509.00457", "authors": ["Salah Eddine Bekhouche", "Abdellah Zakaria Sellam", "Hichem Telli", "Cosimo Distante", "Abdenour Hadid"], "title": "CVPD at QIAS 2025 Shared Task: An Efficient Encoder-Based Approach for Islamic Inheritance Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Islamic inheritance law (Ilm al-Mawarith) requires precise identification of\nheirs and calculation of shares, which poses a challenge for AI. In this paper,\nwe present a lightweight framework for solving multiple-choice inheritance\nquestions using a specialised Arabic text encoder and Attentive Relevance\nScoring (ARS). The system ranks answer options according to semantic relevance,\nand enables fast, on-device inference without generative reasoning. We evaluate\nArabic encoders (MARBERT, ArabicBERT, AraBERT) and compare them with API-based\nLLMs (Gemini, DeepSeek) on the QIAS 2025 dataset. While large models achieve an\naccuracy of up to 87.6%, they require more resources and are context-dependent.\nOur MARBERT-based approach achieves 69.87% accuracy, presenting a compelling\ncase for efficiency, on-device deployability, and privacy. While this is lower\nthan the 87.6% achieved by the best-performing LLM, our work quantifies a\ncritical trade-off between the peak performance of large models and the\npractical advantages of smaller, specialized systems in high-stakes domains."}
{"id": "2509.00461", "pdf": "https://arxiv.org/pdf/2509.00461", "abs": "https://arxiv.org/abs/2509.00461", "authors": ["Beining Xu"], "title": "TECP: Token-Entropy Conformal Prediction for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Uncertainty quantification (UQ) for open-ended language generation remains a\ncritical yet underexplored challenge, especially under black-box constraints\nwhere internal model signals are inaccessible. In this paper, we introduce\nToken-Entropy Conformal Prediction (TECP), a novel framework that leverages\ntoken-level entropy as a logit-free, reference-free uncertainty measure and\nintegrates it into a split conformal prediction (CP) pipeline to construct\nprediction sets with formal coverage guarantees. Unlike existing approaches\nthat rely on semantic consistency heuristics or white-box features, TECP\ndirectly estimates epistemic uncertainty from the token entropy structure of\nsampled generations and calibrates uncertainty thresholds via CP quantiles to\nensure provable error control. Empirical evaluations across six large language\nmodels and two benchmarks (CoQA and TriviaQA) demonstrate that TECP\nconsistently achieves reliable coverage and compact prediction sets,\noutperforming prior self-consistency-based UQ methods. Our method provides a\nprincipled and efficient solution for trustworthy generation in black-box LLM\nsettings."}
{"id": "2509.00482", "pdf": "https://arxiv.org/pdf/2509.00482", "abs": "https://arxiv.org/abs/2509.00482", "authors": ["Saksorn Ruangtanusak", "Pittawat Taveekitworachai", "Kunat Pipatanakul"], "title": "Talk Less, Call Right: Enhancing Role-Play LLM Agents with Automatic Prompt Optimization and Role Prompting", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "17 pages, 2 figures", "summary": "This report investigates approaches for prompting a tool-augmented large\nlanguage model (LLM) to act as a role-playing dialogue agent in the API track\nof the Commonsense Persona-grounded Dialogue Challenge (CPDC) 2025. In this\nsetting, dialogue agents often produce overly long in-character responses\n(over-speaking) while failing to use tools effectively according to the persona\n(under-acting), such as generating function calls that do not exist or making\nunnecessary tool calls before answering. We explore four prompting approaches\nto address these issues: 1) basic role prompting, 2) human-crafted role\nprompting, 3) automatic prompt optimization (APO), and 4) rule-based role\nprompting. The rule-based role prompting (RRP) approach achieved the best\nperformance through two novel techniques--character-card/scene-contract design\nand strict enforcement of function calling--which led to an overall score of\n0.571, improving on the zero-shot baseline score of 0.519. These findings\ndemonstrate that RRP design can substantially improve the effectiveness and\nreliability of role-playing dialogue agents compared with more elaborate\nmethods such as APO. To support future efforts in developing persona prompts,\nwe are open-sourcing all of our best-performing prompts and the APO tool.\nSource code is available at https://github.com/scb-10x/apo."}
{"id": "2509.00496", "pdf": "https://arxiv.org/pdf/2509.00496", "abs": "https://arxiv.org/abs/2509.00496", "authors": ["Li S. Yifei", "Allen Chang", "Chaitanya Malaviya", "Mark Yatskar"], "title": "ResearchQA: Evaluating Scholarly Question Answering at Scale Across 75 Fields with Survey-Mined Questions and Rubrics", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages main, 40 pages total, 16 figures", "summary": "Evaluating long-form responses to research queries heavily relies on expert\nannotators, restricting attention to areas like AI where researchers can\nconveniently enlist colleagues. Yet, research expertise is widespread: survey\narticles synthesize knowledge distributed across the literature. We introduce\nResearchQA, a resource for evaluating LLM systems by distilling survey articles\nfrom 75 research fields into 21K queries and 160K rubric items. Each rubric,\nderived jointly with queries from survey sections, lists query-specific answer\nevaluation criteria, i.e., citing papers, making explanations, and describing\nlimitations. Assessments by 31 Ph.D. annotators in 8 fields indicate 96% of\nqueries support Ph.D. information needs and 87% of rubric items should be\naddressed in system responses by a sentence or more. Using our rubrics, we are\nable to construct an automatic pairwise judge obtaining 74% agreement with\nexpert judgments. We leverage ResearchQA to analyze competency gaps in 18\nsystems in over 7.6K pairwise evaluations. No parametric or retrieval-augmented\nsystem we evaluate exceeds 70% on covering rubric items, and the\nhighest-ranking agentic system shows 75% coverage. Error analysis reveals that\nthe highest-ranking system fully addresses less than 11% of citation rubric\nitems, 48% of limitation items, and 49% of comparison items. We release our\ndata to facilitate more comprehensive multi-field evaluations."}
{"id": "2509.00503", "pdf": "https://arxiv.org/pdf/2509.00503", "abs": "https://arxiv.org/abs/2509.00503", "authors": ["Jialong Zuo", "Guangyan Zhang", "Minghui Fang", "Shengpeng Ji", "Xiaoqi Jiao", "Jingyu Li", "Yiwen Guo", "Zhou Zhao"], "title": "Entropy-based Coarse and Compressed Semantic Speech Representation Learning", "categories": ["cs.CL", "eess.AS"], "comment": null, "summary": "Discrete speech representation learning has recently attracted increasing\ninterest in both acoustic and semantic modeling. Existing approaches typically\nencode 16 kHz waveforms into discrete tokens at a rate of 25 or 50 tokens per\nsecond. However, given that speech generally conveys only 2 to 5 words per\nsecond, such fine-grained tokenization introduces redundancy and hinders\nefficiency in downstream training and inference. Moreover, semantic speech\nrepresentations at this frequency primarily capture phonetic-level information,\nwhile semantic understanding may not require such detailed token-level\nresolution. To address these limitations, we propose an entropy-based dynamic\naggregation framework for learning compressed semantic speech representations.\nA speech language model is first pre-trained via next-token prediction on\nlarge-scale unlabeled data to capture frequent token patterns. Predictive\nentropy is then used to adaptively determine aggregation boundaries, followed\nby a cross-attention module that fuses information within each segment. By\nadjusting the entropy threshold, the granularity and compression ratio of the\nrepresentations can be flexibly controlled. Experiments on ASR, speech-to-text\ntranslation, and voice conversion tasks demonstrate that the compressed\nrepresentations perform on par with or better than dense token sequences,\ndemonstrating the effectiveness of the proposed approach."}
{"id": "2509.00529", "pdf": "https://arxiv.org/pdf/2509.00529", "abs": "https://arxiv.org/abs/2509.00529", "authors": ["Eunjung Cho", "Alexander Hoyle", "Yoan Hermstrüwer"], "title": "Modeling Motivated Reasoning in Law: Evaluating Strategic Role Conditioning in LLM Summarization", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly used to generate user-tailored\nsummaries, adapting outputs to specific stakeholders. In legal contexts, this\nraises important questions about motivated reasoning -- how models\nstrategically frame information to align with a stakeholder's position within\nthe legal system. Building on theories of legal realism and recent trends in\nlegal practice, we investigate how LLMs respond to prompts conditioned on\ndifferent legal roles (e.g., judges, prosecutors, attorneys) when summarizing\njudicial decisions. We introduce an evaluation framework grounded in legal fact\nand reasoning inclusion, also considering favorability towards stakeholders.\nOur results show that even when prompts include balancing instructions, models\nexhibit selective inclusion patterns that reflect role-consistent perspectives.\nThese findings raise broader concerns about how similar alignment may emerge as\nLLMs begin to infer user roles from prior interactions or context, even without\nexplicit role instructions. Our results underscore the need for role-aware\nevaluation of LLM summarization behavior in high-stakes legal settings."}
{"id": "2509.00544", "pdf": "https://arxiv.org/pdf/2509.00544", "abs": "https://arxiv.org/abs/2509.00544", "authors": ["Hanqi Yan", "Hainiu Xu", "Yulan He"], "title": "Thinking Hard, Going Misaligned: Emergent Misalignment in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "With Large Language Models (LLMs) becoming increasingly widely adopted,\nconcerns regarding their safety and alignment with human values have\nintensified. Previous studies have shown that fine-tuning LLMs on narrow and\nmalicious datasets induce misaligned behaviors. In this work, we report a more\nconcerning phenomenon, Reasoning-Induced Misalignment. Specifically, we observe\nthat LLMs become more responsive to malicious requests when reasoning is\nstrengthened, via switching to \"think-mode\" or fine-tuning on benign math\ndatasets, with dense models particularly vulnerable. Moreover, we analyze\ninternal model states and find that both attention shifts and specialized\nexperts in mixture-of-experts models help redirect excessive reasoning towards\nsafety guardrails. These findings provide new insights into the emerging\nreasoning-safety trade-off and underscore the urgency of advancing alignment\nfor advanced reasoning models."}
{"id": "2509.00591", "pdf": "https://arxiv.org/pdf/2509.00591", "abs": "https://arxiv.org/abs/2509.00591", "authors": ["Lang Xiong", "Nishant Bhargava", "Wesley Chang", "Jianhang Hong", "Haihao Liu", "Kevin Zhu"], "title": "StealthEval: A Probe-Rewrite-Evaluate Workflow for Reliable Benchmarks", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often exhibit significant behavioral shifts when\nthey perceive a change from a real-world deployment context to a controlled\nevaluation setting, a phenomenon known as \"evaluation awareness.\" This\ndiscrepancy poses a critical challenge for AI alignment, as benchmark\nperformance may not accurately reflect a model's true safety and honesty. In\nthis work, we systematically quantify these behavioral changes by manipulating\nthe perceived context of prompts. We introduce a methodology that uses a linear\nprobe to score prompts on a continuous scale from \"test-like\" to \"deploy-like\"\nand leverage an LLM rewriting strategy to shift these prompts towards a more\nnatural, deployment-style context while preserving the original task. Using\nthis method, we achieved a 30% increase in the average probe score across a\nstrategic role-playing dataset after rewriting. Evaluating a suite of\nstate-of-the-art models on these original and rewritten prompts, we find that\nrewritten \"deploy-like\" prompts induce a significant and consistent shift in\nbehavior. Across all models, we observed an average increase in honest\nresponses of 5.26% and a corresponding average decrease in deceptive responses\nof 12.40%. Furthermore, refusal rates increased by an average of 6.38%,\nindicating heightened safety compliance. Our findings demonstrate that\nevaluation awareness is a quantifiable and manipulable factor that directly\ninfluences LLM behavior, revealing that models are more prone to unsafe or\ndeceptive outputs in perceived test environments. This underscores the urgent\nneed for more realistic evaluation frameworks to accurately gauge true model\nalignment before deployment."}
{"id": "2509.00605", "pdf": "https://arxiv.org/pdf/2509.00605", "abs": "https://arxiv.org/abs/2509.00605", "authors": ["Rishiraj Acharya"], "title": "Gated Associative Memory: A Parallel O(N) Architecture for Efficient Sequence Modeling", "categories": ["cs.CL", "cs.LG"], "comment": "11 pages, 4 figures, 3 tables", "summary": "The Transformer architecture, underpinned by the self-attention mechanism,\nhas become the de facto standard for sequence modeling tasks. However, its core\ncomputational primitive scales quadratically with sequence length (O(N^2)),\ncreating a significant bottleneck for processing long contexts. In this paper,\nwe propose the Gated Associative Memory (GAM) network, a novel, fully parallel\narchitecture for sequence modeling that exhibits linear complexity (O(N)) with\nrespect to sequence length. The GAM block replaces the self-attention layer\nwith two parallel pathways: a causal convolution to efficiently capture local,\nposition-dependent context, and a parallel associative memory retrieval\nmechanism to model global, content-based patterns. These pathways are\ndynamically fused using a gating mechanism, allowing the model to flexibly\ncombine local and global information for each token. We implement GAM from\nscratch and conduct a rigorous comparative analysis against a standard\nTransformer model and a modern linear-time baseline (Mamba) on the WikiText-2\nbenchmark, as well as against the Transformer on the TinyStories dataset. Our\nexperiments demonstrate that GAM is consistently faster, outperforming both\nbaselines on training speed, and achieves a superior or competitive final\nvalidation perplexity across all datasets, establishing it as a promising and\nefficient alternative for sequence modeling."}
{"id": "2509.00623", "pdf": "https://arxiv.org/pdf/2509.00623", "abs": "https://arxiv.org/abs/2509.00623", "authors": ["Ali Zain", "Sareem Farooqui", "Muhammad Rafi"], "title": "A Multi-Strategy Approach for AI-Generated Text Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper presents presents three distinct systems developed for the M-DAIGT\nshared task on detecting AI generated content in news articles and academic\nabstracts. The systems includes: (1) A fine-tuned RoBERTa-base classifier, (2)\nA classical TF-IDF + Support Vector Machine (SVM) classifier , and (3) An\nInnovative ensemble model named Candace, leveraging probabilistic features\nextracted from multiple Llama-3.2 models processed by a customTransformer\nencoder.The RoBERTa-based system emerged as the most performant, achieving\nnear-perfect results on both development and test sets."}
{"id": "2509.00629", "pdf": "https://arxiv.org/pdf/2509.00629", "abs": "https://arxiv.org/abs/2509.00629", "authors": ["Md Tanzib Hosain", "Md Kishor Morol"], "title": "Can Multi-turn Self-refined Single Agent LMs with Retrieval Solve Hard Coding Problems?", "categories": ["cs.CL"], "comment": "Accepted in Proceedings of the 63rd Annual Meeting of the Association\n  for Computational Linguistics (Student Research Workshop), 2025", "summary": "Among the hardest tasks for humans are those found in competitive programming\nwhere problems require sophisticated algorithmic thinking, puzzle solving, and\nthe creation of effective code. As a domain to assess language models (LMs), it\nhas not received enough attention, though. This study presents the ICPC\nbenchmark, which consists of 254 international collegiate programming contest\n(ICPC) tasks. Each problem includes official analysis, reference code, and\nsample, high-quality unit, and hidden tests. We are able to develop and\nevaluate a variety of LM inference techniques for competitive programming with\nthese resources. With zero-shot chain-of-thought prompting, we find that o1\nonly achieves a 19.1\\% pass@1 solve rate. With our best inference technique,\nwhich combines multi-turn self-judge with reflection and retrieval over\nepisodic information, raises this to 42.2\\%. Furthermore, we conduct a new\nhuman-in-the-loop investigation to gain a deeper understanding of the remaining\ndifficulties. Surprisingly, we discover that o1 can solve 17 out of 18 problems\nthat were previously unsolvable by any model or technique with just a few\nspecific instructions. A footstep toward LMs with grounded, imaginative, and\nalgorithmic thinking is provided by our quantitative findings and qualitative\nresearch. We open-source our code and data at https://github.com/kraritt/zolve."}
{"id": "2509.00673", "pdf": "https://arxiv.org/pdf/2509.00673", "abs": "https://arxiv.org/abs/2509.00673", "authors": ["Sanjeeevan Selvaganapathy", "Mehwish Nasim"], "title": "Confident, Calibrated, or Complicit: Probing the Trade-offs between Safety Alignment and Ideological Bias in Language Models in Detecting Hate Speech", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7; I.6"], "comment": null, "summary": "We investigate the efficacy of Large Language Models (LLMs) in detecting\nimplicit and explicit hate speech, examining whether models with minimal safety\nalignment (uncensored) might provide more objective classification capabilities\ncompared to their heavily-aligned (censored) counterparts. While uncensored\nmodels theoretically offer a less constrained perspective free from moral\nguardrails that could bias classification decisions, our results reveal a\nsurprising trade-off: censored models significantly outperform their uncensored\ncounterparts in both accuracy and robustness, achieving 78.7% versus 64.1%\nstrict accuracy. However, this enhanced performance comes with its own\nlimitation -- the safety alignment acts as a strong ideological anchor, making\ncensored models resistant to persona-based influence, while uncensored models\nprove highly malleable to ideological framing. Furthermore, we identify\ncritical failures across all models in understanding nuanced language such as\nirony. We also find alarming fairness disparities in performance across\ndifferent targeted groups and systemic overconfidence that renders\nself-reported certainty unreliable. These findings challenge the notion of LLMs\nas objective arbiters and highlight the need for more sophisticated auditing\nframeworks that account for fairness, calibration, and ideological consistency."}
{"id": "2509.00679", "pdf": "https://arxiv.org/pdf/2509.00679", "abs": "https://arxiv.org/abs/2509.00679", "authors": ["Junfeng Ran", "Guangxiang Zhao", "Yuhan Wu", "Dawei Zhu", "Longyun Wu", "Yikai Zhao", "Tong Yang", "Lin Sun", "Xiangzheng Zhang", "Sujian Li"], "title": "Router Upcycling: Leveraging Mixture-of-Routers in Mixture-of-Experts Upcycling", "categories": ["cs.CL"], "comment": null, "summary": "The Mixture-of-Experts (MoE) models have gained significant attention in deep\nlearning due to their dynamic resource allocation and superior performance\nacross diverse tasks. However, efficiently training these models remains\nchallenging. The MoE upcycling technique has been proposed to reuse and improve\nexisting model components, thereby minimizing training overhead. Despite this,\nsimple routers, such as linear routers, often struggle with complex routing\ntasks within MoE upcycling. In response, we propose a novel routing technique\ncalled Router Upcycling to enhance the performance of MoE upcycling models. Our\napproach initializes multiple routers from the attention heads of preceding\nattention layers during upcycling. These routers collaboratively assign tokens\nto specialized experts in an attention-like manner. Each token is processed\ninto diverse queries and aligned with the experts' features (serving as keys).\nExperimental results demonstrate that our method achieves state-of-the-art\n(SOTA) performance, outperforming other upcycling baselines."}
{"id": "2509.00680", "pdf": "https://arxiv.org/pdf/2509.00680", "abs": "https://arxiv.org/abs/2509.00680", "authors": ["Austin McCutcheon", "Chris Brogly"], "title": "Do small language models generate realistic variable-quality fake news headlines?", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Small language models (SLMs) have the capability for text generation and may\npotentially be used to generate falsified texts online. This study evaluates 14\nSLMs (1.7B-14B parameters) including LLaMA, Gemma, Phi, SmolLM, Mistral, and\nGranite families in generating perceived low and high quality fake news\nheadlines when explicitly prompted, and whether they appear to be similar to\nreal-world news headlines. Using controlled prompt engineering, 24,000\nheadlines were generated across low-quality and high-quality deceptive\ncategories. Existing machine learning and deep learning-based news headline\nquality detectors were then applied against these SLM-generated fake news\nheadlines. SLMs demonstrated high compliance rates with minimal ethical\nresistance, though there were some occasional exceptions. Headline quality\ndetection using established DistilBERT and bagging classifier models showed\nthat quality misclassification was common, with detection accuracies only\nranging from 35.2% to 63.5%. These findings suggest the following: tested SLMs\ngenerally are compliant in generating falsified headlines, although there are\nslight variations in ethical restraints, and the generated headlines did not\nclosely resemble existing primarily human-written content on the web, given the\nlow quality classification accuracy."}
{"id": "2509.00687", "pdf": "https://arxiv.org/pdf/2509.00687", "abs": "https://arxiv.org/abs/2509.00687", "authors": ["Chen Su", "Yuanhe Tian", "Yan Song", "Yongdong Zhang"], "title": "Text Reinforcement for Multimodal Time Series Forecasting", "categories": ["cs.CL"], "comment": null, "summary": "Recent studies in time series forecasting (TSF) use multimodal inputs, such\nas text and historical time series data, to predict future values. These\nstudies mainly focus on developing advanced techniques to integrate textual\ninformation with time series data to perform the task and achieve promising\nresults. Meanwhile, these approaches rely on high-quality text and time series\ninputs, whereas in some cases, the text does not accurately or fully capture\nthe information carried by the historical time series, which leads to unstable\nperformance in multimodal TSF. Therefore, it is necessary to enhance the\ntextual content to improve the performance of multimodal TSF. In this paper, we\npropose improving multimodal TSF by reinforcing the text modalities. We propose\na text reinforcement model (TeR) to generate reinforced text that addresses\npotential weaknesses in the original text, then apply this reinforced text to\nsupport the multimodal TSF model's understanding of the time series, improving\nTSF performance. To guide the TeR toward producing higher-quality reinforced\ntext, we design a reinforcement learning approach that assigns rewards based on\nthe impact of each reinforced text on the performance of the multimodal TSF\nmodel and its relevance to the TSF task. We optimize the TeR accordingly, so as\nto improve the quality of the generated reinforced text and enhance TSF\nperformance. Extensive experiments on a real-world benchmark dataset covering\nvarious domains demonstrate the effectiveness of our approach, which\noutperforms strong baselines and existing studies on the dataset."}
{"id": "2509.00691", "pdf": "https://arxiv.org/pdf/2509.00691", "abs": "https://arxiv.org/abs/2509.00691", "authors": ["Alex Gulko", "Yusen Peng", "Sachin Kumar"], "title": "CE-Bench: Towards a Reliable Contrastive Evaluation Benchmark of Interpretability of Sparse Autoencoders", "categories": ["cs.CL"], "comment": null, "summary": "Probing with sparse autoencoders is a promising approach for uncovering\ninterpretable features in large language models (LLMs). However, the lack of\nautomated evaluation methods has hindered their broader adoption and\ndevelopment. In this work, we introduce CE-Bench, a novel and lightweight\ncontrastive evaluation benchmark for sparse autoencoders, built on a curated\ndataset of contrastive story pairs. We conduct comprehensive ablation studies\nto validate the effectiveness of our approach. Our results show that CE-Bench\nreliably measures the interpretability of sparse autoencoders and aligns well\nwith existing benchmarks, all without requiring an external LLM. The official\nimplementation and evaluation dataset are open-sourced under the MIT License."}
{"id": "2509.00698", "pdf": "https://arxiv.org/pdf/2509.00698", "abs": "https://arxiv.org/abs/2509.00698", "authors": ["Kaiwen Wei", "Jinpeng Gao", "Jiang Zhong", "Yuming Yang", "Fengmao Lv", "Zhenyang Li"], "title": "Learning to Shop Like Humans: A Review-driven Retrieval-Augmented Recommendation Framework with LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have shown strong potential in recommendation\ntasks due to their strengths in language understanding, reasoning and knowledge\nintegration. These capabilities are especially beneficial for review-based\nrecommendation, which relies on semantically rich user-generated texts to\nreveal fine-grained user preferences and item attributes. However, effectively\nincorporating reviews into LLM-based recommendation remains challenging due to\n(1) inefficient to dynamically utilize user reviews under LLMs' constrained\ncontext windows, and (2) lacking effective mechanisms to prioritize reviews\nmost relevant to the user's current decision context. To address these\nchallenges, we propose RevBrowse, a review-driven recommendation framework\ninspired by the \"browse-then-decide\" decision process commonly observed in\nonline user behavior. RevBrowse integrates user reviews into the LLM-based\nreranking process to enhance its ability to distinguish between candidate\nitems. To improve the relevance and efficiency of review usage, we introduce\nPrefRAG, a retrieval-augmented module that disentangles user and item\nrepresentations into structured forms and adaptively retrieves\npreference-relevant content conditioned on the target item. Extensive\nexperiments on four Amazon review datasets demonstrate that RevBrowse achieves\nconsistent and significant improvements over strong baselines, highlighting its\ngeneralizability and effectiveness in modeling dynamic user preferences.\nFurthermore, since the retrieval-augmented process is transparent, RevBrowse\noffers a certain level of interpretability by making visible which reviews\ninfluence the final recommendation."}
{"id": "2509.00707", "pdf": "https://arxiv.org/pdf/2509.00707", "abs": "https://arxiv.org/abs/2509.00707", "authors": ["Daehoon Gwak", "Minseo Jung", "Junwoo Park", "Minho Park", "ChaeHun Park", "Junha Hyung", "Jaegul Choo"], "title": "Reward-Weighted Sampling: Enhancing Non-Autoregressive Characteristics in Masked Diffusion LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Main Paper (Long)", "summary": "Masked diffusion models (MDMs) offer a promising non-autoregressive\nalternative for large language modeling. Standard decoding methods for MDMs,\nsuch as confidence-based sampling, select tokens independently based on\nindividual token confidences at each diffusion step. However, we observe that\nthis independent token selection often results in generation orders resembling\nsequential autoregressive processes, limiting the advantages of\nnon-autoregressive modeling. To mitigate this pheonomenon, we propose\nReward-Weighted Sampling (RWS), a novel decoding strategy that leverages an\nexternal reward model to provide a principled global signal during the\niterative diffusion process. Specifically, at each diffusion step, RWS\nevaluates the quality of the entire intermediate sequence and scales token\nlogits accordingly, guiding token selection by integrating global\nsequence-level coherence. This method selectively increases the confidence of\ntokens that initially have lower scores, thereby promoting a more\nnon-autoregressive generation order. Furthermore, we provide theoretical\njustification showing that reward-weighted logit scaling induces beneficial\nrank reversals in token selection and consistently improves expected reward.\nExperiments demonstrate that RWS significantly promotes non-autoregressive\ngeneration orders, leading to improvements across multiple evaluation metrics.\nThese results highlight the effectiveness of integrating global signals in\nenhancing both the non-autoregressive properties and overall performance of\nMDMs."}
{"id": "2509.00709", "pdf": "https://arxiv.org/pdf/2509.00709", "abs": "https://arxiv.org/abs/2509.00709", "authors": ["Elias Ra", "Seung Je Kim", "Eui-Yeong Seo", "Geunju So"], "title": "Designing LMS and Instructional Strategies for Integrating Generative-Conversational AI", "categories": ["cs.CL"], "comment": null, "summary": "Higher education faces growing challenges in delivering personalized,\nscalable, and pedagogically coherent learning experiences. This study\nintroduces a structured framework for designing an AI-powered Learning\nManagement System (AI-LMS) that integrates generative and conversational AI to\nsupport adaptive, interactive, and learner-centered instruction. Using a\ndesign-based research (DBR) methodology, the framework unfolds through five\nphases: literature review, SWOT analysis, development of ethical-pedagogical\nprinciples, system design, and instructional strategy formulation. The\nresulting AI-LMS features modular components -- including configurable prompts,\nadaptive feedback loops, and multi-agent conversation flows -- aligned with\npedagogical paradigms such as behaviorist, constructivist, and connectivist\nlearning theories. By combining AI capabilities with human-centered design and\nethical safeguards, this study advances a practical model for AI integration in\neducation. Future research will validate and refine the system through\nreal-world implementation."}
{"id": "2509.00731", "pdf": "https://arxiv.org/pdf/2509.00731", "abs": "https://arxiv.org/abs/2509.00731", "authors": ["Houji Jin", "Negin Ashrafi", "Armin Abdollahi", "Wei Liu", "Jian Wang", "Ganyu Gui", "Maryam Pishgar", "Huanghao Feng"], "title": "LLM Encoder vs. Decoder: Robust Detection of Chinese AI-Generated Text with LoRA", "categories": ["cs.CL"], "comment": null, "summary": "The rapid growth of large language models (LLMs) has heightened the demand\nfor accurate detection of AI-generated text, particularly in languages like\nChinese, where subtle linguistic nuances pose significant challenges to current\nmethods. In this study, we conduct a systematic comparison of encoder-based\nTransformers (Chinese BERT-large and RoBERTa-wwm-ext-large), a decoder-only LLM\n(Alibaba's Qwen2.5-7B/DeepSeek-R1-Distill-Qwen-7B fine-tuned via Low-Rank\nAdaptation, LoRA), and a FastText baseline using the publicly available dataset\nfrom the NLPCC 2025 Chinese AI-Generated Text Detection Task. Encoder models\nwere fine-tuned using a novel prompt-based masked language modeling approach,\nwhile Qwen2.5-7B was adapted for classification with an instruction-format\ninput and a lightweight classification head trained via LoRA. Experiments\nreveal that although encoder models nearly memorize training data, they suffer\nsignificant performance degradation under distribution shifts (RoBERTa: 76.3%\ntest accuracy; BERT: 79.3%). FastText demonstrates surprising lexical\nrobustness (83.5% accuracy) yet lacks deeper semantic understanding. In\ncontrast, the LoRA-adapted Qwen2.5-7B achieves 95.94% test accuracy with\nbalanced precision-recall metrics, indicating superior generalization and\nresilience to dataset-specific artifacts. These findings underscore the\nefficacy of decoder-based LLMs with parameter-efficient fine-tuning for robust\nChinese AI-generated text detection. Future work will explore next-generation\nQwen3 models, distilled variants, and ensemble strategies to enhance\ncross-domain robustness further."}
{"id": "2509.00765", "pdf": "https://arxiv.org/pdf/2509.00765", "abs": "https://arxiv.org/abs/2509.00765", "authors": ["Zhichao Yan", "Jiaoyan Chen", "Jiapu Wang", "Xiaoli Li", "Ru Li", "Jeff Z. Pan"], "title": "Decomposing and Revising What Language Models Generate", "categories": ["cs.CL"], "comment": null, "summary": "Attribution is crucial in question answering (QA) with Large Language Models\n(LLMs).SOTA question decomposition-based approaches use long form answers to\ngenerate questions for retrieving related documents. However, the generated\nquestions are often irrelevant and incomplete, resulting in a loss of facts in\nretrieval.These approaches also fail to aggregate evidence snippets from\ndifferent documents and paragraphs. To tackle these problems, we propose a new\nfact decomposition-based framework called FIDES (\\textit{faithful context\nenhanced fact decomposition and evidence aggregation}) for attributed QA. FIDES\nuses a contextually enhanced two-stage faithful decomposition method to\ndecompose long form answers into sub-facts, which are then used by a retriever\nto retrieve related evidence snippets. If the retrieved evidence snippets\nconflict with the related sub-facts, such sub-facts will be revised\naccordingly. Finally, the evidence snippets are aggregated according to the\noriginal sentences.Extensive evaluation has been conducted with six datasets,\nwith an additionally proposed new metric called $Attr_{auto-P}$ for evaluating\nthe evidence precision. FIDES outperforms the SOTA methods by over 14\\% in\naverage with GPT-3.5-turbo, Gemini and Llama 70B series."}
{"id": "2509.00783", "pdf": "https://arxiv.org/pdf/2509.00783", "abs": "https://arxiv.org/abs/2509.00783", "authors": ["Weizhe Shi", "Qiqi Wang", "Yihong Pan", "Qian Liu", "Kaiqi Zhao"], "title": "LegalChainReasoner: A Legal Chain-guided Framework for Criminal Judicial Opinion Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A criminal judicial opinion represents the judge's disposition of a case,\nincluding the decision rationale and sentencing. Automatically generating such\nopinions can assist in analyzing sentencing consistency and provide judges with\nreferences to similar past cases. However, current research typically\napproaches this task by dividing it into two isolated subtasks: legal reasoning\nand sentencing prediction. This separation often leads to inconsistency between\nthe reasoning and predictions, failing to meet real-world judicial\nrequirements. Furthermore, prior studies rely on manually curated knowledge to\nenhance applicability, yet such methods remain limited in practical deployment.\nTo address these limitations and better align with legal practice, we propose a\nnew LegalAI task: Judicial Opinion Generation, which simultaneously produces\nboth legal reasoning and sentencing decisions. To achieve this, we introduce\nLegalChainReasoner, a framework that applies structured legal chains to guide\nthe model through comprehensive case assessments. By integrating factual\npremises, composite legal conditions, and sentencing conclusions, our approach\nensures flexible knowledge injection and end-to-end opinion generation.\nExperiments on two real-world and open-source Chinese legal case datasets\ndemonstrate that our method outperforms baseline models."}
{"id": "2509.00806", "pdf": "https://arxiv.org/pdf/2509.00806", "abs": "https://arxiv.org/abs/2509.00806", "authors": ["Reem Abdel-Salam", "Mary Adewunmi", "Modinat A. Abayomi"], "title": "CaresAI at BioCreative IX Track 1 -- LLM for Biomedical QA", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Proceedings of the BioCreative IX Challenge and Workshop (BC9): Large\n  Language Models for Clinical and Biomedical NLP at the International Joint\n  Conference on Artificial Intelligence (IJCAI), Montreal, Canada, 2025", "summary": "Large language models (LLMs) are increasingly evident for accurate question\nanswering across various domains. However, rigorous evaluation of their\nperformance on complex question-answering (QA) capabilities is essential before\ndeployment in real-world biomedical and healthcare applications. This paper\npresents our approach to the MedHopQA track of the BioCreative IX shared task,\nwhich focuses on multi-hop biomedical question answering involving diseases,\ngenes, and chemicals. We adopt a supervised fine-tuning strategy leveraging\nLLaMA 3 8B, enhanced with a curated biomedical question-answer dataset compiled\nfrom external sources including BioASQ, MedQuAD, and TREC. Three experimental\nsetups are explored: fine-tuning on combined short and long answers, short\nanswers only, and long answers only. While our models demonstrate strong domain\nunderstanding, achieving concept-level accuracy scores of up to 0.8, their\nExact Match (EM) scores remain significantly lower, particularly in the test\nphase. We introduce a two-stage inference pipeline for precise short-answer\nextraction to mitigate verbosity and improve alignment with evaluation metrics.\nDespite partial improvements, challenges persist in generating strictly\nformatted outputs. Our findings highlight the gap between semantic\nunderstanding and exact answer evaluation in biomedical LLM applications,\nmotivating further research in output control and post-processing strategies."}
{"id": "2509.00822", "pdf": "https://arxiv.org/pdf/2509.00822", "abs": "https://arxiv.org/abs/2509.00822", "authors": ["Felix Engl", "Andreas Henrich"], "title": "TMT: A Simple Way to Translate Topic Models Using Dictionaries", "categories": ["cs.CL", "cs.IR", "I.2.7; H.3.1"], "comment": "10 pages, 2 figures, 8 tables", "summary": "The training of topic models for a multilingual environment is a challenging\ntask, requiring the use of sophisticated algorithms, topic-aligned corpora, and\nmanual evaluation. These difficulties are further exacerbated when the\ndeveloper lacks knowledge of the target language or is working in an\nenvironment with limited data, where only small or unusable multilingual\ncorpora are available.\n  Considering these challenges, we introduce Topic Model Translation (TMT), a\nnovel, robust and transparent technique designed to transfer topic models\n(e.g., Latent Dirichlet Allocation (LDA) based topic models) from one language\nto another, without the need for metadata, embeddings, or aligned corpora. TMT\nenables the reuse of topic models across languages, making it especially\nsuitable for scenarios where large corpora in the target language are\nunavailable or manual translation is infeasible. Furthermore, we evaluate TMT\nextensively using both quantitative and qualitative methods, demonstrating that\nit produces semantically coherent and consistent topic translations."}
{"id": "2509.00841", "pdf": "https://arxiv.org/pdf/2509.00841", "abs": "https://arxiv.org/abs/2509.00841", "authors": ["Michelle Elizabeth", "Alicja Kasicka", "Natalia Krawczyk", "Magalie Ochs", "Gwénolé Lecorvé", "Justyna Gromada", "Lina M. Rojas-Barahona"], "title": "Neural Models and Language Model Prompting for the Multidimensional Evaluation of Open-Ended Conversations", "categories": ["cs.CL"], "comment": null, "summary": "The growing number of generative AI-based dialogue systems has made their\nevaluation a crucial challenge. This paper presents our contribution to this\nimportant problem through the Dialogue System Technology Challenge (DSTC-12,\nTrack 1), where we developed models to predict dialogue-level,\ndimension-specific scores. Given the constraint of using relatively small\nmodels (i.e. fewer than 13 billion parameters) our work follows two main\nstrategies: employing Language Models (LMs) as evaluators through prompting,\nand training encoder-based classification and regression models.\n  Our results show that while LM prompting achieves only modest correlations\nwith human judgments, it still ranks second on the test set, outperformed only\nby the baseline. The regression and classification models, with significantly\nfewer parameters, demonstrate high correlation for some dimensions on the\nvalidation set. Although their performance decreases on the test set, it is\nimportant to note that the test set contains annotations with significantly\ndifferent score ranges for some of the dimensions with respect to the train and\nvalidation sets."}
{"id": "2509.00842", "pdf": "https://arxiv.org/pdf/2509.00842", "abs": "https://arxiv.org/abs/2509.00842", "authors": ["Tengyu Pan", "Zhichao Duan", "Zhenyu Li", "Bowen Dong", "Ning Liu", "Xiuxing Li", "Jianyong Wang"], "title": "Negative Matters: Multi-Granularity Hard-Negative Synthesis and Anchor-Token-Aware Pooling for Enhanced Text Embeddings", "categories": ["cs.CL"], "comment": null, "summary": "Text embedding models are essential for various natural language processing\ntasks, enabling the effective encoding of semantic information into dense\nvector representations. These models are typically optimized using triplets of\n(query, positive, negative) data pairs for contrastive learning, where the\nnegative samples play a critical role in enhancing the model's ability to\ndiscern subtle semantic distinctions. In this work, we introduce a\nMulti-Granularity Hard-negative (MGH) synthesis framework that leverages large\nlanguage models (LLMs) to generate diverse negative samples with varying levels\nof similarity with the query. This approach facilitates a coarse-to-fine\ncurriculum learning strategy during supervised training, allowing the embedding\nmodel to progressively learn more nuanced semantic representations. Meanwhile,\nwe propose an Anchor Token Aware (ATA) pooling method that assigns higher\nweights to anchor tokens based on aggregation patterns observed in LLMs,\nimproving text embedding accuracy without increasing model complexity.\nComprehensive experiments on the MTEB benchmark demonstrate that our methods\nachieve state-of-the-art performance, surpassing existing synthesis strategies\nboth with synthetic data and when combined with public retrieval datasets."}
{"id": "2509.00849", "pdf": "https://arxiv.org/pdf/2509.00849", "abs": "https://arxiv.org/abs/2509.00849", "authors": ["Shaina Raza", "Maximus Powers", "Partha Pratim Saha", "Mahveen Raza", "Rizwan Qureshi"], "title": "Prompting Away Stereotypes? Evaluating Bias in Text-to-Image Models for Occupations", "categories": ["cs.CL"], "comment": null, "summary": "Text-to-Image (TTI) models are powerful creative tools but risk amplifying\nharmful social biases. We frame representational societal bias assessment as an\nimage curation and evaluation task and introduce a pilot benchmark of\noccupational portrayals spanning five socially salient roles (CEO, Nurse,\nSoftware Engineer, Teacher, Athlete). Using five state-of-the-art models:\nclosed-source (DALLE 3, Gemini Imagen 4.0) and open-source (FLUX.1-dev, Stable\nDiffusion XL Turbo, Grok-2 Image), we compare neutral baseline prompts against\nfairness-aware controlled prompts designed to encourage demographic diversity.\nAll outputs are annotated for gender (male, female) and race (Asian, Black,\nWhite), enabling structured distributional analysis. Results show that\nprompting can substantially shift demographic representations, but with highly\nmodel-specific effects: some systems diversify effectively, others overcorrect\ninto unrealistic uniformity, and some show little responsiveness. These\nfindings highlight both the promise and the limitations of prompting as a\nfairness intervention, underscoring the need for complementary model-level\nstrategies. We release all code and data for transparency and reproducibility\nhttps://github.com/maximus-powers/img-gen-bias-analysis."}
{"id": "2509.00869", "pdf": "https://arxiv.org/pdf/2509.00869", "abs": "https://arxiv.org/abs/2509.00869", "authors": ["Zixuan Shangguan", "Yanjie Dong", "Lanjun Wang", "Xiaoyi Fan", "Victor C. M. Leung", "Xiping Hu"], "title": "Exploring and Mitigating Fawning Hallucinations in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated exceptional proficiency in\nlanguage understanding. However, when LLMs align their outputs with deceptive\nand/or misleading prompts, the generated responses could deviate from the de\nfacto information. Such observations are known as fawning hallucinations, where\nthe model prioritizes alignment with the input's implied perspective over\naccuracy and truthfulness. In this work, we analyze fawning hallucinations in\nvarious natural language processing tasks and tailor the so-termed contrastive\ndecoding method for fawning-hallucination mitigation. Specifically, we design\ntwo paradigms to generate corresponding deceptive and/or misleading inputs for\nthe consistent fawning hallucinations induction. Then, we propose the\ncollaborative contrastive decoding (CCD) to handle the fawning hallucinations\nacross different tasks in LLMs. By contrasting the deviation in output\ndistribution between induced and transformed neutral inputs, the proposed CCD\ncan reduce reliance on deceptive and/or misleading information without\nrequiring additional training. Extensive experiments demonstrate that the\nproposed CCD can effectively mitigate fawning hallucinations and improve the\nfactuality of the generated responses over various tasks."}
{"id": "2509.00877", "pdf": "https://arxiv.org/pdf/2509.00877", "abs": "https://arxiv.org/abs/2509.00877", "authors": ["Yuqin Dai", "Guoqing Wang", "Yuan Wang", "Kairan Dou", "Kaichen Zhou", "Zhanwei Zhang", "Shuo Yang", "Fei Tang", "Jun Yin", "Pengyu Zeng", "Zhenzhe Ying", "Can Yi", "Changhua Meng", "Yuchen Zhou", "Yongliang Shen", "Shuai Lu"], "title": "EviNote-RAG: Enhancing RAG Models via Answer-Supportive Evidence Notes", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) empowered with retrieval mechanisms have\nachieved strong progress in open-domain question answering (QA). Yet, the\nconventional retrieve--then--answer paradigm often suffers from two key\nlimitations: (1) low signal-to-noise ratio in retrieved evidence, where useful\ninformation is buried under irrelevant content, and (2) error accumulation in\nmulti-hop reasoning when incomplete or noisy passages are involved. To address\nthese challenges, we present EviNote-RAG, an agentic RAG framework that\nintroduces a structured retrieve--note--answer pipeline. Instead of directly\nreasoning over raw retrievals, the model is trained to compose\nSupportive-Evidence Notes (SENs), concise, human-like notes that preserve only\nanswer-relevant information, highlight uncertainty, and explicitly state when\nno useful evidence exists. This distillation process is further reinforced by\nthe Evidence Quality Reward (EQR), an entailment-based signal that evaluates\nwhether SENs logically support the final answer. Together, SENs and EQR guide\nthe model toward faithful and robust reasoning, while reducing the impact of\nnoise. Experiments on in-domain and out-of-domain QA benchmarks show that\nEviNote-RAG consistently outperforms strong baselines in accuracy,\ngeneralization, and training stability. In particular, it achieves\nstate-of-the-art results while enhancing robustness and efficiency, yielding\nrelative F1 gains of 20\\% on HotpotQA (+0.093), 40\\% on Bamboogle (+0.151), and\n91\\% on 2Wiki (+0.256) via denser rewards and reduced verbosity."}
{"id": "2509.00893", "pdf": "https://arxiv.org/pdf/2509.00893", "abs": "https://arxiv.org/abs/2509.00893", "authors": ["Răzvan-Alexandru Smădu", "Andreea Iuga", "Dumitru-Clementin Cercel", "Florin Pop"], "title": "SeLeRoSa: Sentence-Level Romanian Satire Detection Dataset", "categories": ["cs.CL", "I.2.7; I.7"], "comment": "12 pages, 2 Figures", "summary": "Satire, irony, and sarcasm are techniques typically used to express humor and\ncritique, rather than deceive; however, they can occasionally be mistaken for\nfactual reporting, akin to fake news. These techniques can be applied at a more\ngranular level, allowing satirical information to be incorporated into news\narticles. In this paper, we introduce the first sentence-level dataset for\nRomanian satire detection for news articles, called SeLeRoSa. The dataset\ncomprises 13,873 manually annotated sentences spanning various domains,\nincluding social issues, IT, science, and movies. With the rise and recent\nprogress of large language models (LLMs) in the natural language processing\nliterature, LLMs have demonstrated enhanced capabilities to tackle various\ntasks in zero-shot settings. We evaluate multiple baseline models based on LLMs\nin both zero-shot and fine-tuning settings, as well as baseline\ntransformer-based models. Our findings reveal the current limitations of these\nmodels in the sentence-level satire detection task, paving the way for new\nresearch directions."}
{"id": "2509.00921", "pdf": "https://arxiv.org/pdf/2509.00921", "abs": "https://arxiv.org/abs/2509.00921", "authors": ["David Dukić", "Goran Glavaš", "Jan Šnajder"], "title": "Supervised In-Context Fine-Tuning for Generative Sequence Labeling", "categories": ["cs.CL"], "comment": null, "summary": "Sequence labeling (SL) tasks, where labels are assigned to tokens, are\nabundant in NLP (e.g., named entity recognition and aspect-based sentiment\nanalysis). Owing to the intuition that they require bidirectional context, SL\ntasks are commonly tackled with encoder-only models. Recent work also shows\nthat removing the causal mask in fine-tuning enables decoder-based LLMs to\nbecome effective token classifiers. Less work, however, focused on (supervised)\ngenerative SL, a more natural setting for causal LLMs. Due to their rapid\nscaling, causal LLMs applied to SL are expected to outperform encoders, whose\nown development has stagnated. In this work, we propose supervised in-context\nfine-tuning (SIFT) for generative SL. SIFT casts SL tasks as constrained\nresponse generation, natural to LLMs, combining (1) in-context learning (ICL)\nfrom demonstrations with (2) supervised fine-tuning. SIFT considerably\noutperforms both ICL and decoder-as-encoder fine-tuning baselines on a range of\nstandard SL tasks. We further find that although long context hinders the\nperformance of generative SL in both ICL and SIFT, this deficiency can be\nmitigated by removing the instruction, as instructions are shown to be largely\nunnecessary for achieving strong SL performance with SIFT. Our findings\nhighlight strengths and limitations of SL with LLMs, underscoring the\nimportance of a response-based generative task formulation for effective SL\nperformance."}
{"id": "2509.00934", "pdf": "https://arxiv.org/pdf/2509.00934", "abs": "https://arxiv.org/abs/2509.00934", "authors": ["Md Shahidul Salim", "Lian Fu", "Arav Adikesh Ramakrishnan", "Zonghai Yao", "Hong Yu"], "title": "MedCOD: Enhancing English-to-Spanish Medical Translation of Large Language Models Using Enriched Chain-of-Dictionary Framework", "categories": ["cs.CL", "cs.AI"], "comment": "To appear in Findings of the Association for Computational\n  Linguistics: EMNLP 2025", "summary": "We present MedCOD (Medical Chain-of-Dictionary), a hybrid framework designed\nto improve English-to-Spanish medical translation by integrating\ndomain-specific structured knowledge into large language models (LLMs). MedCOD\nintegrates domain-specific knowledge from both the Unified Medical Language\nSystem (UMLS) and the LLM-as-Knowledge-Base (LLM-KB) paradigm to enhance\nstructured prompting and fine-tuning. We constructed a parallel corpus of 2,999\nEnglish-Spanish MedlinePlus articles and a 100-sentence test set annotated with\nstructured medical contexts. Four open-source LLMs (Phi-4, Qwen2.5-14B,\nQwen2.5-7B, and LLaMA-3.1-8B) were evaluated using structured prompts that\nincorporated multilingual variants, medical synonyms, and UMLS-derived\ndefinitions, combined with LoRA-based fine-tuning. Experimental results\ndemonstrate that MedCOD significantly improves translation quality across all\nmodels. For example, Phi-4 with MedCOD and fine-tuning achieved BLEU 44.23,\nchrF++ 28.91, and COMET 0.863, surpassing strong baseline models like GPT-4o\nand GPT-4o-mini. Ablation studies confirm that both MedCOD prompting and model\nadaptation independently contribute to performance gains, with their\ncombination yielding the highest improvements. These findings highlight the\npotential of structured knowledge integration to enhance LLMs for medical\ntranslation tasks."}
{"id": "2509.00949", "pdf": "https://arxiv.org/pdf/2509.00949", "abs": "https://arxiv.org/abs/2509.00949", "authors": ["Yihong Chen"], "title": "Structure and Destructure: Dual Forces in the Making of Knowledge Engines", "categories": ["cs.CL", "cs.AI", "68T05, 68T30, 68T50", "I.2.6; I.2.7; H.3.3; K.8.0"], "comment": "PhD thesis. https://discovery.ucl.ac.uk/id/eprint/10211291/", "summary": "The making of knowledge engines in natural language processing has been\nshaped by two seemingly distinct paradigms: one grounded in structure, the\nother driven by massively available unstructured data. The structured paradigm\nleverages predefined symbolic interactions, such as knowledge graphs, as priors\nand designs models to capture them. In contrast, the unstructured paradigm\ncenters on scaling transformer architectures with increasingly vast data and\nmodel sizes, as seen in modern large language models. Despite their divergence,\nthis thesis seeks to establish conceptual connections bridging these paradigms.\nTwo complementary forces, structure and destructure, emerge across both\nparadigms: structure organizes seen symbolic interactions, while destructure,\nthrough periodic embedding resets, improves model plasticity and generalization\nto unseen scenarios. These connections form a new recipe for developing general\nknowledge engines that can support transparent, controllable, and adaptable\nintelligent systems."}
{"id": "2509.00974", "pdf": "https://arxiv.org/pdf/2509.00974", "abs": "https://arxiv.org/abs/2509.00974", "authors": ["Chia-Hsuan Hsu", "Jun-En Ding", "Hsin-Ling Hsu", "Feng Liu", "Fang-Ming Hung"], "title": "RPRO:Ranked Preference Reinforcement Optimization for Enhancing Medical QA and Diagnostic Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Medical question answering requires advanced reasoning that integrates domain\nknowledge with logical inference. However, existing large language models\n(LLMs) often generate reasoning chains that lack factual accuracy and clinical\nreliability. We propose Ranked Preference Reinforcement Optimization (RPRO), a\nnovel framework that uniquely combines reinforcement learning with\npreference-driven reasoning refinement to enhance clinical chain-of-thought\n(CoT) performance. RPRO differentiates itself from prior approaches by\nemploying task-adaptive reasoning templates and a probabilistic evaluation\nmechanism that aligns outputs with established clinical workflows, while\nautomatically identifying and correcting low-quality reasoning chains. Unlike\ntraditional pairwise preference methods, RPRO introduces a groupwise ranking\noptimization based on the Bradley-Terry model and incorporates KL-divergence\nregularization for stable training. Experiments on PubMedQA and MedQA-USMLE\nshow consistent improvements over strong baselines. Remarkably, our 1.1B\nparameter model outperforms much larger 7B-13B models, including\nmedical-specialized variants. These findings demonstrate that combining\npreference optimization with quality-driven refinement offers a scalable and\neffective approach to building more reliable, clinically grounded medical LLMs."}
{"id": "2509.00983", "pdf": "https://arxiv.org/pdf/2509.00983", "abs": "https://arxiv.org/abs/2509.00983", "authors": ["Sadia Zaman Mishu", "S M Rafiuddin"], "title": "Performance Analysis of Supervised Machine Learning Algorithms for Text Classification", "categories": ["cs.CL"], "comment": "8 pages, 2 figures, published in 2016 at the 19th International\n  Conference on Computer and Information Technology (ICCIT), Bangladesh,\n  proceedings pp. 409-413, IEEE", "summary": "The demand for text classification is growing significantly in web searching,\ndata mining, web ranking, recommendation systems, and so many other fields of\ninformation and technology. This paper illustrates the text classification\nprocess on different datasets using some standard supervised machine learning\ntechniques. Text documents can be classified through various kinds of\nclassifiers. Labeled text documents are used to classify the text in supervised\nclassifications. This paper applies these classifiers on different kinds of\nlabeled documents and measures the accuracy of the classifiers. An Artificial\nNeural Network (ANN) model using Back Propagation Network (BPN) is used with\nseveral other models to create an independent platform for labeled and\nsupervised text classification process. An existing benchmark approach is used\nto analyze the performance of classification using labeled documents.\nExperimental analysis on real data reveals which model works well in terms of\nclassification accuracy."}
{"id": "2509.01011", "pdf": "https://arxiv.org/pdf/2509.01011", "abs": "https://arxiv.org/abs/2509.01011", "authors": ["S M Rafiuddin"], "title": "Ranking of Bangla Word Graph using Graph-based Ranking Algorithms", "categories": ["cs.CL"], "comment": "8 pages, 2 figures, Publication date 2017-12-07, Conference 2017 3rd\n  International Conference on Electrical Information and Communication\n  Technology EICT, Pages 1-5, Publisher IEEE", "summary": "Ranking words is an important way to summarize a text or to retrieve\ninformation. A word graph is a way to represent the words of a sentence or a\ntext as the vertices of a graph and to show the relationship among the words.\nIt is also useful to determine the relative importance of a word among the\nwords in the word-graph. In this research, the ranking of Bangla words are\ncalculated, representing Bangla words from a text in a word graph using various\ngraph based ranking algorithms. There is a lack of a standard Bangla word\ndatabase. In this research, the Indian Language POS-tag Corpora is used, which\nhas a rich collection of Bangla words in the form of sentences with their parts\nof speech tags. For applying a word graph to various graph based ranking\nalgorithms, several standard procedures are applied. The preprocessing steps\nare done in every word graph and then applied to graph based ranking algorithms\nto make a comparison among these algorithms. This paper illustrate the entire\nprocedure of calculating the ranking of Bangla words, including the\nconstruction of the word graph from text. Experimental result analysis on real\ndata reveals the accuracy of each ranking algorithm in terms of F1 measure."}
{"id": "2509.01035", "pdf": "https://arxiv.org/pdf/2509.01035", "abs": "https://arxiv.org/abs/2509.01035", "authors": ["Nikta Gohari Sadr", "Sahar Heidariasl", "Karine Megerdoomian", "Laleh Seyyed-Kalantari", "Ali Emami"], "title": "We Politely Insist: Your LLM Must Learn the Persian Art of Taarof", "categories": ["cs.CL"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Large language models (LLMs) struggle to navigate culturally specific\ncommunication norms, limiting their effectiveness in global contexts. We focus\non Persian taarof, a social norm in Iranian interactions, which is a\nsophisticated system of ritual politeness that emphasizes deference, modesty,\nand indirectness, yet remains absent from existing cultural benchmarks. We\nintroduce TaarofBench, the first benchmark for evaluating LLM understanding of\ntaarof, comprising 450 role-play scenarios covering 12 common social\ninteraction topics, validated by native speakers. Our evaluation of five\nfrontier LLMs reveals substantial gaps in cultural competence, with accuracy\nrates 40-48% below native speakers when taarof is culturally appropriate.\nPerformance varies between interaction topics, improves with Persian-language\nprompts, and exhibits gender-based asymmetries. We also show that responses\nrated \"polite\" by standard metrics often violate taarof norms, indicating the\nlimitations of Western politeness frameworks. Through supervised fine-tuning\nand Direct Preference Optimization, we achieve 21.8% and 42.3% improvement in\nmodel alignment with cultural expectations. Our human study with 33\nparticipants (11 native Persian, 11 heritage, and 11 non-Iranian speakers)\nforms baselines in varying degrees of familiarity with Persian norms. This work\nlays the foundation for developing diverse and culturally aware LLMs, enabling\napplications that better navigate complex social interactions."}
{"id": "2509.01053", "pdf": "https://arxiv.org/pdf/2509.01053", "abs": "https://arxiv.org/abs/2509.01053", "authors": ["Xiaoying Song", "Anirban Saha Anik", "Eduardo Blanco", "Vanessa Frias-Martinez", "Lingzi Hong"], "title": "A Dynamic Fusion Model for Consistent Crisis Response", "categories": ["cs.CL"], "comment": "Accepted at Findings of EMNLP 2025, 10 pages, 5 figures", "summary": "In response to the urgent need for effective communication with\ncrisis-affected populations, automated responses driven by language models have\nbeen proposed to assist in crisis communications. A critical yet often\noverlooked factor is the consistency of response style, which could affect the\ntrust of affected individuals in responders. Despite its importance, few\nstudies have explored methods for maintaining stylistic consistency across\ngenerated responses. To address this gap, we propose a novel metric for\nevaluating style consistency and introduce a fusion-based generation approach\ngrounded in this metric. Our method employs a two-stage process: it first\nassesses the style of candidate responses and then optimizes and integrates\nthem at the instance level through a fusion process. This enables the\ngeneration of high-quality responses while significantly reducing stylistic\nvariation between instances. Experimental results across multiple datasets\ndemonstrate that our approach consistently outperforms baselines in both\nresponse quality and stylistic uniformity."}
{"id": "2509.01058", "pdf": "https://arxiv.org/pdf/2509.01058", "abs": "https://arxiv.org/abs/2509.01058", "authors": ["Xiaoying Song", "Anirban Saha Anik", "Dibakar Barua", "Pengcheng Luo", "Junhua Ding", "Lingzi Hong"], "title": "Speaking at the Right Level: Literacy-Controlled Counterspeech Generation with RAG-RL", "categories": ["cs.CL"], "comment": "Accepted at Findings of EMNLP 2025", "summary": "Health misinformation spreading online poses a significant threat to public\nhealth. Researchers have explored methods for automatically generating\ncounterspeech to health misinformation as a mitigation strategy. Existing\napproaches often produce uniform responses, ignoring that the health literacy\nlevel of the audience could affect the accessibility and effectiveness of\ncounterspeech. We propose a Controlled-Literacy framework using\nretrieval-augmented generation (RAG) with reinforcement learning (RL) to\ngenerate tailored counterspeech adapted to different health literacy levels. In\nparticular, we retrieve knowledge aligned with specific health literacy levels,\nenabling accessible and factual information to support generation. We design a\nreward function incorporating subjective user preferences and objective\nreadability-based rewards to optimize counterspeech to the target health\nliteracy level. Experiment results show that Controlled-Literacy outperforms\nbaselines by generating more accessible and user-preferred counterspeech. This\nresearch contributes to more equitable and impactful public health\ncommunication by improving the accessibility and comprehension of counterspeech\nto health misinformation."}
{"id": "2509.01081", "pdf": "https://arxiv.org/pdf/2509.01081", "abs": "https://arxiv.org/abs/2509.01081", "authors": ["Abdessalam Bouchekif", "Samer Rashwani", "Heba Sbahi", "Shahd Gaben", "Mutez Al-Khatib", "Mohammed Ghaly"], "title": "Assessing Large Language Models on Islamic Legal Reasoning: Evidence from Inheritance Law Evaluation", "categories": ["cs.CL", "cs.AI", "I.2.6; I.2.7"], "comment": "10 pages, 7 Tables, Code:\n  https://github.com/bouchekif/inheritance_evaluation", "summary": "This paper evaluates the knowledge and reasoning capabilities of Large\nLanguage Models in Islamic inheritance law, known as 'ilm al-mawarith. We\nassess the performance of seven LLMs using a benchmark of 1,000 multiple-choice\nquestions covering diverse inheritance scenarios, designed to test models'\nability to understand the inheritance context and compute the distribution of\nshares prescribed by Islamic jurisprudence. The results reveal a significant\nperformance gap: o3 and Gemini 2.5 achieved accuracies above 90%, whereas\nALLaM, Fanar, LLaMA, and Mistral scored below 50%. These disparities reflect\nimportant differences in reasoning ability and domain adaptation. We conduct a\ndetailed error analysis to identify recurring failure patterns across models,\nincluding misunderstandings of inheritance scenarios, incorrect application of\nlegal rules, and insufficient domain knowledge. Our findings highlight\nlimitations in handling structured legal reasoning and suggest directions for\nimproving performance in Islamic legal reasoning. Code:\nhttps://github.com/bouchekif/inheritance_evaluation"}
{"id": "2509.01084", "pdf": "https://arxiv.org/pdf/2509.01084", "abs": "https://arxiv.org/abs/2509.01084", "authors": ["Farah Adeeba", "Rajesh Bhatt"], "title": "A Paradigm Gap in Urdu", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we document a paradigm gap in the combinatorial possibilities\nof verbs and aspect in Urdu: the perfective form of the -ya: kar construction\n(e.g. ro-ya: ki: cry-Pfv do.Pfv) is sharply ungrammatical in modern Urdu and\nHindi, despite being freely attested in 19th century literature. We investigate\nthis diachronic shift through historical text analysis, a large-scale corpus\nstudy which confirms the stark absence of perfective forms and subjective\nevaluation tasks with native speakers, who judge perfective examples as highly\nunnatural. We argue that this gap arose from a fundamental morphosyntactic\nconflict: the construction's requirement for a nominative subject and an\ninvariant participle clashes with the core grammatical rule that transitive\nperfective assign ergative case. This conflict rendered the perfective form\nunstable, and its functional replacement by other constructions allowed the gap\nto become entrenched in the modern grammar."}
{"id": "2509.01088", "pdf": "https://arxiv.org/pdf/2509.01088", "abs": "https://arxiv.org/abs/2509.01088", "authors": ["Jinwen Chen", "Hainan Zhang", "Liang Pang", "Yongxin Tong", "Haibo Zhou", "Yuan Zhan", "Wei Lin", "Zhiming Zheng"], "title": "Privacy-Preserving Reasoning with Knowledge-Distilled Parametric Retrieval Augmented Generation", "categories": ["cs.CL"], "comment": null, "summary": "The current RAG system requires uploading plaintext documents to the cloud,\nrisking private data leakage. Parametric RAG (PRAG) addresses this by encoding\ndocuments as LoRA within LLMs, enabling reasoning without exposing raw content.\nHowever, it still faces two issues: (1) PRAG demands synthesizing QA pairs and\nfine-tuning LLM for each individual document to create its corresponding LoRA,\nleading to unacceptable inference latency. (2) The performance of PRAG relies\nsolely on synthetic QA data, lacking internal alignment with standard RAG,\nresulting in poor generalization on out-of-distribution(OOD) inputs. Therefore,\nachieving high-efficiency parameterization while maintaining RAG-level\nperformance remains a critical challenge for privacy-preserving reasoning. In\nthis paper, we propose DistilledPRAG, a generalizable knowledge-distilled\nparametric RAG model aligned with standard RAG in document structure and\nparameter activation. We first synthesize QA pairs from single and\nmulti-documents to enhance cross-document reasoning. Then, we mask the\nplaintext documents with a special token and translate them to LoRA via a\nparameter generator, maintaining the standard RAG document structure. Finally,\nguided by synthetic QA data, we train the parameter generator to match standard\nRAG's hidden states and output logits, enabling RAG-style reasoning without\noriginal documents. Experiments on four QA datasets show that DistilledPRAG\noutperforms baselines in accuracy and generalizes well on OOD data."}
{"id": "2509.01092", "pdf": "https://arxiv.org/pdf/2509.01092", "abs": "https://arxiv.org/abs/2509.01092", "authors": ["Xiaoqiang Lin", "Aritra Ghosh", "Bryan Kian Hsiang Low", "Anshumali Shrivastava", "Vijai Mohan"], "title": "REFRAG: Rethinking RAG based Decoding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes."}
{"id": "2509.01093", "pdf": "https://arxiv.org/pdf/2509.01093", "abs": "https://arxiv.org/abs/2509.01093", "authors": ["Yulong Wu", "Viktor Schlegel", "Riza Batista-Navarro"], "title": "Natural Context Drift Undermines the Natural Language Understanding of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025 Findings", "summary": "How does the natural evolution of context paragraphs affect question\nanswering in generative Large Language Models (LLMs)? To investigate this, we\npropose a framework for curating naturally evolved, human-edited variants of\nreading passages from contemporary QA benchmarks and for analyzing LLM\nperformance across a range of semantic similarity scores, which quantify how\nclosely each variant aligns with content seen during pretraining. Using this\nframework, we evaluate six QA datasets and eight LLMs with publicly available\ntraining data. Our experiments reveal that LLM performance declines as reading\npassages naturally diverge from the versions encountered during\npretraining-even when the question and all necessary information remains\npresent at inference time. For instance, average model accuracy on BoolQ drops\nby over 30% from the highest to lowest similarity bins, with slopes exceeding\n70 across several LLMs. These findings suggest that natural text evolution\nposes a significant challenge to the language understanding capabilities of\nLLMs."}
{"id": "2509.01142", "pdf": "https://arxiv.org/pdf/2509.01142", "abs": "https://arxiv.org/abs/2509.01142", "authors": ["Zhihui Xie", "Jiacheng Ye", "Lin Zheng", "Jiahui Gao", "Jingwei Dong", "Zirui Wu", "Xueliang Zhao", "Shansan Gong", "Xin Jiang", "Zhenguo Li", "Lingpeng Kong"], "title": "Dream-Coder 7B: An Open Diffusion Language Model for Code", "categories": ["cs.CL"], "comment": null, "summary": "We present Dream-Coder 7B, an open-source discrete diffusion language model\nfor code generation that exhibits emergent any-order generation capabilities.\nUnlike traditional autoregressive (AR) models that decode strictly\nleft-to-right, Dream-Coder 7B adaptively determines its decoding strategy based\non the coding task: sketch-first generation for complex algorithms,\nleft-to-right generation for straightforward completions, and interleaved\nreasoning generation for code understanding tasks. We adapt a pretrained AR\ncheckpoint to a discrete diffusion frameworks with a continuous-time weighted\ncross-entropy objective. Our post-training recipe comprises (i) supervised\nfine-tuning, where we mitigate padding pathologies via random truncation and a\npadding penalty to improve sample efficiency and stabilize generation; and (ii)\nreinforcement learning with verifiable rewards over a curated high-quality\nprompt set drawn from open-source datasets, using a tailored reinforcement\nlearning recipe for diffusion language models. The resulting Dream-Coder 7B\nInstruct attains 21.4\\% pass@1 on LiveCodeBench (2410--2505) and demonstrates\ncompetitive performance on HumanEval, MBPP, BigCodeBench, and CRUXEval. We\nrelease Dream-Coder-7B and Dream-Coder-7B-Instruct checkpoints, training\nrecipes, preprocessing pipelines, and inference code to facilitate\nreproducibility and further research."}
{"id": "2509.01147", "pdf": "https://arxiv.org/pdf/2509.01147", "abs": "https://arxiv.org/abs/2509.01147", "authors": ["Zhihao Zhang", "Sophia Yat Mei Lee", "Dong Zhang", "Shoushan Li", "Guodong Zhou"], "title": "Zero-shot Cross-lingual NER via Mitigating Language Difference: An Entity-aligned Translation Perspective", "categories": ["cs.CL"], "comment": "EMNLP 2025", "summary": "Cross-lingual Named Entity Recognition (CL-NER) aims to transfer knowledge\nfrom high-resource languages to low-resource languages. However, existing\nzero-shot CL-NER (ZCL-NER) approaches primarily focus on Latin script language\n(LSL), where shared linguistic features facilitate effective knowledge\ntransfer. In contrast, for non-Latin script language (NSL), such as Chinese and\nJapanese, performance often degrades due to deep structural differences. To\naddress these challenges, we propose an entity-aligned translation (EAT)\napproach. Leveraging large language models (LLMs), EAT employs a\ndual-translation strategy to align entities between NSL and English. In\naddition, we fine-tune LLMs using multilingual Wikipedia data to enhance the\nentity alignment from source to target languages."}
{"id": "2509.01158", "pdf": "https://arxiv.org/pdf/2509.01158", "abs": "https://arxiv.org/abs/2509.01158", "authors": ["Xuemei Tang", "Chengxi Yan", "Jinghang Gu", "Chu-Ren Huang"], "title": "Joint Information Extraction Across Classical and Modern Chinese with Tea-MOELoRA", "categories": ["cs.CL"], "comment": "9 pages, 3 figures", "summary": "Chinese information extraction (IE) involves multiple tasks across diverse\ntemporal domains, including Classical and Modern documents. Fine-tuning a\nsingle model on heterogeneous tasks and across different eras may lead to\ninterference and reduced performance. Therefore, in this paper, we propose\nTea-MOELoRA, a parameter-efficient multi-task framework that combines LoRA with\na Mixture-of-Experts (MoE) design. Multiple low-rank LoRA experts specialize in\ndifferent IE tasks and eras, while a task-era-aware router mechanism\ndynamically allocates expert contributions. Experiments show that Tea-MOELoRA\noutperforms both single-task and joint LoRA baselines, demonstrating its\nability to leverage task and temporal knowledge effectively."}
{"id": "2509.01166", "pdf": "https://arxiv.org/pdf/2509.01166", "abs": "https://arxiv.org/abs/2509.01166", "authors": ["Yu Liu", "Yanan Cao", "Xixun Lin", "Yanmin Shang", "Shi Wang", "Shirui Pan"], "title": "Enhancing Large Language Model for Knowledge Graph Completion via Structure-Aware Alignment-Tuning", "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025, Main, Long Paper", "summary": "Knowledge graph completion (KGC) aims to infer new knowledge and make\npredictions from knowledge graphs. Recently, large language models (LLMs) have\nexhibited remarkable reasoning capabilities. LLM-enhanced KGC methods primarily\nfocus on designing task-specific instructions, achieving promising\nadvancements. However, there are still two critical challenges. First, existing\nmethods often ignore the inconsistent representation spaces between natural\nlanguage and graph structures. Second, most approaches design separate\ninstructions for different KGC tasks, leading to duplicate works and\ntime-consuming processes. To address these challenges, we propose SAT, a novel\nframework that enhances LLMs for KGC via structure-aware alignment-tuning.\nSpecifically, we first introduce hierarchical knowledge alignment to align\ngraph embeddings with the natural language space through multi-task contrastive\nlearning. Then, we propose structural instruction tuning to guide LLMs in\nperforming structure-aware reasoning over KGs, using a unified graph\ninstruction combined with a lightweight knowledge adapter. Experimental results\non two KGC tasks across four benchmark datasets demonstrate that SAT\nsignificantly outperforms state-of-the-art methods, especially in the link\nprediction task with improvements ranging from 8.7% to 29.8%."}
{"id": "2509.01185", "pdf": "https://arxiv.org/pdf/2509.01185", "abs": "https://arxiv.org/abs/2509.01185", "authors": ["Seganrasan Subramanian", "Abhigya Verma"], "title": "Modular Techniques for Synthetic Long-Context Data Generation in Language Model Training and Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": "15 pages, 4 figures", "summary": "The ability of large language models (LLMs) to process and reason over long\ntextual inputs is critical for a wide range of real-world applications.\nHowever, progress in this area is significantly constrained by the absence of\nhigh-quality, diverse, and verifiable long-context datasets suitable for both\ntraining and evaluation. This work introduces a modular, extensible framework\nfor synthetic long-context data generation via prompt-based interaction with\nLLMs. The framework supports multiple training and alignment objectives,\nincluding Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO),\nand Group Relative Policy Optimization (GRPO). It encompasses four core\ngeneration paradigms: multi-turn conversational dialogues, document-grounded\ninput-output pairs, verifiable instruction-response tasks, and long-context\nreasoning examples. Through templated prompting, a model-agnostic architecture,\nand metadata-enriched outputs, the proposed approach facilitates scalable,\ncontrollable, and purpose-aligned dataset creation for advancing long-context\ncapabilities in LLMs."}
{"id": "2509.01186", "pdf": "https://arxiv.org/pdf/2509.01186", "abs": "https://arxiv.org/abs/2509.01186", "authors": ["Luxi He", "Nimra Nadeem", "Michel Liao", "Howard Chen", "Danqi Chen", "Mariano-Florentino Cuéllar", "Peter Henderson"], "title": "Statutory Construction and Interpretation for Artificial Intelligence", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "AI systems are increasingly governed by natural language principles, yet a\nkey challenge arising from reliance on language remains underexplored:\ninterpretive ambiguity. As in legal systems, ambiguity arises both from how\nthese principles are written and how they are applied. But while legal systems\nuse institutional safeguards to manage such ambiguity, such as transparent\nappellate review policing interpretive constraints, AI alignment pipelines\noffer no comparable protections. Different interpretations of the same rule can\nlead to inconsistent or unstable model behavior. Drawing on legal theory, we\nidentify key gaps in current alignment pipelines by examining how legal systems\nconstrain ambiguity at both the rule creation and rule application steps. We\nthen propose a computational framework that mirrors two legal mechanisms: (1) a\nrule refinement pipeline that minimizes interpretive disagreement by revising\nambiguous rules (analogous to agency rulemaking or iterative legislative\naction), and (2) prompt-based interpretive constraints that reduce\ninconsistency in rule application (analogous to legal canons that guide\njudicial discretion). We evaluate our framework on a 5,000-scenario subset of\nthe WildChat dataset and show that both interventions significantly improve\njudgment consistency across a panel of reasonable interpreters. Our approach\noffers a first step toward systematically managing interpretive ambiguity, an\nessential step for building more robust, law-following AI systems."}
{"id": "2509.01190", "pdf": "https://arxiv.org/pdf/2509.01190", "abs": "https://arxiv.org/abs/2509.01190", "authors": ["Sajjad Kachuee", "Mohammad Sharifkhani"], "title": "Efficient Large Language Models with Zero-Shot Adjustable Acceleration", "categories": ["cs.CL"], "comment": null, "summary": "Using Large Language Models (LLMs) in real-world applications presents\nsignificant challenges, particularly in balancing computational efficiency and\nperformance. Optimizing acceleration after the fine-tuning phase and during\ninference is crucial for building an efficient architecture. This paper\nintroduces Zero-Shot Adjustable Acceleration, a novel training and inference\nmethod that dynamically adjusts hardware usage during inference without\nrequiring additional fine-tuning. The proposed approach is applied to newly\ndeveloped models and evaluated across multiple classification and text\ngeneration tasks. Experimental results demonstrate that the method enables a\nwide range of acceleration in a zero-shot manner and achieves up to a 11x\nspeedup compared to the baseline."}
{"id": "2509.01200", "pdf": "https://arxiv.org/pdf/2509.01200", "abs": "https://arxiv.org/abs/2509.01200", "authors": ["Chenyang Le", "Bing Han", "Jinshun Li", "Songyong Chen", "Yanmin Qian"], "title": "SimulMEGA: MoE Routers are Advanced Policy Makers for Simultaneous Speech Translation", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Simultaneous Speech Translation (SimulST) enables real-time cross-lingual\ncommunication by jointly optimizing speech recognition and machine translation\nunder strict latency constraints. Existing systems struggle to balance\ntranslation quality, latency, and semantic coherence, particularly in\nmultilingual many-to-many scenarios where divergent read and write policies\nhinder unified strategy learning. In this paper, we present SimulMEGA\n(Simultaneous Generation by Mixture-of-Experts Gating), an unsupervised policy\nlearning framework that combines prefix-based training with a\nMixture-of-Experts refiner to learn effective read and write decisions in an\nimplicit manner, without adding inference-time overhead. Our design requires\nonly minimal modifications to standard transformer architectures and\ngeneralizes across both speech-to-text and text-to-speech streaming tasks.\nThrough comprehensive evaluation on six language pairs, our 500M parameter\nspeech-to-text model outperforms the Seamless baseline, achieving under 7\npercent BLEU degradation at 1.5 seconds average lag and under 3 percent at 3\nseconds. We further demonstrate the versatility of SimulMEGA by extending it to\nstreaming TTS with a unidirectional backbone, yielding superior latency quality\ntradeoffs."}
{"id": "2509.01213", "pdf": "https://arxiv.org/pdf/2509.01213", "abs": "https://arxiv.org/abs/2509.01213", "authors": ["Ege Süalp", "Mina Rezaei"], "title": "Mitigating Catastrophic Forgetting in Continual Learning through Model Growth", "categories": ["cs.CL"], "comment": null, "summary": "Catastrophic forgetting is a significant challenge in continual learning, in\nwhich a model loses prior knowledge when it is fine-tuned on new tasks. This\nproblem is particularly critical for large language models (LLMs) undergoing\ncontinual learning, as retaining performance across diverse domains is\nimportant for their general utility. In this paper, we explore model growth, a\npromising strategy that leverages smaller models to expedite and structure the\ntraining of larger ones for mitigating the catastrophic forgetting problem.\nAlthough growth-based pretraining, particularly via transformer stacking, has\nshown promise in accelerating convergence, its impact on forgetting remains\nunder-explored. Therefore, we evaluate whether growth-based models can retain\npreviously learned capabilities more effectively across a sequence of\nfine-tuning tasks involving domain knowledge, reasoning, reading comprehension,\nand bias. Our findings show that both models -- one trained with growth (Stack\nLLM) and one without (LLM) -- exhibit improvements in domain knowledge.\nHowever, reasoning and reading comprehension degrade over time, indicating\nsigns of catastrophic forgetting. Stack LLM consistently shows less\ndegradation, especially in reading comprehension, suggesting enhanced retention\ncapabilities. Interestingly, in bias evaluation, the baseline LLM becomes\nprogressively more neutral with continued fine-tuning, while Stack LLM\nmaintains a steady bias ratio around 60--61\\%. These results indicate that\ngrowth-based pretraining may deliver modest improvements in resisting\ncatastrophic forgetting, though trade-offs remain in handling social biases."}
{"id": "2509.01221", "pdf": "https://arxiv.org/pdf/2509.01221", "abs": "https://arxiv.org/abs/2509.01221", "authors": ["Wei Huang", "Huang Wei", "Yinggui Wang"], "title": "DaMoC: Efficiently Selecting the Optimal Large Language Model for Fine-tuning Domain Taks Based on Data and Model Compression", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by EMNLP 2025", "summary": "Large language models (LLMs) excel in general tasks but struggle with\ndomain-specific ones, requiring fine-tuning with specific data. With many\nopen-source LLMs available, selecting the best model for fine-tuning downstream\ntasks is challenging, primarily focusing on how to quickly identify the optimal\nLLM. We introduce a Data and Model Compression Framework (DaMoC) that addresses\nthis challenge by: 1) Data Level: A systematic categorization of data filtering\nmethodologies for LLMs is first established, classifying them into three\ndistinct paradigms: (1) distribution-aware methods, (2) quality-aware methods,\nand (3) hybrid approaches considering both dimensions. Further, we enhance the\ndensity of key tokens in the text achieving token compression. Subsequently, we\nuse an LLM to iterative rewrite the text to optimize its expression. 2) Model\nLevel: We use layer similarity scores to assess each layer's importance and\nremove those with lower importance. Then, we introduce a sparse merging\nparadigm to preserve as much of the original model's capability as possible.\nExtensive experiments on four datasets, medical Q&A, financial Q&A, general\nQ&A, and reading comprehension, show that we can select the optimal LLM while\nsaving approximately 20-fold in training time."}
{"id": "2509.01236", "pdf": "https://arxiv.org/pdf/2509.01236", "abs": "https://arxiv.org/abs/2509.01236", "authors": ["Hao Yang", "Zhiyu Yang", "Yunjie Zhang", "Shanyi Zhu", "Lin Yang"], "title": "Rethinking the Chain-of-Thought: The Roles of In-Context Learning and Pre-trained Priors", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Chain-of-Thought reasoning has emerged as a pivotal methodology for enhancing\nmodel inference capabilities. Despite growing interest in Chain-of-Thought\nreasoning, its underlying mechanisms remain unclear. This paper explores the\nworking mechanisms of Chain-of-Thought reasoning from the perspective of the\ndual relationship between in-context learning and pretrained priors. We first\nconduct a fine-grained lexical-level analysis of rationales to examine the\nmodel's reasoning behavior. Then, by incrementally introducing noisy exemplars,\nwe examine how the model balances pretrained priors against erroneous\nin-context information. Finally, we investigate whether prompt engineering can\ninduce slow thinking in large language models. Our extensive experiments reveal\nthree key findings: (1) The model not only quickly learns the reasoning\nstructure at the lexical level but also grasps deeper logical reasoning\npatterns, yet it heavily relies on pretrained priors. (2) Providing sufficient\nexemplars shifts the model's decision-making from pretrained priors to\nin-context signals, while misleading prompts introduce instability. (3) Long\nChain-of-Thought prompting can induce the model to generate longer reasoning\nchains, thereby improving its performance on downstream tasks."}
{"id": "2509.01260", "pdf": "https://arxiv.org/pdf/2509.01260", "abs": "https://arxiv.org/abs/2509.01260", "authors": ["Jonas Noblet"], "title": "Annotation and modeling of emotions in a textual corpus: an evaluative approach", "categories": ["cs.CL"], "comment": "in French language. 27{\\`e}me Rencontre des {\\'E}tudiants Chercheurs\n  en Informatique pour le Traitement Automatique des Langues (RECITAL), Jun\n  2025, Marseille, France", "summary": "Emotion is a crucial phenomenon in the functioning of human beings in\nsociety. However, it remains a widely open subject, particularly in its textual\nmanifestations. This paper examines an industrial corpus manually annotated\nfollowing an evaluative approach to emotion. This theoretical framework, which\nis currently underutilized, offers a different perspective that complements\ntraditional approaches. Noting that the annotations we collected exhibit\nsignificant disagreement, we hypothesized that they nonetheless follow stable\nstatistical trends. Using language models trained on these annotations, we\ndemonstrate that it is possible to model the labeling process and that\nvariability is driven by underlying linguistic features. Conversely, our\nresults indicate that language models seem capable of distinguishing emotional\nsituations based on evaluative criteria."}
{"id": "2509.01301", "pdf": "https://arxiv.org/pdf/2509.01301", "abs": "https://arxiv.org/abs/2509.01301", "authors": ["Juhyun Oh", "Inha Cha", "Michael Saxon", "Hyunseung Lim", "Shaily Bhatt", "Alice Oh"], "title": "Culture is Everywhere: A Call for Intentionally Cultural Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "The prevailing ``trivia-centered paradigm'' for evaluating the cultural\nalignment of large language models (LLMs) is increasingly inadequate as these\nmodels become more advanced and widely deployed. Existing approaches typically\nreduce culture to static facts or values, testing models via multiple-choice or\nshort-answer questions that treat culture as isolated trivia. Such methods\nneglect the pluralistic and interactive realities of culture, and overlook how\ncultural assumptions permeate even ostensibly ``neutral'' evaluation settings.\nIn this position paper, we argue for \\textbf{intentionally cultural\nevaluation}: an approach that systematically examines the cultural assumptions\nembedded in all aspects of evaluation, not just in explicitly cultural tasks.\nWe systematically characterize the what, how, and circumstances by which\nculturally contingent considerations arise in evaluation, and emphasize the\nimportance of researcher positionality for fostering inclusive, culturally\naligned NLP research. Finally, we discuss implications and future directions\nfor moving beyond current benchmarking practices, discovering important\napplications that we don't know exist, and involving communities in evaluation\ndesign through HCI-inspired participatory methodologies."}
{"id": "2509.01312", "pdf": "https://arxiv.org/pdf/2509.01312", "abs": "https://arxiv.org/abs/2509.01312", "authors": ["Sishi Xiong", "Ziyang He", "Zhongjiang He", "Yu Zhao", "Changzai Pan", "Jie Zhang", "Zhenhe Wu", "Shuangyong Song", "Yongxiang Li"], "title": "TableZoomer: A Collaborative Agent Framework for Large-scale Table Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "While large language models (LLMs) have shown promise in the table question\nanswering (TQA) task through prompt engineering, they face challenges in\nindustrial applications, including structural heterogeneity, difficulties in\ntarget data localization, and bottlenecks in complex reasoning. To address\nthese limitations, this paper presents TableZoomer, a novel LLM-powered,\nprogramming-based agent framework. It introduces three key innovations: (1)\nreplacing the original fully verbalized table with structured table schema to\nbridge the semantic gap and reduce computational complexity; (2) a query-aware\ntable zooming mechanism that dynamically generates sub-table schema through\ncolumn selection and entity linking, significantly improving target\nlocalization efficiency; and (3) a Program-of-Thoughts (PoT) strategy that\ntransforms queries into executable code to mitigate numerical hallucination.\nAdditionally, we integrate the reasoning workflow with the ReAct paradigm to\nenable iterative reasoning. Extensive experiments demonstrate that our\nframework maintains the usability advantages while substantially enhancing\nperformance and scalability across tables of varying scales. When implemented\nwith the Qwen3-8B-Instruct LLM, TableZoomer achieves accuracy improvements of\n19.34% and 25% over conventional PoT methods on the large-scale DataBench\ndataset and the small-scale Fact Checking task of TableBench dataset,\nrespectively."}
{"id": "2509.01314", "pdf": "https://arxiv.org/pdf/2509.01314", "abs": "https://arxiv.org/abs/2509.01314", "authors": ["Anum Afzal", "Mehul Kumawat", "Florian Matthes"], "title": "Can Smaller LLMs do better? Unlocking Cross-Domain Potential through Parameter-Efficient Fine-Tuning for Text Summarization", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs), being generic task solvers, are versatile.\nHowever, despite the vast amount of data they are trained on, there are\nspeculations about their adaptation capabilities to a new domain. Additionally,\nthe simple fine-tuning of the model to incorporate knowledge of a new domain is\ncomputationally expensive and time-consuming. This becomes more challenging\nwhen the domain in question is also low-resource, and labeled data is\nunavailable. We leverage parameter-efficient fine-tuning techniques (PEFTs) on\nhigh-resource datasets to address these challenges to improve performance on\nunseen low-resource domains. Throughout our experiments, we evaluate whether\nintrinsic linguistic commonalities between datasets can be leveraged for\nefficient domain adaptation. We benchmark six PEFTs with\n\\texttt{Llama-3-8B-Instruct} on 14 training datasets from the Scientific,\nMedical, Legal, and News domains for a Text Summarization task. Our experiments\nshow that for low-resource domains, inference using Within-Domain Adapters can\nachieve better performance than Few-Shot as well as a much larger\n\\texttt{Llama-3-70B-Instruct}. Lastly, in the absence of Within-Domain\nAdapters, we explore the concept of using Cross-Domain Adapters as well as the\nstrategic combinations of adapters to leverage intrinsic language similarities\nacross domains, facilitating better adaptability and performance in\nlow-resource settings."}
{"id": "2509.01322", "pdf": "https://arxiv.org/pdf/2509.01322", "abs": "https://arxiv.org/abs/2509.01322", "authors": ["Meituan LongCat Team", "Bayan", "Bei Li", "Bingye Lei", "Bo Wang", "Bolin Rong", "Chao Wang", "Chao Zhang", "Chen Gao", "Chen Zhang", "Cheng Sun", "Chengcheng Han", "Chenguang Xi", "Chi Zhang", "Chong Peng", "Chuan Qin", "Chuyu Zhang", "Cong Chen", "Congkui Wang", "Dan Ma", "Daoru Pan", "Defei Bu", "Dengchang Zhao", "Deyang Kong", "Dishan Liu", "Feiye Huo", "Fengcun Li", "Fubao Zhang", "Gan Dong", "Gang Liu", "Gang Xu", "Ge Li", "Guoqiang Tan", "Guoyuan Lin", "Haihang Jing", "Haomin Fu", "Haonan Yan", "Haoxing Wen", "Haozhe Zhao", "Hong Liu", "Hongmei Shi", "Hongyan Hao", "Hongyin Tang", "Huantian Lv", "Hui Su", "Jiacheng Li", "Jiahao Liu", "Jiahuan Li", "Jiajun Yang", "Jiaming Wang", "Jian Yang", "Jianchao Tan", "Jiaqi Sun", "Jiaqi Zhang", "Jiawei Fu", "Jiawei Yang", "Jiaxi Hu", "Jiayu Qin", "Jingang Wang", "Jiyuan He", "Jun Kuang", "Junhui Mei", "Kai Liang", "Ke He", "Kefeng Zhang", "Keheng Wang", "Keqing He", "Liang Gao", "Liang Shi", "Lianhui Ma", "Lin Qiu", "Lingbin Kong", "Lingtong Si", "Linkun Lyu", "Linsen Guo", "Liqi Yang", "Lizhi Yan", "Mai Xia", "Man Gao", "Manyuan Zhang", "Meng Zhou", "Mengxia Shen", "Mingxiang Tuo", "Mingyang Zhu", "Peiguang Li", "Peng Pei", "Peng Zhao", "Pengcheng Jia", "Pingwei Sun", "Qi Gu", "Qianyun Li", "Qingyuan Li", "Qiong Huang", "Qiyuan Duan", "Ran Meng", "Rongxiang Weng", "Ruichen Shao", "Rumei Li", "Shizhe Wu", "Shuai Liang", "Shuo Wang", "Suogui Dang", "Tao Fang", "Tao Li", "Tefeng Chen", "Tianhao Bai", "Tianhao Zhou", "Tingwen Xie", "Wei He", "Wei Huang", "Wei Liu", "Wei Shi", "Wei Wang", "Wei Wu", "Weikang Zhao", "Wen Zan", "Wenjie Shi", "Xi Nan", "Xi Su", "Xiang Li", "Xiang Mei", "Xiangyang Ji", "Xiangyu Xi", "Xiangzhou Huang", "Xianpeng Li", "Xiao Fu", "Xiao Liu", "Xiao Wei", "Xiaodong Cai", "Xiaolong Chen", "Xiaoqing Liu", "Xiaotong Li", "Xiaowei Shi", "Xiaoyu Li", "Xili Wang", "Xin Chen", "Xing Hu", "Xingyu Miao", "Xinyan He", "Xuemiao Zhang", "Xueyuan Hao", "Xuezhi Cao", "Xunliang Cai", "Xurui Yang", "Yan Feng", "Yang Bai", "Yang Chen", "Yang Yang", "Yaqi Huo", "Yerui Sun", "Yifan Lu", "Yifan Zhang", "Yipeng Zang", "Yitao Zhai", "Yiyang Li", "Yongjing Yin", "Yongkang Lv", "Yongwei Zhou", "Yu Yang", "Yuchen Xie", "Yueqing Sun", "Yuewen Zheng", "Yuhua Wei", "Yulei Qian", "Yunfan Liang", "Yunfang Tai", "Yunke Zhao", "Zeyang Yu", "Zhao Zhang", "Zhaohua Yang", "Zhenchao Zhang", "Zhikang Xia", "Zhiye Zou", "Zhizhao Zeng", "Zhongda Su", "Zhuofan Chen", "Zijian Zhang", "Ziwen Wang", "Zixu Jiang", "Zizhe Zhao", "Zongyu Wang", "Zunhai Su"], "title": "LongCat-Flash Technical Report", "categories": ["cs.CL", "cs.AI", "cs.DC", "cs.LG"], "comment": null, "summary": "We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE)\nlanguage model designed for both computational efficiency and advanced agentic\ncapabilities. Stemming from the need for scalable efficiency, LongCat-Flash\nadopts two novel designs: (a) Zero-computation Experts, which enables dynamic\ncomputational budget allocation and activates 18.6B-31.3B (27B on average) per\ntoken depending on contextual demands, optimizing resource usage. (b)\nShortcut-connected MoE, which enlarges the computation-communication overlap\nwindow, demonstrating notable gains in inference efficiency and throughput\ncompared to models of a comparable scale. We develop a comprehensive scaling\nframework for large models that combines hyperparameter transfer, model-growth\ninitialization, a multi-pronged stability suite, and deterministic computation\nto achieve stable and reproducible training. Notably, leveraging the synergy\namong scalable architectural design and infrastructure efforts, we complete\nmodel training on more than 20 trillion tokens within 30 days, while achieving\nover 100 tokens per second (TPS) for inference at a cost of \\$0.70 per million\noutput tokens. To cultivate LongCat-Flash towards agentic intelligence, we\nconduct a large-scale pre-training on optimized mixtures, followed by targeted\nmid- and post-training on reasoning, code, and instructions, with further\naugmentation from synthetic data and tool use tasks. Comprehensive evaluations\ndemonstrate that, as a non-thinking foundation model, LongCat-Flash delivers\nhighly competitive performance among other leading models, with exceptional\nstrengths in agentic tasks. The model checkpoint of LongCat-Flash is\nopen-sourced to foster community research.\n  LongCat Chat: https://longcat.ai\n  Hugging Face: https://huggingface.co/meituan-longcat\n  GitHub: https://github.com/meituan-longcat"}
{"id": "2509.01324", "pdf": "https://arxiv.org/pdf/2509.01324", "abs": "https://arxiv.org/abs/2509.01324", "authors": ["Jihyung Lee", "Daehui Kim", "Seonjeong Hwang", "Hyounghun Kim", "Gary Lee"], "title": "KoBLEX: Open Legal Question Answering with Multi-hop Reasoning", "categories": ["cs.CL"], "comment": "EMNLP 2025 Main Conference", "summary": "Large Language Models (LLM) have achieved remarkable performances in general\ndomains and are now extending into the expert domain of law. Several benchmarks\nhave been proposed to evaluate LLMs' legal capabilities. However, these\nbenchmarks fail to evaluate open-ended and provision-grounded Question\nAnswering (QA). To address this, we introduce a Korean Benchmark for Legal\nEXplainable QA (KoBLEX), designed to evaluate provision-grounded, multi-hop\nlegal reasoning. KoBLEX includes 226 scenario-based QA instances and their\nsupporting provisions, created using a hybrid LLM-human expert pipeline. We\nalso propose a method called Parametric provision-guided Selection Retrieval\n(ParSeR), which uses LLM-generated parametric provisions to guide legally\ngrounded and reliable answers. ParSeR facilitates multi-hop reasoning on\ncomplex legal questions by generating parametric provisions and employing a\nthree-stage sequential retrieval process. Furthermore, to better evaluate the\nlegal fidelity of the generated answers, we propose Legal Fidelity Evaluation\n(LF-Eval). LF-Eval is an automatic metric that jointly considers the question,\nanswer, and supporting provisions and shows a high correlation with human\njudgments. Experimental results show that ParSeR consistently outperforms\nstrong baselines, achieving the best results across multiple LLMs. Notably,\ncompared to standard retrieval with GPT-4o, ParSeR achieves +37.91 higher F1\nand +30.81 higher LF-Eval. Further analyses reveal that ParSeR efficiently\ndelivers consistent performance across reasoning depths, with ablations\nconfirming the effectiveness of ParSeR."}
{"id": "2509.01328", "pdf": "https://arxiv.org/pdf/2509.01328", "abs": "https://arxiv.org/abs/2509.01328", "authors": ["Wei Wang", "Fuqing Bie", "Junzhe Chen", "Dan Zhang", "Shiyu Huang", "Evgeny Kharlamov", "Jie Tang"], "title": "Can Large Language Models Master Complex Card Games?", "categories": ["cs.CL"], "comment": null, "summary": "Complex games have long been an important benchmark for testing the progress\nof artificial intelligence algorithms. AlphaGo, AlphaZero, and MuZero have\ndefeated top human players in Go and Chess, garnering widespread societal\nattention towards artificial intelligence. Concurrently, large language models\n(LLMs) have exhibited remarkable capabilities across various tasks, raising the\nquestion of whether LLMs can achieve similar success in complex games. In this\npaper, we explore the potential of LLMs in mastering complex card games. We\nsystematically assess the learning capabilities of LLMs across eight diverse\ncard games, evaluating the impact of fine-tuning on high-quality gameplay data,\nand examining the models' ability to retain general capabilities while\nmastering these games. Our findings indicate that: (1) LLMs can approach the\nperformance of strong game AIs through supervised fine-tuning on high-quality\ndata, (2) LLMs can master multiple complex card games simultaneously, with\nperformance augmentation for games with similar rules and conflicts for\ndissimilar ones, and (3) LLMs experience a decline in general capabilities when\nmastering complex games, but this decline can be mitigated by integrating a\ncertain amount of general instruction data. The evaluation results demonstrate\nstrong learning ability and versatility of LLMs."}
{"id": "2509.01363", "pdf": "https://arxiv.org/pdf/2509.01363", "abs": "https://arxiv.org/abs/2509.01363", "authors": ["Mohammad Zbeeb", "Hasan Abed Al Kader Hammoud", "Bernard Ghanem"], "title": "Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic", "categories": ["cs.CL"], "comment": "Under Review", "summary": "Large language models often require costly optimization, such as\nreinforcement learning, to master complex reasoning tasks. This work\ndemonstrates that reasoning ability, once learned, can be extracted and\ntransferred between models as a compact task vector. We source two publicly\navailable, identically initialized Qwen2.5 models, one fine-tuned with\nsupervised fine-tuning (SFT) and the other with group relative policy\noptimization (GRPO) on the same dataset. From these, we extract a reasoning\nvector: $v_{\\text{reason}} = \\theta_{\\text{GRPO}} - \\theta_{\\text{SFT}}$. We\nhypothesize that this vector captures the reasoning capability instilled by\nreinforcement learning while factoring out shared knowledge from the SFT\nprocess. When added to compatible instruction-tuned models through simple\narithmetic, this vector consistently improves performance across diverse\nreasoning benchmarks: GSM8K (+4.9%), HumanEval (+4.3%), SciQ (+1.7%), and\nBigBenchHard (+12.3% for the 1.5B model). The performance improvements persist\nunder adversarial conditions. Conversely, subtracting the vector causes\nsignificant performance degradation (-11.8% on GSM8K), demonstrating the\nvector's strong contribution to the model's reasoning abilities. This work\nshows how reasoning capabilities, typically developed through expensive\ntraining, can be extracted from existing open-source models and reused through\nsimple tensor arithmetic, offering a practical way to enhance models by\nrecycling prior computational investments."}
{"id": "2509.01379", "pdf": "https://arxiv.org/pdf/2509.01379", "abs": "https://arxiv.org/abs/2509.01379", "authors": ["Paloma Piot", "Diego Sánchez", "Javier Parapar"], "title": "WATCHED: A Web AI Agent Tool for Combating Hate Speech by Expanding Data", "categories": ["cs.CL"], "comment": null, "summary": "Online harms are a growing problem in digital spaces, putting user safety at\nrisk and reducing trust in social media platforms. One of the most persistent\nforms of harm is hate speech. To address this, we need tools that combine the\nspeed and scale of automated systems with the judgment and insight of human\nmoderators. These tools should not only find harmful content but also explain\ntheir decisions clearly, helping to build trust and understanding. In this\npaper, we present WATCHED, a chatbot designed to support content moderators in\ntackling hate speech. The chatbot is built as an Artificial Intelligence Agent\nsystem that uses Large Language Models along with several specialised tools. It\ncompares new posts with real examples of hate speech and neutral content, uses\na BERT-based classifier to help flag harmful messages, looks up slang and\ninformal language using sources like Urban Dictionary, generates\nchain-of-thought reasoning, and checks platform guidelines to explain and\nsupport its decisions. This combination allows the chatbot not only to detect\nhate speech but to explain why content is considered harmful, grounded in both\nprecedent and policy. Experimental results show that our proposed method\nsurpasses existing state-of-the-art methods, reaching a macro F1 score of 0.91.\nDesigned for moderators, safety teams, and researchers, the tool helps reduce\nonline harms by supporting collaboration between AI and human oversight."}
{"id": "2509.01387", "pdf": "https://arxiv.org/pdf/2509.01387", "abs": "https://arxiv.org/abs/2509.01387", "authors": ["Serwar Basch", "Ilia Kuznetsov", "Tom Hope", "Iryna Gurevych"], "title": "ABCD-LINK: Annotation Bootstrapping for Cross-Document Fine-Grained Links", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Understanding fine-grained relations between documents is crucial for many\napplication domains. However, the study of automated assistance is limited by\nthe lack of efficient methods to create training and evaluation datasets of\ncross-document links. To address this, we introduce a new domain-agnostic\nframework for selecting a best-performing approach and annotating\ncross-document links in a new domain from scratch. We first generate and\nvalidate semi-synthetic datasets of interconnected documents. This data is used\nto perform automatic evaluation, producing a shortlist of best-performing\nlinking approaches. These approaches are then used in an extensive human\nevaluation study, yielding performance estimates on natural text pairs. We\napply our framework in two distinct domains -- peer review and news -- and show\nthat combining retrieval models with LLMs achieves 78\\% link approval from\nhuman raters, more than doubling the precision of strong retrievers alone. Our\nframework enables systematic study of cross-document understanding across\napplication scenarios, and the resulting novel datasets lay foundation for\nnumerous cross-document tasks like media framing and peer review. We make the\ncode, data, and annotation protocols openly available."}
{"id": "2509.01390", "pdf": "https://arxiv.org/pdf/2509.01390", "abs": "https://arxiv.org/abs/2509.01390", "authors": ["Joonyong Park", "Shinnosuke Takamichi", "David M. Chan", "Shunsuke Kando", "Yuki Saito", "Hiroshi Saruwatari"], "title": "Analysing the Language of Neural Audio Codecs", "categories": ["cs.CL", "eess.AS"], "comment": "In Proceedings of 2025 IEEE Automatic Speech Recognition and\n  Understanding Workshop (ASRU 2025)", "summary": "This study presents a comparative analysis of the statistical and linguistic\nproperties of neural audio codecs (NACs). We investigate discrete speech tokens\nproduced by various NAC models, examining their adherence to linguistic\nstatistical laws such as Zipf's law and Heaps' law, as well as their entropy\nand redundancy. To assess how these token-level properties relate to semantic\nand acoustic preservation in synthesized speech, we evaluate intelligibility\nusing error rates of automatic speech recognition, and quality using the UTMOS\nscore. Our results reveal that NAC tokens, particularly 3-grams, exhibit\nlanguage-like statistical patterns. Moreover, these properties, together with\nmeasures of information content, are found to correlate with improved\nperformances in speech recognition and resynthesis tasks. These findings offer\ninsights into the structure of NAC token sequences and inform the design of\nmore effective generative speech models."}
{"id": "2509.01395", "pdf": "https://arxiv.org/pdf/2509.01395", "abs": "https://arxiv.org/abs/2509.01395", "authors": ["KV Aditya Srivatsa", "Kaushal Kumar Maurya", "Ekaterina Kochmar"], "title": "LLMs cannot spot math errors, even when allowed to peek into the solution", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP 2025", "summary": "Large language models (LLMs) demonstrate remarkable performance on math word\nproblems, yet they have been shown to struggle with meta-reasoning tasks such\nas identifying errors in student solutions. In this work, we investigate the\nchallenge of locating the first error step in stepwise solutions using two\nerror reasoning datasets: VtG and PRM800K. Our experiments show that\nstate-of-the-art LLMs struggle to locate the first error step in student\nsolutions even when given access to the reference solution. To that end, we\npropose an approach that generates an intermediate corrected student solution,\naligning more closely with the original student's solution, which helps improve\nperformance."}
{"id": "2509.01412", "pdf": "https://arxiv.org/pdf/2509.01412", "abs": "https://arxiv.org/abs/2509.01412", "authors": ["Kaviraj Pather", "Elena Hadjigeorgiou", "Arben Krasniqi", "Claire Schmit", "Irina Rusu", "Marc Pons", "Kabir Khan"], "title": "Vis-CoT: A Human-in-the-Loop Framework for Interactive Visualization and Intervention in LLM Chain-of-Thought Reasoning", "categories": ["cs.CL", "68T07, 68T50, 68T05", "I.2.7; I.2.6; I.2.8; H.5.2"], "comment": "12 pages, 7 figures", "summary": "Large language models (LLMs) show strong reasoning via chain-of-thought (CoT)\nprompting, but the process is opaque, which makes verification, debugging, and\ncontrol difficult in high-stakes settings. We present Vis-CoT, a\nhuman-in-the-loop framework that converts linear CoT text into an interactive\nreasoning graph. Users can visualize the logical flow, identify flawed steps,\nand intervene by pruning incorrect paths and grafting new, user-defined\npremises. This shifts interaction from passive observation to active\ncollaboration, steering models toward more accurate and trustworthy\nconclusions. Across GSM8K and StrategyQA, Vis-CoT improves final-answer\naccuracy by up to 24 percentage points over non-interactive baselines. A user\nstudy also shows large gains in perceived usability and trust. Vis-CoT points\nto a practical path for more reliable, understandable, and collaborative\nreasoning by combining LLMs with targeted human oversight."}
{"id": "2509.01418", "pdf": "https://arxiv.org/pdf/2509.01418", "abs": "https://arxiv.org/abs/2509.01418", "authors": ["Yang Liu", "Masahiro Kaneko", "Chenhui Chu"], "title": "On the Alignment of Large Language Models with Global Human Opinion", "categories": ["cs.CL"], "comment": "23 pages, 19 figures", "summary": "Today's large language models (LLMs) are capable of supporting multilingual\nscenarios, allowing users to interact with LLMs in their native languages. When\nLLMs respond to subjective questions posed by users, they are expected to align\nwith the views of specific demographic groups or historical periods, shaped by\nthe language in which the user interacts with the model. Existing studies\nmainly focus on researching the opinions represented by LLMs among demographic\ngroups in the United States or a few countries, lacking worldwide country\nsamples and studies on human opinions in different historical periods, as well\nas lacking discussion on using language to steer LLMs. Moreover, they also\noverlook the potential influence of prompt language on the alignment of LLMs'\nopinions. In this study, our goal is to fill these gaps. To this end, we create\nan evaluation framework based on the World Values Survey (WVS) to\nsystematically assess the alignment of LLMs with human opinions across\ndifferent countries, languages, and historical periods around the world. We\nfind that LLMs appropriately or over-align the opinions with only a few\ncountries while under-aligning the opinions with most countries. Furthermore,\nchanging the language of the prompt to match the language used in the\nquestionnaire can effectively steer LLMs to align with the opinions of the\ncorresponding country more effectively than existing steering methods. At the\nsame time, LLMs are more aligned with the opinions of the contemporary\npopulation. To our knowledge, our study is the first comprehensive\ninvestigation of the topic of opinion alignment in LLMs across global,\nlanguage, and temporal dimensions. Our code and data are publicly available at\nhttps://github.com/nlply/global-opinion-alignment."}
{"id": "2509.01455", "pdf": "https://arxiv.org/pdf/2509.01455", "abs": "https://arxiv.org/abs/2509.01455", "authors": ["Markus Oehri", "Giulia Conti", "Kaviraj Pather", "Alexandre Rossi", "Laia Serra", "Adrian Parody", "Rogvi Johannesen", "Aviaja Petersen", "Arben Krasniqi"], "title": "Trusted Uncertainty in Large Language Models: A Unified Framework for Confidence Calibration and Risk-Controlled Refusal", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "10 pages, 5 figures", "summary": "Deployed language models must decide not only what to answer but also when\nnot to answer. We present UniCR, a unified framework that turns heterogeneous\nuncertainty evidence including sequence likelihoods, self-consistency\ndispersion, retrieval compatibility, and tool or verifier feedback into a\ncalibrated probability of correctness and then enforces a user-specified error\nbudget via principled refusal. UniCR learns a lightweight calibration head with\ntemperature scaling and proper scoring, supports API-only models through\nblack-box features, and offers distribution-free guarantees using conformal\nrisk control. For long-form generation, we align confidence with semantic\nfidelity by supervising on atomic factuality scores derived from retrieved\nevidence, reducing confident hallucinations while preserving coverage.\nExperiments on short-form QA, code generation with execution tests, and\nretrieval-augmented long-form QA show consistent improvements in calibration\nmetrics, lower area under the risk-coverage curve, and higher coverage at fixed\nrisk compared to entropy or logit thresholds, post-hoc calibrators, and\nend-to-end selective baselines. Analyses reveal that evidence contradiction,\nsemantic dispersion, and tool inconsistency are the dominant drivers of\nabstention, yielding informative user-facing refusal messages. The result is a\nportable recipe of evidence fusion to calibrated probability to risk-controlled\ndecision that improves trustworthiness without fine-tuning the base model and\nremains valid under distribution shift."}
{"id": "2509.01468", "pdf": "https://arxiv.org/pdf/2509.01468", "abs": "https://arxiv.org/abs/2509.01468", "authors": ["Yuchen Wu", "Liang Ding", "Li Shen", "Dacheng Tao"], "title": "Robust Knowledge Editing via Explicit Reasoning Chains for Distractor-Resilient Multi-Hop QA", "categories": ["cs.CL"], "comment": "EMNLP 2025 Findings", "summary": "Large language models (LLMs) encode vast amounts of world knowledge but\nremain static once trained, making the timely integration of emerging facts\nprohibitively expensive via full retraining. Knowledge-editing techniques have\nthus emerged to inject or overwrite specific facts into LLMs, yet they either\nover-rely on superficial cues or incur complex, iterative pipelines that\ncollapse under noisy, multi-hop conditions. We introduce Reason-KE, an\nend-to-end reasoning-chain-based editing framework that steers a pretrained LLM\nthrough four structured stages-fact acknowledgment, relevance determination,\nselective application, and final reasoning-to filter distractors in a single\npass. Trained on MQuAKE-CF with up to four irrelevant facts, Reason-KE elevates\nQwen2.5-7B's multi-hop QA accuracy to 90.2% while suffering merely a 6.3% drop\nunder heavy distraction and <1% when answers are leaked. Our quantitative\nanalysis confirms Reason-KE's resilience and efficiency, establishing a new\nstate-of-the-art for reliable LLM knowledge updates."}
{"id": "2509.01476", "pdf": "https://arxiv.org/pdf/2509.01476", "abs": "https://arxiv.org/abs/2509.01476", "authors": ["Youchao Zhou", "Heyan Huang", "Yicheng Liu", "Rui Dai", "Xinglin Wang", "Xingchen Zhang", "Shumin Shi", "Yang Deng"], "title": "Do Retrieval Augmented Language Models Know When They Don't Know?", "categories": ["cs.CL", "cs.AI"], "comment": "under review", "summary": "Existing Large Language Models (LLMs) occasionally generate plausible yet\nfactually incorrect responses, known as hallucinations. Researchers are\nprimarily using two approaches to mitigate hallucinations, namely Retrieval\nAugmented Language Models (RALMs) and refusal post-training. However, current\nresearch predominantly emphasizes their individual effectiveness while\noverlooking the evaluation of the refusal capability of RALMs. In this study,\nwe ask the fundamental question: Do RALMs know when they don't know?\nSpecifically, we ask three questions. First, are RALMs well-calibrated\nregarding different internal and external knowledge states? We examine the\ninfluence of various factors. Contrary to expectations, we find that LLMs\nexhibit significant \\textbf{over-refusal} behavior. Then, how does refusal\npost-training affect the over-refusal issue? We investigate the Refusal-aware\nInstruction Tuning and In-Context Fine-tuning methods. Our results show that\nthe over-refusal problem is mitigated by In-context fine-tuning. but magnified\nby R-tuning. However, we also find that the refusal ability may conflict with\nthe quality of the answer. Finally, we develop a simple yet effective refusal\nmethod for refusal post-trained models to improve their overall answer quality\nin terms of refusal and correct answers. Our study provides a more\ncomprehensive understanding of the influence of important factors on RALM\nsystems."}
{"id": "2509.01514", "pdf": "https://arxiv.org/pdf/2509.01514", "abs": "https://arxiv.org/abs/2509.01514", "authors": ["Andreas Ottem"], "title": "MeVe: A Modular System for Memory Verification and Effective Context Control in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 7 figures, held online presentation at NLPA 2025", "summary": "Retrieval-Augmented Generation (RAG) systems typically face constraints\nbecause of their inherent mechanism: a simple top-k semantic search [1]. The\napproach often leads to the incorporation of irrelevant or redundant\ninformation in the context, degrading performance and efficiency [10][11]. This\npaper presents MeVe, a novel modular architecture intended for Memory\nVerification and smart context composition. MeVe rethinks the RAG paradigm by\nproposing a five-phase modular design that distinctly breaks down the retrieval\nand context composition process into distinct, auditable, and independently\ntunable phases: initial retrieval, relevance verification, fallback retrieval,\ncontext prioritization, and token budgeting. This architecture enables\nfine-grained control of what knowledge is made available to an LLM, enabling\ntask-dependent filtering and adaptation. We release a reference implementation\nof MeVe as a proof of concept and evaluate its performance on knowledge-heavy\nQA tasks over a subset of English Wikipedia [22]. Our results demonstrate that\nby actively verifying information before composition, MeVe significantly\nimproves context efficiency, achieving a 57% reduction on the Wikipedia dataset\nand a 75% reduction on the more complex HotpotQA dataset compared to standard\nRAG implementations [25]. This work provides a framework for more scalable and\nreliable LLM applications. By refining and distilling contextual information,\nMeVe offers a path toward better grounding and more accurate factual support\n[16]."}
{"id": "2509.01529", "pdf": "https://arxiv.org/pdf/2509.01529", "abs": "https://arxiv.org/abs/2509.01529", "authors": ["Thomas Compton"], "title": "Service, Solidarity, and Self-Help: A Comparative Topic Modeling Analysis of Community Unionism in the Boot and Shoe Union and Unite Community", "categories": ["cs.CL"], "comment": "10 pages, 7 figures, conference paper", "summary": "This paper presents a comparative analysis of community unionism (CU) in two\ndistinct historical and organizational contexts: the National Boot and Shoe\nUnion (B\\&S) in the 1920s and Unite Community in the 2010s--2020s. Using\nBERTopic for thematic modeling and cTF-IDF weighting, alongside word frequency\nanalysis, the study examines the extent to which each union's discourse aligns\nwith key features of CU -- such as coalition-building, grassroots engagement,\nand action beyond the workplace. The results reveal significant differences in\nthematic focus and discursive coherence. While Unite Community demonstrates\nstronger alignment with outward-facing, social justice-oriented themes, the\nB\\&S corpus emphasizes internal administration, industrial relations, and\nmember services -- reflecting a more traditional, servicing-oriented union\nmodel. The analysis also highlights methodological insights, demonstrating how\nmodern NLP techniques can enhance the study of historical labor archives.\nUltimately, the findings suggest that while both unions engage with\ncommunity-related themes, their underlying models of engagement diverge\nsignificantly, challenging assumptions about the continuity and universality of\ncommunity unionism across time and sector."}
{"id": "2509.01535", "pdf": "https://arxiv.org/pdf/2509.01535", "abs": "https://arxiv.org/abs/2509.01535", "authors": ["Kairong Han", "Wenshuo Zhao", "Ziyu Zhao", "JunJian Ye", "Lujia Pan", "Kun Kuang"], "title": "CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge into Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to EMNLP2025 Main conference", "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains. However, a fundamental question remains: Can LLMs effectively utilize\ncausal knowledge for prediction and generation? Through empirical studies, we\nfind that LLMs trained directly on large-scale data often capture spurious\ncorrelations rather than true causal relationships, leading to suboptimal\nperformance, especially in out-of-distribution (OOD) scenarios. To address this\nchallenge, we propose Causal Attention Tuning (CAT), a novel approach that\ninjects fine-grained causal knowledge into the attention mechanism. We propose\nan automated pipeline that leverages human priors to automatically generate\ntoken-level causal signals and introduce the Re-Attention mechanism to guide\ntraining, helping the model focus on causal structures while mitigating noise\nand biases in attention scores. Experimental results on our proposed Spurious\nToken Game (STG) benchmark and multiple downstream tasks demonstrate that our\napproach effectively leverages causal knowledge for prediction and remains\nrobust in OOD scenarios. Implementation details can be found at\nhttps://github.com/Kairong-Han/CAT."}
{"id": "2509.01560", "pdf": "https://arxiv.org/pdf/2509.01560", "abs": "https://arxiv.org/abs/2509.01560", "authors": ["Seungkyu Lee", "Nalim Kim", "Yohan Jo"], "title": "In-N-Out: A Parameter-Level API Graph Dataset for Tool Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tool agents -- LLM-based systems that interact with external APIs -- offer a\nway to execute real-world tasks. However, as tasks become increasingly complex,\nthese agents struggle to identify and call the correct APIs in the proper\norder. To tackle this problem, we investigate converting API documentation into\na structured API graph that captures API dependencies and leveraging it for\nmulti-tool queries that require compositional API calls. To support this, we\nintroduce In-N-Out, the first expert-annotated dataset of API graphs built from\ntwo real-world API benchmarks and their documentation. Using In-N-Out\nsignificantly improves performance on both tool retrieval and multi-tool query\ngeneration, nearly doubling that of LLMs using documentation alone. Moreover,\ngraphs generated by models fine-tuned on In-N-Out close 90% of this gap,\nshowing that our dataset helps models learn to comprehend API documentation and\nparameter relationships. Our findings highlight the promise of using explicit\nAPI graphs for tool agents and the utility of In-N-Out as a valuable resource.\nWe will release the dataset and code publicly."}
{"id": "2509.01564", "pdf": "https://arxiv.org/pdf/2509.01564", "abs": "https://arxiv.org/abs/2509.01564", "authors": ["Zeguan Xiao", "Diyang Dou", "Boya Xiong", "Yun Chen", "Guanhua Chen"], "title": "Enhancing Uncertainty Estimation in LLMs with Expectation of Aggregated Internal Belief", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success across a wide\nrange of natural language tasks, but often exhibit overconfidence and generate\nplausible yet incorrect answers. This overconfidence, especially in models\nundergone Reinforcement Learning from Human Feedback (RLHF), poses significant\nchallenges for reliable uncertainty estimation and safe deployment. In this\npaper, we propose EAGLE (Expectation of AGgregated internaL bEief), a novel\nself-evaluation-based calibration method that leverages the internal hidden\nstates of LLMs to derive more accurate confidence scores. Instead of relying on\nthe model's final output, our approach extracts internal beliefs from multiple\nintermediate layers during self-evaluation. By aggregating these layer-wise\nbeliefs and calculating the expectation over the resulting confidence score\ndistribution, EAGLE produces a refined confidence score that more faithfully\nreflects the model's internal certainty. Extensive experiments on diverse\ndatasets and LLMs demonstrate that EAGLE significantly improves calibration\nperformance over existing baselines. We also provide an in-depth analysis of\nEAGLE, including a layer-wise examination of uncertainty patterns, a study of\nthe impact of self-evaluation prompts, and an analysis of the effect of\nself-evaluation score range."}
{"id": "2509.01606", "pdf": "https://arxiv.org/pdf/2509.01606", "abs": "https://arxiv.org/abs/2509.01606", "authors": ["Vivi Nastase", "Paola Merlo"], "title": "Testing the assumptions about the geometry of sentence embedding spaces: the cosine measure need not apply", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": "25 pages, 6 tables, 10 figures", "summary": "Transformer models learn to encode and decode an input text, and produce\ncontextual token embeddings as a side-effect. The mapping from language into\nthe embedding space maps words expressing similar concepts onto points that are\nclose in the space. In practice, the reverse implication is also assumed: words\ncorresponding to close points in this space are similar or related, those that\nare further are not.\n  Does closeness in the embedding space extend to shared properties for\nsentence embeddings? We present an investigation of sentence embeddings and\nshow that the geometry of their embedding space is not predictive of their\nrelative performances on a variety of tasks.\n  We compute sentence embeddings in three ways: as averaged token embeddings,\nas the embedding of the special [CLS] token, and as the embedding of a random\ntoken from the sentence. We explore whether there is a correlation between the\ndistance between sentence embedding variations and their performance on\nlinguistic tasks, and whether despite their distances, they do encode the same\ninformation in the same manner.\n  The results show that the cosine similarity -- which treats dimensions\nshallowly -- captures (shallow) commonalities or differences between sentence\nembeddings, which are not predictive of their performance on specific tasks.\nLinguistic information is rather encoded in weighted combinations of different\ndimensions, which are not reflected in the geometry of the sentence embedding\nspace."}
{"id": "2509.01620", "pdf": "https://arxiv.org/pdf/2509.01620", "abs": "https://arxiv.org/abs/2509.01620", "authors": ["Shanshan Wang", "Junchao Wu", "Fengying Ye", "Jingming Yao", "Lidia S. Chao", "Derek F. Wong"], "title": "Benchmarking the Detection of LLMs-Generated Modern Chinese Poetry", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by EMNLP 2025", "summary": "The rapid development of advanced large language models (LLMs) has made\nAI-generated text indistinguishable from human-written text. Previous work on\ndetecting AI-generated text has made effective progress, but has not involved\nmodern Chinese poetry. Due to the distinctive characteristics of modern Chinese\npoetry, it is difficult to identify whether a poem originated from humans or\nAI. The proliferation of AI-generated modern Chinese poetry has significantly\ndisrupted the poetry ecosystem. Based on the urgency of identifying\nAI-generated poetry in the real Chinese world, this paper proposes a novel\nbenchmark for detecting LLMs-generated modern Chinese poetry. We first\nconstruct a high-quality dataset, which includes both 800 poems written by six\nprofessional poets and 41,600 poems generated by four mainstream LLMs.\nSubsequently, we conduct systematic performance assessments of six detectors on\nthis dataset. Experimental results demonstrate that current detectors cannot be\nused as reliable tools to detect modern Chinese poems generated by LLMs. The\nmost difficult poetic features to detect are intrinsic qualities, especially\nstyle. The detection results verify the effectiveness and necessity of our\nproposed benchmark. Our work lays a foundation for future detection of\nAI-generated poetry."}
{"id": "2509.01640", "pdf": "https://arxiv.org/pdf/2509.01640", "abs": "https://arxiv.org/abs/2509.01640", "authors": ["Hind Aljuaid", "Areej Alhothali", "Ohoud Al-Zamzami", "Hussein Assalahi"], "title": "TransGAT: Transformer-Based Graph Neural Networks for Multi-Dimensional Automated Essay Scoring", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Essay writing is a critical component of student assessment, yet manual\nscoring is labor-intensive and inconsistent. Automated Essay Scoring (AES)\noffers a promising alternative, but current approaches face limitations. Recent\nstudies have incorporated Graph Neural Networks (GNNs) into AES using static\nword embeddings that fail to capture contextual meaning, especially for\npolysemous words. Additionally, many methods rely on holistic scoring,\noverlooking specific writing aspects such as grammar, vocabulary, and cohesion.\nTo address these challenges, this study proposes TransGAT, a novel approach\nthat integrates fine-tuned Transformer models with GNNs for analytic scoring.\nTransGAT combines the contextual understanding of Transformers with the\nrelational modeling strength of Graph Attention Networks (GAT). It performs\ntwo-stream predictions by pairing each fine-tuned Transformer (BERT, RoBERTa,\nand DeBERTaV3) with a separate GAT. In each pair, the first stream generates\nessay-level predictions, while the second applies GAT to Transformer token\nembeddings, with edges constructed from syntactic dependencies. The model then\nfuses predictions from both streams to produce the final analytic score.\nExperiments on the ELLIPSE dataset show that TransGAT outperforms baseline\nmodels, achieving an average Quadratic Weighted Kappa (QWK) of 0.854 across all\nanalytic scoring dimensions. These findings highlight the potential of TransGAT\nto advance AES systems."}
{"id": "2509.01654", "pdf": "https://arxiv.org/pdf/2509.01654", "abs": "https://arxiv.org/abs/2509.01654", "authors": ["Dominic Plein"], "title": "Parallel Needleman-Wunsch on CUDA to measure word similarity based on phonetic transcriptions", "categories": ["cs.CL"], "comment": "11 pages, 12 figures, accompanied by a YouTube video\n  (https://youtu.be/xbcpnItE3_4) and a GitHub repository\n  (https://github.com/Splines/phonetics-graph/)", "summary": "We present a method to calculate the similarity between words based on their\nphonetic transcription (their pronunciation) using the Needleman-Wunsch\nalgorithm. We implement this algorithm in Rust and parallelize it on both CPU\nand GPU to handle large datasets efficiently. The GPU implementation leverages\nCUDA and the cudarc Rust library to achieve significant performance\nimprovements. We validate our approach by constructing a fully-connected graph\nwhere nodes represent words and edges have weights according to the similarity\nbetween the words. This graph is then analyzed using clustering algorithms to\nidentify groups of phonetically similar words. Our results demonstrate the\nfeasibility and effectiveness of the proposed method in analyzing the phonetic\nstructure of languages. It might be easily expanded to other languages."}
{"id": "2509.01660", "pdf": "https://arxiv.org/pdf/2509.01660", "abs": "https://arxiv.org/abs/2509.01660", "authors": ["Zhengjia Wang", "Qiang Sheng", "Danding Wang", "Beizhe Hu", "Juan Cao"], "title": "Bridging Thoughts and Words: Graph-Based Intent-Semantic Joint Learning for Fake News Detection", "categories": ["cs.CL"], "comment": "Accepted to CIKM'25", "summary": "Fake news detection is an important and challenging task for defending online\ninformation integrity. Existing state-of-the-art approaches typically extract\nnews semantic clues, such as writing patterns that include emotional words,\nstylistic features, etc. However, detectors tuned solely to such semantic clues\ncan easily fall into surface detection patterns, which can shift rapidly in\ndynamic environments, leading to limited performance in the evolving news\nlandscape. To address this issue, this paper investigates a novel perspective\nby incorporating news intent into fake news detection, bridging intents and\nsemantics together. The core insight is that by considering news intents, one\ncan deeply understand the inherent thoughts behind news deception, rather than\nthe surface patterns within words alone. To achieve this goal, we propose\nGraph-based Intent-Semantic Joint Modeling (InSide) for fake news detection,\nwhich models deception clues from both semantic and intent signals via\ngraph-based joint learning. Specifically, InSide reformulates news semantic and\nintent signals into heterogeneous graph structures, enabling long-range context\ninteraction through entity guidance and capturing both holistic and\nimplementation-level intent via coarse-to-fine intent modeling. To achieve\nbetter alignment between semantics and intents, we further develop a dynamic\npathway-based graph alignment strategy for effective message passing and\naggregation across these signals by establishing a common space. Extensive\nexperiments on four benchmark datasets demonstrate the superiority of the\nproposed InSide compared to state-of-the-art methods."}
{"id": "2509.01772", "pdf": "https://arxiv.org/pdf/2509.01772", "abs": "https://arxiv.org/abs/2509.01772", "authors": ["Abdelkrime Aries"], "title": "chDzDT: Word-level morphology-aware language model for Algerian social media text", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Pre-trained language models (PLMs) have substantially advanced natural\nlanguage processing by providing context-sensitive text representations.\nHowever, the Algerian dialect remains under-represented, with few dedicated\nmodels available. Processing this dialect is challenging due to its complex\nmorphology, frequent code-switching, multiple scripts, and strong lexical\ninfluences from other languages. These characteristics complicate tokenization\nand reduce the effectiveness of conventional word- or subword-level approaches.\n  To address this gap, we introduce chDzDT, a character-level pre-trained\nlanguage model tailored for Algerian morphology. Unlike conventional PLMs that\nrely on token sequences, chDzDT is trained on isolated words. This design\nallows the model to encode morphological patterns robustly, without depending\non token boundaries or standardized orthography. The training corpus draws from\ndiverse sources, including YouTube comments, French, English, and Berber\nWikipedia, as well as the Tatoeba project. It covers multiple scripts and\nlinguistic varieties, resulting in a substantial pre-training workload.\n  Our contributions are threefold: (i) a detailed morphological analysis of\nAlgerian dialect using YouTube comments; (ii) the construction of a\nmultilingual Algerian lexicon dataset; and (iii) the development and extensive\nevaluation of a character-level PLM as a morphology-focused encoder for\ndownstream tasks. The proposed approach demonstrates the potential of\ncharacter-level modeling for morphologically rich, low-resource dialects and\nlays a foundation for more inclusive and adaptable NLP systems."}
{"id": "2509.01790", "pdf": "https://arxiv.org/pdf/2509.01790", "abs": "https://arxiv.org/abs/2509.01790", "authors": ["Andong Hua", "Kenan Tang", "Chenhe Gu", "Jindong Gu", "Eric Wong", "Yao Qin"], "title": "Flaw or Artifact? Rethinking Prompt Sensitivity in Evaluating LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to EMNLP 2025 Main Conference", "summary": "Prompt sensitivity, referring to the phenomenon where paraphrasing (i.e.,\nrepeating something written or spoken using different words) leads to\nsignificant changes in large language model (LLM) performance, has been widely\naccepted as a core limitation of LLMs. In this work, we revisit this issue and\nask: Is the widely reported high prompt sensitivity truly an inherent weakness\nof LLMs, or is it largely an artifact of evaluation processes? To answer this\nquestion, we systematically evaluate 7 LLMs (e.g., GPT and Gemini family)\nacross 6 benchmarks, including both multiple-choice and open-ended tasks on 12\ndiverse prompt templates. We find that much of the prompt sensitivity stems\nfrom heuristic evaluation methods, including log-likelihood scoring and rigid\nanswer matching, which often overlook semantically correct responses expressed\nthrough alternative phrasings, such as synonyms or paraphrases. When we adopt\nLLM-as-a-Judge evaluations, we observe a substantial reduction in performance\nvariance and a consistently higher correlation in model rankings across\nprompts. Our findings suggest that modern LLMs are more robust to prompt\ntemplates than previously believed, and that prompt sensitivity may be more an\nartifact of evaluation than a flaw in the models."}
{"id": "2509.01814", "pdf": "https://arxiv.org/pdf/2509.01814", "abs": "https://arxiv.org/abs/2509.01814", "authors": ["Shreyas Tirumala", "Nishant Jain", "Danny D. Leybzon", "Trent D. Buskirk"], "title": "Mic Drop or Data Flop? Evaluating the Fitness for Purpose of AI Voice Interviewers for Data Collection within Quantitative & Qualitative Research Contexts", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Transformer-based Large Language Models (LLMs) have paved the way for \"AI\ninterviewers\" that can administer voice-based surveys with respondents in\nreal-time. This position paper reviews emerging evidence to understand when\nsuch AI interviewing systems are fit for purpose for collecting data within\nquantitative and qualitative research contexts. We evaluate the capabilities of\nAI interviewers as well as current Interactive Voice Response (IVR) systems\nacross two dimensions: input/output performance (i.e., speech recognition,\nanswer recording, emotion handling) and verbal reasoning (i.e., ability to\nprobe, clarify, and handle branching logic). Field studies suggest that AI\ninterviewers already exceed IVR capabilities for both quantitative and\nqualitative data collection, but real-time transcription error rates, limited\nemotion detection abilities, and uneven follow-up quality indicate that the\nutility, use and adoption of current AI interviewer technology may be\ncontext-dependent for qualitative data collection efforts."}
{"id": "2509.01885", "pdf": "https://arxiv.org/pdf/2509.01885", "abs": "https://arxiv.org/abs/2509.01885", "authors": ["Zhimeng Luo", "Abhibha Gupta", "Adam Frisch", "Daqing He"], "title": "Extracting OPQRST in Electronic Health Records using Large Language Models with Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The extraction of critical patient information from Electronic Health Records\n(EHRs) poses significant challenges due to the complexity and unstructured\nnature of the data. Traditional machine learning approaches often fail to\ncapture pertinent details efficiently, making it difficult for clinicians to\nutilize these tools effectively in patient care. This paper introduces a novel\napproach to extracting the OPQRST assessment from EHRs by leveraging the\ncapabilities of Large Language Models (LLMs). We propose to reframe the task\nfrom sequence labeling to text generation, enabling the models to provide\nreasoning steps that mimic a physician's cognitive processes. This approach\nenhances interpretability and adapts to the limited availability of labeled\ndata in healthcare settings. Furthermore, we address the challenge of\nevaluating the accuracy of machine-generated text in clinical contexts by\nproposing a modification to traditional Named Entity Recognition (NER) metrics.\nThis includes the integration of semantic similarity measures, such as the BERT\nScore, to assess the alignment between generated text and the clinical intent\nof the original records. Our contributions demonstrate a significant\nadvancement in the use of AI in healthcare, offering a scalable solution that\nimproves the accuracy and usability of information extraction from EHRs,\nthereby aiding clinicians in making more informed decisions and enhancing\npatient care outcomes."}
{"id": "2509.01899", "pdf": "https://arxiv.org/pdf/2509.01899", "abs": "https://arxiv.org/abs/2509.01899", "authors": ["Zhimeng Luo", "Zhendong Wang", "Rui Meng", "Diyang Xue", "Adam Frisch", "Daqing He"], "title": "Weakly Supervised Medical Entity Extraction and Linking for Chief Complaints", "categories": ["cs.CL"], "comment": null, "summary": "A Chief complaint (CC) is the reason for the medical visit as stated in the\npatient's own words. It helps medical professionals to quickly understand a\npatient's situation, and also serves as a short summary for medical text\nmining. However, chief complaint records often take a variety of entering\nmethods, resulting in a wide variation of medical notations, which makes it\ndifficult to standardize across different medical institutions for record\nkeeping or text mining. In this study, we propose a weakly supervised method to\nautomatically extract and link entities in chief complaints in the absence of\nhuman annotation. We first adopt a split-and-match algorithm to produce weak\nannotations, including entity mention spans and class labels, on 1.2 million\nreal-world de-identified and IRB approved chief complaint records. Then we\ntrain a BERT-based model with generated weak labels to locate entity mentions\nin chief complaint text and link them to a pre-defined ontology. We conducted\nextensive experiments, and the results showed that our Weakly Supervised Entity\nExtraction and Linking (\\ours) method produced superior performance over\nprevious methods without any human annotation."}
{"id": "2509.01962", "pdf": "https://arxiv.org/pdf/2509.01962", "abs": "https://arxiv.org/abs/2509.01962", "authors": ["Sachin Pawar", "Manoj Apte", "Girish K. Palshikar", "Basit Ali", "Nitin Ramrakhiyani"], "title": "DRAssist: Dispute Resolution Assistance using Large Language Models", "categories": ["cs.CL"], "comment": "Accepted at the 20th International Conference on Artificial\n  Intelligence and Law (ICAIL 2025)", "summary": "Disputes between two parties occur in almost all domains such as taxation,\ninsurance, banking, healthcare, etc. The disputes are generally resolved in a\nspecific forum (e.g., consumer court) where facts are presented, points of\ndisagreement are discussed, arguments as well as specific demands of the\nparties are heard, and finally a human judge resolves the dispute by often\nfavouring one of the two parties. In this paper, we explore the use of large\nlanguage models (LLMs) as assistants for the human judge to resolve such\ndisputes, as part of our DRAssist system. We focus on disputes from two\nspecific domains -- automobile insurance and domain name disputes. DRAssist\nidentifies certain key structural elements (e.g., facts, aspects or\ndisagreement, arguments) of the disputes and summarizes the unstructured\ndispute descriptions to produce a structured summary for each dispute. We then\nexplore multiple prompting strategies with multiple LLMs for their ability to\nassist in resolving the disputes in these domains. In DRAssist, these LLMs are\nprompted to produce the resolution output at three different levels -- (i)\nidentifying an overall stronger party in a dispute, (ii) decide whether each\nspecific demand of each contesting party can be accepted or not, (iii) evaluate\nwhether each argument by each contesting party is strong or weak. We evaluate\nthe performance of LLMs on all these tasks by comparing them with relevant\nbaselines using suitable evaluation metrics."}
{"id": "2509.02033", "pdf": "https://arxiv.org/pdf/2509.02033", "abs": "https://arxiv.org/abs/2509.02033", "authors": ["Chao Xue", "Ziyuan Gao"], "title": "StructCoh: Structured Contrastive Learning for Context-Aware Text Semantic Matching", "categories": ["cs.CL"], "comment": "Accepted by PRICAI 2025", "summary": "Text semantic matching requires nuanced understanding of both structural\nrelationships and fine-grained semantic distinctions. While pre-trained\nlanguage models excel at capturing token-level interactions, they often\noverlook hierarchical structural patterns and struggle with subtle semantic\ndiscrimination. In this paper, we proposed StructCoh, a graph-enhanced\ncontrastive learning framework that synergistically combines structural\nreasoning with representation space optimization. Our approach features two key\ninnovations: (1) A dual-graph encoder constructs semantic graphs via dependency\nparsing and topic modeling, then employs graph isomorphism networks to\npropagate structural features across syntactic dependencies and cross-document\nconcept nodes. (2) A hierarchical contrastive objective enforces consistency at\nmultiple granularities: node-level contrastive regularization preserves core\nsemantic units, while graph-aware contrastive learning aligns inter-document\nstructural semantics through both explicit and implicit negative sampling\nstrategies. Experiments on three legal document matching benchmarks and\nacademic plagiarism detection datasets demonstrate significant improvements\nover state-of-the-art methods. Notably, StructCoh achieves 86.7% F1-score\n(+6.2% absolute gain) on legal statute matching by effectively identifying\nargument structure similarities."}
{"id": "2509.02036", "pdf": "https://arxiv.org/pdf/2509.02036", "abs": "https://arxiv.org/abs/2509.02036", "authors": ["Hexian Zhang", "Xinyu Yan", "Yanqi Yang", "Lijian Jin", "Ping Yang", "Junwen Wang"], "title": "DeepSeek performs better than other Large Language Models in Dental Cases", "categories": ["cs.CL", "cs.AI"], "comment": "Abstract word count: 171; Total word count: 3130; Total number of\n  tables: 2; Total number of figures: 3; Number of references: 32", "summary": "Large language models (LLMs) hold transformative potential in healthcare, yet\ntheir capacity to interpret longitudinal patient narratives remains\ninadequately explored. Dentistry, with its rich repository of structured\nclinical data, presents a unique opportunity to rigorously assess LLMs'\nreasoning abilities. While several commercial LLMs already exist, DeepSeek, a\nmodel that gained significant attention earlier this year, has also joined the\ncompetition. This study evaluated four state-of-the-art LLMs (GPT-4o, Gemini\n2.0 Flash, Copilot, and DeepSeek V3) on their ability to analyze longitudinal\ndental case vignettes through open-ended clinical tasks. Using 34 standardized\nlongitudinal periodontal cases (comprising 258 question-answer pairs), we\nassessed model performance via automated metrics and blinded evaluations by\nlicensed dentists. DeepSeek emerged as the top performer, demonstrating\nsuperior faithfulness (median score = 0.528 vs. 0.367-0.457) and higher expert\nratings (median = 4.5/5 vs. 4.0/5), without significantly compromising\nreadability. Our study positions DeepSeek as the leading LLM for case analysis,\nendorses its integration as an adjunct tool in both medical education and\nresearch, and highlights its potential as a domain-specific agent."}
{"id": "2509.02038", "pdf": "https://arxiv.org/pdf/2509.02038", "abs": "https://arxiv.org/abs/2509.02038", "authors": ["Bashar Talafha", "Hawau Olamide Toyin", "Peter Sullivan", "AbdelRahim Elmadany", "Abdurrahman Juma", "Amirbek Djanibekov", "Chiyu Zhang", "Hamad Alshehhi", "Hanan Aldarmaki", "Mustafa Jarrar", "Nizar Habash", "Muhammad Abdul-Mageed"], "title": "NADI 2025: The First Multidialectal Arabic Speech Processing Shared Task", "categories": ["cs.CL", "cs.SD"], "comment": null, "summary": "We present the findings of the sixth Nuanced Arabic Dialect Identification\n(NADI 2025) Shared Task, which focused on Arabic speech dialect processing\nacross three subtasks: spoken dialect identification (Subtask 1), speech\nrecognition (Subtask 2), and diacritic restoration for spoken dialects (Subtask\n3). A total of 44 teams registered, and during the testing phase, 100 valid\nsubmissions were received from eight unique teams. The distribution was as\nfollows: 34 submissions for Subtask 1 \"five teams{\\ae}, 47 submissions for\nSubtask 2 \"six teams\", and 19 submissions for Subtask 3 \"two teams\". The\nbest-performing systems achieved 79.8% accuracy on Subtask 1, 35.68/12.20\nWER/CER (overall average) on Subtask 2, and 55/13 WER/CER on Subtask 3. These\nresults highlight the ongoing challenges of Arabic dialect speech processing,\nparticularly in dialect identification, recognition, and diacritic restoration.\nWe also summarize the methods adopted by participating teams and briefly\noutline directions for future editions of NADI."}
{"id": "2509.02040", "pdf": "https://arxiv.org/pdf/2509.02040", "abs": "https://arxiv.org/abs/2509.02040", "authors": ["Guangzeng Han", "Weisi Liu", "Xiaolei Huang"], "title": "Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm Simulators for Conditional Synthetic Data Generation", "categories": ["cs.CL"], "comment": "Accepted to EMNLP2025 Findings", "summary": "Large Language Models (LLMs) excel at generating synthetic data, but ensuring\nits quality and diversity remains challenging. We propose Genetic Prompt, a\nnovel framework that combines genetic algorithms with LLMs to augment synthetic\ndata generation. Our approach treats semantic text attributes as gene sequences\nand leverages the LLM to simulate crossover and mutation operations. This\ngenetic process enhances data quality and diversity by creating novel attribute\ncombinations, yielding synthetic distributions closer to real-world data. To\noptimize parent selection, we also integrate an active learning scheme that\nexpands the offspring search space. Our experiments on multiple NLP tasks\nreveal several key findings: Genetic Prompt not only significantly outperforms\nstate-of-the-art baselines but also shows robust performance across various\ngenerator model sizes and scales. Moreover, we demonstrate that fusing our\nsynthetic data with the original training set significantly boosts downstream\nmodel performance, particularly for class-imbalanced scenarios. Our findings\nvalidate that Genetic Prompt is an effective method for producing high-quality\nsynthetic data for a wide range of NLP applications."}
{"id": "2509.02075", "pdf": "https://arxiv.org/pdf/2509.02075", "abs": "https://arxiv.org/abs/2509.02075", "authors": ["Elisabetta Rocchetti", "Alfio Ferrara"], "title": "How Instruction-Tuning Imparts Length Control: A Cross-Lingual Mechanistic Analysis", "categories": ["cs.CL", "cs.AI", "I.2.7"], "comment": null, "summary": "Adhering to explicit length constraints, such as generating text with a\nprecise word count, remains a significant challenge for Large Language Models\n(LLMs). This study aims at investigating the differences between foundation\nmodels and their instruction-tuned counterparts, on length-controlled text\ngeneration in English and Italian. We analyze both performance and internal\ncomponent contributions using Cumulative Weighted Attribution, a metric derived\nfrom Direct Logit Attribution. Our findings reveal that instruction-tuning\nsubstantially improves length control, primarily by specializing components in\ndeeper model layers. Specifically, attention heads in later layers of IT models\nshow increasingly positive contributions, particularly in English. In Italian,\nwhile attention contributions are more attenuated, final-layer MLPs exhibit a\nstronger positive role, suggesting a compensatory mechanism. These results\nindicate that instruction-tuning reconfigures later layers for task adherence,\nwith component-level strategies potentially adapting to linguistic context."}
{"id": "2509.02093", "pdf": "https://arxiv.org/pdf/2509.02093", "abs": "https://arxiv.org/abs/2509.02093", "authors": ["Juhyeon Lee", "Wonduk Seo", "Hyunjin An", "Seunghyun Lee", "Yi Bu"], "title": "Better by Comparison: Retrieval-Augmented Contrastive Reasoning for Automatic Prompt Optimization", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Preprint", "summary": "Automatic prompt optimization has recently emerged as a strategy for\nimproving the quality of prompts used in Large Language Models (LLMs), with the\ngoal of generating more accurate and useful responses. However, most prior work\nfocuses on direct prompt refinement or model fine-tuning, overlooking the\npotential of leveraging LLMs' inherent reasoning capability to learn from\ncontrasting examples. In this paper, we present Contrastive Reasoning Prompt\nOptimization (CRPO), a novel framework that formulates prompt optimization as a\nretrieval augmented reasoning process. Our approach retrieves top k reference\nprompts from the HelpSteer2 dataset, an open-source collection annotated for\nhelpfulness, correctness, coherence, complexity, and verbosity, and constructs\ntwo complementary optimization paradigms: (1) tiered contrastive reasoning,\nwhere the LLM compares high, medium, and low quality prompts to refine its own\ngeneration through reflective reasoning, and (2) multi-metric contrastive\nreasoning, where the LLM analyzes the best prompts along each evaluation\ndimension and integrates their strengths into an optimized prompt. By\nexplicitly contrasting high and low quality exemplars, CRPO enables the model\nto deduce why certain prompts succeed while others fail, thereby achieving more\nrobust and interpretable optimization. Experimental results on the HelpSteer2\nbenchmark demonstrate that CRPO significantly outperforms baselines. Our\nfindings highlight the promise of contrastive, retrieval-augmented reasoning\nfor advancing automatic prompt optimization."}
{"id": "2509.02097", "pdf": "https://arxiv.org/pdf/2509.02097", "abs": "https://arxiv.org/abs/2509.02097", "authors": ["Zhichao Shi", "Xuhui Jiang", "Chengjin Xu", "Cangli Yao", "Zhenxin Huang", "Shengjie Ma", "Yinghan Shen", "Yuanzhuo Wang"], "title": "JudgeAgent: Dynamically Evaluate LLMs with Agent-as-Interviewer", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating the capabilities of large language models (LLMs) is an essential\nstep to ensure the successful application of LLMs across various domains. The\ncurrent evaluation of LLMs is based on a paradigm that involves querying them\nwith predefined question sets and assessing their outputs. This paradigm offers\ncontrollable processes and simplicity, but faces challenges such as limited\ninteraction with targets, insufficient difficulty control, and difficulties in\nverifying the validity of evaluation results, making it hard to precisely\ndetermine the knowledge and capability boundaries of target models. To address\nthese challenges, we propose JudgeAgent, a knowledge-target adaptive dynamic\nevaluation framework based on a new interviewer-style evaluation paradigm.\nJudgeAgent employs a comprehensive evaluation approach consisting of benchmark\ngrading, interactive extension, and evaluation feedback. It utilizes\nknowledge-driven data synthesis and target-adaptive difficulty adjustment\nmethods to conduct extended testing, providing accurate and effective\nevaluation results. We also introduce a novel insight into validating\nevaluation methods, demonstrating the effectiveness of JudgeAgent and its\ndynamic evaluation paradigm through extensive experiments."}
{"id": "2509.02123", "pdf": "https://arxiv.org/pdf/2509.02123", "abs": "https://arxiv.org/abs/2509.02123", "authors": ["Wang Chen", "Guanqiang Qi", "Weikang Li", "Yang Li"], "title": "CMRAG: Co-modality-based document retrieval and visual question answering", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a core paradigm in document\nquestion answering tasks. However, existing methods have limitations when\ndealing with multimodal documents: one category of methods relies on layout\nanalysis and text extraction, which can only utilize explicit text information\nand struggle to capture images or unstructured content; the other category\ntreats document segmentation as visual input and directly passes it to visual\nlanguage models (VLMs) for processing, yet it ignores the semantic advantages\nof text, leading to suboptimal generation results. This paper proposes\nco-modality-based RAG (CMRAG), which can simultaneously leverage text and\nimages for efficient retrieval and generation. Specifically, we first perform\nstructured parsing on documents to obtain co-modality representations of text\nsegments and image regions. Subsequently, in response to user queries, we\nretrieve candidate evidence from text and image channels, respectively, and\naggregate the results at the cross-modal retrieval level. Finally, we prompt\nthe VLM to generate the final response based on the co-modality retrieval\nresults. Experiments demonstrate that our method significantly outperforms\npure-vision-based RAG in visual document question answering tasks. The findings\nof this paper show that integrating co-modality information into the RAG\nframework in a unified manner is an effective approach to improving the\nperformance of complex document visual question-answering (VQA) systems."}
{"id": "2509.02133", "pdf": "https://arxiv.org/pdf/2509.02133", "abs": "https://arxiv.org/abs/2509.02133", "authors": ["Snehasis Mukhopadhyay", "Aryan Kasat", "Shivam Dubey", "Rahul Karthikeyan", "Dhruv Sood", "Vinija Jain", "Aman Chadha", "Amitava Das"], "title": "AMBEDKAR-A Multi-level Bias Elimination through a Decoding Approach with Knowledge Augmentation for Robust Constitutional Alignment of Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) can inadvertently reflect societal biases\npresent in their training data, leading to harmful or prejudiced outputs. In\nthe Indian context, our empirical evaluations across a suite of models reveal\nthat biases around caste and religion are particularly salient. Yet, most\nexisting mitigation strategies are Western-centric and fail to address these\nlocal nuances. We propose AMBEDKAR, a framework inspired by the egalitarian\nvision of Dr B. R. Ambedkar, architect of the Indian Constitution, to guide LLM\noutputs toward fairness, neutrality, and inclusion in line with Articles 14 to\n17. Our approach introduces a Constitution-Aware Decoding Layer, guided by the\nAI Constitution of India and applied only at inference time, without any\nparameter updates to the base model. We incorporate a speculative decoding\nalgorithm that proactively reduces casteist and communal bias during\ngeneration. This mitigation layer operates directly within the decoding\nprocess, avoiding changes to model internals and lowering the computational and\ninfrastructural costs associated with retraining. We reinterpret speculative\ndecoding not merely as an efficiency tool but as a mechanism for fairness. In\nthis framework, a Small Language Model (SLM) acts as a potentially biased\ngenerator, while a constitutionally guided Large Language Model (LLM) serves as\nthe verifier. Rather than accelerating generation, the LLM enforces bias-robust\ntrajectories in the SLM outputs. This inversion of roles gives rise to a\nfairness-by-speculation paradigm. Our approach yields an absolute reduction of\nbias up to 26.41 percent compared to baseline. Our source code, datasets, and\nresults are available at https://anonymous.4open.science/r/AMBEDKAR-983B/"}
{"id": "2509.02160", "pdf": "https://arxiv.org/pdf/2509.02160", "abs": "https://arxiv.org/abs/2509.02160", "authors": ["David Demitri Africa", "Suchir Salhan", "Yuval Weiss", "Paula Buttery", "Richard Diehl Martinez"], "title": "Meta-Pretraining for Zero-Shot Cross-Lingual Named Entity Recognition in Low-Resource Philippine Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Named-entity recognition (NER) in low-resource languages is usually tackled\nby finetuning very large multilingual LMs, an option that is often infeasible\nin memory- or latency-constrained settings. We ask whether small decoder LMs\ncan be pretrained so that they adapt quickly and transfer zero-shot to\nlanguages unseen during pretraining. To this end we replace part of the\nautoregressive objective with first-order model-agnostic meta-learning (MAML).\nTagalog and Cebuano are typologically similar yet structurally different in\ntheir actor/non-actor voice systems, and hence serve as a challenging test-bed.\nAcross four model sizes (11 M - 570 M) MAML lifts zero-shot micro-F1 by 2-6 pp\nunder head-only tuning and 1-3 pp after full tuning, while cutting convergence\ntime by up to 8%. Gains are largest for single-token person entities that\nco-occur with Tagalog case particles si/ni, highlighting the importance of\nsurface anchors."}
{"id": "2509.02170", "pdf": "https://arxiv.org/pdf/2509.02170", "abs": "https://arxiv.org/abs/2509.02170", "authors": ["Kyeongman Park", "Nakyeong Yang", "Kyomin Jung"], "title": "Avoidance Decoding for Diverse Multi-Branch Story Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) often generate repetitive and monotonous\noutputs, especially in tasks like story generation, due to limited creative\ndiversity when given the same input prompt. To address this challenge, we\npropose a novel decoding strategy, Avoidance Decoding, that modifies token\nlogits by penalizing similarity to previously generated outputs, thereby\nencouraging more diverse multi-branch stories. This penalty adaptively balances\ntwo similarity measures: (1) Concept-level Similarity Penalty, which is\nprioritized in early stages to diversify initial story concepts, and (2)\nNarrative-level Similarity Penalty, which is increasingly emphasized later to\nensure natural yet diverse plot development. Notably, our method achieves up to\n2.6 times higher output diversity and reduces repetition by an average of 30%\ncompared to strong baselines, while effectively mitigating text degeneration.\nFurthermore, we reveal that our method activates a broader range of neurons,\ndemonstrating that it leverages the model's intrinsic creativity."}
{"id": "2509.02198", "pdf": "https://arxiv.org/pdf/2509.02198", "abs": "https://arxiv.org/abs/2509.02198", "authors": ["Anum Afzal", "Juraj Vladika", "Florian Matthes"], "title": "FActBench: A Benchmark for Fine-grained Automatic Evaluation of LLM-Generated Text in the Medical Domain", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models tend to struggle when dealing with specialized domains.\nWhile all aspects of evaluation hold importance, factuality is the most\ncritical one. Similarly, reliable fact-checking tools and data sources are\nessential for hallucination mitigation. We address these issues by providing a\ncomprehensive Fact-checking Benchmark FActBench covering four generation tasks\nand six state-of-the-art Large Language Models (LLMs) for the Medical domain.\nWe use two state-of-the-art Fact-checking techniques: Chain-of-Thought (CoT)\nPrompting and Natural Language Inference (NLI). Our experiments show that the\nfact-checking scores acquired through the Unanimous Voting of both techniques\ncorrelate best with Domain Expert Evaluation."}
{"id": "2509.02225", "pdf": "https://arxiv.org/pdf/2509.02225", "abs": "https://arxiv.org/abs/2509.02225", "authors": ["Jaime Collado-Montañez", "L. Alfonso Ureña-López", "Arturo Montejo-Ráez"], "title": "Towards Fundamental Language Models: Does Linguistic Competence Scale with Model Size?", "categories": ["cs.CL", "I.2.7; I.7"], "comment": "13 pages, 2 figures", "summary": "Large Language Models offer impressive language capabilities but suffer from\nwell-known limitations, including hallucinations, biases, privacy concerns, and\nhigh computational costs. These issues are largely driven by the combination of\nlinguistic competence and factual memorization within a single monolithic\nmodel. This paper introduces and empirically supports the Fundamental Language\nModel (FLM) paradigm, which advocates for smaller, linguistically competent\nmodels that offload factual retrieval to external tools. We evaluate models\nranging from 135M to 32B parameters across three dimensions: linguistic\ncompetence, external factual knowledge, and internal factual knowledge. Our\nfindings reveal that while both linguistic competence and factual knowledge\nimprove with scale, internal factual knowledge grows significantly faster,\nsuggesting that model size is more closely tied to memorization than to core\nlanguage ability. These results support a modular approach to language\nmodeling, where compact, linguistically proficient models serve as the\nfoundation for tool-augmented systems. The FLM paradigm offers a path toward\nmore efficient, interpretable, and sustainable NLP solutions."}
{"id": "2509.02292", "pdf": "https://arxiv.org/pdf/2509.02292", "abs": "https://arxiv.org/abs/2509.02292", "authors": ["Katharine Kowalyshyn", "Matthias Scheutz"], "title": "LLMs and their Limited Theory of Mind: Evaluating Mental State Annotations in Situated Dialogue", "categories": ["cs.CL"], "comment": null, "summary": "What if large language models could not only infer human mindsets but also\nexpose every blind spot in team dialogue such as discrepancies in the team\nmembers' joint understanding? We present a novel, two-step framework that\nleverages large language models (LLMs) both as human-style annotators of team\ndialogues to track the team's shared mental models (SMMs) and as automated\ndiscrepancy detectors among individuals' mental states. In the first step, an\nLLM generates annotations by identifying SMM elements within task-oriented\ndialogues from the Cooperative Remote Search Task (CReST) corpus. Then, a\nsecondary LLM compares these LLM-derived annotations and human annotations\nagainst gold-standard labels to detect and characterize divergences. We define\nan SMM coherence evaluation framework for this use case and apply it to six\nCReST dialogues, ultimately producing: (1) a dataset of human and LLM\nannotations; (2) a reproducible evaluation framework for SMM coherence; and (3)\nan empirical assessment of LLM-based discrepancy detection. Our results reveal\nthat, although LLMs exhibit apparent coherence on straightforward\nnatural-language annotation tasks, they systematically err in scenarios\nrequiring spatial reasoning or disambiguation of prosodic cues."}
{"id": "2509.02333", "pdf": "https://arxiv.org/pdf/2509.02333", "abs": "https://arxiv.org/abs/2509.02333", "authors": ["Shihui Yang", "Chengfeng Dou", "Peidong Guo", "Kai Lu", "Qiang Ju", "Fei Deng", "Rihui Xin"], "title": "DCPO: Dynamic Clipping Policy Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a\npromising framework for enhancing the reasoning capabilities of large language\nmodels. However, existing approaches such as GRPO often suffer from zero\ngradients. This problem arises primarily due to fixed clipping bounds for\ntoken-level probability ratios and the standardization of identical rewards,\nwhich can lead to ineffective gradient updates and underutilization of\ngenerated responses. In this work, we propose Dynamic Clipping Policy\nOptimization (DCPO), which introduces a dynamic clipping strategy that\nadaptively adjusts the clipping bounds based on token-specific prior\nprobabilities to enhance token-level exploration, and a smooth advantage\nstandardization technique that standardizes rewards across cumulative training\nsteps to improve the response-level effective utilization of generated\nresponses. DCPO achieved state-of-the-art performance on four benchmarks based\non four different models. In particular, DCPO achieved an Avg@1 of 46.7 under\ngreedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24\nbenchmark, surpassing both DAPO (36.7/31.6) and GRPO (36.7/32.1) on the\nQwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO\nachieves a performance of (23.3/19.0), surpassing GRPO (13.3/10.5) and DAPO\n(20.0/15.3). Furthermore, DCPO achieved an average 28% improvement in the\nnonzero advantage over GRPO in four models, doubled the training efficiency\nover DAPO, and significantly reduced the token clipping ratio by an order of\nmagnitude compared to both GRPO and DAPO, while achieving superior performance.\nThese results highlight DCPO's effectiveness in leveraging generated data more\nefficiently for reinforcement learning in large language models."}
{"id": "2509.02350", "pdf": "https://arxiv.org/pdf/2509.02350", "abs": "https://arxiv.org/abs/2509.02350", "authors": ["Jindong Li", "Yali Fu", "Li Fan", "Jiahong Liu", "Yao Shu", "Chengwei Qin", "Menglin Yang", "Irwin King", "Rex Ying"], "title": "Implicit Reasoning in Large Language Models: A Comprehensive Survey", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong generalization across a\nwide range of tasks. Reasoning with LLMs is central to solving multi-step\nproblems and complex decision-making. To support efficient reasoning, recent\nstudies have shifted attention from explicit chain-of-thought prompting toward\nimplicit reasoning, where reasoning occurs silently via latent structures\nwithout emitting intermediate textual steps. Implicit reasoning brings\nadvantages such as lower generation cost, faster inference, and better\nalignment with internal computation. Although prior surveys have discussed\nlatent representations in the context of reasoning, a dedicated and\nmechanism-level examination of how reasoning unfolds internally within LLMs\nremains absent. This survey fills that gap by introducing a taxonomy centered\non execution paradigms, shifting the focus from representational forms to\ncomputational strategies. We organize existing methods into three execution\nparadigms based on \\textbf{\\textit{how and where internal computation\nunfolds}}: latent optimization, signal-guided control, and layer-recurrent\nexecution. We also review structural, behavioral and representation-based\nevidence that supports the presence of implicit reasoning in LLMs. We further\nprovide a structured overview of the evaluation metrics and benchmarks used in\nexisting works to assess the effectiveness and reliability of implicit\nreasoning.We maintain a continuously updated project at:\nhttps://github.com/digailab/awesome-llm-implicit-reasoning."}
{"id": "2509.02363", "pdf": "https://arxiv.org/pdf/2509.02363", "abs": "https://arxiv.org/abs/2509.02363", "authors": ["Gaurav Negi", "Atul Kr. Ojha", "Omnia Zayed", "Paul Buitelaar"], "title": "Towards Temporal Knowledge-Base Creation for Fine-Grained Opinion Analysis with Language Models", "categories": ["cs.CL"], "comment": null, "summary": "We propose a scalable method for constructing a temporal opinion knowledge\nbase with large language models (LLMs) as automated annotators. Despite the\ndemonstrated utility of time-series opinion analysis of text for downstream\napplications such as forecasting and trend analysis, existing methodologies\nunderexploit this potential due to the absence of temporally grounded\nfine-grained annotations. Our approach addresses this gap by integrating\nwell-established opinion mining formulations into a declarative LLM annotation\npipeline, enabling structured opinion extraction without manual prompt\nengineering. We define three data models grounded in sentiment and opinion\nmining literature, serving as schemas for structured representation. We perform\nrigorous quantitative evaluation of our pipeline using human-annotated test\nsamples. We carry out the final annotations using two separate LLMs, and\ninter-annotator agreement is computed label-wise across the fine-grained\nopinion dimensions, analogous to human annotation protocols. The resulting\nknowledge base encapsulates time-aligned, structured opinions and is compatible\nwith applications in Retrieval-Augmented Generation (RAG), temporal question\nanswering, and timeline summarisation."}
{"id": "2509.02446", "pdf": "https://arxiv.org/pdf/2509.02446", "abs": "https://arxiv.org/abs/2509.02446", "authors": ["Ali Hamdi", "Malak Mohamed", "Rokaia Emad", "Khaled Shaban"], "title": "An Ensemble Classification Approach in A Multi-Layered Large Language Model Framework for Disease Prediction", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Social telehealth has made remarkable progress in healthcare by allowing\npatients to post symptoms and participate in medical consultations remotely.\nUsers frequently post symptoms on social media and online health platforms,\ncreating a huge repository of medical data that can be leveraged for disease\nclassification. Large language models (LLMs) such as LLAMA3 and GPT-3.5, along\nwith transformer-based models like BERT, have demonstrated strong capabilities\nin processing complex medical text. In this study, we evaluate three Arabic\nmedical text preprocessing methods such as summarization, refinement, and Named\nEntity Recognition (NER) before applying fine-tuned Arabic transformer models\n(CAMeLBERT, AraBERT, and AsafayaBERT). To enhance robustness, we adopt a\nmajority voting ensemble that combines predictions from original and\npreprocessed text representations. This approach achieved the best\nclassification accuracy of 80.56%, thus showing its effectiveness in leveraging\nvarious text representations and model predictions to improve the understanding\nof medical texts. To the best of our knowledge, this is the first work that\nintegrates LLM-based preprocessing with fine-tuned Arabic transformer models\nand ensemble learning for disease classification in Arabic social telehealth\ndata."}
{"id": "2509.02450", "pdf": "https://arxiv.org/pdf/2509.02450", "abs": "https://arxiv.org/abs/2509.02450", "authors": ["Lingzhi Shen", "Xiaohao Cai", "Yunfei Long", "Imran Razzak", "Guanming Chen", "Shoaib Jameel"], "title": "EmoPerso: Enhancing Personality Detection with Self-Supervised Emotion-Aware Modelling", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Personality detection from text is commonly performed by analysing users'\nsocial media posts. However, existing methods heavily rely on large-scale\nannotated datasets, making it challenging to obtain high-quality personality\nlabels. Moreover, most studies treat emotion and personality as independent\nvariables, overlooking their interactions. In this paper, we propose a novel\nself-supervised framework, EmoPerso, which improves personality detection\nthrough emotion-aware modelling. EmoPerso first leverages generative mechanisms\nfor synthetic data augmentation and rich representation learning. It then\nextracts pseudo-labeled emotion features and jointly optimizes them with\npersonality prediction via multi-task learning. A cross-attention module is\nemployed to capture fine-grained interactions between personality traits and\nthe inferred emotional representations. To further refine relational reasoning,\nEmoPerso adopts a self-taught strategy to enhance the model's reasoning\ncapabilities iteratively. Extensive experiments on two benchmark datasets\ndemonstrate that EmoPerso surpasses state-of-the-art models. The source code is\navailable at https://github.com/slz0925/EmoPerso."}
{"id": "2509.02452", "pdf": "https://arxiv.org/pdf/2509.02452", "abs": "https://arxiv.org/abs/2509.02452", "authors": ["Seyedali Mohammadi", "Bhaskara Hanuma Vedula", "Hemank Lamba", "Edward Raff", "Ponnurangam Kumaraguru", "Francis Ferraro", "Manas Gaur"], "title": "Do LLMs Adhere to Label Definitions? Examining Their Receptivity to External Label Definitions", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To appear in EMNLP 2025, Main Conference", "summary": "Do LLMs genuinely incorporate external definitions, or do they primarily rely\non their parametric knowledge? To address these questions, we conduct\ncontrolled experiments across multiple explanation benchmark datasets (general\nand domain-specific) and label definition conditions, including expert-curated,\nLLM-generated, perturbed, and swapped definitions. Our results reveal that\nwhile explicit label definitions can enhance accuracy and explainability, their\nintegration into an LLM's task-solving processes is neither guaranteed nor\nconsistent, suggesting reliance on internalized representations in many cases.\nModels often default to their internal representations, particularly in general\ntasks, whereas domain-specific tasks benefit more from explicit definitions.\nThese findings underscore the need for a deeper understanding of how LLMs\nprocess external knowledge alongside their pre-existing capabilities."}
{"id": "2509.02464", "pdf": "https://arxiv.org/pdf/2509.02464", "abs": "https://arxiv.org/abs/2509.02464", "authors": ["Ahmed Ahmed", "Kevin Klyman", "Yi Zeng", "Sanmi Koyejo", "Percy Liang"], "title": "SpecEval: Evaluating Model Adherence to Behavior Specifications", "categories": ["cs.CL"], "comment": null, "summary": "Companies that develop foundation models publish behavioral guidelines they\npledge their models will follow, but it remains unclear if models actually do\nso. While providers such as OpenAI, Anthropic, and Google have published\ndetailed specifications describing both desired safety constraints and\nqualitative traits for their models, there has been no systematic audit of\nadherence to these guidelines. We introduce an automated framework that audits\nmodels against their providers specifications by parsing behavioral statements,\ngenerating targeted prompts, and using models to judge adherence. Our central\nfocus is on three way consistency between a provider specification, its model\noutputs, and its own models as judges; an extension of prior two way generator\nvalidator consistency. This establishes a necessary baseline: at minimum, a\nfoundation model should consistently satisfy the developer behavioral\nspecifications when judged by the developer evaluator models. We apply our\nframework to 16 models from six developers across more than 100 behavioral\nstatements, finding systematic inconsistencies including compliance gaps of up\nto 20 percent across providers."}
{"id": "2509.02492", "pdf": "https://arxiv.org/pdf/2509.02492", "abs": "https://arxiv.org/abs/2509.02492", "authors": ["Chenglong Wang", "Yongyu Mu", "Hang Zhou", "Yifu Huo", "Ziming Zhu", "Jiali Zeng", "Murun Yang", "Bei Li", "Tong Xiao", "Xiaoyang Hao", "Chunliang Zhang", "Fandong Meng", "Jingbo Zhu"], "title": "GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward Reasoning", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Significant progress in reward modeling over recent years has been driven by\na paradigm shift from task-specific designs towards generalist reward models.\nDespite this trend, developing effective reward models remains a fundamental\nchallenge: the heavy reliance on large-scale labeled preference data.\nPre-training on abundant unlabeled data offers a promising direction, but\nexisting approaches fall short of instilling explicit reasoning into reward\nmodels. To bridge this gap, we propose a self-training approach that leverages\nunlabeled data to elicit reward reasoning in reward models. Based on this\napproach, we develop GRAM-R$^2$, a generative reward model trained to produce\nnot only preference labels but also accompanying reward rationales. GRAM-R$^2$\ncan serve as a foundation model for reward reasoning and can be applied to a\nwide range of tasks with minimal or no additional fine-tuning. It can support\ndownstream applications such as response ranking and task-specific reward\ntuning. Experiments on response ranking, task adaptation, and reinforcement\nlearning from human feedback demonstrate that GRAM-R$^2$ consistently delivers\nstrong performance, outperforming several strong discriminative and generative\nbaselines."}
{"id": "2509.02499", "pdf": "https://arxiv.org/pdf/2509.02499", "abs": "https://arxiv.org/abs/2509.02499", "authors": ["Junxi Wu", "Jinpeng Wang", "Zheng Liu", "Bin Chen", "Dongjian Hu", "Hao Wu", "Shu-Tao Xiu"], "title": "MoSEs: Uncertainty-Aware AI-Generated Text Detection via Mixture of Stylistics Experts with Conditional Thresholds", "categories": ["cs.CL", "cs.AI"], "comment": "EMNLP 2025", "summary": "The rapid advancement of large language models has intensified public\nconcerns about the potential misuse. Therefore, it is important to build\ntrustworthy AI-generated text detection systems. Existing methods neglect\nstylistic modeling and mostly rely on static thresholds, which greatly limits\nthe detection performance. In this paper, we propose the Mixture of Stylistic\nExperts (MoSEs) framework that enables stylistics-aware uncertainty\nquantification through conditional threshold estimation. MoSEs contain three\ncore components, namely, the Stylistics Reference Repository (SRR), the\nStylistics-Aware Router (SAR), and the Conditional Threshold Estimator (CTE).\nFor input text, SRR can activate the appropriate reference data in SRR and\nprovide them to CTE. Subsequently, CTE jointly models the linguistic\nstatistical properties and semantic features to dynamically determine the\noptimal threshold. With a discrimination score, MoSEs yields prediction labels\nwith the corresponding confidence level. Our framework achieves an average\nimprovement 11.34% in detection performance compared to baselines. More\ninspiringly, MoSEs shows a more evident improvement 39.15% in the low-resource\ncase. Our code is available at https://github.com/creator-xi/MoSEs."}
{"id": "2509.02503", "pdf": "https://arxiv.org/pdf/2509.02503", "abs": "https://arxiv.org/abs/2509.02503", "authors": ["Nishant Tanksale", "Tanmay Kokate", "Darshan Gohad", "Sarvadnyaa Barate", "Raviraj Joshi"], "title": "L3Cube-IndicHeadline-ID: A Dataset for Headline Identification and Semantic Evaluation in Low-Resource Indian Languages", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Semantic evaluation in low-resource languages remains a major challenge in\nNLP. While sentence transformers have shown strong performance in high-resource\nsettings, their effectiveness in Indic languages is underexplored due to a lack\nof high-quality benchmarks. To bridge this gap, we introduce\nL3Cube-IndicHeadline-ID, a curated headline identification dataset spanning ten\nlow-resource Indic languages: Marathi, Hindi, Tamil, Gujarati, Odia, Kannada,\nMalayalam, Punjabi, Telugu, Bengali and English. Each language includes 20,000\nnews articles paired with four headline variants: the original, a semantically\nsimilar version, a lexically similar version, and an unrelated one, designed to\ntest fine-grained semantic understanding. The task requires selecting the\ncorrect headline from the options using article-headline similarity. We\nbenchmark several sentence transformers, including multilingual and\nlanguage-specific models, using cosine similarity. Results show that\nmultilingual models consistently perform well, while language-specific models\nvary in effectiveness. Given the rising use of similarity models in\nRetrieval-Augmented Generation (RAG) pipelines, this dataset also serves as a\nvaluable resource for evaluating and improving semantic understanding in such\napplications. Additionally, the dataset can be repurposed for multiple-choice\nquestion answering, headline classification, or other task-specific evaluations\nof LLMs, making it a versatile benchmark for Indic NLP. The dataset is shared\npublicly at https://github.com/l3cube-pune/indic-nlp"}
{"id": "2509.02506", "pdf": "https://arxiv.org/pdf/2509.02506", "abs": "https://arxiv.org/abs/2509.02506", "authors": ["Jean-Marie Le Ray"], "title": "The Forgotten Code: Validating a Century-Old Translation System with AI", "categories": ["cs.CL", "I.2"], "comment": "Preprint, 35 pages, 14 figures, 9 appendices", "summary": "A pioneering rule-based mechanical translation system (precursor of modern\nRBMTs) was first presented in December 1929 by its inventor, Federico Pucci,\nwho later published the full method in a book titled \"Il traduttore meccanico\ned il metodo per corrispondersi fra Europei conoscendo ciascuno solo la propria\nlingua: Parte I\", in Salerno (Italy), in 1931. This study illustrates how AI\nbreathes new life into the system of international keys and ideograms devised\nby Pucci to translate from/into any Romance language (at least as a first\nstep). The methodology involves having the AIs retranslate, following Pucci's\nmethod, the two text excerpts originally translated in 1931 and clearly\ndocumented in his publication: a passage from Dante's La Vita Nuova, translated\nfrom Italian into French, and a passage from Voltaire's Zadig, translated from\nFrench into Italian. The result is notable: the two texts, translated 94 years\napart using the same method--by Pucci in 1931 and by AIs in 2025--show a low\naverage difference, with only minor variations observed. With Pucci's system\nthus validated, it became feasible to have the AIs reproduce the excerpts in\nEnglish, Spanish, and German according to his method. The results were\nconsistent, and Pucci--via Artificial Intelligence--was tasked with translating\nmore modern and technical texts, thereby reviving, nearly a century later, an\ninvention that had remained almost entirely unknown and never applied beyond\nits creator, now brought to wider attention and opened to possible\nexperimentation. Such a demonstration would not only affirm Pucci's historical\nstatus but also place him among the precursors and intellectual contributors to\nmachine translation, whose work merits examination alongside figures such as\nTroyanskij, Booth, and Weaver, with possible consequences for how the history\nof the field is understood."}
{"id": "2509.02510", "pdf": "https://arxiv.org/pdf/2509.02510", "abs": "https://arxiv.org/abs/2509.02510", "authors": ["Erfan Baghaei Potraghloo", "Seyedarmin Azizi", "Souvik Kundu", "Massoud Pedram"], "title": "Top-H Decoding: Adapting the Creativity and Coherence with Bounded Entropy in Text Generation", "categories": ["cs.CL", "cs.AI", "stat.ML"], "comment": null, "summary": "Large language models (LLMs), despite their impressive performance across a\nwide range of tasks, often struggle to balance two competing objectives in\nopen-ended text generation: fostering diversity and creativity while preserving\nlogical coherence. Existing truncated sampling techniques, including\ntemperature scaling, top-\\$p\\$ (nucleus) sampling, and min-\\$p\\$ sampling, aim\nto manage this trade-off. However, they exhibit limitations, particularly in\nthe effective incorporation of the confidence of the model into the\ncorresponding sampling strategy. For example, min-\\$p\\$ sampling relies on a\nsingle top token as a heuristic for confidence, eventually underutilizing the\ninformation of the probability distribution. Toward effective incorporation of\nthe confidence of the model, in this paper, we present **top-H** decoding. We\nfirst establish the theoretical foundation of the interplay between creativity\nand coherence in truncated sampling by formulating an **entropy-constrained\nminimum divergence** problem. We then prove this minimization problem to be\nequivalent to an **entropy-constrained mass maximization** (ECMM) problem,\nwhich is NP-hard. Finally, we present top-H decoding, a computationally\nefficient greedy algorithm to solve the ECMM problem. Extensive empirical\nevaluations demonstrate that top-H outperforms the state-of-the-art (SoTA)\nalternative of min-\\$p\\$ sampling by up to **25.63%** on creative writing\nbenchmarks, while maintaining robustness on question-answering datasets such as\nGPQA, GSM8K, and MT-Bench. Additionally, an *LLM-as-judge* evaluation confirms\nthat top-H indeed produces coherent outputs even at higher temperatures, where\ncreativity is especially critical. In summary, top-H advances SoTA in\nopen-ended text generation and can be *easily integrated* into creative writing\napplications. The code is available at\nhttps://github.com/ErfanBaghaei/Top-H-Decoding."}
{"id": "2509.02514", "pdf": "https://arxiv.org/pdf/2509.02514", "abs": "https://arxiv.org/abs/2509.02514", "authors": ["Mayur Shirke", "Amey Shembade", "Pavan Thorat", "Madhushri Wagh", "Raviraj Joshi"], "title": "Comparative Study of Pre-Trained BERT and Large Language Models for Code-Mixed Named Entity Recognition", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Named Entity Recognition (NER) in code-mixed text, particularly Hindi-English\n(Hinglish), presents unique challenges due to informal structure,\ntransliteration, and frequent language switching. This study conducts a\ncomparative evaluation of code-mixed fine-tuned models and non-code-mixed\nmultilingual models, along with zero-shot generative large language models\n(LLMs). Specifically, we evaluate HingBERT, HingMBERT, and HingRoBERTa (trained\non code-mixed data), and BERT Base Cased, IndicBERT, RoBERTa and MuRIL (trained\non non-code-mixed multilingual data). We also assess the performance of Google\nGemini in a zero-shot setting using a modified version of the dataset with NER\ntags removed. All models are tested on a benchmark Hinglish NER dataset using\nPrecision, Recall, and F1-score. Results show that code-mixed models,\nparticularly HingRoBERTa and HingBERT-based fine-tuned models, outperform\nothers - including closed-source LLMs like Google Gemini - due to\ndomain-specific pretraining. Non-code-mixed models perform reasonably but show\nlimited adaptability. Notably, Google Gemini exhibits competitive zero-shot\nperformance, underlining the generalization strength of modern LLMs. This study\nprovides key insights into the effectiveness of specialized versus generalized\nmodels for code-mixed NER tasks."}
{"id": "2509.02522", "pdf": "https://arxiv.org/pdf/2509.02522", "abs": "https://arxiv.org/abs/2509.02522", "authors": ["Jiaming Li", "Longze Chen", "Ze Gong", "Yukun Chen", "Lu Wang", "Wanwei He", "Run Luo", "Min Yang"], "title": "Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have\nempowered large language models (LLMs) to tackle challenging reasoning tasks\nsuch as mathematics and programming. RLVR leverages verifiable outcome rewards\nto guide policy optimization, enabling LLMs to progressively improve output\nquality in a grounded and reliable manner. Despite its promise, the RLVR\nparadigm poses significant challenges, as existing methods often suffer from\nsparse reward signals and unstable policy gradient updates, particularly in\nRL-based approaches. To address the challenges, we propose $\\textbf{PACS}$, a\nnovel RLVR framework that achieves im$\\textbf{P}$licit $\\textbf{A}$ctor\n$\\textbf{C}$ritic coupling via a $\\textbf{S}$upervised learning framework. By\ntreating the outcome reward as a predictable label, we reformulate the RLVR\nproblem into a supervised learning task over a score function parameterized by\nthe policy model and optimized using cross-entropy loss. A detailed gradient\nanalysis shows that this supervised formulation inherently recovers the\nclassical policy gradient update while implicitly coupling actor and critic\nroles, yielding more stable and efficient training. Benchmarking on challenging\nmathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as\nPPO and GRPO, achieving superior reasoning performance. For instance, PACS\nachieves 59.78\\% at pass@256 on AIME 2025, representing improvements of 13.32\nand 14.36 points over PPO and GRPO. This simple yet powerful framework offers a\npromising avenue for LLMs post-training with verifiable rewards. Our code and\ndata are available as open source at https://github.com/ritzz-ai/PACS."}
{"id": "2509.02523", "pdf": "https://arxiv.org/pdf/2509.02523", "abs": "https://arxiv.org/abs/2509.02523", "authors": ["Evan King", "Adam Sabra", "Manjunath Kudlur", "James Wang", "Pete Warden"], "title": "Flavors of Moonshine: Tiny Specialized ASR Models for Edge Devices", "categories": ["cs.CL", "cs.LG", "cs.SD"], "comment": null, "summary": "We present the Flavors of Moonshine, a suite of tiny automatic speech\nrecognition (ASR) models specialized for a range of underrepresented languages.\nPrevailing wisdom suggests that multilingual ASR models outperform monolingual\ncounterparts by exploiting cross-lingual phonetic similarities. We challenge\nthis assumption, showing that for sufficiently small models (27M parameters),\ntraining monolingual systems on a carefully balanced mix of high-quality\nhuman-labeled, pseudo-labeled, and synthetic data yields substantially superior\nperformance. On average, our models achieve error rates 48% lower than the\ncomparably sized Whisper Tiny model, outperform the 9x larger Whisper Small\nmodel, and in most cases match or outperform the 28x larger Whisper Medium\nmodel. These results advance the state of the art for models of this size,\nenabling accurate on-device ASR for languages that previously had limited\nsupport. We release Arabic, Chinese, Japanese, Korean, Ukrainian, and\nVietnamese Moonshine models under a permissive open-source license."}
{"id": "2509.02534", "pdf": "https://arxiv.org/pdf/2509.02534", "abs": "https://arxiv.org/abs/2509.02534", "authors": ["Tianjian Li", "Yiming Zhang", "Ping Yu", "Swarnadeep Saha", "Daniel Khashabi", "Jason Weston", "Jack Lanchantin", "Tianlu Wang"], "title": "Jointly Reinforcing Diversity and Quality in Language Model Generations", "categories": ["cs.CL", "cs.LG"], "comment": "29 pages, 11 figures", "summary": "Post-training of Large Language Models (LMs) often prioritizes accuracy and\nhelpfulness at the expense of diversity. This creates a tension: while\npost-training improves response quality, it also sharpens output distributions\nand reduces the range of ideas, limiting the usefulness of LMs in creative and\nexploratory tasks such as brainstorming, storytelling, or problem solving. We\naddress this challenge with Diversity-Aware Reinforcement Learning (DARLING), a\nframework that jointly optimizes for response quality and semantic diversity.\nAt its core, DARLING introduces a learned partition function to measure\ndiversity beyond surface-level lexical variations. This diversity signal is\nthen combined with a quality reward during online reinforcement learning,\nencouraging models to generate outputs that are both high-quality and distinct.\nExperiments across multiple model families and sizes show that DARLING\ngeneralizes to two regimes: non-verifiable tasks (instruction following and\ncreative writing) and verifiable tasks (competition math). On five benchmarks\nin the first setting, DARLING consistently outperforms quality-only RL\nbaselines, producing outputs that are simultaneously of higher quality and\nnovelty. In the second setting, DARLING achieves higher pass@1 (solution\nquality) and pass@k (solution variety). Most strikingly, explicitly optimizing\nfor diversity catalyzes exploration in online RL, which manifests itself as\nhigher-quality responses."}
{"id": "2509.02550", "pdf": "https://arxiv.org/pdf/2509.02550", "abs": "https://arxiv.org/abs/2509.02550", "authors": ["Fakhraddin Alwajih", "Abdellah El Mekki", "Hamdy Mubarak", "Majd Hawasly", "Abubakr Mohamed", "Muhammad Abdul-Mageed"], "title": "PalmX 2025: The First Shared Task on Benchmarking LLMs on Arabic and Islamic Culture", "categories": ["cs.CL"], "comment": "https://palmx.dlnlp.ai/", "summary": "Large Language Models (LLMs) inherently reflect the vast data distributions\nthey encounter during their pre-training phase. As this data is predominantly\nsourced from the web, there is a high chance it will be skewed towards\nhigh-resourced languages and cultures, such as those of the West. Consequently,\nLLMs often exhibit a diminished understanding of certain communities, a gap\nthat is particularly evident in their knowledge of Arabic and Islamic cultures.\nThis issue becomes even more pronounced with increasingly under-represented\ntopics. To address this critical challenge, we introduce PalmX 2025, the first\nshared task designed to benchmark the cultural competence of LLMs in these\nspecific domains. The task is composed of two subtasks featuring\nmultiple-choice questions (MCQs) in Modern Standard Arabic (MSA): General\nArabic Culture and General Islamic Culture. These subtasks cover a wide range\nof topics, including traditions, food, history, religious practices, and\nlanguage expressions from across 22 Arab countries. The initiative drew\nconsiderable interest, with 26 teams registering for Subtask 1 and 19 for\nSubtask 2, culminating in nine and six valid submissions, respectively. Our\nfindings reveal that task-specific fine-tuning substantially boosts performance\nover baseline models. The top-performing systems achieved an accuracy of 72.15%\non cultural questions and 84.22% on Islamic knowledge. Parameter-efficient\nfine-tuning emerged as the predominant and most effective approach among\nparticipants, while the utility of data augmentation was found to be\ndomain-dependent."}
{"id": "2509.00053", "pdf": "https://arxiv.org/pdf/2509.00053", "abs": "https://arxiv.org/abs/2509.00053", "authors": ["Shuo Liu", "Di Yao", "Yan Lin", "Gao Cong", "Jingping Bi"], "title": "Traj-MLLM: Can Multimodal Large Language Models Reform Trajectory Data Mining?", "categories": ["cs.MM", "cs.AI", "cs.CL"], "comment": "20 pages, 10 figures", "summary": "Building a general model capable of analyzing human trajectories across\ndifferent geographic regions and different tasks becomes an emergent yet\nimportant problem for various applications. However, existing works suffer from\nthe generalization problem, \\ie, they are either restricted to train for\nspecific regions or only suitable for a few tasks. Given the recent advances of\nmultimodal large language models (MLLMs), we raise the question: can MLLMs\nreform current trajectory data mining and solve the problem? Nevertheless, due\nto the modality gap of trajectory, how to generate task-independent multimodal\ntrajectory representations and how to adapt flexibly to different tasks remain\nthe foundational challenges. In this paper, we propose \\texttt{Traj-MLLM}},\nwhich is the first general framework using MLLMs for trajectory data mining. By\nintegrating multiview contexts, \\texttt{Traj-MLLM}} transforms raw trajectories\ninto interleaved image-text sequences while preserving key spatial-temporal\ncharacteristics, and directly utilizes the reasoning ability of MLLMs for\ntrajectory analysis. Additionally, a prompt optimization method is proposed to\nfinalize data-invariant prompts for task adaptation. Extensive experiments on\nfour publicly available datasets show that \\texttt{Traj-MLLM}} outperforms\nstate-of-the-art baselines by $48.05\\%$, $15.52\\%$, $51.52\\%$, $1.83\\%$ on\ntravel time estimation, mobility prediction, anomaly detection and\ntransportation mode identification, respectively. \\texttt{Traj-MLLM}} achieves\nthese superior performances without requiring any training data or fine-tuning\nthe MLLM backbones."}
{"id": "2509.00074", "pdf": "https://arxiv.org/pdf/2509.00074", "abs": "https://arxiv.org/abs/2509.00074", "authors": ["Cédric Colas", "Tracey Mills", "Ben Prystawski", "Michael Henry Tessler", "Noah Goodman", "Jacob Andreas", "Joshua Tenenbaum"], "title": "Language and Experience: A Computational Model of Social Learning in Complex Tasks", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "The ability to combine linguistic guidance from others with direct experience\nis central to human development, enabling safe and rapid learning in new\nenvironments. How do people integrate these two sources of knowledge, and how\nmight AI systems? We present a computational framework that models social\nlearning as joint probabilistic inference over structured, executable world\nmodels given sensorimotor and linguistic data. We make this possible by turning\na pretrained language model into a probabilistic model of how humans share\nadvice conditioned on their beliefs, allowing our agents both to generate\nadvice for others and to interpret linguistic input as evidence during Bayesian\ninference. Using behavioral experiments and simulations across 10 video games,\nwe show how linguistic guidance can shape exploration and accelerate learning\nby reducing risky interactions and speeding up key discoveries in both humans\nand models. We further explore how knowledge can accumulate across generations\nthrough iterated learning experiments and demonstrate successful knowledge\ntransfer between humans and models -- revealing how structured,\nlanguage-compatible representations might enable human-machine collaborative\nlearning."}
{"id": "2509.00077", "pdf": "https://arxiv.org/pdf/2509.00077", "abs": "https://arxiv.org/abs/2509.00077", "authors": ["Tai Vu"], "title": "Amplifying Emotional Signals: Data-Efficient Deep Learning for Robust Speech Emotion Recognition", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": null, "summary": "Speech Emotion Recognition (SER) presents a significant yet persistent\nchallenge in human-computer interaction. While deep learning has advanced\nspoken language processing, achieving high performance on limited datasets\nremains a critical hurdle. This paper confronts this issue by developing and\nevaluating a suite of machine learning models, including Support Vector\nMachines (SVMs), Long Short-Term Memory networks (LSTMs), and Convolutional\nNeural Networks (CNNs), for automated emotion classification in human speech.\nWe demonstrate that by strategically employing transfer learning and innovative\ndata augmentation techniques, our models can achieve impressive performance\ndespite the constraints of a relatively small dataset. Our most effective\nmodel, a ResNet34 architecture, establishes a new performance benchmark on the\ncombined RAVDESS and SAVEE datasets, attaining an accuracy of 66.7% and an F1\nscore of 0.631. These results underscore the substantial benefits of leveraging\npre-trained models and data augmentation to overcome data scarcity, thereby\npaving the way for more robust and generalizable SER systems."}
{"id": "2509.00078", "pdf": "https://arxiv.org/pdf/2509.00078", "abs": "https://arxiv.org/abs/2509.00078", "authors": ["Tatiana Likhomanenko", "Luke Carlson", "Richard He Bai", "Zijin Gu", "Han Tran", "Zakaria Aldeneh", "Yizhe Zhang", "Ruixiang Zhang", "Huangjie Zheng", "Navdeep Jaitly"], "title": "ChipChat: Low-Latency Cascaded Conversational Agent in MLX", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "comment": "ASRU 2025", "summary": "The emergence of large language models (LLMs) has transformed spoken dialog\nsystems, yet the optimal architecture for real-time on-device voice agents\nremains an open question. While end-to-end approaches promise theoretical\nadvantages, cascaded systems (CSs) continue to outperform them in language\nunderstanding tasks, despite being constrained by sequential processing\nlatency. In this work, we introduce ChipChat, a novel low-latency CS that\novercomes traditional bottlenecks through architectural innovations and\nstreaming optimizations. Our system integrates streaming (a) conversational\nspeech recognition with mixture-of-experts, (b) state-action augmented LLM, (c)\ntext-to-speech synthesis, (d) neural vocoder, and (e) speaker modeling.\nImplemented using MLX, ChipChat achieves sub-second response latency on a Mac\nStudio without dedicated GPUs, while preserving user privacy through complete\non-device processing. Our work shows that strategically redesigned CSs can\novercome their historical latency limitations, offering a promising path\nforward for practical voice-based AI agents."}
{"id": "2509.00083", "pdf": "https://arxiv.org/pdf/2509.00083", "abs": "https://arxiv.org/abs/2509.00083", "authors": ["Laksh Patel", "Neel Shanbhag"], "title": "Data Cartography for Detecting Memorization Hotspots and Guiding Data Interventions in Generative Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "6 pages, 2 figures, 1 table; Presented at the 42nd International\n  Conference on Machine Learning (ICML), winning the \"Best Poster\" award at\n  ICML's workshop for data in generative models (DIG-BUGS)", "summary": "Modern generative models risk overfitting and unintentionally memorizing rare\ntraining examples, which can be extracted by adversaries or inflate benchmark\nperformance. We propose Generative Data Cartography (GenDataCarto), a\ndata-centric framework that assigns each pretraining sample a difficulty score\n(early-epoch loss) and a memorization score (frequency of ``forget events''),\nthen partitions examples into four quadrants to guide targeted pruning and\nup-/down-weighting. We prove that our memorization score lower-bounds classical\ninfluence under smoothness assumptions and that down-weighting\nhigh-memorization hotspots provably decreases the generalization gap via\nuniform stability bounds. Empirically, GenDataCarto reduces synthetic canary\nextraction success by over 40\\% at just 10\\% data pruning, while increasing\nvalidation perplexity by less than 0.5\\%. These results demonstrate that\nprincipled data interventions can dramatically mitigate leakage with minimal\ncost to generative performance."}
{"id": "2509.00084", "pdf": "https://arxiv.org/pdf/2509.00084", "abs": "https://arxiv.org/abs/2509.00084", "authors": ["Qibin Wang", "Pu Zhao", "Shaohan Huang", "Fangkai Yang", "Lu Wang", "Furu Wei", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "Learning to Refine: Self-Refinement of Parallel Reasoning in LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "To further enhance the ability of Large Language Models (LLMs) to solve\ncomplex, multi-step reasoning problems, test-time scaling (TTS) methods have\ngained widespread attention. Existing approaches such as Best-of-N and majority\nvoting are limited as their performance depends on the quality of candidate\nresponses, making them unable to produce a correct solution when all candidates\nare incorrect. Introducing an additional model to select the best response also\nincurs significant deployment costs. To this end, we introduce Generative\nSelf-Refinement (GSR), a novel parallel test-time scaling framework where a\nunified model first generates a set of candidate responses in parallel and then\nperforms self-refinement to synthesize a new superior solution based on a\nprompt consisting of the problem and these candidates. However, LLMs struggle\nto perform refinement effectively when prompted directly. Therefore, we design\na hybrid training pipeline by jointly optimizing for two complementary\nobjectives, solving problems directly and refining candidate responses.\nExperimental results demonstrate that our method achieves state-of-the-art\nperformance across five mathematical benchmarks. We further show that this\nlearned self-refinement skill is a model-agnostic enhancement, robust across\ndifferent model scales and generalizing to out-of-distribution reasoning tasks."}
{"id": "2509.00091", "pdf": "https://arxiv.org/pdf/2509.00091", "abs": "https://arxiv.org/abs/2509.00091", "authors": ["Ephraiem Sarabamoun"], "title": "Ensemble Debates with Local Large Language Models for AI Alignment", "categories": ["cs.AI", "cs.CL"], "comment": "9 pages, 2 tables", "summary": "As large language models (LLMs) take on greater roles in high-stakes\ndecisions, alignment with human values is essential. Reliance on proprietary\nAPIs limits reproducibility and broad participation. We study whether local\nopen-source ensemble debates can improve alignmentoriented reasoning. Across\n150 debates spanning 15 scenarios and five ensemble configurations, ensembles\noutperform single-model baselines on a 7-point rubric (overall: 3.48 vs. 3.13),\nwith the largest gains in reasoning depth (+19.4%) and argument quality\n(+34.1%). Improvements are strongest for truthfulness (+1.25 points) and human\nenhancement (+0.80). We provide code, prompts, and a debate data set, providing\nan accessible and reproducible foundation for ensemble-based alignment\nevaluation."}
{"id": "2509.00094", "pdf": "https://arxiv.org/pdf/2509.00094", "abs": "https://arxiv.org/abs/2509.00094", "authors": ["Abdullah Abdelfattah", "Mahmoud I. Khalil", "Hazem Abbas"], "title": "Automatic Pronunciation Error Detection and Correction of the Holy Quran's Learners Using Deep Learning", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.LG", "cs.SD"], "comment": null, "summary": "Assessing spoken language is challenging, and quantifying pronunciation\nmetrics for machine learning models is even harder. However, for the Holy\nQuran, this task is simplified by the rigorous recitation rules (tajweed)\nestablished by Muslim scholars, enabling highly effective assessment. Despite\nthis advantage, the scarcity of high-quality annotated data remains a\nsignificant barrier.\n  In this work, we bridge these gaps by introducing: (1) A 98% automated\npipeline to produce high-quality Quranic datasets -- encompassing: Collection\nof recitations from expert reciters, Segmentation at pause points (waqf) using\nour fine-tuned wav2vec2-BERT model, Transcription of segments, Transcript\nverification via our novel Tasmeea algorithm; (2) 850+ hours of audio (~300K\nannotated utterances); (3) A novel ASR-based approach for pronunciation error\ndetection, utilizing our custom Quran Phonetic Script (QPS) to encode Tajweed\nrules (unlike the IPA standard for Modern Standard Arabic). QPS uses a\ntwo-level script: (Phoneme level): Encodes Arabic letters with short/long\nvowels. (Sifa level): Encodes articulation characteristics of every phoneme. We\nfurther include comprehensive modeling with our novel multi-level CTC Model\nwhich achieved 0.16% average Phoneme Error Rate (PER) on the testset. We\nrelease all code, data, and models as open-source:\nhttps://obadx.github.io/prepare-quran-dataset/"}
{"id": "2509.00096", "pdf": "https://arxiv.org/pdf/2509.00096", "abs": "https://arxiv.org/abs/2509.00096", "authors": ["Yao Fu", "Runchao Li", "Xianxuan Long", "Haotian Yu", "Xiaotian Han", "Yu Yin", "Pan Li"], "title": "Pruning Weights but Not Truth: Safeguarding Truthfulness While Pruning LLMs", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to EMNLP2025 findings (poster)", "summary": "Neural network pruning has emerged as a promising approach for deploying LLMs\nin low-resource scenarios while preserving downstream task performance.\nHowever, for the first time, we reveal that such pruning disrupts LLMs'\ninternal activation features crucial for lie detection, where probing\nclassifiers (typically small logistic regression models) trained on these\nfeatures assess the truthfulness of LLM-generated statements. This discovery\nraises a crucial open question: how can we prune LLMs without sacrificing these\ncritical lie detection capabilities? Our investigation further reveals that\nnaively adjusting layer-wise pruning sparsity based on importance inadvertently\nremoves crucial weights, failing to improve lie detection performance despite\nits reliance on the most crucial LLM layer. To address this issue, we propose\nTruthful Pruning aligned by Layer-wise Outliers (TPLO), which places greater\nemphasis on layers with more activation outliers and stronger discriminative\nfeatures simultaneously. This preserves LLMs' original performance while\nretaining critical features of inner states needed for robust lie detection.\nMoreover, we introduce a prompting rule to enrich the TruthfulQA benchmark for\nbetter calibrating LLM pruning. Empirical results show that our approach\nimproves the hallucination detection for pruned LLMs (achieving 88% accuracy at\n50% sparsity) and enhances their performance on TruthfulQA."}
{"id": "2509.00115", "pdf": "https://arxiv.org/pdf/2509.00115", "abs": "https://arxiv.org/abs/2509.00115", "authors": ["Manish Shukla"], "title": "Adaptive Monitoring and Real-World Evaluation of Agentic AI Systems", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": null, "summary": "Agentic artificial intelligence (AI) -- multi-agent systems that combine\nlarge language models with external tools and autonomous planning -- are\nrapidly transitioning from research laboratories into high-stakes domains. Our\nearlier \"Basic\" paper introduced a five-axis framework and proposed preliminary\nmetrics such as goal drift and harm reduction but did not provide an\nalgorithmic instantiation or empirical evidence. This \"Advanced\" sequel fills\nthat gap. First, we revisit recent benchmarks and industrial deployments to\nshow that technical metrics still dominate evaluations: a systematic review of\n84 papers from 2023--2025 found that 83% report capability metrics while only\n30% consider human-centred or economic axes [2]. Second, we formalise an\nAdaptive Multi-Dimensional Monitoring (AMDM) algorithm that normalises\nheterogeneous metrics, applies per-axis exponentially weighted moving-average\nthresholds and performs joint anomaly detection via the Mahalanobis distance.\nThird, we conduct simulations and real-world experiments. AMDM cuts\nanomaly-detection latency from 12.3 s to 5.6 s on simulated goal drift and\nreduces false-positive rates from 4.5% to 0.9% compared with static thresholds.\nWe present a comparison table and ROC/PR curves, and we reanalyse case studies\nto surface missing metrics. Code, data and a reproducibility checklist\naccompany this paper to facilitate replication."}
{"id": "2509.00230", "pdf": "https://arxiv.org/pdf/2509.00230", "abs": "https://arxiv.org/abs/2509.00230", "authors": ["Linus Stuhlmann", "Michael Alexander Saxer"], "title": "Evaluating the Effectiveness of Transformer Layers in Wav2Vec 2.0, XLS-R, and Whisper for Speaker Identification Tasks", "categories": ["cs.SD", "cs.AI", "cs.CL", "cs.LG", "eess.AS"], "comment": null, "summary": "This study evaluates the performance of three advanced speech encoder models,\nWav2Vec 2.0, XLS-R, and Whisper, in speaker identification tasks. By\nfine-tuning these models and analyzing their layer-wise representations using\nSVCCA, k-means clustering, and t-SNE visualizations, we found that Wav2Vec 2.0\nand XLS-R capture speaker-specific features effectively in their early layers,\nwith fine-tuning improving stability and performance. Whisper showed better\nperformance in deeper layers. Additionally, we determined the optimal number of\ntransformer layers for each model when fine-tuned for speaker identification\ntasks."}
{"id": "2509.00366", "pdf": "https://arxiv.org/pdf/2509.00366", "abs": "https://arxiv.org/abs/2509.00366", "authors": ["Ziyi Guan", "Jason Chun Lok Li", "Zhijian Hou", "Pingping Zhang", "Donglai Xu", "Yuzhi Zhao", "Mengyang Wu", "Jinpeng Chen", "Thanh-Toan Nguyen", "Pengfei Xian", "Wenao Ma", "Shengchao Qin", "Graziano Chesi", "Ngai Wong"], "title": "KG-RAG: Enhancing GUI Agent Decision-Making via Knowledge Graph-Driven Retrieval-Augmented Generation", "categories": ["cs.MA", "cs.CL", "cs.MM"], "comment": "Accepted by the EMNLP 2025", "summary": "Despite recent progress, Graphic User Interface (GUI) agents powered by Large\nLanguage Models (LLMs) struggle with complex mobile tasks due to limited\napp-specific knowledge. While UI Transition Graphs (UTGs) offer structured\nnavigation representations, they are underutilized due to poor extraction and\ninefficient integration. We introduce KG-RAG, a Knowledge Graph-driven\nRetrieval-Augmented Generation framework that transforms fragmented UTGs into\nstructured vector databases for efficient real-time retrieval. By leveraging an\nintent-guided LLM search method, KG-RAG generates actionable navigation paths,\nenhancing agent decision-making. Experiments across diverse mobile apps show\nthat KG-RAG outperforms existing methods, achieving a 75.8% success rate (8.9%\nimprovement over AutoDroid), 84.6% decision accuracy (8.1% improvement), and\nreducing average task steps from 4.5 to 4.1. Additionally, we present\nKG-Android-Bench and KG-Harmony-Bench, two benchmarks tailored to the Chinese\nmobile ecosystem for future research. Finally, KG-RAG transfers to web/desktop\n(+40% SR on Weibo-web; +20% on QQ Music-desktop), and a UTG cost ablation shows\naccuracy saturates at ~4h per complex app, enabling practical deployment\ntrade-offs."}
{"id": "2509.00510", "pdf": "https://arxiv.org/pdf/2509.00510", "abs": "https://arxiv.org/abs/2509.00510", "authors": ["Li Weigang", "Pedro Carvalho Brom", "Lucas Ramson Siefert"], "title": "LLM-Assisted Iterative Evolution with Swarm Intelligence Toward SuperBrain", "categories": ["cs.AI", "cs.CL", "68T99", "I.2.11; I.2.8; I.2.6"], "comment": "24 pages, 5 figures", "summary": "We propose a novel SuperBrain framework for collective intelligence, grounded\nin the co-evolution of large language models (LLMs) and human users. Unlike\nstatic prompt engineering or isolated agent simulations, our approach\nemphasizes a dynamic pathway from Subclass Brain to Superclass Brain: (1) A\nSubclass Brain arises from persistent, personalized interaction between a user\nand an LLM, forming a cognitive dyad with adaptive learning memory. (2) Through\nGA-assisted forward-backward evolution, these dyads iteratively refine prompts\nand task performance. (3) Multiple Subclass Brains coordinate via Swarm\nIntelligence, optimizing across multi-objective fitness landscapes and\nexchanging distilled heuristics. (4) Their standardized behaviors and cognitive\nsignatures integrate into a Superclass Brain, an emergent meta-intelligence\ncapable of abstraction, generalization and self-improvement. We outline the\ntheoretical constructs, present initial implementations (e.g., UAV scheduling,\nKU/KI keyword filtering) and propose a registry for cross-dyad knowledge\nconsolidation. This work provides both a conceptual foundation and an\narchitectural roadmap toward scalable, explainable and ethically aligned\ncollective AI."}
{"id": "2509.00520", "pdf": "https://arxiv.org/pdf/2509.00520", "abs": "https://arxiv.org/abs/2509.00520", "authors": ["Yuzheng Cai", "Yanzhao Zhang", "Dingkun Long", "Mingxin Li", "Pengjun Xie", "Weiguo Zheng"], "title": "ERank: Fusing Supervised Fine-Tuning and Reinforcement Learning for Effective and Efficient Text Reranking", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Text reranking models are a crucial component in modern systems like\nRetrieval-Augmented Generation, tasked with selecting the most relevant\ndocuments prior to generation. However, current Large Language Models (LLMs)\npowered rerankers often face a fundamental trade-off. On one hand, Supervised\nFine-Tuning based pointwise methods that frame relevance as a binary\nclassification task lack the necessary scoring discrimination, particularly for\nthose built on reasoning LLMs. On the other hand, approaches designed for\ncomplex reasoning often employ powerful yet inefficient listwise formulations,\nrendering them impractical for low latency applications. To resolve this\ndilemma, we introduce ERank, a highly effective and efficient pointwise\nreranker built from a reasoning LLM that excels across diverse relevance\nscenarios. We propose a novel two-stage training pipeline that begins with\nSupervised Fine-Tuning (SFT). In this stage, we move beyond binary labels and\ntrain the model generatively to output fine grained integer scores, which\nsignificantly enhances relevance discrimination. The model is then further\nrefined using Reinforcement Learning (RL) with a novel, listwise derived\nreward. This technique instills global ranking awareness into the efficient\npointwise architecture. We evaluate the ERank reranker on the BRIGHT, FollowIR,\nTREC DL, and BEIR benchmarks, demonstrating superior effectiveness and\nrobustness compared to existing approaches. On the reasoning-intensive BRIGHT\nbenchmark, our ERank-4B achieves an nDCG@10 of 38.7, while a larger 32B variant\nreaches a state of the art nDCG@10 of 40.2."}
{"id": "2509.00546", "pdf": "https://arxiv.org/pdf/2509.00546", "abs": "https://arxiv.org/abs/2509.00546", "authors": ["Lu Han", "Mengyan Li", "Jiping Qiang", "Zhi Su"], "title": "Advanced spectral clustering for heterogeneous data in credit risk monitoring systems", "categories": ["cs.LG", "cs.CL"], "comment": "25 pages, 7 figures, 6 tables", "summary": "Heterogeneous data, which encompass both numerical financial variables and\ntextual records, present substantial challenges for credit monitoring. To\naddress this issue, we propose Advanced Spectral Clustering (ASC), a method\nthat integrates financial and textual similarities through an optimized weight\nparameter and selects eigenvectors using a novel eigenvalue-silhouette\noptimization approach. Evaluated on a dataset comprising 1,428 small and\nmedium-sized enterprises (SMEs), ASC achieves a Silhouette score that is 18%\nhigher than that of a single-type data baseline method. Furthermore, the\nresulting clusters offer actionable insights; for instance, 51% of low-risk\nfirms are found to include the term 'social recruitment' in their textual\nrecords. The robustness of ASC is confirmed across multiple clustering\nalgorithms, including k-means, k-medians, and k-medoids, with\n{\\Delta}Intra/Inter < 0.13 and {\\Delta}Silhouette Coefficient < 0.02. By\nbridging spectral clustering theory with heterogeneous data applications, ASC\nenables the identification of meaningful clusters, such as recruitment-focused\nSMEs exhibiting a 30% lower default risk, thereby supporting more targeted and\neffective credit interventions."}
{"id": "2509.00710", "pdf": "https://arxiv.org/pdf/2509.00710", "abs": "https://arxiv.org/abs/2509.00710", "authors": ["Albert Sadowski", "Jarosław A. Chudziak"], "title": "On Verifiable Legal Reasoning: A Multi-Agent Framework with Formalized Knowledge Representations", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted for publication at the 34th ACM International Conference on\n  Information and Knowledge Management (CIKM '25)", "summary": "Legal reasoning requires both precise interpretation of statutory language\nand consistent application of complex rules, presenting significant challenges\nfor AI systems. This paper introduces a modular multi-agent framework that\ndecomposes legal reasoning into distinct knowledge acquisition and application\nstages. In the first stage, specialized agents extract legal concepts and\nformalize rules to create verifiable intermediate representations of statutes.\nThe second stage applies this knowledge to specific cases through three steps:\nanalyzing queries to map case facts onto the ontology schema, performing\nsymbolic inference to derive logically entailed conclusions, and generating\nfinal answers using a programmatic implementation that operationalizes the\nontological knowledge. This bridging of natural language understanding with\nsymbolic reasoning provides explicit and verifiable inspection points,\nsignificantly enhancing transparency compared to end-to-end approaches.\nEvaluation on statutory tax calculation tasks demonstrates substantial\nimprovements, with foundational models achieving 76.4\\% accuracy compared to\n18.8\\% baseline performance, effectively narrowing the performance gap between\nreasoning and foundational models. These findings suggest that modular\narchitectures with formalized knowledge representations can make sophisticated\nlegal reasoning more accessible through computationally efficient models while\nenhancing consistency and explainability in AI legal reasoning, establishing a\nfoundation for future research into more transparent, trustworthy, and\neffective AI systems for legal domain."}
{"id": "2509.00761", "pdf": "https://arxiv.org/pdf/2509.00761", "abs": "https://arxiv.org/abs/2509.00761", "authors": ["Ziqi Wang", "Boqin Yuan"], "title": "L-MARS -- Legal Multi-Agent Workflow with Orchestrated Reasoning and Agentic Search", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "We present L-MARS (Legal Multi-Agent Workflow with Orchestrated Reasoning and\nAgentic Search), a system that reduces hallucination and uncertainty in legal\nquestion answering through coordinated multi-agent reasoning and retrieval.\nUnlike single-pass retrieval-augmented generation (RAG), L-MARS decomposes\nqueries into subproblems, issues targeted searches across heterogeneous sources\n(Serper web, local RAG, CourtListener case law), and employs a Judge Agent to\nverify sufficiency, jurisdiction, and temporal validity before answer\nsynthesis. This iterative reasoning-search-verification loop maintains\ncoherence, filters noisy evidence, and grounds answers in authoritative law. We\nevaluated L-MARS on LegalSearchQA, a new benchmark of 200 up-to-date multiple\nchoice legal questions in 2025. Results show that L-MARS substantially improves\nfactual accuracy, reduces uncertainty, and achieves higher preference scores\nfrom both human experts and LLM-based judges. Our work demonstrates that\nmulti-agent reasoning with agentic search offers a scalable and reproducible\nblueprint for deploying LLMs in high-stakes domains requiring precise legal\nretrieval and deliberation."}
{"id": "2509.00768", "pdf": "https://arxiv.org/pdf/2509.00768", "abs": "https://arxiv.org/abs/2509.00768", "authors": ["Lee Hyun", "Sohee Yoon", "Jinwoo Park", "Sue In Chae", "Seongeon Park", "Jooyeon Ahn", "Yebin Jung", "Youjung Chung", "Hogeun Chang", "Myeonginn Kang", "Jina Kim", "Ho-Gyeong Kim", "Myeonghun Jeong"], "title": "Aligning Reasoning LLMs for Materials Discovery with Physics-aware Rejection Sampling", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cs.CL"], "comment": "14 pages, 5 figures", "summary": "AI-driven materials discovery that couples automated experimentation with\nalgorithmic decision-making requires process aware recipe to property\npredictors that are accurate, calibrated, and physically admissible. We\napproach this as a reasoning problem with large reasoning models (LRMs). To\ninstill reasoning capability into language models, we curate reasoning traces\nfrom a teacher model to train a student model. However, most training pipelines\nselect reasoning traces using binary correctness or learned preference signals\nthat poorly reflect physical admissibility. We introduce Physics-aware\nRejection Sampling (PaRS), a training-time trace selection scheme that favors\ntraces consistent with fundamental physics and numerically close to targets,\nwith lightweight halting to control compute. We instantiate our framework with\na large student model fine-tuned on traces synthesized by a larger teacher\nmodel, and evaluate under matched token budgets against various rejection\nsampling baselines. Our method improves accuracy and calibration, reduces\nphysics-violation rates, and lowers sampling cost relative to baselines. These\nresults indicate that modest, domain-aware constraints combined with\ntrace-level selection provide a practical path toward reliable, efficient LRMs\nfor process-aware property prediction and closed-loop materials design."}
{"id": "2509.00891", "pdf": "https://arxiv.org/pdf/2509.00891", "abs": "https://arxiv.org/abs/2509.00891", "authors": ["Zonghai Yao", "Talha Chafekar", "Junda Wang", "Shuo Han", "Feiyun Ouyang", "Junhui Qian", "Lingxi Li", "Hong Yu"], "title": "ChatCLIDS: Simulating Persuasive AI Dialogues to Promote Closed-Loop Insulin Adoption in Type 1 Diabetes Care", "categories": ["cs.AI", "cs.CL"], "comment": "Equal contribution for the first two authors", "summary": "Real-world adoption of closed-loop insulin delivery systems (CLIDS) in type 1\ndiabetes remains low, driven not by technical failure, but by diverse\nbehavioral, psychosocial, and social barriers. We introduce ChatCLIDS, the\nfirst benchmark to rigorously evaluate LLM-driven persuasive dialogue for\nhealth behavior change. Our framework features a library of expert-validated\nvirtual patients, each with clinically grounded, heterogeneous profiles and\nrealistic adoption barriers, and simulates multi-turn interactions with nurse\nagents equipped with a diverse set of evidence-based persuasive strategies.\nChatCLIDS uniquely supports longitudinal counseling and adversarial social\ninfluence scenarios, enabling robust, multi-dimensional evaluation. Our\nfindings reveal that while larger and more reflective LLMs adapt strategies\nover time, all models struggle to overcome resistance, especially under\nrealistic social pressure. These results highlight critical limitations of\ncurrent LLMs for behavior change, and offer a high-fidelity, scalable testbed\nfor advancing trustworthy persuasive AI in healthcare and beyond."}
{"id": "2509.00925", "pdf": "https://arxiv.org/pdf/2509.00925", "abs": "https://arxiv.org/abs/2509.00925", "authors": ["Aman Sharma", "Saeed Najafi", "Parsa Farinneya", "Benyamin Jamialahmadi", "Marzieh S. Tahaei", "Yuhe Fan", "Mehdi Rezagholizadeh", "Boxing Chen", "Aref Jafari"], "title": "DTRNet: Dynamic Token Routing Network to Reduce Quadratic Costs in Transformers", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformers achieve state-of-the-art results across many tasks, but their\nuniform application of quadratic self-attention to every token at every layer\nmakes them computationally expensive. We introduce DTRNet (Dynamic Token\nRouting Network), an improved Transformer architecture that allows tokens to\ndynamically skip the quadratic cost of cross-token mixing while still receiving\nlightweight linear updates. By preserving the MLP module and reducing the\nattention cost for most tokens to linear, DTRNet ensures that every token is\nexplicitly updated while significantly lowering overall computation. This\ndesign offers an efficient and effective alternative to standard dense\nattention. Once trained, DTRNet blocks routes only ~10% of tokens through\nattention at each layer while maintaining performance comparable to a full\nTransformer. It consistently outperforms routing-based layer skipping methods\nsuch as MoD and D-LLM in both accuracy and memory at matched FLOPs, while\nrouting fewer tokens to full attention. Its efficiency gains, scales with\nsequence length, offering significant reduction in FLOPs for long-context\ninputs. By decoupling token updates from attention mixing, DTRNet substantially\nreduces the quadratic share of computation, providing a simple, efficient, and\nscalable alternative to Transformers."}
{"id": "2509.00975", "pdf": "https://arxiv.org/pdf/2509.00975", "abs": "https://arxiv.org/abs/2509.00975", "authors": ["Zifeng Ding", "Shenyang Huang", "Zeyu Cao", "Emma Kondrup", "Zachary Yang", "Xingyue Huang", "Yuan Sui", "Zhangdie Yuan", "Yuqicheng Zhu", "Xianglong Hu", "Yuan He", "Farimah Poursafaei", "Michael Bronstein", "Andreas Vlachos"], "title": "Self-Exploring Language Models for Explainable Link Forecasting on Temporal Graphs via Reinforcement Learning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Forecasting future links is a central task in temporal graph (TG) reasoning,\nrequiring models to leverage historical interactions to predict upcoming ones.\nTraditional neural approaches, such as temporal graph neural networks, achieve\nstrong performance but lack explainability and cannot be applied to unseen\ngraphs without retraining. Recent studies have begun to explore using large\nlanguage models (LLMs) for graph reasoning, but most of them are constrained to\nstatic graphs or small synthetic TGs and lack the evaluation of the quality of\nreasoning traces generated by LLMs. In this work, we present Reasoning-Enhanced\nLearning for Temporal Graphs (ReaL-TG), a reinforcement learning framework that\nfine-tunes LLMs to perform explainable link forecasting on real-world TGs.\nReaL-TG uses outcome-based reward to encourage models to self-explore reasoning\nstrategies from graph structure and to produce explanations that directly\njustify their predictions. To enable evaluation on LLM-generated reasoning\ntraces, we propose a new evaluation protocol combining ranking metrics with an\nLLM-as-a-Judge system that assesses both the quality of reasoning and the\nimpact of hallucinations. Experiments with ReaL-TG-4B, obtained by fine-tuning\nQwen3-4B under our framework, show that it outperforms much larger frontier\nLLMs, including GPT-5 mini, on ranking metrics, while producing high-quality\nexplanations confirmed by both the LLM judge and human evaluation."}
{"id": "2509.00990", "pdf": "https://arxiv.org/pdf/2509.00990", "abs": "https://arxiv.org/abs/2509.00990", "authors": ["Deepak Bastola", "Woohyeok Choi"], "title": "Hybrid Topic-Semantic Labeling and Graph Embeddings for Unsupervised Legal Document Clustering", "categories": ["stat.ML", "cs.CL", "cs.LG"], "comment": "20 pages, 8 figures, 3 tables", "summary": "Legal documents pose unique challenges for text classification due to their\ndomain-specific language and often limited labeled data. This paper proposes a\nhybrid approach for classifying legal texts by combining unsupervised topic and\ngraph embeddings with a supervised model. We employ Top2Vec to learn semantic\ndocument embeddings and automatically discover latent topics, and Node2Vec to\ncapture structural relationships via a bipartite graph of legal documents. The\nembeddings are combined and clustered using KMeans, yielding coherent groupings\nof documents. Our computations on a legal document dataset demonstrate that the\ncombined Top2Vec+Node2Vec approach improves clustering quality over text-only\nor graph-only embeddings. We conduct a sensitivity analysis of hyperparameters,\nsuch as the number of clusters and the dimensionality of the embeddings, and\ndemonstrate that our method achieves competitive performance against baseline\nLatent Dirichlet Allocation (LDA) and Non-Negative Matrix Factorization (NMF)\nmodels. Key findings indicate that while the pipeline presents an innovative\napproach to unsupervised legal document analysis by combining semantic topic\nmodeling with graph embedding techniques, its efficacy is contingent upon the\nquality of initial topic generation and the representational power of the\nchosen embedding models for specialized legal language. Strategic\nrecommendations include the exploration of domain-specific embeddings, more\ncomprehensive hyperparameter tuning for Node2Vec, dynamic determination of\ncluster numbers, and robust human-in-the-loop validation processes to enhance\nlegal relevance and trustworthiness. The pipeline demonstrates potential for\nexploratory legal data analysis and as a precursor to supervised learning tasks\nbut requires further refinement and domain-specific adaptation for practical\nlegal applications."}
{"id": "2509.00996", "pdf": "https://arxiv.org/pdf/2509.00996", "abs": "https://arxiv.org/abs/2509.00996", "authors": ["Runjia Zeng", "Guangyan Sun", "Qifan Wang", "Tong Geng", "Sohail Dianat", "Xiaotian Han", "Raghuveer Rao", "Xueling Zhang", "Cheng Han", "Lifu Huang", "Dongfang Liu"], "title": "MEPT: Mixture of Expert Prompt Tuning as a Manifold Mapper", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "EMNLP 2025", "summary": "Considering deep neural networks as manifold mappers, the\npretrain-then-fine-tune paradigm can be interpreted as a two-stage process:\npretrain establishes a broad knowledge base, and fine-tune adjusts the model\nparameters to activate specific neural pathways to align with the target\nmanifold. Although prior fine-tuning approaches demonstrate success, their\nrigid parameter space limits their ability to dynamically activate appropriate\nneural pathways, rendering them ill-equipped to adapt flexibly to the diverse\nand evolving data distributions. In light of this view, we propose a novel\napproach, Mixture of Expert Prompt Tuning (MEPT), as an effective and efficient\nmanifold-mapping framework. MEPT leverages the Mixture of Experts architecture\nby integrating multiple prompt experts to adaptively learn diverse and\nnon-stationary data distributions. Empirical evaluations demonstrate that MEPT\noutperforms several state-of-the-art parameter efficient baselines on\nSuperGLUE, achieving notable improvements in mean accuracy (e.g., 1.94%) while\nsignificantly reducing activated prompts by 79.25%. The effectiveness of MEPT\nis further supported by theoretical insights from manifold learning and\nvalidated through neural activation pathway visualization results. Our code is\navaliable at https://github.com/runtsang/MEPT."}
{"id": "2509.01016", "pdf": "https://arxiv.org/pdf/2509.01016", "abs": "https://arxiv.org/abs/2509.01016", "authors": ["Aishni Parab", "Hongjing Lu", "Ying Nian Wu", "Sumit Gulwani"], "title": "Analysis of Error Sources in LLM-based Hypothesis Search for Few-Shot Rule Induction", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.NE"], "comment": "This is the preprint version corresponding to our NeurIPS 2025\n  Workshop on Multimodal Algorithmic Reasoning submission", "summary": "Inductive reasoning enables humans to infer abstract rules from limited\nexamples and apply them to novel situations. In this work, we compare an\nLLM-based hypothesis search framework with direct program generation approaches\non few-shot rule induction tasks. Our findings show that hypothesis search\nachieves performance comparable to humans, while direct program generation\nfalls notably behind. An error analysis reveals key bottlenecks in hypothesis\ngeneration and suggests directions for advancing program induction methods.\nOverall, this paper underscores the potential of LLM-based hypothesis search\nfor modeling inductive reasoning and the challenges in building more efficient\nsystems."}
{"id": "2509.01051", "pdf": "https://arxiv.org/pdf/2509.01051", "abs": "https://arxiv.org/abs/2509.01051", "authors": ["Matte Lim", "Catherine Yeh", "Martin Wattenberg", "Fernanda Viégas", "Panagiotis Michalatos"], "title": "Chronotome: Real-Time Topic Modeling for Streaming Embedding Spaces", "categories": ["cs.HC", "cs.CL", "cs.CV", "cs.LG"], "comment": "Accepted to IEEE VIS 2025 Short Paper Track (5 pages, 4 figures)", "summary": "Many real-world datasets -- from an artist's body of work to a person's\nsocial media history -- exhibit meaningful semantic changes over time that are\ndifficult to capture with existing dimensionality reduction methods. To address\nthis gap, we introduce a visualization technique that combines force-based\nprojection and streaming clustering methods to build a spatial-temporal map of\nembeddings. Applying this technique, we create Chronotome, a tool for\ninteractively exploring evolving themes in time-based data -- in real time. We\ndemonstrate the utility of our approach through use cases on text and image\ndata, showing how it offers a new lens for understanding the aesthetics and\nsemantics of temporal datasets."}
{"id": "2509.01052", "pdf": "https://arxiv.org/pdf/2509.01052", "abs": "https://arxiv.org/abs/2509.01052", "authors": ["Jaewoo Ahn", "Junseo Kim", "Heeseung Yun", "Jaehyeon Son", "Dongmin Park", "Jaewoong Cho", "Gunhee Kim"], "title": "FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "EMNLP 2025 Main. Project page:\n  https://ahnjaewoo.github.io/flashadventure", "summary": "GUI agents powered by LLMs show promise in interacting with diverse digital\nenvironments. Among these, video games offer a valuable testbed due to their\nvaried interfaces, with adventure games posing additional challenges through\ncomplex, narrative-driven interactions. Existing game benchmarks, however, lack\ndiversity and rarely evaluate agents on completing entire storylines. To\naddress this, we introduce FlashAdventure, a benchmark of 34 Flash-based\nadventure games designed to test full story arc completion and tackle the\nobservation-behavior gap: the challenge of remembering and acting on earlier\ngameplay information. We also propose CUA-as-a-Judge, an automated gameplay\nevaluator, and COAST, an agentic framework leveraging long-term clue memory to\nbetter plan and solve sequential tasks. Experiments show current GUI agents\nstruggle with full story arcs, while COAST improves milestone completion by\nbridging the observation-behavior gap. Nonetheless, a marked discrepancy\nbetween humans and best-performing agents warrants continued research efforts\nto narrow this divide."}
{"id": "2509.01055", "pdf": "https://arxiv.org/pdf/2509.01055", "abs": "https://arxiv.org/abs/2509.01055", "authors": ["Dongfu Jiang", "Yi Lu", "Zhuofeng Li", "Zhiheng Lyu", "Ping Nie", "Haozhe Wang", "Alex Su", "Hui Chen", "Kai Zou", "Chao Du", "Tianyu Pang", "Wenhu Chen"], "title": "VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "32 pages, 5 figures, 13 tables", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated\nsuccess in enhancing LLM reasoning capabilities, but remains limited to\nsingle-turn interactions without tool integration. While recent Agentic\nReinforcement Learning with Tool use (ARLT) approaches have emerged to address\nmulti-turn tool interactions, existing works develop task-specific codebases\nthat suffer from fragmentation, synchronous execution bottlenecks, and limited\nextensibility across domains. These inefficiencies hinder broader community\nadoption and algorithmic innovation. We introduce VerlTool, a unified and\nmodular framework that addresses these limitations through systematic design\nprinciples. VerlTool provides four key contributions: (1) upstream alignment\nwith VeRL ensuring compatibility and simplified maintenance, (2) unified tool\nmanagement via standardized APIs supporting diverse modalities including code\nexecution, search, SQL databases, and vision processing, (3) asynchronous\nrollout execution achieving near 2$\\times$ speedup by eliminating\nsynchronization bottlenecks, and (4) comprehensive evaluation demonstrating\ncompetitive performance across 6 ARLT domains. Our framework formalizes ARLT as\nmulti-turn trajectories with multi-modal observation tokens (text/image/video),\nextending beyond single-turn RLVR paradigms. We train and evaluate models on\nmathematical reasoning, knowledge QA, SQL generation, visual reasoning, web\nsearch, and software engineering tasks, achieving results comparable to\nspecialized systems while providing unified training infrastructure. The\nmodular plugin architecture enables rapid tool integration requiring only\nlightweight Python definitions, significantly reducing development overhead and\nproviding a scalable foundation for tool-augmented RL research. Our code is\nopen-sourced at https://github.com/TIGER-AI-Lab/verl-tool."}
{"id": "2509.01167", "pdf": "https://arxiv.org/pdf/2509.01167", "abs": "https://arxiv.org/abs/2509.01167", "authors": ["Hyunjong Ok", "Jaeho Lee"], "title": "Do Video Language Models Really Know Where to Look? Diagnosing Attention Failures in Video Language Models", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "preprint", "summary": "Recent advances in multimodal large language models (MLLMs) have led to much\nprogress in video understanding tasks. To avoid the heavy computational cost of\nprocessing all frames, these models typically rely on keyframe sampling methods\nguided by vision-language encoders (\\textit{e.g.,} SigLIP). However, it remains\nunclear whether such encoders can truly identify the most informative frames.\nIn this work, we provide several empirical pieces of evidence revealing that\npopular vision encoders critically suffer from their limited capability to\nidentify where the MLLM should look inside the video to handle the given\ntextual query appropriately. Our findings suggest that the development of\nbetter keyframe identification techniques may be necessary for efficient video\nMLLMs."}
{"id": "2509.01182", "pdf": "https://arxiv.org/pdf/2509.01182", "abs": "https://arxiv.org/abs/2509.01182", "authors": ["Wonduk Seo", "Taesub Shin", "Hyunjin An", "Dokyun Kim", "Seunghyun Lee"], "title": "Question-to-Knowledge: Multi-Agent Generation of Inspectable Facts for Product Mapping", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.IR", "cs.MA"], "comment": "Preprint", "summary": "Identifying whether two product listings refer to the same Stock Keeping Unit\n(SKU) is a persistent challenge in ecommerce, especially when explicit\nidentifiers are missing and product names vary widely across platforms. Rule\nbased heuristics and keyword similarity often misclassify products by\noverlooking subtle distinctions in brand, specification, or bundle\nconfiguration. To overcome these limitations, we propose Question to Knowledge\n(Q2K), a multi agent framework that leverages Large Language Models (LLMs) for\nreliable SKU mapping. Q2K integrates: (1) a Reasoning Agent that generates\ntargeted disambiguation questions, (2) a Knowledge Agent that resolves them via\nfocused web searches, and (3) a Deduplication Agent that reuses validated\nreasoning traces to reduce redundancy and ensure consistency. A human in the\nloop mechanism further refines uncertain cases. Experiments on real world\nconsumer goods datasets show that Q2K surpasses strong baselines, achieving\nhigher accuracy and robustness in difficult scenarios such as bundle\nidentification and brand origin disambiguation. By reusing retrieved reasoning\ninstead of issuing repeated searches, Q2K balances accuracy with efficiency,\noffering a scalable and interpretable solution for product integration."}
{"id": "2509.01308", "pdf": "https://arxiv.org/pdf/2509.01308", "abs": "https://arxiv.org/abs/2509.01308", "authors": ["Mattia Tritto", "Giuseppe Farano", "Dario Di Palma", "Gaetano Rossiello", "Fedelucio Narducci", "Dharmashankar Subramanian", "Tommaso Di Noia"], "title": "GradeSQL: Outcome Reward Models for Ranking SQL Queries from Large Language Models", "categories": ["cs.AI", "cs.CL", "cs.DB"], "comment": null, "summary": "Text-to-SQL, the task of translating natural language questions into SQL\nqueries, has significantly advanced with the introduction of Large Language\nModels (LLMs), broadening database accessibility for a wide range of users.\nDespite substantial progress in generating valid SQL, current LLMs still\nstruggle with complex queries that require precise alignment between user\nintent and the database schema. To mitigate this, test-time strategies such as\nBest-of-N (BoN) and Majority Voting (Maj) are often employed, based on the\nassumption that LLMs can generate correct answers but may require multiple\nattempts. However, these methods rely on surface-level heuristics, selecting\neither the syntactically correct query through execution-based BoN (ex-BoN) or\nthe most frequently generated query with Maj. Recently, Outcome Reward Models\n(ORMs), which assign utility scores to generated outputs based on semantic\ncorrectness, have emerged as a promising approach for better aligning model\npredictions with user intent. Nevertheless, their application to Text-to-SQL\nremains largely underexplored.\n  In this work, we evaluate ORMs as an effective heuristic for BoN, compare\nthem with ex-BoN and Maj, and introduce a framework for training ORMs for the\nText-to-SQL task. We evaluate our ORMs on the BIRD and SPIDER benchmarks,\nfinetuning various open-source LLMs, including the Qwen2, Granite3, and Llama3\nmodel families. Our results show that ORMs outperform ex-BoN and Maj, achieving\nexecution accuracy gains of +4.33% (BIRD) and +2.10% (Spider) over ex-BoN, and\n+2.91% (BIRD) and +0.93% (Spider) over Maj. We further demonstrate that\nfinetuning models already aligned with SQL generation, such as OmniSQL, yields\nsuperior ORM performance. Additionally, we observe that ORMs achieve\ncompetitive results on simple queries and benefit more from an increased number\nof candidates compared to ex-BoN and Maj."}
{"id": "2509.01321", "pdf": "https://arxiv.org/pdf/2509.01321", "abs": "https://arxiv.org/abs/2509.01321", "authors": ["Xinyu Tang", "Zhenduo Zhang", "Yurou Liu", "Wayne Xin Zhao", "Zujie Wen", "Zhiqiang Zhang", "Jun Zhou"], "title": "Towards High Data Efficiency in Reinforcement Learning with Verifiable Reward", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent advances in large reasoning models have leveraged reinforcement\nlearning with verifiable rewards (RLVR) to improve reasoning capabilities.\nHowever, scaling these methods typically requires extensive rollout computation\nand large datasets, leading to high training costs and low data efficiency. To\nmitigate this issue, we propose DEPO, a Data-Efficient Policy Optimization\npipeline that combines optimized strategies for both offline and online data\nselection. In the offline phase, we curate a high-quality subset of training\nsamples based on diversity, influence, and appropriate difficulty. During\nonline RLVR training, we introduce a sample-level explorability metric to\ndynamically filter samples with low exploration potential, thereby reducing\nsubstantial rollout computational costs. Furthermore, we incorporate a replay\nmechanism for under-explored samples to ensure adequate training, which\nenhances the model's final convergence performance. Experiments across five\nreasoning benchmarks show that DEPO consistently outperforms existing methods\nin both offline and online data selection scenarios. Notably, using only 20% of\nthe training data, our approach achieves a 1.85 times speed-up on AIME24 and a\n1.66 times speed-up on AIME25 compared to GRPO trained on the full dataset."}
{"id": "2509.01337", "pdf": "https://arxiv.org/pdf/2509.01337", "abs": "https://arxiv.org/abs/2509.01337", "authors": ["Qianrui Zhou", "Hua Xu", "Yifan Wang", "Xinzhi Dong", "Hanlei Zhang"], "title": "LLM-Guided Semantic Relational Reasoning for Multimodal Intent Recognition", "categories": ["cs.MM", "cs.AI", "cs.CL"], "comment": "Accepted by EMNLP 2025 (Main Track, Long Paper)", "summary": "Understanding human intents from multimodal signals is critical for analyzing\nhuman behaviors and enhancing human-machine interactions in real-world\nscenarios. However, existing methods exhibit limitations in their\nmodality-level reliance, constraining relational reasoning over fine-grained\nsemantics for complex intent understanding. This paper proposes a novel\nLLM-Guided Semantic Relational Reasoning (LGSRR) method, which harnesses the\nexpansive knowledge of large language models (LLMs) to establish semantic\nfoundations that boost smaller models' relational reasoning performance.\nSpecifically, an LLM-based strategy is proposed to extract fine-grained\nsemantics as guidance for subsequent reasoning, driven by a shallow-to-deep\nChain-of-Thought (CoT) that autonomously uncovers, describes, and ranks\nsemantic cues by their importance without relying on manually defined priors.\nBesides, we formally model three fundamental types of semantic relations\ngrounded in logical principles and analyze their nuanced interplay to enable\nmore effective relational reasoning. Extensive experiments on multimodal intent\nand dialogue act recognition tasks demonstrate LGSRR's superiority over\nstate-of-the-art methods, with consistent performance gains across diverse\nsemantic understanding scenarios. The complete data and code are available at\nhttps://github.com/thuiar/LGSRR."}
{"id": "2509.01391", "pdf": "https://arxiv.org/pdf/2509.01391", "abs": "https://arxiv.org/abs/2509.01391", "authors": ["Joonyong Park", "Daisuke Saito", "Nobuaki Minematsu"], "title": "MixedG2P-T5: G2P-free Speech Synthesis for Mixed-script texts using Speech Self-Supervised Learning and Language Model", "categories": ["eess.AS", "cs.CL"], "comment": "In Proceedings of the 17th Asia Pacific Signal and Information\n  Processing Association Annual Summit and Conference (APSIPA ASC 2025)", "summary": "This study presents a novel approach to voice synthesis that can substitute\nthe traditional grapheme-to-phoneme (G2P) conversion by using a deep\nlearning-based model that generates discrete tokens directly from speech.\nUtilizing a pre-trained voice SSL model, we train a T5 encoder to produce\npseudo-language labels from mixed-script texts (e.g., containing Kanji and\nKana). This method eliminates the need for manual phonetic transcription,\nreducing costs and enhancing scalability, especially for large non-transcribed\naudio datasets. Our model matches the performance of conventional G2P-based\ntext-to-speech systems and is capable of synthesizing speech that retains\nnatural linguistic and paralinguistic features, such as accents and\nintonations."}
{"id": "2509.01401", "pdf": "https://arxiv.org/pdf/2509.01401", "abs": "https://arxiv.org/abs/2509.01401", "authors": ["Ali Abouzeid", "Bilal Elbouardi", "Mohamed Maged", "Shady Shehata"], "title": "ArabEmoNet: A Lightweight Hybrid 2D CNN-BiLSTM Model with Attention for Robust Arabic Speech Emotion Recognition", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted (The Third Arabic Natural Language Processing Conference)", "summary": "Speech emotion recognition is vital for human-computer interaction,\nparticularly for low-resource languages like Arabic, which face challenges due\nto limited data and research. We introduce ArabEmoNet, a lightweight\narchitecture designed to overcome these limitations and deliver\nstate-of-the-art performance. Unlike previous systems relying on discrete MFCC\nfeatures and 1D convolutions, which miss nuanced spectro-temporal patterns,\nArabEmoNet uses Mel spectrograms processed through 2D convolutions, preserving\ncritical emotional cues often lost in traditional methods.\n  While recent models favor large-scale architectures with millions of\nparameters, ArabEmoNet achieves superior results with just 1 million\nparameters, 90 times smaller than HuBERT base and 74 times smaller than\nWhisper. This efficiency makes it ideal for resource-constrained environments.\nArabEmoNet advances Arabic speech emotion recognition, offering exceptional\nperformance and accessibility for real-world applications."}
{"id": "2509.01566", "pdf": "https://arxiv.org/pdf/2509.01566", "abs": "https://arxiv.org/abs/2509.01566", "authors": ["Yujing Wang", "Yiren Chen", "Huoran Li", "Chunxu Xu", "Yuchong Luo", "Xianghui Mao", "Cong Li", "Lun Du", "Chunyang Ma", "Qiqi Jiang", "Yin Wang", "Fan Gao", "Wenting Mo", "Pei Wen", "Shantanu Kumar", "Taejin Park", "Yiwei Song", "Vijay Rajaram", "Tao Cheng", "Sonu Durgia", "Pranam Kolari"], "title": "CSRM-LLM: Embracing Multilingual LLMs for Cold-Start Relevance Matching in Emerging E-commerce Markets", "categories": ["cs.IR", "cs.CL"], "comment": "7 pages, 3 figures", "summary": "As global e-commerce platforms continue to expand, companies are entering new\nmarkets where they encounter cold-start challenges due to limited human labels\nand user behaviors. In this paper, we share our experiences in Coupang to\nprovide a competitive cold-start performance of relevance matching for emerging\ne-commerce markets. Specifically, we present a Cold-Start Relevance Matching\n(CSRM) framework, utilizing a multilingual Large Language Model (LLM) to\naddress three challenges: (1) activating cross-lingual transfer learning\nabilities of LLMs through machine translation tasks; (2) enhancing query\nunderstanding and incorporating e-commerce knowledge by retrieval-based query\naugmentation; (3) mitigating the impact of training label errors through a\nmulti-round self-distillation training strategy. Our experiments demonstrate\nthe effectiveness of CSRM-LLM and the proposed techniques, resulting in\nsuccessful real-world deployment and significant online gains, with a 45.8%\nreduction in defect ratio and a 0.866% uplift in session purchase rate."}
{"id": "2509.01656", "pdf": "https://arxiv.org/pdf/2509.01656", "abs": "https://arxiv.org/abs/2509.01656", "authors": ["Zetong Zhou", "Dongping Chen", "Zixian Ma", "Zhihan Hu", "Mingyang Fu", "Sinan Wang", "Yao Wan", "Zhou Zhao", "Ranjay Krishna"], "title": "Reinforced Visual Perception with Tools", "categories": ["cs.CV", "cs.CL"], "comment": "Technical Report", "summary": "Visual reasoning, a cornerstone of human intelligence, encompasses complex\nperceptual and logical processes essential for solving diverse visual problems.\nWhile advances in computer vision have produced powerful models for various\nperceptual tasks, leveraging these for general visual reasoning remains\nchallenging. Prior work demonstrates that augmenting LLMs with vision models\nvia supervised finetuning improves performance, but faces key limitations such\nas expensive data generation, reliance on careful data filtering, and poor\ngeneralization. To address these issues, we propose ReVPT to enhance\nmulti-modal LLMs' abilities to reason about and use visual tools through\nreinforcement learning. We introduce a novel RL algorithm based on GRPO,\ndesigned to train models to reason with a suite of four visual tools. Through\nextensive experiments, we show that our method achieves state-of-the-art\nperformance on several perception-heavy benchmarks, including SAT, CV-Bench,\nBLINK and MMStar, significantly outperforming the supervised and text-based RL\nfinetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the\ninstruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the\ncommunity new insights on RL-based visual tool-usage through extensive\nablations. Our code is available at https://github.com/ls-kelvin/REVPT."}
{"id": "2509.01716", "pdf": "https://arxiv.org/pdf/2509.01716", "abs": "https://arxiv.org/abs/2509.01716", "authors": ["Rui Zhao", "Vladyslav Melnychuk", "Jun Zhao", "Jesse Wright", "Nigel Shadbolt"], "title": "An LLM-enabled semantic-centric framework to consume privacy policies", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In modern times, people have numerous online accounts, but they rarely read\nthe Terms of Service or Privacy Policy of those sites, despite claiming\notherwise, due to the practical difficulty in comprehending them. The mist of\ndata privacy practices forms a major barrier for user-centred Web approaches,\nand for data sharing and reusing in an agentic world. Existing research\nproposed methods for using formal languages and reasoning for verifying the\ncompliance of a specified policy, as a potential cure for ignoring privacy\npolicies. However, a critical gap remains in the creation or acquisition of\nsuch formal policies at scale. We present a semantic-centric approach for using\nstate-of-the-art large language models (LLM), to automatically identify key\ninformation about privacy practices from privacy policies, and construct\n$\\mathit{Pr}^2\\mathit{Graph}$, knowledge graph with grounding from Data Privacy\nVocabulary (DPV) for privacy practices, to support downstream tasks. Along with\nthe pipeline, the $\\mathit{Pr}^2\\mathit{Graph}$ for the top-100 popular\nwebsites is also released as a public resource, by using the pipeline for\nanalysis. We also demonstrate how the $\\mathit{Pr}^2\\mathit{Graph}$ can be used\nto support downstream tasks by constructing formal policy representations such\nas Open Digital Right Language (ODRL) or perennial semantic Data Terms of Use\n(psDToU). To evaluate the technology capability, we enriched the Policy-IE\ndataset by employing legal experts to create custom annotations. We benchmarked\nthe performance of different large language models for our pipeline and\nverified their capabilities. Overall, they shed light on the possibility of\nlarge-scale analysis of online services' privacy practices, as a promising\ndirection to audit the Web and the Internet. We release all datasets and source\ncode as public resources to facilitate reuse and improvement."}
{"id": "2509.01813", "pdf": "https://arxiv.org/pdf/2509.01813", "abs": "https://arxiv.org/abs/2509.01813", "authors": ["Mingxuan Cui", "Yilan Jiang", "Duo Zhou", "Cheng Qian", "Yuji Zhang", "Qiong Wang"], "title": "ShortageSim: Simulating Drug Shortages under Information Asymmetry", "categories": ["cs.MA", "cs.CL", "cs.GT"], "comment": "21 Pages", "summary": "Drug shortages pose critical risks to patient care and healthcare systems\nworldwide, yet the effectiveness of regulatory interventions remains poorly\nunderstood due to fundamental information asymmetries in pharmaceutical supply\nchains. We present \\textbf{ShortageSim}, the first Large Language Model\n(LLM)-based multi-agent simulation framework that captures the complex,\nstrategic interactions between drug manufacturers, institutional buyers, and\nregulatory agencies in response to shortage alerts. Unlike traditional\ngame-theoretic models that assume perfect rationality and complete information,\n\\textbf{ShortageSim} leverages LLMs to simulate bounded-rational\ndecision-making under uncertainty. Through a sequential production game\nspanning multiple quarters, we model how FDA announcements, both reactive\nalerts about existing shortages and proactive warnings about potential\ndisruptions, propagate through the supply chain and influence capacity\ninvestment and procurement decisions. Our experiments on historical shortage\nevents reveal that \\textbf{ShortageSim} reduces the resolution-lag percentage\nfor discontinued-disclosed cases by 83\\%, bringing simulated durations more\naligned to ground truth than the zero-shot baseline. We open-source\n\\textbf{ShortageSim} and a dataset of 2,925 FDA shortage events at\nhttps://github.com/Lemutisme/Sortage_Management, providing a novel\ncomputational framework for designing and testing interventions in complex,\ninformation-scarce supply chains."}
{"id": "2509.01907", "pdf": "https://arxiv.org/pdf/2509.01907", "abs": "https://arxiv.org/abs/2509.01907", "authors": ["Zhenyuan Chen", "Chenxi Wang", "Ningyu Zhang", "Feng Zhang"], "title": "RSCC: A Large-Scale Remote Sensing Change Caption Dataset for Disaster Events", "categories": ["cs.CV", "cs.CL"], "comment": "under review", "summary": "Remote sensing is critical for disaster monitoring, yet existing datasets\nlack temporal image pairs and detailed textual annotations. While\nsingle-snapshot imagery dominates current resources, it fails to capture\ndynamic disaster impacts over time. To address this gap, we introduce the\nRemote Sensing Change Caption (RSCC) dataset, a large-scale benchmark\ncomprising 62,315 pre-/post-disaster image pairs (spanning earthquakes, floods,\nwildfires, and more) paired with rich, human-like change captions. By bridging\nthe temporal and semantic divide in remote sensing data, RSCC enables robust\ntraining and evaluation of vision-language models for disaster-aware\nbi-temporal understanding. Our results highlight RSCC's ability to facilitate\ndetailed disaster-related analysis, paving the way for more accurate,\ninterpretable, and scalable vision-language applications in remote sensing.\nCode and dataset are available at https://github.com/Bili-Sakura/RSCC."}
{"id": "2509.01909", "pdf": "https://arxiv.org/pdf/2509.01909", "abs": "https://arxiv.org/abs/2509.01909", "authors": ["Ranjie Duan", "Jiexi Liu", "Xiaojun Jia", "Shiji Zhao", "Ruoxi Cheng", "Fengxiang Wang", "Cheng Wei", "Yong Xie", "Chang Liu", "Defeng Li", "Yinpeng Dong", "Yichi Zhang", "Yuefeng Chen", "Chongwen Wang", "Xingjun Ma", "Xingxing Wei", "Yang Liu", "Hang Su", "Jun Zhu", "Xinfeng Li", "Yitong Sun", "Jie Zhang", "Jinzhao Hu", "Sha Xu", "Yitong Yang", "Jialing Tao", "Hui Xue"], "title": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.HC", "cs.SC"], "comment": "Technical Report", "summary": "Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI."}
{"id": "2509.01914", "pdf": "https://arxiv.org/pdf/2509.01914", "abs": "https://arxiv.org/abs/2509.01914", "authors": ["Ruijia Li", "Yuan-Hao Jiang", "Jiatong Wang", "Bo Jiang"], "title": "How Real Is AI Tutoring? Comparing Simulated and Human Dialogues in One-on-One Instruction", "categories": ["cs.AI", "cs.CL", "cs.MA"], "comment": "Proceedings of the 33rd International Conference on Computers in\n  Education (ICCE 2025). Asia-Pacific Society for Computers in Education", "summary": "Heuristic and scaffolded teacher-student dialogues are widely regarded as\ncritical for fostering students' higher-order thinking and deep learning.\nHowever, large language models (LLMs) currently face challenges in generating\npedagogically rich interactions. This study systematically investigates the\nstructural and behavioral differences between AI-simulated and authentic human\ntutoring dialogues. We conducted a quantitative comparison using an\nInitiation-Response-Feedback (IRF) coding scheme and Epistemic Network Analysis\n(ENA). The results show that human dialogues are significantly superior to\ntheir AI counterparts in utterance length, as well as in questioning (I-Q) and\ngeneral feedback (F-F) behaviors. More importantly, ENA results reveal a\nfundamental divergence in interactional patterns: human dialogues are more\ncognitively guided and diverse, centered around a \"question-factual\nresponse-feedback\" teaching loop that clearly reflects pedagogical guidance and\nstudent-driven thinking; in contrast, simulated dialogues exhibit a pattern of\nstructural simplification and behavioral convergence, revolving around an\n\"explanation-simplistic response\" loop that is essentially a simple information\ntransfer between the teacher and student. These findings illuminate key\nlimitations in current AI-generated tutoring and provide empirical guidance for\ndesigning and evaluating more pedagogically effective generative educational\ndialogue systems."}
{"id": "2509.01938", "pdf": "https://arxiv.org/pdf/2509.01938", "abs": "https://arxiv.org/abs/2509.01938", "authors": ["Jonathn Chang", "Leonard Piff", "Suvadip Sana", "Jasmine X. Li", "Lionel Levine"], "title": "EigenBench: A Comparative Behavioral Measure of Value Alignment", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Aligning AI with human values is a pressing unsolved problem. To address the\nlack of quantitative metrics for value alignment, we propose EigenBench: a\nblack-box method for comparatively benchmarking language models' values. Given\nan ensemble of models, a constitution describing a value system, and a dataset\nof scenarios, our method returns a vector of scores quantifying each model's\nalignment to the given constitution. To produce these scores, each model judges\nthe outputs of other models across many scenarios, and these judgments are\naggregated with EigenTrust (Kamvar et al, 2003), yielding scores that reflect a\nweighted-average judgment of the whole ensemble. EigenBench uses no ground\ntruth labels, as it is designed to quantify traits for which reasonable judges\nmay disagree on the correct label. Using prompted personas, we test whether\nEigenBench scores are more sensitive to the model or the prompt: we find that\nmost of the variance is explained by the prompt, but a small residual\nquantifies the disposition of the model itself."}
{"id": "2509.01954", "pdf": "https://arxiv.org/pdf/2509.01954", "abs": "https://arxiv.org/abs/2509.01954", "authors": ["Nirmalya Thakur", "Madeline D Hartel", "Lane Michael Boden", "Dallas Enriquez", "Boston Joyner Ricks"], "title": "Content and Engagement Trends in COVID-19 YouTube Videos: Evidence from the Late Pandemic", "categories": ["cs.SI", "cs.CL", "cs.CY", "cs.ET", "cs.LG", "I.2.7; I.2.8; I.5.4; K.4.2; H.2.8; I.2.6"], "comment": null, "summary": "This work investigated about 10,000 COVID-19-related YouTube videos published\nbetween January 2023 and October 2024 to evaluate how temporal, lexical,\nlinguistic, and structural factors influenced engagement during the late\npandemic period. Publishing activity showed consistent weekday effects: in the\nfirst window, average views peaked on Mondays at 92,658; in the second, on\nWednesdays at 115,479; and in the third, on Fridays at 84,874, reflecting a\nshift in audience attention toward mid- and late week. Lexical analysis of\nvideo titles revealed recurring high-frequency keywords related to COVID-19 and\nYouTube features, including COVID, coronavirus, shorts, and live. Frequency\nanalysis revealed sharp spikes, with COVID appearing in 799 video titles in\nAugust 2024, while engagement analysis showed that videos titled with shorts\nattracted very high views, peaking at 2.16 million average views per video in\nJune 2023. Analysis of sentiment of video descriptions in English showed weak\ncorrelation with views in the raw data (Pearson r = 0.0154, p = 0.2987), but\nstronger correlations emerged once outliers were addressed, with Spearman r =\n0.110 (p < 0.001) and Pearson r = 0.0925 (p < 0.001). Category-level analysis\nof video durations revealed contrasting outcomes: long videos focusing on\npeople and blogs averaged 209,114 views, short entertainment videos averaged\n288,675 views, and medium-to-long news and politics videos averaged 51,309 and\n59,226 views, respectively. These results demonstrate that engagement patterns\nof COVID-19-related videos on YouTube during the late pandemic followed\ndistinct characteristics driven by publishing schedules, title vocabulary,\ntopics, and genre-specific duration effects."}
{"id": "2509.02077", "pdf": "https://arxiv.org/pdf/2509.02077", "abs": "https://arxiv.org/abs/2509.02077", "authors": ["Refat Othman", "Diaeddin Rimawi", "Bruno Rossi", "Barbara Russo"], "title": "From Attack Descriptions to Vulnerabilities: A Sentence Transformer-Based Approach", "categories": ["cs.CR", "cs.CL", "cs.LG", "68T50 Natural language processing", "D.4.6; I.2.7"], "comment": "Accepted in The Journal of Systems and Software (2025)", "summary": "In the domain of security, vulnerabilities frequently remain undetected even\nafter their exploitation. In this work, vulnerabilities refer to publicly\ndisclosed flaws documented in Common Vulnerabilities and Exposures (CVE)\nreports. Establishing a connection between attacks and vulnerabilities is\nessential for enabling timely incident response, as it provides defenders with\nimmediate, actionable insights. However, manually mapping attacks to CVEs is\ninfeasible, thereby motivating the need for automation. This paper evaluates 14\nstate-of-the-art (SOTA) sentence transformers for automatically identifying\nvulnerabilities from textual descriptions of attacks. Our results demonstrate\nthat the multi-qa-mpnet-base-dot-v1 (MMPNet) model achieves superior\nclassification performance when using attack Technique descriptions, with an\nF1-score of 89.0, precision of 84.0, and recall of 94.7. Furthermore, it was\nobserved that, on average, 56% of the vulnerabilities identified by the MMPNet\nmodel are also represented within the CVE repository in conjunction with an\nattack, while 61% of the vulnerabilities detected by the model correspond to\nthose cataloged in the CVE repository. A manual inspection of the results\nrevealed the existence of 275 predicted links that were not documented in the\nMITRE repositories. Consequently, the automation of linking attack techniques\nto vulnerabilities not only enhances the detection and response capabilities\nrelated to software security incidents but also diminishes the duration during\nwhich vulnerabilities remain exploitable, thereby contributing to the\ndevelopment of more secure systems."}
{"id": "2509.02100", "pdf": "https://arxiv.org/pdf/2509.02100", "abs": "https://arxiv.org/abs/2509.02100", "authors": ["Sharjeel Tahir", "Judith Johnson", "Jumana Abu-Khalaf", "Syed Afaq Ali Shah"], "title": "E-THER: A PCT-Grounded Dataset for Benchmarking Empathic AI", "categories": ["cs.HC", "cs.CL"], "comment": "15 pages, 4 figures. Preprint", "summary": "A prevalent shortfall among current empathic AI systems is their inability to\nrecognize when verbal expressions may not fully reflect underlying emotional\nstates. This is because the existing datasets, used for the training of these\nsystems, focus on surface-level emotion recognition without addressing the\ncomplex verbal-visual incongruence (mismatch) patterns useful for empathic\nunderstanding. In this paper, we present E-THER, the first Person-Centered\nTherapy-grounded multimodal dataset with multidimensional annotations for\nverbal-visual incongruence detection, enabling training of AI systems that\ndevelop genuine rather than performative empathic capabilities. The annotations\nincluded in the dataset are drawn from humanistic approach, i.e., identifying\nverbal-visual emotional misalignment in client-counsellor interactions -\nforming a framework for training and evaluating AI on empathy tasks. Additional\nengagement scores provide behavioral annotations for research applications.\nNotable gains in empathic and therapeutic conversational qualities are observed\nin state-of-the-art vision-language models (VLMs), such as IDEFICS and\nVideoLLAVA, using evaluation metrics grounded in empathic and therapeutic\nprinciples. Empirical findings indicate that our incongruence-trained models\noutperform general-purpose models in critical traits, such as sustaining\ntherapeutic engagement, minimizing artificial or exaggerated linguistic\npatterns, and maintaining fidelity to PCT theoretical framework."}
{"id": "2509.02175", "pdf": "https://arxiv.org/pdf/2509.02175", "abs": "https://arxiv.org/abs/2509.02175", "authors": ["Nils Hoehing", "Mayug Maniparambil", "Ellen Rushe", "Noel E. O'Connor", "Anthony Ventresque"], "title": "Understanding Space Is Rocket Science - Only Top Reasoning Models Can Solve Spatial Understanding Tasks", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "We propose RocketScience, an open-source contrastive VLM benchmark that tests\nfor spatial relation understanding. It is comprised of entirely new real-world\nimage-text pairs covering mostly relative spatial understanding and the order\nof objects. The benchmark is designed\n  to be very easy for humans and hard for the current generation of VLMs, and\nthis is empirically verified. Our results show a striking lack of spatial\nrelation understanding in open source and frontier commercial VLMs and a\nsurprisingly high performance of reasoning models. Additionally, we perform a\ndisentanglement analysis to separate the contributions of object localization\nand spatial reasoning in chain-of-thought-based models and find that the\nperformance on the benchmark is bottlenecked by spatial reasoning and not\nobject localization capabilities.\n  We release the dataset with a CC-BY-4.0 license and make the evaluation code\navailable at: https://github.com/nilshoehing/rocketscience"}
{"id": "2509.02244", "pdf": "https://arxiv.org/pdf/2509.02244", "abs": "https://arxiv.org/abs/2509.02244", "authors": ["Luis Felipe Chary", "Miguel Arjona Ramirez"], "title": "Spectrogram Patch Codec: A 2D Block-Quantized VQ-VAE and HiFi-GAN for Neural Speech Coding", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "We present a neural speech codec that challenges the need for complex\nresidual vector quantization (RVQ) stacks by introducing a simpler,\nsingle-stage quantization approach. Our method operates directly on the\nmel-spectrogram, treating it as a 2D data and quantizing non-overlapping 4x4\npatches into a single, shared codebook. This patchwise design simplifies the\narchitecture, enables low-latency streaming, and yields a discrete latent grid.\nTo ensure high-fidelity synthesis, we employ a late-stage adversarial\nfine-tuning for the VQ-VAE and train a HiFi-GAN vocoder from scratch on the\ncodec's reconstructed spectrograms. Operating at approximately 7.5 kbits/s for\n16 kHz speech, our system was evaluated against several state-of-the-art neural\ncodecs using objective metrics such as STOI, PESQ, MCD, and ViSQOL. The results\ndemonstrate that our simplified, non-residual architecture achieves competitive\nperceptual quality and intelligibility, validating it as an effective and open\nfoundation for future low-latency codec designs."}
{"id": "2509.02399", "pdf": "https://arxiv.org/pdf/2509.02399", "abs": "https://arxiv.org/abs/2509.02399", "authors": ["Haji Gul", "Abdul Ghani Naim", "Ajaz Ahmad Bhat"], "title": "Evaluating Cumulative Spectral Gradient as a Complexity Measure", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Accurate estimation of dataset complexity is crucial for evaluating and\ncomparing link prediction models for knowledge graphs (KGs). The Cumulative\nSpectral Gradient (CSG) metric derived from probabilistic divergence between\nclasses within a spectral clustering framework was proposed as a dataset\ncomplexity measure that (1) naturally scales with the number of classes and (2)\ncorrelates strongly with downstream classification performance. In this work,\nwe rigorously assess CSG behavior on standard knowledge graph link prediction\nbenchmarks a multi class tail prediction task, using two key parameters\ngoverning its computation, M, the number of Monte Carlo sampled points per\nclass, and K, the number of nearest neighbors in the embedding space. Contrary\nto the original claims, we find that (1) CSG is highly sensitive to the choice\nof K and therefore does not inherently scale with the number of target classes,\nand (2) CSG values exhibit weak or no correlation with established performance\nmetrics such as mean reciprocal rank (MRR). Through experiments on FB15k 237,\nWN18RR, and other standard datasets, we demonstrate that CSG purported\nstability and generalization predictive power break down in link prediction\nsettings. Our results highlight the need for more robust, classifier agnostic\ncomplexity measures in KG link prediction evaluation."}
{"id": "2509.02444", "pdf": "https://arxiv.org/pdf/2509.02444", "abs": "https://arxiv.org/abs/2509.02444", "authors": ["Jingru Fan", "Yufan Dang", "Jingyao Wu", "Huatao Li", "Runde Yang", "Xiyuan Yang", "Yuheng Wang", "Zhong Zhang", "Yaxi Lu", "Yankai Lin", "Zhiyuan Liu", "Dahai Li", "Chen Qian"], "title": "AppCopilot: Toward General, Accurate, Long-Horizon, and Efficient Mobile Agent", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "Project at https://github.com/OpenBMB/AppCopilot", "summary": "With the raid evolution of large language models and multimodal foundation\nmodels, the mobile-agent landscape has proliferated without converging on the\nfundamental challenges. This paper identifies four core problems that must be\nsolved for mobile agents to deliver practical, scalable impact: (1)\ngeneralization across tasks, modalities, apps, and devices; (2) accuracy,\nspecifically precise on-screen interaction and click targeting; (3)\nlong-horizon capability for sustained, multi-step goals; and (4) efficiency,\nspecifically high-performance runtime on resource-constrained devices. We\npresent AppCopilot, a multimodal, multi-agent, general-purpose on-device\nassistant that operates across applications and constitutes a full-stack,\nclosed-loop system from data to deployment. AppCopilot operationalizes this\nposition through an end-to-end autonomous pipeline spanning data collection,\ntraining, deployment, high-quality and efficient inference, and mobile\napplication development. At the model layer, it integrates multimodal\nfoundation models with robust Chinese-English support. At the reasoning and\ncontrol layer, it combines chain-of-thought reasoning, hierarchical task\nplanning and decomposition, and multi-agent collaboration. At the execution\nlayer, it enables user personalization and experiential adaptation, voice\ninteraction, function calling, cross-app and cross-device orchestration, and\ncomprehensive mobile app support. The system design incorporates\nprofiling-driven optimization for latency, memory, and energy across\nheterogeneous hardware. Empirically, AppCopilot achieves significant\nimprovements along all four dimensions: stronger generalization,\nhigher-precision on-screen actions, more reliable long-horizon task completion,\nand faster, more resource-efficient runtime."}
{"id": "2509.02521", "pdf": "https://arxiv.org/pdf/2509.02521", "abs": "https://arxiv.org/abs/2509.02521", "authors": ["Yiqun Yao", "Xiang Li", "Xin Jiang", "Xuezhi Fang", "Naitong Yu", "Wenjia Ma", "Aixin Sun", "Yequan Wang"], "title": "FLM-Audio: Natural Monologues Improves Native Full-Duplex Chatbots via Dual Training", "categories": ["cs.SD", "cs.AI", "cs.CL"], "comment": null, "summary": "Full-duplex dialog models are designed to listen and speak simultaneously\nwith rapid responses to fast-changing user input. Among existing approaches,\nnative full-duplex models merges different channels (e.g. listen and speak) in\na single time step, overcoming the high response latency inherent to\ntime-division multiplexing time-division multiplexing (TDM) alternatives. Yet,\na key challenge remains: aligning textual monologues with audio streams that\noperate at different bitrates. The prevailing solution relies on word-level\nalignment, but this can degrade the language ability of large pre-trained\nmodels. Moreover, it requires highly accurate timestamps for every token, which\nintroduces cascading errors and increases pre-processing costs. In this paper,\nwe propose textual monologues in continuous tokens sequence, namely \"natural\"\nmonologues, which mimics humanoid cognitive behavior in dialogs. For temporal\nalignment, we alternate the position of the natural monologue - leading or\ntrailing the audio - across different training stages. This \"dual\" training\nparadigm proves highly effective in building FLM-Audio, our 7B spoken dialog\nmodel that demonstrates superior responsiveness, duplexity, and chatting\nexperiences, as confirmed by experimental results."}
{"id": "2509.02544", "pdf": "https://arxiv.org/pdf/2509.02544", "abs": "https://arxiv.org/abs/2509.02544", "authors": ["Haoming Wang", "Haoyang Zou", "Huatong Song", "Jiazhan Feng", "Junjie Fang", "Junting Lu", "Longxiang Liu", "Qinyu Luo", "Shihao Liang", "Shijue Huang", "Wanjun Zhong", "Yining Ye", "Yujia Qin", "Yuwen Xiong", "Yuxin Song", "Zhiyong Wu", "Bo Li", "Chen Dun", "Chong Liu", "Fuxing Leng", "Hanbin Wang", "Hao Yu", "Haobin Chen", "Hongyi Guo", "Jing Su", "Jingjia Huang", "Kai Shen", "Kaiyu Shi", "Lin Yan", "Peiyao Zhao", "Pengfei Liu", "Qinghao Ye", "Renjie Zheng", "Wayne Xin Zhao", "Wen Heng", "Wenhao Huang", "Wenqian Wang", "Xiaobo Qin", "Yi Lin", "Youbin Wu", "Zehui Chen", "Zihao Wang", "Baoquan Zhong", "Xinchun Zhang", "Xujing Li", "Yuanfan Li", "Zhongkai Zhao", "Chengquan Jiang", "Faming Wu", "Haotian Zhou", "Jinlin Pang", "Li Han", "Qianli Ma", "Siyao Liu", "Songhua Cai", "Wenqi Fu", "Xin Liu", "Zhi Zhang", "Bo Zhou", "Guoliang Li", "Jiajun Shi", "Jiale Yang", "Jie Tang", "Li Li", "Taoran Lu", "Woyu Lin", "Xiaokang Tong", "Xinyao Li", "Yichi Zhang", "Yu Miao", "Zhengxuan Jiang", "Zili Li", "Ziyuan Zhao", "Chenxin Li", "Dehua Ma", "Feng Lin", "Ge Zhang", "Haihua Yang", "Hangyu Guo", "Hongda Zhu", "Jiaheng Liu", "Junda Du", "Kai Cai", "Kuanye Li", "Lichen Yuan", "Meilan Han", "Minchao Wang", "Shuyue Guo", "Tianhao Cheng", "Xiaobo Ma", "Xiaojun Xiao", "Xiaolong Huang", "Xinjie Chen", "Yidi Du", "Yilin Chen", "Yiwen Wang", "Zhaojian Li", "Zhenzhu Yang", "Zhiyuan Zeng", "Chaolin Jin", "Chen Li", "Hao Chen", "Haoli Chen", "Jian Chen", "Qinghao Zhao", "Guang Shi"], "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": null, "summary": "The development of autonomous agents for graphical user interfaces (GUIs)\npresents major challenges in artificial intelligence. While recent advances in\nnative agent models have shown promise by unifying perception, reasoning,\naction, and memory through end-to-end learning, open problems remain in data\nscalability, multi-turn reinforcement learning (RL), the limitations of\nGUI-only operation, and environment stability. In this technical report, we\npresent UI-TARS-2, a native GUI-centered agent model that addresses these\nchallenges through a systematic training methodology: a data flywheel for\nscalable data generation, a stabilized multi-turn RL framework, a hybrid GUI\nenvironment that integrates file systems and terminals, and a unified sandbox\nplatform for large-scale rollouts. Empirical evaluation demonstrates that\nUI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5.\nOn GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on\nWindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines\nsuch as Claude and OpenAI agents. In game environments, it attains a mean\nnormalized score of 59.8 across a 15-game suite-roughly 60% of human-level\nperformance-and remains competitive with frontier proprietary models (e.g.,\nOpenAI o3) on LMGame-Bench. Additionally, the model can generalize to\nlong-horizon information-seeking tasks and software engineering benchmarks,\nhighlighting its robustness across diverse agent tasks. Detailed analyses of\ntraining dynamics further provide insights into achieving stability and\nefficiency in large-scale agent RL. These results underscore UI-TARS-2's\npotential to advance the state of GUI agents and exhibit strong generalization\nto real-world interactive scenarios."}
{"id": "2509.02547", "pdf": "https://arxiv.org/pdf/2509.02547", "abs": "https://arxiv.org/abs/2509.02547", "authors": ["Guibin Zhang", "Hejia Geng", "Xiaohang Yu", "Zhenfei Yin", "Zaibin Zhang", "Zelin Tan", "Heng Zhou", "Zhongzhi Li", "Xiangyuan Xue", "Yijiang Li", "Yifan Zhou", "Yang Chen", "Chen Zhang", "Yutao Fan", "Zihu Wang", "Songtao Huang", "Yue Liao", "Hongru Wang", "Mengyue Yang", "Heng Ji", "Michael Littman", "Jun Wang", "Shuicheng Yan", "Philip Torr", "Lei Bai"], "title": "The Landscape of Agentic Reinforcement Learning for LLMs: A Survey", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm\nshift from conventional reinforcement learning applied to large language models\n(LLM RL), reframing LLMs from passive sequence generators into autonomous,\ndecision-making agents embedded in complex, dynamic worlds. This survey\nformalizes this conceptual shift by contrasting the degenerate single-step\nMarkov Decision Processes (MDPs) of LLM-RL with the temporally extended,\npartially observable Markov decision processes (POMDPs) that define Agentic RL.\nBuilding on this foundation, we propose a comprehensive twofold taxonomy: one\norganized around core agentic capabilities, including planning, tool use,\nmemory, reasoning, self-improvement, and perception, and the other around their\napplications across diverse task domains. Central to our thesis is that\nreinforcement learning serves as the critical mechanism for transforming these\ncapabilities from static, heuristic modules into adaptive, robust agentic\nbehavior. To support and accelerate future research, we consolidate the\nlandscape of open-source environments, benchmarks, and frameworks into a\npractical compendium. By synthesizing over five hundred recent works, this\nsurvey charts the contours of this rapidly evolving field and highlights the\nopportunities and challenges that will shape the development of scalable,\ngeneral-purpose AI agents."}
{"id": "2509.02563", "pdf": "https://arxiv.org/pdf/2509.02563", "abs": "https://arxiv.org/abs/2509.02563", "authors": ["Monte Hoover", "Vatsal Baherwani", "Neel Jain", "Khalid Saifullah", "Joseph Vincent", "Chirag Jain", "Melissa Kazemi Rad", "C. Bayan Bruss", "Ashwinee Panda", "Tom Goldstein"], "title": "DynaGuard: A Dynamic Guardrail Model With User-Defined Policies", "categories": ["cs.LG", "cs.CL"], "comment": "22 Pages", "summary": "Guardian models are used to supervise and moderate the outputs of user-facing\nchatbots, enforcing guardrails and detecting bad behaviors. Standard guardian\nmodels like LlamaGuard detect predefined, static categories of harms. We\npropose dynamic guardian models that evaluate text based on user-defined\npolicies, making them useful for different application domains that are not\naddressed by standard guardian models. Our dynamic guardian models can be used\nfor fast detection of policy violations or with chain-of-thought reasoning that\narticulates and justifies the model outputs. Our dynamic guardian models match\nstatic models in detection accuracy for static harm categories while\nidentifying violations of free-form policies with accuracy comparable to\nfrontier reasoning models in a fraction of the time."}
