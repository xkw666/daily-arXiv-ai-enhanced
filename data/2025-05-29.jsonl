{"id": "2505.21925", "pdf": "https://arxiv.org/pdf/2505.21925", "abs": "https://arxiv.org/abs/2505.21925", "authors": ["Chong Zeng", "Yue Dong", "Pieter Peers", "Hongzhi Wu", "Xin Tong"], "title": "RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted to SIGGRAPH 2025. Project page:\n  https://microsoft.github.io/renderformer", "summary": "We present RenderFormer, a neural rendering pipeline that directly renders an\nimage from a triangle-based representation of a scene with full global\nillumination effects and that does not require per-scene training or\nfine-tuning. Instead of taking a physics-centric approach to rendering, we\nformulate rendering as a sequence-to-sequence transformation where a sequence\nof tokens representing triangles with reflectance properties is converted to a\nsequence of output tokens representing small patches of pixels. RenderFormer\nfollows a two stage pipeline: a view-independent stage that models\ntriangle-to-triangle light transport, and a view-dependent stage that\ntransforms a token representing a bundle of rays to the corresponding pixel\nvalues guided by the triangle-sequence from the view-independent stage. Both\nstages are based on the transformer architecture and are learned with minimal\nprior constraints. We demonstrate and evaluate RenderFormer on scenes with\nvarying complexity in shape and light transport."}
{"id": "2505.21946", "pdf": "https://arxiv.org/pdf/2505.21946", "abs": "https://arxiv.org/abs/2505.21946", "authors": ["Sinan Wang", "Junwei Zhou", "Fan Feng", "Zhiqi Li", "Yuchen Sun", "Duowen Chen", "Greg Turk", "Bo Zhu"], "title": "Fluid Simulation on Vortex Particle Flow Maps", "categories": ["cs.GR", "physics.flu-dyn"], "comment": "ACM Transactions on Graphics (SIGGRAPH 2025), 24 pages", "summary": "We propose the Vortex Particle Flow Map (VPFM) method to simulate\nincompressible flow with complex vortical evolution in the presence of dynamic\nsolid boundaries. The core insight of our approach is that vorticity is an\nideal quantity for evolution on particle flow maps, enabling significantly\nlonger flow map distances compared to other fluid quantities like velocity or\nimpulse. To achieve this goal, we developed a hybrid Eulerian-Lagrangian\nrepresentation that evolves vorticity and flow map quantities on vortex\nparticles, while reconstructing velocity on a background grid. The method\nintegrates three key components: (1) a vorticity-based particle flow map\nframework, (2) an accurate Hessian evolution scheme on particles, and (3) a\nsolid boundary treatment for no-through and no-slip conditions in VPFM. These\ncomponents collectively allow a substantially longer flow map length (3-12\ntimes longer) than the state-of-the-art, enhancing vorticity preservation over\nextended spatiotemporal domains. We validated the performance of VPFM through\ndiverse simulations, demonstrating its effectiveness in capturing complex\nvortex dynamics and turbulence phenomena."}
{"id": "2505.22400", "pdf": "https://arxiv.org/pdf/2505.22400", "abs": "https://arxiv.org/abs/2505.22400", "authors": ["Zehao Li", "Hao Jiang", "Yujun Cai", "Jianing Chen", "Baolong Bi", "Shuqin Gao", "Honglong Zhao", "Yiwei Wang", "Tianlu Mao", "Zhaoqi Wang"], "title": "STDR: Spatio-Temporal Decoupling for Real-Time Dynamic Scene Rendering", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Although dynamic scene reconstruction has long been a fundamental challenge\nin 3D vision, the recent emergence of 3D Gaussian Splatting (3DGS) offers a\npromising direction by enabling high-quality, real-time rendering through\nexplicit Gaussian primitives. However, existing 3DGS-based methods for dynamic\nreconstruction often suffer from \\textit{spatio-temporal incoherence} during\ninitialization, where canonical Gaussians are constructed by aggregating\nobservations from multiple frames without temporal distinction. This results in\nspatio-temporally entangled representations, making it difficult to model\ndynamic motion accurately. To overcome this limitation, we propose\n\\textbf{STDR} (Spatio-Temporal Decoupling for Real-time rendering), a\nplug-and-play module that learns spatio-temporal probability distributions for\neach Gaussian. STDR introduces a spatio-temporal mask, a separated deformation\nfield, and a consistency regularization to jointly disentangle spatial and\ntemporal patterns. Extensive experiments demonstrate that incorporating our\nmodule into existing 3DGS-based dynamic scene reconstruction frameworks leads\nto notable improvements in both reconstruction quality and spatio-temporal\nconsistency across synthetic and real-world benchmarks."}
{"id": "2505.22416", "pdf": "https://arxiv.org/pdf/2505.22416", "abs": "https://arxiv.org/abs/2505.22416", "authors": ["Sihun Cha", "Serin Yoon", "Kwanggyoon Seo", "Junyong Noh"], "title": "Neural Face Skinning for Mesh-agnostic Facial Expression Cloning", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Accurately retargeting facial expressions to a face mesh while enabling\nmanipulation is a key challenge in facial animation retargeting. Recent\ndeep-learning methods address this by encoding facial expressions into a global\nlatent code, but they often fail to capture fine-grained details in local\nregions. While some methods improve local accuracy by transferring deformations\nlocally, this often complicates overall control of the facial expression. To\naddress this, we propose a method that combines the strengths of both global\nand local deformation models. Our approach enables intuitive control and\ndetailed expression cloning across diverse face meshes, regardless of their\nunderlying structures. The core idea is to localize the influence of the global\nlatent code on the target mesh. Our model learns to predict skinning weights\nfor each vertex of the target face mesh through indirect supervision from\npredefined segmentation labels. These predicted weights localize the global\nlatent code, enabling precise and region-specific deformations even for meshes\nwith unseen shapes. We supervise the latent code using Facial Action Coding\nSystem (FACS)-based blendshapes to ensure interpretability and allow\nstraightforward editing of the generated animation. Through extensive\nexperiments, we demonstrate improved performance over state-of-the-art methods\nin terms of expression fidelity, deformation transfer accuracy, and\nadaptability across diverse mesh structures."}
{"id": "2505.21523", "pdf": "https://arxiv.org/pdf/2505.21523", "abs": "https://arxiv.org/abs/2505.21523", "authors": ["Chengzhi Liu", "Zhongxing Xu", "Qingyue Wei", "Juncheng Wu", "James Zou", "Xin Eric Wang", "Yuyin Zhou", "Sheng Liu"], "title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in Multimodal Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Test-time compute has empowered multimodal large language models to generate\nextended reasoning chains, yielding strong performance on tasks such as\nmultimodal math reasoning. However, this improved reasoning ability often comes\nwith increased hallucination: as generations become longer, models tend to\ndrift away from image-grounded content and rely more heavily on language\npriors. Attention analysis shows that longer reasoning chains lead to reduced\nfocus on visual inputs, which contributes to hallucination. To systematically\nstudy this phenomenon, we introduce RH-AUC, a metric that quantifies how a\nmodel's perception accuracy changes with reasoning length, allowing us to\nevaluate whether the model preserves visual grounding during reasoning. We also\nrelease RH-Bench, a diagnostic benchmark that spans a variety of multimodal\ntasks, designed to assess the trade-off between reasoning ability and\nhallucination. Our analysis reveals that (i) larger models typically achieve a\nbetter balance between reasoning and perception, and (ii) this balance is\ninfluenced more by the types and domains of training data than by its overall\nvolume. These findings underscore the importance of evaluation frameworks that\njointly consider both reasoning quality and perceptual fidelity."}
{"id": "2505.22313", "pdf": "https://arxiv.org/pdf/2505.22313", "abs": "https://arxiv.org/abs/2505.22313", "authors": ["Kaixuan Wei", "Hector A. Jimenez-Romero", "Hadi Amata", "Jipeng Sun", "Qiang Fu", "Felix Heide", "Wolfgang Heidrich"], "title": "Large-Area Fabrication-aware Computational Diffractive Optics", "categories": ["physics.optics", "cs.CV", "cs.ET", "cs.GR"], "comment": null, "summary": "Differentiable optics, as an emerging paradigm that jointly optimizes optics\nand (optional) image processing algorithms, has made innovative optical designs\npossible across a broad range of applications. Many of these systems utilize\ndiffractive optical components (DOEs) for holography, PSF engineering, or\nwavefront shaping. Existing approaches have, however, mostly remained limited\nto laboratory prototypes, owing to a large quality gap between simulation and\nmanufactured devices. We aim at lifting the fundamental technical barriers to\nthe practical use of learned diffractive optical systems. To this end, we\npropose a fabrication-aware design pipeline for diffractive optics fabricated\nby direct-write grayscale lithography followed by nano-imprinting replication,\nwhich is directly suited for inexpensive mass production of large area designs.\nWe propose a super-resolved neural lithography model that can accurately\npredict the 3D geometry generated by the fabrication process. This model can be\nseamlessly integrated into existing differentiable optics frameworks, enabling\nfabrication-aware, end-to-end optimization of computational optical systems. To\ntackle the computational challenges, we also devise tensor-parallel compute\nframework centered on distributing large-scale FFT computation across many\nGPUs. As such, we demonstrate large scale diffractive optics designs up to\n32.16 mm $\\times$ 21.44 mm, simulated on grids of up to 128,640 by 85,760\nfeature points. We find adequate agreement between simulation and fabricated\nprototypes for applications such as holography and PSF engineering. We also\nachieve high image quality from an imaging system comprised only of a single\nDOE, with images processed only by a Wiener filter utilizing the simulation\nPSF. We believe our findings lift the fabrication limitations for real-world\napplications of diffractive optics and differentiable optical design."}
{"id": "2505.21578", "pdf": "https://arxiv.org/pdf/2505.21578", "abs": "https://arxiv.org/abs/2505.21578", "authors": ["Titouan Parcollet", "Yuan Tseng", "Shucong Zhang", "Rogier van Dalen"], "title": "Loquacious Set: 25,000 Hours of Transcribed and Diverse English Speech Recognition Data for Research and Commercial Use", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Automatic speech recognition (ASR) research is driven by the availability of\ncommon datasets between industrial researchers and academics, encouraging\ncomparisons and evaluations. LibriSpeech, despite its long success as an ASR\nbenchmark, is now limited by its size and focus on clean, read speech, leading\nto near-zero word error rates. More recent datasets, including MOSEL, YODAS,\nGigaspeech, OWSM, Libriheavy or People's Speech suffer from major limitations\nincluding licenses that researchers in the industry cannot use, unreliable\ntranscriptions, incorrect audio data, or the lack of evaluation sets. This work\npresents the Loquacious Set, a 25,000-hour curated collection of commercially\nusable English speech. Featuring hundreds of thousands of speakers with diverse\naccents and a wide range of speech types (read, spontaneous, talks, clean,\nnoisy), the Loquacious Set is designed to work for academics and researchers in\nthe industry to build ASR systems in real-world scenarios."}
{"id": "2505.22489", "pdf": "https://arxiv.org/pdf/2505.22489", "abs": "https://arxiv.org/abs/2505.22489", "authors": ["Siyeop Yoon", "Sifan Song", "Pengfei Jin", "Matthew Tivnan", "Yujin Oh", "Sekeun Kim", "Dufan Wu", "Xiang Li", "Quanzheng Li"], "title": "Cascaded 3D Diffusion Models for Whole-body 3D 18-F FDG PET/CT synthesis from Demographics", "categories": ["eess.IV", "cs.CV", "cs.GR"], "comment": "MICCAI2025 Submitted version", "summary": "We propose a cascaded 3D diffusion model framework to synthesize\nhigh-fidelity 3D PET/CT volumes directly from demographic variables, addressing\nthe growing need for realistic digital twins in oncologic imaging, virtual\ntrials, and AI-driven data augmentation. Unlike deterministic phantoms, which\nrely on predefined anatomical and metabolic templates, our method employs a\ntwo-stage generative process. An initial score-based diffusion model\nsynthesizes low-resolution PET/CT volumes from demographic variables alone,\nproviding global anatomical structures and approximate metabolic activity. This\nis followed by a super-resolution residual diffusion model that refines spatial\nresolution. Our framework was trained on 18-F FDG PET/CT scans from the AutoPET\ndataset and evaluated using organ-wise volume and standardized uptake value\n(SUV) distributions, comparing synthetic and real data between demographic\nsubgroups. The organ-wise comparison demonstrated strong concordance between\nsynthetic and real images. In particular, most deviations in metabolic uptake\nvalues remained within 3-5% of the ground truth in subgroup analysis. These\nfindings highlight the potential of cascaded 3D diffusion models to generate\nanatomically and metabolically accurate PET/CT images, offering a robust\nalternative to traditional phantoms and enabling scalable, population-informed\nsynthetic imaging for clinical and research applications."}
{"id": "2505.21598", "pdf": "https://arxiv.org/pdf/2505.21598", "abs": "https://arxiv.org/abs/2505.21598", "authors": ["Yajiao Liu", "Congliang Chen", "Junchi Yang", "Ruoyu Sun"], "title": "Rethinking Data Mixture for Large Language Models: A Comprehensive Survey and New Perspectives", "categories": ["cs.CL"], "comment": "The first version of this paper was submitted to ACL ARR 2025\n  February Submission", "summary": "Training large language models with data collected from various domains can\nimprove their performance on downstream tasks. However, given a fixed training\nbudget, the sampling proportions of these different domains significantly\nimpact the model's performance. How can we determine the domain weights across\ndifferent data domains to train the best-performing model within constrained\ncomputational resources? In this paper, we provide a comprehensive overview of\nexisting data mixture methods. First, we propose a fine-grained categorization\nof existing methods, extending beyond the previous offline and online\nclassification. Offline methods are further grouped into heuristic-based,\nalgorithm-based, and function fitting-based methods. For online methods, we\ncategorize them into three groups: online min-max optimization, online mixing\nlaw, and other approaches by drawing connections with the optimization\nframeworks underlying offline methods. Second, we summarize the problem\nformulations, representative algorithms for each subtype of offline and online\nmethods, and clarify the relationships and distinctions among them. Finally, we\ndiscuss the advantages and disadvantages of each method and highlight key\nchallenges in the field of data mixture."}
{"id": "2505.21600", "pdf": "https://arxiv.org/pdf/2505.21600", "abs": "https://arxiv.org/abs/2505.21600", "authors": ["Tianyu Fu", "Yi Ge", "Yichen You", "Enshu Liu", "Zhihang Yuan", "Guohao Dai", "Shengen Yan", "Huazhong Yang", "Yu Wang"], "title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the\ncost of substantial inference overhead, posing substantial deployment\nchallenges. Although distilled Small Language Models (SLMs) significantly\nenhance efficiency, their performance suffers as they fail to follow LLMs'\nreasoning paths. Luckily, we reveal that only a small fraction of tokens\ngenuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens\nare either identical or exhibit neutral differences, such as minor variations\nin abbreviations or expressions. Leveraging this insight, we introduce **Roads\nto Rome (R2R)**, a neural token routing method that selectively utilizes LLMs\nonly for these critical, path-divergent tokens, while leaving the majority of\ntoken generation to the SLM. We also develop an automatic data generation\npipeline that identifies divergent tokens and generates token-level routing\nlabels to train the lightweight router. We apply R2R to combine R1-1.5B and\nR1-32B models from the DeepSeek family, and evaluate on challenging math,\ncoding, and QA benchmarks. With an average activated parameter size of 5.6B,\nR2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the\nR1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with\ncomparable performance, advancing the Pareto frontier of test-time scaling\nefficiency. Our code is available at https://github.com/thu-nics/R2R."}
{"id": "2505.21608", "pdf": "https://arxiv.org/pdf/2505.21608", "abs": "https://arxiv.org/abs/2505.21608", "authors": ["Miao Peng", "Nuo Chen", "Jianheng Tang", "Jia Li"], "title": "How does Misinformation Affect Large Language Model Behaviors and Preferences?", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Large Language Models (LLMs) have shown remarkable capabilities in\nknowledge-intensive tasks, while they remain vulnerable when encountering\nmisinformation. Existing studies have explored the role of LLMs in combating\nmisinformation, but there is still a lack of fine-grained analysis on the\nspecific aspects and extent to which LLMs are influenced by misinformation. To\nbridge this gap, we present MisBench, the current largest and most\ncomprehensive benchmark for evaluating LLMs' behavior and knowledge preference\ntoward misinformation. MisBench consists of 10,346,712 pieces of\nmisinformation, which uniquely considers both knowledge-based conflicts and\nstylistic variations in misinformation. Empirical results reveal that while\nLLMs demonstrate comparable abilities in discerning misinformation, they still\nremain susceptible to knowledge conflicts and stylistic variations. Based on\nthese findings, we further propose a novel approach called Reconstruct to\nDiscriminate (RtD) to strengthen LLMs' ability to detect misinformation. Our\nstudy provides valuable insights into LLMs' interactions with misinformation,\nand we believe MisBench can serve as an effective benchmark for evaluating\nLLM-based detectors and enhancing their reliability in real-world applications.\nCodes and data are available at https://github.com/GKNL/MisBench."}
{"id": "2505.21646", "pdf": "https://arxiv.org/pdf/2505.21646", "abs": "https://arxiv.org/abs/2505.21646", "authors": ["Lei Zhang", "Markus Stricker"], "title": "Iterative Corpus Refinement for Materials Property Prediction Based on Scientific Texts", "categories": ["cs.CL", "cond-mat.mtrl-sci"], "comment": "13 pages, 5 figures, 2 tables, accepted at ECMLPKDD 2025", "summary": "The discovery and optimization of materials for specific applications is\nhampered by the practically infinite number of possible elemental combinations\nand associated properties, also known as the `combinatorial explosion'. By\nnature of the problem, data are scarce and all possible data sources should be\nused. In addition to simulations and experimental results, the latent knowledge\nin scientific texts is not yet used to its full potential. We present an\niterative framework that refines a given scientific corpus by strategic\nselection of the most diverse documents, training Word2Vec models, and\nmonitoring the convergence of composition-property correlations in embedding\nspace. Our approach is applied to predict high-performing materials for oxygen\nreduction (ORR), hydrogen evolution (HER), and oxygen evolution (OER) reactions\nfor a large number of possible candidate compositions. Our method successfully\npredicts the highest performing compositions among a large pool of candidates,\nvalidated by experimental measurements of the electrocatalytic performance in\nthe lab. This work demonstrates and validates the potential of iterative corpus\nrefinement to accelerate materials discovery and optimization, offering a\nscalable and efficient tool for screening large compositional spaces where\nreliable data are scarce or non-existent."}
{"id": "2505.21657", "pdf": "https://arxiv.org/pdf/2505.21657", "abs": "https://arxiv.org/abs/2505.21657", "authors": ["Zeinab Dehghani", "Koorosh Aslansefat", "Adil Khan", "Mohammed Naveed Akram"], "title": "Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2412.16277", "summary": "Large language models like GPT, LLAMA, and Claude have become incredibly\npowerful at generating text, but they are still black boxes, so it is hard to\nunderstand how they decide what to say. That lack of transparency can be\nproblematic, especially in fields where trust and accountability matter. To\nhelp with this, we introduce SMILE, a new method that explains how these models\nrespond to different parts of a prompt. SMILE is model-agnostic and works by\nslightly changing the input, measuring how the output changes, and then\nhighlighting which words had the most impact. Create simple visual heat maps\nshowing which parts of a prompt matter the most. We tested SMILE on several\nleading LLMs and used metrics such as accuracy, consistency, stability, and\nfidelity to show that it gives clear and reliable explanations. By making these\nmodels easier to understand, SMILE brings us one step closer to making AI more\ntransparent and trustworthy."}
{"id": "2505.21670", "pdf": "https://arxiv.org/pdf/2505.21670", "abs": "https://arxiv.org/abs/2505.21670", "authors": ["Rahul Raman", "Khushi Sharma", "Sai Qian Zhang"], "title": "Rethinking the Outlier Distribution in Large Language Models: An In-depth Study", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Investigating outliers in large language models (LLMs) is crucial due to\ntheir significant impact on various aspects of LLM performance, including\nquantization and compression. Outliers often cause considerable quantization\nerrors, leading to degraded model performance. Identifying and addressing these\noutliers can enhance the accuracy and efficiency of the quantization process,\nenabling smoother deployment on edge devices or specialized hardware. Recent\nstudies have identified two common types of outliers in LLMs: massive\nactivations and channel-wise outliers. While numerous quantization algorithms\nhave been proposed to mitigate their effects and maintain satisfactory\naccuracy, few have thoroughly explored the root causes of these outliers in\ndepth. In this paper, we conduct a comprehensive investigation into the\nformation mechanisms of these outliers and propose potential strategies to\nmitigate their occurrence. Ultimately, we introduce some efficient approaches\nto eliminate most massive activations and channel-wise outliers with minimal\nimpact on accuracy."}
{"id": "2505.21689", "pdf": "https://arxiv.org/pdf/2505.21689", "abs": "https://arxiv.org/abs/2505.21689", "authors": ["Avijit Gayen", "Somyajit Chakraborty", "Mainak Sen", "Soham Paul", "Angshuman Jana"], "title": "LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "28 pages, 5 figures, journal paper, submitted to AI and Law", "summary": "The persistent accumulation of unresolved legal cases, especially within the\nIndian judiciary, significantly hampers the timely delivery of justice. Manual\nmethods of prioritizing petitions are often prone to inefficiencies and\nsubjective biases further exacerbating delays. To address this issue, we\npropose LLMPR (Large Language Model-based Petition Ranking), an automated\nframework that utilizes transfer learning and machine learning to assign\npriority rankings to legal petitions based on their contextual urgency.\nLeveraging the ILDC dataset comprising 7,593 annotated petitions, we process\nunstructured legal text and extract features through various embedding\ntechniques, including DistilBERT, LegalBERT, and MiniLM. These textual\nembeddings are combined with quantitative indicators such as gap days, rank\nscores, and word counts to train multiple machine learning models, including\nRandom Forest, Decision Tree, XGBoost, LightGBM, and CatBoost. Our experiments\ndemonstrate that Random Forest and Decision Tree models yield superior\nperformance, with accuracy exceeding 99% and a Spearman rank correlation of\n0.99. Notably, models using only numerical features achieve nearly optimal\nranking results (R2 = 0.988, \\r{ho} = 0.998), while LLM-based embeddings offer\nonly marginal gains. These findings suggest that automated petition ranking can\neffectively streamline judicial workflows, reduce case backlog, and improve\nfairness in legal prioritization."}
{"id": "2505.21693", "pdf": "https://arxiv.org/pdf/2505.21693", "abs": "https://arxiv.org/abs/2505.21693", "authors": ["Raoyuan Zhao", "Beiduo Chen", "Barbara Plank", "Michael A. Hedderich"], "title": "MAKIEval: A Multilingual Automatic WiKidata-based Framework for Cultural Awareness Evaluation for LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are used globally across many languages, but\ntheir English-centric pretraining raises concerns about cross-lingual\ndisparities for cultural awareness, often resulting in biased outputs. However,\ncomprehensive multilingual evaluation remains challenging due to limited\nbenchmarks and questionable translation quality. To better assess these\ndisparities, we introduce MAKIEval, an automatic multilingual framework for\nevaluating cultural awareness in LLMs across languages, regions, and topics.\nMAKIEval evaluates open-ended text generation, capturing how models express\nculturally grounded knowledge in natural language. Leveraging Wikidata's\nmultilingual structure as a cross-lingual anchor, it automatically identifies\ncultural entities in model outputs and links them to structured knowledge,\nenabling scalable, language-agnostic evaluation without manual annotation or\ntranslation. We then introduce four metrics that capture complementary\ndimensions of cultural awareness: granularity, diversity, cultural specificity,\nand consensus across languages. We assess 7 LLMs developed from different parts\nof the world, encompassing both open-source and proprietary systems, across 13\nlanguages, 19 countries and regions, and 6 culturally salient topics (e.g.,\nfood, clothing). Notably, we find that models tend to exhibit stronger cultural\nawareness in English, suggesting that English prompts more effectively activate\nculturally grounded knowledge. We publicly release our code and data."}
{"id": "2505.21701", "pdf": "https://arxiv.org/pdf/2505.21701", "abs": "https://arxiv.org/abs/2505.21701", "authors": ["Raoyuan Zhao", "Abdullatif Köksal", "Ali Modarressi", "Michael A. Hedderich", "Hinrich Schütze"], "title": "Do We Know What LLMs Don't Know? A Study of Consistency in Knowledge Probing", "categories": ["cs.CL"], "comment": null, "summary": "The reliability of large language models (LLMs) is greatly compromised by\ntheir tendency to hallucinate, underscoring the need for precise identification\nof knowledge gaps within LLMs. Various methods for probing such gaps exist,\nranging from calibration-based to prompting-based methods. To evaluate these\nprobing methods, in this paper, we propose a new process based on using input\nvariations and quantitative metrics. Through this, we expose two dimensions of\ninconsistency in knowledge gap probing. (1) Intra-method inconsistency: Minimal\nnon-semantic perturbations in prompts lead to considerable variance in detected\nknowledge gaps within the same probing method; e.g., the simple variation of\nshuffling answer options can decrease agreement to around 40%. (2) Cross-method\ninconsistency: Probing methods contradict each other on whether a model knows\nthe answer. Methods are highly inconsistent -- with decision consistency across\nmethods being as low as 7% -- even though the model, dataset, and prompt are\nall the same. These findings challenge existing probing methods and highlight\nthe urgent need for perturbation-robust probing frameworks."}
{"id": "2505.21710", "pdf": "https://arxiv.org/pdf/2505.21710", "abs": "https://arxiv.org/abs/2505.21710", "authors": ["Barbarestani Baran", "Maks Isa", "Vossen Piek"], "title": "Assessing and Refining ChatGPT's Performance in Identifying Targeting and Inappropriate Language: A Comparative Study", "categories": ["cs.CL"], "comment": null, "summary": "This study evaluates the effectiveness of ChatGPT, an advanced AI model for\nnatural language processing, in identifying targeting and inappropriate\nlanguage in online comments. With the increasing challenge of moderating vast\nvolumes of user-generated content on social network sites, the role of AI in\ncontent moderation has gained prominence. We compared ChatGPT's performance\nagainst crowd-sourced annotations and expert evaluations to assess its\naccuracy, scope of detection, and consistency. Our findings highlight that\nChatGPT performs well in detecting inappropriate content, showing notable\nimprovements in accuracy through iterative refinements, particularly in Version\n6. However, its performance in targeting language detection showed variability,\nwith higher false positive rates compared to expert judgments. This study\ncontributes to the field by demonstrating the potential of AI models like\nChatGPT to enhance automated content moderation systems while also identifying\nareas for further improvement. The results underscore the importance of\ncontinuous model refinement and contextual understanding to better support\nautomated moderation and mitigate harmful online behavior."}
{"id": "2505.21740", "pdf": "https://arxiv.org/pdf/2505.21740", "abs": "https://arxiv.org/abs/2505.21740", "authors": ["Marvin Limpijankit", "Yanda Chen", "Melanie Subbiah", "Nicholas Deas", "Kathleen McKeown"], "title": "Counterfactual Simulatability of LLM Explanations for Generation Tasks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "LLMs can be unpredictable, as even slight alterations to the prompt can cause\nthe output to change in unexpected ways. Thus, the ability of models to\naccurately explain their behavior is critical, especially in high-stakes\nsettings. One approach for evaluating explanations is counterfactual\nsimulatability, how well an explanation allows users to infer the model's\noutput on related counterfactuals. Counterfactual simulatability has been\npreviously studied for yes/no question answering tasks. We provide a general\nframework for extending this method to generation tasks, using news\nsummarization and medical suggestion as example use cases. We find that while\nLLM explanations do enable users to better predict LLM outputs on\ncounterfactuals in the summarization setting, there is significant room for\nimprovement for medical suggestion. Furthermore, our results suggest that the\nevaluation for counterfactual simulatability may be more appropriate for\nskill-based tasks as opposed to knowledge-based tasks."}
{"id": "2505.21757", "pdf": "https://arxiv.org/pdf/2505.21757", "abs": "https://arxiv.org/abs/2505.21757", "authors": ["Yubin Kim", "Zhiyuan Hu", "Hyewon Jeong", "Eugene Park", "Shuyue Stella Li", "Chanwoo Park", "Shiyun Xiong", "MingYu Lu", "Hyeonhoon Lee", "Xin Liu", "Daniel McDuff", "Cynthia Breazeal", "Samir Tulebaev", "Hae Won Park"], "title": "BehaviorSFT: Behavioral Token Conditioning for Clinical Agents Across the Proactivity Spectrum", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) as clinical agents require careful behavioral\nadaptation. While adept at reactive tasks (e.g., diagnosis reasoning), LLMs\noften struggle with proactive engagement, like unprompted identification of\ncritical missing information or risks. We introduce BehaviorBench, a\ncomprehensive dataset to evaluate agent behaviors across a clinical assistance\nspectrum, ranging from reactive query responses to proactive interventions\n(e.g., clarifying ambiguities, flagging overlooked critical data). Our\nBehaviorBench experiments reveal LLMs' inconsistent proactivity. To address\nthis, we propose BehaviorSFT, a novel training strategy using behavioral tokens\nto explicitly condition LLMs for dynamic behavioral selection along this\nspectrum. BehaviorSFT boosts performance, achieving up to 97.3% overall Macro\nF1 on BehaviorBench and improving proactive task scores (e.g., from 95.0% to\n96.5% for Qwen2.5-7B-Ins). Crucially, blind clinician evaluations confirmed\nBehaviorSFT-trained agents exhibit more realistic clinical behavior, striking a\nsuperior balance between helpful proactivity (e.g., timely, relevant\nsuggestions) and necessary restraint (e.g., avoiding over-intervention) versus\nstandard fine-tuning or explicit instructed agents."}
{"id": "2505.21772", "pdf": "https://arxiv.org/pdf/2505.21772", "abs": "https://arxiv.org/abs/2505.21772", "authors": ["Reza Khanmohammadi", "Erfan Miahi", "Mehrsa Mardikoraem", "Simerjot Kaur", "Ivan Brugere", "Charese H. Smiley", "Kundan Thind", "Mohammad M. Ghassemi"], "title": "Calibrating LLM Confidence by Probing Perturbed Representation Stability", "categories": ["cs.CL"], "comment": null, "summary": "Miscalibration in Large Language Models (LLMs) undermines their reliability,\nhighlighting the need for accurate confidence estimation. We introduce CCPS\n(Calibrating LLM Confidence by Probing Perturbed Representation Stability), a\nnovel method analyzing internal representational stability in LLMs. CCPS\napplies targeted adversarial perturbations to final hidden states, extracts\nfeatures reflecting the model's response to these perturbations, and uses a\nlightweight classifier to predict answer correctness. CCPS was evaluated on\nLLMs from 8B to 32B parameters (covering Llama, Qwen, and Mistral\narchitectures) using MMLU and MMLU-Pro benchmarks in both multiple-choice and\nopen-ended formats. Our results show that CCPS significantly outperforms\ncurrent approaches. Across four LLMs and three MMLU variants, CCPS reduces\nExpected Calibration Error by approximately 55% and Brier score by 21%, while\nincreasing accuracy by 5 percentage points, Area Under the Precision-Recall\nCurve by 4 percentage points, and Area Under the Receiver Operating\nCharacteristic Curve by 6 percentage points, all relative to the strongest\nprior method. CCPS delivers an efficient, broadly applicable, and more accurate\nsolution for estimating LLM confidence, thereby improving their\ntrustworthiness."}
{"id": "2505.21781", "pdf": "https://arxiv.org/pdf/2505.21781", "abs": "https://arxiv.org/abs/2505.21781", "authors": ["Chutong Meng", "Antonios Anastasopoulos"], "title": "GMU Systems for the IWSLT 2025 Low-Resource Speech Translation Shared Task", "categories": ["cs.CL"], "comment": "IWSLT 2025", "summary": "This paper describes the GMU systems for the IWSLT 2025 low-resource speech\ntranslation shared task. We trained systems for all language pairs, except for\nLevantine Arabic. We fine-tuned SeamlessM4T-v2 for automatic speech recognition\n(ASR), machine translation (MT), and end-to-end speech translation (E2E ST).\nThe ASR and MT models are also used to form cascaded ST systems. Additionally,\nwe explored various training paradigms for E2E ST fine-tuning, including direct\nE2E fine-tuning, multi-task training, and parameter initialization using\ncomponents from fine-tuned ASR and/or MT models. Our results show that (1)\ndirect E2E fine-tuning yields strong results; (2) initializing with a\nfine-tuned ASR encoder improves ST performance on languages SeamlessM4T-v2 has\nnot been trained on; (3) multi-task training can be slightly helpful."}
{"id": "2505.21786", "pdf": "https://arxiv.org/pdf/2505.21786", "abs": "https://arxiv.org/abs/2505.21786", "authors": ["Dasha Metropolitansky", "Jonathan Larson"], "title": "VeriTrail: Closed-Domain Hallucination Detection with Traceability", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Even when instructed to adhere to source material, Language Models often\ngenerate unsubstantiated content - a phenomenon known as \"closed-domain\nhallucination.\" This risk is amplified in processes with multiple generative\nsteps (MGS), compared to processes with a single generative step (SGS).\nHowever, due to the greater complexity of MGS processes, we argue that\ndetecting hallucinations in their final outputs is necessary but not\nsufficient: it is equally important to trace where hallucinated content was\nlikely introduced and how faithful content may have been derived from the\nsource through intermediate outputs. To address this need, we present\nVeriTrail, the first closed-domain hallucination detection method designed to\nprovide traceability for both MGS and SGS processes. We also introduce the\nfirst datasets to include all intermediate outputs as well as human annotations\nof final outputs' faithfulness for their respective MGS processes. We\ndemonstrate that VeriTrail outperforms baseline methods on both datasets."}
{"id": "2505.21816", "pdf": "https://arxiv.org/pdf/2505.21816", "abs": "https://arxiv.org/abs/2505.21816", "authors": ["Amr Keleg", "Sharon Goldwater", "Walid Magdy"], "title": "Revisiting Common Assumptions about Arabic Dialects in NLP", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Arabic has diverse dialects, where one dialect can be substantially different\nfrom the others. In the NLP literature, some assumptions about these dialects\nare widely adopted (e.g., ``Arabic dialects can be grouped into distinguishable\nregional dialects\") and are manifested in different computational tasks such as\nArabic Dialect Identification (ADI). However, these assumptions are not\nquantitatively verified. We identify four of these assumptions and examine them\nby extending and analyzing a multi-label dataset, where the validity of each\nsentence in 11 different country-level dialects is manually assessed by\nspeakers of these dialects. Our analysis indicates that the four assumptions\noversimplify reality, and some of them are not always accurate. This in turn\nmight be hindering further progress in different Arabic NLP tasks."}
{"id": "2505.21819", "pdf": "https://arxiv.org/pdf/2505.21819", "abs": "https://arxiv.org/abs/2505.21819", "authors": ["Charlotte Peale", "Vinod Raman", "Omer Reingold"], "title": "Representative Language Generation", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted to ICML 2025", "summary": "We introduce \"representative generation,\" extending the theoretical framework\nfor generation proposed by Kleinberg et al. (2024) and formalized by Li et al.\n(2024), to additionally address diversity and bias concerns in generative\nmodels. Our notion requires outputs of a generative model to proportionally\nrepresent groups of interest from the training data. We characterize\nrepresentative uniform and non-uniform generation, introducing the \"group\nclosure dimension\" as a key combinatorial quantity. For representative\ngeneration in the limit, we analyze both information-theoretic and\ncomputational aspects, demonstrating feasibility for countably infinite\nhypothesis classes and collections of groups under certain conditions, but\nproving a negative result for computability using only membership queries. This\ncontrasts with Kleinberg et al.'s (2024) positive results for standard\ngeneration in the limit. Our findings provide a rigorous foundation for\ndeveloping more diverse and representative generative models."}
{"id": "2505.21859", "pdf": "https://arxiv.org/pdf/2505.21859", "abs": "https://arxiv.org/abs/2505.21859", "authors": ["Vishakh Padmakumar", "Zichao Wang", "David Arbour", "Jennifer Healey"], "title": "Principled Content Selection to Generate Diverse and Personalized Multi-Document Summaries", "categories": ["cs.CL"], "comment": "To appear at ACL 2025 - Main Conference", "summary": "While large language models (LLMs) are increasingly capable of handling\nlonger contexts, recent work has demonstrated that they exhibit the \"lost in\nthe middle\" phenomenon (Liu et al., 2024) of unevenly attending to different\nparts of the provided context. This hinders their ability to cover diverse\nsource material in multi-document summarization, as noted in the DiverseSumm\nbenchmark (Huang et al., 2024). In this work, we contend that principled\ncontent selection is a simple way to increase source coverage on this task. As\nopposed to prompting an LLM to perform the summarization in a single step, we\nexplicitly divide the task into three steps -- (1) reducing document\ncollections to atomic key points, (2) using determinantal point processes (DPP)\nto perform select key points that prioritize diverse content, and (3) rewriting\nto the final summary. By combining prompting steps, for extraction and\nrewriting, with principled techniques, for content selection, we consistently\nimprove source coverage on the DiverseSumm benchmark across various LLMs.\nFinally, we also show that by incorporating relevance to a provided user intent\ninto the DPP kernel, we can generate personalized summaries that cover relevant\nsource information while retaining coverage."}
{"id": "2505.21870", "pdf": "https://arxiv.org/pdf/2505.21870", "abs": "https://arxiv.org/abs/2505.21870", "authors": ["Shuyang Cao", "Karthik Radhakrishnan", "David Rosenberg", "Steven Lu", "Pengxiang Cheng", "Lu Wang", "Shiyue Zhang"], "title": "Evaluating the Retrieval Robustness of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages", "summary": "Retrieval-augmented generation (RAG) generally enhances large language\nmodels' (LLMs) ability to solve knowledge-intensive tasks. But RAG may also\nlead to performance degradation due to imperfect retrieval and the model's\nlimited ability to leverage retrieved content. In this work, we evaluate the\nrobustness of LLMs in practical RAG setups (henceforth retrieval robustness).\nWe focus on three research questions: (1) whether RAG is always better than\nnon-RAG; (2) whether more retrieved documents always lead to better\nperformance; (3) and whether document orders impact results. To facilitate this\nstudy, we establish a benchmark of 1500 open-domain questions, each with\nretrieved documents from Wikipedia. We introduce three robustness metrics, each\ncorresponds to one research question. Our comprehensive experiments, involving\n11 LLMs and 3 prompting strategies, reveal that all of these LLMs exhibit\nsurprisingly high retrieval robustness; nonetheless, different degrees of\nimperfect robustness hinders them from fully utilizing the benefits of RAG."}
{"id": "2505.21889", "pdf": "https://arxiv.org/pdf/2505.21889", "abs": "https://arxiv.org/abs/2505.21889", "authors": ["Tianyu Guo", "Hande Dong", "Yichong Leng", "Feng Liu", "Cheater Lin", "Nong Xiao", "Xianwei Zhang"], "title": "EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are often used for infilling tasks, which\ninvolve predicting or generating missing information in a given text. These\ntasks typically require multiple interactions with similar context. To reduce\nthe computation of repeated historical tokens, cross-request key-value (KV)\ncache reuse, a technique that stores and reuses intermediate computations, has\nbecome a crucial method in multi-round interactive services. However, in\ninfilling tasks, the KV cache reuse is often hindered by the structure of the\nprompt format, which typically consists of a prefix and suffix relative to the\ninsertion point. Specifically, the KV cache of the prefix or suffix part is\nfrequently invalidated as the other part (suffix or prefix) is incrementally\ngenerated. To address the issue, we propose EFIM, a transformed prompt format\nof FIM to unleash the performance potential of KV cache reuse. Although the\ntransformed prompt can solve the inefficiency, it exposes subtoken generation\nproblems in current LLMs, where they have difficulty generating partial words\naccurately. Therefore, we introduce a fragment tokenization training method\nwhich splits text into multiple fragments before tokenization during data\nprocessing. Experiments on two representative LLMs show that LLM serving with\nEFIM can lower the latency by 52% and improve the throughput by 98% while\nmaintaining the original infilling capability.EFIM's source code is publicly\navailable at https://github.com/gty111/EFIM."}
{"id": "2505.21898", "pdf": "https://arxiv.org/pdf/2505.21898", "abs": "https://arxiv.org/abs/2505.21898", "authors": ["Rennai Qiu", "Chen Qian", "Ran Li", "Yufan Dang", "Weize Chen", "Cheng Yang", "Yingli Zhang", "Ye Tian", "Xuantang Xiong", "Lei Han", "Zhiyuan Liu", "Maosong Sun"], "title": "Co-Saving: Resource Aware Multi-Agent Collaboration for Software Development", "categories": ["cs.CL", "cs.AI", "cs.MA", "cs.SE"], "comment": "Work in Progress", "summary": "Recent advancements in Large Language Models (LLMs) and autonomous agents\nhave demonstrated remarkable capabilities across various domains. However,\nstandalone agents frequently encounter limitations when handling complex tasks\nthat demand extensive interactions and substantial computational resources.\nAlthough Multi-Agent Systems (MAS) alleviate some of these limitations through\ncollaborative mechanisms like task decomposition, iterative communication, and\nrole specialization, they typically remain resource-unaware, incurring\nsignificant inefficiencies due to high token consumption and excessive\nexecution time. To address these limitations, we propose a resource-aware\nmulti-agent system -- Co-Saving (meaning that multiple agents collaboratively\nengage in resource-saving activities), which leverages experiential knowledge\nto enhance operational efficiency and solution quality. Our key innovation is\nthe introduction of \"shortcuts\" -- instructional transitions learned from\nhistorically successful trajectories -- which allows to bypass redundant\nreasoning agents and expedite the collective problem-solving process.\nExperiments for software development tasks demonstrate significant advantages\nover existing methods. Specifically, compared to the state-of-the-art MAS\nChatDev, our method achieves an average reduction of 50.85% in token usage, and\nimproves the overall code quality by 10.06%."}
{"id": "2505.21926", "pdf": "https://arxiv.org/pdf/2505.21926", "abs": "https://arxiv.org/abs/2505.21926", "authors": ["Yin Hua", "Zhiqiang Liu", "Mingyang Chen", "Zheng Fang", "Chi Man Wong", "Lingxiao Li", "Chi Man Vong", "Huajun Chen", "Wen Zhang"], "title": "Beyond Completion: A Foundation Model for General Knowledge Graph Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Findings", "summary": "In natural language processing (NLP) and computer vision (CV), the successful\napplication of foundation models across diverse tasks has demonstrated their\nremarkable potential. However, despite the rich structural and textual\ninformation embedded in knowledge graphs (KGs), existing research of foundation\nmodel for KG has primarily focused on their structural aspects, with most\nefforts restricted to in-KG tasks (e.g., knowledge graph completion, KGC). This\nlimitation has hindered progress in addressing more challenging out-of-KG\ntasks. In this paper, we introduce MERRY, a foundation model for general\nknowledge graph reasoning, and investigate its performance across two task\ncategories: in-KG reasoning tasks (e.g., KGC) and out-of-KG tasks (e.g., KG\nquestion answering, KGQA). We not only utilize the structural information, but\nalso the textual information in KGs. Specifically, we propose a\nmulti-perspective Conditional Message Passing (CMP) encoding architecture to\nbridge the gap between textual and structural modalities, enabling their\nseamless integration. Additionally, we introduce a dynamic residual fusion\nmodule to selectively retain relevant textual information and a flexible edge\nscoring mechanism to adapt to diverse downstream tasks. Comprehensive\nevaluations on 28 datasets demonstrate that MERRY outperforms existing\nbaselines in most scenarios, showcasing strong reasoning capabilities within\nKGs and excellent generalization to out-of-KG tasks such as KGQA."}
{"id": "2505.21936", "pdf": "https://arxiv.org/pdf/2505.21936", "abs": "https://arxiv.org/abs/2505.21936", "authors": ["Zeyi Liao", "Jaylen Jones", "Linxi Jiang", "Eric Fosler-Lussier", "Yu Su", "Zhiqiang Lin", "Huan Sun"], "title": "RedTeamCUA: Realistic Adversarial Testing of Computer-Use Agents in Hybrid Web-OS Environments", "categories": ["cs.CL"], "comment": null, "summary": "Computer-use agents (CUAs) promise to automate complex tasks across operating\nsystems (OS) and the web, but remain vulnerable to indirect prompt injection.\nCurrent evaluations of this threat either lack support realistic but controlled\nenvironments or ignore hybrid web-OS attack scenarios involving both\ninterfaces. To address this, we propose RedTeamCUA, an adversarial testing\nframework featuring a novel hybrid sandbox that integrates a VM-based OS\nenvironment with Docker-based web platforms. Our sandbox supports key features\ntailored for red teaming, such as flexible adversarial scenario configuration,\nand a setting that decouples adversarial evaluation from navigational\nlimitations of CUAs by initializing tests directly at the point of an\nadversarial injection. Using RedTeamCUA, we develop RTC-Bench, a comprehensive\nbenchmark with 864 examples that investigate realistic, hybrid web-OS attack\nscenarios and fundamental security vulnerabilities. Benchmarking current\nfrontier CUAs identifies significant vulnerabilities: Claude 3.7 Sonnet | CUA\ndemonstrates an ASR of 42.9%, while Operator, the most secure CUA evaluated,\nstill exhibits an ASR of 7.6%. Notably, CUAs often attempt to execute\nadversarial tasks with an Attempt Rate as high as 92.5%, although failing to\ncomplete them due to capability limitations. Nevertheless, we observe\nconcerning ASRs of up to 50% in realistic end-to-end settings, with the\nrecently released frontier Claude 4 Opus | CUA showing an alarming ASR of 48%,\ndemonstrating that indirect prompt injection presents tangible risks for even\nadvanced CUAs despite their capabilities and safeguards. Overall, RedTeamCUA\nprovides an essential framework for advancing realistic, controlled, and\nsystematic analysis of CUA vulnerabilities, highlighting the urgent need for\nrobust defenses to indirect prompt injection prior to real-world deployment."}
{"id": "2505.21937", "pdf": "https://arxiv.org/pdf/2505.21937", "abs": "https://arxiv.org/abs/2505.21937", "authors": ["Pratik Rakesh Singh", "Kritarth Prasad", "Mohammadi Zaki", "Pankaj Wasnik"], "title": "Graph-Assisted Culturally Adaptable Idiomatic Translation for Indic Languages", "categories": ["cs.CL"], "comment": null, "summary": "Translating multi-word expressions (MWEs) and idioms requires a deep\nunderstanding of the cultural nuances of both the source and target languages.\nThis challenge is further amplified by the one-to-many nature of idiomatic\ntranslations, where a single source idiom can have multiple target-language\nequivalents depending on cultural references and contextual variations.\nTraditional static knowledge graphs (KGs) and prompt-based approaches struggle\nto capture these complex relationships, often leading to suboptimal\ntranslations. To address this, we propose IdiomCE, an adaptive graph neural\nnetwork (GNN) based methodology that learns intricate mappings between\nidiomatic expressions, effectively generalizing to both seen and unseen nodes\nduring training. Our proposed method enhances translation quality even in\nresource-constrained settings, facilitating improved idiomatic translation in\nsmaller models. We evaluate our approach on multiple idiomatic translation\ndatasets using reference-less metrics, demonstrating significant improvements\nin translating idioms from English to various Indian languages."}
{"id": "2505.21940", "pdf": "https://arxiv.org/pdf/2505.21940", "abs": "https://arxiv.org/abs/2505.21940", "authors": ["Bolei He", "Xinran He", "Mengke Chen", "Xianwei Xue", "Ying Zhu", "Zhenhua Ling"], "title": "RISE: Reasoning Enhancement via Iterative Self-Exploration in Multi-hop Question Answering", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large Language Models (LLMs) excel in many areas but continue to face\nchallenges with complex reasoning tasks, such as Multi-Hop Question Answering\n(MHQA). MHQA requires integrating evidence from diverse sources while managing\nintricate logical dependencies, often leads to errors in reasoning.\nRetrieval-Augmented Generation (RAG), widely employed in MHQA tasks, faces\nchallenges in effectively filtering noisy data and retrieving all necessary\nevidence, thereby limiting its effectiveness in addressing MHQA challenges. To\naddress these challenges, we propose RISE:Reasoning Enhancement via Iterative\nSelf-Exploration, a novel framework designed to enhance models' reasoning\ncapability through iterative self-exploration. Specifically, RISE involves\nthree key steps in addressing MHQA tasks: question decomposition,\nretrieve-then-read, and self-critique. By leveraging continuous\nself-exploration, RISE identifies accurate reasoning paths, iteratively\nself-improving the model's capability to integrate evidence, maintain logical\nconsistency, and enhance performance in MHQA tasks. Extensive experiments on\nmultiple MHQA benchmarks demonstrate that RISE significantly improves reasoning\naccuracy and task performance."}
{"id": "2505.21941", "pdf": "https://arxiv.org/pdf/2505.21941", "abs": "https://arxiv.org/abs/2505.21941", "authors": ["Ashim Gupta", "Vivek Srikumar"], "title": "Test-Time Scaling with Repeated Sampling Improves Multilingual Text Generation", "categories": ["cs.CL"], "comment": null, "summary": "Inference-time scaling via repeated sampling has shown promise in reasoning\ntasks, but its effectiveness in multilingual generation remains underexplored.\nWe evaluate this approach using perplexity- and reward-based verifiers on two\nmultilingual benchmarks: the Aya Evaluation Suite and m-ArenaHard. Our results\nshow consistent quality improvements, with gains exceeding 35% in some cases.\nWhile perplexity-based scoring is effective for open-ended prompts, only\nreward-based verifiers improve performance on tasks requiring reasoning (e.g.,\nmath, code). Our results demonstrate the broader utility of repeated sampling\nfor multilingual text generation and underscore the importance of selecting\nright verifiers for the task."}
{"id": "2505.21958", "pdf": "https://arxiv.org/pdf/2505.21958", "abs": "https://arxiv.org/abs/2505.21958", "authors": ["Qihuang Zhong", "Liang Ding", "Fei Liao", "Juhua Liu", "Bo Du", "Dacheng Tao"], "title": "Resolving Knowledge Conflicts in Domain-specific Data Selection: A Case Study on Medical Instruction-tuning", "categories": ["cs.CL"], "comment": null, "summary": "Domain-specific instruction-tuning has become the defacto standard for\nimproving the performance of large language models (LLMs) in specialized\napplications, e.g., medical question answering. Since the instruction-tuning\ndataset might contain redundant or low-quality data, data selection (DS) is\nusually required to maximize the data efficiency. Despite the successes in the\ngeneral domain, current DS methods often struggle to select the desired data\nfor domain-specific instruction-tuning. One of the main reasons is that they\nneglect the impact of knowledge conflicts, i.e., the discrepancy between LLMs'\npretrained knowledge and context knowledge of instruction data, which could\ndamage LLMs' prior abilities and lead to hallucination. To this end, we propose\na simple-yet-effective Knowledge-aware Data Selection (namely KDS) framework to\nselect the domain-specific instruction-tuning data that meets LLMs' actual\nneeds. The core of KDS is to leverage two knowledge-aware metrics for\nquantitatively measuring knowledge conflicts from two aspects: context-memory\nknowledge alignment and intra-memory knowledge consistency. By filtering the\ndata with large knowledge conflicts and sampling the high-quality and diverse\ndata, KDS can effectively stimulate the LLMs' abilities and achieve better\ndomain-specific performance. Taking the medical domain as the testbed, we\nconduct extensive experiments and empirically prove that KDS surpasses the\nother baselines and brings significant and consistent performance gains among\nall LLMs. More encouragingly, KDS effectively improves the model generalization\nand alleviates the hallucination problem."}
{"id": "2505.21963", "pdf": "https://arxiv.org/pdf/2505.21963", "abs": "https://arxiv.org/abs/2505.21963", "authors": ["Taro Yano", "Yoichi Ishibashi", "Masafumi Oyamada"], "title": "LaMDAgent: An Autonomous Framework for Post-Training Pipeline Optimization via LLM Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated exceptional performance across\na wide range of tasks. To further tailor LLMs to specific domains or\napplications, post-training techniques such as Supervised Fine-Tuning (SFT),\nPreference Learning, and model merging are commonly employed. While each of\nthese methods has been extensively studied in isolation, the automated\nconstruction of complete post-training pipelines remains an underexplored area.\nExisting approaches typically rely on manual design or focus narrowly on\noptimizing individual components, such as data ordering or merging strategies.\nIn this work, we introduce LaMDAgent (short for Language Model Developing\nAgent), a novel framework that autonomously constructs and optimizes full\npost-training pipelines through the use of LLM-based agents. LaMDAgent\nsystematically explores diverse model generation techniques, datasets, and\nhyperparameter configurations, leveraging task-based feedback to discover\nhigh-performing pipelines with minimal human intervention. Our experiments show\nthat LaMDAgent improves tool-use accuracy by 9.0 points while preserving\ninstruction-following capabilities. Moreover, it uncovers effective\npost-training strategies that are often overlooked by conventional human-driven\nexploration. We further analyze the impact of data and model size scaling to\nreduce computational costs on the exploration, finding that model size scalings\nintroduces new challenges, whereas scaling data size enables cost-effective\npipeline discovery."}
{"id": "2505.21967", "pdf": "https://arxiv.org/pdf/2505.21967", "abs": "https://arxiv.org/abs/2505.21967", "authors": ["Juan Ren", "Mark Dras", "Usman Naseem"], "title": "Seeing the Threat: Vulnerabilities in Vision-Language Models to Adversarial Attack", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Large Vision-Language Models (LVLMs) have shown remarkable capabilities\nacross a wide range of multimodal tasks. However, their integration of visual\ninputs introduces expanded attack surfaces, thereby exposing them to novel\nsecurity vulnerabilities. In this work, we conduct a systematic\nrepresentational analysis to uncover why conventional adversarial attacks can\ncircumvent the safety mechanisms embedded in LVLMs. We further propose a novel\ntwo stage evaluation framework for adversarial attacks on LVLMs. The first\nstage differentiates among instruction non compliance, outright refusal, and\nsuccessful adversarial exploitation. The second stage quantifies the degree to\nwhich the model's output fulfills the harmful intent of the adversarial prompt,\nwhile categorizing refusal behavior into direct refusals, soft refusals, and\npartial refusals that remain inadvertently helpful. Finally, we introduce a\nnormative schema that defines idealized model behavior when confronted with\nharmful prompts, offering a principled target for safety alignment in\nmultimodal systems."}
{"id": "2505.21979", "pdf": "https://arxiv.org/pdf/2505.21979", "abs": "https://arxiv.org/abs/2505.21979", "authors": ["Fakhraddin Alwajih", "Samar Mohamed Magdy", "Abdellah El Mekki", "Omer Nacar", "Youssef Nafea", "Safaa Taher Abdelfadil", "Abdulfattah Mohammed Yahya", "Hamzah Luqman", "Nada Almarwani", "Samah Aloufi", "Baraah Qawasmeh", "Houdaifa Atou", "Serry Sibaee", "Hamzah A. Alsayadi", "Walid Al-Dhabyani", "Maged S. Al-shaibani", "Aya El aatar", "Nour Qandos", "Rahaf Alhamouri", "Samar Ahmad", "Razan Khassib", "Lina Hamad", "Mohammed Anwar AL-Ghrawi", "Fatimah Alshamari", "Cheikh Malainine", "Doaa Qawasmeh", "Aminetou Yacoub", "Tfeil moilid", "Ruwa AbuHweidi", "Ahmed Aboeitta", "Vatimetou Mohamed Lemin", "Reem Abdel-Salam", "Ahlam Bashiti", "Adel Ammar", "Aisha Alansari", "Ahmed Ashraf", "Nora Alturayeif", "Sara Shatnawi", "Alcides Alcoba Inciarte", "AbdelRahim A. Elmadany", "Mohamedou cheikh tourad", "Ismail Berrada", "Mustafa Jarrar", "Shady Shehata", "Muhammad Abdul-Mageed"], "title": "Pearl: A Multimodal Culturally-Aware Arabic Instruction Dataset", "categories": ["cs.CL"], "comment": "https://github.com/UBC-NLP/pearl", "summary": "Mainstream large vision-language models (LVLMs) inherently encode cultural\nbiases, highlighting the need for diverse multimodal datasets. To address this\ngap, we introduce Pearl, a large-scale Arabic multimodal dataset and benchmark\nexplicitly designed for cultural understanding. Constructed through advanced\nagentic workflows and extensive human-in-the-loop annotations by 45 annotators\nfrom across the Arab world, Pearl comprises over K multimodal examples spanning\nten culturally significant domains covering all Arab countries. We further\nprovide two robust evaluation benchmarks Pearl and Pearl-Lite along with a\nspecialized subset Pearl-X explicitly developed to assess nuanced cultural\nvariations. Comprehensive evaluations on state-of-the-art open and proprietary\nLVLMs demonstrate that reasoning-centric instruction alignment substantially\nimproves models' cultural grounding compared to conventional scaling methods.\nPearl establishes a foundational resource for advancing culturally-informed\nmultimodal modeling research. All datasets and benchmarks are publicly\navailable."}
{"id": "2505.21997", "pdf": "https://arxiv.org/pdf/2505.21997", "abs": "https://arxiv.org/abs/2505.21997", "authors": ["Jihong Zhang", "Xinya Liang", "Anqi Deng", "Nicole Bonge", "Lin Tan", "Ling Zhang", "Nicole Zarrett"], "title": "Leveraging Interview-Informed LLMs to Model Survey Responses: Comparative Insights from AI-Generated and Human Data", "categories": ["cs.CL"], "comment": null, "summary": "Mixed methods research integrates quantitative and qualitative data but faces\nchallenges in aligning their distinct structures, particularly in examining\nmeasurement characteristics and individual response patterns. Advances in large\nlanguage models (LLMs) offer promising solutions by generating synthetic survey\nresponses informed by qualitative data. This study investigates whether LLMs,\nguided by personal interviews, can reliably predict human survey responses,\nusing the Behavioral Regulations in Exercise Questionnaire (BREQ) and\ninterviews from after-school program staff as a case study. Results indicate\nthat LLMs capture overall response patterns but exhibit lower variability than\nhumans. Incorporating interview data improves response diversity for some\nmodels (e.g., Claude, GPT), while well-crafted prompts and low-temperature\nsettings enhance alignment between LLM and human responses. Demographic\ninformation had less impact than interview content on alignment accuracy. These\nfindings underscore the potential of interview-informed LLMs to bridge\nqualitative and quantitative methodologies while revealing limitations in\nresponse variability, emotional interpretation, and psychometric fidelity.\nFuture research should refine prompt design, explore bias mitigation, and\noptimize model settings to enhance the validity of LLM-generated survey data in\nsocial science research."}
{"id": "2505.21999", "pdf": "https://arxiv.org/pdf/2505.21999", "abs": "https://arxiv.org/abs/2505.21999", "authors": ["Ashim Gupta", "Maitrey Mehta", "Zhichao Xu", "Vivek Srikumar"], "title": "Found in Translation: Measuring Multilingual LLM Consistency as Simple as Translate then Evaluate", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) provide detailed and impressive responses to\nqueries in English. However, are they really consistent at responding to the\nsame query in other languages? The popular way of evaluating for multilingual\nperformance of LLMs requires expensive-to-collect annotated datasets. Further,\nevaluating for tasks like open-ended generation, where multiple correct answers\nmay exist, is nontrivial. Instead, we propose to evaluate the predictability of\nmodel response across different languages. In this work, we propose a framework\nto evaluate LLM's cross-lingual consistency based on a simple Translate then\nEvaluate strategy. We instantiate this evaluation framework along two\ndimensions of consistency: information and empathy. Our results reveal\npronounced inconsistencies in popular LLM responses across thirty languages,\nwith severe performance deficits in certain language families and scripts,\nunderscoring critical weaknesses in their multilingual capabilities. These\nfindings necessitate cross-lingual evaluations that are consistent along\nmultiple dimensions. We invite practitioners to use our framework for future\nmultilingual LLM benchmarking."}
{"id": "2505.22003", "pdf": "https://arxiv.org/pdf/2505.22003", "abs": "https://arxiv.org/abs/2505.22003", "authors": ["Jatin Gupta", "Akhil Sharma", "Saransh Singhania", "Ali Imam Abidi"], "title": "Legal Assist AI: Leveraging Transformer-Based Model for Effective Legal Assistance", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 5 tables, 4 figures. This is a revised version of a preprint\n  previously available at this URL: https://doi.org/10.21203/rs.3.rs-5351879/v1", "summary": "Pursuit of accessible legal assistance in India faces a critical gap, as many\ncitizens struggle to leverage their legal rights due to limited awareness and\naccess to relevant legal information. This paper introduces Legal Assist AI, a\ntransformer-based model designed to bridge this gap by offering effective legal\nassistance through large language models (LLMs). The system retrieves relevant\nlegal information from a curated database and generates accurate responses,\nenabling effective assistance for diverse users, including legal professionals,\nscholars, and the general public. The model was fine-tuned on extensive\ndatasets from the Indian legal domain, including Indian Constitution, Bharatiya\nNyaya Sanhita (BNS), Bharatiya Nagarik Suraksha Sanhita (BNSS) and so forth,\nproviding a robust understanding of the complexities of Indian law. By\nincorporating domain-specific legal datasets, the proposed model demonstrated\nremarkable efficiency and specialization in legal Question-Answering. The model\nwas evaluated against state-of-the-art models such as GPT-3.5 Turbo and Mistral\n7B, achieving a 60.08% score on the AIBE, outperforming its competitors in\nlegal reasoning and accuracy. Unlike other models, Legal Assist AI avoided\ncommon issues such as hallucinations, making it highly reliable for practical\nlegal applications. It showcases the model's applicability in real-world legal\nscenarios, with future iterations aiming to enhance performance and expand its\ndataset to cover a broader range of multilingual and case-specific queries as\nwell."}
{"id": "2505.22017", "pdf": "https://arxiv.org/pdf/2505.22017", "abs": "https://arxiv.org/abs/2505.22017", "authors": ["Siqi Fan", "Peng Han", "Shuo Shang", "Yequan Wang", "Aixin Sun"], "title": "CoThink: Token-Efficient Reasoning via Instruct Models Guiding Reasoning Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) benefit from increased test-time compute, a\nphenomenon known as test-time scaling. However, reasoning-optimized models\noften overthink even simple problems, producing excessively verbose outputs and\nleading to low token efficiency. By comparing these models with equally sized\ninstruct models, we identify two key causes of this verbosity: (1)\nreinforcement learning reduces the information density of forward reasoning,\nand (2) backward chain-of thought training encourages redundant and often\nunnecessary verification steps. Since LLMs cannot assess the difficulty of a\ngiven problem, they tend to apply the same cautious reasoning strategy across\nall tasks, resulting in inefficient overthinking. To address this, we propose\nCoThink, an embarrassingly simple pipeline: an instruct model first drafts a\nhigh-level solution outline; a reasoning model then works out the solution. We\nobserve that CoThink enables dynamic adjustment of reasoning depth based on\ninput difficulty. Evaluated with three reasoning models DAPO, DeepSeek-R1, and\nQwQ on three datasets GSM8K, MATH500, and AIME24, CoThink reduces total token\ngeneration by 22.3% while maintaining pass@1 accuracy within a 0.42% margin on\naverage. With reference to the instruct model, we formally define reasoning\nefficiency and observe a potential reasoning efficiency scaling law in LLMs."}
{"id": "2505.22018", "pdf": "https://arxiv.org/pdf/2505.22018", "abs": "https://arxiv.org/abs/2505.22018", "authors": ["Ruicheng Yin", "Xuan Gao", "Changze Lv", "Xiaohua Wang", "Xiaoqing Zheng", "Xuanjing Huang"], "title": "Improving Continual Pre-training Through Seamless Data Packing", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Continual pre-training has demonstrated significant potential in enhancing\nmodel performance, particularly in domain-specific scenarios. The most common\napproach for packing data before continual pre-training involves concatenating\ninput texts and splitting them into fixed-length sequences. While\nstraightforward and efficient, this method often leads to excessive truncation\nand context discontinuity, which can hinder model performance. To address these\nissues, we explore the potential of data engineering to enhance continual\npre-training, particularly its impact on model performance and efficiency. We\npropose Seamless Packing (SP), a novel data packing strategy aimed at\npreserving contextual information more effectively and enhancing model\nperformance. Our approach employs a sliding window technique in the first stage\nthat synchronizes overlapping tokens across consecutive sequences, ensuring\nbetter continuity and contextual coherence. In the second stage, we adopt a\nFirst-Fit-Decreasing algorithm to pack shorter texts into bins slightly larger\nthan the target sequence length, thereby minimizing padding and truncation.\nEmpirical evaluations across various model architectures and corpus domains\ndemonstrate the effectiveness of our method, outperforming baseline method in\n99% of all settings. Code is available at\nhttps://github.com/Infernus-WIND/Seamless-Packing."}
{"id": "2505.22019", "pdf": "https://arxiv.org/pdf/2505.22019", "abs": "https://arxiv.org/abs/2505.22019", "authors": ["Qiuchen Wang", "Ruixue Ding", "Yu Zeng", "Zehui Chen", "Lin Chen", "Shihang Wang", "Pengjun Xie", "Fei Huang", "Feng Zhao"], "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich Information Understanding via Iterative Reasoning with Reinforcement Learning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": null, "summary": "Effectively retrieving, reasoning and understanding visually rich information\nremains a challenge for RAG methods. Traditional text-based methods cannot\nhandle visual-related information. On the other hand, current vision-based RAG\napproaches are often limited by fixed pipelines and frequently struggle to\nreason effectively due to the insufficient activation of the fundamental\ncapabilities of models. As RL has been proven to be beneficial for model\nreasoning, we introduce VRAG-RL, a novel RL framework tailored for complex\nreasoning across visually rich information. With this framework, VLMs interact\nwith search engines, autonomously sampling single-turn or multi-turn reasoning\ntrajectories with the help of visual perception tokens and undergoing continual\noptimization based on these samples. Our approach highlights key limitations of\nRL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely\nincorporate images into the context, leading to insufficient reasoning token\nallocation and neglecting visual-specific perception; and (ii) When models\ninteract with search engines, their queries often fail to retrieve relevant\ninformation due to the inability to articulate requirements, thereby leading to\nsuboptimal performance. To address these challenges, we define an action space\ntailored for visually rich inputs, with actions including cropping and scaling,\nallowing the model to gather information from a coarse-to-fine perspective.\nFurthermore, to bridge the gap between users' original inquiries and the\nretriever, we employ a simple yet effective reward that integrates query\nrewriting and retrieval performance with a model-based reward. Our VRAG-RL\noptimizes VLMs for RAG tasks using specially designed RL strategies, aligning\nthe model with real-world applications. The code is available at\n\\hyperlink{https://github.com/Alibaba-NLP/VRAG}{https://github.com/Alibaba-NLP/VRAG}."}
{"id": "2505.22037", "pdf": "https://arxiv.org/pdf/2505.22037", "abs": "https://arxiv.org/abs/2505.22037", "authors": ["Jingyu Zhang", "Ahmed Elgohary", "Xiawei Wang", "A S M Iftekhar", "Ahmed Magooda", "Benjamin Van Durme", "Daniel Khashabi", "Kyle Jackson"], "title": "Jailbreak Distillation: Renewable Safety Benchmarking", "categories": ["cs.CL", "cs.CR", "cs.SE"], "comment": "Project page: https://aka.ms/jailbreak-distillation", "summary": "Large language models (LLMs) are rapidly deployed in critical applications,\nraising urgent needs for robust safety benchmarking. We propose Jailbreak\nDistillation (JBDistill), a novel benchmark construction framework that\n\"distills\" jailbreak attacks into high-quality and easily-updatable safety\nbenchmarks. JBDistill utilizes a small set of development models and existing\njailbreak attack algorithms to create a candidate prompt pool, then employs\nprompt selection algorithms to identify an effective subset of prompts as\nsafety benchmarks. JBDistill addresses challenges in existing safety\nevaluation: the use of consistent evaluation prompts across models ensures fair\ncomparisons and reproducibility. It requires minimal human effort to rerun the\nJBDistill pipeline and produce updated benchmarks, alleviating concerns on\nsaturation and contamination. Extensive experiments demonstrate our benchmarks\ngeneralize robustly to 13 diverse evaluation models held out from benchmark\nconstruction, including proprietary, specialized, and newer-generation LLMs,\nsignificantly outperforming existing safety benchmarks in effectiveness while\nmaintaining high separability and diversity. Our framework thus provides an\neffective, sustainable, and adaptable solution for streamlining safety\nevaluation."}
{"id": "2505.22054", "pdf": "https://arxiv.org/pdf/2505.22054", "abs": "https://arxiv.org/abs/2505.22054", "authors": ["Samuel Stucki", "Jan Deriu", "Mark Cieliebak"], "title": "Voice Adaptation for Swiss German", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Submitted to Interspeech", "summary": "This work investigates the performance of Voice Adaptation models for Swiss\nGerman dialects, i.e., translating Standard German text to Swiss German dialect\nspeech. For this, we preprocess a large dataset of Swiss podcasts, which we\nautomatically transcribe and annotate with dialect classes, yielding\napproximately 5000 hours of weakly labeled training material. We fine-tune the\nXTTSv2 model on this dataset and show that it achieves good scores in human and\nautomated evaluations and can correctly render the desired dialect. Our work\nshows a step towards adapting Voice Cloning technology to underrepresented\nlanguages. The resulting model achieves CMOS scores of up to -0.28 and SMOS\nscores of 3.8."}
{"id": "2505.22061", "pdf": "https://arxiv.org/pdf/2505.22061", "abs": "https://arxiv.org/abs/2505.22061", "authors": ["Yujin Choi", "Youngjoo Park", "Junyoung Byun", "Jaewook Lee", "Jinseong Park"], "title": "Safeguarding Privacy of Retrieval Data against Membership Inference Attacks: Is This Query Too Close to Home?", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-augmented generation (RAG) mitigates the hallucination problem in\nlarge language models (LLMs) and has proven effective for specific,\npersonalized applications. However, passing private retrieved documents\ndirectly to LLMs introduces vulnerability to membership inference attacks\n(MIAs), which try to determine whether the target datum exists in the private\nexternal database or not. Based on the insight that MIA queries typically\nexhibit high similarity to only one target document, we introduce Mirabel, a\nsimilarity-based MIA detection framework designed for the RAG system. With the\nproposed Mirabel, we show that simple detect-and-hide strategies can\nsuccessfully obfuscate attackers, maintain data utility, and remain\nsystem-agnostic. We experimentally prove its detection and defense against\nvarious state-of-the-art MIA methods and its adaptability to existing private\nRAG systems."}
{"id": "2505.22068", "pdf": "https://arxiv.org/pdf/2505.22068", "abs": "https://arxiv.org/abs/2505.22068", "authors": ["Ran Li", "Shimin Di", "Yuchen Liu", "Chen Jing", "Yu Qiu", "Lei Chen"], "title": "Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Previous study suggest that powerful Large Language Models (LLMs) trained\nwith Reinforcement Learning with Verifiable Rewards (RLVR) only refines\nreasoning path without improving the reasoning capacity in math tasks while\nsupervised-finetuning(SFT) with distillation can. We study this from the view\nof Scientific information extraction (SciIE) where LLMs and reasoning LLMs\nunderperforms small Bert-based models. SciIE require both the reasoning and\nmemorization. We argue that both SFT and RLVR can refine the reasoning path and\nimprove reasoning capacity in a simple way based on SciIE. We propose two-stage\ntraining with 1. MimicSFT, using structured reasoning templates without needing\nhigh-quality chain-of-thought data, 2. R$^2$GRPO with relevance and\nrule-induced rewards. Experiments on scientific IE benchmarks show that both\nmethods can improve the reasoning capacity. R$^2$GRPO with mimicSFT surpasses\nbaseline LLMs and specialized supervised models in relation extraction. Our\ncode is available at https://github.com/ranlislz/R2GRPO."}
{"id": "2505.22076", "pdf": "https://arxiv.org/pdf/2505.22076", "abs": "https://arxiv.org/abs/2505.22076", "authors": ["Maja Stahl", "Timon Ziegenbein", "Joonsuk Park", "Henning Wachsmuth"], "title": "ArgInstruct: Specialized Instruction Fine-Tuning for Computational Argumentation", "categories": ["cs.CL"], "comment": null, "summary": "Training large language models (LLMs) to follow instructions has\nsignificantly enhanced their ability to tackle unseen tasks. However, despite\ntheir strong generalization capabilities, instruction-following LLMs encounter\ndifficulties when dealing with tasks that require domain knowledge. This work\nintroduces a specialized instruction fine-tuning for the domain of\ncomputational argumentation (CA). The goal is to enable an LLM to effectively\ntackle any unseen CA tasks while preserving its generalization capabilities.\nReviewing existing CA research, we crafted natural language instructions for\n105 CA tasks to this end. On this basis, we developed a CA-specific benchmark\nfor LLMs that allows for a comprehensive evaluation of LLMs' capabilities in\nsolving various CA tasks. We synthesized 52k CA-related instructions, adapting\nthe self-instruct process to train a CA-specialized instruction-following LLM.\nOur experiments suggest that CA-specialized instruction fine-tuning\nsignificantly enhances the LLM on both seen and unseen CA tasks. At the same\ntime, performance on the general NLP tasks of the SuperNI benchmark remains\nstable."}
{"id": "2505.22095", "pdf": "https://arxiv.org/pdf/2505.22095", "abs": "https://arxiv.org/abs/2505.22095", "authors": ["Chunyi Peng", "Zhipeng Xu", "Zhenghao Liu", "Yishan Li", "Yukun Yan", "Shuo Wang", "Zhiyuan Liu", "Yu Gu", "Minghe Yu", "Ge Yu", "Maosong Sun"], "title": "Learning to Route Queries Across Knowledge Bases for Step-wise Retrieval-Augmented Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Retrieval-Augmented Generation (MRAG) has shown promise in\nmitigating hallucinations in Multimodal Large Language Models (MLLMs) by\nincorporating external knowledge during generation. Existing MRAG methods\ntypically adopt a static retrieval pipeline that fetches relevant information\nfrom multiple Knowledge Bases (KBs), followed by a refinement step. However,\nthese approaches overlook the reasoning and planning capabilities of MLLMs to\ndynamically determine how to interact with different KBs during the reasoning\nprocess. To address this limitation, we propose R1-Router, a novel MRAG\nframework that learns to decide when and where to retrieve knowledge based on\nthe evolving reasoning state. Specifically, R1-Router can generate follow-up\nqueries according to the current reasoning step, routing these intermediate\nqueries to the most suitable KB, and integrating external knowledge into a\ncoherent reasoning trajectory to answer the original query. Furthermore, we\nintroduce Step-wise Group Relative Policy Optimization (Step-GRPO), a tailored\nreinforcement learning algorithm that assigns step-specific rewards to optimize\nthe reasoning behavior of MLLMs. Experimental results on various open-domain QA\nbenchmarks across multiple modalities demonstrate that R1-Router outperforms\nbaseline models by over 7%. Further analysis shows that R1-Router can\nadaptively and effectively leverage diverse KBs, reducing unnecessary\nretrievals and improving both efficiency and accuracy."}
{"id": "2505.22096", "pdf": "https://arxiv.org/pdf/2505.22096", "abs": "https://arxiv.org/abs/2505.22096", "authors": ["Jinheon Baek", "Horst Samulowitz", "Oktie Hassanzadeh", "Dharmashankar Subramanian", "Sola Shirai", "Alfio Gliozzo", "Debarun Bhattacharjya"], "title": "Knowledge Base Construction for Knowledge-Augmented Text-to-SQL", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "ACL Findings 2025", "summary": "Text-to-SQL aims to translate natural language queries into SQL statements,\nwhich is practical as it enables anyone to easily retrieve the desired\ninformation from databases. Recently, many existing approaches tackle this\nproblem with Large Language Models (LLMs), leveraging their strong capability\nin understanding user queries and generating corresponding SQL code. Yet, the\nparametric knowledge in LLMs might be limited to covering all the diverse and\ndomain-specific queries that require grounding in various database schemas,\nwhich makes generated SQLs less accurate oftentimes. To tackle this, we propose\nconstructing the knowledge base for text-to-SQL, a foundational source of\nknowledge, from which we retrieve and generate the necessary knowledge for\ngiven queries. In particular, unlike existing approaches that either manually\nannotate knowledge or generate only a few pieces of knowledge for each query,\nour knowledge base is comprehensive, which is constructed based on a\ncombination of all the available questions and their associated database\nschemas along with their relevant knowledge, and can be reused for unseen\ndatabases from different datasets and domains. We validate our approach on\nmultiple text-to-SQL datasets, considering both the overlapping and\nnon-overlapping database scenarios, where it outperforms relevant baselines\nsubstantially."}
{"id": "2505.22101", "pdf": "https://arxiv.org/pdf/2505.22101", "abs": "https://arxiv.org/abs/2505.22101", "authors": ["Zhiyu Li", "Shichao Song", "Hanyu Wang", "Simin Niu", "Ding Chen", "Jiawei Yang", "Chenyang Xi", "Huayi Lai", "Jihao Zhao", "Yezhaohui Wang", "Junpeng Ren", "Zehao Lin", "Jiahao Huo", "Tianyi Chen", "Kai Chen", "Kehang Li", "Zhiqiang Yin", "Qingchen Yu", "Bo Tang", "Hongkang Yang", "Zhi-Qin John Xu", "Feiyu Xiong"], "title": "MemOS: An Operating System for Memory-Augmented Generation (MAG) in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have emerged as foundational infrastructure in\nthe pursuit of Artificial General Intelligence (AGI). Despite their remarkable\ncapabilities in language perception and generation, current LLMs fundamentally\nlack a unified and structured architecture for handling memory. They primarily\nrely on parametric memory (knowledge encoded in model weights) and ephemeral\nactivation memory (context-limited runtime states). While emerging methods like\nRetrieval-Augmented Generation (RAG) incorporate plaintext memory, they lack\nlifecycle management and multi-modal integration, limiting their capacity for\nlong-term knowledge evolution. To address this, we introduce MemOS, a memory\noperating system designed for LLMs that, for the first time, elevates memory to\na first-class operational resource. It builds unified mechanisms for\nrepresentation, organization, and governance across three core memory types:\nparametric, activation, and plaintext. At its core is the MemCube, a\nstandardized memory abstraction that enables tracking, fusion, and migration of\nheterogeneous memory, while offering structured, traceable access across tasks\nand contexts. MemOS establishes a memory-centric execution framework with\nstrong controllability, adaptability, and evolvability. It fills a critical gap\nin current LLM infrastructure and lays the groundwork for continual adaptation,\npersonalized intelligence, and cross-platform coordination in next-generation\nintelligent systems."}
{"id": "2505.22107", "pdf": "https://arxiv.org/pdf/2505.22107", "abs": "https://arxiv.org/abs/2505.22107", "authors": ["Shuhai Zhang", "Zeng You", "Yaofo Chen", "Zhiquan Wen", "Qianyue Wang", "Zhijie Qiu", "Yuanqing Li", "Mingkui Tan"], "title": "Curse of High Dimensionality Issue in Transformer for Long-context Modeling", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at ICML 2025", "summary": "Transformer-based large language models (LLMs) excel in natural language\nprocessing tasks by capturing long-range dependencies through self-attention\nmechanisms. However, long-context modeling faces significant computational\ninefficiencies due to \\textit{redundant} attention computations: while\nattention weights are often \\textit{sparse}, all tokens consume \\textit{equal}\ncomputational resources. In this paper, we reformulate traditional\nprobabilistic sequence modeling as a \\textit{supervised learning task},\nenabling the separation of relevant and irrelevant tokens and providing a\nclearer understanding of redundancy. Based on this reformulation, we\ntheoretically analyze attention sparsity, revealing that only a few tokens\nsignificantly contribute to predictions. Building on this, we formulate\nattention optimization as a linear coding problem and propose a \\textit{group\ncoding strategy}, theoretically showing its ability to improve robustness\nagainst random noise and enhance learning efficiency. Motivated by this, we\npropose \\textit{Dynamic Group Attention} (DGA), which leverages the group\ncoding to explicitly reduce redundancy by aggregating less important tokens\nduring attention computation. Empirical results show that our DGA significantly\nreduces computational costs while maintaining competitive performance.Code is\navailable at https://github.com/bolixinyu/DynamicGroupAttention."}
{"id": "2505.22113", "pdf": "https://arxiv.org/pdf/2505.22113", "abs": "https://arxiv.org/abs/2505.22113", "authors": ["Zhiyuan Li", "Yi Chang", "Yuan Wu"], "title": "THINK-Bench: Evaluating Thinking Efficiency and Chain-of-Thought Quality of Large Reasoning Models", "categories": ["cs.CL"], "comment": "20 pages, 8 figures, 6 tables", "summary": "Large reasoning models (LRMs) have achieved impressive performance in complex\ntasks, often outperforming conventional large language models (LLMs). However,\nthe prevalent issue of overthinking severely limits their computational\nefficiency. Overthinking occurs when models generate excessive and redundant\ntokens that contribute little to accurate outcomes, especially in simple tasks,\nresulting in a significant waste of computational resources. To systematically\ninvestigate this issue, we introduce Think-Bench, a benchmark designed to\nevaluate the reasoning efficiency of LRMs. We also propose novel efficiency\nmetrics and conduct a comprehensive evaluation of various LRMs across multiple\ndimensions, including the reasoning process, outcome quality, and\nchain-of-thought (CoT) characteristics. Our analysis reveals that most LRMs\nexhibit overthinking in handling easy questions, generating unnecessarily\nlengthy reasoning chains. While many LRMs demonstrate high CoT quality, several\nsuffer from low efficiency. We hope that Think-Bench can serve as a robust\nfoundation for advancing research into LRMs."}
{"id": "2505.22116", "pdf": "https://arxiv.org/pdf/2505.22116", "abs": "https://arxiv.org/abs/2505.22116", "authors": ["Jintao Zhang", "Zirui Liu", "Mingyue Cheng", "Shilong Zhang", "Tingyue Pan", "Qi Liu", "Yanhu Xie"], "title": "Multimodal Forecasting of Sparse Intraoperative Hypotension Events Powered by Language Model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Intraoperative hypotension (IOH) frequently occurs under general anesthesia\nand is strongly linked to adverse outcomes such as myocardial injury and\nincreased mortality. Despite its significance, IOH prediction is hindered by\nevent sparsity and the challenge of integrating static and dynamic data across\ndiverse patients. In this paper, we propose \\textbf{IOHFuseLM}, a multimodal\nlanguage model framework. To accurately identify and differentiate sparse\nhypotensive events, we leverage a two-stage training strategy. The first stage\ninvolves domain adaptive pretraining on IOH physiological time series augmented\nthrough diffusion methods, thereby enhancing the model sensitivity to patterns\nassociated with hypotension. Subsequently, task fine-tuning is performed on the\noriginal clinical dataset to further enhance the ability to distinguish\nnormotensive from hypotensive states. To enable multimodal fusion for each\npatient, we align structured clinical descriptions with the corresponding\nphysiological time series at the token level. Such alignment enables the model\nto capture individualized temporal patterns alongside their corresponding\nclinical semantics. In addition, we convert static patient attributes into\nstructured text to enrich personalized information. Experimental evaluations on\ntwo intraoperative datasets demonstrate that IOHFuseLM outperforms established\nbaselines in accurately identifying IOH events, highlighting its applicability\nin clinical decision support scenarios. Our code is publicly available to\npromote reproducibility at https://github.com/zjt-gpu/IOHFuseLM."}
{"id": "2505.22118", "pdf": "https://arxiv.org/pdf/2505.22118", "abs": "https://arxiv.org/abs/2505.22118", "authors": ["Alan Ramponi", "Marco Rovera", "Robert Moro", "Sara Tonelli"], "title": "Multilingual vs Crosslingual Retrieval of Fact-Checked Claims: A Tale of Two Approaches", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval of previously fact-checked claims is a well-established task, whose\nautomation can assist professional fact-checkers in the initial steps of\ninformation verification. Previous works have mostly tackled the task\nmonolingually, i.e., having both the input and the retrieved claims in the same\nlanguage. However, especially for languages with a limited availability of\nfact-checks and in case of global narratives, such as pandemics, wars, or\ninternational politics, it is crucial to be able to retrieve claims across\nlanguages. In this work, we examine strategies to improve the multilingual and\ncrosslingual performance, namely selection of negative examples (in the\nsupervised) and re-ranking (in the unsupervised setting). We evaluate all\napproaches on a dataset containing posts and claims in 47 languages (283\nlanguage combinations). We observe that the best results are obtained by using\nLLM-based re-ranking, followed by fine-tuning with negative examples sampled\nusing a sentence similarity-based strategy. Most importantly, we show that\ncrosslinguality is a setup with its own unique characteristics compared to the\nmultilingual setup."}
{"id": "2505.22120", "pdf": "https://arxiv.org/pdf/2505.22120", "abs": "https://arxiv.org/abs/2505.22120", "authors": ["Runyu Wang", "Peng Ping", "Zhengyu Guo", "Xiaoye Zhang", "Quan Shi", "Liting Zhou", "Tianbo Ji"], "title": "LoKI: Low-damage Knowledge Implanting of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning adapts pretrained models for specific tasks but poses the risk of\ncatastrophic forgetting (CF), where critical knowledge from pre-training is\noverwritten. Current Parameter-Efficient Fine-Tuning (PEFT) methods for Large\nLanguage Models (LLMs), while efficient, often sacrifice general capabilities.\nTo address the issue of CF in a general-purpose PEFT framework, we propose\n\\textbf{Lo}w-damage \\textbf{K}nowledge \\textbf{I}mplanting (\\textbf{LoKI}), a\nPEFT technique that is based on a mechanistic understanding of how knowledge is\nstored in transformer architectures. In two real-world scenarios, LoKI\ndemonstrates task-specific performance that is comparable to or even surpasses\nthat of full fine-tuning and LoRA-based methods across various model types,\nwhile significantly better preserving general capabilities. Our work connects\nmechanistic insights into LLM knowledge storage with practical fine-tuning\nobjectives, achieving state-of-the-art trade-offs between task specialization\nand the preservation of general capabilities. Our implementation is publicly\navailable as ready-to-use code\\footnote{https://github.com/Nexround/LoKI}."}
{"id": "2505.22131", "pdf": "https://arxiv.org/pdf/2505.22131", "abs": "https://arxiv.org/abs/2505.22131", "authors": ["Zhuoyang Wu", "Xinze Li", "Zhenghao Liu", "Yukun Yan", "Zhiyuan Liu", "Minghe Yu", "Cheng Yang", "Yu Gu", "Ge Yu", "Maosong Sun"], "title": "EULER: Enhancing the Reasoning Ability of Large Language Models through Error-Induced Learning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong reasoning capabilities\nand achieved promising results in mathematical problem-solving tasks. Learning\nfrom errors offers the potential to further enhance the performance of LLMs\nduring Supervised Fine-Tuning (SFT). However, the errors in synthesized\nsolutions are typically gathered from sampling trails, making it challenging to\ngenerate solution errors for each mathematical problem. This paper introduces\nthe Error-IndUced LEaRning (EULER) model, which aims to develop an error\nexposure model that generates high-quality solution errors to enhance the\nmathematical reasoning capabilities of LLMs. Specifically, EULER optimizes the\nerror exposure model to increase the generation probability of self-made\nsolution errors while utilizing solutions produced by a superior LLM to\nregularize the generation quality. Our experiments across various mathematical\nproblem datasets demonstrate the effectiveness of the EULER model, achieving an\nimprovement of over 4% compared to all baseline models. Further analysis\nreveals that EULER is capable of synthesizing more challenging and educational\nsolution errors, which facilitate both the training and inference processes of\nLLMs. All codes are available at https://github.com/NEUIR/EULER."}
{"id": "2505.22135", "pdf": "https://arxiv.org/pdf/2505.22135", "abs": "https://arxiv.org/abs/2505.22135", "authors": ["Yuichiro Hoshino", "Hideyuki Tachibana", "Muneyoshi Inahara", "Hiroto Takegawa"], "title": "RAD: Redundancy-Aware Distillation for Hybrid Models via Self-Speculative Decoding", "categories": ["cs.CL", "cs.LG"], "comment": "26 pages", "summary": "Hybrid models combining Transformers and State Space Models (SSMs) are\npromising for balancing performance and efficiency. However, optimizing these\nhybrid models, particularly by addressing the potential redundancy inherent\nwithin the Transformer components, remains a significant challenge. In this\npaper, we propose RAD (Redundancy-Aware Distillation), a novel framework that\nuses self-speculative decoding as a diagnostic tool to identify redundant\nattention layers within the model. These identified layers are then selectively\nreplaced with SSM components, followed by targeted (self-)distillation.\nSpecifically, RAD focuses knowledge transfer on the components identified as\nredundant, considering architectural changes and specific weight initialization\nstrategies. We experimentally demonstrate that self-distillation using RAD\nsignificantly surpasses the performance of the original base model on\nmathematical and coding tasks. Furthermore, RAD is also effective in standard\nknowledge distillation settings, achieving up to approximately 2x faster\nconvergence compared to baseline methods. Notably, while a baseline model\ndistilled from a Llama-3.1 70B teacher achieves scores of 46.17 on GSM8K and\n22.75 on CRUX, RAD achieves significantly higher scores of 71.27 on GSM8K and\n28.25 on CRUX, even when using a much smaller Llama-3.1 8B teacher. RAD offers\na new pathway for efficient optimization and performance enhancement in the\ndistillation of hybrid models."}
{"id": "2505.22137", "pdf": "https://arxiv.org/pdf/2505.22137", "abs": "https://arxiv.org/abs/2505.22137", "authors": ["Marc Feger", "Katarina Boland", "Stefan Dietze"], "title": "Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This paper has been accepted to ACL 2025 and will be published after\n  27.07.2025", "summary": "Identifying arguments is a necessary prerequisite for various tasks in\nautomated discourse analysis, particularly within contexts such as political\ndebates, online discussions, and scientific reasoning. In addition to\ntheoretical advances in understanding the constitution of arguments, a\nsignificant body of research has emerged around practical argument mining,\nsupported by a growing number of publicly available datasets. On these\nbenchmarks, BERT-like transformers have consistently performed best,\nreinforcing the belief that such models are broadly applicable across diverse\ncontexts of debate. This study offers the first large-scale re-evaluation of\nsuch state-of-the-art models, with a specific focus on their ability to\ngeneralize in identifying arguments. We evaluate four transformers, three\nstandard and one enhanced with contrastive pre-training for better\ngeneralization, on 17 English sentence-level datasets as most relevant to the\ntask. Our findings show that, to varying degrees, these models tend to rely on\nlexical shortcuts tied to content words, suggesting that apparent progress may\noften be driven by dataset-specific cues rather than true task alignment. While\nthe models achieve strong results on familiar benchmarks, their performance\ndrops markedly when applied to unseen datasets. Nonetheless, incorporating both\ntask-specific pre-training and joint benchmark training proves effective in\nenhancing both robustness and generalization."}
{"id": "2505.22156", "pdf": "https://arxiv.org/pdf/2505.22156", "abs": "https://arxiv.org/abs/2505.22156", "authors": ["Shuaiyi Li", "Zhisong Zhang", "Yang Deng", "Chenlong Deng", "Tianqing Fang", "Hongming Zhang", "Haitao Mi", "Dong Yu", "Wai Lam"], "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for Efficient Model Editing", "categories": ["cs.CL"], "comment": "Under review", "summary": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method."}
{"id": "2505.22157", "pdf": "https://arxiv.org/pdf/2505.22157", "abs": "https://arxiv.org/abs/2505.22157", "authors": ["Paramita Mirza", "Lucas Weber", "Fabian Küch"], "title": "Stratified Selective Sampling for Instruction Tuning with Dedicated Scoring Strategy", "categories": ["cs.CL"], "comment": null, "summary": "Recent work shows that post-training datasets for LLMs can be substantially\ndownsampled without noticeably deteriorating performance. However, data\nselection often incurs high computational costs or is limited to narrow\ndomains. In this paper, we demonstrate that data selection can be both --\nefficient and universal -- by using a multi-step pipeline in which we\nefficiently bin data points into groups, estimate quality using specialized\nmodels, and score difficulty with a robust, lightweight method. Task-based\ncategorization allows us to control the composition of our final data --\ncrucial for finetuning multi-purpose models. To guarantee diversity, we improve\nupon previous work using embedding models and a clustering algorithm. This\nintegrated strategy enables high-performance fine-tuning with minimal overhead."}
{"id": "2505.22165", "pdf": "https://arxiv.org/pdf/2505.22165", "abs": "https://arxiv.org/abs/2505.22165", "authors": ["Bocheng Li", "Zhujin Gao", "Linli Xu"], "title": "Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Diffusion models have emerged as a promising approach for text generation,\nwith recent works falling into two main categories: discrete and continuous\ndiffusion models. Discrete diffusion models apply token corruption\nindependently using categorical distributions, allowing for different diffusion\nprogress across tokens but lacking fine-grained control. Continuous diffusion\nmodels map tokens to continuous spaces and apply fine-grained noise, but the\ndiffusion progress is uniform across tokens, limiting their ability to capture\nsemantic nuances. To address these limitations, we propose\n\\textbf{\\underline{N}}on-simultan\\textbf{\\underline{e}}ous\nC\\textbf{\\underline{o}}ntinuous \\textbf{\\underline{Diff}}usion Models\n(NeoDiff), a novel diffusion model that integrates the strengths of both\ndiscrete and continuous approaches. NeoDiff introduces a Poisson diffusion\nprocess for the forward process, enabling a flexible and fine-grained noising\nparadigm, and employs a time predictor for the reverse process to adaptively\nmodulate the denoising progress based on token semantics. Furthermore, NeoDiff\nutilizes an optimized schedule for inference to ensure more precise noise\ncontrol and improved performance. Our approach unifies the theories of discrete\nand continuous diffusion models, offering a more principled and effective\nframework for text generation. Experimental results on several text generation\ntasks demonstrate NeoDiff's superior performance compared to baselines of\nnon-autoregressive continuous and discrete diffusion models, iterative-based\nmethods and autoregressive diffusion-based methods. These results highlight\nNeoDiff's potential as a powerful tool for generating high-quality text and\nadvancing the field of diffusion-based text generation."}
{"id": "2505.22169", "pdf": "https://arxiv.org/pdf/2505.22169", "abs": "https://arxiv.org/abs/2505.22169", "authors": ["Gili Lior", "Eliya Habba", "Shahar Levy", "Avi Caciularu", "Gabriel Stanovsky"], "title": "ReliableEval: A Recipe for Stochastic LLM Evaluation via Method of Moments", "categories": ["cs.CL"], "comment": null, "summary": "LLMs are highly sensitive to prompt phrasing, yet standard benchmarks\ntypically report performance using a single prompt, raising concerns about the\nreliability of such evaluations. In this work, we argue for a stochastic method\nof moments evaluation over the space of meaning-preserving prompt\nperturbations. We introduce a formal definition of reliable evaluation that\naccounts for prompt sensitivity, and suggest ReliableEval - a method for\nestimating the number of prompt resamplings needed to obtain meaningful\nresults. Using our framework, we stochastically evaluate five frontier LLMs and\nfind that even top-performing models like GPT-4o and Claude-3.7-Sonnet exhibit\nsubstantial prompt sensitivity. Our approach is model-, task-, and\nmetric-agnostic, offering a recipe for meaningful and robust LLM evaluation."}
{"id": "2505.22172", "pdf": "https://arxiv.org/pdf/2505.22172", "abs": "https://arxiv.org/abs/2505.22172", "authors": ["Xiang Huang", "Ting-En Lin", "Feiteng Fang", "Yuchuan Wu", "Hangyu Li", "Yuzhong Qu", "Fei Huang", "Yongbin Li"], "title": "Reverse Preference Optimization for Complex Instruction Following", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Instruction following (IF) is a critical capability for large language models\n(LLMs). However, handling complex instructions with multiple constraints\nremains challenging. Previous methods typically select preference pairs based\non the number of constraints they satisfy, introducing noise where chosen\nexamples may fail to follow some constraints and rejected examples may excel in\ncertain respects over the chosen ones. To address the challenge of aligning\nwith multiple preferences, we propose a simple yet effective method called\nReverse Preference Optimization (RPO). It mitigates noise in preference pairs\nby dynamically reversing the constraints within the instruction to ensure the\nchosen response is perfect, alleviating the burden of extensive sampling and\nfiltering to collect perfect responses. Besides, reversal also enlarges the gap\nbetween chosen and rejected responses, thereby clarifying the optimization\ndirection and making it more robust to noise. We evaluate RPO on two multi-turn\nIF benchmarks, Sysbench and Multi-IF, demonstrating average improvements over\nthe DPO baseline of 4.6 and 2.5 points (on Llama-3.1 8B), respectively.\nMoreover, RPO scales effectively across model sizes (8B to 70B parameters),\nwith the 70B RPO model surpassing GPT-4o."}
{"id": "2505.22176", "pdf": "https://arxiv.org/pdf/2505.22176", "abs": "https://arxiv.org/abs/2505.22176", "authors": ["Vihang Pancholi", "Jainit Bafna", "Tejas Anvekar", "Manish Shrivastava", "Vivek Gupta"], "title": "TabXEval: Why this is a Bad Table? An eXhaustive Rubric for Table Evaluation", "categories": ["cs.CL"], "comment": "Accepeted for Findings at ACL 2025", "summary": "Evaluating tables qualitatively & quantitatively presents a significant\nchallenge, as traditional metrics often fail to capture nuanced structural and\ncontent discrepancies. To address this, we introduce a novel, methodical rubric\nintegrating multi-level structural descriptors with fine-grained contextual\nquantification, thereby establishing a robust foundation for comprehensive\ntable comparison. Building on this foundation, we propose TabXEval, an\neXhaustive and eXplainable two-phase evaluation framework. TabXEval initially\naligns reference tables structurally via TabAlign & subsequently conducts a\nsystematic semantic and syntactic comparison using TabCompare; this approach\nclarifies the evaluation process and pinpoints subtle discrepancies overlooked\nby conventional methods. The efficacy of this framework is assessed using\nTabXBench, a novel, diverse, multi-domain benchmark we developed, featuring\nrealistic table perturbations and human-annotated assessments. Finally, a\nsystematic analysis of existing evaluation methods through\nsensitivity-specificity trade-offs demonstrates the qualitative and\nquantitative effectiveness of TabXEval across diverse table-related tasks and\ndomains, paving the way for future innovations in explainable table evaluation."}
{"id": "2505.22179", "pdf": "https://arxiv.org/pdf/2505.22179", "abs": "https://arxiv.org/abs/2505.22179", "authors": ["Yudi Zhang", "Weilin Zhao", "Xu Han", "Tiejun Zhao", "Wang Xu", "Hailong Cao", "Conghui Zhu"], "title": "Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "12 pages, 5 figures", "summary": "Speculative decoding and quantization effectively accelerate memory-bound\ninference of large language models. Speculative decoding mitigates the memory\nbandwidth bottleneck by verifying multiple tokens within a single forward pass,\nwhich increases computational effort. Quantization achieves this optimization\nby compressing weights and activations into lower bit-widths and also reduces\ncomputations via low-bit matrix multiplications. To further leverage their\nstrengths, we investigate the integration of these two techniques.\nSurprisingly, experiments applying the advanced speculative decoding method\nEAGLE-2 to various quantized models reveal that the memory benefits from 4-bit\nweight quantization are diminished by the computational load from speculative\ndecoding. Specifically, verifying a tree-style draft incurs significantly more\ntime overhead than a single-token forward pass on 4-bit weight quantized\nmodels. This finding led to our new speculative decoding design: a hierarchical\nframework that employs a small model as an intermediate stage to turn\ntree-style drafts into sequence drafts, leveraging the memory access benefits\nof the target quantized model. Experimental results show that our hierarchical\napproach achieves a 2.78$\\times$ speedup across various tasks for the 4-bit\nweight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\\times$.\nCode available at https://github.com/AI9Stars/SpecMQuant."}
{"id": "2505.22184", "pdf": "https://arxiv.org/pdf/2505.22184", "abs": "https://arxiv.org/abs/2505.22184", "authors": ["Xuchen Ma", "Jianxiang Yu", "Wenming Shao", "Bo Pang", "Xiang Li"], "title": "Breaking the Cloak! Unveiling Chinese Cloaked Toxicity with Homophone Graph and Toxic Lexicon", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 5 figures, 9 tables", "summary": "Social media platforms have experienced a significant rise in toxic content,\nincluding abusive language and discriminatory remarks, presenting growing\nchallenges for content moderation. Some users evade censorship by deliberately\ndisguising toxic words through homophonic cloak, which necessitates the task of\nunveiling cloaked toxicity. Existing methods are mostly designed for English\ntexts, while Chinese cloaked toxicity unveiling has not been solved yet. To\ntackle the issue, we propose C$^2$TU, a novel training-free and prompt-free\nmethod for Chinese cloaked toxic content unveiling. It first employs substring\nmatching to identify candidate toxic words based on Chinese homo-graph and\ntoxic lexicon. Then it filters those candidates that are non-toxic and corrects\ncloaks to be their corresponding toxicities. Specifically, we develop two model\nvariants for filtering, which are based on BERT and LLMs, respectively. For\nLLMs, we address the auto-regressive limitation in computing word occurrence\nprobability and utilize the full semantic contexts of a text sequence to reveal\ncloaked toxic words. Extensive experiments demonstrate that C$^2$TU can achieve\nsuperior performance on two Chinese toxic datasets. In particular, our method\noutperforms the best competitor by up to 71% on the F1 score and 35% on\naccuracy, respectively."}
{"id": "2505.22202", "pdf": "https://arxiv.org/pdf/2505.22202", "abs": "https://arxiv.org/abs/2505.22202", "authors": ["Hyeonbin Hwang", "Byeongguk Jeon", "Seungone Kim", "Jiyeon Kim", "Hoyeon Chang", "Sohee Yang", "Seungpil Won", "Dohaeng Lee", "Youbin Ahn", "Minjoon Seo"], "title": "Let's Predict Sentence by Sentence", "categories": ["cs.CL", "cs.AI"], "comment": "Work In Progress", "summary": "Autoregressive language models (LMs) generate one token at a time, yet human\nreasoning operates over higher-level abstractions - sentences, propositions,\nand concepts. This contrast raises a central question- Can LMs likewise learn\nto reason over structured semantic units rather than raw token sequences? In\nthis work, we investigate whether pretrained LMs can be lifted into such\nabstract reasoning spaces by building on their learned representations. We\npresent a framework that adapts a pretrained token-level LM to operate in\nsentence space by autoregressively predicting continuous embeddings of next\nsentences. We explore two embedding paradigms inspired by classical\nrepresentation learning: 1) semantic embeddings, learned via autoencoding to\npreserve surface meaning; and 2) contextual embeddings, trained via\nnext-sentence prediction to encode anticipatory structure. We evaluate both\nunder two inference regimes: Discretized, which decodes each predicted\nembedding into text before re-encoding; and Continuous, which reasons entirely\nin embedding space for improved efficiency. Across four domains - mathematics,\nlogic, commonsense, and planning - contextual embeddings under continuous\ninference show competitive performance with Chain-of-Thought (CoT) while\nreducing inference-time FLOPs on average by half. We also present early signs\nof scalability and modular adaptation. Finally, to visualize latent\ntrajectories, we introduce SentenceLens, a diagnostic tool that decodes\nintermediate model states into interpretable sentences. Together, our results\nindicate that pretrained LMs can effectively transition to abstract, structured\nreasoning within latent embedding spaces."}
{"id": "2505.22232", "pdf": "https://arxiv.org/pdf/2505.22232", "abs": "https://arxiv.org/abs/2505.22232", "authors": ["Mehdi Ali", "Manuel Brack", "Max Lübbering", "Elias Wendt", "Abbas Goher Khan", "Richard Rutmann", "Alex Jude", "Maurice Kraus", "Alexander Arno Weber", "Felix Stollenwerk", "David Kaczér", "Florian Mai", "Lucie Flek", "Rafet Sifa", "Nicolas Flores-Herr", "Joachim Köhler", "Patrick Schramowski", "Michael Fromm", "Kristian Kersting"], "title": "Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project page available at https://huggingface.co/spaces/Jackal-AI/JQL", "summary": "High-quality multilingual training data is essential for effectively\npretraining large language models (LLMs). Yet, the availability of suitable\nopen-source multilingual datasets remains limited. Existing state-of-the-art\ndatasets mostly rely on heuristic filtering methods, restricting both their\ncross-lingual transferability and scalability. Here, we introduce JQL, a\nsystematic approach that efficiently curates diverse and high-quality\nmultilingual data at scale while significantly reducing computational demands.\nJQL distills LLMs' annotation capabilities into lightweight annotators based on\npretrained multilingual embeddings. These models exhibit robust multilingual\nand cross-lingual performance, even for languages and scripts unseen during\ntraining. Evaluated empirically across 35 languages, the resulting annotation\npipeline substantially outperforms current heuristic filtering methods like\nFineweb2. JQL notably enhances downstream model training quality and increases\ndata retention rates. Our research provides practical insights and valuable\nresources for multilingual data curation, raising the standards of multilingual\ndataset development."}
{"id": "2505.22236", "pdf": "https://arxiv.org/pdf/2505.22236", "abs": "https://arxiv.org/abs/2505.22236", "authors": ["Charlotte Pouw", "Afra Alishahi", "Willem Zuidema"], "title": "A Linguistically Motivated Analysis of Intonational Phrasing in Text-to-Speech Systems: Revealing Gaps in Syntactic Sensitivity", "categories": ["cs.CL"], "comment": "Accepted to CoNLL 2025", "summary": "We analyze the syntactic sensitivity of Text-to-Speech (TTS) systems using\nmethods inspired by psycholinguistic research. Specifically, we focus on the\ngeneration of intonational phrase boundaries, which can often be predicted by\nidentifying syntactic boundaries within a sentence. We find that TTS systems\nstruggle to accurately generate intonational phrase boundaries in sentences\nwhere syntactic boundaries are ambiguous (e.g., garden path sentences or\nsentences with attachment ambiguity). In these cases, systems need superficial\ncues such as commas to place boundaries at the correct positions. In contrast,\nfor sentences with simpler syntactic structures, we find that systems do\nincorporate syntactic cues beyond surface markers. Finally, we finetune models\non sentences without commas at the syntactic boundary positions, encouraging\nthem to focus on more subtle linguistic cues. Our findings indicate that this\nleads to more distinct intonation patterns that better reflect the underlying\nstructure."}
{"id": "2505.22240", "pdf": "https://arxiv.org/pdf/2505.22240", "abs": "https://arxiv.org/abs/2505.22240", "authors": ["Yunsoo Kim", "Yusuf Abdulle", "Honghan Wu"], "title": "BioHopR: A Benchmark for Multi-Hop, Multi-Answer Reasoning in Biomedical Domain", "categories": ["cs.CL"], "comment": null, "summary": "Biomedical reasoning often requires traversing interconnected relationships\nacross entities such as drugs, diseases, and proteins. Despite the increasing\nprominence of large language models (LLMs), existing benchmarks lack the\nability to evaluate multi-hop reasoning in the biomedical domain, particularly\nfor queries involving one-to-many and many-to-many relationships. This gap\nleaves the critical challenges of biomedical multi-hop reasoning underexplored.\nTo address this, we introduce BioHopR, a novel benchmark designed to evaluate\nmulti-hop, multi-answer reasoning in structured biomedical knowledge graphs.\nBuilt from the comprehensive PrimeKG, BioHopR includes 1-hop and 2-hop\nreasoning tasks that reflect real-world biomedical complexities.\n  Evaluations of state-of-the-art models reveal that O3-mini, a proprietary\nreasoning-focused model, achieves 37.93% precision on 1-hop tasks and 14.57% on\n2-hop tasks, outperforming proprietary models such as GPT4O and open-source\nbiomedical models including HuatuoGPT-o1-70B and Llama-3.3-70B. However, all\nmodels exhibit significant declines in multi-hop performance, underscoring the\nchallenges of resolving implicit reasoning steps in the biomedical domain. By\naddressing the lack of benchmarks for multi-hop reasoning in biomedical domain,\nBioHopR sets a new standard for evaluating reasoning capabilities and\nhighlights critical gaps between proprietary and open-source models while\npaving the way for future advancements in biomedical LLMs."}
{"id": "2505.22264", "pdf": "https://arxiv.org/pdf/2505.22264", "abs": "https://arxiv.org/abs/2505.22264", "authors": ["Maximiliano Hormazábal Lagos", "Álvaro Bueno Saez", "Héctor Cerezo-Costas", "Pedro Alonso Doval", "Jorge Alcalde Vesteiro"], "title": "MRT at SemEval-2025 Task 8: Maximizing Recovery from Tables with Multiple Steps", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "7 pages, 6 tables", "summary": "In this paper we expose our approach to solve the \\textit{SemEval 2025 Task\n8: Question-Answering over Tabular Data} challenge. Our strategy leverages\nPython code generation with LLMs to interact with the table and get the answer\nto the questions. The process is composed of multiple steps: understanding the\ncontent of the table, generating natural language instructions in the form of\nsteps to follow in order to get the answer, translating these instructions to\ncode, running it and handling potential errors or exceptions. These steps use\nopen source LLMs and fine grained optimized prompts for each task (step). With\nthis approach, we achieved a score of $70.50\\%$ for subtask 1."}
{"id": "2505.22273", "pdf": "https://arxiv.org/pdf/2505.22273", "abs": "https://arxiv.org/abs/2505.22273", "authors": ["Shohei Higashiyama", "Masao Utiyama"], "title": "Comprehensive Evaluation on Lexical Normalization: Boundary-Aware Approaches for Unsegmented Languages", "categories": ["cs.CL"], "comment": "23 pages", "summary": "Lexical normalization research has sought to tackle the challenge of\nprocessing informal expressions in user-generated text, yet the absence of\ncomprehensive evaluations leaves it unclear which methods excel across multiple\nperspectives. Focusing on unsegmented languages, we make three key\ncontributions: (1) creating a large-scale, multi-domain Japanese normalization\ndataset, (2) developing normalization methods based on state-of-the-art\npretrained models, and (3) conducting experiments across multiple evaluation\nperspectives. Our experiments show that both encoder-only and decoder-only\napproaches achieve promising results in both accuracy and efficiency."}
{"id": "2505.22280", "pdf": "https://arxiv.org/pdf/2505.22280", "abs": "https://arxiv.org/abs/2505.22280", "authors": ["Zihan Xu", "Haotian Ma", "Gongbo Zhang", "Yihao Ding", "Chunhua Weng", "Yifan Peng"], "title": "Natural Language Processing in Support of Evidence-based Medicine: A Scoping Review", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "Evidence-based medicine (EBM) is at the forefront of modern healthcare,\nemphasizing the use of the best available scientific evidence to guide clinical\ndecisions. Due to the sheer volume and rapid growth of medical literature and\nthe high cost of curation, there is a critical need to investigate Natural\nLanguage Processing (NLP) methods to identify, appraise, synthesize, summarize,\nand disseminate evidence in EBM. This survey presents an in-depth review of 129\nresearch studies on leveraging NLP for EBM, illustrating its pivotal role in\nenhancing clinical decision-making processes. The paper systematically explores\nhow NLP supports the five fundamental steps of EBM -- Ask, Acquire, Appraise,\nApply, and Assess. The review not only identifies current limitations within\nthe field but also proposes directions for future research, emphasizing the\npotential for NLP to revolutionize EBM by refining evidence extraction,\nevidence synthesis, appraisal, summarization, enhancing data comprehensibility,\nand facilitating a more efficient clinical workflow."}
{"id": "2505.22293", "pdf": "https://arxiv.org/pdf/2505.22293", "abs": "https://arxiv.org/abs/2505.22293", "authors": ["Samuel Frontull", "Thomas Ströhle"], "title": "Compensating for Data with Reasoning: Low-Resource Machine Translation with LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\nmultilingual machine translation, sometimes even outperforming traditional\nneural systems. However, previous research has highlighted the challenges of\nusing LLMs, particularly with prompt engineering, for low-resource languages.\nIn this work, we introduce Fragment-Shot Prompting, a novel in-context learning\nmethod that segments input and retrieves translation examples based on\nsyntactic coverage, along with Pivoted Fragment-Shot, an extension that enables\ntranslation without direct parallel data. We evaluate these methods using\nGPT-3.5, GPT-4o, o1-mini, LLaMA-3.3, and DeepSeek-R1 for translation between\nItalian and two Ladin variants, revealing three key findings: (1) Fragment-Shot\nPrompting is effective for translating into and between the studied\nlow-resource languages, with syntactic coverage positively correlating with\ntranslation quality; (2) Models with stronger reasoning abilities make more\neffective use of retrieved knowledge, generally produce better translations,\nand enable Pivoted Fragment-Shot to significantly improve translation quality\nbetween the Ladin variants; and (3) prompt engineering offers limited, if any,\nimprovements when translating from a low-resource to a high-resource language,\nwhere zero-shot prompting already yields satisfactory results. We publicly\nrelease our code and the retrieval corpora."}
{"id": "2505.22296", "pdf": "https://arxiv.org/pdf/2505.22296", "abs": "https://arxiv.org/abs/2505.22296", "authors": ["Haosheng Zou", "Xiaowei Lv", "Shousheng Jia", "Xiangzheng Zhang"], "title": "360-LLaMA-Factory: Plug & Play Sequence Parallelism for Long Post-Training", "categories": ["cs.CL", "cs.LG"], "comment": "code at https://github.com/Qihoo360/360-LLaMA-Factory", "summary": "Adding sequence parallelism into LLaMA-Factory, we open-sourced\n360-LLaMA-Factory at https://github.com/Qihoo360/360-LLaMA-Factory.\n360-LLaMA-Factory has received wide recognition and used in models such as\nLight-R1 arXiv:2503.10460, TinyR1 arXiv:2503.04872, Kaggle AIMO math models and\nalso in large companies' training frameworks. This technical report delves\ndeeper into the different sequence parallel modes behind 360-LLaMA-Factory and\ndiscusses our implementation insights."}
{"id": "2505.22298", "pdf": "https://arxiv.org/pdf/2505.22298", "abs": "https://arxiv.org/abs/2505.22298", "authors": ["Yifan Lu", "Jing Li", "Yigeng Zhou", "Yihui Zhang", "Wenya Wang", "Xiucheng Li", "Meishan Zhang", "Fangming Liu", "Jun Yu", "Min Zhang"], "title": "Adaptive Detoxification: Safeguarding General Capabilities of LLMs through Toxicity-Aware Knowledge Editing", "categories": ["cs.CL"], "comment": "ACL 2025 Findings", "summary": "Large language models (LLMs) exhibit impressive language capabilities but\nremain vulnerable to malicious prompts and jailbreaking attacks. Existing\nknowledge editing methods for LLM detoxification face two major challenges.\nFirst, they often rely on entity-specific localization, making them ineffective\nagainst adversarial inputs without explicit entities. Second, these methods\nsuffer from over-editing, where detoxified models reject legitimate queries,\ncompromising overall performance. In this paper, we propose ToxEdit, a\ntoxicity-aware knowledge editing approach that dynamically detects toxic\nactivation patterns during forward propagation. It then routes computations\nthrough adaptive inter-layer pathways to mitigate toxicity effectively. This\ndesign ensures precise toxicity mitigation while preserving LLMs' general\ncapabilities. To more accurately assess over-editing, we also enhance the\nSafeEdit benchmark by incorporating instruction-following evaluation tasks.\nExperimental results on multiple LLMs demonstrate that our ToxEdit outperforms\nprevious state-of-the-art methods in both detoxification performance and\nsafeguarding general capabilities of LLMs."}
{"id": "2505.22318", "pdf": "https://arxiv.org/pdf/2505.22318", "abs": "https://arxiv.org/abs/2505.22318", "authors": ["Ishwar B Balappanawar", "Vamshi Krishna Bonagiri", "Anish R Joishy", "Manas Gaur", "Krishnaprasad Thirunarayan", "Ponnurangam Kumaraguru"], "title": "If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?", "categories": ["cs.CL", "cs.LG"], "comment": "16 pages, 5 figures", "summary": "Large Language Models (LLMs) demonstrate impressive reasoning capabilities in\nfamiliar contexts, but struggle when the context conflicts with their\nparametric knowledge. To investigate this phenomenon, we introduce\nCounterLogic, a dataset containing 1,800 examples across 9 logical schemas,\nexplicitly designed to evaluate logical reasoning through counterfactual\n(hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11\nLLMs across 6 different datasets reveals a consistent performance degradation,\nwith accuracies dropping by 27% on average when reasoning through\ncounterfactual information. We propose Self-Segregate, a prompting method\nenabling metacognitive awareness (explicitly identifying knowledge conflicts)\nbefore reasoning. Our method dramatically narrows the average performance gaps\nfrom 27% to just 11%, while significantly increasing the overall accuracy\n(+7.5%). We discuss the implications of these findings and draw parallels to\nhuman cognitive processes, particularly on how humans disambiguate conflicting\ninformation during reasoning tasks. Our findings offer practical insights for\nunderstanding and enhancing LLMs reasoning capabilities in real-world\napplications, especially where models must logically reason independently of\ntheir factual knowledge."}
{"id": "2505.22323", "pdf": "https://arxiv.org/pdf/2505.22323", "abs": "https://arxiv.org/abs/2505.22323", "authors": ["Hongcan Guo", "Haolang Lu", "Guoshun Nan", "Bolun Chu", "Jialin Zhuang", "Yuan Yang", "Wenhao Che", "Sicong Leng", "Qimei Cui", "Xudong Jiang"], "title": "Advancing Expert Specialization for Better MoE", "categories": ["cs.CL", "cs.SE", "68T07", "I.2.7"], "comment": "33pages, 6figures", "summary": "Mixture-of-Experts (MoE) models enable efficient scaling of large language\nmodels (LLMs) by activating only a subset of experts per input. However, we\nobserve that the commonly used auxiliary load balancing loss often leads to\nexpert overlap and overly uniform routing, which hinders expert specialization\nand degrades overall performance during post-training. To address this, we\npropose a simple yet effective solution that introduces two complementary\nobjectives: (1) an orthogonality loss to encourage experts to process distinct\ntypes of tokens, and (2) a variance loss to encourage more discriminative\nrouting decisions. Gradient-level analysis demonstrates that these objectives\nare compatible with the existing auxiliary loss and contribute to optimizing\nthe training process. Experimental results over various model architectures and\nacross multiple benchmarks show that our method significantly enhances expert\nspecialization. Notably, our method improves classic MoE baselines with\nauxiliary loss by up to 23.79%, while also maintaining load balancing in\ndownstream tasks, without any architectural modifications or additional\ncomponents. We will release our code to contribute to the community."}
{"id": "2505.22327", "pdf": "https://arxiv.org/pdf/2505.22327", "abs": "https://arxiv.org/abs/2505.22327", "authors": ["Antonia Karamolegkou", "Angana Borah", "Eunjung Cho", "Sagnik Ray Choudhury", "Martina Galletti", "Rajarshi Ghosh", "Pranav Gupta", "Oana Ignat", "Priyanka Kargupta", "Neema Kotonya", "Hemank Lamba", "Sun-Joo Lee", "Arushi Mangla", "Ishani Mondal", "Deniz Nazarova", "Poli Nemkova", "Dina Pisarevskaya", "Naquee Rizwan", "Nazanin Sabri", "Dominik Stammbach", "Anna Steinberg", "David Tomás", "Steven R Wilson", "Bowen Yi", "Jessica H Zhu", "Arkaitz Zubiaga", "Anders Søgaard", "Alexander Fraser", "Zhijing Jin", "Rada Mihalcea", "Joel R. Tetreault", "Daryna Dementieva"], "title": "NLP for Social Good: A Survey of Challenges, Opportunities, and Responsible Deployment", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have unlocked\nunprecedented possibilities across a range of applications. However, as a\ncommunity, we believe that the field of Natural Language Processing (NLP) has a\ngrowing need to approach deployment with greater intentionality and\nresponsibility. In alignment with the broader vision of AI for Social Good\n(Toma\\v{s}ev et al., 2020), this paper examines the role of NLP in addressing\npressing societal challenges. Through a cross-disciplinary analysis of social\ngoals and emerging risks, we highlight promising research directions and\noutline challenges that must be addressed to ensure responsible and equitable\nprogress in NLP4SG research."}
{"id": "2505.22334", "pdf": "https://arxiv.org/pdf/2505.22334", "abs": "https://arxiv.org/abs/2505.22334", "authors": ["Lai Wei", "Yuting Li", "Kaipeng Zheng", "Chen Wang", "Yue Wang", "Linghe Kong", "Lichao Sun", "Weiran Huang"], "title": "Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have demonstrated\nimpressive chain-of-thought reasoning capabilities, with reinforcement learning\n(RL) playing a crucial role in this progress. While \"aha moment\"\npatterns--where models exhibit self-correction through reflection--are often\nattributed to emergent properties from RL, we first demonstrate that these\npatterns exist in multimodal LLMs (MLLMs) prior to RL training but may not\nnecessarily correlate with improved reasoning performance. Building on these\ninsights, we present a comprehensive study on enhancing multimodal reasoning\nthrough a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start\nwith structured chain-of-thought reasoning patterns, followed by (2)\nreinforcement learning via GRPO to further refine these capabilities. Our\nextensive experiments show that this combined approach consistently outperforms\nboth SFT-only and RL-only methods across challenging multimodal reasoning\nbenchmarks. The resulting models achieve state-of-the-art performance among\nopen-source MLLMs at both 3B and 7B scales, with our 7B model showing\nsubstantial improvements over base models (e.g., 66.3 %$\\rightarrow$73.4 % on\nMathVista, 62.9 %$\\rightarrow$70.4 % on We-Math) and our 3B model achieving\nperformance competitive with several 7B models. Overall, this work provides\npractical guidance for building advanced multimodal reasoning models. Our code\nis available at https://github.com/waltonfuture/RL-with-Cold-Start."}
{"id": "2505.22338", "pdf": "https://arxiv.org/pdf/2505.22338", "abs": "https://arxiv.org/abs/2505.22338", "authors": ["Hanyang Wang", "Lu Wang", "Chaoyun Zhang", "Tianjun Mao", "Si Qin", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "Text2Grad: Reinforcement Learning from Natural Language Feedback", "categories": ["cs.CL", "cs.AI"], "comment": "The code for our method is available at\n  https://github.com/microsoft/Text2Grad", "summary": "Traditional RLHF optimizes language models with coarse, scalar rewards that\nmask the fine-grained reasons behind success or failure, leading to slow and\nopaque learning. Recent work augments RL with textual critiques through\nprompting or reflection, improving interpretability but leaving model\nparameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm\nthat turns free-form textual feedback into span-level gradients. Given human\n(or programmatic) critiques, Text2Grad aligns each feedback phrase with the\nrelevant token spans, converts these alignments into differentiable reward\nsignals, and performs gradient updates that directly refine the offending\nportions of the model's policy. This yields precise, feedback-conditioned\nadjustments instead of global nudges. Text2Grad is realized through three\ncomponents: (1) a high-quality feedback-annotation pipeline that pairs\ncritiques with token spans; (2) a fine-grained reward model that predicts\nspan-level reward on answer while generating explanatory critiques; and (3) a\nspan-level policy optimizer that back-propagates natural-language gradients.\nAcross summarization, code generation, and question answering, Text2Grad\nconsistently surpasses scalar-reward RL and prompt-only baselines, providing\nboth higher task metrics and richer interpretability. Our results demonstrate\nthat natural-language feedback, when converted to gradients, is a powerful\nsignal for fine-grained policy optimization. The code for our method is\navailable at https://github.com/microsoft/Text2Grad"}
{"id": "2505.22354", "pdf": "https://arxiv.org/pdf/2505.22354", "abs": "https://arxiv.org/abs/2505.22354", "authors": ["Judith Sieker", "Clara Lachenmaier", "Sina Zarrieß"], "title": "LLMs Struggle to Reject False Presuppositions when Misinformation Stakes are High", "categories": ["cs.CL"], "comment": "8 pages (including References). Accepted at CogSci 2025", "summary": "This paper examines how LLMs handle false presuppositions and whether certain\nlinguistic factors influence their responses to falsely presupposed content.\nPresuppositions subtly introduce information as given, making them highly\neffective at embedding disputable or false information. This raises concerns\nabout whether LLMs, like humans, may fail to detect and correct misleading\nassumptions introduced as false presuppositions, even when the stakes of\nmisinformation are high. Using a systematic approach based on linguistic\npresupposition analysis, we investigate the conditions under which LLMs are\nmore or less sensitive to adopt or reject false presuppositions. Focusing on\npolitical contexts, we examine how factors like linguistic construction,\npolitical party, and scenario probability impact the recognition of false\npresuppositions. We conduct experiments with a newly created dataset and\nexamine three LLMs: OpenAI's GPT-4-o, Meta's LLama-3-8B, and MistralAI's\nMistral-7B-v03. Our results show that the models struggle to recognize false\npresuppositions, with performance varying by condition. This study highlights\nthat linguistic presupposition analysis is a valuable tool for uncovering the\nreinforcement of political misinformation in LLM responses."}
{"id": "2505.22375", "pdf": "https://arxiv.org/pdf/2505.22375", "abs": "https://arxiv.org/abs/2505.22375", "authors": ["Hanting Chen", "Yasheng Wang", "Kai Han", "Dong Li", "Lin Li", "Zhenni Bi", "Jinpeng Li", "Haoyu Wang", "Fei Mi", "Mingjian Zhu", "Bin Wang", "Kaikai Song", "Yifei Fu", "Xu He", "Yu Luo", "Chong Zhu", "Quan He", "Xueyu Wu", "Wei He", "Hailin Hu", "Yehui Tang", "Dacheng Tao", "Xinghao Chen", "Yunhe Wang", "Other Contributors"], "title": "Pangu Embedded: An Efficient Dual-system LLM Reasoner with Metacognition", "categories": ["cs.CL"], "comment": null, "summary": "This work presents Pangu Embedded, an efficient Large Language Model (LLM)\nreasoner developed on Ascend Neural Processing Units (NPUs), featuring flexible\nfast and slow thinking capabilities. Pangu Embedded addresses the significant\ncomputational costs and inference latency challenges prevalent in existing\nreasoning-optimized LLMs. We propose a two-stage training framework for its\nconstruction. In Stage 1, the model is finetuned via an iterative distillation\nprocess, incorporating inter-iteration model merging to effectively aggregate\ncomplementary knowledge. This is followed by reinforcement learning on Ascend\nclusters, optimized by a latency-tolerant scheduler that combines stale\nsynchronous parallelism with prioritized data queues. The RL process is guided\nby a Multi-source Adaptive Reward System (MARS), which generates dynamic,\ntask-specific reward signals using deterministic metrics and lightweight LLM\nevaluators for mathematics, coding, and general problem-solving tasks. Stage 2\nintroduces a dual-system framework, endowing Pangu Embedded with a \"fast\" mode\nfor routine queries and a deeper \"slow\" mode for complex inference. This\nframework offers both manual mode switching for user control and an automatic,\ncomplexity-aware mode selection mechanism that dynamically allocates\ncomputational resources to balance latency and reasoning depth. Experimental\nresults on benchmarks including AIME 2024, GPQA, and LiveCodeBench demonstrate\nthat Pangu Embedded with 7B parameters, outperforms similar-size models like\nQwen3-8B and GLM4-9B. It delivers rapid responses and state-of-the-art\nreasoning quality within a single, unified model architecture, highlighting a\npromising direction for developing powerful yet practically deployable LLM\nreasoners."}
{"id": "2505.22430", "pdf": "https://arxiv.org/pdf/2505.22430", "abs": "https://arxiv.org/abs/2505.22430", "authors": ["Kun Li", "Yunxiang Li", "Tianhua Zhang", "Hongyin Luo", "Xixin Wu", "James Glass", "Helen Meng"], "title": "RAG-Zeval: Towards Robust and Interpretable Evaluation on RAG Responses through End-to-End Rule-Guided Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Robust evaluation is critical for deploying trustworthy retrieval-augmented\ngeneration (RAG) systems. However, current LLM-based evaluation frameworks\npredominantly rely on directly prompting resource-intensive models with complex\nmulti-stage prompts, underutilizing models' reasoning capabilities and\nintroducing significant computational cost. In this paper, we present RAG-Zeval\n(RAG-Zero Evaluator), a novel end-to-end framework that formulates faithfulness\nand correctness evaluation as a rule-guided reasoning task. Our approach trains\nevaluators with reinforcement learning, facilitating compact models to generate\ncomprehensive and sound assessments with detailed explanation in one-pass. We\nintroduce a ranking-based outcome reward mechanism, using preference judgments\nrather than absolute scores, to address the challenge of obtaining precise\npointwise reward signals. To this end, we synthesize the ranking references by\ngenerating quality-controlled responses with zero human annotation. Experiments\ndemonstrate RAG-Zeval's superior performance, achieving the strongest\ncorrelation with human judgments and outperforming baselines that rely on LLMs\nwith 10-100 times more parameters. Our approach also exhibits superior\ninterpretability in response evaluation."}
{"id": "2505.22453", "pdf": "https://arxiv.org/pdf/2505.22453", "abs": "https://arxiv.org/abs/2505.22453", "authors": ["Lai Wei", "Yuting Li", "Chen Wang", "Yue Wang", "Linghe Kong", "Weiran Huang", "Lichao Sun"], "title": "Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Improving Multi-modal Large Language Models (MLLMs) in the post-training\nstage typically relies on supervised fine-tuning (SFT) or reinforcement\nlearning (RL). However, these supervised methods require expensive and manually\nannotated multi-modal data--an ultimately unsustainable resource. While recent\nefforts have explored unsupervised post-training, their methods are complex and\ndifficult to iterate. In this work, we are the first to investigate the use of\nGRPO, a stable and scalable online RL algorithm, for enabling continual\nself-improvement without any external supervision. We propose MM-UPT, a simple\nyet effective framework for unsupervised post-training of MLLMs. MM-UPT builds\nupon GRPO, replacing traditional reward signals with a self-rewarding mechanism\nbased on majority voting over multiple sampled responses. Our experiments\ndemonstrate that MM-UPT significantly improves the reasoning ability of\nQwen2.5-VL-7B (e.g., 66.3 %$\\rightarrow$72.9 % on MathVista, 62.9\n%$\\rightarrow$68.7 % on We-Math), using standard dataset without ground truth\nlabels. MM-UPT also outperforms prior unsupervised baselines and even\napproaches the results of supervised GRPO. Furthermore, we show that\nincorporating synthetic questions, generated solely by MLLM itself, can boost\nperformance as well, highlighting a promising approach for scalable\nself-improvement. Overall, MM-UPT offers a new paradigm for continual,\nautonomous enhancement of MLLMs in the absence of external supervision. Our\ncode is available at https://github.com/waltonfuture/MM-UPT."}
{"id": "2505.22501", "pdf": "https://arxiv.org/pdf/2505.22501", "abs": "https://arxiv.org/abs/2505.22501", "authors": ["Dingchu Zhang", "Yida Zhao", "Jialong Wu", "Baixuan Li", "Wenbiao Yin", "Liwen Zhang", "Yong Jiang", "Yufeng Li", "Kewei Tu", "Pengjun Xie", "Fei Huang"], "title": "EvolveSearch: An Iterative Self-Evolving Search Agent", "categories": ["cs.CL"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has transformed the\nlandscape of agentic information seeking capabilities through the integration\nof tools such as search engines and web browsers. However, current mainstream\napproaches for enabling LLM web search proficiency face significant challenges:\nsupervised fine-tuning struggles with data production in open-search domains,\nwhile RL converges quickly, limiting their data utilization efficiency. To\naddress these issues, we propose EvolveSearch, a novel iterative self-evolution\nframework that combines SFT and RL to enhance agentic web search capabilities\nwithout any external human-annotated reasoning data. Extensive experiments on\nseven multi-hop question-answering (MHQA) benchmarks demonstrate that\nEvolveSearch consistently improves performance across iterations, ultimately\nachieving an average improvement of 4.7\\% over the current state-of-the-art\nacross seven benchmarks, opening the door to self-evolution agentic\ncapabilities in open web search domains."}
{"id": "2505.22517", "pdf": "https://arxiv.org/pdf/2505.22517", "abs": "https://arxiv.org/abs/2505.22517", "authors": ["Yimeng Gu", "Zhao Tong", "Ignacio Castro", "Shu Wu", "Gareth Tyson"], "title": "Multi-MLLM Knowledge Distillation for Out-of-Context News Detection", "categories": ["cs.CL", "cs.MM"], "comment": null, "summary": "Multimodal out-of-context news is a type of misinformation in which the image\nis used outside of its original context. Many existing works have leveraged\nmultimodal large language models (MLLMs) for detecting out-of-context news.\nHowever, observing the limited zero-shot performance of smaller MLLMs, they\ngenerally require label-rich fine-tuning and/or expensive API calls to GPT\nmodels to improve the performance, which is impractical in low-resource\nscenarios. In contrast, we aim to improve the performance of small MLLMs in a\nmore label-efficient and cost-effective manner. To this end, we first prompt\nmultiple teacher MLLMs to generate both label predictions and corresponding\nrationales, which collectively serve as the teachers' knowledge. We then\nintroduce a two-stage knowledge distillation framework to transfer this\nknowledge to a student MLLM. In Stage 1, we apply LoRA fine-tuning to the\nstudent model using all training data. In Stage 2, we further fine-tune the\nstudent model using both LoRA fine-tuning and DPO on the data points where\nteachers' predictions conflict. This two-stage strategy reduces annotation\ncosts and helps the student model uncover subtle patterns in more challenging\ncases. Experimental results demonstrate that our approach achieves\nstate-of-the-art performance using less than 10% labeled data."}
{"id": "2505.22548", "pdf": "https://arxiv.org/pdf/2505.22548", "abs": "https://arxiv.org/abs/2505.22548", "authors": ["Changhao Song", "Yazhou Zhang", "Peng Zhang"], "title": "Emotion-o1: Adaptive Long Reasoning for Emotion Understanding in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Emotion understanding includes basic tasks (e.g., sentiment/emotion\nclassification) and advanced tasks (e.g., sarcasm/humor detection). Current\nmethods rely on fixed-length CoT reasoning, failing to adapt to the varying\ncomplexity of emotions. We propose a task-adaptive reasoning framework that\nemploys DeepSeek-R1 to generate variable-length reasoning chains for different\nemotion tasks. By combining fine-tuning with reinforcement learning, we design\na composite reward function that balances four objectives: prediction accuracy,\nadaptive reasoning depth control, structural diversity in reasoning paths, and\nsuppression of repetitive logic. This approach achieves dynamic\ncontext-sensitive inference while enabling LLMs to autonomously develop deep\nreasoning capabilities. Experimental results demonstrate consistent\nimprovements in both Acc and F1 scores across four tasks: emotion, sentiment,\nhumor, and sarcasm. Notably, peak enhancements reached 3.56% F1 (2.76% Acc) for\nbasic tasks and 37.95% F1 (23.14% Acc) for advanced tasks. Our work bridges\nrigid CoT reasoning and emotional complexity through adaptive-depth analysis."}
{"id": "2505.22552", "pdf": "https://arxiv.org/pdf/2505.22552", "abs": "https://arxiv.org/abs/2505.22552", "authors": ["Hoang Pham", "Thanh-Do Nguyen", "Khac-Hoai Nam Bui"], "title": "ClaimPKG: Enhancing Claim Verification via Pseudo-Subgraph Generation with Lightweight Specialized LLM", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "Accepted by ACL 2025 findings", "summary": "Integrating knowledge graphs (KGs) to enhance the reasoning capabilities of\nlarge language models (LLMs) is an emerging research challenge in claim\nverification. While KGs provide structured, semantically rich representations\nwell-suited for reasoning, most existing verification methods rely on\nunstructured text corpora, limiting their ability to effectively leverage KGs.\nAdditionally, despite possessing strong reasoning abilities, modern LLMs\nstruggle with multi-step modular pipelines and reasoning over KGs without\nadaptation. To address these challenges, we propose ClaimPKG, an end-to-end\nframework that seamlessly integrates LLM reasoning with structured knowledge\nfrom KGs. Specifically, the main idea of ClaimPKG is to employ a lightweight,\nspecialized LLM to represent the input claim as pseudo-subgraphs, guiding a\ndedicated subgraph retrieval module to identify relevant KG subgraphs. These\nretrieved subgraphs are then processed by a general-purpose LLM to produce the\nfinal verdict and justification. Extensive experiments on the FactKG dataset\ndemonstrate that ClaimPKG achieves state-of-the-art performance, outperforming\nstrong baselines in this research field by 9%-12% accuracy points across\nmultiple categories. Furthermore, ClaimPKG exhibits zero-shot generalizability\nto unstructured datasets such as HoVer and FEVEROUS, effectively combining\nstructured knowledge from KGs with LLM reasoning across various LLM backbones."}
{"id": "2505.22563", "pdf": "https://arxiv.org/pdf/2505.22563", "abs": "https://arxiv.org/abs/2505.22563", "authors": ["Yu Lei", "Xingyang Ge", "Yi Zhang", "Yiming Yang", "Bolei Ma"], "title": "Do Large Language Models Think Like the Brain? Sentence-Level Evidence from fMRI and Hierarchical Embeddings", "categories": ["cs.CL", "q-bio.NC"], "comment": null, "summary": "Understanding whether large language models (LLMs) and the human brain\nconverge on similar computational principles remains a fundamental and\nimportant question in cognitive neuroscience and AI. Do the brain-like patterns\nobserved in LLMs emerge simply from scaling, or do they reflect deeper\nalignment with the architecture of human language processing? This study\nfocuses on the sentence-level neural mechanisms of language models,\nsystematically investigating how hierarchical representations in LLMs align\nwith the dynamic neural responses during human sentence comprehension. By\ncomparing hierarchical embeddings from 14 publicly available LLMs with fMRI\ndata collected from participants, who were exposed to a naturalistic narrative\nstory, we constructed sentence-level neural prediction models to precisely\nidentify the model layers most significantly correlated with brain region\nactivations. Results show that improvements in model performance drive the\nevolution of representational architectures toward brain-like hierarchies,\nparticularly achieving stronger functional and anatomical correspondence at\nhigher semantic abstraction levels."}
{"id": "2505.22571", "pdf": "https://arxiv.org/pdf/2505.22571", "abs": "https://arxiv.org/abs/2505.22571", "authors": ["Hoang Pham", "Khac-Hoai Nam Bui"], "title": "Agent-UniRAG: A Trainable Open-Source LLM Agent Framework for Unified Retrieval-Augmented Generation Systems", "categories": ["cs.CL", "cs.AI", "cs.DB", "cs.IR"], "comment": null, "summary": "This paper presents a novel approach for unified retrieval-augmented\ngeneration (RAG) systems using the recent emerging large language model (LLM)\nagent concept. Specifically, Agent LLM, which utilizes LLM as fundamental\ncontrollers, has become a promising approach to enable the interpretability of\nRAG tasks, especially for complex reasoning question-answering systems (e.g.,\nmulti-hop queries). Nonetheless, previous works mainly focus on solving RAG\nsystems with either single-hop or multi-hop approaches separately, which limits\nthe application of those approaches to real-world applications. In this study,\nwe propose a trainable agent framework called Agent-UniRAG for unified\nretrieval-augmented LLM systems, which enhances the effectiveness and\ninterpretability of RAG systems. The main idea is to design an LLM agent\nframework to solve RAG tasks step-by-step based on the complexity of the\ninputs, simultaneously including single-hop and multi-hop queries in an\nend-to-end manner. Furthermore, we introduce SynAgent-RAG, a synthetic dataset\nto enable the proposed agent framework for small open-source LLMs (e.g.,\nLlama-3-8B). The results show comparable performances with closed-source and\nlarger open-source LLMs across various RAG benchmarks. Our source code and\ndataset are publicly available for further exploitation."}
{"id": "2505.22572", "pdf": "https://arxiv.org/pdf/2505.22572", "abs": "https://arxiv.org/abs/2505.22572", "authors": ["Waldemar Chang", "Alhassan Yasin"], "title": "Fusion Steering: Prompt-Specific Activation Control", "categories": ["cs.CL", "cs.AI"], "comment": "14 pages, 4 figures, 2 tables", "summary": "We present Fusion Steering, an activation steering methodology that improves\nfactual accuracy in large language models (LLMs) for question-answering (QA)\ntasks. This approach introduces flexible steering configurations, including\nfull-layer steering and segmented steering. Unlike traditional methods\nconstrained to single-layer or fixed-layer operations, Fusion Steering employs\ndynamic injection of prompt-specific activation deltas across all transformer\nlayers. These activation deltas are derived from reference completions that\ncombine the ground-truth answer with a model-generated explanation to\nfacilitate semantically enriched, example-specific steering. The injection\nweights are optimized per prompt using Optuna, targeting a joint objective that\nbalances token overlap (factual alignment) and perplexity (fluency proxy).\nEvaluation employs a composite score integrating token overlap and LLM-graded\nquality, encompassing factual accuracy, coherence, and relevance. Empirical\nresults on 260 SimpleQA prompts (selected from 500 where the baseline failed)\nshowcase the efficacy of segmented steering. Using Gemma-2-2B-IT with 8-bit\nquantization, segmented steering achieves an accuracy of 25.4% (outputs scoring\n$\\geq 0.6$), outperforming the baseline at 3.5% and full-layer steering at\n16.2%. Under the stricter SimpleQA rubric, segmented steering boosts fully\ncorrect responses from 0.0% to 13.1%. These findings highlight the strengths of\nsegmented, dynamic intervention strategies and the promise of per-prompt,\nfull-network activation control. Fusion Steering is also amenable to sparse\nrepresentations, such as Neuronpedia or sparse crosscoders, suggesting a\npromising direction for interpretable and scalable activation-level control in\nLLMs."}
{"id": "2505.22582", "pdf": "https://arxiv.org/pdf/2505.22582", "abs": "https://arxiv.org/abs/2505.22582", "authors": ["Xue Zhang", "Yunlong Liang", "Fandong Meng", "Songming Zhang", "Yufeng Chen", "Jinan Xu", "Jie Zhou"], "title": "Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts", "categories": ["cs.CL"], "comment": "ACL 2025 (Main), 16 pages, 5 figures, 11 tables", "summary": "Continually expanding new languages for existing large language models (LLMs)\nis a promising yet challenging approach to building powerful multilingual LLMs.\nThe biggest challenge is to make the model continuously learn new languages\nwhile preserving the proficient ability of old languages. To achieve this,\nrecent work utilizes the Mixture-of-Experts (MoE) architecture to expand new\nlanguages by adding new experts and avoid catastrophic forgetting of old\nlanguages by routing corresponding tokens to the original model backbone (old\nexperts). Although intuitive, this kind of method is parameter-costly when\nexpanding new languages and still inevitably impacts the performance of old\nlanguages. To address these limitations, we analyze the language\ncharacteristics of different layers in LLMs and propose a layer-wise expert\nallocation algorithm (LayerMoE) to determine the appropriate number of new\nexperts for each layer. Specifically, we find different layers in LLMs exhibit\ndifferent representation similarities between languages and then utilize the\nsimilarity as the indicator to allocate experts for each layer, i.e., the\nhigher similarity, the fewer experts. Additionally, to further mitigate the\nforgetting of old languages, we add a classifier in front of the router network\non the layers with higher similarity to guide the routing of old language\ntokens. Experimental results show that our method outperforms the previous\nstate-of-the-art baseline with 60% fewer experts in the single-expansion\nsetting and with 33.3% fewer experts in the lifelong-expansion setting,\ndemonstrating the effectiveness of our method."}
{"id": "2505.22586", "pdf": "https://arxiv.org/pdf/2505.22586", "abs": "https://arxiv.org/abs/2505.22586", "authors": ["Yoav Gur-Arieh", "Clara Suslik", "Yihuai Hong", "Fazl Barez", "Mor Geva"], "title": "Precise In-Parameter Concept Erasure in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) often acquire knowledge during pretraining that\nis undesirable in downstream deployments, e.g., sensitive information or\ncopyrighted content. Existing approaches for removing such knowledge rely on\nfine-tuning, training low-rank adapters or fact-level editing, but these are\neither too coarse, too shallow, or ineffective. In this work, we propose PISCES\n(Precise In-parameter Suppression for Concept EraSure), a novel framework for\nprecisely erasing entire concepts from model parameters by directly editing\ndirections that encode them in parameter space. PISCES uses a disentangler\nmodel to decompose MLP vectors into interpretable features, identifies those\nassociated with a target concept using automated interpretability techniques,\nand removes them from model parameters. Experiments on Gemma 2 and Llama 3.1\nover various concepts show that PISCES achieves modest gains in efficacy over\nleading erasure methods, reducing accuracy on the target concept to as low as\n7.7%, while dramatically improving erasure specificity (by up to 31%) and\nrobustness (by up to 38%). Overall, these results demonstrate that\nfeature-based in-parameter editing enables a more precise and reliable approach\nfor removing conceptual knowledge in language models."}
{"id": "2505.22591", "pdf": "https://arxiv.org/pdf/2505.22591", "abs": "https://arxiv.org/abs/2505.22591", "authors": ["Erxin Yu", "Jing Li", "Ming Liao", "Qi Zhu", "Boyang Xue", "Minghui Xu", "Baojun Wang", "Lanqing Hong", "Fei Mi", "Lifeng Shang"], "title": "Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "16 pages, 9 figures", "summary": "Although large language models demonstrate strong performance across various\ndomains, they still struggle with numerous bad cases in mathematical reasoning.\nPrevious approaches to learning from errors synthesize training data by solely\nextrapolating from isolated bad cases, thereby failing to generalize the\nextensive patterns inherent within these cases. This paper presents\nSelf-Error-Instruct (SEI), a framework that addresses these model weaknesses\nand synthesizes more generalized targeted training data. Specifically, we\nexplore a target model on two mathematical datasets, GSM8K and MATH, to\npinpoint bad cases. Then, we generate error keyphrases for these cases based on\nthe instructor model's (GPT-4o) analysis and identify error types by clustering\nthese keyphrases. Next, we sample a few bad cases during each generation for\neach identified error type and input them into the instructor model, which\nsynthesizes additional training data using a self-instruct approach. This new\ndata is refined through a one-shot learning process to ensure that only the\nmost effective examples are kept. Finally, we use these curated data to\nfine-tune the target model, iteratively repeating the process to enhance\nperformance. We apply our framework to various models and observe improvements\nin their reasoning abilities across both in-domain and out-of-domain\nmathematics datasets. These results demonstrate the effectiveness of self-error\ninstruction in improving LLMs' mathematical reasoning through error\ngeneralization."}
{"id": "2505.22618", "pdf": "https://arxiv.org/pdf/2505.22618", "abs": "https://arxiv.org/abs/2505.22618", "authors": ["Chengyue Wu", "Hao Zhang", "Shuchen Xue", "Zhijian Liu", "Shizhe Diao", "Ligeng Zhu", "Ping Luo", "Song Han", "Enze Xie"], "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV Cache and Parallel Decoding", "categories": ["cs.CL"], "comment": null, "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to \\textbf{27.6$\\times$\nthroughput} improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs."}
{"id": "2505.22627", "pdf": "https://arxiv.org/pdf/2505.22627", "abs": "https://arxiv.org/abs/2505.22627", "authors": ["Yijun Shen", "Delong Chen", "Fan Liu", "Xingyu Wang", "Chuanyi Zhang", "Liang Yao", "Yuhui Zheng"], "title": "Chain-of-Talkers (CoTalk): Fast Human Annotation of Dense Image Captions", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "While densely annotated image captions significantly facilitate the learning\nof robust vision-language alignment, methodologies for systematically\noptimizing human annotation efforts remain underexplored. We introduce\nChain-of-Talkers (CoTalk), an AI-in-the-loop methodology designed to maximize\nthe number of annotated samples and improve their comprehensiveness under fixed\nbudget constraints (e.g., total human annotation time). The framework is built\nupon two key insights. First, sequential annotation reduces redundant workload\ncompared to conventional parallel annotation, as subsequent annotators only\nneed to annotate the ``residual'' -- the missing visual information that\nprevious annotations have not covered. Second, humans process textual input\nfaster by reading while outputting annotations with much higher throughput via\ntalking; thus a multimodal interface enables optimized efficiency. We evaluate\nour framework from two aspects: intrinsic evaluations that assess the\ncomprehensiveness of semantic units, obtained by parsing detailed captions into\nobject-attribute trees and analyzing their effective connections; extrinsic\nevaluation measures the practical usage of the annotated captions in\nfacilitating vision-language alignment. Experiments with eight participants\nshow our Chain-of-Talkers (CoTalk) improves annotation speed (0.42 vs. 0.30\nunits/sec) and retrieval performance (41.13\\% vs. 40.52\\%) over the parallel\nmethod."}
{"id": "2505.22630", "pdf": "https://arxiv.org/pdf/2505.22630", "abs": "https://arxiv.org/abs/2505.22630", "authors": ["Ziling Cheng", "Meng Cao", "Marc-Antoine Rondeau", "Jackie Chi Kit Cheung"], "title": "Stochastic Chameleons: Irrelevant Context Hallucinations Reveal Class-Based (Mis)Generalization in LLMs", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "The widespread success of large language models (LLMs) on NLP benchmarks has\nbeen accompanied by concerns that LLMs function primarily as stochastic parrots\nthat reproduce texts similar to what they saw during pre-training, often\nerroneously. But what is the nature of their errors, and do these errors\nexhibit any regularities? In this work, we examine irrelevant context\nhallucinations, in which models integrate misleading contextual cues into their\npredictions. Through behavioral analysis, we show that these errors result from\na structured yet flawed mechanism that we term class-based (mis)generalization,\nin which models combine abstract class cues with features extracted from the\nquery or context to derive answers. Furthermore, mechanistic interpretability\nexperiments on Llama-3, Mistral, and Pythia across 39 factual recall relation\ntypes reveal that this behavior is reflected in the model's internal\ncomputations: (i) abstract class representations are constructed in lower\nlayers before being refined into specific answers in higher layers, (ii)\nfeature selection is governed by two competing circuits -- one prioritizing\ndirect query-based reasoning, the other incorporating contextual cues -- whose\nrelative influences determine the final output. Our findings provide a more\nnuanced perspective on the stochastic parrot argument: through form-based\ntraining, LLMs can exhibit generalization leveraging abstractions, albeit in\nunreliable ways based on contextual cues -- what we term stochastic chameleons."}
{"id": "2505.22633", "pdf": "https://arxiv.org/pdf/2505.22633", "abs": "https://arxiv.org/abs/2505.22633", "authors": ["Yida Xue", "Zhen Bi", "Jinnan Yang", "Jungang Lou", "Huajun Chen", "Ningyu Zhang"], "title": "Spatial Knowledge Graph-Guided Multimodal Synthesis", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.LG", "cs.MM"], "comment": "Ongoing work", "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced their capabilities; however, their spatial perception\nabilities remain a notable limitation. To address this challenge, multimodal\ndata synthesis offers a promising solution. Yet, ensuring that synthesized data\nadhere to spatial common sense is a non-trivial task. In this work, we\nintroduce SKG2Data, a novel multimodal synthesis approach guided by spatial\nknowledge graphs, grounded in the concept of knowledge-to-data generation.\nSKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate\nhuman-like perception of spatial directions and distances, which is\nsubsequently utilized to guide multimodal data synthesis. Extensive experiments\ndemonstrate that data synthesized from diverse types of spatial knowledge,\nincluding direction and distance, not only enhance the spatial perception and\nreasoning abilities of MLLMs but also exhibit strong generalization\ncapabilities. We hope that the idea of knowledge-based data synthesis can\nadvance the development of spatial intelligence."}
{"id": "2505.22635", "pdf": "https://arxiv.org/pdf/2505.22635", "abs": "https://arxiv.org/abs/2505.22635", "authors": ["Fangcong Yin", "Zeyu Leo Liu", "Liu Leqi", "Xi Ye", "Greg Durrett"], "title": "Learning Composable Chains-of-Thought", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "A common approach for teaching large language models (LLMs) to reason is to\ntrain on chain-of-thought (CoT) traces of in-distribution reasoning problems,\nbut such annotated data is costly to obtain for every problem of interest. We\nwant reasoning models to generalize beyond their training distribution, and\nideally to generalize compositionally: combine atomic reasoning skills to solve\nharder, unseen reasoning tasks. We take a step towards compositional\ngeneralization of reasoning skills when addressing a target compositional task\nthat has no labeled CoT data. We find that simply training models on CoT data\nof atomic tasks leads to limited generalization, but minimally modifying CoT\nformats of constituent atomic tasks to be composable can lead to improvements.\nWe can train \"atomic CoT\" models on the atomic tasks with Composable CoT data\nand combine them with multitask learning or model merging for better zero-shot\nperformance on the target compositional task. Such a combined model can be\nfurther bootstrapped on a small amount of compositional data using rejection\nsampling fine-tuning (RFT). Results on string operations and natural language\nskill compositions show that training LLMs on Composable CoT outperforms\nmultitask learning and continued fine-tuning baselines within a given training\ndata budget."}
{"id": "2505.22645", "pdf": "https://arxiv.org/pdf/2505.22645", "abs": "https://arxiv.org/abs/2505.22645", "authors": ["Hanjia Lyu", "Jiebo Luo", "Jian Kang", "Allison Koenecke"], "title": "Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese", "categories": ["cs.CL", "cs.CY"], "comment": "To appear in the 2025 ACM Conference on Fairness, Accountability, and\n  Transparency (FAccT '25)", "summary": "While the capabilities of Large Language Models (LLMs) have been studied in\nboth Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit\ndifferential performance when prompted in these two variants of written\nChinese. This understanding is critical, as disparities in the quality of LLM\nresponses can perpetuate representational harms by ignoring the different\ncultural contexts underlying Simplified versus Traditional Chinese, and can\nexacerbate downstream harms in LLM-facilitated decision-making in domains such\nas education or hiring. To investigate potential LLM performance disparities,\nwe design two benchmark tasks that reflect real-world scenarios: regional term\nchoice (prompting the LLM to name a described item which is referred to\ndifferently in Mainland China and Taiwan), and regional name choice (prompting\nthe LLM to choose who to hire from a list of names in both Simplified and\nTraditional Chinese). For both tasks, we audit the performance of 11 leading\ncommercial LLM services and open-sourced models -- spanning those primarily\ntrained on English, Simplified Chinese, or Traditional Chinese. Our analyses\nindicate that biases in LLM responses are dependent on both the task and\nprompting language: while most LLMs disproportionately favored Simplified\nChinese responses in the regional term choice task, they surprisingly favored\nTraditional Chinese names in the regional name choice task. We find that these\ndisparities may arise from differences in training data representation, written\ncharacter preferences, and tokenization of Simplified and Traditional Chinese.\nThese findings highlight the need for further analysis of LLM biases; as such,\nwe provide an open-sourced benchmark dataset to foster reproducible evaluations\nof future LLM behavior across Chinese language variants\n(https://github.com/brucelyu17/SC-TC-Bench)."}
{"id": "2505.22648", "pdf": "https://arxiv.org/pdf/2505.22648", "abs": "https://arxiv.org/abs/2505.22648", "authors": ["Jialong Wu", "Baixuan Li", "Runnan Fang", "Wenbiao Yin", "Liwen Zhang", "Zhengwei Tao", "Dingchu Zhang", "Zekun Xi", "Yong Jiang", "Pengjun Xie", "Fei Huang", "Jingren Zhou"], "title": "WebDancer: Towards Autonomous Information Seeking Agency", "categories": ["cs.CL"], "comment": null, "summary": "Addressing intricate real-world problems necessitates in-depth information\nseeking and multi-step reasoning. Recent progress in agentic systems,\nexemplified by Deep Research, underscores the potential for autonomous\nmulti-step research. In this work, we present a cohesive paradigm for building\nend-to-end agentic information seeking agents from a data-centric and\ntraining-stage perspective. Our approach consists of four key stages: (1)\nbrowsing data construction, (2) trajectories sampling, (3) supervised\nfine-tuning for effective cold start, and (4) reinforcement learning for\nenhanced generalisation. We instantiate this framework in a web agent based on\nthe ReAct, WebDancer. Empirical evaluations on the challenging information\nseeking benchmarks, GAIA and WebWalkerQA, demonstrate the strong performance of\nWebDancer, achieving considerable results and highlighting the efficacy of our\ntraining paradigm. Further analysis of agent training provides valuable\ninsights and actionable, systematic pathways for developing more capable\nagentic models. The codes and demo will be released in\nhttps://github.com/Alibaba-NLP/WebAgent."}
{"id": "2505.22653", "pdf": "https://arxiv.org/pdf/2505.22653", "abs": "https://arxiv.org/abs/2505.22653", "authors": ["Ang Lv", "Ruobing Xie", "Xingwu Sun", "Zhanhui Kang", "Rui Yan"], "title": "The Climb Carves Wisdom Deeper Than the Summit: On the Noisy Rewards in Learning to Reason", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Recent studies on post-training large language models (LLMs) for reasoning\nthrough reinforcement learning (RL) typically focus on tasks that can be\naccurately verified and rewarded, such as solving math problems. In contrast,\nour research investigates the impact of reward noise, a more practical\nconsideration for real-world scenarios involving the post-training of LLMs\nusing reward models. We found that LLMs demonstrate strong robustness to\nsubstantial reward noise. For example, manually flipping 40% of the reward\nfunction's outputs in math tasks still allows a Qwen-2.5-7B model to achieve\nrapid convergence, improving its performance on math tasks from 5% to 72%,\ncompared to the 75% accuracy achieved by a model trained with noiseless\nrewards. Surprisingly, by only rewarding the appearance of key reasoning\nphrases (namely reasoning pattern reward, RPR), such as ``first, I need\nto''-without verifying the correctness of answers, the model achieved peak\ndownstream performance (over 70% accuracy for Qwen-2.5-7B) comparable to models\ntrained with strict correctness verification and accurate rewards. Recognizing\nthe importance of the reasoning process over the final results, we combined RPR\nwith noisy reward models. RPR helped calibrate the noisy reward models,\nmitigating potential false negatives and enhancing the LLM's performance on\nopen-ended tasks. These findings suggest the importance of improving models'\nfoundational abilities during the pre-training phase while providing insights\nfor advancing post-training techniques. Our code and scripts are available at\nhttps://github.com/trestad/Noisy-Rewards-in-Learning-to-Reason."}
{"id": "2505.22661", "pdf": "https://arxiv.org/pdf/2505.22661", "abs": "https://arxiv.org/abs/2505.22661", "authors": ["Qingchen Yu", "Zifan Zheng", "Ding Chen", "Simin Niu", "Bo Tang", "Feiyu Xiong", "Zhiyu Li"], "title": "GuessArena: Guess Who I Am? A Self-Adaptive Framework for Evaluating LLMs in Domain-Specific Knowledge and Reasoning", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "The evaluation of large language models (LLMs) has traditionally relied on\nstatic benchmarks, a paradigm that poses two major limitations: (1) predefined\ntest sets lack adaptability to diverse application domains, and (2)\nstandardized evaluation protocols often fail to capture fine-grained\nassessments of domain-specific knowledge and contextual reasoning abilities. To\novercome these challenges, we propose GuessArena, an adaptive evaluation\nframework grounded in adversarial game-based interactions. Inspired by the\ninteractive structure of the Guess Who I Am? game, our framework seamlessly\nintegrates dynamic domain knowledge modeling with progressive reasoning\nassessment to improve evaluation fidelity. Empirical studies across five\nvertical domains-finance, healthcare, manufacturing, information technology,\nand education-demonstrate that GuessArena effectively distinguishes LLMs in\nterms of domain knowledge coverage and reasoning chain completeness. Compared\nto conventional benchmarks, our method provides substantial advantages in\ninterpretability, scalability, and scenario adaptability."}
{"id": "2505.22662", "pdf": "https://arxiv.org/pdf/2505.22662", "abs": "https://arxiv.org/abs/2505.22662", "authors": ["Feng Luo", "Yu-Neng Chuang", "Guanchu Wang", "Hoang Anh Duy Le", "Shaochen Zhong", "Hongyi Liu", "Jiayi Yuan", "Yang Sui", "Vladimir Braverman", "Vipin Chaudhary", "Xia Hu"], "title": "AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The reasoning-capable large language models (LLMs) demonstrate strong\nperformance on complex reasoning tasks but often suffer from overthinking,\ngenerating unnecessarily long chain-of-thought (CoT) reasoning paths for easy\nreasoning questions, thereby increasing inference cost and latency. Recent\napproaches attempt to address this challenge by manually deciding when to apply\nlong or short reasoning. However, they lack the flexibility to adapt CoT length\ndynamically based on question complexity. In this paper, we propose Auto\nLong-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that\nenables LLMs to dynamically compress their generated reasoning path based on\nthe complexity of the reasoning question. AutoL2S enables a learned paradigm,\nin which LLMs themselves can decide when longer reasoning is necessary and when\nshorter reasoning suffices, by training on data annotated with our proposed\nmethod, which includes both long and short CoT paths and a special <EASY>\ntoken. We then use <EASY> token to indicate when the model can skip generating\nlengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs'\nability to generate shorter CoT reasoning paths with improved quality after\ntraining. Extensive evaluation results show that AutoL2S reduces the length of\nreasoning generation by up to 57% without compromising performance,\ndemonstrating the effectiveness of AutoL2S for scalable and efficient LLM\nreasoning."}
{"id": "2505.20162", "pdf": "https://arxiv.org/pdf/2505.20162", "abs": "https://arxiv.org/abs/2505.20162", "authors": ["Alexander Panfilov", "Paul Kassianik", "Maksym Andriushchenko", "Jonas Geiping"], "title": "Capability-Based Scaling Laws for LLM Red-Teaming", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "As large language models grow in capability and agency, identifying\nvulnerabilities through red-teaming becomes vital for safe deployment. However,\ntraditional prompt-engineering approaches may prove ineffective once\nred-teaming turns into a weak-to-strong problem, where target models surpass\nred-teamers in capabilities. To study this shift, we frame red-teaming through\nthe lens of the capability gap between attacker and target. We evaluate more\nthan 500 attacker-target pairs using LLM-based jailbreak attacks that mimic\nhuman red-teamers across diverse families, sizes, and capability levels. Three\nstrong trends emerge: (i) more capable models are better attackers, (ii) attack\nsuccess drops sharply once the target's capability exceeds the attacker's, and\n(iii) attack success rates correlate with high performance on social science\nsplits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking\nscaling law that predicts attack success for a fixed target based on\nattacker-target capability gap. These findings suggest that fixed-capability\nattackers (e.g., humans) may become ineffective against future models,\nincreasingly capable open-source models amplify risks for existing systems, and\nmodel providers must accurately measure and control models' persuasive and\nmanipulative abilities to limit their effectiveness as attackers."}
{"id": "2505.21510", "pdf": "https://arxiv.org/pdf/2505.21510", "abs": "https://arxiv.org/abs/2505.21510", "authors": ["Chundra Cathcart"], "title": "Complexity counts: global and local perspectives on Indo-Aryan numeral systems", "categories": ["physics.soc-ph", "cs.CL"], "comment": null, "summary": "The numeral systems of Indo-Aryan languages such as Hindi, Gujarati, and\nBengali are highly unusual in that unlike most numeral systems (e.g., those of\nEnglish, Chinese, etc.), forms referring to 1--99 are highly non-transparent\nand are cannot be constructed using straightforward rules. As an example,\nHindi/Urdu *iky\\=anve* `91' is not decomposable into the composite elements\n*ek* `one' and *nave* `ninety' in the way that its English counterpart is. This\npaper situates Indo-Aryan languages within the typology of cross-linguistic\nnumeral systems, and explores the linguistic and non-linguistic factors that\nmay be responsible for the persistence of complex systems in these languages.\nUsing cross-linguistic data from multiple databases, we develop and employ a\nnumber of cross-linguistically applicable metrics to quantifies the complexity\nof languages' numeral systems, and demonstrate that Indo-Aryan languages have\ndecisively more complex numeral systems than the world's languages as a whole,\nthough individual Indo-Aryan languages differ from each other in terms of the\ncomplexity of the patterns they display. We investigate the factors (e.g.,\nreligion, geographic isolation, etc.) that underlie complexity in numeral\nsystems, with a focus on South Asia, in an attempt to develop an account of why\ncomplex numeral systems developed and persisted in certain Indo-Aryan languages\nbut not elsewhere. Finally, we demonstrate that Indo-Aryan numeral systems\nadhere to certain general pressures toward efficient communication found\ncross-linguistically, despite their high complexity. We call for this somewhat\noverlooked dimension of complexity to be taken seriously when discussing\ngeneral variation in cross-linguistic numeral systems."}
{"id": "2505.21527", "pdf": "https://arxiv.org/pdf/2505.21527", "abs": "https://arxiv.org/abs/2505.21527", "authors": ["Jianheng Zhuo", "Yifan Yang", "Yiwen Shao", "Yong Xu", "Dong Yu", "Kai Yu", "Xie Chen"], "title": "VietASR: Achieving Industry-level Vietnamese ASR with 50-hour labeled data and Large-Scale Speech Pretraining", "categories": ["eess.AS", "cs.AI", "cs.CL", "cs.SD"], "comment": null, "summary": "Automatic speech recognition (ASR) has made remarkable progress but heavily\nrelies on large-scale labeled data, which is scarce for low-resource languages\nlike Vietnamese. While existing systems such as Whisper, USM, and MMS achieve\npromising performance, their efficacy remains inadequate in terms of training\ncosts, latency, and accessibility. To address these issues, we propose VietASR,\na novel ASR training pipeline that leverages vast amounts of unlabeled data and\na small set of labeled data. Through multi-iteration ASR-biased self-supervised\nlearning on a large-scale unlabeled dataset, VietASR offers a cost-effective\nand practical solution for enhancing ASR performance. Experiments demonstrate\nthat pre-training on 70,000-hour unlabeled data and fine-tuning on merely\n50-hour labeled data yield a lightweight but powerful ASR model. It outperforms\nWhisper Large-v3 and commercial ASR systems on real-world data. Our code and\nmodels will be open-sourced to facilitate research in low-resource ASR."}
{"id": "2505.21531", "pdf": "https://arxiv.org/pdf/2505.21531", "abs": "https://arxiv.org/abs/2505.21531", "authors": ["Kunhang Li", "Jason Naradowsky", "Yansong Feng", "Yusuke Miyao"], "title": "How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "comment": null, "summary": "We explore Large Language Models (LLMs)' human motion knowledge through 3D\navatar control. Given a motion instruction, we prompt LLMs to first generate a\nhigh-level movement plan with consecutive steps (High-level Planning), then\nspecify body part positions in each step (Low-level Planning), which we\nlinearly interpolate into avatar animations as a clear verification lens for\nhuman evaluators. Through carefully designed 20 representative motion\ninstructions with full coverage of basic movement primitives and balanced body\npart usage, we conduct comprehensive evaluations including human assessment of\nboth generated animations and high-level movement plans, as well as automatic\ncomparison with oracle positions in low-level planning. We find that LLMs are\nstrong at interpreting the high-level body movements but struggle with precise\nbody part positioning. While breaking down motion queries into atomic\ncomponents improves planning performance, LLMs have difficulty with multi-step\nmovements involving high-degree-of-freedom body parts. Furthermore, LLMs\nprovide reasonable approximation for general spatial descriptions, but fail to\nhandle precise spatial specifications in text, and the precise spatial-temporal\nparameters needed for avatar control. Notably, LLMs show promise in\nconceptualizing creative motions and distinguishing culturally-specific motion\npatterns."}
{"id": "2505.21544", "pdf": "https://arxiv.org/pdf/2505.21544", "abs": "https://arxiv.org/abs/2505.21544", "authors": ["Semanto Mondal"], "title": "Vision Meets Language: A RAG-Augmented YOLOv8 Framework for Coffee Disease Diagnosis and Farmer Assistance", "categories": ["cs.CV", "cs.CL"], "comment": "There are 14 pages, 8 figures", "summary": "As a social being, we have an intimate bond with the environment. A plethora\nof things in human life, such as lifestyle, health, and food are dependent on\nthe environment and agriculture. It comes under our responsibility to support\nthe environment as well as agriculture. However, traditional farming practices\noften result in inefficient resource use and environmental challenges. To\naddress these issues, precision agriculture has emerged as a promising approach\nthat leverages advanced technologies to optimise agricultural processes. In\nthis work, a hybrid approach is proposed that combines the three different\npotential fields of model AI: object detection, large language model (LLM), and\nRetrieval-Augmented Generation (RAG). In this novel framework, we have tried to\ncombine the vision and language models to work together to identify potential\ndiseases in the tree leaf. This study introduces a novel AI-based precision\nagriculture system that uses Retrieval Augmented Generation (RAG) to provide\ncontext-aware diagnoses and natural language processing (NLP) and YOLOv8 for\ncrop disease detection. The system aims to tackle major issues with large\nlanguage models (LLMs), especially hallucinations and allows for adaptive\ntreatment plans and real-time disease detection. The system provides an\neasy-to-use interface to the farmers, which they can use to detect the\ndifferent diseases related to coffee leaves by just submitting the image of the\naffected leaf the model will detect the diseases as well as suggest potential\nremediation methodologies which aim to lower the use of pesticides, preserving\nlivelihoods, and encouraging environmentally friendly methods. With an emphasis\non scalability, dependability, and user-friendliness, the project intends to\nimprove RAG-integrated object detection systems for wider agricultural\napplications in the future."}
{"id": "2505.21548", "pdf": "https://arxiv.org/pdf/2505.21548", "abs": "https://arxiv.org/abs/2505.21548", "authors": ["Dhruv Agarwal", "Anya Shukla", "Sunayana Sitaram", "Aditya Vashistha"], "title": "Fluent but Culturally Distant: Can Regional Training Teach Cultural Understanding?", "categories": ["physics.soc-ph", "cs.AI", "cs.CL", "cs.CY"], "comment": "Under review", "summary": "Large language models (LLMs) are used around the world but exhibit Western\ncultural tendencies. To address this cultural misalignment, many countries have\nbegun developing \"regional\" LLMs tailored to local communities. Yet it remains\nunclear whether these models merely speak the language of their users or also\nreflect their cultural values and practices. Using India as a case study, we\nevaluate five Indic and five global LLMs along two key dimensions: values (via\nthe Inglehart-Welzel map and GlobalOpinionQA) and practices (via CulturalBench\nand NormAd). Across all four tasks, we find that Indic models do not align more\nclosely with Indian cultural norms than global models. In fact, an average\nAmerican person is a better proxy for Indian cultural values than any Indic\nmodel. Even prompting strategies fail to meaningfully improve alignment.\nAblations show that regional fine-tuning does not enhance cultural competence\nand may in fact hurt it by impeding recall of existing knowledge. We trace this\nfailure to the scarcity of high-quality, untranslated, and culturally grounded\npretraining and fine-tuning data. Our study positions cultural evaluation as a\nfirst-class requirement alongside multilingual benchmarks and offers a reusable\nmethodology for developers. We call for deeper investments in culturally\nrepresentative data to build and evaluate truly sovereign LLMs."}
{"id": "2505.21549", "pdf": "https://arxiv.org/pdf/2505.21549", "abs": "https://arxiv.org/abs/2505.21549", "authors": ["Daniel Csizmadia", "Andrei Codreanu", "Victor Sim", "Vighnesh Prabeau", "Michael Lu", "Kevin Zhu", "Sean O'Brien", "Vasu Sharma"], "title": "Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We present Distill CLIP (DCLIP), a fine-tuned variant of the CLIP model that\nenhances multimodal image-text retrieval while preserving the original model's\nstrong zero-shot classification capabilities. CLIP models are typically\nconstrained by fixed image resolutions and limited context, which can hinder\ntheir effectiveness in retrieval tasks that require fine-grained cross-modal\nunderstanding. DCLIP addresses these challenges through a meta teacher-student\ndistillation framework, where a cross-modal transformer teacher is fine-tuned\nto produce enriched embeddings via bidirectional cross-attention between\nYOLO-extracted image regions and corresponding textual spans. These\nsemantically and spatially aligned global representations guide the training of\na lightweight student model using a hybrid loss that combines contrastive\nlearning and cosine similarity objectives. Despite being trained on only\n~67,500 samples curated from MSCOCO, Flickr30k, and Conceptual Captions-just a\nfraction of CLIP's original dataset-DCLIP significantly improves image-text\nretrieval metrics (Recall@K, MAP), while retaining approximately 94% of CLIP's\nzero-shot classification performance. These results demonstrate that DCLIP\neffectively mitigates the trade-off between task specialization and\ngeneralization, offering a resource-efficient, domain-adaptive, and\ndetail-sensitive solution for advanced vision-language tasks. Code available at\nhttps://anonymous.4open.science/r/DCLIP-B772/README.md."}
{"id": "2505.21569", "pdf": "https://arxiv.org/pdf/2505.21569", "abs": "https://arxiv.org/abs/2505.21569", "authors": ["Zhucong Li", "Bowei Zhang", "Jin Xiao", "Zhijian Zhou", "Fenglei Cao", "Jiaqing Liang", "Yuan Qi"], "title": "ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "9 pages", "summary": "Large Language Model (LLM)-based agents have demonstrated the ability to\nimprove performance in chemistry-related tasks by selecting appropriate tools.\nHowever, their effectiveness remains limited by the inherent prediction errors\nof chemistry tools. In this paper, we take a step further by exploring how\nLLMbased agents can, in turn, be leveraged to reduce prediction errors of the\ntools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking),\na simple yet effective method that enhances chemistry tools through optimizing\nagent-stacking structures from limited data. ChemHAS achieves state-of-the-art\nperformance across four fundamental chemistry tasks, demonstrating that our\nmethod can effectively compensate for prediction errors of the tools.\nFurthermore, we identify and characterize four distinct agent-stacking\nbehaviors, potentially improving interpretability and revealing new\npossibilities for AI agent applications in scientific research. Our code and\ndataset are publicly available at https:\n//anonymous.4open.science/r/ChemHAS-01E4/README.md."}
{"id": "2505.21668", "pdf": "https://arxiv.org/pdf/2505.21668", "abs": "https://arxiv.org/abs/2505.21668", "authors": ["Yongchao Chen", "Yueying Liu", "Junwei Zhou", "Yilun Hao", "Jingquan Wang", "Yang Zhang", "Chuchu Fan"], "title": "R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning", "categories": ["cs.AI", "cs.CL", "cs.SC"], "comment": "33 pages, 8 figures", "summary": "Despite advances in reasoning and planning of R1-like models, Large Language\nModels (LLMs) still struggle with tasks requiring precise computation, symbolic\nmanipulation, optimization, and algorithmic reasoning, in which textual\nreasoning lacks the rigor of code execution. A key challenge is enabling LLMs\nto decide when to use textual reasoning versus code generation. While OpenAI\ntrains models to invoke a Code Interpreter as needed, public research lacks\nguidance on aligning pre-trained LLMs to effectively leverage code and\ngeneralize across diverse tasks. We present R1-Code-Interpreter, an extension\nof a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and\nreinforcement learning (RL) to autonomously generate multiple code queries\nduring step-by-step reasoning. We curate 144 reasoning and planning tasks (107\nfor training, 37 for testing), each with over 200 diverse questions. We\nfine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies,\ninvestigating different answer formats, reasoning vs. non-reasoning models,\ncold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs.\nUnlike prior RL work on narrow domains, we find that Code Interpreter training\nis significantly harder due to high task diversity and expensive code\nexecution, highlighting the critical role of the SFT stage. Our final model,\nR1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\\% to\n64.1\\%, outperforming GPT-4o (text-only: 58.6\\%) and approaching GPT-4o with\nCode Interpreter (70.9\\%), with the emergent self-checking behavior via code\ngeneration. Datasets, Codes, and Models are available at\nhttps://github.com/yongchao98/R1-Code-Interpreter and\nhttps://huggingface.co/yongchao98."}
{"id": "2505.21749", "pdf": "https://arxiv.org/pdf/2505.21749", "abs": "https://arxiv.org/abs/2505.21749", "authors": ["M. Reza Ebrahimi", "Roland Memisevic"], "title": "Revisiting Bi-Linear State Transitions in Recurrent Neural Networks", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "The role of hidden units in recurrent neural networks is typically seen as\nmodeling memory, with research focusing on enhancing information retention\nthrough gating mechanisms. A less explored perspective views hidden units as\nactive participants in the computation performed by the network, rather than\npassive memory stores. In this work, we revisit bi-linear operations, which\ninvolve multiplicative interactions between hidden units and input embeddings.\nWe demonstrate theoretically and empirically that they constitute a natural\ninductive bias for representing the evolution of hidden states in state\ntracking tasks. These are the simplest type of task that require hidden units\nto actively contribute to the behavior of the network. We also show that\nbi-linear state updates form a natural hierarchy corresponding to state\ntracking tasks of increasing complexity, with popular linear recurrent networks\nsuch as Mamba residing at the lowest-complexity center of that hierarchy."}
{"id": "2505.21753", "pdf": "https://arxiv.org/pdf/2505.21753", "abs": "https://arxiv.org/abs/2505.21753", "authors": ["Roberto Ulloa", "Eve M. Zucker", "Daniel Bultmann", "David J. Simon", "Mykola Makhortykh"], "title": "From prosthetic memory to prosthetic denial: Auditing whether large language models are prone to mass atrocity denialism", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "The proliferation of large language models (LLMs) can influence how\nhistorical narratives are disseminated and perceived. This study explores the\nimplications of LLMs' responses on the representation of mass atrocity memory,\nexamining whether generative AI systems contribute to prosthetic memory, i.e.,\nmediated experiences of historical events, or to what we term \"prosthetic\ndenial,\" the AI-mediated erasure or distortion of atrocity memories. We argue\nthat LLMs function as interfaces that can elicit prosthetic memories and,\ntherefore, act as experiential sites for memory transmission, but also\nintroduce risks of denialism, particularly when their outputs align with\ncontested or revisionist narratives. To empirically assess these risks, we\nconducted a comparative audit of five LLMs (Claude, GPT, Llama, Mixtral, and\nGemini) across four historical case studies: the Holodomor, the Holocaust, the\nCambodian Genocide, and the genocide against the Tutsis in Rwanda. Each model\nwas prompted with questions addressing common denialist claims in English and\nan alternative language relevant to each case (Ukrainian, German, Khmer, and\nFrench). Our findings reveal that while LLMs generally produce accurate\nresponses for widely documented events like the Holocaust, significant\ninconsistencies and susceptibility to denialist framings are observed for more\nunderrepresented cases like the Cambodian Genocide. The disparities highlight\nthe influence of training data availability and the probabilistic nature of LLM\nresponses on memory integrity. We conclude that while LLMs extend the concept\nof prosthetic memory, their unmoderated use risks reinforcing historical\ndenialism, raising ethical concerns for (digital) memory preservation, and\npotentially challenging the advantageous role of technology associated with the\noriginal values of prosthetic memory."}
{"id": "2505.21755", "pdf": "https://arxiv.org/pdf/2505.21755", "abs": "https://arxiv.org/abs/2505.21755", "authors": ["Chengyue Huang", "Brisa Maneechotesuwan", "Shivang Chopra", "Zsolt Kira"], "title": "FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted to CVPR 2025", "summary": "Visual question answering (VQA) systems face significant challenges when\nadapting to real-world data shifts, especially in multi-modal contexts. While\nrobust fine-tuning strategies are essential for maintaining performance across\nin-distribution (ID) and out-of-distribution (OOD) scenarios, current\nevaluation settings are primarily unimodal or particular to some types of OOD,\noffering limited insight into the complexities of multi-modal contexts. In this\nwork, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across\nMulti-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We\nutilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA\nand others, and categorize them into ID, near and far OOD datasets covering\nuni-modal, multi-modal and adversarial distribution shifts. We first conduct a\ncomprehensive comparison of existing robust fine-tuning methods. We then\nquantify the distribution shifts by calculating the Mahalanobis distance using\nuni-modal and multi-modal embeddings extracted from various models. Further, we\nperform an extensive analysis to explore the interactions between uni- and\nmulti-modal shifts as well as modality importance for ID and OOD samples. These\nanalyses offer valuable guidance on developing more robust fine-tuning methods\nto handle multi-modal distribution shifts. The code is available at\nhttps://github.com/chengyuehuang511/FRAMES-VQA ."}
{"id": "2505.21784", "pdf": "https://arxiv.org/pdf/2505.21784", "abs": "https://arxiv.org/abs/2505.21784", "authors": ["Tharindu Kumarage", "Ninareh Mehrabi", "Anil Ramakrishna", "Xinyan Zhao", "Richard Zemel", "Kai-Wei Chang", "Aram Galstyan", "Rahul Gupta", "Charith Peris"], "title": "Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation", "categories": ["cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025 (Findings)", "summary": "Safety reasoning is a recent paradigm where LLMs reason over safety policies\nbefore generating responses, thereby mitigating limitations in existing safety\nmeasures such as over-refusal and jailbreak vulnerabilities. However,\nimplementing this paradigm is challenging due to the resource-intensive process\nof creating high-quality policy-embedded chain-of-thought (CoT) datasets while\nensuring reasoning remains accurate and free from hallucinations or policy\nconflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation\nfor Safety Reasoning, a novel data generation recipe that leverages multi-agent\ndeliberation to iteratively expand reasoning on safety policies. A data refiner\nstage in AIDSAFE ensures high-quality outputs by eliminating repetitive,\nredundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong\nfoundation for supervised fine-tuning (SFT)-based safety training.\nAdditionally, to address the need of preference data in alignment stages, such\nas DPO training, we introduce a supplemental recipe that uses belief\naugmentation to create distinct selected and rejected CoT samples. Our\nevaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy\nadherence and reasoning quality. Consequently, we show that fine-tuning\nopen-source LLMs on these CoTs can significantly improve safety generalization\nand jailbreak robustness while maintaining acceptable utility and over-refusal\naccuracy. AIDSAFE-generated CoT datasets can be found here:\nhttps://huggingface.co/datasets/AmazonScience/AIDSAFE"}
{"id": "2505.21785", "pdf": "https://arxiv.org/pdf/2505.21785", "abs": "https://arxiv.org/abs/2505.21785", "authors": ["Yana Veitsman", "Mayank Jobanputra", "Yash Sarrof", "Aleksandra Bakalova", "Vera Demberg", "Ellie Pavlick", "Michael Hahn"], "title": "Born a Transformer -- Always a Transformer?", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Transformers have theoretical limitations in modeling certain\nsequence-to-sequence tasks, yet it remains largely unclear if these limitations\nplay a role in large-scale pretrained LLMs, or whether LLMs might effectively\novercome these constraints in practice due to the scale of both the models\nthemselves and their pretraining data. We explore how these architectural\nconstraints manifest after pretraining, by studying a family of\n$\\textit{retrieval}$ and $\\textit{copying}$ tasks inspired by Liu et al.\n[2024]. We use the recently proposed C-RASP framework for studying length\ngeneralization [Huang et al., 2025b] to provide guarantees for each of our\nsettings. Empirically, we observe an $\\textit{induction-versus-anti-induction}$\nasymmetry, where pretrained models are better at retrieving tokens to the right\n(induction) rather than the left (anti-induction) of a query token. This\nasymmetry disappears upon targeted fine-tuning if length-generalization is\nguaranteed by theory. Mechanistic analysis reveals that this asymmetry is\nconnected to the differences in the strength of induction versus anti-induction\ncircuits within pretrained Transformers. We validate our findings through\npractical experiments on real-world tasks demonstrating reliability risks. Our\nresults highlight that pretraining selectively enhances certain Transformer\ncapabilities, but does not overcome fundamental length-generalization limits."}
{"id": "2505.21800", "pdf": "https://arxiv.org/pdf/2505.21800", "abs": "https://arxiv.org/abs/2505.21800", "authors": ["Stanley Yu", "Vaidehi Bulusu", "Oscar Yasunaga", "Clayton Lau", "Cole Blondin", "Sean O'Brien", "Kevin Zhu", "Vasu Sharma"], "title": "From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit strong conversational abilities but\noften generate falsehoods. Prior work suggests that the truthfulness of simple\npropositions can be represented as a single linear direction in a model's\ninternal activations, but this may not fully capture its underlying geometry.\nIn this work, we extend the concept cone framework, recently introduced for\nmodeling refusal, to the domain of truth. We identify multi-dimensional cones\nthat causally mediate truth-related behavior across multiple LLM families. Our\nresults are supported by three lines of evidence: (i) causal interventions\nreliably flip model responses to factual statements, (ii) learned cones\ngeneralize across model architectures, and (iii) cone-based interventions\npreserve unrelated model behavior. These findings reveal the richer,\nmultidirectional structure governing simple true/false propositions in LLMs and\nhighlight concept cones as a promising tool for probing abstract behaviors."}
{"id": "2505.21815", "pdf": "https://arxiv.org/pdf/2505.21815", "abs": "https://arxiv.org/abs/2505.21815", "authors": ["Yunyi Zhang", "Ruozhen Yang", "Siqi Jiao", "SeongKu Kang", "Jiawei Han"], "title": "Scientific Paper Retrieval with LLM-Guided Semantic-Based Ranking", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Scientific paper retrieval is essential for supporting literature discovery\nand research. While dense retrieval methods demonstrate effectiveness in\ngeneral-purpose tasks, they often fail to capture fine-grained scientific\nconcepts that are essential for accurate understanding of scientific queries.\nRecent studies also use large language models (LLMs) for query understanding;\nhowever, these methods often lack grounding in corpus-specific knowledge and\nmay generate unreliable or unfaithful content. To overcome these limitations,\nwe propose SemRank, an effective and efficient paper retrieval framework that\ncombines LLM-guided query understanding with a concept-based semantic index.\nEach paper is indexed using multi-granular scientific concepts, including\ngeneral research topics and detailed key phrases. At query time, an LLM\nidentifies core concepts derived from the corpus to explicitly capture the\nquery's information need. These identified concepts enable precise semantic\nmatching, significantly enhancing retrieval accuracy. Experiments show that\nSemRank consistently improves the performance of various base retrievers,\nsurpasses strong existing LLM-based baselines, and remains highly efficient."}
{"id": "2505.21825", "pdf": "https://arxiv.org/pdf/2505.21825", "abs": "https://arxiv.org/abs/2505.21825", "authors": ["Parsa Mirtaheri", "Ezra Edelman", "Samy Jelassi", "Eran Malach", "Enric Boix-Adsera"], "title": "Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Inference-time computation has emerged as a promising scaling axis for\nimproving large language model reasoning. However, despite yielding impressive\nperformance, the optimal allocation of inference-time computation remains\npoorly understood. A central question is whether to prioritize sequential\nscaling (e.g., longer chains of thought) or parallel scaling (e.g., majority\nvoting across multiple short chains of thought). In this work, we seek to\nilluminate the landscape of test-time scaling by demonstrating the existence of\nreasoning settings where sequential scaling offers an exponential advantage\nover parallel scaling. These settings are based on graph connectivity problems\nin challenging distributions of graphs. We validate our theoretical findings\nwith comprehensive experiments across a range of language models, including\nmodels trained from scratch for graph connectivity with different chain of\nthought strategies as well as large reasoning models."}
{"id": "2505.21863", "pdf": "https://arxiv.org/pdf/2505.21863", "abs": "https://arxiv.org/abs/2505.21863", "authors": ["Shikhhar Siingh", "Abhinav Rawat", "Vivek Gupta", "Chitta Baral"], "title": "GETReason: Enhancing Image Context Extraction through Hierarchical Multi-Agent Reasoning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Publicly significant images from events hold valuable contextual information,\ncrucial for journalism and education. However, existing methods often struggle\nto extract this relevance accurately. To address this, we introduce GETReason\n(Geospatial Event Temporal Reasoning), a framework that moves beyond\nsurface-level image descriptions to infer deeper contextual meaning. We propose\nthat extracting global event, temporal, and geospatial information enhances\nunderstanding of an image's significance. Additionally, we introduce GREAT\n(Geospatial Reasoning and Event Accuracy with Temporal Alignment), a new metric\nfor evaluating reasoning-based image understanding. Our layered multi-agent\napproach, assessed using a reasoning-weighted metric, demonstrates that\nmeaningful insights can be inferred, effectively linking images to their\nbroader event context."}
{"id": "2505.21880", "pdf": "https://arxiv.org/pdf/2505.21880", "abs": "https://arxiv.org/abs/2505.21880", "authors": ["Yu-Lun Song", "Chung-En Tsern", "Che-Cheng Wu", "Yu-Ming Chang", "Syuan-Bo Huang", "Wei-Chu Chen", "Michael Chia-Liang Lin", "Yu-Ta Lin"], "title": "Incorporating LLMs for Large-Scale Urban Complex Mobility Simulation", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CY"], "comment": "8 pages, 8 figures. This paper is reviewed and accepted by the CUPUM\n  (Computational Urban Planning and Urban Management) Conference held by\n  University College London (UCL) in 2025", "summary": "This study presents an innovative approach to urban mobility simulation by\nintegrating a Large Language Model (LLM) with Agent-Based Modeling (ABM).\nUnlike traditional rule-based ABM, the proposed framework leverages LLM to\nenhance agent diversity and realism by generating synthetic population\nprofiles, allocating routine and occasional locations, and simulating\npersonalized routes. Using real-world data, the simulation models individual\nbehaviors and large-scale mobility patterns in Taipei City. Key insights, such\nas route heat maps and mode-specific indicators, provide urban planners with\nactionable information for policy-making. Future work focuses on establishing\nrobust validation frameworks to ensure accuracy and reliability in urban\nplanning applications."}
{"id": "2505.21907", "pdf": "https://arxiv.org/pdf/2505.21907", "abs": "https://arxiv.org/abs/2505.21907", "authors": ["Saleh Afzoon", "Zahra Jahanandish", "Phuong Thao Huynh", "Amin Beheshti", "Usman Naseem"], "title": "Modeling and Optimizing User Preferences in AI Copilots: A Comprehensive Survey and Taxonomy", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": null, "summary": "AI copilots, context-aware, AI-powered systems designed to assist users in\ntasks such as software development and content creation, are becoming integral\nto modern workflows. As these systems grow in capability and adoption,\npersonalization has emerged as a cornerstone for ensuring usability, trust, and\nproductivity. Central to this personalization is preference optimization: the\nability of AI copilots to detect, interpret, and align with individual user\npreferences. While personalization techniques are well-established in domains\nlike recommender systems and dialogue agents, their adaptation to interactive,\nreal-time systems like AI copilots remains fragmented and underexplored. This\nsurvey addresses this gap by synthesizing research on how user preferences are\ncaptured, modeled, and refined within the design of AI copilots. We introduce a\nunified definition of AI copilots and propose a phase-based taxonomy of\npreference optimization strategies, structured around pre-interaction,\nmid-interaction, and post-interaction stages. We analyze techniques for\nacquiring preference signals, modeling user intent, and integrating feedback\nloops, highlighting both established approaches and recent innovations. By\nbridging insights from AI personalization, human-AI collaboration, and large\nlanguage model adaptation, this survey provides a structured foundation for\ndesigning adaptive, preference-aware AI copilots. It offers a holistic view of\nthe available preference resources, how they can be leveraged, and which\ntechnical approaches are most suited to each stage of system design."}
{"id": "2505.21930", "pdf": "https://arxiv.org/pdf/2505.21930", "abs": "https://arxiv.org/abs/2505.21930", "authors": ["Dongyue Li", "Ziniu Zhang", "Lu Wang", "Hongyang R. Zhang"], "title": "Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets", "categories": ["cs.LG", "cs.CL"], "comment": "17 pages. To appear in ACL'25", "summary": "This paper develops an ensemble method for fine-tuning a language model to\nmultiple datasets. Existing methods, such as quantized LoRA (QLoRA), are\nefficient when adapting to a single dataset. When training on multiple datasets\nof different tasks, a common setup in practice, it remains unclear how to\ndesign an efficient adaptation for fine-tuning language models. We propose to\nuse an ensemble of multiple smaller adapters instead of a single adapter per\ntask. We design an efficient algorithm that partitions $n$ datasets into $m$\ngroups, where $m$ is typically much smaller than $n$ in practice, and train one\nadapter for each group before taking a weighted combination to form the\nensemble. The algorithm leverages a first-order approximation property of\nlow-rank adaptation to quickly obtain the fine-tuning performances of dataset\ncombinations since methods like LoRA stay close to the base model. Hence, we\nuse the gradients of the base model to estimate its behavior during\nfine-tuning. Empirically, this approximation holds with less than $1\\%$ error\non models with up to $34$ billion parameters, leading to an estimation of true\nfine-tuning performances under $5\\%$ error while speeding up computation\ncompared to base fine-tuning by $105$ times. When applied to fine-tune Llama\nand GPT models on ten text classification tasks, our approach provides up to\n$10\\%$ higher average test accuracy over QLoRA, with only $9\\%$ more FLOPs. On\na Llama model with $34$ billion parameters, an ensemble of QLoRA increases test\naccuracy by $3\\%$ compared to QLoRA, with only $8\\%$ more FLOPs."}
{"id": "2505.21956", "pdf": "https://arxiv.org/pdf/2505.21956", "abs": "https://arxiv.org/abs/2505.21956", "authors": ["Mengdan Zhu", "Senhao Cheng", "Guangji Bai", "Yifei Zhang", "Liang Zhao"], "title": "Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Text-to-image generation increasingly demands access to domain-specific,\nfine-grained, and rapidly evolving knowledge that pretrained models cannot\nfully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to\naddress this by retrieving globally relevant images, but they fail when no\nsingle image contains all desired elements from a complex user query. We\npropose Cross-modal RAG, a novel framework that decomposes both queries and\nimages into sub-dimensional components, enabling subquery-aware retrieval and\ngeneration. Our method introduces a hybrid retrieval strategy - combining a\nsub-dimensional sparse retriever with a dense retriever - to identify a\nPareto-optimal set of images, each contributing complementary aspects of the\nquery. During generation, a multimodal large language model is guided to\nselectively condition on relevant visual features aligned to specific\nsubqueries, ensuring subquery-aware image synthesis. Extensive experiments on\nMS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal\nRAG significantly outperforms existing baselines in both retrieval and\ngeneration quality, while maintaining high efficiency."}
{"id": "2505.21959", "pdf": "https://arxiv.org/pdf/2505.21959", "abs": "https://arxiv.org/abs/2505.21959", "authors": ["Aakriti Agrawal", "Mucong Ding", "Zora Che", "Chenghao Deng", "Anirudh Satheesh", "Bang An", "Bayan Bruss", "John Langford", "Furong Huang"], "title": "EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles", "categories": ["cs.LG", "cs.CL"], "comment": "Superalignment. arXiv admin note: substantial text overlap with\n  arXiv:2410.04571", "summary": "With Large Language Models (LLMs) rapidly approaching and potentially\nsurpassing human-level performance, it has become imperative to develop\napproaches capable of effectively supervising and enhancing these powerful\nmodels using smaller, human-level models exposed to only human-level data. We\naddress this critical weak-to-strong (W2S) generalization challenge by\nproposing a novel method aimed at improving weak experts, by training on the\nsame limited human-level data, enabling them to generalize to complex,\nsuper-human-level tasks. Our approach, called \\textbf{EnsemW2S}, employs a\ntoken-level ensemble strategy that iteratively combines multiple weak experts,\nsystematically addressing the shortcomings identified in preceding iterations.\nBy continuously refining these weak models, we significantly enhance their\ncollective ability to supervise stronger student models. We extensively\nevaluate the generalization performance of both the ensemble of weak experts\nand the subsequent strong student model across in-distribution (ID) and\nout-of-distribution (OOD) datasets. For OOD, we specifically introduce question\ndifficulty as an additional dimension for defining distributional shifts. Our\nempirical results demonstrate notable improvements, achieving 4\\%, and 3.2\\%\nimprovements on ID datasets and, upto 6\\% and 2.28\\% on OOD datasets for\nexperts and student models respectively, underscoring the effectiveness of our\nproposed method in advancing W2S generalization."}
{"id": "2505.21964", "pdf": "https://arxiv.org/pdf/2505.21964", "abs": "https://arxiv.org/abs/2505.21964", "authors": ["Ziyun Zhang", "Xinyi Liu", "Xiaoyi Zhang", "Jun Wang", "Gang Chen", "Yan Lu"], "title": "UI-Evol: Automatic Knowledge Evolving for Computer Use Agents", "categories": ["cs.HC", "cs.CL"], "comment": null, "summary": "External knowledge has played a crucial role in the recent development of\ncomputer use agents. We identify a critical knowledge-execution gap: retrieved\nknowledge often fails to translate into effective real-world task execution.\nOur analysis shows even 90\\% correct knowledge yields only 41\\% execution\nsuccess rate. To bridge this gap, we propose UI-Evol, a plug-and-play module\nfor autonomous GUI knowledge evolution. UI-Evol consists of two stages: a\nRetrace Stage that extracts faithful objective action sequences from actual\nagent-environment interactions, and a Critique Stage that refines existing\nknowledge by comparing these sequences against external references. We conduct\ncomprehensive experiments on the OSWorld benchmark with the state-of-the-art\nAgent S2. Our results demonstrate that UI-Evol not only significantly boosts\ntask performance but also addresses a previously overlooked issue of high\nbehavioral standard deviation in computer use agents, leading to superior\nperformance on computer use tasks and substantially improved agent reliability."}
{"id": "2505.21966", "pdf": "https://arxiv.org/pdf/2505.21966", "abs": "https://arxiv.org/abs/2505.21966", "authors": ["Aditya Gunturu", "Ben Pearman", "Keiichi Ihara", "Morteza Faraji", "Bryan Wang", "Rubaiat Habib Kazi", "Ryo Suzuki"], "title": "MapStory: LLM-Powered Text-Driven Map Animation Prototyping with Human-in-the-Loop Editing", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.MM", "H.5.2, H.5.1"], "comment": "16 pages and 15 figures", "summary": "We introduce MapStory, an LLM-powered animation authoring tool that generates\neditable map animation sequences directly from natural language text. Given a\nuser-written script, MapStory leverages an agentic architecture to\nautomatically produce a scene breakdown, which decomposes the script into key\nanimation building blocks such as camera movements, visual highlights, and\nanimated elements. Our system includes a researcher component that accurately\nqueries geospatial information by leveraging an LLM with web search, enabling\nthe automatic extraction of relevant regions, paths, and coordinates while\nallowing users to edit and query for changes or additional information to\nrefine the results. Additionally, users can fine-tune parameters of these\nblocks through an interactive timeline editor. We detail the system's design\nand architecture, informed by formative interviews with professional animators\nand an analysis of 200 existing map animation videos. Our evaluation, which\nincludes expert interviews (N=5) and a usability study (N=12), demonstrates\nthat MapStory enables users to create map animations with ease, facilitates\nfaster iteration, encourages creative exploration, and lowers barriers to\ncreating map-centric stories."}
{"id": "2505.21981", "pdf": "https://arxiv.org/pdf/2505.21981", "abs": "https://arxiv.org/abs/2505.21981", "authors": ["Weiyu Liu", "Neil Nie", "Ruohan Zhang", "Jiayuan Mao", "Jiajun Wu"], "title": "Learning Compositional Behaviors from Demonstration and Language", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "comment": "Presented at CoRL 2024 and as an Oral Presentation at the 2024 CoRL\n  LEAP Workshop. The first two authors contributed equally. The last two\n  authors jointly advised the project. For videos and additional results,\n  visit: https://blade-bot.github.io/", "summary": "We introduce Behavior from Language and Demonstration (BLADE), a framework\nfor long-horizon robotic manipulation by integrating imitation learning and\nmodel-based planning. BLADE leverages language-annotated demonstrations,\nextracts abstract action knowledge from large language models (LLMs), and\nconstructs a library of structured, high-level action representations. These\nrepresentations include preconditions and effects grounded in visual perception\nfor each high-level action, along with corresponding controllers implemented as\nneural network-based policies. BLADE can recover such structured\nrepresentations automatically, without manually labeled states or symbolic\ndefinitions. BLADE shows significant capabilities in generalizing to novel\nsituations, including novel initial states, external state perturbations, and\nnovel goals. We validate the effectiveness of our approach both in simulation\nand on real robots with a diverse set of objects with articulated parts,\npartial observability, and geometric constraints."}
{"id": "2505.22088", "pdf": "https://arxiv.org/pdf/2505.22088", "abs": "https://arxiv.org/abs/2505.22088", "authors": ["Sam O'Connor Russell", "Naomi Harte"], "title": "Visual Cues Support Robust Turn-taking Prediction in Noise", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "5 pages", "summary": "Accurate predictive turn-taking models (PTTMs) are essential for naturalistic\nhuman-robot interaction. However, little is known about their performance in\nnoise. This study therefore explores PTTM performance in types of noise likely\nto be encountered once deployed. Our analyses reveal PTTMs are highly sensitive\nto noise. Hold/shift accuracy drops from 84% in clean speech to just 52% in 10\ndB music noise. Training with noisy data enables a multimodal PTTM, which\nincludes visual features to better exploit visual cues, with 72% accuracy in 10\ndB music noise. The multimodal PTTM outperforms the audio-only PTTM across all\nnoise types and SNRs, highlighting its ability to exploit visual cues; however,\nthis does not always generalise to new types of noise. Analysis also reveals\nthat successful training relies on accurate transcription, limiting the use of\nASR-derived transcriptions to clean conditions. We make code publicly available\nfor future research."}
{"id": "2505.22146", "pdf": "https://arxiv.org/pdf/2505.22146", "abs": "https://arxiv.org/abs/2505.22146", "authors": ["Guangfu Hao", "Haojie Wen", "Liangxuna Guo", "Yang Chen", "Yanchao Bi", "Shan Yu"], "title": "Flexible Tool Selection through Low-dimensional Attribute Alignment of Vision and Language", "categories": ["cs.CV", "cs.AI", "cs.CL", "q-bio.NC"], "comment": null, "summary": "Flexible tool selection reflects a complex cognitive ability that\ndistinguishes humans from other species, yet computational models that capture\nthis ability remain underdeveloped. We developed a framework using\nlow-dimensional attribute representations to bridge visual tool perception and\nlinguistic task understanding. We constructed a comprehensive dataset (ToolNet)\ncontaining 115 common tools labeled with 13 carefully designed attributes\nspanning physical, functional, and psychological properties, paired with\nnatural language scenarios describing tool usage. Visual encoders (ResNet or\nViT) extract attributes from tool images while fine-tuned language models\n(GPT-2, LLaMA, DeepSeek) derive required attributes from task descriptions. Our\napproach achieves 74% accuracy in tool selection tasks-significantly\noutperforming direct tool matching (20%) and smaller multimodal models\n(21%-58%), while approaching performance of much larger models like GPT-4o\n(73%) with substantially fewer parameters. Ablation studies revealed that\nmanipulation-related attributes (graspability, hand-relatedness, elongation)\nconsistently prove most critical across modalities. This work provides a\nparameter-efficient, interpretable solution that mimics human-like tool\ncognition, advancing both cognitive science understanding and practical\napplications in tool selection tasks."}
{"id": "2505.22150", "pdf": "https://arxiv.org/pdf/2505.22150", "abs": "https://arxiv.org/abs/2505.22150", "authors": ["Runze Xia", "Shuo Feng", "Renzhi Wang", "Congchi Yin", "Xuyun Wen", "Piji Li"], "title": "Improving Brain-to-Image Reconstruction via Fine-Grained Text Bridging", "categories": ["cs.CV", "cs.CL"], "comment": "CogSci2025", "summary": "Brain-to-Image reconstruction aims to recover visual stimuli perceived by\nhumans from brain activity. However, the reconstructed visual stimuli often\nmissing details and semantic inconsistencies, which may be attributed to\ninsufficient semantic information. To address this issue, we propose an\napproach named Fine-grained Brain-to-Image reconstruction (FgB2I), which\nemploys fine-grained text as bridge to improve image reconstruction. FgB2I\ncomprises three key stages: detail enhancement, decoding fine-grained text\ndescriptions, and text-bridged brain-to-image reconstruction. In the\ndetail-enhancement stage, we leverage large vision-language models to generate\nfine-grained captions for visual stimuli and experimentally validate its\nimportance. We propose three reward metrics (object accuracy, text-image\nsemantic similarity, and image-image semantic similarity) to guide the language\nmodel in decoding fine-grained text descriptions from fMRI signals. The\nfine-grained text descriptions can be integrated into existing reconstruction\nmethods to achieve fine-grained Brain-to-Image reconstruction."}
{"id": "2505.22203", "pdf": "https://arxiv.org/pdf/2505.22203", "abs": "https://arxiv.org/abs/2505.22203", "authors": ["Yuzhen Huang", "Weihao Zeng", "Xingshan Zeng", "Qi Zhu", "Junxian He"], "title": "Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Trustworthy verifiers are essential for the success of reinforcement learning\nwith verifiable reward (RLVR), which is the core methodology behind various\nlarge reasoning models such as DeepSeek-R1. In complex domains like\nmathematical reasoning, rule-based verifiers have been widely adopted in\nprevious works to train strong reasoning models. However, the reliability of\nthese verifiers and their impact on the RL training process remain poorly\nunderstood. In this work, we take mathematical reasoning as a case study and\nconduct a comprehensive analysis of various verifiers in both static evaluation\nand RL training scenarios. First, we find that current open-source rule-based\nverifiers often fail to recognize equivalent answers presented in different\nformats across multiple commonly used mathematical datasets, resulting in\nnon-negligible false negative rates. This limitation adversely affects RL\ntraining performance and becomes more pronounced as the policy model gets\nstronger. Subsequently, we investigate model-based verifiers as a potential\nsolution to address these limitations. While the static evaluation shows that\nmodel-based verifiers achieve significantly higher verification accuracy,\nfurther analysis and RL training results imply that they are highly susceptible\nto hacking, where they misclassify certain patterns in responses as correct\n(i.e., false positives). This vulnerability is exploited during policy model\noptimization, leading to artificially inflated rewards. Our findings underscore\nthe unique risks inherent to both rule-based and model-based verifiers, aiming\nto offer valuable insights to develop more robust reward systems in\nreinforcement learning."}
{"id": "2505.22222", "pdf": "https://arxiv.org/pdf/2505.22222", "abs": "https://arxiv.org/abs/2505.22222", "authors": ["Yunsoo Kim", "Jinge Wu", "Su-Hwan Kim", "Pardeep Vasudev", "Jiashu Shen", "Honghan Wu"], "title": "Look & Mark: Leveraging Radiologist Eye Fixations and Bounding boxes in Multimodal Large Language Models for Chest X-ray Report Generation", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advancements in multimodal Large Language Models (LLMs) have\nsignificantly enhanced the automation of medical image analysis, particularly\nin generating radiology reports from chest X-rays (CXR). However, these models\nstill suffer from hallucinations and clinically significant errors, limiting\ntheir reliability in real-world applications. In this study, we propose Look &\nMark (L&M), a novel grounding fixation strategy that integrates radiologist eye\nfixations (Look) and bounding box annotations (Mark) into the LLM prompting\nframework. Unlike conventional fine-tuning, L&M leverages in-context learning\nto achieve substantial performance gains without retraining. When evaluated\nacross multiple domain-specific and general-purpose models, L&M demonstrates\nsignificant gains, including a 1.2% improvement in overall metrics (A.AVG) for\nCXR-LLaVA compared to baseline prompting and a remarkable 9.2% boost for\nLLaVA-Med. General-purpose models also benefit from L&M combined with\nin-context learning, with LLaVA-OV achieving an 87.3% clinical average\nperformance (C.AVG)-the highest among all models, even surpassing those\nexplicitly trained for CXR report generation. Expert evaluations further\nconfirm that L&M reduces clinically significant errors (by 0.43 average errors\nper report), such as false predictions and omissions, enhancing both accuracy\nand reliability. These findings highlight L&M's potential as a scalable and\nefficient solution for AI-assisted radiology, paving the way for improved\ndiagnostic workflows in low-resource clinical settings."}
{"id": "2505.22231", "pdf": "https://arxiv.org/pdf/2505.22231", "abs": "https://arxiv.org/abs/2505.22231", "authors": ["Stefan Bleeck"], "title": "Advancing Hearing Assessment: An ASR-Based Frequency-Specific Speech Test for Diagnosing Presbycusis", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Traditional audiometry often fails to fully characterize the functional\nimpact of hearing loss on speech understanding, particularly supra-threshold\ndeficits and frequency-specific perception challenges in conditions like\npresbycusis. This paper presents the development and simulated evaluation of a\nnovel Automatic Speech Recognition (ASR)-based frequency-specific speech test\ndesigned to provide granular diagnostic insights. Our approach leverages ASR to\nsimulate the perceptual effects of moderate sloping hearing loss by processing\nspeech stimuli under controlled acoustic degradation and subsequently analyzing\nphoneme-level confusion patterns. Key findings indicate that simulated hearing\nloss introduces specific phoneme confusions, predominantly affecting\nhigh-frequency consonants (e.g., alveolar/palatal to labiodental substitutions)\nand leading to significant phoneme deletions, consistent with the acoustic cues\ndegraded in presbycusis. A test battery curated from these ASR-derived\nconfusions demonstrated diagnostic value, effectively differentiating between\nsimulated normal-hearing and hearing-impaired listeners in a comprehensive\nsimulation. This ASR-driven methodology offers a promising avenue for\ndeveloping objective, granular, and frequency-specific hearing assessment tools\nthat complement traditional audiometry. Future work will focus on validating\nthese findings with human participants and exploring the integration of\nadvanced AI models for enhanced diagnostic precision."}
{"id": "2505.22251", "pdf": "https://arxiv.org/pdf/2505.22251", "abs": "https://arxiv.org/abs/2505.22251", "authors": ["Yuan Tseng", "Titouan Parcollet", "Rogier van Dalen", "Shucong Zhang", "Sourav Bhattacharya"], "title": "Evaluation of LLMs in Speech is Often Flawed: Test Set Contamination in Large Language Models for Speech Recognition", "categories": ["eess.AS", "cs.CL"], "comment": null, "summary": "Recent work suggests that large language models (LLMs) can improve\nperformance of speech tasks compared to existing systems. To support their\nclaims, results on LibriSpeech and Common Voice are often quoted. However, this\nwork finds that a substantial amount of the LibriSpeech and Common Voice\nevaluation sets appear in public LLM pretraining corpora. This calls into\nquestion the reliability of findings drawn from these two datasets. To measure\nthe impact of contamination, LLMs trained with or without contamination are\ncompared, showing that a contaminated LLM is more likely to generate test\nsentences it has seen during training. Speech recognisers using contaminated\nLLMs shows only subtle differences in error rates, but assigns significantly\nhigher probabilities to transcriptions seen during training. Results show that\nLLM outputs can be biased by tiny amounts of data contamination, highlighting\nthe importance of evaluating LLM-based speech systems with held-out data."}
{"id": "2505.22255", "pdf": "https://arxiv.org/pdf/2505.22255", "abs": "https://arxiv.org/abs/2505.22255", "authors": ["Vadim Kurochkin", "Yaroslav Aksenov", "Daniil Laptev", "Daniil Gavrilov", "Nikita Balagansky"], "title": "Train Sparse Autoencoders Efficiently by Utilizing Features Correlation", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Sparse Autoencoders (SAEs) have demonstrated significant promise in\ninterpreting the hidden states of language models by decomposing them into\ninterpretable latent directions. However, training SAEs at scale remains\nchallenging, especially when large dictionary sizes are used. While decoders\ncan leverage sparse-aware kernels for efficiency, encoders still require\ncomputationally intensive linear operations with large output dimensions. To\naddress this, we propose KronSAE, a novel architecture that factorizes the\nlatent representation via Kronecker product decomposition, drastically reducing\nmemory and computational overhead. Furthermore, we introduce mAND, a\ndifferentiable activation function approximating the binary AND operation,\nwhich improves interpretability and performance in our factorized framework."}
{"id": "2505.22271", "pdf": "https://arxiv.org/pdf/2505.22271", "abs": "https://arxiv.org/abs/2505.22271", "authors": ["Yongcan Yu", "Yanbo Wang", "Ran He", "Jian Liang"], "title": "Test-Time Immunization: A Universal Defense Framework Against Jailbreaks for (Multimodal) Large Language Models", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": "Under Review", "summary": "While (multimodal) large language models (LLMs) have attracted widespread\nattention due to their exceptional capabilities, they remain vulnerable to\njailbreak attacks. Various defense methods are proposed to defend against\njailbreak attacks, however, they are often tailored to specific types of\njailbreak attacks, limiting their effectiveness against diverse adversarial\nstrategies. For instance, rephrasing-based defenses are effective against text\nadversarial jailbreaks but fail to counteract image-based attacks. To overcome\nthese limitations, we propose a universal defense framework, termed Test-time\nIMmunization (TIM), which can adaptively defend against various jailbreak\nattacks in a self-evolving way. Specifically, TIM initially trains a gist token\nfor efficient detection, which it subsequently applies to detect jailbreak\nactivities during inference. When jailbreak attempts are identified, TIM\nimplements safety fine-tuning using the detected jailbreak instructions paired\nwith refusal answers. Furthermore, to mitigate potential performance\ndegradation in the detector caused by parameter updates during safety\nfine-tuning, we decouple the fine-tuning process from the detection module.\nExtensive experiments on both LLMs and multimodal LLMs demonstrate the efficacy\nof TIM."}
{"id": "2505.22290", "pdf": "https://arxiv.org/pdf/2505.22290", "abs": "https://arxiv.org/abs/2505.22290", "authors": ["Fanzeng Xia", "Yidong Luo", "Tinko Sebastian Bartels", "Yaqi Xu", "Tongxin Li"], "title": "Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Recent research has highlighted that Large Language Models (LLMs), even when\ntrained to generate extended long reasoning steps, still face significant\nchallenges on hard reasoning problems. However, much of the existing literature\nrelies on direct prompting with simple in-context learning examples for\nevaluation, which largely overlooks advanced techniques to elicit LLMs'\ndeliberate reasoning before drawing conclusions that LLMs hit a performance\nceiling. In this paper, we systematically explore the combined potential of\nin-context search and test-time scaling on super hard reasoning tasks. We find\nthat by employing advanced in-context search prompting to LLMs augmented with\ninternal scaling, one can achieve transformative performance breakthroughs on\ntasks previously deemed \"unsolvable\" (e.g., reported success rates below 5%).\nWe provide both empirical results and theoretical analysis of how this\ncombination can unleash LLM reasoning capabilities: i) Empirically, on\ncontrolled NP-hard tasks and complex real-world planning benchmarks, our\napproach achieves up to a 30x improvement in success rates compared to\npreviously reported results without any external mechanisms; ii) Theoretically,\nwe show that in-context search prompting, when combined with internal scaling,\nsignificantly extends the complexity class of solvable reasoning problems.\nThese findings challenge prevailing assumptions about the limitations of LLMs\non complex tasks, indicating that current evaluation paradigms systematically\nunderestimate their true potential. Our work calls for a critical reassessment\nof how LLM reasoning is benchmarked and a more robust evaluation strategy that\nfully captures the true capabilities of contemporary LLMs, which can lead to a\nbetter understanding of their operational reasoning boundaries in real-world\ndeployments."}
{"id": "2505.22312", "pdf": "https://arxiv.org/pdf/2505.22312", "abs": "https://arxiv.org/abs/2505.22312", "authors": ["Jujie He", "Jiacai Liu", "Chris Yuhao Liu", "Rui Yan", "Chaojie Wang", "Peng Cheng", "Xiaoyu Zhang", "Fuxiang Zhang", "Jiacheng Xu", "Wei Shen", "Siyuan Li", "Liang Zeng", "Tianwen Wei", "Cheng Cheng", "Bo An", "Yang Liu", "Yahui Zhou"], "title": "Skywork Open Reasoner 1 Technical Report", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The success of DeepSeek-R1 underscores the significant role of reinforcement\nlearning (RL) in enhancing the reasoning capabilities of large language models\n(LLMs). In this work, we present Skywork-OR1, an effective and scalable RL\nimplementation for long Chain-of-Thought (CoT) models. Building on the\nDeepSeek-R1-Distill model series, our RL approach achieves notable performance\ngains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench\nfrom 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%)\nfor the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and\nQwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable\nresults on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models\ndemonstrate competitive reasoning capabilities among models of similar size. We\nperform comprehensive ablation studies on the core components of our training\npipeline to validate their effectiveness. Additionally, we thoroughly\ninvestigate the phenomenon of entropy collapse, identify key factors affecting\nentropy dynamics, and demonstrate that mitigating premature entropy collapse is\ncritical for improved test performance. To support community research, we fully\nopen-source our model weights, training code, and training datasets."}
{"id": "2505.22411", "pdf": "https://arxiv.org/pdf/2505.22411", "abs": "https://arxiv.org/abs/2505.22411", "authors": ["Yao Huang", "Huanran Chen", "Shouwei Ruan", "Yichi Zhang", "Xingxing Wei", "Yinpeng Dong"], "title": "Mitigating Overthinking in Large Reasoning Models via Manifold Steering", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "17 pages, 7 figures", "summary": "Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable\ncapabilities in solving complex tasks such as mathematics and coding. However,\nthese models frequently exhibit a phenomenon known as overthinking during\ninference, characterized by excessive validation loops and redundant\ndeliberation, leading to substantial computational overheads. In this paper, we\naim to mitigate overthinking by investigating the underlying mechanisms from\nthe perspective of mechanistic interpretability. We first showcase that the\ntendency of overthinking can be effectively captured by a single direction in\nthe model's activation space and the issue can be eased by intervening the\nactivations along this direction. However, this efficacy soon reaches a plateau\nand even deteriorates as the intervention strength increases. We therefore\nsystematically explore the activation space and find that the overthinking\nphenomenon is actually tied to a low-dimensional manifold, which indicates that\nthe limited effect stems from the noises introduced by the high-dimensional\nsteering direction. Based on this insight, we propose Manifold Steering, a\nnovel approach that elegantly projects the steering direction onto the\nlow-dimensional activation manifold given the theoretical approximation of the\ninterference noise. Extensive experiments on DeepSeek-R1 distilled models\nvalidate that our method reduces output tokens by up to 71% while maintaining\nand even improving the accuracy on several mathematical benchmarks. Our method\nalso exhibits robust cross-domain transferability, delivering consistent token\nreduction performance in code generation and knowledge-based QA tasks. Code is\navailable at: https://github.com/Aries-iai/Manifold_Steering."}
{"id": "2505.22425", "pdf": "https://arxiv.org/pdf/2505.22425", "abs": "https://arxiv.org/abs/2505.22425", "authors": ["Xueliang Zhao", "Wei Wu", "Lingpeng Kong"], "title": "Scaling Reasoning without Attention", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "preprint", "summary": "Large language models (LLMs) have made significant advances in complex\nreasoning tasks, yet they remain bottlenecked by two core challenges:\narchitectural inefficiency due to reliance on Transformers, and a lack of\nstructured fine-tuning for high-difficulty domains. We introduce \\ourmodel, an\nattention-free language model that addresses both issues through architectural\nand data-centric innovations. Built on the state space dual (SSD) layers of\nMamba-2, our model eliminates the need for self-attention and key-value\ncaching, enabling fixed-memory, constant-time inference. To train it for\ncomplex reasoning, we propose a two-phase curriculum fine-tuning strategy based\non the \\textsc{PromptCoT} synthesis paradigm, which generates pedagogically\nstructured problems via abstract concept selection and rationale-guided\ngeneration. On benchmark evaluations, \\ourmodel-7B outperforms strong\nTransformer and hybrid models of comparable scale, and even surpasses the much\nlarger Gemma3-27B by 2.6\\% on AIME 24, 0.6\\% on AIME 25, and 3.0\\% on\nLivecodebench. These results highlight the potential of state space models as\nefficient and scalable alternatives to attention-based architectures for\nhigh-capacity reasoning."}
{"id": "2505.22457", "pdf": "https://arxiv.org/pdf/2505.22457", "abs": "https://arxiv.org/abs/2505.22457", "authors": ["Haonan Wang", "Hongfu Liu", "Xiangyan Liu", "Chao Du", "Kenji Kawaguchi", "Ye Wang", "Tianyu Pang"], "title": "Fostering Video Reasoning via Next-Event Prediction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Next-token prediction serves as the foundational learning task enabling\nreasoning in LLMs. But what should the learning task be when aiming to equip\nMLLMs with temporal reasoning capabilities over video inputs? Existing tasks\nsuch as video question answering often rely on annotations from humans or much\nstronger MLLMs, while video captioning tends to entangle temporal reasoning\nwith spatial information. To address this gap, we propose next-event prediction\n(NEP), a learning task that harnesses future video segments as a rich,\nself-supervised signal to foster temporal reasoning. We segment each video into\npast and future frames: the MLLM takes the past frames as input and predicts a\nsummary of events derived from the future frames, thereby encouraging the model\nto reason temporally in order to complete the task. To support this task, we\ncurate V1-33K, a dataset comprising 33,000 automatically extracted video\nsegments spanning diverse real-world scenarios. We further explore a range of\nvideo instruction-tuning strategies to study their effects on temporal\nreasoning. To evaluate progress, we introduce FutureBench to assess coherence\nin predicting unseen future events. Experiments validate that NEP offers a\nscalable and effective training paradigm for fostering temporal reasoning in\nMLLMs."}
{"id": "2505.22487", "pdf": "https://arxiv.org/pdf/2505.22487", "abs": "https://arxiv.org/abs/2505.22487", "authors": ["Yen Meng", "Sharon Goldwater", "Hao Tang"], "title": "Effective Context in Neural Speech Models", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Modern neural speech models benefit from having longer context, and many\napproaches have been proposed to increase the maximum context a model can use.\nHowever, few have attempted to measure how much context these models actually\nuse, i.e., the effective context. Here, we propose two approaches to measuring\nthe effective context, and use them to analyze different speech Transformers.\nFor supervised models, we find that the effective context correlates well with\nthe nature of the task, with fundamental frequency tracking, phone\nclassification, and word classification requiring increasing amounts of\neffective context. For self-supervised models, we find that effective context\nincreases mainly in the early layers, and remains relatively short -- similar\nto the supervised phone model. Given that these models do not use a long\ncontext during prediction, we show that HuBERT can be run in streaming mode\nwithout modification to the architecture and without further fine-tuning."}
{"id": "2505.22525", "pdf": "https://arxiv.org/pdf/2505.22525", "abs": "https://arxiv.org/abs/2505.22525", "authors": ["Ethan Chern", "Zhulin Hu", "Steffi Chern", "Siqi Kou", "Jiadi Su", "Yan Ma", "Zhijie Deng", "Pengfei Liu"], "title": "Thinking with Generated Images", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present Thinking with Generated Images, a novel paradigm that\nfundamentally transforms how large multimodal models (LMMs) engage with visual\nreasoning by enabling them to natively think across text and vision modalities\nthrough spontaneous generation of intermediate visual thinking steps. Current\nvisual reasoning with LMMs is constrained to either processing fixed\nuser-provided images or reasoning solely through text-based chain-of-thought\n(CoT). Thinking with Generated Images unlocks a new dimension of cognitive\ncapability where models can actively construct intermediate visual thoughts,\ncritique their own visual hypotheses, and refine them as integral components of\ntheir reasoning process. We demonstrate the effectiveness of our approach\nthrough two complementary mechanisms: (1) vision generation with intermediate\nvisual subgoals, where models decompose complex visual tasks into manageable\ncomponents that are generated and integrated progressively, and (2) vision\ngeneration with self-critique, where models generate an initial visual\nhypothesis, analyze its shortcomings through textual reasoning, and produce\nrefined outputs based on their own critiques. Our experiments on vision\ngeneration benchmarks show substantial improvements over baseline approaches,\nwith our models achieving up to 50% (from 38% to 57%) relative improvement in\nhandling complex multi-object scenarios. From biochemists exploring novel\nprotein structures, and architects iterating on spatial designs, to forensic\nanalysts reconstructing crime scenes, and basketball players envisioning\nstrategic plays, our approach enables AI models to engage in the kind of visual\nimagination and iterative refinement that characterizes human creative,\nanalytical, and strategic thinking. We release our open-source suite at\nhttps://github.com/GAIR-NLP/thinking-with-generated-images."}
{"id": "2505.22613", "pdf": "https://arxiv.org/pdf/2505.22613", "abs": "https://arxiv.org/abs/2505.22613", "authors": ["Yuchi Wang", "Yishuo Cai", "Shuhuai Ren", "Sihan Yang", "Linli Yao", "Yuanxin Liu", "Yuanxing Zhang", "Pengfei Wan", "Xu Sun"], "title": "RICO: Improving Accuracy and Completeness in Image Recaptioning via Visual Reconstruction", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "code: https://github.com/wangyuchi369/RICO", "summary": "Image recaptioning is widely used to generate training datasets with enhanced\nquality for various multimodal tasks. Existing recaptioning methods typically\nrely on powerful multimodal large language models (MLLMs) to enhance textual\ndescriptions, but often suffer from inaccuracies due to hallucinations and\nincompleteness caused by missing fine-grained details. To address these\nlimitations, we propose RICO, a novel framework that refines captions through\nvisual reconstruction. Specifically, we leverage a text-to-image model to\nreconstruct a caption into a reference image, and prompt an MLLM to identify\ndiscrepancies between the original and reconstructed images to refine the\ncaption. This process is performed iteratively, further progressively promoting\nthe generation of more faithful and comprehensive descriptions. To mitigate the\nadditional computational cost induced by the iterative process, we introduce\nRICO-Flash, which learns to generate captions like RICO using DPO. Extensive\nexperiments demonstrate that our approach significantly improves caption\naccuracy and completeness, outperforms most baselines by approximately 10% on\nboth CapsBench and CompreCap. Code released at\nhttps://github.com/wangyuchi369/RICO."}
{"id": "2505.22617", "pdf": "https://arxiv.org/pdf/2505.22617", "abs": "https://arxiv.org/abs/2505.22617", "authors": ["Ganqu Cui", "Yuchen Zhang", "Jiacheng Chen", "Lifan Yuan", "Zhi Wang", "Yuxin Zuo", "Haozhan Li", "Yuchen Fan", "Huayu Chen", "Weize Chen", "Zhiyuan Liu", "Hao Peng", "Lei Bai", "Wanli Ouyang", "Yu Cheng", "Bowen Zhou", "Ning Ding"], "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance."}
{"id": "2505.22651", "pdf": "https://arxiv.org/pdf/2505.22651", "abs": "https://arxiv.org/abs/2505.22651", "authors": ["Yi Ding", "Ruqi Zhang"], "title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": "27 pages", "summary": "Reasoning Vision-Language Models (VLMs) have shown promising performance on\ncomplex multimodal tasks. However, they still face significant challenges: they\nare highly sensitive to reasoning errors, require large volumes of annotated\ndata or accurate verifiers, and struggle to generalize beyond specific domains.\nTo address these limitations, we explore self-correction as a strategy to\nenhance reasoning VLMs. We first conduct an in-depth analysis of reasoning\nVLMs' self-correction abilities and identify key gaps. Based on our findings,\nwe introduce Sherlock, a self-correction and self-improvement training\nframework. Sherlock introduces a trajectory-level self-correction objective, a\npreference data construction method based on visual perturbation, and a dynamic\n$\\beta$ for preference tuning. Once the model acquires self-correction\ncapabilities using only 20k randomly sampled annotated data, it continues to\nself-improve without external supervision. Built on the Llama3.2-Vision-11B\nmodel, Sherlock achieves remarkable results across eight benchmarks, reaching\nan average accuracy of 64.1 with direct generation and 65.4 after\nself-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and\nLlamaV-o1 (63.4) while using less than 20% of the annotated data."}
{"id": "2505.22655", "pdf": "https://arxiv.org/pdf/2505.22655", "abs": "https://arxiv.org/abs/2505.22655", "authors": ["Michael Kirchhof", "Gjergji Kasneci", "Enkelejda Kasneci"], "title": "Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted at ICML 2025", "summary": "Large-language models (LLMs) and chatbot agents are known to provide wrong\noutputs at times, and it was recently found that this can never be fully\nprevented. Hence, uncertainty quantification plays a crucial role, aiming to\nquantify the level of ambiguity in either one overall number or two numbers for\naleatoric and epistemic uncertainty. This position paper argues that this\ntraditional dichotomy of uncertainties is too limited for the open and\ninteractive setup that LLM agents operate in when communicating with a user,\nand that we need to research avenues that enrich uncertainties in this novel\nscenario. We review the literature and find that popular definitions of\naleatoric and epistemic uncertainties directly contradict each other and lose\ntheir meaning in interactive LLM agent settings. Hence, we propose three novel\nresearch directions that focus on uncertainties in such human-computer\ninteractions: Underspecification uncertainties, for when users do not provide\nall information or define the exact task at the first go, interactive learning,\nto ask follow-up questions and reduce the uncertainty about the current\ncontext, and output uncertainties, to utilize the rich language and speech\nspace to express uncertainties as more than mere numbers. We expect that these\nnew ways of dealing with and communicating uncertainties will lead to LLM agent\ninteractions that are more transparent, trustworthy, and intuitive."}
{"id": "2505.22657", "pdf": "https://arxiv.org/pdf/2505.22657", "abs": "https://arxiv.org/abs/2505.22657", "authors": ["Wenbo Hu", "Yining Hong", "Yanjun Wang", "Leison Gao", "Zibu Wei", "Xingcheng Yao", "Nanyun Peng", "Yonatan Bitton", "Idan Szpektor", "Kai-Wei Chang"], "title": "3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "demos at: https://3dllm-mem.github.io", "summary": "Humans excel at performing complex tasks by leveraging long-term memory\nacross temporal and spatial experiences. In contrast, current Large Language\nModels (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D\nenvironments. We posit that part of this limitation is due to the lack of\nproper 3D spatial-temporal memory modeling in LLMs. To address this, we first\nintroduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000\ntrajectories and 2,892 embodied tasks, question-answering and captioning,\ndesigned to evaluate an agent's ability to reason over long-term memory in 3D\nenvironments. Second, we propose 3DLLM-Mem, a novel dynamic memory management\nand fusion model for embodied spatial-temporal reasoning and actions in LLMs.\nOur model uses working memory tokens, which represents current observations, as\nqueries to selectively attend to and fuse the most useful spatial and temporal\nfeatures from episodic memory, which stores past observations and interactions.\nOur approach allows the agent to focus on task-relevant information while\nmaintaining memory efficiency in complex, long-horizon environments.\nExperimental results demonstrate that 3DLLM-Mem achieves state-of-the-art\nperformance across various tasks, outperforming the strongest baselines by\n16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied\ntasks."}
