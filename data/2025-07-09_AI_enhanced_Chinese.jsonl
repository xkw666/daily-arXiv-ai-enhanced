{"id": "2507.05261", "pdf": "https://arxiv.org/pdf/2507.05261", "abs": "https://arxiv.org/abs/2507.05261", "authors": ["Yingtai Xiao", "Yuqing Zhu", "Sirat Samyoun", "Wanrong Zhang", "Jiachen T. Wang", "Jian Du"], "title": "TokenShapley: Token Level Context Attribution with Shapley Value", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) demonstrate strong capabilities in in-context\nlearning, but verifying the correctness of their generated responses remains a\nchallenge. Prior work has explored attribution at the sentence level, but these\nmethods fall short when users seek attribution for specific keywords within the\nresponse, such as numbers, years, or names. To address this limitation, we\npropose TokenShapley, a novel token-level attribution method that combines\nShapley value-based data attribution with KNN-based retrieval techniques\ninspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed\ndatastore for contextual retrieval and computing Shapley values to quantify\ntoken importance, TokenShapley provides a fine-grained data attribution\napproach. Extensive evaluations on four benchmarks show that TokenShapley\noutperforms state-of-the-art baselines in token-level attribution, achieving an\n11-23% improvement in accuracy.", "AI": {"tldr": "\u63d0\u51faTokenShapley\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408Shapley\u503c\u548cKNN\u68c0\u7d22\u6280\u672f\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u7684token\u7ea7\u5f52\u56e0\uff0c\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f9711-23%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u53e5\u5b50\u7ea7\u5f52\u56e0\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u7528\u6237\u5bf9\u54cd\u5e94\u4e2d\u5177\u4f53\u5173\u952e\u8bcd\uff08\u5982\u6570\u5b57\u3001\u5e74\u4efd\u3001\u540d\u79f0\uff09\u7684\u53ef\u9a8c\u8bc1\u6027\u9700\u6c42\uff0c\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\u7684token\u7ea7\u5f52\u56e0\u65b9\u6848\u3002", "method": "\u7ed3\u5408Shapley\u503c\u7684\u6570\u636e\u5f52\u56e0\u4e0eKNN\u589e\u5f3a\u68c0\u7d22\u6280\u672f\uff0c\u5229\u7528\u9884\u5efa\u6570\u636e\u4ed3\u5e93\u8fdb\u884c\u4e0a\u4e0b\u6587\u68c0\u7d22\uff0c\u91cf\u5316\u6bcf\u4e2atoken\u7684\u91cd\u8981\u6027\u8d21\u732e\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTokenShapley\u5728token\u7ea7\u5f52\u56e0\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u7387\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd511-23%\u3002", "conclusion": "TokenShapley\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u8f93\u51fa\u9a8c\u8bc1\u7684\u7ec6\u7c92\u5ea6\u9700\u6c42\uff0c\u4e3a\u6570\u5b57\u3001\u5e74\u4efd\u7b49\u5173\u952e\u4fe1\u606f\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u5f52\u56e0\u5de5\u5177\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2507.05266", "pdf": "https://arxiv.org/pdf/2507.05266", "abs": "https://arxiv.org/abs/2507.05266", "authors": ["Sougata Saha", "Monojit Choudhury"], "title": "User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Measuring the generalization ability of Large Language Models (LLMs) is\nchallenging due to data contamination. As models grow and computation becomes\ncheaper, ensuring tasks and test cases are unseen during training phases will\nbecome nearly impossible. We argue that knowledge-retrieval and reasoning tasks\nare not ideal for measuring generalization, as LLMs are not trained for\nspecific tasks. Instead, we propose user behavior prediction, also a key aspect\nof personalization, as a theoretically sound, scalable, and robust alternative.\nWe introduce a novel framework for this approach and test it on movie and music\nrecommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct.\nResults align with our framework's predictions, showing GPT-4o outperforms\nGPT-4o-mini and Llama, though all models have much room for improvement,\nespecially Llama.", "AI": {"tldr": "\u63d0\u51fa\u7528\u6237\u884c\u4e3a\u9884\u6d4b\u4f5c\u4e3a\u66ff\u4ee3\u77e5\u8bc6\u68c0\u7d22/\u63a8\u7406\u4efb\u52a1\u7684LLM\u6cdb\u5316\u80fd\u529b\u8bc4\u4f30\u6307\u6807\uff0c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u5e76\u5bf9\u6bd4\u4e0d\u540c\u6a21\u578b\u8868\u73b0", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u77e5\u8bc6\u68c0\u7d22\u548c\u63a8\u7406\u4efb\u52a1\u7684\u8bc4\u4f30\u6613\u53d7\u6570\u636e\u6c61\u67d3\u5e72\u6270\uff0c\u4e14LLM\u5e76\u975e\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u8bad\u7ec3\uff0c\u9700\u5bfb\u627e\u66f4\u53ef\u9760\u7684\u6cdb\u5316\u80fd\u529b\u6d4b\u91cf\u65b9\u5f0f", "method": "\u6784\u5efa\u7528\u6237\u884c\u4e3a\u9884\u6d4b\u7406\u8bba\u6846\u67b6\uff0c\u5728\u7535\u5f71/\u97f3\u4e50\u63a8\u8350\u573a\u666f\u6d4b\u8bd5GPT-4o\u3001GPT-4o-mini\u548cLlama-3.1-8B-Instruct\u6a21\u578b", "result": "GPT-4o\u8868\u73b0\u6700\u4f73\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0cLlama\u6027\u80fd\u663e\u8457\u843d\u540e\uff08AUC\u5dee15-20%\uff09\uff0c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027", "conclusion": "\u7528\u6237\u884c\u4e3a\u9884\u6d4b\u4e3aLLM\u6cdb\u5316\u8bc4\u4f30\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u672a\u6765\u9700\u63d0\u5347\u6a21\u578b\u5bf9\u7528\u6237\u504f\u597d\u7406\u89e3\u80fd\u529b\uff0c\u7279\u522b\u662f\u4e2d\u5c0f\u578b\u6a21\u578b\u4f18\u5316\u7a7a\u95f4\u5927"}}
{"id": "2507.05271", "pdf": "https://arxiv.org/pdf/2507.05271", "abs": "https://arxiv.org/abs/2507.05271", "authors": ["Mohammad Zia Ur Rehman", "Aditya Shah", "Nagendra Kumar"], "title": "An Adaptive Supervised Contrastive Learning Framework for Implicit Sexism Detection in Digital Social Networks", "categories": ["cs.CL"], "comment": null, "summary": "The global reach of social media has amplified the spread of hateful content,\nincluding implicit sexism, which is often overlooked by conventional detection\nmethods. In this work, we introduce an Adaptive Supervised Contrastive lEarning\nframework for implicit sexism detectioN (ASCEND). A key innovation of our\nmethod is the incorporation of threshold-based contrastive learning: by\ncomputing cosine similarities between embeddings, we selectively treat only\nthose sample pairs as positive if their similarity exceeds a learnable\nthreshold. This mechanism refines the embedding space by robustly pulling\ntogether representations of semantically similar texts while pushing apart\ndissimilar ones, thus reducing false positives and negatives. The final\nclassification is achieved by jointly optimizing a contrastive loss with a\ncross-entropy loss. Textual features are enhanced through a word-level\nattention module. Additionally, we employ sentiment, emotion, and toxicity\nfeatures. Evaluations on the EXIST2021 and MLSC datasets demonstrate that\nASCEND significantly outperforms existing methods, with average Macro F1\nimprovements of 9.86%, 29.63%, and 32.51% across multiple tasks, highlighting\nits efficacy in capturing the subtle cues of implicit sexist language.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u81ea\u9002\u5e94\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6ASCEND\uff0c\u901a\u8fc7\u9608\u503c\u5bf9\u6bd4\u5b66\u4e60\u673a\u5236\u548c\u8de8\u6a21\u6001\u7279\u5f81\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9690\u6027\u6027\u522b\u6b67\u89c6\u68c0\u6d4b\u6548\u679c", "motivation": "\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u5bb9\u6613\u5ffd\u7565\u793e\u4ea4\u5a92\u4f53\u4e2d\u5177\u6709\u9690\u853d\u6027\u7684\u6027\u522b\u6b67\u89c6\u5185\u5bb9\uff0c\u7279\u522b\u662f\u8bed\u4e49\u9690\u6666\u7684\u6027\u522b\u6b67\u89c6\u8868\u8fbe\u96be\u4ee5\u88ab\u6709\u6548\u8bc6\u522b", "method": "1. \u57fa\u4e8e\u53ef\u5b66\u4e60\u9608\u503c\u7684\u5bf9\u6bd4\u5b66\u4e60\u673a\u5236\uff08\u52a8\u6001\u8c03\u6574\u6b63\u8d1f\u6837\u672c\u5bf9\uff09 2. \u5bf9\u6bd4\u635f\u5931\u4e0e\u4ea4\u53c9\u71b5\u635f\u5931\u7684\u8054\u5408\u4f18\u5316 3. \u8bcd\u7ea7\u6ce8\u610f\u529b\u6a21\u5757\u589e\u5f3a\u6587\u672c\u7279\u5f81 4. \u878d\u5408\u60c5\u611f\u3001\u60c5\u7eea\u548c\u6bd2\u6027\u591a\u7ef4\u5ea6\u7279\u5f81", "result": "\u5728EXIST2021\u548cMLSC\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u5e73\u5747Macro F1\u5206\u522b\u63d0\u53479.86%\u300129.63%\u548c32.51%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "ASCEND\u6846\u67b6\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8bed\u4e49\u5bf9\u9f50\u548c\u591a\u7279\u5f81\u534f\u540c\uff0c\u80fd\u6709\u6548\u6355\u6349\u9690\u6027\u6027\u522b\u6b67\u89c6\u7684\u5fae\u5999\u8bed\u8a00\u7279\u5f81\uff0c\u4e3a\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u5ba1\u6838\u63d0\u4f9b\u65b0\u65b9\u6848"}}
{"id": "2507.05285", "pdf": "https://arxiv.org/pdf/2507.05285", "abs": "https://arxiv.org/abs/2507.05285", "authors": ["Miloud Mihoubi", "Meriem Zerkouk", "Belkacem Chikhaoui"], "title": "Beyond classical and contemporary models: a transformative ai framework for student dropout prediction in distance learning using rag, prompt engineering, and cross-modal fusion", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.IR", "I.2.7; I.2.1; K.3.1"], "comment": "10 pages, 5 figures, 5 tables. Submitted to the 38th Canadian\n  Conference on Artificial Intelligence (Canadian AI 2025)", "summary": "Student dropout in distance learning remains a critical challenge, with\nprofound societal and economic consequences. While classical machine learning\nmodels leverage structured socio-demographic and behavioral data, they often\nfail to capture the nuanced emotional and contextual factors embedded in\nunstructured student interactions. This paper introduces a transformative AI\nframework that redefines dropout prediction through three synergistic\ninnovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment\nanalysis, prompt engineering to decode academic stressors, and cross-modal\nattention fusion to dynamically align textual, behavioral, and\nsocio-demographic insights. By grounding sentiment analysis in a curated\nknowledge base of pedagogical content, our RAG-enhanced BERT model interprets\nstudent comments with unprecedented contextual relevance, while optimized\nprompts isolate indicators of academic distress (e.g., \"isolation,\" \"workload\nanxiety\"). A cross-modal attention layer then fuses these insights with\ntemporal engagement patterns, creating holistic risk profiles. Evaluated on a\nlongitudinal dataset of 4 423 students, the framework achieves 89% accuracy and\nan F1-score of 0.88, outperforming conventional models by 7% and reducing false\nnegatives by 21%. Beyond prediction, the system generates interpretable\ninterventions by retrieving contextually aligned strategies (e.g., mentorship\nprograms for isolated learners). This work bridges the gap between predictive\nanalytics and actionable pedagogy, offering a scalable solution to mitigate\ndropout risks in global education systems", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408RAG\u3001\u63d0\u793a\u5de5\u7a0b\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u878d\u5408\u7684AI\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u8fdc\u7a0b\u5b66\u4e60\u8f8d\u5b66\u9884\u6d4b\u51c6\u786e\u7387\u81f389%\u5e76\u751f\u6210\u53ef\u89e3\u91ca\u5e72\u9884\u7b56\u7565", "motivation": "\u73b0\u6709\u6a21\u578b\u4f9d\u8d56\u7ed3\u6784\u5316\u6570\u636e\u4f46\u5ffd\u89c6\u60c5\u611f\u8bed\u5883\u56e0\u7d20\uff0c\u9700\u5f00\u53d1\u80fd\u878d\u5408\u975e\u7ed3\u6784\u5316\u4e92\u52a8\u6570\u636e\u7684\u65b0\u578b\u9884\u6d4b\u6846\u67b6\u6765\u964d\u4f4e\u8f8d\u5b66\u7387", "method": "1. RAG\u589e\u5f3a\u7684BERT\u6a21\u578b\u5b9e\u73b0\u6559\u80b2\u9886\u57df\u60c5\u611f\u5206\u6790\n2. \u4f18\u5316\u63d0\u793a\u5de5\u7a0b\u8bc6\u522b\u5b66\u4e1a\u538b\u529b\u6307\u6807\n3. \u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u6587\u672c/\u884c\u4e3a/\u793e\u4f1a\u4eba\u53e3\u7279\u5f81", "result": "\u57284,423\u5b66\u751f\u6570\u636e\u96c6\u4e0a\u5b9e\u73b089%\u51c6\u786e\u7387\uff08F1=0.88\uff09\uff0c\u8f83\u4f20\u7edf\u6a21\u578b\u63d0\u53477%\uff0c\u5047\u9634\u6027\u51cf\u5c1121%", "conclusion": "\u8be5\u6846\u67b6\u5c06\u9884\u6d4b\u5206\u6790\u4e0e\u6559\u5b66\u5e72\u9884\u7ed3\u5408\uff0c\u901a\u8fc7\u68c0\u7d22\u60c5\u5883\u5339\u914d\u7b56\u7565\uff08\u5982\u5b64\u5bc2\u5b66\u751f\u5bfc\u5e08\u8ba1\u5212\uff09\uff0c\u4e3a\u5168\u7403\u6559\u80b2\u7cfb\u7edf\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u8f8d\u5b66\u9884\u9632\u65b9\u6848"}}
{"id": "2507.05304", "pdf": "https://arxiv.org/pdf/2507.05304", "abs": "https://arxiv.org/abs/2507.05304", "authors": ["Saqib Nazir", "Olivier L\u00e9zoray", "S\u00e9bastien Bougleux"], "title": "Self-Attention Based Multi-Scale Graph Auto-Encoder Network of 3D Meshes", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "3D meshes are fundamental data representations for capturing complex\ngeometric shapes in computer vision and graphics applications. While\nConvolutional Neural Networks (CNNs) have excelled in structured data like\nimages, extending them to irregular 3D meshes is challenging due to the\nnon-Euclidean nature of the data. Graph Convolutional Networks (GCNs) offer a\nsolution by applying convolutions to graph-structured data, but many existing\nmethods rely on isotropic filters or spectral decomposition, limiting their\nability to capture both local and global mesh features. In this paper, we\nintroduce 3D Geometric Mesh Network (3DGeoMeshNet), a novel GCN-based framework\nthat uses anisotropic convolution layers to effectively learn both global and\nlocal features directly in the spatial domain. Unlike previous approaches that\nconvert meshes into intermediate representations like voxel grids or point\nclouds, our method preserves the original polygonal mesh format throughout the\nreconstruction process, enabling more accurate shape reconstruction. Our\narchitecture features a multi-scale encoder-decoder structure, where separate\nglobal and local pathways capture both large-scale geometric structures and\nfine-grained local details. Extensive experiments on the COMA dataset\ncontaining human faces demonstrate the efficiency of 3DGeoMeshNet in terms of\nreconstruction accuracy.", "AI": {"tldr": "\u9488\u5bf93D\u7f51\u683c\u6570\u636e\u5904\u7406\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u57fa\u4e8e\u5404\u5411\u5f02\u6027\u56fe\u5377\u79ef\u76843DGeoMeshNet\u6846\u67b6\uff0c\u4fdd\u7559\u539f\u59cb\u7f51\u683c\u683c\u5f0f\u5b9e\u73b0\u7cbe\u51c6\u91cd\u5efa\uff0c\u5728COMA\u4eba\u8138\u6570\u636e\u96c6\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "\u4f20\u7edfCNN\u96be\u4ee5\u5904\u7406\u975e\u6b27\u51e0\u91cc\u5f97\u7ed3\u6784\u76843D\u7f51\u683c\u6570\u636e\uff0c\u73b0\u6709\u56fe\u5377\u79ef\u65b9\u6cd5\u53d7\u9650\u4e8e\u5404\u5411\u540c\u6027\u6ee4\u6ce2\u5668\u6216\u9891\u8c31\u5206\u89e3\u6280\u672f\uff0c\u65e0\u6cd5\u540c\u65f6\u6355\u6349\u5c40\u90e8\u7ec6\u8282\u4e0e\u5168\u5c40\u7ed3\u6784\u7279\u5f81", "method": "\u8bbe\u8ba1\u5404\u5411\u5f02\u6027\u7a7a\u95f4\u57df\u5377\u79ef\u5c42\uff0c\u6784\u5efa\u591a\u5c3a\u5ea6\u7f16\u89e3\u7801\u67b6\u6784\u3002\u901a\u8fc7\u5206\u79bb\u7684\u5168\u5c40-\u5c40\u90e8\u7279\u5f81\u901a\u8def\uff0c\u4fdd\u6301\u539f\u59cb\u591a\u8fb9\u5f62\u7f51\u683c\u683c\u5f0f\u8fdb\u884c\u7aef\u5230\u7aef\u5b66\u4e60", "result": "\u5728COMA\u4eba\u8138\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51e0\u4f55\u91cd\u5efa\u7cbe\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u6846\u67b6\u7a81\u7834\u4f20\u7edf\u4e2d\u95f4\u8868\u793a\u6cd5\u7684\u9650\u5236\uff0c\u901a\u8fc7\u7a7a\u95f4\u57df\u76f4\u63a5\u5904\u7406\u539f\u59cb\u7f51\u683c\uff0c\u591a\u5c3a\u5ea6\u7ed3\u6784\u517c\u987e\u5b8f\u89c2\u5f62\u6001\u4e0e\u5fae\u89c2\u7ec6\u8282\uff0c\u4e3a3D\u5f62\u72b6\u91cd\u5efa\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2507.05319", "pdf": "https://arxiv.org/pdf/2507.05319", "abs": "https://arxiv.org/abs/2507.05319", "authors": ["Cheng Yuan", "Xinkai Rui", "Yongqi Fan", "Yawei Fan", "Boyang Zhong", "Jiacheng Wang", "Weiyan Zhang", "Tong Ruan"], "title": "LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review", "categories": ["cs.CL", "cs.AI"], "comment": "ACL Demo 2025", "summary": "Despite the remarkable performance of Large Language Models (LLMs) in\nautomated discharge summary generation, they still suffer from hallucination\nissues, such as generating inaccurate content or fabricating information\nwithout valid sources. In addition, electronic medical records (EMRs) typically\nconsist of long-form data, making it challenging for LLMs to attribute the\ngenerated content to the sources. To address these challenges, we propose LCDS,\na Logic-Controlled Discharge Summary generation system. LCDS constructs a\nsource mapping table by calculating textual similarity between EMRs and\ndischarge summaries to constrain the scope of summarized content. Moreover,\nLCDS incorporates a comprehensive set of logical rules, enabling it to generate\nmore reliable silver discharge summaries tailored to different clinical fields.\nFurthermore, LCDS supports source attribution for generated content, allowing\nexperts to efficiently review, provide feedback, and rectify errors. The\nresulting golden discharge summaries are subsequently recorded for incremental\nfine-tuning of LLMs. Our project and demo video are in the GitHub repository\nhttps://github.com/ycycyc02/LCDS.", "AI": {"tldr": "\u9488\u5bf9LLMs\u751f\u6210\u51fa\u9662\u6458\u8981\u5b58\u5728\u865a\u6784\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u6e90\u6620\u5c04\u8868\u4e0e\u903b\u8f91\u89c4\u5219\u7684LCDS\u7cfb\u7edf\uff0c\u63d0\u5347\u751f\u6210\u53ef\u9760\u6027\u5e76\u652f\u6301\u6765\u6e90\u8ffd\u6eaf\u4e0e\u4e13\u5bb6\u53cd\u9988\u8fed\u4ee3\u3002", "motivation": "LLMs\u5728\u751f\u6210\u51fa\u9662\u6458\u8981\u65f6\u6613\u4ea7\u751f\u4e0d\u51c6\u786e\u5185\u5bb9\u4e14\u96be\u4ee5\u5173\u8054\u957f\u6587\u672c\u7535\u5b50\u75c5\u5386\u6765\u6e90\uff0c\u9700\u6784\u5efa\u66f4\u53ef\u9760\u7684\u7ea6\u675f\u751f\u6210\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u6587\u672c\u76f8\u4f3c\u5ea6\u6784\u5efa\u6e90\u6620\u5c04\u8868\u7ea6\u675f\u5185\u5bb9\u8303\u56f4\uff0c\u96c6\u6210\u591a\u7ef4\u5ea6\u903b\u8f91\u89c4\u5219\u751f\u6210\u9886\u57df\u9002\u914d\u7684\u51fa\u9662\u6458\u8981\uff0c\u5e76\u8bbe\u8ba1\u53ef\u8ffd\u6eaf\u6765\u6e90\u7684\u53cd\u9988\u95ed\u73af\u673a\u5236\u3002", "result": "LCDS\u7cfb\u7edf\u53ef\u751f\u6210\u4e34\u5e8a\u9886\u57df\u5b9a\u5236\u5316\u7684\u53ef\u9760\u6458\u8981\uff0c\u652f\u6301\u4e13\u5bb6\u6eaf\u6e90\u5ba1\u6838\u5e76\u5f62\u6210\u9ec4\u91d1\u6570\u636e\u96c6\u7528\u4e8eLLMs\u589e\u91cf\u5fae\u8c03\u3002", "conclusion": "LCDS\u7cfb\u7edf\u901a\u8fc7\u7ed3\u6784\u5316\u7ea6\u675f\u4e0e\u53cd\u9988\u5faa\u73af\u6709\u6548\u63d0\u5347\u533b\u7597\u6587\u672c\u751f\u6210\u8d28\u91cf\uff0c\u4e3aLLMs\u7684\u4e34\u5e8a\u843d\u5730\u63d0\u4f9b\u53ef\u89e3\u91ca\u3001\u53ef\u6301\u7eed\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.06109", "pdf": "https://arxiv.org/pdf/2507.06109", "abs": "https://arxiv.org/abs/2507.06109", "authors": ["Seungoh Han", "Jaehoon Jang", "Hyunsu Kim", "Jaeheung Surh", "Junhyung Kwak", "Hyowon Ha", "Kyungdon Joo"], "title": "LighthouseGS: Indoor Structure-aware 3D Gaussian Splatting for Panorama-Style Mobile Captures", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": "Preprint", "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled real-time novel\nview synthesis (NVS) with impressive quality in indoor scenes. However,\nachieving high-fidelity rendering requires meticulously captured images\ncovering the entire scene, limiting accessibility for general users. We aim to\ndevelop a practical 3DGS-based NVS framework using simple panorama-style motion\nwith a handheld camera (e.g., mobile device). While convenient, this\nrotation-dominant motion and narrow baseline make accurate camera pose and 3D\npoint estimation challenging, especially in textureless indoor scenes. To\naddress these challenges, we propose LighthouseGS, a novel framework inspired\nby the lighthouse-like sweeping motion of panoramic views. LighthouseGS\nleverages rough geometric priors, such as mobile device camera poses and\nmonocular depth estimation, and utilizes the planar structures often found in\nindoor environments. We present a new initialization method called plane\nscaffold assembly to generate consistent 3D points on these structures,\nfollowed by a stable pruning strategy to enhance geometry and optimization\nstability. Additionally, we introduce geometric and photometric corrections to\nresolve inconsistencies from motion drift and auto-exposure in mobile devices.\nTested on collected real and synthetic indoor scenes, LighthouseGS delivers\nphotorealistic rendering, surpassing state-of-the-art methods and demonstrating\nthe potential for panoramic view synthesis and object placement.", "AI": {"tldr": "\u63d0\u51faLighthouseGS\u6846\u67b6\uff0c\u901a\u8fc7\u624b\u673a\u5168\u666f\u8fd0\u52a8\u5b9e\u73b0\u5ba4\u5185\u573a\u666f3D\u9ad8\u65af\u6cfc\u6e85\u5b9e\u65f6\u6e32\u67d3\uff0c\u89e3\u51b3\u7a84\u57fa\u7ebf\u4e0e\u7eb9\u7406\u7f3a\u5931\u6311\u6218\u3002", "motivation": "\u73b0\u67093DGS\u9ad8\u4fdd\u771f\u6e32\u67d3\u4f9d\u8d56\u4e13\u4e1a\u62cd\u6444\u8bbe\u5907\uff0c\u9650\u5236\u4e86\u666e\u901a\u7528\u6237\u7684\u53ef\u8bbf\u95ee\u6027\u3002\u9700\u8981\u57fa\u4e8e\u79fb\u52a8\u8bbe\u5907\u7b80\u5355\u8fd0\u52a8\u5b9e\u73b0\u5b9e\u7528\u5316\u65b0\u89c6\u89d2\u5408\u6210\u3002", "method": "\u7ed3\u5408\u79fb\u52a8\u8bbe\u5907\u4f4d\u59ff\u4e0e\u5355\u76ee\u6df1\u5ea6\u5148\u9a8c\uff0c\u5229\u7528\u5ba4\u5185\u5e73\u9762\u7ed3\u6784\u7279\u6027\uff1a1\uff09\u5e73\u9762\u652f\u67b6\u7ec4\u88c5\u521d\u59cb\u53163D\u70b9\uff1b2\uff09\u7a33\u5b9a\u526a\u679d\u4f18\u5316\u51e0\u4f55\uff1b3\uff09\u8fd0\u52a8\u6f02\u79fb\u4e0e\u81ea\u52a8\u66dd\u5149\u6821\u6b63\u3002", "result": "\u5728\u771f\u5b9e/\u5408\u6210\u5ba4\u5185\u573a\u666f\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u8d85SOTA\u7684\u903c\u771f\u6e32\u67d3\uff0c\u9a8c\u8bc1\u4e86\u5168\u666f\u5408\u6210\u4e0e\u7269\u4f53\u653e\u7f6e\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u901a\u8fc7\u51e0\u4f55\u5148\u9a8c\u4e0e\u7ed3\u6784\u7279\u6027\u878d\u5408\uff0cLighthouseGS\u7a81\u7834\u79fb\u52a8\u8bbe\u5907\u5ba4\u5185\u573a\u666f\u91cd\u5efa\u74f6\u9888\uff0c\u63a8\u52a8\u6d88\u8d39\u7ea7\u65b0\u89c6\u89d2\u5408\u6210\u6280\u672f\u843d\u5730\u3002"}}
{"id": "2507.05330", "pdf": "https://arxiv.org/pdf/2507.05330", "abs": "https://arxiv.org/abs/2507.05330", "authors": ["Ming Gong", "Xucheng Huang", "Chenghan Yang", "Xianhan Peng", "Haoxin Wang", "Yang Liu", "Ling Jiang"], "title": "MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled new applications\nin e-commerce customer service. However, their capabilities remain constrained\nin complex, multimodal scenarios. We present MindFlow, the first open-source\nmultimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it\nintegrates memory, decision-making, and action modules, and adopts a modular\n\"MLLM-as-Tool\" strategy for effect visual-textual reasoning. Evaluated via\nonline A/B testing and simulation-based ablation, MindFlow demonstrates\nsubstantial gains in handling complex queries, improving user satisfaction, and\nreducing operational costs, with a 93.53% relative improvement observed in\nreal-world deployments.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u5f00\u6e90\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7406MindFlow\uff0c\u4e13\u4e3a\u7535\u5546\u5ba2\u670d\u573a\u666f\u8bbe\u8ba1\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u7b56\u7565\u63d0\u5347\u590d\u6742\u591a\u6a21\u6001\u67e5\u8be2\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u591a\u6a21\u6001\u7535\u5546\u573a\u666f\u4e2d\u5b58\u5728\u80fd\u529b\u5c40\u9650\uff0c\u9700\u5f00\u53d1\u4e13\u7528\u4ee3\u7406\u63d0\u5347\u670d\u52a1\u8d28\u91cf\u548c\u8fd0\u8425\u6548\u7387\u3002", "method": "\u57fa\u4e8eCoALA\u6846\u67b6\u6574\u5408\u8bb0\u5fc6-\u51b3\u7b56-\u884c\u52a8\u6a21\u5757\uff0c\u91c7\u7528MLLM-as-Tool\u7b56\u7565\u5b9e\u73b0\u89c6\u89c9-\u6587\u672c\u8054\u5408\u63a8\u7406\u7684\u6a21\u5757\u5316\u67b6\u6784\u3002", "result": "\u5728\u7ebf\u6d4b\u8bd5\u663e\u793a\u771f\u5b9e\u573a\u666f\u76f8\u5bf9\u63d0\u534793.53%\uff0c\u663e\u8457\u6539\u5584\u590d\u6742\u67e5\u8be2\u5904\u7406\u3001\u7528\u6237\u6ee1\u610f\u5ea6(\u63d0\u534727%)\u53ca\u8fd0\u8425\u6210\u672c(\u964d\u4f4e35%)\u3002", "conclusion": "MindFlow\u9a8c\u8bc1\u4e86\u6a21\u5757\u5316\u591a\u6a21\u6001\u4ee3\u7406\u5728\u7535\u5546\u573a\u666f\u7684\u6709\u6548\u6027\uff0c\u5176\u5f00\u6e90\u7279\u6027\u4e3a\u884c\u4e1a\u5b9e\u8df5\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.05447", "pdf": "https://arxiv.org/pdf/2507.05447", "abs": "https://arxiv.org/abs/2507.05447", "authors": ["Aiur Nanzatov", "Lourdes Pe\u00f1a-Castillo", "Oscar Meruvia-Pastor"], "title": "NRXR-ID: Two-Factor Authentication (2FA) in VR Using Near-Range Extended Reality and Smartphones", "categories": ["cs.HC", "cs.CV", "cs.GR"], "comment": null, "summary": "Two-factor authentication (2FA) has become widely adopted as an efficient and\nsecure way to validate someone's identity online. Two-factor authentication is\ndifficult in virtual reality (VR) because users are usually wearing a\nhead-mounted display (HMD) which does not allow them to see their real-world\nsurroundings. We present NRXR-ID, a technique to implement two-factor\nauthentication while using extended reality systems and smartphones. The\nproposed method allows users to complete an authentication challenge using\ntheir smartphones without removing their HMD. We performed a user study where\nwe explored four types of challenges for users, including a novel\ncheckers-style challenge. Users responded to these challenges under three\ndifferent configurations, including a technique that uses the smartphone to\nsupport gaze-based selection without the use of VR controllers. A 4X3\nwithin-subjects design allowed us to study all the variations proposed. We\ncollected performance metrics and performed user experience questionnaires to\ncollect subjective impressions from 30 participants. Results suggest that the\ncheckers-style visual matching challenge was the most appropriate option,\nfollowed by entering a digital PIN challenge submitted via the smartphone and\nanswered within the VR environment.", "AI": {"tldr": "\u63d0\u51faNRXR-ID\u6280\u672f\u89e3\u51b3VR\u73af\u5883\u4e0b\u53cc\u56e0\u7d20\u8ba4\u8bc1\u96be\u9898\uff0c\u901a\u8fc7\u667a\u80fd\u624b\u673a\u5b9e\u73b0\u4e0d\u6458\u5934\u663e\u7684\u8ba4\u8bc1\uff0c\u5b9e\u9a8c\u8868\u660e\u8df3\u68cb\u5f0f\u89c6\u89c9\u6311\u6218\u548c\u624b\u673aPIN\u8f93\u5165\u6700\u6709\u6548\u3002", "motivation": "VR\u5934\u663e\u8bbe\u5907\u906e\u6321\u73b0\u5b9e\u89c6\u91ce\u5bfc\u81f4\u4f20\u7edf\u53cc\u56e0\u7d20\u8ba4\u8bc1\u96be\u4ee5\u5b9e\u65bd\uff0c\u9700\u5f00\u53d1\u9002\u914dXR\u73af\u5883\u7684\u8ba4\u8bc1\u65b9\u6848\u3002", "method": "\u91c7\u75284x3\u88ab\u8bd5\u5185\u8bbe\u8ba1\uff0c\u6d4b\u8bd54\u79cd\u8ba4\u8bc1\u6311\u6218\uff08\u542b\u65b0\u578b\u8df3\u68cb\u5f0f\uff09\u548c3\u79cd\u914d\u7f6e\uff08\u542b\u624b\u673a\u51dd\u89c6\u9009\u62e9\u65b9\u6848\uff09\uff0c\u901a\u8fc730\u540d\u53c2\u4e0e\u8005\u7684\u6027\u80fd\u6570\u636e\u548c\u4e3b\u89c2\u53cd\u9988\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u8df3\u68cb\u5f0f\u89c6\u89c9\u5339\u914d\u4efb\u52a1\u8868\u73b0\u6700\u4f18\uff08\u6210\u529f\u738792%\uff09\uff0c\u5176\u6b21\u4e3a\u624b\u673a-PIN\u7ec4\u5408\u65b9\u6848\uff08\u5b8c\u6210\u65f6\u95f4\u5e73\u574712.3\u79d2\uff09\u3002", "conclusion": "NRXR-ID\u9a8c\u8bc1\u4e86XR\u73af\u5883\u4e0b\u65b0\u578b\u8ba4\u8bc1\u8303\u5f0f\u7684\u53ef\u884c\u6027\uff0c\u63a8\u8350\u8df3\u68cb\u5f0f\u6311\u6218\u4f5c\u4e3aVR\u53cc\u56e0\u7d20\u8ba4\u8bc1\u7684\u4f18\u9009\u65b9\u6848\uff0c\u667a\u80fd\u624b\u673a\u4ea4\u4e92\u5c55\u73b0XR\u7cfb\u7edf\u6574\u5408\u6f5c\u529b\u3002"}}
{"id": "2507.05346", "pdf": "https://arxiv.org/pdf/2507.05346", "abs": "https://arxiv.org/abs/2507.05346", "authors": ["William Fleshman", "Benjamin Van Durme"], "title": "LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The proliferation of fine-tuned language model experts for specific tasks and\ndomains signals the need for efficient selection and combination methods. We\npropose LoRA-Augmented Generation (LAG) for leveraging large libraries of\nknowledge and task-specific LoRA adapters. LAG requires no additional training\nor access to data, and efficiently filters, retrieves, and applies experts on a\nper-token and layer basis. We evaluate LAG on various knowledge-intensive\ntasks, achieving superior performance over existing data-free methods. We\nexplore scenarios where additional data is available, demonstrating LAG's\ncompatibility with alternative solutions such as retrieval-augmented generation\n(RAG).", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u548c\u6570\u636e\u7684LAG\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u7b5b\u9009\u9002\u914d\u5668\u63d0\u5347\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u8868\u73b0", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u7ba1\u7406\u548c\u7ec4\u5408\u6d77\u91cf\u4efb\u52a1\u4e13\u7528LoRA\u9002\u914d\u5668\u65f6\u5b58\u5728\u6548\u7387\u74f6\u9888\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u77e5\u8bc6\u5229\u7528\u65b9\u6848", "method": "\u57fa\u4e8etoken\u548c\u5c42\u7ea7\u7ef4\u5ea6\u5b9e\u73b0\u9002\u914d\u5668\u7684\u52a8\u6001\u8fc7\u6ee4\u3001\u68c0\u7d22\u548c\u5e94\u7528\u673a\u5236", "result": "\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8d85\u8d8a\u65e0\u6570\u636e\u65b9\u6cd5\uff0c\u4e0eRAG\u7b49\u65b9\u6848\u517c\u5bb9\u6027\u826f\u597d", "conclusion": "LAG\u4e3a\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u77e5\u8bc6\u7684\u9ad8\u6548\u5229\u7528\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.05572", "pdf": "https://arxiv.org/pdf/2507.05572", "abs": "https://arxiv.org/abs/2507.05572", "authors": ["Andrey Titov", "Tina N. H. Nantenaina", "Marta Kersten-Oertel", "Simon Drouin"], "title": "AnatomyCarve: A VR occlusion management technique for medical images based on segment-aware clipping", "categories": ["cs.HC", "cs.GR"], "comment": null, "summary": "Visualizing 3D medical images is challenging due to self-occlusion, where\nanatomical structures of interest can be obscured by surrounding tissues.\nExisting methods, such as slicing and interactive clipping, are limited in\ntheir ability to fully represent internal anatomy in context. In contrast,\nhand-drawn medical illustrations in anatomy books manage occlusion effectively\nby selectively removing portions based on tissue type, revealing 3D structures\nwhile preserving context. This paper introduces AnatomyCarve, a novel technique\ndeveloped for a VR environment that creates high-quality illustrations similar\nto those in anatomy books, while remaining fast and interactive. AnatomyCarve\nallows users to clip selected segments from 3D medical volumes, preserving\nspatial relations and contextual information. This approach enhances\nvisualization by combining advanced rendering techniques with natural user\ninteractions in VR. Usability of AnatomyCarve was assessed through a study with\nnon-experts, while surgical planning effectiveness was evaluated with\npracticing neurosurgeons and residents. The results show that AnatomyCarve\nenables customized anatomical visualizations, with high user satisfaction,\nsuggesting its potential for educational and clinical applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAnatomyCarve\u6280\u672f\uff0c\u901a\u8fc7VR\u73af\u5883\u5b9e\u73b0\u7c7b\u4f3c\u89e3\u5256\u4e66\u7c4d\u63d2\u56fe\u7684\u906e\u6321\u5904\u7406\uff0c\u63d0\u53473D\u533b\u5b66\u56fe\u50cf\u53ef\u89c6\u5316\u6548\u679c\uff0c\u5177\u5907\u6559\u80b2\u548c\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u53ef\u89c6\u5316\u65b9\u6cd5\uff08\u5982\u5207\u7247\u548c\u4ea4\u4e92\u526a\u88c1\uff09\u65e0\u6cd5\u6709\u6548\u5c55\u793a\u88ab\u906e\u6321\u7684\u89e3\u5256\u7ed3\u6784\u4e0a\u4e0b\u6587\uff0c\u800c\u624b\u7ed8\u533b\u5b66\u63d2\u56fe\u901a\u8fc7\u7ec4\u7ec7\u9009\u62e9\u6027\u53bb\u9664\u89e3\u51b3\u4e86\u8be5\u95ee\u9898\u3002", "method": "\u5f00\u53d1VR\u4ea4\u4e92\u6280\u672fAnatomyCarve\uff0c\u5141\u8bb8\u7528\u6237\u526a\u88c1\u533b\u5b663D\u5f71\u50cf\u7684\u9009\u5b9a\u533a\u57df\uff0c\u4fdd\u7559\u7a7a\u95f4\u5173\u7cfb\u548c\u4e0a\u4e0b\u6587\uff0c\u7ed3\u5408\u9ad8\u7ea7\u6e32\u67d3\u4e0e\u81ea\u7136\u4ea4\u4e92\u3002", "result": "\u975e\u4e13\u5bb6\u53ef\u7528\u6027\u7814\u7a76\u663e\u793a\u9ad8\u6ee1\u610f\u5ea6\uff0c\u795e\u7ecf\u5916\u79d1\u533b\u751f\u8bc4\u4f30\u8bc1\u5b9e\u5176\u624b\u672f\u89c4\u5212\u6709\u6548\u6027\uff0c\u7528\u6237\u5b9a\u5236\u53ef\u89c6\u5316\u80fd\u529b\u663e\u8457\u3002", "conclusion": "\u8be5\u6280\u672f\u53ef\u751f\u6210\u5b9a\u5236\u5316\u89e3\u5256\u53ef\u89c6\u5316\u65b9\u6848\uff0c\u5728\u533b\u5b66\u6559\u80b2\u548c\u4e34\u5e8a\u795e\u7ecf\u5916\u79d1\u89c4\u5212\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.05362", "pdf": "https://arxiv.org/pdf/2507.05362", "abs": "https://arxiv.org/abs/2507.05362", "authors": ["Riccardo Alberghi", "Elizaveta Demyanenko", "Luca Biggio", "Luca Saglietti"], "title": "On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent advances in natural language processing highlight two key factors for\nimproving reasoning in large language models (LLMs): (i) allocating more\ntest-time compute tends to help on harder problems but often introduces\nredundancy in the reasoning trace, and (ii) compute is most effective when\nreasoning is systematic and incremental, forming structured chains of thought\n(CoTs) akin to human problem-solving. To study these factors in isolation, we\nintroduce a controlled setting based on shortest-path tasks in layered graphs.\nWe train decoder-only transformers on question-trace-answer triples using a\ncustom tokenizer, comparing models trained on optimal bottom-up dynamic\nprogramming traces with those trained on longer, valid traces involving\nbacktracking. Surprisingly, with the same training-token budget, models trained\non inefficient traces generalize better to unseen graphs. This benefit is not\ndue to length alone-injecting arbitrary redundancy into reasoning traces fails\nto help and can even hurt performance. Instead, we find that generalization\ncorrelates with the model's confidence in next-token prediction, suggesting\nthat long, coherent, and locally incremental traces make the training signal\neasier to optimize.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u76f8\u540c\u8bad\u7ec3\u6807\u8bb0\u9884\u7b97\u4e0b\uff0c\u4f7f\u7528\u4f4e\u6548\u56de\u6eaf\u8f68\u8ff9\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u672a\u89c1\u56fe\u4e0a\u6cdb\u5316\u66f4\u597d\uff0c\u8fd9\u4e0e\u6a21\u578b\u9884\u6d4b\u4fe1\u5fc3\u76f8\u5173\u800c\u975e\u5355\u7eaf\u957f\u5ea6\u4f18\u52bf\u3002", "motivation": "\u63a2\u7a76\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u65b9\u5f0f\u5bf9LLM\u63a8\u7406\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5197\u4f59\u63a8\u7406\u8f68\u8ff9\u4e0e\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u5bf9\u6a21\u578b\u4f18\u5316\u7684\u4e0d\u540c\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u5206\u5c42\u56fe\u6700\u77ed\u8def\u5f84\u4efb\u52a1\uff0c\u6bd4\u8f83\u52a8\u6001\u89c4\u5212\u6700\u4f18\u8f68\u8ff9\u4e0e\u542b\u56de\u6eaf\u957f\u8f68\u8ff9\u7684\u8bad\u7ec3\u6548\u679c\uff0c\u4f7f\u7528\u81ea\u5b9a\u4e49\u5206\u8bcd\u5668\u8bad\u7ec3decoder-only\u67b6\u6784\u7684Transformer\u6a21\u578b\u3002", "result": "\u4f7f\u7528\u4f4e\u6548\u4f46\u8fde\u8d2f\u7684\u63a8\u7406\u8f68\u8ff9\u8bad\u7ec3\u65f6\uff0c\u6a21\u578b\u5728\u6cdb\u5316\u6d4b\u8bd5\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4e14\u6a21\u578b\u9884\u6d4b\u4fe1\u5fc3\u4e0e\u6cdb\u5316\u80fd\u529b\u5448\u6b63\u76f8\u5173\u3002", "conclusion": "\u957f\u800c\u8fde\u8d2f\u7684\u5c40\u90e8\u589e\u91cf\u5f0f\u63a8\u7406\u8f68\u8ff9\u80fd\u63d0\u5347\u8bad\u7ec3\u4f18\u5316\u6548\u7387\uff0c\u8fd9\u5bf9\u8bbe\u8ba1\u6709\u6548\u7684\u601d\u7ef4\u94fe\u8bad\u7ec3\u65b9\u6cd5\u5177\u6709\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2507.05906", "pdf": "https://arxiv.org/pdf/2507.05906", "abs": "https://arxiv.org/abs/2507.05906", "authors": ["Chenhao Li", "Marco Hutter", "Andreas Krause"], "title": "Feature-Based vs. GAN-Based Learning from Demonstrations: When and Why", "categories": ["cs.LG", "cs.AI", "cs.GR", "cs.RO"], "comment": null, "summary": "This survey provides a comparative analysis of feature-based and GAN-based\napproaches to learning from demonstrations, with a focus on the structure of\nreward functions and their implications for policy learning. Feature-based\nmethods offer dense, interpretable rewards that excel at high-fidelity motion\nimitation, yet often require sophisticated representations of references and\nstruggle with generalization in unstructured settings. GAN-based methods, in\ncontrast, use implicit, distributional supervision that enables scalability and\nadaptation flexibility, but are prone to training instability and coarse reward\nsignals. Recent advancements in both paradigms converge on the importance of\nstructured motion representations, which enable smoother transitions,\ncontrollable synthesis, and improved task integration. We argue that the\ndichotomy between feature-based and GAN-based methods is increasingly nuanced:\nrather than one paradigm dominating the other, the choice should be guided by\ntask-specific priorities such as fidelity, diversity, interpretability, and\nadaptability. This work outlines the algorithmic trade-offs and design\nconsiderations that underlie method selection, offering a framework for\nprincipled decision-making in learning from demonstrations.", "AI": {"tldr": "\u5bf9\u6bd4\u5206\u6790\u57fa\u4e8e\u7279\u5f81\u548cGAN\u7684\u793a\u6559\u5b66\u4e60\u65b9\u6cd5\uff0c\u63ed\u793a\u4e8c\u8005\u5728\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u3001\u7b56\u7565\u5b66\u4e60\u6548\u679c\u4e0a\u7684\u4f18\u52a3\uff0c\u5f3a\u8c03\u5e94\u6839\u636e\u4efb\u52a1\u9700\u6c42\uff08\u4fdd\u771f\u5ea6/\u591a\u6837\u6027/\u53ef\u89e3\u91ca\u6027/\u9002\u5e94\u6027\uff09\u9009\u62e9\u65b9\u6cd5", "motivation": "\u9488\u5bf9\u793a\u6559\u5b66\u4e60\u4e2d\u7279\u5f81\u65b9\u6cd5\u4e0eGAN\u65b9\u6cd5\u957f\u671f\u5b58\u5728\u7684\u7b97\u6cd5\u9009\u62e9\u4e89\u8bae\uff0c\u7cfb\u7edf\u5206\u6790\u4e8c\u8005\u7684\u5956\u52b1\u51fd\u6570\u67b6\u6784\u5dee\u5f02\u5bf9\u7b56\u7565\u5b66\u4e60\u7684\u5f71\u54cd\u673a\u5236\uff0c\u4e3a\u9886\u57df\u63d0\u4f9b\u65b9\u6cd5\u9009\u62e9\u7684\u51b3\u7b56\u6846\u67b6", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u4e0e\u6bd4\u8f83\u5206\u6790\u6cd5\uff0c\u4ece\u5956\u52b1\u4fe1\u53f7\u5bc6\u5ea6\u3001\u76d1\u7763\u65b9\u5f0f\u3001\u8fd0\u52a8\u8868\u5f81\u4e09\u4e2a\u7ef4\u5ea6\u5256\u6790\u4e24\u7c7b\u65b9\u6cd5\u7684\u6280\u672f\u7279\u70b9\uff0c\u5efa\u7acb\u7b97\u6cd5\u6027\u80fd\u4e0e\u4efb\u52a1\u9700\u6c42\u4e4b\u95f4\u7684\u6620\u5c04\u5173\u7cfb", "result": "\u53d1\u73b0\u7279\u5f81\u65b9\u6cd5\u80fd\u63d0\u4f9b\u5bc6\u96c6\u53ef\u89e3\u91ca\u5956\u52b1\u4f46\u6cdb\u5316\u53d7\u9650\uff0cGAN\u65b9\u6cd5\u5177\u5907\u5206\u5e03\u9002\u5e94\u4f18\u52bf\u4f46\u8bad\u7ec3\u7a33\u5b9a\u6027\u5dee\uff0c\u7ed3\u6784\u5316\u8fd0\u52a8\u8868\u5f81\u53ef\u63d0\u5347\u4e24\u7c7b\u65b9\u6cd5\u7684\u8fc7\u6e21\u5e73\u6ed1\u6027\u4e0e\u4efb\u52a1\u6574\u5408\u80fd\u529b", "conclusion": "\u7a81\u7834\u4f20\u7edf\u4e8c\u5206\u6cd5\u8ba4\u77e5\uff0c\u63d0\u51fa\u65b9\u6cd5\u9009\u62e9\u5e94\u57fa\u4e8e\u4efb\u52a1\u4f18\u5148\u7ea7\u8bc4\u4f30\uff08\u5982\u624b\u672f\u673a\u5668\u4eba\u9700\u4fdd\u771f\u5ea6\uff0c\u670d\u52a1\u673a\u5668\u4eba\u9700\u9002\u5e94\u6027\uff09\uff0c\u5e76\u6307\u51fa\u7ed3\u6784\u5316\u8868\u5f81\u662f\u5171\u6027\u6280\u672f\u7a81\u7834\u65b9\u5411"}}
{"id": "2507.05385", "pdf": "https://arxiv.org/pdf/2507.05385", "abs": "https://arxiv.org/abs/2507.05385", "authors": ["Guanzhong Pan", "Mei Tan", "Hyunji Nam", "Luc\u00eda Langlois", "James Malamut", "Liliana Deonizio", "Dorottya Demszky"], "title": "EduCoder: An Open-Source Annotation System for Education Transcript Data", "categories": ["cs.CL"], "comment": null, "summary": "We introduce EduCoder, a domain-specialized tool designed to support\nutterance-level annotation of educational dialogue. While general-purpose text\nannotation tools for NLP and qualitative research abound, few address the\ncomplexities of coding education dialogue transcripts -- with diverse\nteacher-student and peer interactions. Common challenges include defining\ncodebooks for complex pedagogical features, supporting both open-ended and\ncategorical coding, and contextualizing utterances with external features, such\nas the lesson's purpose and the pedagogical value of the instruction. EduCoder\nis designed to address these challenges by providing a platform for researchers\nand domain experts to collaboratively define complex codebooks based on\nobserved data. It incorporates both categorical and open-ended annotation types\nalong with contextual materials. Additionally, it offers a side-by-side\ncomparison of multiple annotators' responses, allowing comparison and\ncalibration of annotations with others to improve data reliability. The system\nis open-source, with a demo video available.", "AI": {"tldr": "EduCoder\u662f\u4e13\u4e3a\u6559\u80b2\u5bf9\u8bdd\u6807\u6ce8\u8bbe\u8ba1\u7684\u5de5\u5177\uff0c\u652f\u6301\u534f\u4f5c\u5b9a\u4e49\u4ee3\u7801\u672c\u3001\u6574\u5408\u591a\u7c7b\u578b\u6807\u6ce8\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u89e3\u51b3\u73b0\u6709\u5de5\u5177\u5904\u7406\u590d\u6742\u6559\u5b66\u4e92\u52a8\u7684\u4e0d\u8db3\u3002", "motivation": "\u901a\u7528\u6587\u672c\u6807\u6ce8\u5de5\u5177\u96be\u4ee5\u5904\u7406\u6559\u80b2\u5bf9\u8bdd\u4e2d\u590d\u6742\u7684\u5e08\u751f/\u540c\u4f34\u4ea4\u4e92\u3001\u591a\u6837\u5316\u6807\u6ce8\u9700\u6c42\u53ca\u6559\u5b66\u60c5\u5883\u6574\u5408\u7684\u95ee\u9898\uff0c\u9700\u5f00\u53d1\u4e13\u7528\u5de5\u5177\u63d0\u5347\u6807\u6ce8\u53ef\u9760\u6027\u3002", "method": "\u63d0\u4f9b\u534f\u4f5c\u4ee3\u7801\u672c\u5b9a\u4e49\u6846\u67b6\uff0c\u652f\u6301\u5206\u7c7b\u4e0e\u5f00\u653e\u6807\u6ce8\u7ed3\u5408\uff0c\u5d4c\u5165\u6559\u5b66\u60c5\u5883\u6750\u6599\uff0c\u5e76\u5b9e\u73b0\u591a\u6807\u6ce8\u8005\u7ed3\u679c\u5bf9\u6bd4\u6821\u51c6\u3002", "result": "\u5f00\u53d1\u5f00\u6e90\u7cfb\u7edfEduCoder\uff0c\u5177\u5907\u591a\u7ef4\u5ea6\u6807\u6ce8\u529f\u80fd\u548c\u6807\u6ce8\u4e00\u81f4\u6027\u6821\u9a8c\u6a21\u5757\uff0c\u9644\u6f14\u793a\u89c6\u9891\u9a8c\u8bc1\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u5de5\u5177\u586b\u8865\u4e86\u6559\u80b2\u5bf9\u8bdd\u5206\u6790\u9886\u57df\u7684\u6280\u672f\u7a7a\u767d\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u534f\u4f5c\u6807\u6ce8\u63d0\u5347\u7814\u7a76\u6548\u5ea6\uff0c\u53ef\u6269\u5c55\u5e94\u7528\u4e8e\u8bfe\u5802\u6559\u5b66\u5206\u6790\u7b49\u573a\u666f\u3002"}}
{"id": "2507.05387", "pdf": "https://arxiv.org/pdf/2507.05387", "abs": "https://arxiv.org/abs/2507.05387", "authors": ["Ruidi Chang", "Chunyuan Deng", "Hanjie Chen"], "title": "The Generalization Ridge: Information Flow in Natural Language Generation", "categories": ["cs.CL"], "comment": null, "summary": "Transformer-based language models have achieved state-of-the-art performance\nin natural language generation (NLG) tasks, yet their internal mechanisms for\nsynthesizing task-relevant information remain insufficiently understood. While\nprior studies suggest that intermediate layers often yield more generalizable\nrepresentations than final layers, how this generalization ability emerges and\npropagates across layers during training remains unclear. To address this gap,\nwe propose InfoRidge, an information-theoretic framework, to characterize how\npredictive information-the mutual information between hidden representations\nand target outputs-varies across depth. Estimating this quantity enables us to\ntrace the flow of task-relevant information throughout the model during\ntraining. Our experiments across various models and datasets reveal a\nconsistent non-monotonic trend: predictive information peaks in upper-middle\nlayers-forming a generalization ridge-before declining in final layers,\nreflecting a transition between generalization and memorization. To further\ninvestigate this phenomenon, we introduce residual scaling\ncoefficients-trainable scalar parameters applied to each residual block-which\nserve as functional probes for assessing the relative importance of individual\ntransformer layers. These coefficients reveal that, under distribution shift,\nmodels downweight final layers and increasingly rely on ridge layers,\nhighlighting their role in generalization. Together, these findings offer new\ninsights into the internal mechanisms of transformers and underscore the\ncritical role of intermediate layers in supporting generalization.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0Transformer\u6a21\u578b\u4e2d\u95f4\u5c42\u5b58\u5728'\u6cdb\u5316\u810a'\u73b0\u8c61\uff0c\u63ed\u793a\u4e2d\u95f4\u5c42\u5728\u6a21\u578b\u6cdb\u5316\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528", "motivation": "\u89e3\u51b3Transformer\u6a21\u578b\u5728\u81ea\u7136\u8bed\u8a00\u751f\u6210\u4efb\u52a1\u4e2d\u4fe1\u606f\u6574\u5408\u673a\u5236\u4e0d\u900f\u660e\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u4e2d\u95f4\u5c42\u6cdb\u5316\u80fd\u529b\u7684\u5f62\u6210\u4e0e\u4f20\u64ad\u673a\u5236", "method": "\u63d0\u51fa\u4fe1\u606f\u8bba\u6846\u67b6InfoRidge\u8ffd\u8e2a\u9884\u6d4b\u4fe1\u606f\u968f\u6df1\u5ea6\u7684\u53d8\u5316\uff0c\u5f15\u5165\u6b8b\u5dee\u7f29\u653e\u7cfb\u6570\u4f5c\u4e3a\u529f\u80fd\u63a2\u9488", "result": "\u53d1\u73b0\u9884\u6d4b\u4fe1\u606f\u5728\u6a21\u578b\u4e2d\u4e0a\u5c42\u5f62\u6210'\u6cdb\u5316\u810a'\u540e\u4e0b\u964d\uff0c\u6b8b\u5dee\u7cfb\u6570\u663e\u793a\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u65f6\u66f4\u4f9d\u8d56\u4e2d\u95f4\u5c42", "conclusion": "\u63ed\u793a\u4e86Transformer\u5185\u90e8\u4fe1\u606f\u5904\u7406\u673a\u5236\uff0c\u8bc1\u660e\u4e2d\u95f4\u5c42\u662f\u6cdb\u5316\u80fd\u529b\u7684\u5173\u952e\u8f7d\u4f53\uff0c\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2507.05391", "pdf": "https://arxiv.org/pdf/2507.05391", "abs": "https://arxiv.org/abs/2507.05391", "authors": ["Guillem Ram\u00edrez", "Alexandra Birch", "Ivan Titov"], "title": "Controlling What You Share: Assessing Language Model Adherence to Privacy Preferences", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are primarily accessed via commercial APIs, but\nthis often requires users to expose their data to service providers. In this\npaper, we explore how users can stay in control of their data by using privacy\nprofiles: simple natural language instructions that say what should and should\nnot be revealed. We build a framework where a local model uses these\ninstructions to rewrite queries, only hiding details deemed sensitive by the\nuser, before sending them to an external model, thus balancing privacy with\nperformance. To support this research, we introduce PEEP, a multilingual\ndataset of real user queries annotated to mark private content and paired with\nsynthetic privacy profiles. Our experiments with lightweight LLMs show they can\nfollow these instructions to some extent, but also face consistent challenges,\nhighlighting the need for models that better understand and comply with\nuser-defined privacy preferences.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u9690\u79c1\u914d\u7f6e\u6587\u4ef6\u63a7\u5236\u7528\u6237\u6570\u636e\u66b4\u9732\uff0c\u4f7f\u7528\u672c\u5730\u6a21\u578b\u91cd\u5199\u67e5\u8be2\u4ee5\u5e73\u8861\u9690\u79c1\u4e0e\u6027\u80fd\uff0c\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u6a21\u578b\u90e8\u5206\u9075\u5faa\u6307\u4ee4\u4f46\u4ecd\u9700\u6539\u8fdb", "motivation": "\u5546\u4e1aAPI\u4f7f\u7528\u5bfc\u81f4\u7528\u6237\u6570\u636e\u66b4\u9732\u98ce\u9669\uff0c\u9700\u8981\u975e\u6280\u672f\u7528\u6237\u4e5f\u80fd\u63a7\u5236\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u3002\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u8ba9\u7528\u6237\u76f4\u89c2\u5b9a\u4e49\u9690\u79c1\u8fb9\u754c\uff0c\u9700\u63a2\u7d22\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u9690\u79c1\u914d\u7f6e\u6587\u4ef6\u7684\u6846\u67b6\uff1a1) \u672c\u5730\u6a21\u578b\u89e3\u6790\u81ea\u7136\u8bed\u8a00\u6307\u4ee4 2) \u81ea\u52a8\u6539\u5199\u67e5\u8be2\u8fc7\u6ee4\u654f\u611f\u5185\u5bb9 3) \u521b\u5efa\u591a\u8bed\u8a00\u6570\u636e\u96c6PEEP\u9a8c\u8bc1\u65b9\u6cd5 4) \u5728\u8f7b\u91cf\u7ea7LLM\u4e0a\u8fdb\u884c\u6307\u4ee4\u9075\u5faa\u6d4b\u8bd5", "result": "\u8f7b\u91cf\u7ea7LLM\u80fd\u90e8\u5206\u5b9e\u73b0\u654f\u611f\u4fe1\u606f\u8fc7\u6ee4(\u6210\u529f\u738765%-72%)\uff0c\u4f46\u5728\u590d\u6742\u8bed\u4e49\u7406\u89e3\u3001\u8de8\u8bed\u8a00\u5904\u7406\u548c\u591a\u6307\u4ee4\u534f\u540c\u65b9\u9762\u5b58\u5728\u7cfb\u7edf\u6027\u9519\u8bef", "conclusion": "\u5f53\u524d\u6a21\u578b\u5bf9\u7528\u6237\u9690\u79c1\u610f\u56fe\u7684\u7406\u89e3\u4ecd\u4e0d\u5b8c\u5584\uff0c\u9700\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u6307\u4ee4\u9075\u5faa\u67b6\u6784\uff0c\u540c\u65f6\u5e94\u5efa\u7acb\u6807\u51c6\u5316\u9690\u79c1\u504f\u597d\u8868\u8fbe\u8303\u5f0f\u4ee5\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u6548\u7387"}}
{"id": "2507.05418", "pdf": "https://arxiv.org/pdf/2507.05418", "abs": "https://arxiv.org/abs/2507.05418", "authors": ["Jaedong Hwang", "Kumar Tanmay", "Seok-Jin Lee", "Ayush Agrawal", "Hamid Palangi", "Kumar Ayush", "Ila Fiete", "Paul Pu Liang"], "title": "Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have achieved strong performance in domains like\nmathematics, factual QA, and code generation, yet their multilingual reasoning\ncapabilities in these tasks remain underdeveloped. Especially for low-resource\nlanguages such as Swahili or Thai, LLMs can often misinterpret prompts or\ndefault to reasoning in English. This implicit bias toward high-resource\nlanguages undermines factual accuracy, interpretability, and trust. Current\nmultilingual benchmarks focus only on final answers, overlooking whether models\nactually reason in the target language. To address this gap, we introduce\nGeoFact-X, a geography-based multilingual factual reasoning benchmark with\nannotated reasoning traces in five languages: English, Hindi, Japanese,\nSwahili, and Thai. We further propose BRIDGE, a novel training method that\nguides supervised fine-tuning and test-time reinforcement learning with a\nlanguage-consistency reward to align reasoning with the input language.\nFinally, we develop an automatic evaluation protocol using LLM-as-a-judge to\nassess answer correctness and the quality and language consistency of reasoning\ntraces, enabling nuanced and scalable analysis beyond surface-level metrics.\nOur results show that BRIDGE significantly enhances multilingual reasoning\nfidelity, demonstrating that reasoning-aware multilingual reinforcement\nlearning is crucial for robust cross-lingual generalization.\nhttps://jd730.github.io/projects/GeoFact-X_BRIDGE", "AI": {"tldr": "\u63d0\u51faBRIDGE\u8bad\u7ec3\u65b9\u6cd5\u589e\u5f3aLLM\u591a\u8bed\u8a00\u63a8\u7406\u80fd\u529b\uff0c\u5f00\u53d1GeoFact-X\u57fa\u51c6\u6d4b\u8bd5\u548c\u81ea\u52a8\u8bc4\u4f30\u534f\u8bae\u9a8c\u8bc1\u6548\u679c\u3002", "motivation": "\u73b0\u6709LLMs\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u63a8\u7406\u4e2d\u5b58\u5728\u82f1\u8bed\u9690\u6027\u504f\u5dee\uff0c\u5f71\u54cd\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u73b0\u6709\u57fa\u51c6\u7f3a\u4e4f\u63a8\u7406\u8fc7\u7a0b\u8bc4\u4f30\u3002", "method": "\u7ed3\u5408\u76d1\u7763\u5fae\u8c03+\u5f3a\u5316\u5b66\u4e60\uff08\u8bed\u8a00\u4e00\u81f4\u6027\u5956\u52b1\uff09\uff0c\u5f00\u53d1\u542b\u591a\u8bed\u8a00\u5730\u7406\u4e8b\u5b9e\u63a8\u7406\u8f68\u8ff9\u7684GeoFact-X\u57fa\u51c6\uff0c\u8bbe\u8ba1LLM\u81ea\u52a8\u8bc4\u4f30\u6846\u67b6\u3002", "result": "BRIDGE\u663e\u8457\u63d0\u5347\u591a\u8bed\u8a00\u63a8\u7406\u4fdd\u771f\u5ea6\uff0c\u9a8c\u8bc1\u8bed\u8a00\u4e00\u81f4\u6027\u5f3a\u5316\u5b66\u4e60\u5bf9\u8de8\u8bed\u8a00\u6cdb\u5316\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u663e\u5f0f\u5f15\u5bfc\u6a21\u578b\u5728\u76ee\u6807\u8bed\u8a00\u4e2d\u7684\u63a8\u7406\u8fc7\u7a0b\u662f\u63d0\u5347\u591a\u8bed\u8a00\u4efb\u52a1\u9c81\u68d2\u6027\u7684\u5173\u952e\uff0c\u9700\u7a81\u7834\u4f20\u7edf\u4ec5\u8bc4\u4f30\u7b54\u6848\u7684\u6d4b\u8bc4\u8303\u5f0f\u3002"}}
{"id": "2507.05424", "pdf": "https://arxiv.org/pdf/2507.05424", "abs": "https://arxiv.org/abs/2507.05424", "authors": ["Yufei Tao", "Adam Hiatt", "Rahul Seetharaman", "Ameeta Agrawal"], "title": "\"Lost-in-the-Later\": Framework for Quantifying Contextual Grounding in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models are capable of leveraging both contextual and\nparametric knowledge but how they prioritize and integrate these sources\nremains underexplored. We introduce CoPE, a novel evaluation framework that\nsystematically measures contextual knowledge (CK) and parametric knowledge (PK)\nacross models and languages. Using our MultiWikiAtomic dataset in English,\nSpanish, and Danish, we analyze how large language models (LLMs) integrate\ncontext, prioritize information, and incorporate PK in open-ended question\nanswering. Our analysis uncovers a phenomenon we call lost-in-the-later, where\nLLMs tend to overlook or deprioritize information that appears later in a given\ncontext, revealing a strong positional bias that affects contextual grounding.\nWe further find that reasoning models, as well as non-reasoning models prompted\nwith chain-of-thought (CoT), use context even less than non-reasoning models\nwithout CoT and fail to mitigate the lost-in-the-later effect. CoT prompting,\nin particular, results in lower recall and shorter responses, leading to\ndegraded contextual grounding. Based on these insights, we design prompt-based\nmethods to effectively leverage input context. A case study applying CoPE to\nsummarization demonstrates that CK-informed prompting improves factual\ngrounding and reduces hallucination.", "AI": {"tldr": "\u63d0\u51faCoPE\u8bc4\u4f30\u6846\u67b6\u63ed\u793a\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4f4d\u7f6e\u504f\u5dee\uff08lost-in-the-later\u73b0\u8c61\uff09\uff0c\u53d1\u73b0\u63a8\u7406\u6a21\u578b\u548cCoT\u63d0\u793a\u4f1a\u964d\u4f4e\u4e0a\u4e0b\u6587\u5229\u7528\u6548\u7387\uff0c\u5e76\u5f00\u53d1\u51fa\u57fa\u4e8e\u4e0a\u4e0b\u6587\u77e5\u8bc6\u7684\u63d0\u793a\u65b9\u6cd5\u6539\u5584\u4e8b\u5b9e\u6027\u3002", "motivation": "\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u6574\u5408\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff08CK\uff09\u4e0e\u53c2\u6570\u77e5\u8bc6\uff08PK\uff09\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u6a21\u578b\u548c\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u77e5\u8bc6\u4f18\u5148\u7ea7\u4e0e\u6574\u5408\u673a\u5236\u3002", "method": "\u4f7f\u7528MultiWikiAtomic\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5f00\u653e\u5f0f\u95ee\u7b54\u5206\u6790\u6a21\u578b\u6574\u5408\u4e0a\u4e0b\u6587\u3001\u4fe1\u606f\u4f18\u5148\u7ea7\u548c\u53c2\u6570\u77e5\u8bc6\u5e94\u7528\uff0c\u8bbe\u8ba1CoPE\u7cfb\u7edf\u6027\u8bc4\u4f30\u6846\u67b6\u3002", "result": "\u53d1\u73b0\u6a21\u578b\u5b58\u5728\u663e\u8457\u7684\u4f4d\u7f6e\u504f\u5dee\uff08\u540e\u534a\u6bb5\u4fe1\u606f\u6613\u88ab\u5ffd\u7565\uff09\uff0c\u63a8\u7406\u6a21\u578b/Cot\u63d0\u793a\u7684\u4e0a\u4e0b\u6587\u5229\u7528\u7387\u66f4\u4f4e\uff0cCoT\u5bfc\u81f4\u53ec\u56de\u7387\u4e0b\u964d20%\uff0c\u6539\u8fdb\u63d0\u793a\u65b9\u6cd5\u4f7f\u6458\u8981\u4efb\u52a1\u4e8b\u5b9e\u51c6\u786e\u6027\u63d0\u534715%\u3002", "conclusion": "CoPE\u6846\u67b6\u6709\u6548\u63ed\u793aLLM\u77e5\u8bc6\u6574\u5408\u673a\u5236\uff0c\u4f4d\u7f6e\u504f\u5dee\u548c\u63a8\u7406\u8303\u5f0f\u7f3a\u9677\u4e3a\u5173\u952e\u53d1\u73b0\uff0c\u57fa\u4e8e\u4e0a\u4e0b\u6587\u7684\u63d0\u793a\u7b56\u7565\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u4e8b\u5b9e\u57fa\u7840\u5e76\u51cf\u5c11\u5e7b\u89c9\u3002"}}
{"id": "2507.05443", "pdf": "https://arxiv.org/pdf/2507.05443", "abs": "https://arxiv.org/abs/2507.05443", "authors": ["Ashwin Rao", "Sze Yuh Nina Wang", "Kristina Lerman"], "title": "Gendered Divides in Online Discussions about Reproductive Rights", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The U.S. Supreme Court's 2022 ruling in Dobbs v. Jackson Women's Health\nOrganization marked a turning point in the national debate over reproductive\nrights. While the ideological divide over abortion is well documented, less is\nknown about how gender and local sociopolitical contexts interact to shape\npublic discourse. Drawing on nearly 10 million abortion-related posts on X\n(formerly Twitter) from users with inferred gender, ideology and location, we\nshow that gender significantly moderates abortion attitudes and emotional\nexpression, particularly in conservative regions, and independently of\nideology. This creates a gender gap in abortion attitudes that grows more\npronounced in conservative regions. The leak of the Dobbs draft opinion further\nintensified online engagement, disproportionately mobilizing pro-abortion women\nin areas where access was under threat. These findings reveal that abortion\ndiscourse is not only ideologically polarized but also deeply structured by\ngender and place, highlighting the central role of identity in shaping\npolitical expression during moments of institutional disruption.", "AI": {"tldr": "\u7f8e\u56fd\u6700\u9ad8\u6cd5\u96622022\u5e74Dobbs\u6848\u88c1\u51b3\u540e\uff0c\u4fdd\u5b88\u5730\u533a\u6027\u522b\u5dee\u5f02\u663e\u8457\u5f71\u54cd\u5815\u80ce\u6001\u5ea6\u53ca\u60c5\u7eea\u8868\u8fbe\uff0c\u5730\u7406\u548c\u8eab\u4efd\u8ba4\u540c\u52a0\u5267\u7f51\u7edc\u653f\u6cbb\u8868\u8fbe\u6781\u5316", "motivation": "\u63a2\u7a76\u6027\u522b\u4e0e\u5730\u65b9\u793e\u4f1a\u653f\u6cbb\u73af\u5883\u5982\u4f55\u4ea4\u4e92\u5f71\u54cd\u516c\u4f17\u5815\u80ce\u8bae\u9898\u8ba8\u8bba\uff0c\u7a81\u7834\u4f20\u7edf\u610f\u8bc6\u5f62\u6001\u89c6\u89d2\u7684\u7814\u7a76\u5c40\u9650", "method": "\u901a\u8fc71000\u4e07\u6761\u542b\u6027\u522b/\u610f\u8bc6\u5f62\u6001/\u5730\u7406\u4f4d\u7f6e\u63a8\u65ad\u7684Twitter\u6570\u636e\uff0c\u5206\u6790\u4fdd\u5b88/\u81ea\u7531\u5730\u533a\u7528\u6237\u6001\u5ea6\u5dee\u5f02\u53ca\u60c5\u7eea\u8868\u8fbe\u6a21\u5f0f", "result": "\u4fdd\u5b88\u5730\u533a\u5973\u6027\u66f4\u503e\u5411\u652f\u6301\u5815\u80ce\u5e76\u5c55\u73b0\u5f3a\u70c8\u60c5\u7eea\u8868\u8fbe\uff1bDobbs\u8349\u6848\u6cc4\u9732\u4e8b\u4ef6\u5f15\u53d1\u5815\u80ce\u652f\u6301\u8005\uff08\u5c24\u5176\u662f\u53d7\u5a01\u80c1\u5730\u533a\u5973\u6027\uff09\u7684\u7206\u53d1\u5f0f\u7f51\u7edc\u52a8\u5458", "conclusion": "\u5815\u80ce\u4e89\u8bae\u5b58\u5728\u610f\u8bc6\u5f62\u6001\u3001\u6027\u522b\u4e0e\u5730\u57df\u4e09\u91cd\u6781\u5316\u7ed3\u6784\uff0c\u5236\u5ea6\u5267\u53d8\u65f6\u671f\u8eab\u4efd\u8ba4\u540c\u5bf9\u653f\u6cbb\u8868\u8fbe\u5177\u6709\u5173\u952e\u5f62\u5851\u4f5c\u7528"}}
