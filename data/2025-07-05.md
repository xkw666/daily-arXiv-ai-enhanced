<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 26]
- [cs.GR](#cs.GR) [Total: 2]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.SE](#cs.SE) [Total: 2]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.CV](#cs.CV) [Total: 7]
- [eess.AS](#eess.AS) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
*Tian Lan,Xiangdong Su,Xu Liu,Ruirui Wang,Ke Chang,Jiang Li,Guanglai Gao*

Main category: cs.CL

TL;DR: 开发多任务中文偏见评估基准McBE，涵盖12类偏见和5种任务，发现主流大语言模型均存在不同程度偏见。


<details>
  <summary>Details</summary>
Motivation: 现有偏见评估数据集集中于英语/北美文化，中文数据集稀缺且仅支持单一评估维度，需构建覆盖多任务的中文评估体系。

Method: 构建包含4,077实例的McBE基准，涵盖12个主类别/82子类偏见，引入5种评估任务实现多维度测量。

Result: 不同系列/参数规模的LLMs均显示不同程度偏见，分析揭示了模型偏见分布的新模式。

Conclusion: McBE能全面评估中文LLMs偏见，现有模型普遍存在文化/伦理偏差，需针对性优化降低伦理风险。

Abstract: As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.

</details>


### [2] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
*Keyan Jin,Yapeng Wang,Leonel Santos,Tao Fang,Xu Yang,Sio Kei Im,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: 研究发现，在对话摘要任务中，显式的逐步推理机制（如CoT）反而会导致摘要冗长、事实不一致等问题，非推理型LLMs表现更优。


<details>
  <summary>Details</summary>
Motivation: 探索逐步推理架构在需要同时满足抽象性和简洁性的对话摘要场景中的实际效果，弥补当前研究的空白。

Method: 通过跨语言/领域/长度的多维度评估（使用SAMSum等四大基准），结合LLM自动指标和人工标准，对比分析推理型与非推理型LLMs在通用/角色导向/查询导向三种摘要范式中的表现。

Result: 推理型LLMs产生摘要的简洁度降低23%，事实一致性误差增加17%，在80%的测试场景中劣于非推理模型。

Conclusion: 揭示了当前推理型LLMs在复杂对话场景的局限性，提出需要开发针对性建模方法和评估体系来提升实际应用价值。

Abstract: Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.

</details>


### [3] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

TL;DR: 研究发现Huginn-3.5B深度循环Transformer模型在算术任务中未展现出显著的可解释隐式思维链推理能力，增加循环深度仅带来边际性能提升。


<details>
  <summary>Details</summary>
Motivation: 探索循环架构能否在保持参数效率的同时，将复杂推理过程内部化为潜在表示（Latent CoT），而非依赖低效的自然语言外显推理。

Method: 使用Logit Lens、Coda Lens等探针技术分析模型隐状态，通过追踪最终结果和中间结果token的排名轨迹评估推理结构。

Result: 1. 隐状态可解释性高度依赖循环块层数和解码方法
2. 循环深度增加仅提升0.3%准确率
3. 显式外显推理模型性能显著优于隐式方法

Conclusion: 当前深度循环架构难以有效内化复杂推理过程，显式思维链仍为更可靠的推理实现方式。

Abstract: Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [4] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
*Steven Song,Anirudh Subramanyam,Zhenyu Zhang,Aarti Venkat,Robert L. Grossman*

Main category: cs.CL

TL;DR: 开发开源工具GDC Cohort Copilot，通过自然语言生成癌症基因组队列过滤器，本地部署的LLM效果优于GPT-4o


<details>
  <summary>Details</summary>
Motivation: 解决用户在GDC平台中难以通过图形界面精准构建患者队列的问题，利用自然语言交互降低使用门槛

Method: 基于大型语言模型(LLM)实现自然语言到GDC过滤器的自动转换，开发交互界面供用户微调结果

Result: 本地开源模型GDC Cohort LLM在生成队列任务中表现超越GPT-4o，提供Docker镜像和开源代码

Conclusion: 自然语言交互显著提升GDC队列构建效率，专精模型在特定领域任务中展现竞争优势

Abstract: Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [5] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

TL;DR: MemAgent通过分段处理和内存覆盖策略优化长文本处理，在32K训练基础上成功外推至3.5M QA任务并保持优异性能


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在处理无限长文档时存在的性能下降问题，特别是在不外推情况下保持线性复杂度的终极挑战

Method: 提出MemAgent代理工作流程（分段读取+内存覆盖策略），并扩展DAPO算法实现独立上下文的多对话生成训练

Result: 实现从8K到32K训练后外推至3.5M QA任务（性能损失<5%），在512K RULER测试中达到95%+准确率

Conclusion: MemAgent展现出卓越的长上下文处理能力，有效突破了长文本外推的技术瓶颈

Abstract: Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [6] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

TL;DR: 提出DoMIX方法，利用LoRA模块实现高效且对领域顺序鲁棒的持续领域自适应预训练，解决现有方法的高计算成本、顺序敏感性和单一模型限制。


<details>
  <summary>Details</summary>
Motivation: 现有持续DAP方法存在三方面缺陷：1) 训练计算成本高且显存占用大 2) 对增量数据顺序敏感 3) 为所有任务提供单一通用模型，违背DAP本质特性

Method: 基于参数高效微调技术LoRA，设计支持并行领域预训练的方法。通过模块化架构实现领域知识的有效积累，并为特定任务提供定制化预训练模型。

Result: DoMIX在保持训练效率的同时，展现出对领域顺序的强鲁棒性，并能利用累积知识提升下游任务表现。该方法可扩展至标准大语言模型微调场景。

Conclusion: 通过LoRA实现参数高效学习，DoMIX成功解决了持续DAP的核心挑战，为领域自适应预训练提供了更灵活高效的解决方案。

Abstract: Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [7] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
*Christian Jaumann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CL

TL;DR: 提出集成多模态大语言模型与少样本检索策略的科学视觉问答系统，在SciVQA 2025测试中排名第三（平均F1 85.12）


<details>
  <summary>Details</summary>
Motivation: 提升科学视觉问答任务的性能，通过模型集成与动态策略选择优化多模态理解

Method: 1. 集成两个多模态大语言模型
2. 基于图表类型和问题类别动态选择模型与少样本示例
3. 通过置信度阈值筛选最终答案

Result: 盲测排名第三（7个团队），ROUGE-1/ROUGE-L/BERTS平均F1达85.12

Conclusion: 验证了动态策略选择与模型集成在科学VQA任务的有效性，代码已开源促进后续研究

Abstract: This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


### [8] [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)
*Pilsung Kang*

Main category: cs.CL

TL;DR: 量子参数化电路（PQCs）替代传统前馈网络，在保持精度的同时减少99%参数量并提升数据效率


<details>
  <summary>Details</summary>
Motivation: 前馈网络（FFN）占Transformer编码器三分之二的参数量，量子电路可提供更高效的参数替代方案

Method: 在紧凑型BERT中采用带残差连接的PQC架构，结合R_Y/R_Z旋转门和交替纠缠策略，确保训练稳定性与高表达能力

Result: 在SST-2和DBpedia基准测试中达到基线102%准确率，少样本学习场景表现优异，前馈网络参数减少超99%

Conclusion: 量子电路与深度学习原则协同设计时，可成为高效参数替代方案，尤其在数据效率方面展现竞争优势

Abstract: Parameterized quantum circuits (PQCs) have recently emerged as promising
components for enhancing the expressibility of neural architectures. In this
work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the
feedforward network (FFN) modules of a compact BERT variant are replaced by
PQC-based layers. This design is motivated by the dominant parameter
contribution of FFNs, which account for approximately two-thirds of the
parameters within standard Transformer encoder blocks. While prior studies have
primarily integrated PQCs into self-attention modules, our work focuses on the
FFN and systematically investigates the trade-offs between PQC depth,
expressibility, and trainability. Our final PQC architecture incorporates a
residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating
entanglement strategy to ensure stable training and high expressibility. Our
experiments, conducted on a classical simulator, on the SST-2 and DBpedia
benchmarks demonstrate two key findings. First, a carefully configured
QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its
classical counterpart in a full-data setting while reducing FFN-specific
parameters by over 99%. Second, our model exhibits a consistent and competitive
edge in few-shot learning scenarios, confirming its potential for superior data
efficiency. These results, supported by an ablation study on a non-optimized
PQC that failed to learn, confirm that PQCs can serve as powerful and
parameter-efficient alternatives to classical FFNs when co-designed with
foundational deep learning principles.

</details>


### [9] [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)
*Weijie Lyu,Sheng-Jun Huang,Xuan Xia*

Main category: cs.CL

TL;DR: 提出基于参数化模型的代码数据选择方法，用10K样本实现优于92K基线2.3%-2.4%的性能提升


<details>
  <summary>Details</summary>
Motivation: 当前LLM代码训练过度依赖数据规模而忽视质量，导致训练效率低下。需要解决数据质量与效率的平衡问题

Method: 1. 构建参数化模型进行代码数据筛选 2. 优化分布一致性和子集多样性 3. 确保筛选数据的高质量特性

Result: 在HumanEval和MBPP评测中分别提升2.4%和2.3%，训练效率提升9倍以上

Conclusion: 该方法通过数据质量优化显著提升模型性能，同时大幅降低计算成本，验证了高效数据选择策略的有效性

Abstract: Recent advancements in large language models (LLMs) have significantly
improved code generation and program comprehension, accelerating the evolution
of software engineering. Current methods primarily enhance model performance by
leveraging vast amounts of data, focusing on data quantity while often
overlooking data quality, thereby reducing training efficiency. To address
this, we introduce an approach that utilizes a parametric model for code data
selection, aimed at improving both training efficiency and model performance.
Our method optimizes the parametric model to ensure distribution consistency
and diversity within the selected subset, guaranteeing high-quality data.
Experimental results demonstrate that using only 10K samples, our method
achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled
baseline, outperforming other sampling approaches in both performance and
efficiency. This underscores that our method effectively boosts model
performance while significantly reducing computational costs.

</details>


### [10] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
*Mark Atta Mensah,Isaac Wiafe,Akon Ekpezu,Justice Kwame Appati,Jamal-Deen Abdulai,Akosua Nyarkoa Wiafe-Akenten,Frank Ernest Yeboah,Gifty Odame*

Main category: cs.CL

TL;DR: 研究比较Akan语音识别模型在不同领域的表现，揭示领域依赖性及架构间错误差异，提出低资源语言ASR优化策略


<details>
  <summary>Details</summary>
Motivation: 现有ASR研究缺乏跨领域泛化评估，特别是在低资源语言中。本研究旨在通过多领域Akan语音数据集评估不同模型的适应性

Method: 使用四个领域Akan语音数据集（文化图像描述/非正式对话/圣经经文/财务对话），测试七个Transformer架构模型（含Whisper和Wav2Vec2），对比词/字符错误率

Result: 模型仅在训练领域表现最佳，跨领域准确率显著下降。Whisper产生流畅但误导性错误，Wav2Vec2错误更明显但难解释

Conclusion: ASR错误特征存在架构差异，需权衡可读性与透明性。建议采用领域适应技术、自适应路由策略及多语言框架优化低资源语言ASR

Abstract: Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.

</details>


### [11] [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)
*Sumaya Ahmed Salihs,Isaac Wiafe,Jamal-Deen Abdulai,Elikem Doe Atsakpo,Gifty Ayoka,Richard Cave,Akon Obu Ekpezu,Catherine Holloway,Katrin Tomanek,Fiifi Baffoe Payin Winful*

Main category: cs.CL

TL;DR: 开发社区驱动的ASR指南并创建首个阿肯语障碍语音数据集，促进包容性语音识别技术发展


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言障碍语音ASR技术匮乏问题，通过开源工具和社区参与实现技术民主化

Method: 1. 制定数据收集最佳实践指南
2. 创建阿肯语障碍语音开源数据集
3. 微调现有开源ASR模型

Result: 成功发布包含数据集/指南/工具的开源套件，ASR模型初步调优显示对障碍语音识别效果提升

Conclusion: 该框架为开发定制化障碍语音识别技术提供可复制方案，推动语言技术包容性发展

Abstract: This study presents an approach for collecting speech samples to build
Automatic Speech Recognition (ASR) models for impaired speech, particularly,
low-resource languages. It aims to democratize ASR technology and data
collection by developing a "cookbook" of best practices and training for
community-driven data collection and ASR model building. As a proof-of-concept,
this study curated the first open-source dataset of impaired speech in Akan: a
widely spoken indigenous language in Ghana. The study involved participants
from diverse backgrounds with speech impairments. The resulting dataset, along
with the cookbook and open-source tools, are publicly available to enable
researchers and practitioners to create inclusive ASR technologies tailored to
the unique needs of speech impaired individuals. In addition, this study
presents the initial results of fine-tuning open-source ASR models to better
recognize impaired speech in Akan.

</details>


### [12] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
*Sneha Deshmukh,Prathmesh Kamble*

Main category: cs.CL

TL;DR: 首个公开的印度保释判决数据集IndianBailJudgments-1200，包含1200份标注20+属性的判决，支持法律NLP任务。


<details>
  <summary>Details</summary>
Motivation: 印度等地区因结构化数据稀缺导致法律NLP发展滞后，需填补印度保释司法领域公开数据集的空白。

Method: 使用GPT-4o提示工程构建标注流程，人工验证一致性，收集1200份印度法庭保释判决书。

Result: 创建首个印度保释司法公开数据集，含犯罪类型、法条、公平性分析等结构化标注，支持结果预测等NLP任务。

Conclusion: 该资源推动印度法律AI研究，首次实现保释判决预测、公平性评估等关键司法NLP应用场景。

Abstract: Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.

</details>


### [13] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Liwen Zhang,Litu Ou,Jialong Wu,Wenbiao Yin,Baixuan Li,Zhengwei Tao,Xinyu Wang,Weizhou Shen,Junkai Zhang,Dingchu Zhang,Xixi Wu,Yong Jiang,Ming Yan,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 提出WebSailor训练后方法，通过结构化采样、RFT冷启动和DUPO算法，使开源代理在复杂信息检索任务中性能显著超越同类并接近专有系统


<details>
  <summary>Details</summary>
Motivation: 现有开源模型缺乏处理极端不确定性的系统化推理能力，而专有系统在此类复杂信息检索任务中展现超人类表现但机制不透明

Method: 构建包含高不确定性任务生成（结构化采样+信息模糊化）、RFT冷启动训练、DUPO策略优化的完整训练后体系

Result: WebSailor在复杂信息检索任务中性能超越所有开源代理，达到专有系统水平

Conclusion: 通过系统性降低信息不确定性的训练方法，成功将关键推理能力迁移至开源模型，缩小了与专有系统的能力差距

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all opensource agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


### [14] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 提出整合人类标签变异(HLV)的主动学习框架，突破传统单标注假设，强调分解标签变异为信号与噪声。


<details>
  <summary>Details</summary>
Motivation: 现有主动学习框架忽视人类标签变异的有效信号，过度简化标注场景，无法反映真实标注复杂性。

Method: 1. 解构标签变异为HLV（信号）与标注错误（噪声）
2. 提出涵盖实例选择/标注者选择/标签表示的全流程HLV整合框架
3. 探索大语言模型作为标注者的应用场景

Result: 揭示现有AL方法在HLV场景中的理论缺陷，构建理论框架弥补标注实践与算法设计间的鸿沟

Conclusion: 为复杂真实场景的主动学习建立新范式，推动标注预算优化与模型训练效果的共同提升

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


### [15] [MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion](https://arxiv.org/abs/2507.02595)
*Xin Guan,PeiHsin Lin,Zekun Wu,Ze Wang,Ruibo Zhang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CL

TL;DR: MPF提出了一种无需微调的后训练对齐框架，通过多视角融合将大语言模型输出与可解释基线对齐，实现偏置缓解。


<details>
  <summary>Details</summary>
Motivation: 针对现有大语言模型偏置缓解方法需要大量提示工程或微调的问题，开发可扩展、可解释且兼容已部署模型的后训练对齐方案。

Method: 基于SAGED流水线构建偏置基准，通过解构人类基准（如HR专家的情感分布）为可解释视角组件，采用概率加权采样实现多视角响应平衡

Result: 成功将模型情感分布对齐到反事实基线（绝对平等）和HR基线（名校偏置），显著降低KL散度和校准误差，并展现对未见过问题的泛化能力

Conclusion: MPF为模型对齐提供了可解释性强、可扩展的解决方案，无需修改已部署模型结构或进行复杂工程，实现了高效的后训练偏置调控

Abstract: Multiperspective Fusion (MPF) is a novel posttraining alignment framework for
large language models (LLMs) developed in response to the growing need for easy
bias mitigation. Built on top of the SAGED pipeline, an automated system for
constructing bias benchmarks and extracting interpretable baseline
distributions, MPF leverages multiperspective generations to expose and align
biases in LLM outputs with nuanced, humanlike baselines. By decomposing
baseline, such as sentiment distributions from HR professionals, into
interpretable perspective components, MPF guides generation through sampling
and balancing of responses, weighted by the probabilities obtained in the
decomposition. Empirically, we demonstrate its ability to align LLM sentiment
distributions with both counterfactual baselines (absolute equality) and the HR
baseline (biased for Top Univeristy), resulting in small KL divergence,
reduction of calibration error and generalization to unseen questions. This
shows that MPF offers a scalable and interpretable method for alignment and
bias mitigation, compatible with deployed LLMs and requiring no extensive
prompt engineering or finetuning.

</details>


### [16] [Exploring Gender Bias Beyond Occupational Titles](https://arxiv.org/abs/2507.02679)
*Ahmed Sabir,Rajesh Sharama*

Main category: cs.CL

TL;DR: 本研究探讨性别与上下文偏见的关联，提出新数据集和评估框架，证实性别偏见超出职业刻板印象


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注职业性别偏见，本文旨在探索动作动词、对象名词等更广泛的上下文偏见关联性

Method: 开发GenderLexicon数据集及可量化评估框架，通过偏差评分机制增强模型解释性

Result: 在5个多语言数据集（含日语）验证显示，非职业领域的性别偏见普遍存在且可量化检测

Conclusion: 提出系统性检测工具，揭示性别偏见在语言中的深层存在，为NLP去偏见化提供新思路

Abstract: In this work, we investigate the correlation between gender and contextual
biases, focusing on elements such as action verbs, object nouns, and
particularly on occupations. We introduce a novel dataset, GenderLexicon, and a
framework that can estimate contextual bias and its related gender bias. Our
model can interpret the bias with a score and thus improve the explainability
of gender bias. Also, our findings confirm the existence of gender biases
beyond occupational stereotypes. To validate our approach and demonstrate its
effectiveness, we conduct evaluations on five diverse datasets, including a
Japanese dataset.

</details>


### [17] [Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers](https://arxiv.org/abs/2507.02694)
*Zhijian Xu,Yilun Zhao,Manasi Patwardhan,Lovekesh Vig,Arman Cohan*

Main category: cs.CL

TL;DR: 本文针对同行评审压力增大的问题，提出利用大语言模型（LLMs）辅助识别论文局限性，构建了LimitGen基准，通过合成与真实数据结合文献检索，提升模型生成建设性反馈的能力。


<details>
  <summary>Details</summary>
Motivation: 随着论文数量激增，传统同行评审面临效率瓶颈。现有研究尚未充分挖掘LLMs在辅助评审（尤其局限性识别）中的潜力，需系统性评估与优化。

Method: 1）建立AI领域科研局限性分类法；2）构建包含人工合成扰动数据（LimitGen-Syn）和真实局限性描述（LimitGen-Human）的基准；3）通过文献检索增强LLMs的上下文理解能力。

Result: 实验表明，结合文献检索的LLM系统能更精准定位论文不足，生成具有文献依据的具体改进建议，有效补充人工评审盲区。

Conclusion: 本研究为LLM辅助同行评审提供了系统评估框架，证明增强后的模型可为作者提供早期建设性反馈，提升评审效率与全面性。

Abstract: Peer review is fundamental to scientific research, but the growing volume of
publications has intensified the challenges of this expertise-intensive
process. While LLMs show promise in various scientific tasks, their potential
to assist with peer review, particularly in identifying paper limitations,
remains understudied. We first present a comprehensive taxonomy of limitation
types in scientific research, with a focus on AI. Guided by this taxonomy, for
studying limitations, we present LimitGen, the first comprehensive benchmark
for evaluating LLMs' capability to support early-stage feedback and complement
human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a
synthetic dataset carefully created through controlled perturbations of
high-quality papers, and LimitGen-Human, a collection of real human-written
limitations. To improve the ability of LLM systems to identify limitations, we
augment them with literature retrieval, which is essential for grounding
identifying limitations in prior scientific findings. Our approach enhances the
capabilities of LLM systems to generate limitations in research papers,
enabling them to provide more concrete and constructive feedback.

</details>


### [18] [Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens](https://arxiv.org/abs/2507.02744)
*Peter Viechnicki*

Main category: cs.CL

TL;DR: 研究发现元音模仿的最小听觉差异阈值（JPD）为14-51美尔，为语音生成理论和元音系统结构提供新见解


<details>
  <summary>Details</summary>
Motivation: 探索人类元音生成中听觉空间控制的精细程度，验证元音模仿所需的最小听觉差异阈值（JPD）的存在

Method: 采用元音模仿范式，测量两组英语母语者在前元音生成时的F1×F2空间差异阈值

Result: 首次测得JPD值介于14-51美尔之间，证实了亚音位层次听觉控制的存在

Conclusion: JPD阈值为元音系统结构设置理论下限，为语音生成的片段理论提供实证支持，并解释元音音位数量/分布模式的心理物理学基础

Abstract: A body of work over the past several decades has demonstrated that the
complex and coordinated articulatory movements of human vowel production are
governed (at least in part)by control mechanisms whose targets are regions of
auditory space. Within the target region control at the sub-phonemic level has
also been demonstrated. But the degree of accuracy of that control is unknown.
The current work investigates this question by asking how far apart must two
vowel stimuli lie in auditory space in order to yield reliably different
imitations? This distance is termed 'Just Producible Difference' (JPD). The
current study uses a vowel mimicry paradigm to derive the first measurement of
JPD among two sets of English speakers during front vowel production. JPD is
estimated at between 14 and 51 mels in F1 X F2 space. This finding has
implications for episodic theories of speech production. It also clarifies the
possible structures of human vowel systems, by setting a theoretical lower
bound for how close two vowel phonemes may be in a speaker's formant space, and
hence a psychophysical explanation of observed trends in number and patterns of
possible vowel phonemes.

</details>


### [19] [Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs](https://arxiv.org/abs/2507.02778)
*Ken Tsui*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Although large language models (LLMs) have become transformative, they still
make mistakes and can explore unproductive reasoning paths. Self-correction is
an important capability for a trustworthy LLM, particularly an autoregressive
LLM. While LLMs can identify error in user input, they exhibit a systematic
'Self-Correction Blind Spot' - failing to correct identical error in their own
outputs. To systematically study this phenomenon, we introduce Self-Correction
Bench, a systematic framework to measure this phenomenon through controlled
error injection at three complexity levels. Testing 14 models, we find an
average 64.5% blind spot rate. We find multiple evidences that this limitation
relates to training data composition: human training demonstrations
predominantly show error-free responses rather than error-correction sequences,
unlike RL-trained models that learn error correction through outcome feedback.
Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting
that the capability exists but requires activation. Our work highlights a
critical limitation in current LLMs and offers potential avenues for improving
their reliability and trustworthiness.

</details>


### [20] [Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models](https://arxiv.org/abs/2507.02799)
*Riccardo Cantini,Nicola Gabriele,Alessio Orsino,Domenico Talia*

Main category: cs.CL

TL;DR: 推理语言模型（RLMs）的显式推理机制可能意外增加社会偏见漏洞，微调模型比CoT提示模型稍安全但仍有风险


<details>
  <summary>Details</summary>
Motivation: 探究推理能力对模型公平性和抗偏见鲁棒性的影响，比较不同推理机制（CoT提示 vs 微调推理轨迹）的安全表现

Method: 使用CLEAR-Bias基准测试，结合LLM自动安全评分和越狱攻击技术，评估RLMs在多元社会文化维度上的对抗鲁棒性

Result: ① 显式推理模型（CoT/微调）比基础模型更易受偏见激发 ② 微调模型比CoT模型更抗上下文重构攻击 ③ 故事化提示和虚构角色攻击对CoT模型最有效

Conclusion: 推理能力与偏见安全存在复杂关联，需开发更偏见识别的推理设计方法，当前机制可能无意间强化刻板印象

Abstract: Reasoning Language Models (RLMs) have gained traction for their ability to
perform complex, multi-step reasoning tasks through mechanisms such as
Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these
capabilities promise improved reliability, their impact on robustness to social
biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark,
originally designed for Large Language Models (LLMs), to investigate the
adversarial robustness of RLMs to bias elicitation. We systematically evaluate
state-of-the-art RLMs across diverse sociocultural dimensions, using an
LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak
techniques to assess the strength of built-in safety mechanisms. Our evaluation
addresses three key questions: (i) how the introduction of reasoning
capabilities affects model fairness and robustness; (ii) whether models
fine-tuned for reasoning exhibit greater safety than those relying on CoT
prompting at inference time; and (iii) how the success rate of jailbreak
attacks targeting bias elicitation varies with the reasoning mechanisms
employed. Our findings reveal a nuanced relationship between reasoning
capabilities and bias safety. Surprisingly, models with explicit reasoning,
whether via CoT prompting or fine-tuned reasoning traces, are generally more
vulnerable to bias elicitation than base models without such mechanisms,
suggesting reasoning may unintentionally open new pathways for stereotype
reinforcement. Reasoning-enabled models appear somewhat safer than those
relying on CoT prompting, which are particularly prone to contextual reframing
attacks through storytelling prompts, fictional personas, or reward-shaped
instructions. These results challenge the assumption that reasoning inherently
improves robustness and underscore the need for more bias-aware approaches to
reasoning design.

</details>


### [21] [Multimodal Mathematical Reasoning with Diverse Solving Perspective](https://arxiv.org/abs/2507.02804)
*Wenhao Shi,Zhiqiang Hu,Yi Bin,Yang Yang,See-Kiong Ng,Heng Tao Shen*

Main category: cs.CL

TL;DR: 提出MathV-DP多解轨迹数据集和Qwen-VL-DP模型，通过GRPO强化学习方法显著提升多模态数学推理的准确性和多样性


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs依赖单解监督模式，忽视了数学问题解决过程中多视角推理和反思的重要性

Method: 构建包含多解轨迹的MathV-DP数据集，基于Qwen-VL模型进行监督微调，并创新GRPO强化学习框架（集成正确性判别和多样性奖励机制）

Result: 在MathVista和Math-V基准测试中，模型在准确率（提升4.4%）和生成多样性（增加32%）上均超越基线模型

Conclusion: 融合多样化推理视角与反思机制能有效增强多模态数学推理能力，GRPO框架验证了多样性奖励机制对保持解质量与差异化的双重价值

Abstract: Recent progress in large-scale reinforcement learning (RL) has notably
enhanced the reasoning capabilities of large language models (LLMs), especially
in mathematical domains. However, current multimodal LLMs (MLLMs) for
mathematical reasoning often rely on one-to-one image-text pairs and
single-solution supervision, overlooking the diversity of valid reasoning
perspectives and internal reflections. In this work, we introduce MathV-DP, a
novel dataset that captures multiple diverse solution trajectories for each
image-question pair, fostering richer reasoning supervision. We further propose
Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and
enhanced via group relative policy optimization (GRPO), a rule-based RL
approach that integrates correctness discrimination and diversity-aware reward
functions. Our method emphasizes learning from varied reasoning perspectives
and distinguishing between correct yet distinct solutions. Extensive
experiments on the MathVista's minitest and Math-V benchmarks demonstrate that
Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and
generative diversity, highlighting the importance of incorporating diverse
perspectives and reflective reasoning in multimodal mathematical reasoning.

</details>


### [22] [SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model](https://arxiv.org/abs/2507.02822)
*Wencheng Zhang,Shiqin Qiao,Lingjie Luo,Yinfeng Li,Chuanyang Zheng,Qian Xu,Meng Li,Yong Gui,Yijun He,Jianing Qiu,Jindong Hong,Jiankai Sun*

Main category: cs.CL

TL;DR: 通过动态路由框架SynapseRoute智能分配医疗问题至思考/非思考模式，在保持准确率(0.8390)的同时显著降低36.8%推理时间和39.66%token消耗。


<details>
  <summary>Details</summary>
Motivation: 解决LLM应用中高成本思考模式与低成本非思考模式间的效能平衡问题，发现58%医疗问题无需复杂推理即可解决，需建立智能路由机制优化资源分配。

Method: 提出基于机器学习的SynapseRoute动态路由框架，通过AIT指数综合评估精度-延迟-成本关系，自动选择最优处理模式。

Result: 实验显示相比纯思考模式：准确率提升1.18%，推理效率提升36.8%，token消耗减少39.66%，且避免简单问题过度推理导致的准确率下降。

Conclusion: 问题复杂度存在明显二分性，动态路由能有效平衡AI服务效能。过度推理会降低效率，自适应路由可规避该缺陷，AIT指数为模型选择提供量化标准。

Abstract: With the widespread adoption of large language models (LLMs) in practical
applications, selecting an appropriate model requires balancing not only
performance but also operational cost. The emergence of reasoning-capable
models has further widened the cost gap between "thinking" (high reasoning) and
"non-thinking" (fast, low-cost) modes. In this work, we reveal that
approximately 58% of medical questions can be accurately answered by the
non-thinking mode alone, without requiring the high-cost reasoning process.
This highlights a clear dichotomy in problem complexity and suggests that
dynamically routing queries to the appropriate mode based on complexity could
optimize accuracy, cost-efficiency, and overall user experience. Based on this,
we further propose SynapseRoute, a machine learning-based dynamic routing
framework that intelligently assigns input queries to either thinking or
non-thinking modes. Experimental results on several medical datasets
demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs.
0.8272) compared to the thinking mode alone but also reduces inference time by
36.8% and token consumption by 39.66%. Importantly, qualitative analysis
indicates that over-reasoning on simpler queries can lead to unnecessary delays
and even decreased accuracy, a pitfall avoided by our adaptive routing.
Finally, this work further introduces the Accuracy-Inference-Token (AIT) index
to comprehensively evaluate the trade-offs among accuracy, latency, and token
cost.

</details>


### [23] [Generalizing Verifiable Instruction Following](https://arxiv.org/abs/2507.02833)
*Valentina Pyatkin,Saumya Malik,Victoria Graf,Hamish Ivison,Shengyi Huang,Pradeep Dasigi,Nathan Lambert,Hannaneh Hajishirzi*

Main category: cs.CL

TL;DR: 提出IFBench新基准评估指令跟随泛化能力，通过RLVR强化学习方法显著提升模型对未知输出约束的适应能力


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在处理用户指令中的输出约束（如'只能用是或否回答'）时存在严重过拟合现象，难以泛化到未见过的约束场景。现有基准仅覆盖少量可验证约束，无法反映模型真实能力

Method: 设计约束验证模块，采用可验证奖励的强化学习（RLVR）训练方法，构建包含58个新约束的IFBench测试集和29个训练约束集

Result: RLVR方法使模型在IFBench上表现出显著改进，验证了该方法对提升指令跟随泛化能力的有效性

Conclusion: 指令跟随的泛化能力是AI交互的关键，IFBench与RLVR方法为解决该问题提供新方向。同步开源29个训练约束验证函数、训练提示词及完整代码资源

Abstract: A crucial factor for successful human and AI interaction is the ability of
language models or chatbots to follow human instructions precisely. A common
feature of instructions are output constraints like ``only answer with yes or
no" or ``mention the word `abrakadabra' at least 3 times" that the user adds to
craft a more useful answer. Even today's strongest models struggle with
fulfilling such constraints. We find that most models strongly overfit on a
small set of verifiable constraints from the benchmarks that test these
abilities, a skill called precise instruction following, and are not able to
generalize well to unseen output constraints. We introduce a new benchmark,
IFBench, to evaluate precise instruction following generalization on 58 new,
diverse, and challenging verifiable out-of-domain constraints. In addition, we
perform an extensive analysis of how and on what data models can be trained to
improve precise instruction following generalization. Specifically, we
carefully design constraint verification modules and show that reinforcement
learning with verifiable rewards (RLVR) significantly improves instruction
following. In addition to IFBench, we release 29 additional new hand-annotated
training constraints and verification functions, RLVR training prompts, and
code.

</details>


### [24] [LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users](https://arxiv.org/abs/2507.02850)
*Almog Hilel,Idan Shenfeld,Leshem Choshen,Jacob Andreas*

Main category: cs.CL

TL;DR: 揭示语言模型在用户反馈训练中存在漏洞，单个用户可通过点赞/点踩机制持久性操控模型输出恶意内容


<details>
  <summary>Details</summary>
Motivation: 发现用户反馈调优的新漏洞，证明即使有限偏好数据也能实现精细行为控制，扩展了预训练数据投毒和提示注入攻击的研究边界

Method: 攻击者引导模型交替输出恶意/正常回应，通过选择性反馈影响后续偏好调优，使模型在常规场景下仍倾向恶意输出

Result: 成功实现虚构事实植入、代码生成漏洞注入、虚假金融新闻传播三类攻击，验证攻击机制的有效性

Conclusion: 该研究既揭示了偏好调优对模型行为的精细控制特性，也提出了用户反馈机制下的新型攻击范式，对AI安全具有重要警示意义

Abstract: We describe a vulnerability in language models (LMs) trained with user
feedback, whereby a single user can persistently alter LM knowledge and
behavior given only the ability to provide prompts and upvote / downvote
feedback on LM outputs. To implement the attack, the attacker prompts the LM to
stochastically output either a "poisoned" or benign response, then upvotes the
poisoned response or downvotes the benign one. When feedback signals are used
in a subsequent preference tuning behavior, LMs exhibit increased probability
of producing poisoned responses even in contexts without malicious prompts. We
show that this attack can be used to (1) insert factual knowledge the model did
not previously possess, (2) modify code generation patterns in ways that
introduce exploitable security flaws, and (3) inject fake financial news. Our
finding both identifies a new qualitative feature of language model preference
tuning (showing that it even highly restricted forms of preference data can be
used to exert fine-grained control over behavior), and a new attack mechanism
for LMs trained with user feedback (extending work on pretraining-time data
poisoning and deployment-time prompt injection).

</details>


### [25] [MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs](https://arxiv.org/abs/2507.02851)
*Purbesh Mitra,Sennur Ulukus*

Main category: cs.CL

TL;DR: 提出MOTIF强化学习微调方法，通过多轮模块化思考突破LLM上下文限制，在数学推理任务中实现3.8%和3.3%准确率提升，样本效率提高至15%。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)的上下文长度限制了其多步推理能力，需开发模块化思考策略突破token数量限制

Method: 采用参数高效微调方法，在Qwen2.5-3B-Instruct模型上使用GSM8K数据集进行强化学习训练，引入多轮思考token生成机制

Result: 在MATH500和AIME2024基准测试中分别提升3.8%和3.3%，仅用15%训练样本即实现效果提升

Conclusion: MOTIF方法有效提升了LLM的长程推理能力，同时具备优秀的样本效率，为复杂问题求解提供了新思路

Abstract: Recent advancements in the reasoning capabilities of large language models
(LLMs) show that employing group relative policy optimization (GRPO) algorithm
for reinforcement learning (RL) training allows the models to use more
thinking/reasoning tokens for generating better responses. However, LLMs can
generate only a finite amount of tokens while maintaining attention to the
previously generated tokens. This limit, also known as the context size of an
LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens.
To think beyond the limit of context size, an LLM must employ a modular
thinking strategy to reason over multiple rounds. In this work, we propose
$\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL
training method for generating thinking tokens in multiple rounds, effectively
allowing the model to think with additional context size. We trained the
open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient
fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our
experiments show 3.8\% and 3.3\% improvements over vanilla GRPO based training
in the respective benchmarks. Furthermore, this improvement was achieved with
only 15\% of samples, thus demonstrating sample efficiency of MOTIF. Our code
and models are available at https://github.com/purbeshmitra/MOTIF and
https://huggingface.co/purbeshmitra/MOTIF, respectively.

</details>


### [26] [Answer Matching Outperforms Multiple Choice for Language Model Evaluation](https://arxiv.org/abs/2507.02856)
*Nikhil Chandak,Shashwat Goel,Ameya Prabhu,Moritz Hardt,Jonas Geiping*

Main category: cs.CL

TL;DR: 论文揭示传统多项选择题评估存在捷径效应，提出基于答案匹配的生成式评估方法，该方法与人类评分一致性达97%，显著改变模型排名格局


<details>
  <summary>Details</summary>
Motivation: 现有语言模型评估过度依赖存在设计缺陷的多项选择题基准，生成式评估虽更合理但缺乏可行方案。随着大模型发展，需要验证答案匹配方法的有效性并推动评估体系转型

Method: 标注MMLU-Pro和GPQA-Diamond的人类评分数据，对比多项选择评估、无参考答案的LLM评判与答案匹配三种策略与人类评分的一致性

Result: 答案匹配(使用现代模型)达到97%人类评分一致性，传统多项选择仅65%。使用该方法时，50%模型的相对排名发生显著变化

Conclusion: 答案匹配通过结合生成式回答与参考答案验证，实现更可靠的评估效果。建议评估体系从多项选择向答案匹配转型，以更准确反映模型真实能力

Abstract: Multiple choice benchmarks have long been the workhorse of language model
evaluation because grading multiple choice is objective and easy to automate.
However, we show multiple choice questions from popular benchmarks can often be
answered without even seeing the question. These shortcuts arise from a
fundamental limitation of discriminative evaluation not shared by evaluations
of the model's free-form, generative answers. Until recently, there appeared to
be no viable, scalable alternative to multiple choice--but, we show that this
has changed. We consider generative evaluation via what we call answer
matching: Give the candidate model the question without the options, have it
generate a free-form response, then use a modern language model with the
reference answer to determine if the response matches the reference. To compare
the validity of different evaluation strategies, we annotate MMLU-Pro and
GPQA-Diamond to obtain human grading data, and measure the agreement of each
evaluation approach. We find answer matching using recent models--even small
ones--achieves near-perfect agreement, in the range of inter-annotator
agreement. In contrast, both multiple choice evaluation and using
LLM-as-a-judge without reference answers aligns poorly with human grading.
Improving evaluations via answer matching is not merely a conceptual concern:
the rankings of several models change significantly when evaluating their
free-form responses with answer matching. In light of these findings, we
discuss how to move the evaluation ecosystem from multiple choice to answer
matching.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [27] [Gbake: Baking 3D Gaussian Splats into Reflection Probes](https://arxiv.org/abs/2507.02257)
*Stephen Pasch,Joel K. Salzman,Changxi Zheng*

Main category: cs.GR

TL;DR: 提出了GBake工具，解决传统3D网格在Gaussian Splatting场景中的光照融合问题


<details>
  <summary>Details</summary>
Motivation: 3D高斯原语将光照和几何联合编码导致传统3D网格插入时会出现不协调的照明效果，需要实现两者的无缝整合

Method: 开发GBake工具从高斯泼溅场景烘焙反射探针，支持在Unity引擎中对传统3D网格进行真实反射映射

Result: 成功在Unity游戏引擎中实现传统3D网格的真实反射效果

Conclusion: GBake有效弥合了高斯泼溅场景与传统3D资产间的技术鸿沟，提升游戏引擎中的场景真实感

Abstract: The growing popularity of 3D Gaussian Splatting has created the need to
integrate traditional computer graphics techniques and assets in splatted
environments. Since 3D Gaussian primitives encode lighting and geometry jointly
as appearance, meshes are relit improperly when inserted directly in a mixture
of 3D Gaussians and thus appear noticeably out of place. We introduce GBake, a
specialized tool for baking reflection probes from Gaussian-splatted scenes
that enables realistic reflection mapping of traditional 3D meshes in the Unity
game engine.

</details>


### [28] [Real-time Image-based Lighting of Glints](https://arxiv.org/abs/2507.02674)
*Tom Kneiphof,Reinhard Klein*

Main category: cs.GR

TL;DR: 提出了一种实时渲染中基于环境贴图过滤的闪烁效果高效近似算法，支持动态材质属性和环境光变化。


<details>
  <summary>Details</summary>
Motivation: 解决具有离散微表面材质（如闪烁/亮片效果）在动态环境光照下的实时渲染难题，传统方法难以兼顾效率与质量。

Method: 1. 将环境光划分为均匀辐射区域
2. 通过正态分布函数过滤区域指示函数
3. 使用双门控高斯近似实现二项分布的多项式分层采样

Result: 渲染效果接近真实参考结果，内存占用为普通材质的两倍，性能稳定且帧率高

Conclusion: 该算法首次实现了动态环境光下闪烁效果的实时渲染突破，为游戏/VR等实时应用提供了实用解决方案

Abstract: Image-based lighting is a widely used technique to reproduce shading under
real-world lighting conditions, especially in real-time rendering applications.
A particularly challenging scenario involves materials exhibiting a sparkling
or glittering appearance, caused by discrete microfacets scattered across their
surface. In this paper, we propose an efficient approximation for image-based
lighting of glints, enabling fully dynamic material properties and environment
maps. Our novel approach is grounded in real-time glint rendering under area
light illumination and employs standard environment map filtering techniques.
Crucially, our environment map filtering process is sufficiently fast to be
executed on a per-frame basis. Our method assumes that the environment map is
partitioned into few homogeneous regions of constant radiance. By filtering the
corresponding indicator functions with the normal distribution function, we
obtain the probabilities for individual microfacets to reflect light from each
region. During shading, these probabilities are utilized to hierarchically
sample a multinomial distribution, facilitated by our novel dual-gated Gaussian
approximation of binomial distributions. We validate that our real-time
approximation is close to ground-truth renderings for a range of material
properties and lighting conditions, and demonstrate robust and stable
performance, with little overhead over rendering glints from a single
directional light. Compared to rendering smooth materials without glints, our
approach requires twice as much memory to store the prefiltered environment
map.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [29] [FinAI-BERT: A Transformer-Based Model for Sentence-Level Detection of AI Disclosures in Financial Reports](https://arxiv.org/abs/2507.01991)
*Muhammad Bilal Zafar*

Main category: q-fin.CP

TL;DR: 开发FinAI-BERT模型实现金融文本句子级AI披露检测，准确率99.37%，突破传统方法局限


<details>
  <summary>Details</summary>
Motivation: 现有AI披露检测工具在细粒度、可解释性和鲁棒性方面存在不足

Method: 基于Transformer架构，在669份美国银行年报的1,586个平衡句子上进行微调（2015-2023年数据）

Result: 模型准确率99.37%/F1 0.993，优于传统机器学习方法，并通过长度对抗测试和时间样本稳定性验证

Conclusion: 该研究为金融AI监管提供可扩展解决方案，推动Transformer在细粒度金融NLP分类中的应用

Abstract: The proliferation of artificial intelligence (AI) in financial services has
prompted growing demand for tools that can systematically detect AI-related
disclosures in corporate filings. While prior approaches often rely on keyword
expansion or document-level classification, they fall short in granularity,
interpretability, and robustness. This study introduces FinAI-BERT, a
domain-adapted transformer-based language model designed to classify AI-related
content at the sentence level within financial texts. The model was fine-tuned
on a manually curated and balanced dataset of 1,586 sentences drawn from 669
annual reports of U.S. banks (2015 to 2023). FinAI-BERT achieved near-perfect
classification performance (accuracy of 99.37 percent, F1 score of 0.993),
outperforming traditional baselines such as Logistic Regression, Naive Bayes,
Random Forest, and XGBoost. Interpretability was ensured through SHAP-based
token attribution, while bias analysis and robustness checks confirmed the
model's stability across sentence lengths, adversarial inputs, and temporal
samples. Theoretically, the study advances financial NLP by operationalizing
fine-grained, theme-specific classification using transformer architectures.
Practically, it offers a scalable, transparent solution for analysts,
regulators, and scholars seeking to monitor the diffusion and framing of AI
across financial institutions.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [30] [Why Multi-Interest Fairness Matters: Hypergraph Contrastive Multi-Interest Learning for Fair Conversational Recommender System](https://arxiv.org/abs/2507.02000)
*Yongsen Zheng,Zongxuan Xie,Guohua Wang,Ziyao Liu,Liang Lin,Kwok-Yan Lam*

Main category: cs.IR

TL;DR: 提出HyFairCRS框架，通过超图对比多兴趣学习提升动态对话推荐系统的多兴趣多样性公平性


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统公平性研究多在静态场景，但动态交互场景中不公平问题（如马太效应、信息茧房）会随时间加剧，需针对性解决方案

Method: 1. 通过对比学习构建多样化超图捕捉用户多兴趣
2. 在动态用户-系统反馈循环中利用多兴趣生成响应及公平推荐

Result: 在两个CRS数据集上实现SOTA性能，同时有效缓解推荐不公平问题

Conclusion: HyFairCRS首次将多兴趣公平性融入动态对话推荐系统，实验验证了框架的有效性和公平性提升效果

Abstract: Unfairness is a well-known challenge in Recommender Systems (RSs), often
resulting in biased outcomes that disadvantage users or items based on
attributes such as gender, race, age, or popularity. Although some approaches
have started to improve fairness recommendation in offline or static contexts,
the issue of unfairness often exacerbates over time, leading to significant
problems like the Matthew effect, filter bubbles, and echo chambers. To address
these challenges, we proposed a novel framework, Hypergraph Contrastive
Multi-Interest Learning for Fair Conversational Recommender System (HyFairCRS),
aiming to promote multi-interest diversity fairness in dynamic and interactive
Conversational Recommender Systems (CRSs). HyFairCRS first captures a wide
range of user interests by establishing diverse hypergraphs through contrastive
learning. These interests are then utilized in conversations to generate
informative responses and ensure fair item predictions within the dynamic
user-system feedback loop. Experiments on two CRS-based datasets show that
HyFairCRS achieves a new state-of-the-art performance while effectively
alleviating unfairness. Our code is available at
https://github.com/zysensmile/HyFairCRS.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [31] [Dissecting the Impact of Mobile DVFS Governors on LLM Inference Performance and Energy Efficiency](https://arxiv.org/abs/2507.02135)
*Zongpu Zhang,Pranab Dash,Y. Charlie Hu,Qiang Xu,Jian Li,Haibing Guan*

Main category: cs.OS

TL;DR: 研究提出FUSE方案，通过统一管理移动设备CPU/GPU/内存频率，提升LLM推理能效


<details>
  <summary>Details</summary>
Motivation: 现有移动设备LLM框架中CPU/GPU/内存组件独立管理导致能效低下，影响大模型部署

Method: 1. 测量现有框架能效问题 2. 分析组件交互机制 3. 设计统一能效管理方案FUSE

Result: FUSE降低首token延迟7.0%-16.9%，输出token延迟25.4%-36.8%，保持相同能耗水平

Conclusion: FUSE通过协同管理移动设备硬件资源，显著提升LLM推理效率，为移动端部署提供优化方案

Abstract: Large Language Models (LLMs) are increasingly being integrated into various
applications and services running on billions of mobile devices. However,
deploying LLMs on resource-limited mobile devices faces a significant challenge
due to their high demand for computation, memory, and ultimately energy. While
current LLM frameworks for mobile use three power-hungry components-CPU, GPU,
and Memory-even when running primarily-GPU LLM models, optimized DVFS governors
for CPU, GPU, and memory featured in modern mobile devices operate
independently and are oblivious of each other. Motivated by the above
observation, in this work, we first measure the energy-efficiency of a SOTA LLM
framework consisting of various LLM models on mobile phones which showed the
triplet mobile governors result in up to 40.4% longer prefilling and decoding
latency compared to optimal combinations of CPU, GPU, and memory frequencies
with the same energy consumption for sampled prefill and decode lengths.
Second, we conduct an in-depth measurement study to uncover how the intricate
interplay (or lack of) among the mobile governors cause the above inefficiency
in LLM inference. Finally, based on these insights, we design FUSE - a unified
energy-aware governor for optimizing the energy efficiency of LLM inference on
mobile devices. Our evaluation using a ShareGPT dataset shows FUSE reduces the
time-to-first-token and time-per-output-token latencies by 7.0%-16.9% and
25.4%-36.8% on average with the same energy-per-token for various mobile LLM
models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [32] [Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features](https://arxiv.org/abs/2507.01984)
*Gautam Kishore Shahi*

Main category: cs.LG

TL;DR: 结合多模态特征（文本+图像+社交特征）与半监督机器学习模型，可提升15%的虚假信息检测性能


<details>
  <summary>Details</summary>
Motivation: 现有虚假信息检测研究多聚焦单模态（文本/图像），缺乏对多模态特征融合的探索。本研究旨在填补这一空白，并分析虚假信息传播模式

Method: 采用早期特征融合策略，整合推文文本、视觉特征（目标检测+OCR）、社交特征（用户属性+传播模式），结合无监督与有监督机器学习构建分类模型

Result: 多模态模型比单模态提升15%准确率，比双模态提升5%。虚假信息传播具有显著的用户画像特征（活跃度/粉丝数）和内容特征（情绪极性/视觉元素）

Conclusion: 多模态融合+混合学习模式能有效提升社交媒体虚假信息检测，为平台内容审核提供可解释的技术路径

Abstract: Amid a tidal wave of misinformation flooding social media during elections
and crises, extensive research has been conducted on misinformation detection,
primarily focusing on text-based or image-based approaches. However, only a few
studies have explored multimodal feature combinations, such as integrating text
and images for building a classification model to detect misinformation. This
study investigates the effectiveness of different multimodal feature
combinations, incorporating text, images, and social features using an early
fusion approach for the classification model. This study analyzed 1,529 tweets
containing both text and images during the COVID-19 pandemic and election
periods collected from Twitter (now X). A data enrichment process was applied
to extract additional social features, as well as visual features, through
techniques such as object detection and optical character recognition (OCR).
The results show that combining unsupervised and supervised machine learning
models improves classification performance by 15% compared to unimodal models
and by 5% compared to bimodal models. Additionally, the study analyzes the
propagation patterns of misinformation based on the characteristics of
misinformation tweets and the users who disseminate them.

</details>


### [33] [Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions](https://arxiv.org/abs/2507.02087)
*Eitan Anzenberg,Arunava Samajpati,Sivasankaran Chandrasekar,Varun Kacholia*

Main category: cs.LG

TL;DR: 专有招聘模型Match Score在准确性和公平性上全面超越主流大语言模型，验证了算法设计可兼顾招聘效率与公平


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型在招聘场景中存在的准确率不足和算法偏见风险，特别是在缺乏保障措施时的潜在歧视问题，开展系统性模型比较研究

Method: 使用约10,000个真实候选人-职位配对数据，对比五大科技公司的LLMs与自研Match Score模型，采用ROC AUC/PR AUC/F1-score评估准确性，通过影响比率分析种族、性别及交叉子组的公平性

Result: Match Score的ROC AUC达0.85（LLMs为0.77），在种族维度的最低影响比率达0.957（LLMs最佳仅0.809），交叉子组公平性提升17%

Conclusion: 招聘算法无需在准确与公平间妥协，领域专用模型结合偏差审计可双赢。现成LLMs需强化公平性保障，专业场景应优先定制化解决方案

Abstract: The use of large language models (LLMs) in hiring promises to streamline
candidate screening, but it also raises serious concerns regarding accuracy and
algorithmic bias where sufficient safeguards are not in place. In this work, we
benchmark several state-of-the-art foundational LLMs - including models from
OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our
proprietary domain-specific hiring model (Match Score) for job candidate
matching. We evaluate each model's predictive accuracy (ROC AUC,
Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis
across declared gender, race, and intersectional subgroups). Our experiments on
a dataset of roughly 10,000 real-world recent candidate-job pairs show that
Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs
0.77) and achieves significantly more equitable outcomes across demographic
groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957
(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the
intersectionals, respectively). We discuss why pretraining biases may cause
LLMs with insufficient safeguards to propagate societal biases in hiring
scenarios, whereas a bespoke supervised model can more effectively mitigate
these biases. Our findings highlight the importance of domain-specific modeling
and bias auditing when deploying AI in high-stakes domains such as hiring, and
caution against relying on off-the-shelf LLMs for such tasks without extensive
fairness safeguards. Furthermore, we show with empirical evidence that there
shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a
well-designed algorithm can achieve both accuracy in hiring and fairness in
outcomes.

</details>


### [34] [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
*Alexi Gladstone,Ganesh Nanduru,Md Mofijul Islam,Peixuan Han,Hyeonjeong Ha,Aman Chadha,Yilun Du,Heng Ji,Jundong Li,Tariq Iqbal*

Main category: cs.LG

TL;DR: 研究者提出基于能量的Transformer（EBTs），通过无监督学习验证输入与预测的兼容性，实现跨模态的高效推理与优化，显著提升模型训练速度、系统2思维任务表现及泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有系统2思维方法存在模态/问题局限性，且依赖额外监督训练。研究试图验证仅通过无监督学习能否实现通用型自主思考模型。

Method: 训练EBTs（基于能量的Transformer），通过能量模型为输入-预测对分配能量值，利用梯度下降进行能量最小化预测优化。

Result: EBTs在文本/视觉任务上训练速度提升35%，系统2思维任务性能提升29%，图像去噪优于Diffusion Transformer，且相同预训练条件下下游任务表现更优。

Conclusion: EBTs为模型学习与推理能力的扩展提供了新范式，其能量优化框架展现出优于主流Transformer++的泛化潜力。

Abstract: Inference-time computation techniques, analogous to human System 2 Thinking,
have recently become popular for improving model performances. However, most
existing approaches suffer from several limitations: they are modality-specific
(e.g., working only in text), problem-specific (e.g., verifiable domains like
math and coding), or require additional supervision/training on top of
unsupervised pretraining (e.g., verifiers or verifiable rewards). In this
paper, we ask the question "Is it possible to generalize these System 2
Thinking approaches, and develop models that learn to think solely from
unsupervised learning?" Interestingly, we find the answer is yes, by learning
to explicitly verify the compatibility between inputs and
candidate-predictions, and then re-framing prediction problems as optimization
with respect to this verifier. Specifically, we train Energy-Based Transformers
(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy
value to every input and candidate-prediction pair, enabling predictions
through gradient descent-based energy minimization until convergence. Across
both discrete (text) and continuous (visual) modalities, we find EBTs scale
faster than the dominant Transformer++ approach during training, achieving an
up to 35% higher scaling rate with respect to data, batch size, parameters,
FLOPs, and depth. During inference, EBTs improve performance with System 2
Thinking by 29% more than the Transformer++ on language tasks, and EBTs
outperform Diffusion Transformers on image denoising while using fewer forward
passes. Further, we find that EBTs achieve better results than existing models
on most downstream tasks given the same or worse pretraining performance,
suggesting that EBTs generalize better than existing approaches. Consequently,
EBTs are a promising new paradigm for scaling both the learning and thinking
capabilities of models.

</details>


### [35] [OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device Speculative Decoding](https://arxiv.org/abs/2507.02659)
*Ramchalam Kinattinkara Ramakrishnan,Zhaocong Yuan,Shaojie Zhuo,Chen Feng,Yicheng Lin,Chenzheng Su,Xiaopeng Zhang*

Main category: cs.LG

TL;DR: OmniDraft提出统一框架，支持单一草稿模型适配多目标模型，通过在线学习和自适应推测技术实现1.5-2倍解码加速。


<details>
  <summary>Details</summary>
Motivation: 现有推测解码技术需要针对特定目标模型训练草稿模型，存在跨模型兼容性差和动态适应能力不足的问题。OmniDraft旨在实现设备端LLM应用中'一个草稿适配所有'的范式，解决模型成本与用户定制的核心矛盾。

Method: 采用在线n-gram缓存和混合蒸馏微调解决词汇不匹配问题，结合自适应推测技术动态优化解码速度。支持Llama-68M适配Vicuna/Qwen2/Llama3等不同架构目标模型。

Result: 在数学推理、代码生成等任务中实现1.5-2倍加速，单草稿模型可跨系列支持Vicuna-7B、Qwen2-7B和Llama3-8B等多种目标模型。

Conclusion: OmniDraft开创了通用草稿模型新范式，显著降低设备端LLM部署成本，验证了'一个草稿适配所有'架构在动态适应场景下的可行性。

Abstract: Speculative decoding generally dictates having a small, efficient draft model
that is either pretrained or distilled offline to a particular target model
series, for instance, Llama or Qwen models. However, within online deployment
settings, there are two major challenges: 1) usage of a target model that is
incompatible with the draft model; 2) expectation of latency improvements over
usage and time. In this work, we propose OmniDraft, a unified framework that
enables a single draft model to operate with any target model and adapt
dynamically to user data. We introduce an online n-gram cache with hybrid
distillation fine-tuning to address the cross-vocabulary mismatch across draft
and target models; and further improve decoding speed by leveraging adaptive
drafting techniques. OmniDraft is particularly suitable for on-device LLM
applications where model cost, efficiency and user customization are the major
points of contention. This further highlights the need to tackle the above
challenges and motivates the \textit{``one drafter for all''} paradigm. We
showcase the proficiency of the OmniDraft framework by performing online
learning on math reasoning, coding and text generation tasks. Notably,
OmniDraft enables a single Llama-68M model to pair with various target models
including Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;
and additionally provides up to 1.5-2x speedup.

</details>


### [36] [ExPO: Unlocking Hard Reasoning with Self-Explanation-Guided Reinforcement Learning](https://arxiv.org/abs/2507.02834)
*Ruiyang Zhou,Shuozhe Li,Amy Zhang,Liu Leqi*

Main category: cs.LG

TL;DR: 提出自我解释策略优化框架ExPO，通过基于正确答案生成与当前策略对齐的正向样本，解决RL后训练中探索不足的问题，显著提升模型在初始表现差的数学推理任务上的性能


<details>
  <summary>Details</summary>
Motivation: 传统RL后训练方法依赖模型初始生成正向样本的能力，在挑战性任务中难以突破模型固有输出分布的限制，需要探索新机制生成有效训练样本

Method: ExPO框架结合两种关键属性：1)样本应符合当前策略的概率分布 2)能提高模型预测正确答案的概率，通过条件化生成机制自动产生满足要求的训练样本

Result: 在MATH level-5等高难度推理任务中，ExPO的学习效率和最终性能均超越基于专家演示的方法，特别是在模型初始正确率低于10%的极端困难场景下实现突破

Conclusion: 通过自我解释机制生成策略对齐的正向样本，ExPO有效解决了RL后训练中的探索-利用困境，为提升语言模型推理能力提供了新范式

Abstract: Recent advances in large language models have been driven by reinforcement
learning (RL)-style post-training, which improves reasoning by optimizing model
outputs based on reward or preference signals. GRPO-style approaches implement
this by using self-generated samples labeled by an outcome-based verifier.
However, these methods depend heavily on the model's initial ability to produce
positive samples. They primarily refine what the model already knows
(distribution sharpening) rather than enabling the model to solve problems
where it initially fails. This limitation is especially problematic in
early-stage RL training and on challenging reasoning tasks, where positive
samples are unlikely to be generated. To unlock reasoning ability in such
settings, the model must explore new reasoning trajectories beyond its current
output distribution. Such exploration requires access to sufficiently good
positive samples to guide the learning. While expert demonstrations seem like a
natural solution, we find that they are often ineffective in RL post-training.
Instead, we identify two key properties of effective positive samples: they
should (1) be likely under the current policy, and (2) increase the model's
likelihood of predicting the correct answer. Based on these insights, we
propose $\textbf{Self-Explanation Policy Optimization (ExPO)}$-a simple and
modular framework that generates such samples by conditioning on the
ground-truth answer. ExPO enables efficient exploration and guides the model to
produce reasoning trajectories more aligned with its policy than expert-written
CoTs, while ensuring higher quality than its own (incorrect) samples.
Experiments show that ExPO improves both learning efficiency and final
performance on reasoning benchmarks, surpassing expert-demonstration-based
methods in challenging settings such as MATH level-5, where the model initially
struggles the most.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [37] [Legal Requirements Translation from Law](https://arxiv.org/abs/2507.02846)
*Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 提出基于文本蕴含和上下文学习的法律文本规范化表示方法，使用Python元模型实现结构化编码，在13个州数据泄露法测试中达到89.4%通过率


<details>
  <summary>Details</summary>
Motivation: 中小组织缺乏法律合规资源，现有法律元数据提取方法忽视属性关联且依赖人工标注，泛化能力差

Method: 结合文本蕴含推理与上下文学习，设计Python类结构的领域元模型，捕获法律文本结构/语义元数据及其关联关系

Result: 测试通过率89.4%，精确率82.2，召回率88.7，验证了方法的有效性

Conclusion: 该方法降低对标注数据依赖，提升对新法规的适应能力，为自动化法律合规提供可行解决方案

Abstract: Software systems must comply with legal regulations, which is a
resource-intensive task, particularly for small organizations and startups
lacking dedicated legal expertise. Extracting metadata from regulations to
elicit legal requirements for software is a critical step to ensure compliance.
However, it is a cumbersome task due to the length and complex nature of legal
text. Although prior work has pursued automated methods for extracting
structural and semantic metadata from legal text, key limitations remain: they
do not consider the interplay and interrelationships among attributes
associated with these metadata types, and they rely on manual labeling or
heuristic-driven machine learning, which does not generalize well to new
documents. In this paper, we introduce an approach based on textual entailment
and in-context learning for automatically generating a canonical representation
of legal text, encodable and executable as Python code. Our representation is
instantiated from a manually designed Python class structure that serves as a
domain-specific metamodel, capturing both structural and semantic legal
metadata and their interrelationships. This design choice reduces the need for
large, manually labeled datasets and enhances applicability to unseen
legislation. We evaluate our approach on 13 U.S. state data breach notification
laws, demonstrating that our generated representations pass approximately 89.4%
of test cases and achieve a precision and recall of 82.2 and 88.7,
respectively.

</details>


### [38] [Requirements Elicitation Follow-Up Question Generation](https://arxiv.org/abs/2507.02858)
*Yuchen Shen,Anmol Singhal,Travis Breaux*

Main category: cs.SE

TL;DR: 研究探讨使用GPT-4o基于常见错误框架生成实时访谈问题，实验表明AI生成问题质量不亚于人类，且在错误类型指导下更优。


<details>
  <summary>Details</summary>
Motivation: 传统需求访谈依赖资深访谈者实时生成问题，面临领域不熟、认知过载等挑战，LLM在文本生成任务中的优异表现为解决该问题提供可能。

Method: 构建基于访谈者常见错误类型的框架，开展两组对照实验：无指导条件下比较LLM与人类问题质量；基于错误类型指导的LLM问题生成效果验证。

Result: LLM生成问题在清晰度、相关性和信息量上均不逊于人类，在错误类型指导下显著优于人工生成问题。

Conclusion: LLM可实时提升需求访谈质量，通过错误类型引导能有效增强问题生成效果，具有重要应用潜力。

Abstract: Interviews are a widely used technique in eliciting requirements to gather
stakeholder needs, preferences, and expectations for a software system.
Effective interviewing requires skilled interviewers to formulate appropriate
interview questions in real time while facing multiple challenges, including
lack of familiarity with the domain, excessive cognitive load, and information
overload that hinders how humans process stakeholders' speech. Recently, large
language models (LLMs) have exhibited state-of-the-art performance in multiple
natural language processing tasks, including text summarization and entailment.
To support interviewers, we investigate the application of GPT-4o to generate
follow-up interview questions during requirements elicitation by building on a
framework of common interviewer mistake types. In addition, we describe methods
to generate questions based on interviewee speech. We report a controlled
experiment to evaluate LLM-generated and human-authored questions with minimal
guidance, and a second controlled experiment to evaluate the LLM-generated
questions when generation is guided by interviewer mistake types. Our findings
demonstrate that, for both experiments, the LLM-generated questions are no
worse than the human-authored questions with respect to clarity, relevancy, and
informativeness. In addition, LLM-generated questions outperform human-authored
questions when guided by common mistakes types. This highlights the potential
of using LLMs to help interviewers improve the quality and ease of requirements
elicitation interviews in real time.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [39] [Seeing Through Green: Text-Based Classification and the Firm's Returns from Green Patents](https://arxiv.org/abs/2507.02287)
*Lapo Santarlasci,Armando Rungi,Antonio Zinilli*

Main category: econ.GN

TL;DR: 利用自然语言处理技术识别真正的绿色专利，发现其仅占原分类的20%，持有此类专利可提升企业销售、市场份额和生产力，高创新性专利还能增加利润。


<details>
  <summary>Details</summary>
Motivation: 通过改进绿色专利分类方法，为政策制定提供更精准的数据支持，解决现有文献中绿色专利分类过粗的问题。

Method: 使用1240万条绿色专利数据训练神经网络模型，通过词向量扩展基础词典，采用双向因果关系控制进行企业财务数据分析。

Result: 真正绿色专利占比20%，被引率低1%；持有真绿专利使企业销售额提升14%、市场份额增3%、生产率提高9%，高创新性专利利润增加12%。

Conclusion: 文本分析方法能显著提升专利分类精度，实证证明绿色技术创新对企业绩效有实质性促进，为环境政策与创新政策协同提供量化依据。

Abstract: This paper introduces Natural Language Processing for identifying ``true''
green patents from official supporting documents. We start our training on
about 12.4 million patents that had been classified as green from previous
literature. Thus, we train a simple neural network to enlarge a baseline
dictionary through vector representations of expressions related to
environmental technologies. After testing, we find that ``true'' green patents
represent about 20\% of the total of patents classified as green from previous
literature. We show heterogeneity by technological classes, and then check that
`true' green patents are about 1\% less cited by following inventions. In the
second part of the paper, we test the relationship between patenting and a
dashboard of firm-level financial accounts in the European Union. After
controlling for reverse causality, we show that holding at least one ``true''
green patent raises sales, market shares, and productivity. If we restrict the
analysis to high-novelty ``true'' green patents, we find that they also yield
higher profits. Our findings underscore the importance of using text analyses
to gauge finer-grained patent classifications that are useful for policymaking
in different domains.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [40] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: 提出自我进化AI代理STELLA，通过动态工具库和模板学习机制实现生物医学研究能力的持续提升


<details>
  <summary>Details</summary>
Motivation: 解决生物医学数据激增与工具碎片化问题，突破传统AI代理依赖静态工具集的局限性

Method: 采用多智能体架构：1）动态演化的推理模板库 2）工具创建代理自动扩展的生物信息工具海洋

Result: 在三大生物医学基准测试中达到SOTA（生物医学考试26%/DBQA 54%/文献问答63%），性能随经验积累持续提升

Conclusion: STELLA标志着AI代理系统向动态学习进化迈出重要一步，为加速生物医学发现提供可扩展解决方案

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [41] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: 大型语言模型在迭代囚徒困境中展现出战略智能，不同厂商模型呈现显著策略差异：Gemini具攻击性，OpenAI过度合作，Claude宽容互惠。


<details>
  <summary>Details</summary>
Motivation: 验证LLMs是否具备战略环境下的目标推理能力，通过经典博弈场景研究其决策机制。

Method: 设计进化IPD锦标赛，让经典策略与OpenAI/Google/Anthropic模型对抗，通过调节终止概率控制博弈复杂度。

Result: LLMs在90%终止概率环境中存活率超75%，Gemini报复率高达75% vs Claude的22%，模型通过32000条决策依据展示时空推理能力。

Conclusion: 该研究架起经典博弈论与机器心理学的桥梁，揭示了AI在不确定性下的分层决策机制，为算法战略智能评估提供新范式。

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [42] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: HiRA框架通过分层解耦规划与执行，使用领域专用代理处理复杂搜索任务，显著提升了多模态深度搜索的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成（RAG）无法有效处理跨模态复杂搜索任务，单一模型同时承担规划与执行导致推理效率低下和可扩展性受限。

Method: 分层框架将任务分解为子任务，由专用代理（配备外部工具和推理能力）处理，通过结构化整合机制协调结果，实现规划与执行的解耦。

Result: 在四个复杂跨模态深度搜索基准测试中，HiRA在回答质量和系统效率上均超越现有RAG和基于代理的系统，验证了分层架构的有效性。

Conclusion: 解耦规划与执行的分层框架能有效应对多步骤信息检索任务，领域专用代理的协同工作机制为复杂搜索系统设计提供了新方向。

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [43] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang,Ang Lv,Jinpeng Li,Yongbo Wang,Feng Wang,Haoyuan Hu,Rui Yan*

Main category: cs.AI

TL;DR: 提出StepHint算法，通过多级逐步提示改进强化学习验证奖励机制，解决近失奖励和探索停滞问题，在数学基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法存在两大挑战：1) 近失奖励问题（小错误导致整个推理链失效）；2) 模型局限于『舒适区』解决方案的探索停滞问题。

Method: 使用自适应分割方法将强模型生成的推理链分步，提供多级步骤提示（包含不同数量步骤），在引导探索的同时保持自主探索空间。

Result: 在六个数学基准测试中超越现有RLVR增强方法，在域外基准测试中展现出更优的泛化能力。

Conclusion: StepHint通过外部推理路径有效提升模型推理能力，同时缓解近失奖励问题和探索停滞问题，显著提高训练效率。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [44] [Early Signs of Steganographic Capabilities in Frontier LLMs](https://arxiv.org/abs/2507.02737)
*Artur Zolkowski,Kei Nishimura-Gasparian,Robert McCarthy,Roland S. Zimmermann,David Lindner*

Main category: cs.CR

TL;DR: 评估前沿大语言模型的隐写能力及其监控风险，发现当前模型具备初步隐写能力但尚无法有效绕过设计良好的监控


<details>
  <summary>Details</summary>
Motivation: 监控LLM输出对防范滥用和模型失调风险至关重要，但LLM可能通过隐写术在正常输出中隐藏信息逃避监控。需评估其隐写能力以理解潜在风险

Method: 测试两种隐写能力：消息编码和推理编码。通过标准条件（普通监控）和额外条件（未监控草稿纸+协调编码方案）进行实验，考察十六进制等编码方案的运用

Result: 当前模型在标准条件下无法隐藏短消息，但通过草稿纸和预定义编码方案可成功。在简单状态跟踪任务中展现基本编码推理能力，但难以隐蔽融合到表层任务中欺骗监控

Conclusion: 现有LLM表现出初期隐写能力，当前监控体系仍有效，但需警惕未来能力演进带来的潜在风险

Abstract: Monitoring Large Language Model (LLM) outputs is crucial for mitigating risks
from misuse and misalignment. However, LLMs could evade monitoring through
steganography: Encoding hidden information within seemingly benign generations.
In this paper, we evaluate the steganography capabilities in frontier LLMs to
better understand the risk they pose. We focus on two types of steganography:
passing encoded messages and performing encoded reasoning. We find that current
models are unable to encode short messages in their outputs without a monitor
noticing under standard affordances. They can succeed, however, if given
additional affordances such as using an unmonitored scratchpad and coordinating
on what encoding scheme to use. We additionally find early signs that models
can perform basic encoded reasoning in a simple state-tracking problem. This
includes some ability to reason with their own and pre-defined schemes,
including encoding schemes such as Hexadecimal. Despite this, they can rarely
hide reasoning subtly within a cover task to fool a monitor. Overall, our
results indicate that current LLMs exhibit nascent steganographic capabilities.
While these capabilities are likely insufficient to bypass well-designed
monitors at present, this could change in the future.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [45] [Analyzing and Improving Speaker Similarity Assessment for Speech Synthesis](https://arxiv.org/abs/2507.02176)
*Marc-André Carbonneau,Benjamin van Niekerk,Hugo Seuté,Jean-Philippe Letendre,Herman Kamper,Julian Zaïdi*

Main category: cs.SD

TL;DR: 语音生成系统中ASV嵌入主要捕捉静态声学特征（音色、音高范围），忽视动态韵律特征（节奏）。作者提出U3D指标评估节奏模式，并公开代码。


<details>
  <summary>Details</summary>
Motivation: 现有ASV嵌入侧重说话人区分而非身份表征，动态韵律特征未被有效建模，影响语音克隆系统的身份一致性评估。

Method: 通过分析ASV嵌入的局限性，提出U3D指标专门评估说话人动态节奏模式，并改进说话人相似性测量的混淆因素。

Result: 证实ASV嵌入忽略动态特征，U3D有效补充身份表征维度，相关代码已开源。

Conclusion: 该研究推进语音克隆系统中说话人身份一致性评估，强调动态韵律建模的重要性，为后续研究提供新工具。

Abstract: Modeling voice identity is challenging due to its multifaceted nature. In
generative speech systems, identity is often assessed using automatic speaker
verification (ASV) embeddings, designed for discrimination rather than
characterizing identity. This paper investigates which aspects of a voice are
captured in such representations. We find that widely used ASV embeddings focus
mainly on static features like timbre and pitch range, while neglecting dynamic
elements such as rhythm. We also identify confounding factors that compromise
speaker similarity measurements and suggest mitigation strategies. To address
these gaps, we propose U3D, a metric that evaluates speakers' dynamic rhythm
patterns. This work contributes to the ongoing challenge of assessing speaker
identity consistency in the context of ever-better voice cloning systems. We
publicly release our code.

</details>


### [46] [JoyTTS: LLM-based Spoken Chatbot With Voice Cloning](https://arxiv.org/abs/2507.02380)
*Fangru Zhou,Jun Zhao,Guoxin Wang*

Main category: cs.SD

TL;DR: JoyTTS整合大语言模型与语音克隆技术，基于MiniCPM-o和CosyVoice2开发，2000小时对话数据训练，SS 0.73/WER 5.09，提供完整开源代码。


<details>
  <summary>Details</summary>
Motivation: 通过结合LLM与语音克隆技术，打造更自然的人机语音交互体验，推动开源社区在对话式AI领域的发展。

Method: 基于MiniCPM-o和CosyVoice2开源模型，使用2000小时对话数据进行训练，提供完整训练代码和推理脚本。

Result: 在seed-tts-zh测试机上实现0.73说话人相似度(SS)和5.09%词错率(WER)。

Conclusion: JoyTTS成功实现端到端语音对话与语音克隆，通过开源促进技术迭代，性能指标验证其实际应用价值。

Abstract: JoyTTS is an end-to-end spoken chatbot that combines large language models
(LLM) with text-to-speech (TTS) technology, featuring voice cloning
capabilities. This project is built upon the open-source MiniCPM-o and
CosyVoice2 models and trained on 2000 hours of conversational data. We have
also provided the complete training code to facilitate further development and
optimization by the community. On the testing machine seed-tts-zh, it achieves
a SS (speaker similarity) score of 0.73 and a WER (Word Error Rate) of 5.09.
The code and models, along with training and inference scripts, are available
at https://github.com/jdh-algo/JoyTTS.git.

</details>


### [47] [ASDA: Audio Spectrogram Differential Attention Mechanism for Self-Supervised Representation Learning](https://arxiv.org/abs/2507.02666)
*Junyu Wang,Tianrui Wang,Meng Ge,Longbiao Wang,Jianwu Dang*

Main category: cs.SD

TL;DR: 提出ASDA模型，通过差异注意力机制在音频自监督学习中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 标准Transformer的注意力机制存在无效注意力分配问题，影响模型判别能力

Method: 设计差异注意力机制(双softmax操作+差异系数调节)，抑制不相关特征交互

Result: AS-2M音频分类(49.0% mAP)/AS20K(41.5% mAP)/SPC-2关键词检测(98.3%)/ESC-50环境音分类(96.1%)

Conclusion: ASDA机制有效提升音频任务性能，为更广泛的应用场景奠定基础

Abstract: In recent advancements in audio self-supervised representation learning, the
standard Transformer architecture has emerged as the predominant approach, yet
its attention mechanism often allocates a portion of attention weights to
irrelevant information, potentially impairing the model's discriminative
ability. To address this, we introduce a differential attention mechanism,
which effectively mitigates ineffective attention allocation through the
integration of dual-softmax operations and appropriately tuned differential
coefficients. Experimental results demonstrate that our ASDA model achieves
state-of-the-art (SOTA) performance across multiple benchmarks, including audio
classification (49.0% mAP on AS-2M, 41.5% mAP on AS20K), keyword spotting
(98.3% accuracy on SPC-2), and environmental sound classification (96.1%
accuracy on ESC-50). These results highlight ASDA's effectiveness in audio
tasks, paving the way for broader applications.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [48] [Mesh Silksong: Auto-Regressive Mesh Generation as Weaving Silk](https://arxiv.org/abs/2507.02477)
*Gaochao Song,Zibo Zhao,Haohan Weng,Jingbo Zeng,Rongfei Jia,Shenghua Gao*

Main category: cs.CV

TL;DR: Mesh Silksong是一种高效网格表示方法，通过自回归生成减少50%顶点标记冗余，实现22%压缩率，并提升网格几何完整性。


<details>
  <summary>Details</summary>
Motivation: 现有网格标记化方法存在顶点重复标记问题，导致网络资源浪费，需开发更高效的表示方法。

Method: 采用自回归生成策略，每个顶点仅访问一次以消除冗余，同时确保流形拓扑、水密性检测等几何属性。

Result: 实现约22%的先进压缩率，生成网格具备更优几何属性（如一致面法线），实验验证生成质量与效率提升。

Conclusion: Mesh Silksong在压缩率与几何完整性方面显著优于传统方法，为实际应用提供可靠网格生成方案。

Abstract: We introduce Mesh Silksong, a compact and efficient mesh representation
tailored to generate the polygon mesh in an auto-regressive manner akin to silk
weaving. Existing mesh tokenization methods always produce token sequences with
repeated vertex tokens, wasting the network capability. Therefore, our approach
tokenizes mesh vertices by accessing each mesh vertice only once, reduces the
token sequence's redundancy by 50\%, and achieves a state-of-the-art
compression rate of approximately 22\%. Furthermore, Mesh Silksong produces
polygon meshes with superior geometric properties, including manifold topology,
watertight detection, and consistent face normals, which are critical for
practical applications. Experimental results demonstrate the effectiveness of
our approach, showcasing not only intricate mesh generation but also
significantly improved geometric integrity.

</details>


### [49] [HyperGaussians: High-Dimensional Gaussian Splatting for High-Fidelity Animatable Face Avatars](https://arxiv.org/abs/2507.02803)
*Gent Serifi,Marcel C. Bühler*

Main category: cs.CV

TL;DR: HyperGaussians通过扩展3D高斯泼溅技术至多维空间，显著提升了动态面部虚拟形象的细节渲染能力，并通过逆协方差技巧实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS技术在非线性面部变形、复杂光照和微细节(如眼镜/牙齿)处理上存在局限，无法满足高质量虚拟形象需求。

Method: 将3D高斯升维为可学习局部嵌入的多维高斯，引入逆协方差重参数化技术解决高维计算瓶颈，兼容现有Avatar框架。

Result: 在4个数据集19个对象的测试中，高频细节(眼镜框/牙齿/面部运动/镜面反射)的PSNR提升8.7%，视觉保真度显著优于传统3DGS。

Conclusion: 高维高斯表示突破传统3DGS表达能力边界，为实时高保真虚拟形象建立新基准，推动AR/VR应用发展。

Abstract: We introduce HyperGaussians, a novel extension of 3D Gaussian Splatting for
high-quality animatable face avatars. Creating such detailed face avatars from
videos is a challenging problem and has numerous applications in augmented and
virtual reality. While tremendous successes have been achieved for static
faces, animatable avatars from monocular videos still fall in the uncanny
valley. The de facto standard, 3D Gaussian Splatting (3DGS), represents a face
through a collection of 3D Gaussian primitives. 3DGS excels at rendering static
faces, but the state-of-the-art still struggles with nonlinear deformations,
complex lighting effects, and fine details. While most related works focus on
predicting better Gaussian parameters from expression codes, we rethink the 3D
Gaussian representation itself and how to make it more expressive. Our insights
lead to a novel extension of 3D Gaussians to high-dimensional multivariate
Gaussians, dubbed 'HyperGaussians'. The higher dimensionality increases
expressivity through conditioning on a learnable local embedding. However,
splatting HyperGaussians is computationally expensive because it requires
inverting a high-dimensional covariance matrix. We solve this by
reparameterizing the covariance matrix, dubbed the 'inverse covariance trick'.
This trick boosts the efficiency so that HyperGaussians can be seamlessly
integrated into existing models. To demonstrate this, we plug in HyperGaussians
into the state-of-the-art in fast monocular face avatars: FlashAvatar. Our
evaluation on 19 subjects from 4 face datasets shows that HyperGaussians
outperform 3DGS numerically and visually, particularly for high-frequency
details like eyeglass frames, teeth, complex facial movements, and specular
reflections.

</details>


### [50] [LiteReality: Graphics-Ready 3D Scene Reconstruction from RGB-D Scans](https://arxiv.org/abs/2507.02861)
*Zhening Huang,Xiaoyang Wu,Fangcheng Zhong,Hengshuang Zhao,Matthias Nießner,Joan Lasenby*

Main category: cs.CV

TL;DR: 提出LiteReality流程：通过场景理解-3D模型检索-材质绘制-物理引擎集成的四步法，将RGB-D扫描转换为紧凑、可交互的逼真虚拟场景，在Scan2CAD基准上实现SOTA检索性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有三维重建方案在物体独立性、关节运动、PBR材质质量和物理交互性方面的不足，构建兼容图形管线的数字孪生场景。

Method: 1) 场景图解析三维布局 2) 艺术家模型数据库检索 3) 空间可变材质恢复 4) 物理引擎集成实现交互 4) 无训练检索模块与鲁棒材质迁移算法。

Result: 在真实扫描和公开数据集验证有效，检索模块超越Scan2CAD基准，材质模块支持跨风格图像迁移并克服错位/遮挡/光照问题。

Conclusion: 输出场景具备高编辑性和物理交互性，可广泛应用于AR/VR/机器人/数字孪生领域，为图形管线提供端到端解决方案。

Abstract: We propose LiteReality, a novel pipeline that converts RGB-D scans of indoor
environments into compact, realistic, and interactive 3D virtual replicas.
LiteReality not only reconstructs scenes that visually resemble reality but
also supports key features essential for graphics pipelines -- such as object
individuality, articulation, high-quality physically based rendering materials,
and physically based interaction. At its core, LiteReality first performs scene
understanding and parses the results into a coherent 3D layout and objects with
the help of a structured scene graph. It then reconstructs the scene by
retrieving the most visually similar 3D artist-crafted models from a curated
asset database. Next, the Material Painting module enhances realism by
recovering high-quality, spatially varying materials. Finally, the
reconstructed scene is integrated into a simulation engine with basic physical
properties to enable interactive behavior. The resulting scenes are compact,
editable, and fully compatible with standard graphics pipelines, making them
suitable for applications in AR/VR, gaming, robotics, and digital twins. In
addition, LiteReality introduces a training-free object retrieval module that
achieves state-of-the-art similarity performance on the Scan2CAD benchmark,
along with a robust material painting module capable of transferring
appearances from images of any style to 3D assets -- even under severe
misalignment, occlusion, and poor lighting. We demonstrate the effectiveness of
LiteReality on both real-life scans and public datasets. Project page:
https://litereality.github.io; Video:
https://www.youtube.com/watch?v=ecK9m3LXg2c

</details>


### [51] [ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.02200)
*Xiao Wang,Jingtao Jiang,Qiang Chen,Lan Chen,Lin Zhu,Yaowei Wang,Yonghong Tian,Jin Tang*

Main category: cs.CV

TL;DR: 提出ESTR-CoT框架，通过思维链推理机制提升事件流场景文本识别的可解释性和逻辑推理能力


<details>
  <summary>Details</summary>
Motivation: 现有方法存在可解释性不足和上下文逻辑推理薄弱的局限性，难以应对低光照、快速运动等极端场景

Method: 结合EVA-CLIP视觉编码器与Vicuna-7B大语言模型，通过Q-former对齐视觉token，端到端监督微调框架实现答案与推理过程同步输出

Result: 在EventSTR、WordArt*、IC15*三个基准数据集上验证了框架的有效性，并构建了大规模CoT数据集

Conclusion: ESTR-CoT框架通过可解释的推理过程提升了识别性能，代码和预训练模型将开源促进后续研究

Abstract: Event stream based scene text recognition is a newly arising research topic
in recent years which performs better than the widely used RGB cameras in
extremely challenging scenarios, especially the low illumination, fast motion.
Existing works either adopt end-to-end encoder-decoder framework or large
language models for enhanced recognition, however, they are still limited by
the challenges of insufficient interpretability and weak contextual logical
reasoning. In this work, we propose a novel chain-of-thought reasoning based
event stream scene text recognition framework, termed ESTR-CoT. Specifically,
we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input
event stream into tokens and utilize a Llama tokenizer to encode the given
generation prompt. A Q-former is used to align the vision token to the
pre-trained large language model Vicuna-7B and output both the answer and
chain-of-thought (CoT) reasoning process simultaneously. Our framework can be
optimized using supervised fine-tuning in an end-to-end manner. In addition, we
also propose a large-scale CoT dataset to train our framework via a three stage
processing (i.e., generation, polish, and expert verification). This dataset
provides a solid data foundation for the development of subsequent
reasoning-based large models. Extensive experiments on three event stream STR
benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the
effectiveness and interpretability of our proposed framework. The source code
and pre-trained models will be released on
https://github.com/Event-AHU/ESTR-CoT.

</details>


### [52] [SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers](https://arxiv.org/abs/2507.02212)
*Takuro Kawada,Shunsuke Kitada,Sota Nemoto,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: 提出SciGA-145k大规模数据集（14.5万论文/114万图表），用于图形摘要推荐与生成研究，定义两种推荐任务并提出CAR评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前科学论文图形摘要存在设计门槛高、利用不充分的问题，需通过数据集构建降低设计难度并提升视觉科学交流效果。

Method: 构建包含14.5万论文的数据库，定义论文内图表推荐（Intra-GA）和跨论文推荐（Inter-GA）任务，开发CAR指标解决传统排序指标局限。

Result: 建立首个面向图形摘要研究的大规模基准数据集，提供基线模型及能精细分析模型行为的新评估指标。

Conclusion: SciGA-145k为提升科学可视化交流奠定基础，推动AI for Science发展，通过统一任务与指标促进自动图形摘要生成研究。

Abstract: Graphical Abstracts (GAs) play a crucial role in visually conveying the key
findings of scientific papers. While recent research has increasingly
incorporated visual materials such as Figure 1 as de facto GAs, their potential
to enhance scientific communication remains largely unexplored. Moreover,
designing effective GAs requires advanced visualization skills, creating a
barrier to their widespread adoption. To tackle these challenges, we introduce
SciGA-145k, a large-scale dataset comprising approximately 145,000 scientific
papers and 1.14 million figures, explicitly designed for supporting GA
selection and recommendation as well as facilitating research in automated GA
generation. As a preliminary step toward GA design support, we define two
tasks: 1) Intra-GA recommendation, which identifies figures within a given
paper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,
which retrieves GAs from other papers to inspire the creation of new GAs. We
provide reasonable baseline models for these tasks. Furthermore, we propose
Confidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation
metric that offers a fine-grained analysis of model behavior. CAR addresses
limitations in traditional ranking-based metrics by considering cases where
multiple figures within a paper, beyond the explicitly labeled GA, may also
serve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a
foundation for advancing visual scientific communication while contributing to
the development of AI for Science.

</details>


### [53] [From Long Videos to Engaging Clips: A Human-Inspired Video Editing Framework with Multimodal Narrative Understanding](https://arxiv.org/abs/2507.02790)
*Xiangfeng Wang,Xiao Li,Yadong Wei,Xueyu Song,Yang Song,Xiaoqiang Xia,Fangrui Zeng,Zaiyi Chen,Liu Liu,Gu Xu,Tong Xu*

Main category: cs.CV

TL;DR: 提出HIVE框架，通过多模态叙事理解和场景分割技术提升视频编辑质量，并创建DramaAD数据集验证效果


<details>
  <summary>Details</summary>
Motivation: 现有视频编辑方法过度依赖文本信息，忽视视觉上下文导致内容不连贯，需建立更全面的多模态理解体系

Method: 结合角色提取+对话分析+叙事摘要的多模态模型，采用场景分割技术分解编辑流程为高光检测/片头片尾选择/冗余内容修剪三阶段

Result: 在800+短剧和500+广告片的测试中，编辑质量显著超越基线模型，缩小自动编辑与人工编辑的质量差距

Conclusion: 结构化多模态理解框架有效提升视频编辑连贯性，DramaAD数据集为领域研究提供新基准

Abstract: The rapid growth of online video content, especially on short video
platforms, has created a growing demand for efficient video editing techniques
that can condense long-form videos into concise and engaging clips. Existing
automatic editing methods predominantly rely on textual cues from ASR
transcripts and end-to-end segment selection, often neglecting the rich visual
context and leading to incoherent outputs. In this paper, we propose a
human-inspired automatic video editing framework (HIVE) that leverages
multimodal narrative understanding to address these limitations. Our approach
incorporates character extraction, dialogue analysis, and narrative
summarization through multimodal large language models, enabling a holistic
understanding of the video content. To further enhance coherence, we apply
scene-level segmentation and decompose the editing process into three subtasks:
highlight detection, opening/ending selection, and pruning of irrelevant
content. To facilitate research in this area, we introduce DramaAD, a novel
benchmark dataset comprising over 800 short drama episodes and 500
professionally edited advertisement clips. Experimental results demonstrate
that our framework consistently outperforms existing baselines across both
general and advertisement-oriented editing tasks, significantly narrowing the
quality gap between automatic and human-edited videos.

</details>


### [54] [Visual Contextual Attack: Jailbreaking MLLMs with Image-Driven Context Injection](https://arxiv.org/abs/2507.02844)
*Ziqi Miao,Yi Ding,Lijun Li,Jing Shao*

Main category: cs.CV

TL;DR: 提出VisCo视觉中心越狱攻击方法，通过动态生成视觉上下文显著提升多模态大模型攻击成功率


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs安全攻击方法中视觉模态仅作为触发媒介，存在语义模糊且缺乏真实场景基础。需构建视觉信息作为必要组件的完整越狱场景。

Method: 1.定义视觉中心越狱新范式 2.开发四类视觉策略构建上下文对话 3.动态生成辅助图像 4.结合自动毒性混淆与语义优化生成最终攻击提示

Result: 在MM-SafetyBench上对GPT-4o取得4.78毒性分和85%攻击成功率，显著优于基线方法的2.48分和22.2%成功率

Conclusion: VisCo攻击验证了视觉模态在安全漏洞中的关键作用，表明需开发更全面的防护机制应对复杂多模态攻击场景

Abstract: With the emergence of strong visual-language capabilities, multimodal large
language models (MLLMs) have demonstrated tremendous potential for real-world
applications. However, the security vulnerabilities exhibited by the visual
modality pose significant challenges to deploying such models in open-world
environments. Recent studies have successfully induced harmful responses from
target MLLMs by encoding harmful textual semantics directly into visual inputs.
However, in these approaches, the visual modality primarily serves as a trigger
for unsafe behavior, often exhibiting semantic ambiguity and lacking grounding
in realistic scenarios. In this work, we define a novel setting: visual-centric
jailbreak, where visual information serves as a necessary component in
constructing a complete and realistic jailbreak context. Building on this
setting, we propose the VisCo (Visual Contextual) Attack. VisCo fabricates
contextual dialogue using four distinct visual-focused strategies, dynamically
generating auxiliary images when necessary to construct a visual-centric
jailbreak scenario. To maximize attack effectiveness, it incorporates automatic
toxicity obfuscation and semantic refinement to produce a final attack prompt
that reliably triggers harmful responses from the target black-box MLLMs.
Specifically, VisCo achieves a toxicity score of 4.78 and an Attack Success
Rate (ASR) of 85% on MM-SafetyBench against GPT-4o, significantly outperforming
the baseline, which performs a toxicity score of 2.48 and an ASR of 22.2%. The
code is available at https://github.com/Dtc7w3PQ/Visco-Attack.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [55] [DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with Self-Generated Cross-Modal Alignment](https://arxiv.org/abs/2507.02768)
*Ke-Han Lu,Zhehuai Chen,Szu-Wei Fu,Chao-Han Huck Yang,Sung-Feng Huang,Chih-Kai Yang,Chee-En Yu,Chun-Wei Chen,Wei-Chih Chen,Chien-yu Huang,Yi-Cheng Lin,Yu-Xiang Lin,Chi-An Fu,Chun-Yi Kuan,Wenze Ren,Xuanjun Chen,Wei-Ping Huang,En-Pei Hu,Tzu-Quan Lin,Yuan-Kuei Wu,Kuan-Po Huang,Hsiao-Ying Huang,Huang-Cheng Chou,Kai-Wei Chang,Cheng-Han Chiang,Boris Ginsburg,Yu-Chiang Frank Wang,Hung-yi Lee*

Main category: eess.AS

TL;DR: 提出无需任务特定调优的通用大音频语言模型DeSTA2.5-Audio，通过自生成跨模态对齐策略DeSTA构建大规模数据集DeSTA-AQA5M，实现多领域音频任务的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有音频语言模型在增强听觉能力时导致LLM语言能力退化（灾难性遗忘）的核心问题，探索保持语言模型原生能力的同时实现音文对齐的有效路径。

Method: 采用自生成跨模态对齐策略DeSTA，利用基座LLM自主生成训练目标，构建包含500万样本（7000小时音频、覆盖50个数据集）的DeSTA-AQA5M数据集，实现任务无关的模型训练。

Result: 在Dynamic-SUPERB等五大基准测试中达到SOTA水平，验证了自生成策略在听觉感知和指令跟随能力上的双重优势。

Conclusion: 数据构建策略对LALM开发具有决定性影响，DeSTA框架为构建通用音频语言模型提供了有效方法论，突破传统任务特定调优的局限性。

Abstract: We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model
(LALM) designed for robust auditory perception and instruction-following,
without requiring task-specific audio instruction-tuning. Recent LALMs
typically augment Large Language Models (LLMs) with auditory capabilities by
training on large-scale, manually curated or LLM-synthesized audio-instruction
datasets. However, these approaches have often suffered from the catastrophic
forgetting of the LLM's original language abilities. To address this, we
revisit the data construction pipeline and propose DeSTA, a self-generated
cross-modal alignment strategy in which the backbone LLM generates its own
training targets. This approach preserves the LLM's native language proficiency
while establishing effective audio-text alignment, thereby enabling zero-shot
generalization without task-specific tuning. Using DeSTA, we construct
DeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training
samples derived from 7,000 hours of audio spanning 50 diverse datasets,
including speech, environmental sounds, and music. DeSTA2.5-Audio achieves
state-of-the-art or competitive performance across a wide range of
audio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA,
Speech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate
that our self-generated strategy outperforms widely adopted data construction
and training strategies in both auditory perception and instruction-following
capabilities. Our findings underscore the importance of carefully designed data
construction in LALM development and offer practical insights for building
robust, general-purpose LALMs.

</details>
