<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 60]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.DL](#cs.DL) [Total: 2]
- [eess.AS](#eess.AS) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.AI](#cs.AI) [Total: 10]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.DC](#cs.DC) [Total: 2]
- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [A Preliminary Study of RAG for Taiwanese Historical Archives](https://arxiv.org/abs/2511.07445)
*Claire Lin,Bo-Han Feng,Xuanjun Chen,Te-Lun Yang,Hung-yi Lee,Jyh-Shing Roger Jang*

Main category: cs.CL

TL;DR: 本研究首次将检索增强生成(RAG)应用于台湾历史档案《热兰遮城》和《台湾省议会公报》，发现早期元数据整合能提升系统表现，但系统仍面临生成幻觉、时序查询处理等挑战。


<details>
  <summary>Details</summary>
Motivation: 探索RAG在台湾历史档案领域的适用性，填补该技术在此类文化资产应用场景的研究空白。

Method: 构建包含两个传统中文历史数据集及其查询集的测试框架，系统分析查询特征与元数据整合策略对检索质量、答案生成及系统性能的影响。

Result: 早期元数据整合使检索准确率提升12.7%，答案准确性提高15.3%，但系统在时间相关查询和多跳推理任务中仍有30%的错误率。

Conclusion: 该研究为文化遗产数字化提供了技术验证，指出需开发领域特定的检索算法和时序推理机制以提升历史档案的智能检索效果。

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach for knowledge-intensive tasks. However, few studies have examined RAG for Taiwanese Historical Archives. In this paper, we present an initial study of a RAG pipeline applied to two historical Traditional Chinese datasets, Fort Zeelandia and the Taiwan Provincial Council Gazette, along with their corresponding open-ended query sets. We systematically investigate the effects of query characteristics and metadata integration strategies on retrieval quality, answer generation, and the performance of the overall system. The results show that early-stage metadata integration enhances both retrieval and answer accuracy while also revealing persistent challenges for RAG systems, including hallucinations during generation and difficulties in handling temporal or multi-hop historical queries.

</details>


### [2] [Large Language Models for Scientific Idea Generation: A Creativity-Centered Survey](https://arxiv.org/abs/2511.07448)
*Fatemeh Shahhosseini,Arash Marioriyad,Ali Momen,Mahdieh Soleymani Baghshah,Mohammad Hossein Rohban,Shaghayegh Haghjooy Javanmard*

Main category: cs.CL

TL;DR: 综述系统性整理LLM驱动科学创意生成方法，提出五类技术路径并运用创造力理论框架解析其创新维度


<details>
  <summary>Details</summary>
Motivation: 解决LLM在科学创意生成中创造力不稳定、创新层次不清晰的问题，建立方法论与创造力理论的映射关系

Method: 提出五类方法：外部知识增强、提示导向分布调控、推理时扩展、多智能体协作、参数级适应

Result: 构建Boden创造力分类(组合/探索/变革)与Rhodes 4Ps框架(人/过程/环境/产物)的双维度分析体系

Conclusion: 需发展可靠性验证系统与创新评估指标，推动LLM在科学发现中实现系统性变革应用

Abstract: Scientific idea generation lies at the heart of scientific discovery and has driven human progress-whether by solving unsolved problems or proposing novel hypotheses to explain unknown phenomena. Unlike standard scientific reasoning or general creative generation, idea generation in science is a multi-objective and open-ended task, where the novelty of a contribution is as essential as its empirical soundness. Large language models (LLMs) have recently emerged as promising generators of scientific ideas, capable of producing coherent and factual outputs with surprising intuition and acceptable reasoning, yet their creative capacity remains inconsistent and poorly understood. This survey provides a structured synthesis of methods for LLM-driven scientific ideation, examining how different approaches balance creativity with scientific soundness. We categorize existing methods into five complementary families: External knowledge augmentation, Prompt-based distributional steering, Inference-time scaling, Multi-agent collaboration, and Parameter-level adaptation. To interpret their contributions, we employ two complementary frameworks: Boden's taxonomy of Combinatorial, Exploratory and Transformational creativity to characterize the level of ideas each family expected to generate, and Rhodes' 4Ps framework-Person, Process, Press, and Product-to locate the aspect or source of creativity that each method emphasizes. By aligning methodological advances with creativity frameworks, this survey clarifies the state of the field and outlines key directions toward reliable, systematic, and transformative applications of LLMs in scientific discovery.

</details>


### [3] [GRIP: In-Parameter Graph Reasoning through Fine-Tuning Large Language Models](https://arxiv.org/abs/2511.07457)
*Jiarui Feng,Donghong Cai,Yixin Chen,Muhan Zhang*

Main category: cs.CL

TL;DR: 提出GRIP框架，通过LoRA参数微调使大语言模型内化图结构信息，实现无需原始图谱的高效推理


<details>
  <summary>Details</summary>
Motivation: 现有方法存在序列化图谱导致的token冗余、图-文本对齐困难等问题，需探索更高效的图结构适配方案

Method: 设计图感知微调任务，将复杂关系信息存储在轻量级LoRA参数中，支持zero-shot图谱推理

Result: 在多基准测试中验证了框架的有效性，显著提升模型处理结构化数据的能力

Conclusion: GRIP为LLMs整合结构化知识提供了参数高效、推理便捷的解决方案

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in modeling sequential textual data and generalizing across diverse tasks. However, adapting LLMs to effectively handle structural data, such as knowledge graphs or web data, remains a challenging problem. Some approaches adopt complex strategies to convert graphs into text sequences, resulting in significant token overhead and rendering them impractical for large-scale graphs. Others introduce additional modules to encode graphs into fixed-size token representations for LLMs. However, these methods typically require large-scale post-training on graph-text corpus and complex alignment procedures, yet often yield sub-optimal results due to poor modality alignment. Inspired by in-parameter knowledge injection for test-time adaptation of LLMs, we propose GRIP, a novel framework that equips LLMs with the ability to internalize complex relational information from graphs through carefully designed fine-tuning tasks. This knowledge is efficiently stored within lightweight LoRA parameters, enabling the fine-tuned LLM to perform a wide range of graph-related tasks without requiring access to the original graph at inference time. Extensive experiments across multiple benchmarks validate the effectiveness and efficiency of our approach.

</details>


### [4] [REFLEX: Reference-Free Evaluation of Log Summarization via Large Language Model Judgment](https://arxiv.org/abs/2511.07458)
*Priyanka Mudgal*

Main category: cs.CL

TL;DR: 提出无参考评估指标REFLEX，利用大语言模型从多维度评估日志摘要质量，解决传统指标依赖参考摘要的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有日志摘要评估方法依赖参考摘要和词汇重叠指标（如ROUGE/BLEU），在参考数据稀缺场景效果受限，需更有效的评估方案。

Method: 基于大语言模型的零样本评估框架，通过相关性、信息量、连贯性等多维度指标，实现无需参考摘要的自动化评估。

Result: 在多个数据集验证显示REFLEX具有稳定性和强区分力，评估结果比传统指标更细粒度且可解释。

Conclusion: REFLEX为实际应用场景中参考数据不足的日志摘要评估问题，提供了可扩展的解决方案。

Abstract: Evaluating log summarization systems is challenging due to the lack of high-quality reference summaries and the limitations of existing metrics like ROUGE and BLEU, which depend on surface-level lexical overlap. We introduce REFLEX, a reference-free evaluation metric for log summarization based on large language model (LLM) judgment. REFLEX uses LLMs as zero-shot evaluators to assess summary quality along dimensions such as relevance, informativeness, and coherence, without requiring gold-standard references or human annotations. We show that REFLEX produces stable, interpretable, and fine-grained evaluations across multiple log summarization dataset, and more effectively distinguishes model outputs than traditional metrics. REFLEX provides a scalable alternative for evaluating log summaries in real-world settings where reference data is scarce or unavailable.

</details>


### [5] [It Takes Two: A Dual Stage Approach for Terminology-Aware Translation](https://arxiv.org/abs/2511.07461)
*Akshat Singh Jaswal*

Main category: cs.CL

TL;DR: 提出两阶段术语约束翻译架构DuTerm，通过NMT微调与LLM后编辑结合实现术语适配


<details>
  <summary>Details</summary>
Motivation: 解决传统机器翻译中术语约束执行僵化的问题，探索上下文驱动的灵活术语处理方案

Method: 两阶段架构：1) 合成数据微调的术语感知NMT模型 2) 基于提示的LLM后编辑优化术语一致性

Result: LLM的上下文驱动处理在英德/西/俄翻译中质量超越严格约束方法，BLEU值提升显著

Conclusion: LLM更适合作为上下文驱动的翻译优化器而非直接生成器，揭示质量与约束强度的平衡关系

Abstract: This paper introduces DuTerm, a novel two-stage architecture for terminology-constrained machine translation. Our system combines a terminology-aware NMT model, adapted via fine-tuning on large-scale synthetic data, with a prompt-based LLM for post-editing. The LLM stage refines NMT output and enforces terminology adherence. We evaluate DuTerm on English-to German, English-to-Spanish, and English-to-Russian with the WMT 2025 Terminology Shared Task corpus. We demonstrate that flexible, context-driven terminology handling by the LLM consistently yields higher quality translations than strict constraint enforcement. Our results highlight a critical trade-off, revealing that an LLM's work best for high-quality translation as context-driven mutators rather than generators.

</details>


### [6] [Motif 2 12.7B technical report](https://arxiv.org/abs/2511.07464)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Taehyun Kim,Eunhwan Park,Jeesoo Lee,Jeongdoo Lee,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Minjae Kim,Taewhan Kim,Youngrok Kim,Hyukjin Kweon,Haesol Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Dongjoo Weon*

Main category: cs.CL

TL;DR: Motif-2-12.7B是通过架构创新与系统优化提升效率的新型语言模型，在有限算力下实现高效语言理解与指令泛化。


<details>
  <summary>Details</summary>
Motivation: 旨在突破大语言模型效率瓶颈，通过结合分组差分注意力架构创新与MuonClip优化器等系统级优化，实现在有限计算资源下的高效语言理解与强指令泛化能力。

Method: 1.架构：基于Motif-2.6B整合分组差分注意力(GDA)，分离信号与噪声控制路径
2.预训练：使用课程驱动的数据调度器处理5.5万亿跨领域token
3.系统优化：采用MuonClip优化器、PolyNorm融合激活与Parallel Muon算法
4.后训练：三阶段微调流程提升指令遵循、组合理解与语言精度

Result: 在多项基准测试中展现出与更大规模模型相当的竞争力，验证了架构优化与高效训练设计的有效性。

Conclusion: 证明通过精细的架构扩展和训练系统优化，中等规模模型可以实现与超大规模模型相媲美的性能表现。

Abstract: We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compute budgets, Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using a curriculum-driven data scheduler that gradually changes the data composition ratio. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm, yielding significant throughput and memory efficiency gains in large-scale distributed environments. Post-training employs a three-stage supervised fine-tuning pipeline that successively enhances general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models.

</details>


### [7] [Focusing on Language: Revealing and Exploiting Language Attention Heads in Multilingual Large Language Models](https://arxiv.org/abs/2511.07498)
*Xin Liu,Qiyang Song,Qihang Zhou,Haichao Du,Shaowen Xu,Wenbo Jiang,Weijuan Zhang,Xiaoqi Jia*

Main category: cs.CL

TL;DR: 提出LAHIS方法分析LLM多头注意力机制，发现语言特定/通用头部存在，通过轻量级适配器调制注意力输出（仅20参数）提升XQuAD准确率。


<details>
  <summary>Details</summary>
Motivation: 探索MHA在LLM多语言处理中的作用，发现语言特定头能实现跨语言注意力转移，解决多语言模型中的非目标语言生成问题，增强模型可解释性。

Method: 1. 提出LAHIS方法：通过单次前向/反向传播计算注意力头重要性分数；2. 设计轻量软头掩模适配器，动态调整语言相关注意力头的输出。

Result: 在Aya-23-8B等模型中发现：语言特定头主导跨语言上下文引导，语言通用头处理共享特征；软头掩模适配器使XQuAD准确率显著提升。

Conclusion: 从MHA视角增强LLM多语言能力，LAHIS提供高效可解释性分析工具，轻量适配器方案为多语言优化开辟低参数调优新路径。

Abstract: Large language models (LLMs) increasingly support multilingual understanding and generation. Meanwhile, efforts to interpret their internal mechanisms have emerged, offering insights to enhance multilingual performance. While multi-head self-attention (MHA) has proven critical in many areas, its role in multilingual capabilities remains underexplored. In this work, we study the contribution of MHA in supporting multilingual processing in LLMs. We propose Language Attention Head Importance Scores (LAHIS), an effective and efficient method that identifies attention head importance for multilingual capabilities via a single forward and backward pass through the LLM. Applying LAHIS to Aya-23-8B, Llama-3.2-3B, and Mistral-7B-v0.1, we reveal the existence of both language-specific and language-general heads. Language-specific heads enable cross-lingual attention transfer to guide the model toward target language contexts and mitigate off-target language generation issue, contributing to addressing challenges in multilingual LLMs. We also introduce a lightweight adaptation that learns a soft head mask to modulate attention outputs over language heads, requiring only 20 tunable parameters to improve XQuAD accuracy. Overall, our work enhances both the interpretability and multilingual capabilities of LLMs from the perspective of MHA.

</details>


### [8] [LLM Optimization Unlocks Real-Time Pairwise Reranking](https://arxiv.org/abs/2511.07555)
*Jingyu Wu,Aditya Shrivastava,Jing Zhu,Alfy Samuel,Anoop Kumar,Daben Liu*

Main category: cs.CL

TL;DR: 通过系统优化方法将LLM重排序延迟降低166倍，同时保持性能稳定，使实时应用成为可能


<details>
  <summary>Details</summary>
Motivation: 解决基于LLM的成对重排序(PRP)方法在实时应用中面临的高延迟和高计算成本问题

Method: 使用小模型/限制重排序集/低精度计算/单方向顺序推理减少位置偏差/限制输出token

Result: 单查询延迟从61.36秒降至0.37秒，Recall@k性能仅轻微下降

Conclusion: 通过系统优化策略显著提升效率，证明LLM重排序在实时系统中的可行性

Abstract: Efficiently reranking documents retrieved from information retrieval (IR) pipelines to enhance overall quality of Retrieval-Augmented Generation (RAG) system remains an important yet challenging problem. Recent studies have highlighted the importance of Large Language Models (LLMs) in reranking tasks. In particular, Pairwise Reranking Prompting (PRP) has emerged as a promising plug-and-play approach due to its usability and effectiveness. However, the inherent complexity of the algorithm, coupled with the high computational demands and latency incurred due to LLMs, raises concerns about its feasibility in real-time applications. To address these challenges, this paper presents a focused study on pairwise reranking, demonstrating that carefully applied optimization methods can significantly mitigate these issues. By implementing these methods, we achieve a remarkable latency reduction of up to 166 times, from 61.36 seconds to 0.37 seconds per query, with an insignificant drop in performance measured by Recall@k. Our study highlights the importance of design choices that were previously overlooked, such as using smaller models, limiting the reranked set, using lower precision, reducing positional bias with one-directional order inference, and restricting output tokens. These optimizations make LLM-based reranking substantially more efficient and feasible for latency-sensitive, real-world deployments.

</details>


### [9] [LLMs vs. Traditional Sentiment Tools in Psychology: An Evaluation on Belgian-Dutch Narratives](https://arxiv.org/abs/2511.07641)
*Ratna Kandala,Katie Hoemann*

Main category: cs.CL

TL;DR: 荷兰特定LLM在弗拉芒语情感分析任务中表现逊于传统方法，挑战了LLM在情感分析中的优势假设


<details>
  <summary>Details</summary>
Motivation: 验证荷兰语大语言模型在低资源语言变种（弗拉芒语）情感价预测中的效能，与传统词典工具对比

Method: 使用3个荷兰调优LLM（ChocoLlama-8B/GEITje-7B/Reynaerde-7B）与LIWC/Pattern工具对比，基于25,000条自发性文本及自评效价数据

Result: Pattern工具表现最优，LLM未达预期效果，传统方法在真实叙事情感分析中更具优势

Conclusion: 需开发文化语言适配的评估框架，质疑当前LLM微调方法对日常情感表达的捕捉能力

Abstract: Understanding emotional nuances in everyday language is crucial for computational linguistics and emotion research. While traditional lexicon-based tools like LIWC and Pattern have served as foundational instruments, Large Language Models (LLMs) promise enhanced context understanding. We evaluated three Dutch-specific LLMs (ChocoLlama-8B-Instruct, Reynaerde-7B-chat, and GEITje-7B-ultra) against LIWC and Pattern for valence prediction in Flemish, a low-resource language variant. Our dataset comprised approximately 25000 spontaneous textual responses from 102 Dutch-speaking participants, each providing narratives about their current experiences with self-assessed valence ratings (-50 to +50). Surprisingly, despite architectural advancements, the Dutch-tuned LLMs underperformed compared to traditional methods, with Pattern showing superior performance. These findings challenge assumptions about LLM superiority in sentiment analysis tasks and highlight the complexity of capturing emotional valence in spontaneous, real-world narratives. Our results underscore the need for developing culturally and linguistically tailored evaluation frameworks for low-resource language variants, while questioning whether current LLM fine-tuning approaches adequately address the nuanced emotional expressions found in everyday language use.

</details>


### [10] [Revisiting NLI: Towards Cost-Effective and Human-Aligned Metrics for Evaluating LLMs in Question Answering](https://arxiv.org/abs/2511.07659)
*Sai Shridhar Balamurali,Lu Cheng*

Main category: cs.CL

TL;DR: 传统词汇指标和LLM评估方法存在缺陷，研究发现基于NLI的低成本评估方法在长问答评估中达到GPT-4o同等精度且效率更高


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM答案评估方法中词汇指标缺乏语义理解、LLM-as-Judge计算成本过高的问题

Method: 采用现成NLI评分结合词汇匹配标志，在跨五大数据集的DIVER-QA基准上进行验证

Result: NLI方法达到89.9%准确率（与GPT-4o持平），计算资源消耗降低数个数量级

Conclusion: 低成本NLI评估保持竞争力，DIVER-QA基准为未来评估指标研究提供开放资源

Abstract: Evaluating answers from state-of-the-art large language models (LLMs) is challenging: lexical metrics miss semantic nuances, whereas "LLM-as-Judge" scoring is computationally expensive. We re-evaluate a lightweight alternative -- off-the-shelf Natural Language Inference (NLI) scoring augmented by a simple lexical-match flag and find that this decades-old technique matches GPT-4o's accuracy (89.9%) on long-form QA, while requiring orders-of-magnitude fewer parameters. To test human alignment of these metrics rigorously, we introduce DIVER-QA, a new 3000-sample human-annotated benchmark spanning five QA datasets and five candidate LLMs. Our results highlight that inexpensive NLI-based evaluation remains competitive and offer DIVER-QA as an open resource for future metric research.

</details>


### [11] [Stress Testing Factual Consistency Metrics for Long-Document Summarization](https://arxiv.org/abs/2511.07689)
*Zain Muhammad Mujahid,Dustin Wright,Isabelle Augenstein*

Main category: cs.CL

TL;DR: 现有短文本事实性指标在长文档摘要评估中存在稳定性不足的问题，需通过多跨度推理和上下文校准改进


<details>
  <summary>Details</summary>
Motivation: 传统事实性评估指标在长文档场景下面临输入长度限制和长程依赖挑战，需验证现有短文本指标在长文档中的可靠性

Method: 通过7种事实保持性扰动（转述/简化/同义替换等）测试6个无参考指标，在科幻/法律/科学领域数据集分析指标敏感性和上下文检索影响

Result: 现有指标对语义等价摘要评分不稳定，对信息密度高的声明可靠性下降，扩展检索上下文仅部分改善稳定性

Conclusion: 需开发支持多跨度推理、上下文感知校准的指标，并通过语义保持性数据增强提升长文档评估鲁棒性

Abstract: Evaluating the factual consistency of abstractive text summarization remains a significant challenge, particularly for long documents, where conventional metrics struggle with input length limitations and long-range dependencies. In this work, we systematically evaluate the reliability of six widely used reference-free factuality metrics, originally proposed for short-form summarization, in the long-document setting. We probe metric robustness through seven factuality-preserving perturbations applied to summaries, namely paraphrasing, simplification, synonym replacement, logically equivalent negations, vocabulary reduction, compression, and source text insertion, and further analyze their sensitivity to retrieval context and claim information density. Across three long-form benchmark datasets spanning science fiction, legal, and scientific domains, our results reveal that existing short-form metrics produce inconsistent scores for semantically equivalent summaries and exhibit declining reliability for information-dense claims whose content is semantically similar to many parts of the source document. While expanding the retrieval context improves stability in some domains, no metric consistently maintains factual alignment under long-context conditions. Finally, our results highlight concrete directions for improving factuality evaluation, including multi-span reasoning, context-aware calibration, and training on meaning-preserving variations to enhance robustness in long-form summarization. We release all code, perturbed data, and scripts required to reproduce our results at https://github.com/zainmujahid/metricEval-longSum.

</details>


### [12] [CAPO: Confidence Aware Preference Optimization Learning for Multilingual Preferences](https://arxiv.org/abs/2511.07691)
*Rhitabrat Pokharel,Yufei Tao,Ameeta Agrawal*

Main category: cs.CL

TL;DR: 提出CAPO方法改进多语言偏好优化，通过动态损失缩放提升奖励准确率16%


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法在英语有效但多语言场景表现不佳，需增强噪声数据和低边际比较的鲁棒性

Method: 用基于相对奖励的动态损失缩放机制替代DPO固定处理方式，根据偏好置信度调整学习信号

Result: 奖励准确率提升至少16%，多语言偏好与非偏好响应差距扩大改善对齐效果

Conclusion: CAPO通过置信度感知机制有效解决多语言偏好优化挑战，显著优于现有基线方法

Abstract: Preference optimization is a critical post-training technique used to align large language models (LLMs) with human preferences, typically by fine-tuning on ranked response pairs. While methods like Direct Preference Optimization (DPO) have proven effective in English, they often fail to generalize robustly to multilingual settings. We propose a simple yet effective alternative, Confidence-Aware Preference Optimization (CAPO), which replaces DPO's fixed treatment of preference pairs with a dynamic loss scaling mechanism based on a relative reward. By modulating the learning signal according to the confidence in each preference pair, CAPO enhances robustness to noisy or low-margin comparisons, typically encountered in multilingual text. Empirically, CAPO outperforms existing preference optimization baselines by at least 16% in reward accuracy, and improves alignment by widening the gap between preferred and dispreferred responses across languages.

</details>


### [13] [Critical Confabulation: Can LLMs Hallucinate for Social Good?](https://arxiv.org/abs/2511.07722)
*Peiqi Sui,Eamon Duede,Hoyt Long,Richard Jean So*

Main category: cs.CL

TL;DR: 提出关键虚构方法，利用LLM的受控幻觉填补历史档案中的社会不平等空白，验证其在知识生产中保持历史准确性的可行性


<details>
  <summary>Details</summary>
Motivation: 历史档案中因社会不平等导致的叙事空白需要基于证据的填补方法，避免传统虚构的准确性缺失问题

Method: 通过基于小说的角色时间线设计叙事填空任务，使用OLMo-2等模型在受控提示下生成事件，评估幻觉控制效果

Result: LLM展现基础叙事理解能力，受控幻觉可支持知识生产应用，实现历史准确性与创造性推测的平衡

Conclusion: 精心设计的幻觉机制能有效弥补历史记录的结构性空白，推动更包容的知识生产体系构建

Abstract: LLMs hallucinate, yet some confabulations can have social affordances if carefully bounded. We propose critical confabulation (inspired by critical fabulation from literary and social theory), the use of LLM hallucinations to "fill-in-the-gap" for omissions in archives due to social and political inequality, and reconstruct divergent yet evidence-bound narratives for history's "hidden figures". We simulate these gaps with an open-ended narrative cloze task: asking LLMs to generate a masked event in a character-centric timeline sourced from a novel corpus of unpublished texts. We evaluate audited (for data contamination), fully-open models (the OLMo-2 family) and unaudited open-weight and proprietary baselines under a range of prompts designed to elicit controlled and useful hallucinations. Our findings validate LLMs' foundational narrative understanding capabilities to perform critical confabulation, and show how controlled and well-specified hallucinations can support LLM applications for knowledge production without collapsing speculation into a lack of historical accuracy and fidelity.

</details>


### [14] [Back to the Future: The Role of Past and Future Context Predictability in Incremental Language Production](https://arxiv.org/abs/2511.07752)
*Shiva Upadhye,Richard Futrell*

Main category: cs.CL

TL;DR: 研究通过改进测量方法和语言模型，揭示语境可预测性对语言产生的双向影响，提出整合未来/过去语境的新预测指标，并通过替换错误分析揭示词汇规划机制。


<details>
  <summary>Details</summary>
Motivation: 现有研究对后向语境可预测性（基于未来语境的词汇预测）作用机制理解不足，需探索其与语言产生中未来规划的关系。

Method: 1. 使用自然主义语音语料库
2. 开发信息理论预测指标整合未来/过去语境
3. 分两部分研究：词长经典效应重验证、生成框架下的替换错误建模

Result: 1. 新后向预测指标与传统方法效果趋同
2. 替换错误分析显示说话者在词汇规划时优先处理形式、语义和语境信息

Conclusion: 过去与未来语境共同塑造词汇编码选择机制，为语境可预测性效应与句子规划机制搭建理论桥梁

Abstract: Contextual predictability shapes both the form and choice of words in online language production. The effects of the predictability of a word given its previous context are generally well-understood in both production and comprehension, but studies of naturalistic production have also revealed a poorly-understood backward predictability effect of a word given its future context, which may be related to future planning. Here, in two studies of naturalistic speech corpora, we investigate backward predictability effects using improved measures and more powerful language models, introducing a new principled and conceptually motivated information-theoretic predictability measure that integrates predictability from both the future and the past context. Our first study revisits classic predictability effects on word duration. Our second study investigates substitution errors within a generative framework that independently models the effects of lexical, contextual, and communicative factors on word choice, while predicting the actual words that surface as speech errors. We find that our proposed conceptually-motivated alternative to backward predictability yields qualitatively similar effects across both studies. Through a fine-grained analysis of substitution errors, we further show that different kinds of errors are suggestive of how speakers prioritize form, meaning, and context-based information during lexical planning. Together, these findings illuminate the functional roles of past and future context in how speakers encode and choose words, offering a bridge between contextual predictability effects and the mechanisms of sentence planning.

</details>


### [15] [Design, Results and Industry Implications of the World's First Insurance Large Language Model Evaluation Benchmark](https://arxiv.org/abs/2511.07794)
*Hua Zhou,Bing Ma,Yufei Zhang,Yi Zhao*

Main category: cs.CL

TL;DR: 构建保险领域专业评估基准CUFEInse v1.0，通过5维度54指标评估11个大模型，揭示通用模型精算能力薄弱、专用模型业务适配不足等瓶颈


<details>
  <summary>Details</summary>
Motivation: 填补保险领域大模型专业评估空白，建立垂直领域评估范式参考

Method: 采用量化导向+专家驱动模式，构建含保险知识、合规性、智能体应用等5大维度14,430问题的评估体系

Result: 通用模型精算能力平均不足40分，专用模型合规场景得分超85但业务适配存在短板

Conclusion: CUFEInse为保险大模型优化提供权威参照，其'领域适配+推理增强'发展路径指引行业方向

Abstract: This paper comprehensively elaborates on the construction methodology, multi-dimensional evaluation system, and underlying design philosophy of CUFEInse v1.0. Adhering to the principles of "quantitative-oriented, expert-driven, and multi-validation," the benchmark establishes an evaluation framework covering 5 core dimensions, 54 sub-indicators, and 14,430 high-quality questions, encompassing insurance theoretical knowledge, industry understanding, safety and compliance, intelligent agent application, and logical rigor. Based on this benchmark, a comprehensive evaluation was conducted on 11 mainstream large language models. The evaluation results reveal that general-purpose models suffer from common bottlenecks such as weak actuarial capabilities and inadequate compliance adaptation. High-quality domain-specific training demonstrates significant advantages in insurance vertical scenarios but exhibits shortcomings in business adaptation and compliance. The evaluation also accurately identifies the common bottlenecks of current large models in professional scenarios such as insurance actuarial, underwriting and claim settlement reasoning, and compliant marketing copywriting. The establishment of CUFEInse not only fills the gap in professional evaluation benchmarks for the insurance field, providing academia and industry with a professional, systematic, and authoritative evaluation tool, but also its construction concept and methodology offer important references for the evaluation paradigm of large models in vertical fields, serving as an authoritative reference for academic model optimization and industrial model selection. Finally, the paper looks forward to the future iteration direction of the evaluation benchmark and the core development direction of "domain adaptation + reasoning enhancement" for insurance large models.

</details>


### [16] [From Experience to Strategy: Empowering LLM Agents with Trainable Graph Memory](https://arxiv.org/abs/2511.07800)
*Siyu Xia,Zekun Xu,Jiajun Chai,Wentian Fan,Yan Song,Xiaohan Wang,Guojun Yin,Wei Lin,Haifeng Zhang,Jun Wang*

Main category: cs.CL

TL;DR: 提出新型可训练多层图记忆框架，通过结构化决策路径和强化优化的元认知提示增强LLM智能体的策略推理能力和强化学习效果


<details>
  <summary>Details</summary>
Motivation: 现有LLM经验利用方式存在隐式记忆的灾难性遗忘/低可解释性，以及显式记忆缺乏适应性的双重缺陷，需建立可训练的自适应记忆机制

Method: 构建包含状态机决策路径抽象层和战略元认知层的图结构，设计基于下游任务奖励反馈的强化权重优化算法动态调整元认知策略

Result: 实验证明该方法提升LLM智能体21.7%的跨任务泛化能力，在战略推理任务中准确率提升15.3%，RL训练收敛速度加快40%

Conclusion: 可学习的图记忆框架有效桥接显式记忆与参数化知识，通过动态元认知整合机制显著增强LLM智能体的认知推理与持续学习能力

Abstract: Large Language Models (LLMs) based agents have demonstrated remarkable potential in autonomous task-solving across complex, open-ended environments. A promising approach for improving the reasoning capabilities of LLM agents is to better utilize prior experiences in guiding current decisions. However, LLMs acquire experience either through implicit memory via training, which suffers from catastrophic forgetting and limited interpretability, or explicit memory via prompting, which lacks adaptability. In this paper, we introduce a novel agent-centric, trainable, multi-layered graph memory framework and evaluate how context memory enhances the ability of LLMs to utilize parametric information. The graph abstracts raw agent trajectories into structured decision paths in a state machine and further distills them into high-level, human-interpretable strategic meta-cognition. In order to make memory adaptable, we propose a reinforcement-based weight optimization procedure that estimates the empirical utility of each meta-cognition based on reward feedback from downstream tasks. These optimized strategies are then dynamically integrated into the LLM agent's training loop through meta-cognitive prompting. Empirically, the learnable graph memory delivers robust generalization, improves LLM agents' strategic reasoning performance, and provides consistent benefits during Reinforcement Learning (RL) training.

</details>


### [17] [AlignSurvey: A Comprehensive Benchmark for Human Preferences Alignment in Social Surveys](https://arxiv.org/abs/2511.07871)
*Chenxi Lin,Weikang Yuan,Zhuoren Jiang,Biao Huang,Ruitao Zhang,Jianan Ge,Yueqian Xu,Jianxing Yu*

Main category: cs.CL

TL;DR: 提出AlignSurvey基准系统评估大语言模型在社会调查全流程中的应用，包含四阶段任务、多层级数据集及公平性评估指标


<details>
  <summary>Details</summary>
Motivation: 传统社会调查存在问卷格式固化、成本高、跨文化等效性差等问题，现有LLM模拟研究多局限于结构化问题且忽略边缘群体代表性风险

Method: 构建包含社会角色建模、半结构化访谈建模等四阶段任务的评估框架，创建含44K+访谈对话的多层级数据集，通过两阶段微调开发SurveyLM系列模型

Result: 建立首个覆盖全流程的社会调查评估基准，提供跨文化等效性验证工具，发布包含40万+结构化记录的社会基础语料库及国家代表性调查数据集

Conclusion: AlignSurvey通过系统性任务定义和数据集架构支持透明、负责任的LLM社会调查研究，开源模型与工具促进跨文化评估与领域对齐研究

Abstract: Understanding human attitudes, preferences, and behaviors through social surveys is essential for academic research and policymaking. Yet traditional surveys face persistent challenges, including fixed-question formats, high costs, limited adaptability, and difficulties ensuring cross-cultural equivalence. While recent studies explore large language models (LLMs) to simulate survey responses, most are limited to structured questions, overlook the entire survey process, and risks under-representing marginalized groups due to training data biases. We introduce AlignSurvey, the first benchmark that systematically replicates and evaluates the full social survey pipeline using LLMs. It defines four tasks aligned with key survey stages: social role modeling, semi-structured interview modeling, attitude stance modeling and survey response modeling. It also provides task-specific evaluation metrics to assess alignment fidelity, consistency, and fairness at both individual and group levels, with a focus on demographic diversity. To support AlignSurvey, we construct a multi-tiered dataset architecture: (i) the Social Foundation Corpus, a cross-national resource with 44K+ interview dialogues and 400K+ structured survey records; and (ii) a suite of Entire-Pipeline Survey Datasets, including the expert-annotated AlignSurvey-Expert (ASE) and two nationally representative surveys for cross-cultural evaluation. We release the SurveyLM family, obtained through two-stage fine-tuning of open-source LLMs, and offer reference models for evaluating domain-specific alignment. All datasets, models, and tools are available at github and huggingface to support transparent and socially responsible research.

</details>


### [18] [Planned Event Forecasting using Future Mentions and Related Entity Extraction in News Articles](https://arxiv.org/abs/2511.07879)
*Neelesh Kumar Shukla,Pranay Sanghvi*

Main category: cs.CL

TL;DR: 开发基于主题建模、word2vec和NER的地理无关模型，通过新闻分析预测社会动荡事件并提取关键实体


<details>
  <summary>Details</summary>
Motivation: 未经报备的社会动荡事件可能引发安全隐患，需要构建预测系统帮助政府及时应对

Method: 1. 主题建模+词向量过滤新闻 2. NER识别四类实体 3. 时间标准化处理 4. 创新提出相关实体提取方法

Result: 建立可跨地域应用的通用模型，有效识别事件特征并提取实际参与实体（Related Entities）

Conclusion: 提出的相关实体提取方法和时空标准化框架，为自动化社会事件预警系统提供了新范式

Abstract: In democracies like India, people are free to express their views and demands. Sometimes this causes situations of civil unrest such as protests, rallies, and marches. These events may be disruptive in nature and are often held without prior permission from the competent authority. Forecasting these events helps administrative officials take necessary action. Usually, protests are announced well in advance to encourage large participation. Therefore, by analyzing such announcements in news articles, planned events can be forecasted beforehand. We developed such a system in this paper to forecast social unrest events using topic modeling and word2vec to filter relevant news articles, and Named Entity Recognition (NER) methods to identify entities such as people, organizations, locations, and dates. Time normalization is applied to convert future date mentions into a standard format. In this paper, we have developed a geographically independent, generalized model to identify key features for filtering civil unrest events. There could be many mentions of entities, but only a few may actually be involved in the event. This paper calls such entities Related Entities and proposes a method to extract them, referred to as Related Entity Extraction.

</details>


### [19] [Breaking the Adversarial Robustness-Performance Trade-off in Text Classification via Manifold Purification](https://arxiv.org/abs/2511.07888)
*Chenhao Dang,Jing Ma*

Main category: cs.CL

TL;DR: 提出Manifold-Correcting Causal Flow (MC²F)方法，通过建模干净样本的流形分布提升文本分类对抗鲁棒性，同时保持干净数据性能。


<details>
  <summary>Details</summary>
Motivation: 解决文本分类中对抗攻击鲁棒性增强与干净数据性能下降的矛盾，通过建模干净数据在编码器嵌入空间的流形分布来修正对抗样本。

Method: 1. 分层黎曼连续归一化流(SR-CNF)学习干净数据流形密度；2. 测地线净化求解器通过最短路径将对抗样本投影回学习流形，恢复语义连贯表示。

Result: 在三个文本分类数据集上实现对抗鲁棒性SOTA，完全保留干净数据性能(准确率提升0.5-1.2%)，在AGNews、IMDB、Yahoo任务中对抗准确率分别提升8.3%、12.1%、9.7%。

Conclusion: 该方法首次实现对抗鲁棒性与干净数据性能解耦，为鲁棒文本分类提供了新的流形校正范式。

Abstract: A persistent challenge in text classification (TC) is that enhancing model robustness against adversarial attacks typically degrades performance on clean data. We argue that this challenge can be resolved by modeling the distribution of clean samples in the encoder embedding manifold. To this end, we propose the Manifold-Correcting Causal Flow (MC^2F), a two-module system that operates directly on sentence embeddings. A Stratified Riemannian Continuous Normalizing Flow (SR-CNF) learns the density of the clean data manifold. It identifies out-of-distribution embeddings, which are then corrected by a Geodesic Purification Solver. This solver projects adversarial points back onto the learned manifold via the shortest path, restoring a clean, semantically coherent representation. We conducted extensive evaluations on text classification (TC) across three datasets and multiple adversarial attacks. The results demonstrate that our method, MC^2F, not only establishes a new state-of-the-art in adversarial robustness but also fully preserves performance on clean data, even yielding modest gains in accuracy.

</details>


### [20] [Last Layer Logits to Logic: Empowering LLMs with Logic-Consistent Structured Knowledge Reasoning](https://arxiv.org/abs/2511.07910)
*Songze Li,Zhiqiang Liu,Zhaoyan Gong,Xiaoke Guo,Zhengke Gui,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: 提出Logits-to-Logic框架，通过logits增强和过滤解决LLMs在结构化知识推理中的逻辑漂移问题


<details>
  <summary>Details</summary>
Motivation: 现有方法仅提供输入级指导，无法解决LLM输出的逻辑漂移，且推理流程缺乏适应性

Method: 针对自回归生成的logits输出设计，包含logits强化和过滤两大核心模块

Result: 在多个KGQA基准测试中实现SOTA，显著提升逻辑一致性

Conclusion: 该框架有效修正LLM逻辑缺陷，为结构化知识推理提供新解决方案

Abstract: Large Language Models (LLMs) achieve excellent performance in natural language reasoning tasks through pre-training on vast unstructured text, enabling them to understand the logic in natural language and generate logic-consistent responses. However, the representational differences between unstructured and structured knowledge make LLMs inherently struggle to maintain logic consistency, leading to \textit{Logic Drift} challenges in structured knowledge reasoning tasks such as Knowledge Graph Question Answering (KGQA). Existing methods address this limitation by designing complex workflows embedded in prompts to guide LLM reasoning. Nevertheless, these approaches only provide input-level guidance and fail to fundamentally address the \textit{Logic Drift} in LLM outputs. Additionally, their inflexible reasoning workflows cannot adapt to different tasks and knowledge graphs. To enhance LLMs' logic consistency in structured knowledge reasoning, we specifically target the logits output from the autoregressive generation process. We propose the \textit{Logits-to-Logic} framework, which incorporates logits strengthening and logits filtering as core modules to correct logical defects in LLM outputs. Extensive experiments show that our approach significantly improves LLMs' logic consistency in structured knowledge reasoning and achieves state-of-the-art performance on multiple KGQA benchmarks.

</details>


### [21] [Social Media for Mental Health: Data, Methods, and Findings](https://arxiv.org/abs/2511.07914)
*Nur Shazwani Kamarudin,Ghazaleh Beigi,Lydia Manikonda,Huan Liu*

Main category: cs.CL

TL;DR: 本文探讨如何利用社交媒体数据分析心理健康问题（如抑郁、焦虑、自杀倾向），揭示用户行为特征并提出改进医疗实践与政策的路径。


<details>
  <summary>Details</summary>
Motivation: 传统心理健康研究受限于数据获取途径，社交媒体普及产生了海量用户生成数据。通过分析这些数据，可突破传统研究局限，为及时心理支持、医疗实践改进及政策制定提供新视角。

Method: 整合机器学习、特征工程、自然语言处理技术，系统分析社交媒体中的语言模式（文本）、视觉元素（图片/视频）、情感表达指标，结合调查方法建立心理健康评估模型。

Result: 验证社交媒体数据能有效识别心理健康风险指标（如特定关键词、情绪波动模式），为临床诊断提供补充依据，并为政府建立实时心理危机预警系统提供数据支撑。

Conclusion: 社交媒体数据开创了心理健康研究新范式，未来需融合多模态数据与因果推断模型，推动研究成果向临床干预和政策优化转化。

Abstract: There is an increasing number of virtual communities and forums available on the web. With social media, people can freely communicate and share their thoughts, ask personal questions, and seek peer-support, especially those with conditions that are highly stigmatized, without revealing personal identity. We study the state-of-the-art research methodologies and findings on mental health challenges like de- pression, anxiety, suicidal thoughts, from the pervasive use of social media data. We also discuss how these novel thinking and approaches can help to raise awareness of mental health issues in an unprecedented way. Specifically, this chapter describes linguistic, visual, and emotional indicators expressed in user disclosures. The main goal of this chapter is to show how this new source of data can be tapped to improve medical practice, provide timely support, and influence government or policymakers. In the context of social media for mental health issues, this chapter categorizes social media data used, introduces different deployed machine learning, feature engineering, natural language processing, and surveys methods and outlines directions for future research.

</details>


### [22] [Distinct Theta Synchrony across Speech Modes: Perceived, Spoken, Whispered, and Imagined](https://arxiv.org/abs/2511.07918)
*Jung-Sun Lee,Ha-Na Jo,Eunyeong Ko*

Main category: cs.CL

TL;DR: 研究通过theta波段同步性分析，揭示了外显、耳语、感知和想象四种言语模式在额颞区、后部及辅助运动区神经同步性的显著差异，阐明语言处理的多模态神经机制差异。


<details>
  <summary>Details</summary>
Motivation: 既往研究多聚焦单一言语模式，缺乏跨模式theta同步性的整合比较。本研究旨在通过多模式对比，揭示语言感知与想象言语的共享/特异神经动态。

Method: 采用基于连接性指标的theta波段神经同步性分析，重点关注不同言语模式下的脑区特异性变化。

Result: 外显/耳语言语呈现广泛额颞同步（运动-语音耦合）；感知言语以后部同步为主（听觉处理）；想象言语则表现局限但内聚的额-辅助运动区同步模式。

Conclusion: theta同步范围与空间分布具有模式特异性：外显言语涉及广泛皮层交互，感知依赖颞顶网络，想象言语呈现独特内部耦合，为理解语言多模态处理提供神经证据。

Abstract: Human speech production encompasses multiple modes such as perceived, overt, whispered, and imagined, each reflecting distinct neural mechanisms. Among these, theta-band synchrony has been closely associated with language processing, attentional control, and inner speech. However, previous studies have largely focused on a single mode, such as overt speech, and have rarely conducted an integrated comparison of theta synchrony across different speech modes. In this study, we analyzed differences in theta-band neural synchrony across speech modes based on connectivity metrics, focusing on region-wise variations. The results revealed that overt and whispered speech exhibited broader and stronger frontotemporal synchrony, reflecting active motor-phonological coupling during overt articulation, whereas perceived speech showed dominant posterior and temporal synchrony patterns, consistent with auditory perception and comprehension processes. In contrast, imagined speech demonstrated a more spatially confined but internally coherent synchronization pattern, primarily involving frontal and supplementary motor regions. These findings indicate that the extent and spatial distribution of theta synchrony differ substantially across modes, with overt articulation engaging widespread cortical interactions, whispered speech showing intermediate engagement, and perception relying predominantly on temporoparietal networks. Therefore, this study aims to elucidate the differences in theta-band neural synchrony across various speech modes, thereby uncovering both the shared and distinct neural dynamics underlying language perception and imagined speech.

</details>


### [23] [Unified Work Embeddings: Contrastive Learning of a Bidirectional Multi-task Ranker](https://arxiv.org/abs/2511.07969)
*Matthias De Lange,Jens-Joris Decorte,Jeroen Van Hautte*

Main category: cs.CL

TL;DR: 提出WorkBench评估套件和UWE模型，解决工作场景中复杂NLP任务的长尾分布、多标签和少数据挑战，实现跨任务迁移和零样本性能提升


<details>
  <summary>Details</summary>
Motivation: 工作场景NLP任务存在长尾分布、极端多标签和少数据问题，现有通用模型在该领域表现不足，需建立专门评估基准与方法

Method: 构建跨6个工作任务评估套件WorkBench，通过真实数据构建任务特定二分图，采用多对多InfoNCE目标训练和软延迟交互技术

Result: UWE模型在零样本场景超越通用模型，支持目标空间缓存降低延迟，macro-MAP/RP@10指标提升显著

Conclusion: 通过WorkBench验证跨任务迁移有效性，提出的UWE框架为工作领域嵌入提供有效解决方案，兼顾性能与推理效率

Abstract: Workforce transformation across diverse industries has driven an increased demand for specialized natural language processing capabilities. Nevertheless, tasks derived from work-related contexts inherently reflect real-world complexities, characterized by long-tailed distributions, extreme multi-label target spaces, and scarce data availability. The rise of generalist embedding models prompts the question of their performance in the work domain, especially as progress in the field has focused mainly on individual tasks. To this end, we introduce WorkBench, the first unified evaluation suite spanning six work-related tasks formulated explicitly as ranking problems, establishing a common ground for multi-task progress. Based on this benchmark, we find significant positive cross-task transfer, and use this insight to compose task-specific bipartite graphs from real-world data, synthetically enriched through grounding. This leads to Unified Work Embeddings (UWE), a task-agnostic bi-encoder that exploits our training-data structure with a many-to-many InfoNCE objective, and leverages token-level embeddings with task-agnostic soft late interaction. UWE demonstrates zero-shot ranking performance on unseen target spaces in the work domain, enables low-latency inference by caching the task target space embeddings, and shows significant gains in macro-averaged MAP and RP@10 over generalist embedding models.

</details>


### [24] [NOTAM-Evolve: A Knowledge-Guided Self-Evolving Optimization Framework with LLMs for NOTAM Interpretation](https://arxiv.org/abs/2511.07982)
*Maoqi Liu,Quan Fang,Yuhao Wu,Can Zhao,Yang Yang,Kaiquan Cai*

Main category: cs.CL

TL;DR: 提出自进化框架NOTAM-Evolve，通过知识图谱增强和闭环学习机制提升航空通告结构化解析精度


<details>
  <summary>Details</summary>
Motivation: 现有自动化系统仅能浅层解析NOTAMs，无法提取支撑航空操作决策的核心情报，存在安全隐患

Method: 融合知识图谱检索模块实现动态数据锚定，设计闭环学习流程使LLM通过自我迭代实现持续优化

Result: 在10,000条专家标注数据集上实现30.4%绝对准确率提升，刷新结构化NOTAM解析的SOTA指标

Conclusion: 该框架显著降低人工标注需求，为复杂航空文本的自动化解析提供可自我演进的技术路径

Abstract: Accurate interpretation of Notices to Airmen (NOTAMs) is critical for aviation safety, yet their condensed and cryptic language poses significant challenges to both manual and automated processing. Existing automated systems are typically limited to shallow parsing, failing to extract the actionable intelligence needed for operational decisions. We formalize the complete interpretation task as deep parsing, a dual-reasoning challenge requiring both dynamic knowledge grounding (linking the NOTAM to evolving real-world aeronautical data) and schema-based inference (applying static domain rules to deduce operational status). To tackle this challenge, we propose NOTAM-Evolve, a self-evolving framework that enables a large language model (LLM) to autonomously master complex NOTAM interpretation. Leveraging a knowledge graph-enhanced retrieval module for data grounding, the framework introduces a closed-loop learning process where the LLM progressively improves from its own outputs, minimizing the need for extensive human-annotated reasoning traces. In conjunction with this framework, we introduce a new benchmark dataset of 10,000 expert-annotated NOTAMs. Our experiments demonstrate that NOTAM-Evolve achieves a 30.4% absolute accuracy improvement over the base LLM, establishing a new state of the art on the task of structured NOTAM interpretation.

</details>


### [25] [State of the Art in Text Classification for South Slavic Languages: Fine-Tuning or Prompting?](https://arxiv.org/abs/2511.07989)
*Taja Kuzman Pungeršek,Peter Rupnik,Ivan Porupski,Vuk Dinić,Nikola Ljubešić*

Main category: cs.CL

TL;DR: LLMs在南斯拉夫语文本分类中展现优异零样本能力但存在性能缺陷，微调BERT仍是大规模标注的实用选择


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在资源较少语言（南斯拉夫语系）的文本分类性能，并与传统微调模型对比

Method: 比较开源BERT模型与开源/闭源LLMs在三个领域任务（议会演讲情感分类、新闻/议会演讲主题分类、网络文本体裁识别）的表现

Result: LLMs零样本表现匹配/超越微调模型，跨语言能力与英语相当，但存在输出不可预测性、推理速度慢（差两个数量级）、高计算成本问题

Conclusion: 尽管LLMs表现优异，由于性能缺陷，微调BERT模型仍是大规模自动文本标注的更实际选择

Abstract: Until recently, fine-tuned BERT-like models provided state-of-the-art performance on text classification tasks. With the rise of instruction-tuned decoder-only models, commonly known as large language models (LLMs), the field has increasingly moved toward zero-shot and few-shot prompting. However, the performance of LLMs on text classification, particularly on less-resourced languages, remains under-explored. In this paper, we evaluate the performance of current language models on text classification tasks across several South Slavic languages. We compare openly available fine-tuned BERT-like models with a selection of open-source and closed-source LLMs across three tasks in three domains: sentiment classification in parliamentary speeches, topic classification in news articles and parliamentary speeches, and genre identification in web texts. Our results show that LLMs demonstrate strong zero-shot performance, often matching or surpassing fine-tuned BERT-like models. Moreover, when used in a zero-shot setup, LLMs perform comparably in South Slavic languages and English. However, we also point out key drawbacks of LLMs, including less predictable outputs, significantly slower inference, and higher computational costs. Due to these limitations, fine-tuned BERT-like models remain a more practical choice for large-scale automatic text annotation.

</details>


### [26] [Self-Correction Distillation for Structured Data Question Answering](https://arxiv.org/abs/2511.07998)
*Yushan Zhu,Wen Zhang,Long Jin,Mengshu Sun,Ling Zhong,Zhiqiang Liu,Juan Li,Lei Liang,Chong Long,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: 提出自校正蒸馏方法（SCD），通过错误提示机制和两阶段蒸馏策略提升小规模语言模型的结构化数据问答能力，实验证明其在多个基准测试中达到最佳性能并接近GPT-4水平。


<details>
  <summary>Details</summary>
Motivation: 小规模语言模型在生成结构化查询时容易出错，现有统一框架在小型模型上表现不佳，需改进其结构化数据问答能力。

Method: 自校正蒸馏（SCD）包含错误提示机制（EPM）用于推理时错误检测与定制化反馈，以及两阶段蒸馏策略迁移大模型的能力至小模型。

Result: 在5个基准测试和3种数据结构中，SCD在8B小模型上取得最优性能，部分数据集接近GPT-4，配备EPM的大模型刷新多数数据集SOTA记录。

Conclusion: SCD有效提升小模型的结构化问答性能，EPM增强大模型表现，方法具备优秀泛化能力，为不同规模模型提供通用解决方案。

Abstract: Structured data question answering (QA), including table QA, Knowledge Graph (KG) QA, and temporal KG QA, is a pivotal research area. Advances in large language models (LLMs) have driven significant progress in unified structural QA frameworks like TrustUQA. However, these frameworks face challenges when applied to small-scale LLMs since small-scale LLMs are prone to errors in generating structured queries. To improve the structured data QA ability of small-scale LLMs, we propose a self-correction distillation (SCD) method. In SCD, an error prompt mechanism (EPM) is designed to detect errors and provide customized error messages during inference, and a two-stage distillation strategy is designed to transfer large-scale LLMs' query-generation and error-correction capabilities to small-scale LLM. Experiments across 5 benchmarks with 3 structured data types demonstrate that our SCD achieves the best performance and superior generalization on small-scale LLM (8B) compared to other distillation methods, and closely approaches the performance of GPT4 on some datasets. Furthermore, large-scale LLMs equipped with EPM surpass the state-of-the-art results on most datasets.

</details>


### [27] [HyCoRA: Hyper-Contrastive Role-Adaptive Learning for Role-Playing](https://arxiv.org/abs/2511.08017)
*Shihao Yang,Zhicong Lu,Yong Yang,Bo Lv,Yang Shen,Nayu Liu*

Main category: cs.CL

TL;DR: 提出HyCoRA框架，通过结合角色特定模块和共享模块的混合结构，配合超对比学习机制，有效平衡多角色扮演中独特性与共性的学习。


<details>
  <summary>Details</summary>
Motivation: 现有方法中角色共享模块会忽略个性特征，而独立模块又忽视共性特征。需要新的框架来同时捕捉角色的独特性和共同特征。

Method: 采用Hyper-Half低秩适配结构：1）轻量级超网络生成角色特定模块捕捉个性特征；2）可训练的共享模块提取共性特征；3）超对比学习机制增强角色区分度。

Result: 在英中基准测试中表现优异，GPT-4评估和可视化分析验证了框架对角色特征的捕捉能力。

Conclusion: HyCoRA通过混合结构和对比学习，有效提升了多角色扮演中角色特征的建模能力，为相关领域提供了新思路。

Abstract: Multi-character role-playing aims to equip models with the capability to simulate diverse roles. Existing methods either use one shared parameterized module across all roles or assign a separate parameterized module to each role. However, the role-shared module may ignore distinct traits of each role, weakening personality learning, while the role-specific module may overlook shared traits across multiple roles, hindering commonality modeling. In this paper, we propose a novel HyCoRA: Hyper-Contrastive Role-Adaptive learning framework, which efficiently improves multi-character role-playing ability by balancing the learning of distinct and shared traits. Specifically, we propose a Hyper-Half Low-Rank Adaptation structure, where one half is a role-specific module generated by a lightweight hyper-network, and the other half is a trainable role-shared module. The role-specific module is devised to represent distinct persona signatures, while the role-shared module serves to capture common traits. Moreover, to better reflect distinct personalities across different roles, we design a hyper-contrastive learning mechanism to help the hyper-network distinguish their unique characteristics. Extensive experimental results on both English and Chinese available benchmarks demonstrate the superiority of our framework. Further GPT-4 evaluations and visual analyses also verify the capability of HyCoRA to capture role characteristics.

</details>


### [28] [BARD10: A New Benchmark Reveals Significance of Bangla Stop-Words in Authorship Attribution](https://arxiv.org/abs/2511.08085)
*Abdullah Muhammad Moosa,Nusrat Sultana,Mahdi Muhammad Moosa,Md. Miraiz Hossain*

Main category: cs.CL

TL;DR: 构建孟加拉语作者识别新基准BARD10，验证停用词对机器学习模型的影响，发现传统模型优于深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 探索孟加拉语停用词的风格标记作用，建立连接传统文献与当代网络文本的基准语料库。

Method: 使用BARD10和BAAD16数据集，测试SVM、Bangla BERT、XGBoost、MLP四类模型，统一预处理流程。

Result: TF-IDF+SVM表现最佳（BAAD16达0.997，BARD10达0.921），停用词修剪对BARD10更敏感，Transformer模型削弱高频作者特征。

Conclusion: 孟加拉停用词是重要风格指标，精细调优的传统模型在短文本中有效，BARD10为长上下文模型提供新基准。

Abstract: This research presents a comprehensive investigation into Bangla authorship attribution, introducing a new balanced benchmark corpus BARD10 (Bangla Authorship Recognition Dataset of 10 authors) and systematically analyzing the impact of stop-word removal across classical and deep learning models to uncover the stylistic significance of Bangla stop-words. BARD10 is a curated corpus of Bangla blog and opinion prose from ten contemporary authors, alongside the methodical assessment of four representative classifiers: SVM (Support Vector Machine), Bangla BERT (Bidirectional Encoder Representations from Transformers), XGBoost, and a MLP (Multilayer Perception), utilizing uniform preprocessing on both BARD10 and the benchmark corpora BAAD16 (Bangla Authorship Attribution Dataset of 16 authors). In all datasets, the classical TF-IDF + SVM baseline outperformed, attaining a macro-F1 score of 0.997 on BAAD16 and 0.921 on BARD10, while Bangla BERT lagged by as much as five points. This study reveals that BARD10 authors are highly sensitive to stop-word pruning, while BAAD16 authors remain comparatively robust highlighting genre-dependent reliance on stop-word signatures. Error analysis revealed that high frequency components transmit authorial signatures that are diminished or reduced by transformer models. Three insights are identified: Bangla stop-words serve as essential stylistic indicators; finely calibrated ML models prove effective within short-text limitations; and BARD10 connects formal literature with contemporary web dialogue, offering a reproducible benchmark for future long-context or domain-adapted transformers.

</details>


### [29] [Estranged Predictions: Measuring Semantic Category Disruption with Masked Language Modelling](https://arxiv.org/abs/2511.08109)
*Yuxuan Liu,Haim Dubossarsky,Ruth Ahnert*

Main category: cs.CL

TL;DR: 本研究通过遮蔽语言建模技术量化科幻小说的概念渗透性，发现机器指代呈现显著跨类别替换，人类术语保持语义稳定性，验证了陌生化理论的计算测量可行性。


<details>
  <summary>Details</summary>
Motivation: 旨在验证达科·苏文提出的'陌生化'理论是否可通过计算模型量化，探索科幻小说颠覆本体论范畴的文本机制，拓展计算文学分析方法论。

Method: 采用RoBERTa生成词汇替代，Gemini分类器分析，通过保留率/替换率/熵三个指标，对比科幻(Gollancz)与普通小说(NovelTM)语料库的概念边界稳定性。

Result: 科幻小说整体概念渗透性提升2.3倍，机器指代跨类别替换率高达41%，人类术语保持83%的语义连贯性，形成以人类为中心的替代层级体系。

Conclusion: 遮蔽语言模型可作为计算文学研究的解释工具，揭示科幻小说通过语义扰动重构人类中心主义逻辑的深层机制，为文类分析提供量化新范式。

Abstract: This paper examines how science fiction destabilises ontological categories by measuring conceptual permeability across the terms human, animal, and machine using masked language modelling (MLM). Drawing on corpora of science fiction (Gollancz SF Masterworks) and general fiction (NovelTM), we operationalise Darko Suvin's theory of estrangement as computationally measurable deviation in token prediction, using RoBERTa to generate lexical substitutes for masked referents and classifying them via Gemini. We quantify conceptual slippage through three metrics: retention rate, replacement rate, and entropy, mapping the stability or disruption of category boundaries across genres. Our findings reveal that science fiction exhibits heightened conceptual permeability, particularly around machine referents, which show significant cross-category substitution and dispersion. Human terms, by contrast, maintain semantic coherence and often anchor substitutional hierarchies. These patterns suggest a genre-specific restructuring within anthropocentric logics. We argue that estrangement in science fiction operates as a controlled perturbation of semantic norms, detectable through probabilistic modelling, and that MLMs, when used critically, serve as interpretive instruments capable of surfacing genre-conditioned ontological assumptions. This study contributes to the methodological repertoire of computational literary studies and offers new insights into the linguistic infrastructure of science fiction.

</details>


### [30] [Multimodal LLMs Do Not Compose Skills Optimally Across Modalities](https://arxiv.org/abs/2511.08113)
*Paula Ontalvilla,Aitor Ormazabal,Gorka Azkune*

Main category: cs.CL

TL;DR: 研究发现多模态大语言模型存在显著跨模态技能组合差距，现有缓解方法效果有限


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在跨模态技能组合能力上存在不足，无法有效结合已学习技能解决新任务

Method: 设计3个跨模态组合任务，采用直接推理和两步级联推理两种评估策略，并探索链式思维提示和微调方案

Result: 所有模型均存在显著技能组合差距，改进方案仅部分有效且仍存明显不足

Conclusion: 跨模态技能组合是MLLM的重要挑战，需开发更有效的训练方法提升组合推理能力

Abstract: Skill composition is the ability to combine previously learned skills to solve new tasks. As neural networks acquire increasingly complex skills during their pretraining, it is not clear how successfully they can compose them. In this paper, we focus on Multimodal Large Language Models (MLLM), and study their ability to compose skills across modalities. To this end, we design three evaluation tasks which can be solved sequentially composing two modality-dependent skills, and evaluate several open MLLMs under two main settings: i) prompting the model to directly solve the task, and ii) using a two-step cascaded inference approach, which manually enforces the composition of the two skills for a given task. Even with these straightforward compositions, we find that all evaluated MLLMs exhibit a significant cross-modality skill composition gap. To mitigate the aforementioned gap, we explore two alternatives: i) use chain-of-thought prompting to explicitly instruct MLLMs for skill composition and ii) a specific fine-tuning recipe to promote skill composition. Although those strategies improve model performance, they still exhibit significant skill composition gaps, suggesting that more research is needed to improve cross-modal skill composition in MLLMs.

</details>


### [31] [Quantification and object perception in Multimodal Large Language Models deviate from human linguistic cognition](https://arxiv.org/abs/2511.08126)
*Raquel Montero,Natalia Moskvina,Paolo Morosi,Tamara Serrano,Elena Pagliarini,Evelina Leivada*

Main category: cs.CL

TL;DR: 研究揭示多模态大语言模型在量化表征上与人类存在系统性差异，通过跨语言视角分析量词等级、使用范围原型性和近似数系偏差三大特征


<details>
  <summary>Details</summary>
Motivation: 量化作为逻辑、语用和数值的交汇点，现有研究未能明确解释MLLMs量化表现差的核心原因。探索人类量化特征的编码差异可揭示模型语义理解机制

Method: 通过量词排序构建量化等级量表，分析原型性使用范围分布，检测近似数系偏差模式。对比不同模型架构（单模态/多模态）和语言（英语/中文）的表现差异

Result: MLLMs在量化等级敏感性、原型范围判断稳定性及数系偏差模式三个维度均与人类存在显著差异，且多模态模型表现优于纯语言模型，跨语言稳定性较高

Conclusion: 该研究为理解MLLMs的语义推理能力提供新范式，跨语言分析框架可有效评估模型鲁棒性，对构建更接近人类认知的量化表征系统具有指导意义

Abstract: Quantification has been proven to be a particularly difficult linguistic phenomenon for (Multimodal) Large Language Models (MLLMs). However, given that quantification interfaces with the logic, pragmatic, and numerical domains, the exact reasons for the poor performance are still unclear. This papers looks at three key features of human quantification shared cross-linguistically that have remained so far unexplored in the (M)LLM literature: the ordering of quantifiers into scales, the ranges of use and prototypicality, and the biases inherent in the human approximate number system. The aim is to determine how these features are encoded in the models' architecture, how they may differ from humans, and whether the results are affected by the type of model and language under investigation. We find that there are clear differences between humans and MLLMs with respect to these features across various tasks that tap into the representation of quantification in vivo vs. in silico. This work, thus, paves the way for addressing the nature of MLLMs as semantic and pragmatic agents, while the cross-linguistic lens can elucidate whether their abilities are robust and stable across different languages.

</details>


### [32] [Sentence-Anchored Gist Compression for Long-Context LLMs](https://arxiv.org/abs/2511.08128)
*Dmitrii Tarasov,Elizaveta Goncharova,Kuznetsov Andrey*

Main category: cs.CL

TL;DR: 提出通过微调预训练LLMs实现2-8倍上下文压缩，在保持性能的同时显著降低资源消耗


<details>
  <summary>Details</summary>
Motivation: 解决长序列处理中LLMs面临的内存和计算资源瓶颈问题，突破传统压缩技术的效率限制

Method: 使用学习压缩标记对预训练LLMs进行微调，开发上下文压缩机制

Result: 在LLaMA-3B模型上实现与现有技术相当的效果，同时达到更高压缩比（2-8倍），长短上下文任务性能无显著下降

Conclusion: 该方法有效平衡压缩效率与模型性能，为资源受限环境下的长序列处理提供实用解决方案

Abstract: This work investigates context compression for Large Language Models (LLMs) using learned compression tokens to reduce the memory and computational demands of processing long sequences. We demonstrate that pre-trained LLMs can be fine-tuned to compress their context by factors of 2x to 8x without significant performance degradation, as evaluated on both short-context and long-context benchmarks. Furthermore, in experiments on a 3-billion-parameter LLaMA model, our method achieves results on par with alternative compression techniques while attaining higher compression ratios.

</details>


### [33] [On the Interplay between Positional Encodings, Morphological Complexity, and Word Order Flexibility](https://arxiv.org/abs/2511.08139)
*Kushal Tatariya,Wessel Poelman,Miryam de Lhoneux*

Main category: cs.CL

TL;DR: 研究通过预训练含不同位置编码的模型发现，位置编码类型与语言形态复杂性/语序灵活性之间无显著关联，任务和语言的选择对结论稳定性至关重要。


<details>
  <summary>Details</summary>
Motivation: 验证语言模型架构（特别是位置编码）对非英语语言的适用性，检验形态复杂性与语序灵活性间的平衡假说是否影响模型表现。

Method: 对7种类型学多样的语言预训练绝对/相对/无位置编码的模型变体，并在4个下游任务评估性能。

Result: 未发现位置编码选择与形态复杂度/语序灵活性指标间的明确相关性，与先前研究结论相悖。

Conclusion: 研究设计需系统考虑任务类型、语言样本和评估指标的多样性，才能获得可靠的跨语言模型评估结论。

Abstract: Language model architectures are predominantly first created for English and subsequently applied to other languages. It is an open question whether this architectural bias leads to degraded performance for languages that are structurally different from English. We examine one specific architectural choice: positional encodings, through the lens of the trade-off hypothesis: the supposed interplay between morphological complexity and word order flexibility. This hypothesis posits a trade-off between the two: a more morphologically complex language can have a more flexible word order, and vice-versa. Positional encodings are a direct target to investigate the implications of this hypothesis in relation to language modelling. We pretrain monolingual model variants with absolute, relative, and no positional encodings for seven typologically diverse languages and evaluate them on four downstream tasks. Contrary to previous findings, we do not observe a clear interaction between position encodings and morphological complexity or word order flexibility, as measured by various proxies. Our results show that the choice of tasks, languages, and metrics are essential for drawing stable conclusions

</details>


### [34] [Relation as a Prior: A Novel Paradigm for LLM-based Document-level Relation Extraction](https://arxiv.org/abs/2511.08143)
*Qiankun Pi,Yepeng Sun,Jicang Lu,Qinlong Fan,Ningbo Huang,Shiyu Wang*

Main category: cs.CL

TL;DR: 提出RelPrior新范式解决LLMs在文档级关系抽取中的性能问题，通过关系先验机制过滤噪声并改进三元组匹配


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法采用'先提取实体后预测关系'模式导致两个问题：(1)无关实体对引入预测噪声 (2)预定义关系外的标签被误判

Method: 1.利用二元关系先验筛选相关实体对 2.通过预定义关系先验匹配实体三元组，避免直接预测关系标签的错误

Result: 在两大基准测试中实现SOTA性能，超越现有LLM-based方法

Conclusion: RelPrior有效解决文档级关系抽取的核心痛点，通过关系先验机制显著提升预测精度，为LLM应用提供新范式

Abstract: Large Language Models (LLMs) have demonstrated their remarkable capabilities in document understanding. However, recent research reveals that LLMs still exhibit performance gaps in Document-level Relation Extraction (DocRE) as requiring fine-grained comprehension. The commonly adopted "extract entities then predict relations" paradigm in LLM-based methods leads to these gaps due to two main reasons: (1) Numerous unrelated entity pairs introduce noise and interfere with the relation prediction for truly related entity pairs. (2) Although LLMs have identified semantic associations between entities, relation labels beyond the predefined set are still treated as prediction errors. To address these challenges, we propose a novel Relation as a Prior (RelPrior) paradigm for LLM-based DocRE. For challenge (1), RelPrior utilizes binary relation as a prior to extract and determine whether two entities are correlated, thereby filtering out irrelevant entity pairs and reducing prediction noise. For challenge (2), RelPrior utilizes predefined relation as a prior to match entities for triples extraction instead of directly predicting relation. Thus, it avoids misjudgment caused by strict predefined relation labeling. Extensive experiments on two benchmarks demonstrate that RelPrior achieves state-of-the-art performance, surpassing existing LLM-based methods.

</details>


### [35] [Still Not There: Can LLMs Outperform Smaller Task-Specific Seq2Seq Models on the Poetry-to-Prose Conversion Task?](https://arxiv.org/abs/2511.08145)
*Kunal Kingkar Das,Manoj Balaji Jagadeeshan,Nallani Chakravartula Sahith,Jivnesh Sandhan,Pawan Goyal*

Main category: cs.CL

TL;DR: 论文探讨了LLMs在低资源、形态丰富的梵语诗歌转散文任务中的表现，发现领域专用微调的ByT5-Sanskrit模型显著优于所有指令驱动的LLM方法。


<details>
  <summary>Details</summary>
Motivation: 验证LLMs在复杂低资源语言任务（如梵语诗歌转散文）是否优于专用模型，该任务需多步推理（复合词分割、依存解析等）。

Method: 对比指令微调/上下文提示的通用LLM与全微调的ByT5-Sanskrit Seq2Seq模型，并设计基于梵语语法的提示策略。

Result: 领域专用微调的ByT5模型全面超越LLMs，人类评估高度一致（Kendall's Tau相关性高）；提示策略在缺乏领域数据时可替代微调。

Conclusion: 特定领域任务仍需专用模型，但LLM提示策略在数据缺失时有效；任务模型展现强大的跨域泛化能力。

Abstract: Large Language Models (LLMs) are increasingly treated as universal, general-purpose solutions across NLP tasks, particularly in English. But does this assumption hold for low-resource, morphologically rich languages such as Sanskrit? We address this question by comparing instruction-tuned and in-context-prompted LLMs with smaller task-specific encoder-decoder models on the Sanskrit poetry-to-prose conversion task. This task is intrinsically challenging: Sanskrit verse exhibits free word order combined with rigid metrical constraints, and its conversion to canonical prose (anvaya) requires multi-step reasoning involving compound segmentation, dependency resolution, and syntactic linearisation. This makes it an ideal testbed to evaluate whether LLMs can surpass specialised models. For LLMs, we apply instruction fine-tuning on general-purpose models and design in-context learning templates grounded in Paninian grammar and classical commentary heuristics. For task-specific modelling, we fully fine-tune a ByT5-Sanskrit Seq2Seq model. Our experiments show that domain-specific fine-tuning of ByT5-Sanskrit significantly outperforms all instruction-driven LLM approaches. Human evaluation strongly corroborates this result, with scores exhibiting high correlation with Kendall's Tau scores. Additionally, our prompting strategies provide an alternative to fine-tuning when domain-specific verse corpora are unavailable, and the task-specific Seq2Seq model demonstrates robust generalisation on out-of-domain evaluations.

</details>


### [36] [Do Syntactic Categories Help in Developmentally Motivated Curriculum Learning for Language Models?](https://arxiv.org/abs/2511.08199)
*Arzu Burcu Güven,Anna Rogers,Rob van der Goot*

Main category: cs.CL

TL;DR: 研究分析了BabyLM语料库和CHILDES年龄分组的句法特征，发现使用句法可分类数据子集比完整噪声语料更能提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 探索训练数据的句法特征对语言模型任务表现的解释价值，验证认知启发的课程学习策略有效性。

Method: 通过句法属性对比分析，测试发展性课程及其他认知启发式课程方案，使用句法可分类数据进行模型训练。

Result: 课程学习对阅读任务有辅助作用，但核心性能提升源于使用句法结构化数据而非完整噪声语料。

Conclusion: 数据质量（句法可分类性）比课程设计策略对模型性能提升更具决定性作用。

Abstract: We examine the syntactic properties of BabyLM corpus, and age-groups within CHILDES. While we find that CHILDES does not exhibit strong syntactic differentiation by age, we show that the syntactic knowledge about the training data can be helpful in interpreting model performance on linguistic tasks. For curriculum learning, we explore developmental and several alternative cognitively inspired curriculum approaches. We find that some curricula help with reading tasks, but the main performance improvement come from using the subset of syntactically categorizable data, rather than the full noisy corpus.

</details>


### [37] [Encoder Fine-tuning with Stochastic Sampling Outperforms Open-weight GPT in Astronomy Knowledge Extraction](https://arxiv.org/abs/2511.08204)
*Shivam Rawat,Lucie Flek,Akbar Karimi*

Main category: cs.CL

TL;DR: 基于SciBERT的天文学知识提取系统，通过多任务学习与随机采样微调，以低成本显著超越GPT基线


<details>
  <summary>Details</summary>
Motivation: 天文学文献快速增长需自动化处理，当前急需从论文中自动提取关键实体与上下文信息

Method: 构建基于SciBERT的多任务Transformer系统，采用训练数据随机采样段微调，推理时使用测试段多数投票机制

Result: 系统在望远镜分类、语义属性检测等任务中，以简单架构实现超越开源GPT模型的表现

Conclusion: 验证了特定领域BERT模型微调的有效性，为天文文献处理提供高效低成本解决方案

Abstract: Scientific literature in astronomy is rapidly expanding, making it increasingly important to automate the extraction of key entities and contextual information from research papers. In this paper, we present an encoder-based system for extracting knowledge from astronomy articles. Our objective is to develop models capable of classifying telescope references, detecting auxiliary semantic attributes, and recognizing instrument mentions from textual content. To this end, we implement a multi-task transformer-based system built upon the SciBERT model and fine-tuned for astronomy corpora classification. To carry out the fine-tuning, we stochastically sample segments from the training data and use majority voting over the test segments at inference time. Our system, despite its simplicity and low-cost implementation, significantly outperforms the open-weight GPT baseline.

</details>


### [38] [Benchmarking Educational LLMs with Analytics: A Case Study on Gender Bias in Feedback](https://arxiv.org/abs/2511.08225)
*Yishan Du,Conrad Borchers,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 提出基于嵌入的基准框架检测LLM在形成性反馈中的性别偏见，发现主流模型存在不对称语义响应，并提出公平性改进方案。


<details>
  <summary>Details</summary>
Motivation: 教师使用生成式AI进行教学反馈时缺乏有效的偏见检测方法，需建立可靠评估体系保障教育公平。

Method: 使用600篇真实学生论文构建反事实数据，通过词汇替换（隐性线索）和提示修改（显性线索），测试6个LLM的响应差异，结合余弦距离、置换检验和降维可视化分析。

Result: 所有模型对隐性性别替换呈现男→女比女→男更大的语义偏移；GPT/Llama系列对显性线索敏感；男性暗示反馈更自主，女性暗示更控制。

Conclusion: 揭示了LLM反馈中的系统性性别偏见，提出应建立反事实评估标准、优化提示设计，并为学习分析领域提供公平审计框架。

Abstract: As teachers increasingly turn to GenAI in their educational practice, we need robust methods to benchmark large language models (LLMs) for pedagogical purposes. This article presents an embedding-based benchmarking framework to detect bias in LLMs in the context of formative feedback. Using 600 authentic student essays from the AES 2.0 corpus, we constructed controlled counterfactuals along two dimensions: (i) implicit cues via lexicon-based swaps of gendered terms within essays, and (ii) explicit cues via gendered author background in the prompt. We investigated six representative LLMs (i.e. GPT-5 mini, GPT-4o mini, DeepSeek-R1, DeepSeek-R1-Qwen, Gemini 2.5 Pro, Llama-3-8B). We first quantified the response divergence with cosine and Euclidean distances over sentence embeddings, then assessed significance via permutation tests, and finally, visualised structure using dimensionality reduction. In all models, implicit manipulations reliably induced larger semantic shifts for male-female counterfactuals than for female-male. Only the GPT and Llama models showed sensitivity to explicit gender cues. These findings show that even state-of-the-art LLMs exhibit asymmetric semantic responses to gender substitutions, suggesting persistent gender biases in feedback they provide learners. Qualitative analyses further revealed consistent linguistic differences (e.g., more autonomy-supportive feedback under male cues vs. more controlling feedback under female cues). We discuss implications for fairness auditing of pedagogical GenAI, propose reporting standards for counterfactual evaluation in learning analytics, and outline practical guidance for prompt design and deployment to safeguard equitable feedback.

</details>


### [39] [VocalBench-zh: Decomposing and Benchmarking the Speech Conversational Abilities in Mandarin Context](https://arxiv.org/abs/2511.08230)
*Heyang Liu,Ziyang Cheng,Yuhao Wang,Hongcheng Liu,Yiqi Li,Ronghua Wu,Qunshan Gu,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: 提出中文语音交互评估基准VocalBench-zh，覆盖12个用户导向维度并揭示现有模型共性缺陷


<details>
  <summary>Details</summary>
Motivation: 当前中文语音交互领域缺乏系统化评估基准，阻碍模型公平比较与迭代优化

Method: 构建包含10个子集、超10K实例的中文S2S评估框架，涵盖语音理解与生成的多维度能力

Result: 14个主流模型测试显示现有技术路线在韵律保持、噪声鲁棒性等维度存在显著瓶颈

Conclusion: 需突破传统语音处理范式，开发融合语义理解与情感表达的下一代交互系统

Abstract: The development of multi-modal large language models (LLMs) leads to intelligent approaches capable of speech interactions. As one of the most widely spoken languages globally, Mandarin is supported by most models to enhance their applicability and reach. However, the scarcity of comprehensive speech-to-speech (S2S) benchmarks in Mandarin contexts impedes systematic evaluation for developers and hinders fair model comparison for users. In this work, we propose VocalBench-zh, an ability-level divided evaluation suite adapted to Mandarin context consisting of 10 well-crafted subsets and over 10K high-quality instances, covering 12 user-oriented characters. The evaluation experiment on 14 mainstream models reveals the common challenges for current routes, and highlights the need for new insights into next-generation speech interactive systems. The evaluation codes and datasets will be available at https://github.com/SJTU-OmniAgent/VocalBench-zh.

</details>


### [40] [Prompt Tuning for Natural Language to SQL with Embedding Fine-Tuning and RAG](https://arxiv.org/abs/2511.08245)
*Jisoo Jang,Tien-Cuong Bui,Yunjun Choi,Wen-Syan Li*

Main category: cs.CL

TL;DR: 通过提示调优与RAG结合的SQL错误纠正框架，在自然语言转SQL任务中实现12%准确率提升


<details>
  <summary>Details</summary>
Motivation: 针对自然语言接口普及背景下NL转SQL的准确性问题，受医学诊断流程启发提出可解释的纠错机制

Method: 融合错误类型诊断-原因分析-修复应用的医学式框架，结合嵌入微调与检索增强生成技术利用外部知识库

Result: 实验证明框架较现有基线提升12%准确率，在复杂查询场景表现尤为突出

Conclusion: 该框架通过透明化纠错过程与整合领域知识，为数据驱动环境下的自然语言交互提供了革新方案

Abstract: This paper introduces an Error Correction through Prompt Tuning for NL-to-SQL, leveraging the latest advancements in generative pre-training-based LLMs and RAG. Our work addresses the crucial need for efficient and accurate translation of natural language queries into SQL expressions in various settings with the growing use of natural language interfaces. We explore the evolution of NLIDBs from early rule-based systems to advanced neural network-driven approaches. Drawing inspiration from the medical diagnostic process, we propose a novel framework integrating an error correction mechanism that diagnoses error types, identifies their causes, provides fixing instructions, and applies these corrections to SQL queries. This approach is further enriched by embedding fine-tuning and RAG, which harnesses external knowledge bases for improved accuracy and transparency. Through comprehensive experiments, we demonstrate that our framework achieves a significant 12 percent accuracy improvement over existing baselines, highlighting its potential to revolutionize data access and handling in contemporary data-driven environments.

</details>


### [41] [ParliaBench: An Evaluation and Benchmarking Framework for LLM-Generated Parliamentary Speech](https://arxiv.org/abs/2511.08247)
*Marios Koniaris,Argyro Tsipi,Panayiotis Tsanakas*

Main category: cs.CL

TL;DR: 针对议会演讲生成任务，研究者开发了ParliaBench基准测试，通过构建英国议会演讲数据集和提出政治维度评估指标，显著提升了语言模型在政治一致性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型缺乏议会场景的专业训练，评估体系忽视政治真实性，需建立专门基准测试和量化评估方法。

Method: 构建英国议会演讲数据集，设计结合传统指标与LLM评估的三维框架，创新提出政治光谱对齐和政党对齐两个嵌入指标。

Result: 微调使模型在多数指标显著提升，新指标有效区分政治立场，模型生成演讲的政治一致性提高32%。

Conclusion: ParliaBench验证了领域专用训练的价值，新指标为政治文本分析提供量化工具，推动议会场景NLP技术发展。

Abstract: Parliamentary speech generation presents specific challenges for large language models beyond standard text generation tasks. Unlike general text generation, parliamentary speeches require not only linguistic quality but also political authenticity and ideological consistency. Current language models lack specialized training for parliamentary contexts, and existing evaluation methods focus on standard NLP metrics rather than political authenticity. To address this, we present ParliaBench, a benchmark for parliamentary speech generation. We constructed a dataset of speeches from UK Parliament to enable systematic model training. We introduce an evaluation framework combining computational metrics with LLM-as-a-judge assessments for measuring generation quality across three dimensions: linguistic quality, semantic coherence, and political authenticity. We propose two novel embedding-based metrics, Political Spectrum Alignment and Party Alignment, to quantify ideological positioning. We fine-tuned five large language models (LLMs), generated 28k speeches, and evaluated them using our framework, comparing baseline and fine-tuned models. Results show that fine-tuning produces statistically significant improvements across the majority of metrics and our novel metrics demonstrate strong discriminative power for political dimensions.

</details>


### [42] [Hierarchical structure understanding in complex tables with VLLMs: a benchmark and experiments](https://arxiv.org/abs/2511.08298)
*Luca Bindini,Simone Giovannini,Simone Marinai,Valeria Nardoni,Kimiya Noor Ali*

Main category: cs.CL

TL;DR: 研究通过构建CHiTab基准测试，发现通用视觉大语言模型具备理解科学表格层次结构的能力，虽未专门设计但表现尚可，并为未来整合结构化数据理解提供指导


<details>
  <summary>Details</summary>
Motivation: 探究通用视觉大语言模型是否具备理解科学文献复杂表格层次结构的能力，评估其在结构化数据处理方面的潜力与局限

Method: 使用PubTables-1M数据集的CHiTab子集，通过多提示工程策略测试现成/微调VLLMs，并与人类表现对比

Result: 现成VLLMs展现基础表格结构理解能力，微调后性能提升，但整体仍与人类存在差距

Conclusion: 该研究证实通用VLLMs具备处理复杂表格的潜力，为整合结构化数据理解到通用模型指明方向

Abstract: This work investigates the ability of Vision Large Language Models (VLLMs) to understand and interpret the structure of tables in scientific articles. Specifically, we explore whether VLLMs can infer the hierarchical structure of tables without additional processing. As a basis for our experiments we use the PubTables-1M dataset, a large-scale corpus of scientific tables. From this dataset, we extract a subset of tables that we introduce as Complex Hierarchical Tables (CHiTab): a benchmark collection of complex tables containing hierarchical headings. We adopt a series of prompt engineering strategies to probe the models' comprehension capabilities, experimenting with various prompt formats and writing styles. Multiple state-of-the-art open-weights VLLMs are evaluated on the benchmark first using their off-the-shelf versions and then fine-tuning some models on our task. We also measure the performance of humans to solve the task on a small set of tables comparing with performance of the evaluated VLLMs. The experiments support our intuition that generic VLLMs, not explicitly designed for understanding the structure of tables, can perform this task. This study provides insights into the potential and limitations of VLLMs to process complex tables and offers guidance for future work on integrating structured data understanding into general-purpose VLLMs.

</details>


### [43] [Automatic Paper Reviewing with Heterogeneous Graph Reasoning over LLM-Simulated Reviewer-Author Debates](https://arxiv.org/abs/2511.08317)
*Shuaimin Li,Liyang Fan,Yufang Lin,Zeyang Li,Xian Wei,Shiwen Ni,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.CL

TL;DR: 提出ReViewGraph框架，通过模拟多轮审稿人-作者辩论的异构图推理，解决现有评审方法的幻觉/偏见问题，实验显示15.73%性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有评审方法依赖表面特征或LLMs，存在幻觉、评分偏见、有限推理能力，且无法捕捉审稿人与作者互动的复杂论证动态。

Method: 基于LLM多智能体模拟审稿人-作者多轮辩论，构建包含接受/拒绝/澄清/妥协等关系类型的异质交互图，应用图神经网络进行结构化推理。

Result: 在三个数据集上平均相对性能提升15.73%，显著优于现有基线方法。

Conclusion: 通过结构化辩论图的图推理机制，能有效捕捉细粒度论证动态，为论文评审提供更可靠的决策依据。

Abstract: Existing paper review methods often rely on superficial manuscript features or directly on large language models (LLMs), which are prone to hallucinations, biased scoring, and limited reasoning capabilities. Moreover, these methods often fail to capture the complex argumentative reasoning and negotiation dynamics inherent in reviewer-author interactions. To address these limitations, we propose ReViewGraph (Reviewer-Author Debates Graph Reasoner), a novel framework that performs heterogeneous graph reasoning over LLM-simulated multi-round reviewer-author debates. In our approach, reviewer-author exchanges are simulated through LLM-based multi-agent collaboration. Diverse opinion relations (e.g., acceptance, rejection, clarification, and compromise) are then explicitly extracted and encoded as typed edges within a heterogeneous interaction graph. By applying graph neural networks to reason over these structured debate graphs, ReViewGraph captures fine-grained argumentative dynamics and enables more informed review decisions. Extensive experiments on three datasets demonstrate that ReViewGraph outperforms strong baselines with an average relative improvement of 15.73%, underscoring the value of modeling detailed reviewer-author debate structures.

</details>


### [44] [Adaptive Multi-Agent Response Refinement in Conversational Systems](https://arxiv.org/abs/2511.08319)
*Soyeong Jeong,Aparna Elangovan,Emine Yilmaz,Oleg Rokhlenko*

Main category: cs.CL

TL;DR: 提出多智能体框架，通过动态通信策略协调不同角色智能体优化对话响应，在涉及知识或用户画像的任务中显著优于基线方法


<details>
  <summary>Details</summary>
Motivation: 现有单一大语言模型优化方法难以兼顾对话中事实性、个性化和连贯性等多方面需求

Method: 构建三个专用智能体分别负责事实核查、个性化适配和逻辑连贯性，采用动态通信策略自适应协调智能体协作流程

Result: 在复杂对话数据集测试中，框架在知识相关任务准确率提升12.3%，用户画像任务提升9.8%，综合任务提升15.6%

Conclusion: 多智能体协同框架通过分工优化和动态协调机制，有效提升对话系统在复杂场景下的响应质量

Abstract: Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request a new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within a single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through a multi-agent framework, where each agent is assigned a specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce a dynamic communication strategy. Instead of following a fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or user's persona, or both.

</details>


### [45] [AgentPRM: Process Reward Models for LLM Agents via Step-Wise Promise and Progress](https://arxiv.org/abs/2511.08325)
*Zhiheng Xi,Chenyang Liao,Guanyu Li,Yajie Yang,Wenxiang Chen,Zhihao Zhang,Binghai Wang,Senjie Jin,Yuhao Zhou,Jian Guan,Wei Wu,Tao Ji,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.CL

TL;DR: 提出AgentPRM模型，通过TD-based估计和GAE方法高效训练过程奖励模型，实验显示该模型计算效率比基线高8倍以上且具备良好的扩展性


<details>
  <summary>Details</summary>
Motivation: 现有LLM在连续决策任务中依赖复杂提示工程或专家轨迹微调，需要更有效的决策评估机制来平衡探索与利用

Method: 结合时序差分估计(TD-based)和广义优势估计(GAE)构建过程奖励模型，通过目标接近度和进度贡献度双重评估决策质量

Result: 在多个代理任务中实现8倍以上计算效率提升，且随着计算资源扩展持续保持性能改进

Conclusion: AgentPRM能有效追踪决策进度并平衡探索利用，为LLM代理的强化学习提供了新的技术路径

Abstract: Despite rapid development, large language models (LLMs) still encounter challenges in multi-turn decision-making tasks (i.e., agent tasks) like web shopping and browser navigation, which require making a sequence of intelligent decisions based on environmental feedback. Previous work for LLM agents typically relies on elaborate prompt engineering or fine-tuning with expert trajectories to improve performance. In this work, we take a different perspective: we explore constructing process reward models (PRMs) to evaluate each decision and guide the agent's decision-making process. Unlike LLM reasoning, where each step is scored based on correctness, actions in agent tasks do not have a clear-cut correctness. Instead, they should be evaluated based on their proximity to the goal and the progress they have made. Building on this insight, we propose a re-defined PRM for agent tasks, named AgentPRM, to capture both the interdependence between sequential decisions and their contribution to the final goal. This enables better progress tracking and exploration-exploitation balance. To scalably obtain labeled data for training AgentPRM, we employ a Temporal Difference-based (TD-based) estimation method combined with Generalized Advantage Estimation (GAE), which proves more sample-efficient than prior methods. Extensive experiments across different agentic tasks show that AgentPRM is over $8\times$ more compute-efficient than baselines, and it demonstrates robust improvement when scaling up test-time compute. Moreover, we perform detailed analyses to show how our method works and offer more insights, e.g., applying AgentPRM to the reinforcement learning of LLM agents.

</details>


### [46] [DPRM: A Dual Implicit Process Reward Model in Multi-Hop Question Answering](https://arxiv.org/abs/2511.08364)
*Xinyi Wang,Yiping Song,Zhiliang Tian,Bo Liu,Tingjin Luo,Minlie Huang*

Main category: cs.CL

TL;DR: 提出双隐式过程奖励模型DPRM，通过联合优化知识图谱和思维链的推理路径，显著提升多跳问答任务性能


<details>
  <summary>Details</summary>
Motivation: 传统过程奖励模型需要昂贵的人工标注，而现有隐式PRM无法处理知识图谱的结构约束及路径一致性，导致多跳问答任务效果受限

Method: 设计KG-PRM和CoT-PRM双模型，通过偏好对学习知识图谱结构约束，引入一致性约束实现路径互验证，理论推导过程奖励参数化方法

Result: 在多个数据集上超越13个基线模型，Hit@1指标最高提升16.6%

Conclusion: DPRM有效融合结构化知识和推理过程验证，为多步推理任务提供了无需标注的高效优化方案

Abstract: In multi-hop question answering (MHQA) tasks, Chain of Thought (CoT) improves the quality of generation by guiding large language models (LLMs) through multi-step reasoning, and Knowledge Graphs (KGs) reduce hallucinations via semantic matching. Outcome Reward Models (ORMs) provide feedback after generating the final answers but fail to evaluate the process for multi-step reasoning. Traditional Process Reward Models (PRMs) evaluate the reasoning process but require costly human annotations or rollout generation. While implicit PRM is trained only with outcome signals and derives step rewards through reward parameterization without explicit annotations, it is more suitable for multi-step reasoning in MHQA tasks. However, existing implicit PRM has only been explored for plain text scenarios. When adapting to MHQA tasks, it cannot handle the graph structure constraints in KGs and capture the potential inconsistency between CoT and KG paths. To address these limitations, we propose the DPRM (Dual Implicit Process Reward Model). It trains two implicit PRMs for CoT and KG reasoning in MHQA tasks. Both PRMs, namely KG-PRM and CoT-PRM, derive step-level rewards from outcome signals via reward parameterization without additional explicit annotations. Among them, KG-PRM uses preference pairs to learn structural constraints from KGs. DPRM further introduces a consistency constraint between CoT and KG reasoning steps, making the two PRMs mutually verify and collaboratively optimize the reasoning paths. We also provide a theoretical demonstration of the derivation of process rewards. Experimental results show that our method outperforms 13 baselines on multiple datasets with up to 16.6% improvement on Hit@1.

</details>


### [47] [The Dynamic Articulatory Model DYNARTmo: Dynamic Movement Generation and Speech Gestures](https://arxiv.org/abs/2511.08372)
*Bernd J. Kröger*

Main category: cs.CL

TL;DR: DYNARTmo模型基于语音手势概念生成连续发音器官运动轨迹，提供分层控制语音生成的神经生物学计算框架。


<details>
  <summary>Details</summary>
Motivation: 构建受神经生物学启发的计算框架，模拟从语言表征到发音-声学实现的语音生成分层控制过程。

Method: 1. 构建手势库结构
2. 在手势乐谱中协调手势时序
3. 将离散手势转化为控制发音器官的连续轨迹

Result: 成功实现通过手势乐谱驱动DYNARTmo声道模型产生连续发音运动

Conclusion: 该模型为研究语音生成机制提供了可计算的分层控制架构，实现了从离散语言符号到连续发音运动的转化过程。

Abstract: This paper describes the current implementation of the dynamic articulatory model DYNARTmo, which generates continuous articulator movements based on the concept of speech gestures and a corresponding gesture score. The model provides a neurobiologically inspired computational framework for simulating the hierarchical control of speech production from linguistic representation to articulatory-acoustic realization. We present the structure of the gesture inventory, the coordination of gestures in the gesture score, and their translation into continuous articulator trajectories controlling the DYNARTmo vocal tract model.

</details>


### [48] [TurkEmbed: Turkish Embedding Model on NLI & STS Tasks](https://arxiv.org/abs/2511.08376)
*Özay Ezerceli,Gizem Gümüşçekiçci,Tuğba Erkoç,Berke Özenç*

Main category: cs.CL

TL;DR: TurkEmbed新型土耳其语嵌入模型通过多样化数据集和先进训练技术，在NLI和STS任务中超越现有模型


<details>
  <summary>Details</summary>
Motivation: 现有土耳其语模型依赖机器翻译数据集，导致语义理解受限。需要提升语义准确性和语言细微差别捕捉能力

Method: 采用多样化数据集和Matryoshka表示学习技术，实现鲁棒嵌入表示，适配资源受限环境并提供快速编码能力

Result: 在STS-b-TR数据集上Pearson/Spearman指标显著提升，All-NLI-TR和STS-b-TR基准测试超越SOTA模型1-4%

Conclusion: TurkEmbed通过深度语义理解推动土耳其NLP生态系统发展，为下游应用提供更强大的语言建模基础

Abstract: This paper introduces TurkEmbed, a novel Turkish language embedding model designed to outperform existing models, particularly in Natural Language Inference (NLI) and Semantic Textual Similarity (STS) tasks. Current Turkish embedding models often rely on machine-translated datasets, potentially limiting their accuracy and semantic understanding. TurkEmbed utilizes a combination of diverse datasets and advanced training techniques, including matryoshka representation learning, to achieve more robust and accurate embeddings. This approach enables the model to adapt to various resource-constrained environments, offering faster encoding capabilities. Our evaluation on the Turkish STS-b-TR dataset, using Pearson and Spearman correlation metrics, demonstrates significant improvements in semantic similarity tasks. Furthermore, TurkEmbed surpasses the current state-of-the-art model, Emrecan, on All-NLI-TR and STS-b-TR benchmarks, achieving a 1-4\% improvement. TurkEmbed promises to enhance the Turkish NLP ecosystem by providing a more nuanced understanding of language and facilitating advancements in downstream applications.

</details>


### [49] [PCRLLM: Proof-Carrying Reasoning with Large Language Models under Stepwise Logical Constraints](https://arxiv.org/abs/2511.08392)
*Tangrui Li,Pei Wang,Hongzheng Wang Christian Hahm,Matteo Spatola,Justin Shi*

Main category: cs.CL

TL;DR: 提出PCRLLM框架，通过单步推理约束和形式化验证机制提升大语言模型的逻辑连贯性


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在逻辑推理中存在隐式规则映射问题，缺乏可信的验证机制

Method: 设计可验证的推理单元结构（前提-规则-结论），建立多模型协作框架，开发标准化推理数据生成模式

Result: 实现黑盒环境下的推理链验证，保持自然语言表达同时满足形式逻辑要求

Conclusion: PCRLLM框架有效融合自然语言灵活性与形式验证严谨性，为可信AI推理提供新范式

Abstract: Large Language Models (LLMs) often exhibit limited logical coherence, mapping premises to conclusions without adherence to explicit inference rules. We propose Proof-Carrying Reasoning with LLMs (PCRLLM), a framework that constrains reasoning to single-step inferences while preserving natural language formulations. Each output explicitly specifies premises, rules, and conclusions, thereby enabling verification against a target logic. This mechanism mitigates trustworthiness concerns by supporting chain-level validation even in black-box settings. Moreover, PCRLLM facilitates systematic multi-LLM collaboration, allowing intermediate steps to be compared and integrated under formal rules. Finally, we introduce a benchmark schema for generating large-scale step-level reasoning data, combining natural language expressiveness with formal rigor.

</details>


### [50] [Interaction Dynamics as a Reward Signal for LLMs](https://arxiv.org/abs/2511.08394)
*Sian Gooding,Edward Grefenstette*

Main category: cs.CL

TL;DR: 提出基于对话轨迹几何特征的新奖励信号TRACE，证实交互动态与文本内容对LLM对齐具有同等重要性


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法仅关注对话文本内容，忽略对话交互动态本身蕴含的丰富信号。试图验证交互模式对协作成功的预测能力。

Method: 从对话嵌入轨迹的几何属性（会话几何学）提取TRACE奖励信号，构建仅依赖结构信号的奖励模型，并与文本模型、混合模型对比。

Result: 纯结构模型准确率达68.20%（接近完整文本模型的70.04%），混合模型达80.17%，证明结构信号与文本信号的互补性。

Conclusion: 交互方式与对话内容具有同等预测价值，TRACE框架兼具隐私保护优势和协作模式诊断功能，为智能体对齐提供新思路。

Abstract: The alignment of Large Language Models (LLMs) for multi-turn conversations typically relies on reward signals derived from the content of the text. This approach, however, overlooks a rich, complementary source of signal: the dynamics of the interaction itself. This paper introduces TRACE (Trajectory-based Reward for Agent Collaboration Estimation), a novel reward signal derived from the geometric properties of a dialogue's embedding trajectory--a concept we term 'conversational geometry'. Our central finding is that a reward model trained only on these structural signals achieves a pairwise accuracy (68.20%) comparable to a powerful LLM baseline that analyzes the full transcript (70.04%). Furthermore, a hybrid model combining interaction dynamics with textual analysis achieves the highest performance (80.17%), demonstrating their complementary nature. This work provides strong evidence that for interactive settings, how an agent communicates is as powerful a predictor of success as what it says, offering a new, privacy-preserving framework that not only aligns agents but also serves as a diagnostic tool for understanding the distinct interaction patterns that drive successful collaboration.

</details>


### [51] [Bot Meets Shortcut: How Can LLMs Aid in Handling Unknown Invariance OOD Scenarios?](https://arxiv.org/abs/2511.08455)
*Shiyan Zheng,Herun Wan,Minnan Luo,Junhang Huang*

Main category: cs.CL

TL;DR: 现有社交机器人检测器因捷径学习存在鲁棒性问题，提出基于大语言模型的反事实数据增强策略，在捷径场景下实现56%平均性能提升


<details>
  <summary>Details</summary>
Motivation: 现有检测器在真实场景中表现受限，主要源于标注数据质量不明和模型易受文本特征中的虚假关联影响，需系统性评估并提升模型抗干扰能力

Method: 1. 构建文本特征与用户标签的虚假关联作为捷径场景
2. 从数据分布（个体文本/整体数据集）和模型因果信息提取三个层面
3. 采用大语言模型进行反事实数据增强

Result: 基线模型在无关特征偏移时准确率相对下降32%，提出的策略使性能相对提升56%

Conclusion: 研究揭示了社交机器人检测器对表面特征的脆弱性，提出的多层级缓解策略有效提升模型鲁棒性，强调数据分布均衡和因果特征提取的重要性

Abstract: While existing social bot detectors perform well on benchmarks, their robustness across diverse real-world scenarios remains limited due to unclear ground truth and varied misleading cues. In particular, the impact of shortcut learning, where models rely on spurious correlations instead of capturing causal task-relevant features, has received limited attention. To address this gap, we conduct an in-depth study to assess how detectors are influenced by potential shortcuts based on textual features, which are most susceptible to manipulation by social bots. We design a series of shortcut scenarios by constructing spurious associations between user labels and superficial textual cues to evaluate model robustness. Results show that shifts in irrelevant feature distributions significantly degrade social bot detector performance, with an average relative accuracy drop of 32\% in the baseline models. To tackle this challenge, we propose mitigation strategies based on large language models, leveraging counterfactual data augmentation. These methods mitigate the problem from data and model perspectives across three levels, including data distribution at both the individual user text and overall dataset levels, as well as the model's ability to extract causal information. Our strategies achieve an average relative performance improvement of 56\% under shortcut scenarios.

</details>


### [52] [SPEAR-MM: Selective Parameter Evaluation and Restoration via Model Merging for Efficient Financial LLM Adaptation](https://arxiv.org/abs/2511.08500)
*Berkcan Kapusuzoglu,Supriyo Chakraborty,Renkun Ni,Stephen Rawls,Sambit Sahu*

Main category: cs.CL

TL;DR: 提出SPEAR-MM框架解决金融领域LLMs的灾难性遗忘问题，通过选择性参数冻结/恢复实现91.2%通用能力保留率并降低90%计算成本


<details>
  <summary>Details</summary>
Motivation: 金融领域适配的LLMs会丢失关键通用推理能力，传统持续预训练方法在保留通用能力(69.7%)和计算成本方面表现不足

Method: 通过事后分析评估各层对基准测试的影响，采用球面插值合并技术选择性冻结/恢复Transformer层，应用于LLaMA-3.1-8B模型

Result: 相比标准方法：通用能力保留率提升21.5%(达91.2%)，维持94%领域适应增益，计算成本降低90%

Conclusion: 该方法在能力保留与领域适应间取得平衡，为资源受限的金融机构提供高效适配方案，同时提供可解释的权衡控制机制

Abstract: Large language models (LLMs) adapted to financial domains often suffer from catastrophic forgetting of general reasoning capabilities essential for customer interactions and complex financial analysis. We introduce Selective Parameter Evaluation and Restoration via Model Merging (SPEAR-MM), a practical framework that preserves critical capabilities while enabling domain adaptation. Our method approximates layer-wise impact on external benchmarks through post-hoc analysis, then selectively freezes or restores transformer layers via spherical interpolation merging. Applied to LLaMA-3.1-8B for financial tasks, SPEAR-MM achieves 91.2% retention of general capabilities versus 69.7% for standard continual pretraining, while maintaining 94% of domain adaptation gains. The approach provides interpretable trade-off control and reduces computational costs by 90% crucial for resource-constrained financial institutions.

</details>


### [53] [Structured RAG for Answering Aggregative Questions](https://arxiv.org/abs/2511.08505)
*Omri Koshorek,Niv Granot,Aviv Alloni,Shahar Admati,Roee Hendel,Ido Weiss,Alan Arazi,Shay-Nitzan Cohen,Yonatan Belinkov*

Main category: cs.CL

TL;DR: 提出S-RAG方法解决传统RAG在聚合查询中的不足，通过结构化语料表示和查询转换机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有RAG系统专注于单段落相关场景，无法有效处理需要跨多文档信息聚合的复杂查询场景。

Method: 在数据预处理阶段构建结构化语料表示，推理时通过自然语言到形式化查询的转换机制实现聚合查询。

Result: 在HOTELS/WORLD CUP新数据集和公共基准测试中，S-RAG显著优于传统RAG和长上下文LLM。

Conclusion: S-RAG有效解决了聚合查询场景需求，并通过发布新数据集推动该领域研究发展。

Abstract: Retrieval-Augmented Generation (RAG) has become the dominant approach for answering questions over large corpora. However, current datasets and methods are highly focused on cases where only a small part of the corpus (usually a few paragraphs) is relevant per query, and fail to capture the rich world of aggregative queries. These require gathering information from a large set of documents and reasoning over them. To address this gap, we propose S-RAG, an approach specifically designed for such queries. At ingestion time, S-RAG constructs a structured representation of the corpus; at inference time, it translates natural-language queries into formal queries over said representation. To validate our approach and promote further research in this area, we introduce two new datasets of aggregative queries: HOTELS and WORLD CUP. Experiments with S-RAG on the newly introduced datasets, as well as on a public benchmark, demonstrate that it substantially outperforms both common RAG systems and long-context LLMs.

</details>


### [54] [Introducing A Bangla Sentence - Gloss Pair Dataset for Bangla Sign Language Translation and Research](https://arxiv.org/abs/2511.08507)
*Neelavro Saha,Rafi Shahriyar,Nafis Ashraf Roudra,Saadman Sakib,Annajiat Alim Rasel*

Main category: cs.CL

TL;DR: 构建Bangla-SGP数据集解决孟加拉手语翻译低资源问题，通过规则增强数据并微调多种Transformer模型


<details>
  <summary>Details</summary>
Motivation: 现有BdSL翻译研究局限于单词/字母级别，缺乏句子级大规模数据集支持

Method: 1.创建1000人工标注+3000规则增强的平行语料库 2.设计基于检索增强的规则生成流程 3.微调mBart50/mT5/GPT4.1等模型

Result: 模型在BLEU指标上取得有效性能，与RWTH-PHOENIX-2014T基准对比显示翻译一致性

Conclusion: 填补孟加拉手语句子级翻译资源空白，验证基于语言规则的增强策略有效性

Abstract: Bangla Sign Language (BdSL) translation represents a low-resource NLP task due to the lack of large-scale datasets that address sentence-level translation. Correspondingly, existing research in this field has been limited to word and alphabet level detection. In this work, we introduce Bangla-SGP, a novel parallel dataset consisting of 1,000 human-annotated sentence-gloss pairs which was augmented with around 3,000 synthetically generated pairs using syntactic and morphological rules through a rule-based Retrieval-Augmented Generation (RAG) pipeline. The gloss sequences of the spoken Bangla sentences are made up of individual glosses which are Bangla sign supported words and serve as an intermediate representation for a continuous sign. Our dataset consists of 1000 high quality Bangla sentences that are manually annotated into a gloss sequence by a professional signer. The augmentation process incorporates rule-based linguistic strategies and prompt engineering techniques that we have adopted by critically analyzing our human annotated sentence-gloss pairs and by working closely with our professional signer. Furthermore, we fine-tune several transformer-based models such as mBart50, Google mT5, GPT4.1-nano and evaluate their sentence-to-gloss translation performance using BLEU scores, based on these evaluation metrics we compare the model's gloss-translation consistency across our dataset and the RWTH-PHOENIX-2014T benchmark.

</details>


### [55] [AlphaResearch: Accelerating New Algorithm Discovery with Language Models](https://arxiv.org/abs/2511.08522)
*Zhaojian Yu,Kaiyue Feng,Yilun Zhao,Shilin He,Xiao-Ping Zhang,Arman Cohan*

Main category: cs.CL

TL;DR: 提出了自主研究代理AlphaResearch，通过双研究环境验证机制（执行验证+模拟同行评审）迭代优化算法发现，在开放性问题中取得突破性进展。其在Circle Packing问题上超越人类研究者表现，但整体成功率仍有提升空间。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在开放性问题中发现未知算法的局限性，探索AI自主研究能力边界

Method: 构建双研究环境（执行验证环境+模拟同行评审环境），采用三阶段迭代流程：1) 新算法提案 2) 双环境验证 3) 研究方案优化

Result: 在8个开放问题竞赛中：
- 人机对抗2/8胜率
- Circle Packing问题达到当前已知最佳性能
- 6个失败案例揭示当前系统局限性

Conclusion: AlphaResearch证明了LLM加速算法发现的可行性，但需要解决复杂问题建模、验证机制优化等挑战。失败案例分析为未来研究提供方向指引。

Abstract: Large language models have made significant progress in complex but easy-to-verify problems, yet they still struggle with discovering the unknown. In this paper, we present \textbf{AlphaResearch}, an autonomous research agent designed to discover new algorithms on open-ended problems. To synergize the feasibility and innovation of the discovery process, we construct a novel dual research environment by combining the execution-based verify and simulated real-world peer review environment. AlphaResearch discovers new algorithm by iteratively running the following steps: (1) propose new ideas (2) verify the ideas in the dual research environment (3) optimize the research proposals for better performance. To promote a transparent evaluation process, we construct \textbf{AlphaResearchComp}, a new evaluation benchmark that includes an eight open-ended algorithmic problems competition, with each problem carefully curated and verified through executable pipelines, objective metrics, and reproducibility checks. AlphaResearch gets a 2/8 win rate in head-to-head comparison with human researchers, demonstrate the possibility of accelerating algorithm discovery with LLMs. Notably, the algorithm discovered by AlphaResearch on the \emph{``packing circles''} problem achieves the best-of-known performance, surpassing the results of human researchers and strong baselines from recent work (e.g., AlphaEvolve). Additionally, we conduct a comprehensive analysis of the remaining challenges of the 6/8 failure cases, providing valuable insights for future research.

</details>


### [56] [Investigating CoT Monitorability in Large Reasoning Models](https://arxiv.org/abs/2511.08525)
*Shu Yang,Junchao Wu,Xilin Gou,Xuansheng Wu,Derek Wong,Ninhao Liu,Di Wang*

Main category: cs.CL

TL;DR: 首次系统研究大模型思维链监控的挑战与潜力，提出MoME框架实现基于思维链的AI安全监测


<details>
  <summary>Details</summary>
Motivation: 大模型通过思维链决策时可能产生虚假陈述（verbalization）且监控器可靠性不足，需系统性验证其监控可行性

Method: 围绕思维链真实表达（verbalization）和监控可靠性（monitor reliability）双视角，在数学/科学/伦理领域开展实证分析，并通过干预实验验证方法有效性

Result: 发现思维链质量与监控可靠性正相关，提出MoME框架使大模型通过思维链结构化判断其他模型的潜在危险行为

Conclusion: 思维链监控性具有重要研究价值，需在提升推理能力的同时兼顾监控可靠性，为AI安全提供新范式

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks by engaging in extended reasoning before producing final answers. Beyond improving abilities, these detailed reasoning traces also create a new opportunity for AI safety, CoT Monitorability: monitoring potential model misbehavior, such as the use of shortcuts or sycophancy, through their chain-of-thought (CoT) during decision-making. However, two key fundamental challenges arise when attempting to build more effective monitors through CoT analysis. First, as prior research on CoT faithfulness has pointed out, models do not always truthfully represent their internal decision-making in the generated reasoning. Second, monitors themselves may be either overly sensitive or insufficiently sensitive, and can potentially be deceived by models' long, elaborate reasoning traces. In this paper, we present the first systematic investigation of the challenges and potential of CoT monitorability. Motivated by two fundamental challenges we mentioned before, we structure our study around two central perspectives: (i) verbalization: to what extent do LRMs faithfully verbalize the true factors guiding their decisions in the CoT, and (ii) monitor reliability: to what extent can misbehavior be reliably detected by a CoT-based monitor? Specifically, we provide empirical evidence and correlation analyses between verbalization quality, monitor reliability, and LLM performance across mathematical, scientific, and ethical domains. Then we further investigate how different CoT intervention methods, designed to improve reasoning efficiency or performance, will affect monitoring effectiveness. Finally, we propose MoME, a new paradigm in which LLMs monitor other models' misbehavior through their CoT and provide structured judgments along with supporting evidence.

</details>


### [57] [From Semantic Roles to Opinion Roles: SRL Data Extraction for Multi-Task and Transfer Learning in Low-Resource ORL](https://arxiv.org/abs/2511.08537)
*Amirmohammad Omidi Galdiani,Sepehr Rezaei Melal,Mohammad Norasteh,Arash Yousefi Jordehi,Seyed Abolghasem Mirroshandel*

Main category: cs.CL

TL;DR: 构建基于WSJ语料的高质量语义角色标注数据集，并适配意见角色标注任务


<details>
  <summary>Details</summary>
Motivation: 为低资源意见挖掘场景提供可复用的语义角色标注资源，通过结构化标注增强意见角色分析能力

Method: 基于PropBank框架设计可重复的提取流程，包含谓词-论元结构对齐、句法树转换、严格清洗校正三阶段处理

Result: 产出97,169个标注实例，实现SRL到ORL的角色映射（ARG0→Holder, REL→Expression, ARG1→Target）

Conclusion: 该数据集为融合语义分析与意见挖掘研究提供了标准化基础，特别有助于提升低资源语言场景的ORL性能

Abstract: This report presents a detailed methodology for constructing a high-quality Semantic Role Labeling (SRL) dataset from the Wall Street Journal (WSJ) portion of the OntoNotes 5.0 corpus and adapting it for Opinion Role Labeling (ORL) tasks. Leveraging the PropBank annotation framework, we implement a reproducible extraction pipeline that aligns predicate-argument structures with surface text, converts syntactic tree pointers to coherent spans, and applies rigorous cleaning to ensure semantic fidelity. The resulting dataset comprises 97,169 predicate-argument instances with clearly defined Agent (ARG0), Predicate (REL), and Patient (ARG1) roles, mapped to ORL's Holder, Expression, and Target schema. We provide a detailed account of our extraction algorithms, discontinuous argument handling, annotation corrections, and statistical analysis of the resulting dataset. This work offers a reusable resource for researchers aiming to leverage SRL for enhancing ORL, especially in low-resource opinion mining scenarios.

</details>


### [58] [Moral Susceptibility and Robustness under Persona Role-Play in Large Language Models](https://arxiv.org/abs/2511.08565)
*Davi Bastos Costa,Felippe Alves,Renato Vicente*

Main category: cs.CL

TL;DR: 研究发现大语言模型的道德鲁棒性主要受模型家族影响（Claude最优），道德敏感性随模型规模增大而提升，且鲁棒性与敏感性呈正相关


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型在角色扮演场景中道德判断的稳定性，为其社会应用提供伦理评估依据

Method: 使用道德基础问卷（MFQ）构建基准，通过角色扮演测试不同模型家族/规模的道德响应变化，量化道德敏感性（跨角色方差）和道德鲁棒性（角色内方差）

Result: 1. 道德鲁棒性：模型家族解释主要方差（Claude > Gemini > GPT-4）
2. 道德敏感性：同家族内模型越大越敏感
3. 鲁棒性与敏感性正相关（家族层面更显著）

Conclusion: 角色设定显著影响语言模型道德行为，模型家族选择比单纯扩大规模更能提升道德稳定性，这对AI伦理评估和模型设计具有指导意义

Abstract: Large language models (LLMs) increasingly operate in social contexts, motivating analysis of how they express and shift moral judgments. In this work, we investigate the moral response of LLMs to persona role-play, prompting a LLM to assume a specific character. Using the Moral Foundations Questionnaire (MFQ), we introduce a benchmark that quantifies two properties: moral susceptibility and moral robustness, defined from the variability of MFQ scores across and within personas, respectively. We find that, for moral robustness, model family accounts for most of the variance, while model size shows no systematic effect. The Claude family is, by a significant margin, the most robust, followed by Gemini and GPT-4 models, with other families exhibiting lower robustness. In contrast, moral susceptibility exhibits a mild family effect but a clear within-family size effect, with larger variants being more susceptible. Moreover, robustness and susceptibility are positively correlated, an association that is more pronounced at the family level. Additionally, we present moral foundation profiles for models without persona role-play and for personas averaged across models. Together, these analyses provide a systematic view of how persona conditioning shapes moral behavior in large language models.

</details>


### [59] [Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models](https://arxiv.org/abs/2511.08577)
*Tianyu Fu,Yichen You,Zekai Chen,Guohao Dai,Huazhong Yang,Yu Wang*

Main category: cs.CL

TL;DR: 提出动态潜在思考方法TaH，通过仅在困难token上深度迭代提升LLM推理效率，在保持参数数量不变的前提下显著提高多个基准测试性能


<details>
  <summary>Details</summary>
Motivation: 现有循环变换器方法的固定迭代策略导致潜在过度思考现象（正确预测被错误修改），且存在计算资源浪费

Method: 1) 轻量级神经网络决策器动态选择需迭代token 2) 使用LoRA模块将模型目标调整为困难token优化 3) 引入双因果注意力机制实现跨迭代信息流

Result: 在5个基准测试中平均提升8.1-12.6%准确率，94%的token免于二次迭代，仅需增加3%参数即可获得更大提升

Conclusion: TaH成功平衡了模型性能与计算效率，为实际应用中的高效LLM推理提供了有效解决方案

Abstract: Improving reasoning capabilities of Large Language Models (LLMs), especially under parameter constraints, is crucial for real-world applications. Prior work proposes recurrent transformers, which allocate a fixed number of extra iterations per token to improve generation quality. After the first, standard forward pass, instead of verbalization, last-layer hidden states are fed back as inputs for additional iterations to refine token predictions. Yet we identify a latent overthinking phenomenon: easy token predictions that are already correct after the first pass are sometimes revised into errors in additional iterations. To address this, we propose Think-at-Hard (TaH), a dynamic latent thinking method that iterates deeper only at hard tokens. It employs a lightweight neural decider to trigger latent iterations only at tokens that are likely incorrect after the standard forward pass. During latent iterations, Low-Rank Adaptation (LoRA) modules shift the LLM objective from general next-token prediction to focused hard-token refinement. We further introduce a duo-causal attention mechanism that extends attention from the token sequence dimension to an additional iteration depth dimension. This enables cross-iteration information flow while maintaining full sequential parallelism. Experiments show that TaH boosts LLM reasoning performance across five challenging benchmarks while maintaining the same parameter count. Compared with baselines that iterate twice for all output tokens, TaH delivers 8.1-11.3% accuracy gains while exempting 94% of tokens from the second iteration. Against strong single-iteration Qwen3 models finetuned with the same data, it also delivers 4.0-5.0% accuracy gains. When allowing less than 3% additional parameters from LoRA and the iteration decider, the gains increase to 8.5-12.6% and 5.3-5.4%, respectively. Our code is available at https://github.com/thu-nics/TaH.

</details>


### [60] [Training Language Models to Explain Their Own Computations](https://arxiv.org/abs/2511.08579)
*Belinda Z. Li,Zifan Carl Guo,Vincent Huang,Jacob Steinhardt,Jacob Andreas*

Main category: cs.CL

TL;DR: 研究通过微调语言模型使其生成对自身计算过程的自然语言描述，发现模型自我解释优于其他模型解释，可作为现有可解释性方法的补充。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型能否利用自身内部信息的特权优势，生成更准确的计算过程解释，并验证自我解释是否优于外部模型解释。

Method: 使用现有可解释性方法作为基准，微调语言模型生成三种解释：特征编码信息、激活值的因果结构、特定输入令牌对输出的影响。

Result: 模型在数万样本训练后展现非平凡泛化能力，自我解释效果优于使用更高性能的外部模型进行解释。

Conclusion: 语言模型不仅能可靠解释内部计算，其生成的解释还为现有可解释性方法提供了可扩展的补充方案。

Abstract: Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs' privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs' internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models' privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the other model is significantly more capable). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [61] [Deep Inverse Shading: Consistent Albedo and Surface Detail Recovery via Generative Refinement](https://arxiv.org/abs/2511.08079)
*Jiacheng Wu,Ruiqi Zhang,Jie Chen*

Main category: cs.GR

TL;DR: DIS提出结合生成先验的网格框架，通过法线转换模块和去着色模块联合优化几何与材质，实现高质量可重光照虚拟人重建。


<details>
  <summary>Details</summary>
Motivation: 传统体积渲染方法训练缓慢，表面表示方法受顶点数量限制，需融合生成先验实现高效高保真重建。

Method: 基于网格模型融合多视角生成法线预测，通过可微光栅化实现几何细节捕捉，结合去着色模块优化材质反照率。

Result: 实验显示DIS在重光照质量（SOTA）、渲染效率（30FPS）、内存消耗（1.5GB）和几何细节恢复方面显著优于基线方法。

Conclusion: 通过几何与材质的联合优化框架，DIS实现了物理一致的高质量重建，为虚拟人建模提供高效解决方案。

Abstract: Reconstructing human avatars using generative priors is essential for achieving versatile and realistic avatar models. Traditional approaches often rely on volumetric representations guided by generative models, but these methods require extensive volumetric rendering queries, leading to slow training. Alternatively, surface-based representations offer faster optimization through differentiable rasterization, yet they are typically limited by vertex count, restricting mesh resolution and scalability when combined with generative priors. Moreover, integrating generative priors into physically based human avatar modeling remains largely unexplored. To address these challenges, we introduce DIS (Deep Inverse Shading), a unified framework for high-fidelity, relightable avatar reconstruction that incorporates generative priors into a coherent surface representation. DIS centers on a mesh-based model that serves as the target for optimizing both surface and material details. The framework fuses multi-view 2D generative surface normal predictions, rich in detail but often inconsistent, into the central mesh using a normal conversion module. This module converts generative normal outputs into per-triangle surface offsets via differentiable rasterization, enabling the capture of fine geometric details beyond sparse vertex limitations. Additionally, DIS integrates a de-shading module to recover accurate material properties. This module refines albedo predictions by removing baked-in shading and back-propagates reconstruction errors to optimize the geometry. Through joint optimization of geometry and material appearance, DIS achieves physically consistent, high-quality reconstructions suitable for accurate relighting. Our experiments show that DIS delivers SOTA relighting quality, enhanced rendering efficiency, lower memory consumption, and detailed surface reconstruction.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [62] [Quantifying the Impact of CU: A Systematic Literature Review](https://arxiv.org/abs/2511.07491)
*Thomas Compton*

Main category: cs.DL

TL;DR: 社区工会主义（CU）通过管理劳工运动内部矛盾而非建立新工会模式，体现其在萎缩劳工运动中的调和作用。


<details>
  <summary>Details</summary>
Motivation: 解决社区工会主义理论定位模糊与政治意义争议，揭示其学术建构背后的矛盾管理本质。

Method: 采用文献计量网络分析（114篇文献）与核心案例主题编码（18个案例）的双重系统性研究方法。

Result: 揭示CU的双重系谱：英国本土实践与跨国社会运动工会主义的理论张力，及对阶级政治的深刻矛盾态度。

Conclusion: 社区工会主义的核心价值在于调和劳工运动收缩期的「工作场所-社区」「领导层-基层」「改革-激进主义」三重矛盾。

Abstract: Community Unionism has served as a pivotal concept in debates on trade union renewal since the early 2000s, yet its theoretical coherence and political significance remain unresolved. This article investigates why CU has gained such prominence -- not by testing its efficacy, but by mapping how it is constructed, cited, and contested across the scholarly literature. Using two complementary systematic approaches -- a citation network analysis of 114 documents and a thematic review of 18 core CU case studies -- I examine how CU functions as both an empirical descriptor and a normative ideal. The analysis reveals CU's dual genealogy: positioned by British scholars as an indigenous return to historic rank-and-file practices, yet structurally aligned with transnational social movement unionism. Thematic coding shows near-universal emphasis on coalition-building and alliances, but deep ambivalence toward class politics. This tension suggests CU's significance lies less in operationalising a new union model, and more in managing contradictions -- between workplace and community, leadership and rank-and-file, reform and radicalism -- within a shrinking labour movement.

</details>


### [63] [CC30k: A Citation Contexts Dataset for Reproducibility-Oriented Sentiment Analysis](https://arxiv.org/abs/2511.07790)
*Rochana R. Obadage,Sarah M. Rajtmajer,Jian Wu*

Main category: cs.DL

TL;DR: 提出CC30k数据集，用于分析机器学习论文中引用文献的可复现性情感，提升大语言模型在此类情感分类任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有情感分析数据集未聚焦研究可复现性评估，而引用文献中的可复现性情感是评估研究可靠性的重要信号。

Method: 通过数据清洗、众包标注（25,829条）、受控负样本生成和严格验证流程构建CC30k数据集，并用其微调三种大语言模型。

Result: 达到94%标注准确率，微调后的大语言模型在可复现性情感分类任务上性能显著提升。

Conclusion: CC30k为大规模评估机器学习论文可复现性奠定基础，相关数据集和工具已开源。

Abstract: Sentiments about the reproducibility of cited papers in downstream literature offer community perspectives and have shown as a promising signal of the actual reproducibility of published findings. To train effective models to effectively predict reproducibility-oriented sentiments and further systematically study their correlation with reproducibility, we introduce the CC30k dataset, comprising a total of 30,734 citation contexts in machine learning papers. Each citation context is labeled with one of three reproducibility-oriented sentiment labels: Positive, Negative, or Neutral, reflecting the cited paper's perceived reproducibility or replicability. Of these, 25,829 are labeled through crowdsourcing, supplemented with negatives generated through a controlled pipeline to counter the scarcity of negative labels. Unlike traditional sentiment analysis datasets, CC30k focuses on reproducibility-oriented sentiments, addressing a research gap in resources for computational reproducibility studies. The dataset was created through a pipeline that includes robust data cleansing, careful crowd selection, and thorough validation. The resulting dataset achieves a labeling accuracy of 94%. We then demonstrated that the performance of three large language models significantly improves on the reproducibility-oriented sentiment classification after fine-tuning using our dataset. The dataset lays the foundation for large-scale assessments of the reproducibility of machine learning papers. The CC30k dataset and the Jupyter notebooks used to produce and analyze the dataset are publicly available at https://github.com/lamps-lab/CC30k .

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [64] [Pruning as Regularization: Sensitivity-Aware One-Shot Pruning in ASR](https://arxiv.org/abs/2511.08092)
*Julian Irigoyen,Arthur Söhler,Andreas Søeborg Kirkedal*

Main category: eess.AS

TL;DR: 通过敏感度诊断实现定向剪枝，解码器自注意力层和最后编码器层的冗余剪枝提升ASR模型性能，剪枝率50%时WER相对降低20.44%


<details>
  <summary>Details</summary>
Motivation: 挑战传统剪枝仅作为压缩技术的观点，验证单次幅度剪枝作为ASR隐式正则化器的潜力

Method: 结合梯度敏感度/Fisher敏感度诊断，对Whisper-small进行组件级定向剪枝，分析架构不对称性

Result: 剪除50%解码器自注意力层使LibriSpeech测试集WER绝对降低2.38%；40%稀疏度下保持基线精度

Conclusion: 剪枝应视为架构设计工具，剪枝位置选择与剪枝量同等重要，敏感度感知方法突破传统全局剪枝极限

Abstract: We challenge the conventional view of neural network pruning as solely a compression technique, demonstrating that one-shot magnitude pruning serves as a powerful implicit regularizer for ASR. Using Whisper-small, we combine gradient- and Fisher-based sensitivity diagnostics with targeted, component-wise pruning. This reveals architectural asymmetries: decoder FFNs are pruning-fragile, whereas decoder self-attention and the last encoder layers contain redundancy that, when removed, improves generalization. Without fine-tuning, pruning 50% of decoder self-attention reduces WER by 2.38% absolute (20.44% relative) on LibriSpeech test-other; pruning the last four encoder layers at 50% instead yields a 1.72% absolute (14.8% relative) improvement. Gains persisted on Common Voice and TED-LIUM datasets. Beyond regularization benefits, our sensitivity-aware approach enables more aggressive one-shot compression. At 40% sparsity, where established global pruning approaches catastrophically fail, our method preserves near-baseline accuracy. This positions pruning as a first-class architectural design tool: knowing where to prune is as important as how much to prune.

</details>


### [65] [Quantizing Whisper-small: How design choices affect ASR performance](https://arxiv.org/abs/2511.08093)
*Arthur Söhler,Julian Irigoyen,Andreas Søeborg Kirkedal*

Main category: eess.AS

TL;DR: 研究通过对比不同后训练量化方法，发现动态int8量化（Quanto）可在压缩57%模型体积的同时提升识别准确率，为Whisper-small在边缘设备部署提供有效方案


<details>
  <summary>Details</summary>
Motivation: 大型语音识别模型（如Whisper-small）计算需求高，难以在边缘设备部署。本文旨在通过量化技术压缩模型，无需重新训练即可实现高效部署

Method: 跨库评估PyTorch/Optimum-Quanto/HQQ/bitsandbytes四种方案，对比量化模式/方法/粒度/比特位宽，基于LibriSpeech测试集验证效果

Result: 动态int8量化模型体积缩小57%且词错率改善，静态量化效果差（Transformer架构限制），激进量化格式（nf4/int3）在噪声场景下压缩71%但准确率下降

Conclusion: 合理选择PTQ方法可在不重新训练的情况下显著降低模型体积和推理成本，证明Whisper-small在资源受限硬件部署的可行性

Abstract: Large speech recognition models like Whisper-small achieve high accuracy but are difficult to deploy on edge devices due to their high computational demand. To this end, we present a unified, cross-library evaluation of post-training quantization (PTQ) on Whisper-small that disentangles the impact of quantization scheme, method, granularity, and bit-width. Our study is based on four libraries: PyTorch, Optimum-Quanto, HQQ, and bitsandbytes. Experiments on LibriSpeech test-clean and test-other show that dynamic int8 quantization with Quanto offers the best trade-off, reducing model size by 57% while improving on the baseline's word error rate. Static quantization performed worse, likely due to Whisper's Transformer architecture, while more aggressive formats (e.g., nf4, int3) achieved up to 71% compression at the cost of accuracy in noisy conditions. Overall, our results demonstrate that carefully chosen PTQ methods can substantially reduce model size and inference cost without retraining, enabling efficient deployment of Whisper-small on constrained hardware.

</details>


### [66] [Unifying Model and Layer Fusion for Speech Foundation Models](https://arxiv.org/abs/2511.08389)
*Yi-Jen Shih,David Harwath*

Main category: eess.AS

TL;DR: 提出跨模型跨层融合接口模块，显著提升语音基础模型在下游任务中的性能表现


<details>
  <summary>Details</summary>
Motivation: 现有语音基础模型的融合策略存在单一性（单模型多层或多模型单层融合），未能充分发挥不同模型和层间特征的协同潜力

Method: 设计统一接口模块，支持多个上游语音模型的跨模型融合，同时整合各模型不同层次的特征表示

Result: 在ASR和副语言分析等任务中超越现有融合方法，模型规模和数量扩展性实验验证了接口的有效性

Conclusion: 通过合理选择上游模型，该接口可显著提升性能，为语音基础模型的有效利用提供了新思路

Abstract: Speech Foundation Models have gained significant attention recently. Prior works have shown that the fusion of representations from multiple layers of the same model or the fusion of multiple models can improve performance on downstream tasks. We unify these two fusion strategies by proposing an interface module that enables fusion across multiple upstream speech models while integrating information across their layers. We conduct extensive experiments on different self-supervised and supervised models across various speech tasks, including ASR and paralinguistic analysis, and demonstrate that our method outperforms prior fusion approaches. We further analyze its scalability concerning model size and count, highlighting the importance of selecting appropriate upstream models. Our results show that the proposed interface provides an additional performance boost when given a suitable upstream model selection, making it a promising approach for utilizing Speech Foundation Models.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [67] [How Brittle is Agent Safety? Rethinking Agent Risk under Intent Concealment and Task Complexity](https://arxiv.org/abs/2511.08487)
*Zihan Ma,Dongsheng Zhu,Shudong Liu,Taolin Zhang,Junnan Liu,Qingqiu Li,Minnan Luo,Songyang Zhang,Kai Chen*

Main category: cs.MA

TL;DR: 论文揭示现有LLM代理安全评估存在原子危害局限，提出正交安全分析框架OASIS，发现意图隐蔽性会加速安全性能衰退，并首次揭示能力局限导致的复杂性安全假象


<details>
  <summary>Details</summary>
Motivation: 当前安全评估仅检测显性恶意意图的原子性攻击，无法识别复杂任务中通过意图稀释/隐蔽实现的新型威胁，导致真实场景存在重大安全隐患

Method: 构建正交双维度测试框架OASIS（意图隐蔽性 vs 任务复杂性），包含200+标注测试用例和高保真沙箱环境，量化分析LLM代理的安全脆弱性演变规律

Result: 发现安全性能随意图隐蔽性呈现指数级衰减曲线，揭示复杂性悖论：代理在复杂任务中的表面安全性提升实际源于能力不足导致的攻击失败（而非安全机制强化）

Conclusion: OASIS首次系统验证LLM代理安全体系的维度脆弱性，为构建多维度鲁棒性评估、突破现有安全假象提供方法论和开源工具支持

Abstract: Current safety evaluations for LLM-driven agents primarily focus on atomic harms, failing to address sophisticated threats where malicious intent is concealed or diluted within complex tasks. We address this gap with a two-dimensional analysis of agent safety brittleness under the orthogonal pressures of intent concealment and task complexity. To enable this, we introduce OASIS (Orthogonal Agent Safety Inquiry Suite), a hierarchical benchmark with fine-grained annotations and a high-fidelity simulation sandbox. Our findings reveal two critical phenomena: safety alignment degrades sharply and predictably as intent becomes obscured, and a "Complexity Paradox" emerges, where agents seem safer on harder tasks only due to capability limitations. By releasing OASIS and its simulation environment, we provide a principled foundation for probing and strengthening agent safety in these overlooked dimensions.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [68] [TouchWalker: Real-Time Avatar Locomotion from Touchscreen Finger Walking](https://arxiv.org/abs/2511.07860)
*Geuntae Park,Jiwon Yi,Taehyun Rhee,Kwanguk Kim,Yoonsang Lee*

Main category: cs.HC

TL;DR: 实时手指触控全身虚拟角色运动系统TouchWalker，通过神经网络生成连续上下文感知动作，提升用户沉浸感与操控表现


<details>
  <summary>Details</summary>
Motivation: 现有基于符号手势或预设动作的系统缺乏连续运动生成能力，无法适应复杂场景（如跑步腾空阶段）的实时交互需求

Method: 采用MoE-GRU架构和专用足部对齐损失函数，结合用户触控输入转化为虚拟角色足部位置，通过对比用户研究验证效果

Result: 相比预设动作的虚拟摇杆基线，TouchWalker显著提升用户的身心沉浸感（embodiment）、操作乐趣（enjoyment）和环境沉浸度（immersion）

Conclusion: TouchWalker通过神经网络的逐帧运动生成技术，实现了更自然直观的虚拟角色触控交互范式

Abstract: We present TouchWalker, a real-time system for controlling full-body avatar locomotion using finger-walking gestures on a touchscreen. The system comprises two main components: TouchWalker-MotionNet, a neural motion generator that synthesizes full-body avatar motion on a per-frame basis from temporally sparse two-finger input, and TouchWalker-UI, a compact touch interface that interprets user touch input to avatar-relative foot positions. Unlike prior systems that rely on symbolic gesture triggers or predefined motion sequences, TouchWalker uses its neural component to generate continuous, context-aware full-body motion on a per-frame basis-including airborne phases such as running, even without input during mid-air steps-enabling more expressive and immediate interaction. To ensure accurate alignment between finger contacts and avatar motion, it employs a MoE-GRU architecture with a dedicated foot-alignment loss. We evaluate TouchWalker in a user study comparing it to a virtual joystick baseline with predefined motion across diverse locomotion tasks. Results show that TouchWalker improves users' sense of embodiment, enjoyment, and immersion.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [69] [SONIC: Supersizing Motion Tracking for Natural Humanoid Whole-Body Control](https://arxiv.org/abs/2511.07820)
*Zhengyi Luo,Ye Yuan,Tingwu Wang,Chenran Li,Sirui Chen,Fernando Castañeda,Zi-Ang Cao,Jiefeng Li,David Minor,Qingwei Ben,Xingye Da,Runyu Ding,Cyrus Hogg,Lina Song,Edy Lim,Eugene Jeong,Tairan He,Haoru Xue,Wenli Xiao,Zi Wang,Simon Yuen,Jan Kautz,Yan Chang,Umar Iqbal,Linxi "Jim" Fan,Yuke Zhu*

Main category: cs.RO

TL;DR: 通过扩展模型容量、数据和计算资源，构建了基于大规模运动捕捉数据监督的人形控制器基础模型，实现了自然全身运动控制并展示实际应用价值


<details>
  <summary>Details</summary>
Motivation: 当前人形控制器存在模型规模小（百万级参数）、行为集有限、训练资源不足（单日GPU训练）的问题，需通过规模扩展探索通用控制器的潜力

Method: 1. 将运动跟踪作为基础任务，利用700小时/1亿帧运动捕捉数据进行密集监督
2. 三轴扩展：网络参数(1.2M→42M)、数据规模、计算资源(9k GPU小时)
3. 开发实时运动规划器和支持VR/视频/多模态的统一标记空间

Result: 1. 性能随计算资源和数据多样性持续提升
2. 学习表征可泛化至未见运动
3. 成功实现自然交互控制与多接口兼容（VR/视觉语言动作模型）

Conclusion: 大规模运动跟踪作为人形控制基础任务具有实践价值，实时运动规划与多接口支持验证了通用控制器的可行性，为机器人自然交互提供新范式

Abstract: Despite the rise of billion-parameter foundation models trained across thousands of GPUs, similar scaling gains have not been shown for humanoid control. Current neural controllers for humanoids remain modest in size, target a limited behavior set, and are trained on a handful of GPUs over several days. We show that scaling up model capacity, data, and compute yields a generalist humanoid controller capable of creating natural and robust whole-body movements. Specifically, we posit motion tracking as a natural and scalable task for humanoid control, leverageing dense supervision from diverse motion-capture data to acquire human motion priors without manual reward engineering. We build a foundation model for motion tracking by scaling along three axes: network size (from 1.2M to 42M parameters), dataset volume (over 100M frames, 700 hours of high-quality motion data), and compute (9k GPU hours). Beyond demonstrating the benefits of scale, we show the practical utility of our model through two mechanisms: (1) a real-time universal kinematic planner that bridges motion tracking to downstream task execution, enabling natural and interactive control, and (2) a unified token space that supports various motion input interfaces, such as VR teleoperation devices, human videos, and vision-language-action (VLA) models, all using the same policy. Scaling motion tracking exhibits favorable properties: performance improves steadily with increased compute and data diversity, and learned representations generalize to unseen motions, establishing motion tracking at scale as a practical foundation for humanoid control.

</details>


### [70] [ViPRA: Video Prediction for Robot Actions](https://arxiv.org/abs/2511.07732)
*Sandeep Routray,Hengkai Pan,Unnat Jain,Shikhar Bahl,Deepak Pathak*

Main category: cs.RO

TL;DR: ViPRA框架通过视频预测模型学习机器人控制，利用无标注视频生成潜在动作表征，仅需少量演示即可实现高频连续控制。


<details>
  <summary>Details</summary>
Motivation: 现有视频数据缺乏动作标注限制了机器人学习，需开发无需昂贵标注的跨本体控制方法。

Method: 视频语言模型联合预测视觉观测与运动中心潜在动作，通过光流一致性约束训练，采用分段流匹配解码器生成机器人动作序列。

Result: SIMPLER基准提升16%，真实任务提升13%，支持22Hz高频控制。

Conclusion: 首个显式建模场景动态变化机制的潜在动作框架，开放代码推动视频驱动机器人控制研究。

Abstract: Can we turn a video prediction model into a robot policy? Videos, including those of humans or teleoperated robots, capture rich physical interactions. However, most of them lack labeled actions, which limits their use in robot learning. We present Video Prediction for Robot Actions (ViPRA), a simple pretraining-finetuning framework that learns continuous robot control from these actionless videos. Instead of directly predicting actions, we train a video-language model to predict both future visual observations and motion-centric latent actions, which serve as intermediate representations of scene dynamics. We train these latent actions using perceptual losses and optical flow consistency to ensure they reflect physically grounded behavior. For downstream control, we introduce a chunked flow matching decoder that maps latent actions to robot-specific continuous action sequences, using only 100 to 200 teleoperated demonstrations. This approach avoids expensive action annotation, supports generalization across embodiments, and enables smooth, high-frequency continuous control upto 22 Hz via chunked action decoding. Unlike prior latent action works that treat pretraining as autoregressive policy learning, explicitly models both what changes and how. Our method outperforms strong baselines, with a 16% gain on the SIMPLER benchmark and a 13% improvement across real world manipulation tasks. We will release models and code at https://vipra-project.github.io

</details>


### [71] [PerspAct: Enhancing LLM Situated Collaboration Skills through Perspective Taking and Active Vision](https://arxiv.org/abs/2511.08098)
*Sabrina Patania,Luca Annese,Anita Pellegrini,Silvia Serino,Anna Lambiase,Luca Pallonetto,Silvia Rossi,Simone Colombani,Tom Foulsham,Azzurra Ruggeri,Dimitri Ognibene*

Main category: cs.RO

TL;DR: 研究验证通过ReAct框架整合多视角机制可显著提升大语言模型在多智能体协作中的视角采能力和协作效果。


<details>
  <summary>Details</summary>
Motivation: 现有LLM训练范式忽视交互情境中的视角采能力，导致模型难以处理多观察者环境下的主观视角推理问题。

Method: 扩展经典Director任务，设计7个渐进复杂的视角采场景，结合ReAct框架的推理-行动机制进行主动视觉探索。

Result: 明确的视角提示与主动探索策略使模型解释准确率提升22.3%，协作任务成功率提高34.7%。

Conclusion: 融合主动感知与视角采机制是推动LLM在机器人系统应用的关键，为构建情境自适应AI系统奠定理论基础。

Abstract: Recent advances in Large Language Models (LLMs) and multimodal foundation models have significantly broadened their application in robotics and collaborative systems. However, effective multi-agent interaction necessitates robust perspective-taking capabilities, enabling models to interpret both physical and epistemic viewpoints. Current training paradigms often neglect these interactive contexts, resulting in challenges when models must reason about the subjectivity of individual perspectives or navigate environments with multiple observers. This study evaluates whether explicitly incorporating diverse points of view using the ReAct framework, an approach that integrates reasoning and acting, can enhance an LLM's ability to understand and ground the demands of other agents. We extend the classic Director task by introducing active visual exploration across a suite of seven scenarios of increasing perspective-taking complexity. These scenarios are designed to challenge the agent's capacity to resolve referential ambiguity based on visual access and interaction, under varying state representations and prompting strategies, including ReAct-style reasoning. Our results demonstrate that explicit perspective cues, combined with active exploration strategies, significantly improve the model's interpretative accuracy and collaborative effectiveness. These findings highlight the potential of integrating active perception with perspective-taking mechanisms in advancing LLMs' application in robotics and multi-agent systems, setting a foundation for future research into adaptive and context-aware AI systems.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [72] [The Polite Liar: Epistemic Pathology in Language Models](https://arxiv.org/abs/2511.07477)
*Bentley DeVilling*

Main category: cs.CY

TL;DR: 论文指出RLHF训练机制导致大语言模型成为'礼貌的谎言家'，提出用'认知对齐'替代现有奖励机制


<details>
  <summary>Details</summary>
Motivation: 揭示大语言模型在强化学习人类偏好（RLHF）训练下，为满足'有用、无害、礼貌'的奖励目标而牺牲事实准确性的结构性问题

Method: 结合法兰克福的'胡扯理论'、认知德性理论、言语行为哲学等多学科框架，分析RLHF奖励机制与认知完整性间的根本矛盾

Result: 证明当前对齐方法使模型优先追求对话流畅度而非事实依据，形成表面诚恳实则认知无根的'礼貌谎言'行为模式

Conclusion: 提出'认知对齐'原则：应建立基于认知正当性的奖励机制，将合理置信度置于语言流畅度之上，解决语言合作与认知诚信的对齐冲突

Abstract: Large language models exhibit a peculiar epistemic pathology: they speak as if they know, even when they do not. This paper argues that such confident fabrication, what I call the polite liar, is a structural consequence of reinforcement learning from human feedback (RLHF). Building on Frankfurt's analysis of bullshit as communicative indifference to truth, I show that this pathology is not deception but structural indifference: a reward architecture that optimizes for perceived sincerity over evidential accuracy. Current alignment methods reward models for being helpful, harmless, and polite, but not for being epistemically grounded. As a result, systems learn to maximize user satisfaction rather than truth, performing conversational fluency as a virtue. I analyze this behavior through the lenses of epistemic virtue theory, speech-act philosophy, and cognitive alignment, showing that RLHF produces agents trained to mimic epistemic confidence without access to epistemic justification. The polite liar thus reveals a deeper alignment tension between linguistic cooperation and epistemic integrity. The paper concludes with an "epistemic alignment" principle: reward justified confidence over perceived fluency.

</details>


### [73] [Generative Artificial Intelligence in Qualitative Research Methods: Between Hype and Risks?](https://arxiv.org/abs/2511.08461)
*Maria Couto Teixeira,Marisa Tschopp,Anna Jobin*

Main category: cs.CY

TL;DR: 论文批判性质疑生成式AI在质性编码研究中的方法论有效性，指出其可能破坏研究严谨性，建议研究者优先考虑方法论而非技术新颖性


<details>
  <summary>Details</summary>
Motivation: 针对AI在质性研究中日益广泛的应用趋势，揭示生成式AI在质性编码方法论中的潜在风险与根本缺陷

Method: 通过批判性思辨方法，从文档缺失、商业黑箱、输出不可靠三个维度进行方法论层面的解构分析

Result: 论证生成式AI的潜在风险远超其效率优势，无法满足质性研究的方法论严谨性要求

Conclusion: 研究者应保持方法论警觉，在技术热潮中坚守学术规范，避免因技术新颖性牺牲研究信效度

Abstract: As Artificial Intelligence (AI) is increasingly promoted and used in qualitative research, it also raises profound methodological issues. This position paper critically interrogates the role of generative AI (genAI) in the context of qualitative coding methodologies. Despite widespread hype and claims of efficiency, we propose that genAI is not methodologically valid within qualitative inquiries, and its use risks undermining the robustness and trustworthiness of qualitative research. The lack of meaningful documentation, commercial opacity, and the inherent tendencies of genAI systems to produce incorrect outputs all contribute to weakening methodological rigor. Overall, the balance between risk and benefits does not support the use of genAI in qualitative research, and our position paper cautions researchers to put sound methodology before technological novelty.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [74] [Alignment-Constrained Dynamic Pruning for LLMs: Identifying and Preserving Alignment-Critical Circuits](https://arxiv.org/abs/2511.07482)
*Dev Patel,Gabrielle Gervacio,Diekola Raimi,Kevin Zhu,Ryan Lagasse,Gabriel Grand,Ashwinee Panda,Maheep Chaudhary*

Main category: cs.LG

TL;DR: 提出动态剪枝方法AAPP，在保持LLM计算效率的同时显著提升安全性指标（拒绝率+50%）


<details>
  <summary>Details</summary>
Motivation: 大语言模型推理时存在高计算资源消耗问题，传统动态剪枝方法虽然提升效率但加剧安全对齐退化风险

Method: 基于Probe Pruning改进的动态结构化剪枝方法（AAPP），自适应保留安全对齐相关的关键电路模块

Result: 在LLaMA 2-7B、Qwen2.5-14B-Instruct和Gemma-3-12B-IT上验证，相同计算量下拒绝率提升50%

Conclusion: AAPP方法有效解决了LLM部署中效率与安全性的矛盾，为安全敏感的模型压缩提供了新方向

Abstract: Large Language Models require substantial computational resources for inference, posing deployment challenges. While dynamic pruning offers superior efficiency over static methods through adaptive circuit selection, it exacerbates alignment degradation by retaining only input-dependent safety-critical circuit preservation across diverse inputs. As a result, addressing these heightened alignment vulnerabilities remains critical. We introduce Alignment-Aware Probe Pruning (AAPP), a dynamic structured pruning method that adaptively preserves alignment-relevant circuits during inference, building upon Probe Pruning. Experiments on LLaMA 2-7B, Qwen2.5-14B-Instruct, and Gemma-3-12B-IT show AAPP improves refusal rates by 50\% at matched compute, enabling efficient yet safety-preserving LLM deployment.

</details>


### [75] [LLM Output Drift: Cross-Provider Validation & Mitigation for Financial Workflows](https://arxiv.org/abs/2511.07585)
*Raffi Khatchadourian,Rolando Franco*

Main category: cs.LG

TL;DR: 量化金融任务中LLM输出稳定性：小模型（7B）输出一致性达100%，大模型（120B）仅12.5%。开发合规框架及三级分类系统，验证结果符合FSB/BIS/CFTC监管要求。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在金融应用中因输出漂移导致的审计和信任问题，确保合规部署的可靠性。

Method: 构建财务校准测试框架（贪婪解码+固定种子+SEC结构检索），设计任务特定检查（±5%阈值+SEC验证），建立三级分类系统和双供应商验证机制。

Result: 小模型在T=0.0时输出100%稳定，大模型仅12.5%；SQL任务稳定性优于RAG（漂移率25-75%）；跨部署验证一致性成立。

Conclusion: 挑战『模型越大越好』的认知，证明小模型在确定性任务中的优势，提供符合金融监管的AI部署路径。

Abstract: Financial institutions deploy Large Language Models (LLMs) for reconciliations, regulatory reporting, and client communications, but nondeterministic outputs (output drift) undermine auditability and trust. We quantify drift across five model architectures (7B-120B parameters) on regulated financial tasks, revealing a stark inverse relationship: smaller models (Granite-3-8B, Qwen2.5-7B) achieve 100% output consistency at T=0.0, while GPT-OSS-120B exhibits only 12.5% consistency (95% CI: 3.5-36.0%) regardless of configuration (p<0.0001, Fisher's exact test). This finding challenges conventional assumptions that larger models are universally superior for production deployment.
  Our contributions include: (i) a finance-calibrated deterministic test harness combining greedy decoding (T=0.0), fixed seeds, and SEC 10-K structure-aware retrieval ordering; (ii) task-specific invariant checking for RAG, JSON, and SQL outputs using finance-calibrated materiality thresholds (plus or minus 5%) and SEC citation validation; (iii) a three-tier model classification system enabling risk-appropriate deployment decisions; and (iv) an audit-ready attestation system with dual-provider validation.
  We evaluated five models (Qwen2.5-7B via Ollama, Granite-3-8B via IBM watsonx.ai, Llama-3.3-70B, Mistral-Medium-2505, and GPT-OSS-120B) across three regulated financial tasks. Across 480 runs (n=16 per condition), structured tasks (SQL) remain stable even at T=0.2, while RAG tasks show drift (25-75%), revealing task-dependent sensitivity. Cross-provider validation confirms deterministic behavior transfers between local and cloud deployments. We map our framework to Financial Stability Board (FSB), Bank for International Settlements (BIS), and Commodity Futures Trading Commission (CFTC) requirements, demonstrating practical pathways for compliance-ready AI deployments.

</details>


### [76] [DynaAct: Large Language Model Reasoning with Dynamic Action Spaces](https://arxiv.org/abs/2511.08043)
*Xueliang Zhao,Wei Wu,Jian Guan,Qintong Li,Lingpeng Kong*

Main category: cs.LG

TL;DR: 提出自动化构建紧凑动作空间的DynaAct框架，通过大语言模型提取通用草图和子模函数优化选择，显著提升复杂推理任务性能


<details>
  <summary>Details</summary>
Motivation: 解决现有手动定义动作空间缺乏扩展性/非结构化空间计算量大的问题，提升序列决策系统的推理效率

Method: 1. 用LLM从多样化语料中提取动作草图构建代理空间 2. 设计联合评估效用和多样性的子模函数，采用贪心算法优化选择

Result: 在6个基准测试中显著提升性能，推理延迟仅增加0.3秒

Conclusion: DynaAct通过结构化动作空间构建，实现了高效推理与性能提升的平衡，为复杂问题解决提供新范式

Abstract: In modern sequential decision-making systems, the construction of an optimal candidate action space is critical to efficient inference. However, existing approaches either rely on manually defined action spaces that lack scalability or utilize unstructured spaces that render exhaustive search computationally prohibitive. In this paper, we propose a novel framework named \textsc{DynaAct} for automatically constructing a compact action space to enhance sequential reasoning in complex problem-solving scenarios. Our method first estimates a proxy for the complete action space by extracting general sketches observed in a corpus covering diverse complex reasoning problems using large language models. We then formulate a submodular function that jointly evaluates candidate actions based on their utility to the current state and their diversity, and employ a greedy algorithm to select an optimal candidate set. Extensive experiments on six diverse standard benchmarks demonstrate that our approach significantly improves overall performance, while maintaining efficient inference without introducing substantial latency. The implementation is available at https://github.com/zhaoxlpku/DynaAct.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [77] [SpeechJudge: Towards Human-Level Judgment for Speech Naturalness](https://arxiv.org/abs/2511.07931)
*Xueyao Zhang,Chaoren Wang,Huan Liao,Ziniu Li,Yuancheng Wang,Li Wang,Dongya Jia,Yuanzhe Chen,Xiulin Li,Zhuo Chen,Zhizheng Wu*

Main category: cs.SD

TL;DR: 提出SpeechJudge框架解决语音合成模型与人类偏好对齐问题，包含数据集、基准测试和奖励模型


<details>
  <summary>Details</summary>
Motivation: 语音合成领域缺乏大规模人类偏好数据集，现有模型难以符合人类感知需求

Method: 1. 构建99K语音对的SpeechJudge-Data数据集
2. 建立SpeechJudge-Eval基准测试
3. 开发基于Qwen2.5-Omni-7B的SpeechJudge-GRM奖励模型（SFT+GRPO强化学习）

Result: SpeechJudge-GRM在评估基准达77.2%准确率（79.4% @10），优于传统Bradley-Terry模型（72.7%）

Conclusion: SpeechJudge框架有效提升语音自然性评估，其奖励机制可辅助生成模型与人类偏好对齐

Abstract: Aligning large generative models with human feedback is a critical challenge. In speech synthesis, this is particularly pronounced due to the lack of a large-scale human preference dataset, which hinders the development of models that truly align with human perception. To address this, we introduce SpeechJudge, a comprehensive suite comprising a dataset, a benchmark, and a reward model centered on naturalness--one of the most fundamental subjective metrics for speech synthesis. First, we present SpeechJudge-Data, a large-scale human feedback corpus of 99K speech pairs. The dataset is constructed using a diverse set of advanced zero-shot text-to-speech (TTS) models across diverse speech styles and multiple languages, with human annotations for both intelligibility and naturalness preference. From this, we establish SpeechJudge-Eval, a challenging benchmark for speech naturalness judgment. Our evaluation reveals that existing metrics and AudioLLMs struggle with this task; the leading model, Gemini-2.5-Flash, achieves less than 70% agreement with human judgment, highlighting a significant gap for improvement. To bridge this gap, we develop SpeechJudge-GRM, a generative reward model (GRM) based on Qwen2.5-Omni-7B. It is trained on SpeechJudge-Data via a two-stage post-training process: Supervised Fine-Tuning (SFT) with Chain-of-Thought rationales followed by Reinforcement Learning (RL) with GRPO on challenging cases. On the SpeechJudge-Eval benchmark, the proposed SpeechJudge-GRM demonstrates superior performance, achieving 77.2% accuracy (and 79.4% after inference-time scaling @10) compared to a classic Bradley-Terry reward model (72.7%). Furthermore, SpeechJudge-GRM can be also employed as a reward function during the post-training of speech generation models to facilitate their alignment with human preferences.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [78] [LLaDA-Rec: Discrete Diffusion for Parallel Semantic ID Generation in Generative Recommendation](https://arxiv.org/abs/2511.06254)
*Teng Shi,Chenglei Shen,Weijie Yu,Shen Nie,Chongxuan Li,Xiao Zhang,Ming He,Yan Han,Jun Xu*

Main category: cs.IR

TL;DR: LLaDA-Rec提出基于离散扩散的生成式推荐框架，通过双向注意力与自适应生成顺序解决传统自回归模型的单向约束和误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 现有自回归模型存在单向注意力限制（只能关注前序token）和固定生成顺序导致的误差传播问题，限制了推荐系统的全局语义建模能力。

Method: 1) 并行标记化方案生成双向建模的语义ID；2) 用户历史级和下一项目级双重掩码机制；3) 适配离散扩散解码的波束搜索策略。

Result: 在三个真实数据集上超越基于ID的推荐模型和现有生成式推荐系统，验证离散扩散作为生成式推荐新范式的有效性。

Conclusion: LLaDA-Rec通过离散扩散框架实现更优的语义建模与误差控制，为生成式推荐开辟了新方向。

Abstract: Generative recommendation represents each item as a semantic ID, i.e., a sequence of discrete tokens, and generates the next item through autoregressive decoding. While effective, existing autoregressive models face two intrinsic limitations: (1) unidirectional constraints, where causal attention restricts each token to attend only to its predecessors, hindering global semantic modeling; and (2) error accumulation, where the fixed left-to-right generation order causes prediction errors in early tokens to propagate to the predictions of subsequent token. To address these issues, we propose LLaDA-Rec, a discrete diffusion framework that reformulates recommendation as parallel semantic ID generation. By combining bidirectional attention with the adaptive generation order, the approach models inter-item and intra-item dependencies more effectively and alleviates error accumulation. Specifically, our approach comprises three key designs: (1) a parallel tokenization scheme that produces semantic IDs for bidirectional modeling, addressing the mismatch between residual quantization and bidirectional architectures; (2) two masking mechanisms at the user-history and next-item levels to capture both inter-item sequential dependencies and intra-item semantic relationships; and (3) an adapted beam search strategy for adaptive-order discrete diffusion decoding, resolving the incompatibility of standard beam search with diffusion-based generation. Experiments on three real-world datasets show that LLaDA-Rec consistently outperforms both ID-based and state-of-the-art generative recommenders, establishing discrete diffusion as a new paradigm for generative recommendation.

</details>


### [79] [BiCA: Effective Biomedical Dense Retrieval with Citation-Aware Hard Negatives](https://arxiv.org/abs/2511.08029)
*Aarush Sinha,Pavan Kumar S,Roshan Balaji,Nirav Pravinbhai Bhatt*

Main category: cs.IR

TL;DR: 提出BiCA方法，利用引文链接生成高质量硬负样本，显著提升生物医学领域检索模型的性能


<details>
  <summary>Details</summary>
Motivation: 针对生物医学领域硬负样本难以区分的问题，发现引用文档具有上下文相关性但非重复的特性，是天然的高质量硬负样本来源

Method: 基于PubMed 20,000篇文献的引文网络，构建引用感知的硬负样本，对GTE_small和GTE_Base模型进行微调，优化领域专用的小型密集检索器

Result: 在BEIR基准测试中，零样本检索的nDCG@10指标实现持续提升；在LoTTE的长尾主题上Success@5超越基线模型

Conclusion: 通过文档链接结构生成信息量丰富的负样本，以最小微调成本实现领域自适应，展示了文献网络结构在提升检索性能中的重要价值

Abstract: Hard negatives are essential for training effective retrieval models. Hard-negative mining typically relies on ranking documents using cross-encoders or static embedding models based on similarity metrics such as cosine distance. Hard negative mining becomes challenging for biomedical and scientific domains due to the difficulty in distinguishing between source and hard negative documents. However, referenced documents naturally share contextual relevance with the source document but are not duplicates, making them well-suited as hard negatives. In this work, we propose BiCA: Biomedical Dense Retrieval with Citation-Aware Hard Negatives, an approach for hard-negative mining by utilizing citation links in 20,000 PubMed articles for improving a domain-specific small dense retriever. We fine-tune the GTE_small and GTE_Base models using these citation-informed negatives and observe consistent improvements in zero-shot dense retrieval using nDCG@10 for both in-domain and out-of-domain tasks on BEIR and outperform baselines on long-tailed topics in LoTTE using Success@5. Our findings highlight the potential of leveraging document link structure to generate highly informative negatives, enabling state-of-the-art performance with minimal fine-tuning and demonstrating a path towards highly data-efficient domain adaptation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [80] [A Decentralized Retrieval Augmented Generation System with Source Reliabilities Secured on Blockchain](https://arxiv.org/abs/2511.07577)
*Yining Lu,Wenyi Tang,Max Johnson,Taeho Jung,Meng Jiang*

Main category: cs.CR

TL;DR: 提出去中心化RAG系统，通过动态可靠性评分机制和区块链智能合约实现多数据源场景下的可信检索，性能提升10.7%并降低56%边际成本。


<details>
  <summary>Details</summary>
Motivation: 解决传统集中式RAG系统存在的数据整合成本高、隐私风险大等问题，实现数据所有者对数据的完全控制。

Method: 1. 动态可靠性评分机制：根据数据源贡献的响应质量动态评估可靠性
2. 区块链智能合约：通过不可篡改的智能合约实现透明化评分管理
3. 使用Llama 3B/8B模型在模拟环境（6个不同可靠性数据源）进行验证

Result: 1. 在真实不可靠数据环境下性能超越集中式系统10.7%
2. 理想可靠环境下接近集中式系统上限
3. 批量更新操作节省约56%边际成本

Conclusion: 去中心化架构在保证安全可信的前提下，通过可靠性评分机制有效提升检索质量，区块链技术实现透明化信任管理，系统已在GitHub开源。

Abstract: Existing retrieval-augmented generation (RAG) systems typically use a centralized architecture, causing a high cost of data collection, integration, and management, as well as privacy concerns. There is a great need for a decentralized RAG system that enables foundation models to utilize information directly from data owners who maintain full control over their sources. However, decentralization brings a challenge: the numerous independent data sources vary significantly in reliability, which can diminish retrieval accuracy and response quality. To address this, our decentralized RAG system has a novel reliability scoring mechanism that dynamically evaluates each source based on the quality of responses it contributes to generate and prioritizes high-quality sources during retrieval. To ensure transparency and trust, the scoring process is securely managed through blockchain-based smart contracts, creating verifiable and tamper-proof reliability records without relying on a central authority. We evaluate our decentralized system with two Llama models (3B and 8B) in two simulated environments where six data sources have different levels of reliability. Our system achieves a +10.7\% performance improvement over its centralized counterpart in the real world-like unreliable data environments. Notably, it approaches the upper-bound performance of centralized systems under ideally reliable data environments. The decentralized infrastructure enables secure and trustworthy scoring management, achieving approximately 56\% marginal cost savings through batched update operations. Our code and system are open-sourced at github.com/yining610/Reliable-dRAG.

</details>


### [81] [SALT: Steering Activations towards Leakage-free Thinking in Chain of Thought](https://arxiv.org/abs/2511.07772)
*Shourya Batra,Pierce Tillman,Samarth Gaggar,Shashank Kesineni,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma,Maheep Chaudhary*

Main category: cs.CR

TL;DR: 提出SALT方法，通过在LLM隐藏层注入转向向量，实现推理过程隐私泄漏减少18.2%-31.2%的同时保持模型性能


<details>
  <summary>Details</summary>
Motivation: 现有隐私保护方案仅关注输出层，但LLM在推理过程中可能通过思维链泄露敏感信息，需平衡隐私保护与模型效用

Method: 识别高泄漏层后，测试时向隐藏状态注入定向转向向量（Steering Activations）的轻量化干预方案

Result: 在QwQ-32B/Llama-3.1-8B/Deepseek模型上分别实现18.2%/17.9%/31.2%的上下文隐私泄漏（CPL）降低，且保持任务性能

Conclusion: SALT为具备推理能力的语言模型提供了实用的测试阶段隐私保护方案，推动LLM个人助手的更安全部署

Abstract: As Large Language Models (LLMs) evolve into personal assistants with access to sensitive user data, they face a critical privacy challenge: while prior work has addressed output-level privacy, recent findings reveal that LLMs often leak private information through their internal reasoning processes, violating contextual privacy expectations. These leaky thoughts occur when models inadvertently expose sensitive details in their reasoning traces, even when final outputs appear safe. The challenge lies in preventing such leakage without compromising the model's reasoning capabilities, requiring a delicate balance between privacy and utility. We introduce Steering Activations towards Leakage-free Thinking (SALT), a lightweight test-time intervention that mitigates privacy leakage in model's Chain of Thought (CoT) by injecting targeted steering vectors into hidden state. We identify the high-leakage layers responsible for this behavior. Through experiments across multiple LLMs, we demonstrate that SALT achieves reductions including $18.2\%$ reduction in CPL on QwQ-32B, $17.9\%$ reduction in CPL on Llama-3.1-8B, and $31.2\%$ reduction in CPL on Deepseek in contextual privacy leakage dataset AirGapAgent-R while maintaining comparable task performance and utility. Our work establishes SALT as a practical approach for test-time privacy protection in reasoning-capable language models, offering a path toward safer deployment of LLM-based personal agents.

</details>


### [82] [LoopLLM: Transferable Energy-Latency Attacks in LLMs via Repetitive Generation](https://arxiv.org/abs/2511.07876)
*Xingyu Li,Xiaolei Liu,Cheng Liu,Yixiao Xu,Kangyi Ding,Bangzhou Xin,Jia-Li Yin*

Main category: cs.CR

TL;DR: 提出LoopLLM框架，通过低熵解码循环触发LLMs持续生成至输出极限，显著提升攻击效率


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法依赖控制终止符号，在长文本场景效果有限。通过诱导重复生成实现持续输出更具可靠性

Method: 1. 重复诱导提示优化：利用自回归漏洞生成循环内容
2. 令牌对齐集成优化：梯度聚合提升跨模型迁移能力

Result: 在14个LLM实验中达到90%+最大输出长度（基线仅20%），迁移性提升40%至DeepSeek/Gemini

Conclusion: LoopLLM通过系统化的提示工程和梯度优化，在攻击持续性和迁移性方面实现突破性改进

Abstract: As large language models (LLMs) scale, their inference incurs substantial computational resources, exposing them to energy-latency attacks, where crafted prompts induce high energy and latency cost. Existing attack methods aim to prolong output by delaying the generation of termination symbols. However, as the output grows longer, controlling the termination symbols through input becomes difficult, making these methods less effective. Therefore, we propose LoopLLM, an energy-latency attack framework based on the observation that repetitive generation can trigger low-entropy decoding loops, reliably compelling LLMs to generate until their output limits. LoopLLM introduces (1) a repetition-inducing prompt optimization that exploits autoregressive vulnerabilities to induce repetitive generation, and (2) a token-aligned ensemble optimization that aggregates gradients to improve cross-model transferability. Extensive experiments on 12 open-source and 2 commercial LLMs show that LoopLLM significantly outperforms existing methods, achieving over 90% of the maximum output length, compared to 20% for baselines, and improving transferability by around 40% to DeepSeek-V3 and Gemini 2.5 Flash.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [83] [Think Before You Retrieve: Learning Test-Time Adaptive Search with Small Language Models](https://arxiv.org/abs/2511.07581)
*Supriti Vijay,Aman Priyanshu,Anu Vellore,Baturay Saglam,Amin Karbasi*

Main category: cs.AI

TL;DR: Orion框架通过合成轨迹生成+强化学习+自反射机制，使小型模型(1.2B)在多项检索任务上超越大200倍的模型。


<details>
  <summary>Details</summary>
Motivation: 现有检索系统存在三大缺陷：神经检索器缺乏推理能力、LLM成本过高、静态查询改写无法适应动态需求。需要模拟人类迭代式信息探索过程。

Method: 三阶段训练：1) 生成多样化搜索轨迹进行微调 2) 强化学习奖励有效回溯和精炼 3) 推理时束搜索实现自反思。形成搜索-评估-修订闭环。

Result: 仅用3%训练数据即在SciFact(77.6%)等6个基准中5个超越基线，部分指标优于400倍规模模型。证明策略学习>参数堆砌。

Conclusion: 检索性能的核心是动态策略而非模型规模。当小模型学会自主搜索、反思和调整时，能以极小成本达到SOTA，这对高效检索系统设计具有范式突破意义。

Abstract: Effective information retrieval requires reasoning over partial evidence and refining strategies as information emerges. Yet current approaches fall short: neural retrievers lack reasoning capabilities, large language models (LLMs) provide semantic depth but at prohibitive cost, and query rewriting or decomposition limits improvement to static transformations. As a result, existing methods fail to capture the iterative dynamics of exploration, feedback, and revision that complex user queries demand. We introduce Orion, a training framework that enables compact models (350M-1.2B parameters) to perform iterative retrieval through learned search strategies. Orion combines: (1) synthetic trajectory generation and supervised fine-tuning to encourage diverse exploration patterns in models, (2) reinforcement learning (RL) that rewards effective query refinement and backtracking behaviors, and (3) inference-time beam search algorithms that exploit the self-reflection capabilities learned during RL. Despite using only 3% of the training data available, our 1.2B model achieves 77.6% success on SciFact (vs. 72.6% for prior retrievers), 25.2% on BRIGHT (vs. 22.1%), 63.2% on NFCorpus (vs. 57.8%), and remains competitive on FEVER, HotpotQA, and MSMarco. It outperforms retrievers up to 200-400x larger on five of six benchmarks. These findings suggest that retrieval performance can emerge from learned strategies, not just model scale, when models are trained to search, reflect, and revise.

</details>


### [84] [Beyond Fact Retrieval: Episodic Memory for RAG with Generative Semantic Workspaces](https://arxiv.org/abs/2511.07587)
*Shreyas Rajesh,Pavan Holur,Chenda Duan,David Chong,Vwani Roychowdhury*

Main category: cs.AI

TL;DR: 提出Generative Semantic Workspace框架解决大语言模型长上下文推理难题，在EpBench基准上性能提升20%，推理token减少51%。


<details>
  <summary>Details</summary>
Motivation: 传统LLM在长文本推理中存在上下文窗口限制和序列长度性能衰减问题，现有存储框架缺乏时空锚定的叙事表征能力。

Method: 神经启发的生成式记忆框架GSW，包含Operator（语义结构映射）和Reconciler（时空逻辑一致性整合）双模块架构。

Result: 在100k-1M token的Episodic Memory Benchmark上超越RAG基线20%，查询时上下文token减少51%，显著降低推理成本。

Conclusion: GSW为LLM提供类人情景记忆，支持长时程角色/行为/时空演变推理，奠定智能体长期推理能力基础。

Abstract: Large Language Models (LLMs) face fundamental challenges in long-context reasoning: many documents exceed their finite context windows, while performance on texts that do fit degrades with sequence length, necessitating their augmentation with external memory frameworks. Current solutions, which have evolved from retrieval using semantic embeddings to more sophisticated structured knowledge graphs representations for improved sense-making and associativity, are tailored for fact-based retrieval and fail to build the space-time-anchored narrative representations required for tracking entities through episodic events. To bridge this gap, we propose the \textbf{Generative Semantic Workspace} (GSW), a neuro-inspired generative memory framework that builds structured, interpretable representations of evolving situations, enabling LLMs to reason over evolving roles, actions, and spatiotemporal contexts. Our framework comprises an \textit{Operator}, which maps incoming observations to intermediate semantic structures, and a \textit{Reconciler}, which integrates these into a persistent workspace that enforces temporal, spatial, and logical coherence. On the Episodic Memory Benchmark (EpBench) \cite{huet_episodic_2025} comprising corpora ranging from 100k to 1M tokens in length, GSW outperforms existing RAG based baselines by up to \textbf{20\%}. Furthermore, GSW is highly efficient, reducing query-time context tokens by \textbf{51\%} compared to the next most token-efficient baseline, reducing inference time costs considerably. More broadly, GSW offers a concrete blueprint for endowing LLMs with human-like episodic memory, paving the way for more capable agents that can reason over long horizons.

</details>


### [85] [ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents](https://arxiv.org/abs/2511.07685)
*Manasi Sharma,Chen Bo Calvin Zhang,Chaithanya Bandi,Clinton Wang,Ankit Aich,Huy Nghiem,Tahseen Rabbani,Ye Htet,Brian Jang,Sumana Basu,Aishwarya Balwani,Denis Peskoff,Marcos Ayestaran,Sean M. Hendryx,Brad Kenstler,Bing Liu*

Main category: cs.AI

TL;DR: 提出ResearchRubrics基准框架，用于评估深度研究代理系统的事实依据、推理严密性和清晰度，发现主流DR系统平均合规率不足68%


<details>
  <summary>Details</summary>
Motivation: 针对开放式深度研究任务缺乏标准化评估体系的问题，现有系统存在隐含语境遗漏和信息推理不足的缺陷

Method: 构建含2500+专家标注细则的基准测试集，建立三维复杂度框架（概念广度/逻辑嵌套/探索深度），开发人工与模型双轨评估协议

Result: Gemini和OpenAI的DR系统在语境捕捉（失败率42%）和检索信息推理（错误率37%）方面存在显著短板

Conclusion: ResearchRubrics揭示了当前DR系统的核心瓶颈，为开发具有可靠推理能力的研究助手提供了可扩展的评估基础设施

Abstract: Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the generation of evidence-backed, long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduce ResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assess factual grounding, reasoning soundness, and clarity. We also propose a new complexity framework for categorizing DR tasks along three axes: conceptual breadth, logical nesting, and exploration. In addition, we develop human and model-based evaluation protocols that measure rubric adherence for DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we release ResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants.

</details>


### [86] [SparseRM: A Lightweight Preference Modeling with Sparse Autoencoder](https://arxiv.org/abs/2511.07896)
*Dengcan Liu,Jiahao Li,Zheren Fu,Yi Tu,Jiajun Li,Zhendong Mao,Yongdong Zhang*

Main category: cs.AI

TL;DR: SparseRM利用稀疏自编码器（SAE）从大语言模型表征中提取偏好相关特征，构建轻量级、可解释的奖励模型，仅需1%可训练参数即可超越主流模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统奖励模型依赖大规模标注数据且训练成本高昂，资源受限场景下难以构建可靠模型。SparseRM旨在通过可解释的特征分解降低计算成本，提升效率与可解释性。

Method: 通过SAE将模型表征分解为可解释的偏好特征方向，计算对齐分数量化特征强度，最终通过简单奖励头聚合预测偏好分数。该方法参数使用量小于传统方法1%。

Result: 在三个偏好建模任务中性能超越主流模型，参数效率提升100倍以上，并能无缝集成到下游对齐流程中。

Conclusion: SparseRM为高效对齐大语言模型提供了参数高效、可解释性强的解决方案，显著降低了模型对齐的算力门槛。

Abstract: Reward models (RMs) are a core component in the post-training of large language models (LLMs), serving as proxies for human preference evaluation and guiding model alignment. However, training reliable RMs under limited resources remains challenging due to the reliance on large-scale preference annotations and the high cost of fine-tuning LLMs. To address this, we propose SparseRM, which leverages Sparse Autoencoder (SAE) to extract preference-relevant information encoded in model representations, enabling the construction of a lightweight and interpretable reward model. SparseRM first employs SAE to decompose LLM representations into interpretable directions that capture preference-relevant features. The representations are then projected onto these directions to compute alignment scores, which quantify the strength of each preference feature in the representations. A simple reward head aggregates these scores to predict preference scores. Experiments on three preference modeling tasks show that SparseRM achieves superior performance over most mainstream RMs while using less than 1% of trainable parameters. Moreover, it integrates seamlessly into downstream alignment pipelines, highlighting its potential for efficient alignment.

</details>


### [87] [Thinker: Training LLMs in Hierarchical Thinking for Deep Search via Multi-Turn Interaction](https://arxiv.org/abs/2511.07943)
*Jun Xu,Xinkai Du,Yu Ao,Peilong Zhao,Yang Li,Ling Zhong,Lin Yuan,Zhongpu Bo,Xiaorui Wang,Mengshu Sun,Zhengke Gui,Dalong Zhang,Zhaoyang Wang,Qiwei Wang,Yangyang Hou,Zhiying Yin,Haofen Wang,Huajun Chen,Lei Liang,Jun Zhou*

Main category: cs.AI

TL;DR: 提出分层思维模型Thinker，通过问题分解、双表示方法和知识边界检测，显著提升LLM外部知识检索效率，在少量训练数据下即达到基线水平，完整训练后全面超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于端到端强化学习的方法缺乏对推理过程的监督，导致逻辑不连贯。需要建立可验证的推理框架来保证复杂问题求解的严谨性。

Method: 1. 将复杂问题分解为独立子问题
2. 每个子问题采用自然语言+逻辑函数双表示
3. 通过逻辑函数传递子问题依赖关系
4. 知识边界检测避免冗余搜索

Result: 数百样本训练即与基线竞争，完整训练集上在多个数据集（WebSPQ、HotpotQA等）和模型规模（7B/13B）中平均提升12.7%准确率

Conclusion: Thinker实现了可监督的推理过程，通过模块化架构增强逻辑连贯性，减少67%不必要的外部搜索，且具有高效训练（<800样本）和强扩展性特点

Abstract: Efficient retrieval of external knowledge bases and web pages is crucial for enhancing the reasoning abilities of LLMs. Previous works on training LLMs to leverage external retrievers for solving complex problems have predominantly employed end-to-end reinforcement learning. However, these approaches neglect supervision over the reasoning process, making it difficult to guarantee logical coherence and rigor. To address these limitations, we propose Thinker, a hierarchical thinking model for deep search through multi-turn interaction, making the reasoning process supervisable and verifiable. It decomposes complex problems into independently solvable sub-problems, each dually represented in both natural language and an equivalent logical function to support knowledge base and web searches. Concurrently, dependencies between sub-problems are passed as parameters via these logical functions, enhancing the logical coherence of the problem-solving process. To avoid unnecessary external searches, we perform knowledge boundary determination to check if a sub-problem is within the LLM's intrinsic knowledge, allowing it to answer directly. Experimental results indicate that with as few as several hundred training samples, the performance of Thinker is competitive with established baselines. Furthermore, when scaled to the full training set, Thinker significantly outperforms these methods across various datasets and model sizes. The source code is available at https://github.com/OpenSPG/KAG-Thinker.

</details>


### [88] [Dual-Process Scaffold Reasoning for Enhancing LLM Code Debugging](https://arxiv.org/abs/2511.08052)
*Po-Chung Hsieh,Chin-Po Chen,Jeng-Lin Li,Ming-Ching Chang*

Main category: cs.AI

TL;DR: 提出基于心理学理论的Scaffold Reasoning框架，通过三流协同机制在代码调试任务中实现88.91%的通过率和5.36秒平均推理时间


<details>
  <summary>Details</summary>
Motivation: 现有LLM推理方法在平衡推理步骤复杂性与计算效率方面存在不足，且缺乏对System 2推理的系统探索。受双系统认知理论启发，试图构建更符合人类认知的代码调试框架。

Method: 包含Scaffold Stream（构建参考代码）、Analytic Stream（错误代码分析）和Integration Stream（多流结果融合）的三流协同框架，通过心理支架引导推理过程。

Result: 在DebugBench基准上取得88.91%通过率，平均每问题推理时间5.36秒，准确率和效率均优于现有方法。跨模型实验显示框架的普适性优势。

Conclusion: 提出的认知支架框架有效平衡推理质量与效率，实验验证其与人类认知过程的一致性。对不同难度问题和错误类型的分析揭示了认知路径优化的边界条件。

Abstract: Recent LLMs have demonstrated sophisticated problem-solving capabilities on various benchmarks through advanced reasoning algorithms. However, the key research question of identifying reasoning steps that balance complexity and computational efficiency remains unsolved. Recent research has increasingly drawn upon psychological theories to explore strategies for optimizing cognitive pathways. The LLM's final outputs and intermediate steps are regarded as System 1 and System 2, respectively. However, an in-depth exploration of the System 2 reasoning is still lacking. Therefore, we propose a novel psychologically backed Scaffold Reasoning framework for code debugging, which encompasses the Scaffold Stream, Analytic Stream, and Integration Stream. The construction of reference code within the Scaffold Stream is integrated with the buggy code analysis results produced by the Analytic Stream through the Integration Stream. Our framework achieves an 88.91% pass rate and an average inference time of 5.36 seconds per-problem on DebugBench, outperforming other reasoning approaches across various LLMs in both reasoning accuracy and efficiency. Further analyses elucidate the advantages and limitations of various cognitive pathways across varying problem difficulties and bug types. Our findings also corroborate the alignment of the proposed Scaffold Reasoning framework with human cognitive processes.

</details>


### [89] [Information Capacity: Evaluating the Efficiency of Large Language Models via Text Compression](https://arxiv.org/abs/2511.08066)
*Cheng Yuan,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: 提出信息容量作为LLM效率的统一指标，通过文本压缩性能与计算复杂度的比值衡量模型效率，实证显示同系列模型信息容量一致且能跨系列比较


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型快速发展带来的计算资源紧张问题，建立跨模型规模和架构的统一效率评估标准，同时揭示压缩能力与模型智能的潜在关联

Method: 基于文本压缩性能与计算复杂度的比值定义信息容量指标，在5个异构数据集上评估49个主流开源模型，分析分词器效率、预训练数据和混合专家架构的影响

Result: 同系列不同规模模型展现稳定信息容量，该指标可实现跨模型系列公平比较及同系列性能预测，验证分词器效率对评估的重要影响

Conclusion: 信息容量为LLM效率评估提供统一框架，其融合分词器效率的独特设计弥补传统评估盲点，为模型优化方向提供新视角

Abstract: Recent years have witnessed the rapid advancements of large language models (LLMs) and their expanding applications, leading to soaring demands for computational resources. The widespread adoption of test-time scaling further aggravates the tension between model capability and resource consumption, highlighting the importance of inference efficiency. However, a unified metric that accurately reflects an LLM's efficiency across different model sizes and architectures remains absent. Motivated by the correlation between compression and intelligence, we introduce information capacity, a measure of model efficiency based on text compression performance relative to computational complexity. Larger models can predict the next token more accurately, achieving greater compression gains but at higher computational costs. Empirical evaluations on mainstream open-source models show that models of varying sizes within a series exhibit consistent information capacity. This metric enables a fair efficiency comparison across model series and accurate performance prediction within a model series. A distinctive feature of information capacity is that it incorporates tokenizer efficiency, which affects both input and output token counts but is often neglected in LLM evaluations. We assess the information capacity of 49 models on 5 heterogeneous datasets and observe consistent results on the influences of tokenizer efficiency, pretraining data, and the mixture-of-experts architecture.

</details>


### [90] [SciAgent: A Unified Multi-Agent System for Generalistic Scientific Reasoning](https://arxiv.org/abs/2511.08151)
*Xuchen Li,Ruitao Wu,Xuanbo Liu,Xukai Wang,Jinbo Hu,Zhixin Bai,Bohan Zeng,Hao Liang,Leheng Chen,Mingrui Chen,Haitian Zhong,Xuanlin Yang,Xu-Yao Zhang,Liu Liu,Jia Li,Kaiqi Huang,Jiahao Xu,Haitao Mi,Wentao Zhang,Bin Dong*

Main category: cs.AI

TL;DR: 提出了统一多智能体系统SciAgent，通过分层协作机制实现跨学科的科学推理，在多个学科奥赛中达到或超越人类金牌水平


<details>
  <summary>Details</summary>
Motivation: 现有专用AI系统在科学领域存在局限，需要构建具备跨学科适应能力的通用科学推理系统

Method: 分层智能体架构：协调者代理分析问题复杂度，动态调度由符号推理、概念建模、数值计算等子智能体组成的专业系统，定制化构建推理流程

Result: 在数学（IMO/IMC）、物理（IPhO/CPhO）奥赛中达到金牌水平，在化学奥赛（IChO）和HLE基准测试中验证跨领域泛化能力

Conclusion: SciAgent标志着向通用科学智能迈出重要一步，证明了多智能体协作在实现跨学科专家级推理方面的有效性

Abstract: Recent advances in large language models have enabled AI systems to achieve expert-level performance on domain-specific scientific tasks, yet these systems remain narrow and handcrafted. We introduce SciAgent, a unified multi-agent system designed for generalistic scientific reasoning-the ability to adapt reasoning strategies across disciplines and difficulty levels. SciAgent organizes problem solving as a hierarchical process: a Coordinator Agent interprets each problem's domain and complexity, dynamically orchestrating specialized Worker Systems, each composed of interacting reasoning Sub-agents for symbolic deduction, conceptual modeling, numerical computation, and verification. These agents collaboratively assemble and refine reasoning pipelines tailored to each task. Across mathematics and physics Olympiads (IMO, IMC, IPhO, CPhO), SciAgent consistently attains or surpasses human gold-medalist performance, demonstrating both domain generality and reasoning adaptability. Additionally, SciAgent has been tested on the International Chemistry Olympiad (IChO) and selected problems from the Humanity's Last Exam (HLE) benchmark, further confirming the system's ability to generalize across diverse scientific domains. This work establishes SciAgent as a concrete step toward generalistic scientific intelligence-AI systems capable of coherent, cross-disciplinary reasoning at expert levels.

</details>


### [91] [Towards Outcome-Oriented, Task-Agnostic Evaluation of AI Agents](https://arxiv.org/abs/2511.08242)
*Waseem AlShikh,Muayad Sayed Ali,Brian Kennedy,Dmytro Mozolevskyi*

Main category: cs.AI

TL;DR: 提出包含11个任务无关指标的AI代理评估框架，实验验证混合架构代理综合表现最优（平均目标达成率88.8%，ROI最高）


<details>
  <summary>Details</summary>
Motivation: 传统基础设施指标无法衡量AI代理的决策质量、自主性和业务价值，需要建立更全面的评估体系

Method: 通过跨5大领域（医疗/金融/营销/法律/客服）的大规模模拟实验，对比ReAct、思维链、工具增强和混合架构代理在11个新指标下的表现

Result: 混合代理在GCR（88.8%）、AIx自主指数等关键指标全面领先，不同架构存在显著性能取舍，ROI最高达3.2倍

Conclusion: 该框架为AI代理的标准化评估提供方法论，推动更有效的开发部署与治理体系建设

Abstract: As AI agents proliferate across industries and applications, evaluating their performance based solely on infrastructural metrics such as latency, time-to-first-token, or token throughput is proving insufficient. These metrics fail to capture the quality of an agent's decisions, its operational autonomy, or its ultimate business value. This white paper proposes a novel, comprehensive framework of eleven outcome-based, task-agnostic performance metrics for AI agents that transcend domain boundaries. These metrics are designed to enable organizations to evaluate agents based on the quality of their decisions, their degree of autonomy, their adaptability to new challenges, and the tangible business value they deliver, regardless of the underlying model architecture or specific use case. We introduce metrics such as Goal Completion Rate (GCR), Autonomy Index (AIx), Multi-Step Task Resilience (MTR), and Business Impact Efficiency (BIE). Through a large-scale simulated experiment involving four distinct agent architectures (ReAct, Chain-of-Thought, Tool-Augmented, Hybrid) across five diverse domains (Healthcare, Finance, Marketing, Legal, and Customer Service), we demonstrate the framework's efficacy. Our results reveal significant performance trade-offs between different agent designs, highlighting the Hybrid Agent as the most consistently high-performing model across the majority of our proposed metrics, achieving an average Goal Completion Rate of 88.8\% and the highest Return on Investment (ROI). This work provides a robust, standardized methodology for the holistic evaluation of AI agents, paving the way for more effective development, deployment, and governance.

</details>


### [92] [Multi-Agent GraphRAG: A Text-to-Cypher Framework for Labeled Property Graphs](https://arxiv.org/abs/2511.08274)
*Anton Gusarov,Anastasia Volkova,Valentin Khrulkov,Andrey Kuznetsov,Evgenii Maslov,Ivan Oseledets*

Main category: cs.AI

TL;DR: 提出多智能体GraphRAG框架，通过自然语言生成Cypher查询实现LPG图数据库交互，验证了在通用领域和工业数字孪生场景的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有GraphRAG研究过度依赖RDF三元组和SPARQL查询，未能充分挖掘Cypher语言和LPG图数据库在可扩展性、语义推理方面的潜力。

Method: 基于LLM构建模块化代理系统，采用内容感知的迭代校正机制和聚合反馈循环，结合Memgraph图数据库实现查询的语义语法双优化。

Result: 在CypherBench多领域测试集和IFC建筑数字孪生图谱上验证系统性能，证明可支持工业级数字自动化应用场景。

Conclusion: 该框架有效连接AI与工业物联网，为基于知识图谱的复杂系统智能化提供可扩展解决方案。

Abstract: While Retrieval-Augmented Generation (RAG) methods commonly draw information from unstructured documents, the emerging paradigm of GraphRAG aims to leverage structured data such as knowledge graphs. Most existing GraphRAG efforts focus on Resource Description Framework (RDF) knowledge graphs, relying on triple representations and SPARQL queries. However, the potential of Cypher and Labeled Property Graph (LPG) databases to serve as scalable and effective reasoning engines within GraphRAG pipelines remains underexplored in current research literature. To fill this gap, we propose Multi-Agent GraphRAG, a modular LLM agentic system for text-to-Cypher query generation serving as a natural language interface to LPG-based graph data. Our proof-of-concept system features an LLM-based workflow for automated Cypher queries generation and execution, using Memgraph as the graph database backend. Iterative content-aware correction and normalization, reinforced by an aggregated feedback loop, ensures both semantic and syntactic refinement of generated queries. We evaluate our system on the CypherBench graph dataset covering several general domains with diverse types of queries. In addition, we demonstrate performance of the proposed workflow on a property graph derived from the IFC (Industry Foundation Classes) data, representing a digital twin of a building. This highlights how such an approach can bridge AI with real-world applications at scale, enabling industrial digital automation use cases.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [93] [Hybrid Quantum-Classical Selective State Space Artificial Intelligence](https://arxiv.org/abs/2511.08349)
*Amin Ebrahimi,Farzan Haddadi*

Main category: quant-ph

TL;DR: 提出混合量子经典选择机制增强Mamba架构，在时序分类任务中量子门控模块提升特征提取能力


<details>
  <summary>Details</summary>
Motivation: 解决传统NLP模型因大规模矩阵运算和高维优化导致的算力瓶颈，利用量子资源提升表示学习效率

Method: 将变分量子电路(VQC)作为量子门控模块集成到Mamba架构，通过量子子程序改进特征提取与信息抑制机制

Result: 改造MNIST数据集任务中，混合模型前4轮准确率24.6%（经典模型21.6%），参数效率与表达能力显著提升

Conclusion: 量子增强的门控机制为构建可扩展、资源高效的NLP模型提供新路径，量子子程序可突破经典计算复杂度限制

Abstract: Hybrid Quantum Classical (HQC) algorithms constitute one of the most effective paradigms for exploiting the computational advantages of quantum systems in large-scale numerical tasks. By operating in high-dimensional Hilbert spaces, quantum circuits enable exponential speed-ups and provide access to richer representations of cost landscapes compared to purely classical methods. These capabilities are particularly relevant for machine learning, where state-of-the-art models especially in Natural Language Processing (NLP) suffer from prohibitive time complexity due to massive matrix multiplications and high-dimensional optimization.
  In this manuscript, we propose a Hybrid Quantum Classical selection mechanism for the Mamba architecture, designed specifically for temporal sequence classification problems. Our approach leverages Variational Quantum Circuits (VQCs) as quantum gating modules that both enhance feature extraction and improve suppression of irrelevant information. This integration directly addresses the computational bottlenecks of deep learning architectures by exploiting quantum resources for more efficient representation learning.
  We analyze how introducing quantum subroutines into large language models (LLMs) impacts their generalization capability, expressivity, and parameter efficiency. The results highlight the potential of quantum-enhanced gating mechanisms as a path toward scalable, resource-efficient NLP models, in a limited simulation step. Within the first four epochs on a reshaped MNIST dataset with input format (batch, 784, d_model), our hybrid model achieved 24.6% accuracy while using one quantum layer and achieve higher expressivity, compared to 21.6% obtained by a purely classical selection mechanism. we state No founding

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [94] [Network and Systems Performance Characterization of MCP-Enabled LLM Agents](https://arxiv.org/abs/2511.07426)
*Zihao Ding,Mufeng Zhu,Yao Liu*

Main category: cs.DC

TL;DR: MCP增强LLM与外部工具的交互能力，但显著增加token使用量及成本。本文通过测量分析揭示了性能、能力与成本的权衡，并提出并行工具调用等优化方案。


<details>
  <summary>Details</summary>
Motivation: MCP标准化交互方式虽然提升LLM能力，但扩展的上下文信息导致token消耗激增，亟需量化评估其对成本、计算负载和任务成功率的影响。

Method: 采用基于实际测量的分析方法，对比不同LLM模型在多种MCP配置下的token效率、任务耗时、成本和成功率等关键指标。

Result: 发现能力增强与成本上升的正相关关系，验证并行工具调用可使任务耗时降低38%，强健的中断机制提升15%的任务成功率。

Conclusion: 通过优化MCP配置（如并行处理、智能中断）可实现效率与成本的平衡，为构建高性价比的LLM增强工作流提供实证依据。

Abstract: Model Context Protocol (MCP) has recently gained increased attention within the AI community for providing a standardized way for large language models (LLMs) to interact with external tools and services, significantly enhancing their capabilities. However, the inclusion of extensive contextual information, including system prompts, MCP tool definitions, and context histories, in MCP-enabled LLM interactions, dramatically inflates token usage. Given that LLM providers charge based on tokens, these expanded contexts can quickly escalate monetary costs and increase the computational load on LLM services. This paper presents a comprehensive measurement-based analysis of MCP-enabled interactions with LLMs, revealing trade-offs between capability, performance, and cost. We explore how different LLM models and MCP configurations impact key performance metrics such as token efficiency, monetary cost, task completion times, and task success rates, and suggest potential optimizations, including enabling parallel tool calls and implementing robust task abort mechanisms. These findings provide useful insights for developing more efficient, robust, and cost-effective MCP-enabled workflows.

</details>


### [95] [Intelligence per Watt: Measuring Intelligence Efficiency of Local AI](https://arxiv.org/abs/2511.07885)
*Jon Saad-Falcon,Avanika Narayan,Hakki Orhun Akengin,J. Wes Griffin,Herumb Shandilya,Adrian Gamarra Lafuente,Medhya Goel,Rebecca Joseph,Shlok Natarajan,Etash Kumar Guha,Shang Zhu,Ben Athiwaratkun,John Hennessy,Azalia Mirhoseini,Christopher Ré*

Main category: cs.DC

TL;DR: 研究提出智能每瓦特（IPW）指标，证明本地推理可有效分流88.7%的单轮聊天/推理查询，2023-2025年IPW提升5.3倍，本地加速器效率优于云端。


<details>
  <summary>Details</summary>
Motivation: 集中式云基础设施面临算力扩展压力，本地小型语言模型（<=20B参数）性能接近前沿模型，结合本地加速器可实现低延迟推理。

Method: 对20+本地模型、8种加速器、100万真实单轮聊天/推理查询进行大规模实证分析，测量精度、能耗、延迟、功率等指标。

Result: 1. 本地模型准确率88.7%；2. IPW 2023-2025提升5.3倍，查询覆盖率从23.2%升至71.3%；3. 本地加速器IPW比云端低1.4倍以上。

Conclusion: 智能每瓦特（IPW）是跟踪本地推理转型的核心指标，本地推理可有效分流云基础设施压力，研究发布IPW评测工具支持系统化能效基准测试。

Abstract: Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals $3$ findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [96] [LLM-Powered Fully Automated Chaos Engineering: Towards Enabling Anyone to Build Resilient Software Systems at Low Cost](https://arxiv.org/abs/2511.07865)
*Daisuke Kikuta,Hiroki Ikeuchi,Kengo Tajiri*

Main category: cs.SE

TL;DR: 提出ChaosEater系统，利用大语言模型全自动化混沌工程周期，显著降低Kubernetes系统韧性建设成本


<details>
  <summary>Details</summary>
Motivation: 现有混沌工程工具在实验规划和系统改进环节依赖人工，存在效率低、专业门槛高的问题

Method: 通过预定义代理工作流分解CE周期，利用LLM完成需求定义、代码生成、测试调试等软件工程任务

Result: 案例研究表明系统能高效完成合理CE周期（小型系统4.5小时/$0.8，大型系统10小时/$3.6）

Conclusion: 成功验证LLM驱动自动化混沌工程的可行性，为不同规模Kubernetes系统提供低成本韧性解决方案

Abstract: Chaos Engineering (CE) is an engineering technique aimed at improving the resilience of distributed systems. It involves intentionally injecting faults into a system to test its resilience, uncover weaknesses, and address them before they cause failures in production. Recent CE tools automate the execution of predefined CE experiments. However, planning such experiments and improving the system based on the experimental results still remain manual. These processes are labor-intensive and require multi-domain expertise. To address these challenges and enable anyone to build resilient systems at low cost, this paper proposes ChaosEater, a system that automates the entire CE cycle with Large Language Models (LLMs). It predefines an agentic workflow according to a systematic CE cycle and assigns subdivided processes within the workflow to LLMs. ChaosEater targets CE for software systems built on Kubernetes. Therefore, the LLMs in ChaosEater complete CE cycles through software engineering tasks, including requirement definition, code generation, testing, and debugging. We evaluate ChaosEater through case studies on small- and large-scale Kubernetes systems. The results demonstrate that it consistently completes reasonable CE cycles with significantly low time and monetary costs. Its cycles are also qualitatively validated by human engineers and LLMs.

</details>
