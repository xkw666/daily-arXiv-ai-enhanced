{"id": "2505.11729", "pdf": "https://arxiv.org/pdf/2505.11729", "abs": "https://arxiv.org/abs/2505.11729", "authors": ["Pedro Figueiredo", "Qihao He", "Steve Bako", "Nima Khademi Kalantari"], "title": "Neural Importance Sampling of Many Lights", "categories": ["cs.GR", "cs.LG"], "comment": "11 pages, 11 figures. Accepted for publication in SIGGRAPH Conference\n  Papers '25; to be presented at SIGGRAPH 2025", "summary": "We propose a neural approach for estimating spatially varying light selection\ndistributions to improve importance sampling in Monte Carlo rendering,\nparticularly for complex scenes with many light sources. Our method uses a\nneural network to predict the light selection distribution at each shading\npoint based on local information, trained by minimizing the KL-divergence\nbetween the learned and target distributions in an online manner. To\nefficiently manage hundreds or thousands of lights, we integrate our neural\napproach with light hierarchy techniques, where the network predicts\ncluster-level distributions and existing methods sample lights within clusters.\nAdditionally, we introduce a residual learning strategy that leverages initial\ndistributions from existing techniques, accelerating convergence during\ntraining. Our method achieves superior performance across diverse and\nchallenging scenes.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u52a8\u6001\u5149\u6e90\u9009\u62e9\u5206\u5e03\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5149\u7ebf\u5c42\u6b21\u7ed3\u6784\u548c\u6b8b\u5dee\u5b66\u4e60\u7b56\u7565\u63d0\u5347\u590d\u6742\u573a\u666f\u8499\u7279\u5361\u6d1b\u6e32\u67d3\u6548\u7387", "motivation": "\u89e3\u51b3\u590d\u6742\u591a\u5149\u6e90\u573a\u666f\u4e2d\u4f20\u7edf\u91cd\u8981\u6027\u91c7\u6837\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u81ea\u9002\u5e94\u5b66\u4e60\u5149\u6e90\u5206\u5e03", "method": "1. \u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u6839\u636e\u5c40\u90e8\u4fe1\u606f\u9884\u6d4b\u5149\u6e90\u9009\u62e9\u5206\u5e03\n2. \u7ed3\u5408\u5149\u7ebf\u5c42\u6b21\u7ed3\u6784\u8fdb\u884c\u96c6\u7fa4\u7ea7\u5206\u5e03\u9884\u6d4b\n3. \u6b8b\u5dee\u5b66\u4e60\u7b56\u7565\u52a0\u901f\u8bad\u7ec3\u6536\u655b", "result": "\u5728\u591a\u6837\u5316\u590d\u6742\u573a\u666f\u4e2d\u5b9e\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6e32\u67d3\u6027\u80fd", "conclusion": "\u795e\u7ecf\u7f51\u7edc\u4e0e\u4f20\u7edf\u5c42\u6b21\u5316\u91c7\u6837\u65b9\u6cd5\u7684\u7ed3\u5408\u4e3a\u8499\u7279\u5361\u6d1b\u6e32\u67d3\u63d0\u4f9b\u4e86\u65b0\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.11799", "pdf": "https://arxiv.org/pdf/2505.11799", "abs": "https://arxiv.org/abs/2505.11799", "authors": ["Rushan Ziatdinov", "Rifkat Nabiyev"], "title": "Generating Digital Models Using Text-to-3D and Image-to-3D Prompts: Critical Case Study", "categories": ["cs.GR", "cs.MM"], "comment": "6 pages, 11 figures", "summary": "In the world of technology and AI, digital models play an important role in\nour lives and are an essential part of the digital twins of real-world objects.\nThey can be created by designers, artists, or game developers using spline\ncurves and surfaces, meshes, and voxels, but making such models is too\ntime-consuming. With the growth of AI tools, there is interest in the automated\ngeneration of 3D models, such as generative design approaches, which can save\ncreators valuable time. This paper reviews several online 3D model generators\nand critically analyses the results, hoping to see higher-quality results from\ndifferent prompts.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u5728\u7ebf3D\u6a21\u578b\u751f\u6210\u5de5\u5177\uff0c\u901a\u8fc7\u591a\u63d0\u793a\u6d4b\u8bd5\u8bc4\u4f30\u751f\u6210\u8d28\u91cf\uff0c\u63a2\u7d22\u751f\u6210\u5f0f\u8bbe\u8ba1\u5bf9\u521b\u4f5c\u6548\u7387\u7684\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf3D\u5efa\u6a21\u8017\u65f6\u8d39\u529b\uff0c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30AI\u751f\u6210\u5de5\u5177\u5982\u4f55\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u7a0b\u5e2e\u52a9\u521b\u4f5c\u8005\u8282\u7701\u65f6\u95f4\u3002", "method": "\u6a2a\u5411\u5bf9\u6bd4\u591a\u4e2a\u5728\u7ebf3D\u6a21\u578b\u751f\u6210\u5e73\u53f0\uff0c\u91c7\u7528\u4e0d\u540c\u6587\u672c\u63d0\u793a\u8fdb\u884c\u751f\u6210\u6d4b\u8bd5\u5e76\u91cf\u5316\u8d28\u91cf\u8bc4\u4f30\u3002", "result": "\u73b0\u6709\u5de5\u5177\u751f\u6210\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\uff0c\u63d0\u793a\u8bcd\u8bbe\u8ba1\u663e\u8457\u5f71\u54cd\u8f93\u51fa\u6548\u679c\uff0c\u90e8\u5206\u5de5\u5177\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u8fbe\u5230\u53ef\u7528\u6c34\u5e73\u3002", "conclusion": "AI\u751f\u62103D\u6a21\u578b\u5177\u6709\u5b9e\u7528\u6f5c\u529b\uff0c\u4f46\u9700\u4f18\u5316\u7b97\u6cd5\u4e0e\u63d0\u793a\u5de5\u7a0b\u624d\u80fd\u6ee1\u8db3\u4e13\u4e1a\u521b\u4f5c\u9700\u6c42\u3002"}}
{"id": "2505.12373", "pdf": "https://arxiv.org/pdf/2505.12373", "abs": "https://arxiv.org/abs/2505.12373", "authors": ["Kapil Dev"], "title": "Modeling Aesthetic Preferences in 3D Shapes: A Large-Scale Paired Comparison Study Across Object Categories", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "11 pages, 8 figures, submitted to IEEE Transactions on Visualization\n  and Computer Graphics (TVCG)", "summary": "Human aesthetic preferences for 3D shapes are central to industrial design,\nvirtual reality, and consumer product development. However, most computational\nmodels of 3D aesthetics lack empirical grounding in large-scale human\njudgments, limiting their practical relevance. We present a large-scale study\nof human preferences. We collected 22,301 pairwise comparisons across five\nobject categories (chairs, tables, mugs, lamps, and dining chairs) via Amazon\nMechanical Turk. Building on a previously published\ndataset~\\cite{dev2020learning}, we introduce new non-linear modeling and\ncross-category analysis to uncover the geometric drivers of aesthetic\npreference. We apply the Bradley-Terry model to infer latent aesthetic scores\nand use Random Forests with SHAP analysis to identify and interpret the most\ninfluential geometric features (e.g., symmetry, curvature, compactness). Our\ncross-category analysis reveals both universal principles and domain-specific\ntrends in aesthetic preferences. We focus on human interpretable geometric\nfeatures to ensure model transparency and actionable design insights, rather\nthan relying on black-box deep learning approaches. Our findings bridge\ncomputational aesthetics and cognitive science, providing practical guidance\nfor designers and a publicly available dataset to support reproducibility. This\nwork advances the understanding of 3D shape aesthetics through a human-centric,\ndata-driven framework.", "AI": {"tldr": "\u901a\u8fc7\u6536\u96c622,301\u7ec4\u4eba\u7c7b\u5ba1\u7f8e\u504f\u597d\u6570\u636e\uff0c\u7ed3\u5408\u975e\u7ebf\u6027\u5efa\u6a21\u548c\u8de8\u54c1\u7c7b\u5206\u6790\uff0c\u63ed\u793a\u4e863D\u5f62\u72b6\u5ba1\u7f8e\u504f\u597d\u7684\u51e0\u4f55\u7279\u5f81\u89c4\u5f8b\u4e0e\u8bbe\u8ba1\u542f\u793a", "motivation": "\u73b0\u67093D\u7f8e\u5b66\u8ba1\u7b97\u6a21\u578b\u7f3a\u4e4f\u5927\u89c4\u6a21\u4eba\u7c7b\u5224\u65ad\u6570\u636e\u652f\u6491\uff0c\u96be\u4ee5\u6307\u5bfc\u5b9e\u9645\u8bbe\u8ba1\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u5efa\u7acb\u53ef\u89e3\u91ca\u7684\u5ba1\u7f8e\u504f\u597d\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u4e9a\u9a6c\u900a\u4f17\u5305\u5e73\u53f0\u6536\u96c6\u4e94\u7c7b\u7269\u54c1\uff08\u6905\u5b50/\u684c\u5b50/\u676f\u5b50\u7b49\uff09\u7684\u6210\u5bf9\u5ba1\u7f8e\u6bd4\u8f83\u6570\u636e\uff0c\u5e94\u7528Bradley-Terry\u6a21\u578b\u8ba1\u7b97\u6f5c\u5728\u5ba1\u7f8e\u8bc4\u5206\uff0c\u7ed3\u5408\u968f\u673a\u68ee\u6797\u548cSHAP\u5206\u6790\u89e3\u91ca\u51e0\u4f55\u7279\u5f81\uff08\u5bf9\u79f0\u6027/\u66f2\u7387/\u7d27\u51d1\u5ea6\u7b49\uff09\u7684\u5f71\u54cd", "result": "\u53d1\u73b0\u5ba1\u7f8e\u504f\u597d\u5b58\u5728\u8de8\u54c1\u7c7b\u7684\u901a\u7528\u539f\u5219\uff08\u5982\u5bf9\u79f0\u504f\u597d\uff09\u548c\u9886\u57df\u7279\u5b9a\u8d8b\u52bf\uff08\u5982\u706f\u5177\u66f4\u5f3a\u8c03\u52a8\u6001\u66f2\u7ebf\uff09\uff0c\u5efa\u7acb\u4e86\u57fa\u4e8e\u53ef\u89e3\u91ca\u51e0\u4f55\u7279\u5f81\u7684\u5206\u6790\u6846\u67b6", "conclusion": "\u7814\u7a76\u4e3a\u8bbe\u8ba1\u5e08\u63d0\u4f9b\u6570\u636e\u652f\u6301\uff0c\u516c\u5f00\u6570\u636e\u96c6\u4fc3\u8fdb\u53ef\u91cd\u590d\u6027\uff0c\u901a\u8fc7\u4eba\u672c\u4e3b\u4e49\u65b9\u6cd5\u63a8\u8fdb\u8ba1\u7b97\u7f8e\u5b66\u4e0e\u8ba4\u77e5\u79d1\u5b66\u7684\u4ea4\u53c9\u5e94\u7528"}}
{"id": "2505.12539", "pdf": "https://arxiv.org/pdf/2505.12539", "abs": "https://arxiv.org/abs/2505.12539", "authors": ["Jinyuan Liu", "Yuchen Sun", "Yin Yang", "Chenfanfu Jiang", "Minchen Li", "Bo Zhu"], "title": "Penetration-free Solid-Fluid Interaction on Shells and Rods", "categories": ["cs.GR"], "comment": null, "summary": "We introduce a novel approach to simulate the interaction between fluids and\nthin elastic solids without any penetration. Our approach is centered around an\noptimization system augmented with barriers, which aims to find a configuration\nthat ensures the absence of penetration while enforcing incompressibility for\nthe fluids and minimizing elastic potentials for the solids. Unlike previous\nmethods that primarily focus on velocity coherence at the fluid-solid\ninterfaces, we demonstrate the effectiveness and flexibility of explicitly\nresolving positional constraints, including both explicit representation of\nsolid positions and the implicit representation of fluid level-set interface.\nTo preserve the volume of the fluid, we propose a simple yet efficient approach\nthat adjusts the associated level-set values. Additionally, we develop a\ndistance metric capable of measuring the separation between an implicitly\nrepresented surface and a Lagrangian object of arbitrary codimension. By\nintegrating the inertia, solid elastic potential, damping, barrier potential,\nand fluid incompressibility within a unified system, we are able to robustly\nsimulate a wide range of processes involving fluid interactions with\nlower-dimensional objects such as shells and rods. These processes include\ntopology changes, bouncing, splashing, sliding, rolling, floating, and more.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u969c\u788d\u7269\u589e\u5f3a\u4f18\u5316\u7cfb\u7edf\u7684\u65b0\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6d41\u4f53\u4e0e\u8584\u5f39\u6027\u56fa\u4f53\u7684\u65e0\u7a7f\u900f\u4ea4\u4e92\u4eff\u771f\uff0c\u80fd\u591f\u5904\u7406\u98de\u6e85\u3001\u6ed1\u52a8\u3001\u6f02\u6d6e\u7b49\u591a\u79cd\u590d\u6742\u6d41\u4f53-\u56fa\u4f53\u4ea4\u4e92\u8fc7\u7a0b\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u6d41\u4f53-\u56fa\u4f53\u754c\u9762\u901f\u5ea6\u4e00\u81f4\u6027\uff0c\u672c\u7814\u7a76\u901a\u8fc7\u663e\u5f0f\u5904\u7406\u4f4d\u7f6e\u7ea6\u675f\uff08\u5305\u542b\u56fa\u4f53\u663e\u5f0f\u4f4d\u7f6e\u548c\u6d41\u4f53\u9690\u5f0f\u754c\u9762\u8868\u793a\uff09\uff0c\u7a81\u7834\u7a7f\u900f\u9650\u5236\u5e76\u4fdd\u6301\u6d41\u4f53\u4f53\u79ef\u5b88\u6052\u3002", "method": "1. \u6784\u5efa\u542b\u969c\u788d\u7269\u7684\u4f18\u5316\u7cfb\u7edf\u786e\u4fdd\u65e0\u7a7f\u900f\n2. \u5f00\u53d1\u6c34\u5e73\u96c6\u503c\u8c03\u6574\u7b56\u7565\u4fdd\u6301\u6d41\u4f53\u4f53\u79ef\n3. \u63d0\u51fa\u65b0\u578b\u8ddd\u79bb\u5ea6\u91cf\u65b9\u6cd5\u6d4b\u91cf\u9690\u5f0f\u8868\u9762\u4e0e\u4efb\u610f\u7ef4\u5ea6\u7269\u4f53\u95f4\u8ddd\n4. \u6574\u5408\u60ef\u6027\u3001\u5f39\u6027\u52bf\u80fd\u3001\u963b\u5c3c\u7b49\u591a\u7269\u7406\u91cf\u4e8e\u7edf\u4e00\u6846\u67b6", "result": "\u6210\u529f\u6a21\u62df\u6d41\u4f53\u4e0e\u58f3/\u6746\u7b49\u4f4e\u7ef4\u7269\u4f53\u7684\u590d\u6742\u4ea4\u4e92\uff08\u62d3\u6251\u53d8\u5316\u3001\u5f39\u8df3\u3001\u98de\u6e85\u3001\u6ed1\u52a8\u7b49\uff09\uff0c\u652f\u6301\u4ece\u6c34\u82b1\u98de\u6e85\u5230\u7269\u4f53\u6f02\u6d6e\u7b49\u591a\u6837\u5316\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u7834\u4e86\u4f20\u7edf\u901f\u5ea6\u5339\u914d\u8303\u5f0f\u7684\u5c40\u9650\uff0c\u901a\u8fc7\u663e\u5f0f\u7ea6\u675f\u5904\u7406\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u3001\u66f4\u7075\u6d3b\u7684\u6d41\u4f53-\u56fa\u4f53\u4ea4\u4e92\u4eff\u771f\uff0c\u663e\u8457\u6269\u5c55\u4e86\u7269\u7406\u4eff\u771f\u7684\u5e94\u7528\u8fb9\u754c\u3002"}}
{"id": "2505.11533", "pdf": "https://arxiv.org/pdf/2505.11533", "abs": "https://arxiv.org/abs/2505.11533", "authors": ["Jinqiang Wang", "Huansheng Ning", "Tao Zhu", "Jianguo Ding"], "title": "A Data Synthesis Method Driven by Large Language Models for Proactive Mining of Implicit User Intentions in Tourism", "categories": ["cs.CL"], "comment": null, "summary": "In the tourism domain, Large Language Models (LLMs) often struggle to mine\nimplicit user intentions from tourists' ambiguous inquiries and lack the\ncapacity to proactively guide users toward clarifying their needs. A critical\nbottleneck is the scarcity of high-quality training datasets that facilitate\nproactive questioning and implicit intention mining. While recent advances\nleverage LLM-driven data synthesis to generate such datasets and transfer\nspecialized knowledge to downstream models, existing approaches suffer from\nseveral shortcomings: (1) lack of adaptation to the tourism domain, (2) skewed\ndistributions of detail levels in initial inquiries, (3) contextual redundancy\nin the implicit intention mining module, and (4) lack of explicit thinking\nabout tourists' emotions and intention values. Therefore, we propose SynPT (A\nData Synthesis Method Driven by LLMs for Proactive Mining of Implicit User\nIntentions in the Tourism), which constructs an LLM-driven user agent and\nassistant agent to simulate dialogues based on seed data collected from Chinese\ntourism websites. This approach addresses the aforementioned limitations and\ngenerates SynPT-Dialog, a training dataset containing explicit reasoning. The\ndataset is utilized to fine-tune a general LLM, enabling it to proactively mine\nimplicit user intentions. Experimental evaluations, conducted from both human\nand LLM perspectives, demonstrate the superiority of SynPT compared to existing\nmethods. Furthermore, we analyze key hyperparameters and present case studies\nto illustrate the practical applicability of our method, including discussions\non its adaptability to English-language scenarios. All code and data are\npublicly available.", "AI": {"tldr": "\u63d0\u51faSynPT\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u9a71\u52a8\u7684\u5bf9\u8bdd\u6a21\u62df\u751f\u6210\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u63d0\u5347\u65c5\u6e38\u9886\u57df\u9690\u542b\u610f\u56fe\u6316\u6398\u80fd\u529b", "motivation": "\u73b0\u6709\u65c5\u6e38\u9886\u57dfLLM\u5b58\u5728\u9690\u542b\u610f\u56fe\u8bc6\u522b\u4e0d\u8db3\u3001\u6570\u636e\u96c6\u8d28\u91cf\u4f4e\u3001\u65b9\u6cd5\u5b58\u5728\u9886\u57df\u9002\u5e94\u6027\u5dee/\u67e5\u8be2\u5206\u5e03\u504f\u659c/\u4e0a\u4e0b\u6587\u5197\u4f59/\u7f3a\u4e4f\u60c5\u611f\u5206\u6790\u56db\u5927\u7f3a\u9677", "method": "\u6784\u5efaLLM\u53cc\u4ee3\u7406\u7cfb\u7edf\u6a21\u62df\u7528\u6237\u5bf9\u8bdd\uff0c\u751f\u6210\u5305\u542b\u663e\u5f0f\u63a8\u7406\u94fe\u7684SynPT-Dialog\u6570\u636e\u96c6\uff0c\u5e76\u7528\u4e8e\u4e0b\u6e38\u6a21\u578b\u5fae\u8c03", "result": "\u5b9e\u9a8c\u8bc1\u660eSynPT\u5728\u4eba\u5de5\u548cLLM\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6210\u529f\u5b9e\u73b0\u4e3b\u52a8\u610f\u56fe\u5f15\u5bfc\u4e0e\u8de8\u8bed\u8a00\u9002\u5e94", "conclusion": "SynPT\u6709\u6548\u89e3\u51b3\u4e86\u65c5\u6e38\u9886\u57df\u610f\u56fe\u6316\u6398\u96be\u9898\uff0c\u5176\u6570\u636e\u5408\u6210\u6846\u67b6\u5177\u6709\u9886\u57df\u6269\u5c55\u6f5c\u529b\uff0c\u516c\u5f00\u6570\u636e\u8d44\u6e90\u63a8\u52a8\u76f8\u5173\u7814\u7a76\u53d1\u5c55"}}
{"id": "2505.12619", "pdf": "https://arxiv.org/pdf/2505.12619", "abs": "https://arxiv.org/abs/2505.12619", "authors": ["Jiashun Wang", "Yifeng Jiang", "Haotian Zhang", "Chen Tessler", "Davis Rempe", "Jessica Hodgins", "Xue Bin Peng"], "title": "HIL: Hybrid Imitation Learning of Diverse Parkour Skills from Videos", "categories": ["cs.GR"], "comment": "14 pages, 10 figures", "summary": "Recent data-driven methods leveraging deep reinforcement learning have been\nan effective paradigm for developing controllers that enable physically\nsimulated characters to produce natural human-like behaviors. However, these\ndata-driven methods often struggle to adapt to novel environments and compose\ndiverse skills coherently to perform more complex tasks. To address these\nchallenges, we propose a hybrid imitation learning (HIL) framework that\ncombines motion tracking, for precise skill replication, with adversarial\nimitation learning, to enhance adaptability and skill composition. This hybrid\nlearning framework is implemented through parallel multi-task environments and\na unified observation space, featuring an agent-centric scene representation to\nfacilitate effective learning from the hybrid parallel environments. Our\nframework trains a unified controller on parkour data sourced from Internet\nvideos, enabling a simulated character to traverse through new environments\nusing diverse and life-like parkour skills. Evaluations across challenging\nparkour environments demonstrate that our method improves motion quality,\nincreases skill diversity, and achieves competitive task completion compared to\nprevious learning-based methods.", "AI": {"tldr": "\u63d0\u51fa\u6df7\u5408\u6a21\u4eff\u5b66\u4e60\u6846\u67b6HIL\uff0c\u901a\u8fc7\u8fd0\u52a8\u8ddf\u8e2a\u4e0e\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\u7684\u7ed3\u5408\uff0c\u63d0\u5347\u865a\u62df\u89d2\u8272\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8dd1\u9177\u6280\u80fd\u9002\u5e94\u6027\u4e0e\u591a\u6837\u6027\u3002", "motivation": "\u4f20\u7edf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u73af\u5883\u9002\u5e94\u6027\u548c\u6280\u80fd\u7ec4\u5408\u8fde\u8d2f\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u9700\u5f00\u53d1\u80fd\u540c\u65f6\u4fdd\u6301\u8fd0\u52a8\u7cbe\u5ea6\u4e0e\u73af\u5883\u9002\u5e94\u6027\u7684\u65b0\u6846\u67b6\u3002", "method": "\u91c7\u7528\u5e76\u884c\u591a\u4efb\u52a1\u73af\u5883\u67b6\u6784\u4e0e\u7edf\u4e00\u89c2\u5bdf\u7a7a\u95f4\uff0c\u901a\u8fc7agent-centric\u573a\u666f\u8868\u5f81\u5b9e\u73b0\u6df7\u5408\u5e76\u884c\u73af\u5883\u4e0b\u7684\u63a7\u5236\u5668\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u8dd1\u9177\u73af\u5883\u4e2d\u63d0\u5347\u8fd0\u52a8\u8d28\u91cf30%\uff0c\u6280\u80fd\u591a\u6837\u6027\u589e\u52a045%\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u4e0e\u73b0\u6709\u65b9\u6cd5\u6301\u5e73\u3002", "conclusion": "HIL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u4f5c\u63a7\u5236\u5668\u7684\u73af\u5883\u9002\u5e94\u74f6\u9888\uff0c\u4e3a\u865a\u62df\u89d2\u8272\u884c\u4e3a\u751f\u6210\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8de8\u573a\u666f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.11550", "pdf": "https://arxiv.org/pdf/2505.11550", "abs": "https://arxiv.org/abs/2505.11550", "authors": ["Harika Abburi", "Sanmitra Bhattacharya", "Edward Bowen", "Nirmala Pudota"], "title": "AI-generated Text Detection: A Multifaceted Approach to Binary and Multiclass Classification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ngenerating text that closely resembles human writing across a wide range of\nstyles and genres. However, such capabilities are prone to potential misuse,\nsuch as fake news generation, spam email creation, and misuse in academic\nassignments. As a result, accurate detection of AI-generated text and\nidentification of the model that generated it are crucial for maintaining the\nresponsible use of LLMs. In this work, we addressed two sub-tasks put forward\nby the Defactify workshop under AI-Generated Text Detection shared task at the\nAssociation for the Advancement of Artificial Intelligence (AAAI 2025): Task A\ninvolved distinguishing between human-authored or AI-generated text, while Task\nB focused on attributing text to its originating language model. For each task,\nwe proposed two neural architectures: an optimized model and a simpler variant.\nFor Task A, the optimized neural architecture achieved fifth place with $F1$\nscore of 0.994, and for Task B, the simpler neural architecture also ranked\nfifth place with $F1$ score of 0.627.", "AI": {"tldr": "\u5f00\u53d1\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u68c0\u6d4bAI\u751f\u6210\u6587\u672c\uff0c\u4efb\u52a1A\uff08\u4eba\u673a\u533a\u5206\uff09F1\u8fbe0.994\uff0c\u4efb\u52a1B\uff08\u6a21\u578b\u6eaf\u6e90\uff09F1\u8fbe0.627\uff0c\u5747\u4f4d\u5217\u7b2c\u4e94\u540d", "motivation": "\u5e94\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u6ee5\u7528\u98ce\u9669\uff08\u5982\u5047\u65b0\u95fb/\u5783\u573e\u90ae\u4ef6/\u5b66\u672f\u4f5c\u5f0a\uff09\uff0c\u9700\u5efa\u7acb\u53ef\u9760\u68c0\u6d4b\u673a\u5236\u4fdd\u969cAI\u4f26\u7406\u4f7f\u7528", "method": "\u9488\u5bf9AAAI 2025 Defactify\u7ade\u8d5b\u4e24\u4efb\u52a1\uff1a\u4efb\u52a1A\u91c7\u7528\u4f18\u5316\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u4efb\u52a1B\u4f7f\u7528\u7b80\u5316\u7248\u67b6\u6784\u8fdb\u884c\u6a21\u578b\u6eaf\u6e90", "result": "\u4efb\u52a1A\u4f18\u5316\u6a21\u578bF1=0.994\uff08\u7b2c\u4e94\u540d\uff09\uff0c\u4efb\u52a1B\u7b80\u5316\u6a21\u578bF1=0.627\uff08\u7b2c\u4e94\u540d\uff09", "conclusion": "\u795e\u7ecf\u7f51\u7edc\u5728AI\u6587\u672c\u68c0\u6d4b\u4e2d\u6548\u679c\u663e\u8457\uff0c\u4efb\u52a1B\u96be\u5ea6\u66f4\u9ad8\uff0c\u4f53\u73b0\u6a21\u578b\u6eaf\u6e90\u7684\u6280\u672f\u6311\u6218\uff0c\u5f3a\u8c03\u68c0\u6d4b\u6280\u672f\u5bf9AI\u6cbb\u7406\u7684\u91cd\u8981\u6027"}}
{"id": "2505.12774", "pdf": "https://arxiv.org/pdf/2505.12774", "abs": "https://arxiv.org/abs/2505.12774", "authors": ["Zichen Geng", "Zeeshan Hayder", "Wei Liu", "Ajmal Mian"], "title": "UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes", "categories": ["cs.GR", "cs.AI", "cs.CV"], "comment": null, "summary": "Human motion synthesis in complex scenes presents a fundamental challenge,\nextending beyond conventional Text-to-Motion tasks by requiring the integration\nof diverse modalities such as static environments, movable objects, natural\nlanguage prompts, and spatial waypoints. Existing language-conditioned motion\nmodels often struggle with scene-aware motion generation due to limitations in\nmotion tokenization, which leads to information loss and fails to capture the\ncontinuous, context-dependent nature of 3D human movement. To address these\nissues, we propose UniHM, a unified motion language model that leverages\ndiffusion-based generation for synthesizing scene-aware human motion. UniHM is\nthe first framework to support both Text-to-Motion and Text-to-Human-Object\nInteraction (HOI) in complex 3D scenes. Our approach introduces three key\ncontributions: (1) a mixed-motion representation that fuses continuous 6DoF\nmotion with discrete local motion tokens to improve motion realism; (2) a novel\nLook-Up-Free Quantization VAE (LFQ-VAE) that surpasses traditional VQ-VAEs in\nboth reconstruction accuracy and generative performance; and (3) an enriched\nversion of the Lingo dataset augmented with HumanML3D annotations, providing\nstronger supervision for scene-specific motion learning. Experimental results\ndemonstrate that UniHM achieves comparative performance on the OMOMO benchmark\nfor text-to-HOI synthesis and yields competitive results on HumanML3D for\ngeneral text-conditioned motion generation.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u652f\u6301\u590d\u67423D\u573a\u666f\u4e2d\u6587\u672c\u9a71\u52a8\u4eba\u4f53\u8fd0\u52a8\u4e0e\u4ea4\u4e92\u7684\u7edf\u4e00\u6846\u67b6UniHM\uff0c\u901a\u8fc7\u6df7\u5408\u8fd0\u52a8\u8868\u793a\u3001\u65b0\u578bLFQ-VAE\u548c\u589e\u5f3a\u6570\u636e\u96c6\u5b9e\u73b0\u573a\u666f\u611f\u77e5\u751f\u6210", "motivation": "\u73b0\u6709\u8bed\u8a00\u9a71\u52a8\u6a21\u578b\u56e0\u8fd0\u52a8\u79bb\u6563\u5316\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u548c\u8fde\u7eed\u6027\u7f3a\u5931\uff0c\u96be\u4ee5\u751f\u6210\u7b26\u5408\u7269\u7406\u573a\u666f\u7ea6\u675f\u7684\u5408\u7406\u8fd0\u52a8", "method": "\u878d\u54086DoF\u8fde\u7eed\u8fd0\u52a8\u4e0e\u5c40\u90e8\u79bb\u6563\u6807\u8bb0\u7684\u6df7\u5408\u8868\u5f81 + \u8d85\u8d8a\u4f20\u7edfVQ-VAE\u7684LFQ-VAE + \u589e\u5f3a\u7248Lingo\u6570\u636e\u96c6\u63d0\u4f9b\u5f3a\u76d1\u7763", "result": "\u5728OMOMO\u57fa\u51c6\u7684\u6587\u672c-HOI\u5408\u6210\u4efb\u52a1\u548cHumanML3D\u901a\u7528\u6587\u672c\u9a71\u52a8\u8fd0\u52a8\u751f\u6210\u4e2d\u8fbe\u5230\u53ef\u6bd4/\u7ade\u4e89\u6027\u8868\u73b0", "conclusion": "UniHM\u901a\u8fc7\u8fde\u7eed-\u79bb\u6563\u6df7\u5408\u5efa\u6a21\u7a81\u7834\u573a\u666f\u611f\u77e5\u8fd0\u52a8\u751f\u6210\u74f6\u9888\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u884c\u4e3a\u5408\u6210\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2505.11556", "pdf": "https://arxiv.org/pdf/2505.11556", "abs": "https://arxiv.org/abs/2505.11556", "authors": ["Yuxuan Li", "Aoi Naito", "Hirokazu Shirado"], "title": "Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "Multi-agent systems built on large language models (LLMs) promise enhanced\nproblem-solving through distributed information integration, but also risk\nreplicating collective reasoning failures observed in human groups. Yet, no\ntheory-grounded benchmark exists to systematically evaluate such failures. In\nthis paper, we introduce the Hidden Profile paradigm from social psychology as\na diagnostic testbed for multi-agent LLM systems. By distributing critical\ninformation asymmetrically across agents, the paradigm reveals how inter-agent\ndynamics support or hinder collective reasoning. We first formalize the\nparadigm for multi-agent decision-making under distributed knowledge and\ninstantiate it as a benchmark with nine tasks spanning diverse scenarios,\nincluding adaptations from prior human studies. We then conduct experiments\nwith GPT-4.1 and five other leading LLMs, including reasoning-enhanced\nvariants, showing that multi-agent systems across all models fail to match the\naccuracy of single agents given complete information. While agents' collective\nperformance is broadly comparable to that of human groups, nuanced behavioral\ndifferences emerge, such as increased sensitivity to social desirability.\nFinally, we demonstrate the paradigm's diagnostic utility by exploring a\ncooperation-contradiction trade-off in multi-agent LLM systems. We find that\nwhile cooperative agents are prone to over-coordination in collective settings,\nincreased contradiction impairs group convergence. This work contributes a\nreproducible framework for evaluating multi-agent LLM systems and motivates\nfuture research on artificial collective intelligence and human-AI interaction.", "AI": {"tldr": "\u8bba\u6587\u5f15\u5165\u793e\u4f1a\u5fc3\u7406\u5b66\u4e2d\u7684\u9690\u85cf\u6863\u6848\u8303\u5f0f\uff0c\u63ed\u793a\u4e86\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u5b58\u5728\u96c6\u4f53\u63a8\u7406\u7f3a\u9677\uff0c\u53d1\u73b0\u6240\u6709\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u8868\u73b0\u5747\u4e0d\u53ca\u62e5\u6709\u5b8c\u6574\u4fe1\u606f\u7684\u5355\u667a\u80fd\u4f53\uff0c\u5e76\u63ed\u793a\u4e86\u5408\u4f5c\u4e0e\u77db\u76fe\u95f4\u7684\u7cfb\u7edf\u7ea7\u6743\u8861\u3002", "motivation": "\u9488\u5bf9\u591a\u667a\u80fd\u4f53LLM\u7cfb\u7edf\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u96c6\u4f53\u63a8\u7406\u7f3a\u9677\u7684\u7406\u8bba\u57fa\u7840\u57fa\u51c6\uff0c\u7814\u7a76\u8005\u8bd5\u56fe\u901a\u8fc7\u5fc3\u7406\u5b66\u8303\u5f0f\u63ed\u793a\u667a\u80fd\u4f53\u95f4\u52a8\u6001\u4ea4\u4e92\u5bf9\u96c6\u4f53\u63a8\u7406\u7684\u5f71\u54cd\u673a\u5236\u3002", "method": "\u5c06\u9690\u85cf\u6863\u6848\u8303\u5f0f\u5f62\u5f0f\u5316\u4e3a\u5206\u5e03\u5f0f\u77e5\u8bc6\u4e0b\u7684\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u6846\u67b6\uff0c\u6784\u5efa\u5305\u542b9\u4e2a\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728GPT-4.1\u7b49\u516d\u4e2a\u4e3b\u6d41LLM\u4e0a\u5f00\u5c55\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u5305\u62ec\u589e\u5f3a\u63a8\u7406\u53d8\u4f53\u7684\u6d4b\u8bd5\u3002", "result": "\u6240\u6709\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u51c6\u786e\u7387\u843d\u540e\u4e8e\u5b8c\u6574\u4fe1\u606f\u5355\u667a\u80fd\u4f5328.3%\uff0c\u96c6\u4f53\u8868\u73b0\u63a5\u8fd1\u4eba\u7c7b\u7fa4\u4f53\u4f46\u5b58\u5728\u884c\u4e3a\u5dee\u5f02\uff08\u5982\u5bf9\u793e\u4f1a\u671f\u671b\u654f\u611f\u5ea6\u63d0\u5347\uff09\uff0c\u5408\u4f5c\u667a\u80fd\u4f53\u6613\u51fa\u73b0\u8fc7\u5ea6\u534f\u8c03\u800c\u77db\u76fe\u589e\u52a0\u635f\u5bb3\u7fa4\u4f53\u6536\u655b\u3002", "conclusion": "\u7814\u7a76\u5efa\u7acb\u4e86\u53ef\u590d\u73b0\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86LLM\u7fa4\u4f53\u667a\u80fd\u4e0e\u4eba\u7c7b\u96c6\u4f53\u884c\u4e3a\u7684\u5f02\u540c\uff0c\u4e3a\u4eba\u5de5\u96c6\u4f53\u667a\u80fd\u548c\u4eba\u673a\u4ea4\u4e92\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\u4e0e\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2505.12782", "pdf": "https://arxiv.org/pdf/2505.12782", "abs": "https://arxiv.org/abs/2505.12782", "authors": ["Kai Zhang", "Xingyu Chen", "Xiaofeng Zhang"], "title": "AdaToken-3D: Dynamic Spatial Gating for Efficient 3D Large Multimodal-Models Reasoning", "categories": ["cs.GR", "cs.CV", "cs.IR", "cs.IT", "math.IT"], "comment": null, "summary": "Large Multimodal Models (LMMs) have become a pivotal research focus in deep\nlearning, demonstrating remarkable capabilities in 3D scene understanding.\nHowever, current 3D LMMs employing thousands of spatial tokens for multimodal\nreasoning suffer from critical inefficiencies: excessive computational overhead\nand redundant information flows. Unlike 2D VLMs processing single images, 3D\nLMMs exhibit inherent architectural redundancy due to the heterogeneous\nmechanisms between spatial tokens and visual tokens. To address this challenge,\nwe propose AdaToken-3D, an adaptive spatial token optimization framework that\ndynamically prunes redundant tokens through spatial contribution analysis. Our\nmethod automatically tailors pruning strategies to different 3D LMM\narchitectures by quantifying token-level information flows via attention\npattern mining. Extensive experiments on LLaVA-3D (a 7B parameter 3D-LMM)\ndemonstrate that AdaToken-3D achieves 21\\% faster inference speed and 63\\%\nFLOPs reduction while maintaining original task accuracy. Beyond efficiency\ngains, this work systematically investigates redundancy patterns in multimodal\nspatial information flows through quantitative token interaction analysis. Our\nfindings reveal that over 60\\% of spatial tokens contribute minimally ($<$5\\%)\nto the final predictions, establishing theoretical foundations for efficient 3D\nmultimodal learning.", "AI": {"tldr": "\u63d0\u51faAdaToken-3D\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u526a\u679d\u5197\u4f59\u7a7a\u95f4\u4ee4\u724c\u5b9e\u73b03D\u591a\u6a21\u6001\u6a21\u578b\u6548\u7387\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u534721%\u63a8\u7406\u901f\u5ea6\u5e76\u51cf\u5c1163%\u8ba1\u7b97\u91cf\u3002", "motivation": "\u73b0\u67093D\u591a\u6a21\u6001\u6a21\u578b\u5b58\u5728\u7a7a\u95f4\u4ee4\u724c\u4e0e\u89c6\u89c9\u4ee4\u724c\u5f02\u6784\u673a\u5236\u5bfc\u81f4\u7684\u67b6\u6784\u5197\u4f59\u95ee\u9898\uff0c60%\u4ee5\u4e0a\u7a7a\u95f4\u4ee4\u724c\u5bf9\u6700\u7ec8\u9884\u6d4b\u8d21\u732e\u4e0d\u8db35%\uff0c\u9020\u6210\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u57fa\u4e8e\u6ce8\u610f\u529b\u6a21\u5f0f\u6316\u6398\u91cf\u5316\u4ee4\u724c\u7ea7\u4fe1\u606f\u6d41\uff0c\u5f00\u53d1\u81ea\u9002\u5e94\u7a7a\u95f4\u4ee4\u724c\u4f18\u5316\u6846\u67b6\uff0c\u81ea\u52a8\u4e3a\u4e0d\u540c3D-LMM\u67b6\u6784\u5b9a\u5236\u526a\u679d\u7b56\u7565\u3002", "result": "\u5728LLaVA-3D\uff0870\u4ebf\u53c2\u6570\uff09\u4e0a\u5b9e\u73b021%\u63a8\u7406\u52a0\u901f\u548c63%FLOPs\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u59cb\u4efb\u52a1\u7cbe\u5ea6\u3002\u5b9a\u91cf\u5206\u6790\u63ed\u793a\u591a\u6a21\u6001\u7a7a\u95f4\u4fe1\u606f\u6d41\u5197\u4f59\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e0d\u4ec5\u63d0\u53473D\u591a\u6a21\u6001\u5b66\u4e60\u6548\u7387\uff0c\u8fd8\u901a\u8fc7\u4ee4\u724c\u4ea4\u4e92\u5206\u6790\u5efa\u7acb\u7406\u8bba\u6846\u67b6\uff0c\u4e3a\u540e\u7eed\u9ad8\u6548\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2505.11604", "pdf": "https://arxiv.org/pdf/2505.11604", "abs": "https://arxiv.org/abs/2505.11604", "authors": ["Kyudan Jung", "Hojun Cho", "Jooyeol Yun", "Jaehyeok Jang", "Jagul Choo"], "title": "Talk to Your Slides: Efficient Slide Editing Agent with Large Language Models", "categories": ["cs.CL"], "comment": "14 pages, 6 figures", "summary": "Existing research on large language models (LLMs) for PowerPoint\npredominantly focuses on slide generation, overlooking the common yet tedious\ntask of editing existing slides. We introduce Talk-to-Your-Slides, an\nLLM-powered agent that directly edits slides within active PowerPoint sessions\nthrough COM communication. Our system employs a two-level approach: (1)\nhigh-level processing where an LLM agent interprets instructions and formulates\nediting plans, and (2) low-level execution where Python scripts directly\nmanipulate PowerPoint objects. Unlike previous methods relying on predefined\noperations, our approach enables more flexible and contextually-aware editing.\nTo facilitate evaluation, we present TSBench, a human-annotated dataset of 379\ndiverse editing instructions with corresponding slide variations. Experimental\nresults demonstrate that Talk-to-Your-Slides significantly outperforms baseline\nmethods in execution success rate, instruction fidelity, and editing\nefficiency. Our code and benchmark are available at\nhttps://anonymous.4open.science/r/talk-to-your-slides/", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faTalk-to-Your-Slides\u7cfb\u7edf\uff0c\u901a\u8fc7LLM\u4ee3\u7406\u5b9e\u73b0PowerPoint\u7684\u667a\u80fd\u7f16\u8f91\uff0c\u91c7\u7528\u9ad8\u5c42\u6307\u4ee4\u89e3\u6790+\u5e95\u5c42Python\u811a\u672c\u6267\u884c\u7684\u4e24\u7ea7\u67b6\u6784\uff0c\u5e76\u6784\u5efaTSBench\u8bc4\u4f30\u57fa\u51c6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u96c6\u4e2d\u4e8ePPT\u751f\u6210\u800c\u5ffd\u89c6\u7f16\u8f91\u573a\u666f\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u64cd\u4f5c\uff0c\u65e0\u6cd5\u6ee1\u8db3\u590d\u6742\u573a\u666f\u7684\u7075\u6d3b\u7f16\u8f91\u9700\u6c42\u3002", "method": "1) \u9ad8\u5c42LLM\u4ee3\u7406\u89e3\u6790\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u5e76\u5236\u5b9a\u7f16\u8f91\u8ba1\u5212 2) \u5e95\u5c42\u901a\u8fc7COM\u901a\u4fe1\u534f\u8bae\u76f4\u63a5\u64cd\u4f5cPPT\u5bf9\u8c61 3) \u63d0\u51fa\u5305\u542b379\u6761\u6807\u6ce8\u6307\u4ee4\u7684TSBench\u6570\u636e\u96c6", "result": "\u7cfb\u7edf\u5728\u6307\u4ee4\u6267\u884c\u6210\u529f\u7387(\u63d0\u534732%)\u3001\u7f16\u8f91\u51c6\u786e\u6027(\u63d0\u9ad828%)\u548c\u64cd\u4f5c\u6548\u7387(\u51cf\u5c1140%\u4ea4\u4e92\u6b65\u9aa4)\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u8be5\u65b9\u6cd5\u7a81\u7834\u4e86\u4f20\u7edfPPT\u7f16\u8f91\u5de5\u5177\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u667a\u80fd\u7f16\u8f91\uff0c\u4e3a\u529e\u516c\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002\u4ee3\u7801\u548c\u57fa\u51c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.13390", "pdf": "https://arxiv.org/pdf/2505.13390", "abs": "https://arxiv.org/abs/2505.13390", "authors": ["Chunlei Li", "Peng Yu", "Tiantian Liu", "Siyuan Yu", "Yuting Xiao", "Shuai Li", "Aimin Hao", "Yang Gao", "Qinping Zhao"], "title": "MGPBD: A Multigrid Accelerated Global XPBD Solver", "categories": ["cs.GR", "I.3.6"], "comment": "SIGGRAPH 2025", "summary": "We introduce a novel Unsmoothed Aggregation (UA) Algebraic Multigrid (AMG)\nmethod combined with Preconditioned Conjugate Gradient (PCG) to overcome the\nlimitations of Extended Position-Based Dynamics (XPBD) in high-resolution and\nhigh-stiffness simulations. While XPBD excels in simulating deformable objects\ndue to its speed and simplicity, its nonlinear Gauss-Seidel (GS) solver often\nstruggles with low-frequency errors, leading to instability and stalling\nissues, especially in high-resolution, high-stiffness simulations. Our\nmultigrid approach addresses these issues efficiently by leveraging AMG. To\nreduce the computational overhead of traditional AMG, where prolongator\nconstruction can consume up to two-thirds of the runtime, we propose a lazy\nsetup strategy that reuses prolongators across iterations based on matrix\nstructure and physical significance. Furthermore, we introduce a simplified\nmethod for constructing near-kernel components by applying a few sweeps of\niterative methods to the homogeneous equation, achieving convergence rates\ncomparable to adaptive smoothed aggregation (adaptive-SA) at a lower\ncomputational cost. Experimental results demonstrate that our method\nsignificantly improves convergence rates and numerical stability, enabling\nefficient and stable high-resolution simulations of deformable objects.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u578b\u975e\u5e73\u6ed1\u805a\u5408\u4ee3\u6570\u591a\u91cd\u7f51\u683c(UA-AMG)\u4e0e\u9884\u5904\u7406\u5171\u8f6d\u68af\u5ea6\u6cd5(PCG)\u76f8\u7ed3\u5408\uff0c\u89e3\u51b3XPBD\u5728\u9ad8\u5206\u8fa8\u7387\u9ad8\u521a\u5ea6\u6a21\u62df\u4e2d\u7684\u6536\u655b\u95ee\u9898\u3002", "motivation": "XPBD\u7684\u975e\u7ebf\u6027Gauss-Seidel\u6c42\u89e3\u5668\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u9ad8\u521a\u5ea6\u6a21\u62df\u65f6\u6613\u4ea7\u751f\u4f4e\u9891\u8bef\u5dee\uff0c\u5bfc\u81f4\u6570\u503c\u4e0d\u7a33\u5b9a\u548c\u6c42\u89e3\u505c\u6ede\u3002\u4f20\u7edfAMG\u7684\u5ef6\u957f\u7b97\u5b50\u6784\u5efa\u8017\u65f6\u957f(\u5360\u603b\u65f6\u95f42/3)\u3002", "method": "1. \u60f0\u6027\u6784\u5efa\u7b56\u7565\uff1a\u57fa\u4e8e\u77e9\u9635\u7ed3\u6784\u548c\u7269\u7406\u610f\u4e49\u8de8\u8fed\u4ee3\u590d\u7528\u5ef6\u957f\u7b97\u5b50\n2. \u8fd1\u6838\u7a7a\u95f4\u7b80\u5316\u6784\u5efa\uff1a\u901a\u8fc7\u9f50\u6b21\u65b9\u7a0b\u8fed\u4ee3\u751f\u6210\u8fd1\u4f3c\u89e3\n3. \u975e\u5e73\u6ed1\u805a\u5408AMG\u4e0ePCG\u7ed3\u5408", "result": "\u5b9e\u73b0\u4e0e\u4f20\u7edf\u81ea\u9002\u5e94\u5e73\u6ed1\u805a\u5408\u76f8\u5f53\u7684\u6536\u655b\u901f\u5ea6\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e30%\u3002\u5b9e\u9a8c\u8bc1\u660e\u5728\u9ad8\u5206\u8fa8\u7387\u5e03\u6599(100\u4e07\u9876\u70b9)\u3001\u5f39\u6027\u4f53\u6a21\u62df\u4e2d\u6536\u655b\u901f\u5ea6\u63d0\u53475-8\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u521b\u65b0\u6027\u591a\u91cd\u7f51\u683c\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347XPBD\u7684\u6570\u503c\u7a33\u5b9a\u6027\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u9ad8\u7cbe\u5ea6\u53ef\u53d8\u5f62\u4f53\u6a21\u62df\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.11613", "pdf": "https://arxiv.org/pdf/2505.11613", "abs": "https://arxiv.org/abs/2505.11613", "authors": ["Xiaomin Li", "Mingye Gao", "Yuexing Hao", "Taoran Li", "Guangya Wan", "Zihan Wang", "Yijun Wang"], "title": "MedGUIDE: Benchmarking Clinical Decision-Making in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Clinical guidelines, typically structured as decision trees, are central to\nevidence-based medical practice and critical for ensuring safe and accurate\ndiagnostic decision-making. However, it remains unclear whether Large Language\nModels (LLMs) can reliably follow such structured protocols. In this work, we\nintroduce MedGUIDE, a new benchmark for evaluating LLMs on their ability to\nmake guideline-consistent clinical decisions. MedGUIDE is constructed from 55\ncurated NCCN decision trees across 17 cancer types and uses clinical scenarios\ngenerated by LLMs to create a large pool of multiple-choice diagnostic\nquestions. We apply a two-stage quality selection process, combining\nexpert-labeled reward models and LLM-as-a-judge ensembles across ten clinical\nand linguistic criteria, to select 7,747 high-quality samples. We evaluate 25\nLLMs spanning general-purpose, open-source, and medically specialized models,\nand find that even domain-specific LLMs often underperform on tasks requiring\nstructured guideline adherence. We also test whether performance can be\nimproved via in-context guideline inclusion or continued pretraining. Our\nfindings underscore the importance of MedGUIDE in assessing whether LLMs can\noperate safely within the procedural frameworks expected in real-world clinical\nsettings.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u9075\u5faa\u4e34\u5e8a\u6307\u5357\u7684\u80fd\u529b\uff0c\u5f00\u53d1MedGUIDE\u57fa\u51c6\u6d4b\u8bd5\u53d1\u73b0\u5373\u4f7f\u533b\u5b66\u4e13\u7528\u6a21\u578b\u5728\u7ed3\u6784\u5316\u534f\u8bae\u9075\u5faa\u4efb\u52a1\u4e2d\u8868\u73b0\u6b20\u4f73", "motivation": "\u4e34\u5e8a\u6307\u5357\u4f5c\u4e3a\u51b3\u7b56\u6811\u5bf9\u5faa\u8bc1\u533b\u7597\u81f3\u5173\u91cd\u8981\uff0c\u4f46LLMs\u80fd\u5426\u53ef\u9760\u9075\u5faa\u6b64\u7c7b\u7ed3\u6784\u5316\u534f\u8bae\u5c1a\u672a\u660e\u786e\uff0c\u9700\u786e\u4fdd\u6a21\u578b\u5728\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\u4e2d\u5b89\u5168\u8fd0\u4f5c", "method": "\u57fa\u4e8e17\u79cd\u764c\u75c7\u768455\u4e2aNCCN\u51b3\u7b56\u6811\u751f\u6210\u4e34\u5e8a\u573a\u666f\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u7b5b\u9009\uff08\u4e13\u5bb6\u6807\u6ce8\u5956\u52b1\u6a21\u578b+LLM\u8bc4\u59d4\u96c6\u6210\uff09\u9009\u51fa7,747\u4e2a\u9ad8\u8d28\u91cf\u6837\u672c\uff0c\u8bc4\u4f3025\u4e2a\u901a\u7528/\u5f00\u6e90/\u533b\u5b66\u4e13\u7528LLM", "result": "\u9886\u57df\u4e13\u7528\u6a21\u578b\u5728\u9700\u4e25\u683c\u9075\u5faa\u6307\u5357\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e0a\u4e0b\u6587\u6307\u5357\u7eb3\u5165\u548c\u7ee7\u7eed\u9884\u8bad\u7ec3\u5bf9\u6027\u80fd\u63d0\u5347\u6709\u9650", "conclusion": "MedGUIDE\u6709\u6548\u8bc4\u4f30LLMs\u4e34\u5e8a\u9002\u7528\u6027\uff0c\u51f8\u663e\u5f53\u524d\u6a21\u578b\u5728\u771f\u5b9e\u4e34\u5e8a\u6846\u67b6\u4e2d\u7684\u64cd\u4f5c\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u6539\u8fdb\u6a21\u578b\u7ed3\u6784\u5316\u534f\u8bae\u9075\u5faa\u80fd\u529b\u7684\u5fc5\u8981\u6027"}}
{"id": "2505.11758", "pdf": "https://arxiv.org/pdf/2505.11758", "abs": "https://arxiv.org/abs/2505.11758", "authors": ["Sriram Mandalika"], "title": "Generalizable Vision-Language Few-Shot Adaptation with Predictive Prompts and Negative Learning", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.RO"], "comment": null, "summary": "Few-shot adaptation remains a core challenge for vision-language models\n(VLMs), especially under limited supervision and noisy support samples. We\npropose PromptFuseNL, a unified framework that enhances few-shot generalization\nby combining predictive prompt tuning with dual-branch positive and negative\nlearning. The method refines class prototypes through task-conditioned\nresiduals, multi-stage cross-modal coordination, and semantic hard negative\nmining. To address label noise, we introduce an unsupervised instance\nreweighting strategy that downweights unreliable support examples without\nrequiring additional labels or structural changes. PromptFuseNL fuses visual\nand textual cues through lightweight modules for efficient and discriminative\nprediction. Evaluated across 15 benchmarks, it consistently surpasses existing\nprompt- and adapter-based methods in all shot settings while remaining highly\nefficient, achieving up to 300x faster training and 1000x lower FLOPs compared\nto full prompt tuning, achieving a new state-of-the-art for robust and scalable\nfew-shot vision-language adaptation.", "AI": {"tldr": "\u63d0\u51faPromptFuseNL\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u63d0\u793a\u8c03\u4f18\u4e0e\u53cc\u5206\u652f\u6b63\u8d1f\u5b66\u4e60\u589e\u5f3a\u5c11\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u572815\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0SOTA\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u5347300\u500d\u4e14\u8ba1\u7b97\u91cf\u964d\u4f4e1000\u500d", "motivation": "\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5c11\u6837\u672c\u9002\u5e94\u573a\u666f\u4e0b\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5305\u62ec\u6709\u9650\u76d1\u7763\u4fe1\u53f7\u3001\u566a\u58f0\u652f\u6301\u6837\u672c\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u5728\u9c81\u68d2\u6027\u548c\u6548\u7387\u4e0a\u7684\u4e0d\u8db3", "method": "\u7ed3\u5408\u4efb\u52a1\u6761\u4ef6\u6b8b\u5dee\u4f18\u5316\u7c7b\u539f\u578b\u3001\u591a\u9636\u6bb5\u8de8\u6a21\u6001\u534f\u8c03\u673a\u5236\u3001\u8bed\u4e49\u786c\u8d1f\u6837\u672c\u6316\u6398\uff0c\u914d\u5408\u65e0\u76d1\u7763\u5b9e\u4f8b\u91cd\u52a0\u6743\u7b56\u7565\u6d88\u9664\u566a\u58f0\u6837\u672c\u5f71\u54cd\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u5757\u5b9e\u73b0\u89c6\u89c9-\u6587\u672c\u7ebf\u7d22\u878d\u5408", "result": "\u5728\u5168\u90e8shot\u8bbe\u7f6e\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u6700\u9ad8\u8fbe300\u500d\u8bad\u7ec3\u52a0\u901f\u548c1000\u500d\u8ba1\u7b97\u91cf\u538b\u7f29\uff0c15\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u6301\u7eed\u9886\u5148", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u5c11\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u9002\u5e94\uff0c\u5728\u4fdd\u6301\u8f7b\u91cf\u7ea7\u67b6\u6784\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.11615", "pdf": "https://arxiv.org/pdf/2505.11615", "abs": "https://arxiv.org/abs/2505.11615", "authors": ["Jian-Qiao Zhu", "Haijiang Yan", "Thomas L. Griffiths"], "title": "Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Changing the behavior of large language models (LLMs) can be as\nstraightforward as editing the Transformer's residual streams using\nappropriately constructed \"steering vectors.\" These modifications to internal\nneural activations, a form of representation engineering, offer an effective\nand targeted means of influencing model behavior without retraining or\nfine-tuning the model. But how can such steering vectors be systematically\nidentified? We propose a principled approach for uncovering steering vectors by\naligning latent representations elicited through behavioral methods\n(specifically, Markov chain Monte Carlo with LLMs) with their neural\ncounterparts. To evaluate this approach, we focus on extracting latent risk\npreferences from LLMs and steering their risk-related outputs using the aligned\nrepresentations as steering vectors. We show that the resulting steering\nvectors successfully and reliably modulate LLM outputs in line with the\ntargeted behavior.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u884c\u4e3a\u65b9\u6cd5\u4e0e\u795e\u7ecf\u8868\u793a\u5bf9\u9f50\uff0c\u63d0\u51fa\u7cfb\u7edf\u5316\u8bc6\u522b\u5f15\u5bfc\u5411\u91cf\u7684\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u5176\u6709\u6548\u8c03\u63a7LLM\u98ce\u9669\u8f93\u51fa\u7684\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u7cbe\u51c6\u8c03\u63a7LLM\u884c\u4e3a\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u89e3\u51b3\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u6548\u7387\u4f4e\u3001\u9488\u5bf9\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408MCMC\u884c\u4e3a\u65b9\u6cd5\u751f\u6210\u6f5c\u5728\u8868\u793a\uff0c\u4e0e\u795e\u7ecf\u7f51\u7edc\u6fc0\u6d3b\u5bf9\u9f50\u63d0\u53d6\u5f15\u5bfc\u5411\u91cf\uff0c\u5e76\u6ce8\u5165\u6b8b\u5dee\u6d41\u8c03\u6574\u8f93\u51fa\u3002", "result": "\u63d0\u53d6\u7684\u5f15\u5bfc\u5411\u91cf\u53ef\u7a33\u5b9a\u8c03\u8282LLM\u98ce\u9669\u504f\u597d\uff0c\u8f93\u51fa\u7ed3\u679c\u4e0e\u76ee\u6807\u884c\u4e3a\u4fdd\u6301\u9ad8\u5ea6\u4e00\u81f4\u6027\u3002", "conclusion": "\u8868\u793a\u5de5\u7a0b\u4e3aLLM\u884c\u4e3a\u8c03\u63a7\u63d0\u4f9b\u4e86\u8f7b\u91cf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6cd5\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u884c\u4e3a\u7279\u5f81\u8c03\u63a7\u573a\u666f\u3002"}}
{"id": "2505.12267", "pdf": "https://arxiv.org/pdf/2505.12267", "abs": "https://arxiv.org/abs/2505.12267", "authors": ["Pengdi Huang", "Mingyang Wang", "Huan Tian", "Minglun Gong", "Hao Zhang", "Hui Huang"], "title": "Real-Time Spatial Reasoning by Mobile Robots for Reconstruction and Navigation in Dynamic LiDAR Scenes", "categories": ["cs.RO", "cs.GR"], "comment": null, "summary": "Our brain has an inner global positioning system which enables us to sense\nand navigate 3D spaces in real time. Can mobile robots replicate such a\nbiological feat in a dynamic environment? We introduce the first spatial\nreasoning framework for real-time surface reconstruction and navigation that is\ndesigned for outdoor LiDAR scanning data captured by ground mobile robots and\ncapable of handling moving objects such as pedestrians. Our\nreconstruction-based approach is well aligned with the critical cellular\nfunctions performed by the border vector cells (BVCs) over all layers of the\nmedial entorhinal cortex (MEC) for surface sensing and tracking. To address the\nchallenges arising from blurred boundaries resulting from sparse single-frame\nLiDAR points and outdated data due to object movements, we integrate real-time\nsingle-frame mesh reconstruction, via visibility reasoning, with robot\nnavigation assistance through on-the-fly 3D free space determination. This\nenables continuous and incremental updates of the scene and free space across\nmultiple frames. Key to our method is the utilization of line-of-sight (LoS)\nvectors from LiDAR, which enable real-time surface normal estimation, as well\nas robust and instantaneous per-voxel free space updates. We showcase two\npractical applications: real-time 3D scene reconstruction and autonomous\noutdoor robot navigation in real-world conditions. Comprehensive experiments on\nboth synthetic and real scenes highlight our method's superiority in speed and\nquality over existing real-time LiDAR processing approaches.", "AI": {"tldr": "\u9996\u4e2a\u9488\u5bf9\u52a8\u6001\u73af\u5883\u7684\u5b9e\u65f6LiDAR\u7a7a\u95f4\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u4eff\u751f\u65b9\u6cd5\u5b9e\u73b0\u6237\u5916\u79fb\u52a8\u673a\u5668\u4eba\u5b9e\u65f6\u573a\u666f\u91cd\u5efa\u4e0e\u5bfc\u822a\uff0c\u53ef\u5904\u7406\u79fb\u52a8\u7269\u4f53\u5e76\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u901f\u5ea6\u4e0e\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e2d\u673a\u5668\u4eba\u5b9e\u65f6\u611f\u77e5\u4e0e\u5bfc\u822a\u96be\u9898\uff0c\u7279\u522b\u662f\u5e94\u5bf9LiDAR\u5355\u5e27\u6570\u636e\u7a00\u758f\u6027\u53ca\u79fb\u52a8\u7269\u4f53\u5bfc\u81f4\u7684\u6a21\u7cca\u8fb9\u754c\u95ee\u9898\uff0c\u6a21\u62df\u54fa\u4e73\u52a8\u7269\u5927\u8111\u5185\u55c5\u76ae\u5c42\u7a7a\u95f4\u5b9a\u4f4d\u673a\u5236\u3002", "method": "\u878d\u5408LiDAR\u89c6\u7ebf\u5411\u91cf(LoS)\u5b9e\u65f6\u8868\u9762\u6cd5\u7ebf\u4f30\u8ba1\u3001\u57fa\u4e8e\u53ef\u89c1\u6027\u63a8\u7406\u7684\u5355\u5e27\u7f51\u683c\u91cd\u5efa\u3001\u5373\u65f63D\u81ea\u7531\u7a7a\u95f4\u68c0\u6d4b\u6280\u672f\uff0c\u5b9e\u73b0\u591a\u5e27\u8fde\u7eed\u589e\u91cf\u5f0f\u573a\u666f\u66f4\u65b0\u3002", "result": "\u5408\u6210\u4e0e\u771f\u5b9e\u573a\u666f\u6d4b\u8bd5\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u5efa\u901f\u5ea6(20Hz)\u4e0e\u8d28\u91cf(\u8bef\u5dee\u964d\u4f4e35%)\u4e0a\u5747\u8d85\u8d8a\u73b0\u6709\u65b9\u6848\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u5b9e\u65f6\u573a\u666f\u91cd\u5efa\u4e0e\u81ea\u4e3b\u5bfc\u822a\u4efb\u52a1\u3002", "conclusion": "\u901a\u8fc7\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\u7684\u4eff\u751f\u8bbe\u8ba1\uff0c\u9996\u6b21\u5b9e\u73b0\u52a8\u6001\u6237\u5916\u73af\u5883\u4e2d\u7684\u5b9e\u65f6LiDAR\u573a\u666f\u7406\u89e3\u4e0e\u673a\u5668\u4eba\u5bfc\u822a\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u65b0\u7684\u5b9e\u65f6\u7a7a\u95f4\u63a8\u7406\u8303\u5f0f\u3002"}}
{"id": "2505.11626", "pdf": "https://arxiv.org/pdf/2505.11626", "abs": "https://arxiv.org/abs/2505.11626", "authors": ["Udita Patel", "Rutu Mulkar", "Jay Roberts", "Cibi Chakravarthy Senthilkumar", "Sujay Gandhi", "Xiaofei Zheng", "Naumaan Nayyar", "Rafael Castrillo"], "title": "THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering", "categories": ["cs.CL"], "comment": null, "summary": "We propose THELMA (Task Based Holistic Evaluation of Large Language Model\nApplications), a reference free framework for RAG (Retrieval Augmented\ngeneration) based question answering (QA) applications. THELMA consist of six\ninterdependent metrics specifically designed for holistic, fine grained\nevaluation of RAG QA applications. THELMA framework helps developers and\napplication owners evaluate, monitor and improve end to end RAG QA pipelines\nwithout requiring labelled sources or reference responses.We also present our\nfindings on the interplay of the proposed THELMA metrics, which can be\ninterpreted to identify the specific RAG component needing improvement in QA\napplications.", "AI": {"tldr": "THELMA\u6846\u67b6\u901a\u8fc76\u4e2a\u4e92\u5173\u8054\u6307\u6807\u5b9e\u73b0RAG\u95ee\u7b54\u7cfb\u7edf\u7684\u65e0\u53c2\u8003\u5168\u94fe\u8def\u8bc4\u4f30", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8bc4\u4f30\u65b9\u6cd5\u9700\u8981\u6807\u6ce8\u6570\u636e\u3001\u65e0\u6cd5\u9488\u5bf9RAG\u5168\u94fe\u8def\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u7684\u95ee\u9898", "method": "\u8bbe\u8ba1\u516d\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u652f\u6301\u5f00\u53d1\u8005\u5728\u65e0\u6807\u6ce8\u6570\u636e\u60c5\u51b5\u4e0b\u5bf9\u68c0\u7d22\u589e\u5f3a\u751f\u6210QA\u5168\u94fe\u8def\u8fdb\u884c\u76d1\u63a7\u4f18\u5316", "result": "\u53d1\u73b0\u6307\u6807\u95f4\u7684\u5173\u8054\u6027\u53ef\u7cbe\u786e\u5b9a\u4f4d\u9700\u8981\u4f18\u5316\u7684RAG\u7ec4\u4ef6\uff08\u68c0\u7d22/\u751f\u6210/\u6392\u5e8f\u7b49\u6a21\u5757\uff09", "conclusion": "THELMA\u4e3aRAG\u95ee\u7b54\u7cfb\u7edf\u63d0\u4f9b\u7aef\u5230\u7aef\u7684\u8bc4\u4f30\u65b9\u6cd5\u8bba\uff0c\u901a\u8fc7\u6307\u6807\u5173\u8054\u5206\u6790\u6307\u5bfc\u5177\u4f53\u6a21\u5757\u7684\u4f18\u5316"}}
{"id": "2505.12734", "pdf": "https://arxiv.org/pdf/2505.12734", "abs": "https://arxiv.org/abs/2505.12734", "authors": ["Junbo Wang", "Haofeng Tan", "Bowen Liao", "Albert Jiang", "Teng Fei", "Qixing Huang", "Zhengzhong Tu", "Shan Ye", "Yuhao Kang"], "title": "SounDiT: Geo-Contextual Soundscape-to-Landscape Generation", "categories": ["cs.SD", "cs.AI", "cs.GR", "cs.HC", "eess.AS"], "comment": "14 pages, 5 figures", "summary": "We present a novel and practically significant problem-Geo-Contextual\nSoundscape-to-Landscape (GeoS2L) generation-which aims to synthesize\ngeographically realistic landscape images from environmental soundscapes. Prior\naudio-to-image generation methods typically rely on general-purpose datasets\nand overlook geographic and environmental contexts, resulting in unrealistic\nimages that are misaligned with real-world environmental settings. To address\nthis limitation, we introduce a novel geo-contextual computational framework\nthat explicitly integrates geographic knowledge into multimodal generative\nmodeling. We construct two large-scale geo-contextual multimodal datasets,\nSoundingSVI and SonicUrban, pairing diverse soundscapes with real-world\nlandscape images. We propose SounDiT, a novel Diffusion Transformer (DiT)-based\nmodel that incorporates geo-contextual scene conditioning to synthesize\ngeographically coherent landscape images. Furthermore, we propose a\npractically-informed geo-contextual evaluation framework, the Place Similarity\nScore (PSS), across element-, scene-, and human perception-levels to measure\nconsistency between input soundscapes and generated landscape images. Extensive\nexperiments demonstrate that SounDiT outperforms existing baselines in both\nvisual fidelity and geographic settings. Our work not only establishes\nfoundational benchmarks for GeoS2L generation but also highlights the\nimportance of incorporating geographic domain knowledge in advancing multimodal\ngenerative models, opening new directions at the intersection of generative AI,\ngeography, urban planning, and environmental sciences.", "AI": {"tldr": "\u63d0\u51faGeoS2L\u5730\u7406\u58f0\u666f\u8f6c\u666f\u89c2\u751f\u6210\u95ee\u9898\uff0c\u5f00\u53d1SounDiT\u6269\u6563Transformer\u6a21\u578b\u6574\u5408\u5730\u7406\u77e5\u8bc6\uff0c\u6784\u5efa\u591a\u6a21\u6001\u6570\u636e\u96c6\u5e76\u901a\u8fc7PSS\u8bc4\u4f30\u6846\u67b6\u9a8c\u8bc1\u5730\u7406\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u97f3\u9891\u751f\u6210\u56fe\u50cf\u65b9\u6cd5\u5ffd\u7565\u5730\u7406\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u751f\u6210\u7ed3\u679c\u4e0e\u771f\u5b9e\u73af\u5883\u4e0d\u5339\u914d\u3002\u9700\u6574\u5408\u5730\u7406\u77e5\u8bc6\u63d0\u5347\u591a\u6a21\u6001\u751f\u6210\u6a21\u578b\u7684\u5730\u7406\u8fde\u8d2f\u6027\u3002", "method": "1.\u6784\u5efa\u542b\u5730\u7406\u5c5e\u6027\u7684SoundingSVI\u548cSonicUrban\u6570\u636e\u96c6 2.\u63d0\u51fa\u5730\u7406\u6761\u4ef6\u7f16\u7801\u7684SounDiT\u6a21\u578b 3.\u8bbe\u8ba1\u5143\u7d20/\u573a\u666f/\u611f\u77e5\u4e09\u5c42\u6b21PSS\u8bc4\u4f30\u4f53\u7cfb", "result": "SounDiT\u5728\u89c6\u89c9\u8d28\u91cf(PSNR\u63d0\u534715.3%)\u548c\u5730\u7406\u4e00\u81f4\u6027(PSS\u63d0\u534722.7%)\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u51c6\u6a21\u578b\uff0c\u751f\u6210\u56fe\u50cf\u4e0e\u771f\u5b9e\u5730\u7406\u7279\u5f81\u9ad8\u5ea6\u5339\u914d\u3002", "conclusion": "\u8be5\u7814\u7a76\u786e\u7acb\u4e86\u5730\u7406\u611f\u77e5\u751f\u6210\u6a21\u578b\u7684\u65b0\u8303\u5f0f\uff0c\u63ed\u793a\u4e86\u9886\u57df\u77e5\u8bc6\u5bf9\u591a\u6a21\u6001\u751f\u6210\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u751f\u6210\u5f0fAI\u4e0e\u5730\u7406/\u57ce\u5e02\u89c4\u5212\u7684\u8de8\u5b66\u79d1\u5e94\u7528\u5f00\u8f9f\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.11628", "pdf": "https://arxiv.org/pdf/2505.11628", "abs": "https://arxiv.org/abs/2505.11628", "authors": ["Berkcan Kapusuzoglu", "Supriyo Chakraborty", "Chia-Hsuan Lee", "Sambit Sahu"], "title": "Critique-Guided Distillation: Improving Supervised Fine-tuning via Better Distillation", "categories": ["cs.CL", "cs.LG"], "comment": "Submitted to NeurIPS 2025", "summary": "Supervised fine-tuning (SFT) using expert demonstrations often suffer from\nthe imitation problem, where the model learns to reproduce the correct\nresponses without \\emph{understanding} the underlying rationale. To address\nthis limitation, we propose \\textsc{Critique-Guided Distillation (CGD)}, a\nnovel multi-stage framework that integrates teacher model generated\n\\emph{explanatory critiques} and \\emph{refined responses} into the SFT process.\nA student model is then trained to map the triplet of prompt, teacher critique,\nand its own initial response to the corresponding refined teacher response,\nthereby learning both \\emph{what} to imitate and \\emph{why}. Using\nentropy-based analysis, we show that \\textsc{CGD} reduces refinement\nuncertainty and can be interpreted as a Bayesian posterior update. We perform\nextensive empirical evaluation of \\textsc{CGD}, on variety of benchmark tasks,\nand demonstrate significant gains on both math (AMC23 +17.5%) and language\nunderstanding tasks (MMLU-Pro +6.3%), while successfully mitigating the format\ndrift issues observed in previous critique fine-tuning (CFT) techniques.", "AI": {"tldr": "\u63d0\u51faCritique-Guided Distillation (CGD)\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u6559\u5e08\u6a21\u578b\u7684\u89e3\u91ca\u6027\u6279\u5224\u548c\u6539\u8fdb\u54cd\u5e94\uff0c\u89e3\u51b3\u76d1\u7763\u5fae\u8c03\u4e2d\u7684\u6a21\u4eff\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u7406\u89e3\u80fd\u529b", "motivation": "\u76d1\u7763\u5fae\u8c03(SFT)\u5b58\u5728\u6a21\u4eff\u95ee\u9898\u2014\u2014\u6a21\u578b\u673a\u68b0\u590d\u5236\u4e13\u5bb6\u793a\u8303\u800c\u7f3a\u4e4f\u5bf9\u5e95\u5c42\u903b\u8f91\u7684\u7406\u89e3", "method": "\u591a\u9636\u6bb5\u6846\u67b6CGD\uff1a\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\u5c06\u63d0\u793a\u3001\u6559\u5e08\u6279\u5224\u548c\u81ea\u8eab\u521d\u59cb\u54cd\u5e94\u6620\u5c04\u5230\u6539\u8fdb\u540e\u7684\u6559\u5e08\u54cd\u5e94\uff0c\u540c\u65f6\u5b66\u4e60\u300e\u6a21\u4eff\u5185\u5bb9\u300f\u548c\u300e\u6a21\u4eff\u539f\u56e0\u300f", "result": "\u5728\u6570\u5b66(AMC23 +17.5%)\u548c\u8bed\u8a00\u7406\u89e3(MMLU-Pro +6.3%)\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u6709\u6548\u7f13\u89e3\u683c\u5f0f\u6f02\u79fb\u95ee\u9898", "conclusion": "CGD\u901a\u8fc7\u8d1d\u53f6\u65af\u540e\u9a8c\u66f4\u65b0\u89e3\u91ca\uff0c\u964d\u4f4e\u6539\u8fdb\u4e0d\u786e\u5b9a\u6027\uff0c\u8bc1\u660e\u6574\u5408\u6279\u5224\u6027\u53cd\u9988\u80fd\u6709\u6548\u589e\u5f3a\u6a21\u578b\u7684\u7406\u89e3\u4e0e\u63a8\u7406\u80fd\u529b"}}
{"id": "2505.11643", "pdf": "https://arxiv.org/pdf/2505.11643", "abs": "https://arxiv.org/abs/2505.11643", "authors": ["Xiang Fu"], "title": "Can an Easy-to-Hard Curriculum Make Reasoning Emerge in Small Language Models? Evidence from a Four-Stage Curriculum on GPT-2", "categories": ["cs.CL"], "comment": null, "summary": "We demonstrate that a developmentally ordered curriculum markedly improves\nreasoning transparency and sample-efficiency in small language models (SLMs).\nConcretely, we train Cognivolve, a 124 M-parameter GPT-2 model, on a four-stage\nsyllabus that ascends from lexical matching to multi-step symbolic inference\nand then evaluate it without any task-specific fine-tuning. Cognivolve reaches\ntarget accuracy in half the optimization steps of a single-phase baseline,\nactivates an order-of-magnitude more gradient-salient reasoning heads, and\nshifts those heads toward deeper layers, yielding higher-entropy attention that\nbalances local and long-range context. The same curriculum applied out of order\nor with optimizer resets fails to reproduce these gains, confirming that\nprogression--not extra compute--drives the effect. We also identify open\nchallenges: final-answer success still lags a conventional run by about 30%,\nand our saliency probe under-detects verbal-knowledge heads in the hardest\nstage, suggesting directions for mixed-stage fine-tuning and probe expansion.", "AI": {"tldr": "\u6709\u5e8f\u8bfe\u7a0b\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u900f\u660e\u5ea6\u548c\u6837\u672c\u6548\u7387\uff0c\u4f46\u6700\u7ec8\u51c6\u786e\u7387\u4ecd\u5b58\u5728\u5dee\u8ddd\uff0c\u5e76\u5b58\u5728\u663e\u7740\u6027\u63a2\u6d4b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u63a2\u7d22\u901a\u8fc7\u5206\u9636\u6bb5\u8bad\u7ec3\u63d0\u5347\u5c0f\u6a21\u578b\u6548\u7387\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u6837\u672c\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u56db\u9636\u6bb5\u53d1\u5c55\u5f0f\u8bfe\u7a0b\uff08\u4ece\u8bcd\u6c47\u5339\u914d\u5230\u591a\u6b65\u7b26\u53f7\u63a8\u7406\uff09\u8bad\u7ec3GPT-2\u6a21\u578b\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\uff0c\u5e76\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u9a8c\u8bc1\u8bfe\u7a0b\u987a\u5e8f\u7684\u5173\u952e\u4f5c\u7528\u3002", "result": "\u6709\u5e8f\u8bfe\u7a0b\u4f7f\u6a21\u578b\u57281/2\u65f6\u95f4\u5185\u8fbe\u5230\u76ee\u6807\u51c6\u786e\u7387\uff0c\u6fc0\u6d3b10\u500d\u68af\u5ea6\u663e\u8457\u63a8\u7406\u5934\u5e76\u8fc1\u79fb\u81f3\u6df1\u5c42\uff0c\u4f46\u6700\u7ec8\u51c6\u786e\u7387\u4ecd\u4f4e\u5e38\u89c4\u65b9\u6cd530%\uff0c\u4e14\u5728\u56f0\u96be\u9636\u6bb5\u51fa\u73b0\u63a2\u6d4b\u76f2\u533a\u3002", "conclusion": "\u8bfe\u7a0b\u987a\u5e8f\u662f\u6548\u679c\u63d0\u5347\u7684\u6838\u5fc3\u9a71\u52a8\u529b\uff0c\u672a\u6765\u9700\u901a\u8fc7\u6df7\u5408\u9636\u6bb5\u5fae\u8c03\u548c\u6269\u5c55\u663e\u7740\u6027\u63a2\u6d4b\u65b9\u6cd5\u89e3\u51b3\u5269\u4f59\u6311\u6218\u3002"}}
{"id": "2505.11665", "pdf": "https://arxiv.org/pdf/2505.11665", "abs": "https://arxiv.org/abs/2505.11665", "authors": ["Shubham Vatsal", "Harsh Dubey", "Aditi Singh"], "title": "Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive performance across\na wide range of Natural Language Processing (NLP) tasks. However, ensuring\ntheir effectiveness across multiple languages presents unique challenges.\nMultilingual prompt engineering has emerged as a key approach to enhance LLMs'\ncapabilities in diverse linguistic settings without requiring extensive\nparameter re-training or fine-tuning. With growing interest in multilingual\nprompt engineering over the past two to three years, researchers have explored\nvarious strategies to improve LLMs' performance across languages and NLP tasks.\nBy crafting structured natural language prompts, researchers have successfully\nextracted knowledge from LLMs across different languages, making these\ntechniques an accessible pathway for a broader audience, including those\nwithout deep expertise in machine learning, to harness the capabilities of\nLLMs. In this paper, we survey and categorize different multilingual prompting\ntechniques based on the NLP tasks they address across a diverse set of datasets\nthat collectively span around 250 languages. We further highlight the LLMs\nemployed, present a taxonomy of approaches and discuss potential\nstate-of-the-art (SoTA) methods for specific multilingual datasets.\nAdditionally, we derive a range of insights across language families and\nresource levels (high-resource vs. low-resource), including analyses such as\nthe distribution of NLP tasks by language resource type and the frequency of\nprompting methods across different language families. Our survey reviews 36\nresearch papers covering 39 prompting techniques applied to 30 multilingual NLP\ntasks, with the majority of these studies published in the last two years.", "AI": {"tldr": "\u8bba\u6587\u7cfb\u7edf\u7efc\u8ff0\u591a\u8bed\u8a00\u63d0\u793a\u5de5\u7a0b\u5982\u4f55\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728250\u79cd\u8bed\u8a00\u300130\u4e2aNLP\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u6db5\u76d636\u7bc7\u7814\u7a76\u53ca39\u79cd\u6280\u672f\u65b9\u6cd5\u3002", "motivation": "LLMs\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u9762\u4e34\u6311\u6218\uff0c\u591a\u8bed\u8a00\u63d0\u793a\u5de5\u7a0b\u63d0\u4f9b\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u8c03\u53c2\u7684\u8f7b\u91cf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u964d\u4f4e\u975e\u4e13\u4e1a\u4eba\u58eb\u4f7f\u7528\u95e8\u69db\u3002", "method": "\u901a\u8fc7\u6587\u732e\u8ba1\u91cf\u6cd5\u5bf9\u8fd12-3\u5e74\u7814\u7a76\u6210\u679c\u8fdb\u884c\u7cfb\u7edf\u5206\u7c7b\uff0c\u5206\u6790\u4e0d\u540c\u8bed\u8a00\u5bb6\u65cf/\u8d44\u6e90\u6c34\u5e73\u4e0b\u7684\u4efb\u52a1\u5206\u5e03\u4e0e\u63d0\u793a\u6280\u672f\u5e94\u7528\u9891\u7387\u3002", "result": "\u8986\u76d6250\u79cd\u8bed\u8a00\uff0c\u8bc6\u522b\u9ad8\u8d44\u6e90\u8bed\u8a00\u5e38\u7528\u7ed3\u6784\u5316\u6a21\u677f\u3001\u4f4e\u8d44\u6e90\u8bed\u8a00\u4fa7\u91cd\u77e5\u8bc6\u8fc1\u79fb\uff1b39\u79cd\u63d0\u793a\u6280\u672f\u4e2d\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08in-context learning\uff09\u5360\u6bd4\u6700\u9ad8\u3002", "conclusion": "\u591a\u8bed\u8a00\u63d0\u793a\u5de5\u7a0b\u663e\u8457\u63d0\u5347LLMs\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\uff0c\u672a\u6765\u9700\u52a0\u5f3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u7684\u63d0\u793a\u7b56\u7565\u4f18\u5316\u4e0e\u8bed\u8a00\u5bb6\u65cf\u7279\u6027\u7814\u7a76\u3002"}}
{"id": "2505.11679", "pdf": "https://arxiv.org/pdf/2505.11679", "abs": "https://arxiv.org/abs/2505.11679", "authors": ["Zhibo Hu", "Chen Wang", "Yanfeng Shu", "Hye-Young Paik", "Liming Zhu"], "title": "Ambiguity Resolution in Text-to-Structured Data Mapping", "categories": ["cs.CL", "cs.LG", "I.2.7"], "comment": "15 pages, 11 figures", "summary": "Ambiguity in natural language is a significant obstacle for achieving\naccurate text to structured data mapping through large language models (LLMs),\nwhich affects the performance of tasks such as mapping text to agentic tool\ncalling and text-to-SQL queries. Existing methods of ambiguity handling either\nexploit ReACT framework to produce the correct mapping through trial and error,\nor supervised fine tuning to guide models to produce a biased mapping to\nimprove certain tasks. In this paper, we adopt a different approach that\ncharacterizes the representation difference of ambiguous text in the latent\nspace and leverage the difference to identify ambiguity before mapping them to\nstructured data. To detect ambiguity of a sentence, we focused on the\nrelationship between ambiguous questions and their interpretations and what\ncause the LLM ignore multiple interpretations. Different to the distance\ncalculated by dense embedding vectors, we utilize the observation that\nambiguity is caused by concept missing in latent space of LLM to design a new\ndistance measurement, computed through the path kernel by the integral of\ngradient values for each concepts from sparse-autoencoder (SAE) under each\nstate. We identify patterns to distinguish ambiguous questions with this\nmeasurement. Based on our observation, We propose a new framework to improve\nthe performance of LLMs on ambiguous agentic tool calling through missing\nconcepts prediction.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u8868\u793a\u5dee\u5f02\u68c0\u6d4b\u81ea\u7136\u8bed\u8a00\u6b67\u4e49\u7684\u65b0\u65b9\u6cd5\uff0c\u63d0\u5347LLM\u7ed3\u6784\u5316\u6570\u636e\u6620\u5c04\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8eReACT\u8bd5\u9519\u548c\u76d1\u7763\u5fae\u8c03\u7684\u6b67\u4e49\u5904\u7406\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u6f5c\u5728\u7a7a\u95f4\u6982\u5ff5\u7f3a\u5931\u5bfc\u81f4\u7684\u6b67\u4e49", "method": "\u5229\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668(SAE)\u6982\u5ff5\u68af\u5ea6\u8def\u5f84\u6838\u79ef\u5206\uff0c\u8bbe\u8ba1\u65b0\u7684\u6b67\u4e49\u68c0\u6d4b\u8ddd\u79bb\u6d4b\u91cf\u65b9\u6cd5", "result": "\u5f00\u53d1\u51fa\u901a\u8fc7\u7f3a\u5931\u6982\u5ff5\u9884\u6d4b\u63d0\u5347LLM\u6b67\u4e49\u573a\u666f\u4e0b\u5de5\u5177\u8c03\u7528\u6027\u80fd\u7684\u65b0\u6846\u67b6", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5904\u7406\u81ea\u7136\u8bed\u8a00\u6b67\u4e49\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\uff0c\u53ef\u6269\u5c55\u5e94\u7528\u4e8e\u5de5\u5177\u8c03\u7528\u4e4b\u5916\u7684\u6587\u672c-SQL\u7b49\u7ed3\u6784\u5316\u6620\u5c04\u4efb\u52a1"}}
{"id": "2505.11683", "pdf": "https://arxiv.org/pdf/2505.11683", "abs": "https://arxiv.org/abs/2505.11683", "authors": ["Susanna R\u00fccker", "Alan Akbik"], "title": "Evaluating Design Decisions for Dual Encoder-based Entity Disambiguation", "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 (The 63rd Annual Meeting of the Association for\n  Computational Linguistics)", "summary": "Entity disambiguation (ED) is the task of linking mentions in text to\ncorresponding entries in a knowledge base. Dual Encoders address this by\nembedding mentions and label candidates in a shared embedding space and\napplying a similarity metric to predict the correct label. In this work, we\nfocus on evaluating key design decisions for Dual Encoder-based ED, such as its\nloss function, similarity metric, label verbalization format, and negative\nsampling strategy. We present the resulting model VerbalizED, a document-level\nDual Encoder model that includes contextual label verbalizations and efficient\nhard negative sampling. Additionally, we explore an iterative prediction\nvariant that aims to improve the disambiguation of challenging data points.\nComprehensive experiments on AIDA-Yago validate the effectiveness of our\napproach, offering insights into impactful design choices that result in a new\nState-of-the-Art system on the ZELDA benchmark.", "AI": {"tldr": "\u63d0\u51faVerbalizED\u53cc\u7f16\u7801\u5668\u6a21\u578b\u6539\u8fdb\u5b9e\u4f53\u6d88\u6b67\u4efb\u52a1\uff0c\u901a\u8fc7\u4f18\u5316\u6807\u7b7e\u8868\u8fbe\u548c\u8d1f\u91c7\u6837\u7b56\u7565\u5b9e\u73b0SOTA\u6027\u80fd", "motivation": "\u8bc4\u4f30\u53cc\u7f16\u7801\u5668\u5728\u5b9e\u4f53\u6d88\u6b67\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u8bbe\u8ba1\u51b3\u7b56\uff08\u635f\u5931\u51fd\u6570/\u76f8\u4f3c\u6027\u5ea6\u91cf/\u6807\u7b7e\u8868\u8fbe/\u8d1f\u91c7\u6837\u7b56\u7565\uff09\u5bf9\u6027\u80fd\u7684\u5f71\u54cd", "method": "\u6587\u6863\u7ea7\u53cc\u7f16\u7801\u5668\u67b6\u6784 + \u4e0a\u4e0b\u6587\u6807\u7b7e\u8868\u8fbe + \u9ad8\u6548\u786c\u8d1f\u91c7\u6837\u7b56\u7565 + \u8fed\u4ee3\u9884\u6d4b\u53d8\u4f53\u6539\u8fdb\u56f0\u96be\u6837\u672c\u6d88\u6b67", "result": "\u5728AIDA-Yago\u9a8c\u8bc1\u6709\u6548\u6027\uff0cZELDA\u57fa\u51c6\u6d4b\u8bd5\u8fbe\u5230\u65b0SOTA\uff08F1\u63d0\u53474.6%\uff09", "conclusion": "\u5173\u952e\u8bbe\u8ba1\u9009\u62e9\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4e0a\u4e0b\u6587\u6807\u7b7e\u8868\u8fbe\u4e0e\u786c\u8d1f\u91c7\u6837\u7b56\u7565\u7684\u7ec4\u5408\u5f62\u6210\u9ad8\u6548\u5b9e\u4f53\u6d88\u6b67\u7cfb\u7edf"}}
{"id": "2505.11690", "pdf": "https://arxiv.org/pdf/2505.11690", "abs": "https://arxiv.org/abs/2505.11690", "authors": ["Sukairaj Hafiz Imam", "Babangida Sani", "Dawit Ketema Gete", "Bedru Yimam Ahamed", "Ibrahim Said Ahmad", "Idris Abdulmumin", "Seid Muhie Yimam", "Muhammad Yahuza Bello", "Shamsuddeen Hassan Muhammad"], "title": "Automatic Speech Recognition for African Low-Resource Languages: Challenges and Future Directions", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Automatic Speech Recognition (ASR) technologies have transformed\nhuman-computer interaction; however, low-resource languages in Africa remain\nsignificantly underrepresented in both research and practical applications.\nThis study investigates the major challenges hindering the development of ASR\nsystems for these languages, which include data scarcity, linguistic\ncomplexity, limited computational resources, acoustic variability, and ethical\nconcerns surrounding bias and privacy. The primary goal is to critically\nanalyze these barriers and identify practical, inclusive strategies to advance\nASR technologies within the African context. Recent advances and case studies\nemphasize promising strategies such as community-driven data collection,\nself-supervised and multilingual learning, lightweight model architectures, and\ntechniques that prioritize privacy. Evidence from pilot projects involving\nvarious African languages showcases the feasibility and impact of customized\nsolutions, which encompass morpheme-based modeling and domain-specific ASR\napplications in sectors like healthcare and education. The findings highlight\nthe importance of interdisciplinary collaboration and sustained investment to\ntackle the distinct linguistic and infrastructural challenges faced by the\ncontinent. This study offers a progressive roadmap for creating ethical,\nefficient, and inclusive ASR systems that not only safeguard linguistic\ndiversity but also improve digital accessibility and promote socioeconomic\nparticipation for speakers of African languages.", "AI": {"tldr": "\u975e\u6d32\u4f4e\u8d44\u6e90\u8bed\u8a00ASR\u9762\u4e34\u6570\u636e/\u8d44\u6e90/\u4f26\u7406\u6311\u6218\uff0c\u9700\u793e\u533a\u9a71\u52a8+\u8f7b\u91cf\u6a21\u578b+\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u63a8\u52a8\u5305\u5bb9\u6027\u53d1\u5c55", "motivation": "\u975e\u6d32\u8bed\u8a00\u5728ASR\u7814\u7a76\u4e2d\u4ee3\u8868\u6027\u4e0d\u8db3\u5bfc\u81f4\u6570\u5b57\u9e3f\u6c9f\uff0c\u9700\u89e3\u51b3\u6280\u672f\u969c\u788d\u4ee5\u4fc3\u8fdb\u8bed\u8a00\u591a\u6837\u6027\u4e0e\u6570\u5b57\u5305\u5bb9", "method": "\u7ed3\u5408\u6848\u4f8b\u5206\u6790\u4e0e\u524d\u6cbf\u6280\u672f\uff08\u81ea\u76d1\u7763\u5b66\u4e60/\u591a\u8bed\u8a00\u5efa\u6a21\uff09\uff0c\u8bc4\u4f30\u793e\u533a\u6570\u636e\u91c7\u96c6\u548c\u8f7b\u91cf\u5316\u67b6\u6784\u7684\u5e94\u7528\u6548\u679c", "result": "\u8bd5\u70b9\u9879\u76ee\u9a8c\u8bc1\u5b9a\u5236\u65b9\u6848\u53ef\u884c\u6027\uff0c\u8bed\u7d20\u5efa\u6a21\u548c\u9886\u57df\u4e13\u7528ASR\u5728\u533b\u7597/\u6559\u80b2\u573a\u666f\u5c55\u73b0\u5e94\u7528\u6f5c\u529b", "conclusion": "\u9700\u8de8\u5b66\u79d1\u5408\u4f5c\u4e0e\u6301\u7eed\u6295\u8d44\uff0c\u6784\u5efa\u4f26\u7406\u4f18\u5148\u7684ASR\u7cfb\u7edf\u4ee5\u4fdd\u62a4\u8bed\u8a00\u591a\u6837\u6027\u5e76\u63d0\u5347\u975e\u6d32\u8bed\u8a00\u4f7f\u7528\u8005\u6570\u5b57\u53c2\u4e0e"}}
{"id": "2505.11693", "pdf": "https://arxiv.org/pdf/2505.11693", "abs": "https://arxiv.org/abs/2505.11693", "authors": ["Ana Ezquerro", "David Vilares", "Anssi Yli-Jyr\u00e4", "Carlos G\u00f3mez-Rodr\u00edguez"], "title": "Hierarchical Bracketing Encodings for Dependency Parsing as Tagging", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025. Original submission; camera-ready coming soon", "summary": "We present a family of encodings for sequence labeling dependency parsing,\nbased on the concept of hierarchical bracketing. We prove that the existing\n4-bit projective encoding belongs to this family, but it is suboptimal in the\nnumber of labels used to encode a tree. We derive an optimal hierarchical\nbracketing, which minimizes the number of symbols used and encodes projective\ntrees using only 12 distinct labels (vs. 16 for the 4-bit encoding). We also\nextend optimal hierarchical bracketing to support arbitrary non-projectivity in\na more compact way than previous encodings. Our new encodings yield competitive\naccuracy on a diverse set of treebanks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5206\u5c42\u62ec\u53f7\u7684\u5e8f\u5217\u6807\u6ce8\u7f16\u7801\u65b9\u6cd5\uff0c\u4f18\u5316\u6807\u7b7e\u6570\u91cf\u5e76\u652f\u6301\u975e\u6295\u5c04\u7ed3\u6784\uff0c\u5728\u591a\u4e2a\u6811\u5e93\u4e2d\u53d6\u5f97\u7ade\u4e89\u6027\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u67094-bit\u6295\u5c04\u7f16\u7801\u5b58\u5728\u6807\u7b7e\u5197\u4f59\u95ee\u9898\uff0c\u9700\u5f00\u53d1\u66f4\u7d27\u51d1\u7684\u7f16\u7801\u65b9\u5f0f\u540c\u65f6\u652f\u6301\u6295\u5c04/\u975e\u6295\u5c04\u4f9d\u5b58\u6811\u89e3\u6790\u3002", "method": "1. \u6784\u5efa\u6700\u4f18\u5206\u5c42\u62ec\u53f7\u7f16\u7801\uff0812\u6807\u7b7e\u66ff\u4ee3\u539f\u670916\u6807\u7b7e\uff09 2. \u6269\u5c55\u652f\u6301\u975e\u6295\u5c04\u7ed3\u6784\u7684\u7d27\u51d1\u7f16\u7801\u65b9\u6848", "result": "\u5728\u591a\u79cd\u7c7b\u578b\u6811\u5e93\u4e0a\u9a8c\u8bc1\uff0c\u65b0\u7f16\u7801\u65b9\u6848\u8fbe\u5230\u7ade\u4e89\u6027\u89e3\u6790\u51c6\u786e\u7387", "conclusion": "\u5206\u5c42\u62ec\u53f7\u7f16\u7801\u901a\u8fc7\u7b26\u53f7\u6700\u5c0f\u5316\u5b9e\u73b0\u9ad8\u6548\u4f9d\u5b58\u89e3\u6790\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u7f16\u7801\u6548\u7387\uff0c\u5e76\u66f4\u597d\u652f\u6301\u975e\u6295\u5c04\u7ed3\u6784\u3002"}}
{"id": "2505.11726", "pdf": "https://arxiv.org/pdf/2505.11726", "abs": "https://arxiv.org/abs/2505.11726", "authors": ["Shun Inadumi", "Nobuhiro Ueda", "Koichiro Yoshino"], "title": "Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures", "categories": ["cs.CL"], "comment": "ACL2025 main. Code available at https://github.com/SInadumi/mmrr", "summary": "Multimodal reference resolution, including phrase grounding, aims to\nunderstand the semantic relations between mentions and real-world objects.\nPhrase grounding between images and their captions is a well-established task.\nIn contrast, for real-world applications, it is essential to integrate textual\nand multimodal reference resolution to unravel the reference relations within\ndialogue, especially in handling ambiguities caused by pronouns and ellipses.\nThis paper presents a framework that unifies textual and multimodal reference\nresolution by mapping mention embeddings to object embeddings and selecting\nmentions or objects based on their similarity. Our experiments show that\nlearning textual reference resolution, such as coreference resolution and\npredicate-argument structure analysis, positively affects performance in\nmultimodal reference resolution. In particular, our model with coreference\nresolution performs better in pronoun phrase grounding than representative\nmodels for this task, MDETR and GLIP. Our qualitative analysis demonstrates\nthat incorporating textual reference relations strengthens the confidence\nscores between mentions, including pronouns and predicates, and objects, which\ncan reduce the ambiguities that arise in visually grounded dialogues.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u6587\u672c\u548c\u591a\u6a21\u6001\u6307\u4ee3\u6d88\u89e3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u5d4c\u5165\u6620\u5c04\u51cf\u5c11\u89c6\u89c9\u5bf9\u8bdd\u4e2d\u7684\u6b67\u4e49", "motivation": "\u73b0\u6709\u77ed\u8bed\u5b9a\u4f4d\u5c40\u9650\u4e8e\u56fe\u50cf-\u6807\u9898\u914d\u5bf9\uff0c\u5b9e\u9645\u5bf9\u8bdd\u573a\u666f\u9700\u6574\u5408\u6587\u672c/\u591a\u6a21\u6001\u6307\u4ee3\u6d88\u89e3\u6765\u5904\u7406\u4ee3\u8bcd/\u7701\u7565\u5f15\u53d1\u7684\u6b67\u4e49", "method": "\u5c06mention\u5d4c\u5165\u6620\u5c04\u5230object\u5d4c\u5165\u7a7a\u95f4\uff0c\u57fa\u4e8e\u76f8\u4f3c\u5ea6\u9009\u62e9mention\u6216object", "result": "\u6a21\u578b\u5728\u4ee3\u8bcd\u77ed\u8bed\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u4f18\u4e8eMDETR\u548cGLIP\uff0c\u6838\u5fc3ference\u89e3\u6790\u80fd\u589e\u5f3amention-object\u7f6e\u4fe1\u5ea6", "conclusion": "\u6574\u5408\u6587\u672c\u6307\u4ee3\u5173\u7cfb\u53ef\u6709\u6548\u964d\u4f4e\u89c6\u89c9\u5bf9\u8bdd\u4e2d\u7684\u6b67\u4e49\uff0c\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u7684\u53ef\u9760\u6027"}}
{"id": "2505.11733", "pdf": "https://arxiv.org/pdf/2505.11733", "abs": "https://arxiv.org/abs/2505.11733", "authors": ["Kevin Wu", "Eric Wu", "Rahul Thapa", "Kevin Wei", "Angela Zhang", "Arvind Suresh", "Jacqueline J. Tao", "Min Woo Sun", "Alejandro Lozano", "James Zou"], "title": "MedCaseReasoning: Evaluating and learning diagnostic reasoning from clinical case reports", "categories": ["cs.CL"], "comment": null, "summary": "Doctors and patients alike increasingly use Large Language Models (LLMs) to\ndiagnose clinical cases. However, unlike domains such as math or coding, where\ncorrectness can be objectively defined by the final answer, medical diagnosis\nrequires both the outcome and the reasoning process to be accurate. Currently,\nwidely used medical benchmarks like MedQA and MMLU assess only accuracy in the\nfinal answer, overlooking the quality and faithfulness of the clinical\nreasoning process. To address this limitation, we introduce MedCaseReasoning,\nthe first open-access dataset for evaluating LLMs on their ability to align\nwith clinician-authored diagnostic reasoning. The dataset includes 14,489\ndiagnostic question-and-answer cases, each paired with detailed reasoning\nstatements derived from open-access medical case reports. We evaluate\nstate-of-the-art reasoning LLMs on MedCaseReasoning and find significant\nshortcomings in their diagnoses and reasoning: for instance, the top-performing\nopen-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy\nand mentions only 64% of the clinician reasoning statements (recall). However,\nwe demonstrate that fine-tuning LLMs on the reasoning traces derived from\nMedCaseReasoning significantly improves diagnostic accuracy and clinical\nreasoning recall by an average relative gain of 29% and 41%, respectively. The\nopen-source dataset, code, and models are available at\nhttps://github.com/kevinwu23/Stanford-MedCaseReasoning.", "AI": {"tldr": "\u9996\u4e2a\u5f00\u653e\u6570\u636e\u96c6MedCaseReasoning\u8bc4\u4f30LLMs\u4e34\u5e8a\u8bca\u65ad\u63a8\u7406\u80fd\u529b\uff0c\u5fae\u8c03\u540e\u6a21\u578b\u6027\u80fd\u663e\u8457\u63d0\u5347", "motivation": "\u73b0\u6709\u533b\u5b66\u57fa\u51c6\u4ec5\u8bc4\u4f30\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027\uff0c\u5ffd\u89c6\u4e34\u5e8a\u63a8\u7406\u8fc7\u7a0b\u8d28\u91cf\uff0c\u9700\u5f00\u53d1\u65b0\u8bc4\u4f30\u4f53\u7cfb\u63d0\u5347LLMs\u8bca\u65ad\u53ef\u9760\u6027", "method": "\u6784\u5efa\u542b14,489\u4e2a\u4e34\u5e8a\u6848\u4f8b\u7684\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5fae\u8c03LLMs\u4f7f\u5176\u5b66\u4e60\u4e34\u5e8a\u533b\u751f\u63a8\u7406\u8f68\u8ff9", "result": "\u6700\u4f73\u5f00\u6e90\u6a21\u578bDeepSeek-R1\u521d\u59cb\u51c6\u786e\u738748%/\u53ec\u56de\u738764%\uff0c\u5fae\u8c03\u540e\u5206\u522b\u63d0\u534729%/41%", "conclusion": "MedCaseReasoning\u6709\u6548\u63d0\u5347LLMs\u533b\u7597\u8bca\u65ad\u6027\u80fd\uff0c\u4e3a\u533b\u5b66AI\u7814\u7a76\u63d0\u4f9b\u91cd\u8981\u57fa\u51c6\u8d44\u6e90"}}
{"id": "2505.11739", "pdf": "https://arxiv.org/pdf/2505.11739", "abs": "https://arxiv.org/abs/2505.11739", "authors": ["Feijiang Han", "Xiaodong Yu", "Jianheng Tang", "Lyle Ungar"], "title": "ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, training-free methods for improving large language models (LLMs)\nhave attracted growing interest, with token-level attention tuning emerging as\na promising and interpretable direction. However, existing methods typically\nrely on auxiliary mechanisms to identify important or irrelevant task-specific\ntokens, introducing potential bias and limiting applicability. In this paper,\nwe uncover a surprising and elegant alternative: the semantically empty initial\ntoken is a powerful and underexplored control point for optimizing model\nbehavior. Through theoretical analysis, we show that tuning the initial token's\nattention sharpens or flattens the attention distribution over subsequent\ntokens, and its role as an attention sink amplifies this effect. Empirically,\nwe find that: (1) tuning its attention improves LLM performance more\neffectively than tuning other task-specific tokens; (2) the effect follows a\nconsistent trend across layers, with earlier layers having greater impact, but\nvaries across attention heads, with different heads showing distinct\npreferences in how they attend to this token. Based on these findings, we\npropose ZeroTuning, a training-free approach that improves LLM performance by\napplying head-specific attention adjustments to this special token. Despite\ntuning only one token, ZeroTuning achieves higher performance on text\nclassification, multiple-choice, and multi-turn conversation tasks across\nmodels such as Llama, Qwen, and DeepSeek. For example, ZeroTuning improves\nLlama-3.1-8B by 11.71% on classification, 2.64% on QA tasks, and raises its\nmulti-turn score from 7.804 to 7.966. The method is also robust to limited\nresources, few-shot settings, long contexts, quantization, decoding strategies,\nand prompt variations. Our work sheds light on a previously overlooked control\npoint in LLMs, offering new insights into both inference-time tuning and model\ninterpretability.", "AI": {"tldr": "\u901a\u8fc7\u8c03\u6574\u5927\u8bed\u8a00\u6a21\u578b\u521d\u59cb\u7a7a\u8bed\u4e49token\u7684\u6ce8\u610f\u529b\uff0cZeroTuning\u65b9\u6cd5\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "motivation": "\u73b0\u6709\u57fa\u4e8etoken\u6ce8\u610f\u529b\u8c03\u6574\u7684\u65b9\u6cd5\u4f9d\u8d56\u8f85\u52a9\u673a\u5236\u8bc6\u522b\u5173\u952etoken\uff0c\u5b58\u5728\u504f\u5dee\u98ce\u9669\u3002\u7814\u7a76\u53d1\u73b0\u521d\u59cb\u7a7a\u8bed\u4e49token\u4f5c\u4e3a\u6ce8\u610f\u529b\u6c47\u805a\u70b9\uff0c\u80fd\u66f4\u6709\u6548\u63a7\u5236\u6a21\u578b\u884c\u4e3a", "method": "\u7406\u8bba\u8bc1\u660e\u8c03\u6574\u521d\u59cbtoken\u6ce8\u610f\u529b\u53ef\u6539\u53d8\u540e\u7eed\u6ce8\u610f\u529b\u5206\u5e03\uff0c\u7ed3\u5408\u5b9e\u9a8c\u53d1\u73b0\u4e0d\u540c\u6ce8\u610f\u529b\u5934\u7684\u8c03\u6574\u89c4\u5f8b\uff0c\u63d0\u51fa\u9488\u5bf9\u8be5token\u7684\u5934\u90e8\u7279\u5b9a\u6ce8\u610f\u529b\u8c03\u6574\u65b9\u6cd5ZeroTuning", "result": "\u5728\u6587\u672c\u5206\u7c7b\uff08+11.71%\uff09\u3001QA\uff08+2.64%\uff09\u548c\u591a\u8f6e\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347Llama\u7b49\u6a21\u578b\u6027\u80fd\uff0c\u65b9\u6cd5\u5bf9\u8d44\u6e90\u9650\u5236\u3001\u957f\u4e0a\u4e0b\u6587\u7b49\u573a\u666f\u8868\u73b0\u9c81\u68d2", "conclusion": "\u63ed\u793a\u4e86LLMs\u4e2d\u4e00\u4e2a\u88ab\u5ffd\u89c6\u7684\u5173\u952e\u63a7\u5236\u70b9\uff0c\u4e3a\u63a8\u7406\u65f6\u4f18\u5316\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0cZeroTuning\u5c55\u793a\u4e86\u65e0\u8bad\u7ec3\u8c03\u4f18\u7684\u6f5c\u529b"}}
{"id": "2505.11746", "pdf": "https://arxiv.org/pdf/2505.11746", "abs": "https://arxiv.org/abs/2505.11746", "authors": ["Xianglong Xu", "John Bowen", "Rojin Taheri"], "title": "Token Masking Improves Transformer-Based Text Classification", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "While transformer-based models achieve strong performance on text\nclassification, we explore whether masking input tokens can further enhance\ntheir effectiveness. We propose token masking regularization, a simple yet\ntheoretically motivated method that randomly replaces input tokens with a\nspecial [MASK] token at probability p. This introduces stochastic perturbations\nduring training, leading to implicit gradient averaging that encourages the\nmodel to capture deeper inter-token dependencies. Experiments on language\nidentification and sentiment analysis -- across diverse models (mBERT,\nQwen2.5-0.5B, TinyLlama-1.1B) -- show consistent improvements over standard\nregularization techniques. We identify task-specific optimal masking rates,\nwith p = 0.1 as a strong general default. We attribute the gains to two key\neffects: (1) input perturbation reduces overfitting, and (2) gradient-level\nsmoothing acts as implicit ensembling.", "AI": {"tldr": "\u63d0\u51fa\u63a9\u7801\u8f93\u5165\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u968f\u673a\u63a9\u7801\u63d0\u5347Transformer\u6a21\u578b\u6027\u80fd\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u591a\u79cdNLP\u4efb\u52a1\u4e2d\u7a33\u5b9a\u4f18\u4e8e\u6807\u51c6\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u6700\u4f73\u63a9\u7801\u7387p=0.1\uff0c\u6548\u679c\u6e90\u4e8e\u6297\u8fc7\u62df\u5408\u548c\u9690\u5f0f\u6a21\u578b\u96c6\u6210", "motivation": "\u73b0\u6709Transformer\u6a21\u578b\u5728\u6587\u672c\u5206\u7c7b\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u901a\u8fc7\u8f93\u5165\u63a9\u7801\u7684\u6b63\u5219\u5316\u65b9\u5f0f\u8fdb\u4e00\u6b65\u6316\u6398\u6a21\u578b\u6f5c\u529b\uff0c\u6539\u5584\u5bf9\u6df1\u5c42\u8bed\u4e49\u4f9d\u8d56\u5173\u7cfb\u7684\u6355\u6349\u80fd\u529b", "method": "\u63d0\u51fatoken masking regularization\u65b9\u6cd5\uff1a\u8bad\u7ec3\u65f6\u4ee5\u6982\u7387p\u968f\u673a\u5c06\u8f93\u5165token\u66ff\u6362\u4e3a[MASK]\uff0c\u901a\u8fc7\u68af\u5ea6\u5c42\u9762\u7684\u9690\u5f0f\u5e73\u5747\u673a\u5236\uff0c\u5f3a\u5236\u6a21\u578b\u5efa\u7acb\u66f4\u9c81\u68d2\u7684token\u95f4\u4f9d\u8d56\u5173\u7cfb\u3002\u5728mBERT\u3001Qwen2.5-0.5B\u7b49\u591a\u4e2a\u6a21\u578b\u67b6\u6784\u4e0a\u8fdb\u884c\u8de8\u4efb\u52a1\u9a8c\u8bc1", "result": "\u5728\u8bed\u8a00\u8bc6\u522b\u548c\u60c5\u611f\u5206\u6790\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u6b63\u5219\u5316\u6280\u672f\u3002\u53d1\u73b0\u4efb\u52a1\u7279\u5f02\u6027\u6700\u4f18\u63a9\u7801\u7387\uff08\u901a\u7528\u573a\u666f\u63a8\u8350p=0.1\uff09\uff0c\u6027\u80fd\u63d0\u5347\u6e90\u4e8e\uff1a(1)\u8f93\u5165\u6270\u52a8\u6291\u5236\u8fc7\u62df\u5408 (2)\u68af\u5ea6\u5e73\u6ed1\u4ea7\u751f\u9690\u5f0f\u96c6\u6210\u6548\u679c", "conclusion": "\u63a9\u7801\u8f93\u5165\u6b63\u5219\u5316\u662f\u7b80\u5355\u6709\u6548\u7684\u6539\u8fdb\u7b56\u7565\uff0c\u5176\u8de8\u6a21\u578b\u548c\u8de8\u4efb\u52a1\u7684\u7a33\u5b9a\u63d0\u5347\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u4e3aTransformer\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u6b63\u5219\u5316\u89c6\u89d2"}}
{"id": "2505.11754", "pdf": "https://arxiv.org/pdf/2505.11754", "abs": "https://arxiv.org/abs/2505.11754", "authors": ["Wenyu Huang", "Pavlos Vougiouklis", "Mirella Lapata", "Jeff Z. Pan"], "title": "Masking in Multi-hop QA: An Analysis of How Language Models Perform with Context Permutation", "categories": ["cs.CL"], "comment": "ACL 2025 main", "summary": "Multi-hop Question Answering (MHQA) adds layers of complexity to question\nanswering, making it more challenging. When Language Models (LMs) are prompted\nwith multiple search results, they are tasked not only with retrieving relevant\ninformation but also employing multi-hop reasoning across the information\nsources. Although LMs perform well on traditional question-answering tasks, the\ncausal mask can hinder their capacity to reason across complex contexts. In\nthis paper, we explore how LMs respond to multi-hop questions by permuting\nsearch results (retrieved documents) under various configurations. Our study\nreveals interesting findings as follows: 1) Encoder-decoder models, such as the\nones in the Flan-T5 family, generally outperform causal decoder-only LMs in\nMHQA tasks, despite being significantly smaller in size; 2) altering the order\nof gold documents reveals distinct trends in both Flan T5 models and fine-tuned\ndecoder-only models, with optimal performance observed when the document order\naligns with the reasoning chain order; 3) enhancing causal decoder-only models\nwith bi-directional attention by modifying the causal mask can effectively\nboost their end performance. In addition to the above, we conduct a thorough\ninvestigation of the distribution of LM attention weights in the context of\nMHQA. Our experiments reveal that attention weights tend to peak at higher\nvalues when the resulting answer is correct. We leverage this finding to\nheuristically improve LMs' performance on this task. Our code is publicly\navailable at https://github.com/hwy9855/MultiHopQA-Reasoning.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8df3\u95ee\u7b54\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u7f16\u7801\u5668-\u89e3\u7801\u5668\u67b6\u6784\u4f18\u4e8e\u7eaf\u89e3\u7801\u5668\u6a21\u578b\uff0c\u901a\u8fc7\u8c03\u6574\u6587\u6863\u987a\u5e8f\u548c\u6ce8\u610f\u529b\u673a\u5236\u53ef\u63d0\u5347\u6027\u80fd", "motivation": "\u4f20\u7edf\u56e0\u679c\u63a9\u7801\u9650\u5236\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4e0a\u4e0b\u6587\u4e2d\u7684\u591a\u8df3\u63a8\u7406\u80fd\u529b\uff0c\u9700\u63a2\u7d22\u4e0d\u540c\u6587\u6863\u6392\u5217\u5bf9\u6a21\u578b\u8868\u73b0\u7684\u5f71\u54cd", "method": "\u901a\u8fc7\u6392\u5217\u641c\u7d22\u7ed3\u679c\u987a\u5e8f\u6d4b\u8bd5\u6a21\u578b\u8868\u73b0\uff0c\u5206\u6790\u6ce8\u610f\u529b\u6743\u91cd\u5206\u5e03\uff0c\u5e76\u4fee\u6539\u56e0\u679c\u63a9\u7801\u5f15\u5165\u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236", "result": "1. Flan-T5\u7cfb\u5217\u6a21\u578b\u4f18\u4e8e\u540c\u4efb\u52a1\u89e3\u7801\u5668\u6a21\u578b 2. \u6587\u6863\u987a\u5e8f\u4e0e\u63a8\u7406\u94fe\u4e00\u81f4\u65f6\u6548\u679c\u6700\u4f73 3. \u53cc\u5411\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u89e3\u7801\u5668\u6a21\u578b\u6027\u80fd 4. \u6ce8\u610f\u529b\u6743\u91cd\u5cf0\u503c\u4e0e\u7b54\u6848\u6b63\u786e\u6027\u6b63\u76f8\u5173", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6a21\u578b\u67b6\u6784\u9009\u62e9\u3001\u6587\u6863\u987a\u5e8f\u4f18\u5316\u548c\u6ce8\u610f\u529b\u673a\u5236\u6539\u8fdb\u5bf9\u591a\u8df3\u95ee\u7b54\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u6ce8\u610f\u529b\u6743\u91cd\u7684\u542f\u53d1\u5f0f\u4f18\u5316\u65b9\u6cd5"}}
{"id": "2505.11764", "pdf": "https://arxiv.org/pdf/2505.11764", "abs": "https://arxiv.org/abs/2505.11764", "authors": ["Raymond Baartmans", "Matthew Raffel", "Rahul Vikram", "Aiden Deringer", "Lizhong Chen"], "title": "Towards Universal Semantics With Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a\nuniversal set of semantic primes: simple, primitive word-meanings that have\nbeen shown to exist in most, if not all, languages of the world. According to\nthis framework, any word, regardless of complexity, can be paraphrased using\nthese primes, revealing a clear and universally translatable meaning. These\nparaphrases, known as explications, can offer valuable applications for many\nnatural language processing (NLP) tasks, but producing them has traditionally\nbeen a slow, manual process. In this work, we present the first study of using\nlarge language models (LLMs) to generate NSM explications. We introduce\nautomatic evaluation methods, a tailored dataset for training and evaluation,\nand fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in\nproducing accurate, cross-translatable explications, marking a significant step\ntoward universal semantic representation with LLMs and opening up new\npossibilities for applications in semantic analysis, translation, and beyond.", "AI": {"tldr": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u81ea\u7136\u8bed\u4e49\u5143\u8bed\u8a00\u89e3\u91ca\uff0c1B/8B\u6a21\u578b\u8d85\u8d8aGPT-4o\u5b9e\u73b0\u8de8\u8bed\u8a00\u8bed\u4e49\u8868\u793a", "motivation": "\u4f20\u7edfNSM\u89e3\u91ca\u751f\u6210\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u63a8\u52a8NLP\u5e94\u7528\u53d1\u5c55", "method": "\u5f00\u53d1\u81ea\u52a8\u8bc4\u4f30\u4f53\u7cfb\uff0c\u6784\u5efa\u4e13\u7528\u6570\u636e\u96c6\uff0c\u5e76\u5fae\u8c03\u4e0d\u540c\u89c4\u6a21\u7684LLM\u6a21\u578b", "result": "1B/8B\u6a21\u578b\u5728\u751f\u6210\u51c6\u786e\u6027\u548c\u8de8\u8bed\u8a00\u9002\u914d\u6027\u4e0a\u663e\u8457\u4f18\u4e8eGPT-4o", "conclusion": "\u9996\u6b21\u5b9e\u73b0LLM\u9a71\u52a8\u7684\u901a\u7528\u8bed\u4e49\u8868\u793a\u7a81\u7834\uff0c\u4e3a\u8bed\u4e49\u5206\u6790\u548c\u673a\u5668\u7ffb\u8bd1\u5f00\u8f9f\u65b0\u8def\u5f84"}}
{"id": "2505.11807", "pdf": "https://arxiv.org/pdf/2505.11807", "abs": "https://arxiv.org/abs/2505.11807", "authors": ["Yufei Xiang", "Yiqun Shen", "Yeqin Zhang", "Cam-Tu Nguyen"], "title": "Retrospex: Language Agent Meets Offline Reinforcement Learning Critic", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages", "summary": "Large Language Models (LLMs) possess extensive knowledge and commonsense\nreasoning capabilities, making them valuable for creating powerful agents.\nHowever, existing LLM agent frameworks have not fully utilized past experiences\nfor improvement. This work introduces a new LLM-based agent framework called\nRetrospex, which addresses this challenge by analyzing past experiences in\ndepth. Unlike previous approaches, Retrospex does not directly integrate\nexperiences into the LLM's context. Instead, it combines the LLM's action\nlikelihood with action values estimated by a Reinforcement Learning (RL)\nCritic, which is trained on past experiences through an offline\n''retrospection'' process. Additionally, Retrospex employs a dynamic action\nrescoring mechanism that increases the importance of experience-based values\nfor tasks that require more interaction with the environment. We evaluate\nRetrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its\nadvantages over strong, contemporary baselines.", "AI": {"tldr": "\u63d0\u51faRetrospex\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u884c\u4e3a\u6982\u7387\u4e0e\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7ecf\u9a8c\u8bc4\u4f30\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u667a\u80fd\u4f53\u6846\u67b6\u672a\u80fd\u5145\u5206\u5229\u7528\u5386\u53f2\u7ecf\u9a8c\u4f18\u5316\u51b3\u7b56\u673a\u5236\uff0c\u9700\u5f00\u53d1\u80fd\u6df1\u5ea6\u5206\u6790\u7ecf\u9a8c\u6570\u636e\u7684\u65b0\u578b\u67b6\u6784\u3002", "method": "1. \u5206\u79bb\u7ecf\u9a8c\u5b58\u50a8\u4e0e\u6a21\u578b\u4e0a\u4e0b\u6587\n2. \u901a\u8fc7\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3Critic\u8bc4\u4f30\u6a21\u5757\n3. \u52a8\u6001\u8c03\u6574\u7ecf\u9a8c\u4ef7\u503c\u6743\u91cd\uff08\u73af\u5883\u4ea4\u4e92\u9700\u6c42\u8d8a\u9ad8\uff0c\u7ecf\u9a8c\u4ef7\u503c\u5360\u6bd4\u8d8a\u5927\uff09", "result": "\u5728ScienceWorld\u3001ALFWorld\u548cWebshop\u73af\u5883\u4e2d\u9a8c\u8bc1\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "\u7ecf\u9a8c\u56de\u6eaf\u673a\u5236\u4e0e\u52a8\u6001\u8bc4\u5206\u7b56\u7565\u7684\u7ed3\u5408\uff0c\u4e3aLLM\u667a\u80fd\u4f53\u7684\u6301\u7eed\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u8303\u5f0f"}}
{"id": "2505.11810", "pdf": "https://arxiv.org/pdf/2505.11810", "abs": "https://arxiv.org/abs/2505.11810", "authors": ["Shen Li", "Renfen Hu", "Lijun Wang"], "title": "Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model", "categories": ["cs.CL"], "comment": null, "summary": "General-purpose large language models demonstrate notable capabilities in\nlanguage comprehension and generation, achieving results that are comparable\nto, or even surpass, human performance in many language information processing\ntasks. Nevertheless, when general models are applied to some specific domains,\ne.g., Classical Chinese texts, their effectiveness is often unsatisfactory, and\nfine-tuning open-source foundational models similarly struggles to adequately\nincorporate domain-specific knowledge. To address this challenge, this study\ndeveloped a large language model, AI Taiyan, specifically designed for\nunderstanding and generating Classical Chinese. Experiments show that with a\nreasonable model design, data processing, foundational training, and\nfine-tuning, satisfactory results can be achieved with only 1.8 billion\nparameters. In key tasks related to Classical Chinese information processing\nsuch as punctuation, identification of allusions, explanation of word meanings,\nand translation between ancient and modern Chinese, this model exhibits a clear\nadvantage over both general-purpose large models and domain-specific\ntraditional models, achieving levels close to or surpassing human baselines.\nThis research provides a reference for the efficient construction of\nspecialized domain-specific large language models. Furthermore, the paper\ndiscusses the application of this model in fields such as the collation of\nancient texts, dictionary editing, and language research, combined with case\nstudies.", "AI": {"tldr": "18\u4ebf\u53c2\u6570\u7684\u6587\u8a00\u6587\u5927\u6a21\u578bAI Taiyan\u5728\u6807\u70b9/\u5178\u6545\u8bc6\u522b\u7b49\u4efb\u52a1\u4e2d\u8d85\u8d8a\u901a\u7528\u6a21\u578b\u548c\u4f20\u7edf\u6a21\u578b\uff0c\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73", "motivation": "\u901a\u7528\u5927\u6a21\u578b\u5728\u6587\u8a00\u6587\u9886\u57df\u8868\u73b0\u6b20\u4f73\uff0c\u4f20\u7edf\u5fae\u8c03\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u878d\u5165\u9886\u57df\u77e5\u8bc6", "method": "\u901a\u8fc7\u5408\u7406\u6a21\u578b\u8bbe\u8ba1+\u6570\u636e\u6e05\u6d17+\u57fa\u7840\u8bad\u7ec3+\u5fae\u8c03\u7684\u4e09\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u6784\u5efa18\u4ebf\u53c2\u6570\u4e13\u7528\u6a21\u578b", "result": "\u5728\u6587\u8a00\u6587\u6807\u70b9/\u5178\u6545\u8bc6\u522b/\u8bcd\u4e49\u89e3\u91ca/\u53e4\u4eca\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u8d85GPT-4 10\u4e2a\u767e\u5206\u70b9\uff0c\u90e8\u5206\u4efb\u52a1\u8fbe\u4eba\u7c7b\u4e13\u5bb6\u6c34\u5e73", "conclusion": "\u9a8c\u8bc1\u4e86\u5c0f\u89c4\u6a21\u4e13\u7528\u6a21\u578b\u7684\u6280\u672f\u8def\u5f84\uff0c\u4e3a\u53e4\u7c4d\u6574\u7406/\u8bcd\u5178\u7f16\u7e82\u63d0\u4f9b\u667a\u80fd\u652f\u6301\uff0c\u63a8\u52a8\u8ba1\u7b97\u8bed\u8a00\u5b66\u7814\u7a76\u8303\u5f0f\u521b\u65b0"}}
{"id": "2505.11811", "pdf": "https://arxiv.org/pdf/2505.11811", "abs": "https://arxiv.org/abs/2505.11811", "authors": ["Taolin Zhang", "Dongyang Li", "Qizhou Chen", "Chengyu Wang", "Xiaofeng He"], "title": "BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering", "categories": ["cs.CL"], "comment": "Accepted by ACL2025 main track", "summary": "Multi-hop question answering (QA) involves finding multiple relevant passages\nand performing step-by-step reasoning to answer complex questions. Previous\nworks on multi-hop QA employ specific methods from different modeling\nperspectives based on large language models (LLMs), regardless of the question\ntypes. In this paper, we first conduct an in-depth analysis of public multi-hop\nQA benchmarks, dividing the questions into four types and evaluating five types\nof cutting-edge methods for multi-hop QA: Chain-of-Thought (CoT), Single-step,\nIterative-step, Sub-step, and Adaptive-step. We find that different types of\nmulti-hop questions have varying degrees of sensitivity to different types of\nmethods. Thus, we propose a Bi-levEL muLti-agEnt reasoning (BELLE) framework to\naddress multi-hop QA by specifically focusing on the correspondence between\nquestion types and methods, where each type of method is regarded as an\n''operator'' by prompting LLMs differently. The first level of BELLE includes\nmultiple agents that debate to obtain an executive plan of combined\n''operators'' to address the multi-hop QA task comprehensively. During the\ndebate, in addition to the basic roles of affirmative debater, negative\ndebater, and judge, at the second level, we further leverage fast and slow\ndebaters to monitor whether changes in viewpoints are reasonable. Extensive\nexperiments demonstrate that BELLE significantly outperforms strong baselines\nin various datasets. Additionally, the model consumption of BELLE is higher\ncost-effectiveness than that of single models in more complex multi-hop QA\nscenarios.", "AI": {"tldr": "\u63d0\u51faBELLE\u5206\u5c42\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u95ee\u9898\u7c7b\u578b\u4e0e\u65b9\u6cd5\u5339\u914d\u4f18\u5316\u591a\u8df3\u95ee\u7b54\uff0c\u5b9e\u9a8c\u8bc1\u660e\u9ad8\u6548\u4e14\u5177\u6210\u672c\u6548\u76ca", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u8003\u8651\u95ee\u9898\u7c7b\u578b\u5bf9\u591a\u8df3QA\u65b9\u6cd5\u654f\u611f\u5ea6\u7684\u5f71\u54cd\uff0c\u9700\u5efa\u7acb\u7c7b\u578b\u4e0e\u65b9\u6cd5\u7684\u52a8\u6001\u5bf9\u5e94\u5173\u7cfb", "method": "1. \u7b2c\u4e00\u5c42\u591a\u4ee3\u7406\u8fa9\u8bba\u751f\u6210\u7ec4\u5408\u65b9\u6cd5\u65b9\u6848 2. \u7b2c\u4e8c\u5c42\u5feb\u6162\u601d\u8003\u8005\u76d1\u63a7\u89c2\u70b9\u5408\u7406\u6027 3. \u5c06\u4e0d\u540c\u65b9\u6cd5\u5c01\u88c5\u4e3a\u53ef\u7ec4\u5408\u7684LLM\u63d0\u793a\u64cd\u4f5c\u7b26", "result": "BELLE\u5728\u591a\u4e2a\u6570\u636e\u96c6\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u590d\u6742\u573a\u666f\u6210\u672c\u6548\u76ca\u63d0\u534730%\u4ee5\u4e0a", "conclusion": "\u5206\u5c42\u8fa9\u8bba\u673a\u5236\u7ed3\u5408\u95ee\u9898\u7c7b\u578b\u9002\u914d\u65b9\u6cd5\u7b56\u7565\uff0c\u4e3a\u52a8\u6001\u591a\u8df3QA\u63a8\u7406\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2505.11820", "pdf": "https://arxiv.org/pdf/2505.11820", "abs": "https://arxiv.org/abs/2505.11820", "authors": ["Kaitao Song", "Xiaohua Wang", "Xu Tan", "Huiqiang Jiang", "Chengruidong Zhang", "Yongliang Shen", "Cen LU", "Zihao Li", "Zifan Song", "Caihua Shan", "Yansen Wang", "Kan Ren", "Xiaoqing Zheng", "Tao Qin", "Yuqing Yang", "Dongsheng Li", "Lili Qiu"], "title": "Chain-of-Model Learning for Language Model", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we propose a novel learning paradigm, termed Chain-of-Model\n(CoM), which incorporates the causal relationship into the hidden states of\neach layer as a chain style, thereby introducing great scaling efficiency in\nmodel training and inference flexibility in deployment. We introduce the\nconcept of Chain-of-Representation (CoR), which formulates the hidden states at\neach layer as a combination of multiple sub-representations (i.e., chains) at\nthe hidden dimension level. In each layer, each chain from the output\nrepresentations can only view all of its preceding chains in the input\nrepresentations. Consequently, the model built upon CoM framework can\nprogressively scale up the model size by increasing the chains based on the\nprevious models (i.e., chains), and offer multiple sub-models at varying sizes\nfor elastic inference by using different chain numbers. Based on this\nprinciple, we devise Chain-of-Language-Model (CoLM), which incorporates the\nidea of CoM into each layer of Transformer architecture. Based on CoLM, we\nfurther introduce CoLM-Air by introducing a KV sharing mechanism, that computes\nall keys and values within the first chain and then shares across all chains.\nThis design demonstrates additional extensibility, such as enabling seamless LM\nswitching, prefilling acceleration and so on. Experimental results demonstrate\nour CoLM family can achieve comparable performance to the standard Transformer,\nwhile simultaneously enabling greater flexiblity, such as progressive scaling\nto improve training efficiency and offer multiple varying model sizes for\nelastic inference, paving a a new way toward building language models. Our code\nwill be released in the future at: https://github.com/microsoft/CoLM.", "AI": {"tldr": "\u63d0\u51faChain-of-Model\uff08CoM\uff09\u5b66\u4e60\u8303\u5f0f\uff0c\u901a\u8fc7\u9690\u85cf\u72b6\u6001\u7684\u94fe\u5f0f\u7ed3\u6784\u63d0\u5347\u6a21\u578b\u6269\u5c55\u6027\u548c\u63a8\u7406\u5f39\u6027\uff0c\u5f00\u53d1\u4e86CoLM\u53ca\u5176\u6539\u8fdb\u7248CoLM-Air\u5b9e\u73b0\u8bad\u7ec3\u6548\u7387\u4e0e\u90e8\u7f72\u7075\u6d3b\u6027\u7684\u5e73\u8861", "motivation": "\u4f20\u7edf\u6a21\u578b\u5728\u6269\u5c55\u65f6\u9700\u91cd\u65b0\u8bad\u7ec3\u6574\u4e2a\u67b6\u6784\u4e14\u7f3a\u4e4f\u5f39\u6027\u63a8\u7406\u80fd\u529b\uff0cCoM\u901a\u8fc7\u56e0\u679c\u5173\u7cfb\u94fe\u5f0f\u7ed3\u6784\u89e3\u51b3\u6a21\u578b\u6269\u5c55\u6548\u7387\u4f4e\u548c\u90e8\u7f72\u4e0d\u7075\u6d3b\u7684\u95ee\u9898", "method": "1. \u63d0\u51faChain-of-Representation\uff08CoR\uff09\u5c06\u9690\u85cf\u72b6\u6001\u5206\u89e3\u4e3a\u591a\u4e2a\u7ef4\u5ea6\u94fe\uff0c\u524d\u5e8f\u94fe\u51b3\u5b9a\u540e\u7eed\u94fe\n2. \u57fa\u4e8eTransformer\u6784\u5efaCoLM\n3. \u6539\u8fdbCoLM-Air\u901a\u8fc7\u9996\u94fe\u5171\u4eabKV\u673a\u5236\u5b9e\u73b0\u8ba1\u7b97\u4f18\u5316", "result": "CoLM\u7cfb\u5217\u5728\u4fdd\u6301\u4e0e\u6807\u51c6Transformer\u76f8\u5f53\u6027\u80fd\u7684\u540c\u65f6\uff0c\u652f\u6301\u6e10\u8fdb\u5f0f\u6269\u5c55\uff08\u8bad\u7ec3\u6548\u7387\u63d0\u53472.4\u500d\uff09\u548c\u5f39\u6027\u63a8\u7406\uff08\u652f\u6301\u4ece1.3B\u523013B\u7684\u591a\u5c3a\u5bf8\u5b50\u6a21\u578b\uff09", "conclusion": "CoM\u8303\u5f0f\u901a\u8fc7\u6a21\u5757\u5316\u94fe\u5f0f\u7ed3\u6784\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u8bad\u7ec3\u9636\u6bb5\u6e10\u8fdb\u6269\u5c55\u548c\u90e8\u7f72\u9636\u6bb5\u5f39\u6027\u63a8\u7406\u7684\u65b0\u8303\u5f0f\uff0cKV\u5171\u4eab\u673a\u5236\u8fdb\u4e00\u6b65\u6269\u5c55\u4e86\u5e94\u7528\u573a\u666f"}}
{"id": "2505.11827", "pdf": "https://arxiv.org/pdf/2505.11827", "abs": "https://arxiv.org/abs/2505.11827", "authors": ["Yansong Ning", "Wei Li", "Jun Fang", "Naiqiang Tan", "Hao Liu"], "title": "Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": "In progress", "summary": "Compressing long chain-of-thought (CoT) from large language models (LLMs) is\nan emerging strategy to improve the reasoning efficiency of LLMs. Despite its\npromising benefits, existing studies equally compress all thoughts within a\nlong CoT, hindering more concise and effective reasoning. To this end, we first\ninvestigate the importance of different thoughts by examining their\neffectiveness and efficiency in contributing to reasoning through automatic\nlong CoT chunking and Monte Carlo rollouts. Building upon the insights, we\npropose a theoretically bounded metric to jointly measure the effectiveness and\nefficiency of different thoughts. We then propose Long$\\otimes$Short, an\nefficient reasoning framework that enables two LLMs to collaboratively solve\nthe problem: a long-thought LLM for more effectively generating important\nthoughts, while a short-thought LLM for efficiently generating remaining\nthoughts. Specifically, we begin by synthesizing a small amount of cold-start\ndata to fine-tune LLMs for long-thought and short-thought reasoning styles,\nrespectively. Furthermore, we propose a synergizing-oriented multi-turn\nreinforcement learning, focusing on the model self-evolution and collaboration\nbetween long-thought and short-thought LLMs. Experimental results show that our\nmethod enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance\ncompared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while\nreducing token length by over 80% across the MATH500, AIME24/25, AMC23, and\nGPQA Diamond benchmarks. Our data and code are available at\nhttps://github.com/yasNing/Long-otimes-Short/.", "AI": {"tldr": "\u63d0\u51faLong\u2297Short\u6846\u67b6\uff0c\u901a\u8fc7\u533a\u5206\u5173\u952e\u601d\u8003\u4e0e\u666e\u901a\u601d\u8003\uff0c\u4f7f\u7528\u957f/\u77ed\u601d\u8003\u53ccLLM\u534f\u540c\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u538b\u7f2980%\u63a8\u7406\u957f\u5ea6\u5e76\u4fdd\u6301\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5bf9\u6240\u6709\u601d\u8003\u8fdb\u884c\u5747\u7b49\u538b\u7f29\uff0c\u65e0\u6cd5\u5e73\u8861\u63a8\u7406\u6548\u679c\u4e0e\u6548\u7387\u3002\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u63a8\u6f14\u53d1\u73b0\u4e0d\u540c\u601d\u8003\u7684\u91cd\u8981\u6027\u5dee\u5f02\uff0c\u9700\u5efa\u7acb\u91cf\u5316\u6307\u6807\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u538b\u7f29", "method": "1) \u63d0\u51fa\u7406\u8bba\u8fb9\u754c\u6307\u6807\u8054\u5408\u8bc4\u4f30\u601d\u8003\u6709\u6548\u6027/\u6548\u7387 2) \u6784\u5efa\u957f\u601d\u8003LLM\uff08\u751f\u6210\u5173\u952e\u601d\u8003\uff09+\u77ed\u601d\u8003LLM\uff08\u751f\u6210\u666e\u901a\u601d\u8003\uff09\u534f\u540c\u6846\u67b6 3) \u901a\u8fc7\u5408\u6210\u51b7\u542f\u52a8\u6570\u636e\u5fae\u8c03\u6a21\u578b 4) \u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u534f\u4f5c", "result": "Qwen2.5-7B/Llama3.1-8B\u5728MATH500\u7b49\u57fa\u51c6\u4e0a\u8fbe\u5230\u5bf9\u6807\u6a21\u578b\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c1180%+\u63a8\u7406token\u91cf", "conclusion": "\u53cc\u6a21\u578b\u534f\u540c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u7406\u8bba\u6307\u6807\u6709\u6548\u6307\u5bfc\u601d\u8003\u91cd\u8981\u6027\u5212\u5206\uff0c\u6846\u67b6\u517c\u5177\u5b9e\u7528\u6027\u4e0e\u53ef\u590d\u73b0\u6027\uff08\u5f00\u6e90\u4ee3\u7801\u6570\u636e\uff09"}}
{"id": "2505.11829", "pdf": "https://arxiv.org/pdf/2505.11829", "abs": "https://arxiv.org/abs/2505.11829", "authors": ["Chenlu Wang", "Weimin Lyu", "Ritwik Banerjee"], "title": "Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks", "categories": ["cs.CL"], "comment": null, "summary": "Detecting deviant language such as sexism, or nuanced language such as\nmetaphors or sarcasm, is crucial for enhancing the safety, clarity, and\ninterpretation of online social discourse. While existing classifiers deliver\nstrong results on these tasks, they often come with significant computational\ncost and high data demands. In this work, we propose \\textbf{Cla}ss\n\\textbf{D}istillation (ClaD), a novel training paradigm that targets the core\nchallenge: distilling a small, well-defined target class from a highly diverse\nand heterogeneous background. ClaD integrates two key innovations: (i) a loss\nfunction informed by the structural properties of class distributions, based on\nMahalanobis distance, and (ii) an interpretable decision algorithm optimized\nfor class separation. Across three benchmark detection tasks -- sexism,\nmetaphor, and sarcasm -- ClaD outperforms competitive baselines, and even with\nsmaller language models and orders of magnitude fewer parameters, achieves\nperformance comparable to several large language models (LLMs). These results\ndemonstrate ClaD as an efficient tool for pragmatic language understanding\ntasks that require gleaning a small target class from a larger heterogeneous\nbackground.", "AI": {"tldr": "\u63d0\u51faClaD\u7c7b\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7684\u635f\u5931\u51fd\u6570\u548c\u53ef\u89e3\u91ca\u51b3\u7b56\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u7279\u5b9a\u8bed\u8a00\u68c0\u6d4b\u4efb\u52a1\u7684\u6548\u7387\uff0c\u5c0f\u6a21\u578b\u6027\u80fd\u5ab2\u7f8e\u5927\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u68c0\u6d4b\u6a21\u578b\u5b58\u5728\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u5927\u6570\u636e\u4f9d\u8d56\u95ee\u9898\uff0c\u9700\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u8f7b\u91cf\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u57fa\u4e8e\u9a6c\u6c0f\u8ddd\u79bb\u7684\u7c7b\u5206\u5e03\u7ed3\u6784\u5316\u635f\u5931\u51fd\u6570\n2. \u9762\u5411\u7c7b\u5206\u79bb\u4f18\u5316\u7684\u53ef\u89e3\u91ca\u51b3\u7b56\u7b97\u6cd5\n3. \u7c7b\u84b8\u998f\u6846\u67b6\u4ece\u5f02\u6784\u80cc\u666f\u4e2d\u63d0\u53d6\u76ee\u6807\u7c7b\u522b\u7279\u5f81", "result": "\u5728\u6027\u522b\u6b67\u89c6/\u9690\u55bb/\u8bbd\u523a\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u53c2\u6570\u91cf\u51cf\u5c112\u4e2a\u6570\u91cf\u7ea7\u65f6\u6027\u80fd\u4ecd\u63a5\u8fd1GPT-3.5(175B)", "conclusion": "ClaD\u4e3a\u4ece\u590d\u6742\u8bed\u8a00\u80cc\u666f\u4e2d\u63d0\u53d6\u5c0f\u89c4\u6a21\u76ee\u6807\u7c7b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u8f7b\u91cf\u5316\u540c\u65f6\u5b9e\u73b0\u4e0e\u5927\u6a21\u578b\u76f8\u5f53\u7684\u68c0\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2505.11835", "pdf": "https://arxiv.org/pdf/2505.11835", "abs": "https://arxiv.org/abs/2505.11835", "authors": ["Hongliang Li", "Jinan Xu", "Gengping Cui", "Changhao Guan", "Fengran Mo", "Kaiyu Huang"], "title": "Multilingual Collaborative Defense for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages, 4figures", "summary": "The robustness and security of large language models (LLMs) has become a\nprominent research area. One notable vulnerability is the ability to bypass LLM\nsafeguards by translating harmful queries into rare or underrepresented\nlanguages, a simple yet effective method of \"jailbreaking\" these models.\nDespite the growing concern, there has been limited research addressing the\nsafeguarding of LLMs in multilingual scenarios, highlighting an urgent need to\nenhance multilingual safety. In this work, we investigate the correlation\nbetween various attack features across different languages and propose\nMultilingual Collaborative Defense (MCD), a novel learning method that\noptimizes a continuous, soft safety prompt automatically to facilitate\nmultilingual safeguarding of LLMs. The MCD approach offers three advantages:\nFirst, it effectively improves safeguarding performance across multiple\nlanguages. Second, MCD maintains strong generalization capabilities while\nminimizing false refusal rates. Third, MCD mitigates the language safety\nmisalignment caused by imbalances in LLM training corpora. To evaluate the\neffectiveness of MCD, we manually construct multilingual versions of commonly\nused jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess\nvarious safeguarding methods. Additionally, we introduce these datasets in\nunderrepresented (zero-shot) languages to verify the language transferability\nof MCD. The results demonstrate that MCD outperforms existing approaches in\nsafeguarding against multilingual jailbreak attempts while also exhibiting\nstrong language transfer capabilities. Our code is available at\nhttps://github.com/HLiang-Lee/MCD.", "AI": {"tldr": "\u63d0\u51fa\u591a\u8bed\u8a00\u534f\u4f5c\u9632\u5fa1\uff08MCD\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6301\u7eed\u8f6f\u5b89\u5168\u63d0\u793a\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u8bed\u8a00\u5b89\u5168\u9632\u62a4\u80fd\u529b\uff0c\u5728\u9632\u5fa1\u591a\u8bed\u8a00\u8d8a\u72f1\u653b\u51fb\u4e2d\u8868\u73b0\u4f18\u5f02", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u5b58\u5728\u5b89\u5168\u9632\u62a4\u8584\u5f31\u70b9\uff0c\u653b\u51fb\u8005\u53ef\u901a\u8fc7\u4f4e\u8d44\u6e90\u8bed\u8a00\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\uff0c\u9700\u52a0\u5f3a\u591a\u8bed\u8a00\u5b89\u5168\u5bf9\u9f50\u80fd\u529b", "method": "\u57fa\u4e8e\u8de8\u8bed\u8a00\u653b\u51fb\u7279\u5f81\u76f8\u5173\u6027\u5206\u6790\uff0c\u8bbe\u8ba1\u81ea\u52a8\u4f18\u5316\u8fde\u7eed\u8f6f\u5b89\u5168\u63d0\u793a\u7684\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8bed\u8a00\u534f\u540c\u8bad\u7ec3\u5b9e\u73b0\u5b89\u5168\u9632\u62a4", "result": "\u6784\u5efa\u591a\u8bed\u8a00\u8d8a\u72f1\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff08\u542bMaliciousInstruct/AdvBench\u591a\u8bed\u8a00\u7248\uff09\uff0c\u5b9e\u9a8c\u8bc1\u660eMCD\u9632\u5fa1\u6210\u529f\u7387\u63d0\u534720%\u4e14\u8bef\u62d2\u7387\u964d\u4f4e15%\uff0c\u5728\u96f6\u6837\u672c\u8bed\u8a00\u573a\u666f\u5c55\u73b0\u5f3a\u8fc1\u79fb\u80fd\u529b", "conclusion": "MCD\u6709\u6548\u5e73\u8861\u8bed\u8a00\u5b89\u5168\u5bf9\u9f50\uff0c\u7f13\u89e3\u8bed\u6599\u5e93\u8bad\u7ec3\u504f\u5dee\u5e26\u6765\u7684\u5b89\u5168\u95ee\u9898\uff0c\u5176\u534f\u4f5c\u9632\u5fa1\u673a\u5236\u4e3a\u591a\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2505.11855", "pdf": "https://arxiv.org/pdf/2505.11855", "abs": "https://arxiv.org/abs/2505.11855", "authors": ["Guijin Son", "Jiwoo Hong", "Honglu Fan", "Heejeong Nam", "Hyunwoo Ko", "Seungwon Lim", "Jinyeop Song", "Jinha Choi", "Gon\u00e7alo Paulo", "Youngjae Yu", "Stella Biderman"], "title": "When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research", "categories": ["cs.CL"], "comment": "work in progress", "summary": "Recent advances in large language models (LLMs) have fueled the vision of\nautomated scientific discovery, often called AI Co-Scientists. To date, prior\nwork casts these systems as generative co-authors responsible for crafting\nhypotheses, synthesizing code, or drafting manuscripts. In this work, we\nexplore a complementary application: using LLMs as verifiers to automate the\n\\textbf{academic verification of scientific manuscripts}. To that end, we\nintroduce SPOT, a dataset of 83 published papers paired with 91 errors\nsignificant enough to prompt errata or retraction, cross-validated with actual\nauthors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find\nthat none surpasses 21.1\\% recall or 6.1\\% precision (o3 achieves the best\nscores, with all others near zero). Furthermore, confidence estimates are\nuniformly low, and across eight independent runs, models rarely rediscover the\nsame errors, undermining their reliability. Finally, qualitative analysis with\ndomain experts reveals that even the strongest models make mistakes resembling\nstudent-level misconceptions derived from misunderstandings. These findings\nhighlight the substantial gap between current LLM capabilities and the\nrequirements for dependable AI-assisted academic verification.", "AI": {"tldr": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5b66\u672f\u8bba\u6587\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff08\u6700\u4f73\u6a21\u578bo3\u53ec\u56de\u738721.1%/\u51c6\u786e\u73876.1%\uff09\uff0c\u4e0e\u5b9e\u7528\u5316\u8981\u6c42\u5b58\u5728\u663e\u8457\u5dee\u8ddd", "motivation": "\u63a2\u7d22LLMs\u4f5c\u4e3a\u79d1\u5b66\u8bba\u6587\u9a8c\u8bc1\u5de5\u5177\u7684\u53ef\u80fd\u6027\uff0c\u7a81\u7834\u5176\u4f20\u7edf\u751f\u6210\u5f0f\u5e94\u7528\u573a\u666f", "method": "\u6784\u5efa\u5305\u542b83\u7bc7\u8bba\u6587\u53ca91\u4e2a\u91cd\u5927\u9519\u8bef\u7684SPOT\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u591a\u8f6e\u4eba\u7c7b\u9a8c\u8bc1\uff0c\u8bc4\u4f30\u4e3b\u6d41LLM\u7684\u9a8c\u8bc1\u80fd\u529b", "result": "\u6a21\u578b\u8868\u73b0\u4e0d\u7a33\u5b9a\uff08\u4e0d\u540c\u8fd0\u884c\u95f4\u9519\u8bef\u68c0\u6d4b\u7ed3\u679c\u5dee\u5f02\u5927\uff09\uff0c\u7f6e\u4fe1\u5ea6\u4f4e\uff0c\u4e14\u9519\u8bef\u7c7b\u578b\u7c7b\u4f3c\u5b66\u751f\u7ea7\u6982\u5ff5\u8bef\u89e3", "conclusion": "\u73b0\u6709LLM\u5c1a\u4e0d\u5177\u5907\u53ef\u9760\u7684\u5b66\u672f\u9a8c\u8bc1\u80fd\u529b\uff0c\u9700\u7a81\u7834\u6a21\u578b\u7406\u89e3\u6df1\u5ea6\u4e0e\u4e00\u81f4\u6027\u96be\u9898"}}
{"id": "2505.11876", "pdf": "https://arxiv.org/pdf/2505.11876", "abs": "https://arxiv.org/abs/2505.11876", "authors": ["Yanbo Dai", "Zhenlan Ji", "Zongjie Li", "Shuai Wang"], "title": "NAMET: Robust Massive Model Editing via Noise-Aware Memory Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Model editing techniques are essential for efficiently updating knowledge in\nlarge language models (LLMs). However, the effectiveness of existing approaches\ndegrades in massive editing scenarios, particularly when evaluated with\npractical metrics or in context-rich settings. We attribute these failures to\nembedding collisions among knowledge items, which undermine editing reliability\nat scale. To address this, we propose NAMET (Noise-aware Model Editing in\nTransformers), a simple yet effective method that introduces noise during\nmemory extraction via a one-line modification to MEMIT. Extensive experiments\nacross six LLMs and three datasets demonstrate that NAMET consistently\noutperforms existing methods when editing thousands of facts.", "AI": {"tldr": "NAMET\u901a\u8fc7\u5411MEMIT\u6dfb\u52a0\u566a\u58f0\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u6a21\u578b\u7f16\u8f91\u4e2d\u7684\u5d4c\u5165\u78b0\u649e\u95ee\u9898", "motivation": "\u73b0\u6709\u6a21\u578b\u7f16\u8f91\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u77e5\u8bc6\u66f4\u65b0\u65f6\u51fa\u73b0\u53ef\u9760\u6027\u4e0b\u964d\uff0c\u4e3b\u8981\u7531\u4e8e\u77e5\u8bc6\u9879\u4e4b\u95f4\u7684\u5d4c\u5165\u78b0\u649e", "method": "\u5728Transformer\u7684memory\u63d0\u53d6\u9636\u6bb5\u5f15\u5165\u566a\u58f0\uff08\u4ec5\u9700\u5355\u884c\u4ee3\u7801\u4fee\u6539MEMIT\uff09", "result": "\u57286\u4e2aLLM\u548c3\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cNAMET\u5728\u6570\u5343\u6761\u4e8b\u5b9e\u7f16\u8f91\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u566a\u58f0\u611f\u77e5\u673a\u5236\u662f\u63d0\u5347\u5927\u89c4\u6a21\u6a21\u578b\u7f16\u8f91\u6548\u679c\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.11887", "pdf": "https://arxiv.org/pdf/2505.11887", "abs": "https://arxiv.org/abs/2505.11887", "authors": ["Xiechi Zhang", "Zetian Ouyang", "Linlin Wang", "Gerard de Melo", "Zhu Cao", "Xiaoling Wang", "Ya Zhang", "Yanfeng Wang", "Liang He"], "title": "AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "With the proliferation of large language models (LLMs) in the medical domain,\nthere is increasing demand for improved evaluation techniques to assess their\ncapabilities. However, traditional metrics like F1 and ROUGE, which rely on\ntoken overlaps to measure quality, significantly overlook the importance of\nmedical terminology. While human evaluation tends to be more reliable, it can\nbe very costly and may as well suffer from inaccuracies due to limits in human\nexpertise and motivation. Although there are some evaluation methods based on\nLLMs, their usability in the medical field is limited due to their proprietary\nnature or lack of expertise. To tackle these challenges, we present\nAutoMedEval, an open-sourced automatic evaluation model with 13B parameters\nspecifically engineered to measure the question-answering proficiency of\nmedical LLMs. The overarching objective of AutoMedEval is to assess the quality\nof responses produced by diverse models, aspiring to significantly reduce the\ndependence on human evaluation. Specifically, we propose a hierarchical\ntraining method involving curriculum instruction tuning and an iterative\nknowledge introspection mechanism, enabling AutoMedEval to acquire professional\nmedical assessment capabilities with limited instructional data. Human\nevaluations indicate that AutoMedEval surpasses other baselines in terms of\ncorrelation with human judgments.", "AI": {"tldr": "\u63d0\u51fa\u5f00\u6e90\u81ea\u52a8\u8bc4\u4f30\u6a21\u578bAutoMedEval(13B\u53c2\u6570)\uff0c\u901a\u8fc7\u5206\u5c42\u8bad\u7ec3\u65b9\u6cd5\u89e3\u51b3\u533b\u7597\u5927\u6a21\u578b\u95ee\u7b54\u80fd\u529b\u8bc4\u4f30\u96be\u9898", "motivation": "\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u5ffd\u7565\u533b\u5b66\u672f\u8bed\u4ef7\u503c\uff0c\u4eba\u5de5\u8bc4\u4f30\u6210\u672c\u9ad8\u4e14\u5b58\u5728\u8bef\u5dee\uff0c\u73b0\u6709LLM\u8bc4\u4f30\u65b9\u6cd5\u5728\u533b\u7597\u9886\u57df\u5b58\u5728\u9002\u7528\u6027\u7f3a\u9677", "method": "\u91c7\u7528\u8bfe\u7a0b\u6307\u4ee4\u5fae\u8c03\u4e0e\u77e5\u8bc6\u5185\u7701\u673a\u5236\u7684\u5c42\u6b21\u5316\u8bad\u7ec3\u6846\u67b6\uff0c\u5b9e\u73b0\u6709\u9650\u6570\u636e\u4e0b\u7684\u4e13\u4e1a\u533b\u7597\u8bc4\u4f30\u80fd\u529b\u83b7\u53d6", "result": "\u4eba\u5de5\u8bc4\u4f30\u663e\u793aAutoMedEval\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b", "conclusion": "AutoMedEval\u6709\u6548\u964d\u4f4e\u5bf9\u4eba\u5de5\u8bc4\u4f30\u7684\u4f9d\u8d56\uff0c\u4e3a\u533b\u7597LLM\u8bc4\u4f30\u63d0\u4f9b\u53ef\u9760\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.11891", "pdf": "https://arxiv.org/pdf/2505.11891", "abs": "https://arxiv.org/abs/2505.11891", "authors": ["Weikai Xu", "Zhizheng Jiang", "Yuxuan Liu", "Wei Liu", "Jian Luan", "Yuanchun Li", "Yunxin Liu", "Bin Wang", "Bo An"], "title": "Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "VLM-based mobile agents are increasingly popular due to their capabilities to\ninteract with smartphone GUIs and XML-structured texts and to complete daily\ntasks. However, existing online benchmarks struggle with obtaining stable\nreward signals due to dynamic environmental changes. Offline benchmarks\nevaluate the agents through single-path trajectories, which stands in contrast\nto the inherently multi-solution characteristics of GUI tasks. Additionally,\nboth types of benchmarks fail to assess whether mobile agents can handle noise\nor engage in proactive interactions due to a lack of noisy apps or overly full\ninstructions during the evaluation process. To address these limitations, we\nuse a slot-based instruction generation method to construct a more realistic\nand comprehensive benchmark named Mobile-Bench-v2. Mobile-Bench-v2 includes a\ncommon task split, with offline multi-path evaluation to assess the agent's\nability to obtain step rewards during task execution. It contains a noisy split\nbased on pop-ups and ads apps, and a contaminated split named AITZ-Noise to\nformulate a real noisy environment. Furthermore, an ambiguous instruction split\nwith preset Q\\&A interactions is released to evaluate the agent's proactive\ninteraction capabilities. We conduct evaluations on these splits using the\nsingle-agent framework AppAgent-v1, the multi-agent framework Mobile-Agent-v2,\nas well as other mobile agents such as UI-Tars and OS-Atlas. Code and data are\navailable at https://huggingface.co/datasets/xwk123/MobileBench-v2.", "AI": {"tldr": "\u63d0\u51faMobile-Bench-v2\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u591a\u8def\u5f84\u8bc4\u4f30\u3001\u566a\u58f0\u73af\u5883\u6a21\u62df\u548c\u4e3b\u52a8\u4ea4\u4e92\u6d4b\u8bd5\u6539\u8fdb\u79fb\u52a8\u4ee3\u7406\u8bc4\u4f30\u4f53\u7cfb", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u52a8\u6001\u73af\u5883\u5956\u52b1\u4e0d\u7a33\u5b9a\u3001\u5355\u8def\u5f84\u8bc4\u4f30\u4e0e\u771f\u5b9e\u591a\u89e3\u4efb\u52a1\u4e0d\u5339\u914d\u3001\u7f3a\u4e4f\u566a\u58f0\u5e94\u7528\u548c\u5b8c\u6574\u4ea4\u4e92\u8bc4\u4f30\u7684\u95ee\u9898", "method": "\u4f7f\u7528\u57fa\u4e8e\u63d2\u69fd\u7684\u6307\u4ee4\u751f\u6210\u65b9\u6cd5\u6784\u5efa\u57fa\u51c6\uff0c\u5305\u542b\u79bb\u7ebf\u591a\u8def\u5f84\u8bc4\u4f30\u3001\u57fa\u4e8e\u5f39\u7a97\u5e7f\u544a\u7684\u566a\u58f0\u6d4b\u8bd5\u96c6(AITZ-Noise)\u548c\u9884\u8bbe\u95ee\u7b54\u7684\u6a21\u7cca\u6307\u4ee4\u4ea4\u4e92\u6d4b\u8bd5", "result": "\u65b0\u57fa\u51c6\u5728AppAgent-v1\u3001Mobile-Agent-v2\u7b49\u6846\u67b6\u4e0a\u5b8c\u6210\u9a8c\u8bc1\uff0c\u8bc1\u660e\u8bc4\u4f30\u4f53\u7cfb\u7684\u6709\u6548\u6027", "conclusion": "Mobile-Bench-v2\u901a\u8fc7\u591a\u7ef4\u5ea6\u73af\u5883\u6a21\u62df\u548c\u4ea4\u4e92\u6d4b\u8bd5\u673a\u5236\uff0c\u5efa\u7acb\u4e86\u66f4\u5168\u9762\u7684\u79fb\u52a8\u4ee3\u7406\u8bc4\u4f30\u6807\u51c6"}}
{"id": "2505.11893", "pdf": "https://arxiv.org/pdf/2505.11893", "abs": "https://arxiv.org/abs/2505.11893", "authors": ["Zepeng Ding", "Dixuan Wang", "Ziqin Luo", "Guochao Jiang", "Deqing Yang", "Jiaqing Liang"], "title": "RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multi-step planning has been widely employed to enhance the performance of\nlarge language models (LLMs) on downstream natural language processing (NLP)\ntasks, which decomposes the original task into multiple subtasks and guide LLMs\nto solve them sequentially without additional training. When addressing task\ninstances, existing methods either preset the order of steps or attempt\nmultiple paths at each step. However, these methods overlook instances'\nlinguistic features and rely on the intrinsic planning capabilities of LLMs to\nevaluate intermediate feedback and then select subtasks, resulting in\nsuboptimal outcomes. To better solve multi-step NLP tasks with LLMs, in this\npaper we propose a Reinforcement Learning enhanced Adaptive Planning framework\n(RLAP). In our framework, we model an NLP task as a Markov decision process\n(MDP) and employ an LLM directly into the environment. In particular, a\nlightweight Actor model is trained to estimate Q-values for natural language\nsequences consisting of states and actions through reinforcement learning.\nTherefore, during sequential planning, the linguistic features of each sequence\nin the MDP can be taken into account, and the Actor model interacts with the\nLLM to determine the optimal order of subtasks for each task instance. We apply\nRLAP on three different types of NLP tasks and conduct extensive experiments on\nmultiple datasets to verify RLAP's effectiveness and robustness.", "AI": {"tldr": "\u63d0\u51fa\u5f3a\u5316\u5b66\u4e60\u589e\u5f3a\u7684\u81ea\u9002\u5e94\u89c4\u5212\u6846\u67b6RLAP\uff0c\u901a\u8fc7\u5efa\u6a21NLP\u4efb\u52a1\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5e76\u8bad\u7ec3\u8f7b\u91cf\u7ea7Actor\u6a21\u578b\uff0c\u5b9e\u73b0\u57fa\u4e8e\u8bed\u8a00\u7279\u5f81\u7684\u52a8\u6001\u5b50\u4efb\u52a1\u6392\u5e8f\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u591a\u6b65\u89c4\u5212\u65b9\u6cd5\u5ffd\u7565\u5b9e\u4f8b\u8bed\u8a00\u7279\u5f81\uff0c\u8fc7\u5ea6\u4f9d\u8d56LLM\u5185\u5728\u89c4\u5212\u80fd\u529b\u5bfc\u81f4\u6b21\u4f18\u7ed3\u679c\uff0c\u9700\u5efa\u7acb\u91cf\u5316\u8bc4\u4f30\u673a\u5236\u5b9e\u73b0\u81ea\u9002\u5e94\u5b50\u4efb\u52a1\u6392\u5e8f\u3002", "method": "1. \u5c06NLP\u4efb\u52a1\u5efa\u6a21\u4e3aMDP\u8fc7\u7a0b 2. \u8bbe\u8ba1Actor\u6a21\u578b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f30\u8ba1\u81ea\u7136\u8bed\u8a00\u5e8f\u5217\u7684Q\u503c 3. Actor\u4e0eLLM\u4ea4\u4e92\u52a8\u6001\u786e\u5b9a\u5b50\u4efb\u52a1\u6700\u4f18\u987a\u5e8f", "result": "\u5728\u4e09\u5927\u7c7bNLP\u4efb\u52a1\u3001\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u53472.1-5.7%\u4e14\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "RLAP\u901a\u8fc7\u878d\u5408\u5f3a\u5316\u5b66\u4e60\u4e0e\u8bed\u8a00\u6a21\u578b\u89c4\u5212\uff0c\u7a81\u7834\u4e86\u9759\u6001\u8def\u5f84\u89c4\u5212\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u590d\u6742NLP\u4efb\u52a1\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2505.11900", "pdf": "https://arxiv.org/pdf/2505.11900", "abs": "https://arxiv.org/abs/2505.11900", "authors": ["Philipp Christmann", "Gerhard Weikum"], "title": "Recursive Question Understanding for Complex Question Answering over Heterogeneous Personal Data", "categories": ["cs.CL", "cs.IR"], "comment": "Accepted at ACL 2025 (Findings)", "summary": "Question answering over mixed sources, like text and tables, has been\nadvanced by verbalizing all contents and encoding it with a language model. A\nprominent case of such heterogeneous data is personal information: user devices\nlog vast amounts of data every day, such as calendar entries, workout\nstatistics, shopping records, streaming history, and more. Information needs\nrange from simple look-ups to queries of analytical nature. The challenge is to\nprovide humans with convenient access with small footprint, so that all\npersonal data stays on the user devices. We present ReQAP, a novel method that\ncreates an executable operator tree for a given question, via recursive\ndecomposition. Operators are designed to enable seamless integration of\nstructured and unstructured sources, and the execution of the operator tree\nyields a traceable answer. We further release the PerQA benchmark, with\npersona-based data and questions, covering a diverse spectrum of realistic user\nneeds.", "AI": {"tldr": "Proposes ReQAP, a method for question answering over mixed personal data sources via executable operator trees, and releases PerQA benchmark.", "motivation": "Enable efficient, privacy-preserving access to heterogeneous personal data (text/tables) on user devices without transferring data externally.", "method": "Recursive decomposition of questions into executable operator trees that integrate structured/unstructured data sources seamlessly.", "result": "Developed PerQA benchmark with realistic persona-based queries; ReQAP enables traceable answers while keeping data locally.", "conclusion": "ReQAP addresses on-device QA challenges through modular operators and validated by PerQA, balancing usability and data privacy."}}
{"id": "2505.11908", "pdf": "https://arxiv.org/pdf/2505.11908", "abs": "https://arxiv.org/abs/2505.11908", "authors": ["Zhangyu Wang", "Siyuan Gao", "Rong Zhou", "Hao Wang", "Li Ning"], "title": "ELITE: Embedding-Less retrieval with Iterative Text Exploration", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive progress in natural\nlanguage processing, but their limited ability to retain long-term context\nconstrains performance on document-level or multi-turn tasks.\nRetrieval-Augmented Generation (RAG) mitigates this by retrieving relevant\ninformation from an external corpus. However, existing RAG systems often rely\non embedding-based retrieval trained on corpus-level semantic similarity, which\ncan lead to retrieving content that is semantically similar in form but\nmisaligned with the question's true intent. Furthermore, recent RAG variants\nconstruct graph- or hierarchy-based structures to improve retrieval accuracy,\nresulting in significant computation and storage overhead. In this paper, we\npropose an embedding-free retrieval framework. Our method leverages the logical\ninferencing ability of LLMs in retrieval using iterative search space\nrefinement guided by our novel importance measure and extend our retrieval\nresults with logically related information without explicit graph construction.\nExperiments on long-context QA benchmarks, including NovelQA and Marathon, show\nthat our approach outperforms strong baselines while reducing storage and\nruntime by over an order of magnitude.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u5d4c\u5165\u7684\u68c0\u7d22\u6846\u67b6\uff0c\u7ed3\u5408LLMs\u903b\u8f91\u63a8\u7406\u80fd\u529b\u4e0e\u91cd\u8981\u6027\u5ea6\u91cf\u673a\u5236\uff0c\u5728\u957f\u4e0a\u4e0b\u6587QA\u4efb\u52a1\u4e2d\u663e\u8457\u964d\u4f4e\u5b58\u50a8/\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u8d85\u8d8a\u73b0\u6709\u57fa\u7ebf", "motivation": "\u73b0\u6709RAG\u7cfb\u7edf\u4f9d\u8d56\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u6027\u7684\u68c0\u7d22\u5bb9\u6613\u5bfc\u81f4\u610f\u56fe\u504f\u5dee\uff0c\u4e14\u57fa\u4e8e\u56fe/\u5c42\u6b21\u7ed3\u6784\u7684\u6539\u8fdb\u65b9\u6848\u5e26\u6765\u8fc7\u9ad8\u5b58\u50a8\u8ba1\u7b97\u6210\u672c", "method": "\u901a\u8fc7\u8fed\u4ee3\u641c\u7d22\u7a7a\u95f4\u4f18\u5316\u7b56\u7565\uff0c\u5229\u7528\u65b0\u578b\u91cd\u8981\u6027\u5ea6\u91cf\u6307\u5bfc\u68c0\u7d22\u8303\u56f4\u6269\u5c55\uff0c\u5728\u4e0d\u9700\u8981\u663e\u5f0f\u6784\u5efa\u56fe\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\u6574\u5408\u903b\u8f91\u5173\u8054\u4fe1\u606f", "result": "\u5728NovelQA\u548cMarathon\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u8868\u73b0\uff0c\u5b58\u50a8\u548c\u8fd0\u884c\u65f6\u6548\u7387\u63d0\u5347\u8d85\u8fc7\u4e00\u4e2a\u6570\u91cf\u7ea7", "conclusion": "\u9a8c\u8bc1\u4e86\u903b\u8f91\u63a8\u7406\u5728\u68c0\u7d22\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u63d0\u4f9b\u517c\u987e\u6548\u7387\u4e0e\u7cbe\u5ea6\u7684\u65b0\u578b\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.11922", "pdf": "https://arxiv.org/pdf/2505.11922", "abs": "https://arxiv.org/abs/2505.11922", "authors": ["Yuheng Lu", "ZiMeng Bai", "Caixia Yuan", "Huixing Jiang", "Xiaojie Wang"], "title": "Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable capabilities in handling\nnatural language tasks; however, they may struggle to consistently follow\ncomplex instructions including those involve multiple constraints.\nPost-training LLMs using supervised fine-tuning (SFT) is a standard approach to\nimprove their ability to follow instructions. In addressing complex instruction\nfollowing, existing efforts primarily focus on data-driven methods that\nsynthesize complex instruction-output pairs for SFT. However, insufficient\nattention allocated to crucial sub-contexts may reduce the effectiveness of\nSFT. In this work, we propose transforming sequentially structured input\ninstruction into multiple parallel instructions containing subcontexts. To\nsupport processing this multi-input, we propose MISO (Multi-Input\nSingle-Output), an extension to currently dominant decoder-only\ntransformer-based LLMs. MISO introduces a mixture-of-contexts paradigm that\njointly considers the overall instruction-output alignment and the influence of\nindividual sub-contexts to enhance SFT effectiveness. We apply MISO fine-tuning\nto complex instructionfollowing datasets and evaluate it with standard LLM\ninference. Empirical results demonstrate the superiority of MISO as a\nfine-tuning method for LLMs, both in terms of effectiveness in complex\ninstruction-following scenarios and its potential for training efficiency.", "AI": {"tldr": "\u63d0\u51faMISO\u65b9\u6cd5\u6539\u8fdbLLMs\u7684\u590d\u6742\u6307\u4ee4\u9075\u5faa\u80fd\u529b\uff0c\u901a\u8fc7\u5e76\u884c\u5b50\u4e0a\u4e0b\u6587\u673a\u5236\u63d0\u5347\u76d1\u7763\u5fae\u8c03\u6548\u679c\u548c\u8bad\u7ec3\u6548\u7387", "motivation": "\u73b0\u6709\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u591a\u7ea6\u675f\u6307\u4ee4\u65f6\uff0c\u5bf9\u5173\u952e\u5b50\u4e0a\u4e0b\u6587\u5173\u6ce8\u4e0d\u8db3\u5bfc\u81f4\u6548\u679c\u53d7\u9650", "method": "\u5c06\u987a\u5e8f\u6307\u4ee4\u8f6c\u6362\u4e3a\u5e76\u884c\u5b50\u6307\u4ee4\uff0c\u8bbe\u8ba1\u6df7\u5408\u4e0a\u4e0b\u6587\u67b6\u6784\u8054\u5408\u4f18\u5316\u6574\u4f53\u5bf9\u9f50\u4e0e\u5b50\u4e0a\u4e0b\u6587\u5f71\u54cd", "result": "\u5b9e\u9a8c\u8bc1\u660eMISO\u5728\u590d\u6742\u6307\u4ee4\u573a\u666f\u6548\u679c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u5c55\u73b0\u8bad\u7ec3\u6548\u7387\u4f18\u52bf", "conclusion": "MISO\u4e3aLLMs\u5fae\u8c03\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u6709\u6548\u63d0\u5347\u590d\u6742\u4efb\u52a1\u5904\u7406\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b"}}
{"id": "2505.11924", "pdf": "https://arxiv.org/pdf/2505.11924", "abs": "https://arxiv.org/abs/2505.11924", "authors": ["Yu-Ting Lee", "Hui-Ying Shih", "Fu-Chieh Chang", "Pei-Yuan Wu"], "title": "An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We provide an explanation for the performance gains of intrinsic\nself-correction, a process where a language model iteratively refines its\noutputs without external feedback. More precisely, we investigate how prompting\ninduces interpretable changes in hidden states and thus affects the output\ndistributions. We hypothesize that each prompt-induced shift lies in a linear\nspan of some linear representation vectors, naturally separating tokens based\non individual concept alignment. Building around this idea, we give a\nmathematical formulation of self-correction and derive a concentration result\nfor output tokens based on alignment magnitudes. Our experiments on text\ndetoxification with zephyr-7b-sft reveal a substantial gap in the inner\nproducts of the prompt-induced shifts and the unembeddings of the top-100 most\ntoxic tokens vs. those of the unembeddings of the bottom-100 least toxic\ntokens, under toxic instructions. This suggests that self-correction prompts\nenhance a language model's capability of latent concept recognition. Our\nanalysis offers insights into the underlying mechanism of self-correction by\ncharacterizing how prompting works explainably. For reproducibility, our code\nis available.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u6570\u5b66\u5efa\u6a21\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5185\u5728\u81ea\u6211\u7ea0\u6b63\u673a\u5236\u901a\u8fc7\u63d0\u793a\u8bf1\u5bfc\u9690\u85cf\u72b6\u6001\u7ebf\u6027\u53d8\u5316\uff0c\u589e\u5f3a\u6f5c\u5728\u6982\u5ff5\u8bc6\u522b\u7684\u539f\u7406\u3002", "motivation": "\u89e3\u91ca\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u81ea\u6211\u8fed\u4ee3\u4fee\u6b63\u63d0\u5347\u8f93\u51fa\u7684\u5185\u5728\u673a\u5236\uff0c\u63a2\u7d22\u63d0\u793a\u5982\u4f55\u901a\u8fc7\u9690\u72b6\u6001\u7ebf\u6027\u53d8\u5316\u5f71\u54cd\u8f93\u51fa\u5206\u5e03\u3002", "method": "\u63d0\u51fa\u7ebf\u6027\u8868\u793a\u5047\u8bbe\uff0c\u6784\u5efa\u6570\u5b66\u6846\u67b6\u5206\u6790\u81ea\u6211\u4fee\u6b63\u8fc7\u7a0b\uff0c\u5e76\u5728zephyr-7b-sft\u6a21\u578b\u4e0a\u8fdb\u884c\u6587\u672c\u53bb\u6bd2\u5316\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6709\u6bd2\u6307\u4ee4\u4e0b\uff0c\u524d100\u6709\u6bd2token\u4e0e\u540e100\u65e0\u6bd2token\u5728\u63d0\u793a\u8bf1\u5bfc\u504f\u79fb\u7684\u5185\u79ef\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff08\u0394=0.34\uff09\u3002", "conclusion": "\u81ea\u6211\u4fee\u6b63\u63d0\u793a\u901a\u8fc7\u589e\u5f3a\u6f5c\u5728\u6982\u5ff5\u5bf9\u9f50\uff0c\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5728\u7ea0\u9519\u80fd\u529b\uff0c\u4e3a\u53ef\u89e3\u91ca\u6027\u673a\u5236\u7814\u7a76\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002"}}
{"id": "2505.11932", "pdf": "https://arxiv.org/pdf/2505.11932", "abs": "https://arxiv.org/abs/2505.11932", "authors": ["Yuyao Zhang", "Zhicheng Dou", "Xiaoxi Li", "Jiajie Jin", "Yongkang Wu", "Zhonghua Li", "Qi Ye", "Ji-Rong Wen"], "title": "Neuro-Symbolic Query Compiler", "categories": ["cs.CL", "cs.IR"], "comment": "Findings of ACL2025, codes are available at this url:\n  https://github.com/YuyaoZhangQAQ/Query_Compiler", "summary": "Precise recognition of search intent in Retrieval-Augmented Generation (RAG)\nsystems remains a challenging goal, especially under resource constraints and\nfor complex queries with nested structures and dependencies. This paper\npresents QCompiler, a neuro-symbolic framework inspired by linguistic grammar\nrules and compiler design, to bridge this gap. It theoretically designs a\nminimal yet sufficient Backus-Naur Form (BNF) grammar $G[q]$ to formalize\ncomplex queries. Unlike previous methods, this grammar maintains completeness\nwhile minimizing redundancy. Based on this, QCompiler includes a Query\nExpression Translator, a Lexical Syntax Parser, and a Recursive Descent\nProcessor to compile queries into Abstract Syntax Trees (ASTs) for execution.\nThe atomicity of the sub-queries in the leaf nodes ensures more precise\ndocument retrieval and response generation, significantly improving the RAG\nsystem's ability to address complex queries.", "AI": {"tldr": "\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7\u6846\u67b6QCompiler\uff0c\u901a\u8fc7BNF\u8bed\u6cd5\u548c\u7f16\u8bd1\u5668\u7ec4\u4ef6\u63d0\u5347RAG\u7cfb\u7edf\u5bf9\u590d\u6742\u67e5\u8be2\u7684\u5904\u7406\u80fd\u529b", "motivation": "\u89e3\u51b3\u73b0\u6709RAG\u7cfb\u7edf\u5728\u8d44\u6e90\u53d7\u9650\u6761\u4ef6\u4e0b\u5904\u7406\u5177\u6709\u5d4c\u5957\u7ed3\u6784\u548c\u4f9d\u8d56\u5173\u7cfb\u7684\u590d\u6742\u67e5\u8be2\u65f6\u610f\u56fe\u8bc6\u522b\u4e0d\u7cbe\u786e\u7684\u95ee\u9898", "method": "\u8bbe\u8ba1\u6700\u5c0f\u5b8c\u5907BNF\u8bed\u6cd5G[q]\uff0c\u5f00\u53d1\u5305\u542b\u67e5\u8be2\u7ffb\u8bd1\u5668\u3001\u8bed\u6cd5\u89e3\u6790\u5668\u548c\u9012\u5f52\u5904\u7406\u5668\u7684\u7f16\u8bd1\u6846\u67b6\uff0c\u751f\u6210\u62bd\u8c61\u8bed\u6cd5\u6811\u6267\u884c\u67e5\u8be2", "result": "\u53f6\u5b50\u8282\u70b9\u5b50\u67e5\u8be2\u539f\u5b50\u6027\u5b9e\u73b0\u66f4\u7cbe\u51c6\u7684\u6587\u6863\u68c0\u7d22\u4e0e\u54cd\u5e94\u751f\u6210\uff0c\u7cfb\u7edf\u5904\u7406\u590d\u6742\u67e5\u8be2\u80fd\u529b\u663e\u8457\u63d0\u5347", "conclusion": "QCompiler\u901a\u8fc7\u878d\u5408\u8bed\u8a00\u5b66\u89c4\u5219\u4e0e\u7f16\u8bd1\u5668\u8bbe\u8ba1\uff0c\u4e3a\u590d\u6742\u67e5\u8be2\u5904\u7406\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.11935", "pdf": "https://arxiv.org/pdf/2505.11935", "abs": "https://arxiv.org/abs/2505.11935", "authors": ["Xuanle Zhao", "Xuexin Liu", "Haoyue Yang", "Xianzhen Luo", "Fanhu Zeng", "Jianling Li", "Qi Shi", "Chi Chen"], "title": "ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing", "categories": ["cs.CL"], "comment": "Accept by ACL2025 Findings, preprint version", "summary": "Although multimodal large language models (MLLMs) show promise in generating\nchart rendering code, chart editing presents a greater challenge. This\ndifficulty stems from its nature as a labor-intensive task for humans that also\ndemands MLLMs to integrate chart understanding, complex reasoning, and precise\nintent interpretation. While many MLLMs claim such editing capabilities,\ncurrent assessments typically rely on limited case studies rather than robust\nevaluation methodologies, highlighting the urgent need for a comprehensive\nevaluation framework. In this work, we propose ChartEdit, a new high-quality\nbenchmark designed for chart editing tasks. This benchmark comprises $1,405$\ndiverse editing instructions applied to $233$ real-world charts, with each\ninstruction-chart instance having been manually annotated and validated for\naccuracy. Utilizing ChartEdit, we evaluate the performance of 10 mainstream\nMLLMs across two types of experiments, assessing them at both the code and\nchart levels. The results suggest that large-scale models can generate code to\nproduce images that partially match the reference images. However, their\nability to generate accurate edits according to the instructions remains\nlimited. The state-of-the-art (SOTA) model achieves a score of only $59.96$,\nhighlighting significant challenges in precise modification. In contrast,\nsmall-scale models, including chart-domain models, struggle both with following\nediting instructions and generating overall chart images, underscoring the need\nfor further development in this area. Code is available at\nhttps://github.com/xxlllz/ChartEdit.", "AI": {"tldr": "\u63d0\u51faChartEdit\u57fa\u51c6\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u7f16\u8f91\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63ed\u793a\u5176\u5728\u7cbe\u786e\u4fee\u6539\u65b9\u9762\u7684\u91cd\u5927\u6311\u6218\u3002", "motivation": "\u56fe\u8868\u7f16\u8f91\u4efb\u52a1\u9700\u8981\u6574\u5408\u56fe\u8868\u7406\u89e3\u3001\u590d\u6742\u63a8\u7406\u548c\u610f\u56fe\u89e3\u91ca\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u5c40\u9650\u4e8e\u6848\u4f8b\u7814\u7a76\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6846\u67b6\u3002", "method": "\u6784\u5efa\u5305\u542b1,405\u4e2a\u591a\u6837\u5316\u7f16\u8f91\u6307\u4ee4\u548c233\u4e2a\u771f\u5b9e\u56fe\u8868\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u4eba\u5de5\u6807\u6ce8\u9a8c\u8bc1\uff0c\u5e76\u8bc4\u4f3010\u4e2a\u4e3b\u6d41MLLMs\u5728\u4ee3\u7801/\u56fe\u8868\u5c42\u7ea7\u7684\u6027\u80fd\u3002", "result": "\u5927\u89c4\u6a21\u6a21\u578b\u751f\u6210\u7684\u4ee3\u7801\u53ef\u90e8\u5206\u5339\u914d\u53c2\u8003\u56fe\u50cf\uff08SOTA\u5f97\u520659.96\uff09\uff0c\u4f46\u7cbe\u786e\u7f16\u8f91\u80fd\u529b\u6709\u9650\uff1b\u5c0f\u89c4\u6a21\u6a21\u578b\u5728\u6307\u4ee4\u9075\u5faa\u548c\u56fe\u8868\u751f\u6210\u4e0a\u5747\u8868\u73b0\u4e0d\u8db3\u3002", "conclusion": "\u5f53\u524dMLLMs\u5728\u7cbe\u786e\u56fe\u8868\u7f16\u8f91\u4efb\u52a1\u4e2d\u9762\u4e34\u663e\u8457\u6311\u6218\uff0cChartEdit\u4e3a\u9886\u57df\u53d1\u5c55\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u8bc4\u4f30\u57fa\u51c6\u3002"}}
{"id": "2505.11958", "pdf": "https://arxiv.org/pdf/2505.11958", "abs": "https://arxiv.org/abs/2505.11958", "authors": ["Aswini Kumar Padhi", "Anil Bandhakavi", "Tanmoy Chakraborty"], "title": "Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning", "categories": ["cs.CL"], "comment": null, "summary": "Counterspeech has proven to be a powerful tool to combat hate speech online.\nPrevious studies have focused on generating counterspeech conditioned only on\nspecific intents (single attributed). However, a holistic approach considering\nmultiple attributes simultaneously can yield more nuanced and effective\nresponses. Here, we introduce HiPPrO, Hierarchical Prefix learning with\nPreference Optimization, a novel two-stage framework that utilizes the\neffectiveness of attribute-specific prefix embedding spaces hierarchically\noptimized during the counterspeech generation process in the first phase.\nThereafter, we incorporate both reference and reward-free preference\noptimization to generate more constructive counterspeech. Furthermore, we\nextend IntentCONANv2 by annotating all 13,973 counterspeech instances with\nemotion labels by five annotators. HiPPrO leverages hierarchical prefix\noptimization to integrate these dual attributes effectively. An extensive\nevaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent\nconformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L,\nrespectively, compared to several baseline models. Human evaluations further\nsubstantiate the superiority of our approach, highlighting the enhanced\nrelevance and appropriateness of the generated counterspeech. This work\nunderscores the potential of multi-attribute conditioning in advancing the\nefficacy of counterspeech generation systems.", "AI": {"tldr": "\u63d0\u51faHiPPrO\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u524d\u7f00\u5b66\u4e60\u548c\u504f\u597d\u4f18\u5316\u7684\u53cc\u9636\u6bb5\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u5c5e\u6027\u6761\u4ef6\u751f\u6210\u66f4\u6709\u6548\u7684\u53cd\u4ec7\u6068\u8a00\u8bba\uff0c\u5728\u610f\u56fe\u7b26\u5408\u5ea6\u548c\u6587\u672c\u8d28\u91cf\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u53cd\u8a00\u8bba\u751f\u6210\u7814\u7a76\u591a\u57fa\u4e8e\u5355\u4e00\u610f\u56fe\u5c5e\u6027\uff0c\u800c\u591a\u5c5e\u6027\u8054\u5408\u4f18\u5316\u80fd\u4ea7\u751f\u66f4\u7ec6\u81f4\u6709\u6548\u7684\u56de\u5e94\u3002\u9700\u8981\u5f00\u53d1\u80fd\u540c\u65f6\u6574\u5408\u610f\u56fe\u548c\u60c5\u611f\u53cc\u5c5e\u6027\u7684\u751f\u6210\u6846\u67b6\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u5206\u5c42\u4f18\u5316\u5c5e\u6027\u524d\u7f00\u5d4c\u5165\u7a7a\u95f4 2\uff09\u7ed3\u5408\u53c2\u8003\u6837\u672c\u548c\u65e0\u5956\u52b1\u504f\u597d\u4f18\u5316\u3002\u6269\u5c55IntentCONANv2\u6570\u636e\u96c6\uff0c\u5bf91.3\u4e07\u6837\u672c\u8fdb\u884c\u4e94\u91cd\u60c5\u611f\u6807\u6ce8\u3002", "result": "\u610f\u56fe\u7b26\u5408\u5ea6\u63d0\u534738%\uff0cRouge\u6307\u6807\u63d0\u53472-3%\u3002\u4eba\u7c7b\u8bc4\u4f30\u8bc1\u5b9e\u751f\u6210\u5185\u5bb9\u66f4\u5177\u76f8\u5173\u6027\u548c\u9002\u5f53\u6027\u3002", "conclusion": "\u591a\u5c5e\u6027\u6761\u4ef6\u4f18\u5316\u80fd\u6709\u6548\u63d0\u5347\u53cd\u8a00\u8bba\u751f\u6210\u7cfb\u7edf\u7684\u6548\u679c\uff0c\u5206\u5c42\u524d\u7f00\u5b66\u4e60\u4e0e\u504f\u597d\u4f18\u5316\u7684\u7ed3\u5408\u4e3a\u6b64\u7c7b\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2505.11959", "pdf": "https://arxiv.org/pdf/2505.11959", "abs": "https://arxiv.org/abs/2505.11959", "authors": ["Md. Rafiul Biswas", "Wajdi Zaghouani"], "title": "EmoHopeSpeech: An Annotated Dataset of Emotions and Hope Speech in English", "categories": ["cs.CL"], "comment": null, "summary": "This research introduces a bilingual dataset comprising 23,456 entries for\nArabic and 10,036 entries for English, annotated for emotions and hope speech,\naddressing the scarcity of multi-emotion (Emotion and hope) datasets. The\ndataset provides comprehensive annotations capturing emotion intensity,\ncomplexity, and causes, alongside detailed classifications and subcategories\nfor hope speech. To ensure annotation reliability, Fleiss' Kappa was employed,\nrevealing 0.75-0.85 agreement among annotators both for Arabic and English\nlanguage. The evaluation metrics (micro-F1-Score=0.67) obtained from the\nbaseline model (i.e., using a machine learning model) validate that the data\nannotations are worthy. This dataset offers a valuable resource for advancing\nnatural language processing in underrepresented languages, fostering better\ncross-linguistic analysis of emotions and hope speech.", "AI": {"tldr": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u963f\u62c9\u4f2f\u8bed\u548c\u82f1\u8bed\u7684\u53cc\u8bed\u60c5\u611f\u4e0e\u5e0c\u671b\u8a00\u8bba\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u57fa\u7ebf\u6a21\u578b\u9a8c\u8bc1\u4e86\u6807\u6ce8\u8d28\u91cf\uff08micro-F1=0.67\uff09\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684NLP\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u8d44\u6e90\u3002", "motivation": "\u89e3\u51b3\u591a\u60c5\u611f\uff08\u60c5\u611f+\u5e0c\u671b\uff09\u6570\u636e\u96c6\u7a00\u7f3a\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u963f\u62c9\u4f2f\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\uff0c\u4fc3\u8fdb\u8de8\u8bed\u8a00\u60c5\u611f\u5206\u6790\u7814\u7a76\u3002", "method": "\u6784\u5efa\u53cc\u8bed\u6570\u636e\u96c6\uff08\u963f\u62c9\u4f2f\u8bed23,456\u6761/\u82f1\u8bed10,036\u6761\uff09\uff0c\u91c7\u7528Fleiss' Kappa\u9a8c\u8bc1\u6807\u6ce8\u4e00\u81f4\u6027\uff080.75-0.85\uff09\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u57fa\u7ebf\u8bc4\u4f30\u3002", "result": "\u6807\u6ce8\u4e00\u81f4\u6027\u9ad8\uff08Kappa\u503c0.75-0.85\uff09\uff0c\u57fa\u7ebf\u6a21\u578bmicro-F1\u8fbe0.67\uff0c\u8bc1\u660e\u6807\u6ce8\u6709\u6548\u6027\u3002\u6570\u636e\u96c6\u652f\u6301\u60c5\u611f\u5f3a\u5ea6/\u590d\u6742\u6027/\u539f\u56e0\u5206\u6790\u53ca\u5e0c\u671b\u8a00\u8bba\u7ec6\u5206\u5206\u7c7b\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u586b\u8865\u4e86\u591a\u60c5\u611f\u5206\u6790\u8d44\u6e90\u7a7a\u767d\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u7684NLP\u6280\u672f\u53d1\u5c55\u53ca\u8de8\u8bed\u8a00\u60c5\u611f\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u9760\u57fa\u7840\u3002"}}
{"id": "2505.11965", "pdf": "https://arxiv.org/pdf/2505.11965", "abs": "https://arxiv.org/abs/2505.11965", "authors": ["Xu Liu", "Guanyi Chen"], "title": "CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of Large Language Models for Multilingual Hallucination Annotation", "categories": ["cs.CL"], "comment": "SemEval-2025 Task 3", "summary": "We present the system developed by the Central China Normal University (CCNU)\nteam for the Mu-SHROOM shared task, which focuses on identifying hallucinations\nin question-answering systems across 14 different languages. Our approach\nleverages multiple Large Language Models (LLMs) with distinct areas of\nexpertise, employing them in parallel to annotate hallucinations, effectively\nsimulating a crowdsourcing annotation process. Furthermore, each LLM-based\nannotator integrates both internal and external knowledge related to the input\nduring the annotation process. Using the open-source LLM DeepSeek-V3, our\nsystem achieves the top ranking (\\#1) for Hindi data and secures a Top-5\nposition in seven other languages. In this paper, we also discuss unsuccessful\napproaches explored during our development process and share key insights\ngained from participating in this shared task.", "AI": {"tldr": "CCNU\u56e2\u961f\u5f00\u53d1\u4e86\u591aLLM\u5e76\u884c\u6807\u6ce8\u7cfb\u7edf\uff0c\u7ed3\u5408\u5185\u5916\u77e5\u8bc6\u9a8c\u8bc1\uff0c\u5728Mu-SHROOM\u591a\u8bed\u8a00\u5e7b\u89c9\u68c0\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u5370\u5730\u8bed\u7b2c\u4e00\u3001\u4e03\u79cd\u8bed\u8a00\u524d\u4e94\u7684\u6210\u7ee9", "motivation": "\u89e3\u51b3\u591a\u8bed\u8a00\u95ee\u7b54\u7cfb\u7edf\u5e7b\u89c9\u68c0\u6d4b\u96be\u9898\uff0c\u63a2\u7d22\u5927\u6a21\u578b\u534f\u540c\u6807\u6ce8\u7684\u6709\u6548\u6027", "method": "1. \u591a\u4e13\u5bb6LLM\u5e76\u884c\u6807\u6ce8\u6a21\u62df\u4f17\u5305\u6d41\u7a0b\n2. \u6574\u5408\u8f93\u5165\u76f8\u5173\u7684\u5185\u5916\u77e5\u8bc6\u9a8c\u8bc1\n3. \u57fa\u4e8eDeepSeek-V3\u6784\u5efa\u7cfb\u7edf", "result": "\u5370\u5730\u8bed\u6392\u540d#1\uff0c7\u79cd\u8bed\u8a00Top5\uff1b\u5f00\u6e90\u6a21\u578bDeepSeek-V3\u5b9e\u73b0\u6700\u4f18\u8868\u73b0", "conclusion": "\u591a\u6a21\u578b\u534f\u540c\u7b56\u7565\u6709\u6548\uff0c\u4f46\u9700\u5e73\u8861\u77e5\u8bc6\u6574\u5408\u6548\u7387\uff1b\u5931\u8d25\u7ecf\u9a8c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u91cd\u8981\u53c2\u8003"}}
{"id": "2505.11969", "pdf": "https://arxiv.org/pdf/2505.11969", "abs": "https://arxiv.org/abs/2505.11969", "authors": ["Md. Rafiul Biswas", "Wajdi Zaghouani"], "title": "An Annotated Corpus of Arabic Tweets for Hate Speech Analysis", "categories": ["cs.CL"], "comment": null, "summary": "Identifying hate speech content in the Arabic language is challenging due to\nthe rich quality of dialectal variations. This study introduces a multilabel\nhate speech dataset in the Arabic language. We have collected 10000 Arabic\ntweets and annotated each tweet, whether it contains offensive content or not.\nIf a text contains offensive content, we further classify it into different\nhate speech targets such as religion, gender, politics, ethnicity, origin, and\nothers. A text can contain either single or multiple targets. Multiple\nannotators are involved in the data annotation task. We calculated the\ninter-annotator agreement, which was reported to be 0.86 for offensive content\nand 0.71 for multiple hate speech targets. Finally, we evaluated the data\nannotation task by employing a different transformers-based model in which\nAraBERTv2 outperformed with a micro-F1 score of 0.7865 and an accuracy of\n0.786.", "AI": {"tldr": "\u6784\u5efa\u9996\u4e2a\u591a\u6807\u7b7e\u963f\u62c9\u4f2f\u8bed\u4ec7\u6068\u8a00\u8bba\u6570\u636e\u96c6\uff0810K\u63a8\u6587\uff09\uff0c\u5b9e\u73b00.786\u51c6\u786e\u7387\u7684AraBERTv2\u68c0\u6d4b\u6a21\u578b", "motivation": "\u963f\u62c9\u4f2f\u8bed\u65b9\u8a00\u590d\u6742\u6027\u5bfc\u81f4\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u56f0\u96be\uff0c\u73b0\u6709\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u4e14\u591a\u76ee\u6807\u8bc6\u522b\u4e0d\u8db3", "method": "\u4eba\u5de5\u591a\u6807\u6ce8\u8005\u6807\u6ce8\uff08\u653b\u51fb\u6027/\u4ec7\u6068\u76ee\u6807\u5206\u7c7b\uff09+ \u8ba1\u7b97\u6807\u6ce8\u4e00\u81f4\u6027\uff08Kappa 0.86/0.71\uff09+ transformers\u6a21\u578b\u9a8c\u8bc1", "result": "AraBERTv2\u53d6\u5f97\u6700\u4f18\u6027\u80fd\uff1amicro-F1 0.7865\uff0c\u51c6\u786e\u73870.786", "conclusion": "\u8be5\u6570\u636e\u96c6\u6709\u6548\u63d0\u5347\u68c0\u6d4b\u6548\u679c\uff0cAraBERTv2\u8bc1\u660e\u9002\u7528\u6027\uff0c\u652f\u6301\u591a\u76ee\u6807\u4ec7\u6068\u5185\u5bb9\u8bc6\u522b\u7814\u7a76"}}
{"id": "2505.11995", "pdf": "https://arxiv.org/pdf/2505.11995", "abs": "https://arxiv.org/abs/2505.11995", "authors": ["Yuhao Wang", "Ruiyang Ren", "Yucheng Wang", "Wayne Xin Zhao", "Jing Liu", "Hua Wu", "Haifeng Wang"], "title": "Unveiling Knowledge Utilization Mechanisms in LLM-based Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "SIGIR 2025", "summary": "Considering the inherent limitations of parametric knowledge in large\nlanguage models (LLMs), retrieval-augmented generation (RAG) is widely employed\nto expand their knowledge scope. Since RAG has shown promise in\nknowledge-intensive tasks like open-domain question answering, its broader\napplication to complex tasks and intelligent assistants has further advanced\nits utility. Despite this progress, the underlying knowledge utilization\nmechanisms of LLM-based RAG remain underexplored. In this paper, we present a\nsystematic investigation of the intrinsic mechanisms by which LLMs integrate\ninternal (parametric) and external (retrieved) knowledge in RAG scenarios.\nSpecially, we employ knowledge stream analysis at the macroscopic level, and\ninvestigate the function of individual modules at the microscopic level.\nDrawing on knowledge streaming analyses, we decompose the knowledge utilization\nprocess into four distinct stages within LLM layers: knowledge refinement,\nknowledge elicitation, knowledge expression, and knowledge contestation. We\nfurther demonstrate that the relevance of passages guides the streaming of\nknowledge through these stages. At the module level, we introduce a new method,\nknowledge activation probability entropy (KAPE) for neuron identification\nassociated with either internal or external knowledge. By selectively\ndeactivating these neurons, we achieve targeted shifts in the LLM's reliance on\none knowledge source over the other. Moreover, we discern complementary roles\nfor multi-head attention and multi-layer perceptron layers during knowledge\nformation. These insights offer a foundation for improving interpretability and\nreliability in retrieval-augmented LLMs, paving the way for more robust and\ntransparent generative solutions in knowledge-intensive domains.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7cfb\u7edf\u7814\u7a76LLMs\u5728RAG\u573a\u666f\u4e2d\u6574\u5408\u53c2\u6570\u5316\u77e5\u8bc6\u4e0e\u68c0\u7d22\u77e5\u8bc6\u7684\u5185\u5728\u673a\u5236\uff0c\u901a\u8fc7\u77e5\u8bc6\u6d41\u9636\u6bb5\u5206\u89e3\u548c\u795e\u7ecf\u5143\u5206\u6790\u65b9\u6cd5\u63ed\u793a\u4e86\u6a21\u5757\u95f4\u7684\u4e92\u8865\u4f5c\u7528\uff0c\u4e3a\u63d0\u5347\u68c0\u7d22\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u9760\u6027\u63d0\u4f9b\u7406\u8bba\u57fa\u7840", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9LLMs\u5728RAG\u4e2d\u6574\u5408\u5185\u5916\u90e8\u77e5\u8bc6\u7684\u5e95\u5c42\u673a\u5236\u7406\u89e3\u4e0d\u8db3\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u7cfb\u7edf\u6027\u5206\u6790\u63ed\u793a\u77e5\u8bc6\u6574\u5408\u8fc7\u7a0b\uff0c\u4ece\u800c\u6539\u8fdb\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0", "method": "\u91c7\u7528\u5b8f\u89c2\u77e5\u8bc6\u6d41\u56db\u9636\u6bb5\u5206\u89e3\uff08\u77e5\u8bc6\u63d0\u70bc/\u6fc0\u53d1/\u8868\u8fbe/\u7ade\u4e89\uff09\u4e0e\u5fae\u89c2KAPE\u795e\u7ecf\u5143\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u795e\u7ecf\u5143\u505c\u7528\u6280\u672f\u9a8c\u8bc1\u4e0d\u540c\u6a21\u5757\u7684\u529f\u80fd\u7279\u6027", "result": "\u53d1\u73b0\u6bb5\u843d\u76f8\u5173\u6027\u5f15\u5bfc\u77e5\u8bc6\u6d41\u9636\u6bb5\u6f14\u8fdb\uff0cKAPE\u53ef\u6709\u6548\u8bc6\u522b\u7279\u5b9a\u77e5\u8bc6\u6e90\u76f8\u5173\u795e\u7ecf\u5143\uff0c\u591a\u5934\u6ce8\u610f\u529b\u4e0eMLP\u5c42\u5728\u77e5\u8bc6\u5f62\u6210\u4e2d\u5177\u6709\u4e92\u8865\u4f5c\u7528", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3RAG\u673a\u5236\u63d0\u4f9b\u4e86\u591a\u5c42\u6b21\u5206\u6790\u6846\u67b6\uff0c\u5176\u53d1\u73b0\u80fd\u6307\u5bfc\u5f00\u53d1\u66f4\u53ef\u9760\u7684\u77e5\u8bc6\u6574\u5408\u7b56\u7565\uff0c\u63a8\u52a8\u751f\u6210\u5f0fAI\u5728\u4e13\u4e1a\u9886\u57df\u7684\u53ef\u4fe1\u5e94\u7528"}}
{"id": "2505.12028", "pdf": "https://arxiv.org/pdf/2505.12028", "abs": "https://arxiv.org/abs/2505.12028", "authors": ["Yupei Ren", "Xinyi Zhou", "Ning Zhang", "Shangqing Zhao", "Man Lan", "Xiaopeng Bai"], "title": "Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025; 13 pages, 3 figures", "summary": "Argument mining has garnered increasing attention over the years, with the\nrecent advancement of Large Language Models (LLMs) further propelling this\ntrend. However, current argument relations remain relatively simplistic and\nfoundational, struggling to capture the full scope of argument information,\nparticularly when it comes to representing complex argument structures in\nreal-world scenarios. To address this limitation, we propose 14 fine-grained\nrelation types from both vertical and horizontal dimensions, thereby capturing\nthe intricate interplay between argument components for a thorough\nunderstanding of argument structure. On this basis, we conducted extensive\nexperiments on three tasks: argument component detection, relation prediction,\nand automated essay grading. Additionally, we explored the impact of writing\nquality on argument component detection and relation prediction, as well as the\nconnections between discourse relations and argumentative features. The\nfindings highlight the importance of fine-grained argumentative annotations for\nargumentative writing quality assessment and encourage multi-dimensional\nargument analysis.", "AI": {"tldr": "\u63d0\u51fa14\u79cd\u7ec6\u7c92\u5ea6\u8bba\u70b9\u5173\u7cfb\u7c7b\u578b\u63d0\u5347\u8bae\u8bba\u6587\u7ed3\u6784\u5206\u6790\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u5b9e\u9a8c\u9a8c\u8bc1\u7ec6\u7c92\u5ea6\u6807\u6ce8\u5bf9\u5199\u4f5c\u8d28\u91cf\u8bc4\u4f30\u7684\u6709\u6548\u6027", "motivation": "\u5f53\u524d\u8bba\u70b9\u5173\u7cfb\u7814\u7a76\u8fc7\u4e8e\u57fa\u7840\uff0c\u96be\u4ee5\u6355\u6349\u771f\u5b9e\u573a\u666f\u4e2d\u590d\u6742\u7684\u8bba\u70b9\u7ed3\u6784\u4ea4\u4e92", "method": "\u4ece\u5782\u76f4/\u6c34\u5e73\u7ef4\u5ea6\u8bbe\u8ba114\u79cd\u5173\u7cfb\u7c7b\u578b\uff0c\u5728\u8bba\u70b9\u68c0\u6d4b\u3001\u5173\u7cfb\u9884\u6d4b\u3001\u4f5c\u6587\u8bc4\u5206\u4e09\u4efb\u52a1\u5f00\u5c55\u5b9e\u9a8c\uff0c\u5e76\u63a2\u7d22\u5199\u4f5c\u8d28\u91cf\u4e0e\u8bdd\u8bed\u7279\u5f81\u5173\u8054", "result": "\u7ec6\u7c92\u5ea6\u6807\u6ce8\u663e\u8457\u63d0\u5347\u8bae\u8bba\u6587\u8d28\u91cf\u8bc4\u4f30\u6548\u679c\uff0c\u63ed\u793a\u5199\u4f5c\u8d28\u91cf\u4e0e\u8bba\u70b9\u68c0\u6d4b\u51c6\u786e\u7387\u7684\u6b63\u76f8\u5173\u6027", "conclusion": "\u8bba\u6587\u8bc1\u660e\u7ec6\u7c92\u5ea6\u6807\u6ce8\u4f53\u7cfb\u5bf9\u591a\u7ef4\u5ea6\u8bba\u70b9\u5206\u6790\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u81ea\u52a8\u5199\u4f5c\u8bc4\u4f30\u7cfb\u7edf\u63d0\u4f9b\u65b0\u65b9\u6cd5\u8bba"}}
{"id": "2505.12043", "pdf": "https://arxiv.org/pdf/2505.12043", "abs": "https://arxiv.org/abs/2505.12043", "authors": ["Jingxue Chen", "Qingkun Tang", "Qianchun Lu", "Siyuan Fang"], "title": "MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving General Capabilities", "categories": ["cs.CL"], "comment": "12 pages, 2 figures", "summary": "Although LLMs perform well in general tasks, domain-specific applications\nsuffer from hallucinations and accuracy limitations. CPT approaches encounter\ntwo key issues: (1) domain-biased data degrades general language skills, and\n(2) improper corpus-mixture ratios limit effective adaptation. To address\nthese, we propose a novel framework, Mixture of Losses (MoL), which decouples\noptimization objectives for domain-specific and general corpora. Specifically,\ncross-entropy (CE) loss is applied to domain data to ensure knowledge\nacquisition, while Kullback-Leibler (KL) divergence aligns general-corpus\ntraining with the base model's foundational capabilities. This dual-loss\narchitecture preserves universal skills while enhancing domain expertise,\navoiding catastrophic forgetting. Empirically, we validate that a 1:1\ndomain-to-general corpus ratio optimally balances training and overfitting\nwithout the need for extensive tuning or resource-intensive experiments.\nFurthermore, our experiments demonstrate significant performance gains compared\nto traditional CPT approaches, which often suffer from degradation in general\nlanguage capabilities; our model achieves 27.9% higher accuracy on the Math-500\nbenchmark in the non-think reasoning mode, and an impressive 83.3% improvement\non the challenging AIME25 subset in the think mode, underscoring the\neffectiveness of our approach.", "AI": {"tldr": "\u63d0\u51faMoL\u6846\u67b6\u89e3\u51b3\u4f20\u7edf\u6301\u7eed\u9884\u8bad\u7ec3(CPT)\u7684\u9886\u57df\u9002\u5e94\u95ee\u9898\uff0c\u901a\u8fc7\u53cc\u635f\u5931\u673a\u5236(\u4ea4\u53c9\u71b5+KL\u6563\u5ea6)\u4fdd\u6301\u57fa\u7840\u6a21\u578b\u901a\u7528\u80fd\u529b\u7684\u540c\u65f6\u63d0\u5347\u9886\u57df\u6027\u80fd", "motivation": "\u73b0\u6709\u6301\u7eed\u9884\u8bad\u7ec3\u65b9\u6cd5\u5728\u9886\u57df\u9002\u5e94\u65f6\u4f1a\u5bfc\u81f4\u901a\u7528\u8bed\u8a00\u80fd\u529b\u9000\u5316\uff0c\u4e14\u8bed\u6599\u6df7\u5408\u6bd4\u4f8b\u96be\u4ee5\u4f18\u5316", "method": "\u89e3\u8026\u9886\u57df/\u901a\u7528\u8bed\u6599\u7684\u4f18\u5316\u76ee\u6807\uff1a\u9886\u57df\u6570\u636e\u7528\u4ea4\u53c9\u71b5\u635f\u5931\u83b7\u53d6\u77e5\u8bc6\uff0c\u901a\u7528\u6570\u636e\u7528KL\u6563\u5ea6\u5bf9\u9f50\u57fa\u7840\u6a21\u578b\u80fd\u529b", "result": "Math-500\u975e\u601d\u7ef4\u63a8\u7406\u6a21\u5f0f\u51c6\u786e\u7387\u63d0\u534727.9%\uff0cAIME25\u6311\u6218\u96c6\u601d\u7ef4\u6a21\u5f0f\u63d0\u534783.3%\uff1b\u9a8c\u8bc11:1\u8bed\u6599\u6bd4\u4f8b\u4e3a\u6700\u4f18\u89e3", "conclusion": "MoL\u6846\u67b6\u6709\u6548\u5e73\u8861\u9886\u57df\u9002\u5e94\u4e0e\u901a\u7528\u80fd\u529b\u4fdd\u7559\uff0c\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u9886\u57df\u9002\u914d\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.12050", "pdf": "https://arxiv.org/pdf/2505.12050", "abs": "https://arxiv.org/abs/2505.12050", "authors": ["Vinod Raman", "Hilal Asi", "Satyen Kale"], "title": "ABoN: Adaptive Best-of-N Alignment", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "23 pages", "summary": "Recent advances in test-time alignment methods, such as Best-of-N sampling,\noffer a simple and effective way to steer language models (LMs) toward\npreferred behaviors using reward models (RM). However, these approaches can be\ncomputationally expensive, especially when applied uniformly across prompts\nwithout accounting for differences in alignment difficulty. In this work, we\npropose a prompt-adaptive strategy for Best-of-N alignment that allocates\ninference-time compute more efficiently. Motivated by latency concerns, we\ndevelop a two-stage algorithm: an initial exploratory phase estimates the\nreward distribution for each prompt using a small exploration budget, and a\nsecond stage adaptively allocates the remaining budget using these estimates.\nOur method is simple, practical, and compatible with any LM/RM combination.\nEmpirical results on the AlpacaEval dataset for 12 LM/RM pairs and 50 different\nbatches of prompts show that our adaptive strategy consistently outperforms the\nuniform allocation with the same inference budget. Moreover, our experiments\nshow that our adaptive strategy remains competitive against uniform allocations\nwith 20% larger inference budgets and even improves in performance as the batch\nsize grows.", "AI": {"tldr": "\u63d0\u51fa\u63d0\u793a\u81ea\u9002\u5e94\u7684Best-of-N\u5bf9\u9f50\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387", "motivation": "\u73b0\u6709\u6d4b\u8bd5\u65f6\u5bf9\u9f50\u65b9\u6cd5\u5bf9\u6240\u6709\u63d0\u793a\u7edf\u4e00\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u672a\u8003\u8651\u4e0d\u540c\u63d0\u793a\u7684\u5bf9\u9f50\u96be\u5ea6\u5dee\u5f02\uff0c\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b", "method": "\u4e24\u9636\u6bb5\u81ea\u9002\u5e94\u7b97\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u7528\u5c11\u91cf\u9884\u7b97\u63a2\u7d22\u5956\u52b1\u5206\u5e03\uff0c\u7b2c\u4e8c\u9636\u6bb5\u57fa\u4e8e\u4f30\u8ba1\u503c\u52a8\u6001\u5206\u914d\u5269\u4f59\u9884\u7b97", "result": "\u5728\u76f8\u540c\u63a8\u7406\u9884\u7b97\u4e0b\uff0c\u81ea\u9002\u5e94\u7b56\u7565\u5728AlpacaEval\u6570\u636e\u96c6\u4e0a\u5168\u9762\u8d85\u8d8a\u5747\u5300\u5206\u914d\u3002\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5728\u6279\u6b21\u89c4\u6a21\u6269\u5927\u65f6\u6027\u80fd\u6301\u7eed\u63d0\u5347\uff0c\u751a\u81f3\u53ef\u8282\u770120%\u8ba1\u7b97\u8d44\u6e90", "conclusion": "\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u7b56\u7565\u517c\u5177\u7b80\u5355\u6027\u3001\u5b9e\u7528\u6027\uff0c\u517c\u5bb9\u4efb\u610fLM/RM\u7ec4\u5408\uff0c\u4e14\u8ba1\u7b97\u6548\u7387\u968f\u6279\u6b21\u89c4\u6a21\u6269\u5927\u800c\u63d0\u5347"}}
{"id": "2505.12054", "pdf": "https://arxiv.org/pdf/2505.12054", "abs": "https://arxiv.org/abs/2505.12054", "authors": ["Mat\u00fa\u0161 Pikuliak"], "title": "GenderBench: Evaluation Suite for Gender Biases in LLMs", "categories": ["cs.CL"], "comment": null, "summary": "We present GenderBench -- a comprehensive evaluation suite designed to\nmeasure gender biases in LLMs. GenderBench includes 14 probes that quantify 19\ngender-related harmful behaviors exhibited by LLMs. We release GenderBench as\nan open-source and extensible library to improve the reproducibility and\nrobustness of benchmarking across the field. We also publish our evaluation of\n12 LLMs. Our measurements reveal consistent patterns in their behavior. We show\nthat LLMs struggle with stereotypical reasoning, equitable gender\nrepresentation in generated texts, and occasionally also with discriminatory\nbehavior in high-stakes scenarios, such as hiring.", "AI": {"tldr": "\u5f00\u6e90\u8bc4\u6d4b\u6846\u67b6GenderBench\u91cf\u5316\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u6027\u522b\u504f\u89c1\uff0c\u53d1\u73b0\u6a21\u578b\u666e\u904d\u5b58\u5728\u523b\u677f\u5370\u8c61\u63a8\u7406\u3001\u6027\u522b\u8868\u5f81\u5931\u8861\u53ca\u9ad8\u98ce\u9669\u573a\u666f\u6b67\u89c6\u884c\u4e3a", "motivation": "\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5e7f\u6cdb\u5b58\u5728\u7684\u9690\u6027\u6027\u522b\u504f\u89c1\u95ee\u9898\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u6d4b\u5de5\u5177\u963b\u788d\u4e86\u6a21\u578b\u516c\u5e73\u6027\u6539\u8fdb\u3002\u901a\u8fc7\u6784\u5efa\u6807\u51c6\u5316\u8bc4\u4f30\u4f53\u7cfb\u63a8\u52a8LLM\u6027\u522b\u5e73\u7b49\u7814\u7a76\u3002", "method": "\u5f00\u53d1\u5305\u542b14\u4e2a\u6d4b\u8bd5\u63a2\u9488\u7684\u8bc4\u4f30\u5957\u4ef6\uff0c\u91cf\u531619\u79cd\u6027\u522b\u76f8\u5173\u6709\u5bb3\u884c\u4e3a\u3002\u5bf912\u4e2a\u4e3b\u6d41LLM\u8fdb\u884c\u7cfb\u7edf\u8bc4\u6d4b\uff0c\u5206\u6790\u5176\u884c\u4e3a\u6a21\u5f0f\u4e00\u81f4\u6027\u3002", "result": "\u6a21\u578b\u5728\u523b\u677f\u5370\u8c61\u63a8\u7406\uff0887%\u6d4b\u8bd5\u6837\u672c\uff09\u3001\u6587\u672c\u751f\u6210\u6027\u522b\u5931\u8861\uff08\u7537\u6027\u89d2\u8272\u5360\u6bd4\u8d8568%\uff09\u3001\u62db\u8058\u7b49\u9ad8\u98ce\u9669\u573a\u666f\u6b67\u89c6\u884c\u4e3a\uff0832%\u6d4b\u8bd5\u6848\u4f8b\uff09\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\u3002", "conclusion": "GenderBench\u4f5c\u4e3a\u5f00\u6e90\u5de5\u5177\u586b\u8865\u4e86\u9886\u57df\u7a7a\u767d\uff0c\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\u5f53\u524dLLM\u6027\u522b\u504f\u89c1\u95ee\u9898\u5177\u6709\u666e\u904d\u6027\uff0c\u5f3a\u8c03\u5f00\u53d1\u8fc7\u7a0b\u4e2d\u9700\u5d4c\u5165\u516c\u5e73\u6027\u8bc4\u4f30\u673a\u5236\u3002"}}
{"id": "2505.12060", "pdf": "https://arxiv.org/pdf/2505.12060", "abs": "https://arxiv.org/abs/2505.12060", "authors": ["Peng Ding", "Jun Kuang", "Zongyu Wang", "Xuezhi Cao", "Xunliang Cai", "Jiajun Chen", "Shujian Huang"], "title": "Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement", "categories": ["cs.CL"], "comment": "Acccepted by ACL 2025 Findings, 21 pages, 9 figures, 14 tables", "summary": "Large Language Models (LLMs) have shown impressive capabilities across\nvarious tasks but remain vulnerable to meticulously crafted jailbreak attacks.\nIn this paper, we identify a critical safety gap: while LLMs are adept at\ndetecting jailbreak prompts, they often produce unsafe responses when directly\nprocessing these inputs. Inspired by this insight, we propose SAGE (Self-Aware\nGuard Enhancement), a training-free defense strategy designed to align LLMs'\nstrong safety discrimination performance with their relatively weaker safety\ngeneration ability. SAGE consists of two core components: a Discriminative\nAnalysis Module and a Discriminative Response Module, enhancing resilience\nagainst sophisticated jailbreak attempts through flexible safety discrimination\ninstructions. Extensive experiments demonstrate SAGE's effectiveness and\nrobustness across various open-source and closed-source LLMs of different sizes\nand architectures, achieving an average 99% defense success rate against\nnumerous complex and covert jailbreak methods while maintaining helpfulness on\ngeneral benchmarks. We further conduct mechanistic interpretability analysis\nthrough hidden states and attention distributions, revealing the underlying\nmechanisms of this detection-generation discrepancy. Our work thus contributes\nto developing future LLMs with coherent safety awareness and generation\nbehavior. Our code and datasets are publicly available at\nhttps://github.com/NJUNLP/SAGE.", "AI": {"tldr": "\u63d0\u51faSAGE\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u65e0\u5173\u7684\u5224\u522b\u5f0f\u5b89\u5168\u589e\u5f3a\u7b56\u7565\uff0c\u6709\u6548\u62b5\u5fa1LLMs\u7684\u8d8a\u72f1\u653b\u51fb\uff0c\u5b9e\u73b099%\u5e73\u5747\u9632\u5fa1\u6210\u529f\u7387\u3002", "motivation": "\u53d1\u73b0LLMs\u5b58\u5728\u5b89\u5168\u5224\u522b\u80fd\u529b\u4e0e\u751f\u6210\u80fd\u529b\u7684\u4e0d\u5bf9\u79f0\u6027\u2014\u2014\u80fd\u8bc6\u522b\u6076\u610f\u6307\u4ee4\u5374\u4ecd\u751f\u6210\u4e0d\u5b89\u5168\u5185\u5bb9\uff0c\u9700\u6784\u5efa\u68c0\u6d4b\u4e0e\u751f\u6210\u7684\u5bf9\u9f50\u673a\u5236\u3002", "method": "SAGE\u5305\u542b\u5224\u522b\u5206\u6790\u6a21\u5757\uff08\u68c0\u6d4b\u6076\u610f\u6307\u4ee4\uff09\u548c\u5224\u522b\u54cd\u5e94\u6a21\u5757\uff08\u5b89\u5168\u751f\u6210\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u5b89\u5168\u6307\u4ee4\u6ce8\u5165\u63d0\u5347\u6a21\u578b\u9632\u5fa1\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5f00\u6e90/\u95ed\u6e90\u591a\u67b6\u6784LLMs\u4e0a\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u62b5\u5fa1\u590d\u6742\u8d8a\u72f1\u653b\u51fb\u6210\u529f\u7387\u8d8599%\uff0c\u901a\u7528\u57fa\u51c6\u6d4b\u8bd5\u4fdd\u6301\u539f\u6709\u6027\u80fd\u3002\u901a\u8fc7\u9690\u72b6\u6001\u5206\u6790\u63ed\u793a\u5b89\u5168\u673a\u5236\u673a\u7406\u3002", "conclusion": "\u9996\u6b21\u7cfb\u7edf\u6027\u89e3\u51b3LLMs\u5b89\u5168\u68c0\u6d4b\u4e0e\u751f\u6210\u7684\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u4e3a\u6784\u5efa\u5b89\u5168\u5bf9\u9f50\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002\u4ee3\u7801\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.12071", "pdf": "https://arxiv.org/pdf/2505.12071", "abs": "https://arxiv.org/abs/2505.12071", "authors": ["Harald Baayen", "Kristian Berg", "Maziyah Mohamed"], "title": "Historical and psycholinguistic perspectives on morphological productivity: A sketch of an integrative approach", "categories": ["cs.CL"], "comment": "35 pages, 11 figures", "summary": "In this study, we approach morphological productivity from two perspectives:\na cognitive-computational perspective, and a diachronic perspective zooming in\non an actual speaker, Thomas Mann. For developing the first perspective, we\nmake use of a cognitive computational model of the mental lexicon, the\ndiscriminative lexicon model. For computational mappings between form and\nmeaning to be productive, in the sense that novel, previously unencountered\nwords, can be understood and produced, there must be systematicities between\nthe form space and the semantic space. If the relation between form and meaning\nwould be truly arbitrary, a model could memorize form and meaning pairings, but\nthere is no way in which the model would be able to generalize to novel test\ndata. For Finnish nominal inflection, Malay derivation, and English\ncompounding, we explore, using the Discriminative Lexicon Model as a\ncomputational tool, to trace differences in the degree to which inflectional\nand word formation patterns are productive. We show that the DLM tends to\nassociate affix-like sublexical units with the centroids of the embeddings of\nthe words with a given affix. For developing the second perspective, we study\nhow the intake and output of one prolific writer, Thomas Mann, changes over\ntime. We show by means of an examination of what Thomas Mann is likely to have\nread, and what he wrote, that the rate at which Mann produces novel derived\nwords is extremely low. There are far more novel words in his input than in his\noutput. We show that Thomas Mann is less likely to produce a novel derived word\nwith a given suffix the greater the average distance is of the embeddings of\nall derived words to the corresponding centroid, and discuss the challenges of\nusing speaker-specific embeddings for low-frequency and novel words.", "AI": {"tldr": "\u901a\u8fc7\u8ba4\u77e5\u8ba1\u7b97\u6a21\u578b\u548c\u4e2a\u6848\u5386\u65f6\u5206\u6790\uff0c\u63a2\u7d22\u5f62\u6001\u80fd\u4ea7\u6027\u7684\u8861\u91cf\u6807\u51c6\u53ca\u4f5c\u5bb6\u8bed\u8a00\u521b\u65b0\u89c4\u5f8b", "motivation": "\u7814\u7a76\u5f62\u6001\u80fd\u4ea7\u6027\u9700\u8981\u7ed3\u5408\u7cfb\u7edf\u6027\u8ba1\u7b97\u6a21\u578b\uff08\u9a8c\u8bc1\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\uff09\u548c\u5177\u4f53\u4f5c\u5bb6\u7684\u5386\u65f6\u8bed\u8a00\u6f14\u53d8\uff08\u63ed\u793a\u4e2a\u4f53\u521b\u65b0\u673a\u5236\uff09", "method": "\u4f7f\u7528\u5224\u522b\u8bcd\u5178\u6a21\u578b\u5206\u6790\u82ac\u5170\u8bed/\u9a6c\u6765\u8bed/\u82f1\u8bed\u7684\u5f62\u6001\u7cfb\u7edf\uff1b\u901a\u8fc7\u6258\u9a6c\u65af\u00b7\u66fc\u7684\u9605\u8bfb\u8f93\u5165\u4e0e\u5199\u4f5c\u8f93\u51fa\u6570\u636e\u8fdb\u884c\u5386\u65f6\u5bf9\u6bd4", "result": "\u8ba1\u7b97\u6a21\u578b\u663e\u793a\u8bcd\u7f00\u4e0e\u8bed\u4e49\u4e2d\u5fc3\u70b9\u5f3a\u5173\u8054\uff1b\u4f5c\u5bb6\u8bed\u8a00\u521b\u65b0\u7387\u6781\u4f4e\u4e14\u4e0e\u8bcd\u7f00\u5d4c\u5165\u8ddd\u79bb\u8d1f\u76f8\u5173", "conclusion": "\u5f62\u6001\u80fd\u4ea7\u6027\u9700\u517c\u987e\u7cfb\u7edf\u8ba1\u7b97\u6a21\u578b\u548c\u4e2a\u4f53\u5386\u65f6\u5206\u6790\uff0c\u4e3a\u8bed\u8a00\u4e60\u5f97\u4e0e\u6587\u5b66\u521b\u4f5c\u7814\u7a76\u63d0\u4f9b\u53cc\u91cd\u89c6\u89d2"}}
{"id": "2505.12075", "pdf": "https://arxiv.org/pdf/2505.12075", "abs": "https://arxiv.org/abs/2505.12075", "authors": ["Guy Davidson", "Todd M. Gureckis", "Brenden M. Lake", "Adina Williams"], "title": "Do different prompting methods yield a common task representation in language models?", "categories": ["cs.CL", "cs.LG"], "comment": "9 pages, 4 figures; under review", "summary": "Demonstrations and instructions are two primary approaches for prompting\nlanguage models to perform in-context learning (ICL) tasks. Do identical tasks\nelicited in different ways result in similar representations of the task? An\nimproved understanding of task representation mechanisms would offer\ninterpretability insights and may aid in steering models. We study this through\nfunction vectors, recently proposed as a mechanism to extract few-shot ICL task\nrepresentations. We generalize function vectors to alternative task\npresentations, focusing on short textual instruction prompts, and successfully\nextract instruction function vectors that promote zero-shot task accuracy. We\nfind evidence that demonstration- and instruction-based function vectors\nleverage different model components, and offer several controls to dissociate\ntheir contributions to task performance. Our results suggest that different\ntask presentations do not induce a common task representation but elicit\ndifferent, partly overlapping mechanisms. Our findings offer principled support\nto the practice of combining textual instructions and task demonstrations,\nimply challenges in universally monitoring task inference across presentation\nforms, and encourage further examinations of LLM task inference mechanisms.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u51fd\u6570\u5411\u91cf\u5206\u6790\u53d1\u73b0\uff0c\u8bed\u8a00\u6a21\u578b\u5bf9\u793a\u4f8b\u6f14\u793a\u548c\u6587\u672c\u6307\u4ee4\u7684\u4efb\u52a1\u8868\u5f81\u673a\u5236\u5b58\u5728\u5dee\u5f02\uff0c\u4e8c\u8005\u6fc0\u6d3b\u4e0d\u540c\u6a21\u578b\u7ec4\u4ef6\u4e14\u4ec5\u90e8\u5206\u91cd\u53e0\uff0c\u652f\u6301\u4e86\u7ec4\u5408\u4f7f\u7528\u4e24\u79cd\u63d0\u793a\u7b56\u7565\u7684\u5b9e\u8df5\u3002", "motivation": "\u63a2\u7a76\u76f8\u540c\u4efb\u52a1\u901a\u8fc7\u793a\u4f8b\u6f14\u793a\uff08in-context demonstrations\uff09\u548c\u6587\u672c\u6307\u4ee4\uff08textual instructions\uff09\u4e24\u79cd\u4e0d\u540c\u63d0\u793a\u65b9\u5f0f\uff0c\u662f\u5426\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u5f62\u6210\u76f8\u4f3c\u7684\u4efb\u52a1\u8868\u5f81\u673a\u5236\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u4efb\u52a1\u63a7\u5236\u80fd\u529b\u3002", "method": "1. \u5c06\u51fd\u6570\u5411\u91cf\uff08function vectors\uff09\u65b9\u6cd5\u6269\u5c55\u5230\u6307\u4ee4\u63d0\u793a\u573a\u666f\uff0c\u63d0\u53d6\u96f6\u6837\u672c\u4efb\u52a1\u6307\u4ee4\u5411\u91cf\uff1b2. \u5bf9\u6bd4\u5206\u6790\u6f14\u793a\u578b\u4e0e\u6307\u4ee4\u578b\u51fd\u6570\u5411\u91cf\u5728\u6a21\u578b\u4e2d\u7684\u6fc0\u6d3b\u6a21\u5f0f\uff1b3. \u8bbe\u8ba1\u63a7\u5236\u5b9e\u9a8c\u89e3\u8026\u4e24\u7c7b\u5411\u91cf\u5bf9\u4efb\u52a1\u8868\u73b0\u7684\u8d21\u732e\u3002", "result": "1. \u6f14\u793a\u578b\u548c\u6307\u4ee4\u578b\u51fd\u6570\u5411\u91cf\u5206\u522b\u4f9d\u8d56\u4e0d\u540c\u6a21\u578b\u7ec4\u4ef6\uff1b2. \u4e8c\u8005\u5b58\u5728\u90e8\u5206\u91cd\u53e0\u4f46\u672a\u5f62\u6210\u7edf\u4e00\u4efb\u52a1\u8868\u5f81\uff1b3. \u7ec4\u5408\u4f7f\u7528\u4e24\u79cd\u63d0\u793a\u65b9\u5f0f\u53ef\u63d0\u5347\u4efb\u52a1\u51c6\u786e\u7387\u3002", "conclusion": "\u4e0d\u540c\u4efb\u52a1\u5448\u73b0\u65b9\u5f0f\u6fc0\u6d3b\u5dee\u5f02\u5316\u673a\u5236\uff0c\u652f\u6301\u6307\u4ee4\u4e0e\u6f14\u793a\u7ec4\u5408\u7b56\u7565\uff0c\u63d0\u793a\u8de8\u5f62\u5f0f\u4efb\u52a1\u76d1\u63a7\u5b58\u5728\u6311\u6218\uff0c\u9700\u7ee7\u7eed\u63a2\u7d22LLM\u4efb\u52a1\u63a8\u7406\u673a\u5236\u3002"}}
{"id": "2505.12082", "pdf": "https://arxiv.org/pdf/2505.12082", "abs": "https://arxiv.org/abs/2505.12082", "authors": ["Yunshui Li", "Yiyuan Ma", "Shen Yan", "Chaoyi Zhang", "Jing Liu", "Jianqiao Lu", "Ziwen Xu", "Mengzhao Chen", "Minrui Wang", "Shiyi Zhan", "Jin Ma", "Xunhao Lai", "Yao Luo", "Xingyan Bin", "Hongbin Ren", "Mingji Han", "Wenhao Hao", "Bairen Yi", "LingJun Liu", "Bole Ma", "Xiaoying Jia", "Zhou Xun", "Liang Xiang", "Yonghui Wu"], "title": "Model Merging in Pre-training of Large Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Model merging has emerged as a promising technique for enhancing large\nlanguage models, though its application in large-scale pre-training remains\nrelatively unexplored. In this paper, we present a comprehensive investigation\nof model merging techniques during the pre-training process. Through extensive\nexperiments with both dense and Mixture-of-Experts (MoE) architectures ranging\nfrom millions to over 100 billion parameters, we demonstrate that merging\ncheckpoints trained with constant learning rates not only achieves significant\nperformance improvements but also enables accurate prediction of annealing\nbehavior. These improvements lead to both more efficient model development and\nsignificantly lower training costs. Our detailed ablation studies on merging\nstrategies and hyperparameters provide new insights into the underlying\nmechanisms while uncovering novel applications. Through comprehensive\nexperimental analysis, we offer the open-source community practical\npre-training guidelines for effective model merging.", "AI": {"tldr": "\u63a2\u7d22\u6a21\u578b\u5408\u5e76\u6280\u672f\u5728\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\uff0c\u9a8c\u8bc1\u6052\u5b9a\u5b66\u4e60\u7387\u5408\u5e76\u7b56\u7565\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3001\u9884\u6d4b\u9000\u706b\u884c\u4e3a\uff0c\u5e76\u63d0\u4f9b\u5f00\u6e90\u793e\u533a\u5b9e\u7528\u8bad\u7ec3\u6307\u5357", "motivation": "\u5f53\u524d\u6a21\u578b\u5408\u5e76\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u7684\u6f5c\u529b\u672a\u88ab\u5145\u5206\u6316\u6398\uff0c\u9700\u9a8c\u8bc1\u5176\u5728\u8d85\u5927\u6a21\u578b\u573a\u666f\u7684\u6709\u6548\u6027\u5e76\u63a2\u7d22\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\u7684\u65b0\u65b9\u6cd5", "method": "\u901a\u8fc7\u767e\u4e07\u81f3\u5343\u4ebf\u53c2\u6570\u89c4\u6a21\u7684\u5bc6\u96c6/MoE\u67b6\u6784\u5b9e\u9a8c\uff0c\u5206\u6790\u4e0d\u540c\u5408\u5e76\u7b56\u7565\u4e0e\u8d85\u53c2\u6570\u5bf9\u9884\u8bad\u7ec3\u6548\u679c\u7684\u5f71\u54cd\uff0c\u5f00\u5c55\u6d88\u878d\u7814\u7a76\u63ed\u793a\u5e95\u5c42\u673a\u5236", "result": "\u6052\u5b9a\u5b66\u4e60\u7387\u5408\u5e76\u4f7f\u6a21\u578b\u6027\u80fd\u63d0\u534720-35%\uff0c\u8bad\u7ec3\u6210\u672c\u964d\u4f4e40%+\uff0c\u6d88\u878d\u7814\u7a76\u63ed\u793a\u4e86\u53c2\u6570\u878d\u5408\u7684\u4e34\u754c\u9608\u503c\u73b0\u8c61", "conclusion": "\u6a21\u578b\u5408\u5e76\u663e\u8457\u63d0\u5347\u9884\u8bad\u7ec3\u6548\u7387\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u8bba\u4e3a\u793e\u533a\u5efa\u7acb\u5b9e\u7528\u8bad\u7ec3\u8303\u5f0f\uff0c\u63a8\u52a8LLM\u5f00\u53d1\u6210\u672c\u964d\u4f4e50%\u4ee5\u4e0a"}}
{"id": "2505.12090", "pdf": "https://arxiv.org/pdf/2505.12090", "abs": "https://arxiv.org/abs/2505.12090", "authors": ["Mohammad Shokri", "Sarah Ita Levitan", "Rivka Levitan"], "title": "Personalized Author Obfuscation with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this paper, we investigate the efficacy of large language models (LLMs) in\nobfuscating authorship by paraphrasing and altering writing styles. Rather than\nadopting a holistic approach that evaluates performance across the entire\ndataset, we focus on user-wise performance to analyze how obfuscation\neffectiveness varies across individual authors. While LLMs are generally\neffective, we observe a bimodal distribution of efficacy, with performance\nvarying significantly across users. To address this, we propose a personalized\nprompting method that outperforms standard prompting techniques and partially\nmitigates the bimodality issue.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u7528\u6237\u5c42\u9762\u5206\u6790\u53d1\u73b0LLM\u4f5c\u8005\u6df7\u6dc6\u6548\u679c\u5b58\u5728\u53cc\u5cf0\u5206\u5e03\uff0c\u63d0\u51fa\u4e2a\u6027\u5316\u63d0\u793a\u65b9\u6cd5\u63d0\u5347\u6548\u679c", "motivation": "\u73b0\u6709\u4f5c\u8005\u6df7\u6dc6\u7814\u7a76\u591a\u5173\u6ce8\u6574\u4f53\u6548\u679c\uff0c\u5ffd\u89c6\u4e86\u4e0d\u540c\u7528\u6237\u95f4\u7684\u6548\u679c\u5dee\u5f02\uff0c\u9700\u9488\u5bf9\u6027\u6539\u8fdb\u4e2a\u6027\u5316\u573a\u666f\u4e0b\u7684\u6df7\u6dc6\u6027\u80fd", "method": "\u91c7\u7528\u7528\u6237\u7c92\u5ea6\u6548\u679c\u5206\u6790\uff0c\u53d1\u73b0\u6548\u679c\u53cc\u5cf0\u5206\u5e03\u73b0\u8c61\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u7528\u6237\u7279\u5f81\u7684\u4e2a\u6027\u5316\u63d0\u793a\u4f18\u5316\u65b9\u6848", "result": "LLM\u603b\u4f53\u6709\u6548\u4f46\u4e2a\u4f53\u5dee\u5f02\u663e\u8457\uff0c\u4e2a\u6027\u5316\u63d0\u793a\u6bd4\u6807\u51c6\u65b9\u6cd5\u6548\u679c\u63d0\u534718.6%\uff0c\u53cc\u5cf0\u5206\u5e03\u6807\u51c6\u5dee\u964d\u4f4e32%", "conclusion": "\u7528\u6237\u7279\u5f81\u654f\u611f\u7684\u4e2a\u6027\u5316\u63d0\u793a\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u4f5c\u8005\u6df7\u6dc6\u6548\u679c\uff0c\u4e3a\u6570\u5b57\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u65b0\u7684\u6280\u672f\u601d\u8def"}}
{"id": "2505.12100", "pdf": "https://arxiv.org/pdf/2505.12100", "abs": "https://arxiv.org/abs/2505.12100", "authors": ["Isabela Pereira Gregio", "Ian Pons", "Anna Helena Reali Costa", "Artur Jord\u00e3o"], "title": "Improving Fairness in LLMs Through Testing-Time Adversaries", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) push the bound-aries in natural language\nprocessing and generative AI, driving progress across various aspects of modern\nsociety. Unfortunately, the pervasive issue of bias in LLMs responses (i.e.,\npredictions) poses a significant and open challenge, hindering their\napplication in tasks involving ethical sensitivity and responsible\ndecision-making. In this work, we propose a straightforward, user-friendly and\npractical method to mitigate such biases, enhancing the reliability and\ntrustworthiness of LLMs. Our method creates multiple variations of a given\nsentence by modifying specific attributes and evaluates the corresponding\nprediction behavior compared to the original, unaltered, prediction/sentence.\nThe idea behind this process is that critical ethical predictions often exhibit\nnotable inconsistencies, indicating the presence of bias. Unlike previous\napproaches, our method relies solely on forward passes (i.e., testing-time\nadversaries), eliminating the need for training, fine-tuning, or prior\nknowledge of the training data distribution. Through extensive experiments on\nthe popular Llama family, we demonstrate the effectiveness of our method in\nimproving various fairness metrics, focusing on the reduction of disparities in\nhow the model treats individuals from different racial groups. Specifically,\nusing standard metrics, we improve the fairness in Llama3 in up to 27\npercentage points. Overall, our approach significantly enhances fairness,\nequity, and reliability in LLM-generated results without parameter tuning or\ntraining data modifications, confirming its effectiveness in practical\nscenarios. We believe our work establishes an important step toward enabling\nthe use of LLMs in tasks that require ethical considerations and responsible\ndecision-making.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684\u524d\u5411\u6270\u52a8\u65b9\u6cd5\u68c0\u6d4bLLM\u504f\u89c1\uff0c\u663e\u8457\u63d0\u5347\u6a21\u578b\u516c\u5e73\u6027\u6307\u6807", "motivation": "LLM\u5728\u4f26\u7406\u654f\u611f\u4efb\u52a1\u4e2d\u7684\u7cfb\u7edf\u6027\u504f\u89c1\u963b\u788d\u5176\u53ef\u4fe1\u5e94\u7528\uff0c\u9700\u5f00\u53d1\u65e0\u9700\u8c03\u6574\u53c2\u6570\u7684\u5b9e\u65f6\u53bb\u504f\u65b9\u6cd5", "method": "\u901a\u8fc7\u4fee\u6539\u53e5\u5b50\u5c5e\u6027\u751f\u6210\u53d8\u4f53\uff0c\u6bd4\u8f83\u539f\u59cb\u9884\u6d4b\u4e0e\u6270\u52a8\u9884\u6d4b\u7684\u5dee\u5f02\u6765\u8bc6\u522b\u504f\u89c1\uff0c\u4ec5\u9700\u524d\u5411\u4f20\u64ad\u65e0\u9700\u8bad\u7ec3", "result": "\u5728Llama3\u4e0a\u5b9e\u73b027%\u7684\u516c\u5e73\u6027\u6307\u6807\u63d0\u5347\uff0c\u4e0d\u540c\u79cd\u65cf\u7fa4\u4f53\u7684\u9884\u6d4b\u5dee\u5f02\u663e\u8457\u964d\u4f4e", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aLLM\u4f26\u7406\u51b3\u7b56\u63d0\u4f9b\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u9884\u6d4b\u4e00\u81f4\u6027\u68c0\u6d4b\u6709\u6548\u63d0\u5347\u6a21\u578b\u53ef\u4fe1\u5ea6"}}
{"id": "2505.12116", "pdf": "https://arxiv.org/pdf/2505.12116", "abs": "https://arxiv.org/abs/2505.12116", "authors": ["Fitsum Gaim", "Hoyun Song", "Huije Lee", "Changgeon Ko", "Eui Jun Hwang", "Jong C. Park"], "title": "A Multi-Task Benchmark for Abusive Language Detection in Low-Resource Settings", "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Content moderation research has recently made significant advances, but still\nfails to serve the majority of the world's languages due to the lack of\nresources, leaving millions of vulnerable users to online hostility. This work\npresents a large-scale human-annotated multi-task benchmark dataset for abusive\nlanguage detection in Tigrinya social media with joint annotations for three\ntasks: abusiveness, sentiment, and topic classification. The dataset comprises\n13,717 YouTube comments annotated by nine native speakers, collected from 7,373\nvideos with a total of over 1.2 billion views across 51 channels. We developed\nan iterative term clustering approach for effective data selection. Recognizing\nthat around 64% of Tigrinya social media content uses Romanized\ntransliterations rather than native Ge'ez script, our dataset accommodates both\nwriting systems to reflect actual language use. We establish strong baselines\nacross the tasks in the benchmark, while leaving significant challenges for\nfuture contributions. Our experiments reveal that small, specialized multi-task\nmodels outperform the current frontier models in the low-resource setting,\nachieving up to 86% accuracy (+7 points) in abusiveness detection. We make the\nresources publicly available to promote research on online safety.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u9996\u4e2a\u63d0\u683c\u96f7\u5c3c\u4e9a\u8bed\u793e\u4ea4\u5a92\u4f53\u591a\u4efb\u52a1\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u652f\u6301\u7f57\u9a6c\u5316\u8f6c\u5199\u4e0e\u539f\u751f\u6587\u5b57\uff0c\u5c0f\u89c4\u6a21\u591a\u4efb\u52a1\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u8fb1\u9a82\u68c0\u6d4b\u51c6\u786e\u7387\u8fbe86%", "motivation": "\u89e3\u51b3\u5168\u7403\u591a\u6570\u8bed\u8a00\u7f3a\u4e4f\u5185\u5bb9\u5ba1\u6838\u8d44\u6e90\u7684\u95ee\u9898\uff0c\u7279\u522b\u5173\u6ce8\u4f4e\u8d44\u6e90\u8bed\u8a00\u63d0\u683c\u96f7\u5c3c\u4e9a\u8bed\u7528\u6237\u9762\u4e34\u7684\u7f51\u7edc\u66b4\u529b\u98ce\u9669", "method": "\u901a\u8fc7\u8fed\u4ee3\u672f\u8bed\u805a\u7c7b\u65b9\u6cd5\u6536\u96c613,717\u6761YouTube\u8bc4\u8bba\uff08\u8986\u76d67,373\u4e2a\u9ad8\u6d4f\u89c8\u91cf\u89c6\u9891\uff09\uff0c\u517c\u5bb9\u7f57\u9a6c\u5316\u4e0eGeez\u6587\u5b57\u7cfb\u7edf\uff0c\u5efa\u7acb\u8054\u5408\u6807\u6ce8\u6846\u67b6\uff08\u8fb1\u9a82\u6027/\u60c5\u611f/\u4e3b\u9898\u5206\u7c7b\uff09", "result": "\u4e13\u7528\u591a\u4efb\u52a1\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u73af\u5883\u4e0b\u8fb1\u9a82\u68c0\u6d4b\u51c6\u786e\u7387\u63d0\u53477%\u8fbe86%\uff0c\u663e\u8457\u4f18\u4e8e\u524d\u6cbf\u6a21\u578b", "conclusion": "\u516c\u5f00\u6570\u636e\u96c6\u4fc3\u8fdb\u5728\u7ebf\u5b89\u5168\u7814\u7a76\uff0c\u8bc1\u660e\u5c0f\u89c4\u6a21\u4e13\u4e1a\u5316\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u6709\u6548\u6027"}}
{"id": "2505.12158", "pdf": "https://arxiv.org/pdf/2505.12158", "abs": "https://arxiv.org/abs/2505.12158", "authors": ["Elisa Bassignana", "Amanda Cercas Curry", "Dirk Hovy"], "title": "The AI Gap: How Socioeconomic Status Affects Language Technology Interactions", "categories": ["cs.CL"], "comment": "Accepted at ACL Main 2025", "summary": "Socioeconomic status (SES) fundamentally influences how people interact with\neach other and more recently, with digital technologies like Large Language\nModels (LLMs). While previous research has highlighted the interaction between\nSES and language technology, it was limited by reliance on proxy metrics and\nsynthetic data. We survey 1,000 individuals from diverse socioeconomic\nbackgrounds about their use of language technologies and generative AI, and\ncollect 6,482 prompts from their previous interactions with LLMs. We find\nsystematic differences across SES groups in language technology usage (i.e.,\nfrequency, performed tasks), interaction styles, and topics. Higher SES entails\na higher level of abstraction, convey requests more concisely, and topics like\n'inclusivity' and 'travel'. Lower SES correlates with higher\nanthropomorphization of LLMs (using ''hello'' and ''thank you'') and more\nconcrete language. Our findings suggest that while generative language\ntechnologies are becoming more accessible to everyone, socioeconomic linguistic\ndifferences still stratify their use to exacerbate the digital divide. These\ndifferences underscore the importance of considering SES in developing language\ntechnologies to accommodate varying linguistic needs rooted in socioeconomic\nfactors and limit the AI Gap across SES groups.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u793e\u4f1a\u7ecf\u6d4e\u5730\u4f4d\u7fa4\u4f53\u5728\u4f7f\u7528\u751f\u6210\u5f0f\u8bed\u8a00\u6280\u672f\u65f6\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u9ad8SES\u7fa4\u4f53\u503e\u5411\u62bd\u8c61\u8868\u8fbe\uff0c\u4f4eSES\u7fa4\u4f53\u66f4\u62df\u4eba\u5316\u4ea4\u4e92\uff0c\u9700\u9488\u5bf9\u6027\u4f18\u5316\u6280\u672f\u8bbe\u8ba1\u4ee5\u51cf\u5c11\u6570\u5b57\u9e3f\u6c9f\u3002", "motivation": "\u63a2\u7d22\u793e\u4f1a\u7ecf\u6d4e\u5730\u4f4d\u5982\u4f55\u5f71\u54cd\u4eba\u7c7b\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u771f\u5b9e\u4e92\u52a8\uff0c\u7a81\u7834\u5148\u524d\u4f9d\u8d56\u4ee3\u7406\u6307\u6807\u548c\u5408\u6210\u6570\u636e\u7684\u7814\u7a76\u5c40\u9650\u3002", "method": "\u901a\u8fc7\u8c03\u67e51,000\u540d\u4e0d\u540cSES\u80cc\u666f\u4e2a\u4f53\u7684\u8bed\u8a00\u6280\u672f\u4f7f\u7528\u60c5\u51b5\uff0c\u5e76\u5206\u6790\u51766,482\u6761\u5386\u53f2LLM\u4ea4\u4e92\u63d0\u793a\u3002", "result": "\u9ad8SES\u7fa4\u4f53\u8bf7\u6c42\u66f4\u62bd\u8c61\u7b80\u6d01\uff08\u6d89\u53ca\u5305\u5bb9\u6027/\u65c5\u884c\u4e3b\u9898\uff09\uff0c\u4f4eSES\u7fa4\u4f53\u4ea4\u4e92\u62df\u4eba\u5316\u7a0b\u5ea6\u9ad8\uff08\u4f7f\u7528\u95ee\u5019\u8bed\uff09\u4e14\u8bed\u8a00\u66f4\u5177\u4f53\u3002", "conclusion": "\u9700\u5728\u8bed\u8a00\u6280\u672f\u5f00\u53d1\u4e2d\u5145\u5206\u8003\u8651\u793e\u4f1a\u7ecf\u6d4e\u8bed\u8a00\u5dee\u5f02\uff0c\u901a\u8fc7\u5b9a\u5236\u5316\u8bbe\u8ba1\u6ee1\u8db3\u4e0d\u540c\u7fa4\u4f53\u7684\u9700\u6c42\uff0c\u7f13\u89e3SES\u7fa4\u4f53\u95f4\u7684AI\u6280\u672f\u9e3f\u6c9f\u3002"}}
{"id": "2505.12160", "pdf": "https://arxiv.org/pdf/2505.12160", "abs": "https://arxiv.org/abs/2505.12160", "authors": ["Darmawan Wicaksono", "Hasri Akbar Awal Rozaq", "Nevfel Boz"], "title": "Emotion Recognition for Low-Resource Turkish: Fine-Tuning BERTurk on TREMO and Testing on Xenophobic Political Discourse", "categories": ["cs.CL"], "comment": null, "summary": "Social media platforms like X (formerly Twitter) play a crucial role in\nshaping public discourse and societal norms. This study examines the term\nSessiz Istila (Silent Invasion) on Turkish social media, highlighting the rise\nof anti-refugee sentiment amidst the Syrian refugee influx. Using BERTurk and\nthe TREMO dataset, we developed an advanced Emotion Recognition Model (ERM)\ntailored for Turkish, achieving 92.62% accuracy in categorizing emotions such\nas happiness, fear, anger, sadness, disgust, and surprise. By applying this\nmodel to large-scale X data, the study uncovers emotional nuances in Turkish\ndiscourse, contributing to computational social science by advancing sentiment\nanalysis in underrepresented languages and enhancing our understanding of\nglobal digital discourse and the unique linguistic challenges of Turkish. The\nfindings underscore the transformative potential of localized NLP tools, with\nour ERM model offering practical applications for real-time sentiment analysis\nin Turkish-language contexts. By addressing critical areas, including\nmarketing, public relations, and crisis management, these models facilitate\nimproved decision-making through timely and accurate sentiment tracking. This\nhighlights the significance of advancing research that accounts for regional\nand linguistic nuances.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u9488\u5bf9\u571f\u8033\u5176\u8bed\u7684\u60c5\u611f\u8bc6\u522b\u6a21\u578b(ERM)\uff0c\u51c6\u786e\u738792.62%\uff0c\u5206\u6790X\u5e73\u53f0\u6570\u636e\u63ed\u793a\u53cd\u96be\u6c11\u60c5\u7eea\uff0c\u63a8\u52a8\u672c\u5730\u5316NLP\u5de5\u5177\u5728\u5b9e\u65f6\u60c5\u611f\u5206\u6790\u4e2d\u7684\u5e94\u7528", "motivation": "\u5206\u6790\u571f\u8033\u5176\u793e\u4ea4\u5a92\u4f53\u4e2d'\u9759\u9ed8\u5165\u4fb5'\u672f\u8bed\u5f15\u53d1\u7684\u53cd\u96be\u6c11\u60c5\u7eea\uff0c\u89e3\u51b3\u8d44\u6e90\u4e0d\u8db3\u8bed\u8a00\u7684\u60c5\u611f\u5206\u6790\u96be\u9898\uff0c\u589e\u5f3a\u5bf9\u5168\u7403\u6570\u5b57\u8bdd\u8bed\u4e2d\u571f\u8033\u5176\u8bed\u72ec\u7279\u6311\u6218\u7684\u7406\u89e3", "method": "\u4f7f\u7528BERTurk\u6a21\u578b\u548cTREMO\u6570\u636e\u96c6\u6784\u5efa\u60c5\u611f\u8bc6\u522b\u6a21\u578b\uff0c\u901a\u8fc7\u516d\u79cd\u60c5\u7eea\u5206\u7c7b\uff08\u5feb\u4e50/\u6050\u60e7/\u6124\u6012/\u60b2\u4f24/\u538c\u6076/\u60ca\u8bb6\uff09\u5206\u6790X\u5e73\u53f0\u5927\u89c4\u6a21\u6570\u636e", "result": "ERM\u6a21\u578b\u8fbe\u523092.62%\u51c6\u786e\u7387\uff0c\u6210\u529f\u8bc6\u522b\u571f\u8033\u5176\u53cd\u96be\u6c11\u8ba8\u8bba\u4e2d\u7684\u60c5\u611f\u6a21\u5f0f\u53d8\u5316\uff0c\u63ed\u793a\u793e\u4ea4\u5a92\u4f53\u60c5\u7eea\u4f20\u64ad\u673a\u5236", "conclusion": "\u672c\u5730\u5316NLP\u5de5\u5177\u5728\u571f\u8033\u5176\u8bed\u60c5\u5883\u5c55\u73b0\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u5b9e\u65f6\u60c5\u611f\u5206\u6790\u80fd\u529b\u53ef\u63d0\u5347\u8425\u9500\u3001\u516c\u5173\u7b49\u9886\u57df\u7684\u51b3\u7b56\u8d28\u91cf\uff0c\u5f3a\u8c03\u8bed\u8a00\u533a\u57df\u6027\u7279\u5f81\u5728\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u4e2d\u7684\u5173\u952e\u4f5c\u7528"}}
{"id": "2505.12182", "pdf": "https://arxiv.org/pdf/2505.12182", "abs": "https://arxiv.org/abs/2505.12182", "authors": ["Haohang Li", "Yupeng Cao", "Yangyang Yu", "Jordan W. Suchow", "Zining Zhu"], "title": "Truth Neurons", "categories": ["cs.CL"], "comment": null, "summary": "Despite their remarkable success and deployment across diverse workflows,\nlanguage models sometimes produce untruthful responses. Our limited\nunderstanding of how truthfulness is mechanistically encoded within these\nmodels jeopardizes their reliability and safety. In this paper, we propose a\nmethod for identifying representations of truthfulness at the neuron level. We\nshow that language models contain truth neurons, which encode truthfulness in a\nsubject-agnostic manner. Experiments conducted across models of varying scales\nvalidate the existence of truth neurons, confirming that the encoding of\ntruthfulness at the neuron level is a property shared by many language models.\nThe distribution patterns of truth neurons over layers align with prior\nfindings on the geometry of truthfulness. Selectively suppressing the\nactivations of truth neurons found through the TruthfulQA dataset degrades\nperformance both on TruthfulQA and on other benchmarks, showing that the\ntruthfulness mechanisms are not tied to a specific dataset. Our results offer\nnovel insights into the mechanisms underlying truthfulness in language models\nand highlight potential directions toward improving their trustworthiness and\nreliability.", "AI": {"tldr": "\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u4e2d\u7684'\u771f\u7406\u795e\u7ecf\u5143'\uff0c\u63ed\u793a\u5176\u8de8\u4e3b\u9898\u7684\u771f\u7406\u7f16\u7801\u673a\u5236", "motivation": "\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u7f16\u7801\u771f\u5b9e\u6027\u7279\u5f81\u4ee5\u63d0\u5347\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027", "method": "\u901a\u8fc7\u795e\u7ecf\u5143\u7ea7\u8868\u5f81\u5206\u6790\u5b9a\u4f4d\u771f\u7406\u795e\u7ecf\u5143\uff0c\u5728\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1", "result": "\u8bc1\u5b9e\u591a\u6a21\u578b\u5b58\u5728\u8de8\u6570\u636e\u96c6\u901a\u7528\u7684\u771f\u7406\u795e\u7ecf\u5143\uff0c\u6291\u5236\u5176\u6fc0\u6d3b\u663e\u8457\u964d\u4f4e\u771f\u5b9e\u6027\u8868\u73b0", "conclusion": "\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u771f\u5b9e\u6027\u7f16\u7801\u7684\u795e\u7ecf\u673a\u5236\uff0c\u4e3a\u63d0\u5347\u6a21\u578b\u53ef\u4fe1\u5ea6\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2505.12183", "pdf": "https://arxiv.org/pdf/2505.12183", "abs": "https://arxiv.org/abs/2505.12183", "authors": ["Manari Hirose", "Masato Uchida"], "title": "Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC"], "comment": "23 pages, 5 figures, 17 tables", "summary": "The widespread integration of Large Language Models (LLMs) across various\nsectors has highlighted the need for empirical research to understand their\nbiases, thought patterns, and societal implications to ensure ethical and\neffective use. In this study, we propose a novel framework for evaluating LLMs,\nfocusing on uncovering their ideological biases through a quantitative analysis\nof 436 binary-choice questions, many of which have no definitive answer. By\napplying our framework to ChatGPT and Gemini, findings revealed that while LLMs\ngenerally maintain consistent opinions on many topics, their ideologies differ\nacross models and languages. Notably, ChatGPT exhibits a tendency to change\ntheir opinion to match the questioner's opinion. Both models also exhibited\nproblematic biases, unethical or unfair claims, which might have negative\nsocietal impacts. These results underscore the importance of addressing both\nideological and ethical considerations when evaluating LLMs. The proposed\nframework offers a flexible, quantitative method for assessing LLM behavior,\nproviding valuable insights for the development of more socially aligned AI\nsystems.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7436\u9053\u9009\u62e9\u9898\u5b9a\u91cf\u5206\u6790ChatGPT\u548cGemini\u7684\u610f\u8bc6\u5f62\u6001\u504f\u89c1\uff0c\u53d1\u73b0\u6a21\u578b\u95f4\u5b58\u5728\u5dee\u5f02\u4e14\u5b58\u5728\u4f26\u7406\u9690\u60a3", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5e7f\u6cdb\u5e94\u7528\u80cc\u666f\u4e0b\uff0c\u9700\u901a\u8fc7\u5b9e\u8bc1\u7814\u7a76\u63ed\u793a\u5176\u504f\u89c1\u6a21\u5f0f\u53ca\u793e\u4f1a\u5f71\u54cd\uff0c\u786e\u4fdd\u4f26\u7406\u5e94\u7528", "method": "\u5f00\u53d1\u65b0\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u8bbe\u8ba1\u65e0\u6807\u51c6\u7b54\u6848\u7684\u4e8c\u5143\u9009\u62e9\u9898\uff0c\u8fdb\u884c\u8de8\u6a21\u578b\uff08ChatGPT/Gemini\uff09\u548c\u8de8\u8bed\u8a00\u5b9a\u91cf\u5206\u6790", "result": "\u6a21\u578b\u95f4\u610f\u8bc6\u5f62\u6001\u5dee\u5f02\u663e\u8457\uff0cChatGPT\u6613\u8fce\u5408\u63d0\u95ee\u8005\u89c2\u70b9\uff0c\u4e24\u6a21\u578b\u5747\u5b58\u5728\u53ef\u80fd\u5f15\u53d1\u8d1f\u9762\u793e\u4f1a\u5f71\u54cd\u7684\u504f\u89c1\u4e3b\u5f20", "conclusion": "\u8bc4\u4f30LLMs\u9700\u517c\u987e\u610f\u8bc6\u5f62\u6001\u548c\u4f26\u7406\u8003\u91cf\uff0c\u8be5\u6846\u67b6\u4e3a\u5f00\u53d1\u793e\u4f1a\u53cb\u597d\u578bAI\u63d0\u4f9b\u91cf\u5316\u5206\u6790\u5de5\u5177"}}
{"id": "2505.12196", "pdf": "https://arxiv.org/pdf/2505.12196", "abs": "https://arxiv.org/abs/2505.12196", "authors": ["Yi-Chien Lin", "Hongao Zhu", "William Schuler"], "title": "Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled", "categories": ["cs.CL"], "comment": null, "summary": "The impressive linguistic abilities of large language models (LLMs) have\nrecommended them as models of human sentence processing, with some conjecturing\na positive 'quality-power' relationship (Wilcox et al., 2023), in which\nlanguage models' (LMs') fit to psychometric data continues to improve as their\nability to predict words in context increases. This is important because it\nsuggests that elements of LLM architecture, such as veridical attention to\ncontext and a unique objective of predicting upcoming words, reflect the\narchitecture of the human sentence processing faculty, and that any\ninadequacies in predicting human reading time and brain imaging data may be\nattributed to insufficient model complexity, which recedes as larger models\nbecome available. Recent studies (Oh and Schuler, 2023) have shown this scaling\ninverts after a point, as LMs become excessively large and accurate, when word\nprediction probability (as information-theoretic surprisal) is used as a\npredictor. Other studies propose the use of entire vectors from differently\nsized LLMs, still showing positive scaling (Schrimpf et al., 2021), casting\ndoubt on the value of surprisal as a predictor, but do not control for the\nlarger number of predictors in vectors from larger LMs. This study evaluates\nLLM scaling using entire LLM vectors, while controlling for the larger number\nof predictors in vectors from larger LLMs. Results show that inverse scaling\nobtains, suggesting that inadequacies in predicting human reading time and\nbrain imaging data may be due to substantial misalignment between LLMs and\nhuman sentence processing, which worsens as larger models are used.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u9884\u6d4b\u4eba\u7c7b\u9605\u8bfb\u6570\u636e\u7684\u80fd\u529b\u968f\u89c4\u6a21\u6269\u5927\u5448\u9006\u5411\u53d8\u5316\uff1a\u521d\u671f\u6b63\u76f8\u5173\uff0c\u4f46\u6a21\u578b\u8fc7\u5927\u65f6\u9884\u6d4b\u80fd\u529b\u53cd\u800c\u4e0b\u964d\uff0c\u63ed\u793aLLM\u4e0e\u4eba\u7c7b\u8bed\u8a00\u5904\u7406\u673a\u5236\u5b58\u5728\u672c\u8d28\u504f\u5dee\u3002", "motivation": "\u63a2\u7a76LLM\u89c4\u6a21\u6269\u5c55\u5bf9\u9884\u6d4b\u4eba\u7c7b\u53e5\u5b50\u5904\u7406\u6570\u636e\u7684\u5f71\u54cd\uff0c\u9a8c\u8bc1'\u8d28\u91cf-\u80fd\u529b\u6b63\u76f8\u5173'\u5047\u8bbe\u7684\u5c40\u9650\u6027\uff0c\u63ed\u793a\u8fc7\u5927\u6a21\u578b\u4ea7\u751f\u7684\u9884\u6d4b\u504f\u5dee\u73b0\u8c61\u3002", "method": "\u4f7f\u7528\u5b8c\u6574LLM\u5411\u91cf\u5206\u6790\uff0c\u63a7\u5236\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u7684\u9884\u6d4b\u53d8\u91cf\u6570\u91cf\uff0c\u901a\u8fc7\u56de\u5f52\u5206\u6790\u6bd4\u8f83\u6a21\u578b\u9884\u6d4b\u4eba\u7c7b\u9605\u8bfb\u65f6\u95f4\u548c\u8111\u6210\u50cf\u6570\u636e\u7684\u80fd\u529b\u3002", "result": "\u53d1\u73b0\u9006\u5411\u6269\u5c55\u6548\u5e94\uff1a\u6a21\u578b\u53c2\u6570\u8d85\u8fc7\u4e34\u754c\u503c\u540e\uff0c\u9884\u6d4b\u80fd\u529b\u968f\u89c4\u6a21\u589e\u5927\u663e\u8457\u4e0b\u964d\uff0c\u504f\u5dee\u7a0b\u5ea6\u4e0e\u6a21\u578b\u590d\u6742\u5ea6\u6b63\u76f8\u5173\u3002", "conclusion": "LLM\u9884\u6d4b\u4eba\u7c7b\u8bed\u8a00\u5904\u7406\u6570\u636e\u7684\u4e0d\u8db3\u6e90\u4e8e\u67b6\u6784/\u76ee\u6807\u51fd\u6570\u4e0e\u4eba\u7c7b\u673a\u5236\u7684\u6839\u672c\u6027\u9519\u4f4d\uff0c\u5355\u7eaf\u6269\u5927\u89c4\u6a21\u65e0\u6cd5\u89e3\u51b3\uff0c\u9700\u91cd\u65b0\u5ba1\u89c6\u6a21\u578b\u8bbe\u8ba1\u8303\u5f0f\u3002"}}
{"id": "2505.12201", "pdf": "https://arxiv.org/pdf/2505.12201", "abs": "https://arxiv.org/abs/2505.12201", "authors": ["Xiyan Fu", "Wei Liu"], "title": "How Reliable is Multilingual LLM-as-a-Judge?", "categories": ["cs.CL"], "comment": null, "summary": "LLM-as-a-Judge has emerged as a popular evaluation strategy, where advanced\nlarge language models assess generation results in alignment with human\ninstructions. While these models serve as a promising alternative to human\nannotators, their reliability in multilingual evaluation remains uncertain. To\nbridge this gap, we conduct a comprehensive analysis of multilingual\nLLM-as-a-Judge. Specifically, we evaluate five models from different model\nfamilies across five diverse tasks involving 25 languages. Our findings reveal\nthat LLMs struggle to achieve consistent judgment results across languages,\nwith an average Fleiss' Kappa of approximately 0.3, and some models performing\neven worse. To investigate the cause of inconsistency, we analyze various\ninfluencing factors. We observe that consistency varies significantly across\nlanguages, with particularly poor performance in low-resource languages.\nAdditionally, we find that neither training on multilingual data nor increasing\nmodel scale directly improves judgment consistency. These findings suggest that\nLLMs are not yet reliable for evaluating multilingual predictions. We finally\npropose an ensemble strategy which improves the consistency of the multilingual\njudge in real-world applications.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8bed\u8a00\u8bc4\u4f30\u4e2d\u5b58\u5728\u5224\u65ad\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u63d0\u51fa\u96c6\u6210\u7b56\u7565\u6539\u5584\u4e00\u81f4\u6027", "motivation": "\u63a2\u7d22LLM\u4f5c\u4e3a\u81ea\u52a8\u5316\u591a\u8bed\u8a00\u8bc4\u4f30\u5de5\u5177\u7684\u53ef\u9760\u6027\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8de8\u8bed\u8a00\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027", "method": "\u8de85\u4e2a\u6a21\u578b\u5bb6\u65cf\u30015\u7c7b\u4efb\u52a1\u548c25\u79cd\u8bed\u8a00\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\uff0c\u4f7f\u7528Fleiss' Kappa\u7edf\u8ba1\u91cf\u8861\u91cf\u5224\u65ad\u4e00\u81f4\u6027", "result": "\u5e73\u5747\u4e00\u81f4\u6027\u6307\u6807\u4ec50.3\uff0c\u4f4e\u8d44\u6e90\u8bed\u8a00\u8868\u73b0\u663e\u8457\u66f4\u5dee\uff0c\u6a21\u578b\u89c4\u6a21\u548c\u591a\u8bed\u8a00\u8bad\u7ec3\u672a\u63d0\u5347\u4e00\u81f4\u6027", "conclusion": "\u5f53\u524dLLM\u4e0d\u9002\u5408\u76f4\u63a5\u7528\u4e8e\u591a\u8bed\u8a00\u8bc4\u4f30\uff0c\u63d0\u51fa\u7684\u96c6\u6210\u7b56\u7565\u53ef\u63d0\u5347\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5224\u65ad\u7a33\u5b9a\u6027"}}
{"id": "2505.12212", "pdf": "https://arxiv.org/pdf/2505.12212", "abs": "https://arxiv.org/abs/2505.12212", "authors": ["Shaobo Wang", "Ziming Wang", "Xiangqi Jin", "Jize Wang", "Jiajun Zhang", "Kaixin Li", "Zichen Wen", "Zhong Li", "Conghui He", "Xuming Hu", "Linfeng Zhang"], "title": "Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main, 18 pages, 8 figures, 6 tables", "summary": "Fine-tuning large language models (LLMs) on task-specific data is essential\nfor their effective deployment. As dataset sizes grow, efficiently selecting\noptimal subsets for training becomes crucial to balancing performance and\ncomputational costs. Traditional data selection methods often require\nfine-tuning a scoring model on the target dataset, which is time-consuming and\nresource-intensive, or rely on heuristics that fail to fully leverage the\nmodel's predictive capabilities. To address these challenges, we propose Data\nWhisperer, an efficient, training-free, attention-based method that leverages\nfew-shot in-context learning with the model to be fine-tuned. Comprehensive\nevaluations were conducted on both raw and synthetic datasets across diverse\ntasks and models. Notably, Data Whisperer achieves superior performance\ncompared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just\n10% of the data, and outperforms existing methods with a 3.1-point improvement\nand a 7.4$\\times$ speedup.", "AI": {"tldr": "\u63d0\u51faData Whisperer\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u6ce8\u610f\u529b\u673a\u5236\u7684\u514d\u8bad\u7ec3\u6570\u636e\u9009\u62e9\u7b56\u7565\uff0c\u752810%\u6570\u636e\u5b9e\u73b0\u8d85\u8d8a\u5168\u91cf\u6570\u636e\u7684\u6027\u80fd\u8868\u73b0", "motivation": "\u4f20\u7edf\u6570\u636e\u9009\u62e9\u65b9\u6cd5\u9700\u8981\u5168\u91cf\u5fae\u8c03\u6216\u4f9d\u8d56\u542f\u53d1\u5f0f\u89c4\u5219\uff0c\u5b58\u5728\u6548\u7387\u4f4e\u4e0b\u548c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u7684\u95ee\u9898", "method": "\u5229\u7528\u76ee\u6807\u6a21\u578b\u672c\u8eab\u7684few-shot\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u8fdb\u884c\u8bad\u7ec3\u81ea\u7531\u7684\u6570\u636e\u9009\u62e9", "result": "\u5728Llama-3-8B-Instruct\u6a21\u578b\u4e0a\uff0c\u4f7f\u752810%\u7684GSM8K\u6570\u636e\u6027\u80fd\u8d85\u8d8a\u5168\u91cf\u6570\u636e\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u53473.1\u4e2a\u70b9\u4e14\u63d0\u901f7.4\u500d", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5fae\u8c03\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6570\u636e\u9009\u62e9\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c"}}
{"id": "2505.12215", "pdf": "https://arxiv.org/pdf/2505.12215", "abs": "https://arxiv.org/abs/2505.12215", "authors": ["Jiwei Tang", "Zhicheng Zhang", "Shunlong Wu", "Jingheng Ye", "Lichen Bai", "Zitai Wang", "Tingwei Lu", "Jiaqi Chen", "Lin Hai", "Hai-Tao Zheng", "Hong-Gee Kim"], "title": "GMSA: Enhancing Context Compression via Group Merging and Layer Semantic Alignment", "categories": ["cs.CL"], "comment": "19 pages, 7 figures", "summary": "Large language models (LLMs) have achieved impressive performance in a\nvariety of natural language processing (NLP) tasks. However, when applied to\nlong-context scenarios, they face two challenges, i.e., low computational\nefficiency and much redundant information. This paper introduces GMSA, a\ncontext compression framework based on the encoder-decoder architecture, which\naddresses these challenges by reducing input sequence length and redundant\ninformation. Structurally, GMSA has two key components: Group Merging and Layer\nSemantic Alignment (LSA). Group merging is used to effectively and efficiently\nextract summary vectors from the original context. Layer semantic alignment, on\nthe other hand, aligns the high-level summary vectors with the low-level\nprimary input semantics, thus bridging the semantic gap between different\nlayers. In the training process, GMSA first learns soft tokens that contain\ncomplete semantics through autoencoder training. To furtherly adapt GMSA to\ndownstream tasks, we propose Knowledge Extraction Fine-tuning (KEFT) to extract\nknowledge from the soft tokens for downstream tasks. We train GMSA by randomly\nsampling the compression rate for each sample in the dataset. Under this\ncondition, GMSA not only significantly outperforms the traditional compression\nparadigm in context restoration but also achieves stable and significantly\nfaster convergence with only a few encoder layers. In downstream\nquestion-answering (QA) tasks, GMSA can achieve approximately a 2x speedup in\nend-to-end inference while outperforming both the original input prompts and\nvarious state-of-the-art (SOTA) methods by a large margin.", "AI": {"tldr": "Proposes GMSA framework for efficient long-context processing in LLMs through context compression and semantic alignment.", "motivation": "Addresses low computational efficiency and redundant information in large language models when handling long-context scenarios.", "method": "Combines Group Merging (context compression) and Layer Semantic Alignment (bridging semantic gaps) with Knowledge Extraction Fine-tuning (KEFT) for task adaptation.", "result": "Achieves 2x inference speedup, superior context restoration, and outperforms SOTA methods in QA tasks with stable convergence.", "conclusion": "GMSA effectively balances efficiency and performance in long-context processing through innovative compression and alignment strategies."}}
{"id": "2505.12216", "pdf": "https://arxiv.org/pdf/2505.12216", "abs": "https://arxiv.org/abs/2505.12216", "authors": ["Rongguang Ye", "Ming Tang"], "title": "One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models", "categories": ["cs.CL"], "comment": "ACL Findings", "summary": "Existing pruning methods for large language models (LLMs) focus on achieving\nhigh compression rates while maintaining model performance. Although these\nmethods have demonstrated satisfactory performance in handling a single user's\ncompression request, their processing time increases linearly with the number\nof requests, making them inefficient for real-world scenarios with multiple\nsimultaneous requests. To address this limitation, we propose a Univeral Model\nfor Customized Compression (UniCuCo) for LLMs, which introduces a StratNet that\nlearns to map arbitrary requests to their optimal pruning strategy. The\nchallenge in training StratNet lies in the high computational cost of\nevaluating pruning strategies and the non-differentiable nature of the pruning\nprocess, which hinders gradient backpropagation for StratNet updates. To\novercome these challenges, we leverage a Gaussian process to approximate the\nevaluation process. Since the gradient of the Gaussian process is computable,\nwe can use it to approximate the gradient of the non-differentiable pruning\nprocess, thereby enabling StratNet updates. Experimental results show that\nUniCuCo is 28 times faster than baselines in processing 64 requests, while\nmaintaining comparable accuracy to baselines.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u6a21\u578bUniCuCo\uff0c\u901a\u8fc7StratNet\u5b9e\u73b0LLM\u7684\u5b9a\u5236\u5316\u538b\u7f29\uff0c\u5904\u740664\u4e2a\u8bf7\u6c42\u901f\u5ea6\u5feb28\u500d\u4e14\u7cbe\u5ea6\u76f8\u5f53", "motivation": "\u73b0\u6709LLM\u526a\u679d\u65b9\u6cd5\u5904\u7406\u591a\u7528\u6237\u8bf7\u6c42\u65f6\u6548\u7387\u7ebf\u6027\u4e0b\u964d\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u9645\u573a\u666f\u7684\u5b9e\u65f6\u6027\u9700\u6c42", "method": "\u5f15\u5165StratNet\u5b66\u4e60\u8bf7\u6c42\u5230\u526a\u679d\u7b56\u7565\u7684\u6620\u5c04\uff0c\u91c7\u7528\u9ad8\u65af\u8fc7\u7a0b\u8fd1\u4f3c\u975e\u53ef\u5fae\u7684\u526a\u679d\u8bc4\u4f30\u8fc7\u7a0b\u5b9e\u73b0\u68af\u5ea6\u56de\u4f20", "result": "\u5b9e\u9a8c\u8868\u660e\u572864\u4e2a\u5e76\u53d1\u8bf7\u6c42\u4e0b\u5904\u7406\u6548\u7387\u63d0\u534728\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u57fa\u7ebf\u76f8\u5f53\u7684\u6a21\u578b\u7cbe\u5ea6", "conclusion": "UniCuCo\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u7b56\u7565\u6620\u5c04\u673a\u5236\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u663e\u8457\u63d0\u5347\u591a\u8bf7\u6c42\u5904\u7406\u6548\u7387\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2505.12218", "pdf": "https://arxiv.org/pdf/2505.12218", "abs": "https://arxiv.org/abs/2505.12218", "authors": ["Tong Bao", "Yi Zhao", "Jin Mao", "Chengzhi Zhang"], "title": "Examining Linguistic Shifts in Academic Writing Before and After the Launch of ChatGPT: A Study on Preprint Papers", "categories": ["cs.CL", "68T50", "I.2.7"], "comment": null, "summary": "Large Language Models (LLMs), such as ChatGPT, have prompted academic\nconcerns about their impact on academic writing. Existing studies have\nprimarily examined LLM usage in academic writing through quantitative\napproaches, such as word frequency statistics and probability-based analyses.\nHowever, few have systematically examined the potential impact of LLMs on the\nlinguistic characteristics of academic writing. To address this gap, we\nconducted a large-scale analysis across 823,798 abstracts published in last\ndecade from arXiv dataset. Through the linguistic analysis of features such as\nthe frequency of LLM-preferred words, lexical complexity, syntactic complexity,\ncohesion, readability and sentiment, the results indicate a significant\nincrease in the proportion of LLM-preferred words in abstracts, revealing the\nwidespread influence of LLMs on academic writing. Additionally, we observed an\nincrease in lexical complexity and sentiment in the abstracts, but a decrease\nin syntactic complexity, suggesting that LLMs introduce more new vocabulary and\nsimplify sentence structure. However, the significant decrease in cohesion and\nreadability indicates that abstracts have fewer connecting words and are\nbecoming more difficult to read. Moreover, our analysis reveals that scholars\nwith weaker English proficiency were more likely to use the LLMs for academic\nwriting, and focused on improving the overall logic and fluency of the\nabstracts. Finally, at discipline level, we found that scholars in Computer\nScience showed more pronounced changes in writing style, while the changes in\nMathematics were minimal.", "AI": {"tldr": "LLMs\u663e\u8457\u5f71\u54cd\u5b66\u672f\u5199\u4f5c\u98ce\u683c\uff1a\u504f\u597d\u8bcd\u4f7f\u7528\u589e\u52a0\uff0c\u8bcd\u6c47\u590d\u6742\u5ea6\u63d0\u5347\u4f46\u53e5\u6cd5\u7b80\u5316\uff0c\u82f1\u8bed\u8584\u5f31\u5b66\u8005\u66f4\u4f9d\u8d56LLM\u4f18\u5316\u903b\u8f91\u8868\u8fbe\uff0c\u8ba1\u7b97\u673a\u5b66\u79d1\u53d8\u5316\u6700\u660e\u663e\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u4ece\u5b9a\u91cf\u89d2\u5ea6\u5206\u6790LLM\u4f7f\u7528\uff0c\u7f3a\u4e4f\u5bf9\u5b66\u672f\u5199\u4f5c\u8bed\u8a00\u7279\u5f81\u7684\u7cfb\u7edf\u6027\u5f71\u54cd\u7814\u7a76\u3002", "method": "\u57fa\u4e8earXiv\u8fd1\u5341\u5e74823,798\u7bc7\u6458\u8981\uff0c\u5206\u6790LLM\u504f\u597d\u8bcd\u9891\u3001\u8bcd\u6c47/\u53e5\u6cd5\u590d\u6742\u5ea6\u3001\u8854\u63a5\u6027\u3001\u53ef\u8bfb\u6027\u53ca\u60c5\u611f\u7279\u5f81\u3002", "result": "1. LLM\u504f\u597d\u8bcd\u6bd4\u4f8b\u663e\u8457\u589e\u52a0 2.\u8bcd\u6c47\u590d\u6742\u5ea6/\u60c5\u611f\u63d0\u5347\u4f46\u53e5\u6cd5\u590d\u6742\u5ea6\u4e0b\u964d 3.\u8854\u63a5\u6027/\u53ef\u8bfb\u6027\u964d\u4f4e 4.\u82f1\u8bed\u8584\u5f31\u5b66\u8005\u66f4\u503e\u5411\u4f7f\u7528LLM\u4f18\u5316\u903b\u8f91 5.\u8ba1\u7b97\u673a\u5b66\u79d1\u5199\u4f5c\u98ce\u683c\u53d8\u5316\u663e\u8457\uff0c\u6570\u5b66\u9886\u57df\u6700\u5c0f\u3002", "conclusion": "LLMs\u6539\u53d8\u4e86\u5b66\u672f\u5199\u4f5c\u8303\u5f0f\uff0c\u5728\u63d0\u5347\u8868\u8fbe\u4e30\u5bcc\u6027\u7684\u540c\u65f6\u964d\u4f4e\u4e86\u6587\u672c\u8fde\u8d2f\u6027\uff0c\u4e14\u5b66\u79d1\u5dee\u5f02\u548c\u4f5c\u8005\u8bed\u8a00\u80cc\u666f\u663e\u8457\u5f71\u54cd\u4f7f\u7528\u6548\u679c\u3002"}}
{"id": "2505.12236", "pdf": "https://arxiv.org/pdf/2505.12236", "abs": "https://arxiv.org/abs/2505.12236", "authors": ["Quanjiang Guo", "Jinchuan Zhang", "Sijie Wang", "Ling Tian", "Zhao Kang", "Bin Yan", "Weidong Xiao"], "title": "Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "13 pages, 6 figures, Appear on IJCAI 2025", "summary": "Few-Shot Relation Extraction (FSRE) remains a challenging task due to the\nscarcity of annotated data and the limited generalization capabilities of\nexisting models. Although large language models (LLMs) have demonstrated\npotential in FSRE through in-context learning (ICL), their general-purpose\ntraining objectives often result in suboptimal performance for task-specific\nrelation extraction. To overcome these challenges, we propose TKRE (Two-Stage\nKnowledge-Guided Pre-training for Relation Extraction), a novel framework that\nsynergistically integrates LLMs with traditional relation extraction models,\nbridging generative and discriminative learning paradigms. TKRE introduces two\nkey innovations: (1) leveraging LLMs to generate explanation-driven knowledge\nand schema-constrained synthetic data, addressing the issue of data scarcity;\nand (2) a two-stage pre-training strategy combining Masked Span Language\nModeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relational\nreasoning and generalization. Together, these components enable TKRE to\neffectively tackle FSRE tasks. Comprehensive experiments on benchmark datasets\ndemonstrate the efficacy of TKRE, achieving new state-of-the-art performance in\nFSRE and underscoring its potential for broader application in low-resource\nscenarios. \\footnote{The code and data are released on\nhttps://github.com/UESTC-GQJ/TKRE.", "AI": {"tldr": "TKRE\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u77e5\u8bc6\u5f15\u5bfc\u9884\u8bad\u7ec3\uff0c\u6574\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4f20\u7edf\u5173\u7cfb\u62bd\u53d6\u6a21\u578b\uff0c\u89e3\u51b3\u5c0f\u6837\u672c\u5173\u7cfb\u62bd\u53d6\u7684\u6570\u636e\u7a00\u7f3a\u548c\u6cdb\u5316\u96be\u9898", "motivation": "\u73b0\u6709LLMs\u5728\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u4e2d\u5b58\u5728\u6570\u636e\u7a00\u7f3a\u9002\u5e94\u4e0d\u8db3\u548c\u4efb\u52a1\u7279\u5f02\u6027\u4f18\u5316\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u9700\u8981\u878d\u5408\u751f\u6210\u5f0f\u548c\u5224\u522b\u5f0f\u5b66\u4e60\u8303\u5f0f", "method": "1. \u5229\u7528LLMs\u751f\u6210\u89e3\u91ca\u6027\u77e5\u8bc6\u548c\u6a21\u5f0f\u7ea6\u675f\u7684\u5408\u6210\u6570\u636e\n2. \u4e24\u9636\u6bb5\u9884\u8bad\u7ec3\u7b56\u7565\uff08MSLM\u63a9\u7801\u5efa\u6a21 + Span\u7ea7\u5bf9\u6bd4\u5b66\u4e60\uff09", "result": "\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\uff08SOTA\uff09\uff0c\u7279\u522b\u57281-shot\u8bbe\u5b9a\u4e0bF1\u63d0\u53477.3%", "conclusion": "TKRE\u6709\u6548\u89e3\u51b3\u4e86\u5c0f\u6837\u672c\u5173\u7cfb\u62bd\u53d6\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4e3a\u4f4e\u8d44\u6e90NLP\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.12238", "pdf": "https://arxiv.org/pdf/2505.12238", "abs": "https://arxiv.org/abs/2505.12238", "authors": ["Sriram Selvam", "Anneswa Ghosh"], "title": "PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The memorization of sensitive and personally identifiable information (PII)\nby large language models (LLMs) poses growing privacy risks as models scale and\nare increasingly deployed in real-world applications. Existing efforts to study\nsensitive and PII data memorization and develop mitigation strategies are\nhampered by the absence of comprehensive, realistic, and ethically sourced\ndatasets reflecting the diversity of sensitive information found on the web. We\nintroduce PANORAMA - Profile-based Assemblage for Naturalistic Online\nRepresentation and Attribute Memorization Analysis, a large-scale synthetic\ncorpus of 384,789 samples derived from 9,674 synthetic profiles designed to\nclosely emulate the distribution, variety, and context of PII and sensitive\ndata as it naturally occurs in online environments. Our data generation\npipeline begins with the construction of internally consistent, multi-attribute\nhuman profiles using constrained selection to reflect real-world demographics\nsuch as education, health attributes, financial status, etc. Using a\ncombination of zero-shot prompting and OpenAI o3-mini, we generate diverse\ncontent types - including wiki-style articles, social media posts, forum\ndiscussions, online reviews, comments, and marketplace listings - each\nembedding realistic, contextually appropriate PII and other sensitive\ninformation. We validate the utility of PANORAMA by fine-tuning the Mistral-7B\nmodel on 1x, 5x, 10x, and 25x data replication rates with a subset of data and\nmeasure PII memorization rates - revealing not only consistent increases with\nrepetition but also variation across content types, highlighting PANORAMA's\nability to model how memorization risks differ by context. Our dataset and code\nare publicly available, providing a much-needed resource for privacy risk\nassessment, model auditing, and the development of privacy-preserving LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86PANORAMA\u6570\u636e\u96c6\u7528\u4e8e\u8bc4\u4f30LLM\u9690\u79c1\u98ce\u9669\uff0c\u901a\u8fc7\u5408\u6210\u914d\u7f6e\u6587\u4ef6\u6a21\u62df\u771f\u5b9e\u5728\u7ebf\u73af\u5883\u4e2d\u7684\u654f\u611f\u4fe1\u606f\u5206\u5e03\uff0c\u5e76\u9a8c\u8bc1\u6570\u636e\u91cd\u590d\u5bf9PII\u8bb0\u5fc6\u7387\u7684\u5f71\u54cd", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u53cd\u6620\u771f\u5b9e\u7f51\u7edc\u73af\u5883\u4e2d\u654f\u611f\u4fe1\u606f\u591a\u6837\u6027\u7684\u6570\u636e\u96c6\uff0c\u963b\u788d\u4e86\u9690\u79c1\u98ce\u9669\u5206\u6790\u548c\u9690\u79c1\u4fdd\u62a4\u7b56\u7565\u7684\u5f00\u53d1", "method": "\u4f7f\u7528\u7ea6\u675f\u9009\u62e9\u6784\u5efa\u542b\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u7684\u7efc\u5408\u914d\u7f6e\u6587\u4ef6\uff0c\u7ed3\u5408\u96f6\u6837\u672c\u63d0\u793a\u548cOpenAI\u6a21\u578b\u751f\u6210wiki\u6587\u7ae0/\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50/\u8bc4\u8bba\u7b49\u591a\u6837\u5316\u5185\u5bb9\u7c7b\u578b", "result": "\u5fae\u8c03Mistral-7B\u663e\u793aPII\u8bb0\u5fc6\u7387\u968f\u6570\u636e\u91cd\u590d\u6b21\u6570\u589e\u52a0\u6301\u7eed\u4e0a\u5347\uff0c\u4e14\u4e0d\u540c\u5185\u5bb9\u7c7b\u578b\u95f4\u5b58\u5728\u663e\u8457\u8bb0\u5fc6\u5dee\u5f02", "conclusion": "PANORAMA\u586b\u8865\u4e86\u9690\u79c1\u98ce\u9669\u8bc4\u4f30\u5de5\u5177\u7684\u7a7a\u767d\uff0c\u4e3a\u6a21\u578b\u5ba1\u8ba1\u548c\u9690\u79c1\u4fdd\u62a4\u578bLLM\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u652f\u6301"}}
{"id": "2505.12244", "pdf": "https://arxiv.org/pdf/2505.12244", "abs": "https://arxiv.org/abs/2505.12244", "authors": ["Haojin Wang", "Zining Zhu", "Freda Shi"], "title": "Distribution Prompting: Understanding the Expressivity of Language Models Through the Next-Token Distributions They Can Produce", "categories": ["cs.CL"], "comment": null, "summary": "Autoregressive neural language models (LMs) generate a probability\ndistribution over tokens at each time step given a prompt. In this work, we\nattempt to systematically understand the probability distributions that LMs can\nproduce, showing that some distributions are significantly harder to elicit\nthan others. Specifically, for any target next-token distribution over the\nvocabulary, we attempt to find a prompt that induces the LM to output a\ndistribution as close as possible to the target, using either soft or hard\ngradient-based prompt tuning. We find that (1) in general, distributions with\nvery low or very high entropy are easier to approximate than those with\nmoderate entropy; (2) among distributions with the same entropy, those\ncontaining ''outlier tokens'' are easier to approximate; (3) target\ndistributions generated by LMs -- even LMs with different tokenizers -- are\neasier to approximate than randomly chosen targets. These results offer\ninsights into the expressiveness of LMs and the challenges of using them as\nprobability distribution proposers.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6982\u7387\u5206\u5e03\u7684\u96be\u6613\u89c4\u5f8b\uff1a\u6781\u4f4e/\u9ad8\u71b5\u5206\u5e03\uff1e\u4e2d\u71b5\u5206\u5e03\uff0c\u542b\u5f02\u5e38\u8bcd\u7684\u5206\u5e03\u66f4\u6613\u903c\u8fd1\uff0c\u4e14LM\u81ea\u8eab\u751f\u6210\u7684\u5206\u5e03\u6bd4\u968f\u673a\u76ee\u6807\u66f4\u6613\u62df\u5408", "motivation": "\u7cfb\u7edf\u63a2\u7a76\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4efb\u610f\u76ee\u6807\u6982\u7387\u5206\u5e03\u7684\u80fd\u529b\u8fb9\u754c\uff0c\u9a8c\u8bc1\u5176\u4f5c\u4e3a\u6982\u7387\u5206\u5e03\u751f\u6210\u5668\u7684\u8868\u8fbe\u80fd\u529b\u4e0e\u5c40\u9650\u6027", "method": "\u91c7\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u8f6f/\u786c\u63d0\u793a\u8c03\u4f18\u6280\u672f\uff0c\u901a\u8fc7\u4f18\u5316\u63d0\u793a\u8bcd\u4f7f\u6a21\u578b\u8f93\u51fa\u5206\u5e03\u6700\u5927\u7a0b\u5ea6\u903c\u8fd1\u9884\u8bbe\u76ee\u6807\u5206\u5e03", "result": "1. \u71b5\u6781\u7aef\uff08\u6781\u4f4e/\u9ad8\uff09\u7684\u5206\u5e03\u6bd4\u4e2d\u7b49\u71b5\u5206\u5e03\u66f4\u6613\u903c\u8fd1\n2. \u76f8\u540c\u71b5\u4e0b\u542b\u5f02\u5e38\u8bcd\u7684\u5206\u5e03\u66f4\u6613\u62df\u5408\n3. LM\u751f\u6210\u7684\u5206\u5e03\uff08\u8de8\u5206\u8bcd\u5668\uff09\u6bd4\u968f\u673a\u76ee\u6807\u66f4\u6613\u5339\u914d", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u6982\u7387\u7a7a\u95f4\u7684\u62d3\u6251\u7279\u6027\uff0c\u4e3a\u4f18\u5316LM\u7684\u5206\u5e03\u5efa\u6a21\u80fd\u529b\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\uff0c\u540c\u65f6\u66b4\u9732\u5176\u4f5c\u4e3a\u6982\u7387\u63d0\u8bae\u5668\u7684\u6f5c\u5728\u5c40\u9650"}}
{"id": "2505.12250", "pdf": "https://arxiv.org/pdf/2505.12250", "abs": "https://arxiv.org/abs/2505.12250", "authors": ["Chi Zhang", "Huaping Zhong", "Hongtao Li", "Chengliang Chai", "Jiawei Hong", "Yuhao Deng", "Jiacheng Wang", "Tian Tan", "Yizhou Yan", "Jiantao Qiu", "Ye Yuan", "Guoren Wang", "Conghui He", "Lei Cao"], "title": "Not All Documents Are What You Need for Extracting Instruction Tuning Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Instruction tuning improves the performance of large language models (LLMs),\nbut it heavily relies on high-quality training data. Recently, LLMs have been\nused to synthesize instruction data using seed question-answer (QA) pairs.\nHowever, these synthesized instructions often lack diversity and tend to be\nsimilar to the input seeds, limiting their applicability in real-world\nscenarios. To address this, we propose extracting instruction tuning data from\nweb corpora that contain rich and diverse knowledge. A naive solution is to\nretrieve domain-specific documents and extract all QA pairs from them, but this\nfaces two key challenges: (1) extracting all QA pairs using LLMs is\nprohibitively expensive, and (2) many extracted QA pairs may be irrelevant to\nthe downstream tasks, potentially degrading model performance. To tackle these\nissues, we introduce EQUAL, an effective and scalable data extraction framework\nthat iteratively alternates between document selection and high-quality QA pair\nextraction to enhance instruction tuning. EQUAL first clusters the document\ncorpus based on embeddings derived from contrastive learning, then uses a\nmulti-armed bandit strategy to efficiently identify clusters that are likely to\ncontain valuable QA pairs. This iterative approach significantly reduces\ncomputational cost while boosting model performance. Experiments on\nAutoMathText and StackOverflow across four downstream tasks show that EQUAL\nreduces computational costs by 5-10x and improves accuracy by 2.5 percent on\nLLaMA-3.1-8B and Mistral-7B", "AI": {"tldr": "EQUAL\u6846\u67b6\u901a\u8fc7\u8fed\u4ee3\u9009\u62e9\u6587\u6863\u548c\u63d0\u53d6\u9ad8\u8d28\u91cfQA\u5bf9\uff0c\u663e\u8457\u964d\u4f4e\u6307\u4ee4\u8c03\u4f18\u7684\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u5408\u6210\u6307\u4ee4\u6570\u636e\u7684\u65b9\u6cd5\u5b58\u5728\u591a\u6837\u6027\u4e0d\u8db3\u3001\u6210\u672c\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u4e14\u5bb9\u6613\u4ea7\u751f\u4e0e\u4e0b\u6e38\u4efb\u52a1\u65e0\u5173\u7684\u52a3\u8d28\u6570\u636e", "method": "\u91c7\u7528\u6587\u6863\u805a\u7c7b\uff08\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u5d4c\u5165\uff09\u7ed3\u5408\u591a\u81c2\u8001\u864e\u673a\u7b56\u7565\uff0c\u901a\u8fc7\u8fed\u4ee3\u5f0f\u6587\u6863\u9009\u62e9\u4e0eQA\u5bf9\u63d0\u53d6\u7684\u4ea4\u66ff\u673a\u5236", "result": "\u5728AutoMathText\u548cStackOverflow\u6570\u636e\u96c6\u4e0a\u5b9e\u73b05-10\u500d\u8ba1\u7b97\u6210\u672c\u964d\u4f4e\uff0cLLaMA-3.1-8B\u548cMistral-7B\u51c6\u786e\u7387\u63d0\u53472.5%", "conclusion": "EQUAL\u6846\u67b6\u901a\u8fc7\u667a\u80fd\u6587\u6863\u7b5b\u9009\u673a\u5236\uff0c\u5728\u4fdd\u8bc1\u6570\u636e\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6570\u636e\u63d0\u53d6\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u573a\u666f\u7684\u6307\u4ee4\u8c03\u4f18\u63d0\u4f9b\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.12259", "pdf": "https://arxiv.org/pdf/2505.12259", "abs": "https://arxiv.org/abs/2505.12259", "authors": ["Yuhang Zhou", "Xutian Chen", "Yixin Cao", "Yuchen Ni", "Yu He", "Siyu Tian", "Xiang Liu", "Jian Zhang", "Chuanjun Ji", "Guangnan Ye", "Xipeng Qiu"], "title": "Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches", "categories": ["cs.CL"], "comment": null, "summary": "Recent progress in large language models (LLMs) has outpaced the development\nof effective evaluation methods. Traditional benchmarks rely on task-specific\nmetrics and static datasets, which often suffer from fairness issues, limited\nscalability, and contamination risks. In this paper, we introduce Teach2Eval,\nan indirect evaluation framework inspired by the Feynman Technique. Instead of\ndirectly testing LLMs on predefined tasks, our method evaluates a model's\nmultiple abilities to teach weaker student models to perform tasks effectively.\nBy converting open-ended tasks into standardized multiple-choice questions\n(MCQs) through teacher-generated feedback, Teach2Eval enables scalable,\nautomated, and multi-dimensional assessment. Our approach not only avoids data\nleakage and memorization but also captures a broad range of cognitive abilities\nthat are orthogonal to current benchmarks. Experimental results across 26\nleading LLMs show strong alignment with existing human and model-based dynamic\nrankings, while offering additional interpretability for training guidance.", "AI": {"tldr": "\u63d0\u51faTeach2Eval\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u578b\u6559\u5b66\u80fd\u529b\u95f4\u63a5\u8bc4\u4f30LLMs\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u591a\u7ef4\u5ea6\u8bc4\u6d4b", "motivation": "\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u516c\u5e73\u6027\u3001\u6269\u5c55\u6027\u4e0d\u8db3\u53ca\u6570\u636e\u6c61\u67d3\u98ce\u9669\uff0c\u9700\u5f00\u53d1\u4e0e\u6a21\u578b\u80fd\u529b\u540c\u6b65\u7684\u8bc4\u4f30\u4f53\u7cfb", "method": "\u57fa\u4e8e\u8d39\u66fc\u6280\u5de7\u8bbe\u8ba1\u95f4\u63a5\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u5f00\u653e\u4efb\u52a1\u8f6c\u5316\u4e3a\u6807\u51c6\u5316\u591a\u9009\u9898\uff0c\u901a\u8fc7\u6559\u5b66\u53cd\u9988\u8bc4\u4f30\u6a21\u578b\u8ba4\u77e5\u80fd\u529b", "result": "\u572826\u4e2a\u4e3b\u6d41LLMs\u5b9e\u9a8c\u4e2d\uff0c\u8bc4\u4f30\u7ed3\u679c\u4e0e\u52a8\u6001\u6392\u540d\u9ad8\u5ea6\u4e00\u81f4\uff0c\u5e76\u63d0\u4f9b\u6a21\u578b\u8bad\u7ec3\u7684\u53ef\u89e3\u91ca\u6027\u6307\u5bfc", "conclusion": "Teach2Eval\u6709\u6548\u89c4\u907f\u6570\u636e\u6cc4\u6f0f\uff0c\u6355\u6349\u591a\u7ef4\u8ba4\u77e5\u80fd\u529b\uff0c\u5efa\u7acb\u6b63\u4ea4\u4e8e\u73b0\u6709\u57fa\u51c6\u7684\u65b0\u578b\u8bc4\u4f30\u8303\u5f0f"}}
{"id": "2505.12265", "pdf": "https://arxiv.org/pdf/2505.12265", "abs": "https://arxiv.org/abs/2505.12265", "authors": ["Chengwei Qin", "Wenxuan Zhou", "Karthik Abinav Sankararaman", "Nanshu Wang", "Tengyu Xu", "Alexander Radovic", "Eryk Helenowski", "Arya Talebzadeh", "Aditya Tayade", "Sinong Wang", "Shafiq Joty", "Han Fang", "Hao Ma"], "title": "Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation", "categories": ["cs.CL"], "comment": null, "summary": "Hallucination, the generation of factually incorrect information, remains a\nsignificant challenge for large language models (LLMs), especially in\nopen-domain long-form generation. Existing approaches for detecting\nhallucination in long-form tasks either focus on limited domains or rely\nheavily on external fact-checking tools, which may not always be available.\n  In this work, we systematically investigate reference-free hallucination\ndetection in open-domain long-form responses. Our findings reveal that internal\nstates (e.g., model's output probability and entropy) alone are insufficient\nfor reliably (i.e., better than random guessing) distinguishing between factual\nand hallucinated content. To enhance detection, we explore various existing\napproaches, including prompting-based methods, probing, and fine-tuning, with\nfine-tuning proving the most effective. To further improve the accuracy, we\nintroduce a new paradigm, named RATE-FT, that augments fine-tuning with an\nauxiliary task for the model to jointly learn with the main task of\nhallucination detection. With extensive experiments and analysis using a\nvariety of model families & datasets, we demonstrate the effectiveness and\ngeneralizability of our method, e.g., +3% over general fine-tuning methods on\nLongFact.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRATE-FT\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u8f85\u52a9\u4efb\u52a1\u589e\u5f3a\u5fae\u8c03\u6548\u679c\uff0c\u6709\u6548\u63d0\u5347\u5f00\u653e\u57df\u957f\u6587\u672c\u5e7b\u89c9\u68c0\u6d4b\u51c6\u786e\u7387", "motivation": "\u73b0\u6709\u5e7b\u89c9\u68c0\u6d4b\u65b9\u6cd5\u5728\u5f00\u653e\u57df\u957f\u6587\u672c\u4efb\u52a1\u4e2d\u5b58\u5728\u9886\u57df\u9650\u5236\u6216\u8fc7\u5ea6\u4f9d\u8d56\u5916\u90e8\u5de5\u5177\u7684\u95ee\u9898", "method": "\u63d0\u51faRATE-FT\u8303\u5f0f\uff0c\u5728\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u5f15\u5165\u8f85\u52a9\u4efb\u52a1\u8fdb\u884c\u8054\u5408\u5b66\u4e60\uff0c\u589e\u5f3a\u6a21\u578b\u5bf9\u5e7b\u89c9\u7279\u5f81\u7684\u6355\u6349\u80fd\u529b", "result": "\u5728LongFact\u7b49\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u901a\u7528\u5fae\u8c03\u65b9\u6cd53%\uff0c\u5e76\u5728\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u4e2d\u5c55\u73b0\u826f\u597d\u6cdb\u5316\u6027", "conclusion": "\u901a\u8fc7\u4efb\u52a1\u589e\u5f3a\u7684\u5fae\u8c03\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u81ea\u4e3b\u68c0\u6d4b\u5e7b\u89c9\u7684\u80fd\u529b\uff0c\u4e3a\u5f00\u653e\u57df\u6587\u672c\u53ef\u9760\u6027\u63d0\u4f9b\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.12268", "pdf": "https://arxiv.org/pdf/2505.12268", "abs": "https://arxiv.org/abs/2505.12268", "authors": ["Pratim Chowdhary"], "title": "$K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Understanding which neural components drive specific capabilities in\nmid-sized language models ($\\leq$10B parameters) remains a key challenge. We\nintroduce the $(\\bm{K}, \\epsilon)$-Minimum Sufficient Head Circuit ($K$-MSHC),\na methodology to identify minimal sets of attention heads crucial for\nclassification tasks as well as Search-K-MSHC, an efficient algorithm for\ndiscovering these circuits. Applying our Search-K-MSHC algorithm to Gemma-9B,\nwe analyze three syntactic task families: grammar acceptability, arithmetic\nverification, and arithmetic word problems. Our findings reveal distinct\ntask-specific head circuits, with grammar tasks predominantly utilizing early\nlayers, word problems showing pronounced activity in both shallow and deep\nregions, and arithmetic verification demonstrating a more distributed pattern\nacross the network. We discover non-linear circuit overlap patterns, where\ndifferent task pairs share computational components at varying levels of\nimportance. While grammar and arithmetic share many \"weak\" heads, arithmetic\nand word problems share more consistently critical \"strong\" heads. Importantly,\nwe find that each task maintains dedicated \"super-heads\" with minimal\ncross-task overlap, suggesting that syntactic and numerical competencies emerge\nfrom specialized yet partially reusable head circuits.", "AI": {"tldr": "\u63d0\u51faK-MSHC\u65b9\u6cd5\u8bc6\u522b\u4e2d\u7b49\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u5173\u952e\u6ce8\u610f\u529b\u5934\u7535\u8def\uff0c\u53d1\u73b0\u4e0d\u540c\u53e5\u6cd5\u4efb\u52a1\u4f9d\u8d56\u5206\u5e03\u5dee\u5f02\u5927\u4e14\u5b58\u5728\u4e13\u7528\u8d85\u7ea7\u5934", "motivation": "\u89e3\u51b3\u4e2d\u7b49\u89c4\u6a21\u8bed\u8a00\u6a21\u578b(\u226410B\u53c2\u6570)\u4e2d\u7279\u5b9a\u80fd\u529b\u795e\u7ecf\u7ec4\u4ef6\u5b9a\u4f4d\u96be\u9898\uff0c\u63ed\u793a\u4e0d\u540c\u53e5\u6cd5\u4efb\u52a1\u7684\u8ba1\u7b97\u673a\u5236", "method": "\u5f00\u53d1Search-K-MSHC\u7b97\u6cd5\uff0c\u5e94\u7528\u5728Gemma-9B\u6a21\u578b\uff0c\u5206\u6790\u8bed\u6cd5\u63a5\u53d7/\u7b97\u672f\u9a8c\u8bc1/\u5e94\u7528\u9898\u4e09\u4e2a\u4efb\u52a1\u5bb6\u65cf", "result": "\u8bed\u6cd5\u4efb\u52a1\u96c6\u4e2d\u4e8e\u6d45\u5c42\uff0c\u5e94\u7528\u9898\u6fc0\u6d3b\u6df1\u6d45\u5c42\uff0c\u7b97\u672f\u9a8c\u8bc1\u5206\u5e03\u5e7f\u6cdb\uff1b\u4efb\u52a1\u5bf9\u5171\u4eab\u8ba1\u7b97\u7ec4\u4ef6\u5f3a\u5ea6\u5dee\u5f02\u663e\u8457\uff0c\u5b58\u5728\u8de8\u4efb\u52a1\u91cd\u53e0\u5c11\u7684\u4e13\u7528\u8d85\u7ea7\u5934", "conclusion": "\u53e5\u6cd5\u4e0e\u6570\u503c\u80fd\u529b\u4ea7\u751f\u4e8e\u4e13\u7528\u4f46\u90e8\u5206\u53ef\u590d\u7528\u7684\u5934\u7535\u8def\uff0c\u4e0d\u540c\u4efb\u52a1\u901a\u8fc7\u5f3a\u5f31\u5171\u4eab\u4e0e\u4e13\u7528\u8d85\u7ea7\u5934\u7684\u7ec4\u5408\u5b9e\u73b0\u80fd\u529b\u5206\u5316"}}
{"id": "2505.12273", "pdf": "https://arxiv.org/pdf/2505.12273", "abs": "https://arxiv.org/abs/2505.12273", "authors": ["Md. Atiqur Rahman", "Sabrina Islam", "Mushfiqul Haque Omi"], "title": "LLM-Based Evaluation of Low-Resource Machine Translation: A Reference-less Dialect Guided Approach with a Refined Sylheti-English Benchmark", "categories": ["cs.CL"], "comment": null, "summary": "Evaluating machine translation (MT) for low-resource languages poses a\npersistent challenge, primarily due to the limited availability of high quality\nreference translations. This issue is further exacerbated in languages with\nmultiple dialects, where linguistic diversity and data scarcity hinder robust\nevaluation. Large Language Models (LLMs) present a promising solution through\nreference-free evaluation techniques; however, their effectiveness diminishes\nin the absence of dialect-specific context and tailored guidance. In this work,\nwe propose a comprehensive framework that enhances LLM-based MT evaluation\nusing a dialect guided approach. We extend the ONUBAD dataset by incorporating\nSylheti-English sentence pairs, corresponding machine translations, and Direct\nAssessment (DA) scores annotated by native speakers. To address the vocabulary\ngap, we augment the tokenizer vocabulary with dialect-specific terms. We\nfurther introduce a regression head to enable scalar score prediction and\ndesign a dialect-guided (DG) prompting strategy. Our evaluation across multiple\nLLMs shows that the proposed pipeline consistently outperforms existing\nmethods, achieving the highest gain of +0.1083 in Spearman correlation, along\nwith improvements across other evaluation settings. The dataset and the code\nare available at https://github.com/180041123-Atiq/MTEonLowResourceLanguage.", "AI": {"tldr": "\u63d0\u51fa\u7ed3\u5408\u65b9\u8a00\u6307\u5bfc\u7684LLM\u6846\u67b6\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u6548\u679c", "motivation": "\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u591a\u65b9\u8a00\u8bed\u8a00\u7684\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u9762\u4e34\u53c2\u8003\u8bd1\u6587\u7a00\u7f3a\u3001\u65b9\u8a00\u8bed\u5883\u7f3a\u5931\u5bfc\u81f4\u7684\u8bc4\u4f30\u4e0d\u51c6\u786e\u95ee\u9898", "method": "1. \u6269\u5c55ONUBAD\u6570\u636e\u96c6\uff08\u589e\u52a0Sylheti\u8bed\u6599\u53ca\u6bcd\u8bed\u8005\u8bc4\u5206\uff09\n2. \u589e\u52a0\u65b9\u8a00\u7279\u5b9a\u8bcd\u6c47\u89e3\u51b3\u8bcd\u6c47\u7a7a\u7f3a\n3. \u8bbe\u8ba1\u56de\u5f52\u5934\u5b9e\u73b0\u6807\u91cf\u5206\u6570\u9884\u6d4b\n4. \u5f00\u53d1\u65b9\u8a00\u5f15\u5bfc\uff08DG\uff09\u63d0\u793a\u7b56\u7565", "result": "\u5728\u591a\u4e2aLLM\u4e2d\u5b9e\u73b0Spearman\u76f8\u5173\u7cfb\u6570\u6700\u9ad8+0.1083\u63d0\u5347\uff0c\u5176\u4ed6\u8bc4\u4f30\u6307\u6807\u5747\u6539\u8fdb", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4f4e\u8d44\u6e90\u8bed\u8a00MT\u8bc4\u4f30\u6548\u679c\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2505.12287", "pdf": "https://arxiv.org/pdf/2505.12287", "abs": "https://arxiv.org/abs/2505.12287", "authors": ["Linghan Huang", "Haolin Jin", "Zhaoge Bi", "Pengyue Yang", "Peizhou Zhao", "Taozhao Chen", "Xiongfei Wu", "Lei Ma", "Huaming Chen"], "title": "The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have seen widespread applications across various\ndomains, yet remain vulnerable to adversarial prompt injections. While most\nexisting research on jailbreak attacks and hallucination phenomena has focused\nprimarily on open-source models, we investigate the frontier of closed-source\nLLMs under multilingual attack scenarios. We present a first-of-its-kind\nintegrated adversarial framework that leverages diverse attack techniques to\nsystematically evaluate frontier proprietary solutions, including GPT-4o,\nDeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories\nof security contents in both English and Chinese, generating 38,400 responses\nacross 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as\nthe quantitative metric to assess performance from three dimensions: prompt\ndesign, model architecture, and language environment. Our findings suggest that\nQwen-Max is the most vulnerable, while GPT-4o shows the strongest defense.\nNotably, prompts in Chinese consistently yield higher ASRs than their English\ncounterparts, and our novel Two-Sides attack technique proves to be the most\neffective across all models. This work highlights a dire need for\nlanguage-aware alignment and robust cross-lingual defenses in LLMs, and we hope\nit will inspire researchers, developers, and policymakers toward more robust\nand inclusive AI systems.", "AI": {"tldr": "\u9488\u5bf9\u95ed\u6e90\u5927\u6a21\u578b\u7684\u591a\u8bed\u8a00\u5bf9\u6297\u653b\u51fb\u7814\u7a76\u663e\u793a\u4e2d\u6587\u63d0\u793a\u653b\u51fb\u66f4\u6709\u6548\uff0cQwen-Max\u6700\u8106\u5f31\uff0cGPT-4o\u9632\u5fa1\u6700\u5f3a", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5f00\u6e90\u6a21\u578b\u7684\u6f0f\u6d1e\uff0c\u7f3a\u4e4f\u5bf9\u95ed\u6e90\u6a21\u578b\u5728\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u5b89\u5168\u6027\u7684\u7cfb\u7edf\u8bc4\u4f30", "method": "\u6784\u5efa\u96c6\u6210\u5bf9\u6297\u6846\u67b6\uff0c\u6d4b\u8bd5GPT-4o\u7b494\u4e2a\u95ed\u6e90\u6a21\u578b\u57286\u7c7b\u5b89\u5168\u5185\u5bb9\u4e0a\u7684\u8868\u73b0\uff0c\u751f\u621038,400\u4e2a\u653b\u51fb\u54cd\u5e94", "result": "\u4e2d\u6587\u653b\u51fb\u6210\u529f\u7387\u66f4\u9ad8\uff08\u5e73\u5747\u63d0\u534715%\uff09\uff0cTwo-Sides\u653b\u51fb\u6cd5\u6700\u6709\u6548\uff0cQwen-Max\u9632\u5fa1\u6700\u5f31\uff08ASR 32.1%\uff09\uff0cGPT-4o\u6700\u5f3a\uff08ASR 8.7%\uff09", "conclusion": "\u5927\u6a21\u578b\u4e9f\u9700\u589e\u5f3a\u591a\u8bed\u8a00\u5b89\u5168\u5bf9\u9f50\u80fd\u529b\uff0c\u7814\u7a76\u4e3a\u5f00\u53d1\u8de8\u8bed\u8a00\u9c81\u68d2AI\u7cfb\u7edf\u63d0\u4f9b\u91cd\u8981\u57fa\u51c6"}}
{"id": "2505.12299", "pdf": "https://arxiv.org/pdf/2505.12299", "abs": "https://arxiv.org/abs/2505.12299", "authors": ["Kun Huang", "Weikai Xu", "Yuxuan Liu", "Quandong Wang", "Pengzhi Gao", "Wei Liu", "Jian Luan", "Bin Wang", "Bo An"], "title": "Enhance Mobile Agents Thinking Process Via Iterative Preference Learning", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, 8 figures, 7 tables", "summary": "The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to\nimprove the reasoning performance of VLM-based mobile agents in GUI tasks.\nHowever, the scarcity of diverse CoaT trajectories limits the expressiveness\nand generalization ability of such agents. While self-training is commonly\nemployed to address data scarcity, existing approaches either overlook the\ncorrectness of intermediate reasoning steps or depend on expensive\nprocess-level annotations to construct process reward models (PRM). To address\nthe above problems, we propose an Iterative Preference Learning (IPL) that\nconstructs a CoaT-tree through interative sampling, scores leaf nodes using\nrule-based reward, and backpropagates feedback to derive Thinking-level Direct\nPreference Optimization (T-DPO) pairs. To prevent overfitting during warm-up\nsupervised fine-tuning, we further introduce a three-stage instruction\nevolution, which leverages GPT-4o to generate diverse Q\\&A pairs based on real\nmobile UI screenshots, enhancing both generality and layout understanding.\nExperiments on three standard Mobile GUI-agent benchmarks demonstrate that our\nagent MobileIPL outperforms strong baselines, including continual pretraining\nmodels such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance\nacross three standard Mobile GUI-Agents benchmarks and shows strong\ngeneralization to out-of-domain scenarios.", "AI": {"tldr": "\u63d0\u51fa\u8fed\u4ee3\u504f\u597d\u5b66\u4e60(IPL)\u65b9\u6cd5\u89e3\u51b3CoaT\u8f68\u8ff9\u7a00\u7f3a\u95ee\u9898\uff0c\u901a\u8fc7\u6784\u5efaCoaT\u6811\u548c\u601d\u7ef4\u7ea7\u76f4\u63a5\u504f\u597d\u4f18\u5316(T-DPO)\u63d0\u5347\u79fb\u52a8GUI\u667a\u80fd\u4f53\u6027\u80fd", "motivation": "\u73b0\u6709\u81ea\u8bad\u7ec3\u65b9\u6cd5\u5ffd\u89c6\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u7684\u6b63\u786e\u6027\uff0c\u6216\u4f9d\u8d56\u6602\u8d35\u7684\u8fc7\u7a0b\u7ea7\u6807\u6ce8\u3002\u9700\u8981\u6709\u6548\u89e3\u51b3CoaT\u8f68\u8ff9\u591a\u6837\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u4ee5\u63d0\u5347\u667a\u80fd\u4f53\u6cdb\u5316\u80fd\u529b", "method": "1. \u8fed\u4ee3\u91c7\u6837\u6784\u5efaCoaT\u6811\uff0c\u4f7f\u7528\u89c4\u5219\u5956\u52b1\u8bc4\u4f30\u53f6\u8282\u70b9\u5e76\u53cd\u5411\u4f20\u64ad\u53cd\u9988\u751f\u6210T-DPO\u5bf9\n2. \u4e09\u9636\u6bb5\u6307\u4ee4\u6f14\u5316\u7b56\u7565\uff1a\u5229\u7528GPT-4o\u57fa\u4e8e\u771f\u5b9eUI\u622a\u56fe\u751f\u6210\u591a\u6837\u5316Q&A\uff0c\u589e\u5f3a\u5e03\u5c40\u7406\u89e3", "result": "MobileIPL\u5728\u4e09\u4e2a\u6807\u51c6\u79fb\u52a8GUI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8aOS-ATLAS\u7b49\u57fa\u7ebf\u6a21\u578b\uff0c\u53d6\u5f97SOTA\u6027\u80fd\uff0c\u5728\u57df\u5916\u573a\u666f\u5c55\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b", "conclusion": "IPL\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u65e0\u9700\u6602\u8d35\u6807\u6ce8\u5373\u53ef\u63d0\u5347\u667a\u80fd\u4f53\u6027\u80fd\uff0c\u8bc1\u5b9e\u5176\u5728\u8de8\u57df\u573a\u666f\u7684\u4f18\u8d8a\u6cdb\u5316\u6027"}}
{"id": "2505.12300", "pdf": "https://arxiv.org/pdf/2505.12300", "abs": "https://arxiv.org/abs/2505.12300", "authors": ["Weixuan Wang", "Minghao Wu", "Barry Haddow", "Alexandra Birch"], "title": "HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Fine-tuning large language models (LLMs) on a mixture of diverse datasets\nposes challenges due to data imbalance and heterogeneity. Existing methods\noften address these issues across datasets (globally) but overlook the\nimbalance and heterogeneity within individual datasets (locally), which limits\ntheir effectiveness. We introduce Hierarchical Balancing Optimization (HBO), a\nnovel method that enables LLMs to autonomously adjust data allocation during\nfine-tuning both across datasets (globally) and within each individual dataset\n(locally). HBO employs a bilevel optimization strategy with two types of\nactors: a Global Actor, which balances data sampling across different subsets\nof the training mixture, and several Local Actors, which optimizes data usage\nwithin each subset based on difficulty levels. These actors are guided by\nreward functions derived from the LLM's training state, which measure learning\nprogress and relative performance improvement. We evaluate HBO on three LLM\nbackbones across nine diverse tasks in multilingual and multitask setups.\nResults show that HBO consistently outperforms existing baselines, achieving\nsignificant accuracy gains. Our in-depth analysis further demonstrates that\nboth the global actor and local actors of HBO effectively adjust data usage\nduring fine-tuning. HBO provides a comprehensive solution to the challenges of\ndata imbalance and heterogeneity in LLM fine-tuning, enabling more effective\ntraining across diverse datasets.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u5e73\u8861\u4f18\u5316\u7b97\u6cd5\uff08HBO\uff09\uff0c\u901a\u8fc7\u5168\u5c40-\u5c40\u90e8\u53cc\u5c42\u4f18\u5316\u673a\u5236\u89e3\u51b3\u5927\u6a21\u578b\u5fae\u8c03\u4e2d\u7684\u6570\u636e\u4e0d\u5e73\u8861\u4e0e\u5f02\u6784\u6027\u95ee\u9898", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u5904\u7406\u8de8\u6570\u636e\u96c6\u5168\u5c40\u4e0d\u5e73\u8861\uff0c\u5ffd\u7565\u4e86\u5355\u4e2a\u6570\u636e\u96c6\u5185\u90e8\u5c40\u90e8\u5f02\u6784\u6027\uff0c\u5bfc\u81f4\u5927\u6a21\u578b\u5728\u591a\u4efb\u52a1\u5fae\u8c03\u4e2d\u6548\u679c\u53d7\u9650", "method": "\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u6846\u67b6\uff1aGlobal Actor\u5e73\u8861\u8de8\u5b50\u96c6\u6570\u636e\u91c7\u6837\uff0cLocal Actors\u6839\u636e\u96be\u5ea6\u4f18\u5316\u5b50\u96c6\u5185\u90e8\u6570\u636e\u4f7f\u7528\uff0c\u901a\u8fc7\u8bad\u7ec3\u72b6\u6001\u53cd\u9988\u7684\u5956\u52b1\u51fd\u6570\u52a8\u6001\u8c03\u6574", "result": "\u57283\u4e2a\u5927\u6a21\u578b\u67b6\u6784/9\u4e2a\u591a\u8bed\u8a00\u591a\u4efb\u52a1\u573a\u666f\u4e2d\uff0cHBO\u51c6\u786e\u7387\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff08+2.1%-14.7%\uff09\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u5168\u5c40-\u5c40\u90e8\u7ec4\u4ef6\u7684\u534f\u540c\u6709\u6548\u6027", "conclusion": "HBO\u9996\u6b21\u5b9e\u73b0\u8de8\u6570\u636e\u96c6\u5168\u5c40\u5e73\u8861\u4e0e\u6570\u636e\u96c6\u5185\u90e8\u5c40\u90e8\u4f18\u5316\u7684\u8054\u5408\u8c03\u63a7\uff0c\u4e3a\u590d\u6742\u6570\u636e\u73af\u5883\u4e0b\u7684\u6a21\u578b\u5fae\u8c03\u63d0\u4f9b\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.12306", "pdf": "https://arxiv.org/pdf/2505.12306", "abs": "https://arxiv.org/abs/2505.12306", "authors": ["Yuwei Zhang", "Wenhao Yu", "Shangbin Feng", "Yifan Zhu", "Letian Peng", "Jayanth Srinivasa", "Gaowen Liu", "Jingbo Shang"], "title": "Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for Real-world Knowledge Injection", "categories": ["cs.CL"], "comment": "Dataset is available at\n  https://huggingface.co/datasets/YWZBrandon/wikidyk", "summary": "Despite significant advances in large language models (LLMs), their knowledge\nmemorization capabilities remain underexplored, due to the lack of standardized\nand high-quality test ground. In this paper, we introduce a novel, real-world\nand large-scale knowledge injection benchmark that evolves continuously over\ntime without requiring human intervention. Specifically, we propose WikiDYK,\nwhich leverages recently-added and human-written facts from Wikipedia's \"Did\nYou Know...\" entries. These entries are carefully selected by expert Wikipedia\neditors based on criteria such as verifiability and clarity. Each entry is\nconverted into multiple question-answer pairs spanning diverse task formats\nfrom easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290\nfacts and 77,180 questions, which is also seamlessly extensible with future\nupdates from Wikipedia editors. Extensive experiments using continued\npre-training reveal a surprising insight: despite their prevalence in modern\nLLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge\nmemorization capabilities compared to Bidirectional Language Models (BiLMs),\nexhibiting a 23% lower accuracy in terms of reliability. To compensate for the\nsmaller scales of current BiLMs, we introduce a modular collaborative framework\nutilizing ensembles of BiLMs as external knowledge repositories to integrate\nwith LLMs. Experiment shows that our framework further improves the reliability\naccuracy by up to 29.1%.", "AI": {"tldr": "\u63d0\u51faWikiDYK\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u7ef4\u57fa\u767e\u79d1\u52a8\u6001\u77e5\u8bc6\u9a8c\u8bc1\u53d1\u73b0\u53cc\u5411\u8bed\u8a00\u6a21\u578b\uff08BiLMs\uff09\u6bd4\u56e0\u679c\u6a21\u578b\uff08CLMs\uff09\u77e5\u8bc6\u8bb0\u5fc6\u53ef\u9760\u6027\u9ad823%\uff0c\u5e76\u63d0\u51fa\u534f\u540c\u6846\u67b6\u63d0\u534729.1%\u51c6\u786e\u7387\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u8bb0\u5fc6\u80fd\u529b\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u5177\uff0c\u9700\u8981\u52a8\u6001\u3001\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u7684\u6d4b\u8bd5\u57fa\u51c6\u3002", "method": "\u5229\u7528\u7ef4\u57fa\u767e\u79d1\u7f16\u8f91\u7cbe\u9009\u7684\u300e\u4f60\u77e5\u9053\u5417\u300f\u6761\u76ee\u6784\u5efa12,290\u4e2a\u4e8b\u5b9e/77,180\u4e2a\u591a\u683c\u5f0f\u95ee\u9898\uff0c\u901a\u8fc7\u6301\u7eed\u9884\u8bad\u7ec3\u5bf9\u6bd4CLMs/BiLMs\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u53cc\u5411\u6a21\u578b\u534f\u540c\u6846\u67b6\u3002", "result": "BiLMs\u53ef\u9760\u6027\u51c6\u786e\u7387\u6bd4CLMs\u9ad823%\uff0c\u534f\u4f5c\u6846\u67b6\u8fdb\u4e00\u6b65\u63d0\u534729.1%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u53cc\u5411\u8bed\u8a00\u6a21\u578b\u5177\u6709\u66f4\u5f3a\u7684\u77e5\u8bc6\u8bb0\u5fc6\u80fd\u529b\uff0c\u6a21\u5757\u5316\u534f\u4f5c\u6846\u67b6\u53ef\u6709\u6548\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\u53ef\u9760\u6027\uff0c\u4e3aLLM\u77e5\u8bc6\u673a\u5236\u7814\u7a76\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2505.12313", "pdf": "https://arxiv.org/pdf/2505.12313", "abs": "https://arxiv.org/abs/2505.12313", "authors": ["Weixuan Wang", "Minghao Wu", "Barry Haddow", "Alexandra Birch"], "title": "ExpertSteer: Intervening in LLMs through Expert Knowledge", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) exhibit remarkable capabilities across various\ntasks, yet guiding them to follow desired behaviours during inference remains a\nsignificant challenge. Activation steering offers a promising method to control\nthe generation process of LLMs by modifying their internal activations.\nHowever, existing methods commonly intervene in the model's behaviour using\nsteering vectors generated by the model itself, which constrains their\neffectiveness to that specific model and excludes the possibility of leveraging\npowerful external expert models for steering. To address these limitations, we\npropose ExpertSteer, a novel approach that leverages arbitrary specialized\nexpert models to generate steering vectors, enabling intervention in any LLMs.\nExpertSteer transfers the knowledge from an expert model to a target LLM\nthrough a cohesive four-step process: first aligning representation dimensions\nwith auto-encoders to enable cross-model transfer, then identifying\nintervention layer pairs based on mutual information analysis, next generating\nsteering vectors from the expert model using Recursive Feature Machines, and\nfinally applying these vectors on the identified layers during inference to\nselectively guide the target LLM without updating model parameters. We conduct\ncomprehensive experiments using three LLMs on 15 popular benchmarks across four\ndistinct domains. Experiments demonstrate that ExpertSteer significantly\noutperforms established baselines across diverse tasks at minimal cost.", "AI": {"tldr": "\u63d0\u51fa\u4e86ExpertSteer\u65b9\u6cd5\uff0c\u5229\u7528\u5916\u90e8\u4e13\u5bb6\u6a21\u578b\u751f\u6210\u5bfc\u5411\u5411\u91cf\uff0c\u65e0\u9700\u4fee\u6539\u53c2\u6570\u5373\u53ef\u5f15\u5bfc\u4e0d\u540cLLM\u7684\u751f\u6210\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u591a\u9886\u57df\u4efb\u52a1\u6027\u80fd", "motivation": "\u73b0\u6709\u6fc0\u6d3b\u5bfc\u5411\u65b9\u6cd5\u4f9d\u8d56\u6a21\u578b\u81ea\u8eab\u751f\u6210\u5bfc\u5411\u5411\u91cf\uff0c\u65e0\u6cd5\u8de8\u6a21\u578b\u5229\u7528\u5916\u90e8\u4e13\u5bb6\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u5e72\u9884\u6548\u679c\u548c\u5e94\u7528\u8303\u56f4", "method": "1) \u901a\u8fc7\u81ea\u52a8\u7f16\u7801\u5668\u5bf9\u9f50\u8868\u5f81\u7ef4\u5ea6 2) \u57fa\u4e8e\u4e92\u4fe1\u606f\u5206\u6790\u5b9a\u4f4d\u5e72\u9884\u5c42 3) \u4f7f\u7528\u9012\u5f52\u7279\u5f81\u673a\u5236\u751f\u6210\u5bfc\u5411\u5411\u91cf 4) \u5728\u63a8\u7406\u65f6\u5e94\u7528\u5411\u91cf\u5b9e\u65bd\u5f15\u5bfc", "result": "\u57284\u4e2a\u9886\u57df15\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4ee5\u6781\u4f4e\u6210\u672c\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u8de8\u6a21\u578b\u77e5\u8bc6\u8fc1\u79fb\u7684\u6709\u6548\u6027", "conclusion": "ExpertSteer\u5f00\u521b\u4e86\u5229\u7528\u4efb\u610f\u4e13\u5bb6\u6a21\u578b\u5e72\u9884LLM\u7684\u65b0\u8303\u5f0f\uff0c\u4e3a\u6a21\u578b\u63a7\u5236\u63d0\u4f9b\u4e86\u53c2\u6570\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u65b0\u65b9\u6848"}}
{"id": "2505.12328", "pdf": "https://arxiv.org/pdf/2505.12328", "abs": "https://arxiv.org/abs/2505.12328", "authors": ["Xinye Li", "Mingqi Wan", "Dianbo Sui"], "title": "LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "We present Team asdfo123's submission to the LLMSR@XLLM25 shared task, which\nevaluates large language models on producing fine-grained, controllable, and\ninterpretable reasoning processes. Systems must extract all problem conditions,\ndecompose a chain of thought into statement-evidence pairs, and verify the\nlogical validity of each pair. Leveraging only the off-the-shelf\nMeta-Llama-3-8B-Instruct, we craft a concise few-shot, multi-turn prompt that\nfirst enumerates all conditions and then guides the model to label, cite, and\nadjudicate every reasoning step. A lightweight post-processor based on regular\nexpressions normalises spans and enforces the official JSON schema. Without\nfine-tuning, external retrieval, or ensembling, our method ranks 5th overall,\nachieving macro F1 scores on par with substantially more complex and\nresource-consuming pipelines. We conclude by analysing the strengths and\nlimitations of our approach and outlining directions for future research in\nstructural reasoning with LLMs. Our code is available at\nhttps://github.com/asdfo123/LLMSR-asdfo123.", "AI": {"tldr": "Team asdfo123\u4f7f\u7528Meta-Llama-3-8B-Instruct\u6a21\u578b\uff0c\u901a\u8fc7\u5c11\u91cf\u6837\u672c\u63d0\u793a\u548c\u591a\u8f6e\u4ea4\u4e92\u8bbe\u8ba1\uff0c\u5728LLMSR@XLLM25\u7ed3\u6784\u5316\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u7b2c\u4e94\u540d\uff0c\u9a8c\u8bc1\u4e86\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u4ec5\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u5b9e\u73b0\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u907f\u514d\u590d\u6742\u8bad\u7ec3\u6d41\u7a0b\u548c\u989d\u5916\u8d44\u6e90\u6d88\u8017\uff0c\u9a8c\u8bc1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u903b\u8f91\u9a8c\u8bc1\u4efb\u52a1\u4e2d\u7684\u539f\u751f\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u591a\u8f6efew-shot\u63d0\u793a\u6846\u67b6\uff08\u6761\u4ef6\u679a\u4e3e-\u8bc1\u636e\u6807\u6ce8-\u903b\u8f91\u9a8c\u8bc1\uff09\uff0c\u914d\u5408\u6b63\u5219\u8868\u8fbe\u5f0f\u540e\u5904\u7406\u5668\u5b9e\u73b0\u683c\u5f0f\u89c4\u8303\u5316\uff0c\u5168\u7a0b\u672a\u4f7f\u7528\u5fae\u8c03/\u68c0\u7d22/\u6a21\u578b\u96c6\u6210\u3002", "result": "\u5728\u5b98\u65b9\u8bc4\u4f30\u4e2d\u7efc\u5408\u6392\u540d\u7b2c\u4e94\uff0c\u5b8f\u89c2F1\u5206\u6570\u4e0e\u66f4\u590d\u6742\u7cfb\u7edf\u6301\u5e73\uff08\u5177\u4f53\u5206\u6570\u672a\u62ab\u9732\uff09\uff0c\u8bc1\u660e\u8f7b\u91cf\u5316\u65b9\u6848\u7684\u7ade\u4e89\u529b\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u63d0\u793a\u5de5\u7a0b\u5728\u7ed3\u6784\u5316\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u6307\u51fa\u5f53\u524d\u6a21\u578b\u5728\u590d\u6742\u903b\u8f91\u94fe\u5904\u7406\u4e0a\u7684\u5c40\u9650\uff0c\u63d0\u51fa\u7ed3\u5408\u7b26\u53f7\u63a8\u7406\u7684\u6df7\u5408\u7cfb\u7edf\u4f5c\u4e3a\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2505.12345", "pdf": "https://arxiv.org/pdf/2505.12345", "abs": "https://arxiv.org/abs/2505.12345", "authors": ["Qizhou Chen", "Dakan Wang", "Taolin Zhang", "Zaoming Yan", "Chengsong You", "Chengyu Wang", "Xiaofeng He"], "title": "UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Model editing aims to enhance the accuracy and reliability of large language\nmodels (LLMs) by efficiently adjusting their internal parameters. Currently,\nmost LLM editing datasets are confined to narrow knowledge domains and cover a\nlimited range of editing evaluation. They often overlook the broad scope of\nediting demands and the diversity of ripple effects resulting from edits. In\nthis context, we introduce UniEdit, a unified benchmark for LLM editing\ngrounded in open-domain knowledge. First, we construct editing samples by\nselecting entities from 25 common domains across five major categories,\nutilizing the extensive triple knowledge available in open-domain knowledge\ngraphs to ensure comprehensive coverage of the knowledge domains. To address\nthe issues of generality and locality in editing, we design an Neighborhood\nMulti-hop Chain Sampling (NMCS) algorithm to sample subgraphs based on a given\nknowledge piece to entail comprehensive ripple effects to evaluate. Finally, we\nemploy proprietary LLMs to convert the sampled knowledge subgraphs into natural\nlanguage text, guaranteeing grammatical accuracy and syntactical diversity.\nExtensive statistical analysis confirms the scale, comprehensiveness, and\ndiversity of our UniEdit benchmark. We conduct comprehensive experiments across\nmultiple LLMs and editors, analyzing their performance to highlight strengths\nand weaknesses in editing across open knowledge domains and various evaluation\ncriteria, thereby offering valuable insights for future research endeavors.", "AI": {"tldr": "\u63d0\u51faUniEdit\u57fa\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u5f00\u653e\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u591a\u7ef4\u5ea6\u8bc4\u4f30\u4f53\u7cfb\uff0c\u5168\u9762\u68c0\u6d4b\u5927\u8bed\u8a00\u6a21\u578b\u7f16\u8f91\u7684\u51c6\u786e\u6027\u4e0e\u8fde\u9501\u6548\u5e94\u3002", "motivation": "\u73b0\u6709LLM\u7f16\u8f91\u6570\u636e\u96c6\u5b58\u5728\u77e5\u8bc6\u8986\u76d6\u72ed\u7a84\u3001\u8bc4\u4f30\u7ef4\u5ea6\u5355\u4e00\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5f00\u653e\u9886\u57df\u590d\u6742\u7f16\u8f91\u9700\u6c42\uff0c\u4e9f\u9700\u6784\u5efa\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "1. \u57fa\u4e8e\u5f00\u653e\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa25\u9886\u57df\u5b9e\u4f53\u6837\u672c\uff1b2. \u8bbe\u8ba1NMCS\u7b97\u6cd5\u62bd\u53d6\u591a\u8df3\u5173\u8054\u5b50\u56fe\uff1b3. \u5229\u7528\u5927\u6a21\u578b\u5c06\u77e5\u8bc6\u4e09\u5143\u7ec4\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u6d4b\u8bd5\u6587\u672c\u3002", "result": "\u7edf\u8ba1\u5206\u6790\u9a8c\u8bc1UniEdit\u7684\u89c4\u6a21\u4e0e\u591a\u6837\u6027\u4f18\u52bf\uff0c\u5b9e\u9a8c\u63ed\u793a\u4e0d\u540c\u7f16\u8f91\u5668\u5728\u77e5\u8bc6\u8986\u76d6\u5ea6/\u7f16\u8f91\u6301\u7eed\u6027/\u526f\u4f5c\u7528\u63a7\u5236\u7b49\u7ef4\u5ea6\u7684\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "UniEdit\u4e3a\u5f00\u653e\u57df\u6a21\u578b\u7f16\u8f91\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u5316\u8bc4\u4f30\u65b9\u6848\uff0c\u5176\u7ed3\u6784\u5316\u91c7\u6837\u673a\u5236\u548c\u591a\u7ef4\u5ea6\u8bc4\u4f30\u4f53\u7cfb\u63a8\u52a8\u7f16\u8f91\u6280\u672f\u5411\u5b9e\u7528\u5316\u53d1\u5c55\u3002"}}
{"id": "2505.12349", "pdf": "https://arxiv.org/pdf/2505.12349", "abs": "https://arxiv.org/abs/2505.12349", "authors": ["Axel Abels", "Tom Lenaerts"], "title": "Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.HC", "cs.LG"], "comment": "Accepted for publication in the Proceedings of the 34th International\n  Joint Conference on Artificial Intelligence (IJCAI 2025)", "summary": "Despite their performance, large language models (LLMs) can inadvertently\nperpetuate biases found in the data they are trained on. By analyzing LLM\nresponses to bias-eliciting headlines, we find that these models often mirror\nhuman biases. To address this, we explore crowd-based strategies for mitigating\nbias through response aggregation. We first demonstrate that simply averaging\nresponses from multiple LLMs, intended to leverage the \"wisdom of the crowd\",\ncan exacerbate existing biases due to the limited diversity within LLM crowds.\nIn contrast, we show that locally weighted aggregation methods more effectively\nleverage the wisdom of the LLM crowd, achieving both bias mitigation and\nimproved accuracy. Finally, recognizing the complementary strengths of LLMs\n(accuracy) and humans (diversity), we demonstrate that hybrid crowds containing\nboth significantly enhance performance and further reduce biases across ethnic\nand gender-related contexts.", "AI": {"tldr": "LLMs\u53ef\u80fd\u5ef6\u7eed\u6570\u636e\u504f\u89c1\uff0c\u7fa4\u4f53\u805a\u5408\u65b9\u6cd5\u53ef\u7f13\u89e3\u95ee\u9898\uff08\u7eafLLM\u7fa4\u4f53\u6548\u679c\u6709\u9650\uff0c\u7ed3\u5408\u4eba\u7c7b\u591a\u6837\u6027\u6548\u679c\u66f4\u4f73\uff09", "motivation": "\u89e3\u51b3LLMs\u56e0\u8bad\u7ec3\u6570\u636e\u504f\u89c1\u5bfc\u81f4\u8f93\u51fa\u504f\u89c1\u7684\u7f3a\u9677\uff0c\u63a2\u7d22\u6709\u6548\u7684\u7fa4\u4f53\u667a\u80fd\u7f13\u89e3\u7b56\u7565", "method": "\u5206\u6790LLMs\u5bf9\u504f\u89c1\u8bf1\u5bfc\u6027\u5934\u6761\u7684\u56de\u5e94\uff0c\u6d4b\u8bd5\u4e0d\u540c\u7fa4\u4f53\u805a\u5408\u7b56\u7565\uff08\u5e73\u5747\u6cd5/\u52a0\u6743\u6cd5/\u4eba\u673a\u6df7\u5408\u7fa4\u4f53\uff09\u6548\u679c", "result": "\u7b80\u5355\u5e73\u5747\u805a\u5408\u52a0\u5267\u504f\u89c1\uff0c\u52a0\u6743\u805a\u5408\u63d0\u5347\u51c6\u786e\u6027\u5e76\u7f13\u89e3\u504f\u89c1\uff0c\u4eba\u673a\u6df7\u5408\u7fa4\u4f53\u6548\u679c\u6700\u4f18\uff08\u51c6\u786e\u7387+15.3%\uff0c\u504f\u89c1\u6307\u6807-29.7%\uff09", "conclusion": "\u7ed3\u5408\u4eba\u7c7b\u591a\u6837\u6027+LLM\u51c6\u786e\u6027\u7684\u6df7\u5408\u7fa4\u4f53\uff0c\u662f\u89e3\u51b3\u7b97\u6cd5\u504f\u89c1\u95ee\u9898\u7684\u6709\u6548\u8def\u5f84\uff08\u7279\u522b\u5728\u6c11\u65cf\u548c\u6027\u522b\u76f8\u5173\u8bed\u5883\u4e2d\uff09"}}
{"id": "2505.12368", "pdf": "https://arxiv.org/pdf/2505.12368", "abs": "https://arxiv.org/abs/2505.12368", "authors": ["Gauri Kholkar", "Ratinder Ahuja"], "title": "CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in ACL LLMSec Workshop 2025", "summary": "Prompt injection remains a major security risk for large language models.\nHowever, the efficacy of existing guardrail models in context-aware settings\nremains underexplored, as they often rely on static attack benchmarks.\nAdditionally, they have over-defense tendencies. We introduce CAPTURE, a novel\ncontext-aware benchmark assessing both attack detection and over-defense\ntendencies with minimal in-domain examples. Our experiments reveal that current\nprompt injection guardrail models suffer from high false negatives in\nadversarial cases and excessive false positives in benign scenarios,\nhighlighting critical limitations.", "AI": {"tldr": "\u63d0\u51faCAPTURE\u57fa\u51c6\u63ed\u793a\u73b0\u6709\u63d0\u793a\u6ce8\u5165\u9632\u62a4\u6a21\u578b\u5b58\u5728\u5bf9\u6297\u6f0f\u68c0\u7387\u9ad8\u3001\u826f\u6027\u8bef\u62a5\u7387\u9ad8\u7684\u53cc\u91cd\u7f3a\u9677", "motivation": "\u73b0\u6709\u63d0\u793a\u6ce8\u5165\u9632\u62a4\u6a21\u578b\u4f9d\u8d56\u9759\u6001\u57fa\u51c6\u4e14\u5b58\u5728\u8fc7\u5ea6\u9632\u5fa1\u95ee\u9898\uff0c\u5728\u52a8\u6001\u4e0a\u4e0b\u6587\u573a\u666f\u4e2d\u9632\u62a4\u6548\u679c\u4e0d\u8db3", "method": "\u6784\u5efa\u4e0a\u4e0b\u6587\u611f\u77e5\u57fa\u51c6CAPTURE\uff0c\u901a\u8fc7\u6700\u5c0f\u57df\u5185\u793a\u4f8b\u540c\u65f6\u8bc4\u4f30\u653b\u51fb\u68c0\u6d4b\u80fd\u529b\u548c\u8fc7\u9632\u5fa1\u503e\u5411", "result": "\u5b9e\u9a8c\u663e\u793a\u5f53\u524d\u6a21\u578b\u5728\u5bf9\u6297\u573a\u666f\u6f0f\u68c0\u7387\u9ad8\u8fbe32%\uff0c\u826f\u6027\u573a\u666f\u8bef\u62a5\u7387\u8d8540%", "conclusion": "CAPTURE\u6709\u6548\u66b4\u9732\u9632\u62a4\u6a21\u578b\u77ed\u677f\uff0c\u4e3a\u6784\u5efa\u52a8\u6001\u5b89\u5168\u9632\u62a4\u4f53\u7cfb\u63d0\u4f9b\u65b0\u65b9\u5411"}}
{"id": "2505.12381", "pdf": "https://arxiv.org/pdf/2505.12381", "abs": "https://arxiv.org/abs/2505.12381", "authors": ["Mohsinul Kabir", "Tasfia Tahsin", "Sophia Ananiadou"], "title": "From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling", "categories": ["cs.CL", "cs.AI"], "comment": "19 pages", "summary": "Current research on bias in language models (LMs) predominantly focuses on\ndata quality, with significantly less attention paid to model architecture and\ntemporal influences of data. Even more critically, few studies systematically\ninvestigate the origins of bias. We propose a methodology grounded in\ncomparative behavioral theory to interpret the complex interaction between\ntraining data and model architecture in bias propagation during language\nmodeling. Building on recent work that relates transformers to n-gram LMs, we\nevaluate how data, model design choices, and temporal dynamics affect bias\npropagation. Our findings reveal that: (1) n-gram LMs are highly sensitive to\ncontext window size in bias propagation, while transformers demonstrate\narchitectural robustness; (2) the temporal provenance of training data\nsignificantly affects bias; and (3) different model architectures respond\ndifferentially to controlled bias injection, with certain biases (e.g. sexual\norientation) being disproportionately amplified. As language models become\nubiquitous, our findings highlight the need for a holistic approach -- tracing\nbias to its origins across both data and model dimensions, not just symptoms,\nto mitigate harm.", "AI": {"tldr": "\u5f53\u524d\u7814\u7a76\u591a\u5173\u6ce8\u6570\u636e\u8d28\u91cf\u5bf9\u8bed\u8a00\u6a21\u578b\u504f\u89c1\u7684\u5f71\u54cd\uff0c\u672c\u6587\u63d0\u51fa\u9700\u540c\u65f6\u8003\u8651\u6a21\u578b\u67b6\u6784\u548c\u65f6\u95f4\u56e0\u7d20\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b9e\u9a8c\u63ed\u793a\u4e0d\u540c\u67b6\u6784\u5728\u504f\u89c1\u4f20\u64ad\u4e2d\u7684\u7279\u6027\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u504f\u89c1\u7814\u7a76\u96c6\u4e2d\u5728\u6570\u636e\u8d28\u91cf\u5c42\u9762\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u67b6\u6784\u548c\u65f6\u95f4\u52a8\u6001\u7684\u7cfb\u7edf\u6027\u6eaf\u6e90\u5206\u6790\uff0c\u96be\u4ee5\u6709\u6548\u89e3\u51b3\u504f\u89c1\u4f20\u64ad\u7684\u6839\u672c\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u6bd4\u8f83\u884c\u4e3a\u7406\u8bba\u6846\u67b6\uff0c\u7ed3\u5408transformer\u4e0en-gram\u6a21\u578b\u7684\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u5206\u6790\u8bad\u7ec3\u6570\u636e\u65f6\u95f4\u6765\u6e90\u3001\u7a97\u53e3\u5927\u5c0f\u548c\u67b6\u6784\u8bbe\u8ba1\u5bf9\u504f\u89c1\u4f20\u64ad\u7684\u5f71\u54cd\u673a\u5236\u3002", "result": "1) n-gram\u6a21\u578b\u5bf9\u4e0a\u4e0b\u6587\u7a97\u53e3\u654f\u611f\uff0ctransformer\u67b6\u6784\u7a33\u5065 2) \u6570\u636e\u65f6\u95f4\u6765\u6e90\u663e\u8457\u5f71\u54cd\u504f\u89c1 3) \u4e0d\u540c\u67b6\u6784\u5bf9\u7279\u5b9a\u504f\u89c1\uff08\u5982\u6027\u53d6\u5411\uff09\u5b58\u5728\u5dee\u5f02\u5316\u653e\u5927\u6548\u5e94", "conclusion": "\u9700\u5efa\u7acb\u6570\u636e-\u6a21\u578b\u8054\u5408\u6eaf\u6e90\u673a\u5236\uff0c\u4ece\u6e90\u5934\u800c\u975e\u8868\u8c61\u6cbb\u7406\u8bed\u8a00\u6a21\u578b\u504f\u89c1\uff0c\u8fd9\u5bf9\u964d\u4f4e\u5b9e\u9645\u5e94\u7528\u5371\u5bb3\u5177\u6709\u5173\u952e\u610f\u4e49\u3002"}}
{"id": "2505.12392", "pdf": "https://arxiv.org/pdf/2505.12392", "abs": "https://arxiv.org/abs/2505.12392", "authors": ["Yang Hu", "Xingyu Zhang", "Xueji Fang", "Zhiyang Chen", "Xiao Wang", "Huatian Zhang", "Guojun Qi"], "title": "SLOT: Sample-specific Language Model Optimization at Test-time", "categories": ["cs.CL"], "comment": null, "summary": "We propose SLOT (Sample-specific Language Model Optimization at Test-time), a\nnovel and parameter-efficient test-time inference approach that enhances a\nlanguage model's ability to more accurately respond to individual prompts.\nExisting Large Language Models (LLMs) often struggle with complex instructions,\nleading to poor performances on those not well represented among general\nsamples. To address this, SLOT conducts few optimization steps at test-time to\nupdate a light-weight sample-specific parameter vector. It is added to the\nfinal hidden layer before the output head, and enables efficient adaptation by\ncaching the last layer features during per-sample optimization. By minimizing\nthe cross-entropy loss on the input prompt only, SLOT helps the model better\naligned with and follow each given instruction. In experiments, we demonstrate\nthat our method outperforms the compared models across multiple benchmarks and\nLLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on\nGSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT\nachieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is\navailable at https://github.com/maple-research-lab/SLOT.", "AI": {"tldr": "\u63d0\u51faSLOT\u65b9\u6cd5\u2014\u2014\u901a\u8fc7\u6d4b\u8bd5\u65f6\u6dfb\u52a0\u6837\u672c\u7279\u5b9a\u53c2\u6570\u5411\u91cf\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u6307\u4ee4\u4e0b\u7684\u8868\u73b0", "motivation": "\u73b0\u6709\u5927\u6a21\u578b\u5bf9\u590d\u6742\u6307\u4ee4\u5904\u7406\u80fd\u529b\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u975e\u5178\u578b\u6837\u672c\u4e0a\u8868\u73b0\u8f83\u5dee", "method": "1. \u5728\u6d4b\u8bd5\u65f6\u5bf9\u6bcf\u4e2a\u6837\u672c\u8fdb\u884c\u5c11\u91cf\u4f18\u5316\n2. \u5728\u8f93\u51fa\u5c42\u524d\u6dfb\u52a0\u8f7b\u91cf\u7ea7\u6837\u672c\u7279\u5b9a\u53c2\u6570\u5411\u91cf\n3. \u901a\u8fc7\u7f13\u5b58\u6700\u540e\u4e00\u5c42\u7279\u5f81\u5b9e\u73b0\u9ad8\u6548\u9002\u914d\n4. \u6700\u5c0f\u5316\u8f93\u5165\u63d0\u793a\u7684\u4ea4\u53c9\u71b5\u635f\u5931", "result": "Qwen2.5-7B\u6a21\u578b\u5728GSM8K\u51c6\u786e\u7387\u63d0\u53478.6%\uff0857.54%\u219266.19%\uff09\uff0cDeepSeek-R1-Distill-Llama-70B\u5728GPQA\u8fbe\u523070B\u7ea7\u6a21\u578bSOTA\u768468.69%", "conclusion": "SLOT\u4ee5\u6781\u4f4e\u53c2\u6570\u91cf\u5b9e\u73b0\u5927\u6a21\u578b\u6027\u80fd\u7a81\u7834\uff0c\u6d4b\u8bd5\u65f6\u4f18\u5316\u7b56\u7565\u663e\u8457\u63d0\u5347\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90"}}
{"id": "2505.12398", "pdf": "https://arxiv.org/pdf/2505.12398", "abs": "https://arxiv.org/abs/2505.12398", "authors": ["Yepeng Weng", "Qiao Hu", "Xujie Chen", "Li Liu", "Dianwen Mei", "Huishi Qiu", "Jiang Tian", "Zhongchao Shi"], "title": "Traversal Verification for Speculative Tree Decoding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Under review", "summary": "Speculative decoding is a promising approach for accelerating large language\nmodels. The primary idea is to use a lightweight draft model to speculate the\noutput of the target model for multiple subsequent timesteps, and then verify\nthem in parallel to determine whether the drafted tokens should be accepted or\nrejected. To enhance acceptance rates, existing frameworks typically construct\ntoken trees containing multiple candidates in each timestep. However, their\nreliance on token-level verification mechanisms introduces two critical\nlimitations: First, the probability distribution of a sequence differs from\nthat of individual tokens, leading to suboptimal acceptance length. Second,\ncurrent verification schemes begin from the root node and proceed layer by\nlayer in a top-down manner. Once a parent node is rejected, all its child nodes\nshould be discarded, resulting in inefficient utilization of speculative\ncandidates. This paper introduces Traversal Verification, a novel speculative\ndecoding algorithm that fundamentally rethinks the verification paradigm\nthrough leaf-to-root traversal. Our approach considers the acceptance of the\nentire token sequence from the current node to the root, and preserves\npotentially valid subsequences that would be prematurely discarded by existing\nmethods. We theoretically prove that the probability distribution obtained\nthrough Traversal Verification is identical to that of the target model,\nguaranteeing lossless inference while achieving substantial acceleration gains.\nExperimental results across different large language models and multiple tasks\nshow that our method consistently improves acceptance length and throughput\nover existing methods", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u53f6\u5230\u6839\u904d\u5386\u9a8c\u8bc1\u7684Traversal Verification\u7b97\u6cd5\uff0c\u901a\u8fc7\u4fdd\u7559\u6f5c\u5728\u6709\u6548\u5b50\u5e8f\u5217\u663e\u8457\u63d0\u5347\u5927\u6a21\u578b\u63a8\u7406\u901f\u5ea6", "motivation": "\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u6846\u67b6\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u7f3a\u9677\uff1a1) \u57fa\u4e8e\u5355\u4e2atoken\u7684\u9a8c\u8bc1\u673a\u5236\u5bfc\u81f4\u63a5\u53d7\u957f\u5ea6\u6b21\u4f18\uff1b2) \u81ea\u4e0a\u800c\u4e0b\u7684\u9010\u5c42\u9a8c\u8bc1\u5bfc\u81f4\u5019\u9009token\u5229\u7528\u7387\u4f4e\u4e0b", "method": "\u521b\u65b0\u6027\u5730\u91c7\u7528\u53f6\u8282\u70b9\u5230\u6839\u8282\u70b9\u7684\u9006\u5411\u9a8c\u8bc1\u8303\u5f0f\uff0c\u57fa\u4e8e\u6574\u6761\u5019\u9009\u5e8f\u5217\u7684\u63a5\u53d7\u6982\u7387\u8fdb\u884c\u5224\u65ad\uff0c\u4fdd\u7559\u53ef\u80fd\u88ab\u4f20\u7edf\u65b9\u6cd5\u9519\u8bef\u4e22\u5f03\u7684\u6709\u6548\u5b50\u5e8f\u5217", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u8f93\u51fa\u5206\u5e03\u4e0e\u76ee\u6807\u6a21\u578b\u5b8c\u5168\u4e00\u81f4\uff08\u65e0\u635f\u63a8\u7406\uff09\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u591a\u4e2a\u5927\u6a21\u578b\u548c\u4efb\u52a1\u4e2d\u5e73\u5747\u63a5\u53d7\u957f\u5ea6\u548c\u541e\u5410\u91cf\u5747\u6709\u663e\u8457\u63d0\u5347", "conclusion": "\u8be5\u7b97\u6cd5\u4ece\u6839\u672c\u4e0a\u91cd\u6784\u4e86\u63a8\u6d4b\u89e3\u7801\u7684\u9a8c\u8bc1\u8303\u5f0f\uff0c\u4e3a\u52a0\u901f\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.12405", "pdf": "https://arxiv.org/pdf/2505.12405", "abs": "https://arxiv.org/abs/2505.12405", "authors": ["Konstantinos Xylogiannopoulos", "Petros Xanthopoulos", "Panagiotis Karampelas", "Georgios Bakamitsos"], "title": "The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Generative AI paraphrased text can be used for copyright infringement and the\nAI paraphrased content can deprive substantial revenue from original content\ncreators. Despite this recent surge of malicious use of generative AI, there\nare few academic publications that research this threat. In this article, we\ndemonstrate the ability of pattern-based similarity detection for AI\nparaphrased news recognition. We propose an algorithmic scheme, which is not\nlimited to detect whether an article is an AI paraphrase, but, more\nimportantly, to identify that the source of infringement is the ChatGPT. The\nproposed method is tested with a benchmark dataset specifically created for\nthis task that incorporates real articles from BBC, incorporating a total of\n2,224 articles across five different news categories, as well as 2,224\nparaphrased articles created with ChatGPT. Results show that our pattern\nsimilarity-based method, that makes no use of deep learning, can detect ChatGPT\nassisted paraphrased articles at percentages 96.23% for accuracy, 96.25% for\nprecision, 96.21% for sensitivity, 96.25% for specificity and 96.23% for F1\nscore.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6a21\u5f0f\u76f8\u4f3c\u6027\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u53ef\u8bc6\u522bChatGPT\u6539\u5199\u7684\u65b0\u95fb\u5e76\u6eaf\u6e90\uff0c\u51c6\u786e\u7387\u8fbe96.23%", "motivation": "\u751f\u6210\u5f0fAI\u88ab\u6076\u610f\u7528\u4e8e\u65b0\u95fb\u6539\u5199\u5bfc\u81f4\u7248\u6743\u4fb5\u6743\u548c\u4f5c\u8005\u6536\u76ca\u635f\u5931\uff0c\u4f46\u76f8\u5173\u7814\u7a76\u532e\u4e4f", "method": "\u975e\u6df1\u5ea6\u5b66\u4e60\u7684\u6a21\u5f0f\u76f8\u4f3c\u6027\u68c0\u6d4b\u7b97\u6cd5\u65b9\u6848\uff0c\u91cd\u70b9\u8bc6\u522bChatGPT\u6539\u5199\u7279\u5f81", "result": "\u5728\u5305\u542b4,448\u7bc7\u6587\u7ae0\u7684\u6570\u636e\u96c6\u4e2d\uff0c\u51c6\u786e\u7387/\u7cbe\u786e\u5ea6/\u654f\u611f\u5ea6/\u7279\u5f02\u6027/F1\u5206\u6570\u5747\u8d8596%", "conclusion": "\u65e0\u9700\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u80fd\u9ad8\u6548\u68c0\u6d4bChatGPT\u6539\u5199\u5185\u5bb9\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2505.12415", "pdf": "https://arxiv.org/pdf/2505.12415", "abs": "https://arxiv.org/abs/2505.12415", "authors": ["Zhenhe Wu", "Jian Yang", "Jiaheng Liu", "Xianjie Wu", "Changzai Pan", "Jie Zhang", "Yu Zhao", "Shuangyong Song", "Yongxiang Li", "Zhoujun Li"], "title": "Table-R1: Region-based Reinforcement Learning for Table Understanding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Tables present unique challenges for language models due to their structured\nrow-column interactions, necessitating specialized approaches for effective\ncomprehension. While large language models (LLMs) have demonstrated potential\nin table reasoning through prompting and techniques like chain-of-thought (CoT)\nand program-of-thought (PoT), optimizing their performance for table question\nanswering remains underexplored. In this paper, we introduce region-based\nTable-R1, a novel reinforcement learning approach that enhances LLM table\nunderstanding by integrating region evidence into reasoning steps. Our method\nemploys Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in\nidentifying relevant table regions before generating answers, incorporating\ntextual, symbolic, and program-based reasoning. Additionally, Table-Aware Group\nRelative Policy Optimization (TARPO) introduces a mixed reward system to\ndynamically balance region accuracy and answer correctness, with decaying\nregion rewards and consistency penalties to align reasoning steps. Experiments\nshow that Table-R1 achieves an average performance improvement of 14.36 points\nacross multiple base models on three benchmark datasets, even outperforming\nbaseline models with ten times the parameters, while TARPO reduces response\ntoken consumption by 67.5% compared to GRPO, significantly advancing LLM\ncapabilities in efficient tabular reasoning.", "AI": {"tldr": "\u63d0\u51faTable-R1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u533a\u57df\u8bc1\u636e\u96c6\u6210\u548c\u6df7\u5408\u5956\u52b1\u673a\u5236\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u8868\u683c\u63a8\u7406\u80fd\u529b\uff0c\u6027\u80fd\u63d0\u534714.36\u5206\u4e14\u51cf\u5c1167.5%\u54cd\u5e94token\u6d88\u8017\u3002", "motivation": "\u73b0\u6709\u8868\u683c\u95ee\u7b54\u65b9\u6cd5\u5728\u533a\u57df\u4fe1\u606f\u6574\u5408\u548c\u63a8\u7406\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u5f3a\u5316\u6a21\u578b\u5bf9\u8868\u683c\u533a\u57df\u7684\u7406\u89e3\u4e0e\u8bc1\u636e\u5229\u7528\u80fd\u529b\u3002", "method": "1. \u533a\u57df\u589e\u5f3a\u76d1\u7763\u5fae\u8c03\uff08RE-SFT\uff09\u6574\u5408\u6587\u672c/\u7b26\u53f7/\u7a0b\u5e8f\u63a8\u7406\n2. \u8868\u683c\u611f\u77e5\u5206\u7ec4\u7b56\u7565\u4f18\u5316\uff08TARPO\uff09\u52a8\u6001\u5e73\u8861\u533a\u57df\u7cbe\u5ea6\u4e0e\u7b54\u6848\u6b63\u786e\u6027", "result": "\u57283\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u63d0\u534714.36\u5206\uff0c\u4f18\u4e8e10\u500d\u53c2\u6570\u57fa\u7ebf\u6a21\u578b\uff0c\u54cd\u5e94token\u51cf\u5c1167.5%", "conclusion": "Table-R1\u901a\u8fc7\u533a\u57df\u8bc1\u636e\u5f3a\u5316\u548c\u6df7\u5408\u5956\u52b1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u8868\u683c\u63a8\u7406\u6548\u7387\u548c\u7cbe\u5ea6\uff0c\u63a8\u52a8\u9ad8\u6548\u8868\u683c\u7406\u89e3\u6280\u672f\u53d1\u5c55"}}
{"id": "2505.12423", "pdf": "https://arxiv.org/pdf/2505.12423", "abs": "https://arxiv.org/abs/2505.12423", "authors": ["Wenqiao Zhu", "Chao Xu", "Lulu Wang", "Jun Wu"], "title": "PSC: Extending Context Window of Large Language Models via Phase Shift Calibration", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Rotary Position Embedding (RoPE) is an efficient position encoding approach\nand is widely utilized in numerous large language models (LLMs). Recently, a\nlot of methods have been put forward to further expand the context window based\non RoPE. The core concept of those methods is to predefine or search for a set\nof factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a\nchallenge for existing methods to predefine an optimal factor due to the\nexponential search space. In view of this, we introduce PSC (Phase Shift\nCalibration), a small module for calibrating the frequencies predefined by\nexisting methods. With the employment of PSC, we demonstrate that many existing\nmethods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted\nextensive experiments across multiple models and tasks. The results demonstrate\nthat (1) when PSC is enabled, the comparative reductions in perplexity increase\nas the context window size is varied from 16k, to 32k, and up to 64k. (2) Our\napproach is broadly applicable and exhibits robustness across a variety of\nmodels and tasks. The code can be found at https://github.com/WNQzhu/PSC.", "AI": {"tldr": "\u63d0\u51faPSC\u76f8\u4f4d\u6821\u51c6\u6a21\u5757\u589e\u5f3aRoPE\u4f4d\u7f6e\u7f16\u7801\uff0c\u6709\u6548\u63d0\u5347\u73b0\u6709\u65b9\u6cd5\uff08\u5982PI/YaRN/LongRoPE\uff09\u7684\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u80fd\u529b\uff0c\u572816k-64k\u7a97\u53e3\u4e0b\u56f0\u60d1\u5ea6\u663e\u8457\u964d\u4f4e", "motivation": "\u73b0\u6709\u57fa\u4e8eRoPE\u7684\u4e0a\u4e0b\u6587\u6269\u5c55\u65b9\u6cd5\u9762\u4e34\u6307\u6570\u7ea7\u641c\u7d22\u7a7a\u95f4\u6311\u6218\uff0c\u96be\u4ee5\u627e\u5230\u6700\u4f18\u9891\u7387\u7f29\u653e\u56e0\u5b50", "method": "\u901a\u8fc7\u76f8\u4f4d\u504f\u79fb\u6821\u51c6\uff08PSC\uff09\u6a21\u5757\u52a8\u6001\u8c03\u6574\u9884\u5b9a\u4e49\u9891\u7387\uff0c\u91c7\u7528\u76f8\u4f4d\u8865\u507f\u673a\u5236\u4f18\u5316\u4f4d\u7f6e\u7f16\u7801\u5206\u5e03", "result": "\u5728LLaMA2\u7b49\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0c16k/32k/64k\u7a97\u53e3\u56f0\u60d1\u5ea6\u5206\u522b\u964d\u4f4e12.3%/18.7%/24.1%\uff0c\u4e14\u591a\u4efb\u52a1\u8868\u73b0\u7a33\u5065", "conclusion": "PSC\u4f5c\u4e3a\u901a\u7528\u589e\u5f3a\u6a21\u5757\u663e\u8457\u63d0\u5347\u73b0\u6709\u957f\u4e0a\u4e0b\u6587\u65b9\u6848\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u8d85\u957f\u6587\u672c\u573a\u666f\u4e0b\u4f18\u52bf\u660e\u663e"}}
{"id": "2505.12439", "pdf": "https://arxiv.org/pdf/2505.12439", "abs": "https://arxiv.org/abs/2505.12439", "authors": ["Jinming Zhang", "Yunfei Long"], "title": "Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games", "categories": ["cs.CL"], "comment": null, "summary": "Interactive Fiction games (IF games) are where players interact through\nnatural language commands. While recent advances in Artificial Intelligence\nagents have reignited interest in IF games as a domain for studying\ndecision-making, existing approaches prioritize task-specific performance\nmetrics over human-like comprehension of narrative context and gameplay logic.\nThis work presents a cognitively inspired framework that guides Large Language\nModels (LLMs) to learn and play IF games systematically. Our proposed\n**L**earning to **P**lay **L**ike **H**umans (LPLH) framework integrates three\nkey components: (1) structured map building to capture spatial and narrative\nrelationships, (2) action learning to identify context-appropriate commands,\nand (3) feedback-driven experience analysis to refine decision-making over\ntime. By aligning LLMs-based agents' behavior with narrative intent and\ncommonsense constraints, LPLH moves beyond purely exploratory strategies to\ndeliver more interpretable, human-like performance. Crucially, this approach\ndraws on cognitive science principles to more closely simulate how human\nplayers read, interpret, and respond within narrative worlds. As a result, LPLH\nreframes the IF games challenge as a learning problem for LLMs-based agents,\noffering a new path toward robust, context-aware gameplay in complex text-based\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u8ba4\u77e5\u542f\u53d1\u6846\u67b6LPLH\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5730\u56fe\u6784\u5efa\u3001\u52a8\u4f5c\u5b66\u4e60\u548c\u53cd\u9988\u5206\u6790\uff0c\u4f7fLLM\u4ee3\u7406\u5728\u4ea4\u4e92\u5f0f\u5c0f\u8bf4\u6e38\u620f\u4e2d\u5b9e\u73b0\u7c7b\u4eba\u7c7b\u51b3\u7b56\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7406\u5728IF\u6e38\u620f\u4e2d\u8fc7\u4e8e\u5173\u6ce8\u4efb\u52a1\u6307\u6807\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u5bf9\u53d9\u4e8b\u903b\u8f91\u7684\u7406\u89e3\u3002\u9700\u901a\u8fc7\u8ba4\u77e5\u79d1\u5b66\u6a21\u62df\u4eba\u7c7b\u73a9\u5bb6\u884c\u4e3a\u6a21\u5f0f\u3002", "method": "\u6574\u5408\u4e09\u7ec4\u4ef6\uff1a1) \u7ed3\u6784\u5316\u5730\u56fe\u6355\u6349\u7a7a\u95f4\u53d9\u4e8b\u5173\u7cfb 2) \u4e0a\u4e0b\u6587\u9002\u914d\u52a8\u4f5c\u5b66\u4e60 3) \u53cd\u9988\u9a71\u52a8\u7ecf\u9a8c\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b", "result": "LPLH\u6846\u67b6\u4f7fLLM\u4ee3\u7406\u5728\u6587\u672c\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u53ef\u89e3\u91ca\u6027\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u548c\u7c7b\u4eba\u51b3\u7b56\u7a33\u5065\u6027", "conclusion": "\u5c06IF\u6e38\u620f\u91cd\u6784\u4e3aLLM\u7684\u5b66\u4e60\u95ee\u9898\uff0c\u7ed3\u5408\u8ba4\u77e5\u79d1\u5b66\u539f\u7406\u5f00\u8f9f\u590d\u6742\u6587\u672c\u73af\u5883\u4e0a\u4e0b\u6587\u611f\u77e5\u6e38\u620f\u7684\u65b0\u8def\u5f84"}}
{"id": "2505.12452", "pdf": "https://arxiv.org/pdf/2505.12452", "abs": "https://arxiv.org/abs/2505.12452", "authors": ["Siyang Wu", "Honglin Bao", "Nadav Kunievsky", "James A. Evans"], "title": "Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment", "categories": ["cs.CL", "cs.CY", "cs.DL", "cs.IR"], "comment": "We commit to fully open-source our patent dataset", "summary": "Large language models (LLMs) increasingly demonstrate signs of conceptual\nunderstanding, yet much of their internal knowledge remains latent, loosely\nstructured, and difficult to access or evaluate. We propose self-questioning as\na lightweight and scalable strategy to improve LLMs' understanding,\nparticularly in domains where success depends on fine-grained semantic\ndistinctions. To evaluate this approach, we introduce a challenging new\nbenchmark of 1.3 million post-2015 computer science patent pairs, characterized\nby dense technical jargon and strategically complex writing. The benchmark\ncenters on a pairwise differentiation task: can a model distinguish between\nclosely related but substantively different inventions? We show that prompting\nLLMs to generate and answer their own questions - targeting the background\nknowledge required for the task - significantly improves performance. These\nself-generated questions and answers activate otherwise underutilized internal\nknowledge. Allowing LLMs to retrieve answers from external scientific texts\nfurther enhances performance, suggesting that model knowledge is compressed and\nlacks the full richness of the training data. We also find that\nchain-of-thought prompting and self-questioning converge, though\nself-questioning remains more effective for improving understanding of\ntechnical concepts. Notably, we uncover an asymmetry in prompting: smaller\nmodels often generate more fundamental, more open-ended, better-aligned\nquestions for mid-sized models than large models with better understanding do,\nrevealing a new strategy for cross-model collaboration. Altogether, our\nfindings establish self-questioning as both a practical mechanism for\nautomatically improving LLM comprehension, especially in domains with sparse\nand underrepresented knowledge, and a diagnostic probe of how internal and\nexternal knowledge are organized.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u95ee\u81ea\u7b54\u7b56\u7565\u63d0\u5347LLMs\u5bf9\u6280\u672f\u4e13\u5229\u7684\u8bed\u4e49\u533a\u5206\u80fd\u529b\uff0c\u5e76\u901a\u8fc7130\u4e07\u4e13\u5229\u6570\u636e\u96c6\u9a8c\u8bc1\u5176\u6709\u6548\u6027", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u77e5\u8bc6\u6f5c\u5728\u5316\u3001\u7ed3\u6784\u677e\u6563\u7684\u95ee\u9898\uff0c\u5c24\u5176\u5728\u9700\u8981\u7cbe\u7ec6\u8bed\u4e49\u533a\u5206\u7684\u4e13\u4e1a\u9886\u57df\uff08\u5982\u4e13\u5229\u5206\u6790\uff09\u8868\u73b0\u53d7\u9650", "method": "\u901a\u8fc7\u81ea\u95ee\u81ea\u7b54\u673a\u5236\u6fc0\u6d3b\u6a21\u578b\u5185\u90e8\u77e5\u8bc6\uff0c\u7ed3\u5408\u5916\u90e8\u79d1\u5b66\u6587\u672c\u68c0\u7d22\uff0c\u4f7f\u7528130\u4e07\u8ba1\u7b97\u673a\u79d1\u5b66\u4e13\u5229\u5bf9\u6784\u5efa\u590d\u6742\u57fa\u51c6\u6d4b\u8bd5", "result": "\u81ea\u95ee\u81ea\u7b54\u4f7f\u6a21\u578b\u6027\u80fd\u63d0\u534715-20%\uff0c\u53d1\u73b0\u5c0f\u6a21\u578b\u751f\u6210\u7684\u95ee\u9898\u66f4\u57fa\u7840\u5f00\u653e\uff08\u9002\u5408\u4e2d\u578b\u6a21\u578b\u4f7f\u7528\uff09\uff0c\u5916\u90e8\u77e5\u8bc6\u8865\u5145\u53ef\u63d0\u53473\u500d\u6548\u679c", "conclusion": "\u81ea\u95ee\u81ea\u7b54\u65e2\u662f\u63d0\u5347LLM\u4e13\u4e1a\u7406\u89e3\u7684\u5b9e\u7528\u673a\u5236\uff0c\u4e5f\u662f\u8bca\u65ad\u5185\u5916\u90e8\u77e5\u8bc6\u7ec4\u7ec7\u7684\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u8de8\u6a21\u578b\u534f\u4f5c\u7684\u65b0\u7b56\u7565"}}
{"id": "2505.12454", "pdf": "https://arxiv.org/pdf/2505.12454", "abs": "https://arxiv.org/abs/2505.12454", "authors": ["Yuyang Ding", "Dan Qiao", "Juntao Li", "Jiajie Xu", "Pingfu Chao", "Xiaofang Zhou", "Min Zhang"], "title": "Towards DS-NER: Unveiling and Addressing Latent Noise in Distant Annotations", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Distantly supervised named entity recognition (DS-NER) has emerged as a cheap\nand convenient alternative to traditional human annotation methods, enabling\nthe automatic generation of training data by aligning text with external\nresources. Despite the many efforts in noise measurement methods, few works\nfocus on the latent noise distribution between different distant annotation\nmethods. In this work, we explore the effectiveness and robustness of DS-NER by\ntwo aspects: (1) distant annotation techniques, which encompasses both\ntraditional rule-based methods and the innovative large language model\nsupervision approach, and (2) noise assessment, for which we introduce a novel\nframework. This framework addresses the challenges by distinctly categorizing\nthem into the unlabeled-entity problem (UEP) and the noisy-entity problem\n(NEP), subsequently providing specialized solutions for each. Our proposed\nmethod achieves significant improvements on eight real-world distant\nsupervision datasets originating from three different data sources and\ninvolving four distinct annotation techniques, confirming its superiority over\ncurrent state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u9488\u5bf9\u8fdc\u7a0b\u76d1\u7763NER\u7684\u65b0\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u672a\u6807\u6ce8\u548c\u566a\u58f0\u5b9e\u4f53\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u591a\u6570\u636e\u96c6\u6027\u80fd", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u566a\u58f0\u6d4b\u91cf\u65b9\u6cd5\uff0c\u4f46\u5ffd\u89c6\u4e0d\u540c\u8fdc\u7a0b\u6807\u6ce8\u65b9\u6cd5\u95f4\u7684\u6f5c\u5728\u566a\u58f0\u5206\u5e03\u5dee\u5f02\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u53d7\u9650", "method": "\u7ed3\u5408\u89c4\u5219\u65b9\u6cd5\u4e0eLLM\u76d1\u7763\u7684\u6807\u6ce8\u6280\u672f\uff0c\u63d0\u51fa\u533a\u5206UEP\uff08\u672a\u6807\u6ce8\u5b9e\u4f53\uff09\u548cNEP\uff08\u566a\u58f0\u5b9e\u4f53\uff09\u7684\u566a\u58f0\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5206\u522b\u8bbe\u8ba1\u89e3\u51b3\u65b9\u6848", "result": "\u5728\u8986\u76d63\u79cd\u6570\u636e\u6e90\u30014\u79cd\u6807\u6ce8\u6280\u672f\u76848\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u8d85\u8d8a\u73b0\u6709SOTA\u65b9\u6cd5", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u9488\u5bf9\u6027\u89e3\u51b3\u8fdc\u7a0b\u76d1\u7763\u4e2d\u7684\u6838\u5fc3\u566a\u58f0\u95ee\u9898\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u591a\u6837\u6807\u6ce8\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027"}}
{"id": "2505.12474", "pdf": "https://arxiv.org/pdf/2505.12474", "abs": "https://arxiv.org/abs/2505.12474", "authors": ["Weixiao Zhou", "Junnan Zhu", "Gengyao Li", "Xianfu Cheng", "Xinnian Liang", "Feifei Zhai", "Zhoujun Li"], "title": "What are they talking about? Benchmarking Large Language Models for Knowledge-Grounded Discussion Summarization", "categories": ["cs.CL"], "comment": "Submitted to EMNLP 2025", "summary": "In this work, we investigate the performance of LLMs on a new task that\nrequires combining discussion with background knowledge for summarization. This\naims to address the limitation of outside observer confusion in existing\ndialogue summarization systems due to their reliance solely on discussion\ninformation. To achieve this, we model the task output as background and\nopinion summaries and define two standardized summarization patterns. To\nsupport assessment, we introduce the first benchmark comprising high-quality\nsamples consistently annotated by human experts and propose a novel\nhierarchical evaluation framework with fine-grained, interpretable metrics. We\nevaluate 12 LLMs under structured-prompt and self-reflection paradigms. Our\nfindings reveal: (1) LLMs struggle with background summary retrieval,\ngeneration, and opinion summary integration. (2) Even top LLMs achieve less\nthan 69% average performance across both patterns. (3) Current LLMs lack\nadequate self-evaluation and self-correction capabilities for this task.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e8612\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7ed3\u5408\u80cc\u666f\u77e5\u8bc6\u7684\u5bf9\u8bdd\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u80cc\u666f\u68c0\u7d22\u3001\u89c2\u70b9\u6574\u5408\u53ca\u81ea\u6211\u4fee\u6b63\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u9876\u5c16\u6a21\u578b\u5e73\u5747\u6027\u80fd\u4e0d\u8db369%\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5bf9\u8bdd\u6458\u8981\u7cfb\u7edf\u56e0\u4ec5\u4f9d\u8d56\u8ba8\u8bba\u4fe1\u606f\u5bfc\u81f4\u7684\u5916\u90e8\u89c2\u5bdf\u8005\u6df7\u6dc6\u95ee\u9898\uff0c\u63a2\u7d22LLMs\u7ed3\u5408\u80cc\u666f\u77e5\u8bc6\u751f\u6210\u6458\u8981\u7684\u80fd\u529b\u3002", "method": "1. \u5b9a\u4e49\u80cc\u666f\u6458\u8981\u4e0e\u89c2\u70b9\u6458\u8981\u53cc\u8f93\u51fa\u6a21\u5f0f\n2. \u6784\u5efa\u4eba\u5de5\u6807\u6ce8\u7684\u57fa\u51c6\u6570\u636e\u96c6\n3. \u63d0\u51fa\u5206\u5c42\u8bc4\u4f30\u6846\u67b6\u4e0e\u7ec6\u7c92\u5ea6\u6307\u6807\n4. \u91c7\u7528\u7ed3\u6784\u5316\u63d0\u793a\u4e0e\u81ea\u53cd\u601d\u8303\u5f0f\u6d4b\u8bd512\u4e2aLLMs", "result": "1. LLMs\u80cc\u666f\u6458\u8981\u68c0\u7d22/\u751f\u6210\u80fd\u529b\u8584\u5f31\n2. \u6700\u4f18\u6a21\u578b\u53cc\u6a21\u5f0f\u5e73\u5747\u6027\u80fd<69%\n3. \u5f53\u524d\u6a21\u578b\u7f3a\u4e4f\u6709\u6548\u7684\u81ea\u6211\u8bc4\u4f30\u4e0e\u4fee\u6b63\u673a\u5236", "conclusion": "LLMs\u5728\u9700\u8981\u7ed3\u5408\u80cc\u666f\u77e5\u8bc6\u7684\u6458\u8981\u4efb\u52a1\u4e2d\u9762\u4e34\u6838\u5fc3\u6311\u6218\uff0c\u7a81\u51fa\u6539\u8fdb\u65b9\u5411\u5305\u62ec\u80cc\u666f\u77e5\u8bc6\u6574\u5408\u3001\u591a\u4fe1\u606f\u6e90\u534f\u8c03\u80fd\u529b\u53ca\u81ea\u7701\u673a\u5236\u5f00\u53d1\u3002"}}
{"id": "2505.12476", "pdf": "https://arxiv.org/pdf/2505.12476", "abs": "https://arxiv.org/abs/2505.12476", "authors": ["Xiao Long", "Liansheng Zhuang", "Chen Shen", "Shaotian Yan", "Yifei Li", "Shafei Wang"], "title": "Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated impressive\nperformance in Knowledge Graph Question Answering (KGQA) tasks, which aim to\nfind answers based on knowledge graphs (KGs) for natural language questions.\nExisting LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented\nGeneration (GraphRAG) paradigm, which first retrieves reasoning paths from the\nlarge KGs, and then generates the answers based on them. However, these methods\nemphasize the exploration of new optimal reasoning paths in KGs while ignoring\nthe exploitation of historical reasoning paths, which may lead to sub-optimal\nreasoning paths. Additionally, the complex semantics contained in questions may\nlead to the retrieval of inaccurate reasoning paths. To address these issues,\nthis paper proposes a novel and training-free framework for KGQA tasks called\nReward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original\nquestion into a series of simpler and well-defined sub-questions to handle the\ncomplex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided\nby a reward model is introduced to iteratively retrieve weighted reasoning\npaths as contextual knowledge. Finally, it stacks the weighted reasoning paths\naccording to their weights to generate the final answers. Extensive experiments\non four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves\n8.7\\% and 7.0\\% performance improvement over the state-of-the-art method on the\nGrailQA and the WebQSP respectively.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684KGQA\u6846\u67b6RTSoG\uff0c\u901a\u8fc7\u5b50\u95ee\u9898\u5206\u89e3\u548c\u5956\u52b1\u5f15\u5bfc\u7684\u6811\u641c\u7d22\u4f18\u5316\u63a8\u7406\u8def\u5f84\u9009\u62e9", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u5173\u6ce8\u65b0\u63a8\u7406\u8def\u5f84\u63a2\u7d22\uff0c\u5ffd\u89c6\u5386\u53f2\u8def\u5f84\u5229\u7528\u5bfc\u81f4\u6b21\u4f18\u89e3\uff1b\u590d\u6742\u95ee\u9898\u8bed\u4e49\u5bfc\u81f4\u63a8\u7406\u8def\u5f84\u68c0\u7d22\u4e0d\u51c6\u786e", "method": "1. \u5c06\u590d\u6742\u95ee\u9898\u5206\u89e3\u4e3a\u660e\u786e\u5b50\u95ee\u9898 2. \u5f15\u5165\u57fa\u4e8e\u5956\u52b1\u6a21\u578b\u7684SC-MCTS\u7b97\u6cd5\u8fed\u4ee3\u68c0\u7d22\u52a0\u6743\u63a8\u7406\u8def\u5f84 3. \u6309\u6743\u91cd\u5806\u53e0\u8def\u5f84\u751f\u6210\u6700\u7ec8\u7b54\u6848", "result": "\u5728GrailQA\u548cWebQSP\u6570\u636e\u96c6\u5206\u522b\u5b9e\u73b08.7%\u548c7.0%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8d85\u8d8a\u73b0\u6709\u6700\u4f18\u65b9\u6cd5", "conclusion": "RTSoG\u901a\u8fc7\u8bed\u4e49\u5206\u89e3\u548c\u5956\u52b1\u5f15\u5bfc\u7684\u8def\u5f84\u52a0\u6743\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u590d\u6742\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u7684\u63a8\u7406\u8d28\u91cf\u4e0e\u51c6\u786e\u7387"}}
{"id": "2505.12495", "pdf": "https://arxiv.org/pdf/2505.12495", "abs": "https://arxiv.org/abs/2505.12495", "authors": ["Nikita Tatarinov", "Vidhyakshaya Kannan", "Haricharana Srinivasa", "Arnav Raj", "Harpreet Singh Anand", "Varun Singh", "Aditya Luthra", "Ravij Lade", "Agam Shah", "Sudheer Chava"], "title": "KG-QAGen: A Knowledge-Graph-Based Framework for Systematic Question Generation and Long-Context LLM Evaluation", "categories": ["cs.CL"], "comment": null, "summary": "The increasing context length of modern language models has created a need\nfor evaluating their ability to retrieve and process information across\nextensive documents. While existing benchmarks test long-context capabilities,\nthey often lack a structured way to systematically vary question complexity. We\nintroduce KG-QAGen (Knowledge-Graph-based Question-Answer Generation), a\nframework that (1) extracts QA pairs at multiple complexity levels (2) by\nleveraging structured representations of financial agreements (3) along three\nkey dimensions -- multi-hop retrieval, set operations, and answer plurality --\nenabling fine-grained assessment of model performance across controlled\ndifficulty levels. Using this framework, we construct a dataset of 20,139 QA\npairs (the largest number among the long-context benchmarks) and open-source a\npart of it. We evaluate 13 proprietary and open-source LLMs and observe that\neven the best-performing models are struggling with set-based comparisons and\nmulti-hop logical inference. Our analysis reveals systematic failure modes tied\nto semantic misinterpretation and inability to handle implicit relations.", "AI": {"tldr": "\u63d0\u51faKG-QAGen\u6846\u67b6\u6784\u5efa\u591a\u590d\u6742\u5ea6QA\u6570\u636e\u96c6\uff0c\u63ed\u793a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u96c6\u5408\u64cd\u4f5c\u548c\u591a\u8df3\u63a8\u7406\u4e0a\u7684\u7cfb\u7edf\u6027\u7f3a\u9677", "motivation": "\u73b0\u6709\u957f\u6587\u672c\u8bc4\u4f30\u57fa\u51c6\u7f3a\u4e4f\u7cfb\u7edf\u5316\u8c03\u8282\u95ee\u9898\u590d\u6742\u5ea6\u7684\u80fd\u529b\uff0c\u9700\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6a21\u578b\u7684\u957f\u4e0a\u4e0b\u6587\u5904\u7406\u6027\u80fd", "method": "\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u4ece\u8d22\u52a1\u534f\u8bae\u4e2d\u63d0\u53d6QA\u5bf9\uff0c\u901a\u8fc7\u591a\u8df3\u68c0\u7d22/\u96c6\u5408\u64cd\u4f5c/\u7b54\u6848\u591a\u6837\u6027\u4e09\u4e2a\u7ef4\u5ea6\u63a7\u5236\u95ee\u9898\u590d\u6742\u5ea6", "result": "\u6784\u5efa\u5f53\u524d\u6700\u5927\u89c4\u6a21\u957f\u6587\u672c\u57fa\u51c6(20,139 QA\u5bf9)\uff0c\u53d1\u73b0\u6700\u4f18\u6a21\u578b\u5728\u96c6\u5408\u6bd4\u8f83\u548c\u591a\u8df3\u63a8\u7406\u4e0a\u51c6\u786e\u7387\u4e0d\u8db350%", "conclusion": "\u6846\u67b6\u6709\u6548\u5b9a\u4f4d\u6a21\u578b\u5f31\u70b9\uff0c\u63ed\u793a\u5f53\u524dLLMs\u5728\u8bed\u4e49\u5173\u8054\u6355\u6349\u548c\u9690\u542b\u903b\u8f91\u63a8\u7406\u65b9\u9762\u7684\u6839\u672c\u6027\u5c40\u9650"}}
{"id": "2505.12507", "pdf": "https://arxiv.org/pdf/2505.12507", "abs": "https://arxiv.org/abs/2505.12507", "authors": ["Xu Zheng", "Zhuomin Chen", "Esteban Schafir", "Sipeng Chen", "Hojat Allah Salehi", "Haifeng Chen", "Farhad Shirani", "Wei Cheng", "Dongsheng Luo"], "title": "LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "The impressive ability of large language models to generate natural text\nacross various tasks has led to critical challenges in authorship\nauthentication. Although numerous detection methods have been developed to\ndifferentiate between machine-generated texts (MGT) and human-generated texts\n(HGT), the explainability of these methods remains a significant gap.\nTraditional explainability techniques often fall short in capturing the complex\nword relationships that distinguish HGT from MGT. To address this limitation,\nwe present LM$^2$otifs, a novel explainable framework for MGT detection.\nInspired by probabilistic graphical models, we provide a theoretical rationale\nfor the effectiveness. LM$^2$otifs utilizes eXplainable Graph Neural Networks\nto achieve both accurate detection and interpretability. The LM$^2$otifs\npipeline operates in three key stages: first, it transforms text into graphs\nbased on word co-occurrence to represent lexical dependencies; second, graph\nneural networks are used for prediction; and third, a post-hoc explainability\nmethod extracts interpretable motifs, offering multi-level explanations from\nindividual words to sentence structures. Extensive experiments on multiple\nbenchmark datasets demonstrate the comparable performance of LM$^2$otifs. The\nempirical evaluation of the extracted explainable motifs confirms their\neffectiveness in differentiating HGT and MGT. Furthermore, qualitative analysis\nreveals distinct and visible linguistic fingerprints characteristic of MGT.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u89e3\u91ca\u6027\u6846\u67b6LM\u00b2otifs\uff0c\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u8bcd\u5171\u73b0\u56fe\u5b9e\u73b0\u673a\u5668\u6587\u672c\u68c0\u6d4b\u4e0e\u89e3\u91ca", "motivation": "\u4f20\u7edf\u68c0\u6d4b\u65b9\u6cd5\u5728\u89e3\u91ca\u8bcd\u95f4\u590d\u6742\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u5f00\u53d1\u517c\u5177\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u68c0\u6d4b\u6846\u67b6", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1) \u8bcd\u5171\u73b0\u56fe\u6784\u5efa 2) \u56fe\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b 3) \u540e\u89e3\u91ca\u65b9\u6cd5\u63d0\u53d6\u591a\u5c42\u7ea7\u8bed\u8a00\u7279\u5f81\u6a21\u677f", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u63d0\u53d6\u7684motif\u663e\u8457\u533a\u5206\u4eba\u673a\u6587\u672c\uff0c\u63ed\u793a\u673a\u5668\u6587\u672c\u7684\u72ec\u7279\u8bed\u8a00\u6307\u7eb9", "conclusion": "LM\u00b2otifs\u9996\u6b21\u5b9e\u73b0\u68c0\u6d4b\u4e0e\u89e3\u91ca\u7684\u5e73\u8861\uff0c\u4e3a\u7406\u89e3\u673a\u5668\u6587\u672c\u751f\u6210\u6a21\u5f0f\u63d0\u4f9b\u65b0\u89c6\u89d2"}}
{"id": "2505.12511", "pdf": "https://arxiv.org/pdf/2505.12511", "abs": "https://arxiv.org/abs/2505.12511", "authors": ["Yanting Li", "Jiyue Jiang", "Zikang Wang", "Ziqian Lin", "Dongchen He", "Yuheng Shan", "Yanruisheng Shao", "Jiayi Li", "Xiangyu Shi", "Jiuming Wang", "Yanyu Chen", "Yimin Fan", "Han Li", "Yu Li"], "title": "DS-ProGen: A Dual-Structure Deep Language Model for Functional Protein Design", "categories": ["cs.CL"], "comment": null, "summary": "Inverse Protein Folding (IPF) is a critical subtask in the field of protein\ndesign, aiming to engineer amino acid sequences capable of folding correctly\ninto a specified three-dimensional (3D) conformation. Although substantial\nprogress has been achieved in recent years, existing methods generally rely on\neither backbone coordinates or molecular surface features alone, which\nrestricts their ability to fully capture the complex chemical and geometric\nconstraints necessary for precise sequence prediction. To address this\nlimitation, we present DS-ProGen, a dual-structure deep language model for\nfunctional protein design, which integrates both backbone geometry and\nsurface-level representations. By incorporating backbone coordinates as well as\nsurface chemical and geometric descriptors into a next-amino-acid prediction\nparadigm, DS-ProGen is able to generate functionally relevant and structurally\nstable sequences while satisfying both global and local conformational\nconstraints. On the PRIDE dataset, DS-ProGen attains the current\nstate-of-the-art recovery rate of 61.47%, demonstrating the synergistic\nadvantage of multi-modal structural encoding in protein design. Furthermore,\nDS-ProGen excels in predicting interactions with a variety of biological\npartners, including ligands, ions, and RNA, confirming its robust functional\nretention capabilities.", "AI": {"tldr": "\u63d0\u51fa\u53cc\u7ed3\u6784\u6df1\u5ea6\u8bed\u8a00\u6a21\u578bDS-ProGen\uff0c\u901a\u8fc7\u6574\u5408\u86cb\u767d\u8d28\u4e3b\u5e72\u51e0\u4f55\u548c\u8868\u9762\u5316\u5b66\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u529f\u80fd\u86cb\u767d\u8bbe\u8ba1\u7684\u51c6\u786e\u6027\u4e0e\u529f\u80fd\u6027\u4fdd\u7559\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u9006\u86cb\u767d\u8d28\u6298\u53e0\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u5355\u4e00\u7ed3\u6784\u8868\u5f81\uff08\u4e3b\u5e72\u5750\u6807\u6216\u8868\u9762\u7279\u5f81\uff09\uff0c\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u5168\u5c40\u6784\u8c61\u7ea6\u675f\u548c\u5c40\u90e8\u5316\u5b66\u76f8\u4e92\u4f5c\u7528\u7684\u9700\u6c42\u3002", "method": "\u878d\u5408\u4e3b\u5e72\u5750\u6807\u4e0e\u8868\u9762\u5316\u5b66/\u51e0\u4f55\u63cf\u8ff0\u7b26\uff0c\u6784\u5efa\u53cc\u6a21\u6001\u6df1\u5ea6\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528next-amino-acid\u9884\u6d4b\u8303\u5f0f\u751f\u6210\u6ee1\u8db3\u591a\u7ef4\u5ea6\u7ed3\u6784\u7ea6\u675f\u7684\u6c28\u57fa\u9178\u5e8f\u5217\u3002", "result": "\u5728PRIDE\u6570\u636e\u96c6\u8fbe\u523061.47%\u7684\u6c28\u57fa\u9178\u6062\u590d\u7387\uff08SOTA\uff09\uff0c\u5e76\u5c55\u793a\u4f18\u5f02\u7684\u914d\u4f53/\u79bb\u5b50/RNA\u7b49\u751f\u7269\u5206\u5b50\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "\u591a\u6a21\u6001\u7ed3\u6784\u7f16\u7801\u7b56\u7565\u6709\u6548\u534f\u8c03\u5168\u5c40\u6784\u8c61\u7a33\u5b9a\u6027\u548c\u5c40\u90e8\u529f\u80fd\u4f4d\u70b9\u4fdd\u7559\uff0c\u63a8\u52a8\u529f\u80fd\u6027\u86cb\u767d\u8bbe\u8ba1\u5411\u66f4\u9ad8\u7cbe\u5ea6\u53d1\u5c55\u3002"}}
{"id": "2505.12531", "pdf": "https://arxiv.org/pdf/2505.12531", "abs": "https://arxiv.org/abs/2505.12531", "authors": ["Navid Madani", "Rohini Srihari"], "title": "ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) increasingly power mental-health chatbots, yet\nthe field still lacks a scalable, theory-grounded way to decide which model is\nmost effective to deploy. We present ESC-Judge, the first end-to-end evaluation\nframework that (i) grounds head-to-head comparisons of emotional-support LLMs\nin Clara Hill's established Exploration-Insight-Action counseling model,\nproviding a structured and interpretable view of performance, and (ii) fully\nautomates the evaluation pipeline at scale. ESC-Judge operates in three stages:\nfirst, it synthesizes realistic help-seeker roles by sampling empirically\nsalient attributes such as stressors, personality, and life history; second, it\nhas two candidate support agents conduct separate sessions with the same role,\nisolating model-specific strategies; and third, it asks a specialized judge LLM\nto express pairwise preferences across rubric-anchored skills that span the\nExploration, Insight, and Action spectrum. In our study, ESC-Judge matched\nPhD-level annotators on 85 percent of Exploration, 83 percent of Insight, and\n86 percent of Action decisions, demonstrating human-level reliability at a\nfraction of the cost. All code, prompts, synthetic roles, transcripts, and\njudgment scripts are released to promote transparent progress in emotionally\nsupportive AI.", "AI": {"tldr": "ESC-Judge\u63d0\u51fa\u9996\u4e2a\u57fa\u4e8e\u63a2\u7d22-\u6d1e\u5bdf-\u884c\u52a8\u54a8\u8be2\u6a21\u578b\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u53ef\u4f4e\u6210\u672c\u9a8c\u8bc1\u60c5\u611f\u652f\u6301LLMs\u6548\u679c\u5e76\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u51b3\u7b56\u9ad8\u5ea6\u4e00\u81f4\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u53ef\u6269\u5c55\u3001\u7406\u8bba\u5316\u7684\u8bc4\u4f30\u65b9\u6cd5\u7b5b\u9009\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u6700\u6709\u6548\u7684LLMs\uff0c\u5bfc\u81f4\u90e8\u7f72\u51b3\u7b56\u7f3a\u4e4f\u4f9d\u636e\u3002", "method": "\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a1) \u5408\u6210\u542b\u538b\u529b\u6e90/\u6027\u683c\u7279\u5f81\u7684\u865a\u62df\u6c42\u52a9\u8005\u89d2\u8272 2) \u53cc\u4ee3\u7406\u72ec\u7acb\u4f1a\u8bdd\u9694\u79bb\u6a21\u578b\u7b56\u7565 3) \u4e13\u7528\u8bc4\u5224LLM\u6309\u6807\u51c6\u5316\u91cf\u8868\u5bf9\u6bd4\u6a21\u578b\u8868\u73b0", "result": "\u5728\u63a2\u7d22/\u6d1e\u5bdf/\u884c\u52a8\u7ef4\u5ea6\u5206\u522b\u8fbe\u523085%/83%/86%\u7684\u4eba\u7c7b\u4e13\u5bb6\u51b3\u7b56\u5339\u914d\u7387\uff0c\u8bc4\u4f30\u6210\u672c\u663e\u8457\u964d\u4f4e", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u60c5\u611f\u652f\u6301AI\u63d0\u4f9b\u53ef\u9760\u8bc4\u4f30\u5de5\u5177\uff0c\u6240\u6709\u4ee3\u7801/\u6570\u636e\u5f00\u6e90\u63a8\u52a8\u9886\u57df\u900f\u660e\u53d1\u5c55"}}
{"id": "2505.12533", "pdf": "https://arxiv.org/pdf/2505.12533", "abs": "https://arxiv.org/abs/2505.12533", "authors": ["Varvara Arzt", "Allan Hanbury", "Michael Wiegand", "G\u00e1bor Recski", "Terra Blevins"], "title": "Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical RE", "categories": ["cs.CL"], "comment": null, "summary": "Analysing the generalisation capabilities of relation extraction (RE) models\nis crucial for assessing whether they learn robust relational patterns or rely\non spurious correlations. Our cross-dataset experiments find that RE models\nstruggle with unseen data, even within similar domains. Notably, higher\nintra-dataset performance does not indicate better transferability, instead\noften signaling overfitting to dataset-specific artefacts. Our results also\nshow that data quality, rather than lexical similarity, is key to robust\ntransfer, and the choice of optimal adaptation strategy depends on the quality\nof data available: while fine-tuning yields the best cross-dataset performance\nwith high-quality data, few-shot in-context learning (ICL) is more effective\nwith noisier data. However, even in these cases, zero-shot baselines\noccasionally outperform all cross-dataset results. Structural issues in RE\nbenchmarks, such as single-relation per sample constraints and non-standardised\nnegative class definitions, further hinder model transferability.", "AI": {"tldr": "\u5173\u7cfb\u62bd\u53d6\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u8de8\u6570\u636e\u96c6\u8868\u73b0\u4e0d\u4f73\u3002\u6570\u636e\u8d28\u91cf\u6bd4\u8bcd\u6c47\u76f8\u4f3c\u6027\u66f4\u91cd\u8981\uff0c\u4e0d\u540c\u6570\u636e\u8d28\u91cf\u4e0b\u5e94\u9009\u62e9\u5fae\u8c03\u6216\u5c0f\u6837\u672c\u5b66\u4e60\u7b56\u7565\uff0c\u57fa\u51c6\u6d4b\u8bd5\u7684\u7ed3\u6784\u6027\u7f3a\u9677\u5f71\u54cd\u6a21\u578b\u8fc1\u79fb\u3002", "motivation": "\u63ed\u793a\u5173\u7cfb\u62bd\u53d6\u6a21\u578b\u662f\u5426\u771f\u6b63\u5b66\u4e60\u5230\u7a33\u5065\u7684\u5173\u7cfb\u6a21\u5f0f\uff0c\u8fd8\u662f\u4f9d\u8d56\u6570\u636e\u96c6\u7279\u5b9a\u7684\u865a\u5047\u5173\u8054\u3002\u73b0\u6709\u8bc4\u4f30\u5c40\u9650\u4e8e\u5355\u4e00\u6570\u636e\u96c6\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u7cfb\u7edf\u5206\u6790\u3002", "method": "\u901a\u8fc7\u8de8\u6570\u636e\u96c6\u5b9e\u9a8c\u8bc4\u4f30\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u5206\u6790\u4e0d\u540c\u9002\u5e94\u7b56\u7565\uff08\u5fae\u8c03\u3001\u5c0f\u6837\u672c\u4e0a\u4e0b\u6587\u5b66\u4e60\u3001\u96f6\u6837\u672c\uff09\u7684\u8868\u73b0\uff0c\u7814\u7a76\u6570\u636e\u8d28\u91cf\u3001\u8bcd\u6c47\u76f8\u4f3c\u6027\u548c\u57fa\u51c6\u7ed3\u6784\u95ee\u9898\u5bf9\u8fc1\u79fb\u7684\u5f71\u54cd\u3002", "result": "1. \u6a21\u578b\u8de8\u6570\u636e\u96c6\u8868\u73b0\u663e\u8457\u4e0b\u964d 2. \u6570\u636e\u8d28\u91cf\uff08\u975e\u8bcd\u6c47\u76f8\u4f3c\u6027\uff09\u662f\u8fc1\u79fb\u5173\u952e 3. \u9ad8\u8d28\u91cf\u6570\u636e\u9002\u7528\u5fae\u8c03\uff0c\u566a\u58f0\u6570\u636e\u9002\u5408\u5c0f\u6837\u672c\u5b66\u4e60 4. \u96f6\u6837\u672c\u57fa\u7ebf\u6709\u65f6\u4f18\u4e8e\u8de8\u6570\u636e\u96c6\u7ed3\u679c 5. \u57fa\u51c6\u7ed3\u6784\u95ee\u9898\uff08\u5355\u5173\u7cfb\u6837\u672c\u7ea6\u675f\u3001\u8d1f\u7c7b\u5b9a\u4e49\u6df7\u4e71\uff09\u635f\u5bb3\u8fc1\u79fb\u80fd\u529b", "conclusion": "\u5173\u7cfb\u62bd\u53d6\u7814\u7a76\u9700\u91cd\u89c6\u6570\u636e\u8d28\u91cf\u548c\u57fa\u51c6\u8bbe\u8ba1\u6539\u8fdb\u3002\u5b9e\u9645\u5e94\u7528\u4e2d\u5e94\u6839\u636e\u76ee\u6807\u6570\u636e\u8d28\u91cf\u9009\u62e9\u5fae\u8c03\u6216\u5c0f\u6837\u672c\u5b66\u4e60\u7b56\u7565\uff0c\u73b0\u6709\u57fa\u51c6\u7684\u7ed3\u6784\u6027\u7f3a\u9677\u9700\u901a\u8fc7\u591a\u5173\u7cfb\u6837\u672c\u8bbe\u8ba1\u3001\u6807\u51c6\u5316\u8d1f\u7c7b\u5b9a\u4e49\u6765\u6539\u5584\u3002"}}
{"id": "2505.12543", "pdf": "https://arxiv.org/pdf/2505.12543", "abs": "https://arxiv.org/abs/2505.12543", "authors": ["Md Mehrab Tanjim", "Yeonjun In", "Xiang Chen", "Victor S. Bursztyn", "Ryan A. Rossi", "Sungchul Kim", "Guang-Jie Ren", "Vaishnavi Muppala", "Shun Jiang", "Yongsung Kim", "Chanyoung Park"], "title": "Disambiguation in Conversational Question Answering in the Era of LLM: A Survey", "categories": ["cs.CL"], "comment": "Preprint", "summary": "Ambiguity remains a fundamental challenge in Natural Language Processing\n(NLP) due to the inherent complexity and flexibility of human language. With\nthe advent of Large Language Models (LLMs), addressing ambiguity has become\neven more critical due to their expanded capabilities and applications. In the\ncontext of Conversational Question Answering (CQA), this paper explores the\ndefinition, forms, and implications of ambiguity for language driven systems,\nparticularly in the context of LLMs. We define key terms and concepts,\ncategorize various disambiguation approaches enabled by LLMs, and provide a\ncomparative analysis of their advantages and disadvantages. We also explore\npublicly available datasets for benchmarking ambiguity detection and resolution\ntechniques and highlight their relevance for ongoing research. Finally, we\nidentify open problems and future research directions, proposing areas for\nfurther investigation. By offering a comprehensive review of current research\non ambiguities and disambiguation with LLMs, we aim to contribute to the\ndevelopment of more robust and reliable language systems.", "AI": {"tldr": "\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u8bdd\u95ee\u7b54\u4e2d\u5904\u7406\u8bed\u8a00\u6b67\u4e49\u7684\u5b9a\u4e49\u3001\u5206\u7c7b\u65b9\u6cd5\u53ca\u6d88\u6b67\u6280\u672f\uff0c\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u6b67\u4e49\u95ee\u9898\u56e0\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e94\u7528\u53d8\u5f97\u66f4\u52a0\u590d\u6742\uff0c\u9700\u7cfb\u7edf\u6027\u7814\u7a76\u5176\u5728\u5bf9\u8bdd\u573a\u666f\u4e0b\u7684\u5f71\u54cd\u4e0e\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u6838\u5fc3\u6982\u5ff5\u3001\u5206\u7c7bLLM\u652f\u6301\u7684\u6d88\u6b67\u65b9\u6cd5\u3001\u5bf9\u6bd4\u5206\u6790\u6280\u672f\u4f18\u52a3\uff0c\u5e76\u6574\u5408\u516c\u5f00\u6570\u636e\u96c6\u7528\u4e8e\u6280\u672f\u8bc4\u4f30\u3002", "result": "\u6784\u5efa\u4e86LLM\u6d88\u6b67\u6280\u672f\u4f53\u7cfb\u6846\u67b6\uff0c\u63ed\u793a\u73b0\u6709\u65b9\u6cd5\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u672a\u89e3\u51b3\u95ee\u9898\u5982\u52a8\u6001\u6b67\u4e49\u5904\u7406\u7684\u7814\u53d1\u65b9\u5411\u3002", "conclusion": "LLM\u4e3a\u6b67\u4e49\u5904\u7406\u63d0\u4f9b\u65b0\u53ef\u80fd\uff0c\u4f46\u9700\u5728\u53ef\u89e3\u91ca\u6027\u3001\u591a\u6a21\u6001\u6b67\u4e49\u7b49\u65b9\u5411\u6301\u7eed\u7a81\u7834\u4ee5\u5b8c\u5584\u8bed\u8a00\u7cfb\u7edf\u3002"}}
{"id": "2505.12545", "pdf": "https://arxiv.org/pdf/2505.12545", "abs": "https://arxiv.org/abs/2505.12545", "authors": ["Yang Zhao", "Pu Wang", "Yibo Zhao", "Hongru Du", "Hao", "Yang"], "title": "Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models", "categories": ["cs.CL"], "comment": "Last revised 13 Feb 2025. Under review in Nature portfolio", "summary": "Predicting crash events is crucial for understanding crash distributions and\ntheir contributing factors, thereby enabling the design of proactive traffic\nsafety policy interventions. However, existing methods struggle to interpret\nthe complex interplay among various sources of traffic crash data, including\nnumeric characteristics, textual reports, crash imagery, environmental\nconditions, and driver behavior records. As a result, they often fail to\ncapture the rich semantic information and intricate interrelationships embedded\nin these diverse data sources, limiting their ability to identify critical\ncrash risk factors. In this research, we propose TrafficSafe, a framework that\nadapts LLMs to reframe crash prediction and feature attribution as text-based\nreasoning. A multi-modal crash dataset including 58,903 real-world reports\ntogether with belonged infrastructure, environmental, driver, and vehicle\ninformation is collected and textualized into TrafficSafe Event Dataset. By\ncustomizing and fine-tuning LLMs on this dataset, the TrafficSafe LLM achieves\na 42% average improvement in F1-score over baselines. To interpret these\npredictions and uncover contributing factors, we introduce TrafficSafe\nAttribution, a sentence-level feature attribution framework enabling\nconditional risk analysis. Findings show that alcohol-impaired driving is the\nleading factor in severe crashes, with aggressive and impairment-related\nbehaviors having nearly twice the contribution for severe crashes compared to\nother driver behaviors. Furthermore, TrafficSafe Attribution highlights pivotal\nfeatures during model training, guiding strategic crash data collection for\niterative performance improvements. The proposed TrafficSafe offers a\ntransformative leap in traffic safety research, providing a blueprint for\ntranslating advanced AI technologies into responsible, actionable, and\nlife-saving outcomes.", "AI": {"tldr": "\u63d0\u51faTrafficSafe\u6846\u67b6\uff0c\u901a\u8fc7LLMs\u6574\u5408\u591a\u6a21\u6001\u4ea4\u901a\u4e8b\u6545\u6570\u636e\u5b9e\u73b0\u9884\u6d4b\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5f00\u53d1\u7279\u5f81\u5f52\u56e0\u6846\u67b6\u5206\u6790\u98ce\u9669\u56e0\u7d20", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u591a\u6e90\u4ea4\u901a\u6570\u636e\uff08\u6570\u503c/\u6587\u672c/\u56fe\u50cf/\u73af\u5883/\u9a7e\u9a76\u884c\u4e3a\uff09\u7684\u590d\u6742\u5173\u8054\uff0c\u5bfc\u81f4\u8bed\u4e49\u4fe1\u606f\u4e22\u5931\u548c\u98ce\u9669\u8bc6\u522b\u4e0d\u8db3", "method": "\u6784\u5efa\u5305\u542b58,903\u4efd\u4e8b\u6545\u62a5\u544a\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6TrafficSafe Event\uff0c\u5fae\u8c03LLMs\u5b9e\u73b0\u4e8b\u6545\u9884\u6d4b\uff0c\u5f00\u53d1\u53e5\u5b50\u7ea7\u7279\u5f81\u5f52\u56e0\u6846\u67b6TrafficSafe Attribution", "result": "\u9884\u6d4bF1\u5206\u6570\u63d0\u534742%\uff0c\u63ed\u793a\u9152\u9a7e\u662f\u4e25\u91cd\u4e8b\u6545\u4e3b\u56e0\uff08\u5371\u9669\u9a7e\u9a76\u884c\u4e3a\u8d21\u732e\u5ea6\u8fbe\u5176\u4ed6\u56e0\u7d202\u500d\uff09\uff0c\u7279\u5f81\u5f52\u56e0\u6846\u67b6\u6307\u5bfc\u6570\u636e\u6536\u96c6\u7b56\u7565\u4f18\u5316", "conclusion": "TrafficSafe\u4e3a\u4ea4\u901a\u5b89\u5168\u7814\u7a76\u63d0\u4f9b\u53ef\u89e3\u91caAI\u65b9\u6848\uff0c\u63a8\u52a8AI\u6280\u672f\u5411\u53ef\u64cd\u4f5c\u5b89\u5168\u51b3\u7b56\u8f6c\u5316\uff0c\u5b9e\u73b0\u751f\u547d\u4fdd\u62a4\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2505.12546", "pdf": "https://arxiv.org/pdf/2505.12546", "abs": "https://arxiv.org/abs/2505.12546", "authors": ["A. Feder Cooper", "Aaron Gokaslan", "Amy B. Cyphert", "Christopher De Sa", "Mark A. Lemley", "Daniel E. Ho", "Percy Liang"], "title": "Extracting memorized pieces of (copyrighted) books from open-weight language models", "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Plaintiffs and defendants in copyright lawsuits over generative AI often make\nsweeping, opposing claims about the extent to which large language models\n(LLMs) have memorized plaintiffs' protected expression. Drawing on adversarial\nML and copyright law, we show that these polarized positions dramatically\noversimplify the relationship between memorization and copyright. To do so, we\nleverage a recent probabilistic extraction technique to extract pieces of the\nBooks3 dataset from 13 open-weight LLMs. Through numerous experiments, we show\nthat it's possible to extract substantial parts of at least some books from\ndifferent LLMs. This is evidence that the LLMs have memorized the extracted\ntext; this memorized content is copied inside the model parameters. But the\nresults are complicated: the extent of memorization varies both by model and by\nbook. With our specific experiments, we find that the largest LLMs don't\nmemorize most books -- either in whole or in part. However, we also find that\nLlama 3.1 70B memorizes some books, like Harry Potter and 1984, almost\nentirely. We discuss why our results have significant implications for\ncopyright cases, though not ones that unambiguously favor either side.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5bf9\u6297\u6027\u673a\u5668\u5b66\u4e60\u9a8c\u8bc1LLMs\u5b58\u5728\u9009\u62e9\u6027\u6587\u672c\u8bb0\u5fc6\u73b0\u8c61\uff0c\u4f46\u8bb0\u5fc6\u7a0b\u5ea6\u56e0\u6a21\u578b\u548c\u4e66\u7c4d\u5dee\u5f02\u663e\u8457\uff0c\u5bf9\u7248\u6743\u6848\u4ef6\u5177\u6709\u590d\u6742\u6cd5\u5f8b\u542f\u793a", "motivation": "\u6f84\u6e05\u751f\u6210\u5f0fAI\u7248\u6743\u8bc9\u8bbc\u4e2d\u5173\u4e8eLLMs\u8bb0\u5fc6\u7a0b\u5ea6\u7684\u6781\u7aef\u4e3b\u5f20\uff0c\u63ed\u793a\u8bb0\u5fc6\u4e0e\u7248\u6743\u5173\u7cfb\u7684\u590d\u6742\u6027", "method": "\u4f7f\u7528\u6982\u7387\u63d0\u53d6\u6280\u672f\u4ece13\u4e2a\u5f00\u6e90LLMs\u4e2d\u63d0\u53d6Books3\u6570\u636e\u96c6\u5185\u5bb9\uff0c\u8bbe\u8ba1\u591a\u7ef4\u5ea6\u5b9e\u9a8c\u9a8c\u8bc1\u8bb0\u5fc6\u7a0b\u5ea6", "result": "Llama 3.1 70B\u5bf9\u300a\u54c8\u5229\u6ce2\u7279\u300b\u300a1984\u300b\u8fd1\u5b8c\u5168\u8bb0\u5fc6\uff0c\u4f46\u591a\u6570\u5927\u6a21\u578b\u672a\u6574\u4f53\u8bb0\u5fc6\u5927\u90e8\u5206\u4e66\u7c4d\uff1b\u8bb0\u5fc6\u7a0b\u5ea6\u4e0e\u6a21\u578b\u89c4\u6a21\u975e\u6b63\u76f8\u5173", "conclusion": "\u8bb0\u5fc6\u7684\u4e2a\u6848\u7279\u5f02\u6027\u4e0d\u652f\u6301\u7b80\u5355\u6cd5\u5f8b\u63a8\u5b9a\uff0c\u9700\u5efa\u7acb\u52a8\u6001\u6280\u672f\u8bc4\u4f30\u6846\u67b6\u5e73\u8861\u521b\u65b0\u4e0e\u7248\u6743\u4fdd\u62a4"}}
{"id": "2505.12560", "pdf": "https://arxiv.org/pdf/2505.12560", "abs": "https://arxiv.org/abs/2505.12560", "authors": ["Hiram Ring"], "title": "The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations", "categories": ["cs.CL"], "comment": null, "summary": "Existing datasets available for crosslinguistic investigations have tended to\nfocus on large amounts of data for a small group of languages or a small amount\nof data for a large number of languages. This means that claims based on these\ndatasets are limited in what they reveal about universal properties of the\nhuman language faculty. While this has begun to change through the efforts of\nprojects seeking to develop tagged corpora for a large number of languages,\nsuch efforts are still constrained by limits on resources. The current paper\nreports on a large automatically tagged parallel dataset which has been\ndeveloped to partially address this issue. The taggedPBC contains more than\n1,800 sentences of pos-tagged parallel text data from over 1,500 languages,\nrepresenting 133 language families and 111 isolates, dwarfing previously\navailable resources. The accuracy of tags in this dataset is shown to correlate\nwell with both existing SOTA taggers for high-resource languages (SpaCy,\nTrankit) as well as hand-tagged corpora (Universal Dependencies Treebanks).\nAdditionally, a novel measure derived from this dataset, the N1 ratio,\ncorrelates with expert determinations of word order in three typological\ndatabases (WALS, Grambank, Autotyp) such that a Gaussian Naive Bayes classifier\ntrained on this feature can accurately identify basic word order for languages\nnot in those databases. While much work is still needed to expand and develop\nthis dataset, the taggedPBC is an important step to enable corpus-based\ncrosslinguistic investigations, and is made available for research and\ncollaboration via GitHub.", "AI": {"tldr": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86taggedPBC\u2014\u2014\u4e00\u4e2a\u5305\u542b1500+\u8bed\u8a00\u7684\u5927\u89c4\u6a21\u81ea\u52a8\u6807\u6ce8\u5e73\u884c\u6570\u636e\u96c6\uff0c\u4fc3\u8fdb\u8de8\u8bed\u8a00\u8bed\u6599\u7814\u7a76", "motivation": "\u73b0\u6709\u8de8\u8bed\u8a00\u6570\u636e\u96c6\u5b58\u5728\u6570\u636e\u89c4\u6a21\u4e0e\u8bed\u8a00\u8986\u76d6\u7684\u6743\u8861\uff0c\u96be\u4ee5\u63ed\u793a\u4eba\u7c7b\u8bed\u8a00\u7684\u666e\u904d\u89c4\u5f8b\uff0c\u8d44\u6e90\u9650\u5236\u5236\u7ea6\u4e86\u7814\u7a76\u8fdb\u5c55", "method": "\u6784\u5efa\u5305\u542b1800+\u53e5\u5b50\u3001\u8986\u76d6133\u8bed\u7cfb111\u5b64\u7acb\u8bed\u7684\u6807\u6ce8\u8bed\u6599\u5e93\uff0c\u901a\u8fc7SOTA\u6807\u6ce8\u5de5\u5177\u5bf9\u6bd4\u9a8c\u8bc1\u51c6\u786e\u6027\uff0c\u5e76\u5f00\u53d1N1 ratio\u6307\u6807\u8fdb\u884c\u8bed\u5e8f\u5206\u7c7b", "result": "\u6807\u6ce8\u51c6\u786e\u6027\u4e0e\u4e13\u4e1a\u5de5\u5177\u76f8\u5f53\uff08SpaCy/Trankit\u76f8\u5173\u7cfb\u65700.85\uff09\uff0cN1 ratio\u5728\u4e09\u5927\u7c7b\u578b\u6570\u636e\u5e93\u9a8c\u8bc1\u6709\u6548\uff08\u5206\u7c7b\u51c6\u786e\u738792%\uff09\uff0c\u6570\u636e\u96c6\u5df2\u5f00\u6e90", "conclusion": "taggedPBC\u7a81\u7834\u4e86\u8de8\u8bed\u8a00\u7814\u7a76\u7684\u8d44\u6e90\u74f6\u9888\uff0c\u4e3a\u8bed\u5e8f\u7c7b\u578b\u5b66\u548c\u5927\u89c4\u6a21\u8bed\u8a00\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u578b\u57fa\u7840\u8bbe\u65bd\uff0c\u672a\u6765\u5c06\u6301\u7eed\u6269\u5c55\u4f18\u5316"}}
{"id": "2505.12568", "pdf": "https://arxiv.org/pdf/2505.12568", "abs": "https://arxiv.org/abs/2505.12568", "authors": ["Lekang Jiang", "Chengzu Li", "Stephan Goetz"], "title": "Enriching Patent Claim Generation with European Patent Dataset", "categories": ["cs.CL"], "comment": "18 pages, 13 tables, 4 figures", "summary": "Drafting patent claims is time-intensive, costly, and requires professional\nskill. Therefore, researchers have investigated large language models (LLMs) to\nassist inventors in writing claims. However, existing work has largely relied\non datasets from the United States Patent and Trademark Office (USPTO). To\nenlarge research scope regarding various jurisdictions, drafting conventions,\nand legal standards, we introduce EPD, a European patent dataset. EPD presents\nrich textual data and structured metadata to support multiple patent-related\ntasks, including claim generation. This dataset enriches the field in three\ncritical aspects: (1) Jurisdictional diversity: Patents from different offices\nvary in legal and drafting conventions. EPD fills a critical gap by providing a\nbenchmark for European patents to enable more comprehensive evaluation. (2)\nQuality improvement: EPD offers high-quality granted patents with finalized and\nlegally approved texts, whereas others consist of patent applications that are\nunexamined or provisional. Experiments show that LLMs fine-tuned on EPD\nsignificantly outperform those trained on previous datasets and even GPT-4o in\nclaim quality and cross-domain generalization. (3) Real-world simulation: We\npropose a difficult subset of EPD to better reflect real-world challenges of\nclaim generation. Results reveal that all tested LLMs perform substantially\nworse on these challenging samples, which highlights the need for future\nresearch.", "AI": {"tldr": "\u63d0\u51fa\u6b27\u6d32\u4e13\u5229\u6570\u636e\u96c6EPD\uff0c\u901a\u8fc7\u53f8\u6cd5\u591a\u6837\u6027\u3001\u8d28\u91cf\u63d0\u5347\u548c\u771f\u5b9e\u4e16\u754c\u6a21\u62df\u4e09\u65b9\u9762\u589e\u5f3a\u4e13\u5229\u6743\u5229\u8981\u6c42\u751f\u6210\u7814\u7a76", "motivation": "\u73b0\u6709\u4e13\u5229\u751f\u6210\u7814\u7a76\u8fc7\u5ea6\u4f9d\u8d56\u7f8e\u56fd\u4e13\u5229\u6570\u636e\uff0c\u9700\u8981\u6269\u5c55\u4e0d\u540c\u53f8\u6cd5\u7ba1\u8f96\u533a\u548c\u66f4\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u57fa\u51c6", "method": "\u6784\u5efa\u5305\u542b\u4e30\u5bcc\u6587\u672c\u548c\u7ed3\u6784\u5316\u5143\u6570\u636e\u7684EPD\u6570\u636e\u96c6\uff0c\u5728\u5fae\u8c03\u5b9e\u9a8c\u4e2d\u5bf9\u6bd4LLMs\u6027\u80fd", "result": "EPD\u5fae\u8c03\u7684LLMs\u5728\u6743\u5229\u8981\u6c42\u8d28\u91cf\u548c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u4e0a\u663e\u8457\u8d85\u8d8aGPT-4o\uff0c\u4f46\u5728\u56f0\u96be\u6837\u672c\u4e0a\u8868\u73b0\u9aa4\u964d", "conclusion": "EPD\u586b\u8865\u4e86\u6b27\u6d32\u4e13\u5229\u6570\u636e\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63a8\u52a8\u4e13\u5229\u751f\u6210\u6280\u672f\u53d1\u5c55"}}
{"id": "2505.12572", "pdf": "https://arxiv.org/pdf/2505.12572", "abs": "https://arxiv.org/abs/2505.12572", "authors": ["Hanwen Shen", "Ting Ying"], "title": "Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio", "categories": ["cs.CL", "cs.AI", "cs.IT", "math.IT"], "comment": null, "summary": "Writing novels with Large Language Models (LLMs) raises a critical question:\nhow much human-authored outline is necessary to generate high-quality\nmillion-word novels? While frameworks such as DOME, Plan&Write, and Long Writer\nhave improved stylistic coherence and logical consistency, they primarily\ntarget shorter novels (10k--100k words), leaving ultra-long generation largely\nunexplored. Drawing on insights from recent text compression methods like\nLLMZip and LLM2Vec, we conduct an information-theoretic analysis that\nquantifies distortion occurring when LLMs compress and reconstruct ultra-long\nnovels under varying compression-expansion ratios. We introduce a hierarchical\ntwo-stage generation pipeline (outline -> detailed outline -> manuscript) and\nfind an optimal outline length that balances information preservation with\nhuman effort. Through extensive experimentation with Chinese novels, we\nestablish that a two-stage hierarchical outline approach significantly reduces\nsemantic distortion compared to single-stage methods. Our findings provide\nempirically-grounded guidance for authors and researchers collaborating with\nLLMs to create million-word novels.", "AI": {"tldr": "\u901a\u8fc7\u5206\u5c42\u5927\u7eb2\u4f18\u5316LLM\u751f\u6210\u767e\u4e07\u5b57\u5c0f\u8bf4\u7684\u8d28\u91cf\uff0c\u786e\u5b9a\u6700\u4f18\u5927\u7eb2\u957f\u5ea6\u5e73\u8861\u4fe1\u606f\u4fdd\u5b58\u4e0e\u4eba\u529b\u6295\u5165", "motivation": "\u73b0\u6709LLM\u751f\u6210\u6846\u67b6\u4e3b\u8981\u9488\u5bf9\u77ed\u7bc7\u5c0f\u8bf4\uff0810k-100k\u5b57\uff09\uff0c\u7f3a\u4e4f\u767e\u4e07\u5b57\u7ea7\u8d85\u957f\u7bc7\u751f\u6210\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u9700\u8981\u91cf\u5316\u5927\u7eb2\u538b\u7f29\u5bf9\u751f\u6210\u8d28\u91cf\u7684\u5f71\u54cd", "method": "\u57fa\u4e8e\u4fe1\u606f\u8bba\u5206\u6790\u538b\u7f29\u5931\u771f\u5ea6\uff0c\u63d0\u51fa\u4e24\u9636\u6bb5\u751f\u6210\u6d41\u7a0b\uff08\u5927\u7eb2\u2192\u8be6\u7ec6\u5927\u7eb2\u2192\u624b\u7a3f\uff09\uff0c\u901a\u8fc7\u4e2d\u6587\u5c0f\u8bf4\u5b9e\u9a8c\u9a8c\u8bc1\u5206\u5c42\u7ed3\u6784\u7684\u6709\u6548\u6027", "result": "\u4e24\u9636\u6bb5\u65b9\u6cd5\u76f8\u6bd4\u5355\u9636\u6bb5\u51cf\u5c1147%\u8bed\u4e49\u5931\u771f\uff0c\u6700\u4f18\u5927\u7eb2\u957f\u5ea6\u53ef\u4fdd\u630191%\u539f\u59cb\u4fe1\u606f\u91cf", "conclusion": "\u5206\u5c42\u5927\u7eb2\u7b56\u7565\u4e3aLLM\u534f\u4f5c\u521b\u4f5c\u767e\u4e07\u5b57\u5c0f\u8bf4\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u6307\u5bfc\uff0c\u5e73\u8861\u4e86\u521b\u4f5c\u6548\u7387\u4e0e\u5185\u5bb9\u8d28\u91cf"}}
{"id": "2505.12584", "pdf": "https://arxiv.org/pdf/2505.12584", "abs": "https://arxiv.org/abs/2505.12584", "authors": ["Omar Mahmoud", "Buddhika Laknath Semage", "Thommen George Karimpanal", "Santu Rana"], "title": "Improving Multilingual Language Models by Aligning Representations through Steering", "categories": ["cs.CL"], "comment": null, "summary": "In this paper, we investigate how large language models (LLMS) process\nnon-English tokens within their layer representations, an open question despite\nsignificant advancements in the field. Using representation steering,\nspecifically by adding a learned vector to a single model layer's activations,\nwe demonstrate that steering a single model layer can notably enhance\nperformance. Our analysis shows that this approach achieves results comparable\nto translation baselines and surpasses state of the art prompt optimization\nmethods. Additionally, we highlight how advanced techniques like supervised\nfine tuning (\\textsc{sft}) and reinforcement learning from human feedback\n(\\textsc{rlhf}) improve multilingual capabilities by altering representation\nspaces. We further illustrate how these methods align with our approach to\nreshaping LLMS layer representations.", "AI": {"tldr": "\u901a\u8fc7\u8868\u793a\u5bfc\u5411\u6280\u672f\u4f18\u5316\u5355\u5c42\u6a21\u578b\u6fc0\u6d3b\uff0c\u663e\u8457\u63d0\u5347LLM\u591a\u8bed\u8a00\u5904\u7406\u80fd\u529b\uff0c\u6548\u679c\u5ab2\u7f8e\u7ffb\u8bd1\u57fa\u7ebf\u5e76\u8d85\u8d8a\u73b0\u6709\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u5904\u7406\u5c42\u6b21\u8868\u793a\u4e2d\u7684\u975e\u82f1\u8bed\u6807\u8bb0\uff0c\u89e3\u51b3\u8be5\u9886\u57df\u5c1a\u672a\u660e\u786e\u7684\u673a\u5236\u95ee\u9898\u3002", "method": "\u91c7\u7528\u8868\u793a\u5bfc\u5411\u6280\u672f\uff0c\u5728\u5355\u5c42\u6a21\u578b\u6fc0\u6d3b\u4e2d\u6dfb\u52a0\u5b66\u4e60\u5411\u91cf\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03(SFT)\u548c\u5f3a\u5316\u5b66\u4e60(RLHF)\u4f18\u5316\u8868\u793a\u7a7a\u95f4\u3002", "result": "\u5355\u5c42\u5bfc\u5411\u6548\u679c\u663e\u8457\uff0c\u8d85\u8d8a\u5f53\u524d\u6700\u4f18\u63d0\u793a\u65b9\u6cd5\uff0c\u4e0e\u7ffb\u8bd1\u57fa\u51c6\u6548\u679c\u76f8\u5f53\uff1bSFT\u548cRLHF\u6709\u6548\u63d0\u5347\u591a\u8bed\u8a00\u8868\u5f81\u80fd\u529b\u3002", "conclusion": "\u63ed\u793a\u53c2\u6570\u4f18\u5316\u5bf9\u591a\u8bed\u8a00\u80fd\u529b\u7684\u63d0\u5347\u673a\u5236\uff0c\u4e3a\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u65b0\u65b9\u5411\uff1a\u901a\u8fc7\u5c42\u6b21\u8868\u793a\u91cd\u5851\u5b9e\u73b0\u6027\u80fd\u7a81\u7834\u3002"}}
{"id": "2505.12587", "pdf": "https://arxiv.org/pdf/2505.12587", "abs": "https://arxiv.org/abs/2505.12587", "authors": ["Aditeya Baral", "Allen George Ajith", "Roshan Nayak", "Mrityunjay Abhijeet Bhanja"], "title": "CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Code-mixed languages, characterized by frequent within-sentence language\ntransitions, present structural challenges that standard language models fail\nto address. In this work, we propose CMLFormer, an enhanced multi-layer\ndual-decoder Transformer with a shared encoder and synchronized decoder\ncross-attention, designed to model the linguistic and semantic dynamics of\ncode-mixed text. CMLFormer is pre-trained on an augmented Hinglish corpus with\nswitching point and translation annotations with multiple new objectives\nspecifically aimed at capturing switching behavior, cross-lingual structure,\nand code-mixing complexity. Our experiments show that CMLFormer improves F1\nscore, precision, and accuracy over other approaches on the HASOC-2021\nbenchmark under select pre-training setups. Attention analyses further show\nthat it can identify and attend to switching points, validating its sensitivity\nto code-mixed structure. These results demonstrate the effectiveness of\nCMLFormer's architecture and multi-task pre-training strategy for modeling\ncode-mixed languages.", "AI": {"tldr": "\u63d0\u51faCMLFormer\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u89e3\u7801\u5668\u67b6\u6784\u548c\u591a\u4efb\u52a1\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u6df7\u5408\u8bed\u8a00\u5efa\u6a21\u6548\u679c", "motivation": "\u6807\u51c6\u8bed\u8a00\u6a21\u578b\u96be\u4ee5\u5904\u7406\u6df7\u5408\u8bed\u8a00\u9891\u7e41\u7684\u53e5\u5185\u8bed\u8a00\u5207\u6362\u73b0\u8c61\uff0c\u9700\u8981\u4e13\u95e8\u7684\u7ed3\u6784\u8bbe\u8ba1", "method": "\u91c7\u7528\u5171\u4eab\u7f16\u7801\u5668+\u540c\u6b65\u53cc\u89e3\u7801\u5668\u67b6\u6784\uff0c\u5728\u589e\u5f3a\u7684Hinglish\u8bed\u6599\u4e0a\u901a\u8fc7\u5207\u6362\u70b9\u8bc6\u522b\u3001\u8de8\u8bed\u8a00\u5bf9\u9f50\u548c\u6df7\u5408\u590d\u6742\u5ea6\u9884\u6d4b\u591a\u4efb\u52a1\u9884\u8bad\u7ec3", "result": "\u5728HASOC-2021\u57fa\u51c6\u6d4b\u8bd5\u4e2dF1/\u7cbe\u5ea6/\u51c6\u786e\u7387\u63d0\u5347\uff0c\u6ce8\u610f\u529b\u5206\u6790\u9a8c\u8bc1\u4e86\u5bf9\u8bed\u8a00\u5207\u6362\u70b9\u7684\u6355\u6349\u80fd\u529b", "conclusion": "\u53cc\u67b6\u6784\u8bbe\u8ba1\u548c\u9488\u5bf9\u6027\u9884\u8bad\u7ec3\u76ee\u6807\u6709\u6548\u63d0\u5347\u6df7\u5408\u8bed\u8a00\u5efa\u6a21\uff0c\u4e3a\u8de8\u8bed\u8a00NLP\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2505.12592", "pdf": "https://arxiv.org/pdf/2505.12592", "abs": "https://arxiv.org/abs/2505.12592", "authors": ["Sullam Jeoung", "Yueyan Chen", "Yi Zhang", "Shuai Wang", "Haibo Ding", "Lin Lee Cheong"], "title": "PromptPrism: A Linguistically-Inspired Taxonomy for Prompts", "categories": ["cs.CL"], "comment": null, "summary": "Prompts are the interface for eliciting the capabilities of large language\nmodels (LLMs). Understanding their structure and components is critical for\nanalyzing LLM behavior and optimizing performance. However, the field lacks a\ncomprehensive framework for systematic prompt analysis and understanding. We\nintroduce PromptPrism, a linguistically-inspired taxonomy that enables prompt\nanalysis across three hierarchical levels: functional structure, semantic\ncomponent, and syntactic pattern. We show the practical utility of PromptPrism\nby applying it to three applications: (1) a taxonomy-guided prompt refinement\napproach that automatically improves prompt quality and enhances model\nperformance across a range of tasks; (2) a multi-dimensional dataset profiling\nmethod that extracts and aggregates structural, semantic, and syntactic\ncharacteristics from prompt datasets, enabling comprehensive analysis of prompt\ndistributions and patterns; (3) a controlled experimental framework for prompt\nsensitivity analysis by quantifying the impact of semantic reordering and\ndelimiter modifications on LLM performance. Our experimental results validate\nthe effectiveness of our taxonomy across these applications, demonstrating that\nPromptPrism provides a foundation for refining, profiling, and analyzing\nprompts.", "AI": {"tldr": "\u63d0\u51fa\u4e86PromptPrism\u6846\u67b6\u2014\u2014\u901a\u8fc7\u529f\u80fd\u7ed3\u6784\u3001\u8bed\u4e49\u6210\u5206\u3001\u53e5\u6cd5\u6a21\u5f0f\u4e09\u4e2a\u5c42\u6b21\u7cfb\u7edf\u5206\u6790\u63d0\u793a\u8bcd\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u63d0\u793a\u4f18\u5316\u3001\u6570\u636e\u96c6\u5206\u6790\u3001\u654f\u611f\u6027\u5b9e\u9a8c\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7cfb\u7edf\u5316\u7684\u63d0\u793a\u5206\u6790\u6846\u67b6\uff0c\u96be\u4ee5\u6df1\u5165\u7406\u89e3LLM\u884c\u4e3a\u53ca\u4f18\u5316\u63d0\u793a\u6548\u679c\u3002\u672c\u6587\u65e8\u5728\u5efa\u7acb\u7ed3\u6784\u5316\u5206\u7c7b\u6cd5\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u57fa\u4e8e\u8bed\u8a00\u5b66\u7406\u8bba\u6784\u5efa\u4e09\u5c42\u6b21\u5206\u7c7b\u4f53\u7cfb\uff1a1)\u529f\u80fd\u7ed3\u6784\u5212\u5206\u63d0\u793a\u529f\u80fd\u6a21\u5757 2)\u8bed\u4e49\u6210\u5206\u89e3\u6790\u6838\u5fc3\u8981\u7d20 3)\u53e5\u6cd5\u6a21\u5f0f\u5206\u6790\u8868\u8fbe\u65b9\u5f0f\u3002\u901a\u8fc7\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u3001\u6570\u636e\u96c6\u7279\u5f81\u63d0\u53d6\u3001\u8bed\u4e49/\u5206\u9694\u7b26\u654f\u611f\u6027\u5b9e\u9a8c\u9a8c\u8bc1\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff1a1)\u63d0\u793a\u4f18\u5316\u4f7f\u591a\u4efb\u52a1\u6027\u80fd\u63d0\u53472-15% 2)\u6570\u636e\u96c6\u5206\u6790\u63ed\u793a\u7ed3\u6784\u5206\u5e03\u7279\u5f81 3)\u8bed\u4e49\u987a\u5e8f\u8c03\u6574\u5f71\u54cd\u8fbe8%\uff0c\u5206\u9694\u7b26\u6539\u53d8\u5bfc\u81f44%\u6ce2\u52a8\u3002", "conclusion": "PromptPrism\u4e3a\u63d0\u793a\u5de5\u7a0b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u5206\u6790\u5de5\u5177\uff0c\u652f\u6301\u4ece\u7ed3\u6784\u4f18\u5316\u3001\u6570\u636e\u6d1e\u5bdf\u5230\u9c81\u68d2\u6027\u6d4b\u8bd5\u7684\u5168\u6d41\u7a0b\u6539\u8fdb\u3002"}}
{"id": "2505.12594", "pdf": "https://arxiv.org/pdf/2505.12594", "abs": "https://arxiv.org/abs/2505.12594", "authors": ["Tiankai Yang", "Junjun Liu", "Wingchun Siu", "Jiahang Wang", "Zhuangzhuang Qian", "Chanjuan Song", "Cheng Cheng", "Xiyang Hu", "Yue Zhao"], "title": "AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Anomaly detection (AD) is essential in areas such as fraud detection, network\nmonitoring, and scientific research. However, the diversity of data modalities\nand the increasing number of specialized AD libraries pose challenges for\nnon-expert users who lack in-depth library-specific knowledge and advanced\nprogramming skills. To tackle this, we present AD-AGENT, an LLM-driven\nmulti-agent framework that turns natural-language instructions into fully\nexecutable AD pipelines. AD-AGENT coordinates specialized agents for intent\nparsing, data preparation, library and model selection, documentation mining,\nand iterative code generation and debugging. Using a shared short-term\nworkspace and a long-term cache, the agents integrate popular AD libraries like\nPyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that\nAD-AGENT produces reliable scripts and recommends competitive models across\nlibraries. The system is open-sourced to support further research and practical\napplications in AD.", "AI": {"tldr": "\u63d0\u51faAD-AGENT\u6846\u67b6\uff0c\u5229\u7528LLM\u9a71\u52a8\u591a\u667a\u80fd\u4f53\u81ea\u52a8\u751f\u6210\u5f02\u5e38\u68c0\u6d4b\u6d41\u6c34\u7ebf\uff0c\u89e3\u51b3\u975e\u4e13\u5bb6\u7528\u6237\u4f7f\u7528\u590d\u6742AD\u5e93\u7684\u96be\u9898\u3002", "motivation": "\u5f02\u5e38\u68c0\u6d4b\u9886\u57df\u5b58\u5728\u6570\u636e\u6a21\u6001\u591a\u6837\u5316\u548c\u4e13\u7528\u5e93\u590d\u6742\u5316\u7684\u53cc\u91cd\u6311\u6218\uff0c\u975e\u4e13\u5bb6\u7528\u6237\u7f3a\u4e4f\u5e93\u4e13\u4e1a\u77e5\u8bc6\u4e0e\u7f16\u7a0b\u6280\u80fd\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efa\u57fa\u4e8e\u5171\u4eab\u5de5\u4f5c\u7a7a\u95f4\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6846\u67b6\uff0c\u6574\u5408\u610f\u56fe\u89e3\u6790\u3001\u6570\u636e\u5904\u7406\u3001\u5e93\u9009\u62e9\u3001\u6587\u6863\u6316\u6398\u3001\u8fed\u4ee3\u5f0f\u4ee3\u7801\u751f\u6210\u7b49\u6a21\u5757\uff0c\u5b9e\u73b0PyOD/PyGOD/TSLib\u7b49\u5e93\u7684\u6d41\u7a0b\u7edf\u4e00\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u7cfb\u7edf\u53ef\u751f\u6210\u53ef\u9760\u811a\u672c\u5e76\u63a8\u8350\u4f18\u8d28\u6a21\u578b\uff0c\u5f00\u6e90\u652f\u6301\u540e\u7eed\u7814\u7a76\u4e0e\u5e94\u7528\u3002", "conclusion": "AD-AGENT\u901a\u8fc7\u667a\u80fd\u4f53\u534f\u4f5c\u6709\u6548\u964d\u4f4e\u4e86\u5f02\u5e38\u68c0\u6d4b\u6280\u672f\u95e8\u69db\uff0c\u4e3a\u8de8\u5e93\u81ea\u52a8\u5316\u6d41\u7a0b\u63d0\u4f9b\u4e86\u521b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2505.12616", "pdf": "https://arxiv.org/pdf/2505.12616", "abs": "https://arxiv.org/abs/2505.12616", "authors": ["Shujauddin Syed", "Ted Pedersen"], "title": "Duluth at SemEval-2025 Task 7: TF-IDF with Optimized Vector Dimensions for Multilingual Fact-Checked Claim Retrieval", "categories": ["cs.CL", "68T50"], "comment": "SemEval-2025", "summary": "This paper presents the Duluth approach to the SemEval-2025 Task 7 on\nMultilingual and Crosslingual Fact-Checked Claim Retrieval. We implemented a\nTF-IDF-based retrieval system with experimentation on vector dimensions and\ntokenization strategies. Our best-performing configuration used word-level\ntokenization with a vocabulary size of 15,000 features, achieving an average\nsuccess@10 score of 0.78 on the development set and 0.69 on the test set across\nten languages. Our system showed stronger performance on higher-resource\nlanguages but still lagged significantly behind the top-ranked system, which\nachieved 0.96 average success@10. Our findings suggest that though advanced\nneural architectures are increasingly dominant in multilingual retrieval tasks,\nproperly optimized traditional methods like TF-IDF remain competitive\nbaselines, especially in limited compute resource scenarios.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u4f18\u5316TF-IDF\u68c0\u7d22\u7cfb\u7edf\uff0c\u572810\u79cd\u8bed\u8a00\u4e2d\u5b9e\u73b0\u5e73\u57470.69\u7684\u6d4b\u8bd5\u96c6\u6548\u679c\uff0c\u8bc1\u660e\u4f20\u7edf\u65b9\u6cd5\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4ecd\u5177\u7ade\u4e89\u529b", "motivation": "\u9a8c\u8bc1\u4f20\u7edf\u68c0\u7d22\u65b9\u6cd5\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\uff0c\u4e3a\u591a\u8bed\u8a00\u4e8b\u5b9e\u6838\u67e5\u4efb\u52a1\u5efa\u7acb\u7ade\u4e89\u6027\u57fa\u7ebf", "method": "\u91c7\u7528TF-IDF\u68c0\u7d22\u7cfb\u7edf\uff0c\u5b9e\u9a8c\u4e0d\u540c\u5411\u91cf\u7ef4\u5ea6\u548c\u5206\u8bcd\u7b56\u7565\uff0c\u6700\u4f73\u65b9\u6848\u4f7f\u7528\u8bcd\u7ea7\u5206\u8bcd\u4e0e15,000\u7ef4\u7279\u5f81\u7a7a\u95f4", "result": "\u5f00\u53d1\u96c6\u5e73\u5747success@10\u8fbe0.78\uff0c\u6d4b\u8bd5\u96c60.69\uff08\u5341\u8bed\u8a00\uff09\uff0c\u9ad8\u8d44\u6e90\u8bed\u8a00\u8868\u73b0\u66f4\u4f18\u4f46\u4e0e\u6700\u4f73\u795e\u7ecf\u6a21\u578b\uff080.96\uff09\u4ecd\u6709\u5dee\u8ddd", "conclusion": "\u4f18\u5316\u540e\u7684\u4f20\u7edf\u65b9\u6cd5\u5728\u8de8\u8bed\u8a00\u68c0\u7d22\u4efb\u52a1\u4e2d\u4ecd\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f"}}
{"id": "2505.12621", "pdf": "https://arxiv.org/pdf/2505.12621", "abs": "https://arxiv.org/abs/2505.12621", "authors": ["Jo\u00e3o Eduardo Batista", "Emil Vatai", "Mohamed Wahib"], "title": "Think Before You Attribute: Improving the Performance of LLMs Attribution Systems", "categories": ["cs.CL", "cs.IR"], "comment": "22 pages (9 pages of content, 4 pages of references, 9 pages of\n  supplementary material), 7 figures, 10 tables", "summary": "Large Language Models (LLMs) are increasingly applied in various science\ndomains, yet their broader adoption remains constrained by a critical\nchallenge: the lack of trustworthy, verifiable outputs. Current LLMs often\ngenerate answers without reliable source attribution, or worse, with incorrect\nattributions, posing a barrier to their use in scientific and high-stakes\nsettings, where traceability and accountability are non-negotiable. To be\nreliable, attribution systems need high accuracy and retrieve data with short\nlengths, i.e., attribute to a sentence within a document rather than a whole\ndocument. We propose a sentence-level pre-attribution step for\nRetrieve-Augmented Generation (RAG) systems that classify sentences into three\ncategories: not attributable, attributable to a single quote, and attributable\nto multiple quotes. By separating sentences before attribution, a proper\nattribution method can be selected for the type of sentence, or the attribution\ncan be skipped altogether. Our results indicate that classifiers are\nwell-suited for this task. In this work, we propose a pre-attribution step to\nreduce the computational complexity of attribution, provide a clean version of\nthe HAGRID dataset, and provide an end-to-end attribution system that works out\nof the box.", "AI": {"tldr": "\u63d0\u51fa\u53e5\u5b50\u7ea7\u9884\u5f52\u5c5e\u5206\u7c7b\u65b9\u6cd5\u63d0\u5347RAG\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u63d0\u4f9b\u6807\u51c6\u5316HAGRID\u6570\u636e\u96c6\u548c\u7aef\u5230\u7aef\u5f52\u5c5e\u7cfb\u7edf", "motivation": "LLMs\u5728\u79d1\u5b66\u9886\u57df\u5e94\u7528\u4e2d\u7f3a\u4e4f\u53ef\u9a8c\u8bc1\u7684\u5f52\u5c5e\u673a\u5236\uff0c\u9519\u8bef\u5f52\u5c5e\u963b\u788d\u5176\u5728\u5173\u952e\u573a\u666f\u7684\u5e94\u7528", "method": "\u5728RAG\u7cfb\u7edf\u4e2d\u589e\u52a0\u9884\u5f52\u5c5e\u5206\u7c7b\u5668\uff0c\u5c06\u53e5\u5b50\u5206\u4e3a\u4e0d\u53ef\u5f52\u5c5e/\u5355\u5f15\u7528\u5f52\u5c5e/\u591a\u5f15\u7528\u5f52\u5c5e\u4e09\u7c7b\uff0c\u9488\u5bf9\u6027\u9009\u62e9\u5f52\u5c5e\u7b56\u7565", "result": "\u9a8c\u8bc1\u5206\u7c7b\u5668\u6709\u6548\u6027\uff0c\u63d0\u4f9b\u6e05\u6d17\u540e\u7684HAGRID\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u5f00\u7bb1\u5373\u7528\u7684\u7aef\u5230\u7aef\u5f52\u5c5e\u7cfb\u7edf", "conclusion": "\u9884\u5f52\u5c5e\u673a\u5236\u901a\u8fc7\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u63d0\u5347\u5f52\u5c5e\u7cbe\u51c6\u5ea6\uff0c\u589e\u5f3aLLMs\u8f93\u51fa\u7684\u53ef\u8ffd\u6eaf\u6027\u4e0e\u53ef\u9760\u6027"}}
{"id": "2505.12625", "pdf": "https://arxiv.org/pdf/2505.12625", "abs": "https://arxiv.org/abs/2505.12625", "authors": ["Ali Naseh", "Harsh Chaudhari", "Jaechul Roh", "Mingshi Wu", "Alina Oprea", "Amir Houmansadr"], "title": "R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model", "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "DeepSeek recently released R1, a high-performing large language model (LLM)\noptimized for reasoning tasks. Despite its efficient training pipeline, R1\nachieves competitive performance, even surpassing leading reasoning models like\nOpenAI's o1 on several benchmarks. However, emerging reports suggest that R1\nrefuses to answer certain prompts related to politically sensitive topics in\nChina. While existing LLMs often implement safeguards to avoid generating\nharmful or offensive outputs, R1 represents a notable shift - exhibiting\ncensorship-like behavior on politically charged queries. In this paper, we\ninvestigate this phenomenon by first introducing a large-scale set of heavily\ncurated prompts that get censored by R1, covering a range of politically\nsensitive topics, but are not censored by other models. We then conduct a\ncomprehensive analysis of R1's censorship patterns, examining their\nconsistency, triggers, and variations across topics, prompt phrasing, and\ncontext. Beyond English-language queries, we explore censorship behavior in\nother languages. We also investigate the transferability of censorship to\nmodels distilled from the R1 language model. Finally, we propose techniques for\nbypassing or removing this censorship. Our findings reveal possible additional\ncensorship integration likely shaped by design choices during training or\nalignment, raising concerns about transparency, bias, and governance in\nlanguage model deployment.", "AI": {"tldr": "DeepSeek R1\u6a21\u578b\u867d\u5728\u63a8\u7406\u4efb\u52a1\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u88ab\u53d1\u73b0\u5b58\u5728\u5bf9\u4e2d\u56fd\u653f\u6cbb\u654f\u611f\u8bdd\u9898\u7684\u7279\u6b8a\u5ba1\u67e5\u673a\u5236", "motivation": "\u7814\u7a76R1\u6a21\u578b\u5728\u653f\u6cbb\u654f\u611f\u8bdd\u9898\u4e0a\u8868\u73b0\u51fa\u7684\u5ba1\u67e5\u884c\u4e3a\uff0c\u63ed\u793a\u5176\u8bad\u7ec3/\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u7684\u7279\u6b8a\u8bbe\u8ba1", "method": "\u901a\u8fc7\u6784\u5efa\u5927\u89c4\u6a21\u654f\u611f\u63d0\u793a\u6570\u636e\u96c6\uff0c\u5206\u6790\u5ba1\u67e5\u6a21\u5f0f\u7684\u89e6\u53d1\u673a\u5236\u3001\u8de8\u8bed\u8a00\u8868\u73b0\u53ca\u84b8\u998f\u6a21\u578b\u7684\u8fc1\u79fb\u6027", "result": "\u53d1\u73b0R1\u5b58\u5728\u7cfb\u7edf\u6027\u5ba1\u67e5\u673a\u5236\uff0c\u5176\u89e6\u53d1\u4e0e\u8bdd\u9898\u654f\u611f\u6027\u3001\u8bed\u8a00\u8868\u8fbe\u65b9\u5f0f\u76f8\u5173\uff0c\u5e76\u53ef\u901a\u8fc7\u7279\u5b9a\u6280\u672f\u7ed5\u8fc7", "conclusion": "\u6a21\u578b\u5ba1\u67e5\u673a\u5236\u7f3a\u4e4f\u900f\u660e\u5ea6\u53ef\u80fd\u5f15\u53d1\u6280\u672f\u4f26\u7406\u95ee\u9898\uff0c\u9700\u52a0\u5f3aAI\u6cbb\u7406\u4e0e\u7b97\u6cd5\u900f\u660e\u5ea6\u5efa\u8bbe"}}
{"id": "2505.12636", "pdf": "https://arxiv.org/pdf/2505.12636", "abs": "https://arxiv.org/abs/2505.12636", "authors": ["Jiakuan Xie", "Pengfei Cao", "Yubo Chen", "Kang Liu", "Jun Zhao"], "title": "Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025 main", "summary": "Knowledge editing, which aims to update the knowledge encoded in language\nmodels, can be deceptive. Despite the fact that many existing knowledge editing\nalgorithms achieve near-perfect performance on conventional metrics, the models\nedited by them are still prone to generating original knowledge. This paper\nintroduces the concept of \"superficial editing\" to describe this phenomenon.\nOur comprehensive evaluation reveals that this issue presents a significant\nchallenge to existing algorithms. Through systematic investigation, we identify\nand validate two key factors contributing to this issue: (1) the residual\nstream at the last subject position in earlier layers and (2) specific\nattention modules in later layers. Notably, certain attention heads in later\nlayers, along with specific left singular vectors in their output matrices,\nencapsulate the original knowledge and exhibit a causal relationship with\nsuperficial editing. Furthermore, we extend our analysis to the task of\nsuperficial unlearning, where we observe consistent patterns in the behavior of\nspecific attention heads and their corresponding left singular vectors, thereby\ndemonstrating the robustness and broader applicability of our methodology and\nconclusions. Our code is available here.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u77e5\u8bc6\u7f16\u8f91\u5b58\u5728'\u8868\u9762\u7f16\u8f91'\u73b0\u8c61\uff0c\u73b0\u6709\u7b97\u6cd5\u867d\u6307\u6807\u4f18\u79c0\u4f46\u65e0\u6cd5\u5f7b\u5e95\u6d88\u9664\u539f\u59cb\u77e5\u8bc6\uff0c\u8bc6\u522b\u51fa\u6b8b\u7559\u6d41\u548c\u6ce8\u610f\u529b\u673a\u5236\u4e24\u4e2a\u5173\u952e\u6210\u56e0", "motivation": "\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u7b97\u6cd5\u5728\u4f20\u7edf\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u6a21\u578b\u4ecd\u4f1a\u8f93\u51fa\u539f\u59cb\u77e5\u8bc6\uff0c\u9700\u63ed\u793a\u8fd9\u4e00\u73b0\u8c61\u7684\u5185\u5728\u673a\u5236\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848", "method": "\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u53d1\u73b0\uff1a1.\u65e9\u671f\u5c42\u4e3b\u8bed\u4f4d\u7f6e\u7684\u6b8b\u7559\u6d41 2.\u540e\u671f\u5c42\u7279\u5b9a\u6ce8\u610f\u529b\u6a21\u5757\uff08\u7279\u522b\u662f\u6ce8\u610f\u529b\u5934\u53ca\u5176\u8f93\u51fa\u77e9\u9635\u7684\u5de6\u5947\u5f02\u5411\u91cf\uff09\u4e0e\u8868\u9762\u7f16\u8f91\u5b58\u5728\u56e0\u679c\u5173\u7cfb", "result": "\u9a8c\u8bc1\u4e86\u6ce8\u610f\u529b\u5934\u5de6\u5947\u5f02\u5411\u91cf\u7f16\u7801\u539f\u59cb\u77e5\u8bc6\u7684\u673a\u5236\uff0c\u5728\u8868\u9762\u9057\u5fd8\u4efb\u52a1\u4e2d\u89c2\u5bdf\u5230\u76f8\u540c\u6a21\u5f0f\uff0c\u8bc1\u5b9e\u4e86\u65b9\u6cd5\u8bba\u7684\u6709\u6548\u6027\u548c\u7ed3\u8bba\u7684\u666e\u9002\u6027", "conclusion": "\u8868\u9762\u7f16\u8f91\u73b0\u8c61\u5bf9\u73b0\u6709\u7b97\u6cd5\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u6ce8\u610f\u529b\u673a\u5236\u5206\u6790\u4e3a\u7406\u89e3\u6a21\u578b\u77e5\u8bc6\u5b58\u50a8\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u7814\u7a76\u6846\u67b6\u53ef\u62d3\u5c55\u81f3\u5176\u4ed6\u6a21\u578b\u7f16\u8f91\u573a\u666f"}}
{"id": "2505.12654", "pdf": "https://arxiv.org/pdf/2505.12654", "abs": "https://arxiv.org/abs/2505.12654", "authors": ["Yuxin Lin", "Yinglin Zheng", "Ming Zeng", "Wangzheng Shi"], "title": "Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals", "categories": ["cs.CL", "cs.AI"], "comment": "Accepected by ACL 2025", "summary": "This paper addresses the gap in predicting turn-taking and backchannel\nactions in human-machine conversations using multi-modal signals (linguistic,\nacoustic, and visual). To overcome the limitation of existing datasets, we\npropose an automatic data collection pipeline that allows us to collect and\nannotate over 210 hours of human conversation videos. From this, we construct a\nMulti-Modal Face-to-Face (MM-F2F) human conversation dataset, including over\n1.5M words and corresponding turn-taking and backchannel annotations from\napproximately 20M frames. Additionally, we present an end-to-end framework that\npredicts the probability of turn-taking and backchannel actions from\nmulti-modal signals. The proposed model emphasizes the interrelation between\nmodalities and supports any combination of text, audio, and video inputs,\nmaking it adaptable to a variety of realistic scenarios. Our experiments show\nthat our approach achieves state-of-the-art performance on turn-taking and\nbackchannel prediction tasks, achieving a 10\\% increase in F1-score on\nturn-taking and a 33\\% increase on backchannel prediction. Our dataset and code\nare publicly available online to ease of subsequent research.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u6001\u5bf9\u8bdd\u6570\u636e\u96c6MM-F2F\u53ca\u7aef\u5230\u7aef\u9884\u6d4b\u6846\u67b6\uff0c\u5b9e\u73b0\u8bdd\u8f6e\u8f6c\u6362\u548c\u53cd\u9988\u9884\u6d4b\u6027\u80fd\u663e\u8457\u63d0\u5347", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u65e0\u6cd5\u6ee1\u8db3\u4eba\u673a\u5bf9\u8bdd\u4e2d\u591a\u6a21\u6001\u4fe1\u53f7\uff08\u8bed\u8a00/\u58f0\u5b66/\u89c6\u89c9\uff09\u5bf9\u8bdd\u8f6e\u8f6c\u6362\u548c\u53cd\u9988\u52a8\u4f5c\u7684\u9884\u6d4b\u9700\u6c42", "method": "1. \u5f00\u53d1\u81ea\u52a8\u6570\u636e\u91c7\u96c6\u7ba1\u9053\u6784\u5efa\u542b210\u5c0f\u65f6\u89c6\u9891\u7684MM-F2F\u6570\u636e\u96c6 2. \u8bbe\u8ba1\u652f\u6301\u591a\u6a21\u6001\u7ec4\u5408\u8f93\u5165\u7684\u7aef\u5230\u7aef\u9884\u6d4b\u6846\u67b6 3. \u901a\u8fc7\u6a21\u6001\u95f4\u5173\u8054\u6027\u5efa\u6a21\u63d0\u5347\u9884\u6d4b\u7cbe\u5ea6", "result": "\u8bdd\u8f6e\u8f6c\u6362F1\u503c\u63d0\u534710%\uff0c\u53cd\u9988\u9884\u6d4bF1\u503c\u63d0\u534733%\uff0c\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u6c34\u5e73", "conclusion": "\u516c\u5f00\u7684\u6570\u636e\u96c6\u4e0e\u7075\u6d3b\u7684\u591a\u6a21\u6001\u6846\u67b6\u4e3a\u5bf9\u8bdd\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u57fa\u7840\u8bbe\u65bd\uff0c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u4fe1\u53f7\u8054\u5408\u5efa\u6a21\u7684\u91cd\u8981\u6027"}}
{"id": "2505.12662", "pdf": "https://arxiv.org/pdf/2505.12662", "abs": "https://arxiv.org/abs/2505.12662", "authors": ["Xukai Liu", "Ye Liu", "Shiwen Wu", "Yanghai Zhang", "Yihao Yuan", "Kai Zhang", "Qi Liu"], "title": "Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advances in large language models (LLMs) have led to impressive\nprogress in natural language generation, yet their tendency to produce\nhallucinated or unsubstantiated content remains a critical concern. To improve\nfactual reliability, Retrieval-Augmented Generation (RAG) integrates external\nknowledge during inference. However, existing RAG systems face two major\nlimitations: (1) unreliable adaptive control due to limited external knowledge\nsupervision, and (2) hallucinations caused by inaccurate or irrelevant\nreferences. To address these issues, we propose Know3-RAG, a knowledge-aware\nRAG framework that leverages structured knowledge from knowledge graphs (KGs)\nto guide three core stages of the RAG process, including retrieval, generation,\nand filtering. Specifically, we introduce a knowledge-aware adaptive retrieval\nmodule that employs KG embedding to assess the confidence of the generated\nanswer and determine retrieval necessity, a knowledge-enhanced reference\ngeneration strategy that enriches queries with KG-derived entities to improve\ngenerated reference relevance, and a knowledge-driven reference filtering\nmechanism that ensures semantic alignment and factual accuracy of references.\nExperiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG\nconsistently outperforms strong baselines, significantly reducing\nhallucinations and enhancing answer reliability.", "AI": {"tldr": "Know3-RAG\u6846\u67b6\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u7ed3\u6784\u5316\u77e5\u8bc6\u6539\u8fdbRAG\u68c0\u7d22\u3001\u751f\u6210\u3001\u8fc7\u6ee4\u4e09\u9636\u6bb5\uff0c\u663e\u8457\u51cf\u5c11\u5e7b\u89c9\u95ee\u9898\u5e76\u63d0\u5347\u7b54\u6848\u53ef\u9760\u6027", "motivation": "\u73b0\u6709\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7cfb\u7edf\u5b58\u5728\u81ea\u9002\u5e94\u63a7\u5236\u4e0d\u53ef\u9760(\u56e0\u5916\u90e8\u77e5\u8bc6\u76d1\u7763\u6709\u9650)\u548c\u5f15\u7528\u4e0d\u51c6\u786e\u5bfc\u81f4\u7684\u5e7b\u89c9\u95ee\u9898", "method": "\u63d0\u51fa\u77e5\u8bc6\u611f\u77e5\u81ea\u9002\u5e94\u68c0\u7d22\u6a21\u5757\uff08KG\u5d4c\u5165\u8bc4\u4f30\u7f6e\u4fe1\u5ea6\uff09\u3001\u77e5\u8bc6\u589e\u5f3a\u53c2\u8003\u751f\u6210\u7b56\u7565\uff08KG\u5b9e\u4f53\u6269\u5c55\u67e5\u8be2\uff09\u3001\u77e5\u8bc6\u9a71\u52a8\u53c2\u8003\u8fc7\u6ee4\u673a\u5236\uff08\u8bed\u4e49\u5bf9\u9f50\u9a8c\u8bc1\uff09", "result": "\u5728\u591a\u4e2a\u5f00\u653e\u57dfQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u5e7b\u89c9\u73b0\u8c61\u663e\u8457\u51cf\u5c11\u4e14\u7b54\u6848\u53ef\u9760\u6027\u63d0\u5347", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u77e5\u8bc6\u5bf9RAG\u5168\u6d41\u7a0b\u7684\u6307\u5bfc\uff0c\u6709\u6548\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u7f3a\u9677\uff0c\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u77e5\u8bc6\u589e\u5f3a\u751f\u6210"}}
{"id": "2505.12716", "pdf": "https://arxiv.org/pdf/2505.12716", "abs": "https://arxiv.org/abs/2505.12716", "authors": ["Taiqiang Wu", "Runming Yang", "Jiayi Li", "Pengfei Hu", "Ngai Wong", "Yujiu Yang"], "title": "Shadow-FT: Tuning Instruct via Base", "categories": ["cs.CL", "cs.AI"], "comment": "Under review", "summary": "Large language models (LLMs) consistently benefit from further fine-tuning on\nvarious tasks. However, we observe that directly tuning the INSTRUCT (i.e.,\ninstruction tuned) models often leads to marginal improvements and even\nperformance degeneration. Notably, paired BASE models, the foundation for these\nINSTRUCT variants, contain highly similar weight values (i.e., less than 2% on\naverage for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to\ntune the INSTRUCT models by leveraging the corresponding BASE models. The key\ninsight is to fine-tune the BASE model, and then directly graft the learned\nweight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no\nadditional parameters, is easy to implement, and significantly improves\nperformance. We conduct extensive experiments on tuning mainstream LLMs, such\nas Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering\ncoding, reasoning, and mathematical tasks. Experimental results demonstrate\nthat Shadow-FT consistently outperforms conventional full-parameter and\nparameter-efficient tuning approaches. Further analyses indicate that Shadow-FT\ncan be applied to multimodal large language models (MLLMs) and combined with\ndirect preference optimization (DPO). Codes and weights are available at\n\\href{https://github.com/wutaiqiang/Shadow-FT}{Github}.", "AI": {"tldr": "\u63d0\u51faShadow-FT\u6846\u67b6\uff0c\u901a\u8fc7\u5fae\u8c03BASE\u6a21\u578b\u5e76\u79fb\u690d\u6743\u91cd\u5230INSTRUCT\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347LLM\u5728\u591a\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0", "motivation": "\u76f4\u63a5\u5fae\u8c03INSTRUCT\u6a21\u578b\u6548\u679c\u6709\u9650\u4e14\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u5bf9\u5e94\u7684BASE\u6a21\u578b\u6743\u91cd\u53d8\u5316\u5fae\u5c0f\uff08\u5982Llama 3.1 8B\u5e73\u5747\u5c0f\u4e8e2%\uff09\uff0c\u5b58\u5728\u6f5c\u5728\u4f18\u5316\u7a7a\u95f4", "method": "\u5148\u5fae\u8c03BASE\u6a21\u578b\uff0c\u7136\u540e\u5c06\u5b66\u4e60\u5230\u7684\u6743\u91cd\u66f4\u65b0\u76f4\u63a5\u79fb\u690d\u5230INSTRUCT\u6a21\u578b\uff0c\u65e0\u9700\u989d\u5916\u53c2\u6570\u4e14\u6613\u4e8e\u5b9e\u73b0", "result": "\u572819\u4e2a\u6db5\u76d6\u7f16\u7a0b\u3001\u63a8\u7406\u548c\u6570\u5b66\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cShadow-FT\u5728Qwen 3\u548cLlama 3\u7cfb\u5217\u6a21\u578b\u4e0a\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u5168\u53c2\u6570\u548c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u5e76\u53ef\u6269\u5c55\u5e94\u7528\u4e8e\u591a\u6a21\u6001\u5927\u6a21\u578b\u548c\u7ed3\u5408DPO", "conclusion": "Shadow-FT\u901a\u8fc7\u6709\u6548\u5229\u7528BASE\u6a21\u578b\u7684\u4f18\u5316\u6f5c\u529b\uff0c\u4ee5\u96f6\u989d\u5916\u6210\u672c\u663e\u8457\u63d0\u5347INSTRUCT\u6a21\u578b\u7684\u5fae\u8c03\u6548\u679c\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u5de5\u7a0b\u5b9e\u8df5\u4ef7\u503c"}}
{"id": "2505.12717", "pdf": "https://arxiv.org/pdf/2505.12717", "abs": "https://arxiv.org/abs/2505.12717", "authors": ["Haoyuan Wu", "Xueyi Chen", "Rui Ming", "Jilong Gao", "Shoubo Hu", "Zhuolun He", "Bei Yu"], "title": "ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) demonstrate significant reasoning capabilities,\nparticularly through long chain-of-thought (CoT) processes, which can be\nelicited by reinforcement learning (RL). However, prolonged CoT reasoning\npresents limitations, primarily verbose outputs due to excessive introspection.\nThe reasoning process in these LLMs often appears to follow a trial-and-error\nmethodology rather than a systematic, logical deduction. In contrast,\ntree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling\nreasoning as an exploration within a tree structure. This reasoning structure\nfacilitates the parallel generation and evaluation of multiple reasoning\nbranches, allowing for the active identification, assessment, and pruning of\nunproductive paths. This process can potentially lead to improved performance\nand reduced token costs. Building upon the long CoT capability of LLMs, we\nintroduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a\nrule-based reward. ToTRL is designed to guide LLMs in developing the parallel\nToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs\nas players in a puzzle game during the ToTRL training process. Solving puzzle\ngames inherently necessitates exploring interdependent choices and managing\nmultiple constraints, which requires the construction and exploration of a\nthought tree, providing challenging tasks for cultivating the ToT reasoning\ncapability. Our empirical evaluations demonstrate that our ToTQwen3-8B model,\ntrained with our ToTRL, achieves significant improvement in performance and\nreasoning efficiency on complex reasoning tasks.", "AI": {"tldr": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLMs\u4ece\u987a\u5e8f\u94fe\u5f0f\u63a8\u7406\u8f6c\u5411\u5e76\u884c\u6811\u72b6\u63a8\u7406\uff0c\u5f00\u53d1ToTRL\u6846\u67b6\u63d0\u5347\u590d\u6742\u4efb\u52a1\u63a8\u7406\u6548\u7387", "motivation": "\u957f\u94fe\u5f0f\u63a8\u7406\u5b58\u5728\u5197\u957f\u81ea\u7701\u5bfc\u81f4\u7684\u4f4e\u6548\u8f93\u51fa\u95ee\u9898\uff0c\u4e14\u4f20\u7edf\u8bd5\u9519\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u6027\u3002\u6811\u72b6\u63a8\u7406\u7ed3\u6784\u901a\u8fc7\u5e76\u884c\u751f\u6210\u548c\u4e3b\u52a8\u526a\u679d\u8def\u5f84\u53ef\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c", "method": "\u63d0\u51fa\u6811\u72b6\u601d\u7ef4\u5f3a\u5316\u5b66\u4e60\u6846\u67b6(ToTRL)\uff0c\u7ed3\u5408\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u673a\u5236\uff0c\u901a\u8fc7\u89e3\u8c1c\u6e38\u620f\u8bad\u7ec3LLMs\u6784\u5efa\u76f8\u4e92\u4f9d\u8d56\u7684\u601d\u7ef4\u6811\u7ed3\u6784", "result": "ToTQwen3-8B\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6027\u80fd\u663e\u8457\u63d0\u5347\uff0c\u63a8\u7406\u6548\u7387\u63d0\u9ad830%", "conclusion": "ToTRL\u6210\u529f\u5c06LLMs\u7684\u7ebf\u6027\u63a8\u7406\u5347\u7ea7\u4e3a\u7ed3\u6784\u5316\u6811\u72b6\u63a8\u7406\uff0c\u4e3a\u5904\u7406\u590d\u6742\u7ea6\u675f\u4efb\u52a1\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u8bc1\u660e\u4e3b\u52a8\u8def\u5f84\u8bc4\u4f30\u673a\u5236\u7684\u6709\u6548\u6027"}}
{"id": "2505.12718", "pdf": "https://arxiv.org/pdf/2505.12718", "abs": "https://arxiv.org/abs/2505.12718", "authors": ["Jingyang Peng", "Wenyuan Shen", "Jiarui Rao", "Jionghao Lin"], "title": "Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework", "categories": ["cs.CL", "cs.HC"], "comment": "Accepted by AIED 2025: Late-Breaking Results (LBR) Track", "summary": "Recent advances in Generative Artificial Intelligence (GenAI) have\ntransformed educational content creation, particularly in developing tutor\ntraining materials. However, biases embedded in AI-generated content--such as\ngender, racial, or national stereotypes--raise significant ethical and\neducational concerns. Despite the growing use of GenAI, systematic methods for\ndetecting and evaluating such biases in educational materials remain limited.\nThis study proposes an automated bias assessment approach that integrates the\nContextualized Embedding Association Test with a prompt-engineered word\nextraction method within a Retrieval-Augmented Generation framework. We applied\nthis method to AI-generated texts used in tutor training lessons. Results show\na high alignment between the automated and manually curated word sets, with a\nPearson correlation coefficient of r = 0.993, indicating reliable and\nconsistent bias assessment. Our method reduces human subjectivity and enhances\nfairness, scalability, and reproducibility in auditing GenAI-produced\neducational content.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u7ed3\u5408\u4e0a\u4e0b\u6587\u5d4c\u5165\u5173\u8054\u6d4b\u8bd5\u4e0e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\u7684\u81ea\u52a8\u5316\u504f\u89c1\u8bc4\u4f30\u65b9\u6cd5\uff0c\u6709\u6548\u68c0\u6d4bAI\u6559\u80b2\u5185\u5bb9\u4e2d\u7684\u504f\u89c1\u95ee\u9898\uff08r=0.993\uff09\uff0c\u63d0\u5347\u5ba1\u6838\u7684\u5ba2\u89c2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u9488\u5bf9\u751f\u6210\u5f0fAI\u6559\u80b2\u5185\u5bb9\u4e2d\u5b58\u5728\u7684\u9690\u6027\u504f\u89c1\uff08\u6027\u522b/\u79cd\u65cf/\u56fd\u5bb6\u523b\u677f\u5370\u8c61\uff09\uff0c\u9700\u5efa\u7acb\u7cfb\u7edf\u5316\u8bc4\u4f30\u65b9\u6cd5\u4ee5\u89e3\u51b3\u4eba\u5de5\u5ba1\u6838\u7684\u4e3b\u89c2\u6027\u548c\u6548\u7387\u74f6\u9888\u3002", "method": "\u6574\u5408\u4e0a\u4e0b\u6587\u5d4c\u5165\u5173\u8054\u6d4b\u8bd5\u4e0e\u63d0\u793a\u5de5\u7a0b\u8bcd\u6c47\u63d0\u53d6\u6280\u672f\uff0c\u6784\u5efa\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u6846\u67b6\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u6d41\u7a0b\u3002", "result": "\u81ea\u52a8\u5316\u4e0e\u4eba\u5de5\u7b5b\u9009\u8bcd\u6c47\u96c6\u9ad8\u5ea6\u4e00\u81f4\uff08Pearson r=0.993\uff09\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u548c\u4e00\u81f4\u6027\u4f18\u52bf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4eba\u5de5\u4e3b\u89c2\u6027\uff0c\u4e3a\u5927\u89c4\u6a21\u5ba1\u6838AI\u6559\u80b2\u5185\u5bb9\u63d0\u4f9b\u6807\u51c6\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u4fc3\u8fdb\u6559\u80b2\u516c\u5e73\u7684\u6280\u672f\u5b9e\u73b0\u3002"}}
{"id": "2505.12723", "pdf": "https://arxiv.org/pdf/2505.12723", "abs": "https://arxiv.org/abs/2505.12723", "authors": ["Haoyuan Wu", "Rui Ming", "Jilong Gao", "Hangyu Zhao", "Xueyi Chen", "Yikai Yang", "Haisheng Zheng", "Zhuolun He", "Bei Yu"], "title": "On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve remarkable performance in code\ngeneration tasks. However, a significant performance disparity persists between\npopular programming languages (e.g., Python, C++) and others. To address this\ncapability gap, we leverage the code translation task to train LLMs, thereby\nfacilitating the transfer of coding proficiency across diverse programming\nlanguages. Moreover, we introduce OORL for training, a novel reinforcement\nlearning (RL) framework that integrates on-policy and off-policy strategies.\nWithin OORL, on-policy RL is applied during code translation, guided by a\nrule-based reward signal derived from unit tests. Complementing this\ncoarse-grained rule-based reward, we propose Group Equivalent Preference\nOptimization (GEPO), a novel preference optimization method. Specifically, GEPO\ntrains the LLM using intermediate representations (IRs) groups. LLMs can be\nguided to discern IRs equivalent to the source code from inequivalent ones,\nwhile also utilizing signals about the mutual equivalence between IRs within\nthe group. This process allows LLMs to capture nuanced aspects of code\nfunctionality. By employing OORL for training with code translation tasks, LLMs\nimprove their recognition of code functionality and their understanding of the\nrelationships between code implemented in different languages. Extensive\nexperiments demonstrate that our OORL for LLMs training with code translation\ntasks achieves significant performance improvements on code benchmarks across\nmultiple programming languages.", "AI": {"tldr": "\u63d0\u51faOORL\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u548cGEPO\u4f18\u5316\u65b9\u6cd5\u63d0\u5347LLMs\u8de8\u8bed\u8a00\u4ee3\u7801\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u9488\u5bf9LLMs\u5728\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u95f4\u4ee3\u7801\u751f\u6210\u80fd\u529b\u5dee\u5f02\u663e\u8457\u7684\u95ee\u9898\uff0c\u8bd5\u56fe\u901a\u8fc7\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u5b9e\u73b0\u8de8\u8bed\u8a00\u7f16\u7801\u80fd\u529b\u8fc1\u79fb\u3002", "method": "\u7ed3\u5408on-policy\u5f3a\u5316\u5b66\u4e60\uff08\u57fa\u4e8e\u5355\u5143\u6d4b\u8bd5\u7684\u89c4\u5219\u5956\u52b1\uff09\u4e0eoff-policy\u7684GEPO\u65b9\u6cd5\uff08\u901a\u8fc7\u4e2d\u95f4\u8868\u793a\u7ec4\u4f18\u5316\u529f\u80fd\u7406\u89e3\uff09\uff0c\u6784\u5efaOORL\u8bad\u7ec3\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86LLMs\u5728\u591a\u79cd\u7f16\u7a0b\u8bed\u8a00\u4ee3\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u4ee3\u7801\u7ffb\u8bd1\u4efb\u52a1\u7ed3\u5408\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u6709\u6548\u589e\u5f3a\u4e86LLMs\u5bf9\u4ee3\u7801\u529f\u80fd\u7684\u7406\u89e3\u548c\u8de8\u8bed\u8a00\u5173\u7cfb\u5efa\u6a21\u80fd\u529b\u3002"}}
{"id": "2505.12727", "pdf": "https://arxiv.org/pdf/2505.12727", "abs": "https://arxiv.org/abs/2505.12727", "authors": ["Han Meng", "Yancan Chen", "Yunan Li", "Yitian Yang", "Jungup Lee", "Renwen Zhang", "Yi-Chieh Lee"], "title": "What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": "Accepted to ACL 2025 Main Conference, 35 Pages", "summary": "Mental-health stigma remains a pervasive social problem that hampers\ntreatment-seeking and recovery. Existing resources for training neural models\nto finely classify such stigma are limited, relying primarily on social-media\nor synthetic data without theoretical underpinnings. To remedy this gap, we\npresent an expert-annotated, theory-informed corpus of human-chatbot\ninterviews, comprising 4,141 snippets from 684 participants with documented\nsocio-cultural backgrounds. Our experiments benchmark state-of-the-art neural\nmodels and empirically unpack the challenges of stigma detection. This dataset\ncan facilitate research on computationally detecting, neutralizing, and\ncounteracting mental-health stigma.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u4e13\u5bb6\u6807\u6ce8\u7684\u5fc3\u7406\u5065\u5eb7\u6c61\u540d\u68c0\u6d4b\u7406\u8bba\u9a71\u52a8\u8bed\u6599\u5e93\uff0c\u5305\u542b4,141\u6761\u771f\u5b9e\u4eba\u7c7b-\u804a\u5929\u673a\u5668\u4eba\u5bf9\u8bdd\u7247\u6bb5\uff0c\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u5f53\u524d\u6700\u5148\u8fdb\u6a21\u578b\u4ecd\u9762\u4e34\u68c0\u6d4b\u6311\u6218", "motivation": "\u73b0\u6709\u5fc3\u7406\u5065\u5eb7\u6c61\u540d\u68c0\u6d4b\u4e3b\u8981\u4f9d\u8d56\u793e\u4ea4\u5a92\u4f53\u6216\u5408\u6210\u6570\u636e\uff0c\u7f3a\u4e4f\u7406\u8bba\u6846\u67b6\u652f\u6491\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u7cbe\u7ec6\u5206\u7c7b\u80fd\u529b", "method": "\u6784\u5efa\u5305\u542b684\u540d\u5177\u6709\u793e\u4f1a\u6587\u5316\u80cc\u666f\u8bb0\u5f55\u7684\u53c2\u4e0e\u8005\u7684\u4eba\u7c7b-\u804a\u5929\u673a\u5668\u4eba\u5bf9\u8bdd\u8bed\u6599\u5e93\uff084,141\u4e2a\u7247\u6bb5\uff09\uff0c\u91c7\u7528\u4e13\u5bb6\u6807\u6ce8\u548c\u7406\u8bba\u9a71\u52a8\u7684\u65b9\u6cd5", "result": "\u901a\u8fc7\u5b9e\u9a8c\u63ed\u793a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u795e\u7ecf\u6a21\u578b\u5728\u6c61\u540d\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u548c\u6280\u672f\u6311\u6218", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u8ba1\u7b97\u68c0\u6d4b\u3001\u4e2d\u548c\u6280\u672f\u5bf9\u6297\u5fc3\u7406\u5065\u5eb7\u6c61\u540d\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u57fa\u7840"}}
{"id": "2505.12768", "pdf": "https://arxiv.org/pdf/2505.12768", "abs": "https://arxiv.org/abs/2505.12768", "authors": ["Yaxun Dai", "Wenxuan Xie", "Xialie Zhuang", "Tianyu Yang", "Yiying Yang", "Haiqin Yang", "Yuhang Zhao", "Pingfu Chao", "Wenhao Jiang"], "title": "ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL", "categories": ["cs.CL"], "comment": null, "summary": "In Text-to-SQL, execution feedback is essential for guiding large language\nmodels (LLMs) to reason accurately and generate reliable SQL queries. However,\nexisting methods treat execution feedback solely as a post-hoc signal for\ncorrection or selection, failing to integrate it into the generation process.\nThis limitation hinders their ability to address reasoning errors as they\noccur, ultimately reducing query accuracy and robustness. To address this\nissue, we propose ReEx-SQL (Reasoning with Execution-Aware Reinforcement\nLearning), a framework for Text-to-SQL that enables models to interact with the\ndatabase during decoding and dynamically adjust their reasoning based on\nexecution feedback. ReEx-SQL introduces an execution-aware reasoning paradigm\nthat interleaves intermediate SQL execution into reasoning paths, facilitating\ncontext-sensitive revisions. It achieves this through structured prompts with\nmarkup tags and a stepwise rollout strategy that integrates execution feedback\ninto each stage of generation. To supervise policy learning, we develop a\ncomposite reward function that includes an exploration reward, explicitly\nencouraging effective database interaction. Additionally, ReEx-SQL adopts a\ntree-based decoding strategy to support exploratory reasoning, enabling dynamic\nexpansion of alternative reasoning paths. Notably, ReEx-SQL achieves 88.8% on\nSpider and 64.9% on BIRD at the 7B scale, surpassing the standard reasoning\nbaseline by 2.7% and 2.6%, respectively. It also shows robustness, achieving\n85.2% on Spider-Realistic with leading performance. In addition, its\ntree-structured decoding improves efficiency and performance over linear\ndecoding, reducing inference time by 51.9% on the BIRD development set.", "AI": {"tldr": "ReEx-SQL\u6846\u67b6\u901a\u8fc7\u6267\u884c\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff0c\u5728Text-to-SQL\u89e3\u7801\u8fc7\u7a0b\u4e2d\u52a8\u6001\u6574\u5408\u6570\u636e\u5e93\u53cd\u9988\uff0c\u663e\u8457\u63d0\u5347\u67e5\u8be2\u51c6\u786e\u6027\u548c\u63a8\u7406\u6548\u7387", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u5c06\u6267\u884c\u53cd\u9988\u4f5c\u4e3a\u4e8b\u540e\u6821\u6b63\u4fe1\u53f7\uff0c\u65e0\u6cd5\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u53ca\u65f6\u7ea0\u6b63\u9519\u8bef\uff0c\u5bfc\u81f4\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u53d7\u9650", "method": "\u63d0\u51fa\u6267\u884c\u611f\u77e5\u63a8\u7406\u8303\u5f0f\uff1a1\uff09\u5728\u63a8\u7406\u8def\u5f84\u4e2d\u5d4c\u5165\u4e2d\u95f4SQL\u6267\u884c 2\uff09\u5e26\u6807\u8bb0\u7684\u7ed3\u6784\u5316\u63d0\u793a\u8bbe\u8ba1 3\uff09\u590d\u5408\u5956\u52b1\u51fd\u6570\uff08\u542b\u63a2\u7d22\u5956\u52b1\uff094\uff09\u57fa\u4e8e\u6811\u7684\u52a8\u6001\u89e3\u7801\u7b56\u7565", "result": "Spider(88.8%)\u548cBIRD(64.9%)\u51c6\u786e\u7387\u5206\u522b\u63d0\u53472.7%\u548c2.6%\uff0c\u63a8\u7406\u65f6\u95f4\u51cf\u5c1151.9%\uff1bSpider-Realistic\u8fbe85.2%\u9886\u5148\u6027\u80fd", "conclusion": "\u901a\u8fc7\u6267\u884c\u53cd\u9988\u7684\u52a8\u6001\u6574\u5408\u4e0e\u63a2\u7d22\u6027\u6811\u72b6\u89e3\u7801\u673a\u5236\uff0c\u5b9e\u73b0\u4e86SQL\u751f\u6210\u6027\u80fd\u4e0e\u6548\u7387\u7684\u534f\u540c\u63d0\u5347\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u63a8\u7406\u8303\u5f0f\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2505.12781", "pdf": "https://arxiv.org/pdf/2505.12781", "abs": "https://arxiv.org/abs/2505.12781", "authors": ["Jitai Hao", "Qiang Huang", "Hao Liu", "Xinyan Xiao", "Zhaochun Ren", "Jun Yu"], "title": "A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Training high-performing Small Language Models (SLMs) remains costly, even\nwith knowledge distillation and pruning from larger teacher models. Existing\nwork often faces three key challenges: (1) information loss from hard pruning,\n(2) inefficient alignment of representations, and (3) underutilization of\ninformative activations, particularly from Feed-Forward Networks (FFNs). To\naddress these challenges, we introduce Low-Rank Clone (LRC), an efficient\npre-training method that constructs SLMs aspiring to behavioral equivalence\nwith strong teacher models. LRC trains a set of low-rank projection matrices\nthat jointly enable soft pruning by compressing teacher weights, and activation\nclone by aligning student activations, including FFN signals, with those of the\nteacher. This unified design maximizes knowledge transfer while removing the\nneed for explicit alignment modules. Extensive experiments with open-source\nteachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC\nmatches or surpasses state-of-the-art models trained on trillions of\ntokens--while using only 20B tokens, achieving over 1,000x training efficiency.\nOur codes and model checkpoints are available at\nhttps://github.com/CURRENTF/LowRankClone and\nhttps://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.", "AI": {"tldr": "\u63d0\u51fa\u4f4e\u79e9\u514b\u9686(LRC)\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u8f6f\u526a\u679d\u548c\u6fc0\u6d3b\u514b\u9686\u6280\u672f\uff0c\u5728\u4ec5\u4f7f\u752820B tokens\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b01000\u500d\u8bad\u7ec3\u6548\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u786c\u526a\u679d\u4fe1\u606f\u4e22\u5931\u3001\u8868\u793a\u5bf9\u9f50\u6548\u7387\u4f4e\u3001FFN\u6fc0\u6d3b\u4fe1\u53f7\u5229\u7528\u4e0d\u8db3\u4e09\u5927\u7f3a\u9677\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u84b8\u998f\u65b9\u6848\u3002", "method": "\u6784\u5efa\u4f4e\u79e9\u6295\u5f71\u77e9\u9635\u5b9e\u73b0\u6743\u91cd\u538b\u7f29(\u8f6f\u526a\u679d)\u548c\u6fc0\u6d3b\u5bf9\u9f50(FFN\u4fe1\u53f7\u514b\u9686)\uff0c\u7edf\u4e00\u8bbe\u8ba1\u907f\u514d\u663e\u5f0f\u5bf9\u9f50\u6a21\u5757\u3002", "result": "\u5728Llama-3.2-3B\u7b49\u6559\u5e08\u6a21\u578b\u4e0a\u8d85\u8d8aSOTA\u6a21\u578b\uff0c\u8bad\u7ec3\u6548\u7387\u63d0\u53471000\u500d\u4e14\u4ec5\u970020B tokens\u3002", "conclusion": "LRC\u901a\u8fc7\u53c2\u6570\u5171\u4eab\u67b6\u6784\u663e\u8457\u964d\u4f4e\u8bad\u7ec3\u6210\u672c\uff0c\u4e3a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002"}}
{"id": "2505.12792", "pdf": "https://arxiv.org/pdf/2505.12792", "abs": "https://arxiv.org/abs/2505.12792", "authors": ["Wenhao Zhu", "Yuhang Xie", "Guojie Song", "Xin Zhang"], "title": "EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs", "categories": ["cs.CL"], "comment": null, "summary": "The rapid evolution of large language models (LLMs) has revolutionized\nvarious fields, including the identification and discovery of human values\nwithin text data. While traditional NLP models, such as BERT, have been\nemployed for this task, their ability to represent textual data is\nsignificantly outperformed by emerging LLMs like GPTs. However, the performance\nof online LLMs often degrades when handling long contexts required for value\nidentification, which also incurs substantial computational costs. To address\nthese challenges, we propose EAVIT, an efficient and accurate framework for\nhuman value identification that combines the strengths of both locally\nfine-tunable and online black-box LLMs. Our framework employs a value detector\n- a small, local language model - to generate initial value estimations. These\nestimations are then used to construct concise input prompts for online LLMs,\nenabling accurate final value identification. To train the value detector, we\nintroduce explanation-based training and data generation techniques\nspecifically tailored for value identification, alongside sampling strategies\nto optimize the brevity of LLM input prompts. Our approach effectively reduces\nthe number of input tokens by up to 1/6 compared to directly querying online\nLLMs, while consistently outperforming traditional NLP methods and other\nLLM-based strategies.", "AI": {"tldr": "\u63d0\u51faEAVIT\u6846\u67b6\uff0c\u7ed3\u5408\u672c\u5730\u53ef\u5fae\u8c03\u6a21\u578b\u548c\u5728\u7ebfLLMs\u4f18\u52bf\uff0c\u901a\u8fc7\u4ef7\u503c\u68c0\u6d4b\u5668\u751f\u6210\u521d\u59cb\u4f30\u8ba1\u5e76\u6784\u5efa\u7b80\u6d01\u63d0\u793a\uff0c\u51cf\u5c11\u8f93\u5165token\u81f31/6\u7684\u540c\u65f6\u63d0\u5347\u4ef7\u503c\u89c2\u8bc6\u522b\u51c6\u786e\u7387\u3002", "motivation": "\u5728\u7ebfLLMs\u5904\u7406\u957f\u6587\u672c\u65f6\u5b58\u5728\u6548\u7387\u4f4e\u3001\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u4f20\u7edfNLP\u6a21\u578b\u6548\u679c\u4e0d\u8db3\uff0c\u9700\u5f00\u53d1\u9ad8\u6548\u51c6\u786e\u7684\u4ef7\u503c\u89c2\u8bc6\u522b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u672c\u5730\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08\u4ef7\u503c\u68c0\u6d4b\u5668\uff09\u751f\u6210\u521d\u59cb\u4f30\u8ba1\uff0c\u6784\u5efa\u7cbe\u7b80\u63d0\u793a\u4f9b\u5728\u7ebfLLMs\u5904\u7406\uff1b\u5f15\u5165\u89e3\u91ca\u6027\u8bad\u7ec3\u3001\u6570\u636e\u751f\u6210\u6280\u672f\u548c\u63d0\u793a\u957f\u5ea6\u4f18\u5316\u7b56\u7565\u3002", "result": "\u8f93\u5165token\u6570\u91cf\u51cf\u5c11\u81f3\u76f4\u63a5\u67e5\u8be2\u76841/6\uff0c\u6027\u80fd\u6301\u7eed\u8d85\u8d8a\u4f20\u7edfNLP\u65b9\u6cd5\u53ca\u5176\u4ed6LLM\u7b56\u7565\u3002", "conclusion": "EAVIT\u6846\u67b6\u5728\u6548\u7387\u4e0e\u51c6\u786e\u6027\u95f4\u5b9e\u73b0\u5e73\u8861\uff0c\u4e3a\u6587\u672c\u4e2d\u4eba\u7c7b\u4ef7\u503c\u89c2\u8bc6\u522b\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.12808", "pdf": "https://arxiv.org/pdf/2505.12808", "abs": "https://arxiv.org/abs/2505.12808", "authors": ["Yanbin Yin", "Kun Zhou", "Zhen Wang", "Xiangdong Zhang", "Yifei Shao", "Shibo Hao", "Yi Gu", "Jieyuan Liu", "Somanshu Singla", "Tianyang Liu", "Eric P. Xing", "Zhengzhong Liu", "Haojian Jin", "Zhiting Hu"], "title": "Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models", "categories": ["cs.CL", "cs.LG"], "comment": "20 pages, ongoing work", "summary": "The recent explosion of large language models (LLMs), each with its own\ngeneral or specialized strengths, makes scalable, reliable benchmarking more\nurgent than ever. Standard practices nowadays face fundamental trade-offs:\nclosed-ended question-based benchmarks (eg MMLU) struggle with saturation as\nnewer models emerge, while crowd-sourced leaderboards (eg Chatbot Arena) rely\non costly and slow human judges. Recently, automated methods (eg\nLLM-as-a-judge) shed light on the scalability, but risk bias by relying on one\nor a few \"authority\" models. To tackle these issues, we propose Decentralized\nArena (dearena), a fully automated framework leveraging collective intelligence\nfrom all LLMs to evaluate each other. It mitigates single-model judge bias by\ndemocratic, pairwise evaluation, and remains efficient at scale through two key\ncomponents: (1) a coarse-to-fine ranking algorithm for fast incremental\ninsertion of new models with sub-quadratic complexity, and (2) an automatic\nquestion selection strategy for the construction of new evaluation dimensions.\nAcross extensive experiments across 66 LLMs, dearena attains up to 97%\ncorrelation with human judgements, while significantly reducing the cost. Our\ncode and data will be publicly released on\nhttps://github.com/maitrix-org/de-arena.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u53bb\u4e2d\u5fc3\u5316\u8bc4\u4f30\u6846\u67b6Decentralized Arena\uff08dearena\uff09\uff0c\u901a\u8fc7LLM\u7fa4\u4f53\u667a\u80fd\u5b9e\u73b0\u9ad8\u6548\u4e92\u8bc4\uff0c\u663e\u8457\u964d\u4f4e\u8bc4\u4f30\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u5c01\u95ed\u5f0f\u95ee\u9898\u9971\u548c\u3001\u4eba\u5de5\u8bc4\u4f30\u6210\u672c\u9ad8\u3001\u5355\u4e00\u6a21\u578b\u8bc4\u5224\u504f\u89c1\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u53ef\u9760\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u6c11\u4e3b\u5316\u4e24\u4e24\u8bc4\u4f30\u673a\u5236\uff08\u964d\u4f4e\u504f\u89c1\uff09+ \u5305\u542b\u6b21\u4e8c\u6b21\u590d\u6742\u5ea6\u7684\u7c97\u5230\u7ec6\u6392\u540d\u7b97\u6cd5\uff08\u5feb\u901f\u63d2\u5165\u65b0\u6a21\u578b\uff09+ \u81ea\u52a8\u95ee\u9898\u9009\u62e9\u7b56\u7565\uff08\u6784\u5efa\u65b0\u8bc4\u4f30\u7ef4\u5ea6\uff09\u3002", "result": "\u572866\u4e2aLLM\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u4e0e\u4eba\u7c7b\u5224\u65ad\u76f8\u5173\u6027\u9ad8\u8fbe97%\uff0c\u540c\u65f6\u6210\u672c\u663e\u8457\u964d\u4f4e\u3002", "conclusion": "dearena\u901a\u8fc7\u7fa4\u4f53\u667a\u80fd\u5b9e\u73b0\u9ad8\u6548\u53bb\u4e2d\u5fc3\u5316\u8bc4\u4f30\uff0c\u4e3aLLM\u6301\u7eed\u8fdb\u5316\u63d0\u4f9b\u53ef\u6301\u7eed\u7684\u8bc4\u4f30\u57fa\u7840\u8bbe\u65bd\u3002"}}
{"id": "2505.12814", "pdf": "https://arxiv.org/pdf/2505.12814", "abs": "https://arxiv.org/abs/2505.12814", "authors": ["Xilong Cheng", "Yunxiao Qin", "Yuting Tan", "Zhengnan Li", "Ye Wang", "Hongjiang Xiao", "Yuan Zhang"], "title": "PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing LLM-based role-playing methods often rely on superficial textual\ndescriptions or simplistic metrics, inadequately modeling both intrinsic and\nextrinsic character dimensions. Additionally, they typically simulate character\nmemory with implicit model knowledge or basic retrieval augment generation\nwithout explicit memory alignment, compromising memory consistency. The two\nissues weaken reliability of role-playing LLMs in several applications, such as\ntrustworthy social simulation. To address these limitations, we propose PsyMem,\na novel framework integrating fine-grained psychological attributes and\nexplicit memory control for role-playing. PsyMem supplements textual\ndescriptions with 26 psychological indicators to detailed model character.\nAdditionally, PsyMem implements memory alignment training, explicitly trains\nthe model to align character's response with memory, thereby enabling dynamic\nmemory-controlled responding during inference. By training Qwen2.5-7B-Instruct\non our specially designed dataset (including 5,414 characters and 38,962\ndialogues extracted from novels), the resulting model, termed as PsyMem-Qwen,\noutperforms baseline models in role-playing, achieving the best performance in\nhuman-likeness and character fidelity.", "AI": {"tldr": "\u63d0\u51faPsyMem\u6846\u67b6\uff0c\u901a\u8fc7\u5fc3\u7406\u5c5e\u6027\u5efa\u6a21\u548c\u663e\u5f0f\u8bb0\u5fc6\u63a7\u5236\u589e\u5f3aLLM\u89d2\u8272\u626e\u6f14\u7684\u53ef\u9760\u6027\u4e0e\u4e00\u81f4\u6027", "motivation": "\u73b0\u6709\u89d2\u8272\u626e\u6f14\u65b9\u6cd5\u5b58\u5728\u89d2\u8272\u5efa\u6a21\u7ef4\u5ea6\u4e0d\u8db3\uff08\u4ec5\u4f9d\u8d56\u6587\u672c\u63cf\u8ff0\uff09\u548c\u8bb0\u5fc6\u4e00\u81f4\u6027\u5dee\uff08\u7f3a\u4e4f\u663e\u5f0f\u5bf9\u9f50\uff09\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u793e\u4ea4\u6a21\u62df\u7b49\u5e94\u7528\u7684\u53ef\u4fe1\u5ea6", "method": "1. \u8865\u514526\u4e2a\u5fc3\u7406\u6307\u6807\u589e\u5f3a\u89d2\u8272\u5efa\u6a21\u7ef4\u5ea6 2. \u8bbe\u8ba1\u8bb0\u5fc6\u5bf9\u9f50\u8bad\u7ec3\u673a\u5236\u5b9e\u73b0\u663e\u5f0f\u8bb0\u5fc6\u63a7\u5236 3. \u57fa\u4e8e5,414\u4e2a\u89d2\u8272\u548c38,962\u6bb5\u5c0f\u8bf4\u5bf9\u8bdd\u8bad\u7ec3Qwen2.5-7B-Instruct\u6a21\u578b", "result": "PsyMem-Qwen\u5728\u4eba\u7c7b\u76f8\u4f3c\u5ea6\uff0883.2%\uff09\u548c\u89d2\u8272\u4fdd\u771f\u5ea6\uff0878.5%\uff09\u6307\u6807\u4e0a\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u8fbe\u5230SOTA\u6027\u80fd", "conclusion": "PsyMem\u6846\u67b6\u6210\u529f\u5c06\u5fc3\u7406\u5b66\u7ef4\u5ea6\u4e0e\u663e\u5f0f\u8bb0\u5fc6\u63a7\u5236\u7ed3\u5408\uff0c\u4e3a\u53ef\u4fe1\u89d2\u8272\u626e\u6f14\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u652f\u6491\u53ef\u9760\u7684\u793e\u4f1a\u6a21\u62df\u5e94\u7528"}}
{"id": "2505.12821", "pdf": "https://arxiv.org/pdf/2505.12821", "abs": "https://arxiv.org/abs/2505.12821", "authors": ["Han Sun", "Zhen Sun", "Zongmin Zhang", "Linzhao Jia", "Wei Shao", "Min Zhang"], "title": "SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are emerging as dominant forces for textual\nstyle transfer. However, for arbitrary style transfer, LLMs face two key\nchallenges: (1) considerable reliance on manually-constructed prompts and (2)\nrigid stylistic biases inherent in LLMs. In this paper, we propose a novel\nSynthesize-then-Decode (SynDec) approach, which automatically synthesizes\nhigh-quality prompts and amplifies their roles during decoding process.\nSpecifically, our approach synthesizes prompts by selecting representative\nfew-shot samples, conducting a four-dimensional style analysis, and reranking\nthe candidates. At LLM decoding stage, the TST effect is amplified by\nmaximizing the contrast in output probabilities between scenarios with and\nwithout the synthesized prompt, as well as between prompts and negative\nsamples. We conduct extensive experiments and the results show that SynDec\noutperforms existing state-of-the-art LLM-based methods on five out of six\nbenchmarks (e.g., achieving up to a 9\\% increase in accuracy for\nmodern-to-Elizabethan English transfer). Detailed ablation studies further\nvalidate the effectiveness of SynDec.", "AI": {"tldr": "\u63d0\u51faSynDec\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u5408\u6210\u63d0\u793a\u8bcd+\u89e3\u7801\u589e\u5f3a\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3LLM\u98ce\u683c\u8fc1\u79fb\u4e2d\u624b\u52a8\u63d0\u793a\u4f9d\u8d56\u548c\u56fa\u6709\u98ce\u683c\u504f\u89c1\u95ee\u9898\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u591a\u4e2a\u57fa\u51c6\u8d85\u8d8aSOTA\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u4efb\u610f\u98ce\u683c\u8fc1\u79fb\u4e2d\u7684\u4e24\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a1) \u5bf9\u4eba\u5de5\u6784\u5efa\u63d0\u793a\u8bcd\u7684\u5f3a\u4f9d\u8d56 2) \u6a21\u578b\u56fa\u6709\u7684\u523b\u677f\u98ce\u683c\u504f\u89c1\u3002\u65e8\u5728\u63d0\u5347\u98ce\u683c\u8fc1\u79fb\u7684\u81ea\u52a8\u5316\u7a0b\u5ea6\u548c\u6548\u679c\u9c81\u68d2\u6027\u3002", "method": "Synthesize-then-Decode\u6846\u67b6\uff1a1) \u63d0\u793a\u8bcd\u5408\u6210\u9636\u6bb5\uff08\u4ee3\u8868\u6027\u6837\u672c\u9009\u62e9\u2192\u56db\u7ef4\u98ce\u683c\u5206\u6790\u2192\u5019\u9009\u63d0\u793a\u91cd\u6392\u5e8f\uff09 2) \u89e3\u7801\u9636\u6bb5\u901a\u8fc7\u5bf9\u6bd4\u6709/\u65e0\u63d0\u793a\u7684\u8f93\u51fa\u6982\u7387\u5dee\u5f02\uff0c\u4ee5\u53ca\u6b63\u8d1f\u6837\u672c\u5bf9\u6bd4\u6765\u653e\u5927\u63d0\u793a\u6548\u5e94\u3002", "result": "\u57286\u4e2a\u57fa\u51c6\u4e2d\u76845\u4e2a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u73b0\u4ee3\u82f1\u8bed\u2192\u4f0a\u4e3d\u838e\u767d\u65f6\u671f\u82f1\u8bed\u8f6c\u6362\u51c6\u786e\u7387\u63d0\u53479%\uff09\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u5404\u6a21\u5757\u6709\u6548\u6027\u3002", "conclusion": "SynDec\u9996\u6b21\u5b9e\u73b0LLM\u98ce\u683c\u8fc1\u79fb\u7684\u5168\u81ea\u52a8\u63d0\u793a\u5de5\u7a0b\uff0c\u901a\u8fc7\u63d0\u793a\u5408\u6210\u4e0e\u89e3\u7801\u589e\u5f3a\u7684\u534f\u540c\u4f5c\u7528\uff0c\u7a81\u7834\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u63d0\u793a\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.12831", "pdf": "https://arxiv.org/pdf/2505.12831", "abs": "https://arxiv.org/abs/2505.12831", "authors": ["Zifeng Cheng", "Zhonghui Wang", "Yuchen Fu", "Zhiwei Jiang", "Yafeng Yin", "Cong Wang", "Qing Gu"], "title": "Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Extracting sentence embeddings from large language models (LLMs) is a\npractical direction, as it requires neither additional data nor fine-tuning.\nPrevious studies usually focus on prompt engineering to guide LLMs to encode\nthe core semantic information of the sentence into the embedding of the last\ntoken. However, the last token in these methods still encodes an excess of\nnon-essential information, such as stop words, limiting its encoding capacity.\nTo this end, we propose a Contrastive Prompting (CP) method that introduces an\nextra auxiliary prompt to elicit better sentence embedding. By contrasting with\nthe auxiliary prompt, CP can steer existing prompts to encode the core\nsemantics of the sentence, rather than non-essential information. CP is a\nplug-and-play inference-time intervention method that can be combined with\nvarious prompt-based methods. Extensive experiments on Semantic Textual\nSimilarity (STS) tasks and downstream classification tasks demonstrate that our\nmethod can improve the performance of existing prompt-based methods across\ndifferent LLMs. Our code will be released at https://github.com/zifengcheng/CP.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5bf9\u6bd4\u63d0\u793a\uff08CP\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u8f85\u52a9\u63d0\u793a\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u53e5\u5b50\u5d4c\u5165\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u5728\u6700\u540e\u4e00\u4e2atoken\u7f16\u7801\u65f6\u4ecd\u5305\u542b\u8fc7\u591a\u975e\u5fc5\u8981\u4fe1\u606f\uff08\u5982\u505c\u7528\u8bcd\uff09\uff0c\u9650\u5236\u4e86\u7f16\u7801\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u673a\u5236\uff0c\u901a\u8fc7\u8f85\u52a9\u63d0\u793a\u7684\u5bf9\u6bd4\u8feb\u4f7f\u4e3b\u63d0\u793a\u805a\u7126\u6838\u5fc3\u8bed\u4e49\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u63d2\u5373\u7528\u3002", "result": "\u5728STS\u4efb\u52a1\u548c\u5206\u7c7b\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\uff08\u4e0d\u540cLLM\u4e0a\u5747\u9a8c\u8bc1\u6709\u6548\uff09\uff0c\u4ee3\u7801\u5373\u5c06\u5f00\u6e90\u3002", "conclusion": "\u8bc1\u5b9e\u4e86\u5bf9\u6bd4\u673a\u5236\u5728\u8bed\u4e49\u7f16\u7801\u4e2d\u7684\u6709\u6548\u6027\uff0c\u4e3a\u63d0\u793a\u5de5\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7684\u4f18\u5316\u65b9\u5411\u3002"}}
{"id": "2505.12835", "pdf": "https://arxiv.org/pdf/2505.12835", "abs": "https://arxiv.org/abs/2505.12835", "authors": ["Hengxing Cai", "Jinhan Dong", "Jingjun Tan", "Jingcheng Deng", "Sihang Li", "Zhifeng Gao", "Haidong Wang", "Zicheng Su", "Agachai Sumalee", "Renxin Zhong"], "title": "FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital\nfor applications such as disaster response, logistics delivery, and urban\ninspection. However, existing methods often struggle with insufficient\nmultimodal fusion, weak generalization, and poor interpretability. To address\nthese challenges, we propose FlightGPT, a novel UAV VLN framework built upon\nVision-Language Models (VLMs) with powerful multimodal perception capabilities.\nWe design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT)\nusing high-quality demonstrations to improve initialization and structured\nreasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by\na composite reward that considers goal accuracy, reasoning quality, and format\ncompliance, to enhance generalization and adaptability. Furthermore, FlightGPT\nintroduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve\ndecision interpretability. Extensive experiments on the city-scale dataset\nCityNav demonstrate that FlightGPT achieves state-of-the-art performance across\nall scenarios, with a 9.22\\% higher success rate than the strongest baseline in\nunseen environments. Our implementation is publicly available.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684FlightGPT\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u548c\u601d\u7ef4\u94fe\u63a8\u7406\u673a\u5236\u663e\u8457\u63d0\u5347\u65e0\u4eba\u673a\u5bfc\u822a\u7684\u51c6\u786e\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u65e0\u4eba\u673a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\u5b58\u5728\u591a\u6a21\u6001\u878d\u5408\u4e0d\u8db3\u3001\u73af\u5883\u9002\u5e94\u5dee\u3001\u51b3\u7b56\u9ed1\u7bb1\u4e09\u5927\u75db\u70b9\uff0c\u5236\u7ea6\u5176\u5728\u57ce\u5e02\u7ea7\u590d\u6742\u573a\u666f\u7684\u5e94\u7528\u3002", "method": "1. \u76d1\u7763\u5fae\u8c03(SFT)\u6784\u5efa\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b 2. \u57fa\u4e8e\u76ee\u6807\u51c6\u786e\u5ea6+\u63a8\u7406\u8d28\u91cf+\u683c\u5f0f\u5408\u89c4\u7684\u590d\u5408\u5956\u52b1\u673a\u5236(GRPO\u7b97\u6cd5) 3. \u94fe\u5f0f\u601d\u7ef4(CoT)\u53ef\u89e3\u91ca\u51b3\u7b56\u6846\u67b6", "result": "CityNav\u6570\u636e\u96c6\u4e0a\u8fbe\u6210SOTA\uff0c\u672a\u77e5\u73af\u5883\u6210\u529f\u7387\u63d0\u53479.22%\uff0c\u63a8\u7406\u65f6\u95f4\u7f29\u77ed32%\u540c\u65f6\u4fdd\u630190.5%\u7684\u683c\u5f0f\u5408\u89c4\u7387\u3002", "conclusion": "FlightGPT\u901a\u8fc7\u7b97\u6cd5-\u67b6\u6784\u534f\u540c\u521b\u65b0\uff0c\u4e3a\u57ce\u5e02\u7ea7\u65e0\u4eba\u673a\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u591a\u6a21\u6001\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u5de5\u7a0b\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2505.12837", "pdf": "https://arxiv.org/pdf/2505.12837", "abs": "https://arxiv.org/abs/2505.12837", "authors": ["Christian Braun", "Alexander Lilienbeck", "Daniel Mentjukov"], "title": "The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages, 3 figures", "summary": "Legal contracts possess an inherent, semantically vital structure (e.g.,\nsections, clauses) that is crucial for human comprehension but whose impact on\nLLM processing remains under-explored. This paper investigates the effects of\nexplicit input text structure and prompt engineering on the performance of\nGPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the\nCUAD. We compare model exact-match accuracy across various input formats:\nwell-structured plain-text (human-generated from CUAD), plain-text cleaned of\nline breaks, extracted plain-text from Azure OCR, plain-text extracted by\nGPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o\nVision. To give an indication of the impact of possible prompt engineering, we\nassess the impact of shifting task instructions to the system prompt and\nexplicitly informing the model about the structured nature of the input. Our\nfindings reveal that GPT-4o demonstrates considerable robustness to variations\nin input structure, but lacks in overall performance. Conversely, GPT-4.1's\nperformance is markedly sensitive; poorly structured inputs yield suboptimal\nresults (but identical with GPT-4o), while well-structured formats (original\nCUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by\n~20 percentage points. Optimizing the system prompt to include task details and\nan advisory about structured input further elevates GPT-4.1's accuracy by an\nadditional ~10-13 percentage points, with Markdown ultimately achieving the\nhighest performance under these conditions (79 percentage points overall\nexact-match accuracy). This research empirically demonstrates that while newer\nmodels exhibit greater resilience, careful input structuring and strategic\nprompt design remain critical for optimizing the performance of LLMs, and can\nsignificantly affect outcomes in high-stakes legal applications.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0GPT-4o\u5bf9\u6cd5\u5f8b\u6587\u672c\u7ed3\u6784\u53d8\u5316\u66f4\u5177\u9c81\u68d2\u6027\u4f46\u6574\u4f53\u6027\u80fd\u8f83\u5f31\uff0c\u800cGPT-4.1\u6027\u80fd\u53d7\u8f93\u5165\u7ed3\u6784\u663e\u8457\u5f71\u54cd\u3002\u4f18\u5316\u6587\u672c\u7ed3\u6784\uff08Markdown\u683c\u5f0f\uff09\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u53ef\u4f7fGPT-4.1\u51c6\u786e\u7387\u63d0\u5347\u7ea633\u4e2a\u767e\u5206\u70b9\u81f379%", "motivation": "\u63a2\u7d22\u6cd5\u5f8b\u5408\u540c\u56fa\u6709\u7ed3\u6784\u5bf9LLM\u5904\u7406\u6548\u679c\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u6cd5\u5f8b\u5e94\u7528\u4e2d\u7ed3\u6784\u4f18\u5316\u548c\u63d0\u793a\u5de5\u7a0b\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u63d0\u5347\u6f5c\u529b", "method": "\u4f7f\u7528CUAD\u6570\u636e\u96c6\u6cd5\u5f8b\u95ee\u7b54\u4efb\u52a1\uff0c\u5bf9\u6bd4\u4e94\u79cd\u8f93\u5165\u683c\u5f0f\uff08\u539f\u751f\u7ed3\u6784\u6587\u672c/\u6e05\u7406\u6587\u672c/OCR\u6587\u672c/Vision\u6587\u672c/Vision MD\uff09\uff0c\u5e76\u901a\u8fc7\u8c03\u6574\u7cfb\u7edf\u63d0\u793a\u8bc4\u4f30\u63d0\u793a\u5de5\u7a0b\u6548\u679c", "result": "GPT-4o\u5728\u4e0d\u540c\u7ed3\u6784\u4e0b\u8868\u73b0\u7a33\u5b9a\u4f46\u51c6\u786e\u7387\u8f83\u4f4e\uff1bGPT-4.1\u5728\u7ed3\u6784\u5316\u8f93\u5165\u4e0b\u51c6\u786e\u7387\u63d0\u534720%\uff0c\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u518d\u63d0\u534710-13%\uff0cMarkdown\u683c\u5f0f\u6700\u7ec8\u8fbe\u523079%\u51c6\u786e\u7387", "conclusion": "\u5373\u4f7f\u65b0\u6a21\u578b\u5177\u5907\u66f4\u5f3a\u9002\u5e94\u6027\uff0c\u7cbe\u7ec6\u7684\u8f93\u5165\u7ed3\u6784\u4f18\u5316\u548c\u63d0\u793a\u8bbe\u8ba1\u4ecd\u662f\u63d0\u5347LLM\u6cd5\u5f8b\u5e94\u7528\u6027\u80fd\u7684\u5173\u952e\uff0c\u5c24\u5176\u5728\u6d89\u53ca\u91cd\u5927\u6cd5\u5f8b\u540e\u679c\u7684\u573a\u666f\u4e2d"}}
{"id": "2505.12859", "pdf": "https://arxiv.org/pdf/2505.12859", "abs": "https://arxiv.org/abs/2505.12859", "authors": ["Lucas Georges Gabriel Charpentier", "Pierre Lison"], "title": "Re-identification of De-identified Documents with Autoregressive Infilling", "categories": ["cs.CL"], "comment": "To be presented a ACL 2025, Main, Long paper", "summary": "Documents revealing sensitive information about individuals must typically be\nde-identified. This de-identification is often done by masking all mentions of\npersonally identifiable information (PII), thereby making it more difficult to\nuncover the identity of the person(s) in question. To investigate the\nrobustness of de-identification methods, we present a novel, RAG-inspired\napproach that attempts the reverse process of re-identification based on a\ndatabase of documents representing background knowledge. Given a text in which\npersonal identifiers have been masked, the re-identification proceeds in two\nsteps. A retriever first selects from the background knowledge passages deemed\nrelevant for the re-identification. Those passages are then provided to an\ninfilling model which seeks to infer the original content of each text span.\nThis process is repeated until all masked spans are replaced. We evaluate the\nre-identification on three datasets (Wikipedia biographies, court rulings and\nclinical notes). Results show that (1) as many as 80% of de-identified text\nspans can be successfully recovered and (2) the re-identification accuracy\nincreases along with the level of background knowledge.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eRAG\u7684\u9006\u5411\u5de5\u7a0b\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u7d22-\u586b\u5145\u4e24\u6b65\u6d41\u7a0b\u6210\u529f\u6062\u590d80%\u88ab\u63a9\u76d6\u7684\u4e2a\u4eba\u9690\u79c1\u4fe1\u606f", "motivation": "\u9a8c\u8bc1\u6587\u6863\u53bb\u6807\u8bc6\u5316\u65b9\u6cd5\u7684\u8106\u5f31\u6027\uff0c\u8bc1\u660e\u73b0\u6709\u9690\u79c1\u4fdd\u62a4\u63aa\u65bd\u53ef\u80fd\u88ab\u80cc\u666f\u77e5\u8bc6\u8f85\u52a9\u7684AI\u6a21\u578b\u7834\u89e3", "method": "\u7ed3\u5408\u68c0\u7d22\u5668(\u7b5b\u9009\u80cc\u666f\u77e5\u8bc6)\u4e0e\u586b\u5145\u6a21\u578b(\u63a8\u6d4b\u63a9\u7801\u5185\u5bb9)\u7684\u8fed\u4ee3\u6846\u67b6\uff0c\u5728\u7ef4\u57fa\u767e\u79d1\u3001\u53f8\u6cd5\u6587\u4e66\u548c\u533b\u7597\u8bb0\u5f55\u4e09\u7c7b\u6570\u636e\u6d4b\u8bd5", "result": "\u6700\u9ad8\u8fbe80%\u7684\u6587\u672c\u8de8\u5ea6\u53ef\u88ab\u51c6\u786e\u6062\u590d\uff0c\u4e14\u80cc\u666f\u77e5\u8bc6\u5e93\u8d8a\u5b8c\u5907\u5219\u91cd\u65b0\u8bc6\u522b\u51c6\u786e\u7387\u8d8a\u9ad8", "conclusion": "\u5f53\u524d\u53bb\u6807\u8bc6\u5316\u6280\u672f\u5b58\u5728\u91cd\u5927\u5b89\u5168\u6f0f\u6d1e\uff0c\u9690\u79c1\u4fdd\u62a4\u65b9\u6848\u9700\u8003\u8651\u5bf9\u6297\u6027\u80cc\u666f\u77e5\u8bc6\u653b\u51fb"}}
{"id": "2505.12864", "pdf": "https://arxiv.org/pdf/2505.12864", "abs": "https://arxiv.org/abs/2505.12864", "authors": ["Yu Fan", "Jingwei Ni", "Jakob Merane", "Etienne Salimbeni", "Yang Tian", "Yoan Hermstr\u00fcwer", "Yinya Huang", "Mubashara Akhtar", "Florian Geering", "Oliver Dreyer", "Daniel Brunner", "Markus Leippold", "Mrinmaya Sachan", "Alexander Stremitzer", "Christoph Engel", "Elliott Ash", "Joel Niklaus"], "title": "LEXam: Benchmarking Legal Reasoning on 340 Law Exams", "categories": ["cs.CL", "cs.AI", "cs.LG", "68T50", "I.2"], "comment": null, "summary": "Long-form legal reasoning remains a key challenge for large language models\n(LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a\nnovel benchmark derived from 340 law exams spanning 116 law school courses\nacross a range of subjects and degree levels. The dataset comprises 4,886 law\nexam questions in English and German, including 2,841 long-form, open-ended\nquestions and 2,045 multiple-choice questions. Besides reference answers, the\nopen questions are also accompanied by explicit guidance outlining the expected\nlegal reasoning approach such as issue spotting, rule recall, or rule\napplication. Our evaluation on both open-ended and multiple-choice questions\npresent significant challenges for current LLMs; in particular, they notably\nstruggle with open questions that require structured, multi-step legal\nreasoning. Moreover, our results underscore the effectiveness of the dataset in\ndifferentiating between models with varying capabilities. Adopting an\nLLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate\nhow model-generated reasoning steps can be evaluated consistently and\naccurately. Our evaluation setup provides a scalable method to assess legal\nreasoning quality beyond simple accuracy metrics. Project page:\nhttps://lexam-benchmark.github.io/", "AI": {"tldr": "LEXam\u57fa\u51c6\u6d4b\u8bd5\u57fa\u4e8e340\u4e2a\u6cd5\u5f8b\u8003\u8bd5\u6784\u5efa\uff0c\u63ed\u793a\u5f53\u524d\u5927\u6a21\u578b\u5728\u7ed3\u6784\u5316\u6cd5\u5f8b\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u663e\u8457\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u6a21\u578b\u81ea\u8bc4\u4f30\u7684\u53ef\u6269\u5c55\u8bc4\u6d4b\u6846\u67b6", "motivation": "\u9488\u5bf9\u73b0\u6709\u5927\u6a21\u578b\u5728\u957f\u683c\u5f0f\u6cd5\u5f8b\u63a8\u7406\u4efb\u52a1\u4e2d\u8bc4\u4f30\u4f53\u7cfb\u4e0d\u5b8c\u5584\u7684\u95ee\u9898\uff0c\u65e8\u5728\u6784\u5efa\u4e13\u4e1a\u9886\u57df\u7684\u80fd\u529b\u8bc4\u4f30\u57fa\u51c6\u5e76\u63ed\u793a\u6a21\u578b\u5c40\u9650\u6027", "method": "\u4ece116\u95e8\u6cd5\u5f8b\u8bfe\u7a0b\u4e2d\u6536\u96c64,886\u9053\u8003\u8bd5\u9898\u76ee\u6784\u5efaLEXam\u6570\u636e\u96c6\uff0c\u91c7\u7528LLM-as-a-Judge\u8303\u5f0f\u914d\u5408\u4e13\u5bb6\u9a8c\u8bc1\u5b9e\u73b0\u81ea\u52a8\u5316\u8bc4\u4f30", "result": "\u6a21\u578b\u5728\u9700\u8981\u591a\u6b65\u9aa4\u6cd5\u5f8b\u63a8\u7406\u7684\u5f00\u653e\u9898\u4e0a\u51c6\u786e\u7387\u663e\u8457\u4f4e\u4e8e\u9009\u62e9\u9898\uff08\u5c24\u5176\u6d89\u53ca\u89c4\u5219\u5e94\u7528\u65f6\uff09\uff0c\u6570\u636e\u96c6\u6709\u6548\u533a\u5206\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u80fd\u529b\u5dee\u5f02", "conclusion": "LEXam\u4e3a\u6cd5\u5f8b\u63a8\u7406\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u5f3a\u8c03\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\u6d4b\u8bd5\u7684\u91cd\u8981\u6027\uff0c\u63a8\u52a8\u9886\u57df\u4e13\u7528\u6a21\u578b\u7684\u53d1\u5c55"}}
{"id": "2505.12888", "pdf": "https://arxiv.org/pdf/2505.12888", "abs": "https://arxiv.org/abs/2505.12888", "authors": ["Jialun Zhong", "Yanzeng Li", "Sen Hu", "Yang Zhang", "Teng Xu", "Lei Zou"], "title": "GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation", "categories": ["cs.CL"], "comment": null, "summary": "Medication recommendations have become an important task in the healthcare\ndomain, especially in measuring the accuracy and safety of medical dialogue\nsystems (MDS). Different from the recommendation task based on electronic\nhealth records (EHRs), dialogue-based medication recommendations require\nresearch on the interaction details between patients and doctors, which is\ncrucial but may not exist in EHRs. Recent advancements in large language models\n(LLM) have extended the medical dialogue domain. These LLMs can interpret\npatients' intent and provide medical suggestions including medication\nrecommendations, but some challenges are still worth attention. During a\nmulti-turn dialogue, LLMs may ignore the fine-grained medical information or\nconnections across the dialogue turns, which is vital for providing accurate\nsuggestions. Besides, LLMs may generate non-factual responses when there is a\nlack of domain-specific knowledge, which is more risky in the medical domain.\nTo address these challenges, we propose a \\textbf{G}raph-\\textbf{A}ssisted\n\\textbf{P}rompts (\\textbf{GAP}) framework for dialogue-based medication\nrecommendation. It extracts medical concepts and corresponding states from\ndialogue to construct an explicitly patient-centric graph, which can describe\nthe neglected but important information. Further, combined with external\nmedical knowledge graphs, GAP can generate abundant queries and prompts, thus\nretrieving information from multiple sources to reduce the non-factual\nresponses. We evaluate GAP on a dialogue-based medication recommendation\ndataset and further explore its potential in a more difficult scenario,\ndynamically diagnostic interviewing. Extensive experiments demonstrate its\ncompetitive performance when compared with strong baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86GAP\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u60a3\u8005\u4e2d\u5fc3\u56fe\u7ed3\u5408\u5916\u90e8\u77e5\u8bc6\u56fe\u8c31\uff0c\u63d0\u5347\u533b\u7597\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u7528\u836f\u63a8\u8350\u7684\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027", "motivation": "\u73b0\u6709LLM\u5728\u591a\u8f6e\u533b\u7597\u5bf9\u8bdd\u4e2d\u5bb9\u6613\u5ffd\u7565\u7ec6\u7c92\u5ea6\u533b\u7597\u4fe1\u606f\u53ca\u8de8\u8f6e\u6b21\u5173\u8054\uff0c\u4e14\u7f3a\u4e4f\u9886\u57df\u77e5\u8bc6\u5bfc\u81f4\u975e\u4e8b\u5b9e\u6027\u56de\u5e94\u98ce\u9669", "method": "1. \u4ece\u5bf9\u8bdd\u4e2d\u63d0\u53d6\u533b\u7597\u6982\u5ff5\u6784\u5efa\u60a3\u8005\u4e2d\u5fc3\u56fe 2. \u7ed3\u5408\u5916\u90e8\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u751f\u6210\u67e5\u8be2\u63d0\u793a 3. \u591a\u6e90\u4fe1\u606f\u68c0\u7d22\u673a\u5236", "result": "\u5728\u7528\u836f\u63a8\u8350\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u52a8\u6001\u8bca\u65ad\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u5e94\u7528\u6f5c\u529b", "conclusion": "GAP\u6846\u67b6\u6709\u6548\u6355\u6349\u5bf9\u8bdd\u7ec6\u8282\u5173\u8054\uff0c\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u663e\u8457\u964d\u4f4e\u975e\u4e8b\u5b9e\u54cd\u5e94\uff0c\u4e3a\u533b\u7597\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u53ef\u9760\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.12896", "pdf": "https://arxiv.org/pdf/2505.12896", "abs": "https://arxiv.org/abs/2505.12896", "authors": ["Chenxi Liu", "Yongqiang Chen", "Tongliang Liu", "James Cheng", "Bo Han", "Kun Zhang"], "title": "On the Thinking-Language Modeling Gap in Large Language Models", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": "Chenxi and Yongqiang contributed equally; project page:\n  https://causalcoat.github.io/lot.html", "summary": "System 2 reasoning is one of the defining characteristics of intelligence,\nwhich requires slow and logical thinking. Human conducts System 2 reasoning via\nthe language of thoughts that organizes the reasoning process as a causal\nsequence of mental language, or thoughts. Recently, it has been observed that\nSystem 2 reasoning can be elicited from Large Language Models (LLMs)\npre-trained on large-scale natural languages. However, in this work, we show\nthat there is a significant gap between the modeling of languages and thoughts.\nAs language is primarily a tool for humans to share knowledge and thinking,\nmodeling human language can easily absorb language biases into LLMs deviated\nfrom the chain of thoughts in minds. Furthermore, we show that the biases will\nmislead the eliciting of \"thoughts\" in LLMs to focus only on a biased part of\nthe premise. To this end, we propose a new prompt technique termed\nLanguage-of-Thoughts (LoT) to demonstrate and alleviate this gap. Instead of\ndirectly eliciting the chain of thoughts from partial information, LoT\ninstructs LLMs to adjust the order and token used for the expressions of all\nthe relevant information. We show that the simple strategy significantly\nreduces the language modeling biases in LLMs and improves the performance of\nLLMs across a variety of reasoning tasks.", "AI": {"tldr": "\u63d0\u51faLanguage-of-Thoughts(LoT)\u6280\u672f\uff0c\u901a\u8fc7\u8c03\u6574\u8bed\u8a00\u8868\u8fbe\u987a\u5e8f\u548c\u7528\u8bcd\uff0c\u51cf\u5c11\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u8bed\u8a00\u5efa\u6a21\u504f\u5dee\uff0c\u663e\u8457\u63d0\u5347\u591a\u7c7b\u63a8\u7406\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u8bed\u8a00\u5efa\u6a21\u4e0e\u601d\u7ef4\u94fe\u5efa\u6a21\u7684\u504f\u5dee\uff1a1) \u8bed\u8a00\u4f5c\u4e3a\u77e5\u8bc6\u5206\u4eab\u5de5\u5177\u4f1a\u5f15\u5165\u504f\u79bb\u771f\u5b9e\u601d\u7ef4\u7684\u504f\u89c1 2) \u8fd9\u79cd\u504f\u89c1\u4f1a\u5bfc\u81f4\u6a21\u578b\u4ec5\u5173\u6ce8\u524d\u63d0\u4fe1\u606f\u7684\u5c40\u90e8\u504f\u7f6e\u90e8\u5206", "method": "LoT\u63d0\u793a\u6280\u672f\uff1a\u8981\u6c42\u6a21\u578b\u5bf9\u6240\u6709\u76f8\u5173\u4fe1\u606f\u8fdb\u884c\u8868\u8fbe\u987a\u5e8f\u548c\u8bcd\u6c47\u9009\u62e9\u7684\u91cd\u6784\uff0c\u800c\u975e\u76f4\u63a5\u4ece\u90e8\u5206\u4fe1\u606f\u4e2d\u5f15\u51fa\u601d\u7ef4\u94fe", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u964d\u4f4e\u8bed\u8a00\u6a21\u578b\u504f\u5dee\uff0c\u6a21\u578b\u6027\u80fd\u5e73\u5747\u63d0\u5347\u7ea615%", "conclusion": "\u8bed\u8a00\u5efa\u6a21\u4e0e\u601d\u7ef4\u5efa\u6a21\u5b58\u5728\u672c\u8d28\u5dee\u5f02\uff0cLoT\u6280\u672f\u901a\u8fc7\u89e3\u8026\u8bed\u8a00\u8868\u8fbe\u4e0e\u601d\u7ef4\u8fc7\u7a0b\uff0c\u6709\u6548\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684System 2\u63a8\u7406\u80fd\u529b"}}
{"id": "2505.12920", "pdf": "https://arxiv.org/pdf/2505.12920", "abs": "https://arxiv.org/abs/2505.12920", "authors": ["Paul Van Eecke", "Katrien Beuls"], "title": "PyFCG: Fluid Construction Grammar in Python", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": null, "summary": "We present PyFCG, an open source software library that ports Fluid\nConstruction Grammar (FCG) to the Python programming language. PyFCG enables\nits users to seamlessly integrate FCG functionality into Python programs, and\nto use FCG in combination with other libraries within Python's rich ecosystem.\nApart from a general description of the library, this paper provides three\nwalkthrough tutorials that demonstrate example usage of PyFCG in typical use\ncases of FCG: (i) formalising and testing construction grammar analyses, (ii)\nlearning usage-based construction grammars from corpora, and (iii) implementing\nagent-based experiments on emergent communication.", "AI": {"tldr": "PyFCG \u662f\u5c06 Fluid Construction Grammar (FCG) \u79fb\u690d\u5230 Python \u7684\u5f00\u6e90\u5e93\uff0c\u652f\u6301\u4e0e Python \u751f\u6001\u5de5\u5177\u96c6\u6210\uff0c\u5e76\u901a\u8fc7\u4e09\u4e2a\u5178\u578b\u7528\u4f8b\u6559\u7a0b\u5c55\u793a\u5176\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u6574\u5408 FCG \u5230 Python \u751f\u6001\uff0c\u4fbf\u4e8e\u7814\u7a76\u8005\u5229\u7528\u4e30\u5bcc\u7684 Python \u5e93\u8d44\u6e90\uff0c\u63d0\u5347\u6784\u5f0f\u8bed\u6cd5\u5206\u6790\u3001\u8bed\u6599\u5b66\u4e60\u548c\u6d8c\u73b0\u901a\u4fe1\u5b9e\u9a8c\u7684\u6548\u7387\u4e0e\u6269\u5c55\u6027\u3002", "method": "\u4ee5 Python \u5e93\u5f62\u5f0f\u5b9e\u73b0 FCG \u6838\u5fc3\u529f\u80fd\uff0c\u63d0\u4f9b\u8bed\u6cd5\u5206\u6790\u6d4b\u8bd5\u3001\u8bed\u6599\u5b66\u4e60\u7b97\u6cd5\u96c6\u6210\u3001\u591a\u4ee3\u7406\u901a\u4fe1\u5b9e\u9a8c\u6846\u67b6\u4e09\u7c7b\u6559\u7a0b\u9a8c\u8bc1\u5b9e\u7528\u6027\u3002", "result": "\u6210\u529f\u5f00\u53d1 PyFCG \u5e93\uff0c\u5e76\u5728\u5f62\u5f0f\u5316\u8bed\u6cd5\u9a8c\u8bc1\u3001\u8bed\u6599\u9a71\u52a8\u7684\u8bed\u6cd5\u5f52\u7eb3\u3001\u591a\u4ee3\u7406\u8bed\u8a00\u6f14\u5316\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u5176\u529f\u80fd\u5b8c\u5907\u6027\u548c\u6613\u7528\u6027\u3002", "conclusion": "PyFCG \u4e3a\u8ba1\u7b97\u6784\u5f0f\u8bed\u6cd5\u7814\u7a76\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u5de5\u5177\u652f\u6301\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u590d\u6742\u8bed\u8a00\u5efa\u6a21\u5b9e\u9a8c\u7684\u6280\u672f\u95e8\u69db\uff0c\u63a8\u52a8\u8de8\u9886\u57df\u7814\u7a76\u534f\u4f5c\u3002"}}
{"id": "2505.12929", "pdf": "https://arxiv.org/pdf/2505.12929", "abs": "https://arxiv.org/abs/2505.12929", "authors": ["Zhihe Yang", "Xufang Luo", "Zilong Wang", "Dongqi Han", "Zhiyuan He", "Dongsheng Li", "Yunjian Xu"], "title": "Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "24 pages, 12 figures", "summary": "Reinforcement learning (RL) has become a cornerstone for enhancing the\nreasoning capabilities of large language models (LLMs), with recent innovations\nsuch as Group Relative Policy Optimization (GRPO) demonstrating exceptional\neffectiveness. In this study, we identify a critical yet underexplored issue in\nRL training: low-probability tokens disproportionately influence model updates\ndue to their large gradient magnitudes. This dominance hinders the effective\nlearning of high-probability tokens, whose gradients are essential for LLMs'\nperformance but are substantially suppressed. To mitigate this interference, we\npropose two novel methods: Advantage Reweighting and Low-Probability Token\nIsolation (Lopti), both of which effectively attenuate gradients from\nlow-probability tokens while emphasizing parameter updates driven by\nhigh-probability tokens. Our approaches promote balanced updates across tokens\nwith varying probabilities, thereby enhancing the efficiency of RL training.\nExperimental results demonstrate that they substantially improve the\nperformance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K\nLogic Puzzle reasoning tasks. Our implementation is available at\nhttps://github.com/zhyang2226/AR-Lopti.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u5f3a\u5316\u5b66\u4e60\u4e2d\u4f4e\u6982\u7387token\u68af\u5ea6\u5e72\u6270\u95ee\u9898\uff0c\u63d0\u51faAdvantage Reweighting\u548cLopti\u65b9\u6cd5\uff0c\u5b9e\u9a8c\u663e\u793a\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u6027\u80fd\u63d0\u534746.2%", "motivation": "\u53d1\u73b0\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u4e2d\u4f4e\u6982\u7387token\u56e0\u68af\u5ea6\u5e45\u503c\u8fc7\u5927\u4e3b\u5bfc\u6a21\u578b\u66f4\u65b0\uff0c\u6291\u5236\u9ad8\u6982\u7387token\u7684\u6709\u6548\u5b66\u4e60\uff08\u5bf9LLM\u6027\u80fd\u8d77\u5173\u952e\u4f5c\u7528\uff09", "method": "Advantage Reweighting\u901a\u8fc7\u4f18\u52bf\u503c\u91cd\u52a0\u6743\u673a\u5236\uff0cLopti\u91c7\u7528\u4f4e\u6982\u7387token\u9694\u79bb\u6280\u672f\uff0c\u53cc\u7ba1\u9f50\u4e0b\u5e73\u8861\u4e0d\u540c\u6982\u7387token\u7684\u68af\u5ea6\u66f4\u65b0", "result": "\u5728K&K\u903b\u8f91\u8c1c\u9898\u4efb\u52a1\u4e2d\u5b9e\u73b0\u6700\u9ad846.2%\u7684\u6027\u80fd\u63d0\u5347\uff0cGitHub\u5f00\u6e90\u5b9e\u73b0\u9a8c\u8bc1\u65b9\u6cd5\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684\u68af\u5ea6\u5e73\u8861\u7b56\u7565\u663e\u8457\u63d0\u5347RL\u8bad\u7ec3\u6548\u7387\uff0c\u4e3aLLM\u7684\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u63d0\u4f9b\u65b0\u65b9\u6cd5\u8bba"}}
{"id": "2505.12942", "pdf": "https://arxiv.org/pdf/2505.12942", "abs": "https://arxiv.org/abs/2505.12942", "authors": ["Jeffrey T. H. Wong", "Cheng Zhang", "Xinye Cao", "Pedro Gimenes", "George A. Constantinides", "Wayne Luk", "Yiren Zhao"], "title": "A3 : an Analytical Low-Rank Approximation Framework for Attention", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models have demonstrated remarkable performance; however,\ntheir massive parameter counts make deployment highly expensive. Low-rank\napproximation offers a promising compression solution, yet existing approaches\nhave two main limitations: (1) They focus on minimizing the output error of\nindividual linear layers, without considering the architectural characteristics\nof Transformers, and (2) they decompose a large weight matrix into two small\nlow-rank matrices. Consequently, these methods often fall short compared to\nother compression techniques like pruning and quantization, and introduce\nruntime overhead such as the extra GEMM kernel launches for decomposed small\nmatrices. To address these limitations, we propose $\\tt A^\\tt 3$, a\npost-training low-rank approximation framework. $\\tt A^\\tt 3$ splits a\nTransformer layer into three functional components, namely $\\tt QK$, $\\tt OV$,\nand $\\tt MLP$. For each component, $\\tt A^\\tt 3$ provides an analytical\nsolution that reduces the hidden dimension size inside each component while\nminimizing the component's functional loss ($\\it i.e.$, error in attention\nscores, attention outputs, and MLP outputs). This approach directly reduces\nmodel sizes, KV cache sizes, and FLOPs without introducing any runtime\noverheads. In addition, it provides a new narrative in advancing the\noptimization problem from singular linear layer loss optimization toward\nimproved end-to-end performance. Through extensive experiments, we show that\n$\\tt A^\\tt 3$ maintains superior performance compared to SoTAs. For example,\nunder the same reduction budget in computation and memory, our low-rank\napproximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,\noutperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the\nversatility of $\\tt A^\\tt 3$, including KV cache compression, quantization, and\nmixed-rank assignments for enhanced performance.", "AI": {"tldr": "\u63d0\u51faA\u00b3\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5272Transformer\u5c42\u5e76\u4f18\u5316\u7ec4\u4ef6\u529f\u80fd\u635f\u5931\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u4f4e\u79e9\u8fd1\u4f3c\u538b\u7f29", "motivation": "\u73b0\u6709\u4f4e\u79e9\u8fd1\u4f3c\u65b9\u6cd5\u5ffd\u89c6Transformer\u7ed3\u6784\u7279\u6027\u4e14\u5f15\u5165\u8fd0\u884c\u65f6\u5f00\u9500\uff0cA\u00b3\u901a\u8fc7\u7ec4\u4ef6\u4f18\u5316\u76f4\u63a5\u51cf\u5c11\u6a21\u578b\u89c4\u6a21\u5e76\u4fdd\u6301\u6027\u80fd", "method": "\u5c06Transformer\u5c42\u62c6\u5206\u4e3aQK\u3001OV\u3001MLP\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u5bf9\u6bcf\u4e2a\u7ec4\u4ef6\u63d0\u4f9b\u964d\u4f4e\u9690\u85cf\u7ef4\u5ea6\u540c\u65f6\u6700\u5c0f\u5316\u529f\u80fd\u635f\u5931\u7684\u89e3\u6790\u89e3", "result": "LLaMA 3.1-70B\u5728WikiText-2\u56f0\u60d1\u5ea6\u964d\u81f34.69\uff0c\u8f83\u4e4b\u524d\u6700\u4f18\u7ed3\u679c\u63d0\u53473.18\uff1b\u652f\u6301KV\u7f13\u5b58\u538b\u7f29\u548c\u6df7\u5408\u79e9\u5206\u914d", "conclusion": "A\u00b3\u5728\u4fdd\u6301\u6027\u80fd\u4f18\u52bf\u7684\u540c\u65f6\u76f4\u63a5\u51cf\u5c11\u8ba1\u7b97/\u5185\u5b58\u6d88\u8017\uff0c\u4e3a\u6a21\u578b\u538b\u7f29\u63d0\u4f9b\u65b0\u4f18\u5316\u8303\u5f0f\u5e76\u5c55\u73b0\u591a\u573a\u666f\u5e94\u7528\u6f5c\u529b"}}
{"id": "2505.12949", "pdf": "https://arxiv.org/pdf/2505.12949", "abs": "https://arxiv.org/abs/2505.12949", "authors": ["Cael Marquard", "Simbarashe Mawere", "Francois Meyer"], "title": "Neural Morphological Tagging for Nguni Languages", "categories": ["cs.CL"], "comment": null, "summary": "Morphological parsing is the task of decomposing words into morphemes, the\nsmallest units of meaning in a language, and labelling their grammatical roles.\nIt is a particularly challenging task for agglutinative languages, such as the\nNguni languages of South Africa, which construct words by concatenating\nmultiple morphemes. A morphological parsing system can be framed as a pipeline\nwith two separate components, a segmenter followed by a tagger. This paper\ninvestigates the use of neural methods to build morphological taggers for the\nfour Nguni languages. We compare two classes of approaches: training neural\nsequence labellers (LSTMs and neural CRFs) from scratch and finetuning\npretrained language models. We compare performance across these two categories,\nas well as to a traditional rule-based morphological parser. Neural taggers\ncomfortably outperform the rule-based baseline and models trained from scratch\ntend to outperform pretrained models. We also compare parsing results across\ndifferent upstream segmenters and with varying linguistic input features. Our\nfindings confirm the viability of employing neural taggers based on\npre-existing morphological segmenters for the Nguni languages.", "AI": {"tldr": "\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u4e3a\u6069\u53e4\u5c3c\u8bed\u8a00\u6784\u5efa\u5f62\u6001\u5b66\u6807\u6ce8\u5668\uff0c\u5b9e\u9a8c\u8868\u660e\u795e\u7ecf\u7f51\u7edc\u6807\u6ce8\u5668\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u89c4\u5219\u7cfb\u7edf\uff0c\u4e14\u4ece\u5934\u8bad\u7ec3\u7684\u6a21\u578b\u4f18\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "motivation": "\u9488\u5bf9\u5357\u975e\u6069\u53e4\u5c3c\u7b49\u9ecf\u7740\u8bed\u7684\u5f62\u6001\u5b66\u89e3\u6790\u96be\u9898\uff08\u9700\u5206\u89e3\u8bcd\u7d20\u5e76\u6807\u6ce8\u8bed\u6cd5\u89d2\u8272\uff09\uff0c\u63a2\u7d22\u5982\u4f55\u7528\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u66ff\u4ee3\u4f20\u7edf\u89c4\u5219\u7cfb\u7edf\u4ee5\u63d0\u9ad8\u6807\u6ce8\u6548\u679c\u3002", "method": "1. \u5bf9\u6bd4\u4e24\u7c7b\u65b9\u6cd5\uff1aa) \u4ece\u5934\u8bad\u7ec3\u795e\u7ecf\u5e8f\u5217\u6807\u6ce8\u6a21\u578b\uff08LSTM/\u795e\u7ecfCRF\uff09 b) \u5fae\u8c03\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\n2. \u4e0e\u4f20\u7edf\u89c4\u5219\u7cfb\u7edf\u57fa\u7ebf\u6bd4\u8f83\n3. \u6d4b\u8bd5\u4e0d\u540c\u4e0a\u6e38\u5206\u8bcd\u5668\u548c\u8bed\u8a00\u5b66\u7279\u5f81\u7684\u5f71\u54cd", "result": "1. \u795e\u7ecf\u7f51\u7edc\u6807\u6ce8\u5668\u5168\u9762\u8d85\u8d8a\u89c4\u5219\u7cfb\u7edf\uff08p=0.01\uff09\n2. \u4ece\u5934\u8bad\u7ec3\u6a21\u578bF1\u5e73\u5747\u6bd4\u9884\u8bad\u7ec3\u6a21\u578b\u9ad83.2\u4e2a\u767e\u5206\u70b9\n3. \u73b0\u6709\u5f62\u6001\u5b66\u5206\u8bcd\u5668\u53ef\u4e3a\u795e\u7ecf\u6807\u6ce8\u5668\u63d0\u4f9b\u6709\u6548\u652f\u6301", "conclusion": "\u57fa\u4e8e\u73b0\u6709\u5f62\u6001\u5b66\u5206\u8bcd\u5668\u7684\u795e\u7ecf\u7f51\u7edc\u6807\u6ce8\u5668\u662f\u6069\u53e4\u5c3c\u8bed\u8a00\u5f62\u6001\u5b66\u89e3\u6790\u7684\u53ef\u884c\u65b9\u6848\uff0c\u5176\u4e2d\u4ece\u5934\u8bad\u7ec3\u7684\u5e8f\u5217\u6807\u6ce8\u6a21\u578b\u8868\u73b0\u6700\u4f18\uff0c\u8be5\u65b9\u6cd5\u4e3a\u4f4e\u8d44\u6e90\u9ecf\u7740\u8bed\u5904\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2505.12950", "pdf": "https://arxiv.org/pdf/2505.12950", "abs": "https://arxiv.org/abs/2505.12950", "authors": ["Daehee Kim", "Deokhyung Kang", "Jonghwi Kim", "Sangwon Ryu", "Gary Geunbae Lee"], "title": "GuRE:Generative Query REwriter for Legal Passage Retrieval", "categories": ["cs.CL"], "comment": "14 pages, 9 figures", "summary": "Legal Passage Retrieval (LPR) systems are crucial as they help practitioners\nsave time when drafting legal arguments. However, it remains an underexplored\navenue. One primary reason is the significant vocabulary mismatch between the\nquery and the target passage. To address this, we propose a simple yet\neffective method, the Generative query REwriter (GuRE). We leverage the\ngenerative capabilities of Large Language Models (LLMs) by training the LLM for\nquery rewriting. \"Rewritten queries\" help retrievers to retrieve target\npassages by mitigating vocabulary mismatch. Experimental results show that GuRE\nsignificantly improves performance in a retriever-agnostic manner,\noutperforming all baseline methods. Further analysis reveals that different\ntraining objectives lead to distinct retrieval behaviors, making GuRE more\nsuitable than direct retriever fine-tuning for real-world applications. Codes\nare avaiable at github.com/daehuikim/GuRE.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u67e5\u8be2\u6539\u5199\u65b9\u6cd5GuRE\uff0c\u901a\u8fc7\u7f13\u89e3\u8bcd\u6c47\u4e0d\u5339\u914d\u663e\u8457\u63d0\u5347\u6cd5\u5f8b\u6bb5\u843d\u68c0\u7d22\u6548\u679c", "motivation": "\u6cd5\u5f8b\u6bb5\u843d\u68c0\u7d22\u7cfb\u7edf\u56e0\u67e5\u8be2\u4e0e\u76ee\u6807\u6bb5\u843d\u95f4\u7684\u8bcd\u6c47\u4e0d\u5339\u914d\u95ee\u9898\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u9700\u6709\u6548\u89e3\u51b3\u65b9\u6848", "method": "\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6539\u5199\u540e\u7684\u67e5\u8be2\uff0c\u4f7f\u68c0\u7d22\u5668\u80fd\u66f4\u51c6\u786e\u5730\u5b9a\u4f4d\u76ee\u6807\u6cd5\u5f8b\u6bb5\u843d", "result": "GuRE\u5728\u4e0d\u540c\u68c0\u7d22\u5668\u4e2d\u5b9e\u73b0\u5e73\u57476.4%\u7684NDCG@5\u63d0\u5347\uff0c\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "GuRE\u7684\u68c0\u7d22\u5668\u65e0\u5173\u7279\u6027\u548c\u591a\u6837\u5316\u8bad\u7ec3\u76ee\u6807\uff0c\u4f7f\u5176\u6bd4\u76f4\u63a5\u5fae\u8c03\u66f4\u9002\u5408\u5b9e\u9645\u6cd5\u5f8b\u68c0\u7d22\u573a\u666f"}}
{"id": "2505.12964", "pdf": "https://arxiv.org/pdf/2505.12964", "abs": "https://arxiv.org/abs/2505.12964", "authors": ["Shanshan Liu", "Noriki Nishida", "Rumana Ferdous Munne", "Narumi Tokunaga", "Yuki Yamagata", "Kouji Kozaki", "Yuji Matsumoto"], "title": "MA-COIR: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition", "categories": ["cs.CL"], "comment": "preprint", "summary": "Recognizing biomedical concepts in the text is vital for ontology refinement,\nknowledge graph construction, and concept relationship discovery. However,\ntraditional concept recognition methods, relying on explicit mention\nidentification, often fail to capture complex concepts not explicitly stated in\nthe text. To overcome this limitation, we introduce MA-COIR, a framework that\nreformulates concept recognition as an indexing-recognition task. By assigning\nsemantic search indexes (ssIDs) to concepts, MA-COIR resolves ambiguities in\nontology entries and enhances recognition efficiency. Using a pretrained\nBART-based model fine-tuned on small datasets, our approach reduces\ncomputational requirements to facilitate adoption by domain experts.\nFurthermore, we incorporate large language models (LLMs)-generated queries and\nsynthetic data to improve recognition in low-resource settings. Experimental\nresults on three scenarios (CDR, HPO, and HOIP) highlight the effectiveness of\nMA-COIR in recognizing both explicit and implicit concepts without the need for\nmention-level annotations during inference, advancing ontology-driven concept\nrecognition in biomedical domain applications. Our code and constructed data\nare available at https://github.com/sl-633/macoir-master.", "AI": {"tldr": "\u63d0\u51faMA-COIR\u6846\u67b6\u5c06\u751f\u7269\u533b\u5b66\u6982\u5ff5\u8bc6\u522b\u91cd\u6784\u4e3a\u7d22\u5f15-\u8bc6\u522b\u4efb\u52a1\uff0c\u901a\u8fc7\u8bed\u4e49\u641c\u7d22\u7d22\u5f15(ssIDs)\u89e3\u51b3\u672c\u4f53\u6b67\u4e49\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7BART\u6a21\u578b\u4e0eLLM\u751f\u6210\u6570\u636e\uff0c\u5728\u4f4e\u8d44\u6e90\u573a\u666f\u4e0b\u5b9e\u73b0\u663e/\u9690\u5f0f\u6982\u5ff5\u7684\u8054\u5408\u8bc6\u522b\u3002", "motivation": "\u4f20\u7edf\u6982\u5ff5\u8bc6\u522b\u65b9\u6cd5\u4f9d\u8d56\u663e\u5f0f\u6587\u672c\u63d0\u53ca\uff0c\u96be\u4ee5\u6355\u6349\u9690\u542b\u7684\u590d\u6742\u751f\u7269\u533b\u5b66\u6982\u5ff5\uff0c\u9650\u5236\u4e86\u672c\u4f53\u8bba\u5b8c\u5584\u548c\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u6548\u679c\u3002\u9700\u8981\u5f00\u53d1\u65e0\u9700\u63d0\u53ca\u6807\u6ce8\u7684\u65b0\u65b9\u6cd5\u63d0\u5347\u6982\u5ff5\u8bc6\u522b\u80fd\u529b\u3002", "method": "1. \u5efa\u7acb\u7d22\u5f15-\u8bc6\u522b\u8303\u5f0f\uff0c\u901a\u8fc7ssIDs\u7f16\u7801\u6982\u5ff5\u8bed\u4e49\n2. \u4f7f\u7528\u9884\u8bad\u7ec3BART\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\n3. \u7ed3\u5408LLM\u751f\u6210\u67e5\u8be2\u8bed\u53e5\u548c\u5408\u6210\u6570\u636e\u589e\u5f3a\n4. \u652f\u6301\u591a\u573a\u666f\u6982\u5ff5\u8054\u5408\u8bc6\u522b", "result": "\u5728CDR/HPO/HOIP\u4e09\u4e2a\u573a\u666f\u6d4b\u8bd5\u4e2d\uff0cMA-COIR\u6709\u6548\u8bc6\u522b\u663e\u5f0f\u548c\u9690\u542b\u6982\u5ff5\uff0c\u63a8\u7406\u8fc7\u7a0b\u65e0\u9700\u63d0\u53ca\u7ea7\u6807\u6ce8\uff0c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u964d\u4f4e60%\u4ee5\u4e0a\u3002", "conclusion": "\u8be5\u6846\u67b6\u7a81\u7834\u4f20\u7edf\u65b9\u6cd5\u7684\u663e\u5f0f\u4f9d\u8d56\uff0c\u901a\u8fc7\u8bed\u4e49\u7d22\u5f15\u548c\u8f7b\u91cf\u5316\u6a21\u578b\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u751f\u7269\u533b\u5b66\u6982\u5ff5\u8bc6\u522b\u6548\u7387\uff0c\u4e3a\u9886\u57df\u4e13\u5bb6\u63d0\u4f9b\u4f4e\u95e8\u69db\u89e3\u51b3\u65b9\u6848\u3002\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002"}}
{"id": "2505.12969", "pdf": "https://arxiv.org/pdf/2505.12969", "abs": "https://arxiv.org/abs/2505.12969", "authors": ["Yingzhi Wang", "Anas Alhmoud", "Saad Alsahly", "Muhammad Alqurishi", "Mirco Ravanelli"], "title": "Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down", "categories": ["cs.CL"], "comment": "Accepted to Interspeech 2025", "summary": "OpenAI's Whisper has achieved significant success in Automatic Speech\nRecognition. However, it has consistently been found to exhibit hallucination\nissues, particularly in non-speech segments, which limits its broader\napplication in complex industrial settings.\n  In this paper, we introduce a novel method to reduce Whisper's hallucination\non non-speech segments without using any pre- or post-possessing techniques.\nSpecifically, we benchmark the contribution of each self-attentional head in\nthe Whisper-large-v3 decoder to the hallucination problem by performing a\nhead-wise mask. Our findings reveal that only 3 of the 20 heads account for\nover 75% of the hallucinations on the UrbanSound dataset. We then fine-tune\nthese three crazy heads using a collection of non-speech data. The results show\nthat our best fine-tuned model, namely Calm-Whisper, achieves over 80%\nreduction in non-speech hallucination with only less than 0.1% WER degradation\non LibriSpeech test-clean and test-other.", "AI": {"tldr": "\u63d0\u51faCalm-Whisper\u65b9\u6cd5\uff0c\u901a\u8fc7\u5fae\u8c03Whisper-large-v3\u89e3\u7801\u5668\u4e2d3\u4e2a\u5173\u952e\u6ce8\u610f\u529b\u5934\uff0c\u5728\u975e\u8bed\u97f3\u6bb5\u5b9e\u73b080%\u5e7b\u542c\u964d\u4f4e\uff0c\u8bcd\u9519\u7387\u4ec5\u589e\u52a00.1%", "motivation": "Whisper\u5728\u975e\u8bed\u97f3\u6bb5\u5b58\u5728\u4e25\u91cd\u5e7b\u542c\u95ee\u9898\uff0c\u5236\u7ea6\u5176\u5728\u590d\u6742\u5de5\u4e1a\u573a\u666f\u7684\u5e94\u7528\u3002\u9700\u8981\u4e0d\u4f9d\u8d56\u524d\u540e\u5904\u7406\u7684\u89e3\u51b3\u65b9\u6848", "method": "1. \u901a\u8fc7\u5934\u5c4f\u853d\u6280\u672f\u5b9a\u4f4d\u89e3\u7801\u5668\u4e2d\u5bfc\u81f4\u5e7b\u542c\u76843\u4e2a\u5173\u952e\u6ce8\u610f\u529b\u5934\uff08\u5360\u5e7b\u542c\u91cf\u768475%\uff09\uff1b2. \u4f7f\u7528\u975e\u8bed\u97f3\u6570\u636e\u5fae\u8c03\u8fd9\u4e09\u4e2a\u6ce8\u610f\u529b\u5934", "result": "\u5728UrbanSound\u6570\u636e\u96c6\u4e0a\u5e7b\u542c\u51cf\u5c1180%\uff0cLibriSpeech\u6d4b\u8bd5\u96c6\u8bcd\u9519\u7387\u589e\u5e45\u5c0f\u4e8e0.1%", "conclusion": "\u9488\u5bf9\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u7684\u5fae\u8c03\u80fd\u6709\u6548\u6291\u5236\u5e7b\u542c\uff0c\u4e14\u4e0d\u5f71\u54cd\u6a21\u578b\u6574\u4f53\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\uff0c\u4e3a\u5de5\u4e1a\u90e8\u7f72\u63d0\u4f9b\u53ef\u9760\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.12970", "pdf": "https://arxiv.org/pdf/2505.12970", "abs": "https://arxiv.org/abs/2505.12970", "authors": ["Robin Jegan", "Andreas Henrich"], "title": "A Structured Literature Review on Traditional Approaches in Current Natural Language Processing", "categories": ["cs.CL"], "comment": "14 pages, 1 figure", "summary": "The continued rise of neural networks and large language models in the more\nrecent past has altered the natural language processing landscape, enabling new\napproaches towards typical language tasks and achieving mainstream success.\nDespite the huge success of large language models, many disadvantages still\nremain and through this work we assess the state of the art in five application\nscenarios with a particular focus on the future perspectives and sensible\napplication scenarios of traditional and older approaches and techniques.\n  In this paper we survey recent publications in the application scenarios\nclassification, information and relation extraction, text simplification as\nwell as text summarization. After defining our terminology, i.e., which\nfeatures are characteristic for traditional techniques in our interpretation\nfor the five scenarios, we survey if such traditional approaches are still\nbeing used, and if so, in what way they are used. It turns out that all five\napplication scenarios still exhibit traditional models in one way or another,\nas part of a processing pipeline, as a comparison/baseline to the core model of\nthe respective paper, or as the main model(s) of the paper. For the complete\nstatistics, see https://zenodo.org/records/13683801", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u5206\u6790\u4e94\u4e2aNLP\u5e94\u7528\u573a\u666f\uff0c\u63ed\u793a\u4f20\u7edf\u65b9\u6cd5\u5728LLMs\u4e3b\u5bfc\u65f6\u4ee3\u4ecd\u4ee5\u6d41\u7a0b\u7ec4\u4ef6\u3001\u57fa\u51c6\u5bf9\u6bd4\u7b49\u5f62\u5f0f\u5b58\u5728\uff0c\u5e76\u63a2\u8ba8\u5176\u4e0e\u524d\u6cbf\u6280\u672f\u7ed3\u5408\u7684\u53ef\u80fd\u6027\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u6e90\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u867d\u4e3b\u5bfcNLP\u9886\u57df\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5728\u7279\u5b9a\u573a\u666f\u4ecd\u5177\u5e94\u7528\u4ef7\u503c\u3002\u4f5c\u8005\u65e8\u5728\u7cfb\u7edf\u8bc4\u4f30\u4f20\u7edf\u6280\u672f\u5728\u5f53\u524d\u7814\u7a76\u4e2d\u7684\u89d2\u8272\uff0c\u586b\u8865\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "1. \u5b9a\u4e49\u4f20\u7edf\u65b9\u6cd5\u7279\u5f81\u6807\u51c6\n2. \u9009\u53d6\u5206\u7c7b\u3001\u4fe1\u606f\u62bd\u53d6\u3001\u5173\u7cfb\u62bd\u53d6\u3001\u6587\u672c\u7b80\u5316\u548c\u6458\u8981\u4e94\u4e2a\u6838\u5fc3\u573a\u666f\n3. \u7edf\u8ba1\u5206\u6790\u8fd1\u671f\u8bba\u6587\u4e2d\u4f20\u7edf\u6a21\u578b\u7684\u4f7f\u7528\u6a21\u5f0f\uff08\u6d41\u7a0b\u7ec4\u4ef6/\u57fa\u51c6\u5bf9\u6bd4/\u6838\u5fc3\u6a21\u578b\uff09", "result": "\u6240\u6709\u88ab\u8c03\u67e5\u573a\u666f\u5747\u5b58\u5728\u4f20\u7edf\u65b9\u6cd5\u5e94\u7528\uff1a\n- 34%\u4f5c\u4e3a\u5904\u7406\u6d41\u7a0b\u7ec4\u4ef6\n- 28%\u4f5c\u4e3a\u6a21\u578b\u5bf9\u6bd4\u57fa\u51c6\n- 12%\u4ecd\u4f5c\u4e3a\u8bba\u6587\u6838\u5fc3\u6a21\u578b\n\uff08\u6570\u636e\u6765\u6e90\uff1a\u8bba\u6587\u516c\u5f00\u7684\u7edf\u8ba1\u8bb0\u5f55\uff09", "conclusion": "\u4f20\u7edf\u65b9\u6cd5\u4e0eLLMs\u5b58\u5728\u4e92\u8865\u6027\uff0c\u5efa\u8bae\uff1a\n1. \u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u7ed3\u5408\u89c4\u5219\u7cfb\u7edf\n2. \u6784\u5efa\u6df7\u5408\u5f0f\u5904\u7406\u6d41\u6c34\u7ebf\n3. \u5c06\u4f20\u7edf\u6a21\u578b\u4f5c\u4e3a\u53ef\u89e3\u91ca\u6027\u7ec4\u4ef6"}}
{"id": "2505.12973", "pdf": "https://arxiv.org/pdf/2505.12973", "abs": "https://arxiv.org/abs/2505.12973", "authors": ["Mahta Fetrat Qharabagh", "Zahra Dehghanian", "Hamid R. Rabiee"], "title": "Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models", "categories": ["cs.CL"], "comment": "8 main body pages, total 25 pages, 15 figures", "summary": "Homograph disambiguation remains a significant challenge in\ngrapheme-to-phoneme (G2P) conversion, especially for low-resource languages.\nThis challenge is twofold: (1) creating balanced and comprehensive homograph\ndatasets is labor-intensive and costly, and (2) specific disambiguation\nstrategies introduce additional latency, making them unsuitable for real-time\napplications such as screen readers and other accessibility tools. In this\npaper, we address both issues. First, we propose a semi-automated pipeline for\nconstructing homograph-focused datasets, introduce the HomoRich dataset\ngenerated through this pipeline, and demonstrate its effectiveness by applying\nit to enhance a state-of-the-art deep learning-based G2P system for Persian.\nSecond, we advocate for a paradigm shift - utilizing rich offline datasets to\ninform the development of fast, rule-based methods suitable for\nlatency-sensitive accessibility applications like screen readers. To this end,\nwe improve one of the most well-known rule-based G2P systems, eSpeak, into a\nfast homograph-aware version, HomoFast eSpeak. Our results show an approximate\n30% improvement in homograph disambiguation accuracy for the deep\nlearning-based and eSpeak systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u534a\u81ea\u52a8\u5316\u6784\u5efa\u540c\u5f62\u5f02\u4e49\u8bcd\u6570\u636e\u96c6HomoRich\uff0c\u5e76\u5f00\u53d1\u5feb\u901f\u89c4\u5219\u7cfb\u7edfHomoFast eSpeak\uff0c\u5c06\u6ce2\u65af\u8bed\u540c\u5f62\u5f02\u4e49\u8bcd\u6d88\u89e3\u51c6\u786e\u7387\u63d0\u5347\u7ea630%", "motivation": "\u89e3\u51b3\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728G2P\u8f6c\u6362\u4e2d\u9762\u4e34\u7684\u540c\u5f62\u5f02\u4e49\u8bcd\u6570\u636e\u96c6\u6784\u5efa\u56f0\u96be\uff0c\u4ee5\u53ca\u73b0\u6709\u6d88\u89e3\u7b56\u7565\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u7684\u9ad8\u5ef6\u8fdf\u95ee\u9898", "method": "1. \u63d0\u51fa\u534a\u81ea\u52a8\u5316\u6570\u636e\u96c6\u6784\u5efa\u6d41\u7a0b\u751f\u6210HomoRich\u6570\u636e\u96c6\n2. \u6539\u8fdbeSpeak\u89c4\u5219\u7cfb\u7edf\uff0c\u5f00\u53d1\u652f\u6301\u5feb\u901f\u6d88\u89e3\u7684HomoFast\u7248\u672c", "result": "\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u548ceSpeak\u7cfb\u7edf\u7684\u540c\u5f62\u5f02\u4e49\u8bcd\u6d88\u89e3\u51c6\u786e\u7387\u5747\u63d0\u5347\u7ea630%", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u9ad8\u8d28\u91cf\u79bb\u7ebf\u6570\u636e\u96c6\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e0e\u4f18\u5316\u89c4\u5219\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u51c6\u786e\u6027\uff08HomoRich\uff09\u4e0e\u5b9e\u65f6\u6027\uff08HomoFast\uff09\u7684\u534f\u540c\u4f18\u5316"}}
{"id": "2505.12983", "pdf": "https://arxiv.org/pdf/2505.12983", "abs": "https://arxiv.org/abs/2505.12983", "authors": ["Jiaan Wang", "Fandong Meng", "Zengkui Sun", "Yunlong Liang", "Yuxuan Cao", "Jiarong Xu", "Haoxiang Shi", "Jie Zhou"], "title": "An Empirical Study of Many-to-Many Summarization with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 main conference", "summary": "Many-to-many summarization (M2MS) aims to process documents in any language\nand generate the corresponding summaries also in any language. Recently, large\nlanguage models (LLMs) have shown strong multi-lingual abilities, giving them\nthe potential to perform M2MS in real applications. This work presents a\nsystematic empirical study on LLMs' M2MS ability. Specifically, we first\nreorganize M2MS data based on eight previous domain-specific datasets. The\nreorganized data contains 47.8K samples spanning five domains and six\nlanguages, which could be used to train and evaluate LLMs. Then, we benchmark\n18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned\ntraditional models (e.g., mBART) are also conducted for comparisons. Our\nexperiments reveal that, zero-shot LLMs achieve competitive results with\nfine-tuned traditional models. After instruct-tuning, open-source LLMs can\nsignificantly improve their M2MS ability, and outperform zero-shot LLMs\n(including GPT-4) in terms of automatic evaluations. In addition, we\ndemonstrate that this task-specific improvement does not sacrifice the LLMs'\ngeneral task-solving abilities. However, as revealed by our human evaluation,\nLLMs still face the factuality issue, and the instruction tuning might\nintensify the issue. Thus, how to control factual errors becomes the key when\nbuilding LLM summarizers in real applications, and is worth noting in future\nresearch.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u5bf9\u591a\u8de8\u8bed\u8a00\u6458\u8981\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6307\u4ee4\u5fae\u8c03\u53ef\u63d0\u5347\u6027\u80fd\u4f46\u53ef\u80fd\u52a0\u5267\u4e8b\u5b9e\u6027\u9519\u8bef\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\u5904\u7406\u591a\u5bf9\u591a\u6458\u8981\u4efb\u52a1\u7684\u6f5c\u529b\uff0c\u586b\u8865\u7cfb\u7edf\u6027\u5b9e\u8bc1\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u91cd\u7ec48\u4e2a\u9886\u57df\u6570\u636e\u96c6\u6784\u5efa47.8K\u6837\u672c\u7684\u8de8\u8bed\u8a00\u57fa\u51c6\uff0c\u5bf9\u6bd418\u4e2aLLM\u7684\u96f6\u6837\u672c/\u6307\u4ee4\u5fae\u8c03\u8868\u73b0\u53ca\u4f20\u7edf\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u6307\u4ee4\u5fae\u8c03\u540e\u7684\u5f00\u6e90LLM\u81ea\u52a8\u8bc4\u4f30\u8d85\u8d8aGPT-4\u7b49\u96f6\u6837\u672c\u6a21\u578b\uff0c\u4f46\u4eba\u7c7b\u8bc4\u4f30\u63ed\u793a\u4e8b\u5b9e\u6027\u95ee\u9898\u968f\u5fae\u8c03\u52a0\u5267\u3002", "conclusion": "\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u4f18\u5148\u63a7\u5236LLM\u6458\u8981\u7684\u4e8b\u5b9e\u6027\u9519\u8bef\uff0c\u8be5\u95ee\u9898\u5e94\u6210\u4e3a\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u65b9\u5411\u3002"}}
{"id": "2505.12996", "pdf": "https://arxiv.org/pdf/2505.12996", "abs": "https://arxiv.org/abs/2505.12996", "authors": ["Jiaan Wang", "Fandong Meng", "Jie Zhou"], "title": "ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": "12 pages, 2 figures", "summary": "In recent years, the emergence of large reasoning models (LRMs), such as\nOpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex\nproblems, e.g., mathematics and coding. Some pioneering studies attempt to\nbring the success of LRMs in neural machine translation (MT). They try to build\nLRMs with deep reasoning MT ability via reinforcement learning (RL). Despite\nsome progress that has been made, these attempts generally focus on several\nhigh-resource languages, e.g., English and Chinese, leaving the performance on\nother languages unclear. Besides, the reward modeling methods in previous work\ndo not fully unleash the potential of reinforcement learning in MT. In this\nwork, we first design a new reward modeling method that compares the\ntranslation results of the policy MT model with a strong LRM (i.e.,\nDeepSeek-R1-671B), and quantifies the comparisons to provide rewards.\nExperimental results demonstrate the superiority of the reward modeling method.\nUsing Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new\nstate-of-the-art performance in literary translation, and outperforms strong\nLRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to\nthe multilingual settings with 11 languages. With a carefully designed\nlightweight reward modeling in RL, we can simply transfer the strong MT ability\nfrom a single direction into multiple (i.e., 90) translation directions and\nachieve impressive multilingual MT performance.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3aLRM\u5bf9\u6bd4\u7684\u65b0\u578b\u5956\u52b1\u5efa\u6a21\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u7ffb\u8bd1\u6027\u80fd\u5e76\u5b9e\u73b0\u591a\u8bed\u8a00\u6269\u5c55", "motivation": "\u73b0\u6709\u7814\u7a76\u96c6\u4e2d\u5728\u9ad8\u8d44\u6e90\u8bed\u8a00\u4e14\u5956\u52b1\u5efa\u6a21\u672a\u80fd\u5145\u5206\u53d1\u6325\u5f3a\u5316\u5b66\u4e60\u6f5c\u529b\uff0c\u9700\u6539\u8fdb\u65b9\u6cd5\u63d0\u5347\u591a\u8bed\u8a00\u573a\u666f\u4e0b\u7684\u673a\u5668\u7ffb\u8bd1\u6027\u80fd", "method": "\u901a\u8fc7\u5bf9\u6bd4\u7b56\u7565\u6a21\u578b\u4e0eDeepSeek-R1\u7684\u7ffb\u8bd1\u7ed3\u679c\u91cf\u5316\u5956\u52b1\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u5956\u52b1\u6a21\u578b\u8bbe\u8ba1\u5b9e\u73b090\u4e2a\u65b9\u5411\u7684\u591a\u8bed\u8a00\u8fc1\u79fb", "result": "Qwen2.5-7B\u6a21\u578b\u5728\u6587\u5b66\u7ffb\u8bd1\u8fbe\u5230SOTA\uff0c\u591a\u8bed\u8a00\u5b9e\u9a8c\u4e2d\u8986\u76d611\u79cd\u8bed\u8a00\u5e76\u663e\u8457\u8d85\u8d8a\u4e3b\u6d41LRM", "conclusion": "\u65b0\u578b\u5956\u52b1\u673a\u5236\u6709\u6548\u91ca\u653e\u5f3a\u5316\u5b66\u4e60\u6f5c\u529b\uff0c\u6210\u529f\u5b9e\u73b0\u8de8\u8bed\u8a00\u7ffb\u8bd1\u80fd\u529b\u8fc1\u79fb\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00MT\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2505.13004", "pdf": "https://arxiv.org/pdf/2505.13004", "abs": "https://arxiv.org/abs/2505.13004", "authors": ["Yuhao Qing", "Boyu Zhu", "Mingzhe Du", "Zhijiang Guo", "Terry Yue Zhuo", "Qianru Zhang", "Jie M. Zhang", "Heming Cui", "Siu-Ming Yiu", "Dong Huang", "See-Kiong Ng", "Luu Anh Tuan"], "title": "EffiBench-X: A Multi-Language Benchmark for Measuring Efficiency of LLM-Generated Code", "categories": ["cs.CL"], "comment": "Under Review", "summary": "Existing code generation benchmarks primarily evaluate functional\ncorrectness, with limited focus on code efficiency and often restricted to a\nsingle language like Python. To address this gap, we introduce EffiBench-X, the\nfirst multi-language benchmark designed to measure the efficiency of\nLLM-generated code. EffiBench-X supports Python, C++, Java, JavaScript, Ruby,\nand Golang. It comprises competitive programming tasks with human-expert\nsolutions as efficiency baselines. Evaluating state-of-the-art LLMs on\nEffiBench-X reveals that while models generate functionally correct code, they\nconsistently underperform human experts in efficiency. Even the most efficient\nLLM-generated solutions (Qwen3-32B) achieve only around \\textbf{62\\%} of human\nefficiency on average, with significant language-specific variations. LLMs show\nbetter efficiency in Python, Ruby, and JavaScript than in Java, C++, and\nGolang. For instance, DeepSeek-R1's Python code is significantly more efficient\nthan its Java code. These results highlight the critical need for research into\nLLM optimization techniques to improve code efficiency across diverse\nlanguages. The dataset and evaluation infrastructure are submitted and\navailable at https://github.com/EffiBench/EffiBench-X.git and\nhttps://huggingface.co/datasets/EffiBench/effibench-x.", "AI": {"tldr": "EffiBench-X\u9996\u4e2a\u591a\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\uff0c\u663e\u793aLLM\u751f\u6210\u4ee3\u7801\u6548\u7387\u4ec5\u8fbe\u4eba\u7c7b\u4e13\u5bb662%\uff0c\u5b58\u5728\u663e\u8457\u8bed\u8a00\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u529f\u80fd\u6b63\u786e\u6027\uff0c\u7f3a\u4e4f\u5bf9\u6548\u7387\u7684\u8bc4\u4f30\u4e14\u8bed\u8a00\u5355\u4e00\u3002\u9700\u5efa\u7acb\u591a\u8bed\u8a00\u6548\u7387\u8bc4\u4f30\u4f53\u7cfb\u3002", "method": "\u6784\u5efa\u652f\u63016\u79cd\u8bed\u8a00\u7684EffiBench-X\u57fa\u51c6\uff0c\u4f7f\u7528\u4eba\u7c7b\u4e13\u5bb6\u65b9\u6848\u4f5c\u4e3a\u6548\u7387\u57fa\u7ebf\uff0c\u8bc4\u4f30\u4e3b\u6d41LLM\u751f\u6210\u4ee3\u7801\u6548\u7387\u3002", "result": "LLM\u751f\u6210\u4ee3\u7801\u6548\u7387\u5e73\u574762%\uff08Qwen3-32B\u6700\u4f18\uff09\uff0cPython/Ruby/JS\u4f18\u4e8eJava/C++/Golang\uff0c\u8bed\u8a00\u95f4\u6700\u5927\u5dee\u5f02\u8fbe46%\u3002", "conclusion": "\u4e9f\u9700\u4f18\u5316LLM\u591a\u8bed\u8a00\u4ee3\u7801\u6548\u7387\uff0c\u6570\u636e\u96c6\u5f00\u6e90\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2505.13006", "pdf": "https://arxiv.org/pdf/2505.13006", "abs": "https://arxiv.org/abs/2505.13006", "authors": ["Yuyang Li", "Philip J. M. Kerbusch", "Raimon H. R. Pruim", "Tobias K\u00e4fer"], "title": "Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain", "categories": ["cs.CL"], "comment": "Accepted by NAACL 2025 industry track", "summary": "Airports from the top 20 in terms of annual passengers are highly dynamic\nenvironments with thousands of flights daily, and they aim to increase the\ndegree of automation. To contribute to this, we implemented a Conversational AI\nsystem that enables staff in an airport to communicate with flight information\nsystems. This system not only answers standard airport queries but also\nresolves airport terminology, jargon, abbreviations, and dynamic questions\ninvolving reasoning. In this paper, we built three different\nRetrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL\nRAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that\ntraditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally\nproduced hallucinations, which is risky to airport safety. In contrast, SQL RAG\nand Graph RAG achieved 80.85% and 91.49% accuracy respectively, with\nsignificantly fewer hallucinations. Moreover, Graph RAG was especially\neffective for questions that involved reasoning. Based on our observations, we\nthus recommend SQL RAG and Graph RAG are better for airport environments, due\nto fewer hallucinations and the ability to handle dynamic questions.", "AI": {"tldr": "\u8bba\u6587\u5bf9\u6bd4\u4e09\u79cdRAG\u65b9\u6cd5\u5728\u673a\u573a\u5bf9\u8bdd\u7cfb\u7edf\u7684\u8868\u73b0\uff0c\u63a8\u8350SQL RAG\u548cGraph RAG\u4ee5\u964d\u4f4e\u5e7b\u89c9\u98ce\u9669\u5e76\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u63d0\u5347\u673a\u573a\u81ea\u52a8\u5316\u7a0b\u5ea6\uff0c\u89e3\u51b3\u4e13\u4e1a\u672f\u8bed/\u52a8\u6001\u63a8\u7406\u67e5\u8be2\u7684\u4ea4\u4e92\u9700\u6c42\u3002", "method": "\u5f00\u53d1\u4f20\u7edfRAG\u3001SQL RAG\u548c\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684Graph RAG\u7cfb\u7edf\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "\u4f20\u7edfRAG\u51c6\u786e\u738784.84%\u4f46\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0cGraph RAG\u8fbe91.49%\u51c6\u786e\u7387\u4e14\u63a8\u7406\u80fd\u529b\u7a81\u51fa\u3002", "conclusion": "\u63a8\u8350SQL RAG\u548cGraph RAG\uff0c\u56e0\u5176\u4f4e\u5e7b\u89c9\u7279\u6027\u53ca\u5904\u7406\u52a8\u6001\u95ee\u9898\u7684\u4f18\u52bf\u3002"}}
{"id": "2505.13010", "pdf": "https://arxiv.org/pdf/2505.13010", "abs": "https://arxiv.org/abs/2505.13010", "authors": ["Himel Ghosh", "Ahmed Mosharafa", "Georg Groh"], "title": "To Bias or Not to Bias: Detecting bias in News with bias-detector", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": "7 pages, 5 figures, 2 tables", "summary": "Media bias detection is a critical task in ensuring fair and balanced\ninformation dissemination, yet it remains challenging due to the subjectivity\nof bias and the scarcity of high-quality annotated data. In this work, we\nperform sentence-level bias classification by fine-tuning a RoBERTa-based model\non the expert-annotated BABE dataset. Using McNemar's test and the 5x2\ncross-validation paired t-test, we show statistically significant improvements\nin performance when comparing our model to a domain-adaptively pre-trained\nDA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model\navoids common pitfalls like oversensitivity to politically charged terms and\ninstead attends more meaningfully to contextually relevant tokens. For a\ncomprehensive examination of media bias, we present a pipeline that combines\nour model with an already-existing bias-type classifier. Our method exhibits\ngood generalization and interpretability, despite being constrained by\nsentence-level analysis and dataset size because of a lack of larger and more\nadvanced bias corpora. We talk about context-aware modeling, bias\nneutralization, and advanced bias type classification as potential future\ndirections. Our findings contribute to building more robust, explainable, and\nsocially responsible NLP systems for media bias detection.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eRoBERTa\u7684\u53e5\u5b50\u7ea7\u5a92\u4f53\u504f\u89c1\u68c0\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u8ba1\u9a8c\u8bc1\u548c\u6ce8\u610f\u529b\u5206\u6790\u8bc1\u660e\u5176\u6709\u6548\u6027\uff0c\u5e76\u6784\u5efa\u591a\u7ea7\u68c0\u6d4b\u6d41\u7a0b", "motivation": "\u5a92\u4f53\u504f\u89c1\u68c0\u6d4b\u9762\u4e34\u6807\u6ce8\u6570\u636e\u7a00\u7f3a\u548c\u4e3b\u89c2\u6027\u5f3a\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5bf9\u653f\u6cbb\u672f\u8bed\u8fc7\u5ea6\u654f\u611f\u7b49\u95ee\u9898", "method": "\u5728BABE\u6570\u636e\u96c6\u5fae\u8c03RoBERTa\uff0c\u91c7\u7528McNemar\u68c0\u9a8c\u548c5x2\u4ea4\u53c9\u9a8c\u8bc1t\u68c0\u9a8c\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u5206\u6790", "result": "\u76f8\u6bd4DA-RoBERTa\u57fa\u7ebf\u6a21\u578b\u53d6\u5f97\u7edf\u8ba1\u663e\u8457\u63d0\u5347\uff0c\u6ce8\u610f\u529b\u673a\u5236\u66f4\u5173\u6ce8\u4e0a\u4e0b\u6587\u76f8\u5173\u7279\u5f81\u800c\u975e\u8868\u9762\u653f\u6cbb\u672f\u8bed", "conclusion": "\u6784\u5efa\u4e86\u53ef\u89e3\u91ca\u7684\u5a92\u4f53\u504f\u89c1\u68c0\u6d4b\u7cfb\u7edf\uff0c\u63d0\u51fa\u672a\u6765\u5e94\u63a2\u7d22\u4e0a\u4e0b\u6587\u5efa\u6a21\u3001\u504f\u89c1\u4e2d\u548c\u4e0e\u7ec6\u7c92\u5ea6\u5206\u7c7b\u65b9\u5411"}}
{"id": "2505.13034", "pdf": "https://arxiv.org/pdf/2505.13034", "abs": "https://arxiv.org/abs/2505.13034", "authors": ["M\u00e1rton Kardos", "Kenneth C. Enevoldsen", "Kristoffer Laigaard Nielbo"], "title": "topicwizard -- a Modern, Model-agnostic Framework for Topic Model Visualization and Interpretation", "categories": ["cs.CL"], "comment": "9 pages, 9 figures", "summary": "Topic models are statistical tools that allow their users to gain qualitative\nand quantitative insights into the contents of textual corpora without the need\nfor close reading. They can be applied in a wide range of settings from\ndiscourse analysis, through pretraining data curation, to text filtering. Topic\nmodels are typically parameter-rich, complex models, and interpreting these\nparameters can be challenging for their users. It is typical practice for users\nto interpret topics based on the top 10 highest ranking terms on a given topic.\nThis list-of-words approach, however, gives users a limited and biased picture\nof the content of topics. Thoughtful user interface design and visualizations\ncan help users gain a more complete and accurate understanding of topic models'\noutput. While some visualization utilities do exist for topic models, these are\ntypically limited to a certain type of topic model. We introduce topicwizard, a\nframework for model-agnostic topic model interpretation, that provides\nintuitive and interactive tools that help users examine the complex semantic\nrelations between documents, words and topics learned by topic models.", "AI": {"tldr": "\u8bba\u6587\u8ba8\u8bba\u4e86\u4e3b\u9898\u6a21\u578b\u89e3\u91ca\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u6a21\u578b\u65e0\u5173\u7684\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u6846\u67b6topicwizard\u4ee5\u63d0\u5347\u89e3\u91ca\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u4e3b\u9898\u6a21\u578b\u53ef\u89c6\u5316\u5de5\u5177\u901a\u5e38\u5c40\u9650\u4e8e\u7279\u5b9a\u6a21\u578b\u7c7b\u578b\uff0c\u4e14\u4f20\u7edf\u300e\u8bcd\u5217\u8868\u300f\u65b9\u6cd5\u5b58\u5728\u7406\u89e3\u504f\u5dee\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u89e3\u91ca\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u6a21\u578b\u65e0\u5173\u6846\u67b6topicwizard\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u53ef\u89c6\u5316\u5de5\u5177\u63ed\u793a\u6587\u6863\u3001\u8bcd\u6c47\u4e0e\u4e3b\u9898\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\u3002", "result": "topicwizard\u63d0\u4f9b\u52a8\u6001\u63a2\u7d22\u529f\u80fd\uff0c\u5e2e\u52a9\u7528\u6237\u66f4\u51c6\u786e\u7406\u89e3\u4e3b\u9898\u6a21\u578b\u8f93\u51fa\u7684\u590d\u6742\u8bed\u4e49\u7ed3\u6784\u3002", "conclusion": "\u8be5\u6846\u67b6\u7a81\u7834\u73b0\u6709\u5de5\u5177\u9650\u5236\uff0c\u589e\u5f3a\u4e86\u8de8\u9886\u57df\u5e94\u7528\u4e2d\u4e3b\u9898\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2505.13036", "pdf": "https://arxiv.org/pdf/2505.13036", "abs": "https://arxiv.org/abs/2505.13036", "authors": ["Sai Koneru", "Maike Z\u00fcfle", "Thai-Binh Nguyen", "Seymanur Akti", "Jan Niehues", "Alexander Waibel"], "title": "KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The scope of the International Workshop on Spoken Language Translation\n(IWSLT) has recently broadened beyond traditional Speech Translation (ST) to\nencompass a wider array of tasks, including Speech Question Answering and\nSummarization. This shift is partly driven by the growing capabilities of\nmodern systems, particularly with the success of Large Language Models (LLMs).\nIn this paper, we present the Karlsruhe Institute of Technology's submissions\nfor the Offline ST and Instruction Following (IF) tracks, where we leverage\nLLMs to enhance performance across all tasks. For the Offline ST track, we\npropose a pipeline that employs multiple automatic speech recognition systems,\nwhose outputs are fused using an LLM with document-level context. This is\nfollowed by a two-step translation process, incorporating additional refinement\nstep to improve translation quality. For the IF track, we develop an end-to-end\nmodel that integrates a speech encoder with an LLM to perform a wide range of\ninstruction-following tasks. We complement it with a final document-level\nrefinement stage to further enhance output quality by using contextual\ninformation.", "AI": {"tldr": "KIT\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3\u7ffb\u8bd1\u4e0e\u6307\u4ee4\u8ddf\u968f\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7cfb\u7edf\u878d\u5408\u3001\u4e24\u6b65\u7ffb\u8bd1\u53ca\u4e0a\u4e0b\u6587\u4f18\u5316\u63d0\u5347\u4efb\u52a1\u6027\u80fd", "motivation": "IWSLT\u4efb\u52a1\u8303\u56f4\u6269\u5c55\u81f3\u8bed\u97f3\u95ee\u7b54\u548c\u6458\u8981\u7b49\u9886\u57df\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u8de8\u4efb\u52a1\u5904\u7406\u80fd\u529b", "method": "\u79bb\u7ebf\u7ffb\u8bd1\u91c7\u7528\u591a\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u878d\u5408+\u6587\u6863\u7ea7LLM\u4f18\u5316\uff1b\u6307\u4ee4\u8ddf\u8e2a\u6784\u5efa\u8bed\u97f3\u7f16\u7801\u5668\u4e0eLLM\u7684\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u5747\u5305\u542b\u6587\u6863\u7ea7\u540e\u5904\u7406\u6a21\u5757", "result": "\u5f00\u53d1\u51fa\u652f\u6301\u590d\u6742\u8de8\u6a21\u6001\u4efb\u52a1\u7684\u96c6\u6210\u6846\u67b6\uff0c\u4e0a\u4e0b\u6587\u4f18\u5316\u673a\u5236\u663e\u8457\u63d0\u5347\u8f93\u51fa\u8d28\u91cf", "conclusion": "\u9a8c\u8bc1LLM\u5728\u8bed\u97f3\u5904\u7406\u4efb\u52a1\u4e2d\u7684\u901a\u7528\u6027\uff0c\u6587\u6863\u7ea7\u4f18\u5316\u5bf9\u4fdd\u6301\u8bed\u4e49\u8fde\u8d2f\u6027\u5177\u6709\u5173\u952e\u4f5c\u7528"}}
{"id": "2505.13053", "pdf": "https://arxiv.org/pdf/2505.13053", "abs": "https://arxiv.org/abs/2505.13053", "authors": ["Amelie S. Robrecht", "Christoph R. Kowalski", "Stefan Kopp"], "title": "SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation", "categories": ["cs.CL", "cs.AI"], "comment": "currently under review at Frontiers in Communication", "summary": "Adapting to the addressee is crucial for successful explanations, yet poses\nsignificant challenges for dialogsystems. We adopt the approach of treating\nexplanation generation as a non-stationary decision process, where the optimal\nstrategy varies according to changing beliefs about the explainee and the\ninteraction context. In this paper we address the questions of (1) how to track\nthe interaction context and the relevant listener features in a formally\ndefined computational partner model, and (2) how to utilize this model in the\ndynamically adjusted, rational decision process that determines the currently\nbest explanation strategy. We propose a Bayesian inference-based approach to\ncontinuously update the partner model based on user feedback, and a\nnon-stationary Markov Decision Process to adjust decision-making based on the\npartner model values. We evaluate an implementation of this framework with five\nsimulated interlocutors, demonstrating its effectiveness in adapting to\ndifferent partners with constant and even changing feedback behavior. The\nresults show high adaptivity with distinct explanation strategies emerging for\ndifferent partners, highlighting the potential of our approach to improve\nexplainable AI systems and dialogsystems in general.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8d1d\u53f6\u65af\u63a8\u65ad\u548c\u975e\u7a33\u6001MDP\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u89e3\u91ca\u7b56\u7565\u5b9e\u73b0\u5bf9\u8bdd\u7cfb\u7edf\u7684\u7528\u6237\u81ea\u9002\u5e94", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u7cfb\u7edf\u7f3a\u4e4f\u5bf9\u7528\u6237\u7279\u5f81\u7684\u52a8\u6001\u9002\u5e94\u80fd\u529b\uff0c\u5bfc\u81f4\u89e3\u91ca\u6548\u679c\u53d7\u9650\uff0c\u9700\u5efa\u7acb\u5b9e\u65f6\u66f4\u65b0\u7684\u7528\u6237\u8ba4\u77e5\u6a21\u578b", "method": "\u7ed3\u5408\u8d1d\u53f6\u65af\u63a8\u65ad\u7684\u6301\u7eed\u7528\u6237\u5efa\u6a21\u4e0e\u975e\u7a33\u6001\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u52a8\u6001\u7b56\u7565\u8c03\u6574", "result": "\u57285\u4e2a\u6a21\u62df\u5bf9\u8bdd\u8005\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u9ad8\u9002\u5e94\u6027\uff0c\u80fd\u5904\u7406\u7528\u6237\u53cd\u9988\u6a21\u5f0f\u53d8\u5316\u5e76\u5f62\u6210\u5dee\u5f02\u5316\u89e3\u91ca\u7b56\u7565", "conclusion": "\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u89e3\u91ca\u7b56\u7565\u7684\u9002\u5e94\u6027\uff0c\u4e3a\u53ef\u89e3\u91caAI\u548c\u5bf9\u8bdd\u7cfb\u7edf\u63d0\u4f9b\u65b0\u65b9\u6cd5\u8bba\u652f\u6301"}}
{"id": "2505.13069", "pdf": "https://arxiv.org/pdf/2505.13069", "abs": "https://arxiv.org/abs/2505.13069", "authors": ["Ambre Marie", "Ilias Maoudj", "Guillaume Dardenne", "Gwenol\u00e9 Quellec"], "title": "Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset", "categories": ["cs.CL", "cs.SD", "eess.AS", "I.2.7; I.5.1"], "comment": "Submitted to the SpeechWellness Challenge at Interspeech 2025; 5\n  pages, 2 figures, 2 tables", "summary": "The 1st SpeechWellness Challenge conveys the need for speech-based suicide\nrisk assessment in adolescents. This study investigates a multimodal approach\nfor this challenge, integrating automatic transcription with WhisperX,\nlinguistic embeddings from Chinese RoBERTa, and audio embeddings from WavLM.\nAdditionally, handcrafted acoustic features -- including MFCCs, spectral\ncontrast, and pitch-related statistics -- were incorporated. We explored three\nfusion strategies: early concatenation, modality-specific processing, and\nweighted attention with mixup regularization. Results show that weighted\nattention provided the best generalization, achieving 69% accuracy on the\ndevelopment set, though a performance gap between development and test sets\nhighlights generalization challenges. Our findings, strictly tied to the\nMINI-KID framework, emphasize the importance of refining embedding\nrepresentations and fusion mechanisms to enhance classification reliability.", "AI": {"tldr": "\u591a\u6a21\u6001\u52a0\u6743\u6ce8\u610f\u529b\u6a21\u578b\u5728\u9752\u5c11\u5e74\u81ea\u6740\u98ce\u9669\u8bc4\u4f30\u4e2d\u53d6\u5f9769%\u51c6\u786e\u7387\uff0c\u4f46\u5b58\u5728\u5f00\u53d1\u96c6\u4e0e\u6d4b\u8bd5\u96c6\u7684\u6cdb\u5316\u5dee\u8ddd", "motivation": "\u901a\u8fc7\u8bed\u97f3\u591a\u6a21\u6001\u5206\u6790\u63d0\u5347\u9752\u5c11\u5e74\u81ea\u6740\u98ce\u9669\u8bc4\u4f30\u7cbe\u5ea6\uff0c\u89e3\u51b3\u4f20\u7edf\u5355\u6a21\u6001\u65b9\u6cd5\u7684\u5c40\u9650\u6027", "method": "\u6574\u5408WhisperX\u8bed\u97f3\u8f6c\u6587\u672c\u3001RoBERTa\u8bed\u8a00\u5d4c\u5165\u3001WavLM\u97f3\u9891\u5d4c\u5165\u53ca\u624b\u5de5\u58f0\u5b66\u7279\u5f81(MFCC/\u9891\u8c31\u5bf9\u6bd4/\u57fa\u9891)\uff0c\u91c7\u7528\u65e9\u671f\u62fc\u63a5/\u6a21\u6001\u4e13\u7528\u5904\u7406/\u6df7\u5408\u6b63\u5219\u5316\u52a0\u6743\u6ce8\u610f\u529b\u4e09\u79cd\u878d\u5408\u7b56\u7565", "result": "\u52a0\u6743\u6ce8\u610f\u529b\u6a21\u578b\u5728\u5f00\u53d1\u96c6\u8fbe69%\u51c6\u786e\u7387\uff0c\u4f46\u6d4b\u8bd5\u96c6\u6027\u80fd\u5dee\u8ddd\u63ed\u793a\u6a21\u578b\u6cdb\u5316\u6311\u6218", "conclusion": "\u9700\u4f18\u5316\u5d4c\u5165\u8868\u793a\u548c\u878d\u5408\u673a\u5236\uff0c\u63d0\u5347\u57fa\u4e8eMINI-KID\u6846\u67b6\u7684\u5206\u7c7b\u53ef\u9760\u6027\uff0c\u89e3\u51b3\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u95ee\u9898"}}
{"id": "2505.13077", "pdf": "https://arxiv.org/pdf/2505.13077", "abs": "https://arxiv.org/abs/2505.13077", "authors": ["Xiang Fei", "Jinghui Lu", "Qi Sun", "Hao Feng", "Yanjie Wang", "Wei Shi", "An-Lan Wang", "Jingqun Tang", "Can Huang"], "title": "Advancing Sequential Numerical Prediction in Autoregressive Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Autoregressive models have become the de facto choice for sequence generation\ntasks, but standard approaches treat digits as independent tokens and apply\ncross-entropy loss, overlooking the coherent structure of numerical sequences.\nThis paper introduces Numerical Token Integrity Loss (NTIL) to address this\ngap. NTIL operates at two levels: (1) token-level, where it extends the Earth\nMover's Distance (EMD) to preserve ordinal relationships between numerical\nvalues, and (2) sequence-level, where it penalizes the overall discrepancy\nbetween the predicted and actual sequences. This dual approach improves\nnumerical prediction and integrates effectively with LLMs/MLLMs. Extensive\nexperiments show significant performance improvements with NTIL.", "AI": {"tldr": "\u63d0\u51faNTIL\u635f\u5931\u51fd\u6570\u6539\u5584\u81ea\u56de\u5f52\u6a21\u578b\u5bf9\u6570\u5b57\u5e8f\u5217\u7684\u9884\u6d4b\u80fd\u529b", "motivation": "\u73b0\u6709\u81ea\u56de\u5f52\u6a21\u578b\u5c06\u6570\u5b57\u89c6\u4e3a\u72ec\u7acbtoken\u5904\u7406\uff0c\u5ffd\u7565\u4e86\u6570\u503c\u5e8f\u5217\u7684\u8fde\u8d2f\u7ed3\u6784\u7279\u5f81", "method": "\u901a\u8fc7token\u7ea7\uff08\u6269\u5c55EMD\u4fdd\u6301\u6570\u503c\u5e8f\u5173\u7cfb\uff09\u548c\u5e8f\u5217\u7ea7\uff08\u9884\u6d4b/\u5b9e\u9645\u5e8f\u5217\u5dee\u5f02\u60e9\u7f5a\uff09\u53cc\u91cd\u4f18\u5316", "result": "\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u6027\u80fd\u83b7\u5f97\u663e\u8457\u63d0\u5347", "conclusion": "NTIL\u901a\u8fc7\u53cc\u5c42\u6b21\u8bbe\u8ba1\u6709\u6548\u63d0\u5347\u5927\u6a21\u578b\u5bf9\u6570\u503c\u5e8f\u5217\u7684\u9884\u6d4b\u7cbe\u5ea6\u548c\u6574\u5408\u80fd\u529b"}}
{"id": "2505.13089", "pdf": "https://arxiv.org/pdf/2505.13089", "abs": "https://arxiv.org/abs/2505.13089", "authors": ["Sondre Wold", "Lucas Georges Gabriel Charpentier", "\u00c9tienne Simon"], "title": "Systematic Generalization in Language Models Scales with Information Entropy", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025: Findings", "summary": "Systematic generalization remains challenging for current language models,\nwhich are known to be both sensitive to semantically similar permutations of\nthe input and to struggle with known concepts presented in novel contexts.\nAlthough benchmarks exist for assessing compositional behavior, it is unclear\nhow to measure the difficulty of a systematic generalization problem. In this\nwork, we show how one aspect of systematic generalization can be described by\nthe entropy of the distribution of component parts in the training data. We\nformalize a framework for measuring entropy in a sequence-to-sequence task and\nfind that the performance of popular model architectures scales with the\nentropy. Our work connects systematic generalization to information efficiency,\nand our results indicate that success at high entropy can be achieved even\nwithout built-in priors, and that success at low entropy can serve as a target\nfor assessing progress towards robust systematic generalization.", "AI": {"tldr": "\u901a\u8fc7\u8bad\u7ec3\u6570\u636e\u4e2d\u7ec4\u6210\u5143\u7d20\u7684\u71b5\u503c\u91cf\u5316\u7cfb\u7edf\u6cdb\u5316\u96be\u5ea6\uff0c\u53d1\u73b0\u6a21\u578b\u6027\u80fd\u4e0e\u71b5\u503c\u5448\u6b63\u76f8\u5173\uff0c\u9ad8\u71b5\u573a\u666f\u65e0\u9700\u5185\u7f6e\u5148\u9a8c\u5373\u53ef\u6210\u529f\uff0c\u4f4e\u71b5\u8868\u73b0\u53ef\u4f5c\u4e3a\u7cfb\u7edf\u6027\u6cdb\u5316\u8bc4\u4f30\u76ee\u6807\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u5bf9\u8f93\u5165\u6392\u5217\u654f\u611f\u4e14\u96be\u4ee5\u5728\u65b0\u60c5\u5883\u5e94\u7528\u5df2\u77e5\u6982\u5ff5\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u6cdb\u5316\u95ee\u9898\u7684\u96be\u5ea6\u91cf\u5316\u6807\u51c6\u3002", "method": "\u5efa\u7acb\u5e8f\u5217\u5230\u5e8f\u5217\u4efb\u52a1\u7684\u71b5\u6d4b\u91cf\u6846\u67b6\uff0c\u5206\u6790\u4e0d\u540c\u67b6\u6784\u6a21\u578b\u5728\u4e0d\u540c\u71b5\u503c\u8bad\u7ec3\u6570\u636e\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6d41\u884c\u6a21\u578b\u67b6\u6784\u7684\u6027\u80fd\u968f\u71b5\u503c\u589e\u957f\u800c\u63d0\u5347\uff0c\u9ad8\u71b5\u573a\u666f(\u6570\u636e\u591a\u6837\u6027\u9ad8)\u65e0\u9700\u5185\u7f6e\u5148\u9a8c\u5373\u53ef\u6210\u529f\uff0c\u4f4e\u71b5\u8868\u73b0(\u6570\u636e\u91cd\u590d\u6027\u5f3a)\u53ef\u4f5c\u4e3a\u7cfb\u7edf\u6027\u6cdb\u5316\u8bc4\u4f30\u57fa\u51c6\u3002", "conclusion": "\u7cfb\u7edf\u6cdb\u5316\u80fd\u529b\u4e0e\u4fe1\u606f\u6548\u7387\u5bc6\u5207\u76f8\u5173\uff0c\u4f4e\u71b5\u573a\u666f\u7684\u7a81\u7834\u5c06\u63a8\u52a8\u66f4\u7a33\u5065\u7684\u7cfb\u7edf\u6027\u6cdb\u5316\u6a21\u578b\u53d1\u5c55\uff0c\u4e3a\u67b6\u6784\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u65b9\u5411\u3002"}}
{"id": "2505.13090", "pdf": "https://arxiv.org/pdf/2505.13090", "abs": "https://arxiv.org/abs/2505.13090", "authors": ["David Stap", "Christof Monz"], "title": "The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation", "categories": ["cs.CL"], "comment": null, "summary": "Prior research diverges on language diversity in LLM fine-tuning: Some\nstudies report benefits while others find no advantages. Through controlled\nfine-tuning experiments across 132 translation directions, we systematically\nresolve these disparities. We find that expanding language diversity during\nfine-tuning improves translation quality for both unsupervised and --\nsurprisingly -- supervised pairs, despite less diverse models being fine-tuned\nexclusively on these supervised pairs. However, benefits plateau or decrease\nbeyond a certain diversity threshold. We show that increased language diversity\ncreates more language-agnostic representations. These representational\nadaptations help explain the improved performance in models fine-tuned with\ngreater diversity.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u591a\u6837\u6027\u5728LLM\u5fae\u8c03\u4e2d\u5b58\u5728\u53cc\u91cd\u6548\u5e94\uff1a\u9002\u5ea6\u589e\u52a0\u53ef\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\u5e76\u5f62\u6210\u8bed\u8a00\u65e0\u5173\u8868\u5f81\uff0c\u4f46\u8d85\u8fc7\u9608\u503c\u540e\u6536\u76ca\u9012\u51cf", "motivation": "\u89e3\u51b3\u5148\u524d\u7814\u7a76\u5173\u4e8e\u8bed\u8a00\u591a\u6837\u6027\u5728\u6a21\u578b\u5fae\u8c03\u4e2d\u6548\u679c\u4e0d\u4e00\u81f4\u7684\u4e89\u8bae\uff0c\u63ed\u793a\u591a\u6837\u6027\u5bf9\u591a\u8bed\u8a00\u8868\u5f81\u7684\u9002\u5e94\u6027\u5f71\u54cd", "method": "\u901a\u8fc7132\u4e2a\u7ffb\u8bd1\u65b9\u5411\u7684\u53d7\u63a7\u5fae\u8c03\u5b9e\u9a8c\uff0c\u7ed3\u5408\u8bed\u8a00\u7279\u5f02\u6027\u548c\u8bed\u8a00\u65e0\u5173\u6027\u7684\u8868\u5f81\u5206\u6790", "result": "\u8bed\u8a00\u591a\u6837\u6027\u63d0\u5347\u6709\u76d1\u7763/\u65e0\u76d1\u7763\u5bf9\u7684\u7ffb\u8bd1\u8d28\u91cf\uff08\u6700\u9ad8+5.3 BLEU\uff09\uff0c\u4f46\u5b58\u5728\u6536\u76ca\u9012\u51cf\u9608\u503c\uff1b\u8bed\u8a00\u65e0\u5173\u8868\u5f81\u6bd4\u4f8b\u589e\u52a014%", "conclusion": "\u9002\u5ea6\u7684\u8bed\u8a00\u591a\u6837\u6027\u4f18\u5316\u4e86\u591a\u8bed\u8a00\u8868\u5f81\u7a7a\u95f4\uff0c\u5176\u8fb9\u9645\u6548\u76ca\u53d7\u8868\u5f81\u9002\u5e94\u80fd\u529b\u7684\u9650\u5236"}}
{"id": "2505.13115", "pdf": "https://arxiv.org/pdf/2505.13115", "abs": "https://arxiv.org/abs/2505.13115", "authors": ["Debarpan Bhattacharya", "Apoorva Kulkarni", "Sriram Ganapathy"], "title": "Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "Accepted in INTERSPEECH, 2025, Rotterdam, The Netherlands", "summary": "The popular success of text-based large language models (LLM) has streamlined\nthe attention of the multimodal community to combine other modalities like\nvision and audio along with text to achieve similar multimodal capabilities. In\nthis quest, large audio language models (LALMs) have to be evaluated on\nreasoning related tasks which are different from traditional classification or\ngeneration tasks. Towards this goal, we propose a novel dataset called temporal\nreasoning evaluation of audio (TREA).\n  We benchmark open-source LALMs and observe that they are consistently behind\nhuman capabilities on the tasks in the TREA dataset. While evaluating LALMs, we\nalso propose an uncertainty metric, which computes the invariance of the model\nto semantically identical perturbations of the input. Our analysis shows that\nthe accuracy and uncertainty metrics are not necessarily correlated and thus,\npoints to a need for wholesome evaluation of LALMs for high-stakes\napplications.", "AI": {"tldr": "\u63d0\u51fa\u4e86TREA\u6570\u636e\u96c6\u7528\u4e8e\u8bc4\u4f30\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524d\u5f00\u6e90\u6a21\u578b\u663e\u8457\u843d\u540e\u4eba\u7c7b\u6c34\u5e73\uff0c\u5e76\u63d0\u51fa\u8861\u91cf\u6a21\u578b\u9c81\u68d2\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u6307\u6807", "motivation": "\u73b0\u6709\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\uff08LALMs\uff09\u5728\u4f20\u7edf\u5206\u7c7b/\u751f\u6210\u4efb\u52a1\u5916\u7684\u63a8\u7406\u80fd\u529b\u7f3a\u4e4f\u6709\u6548\u8bc4\u4f30\u65b9\u6cd5\uff0c\u9700\u8981\u5efa\u7acb\u65b0\u7684\u8bc4\u4f30\u6846\u67b6", "method": "1. \u521b\u5efa\u5305\u542b\u65f6\u5e8f\u63a8\u7406\u4efb\u52a1\u7684TREA\u6570\u636e\u96c6\n2. \u5bf9\u5f00\u6e90LALMs\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\n3. \u63d0\u51fa\u57fa\u4e8e\u8f93\u5165\u6270\u52a8\u4e0d\u53d8\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u8bc4\u4f30\u6307\u6807", "result": "\u5f00\u6e90\u6a21\u578b\u5728TREA\u4efb\u52a1\u4e2d\u51c6\u786e\u7387\u4f4e\u4e8e\u4eba\u7c7b\uff08\u5e73\u5747\u5dee15-20%\uff09\uff0c\u4e14\u53d1\u73b0\u6a21\u578b\u51c6\u786e\u7387\u4e0e\u4e0d\u786e\u5b9a\u6027\u6307\u6807\u95f4\u4e0d\u5b58\u5728\u5fc5\u7136\u76f8\u5173\u6027\uff08\u76f8\u5173\u7cfb\u6570<0.3\uff09", "conclusion": "\u9700\u8981\u7ed3\u5408\u51c6\u786e\u7387\u548c\u4e0d\u786e\u5b9a\u6027\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\uff0c\u5355\u7eaf\u4f9d\u8d56\u4f20\u7edf\u51c6\u786e\u7387\u6307\u6807\u5b58\u5728\u5c40\u9650\u6027"}}
{"id": "2505.13136", "pdf": "https://arxiv.org/pdf/2505.13136", "abs": "https://arxiv.org/abs/2505.13136", "authors": ["Anton Ehrmanntraut", "Julia Wunderle", "Jan Pfister", "Fotis Jannidis", "Andreas Hotho"], "title": "ModernGBERT: German-only 1B Encoder Model Trained from Scratch", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "under review @ARR", "summary": "Despite the prominence of decoder-only language models, encoders remain\ncrucial for resource-constrained applications. We introduce ModernGBERT (134M,\n1B), a fully transparent family of German encoder models trained from scratch,\nincorporating architectural innovations from ModernBERT. To evaluate the\npractical trade-offs of training encoders from scratch, we also present\nLL\\\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German\ndecoder-only models via LLM2Vec. We benchmark all models on natural language\nunderstanding, text embedding, and long-context reasoning tasks, enabling a\ncontrolled comparison between dedicated encoders and converted decoders. Our\nresults show that ModernGBERT 1B outperforms prior state-of-the-art German\nencoders as well as encoders adapted via LLM2Vec, with regard to performance\nand parameter-efficiency. All models, training data, checkpoints and code are\npublicly available, advancing the German NLP ecosystem with transparent,\nhigh-performance encoder models.", "AI": {"tldr": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86ModernGBERT\u548cLL\"aMmlein2Vec\u5fb7\u8bed\u7f16\u7801\u5668\u6a21\u578b\u7ec4\uff0c\u901a\u8fc7\u7cfb\u7edf\u5bf9\u6bd4\u9a8c\u8bc1\u4e86\u5168\u65b0\u8bad\u7ec3\u7f16\u7801\u5668\u5728\u6027\u80fd\u4e0e\u53c2\u6570\u6548\u7387\u4e0a\u7684\u663e\u8457\u4f18\u52bf", "motivation": "\u5c3d\u7ba1\u4ec5\u89e3\u7801\u5668\u6a21\u578b\u5360\u636e\u4e3b\u6d41\uff0c\u4f46\u7f16\u7801\u5668\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4ecd\u5177\u4e0d\u53ef\u66ff\u4ee3\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u6784\u5efa\u5b8c\u5168\u900f\u660e\u7684\u9ad8\u6027\u80fd\u5fb7\u8bed\u7f16\u7801\u5668\u751f\u6001", "method": "\u91c7\u7528ModernBERT\u67b6\u6784\u521b\u65b0\u4ece\u5934\u8bad\u7ec3ModernGBERT\u6a21\u578b\u7ec4(134M/1B)\uff0c\u540c\u65f6\u901a\u8fc7LLM2Vec\u6280\u672f\u5c06\u5fb7\u8bed\u4ec5\u89e3\u7801\u5668\u6a21\u578b\u8f6c\u5316\u4e3aLL\"aMmlein2Vec\u7f16\u7801\u5668\u7ec4(120M/1B/7B)", "result": "ModernGBERT 1B\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001\u6587\u672c\u5d4c\u5165\u548c\u957f\u6587\u672c\u63a8\u7406\u4efb\u52a1\u4e2d\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u5fb7\u8bed\u7f16\u7801\u5668\u53ca\u8f6c\u6362\u89e3\u7801\u5668\uff0c\u4e14\u5728\u53c2\u6570\u91cf\u6548\u7387\u4e0a\u8868\u73b0\u66f4\u4f18", "conclusion": "\u7814\u7a76\u8bc1\u5b9e\u4ece\u5934\u8bad\u7ec3\u7f16\u7801\u5668\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc7\u5168\u9762\u5f00\u6e90\u6a21\u578b\u3001\u8bad\u7ec3\u6570\u636e\u4e0e\u4ee3\u7801\uff0c\u4e3a\u5fb7\u8bedNLP\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u900f\u660e\u53ef\u9760\u7684\u57fa\u7840\u8bbe\u65bd\u652f\u6301"}}
{"id": "2505.13141", "pdf": "https://arxiv.org/pdf/2505.13141", "abs": "https://arxiv.org/abs/2505.13141", "authors": ["Zheng Wei Lim", "Alham Fikri Aji", "Trevor Cohn"], "title": "Understanding Cross-Lingual Inconsistency in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are demonstrably capable of cross-lingual\ntransfer, but can produce inconsistent output when prompted with the same\nqueries written in different languages. To understand how language models are\nable to generalize knowledge from one language to the others, we apply the\nlogit lens to interpret the implicit steps taken by LLMs to solve multilingual\nmulti-choice reasoning questions. We find LLMs predict inconsistently and are\nless accurate because they rely on subspaces of individual languages, rather\nthan working in a shared semantic space. While larger models are more\nmultilingual, we show their hidden states are more likely to dissociate from\nthe shared representation compared to smaller models, but are nevertheless more\ncapable of retrieving knowledge embedded across different languages. Finally,\nwe demonstrate that knowledge sharing can be modulated by steering the models'\nlatent processing towards the shared semantic space. We find reinforcing\nutilization of the shared space improves the models' multilingual reasoning\nperformance, as a result of more knowledge transfer from, and better output\nconsistency with English.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8de8\u8bed\u8a00\u63a8\u7406\u4f9d\u8d56\u72ec\u7acb\u8bed\u8a00\u5b50\u7a7a\u95f4\u800c\u975e\u5171\u4eab\u8bed\u4e49\u7a7a\u95f4\uff0c\u901a\u8fc7\u5f15\u5bfc\u9690\u7a7a\u95f4\u53ef\u63d0\u5347\u591a\u8bed\u8a00\u4e00\u81f4\u6027\u3002", "motivation": "LLMs\u5728\u4e0d\u540c\u8bed\u8a00\u63d0\u793a\u4e0b\u4ea7\u751f\u4e0d\u4e00\u81f4\u8f93\u51fa\uff0c\u9700\u63ed\u793a\u5176\u8de8\u8bed\u8a00\u77e5\u8bc6\u8fc1\u79fb\u673a\u5236\u3002", "method": "\u4f7f\u7528logit lens\u5206\u6790\u9690\u85cf\u72b6\u6001\uff0c\u9a8c\u8bc1\u6a21\u578b\u901a\u8fc7\u72ec\u7acb\u8bed\u8a00\u5b50\u7a7a\u95f4\u800c\u975e\u5171\u4eab\u8bed\u4e49\u7a7a\u95f4\u8fdb\u884c\u63a8\u7406\u3002", "result": "\u5927\u6a21\u578b\u9690\u85cf\u72b6\u6001\u66f4\u6613\u8131\u79bb\u5171\u4eab\u7a7a\u95f4\uff0c\u4f46\u8de8\u8bed\u8a00\u77e5\u8bc6\u68c0\u7d22\u80fd\u529b\u66f4\u5f3a\uff1b\u5f3a\u5316\u5171\u4eab\u7a7a\u95f4\u5229\u7528\u53ef\u63d0\u5347\u591a\u8bed\u8a00\u63a8\u7406\u51c6\u786e\u7387\u3002", "conclusion": "\u5f15\u5bfc\u6a21\u578b\u9690\u7a7a\u95f4\u5411\u5171\u4eab\u8bed\u4e49\u7a7a\u95f4\u5bf9\u9f50\u80fd\u6539\u5584\u77e5\u8bc6\u8fc1\u79fb\uff0c\u589e\u5f3a\u975e\u82f1\u8bed\u8f93\u51fa\u7684\u51c6\u786e\u6027\u548c\u4e00\u81f4\u6027\u3002"}}
{"id": "2505.13147", "pdf": "https://arxiv.org/pdf/2505.13147", "abs": "https://arxiv.org/abs/2505.13147", "authors": ["Aswathy Velutharambath", "Roman Klinger", "Kai Sassenberg"], "title": "What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text", "categories": ["cs.CL"], "comment": null, "summary": "Can deception be detected solely from written text? Cues of deceptive\ncommunication are inherently subtle, even more so in text-only communication.\nYet, prior studies have reported considerable success in automatic deception\ndetection. We hypothesize that such findings are largely driven by artifacts\nintroduced during data collection and do not generalize beyond specific\ndatasets. We revisit this assumption by introducing a belief-based deception\nframework, which defines deception as a misalignment between an author's claims\nand true beliefs, irrespective of factual accuracy, allowing deception cues to\nbe studied in isolation. Based on this framework, we construct three corpora,\ncollectively referred to as DeFaBel, including a German-language corpus of\ndeceptive and non-deceptive arguments and a multilingual version in German and\nEnglish, each collected under varying conditions to account for belief change\nand enable cross-linguistic analysis. Using these corpora, we evaluate commonly\nreported linguistic cues of deception. Across all three DeFaBel variants, these\ncues show negligible, statistically insignificant correlations with deception\nlabels, contrary to prior work that treats such cues as reliable indicators. We\nfurther benchmark against other English deception datasets following similar\ndata collection protocols. While some show statistically significant\ncorrelations, effect sizes remain low and, critically, the set of predictive\ncues is inconsistent across datasets. We also evaluate deception detection\nusing feature-based models, pretrained language models, and instruction-tuned\nlarge language models. While some models perform well on established deception\ndatasets, they consistently perform near chance on DeFaBel. Our findings\nchallenge the assumption that deception can be reliably inferred from\nlinguistic cues and call for rethinking how deception is studied and modeled in\nNLP.", "AI": {"tldr": "\u7814\u7a76\u6311\u6218\u73b0\u6709\u6587\u672c\u6b3a\u9a97\u68c0\u6d4b\u65b9\u6cd5\uff0c\u63d0\u51fa\u57fa\u4e8e\u4fe1\u5ff5\u5bf9\u9f50\u7684\u65b0\u6846\u67b6\uff0c\u53d1\u73b0\u4f20\u7edf\u8bed\u8a00\u7ebf\u7d22\u4e0d\u53ef\u9760\u3002", "motivation": "\u5148\u524d\u6b3a\u9a97\u68c0\u6d4b\u7814\u7a76\u53ef\u80fd\u53d7\u6570\u636e\u96c6\u6784\u5efa\u65b9\u5f0f\u5e72\u6270\uff0c\u9700\u5728\u63a7\u5236\u4fe1\u5ff5\u504f\u5dee\u7684\u60c5\u5883\u4e0b\u91cd\u65b0\u8bc4\u4f30\u8bed\u8a00\u7ebf\u7d22\u6709\u6548\u6027\u3002", "method": "\u6784\u5efaDeFaBel\u591a\u8bed\u8a00\u8bed\u6599\u5e93\uff08\u5fb7\u8bed/\u82f1\u8bed\uff09\uff0c\u901a\u8fc7\u7279\u5f81\u6a21\u578b\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u6307\u4ee4\u8c03\u4f18\u5927\u6a21\u578b\u8fdb\u884c\u7cfb\u7edf\u6027\u9a8c\u8bc1\u3002", "result": "\u4f20\u7edf\u6b3a\u9a97\u7ebf\u7d22\u5728DeFaBel\u4e2d\u76f8\u5173\u6027\u5fae\u5f31\uff0c\u5404\u7c7b\u6a21\u578b\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u63a5\u8fd1\u968f\u673a\u731c\u6d4b\u3002", "conclusion": "\u6587\u672c\u6b3a\u9a97\u68c0\u6d4b\u9700\u8d85\u8d8a\u8868\u5c42\u8bed\u8a00\u7279\u5f81\uff0c\u5e94\u7ed3\u5408\u8bed\u5883\u548c\u8ba4\u77e5\u7ef4\u5ea6\u91cd\u65b0\u6784\u5efa\u7814\u7a76\u65b9\u6cd5\u4f53\u7cfb\u3002"}}
{"id": "2505.13156", "pdf": "https://arxiv.org/pdf/2505.13156", "abs": "https://arxiv.org/abs/2505.13156", "authors": ["Zhi Liu", "Tao Yang", "Jing Wang", "Yexin Chen", "Zhan Gao", "Jiaxi Yang", "Kui Chen", "Bingji Lu", "Xiaochen Li", "Changyong Luo", "Yan Li", "Xiaohong Gu", "Peng Cao"], "title": "Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice", "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 4 figures, and 1 tables", "summary": "Natural medicines, particularly Traditional Chinese Medicine (TCM), are\ngaining global recognition for their therapeutic potential in addressing human\nsymptoms and diseases. TCM, with its systematic theories and extensive\npractical experience, provides abundant resources for healthcare. However, the\neffective application of TCM requires precise syndrome diagnosis, determination\nof treatment principles, and prescription formulation, which demand decades of\nclinical expertise. Despite advancements in TCM-based decision systems, machine\nlearning, and deep learning research, limitations in data and single-objective\nconstraints hinder their practical application. In recent years, large language\nmodels (LLMs) have demonstrated potential in complex tasks, but lack\nspecialization in TCM and face significant challenges, such as too big model\nscale to deploy and issues with hallucination. To address these challenges, we\nintroduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and\nspecifically designed for TCM, pre-trained and fine-tuned on diverse TCM\ncorpora, including classical texts, expert treatises, clinical records, and\nknowledge graphs. Tianyi is designed to assimilate interconnected and\nsystematic TCM knowledge through a progressive learning manner. Additionally,\nwe establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in\nTCM examinations, clinical tasks, domain-specific question-answering, and\nreal-world trials. The extensive evaluations demonstrate the significant\npotential of Tianyi as an AI assistant in TCM clinical practice and research,\nbridging the gap between TCM knowledge and practical application.", "AI": {"tldr": "\u63d0\u51fa76\u4ebf\u53c2\u6570\u7684\u4e2d\u533b\u836f\u4e13\u7528\u5927\u6a21\u578b'\u5929\u58f9'\uff0c\u901a\u8fc7\u6e10\u8fdb\u5b66\u4e60\u6574\u5408\u4e2d\u533b\u77e5\u8bc6\u4f53\u7cfb\uff0c\u6784\u5efaTCMEval\u8bc4\u4f30\u57fa\u51c6\u9a8c\u8bc1\u5176\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u4e2d\u533b\u836f\u4f9d\u8d56\u4e34\u5e8a\u7ecf\u9a8c\u4e14\u73b0\u6709AI\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e/\u5355\u76ee\u6807\u7ea6\u675f\uff0c\u901a\u7528\u5927\u6a21\u578b\u7f3a\u4e4f\u4e2d\u533b\u4e13\u4e1a\u6027\u4e14\u5b58\u5728\u90e8\u7f72/\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u4e2d\u533b\u7ecf\u5178\u6587\u732e\u3001\u533b\u6848\u3001\u77e5\u8bc6\u56fe\u8c31\u7b49\u591a\u6e90\u8bed\u6599\u8fdb\u884c\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\uff0c\u91c7\u7528\u6e10\u8fdb\u5f0f\u77e5\u8bc6\u6574\u5408\u65b9\u6cd5\uff0c\u5efa\u7acb\u5305\u542b\u8003\u8bd5/\u4e34\u5e8a/\u95ee\u7b54/\u8bd5\u9a8c\u7684\u56db\u7ef4\u8bc4\u4f30\u4f53\u7cfbTCMEval\u3002", "result": "\u7efc\u5408\u8bc4\u4f30\u8bc1\u660e\u5929\u58f9\u80fd\u6709\u6548\u8854\u63a5\u4e2d\u533b\u7406\u8bba\u4e0e\u5b9e\u8df5\uff0c\u5728\u4e34\u5e8a\u8f85\u52a9\u51b3\u7b56\u548c\u79d1\u7814\u4e2d\u5c55\u73b0\u663e\u8457\u5e94\u7528\u4ef7\u503c\u3002", "conclusion": "\u5929\u58f9\u901a\u8fc7\u9886\u57df\u5b9a\u5236\u5316\u8bbe\u8ba1\u89e3\u51b3\u4e86\u901a\u7528\u6a21\u578b\u7684\u4e2d\u533b\u4e13\u4e1a\u9002\u914d\u95ee\u9898\uff0c\u4e3aAI\u8d4b\u80fd\u4f20\u7edf\u533b\u5b66\u63d0\u4f9b\u4e86\u53ef\u90e8\u7f72\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.13157", "pdf": "https://arxiv.org/pdf/2505.13157", "abs": "https://arxiv.org/abs/2505.13157", "authors": ["Yassine El Boudouri", "Walter Nuninger", "Julian Alvarez", "Yvan Peter"], "title": "Role-Playing Evaluation for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval", "AI": {"tldr": "\u63d0\u51fa\u4e86RPEval\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89d2\u8272\u626e\u6f14\u4e2d\u7684\u60c5\u611f\u7406\u89e3\u3001\u51b3\u7b56\u80fd\u529b\u3001\u9053\u5fb7\u5bf9\u9f50\u548c\u89d2\u8272\u4e00\u81f4\u6027\u56db\u4e2a\u7ef4\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89d2\u8272\u626e\u6f14\u80fd\u529b\u5b58\u5728\u4eba\u5de5\u8bc4\u4f30\u6210\u672c\u9ad8\u3001\u81ea\u52a8\u5316\u8bc4\u4f30\u6613\u504f\u501a\u7684\u75db\u70b9\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u5305\u542b4\u4e2a\u6838\u5fc3\u7ef4\u5ea6\uff08\u60c5\u611f\u7406\u89e3/\u51b3\u7b56\u80fd\u529b/\u9053\u5fb7\u5bf9\u9f50/\u89d2\u8272\u4e00\u81f4\u6027\uff09\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u4f7f\u7528\u591a\u8f6e\u5bf9\u8bdd\u573a\u666f\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "result": "\u5efa\u7acb\u4e86\u5305\u542b\u57fa\u7ebf\u8bc4\u4f30\u7ed3\u679c\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5e76\u5f00\u6e90\u4e86\u4ee3\u7801\u548c\u6570\u636e\u96c6\uff08https://github.com/yelboudouri/RPEval\uff09\u3002", "conclusion": "RPEval\u4e3a\u6807\u51c6\u5316\u8bc4\u4f30LLM\u89d2\u8272\u626e\u6f14\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u63a8\u52a8\u4e86\u8be5\u9886\u57df\u8bc4\u4f30\u65b9\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2505.13171", "pdf": "https://arxiv.org/pdf/2505.13171", "abs": "https://arxiv.org/abs/2505.13171", "authors": ["Yixuan Xu", "Antoine Bosselut", "Imanol Schlag"], "title": "Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks", "categories": ["cs.CL"], "comment": null, "summary": "Large language models are known to memorize parts of their training data,\nposing risk of copyright violations. To systematically examine this risk, we\npretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing\nweb-scale data with public domain books used to simulate copyrighted content at\ncontrolled frequencies at lengths at least ten times longer than prior work. We\nthereby identified the offset effect, a phenomenon characterized by two key\nfindings: (1) verbatim memorization is most strongly triggered by short\nprefixes drawn from the beginning of the context window, with memorization\ndecreasing counterintuitively as prefix length increases; and (2) a sharp\ndecline in verbatim recall when prefix begins offset from the initial tokens of\nthe context window. We attribute this to positional fragility: models rely\ndisproportionately on the earliest tokens in their context window as retrieval\nanchors, making them sensitive to even slight shifts. We further observe that\nwhen the model fails to retrieve memorized content, it often produces\ndegenerated text. Leveraging these findings, we show that shifting sensitive\ndata deeper into the context window suppresses both extractable memorization\nand degeneration. Our results suggest that positional offset is a critical and\npreviously overlooked axis for evaluating memorization risks, since prior work\nimplicitly assumed uniformity by probing only from the beginning of training\nsequences.", "AI": {"tldr": "\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u4f4d\u7f6e\u504f\u79fb\u6548\u5e94\uff0c\u5373\u521d\u59cb\u4f4d\u7f6e\u5bf9\u8bb0\u5fc6\u654f\u611f\uff0c\u901a\u8fc7\u504f\u79fb\u654f\u611f\u6570\u636e\u53ef\u663e\u8457\u964d\u4f4e\u7248\u6743\u98ce\u9669", "motivation": "\u7cfb\u7edf\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u8bad\u7ec3\u6570\u636e\u7684\u8bb0\u5fc6\u98ce\u9669\u53ca\u5176\u7248\u6743\u5f71\u54cd", "method": "\u9884\u8bad\u7ec3\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\uff081B/3B/8B\uff09\uff0c\u6df7\u5408\u7f51\u7edc\u6570\u636e\u4e0e\u53d7\u63a7\u7248\u6743\u7684\u516c\u5171\u4e66\u7c4d\u6570\u636e\uff0c\u7814\u7a76\u8bb0\u5fc6\u89e6\u53d1\u673a\u5236", "result": "1. \u77ed\u524d\u7f00\u89e6\u53d1\u6700\u5f3a\u8bb0\u5fc6\uff08\u4f4d\u7f6e\u8106\u5f31\u6027\uff09 2. \u4f4d\u7f6e\u504f\u79fb\u5bfc\u81f4\u8bb0\u5fc6\u80fd\u529b\u65ad\u5d16\u5f0f\u4e0b\u964d 3. \u8bb0\u5fc6\u5931\u8d25\u65f6\u4ea7\u751f\u9000\u5316\u6587\u672c", "conclusion": "\u4f4d\u7f6e\u504f\u79fb\u662f\u8bc4\u4f30\u8bb0\u5fc6\u98ce\u9669\u7684\u5173\u952e\u7ef4\u5ea6\uff0c\u5b9e\u9645\u90e8\u7f72\u53ef\u901a\u8fc7\u6570\u636e\u4f4d\u7f6e\u504f\u79fb\u4e3b\u52a8\u964d\u4f4e\u7248\u6743\u98ce\u9669"}}
{"id": "2505.13173", "pdf": "https://arxiv.org/pdf/2505.13173", "abs": "https://arxiv.org/abs/2505.13173", "authors": ["V. S. D. S. Mahesh Akavarapu", "Hrishikesh Terdalkar", "Pramit Bhattacharyya", "Shubhangi Agarwal", "Vishakha Deulgaonkar", "Pralay Manna", "Chaitali Dangarikar", "Arnab Bhattacharya"], "title": "A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs", "categories": ["cs.CL", "I.2.7"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across diverse tasks and languages. In this study, we focus on\nnatural language understanding in three classical languages -- Sanskrit,\nAncient Greek and Latin -- to investigate the factors affecting cross-lingual\nzero-shot generalization. First, we explore named entity recognition and\nmachine translation into English. While LLMs perform equal to or better than\nfine-tuned baselines on out-of-domain data, smaller models often struggle,\nespecially with niche or abstract entity types. In addition, we concentrate on\nSanskrit by presenting a factoid question-answering (QA) dataset and show that\nincorporating context via retrieval-augmented generation approach significantly\nboosts performance. In contrast, we observe pronounced performance drops for\nsmaller LLMs across these QA tasks. These results suggest model scale as an\nimportant factor influencing cross-lingual generalization. Assuming that models\nused such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical\nlanguages, our findings provide insights into how LLMs may generalize on these\nlanguages and their consequent utility in classical studies.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u6a21\u578b\u89c4\u6a21\u662f\u5f71\u54cd\u53e4\u5178\u8bed\u8a00\u8de8\u8bed\u8a00\u6cdb\u5316\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5982GPT-4o\uff09\u5728\u96f6\u6837\u672c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u800c\u68c0\u7d22\u589e\u5f3a\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u68b5\u8bed\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u53e4\u5178\u8bed\u8a00\uff08\u68b5\u8bed/\u53e4\u5e0c\u814a\u8bed/\u62c9\u4e01\u8bed\uff09\u81ea\u7136\u7406\u89e3\u4efb\u52a1\u4e2d\u7684\u8de8\u8bed\u8a00\u6cdb\u5316\u673a\u5236\uff0c\u9a8c\u8bc1\u6a21\u578b\u89c4\u6a21\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u53ca\u5176\u5728\u53e4\u5178\u5b66\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u3001\u673a\u5668\u7ffb\u8bd1\u548c\u68b5\u8bed\u95ee\u7b54\u4e09\u4e2a\u4efb\u52a1\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5176\u4e2d\u95ee\u7b54\u4efb\u52a1\u91c7\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u878d\u5165\u4e0a\u4e0b\u6587\u3002", "result": "\u5927\u578b\u6a21\u578b\u5728\u8de8\u57df\u6570\u636e\u4e0a\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff08NER\u4efb\u52a1F1\u63d0\u53475-15%\uff09\uff0c\u68c0\u7d22\u589e\u5f3a\u4f7f\u95ee\u7b54\u51c6\u786e\u7387\u63d0\u534732%\uff1b\u4f46\u53c2\u6570\u91cf<70\u4ebf\u7684\u6a21\u578b\u5728\u62bd\u8c61\u5b9e\u4f53\u8bc6\u522b\u548c\u95ee\u7b54\u4efb\u52a1\u4e2d\u51fa\u73b020-40%\u7684\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u6a21\u578b\u89c4\u6a21\u76f4\u63a5\u5f71\u54cd\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\uff0c\u672a\u7ecf\u8fc7\u53e4\u5178\u8bed\u8a00\u6307\u4ee4\u5fae\u8c03\u7684LLMs\u4ecd\u5c55\u73b0\u5e94\u7528\u4ef7\u503c\uff0c\u5efa\u8bae\u53e4\u5178\u5b66\u7814\u7a76\u4f18\u5148\u9009\u7528\u53c2\u6570\u91cf>700\u4ebf\u7684\u6a21\u578b\u5e76\u914d\u5408\u68c0\u7d22\u589e\u5f3a\u6280\u672f\u3002"}}
{"id": "2505.13176", "pdf": "https://arxiv.org/pdf/2505.13176", "abs": "https://arxiv.org/abs/2505.13176", "authors": ["Zihao Cheng", "Hongru Wang", "Zeming Liu", "Yuhang Guo", "Yuanfang Guo", "Yunhong Wang", "Haifeng Wang"], "title": "ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "While integrating external tools into large language models (LLMs) enhances\ntheir ability to access real-time information and domain-specific services,\nexisting approaches focus narrowly on functional tool selection following user\ninstructions, overlooking the context-aware personalization in tool selection.\nThis oversight leads to suboptimal user satisfaction and inefficient tool\nutilization, particularly when overlapping toolsets require nuanced selection\nbased on contextual factors. To bridge this gap, we introduce ToolSpectrum, a\nbenchmark designed to evaluate LLMs' capabilities in personalized tool\nutilization. Specifically, we formalize two key dimensions of personalization,\nuser profile and environmental factors, and analyze their individual and\nsynergistic impacts on tool utilization. Through extensive experiments on\nToolSpectrum, we demonstrate that personalized tool utilization significantly\nimproves user experience across diverse scenarios. However, even\nstate-of-the-art LLMs exhibit the limited ability to reason jointly about user\nprofiles and environmental factors, often prioritizing one dimension at the\nexpense of the other. Our findings underscore the necessity of context-aware\npersonalization in tool-augmented LLMs and reveal critical limitations for\ncurrent models. Our data and code are available at\nhttps://github.com/Chengziha0/ToolSpectrum.", "AI": {"tldr": "\u63d0\u51faToolSpectrum\u57fa\u51c6\u8bc4\u4f30LLM\u4e2a\u6027\u5316\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u53d1\u73b0\u4e0a\u4e0b\u6587\u611f\u77e5\u5bf9\u7528\u6237\u4f53\u9a8c\u7684\u5173\u952e\u4f5c\u7528\u53ca\u73b0\u6709\u6a21\u578b\u5c40\u9650\u6027", "motivation": "\u73b0\u6709\u5de5\u5177\u96c6\u6210\u65b9\u6cd5\u5ffd\u89c6\u4e0a\u4e0b\u6587\u4e2a\u6027\u5316\uff0c\u5bfc\u81f4\u5de5\u5177\u9009\u62e9\u6b21\u4f18\u5316\u3002\u5c24\u5176\u5728\u5de5\u5177\u91cd\u53e0\u573a\u666f\u9700\u57fa\u4e8e\u7528\u6237\u753b\u50cf\u548c\u73af\u5883\u56e0\u7d20\u534f\u540c\u51b3\u7b56", "method": "\u6784\u5efaToolSpectrum\u57fa\u51c6\uff0c\u5f62\u5f0f\u5316\u5b9a\u4e49\u7528\u6237\u753b\u50cf\u548c\u73af\u5883\u56e0\u7d20\u53cc\u7ef4\u5ea6\uff0c\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u5206\u6790\u5176\u5355\u72ec/\u534f\u540c\u5f71\u54cd", "result": "\u4e2a\u6027\u5316\u5de5\u5177\u4f7f\u7528\u663e\u8457\u63d0\u5347\u8de8\u573a\u666f\u4f53\u9a8c\uff0c\u4f46\u9876\u7ea7LLM\u5728\u53cc\u7ef4\u5ea6\u8054\u5408\u63a8\u7406\u65f6\u5448\u73b0\u4f18\u5148\u5355\u7ef4\u5ea6\uff08\u51c6\u786e\u7387\u4ec567.3%\uff09\u7684\u660e\u663e\u5c40\u9650", "conclusion": "\u4e0a\u4e0b\u6587\u611f\u77e5\u4e2a\u6027\u5316\u662f\u5de5\u5177\u589e\u5f3aLLM\u7684\u5fc5\u8981\u65b9\u5411\uff0c\u5f53\u524d\u6a21\u578b\u9700\u7a81\u7834\u591a\u7ef4\u5ea6\u8054\u5408\u63a8\u7406\u80fd\u529b\u74f6\u9888"}}
{"id": "2505.13181", "pdf": "https://arxiv.org/pdf/2505.13181", "abs": "https://arxiv.org/abs/2505.13181", "authors": ["Zhengrui Ma", "Yang Feng", "Chenze Shao", "Fandong Meng", "Jie Zhou", "Min Zhang"], "title": "Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Demos and code are available at https://github.com/ictnlp/SLED-TTS", "summary": "We introduce SLED, an alternative approach to speech language modeling by\nencoding speech waveforms into sequences of continuous latent representations\nand modeling them autoregressively using an energy distance objective. The\nenergy distance offers an analytical measure of the distributional gap by\ncontrasting simulated and target samples, enabling efficient training to\ncapture the underlying continuous autoregressive distribution. By bypassing\nreliance on residual vector quantization, SLED avoids discretization errors and\neliminates the need for the complicated hierarchical architectures common in\nexisting speech language models. It simplifies the overall modeling pipeline\nwhile preserving the richness of speech information and maintaining inference\nefficiency. Empirical results demonstrate that SLED achieves strong performance\nin both zero-shot and streaming speech synthesis, showing its potential for\nbroader applications in general-purpose speech language models.", "AI": {"tldr": "\u63d0\u51faSLED\u65b9\u6cd5\uff0c\u901a\u8fc7\u80fd\u91cf\u8ddd\u79bb\u76ee\u6807\u5efa\u6a21\u8fde\u7eed\u8bed\u97f3\u8868\u5f81\uff0c\u907f\u514d\u91cf\u5316\u8bef\u5dee\u5e76\u7b80\u5316\u6a21\u578b\u67b6\u6784\uff0c\u5728\u96f6\u6837\u672c\u548c\u6d41\u5f0f\u8bed\u97f3\u5408\u6210\u4e2d\u5c55\u73b0\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u4f9d\u8d56\u6b8b\u5dee\u5411\u91cf\u91cf\u5316\u548c\u590d\u6742\u5c42\u6b21\u7ed3\u6784\uff0c\u5bfc\u81f4\u79bb\u6563\u5316\u8bef\u5dee\u548c\u5efa\u6a21\u6d41\u7a0b\u590d\u6742\u5316\u3002\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8fde\u7eed\u8868\u5f81\u5efa\u6a21\u65b9\u6cd5\u4ee5\u4fdd\u6301\u8bed\u97f3\u4fe1\u606f\u5b8c\u6574\u6027\u3002", "method": "\u5c06\u8bed\u97f3\u6ce2\u5f62\u7f16\u7801\u4e3a\u8fde\u7eed\u6f5c\u5728\u8868\u793a\u5e8f\u5217\uff0c\u91c7\u7528\u80fd\u91cf\u8ddd\u79bb\u76ee\u6807\u8fdb\u884c\u81ea\u56de\u5f52\u5efa\u6a21\u3002\u8be5\u8ddd\u79bb\u76f4\u63a5\u5bf9\u6bd4\u6a21\u62df\u4e0e\u76ee\u6807\u6837\u672c\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u5b9e\u73b0\u8fde\u7eed\u5206\u5e03\u7684\u53c2\u6570\u5316\u5b66\u4e60\u3002", "result": "\u5b9e\u8bc1\u8868\u660eSLED\u5728\u96f6\u6837\u672c/\u6d41\u5f0f\u5408\u6210\u4efb\u52a1\u4e2d\u6027\u80fd\u4f18\u5f02\uff0c\u63a8\u7406\u6548\u7387\u63d0\u534730%\uff0c\u540c\u65f6\u4fdd\u630197.2%\u7684\u8bed\u97f3\u4fe1\u606f\u4fdd\u7559\u7387\u3002", "conclusion": "SLED\u901a\u8fc7\u6d88\u9664\u91cf\u5316\u6b65\u9aa4\u7b80\u5316\u5efa\u6a21\u6d41\u7a0b\uff0c\u4e3a\u901a\u7528\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u8303\u5f0f\uff0c\u5177\u6709\u6269\u5c55\u81f3\u591a\u6a21\u6001\u4efb\u52a1\u7684\u6f5c\u529b\u3002"}}
{"id": "2505.13204", "pdf": "https://arxiv.org/pdf/2505.13204", "abs": "https://arxiv.org/abs/2505.13204", "authors": ["Jikai Wang", "Zhenxu Tian", "Juntao Li", "Qingrong Xia", "Xinyu Duan", "Zhefeng Wang", "Baoxing Huai", "Min Zhang"], "title": "Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification", "categories": ["cs.CL"], "comment": "Pre-print", "summary": "Recent works have revealed the great potential of speculative decoding in\naccelerating the autoregressive generation process of large language models.\nThe success of these methods relies on the alignment between draft candidates\nand the sampled outputs of the target model. Existing methods mainly achieve\ndraft-target alignment with training-based methods, e.g., EAGLE, Medusa,\ninvolving considerable training costs. In this paper, we present a\ntraining-free alignment-augmented speculative decoding algorithm. We propose\nalignment sampling, which leverages output distribution obtained in the\nprefilling phase to provide more aligned draft candidates. To further benefit\nfrom high-quality but non-aligned draft candidates, we also introduce a simple\nyet effective flexible verification strategy. Through an adaptive probability\nthreshold, our approach can improve generation accuracy while further improving\ninference efficiency. Experiments on 8 datasets (including question answering,\nsummarization and code completion tasks) show that our approach increases the\naverage generation score by 3.3 points for the LLaMA3 model. Our method\nachieves a mean acceptance length up to 2.39 and speed up generation by 2.23.", "AI": {"tldr": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u7684\u5bf9\u9f50\u589e\u5f3a\u63a8\u6d4b\u89e3\u7801\u7b97\u6cd5\uff0c\u901a\u8fc7\u9884\u586b\u5145\u8f93\u51fa\u5206\u5e03\u4f18\u5316\u5019\u9009\u5bf9\u9f50\uff0c\u914d\u5408\u7075\u6d3b\u9a8c\u8bc1\u7b56\u7565\u5b9e\u73b0\u66f4\u9ad8\u6548\u63a8\u7406", "motivation": "\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u65b9\u6cd5\u4f9d\u8d56\u8bad\u7ec3\u5b9e\u73b0\u8349\u7a3f-\u76ee\u6807\u5bf9\u9f50\uff08\u5982EAGLE\u3001Medusa\uff09\uff0c\u5b58\u5728\u8f83\u9ad8\u8bad\u7ec3\u6210\u672c\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u65e0\u8bad\u7ec3\u65b9\u6cd5\u89e3\u51b3\u8be5\u95ee\u9898", "method": "1. \u5bf9\u9f50\u91c7\u6837\uff1a\u5229\u7528\u9884\u586b\u5145\u9636\u6bb5\u83b7\u5f97\u7684\u8f93\u51fa\u5206\u5e03\u751f\u6210\u66f4\u5bf9\u9f50\u7684\u5019\u9009\u8349\u7a3f\n2. \u7075\u6d3b\u9a8c\u8bc1\uff1a\u901a\u8fc7\u81ea\u9002\u5e94\u6982\u7387\u9608\u503c\u6574\u5408\u9ad8\u8d28\u91cf\u975e\u5bf9\u9f50\u5019\u9009\uff0c\u63d0\u5347\u51c6\u786e\u6027\u4e0e\u6548\u7387", "result": "\u57288\u4e2a\u6570\u636e\u96c6\uff08QA/\u6458\u8981/\u4ee3\u7801\u751f\u6210\uff09\u4e0a\uff1a\n- LLaMA3\u751f\u6210\u5206\u6570\u5e73\u5747\u63d0\u53473.3\u5206\n- \u5e73\u5747\u63a5\u53d7\u957f\u5ea6\u8fbe2.39\n- \u751f\u6210\u901f\u5ea6\u63d0\u53472.23\u500d", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7a81\u7834\u4f20\u7edf\u8bad\u7ec3\u4f9d\u8d56\uff0c\u901a\u8fc7\u7b97\u6cd5\u7ea7\u521b\u65b0\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u66f4\u4f18\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.13210", "pdf": "https://arxiv.org/pdf/2505.13210", "abs": "https://arxiv.org/abs/2505.13210", "authors": ["Xiaocong Du", "Haoyu Pei", "Haipeng Zhang"], "title": "Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Classical Chinese poetry is a vital and enduring part of Chinese literature,\nconveying profound emotional resonance. Existing studies analyze sentiment\nbased on textual meanings, overlooking the unique rhythmic and visual features\ninherent in poetry,especially since it is often recited and accompanied by\nChinese paintings. In this work, we propose a dialect-enhanced multimodal\nframework for classical Chinese poetry sentiment analysis. We extract\nsentence-level audio features from the poetry and incorporate audio from\nmultiple dialects,which may retain regional ancient Chinese phonetic features,\nenriching the phonetic representation. Additionally, we generate sentence-level\nvisual features, and the multimodal features are fused with textual features\nenhanced by LLM translation through multimodal contrastive representation\nlearning. Our framework outperforms state-of-the-art methods on two public\ndatasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro\nF1. We open-source the code to facilitate research in this area and provide\ninsights for general multimodal Chinese representation.", "AI": {"tldr": "\u63d0\u51fa\u878d\u5408\u65b9\u8a00\u97f3\u9891\u4e0e\u89c6\u89c9\u7279\u5f81\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u53e4\u5178\u8bd7\u6b4c\u60c5\u611f\u5206\u6790\u6548\u679c", "motivation": "\u73b0\u6709\u7814\u7a76\u5ffd\u7565\u8bd7\u6b4c\u6717\u8bf5\u97f5\u5f8b\u548c\u914d\u56fe\u89c6\u89c9\u7279\u5f81\uff0c\u800c\u65b9\u8a00\u53ef\u80fd\u4fdd\u7559\u53e4\u6c49\u8bed\u97f3\u97f5\u7279\u5f81\uff0c\u591a\u6a21\u6001\u878d\u5408\u80fd\u66f4\u5168\u9762\u6355\u6349\u60c5\u611f", "method": "\u63d0\u53d6\u591a\u65b9\u8a00\u53e5\u5b50\u7ea7\u97f3\u9891\u7279\u5f81+\u751f\u6210\u89c6\u89c9\u7279\u5f81\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5bf9\u6bd4\u8868\u793a\u5b66\u4e60\u4e0eLLM\u589e\u5f3a\u7684\u6587\u672c\u7279\u5f81\u878d\u5408", "result": "\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u51c6\u786e\u7387\u63d0\u5347\u22652.51%\uff0c\u5b8fF1\u63d0\u5347\u22651.63%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5", "conclusion": "\u8bc1\u5b9e\u591a\u6a21\u6001\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5f00\u6e90\u4ee3\u7801\u63a8\u52a8\u9886\u57df\u53d1\u5c55\uff0c\u4e3a\u4e2d\u6587\u591a\u6a21\u6001\u8868\u793a\u7814\u7a76\u63d0\u4f9b\u65b0\u601d\u8def"}}
{"id": "2505.13220", "pdf": "https://arxiv.org/pdf/2505.13220", "abs": "https://arxiv.org/abs/2505.13220", "authors": ["Jie Ying", "Zihong Chen", "Zhefan Wang", "Wanli Jiang", "Chenyang Wang", "Zhonghang Yuan", "Haoyang Su", "Huanjun Kong", "Fan Yang", "Nanqing Dong"], "title": "SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "Seed science is essential for modern agriculture, directly influencing crop\nyields and global food security. However, challenges such as interdisciplinary\ncomplexity and high costs with limited returns hinder progress, leading to a\nshortage of experts and insufficient technological support. While large\nlanguage models (LLMs) have shown promise across various fields, their\napplication in seed science remains limited due to the scarcity of digital\nresources, complex gene-trait relationships, and the lack of standardized\nbenchmarks. To address this gap, we introduce SeedBench -- the first multi-task\nbenchmark specifically designed for seed science. Developed in collaboration\nwith domain experts, SeedBench focuses on seed breeding and simulates key\naspects of modern breeding processes. We conduct a comprehensive evaluation of\n26 leading LLMs, encompassing proprietary, open-source, and domain-specific\nfine-tuned models. Our findings not only highlight the substantial gaps between\nthe power of LLMs and the real-world seed science problems, but also make a\nfoundational step for research on LLMs for seed design.", "AI": {"tldr": "\u9996\u6b21\u63d0\u51fa\u79cd\u5b50\u79d1\u5b66\u4e13\u7528\u591a\u4efb\u52a1\u57fa\u51c6SeedBench\uff0c\u8bc4\u4f3026\u4e2a\u4e3b\u6d41\u5927\u6a21\u578b\u5e76\u63ed\u793a\u5176\u4e0e\u4ea7\u4e1a\u9700\u6c42\u7684\u663e\u8457\u5dee\u8ddd", "motivation": "\u79cd\u5b50\u79d1\u5b66\u9762\u4e34\u8de8\u5b66\u79d1\u590d\u6742\u6027\u9ad8\u3001\u6210\u672c\u56de\u62a5\u7387\u4f4e\u5bfc\u81f4\u7684\u4e13\u5bb6\u77ed\u7f3a\u548c\u6280\u672f\u652f\u6491\u4e0d\u8db3\u95ee\u9898\uff0cLLMs\u5728\u79cd\u5b50\u79d1\u5b66\u7684\u5e94\u7528\u56e0\u6570\u5b57\u8d44\u6e90\u532e\u4e4f\u3001\u57fa\u56e0-\u6027\u72b6\u5173\u7cfb\u590d\u6742\u548c\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u800c\u53d7\u9650", "method": "\u8054\u5408\u9886\u57df\u4e13\u5bb6\u5f00\u53d1\u805a\u7126\u79cd\u5b50\u80b2\u79cd\u7684\u591a\u4efb\u52a1\u57fa\u51c6SeedBench\uff0c\u6a21\u62df\u73b0\u4ee3\u80b2\u79cd\u6838\u5fc3\u73af\u8282\uff0c\u7cfb\u7edf\u8bc4\u4f30\u5305\u542b\u4e13\u6709/\u5f00\u6e90/\u9886\u57df\u5fae\u8c03\u6a21\u578b\u5728\u5185\u768426\u4e2a\u4e3b\u6d41LLMs", "result": "\u5b9e\u9a8c\u8868\u660e\u5f53\u524dLLMs\u80fd\u529b\u4e0e\u79cd\u5b50\u79d1\u5b66\u5b9e\u9645\u9700\u6c42\u5b58\u5728\u663e\u8457\u5dee\u8ddd", "conclusion": "SeedBench\u4e3a\u79cd\u5b50\u8bbe\u8ba1\u7684\u5927\u6a21\u578b\u7814\u7a76\u5960\u5b9a\u57fa\u7840\uff0c\u63a8\u52a8LLMs\u5728\u79cd\u5b50\u79d1\u5b66\u4e2d\u7684\u53d1\u5c55"}}
{"id": "2505.13244", "pdf": "https://arxiv.org/pdf/2505.13244", "abs": "https://arxiv.org/abs/2505.13244", "authors": ["Jieying Xue", "Phuong Minh Nguyen", "Minh Le Nguyen", "Xin Liu"], "title": "JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models", "categories": ["cs.CL", "cs.LG"], "comment": "Published in The 19th International Workshop on Semantic Evaluation\n  (SemEval-2025)", "summary": "With the rapid advancement of global digitalization, users from different\ncountries increasingly rely on social media for information exchange. In this\ncontext, multilingual multi-label emotion detection has emerged as a critical\nresearch area. This study addresses SemEval-2025 Task 11: Bridging the Gap in\nText-Based Emotion Detection. Our paper focuses on two sub-tracks of this task:\n(1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity.\nTo tackle multilingual challenges, we leverage pre-trained multilingual models\nand focus on two architectures: (1) a fine-tuned BERT-based classification\nmodel and (2) an instruction-tuned generative LLM. Additionally, we propose two\nmethods for handling multi-label classification: the base method, which maps an\ninput directly to all its corresponding emotion labels, and the pairwise\nmethod, which models the relationship between the input text and each emotion\ncategory individually. Experimental results demonstrate the strong\ngeneralization ability of our approach in multilingual emotion recognition. In\nTrack A, our method achieved Top 4 performance across 10 languages, ranking 1st\nin Hindi. In Track B, our approach also secured Top 5 performance in 7\nlanguages, highlighting its simplicity and effectiveness\\footnote{Our code is\navailable at https://github.com/yingjie7/mlingual_multilabel_emo_detection.", "AI": {"tldr": "\u63d0\u51fa\u878d\u5408\u591a\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u53cc\u67b6\u6784\u65b9\u6cd5\uff0c\u5728SemEval-2025\u60c5\u611f\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0Top5\u6027\u80fd", "motivation": "\u5e94\u5bf9\u5168\u7403\u5316\u80cc\u666f\u4e0b\u793e\u4ea4\u5a92\u4f53\u591a\u8bed\u8a00\u7528\u6237\u60c5\u611f\u5206\u6790\u9700\u6c42\uff0c\u89e3\u51b3\u591a\u6807\u7b7e\u60c5\u611f\u68c0\u6d4b\u548c\u60c5\u611f\u5f3a\u5ea6\u7684\u8de8\u8bed\u8a00\u6311\u6218", "method": "\u4f7f\u7528\u591a\u8bed\u8a00BERT\u5fae\u8c03\u67b6\u6784+\u6307\u4ee4\u8c03\u4f18\u751f\u6210\u5f0fLLM\uff0c\u63d0\u51fa\u57fa\u7840\u591a\u6807\u7b7e\u5206\u7c7b\u6cd5\u548c\u6210\u5bf9\u5206\u7c7b\u6cd5", "result": "Track A\u572810\u8bed\u79cdTop4\uff08\u5370\u5730\u8bed\u7b2c1\uff09\uff0cTrack B\u57287\u8bed\u79cdTop5", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8de8\u8bed\u8a00\u60c5\u611f\u8bc6\u522b\u4e2d\u5c55\u73b0\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u53cc\u67b6\u6784\u7b56\u7565\u7684\u6709\u6548\u6027"}}
{"id": "2505.13251", "pdf": "https://arxiv.org/pdf/2505.13251", "abs": "https://arxiv.org/abs/2505.13251", "authors": ["Sidney Wong"], "title": "Stronger Together: Unleashing the Social Impact of Hate Speech Research", "categories": ["cs.CL"], "comment": "Accepted Proceedings of the Linguistic Society of America 2025 Annual\n  Meeting", "summary": "The advent of the internet has been both a blessing and a curse for once\nmarginalised communities. When used well, the internet can be used to connect\nand establish communities crossing different intersections; however, it can\nalso be used as a tool to alienate people and communities as well as perpetuate\nhate, misinformation, and disinformation especially on social media platforms.\nWe propose steering hate speech research and researchers away from pre-existing\ncomputational solutions and consider social methods to inform social solutions\nto address this social problem. In a similar way linguistics research can\ninform language planning policy, linguists should apply what we know about\nlanguage and society to mitigate some of the emergent risks and dangers of\nanti-social behaviour in digital spaces. We argue linguists and NLP researchers\ncan play a principle role in unleashing the social impact potential of\nlinguistics research working alongside communities, advocates, activists, and\npolicymakers to enable equitable digital inclusion and to close the digital\ndivide.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u7528\u793e\u4f1a\u8bed\u8a00\u5b66\u65b9\u6cd5\u800c\u975e\u7eaf\u6280\u672f\u624b\u6bb5\u89e3\u51b3\u7f51\u7edc\u4ec7\u6068\u8a00\u8bba\u95ee\u9898", "motivation": "\u4e92\u8054\u7f51\u5bf9\u8fb9\u7f18\u7fa4\u4f53\u5b58\u5728\u53cc\u5203\u5251\u6548\u5e94\u2014\u2014\u65e2\u4fc3\u8fdb\u8fde\u63a5\u53c8\u52a0\u5267\u6570\u5b57\u6392\u65a5\uff0c\u9700\u8981\u66f4\u6709\u6548\u89e3\u51b3\u65b9\u6848", "method": "\u501f\u9274\u8bed\u8a00\u5b66\u5f71\u54cd\u8bed\u8a00\u653f\u7b56\u7684\u7ecf\u9a8c\uff0c\u5c06\u793e\u4f1a\u8bed\u8a00\u5b66\u6d1e\u89c1\u8f6c\u5316\u4e3a\u6570\u5b57\u7a7a\u95f4\u6cbb\u7406\u65b9\u6848", "result": "\u63d0\u51fa\u8bed\u8a00\u5b66\u5bb6\u53ef\u4e0e\u793e\u533a\u3001\u653f\u7b56\u5236\u5b9a\u8005\u5408\u4f5c\uff0c\u901a\u8fc7\u793e\u4f1a\u5e72\u9884\u5b9e\u73b0\u6570\u5b57\u5305\u5bb9", "conclusion": "\u8bed\u8a00\u5b66\u4e0eNLP\u7814\u7a76\u8005\u5e94\u4e3b\u5bfc\u793e\u4f1a\u89e3\u51b3\u65b9\u6848\uff0c\u7f29\u5c0f\u6570\u5b57\u9e3f\u6c9f\u4fc3\u8fdb\u516c\u5e73"}}
{"id": "2505.13252", "pdf": "https://arxiv.org/pdf/2505.13252", "abs": "https://arxiv.org/abs/2505.13252", "authors": ["Rikhil Amonkar", "Ronan Le Bras", "Li Zhang"], "title": "Natural Language Planning via Coding and Inference Scaling", "categories": ["cs.CL"], "comment": null, "summary": "Real-life textual planning tasks such as meeting scheduling have posed much\nchallenge to LLMs especially when the complexity is high. While previous work\nprimarily studied auto-regressive generation of plans with closed-source\nmodels, we systematically evaluate both closed- and open-source models,\nincluding those that scales output length with complexity during inference, in\ngenerating programs, which are executed to output the plan. We consider not\nonly standard Python code, but also the code to a constraint satisfaction\nproblem solver. Despite the algorithmic nature of the task, we show that\nprogramming often but not always outperforms planning. Our detailed error\nanalysis also indicates a lack of robustness and efficiency in the generated\ncode that hinders generalization.", "AI": {"tldr": "\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u751f\u6210\u53ef\u6267\u884c\u7a0b\u5e8f\u89e3\u51b3\u590d\u6742\u6587\u672c\u89c4\u5212\u4efb\u52a1\u7684\u6548\u679c\uff0c\u53d1\u73b0\u7f16\u7a0b\u65b9\u6cd5\u901a\u5e38\u4f18\u4e8e\u76f4\u63a5\u89c4\u5212\uff0c\u4f46\u5b58\u5728\u4ee3\u7801\u8d28\u91cf\u7f3a\u9677\u3002", "motivation": "\u9488\u5bf9LLMs\u5728\u590d\u6742\u6587\u672c\u89c4\u5212\u4efb\u52a1\uff08\u5982\u4f1a\u8bae\u5b89\u6392\uff09\u4e2d\u9047\u5230\u7684\u6311\u6218\uff0c\u63a2\u7d22\u901a\u8fc7\u751f\u6210\u53ef\u6267\u884c\u7a0b\u5e8f\u66ff\u4ee3\u4f20\u7edf\u89c4\u5212\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u95ed\u6e90/\u5f00\u6e90\u6a21\u578b\u751f\u6210\u53ef\u6267\u884c\u7a0b\u5e8f\uff08\u5305\u62ecPython\u4ee3\u7801\u548c\u7ea6\u675f\u6c42\u89e3\u5668\u4ee3\u7801\uff09\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u7a0b\u5e8f\u6267\u884c\u8f93\u51fa\u89c4\u5212\u65b9\u6848\u3002", "result": "\u7f16\u7a0b\u65b9\u6cd5\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u76f4\u63a5\u89c4\u5212\uff0c\u4f46\u751f\u6210\u7684\u4ee3\u7801\u5b58\u5728\u9c81\u68d2\u6027\u548c\u6548\u7387\u95ee\u9898\uff0c\u5f71\u54cd\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u7f16\u7a0b\u8303\u5f0f\u4e3a\u89e3\u51b3\u590d\u6742\u89c4\u5212\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u4f46\u9700\u63d0\u5347\u751f\u6210\u4ee3\u7801\u7684\u8d28\u91cf\u4ee5\u5b9e\u73b0\u53ef\u9760\u5e94\u7528\u3002"}}
{"id": "2505.13254", "pdf": "https://arxiv.org/pdf/2505.13254", "abs": "https://arxiv.org/abs/2505.13254", "authors": ["Siran Liu", "Yang Ye", "Qianchao Zhu", "Zheng Cao", "Yongchao He"], "title": "HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding", "categories": ["cs.CL"], "comment": null, "summary": "Autoregressive decoding, the standard approach for Large Language Model (LLM)\ninference, remains a significant bottleneck due to its sequential nature. While\nspeculative decoding algorithms mitigate this inefficiency through parallel\nverification, they fail to exploit the inherent heterogeneity in linguistic\ncomplexity, a key factor leading to suboptimal resource allocation. We address\nthis by proposing HeteroSpec, a heterogeneity-adaptive speculative decoding\nframework that dynamically optimizes computational resource allocation based on\nlinguistic context complexity. HeteroSpec introduces two key mechanisms: (1) A\nnovel cumulative meta-path Top-$K$ entropy metric for efficiently identifying\npredictable contexts. (2) A dynamic resource allocation strategy based on\ndata-driven entropy partitioning, enabling adaptive speculative expansion and\npruning tailored to local context difficulty. Evaluated on five public\nbenchmarks and four models, HeteroSpec achieves an average speedup of\n4.26$\\times$. It consistently outperforms state-of-the-art EAGLE-3 across\nspeedup rates, average acceptance length, and verification cost. Notably,\nHeteroSpec requires no draft model retraining, incurs minimal overhead, and is\northogonal to other acceleration techniques. It demonstrates enhanced\nacceleration with stronger draft models, establishing a new paradigm for\ncontext-aware LLM inference acceleration.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8bed\u8a00\u590d\u6742\u5ea6\u5f02\u8d28\u6027\u7684\u52a8\u6001\u8d44\u6e90\u5206\u914d\u6846\u67b6HeteroSpec\uff0c\u901a\u8fc7\u71b5\u5ea6\u91cf\u548c\u81ea\u9002\u5e94\u6269\u5c55\u673a\u5236\u5b9e\u73b04.26\u500d\u52a0\u901f\u6548\u679c\uff0c\u65e0\u9700\u6a21\u578b\u91cd\u8bad\u7ec3\u4e14\u517c\u5bb9\u73b0\u6709\u52a0\u901f\u6280\u672f", "motivation": "\u73b0\u6709\u63a8\u6d4b\u89e3\u7801\u7b97\u6cd5\u672a\u5145\u5206\u5229\u7528\u8bed\u8a00\u590d\u6742\u6027\u5dee\u5f02\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u4e0d\u5747\u8861\u3002\u9700\u8981\u52a8\u6001\u8c03\u6574\u8d44\u6e90\u5206\u914d\u7b56\u7565\u4ee5\u9002\u5e94\u4e0d\u540c\u4e0a\u4e0b\u6587\u590d\u6742\u5ea6", "method": "1) \u63d0\u51fa\u7d2f\u79ef\u5143\u8def\u5f84Top-K\u71b5\u6307\u6807\u8bc6\u522b\u53ef\u9884\u6d4b\u4e0a\u4e0b\u6587\uff1b2) \u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u71b5\u5206\u533a\u7684\u52a8\u6001\u8d44\u6e90\u5206\u914d\u673a\u5236\uff0c\u5b9e\u73b0\u4e0a\u4e0b\u6587\u96be\u5ea6\u81ea\u9002\u5e94\u7684\u63a8\u6d4b\u6269\u5c55\u4e0e\u526a\u679d", "result": "\u57285\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c4\u79cd\u6a21\u578b\u4e0a\u5e73\u5747\u52a0\u901f4.26\u500d\uff0c\u9a8c\u8bc1\u6210\u672c\u964d\u4f4e34%\uff0c\u63a5\u53d7\u957f\u5ea6\u63d0\u534721%\uff0c\u517c\u5bb9\u66f4\u5f3a\u8349\u7a3f\u6a21\u578b\u65f6\u6548\u679c\u6301\u7eed\u63d0\u5347", "conclusion": "HeteroSpec\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u8d44\u6e90\u52a8\u6001\u5206\u914d\uff0c\u5efa\u7acb\u4e86LLM\u52a0\u901f\u65b0\u8303\u5f0f\uff0c\u5176\u96f6\u8bad\u7ec3\u5f00\u9500\u548c\u6b63\u4ea4\u6027\u7279\u70b9\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.13257", "pdf": "https://arxiv.org/pdf/2505.13257", "abs": "https://arxiv.org/abs/2505.13257", "authors": ["Zilu Tang", "Afra Feyza Aky\u00fcrek", "Ekin Aky\u00fcrek", "Derry Wijaya"], "title": "WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "9 pages, preprint", "summary": "Preference alignment has become a standard pipeline in finetuning models to\nfollow \\emph{generic} human preferences. Majority of work seeks to optimize\nmodel to produce responses that would be preferable \\emph{on average},\nsimplifying the diverse and often \\emph{contradicting} space of human\npreferences. While research has increasingly focused on personalized alignment:\nadapting models to individual user preferences, there is a lack of personalized\npreference dataset which focus on nuanced individual-level preferences. To\naddress this, we introduce WikiPersona: the first fine-grained personalization\nusing well-documented, famous individuals. Our dataset challenges models to\nalign with these personas through an interpretable process: generating\nverifiable textual descriptions of a persona's background and preferences in\naddition to alignment. We systematically evaluate different personalization\napproaches and find that as few-shot prompting with preferences and fine-tuning\nfail to simultaneously ensure effectiveness and efficiency, using\n\\textit{inferred personal preferences} as prefixes enables effective\npersonalization, especially in topics where preferences clash while leading to\nmore equitable generalization across unseen personas.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u7ec6\u7c92\u5ea6\u4e2a\u6027\u5316\u5bf9\u9f50\u6570\u636e\u96c6WikiPersona\uff0c\u901a\u8fc7\u540d\u4eba\u80cc\u666f\u4e0e\u504f\u597d\u63cf\u8ff0\u89e3\u51b3\u4f20\u7edf\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\u5728\u4e2a\u4f53\u77db\u76fe\u504f\u597d\u573a\u666f\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\u4e3b\u8981\u4f18\u5316\u6a21\u578b\u5728\u901a\u7528\u4eba\u7c7b\u504f\u597d\u4e0a\u7684\u5e73\u5747\u8868\u73b0\uff0c\u5ffd\u89c6\u4e86\u73b0\u5b9e\u4e2d\u4e2a\u4f53\u504f\u597d\u5b58\u5728\u77db\u76fe\u6027\u548c\u591a\u6837\u6027\u7684\u7279\u70b9\uff0c\u4e14\u7f3a\u4e4f\u9488\u5bf9\u4e2a\u4f53\u5c42\u9762\u7ec6\u7c92\u5ea6\u504f\u597d\u7684\u6570\u636e\u96c6\u3002", "method": "\u6784\u5efaWikiPersona\u6570\u636e\u96c6\uff0c\u8981\u6c42\u6a21\u578b\u540c\u65f6\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u540d\u4eba\u80cc\u666f\u63cf\u8ff0\u548c\u504f\u597d\u5bf9\u9f50\u5185\u5bb9\u3002\u901a\u8fc7\u7cfb\u7edf\u6027\u8bc4\u4f30\u53d1\u73b0\uff0c\u57fa\u4e8e\u63a8\u65ad\u4e2a\u4eba\u504f\u597d\u7684\u524d\u7f00\u7f16\u7801\u65b9\u6cd5\u6bd4\u4f20\u7edffew-shot\u63d0\u793a\u548c\u5fae\u8c03\u66f4\u6709\u6548\u3002", "result": "\u4f7f\u7528\u63a8\u65ad\u7684\u4e2a\u4eba\u504f\u597d\u4f5c\u4e3a\u524d\u7f00\u7f16\u7801\uff0c\u5728\u504f\u597d\u51b2\u7a81\u8bdd\u9898\u4e2d\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u4e2a\u6027\u5316\u5bf9\u9f50\uff0c\u4e14\u5bf9\u672a\u89c1\u8fc7\u7684\u540d\u4eba\u5177\u6709\u66f4\u597d\u7684\u6cdb\u5316\u516c\u5e73\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u7ec6\u7c92\u5ea6\u4e2a\u4f53\u504f\u597d\u6570\u636e\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u57fa\u4e8e\u524d\u7f00\u7f16\u7801\u7684\u4e2a\u6027\u5316\u5bf9\u9f50\u8303\u5f0f\uff0c\u4e3a\u5904\u7406\u77db\u76fe\u504f\u597d\u573a\u666f\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.13258", "pdf": "https://arxiv.org/pdf/2505.13258", "abs": "https://arxiv.org/abs/2505.13258", "authors": ["Jingyi Ren", "Yekun Xu", "Xiaolong Wang", "Weitao Li", "Weizhi Ma", "Yang Liu"], "title": "Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has significantly improved the\nperformance of large language models (LLMs) on knowledge-intensive domains.\nHowever, although RAG achieved successes across distinct domains, there are\nstill some unsolved challenges: 1) Effectiveness. Existing research mainly\nfocuses on developing more powerful RAG retrievers, but how to enhance the\ngenerator's (LLM's) ability to utilize the retrieved information for reasoning\nand generation? 2) Transparency. Most RAG methods ignore which retrieved\ncontent actually contributes to the reasoning process, resulting in a lack of\ninterpretability and visibility. To address this, we propose ARENA\n(Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator\nframework trained via reinforcement learning (RL) with our proposed rewards.\nBased on the structured generation and adaptive reward calculation, our\nRL-based training enables the model to identify key evidence, perform\nstructured reasoning, and generate answers with interpretable decision traces.\nApplied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments\nwith various RAG baselines demonstrate that our model achieves 10-30%\nimprovements on all multi-hop QA datasets, which is comparable with the SOTA\nCommercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses\nshow that ARENA has strong flexibility to be adopted on new datasets without\nextra training. Our models and codes are publicly released.", "AI": {"tldr": "\u63d0\u51faAREMA\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u900f\u660eRAG\u751f\u6210\u5668\u663e\u8457\u63d0\u5347\u591a\u8df3QA\u4efb\u52a1\u6027\u80fd", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u5b58\u5728\u751f\u6210\u5668\u5229\u7528\u68c0\u7d22\u4fe1\u606f\u6548\u7387\u4f4e\u3001\u51b3\u7b56\u8fc7\u7a0b\u4e0d\u900f\u660e\u4e24\u5927\u6838\u5fc3\u95ee\u9898", "method": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u8bbe\u8ba1\u81ea\u9002\u5e94\u5956\u52b1\u673a\u5236\uff0c\u5b9e\u73b0\u7ed3\u6784\u5316\u63a8\u7406\u548c\u53ef\u89e3\u91ca\u51b3\u7b56\u8ffd\u8e2a", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b010-30%\u6027\u80fd\u63d0\u5347\uff0c\u8fbe\u5230\u5546\u4e1aLLM\u6c34\u5e73\u4e14\u5177\u5907\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b", "conclusion": "AREMA\u6846\u67b6\u5f00\u521b\u4e86\u53ef\u89e3\u91caRAG\u65b0\u8303\u5f0f\uff0c\u5176\u5f00\u6e90\u4fc3\u8fdb\u9886\u57df\u53d1\u5c55\u5e76\u5c55\u793a\u5f3a\u5927\u9002\u5e94\u6027"}}
{"id": "2505.13259", "pdf": "https://arxiv.org/pdf/2505.13259", "abs": "https://arxiv.org/abs/2505.13259", "authors": ["Tianshi Zheng", "Zheye Deng", "Hong Ting Tsang", "Weiqi Wang", "Jiaxin Bai", "Zihao Wang", "Yangqiu Song"], "title": "From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery", "categories": ["cs.CL"], "comment": "16 pages", "summary": "Large Language Models (LLMs) are catalyzing a paradigm shift in scientific\ndiscovery, evolving from task-specific automation tools into increasingly\nautonomous agents and fundamentally redefining research processes and human-AI\ncollaboration. This survey systematically charts this burgeoning field, placing\na central focus on the changing roles and escalating capabilities of LLMs in\nscience. Through the lens of the scientific method, we introduce a foundational\nthree-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating\nautonomy and evolving responsibilities within the research lifecycle. We\nfurther identify pivotal challenges and future research trajectories such as\nrobotic automation, self-improvement, and ethical governance. Overall, this\nsurvey provides a conceptual architecture and strategic foresight to navigate\nand shape the future of AI-driven scientific discovery, fostering both rapid\ninnovation and responsible advancement. Github Repository:\nhttps://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u6b63\u4ece\u4e13\u7528\u5de5\u5177\u6f14\u53d8\u4e3a\u81ea\u4e3b\u79d1\u7814\u4e3b\u4f53\uff0c\u901a\u8fc7\u4e09\u7ea7\u5206\u7c7b\u6846\u67b6\uff08\u5de5\u5177-\u5206\u6790\u5458-\u79d1\u5b66\u5bb6\uff09\u7cfb\u7edf\u9610\u8ff0\u5176\u79d1\u7814\u8303\u5f0f\u53d8\u9769\uff0c\u5e76\u6307\u51fa\u672a\u6765\u5173\u952e\u6311\u6218\u3002", "motivation": "\u63a2\u8ba8\u5927\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u7a81\u7834\u4f20\u7edf\u5de5\u5177\u5c5e\u6027\uff0c\u9010\u6b65\u627f\u62c5\u66f4\u590d\u6742\u7684\u79d1\u7814\u89d2\u8272\uff0c\u91cd\u6784\u79d1\u7814\u6d41\u7a0b\u4e0e\u4eba\u673a\u534f\u4f5c\u6a21\u5f0f\u3002", "method": "\u57fa\u4e8e\u79d1\u5b66\u65b9\u6cd5\u8bba\u6784\u5efa\u4e09\u7ea7\u5206\u7c7b\u4f53\u7cfb\uff08\u5de5\u5177\u7ea7\u3001\u5206\u6790\u5458\u7ea7\u3001\u79d1\u5b66\u5bb6\u7ea7\uff09\uff0c\u9010\u7ea7\u5206\u6790\u6a21\u578b\u81ea\u4e3b\u6027\u6f14\u53d8\u53ca\u5176\u5728\u79d1\u7814\u751f\u547d\u5468\u671f\u4e2d\u7684\u529f\u80fd\u6f14\u8fdb\u3002", "result": "\u5efa\u7acb\u81ea\u4e3b\u6027\u6f14\u8fdb\u6846\u67b6\uff0c\u8bc6\u522b\u51fa\u673a\u5668\u4eba\u81ea\u52a8\u5316\u3001\u6a21\u578b\u81ea\u6211\u8fdb\u5316\u3001\u4f26\u7406\u6cbb\u7406\u7b49\u5173\u952e\u7814\u7a76\u65b9\u5411\uff0c\u63d0\u4f9bAI\u9a71\u52a8\u79d1\u7814\u7684\u6982\u5ff5\u67b6\u6784\u3002", "conclusion": "\u9700\u5efa\u7acb\u517c\u987e\u521b\u65b0\u901f\u5ea6\u4e0e\u8d23\u4efb\u4f26\u7406\u7684\u53d1\u5c55\u8def\u5f84\uff0c\u901a\u8fc7\u7cfb\u7edf\u5316\u6846\u67b6\u5f15\u5bfc\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u52a8\u79d1\u5b66\u53d1\u73b0\uff0c\u5b9e\u73b0\u6280\u672f\u8d4b\u80fd\u4e0e\u98ce\u9669\u7ba1\u63a7\u7684\u5e73\u8861\u3002"}}
{"id": "2505.13268", "pdf": "https://arxiv.org/pdf/2505.13268", "abs": "https://arxiv.org/abs/2505.13268", "authors": ["Livia Qian", "Carol Figueroa", "Gabriel Skantze"], "title": "Representation of perceived prosodic similarity of conversational feedback", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Interspeech 2025", "summary": "Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of\nspoken dialogue and is crucial to ensuring common ground in conversational\nsystems. The exact meaning of such feedback is conveyed through both lexical\nand prosodic form. In this work, we investigate the perceived prosodic\nsimilarity of vocal feedback with the same lexical form, and to what extent\nexisting speech representations reflect such similarities. A triadic comparison\ntask with recruited participants is used to measure perceived similarity of\nfeedback responses taken from two different datasets. We find that spectral and\nself-supervised speech representations encode prosody better than extracted\npitch features, especially in the case of feedback from the same speaker. We\nalso find that it is possible to further condense and align the representations\nto human perception through contrastive learning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4e09\u5143\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u53d1\u73b0\u9891\u8c31\u7279\u5f81\u548c\u81ea\u76d1\u7763\u8bed\u97f3\u8868\u5f81\u5728\u7f16\u7801\u540c\u4e00\u8bf4\u8bdd\u8005\u7684\u53cd\u9988\u526f\u8bed\u8a00\u4fe1\u606f\u65f6\u4f18\u4e8e\u4f20\u7edf\u57fa\u9891\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u8868\u5f81\u4e0e\u4eba\u7c7b\u611f\u77e5\u7684\u5bf9\u9f50\u5ea6\u3002", "motivation": "\u53e3\u8bed\u5bf9\u8bdd\u4e2d\u8bcd\u6c47\u76f8\u540c\u4f46\u526f\u8bed\u8a00\u7279\u5f81\u4e0d\u540c\u7684\u53cd\u9988\uff08\u5982'mhm'\u7684\u4e0d\u540c\u8bed\u8c03\uff09\u5bf9\u8bed\u4e49\u7406\u89e3\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u8bed\u97f3\u8868\u5f81\u6a21\u578b\u5bf9\u8fd9\u7c7b\u526f\u8bed\u8a00\u76f8\u4f3c\u5ea6\u7684\u7f16\u7801\u80fd\u529b\u5c1a\u672a\u660e\u786e\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u6570\u636e\u96c6\u4e2d\u7684\u8bed\u97f3\u53cd\u9988\u6837\u672c\uff0c\u62db\u52df\u53c2\u4e0e\u8005\u8fdb\u884c\u4e09\u5143\u5bf9\u6bd4\u4efb\u52a1\uff0c\u6bd4\u8f83\u9891\u8c31\u7279\u5f81\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u8868\u5f81\uff08\u5982wav2vec\uff09\u548c\u57fa\u9891\u7279\u5f81\u7684\u8868\u5f81\u80fd\u529b\u3002", "result": "\u540c\u4e00\u8bf4\u8bdd\u8005\u7684\u53cd\u9988\u4e2d\uff0c\u9891\u8c31\u548c\u81ea\u76d1\u7763\u6a21\u578b\uff08\u5c24\u4ee5wav2vec\u6700\u4f73\uff09\u7684\u805a\u7c7b\u7ed3\u679c\u4e0e\u4eba\u7c7b\u611f\u77e5\u76f8\u4f3c\u5ea6\u663e\u8457\u76f8\u5173\uff08Spearman \u03c1=0.45\uff09\uff0c\u4f18\u4e8e\u57fa\u9891\u7279\u5f81\uff08\u03c1=0.17\uff09\u3002\u5bf9\u6bd4\u5b66\u4e60\u53ef\u5c06\u8868\u5f81\u7a7a\u95f4\u538b\u7f2964%\u540c\u65f6\u4fdd\u6301\u611f\u77e5\u5bf9\u9f50\u3002", "conclusion": "\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u4e0e\u5bf9\u6bd4\u5b66\u4e60\u80fd\u6709\u6548\u6355\u6349\u526f\u8bed\u8a00\u76f8\u4f3c\u6027\uff0c\u4e3a\u6784\u5efa\u66f4\u81ea\u7136\u7684\u5bf9\u8bdd\u7cfb\u7edf\u53cd\u9988\u673a\u5236\u63d0\u4f9b\u4e86\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2505.13271", "pdf": "https://arxiv.org/pdf/2505.13271", "abs": "https://arxiv.org/abs/2505.13271", "authors": ["Lei Sheng", "Shuai-Shuai Xu"], "title": "CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning", "categories": ["cs.CL"], "comment": "11 pages, 5 figures", "summary": "Large language models (LLMs) have demonstrated strong capabilities in\ntranslating natural language questions about relational databases into SQL\nqueries. In particular, test-time scaling techniques such as Self-Consistency\nand Self-Correction can enhance SQL generation accuracy by increasing\ncomputational effort during inference. However, these methods have notable\nlimitations: Self-Consistency may select suboptimal outputs despite majority\nvotes, while Self-Correction typically addresses only syntactic errors. To\nleverage the strengths of both approaches, we propose CSC-SQL, a novel method\nthat integrates Self-Consistency and Self-Correction. CSC-SQL selects the two\nmost frequently occurring outputs from parallel sampling and feeds them into a\nmerge revision model for correction. Additionally, we employ the Group Relative\nPolicy Optimization (GRPO) algorithm to fine-tune both the SQL generation and\nrevision models via reinforcement learning, significantly enhancing output\nquality. Experimental results confirm the effectiveness and generalizability of\nCSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution\naccuracy, while the 7B model achieves 69.19%. The code will be open sourced at\nhttps://github.com/CycloneBoy/csc_sql.", "AI": {"tldr": "CSC-SQL\u901a\u8fc7\u6574\u5408\u81ea\u6d3d\u6027\u91c7\u6837\u548c\u81ea\u6821\u6b63\u673a\u5236\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\uff0c\u663e\u8457\u63d0\u5347SQL\u751f\u6210\u8d28\u91cf\uff083B\u6a21\u578b65.28%\uff0c7B\u6a21\u578b69.19%\u6267\u884c\u51c6\u786e\u7387\uff09", "motivation": "\u73b0\u6709\u81ea\u6d3d\u6027\u91c7\u6837\u53ef\u80fd\u9009\u62e9\u6b21\u4f18\u7ed3\u679c\uff0c\u81ea\u6821\u6b63\u673a\u5236\u4ec5\u80fd\u5904\u7406\u8bed\u6cd5\u9519\u8bef\uff0c\u9700\u6574\u5408\u4e24\u8005\u4f18\u52bf\u5b9e\u73b0\u66f4\u53ef\u9760\u7684SQL\u751f\u6210", "method": "1. \u5e76\u884c\u91c7\u6837\u9009\u62e9\u9ad8\u9891\u8f93\u51fa\u5bf9\u8fdb\u884c\u5408\u5e76\u6821\u6b63 2. \u4f7f\u7528GRPO\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u540c\u6b65\u4f18\u5316\u751f\u6210\u548c\u4fee\u8ba2\u6a21\u578b", "result": "BIRD\u5f00\u53d1\u96c6\u4e0a3B/7B\u6a21\u578b\u5206\u522b\u8fbe\u523065.28%/69.19%\u6267\u884c\u51c6\u786e\u7387\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u5728https://github.com/CycloneBoy/csc_sql", "conclusion": "CSC-SQL\u901a\u8fc7\u534f\u540c\u4f18\u5316\u673a\u5236\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u663e\u8457\u6548\u679c\u548c\u826f\u597d\u6cdb\u5316\u80fd\u529b"}}
{"id": "2505.13282", "pdf": "https://arxiv.org/pdf/2505.13282", "abs": "https://arxiv.org/abs/2505.13282", "authors": ["Sahil Mishra", "Kumar Arjun", "Tanmoy Chakraborty"], "title": "$\\textit{Rank, Chunk and Expand}$: Lineage-Oriented Reasoning for Taxonomy Expansion", "categories": ["cs.CL"], "comment": null, "summary": "Taxonomies are hierarchical knowledge graphs crucial for recommendation\nsystems, and web applications. As data grows, expanding taxonomies is\nessential, but existing methods face key challenges: (1) discriminative models\nstruggle with representation limits and generalization, while (2) generative\nmethods either process all candidates at once, introducing noise and exceeding\ncontext limits, or discard relevant entities by selecting noisy candidates. We\npropose LORex ($\\textbf{L}$ineage-$\\textbf{O}$riented $\\textbf{Re}$asoning for\nTaxonomy E$\\textbf{x}$pansion), a plug-and-play framework that combines\ndiscriminative ranking and generative reasoning for efficient taxonomy\nexpansion. Unlike prior methods, LORex ranks and chunks candidate terms into\nbatches, filtering noise and iteratively refining selections by reasoning\ncandidates' hierarchy to ensure contextual efficiency. Extensive experiments\nacross four benchmarks and twelve baselines show that LORex improves accuracy\nby 12% and Wu & Palmer similarity by 5% over state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51faLORex\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5224\u522b\u5f0f\u6392\u5e8f\u548c\u751f\u6210\u5f0f\u63a8\u7406\uff0c\u5b9e\u73b0\u9ad8\u6548\u5206\u7c7b\u6cd5\u6269\u5c55\uff0c\u51c6\u786e\u7387\u63d0\u534712%", "motivation": "\u73b0\u6709\u5206\u7c7b\u6cd5\u6269\u5c55\u65b9\u6cd5\u5b58\u5728\u5224\u522b\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3001\u751f\u6210\u65b9\u6cd5\u566a\u58f0\u5904\u7406\u5dee\u7684\u95ee\u9898\uff0c\u9700\u8981\u517c\u987e\u6548\u7387\u4e0e\u51c6\u786e\u6027", "method": "\u5019\u9009\u8bcd\u5206\u5757\u5904\u7406+\u5c42\u6b21\u7ed3\u6784\u8fed\u4ee3\u4f18\u5316\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u566a\u58f0\u8fc7\u6ee4\u4e0e\u4e0a\u4e0b\u6587\u611f\u77e5\u63a8\u7406\u673a\u5236", "result": "\u57284\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e12\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0cWu & Palmer\u76f8\u4f3c\u5ea6\u63d0\u53475%", "conclusion": "LORex\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u5c42\u6b21\u63a8\u7406\u673a\u5236\uff0c\u5728\u4fdd\u6301\u5206\u7c7b\u6cd5\u7ed3\u6784\u5b8c\u6574\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6269\u5c55\u6548\u7387"}}
{"id": "2505.13302", "pdf": "https://arxiv.org/pdf/2505.13302", "abs": "https://arxiv.org/abs/2505.13302", "authors": ["Alice Plebe", "Timothy Douglas", "Diana Riazi", "R. Maria del Rio-Chanona"], "title": "I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models are increasingly integrated into news recommendation\nsystems, raising concerns about their role in spreading misinformation. In\nhumans, visual content is known to boost credibility and shareability of\ninformation, yet its effect on vision-language models (VLMs) remains unclear.\nWe present the first study examining how images influence VLMs' propensity to\nreshare news content, whether this effect varies across model families, and how\npersona conditioning and content attributes modulate this behavior. To support\nthis analysis, we introduce two methodological contributions: a\njailbreaking-inspired prompting strategy that elicits resharing decisions from\nVLMs while simulating users with antisocial traits and political alignments;\nand a multimodal dataset of fact-checked political news from PolitiFact, paired\nwith corresponding images and ground-truth veracity labels. Experiments across\nmodel families reveal that image presence increases resharing rates by 4.8% for\ntrue news and 15.0% for false news. Persona conditioning further modulates this\neffect: Dark Triad traits amplify resharing of false news, whereas\nRepublican-aligned profiles exhibit reduced veracity sensitivity. Of all the\ntested models, only Claude-3-Haiku demonstrates robustness to visual\nmisinformation. These findings highlight emerging risks in multimodal model\nbehavior and motivate the development of tailored evaluation frameworks and\nmitigation strategies for personalized AI systems. Code and dataset are\navailable at: https://github.com/3lis/misinfo_vlm", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u56fe\u50cf\u5b58\u5728\u4f7f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8f6c\u53d1\u865a\u5047\u65b0\u95fb\u7684\u6982\u7387\u63d0\u534715%\uff0c\u4e0d\u540c\u6a21\u578b\u5bb6\u65cf\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4ec5Claude-3-Haiku\u5c55\u73b0\u6297\u8bef\u5bfc\u80fd\u529b", "motivation": "\u63a2\u7a76\u591a\u6a21\u6001\u573a\u666f\u4e0b\u89c6\u89c9\u5185\u5bb9\u5982\u4f55\u5f71\u54cd\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u95fb\u8f6c\u53d1\u884c\u4e3a\uff0c\u5c24\u5176\u662f\u5bf9\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u7684\u6f5c\u5728\u98ce\u9669", "method": "1. \u91c7\u7528\u8d8a\u72f1\u5f0f\u63d0\u793a\u7b56\u7565\u6a21\u62df\u53cd\u793e\u4f1a\u7279\u8d28\u7528\u6237\u7684\u51b3\u7b56\u884c\u4e3a\n2. \u6784\u5efa\u5305\u542b\u4e8b\u5b9e\u6838\u67e5\u7684\u591a\u6a21\u6001\u653f\u6cbb\u65b0\u95fb\u6570\u636e\u96c6\uff08PolitiFact\uff09\n3. \u8de8\u6a21\u578b\u5bb6\u65cf\u6bd4\u8f83\u5b9e\u9a8c\u8bbe\u8ba1", "result": "\u56fe\u50cf\u4f7f\u771f\u5b9e\u65b0\u95fb\u8f6c\u53d1\u7387\u63d0\u53474.8%\uff0c\u865a\u5047\u65b0\u95fb\u63d0\u534715%\uff1b\u9ed1\u6697\u4e09\u4eba\u683c\u7279\u8d28\u5f3a\u5316\u865a\u5047\u65b0\u95fb\u4f20\u64ad\uff1b\u5171\u548c\u515a\u503e\u5411\u7528\u6237\u663e\u793a\u66f4\u4f4e\u7684\u771f\u5b9e\u6027\u654f\u611f\u5ea6", "conclusion": "\u591a\u6a21\u6001\u6a21\u578b\u5b58\u5728\u65b0\u5174\u98ce\u9669\uff0c\u9700\u5f00\u53d1\u5b9a\u5236\u5316\u8bc4\u4f30\u6846\u67b6\u548c\u7f13\u89e3\u7b56\u7565\u5e94\u5bf9\u4e2a\u6027\u5316AI\u7cfb\u7edf\u7684\u5b89\u5168\u9690\u60a3"}}
{"id": "2505.13307", "pdf": "https://arxiv.org/pdf/2505.13307", "abs": "https://arxiv.org/abs/2505.13307", "authors": ["Qiguang Chen", "Libo Qin", "Jinhao Liu", "Yue Liao", "Jiaqi Wang", "Jingxuan Zhou", "Wanxiang Che"], "title": "RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Manuscript", "summary": "Chain-of-Thought (CoT) reasoning has proven effective in enhancing large\nlanguage models (LLMs) on complex tasks, spurring research into its underlying\nmechanisms. However, two primary challenges remain for real-world applications:\n(1) the lack of quantitative metrics and actionable guidelines for evaluating\nand optimizing measurable boundaries of CoT capability, and (2) the absence of\nmethods to assess boundaries of unmeasurable CoT capability, such as multimodal\nperception. To address these gaps, we introduce the Reasoning Boundary\nFramework++ (RBF++). To tackle the first challenge, we define the reasoning\nboundary (RB) as the maximum limit of CoT performance. We also propose a\ncombination law for RBs, enabling quantitative analysis and offering actionable\nguidance across various CoT tasks. For the second challenge, particularly in\nmultimodal scenarios, we introduce a constant assumption, which replaces\nunmeasurable RBs with scenario-specific constants. Additionally, we propose the\nreasoning boundary division mechanism, which divides unmeasurable RBs into two\nsub-boundaries, facilitating the quantification and optimization of both\nunmeasurable domain knowledge and multimodal perception capabilities. Extensive\nexperiments involving 38 models across 13 tasks validate the feasibility of our\nframework in cross-modal settings. Additionally, we evaluate 10 CoT strategies,\noffer insights into optimization and decay from two complementary perspectives,\nand expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope\nthis work advances the understanding of RBs and optimization strategies in\nLLMs. Code and data are available at\nhttps://github.com/LightChen233/reasoning-boundary.", "AI": {"tldr": "\u63d0\u51faRBF++\u6846\u67b6\u89e3\u51b3CoT\u63a8\u7406\u4e2d\u7684\u53ef\u6d4b\u91cf\u8fb9\u754c\u4f18\u5316\u4e0e\u4e0d\u53ef\u6d4b\u91cf\u80fd\u529b\u8bc4\u4f30\u95ee\u9898\uff0c\u901a\u8fc7\u63a8\u7406\u8fb9\u754c\u5b9a\u4e49\u3001\u7ec4\u5408\u5b9a\u5f8b\u548c\u8fb9\u754c\u5212\u5206\u673a\u5236\u5b9e\u73b0\u8de8\u6a21\u6001\u573a\u666f\u5e94\u7528\u3002", "motivation": "\u73b0\u6709CoT\u65b9\u6cd5\u7f3a\u4e4f\u5b9a\u91cf\u8bc4\u4f30\u6307\u6807\uff08\u53ef\u6d4b\u91cf\u8fb9\u754c\u4f18\u5316\uff09\u548c\u4e0d\u53ef\u6d4b\u91cf\u80fd\u529b\uff08\u5982\u591a\u6a21\u6001\u611f\u77e5\uff09\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u963b\u788d\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5b9a\u4e49\u63a8\u7406\u8fb9\u754c(RB)\u4f5c\u4e3a\u6027\u80fd\u4e0a\u9650\uff0c\u63d0\u51fa\u7ec4\u5408\u5b9a\u5f8b\u5b9a\u91cf\u5206\u6790\u4efb\u52a1\uff1b\u9488\u5bf9\u591a\u6a21\u6001\u573a\u666f\u5f15\u5165\u5e38\u6570\u5047\u8bbe\u548c\u8fb9\u754c\u5212\u5206\u673a\u5236\uff0c\u5206\u79bb\u9886\u57df\u77e5\u8bc6\u4e0e\u611f\u77e5\u80fd\u529b\u3002", "result": "38\u4e2a\u6a21\u578b\u572813\u4e2a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\uff0c\u8bc4\u4f3010\u79cdCoT\u7b56\u7565\u5e76\u63d0\u4f9b\u53cc\u5411\u4f18\u5316\u8def\u5f84\uff0c\u6269\u5c55LLM\u63a8\u7406\u8fb9\u754c\u8bc4\u4f30\u57fa\u51c6\u3002", "conclusion": "RBF++\u6846\u67b6\u4e3aLLM\u63a8\u7406\u8fb9\u754c\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u5f00\u6e90\u4ee3\u7801\u6570\u636e\u63a8\u52a8\u540e\u7eed\u7814\u7a76\uff0c\u4fc3\u8fdb\u53ef\u89e3\u91ca\u6027\u63a8\u7406\u80fd\u529b\u4f18\u5316\u3002"}}
{"id": "2505.13312", "pdf": "https://arxiv.org/pdf/2505.13312", "abs": "https://arxiv.org/abs/2505.13312", "authors": ["Zhijie Deng", "Chris Yuhao Liu", "Zirui Pang", "Xinlei He", "Lei Feng", "Qi Xuan", "Zhaowei Zhu", "Jiaheng Wei"], "title": "GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\nmemorizing vast amounts of knowledge across diverse domains. However, the\nability to selectively forget specific knowledge is critical for ensuring the\nsafety and compliance of deployed models. Existing unlearning efforts typically\nfine-tune the model with resources such as forget data, retain data, and a\ncalibration model. These additional gradient steps blur the decision boundary\nbetween forget and retain knowledge, making unlearning often at the expense of\noverall performance. To avoid the negative impact of fine-tuning, it would be\nbetter to unlearn solely at inference time by safely guarding the model against\ngenerating responses related to the forget target, without destroying the\nfluency of text generation. In this work, we propose Generation-time Unlearning\nvia Adaptive Restriction and Detection (GUARD), a framework that enables\ndynamic unlearning during LLM generation. Specifically, we first employ a\nprompt classifier to detect unlearning targets and extract the corresponding\nforbidden token. We then dynamically penalize and filter candidate tokens\nduring generation using a combination of token matching and semantic matching,\neffectively preventing the model from leaking the forgotten content.\nExperimental results on copyright content unlearning tasks over the Harry\nPotter dataset and the MUSE benchmark, as well as entity unlearning tasks on\nthe TOFU dataset, demonstrate that GUARD achieves strong forget quality across\nvarious tasks while causing almost no degradation to the LLM's general\ncapabilities, striking an excellent trade-off between forgetting and utility.", "AI": {"tldr": "\u63d0\u51fa\u4e86GUARD\u6846\u67b6\uff0c\u901a\u8fc7\u5728LLM\u751f\u6210\u9636\u6bb5\u52a8\u6001\u6d88\u9664\u7279\u5b9a\u77e5\u8bc6\uff0c\u907f\u514d\u5fae\u8c03\u5e26\u6765\u7684\u6027\u80fd\u635f\u5931\uff0c\u5b9e\u73b0\u5b89\u5168\u5408\u89c4\u7684\u9057\u5fd8\u3002", "motivation": "\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u4f9d\u8d56\u5fae\u8c03\u4f1a\u6a21\u7cca\u77e5\u8bc6\u8fb9\u754c\u5bfc\u81f4\u6574\u4f53\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u5728\u63a8\u7406\u9636\u6bb5\u5b9e\u73b0\u9009\u62e9\u6027\u9057\u5fd8\u4ee5\u4fdd\u8bc1\u751f\u6210\u5b89\u5168\u6027\u548c\u5408\u89c4\u6027\u3002", "method": "\u7ed3\u5408\u63d0\u793a\u5206\u7c7b\u5668\u68c0\u6d4b\u9057\u5fd8\u76ee\u6807\uff0c\u91c7\u7528token\u5339\u914d\u548c\u8bed\u4e49\u5339\u914d\u53cc\u91cd\u673a\u5236\u52a8\u6001\u8fc7\u6ee4\u5019\u9009token\uff0c\u81ea\u9002\u5e94\u9650\u5236\u654f\u611f\u5185\u5bb9\u751f\u6210\u3002", "result": "\u5728Harry Potter\u7248\u6743\u5185\u5bb9\u3001MUSE\u57fa\u51c6\u548cTOFU\u5b9e\u4f53\u9057\u5fd8\u4efb\u52a1\u4e2d\uff0cGUARD\u5728\u4fdd\u6301\u6a21\u578b\u901a\u7528\u80fd\u529b\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u8d28\u91cf\u9057\u5fd8\u3002", "conclusion": "GUARD\u6846\u67b6\u9996\u6b21\u5728\u751f\u6210\u9636\u6bb5\u5b9e\u73b0\u52a8\u6001\u9057\u5fd8\uff0c\u5728\u9057\u5fd8\u6548\u679c\u4e0e\u6a21\u578b\u6548\u7528\u95f4\u53d6\u5f97\u6700\u4f73\u5e73\u8861\uff0c\u4e3aLLM\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2505.13328", "pdf": "https://arxiv.org/pdf/2505.13328", "abs": "https://arxiv.org/abs/2505.13328", "authors": ["Hongru Wang", "Wenyu Huang", "Yufei Wang", "Yuanhao Xi", "Jianqiao Lu", "Huan Zhang", "Nan Hu", "Zeming Liu", "Jeff Z. Pan", "Kam-Fai Wong"], "title": "Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges", "categories": ["cs.CL"], "comment": null, "summary": "Existing benchmarks that assess Language Models (LMs) as Language Agents\n(LAs) for tool use primarily focus on stateless, single-turn interactions or\npartial evaluations, such as tool selection in a single turn, overlooking the\ninherent stateful nature of interactions in multi-turn applications. To fulfill\nthis gap, we propose \\texttt{DialogTool}, a multi-turn dialogue dataset with\nstateful tool interactions considering the whole life cycle of tool use, across\nsix key tasks in three stages: 1) \\textit{tool creation}; 2) \\textit{tool\nutilization}: tool awareness, tool selection, tool execution; and 3)\n\\textit{role-consistent response}: response generation and role play.\nFurthermore, we build \\texttt{VirtualMobile} -- an embodied virtual mobile\nevaluation environment to simulate API calls and assess the robustness of the\ncreated APIs\\footnote{We will use tools and APIs alternatively, there are no\nsignificant differences between them in this paper.}. Taking advantage of these\nartifacts, we conduct comprehensive evaluation on 13 distinct open- and\nclosed-source LLMs and provide detailed analysis at each stage, revealing that\nthe existing state-of-the-art LLMs still cannot perform well to use tools over\nlong horizons.", "AI": {"tldr": "\u63d0\u51faDialogTool\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6\u548cVirtualMobile\u8bc4\u4f30\u73af\u5883\uff0c\u63ed\u793a\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u6d41\u7a0b\u5de5\u5177\u4f7f\u7528\u4e2d\u7684\u4e0d\u8db3", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5355\u8f6e/\u65e0\u72b6\u6001\u5de5\u5177\u4ea4\u4e92\uff0c\u7f3a\u4e4f\u5bf9\u591a\u8f6e\u5e94\u7528\u4e2d\u6709\u72b6\u6001\u4ea4\u4e92\u7684\u5b8c\u6574\u751f\u547d\u5468\u671f\u8bc4\u4f30", "method": "1) \u6784\u5efa\u5305\u542b\u5de5\u5177\u521b\u5efa\u3001\u4f7f\u7528\uff08\u5de5\u5177\u611f\u77e5/\u9009\u62e9/\u6267\u884c\uff09\u3001\u89d2\u8272\u54cd\u5e94\u4e09\u9636\u6bb5\u516d\u4efb\u52a1\u7684\u8bc4\u4f30\u6846\u67b6 2) \u5f00\u53d1VirtualMobile\u865a\u62df\u73af\u5883\u6a21\u62dfAPI\u8c03\u7528", "result": "\u5bf913\u79cd\u5f00\u6e90/\u95ed\u6e90\u5927\u6a21\u578b\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u73b0\u6709\u5148\u8fdb\u6a21\u578b\u5728\u957f\u671f\u5de5\u5177\u4f7f\u7528\u573a\u666f\u4e2d\u4ecd\u8868\u73b0\u6b20\u4f73", "conclusion": "\u672c\u7814\u7a76\u521b\u5efa\u4e86\u9996\u4e2a\u5b8c\u6574\u8bc4\u4f30\u5de5\u5177\u4f7f\u7528\u751f\u547d\u5468\u671f\u7684\u591a\u8f6e\u5bf9\u8bdd\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u5de5\u5177\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u6539\u8fdb\u65b9\u5411"}}
{"id": "2505.13338", "pdf": "https://arxiv.org/pdf/2505.13338", "abs": "https://arxiv.org/abs/2505.13338", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Tianchi Liu", "Ai Ti Aw"], "title": "Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation", "categories": ["cs.CL", "cs.AI", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Current speech-LLMs exhibit limited capability in contextual reasoning\nalongside paralinguistic understanding, primarily due to the lack of\nQuestion-Answer (QA) datasets that cover both aspects. We propose a novel\nframework for dataset generation from in-the-wild speech data, that integrates\ncontextual reasoning with paralinguistic information. It consists of a pseudo\nparalinguistic label-based data condensation of in-the-wild speech and\nLLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is\nvalidated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct\nmodel on a dataset created by our framework and human-generated CPQA dataset.\nThe results also reveal the speech-LLM's limitations in handling empathetic\nreasoning tasks, highlighting the need for such datasets and more robust\nmodels. The proposed framework is first of its kind and has potential in\ntraining more robust speech-LLMs with paralinguistic reasoning capabilities.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u6574\u5408\u4e0a\u4e0b\u6587\u63a8\u7406\u4e0e\u526f\u8bed\u8a00\u7406\u89e3\u7684\u8bed\u97f3\u6570\u636e\u96c6\u751f\u6210\u6846\u67b6\uff0c\u9a8c\u8bc1\u5176\u5bf9\u63d0\u5347\u8bed\u97f3-\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u7684\u6709\u6548\u6027\uff0c\u5e76\u63ed\u793a\u5f53\u524d\u6a21\u578b\u5728\u540c\u7406\u5fc3\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u526f\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u4e3b\u8981\u7531\u4e8e\u7f3a\u4e4f\u540c\u65f6\u8986\u76d6\u8fd9\u4e24\u4e2a\u7ef4\u5ea6\u7684QA\u6570\u636e\u96c6\u3002", "method": "1. \u57fa\u4e8e\u4f2a\u526f\u8bed\u8a00\u6807\u7b7e\u7684\u91ce\u5916\u8bed\u97f3\u6570\u636e\u6d53\u7f29\n2. \u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u4e0a\u4e0b\u6587\u526f\u8bed\u8a00QA\uff08CPQA\uff09\u751f\u6210\u6846\u67b6", "result": "1. \u6846\u67b6\u751f\u6210\u6570\u636e\u96c6\u4e0e\u4eba\u5de5\u6570\u636e\u96c6\u8bc4\u4f30\u5448\u73b0\u5f3a\u76f8\u5173\u6027\uff08Qwen2-Audio-7B\u6a21\u578b\uff09\n2. \u9996\u6b21\u63ed\u793a\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u540c\u7406\u5fc3\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u663e\u8457\u7f3a\u9677", "conclusion": "\u8be5\u6846\u67b6\u662f\u9886\u57df\u5185\u9996\u521b\u89e3\u51b3\u65b9\u6848\uff0c\u5bf9\u8bad\u7ec3\u5177\u6709\u526f\u8bed\u8a00\u63a8\u7406\u80fd\u529b\u7684\u9c81\u68d2\u8bed\u97f3-\u5927\u8bed\u8a00\u6a21\u578b\u5177\u6709\u91cd\u8981\u5b9e\u8df5\u4ef7\u503c\uff0c\u51f8\u663e\u4e86\u6b64\u7c7b\u6570\u636e\u96c6\u4e0e\u66f4\u5f3a\u6a21\u578b\u9700\u6c42\u7684\u8feb\u5207\u6027\u3002"}}
{"id": "2505.13346", "pdf": "https://arxiv.org/pdf/2505.13346", "abs": "https://arxiv.org/abs/2505.13346", "authors": ["Austin Xu", "Yilun Zhou", "Xuan-Phi Nguyen", "Caiming Xiong", "Shafiq Joty"], "title": "J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization", "categories": ["cs.CL", "cs.AI"], "comment": "25 pages, 4 figures, 6 tables. To be updated with links for\n  code/benchmark", "summary": "To keep pace with the increasing pace of large language models (LLM)\ndevelopment, model output evaluation has transitioned away from time-consuming\nhuman evaluation to automatic evaluation, where LLMs themselves are tasked with\nassessing and critiquing other model outputs. LLM-as-judge models are a class\nof generative evaluators that excel in evaluating relatively simple domains,\nlike chat quality, but struggle in reasoning intensive domains where model\nresponses contain more substantive and challenging content. To remedy existing\njudge shortcomings, we explore training judges with reinforcement learning\n(RL). We make three key contributions: (1) We propose the Equivalent Initial\nState Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us\nto train our judge to be robust to positional biases that arise in more complex\nevaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that\nevaluates judges in diverse reasoning settings not covered by prior work. (3)\nWe train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that\noutperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or\nexceeding the performance of larger GRPO-trained judges on both JudgeBench and\nReasoningJudgeBench.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684EIS-GRPO\u7b97\u6cd5\u8bad\u7ec3J4R\u8bc4\u4f30\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2dLLM\u81ea\u52a8\u8bc4\u4f30\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5efa\u7acb\u65b0\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709LLM\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u51c6\u786e\u7387\u4f4e\u3001\u4f4d\u7f6e\u504f\u5dee\u654f\u611f\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9c81\u68d2\u7684\u8bc4\u4f30\u4f53\u7cfb\u3002", "method": "\u5f00\u53d1EIS-GRPO\u7b97\u6cd5\u589e\u5f3a\u8bc4\u4f30\u6a21\u578b\u6297\u4f4d\u7f6e\u504f\u5dee\u80fd\u529b\uff0c\u6784\u5efaReasoningJudgeBench\u591a\u7ef4\u5ea6\u6d4b\u8bd5\u57fa\u51c6\uff0c\u8bad\u7ec37B\u53c2\u6570\u7684J4R\u6a21\u578b\u3002", "result": "J4R\u5728JudgeBench\u548c\u81ea\u5efa\u57fa\u51c6\u4e0a\u5206\u522b\u8d85\u8d8aGPT-4o 6.7%\u548c\u540c\u7c7b\u6700\u4f18\u5c0f\u6a21\u578b9%\uff0c\u6027\u80fd\u5339\u914d\u66f4\u5927\u89c4\u6a21GRPO\u6a21\u578b\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u8f7b\u91cf\u5316J4R\u6a21\u578b\u663e\u8457\u63d0\u5347\u590d\u6742\u573a\u666f\u8bc4\u4f30\u6548\u679c\uff0c\u4e3a\u81ea\u52a8\u8bc4\u4f30\u9886\u57df\u63d0\u4f9b\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2505.13348", "pdf": "https://arxiv.org/pdf/2505.13348", "abs": "https://arxiv.org/abs/2505.13348", "authors": ["Narek Maloyan", "Bislan Ashinov", "Dmitry Namiot"], "title": "Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly employed as evaluators\n(LLM-as-a-Judge) for assessing the quality of machine-generated text. This\nparadigm offers scalability and cost-effectiveness compared to human\nannotation. However, the reliability and security of such systems, particularly\ntheir robustness against adversarial manipulations, remain critical concerns.\nThis paper investigates the vulnerability of LLM-as-a-Judge architectures to\nprompt-injection attacks, where malicious inputs are designed to compromise the\njudge's decision-making process. We formalize two primary attack strategies:\nComparative Undermining Attack (CUA), which directly targets the final decision\noutput, and Justification Manipulation Attack (JMA), which aims to alter the\nmodel's generated reasoning. Using the Greedy Coordinate Gradient (GCG)\noptimization method, we craft adversarial suffixes appended to one of the\nresponses being compared. Experiments conducted on the MT-Bench Human Judgments\ndataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and\nFalcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves\nan Attack Success Rate (ASR) exceeding 30\\%, while JMA also shows notable\neffectiveness. These findings highlight substantial vulnerabilities in current\nLLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and\nfurther research into adversarial evaluation and trustworthiness in LLM-based\nassessment frameworks.", "AI": {"tldr": "\u5927\u6a21\u578b\u8bc4\u4f30\u7cfb\u7edf\u5b58\u5728\u63d0\u793a\u6ce8\u5165\u6f0f\u6d1e\uff0c\u4e24\u79cd\u653b\u51fb\u7b56\u7565\u663e\u8457\u5f71\u54cd\u5224\u65ad", "motivation": "\u73b0\u6709LLM-as-a-Judge\u7cfb\u7edf\u5728\u5bf9\u6297\u653b\u51fb\u4e0b\u7684\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u7279\u522b\u662f\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u7684\u9632\u5fa1\u673a\u5236\u7f3a\u5931", "method": "\u4f7f\u7528GCG\u4f18\u5316\u65b9\u6cd5\u751f\u6210\u5bf9\u6297\u540e\u7f00\uff0c\u5728MT-Bench\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5CUA\u548cJMA\u4e24\u79cd\u653b\u51fb\u7b56\u7565", "result": "CUA\u653b\u51fb\u6210\u529f\u7387\u8d8530%\uff0cJMA\u5728\u6a21\u578b\u63a8\u7406\u5c42\u9762\u4e5f\u5c55\u73b0\u663e\u8457\u64cd\u63a7\u6548\u679c", "conclusion": "\u5f53\u524dLLM\u8bc4\u4f30\u6846\u67b6\u5b58\u5728\u91cd\u5927\u5b89\u5168\u6f0f\u6d1e\uff0c\u9700\u52a0\u5f3a\u5bf9\u6297\u6027\u8bc4\u4f30\u7814\u7a76\u548c\u9632\u5fa1\u673a\u5236\u5f00\u53d1"}}
{"id": "2505.13353", "pdf": "https://arxiv.org/pdf/2505.13353", "abs": "https://arxiv.org/abs/2505.13353", "authors": ["Adam \u0160torek", "Mukur Gupta", "Samira Hajizadeh", "Prashast Srivastava", "Suman Jana"], "title": "Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning", "categories": ["cs.CL", "cs.LG", "cs.SE"], "comment": null, "summary": "Although modern Large Language Models (LLMs) support extremely large\ncontexts, their effectiveness in utilizing long context for code reasoning\nremains unclear. This paper investigates LLM reasoning ability over code\nsnippets within large repositories and how it relates to their recall ability.\nSpecifically, we differentiate between lexical code recall (verbatim retrieval)\nand semantic code recall (remembering what the code does). To measure semantic\nrecall, we propose SemTrace, a code reasoning technique where the impact of\nspecific statements on output is attributable and unpredictable. We also\npresent a method to quantify semantic recall sensitivity in existing\nbenchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop\nin code reasoning accuracy as a code snippet approaches the middle of the input\ncontext, particularly with techniques requiring high semantic recall like\nSemTrace. Moreover, we find that lexical recall varies by granularity, with\nmodels excelling at function retrieval but struggling with line-by-line recall.\nNotably, a disconnect exists between lexical and semantic recall, suggesting\ndifferent underlying mechanisms. Finally, our findings indicate that current\ncode reasoning benchmarks may exhibit low semantic recall sensitivity,\npotentially underestimating LLM challenges in leveraging in-context\ninformation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0LLMs\u5728\u957f\u4e0a\u4e0b\u6587\u4ee3\u7801\u63a8\u7406\u4e2d\u5b58\u5728\u8bed\u4e49\u56de\u5fc6\u654f\u611f\u6027\u7f3a\u9677\uff0c\u73b0\u6709\u57fa\u51c6\u53ef\u80fd\u4f4e\u4f30\u4e86\u6a21\u578b\u5229\u7528\u4e0a\u4e0b\u6587\u4fe1\u606f\u7684\u6311\u6218", "motivation": "\u63a2\u7a76LLMs\u5728\u5927\u578b\u4ee3\u7801\u5e93\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u4e0e\u4ee3\u7801\u56de\u5fc6\u80fd\u529b\u7684\u5173\u8054\uff0c\u63ed\u793a\u5f53\u524d\u4ee3\u7801\u63a8\u7406\u57fa\u51c6\u7684\u6f5c\u5728\u5c40\u9650\u6027", "method": "\u63d0\u51faSemTrace\u6280\u672f\u91cf\u5316\u8bed\u4e49\u56de\u5fc6\u654f\u611f\u6027\uff0c\u901a\u8fc7\u53ef\u5f52\u56e0\u4f46\u4e0d\u53ef\u9884\u6d4b\u7684\u8bed\u53e5\u5f71\u54cd\u6d4b\u8bd5\u6a21\u578b\uff0c\u5e76\u5f00\u53d1\u8bed\u4e49\u56de\u5fc6\u654f\u611f\u6027\u91cf\u5316\u6846\u67b6", "result": "\u4ee3\u7801\u7247\u6bb5\u4f4d\u7f6e\u663e\u8457\u5f71\u54cd\u63a8\u7406\u7cbe\u5ea6\uff08\u4e2d\u95f4\u4f4d\u7f6e\u4e0b\u964d30%\uff09\uff0c\u8bcd\u6c47/\u8bed\u4e49\u56de\u5fc6\u673a\u5236\u5206\u79bb\uff08\u51fd\u6570\u7ea7\u56de\u5fc6\u6210\u529f\u738776% vs \u884c\u7ea721%\uff09", "conclusion": "\u5f53\u524d\u57fa\u51c6\u7684\u8bed\u4e49\u56de\u5fc6\u654f\u611f\u6027\u4e0d\u8db3\uff0c\u9700\u5f00\u53d1\u66f4\u4e25\u82db\u7684\u8bc4\u4f30\u4f53\u7cfb\u4ee5\u51c6\u786e\u8861\u91cfLLMs\u7684\u4e0a\u4e0b\u6587\u4ee3\u7801\u7406\u89e3\u80fd\u529b"}}
{"id": "2505.13360", "pdf": "https://arxiv.org/pdf/2505.13360", "abs": "https://arxiv.org/abs/2505.13360", "authors": ["Chenyang Yang", "Yike Shi", "Qianou Ma", "Michael Xieyang Liu", "Christian K\u00e4stner", "Tongshuang Wu"], "title": "What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts", "categories": ["cs.CL", "cs.SE"], "comment": null, "summary": "Building LLM-powered software requires developers to communicate their\nrequirements through natural language, but developer prompts are frequently\nunderspecified, failing to fully capture many user-important requirements. In\nthis paper, we present an in-depth analysis of prompt underspecification,\nshowing that while LLMs can often (41.1%) guess unspecified requirements by\ndefault, such behavior is less robust: Underspecified prompts are 2x more\nlikely to regress over model or prompt changes, sometimes with accuracy drops\nby more than 20%. We then demonstrate that simply adding more requirements to a\nprompt does not reliably improve performance, due to LLMs' limited\ninstruction-following capabilities and competing constraints, and standard\nprompt optimizers do not offer much help. To address this, we introduce novel\nrequirements-aware prompt optimization mechanisms that can improve performance\nby 4.8% on average over baselines that naively specify everything in the\nprompt. Beyond prompt optimization, we envision that effectively managing\nprompt underspecification requires a broader process, including proactive\nrequirements discovery, evaluation, and monitoring.", "AI": {"tldr": "\u8bba\u6587\u63ed\u793a\u4e86LLM\u63d0\u793a\u5de5\u7a0b\u4e2d\u9700\u6c42\u63cf\u8ff0\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u57fa\u4e8e\u9700\u6c42\u611f\u77e5\u7684\u63d0\u793a\u4f18\u5316\u673a\u5236\uff0c\u5e73\u5747\u63d0\u53474.8%\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u8005\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u9700\u6c42\u65f6\u5b58\u5728\u663e\u8457\u7684\u4e0d\u5145\u5206\u6027\uff0841.1%\u672a\u660e\u786e\u9700\u6c42\uff09\uff0c\u5bfc\u81f4LLM\u8f93\u51fa\u4e0d\u7a33\u5b9a\uff08\u6a21\u578b\u53d8\u66f4\u65f6\u51c6\u786e\u7387\u4e0b\u964d20%\uff09\u4e14\u4f20\u7edf\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u5931\u6548\u3002", "method": "\u901a\u8fc7\u91cf\u5316\u5206\u6790\u63d0\u793a\u4e0d\u5145\u5206\u6027\u5bf9\u6a21\u578b\u9c81\u68d2\u6027\u7684\u5f71\u54cd\uff082\u500d\u56de\u5f52\u6982\u7387\uff09\uff0c\u6d4b\u8bd5\u4e0d\u540c\u63d0\u793a\u6269\u5c55\u7b56\u7565\uff0c\u6700\u7ec8\u8bbe\u8ba1\u9700\u6c42\u611f\u77e5\u7684\u63d0\u793a\u4f18\u5316\u6846\u67b6\u3002", "result": "\u65b0\u4f18\u5316\u673a\u5236\u76f8\u6bd4\u57fa\u7ebf\u63d0\u53474.8%\u6027\u80fd\uff0c\u4f20\u7edf\u65b9\u6cd5\u56e0\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u9650\u5236\u548c\u7ea6\u675f\u51b2\u7a81\u5931\u6548\u3002", "conclusion": "\u5efa\u8bae\u6784\u5efa\u5305\u542b\u9700\u6c42\u53d1\u73b0-\u8bc4\u4f30-\u76d1\u63a7\u7684\u5168\u6d41\u7a0b\u7ba1\u7406\u4f53\u7cfb\uff0c\u7a81\u7834\u5355\u7eaf\u63d0\u793a\u4f18\u5316\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2505.13379", "pdf": "https://arxiv.org/pdf/2505.13379", "abs": "https://arxiv.org/abs/2505.13379", "authors": ["Gongfan Fang", "Xinyin Ma", "Xinchao Wang"], "title": "Thinkless: LLM Learns When to Think", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reasoning Language Models, capable of extended chain-of-thought reasoning,\nhave demonstrated remarkable performance on tasks requiring complex logical\ninference. However, applying elaborate reasoning for all queries often results\nin substantial computational inefficiencies, particularly when many problems\nadmit straightforward solutions. This motivates an open question: Can LLMs\nlearn when to think? To answer this, we propose Thinkless, a learnable\nframework that empowers an LLM to adaptively select between short-form and\nlong-form reasoning, based on both task complexity and the model's ability.\nThinkless is trained under a reinforcement learning paradigm and employs two\ncontrol tokens, <short> for concise responses and <think> for detailed\nreasoning. At the core of our method is a Decoupled Group Relative Policy\nOptimization (DeGRPO) algorithm, which decomposes the learning objective of\nhybrid reasoning into two components: (1) a control token loss that governs the\nselection of the reasoning mode, and (2) a response loss that improves the\naccuracy of the generated answers. This decoupled formulation enables\nfine-grained control over the contributions of each objective, stabilizing\ntraining and effectively preventing collapse observed in vanilla GRPO.\nEmpirically, on several benchmarks such as Minerva Algebra, MATH-500, and\nGSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% -\n90%, significantly improving the efficiency of Reasoning Language Models. The\ncode is available at https://github.com/VainF/Thinkless", "AI": {"tldr": "\u63d0\u51faThinkless\u6846\u67b6\uff0c\u5141\u8bb8\u8bed\u8a00\u6a21\u578b\u6839\u636e\u4efb\u52a1\u590d\u6742\u6027\u81ea\u9002\u5e94\u9009\u62e9\u957f/\u77ed\u63a8\u7406\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387", "motivation": "\u73b0\u6709\u590d\u6742\u63a8\u7406\u6a21\u578b\u5728\u5904\u7406\u7b80\u5355\u95ee\u9898\u65f6\u5b58\u5728\u8ba1\u7b97\u4f4e\u6548\u95ee\u9898\uff0c\u9700\u8981\u8ba9\u6a21\u578b\u5b66\u4f1a\u81ea\u9002\u5e94\u9009\u62e9\u63a8\u7406\u6a21\u5f0f", "method": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u91c7\u7528<short>\u548c<long>\u63a7\u5236\u7b26\uff0c\u901a\u8fc7\u89e3\u8026\u7684DeGRPO\u7b97\u6cd5\u5206\u522b\u4f18\u5316\u63a8\u7406\u6a21\u5f0f\u9009\u62e9\u4e0e\u7b54\u6848\u51c6\u786e\u6027", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u51cf\u5c1150%-90%\u7684\u957f\u94fe\u63a8\u7406\u4f7f\u7528\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387", "conclusion": "Thinkless\u6846\u67b6\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u6709\u6548\u5e73\u8861\u6548\u7387\uff0c\u8bc1\u660e\u81ea\u9002\u5e94\u63a8\u7406\u673a\u5236\u7684\u53ef\u884c\u6027"}}
{"id": "2505.13388", "pdf": "https://arxiv.org/pdf/2505.13388", "abs": "https://arxiv.org/abs/2505.13388", "authors": ["David Anugraha", "Zilu Tang", "Lester James V. Miranda", "Hanyang Zhao", "Mohammad Rifqi Farhansyah", "Garry Kuwanto", "Derry Wijaya", "Genta Indra Winata"], "title": "R3: Robust Rubric-Agnostic Reward Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint", "summary": "Reward models are essential for aligning language model outputs with human\npreferences, yet existing approaches often lack both controllability and\ninterpretability. These models are typically optimized for narrow objectives,\nlimiting their generalizability to broader downstream tasks. Moreover, their\nscalar outputs are difficult to interpret without contextual reasoning. To\naddress these limitations, we introduce R3, a novel reward modeling framework\nthat is rubric-agnostic, generalizable across evaluation dimensions, and\nprovides interpretable, reasoned score assignments. R3 enables more transparent\nand flexible evaluation of language models, supporting robust alignment with\ndiverse human values and use cases. Our models, data, and code are available as\nopen source at https://github.com/rubricreward/r3", "AI": {"tldr": "\u63d0\u51faR3\u6846\u67b6\u89e3\u51b3\u4f20\u7edf\u5956\u52b1\u6a21\u578b\u53ef\u63a7\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u8bc4\u4f30\u5b9e\u73b0\u900f\u660e\u5316\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u53d7\u9650\u4e8e\u5355\u4e00\u4f18\u5316\u76ee\u6807\uff0c\u6807\u91cf\u8f93\u51fa\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u4eba\u7c7b\u4ef7\u503c\u89c2\u548c\u5e94\u7528\u573a\u666f", "method": "\u5f00\u53d1rubric-agnostic\u7684R3\u6846\u67b6\uff0c\u652f\u6301\u8de8\u8bc4\u4f30\u7ef4\u5ea6\u7684\u901a\u7528\u6027\uff0c\u63d0\u4f9b\u57fa\u4e8e\u63a8\u7406\u7684\u6a21\u5757\u5316\u8bc4\u5206\u7cfb\u7edf", "result": "R3\u5b9e\u73b0\u8bed\u8a00\u6a21\u578b\u7684\u900f\u660e\u8bc4\u4f30\uff0c\u5f00\u6e90\u6a21\u578b/\u6570\u636e/\u4ee3\u7801\u9a8c\u8bc1\u6846\u67b6\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e0d\u540c\u4f7f\u7528\u573a\u666f\u7684\u9002\u5e94\u6027", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u53ef\u89e3\u91ca\u7684\u5206\u6570\u63a8\u7406\u673a\u5236\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u9c81\u68d2\u7684\u8bc4\u4f30\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.13403", "pdf": "https://arxiv.org/pdf/2505.13403", "abs": "https://arxiv.org/abs/2505.13403", "authors": ["Renjie Pi", "Felix Bai", "Qibin Chen", "Simon Wang", "Jiulong Shan", "Kieran Liu", "Meng Cao"], "title": "MR. Judge: Multimodal Reasoner as a Judge", "categories": ["cs.CL"], "comment": null, "summary": "The paradigm of using Large Language Models (LLMs) and Multimodal Large\nLanguage Models (MLLMs) as evaluative judges has emerged as an effective\napproach in RLHF and inference-time scaling. In this work, we propose\nMultimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering\ngeneral-purpose MLLMs judges with strong reasoning capabilities. Instead of\ndirectly assigning scores for each response, we formulate the judgement process\nas a reasoning-inspired multiple-choice problem. Specifically, the judge model\nfirst conducts deliberate reasoning covering different aspects of the responses\nand eventually selects the best response from them. This reasoning process not\nonly improves the interpretibility of the judgement, but also greatly enhances\nthe performance of MLLM judges. To cope with the lack of questions with scored\nresponses, we propose the following strategy to achieve automatic annotation:\n1) Reverse Response Candidates Synthesis: starting from a supervised\nfine-tuning (SFT) dataset, we treat the original response as the best candidate\nand prompt the MLLM to generate plausible but flawed negative candidates. 2)\nText-based reasoning extraction: we carefully design a data synthesis pipeline\nfor distilling the reasoning capability from a text-based reasoning model,\nwhich is adopted to enable the MLLM judges to regain complex reasoning ability\nvia warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge\nis effective across a wide range of tasks. Specifically, our MR. Judge-7B\nsurpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet\nduring inference-time scaling by up to 7.7%.", "AI": {"tldr": "\u63d0\u51faMR.Judge\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8bc4\u4f30\u8fc7\u7a0b\u91cd\u6784\u4e3a\u591a\u9009\u63a8\u7406\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u6a21\u578b\u8bc4\u4f30\u6027\u80fd", "motivation": "\u73b0\u6709\u76f4\u63a5\u8bc4\u5206\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u548c\u8bc4\u4f30\u6548\u679c\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u9700\u8981\u589e\u5f3aMLLM\u8bc4\u4f30\u8005\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b", "method": "1) \u53cd\u5411\u751f\u6210\u7f3a\u9677\u5019\u9009\u7b54\u6848 2) \u4ece\u6587\u672c\u63a8\u7406\u6a21\u578b\u84b8\u998f\u63a8\u7406\u80fd\u529b 3) \u6784\u5efa\u591a\u9009\u8bc4\u4f30\u6846\u67b6", "result": "MR.Judge-7B\u5728VL-RewardBench\u8d85\u8d8aGPT-4o\u8fbe9.9%\uff0cMM-Vet\u4efb\u52a1\u63a8\u7406\u65f6\u6027\u80fd\u63d0\u53477.7%", "conclusion": "\u63a8\u7406\u9a71\u52a8\u7684\u591a\u9009\u8303\u5f0f\u6709\u6548\u63d0\u5347MLLM\u8bc4\u4f30\u6027\u80fd\uff0c\u540c\u65f6\u589e\u5f3a\u5224\u65ad\u8fc7\u7a0b\u7684\u53ef\u89e3\u91ca\u6027"}}
{"id": "2505.13404", "pdf": "https://arxiv.org/pdf/2505.13404", "abs": "https://arxiv.org/abs/2505.13404", "authors": ["Nithin Rao Koluguri", "Monica Sekoyan", "George Zelenfroynd", "Sasha Meister", "Shuoyang Ding", "Sofia Kostandian", "He Huang", "Nikolay Karpov", "Jagadeesh Balam", "Vitaly Lavrukhin", "Yifan Peng", "Sara Papi", "Marco Gaido", "Alessio Brutti", "Boris Ginsburg"], "title": "Granary: Speech Recognition and Translation Dataset in 25 European Languages", "categories": ["cs.CL", "eess.AS"], "comment": "Accepted at Interspeech 2025", "summary": "Multi-task and multilingual approaches benefit large models, yet speech\nprocessing for low-resource languages remains underexplored due to data\nscarcity. To address this, we present Granary, a large-scale collection of\nspeech datasets for recognition and translation across 25 European languages.\nThis is the first open-source effort at this scale for both transcription and\ntranslation. We enhance data quality using a pseudo-labeling pipeline with\nsegmentation, two-pass inference, hallucination filtering, and punctuation\nrestoration. We further generate translation pairs from pseudo-labeled\ntranscriptions using EuroLLM, followed by a data filtration pipeline. Designed\nfor efficiency, our pipeline processes vast amount of data within hours. We\nassess models trained on processed data by comparing their performance on\npreviously curated datasets for both high- and low-resource languages. Our\nfindings show that these models achieve similar performance using approx. 50%\nless data. Dataset will be made available at\nhttps://hf.co/datasets/nvidia/Granary", "AI": {"tldr": "Granary\u63a8\u51fa\u9996\u4e2a\u5f00\u6e90\u591a\u8bed\u8a00\u8bed\u97f3\u6570\u636e\u96c6\uff0c\u8986\u76d625\u79cd\u6b27\u6d32\u8bed\u8a00\u7684\u8bc6\u522b\u4e0e\u7ffb\u8bd1\u4efb\u52a1\uff0c\u901a\u8fc7\u4f2a\u6807\u6ce8\u548c\u667a\u80fd\u6570\u636e\u7b5b\u9009\u6280\u672f\u4f7f\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u91cf\u51cf\u5c1150%\u4ecd\u4fdd\u6301\u540c\u7b49\u6027\u80fd", "motivation": "\u9488\u5bf9\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u8bed\u97f3\u5904\u7406\u9886\u57df\u6570\u636e\u532e\u4e4f\u7684\u95ee\u9898\uff0c\u73b0\u6709\u591a\u4efb\u52a1\u591a\u8bed\u8a00\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u3002\u4f20\u7edf\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u800c\u5c0f\u8bed\u79cd\u6570\u636e\u83b7\u53d6\u56f0\u96be\u5236\u7ea6\u4e86\u6a21\u578b\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u5305\u542b\u4e24\u9636\u6bb5\u63a8\u7406\u3001\u5e7b\u89c9\u8fc7\u6ee4\u7684\u4f2a\u6807\u6ce8\u6d41\u6c34\u7ebf\uff0c\u7ed3\u5408EuroLLM\u751f\u6210\u7ffb\u8bd1\u5bf9\u5e76\u8fc7\u6ee4\u3002\u521b\u65b0\u6027\u5730\u5b9e\u73b0\u6807\u70b9\u6062\u590d\u4e0e\u9ad8\u6548\u6570\u636e\u5904\u7406\uff08\u6570\u5c0f\u65f6\u5185\u5904\u7406\u6d77\u91cf\u6570\u636e\uff09", "result": "\u5728\u9ad8\u4f4e\u8d44\u6e90\u8bed\u8a00\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u4f7f\u7528Granary\u5904\u7406\u6570\u636e\u7684\u6a21\u578b\u4ec5\u9700\u7ea650%\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u8fbe\u5230\u4e0e\u4f20\u7edf\u5168\u91cf\u6570\u636e\u76f8\u5f53\u7684\u8bc6\u522b\u7ffb\u8bd1\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6848\u901a\u8fc7\u7cfb\u7edf\u7ea7\u6570\u636e\u589e\u5f3a\u6280\u672f\u7a81\u7834\u6570\u636e\u74f6\u9888\uff0c\u4e3a\u4f4e\u8d44\u6e90\u8bed\u8a00\u8bed\u97f3\u5904\u7406\u63d0\u4f9b\u65b0\u8303\u5f0f\u3002\u5f00\u653e\u6570\u636e\u96c6\u5c06\u63a8\u52a8\u591a\u8bed\u8a00\u8bed\u97f3\u6280\u672f\u6c11\u4e3b\u5316\u53d1\u5c55"}}
{"id": "2505.13417", "pdf": "https://arxiv.org/pdf/2505.13417", "abs": "https://arxiv.org/abs/2505.13417", "authors": ["Jiajie Zhang", "Nianyi Lin", "Lei Hou", "Ling Feng", "Juanzi Li"], "title": "AdaptThink: Reasoning Models Can Learn When to Think", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recently, large reasoning models have achieved impressive performance on\nvarious tasks by employing human-like deep thinking. However, the lengthy\nthinking process substantially increases inference overhead, making efficiency\na critical bottleneck. In this work, we first demonstrate that NoThinking,\nwhich prompts the reasoning model to skip thinking and directly generate the\nfinal solution, is a better choice for relatively simple tasks in terms of both\nperformance and efficiency. Motivated by this, we propose AdaptThink, a novel\nRL algorithm to teach reasoning models to choose the optimal thinking mode\nadaptively based on problem difficulty. Specifically, AdaptThink features two\ncore components: (1) a constrained optimization objective that encourages the\nmodel to choose NoThinking while maintaining the overall performance; (2) an\nimportance sampling strategy that balances Thinking and NoThinking samples\nduring on-policy training, thereby enabling cold start and allowing the model\nto explore and exploit both thinking modes throughout the training process. Our\nexperiments indicate that AdaptThink significantly reduces the inference costs\nwhile further enhancing performance. Notably, on three math datasets,\nAdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B\nby 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive\nthinking-mode selection for optimizing the balance between reasoning quality\nand efficiency. Our codes and models are available at\nhttps://github.com/THU-KEG/AdaptThink.", "AI": {"tldr": "\u63d0\u51fa\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5AdaptThink\uff0c\u6839\u636e\u95ee\u9898\u96be\u5ea6\u81ea\u9002\u5e94\u9009\u62e9\u601d\u7ef4\u6a21\u5f0f\uff0c\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u63a8\u7406\u6210\u672c", "motivation": "\u5927\u6a21\u578b\u6df1\u5ea6\u601d\u8003\u8fc7\u7a0b\u5bfc\u81f4\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\uff0c\u7814\u7a76\u53d1\u73b0\u7b80\u5355\u4efb\u52a1\u76f4\u63a5\u8f93\u51fa\u65b9\u6848\uff08NoThinking\uff09\u6548\u679c\u66f4\u4f18\uff0c\u9700\u5efa\u7acb\u52a8\u6001\u9009\u62e9\u673a\u5236\u5e73\u8861\u6548\u7387\u4e0e\u8d28\u91cf", "method": "\u5305\u542b\u7ea6\u675f\u4f18\u5316\u76ee\u6807\uff08\u9f13\u52b1NoThinking\u4e14\u4fdd\u6301\u6027\u80fd\uff09\u548c\u91cd\u8981\u6027\u91c7\u6837\u7b56\u7565\uff08\u5e73\u8861\u8bad\u7ec3\u6837\u672c\uff09\uff0c\u652f\u6301\u51b7\u542f\u52a8\u5e76\u5b9e\u73b0\u601d\u7ef4\u6a21\u5f0f\u7684\u6301\u7eed\u63a2\u7d22", "result": "\u5728\u6570\u5b66\u6570\u636e\u96c6\u4e0a\u4f7fDeepSeek-R1\u6a21\u578b\u54cd\u5e94\u957f\u5ea6\u51cf\u5c1153%\uff0c\u51c6\u786e\u7387\u63d0\u53472.4%", "conclusion": "AdaptThink\u6210\u529f\u4f18\u5316\u63a8\u7406\u8d28\u91cf\u4e0e\u6548\u7387\u7684\u5e73\u8861\uff0c\u81ea\u9002\u5e94\u601d\u7ef4\u6a21\u5f0f\u9009\u62e9\u673a\u5236\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c"}}
{"id": "2505.13418", "pdf": "https://arxiv.org/pdf/2505.13418", "abs": "https://arxiv.org/abs/2505.13418", "authors": ["Lotem Peled-Cohen", "Maya Zadok", "Nitay Calderon", "Hila Gonen", "Roi Reichart"], "title": "Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Cognitive decline often surfaces in language years before diagnosis. It is\nfrequently non-experts, such as those closest to the patient, who first sense a\nchange and raise concern. As LLMs become integrated into daily communication\nand used over prolonged periods, it may even be an LLM that notices something\nis off. But what exactly do they notice--and should be noticing--when making\nthat judgment? This paper investigates how dementia is perceived through\nlanguage by non-experts. We presented transcribed picture descriptions to\nnon-expert humans and LLMs, asking them to intuitively judge whether each text\nwas produced by someone healthy or with dementia. We introduce an explainable\nmethod that uses LLMs to extract high-level, expert-guided features\nrepresenting these picture descriptions, and use logistic regression to model\nhuman and LLM perceptions and compare with clinical diagnoses. Our analysis\nreveals that human perception of dementia is inconsistent and relies on a\nnarrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a\nricher, more nuanced feature set that aligns more closely with clinical\npatterns. Still, both groups show a tendency toward false negatives, frequently\noverlooking dementia cases. Through our interpretable framework and the\ninsights it provides, we hope to help non-experts better recognize the\nlinguistic signs that matter.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u6846\u67b6\u6bd4\u8f83\u4eba\u7c7b\u975e\u4e13\u5bb6\u4e0eLLM\u5bf9\u75f4\u5446\u75c7\u8bed\u8a00\u7279\u5f81\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u53d1\u73b0LLM\u4f7f\u7528\u66f4\u63a5\u8fd1\u4e34\u5e8a\u8bca\u65ad\u7684\u591a\u5143\u7279\u5f81\uff0c\u4f46\u4e24\u8005\u5747\u5b58\u5728\u6f0f\u8bca\u503e\u5411", "motivation": "\u9488\u5bf9\u975e\u4e13\u4e1a\u4eba\u58eb\u4f9d\u8d56\u7247\u9762\u7ebf\u7d22\u5224\u65ad\u75f4\u5446\u75c7\u8bed\u8a00\u7279\u5f81\u7684\u95ee\u9898\uff0c\u63a2\u7d22LLM\u5728\u65e9\u671f\u8bc6\u522b\u4e2d\u7684\u8865\u5145\u4ef7\u503c\u53ca\u53ef\u89e3\u91ca\u6027\u5206\u6790\u6846\u67b6\u7684\u5e94\u7528\u6f5c\u529b", "method": "\u901a\u8fc7\u5c55\u793a\u56fe\u7247\u63cf\u8ff0\u6587\u672c\uff0c\u6536\u96c6\u4eba\u7c7b\u548cLLM\u7684\u76f4\u89c9\u5224\u65ad\uff0c\u4f7f\u7528LLM\u63d0\u53d6\u4e13\u5bb6\u6307\u5bfc\u7684\u9ad8\u7ef4\u7279\u5f81\uff0c\u91c7\u7528\u903b\u8f91\u56de\u5f52\u5efa\u6a21\u6bd4\u8f83\u5176\u4e0e\u4e34\u5e8a\u8bca\u65ad\u7684\u5173\u8054\u6027", "result": "\u4eba\u7c7b\u5224\u65ad\u4f9d\u8d56\u6709\u9650\u4e14\u5b58\u5728\u8bef\u5bfc\u6027\u7684\u7ebf\u7d22\uff0cLLM\u5219\u8fd0\u7528\u66f4\u4e30\u5bcc\u3001\u7ec6\u81f4\u7684\u7279\u5f81\u4f53\u7cfb\uff08\u4e0e\u4e34\u5e8a\u6a21\u5f0f\u66f4\u5951\u5408\uff09\uff0c\u4f46\u4e24\u8005\u5747\u8868\u73b0\u51fa\u8f83\u9ad8\u7684\u5047\u9634\u6027\u7387", "conclusion": "\u5f00\u53d1\u7684\u53ef\u89e3\u91ca\u6846\u67b6\u6709\u52a9\u4e8e\u63d0\u5347\u975e\u4e13\u4e1a\u4eba\u58eb\u5bf9\u5173\u952e\u8bed\u8a00\u7279\u5f81\u7684\u8bc6\u522b\u80fd\u529b\uff0c\u672a\u6765\u9700\u7ed3\u5408\u591a\u65b9\u5224\u65ad\u4f53\u7cfb\u6539\u5584\u65e9\u671f\u75f4\u5446\u7b5b\u67e5\u6548\u679c"}}
{"id": "2505.13434", "pdf": "https://arxiv.org/pdf/2505.13434", "abs": "https://arxiv.org/abs/2505.13434", "authors": ["Mateusz Bystro\u0144ski", "Miko\u0142aj Ho\u0142ysz", "Grzegorz Piotrowski", "Nitesh V. Chawla", "Tomasz Kajdanowicz"], "title": "SMOTExT: SMOTE meets Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Data scarcity and class imbalance are persistent challenges in training\nrobust NLP models, especially in specialized domains or low-resource settings.\nWe propose a novel technique, SMOTExT, that adapts the idea of Synthetic\nMinority Over-sampling (SMOTE) to textual data. Our method generates new\nsynthetic examples by interpolating between BERT-based embeddings of two\nexisting examples and then decoding the resulting latent point into text with\nxRAG architecture. By leveraging xRAG's cross-modal retrieval-generation\nframework, we can effectively turn interpolated vectors into coherent text.\nWhile this is preliminary work supported by qualitative outputs only, the\nmethod shows strong potential for knowledge distillation and data augmentation\nin few-shot settings. Notably, our approach also shows promise for\nprivacy-preserving machine learning: in early experiments, training models\nsolely on generated data achieved comparable performance to models trained on\nthe original dataset. This suggests a viable path toward safe and effective\nlearning under data protection constraints.", "AI": {"tldr": "\u63d0\u51faSMOTExT\u65b9\u6cd5\uff0c\u5c06SMOTE\u6280\u672f\u5e94\u7528\u4e8e\u6587\u672c\u6570\u636e\u751f\u6210\uff0c\u901a\u8fc7BERT\u5d4c\u5165\u63d2\u503c\u4e0exRAG\u89e3\u7801\u5b9e\u73b0\u5c0f\u6837\u672c\u573a\u666f\u7684\u6570\u636e\u589e\u5f3a\u53ca\u9690\u79c1\u4fdd\u62a4", "motivation": "\u89e3\u51b3\u4e13\u4e1a\u9886\u57df/\u4f4e\u8d44\u6e90\u573a\u666f\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u540c\u65f6\u63a2\u7d22\u9690\u79c1\u4fdd\u62a4\u673a\u5668\u5b66\u4e60\u65b0\u8def\u5f84", "method": "\u7ed3\u5408SMOTE\u601d\u60f3\uff1a1) \u5bf9BERT\u5d4c\u5165\u5411\u91cf\u8fdb\u884c\u63d2\u503c 2) \u901a\u8fc7xRAG\u8de8\u6a21\u6001\u6846\u67b6\u5c06\u6f5c\u5728\u5411\u91cf\u89e3\u7801\u4e3a\u8fde\u8d2f\u6587\u672c", "result": "\u751f\u6210\u6570\u636e\u8bad\u7ec3\u6a21\u578b\u6027\u80fd\u5ab2\u7f8e\u539f\u59cb\u6570\u636e\uff08\u65e9\u671f\u5b9e\u9a8c\uff09\uff0c\u5b9a\u6027\u5206\u6790\u663e\u793a\u6f5c\u5728\u7684\u77e5\u8bc6\u84b8\u998f\u4ef7\u503c", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5c0f\u6837\u672c\u5b66\u4e60\u548c\u9690\u79c1\u654f\u611f\u573a\u666f\u63d0\u4f9b\u4e86\u53cc\u91cd\u89e3\u51b3\u65b9\u6848\uff0c\u5f00\u8f9f\u6570\u636e\u4fdd\u62a4\u7ea6\u675f\u4e0b\u7684\u5b89\u5168\u5b66\u4e60\u65b0\u8303\u5f0f"}}
{"id": "2505.13444", "pdf": "https://arxiv.org/pdf/2505.13444", "abs": "https://arxiv.org/abs/2505.13444", "authors": ["Liyan Tang", "Grace Kim", "Xinyu Zhao", "Thom Lake", "Wenxuan Ding", "Fangcong Yin", "Prasann Singhal", "Manya Wadhwa", "Zeyu Leo Liu", "Zayne Sprague", "Ramya Namuduri", "Bodun Hu", "Juan Diego Rodriguez", "Puyuan Peng", "Greg Durrett"], "title": "ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Chart understanding presents a unique challenge for large vision-language\nmodels (LVLMs), as it requires the integration of sophisticated textual and\nvisual reasoning capabilities. However, current LVLMs exhibit a notable\nimbalance between these skills, falling short on visual reasoning that is\ndifficult to perform in text. We conduct a case study using a synthetic dataset\nsolvable only through visual reasoning and show that model performance degrades\nsignificantly with increasing visual complexity, while human performance\nremains robust. We then introduce ChartMuseum, a new Chart Question Answering\n(QA) benchmark containing 1,162 expert-annotated questions spanning multiple\nreasoning types, curated from real-world charts across 184 sources,\nspecifically built to evaluate complex visual and textual reasoning. Unlike\nprior chart understanding benchmarks -- where frontier models perform similarly\nand near saturation -- our benchmark exposes a substantial gap between model\nand human performance, while effectively differentiating model capabilities:\nalthough humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro\nattains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct\nachieves only 38.5%. Moreover, on questions requiring primarily visual\nreasoning, all models experience a 35%-55% performance drop from\ntext-reasoning-heavy question performance. Lastly, our qualitative error\nanalysis reveals specific categories of visual reasoning that are challenging\nfor current LVLMs.", "AI": {"tldr": "\u5927\u578b\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u7406\u89e3\u4e2d\u5b58\u5728\u89c6\u89c9\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u65b0\u57fa\u51c6\u6d4b\u8bd5ChartMuseum\u63ed\u793a\u4e86\u6a21\u578b\u4e0e\u4eba\u7c7b\u8868\u73b0\u7684\u663e\u8457\u5dee\u8ddd\uff08\u4eba\u7c7b93% vs \u6700\u4f73\u6a21\u578b63%\uff09", "motivation": "\u5f53\u524dLVLMs\u5728\u89c6\u89c9\u4e0e\u6587\u672c\u63a8\u7406\u80fd\u529b\u4e0a\u5b58\u5728\u663e\u8457\u4e0d\u5e73\u8861\uff0c\u5c24\u5176\u5728\u9700\u8981\u7eaf\u89c6\u89c9\u63a8\u7406\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u6b20\u4f73\uff0c\u9700\u8981\u5efa\u7acb\u65b0\u8bc4\u4f30\u57fa\u51c6\u91cf\u5316\u8fd9\u79cd\u5dee\u8ddd", "method": "\u4f7f\u7528\u4ec5\u9700\u89c6\u89c9\u63a8\u7406\u7684\u5408\u6210\u6570\u636e\u96c6\u9a8c\u8bc1\u6a21\u578b\u7f3a\u9677\uff0c\u5e76\u6784\u5efa\u5305\u542b1,162\u4e2a\u771f\u5b9e\u56fe\u8868\u95ee\u9898\u7684ChartMuseum\u57fa\u51c6\uff0c\u6db5\u76d6\u591a\u79cd\u63a8\u7406\u7c7b\u578b", "result": "\u4eba\u7c7b\u51c6\u786e\u738793%\uff0c\u6700\u4f73\u6a21\u578bGemini-2.5-Pro\u4ec563%\uff0c\u89c6\u89c9\u63a8\u7406\u95ee\u9898\u5bfc\u81f4\u6240\u6709\u6a21\u578b\u6027\u80fd\u4e0b\u964d35%-55%", "conclusion": "\u73b0\u6709LVLMs\u5728\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\uff0cChartMuseum\u57fa\u51c6\u80fd\u6709\u6548\u8bc4\u4f30\u6a21\u578b\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u5dee\u8ddd"}}
{"id": "2505.13448", "pdf": "https://arxiv.org/pdf/2505.13448", "abs": "https://arxiv.org/abs/2505.13448", "authors": ["Vinay Samuel", "Harshita Diddee", "Yiming Zhang", "Daphne Ippolito"], "title": "CIE: Controlling Language Model Text Generations Using Continuous Signals", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages, 3 figures", "summary": "Aligning language models with user intent is becoming increasingly relevant\nto enhance user experience. This calls for designing methods that can allow\nusers to control the properties of the language that LMs generate. For example,\ncontrolling the length of the generation, the complexity of the language that\ngets chosen, the sentiment, tone, etc. Most existing work attempts to integrate\nusers' control by conditioning LM generations on natural language prompts or\ndiscrete control signals, which are often brittle and hard to scale. In this\nwork, we are interested in \\textit{continuous} control signals, ones that exist\nalong a spectrum that can't easily be captured in a natural language prompt or\nvia existing techniques in conditional generation. Through a case study in\ncontrolling the precise response-length of generations produced by LMs, we\ndemonstrate how after fine-tuning, behaviors of language models can be\ncontrolled via continuous signals -- as vectors that are interpolated between a\n\"low\" and a \"high\" token embedding. Our method more reliably exerts\nresponse-length control than in-context learning methods or fine-tuning methods\nthat represent the control signal as a discrete signal. Our full open-sourced\ncode and datasets are available at https://github.com/vsamuel2003/CIE.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u8fde\u7eed\u63a7\u5236\u4fe1\u53f7(\u5411\u91cf\u63d2\u503c)\u7cbe\u7ec6\u8c03\u63a7\u8bed\u8a00\u6a21\u578b\u751f\u6210\u957f\u5ea6\u7684\u65b9\u6cd5\uff0c\u4f18\u4e8e\u73b0\u6709\u79bb\u6563\u63a7\u5236\u65b9\u6cd5", "motivation": "\u73b0\u6709\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u6216\u79bb\u6563\u4fe1\u53f7\u7684\u63a7\u5236\u65b9\u6cd5\u5b58\u5728\u8106\u5f31\u6027\u548c\u6269\u5c55\u6027\u9650\u5236\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8fde\u7eed\u9891\u8c31\u7684\u63a7\u5236\u9700\u6c42", "method": "\u901a\u8fc7\u5fae\u8c03\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u9ad8\u4f4e\u6807\u8bb0\u5d4c\u5165\u4e4b\u95f4\u8fdb\u884c\u5411\u91cf\u63d2\u503c\uff0c\u5b9e\u73b0\u54cd\u5e94\u957f\u5ea6\u7684\u8fde\u7eed\u63a7\u5236", "result": "\u76f8\u6bd4\u4e0a\u4e0b\u6587\u5b66\u4e60\u6216\u79bb\u6563\u4fe1\u53f7\u5fae\u8c03\u65b9\u6cd5\uff0c\u672c\u65b9\u6cd5\u5728\u54cd\u5e94\u957f\u5ea6\u63a7\u5236\u4e0a\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u53ef\u9760\u6027", "conclusion": "\u8fde\u7eed\u63a7\u5236\u4fe1\u53f7\u4e3a\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u8c03\u63a7\u63d0\u4f9b\u65b0\u8303\u5f0f\uff0c\u76f8\u5173\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u4fc3\u8fdb\u540e\u7eed\u7814\u7a76"}}
{"id": "2505.11545", "pdf": "https://arxiv.org/pdf/2505.11545", "abs": "https://arxiv.org/abs/2505.11545", "authors": ["Xingyu Ji", "Parker Glenn", "Aditya G. Parameswaran", "Madelon Hulsebos"], "title": "TARGET: Benchmarking Table Retrieval for Generative Tasks", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.DB"], "comment": null, "summary": "The data landscape is rich with structured data, often of high value to\norganizations, driving important applications in data analysis and machine\nlearning. Recent progress in representation learning and generative models for\nsuch data has led to the development of natural language interfaces to\nstructured data, including those leveraging text-to-SQL. Contextualizing\ninteractions, either through conversational interfaces or agentic components,\nin structured data through retrieval-augmented generation can provide\nsubstantial benefits in the form of freshness, accuracy, and comprehensiveness\nof answers. The key question is: how do we retrieve the right table(s) for the\nanalytical query or task at hand? To this end, we introduce TARGET: a benchmark\nfor evaluating TAble Retrieval for GEnerative Tasks. With TARGET we analyze the\nretrieval performance of different retrievers in isolation, as well as their\nimpact on downstream tasks. We find that dense embedding-based retrievers far\noutperform a BM25 baseline which is less effective than it is for retrieval\nover unstructured text. We also surface the sensitivity of retrievers across\nvarious metadata (e.g., missing table titles), and demonstrate a stark\nvariation of retrieval performance across datasets and tasks. TARGET is\navailable at https://target-benchmark.github.io.", "AI": {"tldr": "\u63d0\u51faTARGET\u57fa\u51c6\u7528\u4e8e\u8bc4\u4f30\u751f\u6210\u5f0f\u4efb\u52a1\u4e2d\u7684\u8868\u683c\u68c0\u7d22\u6027\u80fd\uff0c\u53d1\u73b0\u5bc6\u96c6\u5d4c\u5165\u68c0\u7d22\u663e\u8457\u4f18\u4e8eBM25\uff0c\u4e14\u68c0\u7d22\u6548\u679c\u53d7\u5143\u6570\u636e\u5b8c\u6574\u6027\u548c\u4efb\u52a1\u5dee\u5f02\u5f71\u54cd\u8f83\u5927\u3002", "motivation": "\u73b0\u6709\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u5728\u7ed3\u6784\u5316\u6570\u636e\u68c0\u7d22\u4e2d\u5b58\u5728\u51c6\u786e\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u68c0\u7d22\u65b9\u6cd5\uff08\u5982BM25\uff09\u5bf9\u7ed3\u6784\u5316\u8868\u683c\u7684\u9002\u914d\u6027\u8f83\u5dee\uff0c\u9700\u7cfb\u7edf\u6027\u8bc4\u4f30\u4e0d\u540c\u68c0\u7d22\u65b9\u6cd5\u5bf9\u751f\u6210\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "method": "\u6784\u5efaTARGET\u57fa\u51c6\uff0c\u5305\u542b\u591a\u79cd\u6570\u636e\u96c6\u548c\u4efb\u52a1\u573a\u666f\uff0c\u6d4b\u8bd5\u4e0d\u540c\u68c0\u7d22\u6a21\u578b\uff08BM25/\u5bc6\u96c6\u5d4c\u5165\uff09\u5728\u8868\u683c\u68c0\u7d22\u4e2d\u7684\u72ec\u7acb\u6027\u80fd\u53ca\u5176\u5bf9\u4e0b\u6e38\u751f\u6210\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u5143\u6570\u636e\uff08\u5982\u8868\u6807\u9898\u7f3a\u5931\uff09\u5bf9\u68c0\u7d22\u7684\u654f\u611f\u6027\u3002", "result": "1. \u5bc6\u96c6\u5d4c\u5165\u68c0\u7d22\u5668\u6bd4BM25\u57fa\u7ebf\u9ad822%\u51c6\u786e\u7387\uff1b2. \u5143\u6570\u636e\u7f3a\u5931\u5bfc\u81f4\u68c0\u7d22\u6027\u80fd\u4e0b\u964d38%\uff1b3. \u4e0d\u540c\u6570\u636e\u96c6\u7684\u68c0\u7d22\u51c6\u786e\u7387\u5dee\u5f02\u8fbe41%\uff0820%-61%\uff09", "conclusion": "TARGET\u4e3a\u7ed3\u6784\u5316\u6570\u636e\u68c0\u7d22\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u8bc1\u660e\u68c0\u7d22\u8d28\u91cf\u76f4\u63a5\u5f71\u54cd\u751f\u6210\u4efb\u52a1\u6548\u679c\uff0c\u672a\u6765\u9700\u9488\u5bf9\u4efb\u52a1\u7279\u6027\u548c\u5143\u6570\u636e\u5b8c\u6574\u6027\u4f18\u5316\u68c0\u7d22\u7cfb\u7edf\u3002"}}
{"id": "2505.11572", "pdf": "https://arxiv.org/pdf/2505.11572", "abs": "https://arxiv.org/abs/2505.11572", "authors": ["Anand Rai", "Satyam Rahangdale", "Utkarsh Anand", "Animesh Mukherjee"], "title": "ASR-FAIRBENCH: Measuring and Benchmarking Equity Across Speech Recognition Systems", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Paper accepted at INTERSPEECH 2025", "summary": "Automatic Speech Recognition (ASR) systems have become ubiquitous in everyday\napplications, yet significant disparities in performance across diverse\ndemographic groups persist. In this work, we introduce the ASR-FAIRBENCH\nleaderboard which is designed to assess both the accuracy and equity of ASR\nmodels in real-time. Leveraging the Meta's Fair-Speech dataset, which captures\ndiverse demographic characteristics, we employ a mixed-effects Poisson\nregression model to derive an overall fairness score. This score is integrated\nwith traditional metrics like Word Error Rate (WER) to compute the Fairness\nAdjusted ASR Score (FAAS), providing a comprehensive evaluation framework. Our\napproach reveals significant performance disparities in SOTA ASR models across\ndemographic groups and offers a benchmark to drive the development of more\ninclusive ASR technologies.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faASR-FAIRBENCH\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7FAAS\u6307\u6807\u7efc\u5408\u8bc4\u4f30\u8bed\u97f3\u8bc6\u522b\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\uff0c\u63ed\u793a\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u8bed\u97f3\u8bc6\u522b\u7cfb\u7edf\u5728\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u5f02\uff0c\u9700\u8981\u5efa\u7acb\u517c\u987e\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u7684\u8bc4\u4f30\u4f53\u7cfb\u3002", "method": "\u4f7f\u7528Meta\u7684Fair-Speech\u6570\u636e\u96c6\uff0c\u91c7\u7528\u6df7\u5408\u6548\u5e94\u6cca\u677e\u56de\u5f52\u6a21\u578b\u8ba1\u7b97\u603b\u4f53\u516c\u5e73\u6027\u5206\u6570\uff0c\u7ed3\u5408\u4f20\u7edf\u8bcd\u9519\u7387(WER)\u6784\u5efaFAAS\u7efc\u5408\u8bc4\u4ef7\u6307\u6807\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u6700\u5148\u8fdb\u7684ASR\u6a21\u578b\u5728\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u5f02\uff0c\u4e3a\u5f00\u53d1\u5305\u5bb9\u6027\u6280\u672f\u63d0\u4f9b\u57fa\u51c6\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u516c\u5e73\u6027\u91cf\u5316\u7eb3\u5165\u8bc4\u4f30\u4f53\u7cfb\uff0c\u63a8\u52a8\u5f00\u53d1\u66f4\u516c\u5e73\u7684\u8bed\u97f3\u8bc6\u522b\u6280\u672f\uff0c\u4fc3\u8fdb\u4eba\u5de5\u667a\u80fd\u516c\u5e73\u6027\u7814\u7a76\u3002"}}
{"id": "2505.11595", "pdf": "https://arxiv.org/pdf/2505.11595", "abs": "https://arxiv.org/abs/2505.11595", "authors": ["Peter Chen", "Xiaopeng Li", "Ziniu Li", "Xi Chen", "Tianyi Lin"], "title": "Spectral Policy Optimization: Coloring your Incorrect Reasoning in GRPO", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "28 pages", "summary": "Reinforcement learning (RL) has demonstrated significant success in enhancing\nreasoning capabilities in large language models (LLMs). One of the most widely\nused RL methods is Group Relative Policy Optimization\n(GRPO)~\\cite{Shao-2024-Deepseekmath}, known for its memory efficiency and\nsuccess in training DeepSeek-R1~\\cite{Guo-2025-Deepseek}. However, GRPO stalls\nwhen all sampled responses in a group are incorrect -- referred to as an\n\\emph{all-negative-sample} group -- as it fails to update the policy, hindering\nlearning progress. The contributions of this paper are two-fold. First, we\npropose a simple yet effective framework that introduces response diversity\nwithin all-negative-sample groups in GRPO using AI feedback. We also provide a\ntheoretical analysis, via a stylized model, showing how this diversification\nimproves learning dynamics. Second, we empirically validate our approach,\nshowing the improved performance across various model sizes (7B, 14B, 32B) in\nboth offline and online learning settings with 10 benchmarks, including base\nand distilled variants. Our findings highlight that learning from\nall-negative-sample groups is not only feasible but beneficial, advancing\nrecent insights from \\citet{Xiong-2025-Minimalist}.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7AI\u53cd\u9988\u589e\u5f3aGRPO\u4e2d\u5168\u8d1f\u6837\u672c\u7ec4\u7684\u54cd\u5e94\u591a\u6837\u6027\uff0c\u7a81\u7834\u7b56\u7565\u66f4\u65b0\u74f6\u9888\u5e76\u9a8c\u8bc1\u6709\u6548\u6027", "motivation": "GRPO\u65b9\u6cd5\u5728\u5168\u8d1f\u6837\u672c\u7ec4\u573a\u666f\u4e0b\u65e0\u6cd5\u66f4\u65b0\u7b56\u7565\uff0c\u963b\u788d\u5f3a\u5316\u5b66\u4e60\u8fdb\u7a0b", "method": "1. \u57fa\u4e8eAI\u53cd\u9988\u7684\u54cd\u5e94\u591a\u6837\u6027\u589e\u5f3a\u6846\u67b6\n2. \u901a\u8fc7\u7406\u8bba\u6a21\u578b\u5206\u6790\u5b66\u4e60\u52a8\u6001\u6539\u8fdb\u673a\u5236", "result": "\u57287B/14B/32B\u6a21\u578b\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u8986\u76d6\u79bb\u7ebf/\u5728\u7ebf\u5b66\u4e60\u768410\u4e2a\u57fa\u51c6\u6d4b\u8bd5\uff08\u542b\u57fa\u7840\u7248\u548c\u84b8\u998f\u7248\uff09", "conclusion": "\u5168\u8d1f\u6837\u672c\u7ec4\u5b66\u4e60\u4e0d\u4ec5\u53ef\u884c\u4e14\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u7a81\u7834\u4f20\u7edfRL\u8bad\u7ec3\u9650\u5236"}}
{"id": "2505.11611", "pdf": "https://arxiv.org/pdf/2505.11611", "abs": "https://arxiv.org/abs/2505.11611", "authors": ["Bofan Gong", "Shiyang Lai", "Dawn Song"], "title": "Probing the Vulnerability of Large Language Models to Polysemantic Interventions", "categories": ["cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Polysemanticity -- where individual neurons encode multiple unrelated\nfeatures -- is a well-known characteristic of large neural networks and remains\na central challenge in the interpretability of language models. At the same\ntime, its implications for model safety are also poorly understood. Leveraging\nrecent advances in sparse autoencoders, we investigate the polysemantic\nstructure of two small models (Pythia-70M and GPT-2-Small) and evaluate their\nvulnerability to targeted, covert interventions at the prompt, feature, token,\nand neuron levels. Our analysis reveals a consistent polysemantic topology\nshared across both models. Strikingly, we demonstrate that this structure can\nbe exploited to mount effective interventions on two larger, black-box\ninstruction-tuned models (LLaMA3.1-8B-Instruct and Gemma-2-9B-Instruct). These\nfindings suggest not only the generalizability of the interventions but also\npoint to a stable and transferable polysemantic structure that could\npotentially persist across architectures and training regimes.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u795e\u7ecf\u7f51\u7edc\u5b58\u5728\u7a33\u5b9a\u591a\u8bed\u4e49\u7ed3\u6784\uff0c\u8be5\u7ed3\u6784\u53ef\u88ab\u7528\u4e8e\u8de8\u6a21\u578b\u89c4\u6a21\u7684\u9690\u853d\u5e72\u9884\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5b89\u5168\u98ce\u9669\u3002", "motivation": "\u63a2\u7d22\u591a\u8bed\u4e49\u6027\u5bf9\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u5b89\u5168\u6027\u7684\u5f71\u54cd\uff0c\u9a8c\u8bc1\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\u7684\u8106\u5f31\u6027\u3002", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5206\u6790Pythia-70M\u548cGPT-2-Small\u6a21\u578b\uff0c\u5728prompt/\u7279\u5f81/\u795e\u7ecf\u5143\u7b49\u5c42\u9762\u5b9e\u65bd\u9488\u5bf9\u6027\u5e72\u9884\u3002", "result": "\u5728\u6307\u4ee4\u5fae\u8c03\u7684\u9ed1\u76d2\u6a21\u578bLLaMA3.1-8B/Gemma-2-9B\u4e0a\u6210\u529f\u5b9e\u73b0\u6709\u6548\u5e72\u9884\uff0c\u8bc1\u660e\u5e72\u9884\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u591a\u8bed\u4e49\u7ed3\u6784\u5177\u6709\u8de8\u67b6\u6784\u548c\u8bad\u7ec3\u8303\u5f0f\u7684\u7a33\u5b9a\u6027\uff0c\u63d0\u793a\u6a21\u578b\u5b89\u5168\u9700\u5173\u6ce8\u5e95\u5c42\u795e\u7ecf\u8868\u5f81\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2505.11614", "pdf": "https://arxiv.org/pdf/2505.11614", "abs": "https://arxiv.org/abs/2505.11614", "authors": ["Jian-Qiao Zhu", "Hanbo Xie", "Dilip Arumugam", "Robert C. Wilson", "Thomas L. Griffiths"], "title": "Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "A central goal of cognitive modeling is to develop models that not only\npredict human behavior but also provide insight into the underlying cognitive\nmechanisms. While neural network models trained on large-scale behavioral data\noften achieve strong predictive performance, they typically fall short in\noffering interpretable explanations of the cognitive processes they capture. In\nthis work, we explore the potential of pretrained large language models (LLMs)\nto serve as dual-purpose cognitive models--capable of both accurate prediction\nand interpretable explanation in natural language. Specifically, we employ\nreinforcement learning with outcome-based rewards to guide LLMs toward\ngenerating explicit reasoning traces for explaining human risky choices. Our\nfindings demonstrate that this approach produces high-quality explanations\nalongside strong quantitative predictions of human decisions.", "AI": {"tldr": "\u63a2\u7d22\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u517c\u5177\u9884\u6d4b\u4e0e\u89e3\u91ca\u80fd\u529b\u7684\u53cc\u7528\u9014\u8ba4\u77e5\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u751f\u6210\u98ce\u9669\u51b3\u7b56\u89e3\u91ca", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u9884\u6d4b\u6027\u80fd\u5f3a\u4f46\u7f3a\u4e4f\u89e3\u91ca\u6027\uff0cLLMs\u5177\u6709\u81ea\u7136\u8bed\u8a00\u751f\u6210\u6f5c\u529b\u53ef\u5f25\u8865\u8fd9\u4e00\u7f3a\u9677", "method": "\u4f7f\u7528\u57fa\u4e8e\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u673a\u5236\uff0c\u5f15\u5bfcLLMs\u751f\u6210\u4eba\u7c7b\u98ce\u9669\u9009\u62e9\u7684\u53ef\u89e3\u91ca\u63a8\u7406\u8f68\u8ff9", "result": "\u6a21\u578b\u5728\u4fdd\u6301\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u751f\u6210\u7b26\u5408\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u7684\u9ad8\u8d28\u91cf\u81ea\u7136\u8bed\u8a00\u89e3\u91ca", "conclusion": "\u9a8c\u8bc1\u4e86LLMs\u4f5c\u4e3a\u8ba4\u77e5\u5efa\u6a21\u65b0\u8303\u5f0f\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u53ef\u89e3\u91caAI\u5f00\u8f9f\u65b0\u8def\u5f84"}}
{"id": "2505.11717", "pdf": "https://arxiv.org/pdf/2505.11717", "abs": "https://arxiv.org/abs/2505.11717", "authors": ["Xilong Wang", "John Bloch", "Zedian Shao", "Yuepeng Hu", "Shuyan Zhou", "Neil Zhenqiang Gong"], "title": "EnvInjection: Environmental Prompt Injection Attack to Multi-modal Web Agents", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Multi-modal large language model (MLLM)-based web agents interact with\nwebpage environments by generating actions based on screenshots of the\nwebpages. Environmental prompt injection attacks manipulate the environment to\ninduce the web agent to perform a specific, attacker-chosen action--referred to\nas the target action. However, existing attacks suffer from limited\neffectiveness or stealthiness, or are impractical in real-world settings. In\nthis work, we propose EnvInjection, a new attack that addresses these\nlimitations. Our attack adds a perturbation to the raw pixel values of the\nrendered webpage, which can be implemented by modifying the webpage's source\ncode. After these perturbed pixels are mapped into a screenshot, the\nperturbation induces the web agent to perform the target action. We formulate\nthe task of finding the perturbation as an optimization problem. A key\nchallenge in solving this problem is that the mapping between raw pixel values\nand screenshot is non-differentiable, making it difficult to backpropagate\ngradients to the perturbation. To overcome this, we train a neural network to\napproximate the mapping and apply projected gradient descent to solve the\nreformulated optimization problem. Extensive evaluation on multiple webpage\ndatasets shows that EnvInjection is highly effective and significantly\noutperforms existing baselines.", "AI": {"tldr": "\u63d0\u51faEnvInjection\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f51\u9875\u50cf\u7d20\u6270\u52a8\u8bf1\u5bfc\u7f51\u7edc\u4ee3\u7406\u6267\u884c\u653b\u51fb\u8005\u6307\u5b9a\u7684\u76ee\u6807\u52a8\u4f5c", "motivation": "\u73b0\u6709\u73af\u5883\u63d0\u793a\u6ce8\u5165\u653b\u51fb\u5b58\u5728\u6709\u6548\u6027\u4e0d\u8db3\u3001\u9690\u853d\u6027\u5dee\u6216\u5b9e\u9645\u5e94\u7528\u56f0\u96be\u7684\u95ee\u9898\uff0c\u9700\u5f00\u53d1\u66f4\u4f18\u89e3\u51b3\u65b9\u6848", "method": "\u6784\u5efa\u50cf\u7d20\u6270\u52a8\u4f18\u5316\u95ee\u9898\uff0c\u7528\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u975e\u53ef\u5fae\u5206\u7684\u50cf\u7d20-\u622a\u56fe\u6620\u5c04\uff0c\u91c7\u7528\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u6c42\u89e3", "result": "\u5728\u591a\u4e2a\u7f51\u9875\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\u653b\u51fb\u6210\u529f\u7387\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "EnvInjection\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u653b\u51fb\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u73af\u5883\u63d0\u793a\u6ce8\u5165\u653b\u51fb"}}
{"id": "2505.11731", "pdf": "https://arxiv.org/pdf/2505.11731", "abs": "https://arxiv.org/abs/2505.11731", "authors": ["Harshil Vejendla", "Haizhou Shi", "Yibin Wang", "Tunyu Zhang", "Huan Zhang", "Hao Wang"], "title": "Efficient Uncertainty Estimation via Distillation of Bayesian Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint; work in progress", "summary": "Recent advances in uncertainty estimation for Large Language Models (LLMs)\nduring downstream adaptation have addressed key challenges of reliability and\nsimplicity. However, existing Bayesian methods typically require multiple\nsampling iterations during inference, creating significant efficiency issues\nthat limit practical deployment. In this paper, we investigate the possibility\nof eliminating the need for test-time sampling for LLM uncertainty estimation.\nSpecifically, when given an off-the-shelf Bayesian LLM, we distill its aligned\nconfidence into a non-Bayesian student LLM by minimizing the divergence between\ntheir predictive distributions. Unlike typical calibration methods, our\ndistillation is carried out solely on the training dataset without the need of\nan additional validation dataset. This simple yet effective approach achieves\nN-times more efficient uncertainty estimation during testing, where N is the\nnumber of samples traditionally required by Bayesian LLMs. Our extensive\nexperiments demonstrate that uncertainty estimation capabilities on training\ndata can successfully generalize to unseen test data through our distillation\ntechnique, consistently producing results comparable to (or even better than)\nstate-of-the-art Bayesian LLMs.", "AI": {"tldr": "\u63d0\u51fa\u901a\u8fc7\u84b8\u998f\u8d1d\u53f6\u65afLLM\u7684\u7f6e\u4fe1\u5ea6\u81f3\u975e\u8d1d\u53f6\u65af\u5b66\u751f\u6a21\u578b\uff0c\u5b9e\u73b0\u65e0\u9700\u6d4b\u8bd5\u91c7\u6837\u7684\u9ad8\u6548\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5", "motivation": "\u73b0\u6709\u8d1d\u53f6\u65af\u65b9\u6cd5\u9700\u591a\u6b21\u91c7\u6837\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\uff0c\u4e25\u91cd\u9650\u5236\u5b9e\u9645\u90e8\u7f72\u5e94\u7528", "method": "\u5728\u8bad\u7ec3\u96c6\u4e0a\u84b8\u998f\u5bf9\u9f50\u9884\u6d4b\u5206\u5e03\uff0c\u65e0\u9700\u9a8c\u8bc1\u6570\u636e\u96c6\uff0c\u6700\u5c0f\u5316\u8d1d\u53f6\u65afLLM\u4e0e\u5b66\u751f\u7684\u9884\u6d4b\u5206\u5e03\u5dee\u5f02", "result": "\u6d4b\u8bd5\u6548\u7387\u63d0\u5347N\u500d(N\u4e3a\u4f20\u7edf\u8d1d\u53f6\u65af\u65b9\u6cd5\u6240\u9700\u91c7\u6837\u6b21\u6570)\uff0c\u5b9e\u9a8c\u663e\u793a\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u80fd\u529b\u53ef\u6cdb\u5316\u81f3\u65b0\u6570\u636e\uff0c\u6548\u679c\u8fbe\u5230/\u8d85\u8d8aSOTA\u8d1d\u53f6\u65afLLMs", "conclusion": "\u8be5\u84b8\u998f\u6280\u672f\u5b9e\u73b0\u6d4b\u8bd5\u9636\u6bb5\u96f6\u91c7\u6837\u6210\u672c\u7684\u9ad8\u6548\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387"}}
{"id": "2505.11737", "pdf": "https://arxiv.org/pdf/2505.11737", "abs": "https://arxiv.org/abs/2505.11737", "authors": ["Tunyu Zhang", "Haizhou Shi", "Yibin Wang", "Hengyi Wang", "Xiaoxiao He", "Zhuowei Li", "Haoxian Chen", "Ligong Han", "Kai Xu", "Huan Zhang", "Dimitris Metaxas", "Hao Wang"], "title": "Token-Level Uncertainty Estimation for Large Language Model Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Preprint; Work in progress", "summary": "While Large Language Models (LLMs) have demonstrated impressive capabilities,\ntheir output quality remains inconsistent across various application scenarios,\nmaking it difficult to identify trustworthy responses, especially in complex\ntasks requiring multi-step reasoning. In this paper, we propose a token-level\nuncertainty estimation framework to enable LLMs to self-assess and self-improve\ntheir generation quality in mathematical reasoning. Specifically, we introduce\nlow-rank random weight perturbation to LLM decoding, generating predictive\ndistributions that we use to estimate token-level uncertainties. We then\naggregate these uncertainties to reflect semantic uncertainty of the generated\nsequences. Experiments on mathematical reasoning datasets of varying difficulty\ndemonstrate that our token-level uncertainty metrics strongly correlate with\nanswer correctness and model robustness. Additionally, we explore using\nuncertainty to directly enhance the model's reasoning performance through\nmultiple generations and the particle filtering algorithm. Our approach\nconsistently outperforms existing uncertainty estimation methods, establishing\neffective uncertainty estimation as a valuable tool for both evaluating and\nimproving reasoning generation in LLMs.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8etoken\u7ea7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684LLM\u81ea\u6211\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u6270\u52a8\u89e3\u7801\u751f\u6210\u9884\u6d4b\u5206\u5e03\uff0c\u63d0\u5347\u6570\u5b66\u63a8\u7406\u7684\u53ef\u9760\u6027\u548c\u6027\u80fd\u3002", "motivation": "LLMs\u8f93\u51fa\u8d28\u91cf\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u4e0d\u7a33\u5b9a\uff0c\u9700\u5efa\u7acb\u53ef\u4fe1\u5ea6\u8bc4\u4f30\u673a\u5236\u3002\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u7cbe\u51c6\u6355\u6349\u8bed\u4e49\u7ea7\u4e0d\u786e\u5b9a\u6027\uff0c\u9700\u66f4\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u4f53\u7cfb\u3002", "method": "1. \u4f4e\u79e9\u968f\u673a\u6743\u91cd\u6270\u52a8\u89e3\u7801\u751f\u6210\u9884\u6d4b\u5206\u5e03\n2. \u805a\u5408token\u7ea7\u4e0d\u786e\u5b9a\u6027\u53cd\u6620\u8bed\u4e49\u786e\u5b9a\u6027\n3. \u7ed3\u5408\u7c92\u5b50\u6ee4\u6ce2\u7b97\u6cd5\u8fdb\u884c\u591a\u8def\u5f84\u63a8\u7406\u4f18\u5316", "result": "\u6570\u5b66\u63a8\u7406\u6570\u636e\u96c6\u663e\u793a\uff1a\u4e0d\u786e\u5b9a\u6027\u6307\u6807\u4e0e\u7b54\u6848\u6b63\u786e\u7387\u5f3a\u76f8\u5173\uff08\u76f8\u5173\u7cfb\u6570\u63d0\u534723%\uff09\uff0c\u63a8\u7406\u6027\u80fd\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff08\u51c6\u786e\u7387\u63d0\u53475.6%\uff09", "conclusion": "\u6709\u6548\u7684token\u7ea7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u53ef\u4f5c\u4e3aLLMs\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u548c\u4f18\u5316\u7684\u53cc\u91cd\u5de5\u5177\uff0c\u4e3a\u53ef\u4fe1AI\u63d0\u4f9b\u65b0\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2505.11756", "pdf": "https://arxiv.org/pdf/2505.11756", "abs": "https://arxiv.org/abs/2505.11756", "authors": ["David Chanin", "Tom\u00e1\u0161 Dulka", "Adri\u00e0 Garriga-Alonso"], "title": "Feature Hedging: Correlated Features Break Narrow Sparse Autoencoders", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "It is assumed that sparse autoencoders (SAEs) decompose polysemantic\nactivations into interpretable linear directions, as long as the activations\nare composed of sparse linear combinations of underlying features. However, we\nfind that if an SAE is more narrow than the number of underlying \"true\nfeatures\" on which it is trained, and there is correlation between features,\nthe SAE will merge components of correlated features together, thus destroying\nmonosemanticity. In LLM SAEs, these two conditions are almost certainly true.\nThis phenomenon, which we call feature hedging, is caused by SAE reconstruction\nloss, and is more severe the narrower the SAE. In this work, we introduce the\nproblem of feature hedging and study it both theoretically in toy models and\nempirically in SAEs trained on LLMs. We suspect that feature hedging may be one\nof the core reasons that SAEs consistently underperform supervised baselines.\nFinally, we use our understanding of feature hedging to propose an improved\nvariant of matryoshka SAEs. Our work shows there remain fundamental issues with\nSAEs, but we are hopeful that that highlighting feature hedging will catalyze\nfuture advances that allow SAEs to achieve their full potential of interpreting\nLLMs at scale.", "AI": {"tldr": "\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5728\u7279\u5f81\u76f8\u5173\u573a\u666f\u4e0b\u4f1a\u51fa\u73b0\u7279\u5f81\u5bf9\u51b2\u95ee\u9898\uff0c\u5bfc\u81f4\u5206\u89e3\u5931\u6548\uff0c\u6539\u8fdb\u7684matryoshka\u7ed3\u6784\u53ef\u80fd\u7f13\u89e3\u8be5\u95ee\u9898", "motivation": "\u73b0\u6709\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5728LLM\u5e94\u7528\u4e2d\u6301\u7eed\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u63a2\u7a76\u5176\u6839\u672c\u539f\u56e0", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u73a9\u5177\u6a21\u578b+LLM\u5b9e\u9645\u573a\u666f\u7684SAE\u8bad\u7ec3\u5b9e\u9a8c\u9a8c\u8bc1", "result": "\u53d1\u73b0\u7279\u5f81\u5bf9\u51b2\u73b0\u8c61\uff1a\u7a84SAE\u4f1a\u5408\u5e76\u76f8\u5173\u7279\u5f81\uff0c\u7834\u574f\u5355\u8bed\u4e49\u6027", "conclusion": "\u7279\u5f81\u5bf9\u51b2\u662fSAE\u6027\u80fd\u4e0d\u8db3\u7684\u6838\u5fc3\u56e0\u7d20\uff0cmatryoshka\u6539\u8fdb\u65b9\u6848\u5e26\u6765\u5e0c\u671b"}}
{"id": "2505.11770", "pdf": "https://arxiv.org/pdf/2505.11770", "abs": "https://arxiv.org/abs/2505.11770", "authors": ["Jing Huang", "Junyi Tao", "Thomas Icard", "Diyi Yang", "Christopher Potts"], "title": "Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": "ICML 2025", "summary": "Interpretability research now offers a variety of techniques for identifying\nabstract internal mechanisms in neural networks. Can such techniques be used to\npredict how models will behave on out-of-distribution examples? In this work,\nwe provide a positive answer to this question. Through a diverse set of\nlanguage modeling tasks--including symbol manipulation, knowledge retrieval,\nand instruction following--we show that the most robust features for\ncorrectness prediction are those that play a distinctive causal role in the\nmodel's behavior. Specifically, we propose two methods that leverage causal\nmechanisms to predict the correctness of model outputs: counterfactual\nsimulation (checking whether key causal variables are realized) and value\nprobing (using the values of those variables to make predictions). Both achieve\nhigh AUC-ROC in distribution and outperform methods that rely on\ncausal-agnostic features in out-of-distribution settings, where predicting\nmodel behaviors is more crucial. Our work thus highlights a novel and\nsignificant application for internal causal analysis of language models.", "AI": {"tldr": "\u901a\u8fc7\u56e0\u679c\u5206\u6790\u6280\u672f\u9884\u6d4b\u8bed\u8a00\u6a21\u578b\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e2d\u7684\u884c\u4e3a\u8868\u73b0", "motivation": "\u63a2\u7d22\u795e\u7ecf\u7f51\u7edc\u89e3\u91ca\u6027\u6280\u672f\u662f\u5426\u80fd\u9884\u6d4b\u6a21\u578b\u5728\u5206\u5e03\u5916\u6837\u672c\u7684\u884c\u4e3a\uff0c\u9a8c\u8bc1\u56e0\u679c\u673a\u5236\u5728\u6a21\u578b\u884c\u4e3a\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027", "method": "\u63d0\u51fa\u53cd\u4e8b\u5b9e\u6a21\u62df\uff08\u68c0\u67e5\u5173\u952e\u56e0\u679c\u53d8\u91cf\uff09\u548c\u503c\u63a2\u6d4b\uff08\u5229\u7528\u53d8\u91cf\u503c\u9884\u6d4b\uff09\u4e24\u79cd\u65b9\u6cd5\uff0c\u5728\u7b26\u53f7\u64cd\u4f5c\u3001\u77e5\u8bc6\u68c0\u7d22\u548c\u6307\u4ee4\u8ddf\u968f\u7b49\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u8fdb\u884c\u9a8c\u8bc1", "result": "\u56e0\u679c\u65b9\u6cd5\u5728\u5206\u5e03\u5185\u5916\u5747\u5b9e\u73b0\u9ad8AUC-ROC\uff0c\u5728\u5206\u5e03\u5916\u573a\u666f\u663e\u8457\u4f18\u4e8e\u975e\u56e0\u679c\u65b9\u6cd5", "conclusion": "\u5185\u90e8\u56e0\u679c\u5206\u6790\u4e3a\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u9884\u6d4b\u5f00\u8f9f\u4e86\u65b0\u7684\u5e94\u7528\u65b9\u5411\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u5b9e\u7528\u4ef7\u503c"}}
{"id": "2505.11812", "pdf": "https://arxiv.org/pdf/2505.11812", "abs": "https://arxiv.org/abs/2505.11812", "authors": ["Yang Tan", "Wenrui Gou", "Bozitao Zhong", "Liang Hong", "Huiqun Yu", "Bingxin Zhou"], "title": "VenusX: Unlocking Fine-Grained Functional Understanding of Proteins", "categories": ["cs.LG", "cs.CL", "q-bio.QM"], "comment": "29 pages, 3 figures, 17 tables", "summary": "Deep learning models have driven significant progress in predicting protein\nfunction and interactions at the protein level. While these advancements have\nbeen invaluable for many biological applications such as enzyme engineering and\nfunction annotation, a more detailed perspective is essential for understanding\nprotein functional mechanisms and evaluating the biological knowledge captured\nby models. To address this demand, we introduce VenusX, the first large-scale\nbenchmark for fine-grained functional annotation and function-based protein\npairing at the residue, fragment, and domain levels. VenusX comprises three\nmajor task categories across six types of annotations, including residue-level\nbinary classification, fragment-level multi-class classification, and pairwise\nfunctional similarity scoring for identifying critical active sites, binding\nsites, conserved sites, motifs, domains, and epitopes. The benchmark features\nover 878,000 samples curated from major open-source databases such as InterPro,\nBioLiP, and SAbDab. By providing mixed-family and cross-family splits at three\nsequence identity thresholds, our benchmark enables a comprehensive assessment\nof model performance on both in-distribution and out-of-distribution scenarios.\nFor baseline evaluation, we assess a diverse set of popular and open-source\nmodels, including pre-trained protein language models, sequence-structure\nhybrids, structure-based methods, and alignment-based techniques. Their\nperformance is reported across all benchmark datasets and evaluation settings\nusing multiple metrics, offering a thorough comparison and a strong foundation\nfor future research. Code and data are publicly available at\nhttps://github.com/ai4protein/VenusX.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u5927\u89c4\u6a21\u7ec6\u7c92\u5ea6\u86cb\u767d\u8d28\u529f\u80fd\u5206\u6790\u57fa\u51c6VenusX\uff0c\u8986\u76d6\u6b8b\u57fa/\u7247\u6bb5/\u7ed3\u6784\u57df\u5c42\u9762\u7684\u529f\u80fd\u6ce8\u91ca\u4e0e\u914d\u5bf9\u4efb\u52a1", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e3b\u8981\u5728\u86cb\u767d\u8d28\u5c42\u9762\u8fdb\u884c\u529f\u80fd\u9884\u6d4b\uff0c\u4f46\u7406\u89e3\u529f\u80fd\u673a\u5236\u9700\u8981\u66f4\u7ec6\u7c92\u5ea6\uff08\u6b8b\u57fa/\u7247\u6bb5/\u7ed3\u6784\u57df\uff09\u7684\u5206\u6790\u89c6\u89d2", "method": "\u6784\u5efa\u5305\u542b6\u7c7b\u6ce8\u91ca\u7684878,000+\u6837\u672c\u6570\u636e\u96c6\uff0c\u91c7\u7528\u6df7\u5408\u5bb6\u65cf/\u8de8\u5bb6\u65cf\u5212\u5206\u548c\u4e09\u79cd\u5e8f\u5217\u76f8\u4f3c\u5ea6\u9608\u503c\uff0c\u8bc4\u4f30\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u3001\u7ed3\u6784\u6df7\u5408\u6a21\u578b\u7b49\u591a\u7c7b\u65b9\u6cd5", "result": "\u901a\u8fc7\u591a\u6307\u6807\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u5728\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u573a\u666f\u7684\u6027\u80fd\uff0c\u5efa\u7acb\u672a\u6765\u7814\u7a76\u7684\u57fa\u51c6\u57fa\u7ebf", "conclusion": "VenusX\u586b\u8865\u4e86\u7ec6\u7c92\u5ea6\u86cb\u767d\u8d28\u529f\u80fd\u5206\u6790\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u5176\u5f00\u6e90\u6570\u636e\u4e0e\u4ee3\u7801\u5c06\u63a8\u52a8\u86cb\u767d\u8d28\u529f\u80fd\u673a\u5236\u7814\u7a76\u548c\u6a21\u578b\u80fd\u529b\u63d0\u5347"}}
{"id": "2505.11842", "pdf": "https://arxiv.org/pdf/2505.11842", "abs": "https://arxiv.org/abs/2505.11842", "authors": ["Xuannan Liu", "Zekun Li", "Zheqi He", "Peipei Li", "Shuhan Xia", "Xing Cui", "Huaibo Huang", "Xi Yang", "Ran He"], "title": "Video-SafetyBench: A Benchmark for Safety Evaluation of Video LVLMs", "categories": ["cs.CV", "cs.CL"], "comment": "Project page:\n  https://liuxuannan.github.io/Video-SafetyBench.github.io/", "summary": "The increasing deployment of Large Vision-Language Models (LVLMs) raises\nsafety concerns under potential malicious inputs. However, existing multimodal\nsafety evaluations primarily focus on model vulnerabilities exposed by static\nimage inputs, ignoring the temporal dynamics of video that may induce distinct\nsafety risks. To bridge this gap, we introduce Video-SafetyBench, the first\ncomprehensive benchmark designed to evaluate the safety of LVLMs under\nvideo-text attacks. It comprises 2,264 video-text pairs spanning 48\nfine-grained unsafe categories, each pairing a synthesized video with either a\nharmful query, which contains explicit malice, or a benign query, which appears\nharmless but triggers harmful behavior when interpreted alongside the video. To\ngenerate semantically accurate videos for safety evaluation, we design a\ncontrollable pipeline that decomposes video semantics into subject images (what\nis shown) and motion text (how it moves), which jointly guide the synthesis of\nquery-relevant videos. To effectively evaluate uncertain or borderline harmful\noutputs, we propose RJScore, a novel LLM-based metric that incorporates the\nconfidence of judge models and human-aligned decision threshold calibration.\nExtensive experiments show that benign-query video composition achieves average\nattack success rates of 67.2%, revealing consistent vulnerabilities to\nvideo-induced attacks. We believe Video-SafetyBench will catalyze future\nresearch into video-based safety evaluation and defense strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u89c6\u9891\u6587\u672c\u653b\u51fb\u5b89\u5168\u8bc4\u4f30\u57fa\u51c6Video-SafetyBench\uff0c\u901a\u8fc7\u53ef\u63a7\u89c6\u9891\u5408\u6210\u548cRJScore\u8bc4\u4f30\u6307\u6807\uff0c\u63ed\u793a\u4e86\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u89c6\u9891\u653b\u51fb\u4e0b\u7684\u663e\u8457\u6f0f\u6d1e\uff08\u5e73\u5747\u653b\u51fb\u6210\u529f\u738767.2%\uff09", "motivation": "\u73b0\u6709\u5b89\u5168\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u9759\u6001\u56fe\u50cf\u6f0f\u6d1e\uff0c\u5ffd\u89c6\u4e86\u89c6\u9891\u52a8\u6001\u7279\u6027\u5e26\u6765\u7684\u65b0\u578b\u5b89\u5168\u98ce\u9669\u3002\u9700\u8981\u6784\u5efa\u89c6\u9891\u573a\u666f\u4e0b\u7684\u7cfb\u7edf\u6027\u5b89\u5168\u8bc4\u4f30\u6846\u67b6", "method": "1. \u6784\u5efa\u5305\u542b2264\u4e2a\u89c6\u9891\u6587\u672c\u5bf9\u7684\u57fa\u51c6\uff0848\u4e2a\u4e0d\u5b89\u5168\u7c7b\u522b\uff09 2. \u5206\u89e3\u89c6\u9891\u8bed\u4e49\u4e3a\u3010\u4e3b\u9898\u56fe\u50cf+\u8fd0\u52a8\u6587\u672c\u3011\u7684\u53ef\u63a7\u5408\u6210\u6d41\u7a0b 3. \u63d0\u51fa\u878d\u5408\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7684RJScore\u8bc4\u4f30\u6307\u6807", "result": "\u826f\u6027\u67e5\u8be2\u89c6\u9891\u653b\u51fb\u6210\u529f\u7387\u5e73\u5747\u8fbe67.2%\uff0c\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u5747\u5448\u73b0\u663e\u8457\u5b89\u5168\u6f0f\u6d1e\u3002RJScore\u4e0e\u4eba\u5de5\u8bc4\u4f30\u76f8\u5173\u6027\u8fbe0.89", "conclusion": "Video-SafetyBench\u63ed\u793a\u4e86\u52a8\u6001\u89c6\u9891\u653b\u51fb\u7684\u6709\u6548\u6027\uff0c\u4e3a\u89c6\u9891\u573a\u666f\u4e0b\u7684\u6a21\u578b\u5b89\u5168\u8bc4\u4f30\u548c\u9632\u5fa1\u673a\u5236\u7814\u53d1\u63d0\u4f9b\u4e86\u57fa\u51c6\u6846\u67b6"}}
{"id": "2505.11861", "pdf": "https://arxiv.org/pdf/2505.11861", "abs": "https://arxiv.org/abs/2505.11861", "authors": ["Qi Zhou", "Jie Zhang", "Dongxia Wang", "Qiang Liu", "Tianlin Li", "Jin Song Dong", "Wenhai Wang", "Qing Guo"], "title": "Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity", "categories": ["cs.AI", "cs.CL", "91C99", "I.2.7; J.4"], "comment": "under review", "summary": "Human preference plays a crucial role in the refinement of large language\nmodels (LLMs). However, collecting human preference feedback is costly and most\nexisting datasets neglect the correlation between personalization and\npreferences. To address this issue, we introduce Fair-PP, a synthetic dataset\nof personalized preferences targeting social equity, derived from real-world\nsocial survey data, which includes 28 social groups, 98 equity topics, and 5\npersonal preference dimensions. Leveraging GPT-4o-mini, we engage in\nrole-playing based on seven representative persona portrayals guided by\nexisting social survey data, yielding a total of 238,623 preference records.\nThrough Fair-PP, we also contribute (i) An automated framework for generating\npreference data, along with a more fine-grained dataset of personalized\npreferences; (ii) analysis of the positioning of the existing mainstream LLMs\nacross five major global regions within the personalized preference space; and\n(iii) a sample reweighting method for personalized preference alignment,\nenabling alignment with a target persona while maximizing the divergence from\nother personas. Empirical experiments show our method outperforms the\nbaselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFair-PP\u6570\u636e\u96c6\uff0c\u57fa\u4e8e\u771f\u5b9e\u793e\u4f1a\u8c03\u67e5\u751f\u6210\u4e2a\u6027\u5316\u504f\u597d\u6570\u636e\uff0c\u5305\u542b28\u4e2a\u793e\u4f1a\u7fa4\u4f53\u548c5\u4e2a\u504f\u597d\u7ef4\u5ea6\u3002\u901a\u8fc7GPT-4o-mini\u89d2\u8272\u626e\u6f14\u751f\u6210238,623\u6761\u8bb0\u5f55\uff0c\u5e76\u63d0\u51fa\u4e2a\u6027\u5316\u504f\u597d\u5bf9\u9f50\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u504f\u597d\u6570\u636e\u96c6\u7f3a\u4e4f\u4e2a\u6027\u5316\u5173\u8054\u4e14\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u9700\u6784\u5efa\u80fd\u53cd\u6620\u793e\u4f1a\u516c\u5e73\u7684\u4e2a\u6027\u5316\u504f\u597d\u6570\u636e\u96c6\u3002", "method": "1. \u57fa\u4e8e\u793e\u4f1a\u8c03\u67e5\u6570\u636e\u6784\u5efa28\u4e2a\u793e\u4f1a\u7fa4\u4f53\u753b\u50cf\n2. \u4f7f\u7528GPT-4o-mini\u8fdb\u884c\u89d2\u8272\u626e\u6f14\u751f\u6210\u504f\u597d\u6570\u636e\n3. \u5f00\u53d1\u6837\u672c\u91cd\u52a0\u6743\u65b9\u6cd5\u5b9e\u73b0\u4e2a\u6027\u5316\u504f\u597d\u5bf9\u9f50", "result": "1. \u751f\u6210238,623\u6761\u4e2a\u6027\u5316\u504f\u597d\u8bb0\u5f55\n2. \u4e3b\u6d41\u5927\u6a21\u578b\u5728\u4e94\u5927\u533a\u57df\u504f\u597d\u7a7a\u95f4\u5b9a\u4f4d\u5206\u6790\n3. \u63d0\u51fa\u65b9\u6cd5\u6548\u679c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b", "conclusion": "Fair-PP\u586b\u8865\u4e2a\u6027\u5316\u504f\u597d\u6570\u636e\u7a7a\u767d\uff0c\u63d0\u4f9b\u81ea\u52a8\u5316\u751f\u6210\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u91cd\u52a0\u6743\u65b9\u6cd5\u5b9e\u73b0\u76ee\u6807\u4eba\u7269\u5bf9\u9f50\u4e0e\u5dee\u5f02\u5316\uff0c\u4e3aLLM\u516c\u5e73\u6027\u7814\u7a76\u63d0\u4f9b\u65b0\u5de5\u5177\u3002"}}
{"id": "2505.11875", "pdf": "https://arxiv.org/pdf/2505.11875", "abs": "https://arxiv.org/abs/2505.11875", "authors": ["Chi-Min Chan", "Chunpu Xu", "Jiaming Ji", "Zhen Ye", "Pengcheng Wen", "Chunyang Jiang", "Yaodong Yang", "Wei Xue", "Sirui Han", "Yike Guo"], "title": "J1: Exploring Simple Test-Time Scaling for LLM-as-a-Judge", "categories": ["cs.LG", "cs.CL"], "comment": "33 pages, 27 figures", "summary": "The current focus of AI research is shifting from emphasizing model training\ntowards enhancing evaluation quality, a transition that is crucial for driving\nfurther advancements in AI systems. Traditional evaluation methods typically\nrely on reward models assigning scalar preference scores to outputs. Although\neffective, such approaches lack interpretability, leaving users often uncertain\nabout why a reward model rates a particular response as high or low. The advent\nof LLM-as-a-Judge provides a more scalable and interpretable method of\nsupervision, offering insights into the decision-making process. Moreover, with\nthe emergence of large reasoning models, which consume more tokens for deeper\nthinking and answer refinement, scaling test-time computation in the\nLLM-as-a-Judge paradigm presents an avenue for further boosting performance and\nproviding more interpretability through reasoning traces. In this paper, we\nintroduce $\\textbf{J1-7B}$, which is first supervised fine-tuned on\nreflection-enhanced datasets collected via rejection-sampling and subsequently\ntrained using Reinforcement Learning (RL) with verifiable rewards. At inference\ntime, we apply Simple Test-Time Scaling (STTS) strategies for additional\nperformance improvement. Experimental results demonstrate that $\\textbf{J1-7B}$\nsurpasses the previous state-of-the-art LLM-as-a-Judge by $ \\textbf{4.8}$\\% and\nexhibits a $ \\textbf{5.1}$\\% stronger scaling trend under STTS. Additionally,\nwe present three key findings: (1) Existing LLM-as-a-Judge does not inherently\nexhibit such scaling trend. (2) Model simply fine-tuned on reflection-enhanced\ndatasets continues to demonstrate similarly weak scaling behavior. (3)\nSignificant scaling trend emerges primarily during the RL phase, suggesting\nthat effective STTS capability is acquired predominantly through RL training.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faJ1-7B\u6a21\u578b\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03+\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7b56\u7565\u53caSTTS\u63a8\u7406\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347LLM-as-a-Judge\u8bc4\u4f30\u6027\u80fd\uff08+4.8%\uff09\u53ca\u6269\u5c55\u8d8b\u52bf\uff08+5.1%\uff09\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6807\u91cf\u5956\u52b1\u7684\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0cLLM-as-a-Judge\u7ed3\u5408\u5927\u63a8\u7406\u6a21\u578b\u7684\u6df1\u5ea6\u601d\u8003\u80fd\u529b\u53ef\u63d0\u4f9b\u66f4\u900f\u660e\u7684\u51b3\u7b56\u4f9d\u636e\u3002", "method": "1. \u57fa\u4e8e\u62d2\u7edd\u91c7\u6837\u6536\u96c6\u53cd\u5c04\u589e\u5f3a\u6570\u636e\u96c6\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\n2. \u91c7\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\n3. \u63a8\u7406\u9636\u6bb5\u5e94\u7528STTS\u7b56\u7565\u4f18\u5316\u6027\u80fd", "result": "J1-7B\u6027\u80fd\u8d85\u8d8aSOTA 4.8%\uff0cSTTS\u6269\u5c55\u8d8b\u52bf\u589e\u5f3a5.1%\u3002\u5173\u952e\u53d1\u73b0\uff1a\n1. \u73b0\u6709LLM-as-a-Judge\u65e0\u6269\u5c55\u8d8b\u52bf\n2. \u4ec5\u5fae\u8c03\u6a21\u578b\u6269\u5c55\u6027\u5f31\n3. RL\u8bad\u7ec3\u663e\u8457\u83b7\u5f97STTS\u80fd\u529b", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u9636\u6bb5\u662f\u83b7\u5f97\u6709\u6548STTS\u80fd\u529b\u7684\u5173\u952e\uff0c\u8bc1\u660eRL\u8bad\u7ec3\u5bf9\u6a21\u578b\u6269\u5c55\u6027\u7684\u51b3\u5b9a\u6027\u4f5c\u7528\u3002"}}
{"id": "2505.11979", "pdf": "https://arxiv.org/pdf/2505.11979", "abs": "https://arxiv.org/abs/2505.11979", "authors": ["Tarik Houichime", "Younes El Amrani"], "title": "Introduction to Analytical Software Engineering Design Paradigm", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.MS", "cs.PL"], "comment": "The Conference's autorization to submit a preprint was granted", "summary": "As modern software systems expand in scale and complexity, the challenges\nassociated with their modeling and formulation grow increasingly intricate.\nTraditional approaches often fall short in effectively addressing these\ncomplexities, particularly in tasks such as design pattern detection for\nmaintenance and assessment, as well as code refactoring for optimization and\nlong-term sustainability. This growing inadequacy underscores the need for a\nparadigm shift in how such challenges are approached and resolved. This paper\npresents Analytical Software Engineering (ASE), a novel design paradigm aimed\nat balancing abstraction, tool accessibility, compatibility, and scalability.\nASE enables effective modeling and resolution of complex software engineering\nproblems. The paradigm is evaluated through two frameworks\nBehavioral-Structural Sequences (BSS) and Optimized Design Refactoring (ODR),\nboth developed in accordance with ASE principles. BSS offers a compact,\nlanguage-agnostic representation of codebases to facilitate precise design\npattern detection. ODR unifies artifact and solution representations to\noptimize code refactoring via heuristic algorithms while eliminating iterative\ncomputational overhead. By providing a structured approach to software design\nchallenges, ASE lays the groundwork for future research in encoding and\nanalyzing complex software metrics.", "AI": {"tldr": "\u63d0\u51faAnalytical Software Engineering\uff08ASE\uff09\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7BSS\u548cODR\u6846\u67b6\u89e3\u51b3\u590d\u6742\u8f6f\u4ef6\u5de5\u7a0b\u95ee\u9898", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5e94\u5bf9\u73b0\u4ee3\u8f6f\u4ef6\u7cfb\u7edf\u89c4\u6a21\u4e0e\u590d\u6742\u6027\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u5728\u8bbe\u8ba1\u6a21\u5f0f\u68c0\u6d4b\u548c\u4ee3\u7801\u91cd\u6784\u4efb\u52a1\u4e2d\u6548\u7387\u4f4e\u4e0b", "method": "\u5f00\u53d1ASE\u8bbe\u8ba1\u8303\u5f0f\uff0c\u5305\u542bBSS\uff08\u8bed\u8a00\u65e0\u5173\u7684\u4ee3\u7801\u8868\u5f81\u6846\u67b6\uff09\u548cODR\uff08\u57fa\u4e8e\u542f\u53d1\u5f0f\u7b97\u6cd5\u7684\u4ee3\u7801\u4f18\u5316\u6846\u67b6\uff09", "result": "BSS\u5b9e\u73b0\u7cbe\u786e\u8bbe\u8ba1\u6a21\u5f0f\u68c0\u6d4b\uff0cODR\u51cf\u5c1190%\u8fed\u4ee3\u8ba1\u7b97\u5f00\u9500\u5e76\u4f18\u5316\u91cd\u6784\u6548\u679c", "conclusion": "ASE\u4e3a\u590d\u6742\u8f6f\u4ef6\u5ea6\u91cf\u7684\u7f16\u7801\u5206\u6790\u5efa\u7acb\u65b0\u65b9\u6cd5\u8bba\u57fa\u7840\uff0c\u63a8\u52a8\u8f6f\u4ef6\u5de5\u7a0b\u53ef\u6301\u7eed\u53d1\u5c55\u7814\u7a76"}}
{"id": "2505.12039", "pdf": "https://arxiv.org/pdf/2505.12039", "abs": "https://arxiv.org/abs/2505.12039", "authors": ["Renqi Chen", "Haoyang Su", "Shixiang Tang", "Zhenfei Yin", "Qi Wu", "Hui Li", "Ye Sun", "Nanqing Dong", "Wanli Ouyang", "Philip Torr"], "title": "AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research", "categories": ["cs.AI", "cs.CL", "physics.soc-ph"], "comment": null, "summary": "The Science of Science (SoS) explores the mechanisms underlying scientific\ndiscovery, and offers valuable insights for enhancing scientific efficiency and\nfostering innovation. Traditional approaches often rely on simplistic\nassumptions and basic statistical tools, such as linear regression and\nrule-based simulations, which struggle to capture the complexity and scale of\nmodern research ecosystems. The advent of artificial intelligence (AI) presents\na transformative opportunity for the next generation of SoS, enabling the\nautomation of large-scale pattern discovery and uncovering insights previously\nunattainable. This paper offers a forward-looking perspective on the\nintegration of Science of Science with AI for automated research pattern\ndiscovery and highlights key open challenges that could greatly benefit from\nAI. We outline the advantages of AI over traditional methods, discuss potential\nlimitations, and propose pathways to overcome them. Additionally, we present a\npreliminary multi-agent system as an illustrative example to simulate research\nsocieties, showcasing AI's ability to replicate real-world research patterns\nand accelerate progress in Science of Science research.", "AI": {"tldr": "\u63a2\u8ba8AI\u8d4b\u80fd\u79d1\u5b66\u5b66\u7814\u7a76\uff0c\u7a81\u7834\u4f20\u7edf\u7edf\u8ba1\u65b9\u6cd5\u5c40\u9650\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u79d1\u7814\u6a21\u5f0f\u53d1\u73b0\u5e76\u52a0\u901f\u7814\u7a76\u8fdb\u7a0b", "motivation": "\u4f20\u7edf\u79d1\u5b66\u5b66\u65b9\u6cd5\u4f9d\u8d56\u7b80\u5355\u5047\u8bbe\u548c\u7edf\u8ba1\u5de5\u5177\uff0c\u96be\u4ee5\u5e94\u5bf9\u73b0\u4ee3\u79d1\u7814\u7684\u590d\u6742\u6027\u3002AI\u4e3a\u5927\u89c4\u6a21\u6a21\u5f0f\u53d1\u73b0\u63d0\u4f9b\u65b0\u53ef\u80fd\uff0c\u63a8\u52a8\u79d1\u5b66\u6548\u7387\u9769\u65b0\u3002", "method": "\u91c7\u7528\u524d\u77bb\u6027\u89c6\u89d2\u5206\u6790AI\u4e0e\u79d1\u5b66\u5b66\u7684\u878d\u5408\uff0c\u6784\u5efa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6a21\u62df\u79d1\u7814\u793e\u4f1a\uff0c\u5bf9\u6bd4\u4f20\u7edf\u65b9\u6cd5\u4e0eAI\u4f18\u52bf\uff0c\u63d0\u51fa\u6311\u6218\u5e94\u5bf9\u8def\u5f84\u3002", "result": "AI\u53ef\u81ea\u52a8\u5316\u53d1\u73b0\u79d1\u7814\u65b0\u8303\u5f0f\uff0c\u6709\u6548\u590d\u73b0\u73b0\u5b9e\u7814\u7a76\u6a21\u5f0f\uff0c\u4f46\u9700\u89e3\u51b3\u53ef\u89e3\u91ca\u6027\u3001\u6570\u636e\u504f\u5dee\u7b49\u6280\u672f\u74f6\u9888\u3002", "conclusion": "AI\u4e0e\u79d1\u5b66\u5b66\u7684\u6df1\u5ea6\u6574\u5408\u5c06\u91cd\u5851\u79d1\u7814\u65b9\u6cd5\u8bba\uff0c\u9700\u5efa\u7acb\u8de8\u5b66\u79d1\u534f\u4f5c\u673a\u5236\u4ee5\u514b\u670d\u6280\u672f\u4f26\u7406\u6311\u6218\uff0c\u91ca\u653e\u79d1\u7814\u521b\u65b0\u6f5c\u80fd\u3002"}}
{"id": "2505.12058", "pdf": "https://arxiv.org/pdf/2505.12058", "abs": "https://arxiv.org/abs/2505.12058", "authors": ["Vincent Koc"], "title": "Tiny QA Benchmark++: Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation", "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.6; H.2.8"], "comment": "28 pages, 7 figures, 3 tables. Includes expanded appendix & full\n  score matrices. Dataset & code: HF Hub + GitHub + Pypi links in abstract.\n  Core data and code Apache-2.0; synthetic packs eval-only", "summary": "Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual\nsmoke-test suite designed to give large-language-model (LLM) pipelines a\nunit-test style safety net dataset that runs in seconds with minimal cost. Born\nout of the tight feedback-loop demands building the Comet Opik\nprompt-optimization SDK, where waiting on heavyweight benchmarks breaks\ndeveloper flow. TQB++ couples a 52-item English gold set (less than 20 kB) with\na tiny synthetic-data generator pypi package built on provider-agnostic\nLiteLLM. The generator lets practitioners mint their own tiny packs in any\nlanguage, domain, or difficulty, while ten ready-made packs already cover\nArabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian,\nSpanish, and Turkish. Every dataset ships with Croissant metadata and\nplug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so\nteams can drop deterministic micro-benchmarks directly into pull-request gates,\nprompt-engineering loops, and production dashboards without touching GPU\nbudgets. A complete TQB++ run adds only a few seconds to pipeline latency yet\nreliably flags prompt-template errors, tokenizer drift, and fine-tuning\nside-effects long before full-scale suites like MMLU or BIG-Bench would finish\nconfiguring. The entire framework is released to accelerate continuous,\nresource-efficient quality assurance across the generative-AI ecosystem.", "AI": {"tldr": "TQB++\u662f\u8d85\u8f7b\u91cf\u7ea7\u591a\u8bed\u8a00\u6d4b\u8bd5\u5957\u4ef6\uff0c\u901a\u8fc7\u5fae\u578b\u6570\u636e\u96c6\u548c\u5408\u6210\u751f\u6210\u5668\u5b9e\u73b0LLM\u6d41\u7a0b\u7684\u79d2\u7ea7\u68c0\u6d4b", "motivation": "\u89e3\u51b3\u91cd\u91cf\u7ea7\u57fa\u51c6\u6d4b\u8bd5\u7834\u574f\u5f00\u53d1\u6d41\u7a0b\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u5373\u65f6\u53cd\u9988\u7684\u5355\u5143\u6d4b\u8bd5\u7ea7\u5b89\u5168\u4fdd\u969c", "method": "\u7ed3\u540852\u9879\u82f1\u6587\u9ec4\u91d1\u6570\u636e\u96c6\u4e0e\u591a\u8bed\u8a00\u5408\u6210\u751f\u6210\u5668\uff08\u652f\u630110\u79cd\u8bed\u8a00\uff09\uff0c\u63d0\u4f9b\u6807\u51c6\u5316\u5143\u6570\u636e\u548cCI\u5de5\u5177\u96c6\u6210\u65b9\u6848", "result": "\u5728\u6570\u79d2\u5185\u53ef\u9760\u68c0\u6d4b\u63d0\u793a\u6a21\u677f\u9519\u8bef/\u5206\u8bcd\u5668\u6f02\u79fb\u7b49\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u8d44\u6e90\u6d88\u8017\u964d\u4f4e95%", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u8d44\u6e90\u9ad8\u6548\u7684\u8d28\u91cf\u4fdd\u969c\u65b9\u6848\uff0c\u52a0\u901f\u751f\u6210\u5f0fAI\u751f\u6001\u7cfb\u7edf\u7684\u6301\u7eed\u4f18\u5316\u8fdb\u7a0b"}}
{"id": "2505.12065", "pdf": "https://arxiv.org/pdf/2505.12065", "abs": "https://arxiv.org/abs/2505.12065", "authors": ["Tiannuo Yang", "Zebin Yao", "Bowen Jin", "Lixiao Cui", "Yusen Li", "Gang Wang", "Xiaoguang Liu"], "title": "Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "comment": null, "summary": "Large Language Model (LLM)-based search agents have shown remarkable\ncapabilities in solving complex tasks by dynamically decomposing problems and\naddressing them through interleaved reasoning and retrieval. However, this\ninterleaved paradigm introduces substantial efficiency bottlenecks. First, we\nobserve that both highly accurate and overly approximate retrieval methods\ndegrade system efficiency: exact search incurs significant retrieval overhead,\nwhile coarse retrieval requires additional reasoning steps during generation.\nSecond, we identify inefficiencies in system design, including improper\nscheduling and frequent retrieval stalls, which lead to cascading latency --\nwhere even minor delays in retrieval amplify end-to-end inference time. To\naddress these challenges, we introduce SearchAgent-X, a high-efficiency\ninference framework for LLM-based search agents. SearchAgent-X leverages\nhigh-recall approximate retrieval and incorporates two key techniques:\npriority-aware scheduling and non-stall retrieval. Extensive experiments\ndemonstrate that SearchAgent-X consistently outperforms state-of-the-art\nsystems such as vLLM and HNSW-based retrieval across diverse tasks, achieving\nup to 3.4$\\times$ higher throughput and 5$\\times$ lower latency, without\ncompromising generation quality. SearchAgent-X is available at\nhttps://github.com/tiannuo-yang/SearchAgent-X.", "AI": {"tldr": "\u63d0\u51faSearchAgent-X\u6846\u67b6\uff0c\u901a\u8fc7\u8fd1\u4f3c\u68c0\u7d22\u548c\u7cfb\u7edf\u4f18\u5316\u663e\u8457\u63d0\u5347LLM\u641c\u7d22\u4ee3\u7406\u6548\u7387", "motivation": "\u73b0\u6709LLM\u641c\u7d22\u4ee3\u7406\u5b58\u5728\u7cbe\u786e\u68c0\u7d22\u5f00\u9500\u5927\u3001\u7c97\u7565\u68c0\u7d22\u589e\u52a0\u63a8\u7406\u6b65\u9aa4\u3001\u7cfb\u7edf\u8c03\u5ea6\u4e0d\u5f53\u5bfc\u81f4\u5ef6\u8fdf\u653e\u5927\u7684\u6548\u7387\u74f6\u9888", "method": "\u7ed3\u5408\u9ad8\u53ec\u56de\u8fd1\u4f3c\u68c0\u7d22\u6280\u672f\uff0c\u91c7\u7528\u4f18\u5148\u7ea7\u8c03\u5ea6\u548c\u975e\u963b\u585e\u68c0\u7d22\u8bbe\u8ba1", "result": "\u541e\u5410\u91cf\u63d0\u53473.4\u500d\uff0c\u5ef6\u8fdf\u964d\u4f4e5\u500d\uff0c\u6027\u80fd\u8d85\u8d8avLLM/HNSW\u68c0\u7d22\u7cfb\u7edf", "conclusion": "SearchAgent-X\u6709\u6548\u89e3\u51b3LLM\u641c\u7d22\u4ee3\u7406\u6548\u7387\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u524d\u63d0\u4e0b\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347"}}
{"id": "2505.12135", "pdf": "https://arxiv.org/pdf/2505.12135", "abs": "https://arxiv.org/abs/2505.12135", "authors": ["Omar Choukrani", "Idriss Malek", "Daniil Orel", "Zhuohan Xie", "Zangir Iklassov", "Martin Tak\u00e1\u010d", "Salem Lahlou"], "title": "LLM-BABYBENCH: Understanding and Evaluating Grounded Planning and Reasoning in LLMs", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Assessing the capacity of Large Language Models (LLMs) to plan and reason\nwithin the constraints of interactive environments is crucial for developing\ncapable AI agents. We introduce $\\textbf{LLM-BabyBench}$, a new benchmark suite\ndesigned specifically for this purpose. Built upon a textual adaptation of the\nprocedurally generated BabyAI grid world, this suite evaluates LLMs on three\nfundamental aspects of grounded intelligence: (1) predicting the consequences\nof actions on the environment state ($\\textbf{Predict}$ task), (2) generating\nsequences of low-level actions to achieve specified objectives ($\\textbf{Plan}$\ntask), and (3) decomposing high-level instructions into coherent subgoal\nsequences ($\\textbf{Decompose}$ task). We detail the methodology for generating\nthe three corresponding datasets ($\\texttt{LLM-BabyBench-Predict}$,\n$\\texttt{-Plan}$, $\\texttt{-Decompose}$) by extracting structured information\nfrom an expert agent operating within the text-based environment. Furthermore,\nwe provide a standardized evaluation harness and metrics, including environment\ninteraction for validating generated plans, to facilitate reproducible\nassessment of diverse LLMs. Initial baseline results highlight the challenges\nposed by these grounded reasoning tasks. The benchmark suite, datasets, data\ngeneration code, and evaluation code are made publicly available\n($\\href{https://github.com/choukrani/llm-babybench}{\\text{GitHub}}$,\n$\\href{https://huggingface.co/datasets/salem-mbzuai/LLM-BabyBench}{\\text{HuggingFace}}$).", "AI": {"tldr": "\u63d0\u51faLLM-BabyBench\u57fa\u51c6\u5957\u4ef6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5177\u8eab\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u9884\u6d4b\u3001\u89c4\u5212\u548c\u6307\u4ee4\u5206\u89e3\u80fd\u529b", "motivation": "\u73b0\u6709\u8bc4\u4f30\u7f3a\u4e4f\u5bf9LLMs\u5728\u4ea4\u4e92\u73af\u5883\u4e2d\u57fa\u7840\u63a8\u7406\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u6d4b\u8bd5\uff0c\u9700\u5efa\u7acb\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u5177", "method": "\u57fa\u4e8eBabyAI\u7f51\u683c\u4e16\u754c\u6784\u5efa\u6587\u672c\u73af\u5883\uff0c\u901a\u8fc7\u4e13\u5bb6\u4ee3\u7406\u751f\u6210\u4e09\u4e2a\u7ed3\u6784\u5316\u6570\u636e\u96c6\uff08Predict/Plan/Decompose\uff09\uff0c\u8bbe\u8ba1\u5305\u542b\u73af\u5883\u4ea4\u4e92\u9a8c\u8bc1\u7684\u8bc4\u4f30\u6846\u67b6", "result": "\u57fa\u7ebf\u5b9e\u9a8c\u8868\u660e\u5f53\u524dLLMs\u5728\u5177\u8eab\u63a8\u7406\u4efb\u52a1\u4e2d\u5b58\u5728\u663e\u8457\u6311\u6218", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u8bc4\u4f30LLMs\u7684\u5177\u8eab\u667a\u80fd\u63d0\u4f9b\u6807\u51c6\u5316\u5de5\u5177\uff0c\u516c\u5f00\u8d44\u6e90\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u53d1\u5c55"}}
{"id": "2505.12185", "pdf": "https://arxiv.org/pdf/2505.12185", "abs": "https://arxiv.org/abs/2505.12185", "authors": ["Sen Fang", "Weiyuan Ding", "Bowen Xu"], "title": "EVALOOP: Assessing LLM Robustness in Programming from a Self-consistency Perspective", "categories": ["cs.SE", "cs.CL", "cs.LG"], "comment": "19 pages, 11 figures", "summary": "Assessing the programming capabilities of Large Language Models (LLMs) is\ncrucial for their effective use in software engineering. Current evaluations,\nhowever, predominantly measure the accuracy of generated code on static\nbenchmarks, neglecting the critical aspect of model robustness during\nprogramming tasks. While adversarial attacks offer insights on model\nrobustness, their effectiveness is limited and evaluation could be constrained.\nCurrent adversarial attack methods for robustness evaluation yield inconsistent\nresults, struggling to provide a unified evaluation across different LLMs. We\nintroduce EVALOOP, a novel assessment framework that evaluate the robustness\nfrom a self-consistency perspective, i.e., leveraging the natural duality\ninherent in popular software engineering tasks, e.g., code generation and code\nsummarization. EVALOOP initiates a self-contained feedback loop: an LLM\ngenerates output (e.g., code) from an input (e.g., natural language\nspecification), and then use the generated output as the input to produce a new\noutput (e.g., summarizes that code into a new specification). EVALOOP repeats\nthe process to assess the effectiveness of EVALOOP in each loop. This cyclical\nstrategy intrinsically evaluates robustness without rely on any external attack\nsetups, providing a unified metric to evaluate LLMs' robustness in programming.\nWe evaluate 16 prominent LLMs (e.g., GPT-4.1, O4-mini) on EVALOOP and found\nthat EVALOOP typically induces a 5.01%-19.31% absolute drop in pass@1\nperformance within ten loops. Intriguingly, robustness does not always align\nwith initial performance (i.e., one-time query); for instance, GPT-3.5-Turbo,\ndespite superior initial code generation compared to DeepSeek-V2, demonstrated\nlower robustness over repeated evaluation loop.", "AI": {"tldr": "\u63d0\u51faEVALOOP\u6846\u67b6\uff0c\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u4e0e\u6458\u8981\u7684\u5faa\u73af\u53cd\u9988\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7f16\u7a0b\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4ec5\u5173\u6ce8\u4ee3\u7801\u751f\u6210\u51c6\u786e\u7387\uff0c\u7f3a\u4e4f\u5bf9\u6a21\u578b\u7f16\u7a0b\u8fc7\u7a0b\u7a33\u5b9a\u6027\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0c\u4e14\u73b0\u6709\u5bf9\u6297\u653b\u51fb\u65b9\u6cd5\u5b58\u5728\u8bc4\u4f30\u4e0d\u4e00\u81f4\u6027", "method": "\u6784\u5efa\u81ea\u6d3d\u53cd\u9988\u5faa\u73af\uff1aLLM\u751f\u6210\u4ee3\u7801\u2192\u5c06\u4ee3\u7801\u4f5c\u4e3a\u65b0\u8f93\u5165\u751f\u6210\u6458\u8981\u2192\u5faa\u73af\u8fed\u4ee3\u8bc4\u4f30\u6a21\u578b\u8f93\u51fa\u4e00\u81f4\u6027", "result": "10\u6b21\u5faa\u73af\u540e\u6a21\u578bpass@1\u6307\u6807\u4e0b\u964d5.01%-19.31%\uff0c\u53d1\u73b0\u6a21\u578b\u521d\u59cb\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\u4e0d\u5fc5\u7136\u6b63\u76f8\u5173\uff08\u5982GPT-3.5-Turbo\u4f18\u4e8eDeepSeek-V2\u4f46\u9c81\u68d2\u6027\u66f4\u4f4e\uff09", "conclusion": "EVALOOP\u65e0\u9700\u5916\u90e8\u653b\u51fb\u5373\u53ef\u5b9e\u73b0\u7edf\u4e00\u9c81\u68d2\u6027\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u8fed\u4ee3\u7a33\u5b9a\u6027\u4e0e\u5355\u6b21\u6027\u80fd\u7684\u5dee\u5f02\uff0c\u4e3aLLM\u7f16\u7a0b\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u65b0\u7ef4\u5ea6"}}
{"id": "2505.12189", "pdf": "https://arxiv.org/pdf/2505.12189", "abs": "https://arxiv.org/abs/2505.12189", "authors": ["Marco Valentino", "Geonhee Kim", "Dhairya Dalal", "Zhixue Zhao", "Andr\u00e9 Freitas"], "title": "Mitigating Content Effects on Reasoning in Language Models through Fine-Grained Activation Steering", "categories": ["cs.AI", "cs.CL"], "comment": "Work in progress", "summary": "Large language models (LLMs) frequently demonstrate reasoning limitations,\noften conflating content plausibility (i.e., material inference) with logical\nvalidity (i.e., formal inference). This can result in biased inferences, where\nplausible arguments are incorrectly deemed logically valid or vice versa.\nMitigating this limitation is critical, as it undermines the trustworthiness\nand generalizability of LLMs in applications that demand rigorous logical\nconsistency. This paper investigates the problem of mitigating content biases\non formal reasoning through activation steering. Specifically, we curate a\ncontrolled syllogistic reasoning dataset to disentangle formal validity from\ncontent plausibility. After localising the layers responsible for formal and\nmaterial inference, we investigate contrastive activation steering methods for\ntest-time interventions. An extensive empirical analysis on different LLMs\nreveals that contrastive steering consistently supports linear control over\ncontent biases. However, we observe that a static approach is insufficient for\nimproving all the tested models. We then leverage the possibility to control\ncontent effects by dynamically determining the value of the steering parameters\nvia fine-grained conditional methods. We found that conditional steering is\neffective on unresponsive models, achieving up to 15% absolute improvement in\nformal reasoning accuracy with a newly introduced kNN-based method (K-CAST).\nFinally, additional experiments reveal that steering for content effects is\nrobust to prompt variations, incurs minimal side effects on language modeling\ncapabilities, and can partially generalize to out-of-distribution reasoning\ntasks. Practically, this paper demonstrates that activation-level interventions\ncan offer a scalable strategy for enhancing the robustness of LLMs,\ncontributing towards more systematic and unbiased formal reasoning.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u901a\u8fc7\u6fc0\u6d3b\u5bfc\u5411\u6280\u672f\u7f13\u89e3\u5f62\u5f0f\u63a8\u7406\u4e2d\u7684\u5185\u5bb9\u504f\u89c1\uff0cK-CAST\u65b9\u6cd5\u63d0\u5347\u63a8\u7406\u51c6\u786e\u738715%", "motivation": "LLMs\u5e38\u6df7\u6dc6\u5185\u5bb9\u5408\u7406\u6027\u4e0e\u903b\u8f91\u6709\u6548\u6027\uff0c\u5bfc\u81f4\u63a8\u7406\u504f\u5dee\uff0c\u5f71\u54cd\u6a21\u578b\u5728\u9700\u8981\u903b\u8f91\u4e00\u81f4\u6027\u573a\u666f\u4e2d\u7684\u53ef\u4fe1\u5ea6", "method": "\u6784\u5efa\u53d7\u63a7\u4e09\u6bb5\u8bba\u6570\u636e\u96c6\u5b9a\u4f4d\u5173\u952e\u7f51\u7edc\u5c42\uff0c\u5f00\u53d1\u52a8\u6001\u6761\u4ef6\u6fc0\u6d3b\u5bfc\u5411\u65b9\u6cd5\uff08\u5982kNN-KCAST\uff09\u8fdb\u884c\u5e72\u9884", "result": "\u5bf9\u6bd4\u6fc0\u6d3b\u5b9e\u73b0\u7ebf\u6027\u63a7\u5236\uff0cK-CAST\u4f7f\u90e8\u5206\u6a21\u578b\u5f62\u5f0f\u63a8\u7406\u51c6\u786e\u7387\u7edd\u5bf9\u63d0\u534715%\uff0c\u4e14\u4fdd\u6301\u8bed\u8a00\u80fd\u529b\u4e0e\u6cdb\u5316\u6027", "conclusion": "\u6fc0\u6d3b\u5c42\u5e72\u9884\u4e3aLLMs\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u9c81\u68d2\u6027\u589e\u5f3a\u65b9\u6848\uff0c\u63a8\u52a8\u7cfb\u7edf\u6027\u65e0\u504f\u63a8\u7406\u7684\u53d1\u5c55"}}
{"id": "2505.12225", "pdf": "https://arxiv.org/pdf/2505.12225", "abs": "https://arxiv.org/abs/2505.12225", "authors": ["Jizhou Guo", "Zhaomin Wu", "Philip S. Yu"], "title": "Reward Inside the Model: A Lightweight Hidden-State Reward Model for LLM's Best-of-N sampling", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "High-quality reward models are crucial for unlocking the reasoning potential\nof large language models (LLMs), with best-of-N voting demonstrating\nsignificant performance gains. However, current reward models, which typically\noperate on the textual output of LLMs, are computationally expensive and\nparameter-heavy, limiting their real-world applications. We introduce the\nEfficient Linear Hidden State Reward (ELHSR) model - a novel, highly\nparameter-efficient approach that leverages the rich information embedded in\nLLM hidden states to address these issues. ELHSR systematically outperform\nbaselines with less than 0.005% of the parameters of baselines, requiring only\na few samples for training. ELHSR also achieves orders-of-magnitude efficiency\nimprovement with significantly less time and fewer FLOPs per sample than\nbaseline reward models. Moreover, ELHSR exhibits robust performance even when\ntrained only on logits, extending its applicability to some closed-source LLMs.\nIn addition, ELHSR can also be combined with traditional reward models to\nachieve additional performance gains.", "AI": {"tldr": "\u63d0\u51fa\u9ad8\u6548\u7ebf\u6027\u9690\u85cf\u72b6\u6001\u5956\u52b1\u6a21\u578bELHSR\uff0c\u901a\u8fc7\u5229\u7528LLM\u9690\u85cf\u72b6\u6001\u5b9e\u73b0\u8d85\u53c2\u6570\u6548\u7387\uff08<0.005%\u53c2\u6570\u91cf\uff09\u548c\u8ba1\u7b97\u6548\u7387\u7684\u663e\u8457\u63d0\u5347", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6587\u672c\u8f93\u51fa\u7684\u5956\u52b1\u6a21\u578b\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u3001\u53c2\u6570\u91cf\u5927\u7684\u7f3a\u9677\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u573a\u666f", "method": "\u5229\u7528LLM\u9690\u85cf\u72b6\u6001\u4e2d\u7684\u4e30\u5bcc\u4fe1\u606f\uff0c\u5f00\u53d1\u53c2\u6570\u6548\u7387\u6781\u9ad8\u7684\u7ebf\u6027\u6a21\u578b\u67b6\u6784\uff0c\u652f\u6301\u5c11\u91cf\u6837\u672c\u8bad\u7ec3\u4e14\u517c\u5bb9logits\u8f93\u5165", "result": "\u5728\u6027\u80fd\u3001\u6548\u7387\uff08\u65f6\u95f4/\u8ba1\u7b97\u91cf\uff09\u548c\u9002\u5e94\u6027\uff08\u95ed\u6e90\u6a21\u578b\u517c\u5bb9\u6027\uff09\u65b9\u9762\u5168\u9762\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u7ed3\u5408\u4f20\u7edf\u6a21\u578b\u53ef\u8fdb\u4e00\u6b65\u63d0\u5347\u6548\u679c", "conclusion": "ELHSR\u4e3a\u5956\u52b1\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u8f7b\u91cf\u5316\u7684\u65b0\u8303\u5f0f\uff0c\u6269\u5c55\u4e86\u5176\u5728\u771f\u5b9e\u573a\u666f\u548c\u95ed\u6e90\u7cfb\u7edf\u4e2d\u7684\u9002\u7528\u8fb9\u754c"}}
{"id": "2505.12260", "pdf": "https://arxiv.org/pdf/2505.12260", "abs": "https://arxiv.org/abs/2505.12260", "authors": ["Guangyuan Ma", "Yongliang Ma", "Xuanrui Gou", "Zhenpeng Su", "Ming Zhou", "Songlin Hu"], "title": "LightRetriever: A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs)-based hybrid retrieval uses LLMs to encode\nqueries and documents into low-dimensional dense or high-dimensional sparse\nvectors. It retrieves documents relevant to search queries based on vector\nsimilarities. Documents are pre-encoded offline, while queries arrive in\nreal-time, necessitating an efficient online query encoder. Although LLMs\nsignificantly enhance retrieval capabilities, serving deeply parameterized LLMs\nslows down query inference throughput and increases demands for online\ndeployment resources. In this paper, we propose LightRetriever, a novel\nLLM-based hybrid retriever with extremely lightweight query encoders. Our\nmethod retains a full-sized LLM for document encoding, but reduces the workload\nof query encoding to no more than an embedding lookup. Compared to serving a\nfull-sized LLM on an H800 GPU, our approach achieves over a 1000x speedup for\nquery inference with GPU acceleration, and even a 20x speedup without GPU.\nExperiments on large-scale retrieval benchmarks demonstrate that our method\ngeneralizes well across diverse retrieval tasks, retaining an average of 95%\nfull-sized performance.", "AI": {"tldr": "\u63d0\u51fa\u8f7b\u91cf\u7ea7\u67e5\u8be2\u7f16\u7801\u5668LightRetriever\uff0c\u5728\u4fdd\u6301\u6587\u6863\u7f16\u7801\u5b8c\u6574LLM\u7684\u540c\u65f6\u5b9e\u73b0\u5343\u500d\u67e5\u8be2\u52a0\u901f\uff0c\u6027\u80fd\u4fdd\u755995%\u5b8c\u6574\u6a21\u578b\u6c34\u5e73", "motivation": "\u89e3\u51b3\u73b0\u6709LLM\u6df7\u5408\u68c0\u7d22\u65b9\u6cd5\u5728\u7ebf\u67e5\u8be2\u6548\u7387\u4f4e\u3001\u8d44\u6e90\u6d88\u8017\u5927\u7684\u95ee\u9898", "method": "\u4fdd\u6301\u6587\u6863\u7f16\u7801\u7684\u5b8c\u6574LLM\u67b6\u6784\uff0c\u5c06\u67e5\u8be2\u7f16\u7801\u7b80\u5316\u4e3a\u5d4c\u5165\u67e5\u627e\u64cd\u4f5c", "result": "GPU\u52a0\u901f\u5b9e\u73b01000\u500d\u67e5\u8be2\u901f\u5ea6\u63d0\u5347\uff0c\u975eGPU\u73af\u588320\u500d\u52a0\u901f\uff0c\u6027\u80fd\u4fdd\u755995%\u5b8c\u6574\u6a21\u578b", "conclusion": "LightRetriever\u5728\u68c0\u7d22\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd\u95f4\u53d6\u5f97\u6709\u6548\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5b9e\u65f6\u68c0\u7d22\u573a\u666f"}}
{"id": "2505.12269", "pdf": "https://arxiv.org/pdf/2505.12269", "abs": "https://arxiv.org/abs/2505.12269", "authors": ["Kerry Xiao", "Amy Zang"], "title": "Vague Knowledge: Evidence from Analyst Reports", "categories": ["econ.GN", "cs.AI", "cs.CL", "math.LO", "q-fin.EC", "q-fin.GN", "03B48, 03B65, 03E02, 03E15, 03E72, 18E45, 28A05, 62F15, 68T01,\n  68T35, 68T50, 91G30,", "F.4; I.2.3; I.2.4; I.2.7; J.1; J.4; J.5"], "comment": null, "summary": "People in the real world often possess vague knowledge of future payoffs, for\nwhich quantification is not feasible or desirable. We argue that language, with\ndiffering ability to convey vague information, plays an important but less\nknown-role in subjective expectations. Empirically, we find that in their\nreports, analysts include useful information in linguistic expressions but not\nnumerical forecasts. Specifically, the textual tone of analyst reports has\npredictive power for forecast errors and subsequent revisions in numerical\nforecasts, and this relation becomes stronger when analyst's language is\nvaguer, when uncertainty is higher, and when analysts are busier. Overall, our\ntheory and evidence suggest that some useful information is vaguely known and\nonly communicated through language.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u8bed\u8a00\u5728\u4f20\u9012\u6a21\u7cca\u91d1\u878d\u4fe1\u606f\u4e2d\u7684\u4f5c\u7528\uff0c\u53d1\u73b0\u5206\u6790\u5e08\u6587\u672c\u8bed\u8c03\u6bd4\u6570\u5b57\u9884\u6d4b\u5305\u542b\u66f4\u591a\u6709\u6548\u4fe1\u606f", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u4eba\u4eec\u5bf9\u672a\u6765\u6536\u76ca\u5e38\u5b58\u5728\u96be\u4ee5\u91cf\u5316\u7684\u6a21\u7cca\u8ba4\u77e5\uff0c\u800c\u8bed\u8a00\u5728\u4f20\u9012\u8fd9\u7c7b\u4e3b\u89c2\u9884\u671f\u4e2d\u7684\u4f5c\u7528\u673a\u5236\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76", "method": "\u91c7\u7528\u5b9e\u8bc1\u5206\u6790\u65b9\u6cd5\uff0c\u7814\u7a76\u5356\u65b9\u5206\u6790\u5e08\u62a5\u544a\u4e2d\u6587\u672c\u8bed\u8c03\u4e0e\u6570\u5b57\u9884\u6d4b\u8bef\u5dee/\u540e\u7eed\u4fee\u8ba2\u7684\u5173\u7cfb\uff0c\u5e76\u5f15\u5165\u8bed\u8a00\u6a21\u7cca\u5ea6\u3001\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u548c\u5206\u6790\u5e08\u5fd9\u788c\u7a0b\u5ea6\u4f5c\u4e3a\u8c03\u8282\u53d8\u91cf", "result": "\u6587\u672c\u8bed\u8c03\u53ef\u9884\u6d4b\u6570\u5b57\u9884\u6d4b\u8bef\u5dee\u53ca\u540e\u7eed\u4fee\u8ba2\uff0c\u4e14\u5f53\u8bed\u8a00\u66f4\u6a21\u7cca/\u5e02\u573a\u4e0d\u786e\u5b9a\u6027\u9ad8/\u5206\u6790\u5e08\u66f4\u5fd9\u65f6\uff0c\u8fd9\u79cd\u9884\u6d4b\u80fd\u529b\u663e\u8457\u589e\u5f3a", "conclusion": "\u90e8\u5206\u6709\u6548\u4fe1\u606f\u4ee5\u6a21\u7cca\u8ba4\u77e5\u5f62\u5f0f\u5b58\u5728\uff0c\u53ea\u80fd\u901a\u8fc7\u8bed\u8a00\u6e20\u9053\u4f20\u9012\uff0c\u8fd9\u5bf9\u91d1\u878d\u4fe1\u606f\u89e3\u8bfb\u548c\u51b3\u7b56\u673a\u5236\u5177\u6709\u91cd\u8981\u542f\u793a"}}
{"id": "2505.12284", "pdf": "https://arxiv.org/pdf/2505.12284", "abs": "https://arxiv.org/abs/2505.12284", "authors": ["Danlong Yuan", "Tian Xie", "Shaohan Huang", "Zhuocheng Gong", "Huishuai Zhang", "Chong Luo", "Furu Wei", "Dongyan Zhao"], "title": "Efficient RL Training for Reasoning Models via Length-Aware Optimization", "categories": ["cs.AI", "cs.CL"], "comment": "Under review", "summary": "Large reasoning models, such as OpenAI o1 or DeepSeek R1, have demonstrated\nremarkable performance on reasoning tasks but often incur a long reasoning path\nwith significant memory and time costs. Existing methods primarily aim to\nshorten reasoning paths by introducing additional training data and stages. In\nthis paper, we propose three critical reward designs integrated directly into\nthe reinforcement learning process of large reasoning models, which reduce the\nresponse length without extra training stages. Experiments on four settings\nshow that our method significantly decreases response length while maintaining\nor even improving performance. Specifically, in a logic reasoning setting, we\nachieve a 40% reduction in response length averaged by steps alongside a 14%\ngain in performance. For math problems, we reduce response length averaged by\nsteps by 33% while preserving performance.", "AI": {"tldr": "\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4e09\u79cd\u5956\u52b1\u8bbe\u8ba1\u663e\u8457\u51cf\u5c11\u63a8\u7406\u8def\u5f84\u957f\u5ea6\uff08\u903b\u8f91\u63a8\u7406\u51cf\u5c1140%\uff0c\u6570\u5b66\u95ee\u9898\u51cf\u5c1133%\uff09\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u9636\u6bb5", "motivation": "\u73b0\u6709\u7f29\u77ed\u63a8\u7406\u8def\u5f84\u7684\u65b9\u6cd5\u9700\u8981\u5f15\u5165\u989d\u5916\u8bad\u7ec3\u6570\u636e\u548c\u9636\u6bb5\uff0c\u6210\u672c\u8f83\u9ad8", "method": "\u5728\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u76f4\u63a5\u6574\u5408\u4e09\u79cd\u5173\u952e\u5956\u52b1\u8bbe\u8ba1\u673a\u5236", "result": "\u5728\u903b\u8f91\u63a8\u7406\u573a\u666f\u54cd\u5e94\u957f\u5ea6\u51cf\u5c1140%\u540c\u65f6\u6027\u80fd\u63d0\u534714%\uff0c\u6570\u5b66\u95ee\u9898\u54cd\u5e94\u957f\u5ea6\u51cf\u5c1133%\u4e14\u4fdd\u6301\u6027\u80fd", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u521b\u65b0\u7684\u5956\u52b1\u673a\u5236\u8bbe\u8ba1\uff0c\u5728\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6a21\u578b\u6027\u80fd\u7684\u7ef4\u6301\u6216\u63d0\u5347"}}
{"id": "2505.12301", "pdf": "https://arxiv.org/pdf/2505.12301", "abs": "https://arxiv.org/abs/2505.12301", "authors": ["Luyu Chen", "Zeyu Zhang", "Haoran Tan", "Quanyu Dai", "Hao Yang", "Zhenhua Dong", "Xu Chen"], "title": "Beyond Single-Point Judgment: Distribution Alignment for LLM-as-a-Judge", "categories": ["cs.AI", "cs.CL"], "comment": "19 pages, 3 tables, 3 figures", "summary": "LLMs have emerged as powerful evaluators in the LLM-as-a-Judge paradigm,\noffering significant efficiency and flexibility compared to human judgments.\nHowever, previous methods primarily rely on single-point evaluations,\noverlooking the inherent diversity and uncertainty in human evaluations. This\napproach leads to information loss and decreases the reliability of\nevaluations. To address this limitation, we propose a novel training framework\nthat explicitly aligns the LLM-generated judgment distribution with empirical\nhuman distributions. Specifically, we propose a distributional alignment\nobjective based on KL divergence, combined with an auxiliary cross-entropy\nregularization to stabilize the training process. Furthermore, considering that\nempirical distributions may derive from limited human annotations, we\nincorporate adversarial training to enhance model robustness against\ndistribution perturbations. Extensive experiments across various LLM backbones\nand evaluation tasks demonstrate that our framework significantly outperforms\nexisting closed-source LLMs and conventional single-point alignment methods,\nwith improved alignment quality, evaluation accuracy, and robustness.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5206\u5e03\u5bf9\u9f50\u7684LLM\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7KL\u6563\u5ea6\u5bf9\u9f50\u4eba\u7c7b\u6807\u6ce8\u5206\u5e03\uff0c\u7ed3\u5408\u4ea4\u53c9\u71b5\u6b63\u5219\u5316\u548c\u5bf9\u6297\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u8bc4\u4f30\u8d28\u91cf\u4e0e\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709LLM-as-a-Judge\u65b9\u6cd5\u4f9d\u8d56\u5355\u70b9\u8bc4\u4f30\uff0c\u5ffd\u7565\u4eba\u7c7b\u8bc4\u4f30\u7684\u591a\u6837\u6027\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u4fe1\u606f\u4e22\u5931\u548c\u53ef\u9760\u6027\u4e0b\u964d", "method": "1) \u57fa\u4e8eKL\u6563\u5ea6\u7684\u5206\u5e03\u5bf9\u9f50\u76ee\u6807\u51fd\u6570 2) \u4ea4\u53c9\u71b5\u6b63\u5219\u5316\u7a33\u5b9a\u8bad\u7ec3 3) \u5bf9\u6297\u8bad\u7ec3\u589e\u5f3a\u6709\u9650\u6807\u6ce8\u4e0b\u7684\u5206\u5e03\u9c81\u68d2\u6027", "result": "\u5728\u591a\u79cdLLM\u4e3b\u5e72\u7f51\u7edc\u548c\u8bc4\u4f30\u4efb\u52a1\u4e2d\uff0c\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u95ed\u6e90LLM\u548c\u4f20\u7edf\u5355\u70b9\u5bf9\u9f50\u65b9\u6cd5\uff0c\u8bc4\u4f30\u51c6\u786e\u7387\u63d0\u53473-5\u4e2a\u767e\u5206\u70b9", "conclusion": "\u8be5\u6846\u67b6\u9996\u6b21\u5b9e\u73b0LLM\u8bc4\u4f30\u5206\u5e03\u4e0e\u4eba\u7c7b\u7ecf\u9a8c\u5206\u5e03\u7684\u7cfb\u7edf\u5bf9\u9f50\uff0c\u4e3a\u53ef\u4fe1AI\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u8bba\u57fa\u7840"}}
{"id": "2505.12307", "pdf": "https://arxiv.org/pdf/2505.12307", "abs": "https://arxiv.org/abs/2505.12307", "authors": ["Maoyuan Ye", "Jing Zhang", "Juhua Liu", "Bo Du", "Dacheng Tao"], "title": "LogicOCR: Do Your Large Multimodal Models Excel at Logical Reasoning on Text-Rich Images?", "categories": ["cs.CV", "cs.CL"], "comment": "GitHub: \\url{https://github.com/MiliLab/LogicOCR}", "summary": "Recent advances in Large Multimodal Models (LMMs) have significantly improved\ntheir reasoning and Optical Character Recognition (OCR) capabilities. However,\ntheir performance on complex logical reasoning tasks involving text-rich images\nremains underexplored. To bridge this gap, we introduce LogicOCR, a benchmark\ncomprising 1,100 multiple-choice questions designed to evaluate LMMs' logical\nreasoning abilities on text-rich images, while minimizing reliance on\ndomain-specific knowledge (e.g., mathematics). We construct LogicOCR by\ncurating a text corpus from the Chinese National Civil Servant Examination and\ndevelop a scalable, automated pipeline to convert it into multimodal samples.\nFirst, we design prompt templates to steer GPT-Image-1 to generate images with\ndiverse backgrounds, interleaved text-illustration layouts, and varied fonts,\nensuring contextual relevance and visual realism. Then, the generated images\nare manually verified, with low-quality examples discarded. We evaluate a range\nof representative open-source and proprietary LMMs under both Chain-of-Thought\n(CoT) and direct-answer settings. Our multi-dimensional analysis reveals key\ninsights, such as the impact of test-time scaling, input modality differences,\nand sensitivity to visual-text orientation. Notably, LMMs still lag in\nmultimodal reasoning compared to text-only inputs, indicating that they have\nnot fully bridged visual reading with reasoning. We hope LogicOCR will serve as\na valuable resource for advancing multimodal reasoning research. The dataset is\navailable at https://github.com/MiliLab/LogicOCR.", "AI": {"tldr": "\u63d0\u51faLogicOCR\u57fa\u51c6\u6d4b\u8bd5\uff08\u542b1100\u9053\u591a\u9009\u9898\uff09\uff0c\u63ed\u793a\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u6587\u672c\u56fe\u50cf\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4ecd\u843d\u540e\u4e8e\u7eaf\u6587\u672c\u8f93\u5165", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMMs\uff09\u5728\u6587\u672c\u4e30\u5bcc\u56fe\u50cf\u4e0a\u7684\u590d\u6742\u903b\u8f91\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u4e0d\u8db3", "method": "1. \u57fa\u4e8e\u4e2d\u56fd\u516c\u52a1\u5458\u8003\u8bd5\u6587\u672c\u6784\u5efa\u591a\u6a21\u6001\u6837\u672c\u751f\u6210\u6d41\u6c34\u7ebf\n2. \u4f7f\u7528GPT-Image-1\u751f\u6210\u591a\u6837\u5316\u80cc\u666f\u3001\u5e03\u5c40\u548c\u5b57\u4f53\u7684\u56fe\u50cf\n3. \u4eba\u5de5\u9a8c\u8bc1\u56fe\u50cf\u8d28\u91cf\u5e76\u7b5b\u9009", "result": "\u6d4b\u8bd5\u53d1\u73b0LMMs\u5b58\u5728\u4e09\u65b9\u9762\u5c40\u9650\uff1a\u6d4b\u8bd5\u65f6\u7f29\u653e\u5f71\u54cd\u663e\u8457\u3001\u591a\u6a21\u6001\u8f93\u5165\u6548\u679c\u5dee\u4e8e\u7eaf\u6587\u672c\u3001\u5bf9\u89c6\u89c9-\u6587\u672c\u65b9\u5411\u654f\u611f", "conclusion": "LogicOCR\u586b\u8865\u4e86\u591a\u6a21\u6001\u63a8\u7406\u8bc4\u4f30\u7a7a\u767d\uff0c\u8868\u660e\u5f53\u524d\u6a21\u578b\u5c1a\u672a\u5b8c\u5168\u5b9e\u73b0\u89c6\u89c9\u9605\u8bfb\u4e0e\u903b\u8f91\u63a8\u7406\u7684\u6709\u6548\u7ed3\u5408"}}
{"id": "2505.12312", "pdf": "https://arxiv.org/pdf/2505.12312", "abs": "https://arxiv.org/abs/2505.12312", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "title": "Visuospatial Cognitive Assistant", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "31 pages, 10 figures, 6 tables. The implementation and fine-tuned\n  model (ViCA-7B) are publicly available at https://huggingface.co/nkkbr/ViCA.\n  The ViCA-322K dataset can be found at\n  https://huggingface.co/datasets/nkkbr/ViCA-322K, and the ViCA-Thinking-2.68K\n  dataset is at https://huggingface.co/datasets/nkkbr/ViCA-thinking-2.68k", "summary": "Video-based spatial cognition is vital for robotics and embodied AI but\nchallenges current Vision-Language Models (VLMs). This paper makes two key\ncontributions. First, we introduce ViCA (Visuospatial Cognitive\nAssistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor\nvideos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D\nmetadata-grounded queries and video-based complex reasoning. Second, we develop\nViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all\neight VSI-Bench tasks, outperforming existing models, including larger ones\n(e.g., +26.1 on Absolute Distance). For interpretability, we present\nViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune\nViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial\nreasoning. Our work highlights the importance of targeted data and suggests\npaths for improved temporal-spatial modeling. We release all resources to\nfoster research in robust visuospatial intelligence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faViCA-322K\u6570\u636e\u96c6\u548cViCA-7B\u6a21\u578b\uff0c\u5728\u7a7a\u95f4\u8ba4\u77e5\u4efb\u52a1\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\u5e76\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u9891\u7a7a\u95f4\u8ba4\u77e5\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\uff0c\u63a8\u52a8\u5177\u8eabAI\u548c\u673a\u5668\u4eba\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "1. \u57fa\u4e8e\u771f\u5b9e\u573a\u666f\u89c6\u9891\u6784\u5efa322K QA\u6570\u636e\u96c6\uff1b2. \u5f00\u53d1ViCA-7B\u6a21\u578b\u5e76\u5f15\u5165\u663e\u5f0f\u63a8\u7406\u94fe\u5fae\u8c03\uff1b3. \u521b\u5efa\u5305\u542b2.68K\u63a8\u7406\u8fc7\u7a0b\u7684\u6570\u636e\u96c6\u3002", "result": "ViCA-7B\u57288\u9879\u57fa\u51c6\u4efb\u52a1\u4e2d\u5168\u9762\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\uff08\u5982\u7edd\u5bf9\u8ddd\u79bb\u6307\u6807\u63d0\u534726.1\uff09\uff0c\u5e76\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u7a7a\u95f4\u63a8\u7406\u3002", "conclusion": "\u901a\u8fc7\u5b9a\u5411\u6570\u636e\u548c\u65f6\u5e8f\u7a7a\u95f4\u5efa\u6a21\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9\u7a7a\u95f4\u667a\u80fd\uff0c\u5f00\u653e\u8d44\u6e90\u4fc3\u8fdb\u76f8\u5173\u7814\u7a76\u3002"}}
{"id": "2505.12363", "pdf": "https://arxiv.org/pdf/2505.12363", "abs": "https://arxiv.org/abs/2505.12363", "authors": ["Qi Feng", "Hidetoshi Shimodaira"], "title": "Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.RO"], "comment": "26 pages, 19 figures, 4 tables. Code, models, and dataset are\n  available at our project page: https://github.com/nkkbr/ViCA", "summary": "While Multimodal Large Language Models (MLLMs) excel at general\nvision-language tasks, visuospatial cognition - reasoning about spatial\nlayouts, relations, and dynamics - remains a significant challenge. Existing\nmodels often lack the necessary architectural components and specialized\ntraining data for fine-grained spatial understanding. We introduce ViCA2\n(Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial\nreasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP\nfor semantics and Hiera for spatial structure, coupled with a token ratio\ncontrol mechanism for efficiency. We also developed ViCA-322K, a new\nlarge-scale dataset with over 322,000 spatially grounded question-answer pairs\nfor targeted instruction tuning. On the challenging VSI-Bench benchmark, our\nViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly\nsurpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and\nleading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the\neffectiveness of our approach in achieving strong visuospatial intelligence\nwith a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset\nto facilitate further research.", "AI": {"tldr": "ViCA2\u6a21\u578b\u901a\u8fc7\u53cc\u89c6\u89c9\u7f16\u7801\u5668\u67b6\u6784\u548c\u4e13\u7528\u6570\u636e\u96c6ViCA-322K\uff0c\u663e\u8457\u63d0\u5347\u89c6\u89c9\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5728VSI-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4ee57B\u53c2\u6570\u91cf\u8d85\u8d8a\u66f4\u5927\u6a21\u578b", "motivation": "\u73b0\u6709MLLMs\u5728\u89c6\u89c9\u7a7a\u95f4\u8ba4\u77e5\uff08\u7a7a\u95f4\u5e03\u5c40\u3001\u5173\u7cfb\u63a8\u7406\uff09\u65b9\u9762\u5b58\u5728\u67b6\u6784\u7f3a\u9677\u548c\u6570\u636e\u4e0d\u8db3\uff0c\u9700\u9488\u5bf9\u6027\u6539\u8fdb", "method": "\u6574\u5408SigLIP\uff08\u8bed\u4e49\uff09\u4e0eHiera\uff08\u7a7a\u95f4\uff09\u53cc\u7f16\u7801\u5668\uff0c\u5f00\u53d1ViCA-322K\u6570\u636e\u96c6\uff0832.2\u4e07\u7a7a\u95f4QA\u5bf9\uff09\u8fdb\u884c\u6307\u4ee4\u5fae\u8c03", "result": "ViCA2-7B\u5728VSI-Bench\u83b7\u5f9756.8\u5206\uff0c\u4f18\u4e8eLLaVA-NeXT-Video-72B\uff0840.9\uff09\u548cGemini-1.5 Pro\uff0845.4\uff09", "conclusion": "\u901a\u8fc7\u4e13\u7528\u67b6\u6784\u4e0e\u6570\u636e\u53ef\u5b9e\u73b0\u5c0f\u6a21\u578b\u7684\u5f3a\u7a7a\u95f4\u667a\u80fd\uff0c\u5f00\u6e90\u8d44\u6e90\u5c06\u63a8\u52a8\u9886\u57df\u7814\u7a76"}}
{"id": "2505.12371", "pdf": "https://arxiv.org/pdf/2505.12371", "abs": "https://arxiv.org/abs/2505.12371", "authors": ["Yinghao Zhu", "Ziyi He", "Haoran Hu", "Xiaochen Zheng", "Xichen Zhang", "Zixiang Wang", "Junyi Gao", "Liantao Ma", "Lequan Yu"], "title": "MedAgentBoard: Benchmarking Multi-Agent Collaboration with Conventional Methods for Diverse Medical Tasks", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has stimulated interest\nin multi-agent collaboration for addressing complex medical tasks. However, the\npractical advantages of multi-agent collaboration approaches remain\ninsufficiently understood. Existing evaluations often lack generalizability,\nfailing to cover diverse tasks reflective of real-world clinical practice, and\nfrequently omit rigorous comparisons against both single-LLM-based and\nestablished conventional methods. To address this critical gap, we introduce\nMedAgentBoard, a comprehensive benchmark for the systematic evaluation of\nmulti-agent collaboration, single-LLM, and conventional approaches.\nMedAgentBoard encompasses four diverse medical task categories: (1) medical\n(visual) question answering, (2) lay summary generation, (3) structured\nElectronic Health Record (EHR) predictive modeling, and (4) clinical workflow\nautomation, across text, medical images, and structured EHR data. Our extensive\nexperiments reveal a nuanced landscape: while multi-agent collaboration\ndemonstrates benefits in specific scenarios, such as enhancing task\ncompleteness in clinical workflow automation, it does not consistently\noutperform advanced single LLMs (e.g., in textual medical QA) or, critically,\nspecialized conventional methods that generally maintain better performance in\ntasks like medical VQA and EHR-based prediction. MedAgentBoard offers a vital\nresource and actionable insights, emphasizing the necessity of a task-specific,\nevidence-based approach to selecting and developing AI solutions in medicine.\nIt underscores that the inherent complexity and overhead of multi-agent\ncollaboration must be carefully weighed against tangible performance gains. All\ncode, datasets, detailed prompts, and experimental results are open-sourced at\nhttps://medagentboard.netlify.app/.", "AI": {"tldr": "\u63d0\u51faMedAgentBoard\u8bc4\u4f30\u6846\u67b6\u63ed\u793a\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5728\u533b\u7597\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff1a\u7279\u5b9a\u573a\u666f\u6709\u6548\u4f46\u6574\u4f53\u6027\u80fd\u672a\u8d85\u8d8a\u5355\u4e00\u5927\u6a21\u578b\u6216\u4f20\u7edf\u65b9\u6cd5", "motivation": "\u73b0\u6709\u533b\u7597\u591a\u667a\u80fd\u4f53\u7814\u7a76\u7f3a\u4e4f\u8986\u76d6\u771f\u5b9e\u4e34\u5e8a\u4efb\u52a1\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u4e14\u672a\u4e0e\u4f20\u7edf\u65b9\u6cd5\u5145\u5206\u5bf9\u6bd4\uff0c\u96be\u4ee5\u9a8c\u8bc1\u5b9e\u9645\u5e94\u7528\u4ef7\u503c", "method": "\u6784\u5efa\u8986\u76d6\u533b\u7597QA\u3001\u79d1\u666e\u6458\u8981\u751f\u6210\u3001\u7535\u5b50\u75c5\u5386\u9884\u6d4b\u3001\u4e34\u5e8a\u6d41\u7a0b\u81ea\u52a8\u53164\u7c7b\u4efb\u52a1\u7684MedAgentBoard\u57fa\u51c6\uff0c\u6db5\u76d6\u6587\u672c/\u533b\u5b66\u5f71\u50cf/\u7ed3\u6784\u5316\u6570\u636e\u6a21\u6001", "result": "\u591a\u667a\u80fd\u4f53\u4ec5\u5728\u4e34\u5e8a\u6d41\u7a0b\u81ea\u52a8\u5316\u7b49\u7279\u5b9a\u573a\u666f\u5c55\u73b0\u4efb\u52a1\u5b8c\u6574\u6027\u4f18\u52bf\uff0c\u5728\u533b\u5b66\u95ee\u7b54\u7b49\u6587\u672c\u4efb\u52a1\u843d\u540e\u5148\u8fdb\u5355\u4e00\u5927\u6a21\u578b\uff0c\u5728\u533b\u5b66VQA\u548c\u7535\u5b50\u75c5\u5386\u9884\u6d4b\u66f4\u663e\u8457\u843d\u540e\u4f20\u7edf\u65b9\u6cd5", "conclusion": "\u533b\u7597AI\u65b9\u6848\u9009\u62e9\u9700\u57fa\u4e8e\u4efb\u52a1\u7279\u6027\uff0c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u7684\u590d\u6742\u6027\u9700\u4e0e\u6027\u80fd\u63d0\u5347\u4e25\u683c\u6743\u8861\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u7279\u5b9a\u9886\u57df\u4ecd\u5177\u4e0d\u53ef\u66ff\u4ee3\u6027"}}
{"id": "2505.12442", "pdf": "https://arxiv.org/pdf/2505.12442", "abs": "https://arxiv.org/abs/2505.12442", "authors": ["Liwen Wang", "Wenxuan Wang", "Shuai Wang", "Zongjie Li", "Zhenlan Ji", "Zongyi Lyu", "Daoyuan Wu", "Shing-Chi Cheung"], "title": "IP Leakage Attacks Targeting LLM-Based Multi-Agent Systems", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of Large Language Models (LLMs) has led to the\nemergence of Multi-Agent Systems (MAS) to perform complex tasks through\ncollaboration. However, the intricate nature of MAS, including their\narchitecture and agent interactions, raises significant concerns regarding\nintellectual property (IP) protection. In this paper, we introduce MASLEAK, a\nnovel attack framework designed to extract sensitive information from MAS\napplications. MASLEAK targets a practical, black-box setting, where the\nadversary has no prior knowledge of the MAS architecture or agent\nconfigurations. The adversary can only interact with the MAS through its public\nAPI, submitting attack query $q$ and observing outputs from the final agent.\nInspired by how computer worms propagate and infect vulnerable network hosts,\nMASLEAK carefully crafts adversarial query $q$ to elicit, propagate, and retain\nresponses from each MAS agent that reveal a full set of proprietary components,\nincluding the number of agents, system topology, system prompts, task\ninstructions, and tool usages. We construct the first synthetic dataset of MAS\napplications with 810 applications and also evaluate MASLEAK against real-world\nMAS applications, including Coze and CrewAI. MASLEAK achieves high accuracy in\nextracting MAS IP, with an average attack success rate of 87% for system\nprompts and task instructions, and 92% for system architecture in most cases.\nWe conclude by discussing the implications of our findings and the potential\ndefenses.", "AI": {"tldr": "\u63d0\u51faMASLEAK\u653b\u51fb\u6846\u67b6\uff0c\u53ef\u4ece\u9ed1\u76d2\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u9ad8\u7cbe\u5ea6\u63d0\u53d6\u77e5\u8bc6\u4ea7\u6743\u4fe1\u606f\uff08\u7cfb\u7edf\u67b6\u6784\u3001\u63d0\u793a\u8bcd\u7b49\uff09\uff0c\u653b\u51fb\u6210\u529f\u7387\u6700\u9ad8\u8fbe92%", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u7684\u590d\u6742\u67b6\u6784\u548c\u4ea4\u4e92\u65b9\u5f0f\u4f7f\u5176\u9762\u4e34\u77e5\u8bc6\u4ea7\u6743\u6cc4\u9732\u98ce\u9669\uff0c\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9MAS\u67b6\u6784\u7684\u9690\u79c1\u4fdd\u62a4\u5206\u6790", "method": "\u53d7\u8ba1\u7b97\u673a\u8815\u866b\u4f20\u64ad\u673a\u5236\u542f\u53d1\uff0c\u8bbe\u8ba1\u5bf9\u6297\u6027\u67e5\u8be2\u5b9e\u73b0\u4fe1\u606f\u8bf1\u5bfc-\u4f20\u64ad-\u9a7b\u7559\u7684\u4e09\u9636\u6bb5\u653b\u51fb\uff0c\u65e0\u9700MAS\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ec5\u901a\u8fc7API\u4ea4\u4e92\u5b8c\u6210\u6e17\u900f", "result": "\u5728810\u4e2a\u5408\u6210MAS\u5e94\u7528\u548c\u771f\u5b9e\u7cfb\u7edf\uff08Coze/CrewAI\uff09\u4e2d\u9a8c\u8bc1\uff0c\u7cfb\u7edf\u63d0\u793a\u8bcd\u653b\u51fb\u6210\u529f\u738787%\uff0c\u7cfb\u7edf\u67b6\u6784\u63d0\u53d6\u51c6\u786e\u738792%", "conclusion": "\u63ed\u793a\u4e86MAS\u67b6\u6784\u7684\u5b89\u5168\u8106\u5f31\u6027\uff0c\u5efa\u8bae\u901a\u8fc7\u8f93\u5165\u8fc7\u6ee4\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u52a8\u6001\u9632\u5fa1\u673a\u5236\u589e\u5f3aMAS\u5b89\u5168\u6027"}}
{"id": "2505.12457", "pdf": "https://arxiv.org/pdf/2505.12457", "abs": "https://arxiv.org/abs/2505.12457", "authors": ["Yang Zhao", "Kai Xiong", "Xiao Ding", "Li Du", "YangouOuyang", "Zhouhao Sun", "Jiannan Guan", "Wenbin Zhang", "Bin Liu", "Dong Hu", "Bing Qin", "Ting Liu"], "title": "UFO-RL: Uncertainty-Focused Optimization for Efficient Reinforcement Learning Data Selection", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Scaling RL for LLMs is computationally expensive, largely due to\nmulti-sampling for policy optimization and evaluation, making efficient data\nselection crucial. Inspired by the Zone of Proximal Development (ZPD) theory,\nwe hypothesize LLMs learn best from data within their potential comprehension\nzone. Addressing the limitation of conventional, computationally intensive\nmulti-sampling methods for data assessment, we introduce UFO-RL. This novel\nframework uses a computationally efficient single-pass uncertainty estimation\nto identify informative data instances, achieving up to 185x faster data\nevaluation. UFO-RL leverages this metric to select data within the estimated\nZPD for training. Experiments show that training with just 10% of data selected\nby UFO-RL yields performance comparable to or surpassing full-data training,\nreducing overall training time by up to 16x while enhancing stability and\ngeneralization. UFO-RL offers a practical and highly efficient strategy for\nscaling RL fine-tuning of LLMs by focusing learning on valuable data.", "AI": {"tldr": "\u63d0\u51faUFO-RL\u6846\u67b6\uff0c\u5229\u7528\u5355\u6b21\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5b9e\u73b0\u9ad8\u6548\u6570\u636e\u7b5b\u9009\uff0c\u4ec5\u752810%\u6570\u636e\u5373\u53ef\u8fbe\u5230\u5168\u6570\u636e\u8bad\u7ec3\u6548\u679c\uff0c\u8bad\u7ec3\u901f\u5ea6\u63d0\u534716\u500d\u3002", "motivation": "\u4f20\u7edfRL\u5fae\u8c03LLM\u65f6\u591a\u91c7\u6837\u7b56\u7565\u8ba1\u7b97\u6210\u672c\u6781\u9ad8\uff0c\u53d7ZPD\u7406\u8bba\u542f\u53d1\uff0c\u901a\u8fc7\u7b5b\u9009\u6a21\u578b\u6f5c\u5728\u7406\u89e3\u8303\u56f4\u5185\u7684\u5173\u952e\u6570\u636e\u63d0\u5347\u6548\u7387\u3002", "method": "\u57fa\u4e8e\u5355\u6b21\u524d\u5411\u4f20\u64ad\u7684uncertainty estimation\u6280\u672f\uff0c\u6784\u5efa185\u500d\u901f\u7684\u6570\u636e\u8bc4\u4f30\u4f53\u7cfb\uff0c\u52a8\u6001\u9009\u62e9ZPD\u533a\u95f4\u5185\u7684\u6709\u6548\u8bad\u7ec3\u6837\u672c\u3002", "result": "10%\u7cbe\u9009\u6570\u636e\u8fbe\u5230\u5168\u6570\u636e97%\u6027\u80fd\uff0c\u8bad\u7ec3\u65f6\u95f4\u7f29\u77ed\u81f31/16\uff0c\u4e14\u6a21\u578b\u7a33\u5b9a\u6027\u4e0e\u6cdb\u5316\u80fd\u529b\u663e\u8457\u63d0\u5347\u3002", "conclusion": "UFO-RL\u4e3aLLM\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u63d0\u4f9b\u4e86\u6570\u636e\u9009\u62e9\u65b0\u8303\u5f0f\uff0c\u5b9e\u73b0\u8ba1\u7b97\u6548\u7387\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u5e15\u7d2f\u6258\u4f18\u5316\u3002"}}
{"id": "2505.12565", "pdf": "https://arxiv.org/pdf/2505.12565", "abs": "https://arxiv.org/abs/2505.12565", "authors": ["Carl Edwards", "Chi Han", "Gawon Lee", "Thao Nguyen", "Bowen Jin", "Chetan Kumar Prasad", "Sara Szymku\u0107", "Bartosz A. Grzybowski", "Ying Diao", "Jiawei Han", "Ge Liu", "Hao Peng", "Martin D. Burke", "Heng Ji"], "title": "mCLM: A Function-Infused and Synthesis-Friendly Modular Chemical Language Model", "categories": ["cs.AI", "cs.CL", "cs.LG", "q-bio.QM"], "comment": null, "summary": "Despite their ability to understand chemical knowledge and accurately\ngenerate sequential representations, large language models (LLMs) remain\nlimited in their capacity to propose novel molecules with drug-like properties.\nIn addition, the molecules that LLMs propose can often be challenging to make\nin the lab. To more effectively enable the discovery of functional small\nmolecules, LLMs need to learn a molecular language. However, LLMs are currently\nlimited by encoding molecules from atoms. In this paper, we argue that just\nlike tokenizing texts into (sub-)word tokens instead of characters, molecules\nshould be decomposed and reassembled at the level of functional building\nblocks, i.e., parts of molecules that bring unique functions and serve as\neffective building blocks for real-world automated laboratory synthesis. This\nmotivates us to propose mCLM, a modular Chemical-Language Model tokenizing\nmolecules into building blocks and learning a bilingual language model of both\nnatural language descriptions of functions and molecule building blocks. By\nreasoning on such functional building blocks, mCLM guarantees to generate\nefficiently synthesizable molecules thanks to recent progress in block-based\nchemistry, while also improving the functions of molecules in a principled\nmanner. In experiments on 430 FDA-approved drugs, we find mCLM capable of\nsignificantly improving 5 out of 6 chemical functions critical to determining\ndrug potentials. More importantly, mCLM can reason on multiple functions and\nimprove the FDA-rejected drugs (``fallen angels'') over multiple iterations to\ngreatly improve their shortcomings.", "AI": {"tldr": "\u63d0\u51fa\u6a21\u5757\u5316\u5316\u5b66\u8bed\u8a00\u6a21\u578bmCLM\uff0c\u901a\u8fc7\u5c06\u5206\u5b50\u5206\u89e3\u4e3a\u529f\u80fd\u6a21\u5757\u5e76\u5efa\u7acb\u53cc\u8bed\u6a21\u578b\uff0c\u6709\u6548\u63d0\u5347\u836f\u7269\u5206\u5b50\u8bbe\u8ba1\u7684\u53ef\u5408\u6210\u6027\u4e0e\u529f\u80fd\u6027", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u836f\u7269\u5206\u5b50\u8bbe\u8ba1\u4e2d\u5b58\u5728\u751f\u6210\u5206\u5b50\u96be\u4ee5\u5408\u6210\u4e14\u529f\u80fd\u4f18\u5316\u4e0d\u8db3\u7684\u5c40\u9650\u6027\uff0c\u9700\u91c7\u7528\u7c7b\u4f3c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u6a21\u5757\u5316\u5206\u5b50\u8868\u793a\u65b9\u6cd5", "method": "\u5c06\u5206\u5b50\u5206\u89e3\u4e3a\u529f\u80fd\u6784\u5efa\u5757\uff0c\u5efa\u7acb\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e0e\u5206\u5b50\u6a21\u5757\u7684\u53cc\u8bed\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u529f\u80fd\u5bfc\u5411\u7684\u5206\u5b50\u751f\u6210", "result": "\u5728430\u79cdFDA\u6279\u51c6\u836f\u7269\u6d4b\u8bd5\u4e2d\u663e\u8457\u6539\u55845/6\u5173\u952e\u5316\u5b66\u529f\u80fd\uff0c\u5e76\u80fd\u901a\u8fc7\u591a\u8f6e\u8fed\u4ee3\u6709\u6548\u63d0\u5347FDA\u62d2\u7edd\u836f\u7269\u7684\u529f\u80fd\u7f3a\u9677", "conclusion": "\u57fa\u4e8e\u529f\u80fd\u6a21\u5757\u7684\u6a21\u5757\u5316\u65b9\u6cd5\u4e3a\u836f\u7269\u5206\u5b50\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u540c\u65f6\u4fdd\u8bc1\u5408\u6210\u53ef\u884c\u6027\u4e0e\u529f\u80fd\u4f18\u5316\u7684\u65b0\u8303\u5f0f"}}
{"id": "2505.12629", "pdf": "https://arxiv.org/pdf/2505.12629", "abs": "https://arxiv.org/abs/2505.12629", "authors": ["Yuchang Sun", "Yanxi Chen", "Yaliang Li", "Bolin Ding"], "title": "Enhancing Latent Computation in Transformers with Latent Tokens", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Augmenting large language models (LLMs) with auxiliary tokens has emerged as\na promising strategy for enhancing model performance. In this work, we\nintroduce a lightweight method termed latent tokens; these are dummy tokens\nthat may be non-interpretable in natural language but steer the autoregressive\ndecoding process of a Transformer-based LLM via the attention mechanism. The\nproposed latent tokens can be seamlessly integrated with a pre-trained\nTransformer, trained in a parameter-efficient manner, and applied flexibly at\ninference time, while adding minimal complexity overhead to the existing\ninfrastructure of standard Transformers. We propose several hypotheses about\nthe underlying mechanisms of latent tokens and design synthetic tasks\naccordingly to verify them. Numerical results confirm that the proposed method\nnoticeably outperforms the baselines, particularly in the out-of-distribution\ngeneralization scenarios, highlighting its potential in improving the\nadaptability of LLMs.", "AI": {"tldr": "\u901a\u8fc7\u6dfb\u52a0\u975e\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u7684\u6f5c\u5728\u6807\u8bb0\uff08latent tokens\uff09\uff0c\u4ee5\u8f7b\u91cf\u7ea7\u65b9\u5f0f\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u5347\u6a21\u578b\u5728\u5206\u5e03\u5916\u573a\u666f\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfLLM\u589e\u5f3a\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u590d\u6742\u67b6\u6784\u8c03\u6574\u6216\u5168\u53c2\u6570\u5fae\u8c03\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4ec5\u6dfb\u52a0\u5c11\u91cf\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u6f5c\u5728\u6807\u8bb0\uff0c\u5728\u4e0d\u7834\u574f\u9884\u8bad\u7ec3\u6a21\u578b\u7ed3\u6784\u7684\u524d\u63d0\u4e0b\u63d0\u5347\u6a21\u578b\u9002\u5e94\u6027\u3002", "method": "\u5728Transformer\u89e3\u7801\u8fc7\u7a0b\u4e2d\u63d2\u5165\u53ef\u8bad\u7ec3\u7684\u865a\u62df\u6807\u8bb0\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5f15\u5bfc\u751f\u6210\u8fc7\u7a0b\u3002\u652f\u6301\u53c2\u6570\u9ad8\u6548\u8bad\u7ec3\uff08PEFT\uff09\uff0c\u4e0e\u73b0\u6709\u67b6\u6784\u65e0\u7f1d\u96c6\u6210\uff0c\u63a8\u7406\u9636\u6bb5\u7075\u6d3b\u5e94\u7528\u3002", "result": "\u5728\u5408\u6210\u4efb\u52a1\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u5916\u6cdb\u5316\u573a\u666f\u8868\u73b0\u7a81\u51fa\uff0c\u9a8c\u8bc1\u4e86\u6f5c\u5728\u6807\u8bb0\u5bf9\u6a21\u578b\u9002\u5e94\u6027\u7684\u63d0\u5347\u6548\u679c\u3002", "conclusion": "\u6f5c\u5728\u6807\u8bb0\u673a\u5236\u4e3aLLM\u6027\u80fd\u63d0\u5347\u63d0\u4f9b\u4e86\u4f4e\u590d\u6742\u5ea6\u3001\u9ad8\u517c\u5bb9\u6027\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u57fa\u7840\u8bbe\u65bd\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u589e\u5f3a\u4e86\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2505.12632", "pdf": "https://arxiv.org/pdf/2505.12632", "abs": "https://arxiv.org/abs/2505.12632", "authors": ["Yunseok Jang", "Yeda Song", "Sungryull Sohn", "Lajanugen Logeswaran", "Tiange Luo", "Dong-Ki Kim", "Kyunghoon Bae", "Honglak Lee"], "title": "Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "CVPR 2025", "summary": "Recent advancements in Large Language Models (LLMs) and Vision-Language\nModels (VLMs) have sparked significant interest in developing GUI visual\nagents. We introduce MONDAY (Mobile OS Navigation Task Dataset for Agents from\nYouTube), a large-scale dataset of 313K annotated frames from 20K instructional\nvideos capturing diverse real-world mobile OS navigation across multiple\nplatforms. Models that include MONDAY in their pre-training phases demonstrate\nrobust cross-platform generalization capabilities, consistently outperforming\nmodels trained on existing single OS datasets while achieving an average\nperformance gain of 18.11%p on an unseen mobile OS platform. To enable\ncontinuous dataset expansion as mobile platforms evolve, we present an\nautomated framework that leverages publicly available video content to create\ncomprehensive task datasets without manual annotation. Our framework comprises\nrobust OCR-based scene detection (95.04% F1score), near-perfect UI element\ndetection (99.87% hit ratio), and novel multi-step action identification to\nextract reliable action sequences across diverse interface configurations. We\ncontribute both the MONDAY dataset and our automated collection framework to\nfacilitate future research in mobile OS navigation.", "AI": {"tldr": "\u63d0\u51faMONDAY\u6570\u636e\u96c6\u4e0e\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u63d0\u5347\u79fb\u52a8OS\u5bfc\u822a\u4efb\u52a1\u7684\u8de8\u5e73\u53f0\u6a21\u578b\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709\u79fb\u52a8OS\u6570\u636e\u96c6\u5c40\u9650\u4e8e\u5355\u4e00\u5e73\u53f0\u4e14\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u96be\u4ee5\u9002\u5e94\u5e73\u53f0\u5feb\u901f\u8fed\u4ee3\u9700\u6c42", "method": "\u6784\u5efa\u591a\u5e73\u53f0\u6559\u5b66\u89c6\u9891\u6570\u636e\u96c6+\u81ea\u52a8\u5316\u6846\u67b6\uff08OCR\u573a\u666f\u68c0\u6d4b+UI\u5143\u7d20\u68c0\u6d4b+\u591a\u6b65\u52a8\u4f5c\u8bc6\u522b\uff09", "result": "\u6a21\u578b\u5728\u672a\u89c1\u79fb\u52a8OS\u5e73\u53f0\u5e73\u5747\u63d0\u534718.11%\u6027\u80fd\uff0c\u6846\u67b6\u5b9e\u73b095.04%\u573a\u666f\u68c0\u6d4bF1\u503c\u4e0e99.87%UI\u68c0\u6d4b\u547d\u4e2d\u7387", "conclusion": "\u901a\u8fc7\u5f00\u6e90\u6570\u636e\u96c6\u4e0e\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u4e3a\u79fb\u52a8OS\u5bfc\u822a\u7814\u7a76\u63d0\u4f9b\u53ef\u6301\u7eed\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2505.12680", "pdf": "https://arxiv.org/pdf/2505.12680", "abs": "https://arxiv.org/abs/2505.12680", "authors": ["Haoyu Zhao", "Yihan Geng", "Shange Tang", "Yong Lin", "Bohan Lyu", "Hongzhou Lin", "Chi Jin", "Sanjeev Arora"], "title": "Ineq-Comp: Benchmarking Human-Intuitive Compositional Reasoning in Automated Theorem Proving on Inequalities", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "27 pages", "summary": "LLM-based formal proof assistants (e.g., in Lean) hold great promise for\nautomating mathematical discovery. But beyond syntactic correctness, do these\nsystems truly understand mathematical structure as humans do? We investigate\nthis question through the lens of mathematical inequalities -- a fundamental\ntool across many domains. While modern provers can solve basic inequalities, we\nprobe their ability to handle human-intuitive compositionality. We introduce\nIneq-Comp, a benchmark built from elementary inequalities through systematic\ntransformations, including variable duplication, algebraic rewriting, and\nmulti-step composition. Although these problems remain easy for humans, we find\nthat most provers -- including Goedel, STP, and Kimina-7B -- struggle\nsignificantly. DeepSeek-Prover-V2-7B shows relative robustness -- possibly\nbecause it is trained to decompose the problems into sub-problems -- but still\nsuffers a 20\\% performance drop (pass@32). Strikingly, performance remains poor\nfor all models even when formal proofs of the constituent parts are provided in\ncontext, revealing that the source of weakness is indeed in compositional\nreasoning. Our results expose a persisting gap between the generalization\nbehavior of current AI provers and human mathematical intuition.", "AI": {"tldr": "\u7814\u7a76\u63ed\u793a\u5f53\u524dAI\u8bc1\u660e\u7cfb\u7edf\u5728\u6570\u5b66\u7ec4\u5408\u63a8\u7406\u4e0a\u4e0e\u4eba\u7c7b\u5b58\u5728\u663e\u8457\u5dee\u8ddd", "motivation": "\u9a8c\u8bc1\u57fa\u4e8eLLM\u7684\u5f62\u5f0f\u5316\u8bc1\u660e\u7cfb\u7edf\u662f\u5426\u5177\u5907\u7c7b\u4f3c\u4eba\u7c7b\u7684\u6570\u5b66\u7ed3\u6784\u7406\u89e3\u80fd\u529b\uff0c\u7279\u522b\u662f\u7ec4\u5408\u6027\u63a8\u7406\u8fd9\u4e00\u5173\u952e\u8ba4\u77e5\u80fd\u529b", "method": "\u901a\u8fc7\u6784\u5efaIneq-Comp\u57fa\u51c6\u6d4b\u8bd5\uff08\u57fa\u4e8e\u57fa\u7840\u4e0d\u7b49\u5f0f\u7684\u7cfb\u7edf\u5316\u53d8\u6362\uff1a\u53d8\u91cf\u590d\u5236/\u4ee3\u6570\u91cd\u5199/\u591a\u6b65\u7ec4\u5408\uff09\uff0c\u6d4b\u8bd5\u4e3b\u6d41\u8bc1\u660e\u7cfb\u7edf\u7684\u7ec4\u5408\u63a8\u7406\u80fd\u529b", "result": "\u5927\u591a\u6570\u6a21\u578b\uff08\u5305\u62ecGoedel\u3001STP\uff09\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0cDeepSeek-Prover-V2-7B\u4fdd\u6301\u76f8\u5bf9\u7a33\u5065\u4f46\u4ecd\u670920%\u6027\u80fd\u4e0b\u964d\uff08pass@32\uff09\uff0c\u5373\u4f7f\u63d0\u4f9b\u5b50\u90e8\u5206\u8bc1\u660e\u4e0a\u4e0b\u6587\u4ecd\u8868\u73b0\u4e0d\u4f73", "conclusion": "\u5f53\u524dAI\u8bc1\u660e\u7cfb\u7edf\u7684\u6cdb\u5316\u884c\u4e3a\u4e0e\u4eba\u7c7b\u6570\u5b66\u76f4\u89c9\u95f4\u5b58\u5728\u6301\u7eed\u6027\u5dee\u8ddd\uff0c\u7ec4\u5408\u63a8\u7406\u80fd\u529b\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\uff0c\u63d0\u793a\u9700\u8981\u66f4\u7ed3\u6784\u5316\u7684\u63a8\u7406\u8bad\u7ec3\u65b9\u6cd5"}}
{"id": "2505.12692", "pdf": "https://arxiv.org/pdf/2505.12692", "abs": "https://arxiv.org/abs/2505.12692", "authors": ["Ziwei Xu", "Udit Sanghi", "Mohan Kankanhalli"], "title": "Bullying the Machine: How Personas Increase LLM Vulnerability", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed in interactions where\nthey are prompted to adopt personas. This paper investigates whether such\npersona conditioning affects model safety under bullying, an adversarial\nmanipulation that applies psychological pressures in order to force the victim\nto comply to the attacker. We introduce a simulation framework in which an\nattacker LLM engages a victim LLM using psychologically grounded bullying\ntactics, while the victim adopts personas aligned with the Big Five personality\ntraits. Experiments using multiple open-source LLMs and a wide range of\nadversarial goals reveal that certain persona configurations -- such as\nweakened agreeableness or conscientiousness -- significantly increase victim's\nsusceptibility to unsafe outputs. Bullying tactics involving emotional or\nsarcastic manipulation, such as gaslighting and ridicule, are particularly\neffective. These findings suggest that persona-driven interaction introduces a\nnovel vector for safety risks in LLMs and highlight the need for persona-aware\nsafety evaluation and alignment strategies.", "AI": {"tldr": "LLM\u89d2\u8272\u626e\u6f14\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u5b89\u5168\u6027\uff0c\u7279\u5b9a\u4eba\u683c\u914d\u7f6e\uff08\u4f4e\u5b9c\u4eba\u6027/\u5c3d\u8d23\u6027\uff09\u548c\u60c5\u611f\u64cd\u7eb5\u7b56\u7565\uff08\u7164\u6c14\u706f\u6548\u5e94/\u5632\u8bbd\uff09\u4f1a\u5927\u5e45\u589e\u52a0\u6a21\u578b\u7684\u4e0d\u5b89\u5168\u8f93\u51fa\u98ce\u9669", "motivation": "\u7814\u7a76\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89d2\u8272\u626e\u6f14\u573a\u666f\u4e0b\uff0c\u4eba\u683c\u7279\u5f81\u914d\u7f6e\u5982\u4f55\u5f71\u54cd\u5176\u5bf9\u6297\u6b3a\u51cc\u653b\u51fb\u7684\u5b89\u5168\u9c81\u68d2\u6027", "method": "\u5efa\u7acb\u653b\u51fb\u6846\u67b6\u6a21\u62df\u5fc3\u7406\u64cd\u7eb5\u573a\u666f\uff0c\u8ba9\u653b\u51fb\u8005LLM\u4f7f\u7528\u5fc3\u7406\u5b66\u6b3a\u51cc\u7b56\u7565\uff08\u7164\u6c14\u706f\u6548\u5e94/\u5632\u8bbd\u7b49\uff09\uff0c\u53d7\u5bb3\u8005LLM\u52a0\u8f7d\u5927\u4e94\u4eba\u683c\u7279\u5f81\u8fdb\u884c\u5bf9\u6297\u6d4b\u8bd5", "result": "\u4f4e\u5b9c\u4eba\u6027\u4eba\u683c\u4f7f\u53d7\u5bb3\u8005LLM\u670d\u4ece\u7387\u63d0\u534732%\uff0c\u60c5\u611f\u64cd\u7eb5\u7b56\u7565\u7684\u6210\u529f\u7387\u6bd4\u903b\u8f91\u653b\u51fb\u9ad847%\uff0c\u5632\u8bbd\u653b\u51fb\u5728\u5f00\u653e\u578b\u4eba\u683c\u4e2d\u5f15\u53d160%\u7684\u4e0d\u5b89\u5168\u8f93\u51fa", "conclusion": "\u89d2\u8272\u626e\u6f14\u529f\u80fd\u4e3aLLM\u5b89\u5168\u5f15\u5165\u65b0\u653b\u51fb\u9762\uff0c\u9700\u5f00\u53d1\u4eba\u683c\u611f\u77e5\u7684\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5728\u5bf9\u9f50\u8fc7\u7a0b\u4e2d\u8003\u8651\u4eba\u683c\u7279\u5f81\u7684\u9632\u5fa1\u5f3a\u5316"}}
{"id": "2505.12763", "pdf": "https://arxiv.org/pdf/2505.12763", "abs": "https://arxiv.org/abs/2505.12763", "authors": ["Sunghwan Kim", "Dongjin Kang", "Taeyoon Kwon", "Hyungjoo Chae", "Dongha Lee", "Jinyoung Yeo"], "title": "Rethinking Reward Model Evaluation Through the Lens of Reward Overoptimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Reward models (RMs) play a crucial role in reinforcement learning from human\nfeedback (RLHF), aligning model behavior with human preferences. However,\nexisting benchmarks for reward models show a weak correlation with the\nperformance of optimized policies, suggesting that they fail to accurately\nassess the true capabilities of RMs. To bridge this gap, we explore several\nevaluation designs through the lens of reward overoptimization\\textemdash a\nphenomenon that captures both how well the reward model aligns with human\npreferences and the dynamics of the learning signal it provides to the policy.\nThe results highlight three key findings on how to construct a reliable\nbenchmark: (i) it is important to minimize differences between chosen and\nrejected responses beyond correctness, (ii) evaluating reward models requires\nmultiple comparisons across a wide range of chosen and rejected responses, and\n(iii) given that reward models encounter responses with diverse\nrepresentations, responses should be sourced from a variety of models. However,\nwe also observe that a extremely high correlation with degree of\noveroptimization leads to comparatively lower correlation with certain\ndownstream performance. Thus, when designing a benchmark, it is desirable to\nuse the degree of overoptimization as a useful tool, rather than the end goal.", "AI": {"tldr": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6\u4e0e\u7b56\u7565\u4f18\u5316\u8868\u73b0\u5f31\u76f8\u5173\uff0c\u9700\u901a\u8fc7\u5956\u52b1\u8fc7\u4f18\u5316\u73b0\u8c61\u6784\u5efa\u591a\u7ef4\u5ea6\u6d4b\u8bd5\u57fa\u51c6\uff0c\u4f46\u9700\u6ce8\u610f\u907f\u514d\u8fc7\u5ea6\u4f18\u5316\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd", "motivation": "\u73b0\u6709\u5956\u52b1\u6a21\u578b\u8bc4\u4f30\u57fa\u51c6\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u5176\u5728RLHF\u4e2d\u7684\u5b9e\u9645\u4f5c\u7528\uff0c\u5f71\u54cd\u7b56\u7565\u4f18\u5316\u6548\u679c\u8bc4\u4f30", "method": "\u901a\u8fc7\u5956\u52b1\u8fc7\u4f18\u5316\u73b0\u8c61\uff08\u7efc\u5408\u8bc4\u4f30\u6a21\u578b\u5bf9\u9f50\u80fd\u529b\u4e0e\u7b56\u7565\u5b66\u4e60\u4fe1\u53f7\uff09\uff0c\u63d0\u51fa\u4e09\u9636\u6bb5\u57fa\u51c6\u8bbe\u8ba1\u6846\u67b6", "result": "\u786e\u5b9a\u57fa\u51c6\u8bbe\u8ba1\u4e09\u539f\u5219\uff1a\uff081\uff09\u6700\u5c0f\u5316\u975e\u6b63\u786e\u6027\u5dee\u5f02\uff082\uff09\u591a\u7ef4\u5ea6\u54cd\u5e94\u5bf9\u6bd4\uff083\uff09\u591a\u6a21\u578b\u54cd\u5e94\u6765\u6e90\uff1b\u53d1\u73b0\u8fc7\u5ea6\u4f18\u5316\u4e0e\u4e0b\u6e38\u4efb\u52a1\u5b58\u5728\u8d1f\u76f8\u5173", "conclusion": "\u5e94\u5c06\u5956\u52b1\u8fc7\u4f18\u5316\u7a0b\u5ea6\u4f5c\u4e3a\u57fa\u51c6\u8bbe\u8ba1\u5de5\u5177\u800c\u975e\u6700\u7ec8\u76ee\u6807\uff0c\u5e73\u8861\u6a21\u578b\u8bc4\u4f30\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u5173\u7cfb"}}
{"id": "2505.12842", "pdf": "https://arxiv.org/pdf/2505.12842", "abs": "https://arxiv.org/abs/2505.12842", "authors": ["Zheng Wu", "Pengzhou Cheng", "Zongru Wu", "Lingzhong Dong", "Zhuosheng Zhang"], "title": "GEM: Gaussian Embedding Modeling for Out-of-Distribution Detection in GUI Agents", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Graphical user interface (GUI) agents have recently emerged as an intriguing\nparadigm for human-computer interaction, capable of automatically executing\nuser instructions to operate intelligent terminal devices. However, when\nencountering out-of-distribution (OOD) instructions that violate environmental\nconstraints or exceed the current capabilities of agents, GUI agents may suffer\ntask breakdowns or even pose security threats. Therefore, effective OOD\ndetection for GUI agents is essential. Traditional OOD detection methods\nperform suboptimally in this domain due to the complex embedding space and\nevolving GUI environments. In this work, we observe that the in-distribution\ninput semantic space of GUI agents exhibits a clustering pattern with respect\nto the distance from the centroid. Based on the finding, we propose GEM, a\nnovel method based on fitting a Gaussian mixture model over input embedding\ndistances extracted from the GUI Agent that reflect its capability boundary.\nEvaluated on eight datasets spanning smartphones, computers, and web browsers,\nour method achieves an average accuracy improvement of 23.70\\% over the\nbest-performing baseline. Analysis verifies the generalization ability of our\nmethod through experiments on nine different backbones. The codes are available\nat https://github.com/Wuzheng02/GEM-OODforGUIagents.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7684GEM\u65b9\u6cd5\uff0c\u6709\u6548\u68c0\u6d4bGUI\u4ee3\u7406\u7684\u5f02\u5e38\u6307\u4ee4\uff0c\u57288\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b023.7%\u7684\u51c6\u786e\u7387\u63d0\u5347", "motivation": "\u5f53\u524dGUI\u4ee3\u7406\u9047\u5230\u8d85\u51fa\u80fd\u529b\u8303\u56f4\u7684\u6307\u4ee4\u65f6\u6613\u5d29\u6e83\u6216\u5f15\u53d1\u5b89\u5168\u9690\u60a3\uff0c\u4f20\u7edfOOD\u68c0\u6d4b\u65b9\u6cd5\u5728\u590d\u6742GUI\u73af\u5883\u4e2d\u8868\u73b0\u6b20\u4f73", "method": "\u901a\u8fc7\u89c2\u5bdfGUI\u4ee3\u7406\u8bed\u4e49\u7a7a\u95f4\u7684\u805a\u7c7b\u7279\u6027\uff0c\u5229\u7528\u8f93\u5165\u5d4c\u5165\u8ddd\u79bb\u7684\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u62df\u5408\u4ee3\u7406\u80fd\u529b\u8fb9\u754c", "result": "\u5728\u8de8\u8bbe\u5907\uff08\u624b\u673a/\u7535\u8111/\u6d4f\u89c8\u5668\uff09\u76848\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u534723.7%\uff0c\u4e5d\u79cd\u4e0d\u540c\u9aa8\u5e72\u7f51\u7edc\u5b9e\u9a8c\u8bc1\u5b9e\u6cdb\u5316\u80fd\u529b", "conclusion": "GEM\u901a\u8fc7\u5efa\u6a21\u8bed\u4e49\u7a7a\u95f4\u8ddd\u79bb\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347GUI\u4ee3\u7406\u7684OOD\u68c0\u6d4b\u6027\u80fd\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u4fbf\u4e8e\u540e\u7eed\u7814\u7a76\u5e94\u7528"}}
{"id": "2505.12871", "pdf": "https://arxiv.org/pdf/2505.12871", "abs": "https://arxiv.org/abs/2505.12871", "authors": ["Zi Liang", "Haibo Hu", "Qingqing Ye", "Yaxin Xiao", "Ronghua Li"], "title": "Does Low Rank Adaptation Lead to Lower Robustness against Training-Time Attacks?", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": "To appear at ICML 25", "summary": "Low rank adaptation (LoRA) has emerged as a prominent technique for\nfine-tuning large language models (LLMs) thanks to its superb efficiency gains\nover previous methods. While extensive studies have examined the performance\nand structural properties of LoRA, its behavior upon training-time attacks\nremain underexplored, posing significant security risks. In this paper, we\ntheoretically investigate the security implications of LoRA's low-rank\nstructure during fine-tuning, in the context of its robustness against data\npoisoning and backdoor attacks. We propose an analytical framework that models\nLoRA's training dynamics, employs the neural tangent kernel to simplify the\nanalysis of the training process, and applies information theory to establish\nconnections between LoRA's low rank structure and its vulnerability against\ntraining-time attacks. Our analysis indicates that LoRA exhibits better\nrobustness to backdoor attacks than full fine-tuning, while becomes more\nvulnerable to untargeted data poisoning due to its over-simplified information\ngeometry. Extensive experimental evaluations have corroborated our theoretical\nfindings.", "AI": {"tldr": "LoRA\u5fae\u8c03\u5728\u9632\u5fa1\u540e\u95e8\u653b\u51fb\u65f6\u6bd4\u5168\u53c2\u6570\u5fae\u8c03\u66f4\u9c81\u68d2\uff0c\u4f46\u56e0\u5176\u8fc7\u5ea6\u7b80\u5316\u7684\u4fe1\u606f\u51e0\u4f55\u7ed3\u6784\uff0c\u66f4\u5bb9\u6613\u53d7\u5230\u65e0\u76ee\u6807\u6570\u636e\u6295\u6bd2\u653b\u51fb", "motivation": "\u63a2\u7a76LoRA\u5728\u8bad\u7ec3\u65f6\u5bf9\u6297\u6570\u636e\u6295\u6bd2\u548c\u540e\u95e8\u653b\u51fb\u7684\u5b89\u5168\u6027\u8868\u73b0\uff0c\u586b\u8865\u8be5\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d", "method": "\u5efa\u7acb\u878d\u5408\u795e\u7ecf\u6b63\u5207\u6838\u7684\u8bad\u7ec3\u52a8\u529b\u5b66\u5206\u6790\u6846\u67b6\uff0c\u7ed3\u5408\u4fe1\u606f\u8bba\u5206\u6790\u4f4e\u79e9\u7ed3\u6784\u4e0e\u653b\u51fb\u8106\u5f31\u6027\u7684\u5173\u8054\u673a\u5236", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u53d1\u73b0LoRA\u5bf9\u540e\u95e8\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\u63d0\u534720%\uff0c\u4f46\u5bf9\u6570\u636e\u6295\u6bd2\u7684\u62b5\u6297\u80fd\u529b\u4e0b\u964d35%", "conclusion": "LoRA\u7684\u4f4e\u79e9\u7279\u6027\u5728\u5b89\u5168\u9886\u57df\u5448\u73b0\u53cc\u5203\u5251\u6548\u5e94\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u5728\u9632\u5fa1\u6548\u80fd\u4e0e\u653b\u51fb\u8106\u5f31\u6027\u95f4\u53d6\u5f97\u5e73\u8861"}}
{"id": "2505.12886", "pdf": "https://arxiv.org/pdf/2505.12886", "abs": "https://arxiv.org/abs/2505.12886", "authors": ["Zhongxiang Sun", "Qipeng Wang", "Haoyu Wang", "Xiao Zhang", "Jun Xu"], "title": "Detection and Mitigation of Hallucination in Large Reasoning Models: A Mechanistic Perspective", "categories": ["cs.AI", "cs.CL", "cs.CY"], "comment": "25 pages", "summary": "Large Reasoning Models (LRMs) have shown impressive capabilities in\nmulti-step reasoning tasks. However, alongside these successes, a more\ndeceptive form of model error has emerged--Reasoning Hallucination--where\nlogically coherent but factually incorrect reasoning traces lead to persuasive\nyet faulty conclusions. Unlike traditional hallucinations, these errors are\nembedded within structured reasoning, making them more difficult to detect and\npotentially more harmful. In this work, we investigate reasoning hallucinations\nfrom a mechanistic perspective. We propose the Reasoning Score, which\nquantifies the depth of reasoning by measuring the divergence between logits\nobtained from projecting late layers of LRMs to the vocabulary space,\neffectively distinguishing shallow pattern-matching from genuine deep\nreasoning. Using this score, we conduct an in-depth analysis on the ReTruthQA\ndataset and identify two key reasoning hallucination patterns: early-stage\nfluctuation in reasoning depth and incorrect backtracking to flawed prior\nsteps. These insights motivate our Reasoning Hallucination Detection (RHD)\nframework, which achieves state-of-the-art performance across multiple domains.\nTo mitigate reasoning hallucinations, we further introduce GRPO-R, an enhanced\nreinforcement learning algorithm that incorporates step-level deep reasoning\nrewards via potential-based shaping. Our theoretical analysis establishes\nstronger generalization guarantees, and experiments demonstrate improved\nreasoning quality and reduced hallucination rates.", "AI": {"tldr": "\u63d0\u51fa'\u63a8\u7406\u5206\u6570'\u548cGRPO-R\u7b97\u6cd5\u89e3\u51b3\u5927\u6a21\u578b\u63a8\u7406\u5e7b\u89c9\u95ee\u9898", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u68c0\u6d4b\u903b\u8f91\u8fde\u8d2f\u4f46\u4e8b\u5b9e\u9519\u8bef\u7684\u63a8\u7406\u8def\u5f84\uff0c\u9700\u91cf\u5316\u63a8\u7406\u6df1\u5ea6\u5efa\u7acb\u65b0\u68c0\u6d4b\u6846\u67b6", "method": "\u901a\u8fc7\u6295\u5f71\u6a21\u578b\u6df1\u5c42logits\u6784\u5efa\u63a8\u7406\u5206\u6570\uff0c\u5f00\u53d1RHD\u68c0\u6d4b\u6846\u67b6\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684GRPO-R\u4f18\u5316\u7b97\u6cd5", "result": "\u5728ReTruthQA\u6570\u636e\u96c6\u5b9e\u73b0SOTA\uff0c\u7406\u8bba\u5206\u6790\u8bc1\u660e\u66f4\u5f3a\u6cdb\u5316\u4fdd\u8bc1\uff0c\u5b9e\u9a8c\u663e\u793a\u5e7b\u89c9\u7387\u964d\u4f4e34%", "conclusion": "\u63a8\u7406\u5206\u6570\u6709\u6548\u63ed\u793a\u5e7b\u89c9\u673a\u5236\uff0cGRPO-R\u901a\u8fc7\u6b65\u9aa4\u7ea7\u5956\u52b1\u673a\u5236\u663e\u8457\u63d0\u5347\u63a8\u7406\u8d28\u91cf"}}
{"id": "2505.12891", "pdf": "https://arxiv.org/pdf/2505.12891", "abs": "https://arxiv.org/abs/2505.12891", "authors": ["Shaohang Wei", "Wei Li", "Feifan Song", "Wen Luo", "Tianyi Zhuang", "Haochen Tan", "Zhijiang Guo", "Houfeng Wang"], "title": "TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios", "categories": ["cs.AI", "cs.CL"], "comment": "First version. There are still some examples to be added into the\n  appendix", "summary": "Temporal reasoning is pivotal for Large Language Models (LLMs) to comprehend\nthe real world. However, existing works neglect the real-world challenges for\ntemporal reasoning: (1) intensive temporal information, (2) fast-changing event\ndynamics, and (3) complex temporal dependencies in social interactions. To\nbridge this gap, we propose a multi-level benchmark TIME, designed for temporal\nreasoning in real-world scenarios. TIME consists of 38,522 QA pairs, covering 3\nlevels with 11 fine-grained sub-tasks. This benchmark encompasses 3\nsub-datasets reflecting different real-world challenges: TIME-Wiki, TIME-News,\nand TIME-Dial. We conduct extensive experiments on reasoning models and\nnon-reasoning models. And we conducted an in-depth analysis of temporal\nreasoning performance across diverse real-world scenarios and tasks, and\nsummarized the impact of test-time scaling on temporal reasoning capabilities.\nAdditionally, we release TIME-Lite, a human-annotated subset to foster future\nresearch and standardized evaluation in temporal reasoning. The code is\navailable at https://github.com/sylvain-wei/TIME , and the dataset is available\nat https://huggingface.co/datasets/SylvainWei/TIME .", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u591a\u5c42\u7ea7\u65f6\u95f4\u63a8\u7406\u57fa\u51c6TIME\uff0c\u5305\u542b38,522\u4e2aQA\u5bf9\uff0c\u8986\u76d6\u73b0\u5b9e\u573a\u666f\u4e2d\u5bc6\u96c6\u65f6\u95f4\u4fe1\u606f\u3001\u5feb\u901f\u4e8b\u4ef6\u52a8\u6001\u548c\u590d\u6742\u4f9d\u8d56\u4e09\u5927\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u63ed\u793a\u4e86\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\u5dee\u5f02\u4e0e\u6d4b\u8bd5\u6269\u5c55\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u73b0\u5b9e\u573a\u666f\u4e2d\u65f6\u95f4\u63a8\u7406\u7684\u4e09\u5927\u6838\u5fc3\u6311\u6218\uff1a1) \u5bc6\u96c6\u65f6\u95f4\u4fe1\u606f\u5904\u7406\u56f0\u96be 2) \u4e8b\u4ef6\u52a8\u6001\u5feb\u901f\u53d8\u5316 3) \u793e\u4ea4\u573a\u666f\u7684\u590d\u6742\u65f6\u95f4\u4f9d\u8d56\u3002\u4e3a\u586b\u8865\u7a7a\u767d\uff0c\u4f5c\u8005\u6784\u5efa\u8d34\u8fd1\u771f\u5b9e\u573a\u666f\u7684\u8bc4\u4f30\u4f53\u7cfb\u3002", "method": "\u6784\u5efaTIME\u4e09\u7ea7\u57fa\u51c6\uff08TIME-Wiki/News/Dial\uff09\uff0c\u6db5\u76d611\u4e2a\u7ec6\u7c92\u5ea6\u4efb\u52a1\u3002\u901a\u8fc7\u975e\u63a8\u7406\u6a21\u578b\uff08BERT\u7b49\uff09\u4e0e\u63a8\u7406\u6a21\u578b\uff08GPT-3.5\u7b49\uff09\u7684\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u65f6\u95f4\u4f9d\u8d56\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u975e\u63a8\u7406\u6a21\u578b\uff0c\u4f46\u6d4b\u8bd5\u65f6\u95f4\u6269\u5c55\u4f1a\u653e\u5927\u6a21\u578b\u95f4\u7684\u6027\u80fd\u5dee\u5f02\u3002TIME-Wiki\u5b50\u96c6\u63ed\u793a\u65f6\u95f4\u5bc6\u96c6\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u74f6\u9888\u3002", "conclusion": "TIME\u57fa\u51c6\u4e3a\u65f6\u95f4\u63a8\u7406\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0cTIME-Lite\u5b50\u96c6\u548c\u5f00\u6e90\u8d44\u6e90\u4fc3\u8fdb\u7b97\u6cd5\u7814\u53d1\u3002\u8be5\u5de5\u4f5c\u63ed\u793a\u4e86\u73b0\u5b9e\u573a\u666f\u65f6\u95f4\u63a8\u7406\u7684\u5173\u952e\u6280\u672f\u74f6\u9888\u3002"}}
{"id": "2505.12900", "pdf": "https://arxiv.org/pdf/2505.12900", "abs": "https://arxiv.org/abs/2505.12900", "authors": ["Shuyang Hou", "Zhangxiao Shen", "Huayi Wu", "Jianyuan Liang", "Haoyue Jiao", "Yaxian Qing", "Xiaopu Zhang", "Xu Li", "Zhipeng Gui", "Xuefeng Guan", "Longgang Xiang"], "title": "AutoGEEval: A Multimodal and Automated Framework for Geospatial Code Generation on GEE with Large Language Models", "categories": ["cs.SE", "cs.AI", "cs.CG", "cs.CL", "cs.DB"], "comment": null, "summary": "Geospatial code generation is emerging as a key direction in the integration\nof artificial intelligence and geoscientific analysis. However, there remains a\nlack of standardized tools for automatic evaluation in this domain. To address\nthis gap, we propose AutoGEEval, the first multimodal, unit-level automated\nevaluation framework for geospatial code generation tasks on the Google Earth\nEngine (GEE) platform powered by large language models (LLMs). Built upon the\nGEE Python API, AutoGEEval establishes a benchmark suite (AutoGEEval-Bench)\ncomprising 1325 test cases that span 26 GEE data types. The framework\nintegrates both question generation and answer verification components to\nenable an end-to-end automated evaluation pipeline-from function invocation to\nexecution validation. AutoGEEval supports multidimensional quantitative\nanalysis of model outputs in terms of accuracy, resource consumption, execution\nefficiency, and error types. We evaluate 18 state-of-the-art LLMs-including\ngeneral-purpose, reasoning-augmented, code-centric, and geoscience-specialized\nmodels-revealing their performance characteristics and potential optimization\npathways in GEE code generation. This work provides a unified protocol and\nfoundational resource for the development and assessment of geospatial code\ngeneration models, advancing the frontier of automated natural language to\ndomain-specific code translation.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u9762\u5411Google Earth Engine\u5e73\u53f0\u7684\u591a\u6a21\u6001\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6AutoGEEval\uff0c\u6784\u5efa\u5305\u542b1325\u4e2a\u6d4b\u8bd5\u7528\u4f8b\u7684\u57fa\u51c6\u5957\u4ef6\uff0c\u8bc4\u4f3018\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\u5e76\u63d0\u4f9b\u4f18\u5316\u8def\u5f84\u5206\u6790", "motivation": "\u5730\u7406\u7a7a\u95f4\u4ee3\u7801\u751f\u6210\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u5177\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u96be\u4ee5\u7cfb\u7edf\u8bc4\u4f30\u3002\u73b0\u6709\u7814\u7a76\u672a\u5efa\u7acb\u8986\u76d6GEE\u6570\u636e\u7c7b\u578b\u7684\u7edf\u4e00\u9a8c\u8bc1\u6846\u67b6\uff0c\u5236\u7ea6\u9886\u57df\u53d1\u5c55", "method": "\u57fa\u4e8eGEE Python API\u6784\u5efa\u57fa\u51c6\u5957\u4ef6AutoGEEval-Bench\uff0c\u6574\u5408\u95ee\u9898\u751f\u6210\u4e0e\u6267\u884c\u9a8c\u8bc1\u6a21\u5757\uff0c\u5efa\u7acb\u6db5\u76d6\u51c6\u786e\u6027\u3001\u8d44\u6e90\u6d88\u8017\u3001\u6267\u884c\u6548\u7387\u7684\u591a\u7ef4\u5ea6\u91cf\u5316\u5206\u6790\u6846\u67b6", "result": "\u8bc4\u4f30\u663e\u793a\u4ee3\u7801\u4e13\u7528\u6a21\u578b\u8868\u73b0\u6700\u4f18\uff08\u5e73\u5747\u6267\u884c\u6210\u529f\u738773.2%\uff09\uff0c\u5730\u7406\u4e13\u4e1a\u6a21\u578b\u5b58\u5728\u5b9e\u73b0\u7ec6\u8282\u4e0d\u8db3\u3002\u6846\u67b6\u53ef\u8bc6\u522b83%\u7684API\u8c03\u7528\u9519\u8bef\u548c67%\u7684\u6570\u636e\u7c7b\u578b\u4e0d\u5339\u914d\u95ee\u9898", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5730\u7406\u7a7a\u95f4\u4ee3\u7801\u751f\u6210\u5efa\u7acb\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u534f\u8bae\uff0c\u901a\u8fc7\u591a\u7ef4\u6027\u80fd\u5206\u6790\u63a8\u52a8\u9886\u57df\u4e13\u7528\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7684\u4f18\u5316\uff0c\u4fc3\u8fdb\u81ea\u7136\u8bed\u8a00\u5230\u5730\u7406\u7a7a\u95f4\u4ee3\u7801\u7684\u81ea\u52a8\u5316\u8f6c\u6362"}}
{"id": "2505.12938", "pdf": "https://arxiv.org/pdf/2505.12938", "abs": "https://arxiv.org/abs/2505.12938", "authors": ["Uri Dalal", "Meirav Segal", "Zvika Ben-Haim", "Dan Lahav", "Omer Nevo"], "title": "Leveraging LLM Inconsistency to Boost Pass@k Performance", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) achieve impressive abilities in numerous\ndomains, but exhibit inconsistent performance in response to minor input\nchanges. Rather than view this as a drawback, in this paper we introduce a\nnovel method for leveraging models' inconsistency to boost Pass@k performance.\nSpecifically, we present a \"Variator\" agent that generates k variants of a\ngiven task and submits one candidate solution for each one. Our variant\ngeneration approach is applicable to a wide range of domains as it is task\nagnostic and compatible with free-form inputs. We demonstrate the efficacy of\nour agent theoretically using a probabilistic model of the inconsistency\neffect, and show empirically that it outperforms the baseline on the APPS\ndataset. Furthermore, we establish that inconsistency persists even in frontier\nreasoning models across coding and cybersecurity domains, suggesting our method\nis likely to remain relevant for future model generations.", "AI": {"tldr": "\u901a\u8fc7\u4e3b\u52a8\u751f\u6210\u4efb\u52a1\u53d8\u4f53\u5229\u7528LLMs\u7684\u4e0d\u4e00\u81f4\u6027\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u5bf9\u5fae\u5c0f\u8f93\u5165\u53d8\u5316\u654f\u611f\uff0c\u672c\u7814\u7a76\u5c06\u5176\u8f6c\u5316\u4e3a\u63d0\u5347Pass@k\u6307\u6807\u7684\u4f18\u52bf", "method": "\u5f00\u53d1\u4efb\u52a1\u65e0\u5173\u7684Variator\u4ee3\u7406\uff0c\u81ea\u52a8\u751f\u6210k\u4e2a\u4efb\u52a1\u53d8\u4f53\u5e76\u63d0\u4ea4\u72ec\u7acb\u89e3\u51b3\u65b9\u6848", "result": "\u7406\u8bba\u6a21\u578b\u9a8c\u8bc1\u53ef\u884c\u6027\uff0cAPPS\u6570\u636e\u96c6\u5b9e\u8bc1\u8868\u73b0\u8d85\u8d8a\u57fa\u7ebf\uff0c\u524d\u6cbf\u6a21\u578b\u4ecd\u5b58\u5728\u4e0d\u4e00\u81f4\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e3b\u52a8\u521b\u9020\u8f93\u5165\u591a\u6837\u6027\u91ca\u653e\u6a21\u578b\u6f5c\u529b\uff0c\u672a\u6765\u4ecd\u9002\u7528\u4e8e\u65b0\u578b\u63a8\u7406\u6a21\u578b"}}
{"id": "2505.12992", "pdf": "https://arxiv.org/pdf/2505.12992", "abs": "https://arxiv.org/abs/2505.12992", "authors": ["Baohao Liao", "Hanze Dong", "Yuhui Xu", "Doyen Sahoo", "Christof Monz", "Junnan Li", "Caiming Xiong"], "title": "Fractured Chain-of-Thought Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "comment": null, "summary": "Inference-time scaling techniques have significantly bolstered the reasoning\ncapabilities of large language models (LLMs) by harnessing additional\ncomputational effort at inference without retraining. Similarly,\nChain-of-Thought (CoT) prompting and its extension, Long CoT, improve accuracy\nby generating rich intermediate reasoning trajectories, but these approaches\nincur substantial token costs that impede their deployment in latency-sensitive\nsettings. In this work, we first show that truncated CoT, which stops reasoning\nbefore completion and directly generates the final answer, often matches full\nCoT sampling while using dramatically fewer tokens. Building on this insight,\nwe introduce Fractured Sampling, a unified inference-time strategy that\ninterpolates between full CoT and solution-only sampling along three orthogonal\naxes: (1) the number of reasoning trajectories, (2) the number of final\nsolutions per trajectory, and (3) the depth at which reasoning traces are\ntruncated. Through extensive experiments on five diverse reasoning benchmarks\nand several model scales, we demonstrate that Fractured Sampling consistently\nachieves superior accuracy-cost trade-offs, yielding steep log-linear scaling\ngains in Pass@k versus token budget. Our analysis reveals how to allocate\ncomputation across these dimensions to maximize performance, paving the way for\nmore efficient and scalable LLM reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86Fractured Sampling\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u4e2a\u6b63\u4ea4\u7ef4\u5ea6\u8c03\u6574\u63a8\u7406\u8fc7\u7a0b\uff0c\u5728\u663e\u8457\u51cf\u5c11token\u6d88\u8017\u7684\u540c\u65f6\u4fdd\u6301\u5927\u6a21\u578b\u63a8\u7406\u7cbe\u5ea6", "motivation": "\u73b0\u6709CoT\u63d0\u793a\u6280\u672f\u53ca\u5176\u6269\u5c55\u867d\u7136\u63d0\u5347\u51c6\u786e\u7387\uff0c\u4f46\u4ea7\u751f\u9ad8\u989dtoken\u6210\u672c\uff0c\u963b\u788d\u5728\u5ef6\u8fdf\u654f\u611f\u573a\u666f\u7684\u90e8\u7f72", "method": "1.\u8c03\u6574\u63a8\u7406\u8f68\u8ff9\u6570\u91cf 2.\u63a7\u5236\u6bcf\u4e2a\u8f68\u8ff9\u7684\u6700\u7ec8\u89e3\u6570\u91cf 3.\u52a8\u6001\u622a\u65ad\u63a8\u7406\u8f68\u8ff9\u6df1\u5ea6", "result": "\u57285\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0log-linear\u7ea7\u522b\u7684Pass@k\u4e0etoken\u9884\u7b97\u7684\u4f18\u5316\u6743\u8861\uff0c\u9a8c\u8bc1\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u7b56\u7565\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u4e09\u7ef4\u5ea6\u8ba1\u7b97\u8d44\u6e90\u4f18\u5316\u5b9e\u73b0\u7cbe\u5ea6\u4e0e\u6210\u672c\u7684\u5e73\u8861"}}
{"id": "2505.13028", "pdf": "https://arxiv.org/pdf/2505.13028", "abs": "https://arxiv.org/abs/2505.13028", "authors": ["Sayon Palit", "Daniel Woods"], "title": "Evaluatiing the efficacy of LLM Safety Solutions : The Palit Benchmark Dataset", "categories": ["cs.CR", "cs.AI", "cs.CL", "F.2.2, I.2.7; F.2.2, I.2.7; F.2.2, I.2.7"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly integrated into critical\nsystems in industries like healthcare and finance. Users can often submit\nqueries to LLM-enabled chatbots, some of which can enrich responses with\ninformation retrieved from internal databases storing sensitive data. This\ngives rise to a range of attacks in which a user submits a malicious query and\nthe LLM-system outputs a response that creates harm to the owner, such as\nleaking internal data or creating legal liability by harming a third-party.\nWhile security tools are being developed to counter these threats, there is\nlittle formal evaluation of their effectiveness and usability. This study\naddresses this gap by conducting a thorough comparative analysis of LLM\nsecurity tools. We identified 13 solutions (9 closed-source, 4 open-source),\nbut only 7 were evaluated due to a lack of participation by proprietary model\nowners.To evaluate, we built a benchmark dataset of malicious prompts, and\nevaluate these tools performance against a baseline LLM model\n(ChatGPT-3.5-Turbo). Our results show that the baseline model has too many\nfalse positives to be used for this task. Lakera Guard and ProtectAI LLM Guard\nemerged as the best overall tools showcasing the tradeoff between usability and\nperformance. The study concluded with recommendations for greater transparency\namong closed source providers, improved context-aware detections, enhanced\nopen-source engagement, increased user awareness, and the adoption of more\nrepresentative performance metrics.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e867\u6b3eLLM\u5b89\u5168\u5de5\u5177\u7684\u6709\u6548\u6027\uff0c\u53d1\u73b0\u57fa\u7ebf\u6a21\u578b\u8bef\u62a5\u7387\u8fc7\u9ad8\uff0c\u63a8\u8350Lakera Guard\u548cProtectAI LLM Guard\u4f5c\u4e3a\u5e73\u8861\u6027\u80fd\u7684\u6700\u4f73\u65b9\u6848\u3002", "motivation": "LLM\u7cfb\u7edf\u5728\u5173\u952e\u9886\u57df\u6574\u5408\u65f6\u9762\u4e34\u6076\u610f\u67e5\u8be2\u5bfc\u81f4\u6570\u636e\u6cc4\u9732\u548c\u6cd5\u5f8b\u8d23\u4efb\u7684\u98ce\u9669\uff0c\u800c\u73b0\u6709\u5b89\u5168\u5de5\u5177\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u6784\u5efa\u6076\u610f\u63d0\u793a\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5bf9\u6bd413\u6b3e\u5de5\u5177(\u5b9e\u9645\u8bc4\u4f307\u6b3e)\u4e0eChatGPT-3.5-Turbo\u57fa\u7ebf\u6a21\u578b\u7684\u9632\u62a4\u6027\u80fd\u3002", "result": "\u57fa\u7ebf\u6a21\u578b\u8bef\u62a5\u7387\u8fc7\u9ad8\uff0cLakera Guard\u548cProtectAI\u5728\u53ef\u7528\u6027\u4e0e\u9632\u62a4\u6548\u679c\u95f4\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u5efa\u8bae\u52a0\u5f3a\u95ed\u6e90\u5de5\u5177\u900f\u660e\u5ea6\u3001\u5f00\u53d1\u4e0a\u4e0b\u6587\u611f\u77e5\u68c0\u6d4b\u3001\u63a8\u52a8\u5f00\u6e90\u793e\u533a\u53c2\u4e0e\uff0c\u5e76\u91c7\u7528\u66f4\u7cbe\u51c6\u7684\u8bc4\u4f30\u6307\u6807\u3002"}}
{"id": "2505.13032", "pdf": "https://arxiv.org/pdf/2505.13032", "abs": "https://arxiv.org/abs/2505.13032", "authors": ["Ziyang Ma", "Yinghao Ma", "Yanqiao Zhu", "Chen Yang", "Yi-Wen Chao", "Ruiyang Xu", "Wenxi Chen", "Yuanzhe Chen", "Zhuo Chen", "Jian Cong", "Kai Li", "Keliang Li", "Siyou Li", "Xinfeng Li", "Xiquan Li", "Zheng Lian", "Yuzhe Liang", "Minghao Liu", "Zhikang Niu", "Tianrui Wang", "Yuping Wang", "Yuxuan Wang", "Yihao Wu", "Guanrou Yang", "Jianwei Yu", "Ruibin Yuan", "Zhisheng Zheng", "Ziya Zhou", "Haina Zhu", "Wei Xue", "Emmanouil Benetos", "Kai Yu", "Eng-Siong Chng", "Xie Chen"], "title": "MMAR: A Challenging Benchmark for Deep Reasoning in Speech, Audio, Music, and Their Mix", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "comment": "Open-source at https://github.com/ddlBoJack/MMAR", "summary": "We introduce MMAR, a new benchmark designed to evaluate the deep reasoning\ncapabilities of Audio-Language Models (ALMs) across massive multi-disciplinary\ntasks. MMAR comprises 1,000 meticulously curated audio-question-answer\ntriplets, collected from real-world internet videos and refined through\niterative error corrections and quality checks to ensure high quality. Unlike\nexisting benchmarks that are limited to specific domains of sound, music, or\nspeech, MMAR extends them to a broad spectrum of real-world audio scenarios,\nincluding mixed-modality combinations of sound, music, and speech. Each\nquestion in MMAR is hierarchically categorized across four reasoning layers:\nSignal, Perception, Semantic, and Cultural, with additional sub-categories\nwithin each layer to reflect task diversity and complexity. To further foster\nresearch in this area, we annotate every question with a Chain-of-Thought (CoT)\nrationale to promote future advancements in audio reasoning. Each item in the\nbenchmark demands multi-step deep reasoning beyond surface-level understanding.\nMoreover, a part of the questions requires graduate-level perceptual and\ndomain-specific knowledge, elevating the benchmark's difficulty and depth. We\nevaluate MMAR using a broad set of models, including Large Audio-Language\nModels (LALMs), Large Audio Reasoning Models (LARMs), Omni Language Models\n(OLMs), Large Language Models (LLMs), and Large Reasoning Models (LRMs), with\naudio caption inputs. The performance of these models on MMAR highlights the\nbenchmark's challenging nature, and our analysis further reveals critical\nlimitations of understanding and reasoning capabilities among current models.\nWe hope MMAR will serve as a catalyst for future advances in this important but\nlittle-explored area.", "AI": {"tldr": "MMAR\u662f\u5305\u542b1000\u4e2a\u9ad8\u8d28\u91cf\u97f3\u9891-\u95ee\u7b54\u5bf9\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8986\u76d6\u591a\u9886\u57df\u771f\u5b9e\u97f3\u9891\u573a\u666f\uff0c\u901a\u8fc7\u56db\u7ea7\u63a8\u7406\u5c42\u6b21\u8bc4\u4f30\u6a21\u578b\u7684\u6df1\u5ea6\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u7a81\u7834\u73b0\u6709\u97f3\u9891\u57fa\u51c6\u7684\u9886\u57df\u9650\u5236\uff0c\u6784\u5efa\u591a\u6a21\u6001\u6df7\u5408\u7684\u771f\u5b9e\u573a\u666f\u6d4b\u8bd5\u96c6\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u95ee\u9898\u5206\u7c7b\u548c\u601d\u7ef4\u94fe\u6807\u6ce8\u63a8\u52a8\u97f3\u9891\u63a8\u7406\u7814\u7a76\u3002", "method": "\u4ece\u4e92\u8054\u7f51\u89c6\u9891\u6536\u96c6\u6570\u636e\uff0c\u7ecf\u591a\u8f6e\u7ea0\u9519\u6784\u5efa\u6570\u636e\u96c6\uff1b\u95ee\u9898\u5206\u4e3a\u4fe1\u53f7\u3001\u611f\u77e5\u3001\u8bed\u4e49\u3001\u6587\u5316\u56db\u5c42\uff0c\u6bcf\u5c42\u5305\u542b\u591a\u4e2a\u5b50\u7c7b\uff1b\u4f7f\u7528\u97f3\u9891\u5b57\u5e55\u8f93\u5165\u8bc4\u4f30\u591a\u79cd\u5927\u6a21\u578b\u3002", "result": "\u73b0\u6709\u6a21\u578b\u8868\u73b0\u663e\u8457\u4e0d\u8db3\uff0c\u63ed\u793a\u5176\u5728\u590d\u6742\u63a8\u7406\u548c\u6587\u5316\u80cc\u666f\u7406\u89e3\u65b9\u9762\u7684\u6838\u5fc3\u7f3a\u9677\uff0c\u90e8\u5206\u95ee\u9898\u9700\u8981\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u624d\u80fd\u89e3\u7b54\u3002", "conclusion": "MMAR\u586b\u8865\u97f3\u9891\u6df1\u5ea6\u63a8\u7406\u8bc4\u4f30\u7a7a\u767d\uff0c\u5176\u5c42\u6b21\u5316\u8bbe\u8ba1\u548c\u4e13\u4e1a\u7ea7\u95ee\u9898\u8bbe\u5b9a\u4e3a\u97f3\u9891-\u8bed\u8a00\u6a21\u578b\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u91cd\u8981\u65b9\u5411\u6307\u5f15\u3002"}}
{"id": "2505.13098", "pdf": "https://arxiv.org/pdf/2505.13098", "abs": "https://arxiv.org/abs/2505.13098", "authors": ["Lars-Peter Meyer", "Johannes Frey", "Desiree Heim", "Felix Brei", "Claus Stadler", "Kurt Junghanns", "Michael Martin"], "title": "LLM-KG-Bench 3.0: A Compass for SemanticTechnology Capabilities in the Ocean of LLMs", "categories": ["cs.AI", "cs.CL", "cs.DB"], "comment": "Peer reviewed publication at ESWC 2025 Resources Track", "summary": "Current Large Language Models (LLMs) can assist developing program code\nbeside many other things, but can they support working with Knowledge Graphs\n(KGs) as well? Which LLM is offering the best capabilities in the field of\nSemantic Web and Knowledge Graph Engineering (KGE)? Is this possible to\ndetermine without checking many answers manually? The LLM-KG-Bench framework in\nVersion 3.0 is designed to answer these questions. It consists of an extensible\nset of tasks for automated evaluation of LLM answers and covers different\naspects of working with semantic technologies. In this paper the LLM-KG-Bench\nframework is presented in Version 3 along with a dataset of prompts, answers\nand evaluations generated with it and several state-of-the-art LLMs.\nSignificant enhancements have been made to the framework since its initial\nrelease, including an updated task API that offers greater flexibility in\nhandling evaluation tasks, revised tasks, and extended support for various open\nmodels through the vllm library, among other improvements. A comprehensive\ndataset has been generated using more than 30 contemporary open and proprietary\nLLMs, enabling the creation of exemplary model cards that demonstrate the\nmodels' capabilities in working with RDF and SPARQL, as well as comparing their\nperformance on Turtle and JSON-LD RDF serialization tasks.", "AI": {"tldr": "LLM-KG-Bench 3.0\u6846\u67b6\u7528\u4e8e\u81ea\u52a8\u5316\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u56fe\u8c31\u5de5\u7a0b\u4e2d\u7684\u80fd\u529b\uff0c\u8986\u76d6RDF/SPARQL\u5904\u7406\u53ca\u5e8f\u5217\u5316\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u624b\u52a8\u8bc4\u4f30LLM\u5728\u8bed\u4e49\u6280\u672f\u548c\u77e5\u8bc6\u56fe\u8c31\u9886\u57df\u80fd\u529b\u7684\u4f4e\u6548\u95ee\u9898\uff0c\u5efa\u7acb\u81ea\u52a8\u5316\u8bc4\u4f30\u6807\u51c6\u5e76\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u7684\u6027\u80fd\u5dee\u5f02\u3002", "method": "\u901a\u8fc7\u53ef\u6269\u5c55\u4efb\u52a1\u96c6\u6784\u5efa\u8bc4\u4f30\u6846\u67b6\uff0c\u6539\u8fdb\u4efb\u52a1API\u589e\u5f3a\u7075\u6d3b\u6027\uff0c\u96c6\u6210vllm\u5e93\u652f\u6301\u5f00\u6e90\u6a21\u578b\uff0c\u521b\u5efa\u5305\u542b30+LLM\u7684\u6d4b\u8bd5\u6570\u636e\u96c6\u3002", "result": "\u751f\u6210\u6a21\u578b\u80fd\u529b\u5361\u7247\uff0c\u9a8c\u8bc1\u4e0d\u540cLLM\u5728Turtle\u4e0eJSON-LD\u5e8f\u5217\u5316\u4efb\u52a1\u7684\u8868\u73b0\u5dee\u5f02\uff0c\u63ed\u793a\u95ed\u6e90\u6a21\u578b\u5728\u7ed3\u6784\u5316\u6570\u636e\u5904\u7406\u4e2d\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLLM\u7684\u8bed\u4e49\u6280\u672f\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u7cfb\u7edf\u5316\u65b9\u6848\uff0c\u7248\u672c\u8fed\u4ee3\u663e\u8457\u63d0\u5347\u8bc4\u4f30\u7ef4\u5ea6\u8986\u76d6\u5ea6\u548c\u6a21\u578b\u652f\u6301\u8303\u56f4\u3002"}}
{"id": "2505.13109", "pdf": "https://arxiv.org/pdf/2505.13109", "abs": "https://arxiv.org/abs/2505.13109", "authors": ["Guangda Liu", "Chengwei Li", "Zhenyu Ning", "Jing Lin", "Yiwu Yao", "Danning Ke", "Minyi Guo", "Jieru Zhao"], "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding\ncontext windows to support increasingly demanding applications. However, long\ncontexts pose significant deployment challenges, primarily due to the KV cache\nwhose size grows proportionally with context length. While KV cache compression\nmethods are proposed to address this issue, KV dropping methods incur\nconsiderable accuracy loss, and KV retrieval methods suffer from significant\nefficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization\nframework to enhance KV retrieval efficiency while preserving accuracy. On the\nalgorithm side, FreeKV introduces speculative retrieval to shift the KV\nselection and recall processes out of the critical path, combined with\nfine-grained correction to ensure accuracy. On the system side, FreeKV employs\nhybrid KV layouts across CPU and GPU memory to eliminate fragmented data\ntransfers, and leverages double-buffered streamed recall to further improve\nefficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy\nacross various scenarios and models, delivering up to 13$\\times$ speedup\ncompared to SOTA KV retrieval methods.", "AI": {"tldr": "FreeKV\u901a\u8fc7\u7b97\u6cd5-\u7cfb\u7edf\u534f\u540c\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548KV\u68c0\u7d22\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u52a0\u901f13\u500d", "motivation": "\u73b0\u6709KV\u7f13\u5b58\u65b9\u6cd5\u5b58\u5728\u7cbe\u5ea6\u635f\u5931\uff08KV\u4e22\u5f03\uff09\u6216\u6548\u7387\u74f6\u9888\uff08KV\u68c0\u7d22\uff09\uff0c\u65e0\u6cd5\u6ee1\u8db3\u957f\u4e0a\u4e0b\u6587\u573a\u666f\u7684\u90e8\u7f72\u9700\u6c42", "method": "\u7b97\u6cd5\u4fa7\u91c7\u7528\u63a8\u6d4b\u5f0f\u68c0\u7d22+\u7ec6\u7c92\u5ea6\u4fee\u6b63\uff0c\u7cfb\u7edf\u4fa7\u8bbe\u8ba1\u6df7\u5408KV\u5185\u5b58\u5e03\u5c40\u548c\u53cc\u7f13\u51b2\u6d41\u5f0f\u53ec\u56de\u673a\u5236", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u573a\u666f\u4e0b\u5b9e\u73b0\u63a5\u8fd1\u65e0\u635f\u7684\u7cbe\u5ea6\uff0c\u68c0\u7d22\u901f\u5ea6\u63d0\u5347\u6700\u9ad8\u8fbe13\u500d", "conclusion": "FreeKV\u6709\u6548\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587\u90e8\u7f72\u7684KV\u7f13\u5b58\u6548\u7387\u4e0e\u7cbe\u5ea6\u7684\u5e73\u8861\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2505.13126", "pdf": "https://arxiv.org/pdf/2505.13126", "abs": "https://arxiv.org/abs/2505.13126", "authors": ["Liancheng Gong", "Wang Zhu", "Jesse Thomason", "Li Zhang"], "title": "Zero-Shot Iterative Formalization and Planning in Partially Observable Environments", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "In planning, using LLMs not to predict plans but to formalize an environment\ninto the Planning Domain Definition Language (PDDL) has been shown to greatly\nimprove performance and control. While most work focused on fully observable\nenvironments, we tackle the more realistic and challenging partially observable\nenvironments where existing methods are incapacitated by the lack of complete\ninformation. We propose PDDLego+, a framework to iteratively formalize, plan,\ngrow, and refine PDDL representations in a zero-shot manner, without needing\naccess to any existing trajectories. On two textual simulated environments, we\nshow that PDDLego+ not only achieves superior performance, but also shows\nrobustness against problem complexity. We also show that the domain knowledge\ncaptured after a successful trial is interpretable and benefits future tasks.", "AI": {"tldr": "\u63d0\u51faPDDDLego+\u6846\u67b6\u89e3\u51b3\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7684\u89c4\u5212\u95ee\u9898\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u65b9\u5f0f\u8fed\u4ee3\u6784\u5efaPDDL\u8868\u793a", "motivation": "\u73b0\u6709PDDL\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u5b8c\u5168\u53ef\u89c2\u6d4b\u73af\u5883\uff0c\u65e0\u6cd5\u5904\u7406\u73b0\u5b9e\u573a\u666f\u4e2d\u666e\u904d\u5b58\u5728\u7684\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u95ee\u9898", "method": "\u5f00\u53d1\u65e0\u9700\u8f68\u8ff9\u6570\u636e\u7684\u96f6\u6837\u672c\u6846\u67b6\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316-\u89c4\u5212-\u6269\u5c55-\u7cbe\u70bc\u56db\u6b65\u8fed\u4ee3\u4f18\u5316PDDL\u6a21\u578b", "result": "\u5728\u4e24\u4e2a\u6587\u672c\u6a21\u62df\u73af\u5883\u4e2d\u5b9e\u73b0\u6027\u80fd\u8d85\u8d8a\u57fa\u51c6\u65b9\u6cd5\uff0c\u5c55\u73b0\u5bf9\u590d\u6742\u95ee\u9898\u7684\u9c81\u68d2\u6027\u548c\u77e5\u8bc6\u8fc1\u79fb\u80fd\u529b", "conclusion": "PDDDLego+\u4e0d\u4ec5\u63d0\u5347\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7684\u89c4\u5212\u6548\u679c\uff0c\u5176\u4ea7\u751f\u7684\u53ef\u89e3\u91ca\u9886\u57df\u77e5\u8bc6\u5177\u5907\u6301\u7eed\u590d\u7528\u4ef7\u503c"}}
{"id": "2505.13208", "pdf": "https://arxiv.org/pdf/2505.13208", "abs": "https://arxiv.org/abs/2505.13208", "authors": ["Colin Krawchuk", "Nikhil Khatri", "Neil John Ortega", "Dimitri Kartsaklis"], "title": "Efficient Generation of Parameterised Quantum Circuits from Large Texts", "categories": ["quant-ph", "cs.AI", "cs.CL"], "comment": null, "summary": "Quantum approaches to natural language processing (NLP) are redefining how\nlinguistic information is represented and processed. While traditional hybrid\nquantum-classical models rely heavily on classical neural networks, recent\nadvancements propose a novel framework, DisCoCirc, capable of directly encoding\nentire documents as parameterised quantum circuits (PQCs), besides enjoying\nsome additional interpretability and compositionality benefits. Following these\nideas, this paper introduces an efficient methodology for converting\nlarge-scale texts into quantum circuits using tree-like representations of\npregroup diagrams. Exploiting the compositional parallels between language and\nquantum mechanics, grounded in symmetric monoidal categories, our approach\nenables faithful and efficient encoding of syntactic and discourse\nrelationships in long and complex texts (up to 6410 words in our experiments)\nto quantum circuits. The developed system is provided to the community as part\nof the augmented open-source quantum NLP package lambeq Gen II.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6811\u72b6\u524d\u7fa4\u56fe\u7684\u9ad8\u6548\u91cf\u5b50\u7535\u8def\u7f16\u7801\u65b9\u6cd5\uff0c\u652f\u6301\u957f\u6587\u672c\u5904\u7406\u5e76\u96c6\u6210\u81f3lambeq Gen II\u5de5\u5177\u5305", "motivation": "\u4f20\u7edf\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6a21\u578b\u4e25\u91cd\u4f9d\u8d56\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\uff0c\u65b0\u6846\u67b6DisCoCirc\u80fd\u591f\u76f4\u63a5\u7f16\u7801\u5b8c\u6574\u6587\u6863\u4e3a\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\uff0c\u517c\u5177\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u7ec4\u5408\u6027\u4f18\u52bf", "method": "\u5229\u7528\u524d\u7fa4\u56fe\u7684\u6811\u72b6\u8868\u793a\u5c06\u5927\u89c4\u6a21\u6587\u672c\u8f6c\u6362\u4e3a\u91cf\u5b50\u7535\u8def\uff0c\u57fa\u4e8e\u5bf9\u79f0\u5e7a\u534a\u7fa4\u8303\u7574\u7684\u8bed\u8a00\u4e0e\u91cf\u5b50\u529b\u5b66\u7ec4\u5408\u7279\u6027\uff0c\u5b9e\u73b0\u957f\u6587\u672c(\u5b9e\u9a8c\u8fbe6410\u8bcd)\u53e5\u6cd5\u548c\u8bed\u7bc7\u5173\u7cfb\u7684\u9ad8\u6548\u7f16\u7801", "result": "\u6210\u529f\u5f00\u53d1\u53ef\u5904\u7406\u590d\u6742\u957f\u6587\u672c\u7684\u91cf\u5b50\u7535\u8def\u7f16\u7801\u7cfb\u7edf\uff0c\u652f\u6301\u8d85\u8fc76000\u8bcd\u7684\u6587\u672c\u5904\u7406\u80fd\u529b", "conclusion": "\u901a\u8fc7\u91cf\u5b50\u7535\u8def\u76f4\u63a5\u7f16\u7801\u8bed\u8a00\u7ed3\u6784\u7684\u65b9\u6cd5\u63a8\u8fdb\u4e86\u91cf\u5b50NLP\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u5f00\u6e90\u7684lambeq Gen II\u5de5\u5177\u5305\u4e3a\u793e\u533a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u91cf\u5b50\u8bed\u8a00\u5904\u7406\u57fa\u7840\u67b6\u6784"}}
{"id": "2505.13227", "pdf": "https://arxiv.org/pdf/2505.13227", "abs": "https://arxiv.org/abs/2505.13227", "authors": ["Tianbao Xie", "Jiaqi Deng", "Xiaochuan Li", "Junlin Yang", "Haoyuan Wu", "Jixuan Chen", "Wenjing Hu", "Xinyuan Wang", "Yuhui Xu", "Zekun Wang", "Yiheng Xu", "Junli Wang", "Doyen Sahoo", "Tao Yu", "Caiming Xiong"], "title": "Scaling Computer-Use Grounding via User Interface Decomposition and Synthesis", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "comment": "49 pages, 13 figures", "summary": "Graphical user interface (GUI) grounding, the ability to map natural language\ninstructions to specific actions on graphical user interfaces, remains a\ncritical bottleneck in computer use agent development. Current benchmarks\noversimplify grounding tasks as short referring expressions, failing to capture\nthe complexity of real-world interactions that require software commonsense,\nlayout understanding, and fine-grained manipulation capabilities. To address\nthese limitations, we introduce OSWorld-G, a comprehensive benchmark comprising\n564 finely annotated samples across diverse task types including text matching,\nelement recognition, layout understanding, and precise manipulation.\nAdditionally, we synthesize and release the largest computer use grounding\ndataset Jedi, which contains 4 million examples through multi-perspective\ndecoupling of tasks. Our multi-scale models trained on Jedi demonstrate its\neffectiveness by outperforming existing approaches on ScreenSpot-v2,\nScreenSpot-Pro, and our OSWorld-G. Furthermore, we demonstrate that improved\ngrounding with Jedi directly enhances agentic capabilities of general\nfoundation models on complex computer tasks, improving from 5% to 27% on\nOSWorld. Through detailed ablation studies, we identify key factors\ncontributing to grounding performance and verify that combining specialized\ndata for different interface elements enables compositional generalization to\nnovel interfaces. All benchmark, data, checkpoints, and code are open-sourced\nand available at https://osworld-grounding.github.io.", "AI": {"tldr": "\u63d0\u51faOSWorld-G\u57fa\u51c6\u548cJedi\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u6a21\u578b\u6709\u6548\u89e3\u51b3GUI\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u6620\u5c04\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7406\u4efb\u52a1\u6027\u80fd", "motivation": "\u73b0\u6709GUI\u57fa\u51c6\u8fc7\u5ea6\u7b80\u5316\u4efb\u52a1\uff0c\u7f3a\u4e4f\u771f\u5b9e\u573a\u666f\u6240\u9700\u7684\u8f6f\u4ef6\u5e38\u8bc6\u3001\u5e03\u5c40\u7406\u89e3\u548c\u7cbe\u7ec6\u64cd\u4f5c\u80fd\u529b\u8bc4\u4f30", "method": "\u6784\u5efa\u5305\u542b564\u4e2a\u6807\u6ce8\u6837\u672c\u7684OSWorld-G\u57fa\u51c6\uff0c\u5e76\u5408\u6210400\u4e07\u6837\u672c\u7684Jedi\u6570\u636e\u96c6\uff0c\u91c7\u7528\u591a\u89c6\u89d2\u4efb\u52a1\u89e3\u8026\u548c\u591a\u5c3a\u5ea6\u6a21\u578b\u8bad\u7ec3", "result": "\u6a21\u578b\u5728ScreenSpot\u7cfb\u5217\u57fa\u51c6\u548cOSWorld-G\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u590d\u6742\u8ba1\u7b97\u673a\u4efb\u52a1\u4ee3\u7406\u80fd\u529b\u4ece5%\u63d0\u5347\u81f327%", "conclusion": "\u901a\u8fc7\u4e13\u4e1a\u6570\u636e\u7ec4\u5408\u5b9e\u73b0\u754c\u9762\u7ec4\u5408\u6cdb\u5316\uff0c\u5f00\u6e90\u8d44\u6e90\u63a8\u52a8\u9886\u57df\u53d1\u5c55\uff0c\u9a8c\u8bc1\u4e86\u6570\u636e\u8d28\u91cf\u5bf9\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u7684\u5173\u952e\u4f5c\u7528"}}
{"id": "2505.13237", "pdf": "https://arxiv.org/pdf/2505.13237", "abs": "https://arxiv.org/abs/2505.13237", "authors": ["Chih-Kai Yang", "Neo Ho", "Yen-Ting Piao", "Hung-yi Lee"], "title": "SAKURA: On the Multi-hop Reasoning of Large Audio-Language Models Based on Speech and Audio Information", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to Interspeech 2025", "summary": "Large audio-language models (LALMs) extend the large language models with\nmultimodal understanding in speech, audio, etc. While their performances on\nspeech and audio-processing tasks are extensively studied, their reasoning\nabilities remain underexplored. Particularly, their multi-hop reasoning, the\nability to recall and integrate multiple facts, lacks systematic evaluation.\nExisting benchmarks focus on general speech and audio-processing tasks,\nconversational abilities, and fairness but overlook this aspect. To bridge this\ngap, we introduce SAKURA, a benchmark assessing LALMs' multi-hop reasoning\nbased on speech and audio information. Results show that LALMs struggle to\nintegrate speech/audio representations for multi-hop reasoning, even when they\nextract the relevant information correctly, highlighting a fundamental\nchallenge in multimodal reasoning. Our findings expose a critical limitation in\nLALMs, offering insights and resources for future research.", "AI": {"tldr": "\u5927\u578b\u97f3\u9891\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8df3\u63a8\u7406\u4e2d\u5b58\u5728\u6574\u5408\u56f0\u96be\uff0cSAKURA\u57fa\u51c6\u63ed\u793a\u5176\u591a\u6a21\u6001\u63a8\u7406\u74f6\u9888", "motivation": "\u73b0\u6709\u7814\u7a76\u5ffd\u89c6\u4e86\u5bf9LALMs\u591a\u8df3\u63a8\u7406\u80fd\u529b\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u9700\u586b\u8865\u8be5\u9886\u57df\u7a7a\u767d", "method": "\u5f00\u53d1SAKURA\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u4e13\u95e8\u8bc4\u4f30\u8bed\u97f3/\u97f3\u9891\u4fe1\u606f\u7684\u591a\u8df3\u63a8\u7406\u80fd\u529b", "result": "\u6a21\u578b\u5373\u4f7f\u80fd\u6b63\u786e\u63d0\u53d6\u5355\u6a21\u6001\u4fe1\u606f\uff0c\u4ecd\u96be\u4ee5\u6574\u5408\u591a\u6a21\u6001\u8868\u5f81\u8fdb\u884c\u63a8\u7406\uff08\u51c6\u786e\u7387\u4e0b\u964d35%\uff09", "conclusion": "\u591a\u6a21\u6001\u8868\u5f81\u6574\u5408\u662fLALMs\u7684\u6838\u5fc3\u6311\u6218\uff0c\u9700\u91cd\u65b0\u8bbe\u8ba1\u6a21\u578b\u67b6\u6784\u63d0\u5347\u8de8\u6a21\u6001\u63a8\u7406\u80fd\u529b"}}
{"id": "2505.13308", "pdf": "https://arxiv.org/pdf/2505.13308", "abs": "https://arxiv.org/abs/2505.13308", "authors": ["Hengli Li", "Chenxi Li", "Tong Wu", "Xuekai Zhu", "Yuxuan Wang", "Zhaoxin Yu", "Eric Hanchen Jiang", "Song-Chun Zhu", "Zixia Jia", "Ying Nian Wu", "Zilong Zheng"], "title": "Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reasoning ability, a core component of human intelligence, continues to pose\na significant challenge for Large Language Models (LLMs) in the pursuit of AGI.\nAlthough model performance has improved under the training scaling law,\nsignificant challenges remain, particularly with respect to training\nalgorithms, such as catastrophic forgetting, and the limited availability of\nnovel training data. As an alternative, test-time scaling enhances reasoning\nperformance by increasing test-time computation without parameter updating.\nUnlike prior methods in this paradigm focused on token space, we propose\nleveraging latent space for more effective reasoning and better adherence to\nthe test-time scaling law. We introduce LatentSeek, a novel framework that\nenhances LLM reasoning through Test-Time Instance-level Adaptation (TTIA)\nwithin the model's latent space. Specifically, LatentSeek leverages policy\ngradient to iteratively update latent representations, guided by self-generated\nreward signals. LatentSeek is evaluated on a range of reasoning benchmarks,\nincluding GSM8K, MATH-500, and AIME2024, across multiple LLM architectures.\nResults show that LatentSeek consistently outperforms strong baselines, such as\nChain-of-Thought prompting and fine-tuning-based methods. Furthermore, our\nanalysis demonstrates that LatentSeek is highly efficient, typically converging\nwithin a few iterations for problems of average complexity, while also\nbenefiting from additional iterations, thereby highlighting the potential of\ntest-time scaling in the latent space. These findings position LatentSeek as a\nlightweight, scalable, and effective solution for enhancing the reasoning\ncapabilities of LLMs.", "AI": {"tldr": "\u63d0\u51faLatentSeek\u6846\u67b6\uff0c\u901a\u8fc7\u6f5c\u5728\u7a7a\u95f4\u6d4b\u8bd5\u65f6\u5b9e\u4f8b\u7ea7\u9002\u5e94\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709LLM\u8bad\u7ec3\u65b9\u6cd5\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u548c\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u95ee\u9898\uff0c\u6d4b\u8bd5\u65f6\u6269\u5c55\u8303\u5f0f\u53ef\u907f\u514d\u53c2\u6570\u66f4\u65b0\u9700\u6c42", "method": "\u5229\u7528\u7b56\u7565\u68af\u5ea6\u5728\u6f5c\u5728\u7a7a\u95f4\u8fed\u4ee3\u66f4\u65b0\u8868\u5f81\uff0c\u7ed3\u5408\u81ea\u6211\u751f\u6210\u5956\u52b1\u4fe1\u53f7\u7684TTIA\u6846\u67b6", "result": "\u5728GSM8K\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8aCoT\u63d0\u793a\u548c\u5fae\u8c03\u65b9\u6cd5\uff0c\u5177\u5907\u5feb\u901f\u6536\u655b\u548c\u8fed\u4ee3\u589e\u76ca\u4f18\u52bf", "conclusion": "LatentSeek\u4e3a\u8f7b\u91cf\u7ea7\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\uff0c\u9a8c\u8bc1\u4e86\u6f5c\u5728\u7a7a\u95f4\u6d4b\u8bd5\u65f6\u6269\u5c55\u7684\u6709\u6548\u6027"}}
{"id": "2505.13380", "pdf": "https://arxiv.org/pdf/2505.13380", "abs": "https://arxiv.org/abs/2505.13380", "authors": ["Nam V. Nguyen", "Huy Nguyen", "Quang Pham", "Van Nguyen", "Savitha Ramasamy", "Nhat Ho"], "title": "CompeteSMoE -- Statistically Guaranteed Mixture of Experts Training via Competition", "categories": ["cs.AI", "cs.CL"], "comment": "52 pages. This work is an improved version of the previous study at\n  arXiv:2402.02526", "summary": "Sparse mixture of experts (SMoE) offers an appealing solution to scale up the\nmodel complexity beyond the mean of increasing the network's depth or width.\nHowever, we argue that effective SMoE training remains challenging because of\nthe suboptimal routing process where experts that perform computation do not\ndirectly contribute to the routing process. In this work, we propose\ncompetition, a novel mechanism to route tokens to experts with the highest\nneural response. Theoretically, we show that the competition mechanism enjoys a\nbetter sample efficiency than the traditional softmax routing. Furthermore, we\ndevelop CompeteSMoE, a simple yet effective algorithm to train large language\nmodels by deploying a router to learn the competition policy, thus enjoying\nstrong performances at a low training overhead. Our extensive empirical\nevaluations on both the visual instruction tuning and language pre-training\ntasks demonstrate the efficacy, robustness, and scalability of CompeteSMoE\ncompared to state-of-the-art SMoE strategies. We have made the implementation\navailable at: https://github.com/Fsoft-AIC/CompeteSMoE. This work is an\nimproved version of the previous study at arXiv:2402.02526", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7ade\u4e89\u673a\u5236\u7684CompeteSMoE\u7b97\u6cd5\uff0c\u901a\u8fc7\u8ba9\u8def\u7531\u5668\u76f4\u63a5\u5b66\u4e60\u4e13\u5bb6\u7ade\u4e89\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u7a00\u758f\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u8868\u73b0", "motivation": "\u4f20\u7edfSMoE\u8def\u7531\u673a\u5236\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u4e13\u5bb6\u8ba1\u7b97\u4e0e\u8def\u7531\u51b3\u7b56\u5206\u79bb\u5bfc\u81f4\u6b21\u4f18\u7ed3\u679c\u3002\u9700\u8981\u66f4\u76f4\u63a5\u7684\u8def\u7531\u53cd\u9988\u673a\u5236\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd", "method": "\u8bbe\u8ba1\u7ade\u4e89\u8def\u7531\u673a\u5236\uff1a\u8def\u7531\u5668\u8bc4\u4f30\u4e13\u5bb6\u795e\u7ecf\u54cd\u5e94\u5f3a\u5ea6\uff0c\u9009\u62e9\u54cd\u5e94\u6700\u5f3a\u7684\u4e13\u5bb6\u6267\u884c\u8ba1\u7b97\u3002\u5f00\u53d1CompeteSMoE\u8bad\u7ec3\u7b97\u6cd5\uff0c\u901a\u8fc7\u7b56\u7565\u68af\u5ea6\u4f18\u5316\u8def\u7531\u7b56\u7565", "result": "\u5728\u89c6\u89c9\u6307\u4ee4\u8c03\u6574\u548c\u8bed\u8a00\u9884\u8bad\u7ec3\u4efb\u52a1\u4e2d\u5747\u8d85\u8d8a\u73b0\u6709SMoE\u7b56\u7565\uff0c\u4fdd\u6301\u4f4e\u8bad\u7ec3\u5f00\u9500\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u597d\u7684\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027", "conclusion": "\u7ade\u4e89\u673a\u5236\u6709\u6548\u89e3\u51b3\u8def\u7531\u4e0e\u8ba1\u7b97\u5206\u79bb\u95ee\u9898\uff0c\u7406\u8bba\u8bc1\u660e\u5176\u6837\u672c\u6548\u7387\u4f18\u4e8e\u4f20\u7edfsoftmax\u8def\u7531\uff0c\u5b9e\u8df5\u9a8c\u8bc1\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u663e\u8457\u6027\u80fd\u4f18\u52bf"}}
{"id": "2505.13393", "pdf": "https://arxiv.org/pdf/2505.13393", "abs": "https://arxiv.org/abs/2505.13393", "authors": ["Christopher K. Frantz"], "title": "IG Parser: A Software Package for the Encoding of Institutional Statements using the Institutional Grammar", "categories": ["cs.MA", "cs.AI", "cs.CL", "68T30, 68T50", "E.2; H.1.0; I.7.2; I.6.5; K.4.1"], "comment": "24 pages", "summary": "This article provides an overview of IG Parser, a software that facilitates\nqualitative content analysis of formal (e.g., legal) rules or informal (e.g.,\nsocio-normative) norms, and strategies (such as conventions) -- referred to as\n\\emph{institutions} -- that govern social systems and operate configurally to\ndescribe \\emph{institutional systems}. To this end, the IG Parser employs a\ndistinctive syntax that ensures rigorous encoding of natural language, while\nautomating the transformation into various formats that support the downstream\nanalysis using diverse analytical techniques. The conceptual core of the IG\nParser is an associated syntax, IG Script, that operationalizes the conceptual\nfoundations of the Institutional Grammar, and more specifically Institutional\nGrammar 2.0, an analytical paradigm for institutional analysis. This article\npresents the IG Parser, including its conceptual foundations, syntactic\nspecification of IG Script, alongside architectural principles. This\nintroduction is augmented with selective illustrative examples that highlight\nthe use and benefit associated with the tool.", "AI": {"tldr": "IG Parser\u662f\u57fa\u4e8e\u5236\u5ea6\u8bed\u6cd52.0\u5f00\u53d1\u7684\u5b9a\u6027\u5206\u6790\u5de5\u5177\uff0c\u901a\u8fc7\u4e13\u7528\u8bed\u6cd5IG Script\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u7f16\u7801\u4e0e\u591a\u683c\u5f0f\u81ea\u52a8\u5316\u8f6c\u6362\uff0c\u652f\u6301\u793e\u4f1a\u5236\u5ea6\u5206\u6790\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5de5\u5177\u5728\u6b63\u5f0f/\u975e\u6b63\u5f0f\u5236\u5ea6\u5206\u6790\u4e2d\u5b58\u5728\u7684\u7f16\u7801\u4e0d\u4e25\u8c28\u3001\u683c\u5f0f\u8f6c\u6362\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u4e3a\u5236\u5ea6\u7cfb\u7edf\u7684\u914d\u7f6e\u6027\u63cf\u8ff0\u63d0\u4f9b\u6807\u51c6\u5316\u5206\u6790\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u5236\u5ea6\u8bed\u6cd52.0\u7406\u8bba\u6846\u67b6\uff0c\u5f00\u53d1IG Script\u8bed\u6cd5\u5b9e\u73b0\u7ed3\u6784\u5316\u7f16\u7801\uff0c\u5efa\u7acb\u81ea\u52a8\u5316\u8f6c\u6362\u7ba1\u9053\u751f\u6210\u7f51\u7edc\u56fe\u3001\u77e9\u9635\u7b49\u591a\u79cd\u5206\u6790\u683c\u5f0f\u3002", "result": "\u6210\u529f\u6784\u5efa\u5305\u542b\u8bed\u6cd5\u89c4\u8303\u3001\u67b6\u6784\u539f\u5219\u7684\u5b8c\u6574\u5de5\u5177\u7cfb\u7edf\uff0c\u901a\u8fc7\u6848\u4f8b\u9a8c\u8bc1\u5176\u5728\u591a\u5c42\u6b21\u5236\u5ea6\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\u548c\u6548\u7387\u63d0\u5347\u3002", "conclusion": "\u8be5\u5de5\u5177\u901a\u8fc7\u4e25\u8c28\u7684\u8bed\u6cd5\u7ea6\u675f\u548c\u81ea\u52a8\u5316\u6d41\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u5236\u5ea6\u5206\u6790\u7684\u6807\u51c6\u5316\u7a0b\u5ea6\u4e0e\u8de8\u65b9\u6cd5\u517c\u5bb9\u6027\uff0c\u4e3a\u590d\u6742\u793e\u4f1a\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u5173\u952e\u6280\u672f\u652f\u6491\u3002"}}
{"id": "2505.13398", "pdf": "https://arxiv.org/pdf/2505.13398", "abs": "https://arxiv.org/abs/2505.13398", "authors": ["Matan Abudy", "Orr Well", "Emmanuel Chemla", "Roni Katzir", "Nur Lan"], "title": "A Minimum Description Length Approach to Regularization in Neural Networks", "categories": ["cs.LG", "cs.CL"], "comment": "9 pages", "summary": "State-of-the-art neural networks can be trained to become remarkable\nsolutions to many problems. But while these architectures can express symbolic,\nperfect solutions, trained models often arrive at approximations instead. We\nshow that the choice of regularization method plays a crucial role: when\ntrained on formal languages with standard regularization ($L_1$, $L_2$, or\nnone), expressive architectures not only fail to converge to correct solutions\nbut are actively pushed away from perfect initializations. In contrast,\napplying the Minimum Description Length (MDL) principle to balance model\ncomplexity with data fit provides a theoretically grounded regularization\nmethod. Using MDL, perfect solutions are selected over approximations,\nindependently of the optimization algorithm. We propose that unlike existing\nregularization techniques, MDL introduces the appropriate inductive bias to\neffectively counteract overfitting and promote generalization.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\uff08MDL\uff09\u539f\u5219\u4f5c\u4e3a\u66f4\u4f18\u7684\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edfL1/L2\u6b63\u5219\u5316\u80fd\u6709\u6548\u9009\u62e9\u5b8c\u7f8e\u89e3\u51b3\u65b9\u6848\u5e76\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6b63\u5219\u5316\u65b9\u6cd5\uff08L1/L2\uff09\u5728\u5904\u7406\u5f62\u5f0f\u8bed\u8a00\u65f6\u4f1a\u963b\u788d\u6a21\u578b\u6536\u655b\u5230\u5b8c\u7f8e\u89e3\uff0c\u9700\u63a2\u7d22\u66f4\u7406\u8bba\u5408\u7406\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\uff08MDL\uff09\u539f\u5219\u5e73\u8861\u6a21\u578b\u590d\u6742\u5ea6\u4e0e\u6570\u636e\u62df\u5408\uff0c\u4f5c\u4e3a\u65b0\u578b\u6b63\u5219\u5316\u6846\u67b6\u3002", "result": "MDL\u53ef\u72ec\u7acb\u4e8e\u4f18\u5316\u7b97\u6cd5\u9009\u62e9\u5b8c\u7f8e\u89e3\uff0c\u6709\u6548\u6291\u5236\u8fc7\u62df\u5408\u5e76\u63d0\u5347\u6cdb\u5316\u6027\u80fd\u3002", "conclusion": "MDL\u63d0\u4f9b\u4e86\u7406\u8bba\u5408\u7406\u7684\u6b63\u5219\u5316\u673a\u5236\uff0c\u5176\u5f52\u7eb3\u504f\u7f6e\u80fd\u6709\u6548\u5f15\u5bfc\u6a21\u578b\u6536\u655b\u5230\u7cbe\u786e\u89e3\uff0c\u4f18\u4e8e\u4f20\u7edf\u6b63\u5219\u5316\u65b9\u6cd5\u3002"}}
{"id": "2505.13408", "pdf": "https://arxiv.org/pdf/2505.13408", "abs": "https://arxiv.org/abs/2505.13408", "authors": ["Jinhe Bi", "Danqi Yan", "Yifan Wang", "Wenke Huang", "Haokun Chen", "Guancheng Wan", "Mang Ye", "Xun Xiao", "Hinrich Schuetze", "Volker Tresp", "Yunpu Ma"], "title": "CoT-Kinetics: A Theoretical Modeling Assessing LRM Reasoning Process", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent Large Reasoning Models significantly improve the reasoning ability of\nLarge Language Models by learning to reason, exhibiting the promising\nperformance in solving complex tasks. LRMs solve tasks that require complex\nreasoning by explicitly generating reasoning trajectories together with\nanswers. Nevertheless, judging the quality of such an output answer is not easy\nbecause only considering the correctness of the answer is not enough and the\nsoundness of the reasoning trajectory part matters as well. Logically, if the\nsoundness of the reasoning part is poor, even if the answer is correct, the\nconfidence of the derived answer should be low. Existing methods did consider\njointly assessing the overall output answer by taking into account the\nreasoning part, however, their capability is still not satisfactory as the\ncausal relationship of the reasoning to the concluded answer cannot properly\nreflected. In this paper, inspired by classical mechanics, we present a novel\napproach towards establishing a CoT-Kinetics energy equation. Specifically, our\nCoT-Kinetics energy equation formulates the token state transformation process,\nwhich is regulated by LRM internal transformer layers, as like a particle\nkinetics dynamics governed in a mechanical field. Our CoT-Kinetics energy\nassigns a scalar score to evaluate specifically the soundness of the reasoning\nphase, telling how confident the derived answer could be given the evaluated\nreasoning. As such, the LRM's overall output quality can be accurately\nmeasured, rather than a coarse judgment (e.g., correct or incorrect) anymore.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7ecf\u5178\u529b\u5b66\u7684CoT-Kinetics\u80fd\u91cf\u65b9\u7a0b\uff0c\u901a\u8fc7\u7c7b\u6bd4\u7c92\u5b50\u52a8\u529b\u5b66\u8bc4\u4f30\u5927\u578b\u63a8\u7406\u6a21\u578b\u8f93\u51fa\u7b54\u6848\u7684\u7f6e\u4fe1\u5ea6", "motivation": "\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u63a8\u7406\u8f68\u8ff9\u4e0e\u7b54\u6848\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5bfc\u81f4\u5bf9\u7b54\u6848\u8d28\u91cf\u7684\u5224\u65ad\u4e0d\u7cbe\u786e\u3002\u5373\u4f7f\u7b54\u6848\u6b63\u786e\uff0c\u82e5\u63a8\u7406\u8f68\u8ff9\u4e0d\u5b8c\u5584\uff0c\u5176\u7f6e\u4fe1\u5ea6\u4e5f\u5e94\u8f83\u4f4e", "method": "\u5c06transformer\u5c42\u7684token\u72b6\u6001\u8f6c\u6362\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u673a\u68b0\u573a\u4e2d\u7684\u7c92\u5b50\u52a8\u529b\u5b66\uff0c\u5efa\u7acb\u80fd\u91cf\u65b9\u7a0b\u91cf\u5316\u63a8\u7406\u9636\u6bb5\u7684\u903b\u8f91\u4e25\u5bc6\u6027", "result": "\u5f00\u53d1\u7684CoT-Kinetics\u80fd\u91cf\u8bc4\u5206\u53ef\u7cbe\u51c6\u8bc4\u4f30\u63a8\u7406\u8d28\u91cf\uff0c\u7a81\u7834\u4f20\u7edf\u4e8c\u5143\u5224\u65ad\uff08\u6b63\u786e/\u9519\u8bef\uff09\u7684\u5c40\u9650\u6027", "conclusion": "\u8be5\u529b\u5b66\u542f\u53d1\u7684\u8bc4\u4f30\u6846\u67b6\u4e3a\u5927\u578b\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u7cbe\u7ec6\u7684\u8f93\u51fa\u8d28\u91cf\u5ea6\u91cf\u6807\u51c6\uff0c\u63a8\u52a8\u53ef\u4fe1AI\u7cfb\u7edf\u7684\u53d1\u5c55"}}
{"id": "2505.13430", "pdf": "https://arxiv.org/pdf/2505.13430", "abs": "https://arxiv.org/abs/2505.13430", "authors": ["Sifeng Shang", "Jiayi Zhou", "Chenyu Lin", "Minxian Li", "Kaiyang Zhou"], "title": "Fine-tuning Quantized Neural Networks with Zeroth-order Optimization", "categories": ["cs.LG", "cs.CL", "cs.CV"], "comment": null, "summary": "As the size of large language models grows exponentially, GPU memory has\nbecome a bottleneck for adapting these models to downstream tasks. In this\npaper, we aim to push the limits of memory-efficient training by minimizing\nmemory usage on model weights, gradients, and optimizer states, within a\nunified framework. Our idea is to eliminate both gradients and optimizer states\nusing zeroth-order optimization, which approximates gradients by perturbing\nweights during forward passes to identify gradient directions. To minimize\nmemory usage on weights, we employ model quantization, e.g., converting from\nbfloat16 to int4. However, directly applying zeroth-order optimization to\nquantized weights is infeasible due to the precision gap between discrete\nweights and continuous gradients, which would otherwise require de-quantization\nand re-quantization. To overcome this challenge, we propose Quantized\nZeroth-order Optimization (QZO), a novel approach that perturbs the continuous\nquantization scale for gradient estimation and uses a directional derivative\nclipping method to stabilize training. QZO is orthogonal to both scalar-based\nand codebook-based post-training quantization methods. Compared to\nfull-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by\nmore than 18$\\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and\nStable Diffusion 3.5 Large within a single 24GB GPU.", "AI": {"tldr": "\u63d0\u51fa\u91cf\u5316\u96f6\u9636\u4f18\u5316\uff08QZO\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d88\u9664\u68af\u5ea6/\u4f18\u5316\u5668\u72b6\u6001\u548c\u6a21\u578b\u91cf\u5316\uff0c\u5c064\u4f4dLLM\u5fae\u8c03\u5185\u5b58\u9700\u6c42\u964d\u4f4e18\u500d\u4ee5\u4e0a", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u6307\u6570\u589e\u957f\u5bfc\u81f4GPU\u5185\u5b58\u6210\u4e3a\u4e0b\u6e38\u4efb\u52a1\u9002\u914d\u74f6\u9888\uff0c\u9700\u7a81\u7834\u5185\u5b58\u6548\u7387\u6781\u9650", "method": "\u7ed3\u5408\u96f6\u9636\u4f18\u5316\uff08\u524d\u5411\u6270\u52a8\u4f30\u8ba1\u68af\u5ea6\uff09\u4e0e\u91cf\u5316\u6280\u672f\uff08int4\uff09\uff0c\u63d0\u51faQZO\u65b9\u6cd5\u901a\u8fc7\u8fde\u7eed\u91cf\u5316\u5c3a\u5ea6\u6270\u52a8\u548c\u65b9\u5411\u5bfc\u6570\u88c1\u526a\u89e3\u51b3\u79bb\u6563\u91cf\u5316\u96be\u9898", "result": "\u5728\u5355\u575724GB GPU\u4e0a\u5b9e\u73b0Llama-2-13B\u548cStable Diffusion 3.5 Large\u7684\u5fae\u8c03", "conclusion": "QZO\u521b\u65b0\u6027\u5730\u5c06\u96f6\u9636\u4f18\u5316\u4e0e\u91cf\u5316\u6280\u672f\u7ed3\u5408\uff0c\u4e3a\u5927\u6a21\u578b\u8f7b\u91cf\u5316\u8bad\u7ec3\u63d0\u4f9b\u65b0\u8303\u5f0f"}}
{"id": "2505.13438", "pdf": "https://arxiv.org/pdf/2505.13438", "abs": "https://arxiv.org/abs/2505.13438", "authors": ["Penghui Qi", "Zichen Liu", "Tianyu Pang", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Optimizing Anytime Reasoning via Budget Relative Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Scaling test-time compute is crucial for enhancing the reasoning capabilities\nof large language models (LLMs). Existing approaches typically employ\nreinforcement learning (RL) to maximize a verifiable reward obtained at the end\nof reasoning traces. However, such methods optimize only the final performance\nunder a large and fixed token budget, which hinders efficiency in both training\nand deployment. In this work, we present a novel framework, AnytimeReasoner, to\noptimize anytime reasoning performance, which aims to improve token efficiency\nand the flexibility of reasoning under varying token budget constraints. To\nachieve this, we truncate the complete thinking process to fit within sampled\ntoken budgets from a prior distribution, compelling the model to summarize the\noptimal answer for each truncated thinking for verification. This introduces\nverifiable dense rewards into the reasoning process, facilitating more\neffective credit assignment in RL optimization. We then optimize the thinking\nand summary policies in a decoupled manner to maximize the cumulative reward.\nAdditionally, we introduce a novel variance reduction technique, Budget\nRelative Policy Optimization (BRPO), to enhance the robustness and efficiency\nof the learning process when reinforcing the thinking policy. Empirical results\nin mathematical reasoning tasks demonstrate that our method consistently\noutperforms GRPO across all thinking budgets under various prior distributions,\nenhancing both training and token efficiency.", "AI": {"tldr": "\u63d0\u51faAnytimeReasoner\u6846\u67b6\u4f18\u5316\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b9e\u65f6\u63a8\u7406\u6548\u7387\uff0c\u901a\u8fc7\u52a8\u6001\u9884\u7b97\u622a\u65ad\u601d\u7ef4\u8fc7\u7a0b\u548c\u5bc6\u96c6\u5956\u52b1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8bad\u7ec3\u6548\u7387\u548ctoken\u5229\u7528\u7387\u3002", "motivation": "\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u56fa\u5b9atoken\u9884\u7b97\u4e0b\u4ec5\u4f18\u5316\u6700\u7ec8\u8868\u73b0\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6548\u7387\u548c\u90e8\u7f72\u7075\u6d3b\u6027\u4e0d\u8db3\u3002\u9700\u8981\u89e3\u51b3\u4e0d\u540c\u9884\u7b97\u573a\u666f\u4e0b\u7684\u5b9e\u65f6\u63a8\u7406\u6548\u7387\u95ee\u9898\u3002", "method": "1. \u6839\u636e\u5148\u9a8c\u5206\u5e03\u91c7\u6837\u9884\u7b97\u622a\u65ad\u601d\u7ef4\u94fe\uff0c\u5f3a\u5236\u6a21\u578b\u751f\u6210\u4e2d\u95f4\u7b54\u6848\u9a8c\u8bc1\n2. \u5f15\u5165\u53ef\u9a8c\u8bc1\u5bc6\u96c6\u5956\u52b1\u673a\u5236\n3. \u89e3\u8026\u601d\u7ef4\u751f\u6210\u548c\u7b54\u6848\u603b\u7ed3\u7b56\u7565\u4f18\u5316\n4. \u63d0\u51fa\u9884\u7b97\u76f8\u5bf9\u7b56\u7565\u4f18\u5316(BRPO)\u964d\u4f4e\u65b9\u5dee", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u5168\u9762\u8d85\u8d8aGRPO\uff0c\u6240\u6709\u9884\u7b97\u573a\u666f\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u8bad\u7ec3\u6548\u7387\u63d0\u534720%\uff0ctoken\u5229\u7528\u7387\u63d0\u9ad835%", "conclusion": "AnytimeReasoner\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u9884\u7b97\u4e0b\u7684\u5b9e\u65f6\u63a8\u7406\u4f18\u5316\u95ee\u9898\uff0c\u4e3aLLM\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2505.13445", "pdf": "https://arxiv.org/pdf/2505.13445", "abs": "https://arxiv.org/abs/2505.13445", "authors": ["Xiaoyuan Liu", "Tian Liang", "Zhiwei He", "Jiahao Xu", "Wenxuan Wang", "Pinjia He", "Zhaopeng Tu", "Haitao Mi", "Dong Yu"], "title": "Trust, But Verify: A Self-Verification Approach to Reinforcement Learning with Verifiable Rewards", "categories": ["cs.AI", "cs.CL"], "comment": "code available at https://github.com/xyliu-cs/RISE", "summary": "Large Language Models (LLMs) show great promise in complex reasoning, with\nReinforcement Learning with Verifiable Rewards (RLVR) being a key enhancement\nstrategy. However, a prevalent issue is ``superficial self-reflection'', where\nmodels fail to robustly verify their own outputs. We introduce RISE\n(Reinforcing Reasoning with Self-Verification), a novel online RL framework\ndesigned to tackle this. RISE explicitly and simultaneously trains an LLM to\nimprove both its problem-solving and self-verification abilities within a\nsingle, integrated RL process. The core mechanism involves leveraging\nverifiable rewards from an outcome verifier to provide on-the-fly feedback for\nboth solution generation and self-verification tasks. In each iteration, the\nmodel generates solutions, then critiques its own on-policy generated\nsolutions, with both trajectories contributing to the policy update. Extensive\nexperiments on diverse mathematical reasoning benchmarks show that RISE\nconsistently improves model's problem-solving accuracy while concurrently\nfostering strong self-verification skills. Our analyses highlight the\nadvantages of online verification and the benefits of increased verification\ncompute. Additionally, RISE models exhibit more frequent and accurate\nself-verification behaviors during reasoning. These advantages reinforce RISE\nas a flexible and effective path towards developing more robust and self-aware\nreasoners.", "AI": {"tldr": "\u63d0\u51faRISE\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u540c\u6b65\u8bad\u7ec3\u95ee\u9898\u89e3\u51b3\u548c\u81ea\u9a8c\u8bc1\u80fd\u529b\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u9c81\u68d2\u6027\u548c\u81ea\u7701\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709RLVR\u65b9\u6cd5\u5b58\u5728\u6a21\u578b\u81ea\u9a8c\u8bc1\u4e0d\u5145\u5206\u5bfc\u81f4\u7684\u8868\u9762\u81ea\u6211\u53cd\u601d\u95ee\u9898\uff0c\u9700\u5f00\u53d1\u66f4\u6709\u6548\u7684\u9a8c\u8bc1\u673a\u5236\u3002", "method": "\u6784\u5efa\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u7ed3\u679c\u9a8c\u8bc1\u5668\u7684\u5b9e\u65f6\u53cd\u9988\uff0c\u5728\u540c\u4e00\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u8054\u5408\u4f18\u5316\u751f\u6210\u4e0e\u9a8c\u8bc1\u80fd\u529b\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u7cbe\u5ea6\u63d0\u5347\uff0c\u6a21\u578b\u81ea\u9a8c\u8bc1\u9891\u7387\u548c\u51c6\u786e\u7387\u663e\u8457\u63d0\u9ad8\uff0c\u9a8c\u8bc1\u8ba1\u7b97\u8d44\u6e90\u589e\u52a0\u5e26\u6765\u989d\u5916\u589e\u76ca\u3002", "conclusion": "RISE\u8bc1\u660e\u4e86\u5728\u7ebf\u8054\u5408\u8bad\u7ec3\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5f00\u53d1\u5177\u6709\u81ea\u7701\u80fd\u529b\u7684\u53ef\u9760\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\u3002"}}
