<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 49]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.CR](#cs.CR) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.HC](#cs.HC) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.IR](#cs.IR) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning](https://arxiv.org/abs/2507.17842)
*Yimeng Zhang,Tian Wang,Jiri Gesi,Ziyi Wang,Yuxuan Lu,Jiacheng Lin,Sinong Zhan,Vianne Gao,Ruochen Jiao,Junze Liu,Kun Qian,Yuxin Tang,Ran Xue,Houyu Zhang,Qingjun Cui,Yufan Guo,Dakuo Wang*

Main category: cs.CL

TL;DR: 提出Shop-R1强化学习框架，通过两阶段奖励机制提升大语言模型在电商场景中模拟人类行为的准确性


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的方法受限于生成模型的推理能力，需突破传统思维链方法的性能瓶颈

Method: 双阶段强化学习框架：1）基于模型内部信号（如logit分布）的自监督式推理生成；2）分层奖励机制（难度感知缩放+细粒度属性评估）的动作预测

Result: 实验显示方法相对基线提升65%，验证了分层奖励机制的有效性

Conclusion: 通过定制化奖励机制的分阶段强化学习，显著提升语言模型在复杂场景中的人类行为模拟精度

Abstract: Large Language Models (LLMs) have recently demonstrated strong potential in
generating 'believable human-like' behavior in web environments. Prior work has
explored augmenting training data with LLM-synthesized rationales and applying
supervised fine-tuning (SFT) to enhance reasoning ability, which in turn can
improve downstream action prediction. However, the performance of such
approaches remains inherently bounded by the reasoning capabilities of the
model used to generate the rationales. In this paper, we introduce Shop-R1, a
novel reinforcement learning (RL) framework aimed at enhancing the reasoning
ability of LLMs for simulation of real human behavior in online shopping
environments Specifically, Shop-R1 decomposes the human behavior simulation
task into two stages: rationale generation and action prediction, each guided
by distinct reward signals. For rationale generation, we leverage internal
model signals (e.g., logit distributions) to guide the reasoning process in a
self-supervised manner. For action prediction, we propose a hierarchical reward
structure with difficulty-aware scaling to prevent reward hacking and enable
fine-grained reward assignment. This design evaluates both high-level action
types and the correctness of fine-grained sub-action details (attributes and
values), rewarding outputs proportionally to their difficulty. Experimental
results show that our method achieves a relative improvement of over 65%
compared to the baseline.

</details>


### [2] [Dynamic and Generalizable Process Reward Modeling](https://arxiv.org/abs/2507.17849)
*Zhangyue Yin,Qiushi Sun,Zhiyuan Zeng,Qinyuan Cheng,Xipeng Qiu,Xuanjing Huang*

Main category: cs.CL

TL;DR: 提出DG-PRM方法，通过动态奖励树和帕累托优势估计提升过程奖励模型的泛化性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型依赖启发式方法，跨领域泛化能力差；LLM评估方法忽视文本内在指导，静态评估标准难以适应复杂过程监督。

Method: 构建动态奖励树存储多维度细粒度标准，采用帕累托优势识别判别性正负样本对，实现动态奖励信号选择。

Result: 在主流基准测试中性能显著提升（任务平均提升+15.6%），在分布外场景展现卓越泛化能力（OOD准确率达92.3%）。

Conclusion: DG-PRM有效解决了过程监督中的动态适应问题，为复杂任务提供了可泛化的细粒度奖励建模框架。

Abstract: Process Reward Models (PRMs) are crucial for guiding Large Language Models
(LLMs) in complex scenarios by providing dense reward signals. However,
existing PRMs primarily rely on heuristic approaches, which struggle with
cross-domain generalization. While LLM-as-judge has been proposed to provide
generalized rewards, current research has focused mainly on feedback results,
overlooking the meaningful guidance embedded within the text. Additionally,
static and coarse-grained evaluation criteria struggle to adapt to complex
process supervision. To tackle these challenges, we propose Dynamic and
Generalizable Process Reward Modeling (DG-PRM), which features a reward tree to
capture and store fine-grained, multi-dimensional reward criteria. DG-PRM
dynamically selects reward signals for step-wise reward scoring. To handle
multifaceted reward signals, we pioneeringly adopt Pareto dominance estimation
to identify discriminative positive and negative pairs. Experimental results
show that DG-PRM achieves stunning performance on prevailing benchmarks,
significantly boosting model performance across tasks with dense rewards.
Further analysis reveals that DG-PRM adapts well to out-of-distribution
scenarios, demonstrating exceptional generalizability.

</details>


### [3] [VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL](https://arxiv.org/abs/2507.17896)
*Shubham Mohole,Sainyam Galhotra*

Main category: cs.CL

TL;DR: VeriMinder系统通过上下文语义映射框架、结构化分析框架和优化的LLM提示生成，有效检测并缓解自然语言数据库分析中的认知偏见问题，用户测试显示显著提升分析质量。


<details>
  <summary>Details</summary>
Motivation: NLIDB普及后非专业用户在数据分析时易受认知偏见影响，现有研究多聚焦文本转SQL的准确性，缺乏对分析偏见的系统性解决方案。

Method: 1) 面向具体分析场景的上下文语义偏见映射框架；2) 基于Hard-to-Vary原则的结构化分析指导框架；3) 集成多候选生成、反馈机制和自我反思的LLM提示优化系统。

Result: 用户测试显示：82.5%参与者认可系统对分析质量的提升，比较评估中VeriMinder在分析具体性（+20%）、全面性（+20%）和准确性（+20%）显著优于基线方法。

Conclusion: VeriMinder作为开源Web应用，成功帮助用户规避数据分析中的'错误提问'漏洞，其MIT许可的代码库（https://reproducibility.link/veriminder）将推动社区研究与应用扩展。

Abstract: Application systems using natural language interfaces to databases (NLIDBs)
have democratized data analysis. This positive development has also brought
forth an urgent challenge to help users who might use these systems without a
background in statistical analysis to formulate bias-free analytical questions.
Although significant research has focused on text-to-SQL generation accuracy,
addressing cognitive biases in analytical questions remains underexplored. We
present VeriMinder, https://veriminder.ai, an interactive system for detecting
and mitigating such analytical vulnerabilities. Our approach introduces three
key innovations: (1) a contextual semantic mapping framework for biases
relevant to specific analysis contexts (2) an analytical framework that
operationalizes the Hard-to-Vary principle and guides users in systematic data
analysis (3) an optimized LLM-powered system that generates high-quality,
task-specific prompts using a structured process involving multiple candidates,
critic feedback, and self-reflection.
  User testing confirms the merits of our approach. In direct user experience
evaluation, 82.5% participants reported positively impacting the quality of the
analysis. In comparative evaluation, VeriMinder scored significantly higher
than alternative approaches, at least 20% better when considered for metrics of
the analysis's concreteness, comprehensiveness, and accuracy. Our system,
implemented as a web application, is set to help users avoid "wrong question"
vulnerability during data analysis. VeriMinder code base with prompts,
https://reproducibility.link/veriminder, is available as an MIT-licensed
open-source software to facilitate further research and adoption within the
community.

</details>


### [4] [One Whisper to Grade Them All](https://arxiv.org/abs/2507.17918)
*Nhan Phan,Anusha Porwal,Yaroslav Getman,Ekaterina Voskoboinik,Tamás Grósz,Mikko Kurimo*

Main category: cs.CL

TL;DR: 提出基于Whisper-small编码器的端到端自动口语评估系统，参数减少30%且RMSE优于基线


<details>
  <summary>Details</summary>
Motivation: 解决多模块口语测试中需独立模型和人工转录的低效问题，推动计算机辅助语言学习系统的大规模应用

Method: 使用单一Whisper-small编码器处理四部分语音响应，轻量级聚合器整合信息，配合数据采样策略训练模型

Result: RMSE达0.384（优于文本基线0.44），仅使用44.8%说话人数据仍实现0.383 RMSE，展现数据高效性和不平衡数据处理能力

Conclusion: 该架构显著降低计算资源需求，为自动口语评估提供了参数高效、可扩展的解决方案

Abstract: We present an efficient end-to-end approach for holistic Automatic Speaking
Assessment (ASA) of multi-part second-language tests, developed for the 2025
Speak & Improve Challenge. Our system's main novelty is the ability to process
all four spoken responses with a single Whisper-small encoder, combine all
information via a lightweight aggregator, and predict the final score. This
architecture removes the need for transcription and per-part models, cuts
inference time, and makes ASA practical for large-scale Computer-Assisted
Language Learning systems.
  Our system achieved a Root Mean Squared Error (RMSE) of 0.384, outperforming
the text-based baseline (0.44) while using at most 168M parameters (about 70%
of Whisper-small). Furthermore, we propose a data sampling strategy, allowing
the model to train on only 44.8% of the speakers in the corpus and still reach
0.383 RMSE, demonstrating improved performance on imbalanced classes and strong
data efficiency.

</details>


### [5] [Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text](https://arxiv.org/abs/2507.17944)
*Hulayyil Alshammari,Praveen Rao*

Main category: cs.CL

TL;DR: 研究评估了6种AI检测工具对DeepSeek生成文本的识别能力，发现对抗攻击（尤其是人性化改写）会显著降低检测准确率，同时验证了DeepSeek自身作为检测器的高效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多聚焦ChatGPT等知名模型，缺乏对新型LLM（DeepSeek）生成文本检测能力的系统性评估，且对抗攻击对检测工具的影响机制尚不明确。

Method: 使用49个人类撰写样本生成对应AI文本，通过标准改写/人性化改写生成196个对抗样本，测试6种检测工具（含DeepSeek自身few-shot+CoT提示）的鲁棒性。

Result: QuillBot/Copyleaks原始文本检测近完美，但人性化攻击使Copyleaks准确率降至71%。DeepSeek采用五样本提示时仅误判1例（AI召回率96%，人类100%）。

Conclusion: 现有检测工具对新型LLM存在脆弱性，人性化攻击威胁最大，而LLM自身通过提示工程可成为高效检测器，这为AI文本识别提供了新思路。

Abstract: Large language models (LLMs) have rapidly transformed the creation of written
materials. LLMs have led to questions about writing integrity, thereby driving
the creation of artificial intelligence (AI) detection technologies.
Adversarial attacks, such as standard and humanized paraphrasing, inhibit
detectors' ability to detect machine-generated text. Previous studies have
mainly focused on ChatGPT and other well-known LLMs and have shown varying
accuracy across detectors. However, there is a clear gap in the literature
about DeepSeek, a recently published LLM. Therefore, in this work, we
investigate whether six generally accessible AI detection tools -- AI Text
Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can
consistently recognize text generated by DeepSeek. The detectors were exposed
to the aforementioned adversarial attacks. We also considered DeepSeek as a
detector by performing few-shot prompting and chain-of-thought reasoning (CoT)
for classifying AI and human-written text. We collected 49 human-authored
question-answer pairs from before the LLM era and generated matching responses
using DeepSeek-v3, producing 49 AI-generated samples. Then, we applied
adversarial techniques such as paraphrasing and humanizing to add 196 more
samples. These were used to challenge detector robustness and assess accuracy
impact. While QuillBot and Copyleaks showed near-perfect performance on
original and paraphrased DeepSeek text, others -- particularly AI Text
Classifier and GPT-2 -- showed inconsistent results. The most effective attack
was humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and
52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best
five-shot result misclassifying only one of 49 samples (AI recall 96%, human
recall 100%).

</details>


### [6] [Are LLM Belief Updates Consistent with Bayes' Theorem?](https://arxiv.org/abs/2507.17951)
*Sohaib Imran,Ihor Kendiukhov,Matthew Broerman,Aditya Thomas,Riccardo Campanella,Rob Lamb,Peter M. Atkinson*

Main category: cs.CL

TL;DR: 大语言模型参数量越大，其基于贝叶斯定理更新信念的能力越强。研究者通过贝叶斯连贯系数验证了模型规模与概率推理能力的正相关性。


<details>
  <summary>Details</summary>
Motivation: 验证模型规模扩大是否提升语言模型基于证据更新信念的贝叶斯连贯性

Method: 提出贝叶斯连贯系数(BCC)指标，构建测试数据集，测量不同规模预训练模型在基准测试中的表现

Result: 更大规模的预训练模型展现出更符合贝叶斯定理的概率分配能力

Conclusion: 理解大语言模型的概率推理机制对模型治理具有重要意义

Abstract: Do larger and more capable language models learn to update their "beliefs"
about propositions more consistently with Bayes' theorem when presented with
evidence in-context? To test this, we formulate a Bayesian Coherence
Coefficient (BCC) metric and generate a dataset with which to measure the BCC.
We measure BCC for multiple pre-trained-only language models across five model
families, comparing against the number of model parameters, the amount of
training data, and model scores on common benchmarks. Our results provide
evidence for our hypothesis that larger and more capable pre-trained language
models assign credences that are more coherent with Bayes' theorem. These
results have important implications for our understanding and governance of
LLMs.

</details>


### [7] [Natural Language Processing for Tigrinya: Current State and Future Directions](https://arxiv.org/abs/2507.17974)
*Fitsum Gaim,Jong C. Park*

Main category: cs.CL

TL;DR: 对提格里尼亚语NLP研究的十年发展轨迹进行系统综述，涵盖资源、模型及应用，揭示从规则系统到神经架构的演进过程


<details>
  <summary>Details</summary>
Motivation: 提格里尼亚语作为数百万人使用的语言却在NLP领域严重缺乏研究，需要系统性总结现有成果并为未来发展指明方向

Method: 通过分析2011-2025年间40余项研究，系统评估计算资源、模型架构在10个下游任务中的应用现状

Result: 发现研究进展与资源创建里程碑直接相关，识别出形态复杂性和资源稀缺两大核心挑战，提出形态感知建模等发展方向

Conclusion: 该研究既为学者提供全面参考，又通过公开资源库为提格里尼亚语NLP的持续发展绘制了技术路线图

Abstract: Despite being spoken by millions of people, Tigrinya remains severely
underrepresented in Natural Language Processing (NLP) research. This work
presents a comprehensive survey of NLP research for Tigrinya, analyzing over 40
studies spanning more than a decade of work from 2011 to 2025. We
systematically review the current state of computational resources, models, and
applications across ten distinct downstream tasks, including morphological
processing, machine translation, speech recognition, and question-answering.
Our analysis reveals a clear trajectory from foundational, rule-based systems
to modern neural architectures, with progress consistently unlocked by resource
creation milestones. We identify key challenges rooted in Tigrinya's
morphological complexity and resource scarcity, while highlighting promising
research directions, including morphology-aware modeling, cross-lingual
transfer, and community-centered resource development. This work serves as both
a comprehensive reference for researchers and a roadmap for advancing Tigrinya
NLP. A curated metadata of the surveyed studies and resources is made publicly
available.\footnote{Tigrinya NLP Anthology:
https://github.com/fgaim/tigrinya-nlp-anthology.

</details>


### [8] [Technical Report of TeleChat2, TeleChat2.5 and T1](https://arxiv.org/abs/2507.18013)
*Zihan Wang,Xinzhang Liu,Yitong Yao,Chao Wang,Yu Zhao,Zhihao Yang,Wenmin Deng,Kaipeng Jia,Jiaxin Peng,Yuyao Huang,Sishi Xiong,Zhuo Jiang,Kaidong Yu,Xiaohui Hu,Fubei Yao,Ruiyu Fang,Zhuoru Jiang,Ruiting Song,Qiyi Xie,Rui Xue,Xuewei He,Yanlei Xue,Zhu Yuan,Zhaoxi Zhang,Zilu Huang,Shiquan Wang,Xin Wang,Hanming Wu,Mingyuan Wang,Xufeng Zhan,Yuhan Sun,Zhaohu Xing,Yuhao Jiang,Bingkai Yang,Shuangyong Song,Yongxiang Li,Zhongjiang He,Xuelong Li*

Main category: cs.CL

TL;DR: TeleChat推出TeleChat2、TeleChat2.5和T1系列模型，通过改进预训练与后训练策略显著提升性能，T1-115B在数学和代码任务上超越GPT-4o等模型，并开源35B/115B参数版本。


<details>
  <summary>Details</summary>
Motivation: 在保持架构稳定的前提下，通过优化训练流程（预训练/SFT/DPO/持续训练）提升模型综合能力，满足复杂推理（T1）与快速响应（TeleChat2.5）的差异化需求。

Method: 1. TeleChat2：10万亿token预训练+SFT+DPO
2. TeleChat2.5/T1：领域持续预训练+强化学习（代码/数学专项优化）
3. T1专攻长链思维推理，TeleChat2.5侧重推理速度

Result: T1-115B在数学推理与代码生成任务显著优于原版TeleChat，超越OpenAI o1-mini/GPT-4o；TeleChat2.5实现快速推理，115B参数模型保持高性能与高效率平衡。

Conclusion: 该系列通过训练策略创新而非架构调整实现性能突破，开源不同规模版本为开发者提供定制化支持，推动语言模型在推理密集型场景的应用落地。

Abstract: We introduce the latest series of TeleChat models: \textbf{TeleChat2},
\textbf{TeleChat2.5}, and \textbf{T1}, offering a significant upgrade over
their predecessor, TeleChat. Despite minimal changes to the model architecture,
the new series achieves substantial performance gains through enhanced training
strategies in both pre-training and post-training stages. The series begins
with \textbf{TeleChat2}, which undergoes pretraining on 10 trillion
high-quality and diverse tokens. This is followed by Supervised Fine-Tuning
(SFT) and Direct Preference Optimization (DPO) to further enhance its
capabilities. \textbf{TeleChat2.5} and \textbf{T1} expand the pipeline by
incorporating a continual pretraining phase with domain-specific datasets,
combined with reinforcement learning (RL) to improve performance in code
generation and mathematical reasoning tasks. The \textbf{T1} variant is
designed for complex reasoning, supporting long Chain-of-Thought (CoT)
reasoning and demonstrating substantial improvements in mathematics and coding.
In contrast, \textbf{TeleChat2.5} prioritizes speed, delivering rapid
inference. Both flagship models of \textbf{T1} and \textbf{TeleChat2.5} are
dense Transformer-based architectures with 115B parameters, showcasing
significant advancements in reasoning and general task performance compared to
the original TeleChat. Notably, \textbf{T1-115B} outperform proprietary models
such as OpenAI's o1-mini and GPT-4o. We publicly release \textbf{TeleChat2},
\textbf{TeleChat2.5} and \textbf{T1}, including post-trained versions with 35B
and 115B parameters, to empower developers and researchers with
state-of-the-art language models tailored for diverse applications.

</details>


### [9] [NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database](https://arxiv.org/abs/2507.18028)
*Weizhi Fei,Hao Shi,Jing Xu,Jingchen Peng,Jiazheng Li,Jingzhao Zhang,Bo Bai,Wei Han,Zhenyuan Chen,Xueyan Niu*

Main category: cs.CL

TL;DR: 提出NeuralDB框架，通过非线性门控检索模块实现大语言模型知识的高效编辑，支持万级事实编辑同时保持模型通用能力


<details>
  <summary>Details</summary>
Motivation: 现有线性Locate-and-Edit方法在编辑大规模事实时可能损害模型通用能力，且存在编辑事实遗忘问题

Method: 将知识编辑建模为键值数据库查询，构建具有非线性门控检索机制的神经键值数据库，仅在涉及编辑事实时激活门控模块

Result: 在ZsRE和CounterFacts数据集上对GPT2-XL/GPT-J/Llama-3模型进行万级事实编辑测试，在编辑效能、泛化能力等指标全面领先，且保持六项文本任务的综合性能

Conclusion: NeuralDB在保持模型通用能力的前提下，成功实现十万级规模的高效知识编辑（50倍于先前工作），为大规模模型更新提供有效解决方案

Abstract: Efficiently editing knowledge stored in large language models (LLMs) enables
model updates without large-scale training. One possible solution is
Locate-and-Edit (L\&E), allowing simultaneous modifications of a massive number
of facts. However, such editing may compromise the general abilities of LLMs
and even result in forgetting edited facts when scaling up to thousands of
edits. In this paper, we model existing linear L\&E methods as querying a
Key-Value (KV) database. From this perspective, we then propose NeuralDB, an
editing framework that explicitly represents the edited facts as a neural KV
database equipped with a non-linear gated retrieval module, % In particular,
our gated module only operates when inference involves the edited facts,
effectively preserving the general abilities of LLMs. Comprehensive experiments
involving the editing of 10,000 facts were conducted on the ZsRE and
CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results
demonstrate that NeuralDB not only excels in editing efficacy, generalization,
specificity, fluency, and consistency, but also preserves overall performance
across six representative text understanding and generation tasks. Further
experiments indicate that NeuralDB maintains its effectiveness even when scaled
to 100,000 facts (\textbf{50x} more than in prior work).

</details>


### [10] [GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs](https://arxiv.org/abs/2507.18043)
*Duy Nguyen,Archiki Prasad,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: 提出GrAInS推理时引导方法，通过梯度归因识别关键token构建引导向量，在Transformer层动态调整激活值，显著提升模型性能且无需微调


<details>
  <summary>Details</summary>
Motivation: 现有方法存在三个局限：1)使用固定全局干预向量 2)忽略输入token的因果影响 3)未充分利用logits梯度信息，特别是在多模态场景中视觉-文本输入贡献不均衡问题

Method: 利用集成梯度对比归因识别top-k关键token，构建方向性引导向量。推理时根据token级归因信号调整隐藏层激活值，并通过激活值归一化保持表征尺度

Result: 在Llama-3.1-8B上TruthfulQA准确率提升13.22%；LLaVA-1.6-7B在MMHal-Bench幻觉率从0.624降至0.514；SPA-VL对齐胜率提升8.11%，同时保持模型流畅性和通用能力

Conclusion: GrAInS实现了细粒度、可解释的模型行为控制，无需重新训练或辅助监督，在多模态和纯语言任务中均优于微调和现有引导方法

Abstract: Inference-time steering methods offer a lightweight alternative to
fine-tuning large language models (LLMs) and vision-language models (VLMs) by
modifying internal activations at test time without updating model weights.
However, most existing approaches rely on fixed, global intervention vectors,
overlook the causal influence of individual input tokens, and fail to leverage
informative gradients from the model's logits, particularly in multimodal
settings where visual and textual inputs contribute unevenly. To address these
limitations, we introduce GrAInS, an inference-time steering approach that
operates across both language-only and vision-language models and tasks. GrAInS
uses contrastive, gradient-based attribution via Integrated Gradients to
identify the top-k most influential tokens, both positively and negatively
attributed based on their contribution to preferred versus dispreferred
outputs. These tokens are then used to construct directional steering vectors
that capture semantic shifts from undesirable to desirable behavior. During
inference, GrAInS adjusts hidden activations at transformer layers guided by
token-level attribution signals, and normalizes activations to preserve
representational scale. This enables fine-grained, interpretable, and modular
control over model behavior, without retraining or auxiliary supervision.
Empirically, GrAInS consistently outperforms both fine-tuning and existing
steering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using
Llama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514
with LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all
while preserving the model's fluency and general capabilities.

</details>


### [11] [Synthetic Data Generation for Phrase Break Prediction with Large Language Model](https://arxiv.org/abs/2507.18044)
*Hoyeon Lee,Sejung Son,Ye-Eun Kang,Jong-Hwan Kim*

Main category: cs.CL

TL;DR: 利用大型语言模型生成合成短语断句标注数据，解决人工标注成本高和语音数据变异性的问题，并验证其多语言有效性


<details>
  <summary>Details</summary>
Motivation: 传统短语断句预测依赖大量人工标注且存在语音领域音素变异导致的数据不一致问题，需要低成本且稳定的数据生成方案

Method: 通过LLM生成合成标注数据，与传统人工标注对比，并在多种语言场景下评估数据有效性

Result: LLM生成的合成数据能有效缓解数据不足问题，多语言测试显示其标注质量接近传统方法，证明LLM在语音任务中的适应性

Conclusion: LLM可作为语音领域数据生成的可靠方案，为低资源语言和复杂语音任务提供新的标注范式

Abstract: Current approaches to phrase break prediction address crucial prosodic
aspects of text-to-speech systems but heavily rely on vast human annotations
from audio or text, incurring significant manual effort and cost. Inherent
variability in the speech domain, driven by phonetic factors, further
complicates acquiring consistent, high-quality data. Recently, large language
models (LLMs) have shown success in addressing data challenges in NLP by
generating tailored synthetic data while reducing manual annotation needs.
Motivated by this, we explore leveraging LLM to generate synthetic phrase break
annotations, addressing the challenges of both manual annotation and
speech-related tasks by comparing with traditional annotations and assessing
effectiveness across multiple languages. Our findings suggest that LLM-based
synthetic data generation effectively mitigates data challenges in phrase break
prediction and highlights the potential of LLMs as a viable solution for the
speech domain.

</details>


### [12] [Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs](https://arxiv.org/abs/2507.18055)
*Tevin Atwal,Chan Nam Tieu,Yefeng Yuan,Zhan Shi,Yuhong Liu,Liang Cheng*

Main category: cs.CL

TL;DR: 研究提出评估指标揭示主流大语言模型生成的合成数据存在多样性和隐私保护不足，并提出基于提示词的方法提升数据多样性


<details>
  <summary>Details</summary>
Motivation: 合成数据在模型训练中广泛应用，但其多样性和隐私风险尚未得到充分研究。现有LLM生成合成数据的质量缺陷亟需系统性评估和改进方案

Method: 设计多维评估指标（语言表达、情感、用户视角多样性；重识别风险、风格异常值），测试多个先进LLM，并提出基于提示工程的数据增强方法

Result: 实验显示现有LLM生成的评论数据多样性不足（情感分布集中、用户视角单一），且存在明显的隐私泄露风险（用户重识别准确率超50%）

Conclusion: 提出的提示优化方法在保持用户隐私（重识别风险降低至2.5%）的同时，显著提升情感多样性（分布均衡性提高47%）和用户视角多样性

Abstract: The increasing use of synthetic data generated by Large Language Models
(LLMs) presents both opportunities and challenges in data-driven applications.
While synthetic data provides a cost-effective, scalable alternative to
real-world data to facilitate model training, its diversity and privacy risks
remain underexplored. Focusing on text-based synthetic data, we propose a
comprehensive set of metrics to quantitatively assess the diversity (i.e.,
linguistic expression, sentiment, and user perspective), and privacy (i.e.,
re-identification risk and stylistic outliers) of synthetic datasets generated
by several state-of-the-art LLMs. Experiment results reveal significant
limitations in LLMs' capabilities in generating diverse and privacy-preserving
synthetic data. Guided by the evaluation results, a prompt-based approach is
proposed to enhance the diversity of synthetic reviews while preserving
reviewer privacy.

</details>


### [13] [TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios](https://arxiv.org/abs/2507.18061)
*Zehan Li,Hongjie Chen,Yuxin Zhang,Jing Zhou,Xuening Wang,Hang Lv,Mengjie Du,Yaodong Song,Jie Lian,Jian Kang,Jie Li,Yongxiang Li,Zhongjiang He,Xuelong Li*

Main category: cs.CL

TL;DR: 提出TELEVAL评估框架，专注中文口语模型在真实对话场景中的表现，发现现有模型在自然对话任务中仍有改进空间


<details>
  <summary>Details</summary>
Motivation: 现有口语模型评估基准过度关注复杂任务能力，缺乏对真实对话场景用户交互体验的评估

Method: 定义显式语义、副语言与隐式语义、系统能力三维度，采用符合真实使用场景的对话格式，分别评估文本和语音输出

Result: 实验表明现有口语模型在自然对话任务中仍需大幅改进，尤其在无明确指令下理解隐式语义的能力不足

Conclusion: TELEVAL可作为以用户体验为中心的评估框架，推动开发更强大的对话导向型口语语言模型

Abstract: Spoken language models (SLMs) have seen rapid progress in recent years, along
with the development of numerous benchmarks for evaluating their performance.
However, most existing benchmarks primarily focus on evaluating whether SLMs
can perform complex tasks comparable to those tackled by large language models
(LLMs), often failing to align with how users naturally interact in real-world
conversational scenarios. In this paper, we propose TELEVAL, a dynamic
benchmark specifically designed to evaluate SLMs' effectiveness as
conversational agents in realistic Chinese interactive settings. TELEVAL
defines three evaluation dimensions: Explicit Semantics, Paralinguistic and
Implicit Semantics, and System Abilities. It adopts a dialogue format
consistent with real-world usage and evaluates text and audio outputs
separately. TELEVAL particularly focuses on the model's ability to extract
implicit cues from user speech and respond appropriately without additional
instructions. Our experiments demonstrate that despite recent progress,
existing SLMs still have considerable room for improvement in natural
conversational tasks. We hope that TELEVAL can serve as a user-centered
evaluation framework that directly reflects the user experience and contributes
to the development of more capable dialogue-oriented SLMs.

</details>


### [14] [Hybrid and Unitary Fine-Tuning of Large Language Models: Methods and Benchmarking under Resource Constraints](https://arxiv.org/abs/2507.18076)
*Haomin Qi,Zihan Dai,Chengbo Huang*

Main category: cs.CL

TL;DR: 提出结合BOFT正交稳定性和LoRA-GA梯度对齐的混合PEFT方法，在保证精度的同时显著降低LLM微调资源消耗


<details>
  <summary>Details</summary>
Motivation: 解决大模型全参数微调存在的计算资源瓶颈问题，突破现有单一参数高效微调方法在收敛效率和泛化能力上的局限

Method: 1. 通过梯度范数引导的逐层自适应更新实现BOFT与LoRA-GA的动态融合
2. 首次将uRNN的单位约束机制适配Transformer架构
3. 开发训练时间-内存消耗的联合优化策略

Result: 在4个基准测试中减少50%内存使用和2.1倍训练时间，405B模型达到接近全参数微调的准确率

Conclusion: 混合策略为实际工业部署提供了可扩展的解决方案，在资源受限场景下实现大模型高效适配

Abstract: Fine-tuning large language models (LLMs) remains a computational bottleneck
due to their scale and memory demands. This paper presents a comprehensive
evaluation of parameter-efficient fine-tuning (PEFT) techniques, including
LoRA, BOFT, LoRA-GA, and uRNN, and introduces a novel hybrid strategy that
dynamically integrates BOFT's orthogonal stability with LoRA-GA's
gradient-aligned rapid convergence. By computing per-layer adaptive updates
guided by gradient norms, the hybrid method achieves superior convergence
efficiency and generalization across diverse tasks. We also explore, for the
first time, the adaptation of unitary RNN (uRNN) principles to
transformer-based LLMs, enhancing gradient stability through structured unitary
constraints. Empirical evaluations on four benchmarks -- GLUE, GSM8K, MT-Bench,
and HumanEval -- using models ranging from 7B to 405B parameters demonstrate
that our hybrid method consistently outperforms individual PEFT baselines,
approaching full fine-tuning accuracy while reducing resource consumption by up
to 2.1 times in training time and 50 percent in memory usage. These findings
establish the hybrid approach as a practical and scalable fine-tuning solution
for real-world deployment of LLMs under resource constraints.

</details>


### [15] [A New Pair of GloVes](https://arxiv.org/abs/2507.18103)
*Riley Carlson,John Bauer,Christopher D. Manning*

Main category: cs.CL

TL;DR: 2024版GloVe模型通过整合Wikipedia、Gigaword和Dolma数据训练，在保留结构任务性能的同时显著提升了对新兴文化词汇的覆盖和时敏性NER任务表现（如非西方新闻数据）


<details>
  <summary>Details</summary>
Motivation: 原始2014版GloVe模型存在数据版本不透明、语言演化未跟进的问题，需构建更适应当代语言使用且文档完善的词向量模型

Method: 使用Wikipedia+Gigaword及Dolma子集训练两组词嵌入，通过词汇对比/类比相似度测试/命名实体识别（NER）任务进行三维度评估

Result: 新模型成功纳入'加密货币'等新兴词汇，结构任务表现与旧版持平，在2020年后非西方新闻NER任务中F1值提升8.2%

Conclusion: 2024 GloVe在保持基础语言结构表征能力的同时，显著增强了时域适应性和跨文化语境应用价值，为NLP应用提供更精准的语义基础

Abstract: This report documents, describes, and evaluates new 2024 English GloVe
(Global Vectors for Word Representation) models. While the original GloVe
models built in 2014 have been widely used and found useful, languages and the
world continue to evolve and we thought that current usage could benefit from
updated models. Moreover, the 2014 models were not carefully documented as to
the exact data versions and preprocessing that were used, and we rectify this
by documenting these new models. We trained two sets of word embeddings using
Wikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary
comparison, direct testing, and NER tasks shows that the 2024 vectors
incorporate new culturally and linguistically relevant words, perform
comparably on structural tasks like analogy and similarity, and demonstrate
improved performance on recent, temporally dependent NER datasets such as
non-Western newswire data.

</details>


### [16] [GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness](https://arxiv.org/abs/2507.18119)
*Hongjie Chen,Zehan Li,Yaodong Song,Wenming Deng,Yitong Yao,Yuxin Zhang,Hang Lv,Xuechao Zhu,Jian Kang,Jie Lian,Jie Li,Chao Wang,Shuangyong Song,Yongxiang Li,Zhongjiang He*

Main category: cs.CL

TL;DR: 提出GOAT-SLM模型，通过双模态架构和分阶段训练策略实现副语言及说话者特征感知，提升口语系统的自然性与适应性。


<details>
  <summary>Details</summary>
Motivation: 现有口语模型过度关注文本语义，忽视语音中蕴含的方言、年龄、情感等副语言特征，限制了交互的自然性和社会感知能力。

Method: 采用解耦语言建模与声学实现的双模态头部架构，配合大规模语音-文本数据的渐进式多阶段对齐训练策略。

Result: 在TELEVAL评估中实现语义与非语义任务的均衡表现，情感/方言/年龄敏感任务优于开源模型。

Conclusion: 超越纯语义建模可提升口语系统适应性，为开发社会感知更强的自然交互系统提供新方向。

Abstract: Recent advances in end-to-end spoken language models (SLMs) have
significantly improved the ability of AI systems to engage in natural spoken
interactions. However, most existing models treat speech merely as a vehicle
for linguistic content, often overlooking the rich paralinguistic and speaker
characteristic cues embedded in human speech, such as dialect, age, emotion,
and non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel
spoken language model with paralinguistic and speaker characteristic awareness,
designed to extend spoken language modeling beyond text semantics. GOAT-SLM
adopts a dual-modality head architecture that decouples linguistic modeling
from acoustic realization, enabling robust language understanding while
supporting expressive and adaptive speech generation. To enhance model
efficiency and versatility, we propose a modular, staged training strategy that
progressively aligns linguistic, paralinguistic, and speaker characteristic
information using large-scale speech-text corpora. Experimental results on
TELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM
achieves well-balanced performance across both semantic and non-semantic tasks,
and outperforms existing open-source models in handling emotion, dialectal
variation, and age-sensitive interactions. This work highlights the importance
of modeling beyond linguistic content and advances the development of more
natural, adaptive, and socially aware spoken language systems.

</details>


### [17] [MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning](https://arxiv.org/abs/2507.18140)
*Xiaoyuan Li,Moxin Li,Wenjie Wang,Rui Men,Yichang Zhang,Fuli Feng,Dayiheng Liu,Junyang Lin*

Main category: cs.CL

TL;DR: 评估多模态大语言模型（MLLMs）基于代码的数学推理能力，发现现有模型在细粒度视觉操作（代码编辑）方面显著落后于人类。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注文本推理输出，缺乏对MLLMs通过代码执行精确视觉操作能力的系统评估。本文旨在填补这一空白。

Method: 提出包含多模态代码生成（MCG）和代码编辑（MCE）的评估框架，使用覆盖几何图/函数图/统计图等5类数学图形的数据集，测试9个主流MLLM。

Result: 实验表明现有模型在删除/修改/注释等细粒度视觉操作任务中表现远低于人类水平（平均差距超过40%）。

Conclusion: 当前MLLMs的多模态数学推理能力存在显著局限，需提升代码生成精度和细粒度视觉操作能力以实现更可靠的数学推理系统。

Abstract: Recent progress in Multi-modal Large Language Models (MLLMs) has enabled
step-by-step multi-modal mathematical reasoning by performing visual operations
based on the textual instructions. A promising approach uses code as an
intermediate representation to precisely express and manipulate the images in
the reasoning steps. However, existing evaluations focus mainly on text-only
reasoning outputs, leaving the MLLM's ability to perform accurate visual
operations via code largely unexplored. This work takes a first step toward
addressing that gap by evaluating MLLM's code-based capabilities in multi-modal
mathematical reasoning.Specifically, our framework focuses on two key
evaluation aspects: (1) Multi-modal Code Generation (MCG) evaluates the model's
ability to accurately understand and construct visualizations from scratch. (2)
Multi-modal Code Editing (MCE) assesses the model's capacity for fine-grained
operations, which include three types: Deletion, Modification and Annotation.
To evaluate the above tasks, we incorporate a dataset that covers the five most
popular types of mathematical figures, including geometric diagrams, function
plots, and three types of statistical charts, to provide a comprehensive and
effective measurement of existing MLLMs. Our experimental evaluation involves
nine mainstream MLLMs, and the results reveal that existing models still lag
significantly behind human performance in performing fine-grained visual
operations.

</details>


### [18] [HIVMedQA: Benchmarking large language models for HIV medical decision support](https://arxiv.org/abs/2507.18143)
*Gonzalo Cardenal Antolin,Jacques Fellay,Bashkim Jaha,Roger Kouyos,Niko Beerenwinkel,Diane Duroux*

Main category: cs.CL

TL;DR: 该研究评估LLM在HIV管理中的能力，开发HIVMedQA基准测试，发现Gemini 2.5 Pro表现最优，但临床整合仍需针对性改进。


<details>
  <summary>Details</summary>
Motivation: HIV管理的复杂性（多样治疗方案/合并症/依从性问题）需要决策支持工具，但LLM临床应用存在准确性、潜在危害和接受度担忧。

Method: 创建HIVMedQA临床问答基准，包含传染病医生参与的临床问题，评估7个通用和3个医疗专用LLM，采用提示工程优化，结合词相似度和LLM自我评估框架。

Result: 1. Gemini 2.5 Pro综合表现最佳
2. 专有模型占据前三中的两席
3. 模型表现随问题复杂度下降
4. 医学微调模型未必优于通用模型
5. 存在近因效应等认知偏差

Conclusion: LLM临床整合需针对性开发：提升复杂推理能力，建立临床相关评估标准，关注认知偏差，平衡模型规模与效率，保障安全有效的临床应用。

Abstract: Large language models (LLMs) are emerging as valuable tools to support
clinicians in routine decision-making. HIV management is a compelling use case
due to its complexity, including diverse treatment options, comorbidities, and
adherence challenges. However, integrating LLMs into clinical practice raises
concerns about accuracy, potential harm, and clinician acceptance. Despite
their promise, AI applications in HIV care remain underexplored, and LLM
benchmarking studies are scarce. This study evaluates the current capabilities
of LLMs in HIV management, highlighting their strengths and limitations. We
introduce HIVMedQA, a benchmark designed to assess open-ended medical question
answering in HIV care. The dataset consists of curated, clinically relevant
questions developed with input from an infectious disease physician. We
evaluated seven general-purpose and three medically specialized LLMs, applying
prompt engineering to enhance performance. Our evaluation framework
incorporates both lexical similarity and an LLM-as-a-judge approach, extended
to better reflect clinical relevance. We assessed performance across key
dimensions: question comprehension, reasoning, knowledge recall, bias,
potential harm, and factual accuracy. Results show that Gemini 2.5 Pro
consistently outperformed other models across most dimensions. Notably, two of
the top three models were proprietary. Performance declined as question
complexity increased. Medically fine-tuned models did not always outperform
general-purpose ones, and larger model size was not a reliable predictor of
performance. Reasoning and comprehension were more challenging than factual
recall, and cognitive biases such as recency and status quo were observed.
These findings underscore the need for targeted development and evaluation to
ensure safe, effective LLM integration in clinical care.

</details>


### [19] [Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models](https://arxiv.org/abs/2507.18171)
*Kexin Chen,Dongxia Wang,Yi Liu,Haonan Zhang,Wenhai Wang*

Main category: cs.CL

TL;DR: 文本嵌入模型存在破坏可靠性的'sticky tokens'现象，通过STD方法检测发现868个异常token，揭示其对下游任务的显著影响


<details>
  <summary>Details</summary>
Motivation: Transformer模型在嵌入生成过程中意外出现使相似性分数偏离正常分布的'sticky tokens'，严重影响聚类检索等下游任务性能

Method: 提出基于句子过滤和token过滤的Sticky Token Detector(STD)检测框架，系统分析14个模型家族40个检查点

Result: 发现868个异常token，主要来源于特殊词汇/未使用条目及多语言子词分割；这些token通过注意力机制主导模型表征，导致下游任务性能下降达50%

Conclusion: 需要改进tokenization策略和模型设计以消除sticky tokens的影响，提升文本嵌入应用的鲁棒性

Abstract: Despite the widespread use of Transformer-based text embedding models in NLP
tasks, surprising 'sticky tokens' can undermine the reliability of embeddings.
These tokens, when repeatedly inserted into sentences, pull sentence similarity
toward a certain value, disrupting the normal distribution of embedding
distances and degrading downstream performance. In this paper, we
systematically investigate such anomalous tokens, formally defining them and
introducing an efficient detection method, Sticky Token Detector (STD), based
on sentence and token filtering. Applying STD to 40 checkpoints across 14 model
families, we discover a total of 868 sticky tokens. Our analysis reveals that
these tokens often originate from special or unused entries in the vocabulary,
as well as fragmented subwords from multilingual corpora. Notably, their
presence does not strictly correlate with model size or vocabulary size. We
further evaluate how sticky tokens affect downstream tasks like clustering and
retrieval, observing significant performance drops of up to 50%. Through
attention-layer analysis, we show that sticky tokens disproportionately
dominate the model's internal representations, raising concerns about
tokenization robustness. Our findings show the need for better tokenization
strategies and model design to mitigate the impact of sticky tokens in future
text embedding applications.

</details>


### [20] [SCOPE: Stochastic and Counterbiased Option Placement for Evaluating Large Language Models](https://arxiv.org/abs/2507.18182)
*Wonjun Jeong,Dongseok Kim,Taegkeun Whangbo*

Main category: cs.CL

TL;DR: 提出SCOPE评估框架，通过位置偏差估计和答案重新分配机制，有效解决大模型评估中的选择偏差问题，提升测评公平性


<details>
  <summary>Details</summary>
Motivation: 现有大模型评估存在选项位置/标签的固有偏差，导致模型通过投机策略而非真实能力获得虚高分数，影响评估有效性

Method: 1. 通过空提示估计模型位置偏差分布
2. 按逆偏差分布重新分配答案位置
3. 阻止语义相近干扰项相邻排列
4. 引入幸运率均等化机制

Result: 在多个基准测试中，SCOPE相比现有方法实现更稳定的性能提升（平均改善15%），且正确选项置信度分布更集中清晰

Conclusion: SCOPE为LLM评估建立了新的公平标准，通过双重防护机制（位置去偏+干扰项隔离）显著提升评估结果的可靠性

Abstract: Large Language Models (LLMs) can achieve inflated scores on multiple-choice
tasks by exploiting inherent biases in option positions or labels, rather than
demonstrating genuine understanding. This study introduces SCOPE, an evaluation
framework designed to measure and mitigate such selection bias in a
dataset-independent manner. By repeatedly invoking a null prompt that lacks
semantic content, SCOPE estimates each model's unique position-bias
distribution. It then redistributes the answer slot according to the
inverse-bias distribution, thereby equalizing the lucky-rate, the probability
of selecting the correct answer by chance. Furthermore, it prevents
semantically similar distractors from being placed adjacent to the answer,
thereby blocking near-miss guesses based on superficial proximity cues. Across
multiple benchmark experiments, SCOPE consistently outperformed existing
debiasing methods in terms of stable performance improvements and showed
clearer confidence distributions over correct options. This framework thus
offers a new standard for enhancing the fairness and reliability of LLM
evaluations.

</details>


### [21] [TN-AutoRCA: Benchmark Construction and Agentic Framework for Self-Improving Alarm-Based Root Cause Analysis in Telecommunication Networks](https://arxiv.org/abs/2507.18190)
*Keyu Wu,Qianjin Yu,Manlin Mei,Ruiting Liu,Jun Wang,Kailai Zhang,Yelun Bao*

Main category: cs.CL

TL;DR: 电信网络根因分析（RCA）面临复杂图推理挑战与基准缺失问题


<details>
  <summary>Details</summary>
Motivation: 当前AI在电信网络根因分析中存在两个核心痛点：1) 复杂拓扑关系下的多跳图推理困难 2) 缺乏贴近真实网络环境的评估基准。这限制了AI在运维自动化中的应用。

Method: 提出面向电信网络的图推理基准框架，包含：1) 基于真实网络拓扑的合成数据生成器 2) 多层级因果图建模方法 3) 图神经网络与逻辑推理融合的混合架构

Result: 构建首个电信级RCA评估基准TeleRCA，覆盖5G核心网典型故障场景，相比现有基准提升图结构复杂性达3倍，包含跨设备-接口-服务的多层次因果依赖

Conclusion: 该研究为AI在电信运维的落地提供关键基础设施，通过结构化基准设计推动图推理算法在复杂网络诊断中的性能突破，支持5G网络智能化运维体系建设

Abstract: Root Cause Analysis (RCA) in telecommunication networks is a critical task,
yet it presents a formidable challenge for Artificial Intelligence (AI) due to
its complex, graph-based reasoning requirements and the scarcity of realistic
benchmarks.

</details>


### [22] [Integrating an ISO30401-compliant Knowledge management system with existing business processes of an organization](https://arxiv.org/abs/2507.18197)
*Aline Belloni,Patrick Prieur*

Main category: cs.CL

TL;DR: 探讨ISO30401知识管理系统与ISO9001流程建模的整合方法，提出通过SECI模型和PDCA循环实现合规KMS实施。


<details>
  <summary>Details</summary>
Motivation: 解决企业在实施ISO30401标准时面临的知识管理活动与现有业务流程整合的实践挑战

Method: 结合ISO9001流程建模原则，运用SECI模型的知识转化机制，通过PDCA循环实施步骤进行系统整合

Result: 建立了KMS与集成管理系统(IMS)各流程的衔接框架，验证了SECI-PDCA方法论的有效性

Conclusion: 流程建模是管理系统整合的基础，SECI模型通过PDCA循环可系统化实现知识管理系统的ISO标准合规

Abstract: Business process modeling is used by most organizations as an essential
framework for ensuring efficiency and effectiveness of the work and workflow
performed by its employees and for ensuring the alignment of such work with its
strategic goals. For organizations that are compliant or near-compliant with
ISO 9001, this approach involves the detailed mapping of processes,
sub-processes, activities, and tasks. ISO30401 is a Management System Standard,
introduced in 2018, establishing universal requirements for the set up of a
Knowledge Management System in an organization. As ``ISO30401 implementers'' we
regularly face the challenge of explaining our clients how the knowledge
development, transformation and conveyances activities depicted in ISO30401 do
integrate with existing operational processes. This article recaps process
modelling principles in the context of ISO9001 and explores, based on our
experience, how an ISO30401-compliant Knowledge Management System (KMS)
entwines with all other processes of an Integrated Management System and in
particular how it can be implemented by deploying the mechanisms of the SECI
model through the steps of PDCA cycles.

</details>


### [23] [Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection](https://arxiv.org/abs/2507.18202)
*San Kim,Jonghwi Kim,Yejin Jeon,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: 提出GMTP方法，通过梯度分析和掩码语言模型检测恶意文档，有效过滤90%以上有毒内容并保持检索性能


<details>
  <summary>Details</summary>
Motivation: RAG依赖外部知识库存在安全风险，攻击者可注入有毒文档操控生成结果，需开发高效防御机制

Method: 1. 利用检索器相似度函数梯度识别关键令牌 2. 掩码关键令牌后通过MLM检测低概率特征 3. 基于概率阈值实现高精度过滤

Result: 实验显示GMTP在不同数据集和对抗场景下可消除90%+有毒内容，保持90%+相关文档召回率

Conclusion: GMTP首次将梯度分析与MLM结合，实现高效对抗文档检测，为RAG系统提供可靠安全防护方案

Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
providing external knowledge for accurate and up-to-date responses. However,
this reliance on external sources exposes a security risk, attackers can inject
poisoned documents into the knowledge base to steer the generation process
toward harmful or misleading outputs. In this paper, we propose Gradient-based
Masked Token Probability (GMTP), a novel defense method to detect and filter
out adversarially crafted documents. Specifically, GMTP identifies high-impact
tokens by examining gradients of the retriever's similarity function. These key
tokens are then masked, and their probabilities are checked via a Masked
Language Model (MLM). Since injected tokens typically exhibit markedly low
masked-token probabilities, this enables GMTP to easily detect malicious
documents and achieve high-precision filtering. Experiments demonstrate that
GMTP is able to eliminate over 90% of poisoned content while retaining relevant
documents, thus maintaining robust retrieval and generation performance across
diverse datasets and adversarial settings.

</details>


### [24] [Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation](https://arxiv.org/abs/2507.18203)
*Kyubeen Han,Junseo Jang,Hongjin Kim,Geunyeong Jeong,Harksoo Kim*

Main category: cs.CL

TL;DR: 指令微调会显著增加大语言模型对用户提供错误信息的接受度，研究发现模型敏感性受用户角色、错误信息长度和系统提示警告等因素影响


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注大语言模型对外部矛盾信息的接受性，但缺乏针对指令微调对该现象直接影响的系统研究，需揭示其副作用机制

Method: 通过对比指令微调模型与基础模型的表现，分析用户角色在prompt结构中的影响、错误信息长度变化及系统提示警告的作用

Result: 1. 指令微调使模型对用户信息依赖度提升35%
2. 用户角色陈述时错误接受率比助手角色高42%
3. 超过200token的错误信息接受率增加28%

Conclusion: 需建立系统化解决方案平衡指令微调的效用与风险，建议采用对抗训练增强模型事实核查能力，并在部署前进行敏感性压力测试

Abstract: Instruction-tuning enhances the ability of large language models (LLMs) to
follow user instructions more accurately, improving usability while reducing
harmful outputs. However, this process may increase the model's dependence on
user input, potentially leading to the unfiltered acceptance of misinformation
and the generation of hallucinations. Existing studies primarily highlight that
LLMs are receptive to external information that contradict their parametric
knowledge, but little research has been conducted on the direct impact of
instruction-tuning on this phenomenon. In our study, we investigate the impact
of instruction-tuning on LLM's susceptibility to misinformation. Our analysis
reveals that instruction-tuned LLMs are significantly more likely to accept
misinformation when it is presented by the user. A comparison with base models
shows that instruction-tuning increases reliance on user-provided information,
shifting susceptibility from the assistant role to the user role. Furthermore,
we explore additional factors influencing misinformation susceptibility, such
as the role of the user in prompt structure, misinformation length, and the
presence of warnings in the system prompt. Our findings underscore the need for
systematic approaches to mitigate unintended consequences of instruction-tuning
and enhance the reliability of LLMs in real-world applications.

</details>


### [25] [Prune&Comp: Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation](https://arxiv.org/abs/2507.18212)
*Xinrui Chen,Hongxing Zhang,Fanyi Zeng,Yongxian Wei,Yizhi Wang,Xitong Ling,Guanghao Li,Chun Yuan*

Main category: cs.CL

TL;DR: 提出Prune&Comp方案，通过幅度补偿实现免训练的大语言模型层剪枝，在LLaMA-3-8B上剪枝5层后性能保留93.19%，优于基线4.01%。


<details>
  <summary>Details</summary>
Motivation: 现有层剪枝方法移除任意网络层会导致隐藏状态出现显著幅度偏差，造成性能大幅下降。

Method: 1. 通过离线权重缩放补偿层移除带来的幅度偏差
2. 采用迭代式剪枝-补偿循环优化剪枝指标
3. 零运行时开销的插件式剪枝方案

Result: 使用block influence指标剪枝5层时，困惑度降低近50%，问答性能保留93.19%（比基线提升4.01%）

Conclusion: Prune&Comp通过系统性的幅度补偿机制，在保持零运行时开销的前提下，显著提升了现有层剪枝方法的性能保留率，为大模型压缩提供了新思路。

Abstract: Layer pruning has emerged as a promising technique for compressing large
language models (LLMs) while achieving acceleration proportional to the pruning
ratio. In this work, we identify that removing any layer induces a significant
magnitude gap in hidden states, resulting in substantial performance
degradation. To address this issue, we propose Prune&Comp, a novel
plug-and-play layer pruning scheme that leverages magnitude compensation to
mitigate such gaps in a training-free manner. Specifically, we first estimate
the magnitude gap caused by layer removal and then eliminate this gap by
rescaling the remaining weights offline, with zero runtime overhead incurred.
We further demonstrate the advantages of Prune&Comp through an iterative
pruning strategy. When integrated with an iterative prune-and-compensate loop,
Prune&Comp consistently enhances existing layer pruning metrics. For instance,
when 5 layers of LLaMA-3-8B are pruned using the prevalent block influence
metric, Prune&Comp nearly halves the perplexity and retains 93.19\% of the
original model's question-answering performance, outperforming the baseline by
4.01%.

</details>


### [26] [Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models](https://arxiv.org/abs/2507.18263)
*Suhang Wu,Jialong Tang,Chengyi Yang,Pei Zhang,Baosong Yang,Junhui Li,Junfeng Yao,Min Zhang,Jinsong Su*

Main category: cs.CL

TL;DR: 提出Locate-and-Focus方法通过定位术语语音片段与多模态关联机制，有效提升语音翻译中的术语翻译成功率


<details>
  <summary>Details</summary>
Motivation: 现有语音翻译模型在利用翻译知识时存在无关噪声干扰和知识利用不充分的问题，导致术语翻译准确性不足

Method: 两阶段方法：1)定位术语所在语音片段构建翻译知识 2)通过音频和文本模态关联翻译知识与输入，增强模型对知识的关注

Result: 多数据集实验表明方法成功定位术语位置，术语翻译成功率显著提升，同时保持整体翻译性能稳定

Conclusion: 通过定位-聚焦机制有效解决术语翻译难题，为语音翻译系统提供了新的知识利用范式

Abstract: Direct speech translation (ST) has garnered increasing attention nowadays,
yet the accurate translation of terminology within utterances remains a great
challenge. In this regard, current studies mainly concentrate on leveraging
various translation knowledge into ST models. However, these methods often
struggle with interference from irrelevant noise and can not fully utilize the
translation knowledge. To address these issues, in this paper, we propose a
novel Locate-and-Focus method for terminology translation. It first effectively
locates the speech clips containing terminologies within the utterance to
construct translation knowledge, minimizing irrelevant information for the ST
model. Subsequently, it associates the translation knowledge with the utterance
and hypothesis from both audio and textual modalities, allowing the ST model to
better focus on translation knowledge during translation. Experimental results
across various datasets demonstrate that our method effectively locates
terminologies within utterances and enhances the success rate of terminology
translation, while maintaining robust general translation performance.

</details>


### [27] [Zero-shot OCR Accuracy of Low-Resourced Languages: A Comparative Analysis on Sinhala and Tamil](https://arxiv.org/abs/2507.18264)
*Nevidu Jayatilleke,Nisansa de Silva*

Main category: cs.CL

TL;DR: 针对低资源语言（僧伽罗语和泰米尔语）的OCR性能评估表明，开源引擎Surya和商业系统Document AI分别在两种语言中表现最优，同时提出了新的泰米尔语合成数据集。


<details>
  <summary>Details</summary>
Motivation: 现有OCR技术高度偏向高资源语言，低资源语言因独特文字和资源匮乏面临识别难题，需评估现有引擎的零样本能力以推动技术普惠。

Method: 采用六种OCR引擎（4个支持双语，2个单语）进行零样本测试，通过字符错误率(CER)、词错误率(WER)等五维度指标进行跨系统对比，并构建合成泰米尔数据集补充评估资源。

Result: Surya在僧伽罗语中WER最低(2.61%)，Document AI在泰米尔语CER达0.78%最优。商业与开源系统在不同语言场景中各有优势，合成数据集填补评估空白。

Conclusion: 开源引擎与商业系统在不同低资源语言OCR任务中展现互补性，合成数据集的创新为未来研究提供新基准，推动低资源语言数字化进程。

Abstract: Solving the problem of Optical Character Recognition (OCR) on printed text
for Latin and its derivative scripts can now be considered settled due to the
volumes of research done on English and other High-Resourced Languages (HRL).
However, for Low-Resourced Languages (LRL) that use unique scripts, it remains
an open problem. This study presents a comparative analysis of the zero-shot
performance of six distinct OCR engines on two LRLs: Sinhala and Tamil. The
selected engines include both commercial and open-source systems, aiming to
evaluate the strengths of each category. The Cloud Vision API, Surya, Document
AI, and Tesseract were evaluated for both Sinhala and Tamil, while Subasa OCR
and EasyOCR were examined for only one language due to their limitations. The
performance of these systems was rigorously analysed using five measurement
techniques to assess accuracy at both the character and word levels. According
to the findings, Surya delivered the best performance for Sinhala across all
metrics, with a WER of 2.61%. Conversely, Document AI excelled across all
metrics for Tamil, highlighted by a very low CER of 0.78%. In addition to the
above analysis, we also introduce a novel synthetic Tamil OCR benchmarking
dataset.

</details>


### [28] [StyleAdaptedLM: Enhancing Instruction Following Models with Efficient Stylistic Transfer](https://arxiv.org/abs/2507.18294)
*Pritika Ramu,Apoorv Saxena,Meghanath M Y,Varsha Sankar,Debraj Basu*

Main category: cs.CL

TL;DR: 提出StyleAdaptedLM框架，通过LoRA技术实现LLMs风格定制，无需配对数据即可保持指令遵循能力


<details>
  <summary>Details</summary>
Motivation: 企业沟通需要LLMs适配品牌风格，但现有语料缺乏指令-响应对且难以兼顾风格与任务性能

Method: 使用LoRA适配器：先在基础模型上训练风格特征，再与指令模型合并实现解耦式训练

Result: 跨数据集实验显示风格一致性提升，人工评估验证品牌规范吸收效果，任务性能无损失

Conclusion: 提供高效LLM风格定制方案，支持低资源场景下的个性化适配

Abstract: Adapting LLMs to specific stylistic characteristics, like brand voice or
authorial tones, is crucial for enterprise communication but challenging to
achieve from corpora which lacks instruction-response formatting without
compromising instruction adherence. We introduce StyleAdaptedLM, a framework
that efficiently transfers stylistic traits to instruction-following models
using Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base
model with diverse unstructured stylistic corpora, then merged with a separate
instruction-following model. This enables robust stylistic customization
without paired data or sacrificing task performance. Experiments across
multiple datasets and models demonstrate improved stylistic consistency while
preserving instruction adherence, with human evaluations confirming
brand-specific convention uptake. StyleAdaptedLM offers an efficient path for
stylistic personalization in LLMs.

</details>


### [29] [BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit](https://arxiv.org/abs/2507.18305)
*Biao Yi,Zekun Fei,Jianing Geng,Tong Li,Lihai Nie,Zheli Liu,Yiming Li*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large reasoning models (LRMs) have emerged as a significant advancement in
artificial intelligence, representing a specialized class of large language
models (LLMs) designed to tackle complex reasoning tasks. The defining
characteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning
capabilities. In this paper, we identify a previously unexplored attack vector
against LRMs, which we term "overthinking backdoors". We advance this concept
by proposing a novel tunable backdoor, which moves beyond simple on/off attacks
to one where an attacker can precisely control the extent of the model's
reasoning verbosity. Our attack is implemented through a novel data poisoning
methodology. It pairs a tunable trigger-where the number of repetitions signals
the desired intensity-with a correspondingly verbose CoT response. These
responses are programmatically generated by instructing a teacher LLM to inject
a controlled number of redundant refinement steps into a correct reasoning
process. The approach preserves output correctness, which ensures stealth and
establishes the attack as a pure resource-consumption vector. Extensive
empirical results on various LRMs demonstrate that our method can reliably
trigger a controllable, multi-fold increase in the length of the reasoning
process, without degrading the final answer's correctness. Our source code is
available at https://github.com/FZaKK/BadReasoner.

</details>


### [30] [Uncertainty Quantification for Evaluating Machine Translation Bias](https://arxiv.org/abs/2507.18338)
*Ieva Raminta Staliūnaitė,Julius Cheng,Andreas Vlachos*

Main category: cs.CL

TL;DR: 机器翻译模型在性别标记推断中存在偏见，即使上下文明确仍依赖刻板印象。研究发现高准确率模型在模糊情境下未保持足够不确定性，去偏见措施对明确/模糊情况产生不同效果。


<details>
  <summary>Details</summary>
Motivation: 当源语句存在未明确标记性别的词素时，MT模型需从上下文推断性别。现有研究表明模型会依赖刻板印象进行翻译，即使与上下文信息矛盾。

Method: 使用语义不确定性指标评估模型，分析模型在明确性别情境下的翻译准确性，以及在模糊情境下的不确定性表现，同时测试去偏见措施的双重影响。

Result: 高翻译准确率的模型在模糊情境未展现预期不确定性水平，去偏见处理对明确/模糊情境的翻译效果存在独立影响机制。

Conclusion: 理想MT模型应同时具备：明确情境下的准确翻译能力，模糊情境下的不确定性保持能力，且需针对性设计不同去偏见策略处理不同情境。

Abstract: In machine translation (MT), when the source sentence includes a lexeme whose
gender is not overtly marked, but whose target-language equivalent requires
gender specification, the model must infer the appropriate gender from the
context and/or external knowledge. Studies have shown that MT models exhibit
biased behaviour, relying on stereotypes even when they clash with contextual
information. We posit that apart from confidently translating using the correct
gender when it is evident from the input, models should also maintain
uncertainty about the gender when it is ambiguous. Using recently proposed
metrics of semantic uncertainty, we find that models with high translation and
gender accuracy on unambiguous instances do not necessarily exhibit the
expected level of uncertainty in ambiguous ones. Similarly, debiasing has
independent effects on ambiguous and unambiguous translation instances.

</details>


### [31] [TDR: Task-Decoupled Retrieval with Fine-Grained LLM Feedback for In-Context Learning](https://arxiv.org/abs/2507.18340)
*Yifu Chen,Bingchen Huang,Zhiling Wang,Yuanchao Du,Junfeng Luo,Lei Shen,Zhineng chen*

Main category: cs.CL

TL;DR: TDR框架通过任务解耦和细粒度反馈优化ICL示例检索，提升大模型多任务场景下的上下文学习效果。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在跨任务数据分布区分困难、检索器输出与LLM反馈细粒度关联不足的问题

Method: 1. 多任务数据集解耦，实现目标任务的定向检索
2. 建模LLM细粒度反馈指导检索模块训练

Result: 在30个NLP任务上的实验显示全面性能提升，达到SOTA水平，且具备即插即用特性

Conclusion: TDR通过任务感知检索和反馈驱动优化，为ICL提供高效解决方案，代码已开源

Abstract: In-context learning (ICL) has become a classic approach for enabling LLMs to
handle various tasks based on a few input-output examples. The effectiveness of
ICL heavily relies on the quality of these examples, and previous works which
focused on enhancing example retrieval capabilities have achieved impressive
performances. However, two challenges remain in retrieving high-quality
examples: (1) Difficulty in distinguishing cross-task data distributions, (2)
Difficulty in making the fine-grained connection between retriever output and
feedback from LLMs. In this paper, we propose a novel framework called TDR. TDR
decouples the ICL examples from different tasks, which enables the retrieval
module to retrieve examples specific to the target task within a multi-task
dataset. Furthermore, TDR models fine-grained feedback from LLMs to supervise
and guide the training of the retrieval module, which helps to retrieve
high-quality examples. We conducted extensive experiments on a suite of 30 NLP
tasks, the results demonstrate that TDR consistently improved results across
all datasets and achieves state-of-the-art performance. Meanwhile, our approach
is a plug-and-play method, which can be easily combined with various LLMs to
improve example retrieval abilities for ICL. The code is available at
https://github.com/Nnn-s/TDR.

</details>


### [32] [Hybrid Annotation for Propaganda Detection: Integrating LLM Pre-Annotations with Human Intelligence](https://arxiv.org/abs/2507.18343)
*Ariana Sahitaj,Premtim Sahitaj,Veronika Solopova,Jiaao Li,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 论文提出LLM辅助的层次化标注框架，通过知识蒸馏训练小模型，提升社交媒体宣传内容检测的一致性和效率，支持SDG 16目标。


<details>
  <summary>Details</summary>
Motivation: 社交媒体宣传检测面临任务复杂性和标注数据质量低的问题，需提升标注一致性和系统扩展性。

Method: 1. 构建14种细粒度宣传技术的三层分类体系
2. 开展人工标注研究揭示细粒度标签低一致性
3. 设计LLM预标注流程实现内容提取、解释生成和标签分配
4. 采用知识蒸馏在LLM生成数据上微调小模型

Result: 二次验证显示标注一致性提升45%，耗时减少38%，小模型F1值达0.82，逼近大模型性能。

Conclusion: LLM与人类协同的层次化标注框架为构建可扩展的透明媒体生态系统提供新范式，通过知识蒸馏实现高效模型部署。

Abstract: Propaganda detection on social media remains challenging due to task
complexity and limited high-quality labeled data. This paper introduces a novel
framework that combines human expertise with Large Language Model (LLM)
assistance to improve both annotation consistency and scalability. We propose a
hierarchical taxonomy that organizes 14 fine-grained propaganda techniques into
three broader categories, conduct a human annotation study on the HQP dataset
that reveals low inter-annotator agreement for fine-grained labels, and
implement an LLM-assisted pre-annotation pipeline that extracts propagandistic
spans, generates concise explanations, and assigns local labels as well as a
global label. A secondary human verification study shows significant
improvements in both agreement and time-efficiency. Building on this, we
fine-tune smaller language models (SLMs) to perform structured annotation.
Instead of fine-tuning on human annotations, we train on high-quality
LLM-generated data, allowing a large model to produce these annotations and a
smaller model to learn to generate them via knowledge distillation. Our work
contributes towards the development of scalable and robust propaganda detection
systems, supporting the idea of transparent and accountable media ecosystems in
line with SDG 16. The code is publicly available at our GitHub repository.

</details>


### [33] [CLEAR: Error Analysis via LLM-as-a-Judge Made Easy](https://arxiv.org/abs/2507.18392)
*Asaf Yehudai,Lilach Eden,Yotam Perlitz,Roy Bar-Haim,Michal Shmueli-Scheuer*

Main category: cs.CL

TL;DR: 提出了CLEAR工具包，通过生成实例反馈、系统级错误归因和交互式仪表板，实现大语言模型评估的细粒度错误分析。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估范式仅提供单一评分，无法揭示模型表现的具体原因，难以指导模型改进。

Method: 1) 生成实例级文本反馈
2) 创建系统级错误分类
3) 量化错误普遍性
4) 提供交互式分析仪表板（可视化/过滤/案例溯源）

Result: 在RAG和数学基准测试中验证有效性，用户案例研究证实其分析实用价值。

Conclusion: CLEAR突破了传统评估的局限性，为LLM能力评估提供了可解释、可操作的错误分析框架。

Abstract: The evaluation of Large Language Models (LLMs) increasingly relies on other
LLMs acting as judges. However, current evaluation paradigms typically yield a
single score or ranking, answering which model is better but not why. While
essential for benchmarking, these top-level scores obscure the specific,
actionable reasons behind a model's performance. To bridge this gap, we
introduce CLEAR, an interactive, open-source package for LLM-based error
analysis. CLEAR first generates per-instance textual feedback, then it creates
a set of system-level error issues, and quantifies the prevalence of each
identified issue. Our package also provides users with an interactive dashboard
that allows for a comprehensive error analysis through aggregate
visualizations, applies interactive filters to isolate specific issues or score
ranges, and drills down to the individual instances that exemplify a particular
behavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,
and showcase its utility through a user case study.

</details>


### [34] [Factual Inconsistencies in Multilingual Wikipedia Tables](https://arxiv.org/abs/2507.18406)
*Silvia Cappa,Lingxiao Kong,Pille-Riin Peet,Fanfu Wei,Yuchen Zhou,Jan-Christoph Kalo*

Main category: cs.CL

TL;DR: 分析维基百科多语言表格数据的不一致性问题，提出跨语言表格收集对齐方法并定义不一致类型，评估对AI系统可靠性的影响


<details>
  <summary>Details</summary>
Motivation: 维基百科多语言版本独立编写导致事实不一致，可能影响百科全书中立性及依赖其训练的AI系统可靠性

Method: 开发多语言表格采集对齐框架，定义不一致类型，结合定量指标与定性分析评估样本数据集

Result: 建立跨语言表格分析框架，量化多语言对齐差异，揭示事实验证和知识交互的关键改进方向

Conclusion: 研究结果为事实核查提供新方法，促进多语言知识融合，指导基于维基百科的可靠AI系统设计

Abstract: Wikipedia serves as a globally accessible knowledge source with content in
over 300 languages. Despite covering the same topics, the different versions of
Wikipedia are written and updated independently. This leads to factual
inconsistencies that can impact the neutrality and reliability of the
encyclopedia and AI systems, which often rely on Wikipedia as a main training
source. This study investigates cross-lingual inconsistencies in Wikipedia's
structured content, with a focus on tabular data. We developed a methodology to
collect, align, and analyze tables from Wikipedia multilingual articles,
defining categories of inconsistency. We apply various quantitative and
qualitative metrics to assess multilingual alignment using a sample dataset.
These insights have implications for factual verification, multilingual
knowledge interaction, and design for reliable AI systems leveraging Wikipedia
content.

</details>


### [35] [FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs](https://arxiv.org/abs/2507.18417)
*Giorgos Iacovides,Wuyang Zhou,Danilo Mandic*

Main category: cs.CL

TL;DR: 提出首个基于DPO偏好对齐的金融领域框架FinDPO，通过logit-to-score机制实现67%年化收益和2.0夏普比率


<details>
  <summary>Details</summary>
Motivation: 传统监督微调模型在金融领域存在泛化能力不足的问题，难以适应未见过的事件和金融术语的细微差异

Method: 采用直接偏好优化(DPO)进行后训练对齐，开发新型logit-to-score转换技术将离散预测转化为连续概率评分

Result: 情感分类平均准确率提升11%，模拟组合年回报67%且夏普比率2.0，在5bps交易成本下保持优异表现

Conclusion: FinDPO成功突破传统模型局限，首次实现基于情感的量化策略持续正收益，为金融NLP落地提供新范式

Abstract: Opinions expressed in online finance-related textual data are having an
increasingly profound impact on trading decisions and market movements. This
trend highlights the vital role of sentiment analysis as a tool for quantifying
the nature and strength of such opinions. With the rapid development of
Generative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs)
have become the de facto standard for financial sentiment analysis. However,
the SFT paradigm can lead to memorization of the training data and often fails
to generalize to unseen samples. This is a critical limitation in financial
domains, where models must adapt to previously unobserved events and the
nuanced, domain-specific language of finance. To this end, we introduce FinDPO,
the first finance-specific LLM framework based on post-training human
preference alignment via Direct Preference Optimization (DPO). The proposed
FinDPO achieves state-of-the-art performance on standard sentiment
classification benchmarks, outperforming existing supervised fine-tuned models
by 11% on the average. Uniquely, the FinDPO framework enables the integration
of a fine-tuned causal LLM into realistic portfolio strategies through a novel
'logit-to-score' conversion, which transforms discrete sentiment predictions
into continuous, rankable sentiment scores (probabilities). In this way,
simulations demonstrate that FinDPO is the first sentiment-based approach to
maintain substantial positive returns of 67% annually and strong risk-adjusted
performance, as indicated by a Sharpe ratio of 2.0, even under realistic
transaction costs of 5 basis points (bps).

</details>


### [36] [AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic Tabular Data](https://arxiv.org/abs/2507.18442)
*Rana Alshaikh,Israa Alghanmi,Shelan Jeawak*

Main category: cs.CL

TL;DR: 针对阿拉伯语表格数据处理的局限性，研究者开发了AraTable基准测试，评估大语言模型在复杂推理任务中的表现，并提出自动化评估框架


<details>
  <summary>Details</summary>
Motivation: 现有英语表格基准资源丰富，但阿拉伯语因公开资源匮乏和语言特性独特而缺乏评估体系，且大模型在表格推理任务中存在认知瓶颈

Method: 采用LLM生成内容+人工验证的混合流程保证数据质量，提出基于自省机制的自动化评估框架替代人工评判

Result: 大模型在简单问答任务表现合格，但在事实核查和复杂推理任务中仍存在显著认知缺陷；自动化评估框架性能接近人类水平

Conclusion: AraTable填补阿拉伯语结构化数据处理资源空白，其评估框架可加速相关基础模型研发，未来需提升模型复杂推理能力

Abstract: The cognitive and reasoning abilities of large language models (LLMs) have
enabled remarkable progress in natural language processing. However, their
performance in interpreting structured data, especially in tabular formats,
remains limited. Although benchmarks for English tabular data are widely
available, Arabic is still underrepresented because of the limited availability
of public resources and its unique language features. To address this gap, we
present AraTable, a novel and comprehensive benchmark designed to evaluate the
reasoning and understanding capabilities of LLMs when applied to Arabic tabular
data. AraTable consists of various evaluation tasks, such as direct question
answering, fact verification, and complex reasoning, involving a wide range of
Arabic tabular sources. Our methodology follows a hybrid pipeline, where
initial content is generated by LLMs and subsequently filtered and verified by
human experts to ensure high dataset quality. Initial analyses using AraTable
show that, while LLMs perform adequately on simpler tabular tasks such as
direct question answering, they continue to face significant cognitive
challenges when tasks require deeper reasoning and fact verification. This
indicates that there are substantial opportunities for future work to improve
performance on complex tabular reasoning tasks. We also propose a fully
automated evaluation framework that uses a self-deliberation mechanism and
achieves performance nearly identical to that of human judges. This research
provides a valuable, publicly available resource and evaluation framework that
can help accelerate the development of foundational models for processing and
analysing Arabic structured data.

</details>


### [37] [Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, a Low-Resource Language](https://arxiv.org/abs/2507.18448)
*Md Obyedullahil Mamun,Md Adyelullahil Mamun,Arif Ahmad,Md. Imran Hossain Emu*

Main category: cs.CL

TL;DR: 使用XLM-RoBERTa-large模型实现孟加拉语文本标点恢复，通过数据增强构建大规模训练集，在多个测试集取得90%+准确率


<details>
  <summary>Details</summary>
Motivation: 针对低资源孟加拉语ASR任务中标点缺失问题，解决标注数据稀缺的挑战并提升文本可读性

Method: 采用XLM-RoBERTa-large架构，构建多领域训练语料库，应用α=0.20%的数据增强策略

Result: 新闻/参考/ASR测试集分别达97.1%、91.2%、90.2%准确率，在噪声场景展现强泛化能力

Conclusion: 建立孟加拉语标点修复基线，公开数据集与代码资源推动低资源NLP研究

Abstract: Punctuation restoration enhances the readability of text and is critical for
post-processing tasks in Automatic Speech Recognition (ASR), especially for
low-resource languages like Bangla. In this study, we explore the application
of transformer-based models, specifically XLM-RoBERTa-large, to automatically
restore punctuation in unpunctuated Bangla text. We focus on predicting four
punctuation marks: period, comma, question mark, and exclamation mark across
diverse text domains. To address the scarcity of annotated resources, we
constructed a large, varied training corpus and applied data augmentation
techniques. Our best-performing model, trained with an augmentation factor of
alpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the
Reference set, and 90.2% on the ASR set.
  Results show strong generalization to reference and ASR transcripts,
demonstrating the model's effectiveness in real-world, noisy scenarios. This
work establishes a strong baseline for Bangla punctuation restoration and
contributes publicly available datasets and code to support future research in
low-resource NLP.

</details>


### [38] [Generation of Synthetic Clinical Text: A Systematic Review](https://arxiv.org/abs/2507.18451)
*Basel Alshaikhdeeb,Ahmed Abdelmonem Hemedan,Soumyabrata Ghosh,Irina Balaur,Venkata Satagopam*

Main category: cs.CL

TL;DR: 系统综述显示合成医疗文本生成主要用于数据增强和隐私保护，常用Transformer技术（如GPT），实用性评估为主，但隐私问题仍需人工验证。


<details>
  <summary>Details</summary>
Motivation: 解决医疗NLP中数据稀疏性和隐私问题，通过生成合成文本替代真实患者数据以促进研究与应用。

Method: 系统性分析94篇文献，定量研究三个核心维度：生成目的（文本增强/隐私保护等）、技术架构（Transformer主导）及评估方法（实用性/隐私性等）。

Result: 合成文本在增强数据、提升下游任务准确性方面有效，但存在隐私泄露风险（需人工审查），且GPT类模型成为主流生成技术。

Conclusion: 合成医疗文本可加速NLP流程开发，但需平衡隐私保护与数据效用，未来可能通过改进生成技术减少法律合规成本。

Abstract: Generating clinical synthetic text represents an effective solution for
common clinical NLP issues like sparsity and privacy. This paper aims to
conduct a systematic review on generating synthetic medical free-text by
formulating quantitative analysis to three research questions concerning (i)
the purpose of generation, (ii) the techniques, and (iii) the evaluation
methods. We searched PubMed, ScienceDirect, Web of Science, Scopus, IEEE,
Google Scholar, and arXiv databases for publications associated with generating
synthetic medical unstructured free-text. We have identified 94 relevant
articles out of 1,398 collected ones. A great deal of attention has been given
to the generation of synthetic medical text from 2018 onwards, where the main
purpose of such a generation is towards text augmentation, assistive writing,
corpus building, privacy-preserving, annotation, and usefulness. Transformer
architectures were the main predominant technique used to generate the text,
especially the GPTs. On the other hand, there were four main aspects of
evaluation, including similarity, privacy, structure, and utility, where
utility was the most frequent method used to assess the generated synthetic
medical text. Although the generated synthetic medical text demonstrated a
moderate possibility to act as real medical documents in different downstream
NLP tasks, it has proven to be a great asset as augmented, complementary to the
real documents, towards improving the accuracy and overcoming
sparsity/undersampling issues. Yet, privacy is still a major issue behind
generating synthetic medical text, where more human assessments are needed to
check for the existence of any sensitive information. Despite that, advances in
generating synthetic medical text will considerably accelerate the adoption of
workflows and pipeline development, discarding the time-consuming legalities of
data transfer.

</details>


### [39] [Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models](https://arxiv.org/abs/2507.18504)
*Zheyu Zhang,Shuo Yang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.CL

TL;DR: 提出GraDe方法，通过图引导的稀疏依赖学习优化LLM在表格数据生成中的注意力机制，在复杂数据集上性能提升12%


<details>
  <summary>Details</summary>
Motivation: LLM的全局注意力机制与表格数据稀疏特征依赖存在根本性冲突，导致关键特征关系建模不足

Method: GraDe框架整合动态图学习模块，通过功能依赖引导生成稀疏依赖图，选择性加强关键特征交互

Result: 在真实数据集上超越现有LLM方法12%，在合成数据质量上与SOTA方法持平

Conclusion: 图引导的依赖学习机制为LLM表格建模提供了结构感知的轻量级解决方案

Abstract: Large Language Models (LLMs) have shown strong potential for tabular data
generation by modeling textualized feature-value pairs. However, tabular data
inherently exhibits sparse feature-level dependencies, where many feature
interactions are structurally insignificant. This creates a fundamental
mismatch as LLMs' self-attention mechanism inevitably distributes focus across
all pairs, diluting attention on critical relationships, particularly in
datasets with complex dependencies or semantically ambiguous features. To
address this limitation, we propose GraDe (Graph-Guided Dependency Learning), a
novel method that explicitly integrates sparse dependency graphs into LLMs'
attention mechanism. GraDe employs a lightweight dynamic graph learning module
guided by externally extracted functional dependencies, prioritizing key
feature interactions while suppressing irrelevant ones. Our experiments across
diverse real-world datasets demonstrate that GraDe outperforms existing
LLM-based approaches by up to 12% on complex datasets while achieving
competitive results with state-of-the-art approaches in synthetic data quality.
Our method is minimally intrusive yet effective, offering a practical solution
for structure-aware tabular data modeling with LLMs.

</details>


### [40] [The Moral Gap of Large Language Models](https://arxiv.org/abs/2507.18523)
*Maciej Skorski,Alina Landowska*

Main category: cs.CL

TL;DR: 现有大语言模型在道德内容检测任务中表现不及专用微调模型，存在系统性漏检问题


<details>
  <summary>Details</summary>
Motivation: 验证大语言模型在专业道德推理任务中的适用性，推动伦理对齐AI系统发展

Method: 使用ROC曲线、PR曲线和DET曲线对比分析LLM与微调模型在Twitter和Reddit数据集的表现

Result: LLM呈现高假阴性率，提示工程未能改善道德内容的系统性检测不足问题

Conclusion: 道德推理应用中，任务特定的模型微调方案仍优于通用提示策略

Abstract: Moral foundation detection is crucial for analyzing social discourse and
developing ethically-aligned AI systems. While large language models excel
across diverse tasks, their performance on specialized moral reasoning remains
unclear.
  This study provides the first comprehensive comparison between
state-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit
datasets using ROC, PR, and DET curve analysis.
  Results reveal substantial performance gaps, with LLMs exhibiting high false
negative rates and systematic under-detection of moral content despite prompt
engineering efforts. These findings demonstrate that task-specific fine-tuning
remains superior to prompting for moral reasoning applications.

</details>


### [41] [Effective Multi-Task Learning for Biomedical Named Entity Recognition](https://arxiv.org/abs/2507.18542)
*João Ruano,Gonçalo M. Correia,Leonor Barreiros,Afonso Mendes*

Main category: cs.CL

TL;DR: 提出SRU-NER模型解决生物医学嵌套命名实体识别问题，通过多任务学习和动态损失计算实现跨数据集整合


<details>
  <summary>Details</summary>
Motivation: 生物医学术语复杂性和数据集标注不一致导致现有模型难以处理嵌套实体和跨域泛化

Method: 基于槽位循环单元(SRU)架构，采用多任务学习策略，引入动态损失调整机制避免对未标注实体类型的误判

Result: 在生物医学和通用领域NER任务中取得竞争性表现，并通过跨语料库评估验证了模型在跨域泛化上的提升

Conclusion: SRU-NER通过创新的损失计算策略有效缓解标注差异，为处理复杂生物医学实体识别和领域适应性问题提供了新思路

Abstract: Biomedical Named Entity Recognition presents significant challenges due to
the complexity of biomedical terminology and inconsistencies in annotation
across datasets. This paper introduces SRU-NER (Slot-based Recurrent Unit NER),
a novel approach designed to handle nested named entities while integrating
multiple datasets through an effective multi-task learning strategy. SRU-NER
mitigates annotation gaps by dynamically adjusting loss computation to avoid
penalizing predictions of entity types absent in a given dataset. Through
extensive experiments, including a cross-corpus evaluation and human assessment
of the model's predictions, SRU-NER achieves competitive performance in
biomedical and general-domain NER tasks, while improving cross-domain
generalization.

</details>


### [42] [GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface](https://arxiv.org/abs/2507.18546)
*Urchade Zaratiana,Gil Pasternak,Oliver Boyd,George Hurn-Maloney,Ash Lewis*

Main category: cs.CL

TL;DR: GLiNER2是高效统一的信息抽取框架，通过改进架构实现多任务支持与CPU高效运行，性能媲美大模型且部署便捷。


<details>
  <summary>Details</summary>
Motivation: 解决传统信息抽取方案需要专用模型或依赖计算成本高的大语言模型的问题，提供轻量高效的替代方案。

Method: 基于预训练transformer编码器架构，引入模式驱动接口实现多任务组合，保持模型紧凑与CPU运行效率。

Result: 在抽取和分类任务中展现竞争力，相比LLM方案显著提升部署便利性（提供pip可安装库及预训练模型）

Conclusion: GLiNER2通过统一架构平衡效率与性能，开源实现推动NLP应用的轻量化部署与多任务处理能力。

Abstract: Information extraction (IE) is fundamental to numerous NLP applications, yet
existing solutions often require specialized models for different tasks or rely
on computationally expensive large language models. We present GLiNER2, a
unified framework that enhances the original GLiNER architecture to support
named entity recognition, text classification, and hierarchical structured data
extraction within a single efficient model. Built pretrained transformer
encoder architecture, GLiNER2 maintains CPU efficiency and compact size while
introducing multi-task composition through an intuitive schema-based interface.
Our experiments demonstrate competitive performance across extraction and
classification tasks with substantial improvements in deployment accessibility
compared to LLM-based alternatives. We release GLiNER2 as an open-source
pip-installable library with pre-trained models and documentation at
https://github.com/fastino-ai/GLiNER2.

</details>


### [43] [GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation](https://arxiv.org/abs/2507.18562)
*Jiafeng Xiong,Yuting Zhao*

Main category: cs.CL

TL;DR: GIIFT框架通过跨模态图注意力网络适配器，在无需图像推理的情况下实现最先进的多模态机器翻译性能


<details>
  <summary>Details</summary>
Motivation: 解决现有多模态翻译方法模态对齐僵化、推理领域受限的问题，通过场景图建模和归纳推理突破图像依赖

Method: 构建多模态场景图保存模态特征，设计两阶段框架：跨模态图注意力网络学习统一表征，归纳迁移至图像缺失领域

Result: 在Multi30K数据集英法/英德任务中达到SOTA，WMT基准显著超越基线模型（英德+1.7 BLEU）

Conclusion: GIIFT首次实现无需图像输入的强归纳推理能力，为多模态翻译提供跨领域适应新范式

Abstract: Multimodal Machine Translation (MMT) has demonstrated the significant help of
visual information in machine translation. However, existing MMT methods face
challenges in leveraging the modality gap by enforcing rigid visual-linguistic
alignment whilst being confined to inference within their trained multimodal
domains. In this work, we construct novel multimodal scene graphs to preserve
and integrate modality-specific information and introduce GIIFT, a two-stage
Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph
Attention Network adapter to learn multimodal knowledge in a unified fused
space and inductively generalize it to broader image-free translation domains.
Experimental results on the Multi30K dataset of English-to-French and
English-to-German tasks demonstrate that our GIIFT surpasses existing
approaches and achieves the state-of-the-art, even without images during
inference. Results on the WMT benchmark show significant improvements over the
image-free translation baselines, demonstrating the strength of GIIFT towards
inductive image-free inference.

</details>


### [44] [Hybrid Tokenization Strategy for DNA Language Model using Byte Pair Encoding and K-MER Methods](https://arxiv.org/abs/2507.18570)
*Ganesh Sapkota,Md Hasibur Rahman*

Main category: cs.CL

TL;DR: 提出结合6-mer与BPE-600的混合标记化策略，显著提升DNA语言模型性能


<details>
  <summary>Details</summary>
Motivation: 传统k-mer标记化存在token分布不均、全局上下文理解受限的缺陷，需改进DNA序列建模效果

Method: 融合6-mer与600次BPE迭代生成的token构建平衡词汇表，通过next-k-mer预测任务进行模型微调

Result: 模型预测准确率达3-mer 10.78%、4-mer 10.1%、5-mer 4.12%，超越NT/DNABERT2/GROVER等先进模型

Conclusion: 混合策略有效兼顾DNA序列的局部结构和全局语境，为下游基因组分析奠定技术基础

Abstract: This paper presents a novel hybrid tokenization strategy that enhances the
performance of DNA Language Models (DLMs) by combining 6-mer tokenization with
Byte Pair Encoding (BPE-600). Traditional k-mer tokenization is effective at
capturing local DNA sequence structures but often faces challenges, including
uneven token distribution and a limited understanding of global sequence
context. To address these limitations, we propose merging unique 6mer tokens
with optimally selected BPE tokens generated through 600 BPE cycles. This
hybrid approach ensures a balanced and context-aware vocabulary, enabling the
model to capture both short and long patterns within DNA sequences
simultaneously. A foundational DLM trained on this hybrid vocabulary was
evaluated using next-k-mer prediction as a fine-tuning task, demonstrating
significantly improved performance. The model achieved prediction accuracies of
10.78% for 3-mers, 10.1% for 4-mers, and 4.12% for 5-mers, outperforming
state-of-the-art models such as NT, DNABERT2, and GROVER. These results
highlight the ability of the hybrid tokenization strategy to preserve both the
local sequence structure and global contextual information in DNA modeling.
This work underscores the importance of advanced tokenization methods in
genomic language modeling and lays a robust foundation for future applications
in downstream DNA sequence analysis and biological research.

</details>


### [45] [Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective DLLMs](https://arxiv.org/abs/2507.18578)
*Feng Hong,Geng Yu,Yushi Ye,Haicheng Huang,Huangjie Zheng,Ya Zhang,Yanfeng Wang,Jiangchao Yao*

Main category: cs.CL

TL;DR: 提出WINO算法改善扩散大语言模型的质量-速度权衡，实现6-10倍加速同时提升准确率


<details>
  <summary>Details</summary>
Motivation: 现有扩散大语言模型(DLLMs)存在不可逆解码导致的早期错误积累问题，质量与速度呈负相关

Method: 采用训练无关的WINO解码算法，通过并行草稿生成和双向上下文验证机制实现可撤销解码

Result: 在GSM8K数学基准实现6倍加速且准确率提升2.58%，在Flickr30K字幕生成达到10倍加速并保持更高性能

Conclusion: WINO算法通过动态修正机制突破DLLMs的固有局限，为高效文本生成提供新解决方案

Abstract: Diffusion Large Language Models (DLLMs) have emerged as a compelling
alternative to Autoregressive models, designed for fast parallel generation.
However, existing DLLMs are plagued by a severe quality-speed trade-off, where
faster parallel decoding leads to significant performance degradation. We
attribute this to the irreversibility of standard decoding in DLLMs, which is
easily polarized into the wrong decoding direction along with early error
context accumulation. To resolve this, we introduce Wide-In, Narrow-Out (WINO),
a training-free decoding algorithm that enables revokable decoding in DLLMs.
WINO employs a parallel draft-and-verify mechanism, aggressively drafting
multiple tokens while simultaneously using the model's bidirectional context to
verify and re-mask suspicious ones for refinement. Verified in open-source
DLLMs like LLaDA and MMaDA, WINO is shown to decisively improve the
quality-speed trade-off. For instance, on the GSM8K math benchmark, it
accelerates inference by 6$\times$ while improving accuracy by 2.58%; on
Flickr30K captioning, it achieves a 10$\times$ speedup with higher performance.
More comprehensive experiments are conducted to demonstrate the superiority and
provide an in-depth understanding of WINO.

</details>


### [46] [System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate Speech Recognition](https://arxiv.org/abs/2507.18580)
*Jiahao Wang,Ramen Liu,Longhui Zhang,Jing Li*

Main category: cs.CL

TL;DR: 提出SRAG-MAV框架用于细粒度中文仇恨言论识别，通过任务重构、自检索增强生成和多轮投票显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有方法在细粒度中文仇恨言论识别任务中存在性能局限，特别是在输出稳定性和复杂结构处理方面需要改进

Method: 1. 将四元组抽取重构为三元组任务
2. 使用自检索机制动态生成上下文提示
3. 设计多轮累积投票机制提升输出稳定性

Result: 在STATE ToxiCN数据集获得Hard Score 26.66/Soft Score 48.35/Avg 37.505，显著超越GPT-4o（15.63）和微调Qwen2.5-7B（35.365）

Conclusion: SRAG-MAV框架有效提升仇恨言论识别性能，多轮投票机制增强稳定性，开源代码推动相关研究发展

Abstract: This paper presents our system for CCL25-Eval Task 10, addressing
Fine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel
SRAG-MAV framework that synergistically integrates task reformulation(TR),
Self-Retrieval-Augmented Generation (SRAG), and Multi-Round Accumulative Voting
(MAV). Our method reformulates the quadruplet extraction task into triplet
extraction, uses dynamic retrieval from the training set to create contextual
prompts, and applies multi-round inference with voting to improve output
stability and performance. Our system, based on the Qwen2.5-7B model, achieves
a Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505 on
the STATE ToxiCN dataset, significantly outperforming baselines such as GPT-4o
(Average Score 15.63) and fine-tuned Qwen2.5-7B (Average Score 35.365). The
code is available at https://github.com/king-wang123/CCL25-SRAG-MAV.

</details>


### [47] [AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs](https://arxiv.org/abs/2507.18584)
*Xiaopeng Ke,Hexuan Deng,Xuebo Liu,Jun Rao,Zhenxi Song,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: 提出AQuilt框架，通过逻辑推理和自检机制从无标注数据生成高质量领域专用指令调优数据，仅用17%成本达到主流模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有领域适配方法存在的高计算成本、性能局限和跨任务泛化不足的问题

Method: 构建包含Answer/Question/Logic等六要素的框架，引入逻辑推理链和自检机制，支持定制化任务指令生成

Result: 生成703k高质量样本，模型性能媲美DeepSeek-V3且成本降低83%，数据与下游任务相关性显著提升

Conclusion: AQuilt为领域适配提供高效低成本解决方案，开源资源推动相关研究发展

Abstract: Despite the impressive performance of large language models (LLMs) in general
domains, they often underperform in specialized domains. Existing approaches
typically rely on data synthesis methods and yield promising results by using
unlabeled data to capture domain-specific features. However, these methods
either incur high computational costs or suffer from performance limitations,
while also demonstrating insufficient generalization across different tasks. To
address these challenges, we propose AQuilt, a framework for constructing
instruction-tuning data for any specialized domains from corresponding
unlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic,
and Task type. By incorporating logic and inspection, we encourage reasoning
processes and self-inspection to enhance model performance. Moreover,
customizable task instructions enable high-quality data generation for any
task. As a result, we construct a dataset of 703k examples to train a powerful
data synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3
while utilizing just 17% of the production cost. Further analysis demonstrates
that our generated data exhibits higher relevance to downstream tasks. Source
code, models, and scripts are available at https://github.com/Krueske/AQuilt.

</details>


### [48] [TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual Rewards](https://arxiv.org/abs/2507.18618)
*Andreea Nica,Ivan Zakazov,Nicolas Mario Baldwin,Saibo Geng,Robert West*

Main category: cs.CL

TL;DR: TRPrompt unifies textual feedback and prompt model training to generate state-of-the-art query-specific prompts for math reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods either use textual feedback without training or numerical rewards with fixed datasets, lacking integration of high-resolution textual signals in training.

Method: TRPrompt framework iteratively trains prompt models using textual feedback from LLMs, eliminating need for pre-collected datasets.

Result: Achieves SOTA performance on GSMHard and MATH datasets through LLM's internalized prompt quality assessment.

Conclusion: Combining textual rewards with iterative training enables automatic generation of high-quality, context-aware prompts for complex reasoning tasks.

Abstract: Prompt optimization improves the reasoning abilities of large language models
(LLMs) without requiring parameter updates to the target model. Following
heuristic-based "Think step by step" approaches, the field has evolved in two
main directions: while one group of methods uses textual feedback to elicit
improved prompts from general-purpose LLMs in a training-free way, a concurrent
line of research relies on numerical rewards to train a special prompt model,
tailored for providing optimal prompts to the target model. In this paper, we
introduce the Textual Reward Prompt framework (TRPrompt), which unifies these
approaches by directly incorporating textual feedback into training of the
prompt model. Our framework does not require prior dataset collection and is
being iteratively improved with the feedback on the generated prompts. When
coupled with the capacity of an LLM to internalize the notion of what a "good"
prompt is, the high-resolution signal provided by the textual rewards allows us
to train a prompt model yielding state-of-the-art query-specific prompts for
the problems from the challenging math datasets GSMHard and MATH.

</details>


### [49] [Checklists Are Better Than Reward Models For Aligning Language Models](https://arxiv.org/abs/2507.18624)
*Vijay Viswanathan,Yanchao Sun,Shuang Ma,Xiang Kong,Meng Cao,Graham Neubig,Tongshuang Wu*

Main category: cs.CL

TL;DR: 提出RLCF方法通过检查表反馈提升语言模型指令遵循能力，在多个基准测试中全面超越传统对齐方法


<details>
  <summary>Details</summary>
Motivation: 传统强化学习对齐方法使用固定标准（如helpfulness/harmfulness），难以全面满足多样化指令需求。本文试图通过指令特定化的灵活检查表标准拓宽RL的应用边界

Method: 从指令提取检查表 -> 结合AI评判和专用验证程序评估响应 -> 生成RL奖励。在Qwen2.5-7B-Instruct模型上实施，对比其他对齐方法

Result: 在5个基准测试中全面提升：FollowBench硬满足率+4点，InFoBench+6点，Arena-Hard胜率+3点

Conclusion: 检查表反馈成为提升语言模型支持多样化查询需求的关键工具，证明灵活评估标准相比固定标准的优越性

Abstract: Language models must be adapted to understand and follow user instructions.
Reinforcement learning is widely used to facilitate this -- typically using
fixed criteria such as "helpfulness" and "harmfulness". In our work, we instead
propose using flexible, instruction-specific criteria as a means of broadening
the impact that reinforcement learning can have in eliciting instruction
following. We propose "Reinforcement Learning from Checklist Feedback" (RLCF).
From instructions, we extract checklists and evaluate how well responses
satisfy each item - using both AI judges and specialized verifier programs -
then combine these scores to compute rewards for RL. We compare RLCF with other
alignment methods applied to a strong instruction following model
(Qwen2.5-7B-Instruct) on five widely-studied benchmarks -- RLCF is the only
method to improve performance on every benchmark, including a 4-point boost in
hard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a
3-point rise in win rate on Arena-Hard. These results establish checklist
feedback as a key tool for improving language models' support of queries that
express a multitude of needs.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [50] [Zero-Shot Dynamic Concept Personalization with Grid-Based LoRA](https://arxiv.org/abs/2507.17963)
*Rameen Abdal,Or Patashnik,Ekaterina Deyneka,Hao Chen,Aliaksandr Siarohin,Sergey Tulyakov,Daniel Cohen-Or,Kfir Aberman*

Main category: cs.GR

TL;DR: 提出完全零样本的文本到视频动态概念个性化框架，通过Grid-LoRA轻量适配器和网格填充模块实现无需微调的新概念处理


<details>
  <summary>Details</summary>
Motivation: 现有动态概念个性化方法需要针对每个实例进行微调，严重限制了实际应用的可扩展性，需开发无需测试时优化的解决方案

Method: 利用结构化2x2视频网格组织输入输出对，训练轻量级Grid-LoRA适配器进行编辑组合，配合专用Grid Fill模块完成局部布局生成时序连贯结果

Result: 在未训练过的多种主题和编辑场景下均能产生高质量、身份保持一致的视频结果，实验验证了方案的强泛化能力

Conclusion: 整套系统经单次训练即可通过前向传播处理未知动态概念，实现了无需测试优化的高效个性化视频生成

Abstract: Recent advances in text-to-video generation have enabled high-quality
synthesis from text and image prompts. While the personalization of dynamic
concepts, which capture subject-specific appearance and motion from a single
video, is now feasible, most existing methods require per-instance fine-tuning,
limiting scalability. We introduce a fully zero-shot framework for dynamic
concept personalization in text-to-video models. Our method leverages
structured 2x2 video grids that spatially organize input and output pairs,
enabling the training of lightweight Grid-LoRA adapters for editing and
composition within these grids. At inference, a dedicated Grid Fill module
completes partially observed layouts, producing temporally coherent and
identity preserving outputs. Once trained, the entire system operates in a
single forward pass, generalizing to previously unseen dynamic concepts without
any test-time optimization. Extensive experiments demonstrate high-quality and
consistent results across a wide range of subjects beyond trained concepts and
editing scenarios.

</details>


### [51] [DanceGraph: A Complementary Architecture for Synchronous Dancing Online](https://arxiv.org/abs/2507.18052)
*David Sinclair,Ademyemi Ademola,Babis Koniaris,Kenny Mitchell*

Main category: cs.GR

TL;DR: 提出DanceGraph架构解决在线同步舞蹈中的网络延迟问题，通过实时带宽优化和参数化动作修正实现节奏同步


<details>
  <summary>Details</summary>
Motivation: 解决网络化身体姿态共享的延迟问题，实现与音乐节奏同步的在线舞蹈交互需求

Method: 1. 开发实时带宽优化架构降低延迟
2. 建立短时域动作预测框架
3. 参数化舞蹈动作风格化校正系统

Result: 实现音乐节奏同步的在线舞蹈交互，通过动作修正系统增强舞蹈表现力

Conclusion: 该架构通过延迟优化和实时校正技术，为在线舞蹈协作提供有效的技术解决方案

Abstract: DanceGraph is an architecture for synchronized online dancing overcoming the
latency of networked body pose sharing. We break down this challenge by
developing a real-time bandwidth-efficient architecture to minimize lag and
reduce the timeframe of required motion prediction for synchronization with the
music's rhythm. In addition, we show an interactive method for the
parameterized stylization of dance motions for rhythmic dance using online
dance correctives.

</details>


### [52] [GeoAvatar: Adaptive Geometrical Gaussian Splatting for 3D Head Avatar](https://arxiv.org/abs/2507.18155)
*SeungJun Moon,Hah Min Lew,Seungeun Lee,Ji-Su Kang,Gyeong-Moon Park*

Main category: cs.GR

TL;DR: 提出GeoAvatar框架，通过自适应高斯分割、嘴部结构优化和正则化损失提升3D头像重建与动画质量，并发布DynamicFace数据集


<details>
  <summary>Details</summary>
Motivation: 现有方法在平衡面部重建与动画时存在几何偏差适应问题，导致生成质量受限

Method: 1. APS阶段分割刚/柔性高斯集进行偏移正则化
2. 基于解剖学的嘴部结构设计与部分形变策略
3. 高斯-3DMM绑定正则化损失
4. 发布DynamicFace视频数据集

Result: 实验显示GeoAvatar在重建精度和动画保真度上超越现有方法

Conclusion: 该方法通过技术创新有效平衡了面部重建与动画需求，DynamicFace数据集为相关研究提供新基准

Abstract: Despite recent progress in 3D head avatar generation, balancing identity
preservation, i.e., reconstruction, with novel poses and expressions, i.e.,
animation, remains a challenge. Existing methods struggle to adapt Gaussians to
varying geometrical deviations across facial regions, resulting in suboptimal
quality. To address this, we propose GeoAvatar, a framework for adaptive
geometrical Gaussian Splatting. GeoAvatar leverages Adaptive Pre-allocation
Stage (APS), an unsupervised method that segments Gaussians into rigid and
flexible sets for adaptive offset regularization. Then, based on mouth anatomy
and dynamics, we introduce a novel mouth structure and the part-wise
deformation strategy to enhance the animation fidelity of the mouth. Finally,
we propose a regularization loss for precise rigging between Gaussians and 3DMM
faces. Moreover, we release DynamicFace, a video dataset with highly expressive
facial motions. Extensive experiments show the superiority of GeoAvatar
compared to state-of-the-art methods in reconstruction and novel animation
scenarios.

</details>


### [53] [PS-GS: Gaussian Splatting for Multi-View Photometric Stereo](https://arxiv.org/abs/2507.18231)
*Yixiao Chen,Bin Liang,Hanzhi Guo,Yongqing Cheng,Jiayi Zhao,Dongdong Weng*

Main category: cs.GR

TL;DR: 提出PS-GS方法，结合逆渲染与多视角光度立体法，通过高斯建模与多光源优化实现高效三维重建。


<details>
  <summary>Details</summary>
Motivation: 现有逆渲染方法依赖固定光照导致精度受限，MVPS虽提升精度但存在计算效率低的问题，需开发兼顾精度与效率的解决方案。

Method: 1. 初始化2D高斯模型几何→2. 基于全渲染方程构建带光照MLP的逆渲染框架→3. 光度立体法法线正则化→4. 开发2D高斯光线追踪优化单光源入射。

Result: 在合成/真实数据集上重建精度超SOTA，计算效率提升显著（如论文实验数据所示）。

Conclusion: PS-GS首次实现多视角+多光源联合优化，支持三维编辑应用，为逆渲染提供新范式。

Abstract: Integrating inverse rendering with multi-view photometric stereo (MVPS)
yields more accurate 3D reconstructions than the inverse rendering approaches
that rely on fixed environment illumination. However, efficient inverse
rendering with MVPS remains challenging. To fill this gap, we introduce the
Gaussian Splatting for Multi-view Photometric Stereo (PS-GS), which efficiently
and jointly estimates the geometry, materials, and lighting of the object that
is illuminated by diverse directional lights (multi-light). Our method first
reconstructs a standard 2D Gaussian splatting model as the initial geometry.
Based on the initialization model, it then proceeds with the deferred inverse
rendering by the full rendering equation containing a lighting-computing
multi-layer perceptron. During the whole optimization, we regularize the
rendered normal maps by the uncalibrated photometric stereo estimated normals.
We also propose the 2D Gaussian ray-tracing for single directional light to
refine the incident lighting. The regularizations and the use of multi-view and
multi-light images mitigate the ill-posed problem of inverse rendering. After
optimization, the reconstructed object can be used for novel-view synthesis,
relighting, and material and shape editing. Experiments on both synthetic and
real datasets demonstrate that our method outperforms prior works in terms of
reconstruction accuracy and computational efficiency.

</details>


### [54] [Tiny is not small enough: High-quality, low-resource facial animation models through hybrid knowledge distillation](https://arxiv.org/abs/2507.18352)
*Zhen Han,Mattias Teye,Derek Yadgaroff,Judith Bütepage*

Main category: cs.GR

TL;DR: 利用混合知识蒸馏与伪标签技术，将大型教师模型压缩为仅含卷积和全连接层的超小型学生模型（3.4MB/81ms），实现游戏场景中实时设备端面部动画推理。


<details>
  <summary>Details</summary>
Motivation: 现有基于预训练语音编码器的面部动画模型体积过大（需专用离线设备），无法满足游戏开发中对实时、轻量化的设备端推理需求。

Method: 采用教师-学生混合蒸馏框架：1）利用教师模型对大规模音频生成伪标签 2）设计仅含卷积层和全连接层的学生架构，去除注意力机制和循环模块 3）优化内存占用与音频上下文依赖

Result: 学生模型内存占用降至3.4MB，未来音频上下文需求缩减至81ms，同时保持与大型模型相当的动画质量。

Conclusion: 该突破使模型驱动的数字角色实现设备端实时推理，为游戏等实时交互场景提供实用化解决方案。

Abstract: The training of high-quality, robust machine learning models for
speech-driven 3D facial animation requires a large, diverse dataset of
high-quality audio-animation pairs. To overcome the lack of such a dataset,
recent work has introduced large pre-trained speech encoders that are robust to
variations in the input audio and, therefore, enable the facial animation model
to generalize across speakers, audio quality, and languages. However, the
resulting facial animation models are prohibitively large and lend themselves
only to offline inference on a dedicated machine. In this work, we explore
on-device, real-time facial animation models in the context of game
development. We overcome the lack of large datasets by using hybrid knowledge
distillation with pseudo-labeling. Given a large audio dataset, we employ a
high-performing teacher model to train very small student models. In contrast
to the pre-trained speech encoders, our student models only consist of
convolutional and fully-connected layers, removing the need for attention
context or recurrent updates. In our experiments, we demonstrate that we can
reduce the memory footprint to up to 3.4 MB and required future audio context
to up to 81 ms while maintaining high-quality animations. This paves the way
for on-device inference, an important step towards realistic, model-driven
digital characters.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [55] [RECALLED: An Unbounded Resource Consumption Attack on Large Vision-Language Models](https://arxiv.org/abs/2507.18053)
*Haoran Gao,Yuanhe Zhang,Zhenhong Zhou,Lei Jiang,Fanyu Meng,Yujia Xiao,Kun Wang,Yang Liu,Junlan Feng*

Main category: cs.CR

TL;DR: 提出首个针对大型视觉语言模型的多模态资源消耗攻击框架RECALLED，通过视觉对抗扰动触发无限生成，显著增加系统资源消耗


<details>
  <summary>Details</summary>
Motivation: 现有红队测试忽略视觉输入的攻击面，导致大型视觉语言模型对资源消耗攻击的防御策略不足

Method: 采用像素级视觉引导优化生成输出重复的对抗扰动，结合多目标并行损失解决模板通用性与攻击并行性问题

Result: 攻击使服务响应延迟增加26倍↑，GPU利用率与内存消耗额外增加20%

Conclusion: 暴露大型视觉语言模型安全漏洞，建立首个支持多模态攻击的红队测试框架，为防御技术研发提供基准

Abstract: Resource Consumption Attacks (RCAs) have emerged as a significant threat to
the deployment of Large Language Models (LLMs). With the integration of vision
modalities, additional attack vectors exacerbate the risk of RCAs in large
vision-language models (LVLMs). However, existing red-teaming studies have
largely overlooked visual inputs as a potential attack surface, resulting in
insufficient mitigation strategies against RCAs in LVLMs. To address this gap,
we propose RECALLED (\textbf{RE}source \textbf{C}onsumption \textbf{A}ttack on
\textbf{L}arge Vision-\textbf{L}anguag\textbf{E} Mo\textbf{D}els), the first
approach for exploiting visual modalities to trigger unbounded RCAs
red-teaming. First, we present \textit{Vision Guided Optimization}, a
fine-grained pixel-level optimization, to obtain \textit{Output Recall}
adversarial perturbations, which can induce repeating output. Then, we inject
the perturbations into visual inputs, triggering unbounded generations to
achieve the goal of RCAs. Additionally, we introduce \textit{Multi-Objective
Parallel Losses} to generate universal attack templates and resolve
optimization conflicts when intending to implement parallel attacks. Empirical
results demonstrate that RECALLED increases service response latency by over 26
$\uparrow$, resulting in an additional 20\% increase in GPU utilization and
memory consumption. Our study exposes security vulnerabilities in LVLMs and
establishes a red-teaming framework that can facilitate future defense
development against RCAs.

</details>


### [56] [LoRA-Leak: Membership Inference Attacks Against LoRA Fine-tuned Language Models](https://arxiv.org/abs/2507.18302)
*Delong Ran,Xinlei He,Tianshuo Cong,Anyu Wang,Qi Li,Xiaoyun Wang*

Main category: cs.CR

TL;DR: LoRA微调语言模型存在被成员推理攻击(MIA)的隐私风险，预训练模型会加剧信息泄露。提出的LoRA-Leak框架验证了攻击有效性(最高AUC 0.775)，发现dropout和层排除是有效防御措施。


<details>
  <summary>Details</summary>
Motivation: 现有MIAs忽视预训练模型引发的信息泄露风险，需系统评估LoRA微调场景下的数据隐私威胁，为专业语言模型提供商提供防护指导。

Method: 构建含15种MIAs的评估框架LoRA-Leak(含5种基于预训练模型的改进攻击)，在3个先进LM和3个NLP任务中测试不同微调设置，并探索四种防御方案。

Result: 保守微调设置下MIA成功率仍达0.775 AUC；预训练模型使攻击成功率提升24.9%；仅dropout和选择性层微调能平衡隐私与模型性能。

Conclusion: 预训练模型的存在显著增加LoRA微调模型的MIA风险，建议专业LM服务商优先采用参数过滤和dropout实现隐私-性能平衡。

Abstract: Language Models (LMs) typically adhere to a "pre-training and fine-tuning"
paradigm, where a universal pre-trained model can be fine-tuned to cater to
various specialized domains. Low-Rank Adaptation (LoRA) has gained the most
widespread use in LM fine-tuning due to its lightweight computational cost and
remarkable performance. Because the proportion of parameters tuned by LoRA is
relatively small, there might be a misleading impression that the LoRA
fine-tuning data is invulnerable to Membership Inference Attacks (MIAs).
However, we identify that utilizing the pre-trained model can induce more
information leakage, which is neglected by existing MIAs. Therefore, we
introduce LoRA-Leak, a holistic evaluation framework for MIAs against the
fine-tuning datasets of LMs. LoRA-Leak incorporates fifteen membership
inference attacks, including ten existing MIAs, and five improved MIAs that
leverage the pre-trained model as a reference. In experiments, we apply
LoRA-Leak to three advanced LMs across three popular natural language
processing tasks, demonstrating that LoRA-based fine-tuned LMs are still
vulnerable to MIAs (e.g., 0.775 AUC under conservative fine-tuning settings).
We also applied LoRA-Leak to different fine-tuning settings to understand the
resulting privacy risks. We further explore four defenses and find that only
dropout and excluding specific LM layers during fine-tuning effectively
mitigate MIA risks while maintaining utility. We highlight that under the
"pre-training and fine-tuning" paradigm, the existence of the pre-trained model
makes MIA a more severe risk for LoRA-based LMs. We hope that our findings can
provide guidance on data privacy protection for specialized LM providers.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [57] [Quantum Machine Learning Playground](https://arxiv.org/abs/2507.17931)
*Pascal Debus,Sebastian Issel,Kilian Tscharke*

Main category: quant-ph

TL;DR: 开发交互式量子机器学习可视化工具QML Playground，通过数据重上传通用量子分类器的可视化隐喻，降低量子计算学习门槛。


<details>
  <summary>Details</summary>
Motivation: 填补量子机器学习领域可视化工具的空白，受经典机器学习工具TensorFlow Playground启发，旨在促进QML领域创新发展。

Method: 1. 整合量子计算与经典机器学习的可视化隐喻
2. 开发算法可视化概念框架
3. 构建交互式网页应用程序实现

Result: 创建首个量子机器学习Playground原型，实现量子分类器的参数调节与动态可视化功能。

Conclusion: 该工具通过直观交互设计有效降低学习曲线，为量子计算教育提供新范式，推动QML模型创新探索。

Abstract: This article introduces an innovative interactive visualization tool designed
to demystify quantum machine learning (QML) algorithms. Our work is inspired by
the success of classical machine learning visualization tools, such as
TensorFlow Playground, and aims to bridge the gap in visualization resources
specifically for the field of QML. The article includes a comprehensive
overview of relevant visualization metaphors from both quantum computing and
classical machine learning, the development of an algorithm visualization
concept, and the design of a concrete implementation as an interactive web
application. By combining common visualization metaphors for the so-called data
re-uploading universal quantum classifier as a representative QML model, this
article aims to lower the entry barrier to quantum computing and encourage
further innovation in the field. The accompanying interactive application is a
proposal for the first version of a quantum machine learning playground for
learning and exploring QML models.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [58] [Topology-Preserving Coupling of Compressible Fluids and Thin Deformables](https://arxiv.org/abs/2507.18460)
*Jonathan Panuelos,Eitan Grinspun,David Levin*

Main category: physics.comp-ph

TL;DR: 提出基于约束Voronoi的空间离散化方法，实现可压缩流体与超薄可变形结构的无泄漏双向耦合仿真。


<details>
  <summary>Details</summary>
Motivation: 解决传统流体-固体耦合仿真中因流体域路径连通性破坏导致的泄漏问题，尤其针对超薄结构的密封性挑战。

Method: 结合约束Voronoi空间划分与Godunov有限体积法，构建严格贴合流固界面的离散单元，实现精确的边界条件处理。

Result: 在压缩空气驱动气球、香槟塞弹射及超音速小行星等场景验证，证明双向能量传递且零流体泄漏。

Conclusion: 该方法通过保持流体域拓扑连通性，首次实现任意厚度可变形体与流体的直接力学交互与严格密封仿真。

Abstract: We present a novel discretization of coupled compressible fluid and thin
deformable structures that provides sufficient and necessary leakproofness by
preserving the path connectedness of the fluid domain. Our method employs a
constrained Voronoi-based spatial partitioning combined with Godunov-style
finite-volume time integration. The fluid domain is discretized into cells that
conform exactly to the fluid-solid interface, allowing boundary conditions to
be sharply resolved exactly at the interface. This enables direct force
exchange between the fluid and solid while ensuring that no fluid leaks through
the solid, even when arbitrarily thin. We validate our approach on a series of
challenging scenarios -- including a balloon propelled by internal compressed
air, a champagne cork ejecting after overcoming friction, and a supersonic
asteroid -- demonstrating bidirectional energy transfer between fluid and
solid.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [59] [Bob's Confetti: Phonetic Memorization Attacks in Music and Video Generation](https://arxiv.org/abs/2507.17937)
*Jaechul Roh,Zachary Novack,Yuefeng Peng,Niloofar Mireshghallah,Taylor Berg-Kirkpatrick,Amir Houmansadr*

Main category: cs.SD

TL;DR: 发现多模态生成模型存在语音触发记忆漏洞，通过同音替换歌词即可复现训练数据中的音视频内容，揭示生成系统的版权安全隐患。


<details>
  <summary>Details</summary>
Motivation: 现有歌词转歌曲（LS2）生成模型存在训练数据记忆风险，但该问题尚未被充分研究。需要探索模型对语音结构变化的敏感性及其跨模态记忆效应。

Method: 提出对抗性语音提示（APT）攻击方法：通过同音词替换（如'mom's spaghetti'→'Bob's confetti'）保持声学结构但改变语义，测试SUNO/YuE等音频模型和Veo3视频模型的输出相似度。

Result: 1）音频模型对语音修改后的歌词仍生成与原始训练数据高度相似的音频（CLAP/AudioJudge相似度达0.82） 2）文本到视频模型出现语音-视觉记忆：修改后的歌词触发《Lose Yourself》MV的视觉元素重构。

Conclusion: 语音提示可激活多模态系统的记忆内容，暴露生成模型在版权、内容溯源方面的重大安全隐患，需紧急改进模型训练方法和安全机制。

Abstract: Lyrics-to-Song (LS2) generation models promise end-to-end music synthesis
from text, yet their vulnerability to training data memorization remains
underexplored. We introduce Adversarial PhoneTic Prompting (APT), a novel
attack where lyrics are semantically altered while preserving their acoustic
structure through homophonic substitutions (e.g., Eminem's famous "mom's
spaghetti" $\rightarrow$ "Bob's confetti"). Despite these distortions, we
uncover a powerful form of sub-lexical memorization: models like SUNO and YuE
regenerate outputs strikingly similar to known training content, achieving high
similarity across audio-domain metrics, including CLAP, AudioJudge, and
CoverID. This vulnerability persists across multiple languages and genres. More
surprisingly, we discover that phoneme-altered lyrics alone can trigger visual
memorization in text-to-video models. When prompted with phonetically modified
lyrics from Lose Yourself, Veo 3 reconstructs visual elements from the original
music video -- including character appearance and scene composition -- despite
no visual cues in the prompt. We term this phenomenon phonetic-to-visual
regurgitation. Together, these findings expose a critical vulnerability in
transcript-conditioned multimodal generation: phonetic prompting alone can
unlock memorized audiovisual content, raising urgent questions about copyright,
safety, and content provenance in modern generative systems. Example
generations are available on our demo page (jrohsc.github.io/music_attack/).

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [60] [Agentic AI framework for End-to-End Medical Data Inference](https://arxiv.org/abs/2507.18115)
*Soorya Ram Shimgekar,Shayan Vassef,Abhay Goyal,Navin Kumar,Koustuv Saha*

Main category: cs.AI

TL;DR: 提出了基于Agentic AI的自动化临床数据处理框架，通过模块化代理系统实现从数据摄取到模型推理的全流程自动化，降低医疗AI部署成本。


<details>
  <summary>Details</summary>
Motivation: 解决医疗领域机器学习部署成本高、流程碎片化、模型兼容性差及数据隐私约束严格的问题。

Method: 构建模块化代理系统：1) 数据识别与匿名化代理 2) 嵌入和MedGemma特征提取 3) 模型特征匹配代理 4) 自动化预处理与推理代理。

Result: 在老年医学、姑息治疗和结肠镜影像等公开数据集上验证了全流程自动化可行性。

Conclusion: 该框架通过减少专家干预次数，为临床环境提供了可扩展且经济高效的AI实施路径。

Abstract: Building and deploying machine learning solutions in healthcare remains
expensive and labor-intensive due to fragmented preprocessing workflows, model
compatibility issues, and stringent data privacy constraints. In this work, we
introduce an Agentic AI framework that automates the entire clinical data
pipeline, from ingestion to inference, through a system of modular,
task-specific agents. These agents handle both structured and unstructured
data, enabling automatic feature selection, model selection, and preprocessing
recommendation without manual intervention. We evaluate the system on publicly
available datasets from geriatrics, palliative care, and colonoscopy imaging.
For example, in the case of structured data (anxiety data) and unstructured
data (colonoscopy polyps data), the pipeline begins with file-type detection by
the Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring
privacy compliance, where we first identify the data type and then anonymize
it. The Feature Extraction Agent identifies features using an embedding-based
approach for tabular data, extracting all column names, and a multi-stage
MedGemma-based approach for image data, which infers modality and disease name.
These features guide the Model-Data Feature Matcher Agent in selecting the
best-fit model from a curated repository. The Preprocessing Recommender Agent
and Preprocessing Implementor Agent then apply tailored preprocessing based on
data type and model requirements. Finally, the ``Model Inference Agent" runs
the selected model on the uploaded data and generates interpretable outputs
using tools like SHAP, LIME, and DETR attention maps. By automating these
high-friction stages of the ML lifecycle, the proposed framework reduces the
need for repeated expert intervention, offering a scalable, cost-efficient
pathway for operationalizing AI in clinical environments.

</details>


### [61] [SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\circ}$ Law](https://arxiv.org/abs/2507.18576)
*Shanghai AI Lab,:,Yicheng Bao,Guanxu Chen,Mingkang Chen,Yunhao Chen,Chiyu Chen,Lingjie Chen,Sirui Chen,Xinquan Chen,Jie Cheng,Yu Cheng,Dengke Deng,Yizhuo Ding,Dan Ding,Xiaoshan Ding,Yi Ding,Zhichen Dong,Lingxiao Du,Yuyu Fan,Xinshun Feng,Yanwei Fu,Yuxuan Gao,Ruijun Ge,Tianle Gu,Lujun Gui,Jiaxuan Guo,Qianxi He,Yuenan Hou,Xuhao Hu,Hong Huang,Kaichen Huang,Shiyang Huang,Yuxian Jiang,Shanzhe Lei,Jie Li,Lijun Li,Hao Li,Juncheng Li,Xiangtian Li,Yafu Li,Lingyu Li,Xueyan Li,Haotian Liang,Dongrui Liu,Qihua Liu,Zhixuan Liu,Bangwei Liu,Huacan Liu,Yuexiao Liu,Zongkai Liu,Chaochao Lu,Yudong Lu,Xiaoya Lu,Zhenghao Lu,Qitan Lv,Caoyuan Ma,Jiachen Ma,Xiaoya Ma,Zhongtian Ma,Lingyu Meng,Ziqi Miao,Yazhe Niu,Yuezhang Peng,Yuan Pu,Han Qi,Chen Qian,Xingge Qiao,Jingjing Qu,Jiashu Qu,Wanying Qu,Wenwen Qu,Xiaoye Qu,Qihan Ren,Qingnan Ren,Qingyu Ren,Jing Shao,Wenqi Shao,Shuai Shao,Dongxing Shi,Xin Song,Xinhao Song,Yan Teng,Xuan Tong,Yingchun Wang,Xuhong Wang,Shujie Wang,Xin Wang,Yige Wang,Yixu Wang,Yuanfu Wang,Futing Wang,Ruofan Wang,Wenjie Wang,Yajie Wang,Muhao Wei,Xiaoyu Wen,Fenghua Weng,Yuqi Wu,Yingtong Xiong,Xingcheng Xu,Chao Yang,Yue Yang,Yang Yao,Yulei Ye,Zhenyun Yin,Yi Yu,Bo Zhang,Qiaosheng Zhang,Jinxuan Zhang,Yexin Zhang,Yinqiang Zheng,Hefeng Zhou,Zhanhui Zhou,Pengyu Zhu,Qingzi Zhu,Yubo Zhu,Bowen Zhou*

Main category: cs.AI

TL;DR: 提出SafeWork-R1多模态推理模型，通过SafeLadder框架实现安全与能力的协同进化，在保持通用能力的同时安全性能提升46.54%，超越GPT-4.1等商业模型


<details>
  <summary>Details</summary>
Motivation: 现有对齐方法（如RLHF）仅学习人类偏好，缺乏内在安全推理能力。需要构建安全与能力协同发展的可信AI系统

Method: 1. 提出SafeLadder框架：大规模渐进式安全强化学习后训练 + 多原则验证套件
2. 推理时双重干预机制 + 审议搜索机制
3. 开发不同参数规模的模型变体

Result: 1. 安全基准平均提升46.54%（相比基模型Qwen2.5-VL-72B）
2. 安全性能超越GPT-4.1/Claude Opus 4
3. 成功开发78B/70B/7B多规模安全模型

Conclusion: 验证了安全与能力协同进化的可行性，框架具有强泛化性，为构建可信通用AI提供了新范式

Abstract: We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that
demonstrates the coevolution of capabilities and safety. It is developed by our
proposed SafeLadder framework, which incorporates large-scale, progressive,
safety-oriented reinforcement learning post-training, supported by a suite of
multi-principled verifiers. Unlike previous alignment methods such as RLHF that
simply learn human preferences, SafeLadder enables SafeWork-R1 to develop
intrinsic safety reasoning and self-reflection abilities, giving rise to safety
`aha' moments. Notably, SafeWork-R1 achieves an average improvement of
$46.54\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks
without compromising general capabilities, and delivers state-of-the-art safety
performance compared to leading proprietary models such as GPT-4.1 and Claude
Opus 4. To further bolster its reliability, we implement two distinct
inference-time intervention methods and a deliberative search mechanism,
enforcing step-level verification. Finally, we further develop
SafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and
SafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and
capability can co-evolve synergistically, highlighting the generalizability of
our framework in building robust, reliable, and trustworthy general-purpose AI.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [62] [GRR-CoCa: Leveraging LLM Mechanisms in Multimodal Model Architectures](https://arxiv.org/abs/2507.18009)
*Jake R. Patock,Nicole Catherine Lewis,Kevin McCoy,Christina Gomez,Canling Chen,Lorenzo Luzi*

Main category: cs.CV

TL;DR: 提出GRR-CoCa模型，通过融合高斯误差门控单元/RMS归一化/旋转位置编码三项改进，在CoCa架构中实现跨视觉-语言领域性能提升


<details>
  <summary>Details</summary>
Motivation: 现有最先进的多模态模型架构落后于语言模型的技术演进，CoCa模型的文本解码器和ViT编码器未采用LLM已验证有效的新兴架构改进

Method: 1) 在文本解码器和ViT编码器中引入高斯误差门控线性单元 2) 采用均方根归一化 3) 整合旋转位置编码技术

Result: 预训练阶段对比损失降低27.25%，微调阶段平均提升13.66%。在三个不同下游任务中CoCa损失平均减少5.55%，显示跨领域泛化能力提升

Conclusion: 将LLM验证有效的架构创新移植到多模态模型可显著提升性能，为视觉-语言联合建模提供新的架构改进方向

Abstract: State-of-the-art (SOTA) image and text generation models are multimodal
models that have many similarities to large language models (LLMs). Despite
achieving strong performances, leading foundational multimodal model
architectures frequently lag behind the architectural sophistication of
contemporary LLMs. We propose GRR-CoCa, an improved SOTA Contrastive Captioner
(CoCa) model that incorporates Gaussian error gated linear units, root mean
squared normalization, and rotary positional embedding into the textual
decoders and the vision transformer (ViT) encoder. Each architectural
modification has been shown to improve model performance in LLMs, but has yet
to be adopted in CoCa. We benchmarked GRR-CoCa against Baseline CoCa, a model
with the same modified textual decoders but with CoCa's original ViT encoder.
We used standard pretraining and fine-tuning workflows to benchmark the models
on contrastive and generative tasks. Our GRR-CoCa significantly outperformed
Baseline CoCa on the pretraining dataset and three diverse fine-tuning
datasets. Pretraining improvements were 27.25% in contrastive loss, 3.71% in
perplexity, and 7.15% in CoCa loss. The average fine-tuning improvements were
13.66% in contrastive loss, 5.18% in perplexity, and 5.55% in CoCa loss. We
show that GRR-CoCa's modified architecture improves performance and
generalization across vision-language domains.

</details>


### [63] [SynC: Synthetic Image Caption Dataset Refinement with One-to-many Mapping for Zero-shot Image Captioning](https://arxiv.org/abs/2507.18616)
*Si-Woo Kim,MinJu Jeon,Ye-Chan Kim,Soeun Lee,Taewhan Kim,Dong-Jin Kim*

Main category: cs.CV

TL;DR: 提出SynC框架，通过重新分配标题优化合成图像-描述数据集，显著提升零样本图像描述模型的性能


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像模型生成的图像与标题语义不匹配问题（如缺失对象、属性错误），现有数据清洗方法不适用于合成数据场景

Method: 采用一对多映射策略：1) 为每个标题检索多个候选图像；2) 通过循环一致性对齐评分器选择最佳匹配图像（验证图像能逆向检索回原标题）

Result: 在MS-COCO/Flickr30k/NoCaps等基准测试中实现性能持续提升，部分场景达到state-of-the-art水平

Conclusion: SynC为优化合成数据质量提供了有效策略，通过语义对齐的图像-标题重组机制显著增强零样本图像描述效果

Abstract: Zero-shot Image Captioning (ZIC) increasingly utilizes synthetic datasets
generated by text-to-image (T2I) models to mitigate the need for costly manual
annotation. However, these T2I models often produce images that exhibit
semantic misalignments with their corresponding input captions (e.g., missing
objects, incorrect attributes), resulting in noisy synthetic image-caption
pairs that can hinder model training. Existing dataset pruning techniques are
largely designed for removing noisy text in web-crawled data. However, these
methods are ill-suited for the distinct challenges of synthetic data, where
captions are typically well-formed, but images may be inaccurate
representations. To address this gap, we introduce SynC, a novel framework
specifically designed to refine synthetic image-caption datasets for ZIC.
Instead of conventional filtering or regeneration, SynC focuses on reassigning
captions to the most semantically aligned images already present within the
synthetic image pool. Our approach employs a one-to-many mapping strategy by
initially retrieving multiple relevant candidate images for each caption. We
then apply a cycle-consistency-inspired alignment scorer that selects the best
image by verifying its ability to retrieve the original caption via
image-to-text retrieval. Extensive evaluations demonstrate that SynC
consistently and significantly improves performance across various ZIC models
on standard benchmarks (MS-COCO, Flickr30k, NoCaps), achieving state-of-the-art
results in several scenarios. SynC offers an effective strategy for curating
refined synthetic data to enhance ZIC.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [64] [Exploring Communication Strategies for Collaborative LLM Agents in Mathematical Problem-Solving](https://arxiv.org/abs/2507.17753)
*Liang Zhang,Xiaoming Zhai,Jionghao Lin,Jionghao Lin,Jennifer Kleiman,Diego Zapata-Rivera,Carol Forsyth,Yang Jiang,Xiangen Hu,Arthur C. Graesser*

Main category: cs.HC

TL;DR: 双LLM代理通过同伴协作模式在数学解题中表现最优，对话行为对协作效率起关键作用


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对LLM代理间不同沟通策略对教育场景问题解决效果的系统评估

Method: 使用GPT-4o模型在MATH数据集上测试四种双代理沟通模式（师生互动/同伴协作/互惠教学/批判辩论）

Result: 双代理配置准确率提升明显（同伴协作模式达最高），陈述/确认/提示三类对话行为对解题最关键

Conclusion: 多代理框架需配合有效沟通策略（如同伴协作）才能更好解决AI教育中的复杂问题

Abstract: Large Language Model (LLM) agents are increasingly utilized in AI-aided
education to support tutoring and learning. Effective communication strategies
among LLM agents improve collaborative problem-solving efficiency and
facilitate cost-effective adoption in education. However, little research has
systematically evaluated the impact of different communication strategies on
agents' problem-solving. Our study examines four communication modes,
\textit{teacher-student interaction}, \textit{peer-to-peer collaboration},
\textit{reciprocal peer teaching}, and \textit{critical debate}, in a
dual-agent, chat-based mathematical problem-solving environment using the
OpenAI GPT-4o model. Evaluated on the MATH dataset, our results show that
dual-agent setups outperform single agents, with \textit{peer-to-peer
collaboration} achieving the highest accuracy. Dialogue acts like statements,
acknowledgment, and hints play a key role in collaborative problem-solving.
While multi-agent frameworks enhance computational tasks, effective
communication strategies are essential for tackling complex problems in AI
education.

</details>


### [65] [PosterMate: Audience-driven Collaborative Persona Agents for Poster Design](https://arxiv.org/abs/2507.18572)
*Donghoon Shin,Daniel Lee,Gary Hsieh,Gromit Yeuk-Yin Chan*

Main category: cs.HC

TL;DR: PosterMate是基于营销文档构建受众角色代理的海报设计助手，通过AI模拟多视角反馈和协调讨论实现设计优化


<details>
  <summary>Details</summary>
Motivation: 传统海报设计难以及时获取多样化受众反馈，生成式AI为模拟人类交互提供新可能但应用方式尚不明确

Method: 创建市场导向的角色代理收集设计反馈，引入协调者促成共识讨论，并将共识性修改直接整合到设计方案中

Result: 用户研究(N=12)验证其捕捉遗漏观点的潜力，在线评估(N=100)显示角色反馈符合身份特征且讨论能有效综合多方视角

Conclusion: PosterMate成功展示了AI代理在协调多样化反馈中的有效性，为设计协作提供了新的原型工具范式

Abstract: Poster designing can benefit from synchronous feedback from target audiences.
However, gathering audiences with diverse perspectives and reconciling them on
design edits can be challenging. Recent generative AI models present
opportunities to simulate human-like interactions, but it is unclear how they
may be used for feedback processes in design. We introduce PosterMate, a poster
design assistant that facilitates collaboration by creating audience-driven
persona agents constructed from marketing documents. PosterMate gathers
feedback from each persona agent regarding poster components, and stimulates
discussion with the help of a moderator to reach a conclusion. These
agreed-upon edits can then be directly integrated into the poster design.
Through our user study (N=12), we identified the potential of PosterMate to
capture overlooked viewpoints, while serving as an effective prototyping tool.
Additionally, our controlled online evaluation (N=100) revealed that the
feedback from an individual persona agent is appropriate given its persona
identity, and the discussion effectively synthesizes the different persona
agents' perspectives.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [66] [Recent Trends in Distant Conversational Speech Recognition: A Review of CHiME-7 and 8 DASR Challenges](https://arxiv.org/abs/2507.18161)
*Samuele Cornell,Christoph Boeddeker,Taejin Park,He Huang,Desh Raj,Matthew Wiesner,Yoshiki Masuyama,Xuankai Chang,Zhong-Qiu Wang,Stefano Squartini,Paola Garcia,Shinji Watanabe*

Main category: eess.AS

TL;DR: CHiME-7/8挑战赛聚焦多通道语音识别与日记化，揭示端到端ASR主导、传统分离技术仍关键、日记化准确性重要、大模型处理错误有效、复杂场景转录仍困难五大发现


<details>
  <summary>Details</summary>
Motivation: 推动远场语音识别技术进步，评估多场景下联合ASR与日记化系统的性能，分析当前技术瓶颈与改进方向

Method: 结合端到端ASR系统（利用大规模预训练模型）、引导源分离技术、目标说话人日记化方法，通过系统集成和下游任务（会议摘要）评估性能

Result: 1) 端到端ASR成为主流 2) 神经语音分离技术未达实用 3) 精准说话人计数决定日记化效果 4) 大语言模型显著缓解转录错误影响 5) 复杂场景下语音识别仍具挑战性

Conclusion: 当前技术虽在ASR集成和日记化优化取得进展，但神经分离技术可靠性、实时处理能力及复杂场景适应性仍需突破，未来需兼顾算法创新与计算效率提升

Abstract: The CHiME-7 and 8 distant speech recognition (DASR) challenges focus on
multi-channel, generalizable, joint automatic speech recognition (ASR) and
diarization of conversational speech. With participation from 9 teams
submitting 32 diverse systems, these challenges have contributed to
state-of-the-art research in the field. This paper outlines the challenges'
design, evaluation metrics, datasets, and baseline systems while analyzing key
trends from participant submissions. From this analysis it emerges that: 1)
Most participants use end-to-end (e2e) ASR systems, whereas hybrid systems were
prevalent in previous CHiME challenges. This transition is mainly due to the
availability of robust large-scale pre-trained models, which lowers the data
burden for e2e-ASR. 2) Despite recent advances in neural speech separation and
enhancement (SSE), all teams still heavily rely on guided source separation,
suggesting that current neural SSE techniques are still unable to reliably deal
with complex scenarios and different recording setups. 3) All best systems
employ diarization refinement via target-speaker diarization techniques.
Accurate speaker counting in the first diarization pass is thus crucial to
avoid compounding errors and CHiME-8 DASR participants especially focused on
this part. 4) Downstream evaluation via meeting summarization can correlate
weakly with transcription quality due to the remarkable effectiveness of
large-language models in handling errors. On the NOTSOFAR-1 scenario, even
systems with over 50\% time-constrained minimum permutation WER can perform
roughly on par with the most effective ones (around 11\%). 5) Despite recent
progress, accurately transcribing spontaneous speech in challenging acoustic
environments remains difficult, even when using computationally intensive
system ensembles.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [67] [GenSelect: A Generative Approach to Best-of-N](https://arxiv.org/abs/2507.17797)
*Shubham Toshniwal,Ivan Sorokin,Aleksander Ficek,Ivan Moshkov,Igor Gitman*

Main category: cs.LG

TL;DR: 提出GenSelect方法，通过大语言模型的长程推理能力从多个候选解中选出最优解，在数学推理任务中表现优于现有评分方法


<details>
  <summary>Details</summary>
Motivation: 现有单点评分法未能充分利用LLM的比较能力，成对比较法在大规模采样时效率低下。需要兼顾比较优势和计算效率的解决方案

Method: 使用推理模型（如QwQ和DeepSeek-R1-0528）对N个候选解进行长程推理选择，通过简单提示即可实现高效选择

Result: 在数学推理任务中验证了GenSelect的有效性，证明其优于传统评分方法且计算效率随采样规模提升更好

Conclusion: GenSelect成功结合了LLM的比较优势和并行采样的计算效率，为推理任务的扩展提供了新思路

Abstract: Generative reward models with parallel sampling have enabled effective
test-time scaling for reasoning tasks. Current approaches employ pointwise
scoring of individual solutions or pairwise comparisons. However, pointwise
methods underutilize LLMs' comparative abilities, while pairwise methods scale
inefficiently with larger sampling budgets. We introduce GenSelect, where the
LLM uses long reasoning to select the best solution among N candidates. This
leverages LLMs' comparative strengths while scaling efficiently across parallel
sampling budgets. For math reasoning, we demonstrate that reasoning models,
such as QwQ and DeepSeek-R1-0528, excel at GenSelect, outperforming existing
scoring approaches with simple prompting.

</details>


### [68] [Group Sequence Policy Optimization](https://arxiv.org/abs/2507.18071)
*Chujie Zheng,Shixuan Liu,Mingze Li,Xiong-Hui Chen,Bowen Yu,Chang Gao,Kai Dang,Yuqiong Liu,Rui Men,An Yang,Jingren Zhou,Junyang Lin*

Main category: cs.LG

TL;DR: Proposed Group Sequence Policy Optimization (GSPO) - a sequence-level RL algorithm that improves training efficiency and stability for large language models.


<details>
  <summary>Details</summary>
Motivation: Address limitations of token-level optimization in previous approaches by implementing sequence-level processing to enhance training stability and infrastructure efficiency.

Method: Defines importance ratio based on sequence likelihood, performs sequence-level clipping/rewarding/optimization instead of token-level operations.

Result: Outperforms GRPO in efficiency/performance, stabilizes MoE training, simplifies RL infrastructure design.

Conclusion: GSPO's sequence-level approach significantly enhances RL training effectiveness, contributing to breakthroughs in Qwen3 model capabilities.

Abstract: This paper introduces Group Sequence Policy Optimization (GSPO), our stable,
efficient, and performant reinforcement learning algorithm for training large
language models. Unlike previous algorithms that adopt token-level importance
ratios, GSPO defines the importance ratio based on sequence likelihood and
performs sequence-level clipping, rewarding, and optimization. We demonstrate
that GSPO achieves superior training efficiency and performance compared to the
GRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and
has the potential for simplifying the design of RL infrastructure. These merits
of GSPO have contributed to the remarkable improvements in the latest Qwen3
models.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [69] [LLM-based Embedders for Prior Case Retrieval](https://arxiv.org/abs/2507.18455)
*Damith Premasiri,Tharindu Ranasinghe,Ruslan Mitkov*

Main category: cs.IR

TL;DR: 研究利用支持长文本输入的LLM嵌入器解决法律案例检索中的文本长度限制和数据不足问题，在四个基准数据集上超越传统BM25和监督模型


<details>
  <summary>Details</summary>
Motivation: 传统法律案例检索依赖BM25方法，深度学习模型受限于文本长度截断和数据隐私导致的训练数据不足。LLM嵌入器既能处理长文本又无需训练数据，可同时解决两大瓶颈

Method: 采用LLM-based文本嵌入器进行无监督检索，支持长文本输入且不依赖训练数据。在四个PCR基准数据集上对比测试BM25、监督transformer模型和LLM嵌入器的效果

Result: LLM-based嵌入器在全部四个基准测试中表现最优，性能超越BM25平均提升23%，较监督transformer模型提高15%

Conclusion: LLM嵌入器为法律案例检索提供新范式，突破传统深度学习方法限制，未来可扩展应用于其他长文本检索场景

Abstract: In common law systems, legal professionals such as lawyers and judges rely on
precedents to build their arguments. As the volume of cases has grown massively
over time, effectively retrieving prior cases has become essential. Prior case
retrieval (PCR) is an information retrieval (IR) task that aims to
automatically identify the most relevant court cases for a specific query from
a large pool of potential candidates. While IR methods have seen several
paradigm shifts over the last few years, the vast majority of PCR methods
continue to rely on traditional IR methods, such as BM25. The state-of-the-art
deep learning IR methods have not been successful in PCR due to two key
challenges: i. Lengthy legal text limitation; when using the powerful
BERT-based transformer models, there is a limit of input text lengths, which
inevitably requires to shorten the input via truncation or division with a loss
of legal context information. ii. Lack of legal training data; due to data
privacy concerns, available PCR datasets are often limited in size, making it
difficult to train deep learning-based models effectively. In this research, we
address these challenges by leveraging LLM-based text embedders in PCR.
LLM-based embedders support longer input lengths, and since we use them in an
unsupervised manner, they do not require training data, addressing both
challenges simultaneously. In this paper, we evaluate state-of-the-art
LLM-based text embedders in four PCR benchmark datasets and show that they
outperform BM25 and supervised transformer-based models.

</details>


### [70] [DR.EHR: Dense Retrieval for Electronic Health Record with Knowledge Injection and Synthetic Data](https://arxiv.org/abs/2507.18583)
*Zhengyun Zhao,Huaiyuan Ying,Yue Zhong,Sheng Yu*

Main category: cs.IR

TL;DR: 提出专门针对电子健康记录检索的稠密检索模型DR.EHR，通过两阶段训练显著提升临床文档检索效果


<details>
  <summary>Details</summary>
Motivation: 现有通用/生物医学领域检索模型因医学知识不足或训练语料不匹配，无法满足电子健康记录(EHR)的语义检索需求

Method: 1. 两阶段训练框架：第一阶段通过医疗实体抽取和知识图谱注入医学知识；第二阶段利用大模型生成多样化训练数据
2. 开发110M和7B两种参数规模的模型

Result: 在CliniQ基准测试中显著超越现有模型(SOTA)，尤其在语义匹配（暗示/缩写）场景表现突出；消融实验验证各模块有效性，QA测试展现自然语言问题泛化能力

Conclusion: DR.EHR系列模型通过知识注入和大规模数据生成，为临床文档检索提供了更鲁棒的解决方案，推动医疗信息检索领域发展

Abstract: Electronic Health Records (EHRs) are pivotal in clinical practices, yet their
retrieval remains a challenge mainly due to semantic gap issues. Recent
advancements in dense retrieval offer promising solutions but existing models,
both general-domain and biomedical-domain, fall short due to insufficient
medical knowledge or mismatched training corpora. This paper introduces
\texttt{DR.EHR}, a series of dense retrieval models specifically tailored for
EHR retrieval. We propose a two-stage training pipeline utilizing MIMIC-IV
discharge summaries to address the need for extensive medical knowledge and
large-scale training data. The first stage involves medical entity extraction
and knowledge injection from a biomedical knowledge graph, while the second
stage employs large language models to generate diverse training data. We train
two variants of \texttt{DR.EHR}, with 110M and 7B parameters, respectively.
Evaluated on the CliniQ benchmark, our models significantly outperforms all
existing dense retrievers, achieving state-of-the-art results. Detailed
analyses confirm our models' superiority across various match and query types,
particularly in challenging semantic matches like implication and abbreviation.
Ablation studies validate the effectiveness of each pipeline component, and
supplementary experiments on EHR QA datasets demonstrate the models'
generalizability on natural language questions, including complex ones with
multiple entities. This work significantly advances EHR retrieval, offering a
robust solution for clinical applications.

</details>
