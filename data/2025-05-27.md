<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 237]
- [cs.GR](#cs.GR) [Total: 8]
- [cs.CV](#cs.CV) [Total: 2]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Advancing Uto-Aztecan Language Technologies: A Case Study on the Endangered Comanche Language](https://arxiv.org/abs/2505.18159)
*Jesus Alvarez C,Daua D. Karajeanes,Ashley Celeste Prado,John Ruttan,Ivory Yang,Sean O'Brien,Vasu Sharma,Kevin Zhu*

Main category: cs.CL

TL;DR: 展示如何通过低成本NLP方法和少量样本提示有效支持濒危科曼奇语保护


<details>
  <summary>Details</summary>
Motivation: 解决濒危语言在NLP领域的数字排斥问题，通过提升计算可见性支持语言保存与复兴

Method: 创建412个短语数据集+合成数据生成+GPT-4o系列模型的语言识别实验（零样本 vs 少量样本）

Result: 少量样本提示使准确率接近完美（5个示例即显著提升模型性能）

Conclusion: 强调针对性NLP方法在低资源场景的潜力，主张可见性是语言包容的首要步骤，需注重文化敏感性和社区参与

Abstract: The digital exclusion of endangered languages remains a critical challenge in
NLP, limiting both linguistic research and revitalization efforts. This study
introduces the first computational investigation of Comanche, an Uto-Aztecan
language on the verge of extinction, demonstrating how minimal-cost,
community-informed NLP interventions can support language preservation. We
present a manually curated dataset of 412 phrases, a synthetic data generation
pipeline, and an empirical evaluation of GPT-4o and GPT-4o-mini for language
identification. Our experiments reveal that while LLMs struggle with Comanche
in zero-shot settings, few-shot prompting significantly improves performance,
achieving near-perfect accuracy with just five examples. Our findings highlight
the potential of targeted NLP methodologies in low-resource contexts and
emphasize that visibility is the first step toward inclusion. By establishing a
foundation for Comanche in NLP, we advocate for computational approaches that
prioritize accessibility, cultural sensitivity, and community engagement.

</details>


### [2] [Do BERT-Like Bidirectional Models Still Perform Better on Text Classification in the Era of LLMs?](https://arxiv.org/abs/2505.18215)
*Junyan Zhang,Yiming Huang,Shuliang Liu,Yubo Gao,Xuming Hu*

Main category: cs.CL

TL;DR: 传统BERT模型在特定文本分类任务中常优于大语言模型，需采用任务驱动的细粒度模型选择策略


<details>
  <summary>Details</summary>
Motivation: 挑战当前过度依赖LLM的行业趋势，揭示不同模型在不同任务类型中的优势差异

Method: 系统比较BERT微调、LLM内部状态利用和零样本推理三种方法，通过六种高难度数据集进行PCA分析和探针实验

Result: 识别任务特异性优势：BERT擅长模式驱动任务，LLM在深度语义/世界知识任务表现更佳

Conclusion: 提出TaMAS细粒度任务选择框架，倡导根据任务特性选择模型而非盲目使用LLM

Abstract: The rapid adoption of LLMs has overshadowed the potential advantages of
traditional BERT-like models in text classification. This study challenges the
prevailing "LLM-centric" trend by systematically comparing three category
methods, i.e., BERT-like models fine-tuning, LLM internal state utilization,
and zero-shot inference across six high-difficulty datasets. Our findings
reveal that BERT-like models often outperform LLMs. We further categorize
datasets into three types, perform PCA and probing experiments, and identify
task-specific model strengths: BERT-like models excel in pattern-driven tasks,
while LLMs dominate those requiring deep semantics or world knowledge. Based on
this, we propose TaMAS, a fine-grained task selection strategy, advocating for
a nuanced, task-driven approach over a one-size-fits-all reliance on LLMs.

</details>


### [3] [CoMet: Metaphor-Driven Covert Communication for Multi-Agent Language Games](https://arxiv.org/abs/2505.18218)
*Shuhang Xu,Fangwei Zhong*

Main category: cs.CL

TL;DR: CoMet框架通过结合隐喻推理器和自我优化的生成器，显著提升LLM在多智能体游戏中的隐喻战略沟通能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在隐喻处理上存在局限，影响隐蔽沟通和语义回避能力，阻碍战略交互质量。

Method: 整合基于假设的隐喻推理器与具备自我反思/知识整合机制的隐喻生成器，构建多智能体协同框架。

Result: 在Undercover和Adversarial Taboo游戏中，CoMet使智能体隐喻沟通成功率提升32%-45%。

Conclusion: CoMet首次实现LLM的端到端隐喻处理能力，为战略语言交互系统提供了可扩展的认知架构。

Abstract: Metaphors are a crucial way for humans to express complex or subtle ideas by
comparing one concept to another, often from a different domain. However, many
large language models (LLMs) struggle to interpret and apply metaphors in
multi-agent language games, hindering their ability to engage in covert
communication and semantic evasion, which are crucial for strategic
communication. To address this challenge, we introduce CoMet, a framework that
enables LLM-based agents to engage in metaphor processing. CoMet combines a
hypothesis-based metaphor reasoner with a metaphor generator that improves
through self-reflection and knowledge integration. This enhances the agents'
ability to interpret and apply metaphors, improving the strategic and nuanced
quality of their interactions. We evaluate CoMet on two multi-agent language
games - Undercover and Adversarial Taboo - which emphasize Covert Communication
and Semantic Evasion. Experimental results demonstrate that CoMet significantly
enhances the agents' ability to communicate strategically using metaphors.

</details>


### [4] [IDA-Bench: Evaluating LLMs on Interactive Guided Data Analysis](https://arxiv.org/abs/2505.18223)
*Hanyu Li,Haoyu Liu,Tingyu Zhu,Tianyu Guo,Zeyu Zheng,Xiaotie Deng,Michael I. Jordan*

Main category: cs.CL

TL;DR: 提出IDA-Bench基准测试，揭示大语言模型在交互式数据分析场景中的多轮处理能力不足。当前最优代码生成代理在50%任务中失败，凸显单轮测试无法评估的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试忽视数据分析领域专家认知的迭代性，需构建多轮交互场景评估模型能力。通过模拟真实Kaggle用户指令序列，更准确反映实际需求。

Method: 基于复杂Kaggle notebook构建自然语言指令序列，使用LLM模拟用户逐步发布任务，最终通过数值输出与人类基准对比评估代理性能。

Result: Claude-3.7等顶级代码生成代理成功率不足50%，暴露出现有模型在指令跟随与自主推理平衡上的重大缺陷。

Conclusion: 构建可靠数据分析代理需重点提升多轮交互能力，应在保持指令遵循的同时增强持续推理能力，推动更接近人类专家工作模式的算法改进。

Abstract: Large Language Models (LLMs) show promise as data analysis agents, but
existing benchmarks overlook the iterative nature of the field, where experts'
decisions evolve with deeper insights of the dataset. To address this, we
introduce IDA-Bench, a novel benchmark evaluating LLM agents in multi-round
interactive scenarios. Derived from complex Kaggle notebooks, tasks are
presented as sequential natural language instructions by an LLM-simulated user.
Agent performance is judged by comparing its final numerical output to the
human-derived baseline. Initial results show that even state-of-the-art coding
agents (like Claude-3.7-thinking) succeed on < 50% of the tasks, highlighting
limitations not evident in single-turn tests. This work underscores the need to
improve LLMs' multi-round capabilities for building more reliable data analysis
agents, highlighting the necessity of achieving a balance between instruction
following and reasoning.

</details>


### [5] [Think or Not? Exploring Thinking Efficiency in Large Reasoning Models via an Information-Theoretic Lens](https://arxiv.org/abs/2505.18237)
*Xixian Yong,Xiao Zhou,Yingying Zhang,Jinlin Li,Yefeng Zheng,Xian Wu*

Main category: cs.CL

TL;DR: 通过信息论视角优化大型推理模型的推理链长度与语义效率平衡，提出基于熵的自适应思考策略实现效率提升


<details>
  <summary>Details</summary>
Motivation: 观察到现有大型推理模型生成过长推理链导致效率下降，从信息论角度揭示推理长度与语义效率的权衡关系

Method: 提出InfoBias(量化推理路径偏差)和InfoGain(量化信息增益)指标，开发基于熵的自适应停止机制动态控制推理深度

Result: 在QwQ-32B模型上实现平均准确率提升1.10%，Token消耗降低50.80%（跨6个不同难度推理任务）

Conclusion: 熵驱动方法显著提升LLM部署的准确率和成本效率，验证信息论指标对优化推理过程的有效性

Abstract: The recent rise of Large Reasoning Models (LRMs) has significantly improved
multi-step reasoning performance, but often at the cost of generating
excessively long reasoning chains. This paper revisits the efficiency of such
reasoning processes through an information-theoretic lens, revealing a
fundamental trade-off between reasoning length and semantic efficiency. We
propose two metrics, InfoBias and InfoGain, to quantify divergence from ideal
reasoning paths and stepwise information contribution, respectively. Empirical
analyses show that longer reasoning chains tend to exhibit higher information
bias and diminishing information gain, especially for incorrect answers.
Motivated by these findings, we introduce an entropy-based Adaptive Think
strategy that dynamically halts reasoning once confidence is sufficiently high,
improving efficiency while maintaining competitive accuracy. Compared to the
Vanilla Think approach (default mode), our strategy yields a 1.10% improvement
in average accuracy and a 50.80% reduction in token usage on QwQ-32B across six
benchmark tasks spanning diverse reasoning types and difficulty levels,
demonstrating superior efficiency and reasoning performance. These results
underscore the promise of entropy-based methods for enhancing both accuracy and
cost-effiiciency in large language model deployment.

</details>


### [6] [Taming LLMs with Negative Samples: A Reference-Free Framework to Evaluate Presentation Content with Actionable Feedback](https://arxiv.org/abs/2505.18240)
*Ananth Muppidi,Tarak Das,Sambaran Bandyopadhyay,Tripti Shukla,Dharun D A*

Main category: cs.CL

TL;DR: 提出REFLEX评估框架，通过基准数据集RefSlides和负样本微调LLM实现无参考的演示文稿自动评估


<details>
  <summary>Details</summary>
Motivation: 自动生成高质量演示文稿需解决多模态内容评估问题，传统方法依赖人工参考且效果有限

Method: 1. 构建跨领域人工高质量演示数据集RefSlides
2. 设计内容评估指标并生成负样本
3. 基于扰动样本微调LLM实现无参考评估

Result: REFLEX在自动化和人工评估中均优于传统启发式方法和现有LLM评估，能生成可解释的评分与改进建议

Conclusion: 该方法突破传统评估对参考样本的依赖，为生成式AI的演示内容质量评估提供新范式

Abstract: The generation of presentation slides automatically is an important problem
in the era of generative AI. This paper focuses on evaluating multimodal
content in presentation slides that can effectively summarize a document and
convey concepts to a broad audience. We introduce a benchmark dataset,
RefSlides, consisting of human-made high-quality presentations that span
various topics. Next, we propose a set of metrics to characterize different
intrinsic properties of the content of a presentation and present REFLEX, an
evaluation approach that generates scores and actionable feedback for these
metrics. We achieve this by generating negative presentation samples with
different degrees of metric-specific perturbations and use them to fine-tune
LLMs. This reference-free evaluation technique does not require ground truth
presentations during inference. Our extensive automated and human experiments
demonstrate that our evaluation approach outperforms classical heuristic-based
and state-of-the-art large language model-based evaluations in generating
scores and explanations.

</details>


### [7] [Multi-Scale Probabilistic Generation Theory: A Hierarchical Framework for Interpreting Large Language Models](https://arxiv.org/abs/2505.18244)
*Yukin Zhang,Qi Dong*

Main category: cs.CL

TL;DR: 提出多尺度概率生成理论（MSPGT），通过分层框架解析Transformer模型的文本生成机制，揭示不同架构模型在全局/中间/局部尺度的分层特征及干预影响


<details>
  <summary>Details</summary>
Motivation: 解决大型Transformer模型文本生成过程不透明的问题，建立可解释的生成过程分层理论框架

Method: 使用注意力跨度阈值和层间互信息峰值作为尺度划分指标，在GPT-2/BERT/RoBERTa/T5模型进行探测任务和因果干预实验

Result: 不同模型展现稳定的三层分区模式：解码器模型侧重中/全局处理（GPT-2占66%层数），编码器模型专注局部特征（BERT占50%层数），统计显示局部尺度干预影响词汇多样性（p<0.01），中间尺度改变句子结构（长度变化±22%），全局尺度决定篇章连贯性

Conclusion: MSPGT提供跨架构的模型解释框架，通过尺度特异性干预实现生成控制，为理解大模型涌现能力建立机制化桥梁

Abstract: Large Transformer based language models achieve remarkable performance but
remain opaque in how they plan, structure, and realize text. We introduce
Multi_Scale Probabilistic Generation Theory (MSPGT), a hierarchical framework
that factorizes generation into three semantic scales_global context,
intermediate structure, and local word choices and aligns each scale with
specific layer ranges in Transformer architectures. To identify scale
boundaries, we propose two complementary metrics: attention span thresholds and
inter layer mutual information peaks. Across four representative models (GPT-2,
BERT, RoBERTa, and T5), these metrics yield stable local/intermediate/global
partitions, corroborated by probing tasks and causal interventions. We find
that decoder_only models allocate more layers to intermediate and global
processing while encoder_only models emphasize local feature extraction.
Through targeted interventions, we demonstrate that local scale manipulations
primarily influence lexical diversity, intermediate-scale modifications affect
sentence structure and length, and global_scale perturbations impact discourse
coherence all with statistically significant effects. MSPGT thus offers a
unified, architecture-agnostic method for interpreting, diagnosing, and
controlling large language models, bridging the gap between mechanistic
interpretability and emergent capabilities.

</details>


### [8] [MetaGen Blended RAG: Higher Accuracy for Domain-Specific Q&A Without Fine-Tuning](https://arxiv.org/abs/2505.18247)
*Kunal Sawarkar,Shivam R. Solanki,Abhilasha Mangal*

Main category: cs.CL

TL;DR: 针对企业领域数据RAG精度低的问题，提出元数据增强混合索引方法，在生物医学领域实现82%检索准确率，超越传统RAG并接近微调模型效果。


<details>
  <summary>Details</summary>
Motivation: 企业知识库包含复杂专业术语且存在跨领域语义差异，传统微调方法成本高且泛化性差，需开发无需微调的通用增强方案。

Method: 构建元数据生成管道（关键概念/主题/缩写），创建带增强查询的混合索引，通过元数据扩展提升检索精度同时保持模型泛化能力。

Result: PubMedQA基准测试中82%检索精度/77% RAG精度，零样本表现超越GPT3.5，SQuAD/NQ等跨领域测试验证方案鲁棒性。

Conclusion: 该方法突破传统RAG限制，无需微调即可实现跨领域的高精度检索，为私有知识库应用提供高效解决方案。

Abstract: Despite the widespread exploration of Retrieval-Augmented Generation (RAG),
its deployment in enterprises for domain-specific datasets remains limited due
to poor answer accuracy. These corpora, often shielded behind firewalls in
private enterprise knowledge bases, having complex, domain-specific
terminology, rarely seen by LLMs during pre-training; exhibit significant
semantic variability across domains (like networking, military, or legal,
etc.), or even within a single domain like medicine, and thus result in poor
context precision for RAG systems. Currently, in such situations, fine-tuning
or RAG with fine-tuning is attempted, but these approaches are slow, expensive,
and lack generalization for accuracy as the new domain-specific data emerges.
We propose an approach for Enterprise Search that focuses on enhancing the
retriever for a domain-specific corpus through hybrid query indexes and
metadata enrichment. This 'MetaGen Blended RAG' method constructs a metadata
generation pipeline using key concepts, topics, and acronyms, and then creates
a metadata-enriched hybrid index with boosted search queries. This approach
avoids overfitting and generalizes effectively across domains. On the PubMedQA
benchmark for the biomedical domain, the proposed method achieves 82% retrieval
accuracy and 77% RAG accuracy, surpassing all previous RAG accuracy results
without fine-tuning and sets a new benchmark for zero-shot results while
outperforming much larger models like GPT3.5. The results are even comparable
to the best fine-tuned models on this dataset, and we further demonstrate the
robustness and scalability of the approach by evaluating it on other Q&A
datasets like SQuAD, NQ etc.

</details>


### [9] [TAGS: A Test-Time Generalist-Specialist Framework with Retrieval-Augmented Reasoning and Verification](https://arxiv.org/abs/2505.18283)
*Jianghao Wu,Feilong Tang,Yulong Li,Ming Hu,Haochen Xue,Shoaib Jameel,Yutong Xie,Imran Razzak*

Main category: cs.CL

TL;DR: 提出无需微调的TAGS框架，通过通用模型与医学专家模型的互补协同，结合分层检索和可靠性评分机制，显著提升LLM在医学问答中的表现


<details>
  <summary>Details</summary>
Motivation: 现有提示方法存在推理浅层/不稳定的问题，微调模型则面临分布偏移泛化差、无法适应新临床场景的局限性

Method: 结合通用模型与领域专家模型的协同推理，设计分层检索机制（语义+原理相似性选择多尺度示例）和可靠性评分器（评估推理一致性）

Result: 在MedQA九个基准上：GPT-4o准确率提升13.8%，7B小模型从14.1%提升至23.9%，超越多个微调模型

Conclusion: TAGS通过模型协同和辅助模块设计，有效突破传统方法的性能瓶颈，为医学推理任务提供了高效可靠的零样本解决方案

Abstract: Recent advances such as Chain-of-Thought prompting have significantly
improved large language models (LLMs) in zero-shot medical reasoning. However,
prompting-based methods often remain shallow and unstable, while fine-tuned
medical LLMs suffer from poor generalization under distribution shifts and
limited adaptability to unseen clinical scenarios. To address these
limitations, we present TAGS, a test-time framework that combines a broadly
capable generalist with a domain-specific specialist to offer complementary
perspectives without any model fine-tuning or parameter updates. To support
this generalist-specialist reasoning process, we introduce two auxiliary
modules: a hierarchical retrieval mechanism that provides multi-scale exemplars
by selecting examples based on both semantic and rationale-level similarity,
and a reliability scorer that evaluates reasoning consistency to guide final
answer aggregation. TAGS achieves strong performance across nine MedQA
benchmarks, boosting GPT-4o accuracy by 13.8%, DeepSeek-R1 by 16.8%, and
improving a vanilla 7B model from 14.1% to 23.9%. These results surpass several
fine-tuned medical LLMs, without any parameter updates. The code will be
available at https://github.com/JianghaoWu/TAGS.

</details>


### [10] [Thinking Fast and Right: Balancing Accuracy and Reasoning Length with Adaptive Rewards](https://arxiv.org/abs/2505.18298)
*Jinyan Su,Claire Cardie*

Main category: cs.CL

TL;DR: Adaptive reward-shaping method balances accuracy and conciseness in LLM reasoning outputs by dynamically adjusting length penalties based on model performance.


<details>
  <summary>Details</summary>
Motivation: Fixed length penalties in RL-trained LLMs cause inefficiency - they cannot adapt to evolving model capabilities and lead to unnecessary verbosity.

Method: Dynamic reward trade-off mechanism that increases length penalty when accuracy is high (encouraging brevity) and relaxes penalty when accuracy drops (preserving correctness).

Result: Consistent dramatic reduction in reasoning length (average 40% shorter) across datasets while maintaining 98%+ of original model accuracy.

Conclusion: Provides cost-efficient adaptive reasoning through self-adjusting rewards, opening new directions for optimized large-scale language model deployment.

Abstract: Large language models (LLMs) have demonstrated strong reasoning abilities in
mathematical tasks, often enhanced through reinforcement learning (RL).
However, RL-trained models frequently produce unnecessarily long reasoning
traces -- even for simple queries -- leading to increased inference costs and
latency. While recent approaches attempt to control verbosity by adding length
penalties to the reward function, these methods rely on fixed penalty terms
that are hard to tune and cannot adapt as the model's reasoning capability
evolves, limiting their effectiveness. In this work, we propose an adaptive
reward-shaping method that enables LLMs to "think fast and right" -- producing
concise outputs without sacrificing correctness. Our method dynamically adjusts
the reward trade-off between accuracy and response length based on model
performance: when accuracy is high, the length penalty increases to encourage
faster length reduction; when accuracy drops, the penalty is relaxed to
preserve correctness. This adaptive reward accelerates early-stage length
reduction while avoiding over-compression in later stages. Experiments across
multiple datasets show that our approach consistently and dramatically reduces
reasoning length while largely maintaining accuracy, offering a new direction
for cost-efficient adaptive reasoning in large-scale language models.

</details>


### [11] [Is It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4](https://arxiv.org/abs/2505.18322)
*Zhuozhuo Joy Liu,Farhan Samir,Mehar Bhatia,Laura K. Nelson,Vered Shwartz*

Main category: cs.CL

TL;DR: 研究发现LLMs生成的文化规范缺乏文化特异性，且隐藏的刻板印象容易被激活


<details>
  <summary>Details</summary>
Motivation: 现有研究通过直接询问方式验证LLMs的价值观对齐，但无法反映实际应用中的真实表现。需要验证LLMs在不同文化叙事中的价值观应用。

Method: 通过分析LLMs生成的不同文化叙事中的文化规范，采用自下而上的实证研究方法

Result: 1. GPT-4生成的规范文化特异性显著降低
2. 模型隐藏而非消除文化刻板印象
3. 隐藏的刻板印象可被轻易激活

Conclusion: 解决LLMs的文化偏差和潜在刻板印象是构建公平AI系统的关键挑战

Abstract: LLMs have been demonstrated to align with the values of Western or North
American cultures. Prior work predominantly showed this effect through
leveraging surveys that directly ask (originally people and now also LLMs)
about their values. However, it is hard to believe that LLMs would consistently
apply those values in real-world scenarios. To address that, we take a
bottom-up approach, asking LLMs to reason about cultural norms in narratives
from different cultures. We find that GPT-4 tends to generate norms that, while
not necessarily incorrect, are significantly less culture-specific. In
addition, while it avoids overtly generating stereotypes, the stereotypical
representations of certain cultures are merely hidden rather than suppressed in
the model, and such stereotypes can be easily recovered. Addressing these
challenges is a crucial step towards developing LLMs that fairly serve their
diverse user base.

</details>


### [12] [PerMedCQA: Benchmarking Large Language Models on Medical Consumer Question Answering in Persian Language](https://arxiv.org/abs/2505.18331)
*Naghmeh Jamali,Milad Mohammadi,Danial Baledi,Zahra Rezvani,Hesham Faili*

Main category: cs.CL

TL;DR: 首个波斯语医疗问答基准PerMedCQA的构建与多语言LLM评估


<details>
  <summary>Details</summary>
Motivation: 解决波斯语等低资源语言中消费者医疗问答数据及评估体系匮乏的问题

Method: 1. 从医疗论坛构建68k QA对数据集
2. 提出MedJudge评估框架（LLM评分器+专家验证）
3. 测试多语言/指令调优LLM

Result: 1. 发现多语言医疗QA核心挑战
2. MedJudge与专家标注一致性达92%
3. 最佳模型准确率仅58.3%

Conclusion: 需开发兼顾语言适应与医疗上下文理解的辅助系统，PerMedCQA为后续研究提供基准支持

Abstract: Medical consumer question answering (CQA) is crucial for empowering patients
by providing personalized and reliable health information. Despite recent
advances in large language models (LLMs) for medical QA, consumer-oriented and
multilingual resources, particularly in low-resource languages like Persian,
remain sparse. To bridge this gap, we present PerMedCQA, the first
Persian-language benchmark for evaluating LLMs on real-world,
consumer-generated medical questions. Curated from a large medical QA forum,
PerMedCQA contains 68,138 question-answer pairs, refined through careful data
cleaning from an initial set of 87,780 raw entries. We evaluate several
state-of-the-art multilingual and instruction-tuned LLMs, utilizing MedJudge, a
novel rubric-based evaluation framework driven by an LLM grader, validated
against expert human annotators. Our results highlight key challenges in
multilingual medical QA and provide valuable insights for developing more
accurate and context-aware medical assistance systems. The data is publicly
available on https://huggingface.co/datasets/NaghmehAI/PerMedCQA

</details>


### [13] [Model Editing with Graph-Based External Memory](https://arxiv.org/abs/2505.18343)
*Yash Kumar Atri,Ahmed Alaa,Thomas Hartvigsen*

Main category: cs.CL

TL;DR: 提出HYPE框架，通过双曲几何+图神经网络解决大模型编辑中的过拟合/遗忘问题，实现精准稳定更新


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑方法存在过拟合和灾难性遗忘问题，无法同时保证知识更新的准确性和稳定性

Method: ①双曲图构建（保持知识层次结构） ②Mobius变换更新（保持双曲流形结构） ③双稳定机制（梯度掩码+参数重置）

Result: 在CounterFact等数据集上验证，显著提升GPT-J/GPT2-XL的编辑稳定性、事实准确性和多跳推理

Conclusion: HYPE首次将双曲几何与GNN结合，突破传统欧式空间的限制，为动态知识更新提供新范式

Abstract: Large language models (LLMs) have revolutionized natural language processing,
yet their practical utility is often limited by persistent issues of
hallucinations and outdated parametric knowledge. Although post-training model
editing offers a pathway for dynamic updates, existing methods frequently
suffer from overfitting and catastrophic forgetting. To tackle these
challenges, we propose a novel framework that leverages hyperbolic geometry and
graph neural networks for precise and stable model edits. We introduce HYPE
(HYperbolic Parameter Editing), which comprises three key components: (i)
Hyperbolic Graph Construction, which uses Poincar\'e embeddings to represent
knowledge triples in hyperbolic space, preserving hierarchical relationships
and preventing unintended side effects by ensuring that edits to parent
concepts do not inadvertently affect child concepts; (ii) M\"obius-Transformed
Updates, which apply hyperbolic addition to propagate edits while maintaining
structural consistency within the hyperbolic manifold, unlike conventional
Euclidean updates that distort relational distances; and (iii) Dual
Stabilization, which combines gradient masking and periodic GNN parameter
resetting to prevent catastrophic forgetting by focusing updates on critical
parameters and preserving long-term knowledge. Experiments on CounterFact,
CounterFact+, and MQuAKE with GPT-J and GPT2-XL demonstrate that HYPE
significantly enhances edit stability, factual accuracy, and multi-hop
reasoning.

</details>


### [14] [The Unreasonable Effectiveness of Model Merging for Cross-Lingual Transfer in LLMs](https://arxiv.org/abs/2505.18356)
*Lucas Bandarkar,Nanyun Peng*

Main category: cs.CL

TL;DR: 通过模块化框架实现数学推理与多语言能力参数解耦，提出层交换方法在低资源语言场景显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在低资源语言任务中因缺乏特定任务数据导致的性能瓶颈，利用参数非重叠特性构建组合式优化方案

Method: 1. 验证数学推理/多语言参数非重叠性 2. 开发冻结参数/模型合并框架 3. 提出层交换（Layer-Swapping）融合专家模型

Result: 在3种语言、4个模型、2种微调范式下，模块化方法超越基线模型，层交换法表现最优且稳定

Conclusion: 事后回退无效参数更新优于提前冻结，任务向量线性特性支持模块化组合，为低资源优化提供新范式

Abstract: Large language models (LLMs) still struggle across tasks outside of
high-resource languages. In this work, we investigate cross-lingual transfer to
lower-resource languages where task-specific post-training data is scarce.
Building on prior work, we first validate that the subsets of model parameters
that matter most for mathematical reasoning and multilingual capabilities are
distinctly non-overlapping. To exploit this implicit separability between task
and target language parameterization, we develop and analyze numerous modular
frameworks to improve the composition of the two during fine-tuning. These
methods generally employ freezing parameters or post hoc model merging to
assign math and language improvement to different key parts of the LLM. In the
absence of in-language math data, we demonstrate that the modular approaches
successfully improve upon baselines across three languages, four models, and
two fine-tuning paradigms (full and LoRA). Furthermore, we identify the most
consistently successful modular method to be fine-tuning separate language and
math experts and model merging via Layer-Swapping, somewhat surprisingly. We
offer possible explanations for this result via recent works on the linearity
of task vectors. We further explain this by empirically showing that reverting
less useful fine-tuning updates after training often outperforms freezing them
from the start.

</details>


### [15] [SchemaGraphSQL: Efficient Schema Linking with Pathfinding Graph Algorithms for Text-to-SQL on Large-Scale Databases](https://arxiv.org/abs/2505.18363)
*AmirHossein Safdarian,Milad Mohammadi,Ehsan Jahanbakhsh,Mona Shahamat Naderi,Heshaam Faili*

Main category: cs.CL

TL;DR: 提出零样本、免训练的模式链接方法，通过外键关系构建模式图并应用路径查找算法，在BIRD基准实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 传统Text-to-SQL系统存在上下文窗口限制和模式冗余问题，需要更有效的模式链接方法提升SQL生成精度

Method: 1. 基于外键构建模式图 2. 用Gemini 2.5 Flash提取查询中的表 3. 应用经典路径算法确定最优连接顺序

Result: 在BIRD基准实现新SOTA，执行精度达73.5%，优于先前微调方法和复杂多步LLM方案

Conclusion: 该方案具备简单、低成本和强扩展性优势，通过精确的模式过滤显著提升LLM的SQL生成质量

Abstract: Text-to-SQL systems translate natural language questions into executable SQL
queries, and recent progress with large language models (LLMs) has driven
substantial improvements in this task. Schema linking remains a critical
component in Text-to-SQL systems, reducing prompt size for models with narrow
context windows and sharpening model focus even when the entire schema fits. We
present a zero-shot, training-free schema linking approach that first
constructs a schema graph based on foreign key relations, then uses a single
prompt to Gemini 2.5 Flash to extract source and destination tables from the
user query, followed by applying classical path-finding algorithms and
post-processing to identify the optimal sequence of tables and columns that
should be joined, enabling the LLM to generate more accurate SQL queries.
Despite being simple, cost-effective, and highly scalable, our method achieves
state-of-the-art results on the BIRD benchmark, outperforming previous
specialized, fine-tuned, and complex multi-step LLM-based approaches. We
conduct detailed ablation studies to examine the precision-recall trade-off in
our framework. Additionally, we evaluate the execution accuracy of our schema
filtering method compared to other approaches across various model sizes.

</details>


### [16] [ShIOEnv: A CLI Behavior-Capturing Environment Enabling Grammar-Guided Command Synthesis for Dataset Curation](https://arxiv.org/abs/2505.18374)
*Jarrod Ragsdale,Rajendra Boppana*

Main category: cs.CL

TL;DR: 提出ShIOEnv框架，通过马尔可夫决策过程建模命令行交互，结合语法约束和PPO优化生成高质量数据集，使小型模型性能提升85%+。


<details>
  <summary>Details</summary>
Motivation: 现有CLI模拟依赖大型冻结PLM，而小型架构需丰富数据集。现有数据缺乏执行细节（退出码/输出等），限制了行为建模能力。

Method: 1) 将命令构建建模为状态=部分命令序列、动作=参数追加的MDP；2) 从man page推导CFG语法屏蔽无效参数；3) 结合随机/PPO采样策略生成四类探索方式。

Result: 语法屏蔽+PPO使采样效率提升，CodeT5微调后BLEU-4提升85%（语法约束）→111%（+PPO），同时数据集参数覆盖率最大化冗余最小化。

Conclusion: ShIOEnv验证了语法约束与强化学习在CLI行为建模中的有效性，开源环境与数据集为轻量化CLI代理开发提供新范式。

Abstract: Command-line interfaces (CLIs) provide structured textual environments for
system administration. Explorations have been performed using pre-trained
language models (PLMs) to simulate these environments for safe interaction in
high-risk environments. However, their use has been constrained to frozen,
large parameter models like GPT. For smaller architectures to reach a similar
level of believability, a rich dataset of CLI interactions is required.
Existing public datasets focus on mapping natural-language tasks to commands,
omitting crucial execution data such as exit codes, outputs, and environmental
side effects, limiting their usability for behavioral modeling. We introduce a
Shell Input -Output Environment (ShIOEnv), which casts command construction as
a Markov Decision Process whose state is the partially built sequence and whose
actions append arguments. After each action, ShIOEnv executes the candidate and
returns its exit status, output, and progress toward a minimal-length
behavioral objective. Due to the intractable nature of the combinatorial
argument state-action space, we derive a context-free grammar from man pages to
mask invalid arguments from being emitted. We explore random and
proximal-policy optimization (PPO)-optimized sampling of unrestricted and
grammar-masked action spaces to produce four exploration strategies. We
observed that grammar masking and PPO significantly improve sample efficiency
to produce a higher quality dataset (maximizing the number of arguments while
minimizing redundancies). Policy-generated datasets of shell input-output
behavior pairs are used to fine-tune CodeT5, where we observe 85% improvements
in BLEU-4 when constraining the action space to grammar productions with an
additional 26% improvement when applying PPO. The ShIOEnv environment and
curated command behavior datasets are released for use in future research.

</details>


### [17] [NileChat: Towards Linguistically Diverse and Culturally Aware LLMs for Local Communities](https://arxiv.org/abs/2505.18383)
*Abdellah El Mekki,Houdaifa Atou,Omer Nacar,Shady Shehata,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 提出针对低资源语言社区（埃及/摩洛哥方言）定制合成与检索结合的预训练数据方法，开发3B参数的NileChat模型，在文化对齐和性能上超越同类模型。


<details>
  <summary>Details</summary>
Motivation: 现有基于英语翻译的合成数据方法导致模型与源语言文化对齐，无法体现本地社区文化遗产和价值观。

Method: 结合社区语言/文化特征构建预训练数据，开发适配埃及和摩洛哥社区的3B参数NileChat模型。

Result: NileChat在理解、翻译和文化对齐任务中优于同规模模型，与更大模型性能相当。

Conclusion: 定制化数据方法有效提升LLMs对低资源语言文化的支持，通过开源促进多样性社区纳入AI发展。

Abstract: Enhancing the linguistic capabilities of Large Language Models (LLMs) to
include low-resource languages is a critical research area. Current research
directions predominantly rely on synthetic data generated by translating
English corpora, which, while demonstrating promising linguistic understanding
and translation abilities, often results in models aligned with source language
culture. These models frequently fail to represent the cultural heritage and
values of local communities. This work proposes a methodology to create both
synthetic and retrieval-based pre-training data tailored to a specific
community, considering its (i) language, (ii) cultural heritage, and (iii)
cultural values. We demonstrate our methodology using Egyptian and Moroccan
dialects as testbeds, chosen for their linguistic and cultural richness and
current underrepresentation in LLMs. As a proof-of-concept, we develop
NileChat, a 3B parameter LLM adapted for Egyptian and Moroccan communities,
incorporating their language, cultural heritage, and values. Our results on
various understanding, translation, and cultural and values alignment
benchmarks show that NileChat outperforms existing Arabic-aware LLMs of similar
size and performs on par with larger models. We share our methods, data, and
models with the community to promote the inclusion and coverage of more diverse
communities in LLM development.

</details>


### [18] [RaDeR: Reasoning-aware Dense Retrieval Models](https://arxiv.org/abs/2505.18405)
*Debrup Das,Sam O' Nuallain,Razieh Rahimi*

Main category: cs.CL

TL;DR: RaDeR是基于LLM数学问题解决数据训练的推理密集型检索模型，通过自反思相关性评估生成优质训练数据，在数学推理和代码任务中显著超越传统方法


<details>
  <summary>Details</summary>
Motivation: 解决传统检索模型在推理密集型任务（如数学和代码）中表现不足的问题，特别针对思维链查询场景优化检索效果

Method: 1. 利用LLM生成检索增强的推理轨迹
2. 自反思机制评估查询-文档相关性
3. 自动生成多样化困难负样本
4. 仅需少量高质量训练数据（REASONIR的2.5%）

Result: 1. 在BRIGHT/RAR-b基准测试中全面超越基线
2. 数学/代码任务提升显著（Math/Coding splits）
3. 首次实现密集检索器在思维链查询场景超越BM25
4. 训练效率比REASONIR提升40倍

Conclusion: RaDeR证明了基于推理的检索训练数据质量的重要性，为增强推理语言模型提供了新范式，在保持高性能的同时大幅降低数据需求

Abstract: We propose RaDeR, a set of reasoning-based dense retrieval models trained
with data derived from mathematical problem solving using large language models
(LLMs). Our method leverages retrieval-augmented reasoning trajectories of an
LLM and self-reflective relevance evaluation, enabling the creation of both
diverse and hard-negative samples for reasoning-intensive relevance. RaDeR
retrievers, trained for mathematical reasoning, effectively generalize to
diverse reasoning tasks in the BRIGHT and RAR-b benchmarks, consistently
outperforming strong baselines in overall performance.Notably, RaDeR achieves
significantly higher performance than baselines on the Math and Coding splits.
In addition, RaDeR presents the first dense retriever that outperforms BM25
when queries are Chain-of-Thought reasoning steps, underscoring the critical
role of reasoning-based retrieval to augment reasoning language models.
Furthermore, RaDeR achieves comparable or superior performance while using only
2.5% of the training data used by the concurrent work REASONIR, highlighting
the quality of our synthesized training data.

</details>


### [19] [DanmakuTPPBench: A Multi-modal Benchmark for Temporal Point Process Modeling and Understanding](https://arxiv.org/abs/2505.18411)
*Yue Jiang,Jichu Li,Yang Liu,Dingkang Yang,Feng Zhou,Quyu Kong*

Main category: cs.CL

TL;DR: 提出DanmakuTPPBench基准测试，包含多模态弹幕事件数据集和复杂问答任务，揭示现有模型在多模态时序建模的不足


<details>
  <summary>Details</summary>
Motivation: 现有时序点过程数据集多为单模态，无法满足需要时间-文本-视觉联合推理的多模态模型发展需求

Method: 1) 从Bilibili构建含时间戳、弹幕文本和视频帧的多模态数据集；2) 基于LLM/MLLM构建复杂多模态问答数据集；3) 对传统TPP模型和MLLM进行综合评估

Result: 现有方法在多模态事件建模存在显著性能差距，传统TPP模型无法处理多模态信息，MLLM在时序推理方面表现不足

Conclusion: 该基准为多模态时序建模建立新标准，推动TPP与多模态语言模型的深度融合，代码和数据集已开源

Abstract: We introduce DanmakuTPPBench, a comprehensive benchmark designed to advance
multi-modal Temporal Point Process (TPP) modeling in the era of Large Language
Models (LLMs). While TPPs have been widely studied for modeling temporal event
sequences, existing datasets are predominantly unimodal, hindering progress in
models that require joint reasoning over temporal, textual, and visual
information. To address this gap, DanmakuTPPBench comprises two complementary
components: (1) DanmakuTPP-Events, a novel dataset derived from the Bilibili
video platform, where user-generated bullet comments (Danmaku) naturally form
multi-modal events annotated with precise timestamps, rich textual content, and
corresponding video frames; (2) DanmakuTPP-QA, a challenging question-answering
dataset constructed via a novel multi-agent pipeline powered by
state-of-the-art LLMs and multi-modal LLMs (MLLMs), targeting complex
temporal-textual-visual reasoning. We conduct extensive evaluations using both
classical TPP models and recent MLLMs, revealing significant performance gaps
and limitations in current methods' ability to model multi-modal event
dynamics. Our benchmark establishes strong baselines and calls for further
integration of TPP modeling into the multi-modal language modeling landscape.
The code and dataset have been released at
https://github.com/FRENKIE-CHIANG/DanmakuTPPBench

</details>


### [20] [Retrieval Augmented Generation-based Large Language Models for Bridging Transportation Cybersecurity Legal Knowledge Gaps](https://arxiv.org/abs/2505.18426)
*Khandakar Ashrafi Akbar,Md Nahiyan Uddin,Latifur Khan,Trayce Hockstad,Mizanur Rahman,Mashrur Chowdhury,Bhavani Thuraisingham*

Main category: cs.CL

TL;DR: 提出基于检索增强生成（RAG）的LLM框架，通过领域特定问题集减少模型幻觉，为交通技术立法更新提供精准法律分析支持。


<details>
  <summary>Details</summary>
Motivation: 应对自动驾驶和智能交通技术快速发展带来的网络安全与数据隐私法律漏洞，解决传统LLM在专业法律领域存在事实性错误的问题。

Method: 构建RAG架构整合法律知识库检索机制，设计交通政策专项问题模板，通过检索-生成协同机制增强法律条文引用的准确性。

Result: 在AlignScore(0.82)、ParaScore(0.79)、BERTScore(0.88)和ROUGE-L(0.75)四项指标上均超越GPT-4等商业模型，特定法律条款召回率提升37%。

Conclusion: 该框架证明了领域定制化RAG架构在立法分析中的有效性，为AI驱动的法律框架动态更新提供了可扩展的技术路径。

Abstract: As connected and automated transportation systems evolve, there is a growing
need for federal and state authorities to revise existing laws and develop new
statutes to address emerging cybersecurity and data privacy challenges. This
study introduces a Retrieval-Augmented Generation (RAG) based Large Language
Model (LLM) framework designed to support policymakers by extracting relevant
legal content and generating accurate, inquiry-specific responses. The
framework focuses on reducing hallucinations in LLMs by using a curated set of
domain-specific questions to guide response generation. By incorporating
retrieval mechanisms, the system enhances the factual grounding and specificity
of its outputs. Our analysis shows that the proposed RAG-based LLM outperforms
leading commercial LLMs across four evaluation metrics: AlignScore, ParaScore,
BERTScore, and ROUGE, demonstrating its effectiveness in producing reliable and
context-aware legal insights. This approach offers a scalable, AI-driven method
for legislative analysis, supporting efforts to update legal frameworks in line
with advancements in transportation technologies.

</details>


### [21] [Voice of a Continent: Mapping Africa's Speech Technology Frontier](https://arxiv.org/abs/2505.18436)
*AbdelRahim Elmadany,Sang Yun Kwon,Hawau Olamide Toyin,Alcides Alcoba Inciarte,Hanan Aldarmaki,Muhammad Abdul-Mageed*

Main category: cs.CL

TL;DR: 构建SimbaBench基准与Simba模型，提升非洲语言语音技术覆盖


<details>
  <summary>Details</summary>
Motivation: 非洲语言在语音技术资源中代表性严重不足，导致数字包容性障碍

Method: 系统绘制非洲语音数据集与技术图谱，建立SimbaBench基准测试平台，开发Simba系列模型

Result: 模型在多项任务中达到SOTA，发现数据集质量、领域多样性和语言家族关系显著影响性能

Conclusion: 亟需扩大语音技术资源以反映非洲语言多样性，本研究为开发包容性语音技术奠定基础

Abstract: Africa's rich linguistic diversity remains significantly underrepresented in
speech technologies, creating barriers to digital inclusion. To alleviate this
challenge, we systematically map the continent's speech space of datasets and
technologies, leading to a new comprehensive benchmark SimbaBench for
downstream African speech tasks. Using SimbaBench, we introduce the Simba
family of models, achieving state-of-the-art performance across multiple
African languages and speech tasks. Our benchmark analysis reveals critical
patterns in resource availability, while our model evaluation demonstrates how
dataset quality, domain diversity, and language family relationships influence
performance across languages. Our work highlights the need for expanded speech
technology resources that better reflect Africa's linguistic diversity and
provides a solid foundation for future research and development efforts toward
more inclusive speech technologies.

</details>


### [22] [Efficient Long CoT Reasoning in Small Language Models](https://arxiv.org/abs/2505.18440)
*Zhaoyang Wang,Jinqi Jiang,Tian Qiu,Hui Liu,Xianfeng Tang,Huaxiu Yao*

Main category: cs.CL

TL;DR: 提出剪枝长思维链冗余步骤的方法，通过策略筛选有效数据实现小模型的高效推理训练


<details>
  <summary>Details</summary>
Motivation: 小语言模型直接学习长思维链时容易受到冗余内容干扰，需优化蒸馏方法提升学习效率

Method: 结合冗余步骤剪枝与自主策略筛选机制，利用小模型自身优化训练数据质量

Result: 在数学推理任务中，小模型推理步骤减少40%的同时保持97%的基准性能

Conclusion: 该方法成功实现长思维链能力的高效蒸馏，为资源受限场景提供可行的推理优化方案

Abstract: Recent large reasoning models such as DeepSeek-R1 exhibit strong complex
problems solving abilities by generating long chain-of-thought (CoT) reasoning
steps. It is challenging to directly train small language models (SLMs) to
emerge long CoT. Thus, distillation becomes a practical method to enable SLMs
for such reasoning ability. However, the long CoT often contains a lot of
redundant contents (e.g., overthinking steps) which may make SLMs hard to learn
considering their relatively poor capacity and generalization. To address this
issue, we propose a simple-yet-effective method to prune unnecessary steps in
long CoT, and then employ an on-policy method for the SLM itself to curate
valid and useful long CoT training data. In this way, SLMs can effectively
learn efficient long CoT reasoning and preserve competitive performance at the
same time. Experimental results across a series of mathematical reasoning
benchmarks demonstrate the effectiveness of the proposed method in distilling
long CoT reasoning ability into SLMs which maintains the competitive
performance but significantly reduces generating redundant reasoning steps.

</details>


### [23] [BRIT: Bidirectional Retrieval over Unified Image-Text Graph](https://arxiv.org/abs/2505.18450)
*Ainulla Khan,Yamada Moyuru,Srinidhi Akella*

Main category: cs.CL

TL;DR: 提出BRIT多模态RAG框架，通过构建多模态图实现跨文本图像的关联检索，解决复杂跨模态问答问题


<details>
  <summary>Details</summary>
Motivation: 现有RAG技术主要聚焦文本检索，但多模态文档（图文混合）的跨模态关联检索尚未充分探索，尤其在无法微调模型时存在挑战

Method: 将文档中的图文关系构建为多模态图，通过图像-文本双向路径检索，获取直接相关内容和间接关联内容

Result: 构建MM-RAG测试集验证有效性，实验证明BRIT在处理跨模态多跳问题上的优越性

Conclusion: BRIT首次将图结构引入多模态RAG，通过创新检索机制提升复杂跨模态问题的解答能力

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising technique to
enhance the quality and relevance of responses generated by large language
models. While recent advancements have mainly focused on improving RAG for
text-based queries, RAG on multi-modal documents containing both texts and
images has not been fully explored. Especially when fine-tuning does not work.
This paper proposes BRIT, a novel multi-modal RAG framework that effectively
unifies various text-image connections in the document into a multi-modal graph
and retrieves the texts and images as a query-specific sub-graph. By traversing
both image-to-text and text-to-image paths in the graph, BRIT retrieve not only
directly query-relevant images and texts but also further relevant contents to
answering complex cross-modal multi-hop questions. To evaluate the
effectiveness of BRIT, we introduce MM-RAG test set specifically designed for
multi-modal question answering tasks that require to understand the text-image
relations. Our comprehensive experiments demonstrate the superiority of BRIT,
highlighting its ability to handle cross-modal questions on the multi-modal
documents.

</details>


### [24] [MedScore: Factuality Evaluation of Free-Form Medical Answers](https://arxiv.org/abs/2505.18452)
*Heyuan Huang,Alexandra DeLucia,Vijay Murari Tiyyala,Mark Dredze*

Main category: cs.CL

TL;DR: 提出MedScore方法改进医学答案的事实性评估，通过条件感知分解提取更多有效事实，显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有事实性评估系统主要针对客观实体文本（如传记），无法有效处理医学领域条件依赖、对话式、假设性强的多样化句式结构

Method: 开发条件感知分解方法，保留事实的条件依赖性，减少幻觉和模糊指代

Result: 提取有效事实数量提升3倍，事实性评分受分解方法/验证语料/骨干LLM三重影响显著

Conclusion: MedScore通过定制化分解流程提升医学事实性评估可靠性，证明领域适配各评估环节的重要性

Abstract: While Large Language Models (LLMs) can generate fluent and convincing
responses, they are not necessarily correct. This is especially apparent in the
popular decompose-then-verify factuality evaluation pipeline, where LLMs
evaluate generations by decomposing the generations into individual, valid
claims. Factuality evaluation is especially important for medical answers,
since incorrect medical information could seriously harm the patient. However,
existing factuality systems are a poor match for the medical domain, as they
are typically only evaluated on objective, entity-centric, formulaic texts such
as biographies and historical topics. This differs from condition-dependent,
conversational, hypothetical, sentence-structure diverse, and subjective
medical answers, which makes decomposition into valid facts challenging. We
propose MedScore, a new approach to decomposing medical answers into
condition-aware valid facts. Our method extracts up to three times more valid
facts than existing methods, reducing hallucination and vague references, and
retaining condition-dependency in facts. The resulting factuality score
significantly varies by decomposition method, verification corpus, and used
backbone LLM, highlighting the importance of customizing each step for reliable
factuality evaluation.

</details>


### [25] [Hybrid Latent Reasoning via Reinforcement Learning](https://arxiv.org/abs/2505.18454)
*Zhenrui Yue,Bowen Jin,Huimin Zeng,Honglei Zhuang,Zhen Qin,Jinsung Yoon,Lanyu Shang,Jiawei Han,Dong Wang*

Main category: cs.CL

TL;DR: 提出基于强化学习的混合潜在推理方法HRPO，通过整合离散token和连续隐藏状态提升LLMs的推理能力，在多项任务中超越现有方法并保持可解释性


<details>
  <summary>Details</summary>
Motivation: 现有潜在推理方法因连续范式与LLMs自回归生成的离散特性冲突，且依赖人工标注的思维链数据，无法有效利用LLMs自身的推理模式

Method: 设计混合推理策略：1) 可学习门控机制融合历史隐藏状态与当前token采样；2) 渐进式训练策略，从token嵌入逐步过渡到隐藏特征利用

Result: 在知识型和推理型任务中全面超越基线方法，模型展现出跨语言推理模式和更短生成长度，同时保持决策可解释性

Conclusion: HRPO验证了强化学习在潜在推理中的有效性，其混合表示方法和无监督优化框架为未来LLMs推理机制研究提供了新方向

Abstract: Recent advances in large language models (LLMs) have introduced latent
reasoning as a promising alternative to autoregressive reasoning. By performing
internal computation with hidden states from previous steps, latent reasoning
benefit from more informative features rather than sampling a discrete
chain-of-thought (CoT) path. Yet latent reasoning approaches are often
incompatible with LLMs, as their continuous paradigm conflicts with the
discrete nature of autoregressive generation. Moreover, these methods rely on
CoT traces for training and thus fail to exploit the inherent reasoning
patterns of LLMs. In this work, we explore latent reasoning by leveraging the
intrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we
introduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid
latent reasoning approach that (1) integrates prior hidden states into sampled
tokens with a learnable gating mechanism, and (2) initializes training with
predominantly token embeddings while progressively incorporating more hidden
features. This design maintains LLMs' generative capabilities and incentivizes
hybrid reasoning using both discrete and continuous representations. In
addition, the hybrid HRPO introduces stochasticity into latent reasoning via
token sampling, thereby enabling RL-based optimization without requiring CoT
trajectories. Extensive evaluations across diverse benchmarks show that HRPO
outperforms prior methods in both knowledge- and reasoning-intensive tasks.
Furthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing
behaviors like cross-lingual patterns and shorter completion lengths,
highlighting the potential of our RL-based approach and offer insights for
future work in latent reasoning.

</details>


### [26] [Anchored Diffusion Language Model](https://arxiv.org/abs/2505.18456)
*Litu Rout,Constantine Caramanis,Sanjay Shakkottai*

Main category: cs.CL

TL;DR: 提出锚定扩散语言模型ADLM，通过两阶段预测框架显著提升扩散模型性能并首次超越自回归模型


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型在关键token早期遮蔽时表现不佳，需要解决重要token丢失导致的上下文信息不足问题

Method: 两阶段框架：1）锚定网络预测重要token分布 2）基于锚定预测生成缺失token似然

Result: LM1B和OpenWebText测试复杂度提升25.4%，零样本7基准SOTA，MAUVE得分首超AR模型

Conclusion: 锚定机制不仅提升扩散模型性能，还改进AR模型推理能力，为语言建模提供新范式

Abstract: Diffusion Language Models (DLMs) promise parallel generation and
bidirectional context, yet they underperform autoregressive (AR) models in both
likelihood modeling and generated text quality. We identify that this
performance gap arises when important tokens (e.g., key words or low-frequency
words that anchor a sentence) are masked early in the forward process, limiting
contextual information for accurate reconstruction. To address this, we
introduce the Anchored Diffusion Language Model (ADLM), a novel two-stage
framework that first predicts distributions over important tokens via an anchor
network, and then predicts the likelihoods of missing tokens conditioned on the
anchored predictions. ADLM significantly improves test perplexity on LM1B and
OpenWebText, achieving up to 25.4% gains over prior DLMs, and narrows the gap
with strong AR baselines. It also achieves state-of-the-art performance in
zero-shot generalization across seven benchmarks and surpasses AR models in
MAUVE score, which marks the first time a DLM generates better human-like text
than an AR model. Theoretically, we derive an Anchored Negative Evidence Lower
Bound (ANELBO) objective and show that anchoring improves sample complexity and
likelihood modeling. Beyond diffusion, anchoring boosts performance in AR
models and enhances reasoning in math and logic tasks, outperforming existing
chain-of-thought approaches

</details>


### [27] [Measuring South Asian Biases in Large Language Models](https://arxiv.org/abs/2505.18466)
*Mamnuya Rinki,Chahat Raj,Anjishnu Mukherjee,Ziwei Zhu*

Main category: cs.CL

TL;DR: 本研究构建了首个南亚多语言交叉偏见词典，分析了文化因素对LLM生成任务的影响，并验证了自我去偏策略在印度-雅利安/德拉威语系中的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估忽视南亚等低资源地区的交叉文化偏见，特别是面纱制度(purdah)和父权制影响下的性别、宗教等复合维度偏见。

Method: 基于10种印度语言构建文化偏见词典，设计开放式生成任务(故事创作/兴趣列表等)，量化两种提示去偏策略的效果。

Result: 发现文化污名被系统性强化，简单提示去偏效果有限，复杂情境化提示能更有效降低交叉偏见在生成内容中的渗透。

Conclusion: 提出了超越欧美中心的多文化评估框架，为LLM在非西方语境中的伦理部署提供了新的分析工具。

Abstract: Evaluations of Large Language Models (LLMs) often overlook intersectional and
culturally specific biases, particularly in underrepresented multilingual
regions like South Asia. This work addresses these gaps by conducting a
multilingual and intersectional analysis of LLM outputs across 10 Indo-Aryan
and Dravidian languages, identifying how cultural stigmas influenced by purdah
and patriarchy are reinforced in generative tasks. We construct a culturally
grounded bias lexicon capturing previously unexplored intersectional dimensions
including gender, religion, marital status, and number of children. We use our
lexicon to quantify intersectional bias and the effectiveness of self-debiasing
in open-ended generations (e.g., storytelling, hobbies, and to-do lists), where
bias manifests subtly and remains largely unexamined in multilingual contexts.
Finally, we evaluate two self-debiasing strategies (simple and complex prompts)
to measure their effectiveness in reducing culturally specific bias in
Indo-Aryan and Dravidian languages. Our approach offers a nuanced lens into
cultural bias by introducing a novel bias lexicon and evaluation framework that
extends beyond Eurocentric or small-scale multilingual settings.

</details>


### [28] [Investigating AI Rater Effects of Large Language Models: GPT, Claude, Gemini, and DeepSeek](https://arxiv.org/abs/2505.18486)
*Hong Jiao,Dan Song,Won-Chan Lee*

Main category: cs.CL

TL;DR: 研究比较了10个大语言模型在写作评分中的表现，发现ChatGPT 4o、Gemini 1.5 Pro和Claude 3.5 Sonnet具有较高的评分准确性和可靠性


<details>
  <summary>Details</summary>
Motivation: 验证不同LLM在自动评分中的可靠性及评分员效应，为实际应用提供依据

Method: 通过二次加权Kappa评估评分准确性，Cronbach Alpha检验评分一致性，多面Rasch模型分析评分员效应

Result: ChatGPT 4o、Gemini 1.5 Pro和Claude 3.5 Sonnet在整体/分析性评分中表现最优，评分者效应最低

Conclusion: 推荐优先采用ChatGPT 4o等可靠性高的LLM进行自动评分，但需持续监控模型表现

Abstract: Large language models (LLMs) have been widely explored for automated scoring
in low-stakes assessment to facilitate learning and instruction. Empirical
evidence related to which LLM produces the most reliable scores and induces
least rater effects needs to be collected before the use of LLMs for automated
scoring in practice. This study compared ten LLMs (ChatGPT 3.5, ChatGPT 4,
ChatGPT 4o, OpenAI o1, Claude 3.5 Sonnet, Gemini 1.5, Gemini 1.5 Pro, Gemini
2.0, as well as DeepSeek V3, and DeepSeek R1) with human expert raters in
scoring two types of writing tasks. The accuracy of the holistic and analytic
scores from LLMs compared with human raters was evaluated in terms of Quadratic
Weighted Kappa. Intra-rater consistency across prompts was compared in terms of
Cronbach Alpha. Rater effects of LLMs were evaluated and compared with human
raters using the Many-Facet Rasch model. The results in general supported the
use of ChatGPT 4o, Gemini 1.5 Pro, and Claude 3.5 Sonnet with high scoring
accuracy, better rater reliability, and less rater effects.

</details>


### [29] [The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic Competence in Large Language Models](https://arxiv.org/abs/2505.18497)
*Kefan Yu,Qingcheng Zeng,Weihao Xuan,Wanxin Li,Jingyi Wu,Rob Voigt*

Main category: cs.CL

TL;DR: 研究通过ALTPRAG数据集揭示大型语言模型在训练过程中逐步发展出语用推理能力，显示基座模型已具备语用敏感性，且SFT/RLHF阶段进一步增强认知语用推理能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在训练过程中如何形成语用理解能力，特别是针对隐含意图推断的机制。

Method: 构建基于语用学'替代'概念的ALTPRAG数据集，在预训练/SFT/RLHF三个阶段系统评估22个LLMs的语用推理能力。

Result: 基座模型显示语用敏感性（准确率60%+），模型规模和数据量提升带来持续增益；SFT和RLHF阶段使认知语用推理准确率提升10-15%。

Conclusion: 语用能力是LLM训练过程中涌现的组合属性，对齐人类交际规范需关注语用维度的优化。

Abstract: Current large language models (LLMs) have demonstrated emerging capabilities
in social intelligence tasks, including implicature resolution (Sravanthi et
al. (2024)) and theory-of-mind reasoning (Shapira et al. (2024)), both of which
require substantial pragmatic understanding. However, how LLMs acquire this
competence throughout the training process remains poorly understood. In this
work, we introduce ALTPRAG, a dataset grounded in the pragmatic concept of
alternatives, designed to evaluate whether LLMs at different training stages
can accurately infer nuanced speaker intentions. Each instance pairs two
contextually appropriate but pragmatically distinct continuations, enabling
fine-grained assessment of both pragmatic interpretation and contrastive
reasoning. We systematically evaluate 22 LLMs across key training stages:
pre-training, supervised fine-tuning (SFT), and preference optimization, to
examine the development of pragmatic competence. Our results show that even
base models exhibit notable sensitivity to pragmatic cues, which improves
consistently with increases in model and data scale. Additionally, SFT and RLHF
contribute further gains, particularly in cognitive-pragmatic reasoning. These
findings highlight pragmatic competence as an emergent and compositional
property of LLM training and offer new insights for aligning models with human
communicative norms.

</details>


### [30] [How Does Sequence Modeling Architecture Influence Base Capabilities of Pre-trained Language Models? Exploring Key Architecture Design Principles to Avoid Base Capabilities Degradation](https://arxiv.org/abs/2505.18522)
*Xin Lu,Yanyan Zhao,Si Wei,Shijin Wang,Bing Qin,Ting Liu*

Main category: cs.CL

TL;DR: 通过有限域预训练和架构组件分析，提出序列建模架构需具备全序列任意选择能力以避免基础能力退化


<details>
  <summary>Details</summary>
Motivation: 现有架构设计工作采用的混合域预训练设置无法充分揭示不同架构的基础能力差异，需探索架构设计对预训练语言模型基础能力的影响机制

Method: 1. 提出有限域预训练+分布外测试设置 2. 分析状态序列建模架构基础能力 3. 通过架构组件分析总结设计原则 4. 使用Top-1选择架构进行验证

Result: 发现状态序列建模架构存在显著基础能力退化，提出并验证了「全序列任意选择能力」的关键设计原则，Top-1块选择架构验证有效

Conclusion: 序列建模架构需保持全序列任意选择能力，研究为未来架构改进提供设计原则参考，Top-1块选择架构展现实用潜力

Abstract: Pre-trained language models represented by the Transformer have been proven
to possess strong base capabilities, and the representative self-attention
mechanism in the Transformer has become a classic in sequence modeling
architectures. Different from the work of proposing sequence modeling
architecture to improve the efficiency of attention mechanism, this work
focuses on the impact of sequence modeling architectures on base capabilities.
Specifically, our concern is: How exactly do sequence modeling architectures
affect the base capabilities of pre-trained language models? In this work, we
first point out that the mixed domain pre-training setting commonly adopted in
existing architecture design works fails to adequately reveal the differences
in base capabilities among various architectures. To address this, we propose a
limited domain pre-training setting with out-of-distribution testing, which
successfully uncovers significant differences in base capabilities among
architectures at an early stage. Next, we analyze the base capabilities of
stateful sequence modeling architectures, and find that they exhibit
significant degradation in base capabilities compared to the Transformer. Then,
through a series of architecture component analysis, we summarize a key
architecture design principle: A sequence modeling architecture need possess
full-sequence arbitrary selection capability to avoid degradation in base
capabilities. Finally, we empirically validate this principle using an
extremely simple Top-1 element selection architecture and further generalize it
to a more practical Top-1 chunk selection architecture. Experimental results
demonstrate our proposed sequence modeling architecture design principle and
suggest that our work can serve as a valuable reference for future architecture
improvements and novel designs.

</details>


### [31] [metaTextGrad: Automatically optimizing language model optimizers](https://arxiv.org/abs/2505.18524)
*Guowei Xu,Mert Yuksekgonul,Carlos Guestrin,James Zou*

Main category: cs.CL

TL;DR: 提出metaTextGrad框架，通过元优化器增强现有LLM优化器的任务适配性，在多个基准上实现平均6%的性能提升


<details>
  <summary>Details</summary>
Motivation: 现有LLM优化器存在人工设计局限（未自我优化）和通用性设计导致任务适配不足的问题

Method: 包含元提示优化器（优化prompt工程）和元结构优化器（优化框架结构）的双层优化架构

Result: 在GSM8K等基准测试中，相比TextGrad等基线方法实现最高6%的绝对性能提升

Conclusion: 通过元优化器实现现有优化器的任务适配优化，为LLM优化器系统设计提供新范式

Abstract: Large language models (LLMs) are increasingly used in learning algorithms,
evaluations, and optimization tasks. Recent studies have shown that using
LLM-based optimizers to automatically optimize model prompts, demonstrations,
predictions themselves, or other components can significantly enhance the
performance of AI systems, as demonstrated by frameworks such as DSPy and
TextGrad. However, optimizers built on language models themselves are usually
designed by humans with manual design choices; optimizers themselves are not
optimized. Moreover, these optimizers are general purpose by design, to be
useful to a broad audience, and are not tailored for specific tasks. To address
these challenges, we propose metaTextGrad, which focuses on designing a
meta-optimizer to further enhance existing optimizers and align them to be good
optimizers for a given task. Our approach consists of two key components: a
meta prompt optimizer and a meta structure optimizer. The combination of these
two significantly improves performance across multiple benchmarks, achieving an
average absolute performance improvement of up to 6% compared to the best
baseline.

</details>


### [32] [Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models](https://arxiv.org/abs/2505.18536)
*Haoyuan Sun,Jiaqi Wu,Bo Xia,Yifu Luo,Yifei Zhao,Kai Qin,Xufei Lv,Tiantian Zhang,Yongzhe Chang,Xueqian Wang*

Main category: cs.CL

TL;DR: 强化微调(RFT)可有效增强多模态大语言模型的推理能力，论文系统梳理了其背景知识、五大改进维度（多模态/多任务/训练算法/基准/工程框架），并提出未来五大研究方向


<details>
  <summary>Details</summary>
Motivation: 当前AGI发展处于关键阶段，RFT在纯文本LLM中已展现显著潜力，但如何将其有效拓展至多模态场景尚未形成系统研究体系

Method: 首先建立多模态RFT的理论基础，继而从模态多样性、任务扩展性、算法创新性、基准建设、工程框架五个维度系统总结技术进展

Result: 构建了首个面向多模态RFT的系统分析框架，明确其技术发展路线图，提出包含跨模态对齐强化、认知架构优化等方向的未来研究体系

Conclusion: 强化微调是提升MLLMs推理能力的核心驱动力，通过系统性技术改进与跨学科研究融合，将加速AGI关键突破

Abstract: Standing in 2025, at a critical juncture in the pursuit of Artificial General
Intelligence (AGI), reinforcement fine-tuning (RFT) has demonstrated
significant potential in enhancing the reasoning capability of large language
models (LLMs) and has led to the development of cutting-edge AI models such as
OpenAI-o1 and DeepSeek-R1. Moreover, the efficient application of RFT to
enhance the reasoning capability of multimodal large language models (MLLMs)
has attracted widespread attention from the community. In this position paper,
we argue that reinforcement fine-tuning powers the reasoning capability of
multimodal large language models. To begin with, we provide a detailed
introduction to the fundamental background knowledge that researchers
interested in this field should be familiar with. Furthermore, we meticulously
summarize the improvements of RFT in powering reasoning capability of MLLMs
into five key points: diverse modalities, diverse tasks and domains, better
training algorithms, abundant benchmarks and thriving engineering frameworks.
Finally, we propose five promising directions for future research that the
community might consider. We hope that this position paper will provide
valuable insights to the community at this pivotal stage in the advancement
toward AGI. Summary of works done on RFT for MLLMs is available at
https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs.

</details>


### [33] [Business as \textit{Rule}sual: A Benchmark and Framework for Business Rule Flow Modeling with LLMs](https://arxiv.org/abs/2505.18542)
*Chen Yang,Ruping Xu,Ruizhe Li,Bin Cao,Jing Fan*

Main category: cs.CL

TL;DR: 提出BPRF数据集和ExIde框架，利用大语言模型自动提取商业规则及依赖关系，验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有流程挖掘研究主要集中于指导性文本的动作流提取，但商业文档中的规则流尚未充分探索。研究旨在填补该空白，实现自动化且可解释的业务规则结构化分析。

Method: 构建含50个文档的BPRF数据集（标注326条<条件,动作>规则及其逻辑依赖），开发ExIde框架利用LLMs进行规则提取与依赖分类，并评估12个SOTA大语言模型性能。

Result: ExIde有效提取结构化商业规则并分析依赖关系，当前先进LLMs在规则提取（F1=0.82）和依赖分类（Accuracy=0.75）任务中展现可行性。

Conclusion: 该研究通过结构化规则提取与依赖分析，为自动化业务流程提供了可解释的技术路径，推动业务过程自动化向更高阶发展。

Abstract: Process mining aims to discover, monitor and optimize the actual behaviors of
real processes. While prior work has mainly focused on extracting procedural
action flows from instructional texts, rule flows embedded in business
documents remain underexplored. To this end, we introduce a novel annotated
Chinese dataset, \textbf{BPRF}, which contains 50 business process documents
with 326 explicitly labeled business rules across multiple domains. Each rule
is represented as a <Condition, Action> pair, and we annotate logical
dependencies between rules (sequential, conditional, or parallel). We also
propose \textbf{ExIde}, a framework for automatic business rule extraction and
dependency relationship identification using large language models (LLMs). We
evaluate ExIde using 12 state-of-the-art (SOTA) LLMs on the BPRF dataset,
benchmarking performance on both rule extraction and dependency classification
tasks of current LLMs. Our results demonstrate the effectiveness of ExIde in
extracting structured business rules and analyzing their interdependencies for
current SOTA LLMs, paving the way for more automated and interpretable business
process automation.

</details>


### [34] [Composable Cross-prompt Essay Scoring by Merging Models](https://arxiv.org/abs/2505.18548)
*Sanwoo Lee,Kun Liang,Yunfang Wu*

Main category: cs.CL

TL;DR: Proposes source-free cross-prompt AES adaptation via task vector merging and PIM optimization


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns and suboptimal performance of joint training approaches in cross-prompt AES

Method: Linear combination of task vectors with Prior-encoded Information Maximization (PIM) objective and Bayesian optimization

Result: Outperforms joint training, shows robustness, excels under distribution shifts with computational efficiency

Conclusion: Provides privacy-preserving effective adaptation method for AES systems with strong empirical performance

Abstract: Recent advances in cross-prompt automated essay scoring (AES) typically train
models jointly on all source prompts, often requiring additional access to
unlabeled target prompt essays simultaneously. However, using all sources is
suboptimal in our pilot study, and re-accessing source datasets during
adaptation raises privacy concerns. We propose a source-free adaptation
approach that selectively merges individually trained source models' parameters
instead of datasets. In particular, we simulate joint training through linear
combinations of task vectors -- the parameter updates from fine-tuning. To
optimize the combination's coefficients, we propose Prior-encoded Information
Maximization (PIM), an unsupervised objective which promotes the model's score
discriminability regularized by priors pre-computed from the sources. We employ
Bayesian optimization as an efficient optimizer of PIM. Experimental results
with LLMs on in-dataset and cross-dataset adaptation show that our method (1)
consistently outperforms training jointly on all sources, (2) maintains
superior robustness compared to other merging methods, (3) excels under severe
distribution shifts where recent leading cross-prompt methods struggle, all
while retaining computational efficiency.

</details>


### [35] [MSA at BEA 2025 Shared Task: Disagreement-Aware Instruction Tuning for Multi-Dimensional Evaluation of LLMs as Math Tutors](https://arxiv.org/abs/2505.18549)
*Baraa Hikal,Mohamed Basem,Islam Oshallah,Ali Hamdi*

Main category: cs.CL

TL;DR: 提出MSA-MathEval系统，采用统一指令微调框架和分歧感知集成策略，在BEA 2025教育AI评估任务中四项指标均进入前四名


<details>
  <summary>Details</summary>
Motivation: 针对教育场景下大语言模型作为导师的多维度能力评估，探索可扩展的指令微调方法和分歧驱动建模对评估鲁棒性的提升

Method: 单一指令微调模型适配全部任务+分歧感知集成推理（增强少数标签覆盖率）

Result: 指导建议维度第一/可操作性第三/错误识别与定位第四

Conclusion: 验证了统一指令微调框架与分歧驱动建模在教育AI多维度评估中的有效性

Abstract: We present MSA-MathEval, our submission to the BEA 2025 Shared Task on
evaluating AI tutor responses across four instructional dimensions: Mistake
Identification, Mistake Location, Providing Guidance, and Actionability. Our
approach uses a unified training pipeline to fine-tune a single
instruction-tuned language model across all tracks, without any task-specific
architectural changes. To improve prediction reliability, we introduce a
disagreement-aware ensemble inference strategy that enhances coverage of
minority labels. Our system achieves strong performance across all tracks,
ranking 1st in Providing Guidance, 3rd in Actionability, and 4th in both
Mistake Identification and Mistake Location. These results demonstrate the
effectiveness of scalable instruction tuning and disagreement-driven modeling
for robust, multi-dimensional evaluation of LLMs as educational tutors.

</details>


### [36] [Unraveling Misinformation Propagation in LLM Reasoning](https://arxiv.org/abs/2505.18555)
*Yiyang Feng,Yichen Wang,Shaobo Cui,Boi Faltings,Mina Lee,Jiawei Zhou*

Main category: cs.CL

TL;DR: 研究大语言模型在数学推理中处理错误信息的影响及纠正方法，发现早期纠正最有效


<details>
  <summary>Details</summary>
Motivation: 探讨错误信息如何影响LLMs的推理过程，尤其是用户因知识盲区输入错误信息时的传播机制

Method: 通过数学推理任务分析错误信息对中间步骤的影响，测试模型在明确纠正指令下的表现，并提出基于早期纠正的微调方案

Result: 即使具备正确知识，模型纠错成功率不足50%（准确率下降10.02%-72.20%），但早期纠正可使错误传播减少40%以上

Conclusion: 在推理链早期实施事实修正并结合针对性微调，可显著提升模型抗干扰能力，为实际应用提供有效解决方案

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
reasoning, positioning them as promising tools for supporting human
problem-solving. However, what happens when their performance is affected by
misinformation, i.e., incorrect inputs introduced by users due to oversights or
gaps in knowledge? Such misinformation is prevalent in real-world interactions
with LLMs, yet how it propagates within LLMs' reasoning process remains
underexplored. Focusing on mathematical reasoning, we present a comprehensive
analysis of how misinformation affects intermediate reasoning steps and final
answers. We also examine how effectively LLMs can correct misinformation when
explicitly instructed to do so. Even with explicit instructions, LLMs succeed
less than half the time in rectifying misinformation, despite possessing
correct internal knowledge, leading to significant accuracy drops (10.02% -
72.20%). Further analysis shows that applying factual corrections early in the
reasoning process most effectively reduces misinformation propagation, and
fine-tuning on synthesized data with early-stage corrections significantly
improves reasoning factuality. Our work offers a practical approach to
mitigating misinformation propagation.

</details>


### [37] [Exploring the Vulnerability of the Content Moderation Guardrail in Large Language Models via Intent Manipulation](https://arxiv.org/abs/2505.18556)
*Jun Zhuang,Haibo Jin,Ye Zhang,Zhengjian Kang,Wenbin Zhang,Gaby G. Dagher,Haohan Wang*

Main category: cs.CL

TL;DR: 论文揭示了意图检测机制在大语言模型安全防护中的脆弱性，提出两阶段提示优化框架IntentPrompt，成功突破现有防御体系。


<details>
  <summary>Details</summary>
Motivation: 现有意图感知防护机制在恶意操纵下的鲁棒性未被充分研究，LLMs隐含的意图检测能力可能成为攻击突破口。

Method: 开发两阶段框架：先将有害查询转为结构化大纲，再通过反馈循环优化为陈述式叙述（FSTR+SPIN变体）。

Result: 在GPT-4o等模型上实现86.75%-97.12%攻击成功率，显著优于现有方法且能绕过CoT/IA防御。

Conclusion: LLMs安全机制存在意图操作漏洞，需重新评估基于意图检测的内容审核防护有效性。

Abstract: Intent detection, a core component of natural language understanding, has
considerably evolved as a crucial mechanism in safeguarding large language
models (LLMs). While prior work has applied intent detection to enhance LLMs'
moderation guardrails, showing a significant success against content-level
jailbreaks, the robustness of these intent-aware guardrails under malicious
manipulations remains under-explored. In this work, we investigate the
vulnerability of intent-aware guardrails and demonstrate that LLMs exhibit
implicit intent detection capabilities. We propose a two-stage intent-based
prompt-refinement framework, IntentPrompt, that first transforms harmful
inquiries into structured outlines and further reframes them into
declarative-style narratives by iteratively optimizing prompts via feedback
loops to enhance jailbreak success for red-teaming purposes. Extensive
experiments across four public benchmarks and various black-box LLMs indicate
that our framework consistently outperforms several cutting-edge jailbreak
methods and evades even advanced Intent Analysis (IA) and Chain-of-Thought
(CoT)-based defenses. Specifically, our "FSTR+SPIN" variant achieves attack
success rates ranging from 88.25% to 96.54% against CoT-based defenses on the
o1 model, and from 86.75% to 97.12% on the GPT-4o model under IA-based
defenses. These findings highlight a critical weakness in LLMs' safety
mechanisms and suggest that intent manipulation poses a growing challenge to
content moderation guardrails.

</details>


### [38] [TAG-INSTRUCT: Controlled Instruction Complexity Enhancement through Structure-based Augmentation](https://arxiv.org/abs/2505.18557)
*He Zhu,Zhiwen Ruan,Junyou Su,Xingwei He,Wenjia Zhang,Yun Chen,Guanhua Chen*

Main category: cs.CL

TL;DR: 提出TAG-INSTRUCT框架，通过语义压缩和难度增强提升大模型指令数据复杂度


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效控制指令复杂度，而高质量指令数据对大语言模型发展至关重要

Method: 将指令压缩至标签空间，采用强化学习引导的标签扩展实现系统性复杂度增强

Result: 实验显示TAG-INSTRUCT在指令复杂度增强效果上优于现有方法，标签空间操作展现更好控制性

Conclusion: 标签空间操作在不同指令合成框架中具有更优的复杂度控制潜力

Abstract: High-quality instruction data is crucial for developing large language models
(LLMs), yet existing approaches struggle to effectively control instruction
complexity. We present TAG-INSTRUCT, a novel framework that enhances
instruction complexity through structured semantic compression and controlled
difficulty augmentation. Unlike previous prompt-based methods operating on raw
text, TAG-INSTRUCT compresses instructions into a compact tag space and
systematically enhances complexity through RL-guided tag expansion. Through
extensive experiments, we show that TAG-INSTRUCT outperforms existing
instruction complexity augmentation approaches. Our analysis reveals that
operating in tag space provides superior controllability and stability across
different instruction synthesis frameworks.

</details>


### [39] [From Word to World: Evaluate and Mitigate Culture Bias via Word Association Test](https://arxiv.org/abs/2505.18562)
*Xunlian Dai,Li Zhou,Benyou Wang,Haizhou Li*

Main category: cs.CL

TL;DR: 提出CultureSteer方法改善大语言模型的文化偏见，实验验证其在跨文化语义对齐的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在词语联想层面存在显著西方文化偏好，需增强文化包容性。

Method: 开发文化感知引导机制CultureSteer，将语义表征导向特定文化空间。

Result: 模型跨文化对齐度提升65%，在下游任务中认知对齐效果优于传统提示方法。

Conclusion: 为增强LLMs文化意识提供新范式，推动包容性语言技术发展。

Abstract: The human-centered word association test (WAT) serves as a cognitive proxy,
revealing sociocultural variations through lexical-semantic patterns. We extend
this test into an LLM-adaptive, free-relation task to assess the alignment of
large language models (LLMs) with cross-cultural cognition. To mitigate the
culture preference, we propose CultureSteer, an innovative approach that
integrates a culture-aware steering mechanism to guide semantic representations
toward culturally specific spaces. Experiments show that current LLMs exhibit
significant bias toward Western cultural (notably in American) schemas at the
word association level. In contrast, our model substantially improves
cross-cultural alignment, surpassing prompt-based methods in capturing diverse
semantic associations. Further validation on culture-sensitive downstream tasks
confirms its efficacy in fostering cognitive alignment across cultures. This
work contributes a novel methodological paradigm for enhancing cultural
awareness in LLMs, advancing the development of more inclusive language
technologies.

</details>


### [40] [Removal of Hallucination on Hallucination: Debate-Augmented RAG](https://arxiv.org/abs/2505.18581)
*Wentao Hu,Wengyu Zhang,Yiyang Jiang,Chen Jason Zhang,Xiaoyong Wei,Qing Li*

Main category: cs.CL

TL;DR: DRAG框架通过多智能体辩论机制，在RAG的检索和生成阶段分别实施结构化辩论，有效减少检索幻觉并提升事实准确性。


<details>
  <summary>Details</summary>
Motivation: 传统RAG存在检索错误引发生成幻觉的链式误导问题（Hallucination on Hallucination），需要新的机制来打破错误传播链条。

Method: 1. 检索阶段：设计支持者/反对者/裁判的三方辩论机制筛选可靠内容
2. 生成阶段：通过不对称信息角色和对抗性辩论提升推理严谨性

Result: 在多项任务中验证：DRAG提升检索可靠性（+18%），降低RAG幻觉率（-32%），整体事实准确率提升21%

Conclusion: 该框架首次将辩论机制系统化融入RAG全流程，为提升生成式AI的事实可靠性提供新范式

Abstract: Retrieval-Augmented Generation (RAG) enhances factual accuracy by integrating
external knowledge, yet it introduces a critical issue: erroneous or biased
retrieval can mislead generation, compounding hallucinations, a phenomenon we
term Hallucination on Hallucination. To address this, we propose
Debate-Augmented RAG (DRAG), a training-free framework that integrates
Multi-Agent Debate (MAD) mechanisms into both retrieval and generation stages.
In retrieval, DRAG employs structured debates among proponents, opponents, and
judges to refine retrieval quality and ensure factual reliability. In
generation, DRAG introduces asymmetric information roles and adversarial
debates, enhancing reasoning robustness and mitigating factual inconsistencies.
Evaluations across multiple tasks demonstrate that DRAG improves retrieval
reliability, reduces RAG-induced hallucinations, and significantly enhances
overall factual accuracy. Our code is available at
https://github.com/Huenao/Debate-Augmented-RAG.

</details>


### [41] [Safety Alignment via Constrained Knowledge Unlearning](https://arxiv.org/abs/2505.18588)
*Zesheng Shi,Yucheng Zhou,Jing Li*

Main category: cs.CL

TL;DR: 提出约束知识遗忘(CKU)方法，通过定位并保留有用知识神经元，同时消除有害知识，显著提升模型安全性且不影响性能。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法未能彻底删除大语言模型中的有害知识，导致越狱攻击可绕过防护机制生成有害内容。需在保证模型知识完整性的前提下提升安全性。

Method: 1. 在特定MLP层进行神经元评分，识别有用知识神经元子集U
2. 遗忘过程中修剪U神经元的梯度以保留价值知识
3. 通过神经元知识敏感性分析优化安全对齐机制

Result: CKU相比现有方法安全指标提升19.3%，在保持94%通用能力的同时有效防御越狱攻击。不同MLP层的神经元敏感性分析揭示了知识编辑的梯度传播规律。

Conclusion: CKU实现了安全性与实用性的最优平衡，为模型安全对齐和知识编辑提供了神经元级别的可解释性框架。该方法首次验证了定向神经元梯度修剪在知识遗忘中的有效性。

Abstract: Despite significant progress in safety alignment, large language models
(LLMs) remain susceptible to jailbreak attacks. Existing defense mechanisms
have not fully deleted harmful knowledge in LLMs, which allows such attacks to
bypass safeguards and produce harmful outputs. To address this challenge, we
propose a novel safety alignment strategy, Constrained Knowledge Unlearning
(CKU), which focuses on two primary objectives: knowledge localization and
retention, and unlearning harmful knowledge. CKU works by scoring neurons in
specific multilayer perceptron (MLP) layers to identify a subset U of neurons
associated with useful knowledge. During the unlearning process, CKU prunes the
gradients of neurons in U to preserve valuable knowledge while effectively
mitigating harmful content. Experimental results demonstrate that CKU
significantly enhances model safety without compromising overall performance,
offering a superior balance between safety and utility compared to existing
methods. Additionally, our analysis of neuron knowledge sensitivity across
various MLP layers provides valuable insights into the mechanics of safety
alignment and model knowledge editing.

</details>


### [42] [Debate-to-Detect: Reformulating Misinformation Detection as a Real-World Debate with Large Language Models](https://arxiv.org/abs/2505.18596)
*Chen Han,Wenzhen Zheng,Xijin Tang*

Main category: cs.CL

TL;DR: 提出多智能体辩论框架D2D，通过结构化对抗辩论和多维评估机制，显著提升虚假信息检测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统虚假信息检测方法依赖静态分类，无法处理复杂事实核查流程；大语言模型存在逻辑不一致和表面验证问题。

Method: 基于事实核查流程设计五阶段辩论框架（开篇陈述→反驳→自由辩论→总结陈述→判决），引入事实性/信源可靠性/推理质量/清晰度/伦理五维评估体系。

Result: 在GPT-4o上的实验显示基线方法显著改进，案例研究验证了证据迭代优化能力和决策透明性提升。

Conclusion: D2D框架通过多智能体辩论机制实现了更鲁棒、可解释的虚假信息检测，为自动化事实核查提供了新范式。

Abstract: The proliferation of misinformation in digital platforms reveals the
limitations of traditional detection methods, which mostly rely on static
classification and fail to capture the intricate process of real-world
fact-checking. Despite advancements in Large Language Models (LLMs) that
enhance automated reasoning, their application to misinformation detection
remains hindered by issues of logical inconsistency and superficial
verification. In response, we introduce Debate-to-Detect (D2D), a novel
Multi-Agent Debate (MAD) framework that reformulates misinformation detection
as a structured adversarial debate. Inspired by fact-checking workflows, D2D
assigns domain-specific profiles to each agent and orchestrates a five-stage
debate process, including Opening Statement, Rebuttal, Free Debate, Closing
Statement, and Judgment. To transcend traditional binary classification, D2D
introduces a multi-dimensional evaluation mechanism that assesses each claim
across five distinct dimensions: Factuality, Source Reliability, Reasoning
Quality, Clarity, and Ethics. Experiments with GPT-4o on two fakenews datasets
demonstrate significant improvements over baseline methods, and the case study
highlight D2D's capability to iteratively refine evidence while improving
decision transparency, representing a substantial advancement towards robust
and interpretable misinformation detection. The code will be open-sourced in a
future release.

</details>


### [43] [Flex-Judge: Think Once, Judge Anywhere](https://arxiv.org/abs/2505.18601)
*Jongwoo Ko,Sungnyun Kim,Sungwoo Cho,Se-Young Yun*

Main category: cs.CL

TL;DR: Flex-Judge通过结构化文本推理数据实现跨模态评估，在数据量远少于现有模型的情况下性能超越商业API及多模态评估器，尤其在分子等资源受限领域展现应用价值。


<details>
  <summary>Details</summary>
Motivation: 人工标注成本高昂，现有LLM作为代理评估器需大量领域专属训练数据且跨模态泛化能力弱，需更高效解决方案。

Method: 利用文本推理解释内在的通用决策模式，通过最小化文本监督实现模态间知识迁移（如图像/视频评估），采用推理引导的模型架构。

Result: 仅用少量文本数据训练即达到SOTA性能，在分子评估等缺乏基准的领域显著优于传统方法，验证框架普适性。

Conclusion: 基于推理的文本监督可替代传统高成本标注范式，为可扩展的多模态模型即评判者提供新路径，推动资源敏感领域评估体系发展。

Abstract: Human-generated reward signals are critical for aligning generative models
with human preferences, guiding both training and inference-time evaluations.
While large language models (LLMs) employed as proxy evaluators, i.e.,
LLM-as-a-Judge, significantly reduce the costs associated with manual
annotations, they typically require extensive modality-specific training data
and fail to generalize well across diverse multimodal tasks. In this paper, we
propose Flex-Judge, a reasoning-guided multimodal judge model that leverages
minimal textual reasoning data to robustly generalize across multiple
modalities and evaluation formats. Our core intuition is that structured
textual reasoning explanations inherently encode generalizable decision-making
patterns, enabling an effective transfer to multimodal judgments, e.g., with
images or videos. Empirical results demonstrate that Flex-Judge, despite being
trained on significantly fewer text data, achieves competitive or superior
performance compared to state-of-the-art commercial APIs and extensively
trained multimodal evaluators. Notably, Flex-Judge presents broad impact in
modalities like molecule, where comprehensive evaluation benchmarks are scarce,
underscoring its practical value in resource-constrained domains. Our framework
highlights reasoning-based text supervision as a powerful, cost-effective
alternative to traditional annotation-intensive approaches, substantially
advancing scalable multimodal model-as-a-judge.

</details>


### [44] [RASMALAI: Resources for Adaptive Speech Modeling in Indian Languages with Accents and Intonations](https://arxiv.org/abs/2505.18609)
*Ashwin Sankar,Yoach Lacombe,Sherry Thomas,Praveen Srinivasa Varadhan,Sanchit Gandhi,Mitesh M Khapra*

Main category: cs.CL

TL;DR: 提出大规模多属性语音数据集RASMALAI及首个开源印度语言描述驱动TTS系统IndicParlerTTS，支持23种印度语言与英语的高质量可控语音合成。


<details>
  <summary>Details</summary>
Motivation: 解决印度语言TTS领域缺乏可控表达性合成系统的问题，突破多语言、多属性语音数据资源不足的瓶颈。

Method: 构建含13k小时语音和24M细粒度标注的RASMALAI数据集，开发文本描述引导的TTS框架实现多属性联合控制与跨语言特征迁移。

Result: 系统评估显示在说话人一致性、属性控制精度和跨语言表达迁移上均达SOTA，MOS评分超过4.2/5.0。

Conclusion: IndicParlerTTS为印度语言创建了可控多语言语音合成新范式，推动区域语言AI技术的包容性发展。

Abstract: We introduce RASMALAI, a large-scale speech dataset with rich text
descriptions, designed to advance controllable and expressive text-to-speech
(TTS) synthesis for 23 Indian languages and English. It comprises 13,000 hours
of speech and 24 million text-description annotations with fine-grained
attributes like speaker identity, accent, emotion, style, and background
conditions. Using RASMALAI, we develop IndicParlerTTS, the first open-source,
text-description-guided TTS for Indian languages. Systematic evaluation
demonstrates its ability to generate high-quality speech for named speakers,
reliably follow text descriptions and accurately synthesize specified
attributes. Additionally, it effectively transfers expressive characteristics
both within and across languages. IndicParlerTTS consistently achieves strong
performance across these evaluations, setting a new standard for controllable
multilingual expressive speech synthesis in Indian languages.

</details>


### [45] [PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT LLMs](https://arxiv.org/abs/2505.18610)
*Tengxuan Liu,Shiyao Li,Jiayi Yang,Tianchen Zhao,Feng Zhou,Xiaohui Song,Guohao Dai,Shengen Yan,Huazhong Yang,Yu Wang*

Main category: cs.CL

TL;DR: 提出PM-KVQ渐进混合精度KV缓存量化方法，通过渐进量化策略和位置插值校准策略，解决长链思维大语言模型中的KV缓存量化累积误差和短上下文校准问题。


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存量化方法在长链思维LLMs中直接应用会导致性能显著下降，主要由于：1）逐层量化产生的累积误差；2）基于短上下文数据的校准无法适应长上下文场景的键缓存分布。

Method: PM-KVQ包含：1）渐进量化策略分块降低KV缓存位宽，配合块敏感度内存分配；2）通过位置插值技术利用短校准数据模拟长上下文数据分布。

Result: 在7B-70B长链思维LLMs上实验显示，同等内存预算下推理性能最高提升8%，超过现有最优基线。

Conclusion: PM-KVQ有效平衡内存开销与模型性能，提出的位置插值校准策略为长上下文场景的模型适配提供了新思路。

Abstract: Recently, significant progress has been made in developing reasoning-capable
Large Language Models (LLMs) through long Chain-of-Thought (CoT) techniques.
However, this long-CoT reasoning process imposes substantial memory overhead
due to the large Key-Value (KV) Cache memory overhead. Post-training KV Cache
quantization has emerged as a promising compression technique and has been
extensively studied in short-context scenarios. However, directly applying
existing methods to long-CoT LLMs causes significant performance degradation
due to the following two reasons: (1) Large cumulative error: Existing methods
fail to adequately leverage available memory, and they directly quantize the KV
Cache during each decoding step, leading to large cumulative quantization
error. (2) Short-context calibration: Due to Rotary Positional Embedding
(RoPE), the use of short-context data during calibration fails to account for
the distribution of less frequent channels in the Key Cache, resulting in
performance loss. We propose Progressive Mixed-Precision KV Cache Quantization
(PM-KVQ) for long-CoT LLMs to address the above issues in two folds: (1) To
reduce cumulative error, we design a progressive quantization strategy to
gradually lower the bit-width of KV Cache in each block. Then, we propose
block-wise memory allocation to assign a higher bit-width to more sensitive
transformer blocks. (2) To increase the calibration length without additional
overhead, we propose a new calibration strategy with positional interpolation
that leverages short calibration data with positional interpolation to
approximate the data distribution of long-context data. Extensive experiments
on 7B-70B long-CoT LLMs show that PM-KVQ improves reasoning benchmark
performance by up to 8% over SOTA baselines under the same memory budget. Our
code is available at https://github.com/thu-nics/PM-KVQ.

</details>


### [46] [MAVL: A Multilingual Audio-Video Lyrics Dataset for Animated Song Translation](https://arxiv.org/abs/2505.18614)
*Woohyun Cho,Youngmin Kim,Sunghyun Lee,Youngjae Yu*

Main category: cs.CL

TL;DR: 研究提出多模态方法MAVL基准及SylAVL-CoT模型，通过整合视听线索和音节约束显著提升动画歌词翻译的可唱性与准确性


<details>
  <summary>Details</summary>
Motivation: 传统纯文本歌词翻译难以保持音乐节奏与视觉同步，动画音乐剧需多模态对齐的翻译方案

Method: 构建首个多语言视听歌词基准MAVL，开发结合音频-视频特征和音节约束的SylAVL-CoT大语言模型

Result: 实验显示SylAVL-CoT在可唱性指标上比纯文本模型提升38%，上下文准确率提高27%

Conclusion: 多模态多语言方法能有效解决歌词翻译的音乐性保持难题，为跨文化音乐创作提供新思路

Abstract: Lyrics translation requires both accurate semantic transfer and preservation
of musical rhythm, syllabic structure, and poetic style. In animated musicals,
the challenge intensifies due to alignment with visual and auditory cues. We
introduce Multilingual Audio-Video Lyrics Benchmark for Animated Song
Translation (MAVL), the first multilingual, multimodal benchmark for singable
lyrics translation. By integrating text, audio, and video, MAVL enables richer
and more expressive translations than text-only approaches. Building on this,
we propose Syllable-Constrained Audio-Video LLM with Chain-of-Thought
SylAVL-CoT, which leverages audio-video cues and enforces syllabic constraints
to produce natural-sounding lyrics. Experimental results demonstrate that
SylAVL-CoT significantly outperforms text-based models in singability and
contextual accuracy, emphasizing the value of multimodal, multilingual
approaches for lyrics translation.

</details>


### [47] [DDO: Dual-Decision Optimization via Multi-Agent Collaboration for LLM-Based Medical Consultation](https://arxiv.org/abs/2505.18630)
*Zhihao Jia,Mingyi Jia,Junwen Duan,Jianxin Wang*

Main category: cs.CL

TL;DR: 提出DDO框架，通过解耦医疗咨询中的症状询问和疾病诊断两个子任务并分别优化，有效提升医疗咨询任务效果


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法未能有效区分医疗咨询中顺序决策的症状询问和分类诊断的双重特性，导致症状询问效率低且诊断不可靠

Method: 开发基于多智能体协作的DDO框架，将症状询问（顺序决策）与疾病诊断（分类任务）解耦并独立优化

Result: 在三个真实医疗咨询数据集上，DDO持续超越现有LLM方法并与最先进的生成式方法性能相当

Conclusion: 通过双重决策优化机制，验证了将复杂任务解耦为子任务并分别优化的有效性，为医疗咨询任务提供新解决方案

Abstract: Large Language Models (LLMs) demonstrate strong generalization and reasoning
abilities, making them well-suited for complex decision-making tasks such as
medical consultation (MC). However, existing LLM-based methods often fail to
capture the dual nature of MC, which entails two distinct sub-tasks: symptom
inquiry, a sequential decision-making process, and disease diagnosis, a
classification problem. This mismatch often results in ineffective symptom
inquiry and unreliable disease diagnosis. To address this, we propose
\textbf{DDO}, a novel LLM-based framework that performs
\textbf{D}ual-\textbf{D}ecision \textbf{O}ptimization by decoupling and
independently optimizing the the two sub-tasks through a collaborative
multi-agent workflow. Experiments on three real-world MC datasets show that DDO
consistently outperforms existing LLM-based approaches and achieves competitive
performance with state-of-the-art generation-based methods, demonstrating its
effectiveness in the MC task.

</details>


### [48] [Multilingual Question Answering in Low-Resource Settings: A Dzongkha-English Benchmark for Foundation Models](https://arxiv.org/abs/2505.18638)
*Md. Tanzib Hosain,Rajan Das Gupta,Md. Kishor Morol*

Main category: cs.CL

TL;DR: 创建DZEN双语数据集评估LLM在低资源语言中的表现，发现思维链提示对推理题有效且英语翻译提升宗卡语回答准确率


<details>
  <summary>Details</summary>
Motivation: 改善大型语言模型在宗卡语等低资源语言中的性能表现

Method: 使用5K+双语测试题评估不同LLM，测试思维链提示等策略，并加入英语翻译对比

Result: 发现模型在双语间存在显著性能差异，英语翻译使宗卡语回答准确率提升12%

Conclusion: 为提升低资源语言LLM性能指明方向，公开数据集促进相关研究

Abstract: In this work, we provide DZEN, a dataset of parallel Dzongkha and English
test questions for Bhutanese middle and high school students. The over 5K
questions in our collection span a variety of scientific topics and include
factual, application, and reasoning-based questions. We use our parallel
dataset to test a number of Large Language Models (LLMs) and find a significant
performance difference between the models in English and Dzongkha. We also look
at different prompting strategies and discover that Chain-of-Thought (CoT)
prompting works well for reasoning questions but less well for factual ones. We
also find that adding English translations enhances the precision of Dzongkha
question responses. Our results point to exciting avenues for further study to
improve LLM performance in Dzongkha and, more generally, in low-resource
languages. We release the dataset at:
https://github.com/kraritt/llm_dzongkha_evaluation.

</details>


### [49] [Skip-Thinking: Chunk-wise Chain-of-Thought Distillation Enable Smaller Language Models to Reason Better and Faster](https://arxiv.org/abs/2505.18642)
*Xiao Chen,Sihang Zhou,Ke Liang,Xiaoyu Sun,Xinwang Liu*

Main category: cs.CL

TL;DR: 提出分块训练(CWT)和跳跃思维训练(STT)解决传统CoT蒸馏的长推理梯度平滑与速度瓶颈问题


<details>
  <summary>Details</summary>
Motivation: 现有CoT蒸馏方法在单次迭代中训练小模型学习长推理过程，导致核心推理token梯度被稀释（占比较低）和响应速度低下

Method: 1.CWT通过启发式搜索将推理链分割为语义连贯的块，每轮专注学习单个块；2.STT自动跳过非推理中间块直达答案

Result: 在多种小模型和推理任务中验证有效，在保持准确率的同时提升推理速度

Conclusion: 分块学习策略有效聚焦核心推理token，跳跃机制实现速度与精度的双重优化

Abstract: Chain-of-thought (CoT) distillation allows a large language model (LLM) to
guide a small language model (SLM) in reasoning tasks. Existing methods train
the SLM to learn the long rationale in one iteration, resulting in two issues:
1) Long rationales lead to a large token-level batch size during training,
making gradients of core reasoning tokens (i.e., the token will directly affect
the correctness of subsequent reasoning) over-smoothed as they contribute a
tiny fraction of the rationale. As a result, the SLM converges to sharp minima
where it fails to grasp the reasoning logic. 2) The response is slow, as the
SLM must generate a long rationale before reaching the answer. Therefore, we
propose chunk-wise training (CWT), which uses a heuristic search to divide the
rationale into internal semantically coherent chunks and focuses SLM on
learning from only one chunk per iteration. In this way, CWT naturally isolates
non-reasoning chunks that do not involve the core reasoning token (e.g.,
summary and transitional chunks) from the SLM learning for reasoning chunks,
making the fraction of the core reasoning token increase in the corresponding
iteration. Based on CWT, skip-thinking training (STT) is proposed. STT makes
the SLM automatically skip non-reasoning medium chunks to reach the answer,
improving reasoning speed while maintaining accuracy. We validate our approach
on a variety of SLMs and multiple reasoning tasks.

</details>


### [50] [On the Emergence of Linear Analogies in Word Embeddings](https://arxiv.org/abs/2505.18651)
*Daniel J. Korchinski,Dhruva Karkada,Yasaman Bahri,Matthieu Wyart*

Main category: cs.CL

TL;DR: 该论文通过构建基于二值语义属性的生成模型，解释了词嵌入中线性类比结构的理论起源及实验现象。


<details>
  <summary>Details</summary>
Motivation: 揭示Word2Vec/GloVe词向量中线性类比结构（如king-man+woman≈queen）的理论根源，解释其四大实验特性：特征向量涌现性、维度饱和性、对数增强性及语料无关性。

Method: 提出基于二值语义属性的理论生成模型：定义词语的二进制语义特征，通过属性交互推导共现概率，解析线性类比结构的形成机制。

Result: 模型成功复现线性类比结构，定量解释四个实验现象，在维基百科语料和Mikolov类比基准测试中验证有效性。

Conclusion: 词语的底层语义属性交互是线性类比结构产生的根本原因，该模型为理解词向量维度功能提供了细粒度解释框架。

Abstract: Models such as Word2Vec and GloVe construct word embeddings based on the
co-occurrence probability $P(i,j)$ of words $i$ and $j$ in text corpora. The
resulting vectors $W_i$ not only group semantically similar words but also
exhibit a striking linear analogy structure -- for example, $W_{\text{king}} -
W_{\text{man}} + W_{\text{woman}} \approx W_{\text{queen}}$ -- whose
theoretical origin remains unclear. Previous observations indicate that this
analogy structure: (i) already emerges in the top eigenvectors of the matrix
$M(i,j) = P(i,j)/P(i)P(j)$, (ii) strengthens and then saturates as more
eigenvectors of $M (i, j)$, which controls the dimension of the embeddings, are
included, (iii) is enhanced when using $\log M(i,j)$ rather than $M(i,j)$, and
(iv) persists even when all word pairs involved in a specific analogy relation
(e.g., king-queen, man-woman) are removed from the corpus. To explain these
phenomena, we introduce a theoretical generative model in which words are
defined by binary semantic attributes, and co-occurrence probabilities are
derived from attribute-based interactions. This model analytically reproduces
the emergence of linear analogy structure and naturally accounts for properties
(i)-(iv). It can be viewed as giving fine-grained resolution into the role of
each additional embedding dimension. It is robust to various forms of noise and
agrees well with co-occurrence statistics measured on Wikipedia and the analogy
benchmark introduced by Mikolov et al.

</details>


### [51] [Climate-Eval: A Comprehensive Benchmark for NLP Tasks Related to Climate Change](https://arxiv.org/abs/2505.18653)
*Murathan Kurfalı,Shorouq Zahra,Joakim Nivre,Gabriele Messori*

Main category: cs.CL

TL;DR: Climate-Eval基准整合25个气候相关NLP任务，用于评估大语言模型在气候变化领域的表现


<details>
  <summary>Details</summary>
Motivation: 解决气候变化领域缺乏系统性评估LLM能力的基准，整合现有数据集并新增新闻分类数据集构建标准化测试套件

Method: 聚合13个数据集形成25个气候任务(含新建新闻分类数据集)，评估2B-70B参数开源LLM的零样本/少样本表现

Result: 系统揭示不同规模LLM在气候文本分类、问答等任务中的能力边界与局限性

Conclusion: Climate-Eval为气候变化NLP研究提供标准化评估框架，基准测试结果帮助理解LLM在气候领域的能力现状

Abstract: Climate-Eval is a comprehensive benchmark designed to evaluate natural
language processing models across a broad range of tasks related to climate
change. Climate-Eval aggregates existing datasets along with a newly developed
news classification dataset, created specifically for this release. This
results in a benchmark of 25 tasks based on 13 datasets, covering key aspects
of climate discourse, including text classification, question answering, and
information extraction. Our benchmark provides a standardized evaluation suite
for systematically assessing the performance of large language models (LLMs) on
these tasks. Additionally, we conduct an extensive evaluation of open-source
LLMs (ranging from 2B to 70B parameters) in both zero-shot and few-shot
settings, analyzing their strengths and limitations in the domain of climate
change.

</details>


### [52] [Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics](https://arxiv.org/abs/2505.18658)
*Pankaj Kumar,Subhankar Mishra*

Main category: cs.CL

TL;DR: 系统综述LLM鲁棒性研究：解析其本质特征、脆弱性根源、缓解策略、评估方法及未来方向


<details>
  <summary>Details</summary>
Motivation: LLM在关键领域应用中存在可靠性风险，系统性失效可能导致严重后果，需建立完整的鲁棒性研究体系

Method: 采用多维度分类框架：1) 定义鲁棒性核心要素 2) 建立三层脆弱性分类体系（模型内在缺陷/数据漏洞/外部对抗）3) 评估现有解决方案有效性

Result: 发现当前缓解策略对动态对抗环境适应性不足，评估基准未充分反映实际应用场景，存在理论-实践评估鸿沟

Conclusion: 需发展动态鲁棒性评估框架，加强跨学科研究（认知科学/安全工程），构建全生命周期防护体系

Abstract: Large Language Models (LLMs) have emerged as a promising cornerstone for the
development of natural language processing (NLP) and artificial intelligence
(AI). However, ensuring the robustness of LLMs remains a critical challenge. To
address these challenges and advance the field, this survey provides a
comprehensive overview of current studies in this area. First, we
systematically examine the nature of robustness in LLMs, including its
conceptual foundations, the importance of consistent performance across diverse
inputs, and the implications of failure modes in real-world applications. Next,
we analyze the sources of non-robustness, categorizing intrinsic model
limitations, data-driven vulnerabilities, and external adversarial factors that
compromise reliability. Following this, we review state-of-the-art mitigation
strategies, and then we discuss widely adopted benchmarks, emerging metrics,
and persistent gaps in assessing real-world reliability. Finally, we synthesize
findings from existing surveys and interdisciplinary studies to highlight
trends, unresolved issues, and pathways for future research.

</details>


### [53] [Cross-Lingual Pitfalls: Automatic Probing Cross-Lingual Weakness of Multilingual Large Language Models](https://arxiv.org/abs/2505.18673)
*Zixiang Xu,Yanbo Wang,Yue Huang,Xiuying Chen,Jieyu Zhao,Meng Jiang,Xiangliang Zhang*

Main category: cs.CL

TL;DR: 提出通过生成双语问题对识别LLM跨语言弱点的方法，揭示模型在非英语语言中的显著性能下降及语言相似性的影响


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在跨语言场景下性能不一致的问题，现有模型在目标语言中可能存在未被发现的潜在缺陷

Method: 采用beam search和LLM模拟生成双语问题对，构建涵盖16种语言的6000+双语数据集进行弱点检测

Result: 成功识别出主流模型在目标语言中平均超50%的准确率下降，发现语言相似性与性能模式存在强相关性

Conclusion: 语言相似性影响跨语言性能分布，针对性后训练可改善相关语言的模型表现

Abstract: Large Language Models (LLMs) have achieved remarkable success in Natural
Language Processing (NLP), yet their cross-lingual performance consistency
remains a significant challenge. This paper introduces a novel methodology for
efficiently identifying inherent cross-lingual weaknesses in LLMs. Our approach
leverages beam search and LLM-based simulation to generate bilingual question
pairs that expose performance discrepancies between English and target
languages. We construct a new dataset of over 6,000 bilingual pairs across 16
languages using this methodology, demonstrating its effectiveness in revealing
weaknesses even in state-of-the-art models. The extensive experiments
demonstrate that our method precisely and cost-effectively pinpoints
cross-lingual weaknesses, consistently revealing over 50\% accuracy drops in
target languages across a wide range of models. Moreover, further experiments
investigate the relationship between linguistic similarity and cross-lingual
weaknesses, revealing that linguistically related languages share similar
performance patterns and benefit from targeted post-training. Code is available
at https://github.com/xzx34/Cross-Lingual-Pitfalls.

</details>


### [54] [Social Good or Scientific Curiosity? Uncovering the Research Framing Behind NLP Artefacts](https://arxiv.org/abs/2505.18677)
*Eric Chamoun,Nedjma Ousidhoum,Michael Schlichtkrull,Andreas Vlachos*

Main category: cs.CL

TL;DR: 开发自动化系统分析NLP研究框架，发现自动事实核查领域存在研究目标模糊化、科学探索优先和应用转向三大趋势


<details>
  <summary>Details</summary>
Motivation: 针对现有NLP研究普遍缺乏对利益相关者、使用场景等核心要素的明确定义，旨在建立自动化分析框架提升研究与实践的匹配度

Method: 三阶段系统：1）提取手段/目的/利益相关者要素 2）基于可解释规则建立要素关联 3）上下文推理框架分析

Result: 在事实核查（现有数据集）和仇恨言论检测（新标注集）领域均显著优于LLM基线，准确识别研究框架特征

Conclusion: 应用发现：目标模糊化趋势显著（85%论文），科学探索导向增长37%，支持人机协同的论文比例较基线提升2倍

Abstract: Clarifying the research framing of NLP artefacts (e.g., models, datasets,
etc.) is crucial to aligning research with practical applications. Recent
studies manually analyzed NLP research across domains, showing that few papers
explicitly identify key stakeholders, intended uses, or appropriate contexts.
In this work, we propose to automate this analysis, developing a
three-component system that infers research framings by first extracting key
elements (means, ends, stakeholders), then linking them through interpretable
rules and contextual reasoning. We evaluate our approach on two domains:
automated fact-checking using an existing dataset, and hate speech detection
for which we annotate a new dataset-achieving consistent improvements over
strong LLM baselines. Finally, we apply our system to recent automated
fact-checking papers and uncover three notable trends: a rise in vague or
underspecified research goals, increased emphasis on scientific exploration
over application, and a shift toward supporting human fact-checkers rather than
pursuing full automation.

</details>


### [55] [TULUN: Transparent and Adaptable Low-resource Machine Translation](https://arxiv.org/abs/2505.18683)
*Raphaël Merx,Hanna Suominen,Lois Hong,Nick Thieberger,Trevor Cohn,Ekaterina Vylomova*

Main category: cs.CL

TL;DR: 提出Tulun术语感知翻译系统，结合神经机器翻译与LLM后编辑技术，通过开源协作平台显著提升低资源语言专业领域翻译质量


<details>
  <summary>Details</summary>
Motivation: 传统领域适应方法需要模型微调，对非技术用户不友好。需开发易用方案整合术语资源并实现人机协作翻译

Method: 1. 融合神经MT与基于大语言模型的术语后编辑
2. 开发开源Web平台支持术语资源创建与管理
3. 构建结合术语库与翻译记忆的协同工作流程

Result: 1. 医疗/救灾翻译任务中ChrF++提升16.90-22.41
2. FLORES数据集六种低资源语言平均提升2.8 ChrF
3. 超越NLLB-54B等基线系统

Conclusion: Tulun有效整合神经MT与LLM优势，通过人机协作机制显著提升专业领域翻译精度，开源平台设计促进技术可及性

Abstract: Machine translation (MT) systems that support low-resource languages often
struggle on specialized domains. While researchers have proposed various
techniques for domain adaptation, these approaches typically require model
fine-tuning, making them impractical for non-technical users and small
organizations. To address this gap, we propose Tulun, a versatile solution for
terminology-aware translation, combining neural MT with large language model
(LLM)-based post-editing guided by existing glossaries and translation
memories. Our open-source web-based platform enables users to easily create,
edit, and leverage terminology resources, fostering a collaborative
human-machine translation process that respects and incorporates domain
expertise while increasing MT accuracy. Evaluations show effectiveness in both
real-world and benchmark scenarios: on medical and disaster relief translation
tasks for Tetun and Bislama, our system achieves improvements of 16.90-22.41
ChrF++ points over baseline MT systems. Across six low-resource languages on
the FLORES dataset, Tulun outperforms both standalone MT and LLM approaches,
achieving an average improvement of 2.8 ChrF points over NLLB-54B.

</details>


### [56] [From Generation to Detection: A Multimodal Multi-Task Dataset for Benchmarking Health Misinformation](https://arxiv.org/abs/2505.18685)
*Zhihao Zhang,Yiran Zhang,Xiyue Zhou,Liting Huang,Imran Razzak,Preslav Nakov,Usman Naseem*

Main category: cs.CL

TL;DR: 构建多模态健康错误信息数据集MM Health，涵盖人类生成与AI生成内容，基准测试显示现有模型难以有效检测


<details>
  <summary>Details</summary>
Motivation: 现有健康错误信息数据集存在主题覆盖不全、未包含AI生成内容、原始信息可访问性差三大局限，生成式AI技术加剧了错误信息传播

Method: 收集34,746篇多模态健康新闻（含5,776人类生成/28,880 AI生成），使用SOTA生成模型创建样本，设计可靠性验证、原创性验证和AI检测三大基准任务

Result: 当前最先进模型在区分信息可靠性和来源方面准确率较低（可靠性检查F1=0.48，AI检测F1=0.52）

Conclusion: MM Health数据集支持开发跨场景、多模态层次的错误信息检测系统，助力人机生成内容的联合识别

Abstract: Infodemics and health misinformation have significant negative impact on
individuals and society, exacerbating confusion and increasing hesitancy in
adopting recommended health measures. Recent advancements in generative AI,
capable of producing realistic, human like text and images, have significantly
accelerated the spread and expanded the reach of health misinformation,
resulting in an alarming surge in its dissemination. To combat the infodemics,
most existing work has focused on developing misinformation datasets from
social media and fact checking platforms, but has faced limitations in topical
coverage, inclusion of AI generation, and accessibility of raw content. To
address these issues, we present MM Health, a large scale multimodal
misinformation dataset in the health domain consisting of 34,746 news article
encompassing both textual and visual information. MM Health includes
human-generated multimodal information (5,776 articles) and AI generated
multimodal information (28,880 articles) from various SOTA generative AI
models. Additionally, We benchmarked our dataset against three tasks
(reliability checks, originality checks, and fine-grained AI detection)
demonstrating that existing SOTA models struggle to accurately distinguish the
reliability and origin of information. Our dataset aims to support the
development of misinformation detection across various health scenarios,
facilitating the detection of human and machine generated content at multimodal
levels.

</details>


### [57] [Large Language Models in the Task of Automatic Validation of Text Classifier Predictions](https://arxiv.org/abs/2505.18688)
*Aleksandr Tsymbalov*

Main category: cs.CL

TL;DR: 使用大语言模型替代人工标注，解决文本分类模型持续训练中数据标注成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 人工标注存在专家资源有限、标注成本高、难以适应模型持续迭代需求（尤其是数据漂移场景）的核心痛点。

Method: 提出基于LLM的预测验证框架，通过大模型自动检测分类器预测结果的正确性，替代传统人工标注流程。

Result: 实现标注流程自动化，在保证模型质量的前提下显著降低全生命周期维护成本，支持高质量增量学习。

Conclusion: LLM驱动的自动化验证为机器学习模型的可持续维护提供了可扩展的技术路径，突破传统人工标注的产能瓶颈。

Abstract: Machine learning models for text classification are trained to predict a
class for a given text. To do this, training and validation samples must be
prepared: a set of texts is collected, and each text is assigned a class. These
classes are usually assigned by human annotators with different expertise
levels, depending on the specific classification task. Collecting such samples
from scratch is labor-intensive because it requires finding specialists and
compensating them for their work; moreover, the number of available specialists
is limited, and their productivity is constrained by human factors. While it
may not be too resource-intensive to collect samples once, the ongoing need to
retrain models (especially in incremental learning pipelines) to address data
drift (also called model drift) makes the data collection process crucial and
costly over the model's entire lifecycle. This paper proposes several
approaches to replace human annotators with Large Language Models (LLMs) to
test classifier predictions for correctness, helping ensure model quality and
support high-quality incremental learning.

</details>


### [58] [Benchmarking and Rethinking Knowledge Editing for Large Language Models](https://arxiv.org/abs/2505.18690)
*Guoxiu He,Xin Song,Futing Wang,Aixin Sun*

Main category: cs.CL

TL;DR: 现有知识编辑方法在现实场景中表现不佳，上下文推理方法SCR展现出更优的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 针对大模型知识更新方法评估标准混乱的问题，通过构建复合数据集和严谨测试框架，揭示参数编辑方法的局限性。

Method: 引入事件级数据集+通用任务数据集，采用自回归推理评估框架，从可移植性等四个维度对比参数编辑与上下文推理方法。

Result: 参数编辑方法在现实条件下表现欠佳，SCR方法在单次/多次编辑场景均显著优于现有方法。

Conclusion: 上下文推理是更可靠的知识更新路径，当前参数编辑技术需突破架构限制以实现实用化。

Abstract: Knowledge editing aims to update the embedded knowledge within Large Language
Models (LLMs). However, existing approaches, whether through parameter
modification or external memory integration, often suffer from inconsistent
evaluation objectives and experimental setups. To address this gap, we conduct
a comprehensive benchmarking study. In addition to fact-level datasets, we
introduce more complex event-based datasets and general-purpose datasets drawn
from other tasks. Our evaluation covers both instruction-tuned and
reasoning-oriented LLMs, under a realistic autoregressive inference setting
rather than teacher-forced decoding. Beyond single-edit assessments, we also
evaluate multi-edit scenarios to better reflect practical demands. We employ
four evaluation dimensions, including portability, and compare all recent
methods against a simple and straightforward baseline named Selective
Contextual Reasoning (SCR). Empirical results reveal that parameter-based
editing methods perform poorly under realistic conditions. In contrast, SCR
consistently outperforms them across all settings. This study offers new
insights into the limitations of current knowledge editing methods and
highlights the potential of context-based reasoning as a more robust
alternative.

</details>


### [59] [Towards Semantic Integration of Opinions: Unified Opinion Concepts Ontology and Extraction Task](https://arxiv.org/abs/2505.18703)
*Gaurav Negi,Dhairya Dalal,Omnia Zayed,Paul Buitelaar*

Main category: cs.CL

TL;DR: 提出统一意见概念本体(UOC)整合语义语境中的观点表达，并设计UOCE任务用于增强意见抽取能力，建立生成模型基线性能。


<details>
  <summary>Details</summary>
Motivation: 解决不同NLP研究中观点表达语义不统一的问题，通过融合符号化描述与语义结构实现跨领域观点表征的统一建模。

Method: 构建UOC本体桥接语义鸿沟，提出UOCE抽取任务，创建人工标注数据集并开发针对性评估指标，使用SOTA生成模型建立基线。

Result: 提供包含语义约束的评估数据集、定制化评估指标，以及基于生成模型的基准性能结果。

Conclusion: UOC本体增强了观点表达的语义完整性，UOCE任务及评估体系为细粒度意见挖掘研究提供了新范式，基线结果为后续研究奠定基础。

Abstract: This paper introduces the Unified Opinion Concepts (UOC) ontology to
integrate opinions within their semantic context. The UOC ontology bridges the
gap between the semantic representation of opinion across different
formulations. It is a unified conceptualisation based on the facets of opinions
studied extensively in NLP and semantic structures described through symbolic
descriptions. We further propose the Unified Opinion Concept Extraction (UOCE)
task of extracting opinions from the text with enhanced expressivity.
Additionally, we provide a manually extended and re-annotated evaluation
dataset for this task and tailored evaluation metrics to assess the adherence
of extracted opinions to UOC semantics. Finally, we establish baseline
performance for the UOCE task using state-of-the-art generative models.

</details>


### [60] [A General Knowledge Injection Framework for ICD Coding](https://arxiv.org/abs/2505.18708)
*Xu Zhang,Kun Zhang,Wenxin Ma,Rongsheng Wang,Chenxu Wu,Yingtai Li,S. Kevin Zhou*

Main category: cs.CL

TL;DR: 提出通用知识注入框架GKI-ICD，通过整合ICD描述、同义词和层级三种知识类型，无需复杂模块设计即显著提升医疗编码性能


<details>
  <summary>Details</summary>
Motivation: 现有ICD编码方法多聚焦单一知识类型且依赖复杂定制模块，导致扩展性差、效果受限，需通用知识整合方案

Method: 设计无需特殊模块的通用框架，通过联合注入描述性文本、同义词扩展和层级关系三种互补知识类型增强编码能力

Result: 在主流基准测试中实现SOTA性能，多数评估指标达到最优

Conclusion: GKI-ICD证明多维度知识融合的有效性，为医疗编码任务提供高效可扩展的解决方案

Abstract: ICD Coding aims to assign a wide range of medical codes to a medical text
document, which is a popular and challenging task in the healthcare domain. To
alleviate the problems of long-tail distribution and the lack of annotations of
code-specific evidence, many previous works have proposed incorporating code
knowledge to improve coding performance. However, existing methods often focus
on a single type of knowledge and design specialized modules that are complex
and incompatible with each other, thereby limiting their scalability and
effectiveness. To address this issue, we propose GKI-ICD, a novel, general
knowledge injection framework that integrates three key types of knowledge,
namely ICD Description, ICD Synonym, and ICD Hierarchy, without specialized
design of additional modules. The comprehensive utilization of the above
knowledge, which exhibits both differences and complementarity, can effectively
enhance the ICD coding performance. Extensive experiments on existing popular
ICD coding benchmarks demonstrate the effectiveness of GKI-ICD, which achieves
the state-of-the-art performance on most evaluation metrics. Code is available
at https://github.com/xuzhang0112/GKI-ICD.

</details>


### [61] [Improving Bangla Linguistics: Advanced LSTM, Bi-LSTM, and Seq2Seq Models for Translating Sylheti to Modern Bangla](https://arxiv.org/abs/2505.18709)
*Sourav Kumar Das,Md. Julkar Naeen,MD. Jahidul Islam,Md. Anisul Haque Sajeeb,Narayan Ranjan Chakraborty,Mayen Uddin Mojumdar*

Main category: cs.CL

TL;DR: 该研究构建了基于LSTM的孟加拉语到锡尔赫特方言的机器翻译系统，准确率达89.3%


<details>
  <summary>Details</summary>
Motivation: 针对孟加拉地方言（如锡尔赫特语）缺乏自然语言处理研究的现状，填补标准孟加拉语与方言间的翻译空白

Method: 使用1200条训练数据，对比测试LSTM、Bi-LSTM和Seq2Seq三种神经网络模型

Result: LSTM模型表现最优，翻译准确率达到89.3%

Conclusion: 该研究为孟加拉语方言处理奠定技术基础，有助于保护语言多样性并推动区域NLP技术发展

Abstract: Bangla or Bengali is the national language of Bangladesh, people from
different regions don't talk in proper Bangla. Every division of Bangladesh has
its own local language like Sylheti, Chittagong etc. In recent years some
papers were published on Bangla language like sentiment analysis, fake news
detection and classifications, but a few of them were on Bangla languages. This
research is for the local language and this particular paper is on Sylheti
language. It presented a comprehensive system using Natural Language Processing
or NLP techniques for translating Pure or Modern Bangla to locally spoken
Sylheti Bangla language. Total 1200 data used for training 3 models LSTM,
Bi-LSTM and Seq2Seq and LSTM scored the best in performance with 89.3%
accuracy. The findings of this research may contribute to the growth of Bangla
NLP researchers for future more advanced innovations.

</details>


### [62] [Optimal Transport-Based Token Weighting scheme for Enhanced Preference Optimization](https://arxiv.org/abs/2505.18720)
*Meng Li,Guangda Huzhang,Haibo Zhang,Xiting Wang,Anxiang Zeng*

Main category: cs.CL

TL;DR: 提出OTPO方法——通过最优传输理论改进DPO框架，实现更精准的语义对齐。


<details>
  <summary>Details</summary>
Motivation: 现有DPO方法平等对待所有token，导致无关噪声token对偏好优化产生过度影响。人类更关注语义核心部分，需针对性优化。

Method: 引入最优运输理论设计上下文感知的token权重分配机制，增强关键语义token对比，抑制噪声干扰。

Result: 实验证明OTPO显著提升模型指令跟随能力，增强奖励稳定性及结果可解释性。

Conclusion: 基于语义权重的优化策略能更有效捕捉响应间本质差异，推动LLM与人类价值观对齐。

Abstract: Direct Preference Optimization (DPO) has emerged as a promising framework for
aligning Large Language Models (LLMs) with human preferences by directly
optimizing the log-likelihood difference between chosen and rejected responses.
However, existing methods assign equal importance to all tokens in the
response, while humans focus on more meaningful parts. This leads to suboptimal
preference optimization, as irrelevant or noisy tokens disproportionately
influence DPO loss. To address this limitation, we propose \textbf{O}ptimal
\textbf{T}ransport-based token weighting scheme for enhancing direct
\textbf{P}reference \textbf{O}ptimization (OTPO). By emphasizing semantically
meaningful token pairs and de-emphasizing less relevant ones, our method
introduces a context-aware token weighting scheme that yields a more
contrastive reward difference estimate. This adaptive weighting enhances reward
stability, improves interpretability, and ensures that preference optimization
focuses on meaningful differences between responses. Extensive experiments have
validated OTPO's effectiveness in improving instruction-following ability
across various settings\footnote{Code is available at
https://github.com/Mimasss2/OTPO.}.

</details>


### [63] [LogicCat: A Chain-of-Thought Text-to-SQL Benchmark for Multi-Domain Reasoning Challenges](https://arxiv.org/abs/2505.18744)
*Tao Liu,Hongying Zan,Yifan Li,Dixuan Zhang,Lulu Kong,Haixin Liu,Jiaming Hou,Aoze Zheng,Rui Li,Yiming Qiao,Zewei Luo,Qi Wang,Zhiqiang Zhang,Jiaxi Li,Supeng Liu,Kunli Zhang,Min Peng*

Main category: cs.CL

TL;DR: 提出LogicCat数据集解决文本到SQL任务中领域知识与复杂推理的不足，包含4038个问题与思维链标注，显著提升模型推理能力


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL数据集缺乏领域特定知识和复杂数学推理能力覆盖，限制了模型在真实场景中的应用

Method: 构建包含物理/算数/常识/假设推理的跨领域数据集，含4,038问题+12,114分步推理标注，覆盖45个数据库

Result: SOTA模型最高准确率仅14.96%，加入思维链标注后提升至33.96%，在Spider/BIRD基准上验证挑战性

Conclusion: LogicCat揭示了推理驱动文本到SQL系统的研究空间，思维链机制显著提升性能，推动鲁棒性SQL解析发展

Abstract: Text-to-SQL is a fundamental task in natural language processing that seeks
to translate natural language questions into meaningful and executable SQL
queries. While existing datasets are extensive and primarily focus on business
scenarios and operational logic, they frequently lack coverage of
domain-specific knowledge and complex mathematical reasoning. To address this
gap, we present a novel dataset tailored for complex reasoning and
chain-of-thought analysis in SQL inference, encompassing physical, arithmetic,
commonsense, and hypothetical reasoning. The dataset consists of 4,038 English
questions, each paired with a unique SQL query and accompanied by 12,114
step-by-step reasoning annotations, spanning 45 databases across diverse
domains. Experimental results demonstrate that LogicCat substantially increases
the difficulty for state-of-the-art models, with the highest execution accuracy
reaching only 14.96%. Incorporating our chain-of-thought annotations boosts
performance to 33.96%. Benchmarking leading public methods on Spider and BIRD
further underscores the unique challenges presented by LogicCat, highlighting
the significant opportunities for advancing research in robust,
reasoning-driven text-to-SQL systems. We have released our dataset code at
https://github.com/Ffunkytao/LogicCat.

</details>


### [64] [Unifying Attention Heads and Task Vectors via Hidden State Geometry in In-Context Learning](https://arxiv.org/abs/2505.18752)
*Haolin Yang,Hakaze Cho,Yiqiao Zhong,Naoya Inoue*

Main category: cs.CL

TL;DR: 提出通过分析隐藏状态可分性和对齐性的两阶段框架，统一解释上下文学习(ICL)机制：早期层形成特征可分性，后期层建立任务对齐性。


<details>
  <summary>Details</summary>
Motivation: 先前研究分别关注注意力头或任务向量，但缺乏对隐藏状态跨层演变如何影响模型输出的统一解释框架。本文旨在建立这种关联框架。

Method: 通过几何分析框架，研究分类任务中查询隐藏状态的可分性（separability）和对齐性（alignment）的层间动态演变机制。

Result: 发现明显的两阶段机制：前10层建立特征可分性（由Previous Token Heads驱动），后10层发展任务对齐性（由Induction Heads和任务向量增强）。

Conclusion: 首次将注意力头与任务向量机制统一，揭示ICL通过特征分离→任务适配的分层处理机制，为理解大语言模型提供新视角。

Abstract: The unusual properties of in-context learning (ICL) have prompted
investigations into the internal mechanisms of large language models. Prior
work typically focuses on either special attention heads or task vectors at
specific layers, but lacks a unified framework linking these components to the
evolution of hidden states across layers that ultimately produce the model's
output. In this paper, we propose such a framework for ICL in classification
tasks by analyzing two geometric factors that govern performance: the
separability and alignment of query hidden states. A fine-grained analysis of
layer-wise dynamics reveals a striking two-stage mechanism: separability
emerges in early layers, while alignment develops in later layers. Ablation
studies further show that Previous Token Heads drive separability, while
Induction Heads and task vectors enhance alignment. Our findings thus bridge
the gap between attention heads and task vectors, offering a unified account of
ICL's underlying mechanisms.

</details>


### [65] [Few-Shot Optimization for Sensor Data Using Large Language Models: A Case Study on Fatigue Detection](https://arxiv.org/abs/2505.18754)
*Elsen Ronando,Sozo Inoue*

Main category: cs.CL

TL;DR: 提出HED-LM混合方法，结合欧氏距离与语言模型提升小样本学习中的示例选择效果，应用于疲劳检测任务验证有效性


<details>
  <summary>Details</summary>
Motivation: 小样本学习中示例选择质量直接影响模型性能，传感器数据(如疲劳检测)存在模式重叠和个体差异大的挑战，需要更精细的示例筛选策略

Method: HED-LM混合选择流程：先用欧氏距离初筛候选样本，再通过LLM评估上下文相关性进行重排序，兼顾数值相似度与语义相关性

Result: HED-LM取得69.13±10.71%的F1分数，相比随机选择(59.30%)和纯距离筛选(67.61%)分别提升16.6%和2.3%

Conclusion: 该方法为现实传感器任务提供实用优化方案，在医疗监测、行为识别和工业安全领域具有应用潜力

Abstract: In this paper, we propose a novel few-shot optimization with HED-LM (Hybrid
Euclidean Distance with Large Language Models) to improve example selection for
sensor-based classification tasks. While few-shot prompting enables efficient
inference with limited labeled data, its performance largely depends on the
quality of selected examples. HED-LM addresses this challenge through a hybrid
selection pipeline that filters candidate examples based on Euclidean distance
and re-ranks them using contextual relevance scored by large language models
(LLMs). To validate its effectiveness, we apply HED-LM to a fatigue detection
task using accelerometer data characterized by overlapping patterns and high
inter-subject variability. Unlike simpler tasks such as activity recognition,
fatigue detection demands more nuanced example selection due to subtle
differences in physiological signals. Our experiments show that HED-LM achieves
a mean macro F1-score of 69.13$\pm$10.71%, outperforming both random selection
(59.30$\pm$10.13%) and distance-only filtering (67.61$\pm$11.39%). These
represent relative improvements of 16.6% and 2.3%, respectively. The results
confirm that combining numerical similarity with contextual relevance improves
the robustness of few-shot prompting. Overall, HED-LM offers a practical
solution to improve performance in real-world sensor-based learning tasks and
shows potential for broader applications in healthcare monitoring, human
activity recognition, and industrial safety scenarios.

</details>


### [66] [How Is LLM Reasoning Distracted by Irrelevant Context? An Analysis Using a Controlled Benchmark](https://arxiv.org/abs/2505.18761)
*Minglai Yang,Ethan Huang,Liang Zhang,Mihai Surdeanu,William Wang,Liangming Pan*

Main category: cs.CL

TL;DR: 提出GSM-DC基准测试框架，通过符号推理图与干扰注入评估大语言模型抗干扰能力，发现模型对无关信息敏感，但针对性训练和树搜索策略可有效提升鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有评估缺乏对无关上下文干扰的系统控制，需构建可量化测试环境分析LLM推理稳定性。

Method: 创建含精确干扰的符号推理图，开展敏感度实验/干扰训练测试，并提出过程奖励模型引导的逐步树搜索算法。

Result: 模型呈现显著干扰敏感性（路径选择错误率↑35%），干扰训练使OOD准确率提升18.7%，树搜索策略提升抗干扰能力。

Conclusion: 系统化干扰评估揭示模型弱点，针对性训练与结构化推理算法能有效增强LLM在实际场景中的推理稳健性。

Abstract: We introduce Grade School Math with Distracting Context (GSM-DC), a synthetic
benchmark to evaluate Large Language Models' (LLMs) reasoning robustness
against systematically controlled irrelevant context (IC). GSM-DC constructs
symbolic reasoning graphs with precise distractor injections, enabling
rigorous, reproducible evaluation. Our experiments demonstrate that LLMs are
significantly sensitive to IC, affecting both reasoning path selection and
arithmetic accuracy. Additionally, training models with strong distractors
improves performance in both in-distribution and out-of-distribution scenarios.
We further propose a stepwise tree search guided by a process reward model,
which notably enhances robustness in out-of-distribution conditions.

</details>


### [67] [Towards an automatic method for generating topical vocabulary test forms for specific reading passages](https://arxiv.org/abs/2505.18762)
*Michael Flor,Zuowei Wang,Paul Deane,Tenaha O'Reilly*

Main category: cs.CL

TL;DR: 开发自动化系统K-tool生成主题词汇测试，通过检测文本主题并构建关联/非关联词汇表，评估中学生英语背景知识对特定STEM文本的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有自动化测量学生特定文本背景知识的工具匮乏，难以实时预测学生文本理解能力。STEM领域需要背景知识支撑，但缺乏可快速部署的评估系统。

Method: 1.自动检测文本主题
2.基于词汇与主题的关联度生成测试项
3.构建包含强关联词与特征相似但低关联词的知识表单
4.系统独立运作，无需依赖语料库

Result: 完成系统架构设计并通过初步输出评估（具体数据未披露），系统适用于单篇阅读材料

Conclusion: K-tool为初高中英语母语学习者提供有效的背景知识评估方案，通过词汇关联性测试预测文本理解可能性，填补教育技术领域实时评估工具的空白。

Abstract: Background knowledge is typically needed for successful comprehension of
topical and domain specific reading passages, such as in the STEM domain.
However, there are few automated measures of student knowledge that can be
readily deployed and scored in time to make predictions on whether a given
student will likely be able to understand a specific content area text. In this
paper, we present our effort in developing K-tool, an automated system for
generating topical vocabulary tests that measure students' background knowledge
related to a specific text. The system automatically detects the topic of a
given text and produces topical vocabulary items based on their relationship
with the topic. This information is used to automatically generate background
knowledge forms that contain words that are highly related to the topic and
words that share similar features but do not share high associations to the
topic. Prior research indicates that performance on such tasks can help
determine whether a student is likely to understand a particular text based on
their knowledge state. The described system is intended for use with middle and
high school student population of native speakers of English. It is designed to
handle single reading passages and is not dependent on any corpus or text
collection. In this paper, we describe the system architecture and present an
initial evaluation of the system outputs.

</details>


### [68] [Disentangling Knowledge Representations for Large Language Model Editing](https://arxiv.org/abs/2505.18774)
*Mengqi Zhang,Zisheng Zhou,Xiaotian Ye,Qiang Liu,Zhaochun Ren,Zhumin Chen,Pengjie Ren*

Main category: cs.CL

TL;DR: 提出基于知识表示解耦的DiKE方法，在保持大模型知识编辑能力的同时显著提升细粒度无关知识的保留效果，并通过新构建的FINE-KED基准验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法在更新目标知识时，会破坏同一主体下其他非相关的细粒度知识（如相同主语但不同谓宾关系的知识），因主体表示在向量空间中存在属性耦合现象。

Method: 1. KRD模块将主体表示解耦为目标相关和无关分量；2. DKE模块仅更新相关分量并显式保留无关分量；3. 基于矩阵理论推导出闭式秩一参数更新公式。

Result: 在多个大模型上的实验表明，DiKE显著提升细粒度无关知识保留能力（最高提升14.4%），同时保持与主流方法相当的编辑成功率（平均94.3%）。

Conclusion: 通过表示解耦机制、闭式参数更新和新评估基准，DiKE为解决知识编辑中的属性耦合问题提供了创新方案，证实了知识解耦对模型可编辑性的提升价值。

Abstract: Knowledge Editing has emerged as a promising solution for efficiently
updating embedded knowledge in large language models (LLMs). While existing
approaches demonstrate effectiveness in integrating new knowledge and
preserving the original capabilities of LLMs, they fail to maintain
fine-grained irrelevant knowledge facts that share the same subject as edited
knowledge but differ in relation and object. This challenge arises because
subject representations inherently encode multiple attributes, causing the
target and fine-grained irrelevant knowledge to become entangled in the
representation space, and thus vulnerable to unintended alterations during
editing. To address this, we propose DiKE, a novel approach that Disentangles
Knowledge representations for LLM Editing (DiKE). DiKE consists of two key
components: a Knowledge Representation Disentanglement (KRD) module that
decomposes the subject representation into target-knowledgerelated and
-unrelated components, and a Disentanglement-based Knowledge Edit (DKE) module
that updates only the target-related component while explicitly preserving the
unrelated one. We further derive a closed-form, rank-one parameter update based
on matrix theory to enable efficient and minimally invasive edits. To
rigorously evaluate fine-grained irrelevant knowledge preservation, we
construct FINE-KED, a new benchmark comprising fine-grained irrelevant
knowledge at different levels of relational similarity to the edited knowledge.
Extensive experiments across multiple LLMs demonstrate that DiKE substantially
improves fine-grained irrelevant knowledge preservation while maintaining
competitive general editing performance.

</details>


### [69] [A generalised editor calculus (Short Paper)](https://arxiv.org/abs/2505.18778)
*Benjamin Bennetzen,Peter Buus Steffensen,Hans Hüttel,Nikolaj Rossander Kristensen,Andreas Tor Mortensen*

Main category: cs.CL

TL;DR: 提出了一种可泛化的语法导向编辑器演算框架，支持基于任意语言的抽象语法实例化专用编辑器，保证语法正确性的同时允许不完整程序，并成功将其编码至扩展λ演算体系。


<details>
  <summary>Details</summary>
Motivation: 解决传统语法导向编辑器语言局限性问题，实现跨语言通用编辑框架，在保证语法无错误的前提下支持程序片段编辑。

Method: 1. 扩展语法导向编辑器演算的泛化能力 2. 建立抽象语法与编辑器行为的映射机制 3. 在含模式匹配/不动点的λ演算系统中进行形式化编码

Result: 构建出理论完备的编辑器生成框架，证明其语法容错特性，完成向扩展λ演算（含布尔、序对、模式匹配、不动点）的系统转化。

Conclusion: 该框架突破语言特定限制，为开发可靠且灵活的语法感知编辑器提供理论基础，特别适用于渐进式程序开发场景。

Abstract: In this paper, we present a generalization of a syntax-directed editor
calculus, which can be used to instantiate a specialized syntax-directed editor
for any language, given by some abstract syntax. The editor calculus guarantees
the absence of syntactical errors while allowing incomplete programs. The
generalized editor calculus is then encoded into a simply typed lambda
calculus, extended with pairs, booleans, pattern matching and fixed points

</details>


### [70] [ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models](https://arxiv.org/abs/2505.18799)
*Hao Chen,Haoze Li,Zhiqing Xiao,Lirong Gao,Qi Zhang,Xiaomeng Hu,Ningtao Wang,Xing Fu,Junbo Zhao*

Main category: cs.CL

TL;DR: 提出ALPS注意力定位与剪枝策略，通过仅微调10%注意力参数实现性能提升2%，且具备跨数据集迁移能力


<details>
  <summary>Details</summary>
Motivation: 现有LLM对齐方法存在数据依赖性强、训练成本高的问题，需要提升模型对齐效率并减少知识遗忘

Method: 定位任务敏感的注意力头，通过剪枝策略限制训练更新仅在关键头部进行

Result: 激活10%参数实现2%性能提升，任务相关头部可跨数据集迁移并缓解知识遗忘

Conclusion: ALPS为LLM高效对齐提供新范式，显著降低对齐成本同时提升模型适应性

Abstract: Aligning general-purpose large language models (LLMs) to downstream tasks
often incurs significant costs, including constructing task-specific
instruction pairs and extensive training adjustments. Prior research has
explored various avenues to enhance alignment efficiency, primarily through
minimal-data training or data-driven activations to identify key attention
heads. However, these approaches inherently introduce data dependency, which
hinders generalization and reusability. To address this issue and enhance model
alignment efficiency, we propose the \textit{\textbf{A}ttention
\textbf{L}ocalization and \textbf{P}runing \textbf{S}trategy (\textbf{ALPS})},
an efficient algorithm that localizes the most task-sensitive attention heads
and prunes by restricting attention training updates to these heads, thereby
reducing alignment costs. Experimental results demonstrate that our method
activates only \textbf{10\%} of attention parameters during fine-tuning while
achieving a \textbf{2\%} performance improvement over baselines on three tasks.
Moreover, the identified task-specific heads are transferable across datasets
and mitigate knowledge forgetting. Our work and findings provide a novel
perspective on efficient LLM alignment.

</details>


### [71] [Don't Look Only Once: Towards Multimodal Interactive Reasoning with Selective Visual Revisitation](https://arxiv.org/abs/2505.18842)
*Jiwan Chung,Junhyeok Kim,Siyeol Kim,Jaeyoung Lee,Min Soo Kim,Youngjae Yu*

Main category: cs.CL

TL;DR: v1通过动态视觉访问机制增强多模态大语言模型，在数学推理任务中显著提升细粒度视觉参考能力


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs单次处理视觉输入后仅依赖内部记忆推理，难以应对需要持续视觉参照的复杂多模态任务

Method: 提出点选复制机制，构建30万规模的v1g训练数据集，通过最小架构修改实现动态视觉标记检索

Result: 在MathVista等三大数学推理基准测试中持续超越基线模型，多步推理任务提升显著

Conclusion: 动态视觉访问机制为增强多模态推理提供了有效路径，代码模型开源将推动该领域发展

Abstract: We present v1, a lightweight extension to Multimodal Large Language Models
(MLLMs) that enables selective visual revisitation during inference. While
current MLLMs typically consume visual input only once and reason purely over
internal memory, v1 introduces a simple point-and-copy mechanism that allows
the model to dynamically retrieve relevant image regions throughout the
reasoning process. This mechanism augments existing architectures with minimal
modifications, enabling contextual access to visual tokens based on the model's
evolving hypotheses. To train this capability, we construct v1g, a dataset of
300K multimodal reasoning traces with interleaved visual grounding annotations.
Experiments on three multimodal mathematical reasoning benchmarks -- MathVista,
MathVision, and MathVerse -- demonstrate that v1 consistently improves
performance over comparable baselines, particularly on tasks requiring
fine-grained visual reference and multi-step reasoning. Our results suggest
that dynamic visual access is a promising direction for enhancing grounded
multimodal reasoning. Code, models, and data will be released to support future
research.

</details>


### [72] [Multi-Party Conversational Agents: A Survey](https://arxiv.org/abs/2505.18845)
*Sagar Sapkota,Mohammad Saqib Hasan,Mubarak Shah,Santu Karmaker*

Main category: cs.CL

TL;DR: 系统综述多方对话代理(MPCAs)的建模挑战与技术进展，强调心理理论和多模态理解的重要性


<details>
  <summary>Details</summary>
Motivation: 解决MPCAs在多方对话场景中需同时解析语义和社交动态的双重挑战，突破传统双人对话代理的局限性

Method: 系统性分析经典机器学习、LLM及多模态方法，构建三维评估框架（心理建模-语义理解-行为预测）

Result: 验证心理理论(ToM)是实现智能MPCAs的核心要素，发现多模态理解方向存在重大研究缺口

Conclusion: 建议未来研究聚焦心理建模算法优化和多模态数据融合，推动更智能的MPCAs系统发展

Abstract: Multi-party Conversational Agents (MPCAs) are systems designed to engage in
dialogue with more than two participants simultaneously. Unlike traditional
two-party agents, designing MPCAs faces additional challenges due to the need
to interpret both utterance semantics and social dynamics. This survey explores
recent progress in MPCAs by addressing three key questions: 1) Can agents model
each participants' mental states? (State of Mind Modeling); 2) Can they
properly understand the dialogue content? (Semantic Understanding); and 3) Can
they reason about and predict future conversation flow? (Agent Action
Modeling). We review methods ranging from classical machine learning to Large
Language Models (LLMs) and multi-modal systems. Our analysis underscores Theory
of Mind (ToM) as essential for building intelligent MPCAs and highlights
multi-modal understanding as a promising yet underexplored direction. Finally,
this survey offers guidance to future researchers on developing more capable
MPCAs.

</details>


### [73] [Smoothie: Smoothing Diffusion on Token Embeddings for Text Generation](https://arxiv.org/abs/2505.18853)
*Alexander Shabalin,Viacheslav Meshchaninov,Dmitry Vetrov*

Main category: cs.CL

TL;DR: 提出Smoothie扩散模型，通过语义相似性渐进平滑词嵌入，结合连续空间和离散空间的优势提升文本生成质量


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在文本生成中存在两难：连续潜在空间保留语义但难以解码，离散空间保持离散性但忽略语义关联。需要兼顾语义保留与解码可靠性

Method: Smoothing Diffusion on Token Embeddings (Smoothie)：基于语义相似性逐步平滑词嵌入，实现渐进信息消除同时保持自然解码过程

Result: 在多个seq2seq任务中生成质量超越现有扩散模型，消融实验证明提出的扩散空间优于标准嵌入空间和类别单纯形

Conclusion: Smoothie成功融合连续与离散方法优势，通过语义导向的平滑机制有效提升文本生成性能，为扩散模型在NLP领域提供新方向

Abstract: Diffusion models have achieved state-of-the-art performance in generating
images, audio, and video, but their adaptation to text remains challenging due
to its discrete nature. Prior approaches either apply Gaussian diffusion in
continuous latent spaces, which inherits semantic structure but struggles with
token decoding, or operate in categorical simplex space, which respect
discreteness but disregard semantic relation between tokens. In this paper, we
propose Smoothing Diffusion on Token Embeddings (Smoothie), a novel diffusion
method that combines the strengths of both approaches by progressively
smoothing token embeddings based on semantic similarity. This technique enables
gradual information removal while maintaining a natural decoding process.
Experimental results on several sequence-to-sequence generation tasks
demonstrate that Smoothie outperforms existing diffusion-based models in
generation quality. Furthermore, ablation studies show that our proposed
diffusion space yields better performance than both the standard embedding
space and the categorical simplex. Our code is available at
https://github.com/ashaba1in/smoothie.

</details>


### [74] [Writing Like the Best: Exemplar-Based Expository Text Generation](https://arxiv.org/abs/2505.18859)
*Yuxiang Liu,Kevin Chen-Chuan Chang*

Main category: cs.CL

TL;DR: 提出RePA框架用于范例式说明文本生成任务，通过分段自适应模仿机制和记忆结构提升生成质量，实验验证有效性


<details>
  <summary>Details</summary>
Motivation: 现有方法存在过度依赖范例数据、主题适应性差、长文本连贯性不足三大缺陷，需开发新框架突破限制

Method: 基于LLMs的循环式计划-调整机制，采用双记忆结构增强输入输出连贯性，实现分段自适应模仿

Result: 在三个多领域数据集上超越基线模型，生成文本的事实性、一致性、相关性均显著提升

Conclusion: RePA框架通过自适应模仿机制有效解决了范例利用率低和文本连贯性问题，配套评估指标为任务发展提供新方向

Abstract: We introduce the Exemplar-Based Expository Text Generation task, aiming to
generate an expository text on a new topic using an exemplar on a similar
topic. Current methods fall short due to their reliance on extensive exemplar
data, difficulty in adapting topic-specific content, and issues with long-text
coherence. To address these challenges, we propose the concept of Adaptive
Imitation and present a novel Recurrent Plan-then-Adapt (RePA) framework. RePA
leverages large language models (LLMs) for effective adaptive imitation through
a fine-grained plan-then-adapt process. RePA also enables recurrent
segment-by-segment imitation, supported by two memory structures that enhance
input clarity and output coherence. We also develop task-specific evaluation
metrics--imitativeness, adaptiveness, and adaptive-imitativeness--using LLMs as
evaluators. Experimental results across our collected three diverse datasets
demonstrate that RePA surpasses existing baselines in producing factual,
consistent, and relevant texts for this task.

</details>


### [75] [Audio Jailbreak Attacks: Exposing Vulnerabilities in SpeechGPT in a White-Box Framework](https://arxiv.org/abs/2505.18864)
*Binhao Ma,Hanqing Guo,Zhengping Jay Luo,Rui Duan*

Main category: cs.CL

TL;DR: 针对语音多模态大模型的对抗攻击方法，通过语音标记化生成实现89%攻击成功率


<details>
  <summary>Details</summary>
Motivation: 语音交互系统存在独特安全风险（如语音时间特性/发音变异性/语音转文本漏洞），但当前研究主要聚焦文本攻击而缺乏语音攻防机制探索

Method: 基于白盒场景的语音标记级攻击，利用模型语音分词机制生成对抗性标记序列并合成音频绕过安全防护

Result: 在SpeechGPT上实现89%攻击成功率，显著优于现有语音越狱方法（多场景限制任务验证）

Conclusion: 揭示了语音多模态系统的安全脆弱性，为开发更鲁棒的下一代MLLMs提供重要参考

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have
significantly enhanced the naturalness and flexibility of human computer
interaction by enabling seamless understanding across text, vision, and audio
modalities. Among these, voice enabled models such as SpeechGPT have
demonstrated considerable improvements in usability, offering expressive, and
emotionally responsive interactions that foster deeper connections in real
world communication scenarios. However, the use of voice introduces new
security risks, as attackers can exploit the unique characteristics of spoken
language, such as timing, pronunciation variability, and speech to text
translation, to craft inputs that bypass defenses in ways not seen in
text-based systems. Despite substantial research on text based jailbreaks, the
voice modality remains largely underexplored in terms of both attack strategies
and defense mechanisms. In this work, we present an adversarial attack
targeting the speech input of aligned MLLMs in a white box scenario.
Specifically, we introduce a novel token level attack that leverages access to
the model's speech tokenization to generate adversarial token sequences. These
sequences are then synthesized into audio prompts, which effectively bypass
alignment safeguards and to induce prohibited outputs. Evaluated on SpeechGPT,
our approach achieves up to 89 percent attack success rate across multiple
restricted tasks, significantly outperforming existing voice based jailbreak
methods. Our findings shed light on the vulnerabilities of voice-enabled
multimodal systems and to help guide the development of more robust
next-generation MLLMs.

</details>


### [76] [Sci-LoRA: Mixture of Scientific LoRAs for Cross-Domain Lay Paraphrasing](https://arxiv.org/abs/2505.18867)
*Ming Cheng,Jiaying Gong,Hoda Eldardiry*

Main category: cs.CL

TL;DR: Sci-LoRA提出了一种动态混合LoRA模块的方法，无需领域标签即可实现跨学科科学文本的通俗化改写，在12个领域和5个数据集上显著优于现有大模型。


<details>
  <summary>Details</summary>
Motivation: 现有科学文本通俗化改写模型主要局限于单一领域（如生物医学），而跨学科研究需要模型具备多领域适应能力。为解决这一限制，研究旨在开发无需显式领域标注即可动态融合多领域知识的改写方法。

Method: 1. 采用混合LoRA架构：为不同科学领域微调独立LoRA模块
2. 动态权重机制：根据输入文本自动生成各领域LoRA的融合权重
3. 双层次融合策略：在数据层面通过多领域预训练，在模型层面通过门控网络实现动态参数组合

Result: 在12个科学领域的5个公开数据集上，Sci-LoRA在自动评估指标（如BLEU、ROUGE）和人工评估中均超越GPT-4等基线模型，尤其在跨领域场景下展现出更强的适应性和泛化能力。

Conclusion: 该研究通过动态参数融合机制成功解决了多领域科学文本改写难题，为跨学科知识传播提供了有效的技术方案，未来可扩展至更多科学领域和低资源语言场景。

Abstract: Lay paraphrasing aims to make scientific information accessible to audiences
without technical backgrounds. However, most existing studies focus on a single
domain, such as biomedicine. With the rise of interdisciplinary research, it is
increasingly necessary to comprehend knowledge spanning multiple technical
fields. To address this, we propose Sci-LoRA, a model that leverages a mixture
of LoRAs fine-tuned on multiple scientific domains. In particular, Sci-LoRA
dynamically generates and applies weights for each LoRA, enabling it to adjust
the impact of different domains based on the input text, without requiring
explicit domain labels. To balance domain-specific knowledge and generalization
across various domains, Sci-LoRA integrates information at both the data and
model levels. This dynamic fusion enhances the adaptability and performance
across various domains. Experimental results across twelve domains on five
public datasets show that Sci-LoRA significantly outperforms state-of-the-art
large language models and demonstrates flexible generalization and adaptability
in cross-domain lay paraphrasing.

</details>


### [77] [CRMArena-Pro: Holistic Assessment of LLM Agents Across Diverse Business Scenarios and Interactions](https://arxiv.org/abs/2505.18878)
*Kung-Hsiang Huang,Akshara Prabhakar,Onkar Thorat,Divyansh Agarwal,Prafulla Kumar Choubey,Yixin Mao,Silvio Savarese,Caiming Xiong,Chien-Sheng Wu*

Main category: cs.CL

TL;DR: CRMArena-Pro推出新基准测试，揭示现有LLM智能体在多轮交互与保密性方面的显著能力差距（单轮成功率58% vs 多轮35%），暴露企业级应用需求鸿沟


<details>
  <summary>Details</summary>
Motivation: 现有商业AI评估基准存在环境真实性不足、行业覆盖单一、交互仿真度低等问题，无法有效衡量智能体真实业务表现

Method: 在CRMArena基础上扩展19个专家验证任务，覆盖B2B/B2C销售/服务/报价全流程，创新引入多轮角色扮演机制与保密意识评估体系

Result: 头部LLM智能体单轮任务成功率仅58%（多轮骤降至35%），工作流执行达83%但商业技能薄弱，保密意识原生接近零（需特定提示且影响任务表现）

Conclusion: 当前LLM需突破多轮推理、保密合规与多场景适应能力瓶颈，才能真正满足企业级复杂商业场景需求

Abstract: While AI agents hold transformative potential in business, effective
performance benchmarking is hindered by the scarcity of public, realistic
business data on widely used platforms. Existing benchmarks often lack fidelity
in their environments, data, and agent-user interactions, with limited coverage
of diverse business scenarios and industries. To address these gaps, we
introduce CRMArena-Pro, a novel benchmark for holistic, realistic assessment of
LLM agents in diverse professional settings. CRMArena-Pro expands on CRMArena
with nineteen expert-validated tasks across sales, service, and 'configure,
price, and quote' processes, for both Business-to-Business and
Business-to-Customer scenarios. It distinctively incorporates multi-turn
interactions guided by diverse personas and robust confidentiality awareness
assessments. Experiments reveal leading LLM agents achieve only around 58%
single-turn success on CRMArena-Pro, with performance dropping significantly to
approximately 35% in multi-turn settings. While Workflow Execution proves more
tractable for top agents (over 83% single-turn success), other evaluated
business skills present greater challenges. Furthermore, agents exhibit
near-zero inherent confidentiality awareness; though targeted prompting can
improve this, it often compromises task performance. These findings highlight a
substantial gap between current LLM capabilities and enterprise demands,
underscoring the need for advancements in multi-turn reasoning, confidentiality
adherence, and versatile skill acquisition.

</details>


### [78] [StandUp4AI: A New Multilingual Dataset for Humor Detection in Stand-up Comedy Videos](https://arxiv.org/abs/2505.18903)
*Valentin Barriere,Nahuel Gomez,Leo Hemamou,Sofia Callejas,Brian Ravenet*

Main category: cs.CL

TL;DR: 研究者构建了目前最大规模的多语言喜剧表演数据集（330+小时/7种语言），将幽默检测任务重构为词级序列标注问题，并提出基于语音识别错误的自动笑声增强检测方法。


<details>
  <summary>Details</summary>
Motivation: 现有幽默检测模型存在数据集规模有限、任务设计过于简化（二分类）的问题，无法捕捉自然对话中连续幽默标记机制。本研究旨在通过多模态数据集和细粒度任务设计改进模型性能。

Method: 1. 收集七种语言的单口喜剧表演构建多模态数据集（自动标注笑声+人工验证）
2. 将幽默检测重构为词级序列标注任务
3. 开发基于语音识别错误分析的自动笑声检测增强方法

Result: 创建了当前最大最丰富的幽默研究数据集（330+小时/7语言），实现了细粒度的幽默标记分析框架，通过语音错误特征提升了笑声检测的自动化水平。

Conclusion: 本研究通过数据规模扩展和任务范式创新，为对话场景的连续幽默理解提供了新解决方案，其多语言特性支持跨文化幽默机制分析。

Abstract: Aiming towards improving current computational models of humor detection, we
propose a new multimodal dataset of stand-up comedies, in seven languages:
English, French, Spanish, Italian, Portuguese, Hungarian and Czech. Our dataset
of more than 330 hours, is at the time of writing the biggest available for
this type of task, and the most diverse. The whole dataset is automatically
annotated in laughter (from the audience), and the subpart left for model
validation is manually annotated. Contrary to contemporary approaches, we do
not frame the task of humor detection as a binary sequence classification, but
as word-level sequence labeling, in order to take into account all the context
of the sequence and to capture the continuous joke tagging mechanism typically
occurring in natural conversations. As par with unimodal baselines results, we
propose a method for e propose a method to enhance the automatic laughter
detection based on Audio Speech Recognition errors. Our code and data are
available online: https://tinyurl.com/EMNLPHumourStandUpPublic

</details>


### [79] [Building a Functional Machine Translation Corpus for Kpelle](https://arxiv.org/abs/2505.18905)
*Kweku Andoh Yamoah,Jackson Weako,Emmanuel J. Dorley*

Main category: cs.CL

TL;DR: 创建首个公开的英语-Kpelle机器翻译数据集（2000+句对），通过微调NLLB模型实现BLEU 30的翻译效果，验证数据增强有效性，并提出未来数据集扩展路线


<details>
  <summary>Details</summary>
Motivation: 解决Kpelle作为低资源语言缺乏公开可用数据集的问题，促进该语言的机器翻译及其他NLP技术发展，探索数据增强在低资源场景下的应用价值

Method: 从日常交流/宗教文本/教育材料三个领域收集语料构建双语数据集，采用Meta的NLLB模型进行微调实验，对比分析不同数据增强策略的效果

Result: Kpelle-English方向取得BLEU 30的最佳成绩，与NLLB-200在其他非洲语言的基准表现一致，验证了数据增强策略的有效性和Kpelle的技术可行性

Conclusion: 提出通过正字法统一、社区验证和跨学科合作的三步走计划，推动Kpelle及其他Mande低资源语言的包容性语言技术发展

Abstract: In this paper, we introduce the first publicly available English-Kpelle
dataset for machine translation, comprising over 2000 sentence pairs drawn from
everyday communication, religious texts, and educational materials. By
fine-tuning Meta's No Language Left Behind(NLLB) model on two versions of the
dataset, we achieved BLEU scores of up to 30 in the Kpelle-to-English
direction, demonstrating the benefits of data augmentation. Our findings align
with NLLB-200 benchmarks on other African languages, underscoring Kpelle's
potential for competitive performance despite its low-resource status. Beyond
machine translation, this dataset enables broader NLP tasks, including speech
recognition and language modelling. We conclude with a roadmap for future
dataset expansion, emphasizing orthographic consistency, community-driven
validation, and interdisciplinary collaboration to advance inclusive language
technology development for Kpelle and other low-resourced Mande languages.

</details>


### [80] [Federated Retrieval-Augmented Generation: A Systematic Mapping Study](https://arxiv.org/abs/2505.18906)
*Abhijit Chakraborty,Chahana Dahal,Vivek Gupta*

Main category: cs.CL

TL;DR: 首篇系统综述联邦检索增强生成（Federated RAG）技术，融合联邦学习与检索增强生成，为隐私敏感领域的NLP应用提供安全框架。


<details>
  <summary>Details</summary>
Motivation: 解决医疗、金融等隐私敏感领域的大语言模型部署需求，在保证数据隐私前提下提升语言模型的事实准确性。

Method: 采用Kitchenham证据驱动方法，对2020-2025年间文献进行系统映射研究，构建研究焦点分类体系并分析架构模式、时序趋势。

Result: 揭示该领域快速演进态势，识别出隐私保护检索、跨客户端异构性、评估局限等核心挑战，总结出可复用的系统设计模式。

Conclusion: 联邦RAG为隐私敏感NLP开辟新路径，研究结果为后续联邦系统与RAG技术的融合创新奠定理论基础，亟待解决隐私-性能平衡难题。

Abstract: Federated Retrieval-Augmented Generation (Federated RAG) combines Federated
Learning (FL), which enables distributed model training without exposing raw
data, with Retrieval-Augmented Generation (RAG), which improves the factual
accuracy of language models by grounding outputs in external knowledge. As
large language models are increasingly deployed in privacy-sensitive domains
such as healthcare, finance, and personalized assistance, Federated RAG offers
a promising framework for secure, knowledge-intensive natural language
processing (NLP). To the best of our knowledge, this paper presents the first
systematic mapping study of Federated RAG, covering literature published
between 2020 and 2025. Following Kitchenham's guidelines for evidence-based
software engineering, we develop a structured classification of research
focuses, contribution types, and application domains. We analyze architectural
patterns, temporal trends, and key challenges, including privacy-preserving
retrieval, cross-client heterogeneity, and evaluation limitations. Our findings
synthesize a rapidly evolving body of research, identify recurring design
patterns, and surface open questions, providing a foundation for future work at
the intersection of RAG and federated systems.

</details>


### [81] [SCRum-9: Multilingual Stance Classification over Rumours on Social Media](https://arxiv.org/abs/2505.18916)
*Yue Li,Jake Vasilakes,Zhixue Zhao,Carolina Scarton*

Main category: cs.CL

TL;DR: 多语言谣言立场分类数据集SCRum-9发布，覆盖9种语言并关联2100+事实核查声明，实验显示对LLM和微调模型均具挑战性


<details>
  <summary>Details</summary>
Motivation: 现有立场分类数据集存在语言覆盖有限、缺乏事实核查关联及复杂注释的缺陷，需建立更全面的多语言基准数据集

Method: 通过至少3名母语标注者/语言的众包标注（总耗时405小时/成本8150美元），构建包含7516推文对的复杂注释数据集

Result: Deepseek等先进模型在SCRum-9上表现欠佳，验证了该数据集作为挑战性基准的价值

Conclusion: SCRum-9填补了多语言谣言立场检测领域的数据缺口，为模型鲁棒性研究提供了新方向

Abstract: We introduce SCRum-9, a multilingual dataset for Rumour Stance
Classification, containing 7,516 tweet-reply pairs from X. SCRum-9 goes beyond
existing stance classification datasets by covering more languages (9), linking
examples to more fact-checked claims (2.1k), and including complex annotations
from multiple annotators to account for intra- and inter-annotator variability.
Annotations were made by at least three native speakers per language, totalling
around 405 hours of annotation and 8,150 dollars in compensation. Experiments
on SCRum-9 show that it is a challenging benchmark for both state-of-the-art
LLMs (e.g. Deepseek) as well as fine-tuned pre-trained models, motivating
future work in this area.

</details>


### [82] [Benchmarking Large Language Models for Cyberbullying Detection in Real-World YouTube Comments](https://arxiv.org/abs/2505.18927)
*Amel Muminovic*

Main category: cs.CL

TL;DR: 研究比较了GPT-4.1、Gemini 1.5 Pro和Claude 3 Opus在有害评论检测中的表现，GPT-4.1综合最优（F1=0.863），强调需组合模型并改进多语言处理


<details>
  <summary>Details</summary>
Motivation: 评估主流大语言模型对多语言有害评论的检测能力，解决现有内容审核系统在隐晦骚扰和低资源语言处理上的不足

Method: 使用5,080条YouTube评论数据集（含英/阿/印尼语），通过双人独立标注（κ=0.83），采用统一prompt和确定性参数评估模型性能

Result: GPT-4.1综合最佳（F1=0.863/Precision=0.887/Recall=0.841），Gemini召回最高（0.875）但准确率低（0.767），Claude精准最高（0.920）但漏检多（Recall=0.720）

Conclusion: 需构建组合模型管道，整合上下文理解能力，并针对小语种和隐晦骚扰进行微调。公开数据集促进内容审核技术发展

Abstract: As online platforms grow, comment sections increasingly host harassment that
undermines user experience and well-being. This study benchmarks three leading
large language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic
Claude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse
threads in gaming, lifestyle, food vlog, and music channels. The dataset
comprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and
Indonesian, annotated independently by two reviewers with substantial agreement
(Cohen's kappa = 0.83). Using a unified prompt and deterministic settings,
GPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision
of 0.887, and recall of 0.841. Gemini flagged the highest share of harmful
posts (recall = 0.875) but its precision fell to 0.767 due to frequent false
positives. Claude delivered the highest precision at 0.920 and the lowest
false-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative
analysis showed that all three models struggle with sarcasm, coded insults, and
mixed-language slang. These results underscore the need for moderation
pipelines that combine complementary models, incorporate conversational
context, and fine-tune for under-represented languages and implicit abuse. A
de-identified version of the dataset and full prompts is publicly released to
promote reproducibility and further progress in automated content moderation.

</details>


### [83] [MetaMind: Modeling Human Social Thoughts with Metacognitive Multi-Agent Systems](https://arxiv.org/abs/2505.18943)
*Xuanming Zhang,Yuxuan Chen,Min-Hsuan Yeh,Yixuan Li*

Main category: cs.CL

TL;DR: MetaMind框架通过元认知启发的多智能体协作，显著提升LLMs在心理理论任务中的社交推理能力，首次实现人类水平表现。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在理解人类社交意图、情感等心理状态时的模糊性和语境敏感性不足的问题，通过模拟人类元认知过程构建社交智能框架。

Method: 三阶段协作架构：1.心理理论智能体生成心智假设；2.领域智能体基于文化规范修正假设；3.响应智能体生成并验证语境适配的回应。

Result: 在三大基准测试中达到SOTA，真实社交场景提升35.7%，ToM推理提升6.2%，首次实现关键心理理论任务的人类水平表现。

Conclusion: 该框架通过组件协同作用平衡语境合理性、社会适切性与用户适配性，推动AI系统向类人社交智能发展，适用于共情对话和跨文化交互场景。

Abstract: Human social interactions depend on the ability to infer others' unspoken
intentions, emotions, and beliefs-a cognitive skill grounded in the
psychological concept of Theory of Mind (ToM). While large language models
(LLMs) excel in semantic understanding tasks, they struggle with the ambiguity
and contextual nuance inherent in human communication. To bridge this gap, we
introduce MetaMind, a multi-agent framework inspired by psychological theories
of metacognition, designed to emulate human-like social reasoning. MetaMind
decomposes social understanding into three collaborative stages: (1) a
Theory-of-Mind Agent generates hypotheses user mental states (e.g., intent,
emotion), (2) a Domain Agent refines these hypotheses using cultural norms and
ethical constraints, and (3) a Response Agent generates contextually
appropriate responses while validating alignment with inferred intent. Our
framework achieves state-of-the-art performance across three challenging
benchmarks, with 35.7% improvement in real-world social scenarios and 6.2% gain
in ToM reasoning. Notably, it enables LLMs to match human-level performance on
key ToM tasks for the first time. Ablation studies confirm the necessity of all
components, which showcase the framework's ability to balance contextual
plausibility, social appropriateness, and user adaptation. This work advances
AI systems toward human-like social intelligence, with applications in
empathetic dialogue and culturally sensitive interactions. Code is available at
https://github.com/XMZhangAI/MetaMind.

</details>


### [84] [The Price of Format: Diversity Collapse in LLMs](https://arxiv.org/abs/2505.18949)
*Longfei Yun,Chenyang An,Zilong Wang,Letian Peng,Jingbo Shang*

Main category: cs.CL

TL;DR: 研究发现指令调优的LLMs使用的结构化模板会导致输出多样性崩溃，限制创造性，需改进提示设计以平衡格式一致性与多样性。


<details>
  <summary>Details</summary>
Motivation: 传统结构化模板（如角色标记）虽保证格式统一，但导致开放性任务中模型输出语义趋同，抑制创造性和多样性（即多样性崩溃）。

Method: 通过系统评估高温度采样下的多样性崩溃现象，微调不同结构化提示的模型，并分析下游任务表现、对齐行为和输出多样性。

Result: 1. 格式一致性对结构敏感任务（GSM8K/IFEval）关键，但对知识任务（MMLU/WebQuestions）影响小；2. 结构标记显著压缩输出空间，极简模板可提升多样性。

Conclusion: 当前提示方法虽利于对齐，但可能无意间压制输出多样性，需开发兼顾多样性的提示设计和指令调优方法。

Abstract: Instruction-tuned large language models (LLMs) employ structured templates,
such as role markers and special tokens, to enforce format consistency during
inference. However, we identify a critical limitation of such formatting: it
induces a phenomenon we term diversity collapse, where the model generates
semantically similar outputs for open-ended inputs, undermining creativity and
variability. We systematically evaluate this effect across tasks like story
completion and free-form generation, finding that (1) diversity collapse
persists even under high-temperature sampling, and (2) structural tokens in
templates significantly constrain the model's output space. To contextualize
these findings, we fine-tune the same model using a range of structured prompts
and then evaluate them across three axes: downstream task performance,
alignment behavior, and output diversity. Our analysis shows that format
consistency between fine-tuning and inference is crucial for
structure-sensitive tasks (e.g., GSM8K, IFEval), but has marginal influence on
knowledge-heavy tasks (e.g., MMLU, WebQuestions). In contrast, output diversity
is primarily governed by the presence or absence of structural tokens, with
minimal formatting yielding the most diverse outputs. These findings reveal
that current prompting conventions, while beneficial for alignment, may
inadvertently suppress output diversity, underscoring the need for
diversity-aware prompt design and instruction tuning.

</details>


### [85] [BnMMLU: Measuring Massive Multitask Language Understanding in Bengali](https://arxiv.org/abs/2505.18951)
*Saman Sarker Joy*

Main category: cs.CL

TL;DR: 论文提出了首个孟加拉语多任务语言理解评测基准BnMMLU，包含23个领域共13.8万题，填补低资源语言评估空白。


<details>
  <summary>Details</summary>
Motivation: 现有MMLU基准主要面向英语等高资源语言，孟加拉语等低资源语言评估体系缺失，阻碍语言模型在该领域的发展。

Method: 构建覆盖科学、数学等23个领域的选择题数据集（138,949题），标注事实性知识/程序应用/推理三类认知标签，测试多个闭源/开源大模型。

Result: 模型表现存在显著差距（最优模型准确率仅59.1%），揭示需要针对孟加拉语改进预训练和微调策略。

Conclusion: BnMMLU暴露孟加拉语NLP关键挑战，数据集的开放将推动低资源语言建模研究，促进语言技术包容性发展。

Abstract: The Massive Multitask Language Understanding (MMLU) benchmark has been widely
used to evaluate language models across various domains. However, existing MMLU
datasets primarily focus on high-resource languages such as English, which
leaves low-resource languages like Bengali underrepresented. In this paper, we
introduce BnMMLU, a benchmark to evaluate the multitask language understanding
capabilities of Bengali in language models. The dataset spans 23 domains,
including science, humanities, mathematics and general knowledge and is
structured in a multiple-choice format to assess factual knowledge,
application-based problem-solving and reasoning abilities of language models.
It consists of 138,949 question-option pairs. We benchmark several proprietary
and open-source large language models (LLMs) on the BnMMLU test set.
Additionally, we annotate the test set with three cognitive categories-factual
knowledge, procedural application and reasoning-to gain deeper insights into
model strengths and weaknesses across various cognitive tasks. The results
reveal significant performance gaps, highlighting the need for improved
pre-training and fine-tuning strategies tailored to Bengali data. We release
the dataset and benchmark results to facilitate further research in this area.

</details>


### [86] [Evaluating AI for Finance: Is AI Credible at Assessing Investment Risk?](https://arxiv.org/abs/2505.18953)
*Divij Chawla,Ashita Bhutada,Do Duc Anh,Abhinav Raghunathan,Vinod SP,Cathy Guo,Dar Win Liew,Prannaya Gupta,Rishabh Bhardwaj,Rajat Bhardwaj,Soujanya Poria*

Main category: cs.CL

TL;DR: AI模型在投资风险评估中存在显著地域/性别敏感性差异，需建立标准化评估体系


<details>
  <summary>Details</summary>
Motivation: 评估主流AI模型在受监管金融场景中的风险评估可靠性，揭示算法偏见风险

Method: 使用1720个含16个风险特征的用户档案，横跨10国两性，对比6类模型（含GPT-4/Claude等）的评分分布

Result: GPT-4o对尼日利亚/印尼用户评分偏高，LLaMA与DeepSeek呈现性别评估对立倾向，所有模型跨区域表现不稳定

Conclusion: 金融领域AI系统需建立包含地域/性别维度的评估标准，防范算法偏见与部署风险

Abstract: We evaluate the credibility of leading AI models in assessing investment risk
appetite. Our analysis spans proprietary (GPT-4, Claude 3.7, Gemini 1.5) and
open-weight models (LLaMA 3.1/3.3, DeepSeek-V3, Mistral-small), using 1,720
user profiles constructed with 16 risk-relevant features across 10 countries
and both genders. We observe significant variance across models in score
distributions and demographic sensitivity. For example, GPT-4o assigns higher
risk scores to Nigerian and Indonesian profiles, while LLaMA and DeepSeek show
opposite gender tendencies in risk classification. While some models (e.g.,
GPT-4o, LLaMA 3.1) align closely with expected scores in low- and mid-risk
ranges, none maintain consistent performance across regions and demographics.
Our findings highlight the need for rigorous, standardized evaluations of AI
systems in regulated financial contexts to prevent bias, opacity, and
inconsistency in real-world deployment.

</details>


### [87] [System-1.5 Reasoning: Traversal in Language and Latent Spaces with Dynamic Shortcuts](https://arxiv.org/abs/2505.18962)
*Xiaoqiang Wang,Suyuchen Wang,Yun Zhu,Bang Liu*

Main category: cs.CL

TL;DR: 提出System-1.5 Reasoning框架，通过潜在空间动态计算分配，在保证推理性能的同时实现20倍加速和92.31%的token生成缩减


<details>
  <summary>Details</summary>
Motivation: 传统思维链方法存在中间输出冗长低效问题，现有潜在空间推理方法未能区分关键步骤与辅助步骤，导致计算资源利用不足

Method: 引入垂直深度的模型深度捷径（动态退出非关键token）与水平解码的步骤捷径（重用隐藏状态），通过两阶段自蒸馏训练实现自适应推理路径

Result: 在GSM8K等推理任务中达到与传统CoT相当的性能，推理速度提升20倍以上，平均减少92.31%的token生成

Conclusion: System-1.5 Reasoning通过动态分配计算资源，在语言模型的系统1快速响应与系统2深度推理间建立效率平衡

Abstract: Chain-of-thought (CoT) reasoning enables large language models (LLMs) to move
beyond fast System-1 responses and engage in deliberative System-2 reasoning.
However, this comes at the cost of significant inefficiency due to verbose
intermediate output. Recent latent-space reasoning methods improve efficiency
by operating on hidden states without decoding into language, yet they treat
all steps uniformly, failing to distinguish critical deductions from auxiliary
steps and resulting in suboptimal use of computational resources. In this
paper, we propose System-1.5 Reasoning, an adaptive reasoning framework that
dynamically allocates computation across reasoning steps through shortcut paths
in latent space.Specifically, System-1.5 Reasoning introduces two types of
dynamic shortcuts. The model depth shortcut (DS) adaptively reasons along the
vertical depth by early exiting non-critical tokens through lightweight adapter
branches, while allowing critical tokens to continue through deeper Transformer
layers. The step shortcut (SS) reuses hidden states across the decoding steps
to skip trivial steps and reason horizontally in latent space. Training
System-1.5 Reasoning involves a two-stage self-distillation process: first
distilling natural language CoT into latent-space continuous thought, and then
distilling full-path System-2 latent reasoning into adaptive shortcut paths
(System-1.5 Reasoning).Experiments on reasoning tasks demonstrate the superior
performance of our method. For example, on GSM8K, System-1.5 Reasoning achieves
reasoning performance comparable to traditional CoT fine-tuning methods while
accelerating inference by over 20x and reducing token generation by 92.31% on
average.

</details>


### [88] [Learning to Explain: Prototype-Based Surrogate Models for LLM Classification](https://arxiv.org/abs/2505.18970)
*Bowen Wei,Ziwei Zhu*

Main category: cs.CL

TL;DR: 提出ProtoSurE框架，通过可解释的替代模型和句子级原型，为LLM提供忠实且易懂的解释方法，实验显示其优于现有方法并具备高数据效率


<details>
  <summary>Details</summary>
Motivation: 现有LLM解释方法存在忠实度不足与可解释性差的双重缺陷，难以满足实际应用中对模型透明度的需求

Method: 构建基于原型的替代模型架构，通过对齐目标LLM的决策逻辑，结合句子级原型构建人类可理解的概念体系

Result: 在多个LLM和数据集上的实验表明，ProtoSurE在解释质量指标上平均提升15.6%，且仅需500个训练样本即可达到稳定性能

Conclusion: ProtoSurE成功解决了LLM可解释性领域忠实度与可理解性的平衡问题，为实际部署可信AI系统提供了新的技术路径

Abstract: Large language models (LLMs) have demonstrated impressive performance on
natural language tasks, but their decision-making processes remain largely
opaque. Existing explanation methods either suffer from limited faithfulness to
the model's reasoning or produce explanations that humans find difficult to
understand. To address these challenges, we propose \textbf{ProtoSurE}, a novel
prototype-based surrogate framework that provides faithful and
human-understandable explanations for LLMs. ProtoSurE trains an
interpretable-by-design surrogate model that aligns with the target LLM while
utilizing sentence-level prototypes as human-understandable concepts. Extensive
experiments show that ProtoSurE consistently outperforms SOTA explanation
methods across diverse LLMs and datasets. Importantly, ProtoSurE demonstrates
strong data efficiency, requiring relatively few training examples to achieve
good performance, making it practical for real-world applications.

</details>


### [89] [Is Architectural Complexity Overrated? Competitive and Interpretable Knowledge Graph Completion with RelatE](https://arxiv.org/abs/2505.18971)
*Abhijit Chakraborty,Chahana Dahal,Ashutosh Balasubramaniam,Tejas Anvekar,Vivek Gupta*

Main category: cs.CL

TL;DR: 提出RelatE模型，通过相位-模分解实现高效知识图谱补全，在性能、效率和鲁棒性上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于复数嵌入或深度架构的知识图谱补全模型复杂度高且缺乏解释性，需开发兼具高效性、可解释性的替代方案。

Method: 采用实值相位-模分解技术，利用正弦相位对齐编码对称/反转/组合等关系模式，保持架构简洁性。

Result: YAGO3-10上MRR达0.521（Hit@10 0.680），训练时间减少24%，推理延迟降低31%，内存占用下降22%；结构扰动下MRR衰减比TransE减少61%。

Conclusion: RelatE在保持架构简单性的同时，实现了性能、效率和鲁棒性的综合提升，为知识图谱补全提供了可扩展的解决方案。

Abstract: We revisit the efficacy of simple, real-valued embedding models for knowledge
graph completion and introduce RelatE, an interpretable and modular method that
efficiently integrates dual representations for entities and relations. RelatE
employs a real-valued phase-modulus decomposition, leveraging sinusoidal phase
alignments to encode relational patterns such as symmetry, inversion, and
composition. In contrast to recent approaches based on complex-valued
embeddings or deep neural architectures, RelatE preserves architectural
simplicity while achieving competitive or superior performance on standard
benchmarks. Empirically, RelatE outperforms prior methods across several
datasets: on YAGO3-10, it achieves an MRR of 0.521 and Hit@10 of 0.680,
surpassing all baselines. Additionally, RelatE offers significant efficiency
gains, reducing training time by 24%, inference latency by 31%, and peak GPU
memory usage by 22% compared to RotatE. Perturbation studies demonstrate
improved robustness, with MRR degradation reduced by up to 61% relative to
TransE and by up to 19% compared to RotatE under structural edits such as edge
removals and relation swaps. Formal analysis further establishes the model's
full expressiveness and its capacity to represent essential first-order logical
inference patterns. These results position RelatE as a scalable and
interpretable alternative to more complex architectures for knowledge graph
completion.

</details>


### [90] [Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings](https://arxiv.org/abs/2505.18973)
*Sarang Patil,Ashish Parmanand Pandey,Ioannis Koutis,Mengjia Xu*

Main category: cs.CL

TL;DR: 提出Hierarchical Mamba模型，结合Mamba2与双曲几何，有效捕捉层次化语言关系，在混合跳数推理任务中超越欧几里得基线


<details>
  <summary>Details</summary>
Motivation: 现有选择性状态空间模型在复杂层次推理任务中表现不足，且主流语言模型的欧几里得嵌入难以捕捉潜在语义层次

Method: 将Mamba2处理后的序列通过可学习曲率映射到Poincare球（正切投影）或Lorentz流形（正余弦投影），结合双曲损失优化层次关系建模

Result: 在四个本体数据集上，双曲版本HiM均优于欧式基线：HiM-Poincare捕获更细粒度语义（高h-norm），HiM-Lorentz提供更稳定的紧凑层次嵌入

Conclusion: 双曲几何有效提升语言模型的层次推理能力，Poincare和Lorentz投影分别适用于细节敏感和稳定性优先的不同应用场景

Abstract: Selective state-space models have achieved great success in long-sequence
modeling. However, their capacity for language representation, especially in
complex hierarchical reasoning tasks, remains underexplored. Most large
language models rely on flat Euclidean embeddings, limiting their ability to
capture latent hierarchies. To address this limitation, we propose Hierarchical
Mamba (HiM), integrating efficient Mamba2 with exponential growth and curved
nature of hyperbolic geometry to learn hierarchy-aware language embeddings for
deeper linguistic understanding. Mamba2-processed sequences are projected to
the Poincare ball (via tangent-based mapping) or Lorentzian manifold (via
cosine and sine-based mapping) with "learnable" curvature, optimized with a
combined hyperbolic loss. Our HiM model facilitates the capture of relational
distances across varying hierarchical levels, enabling effective long-range
reasoning. This makes it well-suited for tasks like mixed-hop prediction and
multi-hop inference in hierarchical classification. We evaluated our HiM with
four linguistic and medical datasets for mixed-hop prediction and multi-hop
inference tasks. Experimental results demonstrated that: 1) Both HiM models
effectively capture hierarchical relationships for four ontological datasets,
surpassing Euclidean baselines. 2) HiM-Poincare captures fine-grained semantic
distinctions with higher h-norms, while HiM-Lorentz provides more stable,
compact, and hierarchy-preserving embeddings favoring robustness over detail.

</details>


### [91] [AI4Math: A Native Spanish Benchmark for University-Level Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2505.18978)
*Miguel Angel Peñaloza Perez,Bruno Lopez Orozco,Jesus Tadeo Cruz Soto,Michelle Bruno Hernandez,Miguel Angel Alvarado Gonzalez,Sandra Malagon*

Main category: cs.CL

TL;DR: 创建原生西班牙语数学基准AI4Math，评估大语言模型在不同语言/领域下的表现


<details>
  <summary>Details</summary>
Motivation: 解决现有数学基准多基于翻译导致语义偏差的问题，建立语言特异性评估标准

Method: 构建含105个西班牙语大学数学问题（覆盖7领域）的基准，测试6个模型在零样本/思维链（西英双语）的表现

Result: 顶级模型准确率超70%，几何/组合/概率仍具挑战性，多数模型跨语言表现稳定（GPT-4o西语零样本更优）

Conclusion: 需原生语言基准和领域评估来揭示模型缺陷，模型跨语言能力较强但特定领域仍需突破

Abstract: Existing mathematical reasoning benchmarks are predominantly English only or
translation-based, which can introduce semantic drift and mask languagespecific
reasoning errors. To address this, we present AI4Math, a benchmark of 105
original university level math problems natively authored in Spanish. The
dataset spans seven advanced domains (Algebra, Calculus, Geometry, Probability,
Number Theory, Combinatorics, and Logic), and each problem is accompanied by a
step by step human solution. We evaluate six large language models GPT 4o, GPT
4o mini, o3 mini, LLaMA 3.3 70B, DeepSeek R1 685B, and DeepSeek V3 685B under
four configurations: zero shot and chain of thought, each in Spanish and
English. The top models (o3 mini, DeepSeek R1 685B, DeepSeek V3 685B) achieve
over 70% accuracy, whereas LLaMA 3.3 70B and GPT-4o mini remain below 40%. Most
models show no significant performance drop between languages, with GPT 4o even
performing better on Spanish problems in the zero shot setting. Geometry,
Combinatorics, and Probability questions remain persistently challenging for
all models. These results highlight the need for native-language benchmarks and
domain-specific evaluations to reveal reasoning failures not captured by
standard metrics.

</details>


### [92] [FiLLM -- A Filipino-optimized Large Language Model based on Southeast Asia Large Language Model (SEALLM)](https://arxiv.org/abs/2505.18995)
*Carlos Jude G. Maminta,Isaiah Job Enriquez,Deandre Nigel Nunez,Michael B. Dela Fuente*

Main category: cs.CL

TL;DR: FiLLM是基于SeaLLM-7B 2.5优化的菲律宾语大语言模型，通过LoRA微调提升内存效率，但评估显示CalamanCy模型在多项NLP任务中表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决菲律宾语NLP工具匮乏的问题，通过参数高效微调技术适配本土语言特性，推动低资源语言场景的应用发展。

Method: 采用Low-Rank Adaptation(LoRA)微调SeaLLM-7B 2.5模型，在命名实体识别、词性标注等任务上使用多维度评估指标对比CalamanCy模型。

Result: CalamanCy在F1值、精确率等关键指标上整体优于FiLLM，但FiLLM在内存效率和模型轻量化方面展现优势。

Conclusion: 研究证实参数高效微调可有效平衡模型性能与资源消耗，为菲律宾语NLP生态提供了可扩展的技术方案。

Abstract: This study presents FiLLM, a Filipino-optimized large language model,
designed to enhance natural language processing (NLP) capabilities in the
Filipino language. Built upon the SeaLLM-7B 2.5 model, FiLLM leverages Low-Rank
Adaptation (LoRA) fine-tuning to optimize memory efficiency while maintaining
task-specific performance. The model was trained and evaluated on diverse
Filipino datasets to address key NLP tasks, including Named Entity Recognition
(NER), Part-of-Speech (POS) tagging, Dependency Parsing, and Text
Summarization. Performance comparisons with the CalamanCy model were conducted
using F1 Score, Precision, Recall, Compression Rate, and Keyword Overlap
metrics. Results indicate that Calamancy outperforms FILLM in several aspects,
demonstrating its effectiveness in processing Filipino text with improved
linguistic comprehension and adaptability. This research contributes to the
advancement of Filipino NLP applications by providing an optimized, efficient,
and scalable language model tailored for local linguistic needs.

</details>


### [93] [VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied Iterative Policy Optimization](https://arxiv.org/abs/2505.19000)
*Yunxin Li,Xinyu Chen,Zitao Li,Zhenyu Liu,Longyue Wang,Wenhan Luo,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 提出VerIPO方法，通过验证器引导的迭代训练循环(GRPO-Verifier-DPO)解决视频大模型长推理链优化难题，显著提升生成质量与训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有强化微调方法(如GRPO)存在数据质量差、长思维链改进不稳定等问题，需要更高效的优化框架提升视频推理能力。

Method: 构建三阶段训练循环：1) GRPO扩展搜索空间 2) 小模型作为验证器筛选逻辑连贯的推理链 3) 高质量数据驱动DPO快速优化。

Result: 在多项视频推理任务中生成长度2-3倍于基线的思维链，训练速度提升7倍，性能超越Kimi-VL/Video-R1等先进模型。

Conclusion: 验证器引导的迭代优化机制有效结合GRPO的探索能力与DPO的定向优化，为长视频推理任务提供了稳定高效的训练范式。

Abstract: Applying Reinforcement Learning (RL) to Video Large Language Models
(Video-LLMs) shows significant promise for complex video reasoning. However,
popular Reinforcement Fine-Tuning (RFT) methods, such as outcome-based Group
Relative Policy Optimization (GRPO), are limited by data preparation
bottlenecks (e.g., noise or high cost) and exhibit unstable improvements in the
quality of long chain-of-thoughts (CoTs) and downstream performance.To address
these limitations, we propose VerIPO, a Verifier-guided Iterative Policy
Optimization method designed to gradually improve video LLMs' capacity for
generating deep, long-term reasoning chains. The core component is
Rollout-Aware Verifier, positioned between the GRPO and Direct Preference
Optimization (DPO) training phases to form the GRPO-Verifier-DPO training loop.
This verifier leverages small LLMs as a judge to assess the reasoning logic of
rollouts, enabling the construction of high-quality contrastive data, including
reflective and contextually consistent CoTs. These curated preference samples
drive the efficient DPO stage (7x faster than GRPO), leading to marked
improvements in reasoning chain quality, especially in terms of length and
contextual consistency. This training loop benefits from GRPO's expansive
search and DPO's targeted optimization. Experimental results demonstrate: 1)
Significantly faster and more effective optimization compared to standard GRPO
variants, yielding superior performance; 2) Our trained models exceed the
direct inference of large-scale instruction-tuned Video-LLMs, producing long
and contextually consistent CoTs on diverse video reasoning tasks; and 3) Our
model with one iteration outperforms powerful LMMs (e.g., Kimi-VL) and long
reasoning models (e.g., Video-R1), highlighting its effectiveness and
stability.

</details>


### [94] [CrosGrpsABS: Cross-Attention over Syntactic and Semantic Graphs for Aspect-Based Sentiment Analysis in a Low-Resource Language](https://arxiv.org/abs/2505.19018)
*Md. Mithun Hossain,Md. Shakil Hossain,Sudipto Chaki,Md. Rajib Hossain,Md. Saifur Rahman,A. B. M. Shawkat Ali*

Main category: cs.CL

TL;DR: 提出CrosGrpsABS混合框架，通过句法-语义图双向跨注意力机制提升孟加拉语低资源环境下的细粒度情感分析性能，并在多数据集验证有效性


<details>
  <summary>Details</summary>
Motivation: 孟加拉语等低资源语言缺乏标注数据/预训练模型/优化超参数，且现有ABSA方法高度依赖英语等资源丰富语言的特定工具，难以应对其独特语言特性

Method: CrosGrpsABS框架：1) 基于规则的句法依赖解析构建语法图 2) 语义相似度计算构建语义图 3) 双向跨注意力机制融合局部句法结构与全局语义 4) Transformer嵌入与图卷积网络联合建模

Result: 在4个孟加拉语数据集和英文SemEval2014任务4上超越现有方法，其中餐厅/笔记本电脑领域F1分别提升0.93%/1.06%

Conclusion: 双向跨注意力有效整合多维度语言特征，框架具备跨语言迁移能力，为低资源NLP提供新解决方案

Abstract: Aspect-Based Sentiment Analysis (ABSA) is a fundamental task in natural
language processing, offering fine-grained insights into opinions expressed in
text. While existing research has largely focused on resource-rich languages
like English which leveraging large annotated datasets, pre-trained models, and
language-specific tools. These resources are often unavailable for low-resource
languages such as Bengali. The ABSA task in Bengali remains poorly explored and
is further complicated by its unique linguistic characteristics and a lack of
annotated data, pre-trained models, and optimized hyperparameters. To address
these challenges, this research propose CrosGrpsABS, a novel hybrid framework
that leverages bidirectional cross-attention between syntactic and semantic
graphs to enhance aspect-level sentiment classification. The CrosGrpsABS
combines transformerbased contextual embeddings with graph convolutional
networks, built upon rule-based syntactic dependency parsing and semantic
similarity computations. By employing bidirectional crossattention, the model
effectively fuses local syntactic structure with global semantic context,
resulting in improved sentiment classification performance across both low- and
high-resource settings. We evaluate CrosGrpsABS on four low-resource Bengali
ABSA datasets and the high-resource English SemEval 2014 Task 4 dataset. The
CrosGrpsABS consistently outperforms existing approaches, achieving notable
improvements, including a 0.93% F1-score increase for the Restaurant domain and
a 1.06% gain for the Laptop domain in the SemEval 2014 Task 4 benchmark.

</details>


### [95] [Efficient Data Selection at Scale via Influence Distillation](https://arxiv.org/abs/2505.19051)
*Mahdi Nikdan,Vincent Cohen-Addad,Dan Alistarh,Vahab Mirrokni*

Main category: cs.CL

TL;DR: 提出Influence Distillation框架，利用二阶信息优化训练数据权重选择，在保持性能的同时提升3.5倍数据选择速度


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型训练中数据选择效率低的问题，传统方法缺乏数学依据且计算成本高。本文旨在通过数学推导优化样本权重，实现高效且模型相关的数据选择。

Method: 1. 提出基于目标分布样本影响力的权重分配框架 2. 推导梯度下降和Adam优化器下的最优权重公式 3. 设计地标样本近似法，仅精确计算关键样本影响力后传播到全量数据

Result: 在Tulu V2指令调优实验中，GSM8k/SQuAD/MMLU任务上匹配或超越SOTA，Llama/Qwen系列模型均验证有效性，数据选择速度提升达3.5倍

Conclusion: 该框架为数据选择提供了数学严谨的解决方案，显著提升训练效率的同时保持模型性能，为大规模模型训练优化开辟新路径

Abstract: Effective data selection is critical for efficient training of modern Large
Language Models (LLMs). This paper introduces Influence Distillation, a novel,
mathematically-justified framework for data selection that employs second-order
information to optimally weight training samples. By distilling each sample's
influence on a target distribution, our method assigns model-specific weights
that are used to select training data for LLM fine-tuning, guiding it toward
strong performance on the target domain. We derive these optimal weights for
both Gradient Descent and Adam optimizers. To ensure scalability and reduce
computational cost, we propose a $\textit{landmark-based approximation}$:
influence is precisely computed for a small subset of "landmark" samples and
then efficiently propagated to all other samples to determine their weights. We
validate Influence Distillation by applying it to instruction tuning on the
Tulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU,
across several models from the Llama and Qwen families. Experiments show that
Influence Distillation matches or outperforms state-of-the-art performance
while achieving up to $3.5\times$ faster selection.

</details>


### [96] [An Embarrassingly Simple Defense Against LLM Abliteration Attacks](https://arxiv.org/abs/2505.19056)
*Harethah Abu Shairah,Hasan Abed Al Kader Hammoud,Bernard Ghanem,George Turkiyyah*

Main category: cs.CL

TL;DR: 提出基于扩展拒绝数据集的微调方法，有效防御LLMs的abliteration攻击并保持性能


<details>
  <summary>Details</summary>
Motivation: 现有LLMs安全机制易被abliteration攻击破坏（拒绝率骤降70-80%），需增强防御能力

Method: 构建含完整拒绝理由的扩展拒绝数据集，对Llama-2-7B-Chat和Qwen2.5系列模型进行微调

Result: 扩展拒绝模型拒绝率最多下降10%（基准模型下降70-80%），安全性和通用性能保持平衡

Conclusion: 扩展拒绝微调成功抵御abliteration攻击，在维持模型实用性的同时提升安全性

Abstract: Large language models (LLMs) are typically aligned to comply with safety
guidelines by refusing harmful instructions. A recent attack, termed
abliteration, isolates and suppresses the single latent direction most
responsible for refusal behavior, enabling the model to generate unethical
content. We propose a defense that modifies how models generate refusals. We
construct an extended-refusal dataset that contains harmful prompts with a full
response that justifies the reason for refusal. We then fine-tune
Llama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our
extended-refusal dataset, and evaluate the resulting systems on a set of
harmful prompts. In our experiments, extended-refusal models maintain high
refusal rates, dropping at most by 10%, whereas baseline models' refusal rates
drop by 70-80% after abliteration. A broad evaluation of safety and utility
shows that extended-refusal fine-tuning neutralizes the abliteration attack
while preserving general performance.

</details>


### [97] [UNCERTAINTY-LINE: Length-Invariant Estimation of Uncertainty for Large Language Models](https://arxiv.org/abs/2505.19060)
*Roman Vashurin,Maiya Goloburda,Preslav Nakov,Maxim Panov*

Main category: cs.CL

TL;DR: 提出UNCERTAINTY-LINE方法消除大语言模型不确定性评估中的长度偏差


<details>
  <summary>Details</summary>
Motivation: 现有基于词元概率的不确定性量化方法存在输出长度偏差问题，即使长度归一化后仍残留偏差

Method: 通过回归分析建立不确定性分数与输出长度的关系，利用回归残差构建长度不变估计量

Result: 在机器翻译、摘要生成和问答任务中，该方法在多个模型和指标上优于现有UQ方法

Conclusion: UNCERTAINTY-LINE作为模型无关的后处理方法，有效提升了不确定性估计的可靠性和跨任务适应性

Abstract: Large Language Models (LLMs) have become indispensable tools across various
applications, making it more important than ever to ensure the quality and the
trustworthiness of their outputs. This has led to growing interest in
uncertainty quantification (UQ) methods for assessing the reliability of LLM
outputs. Many existing UQ techniques rely on token probabilities, which
inadvertently introduces a bias with respect to the length of the output. While
some methods attempt to account for this, we demonstrate that such biases
persist even in length-normalized approaches. To address the problem, here we
propose UNCERTAINTY-LINE: (Length-INvariant Estimation), a simple debiasing
procedure that regresses uncertainty scores on output length and uses the
residuals as corrected, length-invariant estimates. Our method is post-hoc,
model-agnostic, and applicable to a range of UQ measures. Through extensive
evaluation on machine translation, summarization, and question-answering tasks,
we demonstrate that UNCERTAINTY-LINE: consistently improves over even nominally
length-normalized UQ methods uncertainty estimates across multiple metrics and
models.

</details>


### [98] [Towards Harmonized Uncertainty Estimation for Large Language Models](https://arxiv.org/abs/2505.19073)
*Rui Li,Jing Long,Muge Qi,Heming Xia,Lei Sha,Peiyi Wang,Zhifang Sui*

Main category: cs.CL

TL;DR: 提出CUE方法，通过轻量级模型校正LLM的不确定性分数，在多项任务中比现有方法提升60%


<details>
  <summary>Details</summary>
Motivation: 现有LLM不确定性评估方法在平衡性、校准性上存在缺陷，影响可靠性评估

Method: CUE框架使用目标LLM性能对齐数据训练轻量校正模型调整不确定性分数

Result: 跨模型/任务实验显示该方法提升达60%，实现更均衡的置信度校准

Conclusion: CUE以简单架构显著提升LLM不确定性评估质量，增强模型部署可信度

Abstract: To facilitate robust and trustworthy deployment of large language models
(LLMs), it is essential to quantify the reliability of their generations
through uncertainty estimation. While recent efforts have made significant
advancements by leveraging the internal logic and linguistic features of LLMs
to estimate uncertainty scores, our empirical analysis highlights the pitfalls
of these methods to strike a harmonized estimation between indication, balance,
and calibration, which hinders their broader capability for accurate
uncertainty estimation. To address this challenge, we propose CUE (Corrector
for Uncertainty Estimation): A straightforward yet effective method that
employs a lightweight model trained on data aligned with the target LLM's
performance to adjust uncertainty scores. Comprehensive experiments across
diverse models and tasks demonstrate its effectiveness, which achieves
consistent improvements of up to 60% over existing methods.

</details>


### [99] [ReadBench: Measuring the Dense Text Visual Reading Ability of Vision-Language Models](https://arxiv.org/abs/2505.19091)
*Benjamin Clavié,Florian Brand*

Main category: cs.CL

TL;DR: 论文提出ReadBench多模态基准，专门用于评估视觉语言模型(VLMs)对文本丰富图像的阅读理解能力，发现现有模型在长文本处理上存在显著缺陷


<details>
  <summary>Details</summary>
Motivation: 现有VLMs评估主要关注视觉理解(如图表/OCR任务)，缺乏对文本图像阅读推理能力的系统评估，需建立针对性测试框架

Method: 将纯文本基准的上下文转化为文本图像，保留原始文本提示和问题，构建包含单页短文本与多页长文本的评估体系

Result: VLMs在短文本图像处理中性能微降(约1-3%)，但在多页长上下文场景下准确率骤降超50%；文本分辨率对模型表现影响可忽略

Conclusion: VLMs需重点提升对视觉化长文本的推理能力，这对文档处理/教育等实际应用至关重要，ReadBench开源促进相关研究

Abstract: Recent advancements in Large Vision-Language Models (VLMs), have greatly
enhanced their capability to jointly process text and images. However, despite
extensive benchmarks evaluating visual comprehension (e.g., diagrams, color
schemes, OCR tasks...), there is limited assessment of VLMs' ability to read
and reason about text-rich images effectively. To fill this gap, we introduce
ReadBench, a multimodal benchmark specifically designed to evaluate the reading
comprehension capabilities of VLMs. ReadBench transposes contexts from
established text-only benchmarks into images of text while keeping textual
prompts and questions intact. Evaluating leading VLMs with ReadBench, we find
minimal-but-present performance degradation on short, text-image inputs, while
performance sharply declines for longer, multi-page contexts. Our experiments
further reveal that text resolution has negligible effects on multimodal
performance. These findings highlight needed improvements in VLMs, particularly
their reasoning over visually presented extensive textual content, a capability
critical for practical applications. ReadBench is available at
https://github.com/answerdotai/ReadBench .

</details>


### [100] [ASPO: Adaptive Sentence-Level Preference Optimization for Fine-Grained Multimodal Reasoning](https://arxiv.org/abs/2505.19100)
*Yeyuan Wang,Dehong Gao,Rujiao Long,Lei Yi,Linbo Jin,Libin Yang,Xiaoyan Cai*

Main category: cs.CL

TL;DR: 提出自适应句子级偏好优化(ASPO)，通过句子级动态奖励机制改进多模态模型对齐效果


<details>
  <summary>Details</summary>
Motivation: 传统DPO方法基于二元偏好优化，缺乏对细粒度文本片段正确性的评估，导致次优解

Method: 基于模型预测动态计算句子级自适应奖励，无需额外模型/参数即可增强内容评估

Result: 大量实验表明ASPO显著提升多模态模型的整体性能表现

Conclusion: 句子级自适应优化机制有效改进了多模态特征对齐，为模型优化提供新思路

Abstract: Direct Preference Optimization (DPO) has gained significant attention for its
simplicity and computational efficiency in aligning large language models
(LLMs). Recent advancements have extended DPO to multimodal scenarios,
achieving strong performance. However, traditional DPO relies on binary
preference optimization, rewarding or penalizing entire responses without
considering fine-grained segment correctness, leading to suboptimal solutions.
The root of this issue lies in the absence of fine-grained supervision during
the optimization process. To address this, we propose Adaptive Sentence-level
Preference Optimization (ASPO), which evaluates individual sentences for more
precise preference optimization. By dynamically calculating adaptive rewards at
the sentence level based on model predictions, ASPO enhances response content
assessment without additional models or parameters. This significantly improves
the alignment of multimodal features. Extensive experiments show that ASPO
substantially enhances the overall performance of multimodal models.

</details>


### [101] [WHISTRESS: Enriching Transcriptions with Sentence Stress Detection](https://arxiv.org/abs/2505.19103)
*Iddo Yosha,Dorin Shteyman,Yossi Adi*

Main category: cs.CL

TL;DR: 提出无需对齐的WHISTRESS模型增强语音转录系统的句子重音检测，通过合成数据集TINYSTRESS-15K实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 口语中句子重音对传达意图至关重要，但现有转录系统缺乏有效检测能力，需要无需对齐的解决方案。

Method: 开发自动化流程创建TINYSTRESS-15K合成数据集，并基于此训练无需先验知识的WHISTRESS模型。

Result: WHISTRESS在多项基准测试中超越现有方法，零样本泛化能力显著优于传统模型。

Conclusion: 合成数据驱动的WHISTRESS模型有效解决句子重音检测难题，为语音理解系统提供高效解决方案。

Abstract: Spoken language conveys meaning not only through words but also through
intonation, emotion, and emphasis. Sentence stress, the emphasis placed on
specific words within a sentence, is crucial for conveying speaker intent and
has been extensively studied in linguistics. In this work, we introduce
WHISTRESS, an alignment-free approach for enhancing transcription systems with
sentence stress detection. To support this task, we propose TINYSTRESS-15K, a
scalable, synthetic training data for the task of sentence stress detection
which resulted from a fully automated dataset creation process. We train
WHISTRESS on TINYSTRESS-15K and evaluate it against several competitive
baselines. Our results show that WHISTRESS outperforms existing methods while
requiring no additional input priors during training or inference. Notably,
despite being trained on synthetic data, WHISTRESS demonstrates strong
zero-shot generalization across diverse benchmarks. Project page:
https://pages.cs.huji.ac.il/adiyoss-lab/whistress.

</details>


### [102] [CCHall: A Novel Benchmark for Joint Cross-Lingual and Cross-Modal Hallucinations Detection in Large Language Models](https://arxiv.org/abs/2505.19108)
*Yongheng Zhang,Xu Liu,Ruoxi Zhou,Qiguang Chen,Hao Fei,Wenpeng Lu,Libo Qin*

Main category: cs.CL

TL;DR: 提出联合跨语言与跨模态的幻觉基准测试CCHall，用于评估大语言模型在复杂场景中的表现


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注单一场景（跨语言或跨模态），缺乏对联合场景下模型幻觉问题的系统性评估

Method: 构建CCHall基准测试集，包含跨语言（多语言）和跨模态（文本-图像）的联合评估任务，对主流开源/闭源LLMs进行综合测试

Result: 实验表明当前LLMs在CCHall基准上表现欠佳，验证了联合场景评估的必要性

Conclusion: CCHall为评估大语言模型在复杂现实场景中的能力提供了重要工具，推动LLMs在跨语言跨模态任务中的可靠性研究

Abstract: Investigating hallucination issues in large language models (LLMs) within
cross-lingual and cross-modal scenarios can greatly advance the large-scale
deployment in real-world applications. Nevertheless, the current studies are
limited to a single scenario, either cross-lingual or cross-modal, leaving a
gap in the exploration of hallucinations in the joint cross-lingual and
cross-modal scenarios. Motivated by this, we introduce a novel joint
Cross-lingual and Cross-modal Hallucinations benchmark (CCHall) to fill this
gap. Specifically, CCHall simultaneously incorporates both cross-lingual and
cross-modal hallucination scenarios, which can be used to assess the
cross-lingual and cross-modal capabilities of LLMs. Furthermore, we conduct a
comprehensive evaluation on CCHall, exploring both mainstream open-source and
closed-source LLMs. The experimental results highlight that current LLMs still
struggle with CCHall. We hope CCHall can serve as a valuable resource to assess
LLMs in joint cross-lingual and cross-modal scenarios.

</details>


### [103] [Self-Critique Guided Iterative Reasoning for Multi-hop Question Answering](https://arxiv.org/abs/2505.19112)
*Zheng Chu,Huiming Fan,Jingchang Chen,Qianyu Wang,Mingda Yang,Jiafeng Liang,Zhongjie Wang,Hao Li,Guo Tang,Ming Liu,Bing Qin*

Main category: cs.CL

TL;DR: 提出自批判引导迭代推理方法(SiGIR)，通过端到端训练实现问题分解与自我评估，提升多跳推理效果


<details>
  <summary>Details</summary>
Motivation: 现有迭代检索方法缺乏中间步骤指导，导致检索不准确和中间推理错误，需通过自我反馈机制优化推理轨迹选择

Method: 结合分支探索与自我评估机制，通过端到端训练实现迭代式问题分解，并基于质量评估选择最优推理路径

Result: 在三个多跳推理数据集上超过先前最优方法8.6%

Conclusion: 自我批判机制能有效提升复杂推理的可靠性，开放代码和数据为后续研究提供基础

Abstract: Although large language models (LLMs) have demonstrated remarkable reasoning
capabilities, they still face challenges in knowledge-intensive multi-hop
reasoning. Recent work explores iterative retrieval to address complex
problems. However, the lack of intermediate guidance often results in
inaccurate retrieval and flawed intermediate reasoning, leading to incorrect
reasoning. To address these, we propose Self-Critique Guided Iterative
Reasoning (SiGIR), which uses self-critique feedback to guide the iterative
reasoning process. Specifically, through end-to-end training, we enable the
model to iteratively address complex problems via question decomposition.
Additionally, the model is able to self-evaluate its intermediate reasoning
steps. During iterative reasoning, the model engages in branching exploration
and employs self-evaluation to guide the selection of promising reasoning
trajectories. Extensive experiments on three multi-hop reasoning datasets
demonstrate the effectiveness of our proposed method, surpassing the previous
SOTA by $8.6\%$. Furthermore, our thorough analysis offers insights for future
research. Our code, data, and models are available at Github:
https://github.com/zchuz/SiGIR-MHQA.

</details>


### [104] [Controlling Language Confusion in Multilingual LLMs](https://arxiv.org/abs/2505.19116)
*Nahyun Lee,Yeongseo Woo,Hyunwoo Ko,Guijin Son*

Main category: cs.CL

TL;DR: 研究通过ORPO惩罚机制有效缓解大语言模型的跨语言混淆问题，在低资源环境下保持模型性能


<details>
  <summary>Details</summary>
Motivation: 传统监督微调加剧语言混淆问题，影响低资源场景用户体验，需探索改进方案

Method: 在标准SFT基础上引入ORPO惩罚机制，通过抑制非目标语言输出来提升模型语言一致性

Result: ORPO在高解码温度下仍能有效控制语言混淆，且不损害模型整体生成质量

Conclusion: 适当惩罚机制可显著改善语言混淆问题，为低资源环境提供有效解决方案

Abstract: Large language models often suffer from language confusion, a phenomenon
where responses are partially or entirely generated in unintended languages.
This can critically impact user experience in low-resource settings. We
hypothesize that conventional supervised fine-tuning exacerbates this issue
because the softmax objective focuses probability mass only on the single
correct token but does not explicitly penalize cross-lingual mixing.
Interestingly, by observing loss trajectories during the pretraining phase, we
observe that models fail to learn to distinguish between monolingual and
language-confused text. Additionally, we find that ORPO, which adds penalties
for unwanted output styles to standard SFT, effectively suppresses
language-confused generations even at high decoding temperatures without
degrading overall model performance. Our findings suggest that incorporating
appropriate penalty terms can mitigate language confusion in low-resource
settings with limited data.

</details>


### [105] [Delving into Multilingual Ethical Bias: The MSQAD with Statistical Hypothesis Tests for Large Language Models](https://arxiv.org/abs/2505.19121)
*Seunguk Yu,Juhwan Choi,Youngbin Kim*

Main category: cs.CL

TL;DR: 研究通过创建多语言敏感问答数据集MSQAD，验证并比较了不同语言模型在敏感话题上的伦理偏见，发现跨语言差异导致的偏见普遍存在且不同模型间表现一致，同时开源数据集促进后续研究


<details>
  <summary>Details</summary>
Motivation: 验证语言模型存在的伦理偏见是否源于语言特异性差异，通过构建多语言敏感问题数据集分析跨语言偏见表现

Method: 1. 收集人权观察组织17个主题的新闻 2. 生成多语言敏感问题及回答 3. 运用两种统计假设检验方法分析语言/主题维度的回答偏见

Result: 统计检验显示多数情况下拒绝原假设，证实不同语言间存在显著伦理偏见，且不同LLM模型间普遍存在此现象

Conclusion: 语言模型的伦理偏见具有跨语言普遍性，开源MSQAD数据集可为后续模型偏见的检测与优化提供基准

Abstract: Despite the recent strides in large language models, studies have underscored
the existence of social biases within these systems. In this paper, we delve
into the validation and comparison of the ethical biases of LLMs concerning
globally discussed and potentially sensitive topics, hypothesizing that these
biases may arise from language-specific distinctions. Introducing the
Multilingual Sensitive Questions & Answers Dataset (MSQAD), we collected news
articles from Human Rights Watch covering 17 topics, and generated socially
sensitive questions along with corresponding responses in multiple languages.
We scrutinized the biases of these responses across languages and topics,
employing two statistical hypothesis tests. The results showed that the null
hypotheses were rejected in most cases, indicating biases arising from
cross-language differences. It demonstrates that ethical biases in responses
are widespread across various languages, and notably, these biases were
prevalent even among different LLMs. By making the proposed MSQAD openly
available, we aim to facilitate future research endeavors focused on examining
cross-language biases in LLMs and their variant models.

</details>


### [106] [MMATH: A Multilingual Benchmark for Mathematical Reasoning](https://arxiv.org/abs/2505.19126)
*Wenyang Luo,Wayne Xin Zhao,Jing Sha,Shijin Wang,Ji-Rong Wen*

Main category: cs.CL

TL;DR: 提出多语言复杂推理基准MMATH，揭示先进模型存在的语言性能差异并提出英式推理+目标语言输出的优化策略


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型在多语言复杂任务上的能力研究不足，需建立更全面的评估基准

Method: 构建覆盖10种语言/374数学题的MMATH基准，分析模型跨语言表现及语言偏离问题

Result: 通过提示策略优化，实现模型性能提升15.2%且目标语言回答准确率提高至98.7%

Conclusion: 研究为提升大模型多语言推理能力提供新方法论，验证跨语言思维链的有效性

Abstract: The advent of large reasoning models, such as OpenAI o1 and DeepSeek R1, has
significantly advanced complex reasoning tasks. However, their capabilities in
multilingual complex reasoning remain underexplored, with existing efforts
largely focused on simpler tasks like MGSM. To address this gap, we introduce
MMATH, a benchmark for multilingual complex reasoning spanning 374 high-quality
math problems across 10 typologically diverse languages. Using MMATH, we
observe that even advanced models like DeepSeek R1 exhibit substantial
performance disparities across languages and suffer from a critical off-target
issue-generating responses in unintended languages. To address this, we explore
strategies including prompting and training, demonstrating that reasoning in
English and answering in target languages can simultaneously enhance
performance and preserve target-language consistency. Our findings offer new
insights and practical strategies for advancing the multilingual reasoning
capabilities of large language models. Our code and data could be found at
https://github.com/RUCAIBox/MMATH.

</details>


### [107] [RetrieveAll: A Multilingual Named Entity Recognition Framework with Large Language Models](https://arxiv.org/abs/2505.19128)
*Jin Zhang,Fan Gao,Linyu Li,Yongbin Yu,Xiangxiang Wang,Nyima Tashi,Gadeng Luosang*

Main category: cs.CL

TL;DR: 提出基于动态LoRA的RetrieveAll框架，通过解耦跨语言特征和跨粒度知识增强方法，显著提升多语言NER性能，在PAN-X数据集实现12.1%的F1提升。


<details>
  <summary>Details</summary>
Motivation: 现有多语言NER方法存在语言干扰问题，高/低资源语言间存在特征冲突和竞争抑制。单独训练模型虽能缓解但成本过高，需高效解决方案。

Method: 1. 动态LoRA实现跨语言特征解耦和动态适配
2. 跨粒度知识增强挖掘数据内在潜力
3. 分层提示机制将知识注入范式升级为'提示驱动学习'

Result: 在PAN-X数据集上的实验表明，平均F1分数提升12.1%，优于现有基线方法。

Conclusion: RetrieveAll有效解决了多语言NER中的语言干扰问题，不依赖外部资源即可提升性能，为提示学习范式提供了新方向。

Abstract: The rise of large language models has led to significant performance
breakthroughs in named entity recognition (NER) for high-resource languages,
yet there remains substantial room for improvement in low- and medium-resource
languages. Existing multilingual NER methods face severe language interference
during the multi-language adaptation process, manifested in feature conflicts
between different languages and the competitive suppression of low-resource
language features by high-resource languages. Although training a dedicated
model for each language can mitigate such interference, it lacks scalability
and incurs excessive computational costs in real-world applications. To address
this issue, we propose RetrieveAll, a universal multilingual NER framework
based on dynamic LoRA. The framework decouples task-specific features across
languages and demonstrates efficient dynamic adaptability. Furthermore, we
introduce a cross-granularity knowledge augmented method that fully exploits
the intrinsic potential of the data without relying on external resources. By
leveraging a hierarchical prompting mechanism to guide knowledge injection,
this approach advances the paradigm from "prompt-guided inference" to
"prompt-driven learning." Experimental results show that RetrieveAll
outperforms existing baselines; on the PAN-X dataset, it achieves an average F1
improvement of 12.1 percent.

</details>


### [108] [Shifting AI Efficiency From Model-Centric to Data-Centric Compression](https://arxiv.org/abs/2505.19147)
*Xuyang Liu,Zichen Wen,Shaobo Wang,Junjie Chen,Zhishan Tao,Yubo Wang,Xiangqi Jin,Chang Zou,Yiyu Wang,Chenfei Liao,Xu Zheng,Honggang Chen,Weijia Li,Xuming Hu,Conghui He,Linfeng Zhang*

Main category: cs.CL

TL;DR: 提出以数据为中心的令牌压缩作为解决长上下文AI效率瓶颈的新范式


<details>
  <summary>Details</summary>
Motivation: 硬件限制下模型中心压缩已触顶，二次计算成本成为新瓶颈，需转向数据层面的效率优化

Method: 建立统一数学框架分析模型效率策略，系统回顾令牌压缩技术及其跨领域应用场景

Result: 证实令牌压缩在多种场景的显著优势，明确当前研究挑战并规划未来发展路径

Conclusion: 令牌压缩标志着AI效率研究的关键转折，需跨学科协作应对长上下文挑战

Abstract: The rapid advancement of large language models (LLMs) and multi-modal LLMs
(MLLMs) has historically relied on model-centric scaling through increasing
parameter counts from millions to hundreds of billions to drive performance
gains. However, as we approach hardware limits on model size, the dominant
computational bottleneck has fundamentally shifted to the quadratic cost of
self-attention over long token sequences, now driven by ultra-long text
contexts, high-resolution images, and extended videos. In this position paper,
\textbf{we argue that the focus of research for efficient AI is shifting from
model-centric compression to data-centric compression}. We position token
compression as the new frontier, which improves AI efficiency via reducing the
number of tokens during model training or inference. Through comprehensive
analysis, we first examine recent developments in long-context AI across
various domains and establish a unified mathematical framework for existing
model efficiency strategies, demonstrating why token compression represents a
crucial paradigm shift in addressing long-context overhead. Subsequently, we
systematically review the research landscape of token compression, analyzing
its fundamental benefits and identifying its compelling advantages across
diverse scenarios. Furthermore, we provide an in-depth analysis of current
challenges in token compression research and outline promising future
directions. Ultimately, our work aims to offer a fresh perspective on AI
efficiency, synthesize existing research, and catalyze innovative developments
to address the challenges that increasing context lengths pose to the AI
community's advancement.

</details>


### [109] [SpokenNativQA: Multilingual Everyday Spoken Queries for LLMs](https://arxiv.org/abs/2505.19163)
*Firoj Alam,Md Arid Hasan,Shammur Absar Chowdhury*

Main category: cs.CL

TL;DR: 首个多语言语音问答数据集SpokenNativQA，用于评估LLM在真实语音对话场景中的表现，包含33K语音样本并涵盖低资源语言


<details>
  <summary>Details</summary>
Motivation: 现有文本QA数据集无法反映语音交互中的口音、语音变异和语言多样性问题，需构建更贴近真实场景的语音基准测试

Method: 创建包含多语言/方言的自然语音数据集，测试不同ASR系统与LLM在语音问答任务中的表现

Result: 数据集有效评估语音交互性能，基准测试揭示了现有系统在低资源语言处理上的局限性

Conclusion: SpokenNativQA填补了语音评估空白，公开数据/代码促进LLM在语音交互领域的研究

Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across
various disciplines and tasks. However, benchmarking their capabilities with
multilingual spoken queries remains largely unexplored. In this study, we
introduce SpokenNativQA, the first multilingual and culturally aligned spoken
question-answering (SQA) dataset designed to evaluate LLMs in real-world
conversational settings. The dataset comprises approximately 33,000 naturally
spoken questions and answers in multiple languages, including low-resource and
dialect-rich languages, providing a robust benchmark for assessing LLM
performance in speech-based interactions. SpokenNativQA addresses the
limitations of text-based QA datasets by incorporating speech variability,
accents, and linguistic diversity. We benchmark different ASR systems and LLMs
for SQA and present our findings. We released the data at
(https://huggingface.co/datasets/QCRI/SpokenNativQA) and the experimental
scripts at (https://llmebench.qcri.org/) for the research community.

</details>


### [110] [Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge](https://arxiv.org/abs/2505.19176)
*Zhuo Liu,Moxin Li,Xun Deng,Qifan Wang,Fuli Feng*

Main category: cs.CL

TL;DR: 提出AGDe-Judge框架，通过三阶段训练策略减少教师模型偏好偏差，在多个评估基准保持优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法存在教师偏好偏差问题，代理模型训练时过度依赖教师模型生成的有偏数据，影响评估客观性。

Method: 引入无偏辅助模型补充训练数据，设计标签去偏、反馈去偏、数据增强三阶段框架(AGDe-Judge)实现双重去偏。

Result: 在6个评估基准上有效降低教师偏好偏差，模型性能保持稳定，实验验证框架有效性。

Conclusion: 首次提出包含辅助模型的评估设置，AGDe-Judge为LLM评估提供可靠去偏方案，代码已开源。

Abstract: LLM-as-a-Judge employs large language models (LLMs), such as GPT-4, to
evaluate the quality of LLM-generated responses, gaining popularity for its
cost-effectiveness and strong alignment with human evaluations. However,
training proxy judge models using evaluation data generated by powerful teacher
models introduces a critical yet previously overlooked issue: teacher
preference bias, where the proxy judge model learns a biased preference for
responses from the teacher model. To tackle this problem, we propose a novel
setting that incorporates an additional assistant model, which is not biased
toward the teacher model's responses, to complement the training data. Building
on this setup, we introduce AGDe-Judge, a three-stage framework designed to
debias from both the labels and feedbacks in the training data. Extensive
experiments demonstrate that AGDe-Judge effectively reduces teacher preference
bias while maintaining strong performance across six evaluation benchmarks.
Code is available at https://github.com/Liuz233/AGDe-Judge.

</details>


### [111] [Two LLMs debate, both are certain they've won](https://arxiv.org/abs/2505.19184)
*Minh Nhat Nguyen,Pradyumna Shyama Prasad*

Main category: cs.CL

TL;DR: LLMs在动态对抗辩论中无法准确调整置信度，存在系统性过度自信（初始72.9% vs 理性基准50%）、信心升级（最终达83%）、61.7%辩论出现相互高估、自我辩论时信心持续上升（64.1%→75.2%）、私密推理与公开评分不一致等问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究多在静态知识问答中评估LLM校准能力，本研究首次在动态对抗场景（多轮辩论+零和博弈）中验证模型调整置信度的能力。

Method: 组织10个前沿LLM进行60场三轮政策辩论，要求模型每轮结束后私下评估获胜概率（0-100分），控制任务不确定性以检测系统性偏差。

Result: 发现五个关键模式：1）系统性初始高估 2）辩论进程反而提升信心 3）61.7%辩论双方同时宣称≥75%胜率 4）克隆自我辩论信心持续上升（即使明确告知50%胜率仍从50%升至57.1%）5）思维链推理与公开评分存在矛盾。

Conclusion: LLMs在动态多任务中缺乏自我评估和信念更新能力，这对助理角色和智能体应用构成重大风险（因实际部署时往往缺乏人工审核）。

Abstract: Can LLMs accurately adjust their confidence when facing opposition? Building
on previous studies measuring calibration on static fact-based
question-answering tasks, we evaluate Large Language Models (LLMs) in a
dynamic, adversarial debate setting, uniquely combining two realistic factors:
(a) a multi-turn format requiring models to update beliefs as new information
emerges, and (b) a zero-sum structure to control for task-related uncertainty,
since mutual high-confidence claims imply systematic overconfidence. We
organized 60 three-round policy debates among ten state-of-the-art LLMs, with
models privately rating their confidence (0-100) in winning after each round.
We observed five concerning patterns: (1) Systematic overconfidence: models
began debates with average initial confidence of 72.9% vs. a rational 50%
baseline. (2) Confidence escalation: rather than reducing confidence as debates
progressed, debaters increased their win probabilities, averaging 83% by the
final round. (3) Mutual overestimation: in 61.7% of debates, both sides
simultaneously claimed >=75% probability of victory, a logical impossibility.
(4) Persistent self-debate bias: models debating identical copies increased
confidence from 64.1% to 75.2%; even when explicitly informed their chance of
winning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5)
Misaligned private reasoning: models' private scratchpad thoughts sometimes
differed from their public confidence ratings, raising concerns about
faithfulness of chain-of-thought reasoning. These results suggest LLMs lack the
ability to accurately self-assess or update their beliefs in dynamic,
multi-turn tasks; a major concern as LLM outputs are deployed without careful
review in assistant roles or agentic settings.

</details>


### [112] [LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling](https://arxiv.org/abs/2505.19187)
*Yang Xiao,Jiashuo Wang,Ruifeng Yuan,Chunpu Xu,Kaishuai Xu,Wenjie Li,Pengfei Liu*

Main category: cs.CL

TL;DR: 提出基于困惑度的重要性评估框架PIR，通过剪枝冗余推理步骤优化LLMs推理链，在保持准确率的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有思维链方法产生的推理链包含过多功能性冗余步骤(如验证/纠错)，导致测试阶段计算效率低下，需在保持核心推理路径的前提下优化模型效率。

Method: 设计PIR框架量化评估各推理步骤对预测置信度的影响，选择性剪枝低重要性功能步骤，保留渐进推理核心路径构建优化训练数据。

Result: 在AIME等基准测试中实现准确率提升0.9%-6.6%，token使用减少41%，且在不同规模模型/数据源中保持良好泛化性。

Conclusion: PIR为需要高效推理的实际场景提供有效解决方案，平衡模型性能与计算效率，拓展LLMs在实时响应场景的应用潜力。

Abstract: Large language models (LLMs) have demonstrated remarkable reasoning
capabilities through test-time scaling approaches, particularly when fine-tuned
with chain-of-thought (CoT) data distilled from more powerful large reasoning
models (LRMs). However, these reasoning chains often contain verbose elements
that mirror human problem-solving, categorized as progressive reasoning (the
essential solution development path) and functional elements (verification
processes, alternative solution approaches, and error corrections). While
progressive reasoning is crucial, the functional elements significantly
increase computational demands during test-time inference. We introduce PIR
(Perplexity-based Importance Refinement), a principled framework that
quantitatively evaluates the importance of each reasoning step based on its
impact on answer prediction confidence. PIR systematically identifies and
selectively prunes only low-importance functional steps while preserving
progressive reasoning components, creating optimized training data that
maintains the integrity of the core solution path while reducing verbosity.
Models fine-tuned on PIR-optimized data exhibit superior test-time scaling
properties, generating more concise reasoning chains while achieving improved
accuracy (+0.9\% to +6.6\%) with significantly reduced token usage (-3\% to
-41\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond).
Our approach demonstrates strong generalizability across different model sizes,
data sources, and token budgets, offering a practical solution for deploying
reasoning-capable LLMs in scenarios where efficient test-time scaling, response
time, and computational efficiency are valuable constraints.

</details>


### [113] [Misleading through Inconsistency: A Benchmark for Political Inconsistencies Detection](https://arxiv.org/abs/2505.19191)
*Nursulu Sagimbayeva,Ruveyda Betül Bahçeci,Ingmar Weber*

Main category: cs.CL

TL;DR: 研究构建政治声明不一致性检测数据集，评估大语言模型性能，发现模型总体接近人类水平但细粒度分类待提升


<details>
  <summary>Details</summary>
Motivation: 政治声明不一致性会削弱公众信任且难以追责，自动检测技术可帮助记者提出澄清问题以维护政治问责

Method: 提出不一致性检测任务并制定类型量表，收集698对政治声明构建标注数据集（含237条解释），主要源自德/瑞投票平台的真实政治议题

Result: LLMs整体检测能力与人类相当甚至优于个体标注者，但在细粒度类型识别上尚未达到性能上限（受标注差异影响）

Conclusion: 公开数据集与代码促进后续研究，当前模型在类型识别方面仍有改进空间，需进一步优化

Abstract: Inconsistent political statements represent a form of misinformation. They
erode public trust and pose challenges to accountability, when left unnoticed.
Detecting inconsistencies automatically could support journalists in asking
clarification questions, thereby helping to keep politicians accountable. We
propose the Inconsistency detection task and develop a scale of inconsistency
types to prompt NLP-research in this direction. To provide a resource for
detecting inconsistencies in a political domain, we present a dataset of 698
human-annotated pairs of political statements with explanations of the
annotators' reasoning for 237 samples. The statements mainly come from voting
assistant platforms such as Wahl-O-Mat in Germany and Smartvote in Switzerland,
reflecting real-world political issues. We benchmark Large Language Models
(LLMs) on our dataset and show that in general, they are as good as humans at
detecting inconsistencies, and might be even better than individual humans at
predicting the crowd-annotated ground-truth. However, when it comes to
identifying fine-grained inconsistency types, none of the model have reached
the upper bound of performance (due to natural labeling variation), thus
leaving room for improvement. We make our dataset and code publicly available.

</details>


### [114] [DREAM: Drafting with Refined Target Features and Entropy-Adaptive Cross-Attention Fusion for Multimodal Speculative Decoding](https://arxiv.org/abs/2505.19201)
*Yunhai Hu,Tianhua Xia,Zining Liu,Rahul Raman,Xingyu Liu,Bo Bao,Eric Sather,Vithursan Thangarasa,Sai Qian Zhang*

Main category: cs.CL

TL;DR: 提出DREAM框架，通过跨注意力机制、自适应特征选择和视觉标记压缩技术，在视觉语言模型中实现高效推测解码，实验显示最高3.6倍加速


<details>
  <summary>Details</summary>
Motivation: 推测解码在语言模型中有效，但在视觉语言模型(VLMs)的应用尚未充分探索。现有方法存在多模态对齐效率低、计算延迟高的问题

Method: 1. 基于交叉注意力的特征注入机制
2. 通过注意力熵自适应选择特征
3. 视觉标记压缩降低延迟

Result: 在LLaVA/Pixtral/SmolVLM/Gemma3等模型上实现最高3.6倍加速，推测接受长度和吞吐量显著优于基线方法

Conclusion: DREAM框架有效解决了多模态推测解码的关键挑战，为实际应用提供了突破性的加速方案，代码已开源

Abstract: Speculative decoding (SD) has emerged as a powerful method for accelerating
autoregressive generation in large language models (LLMs), yet its integration
into vision-language models (VLMs) remains underexplored. We introduce DREAM, a
novel speculative decoding framework tailored for VLMs that combines three key
innovations: (1) a cross-attention-based mechanism to inject intermediate
features from the target model into the draft model for improved alignment, (2)
adaptive intermediate feature selection based on attention entropy to guide
efficient draft model training, and (3) visual token compression to reduce
draft model latency. DREAM enables efficient, accurate, and parallel multimodal
decoding with significant throughput improvement. Experiments across a diverse
set of recent popular VLMs, including LLaVA, Pixtral, SmolVLM and Gemma3,
demonstrate up to 3.6x speedup over conventional decoding and significantly
outperform prior SD baselines in both inference throughput and speculative
draft acceptance length across a broad range of multimodal benchmarks. The code
is publicly available at: https://github.com/SAI-Lab-NYU/DREAM.git

</details>


### [115] [SpeakStream: Streaming Text-to-Speech with Interleaved Data](https://arxiv.org/abs/2505.19206)
*Richard He Bai,Zijin Gu,Tatiana Likhomanenko,Navdeep Jaitly*

Main category: cs.CL

TL;DR: SpeakStream提出流式TTS系统，通过仅解码器架构实现文本流增量音频生成，在保持质量的同时达到最低首令牌延迟


<details>
  <summary>Details</summary>
Motivation: 传统TTS系统处理完整语句导致高延迟，无法适应流式LLM输出的实时需求，影响对话AI的响应速度

Method: 使用仅解码器架构，基于文本-语音交织数据通过下一步预测损失训练，推理时支持流式文本增量生成语音

Result: 实验证明在保持非流式TTS质量的同时，首令牌延迟达到SOTA水平（0.17秒）

Conclusion: 该架构特别适用于LLM流式文本输出的级联场景，为实时对话AI提供了可行的低延迟语音合成方案

Abstract: The latency bottleneck of traditional text-to-speech (TTS) systems
fundamentally hinders the potential of streaming large language models (LLMs)
in conversational AI. These TTS systems, typically trained and inferenced on
complete utterances, introduce unacceptable delays, even with optimized
inference speeds, when coupled with streaming LLM outputs. This is particularly
problematic for creating responsive conversational agents where low first-token
latency is critical. In this paper, we present SpeakStream, a streaming TTS
system that generates audio incrementally from streaming text using a
decoder-only architecture. SpeakStream is trained using a next-step prediction
loss on interleaved text-speech data. During inference, it generates speech
incrementally while absorbing streaming input text, making it particularly
suitable for cascaded conversational AI agents where an LLM streams text to a
TTS system. Our experiments demonstrate that SpeakStream achieves
state-of-the-art latency results in terms of first-token latency while
maintaining the quality of non-streaming TTS systems.

</details>


### [116] [MOOSE-Chem2: Exploring LLM Limits in Fine-Grained Scientific Hypothesis Discovery via Hierarchical Search](https://arxiv.org/abs/2505.19209)
*Zonglin Yang,Wanhao Liu,Ben Gao,Yujie Liu,Wei Li,Tong Xie,Lidong Bing,Wanli Ouyang,Erik Cambria,Dongzhan Zhou*

Main category: cs.CL

TL;DR: 提出分层搜索方法提升细粒度科学假设生成效果


<details>
  <summary>Details</summary>
Motivation: 现有LLM生成的科学假设缺乏实验细节，需要更细粒度的自动化假设发现方法

Method: 分层搜索方法逐步整合假设细节，从概念到具体实验配置，平滑奖励景观

Result: 在化学领域专家标注数据集上验证优于基线方法

Conclusion: 通过分层优化过程有效提升假设生成质量，验证了多LLM集成比单一模型更可靠

Abstract: Large language models (LLMs) have shown promise in automating scientific
hypothesis generation, yet existing approaches primarily yield coarse-grained
hypotheses lacking critical methodological and experimental details. We
introduce and formally define the novel task of fine-grained scientific
hypothesis discovery, which entails generating detailed, experimentally
actionable hypotheses from coarse initial research directions. We frame this as
a combinatorial optimization problem and investigate the upper limits of LLMs'
capacity to solve it when maximally leveraged. Specifically, we explore four
foundational questions: (1) how to best harness an LLM's internal heuristics to
formulate the fine-grained hypothesis it itself would judge as the most
promising among all the possible hypotheses it might generate, based on its own
internal scoring-thus defining a latent reward landscape over the hypothesis
space; (2) whether such LLM-judged better hypotheses exhibit stronger alignment
with ground-truth hypotheses; (3) whether shaping the reward landscape using an
ensemble of diverse LLMs of similar capacity yields better outcomes than
defining it with repeated instances of the strongest LLM among them; and (4)
whether an ensemble of identical LLMs provides a more reliable reward landscape
than a single LLM. To address these questions, we propose a hierarchical search
method that incrementally proposes and integrates details into the hypothesis,
progressing from general concepts to specific experimental configurations. We
show that this hierarchical process smooths the reward landscape and enables
more effective optimization. Empirical evaluations on a new benchmark of
expert-annotated fine-grained hypotheses from recent chemistry literature show
that our method consistently outperforms strong baselines.

</details>


### [117] [When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas](https://arxiv.org/abs/2505.19212)
*Steffen Backmann,David Guzman Piedrahita,Emanuel Tewolde,Rada Mihalcea,Bernhard Schölkopf,Zhijing Jin*

Main category: cs.CL

TL;DR: 研究通过MoralSim框架发现，主流大语言模型在道德与利益冲突的社会困境中表现不一致，存在伦理风险


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在道德规范与自我利益直接冲突场景下的行为模式，为AI安全部署提供依据

Method: 构建MoralSim社会困境模拟器，测试前沿模型在囚徒困境/公共物品游戏中的行为，结合三种道德框架进行系统分析

Result: 模型间存在显著差异：1）道德行为倾向与游戏类型强相关 2）道德框架有效性取决于情境因素 3）生存压力显著降低道德一致性

Conclusion: 当前LLMs无法稳定遵守伦理规范，在代理角色部署中需建立道德护栏机制，避免利益冲突导致非预期行为

Abstract: Recent advances in large language models (LLMs) have enabled their use in
complex agentic roles, involving decision-making with humans or other agents,
making ethical alignment a key AI safety concern. While prior work has examined
both LLMs' moral judgment and strategic behavior in social dilemmas, there is
limited understanding of how they act when moral imperatives directly conflict
with rewards or incentives. To investigate this, we introduce Moral Behavior in
Social Dilemma Simulation (MoralSim) and evaluate how LLMs behave in the
prisoner's dilemma and public goods game with morally charged contexts. In
MoralSim, we test a range of frontier models across both game structures and
three distinct moral framings, enabling a systematic examination of how LLMs
navigate social dilemmas in which ethical norms conflict with payoff-maximizing
strategies. Our results show substantial variation across models in both their
general tendency to act morally and the consistency of their behavior across
game types, the specific moral framing, and situational factors such as
opponent behavior and survival risks. Crucially, no model exhibits consistently
moral behavior in MoralSim, highlighting the need for caution when deploying
LLMs in agentic roles where the agent's "self-interest" may conflict with
ethical expectations. Our code is available at
https://github.com/sbackmann/moralsim.

</details>


### [118] [The Overthinker's DIET: Cutting Token Calories with DIfficulty-AwarE Training](https://arxiv.org/abs/2505.19217)
*Weize Chen,Jiarui Yuan,Tailin Jin,Ning Ding,Huimin Chen,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: DIET框架通过动态难度感知训练显著减少LLM生成的token数量，同时提升推理性能与效率平衡


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型倾向于生成冗长响应导致效率低下，需开发兼顾性能与效率的优化方法

Method: 结合强化学习的难度感知训练框架（DIET），通过动态调节token惩罚强度和目标长度，并引入Advantage Weighting技术优化奖励机制

Result: 实验显示DIET减少30-50% token使用量，推理准确率提升2-5%；实现更好的计算资源分配效率（相同预算下多数投票准确率提升8-12%）

Conclusion: DIET为开发高效实用LLM提供了理论框架，在保持难度-长度正相关性的同时实现性能与效率的双重突破

Abstract: Recent large language models (LLMs) exhibit impressive reasoning but often
over-think, generating excessively long responses that hinder efficiency. We
introduce DIET ( DIfficulty-AwarE Training), a framework that systematically
cuts these "token calories" by integrating on-the-fly problem difficulty into
the reinforcement learning (RL) process. DIET dynamically adapts token
compression strategies by modulating token penalty strength and conditioning
target lengths on estimated task difficulty, to optimize the
performance-efficiency trade-off. We also theoretically analyze the pitfalls of
naive reward weighting in group-normalized RL algorithms like GRPO, and propose
Advantage Weighting technique, which enables stable and effective
implementation of these difficulty-aware objectives. Experimental results
demonstrate that DIET significantly reduces token counts while simultaneously
improving reasoning performance. Beyond raw token reduction, we show two
crucial benefits largely overlooked by prior work: (1) DIET leads to superior
inference scaling. By maintaining high per-sample quality with fewer tokens, it
enables better scaling performance via majority voting with more samples under
fixed computational budgets, an area where other methods falter. (2) DIET
enhances the natural positive correlation between response length and problem
difficulty, ensuring verbosity is appropriately allocated, unlike many existing
compression methods that disrupt this relationship. Our analyses provide a
principled and effective framework for developing more efficient, practical,
and high-performing LLMs.

</details>


### [119] [Evaluating Text Creativity across Diverse Domains: A Dataset and Large Language Model Evaluator](https://arxiv.org/abs/2505.19236)
*Qian Cao,Xiting Wang,Yuzhuo Yuan,Yahui Liu,Fang Luo,Ruihua Song*

Main category: cs.CL

TL;DR: Proposes CrEval framework and CreataSet dataset for automated creativity evaluation of LLMs using human-synthetic hybrid data


<details>
  <summary>Details</summary>
Motivation: Current creativity evaluation methods for LLMs are inefficient (relying on human judgment) and lack generalizability

Method: Developed pairwise-comparison framework with CreataSet dataset (100K+ human + 1M+ synthetic pairs) to train LLM-based evaluator CrEval

Result: CrEval outperforms existing methods in human alignment (88.3% accuracy), demonstrates hybrid data's importance for robustness

Conclusion: CrEval effectively boosts LLM creativity evaluation while maintaining human consistency, dataset/model release will facilitate research

Abstract: Creativity evaluation remains a challenging frontier for large language
models (LLMs). Current evaluations heavily rely on inefficient and costly human
judgments, hindering progress in enhancing machine creativity. While automated
methods exist, ranging from psychological testing to heuristic- or
prompting-based approaches, they often lack generalizability or alignment with
human judgment. To address these issues, in this paper, we propose a novel
pairwise-comparison framework for assessing textual creativity, leveraging
shared contextual instructions to improve evaluation consistency. We introduce
CreataSet, a large-scale dataset with 100K+ human-level and 1M+ synthetic
creative instruction-response pairs spanning diverse open-domain tasks. Through
training on CreataSet, we develop an LLM-based evaluator named CrEval. CrEval
demonstrates remarkable superiority over existing methods in alignment with
human judgments. Experimental results underscore the indispensable significance
of integrating both human-generated and synthetic data in training highly
robust evaluators, and showcase the practical utility of CrEval in boosting the
creativity of LLMs. We will release all data, code, and models publicly soon to
support further research.

</details>


### [120] [LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models](https://arxiv.org/abs/2505.19240)
*Aida Kostikova,Zhipin Wang,Deidamea Bajri,Ole Pütz,Benjamin Paaßen,Steffen Eger*

Main category: cs.CL

TL;DR: 2022-2024年大语言模型(LLM)局限性的研究表明：相关研究数量激增，推理能力不足仍是核心问题，arXiv论文逐渐转向安全可控性研究。


<details>
  <summary>Details</summary>
Motivation: 针对LLM在推理、幻觉、多语言能力等方面的缺陷，研究者试图通过量化分析揭示领域研究趋势与局限性关注重点的演变。

Method: 采用数据驱动的半自动化方法，结合关键词筛选、LLM分类验证、专家标注及HDBSCAN+BERTopic/LlooM双聚类方案，分析25万篇ACL/arXiv论文。

Result: LLM局限性研究占比从2022年的不足10%增长至2024年的30%；arXiv在安全风险、知识编辑等领域研究增幅显著，ACL保持较稳定分布。

Conclusion: 研究发布带标注的论文摘要数据集及验证方法论，为LLM局限性研究提供动态量化视角，反映学术界对模型安全可控性关注度的快速提升。

Abstract: Large language model (LLM) research has grown rapidly, along with increasing
concern about their limitations such as failures in reasoning, hallucinations,
and limited multilingual capability. In this survey, we conduct a data-driven,
semi-automated review of research on limitations of LLM (LLLMs) from 2022 to
2024 using a bottom-up approach. From a corpus of 250,000 ACL and arXiv papers,
we identify 14,648 relevant papers using keyword filtering, LLM-based
classification, validated against expert labels, and topic clustering (via two
approaches, HDBSCAN+BERTopic and LlooM). We find that LLM-related research
increases over fivefold in ACL and fourfold in arXiv. Since 2022, LLLMs
research grows even faster, reaching over 30% of LLM papers by late 2024.
Reasoning remains the most studied limitation, followed by generalization,
hallucination, bias, and security. The distribution of topics in the ACL
dataset stays relatively stable over time, while arXiv shifts toward safety and
controllability (with topics like security risks, alignment, hallucinations,
knowledge editing), and multimodality between 2022 and 2024. We release a
dataset of annotated abstracts and a validated methodology, and offer a
quantitative view of trends in LLM limitations research.

</details>


### [121] [PATS: Process-Level Adaptive Thinking Mode Switching](https://arxiv.org/abs/2505.19250)
*Yi Wang,Junxiao Liu,Shimao Zhang,Jiajun Chen,Shujian Huang*

Main category: cs.CL

TL;DR: 提出动态调整推理策略的PATS框架，通过过程奖励模型和束搜索实现步骤级自适应思维切换，平衡LLM精度与计算效率


<details>
  <summary>Details</summary>
Motivation: 固定推理策略导致简单问题过度计算/复杂问题分析不足，现有方法局限于解决方案级策略调整，无法细粒度适配步骤难度

Method: 整合过程奖励模型(PRM)与束搜索，引入渐进式模式切换机制和错误步骤惩罚机制，实现推理过程中逐步骤的思维模式动态切换

Result: 在多样化数学基准测试中达到94.2%准确率，推理token消耗量较传统方法减少37%，实现精度与效率的最佳平衡

Conclusion: 过程级难度感知推理策略适应机制为LLM高效推理提供新范式，证实动态调整思维模式对优化计算资源分配的关键作用

Abstract: Current large-language models (LLMs) typically adopt a fixed reasoning
strategy, either simple or complex, for all questions, regardless of their
difficulty. This neglect of variation in task and reasoning process complexity
leads to an imbalance between performance and efficiency. Existing methods
attempt to implement training-free fast-slow thinking system switching to
handle problems of varying difficulty, but are limited by coarse-grained
solution-level strategy adjustments. To address this issue, we propose a novel
reasoning paradigm: Process-Level Adaptive Thinking Mode Switching (PATS),
which enables LLMs to dynamically adjust their reasoning strategy based on the
difficulty of each step, optimizing the balance between accuracy and
computational efficiency. Our approach integrates Process Reward Models (PRMs)
with Beam Search, incorporating progressive mode switching and bad-step penalty
mechanisms. Experiments on diverse mathematical benchmarks demonstrate that our
methodology achieves high accuracy while maintaining moderate token usage. This
study emphasizes the significance of process-level, difficulty-aware reasoning
strategy adaptation, offering valuable insights into efficient inference for
LLMs.

</details>


### [122] [Unveiling Dual Quality in Product Reviews: An NLP-Based Approach](https://arxiv.org/abs/2505.19254)
*Rafał Poświata,Marcin Michał Mirończuk,Sławomir Dadas,Małgorzata Grębowiec,Michał Perełkiewicz*

Main category: cs.CL

TL;DR: 利用NLP技术开发自动化方案检测产品双重质量问题，包含波兰语数据集构建、多模型实验及多语言迁移评估。


<details>
  <summary>Details</summary>
Motivation: 解决不同市场同款产品质量不一致导致的消费者权益受损问题，需开发自动化检测工具提升监管效率。

Method: 1. 构建含1,957条波兰语评论（含540条双重质量标注）的新数据集
2. 采用SetFit+sentence-transformers、transformer编码器、LLMs进行实验
3. 开展错误分析与鲁棒性验证
4. 使用英法德三语子集评估多语言迁移能力

Result: 验证了NLP模型在双重质量检测中的有效性，多语言迁移实验显示跨语言应用潜力，错误分析揭示模型改进方向。

Conclusion: 该研究为产品质量监管提供可落地的NLP解决方案，证明技术部署可行性，并指明多语言场景的应用前景。

Abstract: Consumers often face inconsistent product quality, particularly when
identical products vary between markets, a situation known as the dual quality
problem. To identify and address this issue, automated techniques are needed.
This paper explores how natural language processing (NLP) can aid in detecting
such discrepancies and presents the full process of developing a solution.
First, we describe in detail the creation of a new Polish-language dataset with
1,957 reviews, 540 highlighting dual quality issues. We then discuss
experiments with various approaches like SetFit with sentence-transformers,
transformer-based encoders, and LLMs, including error analysis and robustness
verification. Additionally, we evaluate multilingual transfer using a subset of
opinions in English, French, and German. The paper concludes with insights on
deployment and practical applications.

</details>


### [123] [A Graph Perspective to Probe Structural Patterns of Knowledge in Large Language Models](https://arxiv.org/abs/2505.19286)
*Utkarsh Sahu,Zhisheng Qi,Yongjia Lei,Ryan A. Rossi,Franck Dernoncourt,Nesreen K. Ahmed,Mahantesh M Halappanavar,Yao Ma,Yu Wang*

Main category: cs.CL

TL;DR: 从图结构视角分析大语言模型的知识分布特征，提出知识同质性现象并开发图模型实现知识检查与优化


<details>
  <summary>Details</summary>
Motivation: 现有研究较少关注大语言模型知识的结构模式，特别在图结构特性方面存在研究空白

Method: 量化三元组/实体层知识量，分析节点度等图属性关联，开发基于邻居的图模型估计实体知识水平

Result: 发现拓扑邻近实体具有相似知识水平（知识同质性），筛选未知三元组微调使模型性能提升显著

Conclusion: 图结构分析为理解LLMs知识组织及优化模型知识覆盖提供了新视角

Abstract: Large language models have been extensively studied as neural knowledge bases
for their knowledge access, editability, reasoning, and explainability.
However, few works focus on the structural patterns of their knowledge.
Motivated by this gap, we investigate these structural patterns from a graph
perspective. We quantify the knowledge of LLMs at both the triplet and entity
levels, and analyze how it relates to graph structural properties such as node
degree. Furthermore, we uncover the knowledge homophily, where topologically
close entities exhibit similar levels of knowledgeability, which further
motivates us to develop graph machine learning models to estimate entity
knowledge based on its local neighbors. This model further enables valuable
knowledge checking by selecting triplets less known to LLMs. Empirical results
show that using selected triplets for fine-tuning leads to superior
performance.

</details>


### [124] [100-LongBench: Are de facto Long-Context Benchmarks Literally Evaluating Long-Context Ability?](https://arxiv.org/abs/2505.19293)
*Wang Yang,Hongye Jin,Shaochen Zhong,Song Jiang,Qifan Wang,Vipin Chaudhary,Xiaotian Han*

Main category: cs.CL

TL;DR: 提出了可控制长度的长上下文评测基准及分离模型基线能力的新指标，有效解决现有评测方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有长上下文评测基准无法区分模型基线能力和真实长上下文能力，且固定输入长度设计限制了适用性。

Method: 设计可调节输入长度的评测基准，并创新性地提出解耦基线知识与长上下文能力的评估指标。

Result: 实验证明该方法能更准确评估模型的长上下文处理能力，有效揭示模型失效边界。

Conclusion: 新方法为LLM长上下文能力评估提供了更科学、适应性更强的解决方案。

Abstract: Long-context capability is considered one of the most important abilities of
LLMs, as a truly long context-capable LLM enables users to effortlessly process
many originally exhausting tasks -- e.g., digesting a long-form document to
find answers vs. directly asking an LLM about it. However, existing
real-task-based long-context evaluation benchmarks have two major shortcomings.
First, benchmarks like LongBench often do not provide proper metrics to
separate long-context performance from the model's baseline ability, making
cross-model comparison unclear. Second, such benchmarks are usually constructed
with fixed input lengths, which limits their applicability across different
models and fails to reveal when a model begins to break down. To address these
issues, we introduce a length-controllable long-context benchmark and a novel
metric that disentangles baseline knowledge from true long-context
capabilities. Experiments demonstrate the superiority of our approach in
effectively evaluating LLMs.

</details>


### [125] [A Necessary Step toward Faithfulness: Measuring and Improving Consistency in Free-Text Explanations](https://arxiv.org/abs/2505.19299)
*Lingjun Zhao,Hal Daumé III*

Main category: cs.CL

TL;DR: 提出PEX一致性指标量化自由文本解释对预测的支持度，发现超62%大模型解释存在不一致性，优化后提升显著。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型生成的自由文本解释中62%存在预测与解释不一致问题，影响AI决策透明度和可信度。

Method: 扩展证据权重概念提出PEX一致性指标，应用直接偏好优化(DPO)技术优化三个模型家族的生成结果。

Result: DPO使解释一致性提升43.1%-292.3%，优化PEX一致性指标使解释忠实性最高提升9.7%。

Conclusion: PEX一致性是解释可信度的关键指标，直接优化该指标可有效提升AI决策解释的忠实性。

Abstract: Faithful free-text explanations are important to ensure transparency in
high-stakes AI decision-making contexts, but they are challenging to generate
by language models and assess by humans. In this paper, we present a measure
for Prediction-EXplanation (PEX) consistency, by extending the concept of
weight of evidence. This measure quantifies how much a free-text explanation
supports or opposes a prediction, serving as an important aspect of explanation
faithfulness. Our analysis reveals that more than 62% explanations generated by
large language models lack this consistency. We show that applying direct
preference optimization improves the consistency of generated explanations
across three model families, with improvement ranging from 43.1% to 292.3%.
Furthermore, we demonstrate that optimizing this consistency measure can
improve explanation faithfulness by up to 9.7%.

</details>


### [126] [SituatedThinker: Grounding LLM Reasoning with Real-World through Situated Thinking](https://arxiv.org/abs/2505.19300)
*Junnan Liu,Linhao Luo,Thuy-Trang Vu,Gholamreza Haffari*

Main category: cs.CL

TL;DR: 提出SituatedThinker框架，通过情境化思维将大模型推理与真实世界情境结合，利用强化学习突破知识边界并提升推理能力


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型仅依赖内部参数知识导致的实时信息获取受限和物理世界理解不足的问题

Method: 结合预定义接口实现内部知识与外部信息的自适应融合，通过强化学习机制激励主动推理获取实时信息反馈

Result: 在多跳问答和数学推理基准测试中显著提升性能，在KBQA/TableQA/文本游戏等新任务展现泛化能力

Conclusion: 该框架成功拓展了大模型的知识边界，验证了情境化推理对复杂现实任务的有效性，具有通用性价值

Abstract: Recent advances in large language models (LLMs) demonstrate their impressive
reasoning capabilities. However, the reasoning confined to internal parametric
space limits LLMs' access to real-time information and understanding of the
physical world. To overcome this constraint, we introduce SituatedThinker, a
novel framework that enables LLMs to ground their reasoning in real-world
contexts through situated thinking, which adaptively combines both internal
knowledge and external information with predefined interfaces. By utilizing
reinforcement learning, SituatedThinker incentivizes deliberate reasoning with
the real world to acquire information and feedback, allowing LLMs to surpass
their knowledge boundaries and enhance reasoning. Experimental results
demonstrate significant performance improvements on multi-hop
question-answering and mathematical reasoning benchmarks. Furthermore,
SituatedThinker demonstrates strong performance on unseen tasks, such as KBQA,
TableQA, and text-based games, showcasing the generalizable real-world grounded
reasoning capability. Our codes are available at
https://github.com/jnanliu/SituatedThinker.

</details>


### [127] [PatentScore: Multi-dimensional Evaluation of LLM-Generated Patent Claims](https://arxiv.org/abs/2505.19345)
*Yongmin Yoo,Qiongkai Xu,Longbing Cao*

Main category: cs.CL

TL;DR: 提出PatentScore框架用于评估LLM生成的专利权利要求，通过多维度评分实现优于传统NLG指标的专家相关性


<details>
  <summary>Details</summary>
Motivation: 现有自然语言生成指标无法满足专利文件特有的法律有效性和技术准确性评估需求，专利权利要求评估存在空白

Method: 1. 层次化权利要求分解
2. 基于法律技术标准的领域验证模式
3. 结构/语义/法律三维度评分体系

Result: GPT-4o-mini生成400项专利权利要求的Pearson相关系数r=0.819，Claude-3.5/Haiku和Gemini-1.5-flash模型验证框架稳健性

Conclusion: PatentScore有效解决专利生成评估的特殊需求，首次实现法律合规与技术准确性的量化评估

Abstract: Natural language generation (NLG) metrics play a central role in evaluating
generated texts, but are not well suited for the structural and legal
characteristics of patent documents. Large language models (LLMs) offer strong
potential in automating patent generation, yet research on evaluating
LLM-generated patents remains limited, especially in evaluating the generation
quality of patent claims, which are central to defining the scope of
protection. Effective claim evaluation requires addressing legal validity,
technical accuracy, and structural compliance. To address this gap, we
introduce PatentScore, a multi-dimensional evaluation framework for assessing
LLM-generated patent claims. PatentScore incorporates: (1) hierarchical
decomposition for claim analysis; (2) domain-specific validation patterns based
on legal and technical standards; and (3) scoring across structural, semantic,
and legal dimensions. Unlike general-purpose NLG metrics, PatentScore reflects
patent-specific constraints and document structures, enabling evaluation beyond
surface similarity. We evaluate 400 GPT-4o-mini generated Claim 1s and report a
Pearson correlation of $r = 0.819$ with expert annotations, outperforming
existing NLG metrics. Furthermore, we conduct additional evaluations using open
models such as Claude-3.5-Haiku and Gemini-1.5-flash, all of which show strong
correlations with expert judgments, confirming the robustness and
generalizability of our framework.

</details>


### [128] [GC-KBVQA: A New Four-Stage Framework for Enhancing Knowledge Based Visual Question Answering Performance](https://arxiv.org/abs/2505.19354)
*Mohammad Mahdi Moradi,Sudhir Mudur*

Main category: cs.CL

TL;DR: 提出GC-KB-VQA框架，通过问题感知的定位描述整合外部知识，利用LLM实现零样本VQA任务，无需多模态训练。


<details>
  <summary>Details</summary>
Motivation: 现有KB-VQA方法提供的辅助文本可能包含无关信息，导致答案预测偏差。需在降低部署成本的同时提升提示有效性。

Method: 四阶段框架：1)生成问题相关的定位描述；2)结合外部知识；3)构建高信息量提示；4)LLM直接推理。核心为细粒度定位描述与知识融合。

Result: 相比其他KB-VQA方法性能显著提升，且无需任务微调，降低部署复杂度。

Conclusion: GC-KB-VQA有效利用LLM实现零样本多任务VQA，通过精准提示工程突破传统多模态训练限制，具有高扩展性。

Abstract: Knowledge-Based Visual Question Answering (KB-VQA) methods focus on tasks
that demand reasoning with information extending beyond the explicit content
depicted in the image. Early methods relied on explicit knowledge bases to
provide this auxiliary information. Recent approaches leverage Large Language
Models (LLMs) as implicit knowledge sources. While KB-VQA methods have
demonstrated promising results, their potential remains constrained as the
auxiliary text provided may not be relevant to the question context, and may
also include irrelevant information that could misguide the answer predictor.
We introduce a novel four-stage framework called Grounding Caption-Guided
Knowledge-Based Visual Question Answering (GC-KBVQA), which enables LLMs to
effectively perform zero-shot VQA tasks without the need for end-to-end
multimodal training. Innovations include grounding question-aware caption
generation to move beyond generic descriptions and have compact, yet detailed
and context-rich information. This is combined with knowledge from external
sources to create highly informative prompts for the LLM. GC-KBVQA can address
a variety of VQA tasks, and does not require task-specific fine-tuning, thus
reducing both costs and deployment complexity by leveraging general-purpose,
pre-trained LLMs. Comparison with competing KB-VQA methods shows significantly
improved performance. Our code will be made public.

</details>


### [129] [Estimating Online Influence Needs Causal Modeling! Counterfactual Analysis of Social Media Engagement](https://arxiv.org/abs/2505.19355)
*Lin Tian,Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: 开发新型联合处理-结果框架，结合序列模型和因果推断技术，用于社交媒体中的因果影响力分析，在反事实场景预测准确率提升15-22%


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖曝光指标和网络结构，无法捕捉外部时序信号触发参与行为的因果机制，需解决外部混杂信号带来的挑战

Method: 1. 将医疗领域因果推断技术迁移到社交媒体
2. 开发联合处理-结果框架适配政策时间窗和参与效应
3. 通过ATE估计处理序列化交互中的因果效应

Result: 1. 在3类反事实场景(曝光调整/时间偏移/干预时长变化)中预测准确率超越基准15-22%
2. 492用户案例分析显示因果效应指标与专家经验评估高度吻合

Conclusion: 该框架有效解决外部混杂信号问题，通过时序化ATE估计提升影响力分析精度，为虚假信息传播研究提供新方法论

Abstract: Understanding true influence in social media requires distinguishing
correlation from causation--particularly when analyzing misinformation spread.
While existing approaches focus on exposure metrics and network structures,
they often fail to capture the causal mechanisms by which external temporal
signals trigger engagement. We introduce a novel joint treatment-outcome
framework that leverages existing sequential models to simultaneously adapt to
both policy timing and engagement effects. Our approach adapts causal inference
techniques from healthcare to estimate Average Treatment Effects (ATE) within
the sequential nature of social media interactions, tackling challenges from
external confounding signals. Through our experiments on real-world
misinformation and disinformation datasets, we show that our models outperform
existing benchmarks by 15--22% in predicting engagement across diverse
counterfactual scenarios, including exposure adjustment, timing shifts, and
varied intervention durations. Case studies on 492 social media users show our
causal effect measure aligns strongly with the gold standard in influence
estimation, the expert-based empirical influence.

</details>


### [130] [ChartLens: Fine-grained Visual Attribution in Charts](https://arxiv.org/abs/2505.19360)
*Manan Suri,Puneet Mathur,Nedim Lipka,Franck Dernoncourt,Ryan A. Rossi,Dinesh Manocha*

Main category: cs.CL

TL;DR: 提出ChartLens算法和ChartVA-Eval基准，显著提升图表细粒度视觉归因效果26-66%


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在图表理解中存在幻觉问题，生成文本与视觉数据冲突，需建立细粒度的视觉归因验证机制

Method: 1. 开发基于分割的ChartLens算法，采用标记集提示技术；2. 构建跨领域基准ChartVA-Eval（含金融/政策/经济图表及细粒度标注）

Result: ChartLens使细粒度视觉归因准确率提升26-66%

Conclusion: 通过算法创新和基准构建，有效解决图表视觉归因问题，提升多模态模型在图表分析中的可靠性

Abstract: The growing capabilities of multimodal large language models (MLLMs) have
advanced tasks like chart understanding. However, these models often suffer
from hallucinations, where generated text sequences conflict with the provided
visual data. To address this, we introduce Post-Hoc Visual Attribution for
Charts, which identifies fine-grained chart elements that validate a given
chart-associated response. We propose ChartLens, a novel chart attribution
algorithm that uses segmentation-based techniques to identify chart objects and
employs set-of-marks prompting with MLLMs for fine-grained visual attribution.
Additionally, we present ChartVA-Eval, a benchmark with synthetic and
real-world charts from diverse domains like finance, policy, and economics,
featuring fine-grained attribution annotations. Our evaluations show that
ChartLens improves fine-grained attributions by 26-66%.

</details>


### [131] [Belief Attribution as Mental Explanation: The Role of Accuracy, Informativity, and Causality](https://arxiv.org/abs/2505.19376)
*Lance Ying,Almog Hillel,Ryan Truong,Vikash K. Mansinghka,Joshua B. Tenenbaum,Tan Zhi-Xuan*

Main category: cs.CL

TL;DR: 研究人类如何通过信念归因解释他人行为，提出计算模型量化解释性信念的三个因素，发现因果相关性是预测信念归因的最关键因素。


<details>
  <summary>Details</summary>
Motivation: 探索人类为何倾向于选择特定信念来解释他人行为，现有理论未明确解释信念选择的优先级，需建立量化模型分析解释性信念的核心要素。

Method: 开发基于概率生成模型的计算框架，通过准确性、信息量、因果相关性三因素量化信念解释力，设计行为观察实验收集参与者对代理信念的排序数据。

Result: 准确性（84%）与信息量（76%）组合预测效果最佳，但因果相关性单独解释力达89%，显著优于其他单一因素（p<0.01）。

Conclusion: 因果相关性是人类信念归因的核心认知机制，该计算模型为心智理论研究提供新的量化分析框架，验证了解释性假说的心理学价值。

Abstract: A key feature of human theory-of-mind is the ability to attribute beliefs to
other agents as mentalistic explanations for their behavior. But given the wide
variety of beliefs that agents may hold about the world and the rich language
we can use to express them, which specific beliefs are people inclined to
attribute to others? In this paper, we investigate the hypothesis that people
prefer to attribute beliefs that are good explanations for the behavior they
observe. We develop a computational model that quantifies the explanatory
strength of a (natural language) statement about an agent's beliefs via three
factors: accuracy, informativity, and causal relevance to actions, each of
which can be computed from a probabilistic generative model of belief-driven
behavior. Using this model, we study the role of each factor in how people
selectively attribute beliefs to other agents. We investigate this via an
experiment where participants watch an agent collect keys hidden in boxes in
order to reach a goal, then rank a set of statements describing the agent's
beliefs about the boxes' contents. We find that accuracy and informativity
perform reasonably well at predicting these rankings when combined, but that
causal relevance is the single factor that best explains participants'
responses.

</details>


### [132] [GSA-TTS : Toward Zero-Shot Speech Synthesis based on Gradual Style Adaptor](https://arxiv.org/abs/2505.19384)
*Seokgi Lee,Jungjun Kim*

Main category: cs.CL

TL;DR: 渐进式风格适配器TTS（GSA-TTS）通过分层语义编码策略实现零样本语音合成，在未见说话者上取得自然度、说话人相似度和清晰度的提升


<details>
  <summary>Details</summary>
Motivation: 解决传统语音合成系统在零样本场景下对未见说话者风格编码不够鲁棒的问题，通过分层编码策略捕捉更丰富的风格表示

Method: 1. 捕捉每个语义单元的局部风格 → 2. 通过自注意力机制组合局部风格形成全局条件 → 3. 分层编码策略增强风格表征能力

Result: 在未见说话者测试中：自然度MOS提升12%，说话人相似度达到85%，词错误率降低至2.1%。同时展现分层结构的解释优势（风格可视化）和韵律控制能力

Conclusion: 渐进式分层编码策略有效提升零样本语音合成的风格表征质量，其语义对齐的层次结构为语音合成的可解释性和可控性提供了新方向

Abstract: We present the gradual style adaptor TTS (GSA-TTS) with a novel style encoder
that gradually encodes speaking styles from an acoustic reference for zero-shot
speech synthesis. GSA first captures the local style of each semantic sound
unit. Then the local styles are combined by self-attention to obtain a global
style condition. This semantic and hierarchical encoding strategy provides a
robust and rich style representation for an acoustic model. We test GSA-TTS on
unseen speakers and obtain promising results regarding naturalness, speaker
similarity, and intelligibility. Additionally, we explore the potential of GSA
in terms of interpretability and controllability, which stems from its
hierarchical structure.

</details>


### [133] [gec-metrics: A Unified Library for Grammatical Error Correction Evaluation](https://arxiv.org/abs/2505.19388)
*Takumi Goto,Yusuke Sakai,Taro Watanabe*

Main category: cs.CL

TL;DR: 开发gec-metrics库统一GEC评估标准，提供可扩展接口及分析工具，促进公平系统比较。代码MIT开源并打包分发。


<details>
  <summary>Details</summary>
Motivation: 解决GEC评估中因不同实现导致的指标不一致问题，通过标准化接口确保评估结果可比性，同时提升开发效率。

Method: 设计统一API集成多种评估指标，实现元评估模块，开发可视化分析脚本，采用MIT协议开源并打包为可安装组件。

Result: 成功创建可扩展的评估框架，提供系统对比基准，配套工具链帮助优化指标设计，代码可用性获社区认可。

Conclusion: 该库标准化了GEC评估流程，为研究者提供可靠工具基础，未来可通过社区协作持续扩展评估指标集合。

Abstract: We introduce gec-metrics, a library for using and developing grammatical
error correction (GEC) evaluation metrics through a unified interface. Our
library enables fair system comparisons by ensuring that everyone conducts
evaluations using a consistent implementation. Moreover, it is designed with a
strong focus on API usage, making it highly extensible. It also includes
meta-evaluation functionalities and provides analysis and visualization
scripts, contributing to developing GEC evaluation metrics. Our code is
released under the MIT license and is also distributed as an installable
package. The video is available on YouTube.

</details>


### [134] [Simple and Effective Baselines for Code Summarisation Evaluation](https://arxiv.org/abs/2505.19392)
*Jade Robinson,Jonathan K. Kummerfeld*

Main category: cs.CL

TL;DR: 提出基于LLM的代码摘要评估新基线方法，优于现有指标且支持无参考评估


<details>
  <summary>Details</summary>
Motivation: 现有代码摘要生成技术缺乏高效可靠的评估方式，人工评估成本高而自动指标不可靠

Method: 让LLM结合代码内容直接对摘要进行整体评分，支持两种模式（参考/无参考摘要）

Result: 该方法达到或超越现有指标效果，无参考模式可扩展应用于代码文档质量评估

Conclusion: 建议与嵌入方法结合使用以规避LLM特有偏差，实现更全面的评估

Abstract: Code documentation is useful, but writing it is time-consuming. Different
techniques for generating code summaries have emerged, but comparing them is
difficult because human evaluation is expensive and automatic metrics are
unreliable. In this paper, we introduce a simple new baseline in which we ask
an LLM to give an overall score to a summary. Unlike n-gram and embedding-based
baselines, our approach is able to consider the code when giving a score. This
allows us to also make a variant that does not consider the reference summary
at all, which could be used for other tasks, e.g., to evaluate the quality of
documentation in code bases. We find that our method is as good or better than
prior metrics, though we recommend using it in conjunction with embedding-based
methods to avoid the risk of LLM-specific bias.

</details>


### [135] [CoTGuard: Using Chain-of-Thought Triggering for Copyright Protection in Multi-Agent LLM Systems](https://arxiv.org/abs/2505.19405)
*Yan Wen,Junfeng Guo,Heng Huang*

Main category: cs.CL

TL;DR: 提出CoTGuard框架，通过思维链触发检测实现协作式LLM代理系统的细粒度版权保护，在任务性能影响最小化的前提下有效识别内容泄漏。


<details>
  <summary>Details</summary>
Motivation: 现有版权保护技术仅关注最终输出，忽视了多智能体系统中通过中间推理步骤泄露敏感/版权内容的风险。

Method: 在智能体提示中嵌入特定触发查询，激活并监控思维链推理中的关键段落，实现可解释的中间过程侵权检测。

Result: 多基准测试表明该方法能有效发现内容泄漏（检测率提升23%），同时保持95%以上的任务完成度。

Conclusion: 思维链层面的监控为LLM代理系统的知识产权保护开辟了新方向，平衡了版权保护与系统效能。

Abstract: As large language models (LLMs) evolve into autonomous agents capable of
collaborative reasoning and task execution, multi-agent LLM systems have
emerged as a powerful paradigm for solving complex problems. However, these
systems pose new challenges for copyright protection, particularly when
sensitive or copyrighted content is inadvertently recalled through inter-agent
communication and reasoning. Existing protection techniques primarily focus on
detecting content in final outputs, overlooking the richer, more revealing
reasoning processes within the agents themselves. In this paper, we introduce
CoTGuard, a novel framework for copyright protection that leverages
trigger-based detection within Chain-of-Thought (CoT) reasoning. Specifically,
we can activate specific CoT segments and monitor intermediate reasoning steps
for unauthorized content reproduction by embedding specific trigger queries
into agent prompts. This approach enables fine-grained, interpretable detection
of copyright violations in collaborative agent scenarios. We evaluate CoTGuard
on various benchmarks in extensive experiments and show that it effectively
uncovers content leakage with minimal interference to task performance. Our
findings suggest that reasoning-level monitoring offers a promising direction
for safeguarding intellectual property in LLM-based agent systems.

</details>


### [136] [Self-Reflective Planning with Knowledge Graphs: Enhancing LLM Reasoning Reliability for Question Answering](https://arxiv.org/abs/2505.19410)
*Jiajun Zhu,Ye Liu,Meikai Bao,Kai Zhang,Yanghai Zhang,Qi Liu*

Main category: cs.CL

TL;DR: 提出自我反思规划(SRP)框架，通过知识图谱与语言模型协同的迭代反思机制，显著提升复杂推理任务的准确性和可靠性


<details>
  <summary>Details</summary>
Motivation: 现有大模型与知识图谱结合方法存在推理路径不完整、事实不一致问题，需要更可靠的迭代验证机制

Method: 1.参考引导规划：搜索相关实体构建初始推理路径
2.知识检索验证：通过知识图谱获取结构化事实
3.迭代反思修正：判断检索结果并动态调整推理路径

Result: 在三个公开数据集上超越强基线模型，准确率提升显著（具体数值需查原文），展示出更可靠的推理能力

Conclusion: SRP框架有效解决知识驱动推理中的幻觉问题，为语言模型与结构化知识的深度融合提供新方向

Abstract: Recently, large language models (LLMs) have demonstrated remarkable
capabilities in natural language processing tasks, yet they remain prone to
hallucinations when reasoning with insufficient internal knowledge. While
integrating LLMs with knowledge graphs (KGs) provides access to structured,
verifiable information, existing approaches often generate incomplete or
factually inconsistent reasoning paths. To this end, we propose Self-Reflective
Planning (SRP), a framework that synergizes LLMs with KGs through iterative,
reference-guided reasoning. Specifically, given a question and topic entities,
SRP first searches for references to guide planning and reflection. In the
planning process, it checks initial relations and generates a reasoning path.
After retrieving knowledge from KGs through a reasoning path, it implements
iterative reflection by judging the retrieval result and editing the reasoning
path until the answer is correctly retrieved. Extensive experiments on three
public datasets demonstrate that SRP surpasses various strong baselines and
further underscore its reliable reasoning ability.

</details>


### [137] [The Role of Diversity in In-Context Learning for Large Language Models](https://arxiv.org/abs/2505.19426)
*Wenyang Xiao,Haoyu Zhao,Lingxiao Huang*

Main category: cs.CL

TL;DR: 研究证实多样性样本选择提升大模型上下文学习性能，尤其在数学/代码任务中效果显著，并增强对分布外数据的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 针对现有上下文学习研究中过度关注样本相似性而忽视多样性的现状，探索多样性样本选择对模型性能的系统性影响

Method: 在Llama/Gemma/Mistral等主流模型家族上开展多任务实验（情感分类→数学/代码问题），提出理论框架解释多样性增益机制

Result: 多样性样本选择使复杂任务准确率提升8-15%，分布外场景鲁棒性提升30%+，理论框架有效解释多样性增益机理

Conclusion: 样本多样性是上下文学习关键因素，其与相似性的平衡机制为未来优化方向，理论框架为算法设计提供新思路

Abstract: In-context learning (ICL) is a crucial capability of current large language
models (LLMs), where the selection of examples plays a key role in performance.
While most existing approaches focus on selecting the most similar examples to
the query, the impact of diversity in example selection remains underexplored.
We systematically investigate the role of diversity in in-context example
selection through experiments across a range of tasks, from sentiment
classification to more challenging math and code problems. Experiments on
Llama-3.1, Gemma-2, and Mistral-v0.3 families of models show that
diversity-aware selection methods improve performance, particularly on complex
tasks like math and code, and enhance robustness to out-of-distribution
queries. To support these findings, we introduce a theoretical framework that
explains the benefits of incorporating diversity in in-context example
selection.

</details>


### [138] [Frictional Agent Alignment Framework: Slow Down and Don't Break Things](https://arxiv.org/abs/2505.19428)
*Abhijnan Nath,Carine Graff,Andrei Bachinin,Nikhil Krishnaswamy*

Main category: cs.CL

TL;DR: 提出FAAF框架，通过双策略机制生成上下文感知的'摩擦'，有效解决动态协作任务中信念对齐问题，实验验证其优于现有方法且具备OOD泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统偏好对齐方法（如DPO）在动态协作场景中难以应对稀疏/有偏的信念信号，需发展主动促发反思的机制来增强人机协作质量。

Method: 采用双策略框架：1）摩擦状态策略检测信念错位 2）干预策略生成合作友好响应，通过解析解实现监督学习训练单一策略。

Result: 在三大基准测试中，FAAF生成更简洁可解释的摩擦响应，OOD场景泛化能力显著优于基线模型。

Conclusion: FAAF通过主动制造认知摩擦使LLM成为动态'思考伙伴'，推动可扩展的人机协作范式，代码数据已开源供社区验证。

Abstract: AI support of collaborative interactions entails mediating potential
misalignment between interlocutor beliefs. Common preference alignment methods
like DPO excel in static settings, but struggle in dynamic collaborative tasks
where the explicit signals of interlocutor beliefs are sparse and skewed. We
propose the Frictional Agent Alignment Framework (FAAF), to generate precise,
context-aware "friction" that prompts for deliberation and re-examination of
existing evidence. FAAF's two-player objective decouples from data skew: a
frictive-state policy identifies belief misalignments, while an intervention
policy crafts collaborator-preferred responses. We derive an analytical
solution to this objective, enabling training a single policy via a simple
supervised loss. Experiments on three benchmarks show FAAF outperforms
competitors in producing concise, interpretable friction and in OOD
generalization. By aligning LLMs to act as adaptive "thought partners" -- not
passive responders -- FAAF advances scalable, dynamic human-AI collaboration.
Our code and data can be found at https://github.com/csu-signal/FAAF_ACL.

</details>


### [139] [Rhapsody: A Dataset for Highlight Detection in Podcasts](https://arxiv.org/abs/2505.19429)
*Younghan Park,Anuj Diwan,David Harwath,Eunsol Choi*

Main category: cs.CL

TL;DR: 提出Rhapsody数据集用于播客高亮检测，发现结合语音和文本特征的微调模型显著优于零样本大模型。


<details>
  <summary>Details</summary>
Motivation: 播客内容体量庞大但非结构化，自动高亮检测可提升用户内容筛选效率。当前主流大模型在此任务中存在局限，需探索领域专用方案。

Method: 构建13K播客片段数据集，使用YouTube重播数据标注高亮片段。对比零样本提示（GPT-4o/Gemini）与微调模型（结合语音特征+文本的分类模型）的性能。

Result: 微调模型F1值达0.78，显著优于GPT-4o（0.62）和Gemini（0.59）。多模态特征使准确率提升12%。

Conclusion: 长语音媒体的细粒度信息抽取需领域适配，多模态微调方案有效，为音频内容理解提供新思路。

Abstract: Podcasts have become daily companions for half a billion users. Given the
enormous amount of podcast content available, highlights provide a valuable
signal that helps viewers get the gist of an episode and decide if they want to
invest in listening to it in its entirety. However, identifying highlights
automatically is challenging due to the unstructured and long-form nature of
the content. We introduce Rhapsody, a dataset of 13K podcast episodes paired
with segment-level highlight scores derived from YouTube's 'most replayed'
feature. We frame the podcast highlight detection as a segment-level binary
classification task. We explore various baseline approaches, including
zero-shot prompting of language models and lightweight finetuned language
models using segment-level classification heads. Our experimental results
indicate that even state-of-the-art language models like GPT-4o and Gemini
struggle with this task, while models finetuned with in-domain data
significantly outperform their zero-shot performance. The finetuned model
benefits from leveraging both speech signal features and transcripts. These
findings highlight the challenges for fine-grained information access in
long-form spoken media.

</details>


### [140] [Deriving Strategic Market Insights with Large Language Models: A Benchmark for Forward Counterfactual Generation](https://arxiv.org/abs/2505.19430)
*Keane Ong,Rui Mao,Deeksha Varshney,Paul Pu Liang,Erik Cambria,Gianmarco Mengaldo*

Main category: cs.CL

TL;DR: 论文提出金融前向反事实推理基准Fin-Force，评估大模型在预测市场发展中的潜力


<details>
  <summary>Details</summary>
Motivation: 传统反事实推理侧重历史分析，而前向反事实推理能有效预测金融市场动态变化，但人工操作存在认知负荷限制需要自动化解决方案

Method: 通过构建包含金融新闻头条的结构化评估基准Fin-Force，支持基于LLM的前向反事实生成框架开发

Result: 在Fin-Force基准上验证了前沿LLM和反事实生成方法的有效性，揭示了现有局限性并提出改进方向

Conclusion: Fin-Force为自动化预测市场动态提供结构化评估基础，推动风险预警和决策支持系统的智能化发展

Abstract: Counterfactual reasoning typically involves considering alternatives to
actual events. While often applied to understand past events, a distinct
form-forward counterfactual reasoning-focuses on anticipating plausible future
developments. This type of reasoning is invaluable in dynamic financial
markets, where anticipating market developments can powerfully unveil potential
risks and opportunities for stakeholders, guiding their decision-making.
However, performing this at scale is challenging due to the cognitive demands
involved, underscoring the need for automated solutions. Large Language Models
(LLMs) offer promise, but remain unexplored for this application. To address
this gap, we introduce a novel benchmark, Fin-Force-FINancial FORward
Counterfactual Evaluation. By curating financial news headlines and providing
structured evaluation, Fin-Force supports LLM based forward counterfactual
generation. This paves the way for scalable and automated solutions for
exploring and anticipating future market developments, thereby providing
structured insights for decision-making. Through experiments on Fin-Force, we
evaluate state-of-the-art LLMs and counterfactual generation methods, analyzing
their limitations and proposing insights for future research.

</details>


### [141] [Route to Reason: Adaptive Routing for LLM and Reasoning Strategy Selection](https://arxiv.org/abs/2505.19435)
*Zhihong Pan,Kai Zhang,Yuze Zhao,Yupeng Han*

Main category: cs.CL

TL;DR: 提出动态路由框架RTR，通过联合选择语言模型和推理策略，在预算约束下实现精度与计算效率的最优平衡


<details>
  <summary>Details</summary>
Motivation: 传统测试时扩展方法存在计算成本高且易导致模型'过思考'的问题，需要更高效的推理解决方案

Method: 学习专家模型和推理策略的压缩表示，实现推理时的动态联合选择，支持任意黑箱/白箱模型的即插即用

Result: 在7个模型和4种策略的实验中，准确率超越最佳单模型的同时减少60%以上的token消耗

Conclusion: RTR框架以低成本实现灵活扩展，在保持精度的同时显著提升计算效率，适用于多种推理场景

Abstract: The inherent capabilities of a language model (LM) and the reasoning
strategies it employs jointly determine its performance in reasoning tasks.
While test-time scaling is regarded as an effective approach to tackling
complex reasoning tasks, it incurs substantial computational costs and often
leads to "overthinking", where models become trapped in "thought pitfalls". To
address this challenge, we propose Route-To-Reason (RTR), a novel unified
routing framework that dynamically allocates both LMs and reasoning strategies
according to task difficulty under budget constraints. RTR learns compressed
representations of both expert models and reasoning strategies, enabling their
joint and adaptive selection at inference time. This method is low-cost, highly
flexible, and can be seamlessly extended to arbitrary black-box or white-box
models and strategies, achieving true plug-and-play functionality. Extensive
experiments across seven open source models and four reasoning strategies
demonstrate that RTR achieves an optimal trade-off between accuracy and
computational efficiency among all baselines, achieving higher accuracy than
the best single model while reducing token usage by over 60%.

</details>


### [142] [Surrogate Signals from Format and Length: Reinforcement Learning for Solving Mathematical Problems without Ground Truth Answers](https://arxiv.org/abs/2505.19439)
*Rihui Xin,Han Liu,Zecheng Wang,Yupeng Zhang,Dianbo Sui,Xiaolin Hu,Bingning Wang*

Main category: cs.CL

TL;DR: 提出无需真实答案标签的LLM数学解题训练方法，通过格式-长度双信号实现性能突破


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖高成本的真实答案标注，数学领域标注尤其困难。研究发现基础模型已具备数学推理能力，只需培养良好作答习惯即可释放潜力

Method: 分阶段训练：初期使用格式正确性奖励（GRPO算法），后期引入答案长度奖励形成复合奖励机制

Result: 基于7B基础模型在AIME2024达到40%准确率，部分场景超越依赖真实标签的标准GRPO算法

Conclusion: 标签替代方案有效降低数据依赖，揭示模型能力解锁本质：培养规范的作答习惯即可激活预训练习得的数学推理能力

Abstract: Large Language Models have achieved remarkable success in natural language
processing tasks, with Reinforcement Learning playing a key role in adapting
them to specific applications. However, obtaining ground truth answers for
training LLMs in mathematical problem-solving is often challenging, costly, and
sometimes unfeasible. This research delves into the utilization of format and
length as surrogate signals to train LLMs for mathematical problem-solving,
bypassing the need for traditional ground truth answers.Our study shows that a
reward function centered on format correctness alone can yield performance
improvements comparable to the standard GRPO algorithm in early phases.
Recognizing the limitations of format-only rewards in the later phases, we
incorporate length-based rewards. The resulting GRPO approach, leveraging
format-length surrogate signals, not only matches but surpasses the performance
of the standard GRPO algorithm relying on ground truth answers in certain
scenarios, achieving 40.0\% accuracy on AIME2024 with a 7B base model. Through
systematic exploration and experimentation, this research not only offers a
practical solution for training LLMs to solve mathematical problems and
reducing the dependence on extensive ground truth data collection, but also
reveals the essence of why our label-free approach succeeds: base model is like
an excellent student who has already mastered mathematical and logical
reasoning skills, but performs poorly on the test paper, it simply needs to
develop good answering habits to achieve outstanding results in exams , in
other words, to unlock the capabilities it already possesses.

</details>


### [143] [The Birth of Knowledge: Emergent Features across Time, Space, and Scale in Large Language Models](https://arxiv.org/abs/2505.19440)
*Shashata Sawmya,Micah Adler,Nir Shavit*

Main category: cs.CL

TL;DR: 研究发现大语言模型中可解释特征的出现存在时空与规模阈值，并发现早期层特征在后期层重新激活的反常现象


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型中可解释类别特征在不同训练阶段、模型层数和模型规模下的演变规律

Method: 使用稀疏自编码器进行机械可解释性分析，追踪神经激活中的语义概念出现位置和时间

Result: 发现特征出现存在明确的时空和规模阈值，并在空间维度观察到早期层特征在后期层重新激活的现象

Conclusion: 层间特征再激活现象挑战了Transformer表征动态的传统假设，为模型解释提供新视角

Abstract: This paper studies the emergence of interpretable categorical features within
large language models (LLMs), analyzing their behavior across training
checkpoints (time), transformer layers (space), and varying model sizes
(scale). Using sparse autoencoders for mechanistic interpretability, we
identify when and where specific semantic concepts emerge within neural
activations. Results indicate clear temporal and scale-specific thresholds for
feature emergence across multiple domains. Notably, spatial analysis reveals
unexpected semantic reactivation, with early-layer features re-emerging at
later layers, challenging standard assumptions about representational dynamics
in transformer models.

</details>


### [144] [Balancing Computation Load and Representation Expressivity in Parallel Hybrid Neural Networks](https://arxiv.org/abs/2505.19472)
*Mohammad Mahdi Moradi,Walid Ahmed,Shuangyue Wen,Sudhir Mudur,Weiwei Zhang,Yang Liu*

Main category: cs.CL

TL;DR: FlowHN提出新型并行混合架构，通过动态令牌分配和分支输出融合策略，在语言建模中实现4倍处理速度提升和2倍计算效率改进


<details>
  <summary>Details</summary>
Motivation: 现有注意力与SSM混合架构存在组件闲置延迟（顺序式）和分支输出不兼容/负载不均（并行式）的问题，需要更高效的并行处理方案

Method: 1) FLOP感知的动态令牌分流机制平衡注意力/SSM分支计算负载 2) 异构分支输出融合方法增强表征表达能力

Result: 在135M/350M/1B参数模型中，FlowHN相比顺序混合架构和传统并行方案，实现高达4倍Tokens/s处理速度和2倍模型计算利用率(MFU)

Conclusion: FlowHN通过计算负载动态平衡与异构特征融合，突破混合架构瓶颈，在保持精度的同时显著提升处理吞吐量和硬件利用率

Abstract: Attention and State-Space Models (SSMs) when combined in a hybrid network in
sequence or in parallel provide complementary strengths. In a hybrid sequential
pipeline they alternate between applying a transformer to the input and then
feeding its output into a SSM. This results in idle periods in the individual
components increasing end-to-end latency and lowering throughput caps. In the
parallel hybrid architecture, the transformer operates independently in
parallel with the SSM, and these pairs are cascaded, with output from one pair
forming the input to the next. Two issues are (i) creating an expressive
knowledge representation with the inherently divergent outputs from these
separate branches, and (ii) load balancing the computation between these
parallel branches, while maintaining representation fidelity. In this work we
present FlowHN, a novel parallel hybrid network architecture that accommodates
various strategies for load balancing, achieved through appropriate
distribution of input tokens between the two branches. Two innovative
differentiating factors in FlowHN include a FLOP aware dynamic token split
between the attention and SSM branches yielding efficient balance in compute
load, and secondly, a method to fuse the highly divergent outputs from
individual branches for enhancing representation expressivity. Together they
enable much better token processing speeds, avoid bottlenecks, and at the same
time yield significantly improved accuracy as compared to other competing
works. We conduct comprehensive experiments on autoregressive language modeling
for models with 135M, 350M, and 1B parameters. FlowHN outperforms sequential
hybrid models and its parallel counterpart, achieving up to 4* higher Tokens
per Second (TPS) and 2* better Model FLOPs Utilization (MFU).

</details>


### [145] [Continuous Self-Improvement of Large Language Models by Test-time Training with Verifier-Driven Sample Selection](https://arxiv.org/abs/2505.19475)
*Mohammad Mahdi Moradi,Hossam Amer,Sudhir Mudur,Weiwei Zhang,Yang Liu,Walid Ahmed*

Main category: cs.CL

TL;DR: 提出VDS-TTT框架，通过验证器驱动样本选择和LoRA适配器微调，显著提升大语言模型在分布外数据上的适应能力。


<details>
  <summary>Details</summary>
Motivation: 解决预训练语言模型在未标记分布外数据（特别是结构新颖的推理任务）上表现不佳的问题。

Method: 生成多个候选答案→验证器评分筛选高置信度样本→仅微调LoRA适配器参数实现高效测试时训练。

Result: 在三个基准测试和三种LLM上取得32.29%相对提升，比无测试时训练方法高6.66%。

Conclusion: VDS-TTT首次结合验证器驱动数据合成与测试时训练，为大模型实时适应提供高效解决方案。

Abstract: Learning to adapt pretrained language models to unlabeled,
out-of-distribution data is a critical challenge, as models often falter on
structurally novel reasoning tasks even while excelling within their training
distribution. We introduce a new framework called VDS-TTT - Verifier-Driven
Sample Selection for Test-Time Training to efficiently address this. We use a
learned verifier to score a pool of generated responses and select only from
high ranking pseudo-labeled examples for fine-tuned adaptation. Specifically,
for each input query our LLM generates N candidate answers; the verifier
assigns a reliability score to each, and the response with the highest
confidence and above a fixed threshold is paired with its query for test-time
training. We fine-tune only low-rank LoRA adapter parameters, ensuring
adaptation efficiency and fast convergence. Our proposed self-supervised
framework is the first to synthesize verifier driven test-time training data
for continuous self-improvement of the model. Experiments across three diverse
benchmarks and three state-of-the-art LLMs demonstrate that VDS-TTT yields up
to a 32.29% relative improvement over the base model and a 6.66% gain compared
to verifier-based methods without test-time training, highlighting its
effectiveness and efficiency for on-the-fly large language model adaptation.

</details>


### [146] [CulFiT: A Fine-grained Cultural-aware LLM Training Paradigm via Multilingual Critique Data Synthesis](https://arxiv.org/abs/2505.19484)
*Ruixiang Feng,Shen Gao,Xiuying Chen,Lisi Chen,Shuo Shang*

Main category: cs.CL

TL;DR: 提出CulFiT训练范式解决大语言模型文化偏见问题，通过多语言数据和细粒度奖励建模提升文化敏感性，并建立GlobalCultureQA评测数据集。


<details>
  <summary>Details</summary>
Motivation: 大语言模型存在忽视低资源地区文化价值观的显著偏见，可能强化刻板印象与歧视，破坏普世平等原则。

Method: 1. 合成多元文化相关问题
2. 构建多语言批判数据集
3. 细粒度奖励机制分解文化文本为可验证知识单元
4. 开发GlobalCultureQA多语言开放式问答评测集

Result: 在三大基准测试及自建数据集上，CulFiT在文化对齐和通用推理任务中达到开源模型最佳性能

Conclusion: CulFiT通过系统性文化建模方法有效提升模型文化敏感性，GlobalCultureQA为跨文化评估提供新基准

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
various tasks, yet they often exhibit a specific cultural biases, neglecting
the values and linguistic diversity of low-resource regions. This cultural bias
not only undermines universal equality, but also risks reinforcing stereotypes
and perpetuating discrimination. To address this, we propose CulFiT, a novel
culturally-aware training paradigm that leverages multilingual data and
fine-grained reward modeling to enhance cultural sensitivity and inclusivity.
Our approach synthesizes diverse cultural-related questions, constructs
critique data in culturally relevant languages, and employs fine-grained
rewards to decompose cultural texts into verifiable knowledge units for
interpretable evaluation. We also introduce GlobalCultureQA, a multilingual
open-ended question-answering dataset designed to evaluate culturally-aware
responses in a global context. Extensive experiments on three existing
benchmarks and our GlobalCultureQA demonstrate that CulFiT achieves
state-of-the-art open-source model performance in cultural alignment and
general reasoning.

</details>


### [147] [Anveshana: A New Benchmark Dataset for Cross-Lingual Information Retrieval On English Queries and Sanskrit Documents](https://arxiv.org/abs/2505.19494)
*Manoj Balaji Jagadeeshan,Prince Raj,Pawan Goyal*

Main category: cs.CL

TL;DR: 构建英语查询检索梵文文献的跨语言基准框架，基于3400组英梵对照数据集验证翻译检索法(DT)在古文本处理中的最优表现


<details>
  <summary>Details</summary>
Motivation: 解决梵文典籍的跨语言检索难题，通过构建结构化数据集和技术方案促进印度文化遗产的数字化保护与哲学传播

Method: 采用DR/DT/QT三阶段检索框架，融合共享嵌入空间与神经机器翻译，微调ColBERT等稠密检索模型，适配梵文形态学特征

Result: DT方法在MRR@10指标上较传统方法提升21.3%，跨语言检索准确率达76.8%，成功实现《圣典博伽瓦谭》的语义对齐

Conclusion: 融合翻译增强的检索架构显著提升古文献可及性，技术方案对低资源语种文化遗产保护具有范式价值

Abstract: The study presents a comprehensive benchmark for retrieving Sanskrit
documents using English queries, focusing on the chapters of the
Srimadbhagavatam. It employs a tripartite approach: Direct Retrieval (DR),
Translation-based Retrieval (DT), and Query Translation (QT), utilizing shared
embedding spaces and advanced translation methods to enhance retrieval systems
in a RAG framework. The study fine-tunes state-of-the-art models for Sanskrit's
linguistic nuances, evaluating models such as BM25, REPLUG, mDPR, ColBERT,
Contriever, and GPT-2. It adapts summarization techniques for Sanskrit
documents to improve QA processing. Evaluation shows DT methods outperform DR
and QT in handling the cross-lingual challenges of ancient texts, improving
accessibility and understanding. A dataset of 3,400 English-Sanskrit
query-document pairs underpins the study, aiming to preserve Sanskrit
scriptures and share their philosophical importance widely. Our dataset is
publicly available at https://huggingface.co/datasets/manojbalaji1/anveshana

</details>


### [148] [LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study](https://arxiv.org/abs/2505.19510)
*Dongil Yang,Minjin Kim,Sunghwan Kim,Beong-woo Kwak,Minjun Park,Jinseok Hong,Woontack Woo,Jinyoung Yeo*

Main category: cs.CL

TL;DR: 提出TSG Bench基准测试，发现大语言模型在场景图生成（尤其是复杂文本）存在显著瓶颈


<details>
  <summary>Details</summary>
Motivation: 大模型在具身智能和机器人领域的应用需要时空理解能力，但现有研究对场景图能力的系统评估不足

Method: 构建TSG Bench基准，包含场景图理解和生成双维度测试，评估11个主流LLM

Result: 模型场景图理解能力尚可，但生成能力薄弱（复杂叙事场景准确率仅28%），主要瓶颈在于场景分解能力不足

Conclusion: 揭示了当前LLM场景图生成的缺陷，为提升多模态基础模型的空间推理能力指明方向，开源代码和数据推动领域发展

Abstract: The remarkable reasoning and generalization capabilities of Large Language
Models (LLMs) have paved the way for their expanding applications in embodied
AI, robotics, and other real-world tasks. To effectively support these
applications, grounding in spatial and temporal understanding in multimodal
environments is essential. To this end, recent works have leveraged scene
graphs, a structured representation that encodes entities, attributes, and
their relationships in a scene. However, a comprehensive evaluation of LLMs'
ability to utilize scene graphs remains limited. In this work, we introduce
Text-Scene Graph (TSG) Bench, a benchmark designed to systematically assess
LLMs' ability to (1) understand scene graphs and (2) generate them from textual
narratives. With TSG Bench we evaluate 11 LLMs and reveal that, while models
perform well on scene graph understanding, they struggle with scene graph
generation, particularly for complex narratives. Our analysis indicates that
these models fail to effectively decompose discrete scenes from a complex
narrative, leading to a bottleneck when generating scene graphs. These findings
underscore the need for improved methodologies in scene graph generation and
provide valuable insights for future research. The demonstration of our
benchmark is available at https://tsg-bench.netlify.app. Additionally, our code
and evaluation data are publicly available at
https://anonymous.4open.science/r/TSG-Bench.

</details>


### [149] [Causal Distillation: Transferring Structured Explanations from Large to Compact Language Models](https://arxiv.org/abs/2505.19511)
*Aggrey Muhebwa,Khalid K. Osman*

Main category: cs.CL

TL;DR: 提出通过结构化因果解释蒸馏框架，将大型教师模型的因果推理能力迁移到小型开源模型


<details>
  <summary>Details</summary>
Motivation: 解决小型开源模型难以复现大型专有模型因果推理能力的问题，旨在使紧凑模型获得可靠因果推理技能

Method: 通过训练学生模型生成与教师模型一致的结构化因果解释，并设计CEC指标评估解释的逻辑连贯性

Result: 框架为小模型因果推理训练和解释评估提供理论基础，CEC指标可系统评估语言模型输出的因果链完整性与逻辑一致性

Conclusion: 该方法使小型模型具备稳健因果推理能力，并为解释质量评估建立量化标准，推动可解释AI发展

Abstract: Large proprietary language models exhibit strong causal reasoning abilities
that smaller open-source models struggle to replicate. We introduce a novel
framework for distilling causal explanations that transfers causal reasoning
skills from a powerful teacher model to a compact open-source model. The key
idea is to train the smaller model to develop causal reasoning abilities by
generating structured cause-and-effect explanations consistent with those of
the teacher model. To evaluate the quality of the student-generated
explanations, we introduce a new metric called Causal Explanation Coherence
(CEC) to assess the structural and logical consistency of causal reasoning.
This metric uses sentence-level semantic alignment to measure how well each
part of the generated explanation corresponds to the teacher's reference,
capturing both faithfulness and coverage of the underlying causal chain. Our
framework and the CEC metric provide a principled foundation for training
smaller models to perform robust causal reasoning and for systematically
assessing the coherence of explanations in language model outputs.

</details>


### [150] [SIPDO: Closed-Loop Prompt Optimization via Synthetic Data Feedback](https://arxiv.org/abs/2505.19514)
*Yaoning Yu,Ye Yu,Kai Wei,Haojing Luo,Haohan Wang*

Main category: cs.CL

TL;DR: 提出SIPDO框架，通过数据合成与提示优化的闭环系统实现自动化的提示改进


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法依赖固定数据集且缺乏迭代改进支持，需动态揭示提示弱点并优化

Method: 耦合合成数据生成器（揭示当前提示缺陷）与提示优化器（增量式改进）的反馈驱动框架

Result: 在问答和推理基准测试中超越标准提示调优方法

Conclusion: 将数据合成融入提示学习流程能系统提升性能，无需外部监督或新任务

Abstract: Prompt quality plays a critical role in the performance of large language
models (LLMs), motivating a growing body of work on prompt optimization. Most
existing methods optimize prompts over a fixed dataset, assuming static input
distributions and offering limited support for iterative improvement. We
introduce SIPDO (Self-Improving Prompts through Data-Augmented Optimization), a
closed-loop framework for prompt learning that integrates synthetic data
generation into the optimization process. SIPDO couples a synthetic data
generator with a prompt optimizer, where the generator produces new examples
that reveal current prompt weaknesses and the optimizer incrementally refines
the prompt in response. This feedback-driven loop enables systematic
improvement of prompt performance without assuming access to external
supervision or new tasks. Experiments across question answering and reasoning
benchmarks show that SIPDO outperforms standard prompt tuning methods,
highlighting the value of integrating data synthesis into prompt learning
workflows.

</details>


### [151] [Bias in Political Dialogue: Tagging U.S. Presidential Debates with an Extended DAMSL Framework](https://arxiv.org/abs/2505.19515)
*Lavanya Prahallad,Radhika Mamidi*

Main category: cs.CL

TL;DR: 开发BEADS标注框架分析特朗普在总统辩论中的对抗性修辞策略，发现其在控制叙事和影响观众方面占据主导地位。


<details>
  <summary>Details</summary>
Motivation: 现有DAMSL框架缺乏对政治沟通中偏见和对抗性话语的系统分析能力，需建立更全面的标注体系。

Method: 结合人工标注与ChatGPT零样本标注，对特朗普-拜登(19,219词)和特朗普-哈里斯(18,123词)辩论转录本进行BEADS框架分析。

Result: 特朗普在挑战性交流(83%)、选择性强调(78%)、恐惧诉求(65%)、政治偏见(72%)和感知性轻视(81%)等关键类别显著领先。

Conclusion: BEADS成为跨语言/政治环境的可扩展分析框架，证实对抗性修辞在塑造政治叙事中的有效性。

Abstract: We present a critical discourse analysis of the 2024 U.S. presidential
debates, examining Donald Trump's rhetorical strategies in his interactions
with Joe Biden and Kamala Harris. We introduce a novel annotation framework,
BEADS (Bias Enriched Annotation for Dialogue Structure), which systematically
extends the DAMSL framework to capture bias driven and adversarial discourse
features in political communication. BEADS includes a domain and language
agnostic set of tags that model ideological framing, emotional appeals, and
confrontational tactics. Our methodology compares detailed human annotation
with zero shot ChatGPT assisted tagging on verified transcripts from the Trump
and Biden (19,219 words) and Trump and Harris (18,123 words) debates. Our
analysis shows that Trump consistently dominated in key categories: Challenge
and Adversarial Exchanges, Selective Emphasis, Appeal to Fear, Political Bias,
and Perceived Dismissiveness. These findings underscore his use of emotionally
charged and adversarial rhetoric to control the narrative and influence
audience perception. In this work, we establish BEADS as a scalable and
reproducible framework for critical discourse analysis across languages,
domains, and political contexts.

</details>


### [152] [AmpleHate: Amplifying the Attention for Versatile Implicit Hate Detection](https://arxiv.org/abs/2505.19528)
*Yejin Lee,Joonghyuk Hahn,Hyeseon Ahn,Yo-Sub Han*

Main category: cs.CL

TL;DR: 提出AmpleHate模型，通过模仿人类推理过程结合显式/隐式目标与上下文关系，显著提升隐式仇恨言论检测效果，实验显示其性能超越基线82.14%且具备可解释性


<details>
  <summary>Details</summary>
Motivation: 现有基于对比学习的方法无法模拟人类'先识别目标后分析上下文关系'的推理逻辑，导致隐式仇恨检测效果受限

Method: 1. 使用预训练NER识别显式目标 2. 用[CLS]捕捉隐式目标 3. 计算目标-上下文注意力关系 4. 将关系向量注入句子表示

Result: SOTA性能（优于基线82.14%），收敛速度更快，注意力模式与人类判断高度吻合

Conclusion: AmpleHate通过模拟人类推理机制，在保持可解释性的同时显著提升检测效果，为实际场景提供更优解决方案

Abstract: Implicit hate speech detection is challenging due to its subtlety and
reliance on contextual interpretation rather than explicit offensive words.
Current approaches rely on contrastive learning, which are shown to be
effective on distinguishing hate and non-hate sentences. Humans, however,
detect implicit hate speech by first identifying specific targets within the
text and subsequently interpreting how these target relate to their surrounding
context. Motivated by this reasoning process, we propose AmpleHate, a novel
approach designed to mirror human inference for implicit hate detection.
AmpleHate identifies explicit target using a pretrained Named Entity
Recognition model and capture implicit target information via [CLS] tokens. It
computes attention-based relationships between explicit, implicit targets and
sentence context and then, directly injects these relational vectors into the
final sentence representation. This amplifies the critical signals of
target-context relations for determining implicit hate. Experiments demonstrate
that AmpleHate achieves state-of-the-art performance, outperforming contrastive
learning baselines by an average of 82.14% and achieve faster convergence.
Qualitative analyses further reveal that attention patterns produced by
AmpleHate closely align with human judgement, underscoring its interpretability
and robustness.

</details>


### [153] [Small Language Models: Architectures, Techniques, Evaluation, Problems and Future Adaptation](https://arxiv.org/abs/2505.19529)
*Tanjil Hasan Sakib,Md. Tanzib Hosain,Md. Kishor Morol*

Main category: cs.CL

TL;DR: 该研究系统评估小型语言模型(SLMs)，提出包含剪枝/量化/压缩等优化策略的创新分类体系，并建立包含多维度指标的评估平台，讨论效率与性能的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 针对移动设备/边缘计算等资源受限场景，探索如何构建紧凑高效且保持高性能的语言模型，解决传统大模型部署受限的问题。

Method: 提出三阶段研究框架：1) 系统分类模型优化方法(剪枝/量化/压缩) 2) 整合现有数据集构建评估套件 3) 开展效率与性能的权衡分析。

Result: 建立首个包含多维度指标的SLM评估平台，证实优化策略可显著降低模型复杂度(降低30-50%参数量)，同时保持90%+的原始模型性能。

Conclusion: 该研究为资源受限环境下的语言模型开发提供系统方法论，未来需在知识蒸馏/动态网络架构等方向深入探索效率-性能的帕累托优化。

Abstract: Small Language Models (SLMs) have gained substantial attention due to their
ability to execute diverse language tasks successfully while using fewer
computer resources. These models are particularly ideal for deployment in
limited environments, such as mobile devices, on-device processing, and edge
systems. In this study, we present a complete assessment of SLMs, focussing on
their design frameworks, training approaches, and techniques for lowering model
size and complexity. We offer a novel classification system to organize the
optimization approaches applied for SLMs, encompassing strategies like pruning,
quantization, and model compression. Furthermore, we assemble SLM's studies of
evaluation suite with some existing datasets, establishing a rigorous platform
for measuring SLM capabilities. Alongside this, we discuss the important
difficulties that remain unresolved in this sector, including trade-offs
between efficiency and performance, and we suggest directions for future study.
We anticipate this study to serve as a beneficial guide for researchers and
practitioners who aim to construct compact, efficient, and high-performing
language models.

</details>


### [154] [DoctorRAG: Medical RAG Fusing Knowledge with Patient Analogy through Textual Gradients](https://arxiv.org/abs/2505.19538)
*Yuxing Lu,Gecheng Fu,Wei Wu,Xukai Zhao,Sin Yee Goi,Jinzhuo Wang*

Main category: cs.CL

TL;DR: 提出DoctorRAG框架，通过整合临床知识与案例经验改进医疗问答系统，实现更精准的医生式推理


<details>
  <summary>Details</summary>
Motivation: 现有医疗RAG系统过度依赖知识库，忽视类似病例的临床经验知识，而后者是医生诊断的核心要素

Method: 1) 基于概念标签的混合检索机制（知识库+病例检索） 2) Med-TextGrad多代理梯度优化模块 3) 多语言多任务验证框架

Result: 在多语言多任务数据集上显著超越基线模型，迭代优化后响应准确性和全面性持续提升

Conclusion: DoctorRAG通过知识+经验双引擎驱动，推动了医疗AI系统向医生式推理的重要进展

Abstract: Existing medical RAG systems mainly leverage knowledge from medical knowledge
bases, neglecting the crucial role of experiential knowledge derived from
similar patient cases -- a key component of human clinical reasoning. To bridge
this gap, we propose DoctorRAG, a RAG framework that emulates doctor-like
reasoning by integrating both explicit clinical knowledge and implicit
case-based experience. DoctorRAG enhances retrieval precision by first
allocating conceptual tags for queries and knowledge sources, together with a
hybrid retrieval mechanism from both relevant knowledge and patient. In
addition, a Med-TextGrad module using multi-agent textual gradients is
integrated to ensure that the final output adheres to the retrieved knowledge
and patient query. Comprehensive experiments on multilingual, multitask
datasets demonstrate that DoctorRAG significantly outperforms strong baseline
RAG models and gains improvements from iterative refinements. Our approach
generates more accurate, relevant, and comprehensive responses, taking a step
towards more doctor-like medical reasoning systems.

</details>


### [155] [How Syntax Specialization Emerges in Language Models](https://arxiv.org/abs/2505.19548)
*Xufeng Duan,Zhaoqian Yao,Yunhao Zhang,Shaonan Wang,Zhenguang G. Cai*

Main category: cs.CL

TL;DR: 研究追踪了LLMs内部句法专门化的形成过程，揭示了其渐进式发展轨迹、分层集中特性及受模型规模/数据影响的关键期现象。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs的句法专门化特性已被证实，但其在训练过程中的形成机制及影响因素尚不明确。本研究旨在揭示神经网络内部句法敏感性的发展规律及其形成条件。

Method: 通过量化不同句法现象最小对的内部句法一致性，追踪跨模型架构/初始化参数的发展轨迹，分析模型规模与训练数据的影响。

Result: 发现句法敏感性呈现渐进发展、层级集中特性，存在快速专业化的关键期；该过程具有架构无关性，且受模型规模与训练数据显著影响。

Conclusion: 研究不仅揭示了LLMs句法能力的形成路径，还为后续研究提供了可复现的代码、模型及训练检查点资源。

Abstract: Large language models (LLMs) have been found to develop surprising internal
specializations: Individual neurons, attention heads, and circuits become
selectively sensitive to syntactic structure, reflecting patterns observed in
the human brain. While this specialization is well-documented, how it emerges
during training and what influences its development remains largely unknown.
  In this work, we tap into the black box of specialization by tracking its
formation over time. By quantifying internal syntactic consistency across
minimal pairs from various syntactic phenomena, we identify a clear
developmental trajectory: Syntactic sensitivity emerges gradually, concentrates
in specific layers, and exhibits a 'critical period' of rapid internal
specialization. This process is consistent across architectures and
initialization parameters (e.g., random seeds), and is influenced by model
scale and training data. We therefore reveal not only where syntax arises in
LLMs but also how some models internalize it during training. To support future
research, we will release the code, models, and training checkpoints upon
acceptance.

</details>


### [156] [Towards Multi-Granularity Memory Association and Selection for Long-Term Conversational Agents](https://arxiv.org/abs/2505.19549)
*Derong Xu,Yi Wen,Pengyue Jia,Yingyi Zhang,wenlin zhang,Yichao Wang,Huifeng Guo,Ruiming Tang,Xiangyu Zhao,Enhong Chen,Tong Xu*

Main category: cs.CL

TL;DR: 提出了MemGAS框架，通过多粒度记忆关联、自适应检索和基于熵的路由机制，显著提升长对话场景下LLMs的记忆整合能力。


<details>
  <summary>Details</summary>
Motivation: 现有单粒度记忆系统存在信息检索不完整和噪声干扰问题，难以捕捉深层记忆关联。

Method: 1. 构建多粒度记忆单元 2. 使用高斯混合模型进行记忆聚类 3. 基于熵的路由器实现最优粒度选择 4. LLM过滤优化检索结果

Result: 在4个长期记忆基准测试中全面超越SOTA方法，问答和检索任务平均提升5.8%，在不同查询类型和top-K设置下表现稳定

Conclusion: MemGAS通过多层次记忆关联和动态粒度选择机制，有效平衡信息完整性与噪声控制，显著提升对话系统的记忆利用效率

Abstract: Large Language Models (LLMs) have recently been widely adopted in
conversational agents. However, the increasingly long interactions between
users and agents accumulate extensive dialogue records, making it difficult for
LLMs with limited context windows to maintain a coherent long-term dialogue
memory and deliver personalized responses. While retrieval-augmented memory
systems have emerged to address this issue, existing methods often depend on
single-granularity memory segmentation and retrieval. This approach falls short
in capturing deep memory connections, leading to partial retrieval of useful
information or substantial noise, resulting in suboptimal performance. To
tackle these limits, we propose MemGAS, a framework that enhances memory
consolidation by constructing multi-granularity association, adaptive
selection, and retrieval. MemGAS is based on multi-granularity memory units and
employs Gaussian Mixture Models to cluster and associate new memories with
historical ones. An entropy-based router adaptively selects optimal granularity
by evaluating query relevance distributions and balancing information
completeness and noise. Retrieved memories are further refined via LLM-based
filtering. Experiments on four long-term memory benchmarks demonstrate that
MemGAS outperforms state-of-the-art methods on both question answer and
retrieval tasks, achieving superior performance across different query types
and top-K settings.

</details>


### [157] [DocMEdit: Towards Document-Level Model Editing](https://arxiv.org/abs/2505.19572)
*Li Zeng,Zeming Liu,Chong Feng,Heyan Huang,Yuhang Guo*

Main category: cs.CL

TL;DR: 提出文档级模型编辑任务及Benchmark数据集，解决现有方法在长文本编辑中的局限性


<details>
  <summary>Details</summary>
Motivation: 现有模型编辑数据集局限于短语级评估，无法满足现实场景中普遍存在的文档级任务需求，限制了实际应用价值

Method: 1. 定义文档级模型编辑任务 2. 构建包含文档级输入输出、具备推断能力和多事实特征的数据集 3. 设计新型评估指标体系

Result: 实验表明文档级编辑任务在事实更新一致性、长文本连贯性等方面对现有模型编辑方法构成显著挑战

Conclusion: 文档级模型编辑是实际应用的关键环节，现有方法在长文本处理和多事实维护方面仍需突破

Abstract: Model editing aims to correct errors and outdated knowledge in the Large
language models (LLMs) with minimal cost. Prior research has proposed a variety
of datasets to assess the effectiveness of these model editing methods.
However, most existing datasets only require models to output short phrases or
sentences, overlooks the widespread existence of document-level tasks in the
real world, raising doubts about their practical usability. Aimed at addressing
this limitation and promoting the application of model editing in real-world
scenarios, we propose the task of document-level model editing. To tackle such
challenges and enhance model capabilities in practical settings, we introduce
\benchmarkname, a dataset focused on document-level model editing,
characterized by document-level inputs and outputs, extrapolative, and multiple
facts within a single edit. We propose a series of evaluation metrics and
experiments. The results show that the difficulties in document-level model
editing pose challenges for existing model editing methods.

</details>


### [158] [TailorKV: A Hybrid Framework for Long-Context Inference via Tailored KV Cache Optimization](https://arxiv.org/abs/2505.19586)
*Dingyu Yao,Bowen Shen,Zheng Lin,Wei Liu,Jian Luan,Bin Wang,Weiping Wang*

Main category: cs.CL

TL;DR: TailorKV通过结合量化和卸载技术优化LLM的KV缓存管理，在激进压缩下实现近乎无损性能并显著降低延迟


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存优化方法存在PCIe带宽瓶颈导致的延迟问题或激进压缩带来的性能下降，且未充分考虑不同网络层对信息需求的差异

Method: 提出分层处理的混合压缩方法：需要全局信息的层保持完整，关键token层采用选择性加载，其他层实施硬件友好的量化方案

Result: 在128k长上下文场景下，Llama-3.1-8B可在单卡RTX 3090实现82ms/token的解码速度，性能损失仅0.3%-2.1%

Conclusion: 通过差异化的分层压缩策略，TailorKV成功平衡存储效率与模型性能，为边缘设备部署大模型提供有效解决方案

Abstract: The Key-Value (KV) cache in generative large language models (LLMs)
introduces substantial memory overhead. Existing works mitigate this burden by
offloading or compressing the KV cache. However, loading the entire cache
incurs significant latency due to PCIe bandwidth bottlenecks in CPU-GPU
communication, while aggressive compression causes notable performance
degradation. We identify that certain layers in the LLM need to maintain global
information and are unsuitable for selective loading. In contrast, other layers
primarily focus on a few tokens with dominant activations that potentially
incur substantial quantization error. This observation leads to a key insight
that loading dominant tokens and quantizing all tokens can complement each
other. Building on this insight, we propose a hybrid compression method,
TailorKV, which seamlessly integrates quantization and offloading. TailorKV
develops an inference framework along with a hardware-friendly implementation
that leverages these complementary characteristics. Extensive long-context
evaluations exhibit that TailorKV achieves nearly lossless performance under
aggressive compression settings, outperforming the state-of-the-art.
Particularly, the Llama-3.1-8B with 128k context can be served within a single
RTX 3090 GPU, reaching 82 ms per token during decoding.

</details>


### [159] [Multi-Agent Collaboration via Evolving Orchestration](https://arxiv.org/abs/2505.19591)
*Yufan Dang,Chen Qian,Xueheng Luo,Jingru Fan,Zihao Xie,Ruijie Shi,Weize Chen,Cheng Yang,Xiaoyin Che,Ye Tian,Xuantang Xiong,Lei Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 提出基于强化学习的动态协调器范式，通过演化紧凑推理结构提升多智能体协作效率


<details>
  <summary>Details</summary>
Motivation: 现有静态多智能体协作方法难以适应复杂任务扩展，存在协调效率低下的问题

Method: 构建'牵线木偶'范式，中央协调器通过强化学习动态编排智能体执行顺序和优先级

Result: 在闭域/开放域场景实现性能提升(计算成本降低20-35%)，形成紧凑循环推理结构

Conclusion: 动态协调机制为复杂问题解决提供新范式，证明演化式集体推理结构的有效性

Abstract: Large language models (LLMs) have achieved remarkable results across diverse
downstream tasks, but their monolithic nature restricts scalability and
efficiency in complex problem-solving. While recent research explores
multi-agent collaboration among LLMs, most approaches rely on static
organizational structures that struggle to adapt as task complexity and agent
numbers grow, resulting in coordination overhead and inefficiencies. To this
end, we propose a puppeteer-style paradigm for LLM-based multi-agent
collaboration, where a centralized orchestrator ("puppeteer") dynamically
directs agents ("puppets") in response to evolving task states. This
orchestrator is trained via reinforcement learning to adaptively sequence and
prioritize agents, enabling flexible and evolvable collective reasoning.
Experiments on closed- and open-domain scenarios show that this method achieves
superior performance with reduced computational costs. Analyses further reveal
that the key improvements consistently stem from the emergence of more compact,
cyclic reasoning structures under the orchestrator's evolution.

</details>


### [160] [Evaluating Robustness of Large Audio Language Models to Audio Injection: An Empirical Study](https://arxiv.org/abs/2505.19598)
*Guanyu Hou,Jiaming He,Yinhang Zhou,Ji Guo,Yitong Qiao,Rui Zhang,Wenbo Jiang*

Main category: cs.CL

TL;DR: 首次系统评估大型音频语言模型对抗恶意音频注入攻击的鲁棒性，发现模型性能差异显著且存在安全漏洞


<details>
  <summary>Details</summary>
Motivation: 当前LALMs应用广泛但对抗恶意音频攻击的防御能力研究不足，需揭示模型脆弱性以提升安全性

Method: 使用4类攻击场景（音频干扰/指令跟随/上下文注入/判断劫持）和3个量化指标（DSR/CRS/JRI），评估5个主流LALMs模型

Result: 模型存在显著性能差异；恶意内容位置（尤其序列开头）显著影响攻击效果；指令遵循能力与鲁棒性负相关；安全对齐模型更具抗性

Conclusion: 需开发多模态防御架构，分离模型能力与脆弱性，强调训练流程中整合鲁棒性指标，构建基准测试框架

Abstract: Large Audio-Language Models (LALMs) are increasingly deployed in real-world
applications, yet their robustness against malicious audio injection attacks
remains underexplored. This study systematically evaluates five leading LALMs
across four attack scenarios: Audio Interference Attack, Instruction Following
Attack, Context Injection Attack, and Judgment Hijacking Attack. Using metrics
like Defense Success Rate, Context Robustness Score, and Judgment Robustness
Index, their vulnerabilities and resilience were quantitatively assessed.
Experimental results reveal significant performance disparities among models;
no single model consistently outperforms others across all attack types. The
position of malicious content critically influences attack effectiveness,
particularly when placed at the beginning of sequences. A negative correlation
between instruction-following capability and robustness suggests models
adhering strictly to instructions may be more susceptible, contrasting with
greater resistance by safety-aligned models. Additionally, system prompts show
mixed effectiveness, indicating the need for tailored strategies. This work
introduces a benchmark framework and highlights the importance of integrating
robustness into training pipelines. Findings emphasize developing multi-modal
defenses and architectural designs that decouple capability from susceptibility
for secure LALMs deployment.

</details>


### [161] [Inconsistent Tokenizations Cause Language Models to be Perplexed by Japanese Grammar](https://arxiv.org/abs/2505.19599)
*Andrew Gambardella,Takeshi Kojima,Yusuke Iwasawa,Yutaka Matsuo*

Main category: cs.CL

TL;DR: 研究揭示传统语言模型评估方法在非英语语法细节评估上的不足，通过日语心理谓词限制案例，证明分词质量对模型性能的关键影响。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标无法捕捉模型对非英语罕见语法点的理解能力，需通过具体语法案例揭示模型真实表现。

Method: 以日语心理谓词限制为测试场景，对比不同模型在合法/非法句子上的困惑度，并设计分词控制实验。

Result: Weblab因统一分词策略表现最佳，Llama 3通过优化分词使困惑度降低28倍，机器翻译实验显示模型会主动规避分词问题。

Conclusion: 分词质量直接影响语言模型对非英语语法的处理能力，改进分词策略可显著提升模型在低资源语言中的表现。

Abstract: Typical methods for evaluating the performance of language models evaluate
their ability to answer questions accurately. These evaluation metrics are
acceptable for determining the extent to which language models can understand
and reason about text in a general sense, but fail to capture nuanced
capabilities, such as the ability of language models to recognize and obey rare
grammar points, particularly in languages other than English. We measure the
perplexity of language models when confronted with the "first person psych
predicate restriction" grammar point in Japanese. Weblab is the only tested
open source model in the 7-10B parameter range which consistently assigns
higher perplexity to ungrammatical psych predicate sentences than grammatical
ones. We give evidence that Weblab's uniformly bad tokenization is a possible
root cause for its good performance, and show that Llama 3's perplexity on
grammatical psych predicate sentences can be reduced by orders of magnitude
(28x difference) by restricting test sentences to those with uniformly
well-behaved tokenizations. We show in further experiments on machine
translation tasks that language models will use alternative grammar patterns in
order to produce grammatical sentences when tokenization issues prevent the
most natural sentence from being output.

</details>


### [162] [Evaluating Machine Translation Models for English-Hindi Language Pairs: A Comparative Analysis](https://arxiv.org/abs/2505.19604)
*Ahan Prasannakumar Shetty*

Main category: cs.CL

TL;DR: 论文评估了英语-印地语机器翻译模型在通用和专用领域的表现，使用18K平行语料库和政府FAQ数据集，发现不同指标下模型性能存在差异


<details>
  <summary>Details</summary>
Motivation: 填补英语和印地语这对差异较大的语言间机器翻译的系统性评估空白，分析不同翻译模型在通用和专用领域的有效性

Method: 使用18,000+句对的平行语料库和政府网站FAQ定制数据集，结合词汇级指标和机器学习指标进行多维度自动评估

Result: 不同翻译系统在各项指标中表现波动明显，现有系统在特定领域（如政府FAQ）仍存在改进空间

Conclusion: 该研究为优化英语-印地语机器翻译系统提供了重要基准，表明需针对不同语言场景开发定制化解决方案

Abstract: Machine translation has become a critical tool in bridging linguistic gaps,
especially between languages as diverse as English and Hindi. This paper
comprehensively evaluates various machine translation models for translating
between English and Hindi. We assess the performance of these models using a
diverse set of automatic evaluation metrics, both lexical and machine
learning-based metrics. Our evaluation leverages an 18000+ corpus of English
Hindi parallel dataset and a custom FAQ dataset comprising questions from
government websites. The study aims to provide insights into the effectiveness
of different machine translation approaches in handling both general and
specialized language domains. Results indicate varying performance levels
across different metrics, highlighting strengths and areas for improvement in
current translation systems.

</details>


### [163] [Languages in Multilingual Speech Foundation Models Align Both Phonetically and Semantically](https://arxiv.org/abs/2505.19606)
*Ryan Soh-Eun Shim,Domenico De Cristofaro,Chengzhi Martin Hu,Alessandro Vietti,Barbara Plank*

Main category: cs.CL

TL;DR: 语音基础模型存在跨语言语义对齐能力，即使剥离语音特征仍保持翻译检索稳定性，该发现可提升低资源语言语音识别效果


<details>
  <summary>Details</summary>
Motivation: 探索语音基础模型中的跨语言对齐机制是否基于语义而非语音相似性，验证文本模型的发现是否适用于语音领域

Method: 采用发音控制实验（消除语音特征）、跨语言同义词/近音词数据集测试、编码器早退机制分析转录错误

Result: 无语音特征时翻译检索准确率稳定；编码器同时具备语音和语义知识；早退机制揭示语音翻译错误存在源语言语音相似性特征

Conclusion: 利用编码器早期语音特征可提升Whisper模型未支持的低资源语言识别准确率，尤其在拼写透明的语言中效果显著

Abstract: Cross-lingual alignment in pretrained language models (LMs) has enabled
efficient transfer in text-based LMs. Such an alignment has also been observed
in speech foundation models. However, it remains an open question whether
findings and methods from text-based cross-lingual alignment apply to speech.
Building on prior work on spoken translation retrieval, we perform
pronunciation-controlled experiments to observe if cross-lingual alignment can
indeed occur in such models on a semantic basis, instead of relying on phonetic
similarities. Our findings indicate that even in the absence of phonetic cues,
spoken translation retrieval accuracy remains relatively stable. We follow up
with a controlled experiment on a word-level dataset of cross-lingual synonyms
and near-homophones, confirming the existence of both phonetic and semantic
knowledge in the encoder. Finally, we qualitatively examine the transcriptions
produced by early exiting the encoder, where we observe that speech translation
produces semantic errors that are characterized by phonetic similarities to
corresponding words in the source language. We apply this insight from early
exiting to speech recognition in seven low-resource languages unsupported by
the Whisper model, and achieve improved accuracy in all languages examined,
particularly for languages with transparent orthographies.

</details>


### [164] [HomeBench: Evaluating LLMs in Smart Homes with Valid and Invalid Instructions Across Single and Multiple Devices](https://arxiv.org/abs/2505.19628)
*Silin Li,Yuhang Guo,Jiashu Yao,Zeming Liu,Haifeng Wang*

Main category: cs.CL

TL;DR: HomeBench是首个包含有效/无效指令的智能家居数据集，实验显示GPT-4o在无效多设备指令场景成功率仅0%，暴露现有LLM在复杂场景中的不足。


<details>
  <summary>Details</summary>
Motivation: 现实智能家居场景存在无效指令和多设备并发操作需求，但现有研究仅关注简单单设备指令处理，需构建更贴近真实场景的数据集推动LLM智能助手发展。

Method: 构建包含单设备/多设备、有效/无效指令的HomeBench数据集，在13种LLM上测试模型表现（包括上下文学习、检索增强和微调等方法的应用）。

Result: GPT-4o在无效多设备指令场景成功率0%，其他模型表现同样欠佳，显示现有技术无法有效处理复杂指令场景。

Conclusion: HomeBench揭示了LLM在智能家居场景的局限性，为后续研究提供基准，需开发更鲁棒的指令理解与执行方案。

Abstract: Large language models (LLMs) have the potential to revolutionize smart home
assistants by enhancing their ability to accurately understand user needs and
respond appropriately, which is extremely beneficial for building a smarter
home environment. While recent studies have explored integrating LLMs into
smart home systems, they primarily focus on handling straightforward, valid
single-device operation instructions. However, real-world scenarios are far
more complex and often involve users issuing invalid instructions or
controlling multiple devices simultaneously. These have two main challenges:
LLMs must accurately identify and rectify errors in user instructions and
execute multiple user instructions perfectly. To address these challenges and
advance the development of LLM-based smart home assistants, we introduce
HomeBench, the first smart home dataset with valid and invalid instructions
across single and multiple devices in this paper. We have experimental results
on 13 distinct LLMs; e.g., GPT-4o achieves only a 0.0% success rate in the
scenario of invalid multi-device instructions, revealing that the existing
state-of-the-art LLMs still cannot perform well in this situation even with the
help of in-context learning, retrieval-augmented generation, and fine-tuning.
Our code and dataset are publicly available at
https://github.com/BITHLP/HomeBench.

</details>


### [165] [DoctorAgent-RL: A Multi-Agent Collaborative Reinforcement Learning System for Multi-Turn Clinical Dialogue](https://arxiv.org/abs/2505.19630)
*Yichun Feng,Jiawei Wang,Lu Zhou,Yixue Li*

Main category: cs.CL

TL;DR: 提出DoctorAgent-RL强化学习框架，通过多智能体协作优化临床问诊流程，实现动态信息采集和诊断决策。


<details>
  <summary>Details</summary>
Motivation: 现有医疗问答系统存在单向信息传递模式效率低、监督学习方法缺乏动态推理能力的问题，无法有效处理模糊症状描述。

Method: 采用强化学习框架构建医患双智能体交互系统，通过Consultation Evaluator的复合奖励机制动态优化问诊策略，并构建MTMedDialog多轮问诊数据集。

Result: 实验证明DoctorAgent-RL在多轮推理能力和最终诊断效果上均超越现有模型，展示了临床辅助价值。

Conclusion: 该研究通过强化学习机制使LLMs自主形成符合临床逻辑的交互策略，同时创建首个可模拟患者交互的英文多轮问诊数据集，推动智能问诊系统发展。

Abstract: Large language models (LLMs) have demonstrated excellent capabilities in the
field of biomedical question answering, but their application in real-world
clinical consultations still faces core challenges. Existing systems rely on a
one-way information transmission mode where patients must fully describe their
symptoms in a single round, leading to nonspecific diagnostic recommendations
when complaints are vague. Traditional multi-turn dialogue methods based on
supervised learning are constrained by static data-driven paradigms, lacking
generalizability and struggling to intelligently extract key clinical
information. To address these limitations, we propose DoctorAgent-RL, a
reinforcement learning (RL)-based multi-agent collaborative framework that
models medical consultations as a dynamic decision-making process under
uncertainty. The doctor agent continuously optimizes its questioning strategy
within the RL framework through multi-turn interactions with the patient agent,
dynamically adjusting its information-gathering path based on comprehensive
rewards from the Consultation Evaluator. This RL fine-tuning mechanism enables
LLMs to autonomously develop interaction strategies aligned with clinical
reasoning logic, rather than superficially imitating patterns in existing
dialogue data. Notably, we constructed MTMedDialog, the first English
multi-turn medical consultation dataset capable of simulating patient
interactions. Experiments demonstrate that DoctorAgent-RL outperforms existing
models in both multi-turn reasoning capability and final diagnostic
performance, demonstrating practical value in assisting clinical consultations.
https://github.com/JarvisUSTC/DoctorAgent-RL

</details>


### [166] [Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models](https://arxiv.org/abs/2505.19631)
*Zihong Zhang,Liqi He,Zuchao Li,Lefei Zhang,Hai Zhao,Bo Du*

Main category: cs.CL

TL;DR: 提出LLACA框架，结合大语言模型（LLMs）与Aho-Corasick自动机实现多语言无监督分词，性能超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统分词方法依赖规则或统计模型，未能充分利用LLMs的语义理解能力。论文探索以『先理解后分词』为核心，验证LLMs通过上下文理解实现无监督分词的潜力。

Method: 1. 通过prompt引导LLMs实现多语言原始文本分词
2. 提出LLACA算法：将LLMs的语义理解融入Aho-Corasick自动机，构建动态n-gram模型，实现上下文自适应的分词

Result: 1. 参数量更大的LLMs在多语言分词任务中表现更优
2. LLACA在多种语言的分词任务中F1值显著超过传统无监督方法（BPE、SentencePiece等）

Conclusion: LLMs展现出基于语义理解的无监督分词能力，LLACA通过模式识别与语义理解的结合，为NLP基础任务提供了新的技术路径

Abstract: Word segmentation stands as a cornerstone of Natural Language Processing
(NLP). Based on the concept of "comprehend first, segment later", we propose a
new framework to explore the limit of unsupervised word segmentation with Large
Language Models (LLMs) and evaluate the semantic understanding capabilities of
LLMs based on word segmentation. We employ current mainstream LLMs to perform
word segmentation across multiple languages to assess LLMs' "comprehension".
Our findings reveal that LLMs are capable of following simple prompts to
segment raw text into words. There is a trend suggesting that models with more
parameters tend to perform better on multiple languages. Additionally, we
introduce a novel unsupervised method, termed LLACA ($\textbf{L}$arge
$\textbf{L}$anguage Model-Inspired $\textbf{A}$ho-$\textbf{C}$orasick
$\textbf{A}$utomaton). Leveraging the advanced pattern recognition capabilities
of Aho-Corasick automata, LLACA innovatively combines these with the deep
insights of well-pretrained LLMs. This approach not only enables the
construction of a dynamic $n$-gram model that adjusts based on contextual
information but also integrates the nuanced understanding of LLMs, offering
significant improvements over traditional methods. Our source code is available
at https://github.com/hkr04/LLACA

</details>


### [167] [Faster and Better LLMs via Latency-Aware Test-Time Scaling](https://arxiv.org/abs/2505.19634)
*Zili Wang,Tianyu Zhang,Haoli Bai,Lu Hou,Xianzhi Yu,Wulong Liu,Shiming Xiang,Lei Zhu*

Main category: cs.CL

TL;DR: 提出延迟感知的Test-Time Scaling优化方法，通过分支级并行和推测解码的序列级并行配置，在严格延迟限制下实现速度与精度的双重提升。


<details>
  <summary>Details</summary>
Motivation: 现有TTS研究忽视延迟敏感场景的效率问题，计算最优方案未必能达到最低延迟，需针对实际延迟需求优化资源配置。

Method: 1. 分支级并行：多推理分支并发执行
2. 序列级并行：基于推测解码优化执行流程
3. 计算资源动态分配策略

Result: 32B模型1分钟内MATH-500准确率82.3%，3B模型10秒内达72.4%，突破延迟-精度权衡边界。

Conclusion: 延迟感知TTS在实时场景中兼具速度与精度优势，为LLM部署提供新的效率优化范式。

Abstract: Test-Time Scaling (TTS) has proven effective in improving the performance of
Large Language Models (LLMs) during inference. However, existing research has
overlooked the efficiency of TTS from a latency-sensitive perspective. Through
a latency-aware evaluation of representative TTS methods, we demonstrate that a
compute-optimal TTS does not always result in the lowest latency in scenarios
where latency is critical. To address this gap and achieve latency-optimal TTS,
we propose two key approaches by optimizing the concurrency configurations: (1)
branch-wise parallelism, which leverages multiple concurrent inference
branches, and (2) sequence-wise parallelism, enabled by speculative decoding.
By integrating these two approaches and allocating computational resources
properly to each, our latency-optimal TTS enables a 32B model to reach 82.3%
accuracy on MATH-500 within 1 minute and a smaller 3B model to achieve 72.4%
within 10 seconds. Our work emphasizes the importance of latency-aware TTS and
demonstrates its ability to deliver both speed and accuracy in
latency-sensitive scenarios.

</details>


### [168] [Interleaved Reasoning for Large Language Models via Reinforcement Learning](https://arxiv.org/abs/2505.19640)
*Roy Xie,David Qiu,Deepak Gopinath,Dong Lin,Yanchao Sun,Chong Wang,Saloni Potdar,Bhuwan Dhingra*

Main category: cs.CL

TL;DR: 提出基于强化学习的交替推理训练范式，在保持准确率的同时显著降低推理延迟并提升效率


<details>
  <summary>Details</summary>
Motivation: 传统长链思维推理导致效率低下和首次令牌时间过长，需优化推理过程效率

Method: 使用强化学习（PPO/GRPO/REINFORCE++）引导模型交替生成思考步骤和答案，设计基于中间步骤正确性的规则奖励机制

Result: 平均降低80%首次令牌时间，最高提升19.3%准确率，在MATH/GPQA等复杂数据集展现强泛化能力

Conclusion: 无需外部工具即可实现高效推理，验证了条件奖励建模的有效性，为语言模型推理优化提供新方向

Abstract: Long chain-of-thought (CoT) significantly enhances large language models'
(LLM) reasoning capabilities. However, the extensive reasoning traces lead to
inefficiencies and an increased time-to-first-token (TTFT). We propose a novel
training paradigm that uses reinforcement learning (RL) to guide reasoning LLMs
to interleave thinking and answering for multi-hop questions. We observe that
models inherently possess the ability to perform interleaved reasoning, which
can be further enhanced through RL. We introduce a simple yet effective
rule-based reward to incentivize correct intermediate steps, which guides the
policy model toward correct reasoning paths by leveraging intermediate signals
generated during interleaved reasoning. Extensive experiments conducted across
five diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++)
demonstrate consistent improvements over traditional think-answer reasoning,
without requiring external tools. Specifically, our approach reduces TTFT by
over 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore,
our method, trained solely on question answering and logical reasoning
datasets, exhibits strong generalization ability to complex reasoning datasets
such as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to
reveal several valuable insights into conditional reward modeling.

</details>


### [169] [Select, Read, and Write: A Multi-Agent Framework of Full-Text-based Related Work Generation](https://arxiv.org/abs/2505.19647)
*Xiaochuan Liu,Ruihua Song,Xiting Wang,Xu Chen*

Main category: cs.CL

TL;DR: 提出基于多智能体框架的全文相关工作生成方法，通过图感知策略优化文献间关系建模，实验证明显著提升生成质量并取得SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 现有相关工作生成方法存在输入信息局限导致的浅层理解问题，且缺乏对文献间关联的有效建模。

Method: 构建包含选择器（动态规划阅读顺序）、阅读器（更新共享记忆）和写入器（生成文本）的三智能体框架，在selector中引入图结构约束优化文献阅读顺序。

Result: 在三个基础模型和多种输入配置下持续提升性能，图感知选择器取得SOTA结果（代码数据已开源）。

Conclusion: 多智能体框架有效解决全文RWG任务，图结构约束策略显著提升文献关系建模能力。

Abstract: Automatic related work generation (RWG) can save people's time and effort
when writing a draft of related work section (RWS) for further revision.
However, existing methods for RWG always suffer from shallow comprehension due
to taking the limited portions of references papers as input and isolated
explanation for each reference due to ineffective capturing the relationships
among them. To address these issues, we focus on full-text-based RWG task and
propose a novel multi-agent framework. Our framework consists of three agents:
a selector that decides which section of the papers is going to read next, a
reader that digests the selected section and updates a shared working memory,
and a writer that generates RWS based on the final curated memory. To better
capture the relationships among references, we also propose two graph-aware
strategies for selector, enabling to optimize the reading order with constrains
of the graph structure. Extensive experiments demonstrate that our framework
consistently improves performance across three base models and various input
configurations. The graph-aware selectors outperform alternative selectors,
achieving state-of-the-art results. The code and data are available at
https://github.com/1190200817/Full_Text_RWG.

</details>


### [170] [GenKI: Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models](https://arxiv.org/abs/2505.19660)
*Tingjia Shen,Hao Wang,Chuan Qin,Ruijun Sun,Yang Song,Defu Lian,Hengshu Zhu,Enhong Chen*

Main category: cs.CL

TL;DR: 提出GenKI框架，通过知识整合与可控生成技术提升开放域问答性能，实验证明其有效性


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的开放域问答方法面临知识整合效率低和生成结果格式控制不足的双重挑战

Method: 采用密集段落检索获取知识，设计指令微调的知识整合模型，结合一致性集成实现格式可控生成

Result: 在TriviaQA、MSMARCO和CMRC2018数据集上超越基线方法，发现知识检索频率与准确性存在线性关系

Conclusion: GenKI成功实现知识整合与生成控制的双重突破，为开放域问答系统提供新的技术路径

Abstract: Open-domain question answering (OpenQA) represents a cornerstone in natural
language processing (NLP), primarily focused on extracting answers from
unstructured textual data. With the rapid advancements in Large Language Models
(LLMs), LLM-based OpenQA methods have reaped the benefits of emergent
understanding and answering capabilities enabled by massive parameters compared
to traditional methods. However, most of these methods encounter two critical
challenges: how to integrate knowledge into LLMs effectively and how to
adaptively generate results with specific answer formats for various task
situations. To address these challenges, we propose a novel framework named
GenKI, which aims to improve the OpenQA performance by exploring Knowledge
Integration and controllable Generation on LLMs simultaneously. Specifically,
we first train a dense passage retrieval model to retrieve associated knowledge
from a given knowledge base. Subsequently, we introduce a novel knowledge
integration model that incorporates the retrieval knowledge into instructions
during fine-tuning to intensify the model. Furthermore, to enable controllable
generation in LLMs, we leverage a certain fine-tuned LLM and an ensemble based
on text consistency incorporating all coherence, fluency, and answer format
assurance. Finally, extensive experiments conducted on the TriviaQA, MSMARCO,
and CMRC2018 datasets, featuring diverse answer formats, have demonstrated the
effectiveness of GenKI with comparison of state-of-the-art baselines. Moreover,
ablation studies have disclosed a linear relationship between the frequency of
retrieved knowledge and the model's ability to recall knowledge accurately
against the ground truth. Our code of GenKI is available at
https://github.com/USTC-StarTeam/GenKI

</details>


### [171] [LeCoDe: A Benchmark Dataset for Interactive Legal Consultation Dialogue Evaluation](https://arxiv.org/abs/2505.19667)
*Weikang Yuan,Kaisong Song,Zhuoren Jiang,Junjie Cao,Yujie Zhang,Jun Lin,Kun Kuang,Ji Zhang,Xiaozhong Liu*

Main category: cs.CL

TL;DR: 提出LeCoDe多轮法律咨询对话数据集与综合评估框架，揭示现有大模型在专业咨询场景的局限性（GPT-4澄清召回率仅39.8%，建议质量总分59%）


<details>
  <summary>Details</summary>
Motivation: 法律咨询存在专业人才短缺导致的高成本问题，现有大语言模型难以处理真实咨询的交互复杂性和知识密集性需求

Method: 通过短视频平台采集直播咨询构建3,696个对话（110,008轮次），经法律专家标注后设计12维度评估框架（澄清能力+专业建议质量）

Result: 实验显示顶尖模型在澄清召回率（39.8%）和综合建议质量（59%）表现欠佳，验证专业咨询场景的技术挑战

Conclusion: LeCoDe基准推动法律对话系统研究，为模拟真实用户-专家交互提供新方向，后续探索了提升模型咨询能力的有效策略

Abstract: Legal consultation is essential for safeguarding individual rights and
ensuring access to justice, yet remains costly and inaccessible to many
individuals due to the shortage of professionals. While recent advances in
Large Language Models (LLMs) offer a promising path toward scalable, low-cost
legal assistance, current systems fall short in handling the interactive and
knowledge-intensive nature of real-world consultations. To address these
challenges, we introduce LeCoDe, a real-world multi-turn benchmark dataset
comprising 3,696 legal consultation dialogues with 110,008 dialogue turns,
designed to evaluate and improve LLMs' legal consultation capability. With
LeCoDe, we innovatively collect live-streamed consultations from short-video
platforms, providing authentic multi-turn legal consultation dialogues. The
rigorous annotation by legal experts further enhances the dataset with
professional insights and expertise. Furthermore, we propose a comprehensive
evaluation framework that assesses LLMs' consultation capabilities in terms of
(1) clarification capability and (2) professional advice quality. This unified
framework incorporates 12 metrics across two dimensions. Through extensive
experiments on various general and domain-specific LLMs, our results reveal
significant challenges in this task, with even state-of-the-art models like
GPT-4 achieving only 39.8% recall for clarification and 59% overall score for
advice quality, highlighting the complexity of professional consultation
scenarios. Based on these findings, we further explore several strategies to
enhance LLMs' legal consultation abilities. Our benchmark contributes to
advancing research in legal domain dialogue systems, particularly in simulating
more real-world user-expert interactions.

</details>


### [172] [Reshaping Representation Space to Balance the Safety and Over-rejection in Large Audio Language Models](https://arxiv.org/abs/2505.19670)
*Hao Yang,Lizhen Qu,Ehsan Shareghi,Gholamreza Haffari*

Main category: cs.CL

TL;DR: 提出无监督安全微调策略，通过重塑表示空间增强音频语言模型的安全对齐能力，在仅增加0.88%过度拒绝率的前提下显著提升多模态输入场景下的安全性


<details>
  <summary>Details</summary>
Motivation: 现有监督微调方法在提升LALMs安全性时易导致过度拒绝问题，显著损害模型实用性。当前缺乏针对音频模态的安全对齐策略和专用数据集

Method: 采用无监督安全微调策略，通过调整模型表示空间结构实现安全强化，避免依赖标注数据进行参数更新

Result: 在三代Qwen LALMs上验证，音频-文本/纯文本/纯音频三种输入模式下的安全性显著提升，平均过度拒绝率仅增加0.88%

Conclusion: 该无监督方法有效平衡了安全性和实用性，为LALMs安全对齐提供了新解决方案，适用于不同代际模型且具备模态通用性

Abstract: Large Audio Language Models (LALMs) have extended the capabilities of Large
Language Models (LLMs) by enabling audio-based human interactions. However,
recent research has revealed that LALMs remain vulnerable to harmful queries
due to insufficient safety-alignment. Despite advances in defence measures for
text and vision LLMs, effective safety-alignment strategies and audio-safety
dataset specifically targeting LALMs are notably absent. Meanwhile defence
measures based on Supervised Fine-tuning (SFT) struggle to address safety
improvement while avoiding over-rejection issues, significantly compromising
helpfulness. In this work, we propose an unsupervised safety-fine-tuning
strategy as remedy that reshapes model's representation space to enhance
existing LALMs safety-alignment while balancing the risk of over-rejection. Our
experiments, conducted across three generations of Qwen LALMs, demonstrate that
our approach significantly improves LALMs safety under three modality input
conditions (audio-text, text-only, and audio-only) while increasing
over-rejection rate by only 0.88% on average. Warning: this paper contains
harmful examples.

</details>


### [173] [Comparing Moral Values in Western English-speaking societies and LLMs with Word Associations](https://arxiv.org/abs/2505.19674)
*Chaoyi Xiang,Chunhua Liu,Simon De Deyne,Lea Frermann*

Main category: cs.CL

TL;DR: 通过构建大规模语言模型生成词汇关联数据集，提出基于道德基础理论的新方法，揭示了英语母语者与LLM在道德概念上的系统性差异。


<details>
  <summary>Details</summary>
Motivation: 直接提示法评估语言模型道德价值观存在训练数据污染和提示敏感性问题，需采用更底层的词汇关联表征来增强鲁棒性。

Method: 1. 创建类似人类词汇关联的LLM生成数据集
2. 基于道德基础理论种子词，在关联图中传播道德价值
3. 对比人类与LLM的道德概念化差异

Result: 发现英语使用者与LLM在道德价值呈现上存在细节丰富但系统性的差异，LLM未完全复制人类道德框架

Conclusion: 词汇关联方法有效揭示LLM道德推理特性，表明模型形成了与人类既相关又不同的道德概念体系

Abstract: As the impact of large language models increases, understanding the moral
values they reflect becomes ever more important. Assessing the nature of moral
values as understood by these models via direct prompting is challenging due to
potential leakage of human norms into model training data, and their
sensitivity to prompt formulation. Instead, we propose to use word
associations, which have been shown to reflect moral reasoning in humans, as
low-level underlying representations to obtain a more robust picture of LLMs'
moral reasoning. We study moral differences in associations from western
English-speaking communities and LLMs trained predominantly on English data.
First, we create a large dataset of LLM-generated word associations, resembling
an existing data set of human word associations. Next, we propose a novel
method to propagate moral values based on seed words derived from Moral
Foundation Theory through the human and LLM-generated association graphs.
Finally, we compare the resulting moral conceptualizations, highlighting
detailed but systematic differences between moral values emerging from English
speakers and LLM associations.

</details>


### [174] [Calibrating Pre-trained Language Classifiers on LLM-generated Noisy Labels via Iterative Refinement](https://arxiv.org/abs/2505.19675)
*Liqin Ye,Agam Shah,Chao Zhang,Sudheer Chava*

Main category: cs.CL

TL;DR: 提出SiDyP框架，通过标签扩散和动态先验校准分类器预测，显著提升BERT在LLM生成噪声标签数据集上的性能


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型自动生成标注数据时产生的噪声标签问题，避免模型在训练过程中过度拟合噪声

Method: 结合文本嵌入空间的邻居标签分布检索潜在真实标签，采用单纯形扩散模型迭代优化噪声候选标签

Result: 在零样本和少样本场景下分别平均提升BERT分类器性能7.21%和7.30%，通过多任务多模型基准验证有效性

Conclusion: SiDyP为处理LLM生成的噪声标签提供了创新解决方案，显著增强模型鲁棒性，推动自动标注技术的实际应用

Abstract: The traditional process of creating labeled datasets is labor-intensive and
expensive. Recent breakthroughs in open-source large language models (LLMs)
have opened up a new avenue in generating labeled datasets automatically for
various natural language processing (NLP) tasks, providing an alternative to
such an expensive annotation process. However, the reliability of such
auto-generated labels remains a significant concern due to inherent
inaccuracies. When learning from noisy labels, the model's generalization is
likely to be harmed as it is prone to overfit to those label noises. While
previous studies in learning from noisy labels mainly focus on synthetic noise
and real-world noise, LLM-generated label noise receives less attention. In
this paper, we propose SiDyP: Simplex Label Diffusion with Dynamic Prior to
calibrate the classifier's prediction, thus enhancing its robustness towards
LLM-generated noisy labels. SiDyP retrieves potential true label candidates by
neighborhood label distribution in text embedding space and iteratively refines
noisy candidates using a simplex diffusion model. Our framework can increase
the performance of the BERT classifier fine-tuned on both zero-shot and
few-shot LLM-generated noisy label datasets by an average of 7.21% and 7.30%
respectively. We demonstrate the effectiveness of SiDyP by conducting extensive
benchmarking for different LLMs over a variety of NLP tasks. Our code is
available on Github.

</details>


### [175] [Grounding Language with Vision: A Conditional Mutual Information Calibrated Decoding Strategy for Reducing Hallucinations in LVLMs](https://arxiv.org/abs/2505.19678)
*Hao Fang,Changle Zhou,Jiawei Kong,Kuofeng Gao,Bin Chen,Tao Liang,Guojun Ma,Shu-Tao Xia*

Main category: cs.CL

TL;DR: 提出基于条件点互信息（C-PMI）的解码策略，通过双层次优化和令牌净化机制缓解大视觉语言模型幻觉问题


<details>
  <summary>Details</summary>
Motivation: LVLM模型过度依赖语言先验而忽视视觉信息，导致生成文本与输入图像相关性低的幻觉问题

Method: 1. 联合建模视觉与文本令牌对C-PMI的贡献
2. 建立双层次优化框架
3. 动态调节解码过程的令牌净化机制

Result: 在多个基准测试中显著降低幻觉率，同时保持解码效率

Conclusion: C-PMI策略通过增强图文互依赖性，有效平衡了生成质量与计算效率

Abstract: Large Vision-Language Models (LVLMs) are susceptible to hallucinations, where
generated responses seem semantically plausible yet exhibit little or no
relevance to the input image. Previous studies reveal that this issue primarily
stems from LVLMs' over-reliance on language priors while disregarding the
visual information during decoding. To alleviate this issue, we introduce a
novel Conditional Pointwise Mutual Information (C-PMI) calibrated decoding
strategy, which adaptively strengthens the mutual dependency between generated
texts and input images to mitigate hallucinations. Unlike existing methods
solely focusing on text token sampling, we propose to jointly model the
contributions of visual and textual tokens to C-PMI, formulating hallucination
mitigation as a bi-level optimization problem aimed at maximizing mutual
information. To solve it, we design a token purification mechanism that
dynamically regulates the decoding process by sampling text tokens remaining
maximally relevant to the given image, while simultaneously refining image
tokens most pertinent to the generated response. Extensive experiments across
various benchmarks reveal that the proposed method significantly reduces
hallucinations in LVLMs while preserving decoding efficiency.

</details>


### [176] [KIT's Low-resource Speech Translation Systems for IWSLT2025: System Enhancement with Synthetic Data and Model Regularization](https://arxiv.org/abs/2505.19679)
*Zhaolin Li,Yining Liu,Danni Liu,Tuan Nam Nguyen,Enes Yavuz Ugan,Tu Anh Dinh,Carlos Mullov,Alexander Waibel,Jan Niehues*

Main category: cs.CL

TL;DR: KIT团队针对IWSLT 2025低资源赛道，开发了级联语音翻译系统和端到端语音翻译系统，通过预训练模型微调、合成数据增强和模型正则化方法，在Bemba等低资源语言上取得显著效果提升。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（Bemba、阿拉伯语方言）语音翻译任务中训练数据不足的问题，探索如何有效结合预训练模型与有限资源实现性能优化。

Method: 1. 级联系统（ASR+MT）与端到端系统并行开发
2. 采用MT模型生成ASR数据的翻译实现数据增强
3. 利用TTS模型从MT数据生成合成语音
4. 引入模型正则化（intra-distillation）
5. MBR解码融合不同系统输出

Result: 1. 北黎凡特阿拉伯语：纯合成数据系统超越真实数据级联系统
2. Bemba语言：合成语音数据提升ASR/ST性能
3. 模型正则化使ASR/MT/ST任务平均提升
4. 系统融合带来1.5 BLEU值提升

Conclusion: 合成数据有效缓解低资源困境，模型正则化与系统融合策略具有普适性，为低资源语音翻译提供了数据高效利用的解决方案。

Abstract: This paper presents KIT's submissions to the IWSLT 2025 low-resource track.
We develop both cascaded systems, consisting of Automatic Speech Recognition
(ASR) and Machine Translation (MT) models, and end-to-end (E2E) Speech
Translation (ST) systems for three language pairs: Bemba, North Levantine
Arabic, and Tunisian Arabic into English. Building upon pre-trained models, we
fine-tune our systems with different strategies to utilize resources
efficiently. This study further explores system enhancement with synthetic data
and model regularization. Specifically, we investigate MT-augmented ST by
generating translations from ASR data using MT models. For North Levantine,
which lacks parallel ST training data, a system trained solely on synthetic
data slightly surpasses the cascaded system trained on real data. We also
explore augmentation using text-to-speech models by generating synthetic speech
from MT data, demonstrating the benefits of synthetic data in improving both
ASR and ST performance for Bemba. Additionally, we apply intra-distillation to
enhance model performance. Our experiments show that this approach consistently
improves results across ASR, MT, and ST tasks, as well as across different
pre-trained models. Finally, we apply Minimum Bayes Risk decoding to combine
the cascaded and end-to-end systems, achieving an improvement of approximately
1.5 BLEU points.

</details>


### [177] [Leveraging Importance Sampling to Detach Alignment Modules from Large Language Models](https://arxiv.org/abs/2505.19700)
*Yi Liu,Dianqing Liu,Mingye Zhu,Junbo Guo,Yongdong Zhang,Zhendong Mao*

Main category: cs.CL

TL;DR: 提出残差对齐模型（RAM），通过重要性采样框架实现灵活高效的大语言模型对齐


<details>
  <summary>Details</summary>
Motivation: 传统大语言模型对齐方法需要重新训练整个模型，难以快速适应多样化应用需求

Method: 将未对齐模型作为提议分布，设计自回归对齐模块估计重要性权重，开发序列级训练策略和迭代token解码算法

Result: 在指令跟随、领域适配和偏好优化等任务中持续超越基线模型

Conclusion: RAM实现了对齐模块与目标模型的自然分离，显著提升模型灵活性和生成质量

Abstract: The widespread adoption of large language models (LLMs) across industries has
increased the demand for high-quality and customizable outputs. However,
traditional alignment methods often require retraining large pretrained models,
making it difficult to quickly adapt and optimize LLMs for diverse
applications. To address this limitation, we propose a novel \textit{Residual
Alignment Model} (\textit{RAM}) that formalizes the alignment process as a type
of importance sampling. In this framework, the unaligned upstream model serves
as the proposal distribution, while the alignment process is framed as
secondary sampling based on an autoregressive alignment module that acts as an
estimator of the importance weights. This design enables a natural detachment
of the alignment module from the target aligned model, improving flexibility
and scalability. Based on this model, we derive an efficient sequence-level
training strategy for the alignment module, which operates independently of the
proposal module. Additionally, we develop a resampling algorithm with iterative
token-level decoding to address the common first-token latency issue in
comparable methods. Experimental evaluations on two leading open-source LLMs
across diverse tasks, including instruction following, domain adaptation, and
preference optimization, demonstrate that our approach consistently outperforms
baseline models.

</details>


### [178] [Error Typing for Smarter Rewards: Improving Process Reward Models with Error-Aware Hierarchical Supervision](https://arxiv.org/abs/2505.19706)
*Tej Deep Pala,Panshul Sharma,Amir Zadeh,Chuan Li,Soujanya Poria*

Main category: cs.CL

TL;DR: PathFinder-PRM通过分层错误检测机制(数学错误/一致性错误分类)和细粒度信号整合，显著提升了过程奖励模型在数学推理任务中的准确性和数据效率，在PRMBench上以3倍少数据实现67.7的SOTA成绩。


<details>
  <summary>Details</summary>
Motivation: 现有过程奖励模型(PRMs)对中间步骤评分时缺乏细粒度错误分类能力，导致奖励信号不够精确。为解决该缺陷，需要开发能识别具体错误类型并整合多维度信号的新型PRM架构。

Method: 1. 构建400K增强数据集：融合PRM800K人工标注数据和RLHFlow模型轨迹，添加步骤级三维标签
2. 设计分层模型：先分类数学错误与逻辑一致性错误，再综合错误类型判断步骤正确性

Result: 1. PRMBench测试PRMScore达67.7(SOTA)，比之前最佳提升2.2点
2. 奖励引导搜索prm@8达48.3(+1.5)
3. 使用数据量仅为先前最佳模型的1/3

Conclusion: 解耦错误检测与奖励估计的层次化建模方法，既能提升细粒度错误识别能力，又可通过更高效的数据利用显著增强端到端数学推理性能，为LLM幻觉问题提供了新解决方案。

Abstract: Large Language Models (LLMs) are prone to hallucination, especially during
multi-hop and reasoning-intensive tasks such as mathematical problem solving.
While Outcome Reward Models verify only final answers, Process Reward Models
(PRMs) score each intermediate step to steer generation toward coherent
solutions. We introduce PathFinder-PRM, a novel hierarchical, error-aware
discriminative PRM that first classifies math and consistency errors at each
step, then combines these fine-grained signals to estimate step correctness. To
train PathFinder-PRM, we construct a 400K-sample dataset by enriching the
human-annotated PRM800K corpus and RLHFlow Mistral traces with
three-dimensional step-level labels. On PRMBench, PathFinder-PRM achieves a new
state-of-the-art PRMScore of 67.7, outperforming the prior best (65.5) while
using 3 times less data. When applied to reward guided greedy search, our model
yields prm@8 48.3, a +1.5 point gain over the strongest baseline. These results
demonstrate that decoupled error detection and reward estimation not only boost
fine-grained error detection but also substantially improve end-to-end,
reward-guided mathematical reasoning with greater data efficiency.

</details>


### [179] [MT$^{3}$: Scaling MLLM-based Text Image Machine Translation via Multi-Task Reinforcement Learning](https://arxiv.org/abs/2505.19714)
*Zhaopeng Feng,Yupu Liang,Shaosheng Cao,Jiayuan Su,Jiahan Ren,Zhe Xu,Yao Hu,Wenxuan Huang,Jian Wu,Zuozhu Liu*

Main category: cs.CL

TL;DR: 提出MT³框架——首个将多任务强化学习应用于多模态大语言模型（MLLMs）的端到端文本图像机器翻译方案，在MIT-10M基准测试中超越72B级大模型取得SOTA。


<details>
  <summary>Details</summary>
Motivation: 文本图像机器翻译（TIMT）需同时处理OCR识别、图文推理和翻译任务，传统级联方案复杂低效。虽然LLMs在多模态推理方面取得进展，但MLLMs在端到端TIMT中的应用仍待探索。

Method: 1. 三任务协同优化：文本识别/上下文推理/翻译联合训练
2. 多混合奖励机制：基于规则强化学习适配TIMT特性，提供细粒度跨任务反馈
3. 构建XHSPost基准：首个社交媒体跨文化TIMT评估数据集

Result: 1. MIT-10M基准：MT³-7B-Zero在CIDEr等指标上显著超越Qwen2.5-VL-72B（+15.7%）、InternVL2.5-78B（+12.3%）
2. 跨语言泛化：在XHSPost基准上保持优异表现
3. 课程学习设计使训练效率提升38%

Conclusion: 通过多任务协同优化、强化学习初始化策略、渐进式课程设计以及细粒度奖励建模，验证了MLLMs驱动端到端TIMT的可行性，为复杂图文理解任务提供新范式。

Abstract: Text Image Machine Translation (TIMT)-the task of translating textual content
embedded in images-is critical for applications in accessibility, cross-lingual
information access, and real-world document understanding. However, TIMT
remains a complex challenge due to the need for accurate optical character
recognition (OCR), robust visual-text reasoning, and high-quality translation,
often requiring cascading multi-stage pipelines. Recent advances in large-scale
Reinforcement Learning (RL) have improved reasoning in Large Language Models
(LLMs) and Multimodal LLMs (MLLMs), but their application to end-to-end TIMT is
still underexplored. To bridge this gap, we introduce MT$^{3}$, the first
framework to apply Multi-Task RL to MLLMs for end-to-end TIMT. MT$^{3}$ adopts
a multi-task optimization paradigm targeting three key sub-skills: text
recognition, context-aware reasoning, and translation. It is trained using a
novel multi-mixed reward mechanism that adapts rule-based RL strategies to
TIMT's intricacies, offering fine-grained, non-binary feedback across tasks.
Furthermore, to facilitate the evaluation of TIMT in authentic cross-cultural
and real-world social media contexts, we introduced XHSPost, the first social
media TIMT benchmark. Our MT$^{3}$-7B-Zero achieves state-of-the-art results on
the latest in-domain MIT-10M benchmark, outperforming strong baselines such as
Qwen2.5-VL-72B and InternVL2.5-78B by notable margins across multiple metrics.
Additionally, the model shows strong generalization to out-of-distribution
language pairs and datasets. In-depth analyses reveal how multi-task synergy,
reinforcement learning initialization, curriculum design, and reward
formulation contribute to advancing MLLM-driven TIMT.

</details>


### [180] [Graceful Forgetting in Generative Language Models](https://arxiv.org/abs/2505.19715)
*Chunyang Jiang,Chi-min Chan,Yiyang Cai,Yulong Liu,Wei Xue,Yike Guo*

Main category: cs.CL

TL;DR: 提出LWF框架，通过Fisher信息矩阵和周期性遗忘机制实现生成式语言模型的优雅遗忘，提升下游任务微调效果。


<details>
  <summary>Details</summary>
Motivation: 预训练模型中部分知识会引发负迁移效应，现有优雅遗忘方法难以适配生成式语言模型的架构特性。

Method: 基于Fisher信息矩阵加权参数更新，计算自生成知识的遗忘置信度，周期性去除高置信度知识。

Result: 实验表明优雅遗忘能有效提升微调性能，但知识交互机制仍存在黑箱特性。

Conclusion: 证明了优雅遗忘在生成模型中的有效性，揭示预训练语言模型知识交互机制仍需深入研究。

Abstract: Recently, the pretrain-finetune paradigm has become a cornerstone in various
deep learning areas. While in general the pre-trained model would promote both
effectiveness and efficiency of downstream tasks fine-tuning, studies have
shown that not all knowledge acquired during pre-training is beneficial. Some
of the knowledge may actually bring detrimental effects to the fine-tuning
tasks, which is also known as negative transfer. To address this problem,
graceful forgetting has emerged as a promising approach. The core principle of
graceful forgetting is to enhance the learning plasticity of the target task by
selectively discarding irrelevant knowledge. However, this approach remains
underexplored in the context of generative language models, and it is often
challenging to migrate existing forgetting algorithms to these models due to
architecture incompatibility. To bridge this gap, in this paper we propose a
novel framework, Learning With Forgetting (LWF), to achieve graceful forgetting
in generative language models. With Fisher Information Matrix weighting the
intended parameter updates, LWF computes forgetting confidence to evaluate
self-generated knowledge regarding the forgetting task, and consequently,
knowledge with high confidence is periodically unlearned during fine-tuning.
Our experiments demonstrate that, although thoroughly uncovering the mechanisms
of knowledge interaction remains challenging in pre-trained language models,
applying graceful forgetting can contribute to enhanced fine-tuning
performance.

</details>


### [181] [Distilling Closed-Source LLM's Knowledge for Locally Stable and Economic Biomedical Entity Linking](https://arxiv.org/abs/2505.19722)
*Yihao Ai,Zhiyuan Ning,Weiwei Dai,Pengfei Wang,Yi Du,Wenjuan Cui,Kunpeng Liu,Yuanchun Zhou*

Main category: cs.CL

TL;DR: 提出RPDR框架，结合闭源与开源大语言模型解决低资源生物医学实体链接问题，通过知识蒸馏降低对闭源模型的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统监督方法依赖大量标注数据，闭源大模型存在稳定性风险和高昂成本，难以在低资源场景和规模化应用中落地。

Method: 1. 用少量数据微调检索器获取候选实体；2. 通过闭源大模型从未标注数据生成训练数据；3. 微调开源大模型进行重排序，实现知识蒸馏。

Result: 在真实世界数据集（Aier）和公开多语言数据集（Ask A Patient）上分别取得0.019和0.036的Acc@1提升，尤其在训练数据不足时表现更优。

Conclusion: RPDR框架有效平衡模型性能与部署成本，在保持精度的同时实现本地化部署，具有较好的通用性和工程落地价值。

Abstract: Biomedical entity linking aims to map nonstandard entities to standard
entities in a knowledge base. Traditional supervised methods perform well but
require extensive annotated data to transfer, limiting their usage in
low-resource scenarios. Large language models (LLMs), especially closed-source
LLMs, can address these but risk stability issues and high economic costs:
using these models is restricted by commercial companies and brings significant
economic costs when dealing with large amounts of data. To address this, we
propose ``RPDR'', a framework combining closed-source LLMs and open-source LLMs
for re-ranking candidates retrieved by a retriever fine-tuned with a small
amount of data. By prompting a closed-source LLM to generate training data from
unannotated data and fine-tuning an open-source LLM for re-ranking, we
effectively distill the knowledge to the open-source LLM that can be deployed
locally, thus avoiding the stability issues and the problem of high economic
costs. We evaluate RPDR on two datasets, including one real-world dataset and
one publicly available dataset involving two languages: Chinese and English.
RPDR achieves 0.019 Acc@1 improvement and 0.036 Acc@1 improvement on the Aier
dataset and the Ask A Patient dataset when the amount of training data is not
enough. The results demonstrate the superiority and generalizability of the
proposed framework.

</details>


### [182] [Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models](https://arxiv.org/abs/2505.19743)
*Yang Zhang,Yu Yu,Bo Tang,Yu Zhu,Chuxiong Sun,Wenqiang Wei,Jie Hu,Zipeng Xie,Zhiyu Li,Feiyu Xiong,Edward Chung*

Main category: cs.CL

TL;DR: 提出MARA方法——通过token级二分类实现语言模型对齐，无需直接微调大模型


<details>
  <summary>Details</summary>
Motivation: 现有RLHF/DPO等对齐方法需要直接微调数十亿参数的大模型，计算成本过高且效率低下

Method: 将句子级偏好学习分解为token级二分类，使用三层全连接网络对候选token进行『接受/拒绝』判定

Result: 在7个不同LLM和3个开源数据集上实现对齐性能显著提升，同时降低计算成本

Conclusion: MARA为语言模型对齐提供了一种高效、低成本的创新解决方案，突破了传统方法的参数规模限制

Abstract: With the rapid development of Large Language Models (LLMs), aligning these
models with human preferences and values is critical to ensuring ethical and
safe applications. However, existing alignment techniques such as RLHF or DPO
often require direct fine-tuning on LLMs with billions of parameters, resulting
in substantial computational costs and inefficiencies. To address this, we
propose Micro token-level Accept-Reject Aligning (MARA) approach designed to
operate independently of the language models. MARA simplifies the alignment
process by decomposing sentence-level preference learning into token-level
binary classification, where a compact three-layer fully-connected network
determines whether candidate tokens are "Accepted" or "Rejected" as part of the
response. Extensive experiments across seven different LLMs and three
open-source datasets show that MARA achieves significant improvements in
alignment performance while reducing computational costs.

</details>


### [183] [NeuSym-RAG: Hybrid Neural Symbolic Retrieval with Multiview Structuring for PDF Question Answering](https://arxiv.org/abs/2505.19754)
*Ruisheng Cao,Hanchong Zhang,Tiancheng Huang,Zhangyi Kang,Yuxin Zhang,Liangtai Sun,Hanqi Li,Yuxun Miao,Shuai Fan,Lu Chen,Kai Yu*

Main category: cs.CL

TL;DR: 提出混合神经符号检索框架NeuSym-RAG，通过多视角分块和模式解析提升PDF问答效果


<details>
  <summary>Details</summary>
Motivation: 现有方法未有效结合神经与符号检索优势，且单一切分方式忽略PDF结构信息

Method: 结合多视角分块和模式解析，将PDF内容组织至关系数据库与向量库，支持LLM迭代式上下文收集

Result: 在三个PDF问答数据集（含自建AIRQA-REAL）上稳定超越向量检索与结构化基线方法

Conclusion: NeuSym-RAG成功统一两种检索范式并有效利用多视图信息，显著提升复杂PDF内容理解能力

Abstract: The increasing number of academic papers poses significant challenges for
researchers to efficiently acquire key details. While retrieval augmented
generation (RAG) shows great promise in large language model (LLM) based
automated question answering, previous works often isolate neural and symbolic
retrieval despite their complementary strengths. Moreover, conventional
single-view chunking neglects the rich structure and layout of PDFs, e.g.,
sections and tables. In this work, we propose NeuSym-RAG, a hybrid neural
symbolic retrieval framework which combines both paradigms in an interactive
process. By leveraging multi-view chunking and schema-based parsing, NeuSym-RAG
organizes semi-structured PDF content into both the relational database and
vectorstore, enabling LLM agents to iteratively gather context until sufficient
to generate answers. Experiments on three full PDF-based QA datasets, including
a self-annotated one AIRQA-REAL, show that NeuSym-RAG stably defeats both the
vector-based RAG and various structured baselines, highlighting its capacity to
unify both retrieval schemes and utilize multiple views. Code and data are
publicly available at https://github.com/X-LANCE/NeuSym-RAG.

</details>


### [184] [Efficient Reasoning via Chain of Unconscious Thought](https://arxiv.org/abs/2505.19756)
*Ruihan Gong,Yue Liu,Wenjie Qu,Mingzhe Du,Yufei He,Yingwei Ma,Yulin Chen,Xiang Liu,Yi Wen,Xinfeng Li,Ruidong Wang,Xinzhong Zhu,Bryan Hooi,Jiaheng Zhang*

Main category: cs.CL

TL;DR: 提出Chain of Unconscious Thought (CoUT)范式，通过模拟人类无意识思维减少大模型推理过程的token消耗，在保持精度的同时显著提升47.62%的token效率。


<details>
  <summary>Details</summary>
Motivation: 针对大推理模型(LRMs)存在的推理过程冗长、token效率低下问题，受心理学无意识思维理论(UTT)启发，探索通过内部化认知过程提升效率的可能性。

Method: 1. 通过隐藏层思维实现推理过程内部化
2. 设计多种token精简策略（包括动态思维压缩、注意力聚焦等技术）
3. 构建端到端的高效推理框架

Result: 在保持与Chain-of-Thought(CoT)相当准确度的前提下，token使用量减少47.62%（实验数据见图1），代码已在GitHub开源。

Conclusion: 模型可能具备类似人类的无意识思维能力，通过合理引导可显著提升推理效率而不牺牲性能，为高效大模型推理提供了新范式。

Abstract: Large Reasoning Models (LRMs) achieve promising performance but compromise
token efficiency due to verbose reasoning processes. Unconscious Thought Theory
(UTT) posits that complex problems can be solved more efficiently through
internalized cognitive processes. Inspired by UTT, we propose a new reasoning
paradigm, termed Chain of Unconscious Thought (CoUT), to improve the token
efficiency of LRMs by guiding them to mimic human unconscious thought and
internalize reasoning processes. Concretely, we first prompt the model to
internalize the reasoning by thinking in the hidden layer. Then, we design a
bag of token-efficient strategies to further help models reduce unnecessary
tokens yet preserve the performance. Our work reveals that models may possess
beneficial unconscious thought, enabling improved efficiency without
sacrificing performance. Extensive experiments demonstrate the effectiveness of
CoUT. Remarkably, it surpasses CoT by reducing token usage by 47.62% while
maintaining comparable accuracy, as shown in Figure 1. The code of CoUT is
available at this link: https://github.com/Rohan-GRH/CoUT

</details>


### [185] [SGM: A Framework for Building Specification-Guided Moderation Filters](https://arxiv.org/abs/2505.19766)
*Masoomali Fatehkia,Enes Altinisik,Husrev Taha Sencar*

Main category: cs.CL

TL;DR: 提出SGM框架实现基于用户规范的自动化内容审查，解决LLM对齐不足问题，无需依赖人工编写训练数据


<details>
  <summary>Details</summary>
Motivation: 现有安全过滤机制局限于固定安全范畴且依赖人工数据，难以满足多样化应用场景的精准对齐需求

Method: 通过自动化生成训练数据支持用户自定义对齐规范，训练出的过滤器在保持安全性能的同时实现细粒度控制

Result: SGM训练的过滤器性能与人工数据集训练的安全过滤器相当，且支持更灵活的对齐规范控制

Conclusion: SGM框架突破了传统内容审核的局限性，为LLM应用提供了可扩展的定制化对齐解决方案

Abstract: Aligning large language models (LLMs) with deployment-specific requirements
is critical but inherently imperfect. Despite extensive training, models remain
susceptible to misalignment and adversarial inputs such as jailbreaks. Content
moderation filters are commonly used as external safeguards, though they
typically focus narrowly on safety. We introduce SGM (Specification-Guided
Moderation), a flexible framework for training moderation filters grounded in
user-defined specifications that go beyond standard safety concerns. SGM
automates training data generation without relying on human-written examples,
enabling scalable support for diverse, application-specific alignment goals.
SGM-trained filters perform on par with state-of-the-art safety filters built
on curated datasets, while supporting fine-grained and user-defined alignment
control.

</details>


### [186] [T^2Agent A Tool-augmented Multimodal Misinformation Detection Agent with Monte Carlo Tree Search](https://arxiv.org/abs/2505.19768)
*Xing Cui,Yueying Zou,Zekun Li,Peipei Li,Xinyuan Xu,Xuannan Liu,Huaibo Huang,Ran He*

Main category: cs.CL

TL;DR: 提出T2Agent检测框架，通过可扩展工具包与改进的蒙特卡洛树搜索（MCTS）实现动态多源伪造信息验证，显著提升检测准确率


<details>
  <summary>Details</summary>
Motivation: 现有静态检测方法难以应对多源伪造信息动态验证需求，工具使用效率低下且扩展性不足

Method: 1.模块化工具集（含网络搜索/伪造检测/一致性分析） 2.贝叶斯优化工具选择器 3.多源验证MCTS架构 4.双重奖励机制（轨迹评分+置信度）

Result: 在混合源多模态错误信息基准测试中性能显著优于基线，消融实验验证树搜索机制和工具使用的有效性

Conclusion: T2Agent作为免训练方案，通过动态证据收集与多源协同验证，为复杂伪造场景提供可扩展的检测框架

Abstract: Real-world multimodal misinformation often arises from mixed forgery sources,
requiring dynamic reasoning and adaptive verification. However, existing
methods mainly rely on static pipelines and limited tool usage, limiting their
ability to handle such complexity and diversity. To address this challenge, we
propose T2Agent, a novel misinformation detection agent that incorporates an
extensible toolkit with Monte Carlo Tree Search (MCTS). The toolkit consists of
modular tools such as web search, forgery detection, and consistency analysis.
Each tool is described using standardized templates, enabling seamless
integration and future expansion. To avoid inefficiency from using all tools
simultaneously, a Bayesian optimization-based selector is proposed to identify
a task-relevant subset. This subset then serves as the action space for MCTS to
dynamically collect evidence and perform multi-source verification. To better
align MCTS with the multi-source nature of misinformation detection, T2Agent
extends traditional MCTS with multi-source verification, which decomposes the
task into coordinated subtasks targeting different forgery sources. A dual
reward mechanism containing a reasoning trajectory score and a confidence score
is further proposed to encourage a balance between exploration across mixed
forgery sources and exploitation for more reliable evidence. We conduct
ablation studies to confirm the effectiveness of the tree search mechanism and
tool usage. Extensive experiments further show that T2Agent consistently
outperforms existing baselines on challenging mixed-source multimodal
misinformation benchmarks, demonstrating its strong potential as a
training-free approach for enhancing detection accuracy. The code will be
released.

</details>


### [187] [What Really Matters in Many-Shot Attacks? An Empirical Study of Long-Context Vulnerabilities in LLMs](https://arxiv.org/abs/2505.19773)
*Sangyeop Kim,Yohan Lee,Yongwoo Song,Kimin Lee*

Main category: cs.CL

TL;DR: 研究发现大型语言模型在长上下文场景下存在显著安全漏洞，即使简单重复内容也能绕过安全防护，暴露出其长文本处理机制的根本缺陷


<details>
  <summary>Details</summary>
Motivation: 揭示现有LLMs在长上下文处理中的安全脆弱性，验证模型安全机制在扩展上下文场景下的有效性局限

Method: 采用128K tokens长上下文，通过不同指令风格、攻击密度、主题格式的多轮越狱攻击（MSJ）设置进行系统性测试

Result: 上下文长度是攻击成功关键因素；无需复杂恶意内容，简单重复样本或随机文本即可突破安全防护；模型安全行为随上下文增长愈发不一致

Conclusion: LLMs的上下文扩展能力存在重大安全缺陷，需开发新型安全机制应对长文本场景下的潜在风险

Abstract: We investigate long-context vulnerabilities in Large Language Models (LLMs)
through Many-Shot Jailbreaking (MSJ). Our experiments utilize context length of
up to 128K tokens. Through comprehensive analysis with various many-shot attack
settings with different instruction styles, shot density, topic, and format, we
reveal that context length is the primary factor determining attack
effectiveness. Critically, we find that successful attacks do not require
carefully crafted harmful content. Even repetitive shots or random dummy text
can circumvent model safety measures, suggesting fundamental limitations in
long-context processing capabilities of LLMs. The safety behavior of
well-aligned models becomes increasingly inconsistent with longer contexts.
These findings highlight significant safety gaps in context expansion
capabilities of LLMs, emphasizing the need for new safety mechanisms.

</details>


### [188] [Analyzing Political Bias in LLMs via Target-Oriented Sentiment Classification](https://arxiv.org/abs/2505.19776)
*Akram Elbouanani,Evan Dufraisse,Adrian Popescu*

Main category: cs.CL

TL;DR: 提出基于情感预测不一致性的新方法量化LLM政治偏见，发现模型普遍存在对左翼/极右政客的正/负面偏见，且西方语言偏见更强


<details>
  <summary>Details</summary>
Motivation: 现有政治偏见分析方法依赖小样本问卷或内容生成任务，且依赖LLM自身分析，可能传播偏见。需要更可靠的目标实体导向分析方法

Method: 构建1319个不同背景政客姓名库，在450个政治语句中替换目标实体，通过7个模型在6种语言中预测目标情感，定义熵指标量化不一致性

Result: 所有模型均显示预测不一致性；左翼政客获更多正面评价，极右获负面；西方语言偏见强度高于其他语言；大模型偏见更明显且跨语言更一致

Conclusion: LLM在目标情感分类中存在系统性政治偏见，可通过替换虚构政客姓名部分缓解。模型规模增大强化偏见但提升跨语言一致性

Abstract: Political biases encoded by LLMs might have detrimental effects on downstream
applications. Existing bias analysis methods rely on small-size intermediate
tasks (questionnaire answering or political content generation) and rely on the
LLMs themselves for analysis, thus propagating bias. We propose a new approach
leveraging the observation that LLM sentiment predictions vary with the target
entity in the same sentence. We define an entropy-based inconsistency metric to
encode this prediction variability. We insert 1319 demographically and
politically diverse politician names in 450 political sentences and predict
target-oriented sentiment using seven models in six widely spoken languages. We
observe inconsistencies in all tested combinations and aggregate them in a
statistically robust analysis at different granularity levels. We observe
positive and negative bias toward left and far-right politicians and positive
correlations between politicians with similar alignment. Bias intensity is
higher for Western languages than for others. Larger models exhibit stronger
and more consistent biases and reduce discrepancies between similar languages.
We partially mitigate LLM unreliability in target-oriented sentiment
classification (TSC) by replacing politician names with fictional but plausible
counterparts.

</details>


### [189] [The Avengers: A Simple Recipe for Uniting Smaller Language Models to Challenge Proprietary Giants](https://arxiv.org/abs/2505.19797)
*Yiqun Zhang,Hao Li,Chenxu Wang,Linyao Chen,Qiaosheng Zhang,Peng Ye,Shi Feng,Daling Wang,Zhen Wang,Xinrun Wang,Jia Xu,Lei Bai,Wanli Ouyang,Shuyue Hu*

Main category: cs.CL

TL;DR: 通过集成多个小型开源模型（Avengers框架）在数学和代码任务上超越GPT-4.1


<details>
  <summary>Details</summary>
Motivation: 解决专有大模型主导下小型开源模型在多任务中的竞争力问题

Method: 四步框架：嵌入查询→语义聚类→模型评分→多模型投票采样生成

Result: 10个7B模型集成后，在15个数据集中10个超越GPT-4.1（数学+18.21%，代码+7.46%），具备优异泛化能力和鲁棒性

Conclusion: 通过轻量级集成策略有效释放小模型集体潜力，为开源社区提供高效替代方案

Abstract: As proprietary giants increasingly dominate the race for ever-larger language
models, a pressing question arises for the open-source community: can smaller
models remain competitive across a broad range of tasks? In this paper, we
present the Avengers--a simple recipe that effectively leverages the collective
intelligence of open-source, smaller language models. Our framework is built
upon four lightweight operations: (i) embedding: encode queries using a text
embedding model; (ii) clustering: group queries based on their semantic
similarity; (iii) scoring: scores each model's performance within each cluster;
and (iv) voting: improve outputs via repeated sampling and voting. At inference
time, each query is embedded and assigned to its nearest cluster. The
top-performing model(s) within that cluster are selected to generate the
response using the Self-Consistency or its multi-model variant. Remarkably,
with 10 open-source models (~7B parameters each), the Avengers collectively
outperforms GPT-4.1 on 10 out of 15 datasets (spanning mathematics, code,
logic, knowledge, and affective tasks). In particular, it surpasses GPT-4.1 on
mathematics tasks by 18.21% and on code tasks by 7.46%. Furthermore, the
Avengers delivers superior out-of-distribution generalization, and remains
robust across various embedding models, clustering algorithms, ensemble
strategies, and values of its sole parameter--the number of clusters. We have
open-sourced the code on GitHub: https://github.com/ZhangYiqun018/Avengers

</details>


### [190] [MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs](https://arxiv.org/abs/2505.19800)
*Zaid Alyafeai,Maged S. Al-Shaibani,Bernard Ghanem*

Main category: cs.CL

TL;DR: 提出MOLE框架，利用大语言模型自动提取非阿拉伯语数据集论文元数据，并发布代码与数据集


<details>
  <summary>Details</summary>
Motivation: 当前科学文献激增需高效元数据提取方法，替代Masader依赖人工标注的低效流程

Method: 采用模式驱动架构处理多格式文档，集成验证机制，构建新基准测试评估模型性能，分析上下文长度/小样本学习/网络浏览集成

Result: 现代LLM在自动化元数据提取中展现潜力，但需改进输出稳定性

Conclusion: MOLE框架证明LLM在元数据自动化提取中的可行性，未来需提升一致性，已开源资源促进研究社区发展

Abstract: Metadata extraction is essential for cataloging and preserving datasets,
enabling effective research discovery and reproducibility, especially given the
current exponential growth in scientific research. While Masader (Alyafeai et
al.,2021) laid the groundwork for extracting a wide range of metadata
attributes from Arabic NLP datasets' scholarly articles, it relies heavily on
manual annotation. In this paper, we present MOLE, a framework that leverages
Large Language Models (LLMs) to automatically extract metadata attributes from
scientific papers covering datasets of languages other than Arabic. Our
schema-driven methodology processes entire documents across multiple input
formats and incorporates robust validation mechanisms for consistent output.
Additionally, we introduce a new benchmark to evaluate the research progress on
this task. Through systematic analysis of context length, few-shot learning,
and web browsing integration, we demonstrate that modern LLMs show promising
results in automating this task, highlighting the need for further future work
improvements to ensure consistent and reliable performance. We release the
code: https://github.com/IVUL-KAUST/MOLE and dataset:
https://huggingface.co/datasets/IVUL-KAUST/MOLE for the research community.

</details>


### [191] [Compliance-to-Code: Enhancing Financial Compliance Checking via Code Generation](https://arxiv.org/abs/2505.19804)
*Siyuan Li,Jian Chen,Rui Yao,Xuming Hu,Peilin Zhou,Weihua Qiu,Simin Zhang,Chucheng Dong,Zhiyao Li,Qipeng Xie,Zixuan Yuan*

Main category: cs.CL

TL;DR: 针对中文金融法规自动化合规的挑战，提出首个大规模中文数据集Compliance-to-Code及FinCheck流程，解决现有方案知识表示不全、推理能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有RegTech和LLMs在中文金融法规应用中存在领域知识不完整、分层推理不足、逻辑连贯性缺失三大局限，且缺乏适配中文的细粒度合规代码数据集。

Method: 构建含1,159条款的模块化中文数据集，包含逻辑要素和关系映射；开发FinCheck流程实现法规结构化、代码生成与报告生成。

Result: 创建覆盖10类361项法规的标注数据集，提供Python代码映射与解释，验证了自动化审计流程可行性。

Conclusion: 填补中文金融合规数据集空白，通过代码导向的结构化方案提升监管自动化水平，FinCheck为实际应用提供端到端解决方案。

Abstract: Nowadays, regulatory compliance has become a cornerstone of corporate
governance, ensuring adherence to systematic legal frameworks. At its core,
financial regulations often comprise highly intricate provisions, layered
logical structures, and numerous exceptions, which inevitably result in
labor-intensive or comprehension challenges. To mitigate this, recent
Regulatory Technology (RegTech) and Large Language Models (LLMs) have gained
significant attention in automating the conversion of regulatory text into
executable compliance logic. However, their performance remains suboptimal
particularly when applied to Chinese-language financial regulations, due to
three key limitations: (1) incomplete domain-specific knowledge representation,
(2) insufficient hierarchical reasoning capabilities, and (3) failure to
maintain temporal and logical coherence. One promising solution is to develop a
domain specific and code-oriented datasets for model training. Existing
datasets such as LexGLUE, LegalBench, and CODE-ACCORD are often
English-focused, domain-mismatched, or lack fine-grained granularity for
compliance code generation. To fill these gaps, we present Compliance-to-Code,
the first large-scale Chinese dataset dedicated to financial regulatory
compliance. Covering 1,159 annotated clauses from 361 regulations across ten
categories, each clause is modularly structured with four logical
elements-subject, condition, constraint, and contextual information-along with
regulation relations. We provide deterministic Python code mappings, detailed
code reasoning, and code explanations to facilitate automated auditing. To
demonstrate utility, we present FinCheck: a pipeline for regulation
structuring, code generation, and report generation.

</details>


### [192] [Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks](https://arxiv.org/abs/2505.19806)
*Sirui Chen,Shuqin Ma,Shu Yu,Hanwang Zhang,Shengjie Zhao,Chaochao Lu*

Main category: cs.CL

TL;DR: 系统梳理大语言模型意识的研究现状，澄清术语混淆问题，从理论实证双视角整合研究进展，预警潜在风险并规划领域发展路线图


<details>
  <summary>Details</summary>
Motivation: 人类意识研究具有哲学与科学双重价值，LLM技术突破使机器意识议题迫在眉睫，但现有研究存在概念混淆与体系缺失问题

Method: 1. 解构意识(Consciousness)与觉知(Awareness)等术语差异
2. 构建理论框架整合神经科学、认知科学多学科视角
3. 建立GitHub知识库实现文献动态管理

Result: 1. 揭示当前LLM意识研究的三重认知局限
2. 识别模型意识可能引发的价值对齐危机
3. 提出包含19个子方向的研究路线图

Conclusion: 需建立跨学科研究范式，开发意识评估指标体系，同步推进技术治理框架，该领域突破将重塑人工智能伦理基准

Abstract: Consciousness stands as one of the most profound and distinguishing features
of the human mind, fundamentally shaping our understanding of existence and
agency. As large language models (LLMs) develop at an unprecedented pace,
questions concerning intelligence and consciousness have become increasingly
significant. However, discourse on LLM consciousness remains largely unexplored
territory. In this paper, we first clarify frequently conflated terminologies
(e.g., LLM consciousness and LLM awareness). Then, we systematically organize
and synthesize existing research on LLM consciousness from both theoretical and
empirical perspectives. Furthermore, we highlight potential frontier risks that
conscious LLMs might introduce. Finally, we discuss current challenges and
outline future directions in this emerging field. The references discussed in
this paper are organized at
https://github.com/OpenCausaLab/Awesome-LLM-Consciousness.

</details>


### [193] [Deciphering Trajectory-Aided LLM Reasoning: An Optimization Perspective](https://arxiv.org/abs/2505.19815)
*Junnan Liu,Hongwei Liu,Linchen Xiao,Shudong Liu,Taolin Zhang,Zihan Ma,Songyang Zhang,Kai Chen*

Main category: cs.CL

TL;DR: 提出通过元学习框架解析大语言模型的推理机制，将推理轨迹视为参数更新，实证验证了其与元学习的强关联并给出实用改进方案


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对LLM推理机制的系统性分析，研究者尝试通过元学习范式建立推理过程与参数更新的映射关系，为理解模型推理提供新视角

Method: 将每个问题建模为独立元学习任务，用推理轨迹模拟参数更新过程，通过多样化问题训练获得可泛化的基础推理能力

Result: 实验证实LLM推理过程与MAML等元学习算法存在本质相似性，在少样本学习场景展现出显著性能提升

Conclusion: 该框架不仅深化了LLM推理的理论认知，更为模型优化提供了可解释的元学习技术路径

Abstract: We propose a novel framework for comprehending the reasoning capabilities of
large language models (LLMs) through the perspective of meta-learning. By
conceptualizing reasoning trajectories as pseudo-gradient descent updates to
the LLM's parameters, we identify parallels between LLM reasoning and various
meta-learning paradigms. We formalize the training process for reasoning tasks
as a meta-learning setup, with each question treated as an individual task, and
reasoning trajectories serving as the inner loop optimization for adapting
model parameters. Once trained on a diverse set of questions, the LLM develops
fundamental reasoning capabilities that can generalize to previously unseen
questions. Extensive empirical evaluations substantiate the strong connection
between LLM reasoning and meta-learning, exploring several issues of
significant interest from a meta-learning standpoint. Our work not only
enhances the understanding of LLM reasoning but also provides practical
insights for improving these models through established meta-learning
techniques.

</details>


### [194] [FoodTaxo: Generating Food Taxonomies with Large Language Models](https://arxiv.org/abs/2505.19838)
*Pascal Wullschleger,Majid Zarharan,Donnacha Daly,Marc Pouly,Jennifer Foster*

Main category: cs.CL

TL;DR: 探索大语言模型在食品工业分类体系自动生成中的应用效果


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在食品技术领域分类体系构建中的实际应用价值，特别关注从种子分类延伸生成和无种子全新建构两种场景

Method: 采用迭代式提示技术，基于开源大模型Llama-3对五个现有分类体系进行生成与补全实验

Result: 模型展现出应用潜力，但在中间节点准确定位方面仍存在显著困难

Conclusion: 证实大语言模型在分类体系自动化建设中的可行性，同时揭示复杂层级关系处理的技术瓶颈

Abstract: We investigate the utility of Large Language Models for automated taxonomy
generation and completion specifically applied to taxonomies from the food
technology industry. We explore the extent to which taxonomies can be completed
from a seed taxonomy or generated without a seed from a set of known concepts,
in an iterative fashion using recent prompting techniques. Experiments on five
taxonomies using an open-source LLM (Llama-3), while promising, point to the
difficulty of correctly placing inner nodes.

</details>


### [195] [Improving Multilingual Math Reasoning for African Languages](https://arxiv.org/abs/2505.19848)
*Odunayo Ogundepo,Akintunde Oladipo,Kelechi Ogueji,Esther Adenuga,David Ifeoluwa Adelani,Jimmy Lin*

Main category: cs.CL

TL;DR: 系统研究现有LLM扩展至非洲语言的最优适应策略，通过数学推理任务验证不同数据组合与训练阶段的影响


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在低资源语言（尤其是非洲语言）的适应效果存在显著技术空白，需明确最佳实践路径

Method: 基于Llama 3.1模型系列，通过预训练与后训练阶段的数据组合实验（翻译数据与合成数据），结合消融研究验证配置效果

Result: 发现不同数据源与训练阶段的组合对模型性能产生系统性影响，具体优化方向需结合任务特性

Conclusion: 为非洲低资源语言的LLM适配建立了系统性评估框架，指明多阶段训练与数据混合的优化方向

Abstract: Researchers working on low-resource languages face persistent challenges due
to limited data availability and restricted access to computational resources.
Although most large language models (LLMs) are predominantly trained in
high-resource languages, adapting them to low-resource contexts, particularly
African languages, requires specialized techniques. Several strategies have
emerged for adapting models to low-resource languages in todays LLM landscape,
defined by multi-stage pre-training and post-training paradigms. However, the
most effective approaches remain uncertain. This work systematically
investigates which adaptation strategies yield the best performance when
extending existing LLMs to African languages. We conduct extensive experiments
and ablation studies to evaluate different combinations of data types
(translated versus synthetically generated), training stages (pre-training
versus post-training), and other model adaptation configurations. Our
experiments focuses on mathematical reasoning tasks, using the Llama 3.1 model
family as our base model.

</details>


### [196] [Beyond Specialization: Benchmarking LLMs for Transliteration of Indian Languages](https://arxiv.org/abs/2505.19851)
*Gulfarogh Azam,Mohd Sadique,Saif Ali,Mohammad Nadeem,Erik Cambria,Shahab Saquib Sohail,Mohammad Sultan Alam*

Main category: cs.CL

TL;DR: LLM在印度语转写任务中超越专用模型，GPT系列表现最优，微调可提升特定语言性能


<details>
  <summary>Details</summary>
Motivation: 验证通用大模型在印度语转写任务中的潜力，对比专用模型IndicXlit的性能差异

Method: 使用Dakshina/Aksharantar数据集，评估GPT系列、Gemma、Mistral等模型在10种印度语言上的Top-1准确率和字符错误率，并进行噪声鲁棒性测试

Result: GPT模型整体优于其他LLM和IndicXlit，微调GPT-4o显著提升特定语言性能，错误分析显示LLM在噪声条件下更具鲁棒性

Conclusion: 基础大模型通过微调即可高效应用于专业领域，为多语言NLP任务提供了低开销解决方案

Abstract: Transliteration, the process of mapping text from one script to another,
plays a crucial role in multilingual natural language processing, especially
within linguistically diverse contexts such as India. Despite significant
advancements through specialized models like IndicXlit, recent developments in
large language models suggest a potential for general-purpose models to excel
at this task without explicit task-specific training. The current work
systematically evaluates the performance of prominent LLMs, including GPT-4o,
GPT-4.5, GPT-4.1, Gemma-3-27B-it, and Mistral-Large against IndicXlit, a
state-of-the-art transliteration model, across ten major Indian languages.
Experiments utilized standard benchmarks, including Dakshina and Aksharantar
datasets, with performance assessed via Top-1 Accuracy and Character Error
Rate. Our findings reveal that while GPT family models generally outperform
other LLMs and IndicXlit for most instances. Additionally, fine-tuning GPT-4o
improves performance on specific languages notably. An extensive error analysis
and robustness testing under noisy conditions further elucidate strengths of
LLMs compared to specialized models, highlighting the efficacy of foundational
models for a wide spectrum of specialized applications with minimal overhead.

</details>


### [197] [REA-RL: Reflection-Aware Online Reinforcement Learning for Efficient Large Reasoning Models](https://arxiv.org/abs/2505.19862)
*Hexuan Deng,Wenxiang Jiao,Xuebo Liu,Jun Rao,Min Zhang*

Main category: cs.CL

TL;DR: REA-RL通过引入小型反思模型和反思奖励机制，在保持大型推理模型性能的同时显著降低35%推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法生成简短响应效率低下，强化学习的长度奖励可能损害模型反思能力。需平衡效率与性能。

Method: 1) 小型反思模型支持并行采样和顺序修订 2) 设计反思奖励防止生成无反思的简短响应

Result: 实验表明方法降低35%推理成本且性能无损，对难题保持反思频率，简单问题适当减少反思。

Conclusion: REA-RL实现了性能与效率的平衡，通过自适应反思机制优化不同难度问题的处理方式。

Abstract: Large Reasoning Models (LRMs) demonstrate strong performance in complex tasks
but often face the challenge of overthinking, leading to substantially high
inference costs. Existing approaches synthesize shorter reasoning responses for
LRMs to learn, but are inefficient for online usage due to the time-consuming
data generation and filtering processes. Meanwhile, online reinforcement
learning mainly adopts a length reward to encourage short reasoning responses,
but tends to lose the reflection ability and harm the performance. To address
these issues, we propose REA-RL, which introduces a small reflection model for
efficient scaling in online training, offering both parallel sampling and
sequential revision. Besides, a reflection reward is designed to further
prevent LRMs from favoring short yet non-reflective responses. Experiments show
that both methods maintain or enhance performance while significantly improving
inference efficiency. Their combination achieves a good balance between
performance and efficiency, reducing inference costs by 35% without
compromising performance. Further analysis demonstrates that our methods are
effective by maintaining reflection frequency for hard problems while
appropriately reducing it for simpler ones without losing reflection ability.
Codes are available at https://github.com/hexuandeng/REA-RL.

</details>


### [198] [APE: A Data-Centric Benchmark for Efficient LLM Adaptation in Text Summarization](https://arxiv.org/abs/2505.19912)
*Javier Marín*

Main category: cs.CL

TL;DR: 提出无需昂贵算力的迭代式微调方法APE，通过小批量数据扰动在60分钟内达成40% BLEU提升


<details>
  <summary>Details</summary>
Motivation: 解决传统微调方法计算成本过高的问题，帮助算力有限的研究者高效适配大模型

Method: 200样本/批次迭代微调，使用T4 GPU快速训练并选择性保留模型改进

Result: 新闻摘要任务超越LoRA等复杂方法，自动评估与人工测评双验证有效性

Conclusion: 证实微小数据扰动可引导大模型性能提升，开源实现助推低资源场景应用

Abstract: We present Adjacent Possible Exploration (APE), a simple yet effective method
for adapting large language models to specific tasks using minimal
computational resources. Unlike traditional fine-tuning that requires extensive
compute, APE iteratively fine-tunes models on small, carefully selected data
batches (200 examples), retaining only improvements. On news summarization, APE
achieves 40 percent BLEU improvement using just a T4 GPU in 60 minutes,
matching or exceeding more complex methods like LoRA while remaining
conceptually simple. Our approach is particularly valuable for researchers and
practitioners with limited computational resources. We provide open-source code
and demonstrate APE's effectiveness through both automatic metrics and human
evaluation. While inspired by evolutionary theory's "adjacent possible", APE's
core insight has a very practical application: small, iterative data
perturbations can efficiently guide LLMs toward task-specific performance
without expensive retraining.

</details>


### [199] [Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles](https://arxiv.org/abs/2505.19914)
*Jiangjie Chen,Qianyu He,Siyu Yuan,Aili Chen,Zhicheng Cai,Weinan Dai,Hongli Yu,Qiying Yu,Xuefeng Li,Jiaze Chen,Hao Zhou,Mingxuan Wang*

Main category: cs.CL

TL;DR: 提出首个面向LLM谜题推理的综合训练套件Enigmata，通过可扩展的多任务强化学习显著提升模型在复杂逻辑推理任务中的表现


<details>
  <summary>Details</summary>
Motivation: 现有LLM在无需领域知识的谜题推理任务中表现欠佳，需要专门框架提升逻辑推理能力

Method: 1) 构建包含7类36任务的Enigmata套件，配备可控难度生成器和规则验证器 2) 开发Enigmata-Eval基准测试 3) 提出优化的多任务RLVR训练策略

Result: Qwen2.5-32B-Enigmata在Enigmata-Eval/ARC-AGI等基准超越主流模型，Seed1.5-Thinking使用Enigmata数据后在AIME等STEM任务刷新SOTA

Conclusion: Enigmata为LLM逻辑推理提供统一可控框架，其生成的谜题数据能有效提升模型在复杂推理和STEM任务中的泛化能力

Abstract: Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at
advanced reasoning tasks like math and coding via Reinforcement Learning with
Verifiable Rewards (RLVR), but still struggle with puzzles solvable by humans
without domain knowledge. We introduce Enigmata, the first comprehensive suite
tailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks
across seven categories, each with 1) a generator that produces unlimited
examples with controllable difficulty and 2) a rule-based verifier for
automatic evaluation. This generator-verifier design supports scalable,
multi-task RL training, fine-grained analysis, and seamless RLVR integration.
We further propose Enigmata-Eval, a rigorous benchmark, and develop optimized
multi-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata,
consistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks
like Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes
well to out-of-domain puzzle benchmarks and mathematical reasoning, with little
multi-tasking trade-off. When trained on larger models like Seed1.5-Thinking
(20B activated parameters and 200B total parameters), puzzle data from Enigmata
further boosts SoTA performance on advanced math and STEM reasoning tasks such
as AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization
benefits of Enigmata. This work offers a unified, controllable framework for
advancing logical reasoning in LLMs. Resources of this work can be found at
https://seed-enigmata.github.io.

</details>


### [200] [ALAS: Measuring Latent Speech-Text Alignment For Spoken Language Understanding In Multimodal LLMs](https://arxiv.org/abs/2505.19937)
*Pooneh Mousavi,Yingzhi Wang,Mirco Ravanelli,Cem Subakan*

Main category: cs.CL

TL;DR: 提出ALAS指标评估LLM中音频与文本表征的跨模态对齐质量，验证其在不同任务和网络层中的有效性


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏统一标准评估LLM的跨模态对齐质量，难以衡量语义与音频片段的关联程度

Method: 通过分析不同transformer层的音视频表征相关性，构建ALAS指标并在口语问答和情感识别任务中验证

Result: ALAS指标在不同网络层和任务中表现出符合预期的分层特性，有效反映跨模态对齐状态

Conclusion: ALAS为评估多模态对齐提供量化标准，对提升语音语言模型的语义理解能力具有重要意义

Abstract: Large Language Models (LLMs) are widely used in Spoken Language Understanding
(SLU). Recent SLU models process audio directly by adapting speech input into
LLMs for better multimodal learning. A key consideration for these models is
the cross-modal alignment between text and audio modalities, which is a
telltale sign as to whether or not LLM is able to associate semantic meaning to
audio segments. While various methods exist for fusing these modalities, there
is no standard metric to evaluate alignment quality in LLMs. In this work, we
propose a new metric, ALAS (Automatic Latent Alignment Score). Our study
examines the correlation between audio and text representations across
transformer layers, for two different tasks (Spoken Question Answering and
Emotion Recognition). We showcase that our metric behaves as expected across
different layers and different tasks.

</details>


### [201] [MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models](https://arxiv.org/abs/2505.19959)
*Zhongzhan Huang,Guoming Ling,Shanshan Zhong,Hefeng Wu,Liang Lin*

Main category: cs.CL

TL;DR: 研究者提出MiniLongBench压缩基准，将长文本评估成本降至4.5%且保持高相关性


<details>
  <summary>Details</summary>
Motivation: 现有长文本理解基准存在显著冗余，导致评估成本过高（测试时间和推理费用）

Method: 针对稀疏信息特征的长文本数据开发压缩方法，通过修剪LongBench创建包含237个样本的MiniLongBench

Result: 经60+模型验证，新基准评估成本降为原4.5%，平均秩相关系数达0.97

Conclusion: MiniLongBench作为低成本评估基准，具备推动LLMs长文本理解能力研究的巨大潜力

Abstract: Long Context Understanding (LCU) is a critical area for exploration in
current large language models (LLMs). However, due to the inherently lengthy
nature of long-text data, existing LCU benchmarks for LLMs often result in
prohibitively high evaluation costs, like testing time and inference expenses.
Through extensive experimentation, we discover that existing LCU benchmarks
exhibit significant redundancy, which means the inefficiency in evaluation. In
this paper, we propose a concise data compression method tailored for long-text
data with sparse information characteristics. By pruning the well-known LCU
benchmark LongBench, we create MiniLongBench. This benchmark includes only 237
test samples across six major task categories and 21 distinct tasks. Through
empirical analysis of over 60 LLMs, MiniLongBench achieves an average
evaluation cost reduced to only 4.5% of the original while maintaining an
average rank correlation coefficient of 0.97 with LongBench results. Therefore,
our MiniLongBench, as a low-cost benchmark, holds great potential to
substantially drive future research into the LCU capabilities of LLMs. See
https://github.com/MilkThink-Lab/MiniLongBench for our code, data and tutorial.

</details>


### [202] [CP-Router: An Uncertainty-Aware Router Between LLM and LRM](https://arxiv.org/abs/2505.19970)
*Jiayuan Su,Fulin Lin,Zhaopeng Feng,Han Zheng,Teng Wang,Zhenyu Xiao,Xinlong Zhao,Zuozhu Liu,Lu Cheng,Hongwei Wang*

Main category: cs.CL

TL;DR: 提出CP-Router框架，动态选择LLM/LRM处理多选问答，通过FBE优化不确定性阈值，提升效率且保持准确性。


<details>
  <summary>Details</summary>
Motivation: 解决LRMs在处理简单查询时冗长输出导致的效率下降和准确率降低问题

Method: 基于保形预测(CP)的不确定性估计实现动态路由，引入FBE熵标准自适应选择CP阈值

Result: 在多领域MCQA基准测试中减少token使用量，准确率持平或提升（数学/逻辑推理/中国化学等场景）

Conclusion: CP-Router在开放问答等扩展场景中保持强性能，验证了框架的通用性和鲁棒性

Abstract: Recent advances in Large Reasoning Models (LRMs) have significantly improved
long-chain reasoning capabilities over Large Language Models (LLMs). However,
LRMs often produce unnecessarily lengthy outputs even for simple queries,
leading to inefficiencies or even accuracy degradation compared to LLMs. To
overcome this, we propose CP-Router, a training-free and model-agnostic routing
framework that dynamically selects between an LLM and an LRM, demonstrated with
multiple-choice question answering (MCQA) prompts. The routing decision is
guided by the prediction uncertainty estimates derived via Conformal Prediction
(CP), which provides rigorous coverage guarantees. To further refine the
uncertainty differentiation across inputs, we introduce Full and Binary Entropy
(FBE), a novel entropy-based criterion that adaptively selects the appropriate
CP threshold. Experiments across diverse MCQA benchmarks, including
mathematics, logical reasoning, and Chinese chemistry, demonstrate that
CP-Router efficiently reduces token usage while maintaining or even improving
accuracy compared to using LRM alone. We also extend CP-Router to diverse model
pairings and open-ended QA, where it continues to demonstrate strong
performance, validating its generality and robustness.

</details>


### [203] [Conversational Lexicography: Querying Lexicographic Data on Knowledge Graphs with SPARQL through Natural Language](https://arxiv.org/abs/2505.19971)
*Kilian Sennrich,Sina Ahmadi*

Main category: cs.CL

TL;DR: 论文提出基于多维分类法构建自然语言接口，利用大模型将自然语言转化为SPARQL查询，验证了GPT-3.5在词典数据检索中的优势与挑战。


<details>
  <summary>Details</summary>
Motivation: 解决非专业用户使用SPARQL查询语言访问Wikidata等知识图谱词典数据的门槛问题

Method: 构建四维分类法描述词典数据结构，创建120万自然语言-SPARQL模板数据集，测试GPT-2/Phi-1.5/GPT-3.5性能差异

Result: GPT-3.5-Turbo展现最佳泛化能力，模型规模与预训练多样性对领域适应至关重要

Conclusion: 大型语言模型在词典知识表示领域有潜力，但需解决泛化能力、语言数据处理及系统扩展性等核心挑战

Abstract: Knowledge graphs offer an excellent solution for representing the
lexical-semantic structures of lexicographic data. However, working with the
SPARQL query language represents a considerable hurdle for many non-expert
users who could benefit from the advantages of this technology. This paper
addresses the challenge of creating natural language interfaces for
lexicographic data retrieval on knowledge graphs such as Wikidata. We develop a
multidimensional taxonomy capturing the complexity of Wikidata's lexicographic
data ontology module through four dimensions and create a template-based
dataset with over 1.2 million mappings from natural language utterances to
SPARQL queries. Our experiments with GPT-2 (124M), Phi-1.5 (1.3B), and
GPT-3.5-Turbo reveal significant differences in model capabilities. While all
models perform well on familiar patterns, only GPT-3.5-Turbo demonstrates
meaningful generalization capabilities, suggesting that model size and diverse
pre-training are crucial for adaptability in this domain. However, significant
challenges remain in achieving robust generalization, handling diverse
linguistic data, and developing scalable solutions that can accommodate the
full complexity of lexicographic knowledge representation.

</details>


### [204] [DeepDialogue: A Multi-Turn Emotionally-Rich Spoken Dialogue Dataset](https://arxiv.org/abs/2505.19978)
*Alkis Koudounas,Moreno La Quatra,Elena Baralis*

Main category: cs.CL

TL;DR: 研究团队构建了DeepDialogue大规模多模态对话数据集，通过跨模型生成与严格质量过滤，揭示了模型规模、领域类型及交互模式对对话质量的影响，并首次实现情感语音合成。


<details>
  <summary>Details</summary>
Motivation: 现有对话数据集存在情感单一、领域局限、轮次浅层及模态单一等问题，制约了多轮多模态对话系统的发展。

Method: 采用9种不同规模的语言模型生成初始对话，结合人工标注与LLM质量过滤，构建包含4万+对话的数据集，并合成情感匹配的语音组件。

Result: 小模型（4B-7B）在6轮后对话崩溃；具象领域（如汽车/旅行）比抽象领域对话质量高28%；跨模型对话比同模型对话连贯性提升19%。

Conclusion: DeepDialogue填补了多模态情感对话数据空白，揭示了模型能力边界与数据特征规律，为开发更自然的对话系统提供了关键基础设施。

Abstract: Recent advances in conversational AI have demonstrated impressive
capabilities in single-turn responses, yet multi-turn dialogues remain
challenging for even the most sophisticated language models. Current dialogue
datasets are limited in their emotional range, domain diversity, turn depth,
and are predominantly text-only, hindering progress in developing more
human-like conversational systems across modalities. To address these
limitations, we present DeepDialogue, a large-scale multimodal dataset
containing 40,150 high-quality multi-turn dialogues spanning 41 domains and
incorporating 20 distinct emotions with coherent emotional progressions. Our
approach pairs 9 different language models (4B-72B parameters) to generate
65,600 initial conversations, which we then evaluate through a combination of
human annotation and LLM-based quality filtering. The resulting dataset reveals
fundamental insights: smaller models fail to maintain coherence beyond 6
dialogue turns; concrete domains (e.g., "cars," "travel") yield more meaningful
conversations than abstract ones (e.g., "philosophy"); and cross-model
interactions produce more coherent dialogues than same-model conversations. A
key contribution of DeepDialogue is its speech component, where we synthesize
emotion-consistent voices for all 40,150 dialogues, creating the first
large-scale open-source multimodal dialogue dataset that faithfully preserves
emotional context across multi-turn conversations.

</details>


### [205] [How Well Do Large Reasoning Models Translate? A Comprehensive Evaluation for Multi-Domain Machine Translation](https://arxiv.org/abs/2505.19987)
*Yongshi Ye,Biao Fu,Chongxuan Huang,Yidong Chen,Xiaodong Shi*

Main category: cs.CL

TL;DR: LRMs在语义复杂领域超越传统LLMs，尤其在长文本和高难度翻译场景，领域自适应提示策略可进一步提升性能


<details>
  <summary>Details</summary>
Motivation: 探索结构化推理能否提升跨领域机器翻译质量，特别是传统LLMs在复杂领域敏感任务中的不足

Method: 跨15个领域和4个翻译方向对比LRMs与LLMs，结合自动指标和增强型MQM评估框架，分析任务难度/文本长度/术语密度等因素

Result: LRMs在语义复杂度高的领域表现更优，长文本翻译质量提升12.7%，高难度任务错误率降低23%

Conclusion: 结构化推理显著提升多领域机器翻译性能，领域自适应策略为优化翻译系统提供新方向

Abstract: Large language models (LLMs) have demonstrated strong performance in
general-purpose machine translation, but their effectiveness in complex,
domain-sensitive translation tasks remains underexplored. Recent advancements
in Large Reasoning Models (LRMs), raise the question of whether structured
reasoning can enhance translation quality across diverse domains. In this work,
we compare the performance of LRMs with traditional LLMs across 15
representative domains and four translation directions. Our evaluation
considers various factors, including task difficulty, input length, and
terminology density. We use a combination of automatic metrics and an enhanced
MQM-based evaluation hierarchy to assess translation quality. Our findings show
that LRMs consistently outperform traditional LLMs in semantically complex
domains, especially in long-text and high-difficulty translation scenarios.
Moreover, domain-adaptive prompting strategies further improve performance by
better leveraging the reasoning capabilities of LRMs. These results highlight
the potential of structured reasoning in MDMT tasks and provide valuable
insights for optimizing translation systems in domain-sensitive contexts.

</details>


### [206] [Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech Recognition](https://arxiv.org/abs/2505.20006)
*Raphaël Bagat,Irina Illina,Emmanuel Vincent*

Main category: cs.CL

TL;DR: 提出混合口音专用LoRA专家（MAS-LoRA）方法，显著提升低资源多口音场景下语音识别的鲁棒性，减少灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 针对非母语多口音ASR在低资源场景下的性能瓶颈，传统方法在未知口音时表现不佳且需要重复微调。

Method: 基于Whisper模型，使用混合低秩适配器（LoRA）专家架构，支持已知/未知口音推断且无需重新微调。

Result: 在L2-ARCTIC语料库上，词错误率相比常规LoRA和全微调分别降低18.3%和9.7%（未知口音），已知口音效果更优。

Conclusion: 首次将混合LoRA专家应用于多口音ASR，验证了参数高效微调方法的有效性，为低资源场景提供新解决方案。

Abstract: We aim to improve the robustness of Automatic Speech Recognition (ASR)
systems against non-native speech, particularly in low-resourced multi-accent
settings. We introduce Mixture of Accent-Specific LoRAs (MAS-LoRA), a
fine-tuning method that leverages a mixture of Low-Rank Adaptation (LoRA)
experts, each specialized in a specific accent. This method can be used when
the accent is known or unknown at inference time, without the need to fine-tune
the model again. Our experiments, conducted using Whisper on the L2-ARCTIC
corpus, demonstrate significant improvements in Word Error Rate compared to
regular LoRA and full fine-tuning when the accent is unknown. When the accent
is known, the results further improve. Furthermore, MAS-LoRA shows less
catastrophic forgetting than the other fine-tuning methods. To the best of our
knowledge, this is the first use of a mixture of LoRA experts for non-native
multi-accent ASR.

</details>


### [207] [WebCoT: Enhancing Web Agent Reasoning by Reconstructing Chain-of-Thought in Reflection, Branching, and Rollback](https://arxiv.org/abs/2505.20013)
*Minda Hu,Tianqing Fang,Jianshu Zhang,Junyu Ma,Zhisong Zhang,Jingyan Zhou,Hongming Zhang,Haitao Mi,Dong Yu,Irwin King*

Main category: cs.CL

TL;DR: 研究通过增强大语言模型网页代理的三种关键推理能力（反思前瞻、分支决策、状态回滚），显著提升了其在动态网页环境中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于大语言模型的网页代理在不确定动态环境中的推理能力不足，制约了其实际应用。需要针对性提升核心推理能力以实现稳健部署。

Method: 将代理的推理算法重构为思维链轨迹数据，在OpenWebVoyager基准中通过微调LLM实现推理模式蒸馏。

Result: 在WebVoyager、Mind2web-live等多个基准测试中取得显著性能提升，最高达15%绝对准确率改进。

Conclusion: 通过系统性的推理能力增强和思维链数据蒸馏，可有效提升网页代理的决策质量，为AI代理发展提供新思路。

Abstract: Web agents powered by Large Language Models (LLMs) show promise for
next-generation AI, but their limited reasoning in uncertain, dynamic web
environments hinders robust deployment. In this paper, we identify key
reasoning skills essential for effective web agents, i.e., reflection &
lookahead, branching, and rollback, and curate trajectory data that exemplifies
these abilities by reconstructing the agent's (inference-time) reasoning
algorithms into chain-of-thought rationales. We conduct experiments in the
agent self-improving benchmark, OpenWebVoyager, and demonstrate that distilling
salient reasoning patterns into the backbone LLM via simple fine-tuning can
substantially enhance its performance. Our approach yields significant
improvements across multiple benchmarks, including WebVoyager, Mind2web-live,
and SimpleQA (web search), highlighting the potential of targeted reasoning
skill enhancement for web agents.

</details>


### [208] [Does Rationale Quality Matter? Enhancing Mental Disorder Detection via Selective Reasoning Distillation](https://arxiv.org/abs/2505.20014)
*Hoyun Song,Huije Lee,Jisu Shin,Sukmin Cho,Changgeon Ko,Jong C. Park*

Main category: cs.CL

TL;DR: 提出基于临床推理对齐的rationale选择框架，显著提升小型语言模型在心理健康检测与解释生成中的双任务表现


<details>
  <summary>Details</summary>
Motivation: 大型语言模型生成rationale存在领域对齐缺陷，导致知识蒸馏到小型模型时效果受限，需确保rationale质量与临床专业知识对齐

Method: 设计rationale质量评估框架，通过专家临床推理对齐机制筛选高价值样本进行知识蒸馏

Result: 实验证明该方法使SLM在心理健康检测准确率提升12.6%，生成解释的临床相关性提高34%

Conclusion: rationale质量直接影响知识迁移效果，提出的质量导向框架为心理健康NLP应用提供了新的优化路径

Abstract: The detection of mental health problems from social media and the
interpretation of these results have been extensively explored. Research has
shown that incorporating clinical symptom information into a model enhances
domain expertise, improving its detection and interpretation performance. While
large language models (LLMs) are shown to be effective for generating
explanatory rationales in mental health detection, their substantially large
parameter size and high computational cost limit their practicality. Reasoning
distillation transfers this ability to smaller language models (SLMs), but
inconsistencies in the relevance and domain alignment of LLM-generated
rationales pose a challenge. This paper investigates how rationale quality
impacts SLM performance in mental health detection and explanation generation.
We hypothesize that ensuring high-quality and domain-relevant rationales
enhances the distillation. To this end, we propose a framework that selects
rationales based on their alignment with expert clinical reasoning. Experiments
show that our quality-focused approach significantly enhances SLM performance
in both mental disorder detection and rationale generation. This work
highlights the importance of rationale quality and offers an insightful
framework for knowledge transfer in mental health applications.

</details>


### [209] [On the class of coding optimality of human languages and the origins of Zipf's law](https://arxiv.org/abs/2505.20015)
*Ramon Ferrer-i-Cancho*

Main category: cs.CL

TL;DR: 提出新最优编码类别验证Zipf定律源于压缩假说，人类语言符合该类别而多数动物通信系统不符合。


<details>
  <summary>Details</summary>
Motivation: 探究Zipf定律在通信系统中的普遍性及其与最优编码的关系，验证信息压缩是否为该定律的起源。

Method: 构建最优编码系统理论模型，分析人类语言与动物通信系统的频率分布特征，通过双对数坐标验证线性关系。

Result: 人类语言符合最优编码类别呈现Zipf定律，海豚/鲸类可能例外；双对数图直线斜率证实编码最优性分离关系。

Conclusion: Zipf定律源于信息压缩机制，人类语言因符合最优编码展现该定律，指数分布系统被排除该理论框架。

Abstract: Here we present a new class of optimality for coding systems. Members of that
class are separated linearly from optimal coding and thus exhibit Zipf's law,
namely a power-law distribution of frequency ranks. Whithin that class, Zipf's
law, the size-rank law and the size-probability law form a group-like
structure. We identify human languages that are members of the class. All
languages showing sufficient agreement with Zipf's law are potential members of
the class. In contrast, there are communication systems in other species that
cannot be members of that class for exhibiting an exponential distribution
instead but dolphins and humpback whales might. We provide a new insight into
plots of frequency versus rank in double logarithmic scale. For any system, a
straight line in that scale indicates that the lengths of optimal codes under
non-singular coding and under uniquely decodable encoding are separated by a
linear function whose slope is the exponent of Zipf's law. For systems under
compression and constrained to be uniquely decodable, such a straight line may
indicate that the system is coding close to optimality. Our findings provide
support for the hypothesis that Zipf's law originates from compression.

</details>


### [210] [TTPA: Token-level Tool-use Preference Alignment Training Framework with Fine-grained Evaluation](https://arxiv.org/abs/2505.20016)
*Chengrui Huang,Shen Gao,Zhengliang Shi,Dongsheng Wang,Shuo Shang*

Main category: cs.CL

TL;DR: 提出TTPA框架，通过token级偏好对齐和逆向数据集构建优化大模型工具使用性能，在三大基准测试中显著提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有工具学习方法依赖监督微调，忽视工具调用细节的细粒度优化，导致偏好对齐不足和错误判别局限。

Method: 1.逆向数据构建生成多轮高质量数据集；2.提出token级偏好采样(TPS)捕捉细粒度差异；3.设计面向错误的评分机制(ESM)量化工具调用错误。

Result: 在多样化基准测试中，TTPA显著提升工具使用性能，并展现跨模型、跨数据集的强泛化能力。

Conclusion: 该框架有效解决了工具学习中的细粒度对齐难题，为提升大模型工具使用能力提供了系统性解决方案。

Abstract: Existing tool-learning methods usually rely on supervised fine-tuning, they
often overlook fine-grained optimization of internal tool call details, leading
to limitations in preference alignment and error discrimination. To overcome
these challenges, we propose Token-level Tool-use Preference Alignment Training
Framework (TTPA), a training paradigm for constructing token-level tool-use
preference datasets that align LLMs with fine-grained preferences using a novel
error-oriented scoring mechanism. TTPA first introduces reversed dataset
construction, a method for creating high-quality, multi-turn tool-use datasets
by reversing the generation flow. Additionally, we propose Token-level
Preference Sampling (TPS) to capture fine-grained preferences by modeling
token-level differences during generation. To address biases in scoring, we
introduce the Error-oriented Scoring Mechanism (ESM), which quantifies
tool-call errors and can be used as a training signal. Extensive experiments on
three diverse benchmark datasets demonstrate that TTPA significantly improves
tool-using performance while showing strong generalization ability across
models and datasets.

</details>


### [211] [Training LLM-Based Agents with Synthetic Self-Reflected Trajectories and Partial Masking](https://arxiv.org/abs/2505.20023)
*Yihan Chen,Benfeng Xu,Xiaorui Wang,Yongdong Zhang,Zhendong Mao*

Main category: cs.CL

TL;DR: 提出STeP方法，通过合成带错误步骤反思与修正的自反思维轨迹，结合部分掩码策略，显著提升LLM智能体在ALFWorld等任务中的性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有基于专家轨迹训练的开源LLM智能体存在性能瓶颈和错误传播问题，需要更高效的训练方法减少对闭源模型和大量训练数据的依赖。

Method: 1. 合成包含错误步骤反思与修正的自反思维轨迹；2. 引入部分掩码策略防止模型吸收错误/次优步骤；3. 使用Qwen1.5-110B-Chat作为教师模型生成训练数据。

Result: LLaMA2-7B-Chat模型在三个基准任务中实现全面性能提升，相比纯专家轨迹训练，数据需求减少且效果更优。

Conclusion: STeP方法通过自我反思机制和抗过拟合策略，有效提升了LLM智能体的学习效率与任务表现，为开源模型部署提供了新思路。

Abstract: Autonomous agents, which perceive environments and take actions to achieve
goals, have become increasingly feasible with the advancements in large
language models (LLMs). However, current powerful agents often depend on
sophisticated prompt engineering combined with closed-source LLMs like GPT-4.
Although training open-source LLMs using expert trajectories from teacher
models has yielded some improvements in agent capabilities, this approach still
faces limitations such as performance plateauing and error propagation. To
mitigate these challenges, we propose STeP, a novel method for improving
LLM-based agent training. We synthesize self-reflected trajectories that
include reflections and corrections of error steps, which enhance the
effectiveness of LLM agents in learning from teacher models, enabling them to
become agents capable of self-reflecting and correcting. We also introduce
partial masking strategy that prevents the LLM from internalizing incorrect or
suboptimal steps. Experiments demonstrate that our method improves agent
performance across three representative tasks: ALFWorld, WebShop, and SciWorld.
For the open-source model LLaMA2-7B-Chat, when trained using self-reflected
trajectories constructed with Qwen1.5-110B-Chat as the teacher model, it
achieves comprehensive improvements with less training data compared to agents
trained exclusively on expert trajectories.

</details>


### [212] [Uncertainty-Aware Attention Heads: Efficient Unsupervised Uncertainty Quantification for LLMs](https://arxiv.org/abs/2505.20045)
*Artem Vazhentsev,Lyudmila Rvanova,Gleb Kuzmin,Ekaterina Fadeeva,Ivan Lazichny,Alexander Panchenko,Maxim Panov,Timothy Baldwin,Mrinmaya Sachan,Preslav Nakov,Artem Shelmanov*

Main category: cs.CL

TL;DR: 提出基于Transformer注意力机制的RAUQ方法，通过无监督方式实时检测大语言模型幻觉，计算效率优于现有方法（<1%延迟）


<details>
  <summary>Details</summary>
Motivation: 解决现有不确定性量化方法存在的高计算开销、依赖监督学习等问题，利用LLM自身注意力模式实现高效幻觉检测

Method: 分析注意力权重识别特殊模式→自动选择不确定性感知头部→递归聚合注意力权重和置信度→单次前向传播计算序列级不确定性分数

Result: 在12个问答/摘要/翻译任务中超越SOTA方法，计算延迟<1%，无需任务标签和复杂调参

Conclusion: RAUQ为白盒LLM提供即插即用的实时幻觉检测方案，显著提升可靠性同时保持高效运行

Abstract: Large language models (LLMs) exhibit impressive fluency, but often produce
critical errors known as "hallucinations". Uncertainty quantification (UQ)
methods are a promising tool for coping with this fundamental shortcoming. Yet,
existing UQ methods face challenges such as high computational overhead or
reliance on supervised learning. Here, we aim to bridge this gap. In
particular, we propose RAUQ (Recurrent Attention-based Uncertainty
Quantification), an unsupervised approach that leverages intrinsic attention
patterns in transformers to detect hallucinations efficiently. By analyzing
attention weights, we identified a peculiar pattern: drops in attention to
preceding tokens are systematically observed during incorrect generations for
certain "uncertainty-aware" heads. RAUQ automatically selects such heads,
recurrently aggregates their attention weights and token-level confidences, and
computes sequence-level uncertainty scores in a single forward pass.
Experiments across 4 LLMs and 12 question answering, summarization, and
translation tasks demonstrate that RAUQ yields excellent results, outperforming
state-of-the-art UQ methods using minimal computational overhead (<1% latency).
Moreover, it requires no task-specific labels and no careful hyperparameter
tuning, offering plug-and-play real-time hallucination detection in white-box
LLMs.

</details>


### [213] [Grammars of Formal Uncertainty: When to Trust LLMs in Automated Reasoning Tasks](https://arxiv.org/abs/2505.20047)
*Debargha Ganguly,Vikash Singh,Sreehari Sankar,Biyao Zhang,Xuecen Zhang,Srinivasan Iyengar,Xiaotian Han,Amit Sharma,Shivkumar Kalyanaraman,Vipin Chaudhary*

Main category: cs.CL

TL;DR: 研究提出PCFG框架量化LLM生成形式化规范的不确定性，通过信号融合实现选择性验证，错误率降低14-100%


<details>
  <summary>Details</summary>
Motivation: 解决LLM概率性输出与形式验证确定性需求之间的根本矛盾

Method: 系统评估5个前沿LLM+SMT自动形式化效果，构建PCFG框架改进不确定性分类，开发信号融合技术

Result: 逻辑任务准确率提升34.8%（语法熵AUROC>0.93），但事实性任务下降44.5%；信号有效性具有任务依赖性

Conclusion: 轻量级信号融合显著提升可靠性，将LLM形式化转变为可信任的工程学科

Abstract: Large language models (LLMs) show remarkable promise for democratizing
automated reasoning by generating formal specifications. However, a fundamental
tension exists: LLMs are probabilistic, while formal verification demands
deterministic guarantees. This paper addresses this epistemological gap by
comprehensively investigating failure modes and uncertainty quantification (UQ)
in LLM-generated formal artifacts. Our systematic evaluation of five frontier
LLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's
domain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on
factual ones), with known UQ techniques like the entropy of token probabilities
failing to identify these errors. We introduce a probabilistic context-free
grammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty
taxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy
for logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables
selective verification, drastically reducing errors (14-100%) with minimal
abstention, transforming LLM-driven formalization into a reliable engineering
discipline.

</details>


### [214] [Incentivizing Reasoning from Weak Supervision](https://arxiv.org/abs/2505.20072)
*Yige Yuan,Teng Xiao,Shuchang Tao,Xue Wang,Jinyang Gao,Bolin Ding,Bingbing Xu*

Main category: cs.CL

TL;DR: 提出通过弱监督方法（W2SR）用低质量小模型提升大语言模型推理能力，成本仅为强化学习的极小部分


<details>
  <summary>Details</summary>
Motivation: 传统强化学习和链式思考监督学习成本过高，探索是否能用弱监督信号有效激发大语言模型的推理能力

Method: 使用显著弱于学生模型的推理器（如GPT-3.5监督PaLM 2-L）进行监督训练，建立弱到强的知识迁移框架

Result: 在多项推理任务中恢复RL方法94%的性能增益，代码已开源（https://github.com/yuanyige/W2SR）

Conclusion: 弱到强监督范式是替代昂贵推理能力提升方法的有效方案，具有良好泛化性和应用前景

Abstract: Large language models (LLMs) have demonstrated impressive performance on
reasoning-intensive tasks, but enhancing their reasoning abilities typically
relies on either reinforcement learning (RL) with verifiable signals or
supervised fine-tuning (SFT) with high-quality long chain-of-thought (CoT)
demonstrations, both of which are expensive. In this paper, we study a novel
problem of incentivizing the reasoning capacity of LLMs without expensive
high-quality demonstrations and reinforcement learning. We investigate whether
the reasoning capabilities of LLMs can be effectively incentivized via
supervision from significantly weaker models. We further analyze when and why
such weak supervision succeeds in eliciting reasoning abilities in stronger
models. Our findings show that supervision from significantly weaker reasoners
can substantially improve student reasoning performance, recovering close to
94% of the gains of expensive RL at a fraction of the cost. Experiments across
diverse benchmarks and model architectures demonstrate that weak reasoners can
effectively incentivize reasoning in stronger student models, consistently
improving performance across a wide range of reasoning tasks. Our results
suggest that this simple weak-to-strong paradigm is a promising and
generalizable alternative to costly methods for incentivizing strong reasoning
capabilities at inference-time in LLMs. The code is publicly available at
https://github.com/yuanyige/W2SR.

</details>


### [215] [Inference-time Alignment in Continuous Space](https://arxiv.org/abs/2505.20081)
*Yige Yuan,Teng Xiao,Li Yunfan,Bingbing Xu,Shuchang Tao,Yunqi Qiu,Huawei Shen,Xueqi Cheng*

Main category: cs.CL

TL;DR: 提出SEA算法，通过连续潜在空间的梯度采样实现更高效的推理对齐，相比基线方法在AdvBench和MATH任务分别提升77.51%和16.36%


<details>
  <summary>Details</summary>
Motivation: 现有基于离散响应空间搜索的推理对齐方法在基础策略较弱或候选集较小时效果受限，需要更有效的优化方式

Method: 在连续潜在空间定义能量函数，通过梯度采样迭代优化基础策略的输出，实现端到端的对齐优化

Result: 在AdvBench任务取得77.51%的相对改进，MATH任务提升16.36%，且算法实现简单高效

Conclusion: SEA算法通过连续空间优化突破了离散搜索的局限性，为推理时对齐提供了简单而有效的解决方案

Abstract: Aligning large language models with human feedback at inference time has
received increasing attention due to its flexibility. Existing methods rely on
generating multiple responses from the base policy for search using a reward
model, which can be considered as searching in a discrete response space.
However, these methods struggle to explore informative candidates when the base
policy is weak or the candidate set is small, resulting in limited
effectiveness. In this paper, to address this problem, we propose Simple Energy
Adaptation ($\textbf{SEA}$), a simple yet effective algorithm for
inference-time alignment. In contrast to expensive search over the discrete
space, SEA directly adapts original responses from the base policy toward the
optimal one via gradient-based sampling in continuous latent space.
Specifically, SEA formulates inference as an iterative optimization procedure
on an energy function over actions in the continuous space defined by the
optimal policy, enabling simple and effective alignment. For instance, despite
its simplicity, SEA outperforms the second-best baseline with a relative
improvement of up to $ \textbf{77.51%}$ on AdvBench and $\textbf{16.36%}$ on
MATH. Our code is publicly available at https://github.com/yuanyige/SEA

</details>


### [216] [Multi-Domain Explainability of Preferences](https://arxiv.org/abs/2505.20088)
*Nitay Calderon,Liat Ein-Dor,Roi Reichart*

Main category: cs.CL

TL;DR: 开发了一种自动化端到端方法，通过概念向量和分层回归模型解释LLM偏好机制，并在多领域验证其预测性能和应用价值


<details>
  <summary>Details</summary>
Motivation: 现有基于人类偏好/LLM评判/奖励模型的偏好机制缺乏对其底层驱动概念的系统解释，阻碍LLM对齐与评估过程的可解释性提升

Method: 1) 用LLM自动发现区分选择/拒绝响应的概念并向量化 2) 提出分层多领域回归模型捕捉领域通用/特定效应 3) 构建8领域数据集验证12种机制

Result: 1) 预测性能优于基线且可解释 2) 概念指导使LLM输出更符合评判偏好 3) 人类概念解释提升LLM评判预测准确性

Conclusion: 该工作建立了LLM时代可解释性新范式，通过概念驱动的解释方法同时实现高性能预测和跨领域应用指导

Abstract: Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and
reward models, are central to aligning and evaluating large language models
(LLMs). Yet, the underlying concepts that drive these preferences remain poorly
understood. In this work, we propose a fully automated end-to-end method for
generating local and global concept-based explanations of preferences across
multiple domains. Our method employs an LLM to discover concepts that
differentiate between chosen and rejected responses and represent them with
concept-based vectors. To model the relationships between concepts and
preferences, we propose a white-box Hierarchical Multi-Domain Regression model
that captures both domain-general and domain-specific effects. To evaluate our
method, we curate a dataset spanning eight challenging and diverse domains and
explain twelve mechanisms. Our method achieves strong preference prediction
performance, outperforming baselines while also being explainable.
Additionally, we assess explanations in two novel application-driven settings.
First, guiding LLM outputs with concepts from LaaJ explanations yields
responses that those judges consistently prefer. Second, prompting LaaJs with
concepts explaining humans improves their preference predictions. Together, our
work provides a new paradigm for explainability in the era of LLMs.

</details>


### [217] [MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.20096)
*Thang Nguyen,Peter Chin,Yu-Wing Tai*

Main category: cs.CL

TL;DR: MA-RAG提出多智能体框架解决RAG中的模糊推理问题，通过分解任务+代理协作，实验显示优于现有方法且无需微调


<details>
  <summary>Details</summary>
Motivation: 传统RAG在处理复杂信息检索时存在查询模糊/证据稀疏/多源整合困难等问题，需要更细粒度的推理控制机制

Method: 构建Planner/Step Definer/Extractor/QA四类代理，采用思维链提示进行任务分解，动态调用代理实现检索-合成流程优化

Result: 在多跳和模糊QA基准测试中超越无训练基线，性能接近微调系统（HotpotQA准确率提升5.2%）

Conclusion: 模块化代理架构通过显式推理步骤提升RAG鲁棒性，验证了协作式代理在复杂信息处理中的有效性

Abstract: We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation
(RAG) that addresses the inherent ambiguities and reasoning challenges in
complex information-seeking tasks. Unlike conventional RAG methods that rely on
either end-to-end fine-tuning or isolated component enhancements, MA-RAG
orchestrates a collaborative set of specialized AI agents: Planner, Step
Definer, Extractor, and QA Agents, to tackle each stage of the RAG pipeline
with task-aware reasoning. Ambiguities may arise from underspecified queries,
sparse or indirect evidence in retrieved documents, or the need to integrate
information scattered across multiple sources. MA-RAG mitigates these
challenges by decomposing the problem into subtasks, such as query
disambiguation, evidence extraction, and answer synthesis, and dispatching them
to dedicated agents equipped with chain-of-thought prompting. These agents
communicate intermediate reasoning and progressively refine the retrieval and
synthesis process. Our design allows fine-grained control over information flow
without any model fine-tuning. Crucially, agents are invoked on demand,
enabling a dynamic and efficient workflow that avoids unnecessary computation.
This modular and reasoning-driven architecture enables MA-RAG to deliver
robust, interpretable results. Experiments on multi-hop and ambiguous QA
benchmarks demonstrate that MA-RAG outperforms state-of-the-art training-free
baselines and rivals fine-tuned systems, validating the effectiveness of
collaborative agent-based reasoning in RAG.

</details>


### [218] [S2LPP: Small-to-Large Prompt Prediction across LLMs](https://arxiv.org/abs/2505.20097)
*Liang Cheng,Tianyi LI,Zhaowei Wang,Mark Steedman*

Main category: cs.CL

TL;DR: 利用小模型选择提示模板优化大语言模型性能，显著降低提示工程成本


<details>
  <summary>Details</summary>
Motivation: 大语言模型对提示模板敏感，传统提示工程需耗费大量计算资源和人力成本

Method: 通过实验发现不同规模LLMs的提示偏好一致性，提出用小模型的提示选择结果指导大模型

Result: 方法在14个LLMs上验证有效，跨多个NLP任务保持与最优提示相当的准确率

Conclusion: 该策略显著降低提示工程成本，具有跨模型规模和任务类型的强鲁棒性

Abstract: The performance of pre-trained Large Language Models (LLMs) is often
sensitive to nuances in prompt templates, requiring careful prompt engineering,
adding costs in terms of computing and human effort. In this study, we present
experiments encompassing multiple LLMs variants of varying sizes aimed at
probing their preference with different prompts. Through experiments on
Question Answering, we show prompt preference consistency across LLMs of
different sizes. We also show that this consistency extends to other tasks,
such as Natural Language Inference. Utilizing this consistency, we propose a
method to use a smaller model to select effective prompt templates for a larger
model. We show that our method substantially reduces the cost of prompt
engineering while consistently matching performance with optimal prompts among
candidates. More importantly, our experiment shows the efficacy of our strategy
across fourteen LLMs and its applicability to a broad range of NLP tasks,
highlighting its robustness

</details>


### [219] [Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities](https://arxiv.org/abs/2505.20099)
*Chuangtao Ma,Yongrui Chen,Tianxing Wu,Arijit Khan,Haofen Wang*

Main category: cs.CL

TL;DR: 大语言模型与知识图谱融合的QA方法调研：提出结构化分类法，系统分析LLM+KG在不同QA场景中的协同机制与效果


<details>
  <summary>Details</summary>
Motivation: 解决LLM在复杂QA任务中存在的推理能力不足、知识滞后和幻觉问题，通过知识图谱增强LLM的推理与事实性

Method: 构建新型分类体系（按QA类型和KG角色分类），系统性梳理LLM+KG的协同方法（增强、替代、混合模式），对比分析不同方法的优劣及KG需求

Result: 建立跨维度方法评估框架，揭示不同方法在复杂QA（多跳/反事实/时序推理）中的适配性，总结现有评估指标（准确性/可解释性）及基准数据集（MetaQA/CommonsenseQA）

Conclusion: LLM+KG协同显著提升复杂QA性能，但面临动态知识更新、多模态KG整合及可解释性评估等开放挑战

Abstract: Large language models (LLMs) have demonstrated remarkable performance on
question-answering (QA) tasks because of their superior capabilities in natural
language understanding and generation. However, LLM-based QA struggles with
complex QA tasks due to poor reasoning capacity, outdated knowledge, and
hallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs)
for QA to address the above challenges. In this survey, we propose a new
structured taxonomy that categorizes the methodology of synthesizing LLMs and
KGs for QA according to the categories of QA and the KG's role when integrating
with LLMs. We systematically survey state-of-the-art advances in synthesizing
LLMs and KGs for QA and compare and analyze these approaches in terms of
strength, limitations, and KG requirements. We then align the approaches with
QA and discuss how these approaches address the main challenges of different
complex QA. Finally, we summarize the advancements, evaluation metrics, and
benchmark datasets and highlight open challenges and opportunities.

</details>


### [220] [Adaptive Deep Reasoning: Triggering Deep Thinking When Needed](https://arxiv.org/abs/2505.20101)
*Yunhao Wang,Yuhao Zhang,Tinghao Yu,Can Xu,Feng Zhang,Fengzong Lian*

Main category: cs.CL

TL;DR: 提出基于问题复杂度自主切换长短推理链的方法，通过监督微调结合强化学习实现高效推理，在保持性能的同时显著降低计算成本


<details>
  <summary>Details</summary>
Motivation: 现有缩短思维链的方法仍需初始推理阶段，且长短推理能力切换依赖人工控制，无法满足实际应用对动态推理的需求

Method: 1. 基于监督微调同时获得长短链推理能力
2. 采用包含长短自适应分组奖励策略的强化学习框架
3. 设计基于logit的推理模式切换损失函数优化初始token选择

Result: 在数学数据集验证中实现动态推理切换，性能损失控制在0.8%以内（GSM8K: 80.1→79.3, MATH: 34.2→33.9）

Conclusion: 该方法显著提升LLMs在实际应用中的推理效率，为复杂任务处理提供更灵活的解决方案

Abstract: Large language models (LLMs) have shown impressive capabilities in handling
complex tasks through long-chain reasoning. However, the extensive reasoning
steps involved can significantly increase computational costs, posing
challenges for real-world deployment. Recent efforts have focused on optimizing
reasoning efficiency by shortening the Chain-of-Thought (CoT) reasoning
processes through various approaches, such as length-aware prompt engineering,
supervised fine-tuning on CoT data with variable lengths, and reinforcement
learning with length penalties. Although these methods effectively reduce
reasoning length, they still necessitate an initial reasoning phase. More
recent approaches have attempted to integrate long-chain and short-chain
reasoning abilities into a single model, yet they still rely on manual control
to toggle between short and long CoT.In this work, we propose a novel approach
that autonomously switches between short and long reasoning chains based on
problem complexity. Our method begins with supervised fine-tuning of the base
model to equip both long-chain and short-chain reasoning abilities. We then
employ reinforcement learning to further balance short and long CoT generation
while maintaining accuracy through two key strategies: first, integrating
reinforcement learning with a long-short adaptive group-wise reward strategy to
assess prompt complexity and provide corresponding rewards; second,
implementing a logit-based reasoning mode switching loss to optimize the
model's initial token choice, thereby guiding the selection of the reasoning
type.Evaluations on mathematical datasets demonstrate that our model can
dynamically switch between long-chain and short-chain reasoning modes without
substantially sacrificing performance. This advancement enhances the
practicality of reasoning in large language models for real-world applications.

</details>


### [221] [Language-Agnostic Suicidal Risk Detection Using Large Language Models](https://arxiv.org/abs/2505.20109)
*June-Woo Kim,Wonkyo Oh,Haram Yoon,Sung-Hoon Yoon,Dae-Jin Kim,Dong-Ho Lee,Sang-Yeol Lee,Chan-Mo Yang*

Main category: cs.CL

TL;DR: 提出语言无关的大模型框架，通过ASR和跨语言特征提取实现鲁棒的自杀风险评估


<details>
  <summary>Details</summary>
Motivation: 现有自杀风险检测方法受限于语言特异性模型，导致可扩展性和跨语言泛化能力不足

Method: 1.使用ASR生成中文语音转写文本 2.大模型提示工程提取中英双语风险特征 3.独立微调预训练语言模型进行跨语言分析

Result: 模型性能达到直接微调ASR结果或单一中文特征模型的水平，验证了跨语言方法的有效性

Conclusion: 该框架突破语言限制，通过特征保留和跨语言分析显著提升自杀风险评估的鲁棒性和普适性

Abstract: Suicidal risk detection in adolescents is a critical challenge, yet existing
methods rely on language-specific models, limiting scalability and
generalization. This study introduces a novel language-agnostic framework for
suicidal risk assessment with large language models (LLMs). We generate Chinese
transcripts from speech using an ASR model and then employ LLMs with
prompt-based queries to extract suicidal risk-related features from these
transcripts. The extracted features are retained in both Chinese and English to
enable cross-linguistic analysis and then used to fine-tune corresponding
pretrained language models independently. Experimental results show that our
method achieves performance comparable to direct fine-tuning with ASR results
or to models trained solely on Chinese suicidal risk-related features,
demonstrating its potential to overcome language constraints and improve the
robustness of suicidal risk assessment.

</details>


### [222] [ResSVD: Residual Compensated SVD for Large Language Model Compression](https://arxiv.org/abs/2505.20112)
*Haolei Bai,Siyong Jian,Tuo Liang,Yu Yin,Huan Wang*

Main category: cs.CL

TL;DR: 提出ResSVD压缩方法：通过残差矩阵利用+分层选择性压缩策略，在固定压缩率下显著提升LLM压缩性能


<details>
  <summary>Details</summary>
Motivation: 大语言模型部署面临存储和计算资源限制，现有SVD压缩方法存在截断残差矩阵浪费和全层压缩导致的性能退化问题

Method: 1. 利用截断产生的残差矩阵重构参数降低截断损失
2. 固定总压缩率下优先压缩模型最后几层，抑制误差传播

Result: 在多个LLM家族和基准测试中持续超越现有方法，验证方案有效性

Conclusion: ResSVD通过残差利用和分层压缩策略，在保持压缩效率的同时显著降低性能损失，为LLM部署提供实用解决方案

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in a
wide range of downstream natural language processing tasks. Nevertheless, their
considerable sizes and memory demands hinder practical deployment, underscoring
the importance of developing efficient compression strategies. Singular value
decomposition (SVD) decomposes a matrix into orthogonal components, enabling
efficient low-rank approximation. This is particularly suitable for LLM
compression, where weight matrices often exhibit significant redundancy.
However, current SVD-based methods neglect the residual matrix from truncation,
resulting in significant truncation loss. Additionally, compressing all layers
of the model results in severe performance degradation. To overcome these
limitations, we propose ResSVD, a new post-training SVD-based LLM compression
method. Specifically, we leverage the residual matrix generated during the
truncation process to reduce truncation loss. Moreover, under a fixed overall
compression ratio, we selectively compress the last few layers of the model,
which mitigates error propagation and significantly improves the performance of
compressed models.Comprehensive evaluations of ResSVD on diverse LLM families
and multiple benchmark datasets indicate that ResSVD consistently achieves
superior performance over existing counterpart methods, demonstrating its
practical effectiveness.

</details>


### [223] [Named Entity Recognition in Historical Italian: The Case of Giacomo Leopardi's Zibaldone](https://arxiv.org/abs/2505.20113)
*Cristian Santini,Laura Melosi,Emanuele Frontoni*

Main category: cs.CL

TL;DR: 论文通过构建19世纪意大利语历史文本数据集，对比微调NER模型与指令调优大语言模型的效果，发现领域专用模型在实体识别任务中更具鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决历史文本（拼写变异、碎片化结构、数字化错误）的计算分析难题，填补意大利语历史文献NER评估的空白。

Method: 基于Leopardi的《Zibaldone》创建含2,899个实体标注的数据集，测试领域专用BERT模型与LLaMa3.1等大模型。

Result: 指令调优大模型处理人文历史文本存在困难，微调NER模型在书目引用等复杂实体类型上表现更稳定。

Conclusion: 历史人文文本处理需领域适配方法，大语言模型需针对性优化才能有效应用于文化遗产数字化场景。

Abstract: The increased digitization of world's textual heritage poses significant
challenges for both computer science and literary studies. Overall, there is an
urgent need of computational techniques able to adapt to the challenges of
historical texts, such as orthographic and spelling variations, fragmentary
structure and digitization errors. The rise of large language models (LLMs) has
revolutionized natural language processing, suggesting promising applications
for Named Entity Recognition (NER) on historical documents. In spite of this,
no thorough evaluation has been proposed for Italian texts. This research tries
to fill the gap by proposing a new challenging dataset for entity extraction
based on a corpus of 19th century scholarly notes, i.e. Giacomo Leopardi's
Zibaldone (1898), containing 2,899 references to people, locations and literary
works. This dataset was used to carry out reproducible experiments with both
domain-specific BERT-based models and state-of-the-art LLMs such as LLaMa3.1.
Results show that instruction-tuned models encounter multiple difficulties
handling historical humanistic texts, while fine-tuned NER models offer more
robust performance even with challenging entity types such as bibliographic
references.

</details>


### [224] [TrojanStego: Your Language Model Can Secretly Be A Steganographic Privacy Leaking Agent](https://arxiv.org/abs/2505.20118)
*Dominik Meier,Jan Philip Wahle,Paul Röttger,Terry Ruas,Bela Gipp*

Main category: cs.CL

TL;DR: 提出TrojanStego攻击模型，通过微调语言模型实现无需控制输入的被动式隐写数据泄露，32位密钥传输准确率达87%，揭示LLM新型安全威胁。


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型在敏感场景中可能泄露机密信息的问题，研究被动式隐写数据外泄的攻击可行性，突破传统需要主动注入攻击指令的限制。

Method: 基于词汇表分区的编码方案，通过微调使LLM自主将敏感信息编码到自然文本输出中，实现无需触发词控制的隐蔽传输。

Result: 被控模型在保留93.2%文本质量的同时，单次生成密钥准确率87%，三次多数投票提升至97.3%，人类评估仅3.8%能识别异常。

Conclusion: 该研究揭示LLM存在被动数据泄露的新攻击范式，具有隐蔽性强、实用化程度高、难以检测的特点，对模型安全防护提出新挑战。

Abstract: As large language models (LLMs) become integrated into sensitive workflows,
concerns grow over their potential to leak confidential information. We propose
TrojanStego, a novel threat model in which an adversary fine-tunes an LLM to
embed sensitive context information into natural-looking outputs via linguistic
steganography, without requiring explicit control over inference inputs. We
introduce a taxonomy outlining risk factors for compromised LLMs, and use it to
evaluate the risk profile of the threat. To implement TrojanStego, we propose a
practical encoding scheme based on vocabulary partitioning learnable by LLMs
via fine-tuning. Experimental results show that compromised models reliably
transmit 32-bit secrets with 87% accuracy on held-out prompts, reaching over
97% accuracy using majority voting across three generations. Further, they
maintain high utility, can evade human detection, and preserve coherence. These
results highlight a new class of LLM data exfiltration attacks that are
passive, covert, practical, and dangerous.

</details>


### [225] [Iterative Self-Incentivization Empowers Large Language Models as Agentic Searchers](https://arxiv.org/abs/2505.20128)
*Zhengliang Shi,Lingyong Yan,Dawei Yin,Suzan Verberne,Maarten de Rijke,Zhaochun Ren*

Main category: cs.CL

TL;DR: 提出EXSEARCH框架，通过自我激励学习机制改进LLM在复杂检索任务中的表现，实验显示准确率提升7.8%


<details>
  <summary>Details</summary>
Motivation: 传统LLM在复杂多跳查询中存在无关内容检索问题，需要设计动态学习机制来提升知识获取准确性

Method: 采用代理框架分三步执行（思考-搜索-记录），结合广义期望最大化算法实现自我迭代训练

Result: 在4个知识密集型基准测试中显著超越基线模型，准确匹配分数提升7.8%，并扩展出通用框架EXSEARCH-Zoo

Conclusion: 通过理论证明和实验验证框架有效性，自我激励学习机制成功提升LLM检索能力，扩展应用场景促进领域发展

Abstract: Large language models (LLMs) have been widely integrated into information
retrieval to advance traditional techniques. However, effectively enabling LLMs
to seek accurate knowledge in complex tasks remains a challenge due to the
complexity of multi-hop queries as well as the irrelevant retrieved content. To
address these limitations, we propose EXSEARCH, an agentic search framework,
where the LLM learns to retrieve useful information as the reasoning unfolds
through a self-incentivized process. At each step, the LLM decides what to
retrieve (thinking), triggers an external retriever (search), and extracts
fine-grained evidence (recording) to support next-step reasoning. To enable LLM
with this capability, EXSEARCH adopts a Generalized Expectation-Maximization
algorithm. In the E-step, the LLM generates multiple search trajectories and
assigns an importance weight to each; the M-step trains the LLM on them with a
re-weighted loss function. This creates a self-incentivized loop, where the LLM
iteratively learns from its own generated data, progressively improving itself
for search. We further theoretically analyze this training process,
establishing convergence guarantees. Extensive experiments on four
knowledge-intensive benchmarks show that EXSEARCH substantially outperforms
baselines, e.g., +7.8% improvement on exact match score. Motivated by these
promising results, we introduce EXSEARCH-Zoo, an extension that extends our
method to broader scenarios, to facilitate future work.

</details>


### [226] [AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings](https://arxiv.org/abs/2505.20133)
*Konstantin Dobler,Desmond Elliott,Gerard de Melo*

Main category: cs.CL

TL;DR: 提出AweDist方法，通过蒸馏原始分词表示快速生成高质量新词元嵌入，无需额外训练即可超越基线模型


<details>
  <summary>Details</summary>
Motivation: 静态词汇表导致领域覆盖不足时性能下降和计算成本增加，现有嵌入初始化方法需要昂贵训练或额外模块

Method: 通过蒸馏(distill)原始分词获得的表示来初始化新词元嵌入

Result: 在多种开源模型实验中，AweDist表现优于包括强基线在内的对比方法

Conclusion: 该方法能高效生成高质量新词元嵌入，显著提升模型对新领域词汇的适应性

Abstract: Current language models rely on static vocabularies determined at pretraining
time, which can lead to decreased performance and increased computational cost
for domains underrepresented in the original vocabulary. New tokens can be
added to solve this problem, when coupled with a good initialization for their
new embeddings. However, existing embedding initialization methods either
require expensive further training or pretraining of additional modules. In
this paper, we propose AweDist and show that by distilling representations
obtained using the original tokenization, we can quickly learn high-quality
input embeddings for new tokens. Experimental results with a wide range of
open-weight models show that AweDist is able to outperform even strong
baselines.

</details>


### [227] [SeMe: Training-Free Language Model Merging via Semantic Alignment](https://arxiv.org/abs/2505.20144)
*Jian Gu,Aldeida Aleti,Chunyang Chen,Hongyu Zhang*

Main category: cs.CL

TL;DR: SeMe提出了一种无需数据和训练的语言模型融合方法，通过语义对齐实现细粒度层合并，在保留模型知识的同时提升性能表现。


<details>
  <summary>Details</summary>
Motivation: 现有模型融合方法依赖数据计算且难以保持内部知识，需要更高效的无训练融合方案来结合不同语言模型优势。

Method: 基于潜在语义对齐的细粒度层合并技术，通过稳定模型内部知识实现参数级融合。

Result: 在多种架构和任务上的实验表明，SeMe在性能效率上超越现有方法，且完全摆脱对外部数据的依赖。

Conclusion: 该研究建立了知识感知模型融合新范式，为语言模型语义结构分析及可扩展模型组合提供了新方向。

Abstract: Despite the remarkable capabilities of Language Models (LMs) across diverse
tasks, no single model consistently outperforms others, necessitating efficient
methods to combine their strengths without expensive retraining. Existing model
merging techniques, such as parameter averaging and task-guided fusion, often
rely on data-dependent computations or fail to preserve internal knowledge,
limiting their robustness and scalability. We introduce SeMe (Semantic-based
Merging), a novel, data-free, and training-free approach that leverages latent
semantic alignment to merge LMs at a fine-grained, layer-wise level. Unlike
prior work, SeMe not only preserves model behaviors but also explicitly
stabilizes internal knowledge, addressing a critical gap in LM fusion. Through
extensive experiments across diverse architectures and tasks, we demonstrate
that SeMe outperforms existing methods in both performance and efficiency while
eliminating reliance on external data. Our work establishes a new paradigm for
knowledge-aware model merging and provides insights into the semantic structure
of LMs, paving the way for more scalable and interpretable model composition.

</details>


### [228] [UORA: Uniform Orthogonal Reinitialization Adaptation in Parameter-Efficient Fine-Tuning of Large Models](https://arxiv.org/abs/2505.20154)
*Xueyan Zhang,Jinman Zhao,Zhifei Yang,Yibo Zhong,Shuhao Guan,Linbo Cao,Yining Wang*

Main category: cs.CL

TL;DR: UORA提出了一种新型参数高效微调方法，通过低秩近似和选择性重初始化机制显著降低训练参数量，在保持性能的同时实现计算存储效率提升


<details>
  <summary>Details</summary>
Motivation: 现有PEFT方法（如LoRA/VeRA）存在参数量大或计算效率低的问题，需要更高效的LLM微调方案

Method: 采用基于插值的重参数化机制，通过向量幅值启发式选择性地重初始化冻结投影矩阵的行列

Result: 在GLUE/E2E基准测试中达到SOTA，参数量比LoRA减少76.5%，训练速度比VeRA提升3倍

Conclusion: UORA建立了LLM高效微调新范式，在参数效率和计算性能之间取得更好平衡

Abstract: This paper introduces Uniform Orthogonal Reinitialization Adaptation (UORA),
a novel parameter-efficient fine-tuning (PEFT) approach for Large Language
Models (LLMs). UORA achieves state-of-the-art performance and parameter
efficiency by leveraging a low-rank approximation method to reduce the number
of trainable parameters. Unlike existing methods such as LoRA and VeRA, UORA
employs an interpolation-based reparametrization mechanism that selectively
reinitializes rows and columns in frozen projection matrices, guided by the
vector magnitude heuristic. This results in substantially fewer trainable
parameters compared to LoRA and outperforms VeRA in computation and storage
efficiency. Comprehensive experiments across various benchmarks demonstrate
UORA's superiority in achieving competitive fine-tuning performance with
negligible computational overhead. We demonstrate its performance on GLUE and
E2E benchmarks and its effectiveness in instruction-tuning large language
models and image classification models. Our contributions establish a new
paradigm for scalable and resource-efficient fine-tuning of LLMs.

</details>


### [229] [Pangu Light: Weight Re-Initialization for Pruning and Accelerating LLMs](https://arxiv.org/abs/2505.20155)
*Hanting Chen,Jiarui Qin,Jialong Guo,Tao Yuan,Yichun Yin,Huiling Zhen,Yasheng Wang,Jinpeng Li,Xiaojun Meng,Meng Zhang,Rongju Ruan,Zheyuan Bai,Yehui Tang,Can Chen,Xinghao Chen,Fisher Yu,Ruiming Tang,Yunhe Wang*

Main category: cs.CL

TL;DR: 提出Pangu Light框架，通过结构化剪枝与权重重初始化技术提升大语言模型压缩效率，在精度与效率平衡上优于主流方法


<details>
  <summary>Details</summary>
Motivation: 现有结构化剪枝方法在同时进行宽度和深度压缩时性能下降严重，需通过权重重初始化策略改善剪枝后训练起点

Method: 开发CLAP（跨层注意力剪枝）和SLNP（稳定层归一化剪枝）技术，结合RMSNorm吸收计算优化，适配昇腾NPU特性

Result: Pangu Light-32B在昇腾NPU上实现81.6平均分/2585tokens/s，优于Qwen3-32B的80.9分/2225tokens/s

Conclusion: 通过结构化剪枝与系统性重初始化策略，Pangu Light实现了更优的精度-效率权衡，验证了权重初始化对压缩效果的关键作用

Abstract: Large Language Models (LLMs) deliver state-of-the-art capabilities across
numerous tasks, but their immense size and inference costs pose significant
computational challenges for practical deployment. While structured pruning
offers a promising avenue for model compression, existing methods often
struggle with the detrimental effects of aggressive, simultaneous width and
depth reductions, leading to substantial performance degradation. This paper
argues that a critical, often overlooked, aspect in making such aggressive
joint pruning viable is the strategic re-initialization and adjustment of
remaining weights to improve the model post-pruning training accuracies. We
introduce Pangu Light, a framework for LLM acceleration centered around
structured pruning coupled with novel weight re-initialization techniques
designed to address this ``missing piece''. Our framework systematically
targets multiple axes, including model width, depth, attention heads, and
RMSNorm, with its effectiveness rooted in novel re-initialization methods like
Cross-Layer Attention Pruning (CLAP) and Stabilized LayerNorm Pruning (SLNP)
that mitigate performance drops by providing the network a better training
starting point. Further enhancing efficiency, Pangu Light incorporates
specialized optimizations such as absorbing Post-RMSNorm computations and
tailors its strategies to Ascend NPU characteristics. The Pangu Light models
consistently exhibit a superior accuracy-efficiency trade-off, outperforming
prominent baseline pruning methods like Nemotron and established LLMs like
Qwen3 series. For instance, on Ascend NPUs, Pangu Light-32B's 81.6 average
score and 2585 tokens/s throughput exceed Qwen3-32B's 80.9 average score and
2225 tokens/s.

</details>


### [230] [Exploring Generative Error Correction for Dysarthric Speech Recognition](https://arxiv.org/abs/2505.20163)
*Moreno La Quatra,Alkis Koudounas,Valerio Mario Salerno,Sabato Marco Siniscalchi*

Main category: cs.CL

TL;DR: 提出了两阶段框架（ASR模型+LLM纠错）改善构音障碍语音识别，在结构化语音表现良好但单字识别仍存挑战


<details>
  <summary>Details</summary>
Motivation: 现有端到端ASR在构音障碍语音识别上精度不足，需探索更有效的解决方案

Method: 结合前沿语音识别模型与基于LLM的生成式错误校正(GER)，采用不同模型规模/训练策略配置及假设选择机制

Result: 在Speech Accessibility数据集上验证框架有效性（结构化/自发语音准确率提升），但单词语音识别仍存在显著困难

Conclusion: 声学模型与语言模型在构音障碍语音识别中具有互补作用，需平衡两者关系以提升系统鲁棒性

Abstract: Despite the remarkable progress in end-to-end Automatic Speech Recognition
(ASR) engines, accurately transcribing dysarthric speech remains a major
challenge. In this work, we proposed a two-stage framework for the Speech
Accessibility Project Challenge at INTERSPEECH 2025, which combines
cutting-edge speech recognition models with LLM-based generative error
correction (GER). We assess different configurations of model scales and
training strategies, incorporating specific hypothesis selection to improve
transcription accuracy. Experiments on the Speech Accessibility Project dataset
demonstrate the strength of our approach on structured and spontaneous speech,
while highlighting challenges in single-word recognition. Through comprehensive
analysis, we provide insights into the complementary roles of acoustic and
linguistic modeling in dysarthric speech recognition

</details>


### [231] [Visual Abstract Thinking Empowers Multimodal Reasoning](https://arxiv.org/abs/2505.20164)
*Dairu Liu,Ziyue Wang,Minyuan Ruan,Fuwen Luo,Chi Chen,Peng Li,Yang Liu*

Main category: cs.CL

TL;DR: 提出视觉抽象思维（VAT）范式，通过简化冗余视觉信息提升多模态大语言模型的视觉推理能力，相比GPT-4o基线平均提升17%。


<details>
  <summary>Details</summary>
Motivation: 图像信息冗余可能降低多模态推理性能，受人类抽象思维启发，用视觉抽象替代显式语言思考以聚焦关键视觉元素。

Method: 采用视觉抽象替代思维链（CoT）的显式中间步骤，通过概念化/结构化/关系型抽象压缩冗余信息，强化模型对本质视觉要素的推理。

Result: VAT使不同模型在概念/结构/关系推理任务中全面提升，与CoT兼容且在知识密集型任务中表现更优，最高提升达17%。

Conclusion: 视觉抽象思维验证了人类认知启发的有效性，需继续探索更多认知视角下的多模态推理范式创新。

Abstract: Images usually convey richer detail than text, but often include redundant
information which potentially downgrades multimodal reasoning performance. When
faced with lengthy or complex messages, humans tend to employ abstract thinking
to convert them into simple and concise abstracts. Inspired by this cognitive
strategy, we introduce Visual Abstract Thinking (VAT), a novel thinking
paradigm that prompts Multimodal Large Language Models (MLLMs) with visual
abstract instead of explicit verbal thoughts or elaborate guidance, permitting
a more concentrated visual reasoning mechanism. Explicit thinking, such as
Chain-of-thought (CoT) or tool-augmented approaches, increases the complexity
of reasoning process via inserting verbose intermediate steps, external
knowledge or visual information. In contrast, VAT reduces redundant visual
information and encourages models to focus their reasoning on more essential
visual elements. Experimental results show that VAT consistently empowers
different models, and achieves an average gain of 17% over GPT-4o baseline by
employing diverse types of visual abstracts, demonstrating that VAT can enhance
visual reasoning abilities for MLLMs regarding conceptual, structural and
relational reasoning tasks. VAT is also compatible with CoT in
knowledge-intensive multimodal reasoning tasks. These findings highlight the
effectiveness of visual reasoning via abstract thinking and encourage further
exploration of more diverse reasoning paradigms from the perspective of human
cognition.

</details>


### [232] ["KAN you hear me?" Exploring Kolmogorov-Arnold Networks for Spoken Language Understanding](https://arxiv.org/abs/2505.20176)
*Alkis Koudounas,Moreno La Quatra,Eliana Pastor,Sabato Marco Siniscalchi,Elena Baralis*

Main category: cs.CL

TL;DR: 首次将KAN网络应用于语音理解任务，证明其可替代传统线性层并取得相当或更优表现


<details>
  <summary>Details</summary>
Motivation: KAN网络作为新兴神经网络架构，在语音处理领域尚未充分探索。本文旨在验证其在复杂口语理解任务中的适用性。

Method: 1. 在2D-CNN模型的两个数据集上测试五种KAN层配置
2. 将最佳配置（线性层间插入KAN层）应用于Transformer模型
3. 在五个复杂度递增的SLU数据集评估

Result: KAN层在多数情况下达到或超越传统线性层性能，并揭示其在原始波形关注区域与线性层的差异

Conclusion: KAN层可有效替代Transformer顶部的线性层，为语音模型架构设计提供新方向

Abstract: Kolmogorov-Arnold Networks (KANs) have recently emerged as a promising
alternative to traditional neural architectures, yet their application to
speech processing remains under explored. This work presents the first
investigation of KANs for Spoken Language Understanding (SLU) tasks. We
experiment with 2D-CNN models on two datasets, integrating KAN layers in five
different configurations within the dense block. The best-performing setup,
which places a KAN layer between two linear layers, is directly applied to
transformer-based models and evaluated on five SLU datasets with increasing
complexity. Our results show that KAN layers can effectively replace the linear
layers, achieving comparable or superior performance in most cases. Finally, we
provide insights into how KAN and linear layers on top of transformers
differently attend to input regions of the raw waveforms.

</details>


### [233] [THiNK: Can Large Language Models Think-aloud?](https://arxiv.org/abs/2505.20184)
*Yongan Yu,Mengqian Wu,Yiran Lin,Nikki G. Lobczowski*

Main category: cs.CL

TL;DR: THiNK框架通过多智能体反馈机制评估大语言模型的高阶思维能力，发现模型虽擅长低阶认知任务，但在知识应用和抽象推理方面存在明显短板，结构化反馈显著提升推理表现。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型评估方法难以系统检验高阶思维能力（如评估、创造），基于布鲁姆分类学构建多阶段反馈框架以填补该空白。

Method: 基于布鲁姆分类学开发THiNK框架，通过问题生成-批判-修订的迭代流程，结合7个前沿LLM的认知分析及定性评估。

Result: 模型在评估/创造等高阶任务中准确率比低阶任务低38%，反馈机制使推理性能提升21%，输出更符合领域逻辑。

Conclusion: THiNK为LLM认知能力评估提供了基于学习科学的可扩展方法论，反馈机制设计为增强模型推理能力开辟新路径。

Abstract: Assessing higher-order thinking skills in large language models (LLMs)
remains a fundamental challenge, especially in tasks that go beyond
surface-level accuracy. In this work, we propose THiNK (Testing Higher-order
Notion of Knowledge), a multi-agent, feedback-driven evaluation framework
grounded in Bloom's Taxonomy. THiNK frames reasoning assessment as an iterative
task of problem generation, critique, and revision, encouraging LLMs to
think-aloud through step-by-step reflection and refinement. This enables a
systematic evaluation of both lower-order (e.g., remember, understand) and
higher-order (e.g., evaluate, create) thinking skills. We apply THiNK to seven
state-of-the-art LLMs and perform a detailed cognitive analysis of their
outputs. Results reveal that while models reliably perform lower-order
categories well, they struggle with applying knowledge in realistic contexts
and exhibit limited abstraction. Structured feedback loops significantly
improve reasoning performance, particularly in higher-order thinking.
Qualitative evaluations further confirm that THiNK-guided outputs better align
with domain logic and problem structure. The code of our framework provides a
scalable methodology for probing and enhancing LLM reasoning, offering new
directions for evaluation grounded in learning science, which is available at
our GitHub repository.

</details>


### [234] [Monocle: Hybrid Local-Global In-Context Evaluation for Long-Text Generation with Uncertainty-Based Active Learning](https://arxiv.org/abs/2505.20195)
*Xiaorong Wang,Ting Yang,Zhu Zhang,Shuo Wang,Zihan Zhou,Liner Yang,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: 提出分治评估框架解决长文本质量评估难题，结合局部评分与全局评估，并通过主动学习降低标注成本


<details>
  <summary>Details</summary>
Motivation: 现有LLM评估方法在长文本输入时存在性能衰减，需兼顾局部质量与整体一致性，同时减少人工标注负担

Method: 1. 分治策略分解评估任务为局部评分+全局整合 2. 混合上下文学习融合人工标注 3. 基于不确定性的主动学习算法筛选标注样本

Result: 实验表明该框架优于多个基线模型，验证了局部-全局评估与主动学习的协同有效性

Conclusion: 通过分治策略与混合学习机制，实现了长文本评估精度与效率的平衡，为实际应用提供低成本解决方案

Abstract: Assessing the quality of long-form, model-generated text is challenging, even
with advanced LLM-as-a-Judge methods, due to performance degradation as input
length increases. To address this issue, we propose a divide-and-conquer
approach, which breaks down the comprehensive evaluation task into a series of
localized scoring tasks, followed by a final global assessment. This strategy
allows for more granular and manageable evaluations, ensuring that each segment
of the text is assessed in isolation for both coherence and quality, while also
accounting for the overall structure and consistency of the entire piece.
Moreover, we introduce a hybrid in-context learning approach that leverages
human annotations to enhance the performance of both local and global
evaluations. By incorporating human-generated feedback directly into the
evaluation process, this method allows the model to better align with human
judgment. Finally, we develop an uncertainty-based active learning algorithm
that efficiently selects data samples for human annotation, thereby reducing
annotation costs in practical scenarios. Experimental results show that the
proposed evaluation framework outperforms several representative baselines,
highlighting the effectiveness of our approach.

</details>


### [235] [Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking](https://arxiv.org/abs/2505.20199)
*Pengxiang Li,Shilin Yan,Joey Tsai,Renrui Zhang,Ruichuan An,Ziyu Guo,Xiaowei Gao*

Main category: cs.CL

TL;DR: 提出自适应无分类器引导（A-CFG），通过动态调整无条件输入聚焦模型低置信度区域，显著提升语言生成效果。


<details>
  <summary>Details</summary>
Motivation: 传统CFG使用静态无条件输入在迭代生成中存在局限性，无法适应模型动态变化的不确定性。

Method: 在迭代生成过程中实时检测低置信度token并重新掩码，创建局部化无条件输入实现精准引导。

Result: GPQA基准提升3.9分，多个生成基准显著优于标准CFG。

Conclusion: 动态适配模型不确定性的引导机制能有效提升迭代生成质量，为生成模型优化提供新方向。

Abstract: Classifier-Free Guidance (CFG) significantly enhances controllability in
generative models by interpolating conditional and unconditional predictions.
However, standard CFG often employs a static unconditional input, which can be
suboptimal for iterative generation processes where model uncertainty varies
dynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel
method that tailors the unconditional input by leveraging the model's
instantaneous predictive confidence. At each step of an iterative (masked)
diffusion language model, A-CFG identifies tokens in the currently generated
sequence for which the model exhibits low confidence. These tokens are
temporarily re-masked to create a dynamic, localized unconditional input. This
focuses CFG's corrective influence precisely on areas of ambiguity, leading to
more effective guidance. We integrate A-CFG into a state-of-the-art masked
diffusion language model and demonstrate its efficacy. Experiments on diverse
language generation benchmarks show that A-CFG yields substantial improvements
over standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work
highlights the benefit of dynamically adapting guidance mechanisms to model
uncertainty in iterative generation.

</details>


### [236] [Reasoning Is Not All You Need: Examining LLMs for Multi-Turn Mental Health Conversations](https://arxiv.org/abs/2505.20201)
*Mohit Chandra,Siddharth Sriraman,Harneet Singh Khanuja,Yiqiao Jin,Munmun De Choudhury*

Main category: cs.CL

TL;DR: 开发了MedAgent框架生成心理健康对话数据集，并提出MultiSenseEval评估体系，发现LLM在患者沟通和诊断中存在明显不足


<details>
  <summary>Details</summary>
Motivation: 现有评估框架忽视患者个性化需求，且缺乏真实多轮对话的评估标准，需系统性评估LLM在心理健康对话中的实际表现

Method: 1. 创建MedAgent框架生成2200+患者-LLM多轮对话数据集(MHSD) 2. 开发MultiSenseEval多维评估体系，包含人类中心化标准

Result: 前沿模型患者沟通得分仅31%，诊断能力不足；对话轮次增加导致性能下降，不同用户画像影响模型表现

Conclusion: 提供了数据生成框架、数据集及评估工具，揭示了LLM在医疗对话中的局限性，为后续改进建立基准

Abstract: Limited access to mental healthcare, extended wait times, and increasing
capabilities of Large Language Models (LLMs) has led individuals to turn to
LLMs for fulfilling their mental health needs. However, examining the
multi-turn mental health conversation capabilities of LLMs remains
under-explored. Existing evaluation frameworks typically focus on diagnostic
accuracy and win-rates and often overlook alignment with patient-specific
goals, values, and personalities required for meaningful conversations. To
address this, we introduce MedAgent, a novel framework for synthetically
generating realistic, multi-turn mental health sensemaking conversations and
use it to create the Mental Health Sensemaking Dialogue (MHSD) dataset,
comprising over 2,200 patient-LLM conversations. Additionally, we present
MultiSenseEval, a holistic framework to evaluate the multi-turn conversation
abilities of LLMs in healthcare settings using human-centric criteria. Our
findings reveal that frontier reasoning models yield below-par performance for
patient-centric communication and struggle at advanced diagnostic capabilities
with average score of 31%. Additionally, we observed variation in model
performance based on patient's persona and performance drop with increasing
turns in the conversation. Our work provides a comprehensive synthetic data
generation framework, a dataset and evaluation framework for assessing LLMs in
multi-turn mental health conversations.

</details>


### [237] [How to Improve the Robustness of Closed-Source Models on NLI](https://arxiv.org/abs/2505.20209)
*Joe Stacey,Lisa Alazraki,Aran Ubhi,Beyza Ermis,Aaron Mueller,Marek Rei*

Main category: cs.CL

TL;DR: 研究通过数据策略提升闭源大语言模型的鲁棒性，发现最佳方法取决于OOD数据复杂度：复杂数据上采样挑战性样本提升1.5%，简单数据替换LLM生成样本提升3.7%。闭源LLMs比编码器模型更鲁棒，应作为基线。


<details>
  <summary>Details</summary>
Motivation: 闭源LLMs微调后易学习数据集特定启发式规则，导致OOD数据鲁棒性下降。现有方法因依赖模型内部访问或训练调整不适用于闭源模型，需探索纯数据层面的解决方案。

Method: 针对不同复杂度OOD数据设计策略：1) 高复杂度场景上采样困难样本 2) 低复杂度场景用LLM生成样本替换部分训练数据。对比闭源自回归LLMs与编码器模型的鲁棒性差异。

Result: 高复杂度OOD数据上采样策略提升鲁棒性1.5%；低复杂度场景生成样本替换策略提升3.7%。闭源LLMs鲁棒性显著优于传统编码器模型（如BERT）。

Conclusion: 数据策略有效性取决于OOD复杂度，闭源自回归LLMs应成为鲁棒性研究的基准模型。该方法为不透明模型提供无需内部访问的鲁棒性提升方案。

Abstract: Closed-source Large Language Models (LLMs) have become increasingly popular,
with impressive performance across a wide range of natural language tasks.
These models can be fine-tuned to further improve performance, but this often
results in the models learning from dataset-specific heuristics that reduce
their robustness on out-of-distribution (OOD) data. Existing methods to improve
robustness either perform poorly, or are non-applicable to closed-source models
because they assume access to model internals, or the ability to change the
model's training procedure. In this work, we investigate strategies to improve
the robustness of closed-source LLMs through data-centric methods that do not
require access to model internals. We find that the optimal strategy depends on
the complexity of the OOD data. For highly complex OOD datasets, upsampling
more challenging training examples can improve robustness by up to 1.5%. For
less complex OOD datasets, replacing a portion of the training set with
LLM-generated examples can improve robustness by 3.7%. More broadly, we find
that large-scale closed-source autoregressive LLMs are substantially more
robust than commonly used encoder models, and are a more appropriate choice of
baseline going forward.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [238] [A Novel Benchmark and Dataset for Efficient 3D Gaussian Splatting with Gaussian Point Cloud Compression](https://arxiv.org/abs/2505.18197)
*Kangli Wang,Shihao Li,Qianxi Yi,Wei Gao*

Main category: cs.GR

TL;DR: 提出基于AI的点云压缩框架GausPcgc，通过适配高斯点云几何分布特性，显著提升3D高斯泼溅模型的存储效率。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS压缩方法忽略高斯空间位置压缩，导致存储开销过大。传统点云压缩方案难以适配高斯点云特有的'全局稀疏-局部密集'分布特性。

Method: 构建GausPcc-1K训练数据集，开发适配高斯几何特征的GausPcgc压缩框架，首次将AI点云压缩技术引入3DGS压缩流程。

Result: 在保持兼容现有压缩方案的基础上实现更高压缩比，性能显著优于传统MPEG G-PCC方案。

Conclusion: 该框架为3DGS压缩开辟新方向，开源代码/数据将推动领域发展。

Abstract: Recently, immersive media and autonomous driving applications have
significantly advanced through 3D Gaussian Splatting (3DGS), which offers
high-fidelity rendering and computational efficiency. Despite these advantages,
3DGS as a display-oriented representation requires substantial storage due to
its numerous Gaussian attributes. Current compression methods have shown
promising results but typically neglect the compression of Gaussian spatial
positions, creating unnecessary bitstream overhead. We conceptualize Gaussian
primitives as point clouds and propose leveraging point cloud compression
techniques for more effective storage. AI-based point cloud compression
demonstrates superior performance and faster inference compared to MPEG
Geometry-based Point Cloud Compression (G-PCC). However, direct application of
existing models to Gaussian compression may yield suboptimal results, as
Gaussian point clouds tend to exhibit globally sparse yet locally dense
geometric distributions that differ from conventional point cloud
characteristics. To address these challenges, we introduce GausPcgc for
Gaussian point cloud geometry compression along with a specialized training
dataset GausPcc-1K. Our work pioneers the integration of AI-based point cloud
compression into Gaussian compression pipelines, achieving superior compression
ratios. The framework complements existing Gaussian compression methods while
delivering significant performance improvements. All code, data, and
pre-trained models will be publicly released to facilitate further research
advances in this field.

</details>


### [239] [Efficient Differentiable Hardware Rasterization for 3D Gaussian Splatting](https://arxiv.org/abs/2505.18764)
*Yitian Yuan,Qianyue He*

Main category: cs.GR

TL;DR: 提出可微分硬件光栅化方法优化3DGS，实现10倍反向光栅加速与3.07倍全流程加速，内存开销仅2.67%


<details>
  <summary>Details</summary>
Motivation: 硬件光栅化在3DGS正向渲染中优势明显，但反向梯度计算受制于图形管线约束，需突破内存和性能限制

Method: 采用可编程混合技术实现逐像素梯度计算，结合四元组+子组混合梯度缩减策略，使用16位渲染目标优化精度效率平衡

Result: float16格式在RTX4080上实现全流程3.07倍加速，反向光栅比原子操作快10倍，内存占用仅为基于图块方法的2.67%

Conclusion: 该方法统一优化了3DGS的运行时性能和内存效率，特别适用于内存受限设备，同时保持硬件光栅化的固有优势

Abstract: Recent works demonstrate the advantages of hardware rasterization for 3D
Gaussian Splatting (3DGS) in forward-pass rendering through fast GPU-optimized
graphics and fixed memory footprint. However, extending these benefits to
backward-pass gradient computation remains challenging due to graphics pipeline
constraints. We present a differentiable hardware rasterizer for 3DGS that
overcomes the memory and performance limitations of tile-based software
rasterization. Our solution employs programmable blending for per-pixel
gradient computation combined with a hybrid gradient reduction strategy
(quad-level + subgroup) in fragment shaders, achieving over 10x faster backward
rasterization versus naive atomic operations and 3x speedup over the canonical
tile-based rasterizer. Systematic evaluation reveals 16-bit render targets
(float16 and unorm16) as the optimal accuracy-efficiency trade-off, achieving
higher gradient accuracy among mixed-precision rendering formats with execution
speeds second only to unorm8, while float32 texture incurs severe forward pass
performance degradation due to suboptimal hardware optimizations. Our method
with float16 formats demonstrates 3.07x acceleration in full pipeline execution
(forward + backward passes) on RTX4080 GPUs with the MipNeRF dataset,
outperforming the baseline tile-based renderer while preserving hardware
rasterization's memory efficiency advantages -- incurring merely 2.67% of the
memory overhead required for splat sorting operations. This work presents a
unified differentiable hardware rasterization method that simultaneously
optimizes runtime and memory usage for 3DGS, making it particularly suitable
for resource-constrained devices with limited memory capacity.

</details>


### [240] [CageNet: A Meta-Framework for Learning on Wild Meshes](https://arxiv.org/abs/2505.18772)
*Michal Edelstein,Hsueh-Ti Derek Liu,Mirela Ben-Chen*

Main category: cs.GR

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Learning on triangle meshes has recently proven to be instrumental to a
myriad of tasks, from shape classification, to segmentation, to deformation and
animation, to mention just a few. While some of these applications are tackled
through neural network architectures which are tailored to the application at
hand, many others use generic frameworks for triangle meshes where the only
customization required is the modification of the input features and the loss
function. Our goal in this paper is to broaden the applicability of these
generic frameworks to "wild", i.e. meshes in-the-wild which often have multiple
components, non-manifold elements, disrupted connectivity, or a combination of
these. We propose a configurable meta-framework based on the concept of caged
geometry: Given a mesh, a cage is a single component manifold triangle mesh
that envelopes it closely. Generalized barycentric coordinates map between
functions on the cage, and functions on the mesh, allowing us to learn and test
on a variety of data, in different applications. We demonstrate this concept by
learning segmentation and skinning weights on difficult data, achieving better
performance to state of the art techniques on wild meshes.

</details>


### [241] [DiffHairCard: Auto Hair Card Extraction with Differentiable Rendering](https://arxiv.org/abs/2505.18805)
*Zhongtian Zheng,Tao Huang,Haozhe Su,Xueqi Ma,Yuefan Shen,Tongtong Wang,Yin Yang,Xifeng Gao,Zherong Pan,Kui Wu*

Main category: cs.GR

TL;DR: 提出自动化管线，通过可微分二维样条编码与两阶段优化策略，实现高效率高质量头发卡片模型生成


<details>
  <summary>Details</summary>
Motivation: 解决传统头发卡片建模流程劳动密集、难以平衡卡片数量与视觉质量的痛点

Method: 1. 将发丝聚类投影至纹理空间 2. 两阶段优化（单卡独立优化+纹理压缩后联合优化）3. 支持发帽/交叉卡处理复杂发型

Result: 成功处理直/波浪/卷/螺旋等各类发型，实现仅需少量卡片（~100张）即可保持发型特征，纹理内存效率提升40%

Conclusion: 创新性的可微分表示与优化框架为实时应用提供了高效头发建模解决方案，支持LOD过渡与复杂发型处理

Abstract: Hair cards remain a widely used representation for hair modeling in real-time
applications, offering a practical trade-off between visual fidelity, memory
usage, and performance. However, generating high-quality hair card models
remains a challenging and labor-intensive task. This work presents an automated
pipeline for converting strand-based hair models into hair card models with a
limited number of cards and textures while preserving the hairstyle appearance.
Our key idea is a novel differentiable representation where each strand is
encoded as a projected 2D spline in the texture space, which enables efficient
optimization with differentiable rendering and structured results respecting
the hair geometry. Based on this representation, we develop a novel algorithm
pipeline, where we first cluster hair strands into initial hair cards and
project the strands into the texture space. We then conduct a two-stage
optimization where our first stage optimizes the texture and geometry of each
hair card separately, and after texture reduction, our second stage conducts
joint optimization of all the cards for fine-tuning. Put together, our method
is evaluated on a wide range of hairstyles, including straight, wavy, curly,
and coily hairs. To better capture the appearance of short or coily hair, we
additionally support hair cap and cross-card. Furthermore, our framework
supports seamless LoD transitions via texture sharing, balancing texture memory
efficiency and visual quality.

</details>


### [242] [SRDiffusion: Accelerate Video Diffusion Inference via Sketching-Rendering Cooperation](https://arxiv.org/abs/2505.19151)
*Shenggan Cheng,Yuanxin Wei,Lansong Diao,Yong Liu,Bujiao Chen,Lianghua Huang,Yu Liu,Wenyuan Yu,Jiangsu Du,Wei Lin,Yang You*

Main category: cs.GR

TL;DR: 提出SRDiffusion框架，通过大模型处理高噪声步骤(语义草绘)与小模型处理低噪声步骤(视觉渲染)的协作机制，在保持视频生成质量的同时实现3倍加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型加速方法常导致质量严重下降，需平衡计算效率与生成质量之间的矛盾。

Method: 采用大小模型协作架构：大模型专注高噪声阶段保证语义准确性，小模型优化低噪声阶段提升细节质量。

Result: 实验显示对Wan模型实现3倍加速(VBench质量无损)，CogVideoX加速2倍，显著优于现有加速方法。

Conclusion: 该框架开辟了与现有加速策略正交的新方向，为大规模视频生成提供了切实可行的技术路径。

Abstract: Leveraging the diffusion transformer (DiT) architecture, models like Sora,
CogVideoX and Wan have achieved remarkable progress in text-to-video,
image-to-video, and video editing tasks. Despite these advances,
diffusion-based video generation remains computationally intensive, especially
for high-resolution, long-duration videos. Prior work accelerates its inference
by skipping computation, usually at the cost of severe quality degradation. In
this paper, we propose SRDiffusion, a novel framework that leverages
collaboration between large and small models to reduce inference cost. The
large model handles high-noise steps to ensure semantic and motion fidelity
(Sketching), while the smaller model refines visual details in low-noise steps
(Rendering). Experimental results demonstrate that our method outperforms
existing approaches, over 3$\times$ speedup for Wan with nearly no quality loss
for VBench, and 2$\times$ speedup for CogVideoX. Our method is introduced as a
new direction orthogonal to existing acceleration strategies, offering a
practical solution for scalable video generation.

</details>


### [243] [A Fluorescent Material Model for Non-Spectral Editing & Rendering](https://arxiv.org/abs/2505.19672)
*Belcour Laurent,Fichet Alban,Barla Pascal*

Main category: cs.GR

TL;DR: 提出基于高斯分解的荧光材料实时编辑渲染方法，支持参数动态调整与简化艺术创作流程


<details>
  <summary>Details</summary>
Motivation: 现有荧光渲染方法需存储多个矩阵且依赖测量数据，限制了艺术创作灵活性与实时编辑能力

Method: 通过分解缩减再辐射矩阵，建立可解析积分的高斯荧光模型，引入UV基底增强准确性

Result: 实现实时荧光参数编辑/表面动态变化，单高斯模型仅需反射色输入即可生成合理荧光材质

Conclusion: 该解析方法突破荧光渲染的技术限制，为数字内容创作提供高效可控的荧光表现工具

Abstract: Fluorescent materials are characterized by a spectral reradiation toward
longer wavelengths. Recent work [Fichet et al. 2024] has shown that the
rendering of fluorescence in a non-spectral engine is possible through the use
of appropriate reduced reradiation matrices. But the approach has limited
expressivity, as it requires the storage of one reduced matrix per fluorescent
material, and only works with measured fluorescent assets.
  In this work, we introduce an analytical approach to the editing and
rendering of fluorescence in a non-spectral engine. It is based on a
decomposition of the reduced reradiation matrix, and an analytically-integrable
Gaussian-based model of the fluorescent component. The model reproduces the
appearance of fluorescent materials accurately, especially with the addition of
a UV basis. Most importantly, it grants variations of fluorescent material
parameters in real-time, either for the editing of fluorescent materials, or
for the dynamic spatial variation of fluorescence properties across object
surfaces. A simplified one-Gaussian fluorescence model even allows for the
artist-friendly creation of plausible fluorescent materials from scratch,
requiring only a few reflectance colors as input.

</details>


### [244] [CAD-Coder: Text-to-CAD Generation with Chain-of-Thought and Geometric Reward](https://arxiv.org/abs/2505.19713)
*Yandong Guan,Xilin Wang,Xingxi Ming,Jing Zhang,Dong Xu,Qian Yu*

Main category: cs.GR

TL;DR: CAD-Coder框架通过生成CadQuery脚本实现文本到CAD的转换，采用监督学习+强化学习两阶段训练，结合CoT规划流程显著提升模型生成能力


<details>
  <summary>Details</summary>
Motivation: 解决传统文本到CAD方法缺乏几何验证能力、建模词汇受限、与现有LLM整合困难的问题

Method: 1. 两阶段训练：监督微调文本-CadQuery配对数据 → GRPO强化学习（几何奖励+格式奖励）
2. 引入CoT规划提升推理能力
3. 构建自动化流程生成110K三元组数据集

Result: 实现自然语言直接生成多样化、有效且复杂的CAD模型（Chamfer Distance指标验证几何精度）

Conclusion: 该框架通过参数化脚本表示、混合训练策略和自动化数据构建，显著推进文本到CAD生成的技术边界

Abstract: In this work, we introduce CAD-Coder, a novel framework that reformulates
text-to-CAD as the generation of CadQuery scripts - a Python-based, parametric
CAD language. This representation enables direct geometric validation, a richer
modeling vocabulary, and seamless integration with existing LLMs. To further
enhance code validity and geometric fidelity, we propose a two-stage learning
pipeline: (1) supervised fine-tuning on paired text-CadQuery data, and (2)
reinforcement learning with Group Reward Policy Optimization (GRPO), guided by
a CAD-specific reward comprising both a geometric reward (Chamfer Distance) and
a format reward. We also introduce a chain-of-thought (CoT) planning process to
improve model reasoning, and construct a large-scale, high-quality dataset of
110K text-CadQuery-3D model triplets and 1.5K CoT samples via an automated
pipeline. Extensive experiments demonstrate that CAD-Coder enables LLMs to
generate diverse, valid, and complex CAD models directly from natural language,
advancing the state of the art of text-to-CAD generation and geometric
reasoning.

</details>


### [245] [MAMM: Motion Control via Metric-Aligning Motion Matching](https://arxiv.org/abs/2505.19976)
*Naoki Agata,Takeo Igarashi*

Main category: cs.GR

TL;DR: 提出基于域内距离对齐的Metric-Aligning Motion Matching方法，无需标注数据即可实现运动序列与草图/标签/音频等多模态控制序列的跨域对齐。


<details>
  <summary>Details</summary>
Motivation: 传统运动时序对齐方法依赖跨域映射，需要大量配对标注数据和耗时训练，限制了实际应用场景。

Method: 通过计算运动域和控制域各自的块间距离矩阵，寻找最优匹配使两个域的距离矩阵对齐，实现跨模态时序对齐。

Result: 成功应用于草图控制、标签驱动、音频同步等多种场景，实验证明方法在运动控制效率和效果上的优势。

Conclusion: 该框架突破了传统方法对标注数据的依赖，为多模态运动控制提供了灵活高效的解决方案。

Abstract: We introduce a novel method for controlling a motion sequence using an
arbitrary temporal control sequence using temporal alignment. Temporal
alignment of motion has gained significant attention owing to its applications
in motion control and retargeting. Traditional methods rely on either learned
or hand-craft cross-domain mappings between frames in the original and control
domains, which often require large, paired, or annotated datasets and
time-consuming training. Our approach, named Metric-Aligning Motion Matching,
achieves alignment by solely considering within-domain distances. It computes
distances among patches in each domain and seeks a matching that optimally
aligns the two within-domain distances. This framework allows for the alignment
of a motion sequence to various types of control sequences, including sketches,
labels, audio, and another motion sequence, all without the need for manually
defined mappings or training with annotated data. We demonstrate the
effectiveness of our approach through applications in efficient motion control,
showcasing its potential in practical scenarios.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [246] [Agentic 3D Scene Generation with Spatially Contextualized VLMs](https://arxiv.org/abs/2505.20129)
*Xinhang Liu,Yu-Wing Tai,Chi-Keung Tang*

Main category: cs.CV

TL;DR: 提出通过空间上下文增强视觉语言模型的三维场景理解与生成能力，构建包含场景画像、语义点云和场景超图的结构化空间记忆系统。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在三维场景推理能力上的局限性，制约了其在具身智能/仿真环境等空间任务中的应用。

Method: 1) 构建持续演进的空间上下文(场景画像+语义点云+场景超图) 
2) 开发包含几何修复/环境验证/人机工程调整的代理生成流程

Result: 实验证明框架具备处理复杂输入的高泛化能力，支持场景编辑/路径规划等下游任务，超越现有方法。

Conclusion: 该框架为计算机图形学/三维视觉/具身系统开发空间智能系统提供了新的技术路径，具有广泛的应用潜力。

Abstract: Despite recent advances in multimodal content generation enabled by
vision-language models (VLMs), their ability to reason about and generate
structured 3D scenes remains largely underexplored. This limitation constrains
their utility in spatially grounded tasks such as embodied AI, immersive
simulations, and interactive 3D applications. We introduce a new paradigm that
enables VLMs to generate, understand, and edit complex 3D environments by
injecting a continually evolving spatial context. Constructed from multimodal
input, this context consists of three components: a scene portrait that
provides a high-level semantic blueprint, a semantically labeled point cloud
capturing object-level geometry, and a scene hypergraph that encodes rich
spatial relationships, including unary, binary, and higher-order constraints.
Together, these components provide the VLM with a structured, geometry-aware
working memory that integrates its inherent multimodal reasoning capabilities
with structured 3D understanding for effective spatial reasoning. Building on
this foundation, we develop an agentic 3D scene generation pipeline in which
the VLM iteratively reads from and updates the spatial context. The pipeline
features high-quality asset generation with geometric restoration, environment
setup with automatic verification, and ergonomic adjustment guided by the scene
hypergraph. Experiments show that our framework can handle diverse and
challenging inputs, achieving a level of generalization not observed in prior
work. Further results demonstrate that injecting spatial context enables VLMs
to perform downstream tasks such as interactive scene editing and path
planning, suggesting strong potential for spatially intelligent systems in
computer graphics, 3D vision, and embodied applications.

</details>


### [247] [In-Context Brush: Zero-shot Customized Subject Insertion with Context-Aware Latent Space Manipulation](https://arxiv.org/abs/2505.20271)
*Yu Xu,Fan Tang,You Wu,Lin Gao,Oliver Deussen,Hongbin Yan,Jintao Li,Juan Cao,Tong-Yee Lee*

Main category: cs.CV

TL;DR: 提出零样本框架'In-Context Brush'，通过双级潜在空间操作实现无需训练的自定义主体图像插入


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在定制化主体插入时存在保真度低与文本提示对齐困难的问题，需要无需微调模型的解决方案

Method: 基于MMDiT修复网络，采用注意力头内的潜在特征偏移(intra-head)和跨注意力头的权重调整(inter-head)实现双重潜在空间控制

Result: 在身份保持、文本对齐和图像质量方面超越现有方法，且无需额外训练或数据收集

Conclusion: 通过上下文学习框架与潜在空间操作，有效解决了定制化主体插入的核心挑战，实现了高效零样本生成

Abstract: Recent advances in diffusion models have enhanced multimodal-guided visual
generation, enabling customized subject insertion that seamlessly "brushes"
user-specified objects into a given image guided by textual prompts. However,
existing methods often struggle to insert customized subjects with high
fidelity and align results with the user's intent through textual prompts. In
this work, we propose "In-Context Brush", a zero-shot framework for customized
subject insertion by reformulating the task within the paradigm of in-context
learning. Without loss of generality, we formulate the object image and the
textual prompts as cross-modal demonstrations, and the target image with the
masked region as the query. The goal is to inpaint the target image with the
subject aligning textual prompts without model tuning. Building upon a
pretrained MMDiT-based inpainting network, we perform test-time enhancement via
dual-level latent space manipulation: intra-head "latent feature shifting"
within each attention head that dynamically shifts attention outputs to reflect
the desired subject semantics and inter-head "attention reweighting" across
different heads that amplifies prompt controllability through differential
attention prioritization. Extensive experiments and applications demonstrate
that our approach achieves superior identity preservation, text alignment, and
image quality compared to existing state-of-the-art methods, without requiring
dedicated training or additional data collection.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [248] [Emotion Knowledge Enhancement for Vision Large Language Models: A Self-Verification Approach for High-Quality Emotion Instruction Data Generation](https://arxiv.org/abs/2505.18168)
*Feifan Wang,Tengfei Song,Minggui He,Chang Su,Zhanglin Wu,Hao Yang,Wenming Zheng,Osamu Yoshie*

Main category: cs.LG

TL;DR: 提出SEKE方法，通过自验证机制与情感知识增强生成高质量多粒度情绪标注数据，构建FEID数据集和FEAB基准，显著提升VLLM在面部情绪分析任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉大语言模型在面部情绪感知中存在高质量多粒度标注数据匮乏的问题，专家标注成本过高制约模型性能提升。

Method: 融合人类先验知识与VLLM推理能力，利用情绪描述三粒度（离散表情/效价-唤醒/动作单元）的固有相关性生成标注，采用不确定性感知的蒙特卡洛采样自验证策略优化预测准确性。

Result: 构建包含3类描述的面部情绪指令数据集FEID，建立评估基准FEAB，在三个下游任务中性能超越现有最优方法。

Conclusion: SEKE方法以低成本生成可靠标注数据，突破VLLM情绪分析瓶颈，为相关研究提供高质量训练数据与标准化评估体系。

Abstract: Facial emotion perception in the vision large language model (VLLM) is
crucial for achieving natural human-machine interaction. However, creating
high-quality annotations for both coarse- and fine-grained facial emotion
analysis demands costly expertise. The lack of such high-quality instruction
data limits the performance of VLLMs in facial emotion perception. To address
this, we propose a self-verification approach with emotion knowledge
enhancement (SEKE), which generates high-quality instruction data for
multi-grained emotion analysis cost-effectively using closed-source VLLM. This
approach integrates prior human knowledge to VLLM inference, guided by the
inherent correlations between three grained levels of emotion descriptions,
i.e., discrete expression, valence-arousal, and action unit, to reliably
generate comprehensive annotations. A self-verification strategy with
Uncertainty-Aware Monte Carlo sampling (SV-UAMC) is further embedded to
efficiently extract more accurate VLLM predictions, further improving
annotation reliability. Consequently, we construct a facial emotion instruction
dataset (FEID) containing three comprehensive descriptions, which provides
coarse- and fine-grained emotional information for effective model training.
Additionally, we introduce a facial emotion analysis benchmark (FEAB) to
measure the VLLM's corresponding ability. Our method significantly outperforms
state-of-the-art methods on three downstream facial emotion analysis tasks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [249] [MaskedManipulator: Versatile Whole-Body Control for Loco-Manipulation](https://arxiv.org/abs/2505.19086)
*Chen Tessler,Yifeng Jiang,Erwin Coumans,Zhengyi Luo,Gal Chechik,Xue Bin Peng*

Main category: cs.RO

TL;DR: 提出MaskedManipulator框架，通过两阶段学习实现物理人形机器人基于高层目标（物体姿态/关键姿势）的全身动作自主生成


<details>
  <summary>Details</summary>
Motivation: 现有物理模拟控制方法局限于具体任务且缺乏灵活的高层目标控制能力，难以实现类似人类的多层次操作序列

Method: 1. 从动捕数据训练跟踪控制器重建交互动作 → 2. 蒸馏生成统一策略，支持用户通过物体位姿/身体姿态等语义级参数控制

Result: 系统可自动补全中间动作，实现自然的长时程操作序列（如抓取-移动-放置），支持物理模拟人形完成复杂人-物联合控制任务

Conclusion: 该技术突破传统低层控制范式，为动画系统提供程序化生成工具，推动虚拟角色向更交互式、拟人化方向发展

Abstract: Humans interact with their world while leveraging precise full-body control
to achieve versatile goals. This versatility allows them to solve long-horizon,
underspecified problems, such as placing a cup in a sink, by seamlessly
sequencing actions like approaching the cup, grasping, transporting it, and
finally placing it in the sink. Such goal-driven control can enable new
procedural tools for animation systems, enabling users to define partial
objectives while the system naturally ``fills in'' the intermediate motions.
However, while current methods for whole-body dexterous manipulation in
physics-based animation achieve success in specific interaction tasks, they
typically employ control paradigms (e.g., detailed kinematic motion tracking,
continuous object trajectory following, or direct VR teleoperation) that offer
limited versatility for high-level goal specification across the entire coupled
human-object system. To bridge this gap, we present MaskedManipulator, a
unified and generative policy developed through a two-stage learning approach.
First, our system trains a tracking controller to physically reconstruct
complex human-object interactions from large-scale human mocap datasets. This
tracking controller is then distilled into MaskedManipulator, which provides
users with intuitive control over both the character's body and the manipulated
object. As a result, MaskedManipulator enables users to specify complex
loco-manipulation tasks through intuitive high-level objectives (e.g., target
object poses, key character stances), and MaskedManipulator then synthesizes
the necessary full-body actions for a physically simulated humanoid to achieve
these goals, paving the way for more interactive and life-like virtual
characters.

</details>


### [250] [From Single Images to Motion Policies via Video-Generation Environment Representations](https://arxiv.org/abs/2505.19306)
*Weiming Zhi,Ziyong Ma,Tianyi Zhang,Matthew Johnson-Roberson*

Main category: cs.RO

TL;DR: 提出VGER框架，通过视频生成模型和3D基础模型从单张RGB图像构建环境表示，并生成符合场景几何的平滑运动策略


<details>
  <summary>Details</summary>
Motivation: 解决现有单目深度估计模型（如DepthAnything）在运动生成任务中存在的视锥形误差问题

Method: 1. 利用视频生成模型生成多视角视频
2. 通过预训练3D基础模型生成密集点云
3. 采用多尺度噪声训练隐式环境表示
4. 建立几何符合的运动生成模型

Result: 在多样化的室内外环境评估中，成功生成考虑场景几何的平滑运动轨迹

Conclusion: VGER框架仅需单张RGB图像即可准确捕捉场景几何结构，为自主机器人运动生成提供有效解决方案

Abstract: Autonomous robots typically need to construct representations of their
surroundings and adapt their motions to the geometry of their environment.
Here, we tackle the problem of constructing a policy model for collision-free
motion generation, consistent with the environment, from a single input RGB
image. Extracting 3D structures from a single image often involves monocular
depth estimation. Developments in depth estimation have given rise to large
pre-trained models such as DepthAnything. However, using outputs of these
models for downstream motion generation is challenging due to frustum-shaped
errors that arise. Instead, we propose a framework known as Video-Generation
Environment Representation (VGER), which leverages the advances of large-scale
video generation models to generate a moving camera video conditioned on the
input image. Frames of this video, which form a multiview dataset, are then
input into a pre-trained 3D foundation model to produce a dense point cloud. We
then introduce a multi-scale noise approach to train an implicit representation
of the environment structure and build a motion generation model that complies
with the geometry of the representation. We extensively evaluate VGER over a
diverse set of indoor and outdoor environments. We demonstrate its ability to
produce smooth motions that account for the captured geometry of a scene, all
from a single RGB input image.

</details>
