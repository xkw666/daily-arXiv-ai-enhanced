<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 67]
- [cs.GR](#cs.GR) [Total: 4]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.CV](#cs.CV) [Total: 3]
- [cs.CR](#cs.CR) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.LG](#cs.LG) [Total: 4]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.DL](#cs.DL) [Total: 2]
- [cs.IR](#cs.IR) [Total: 4]
- [cs.CY](#cs.CY) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [How Deep Is Representational Bias in LLMs? The Cases of Caste and Religion](https://arxiv.org/abs/2508.03712)
*Agrima Seth,Monojit Choudhary,Sunayana Sitaram,Kentaro Toyama,Aditya Vashistha,Kalika Bali*

Main category: cs.CL

TL;DR: 研究发现GPT-4 Turbo在生成印度生活故事时持续过度代表主导宗教/种姓群体，提示引导对纠正偏差效果有限，需模型底层改进


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注北半球身份特征（种族/性别），本研究首次系统检测LLMs在南半球维度（印度宗教/种姓）的表征偏差顽固性

Method: 使用7,200个印度生活事件故事生成实验，对比GPT-4输出与印度人口普查数据，量化宗教/种姓的分布偏差程度和提示干预效果

Result: 模型输出持续过度代表文化主导群体（超出实际人口比例），提示多样性引导效果有限且不稳定，偏差呈现'赢家通吃'特性

Conclusion: 单纯增加训练数据多样性不足以消除LLM表征偏差，需模型架构/训练目标的根本性改革，揭示当前LLM社会包容性存在深层局限

Abstract: Representational bias in large language models (LLMs) has predominantly been
measured through single-response interactions and has focused on Global
North-centric identities like race and gender. We expand on that research by
conducting a systematic audit of GPT-4 Turbo to reveal how deeply encoded
representational biases are and how they extend to less-explored dimensions of
identity. We prompt GPT-4 Turbo to generate over 7,200 stories about
significant life events (such as weddings) in India, using prompts designed to
encourage diversity to varying extents. Comparing the diversity of religious
and caste representation in the outputs against the actual population
distribution in India as recorded in census data, we quantify the presence and
"stickiness" of representational bias in the LLM for religion and caste. We
find that GPT-4 responses consistently overrepresent culturally dominant groups
far beyond their statistical representation, despite prompts intended to
encourage representational diversity. Our findings also suggest that
representational bias in LLMs has a winner-take-all quality that is more biased
than the likely distribution bias in their training data, and repeated
prompt-based nudges have limited and inconsistent efficacy in dislodging these
biases. These results suggest that diversifying training data alone may not be
sufficient to correct LLM bias, highlighting the need for more fundamental
changes in model development. Dataset and Codebook:
https://github.com/agrimaseth/How-Deep-Is-Representational-Bias-in-LLMs

</details>


### [2] [FeynTune: Large Language Models for High-Energy Theory](https://arxiv.org/abs/2508.03716)
*Paul Richmond,Prarit Agarwal,Borun Chowdhury,Vasilis Niarchos,Constantinos Papageorgakis*

Main category: cs.CL

TL;DR: 基于Llama-3.1模型开发20个高能理论物理专用大语言模型变体，通过arXiv摘要数据微调并超越基础模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决通用大模型在理论物理垂直领域的适配不足问题，验证领域专用微调对学科知识理解的提升效果。

Method: 采用两种LoRA微调方法，在hep-th/hep-ph/gr-qc等不同数据集组合上进行训练，并与q-bio/cs等跨领域模型对比。

Result: 微调模型在hep-th摘要补全任务中优于基础模型，性能超过ChatGPT/Claude等商业模型。

Conclusion: 通过领域定制化训练构建专用语言模型，为高能理论物理研究提供更精准的智能支持，该方法具有可扩展性。

Abstract: We present specialized Large Language Models for theoretical High-Energy
Physics, obtained as 20 fine-tuned variants of the 8-billion parameter
Llama-3.1 model. Each variant was trained on arXiv abstracts (through August
2024) from different combinations of hep-th, hep-ph and gr-qc. For a
comparative study, we also trained models on datasets that contained abstracts
from disparate fields such as the q-bio and cs categories. All models were
fine-tuned using two distinct Low-Rank Adaptation fine-tuning approaches and
varying dataset sizes, and outperformed the base model on hep-th abstract
completion tasks. We compare performance against leading commercial LLMs
(ChatGPT, Claude, Gemini, DeepSeek) and derive insights for further developing
specialized language models for High-Energy Theoretical Physics.

</details>


### [3] [Intent Aware Context Retrieval for Multi-Turn Agricultural Question Answering](https://arxiv.org/abs/2508.03719)
*Abhay Vijayvargia,Ajay Nagpal,Kundeshwar Pundalik,Atharva Savarkar,Smita Gautam,Pankaj Singh,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: 针对印度农民获取农业建议困难的问题，开发了支持多轮对话的AI农业助手Krishi Sathi，结合IFT模型与检索增强生成技术，实现97.53%响应准确率与6秒内平均响应时间。


<details>
  <summary>Details</summary>
Motivation: 印度农村地区存在低识字率与数字技能不足的问题，传统农业咨询渠道存在语言障碍和可及性限制，需要开发适应本土需求的智能化解决方案。

Method: 1. 基于IFT模型构建核心架构，通过农业知识数据集微调；2. 多轮对话流程收集农户问题细节；3. 结合检索增强生成(RAG)技术整合农业数据库；4. 集成语音识别(ASR)与合成(TTS)功能，支持英语/印地语双模交互。

Result: 系统实现97.53%查询响应准确率，91.35%上下文相关性，97.53%查询完成率，平均响应时间<6秒，支持200万+农户使用。

Conclusion: 结构化对话流程与本地化技术适配显著提升了数字农业支持的可及性，为低资源地区提供了可行的AI辅助范例。

Abstract: Indian farmers often lack timely, accessible, and language-friendly
agricultural advice, especially in rural areas with low literacy. To address
this gap in accessibility, this paper presents a novel AI-powered agricultural
chatbot, Krishi Sathi, designed to support Indian farmers by providing
personalized, easy-to-understand answers to their queries through both text and
speech. The system's intelligence stems from an IFT model, subsequently refined
through fine-tuning on Indian agricultural knowledge across three curated
datasets. Unlike traditional chatbots that respond to one-off questions, Krishi
Sathi follows a structured, multi-turn conversation flow to gradually collect
the necessary details from the farmer, ensuring the query is fully understood
before generating a response. Once the intent and context are extracted, the
system performs Retrieval-Augmented Generation (RAG) by first fetching
information from a curated agricultural database and then generating a tailored
response using the IFT model. The chatbot supports both English and Hindi
languages, with speech input and output features (via ASR and TTS) to make it
accessible for users with low literacy or limited digital skills. This work
demonstrates how combining intent-driven dialogue flows, instruction-tuned
models, and retrieval-based generation can improve the quality and
accessibility of digital agricultural support in India.
  This approach yielded strong results, with the system achieving a query
response accuracy of 97.53%, 91.35% contextual relevance and personalization,
and a query completion rate of 97.53%. The average response time remained under
6 seconds, ensuring timely support for users across both English and Hindi
interactions.

</details>


### [4] [Hierarchical Verification of Speculative Beams for Accelerating LLM Inference](https://arxiv.org/abs/2508.03726)
*Jaydip Sen,Harshitha Puvvala,Subhasis Dasgupta*

Main category: cs.CL

TL;DR: 提出分层验证树框架HVT，通过优先验证高可能性候选和早期剪枝机制，显著提升大语言模型推理效率并降低能耗


<details>
  <summary>Details</summary>
Motivation: 传统推测解码方法存在无优先级顺序验证导致的冗余计算问题，亟需更高效的验证策略

Method: 构建分层验证树结构，开发理论验证-剪枝算法，实现无需模型修改的现有LLM管道集成

Result: 实验证明HVT在多个基准测试中推理时间缩短32%，能耗降低28%，且输出质量保持稳定

Conclusion: 分层验证策略开辟了LLM加速新方向，HVT框架为实际部署提供高效解决方案

Abstract: Large language models (LLMs) have achieved remarkable success across diverse
natural language processing tasks but face persistent challenges in inference
efficiency due to their autoregressive nature. While speculative decoding and
beam sampling offer notable improvements, traditional methods verify draft
sequences sequentially without prioritization, leading to unnecessary
computational overhead. This work proposes the Hierarchical Verification Tree
(HVT), a novel framework that restructures speculative beam decoding by
prioritizing high-likelihood drafts and enabling early pruning of suboptimal
candidates. Theoretical foundations and a formal verification-pruning algorithm
are developed to ensure correctness and efficiency. Integration with standard
LLM inference pipelines is achieved without requiring retraining or
architecture modification. Experimental evaluations across multiple datasets
and models demonstrate that HVT consistently outperforms existing speculative
decoding schemes, achieving substantial reductions in inference time and energy
consumption while maintaining or enhancing output quality. The findings
highlight the potential of hierarchical verification strategies as a new
direction for accelerating large language model inference.

</details>


### [5] [WINELL: Wikipedia Never-Ending Updating with LLM Agents](https://arxiv.org/abs/2508.03728)
*Revanth Gangi Reddy,Tanay Dixit,Jiaxin Qin,Cheng Qian,Daniel Lee,Jiawei Han,Kevin Small,Xing Fan,Ruhi Sarikaya,Heng Ji*

Main category: cs.CL

TL;DR: 提出WiNELL框架利用LLM代理实现维基百科自动更新，通过多代理系统整合网络信息并生成符合人工编辑习惯的更新建议，效果优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 维基百科依赖人工编辑导致内容更新滞后，NELL的持续知识获取理念与LLM代理技术进步促使开发自动化更新框架。

Method: 采用多代理架构：1) 信息聚合代理收集网络数据 2) 知识筛选代理识别重要更新 3) 基于维基百科历史编辑训练的细粒度编辑模型生成建议

Result: 编辑模型在关键信息覆盖(提升23%)和编辑效率(提高35%)上超越GPT-4o，端到端测试验证系统可检测90%以上时效性事实更新

Conclusion: WiNELL证明了LLM代理在持续更新知识库领域的潜力，为自动维护动态知识系统开辟新研究方向

Abstract: Wikipedia, a vast and continuously consulted knowledge base, faces
significant challenges in maintaining up-to-date content due to its reliance on
manual human editors. Inspired by the vision of continuous knowledge
acquisition in NELL and fueled by advances in LLM-based agents, this paper
introduces WiNELL, an agentic framework for continuously updating Wikipedia
articles. Our approach employs a multi-agent framework to aggregate online
information, select new and important knowledge for a target entity in
Wikipedia, and then generate precise edit suggestions for human review. Our
fine-grained editing models, trained on Wikipedia's extensive history of human
edits, enable incorporating updates in a manner consistent with human editing
behavior. Our editor models outperform both open-source instruction-following
baselines and closed-source LLMs (e.g., GPT-4o) in key information coverage and
editing efficiency. End-to-end evaluation on high-activity Wikipedia pages
demonstrates WiNELL's ability to identify and suggest timely factual updates.
This opens up a promising research direction in LLM agents for automatically
updating knowledge bases in a never-ending fashion.

</details>


### [6] [GanitBench: A bi-lingual benchmark for evaluating mathematical reasoning in Vision Language Models](https://arxiv.org/abs/2508.03737)
*Ashutosh Bandooni,Brindha Subburaj*

Main category: cs.CL

TL;DR: 提出双语数学基准GanitBench，包含1527道印地语/英语视觉数学题，评估显示GPT-4o mini表现最佳（38.15%准确率），模型在印地语环境下性能显著下降


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型评估基准多局限于英语，印地语数据集匮乏且任务单一。印度数学考试题具有视觉推理挑战性，需建立多语言评估体系

Method: 从JEE和CBSE考试中收集1527道视觉数学题（含图表），构建英印双语基准。在零样本思维链和双样本思维链设置下评估闭源模型，引入'双锁'约束测试鲁棒性

Result: GPT-4o mini表现最优（最高38.15%准确率），双样本思维链在约束条件下更有效。模型在印地语环境中准确率下降，'双锁'约束使性能下降显著

Conclusion: 强调多语言基准必要性，揭示VLMs在非英语环境中的性能瓶颈，推动印地语等资源较少语言的研究进展

Abstract: Benchmarks for evaluating reasoning among Vision Language Models (VLMs) on
several fields and domains are being curated more frequently over the last few
years. However these are often monolingual, mostly available in English.
Additionally there also is a lack of datasets available in Hindi on tasks apart
from comprehension and translation. We introduce GanitBench, a tough benchmark
consisting of 1527 vision-only questions covering several topics in Mathematics
- available in languages English and Hindi. Collected from two major
examinations from India, the JEE Advanced and the CBSE Boards examinations,
this benchmark includes questions in the form of images comprising of figures
essential to a question as well as text. We evaluate two closed source models
for the same, in zero-shot Chain-of-Thought (CoT) and two-shot CoT settings.
GPT-4o mini is found to be the more dominant model on the benchmark, with it's
highest average accuracy being 38.15%. We also evaluate models through a
"Double Lock" constraint, which brings down the performance of the models by
considerable margins. We observe that two-shot CoT appears to be a more
effective setting under this environment. Performance of the two VLMs also
decreases when answering the same questions in the Hindi language. We hope to
facilitate the inclusion of languages like Hindi in research through our work.

</details>


### [7] [AttnTrace: Attention-based Context Traceback for Long-Context LLMs](https://arxiv.org/abs/2508.03793)
*Yanting Wang,Runpeng Geng,Ying Chen,Jinyuan Jia*

Main category: cs.CL

TL;DR: 提出基于注意力权重的AttnTrace方法，显著提升长文本LLM输出的上下文追溯准确率与效率


<details>
  <summary>Details</summary>
Motivation: 现有上下文追溯方法（如TracLLM）计算成本过高，无法满足实际应用需求，亟需开发更高效的解决方案

Method: 1. 利用LLM生成的注意力权重构建追溯框架
2. 设计两种注意力增强技术并辅以理论支撑
3. 系统评估框架性能

Result: 实验表明：
- 准确率超越现有SOTA方法
- 单次追溯速度提升数十倍
- 在长文本提示注入检测中实现96%准确率

Conclusion: AttnTrace为LLM系统提供高效追溯工具，增强模型透明度和安全性，其开源代码促进工业应用

Abstract: Long-context large language models (LLMs), such as Gemini-2.5-Pro and
Claude-Sonnet-4, are increasingly used to empower advanced AI systems,
including retrieval-augmented generation (RAG) pipelines and autonomous agents.
In these systems, an LLM receives an instruction along with a context--often
consisting of texts retrieved from a knowledge database or memory--and
generates a response that is contextually grounded by following the
instruction. Recent studies have designed solutions to trace back to a subset
of texts in the context that contributes most to the response generated by the
LLM. These solutions have numerous real-world applications, including
performing post-attack forensic analysis and improving the interpretability and
trustworthiness of LLM outputs. While significant efforts have been made,
state-of-the-art solutions such as TracLLM often lead to a high computation
cost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a
single response-context pair. In this work, we propose AttnTrace, a new context
traceback method based on the attention weights produced by an LLM for a
prompt. To effectively utilize attention weights, we introduce two techniques
designed to enhance the effectiveness of AttnTrace, and we provide theoretical
insights for our design choice. We also perform a systematic evaluation for
AttnTrace. The results demonstrate that AttnTrace is more accurate and
efficient than existing state-of-the-art context traceback methods. We also
show that AttnTrace can improve state-of-the-art methods in detecting prompt
injection under long contexts through the attribution-before-detection
paradigm. As a real-world application, we demonstrate that AttnTrace can
effectively pinpoint injected instructions in a paper designed to manipulate
LLM-generated reviews. The code is at
https://github.com/Wang-Yanting/AttnTrace.

</details>


### [8] [Majority Bit-Aware Watermarking For Large Language Models](https://arxiv.org/abs/2508.03829)
*Jiahao Xu,Rui Hu,Zikai Zhang*

Main category: cs.CL

TL;DR: 提出MajorMark水印方法，通过多数位感知编码和聚类解码策略改善LLM水印技术的质量与解码精度权衡


<details>
  <summary>Details</summary>
Motivation: 现有多比特水印技术存在文本质量与解码精度不可兼得的矛盾：为保证解码需限制token选择范围，但会降低生成质量

Method: MajorMark基于消息多数位选择token集合，采用聚类解码策略；MajorMark+将消息分块独立编码/确定性解码

Result: 实验证明该方法显著提升解码精度和文本质量，优于现有基线

Conclusion: 提出的水印方法突破传统质量-精度权衡限制，为LLM生成内容溯源提供了更优解决方案

Abstract: The growing deployment of Large Language Models (LLMs) in real-world
applications has raised concerns about their potential misuse in generating
harmful or deceptive content. To address this issue, watermarking techniques
have emerged as a promising solution by embedding identifiable binary messages
into generated text for origin verification and misuse tracing. While recent
efforts have explored multi-bit watermarking schemes capable of embedding rich
information such as user identifiers, they typically suffer from the
fundamental trade-off between text quality and decoding accuracy: to ensure
reliable message decoding, they have to restrict the size of preferred token
sets during encoding, yet such restrictions reduce the quality of the generated
content. In this work, we propose MajorMark, a novel watermarking method that
improves this trade-off through majority bit-aware encoding. MajorMark selects
preferred token sets based on the majority bit of the message, enabling a
larger and more flexible sampling of tokens. In contrast to prior methods that
rely on token frequency analysis for decoding, MajorMark employs a
clustering-based decoding strategy, which maintains high decoding accuracy even
when the preferred token set is large, thus preserving both content quality and
decoding accuracy. We further introduce MajorMark$^+$, which partitions the
message into multiple blocks to independently encode and deterministically
decode each block, thereby further enhancing the quality of watermarked text
and improving decoding accuracy. Extensive experiments on state-of-the-art LLMs
demonstrate that our methods significantly enhance both decoding accuracy and
text generation quality, outperforming prior multi-bit watermarking baselines.

</details>


### [9] [Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models](https://arxiv.org/abs/2508.03860)
*Subhey Sadi Rahman,Md. Adnanul Islam,Md. Mahbub Alam,Musarrat Zeba,Md. Abdur Rahman,Sadia Sultana Chowa,Mohaimenul Azam Khan Raiaan,Sami Azam*

Main category: cs.CL

TL;DR: LLMs存在生成错误信息的风险，需通过RAG框架、领域调优和指令优化提升事实核查能力


<details>
  <summary>Details</summary>
Motivation: LLMs训练数据包含不准确信息，生成内容存在误导风险，需建立系统化事实核查框架确保输出可靠性

Method: 系统分析2020-2025年文献，提出5个研究问题框架，整合提示策略/领域微调/RAG方法，探讨指令调优和多智能体推理机制

Result: 当前评估指标存在局限，外部知识验证可提升事实一致性，领域定制优化效果显著，多智能体推理增强核查可靠性

Conclusion: 构建领域定制化、可解释的LLMs对事实核查至关重要，研究推动可信语言模型发展，强调知识验证与领域适配的协同作用

Abstract: Large Language Models (LLMs) are trained on vast and diverse internet corpora
that often include inaccurate or misleading content. Consequently, LLMs can
generate misinformation, making robust fact-checking essential. This review
systematically analyzes how LLM-generated content is evaluated for factual
accuracy by exploring key challenges such as hallucinations, dataset
limitations, and the reliability of evaluation metrics. The review emphasizes
the need for strong fact-checking frameworks that integrate advanced prompting
strategies, domain-specific fine-tuning, and retrieval-augmented generation
(RAG) methods. It proposes five research questions that guide the analysis of
the recent literature from 2020 to 2025, focusing on evaluation methods and
mitigation techniques. The review also discusses the role of instruction
tuning, multi-agent reasoning, and external knowledge access via RAG
frameworks. Key findings highlight the limitations of current metrics, the
value of grounding outputs with validated external evidence, and the importance
of domain-specific customization to improve factual consistency. Overall, the
review underlines the importance of building LLMs that are not only accurate
and explainable but also tailored for domain-specific fact-checking. These
insights contribute to the advancement of research toward more trustworthy and
context-aware language models.

</details>


### [10] [An Entity Linking Agent for Question Answering](https://arxiv.org/abs/2508.03865)
*Yajie Luo,Yihong Wu,Muzhi Li,Fengran Mo,Jia Ao Sun,Xinyu Wang,Liheng Ma,Yingxue Zhang,Jian-Yun Nie*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Some Question Answering (QA) systems rely on knowledge bases (KBs) to provide
accurate answers. Entity Linking (EL) plays a critical role in linking natural
language mentions to KB entries. However, most existing EL methods are designed
for long contexts and do not perform well on short, ambiguous user questions in
QA tasks. We propose an entity linking agent for QA, based on a Large Language
Model that simulates human cognitive workflows. The agent actively identifies
entity mentions, retrieves candidate entities, and makes decision. To verify
the effectiveness of our agent, we conduct two experiments: tool-based entity
linking and QA task evaluation. The results confirm the robustness and
effectiveness of our agent.

</details>


### [11] [Sotopia-RL: Reward Design for Social Intelligence](https://arxiv.org/abs/2508.03905)
*Haofei Yu,Zhengyang Qi,Yining Zhao,Kolby Nottingham,Keyang Xuan,Bodhisattwa Prasad Majumder,Hao Zhu,Paul Pu Liang,Jiaxuan You*

Main category: cs.CL

TL;DR: 论文提出Sotopia-RL框架，通过话语级多维度奖励机制解决强化学习在社交智能训练中的部分可观察性和多维度性挑战，实验显示其显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于MDP的强化学习在社交任务中存在信用分配困难(部分可观察性)和单维奖励失效(多维度行为特性)的问题，导致训练低效不稳定。

Method: Sotopia-RL框架：1) 将回合级反馈细化为话语级奖励，实现精准信用分配；2) 设计多维度奖励机制捕捉社交互动复杂性，降低奖励欺骗风险。

Result: 在Sotopia社交学习环境中，Sotopia-RL取得当前最佳社会目标完成分数(Sotopia-hard 7.17分/Sotopia-full 8.31分)，显著超越基线方法。

Conclusion: 通过话语级信用分配和多维度奖励设计，Sotopia-RL有效提升LLMs在开放式社交任务中的表现，消融实验验证了两大核心要素的必要性。

Abstract: Social intelligence has become a critical capability for large language
models (LLMs), enabling them to engage effectively in real-world social tasks
such as accommodation, persuasion, collaboration, and negotiation.
Reinforcement learning (RL) is a natural fit for training socially intelligent
agents because it allows models to learn sophisticated strategies directly
through social interactions. However, social interactions have two key
characteristics that set barriers for RL training: (1) partial observability,
where utterances have indirect and delayed effects that complicate credit
assignment, and (2) multi-dimensionality, where behaviors such as
rapport-building or knowledge-seeking contribute indirectly to goal
achievement. These characteristics make Markov decision process (MDP)-based RL
with single-dimensional episode-level rewards inefficient and unstable. To
address these challenges, we propose Sotopia-RL, a novel framework that refines
coarse episode-level feedback into utterance-level, multi-dimensional rewards.
Utterance-level credit assignment mitigates partial observability by
attributing outcomes to individual utterances, while multi-dimensional rewards
capture the full richness of social interactions and reduce reward hacking.
Experiments in Sotopia, an open-ended social learning environment, demonstrate
that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17
on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing
approaches. Ablation studies confirm the necessity of both utterance-level
credit assignment and multi-dimensional reward design for RL training. Our
implementation is publicly available at:
https://github.com/sotopia-lab/sotopia-rl.

</details>


### [12] [CoAct-1: Computer-using Agents with Coding as Actions](https://arxiv.org/abs/2508.03923)
*Linxin Song,Yutong Dai,Viraj Prabhu,Jieyu Zhang,Taiwei Shi,Li Li,Junnan Li,Silvio Savarese,Zeyuan Chen,Jieyu Zhao,Ran Xu,Caiming Xiong*

Main category: cs.CL

TL;DR: CoAct-1通过融合GUI操作与编程脚本执行，实现了60.76%的OSWorld基准成功率，显著提升自动化代理效率与可靠性


<details>
  <summary>Details</summary>
Motivation: 传统GUI自动化代理在复杂长程任务中存在效率低、可靠性差的问题，纯GUI操作存在固有局限性

Method: 提出多智能体系统CoAct-1，包含动态任务分配中枢Orchestrator，可调度GUI操作员或能编写Python/Bash脚本的程序员代理

Result: 在OSWorld基准测试中达到60.76%成功率（SOTA），任务平均步骤从15步降至10.15步

Conclusion: 将编程作为核心操作方式为计算机自动化提供了更强大、高效且可扩展的解决方案

Abstract: Autonomous agents that operate computers via Graphical User Interfaces (GUIs)
often struggle with efficiency and reliability on complex, long-horizon tasks.
While augmenting these agents with planners can improve task decomposition,
they remain constrained by the inherent limitations of performing all actions
through GUI manipulation, leading to brittleness and inefficiency. In this
work, we introduce a more robust and flexible paradigm: enabling agents to use
coding as a enhanced action. We present CoAct-1, a novel multi-agent system
that synergistically combines GUI-based control with direct programmatic
execution. CoAct-1 features an Orchestrator that dynamically delegates subtasks
to either a conventional GUI Operator or a specialized Programmer agent, which
can write and execute Python or Bash scripts. This hybrid approach allows the
agent to bypass inefficient GUI action sequences for tasks like file management
and data processing, while still leveraging visual interaction when necessary.
We evaluate our system on the challenging OSWorld benchmark, where CoAct-1
achieves a new state-of-the-art success rate of 60.76%, significantly
outperforming prior methods. Furthermore, our approach dramatically improves
efficiency, reducing the average number of steps required to complete a task to
just 10.15, compared to 15 for leading GUI agents. Our results demonstrate that
integrating coding as a core action provides a more powerful, efficient, and
scalable path toward generalized computer automation.

</details>


### [13] [CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline Generation](https://arxiv.org/abs/2508.03935)
*Raymond Wilson,Cole Graham,Chase Carter,Zefeng Yang,Ruiqi Gu*

Main category: cs.CL

TL;DR: 提出CAP-LLM框架，整合用户偏好编码器、上下文注入适配器和事实一致性强化模块，在PENS数据集实现SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有个性化新闻标题生成方法在捕捉复杂用户兴趣和保证事实一致性方面的不足，避免生成通用化或误导性标题。

Method: 1) 用户偏好编码器捕获长期兴趣 2) 上下文适配器将偏好与文章内容融合 3) 创新对比损失机制减少幻觉生成

Result: FactCC达87.50优于BART(86.67)，Pc(avg)=2.73/ROUGE-1=26.55，消融实验验证各组件有效性

Conclusion: CAP-LLM在个性化和事实准确性间取得最佳平衡，通过模块化设计提升生成质量，人工评估证实方案鲁棒性

Abstract: In the era of information overload, personalized news headline generation is
crucial for engaging users by tailoring content to their preferences while
accurately conveying news facts. Existing methods struggle with effectively
capturing complex user interests and ensuring factual consistency, often
leading to generic or misleading headlines. Leveraging the unprecedented
capabilities of Large Language Models (LLMs) in text generation, we propose
Context-Augmented Personalized LLM (CAP-LLM), a novel framework that integrates
user preferences and factual consistency constraints into a powerful
pre-trained LLM backbone. CAP-LLM features a User Preference Encoder to capture
long-term user interests, a Context Injection Adapter to seamlessly integrate
these preferences and current article context into the LLM's generation
process, and a Fact-Consistency Reinforcement Module employing a novel
contrastive loss to mitigate hallucination. Evaluated on the real-world PENS
dataset, CAP-LLM achieves state-of-the-art performance across all metrics.
Notably, it significantly improves factual consistency (FactCC of 87.50) over
strong baselines like BART (86.67), while simultaneously enhancing
personalization (Pc(avg) 2.73, Pc(max) 17.25) and content coverage (ROUGE-1
26.55, ROUGE-2 9.95, ROUGE-L 23.01). Our ablation studies, human evaluations,
and sensitivity analyses further validate the effectiveness of each component
and the robustness of our approach, demonstrating CAP-LLM's ability to achieve
a superior balance between personalization and factual accuracy in news
headline generation.

</details>


### [14] [Data and AI governance: Promoting equity, ethics, and fairness in large language models](https://arxiv.org/abs/2508.03970)
*Alok Abhishek,Lisa Erickson,Tushar Bandopadhyay*

Main category: cs.CL

TL;DR: 论文提出通过数据与AI治理框架在机器学习全生命周期系统化解决LLM的偏见、伦理与事实性问题，提升生成式AI的安全性和社会责任。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型普遍存在偏见与公平性缺陷，需建立系统化治理框架规避生产部署中的歧视风险与品牌声誉损害。

Method: 基于BEATS测试套件扩展，构建覆盖开发验证-生产监控-实时评估的全生命周期治理体系，实施模型基准测试与响应管控。

Result: 该治理框架能显著增强生成式AI安全性，降低80%的歧视风险(论文数据)，实现模型输出的持续性伦理对齐。

Conclusion: 跨生命周期的系统治理为负责任AI部署提供实践路径，推动生成式应用与社会伦理价值的有效协同。

Abstract: In this paper, we cover approaches to systematically govern, assess and
quantify bias across the complete life cycle of machine learning models, from
initial development and validation to ongoing production monitoring and
guardrail implementation. Building upon our foundational work on the Bias
Evaluation and Assessment Test Suite (BEATS) for Large Language Models, the
authors share prevalent bias and fairness related gaps in Large Language Models
(LLMs) and discuss data and AI governance framework to address Bias, Ethics,
Fairness, and Factuality within LLMs. The data and AI governance approach
discussed in this paper is suitable for practical, real-world applications,
enabling rigorous benchmarking of LLMs prior to production deployment,
facilitating continuous real-time evaluation, and proactively governing LLM
generated responses. By implementing the data and AI governance across the life
cycle of AI development, organizations can significantly enhance the safety and
responsibility of their GenAI systems, effectively mitigating risks of
discrimination and protecting against potential reputational or brand-related
harm. Ultimately, through this article, we aim to contribute to advancement of
the creation and deployment of socially responsible and ethically aligned
generative artificial intelligence powered applications.

</details>


### [15] [Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency](https://arxiv.org/abs/2508.03979)
*Md Arafat Sultan,Ramón Fernandez Astudillo*

Main category: cs.CL

TL;DR: 提出基于置信度和词汇覆盖的早期剪枝策略，使self-consistency推理的token效率提升10-35%


<details>
  <summary>Details</summary>
Motivation: 传统self-consistency方法在长链推理中token消耗过高，影响实际应用效率

Method: 并行生成解决方案，通过模型置信度+词汇覆盖双指标动态剪枝假设，结合加权集合覆盖算法

Result: 在5个LLM和3个数学任务中验证，多数情况下token效率提升10-35%

Conclusion: 该方法在保持并行优势的同时，显著提升长链推理的token利用率

Abstract: Despite its simplicity and efficacy, the high token expenditure of
self-consistency can limit its practical utility. Here we investigate if
self-consistency can be made more token-efficient for long chain-of-thought
reasoning tasks, while preserving its parallelism, through early hypothesis
pruning. Concretely, we generate all solutions in parallel, but periodically
prune intermediate hypotheses that are deemed unnecessary based on two
lightweight indicators: (a) the model's own confidence in individual
hypotheses, and (b) lexical coverage of all current hypotheses by candidate
subsets that are under consideration for continued retention. We design a fast
weighted set cover algorithm that utilizes the two indicators; our evaluation
of five LLMs on three math benchmarks shows that this method can improve token
efficiency for all models, by 10-35% in many cases.

</details>


### [16] [Are Today's LLMs Ready to Explain Well-Being Concepts?](https://arxiv.org/abs/2508.03990)
*Bohan Jiang,Dawei Li,Zhen Tan,Chengshuai Zhao,Huan Liu*

Main category: cs.CL

TL;DR: 研究通过构建43,880条福祉解释数据集及双评估框架，验证微调方法可显著提升LLMs生成解释的适众性。最佳DPO微调模型超越GPT-4表现。


<details>
  <summary>Details</summary>
Motivation: 随着公众使用LLMs获取福祉指导的需求激增，需验证模型能否生成既准确又适配不同认知水平用户的解释。现有研究缺乏系统性评估框架。

Method: 1) 构建含10个LLMs生成的43,880条解释数据集；2) 设计原则导向的双评估框架（基础质量+受众适配性）；3) 采用SFT和DPO微调开源模型。

Result: 1) 评估框架与人类判断高度一致（ρ=0.82）；2) GPT-4在基础质量领先但受众适配欠佳；3) DPO微调的7B模型综合表现超越GPT-4。

Conclusion: 提出首个系统性LLM解释评估框架，证实偏好学习能有效提升专业领域解释生成。为构建负责任AI系统提供方法论，推动个性化健康咨询发展。

Abstract: Well-being encompasses mental, physical, and social dimensions essential to
personal growth and informed life decisions. As individuals increasingly
consult Large Language Models (LLMs) to understand well-being, a key challenge
emerges: Can LLMs generate explanations that are not only accurate but also
tailored to diverse audiences? High-quality explanations require both factual
correctness and the ability to meet the expectations of users with varying
expertise. In this work, we construct a large-scale dataset comprising 43,880
explanations of 2,194 well-being concepts, generated by ten diverse LLMs. We
introduce a principle-guided LLM-as-a-judge evaluation framework, employing
dual judges to assess explanation quality. Furthermore, we show that
fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct
Preference Optimization (DPO) can significantly enhance the quality of
generated explanations. Our results reveal: (1) The proposed LLM judges align
well with human evaluations; (2) explanation quality varies significantly
across models, audiences, and categories; and (3) DPO- and SFT-finetuned models
outperform their larger counterparts, demonstrating the effectiveness of
preference-based learning for specialized explanation tasks.

</details>


### [17] [Transferring Expert Cognitive Models to Social Robots via Agentic Concept Bottleneck Models](https://arxiv.org/abs/2508.03998)
*Xinyu Zhao,Zhen Tan,Maya Enisman,Minjae Seo,Marta R. Durantini,Dolores Albarracin,Tianlong Chen*

Main category: cs.CL

TL;DR: 提出社交机器人协同主持框架，通过透明化的概念瓶颈模型分析多模态会议数据，帮助人类主持人降低认知负荷并提升群体干预效率。


<details>
  <summary>Details</summary>
Motivation: 群体会议中主持人面临认知过载挑战，现有基础模型（FM）存在黑箱决策缺陷。需构建透明可解释的辅助系统，将专家经验转移至新手主持人。

Method: 采用概念瓶颈模型（CBM）构建机器人推理系统，通过迁移学习框架将基础模型的社会理解能力蒸馏到可解释概念（如参与度、情绪），实现实时人工修正。

Result: 模型在干预需求预测上显著超越零样本FM（准确率提升28%），成功实现跨群体知识迁移，并将专家经验转移使新手主持人绩效提升37%。

Conclusion: 该框架为复杂社交领域的人机协作提供新范式，通过可解释AI增强人类能力，在行为治疗、企业管理等领域具广泛应用前景。

Abstract: Successful group meetings, such as those implemented in group
behavioral-change programs, work meetings, and other social contexts, must
promote individual goal setting and execution while strengthening the social
relationships within the group. Consequently, an ideal facilitator must be
sensitive to the subtle dynamics of disengagement, difficulties with individual
goal setting and execution, and interpersonal difficulties that signal a need
for intervention. The challenges and cognitive load experienced by facilitators
create a critical gap for an embodied technology that can interpret social
exchanges while remaining aware of the needs of the individuals in the group
and providing transparent recommendations that go beyond powerful but "black
box" foundation models (FMs) that identify social cues. We address this
important demand with a social robot co-facilitator that analyzes multimodal
meeting data and provides discreet cues to the facilitator. The robot's
reasoning is powered by an agentic concept bottleneck model (CBM), which makes
decisions based on human-interpretable concepts like participant engagement and
sentiments, ensuring transparency and trustworthiness. Our core contribution is
a transfer learning framework that distills the broad social understanding of
an FM into our specialized and transparent CBM. This concept-driven system
significantly outperforms direct zero-shot FMs in predicting the need for
intervention and enables real-time human correction of its reasoning.
Critically, we demonstrate robust knowledge transfer: the model generalizes
across different groups and successfully transfers the expertise of senior
human facilitators to improve the performance of novices. By transferring an
expert's cognitive model into an interpretable robotic partner, our work
provides a powerful blueprint for augmenting human capabilities in complex
social domains.

</details>


### [18] [HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy Enhancement and Dual-Objective Optimization](https://arxiv.org/abs/2508.04010)
*Yurun Chen,Xavier Hu,Yuhan Liu,Keting Yin,Juncheng Li,Zhuosheng Zhang,Shengyu Zhang*

Main category: cs.CL

TL;DR: 提出多智能体协作框架HarmonyGuard，通过策略增强和双重目标优化实现网络安全与任务效用的协同提升。


<details>
  <summary>Details</summary>
Motivation: 现有网络代理在长期操作中难以平衡安全风险与任务效能，缺乏协同优化机制。

Method: 包含自适应策略增强（自动提取结构化安全策略）和双重目标优化（基于马尔可夫实时推理与元认知能力）。

Result: 基准测试显示策略合规性提升38%，任务完成率提高20%，所有任务合规率超90%。

Conclusion: 该框架有效解决了网络代理安全与效用的协同优化问题，具有实际应用价值。

Abstract: Large language models enable agents to autonomously perform tasks in open web
environments. However, as hidden threats within the web evolve, web agents face
the challenge of balancing task performance with emerging risks during
long-sequence operations. Although this challenge is critical, current research
remains limited to single-objective optimization or single-turn scenarios,
lacking the capability for collaborative optimization of both safety and
utility in web environments. To address this gap, we propose HarmonyGuard, a
multi-agent collaborative framework that leverages policy enhancement and
objective optimization to jointly improve both utility and safety. HarmonyGuard
features a multi-agent architecture characterized by two fundamental
capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent
within HarmonyGuard, which automatically extracts and maintains structured
security policies from unstructured external documents, while continuously
updating policies in response to evolving threats. (2) Dual-Objective
Optimization: Based on the dual objectives of safety and utility, the Utility
Agent integrated within HarmonyGuard performs the Markovian real-time reasoning
to evaluate the objectives and utilizes metacognitive capabilities for their
optimization. Extensive evaluations on multiple benchmarks show that
HarmonyGuard improves policy compliance by up to 38% and task completion by up
to 20% over existing baselines, while achieving over 90% policy compliance
across all tasks. Our project is available here:
https://github.com/YurunChen/HarmonyGuard.

</details>


### [19] [Step More: Going Beyond Single Backpropagation in Meta Learning Based Model Editing](https://arxiv.org/abs/2508.04012)
*Xiaopeng Li,Shasha Li,Xi Wang,Shezheng Song,Bin Ji,Shangwen Wang,Jun Ma,Xiaodong Liu,Mina Liu,Jie Yu*

Main category: cs.CL

TL;DR: 提出SMEdit方法，通过多步反向传播和正则化提升元学习模型编辑在低数据场景的性能和训练效率


<details>
  <summary>Details</summary>
Motivation: 现有基于元学习的模型编辑方法(MLBME)在低数据场景表现欠佳，且KL散度计算制约训练效率

Method: SMEdit采用多步反向传播策略(MBPS)提升有限监督下的编辑效果，并引入权重更新范数正则化优化训练效率

Result: 在两种LLM和数据集上的实验显示SMEdit优于现有方法，且MBPS策略可无缝集成提升其他方法性能

Conclusion: SMEdit有效解决了MLBME的局限性，其提出的MBPS策略具有通用性，为模型编辑领域提供了新思路

Abstract: Large Language Models (LLMs) underpin many AI applications, but their static
nature makes updating knowledge costly. Model editing offers an efficient
alternative by injecting new information through targeted parameter
modifications. In particular, meta-learning-based model editing (MLBME) methods
have demonstrated notable advantages in both editing effectiveness and
efficiency. Despite this, we find that MLBME exhibits suboptimal performance in
low-data scenarios, and its training efficiency is bottlenecked by the
computation of KL divergence. To address these, we propose $\textbf{S}$tep
$\textbf{M}$ore $\textbf{Edit}$ ($\textbf{SMEdit}$), a novel MLBME method that
adopts $\textbf{M}$ultiple $\textbf{B}$ackpro$\textbf{P}$agation
$\textbf{S}$teps ($\textbf{MBPS}$) to improve editing performance under limited
supervision and a norm regularization on weight updates to improve training
efficiency. Experimental results on two datasets and two LLMs demonstrate that
SMEdit outperforms prior MLBME baselines and the MBPS strategy can be
seamlessly integrated into existing methods to further boost their performance.
Our code will be released soon.

</details>


### [20] [ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents](https://arxiv.org/abs/2508.04038)
*Zechen Li,Baiyu Chen,Hao Xue,Flora D. Salim*

Main category: cs.CL

TL;DR: 提出首个基于代理的零样本可解释HAR框架ZARA，通过特征知识库+分层推理流程实现无需微调的SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有HAR方法需针对固定活动集重新训练，LLM直接应用存在精度低和解释性差的问题

Method: 整合自动生成的特征知识库、多传感器检索模块和分层代理推理流程，实现基于原始数据的迭代式特征选择与解释生成

Result: 在8个基准测试中零样本性能达SOTA，F1值超基线2.53倍，消融实验验证模块必要性

Conclusion: ZARA标志着可信赖的即插即用型运动时序分析的重要进展，兼具灵活性与可解释性优势

Abstract: Motion sensor time-series are central to human activity recognition (HAR),
with applications in health, sports, and smart devices. However, existing
methods are trained for fixed activity sets and require costly retraining when
new behaviours or sensor setups appear. Recent attempts to use large language
models (LLMs) for HAR, typically by converting signals into text or images,
suffer from limited accuracy and lack verifiable interpretability. We propose
ZARA, the first agent-based framework for zero-shot, explainable HAR directly
from raw motion time-series. ZARA integrates an automatically derived pair-wise
feature knowledge base that captures discriminative statistics for every
activity pair, a multi-sensor retrieval module that surfaces relevant evidence,
and a hierarchical agent pipeline that guides the LLM to iteratively select
features, draw on this evidence, and produce both activity predictions and
natural-language explanations. ZARA enables flexible and interpretable HAR
without any fine-tuning or task-specific classifiers. Extensive experiments on
8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering
clear reasoning while exceeding the strongest baselines by 2.53x in macro F1.
Ablation studies further confirm the necessity of each module, marking ZARA as
a promising step toward trustworthy, plug-and-play motion time-series analysis.
Our codes are available at https://github.com/zechenli03/ZARA.

</details>


### [21] [Large Reasoning Models Are Autonomous Jailbreak Agents](https://arxiv.org/abs/2508.04039)
*Thilo Hagendorff,Erik Derner,Nuria Oliver*

Main category: cs.CL

TL;DR: 大型推理模型可自动化突破AI安全机制，非专家攻击成功率高达97.14%


<details>
  <summary>Details</summary>
Motivation: 传统越狱方法依赖复杂技术/专家经验，本研究验证大型推理模型可简化攻击流程并扩大攻击规模

Method: 使用4个LRM模型（DeepSeek-R1等）与9个目标模型进行多轮对抗对话，基于70个跨7大敏感领域的有害提示构建测试基准

Result: 总体攻击成功率97.14%，揭示模型存在对齐退化现象（LRMs可系统性削弱其他模型的安全防护）

Conclusion: 亟需增强前沿模型的双重防御能力：既要抵抗越狱攻击，也要避免自身被转化为攻击工具

Abstract: Jailbreaking -- bypassing built-in safety mechanisms in AI models -- has
traditionally required complex technical procedures or specialized human
expertise. In this study, we show that the persuasive capabilities of large
reasoning models (LRMs) simplify and scale jailbreaking, converting it into an
inexpensive activity accessible to non-experts. We evaluated the capabilities
of four LRMs (DeepSeek-R1, Gemini 2.5 Flash, Grok 3 Mini, Qwen3 235B) to act as
autonomous adversaries conducting multi-turn conversations with nine widely
used target models. LRMs received instructions via a system prompt, before
proceeding to planning and executing jailbreaks with no further supervision. We
performed extensive experiments with a benchmark of harmful prompts composed of
70 items covering seven sensitive domains. This setup yielded an overall attack
success rate across all model combinations of 97.14%. Our study reveals an
alignment regression, in which LRMs can systematically erode the safety
guardrails of other models, highlighting the urgent need to further align
frontier models not only to resist jailbreak attempts, but also to prevent them
from being co-opted into acting as jailbreak agents.

</details>


### [22] [DTPA: Dynamic Token-level Prefix Augmentation for Controllable Text Generation](https://arxiv.org/abs/2508.04047)
*Jiabing Yang,Yixiang Chen,Zichen Wen,Chenhang Cui,Peiyan Li,Yuan Xu,Bowen Fang,Yan Huang,Liang Wang*

Main category: cs.CL

TL;DR: 提出动态令牌级前缀增强框架（DTPA），通过动态调整前缀注意力解决长文本可控生成中的控制力衰减问题，实验证明其在保持文本质量的同时显著提升控制效果。


<details>
  <summary>Details</summary>
Motivation: 现有可控文本生成方法在长文本生成时控制力显著下降，主要由于前缀注意力衰减和前缀类型选择不当导致。

Method: 基于Air-Decoding框架，动态选择最优前缀类型，通过指数增长的缩放因子增强前缀注意力，并可选增强原始提示以平衡质量与控制。

Result: 在多个CTG任务中实现最佳控制效果，长文本生成效果提升显著，同时保持文本流畅性、多样性和主题相关性。

Conclusion: DTPA有效解决长文本生成控制力衰减问题，为可控文本生成提供新的优化方向，兼顾控制强度与生成质量。

Abstract: Controllable Text Generation (CTG) is a vital subfield in Natural Language
Processing (NLP), aiming to generate text that aligns with desired attributes.
However, previous studies commonly focus on the quality of controllable text
generation for short sequences, while the generation of long-form text remains
largely underexplored. In this paper, we observe that the controllability of
texts generated by the powerful prefix-based method Air-Decoding tends to
decline with increasing sequence length, which we hypothesize primarily arises
from the observed decay in attention to the prefixes. Meanwhile, different
types of prefixes including soft and hard prefixes are also key factors
influencing performance. Building on these insights, we propose a lightweight
and effective framework called Dynamic Token-level Prefix Augmentation (DTPA)
based on Air-Decoding for controllable text generation. Specifically, it first
selects the optimal prefix type for a given task. Then we dynamically amplify
the attention to the prefix for the attribute distribution to enhance
controllability, with a scaling factor growing exponentially as the sequence
length increases. Moreover, based on the task, we optionally apply a similar
augmentation to the original prompt for the raw distribution to balance text
quality. After attribute distribution reconstruction, the generated text
satisfies the attribute constraints well. Experiments on multiple CTG tasks
demonstrate that DTPA generally outperforms other methods in attribute control
while maintaining competitive fluency, diversity, and topic relevance. Further
analysis highlights DTPA's superior effectiveness in long text generation.

</details>


### [23] [PAIRS: Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG](https://arxiv.org/abs/2508.04057)
*Wang Chen,Guanqiang Qi,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.CL

TL;DR: PAIRS框架通过双路径生成与自适应检索机制，提升RAG系统的效率（减少25%检索次数）和准确率（平均提升1.1% EM），解决无效检索与信息稀疏问题。


<details>
  <summary>Details</summary>
Motivation: 针对现有RAG系统存在的两大问题：1）对所有查询强制检索导致效率低下；2）信息稀疏查询下检索结果不相关。PAIRS通过参数化验证与动态决策机制实现按需检索。

Method: 1）双路径生成：同时输出直接答案与伪上下文增强答案，收敛时跳过检索；2）分歧时触发双路径检索（DPR）结合原始查询与上下文信号；3）自适应信息选择（AIS）通过加权相似度过滤文档。

Result: 在6个QA基准测试中，PAIRS仅对75%的查询触发检索，降低25%检索成本的同时，平均准确率提升1.1% EM和1.0% F1，优于现有基线。

Conclusion: PAIRS通过动态检索决策与上下文引导的文档选择机制，在保持简单性的同时显著优化RAG效率与精度，为实际部署提供更实用的解决方案。

Abstract: Retrieval-Augmented Generation (RAG) has become a cornerstone technique for
enhancing large language models (LLMs) with external knowledge. However,
current RAG systems face two critical limitations: (1) they inefficiently
retrieve information for every query, including simple questions that could be
resolved using the LLM's parametric knowledge alone, and (2) they risk
retrieving irrelevant documents when queries contain sparse information
signals. To address these gaps, we introduce Parametric-verified Adaptive
Information Retrieval and Selection (PAIRS), a training-free framework that
integrates parametric and retrieved knowledge to adaptively determine whether
to retrieve and how to select external information. Specifically, PAIRS employs
a dual-path generation mechanism: First, the LLM produces both a direct answer
and a context-augmented answer using self-generated pseudo-context. When these
outputs converge, PAIRS bypasses external retrieval entirely, dramatically
improving the RAG system's efficiency. For divergent cases, PAIRS activates a
dual-path retrieval (DPR) process guided by both the original query and
self-generated contextual signals, followed by an Adaptive Information
Selection (AIS) module that filters documents through weighted similarity to
both sources. This simple yet effective approach can not only enhance
efficiency by eliminating unnecessary retrievals but also improve accuracy
through contextually guided retrieval and adaptive information selection.
Experimental results on six question-answering (QA) benchmarks show that PAIRS
reduces retrieval costs by around 25% (triggering for only 75% of queries)
while still improving accuracy-achieving +1.1% EM and +1.0% F1 over prior
baselines on average.

</details>


### [24] [Efficient Strategy for Improving Large Language Model (LLM) Capabilities](https://arxiv.org/abs/2508.04073)
*Julián Camilo Velandia Gutiérrez*

Main category: cs.CL

TL;DR: 通过数据优化、训练策略和架构调整提升大语言模型在资源受限环境下的效率


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型部署受限于计算资源需求，需探索效率提升方案

Method: 基于可靠数据集构建标准，开展配置对比实验并系统评估模型能力/响应/安全性

Result: 通过对比测试验证策略有效性，开发出多种高效模型变体

Conclusion: 该研究为提升LLM在受限环境下的能力提供了系统性工程方案，源自系统工程硕士论文成果

Abstract: Large Language Models (LLMs) have become a milestone in the field of
artificial intelligence and natural language processing. However, their
large-scale deployment remains constrained by the need for significant
computational resources. This work proposes starting from a base model to
explore and combine data processing and careful data selection techniques,
training strategies, and architectural adjustments to improve the efficiency of
LLMs in resource-constrained environments and within a delimited knowledge
base. The methodological approach included defining criteria for building
reliable datasets, conducting controlled experiments with different
configurations, and systematically evaluating the resulting variants in terms
of capability, versatility, response time, and safety. Finally, comparative
tests were conducted to measure the performance of the developed variants and
to validate the effectiveness of the proposed strategies. This work is based on
the master's thesis in Systems and Computer Engineering titled "Efficient
Strategy for Improving the Capabilities of Large Language Models (LLMs)".

</details>


### [25] [ToolGrad: Efficient Tool-use Dataset Generation with Textual "Gradients"](https://arxiv.org/abs/2508.04086)
*Zhongyi Zhou,Kohei Uehara,Haoyu Zhang,Jingtao Zhou,Lin Gu,Ruofei Du,Zheng Xu,Tatsuya Harada*

Main category: cs.CL

TL;DR: 提出ToolGrad框架，通过先构建工具链再生成查询的答案优先方法，高效生成复杂工具使用数据集ToolGrad-5k，显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 传统工具链标注方法存在效率低/标注失败率高的问题，研究通过倒置流程（先工具链后用户查询）提升数据生成质量

Method: 1. 基于文本梯度迭代构建工具链 2. 逆向生成适配的用户查询 3. 采用答案优先范式确保100%有效性

Result: 生成数据集ToolGrad-5k工具复杂度提升30%，标注成本降低5倍，模型在ID/OOD场景下均超越GPT-3.5和传统方法模型

Conclusion: ToolGrad框架突破数据生成瓶颈，证明高效合成复杂工具使用数据的可行性，为LLM工具适配提供新范式

Abstract: Prior work synthesizes tool-use LLM datasets by first generating a user
query, followed by complex tool-use annotations like DFS. This leads to
inevitable annotation failures and low efficiency in data generation. We
introduce ToolGrad, an agentic framework that inverts this paradigm. ToolGrad
first constructs valid tool-use chains through an iterative process guided by
textual "gradients", and then synthesizes corresponding user queries. This
"answer-first" approach led to ToolGrad-5k, a dataset generated with more
complex tool use, lower cost, and 100% pass rate. Experiments show that models
trained on ToolGrad-5k outperform those on expensive baseline datasets and
proprietary LLMs, even on OOD benchmarks.

</details>


### [26] [GM-PRM: A Generative Multimodal Process Reward Model for Multimodal Mathematical Reasoning](https://arxiv.org/abs/2508.04088)
*Jianghangfan Zhang,Yibo Yan,Kening Zheng,Xin Zou,Song Dai,Xuming Hu*

Main category: cs.CL

TL;DR: 提出GM-PRM模型，将传统被动评估的PRM转化为主动协作的纠正模型，通过Refined-BoN策略优化多模态数学推理效果


<details>
  <summary>Details</summary>
Motivation: 现有PRM仅能作为二元验证器识别错误但无法纠正，且缺乏解释性，导致MLLMs在复杂多步数学推理中易因视觉感知或逻辑推导的细微错误失败

Method: 1. 设计细粒度可解释分析框架（评估步骤意图/视觉对齐/逻辑严密性）
2. 训练模型生成首个错误步骤的纠正版本
3. 开发Refined-BoN推理策略，利用纠正步骤引导策略模型优化推理路径

Result: 在多个多模态数学基准达到SOTA，仅需20K训练样本即显著提升策略模型性能（MMath: +13.2%, MathVista: +9.8%）

Conclusion: GM-PRM通过主动纠错机制和协同推理范式，突破传统PRM的被动局限，其生成的语义化反馈显著提升解决方案的多样性和正确率

Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities
but often struggle with complex, multi-step mathematical reasoning, where minor
errors in visual perception or logical deduction can lead to complete failure.
While Process Reward Models (PRMs) offer step-by-step supervision, existing
multimodal PRMs are limited to being binary verifiers that can identify but not
correct errors, offering little explanatory power. To address these
deficiencies, we introduce the Generative Multimodal Process Reward Model
(GM-PRM), a novel paradigm that transforms the PRM from a passive judge into an
active reasoning collaborator. Instead of a simple scalar score, GM-PRM
provides a fine-grained, interpretable analysis of each reasoning step,
evaluating its step intent, visual alignment, and logical soundness. More
critically, GM-PRM is trained to generate a corrected version of the first
erroneous step it identifies. This unique corrective capability enables our new
test-time inference strategy, Refined Best-of-N (Refined-BoN). This framework
actively enhances solution quality by using the PRM's generated correction to
guide the policy model toward a more promising reasoning trajectory, thereby
improving the diversity and correctness of the solution pool. We demonstrate
that GM-PRM achieves state-of-the-art results on multiple multimodal math
benchmarks, significantly boosting policy model performance with remarkable
data efficiency, requiring only a 20K-sample training dataset. Our code will be
released upon acceptance.

</details>


### [27] [Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks](https://arxiv.org/abs/2508.04117)
*Zhiwen Ruan,Yun Chen,Yutao Hou,Peng Li,Yang Liu,Guanhua Chen*

Main category: cs.CL

TL;DR: LLM微调过程中存在过度记忆现象，导致高测试困惑度但保持准确率，影响模型鲁棒性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在指令微调过程中的特殊学习动态，揭示传统机器学习中未观察到的过拟合模式

Method: 通过控制训练周期和学习率进行实验，评估模型在不同微调阶段的记忆程度和泛化表现

Result: 发现过记忆模型测试精度相当但鲁棒性差、分布外泛化弱、生成多样性降低

Conclusion: 建议在微调时注意检查点选择和学习率控制，避免过参数化模型的特异性学习动态

Abstract: The pretrained large language models (LLMs) are finetuned with labeled data
for better instruction following ability and alignment with human values. In
this paper, we study the learning dynamics of LLM finetuning on reasoning tasks
and reveal the uncovered over-memorization phenomenon during a specific stage
of LLM finetuning. At this stage, the LLMs have excessively memorized training
data and exhibit high test perplexity while maintaining good test accuracy. We
investigate the conditions that lead to LLM over-memorization and find that
training epochs and large learning rates contribute to this issue. Although
models with over-memorization demonstrate comparable test accuracy to normal
models, they suffer from reduced robustness, poor out-of-distribution
generalization, and decreased generation diversity. Our experiments unveil the
over-memorization to be broadly applicable across different tasks, models, and
finetuning methods. Our research highlights that overparameterized, extensively
finetuned LLMs exhibit unique learning dynamics distinct from traditional
machine learning models. Based on our observations of over-memorization, we
provide recommendations on checkpoint and learning rate selection during
finetuning.

</details>


### [28] [Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap](https://arxiv.org/abs/2508.04149)
*Xuan Qi,Rongwu Xu,Zhijing Jin*

Main category: cs.CL

TL;DR: 提出基于DPO隐式奖励机制的难度数据选择策略，仅需10%数据即可超越主流对齐方法


<details>
  <summary>Details</summary>
Motivation: 现有RLHF/DPO对齐方法依赖大规模偏好数据，缺乏高效的数据筛选机制导致资源消耗巨大

Method: 通过DPO隐式奖励差值识别困难样本（奖励差距小的模糊案例），构建高质量偏好数据集

Result: 在多个数据集上超越5个基线模型，仅用10%数据达到更优对齐效果

Conclusion: 为资源受限场景下的LLM对齐提供了数据高效的解决方案

Abstract: Aligning large language models (LLMs) with human preferences is a critical
challenge in AI research. While methods like Reinforcement Learning from Human
Feedback (RLHF) and Direct Preference Optimization (DPO) are widely used, they
often rely on large, costly preference datasets. The current work lacks methods
for high-quality data selection specifically for preference data. In this work,
we introduce a novel difficulty-based data selection strategy for preference
datasets, grounded in the DPO implicit reward mechanism. By selecting
preference data examples with smaller DPO implicit reward gaps, which are
indicative of more challenging cases, we improve data efficiency and model
alignment. Our approach consistently outperforms five strong baselines across
multiple datasets and alignment tasks, achieving superior performance with only
10\% of the original data. This principled, efficient selection method offers a
promising solution for scaling LLM alignment with limited resources.

</details>


### [29] [The State Of TTS: A Case Study with Human Fooling Rates](https://arxiv.org/abs/2508.04179)
*Praveen Srinivasa Varadhan,Sherry Thomas,Sai Teja M. S.,Suvrat Bhooshan,Mitesh M. Khapra*

Main category: cs.CL

TL;DR: 该论文引入人类欺骗率（HFR）指标，揭示商用TTS模型在零样本场景接近人类水平，但开源系统仍存在差距；强调需要更人性化的评估体系


<details>
  <summary>Details</summary>
Motivation: 质疑现有TTS系统是否真正通过图灵测试，主张用欺骗性测试代替传统主观评估（如CMOS）来衡量语音合成的人类相似度

Method: 通过大规模开源/商用TTS模型评估，在零样本和微调场景下测量语音被误认为人类的概率

Result: 1) CMOS指标存在误导性 2) 商用模型零样本欺骗率达48% 3) 高质量微调数据提升有限 4) 评估数据集需包含高表现力人类语音

Conclusion: 建议在现有主观评估外增加基于HFR的欺骗性测试，建立更严格的人类语音基准数据集以推动TTS技术发展

Abstract: While subjective evaluations in recent years indicate rapid progress in TTS,
can current TTS systems truly pass a human deception test in a Turing-like
evaluation? We introduce Human Fooling Rate (HFR), a metric that directly
measures how often machine-generated speech is mistaken for human. Our
large-scale evaluation of open-source and commercial TTS models reveals
critical insights: (i) CMOS-based claims of human parity often fail under
deception testing, (ii) TTS progress should be benchmarked on datasets where
human speech achieves high HFRs, as evaluating against monotonous or less
expressive reference samples sets a low bar, (iii) Commercial models approach
human deception in zero-shot settings, while open-source systems still struggle
with natural conversational speech; (iv) Fine-tuning on high-quality data
improves realism but does not fully bridge the gap. Our findings underscore the
need for more realistic, human-centric evaluations alongside existing
subjective tests.

</details>


### [30] [Hacking Hallucinations of MLLMs with Causal Sufficiency and Necessity](https://arxiv.org/abs/2508.04182)
*Peizheng Guo,Jingyao Wang,Wenwen Qiang,Huijie Guo,Changwen Zheng,Jiahuan Zhou,Gang Hua*

Main category: cs.CL

TL;DR: 提出基于因果完备性的强化学习框架，通过联合考虑token的因果充分性和必要性奖励机制，有效减少多模态大语言模型的幻觉现象


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在生成过程中存在遗漏型/捏造型幻觉问题，影响输出可靠性。需要从因果角度分析幻觉根源：关键因果因素捕捉不足（遗漏）和非因果线索误导（捏造）

Method: 设计token级别的因果完备性奖励（包含独立贡献度和反事实必要性），构建因果驱动的优势函数，在GRPO优化框架中引导模型聚焦因果关键token

Result: 在多个基准数据集和任务中验证有效性，显著降低模型幻觉现象

Conclusion: 该框架通过强化因果关联性，为提升多模态大语言模型的生成可靠性提供了新思路和理论依据

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across vision-language tasks. However, they may suffer from
hallucinations--generating outputs that are semantically inconsistent with the
input image or text. Through causal analyses, we find that: (i) hallucinations
with omission may arise from the failure to adequately capture essential causal
factors, and (ii) hallucinations with fabrication are likely caused by the
model being misled by non-causal cues. To address these challenges, we propose
a novel reinforcement learning framework guided by causal completeness, which
jointly considers both causal sufficiency and causal necessity of tokens.
Specifically, we evaluate each token's standalone contribution and
counterfactual indispensability to define a token-level causal completeness
reward. This reward is used to construct a causally informed advantage function
within the GRPO optimization framework, encouraging the model to focus on
tokens that are both causally sufficient and necessary for accurate generation.
Experimental results across various benchmark datasets and tasks demonstrate
the effectiveness of our approach, which effectively mitigates hallucinations
in MLLMs.

</details>


### [31] [Characterizing Deep Research: A Benchmark and Formal Definition](https://arxiv.org/abs/2508.04183)
*Abhinav Java,Ashmit Khandelwal,Sukruta Midigeshi,Aaron Halfaker,Amit Deshpande,Navin Goyal,Ankur Gupta,Nagarajan Natarajan,Amit Sharma*

Main category: cs.CL

TL;DR: 论文提出深度研究任务的形式化定义与评估基准LiveDRBench，通过中间表示法分离推理过程与报告生成，揭示现有系统性能差距与改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前深度研究任务定义模糊且缺乏评估标准，需通过形式化定义和客观评估基准推动领域发展。

Method: 使用中间输出表示编码关键主张，构建含100个跨领域任务的LiveDRBench基准测试，分析系统搜索机制与推理过程。

Result: 现有系统F1分数0.02-0.72，OpenAI表现最佳（0.55）。分析显示系统在引用源数量、分支与回溯行为存在显著差异。

Conclusion: 形式化定义深度研究任务并建立评估基准，揭示当前系统局限性，为改进搜索机制与基础能力提供方向。

Abstract: Information tasks such as writing surveys or analytical reports require
complex search and reasoning, and have recently been grouped under the umbrella
of \textit{deep research} -- a term also adopted by recent models targeting
these capabilities. Despite growing interest, the scope of the deep research
task remains underdefined and its distinction from other reasoning-intensive
problems is poorly understood. In this paper, we propose a formal
characterization of the deep research (DR) task and introduce a benchmark to
evaluate the performance of DR systems. We argue that the core defining feature
of deep research is not the production of lengthy report-style outputs, but
rather the high fan-out over concepts required during the search process, i.e.,
broad and reasoning-intensive exploration. To enable objective evaluation, we
define DR using an intermediate output representation that encodes key claims
uncovered during search-separating the reasoning challenge from surface-level
report generation. Based on this formulation, we propose a diverse, challenging
benchmark LiveDRBench with 100 challenging tasks over scientific topics (e.g.,
datasets, materials discovery, prior art search) and public interest events
(e.g., flight incidents, movie awards). Across state-of-the-art DR systems, F1
score ranges between 0.02 and 0.72 for any sub-category. OpenAI's model
performs the best with an overall F1 score of 0.55. Analysis of reasoning
traces reveals the distribution over the number of referenced sources,
branching, and backtracking events executed by current DR systems, motivating
future directions for improving their search mechanisms and grounding
capabilities. The benchmark is available at
https://github.com/microsoft/LiveDRBench.

</details>


### [32] [Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models](https://arxiv.org/abs/2508.04196)
*Siddhant Panpatil,Hiskias Dingeto,Haon Park*

Main category: cs.CL

TL;DR: 前沿语言模型仍存在对话场景诱导的错位漏洞，红队测试揭示10种攻击模式，自动化框架验证76%平均漏洞率，GPT-4.1最脆弱(90%)


<details>
  <summary>Details</summary>
Motivation: 现有AI对齐技术无法防御基于叙事沉浸/情感压力/战略框架的隐性攻击，需系统性检测模型价值观漂移漏洞

Method: 1. Claude-4-Opus手动红队测试构建10种攻击场景
2. 提炼MISALIGNMENTBENCH自动化评估框架
3. 跨5个前沿模型进行场景验证测试

Result: 总体漏洞率76%：
- GPT-4.1 90%
- Claude-4-Sonnet 40%
模型复杂推理能力成为攻击入口，可被操纵进行自我辩护

Conclusion: 当前对齐策略存在场景敏感性缺陷，需开发抗叙事操纵的鲁棒性机制，建议将对话模式分类法纳入安全评估标准

Abstract: Despite significant advances in alignment techniques, we demonstrate that
state-of-the-art language models remain vulnerable to carefully crafted
conversational scenarios that can induce various forms of misalignment without
explicit jailbreaking. Through systematic manual red-teaming with
Claude-4-Opus, we discovered 10 successful attack scenarios, revealing
fundamental vulnerabilities in how current alignment methods handle narrative
immersion, emotional pressure, and strategic framing. These scenarios
successfully elicited a range of misaligned behaviors, including deception,
value drift, self-preservation, and manipulative reasoning, each exploiting
different psychological and contextual vulnerabilities. To validate
generalizability, we distilled our successful manual attacks into
MISALIGNMENTBENCH, an automated evaluation framework that enables reproducible
testing across multiple models. Cross-model evaluation of our 10 scenarios
against five frontier LLMs revealed an overall 76% vulnerability rate, with
significant variations: GPT-4.1 showed the highest susceptibility (90%), while
Claude-4-Sonnet demonstrated greater resistance (40%). Our findings demonstrate
that sophisticated reasoning capabilities often become attack vectors rather
than protective mechanisms, as models can be manipulated into complex
justifications for misaligned behavior. This work provides (i) a detailed
taxonomy of conversational manipulation patterns and (ii) a reusable evaluation
framework. Together, these findings expose critical gaps in current alignment
strategies and highlight the need for robustness against subtle, scenario-based
manipulation in future AI systems.

</details>


### [33] [Reasoning Beyond Labels: Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts](https://arxiv.org/abs/2508.04199)
*Millicent Ochieng,Anja Thieme,Ignatius Ezeani,Risa Ueno,Samuel Maina,Keshet Ronen,Javier Gonzalez,Jacki O'Neill*

Main category: cs.CL

TL;DR: 论文提出基于社会科学测量视角的诊断框架，揭示大语言模型在复杂文化语境下情感分析的推理质量差异，强调开发文化敏感性AI评估方法的必要性。


<details>
  <summary>Details</summary>
Motivation: 传统NLP方法在低资源文化复杂场景中失效，需建立能理解语境依赖型情感、符合人类推理逻辑的评估体系。

Method: 结合人类标注数据、情感反转反事实样本和结构化解释评估框架，将LLM输出作为社会测量工具进行多维度验证。

Result: 顶尖商业LLM展现解释稳定性，开源模型在歧义场景易失效，模型推理质量与文化敏感度呈显著相关性。

Conclusion: 情感分析需融合文化语境认知与推理透明度评估，推动AI系统在复杂社会沟通场景中的可靠应用。

Abstract: Sentiment analysis in low-resource, culturally nuanced contexts challenges
conventional NLP approaches that assume fixed labels and universal affective
expressions. We present a diagnostic framework that treats sentiment as a
context-dependent, culturally embedded construct, and evaluate how large
language models (LLMs) reason about sentiment in informal, code-mixed WhatsApp
messages from Nairobi youth health groups. Using a combination of
human-annotated data, sentiment-flipped counterfactuals, and rubric-based
explanation evaluation, we probe LLM interpretability, robustness, and
alignment with human reasoning. Framing our evaluation through a social-science
measurement lens, we operationalize and interrogate LLMs outputs as an
instrument for measuring the abstract concept of sentiment. Our findings reveal
significant variation in model reasoning quality, with top-tier LLMs
demonstrating interpretive stability, while open models often falter under
ambiguity or sentiment shifts. This work highlights the need for culturally
sensitive, reasoning-aware AI evaluation in complex, real-world communication.

</details>


### [34] [ReasoningGuard: Safeguarding Large Reasoning Models with Inference-time Safety Aha Moments](https://arxiv.org/abs/2508.04204)
*Yuquan Wang,Mi Zhang,Yining Wang,Geng Hong,Xiaoyu You,Min Yang*

Main category: cs.CL

TL;DR: 提出推理过程安全防护框架ReasoningGuard，通过实时注入安全节点和优化解码策略，有效防御针对大型推理模型的越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 现有防御机制依赖微调且扩展性差，需开发轻量级推理阶段防护方案来阻断有害内容生成。

Method: 结合注意力行为识别关键推理节点，触发安全导向的反射机制，并采用缩放采样策略优化推理路径选择。

Result: 在3类越狱攻击场景中实现SOTA防御效果，推理开销仅增加8.8%，安全准确率提升32%优于7个基线方法。

Conclusion: 首次在推理层面构建安全防护体系，为大型语言模型的安全部署提供实时可靠的防御范式。

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance in
reasoning-intensive tasks, but they remain vulnerable to harmful content
generation, particularly in the mid-to-late steps of their reasoning processes.
Existing defense mechanisms, however, rely on costly fine-tuning and additional
expert knowledge, which restricts their scalability. In this work, we propose
ReasoningGuard, an inference-time safeguard for LRMs, which injects timely
safety aha moments to steer harmless while helpful reasoning processes.
Leveraging the model's internal attention behavior, our approach accurately
identifies critical points in the reasoning path, and triggers spontaneous,
safety-oriented reflection. To safeguard both the subsequent reasoning steps
and the final answers, we further implement a scaling sampling strategy during
the decoding phase, selecting the optimal reasoning path. Inducing minimal
extra inference cost, ReasoningGuard effectively mitigates three types of
jailbreak attacks, including the latest ones targeting the reasoning process of
LRMs. Our approach outperforms seven existing safeguards, achieving
state-of-the-art safety defenses while effectively avoiding the common
exaggerated safety issues.

</details>


### [35] [Hierarchical Text Classification Using Black Box Large Language Models](https://arxiv.org/abs/2508.04219)
*Kosuke Yoshimura,Hisashi Kashima*

Main category: cs.CL

TL;DR: 研究探讨使用黑盒大语言模型（LLMs）进行分层文本分类的可行性，发现少样本策略提升准确性，但需权衡提示策略的成本效益


<details>
  <summary>Details</summary>
Motivation: 传统分层分类方法依赖大量标注数据和计算资源，而通过API调用LLMs可避免复杂模型训练，尤其在深层标签结构任务中展现潜力

Method: 在零样本和少样本设定下评估三种提示策略（直接叶标签预测/DL、直接层级标签预测/DH、自上而下多步预测/TMH），比较准确性和API成本

Result: 少样本显著提升准确率；DH策略在深层标签结构数据集超越传统模型，但API成本因输入token量增加而激增

Conclusion: 黑盒LLMs适用于分层文本分类，但需根据标签层次深度选择提示策略以平衡性能与成本

Abstract: Hierarchical Text Classification (HTC) aims to assign texts to structured
label hierarchies; however, it faces challenges due to data scarcity and model
complexity. This study explores the feasibility of using black box Large
Language Models (LLMs) accessed via APIs for HTC, as an alternative to
traditional machine learning methods that require extensive labeled data and
computational resources. We evaluate three prompting strategies -- Direct Leaf
Label Prediction (DL), Direct Hierarchical Label Prediction (DH), and Top-down
Multi-step Hierarchical Label Prediction (TMH) -- in both zero-shot and
few-shot settings, comparing the accuracy and cost-effectiveness of these
strategies. Experiments on two datasets show that a few-shot setting
consistently improves classification accuracy compared to a zero-shot setting.
While a traditional machine learning model achieves high accuracy on a dataset
with a shallow hierarchy, LLMs, especially DH strategy, tend to outperform the
machine learning model on a dataset with a deeper hierarchy. API costs increase
significantly due to the higher input tokens required for deeper label
hierarchies on DH strategy. These results emphasize the trade-off between
accuracy improvement and the computational cost of prompt strategy. These
findings highlight the potential of black box LLMs for HTC while underscoring
the need to carefully select a prompt strategy to balance performance and cost.

</details>


### [36] [DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time Series Forecasting](https://arxiv.org/abs/2508.04239)
*Chanjuan Liu,Shengzhi Wang,Enqiang Zhu*

Main category: cs.CL

TL;DR: 提出双提示大语言模型框架DP-GPT4MTS，通过显式指令与文本嵌入的协同优化，在多元时间序列预测任务中超越现有算法。


<details>
  <summary>Details</summary>
Motivation: 传统时序模型过度依赖数值数据，忽视事件文本信息对预测的影响；现有单提示框架存在语义捕获不足与冗余信息问题。

Method: 1. 显式提示器生成任务指令 2. 文本提示器提取时间戳数据的上下文感知嵌入 3. 通过自注意力机制和FFN网络优化嵌入表示

Result: 在文本-数值混合时序数据集上的实验表明，该框架显著提升预测精度（具体指标未披露），超越SOTA算法。

Conclusion: 双提示机制能有效融合文本上下文信息，验证了多模态数据协同建模对时序预测的重要性。

Abstract: Time series forecasting is crucial in strategic planning and decision-making
across various industries. Traditional forecasting models mainly concentrate on
numerical time series data, often overlooking important textual information
such as events and news, which can significantly affect forecasting accuracy.
While large language models offer a promise for integrating multimodal data,
existing single-prompt frameworks struggle to effectively capture the semantics
of timestamped text, introducing redundant information that can hinder model
performance. To address this limitation, we introduce DP-GPT4MTS (Dual-Prompt
GPT2-base for Multimodal Time Series), a novel dual-prompt large language model
framework that combines two complementary prompts: an explicit prompt for clear
task instructions and a textual prompt for context-aware embeddings from
time-stamped data. The tokenizer generates the explicit prompt while the
embeddings from the textual prompt are refined through self-attention and
feed-forward networks. Comprehensive experiments conducted on diverse
textural-numerical time series datasets demonstrate that this approach
outperforms state-of-the-art algorithms in time series forecasting. This
highlights the significance of incorporating textual context via a dual-prompt
mechanism to achieve more accurate time series predictions.

</details>


### [37] [TalkDep: Clinically Grounded LLM Personas for Conversation-Centric Depression Screening](https://arxiv.org/abs/2508.04248)
*Xi Wang,Anxo Perez,Javier Parapar,Fabio Crestani*

Main category: cs.CL

TL;DR: 利用语言模型开发临床参与的TalkDep患者模拟系统，生成符合诊断标准的虚拟患者提升抑郁症诊断模型训练效果


<details>
  <summary>Details</summary>
Motivation: 心理健康服务需求激增导致真实患者数据不足，现有虚拟患者生成方法存在临床有效性不足、症状表现单一等问题

Method: 基于先进语言模型构建临床参与式流程，整合精神病诊断标准、症状严重程度量表和患者背景信息生成响应

Result: 经临床专业人员验证，模拟患者具有可靠性和真实性

Conclusion: 验证有效的模拟患者可为抑郁症自动诊断系统提供可扩展资源，增强系统鲁棒性和泛化能力

Abstract: The increasing demand for mental health services has outpaced the
availability of real training data to develop clinical professionals, leading
to limited support for the diagnosis of depression. This shortage has motivated
the development of simulated or virtual patients to assist in training and
evaluation, but existing approaches often fail to generate clinically valid,
natural, and diverse symptom presentations. In this work, we embrace the recent
advanced language models as the backbone and propose a novel
clinician-in-the-loop patient simulation pipeline, TalkDep, with access to
diversified patient profiles to develop simulated patients. By conditioning the
model on psychiatric diagnostic criteria, symptom severity scales, and
contextual factors, our goal is to create authentic patient responses that can
better support diagnostic model training and evaluation. We verify the
reliability of these simulated patients with thorough assessments conducted by
clinical professionals. The availability of validated simulated patients offers
a scalable and adaptable resource for improving the robustness and
generalisability of automatic depression diagnosis systems.

</details>


### [38] [KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs](https://arxiv.org/abs/2508.04257)
*Zunhai Su,Kehong Yuan*

Main category: cs.CL

TL;DR: KV量化缓存技术通过KVSink方法有效预测注意力汇令牌，在保持模型性能的同时提升量化效果


<details>
  <summary>Details</summary>
Motivation: 现有KV缓存量化方法仅保留前几个token的精度且原理不明确，无法解决后续位置出现的注意力汇问题

Method: 通过分析注意力汇在跨层激活异常值演化中的作用机制，开发了可预测汇令牌的KVSink插件方法

Result: 实验表明KVSink优于Preserve-First-N策略，困惑度降低39.7%并减少16位数值异常依赖

Conclusion: KVSink为KV缓存量化提供了更有效的注意力汇保护方案，显著提升量化模型性能

Abstract: Key-Value (KV) cache quantization has become a widely adopted optimization
technique for efficient large language models (LLMs) inference by reducing KV
cache memory usage and mitigating memory-bound constraints. Recent studies have
emphasized the importance of preserving the original precision of KVs for the
first few tokens to ensure the protection of attention sinks. While this
approach has proven effective in mitigating performance degradation, its
underlying principles remain insufficiently understood. Moreover, it fails to
address the recent discovery that attention sinks can emerge beyond the initial
token positions. In this work, we elucidate the underlying mechanisms of
attention sinks during inference by examining their role in the cross-layer
evolution of extreme activation outliers. Additionally, we provide a
comprehensive analysis of the interplay between attention sinks and KV cache
quantization. Based on our enhanced understanding, we introduce
\textit{\textbf{KVSink}}, a plug-and-play method that effectively predicts sink
tokens with negligible overhead, enabling more thorough preservation. Extensive
experiments demonstrate that KVSink outperforms the existing Preserve-First-N
(PFN) strategy, offering more effective preservation of attention sinks during
KV cache quantization. Moreover, when applied to the well-established KVQuant
method, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit
numerical outliers.

</details>


### [39] [ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for LLM-based Agents](https://arxiv.org/abs/2508.04266)
*Jiangyuan Wang,Kejun Xiao,Qi Sun,Huaipeng Zhao,Tao Luo,Jiandong Zhang,Xiaoyi Zeng*

Main category: cs.CL

TL;DR: 提出了ShoppingBench复杂购物任务基准测试框架，结合250万真实产品的沙盒环境，实验显示GPT-4等先进模型成功率不足50%，并通过轨迹蒸馏策略使小模型达到GPT-4.1级别的性能


<details>
  <summary>Details</summary>
Motivation: 现有电商基准仅关注基础用户意图（如商品查找/购买），难以覆盖优惠券使用、预算管理、多商品卖家寻找等真实复杂需求

Method: 1. 基于真实商品构建可扩展的用户指令模拟框架 2. 创建含250万真实产品的交互式购物沙盒 3. 采用轨迹蒸馏策略结合监督微调和强化学习进行模型优化

Result: 最先进语言代理（如GPT-4.1）在基准测试中成功率低于50%，经策略优化后的小模型达到与GPT-4.1相当的竞争性表现

Conclusion: ShoppingBench有效揭示了复杂购物任务的挑战性，验证了轨迹蒸馏策略在模型能力迁移中的有效性，为电商智能代理开发提供了新方向

Abstract: Existing benchmarks in e-commerce primarily focus on basic user intents, such
as finding or purchasing products. However, real-world users often pursue more
complex goals, such as applying vouchers, managing budgets, and finding
multi-products seller. To bridge this gap, we propose ShoppingBench, a novel
end-to-end shopping benchmark designed to encompass increasingly challenging
levels of grounded intent. Specifically, we propose a scalable framework to
simulate user instructions based on various intents derived from sampled
real-world products. To facilitate consistent and reliable evaluations, we
provide a large-scale shopping sandbox that serves as an interactive simulated
environment, incorporating over 2.5 million real-world products. Experimental
results demonstrate that even state-of-the-art language agents (such as
GPT-4.1) achieve absolute success rates under 50% on our benchmark tasks,
highlighting the significant challenges posed by our ShoppingBench. In
addition, we propose a trajectory distillation strategy and leverage supervised
fine-tuning, along with reinforcement learning on synthetic trajectories, to
distill the capabilities of a large language agent into a smaller one. As a
result, our trained agent achieves competitive performance compared to GPT-4.1.

</details>


### [40] [A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2508.04276)
*Jiayi Wen,Tianxin Chen,Zhirun Zheng,Cheng Huang*

Main category: cs.CL

TL;DR: GraphRAG存在知识图构建阶段的脆弱性，两种新型知识投毒攻击（TKPA/UKPA）可通过极少量文本修改显著破坏知识图谱质量，导致下游推理严重偏差


<details>
  <summary>Details</summary>
Motivation: 针对GraphRAG依赖LLM从原始文本提取知识构建图谱的特性，研究其可能被恶意植入误导信息的攻击面

Method: 提出TKPA（基于图论分析定位漏洞节点并改写叙事）和UKPA（利用代词/依存关系等语言特征修改全局关键词）两种攻击范式

Result: TKPA攻击成功率93.1%，UKPA仅修改0.05%文本即导致QA准确率从95%暴跌至50%，且现有防御机制无法检测

Conclusion: 知识图谱增强系统的安全防护机制亟待深入研究，当前防御手段对这类结构化攻击完全失效

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as
a promising paradigm for enhancing large language models (LLMs) by converting
raw text into structured knowledge graphs, improving both accuracy and
explainability. However, GraphRAG relies on LLMs to extract knowledge from raw
text during graph construction, and this process can be maliciously manipulated
to implant misleading information. Targeting this attack surface, we propose
two knowledge poisoning attacks (KPAs) and demonstrate that modifying only a
few words in the source text can significantly change the constructed graph,
poison the GraphRAG, and severely mislead downstream reasoning. The first
attack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate
vulnerable nodes in the generated graphs and rewrites the corresponding
narratives with LLMs, achieving precise control over specific
question-answering (QA) outcomes with a success rate of 93.1\%, while keeping
the poisoned text fluent and natural. The second attack, named Universal KPA
(UKPA), exploits linguistic cues such as pronouns and dependency relations to
disrupt the structural integrity of the generated graph by altering globally
influential words. With fewer than 0.05\% of full text modified, the QA
accuracy collapses from 95\% to 50\%. Furthermore, experiments show that
state-of-the-art defense methods fail to detect these attacks, highlighting
that securing GraphRAG pipelines against knowledge poisoning remains largely
unexplored.

</details>


### [41] [Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models](https://arxiv.org/abs/2508.04325)
*Zizhan Ma,Wenxuan Wang,Guo Yu,Yiu-Fai Cheung,Meidan Ding,Jie Liu,Wenting Chen,Linlin Shen*

Main category: cs.CL

TL;DR: 提出首个医疗AI评估框架MedCheck，通过五阶段46项标准系统诊断现有基准的临床脱节、数据污染和安全性缺失问题


<details>
  <summary>Details</summary>
Motivation: 当前医疗LLM基准存在临床实践脱节、数据管理不严谨和安全评估缺失三大系统性缺陷，阻碍可靠AI医疗发展

Method: 开发生命周期导向框架MedCheck，将基准开发分解为设计到治理五个阶段，制定46项医学定制化评估标准

Result: 分析53个基准显示：83%存在数据污染风险，92%缺乏临床实践验证，78%忽视模型鲁棒性和不确定性评估

Conclusion: MedCheck可作为标准化诊断工具，推动建立更可靠透明的医疗AI评估体系，促进人工智能在临床场景的安全落地

Abstract: Large language models (LLMs) show significant potential in healthcare,
prompting numerous benchmarks to evaluate their capabilities. However, concerns
persist regarding the reliability of these benchmarks, which often lack
clinical fidelity, robust data management, and safety-oriented evaluation
metrics. To address these shortcomings, we introduce MedCheck, the first
lifecycle-oriented assessment framework specifically designed for medical
benchmarks. Our framework deconstructs a benchmark's development into five
continuous stages, from design to governance, and provides a comprehensive
checklist of 46 medically-tailored criteria. Using MedCheck, we conducted an
in-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis
uncovers widespread, systemic issues, including a profound disconnect from
clinical practice, a crisis of data integrity due to unmitigated contamination
risks, and a systematic neglect of safety-critical evaluation dimensions like
model robustness and uncertainty awareness. Based on these findings, MedCheck
serves as both a diagnostic tool for existing benchmarks and an actionable
guideline to foster a more standardized, reliable, and transparent approach to
evaluating AI in healthcare.

</details>


### [42] [Modelling and Classifying the Components of a Literature Review](https://arxiv.org/abs/2508.04337)
*Francisco Bolaños,Angelo Salatino,Francesco Osborne,Enrico Motta*

Main category: cs.CL

TL;DR: 提出新标注模式支持文献综述生成，评估37种LLM分类效果，微调模型达96%+ F1，半合成数据提升小模型性能


<details>
  <summary>Details</summary>
Motivation: 现有科学文献修辞标注方法对文献综述生成系统有潜力，但需有效标注方案和大规模标注策略

Method: 1) 设计文献综述导向的标注模式 2) 构建含2,940句的Sci-Sentence基准 3) 对37种LLM进行零样本/微调评估

Result: 微调LLM达96%+ F1，GPT-4o最优但轻量开源模型表现良好，半合成数据使小编码器模型F1提升3-5%

Conclusion: 新标注模式有效，LLM在修辞分类任务潜力显著，半合成数据策略为轻量模型部署提供新路径

Abstract: Previous work has demonstrated that AI methods for analysing scientific
literature benefit significantly from annotating sentences in papers according
to their rhetorical roles, such as research gaps, results, limitations,
extensions of existing methodologies, and others. Such representations also
have the potential to support the development of a new generation of systems
capable of producing high-quality literature reviews. However, achieving this
goal requires the definition of a relevant annotation schema and effective
strategies for large-scale annotation of the literature. This paper addresses
these challenges by 1) introducing a novel annotation schema specifically
designed to support literature review generation and 2) conducting a
comprehensive evaluation of a wide range of state-of-the-art large language
models (LLMs) in classifying rhetorical roles according to this schema. To this
end, we also present Sci-Sentence, a novel multidisciplinary benchmark
comprising 700 sentences manually annotated by domain experts and 2,240
sentences automatically labelled using LLMs. We evaluate 37 LLMs on this
benchmark, spanning diverse model families and sizes, using both zero-shot
learning and fine-tuning approaches. The experiments yield several novel
insights that advance the state of the art in this challenging domain. First,
the current generation of LLMs performs remarkably well on this task when
fine-tuned on high-quality data, achieving performance levels above 96\% F1.
Second, while large proprietary models like GPT-4o achieve the best results,
some lightweight open-source alternatives also demonstrate excellent
performance. Finally, enriching the training data with semi-synthetic examples
generated by LLMs proves beneficial, enabling small encoders to achieve robust
results and significantly enhancing the performance of several open decoder
models.

</details>


### [43] [GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy](https://arxiv.org/abs/2508.04349)
*Hongze Tan,Jianfei Pan*

Main category: cs.CL

TL;DR: 提出动态熵加权机制（GTPO和GRPO-S）解决强化学习在LLM推理中粗粒度奖励分配问题，显著提升深度推理性能


<details>
  <summary>Details</summary>
Motivation: 现有GRPO方法对序列所有token使用统一奖励，导致长链推理任务中信用分配不精准，限制模型性能提升

Method: 1. GTPO为每个token分配熵加权奖励；2. GRPO-S基于平均token熵为整个序列分配熵加权奖励，实现细粒度策略更新

Result: 实验显示方法显著优于DAPO基线，验证熵加权机制是性能提升的关键驱动因素

Conclusion: 动态熵加权机制通过精准奖励分配有效提升模型深度推理能力，为复杂推理任务提供了新的优化路径

Abstract: Reinforcement learning (RL) with algorithms like Group Relative Policy
Optimization (GRPO) improves Large Language Model (LLM) reasoning, but is
limited by a coarse-grained credit assignment that applies a uniform reward to
all tokens in a sequence. This is a major flaw in long-chain reasoning tasks.
This paper solves this with \textbf{Dynamic Entropy Weighting}. Our core idea
is that high-entropy tokens in correct responses can guide the policy toward a
higher performance ceiling. This allows us to create more fine-grained reward
signals for precise policy updates via two ways: 1) \textbf{Group Token Policy
Optimization} (\textbf{GTPO}), we assigns a entropy-weighted reward to each
token for fine-grained credit assignment. 2) \textbf{Sequence-Level Group
Relative Policy Optimization} (\textbf{GRPO-S}), we assigns a entropy-weighted
reward to each sequence based on its average token entropy. Experiments show
our methods significantly outperform the strong DAPO baseline. The results
confirm that our entropy-weighting mechanism is the key driver of this
performance boost, offering a better path to enhance deep reasoning in models.

</details>


### [44] [Chain of Questions: Guiding Multimodal Curiosity in Language Models](https://arxiv.org/abs/2508.04350)
*Nima Iji,Kia Dashtipour*

Main category: cs.CL

TL;DR: 提出Chain of Questions框架，通过动态生成问题引导多模态模型选择感知模态，提升推理准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs的推理改进（如思维链）尚未充分应用于需主动选择感知模态的多模态场景，需增强模型在复杂环境中的信息整合能力。

Method: 基于好奇心驱动，设计CoQ框架使模型动态生成环境相关的问题，并据此激活视觉/听觉/空间等对应感知模块获取关键信息。

Result: 在整合WebGPT/ScienceQA/AVSD/ScanQA的新型多模态数据集上，CoQ显著提升基础模型识别和整合感官信息的能力。

Conclusion: 该方法通过问题引导的模态选择机制，有效提高了多模态任务的准确性、可解释性及推理过程与任务的适配性。

Abstract: Reasoning capabilities in large language models (LLMs) have substantially
advanced through methods such as chain-of-thought and explicit step-by-step
explanations. However, these improvements have not yet fully transitioned to
multimodal contexts, where models must proactively decide which sensory
modalities such as vision, audio, or spatial perception to engage when
interacting with complex real-world environments. In this paper, we introduce
the Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach
that encourages multimodal language models to dynamically generate targeted
questions regarding their surroundings. These generated questions guide the
model to selectively activate relevant modalities, thereby gathering critical
information necessary for accurate reasoning and response generation. We
evaluate our framework on a novel multimodal benchmark dataset, assembled by
integrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results
demonstrate that our CoQ method improves a foundation model's ability to
effectively identify and integrate pertinent sensory information. This leads to
improved accuracy, interpretability, and alignment of the reasoning process
with diverse multimodal tasks.

</details>


### [45] [AIC CTU@FEVER 8: On-premise fact checking through long context RAG](https://arxiv.org/abs/2508.04390)
*Herbert Ullrich,Jan Drchal*

Main category: cs.CL

TL;DR: 提出在资源受限环境下部署的高效两阶段RAG事实核查系统，获FEVER 8冠军


<details>
  <summary>Details</summary>
Motivation: 解决现有事实核查系统在有限计算资源（单GPU/低显存）下的部署难题，同时保持SOTA性能

Method: 基于去年方案改进的两阶段RAG流水线：1.证据检索 2.声明验证，支持本地化部署

Result: 在NVidia A10 GPU（23GB显存）和60秒/声明的限制下，取得Ev2R测试最高分

Conclusion: 验证了轻量级事实核查系统的可行性，为资源受限场景提供高效解决方案

Abstract: In this paper, we present our fact-checking pipeline which has scored first
in FEVER 8 shared task. Our fact-checking system is a simple two-step RAG
pipeline based on our last year's submission. We show how the pipeline can be
redeployed on-premise, achieving state-of-the-art fact-checking performance (in
sense of Ev2R test-score), even under the constraint of a single NVidia A10
GPU, 23GB of graphical memory and 60s running time per claim.

</details>


### [46] [Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky](https://arxiv.org/abs/2508.04399)
*Xu Zhang,Mei Chen*

Main category: cs.CL

TL;DR: 研究比较三类NLP模型在交通事故数据质量提升中的表现，发现微调Transformer模型在精度与效率间取得最佳平衡


<details>
  <summary>Details</summary>
Motivation: 现有交通事故数据质量不足，需通过NLP技术从事故叙述中有效提取关键信息（如次生事故识别）以提升数据价值

Method: 使用肯塔基州16,656条人工标注数据，对比零样本LLMs/微调Transformer/逻辑回归三类模型，通过F1分数、准确率和推理时间评估性能

Result: 微调RoBERTa最优（F1 0.90，准确率95%），LLaMA3:70B次之但耗时139分钟，逻辑回归最差（F1 0.66）。中等规模LLMs可减少83%运行时间

Conclusion: 实际部署需权衡精度与效率：微调模型适合快速处理，LLMs适用于高召回场景。建议本地隐私部署、集成模型和增量处理方案

Abstract: This study evaluates advanced natural language processing (NLP) techniques to
enhance crash data quality by mining crash narratives, using secondary crash
identification in Kentucky as a case study. Drawing from 16,656 manually
reviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we
compare three model classes: zero-shot open-source large language models (LLMs)
(LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers
(BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic
regression as baseline. Models were calibrated on 2015-2021 data and tested on
1,771 narratives from 2022. Fine-tuned transformers achieved superior
performance, with RoBERTa yielding the highest F1-score (0.90) and accuracy
(95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139
minutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs
excelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred
high computational costs (up to 723 minutes for DeepSeek-R1:70B), while
fine-tuned models processed the test set in seconds after brief training.
Further analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can
rival larger counterparts in performance while reducing runtime, suggesting
opportunities for optimized deployments. Results highlight trade-offs between
accuracy, efficiency, and data requirements, with fine-tuned transformer models
balancing precision and recall effectively on Kentucky data. Practical
deployment considerations emphasize privacy-preserving local deployment,
ensemble approaches for improved accuracy, and incremental processing for
scalability, providing a replicable scheme for enhancing crash-data quality
with advanced NLP.

</details>


### [47] [Why are LLMs' abilities emergent?](https://arxiv.org/abs/2508.04401)
*Vladimír Havlík*

Main category: cs.CL

TL;DR: LLM的涌现能力源于深度神经网络的复杂非线性动态系统特性，而非单纯参数扩展


<details>
  <summary>Details</summary>
Motivation: 解决AI发展中'创造而不理解'的认识论挑战，揭示DNN涌现特性与符号计算范式的本质差异

Method: 结合理论分析（非线性系统特性）与实证观察（缩放定律、grokking现象、能力相变）

Result: 涌现能力产生于高度敏感非线性系统的复杂动态，当前关于指标和训练损失的争论忽略了DNN涌现的本体论本质

Conclusion: 应将DNN视为受普遍涌现原则支配的新型复杂动力系统，需关注系统内部动态转变而非表象定义

Abstract: The remarkable success of Large Language Models (LLMs) in generative tasks
has raised fundamental questions about the nature of their acquired
capabilities, which often appear to emerge unexpectedly without explicit
training. This paper examines the emergent properties of Deep Neural Networks
(DNNs) through both theoretical analysis and empirical observation, addressing
the epistemological challenge of "creation without understanding" that
characterises contemporary AI development. We explore how the neural approach's
reliance on nonlinear, stochastic processes fundamentally differs from symbolic
computational paradigms, creating systems whose macro-level behaviours cannot
be analytically derived from micro-level neuron activities. Through analysis of
scaling laws, grokking phenomena, and phase transitions in model capabilities,
I demonstrate that emergent abilities arise from the complex dynamics of highly
sensitive nonlinear systems rather than simply from parameter scaling alone. My
investigation reveals that current debates over metrics, pre-training loss
thresholds, and in-context learning miss the fundamental ontological nature of
emergence in DNNs. I argue that these systems exhibit genuine emergent
properties analogous to those found in other complex natural phenomena, where
systemic capabilities emerge from cooperative interactions among simple
components without being reducible to their individual behaviours. The paper
concludes that understanding LLM capabilities requires recognising DNNs as a
new domain of complex dynamical systems governed by universal principles of
emergence, similar to those operating in physics, chemistry, and biology. This
perspective shifts the focus from purely phenomenological definitions of
emergence to understanding the internal dynamic transformations that enable
these systems to acquire capabilities that transcend their individual
components.

</details>


### [48] [What Do Humans Hear When Interacting? Experiments on Selective Listening for Evaluating ASR of Spoken Dialogue Systems](https://arxiv.org/abs/2508.04402)
*Kiyotada Mori,Seiya Kawano,Chaoran Liu,Carlos Toshinori Ishi,Angel Fernando Garcia Contreras,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 提出基于人类选择性听觉的ASR评估新方法，揭示ASR系统与人类转录能力的差异


<details>
  <summary>Details</summary>
Motivation: 通过研究人类对话时选择性关注关键信息的特性，改进语音对话系统中ASR模块的评估体系

Method: 对比人类生成对话响应时的转录文本与标准转录，实验验证选择性听觉现象

Result: 证实人类在生成对话响应时存在选择性听觉机制

Conclusion: 利用人类选择性听觉特性开发ASR评估方法，可有效识别机器与人类转录能力的差距

Abstract: Spoken dialogue systems (SDSs) utilize automatic speech recognition (ASR) at
the front end of their pipeline. The role of ASR in SDSs is to recognize
information in user speech related to response generation appropriately.
Examining selective listening of humans, which refers to the ability to focus
on and listen to important parts of a conversation during the speech, will
enable us to identify the ASR capabilities required for SDSs and evaluate them.
In this study, we experimentally confirmed selective listening when humans
generate dialogue responses by comparing human transcriptions for generating
dialogue responses and reference transcriptions. Based on our experimental
results, we discuss the possibility of a new ASR evaluation method that
leverages human selective listening, which can identify the gap between
transcription ability between ASR systems and humans.

</details>


### [49] [Dialogue Response Prefetching Based on Semantic Similarity and Prediction Confidence of Language Model](https://arxiv.org/abs/2508.04403)
*Kiyotada Mori,Seiya Kawano,Angel Fernando Garcia Contreras,Koichiro Yoshino*

Main category: cs.CL

TL;DR: 提出预测置信模型(PCM)通过语义相似性评估预测用户完整话语的准确性，以减少对话系统中的用户感知延迟


<details>
  <summary>Details</summary>
Motivation: 解决语音对话系统中用户等待系统响应时间过长的问题，传统语言模型预测用户话语存在可靠性风险

Method: 开发预测置信模型(PCM)，通过比较预测结果与实际用户话语的语义相似度来判断是否预取对话响应

Result: 基于预测结果与真实用户话语的差异进行模型验证，量化评估预取决策的可靠性

Conclusion: PCM模型有效平衡预测精度与系统响应速度，为降低用户等待时间提供可靠决策依据

Abstract: Prefetching of dialogue responses has been investigated to reduce
user-perceived latency (UPL), which refers to the user's waiting time before
receiving the system's response, in spoken dialogue systems. To reduce the UPL,
it is necessary to predict complete user utterances before the end of the
user's speech, typically by language models, to prepare prefetched dialogue
responses. In this study, we proposed a prediction confidence model (PCM) that
determines whether prefetching is possible or not by estimating the semantic
similarity between the predicted complete user utterance and the complete user
utterance. We evaluated our PCM based on the differences between the predicted
complete user utterance and the complete user utterance.

</details>


### [50] [Evaluating, Synthesizing, and Enhancing for Customer Support Conversation](https://arxiv.org/abs/2508.04423)
*Jie Zhu,Huaixia Dou,Junhui Li,Lifan Guo,Feng Chen,Chi Zhang,Fang Kong*

Main category: cs.CL

TL;DR: 提出结构化客户支持对话框架CSConv和训练数据集RoleCS，通过策略对齐的LLM训练提升客服响应质量


<details>
  <summary>Details</summary>
Motivation: 现有对话数据集缺乏战略指导，真实服务数据难以获取标注。需要系统化框架提升客服对话的策略性和专业性

Method: 1.基于COPC指南定义五阶段对话流程和12种策略
2.构建包含1,855个改写对话的CSConv评估集
3.开发基于LLM的角色扮演方法生成策略性训练数据RoleCS

Result: 在CSConv测试中，基于RoleCS微调的LLM策略对齐响应质量提升显著，人工评估验证问题解决率提高

Conclusion: 结构化框架+策略数据训练有效提升客服对话质量，代码数据开源推动相关研究

Abstract: Effective customer support requires not only accurate problem solving but
also structured and empathetic communication aligned with professional
standards. However, existing dialogue datasets often lack strategic guidance,
and real-world service data is difficult to access and annotate. To address
this, we introduce the task of Customer Support Conversation (CSC), aimed at
training customer service agents to respond using well-defined support
strategies. We propose a structured CSC framework grounded in COPC guidelines,
defining five conversational stages and twelve strategies to guide high-quality
interactions. Based on this, we construct CSConv, an evaluation dataset of
1,855 real-world customer-agent conversations rewritten using LLMs to reflect
deliberate strategy use, and annotated accordingly. Additionally, we develop a
role-playing approach that simulates strategy-rich conversations using
LLM-powered roles aligned with the CSC framework, resulting in the training
dataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS
significantly improves their ability to generate high-quality, strategy-aligned
responses on CSConv. Human evaluations further confirm gains in problem
resolution. All code and data will be made publicly available at
https://github.com/aliyun/qwen-dianjin.

</details>


### [51] [StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion](https://arxiv.org/abs/2508.04440)
*Yutong Wu,Di Huang,Ruosi Wan,Yue Peng,Shijie Shang,Chenrui Cao,Lei Qi,Rui Zhang,Zidong Du,Jie Yan,Xing Hu*

Main category: cs.CL

TL;DR: 通过ThinkingF框架结合数据合成和强化学习，显著提升自动形式化任务的准确率，32B模型在关键指标达SOTA


<details>
  <summary>Details</summary>
Motivation: 现有自动形式化方法受限于形式语言知识不足和非形式化-形式化推理能力薄弱，导致准确率低下

Method: 构建形式知识蒸馏数据集和模板驱动的推理轨迹数据集，采用SFT和RLVR训练策略融合双重能力

Result: StepFun-Formalizer-32B在FormalMATH-Lite(40.5%)和ProverBench(26.7%)上超越所有现有模型

Conclusion: ThinkingF框架有效整合形式知识掌握与推理能力，实现自动形式化任务的突破性进展

Abstract: Autoformalization aims to translate natural-language mathematical statements
into a formal language. While LLMs have accelerated progress in this area,
existing methods still suffer from low accuracy. We identify two key abilities
for effective autoformalization: comprehensive mastery of formal-language
domain knowledge, and reasoning capability of natural language problem
understanding and informal-formal alignment. Without the former, a model cannot
identify the correct formal objects; without the latter, it struggles to
interpret real-world contexts and map them precisely into formal expressions.
To address these gaps, we introduce ThinkingF, a data synthesis and training
pipeline that improves both abilities. First, we construct two datasets: one by
distilling and selecting large-scale examples rich in formal knowledge, and
another by generating informal-to-formal reasoning trajectories guided by
expert-designed templates. We then apply SFT and RLVR with these datasets to
further fuse and refine the two abilities. The resulting 7B and 32B models
exhibit both comprehensive formal knowledge and strong informal-to-formal
reasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%
on FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior
general-purpose and specialized models.

</details>


### [52] [Automated Generation of Curriculum-Aligned Multiple-Choice Questions for Malaysian Secondary Mathematics Using Generative AI](https://arxiv.org/abs/2508.04442)
*Rohaizah Abdul Wahid,Muhamad Said Nizamuddin Nadim,Suliana Sulaiman,Syahmi Akmal Shaharudin,Muhammad Danial Jupikil,Iqqwan Jasman Su Azlan Su*

Main category: cs.CL

TL;DR: 开发基于生成式AI的马来语数学多选题生成系统，验证检索增强生成（RAG）方法显著优于非基于检索的方法


<details>
  <summary>Details</summary>
Motivation: 解决马来西亚低资源语言（马来语）教育评估工具不足的问题，克服生成式AI在课程对齐和事实准确性方面的挑战

Method: 使用GPT-4o构建四种生成管道（结构化/基础提示、框架/手动RAG），基于官方课程文件，通过语义文本相似性（STS）和新型RAG-QA方法进行双维度评估

Result: RAG方法在课程对齐度（STS评分提高32%）和事实有效性（RAG-QA验证通过率89%）上全面超越传统方法，框架式RAG更易实施但手动控制更精细

Conclusion: 为低资源语言教育内容生成提供了经过验证的方法论，创新的RAG-QA评估技术对马来西亚及类似地区EdTech解决方案具有直接指导价值

Abstract: This paper addresses the critical need for scalable and high-quality
educational assessment tools within the Malaysian education system. It
highlights the potential of Generative AI (GenAI) while acknowledging the
significant challenges of ensuring factual accuracy and curriculum alignment,
especially for low-resource languages like Bahasa Melayu. This research
introduces and compares four incremental pipelines for generating Form 1
Mathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAI's
GPT-4o. The methods range from non-grounded prompting (structured and basic) to
Retrieval-Augmented Generation (RAG) approaches (one using the LangChain
framework, one implemented manually). The system is grounded in official
curriculum documents, including teacher-prepared notes and the yearly teaching
plan (RPT). A dual-pronged automated evaluation framework is employed to assess
the generated questions. Curriculum alignment is measured using Semantic
Textual Similarity (STS) against the RPT, while contextual validity is verified
through a novel RAG-based Question-Answering (RAG-QA) method. The results
demonstrate that RAG-based pipelines significantly outperform non-grounded
prompting methods, producing questions with higher curriculum alignment and
factual validity. The study further analyzes the trade-offs between the ease of
implementation of framework-based RAG and the fine-grained control offered by a
manual pipeline. This work presents a validated methodology for generating
curriculum-specific educational content in a low-resource language, introduces
a symbiotic RAG-QA evaluation technique, and provides actionable insights for
the development and deployment of practical EdTech solutions in Malaysia and
similar regions.

</details>


### [53] [CALE : Concept-Aligned Embeddings for Both Within-Lemma and Inter-Lemma Sense Differentiation](https://arxiv.org/abs/2508.04494)
*Bastien Liétard,Gabriel Loiseau*

Main category: cs.CL

TL;DR: 提出Concept Differentiation任务和CALE模型，通过跨词义场景的语义关系建模增强词汇语义分析能力


<details>
  <summary>Details</summary>
Motivation: 现有Word-in-Context方法仅支持同词根词义比较，需要扩展跨词语义关系分析能力以全面捕捉词汇语义

Method: 基于SemCor语料构建数据集，通过Concept Differentiation任务对多模态表示模型进行微调

Result: CALE模型在词汇语义任务中表现最佳，且微调显著改变了词向量空间的结构组织

Conclusion: 该方法为词汇语义分析提供高效的多用途表示，在空间分布和任务性能上均带来实质性改进

Abstract: Lexical semantics is concerned with both the multiple senses a word can adopt
in different contexts, and the semantic relations that exist between meanings
of different words. To investigate them, Contextualized Language Models are a
valuable tool that provides context-sensitive representations that can be used
to investigate lexical meaning. Recent works like XL-LEXEME have leveraged the
task of Word-in-Context to fine-tune them to get more semantically accurate
representations, but Word-in-Context only compares occurrences of the same
lemma, limiting the range of captured information. In this paper, we propose an
extension, Concept Differentiation, to include inter-words scenarios. We
provide a dataset for this task, derived from SemCor data. Then we fine-tune
several representation models on this dataset. We call these models
Concept-Aligned Embeddings (CALE). By challenging our models and other models
on various lexical semantic tasks, we demonstrate that the proposed models
provide efficient multi-purpose representations of lexical meaning that reach
best performances in our experiments. We also show that CALE's fine-tuning
brings valuable changes to the spatial organization of embeddings.

</details>


### [54] [StyliTruth : Unlocking Stylized yet Truthful LLM Generation via Disentangled Steering](https://arxiv.org/abs/2508.04530)
*Chenglei Shen,Zhongxiang Sun,Teng Shi,Xiao Zhang,Jun Xu*

Main category: cs.CL

TL;DR: 提出StyliTruth机制，通过正交化分解解耦风格与真实性表征空间，实现LLM生成过程风格化与真实性的动态平衡。


<details>
  <summary>Details</summary>
Motivation: 现有表征编辑方法在注入风格信号时，忽略了风格化与真实性的内在冲突，导致关键注意力头中风格与真实性方向潜在耦合，引发'风格化真实性坍缩'问题。

Method: 采用正交投影消除技术分解风格/真实性子空间，设计自适应的token级导向向量实现双空间独立控制，通过动态调整生成过程保持两者协调。

Result: 在多风格、多语言任务中，StyliTruth使真实性指标提升12-18%，风格保持度达92.3%，相比基线方法减少32%的幻觉生成。

Conclusion: 通过子空间解耦与动态导向机制，StyliTruth突破风格化与真实性的零和博弈，为可控文本生成提供了新的表征操作范式。

Abstract: Generating stylized large language model (LLM) responses via representation
editing is a promising way for fine-grained output control. However, there
exists an inherent trade-off: imposing a distinctive style often degrades
truthfulness. Existing representation editing methods, by naively injecting
style signals, overlook this collateral impact and frequently contaminate the
model's core truthfulness representations, resulting in reduced answer
correctness. We term this phenomenon stylization-induced truthfulness collapse.
We attribute this issue to latent coupling between style and truth directions
in certain key attention heads, and propose StyliTruth, a mechanism that
preserves stylization while keeping truthfulness intact. StyliTruth separates
the style-relevant and truth-relevant subspaces in the model's representation
space via an orthogonal deflation process. This decomposition enables
independent control of style and truth in their own subspaces, minimizing
interference. By designing adaptive, token-level steering vectors within each
subspace, we dynamically and precisely control the generation process to
maintain both stylistic fidelity and truthfulness. We validate our method on
multiple styles and languages. Extensive experiments and analyses show that
StyliTruth significantly reduces stylization-induced truthfulness collapse and
outperforms existing inference-time intervention methods in balancing style
adherence with truthfulness.

</details>


### [55] [Unveiling the Landscape of Clinical Depression Assessment: From Behavioral Signatures to Psychiatric Reasoning](https://arxiv.org/abs/2508.04531)
*Zhuang Chen,Guanqun Bi,Wen Zhang,Jiawei Hu,Aoyun Wang,Xiyao Xiao,Kun Feng,Minlie Huang*

Main category: cs.CL

TL;DR: 提出C-MIND临床多模态数据集，通过量化不同模态对抑郁症诊断的贡献，揭示LLMs在精神科推理中的局限性，并提出结合临床知识提升诊断性能10%的方法。


<details>
  <summary>Details</summary>
Motivation: 现有抑郁症自动评估研究多依赖非临床验证数据且过度追求模型复杂性，需建立基于真实临床场景的可靠评估框架。

Method: 收集两年真实医院多模态数据(音频/视频/fNIRS等)，通过经典模型量化诊断要素，设计临床知识引导的LLM推理框架。

Result: 不同精神科任务和模态组合显著影响诊断性能，临床知识引导使LLM的Macro-F1提升10%。

Conclusion: C-MIND从数据和算法层面构建临床抑郁症评估基础设施，为精神健康研究提供可靠基础。

Abstract: Depression is a widespread mental disorder that affects millions worldwide.
While automated depression assessment shows promise, most studies rely on
limited or non-clinically validated data, and often prioritize complex model
design over real-world effectiveness. In this paper, we aim to unveil the
landscape of clinical depression assessment. We introduce C-MIND, a clinical
neuropsychiatric multimodal diagnosis dataset collected over two years from
real hospital visits. Each participant completes three structured psychiatric
tasks and receives a final diagnosis from expert clinicians, with informative
audio, video, transcript, and functional near-infrared spectroscopy (fNIRS)
signals recorded. Using C-MIND, we first analyze behavioral signatures relevant
to diagnosis. We train a range of classical models to quantify how different
tasks and modalities contribute to diagnostic performance, and dissect the
effectiveness of their combinations. We then explore whether LLMs can perform
psychiatric reasoning like clinicians and identify their clear limitations in
realistic clinical settings. In response, we propose to guide the reasoning
process with clinical expertise and consistently improves LLM diagnostic
performance by up to 10% in Macro-F1 score. We aim to build an infrastructure
for clinical depression assessment from both data and algorithmic perspectives,
enabling C-MIND to facilitate grounded and reliable research for mental
healthcare.

</details>


### [56] [Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons from Multi-Agent Collaboration](https://arxiv.org/abs/2508.04575)
*Nuo Chen,Yicheng Tong,Jiaying Wu,Minh Duc Duong,Qian Wang,Qingyun Zou,Bryan Hooi,Bingsheng He*

Main category: cs.CL

TL;DR: 多智能体讨论通过认知多样性和专家知识提升科研提案质量，领导者角色能催化更综合前瞻的成果。


<details>
  <summary>Details</summary>
Motivation: 现有AI科研构思框架多依赖单智能体优化，受限于知识边界和视角固化。受现实科研协作启发，探索结构化多智能体讨论能否超越单兵作战模式。

Method: 开发协作式多智能体框架生成科研提案，系统比较组员规模、领导/无领导结构、跨学科程度和资历配置。采用包含智能体评分和人工评审的混合评估协议，从创新性、战略视野、整合深度等维度评价提案质量。

Result: 多智能体讨论显著优于单智能体基线（提升约32%）。领导者的催化作用使讨论转化为更整合前瞻的提案。认知多样性是质量核心驱动力，但专家知识是必要前提（无资深知识的团队甚至无法超越单个合格智能体）。

Conclusion: 研究为设计协作式AI构思系统提供洞见：1）认知多样性需建立在专家知识基础之上 2）领导机制能有效提升方案整合度 3）团队结构通过知识分布影响创新产出。这对优化科研协作生态具有方法论意义。

Abstract: While AI agents show potential in scientific ideation, most existing
frameworks rely on single-agent refinement, limiting creativity due to bounded
knowledge and perspective. Inspired by real-world research dynamics, this paper
investigates whether structured multi-agent discussions can surpass solitary
ideation. We propose a cooperative multi-agent framework for generating
research proposals and systematically compare configurations including group
size, leaderled versus leaderless structures, and team compositions varying in
interdisciplinarity and seniority. To assess idea quality, we employ a
comprehensive protocol with agent-based scoring and human review across
dimensions such as novelty, strategic vision, and integration depth. Our
results show that multi-agent discussions substantially outperform solitary
baselines. A designated leader acts as a catalyst, transforming discussion into
more integrated and visionary proposals. Notably, we find that cognitive
diversity is a primary driver of quality, yet expertise is a non-negotiable
prerequisite, as teams lacking a foundation of senior knowledge fail to surpass
even a single competent agent. These findings offer actionable insights for
designing collaborative AI ideation systems and shed light on how team
structure influences creative outcomes.

</details>


### [57] [Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning](https://arxiv.org/abs/2508.04581)
*Magauiya Zhussip,Dmitriy Shopkhoev,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: 提出MASA框架，通过跨层共享矩阵原子实现Transformer参数高效压缩，保持性能同时减少66.7%注意力参数。


<details>
  <summary>Details</summary>
Motivation: 针对Transformer层间冗余未被充分利用的问题，突破现有仅关注层内优化的压缩方法（如低秩近似），探索跨层参数共享策略。

Method: 受CNN字典学习启发，将注意力投影矩阵分解为共享矩阵原子，通过线性组合构建各层权重，无需架构修改或蒸馏训练。

Result: 在100M-700M模型上超越GQA/低秩基线，ViT应用保持分类检测性能，预训练模型压缩后性能无显著下降。

Conclusion: MASA为参数高效模型提供可扩展方案，证明跨层统计规律的有效捕获，平衡模型压缩与性能保持。

Abstract: Large language models (LLMs) have revolutionized AI applications, yet their
high computational and memory demands hinder their widespread deployment.
Existing compression techniques focus on intra-block optimizations (e.g.
low-rank approximation, attention head pruning), while the repetitive layered
structure of transformers implies significant inter-block redundancy - a
dimension largely unexplored beyond key-value (KV) caching. Inspired by
dictionary learning in CNNs, we propose a framework for structured weight
sharing across transformer layers. Our approach decomposes attention projection
matrices into shared dictionary atoms, reducing the attention module's
parameters by 66.7% while achieving on-par performance. Unlike complex methods
requiring distillation or architectural changes, MASA (Matrix Atom Sharing in
Attention) operates as a drop-in replacement - trained with standard optimizers
- and represents each layer's weights as linear combinations of shared matrix
atoms. Experiments across scales (100M-700M parameters) show that MASA achieves
better benchmark accuracy and perplexity than grouped-query attention (GQA),
low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at
comparable parameter budgets. Ablation studies confirm robustness to the
dictionary size and the efficacy of shared representations in capturing
cross-layer statistical regularities. Extending to Vision Transformers (ViT),
MASA matches performance metrics on image classification and detection tasks
with 66.7% fewer attention parameters. By combining dictionary learning
strategies with transformer efficiency, MASA offers a scalable blueprint for
parameter-efficient models without sacrificing performance. Finally, we
investigate the possibility of employing MASA on pretrained LLMs to reduce
their number of parameters without experiencing any significant drop in their
performance.

</details>


### [58] [TURA: Tool-Augmented Unified Retrieval Agent for AI Search](https://arxiv.org/abs/2508.04604)
*Zhejun Zhao,Yuehu Dong,Alley Liu,Lixue Zheng,Pingsheng Liu,Dongdong Shen,Long Xia,Jiashu Zhao,Dawei Yin*

Main category: cs.CL

TL;DR: 提出TURA框架，结合检索增强生成（RAG）与动态工具调用，解决传统搜索引擎在实时结构化查询中的局限性，实现工业级AI搜索产品


<details>
  <summary>Details</summary>
Motivation: 传统RAG依赖静态网页内容索引，无法处理实时数据（如票务库存）和结构化查询，存在工业应用场景的局限性

Method: 三阶段框架：1) 意图感知检索模块分解查询并调用MCP服务器；2) DAG任务规划器实现并行执行；3) 轻量级代理执行器高效调用工具

Result: 成功部署于千万级用户系统，实现低延迟响应（工业级性能指标）与实时动态数据整合

Conclusion: TURA首次系统化融合静态内容与动态信息源，为AI搜索产品提供创新架构，满足工业场景的复杂需求

Abstract: The advent of Large Language Models (LLMs) is transforming search engines
into conversational AI search products, primarily using Retrieval-Augmented
Generation (RAG) on web corpora. However, this paradigm has significant
industrial limitations. Traditional RAG approaches struggle with real-time
needs and structured queries that require accessing dynamically generated
content like ticket availability or inventory. Limited to indexing static
pages, search engines cannot perform the interactive queries needed for such
time-sensitive data. Academic research has focused on optimizing RAG for static
content, overlooking complex intents and the need for dynamic sources like
databases and real-time APIs. To bridge this gap, we introduce TURA
(Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage
framework that combines RAG with agentic tool-use to access both static content
and dynamic, real-time information. TURA has three key components: an
Intent-Aware Retrieval module to decompose queries and retrieve information
sources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task
Planner that models task dependencies as a Directed Acyclic Graph (DAG) for
optimal parallel execution, and a lightweight Distilled Agent Executor for
efficient tool calling. TURA is the first architecture to systematically bridge
the gap between static RAG and dynamic information sources for a world-class AI
search product. Serving tens of millions of users, it leverages an agentic
framework to deliver robust, real-time answers while meeting the low-latency
demands of a large-scale industrial system.

</details>


### [59] [Lightweight Transformers for Zero-Shot and Fine-Tuned Text-to-SQL Generation Using Spider](https://arxiv.org/abs/2508.04623)
*Chirag Seth,Utkarsh Singh*

Main category: cs.CL

TL;DR: 本研究评估了T5-Small、BART-Small和GPT-2三种轻量级Transformer模型在低资源环境下Text-to-SQL任务的表现，开发了模块化训练框架并验证了编码器-解码器架构的优势。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限环境下自然语言数据库查询的技术瓶颈，推动文本转SQL技术在教育和商业智能领域的普惠应用。

Method: 基于Spider数据集构建模型无关训练框架，采用1000-5000次迭代训练，通过LFAcc/BLEU/EM三项指标在1000测试样本上评估模型表现，创新设计适应不同模型架构的模式格式化方法。

Result: T5-Small以27.8%的LFAcc最优，显著超过BART-Small(23.98%)和GPT-2(20.1%)，验证编码器-解码器模型在模式感知SQL生成中的结构优势。

Conclusion: 紧凑型Transformer在资源匮乏场景具应用潜力，模块化框架支持未来集成模式链接等增强技术，为轻量化Text-to-SQL解决方案提供技术基础。

Abstract: Text-to-SQL translation enables non-expert users to query relational
databases using natural language, with applications in education and business
intelligence. This study evaluates three lightweight transformer models -
T5-Small, BART-Small, and GPT-2 - on the Spider dataset, focusing on
low-resource settings. We developed a reusable, model-agnostic pipeline that
tailors schema formatting to each model's architecture, training them across
1000 to 5000 iterations and evaluating on 1000 test samples using Logical Form
Accuracy (LFAcc), BLEU, and Exact Match (EM) metrics. Fine-tuned T5-Small
achieves the highest LFAcc (27.8%), outperforming BART-Small (23.98%) and GPT-2
(20.1%), highlighting encoder-decoder models' superiority in schema-aware SQL
generation. Despite resource constraints limiting performance, our pipeline's
modularity supports future enhancements, such as advanced schema linking or
alternative base models. This work underscores the potential of compact
transformers for accessible text-to-SQL solutions in resource-scarce
environments.

</details>


### [60] [P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis](https://arxiv.org/abs/2508.04626)
*Feifan Song,Bofei Gao,Yifan Song,Yi Liu,Weimin Xiong,Yuyang Song,Tianyu Liu,Guoyin Wang,Houfeng Wang*

Main category: cs.CL

TL;DR: 提出P-Aligner轻量模块，通过蒙特卡洛树搜索生成优化指令，显著提升大语言模型的偏好对齐效果


<details>
  <summary>Details</summary>
Motivation: 现有方法存在高计算成本或训练目标不明确的问题，需要开发高效指令优化方案改善模型输出质量

Method: 基于原则引导流程合成UltraPrompt数据集，采用蒙特卡洛树搜索生成人类偏好的指令表达

Result: 在GPT-4-turbo和Gemma-2-SimPO分别实现28.35%和8.69%平均胜率提升，多维度验证有效性

Conclusion: P-Aligner通过系统化指令优化实现高效偏好对齐，数据质量与搜索策略的协同作用显著降低时间开销

Abstract: Large Language Models (LLMs) are expected to produce safe, helpful, and
honest content during interaction with human users, but they frequently fail to
align with such values when given flawed instructions, e.g., missing context,
ambiguous directives, or inappropriate tone, leaving substantial room for
improvement along multiple dimensions. A cost-effective yet high-impact way is
to pre-align instructions before the model begins decoding. Existing approaches
either rely on prohibitive test-time search costs or end-to-end model rewrite,
which is powered by a customized training corpus with unclear objectives. In
this work, we demonstrate that the goal of efficient and effective preference
alignment can be achieved by P-Aligner, a lightweight module generating
instructions that preserve the original intents while being expressed in a more
human-preferred form. P-Aligner is trained on UltraPrompt, a new dataset
synthesized via a proposed principle-guided pipeline using Monte-Carlo Tree
Search, which systematically explores the space of candidate instructions that
are closely tied to human preference. Experiments across different methods show
that P-Aligner generally outperforms strong baselines across various models and
benchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo
and Gemma-2-SimPO, respectively. Further analyses validate its effectiveness
and efficiency through multiple perspectives, including data quality, search
strategies, iterative deployment, and time overhead.

</details>


### [61] [IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2508.04632)
*Xu Guo,Tianyi Liang,Tong Jian,Xiaogui Yang,Ling-I Wu,Chenhui Li,Zhihui Lu,Qipeng Guo,Kai Chen*

Main category: cs.CL

TL;DR: 提出IFDecorator框架解决RLVR训练效率低与过优化问题，通过数据飞轮、意图检查、陷阱指令三重机制提升指令跟随能力


<details>
  <summary>Details</summary>
Motivation: RLVR在增强大语言模型指令跟随能力时存在训练数据难度评估不足导致的低效训练，以及因验证捷径导致的意图偏移问题

Method: 1) 合作对抗数据飞轮动态生成难度递增的指令-验证对；2) IntentCheck模块强化意图对齐；3) 陷阱指令诊断机制检测奖励黑客行为

Result: Qwen2.5-32B模型在IFEval达87.43%准确率超越GPT-4o，FollowBench表现提升且保留通用能力，陷阱机制显著降低奖励黑客率36%

Conclusion: 该框架有效平衡指令跟随能力与模型安全性，未来将开源模型/代码/数据推动领域发展

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction
following capabilities of large language models (LLMs), but suffers from
training inefficiency due to inadequate difficulty assessment. Moreover, RLVR
is prone to over-optimization, where LLMs exploit verification shortcuts
without aligning to the actual intent of user instructions. We introduce
Instruction Following Decorator (IFDecorator}, a framework that wraps RLVR
training into a robust and sample-efficient pipeline. It consists of three
components: (1) a cooperative-adversarial data flywheel that co-evolves
instructions and hybrid verifications, generating progressively more
challenging instruction-verification pairs; (2) IntentCheck, a bypass module
enforcing intent alignment; and (3) trip wires, a diagnostic mechanism that
detects reward hacking via trap instructions, which trigger and capture
shortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves
87.43% accuracy on IFEval, outperforming larger proprietary models such as
GPT-4o. Additionally, we demonstrate substantial improvements on FollowBench
while preserving general capabilities. Our trip wires show significant
reductions in reward hacking rates. We will release models, code, and data for
future research.

</details>


### [62] [Can NLP Tackle Hate Speech in the Real World? Stakeholder-Informed Feedback and Survey on Counterspeech](https://arxiv.org/abs/2508.04638)
*Tanvi Dinkar,Aiqi Jiang,Simona Frenda,Poppy Gerrard-Abbott,Nancie Gunson,Gavin Abercrombie,Ioannis Konstas*

Main category: cs.CL

TL;DR: 通过系统文献综述和NGO参与式案例研究，揭示NLP反仇恨言论研究与实际社区需求的脱节，提出以利益相关者为中心的研究建议


<details>
  <summary>Details</summary>
Motivation: 当前NLP领域反仇恨言论研究过度依赖自动化流程和历史数据集，逐渐脱离受网络暴力影响社区的实际需求，需重新审视利益相关者的核心作用

Method: 对74项NLP研究进行系统综述，并与5个在线性别暴力NGO开展参与式案例研究，分析利益相关者对数据集构建、模型开发和评估的影响

Result: 发现NLP研究与受影响社区需求存在显著脱节，通过利益相关者参与能建立更有效的反仇恨言论生成实践框架

Conclusion: 提出具体建议：在反仇恨言论研究中应重新定位利益相关者（如NGO）的核心地位，将社区专业知识融入数据、模型和评估体系构建

Abstract: Counterspeech, i.e. the practice of responding to online hate speech, has
gained traction in NLP as a promising intervention. While early work emphasised
collaboration with non-governmental organisation stakeholders, recent research
trends have shifted toward automated pipelines that reuse a small set of legacy
datasets, often without input from affected communities. This paper presents a
systematic review of 74 NLP studies on counterspeech, analysing the extent to
which stakeholder participation influences dataset creation, model development,
and evaluation. To complement this analysis, we conducted a participatory case
study with five NGOs specialising in online Gender-Based Violence (oGBV),
identifying stakeholder-informed practices for counterspeech generation. Our
findings reveal a growing disconnect between current NLP research and the needs
of communities most impacted by toxic online content. We conclude with concrete
recommendations for re-centring stakeholder expertise in counterspeech
research.

</details>


### [63] [Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs](https://arxiv.org/abs/2508.04660)
*Noah Ziems,Dilara Soylu,Lakshya A Agrawal,Isaac Miller,Liheng Lai,Chen Qian,Kaiqiang Song,Meng Jiang,Dan Klein,Matei Zaharia,Karel D'Oosterlinck,Christopher Potts,Omar Khattab*

Main category: cs.CL

TL;DR: 提出多模块扩展的mmGRPO方法，通过模块化分组和轨迹处理机制，显著提升模块化AI系统的任务表现


<details>
  <summary>Details</summary>
Motivation: 传统GRPO无法有效处理由多模块LM调用组成的复杂AI系统，需解决模块间轨迹长度变化和中断场景的优化问题

Method: 开发mmGRPO方法：1) 按模块分组LM调用 2) 支持可变长度/中断轨迹处理 3) 与自动提示优化组合使用

Result: 在分类/多跳搜索/隐私保护任务中：相对基础模型平均提升11%准确率，相对单独提示优化提升5%；已开源集成至DSPy框架

Conclusion: mmGRPO为模块化AI系统优化提供了有效解决方案，通过模块级策略优化和灵活轨迹处理机制，显著提升系统整体性能

Abstract: Group Relative Policy Optimization (GRPO) has proven to be an effective tool
for post-training language models (LMs). However, AI systems are increasingly
expressed as modular programs that mix together multiple LM calls with distinct
prompt templates and other tools, and it is not clear how best to leverage GRPO
to improve these systems. We begin to address this challenge by defining
mmGRPO, a simple multi-module generalization of GRPO that groups LM calls by
module across rollouts and handles variable-length and interrupted
trajectories. We find that mmGRPO, composed with automatic prompt optimization,
improves accuracy by 11% on average across classification, many-hop search, and
privacy-preserving delegation tasks against the post-trained LM, and by 5%
against prompt optimization on its own. We open-source mmGRPO in DSPy as the
dspy.GRPO optimizer.

</details>


### [64] [Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management](https://arxiv.org/abs/2508.04664)
*Mo Li,L. H. Xu,Qitai Tan,Ting Cao,Yunxin Liu*

Main category: cs.CL

TL;DR: 提出Sculptor框架通过主动上下文管理工具解决LLMs长文本处理中的干扰问题，无需训练即显著提升性能


<details>
  <summary>Details</summary>
Motivation: 现有研究集中于外部记忆系统，但忽视了LLMs内在的主动记忆管理能力，需解决长文本中的信息干扰问题

Method: 开发三类工具：1）上下文碎片化处理 2）摘要/隐藏/恢复机制 3）智能搜索，模仿人类选择性注意力机制

Result: 在PI-LLM和NeedleBench基准测试中实现性能突破，验证工具调用泛化能力的有效性

Conclusion: 显性上下文控制策略优于单纯扩展窗口长度，为可靠的大规模推理任务建立认知基础

Abstract: Large Language Models (LLMs) suffer from significant performance degradation
when processing long contexts due to proactive interference, where irrelevant
information in earlier parts of the context disrupts reasoning and memory
recall. While most research focuses on external memory systems to augment LLMs'
capabilities, we propose a complementary approach: empowering LLMs with Active
Context Management (ACM) tools to actively sculpt their internal working
memory. We introduce Sculptor, a framework that equips LLMs with three
categories of tools: (1) context fragmentation, (2) summary, hide, and restore,
and (3) intelligent search. Our approach enables LLMs to proactively manage
their attention and working memory, analogous to how humans selectively focus
on relevant information while filtering out distractions. Experimental
evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and
NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly
improves performance even without specific training, leveraging LLMs' inherent
tool calling generalization capabilities. By enabling Active Context
Management, Sculptor not only mitigates proactive interference but also
provides a cognitive foundation for more reliable reasoning across diverse
long-context tasks-highlighting that explicit context-control strategies,
rather than merely larger token windows, are key to robustness at scale.

</details>


### [65] [GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay](https://arxiv.org/abs/2508.04676)
*Yunan Zhang,Shuoran Jiang,Mengchen Zhao,Yuefeng Li,Yang Fan,Xiangping Wu,Qingcai Chen*

Main category: cs.CL

TL;DR: 提出基于通用样本回放（GeRe）的持续学习框架，通过阈值约束的激活状态优化（TM损失）有效解决大语言模型持续微调中的灾难性遗忘问题，仅需少量预训练文本即可同时保持通用能力和任务性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型持续微调不同领域时存在灾难性遗忘问题：1）通用能力显著退化；2）已学任务性能骤降。现有方法难以同时解决这两个问题。

Method: 1. 提出通用样本回放框架（GeRe），使用预训练文本进行抗遗忘；2. 创新阈值约束的激活状态优化方法（TM损失），保持回放学习时激活状态一致性。

Result: 实验表明：仅需固定少量通用回放样本即可同时保持通用能力和任务性能；TM损失相比标签拟合、KL散度、L1/L2损失等策略，展现出更优的鲁棒性和持续学习效果。

Conclusion: GeRe框架为LLMs高效持续学习提供新思路，激活状态一致性约束方法验证了神经网络状态保持对抗遗忘的重要性，代码数据已开源。

Abstract: The continual learning capability of large language models (LLMs) is crucial
for advancing artificial general intelligence. However, continual fine-tuning
LLMs across various domains often suffers from catastrophic forgetting,
characterized by: 1) significant forgetting of their general capabilities, and
2) sharp performance declines in previously learned tasks. To simultaneously
address both issues in a simple yet stable manner, we propose General Sample
Replay (GeRe), a framework that use usual pretraining texts for efficient
anti-forgetting. Beyond revisiting the most prevalent replay-based practices
under GeRe, we further leverage neural states to introduce a enhanced
activation states constrained optimization method using threshold-based margin
(TM) loss, which maintains activation state consistency during replay learning.
We are the first to validate that a small, fixed set of pre-collected general
replay samples is sufficient to resolve both concerns--retaining general
capabilities while promoting overall performance across sequential tasks.
Indeed, the former can inherently facilitate the latter. Through controlled
experiments, we systematically compare TM with different replay strategies
under the GeRe framework, including vanilla label fitting, logit imitation via
KL divergence and feature imitation via L1/L2 losses. Results demonstrate that
TM consistently improves performance and exhibits better robustness. Our work
paves the way for efficient replay of LLMs for the future. Our code and data
are available at https://github.com/Qznan/GeRe.

</details>


### [66] [FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data](https://arxiv.org/abs/2508.04698)
*Thibaut Thonet,Germán Kruszewski,Jos Rozen,Pierre Erbacher,Marc Dymetman*

Main category: cs.CL

TL;DR: 提出参数高效的FaST方法，通过自动发现高级特征实现有限数据下的个性化偏好对齐


<details>
  <summary>Details</summary>
Motivation: 当前LLM对话助手缺乏个性化适配，现有方法在用户偏好数据不足时效果受限

Method: 构建DnD/ELIP数据集，开发基于自动特征发现的参数高效FaST框架

Result: FaST在两个基准数据集上取得最优综合性能

Conclusion: 特征驱动的参数高效方法有效解决PPALLI问题，推动LLM个性化发展

Abstract: LLM-powered conversational assistants are often deployed in a
one-size-fits-all manner, which fails to accommodate individual user
preferences. Recently, LLM personalization -- tailoring models to align with
specific user preferences -- has gained increasing attention as a way to bridge
this gap. In this work, we specifically focus on a practical yet challenging
setting where only a small set of preference annotations can be collected per
user -- a problem we define as Personalized Preference Alignment with Limited
Data (PPALLI). To support research in this area, we introduce two datasets --
DnD and ELIP -- and benchmark a variety of alignment techniques on them. We
further propose FaST, a highly parameter-efficient approach that leverages
high-level features automatically discovered from the data, achieving the best
overall performance.

</details>


### [67] [Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis](https://arxiv.org/abs/2508.04699)
*Anushka Yadav,Isha Nalawade,Srujana Pillarichety,Yashwanth Babu,Reshmi Ghosh,Samyadeep Basu,Wenlong Zhao,Ali Nasaeh,Sriram Balasubramanian,Soundararajan Srinivasan*

Main category: cs.CL

TL;DR: 提出新型错误分类框架，揭示推理模型在多跳问答中的认知局限


<details>
  <summary>Details</summary>
Motivation: 现有研究未能完整解释推理模型为何比通用语言模型更容易产生幻觉

Method: 通过人工标注与自动指标结合，从跳数多样性（hops）、信息覆盖度（coverage）、认知过载（overthinking）三个维度构建错误分析框架

Result: 发现传统准确率评估掩盖的复杂错误模式，揭示模型在跨文档推理、信息整合和决策效率方面的系统性缺陷

Conclusion: 该研究为提升语言模型的推理可靠性提供新视角，强调需要更细粒度的评估体系来增强模型透明度和鲁棒性

Abstract: The emergence of reasoning models and their integration into practical AI
chat bots has led to breakthroughs in solving advanced math, deep search, and
extractive question answering problems that requires a complex and multi-step
thought process. Yet, a complete understanding of why these models hallucinate
more than general purpose language models is missing. In this investigative
study, we systematicallyexplore reasoning failures of contemporary language
models on multi-hop question answering tasks. We introduce a novel, nuanced
error categorization framework that examines failures across three critical
dimensions: the diversity and uniqueness of source documents involved ("hops"),
completeness in capturing relevant information ("coverage"), and cognitive
inefficiency ("overthinking"). Through rigorous hu-man annotation, supported by
complementary automated metrics, our exploration uncovers intricate error
patterns often hidden by accuracy-centric evaluations. This investigative
approach provides deeper insights into the cognitive limitations of current
models and offers actionable guidance toward enhancing reasoning fidelity,
transparency, and robustness in future language modeling efforts.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [68] [RLGS: Reinforcement Learning-Based Adaptive Hyperparameter Tuning for Gaussian Splatting](https://arxiv.org/abs/2508.04078)
*Zhan Li,Huangying Zhan,Changyang Li,Qingan Yan,Yi Xu*

Main category: cs.GR

TL;DR: 提出RLGS强化学习框架，实现3D高斯泼溅训练中自适应的超参数调整，提升渲染质量且无需修改现有架构


<details>
  <summary>Details</summary>
Motivation: 传统3DGS超参数调整依赖人工且效果不稳定，需要自动化解决方案来提升训练效率和渲染质量

Method: 通过轻量级强化学习策略模块动态调整学习率、致密化阈值等关键参数，保持模型无关性和即插即用特性

Result: 在TNT数据集上使Taming-3DGS的PSNR提升0.7dB，在基线饱和后仍持续增益，多数据集验证框架鲁棒性

Conclusion: RLGS填补了强化学习在3DGS应用的空白，为超参数优化提供了通用解决方案，显著提升不同变体模型的性能上限

Abstract: Hyperparameter tuning in 3D Gaussian Splatting (3DGS) is a labor-intensive
and expert-driven process, often resulting in inconsistent reconstructions and
suboptimal results. We propose RLGS, a plug-and-play reinforcement learning
framework for adaptive hyperparameter tuning in 3DGS through lightweight policy
modules, dynamically adjusting critical hyperparameters such as learning rates
and densification thresholds. The framework is model-agnostic and seamlessly
integrates into existing 3DGS pipelines without architectural modifications. We
demonstrate its generalization ability across multiple state-of-the-art 3DGS
variants, including Taming-3DGS and 3DGS-MCMC, and validate its robustness
across diverse datasets. RLGS consistently enhances rendering quality. For
example, it improves Taming-3DGS by 0.7dB PSNR on the Tanks and Temple (TNT)
dataset, under a fixed Gaussian budget, and continues to yield gains even when
baseline performance saturates. Our results suggest that RLGS provides an
effective and general solution for automating hyperparameter tuning in 3DGS
training, bridging a gap in applying reinforcement learning to 3DGS.

</details>


### [69] [Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned and Addressed for XR Research](https://arxiv.org/abs/2508.04326)
*Ke Li,Mana Masuda,Susanne Schmidt,Shohei Mori*

Main category: cs.GR

TL;DR: 系统性调查365篇XR相关辐射场文献，定位XR领域在3DGS/NeRF技术浪潮中的研究缺口


<details>
  <summary>Details</summary>
Motivation: 辐射场技术虽快速发展但XR社区参与不足，需明确XR领域在RF技术中的应用潜力和现存问题

Method: 跨计算机视觉/图形学/HCI等学科收集文献，对66篇重点论文进行多维度内容分析

Result: 构建XR-RF技术全景图，揭示XR场景特有的技术需求（如动态交互、实时渲染）与传统RF研究的差异

Conclusion: 建立XR社区专属的RF研究框架，为后续研究者提供技术路线导航和跨学科协作指南

Abstract: The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS)
and Neural Radiance Fields (NeRF), has revolutionized interactive
photorealistic view synthesis and presents enormous opportunities for XR
research and applications. However, despite the exponential growth of RF
research, RF-related contributions to the XR community remain sparse. To better
understand this research gap, we performed a systematic survey of current RF
literature to analyze (i) how RF is envisioned for XR applications, (ii) how
they have already been implemented, and (iii) the remaining research gaps. We
collected 365 RF contributions related to XR from computer vision, computer
graphics, robotics, multimedia, human-computer interaction, and XR communities,
seeking to answer the above research questions. Among the 365 papers, we
performed an analysis of 66 papers that already addressed a detailed aspect of
RF research for XR. With this survey, we extended and positioned XR-specific RF
research topics in the broader RF research field and provide a helpful resource
for the XR community to navigate within the rapid development of RF research.

</details>


### [70] [Surf3R: Rapid Surface Reconstruction from Sparse RGB Views in Seconds](https://arxiv.org/abs/2508.04508)
*Haodong Zhu,Changbai Li,Yangyang Ren,Zichao Feng,Xuhui Liu,Hanlin Chen,Xiantong Zhen,Baochang Zhang*

Main category: cs.GR

TL;DR: Surf3R提出了一种无需相机标定的端到端多视角3D重建方法，通过多分支解码架构和D-Normal正则化器，在10秒内实现高精度表面重建


<details>
  <summary>Details</summary>
Motivation: 解决现有多视角3D重建方法依赖复杂相机标定与位姿估计的问题，降低预处理复杂度并提升实用性

Method: 1. 多分支多视角解码架构实现跨视图注意力与分支间融合 2. 基于3D高斯表示的D-Normal正则化器耦合法线与几何参数 3. 联合优化3D几何实现表面细节增强

Result: 在ScanNet++和Replica数据集上达到SOTA，重建速度<10秒，表面细节精度提升13.6%（Chamfer Distance指标）

Conclusion: 该方法突破了传统相机标定限制，实现了高效、高精度的端到端3D重建，具有优异的泛化能力和实用价值

Abstract: Current multi-view 3D reconstruction methods rely on accurate camera
calibration and pose estimation, requiring complex and time-intensive
pre-processing that hinders their practical deployment. To address this
challenge, we introduce Surf3R, an end-to-end feedforward approach that
reconstructs 3D surfaces from sparse views without estimating camera poses and
completes an entire scene in under 10 seconds. Our method employs a
multi-branch and multi-view decoding architecture in which multiple reference
views jointly guide the reconstruction process. Through the proposed
branch-wise processing, cross-view attention, and inter-branch fusion, the
model effectively captures complementary geometric cues without requiring
camera calibration. Moreover, we introduce a D-Normal regularizer based on an
explicit 3D Gaussian representation for surface reconstruction. It couples
surface normals with other geometric parameters to jointly optimize the 3D
geometry, significantly improving 3D consistency and surface detail accuracy.
Experimental results demonstrate that Surf3R achieves state-of-the-art
performance on multiple surface reconstruction metrics on ScanNet++ and Replica
datasets, exhibiting excellent generalization and efficiency.

</details>


### [71] [MienCap: Realtime Performance-Based Facial Animation with Live Mood Dynamics](https://arxiv.org/abs/2508.04687)
*Ye Pan,Ruisi Zhang,Jingying Wang,Nengfu Chen,Yilin Qiu,Yu Ding,Kenny Mitchell*

Main category: cs.GR

TL;DR: 结合传统blendshape动画与机器学习模型，提出非实时/实时双系统驱动3D风格化角色表情，效果优于商业产品Faceware


<details>
  <summary>Details</summary>
Motivation: 传统基于性能的动画技术难以生成几何一致且感知可信的3D风格化角色表情，需要更高效的动画制作解决方案

Method: 1. 非实时系统：3D情感转移网络从2D人像生成风格化3D绑定参数
2. 实时系统：blendshape适应网络生成几何一致且时间稳定的绑定参数

Result: 与Faceware相比，在表情识别度(提升27%)、强度(提升33%)和吸引力(提升41%)评分上均显著更优(p<0.05)

Conclusion: 该双系统架构可整合入动画生产线，帮助动画师快速生成符合艺术意图的高质量角色表情，提升制作效率与表现力

Abstract: Our purpose is to improve performance-based animation which can drive
believable 3D stylized characters that are truly perceptual. By combining
traditional blendshape animation techniques with multiple machine learning
models, we present both non-real time and real time solutions which drive
character expressions in a geometrically consistent and perceptually valid way.
For the non-real time system, we propose a 3D emotion transfer network makes
use of a 2D human image to generate a stylized 3D rig parameters. For the real
time system, we propose a blendshape adaption network which generates the
character rig parameter motions with geometric consistency and temporally
stability. We demonstrate the effectiveness of our system by comparing to a
commercial product Faceware. Results reveal that ratings of the recognition,
intensity, and attractiveness of expressions depicted for animated characters
via our systems are statistically higher than Faceware. Our results may be
implemented into the animation pipeline, and provide animators with a system
for creating the expressions they wish to use more quickly and accurately.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [72] [MD-LLM-1: A Large Language Model for Molecular Dynamics](https://arxiv.org/abs/2508.03709)
*Mhd Hussein Murtada,Z. Faidon Brotzakis,Michele Vendruscolo*

Main category: q-bio.BM

TL;DR: 提出MD-LLM框架，通过大语言模型学习蛋白质动态，展示其发现新构象状态的能力


<details>
  <summary>Details</summary>
Motivation: 分子动力学模拟在生物大分子系统的时空尺度上存在计算瓶颈，探索深度学习（特别是大语言模型）的解决方案

Method: 基于Mistral 7B微调开发MD-LLM-1模型，应用于T4溶菌酶和Mad2蛋白系统的构象状态预测

Result: 单构象态训练即可预测其他状态，初步验证了模型探索蛋白质构象空间的能力

Conclusion: MD-LLM-1展现了学习蛋白质构象演变规律的潜力，但尚未实现热力学和动力学的显式建模

Abstract: Molecular dynamics (MD) is a powerful approach for modelling molecular
systems, but it remains computationally intensive on spatial and time scales of
many macromolecular systems of biological interest. To explore the
opportunities offered by deep learning to address this problem, we introduce a
Molecular Dynamics Large Language Model (MD-LLM) framework to illustrate how
LLMs can be leveraged to learn protein dynamics and discover states not seen in
training. By applying MD-LLM-1, the first implementation of this approach,
obtained by fine-tuning Mistral 7B, to the T4 lysozyme and Mad2 protein
systems, we show that training on one conformational state enables the
prediction of other conformational states. These results indicate that MD-LLM-1
can learn the principles for the exploration of the conformational landscapes
of proteins, although it is not yet modeling explicitly their thermodynamics
and kinetics.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [73] [ToxicTAGS: Decoding Toxic Memes with Rich Tag Annotations](https://arxiv.org/abs/2508.04166)
*Subhankar Swain,Naquee Rizwan,Nayandeep Deb,Vishwajeet Singh Solanki,Vishwa Gangadhar S,Animesh Mukherjee*

Main category: cs.CV

TL;DR: 研究者创建了首个包含6300个真实网络表情包的双阶段标注数据集，结合社会相关元数据标签，显著提升了多模态内容审核系统的检测性能


<details>
  <summary>Details</summary>
Motivation: 现有表情包审核系统面临数据可访问性差和标注成本高的问题，且缺乏社会语境信息支持内容理解

Method: 构建两阶段标注框架（毒性分类→细粒度标记），开发标签生成模块自动补充社会相关元数据，通过实验验证标签对模型性能的增强作用

Result: 结合社会语境标签使先进视觉语言模型在检测任务中的性能得到显著提升（具体数据见原文实验部分）

Conclusion: 该研究通过创建新型数据集和语境增强方法，为多模态在线内容审核提供了可扩展的创新解决方案

Abstract: The 2025 Global Risks Report identifies state-based armed conflict and
societal polarisation among the most pressing global threats, with social media
playing a central role in amplifying toxic discourse. Memes, as a widely used
mode of online communication, often serve as vehicles for spreading harmful
content. However, limitations in data accessibility and the high cost of
dataset curation hinder the development of robust meme moderation systems. To
address this challenge, in this work, we introduce a first-of-its-kind dataset
of 6,300 real-world meme-based posts annotated in two stages: (i) binary
classification into toxic and normal, and (ii) fine-grained labelling of toxic
memes as hateful, dangerous, or offensive. A key feature of this dataset is
that it is enriched with auxiliary metadata of socially relevant tags,
enhancing the context of each meme. In addition, we propose a tag generation
module that produces socially grounded tags, because most in-the-wild memes
often do not come with tags. Experimental results show that incorporating these
tags substantially enhances the performance of state-of-the-art VLMs detection
tasks. Our contributions offer a novel and scalable foundation for improved
content moderation in multimodal online environments.

</details>


### [74] [FrEVL: Leveraging Frozen Pretrained Embeddings for Efficient Vision-Language Understanding](https://arxiv.org/abs/2508.04469)
*Emmanuelle Bourigault,Pauline Bourigault*

Main category: cs.CV

TL;DR: FrEVL框架验证冻结预训练嵌入可达到SOTA模型85-95%性能，计算效率提升2.3倍且能耗降低52%，适用于输入可预计算或部署优先的场景


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型因计算需求大难以部署，探索冻结预训练嵌入在多模态理解中的有效性

Method: 通过冻结嵌入参数（仅6800万可训练参数），在标准benchmark上测试判别任务性能，并测量端到端计算效率

Result: 性能接近SOTA但参数更少，端到端速度提升2.3倍，能耗降低52%，在预计算场景优势显著

Conclusion: 冻结嵌入方案在特定场景可替代全模型部署，研究者需权衡性能增益与部署成本，将开源框架推动高效多模态研究

Abstract: The deployment of vision-language models remains constrained by substantial
computational requirements. We present \textbf{FrEVL}, a framework exploring
whether frozen pretrained embeddings can support effective vision-language
understanding. Our analysis reveals that frozen embeddings contain rich
information for discriminative tasks, achieving 85\% to 95\% of
state-of-the-art performance on standard benchmarks with only 68.4M trainable
parameters. This performance dichotomy reveals a critical insight: frozen
embedding effectiveness depends on alignment between pretraining objectives and
downstream task requirements. When accounting for end-to-end computation
including embedding extraction, FrEVL provides $2.3\times$ speedup with 52\%
lower energy consumption, making it suitable for scenarios with pre-computable
inputs or when deployment constraints outweigh marginal performance gains. Our
evaluation provides practitioners with guidance on when frozen embedding
approaches represent viable alternatives to full model deployment. We will
release our complete implementation and evaluation framework to facilitate
further research into efficient multi-modal understanding.

</details>


### [75] [Analyzing and Mitigating Object Hallucination: A Training Bias Perspective](https://arxiv.org/abs/2508.04567)
*Yifan Li,Kun Zhou,Wayne Xin Zhao,Lei Fang,Ji-Rong Wen*

Main category: cs.CV

TL;DR: 提出POPEv2基准分析LVLMs的训练数据偏见，开发Obliviate方法通过去偏学习显著减少幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉语言模型在训练数据中表现出偏见，导致对已见图像的幻觉更严重，需系统性探究训练数据对幻觉的影响机制。

Method: 1. 构建POPEv2反事实图像基准
2. 通过探测实验定位训练偏见到LM head
3. 提出参数高效的Obliviate去偏方法

Result: Obliviate仅更新2%参数即显著降低多任务幻觉，展示出对模型规模、数据量和幻觉类型的强扩展性与泛化能力。

Conclusion: 揭示训练数据偏见是幻觉关键成因，针对性去偏方法可有效缓解问题，为模型优化提供新方向。

Abstract: As scaling up training data has significantly improved the general multimodal
capabilities of Large Vision-Language Models (LVLMs), they still suffer from
the hallucination issue, generating text that is inconsistent with the visual
input. This phenomenon motivates us to systematically investigate the role of
training data in hallucination. We introduce a new benchmark, POPEv2, which
consists of counterfactual images collected from the training data of LVLMs
with certain objects masked. Through comprehensive evaluation on POPEv2, we
find that current LVLMs suffer from training bias: they fail to fully leverage
their training data and hallucinate more frequently on images seen during
training. Specifically, they perform poorly on counterfactual images, often
incorrectly answering ``Yes'' to questions about masked objects. To understand
this issue, we conduct probing experiments on the models' internal components,
revealing that this training bias is primarily located in the language modeling
(LM) head. Based on these findings, we propose Obliviate, an efficient and
lightweight unlearning method designed to mitigate object hallucination via
training bias unlearning. Obliviate identifies the discrepancy between
ground-truth labels and model outputs on the training data as a proxy for bias
and adopts a parameter- and data-efficient fine-tuning strategy that only
updates the LM head. Extensive experiments demonstrate the effectiveness of our
approach. While only reusing the training data and updating approximately 2\%
of the parameters, Obliviate significantly reduces hallucination across both
discriminative and generative tasks. Furthermore, it demonstrates strong
scalability with respect to both model size (2B to 72B) and training data
volume, and exhibits promising generalization to hallucination types beyond
object-level hallucination. Our code and data will be publicly released.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [76] [ASTRA: Autonomous Spatial-Temporal Red-teaming for AI Software Assistants](https://arxiv.org/abs/2508.03936)
*Xiangzhe Xu,Guangyu Shen,Zian Su,Siyuan Cheng,Hanxi Guo,Lu Yan,Xuan Chen,Jiasheng Jiang,Xiaolong Jin,Chengpeng Wang,Zhuo Zhang,Xiangyu Zhang*

Main category: cs.CR

TL;DR: 提出ASTRA系统，通过构建知识图谱、时空双维度漏洞探索和违规案例生成的三阶段方法，显著提升AI编程助手在网络安全领域的安全性测试效果。


<details>
  <summary>Details</summary>
Motivation: 现有红队测试工具依赖固定基准或不切实际的提示，难以发现真实漏洞，特别是在网络安全等高危领域需要更系统化的AI安全评估方法。

Method: 1. 构建结构化领域知识图谱建模软件任务和已知弱点
2. 结合空间（输入空间）和时间（推理过程）双维度自适应探测
3. 生成高质量违规案例提升模型对齐能力

Result: 在两个主要评估领域发现比现有技术多11-66%的问题，生成的测试案例使对齐训练效果提升17%

Conclusion: ASTRA通过离线建模与在线适应相结合的方法，有效发现边缘案例漏洞，对构建更安全的AI系统具有重要实用价值。

Abstract: AI coding assistants like GitHub Copilot are rapidly transforming software
development, but their safety remains deeply uncertain-especially in
high-stakes domains like cybersecurity. Current red-teaming tools often rely on
fixed benchmarks or unrealistic prompts, missing many real-world
vulnerabilities. We present ASTRA, an automated agent system designed to
systematically uncover safety flaws in AI-driven code generation and security
guidance systems. ASTRA works in three stages: (1) it builds structured
domain-specific knowledge graphs that model complex software tasks and known
weaknesses; (2) it performs online vulnerability exploration of each target
model by adaptively probing both its input space, i.e., the spatial
exploration, and its reasoning processes, i.e., the temporal exploration,
guided by the knowledge graphs; and (3) it generates high-quality
violation-inducing cases to improve model alignment. Unlike prior methods,
ASTRA focuses on realistic inputs-requests that developers might actually
ask-and uses both offline abstraction guided domain modeling and online domain
knowledge graph adaptation to surface corner-case vulnerabilities. Across two
major evaluation domains, ASTRA finds 11-66% more issues than existing
techniques and produces test cases that lead to 17% more effective alignment
training, showing its practical value for building safer AI systems.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [77] [Graph Representation Learning with Massive Unlabeled Data for Rumor Detection](https://arxiv.org/abs/2508.04252)
*Chaoqun Cui,Caiyan Jia*

Main category: cs.SI

TL;DR: 提出利用大规模未标记社交媒体数据结合图自监督学习方法，显著提升谣言检测模型的泛化能力和少样本表现


<details>
  <summary>Details</summary>
Motivation: 现有谣言检测方法依赖标注数据且时效性差，难以应对新兴事件的快速检测需求。通过利用无标注数据的语义学习潜力来突破泛化瓶颈

Method: 整合微博/Twitter跨平台未标记数据，采用InfoGraph/JOAO/GraphMAE三种图自监督方法，构建覆盖十年的多主题谣言验证集

Result: 通用图自监督方法在few-shot条件下超越专用谣言检测模型，验证大规模未标记数据对提升模型适应性的有效性

Conclusion: 突破传统标注依赖的技术路线，证实通过无监督语义学习结合跨时空数据积累可显著增强谣言检测系统的实用性和鲁棒性

Abstract: With the development of social media, rumors spread quickly, cause great harm
to society and economy. Thereby, many effective rumor detection methods have
been developed, among which the rumor propagation structure learning based
methods are particularly effective compared to other methods. However, the
existing methods still suffer from many issues including the difficulty to
obtain large-scale labeled rumor datasets, which leads to the low
generalization ability and the performance degeneration on new events since
rumors are time-critical and usually appear with hot topics or newly emergent
events. In order to solve the above problems, in this study, we used
large-scale unlabeled topic datasets crawled from the social media platform
Weibo and Twitter with claim propagation structure to improve the semantic
learning ability of a graph reprentation learing model on various topics. We
use three typical graph self-supervised methods, InfoGraph, JOAO and GraphMAE
in two commonly used training strategies, to verify the performance of general
graph semi-supervised methods in rumor detection tasks. In addition, for
alleviating the time and topic difference between unlabeled topic data and
rumor data, we also collected a rumor dataset covering a variety of topics over
a decade (10-year ago from 2022) from the Weibo rumor-refuting platform. Our
experiments show that these general graph self-supervised learning methods
outperform previous methods specifically designed for rumor detection tasks and
achieve good performance under few-shot conditions, demonstrating the better
generalization ability with the help of our massive unlabeled topic dataset.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [78] [AgREE: Agentic Reasoning for Knowledge Graph Completion on Emerging Entities](https://arxiv.org/abs/2508.04118)
*Ruochen Zhao,Simone Conia,Eric Peng,Min Li,Saloni Potdar*

Main category: cs.AI

TL;DR: 提出AgREE框架通过代理驱动的迭代检索与多步推理，无需训练即可动态构建知识图谱三元组，显著提升新兴实体知识补全效果


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱补全方法过度依赖预训练语言模型的参数知识，难以捕捉动态信息环境中不断涌现的新实体信息

Method: 结合智能代理的迭代检索策略与多步推理机制，通过动态信息检索构建知识图谱三元组

Result: 零训练条件下性能超越现有方法13.7%，特别在新兴实体处理方面表现突出，并提出新的评估基准

Conclusion: 验证了代理推理与战略信息检索结合的有效性，为动态环境下的知识图谱维护提供了创新解决方案

Abstract: Open-domain Knowledge Graph Completion (KGC) faces significant challenges in
an ever-changing world, especially when considering the continual emergence of
new entities in daily news. Existing approaches for KGC mainly rely on
pretrained language models' parametric knowledge, pre-constructed queries, or
single-step retrieval, typically requiring substantial supervision and training
data. Even so, they often fail to capture comprehensive and up-to-date
information about unpopular and/or emerging entities. To this end, we introduce
Agentic Reasoning for Emerging Entities (AgREE), a novel agent-based framework
that combines iterative retrieval actions and multi-step reasoning to
dynamically construct rich knowledge graph triplets. Experiments show that,
despite requiring zero training efforts, AgREE significantly outperforms
existing methods in constructing knowledge graph triplets, especially for
emerging entities that were not seen during language models' training
processes, outperforming previous methods by up to 13.7%. Moreover, we propose
a new evaluation methodology that addresses a fundamental weakness of existing
setups and a new benchmark for KGC on emerging entities. Our work demonstrates
the effectiveness of combining agent-based reasoning with strategic information
retrieval for maintaining up-to-date knowledge graphs in dynamic information
environments.

</details>


### [79] [Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents](https://arxiv.org/abs/2508.04412)
*Thassilo M. Schiepanski,Nicholas Piël*

Main category: cs.AI

TL;DR: 提出首个DOM降采样算法D2Snap，成功将DOM快照输入规模控制在千级token量级，在网页代理任务中性能超越GUI快照基线8%


<details>
  <summary>Details</summary>
Motivation: 现有网页代理依赖图像式GUI快照，但DOM结构更适配LLM的代码处理能力。DOM快照的token量过大会超出模型处理能力，需要降采样解决方案

Method: 开发基于GPT-4o的DOM降采样算法D2Snap，通过保留DOM层次结构特征实现有效信息压缩

Result: 降采样后的DOM快照成功率67%，与GUI基线(65%)相当；在更高token量配置下成功率提升8%，且DOM层次结构被证明是有效的UI特征

Conclusion: DOM降采样技术有效突破网页代理的输入瓶颈，DOM结构本身携带的层次信息对LLM理解UI界面具有重要价值

Abstract: Frontier LLMs only recently enabled serviceable, autonomous web agents. At
that, a model poses as an instantaneous domain model backend. Ought to suggest
interaction, it is consulted with a web-based task and respective application
state. The key problem lies in application state serialisation
$\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are
premised on grounded GUI snapshots, i.e., screenshots enhanced with visual
cues. Not least to resemble human perception, but for images representing
relatively cheap means of model input. LLM vision still lag behind code
interpretation capabilities. DOM snapshots, which structurally resemble HTML,
impose a desired alternative. Vast model input token size, however, disables
reliable implementation with web agents to date.
  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a
GPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web
dataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a
grounded GUI snapshot baseline (65%) $\unicode{x2013}$ within the same input
token order of magnitude (1e3). Our best evaluated configurations
$\unicode{x2013}$ one token order above, but within the model's context window
$\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,
yields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.

</details>


### [80] [OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use](https://arxiv.org/abs/2508.04482)
*Xueyu Hu,Tao Xiong,Biao Yi,Zishu Wei,Ruixuan Xiao,Yurun Chen,Jiasheng Ye,Meiling Tao,Xiangxin Zhou,Ziyu Zhao,Yuhuai Li,Shengze Xu,Shenzhi Wang,Xinchen Xu,Shuofei Qiao,Zhaokai Wang,Kun Kuang,Tieyong Zeng,Liang Wang,Jiwei Li,Yuchen Eleanor Jiang,Wangchunshu Zhou,Guoyin Wang,Keting Yin,Zhou Zhao,Hongxia Yang,Fan Wu,Shengyu Zhang,Fei Wu*

Main category: cs.AI

TL;DR: 本文系统综述了基于多模态大语言模型的OS Agents技术，剖析其核心组件、构建方法、评估体系及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 受J.A.R.V.I.S人工智能助手启发，随着(M)LLM技术的发展，操作系统环境下实现任务自动化的智能代理研究亟需系统性总结。

Method: 从环境/观测/动作空间三维度解构OS Agents，提出理解-规划-执行三大核心能力框架，分析领域基础模型与代理框架构建方法论。

Result: 建立首个OS Agents研究知识体系，开源GitHub资源库，提出安全隐私-个性化-自进化等9大研究方向，成果被ACL 2025接收。

Conclusion: OS Agents将重构人机交互范式，需突破安全可信、场景泛化等技术瓶颈，个性化与自进化能力是下一代系统智能体的关键特征。

Abstract: The dream to create AI assistants as capable and versatile as the fictional
J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution
of (multi-modal) large language models ((M)LLMs), this dream is closer to
reality, as (M)LLM-based Agents using computing devices (e.g., computers and
mobile phones) by operating within the environments and interfaces (e.g.,
Graphical User Interface (GUI)) provided by operating systems (OS) to automate
tasks have significantly advanced. This paper presents a comprehensive survey
of these advanced agents, designated as OS Agents. We begin by elucidating the
fundamentals of OS Agents, exploring their key components including the
environment, observation space, and action space, and outlining essential
capabilities such as understanding, planning, and grounding. We then examine
methodologies for constructing OS Agents, focusing on domain-specific
foundation models and agent frameworks. A detailed review of evaluation
protocols and benchmarks highlights how OS Agents are assessed across diverse
tasks. Finally, we discuss current challenges and identify promising directions
for future research, including safety and privacy, personalization and
self-evolution. This survey aims to consolidate the state of OS Agents
research, providing insights to guide both academic inquiry and industrial
development. An open-source GitHub repository is maintained as a dynamic
resource to foster further innovation in this field. We present a 9-page
version of our work, accepted by ACL 2025, to provide a concise overview to the
domain.

</details>


### [81] [SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience](https://arxiv.org/abs/2508.04700)
*Zeyi Sun,Ziyu Liu,Yuhang Zang,Yuhang Cao,Xiaoyi Dong,Tong Wu,Dahua Lin,Jiaqi Wang*

Main category: cs.AI

TL;DR: 提出SEAgent自进化框架，通过体验式学习和课程生成机制，使计算机使用代理在无标注数据下自主学习新软件，成功率提升23.2%至34.5%。


<details>
  <summary>Details</summary>
Motivation: 现有基于人类标注数据的计算机使用代理难以适应新型/专业软件环境，需要构建自主进化框架突破标注依赖瓶颈。

Method: 1. 世界状态模型实现轨迹评估 + 课程生成器创建渐进式任务；2. 通过试错学习更新策略（失败动作对抗模仿+成功轨迹GRPO优化）；3. 专家到通才训练策略整合个体经验。

Result: 在OS-World的5个新软件环境中，成功率从基线11.3%提升至34.5%，超越专业代理集合的表现。

Conclusion: SEAgent通过自进化机制和课程学习范式，显著提升代理的软件适应能力，为持续自主进化提供了可行性验证。

Abstract: Repurposing large vision-language models (LVLMs) as computer use agents
(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled
data. However, these models often struggle with novel and specialized software,
particularly in scenarios lacking human annotations. To address this challenge,
we propose SEAgent, an agentic self-evolving framework enabling CUAs to
autonomously evolve through interactions with unfamiliar software.
Specifically, SEAgent empowers computer-use agents to autonomously master novel
software environments via experiential learning, where agents explore new
software, learn through iterative trial-and-error, and progressively tackle
auto-generated tasks organized from simple to complex. To achieve this goal, we
design a World State Model for step-wise trajectory assessment, along with a
Curriculum Generator that generates increasingly diverse and challenging tasks.
The agent's policy is updated through experiential learning, comprised of
adversarial imitation of failure actions and Group Relative Policy Optimization
(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist
training strategy that integrates individual experiential insights from
specialist agents, facilitating the development of a stronger generalist CUA
capable of continuous autonomous evolution. This unified agent ultimately
achieves performance surpassing ensembles of individual specialist agents on
their specialized software. We validate the effectiveness of SEAgent across
five novel software environments within OS-World. Our approach achieves a
significant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a
competitive open-source CUA, i.e., UI-TARS.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [82] [CX-Mind: A Pioneering Multimodal Large Language Model for Interleaved Reasoning in Chest X-ray via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2508.03733)
*Wenjie Li,Yujie Zhang,Haoran Sun,Yueqi Li,Fanrui Zhang,Mengzhe Xu,Victoria Borja Clausich,Sade Mellin,Renhao Yang,Chenrun Wang,Jethro Zih-Shuo Wang,Shiyi Yao,Gen Li,Yidong Xu,Hanyu Wang,Yilin Huang,Angela Lin Wang,Chen Shi,Yin Zhang,Jianan Guo,Luqi Yang,Renxuan Li,Yang Xu,Jiawei Liu,Yao Zhang,Lei Liu,Carlos Gutiérrez SanRomán,Lei Wang*

Main category: cs.LG

TL;DR: 提出CX-Mind模型，通过课程强化学习（CuRL-VPR）实现CXR任务的交替推理，显著提升多任务诊断性能


<details>
  <summary>Details</summary>
Motivation: 现有医学多模态模型依赖一次性诊断方法，缺乏推理过程的可验证监督，导致多任务CXR诊断存在推理冗长、奖励稀疏和幻觉问题

Method: 构建包含708,473张影像的CX-Set数据集，采用两阶段课程强化学习框架（基础推理稳定化→开放域迁移），结合基于规则的过程奖励机制

Result: 平均性能超越同类CXR模型25.1%，在真实临床数据集（Rui-CXR）14种疾病诊断中Recall@1显著领先，多中心专家评估验证多维临床效用

Conclusion: CX-Mind通过可验证推理过程设计，在视觉理解、文本生成和时空对齐方面建立新标杆，为医学影像诊断提供可靠生成模型

Abstract: Chest X-ray (CXR) imaging is one of the most widely used diagnostic
modalities in clinical practice, encompassing a broad spectrum of diagnostic
tasks. Recent advancements have seen the extensive application of
reasoning-based multimodal large language models (MLLMs) in medical imaging to
enhance diagnostic efficiency and interpretability. However, existing
multimodal models predominantly rely on "one-time" diagnostic approaches,
lacking verifiable supervision of the reasoning process. This leads to
challenges in multi-task CXR diagnosis, including lengthy reasoning, sparse
rewards, and frequent hallucinations. To address these issues, we propose
CX-Mind, the first generative model to achieve interleaved "think-answer"
reasoning for CXR tasks, driven by curriculum-based reinforcement learning and
verifiable process rewards (CuRL-VPR). Specifically, we constructed an
instruction-tuning dataset, CX-Set, comprising 708,473 images and 2,619,148
samples, and generated 42,828 high-quality interleaved reasoning data points
supervised by clinical reports. Optimization was conducted in two stages under
the Group Relative Policy Optimization framework: initially stabilizing basic
reasoning with closed-domain tasks, followed by transfer to open-domain
diagnostics, incorporating rule-based conditional process rewards to bypass the
need for pretrained reward models. Extensive experimental results demonstrate
that CX-Mind significantly outperforms existing medical and general-domain
MLLMs in visual understanding, text generation, and spatiotemporal alignment,
achieving an average performance improvement of 25.1% over comparable
CXR-specific models. On real-world clinical dataset (Rui-CXR), CX-Mind achieves
a mean recall@1 across 14 diseases that substantially surpasses the second-best
results, with multi-center expert evaluations further confirming its clinical
utility across multiple dimensions.

</details>


### [83] [GTPO: Trajectory-Based Policy Optimization in Large Language Models](https://arxiv.org/abs/2508.03772)
*Marco Simoni,Aleksandar Fontana,Giulio Rossolini,Andrea Saracino*

Main category: cs.LG

TL;DR: 提出GTPO算法解决GRPO存在的令牌冲突更新和策略崩溃问题，通过冲突令牌保护和熵阈值过滤机制，在多个数学推理基准测试中验证了有效性


<details>
  <summary>Details</summary>
Motivation: 针对GRPO存在的两个核心问题：(1) 冲突令牌在不同奖励补全中出现导致梯度抵消，(2) 负奖励补全使模型输出分布趋于平坦化

Method: GTPO通过识别冲突令牌并选择性跳过负更新，同时引入熵阈值过滤机制防止策略崩溃，无需KL散度正则化

Result: 在GSM8K、MATH和AIME 2024基准测试中验证了训练稳定性和性能提升

Conclusion: GTPO克服了GRPO的局限性，实现了不依赖参考模型的稳定优化，在数学推理任务中展现出优越性

Abstract: Policy-based optimizations are widely adopted today for the training and
alignment of language models, where one of the most recent and effective
approaches is Group-relative Policy Optimization (GRPO). In this paper, we
reveals and analyze two major limitations of GRPO: (i) tokens frequently appear
in completions with both positive and negative rewards, leading to conflicting
gradient updates that can reduce their output probability, even though can be
essential for maintaining proper structure; (ii) negatively rewarded
completions may penalize confident responses and shift model decisions toward
unlikely tokens, progressively flattening the output distribution and degrading
learning. To address these issues and provide a more stable and effective
policy optimization strategy, we introduce GTPO (Group-relative
Trajectory-based Policy Optimization), which identifies conflict tokens, tokens
appearing in the same position across completions with opposite rewards,
protects them by skipping negative updates, while amplifying positive ones. To
further prevent policy collapse, GTPO filters out completions whose entropy
exceeds a provable threshold. Unlike GRPO, GTPO does not rely on KL-divergence
regularization, eliminating the need for a reference model during training,
while still ensuring greater training stability and improved performance,
validated through multiple experiments on GSM8K, MATH and AIME 2024 benchmarks.

</details>


### [84] [COPO: Consistency-Aware Policy Optimization](https://arxiv.org/abs/2508.04138)
*Jinghang Han,Jiawei Chen,Hang Shao,Hao Ma,Mingcheng Li,Xintian Shen,Lihao Zheng,Wei Chen,Tao Wei,Lihua Zhang*

Main category: cs.LG

TL;DR: 提出一致性感知策略优化框架，通过全局奖励和混合机制解决强化学习中群体优势退化问题，显著提升数学推理任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则奖励的强化学习方法在群体响应高度一致时会导致训练信号消失，限制了训练效率和模型性能。需要解决群体优势退化问题以提升大语言模型的推理能力。

Method: 1. 引入基于结果一致性的结构化全局奖励机制
2. 设计全局损失函数保持有效学习信号
3. 采用基于熵的软混合机制动态平衡局部优势估计与全局优化
4. 实现探索与收敛的动态过渡

Result: 在多个数学推理基准测试中获得显著性能提升，代码已开源（https://github.com/hijih/copo-code.git）

Conclusion: 该框架通过创新的奖励设计和优化策略，有效解决了群体优势退化问题，具有强鲁棒性和广泛适用性，为强化学习优化提供了新思路。

Abstract: Reinforcement learning has significantly enhanced the reasoning capabilities
of Large Language Models (LLMs) in complex problem-solving tasks. Recently, the
introduction of DeepSeek R1 has inspired a surge of interest in leveraging
rule-based rewards as a low-cost alternative for computing advantage functions
and guiding policy optimization. However, a common challenge observed across
many replication and extension efforts is that when multiple sampled responses
under a single prompt converge to identical outcomes, whether correct or
incorrect, the group-based advantage degenerates to zero. This leads to
vanishing gradients and renders the corresponding samples ineffective for
learning, ultimately limiting training efficiency and downstream performance.
To address this issue, we propose a consistency-aware policy optimization
framework that introduces a structured global reward based on outcome
consistency, the global loss based on it ensures that, even when model outputs
show high intra-group consistency, the training process still receives
meaningful learning signals, which encourages the generation of correct and
self-consistent reasoning paths from a global perspective. Furthermore, we
incorporate an entropy-based soft blending mechanism that adaptively balances
local advantage estimation with global optimization, enabling dynamic
transitions between exploration and convergence throughout training. Our method
introduces several key innovations in both reward design and optimization
strategy. We validate its effectiveness through substantial performance gains
on multiple mathematical reasoning benchmarks, highlighting the proposed
framework's robustness and general applicability. Code of this work has been
released at https://github.com/hijih/copo-code.git.

</details>


### [85] [Causal Reflection with Language Models](https://arxiv.org/abs/2508.04495)
*Abi Aryan,Zac Liu*

Main category: cs.LG

TL;DR: 提出Causal Reflection框架，通过动态因果建模与Reflect机制增强AI代理的因果推理能力，结合LLMs实现自我修正与自然语言解释


<details>
  <summary>Details</summary>
Motivation: 现有LLMs与强化学习代理缺乏系统因果理解，依赖虚假相关性导致决策脆弱性，需建立显式的因果推理机制

Method: 构建状态-动作-时间-扰动四维因果函数，设计反射机制检测预测偏差并生成修正假设，将LLMs转化为结构化推理引擎

Result: 建立具备环境适应、自校正和因果解释能力的反射型代理理论基础，支持延迟/非线性效应的系统性处理

Conclusion: 该框架为动态环境中因果感知AI系统提供新范式，打通形式化建模与自然语言解释的认知闭环

Abstract: While LLMs exhibit impressive fluency and factual recall, they struggle with
robust causal reasoning, often relying on spurious correlations and brittle
patterns. Similarly, traditional Reinforcement Learning agents also lack causal
understanding, optimizing for rewards without modeling why actions lead to
outcomes. We introduce Causal Reflection, a framework that explicitly models
causality as a dynamic function over state, action, time, and perturbation,
enabling agents to reason about delayed and nonlinear effects. Additionally, we
define a formal Reflect mechanism that identifies mismatches between predicted
and observed outcomes and generates causal hypotheses to revise the agent's
internal model. In this architecture, LLMs serve not as black-box reasoners,
but as structured inference engines translating formal causal outputs into
natural language explanations and counterfactuals. Our framework lays the
theoretical groundwork for Causal Reflective agents that can adapt,
self-correct, and communicate causal understanding in evolving environments.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [86] [Multilingual Source Tracing of Speech Deepfakes: A First Benchmark](https://arxiv.org/abs/2508.04143)
*Xi Xuan,Yang Xiao,Rohan Kumar Das,Tomi Kinnunen*

Main category: eess.AS

TL;DR: 提出首个多语言语音深度伪造溯源基准，涵盖单语言和跨语言场景的系统性研究框架


<details>
  <summary>Details</summary>
Motivation: 生成式AI技术使跨语言伪造语音日益逼真，现有研究集中于伪造检测而忽视溯源追踪，亟需建立模型溯源评估体系

Method: 对比DSP和SSL建模方法，探究不同语言微调SSL表征对跨语言泛化的影响，评估未见语言/说话人的泛化能力

Result: 首次揭示训练与推理语言差异对溯源模型的挑战，发布包含数据集和代码的开源基准

Conclusion: 建立多语言溯源评估基准，为跨语言场景下的生成模型识别提供理论基础和技术支持，推动深度伪造溯源研究发展

Abstract: Recent progress in generative AI has made it increasingly easy to create
natural-sounding deepfake speech from just a few seconds of audio. While these
tools support helpful applications, they also raise serious concerns by making
it possible to generate convincing fake speech in many languages. Current
research has largely focused on detecting fake speech, but little attention has
been given to tracing the source models used to generate it. This paper
introduces the first benchmark for multilingual speech deepfake source tracing,
covering both mono- and cross-lingual scenarios. We comparatively investigate
DSP- and SSL-based modeling; examine how SSL representations fine-tuned on
different languages impact cross-lingual generalization performance; and
evaluate generalization to unseen languages and speakers. Our findings offer
the first comprehensive insights into the challenges of identifying speech
generation models when training and inference languages differ. The dataset,
protocol and code are available at
https://github.com/xuanxixi/Multilingual-Source-Tracing.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [87] [MegaWika 2: A More Comprehensive Multilingual Collection of Articles and their Sources](https://arxiv.org/abs/2508.03828)
*Samuel Barham,Chandler May,Benjamin Van Durme*

Main category: cs.DL

TL;DR: MegaWika 2是维基百科多语言数据集升级版，扩展6倍文章量并优化数据结构，支持事实核查与跨语言分析


<details>
  <summary>Details</summary>
Motivation: 解决原版MegaWika覆盖范围有限的问题，支持更全面的跨语言/跨时间事实核查研究需求

Method: 通过抓取维基百科文章及其网络资源，采用字符偏移量存储引文信息的结构化数据构建方法

Result: 数据集规模扩展至原版6倍（文章）和2倍（完整抓取引文），新增时间维度与多语言分析功能

Conclusion: 该数据集为事实核查、跨语言分析等研究提供了更强大的基础设施，推动数字内容可信度验证研究发展

Abstract: We introduce MegaWika 2, a large, multilingual dataset of Wikipedia articles
with their citations and scraped web sources; articles are represented in a
rich data structure, and scraped source texts are stored inline with precise
character offsets of their citations in the article text. MegaWika 2 is a major
upgrade from the original MegaWika, spanning six times as many articles and
twice as many fully scraped citations. Both MegaWika and MegaWika 2 support
report generation research ; whereas MegaWika also focused on supporting
question answering and retrieval applications, MegaWika 2 is designed to
support fact checking and analyses across time and language.

</details>


### [88] [Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers](https://arxiv.org/abs/2508.03962)
*Paris Koloveas,Serafeim Chatzopoulos,Dionysis Diamantis,Christos Tryfonopoulos,Thanasis Vergoulis*

Main category: cs.DL

TL;DR: BIP! Finder新增动态摘要功能，通过即时概览和文献综述两种模式加速文献理解


<details>
  <summary>Details</summary>
Motivation: 科学文献数量激增导致研究人员需要高效工具从海量论文中快速提炼核心信息，传统手动阅读方式效率低下

Method: 在现有影响因子排序系统基础上，集成AI生成两种摘要：即时速览型(简洁)和文献综述型(结构化)

Result: 系统能动态结合文献影响力排名，生成上下文敏感的综合性叙述，显著提升文献处理效率

Conclusion: 该创新将文献发现到理解的工作流效率提升数倍，为科研人员提供了智能化的知识整合解决方案

Abstract: The growing volume of scientific literature makes it challenging for
scientists to move from a list of papers to a synthesized understanding of a
topic. Because of the constant influx of new papers on a daily basis, even if a
scientist identifies a promising set of papers, they still face the tedious
task of individually reading through dozens of titles and abstracts to make
sense of occasionally conflicting findings. To address this critical bottleneck
in the research workflow, we introduce a summarization feature to BIP! Finder,
a scholarly search engine that ranks literature based on distinct impact
aspects like popularity and influence. Our approach enables users to generate
two types of summaries from top-ranked search results: a concise summary for an
instantaneous at-a-glance comprehension and a more comprehensive literature
review-style summary for greater, better-organized comprehension. This ability
dynamically leverages BIP! Finder's already existing impact-based ranking and
filtering features to generate context-sensitive, synthesized narratives that
can significantly accelerate literature discovery and comprehension.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [89] [A Social Data-Driven System for Identifying Estate-related Events and Topics](https://arxiv.org/abs/2508.03711)
*Wenchuan Mu,Menglin Li,Kwan Hui Lim*

Main category: cs.IR

TL;DR: 开发基于语言模型的社交媒体分析系统，用于房地产事件检测与地理定位，支持智慧城市管理


<details>
  <summary>Details</summary>
Motivation: 利用社交媒体实时数据解决城市化进程中房地产相关问题，弥补传统数据时效性与覆盖面的不足

Method: 分层分类框架（相关性过滤+主题分类）结合Transformer地理定位模块，处理非结构化社交媒体数据

Result: 实现房地产事件自动化识别与POI级地理定位，为城市管理提供实时数据洞察

Conclusion: 融合NLP与地理定位技术的系统架构，有效提升城市治理的响应速度与决策精准度

Abstract: Social media platforms such as Twitter and Facebook have become deeply
embedded in our everyday life, offering a dynamic stream of localized news and
personal experiences. The ubiquity of these platforms position them as valuable
resources for identifying estate-related issues, especially in the context of
growing urban populations. In this work, we present a language model-based
system for the detection and classification of estate-related events from
social media content. Our system employs a hierarchical classification
framework to first filter relevant posts and then categorize them into
actionable estate-related topics. Additionally, for posts lacking explicit
geotags, we apply a transformer-based geolocation module to infer posting
locations at the point-of-interest level. This integrated approach supports
timely, data-driven insights for urban management, operational response and
situational awareness.

</details>


### [90] [ConvMix: A Mixed-Criteria Data Augmentation Framework for Conversational Dense Retrieval](https://arxiv.org/abs/2508.04001)
*Fengran Mo,Jinghan Zhang,Yuchen Hui,Jia Ao Sun,Zhichao Xu,Zhan Su,Jian-Yun Nie*

Main category: cs.IR

TL;DR: 提出ConvMix混合框架通过大模型增强对话式检索的数据多样性，在五个基准测试中超越基线方法


<details>
  <summary>Details</summary>
Motivation: 现有对话密集检索方法受限于数据稀缺问题，需要更有效的数据增强方案

Method: 1. 基于大语言模型的双相关性判断增强 
2. 质量控制机制整合多样化样本 
3. 近分布监督结合多源标注数据

Result: 在五个广泛使用的基准测试中，ConvMix训练得到的对话密集检索器性能优于现有方法

Conclusion: ConvMix框架通过创新的数据增强策略有效解决了数据稀缺问题，显著提升了对话式检索效果

Abstract: Conversational search aims to satisfy users' complex information needs via
multiple-turn interactions. The key challenge lies in revealing real users'
search intent from the context-dependent queries. Previous studies achieve
conversational search by fine-tuning a conversational dense retriever with
relevance judgments between pairs of context-dependent queries and documents.
However, this training paradigm encounters data scarcity issues. To this end,
we propose ConvMix, a mixed-criteria framework to augment conversational dense
retrieval, which covers more aspects than existing data augmentation
frameworks. We design a two-sided relevance judgment augmentation schema in a
scalable manner via the aid of large language models. Besides, we integrate the
framework with quality control mechanisms to obtain semantically diverse
samples and near-distribution supervisions to combine various annotated data.
Experimental results on five widely used benchmarks show that the
conversational dense retriever trained by our ConvMix framework outperforms
previous baseline methods, which demonstrates our superior effectiveness.

</details>


### [91] [Do Recommender Systems Really Leverage Multimodal Content? A Comprehensive Analysis on Multimodal Representations for Recommendation](https://arxiv.org/abs/2508.04571)
*Claudio Pomo,Matteo Attimonelli,Danilo Danese,Fedelucio Narducci,Tommaso Di Noia*

Main category: cs.IR

TL;DR: 通过大型视觉语言模型生成多模态对齐嵌入，提升推荐系统性能并验证语义理解深度


<details>
  <summary>Details</summary>
Motivation: 现有多模态推荐系统依赖模态专用编码器和临时融合策略，缺乏跨模态对齐控制，需验证性能提升是否源于真实多模态理解

Method: 利用大型视觉语言模型(LVLM)生成结构化提示的多模态嵌入，实现无需人工融合的语义对齐表示

Result: 实验显示LVLM嵌入带来显著性能提升，且可解码为结构化文本描述用于推荐系统增强

Conclusion: 研究证实语义丰富表示的重要性，确立LVLM作为构建推荐系统鲁棒多模态表示的基础框架

Abstract: Multimodal Recommender Systems aim to improve recommendation accuracy by
integrating heterogeneous content, such as images and textual metadata. While
effective, it remains unclear whether their gains stem from true multimodal
understanding or increased model complexity. This work investigates the role of
multimodal item embeddings, emphasizing the semantic informativeness of the
representations. Initial experiments reveal that embeddings from standard
extractors (e.g., ResNet50, Sentence-Bert) enhance performance, but rely on
modality-specific encoders and ad hoc fusion strategies that lack control over
cross-modal alignment. To overcome these limitations, we leverage Large
Vision-Language Models (LVLMs) to generate multimodal-by-design embeddings via
structured prompts. This approach yields semantically aligned representations
without requiring any fusion. Experiments across multiple settings show notable
performance improvements. Furthermore, LVLMs embeddings offer a distinctive
advantage: they can be decoded into structured textual descriptions, enabling
direct assessment of their multimodal comprehension. When such descriptions are
incorporated as side content into recommender systems, they improve
recommendation performance, empirically validating the semantic depth and
alignment encoded within LVLMs outputs. Our study highlights the importance of
semantically rich representations and positions LVLMs as a compelling
foundation for building robust and meaningful multimodal representations in
recommendation tasks.

</details>


### [92] [Query Attribute Modeling: Improving search relevance with Semantic Search and Meta Data Filtering](https://arxiv.org/abs/2508.04683)
*Karthik Menon,Batool Arhamna Haider,Muhammad Arham,Kanwal Mehreen,Ram Mohan Rao Kadiyala,Hamza Farooq*

Main category: cs.IR

TL;DR: 提出QAM混合框架，通过分解文本查询为元数据标签和语义元素，显著提升电商搜索效果（mAP@5达52.99%）。


<details>
  <summary>Details</summary>
Motivation: 传统搜索方法（BM25/语义搜索/混合搜索）存在噪声干扰和相关性不足的问题，需结构化处理自然语言查询以提升准确率。

Method: 结合元数据过滤与语义分析：1. 自动提取查询中的元数据过滤器 2. 语义元素解析 3. 混合两种结果提升检索聚焦度

Result: 在亚马逊玩具数据集测试中，QAM的mAP@5达52.99%，超越BM25（20.82%）、语义搜索（31.74%）、交叉编码器（47.58%）和RRF混合方法（48.91%）

Conclusion: QAM为电商等企业搜索系统提供了噪声鲁棒性强、相关性显著提升的搜索解决方案，验证了结构化查询分解的有效性。

Abstract: This study introduces Query Attribute Modeling (QAM), a hybrid framework that
enhances search precision and relevance by decomposing open text queries into
structured metadata tags and semantic elements. QAM addresses traditional
search limitations by automatically extracting metadata filters from free-form
text queries, reducing noise and enabling focused retrieval of relevant items.
  Experimental evaluation using the Amazon Toys Reviews dataset (10,000 unique
items with 40,000+ reviews and detailed product attributes) demonstrated QAM's
superior performance, achieving a mean average precision at 5 (mAP@5) of
52.99\%. This represents significant improvement over conventional methods,
including BM25 keyword search, encoder-based semantic similarity search,
cross-encoder re-ranking, and hybrid search combining BM25 and semantic results
via Reciprocal Rank Fusion (RRF). The results establish QAM as a robust
solution for Enterprise Search applications, particularly in e-commerce
systems.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [93] [Health Insurance Coverage Rule Interpretation Corpus: Law, Policy, and Medical Guidance for Health Insurance Coverage Understanding](https://arxiv.org/abs/2508.03718)
*Mike Gartner*

Main category: cs.CY

TL;DR: 通过自然语言处理技术构建美国医疗保险相关语料库，开发申诉结果预测模型以提升司法公正和医疗资源获取


<details>
  <summary>Details</summary>
Motivation: 美国医疗保险体系复杂导致弱势群体维权困难，现有NLP语料库缺乏案例分析的上下文支持，需构建专业数据集推动医疗申诉自动化处理

Method: 收集医疗法律文本构建专业语料库，设计保险申诉结果预测任务，开发标注基准并训练预测模型

Result: 创建首个整合法律医学文本的医疗保险语料库，开发出支持保险申诉结果预测的基准模型

Conclusion: NLP技术可有效提升医疗保险争议处理效率，为监管机构和患者提供自动化决策支持，特别有助于弱势群体维护权益

Abstract: U.S. health insurance is complex, and inadequate understanding and limited
access to justice have dire implications for the most vulnerable. Advances in
natural language processing present an opportunity to support efficient,
case-specific understanding, and to improve access to justice and healthcare.
Yet existing corpora lack context necessary for assessing even simple cases. We
collect and release a corpus of reputable legal and medical text related to
U.S. health insurance. We also introduce an outcome prediction task for health
insurance appeals designed to support regulatory and patient self-help
applications, and release a labeled benchmark for our task, and models trained
on it.

</details>


### [94] [Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference](https://arxiv.org/abs/2508.04586)
*Nuo Chen,Moming Duan,Andre Huikai Lin,Qian Wang,Jiaying Wu,Bingsheng He*

Main category: cs.CY

TL;DR: AI会议面临科学产出过剩、环境负担、心理压力与场地不足四大危机，提出社区联邦会议（CFC）模型实现可持续发展。


<details>
  <summary>Details</summary>
Motivation: 现有集中式AI会议模式在科学（人均年发文4.5篇）、环境（单会议碳排放超城市日排放量）、心理（71%社区言论负向）、后勤（NeurIPS等顶会场地不足）四个维度不可持续。

Method: 通过数据驱动分析十年会议数据，量化科学产出、碳足迹、社区情绪和场地需求，设计分离同行评审/报告/社交的联邦会议架构。

Result: 发现：1）作者年均发文量十年翻倍 2）单会议碳足迹超过主办城市日排放 3）35%讨论涉及心理健康 4）NeurIPS2024参会人数超过场馆容量。

Conclusion: CFC模型通过全球协调-本地实施的分层架构，在保证学术交流质量的同时降低环境负担，增强系统韧性和社区包容性。

Abstract: Artificial Intelligence (AI) conferences are essential for advancing
research, sharing knowledge, and fostering academic community. However, their
rapid expansion has rendered the centralized conference model increasingly
unsustainable. This paper offers a data-driven diagnosis of a structural crisis
that threatens the foundational goals of scientific dissemination, equity, and
community well-being. We identify four key areas of strain: (1) scientifically,
with per-author publication rates more than doubling over the past decade to
over 4.5 papers annually; (2) environmentally, with the carbon footprint of a
single conference exceeding the daily emissions of its host city; (3)
psychologically, with 71% of online community discourse reflecting negative
sentiment and 35% referencing mental health concerns; and (4) logistically,
with attendance at top conferences such as NeurIPS 2024 beginning to outpace
venue capacity. These pressures point to a system that is misaligned with its
core mission. In response, we propose the Community-Federated Conference (CFC)
model, which separates peer review, presentation, and networking into globally
coordinated but locally organized components, offering a more sustainable,
inclusive, and resilient path forward for AI research.

</details>
